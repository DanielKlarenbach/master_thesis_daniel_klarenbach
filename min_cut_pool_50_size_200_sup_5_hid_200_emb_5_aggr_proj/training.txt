Started preprocessing dataset
Number of training samples: 2040
Number of validation samples: 582
Number of testing samples: 291
Using cuda device
Epoch 1
-------------------------------
Batch 1/64 loss: 0.47995519638061523
Batch 2/64 loss: 0.3933190107345581
Batch 3/64 loss: 0.37262535095214844
Batch 4/64 loss: 0.3676829934120178
Batch 5/64 loss: 0.36380136013031006
Batch 6/64 loss: 0.3616943955421448
Batch 7/64 loss: 0.3602551817893982
Batch 8/64 loss: 0.35878986120224
Batch 9/64 loss: 0.356167197227478
Batch 10/64 loss: 0.3569130301475525
Batch 11/64 loss: 0.35589122772216797
Batch 12/64 loss: 0.3580852746963501
Batch 13/64 loss: 0.35516631603240967
Batch 14/64 loss: 0.35706281661987305
Batch 15/64 loss: 0.3548199534416199
Batch 16/64 loss: 0.3537425994873047
Batch 17/64 loss: 0.3522038459777832
Batch 18/64 loss: 0.35240113735198975
Batch 19/64 loss: 0.3508744239807129
Batch 20/64 loss: 0.349706768989563
Batch 21/64 loss: 0.3506733179092407
Batch 22/64 loss: 0.3501243591308594
Batch 23/64 loss: 0.35017454624176025
Batch 24/64 loss: 0.3508898615837097
Batch 25/64 loss: 0.34917593002319336
Batch 26/64 loss: 0.3477966785430908
Batch 27/64 loss: 0.3480224013328552
Batch 28/64 loss: 0.3471720218658447
Batch 29/64 loss: 0.3474407196044922
Batch 30/64 loss: 0.3433798551559448
Batch 31/64 loss: 0.3459211587905884
Batch 32/64 loss: 0.34688496589660645
Batch 33/64 loss: 0.34750503301620483
Batch 34/64 loss: 0.34253984689712524
Batch 35/64 loss: 0.3442894220352173
Batch 36/64 loss: 0.34412699937820435
Batch 37/64 loss: 0.34295928478240967
Batch 38/64 loss: 0.34344685077667236
Batch 39/64 loss: 0.3421001434326172
Batch 40/64 loss: 0.339228093624115
Batch 41/64 loss: 0.3421316146850586
Batch 42/64 loss: 0.3437119722366333
Batch 43/64 loss: 0.3410297632217407
Batch 44/64 loss: 0.3365216851234436
Batch 45/64 loss: 0.33937668800354004
Batch 46/64 loss: 0.33975255489349365
Batch 47/64 loss: 0.34012091159820557
Batch 48/64 loss: 0.33670133352279663
Batch 49/64 loss: 0.33925533294677734
Batch 50/64 loss: 0.3352609872817993
Batch 51/64 loss: 0.33907997608184814
Batch 52/64 loss: 0.3370457887649536
Batch 53/64 loss: 0.3366931676864624
Batch 54/64 loss: 0.33217954635620117
Batch 55/64 loss: 0.3365294337272644
Batch 56/64 loss: 0.3329286575317383
Batch 57/64 loss: 0.33600425720214844
Batch 58/64 loss: 0.3380049467086792
Batch 59/64 loss: 0.33436959981918335
Batch 60/64 loss: 0.33179765939712524
Batch 61/64 loss: 0.33427441120147705
Batch 62/64 loss: 0.3338291645050049
Batch 63/64 loss: 0.33011937141418457
Batch 64/64 loss: 0.33421415090560913
Epoch 1  Train loss: 0.34874335480671304  Val loss: 0.33903140198324144
Saving best model, epoch: 1
Epoch 2
-------------------------------
Batch 1/64 loss: 0.33007681369781494
Batch 2/64 loss: 0.33628469705581665
Batch 3/64 loss: 0.3307974338531494
Batch 4/64 loss: 0.3313199281692505
Batch 5/64 loss: 0.33206772804260254
Batch 6/64 loss: 0.3259226679801941
Batch 7/64 loss: 0.33361613750457764
Batch 8/64 loss: 0.33090734481811523
Batch 9/64 loss: 0.32554715871810913
Batch 10/64 loss: 0.3327542543411255
Batch 11/64 loss: 0.32757771015167236
Batch 12/64 loss: 0.32875603437423706
Batch 13/64 loss: 0.32961761951446533
Batch 14/64 loss: 0.325763463973999
Batch 15/64 loss: 0.3290517330169678
Batch 16/64 loss: 0.33151185512542725
Batch 17/64 loss: 0.33013916015625
Batch 18/64 loss: 0.32863670587539673
Batch 19/64 loss: 0.32363277673721313
Batch 20/64 loss: 0.3302408456802368
Batch 21/64 loss: 0.3252127766609192
Batch 22/64 loss: 0.33037006855010986
Batch 23/64 loss: 0.3287508487701416
Batch 24/64 loss: 0.3263152837753296
Batch 25/64 loss: 0.3199152946472168
Batch 26/64 loss: 0.32065868377685547
Batch 27/64 loss: 0.3196250796318054
Batch 28/64 loss: 0.31841742992401123
Batch 29/64 loss: 0.32153797149658203
Batch 30/64 loss: 0.3226502537727356
Batch 31/64 loss: 0.32490408420562744
Batch 32/64 loss: 0.3217250108718872
Batch 33/64 loss: 0.32428181171417236
Batch 34/64 loss: 0.32716673612594604
Batch 35/64 loss: 0.32373160123825073
Batch 36/64 loss: 0.3279055953025818
Batch 37/64 loss: 0.3229668140411377
Batch 38/64 loss: 0.3241403102874756
Batch 39/64 loss: 0.3227025270462036
Batch 40/64 loss: 0.3213449716567993
Batch 41/64 loss: 0.33032292127609253
Batch 42/64 loss: 0.32202816009521484
Batch 43/64 loss: 0.32134711742401123
Batch 44/64 loss: 0.32485729455947876
Batch 45/64 loss: 0.32344138622283936
Batch 46/64 loss: 0.3263933062553406
Batch 47/64 loss: 0.32406920194625854
Batch 48/64 loss: 0.3160398602485657
Batch 49/64 loss: 0.3205634355545044
Batch 50/64 loss: 0.3174560070037842
Batch 51/64 loss: 0.3184780478477478
Batch 52/64 loss: 0.31575173139572144
Batch 53/64 loss: 0.31748437881469727
Batch 54/64 loss: 0.31841862201690674
Batch 55/64 loss: 0.3231094479560852
Batch 56/64 loss: 0.3205174207687378
Batch 57/64 loss: 0.32580602169036865
Batch 58/64 loss: 0.32138490676879883
Batch 59/64 loss: 0.3152545094490051
Batch 60/64 loss: 0.31970375776290894
Batch 61/64 loss: 0.3191009759902954
Batch 62/64 loss: 0.32259905338287354
Batch 63/64 loss: 0.3218529224395752
Batch 64/64 loss: 0.31745636463165283
Epoch 2  Train loss: 0.32459008600197586  Val loss: 0.3208598865266518
Saving best model, epoch: 2
Epoch 3
-------------------------------
Batch 1/64 loss: 0.3214561939239502
Batch 2/64 loss: 0.317474365234375
Batch 3/64 loss: 0.3125576972961426
Batch 4/64 loss: 0.32301390171051025
Batch 5/64 loss: 0.3180965781211853
Batch 6/64 loss: 0.32077091932296753
Batch 7/64 loss: 0.3242778778076172
Batch 8/64 loss: 0.3241662383079529
Batch 9/64 loss: 0.3247905969619751
Batch 10/64 loss: 0.31629741191864014
Batch 11/64 loss: 0.3187469244003296
Batch 12/64 loss: 0.3133058547973633
Batch 13/64 loss: 0.31410908699035645
Batch 14/64 loss: 0.31518441438674927
Batch 15/64 loss: 0.3166515827178955
Batch 16/64 loss: 0.319907546043396
Batch 17/64 loss: 0.3121575713157654
Batch 18/64 loss: 0.31556951999664307
Batch 19/64 loss: 0.31424200534820557
Batch 20/64 loss: 0.32335948944091797
Batch 21/64 loss: 0.31989240646362305
Batch 22/64 loss: 0.3218303918838501
Batch 23/64 loss: 0.3246883749961853
Batch 24/64 loss: 0.31661713123321533
Batch 25/64 loss: 0.3144127130508423
Batch 26/64 loss: 0.31941038370132446
Batch 27/64 loss: 0.3190639615058899
Batch 28/64 loss: 0.3176688551902771
Batch 29/64 loss: 0.3208288550376892
Batch 30/64 loss: 0.3176614046096802
Batch 31/64 loss: 0.3188592195510864
Batch 32/64 loss: 0.32047921419143677
Batch 33/64 loss: 0.31405866146087646
Batch 34/64 loss: 0.3140060305595398
Batch 35/64 loss: 0.3198390007019043
Batch 36/64 loss: 0.3199653625488281
Batch 37/64 loss: 0.3196306824684143
Batch 38/64 loss: 0.3137613534927368
Batch 39/64 loss: 0.3109775185585022
Batch 40/64 loss: 0.31255942583084106
Batch 41/64 loss: 0.31192052364349365
Batch 42/64 loss: 0.30654531717300415
Batch 43/64 loss: 0.3119916319847107
Batch 44/64 loss: 0.3111312985420227
Batch 45/64 loss: 0.31172841787338257
Batch 46/64 loss: 0.31152236461639404
Batch 47/64 loss: 0.3229777216911316
Batch 48/64 loss: 0.31486594676971436
Batch 49/64 loss: 0.31820517778396606
Batch 50/64 loss: 0.3168792724609375
Batch 51/64 loss: 0.3143026828765869
Batch 52/64 loss: 0.31559866666793823
Batch 53/64 loss: 0.3229162096977234
Batch 54/64 loss: 0.3170260787010193
Batch 55/64 loss: 0.3146967887878418
Batch 56/64 loss: 0.3188741207122803
Batch 57/64 loss: 0.3101670742034912
Batch 58/64 loss: 0.3227763772010803
Batch 59/64 loss: 0.30945682525634766
Batch 60/64 loss: 0.3133666515350342
Batch 61/64 loss: 0.31385916471481323
Batch 62/64 loss: 0.30803173780441284
Batch 63/64 loss: 0.3144630193710327
Batch 64/64 loss: 0.3149201273918152
Epoch 3  Train loss: 0.31673474335202983  Val loss: 0.3159993817306466
Saving best model, epoch: 3
Epoch 4
-------------------------------
Batch 1/64 loss: 0.30806195735931396
Batch 2/64 loss: 0.3124192953109741
Batch 3/64 loss: 0.31361109018325806
Batch 4/64 loss: 0.3073449730873108
Batch 5/64 loss: 0.3153409957885742
Batch 6/64 loss: 0.3182828426361084
Batch 7/64 loss: 0.3179306387901306
Batch 8/64 loss: 0.3114532232284546
Batch 9/64 loss: 0.310349702835083
Batch 10/64 loss: 0.3123595714569092
Batch 11/64 loss: 0.3114888072013855
Batch 12/64 loss: 0.3102119565010071
Batch 13/64 loss: 0.31326574087142944
Batch 14/64 loss: 0.3103010654449463
Batch 15/64 loss: 0.3082723021507263
Batch 16/64 loss: 0.3077127933502197
Batch 17/64 loss: 0.31674742698669434
Batch 18/64 loss: 0.31140488386154175
Batch 19/64 loss: 0.3205423951148987
Batch 20/64 loss: 0.3152375817298889
Batch 21/64 loss: 0.3107808232307434
Batch 22/64 loss: 0.32209861278533936
Batch 23/64 loss: 0.31059473752975464
Batch 24/64 loss: 0.3112017512321472
Batch 25/64 loss: 0.3116391897201538
Batch 26/64 loss: 0.3113604187965393
Batch 27/64 loss: 0.31431931257247925
Batch 28/64 loss: 0.30873382091522217
Batch 29/64 loss: 0.3064590096473694
Batch 30/64 loss: 0.30902135372161865
Batch 31/64 loss: 0.30758416652679443
Batch 32/64 loss: 0.3131425976753235
Batch 33/64 loss: 0.3102761507034302
Batch 34/64 loss: 0.31201303005218506
Batch 35/64 loss: 0.31050384044647217
Batch 36/64 loss: 0.31484800577163696
Batch 37/64 loss: 0.31284868717193604
Batch 38/64 loss: 0.30548763275146484
Batch 39/64 loss: 0.3006550073623657
Batch 40/64 loss: 0.30696582794189453
Batch 41/64 loss: 0.3089871406555176
Batch 42/64 loss: 0.310696542263031
Batch 43/64 loss: 0.31218278408050537
Batch 44/64 loss: 0.3126431107521057
Batch 45/64 loss: 0.3104887008666992
Batch 46/64 loss: 0.3052009344100952
Batch 47/64 loss: 0.3076476454734802
Batch 48/64 loss: 0.30702197551727295
Batch 49/64 loss: 0.3050870895385742
Batch 50/64 loss: 0.3133634328842163
Batch 51/64 loss: 0.3072700500488281
Batch 52/64 loss: 0.30284881591796875
Batch 53/64 loss: 0.3174475431442261
Batch 54/64 loss: 0.3145831823348999
Batch 55/64 loss: 0.30625391006469727
Batch 56/64 loss: 0.31309497356414795
Batch 57/64 loss: 0.3042541742324829
Batch 58/64 loss: 0.3099353313446045
Batch 59/64 loss: 0.3109971284866333
Batch 60/64 loss: 0.3074423670768738
Batch 61/64 loss: 0.31337815523147583
Batch 62/64 loss: 0.31054967641830444
Batch 63/64 loss: 0.3099047541618347
Batch 64/64 loss: 0.3130931854248047
Epoch 4  Train loss: 0.3108858435761695  Val loss: 0.31916936218124076
Epoch 5
-------------------------------
Batch 1/64 loss: 0.30756425857543945
Batch 2/64 loss: 0.3086972236633301
Batch 3/64 loss: 0.3106096386909485
Batch 4/64 loss: 0.30559325218200684
Batch 5/64 loss: 0.3160529136657715
Batch 6/64 loss: 0.3100392818450928
Batch 7/64 loss: 0.30636870861053467
Batch 8/64 loss: 0.3025481700897217
Batch 9/64 loss: 0.30591630935668945
Batch 10/64 loss: 0.3044081926345825
Batch 11/64 loss: 0.31383007764816284
Batch 12/64 loss: 0.3069416284561157
Batch 13/64 loss: 0.31324470043182373
Batch 14/64 loss: 0.3037949204444885
Batch 15/64 loss: 0.30666035413742065
Batch 16/64 loss: 0.3017350435256958
Batch 17/64 loss: 0.3120706081390381
Batch 18/64 loss: 0.3038337826728821
Batch 19/64 loss: 0.3043249845504761
Batch 20/64 loss: 0.30500203371047974
Batch 21/64 loss: 0.3022468090057373
Batch 22/64 loss: 0.3087891936302185
Batch 23/64 loss: 0.29946088790893555
Batch 24/64 loss: 0.30873996019363403
Batch 25/64 loss: 0.3063572645187378
Batch 26/64 loss: 0.3056960105895996
Batch 27/64 loss: 0.30696797370910645
Batch 28/64 loss: 0.30779600143432617
Batch 29/64 loss: 0.3084590435028076
Batch 30/64 loss: 0.3037780523300171
Batch 31/64 loss: 0.3085055351257324
Batch 32/64 loss: 0.31219029426574707
Batch 33/64 loss: 0.30600059032440186
Batch 34/64 loss: 0.31311070919036865
Batch 35/64 loss: 0.29574263095855713
Batch 36/64 loss: 0.30757349729537964
Batch 37/64 loss: 0.3133419156074524
Batch 38/64 loss: 0.3048804998397827
Batch 39/64 loss: 0.3046290874481201
Batch 40/64 loss: 0.30397164821624756
Batch 41/64 loss: 0.3018834590911865
Batch 42/64 loss: 0.2963296175003052
Batch 43/64 loss: 0.3104339838027954
Batch 44/64 loss: 0.298613965511322
Batch 45/64 loss: 0.295011043548584
Batch 46/64 loss: 0.29934853315353394
Batch 47/64 loss: 0.30470097064971924
Batch 48/64 loss: 0.3076491951942444
Batch 49/64 loss: 0.2957884669303894
Batch 50/64 loss: 0.30294734239578247
Batch 51/64 loss: 0.3050888776779175
Batch 52/64 loss: 0.3077366352081299
Batch 53/64 loss: 0.302742600440979
Batch 54/64 loss: 0.2974931001663208
Batch 55/64 loss: 0.3038176894187927
Batch 56/64 loss: 0.2946016192436218
Batch 57/64 loss: 0.3007466793060303
Batch 58/64 loss: 0.3022879958152771
Batch 59/64 loss: 0.30973589420318604
Batch 60/64 loss: 0.30287158489227295
Batch 61/64 loss: 0.3047652244567871
Batch 62/64 loss: 0.3020704388618469
Batch 63/64 loss: 0.30761992931365967
Batch 64/64 loss: 0.30018556118011475
Epoch 5  Train loss: 0.3052062385222491  Val loss: 0.3079429250402549
Saving best model, epoch: 5
Epoch 6
-------------------------------
Batch 1/64 loss: 0.3005344867706299
Batch 2/64 loss: 0.3023031949996948
Batch 3/64 loss: 0.29193115234375
Batch 4/64 loss: 0.3083494305610657
Batch 5/64 loss: 0.2976340055465698
Batch 6/64 loss: 0.305736780166626
Batch 7/64 loss: 0.30017995834350586
Batch 8/64 loss: 0.2998923063278198
Batch 9/64 loss: 0.30085277557373047
Batch 10/64 loss: 0.30023789405822754
Batch 11/64 loss: 0.30756211280822754
Batch 12/64 loss: 0.3026222586631775
Batch 13/64 loss: 0.3093579411506653
Batch 14/64 loss: 0.299856960773468
Batch 15/64 loss: 0.30612754821777344
Batch 16/64 loss: 0.299269437789917
Batch 17/64 loss: 0.29786431789398193
Batch 18/64 loss: 0.29806292057037354
Batch 19/64 loss: 0.3051835298538208
Batch 20/64 loss: 0.3060567378997803
Batch 21/64 loss: 0.2982518672943115
Batch 22/64 loss: 0.3010168671607971
Batch 23/64 loss: 0.3020029664039612
Batch 24/64 loss: 0.2995588779449463
Batch 25/64 loss: 0.2940906882286072
Batch 26/64 loss: 0.3033851385116577
Batch 27/64 loss: 0.3038935661315918
Batch 28/64 loss: 0.3014122247695923
Batch 29/64 loss: 0.3044321537017822
Batch 30/64 loss: 0.3026646375656128
Batch 31/64 loss: 0.29717904329299927
Batch 32/64 loss: 0.2935127019882202
Batch 33/64 loss: 0.2947758436203003
Batch 34/64 loss: 0.29786014556884766
Batch 35/64 loss: 0.2984768748283386
Batch 36/64 loss: 0.30532461404800415
Batch 37/64 loss: 0.29936152696609497
Batch 38/64 loss: 0.2977263927459717
Batch 39/64 loss: 0.28942012786865234
Batch 40/64 loss: 0.2959328889846802
Batch 41/64 loss: 0.3058319091796875
Batch 42/64 loss: 0.30480170249938965
Batch 43/64 loss: 0.2956552505493164
Batch 44/64 loss: 0.2969024181365967
Batch 45/64 loss: 0.2963132858276367
Batch 46/64 loss: 0.3045646548271179
Batch 47/64 loss: 0.30155670642852783
Batch 48/64 loss: 0.2991359233856201
Batch 49/64 loss: 0.29497647285461426
Batch 50/64 loss: 0.30606508255004883
Batch 51/64 loss: 0.30421626567840576
Batch 52/64 loss: 0.30030226707458496
Batch 53/64 loss: 0.3002665042877197
Batch 54/64 loss: 0.29667139053344727
Batch 55/64 loss: 0.29723191261291504
Batch 56/64 loss: 0.30416011810302734
Batch 57/64 loss: 0.2996792197227478
Batch 58/64 loss: 0.2917156219482422
Batch 59/64 loss: 0.3088216185569763
Batch 60/64 loss: 0.2959992289543152
Batch 61/64 loss: 0.2996866703033447
Batch 62/64 loss: 0.28869688510894775
Batch 63/64 loss: 0.3048858046531677
Batch 64/64 loss: 0.3046744465827942
Epoch 6  Train loss: 0.30033784543766695  Val loss: 0.30362124520888445
Saving best model, epoch: 6
Epoch 7
-------------------------------
Batch 1/64 loss: 0.2939515709877014
Batch 2/64 loss: 0.3020930290222168
Batch 3/64 loss: 0.3031191825866699
Batch 4/64 loss: 0.2973160147666931
Batch 5/64 loss: 0.29890716075897217
Batch 6/64 loss: 0.2993037700653076
Batch 7/64 loss: 0.3035609722137451
Batch 8/64 loss: 0.2978031635284424
Batch 9/64 loss: 0.2987782955169678
Batch 10/64 loss: 0.3005969524383545
Batch 11/64 loss: 0.30003821849823
Batch 12/64 loss: 0.3033123016357422
Batch 13/64 loss: 0.2886878252029419
Batch 14/64 loss: 0.2925349473953247
Batch 15/64 loss: 0.2968102693557739
Batch 16/64 loss: 0.2978367209434509
Batch 17/64 loss: 0.302277147769928
Batch 18/64 loss: 0.30475640296936035
Batch 19/64 loss: 0.2966090440750122
Batch 20/64 loss: 0.2984861135482788
Batch 21/64 loss: 0.29423511028289795
Batch 22/64 loss: 0.30944615602493286
Batch 23/64 loss: 0.2974916696548462
Batch 24/64 loss: 0.2964608669281006
Batch 25/64 loss: 0.29988813400268555
Batch 26/64 loss: 0.29589301347732544
Batch 27/64 loss: 0.29634952545166016
Batch 28/64 loss: 0.2949918508529663
Batch 29/64 loss: 0.296434223651886
Batch 30/64 loss: 0.2967694401741028
Batch 31/64 loss: 0.2971813678741455
Batch 32/64 loss: 0.30008864402770996
Batch 33/64 loss: 0.2972102165222168
Batch 34/64 loss: 0.29657554626464844
Batch 35/64 loss: 0.2986908555030823
Batch 36/64 loss: 0.29390668869018555
Batch 37/64 loss: 0.29531556367874146
Batch 38/64 loss: 0.3011963367462158
Batch 39/64 loss: 0.3063102960586548
Batch 40/64 loss: 0.3002772927284241
Batch 41/64 loss: 0.2990506887435913
Batch 42/64 loss: 0.30084407329559326
Batch 43/64 loss: 0.2960319519042969
Batch 44/64 loss: 0.3036770224571228
Batch 45/64 loss: 0.2946770191192627
Batch 46/64 loss: 0.29919540882110596
Batch 47/64 loss: 0.2960394620895386
Batch 48/64 loss: 0.29362642765045166
Batch 49/64 loss: 0.29369300603866577
Batch 50/64 loss: 0.2970429062843323
Batch 51/64 loss: 0.28896796703338623
Batch 52/64 loss: 0.29011213779449463
Batch 53/64 loss: 0.29656267166137695
Batch 54/64 loss: 0.2913898229598999
Batch 55/64 loss: 0.29854440689086914
Batch 56/64 loss: 0.3051043152809143
Batch 57/64 loss: 0.29435545206069946
Batch 58/64 loss: 0.2900465130805969
Batch 59/64 loss: 0.3034600019454956
Batch 60/64 loss: 0.2988426685333252
Batch 61/64 loss: 0.2993890047073364
Batch 62/64 loss: 0.29609936475753784
Batch 63/64 loss: 0.30730485916137695
Batch 64/64 loss: 0.29209351539611816
Epoch 7  Train loss: 0.29795481083439845  Val loss: 0.3026353563230062
Saving best model, epoch: 7
Epoch 8
-------------------------------
Batch 1/64 loss: 0.29812490940093994
Batch 2/64 loss: 0.29852890968322754
Batch 3/64 loss: 0.29717153310775757
Batch 4/64 loss: 0.29575884342193604
Batch 5/64 loss: 0.30164003372192383
Batch 6/64 loss: 0.29968786239624023
Batch 7/64 loss: 0.29429447650909424
Batch 8/64 loss: 0.2984483242034912
Batch 9/64 loss: 0.29326897859573364
Batch 10/64 loss: 0.2923056483268738
Batch 11/64 loss: 0.2911178469657898
Batch 12/64 loss: 0.29652321338653564
Batch 13/64 loss: 0.2959171533584595
Batch 14/64 loss: 0.2898147702217102
Batch 15/64 loss: 0.2970712184906006
Batch 16/64 loss: 0.29364532232284546
Batch 17/64 loss: 0.28611916303634644
Batch 18/64 loss: 0.29356658458709717
Batch 19/64 loss: 0.29608362913131714
Batch 20/64 loss: 0.2912612557411194
Batch 21/64 loss: 0.29354310035705566
Batch 22/64 loss: 0.2906292676925659
Batch 23/64 loss: 0.29945385456085205
Batch 24/64 loss: 0.2957277297973633
Batch 25/64 loss: 0.2887735366821289
Batch 26/64 loss: 0.2906026840209961
Batch 27/64 loss: 0.2948263883590698
Batch 28/64 loss: 0.29173243045806885
Batch 29/64 loss: 0.2952619194984436
Batch 30/64 loss: 0.29069119691848755
Batch 31/64 loss: 0.2898293733596802
Batch 32/64 loss: 0.2975759506225586
Batch 33/64 loss: 0.29403156042099
Batch 34/64 loss: 0.29602086544036865
Batch 35/64 loss: 0.29977142810821533
Batch 36/64 loss: 0.29657459259033203
Batch 37/64 loss: 0.2941746711730957
Batch 38/64 loss: 0.296758234500885
Batch 39/64 loss: 0.29773640632629395
Batch 40/64 loss: 0.298220157623291
Batch 41/64 loss: 0.2921406030654907
Batch 42/64 loss: 0.28978604078292847
Batch 43/64 loss: 0.2981422543525696
Batch 44/64 loss: 0.29200035333633423
Batch 45/64 loss: 0.2959710955619812
Batch 46/64 loss: 0.2969282269477844
Batch 47/64 loss: 0.2954065799713135
Batch 48/64 loss: 0.29663658142089844
Batch 49/64 loss: 0.287040650844574
Batch 50/64 loss: 0.2864187955856323
Batch 51/64 loss: 0.2939523458480835
Batch 52/64 loss: 0.2917633056640625
Batch 53/64 loss: 0.2894477844238281
Batch 54/64 loss: 0.295629620552063
Batch 55/64 loss: 0.2983294725418091
Batch 56/64 loss: 0.2959655523300171
Batch 57/64 loss: 0.2919734716415405
Batch 58/64 loss: 0.29727792739868164
Batch 59/64 loss: 0.28956329822540283
Batch 60/64 loss: 0.2942589521408081
Batch 61/64 loss: 0.2963123321533203
Batch 62/64 loss: 0.29956960678100586
Batch 63/64 loss: 0.30046021938323975
Batch 64/64 loss: 0.28669875860214233
Epoch 8  Train loss: 0.29446720258862363  Val loss: 0.30031327427048043
Saving best model, epoch: 8
Epoch 9
-------------------------------
Batch 1/64 loss: 0.2908233404159546
Batch 2/64 loss: 0.29771363735198975
Batch 3/64 loss: 0.290188729763031
Batch 4/64 loss: 0.3081529140472412
Batch 5/64 loss: 0.2880397439002991
Batch 6/64 loss: 0.29418087005615234
Batch 7/64 loss: 0.2984035015106201
Batch 8/64 loss: 0.28801947832107544
Batch 9/64 loss: 0.2864115834236145
Batch 10/64 loss: 0.2874637246131897
Batch 11/64 loss: 0.29797905683517456
Batch 12/64 loss: 0.3001335859298706
Batch 13/64 loss: 0.29153335094451904
Batch 14/64 loss: 0.2916374206542969
Batch 15/64 loss: 0.2852323055267334
Batch 16/64 loss: 0.2959410548210144
Batch 17/64 loss: 0.28706657886505127
Batch 18/64 loss: 0.29146385192871094
Batch 19/64 loss: 0.29406094551086426
Batch 20/64 loss: 0.2854654788970947
Batch 21/64 loss: 0.3072044253349304
Batch 22/64 loss: 0.291695237159729
Batch 23/64 loss: 0.29587680101394653
Batch 24/64 loss: 0.2913913130760193
Batch 25/64 loss: 0.2863565683364868
Batch 26/64 loss: 0.2937694191932678
Batch 27/64 loss: 0.29058319330215454
Batch 28/64 loss: 0.2953033447265625
Batch 29/64 loss: 0.2854194641113281
Batch 30/64 loss: 0.28689253330230713
Batch 31/64 loss: 0.29093778133392334
Batch 32/64 loss: 0.29289352893829346
Batch 33/64 loss: 0.2898947596549988
Batch 34/64 loss: 0.29476404190063477
Batch 35/64 loss: 0.2865811586380005
Batch 36/64 loss: 0.29127395153045654
Batch 37/64 loss: 0.28520750999450684
Batch 38/64 loss: 0.2966376543045044
Batch 39/64 loss: 0.2843606472015381
Batch 40/64 loss: 0.2940589189529419
Batch 41/64 loss: 0.29205024242401123
Batch 42/64 loss: 0.29238760471343994
Batch 43/64 loss: 0.29401010274887085
Batch 44/64 loss: 0.2954255938529968
Batch 45/64 loss: 0.29774922132492065
Batch 46/64 loss: 0.29876774549484253
Batch 47/64 loss: 0.2936384677886963
Batch 48/64 loss: 0.2905197739601135
Batch 49/64 loss: 0.29236358404159546
Batch 50/64 loss: 0.28873956203460693
Batch 51/64 loss: 0.2852810025215149
Batch 52/64 loss: 0.2941017746925354
Batch 53/64 loss: 0.2925231456756592
Batch 54/64 loss: 0.2969393730163574
Batch 55/64 loss: 0.295491099357605
Batch 56/64 loss: 0.293270468711853
Batch 57/64 loss: 0.2922782897949219
Batch 58/64 loss: 0.2847675681114197
Batch 59/64 loss: 0.2886585593223572
Batch 60/64 loss: 0.29587066173553467
Batch 61/64 loss: 0.2926253080368042
Batch 62/64 loss: 0.29448646306991577
Batch 63/64 loss: 0.29169023036956787
Batch 64/64 loss: 0.30017197132110596
Epoch 9  Train loss: 0.29238867806453334  Val loss: 0.29904318572729316
Saving best model, epoch: 9
Epoch 10
-------------------------------
Batch 1/64 loss: 0.2939859628677368
Batch 2/64 loss: 0.29098308086395264
Batch 3/64 loss: 0.29279303550720215
Batch 4/64 loss: 0.29111671447753906
Batch 5/64 loss: 0.286273717880249
Batch 6/64 loss: 0.29334813356399536
Batch 7/64 loss: 0.29229944944381714
Batch 8/64 loss: 0.2921995520591736
Batch 9/64 loss: 0.29294973611831665
Batch 10/64 loss: 0.2924962639808655
Batch 11/64 loss: 0.2856940031051636
Batch 12/64 loss: 0.2919718027114868
Batch 13/64 loss: 0.2887827754020691
Batch 14/64 loss: 0.28932273387908936
Batch 15/64 loss: 0.2902611494064331
Batch 16/64 loss: 0.29394590854644775
Batch 17/64 loss: 0.2868993878364563
Batch 18/64 loss: 0.29473209381103516
Batch 19/64 loss: 0.29176104068756104
Batch 20/64 loss: 0.294661283493042
Batch 21/64 loss: 0.2932608127593994
Batch 22/64 loss: 0.29477351903915405
Batch 23/64 loss: 0.29576683044433594
Batch 24/64 loss: 0.2852453589439392
Batch 25/64 loss: 0.2881886959075928
Batch 26/64 loss: 0.2866588830947876
Batch 27/64 loss: 0.29000043869018555
Batch 28/64 loss: 0.28790807723999023
Batch 29/64 loss: 0.2990950345993042
Batch 30/64 loss: 0.2934989929199219
Batch 31/64 loss: 0.29261499643325806
Batch 32/64 loss: 0.298686146736145
Batch 33/64 loss: 0.28818124532699585
Batch 34/64 loss: 0.2873414158821106
Batch 35/64 loss: 0.28659701347351074
Batch 36/64 loss: 0.2906494736671448
Batch 37/64 loss: 0.2934727072715759
Batch 38/64 loss: 0.2958231568336487
Batch 39/64 loss: 0.28275036811828613
Batch 40/64 loss: 0.2803348898887634
Batch 41/64 loss: 0.29527437686920166
Batch 42/64 loss: 0.2864149808883667
Batch 43/64 loss: 0.2916429042816162
Batch 44/64 loss: 0.28600871562957764
Batch 45/64 loss: 0.2917933464050293
Batch 46/64 loss: 0.2894507646560669
Batch 47/64 loss: 0.2901233434677124
Batch 48/64 loss: 0.29036420583724976
Batch 49/64 loss: 0.29547345638275146
Batch 50/64 loss: 0.2964303493499756
Batch 51/64 loss: 0.29183584451675415
Batch 52/64 loss: 0.2933386564254761
Batch 53/64 loss: 0.2911338806152344
Batch 54/64 loss: 0.2890405058860779
Batch 55/64 loss: 0.29253441095352173
Batch 56/64 loss: 0.2955853343009949
Batch 57/64 loss: 0.2965264320373535
Batch 58/64 loss: 0.2907187342643738
Batch 59/64 loss: 0.29252439737319946
Batch 60/64 loss: 0.29264140129089355
Batch 61/64 loss: 0.29879748821258545
Batch 62/64 loss: 0.29719972610473633
Batch 63/64 loss: 0.2922879457473755
Batch 64/64 loss: 0.2860404849052429
Epoch 10  Train loss: 0.2914352540876351  Val loss: 0.2957390882714917
Saving best model, epoch: 10
Epoch 11
-------------------------------
Batch 1/64 loss: 0.2871229648590088
Batch 2/64 loss: 0.2922145128250122
Batch 3/64 loss: 0.28759098052978516
Batch 4/64 loss: 0.28800106048583984
Batch 5/64 loss: 0.28384584188461304
Batch 6/64 loss: 0.2896852493286133
Batch 7/64 loss: 0.2892383337020874
Batch 8/64 loss: 0.28463178873062134
Batch 9/64 loss: 0.28214967250823975
Batch 10/64 loss: 0.2938716411590576
Batch 11/64 loss: 0.28570008277893066
Batch 12/64 loss: 0.28655552864074707
Batch 13/64 loss: 0.28795814514160156
Batch 14/64 loss: 0.28698545694351196
Batch 15/64 loss: 0.29701483249664307
Batch 16/64 loss: 0.2910378575325012
Batch 17/64 loss: 0.2945380210876465
Batch 18/64 loss: 0.2868993878364563
Batch 19/64 loss: 0.2813515067100525
Batch 20/64 loss: 0.2851285934448242
Batch 21/64 loss: 0.29411041736602783
Batch 22/64 loss: 0.2834463119506836
Batch 23/64 loss: 0.2878497838973999
Batch 24/64 loss: 0.28885549306869507
Batch 25/64 loss: 0.2921394109725952
Batch 26/64 loss: 0.2906847596168518
Batch 27/64 loss: 0.29461753368377686
Batch 28/64 loss: 0.28868377208709717
Batch 29/64 loss: 0.28879034519195557
Batch 30/64 loss: 0.29450786113739014
Batch 31/64 loss: 0.28959548473358154
Batch 32/64 loss: 0.2967071533203125
Batch 33/64 loss: 0.2933063507080078
Batch 34/64 loss: 0.2857673168182373
Batch 35/64 loss: 0.29479026794433594
Batch 36/64 loss: 0.29110413789749146
Batch 37/64 loss: 0.29234278202056885
Batch 38/64 loss: 0.28640294075012207
Batch 39/64 loss: 0.2891479730606079
Batch 40/64 loss: 0.286907434463501
Batch 41/64 loss: 0.2875692844390869
Batch 42/64 loss: 0.29478752613067627
Batch 43/64 loss: 0.28952890634536743
Batch 44/64 loss: 0.2882879972457886
Batch 45/64 loss: 0.29201817512512207
Batch 46/64 loss: 0.28781378269195557
Batch 47/64 loss: 0.2886338233947754
Batch 48/64 loss: 0.28882265090942383
Batch 49/64 loss: 0.2901456356048584
Batch 50/64 loss: 0.28386902809143066
Batch 51/64 loss: 0.2883232831954956
Batch 52/64 loss: 0.2825357913970947
Batch 53/64 loss: 0.28406453132629395
Batch 54/64 loss: 0.28775572776794434
Batch 55/64 loss: 0.29145240783691406
Batch 56/64 loss: 0.28832972049713135
Batch 57/64 loss: 0.2767239809036255
Batch 58/64 loss: 0.291534960269928
Batch 59/64 loss: 0.2873445749282837
Batch 60/64 loss: 0.30085206031799316
Batch 61/64 loss: 0.28683942556381226
Batch 62/64 loss: 0.29306310415267944
Batch 63/64 loss: 0.2808501124382019
Batch 64/64 loss: 0.2924305200576782
Epoch 11  Train loss: 0.28896860188128903  Val loss: 0.29259059109638647
Saving best model, epoch: 11
Epoch 12
-------------------------------
Batch 1/64 loss: 0.2910771369934082
Batch 2/64 loss: 0.286246120929718
Batch 3/64 loss: 0.28834211826324463
Batch 4/64 loss: 0.2963278293609619
Batch 5/64 loss: 0.2932577133178711
Batch 6/64 loss: 0.2871685028076172
Batch 7/64 loss: 0.2864058017730713
Batch 8/64 loss: 0.29173219203948975
Batch 9/64 loss: 0.283205509185791
Batch 10/64 loss: 0.2877628803253174
Batch 11/64 loss: 0.2955729365348816
Batch 12/64 loss: 0.2938477396965027
Batch 13/64 loss: 0.28506046533584595
Batch 14/64 loss: 0.2896641492843628
Batch 15/64 loss: 0.29037320613861084
Batch 16/64 loss: 0.2841297388076782
Batch 17/64 loss: 0.2817554473876953
Batch 18/64 loss: 0.2827379107475281
Batch 19/64 loss: 0.28825843334198
Batch 20/64 loss: 0.2832130789756775
Batch 21/64 loss: 0.2872300148010254
Batch 22/64 loss: 0.28832972049713135
Batch 23/64 loss: 0.2740345597267151
Batch 24/64 loss: 0.2822062373161316
Batch 25/64 loss: 0.28609687089920044
Batch 26/64 loss: 0.2838227152824402
Batch 27/64 loss: 0.2813277244567871
Batch 28/64 loss: 0.2877410650253296
Batch 29/64 loss: 0.2883951663970947
Batch 30/64 loss: 0.2931005358695984
Batch 31/64 loss: 0.28891420364379883
Batch 32/64 loss: 0.2910200357437134
Batch 33/64 loss: 0.29054856300354004
Batch 34/64 loss: 0.2843703031539917
Batch 35/64 loss: 0.286842942237854
Batch 36/64 loss: 0.2882571816444397
Batch 37/64 loss: 0.2902568578720093
Batch 38/64 loss: 0.29126226902008057
Batch 39/64 loss: 0.283117413520813
Batch 40/64 loss: 0.29565465450286865
Batch 41/64 loss: 0.2897087335586548
Batch 42/64 loss: 0.30217409133911133
Batch 43/64 loss: 0.28898072242736816
Batch 44/64 loss: 0.286342978477478
Batch 45/64 loss: 0.2855837345123291
Batch 46/64 loss: 0.2880491018295288
Batch 47/64 loss: 0.2841525673866272
Batch 48/64 loss: 0.2923485040664673
Batch 49/64 loss: 0.2885475158691406
Batch 50/64 loss: 0.28349941968917847
Batch 51/64 loss: 0.2881983518600464
Batch 52/64 loss: 0.28908634185791016
Batch 53/64 loss: 0.2821028232574463
Batch 54/64 loss: 0.2917635440826416
Batch 55/64 loss: 0.28098464012145996
Batch 56/64 loss: 0.28813767433166504
Batch 57/64 loss: 0.2925258278846741
Batch 58/64 loss: 0.27910590171813965
Batch 59/64 loss: 0.2827053666114807
Batch 60/64 loss: 0.28209561109542847
Batch 61/64 loss: 0.28525590896606445
Batch 62/64 loss: 0.2766430377960205
Batch 63/64 loss: 0.2853524088859558
Batch 64/64 loss: 0.2851344347000122
Epoch 12  Train loss: 0.2873076639923395  Val loss: 0.2908232761002898
Saving best model, epoch: 12
Epoch 13
-------------------------------
Batch 1/64 loss: 0.2886534333229065
Batch 2/64 loss: 0.2885094881057739
Batch 3/64 loss: 0.28581011295318604
Batch 4/64 loss: 0.27661454677581787
Batch 5/64 loss: 0.294705867767334
Batch 6/64 loss: 0.2884722352027893
Batch 7/64 loss: 0.2879296541213989
Batch 8/64 loss: 0.2844523787498474
Batch 9/64 loss: 0.2791968584060669
Batch 10/64 loss: 0.2772228717803955
Batch 11/64 loss: 0.28072595596313477
Batch 12/64 loss: 0.27435797452926636
Batch 13/64 loss: 0.27357614040374756
Batch 14/64 loss: 0.2805821895599365
Batch 15/64 loss: 0.2822273373603821
Batch 16/64 loss: 0.2889973521232605
Batch 17/64 loss: 0.282004714012146
Batch 18/64 loss: 0.2876476049423218
Batch 19/64 loss: 0.27855348587036133
Batch 20/64 loss: 0.27855849266052246
Batch 21/64 loss: 0.2920324206352234
Batch 22/64 loss: 0.2858748435974121
Batch 23/64 loss: 0.2834603786468506
Batch 24/64 loss: 0.28181636333465576
Batch 25/64 loss: 0.2818775177001953
Batch 26/64 loss: 0.28827404975891113
Batch 27/64 loss: 0.2862563729286194
Batch 28/64 loss: 0.28595852851867676
Batch 29/64 loss: 0.2903534173965454
Batch 30/64 loss: 0.27832210063934326
Batch 31/64 loss: 0.29375237226486206
Batch 32/64 loss: 0.28348565101623535
Batch 33/64 loss: 0.28608477115631104
Batch 34/64 loss: 0.2873561382293701
Batch 35/64 loss: 0.2920483350753784
Batch 36/64 loss: 0.2870749235153198
Batch 37/64 loss: 0.29097121953964233
Batch 38/64 loss: 0.2891814112663269
Batch 39/64 loss: 0.28067153692245483
Batch 40/64 loss: 0.29605960845947266
Batch 41/64 loss: 0.2849158048629761
Batch 42/64 loss: 0.28806406259536743
Batch 43/64 loss: 0.2802298069000244
Batch 44/64 loss: 0.283550500869751
Batch 45/64 loss: 0.2834123373031616
Batch 46/64 loss: 0.2925853729248047
Batch 47/64 loss: 0.29594969749450684
Batch 48/64 loss: 0.2843722701072693
Batch 49/64 loss: 0.2897269129753113
Batch 50/64 loss: 0.2779136896133423
Batch 51/64 loss: 0.279415488243103
Batch 52/64 loss: 0.28511637449264526
Batch 53/64 loss: 0.28468334674835205
Batch 54/64 loss: 0.283072292804718
Batch 55/64 loss: 0.28585702180862427
Batch 56/64 loss: 0.2838752269744873
Batch 57/64 loss: 0.28739500045776367
Batch 58/64 loss: 0.2879129648208618
Batch 59/64 loss: 0.2909134030342102
Batch 60/64 loss: 0.2882499694824219
Batch 61/64 loss: 0.2882019281387329
Batch 62/64 loss: 0.2855032682418823
Batch 63/64 loss: 0.284356951713562
Batch 64/64 loss: 0.27746570110321045
Epoch 13  Train loss: 0.28522484115525787  Val loss: 0.2907618288322003
Saving best model, epoch: 13
Epoch 14
-------------------------------
Batch 1/64 loss: 0.28201472759246826
Batch 2/64 loss: 0.27739447355270386
Batch 3/64 loss: 0.28192365169525146
Batch 4/64 loss: 0.27394402027130127
Batch 5/64 loss: 0.28422439098358154
Batch 6/64 loss: 0.28819096088409424
Batch 7/64 loss: 0.2777292728424072
Batch 8/64 loss: 0.2826569080352783
Batch 9/64 loss: 0.28713029623031616
Batch 10/64 loss: 0.2861900329589844
Batch 11/64 loss: 0.27670079469680786
Batch 12/64 loss: 0.2809346914291382
Batch 13/64 loss: 0.2884366512298584
Batch 14/64 loss: 0.27858543395996094
Batch 15/64 loss: 0.2780386209487915
Batch 16/64 loss: 0.2846311330795288
Batch 17/64 loss: 0.27831244468688965
Batch 18/64 loss: 0.27724432945251465
Batch 19/64 loss: 0.28249824047088623
Batch 20/64 loss: 0.29200923442840576
Batch 21/64 loss: 0.28601694107055664
Batch 22/64 loss: 0.2819012403488159
Batch 23/64 loss: 0.28295159339904785
Batch 24/64 loss: 0.2845625877380371
Batch 25/64 loss: 0.27938830852508545
Batch 26/64 loss: 0.2921649217605591
Batch 27/64 loss: 0.2864946126937866
Batch 28/64 loss: 0.2855032682418823
Batch 29/64 loss: 0.2763117551803589
Batch 30/64 loss: 0.28716492652893066
Batch 31/64 loss: 0.27696382999420166
Batch 32/64 loss: 0.27991044521331787
Batch 33/64 loss: 0.286674439907074
Batch 34/64 loss: 0.2826836109161377
Batch 35/64 loss: 0.2859705090522766
Batch 36/64 loss: 0.2879648804664612
Batch 37/64 loss: 0.27618157863616943
Batch 38/64 loss: 0.2841596007347107
Batch 39/64 loss: 0.2847883701324463
Batch 40/64 loss: 0.2836548686027527
Batch 41/64 loss: 0.28894108533859253
Batch 42/64 loss: 0.28475284576416016
Batch 43/64 loss: 0.2852171063423157
Batch 44/64 loss: 0.2918773889541626
Batch 45/64 loss: 0.2884138822555542
Batch 46/64 loss: 0.2808986306190491
Batch 47/64 loss: 0.2803736925125122
Batch 48/64 loss: 0.2827101945877075
Batch 49/64 loss: 0.276900053024292
Batch 50/64 loss: 0.2859213352203369
Batch 51/64 loss: 0.28459692001342773
Batch 52/64 loss: 0.28295207023620605
Batch 53/64 loss: 0.28625965118408203
Batch 54/64 loss: 0.2782976031303406
Batch 55/64 loss: 0.28488898277282715
Batch 56/64 loss: 0.27934807538986206
Batch 57/64 loss: 0.2847462296485901
Batch 58/64 loss: 0.28950726985931396
Batch 59/64 loss: 0.286098837852478
Batch 60/64 loss: 0.283699631690979
Batch 61/64 loss: 0.29433882236480713
Batch 62/64 loss: 0.28770893812179565
Batch 63/64 loss: 0.2746938467025757
Batch 64/64 loss: 0.2876015901565552
Epoch 14  Train loss: 0.28343602956510056  Val loss: 0.2895594499365161
Saving best model, epoch: 14
Epoch 15
-------------------------------
Batch 1/64 loss: 0.2822837233543396
Batch 2/64 loss: 0.2835719585418701
Batch 3/64 loss: 0.28462356328964233
Batch 4/64 loss: 0.28726303577423096
Batch 5/64 loss: 0.2850639820098877
Batch 6/64 loss: 0.282370924949646
Batch 7/64 loss: 0.28092342615127563
Batch 8/64 loss: 0.27991485595703125
Batch 9/64 loss: 0.2864595651626587
Batch 10/64 loss: 0.2848668694496155
Batch 11/64 loss: 0.28426551818847656
Batch 12/64 loss: 0.2836076021194458
Batch 13/64 loss: 0.28232336044311523
Batch 14/64 loss: 0.28050124645233154
Batch 15/64 loss: 0.2812092900276184
Batch 16/64 loss: 0.28948795795440674
Batch 17/64 loss: 0.28413814306259155
Batch 18/64 loss: 0.2833021879196167
Batch 19/64 loss: 0.2805325984954834
Batch 20/64 loss: 0.28031647205352783
Batch 21/64 loss: 0.2848653793334961
Batch 22/64 loss: 0.28627365827560425
Batch 23/64 loss: 0.28398776054382324
Batch 24/64 loss: 0.27273422479629517
Batch 25/64 loss: 0.2803306579589844
Batch 26/64 loss: 0.2706755995750427
Batch 27/64 loss: 0.28459465503692627
Batch 28/64 loss: 0.27376359701156616
Batch 29/64 loss: 0.2907925844192505
Batch 30/64 loss: 0.28261756896972656
Batch 31/64 loss: 0.28617197275161743
Batch 32/64 loss: 0.28279268741607666
Batch 33/64 loss: 0.2885926365852356
Batch 34/64 loss: 0.27921241521835327
Batch 35/64 loss: 0.2874469757080078
Batch 36/64 loss: 0.2822548747062683
Batch 37/64 loss: 0.27784693241119385
Batch 38/64 loss: 0.28249257802963257
Batch 39/64 loss: 0.28240203857421875
Batch 40/64 loss: 0.2777690887451172
Batch 41/64 loss: 0.28322911262512207
Batch 42/64 loss: 0.282220721244812
Batch 43/64 loss: 0.2760903835296631
Batch 44/64 loss: 0.28330880403518677
Batch 45/64 loss: 0.2838541865348816
Batch 46/64 loss: 0.2760157585144043
Batch 47/64 loss: 0.2888585329055786
Batch 48/64 loss: 0.2839377522468567
Batch 49/64 loss: 0.2821216583251953
Batch 50/64 loss: 0.27231675386428833
Batch 51/64 loss: 0.27322709560394287
Batch 52/64 loss: 0.28839385509490967
Batch 53/64 loss: 0.2826961874961853
Batch 54/64 loss: 0.2760409712791443
Batch 55/64 loss: 0.2790542244911194
Batch 56/64 loss: 0.27490127086639404
Batch 57/64 loss: 0.278083860874176
Batch 58/64 loss: 0.28346192836761475
Batch 59/64 loss: 0.2850889563560486
Batch 60/64 loss: 0.2854219675064087
Batch 61/64 loss: 0.27871787548065186
Batch 62/64 loss: 0.28746479749679565
Batch 63/64 loss: 0.2816774845123291
Batch 64/64 loss: 0.28533512353897095
Epoch 15  Train loss: 0.2821149748914382  Val loss: 0.28751729321234004
Saving best model, epoch: 15
Epoch 16
-------------------------------
Batch 1/64 loss: 0.2824183702468872
Batch 2/64 loss: 0.2836341857910156
Batch 3/64 loss: 0.28077852725982666
Batch 4/64 loss: 0.2751309871673584
Batch 5/64 loss: 0.27358853816986084
Batch 6/64 loss: 0.27556121349334717
Batch 7/64 loss: 0.28928983211517334
Batch 8/64 loss: 0.28889501094818115
Batch 9/64 loss: 0.2826106548309326
Batch 10/64 loss: 0.27517032623291016
Batch 11/64 loss: 0.28155195713043213
Batch 12/64 loss: 0.27657198905944824
Batch 13/64 loss: 0.283805251121521
Batch 14/64 loss: 0.2818406820297241
Batch 15/64 loss: 0.2800896167755127
Batch 16/64 loss: 0.28275030851364136
Batch 17/64 loss: 0.27226442098617554
Batch 18/64 loss: 0.28042250871658325
Batch 19/64 loss: 0.281915545463562
Batch 20/64 loss: 0.2889147996902466
Batch 21/64 loss: 0.28767865896224976
Batch 22/64 loss: 0.28558290004730225
Batch 23/64 loss: 0.2844124436378479
Batch 24/64 loss: 0.27550357580184937
Batch 25/64 loss: 0.28273916244506836
Batch 26/64 loss: 0.27708661556243896
Batch 27/64 loss: 0.28170228004455566
Batch 28/64 loss: 0.28519701957702637
Batch 29/64 loss: 0.2717951536178589
Batch 30/64 loss: 0.28088998794555664
Batch 31/64 loss: 0.2870889902114868
Batch 32/64 loss: 0.291229248046875
Batch 33/64 loss: 0.27856916189193726
Batch 34/64 loss: 0.270393431186676
Batch 35/64 loss: 0.27676838636398315
Batch 36/64 loss: 0.2782944440841675
Batch 37/64 loss: 0.2760869264602661
Batch 38/64 loss: 0.2757357954978943
Batch 39/64 loss: 0.27119410037994385
Batch 40/64 loss: 0.2824738025665283
Batch 41/64 loss: 0.27613937854766846
Batch 42/64 loss: 0.2871462106704712
Batch 43/64 loss: 0.2742486596107483
Batch 44/64 loss: 0.28184670209884644
Batch 45/64 loss: 0.27802330255508423
Batch 46/64 loss: 0.2875590920448303
Batch 47/64 loss: 0.2764195203781128
Batch 48/64 loss: 0.2791006565093994
Batch 49/64 loss: 0.2760165333747864
Batch 50/64 loss: 0.2863841652870178
Batch 51/64 loss: 0.2837584614753723
Batch 52/64 loss: 0.28419119119644165
Batch 53/64 loss: 0.2801460027694702
Batch 54/64 loss: 0.2847864031791687
Batch 55/64 loss: 0.28381508588790894
Batch 56/64 loss: 0.27619361877441406
Batch 57/64 loss: 0.28067445755004883
Batch 58/64 loss: 0.2791111469268799
Batch 59/64 loss: 0.278531551361084
Batch 60/64 loss: 0.2814213037490845
Batch 61/64 loss: 0.2762609124183655
Batch 62/64 loss: 0.2799481153488159
Batch 63/64 loss: 0.28343665599823
Batch 64/64 loss: 0.26713353395462036
Epoch 16  Train loss: 0.280362919031405  Val loss: 0.287078652799744
Saving best model, epoch: 16
Epoch 17
-------------------------------
Batch 1/64 loss: 0.28293871879577637
Batch 2/64 loss: 0.2832305431365967
Batch 3/64 loss: 0.2855681777000427
Batch 4/64 loss: 0.2774255871772766
Batch 5/64 loss: 0.2850198745727539
Batch 6/64 loss: 0.27231037616729736
Batch 7/64 loss: 0.2857325077056885
Batch 8/64 loss: 0.2773759365081787
Batch 9/64 loss: 0.2735929489135742
Batch 10/64 loss: 0.28098416328430176
Batch 11/64 loss: 0.2850595712661743
Batch 12/64 loss: 0.27447324991226196
Batch 13/64 loss: 0.27563488483428955
Batch 14/64 loss: 0.2793468236923218
Batch 15/64 loss: 0.27546942234039307
Batch 16/64 loss: 0.28404104709625244
Batch 17/64 loss: 0.28398799896240234
Batch 18/64 loss: 0.28173828125
Batch 19/64 loss: 0.27407968044281006
Batch 20/64 loss: 0.2795780897140503
Batch 21/64 loss: 0.2815011739730835
Batch 22/64 loss: 0.27023959159851074
Batch 23/64 loss: 0.2815145254135132
Batch 24/64 loss: 0.28310084342956543
Batch 25/64 loss: 0.2763638496398926
Batch 26/64 loss: 0.28618842363357544
Batch 27/64 loss: 0.2798210382461548
Batch 28/64 loss: 0.2818949222564697
Batch 29/64 loss: 0.2812650203704834
Batch 30/64 loss: 0.28560924530029297
Batch 31/64 loss: 0.2775827646255493
Batch 32/64 loss: 0.2858728766441345
Batch 33/64 loss: 0.28049808740615845
Batch 34/64 loss: 0.2936830520629883
Batch 35/64 loss: 0.2795149087905884
Batch 36/64 loss: 0.2817307114601135
Batch 37/64 loss: 0.2823615074157715
Batch 38/64 loss: 0.27938759326934814
Batch 39/64 loss: 0.27892065048217773
Batch 40/64 loss: 0.2759897708892822
Batch 41/64 loss: 0.27678847312927246
Batch 42/64 loss: 0.28250157833099365
Batch 43/64 loss: 0.26957958936691284
Batch 44/64 loss: 0.27652883529663086
Batch 45/64 loss: 0.2820819616317749
Batch 46/64 loss: 0.2692972421646118
Batch 47/64 loss: 0.27871429920196533
Batch 48/64 loss: 0.2834409475326538
Batch 49/64 loss: 0.2747315764427185
Batch 50/64 loss: 0.2680675983428955
Batch 51/64 loss: 0.2739080786705017
Batch 52/64 loss: 0.2772068381309509
Batch 53/64 loss: 0.28257179260253906
Batch 54/64 loss: 0.2881662845611572
Batch 55/64 loss: 0.2839087247848511
Batch 56/64 loss: 0.2839348316192627
Batch 57/64 loss: 0.2813684940338135
Batch 58/64 loss: 0.27603524923324585
Batch 59/64 loss: 0.27128636837005615
Batch 60/64 loss: 0.282955527305603
Batch 61/64 loss: 0.2780948877334595
Batch 62/64 loss: 0.2717435956001282
Batch 63/64 loss: 0.2838451862335205
Batch 64/64 loss: 0.271705687046051
Epoch 17  Train loss: 0.27954769672132007  Val loss: 0.28552322166482197
Saving best model, epoch: 17
Epoch 18
-------------------------------
Batch 1/64 loss: 0.28305160999298096
Batch 2/64 loss: 0.2697368860244751
Batch 3/64 loss: 0.27579623460769653
Batch 4/64 loss: 0.2755866050720215
Batch 5/64 loss: 0.2839646339416504
Batch 6/64 loss: 0.28727900981903076
Batch 7/64 loss: 0.27477598190307617
Batch 8/64 loss: 0.28522300720214844
Batch 9/64 loss: 0.2743147015571594
Batch 10/64 loss: 0.27570950984954834
Batch 11/64 loss: 0.2820169925689697
Batch 12/64 loss: 0.27859461307525635
Batch 13/64 loss: 0.2714059352874756
Batch 14/64 loss: 0.2818061113357544
Batch 15/64 loss: 0.28330177068710327
Batch 16/64 loss: 0.27937638759613037
Batch 17/64 loss: 0.2719852924346924
Batch 18/64 loss: 0.27833855152130127
Batch 19/64 loss: 0.27826327085494995
Batch 20/64 loss: 0.2734355926513672
Batch 21/64 loss: 0.2792094945907593
Batch 22/64 loss: 0.27138495445251465
Batch 23/64 loss: 0.2854844331741333
Batch 24/64 loss: 0.2816476821899414
Batch 25/64 loss: 0.2757914662361145
Batch 26/64 loss: 0.27990543842315674
Batch 27/64 loss: 0.2733606696128845
Batch 28/64 loss: 0.2818222641944885
Batch 29/64 loss: 0.2712925672531128
Batch 30/64 loss: 0.28023290634155273
Batch 31/64 loss: 0.2757865786552429
Batch 32/64 loss: 0.27846431732177734
Batch 33/64 loss: 0.2675800323486328
Batch 34/64 loss: 0.2766982913017273
Batch 35/64 loss: 0.2763408422470093
Batch 36/64 loss: 0.27077698707580566
Batch 37/64 loss: 0.27849799394607544
Batch 38/64 loss: 0.2802605628967285
Batch 39/64 loss: 0.2836351990699768
Batch 40/64 loss: 0.28083598613739014
Batch 41/64 loss: 0.28737008571624756
Batch 42/64 loss: 0.2860335111618042
Batch 43/64 loss: 0.27701836824417114
Batch 44/64 loss: 0.2795930504798889
Batch 45/64 loss: 0.27546679973602295
Batch 46/64 loss: 0.2718493342399597
Batch 47/64 loss: 0.2726758122444153
Batch 48/64 loss: 0.2761375904083252
Batch 49/64 loss: 0.27645665407180786
Batch 50/64 loss: 0.2762629985809326
Batch 51/64 loss: 0.27706634998321533
Batch 52/64 loss: 0.278179407119751
Batch 53/64 loss: 0.2853127717971802
Batch 54/64 loss: 0.26566779613494873
Batch 55/64 loss: 0.2779916524887085
Batch 56/64 loss: 0.27126336097717285
Batch 57/64 loss: 0.26956963539123535
Batch 58/64 loss: 0.2755354046821594
Batch 59/64 loss: 0.2806968688964844
Batch 60/64 loss: 0.27185750007629395
Batch 61/64 loss: 0.2761077880859375
Batch 62/64 loss: 0.2708446979522705
Batch 63/64 loss: 0.2829461097717285
Batch 64/64 loss: 0.27316081523895264
Epoch 18  Train loss: 0.2773293415705363  Val loss: 0.2848872655036114
Saving best model, epoch: 18
Epoch 19
-------------------------------
Batch 1/64 loss: 0.2765907049179077
Batch 2/64 loss: 0.2827354669570923
Batch 3/64 loss: 0.2725924253463745
Batch 4/64 loss: 0.2706797122955322
Batch 5/64 loss: 0.2820357084274292
Batch 6/64 loss: 0.27456599473953247
Batch 7/64 loss: 0.27856290340423584
Batch 8/64 loss: 0.28281843662261963
Batch 9/64 loss: 0.271271288394928
Batch 10/64 loss: 0.28626394271850586
Batch 11/64 loss: 0.27984583377838135
Batch 12/64 loss: 0.2801598310470581
Batch 13/64 loss: 0.2718985080718994
Batch 14/64 loss: 0.2737390398979187
Batch 15/64 loss: 0.2656216025352478
Batch 16/64 loss: 0.26959067583084106
Batch 17/64 loss: 0.27135950326919556
Batch 18/64 loss: 0.27894604206085205
Batch 19/64 loss: 0.2651854157447815
Batch 20/64 loss: 0.2687159776687622
Batch 21/64 loss: 0.2879640460014343
Batch 22/64 loss: 0.27580660581588745
Batch 23/64 loss: 0.275692880153656
Batch 24/64 loss: 0.26776593923568726
Batch 25/64 loss: 0.27914929389953613
Batch 26/64 loss: 0.2742541432380676
Batch 27/64 loss: 0.27334070205688477
Batch 28/64 loss: 0.2747577428817749
Batch 29/64 loss: 0.2803640365600586
Batch 30/64 loss: 0.2803010940551758
Batch 31/64 loss: 0.2737233638763428
Batch 32/64 loss: 0.2771815061569214
Batch 33/64 loss: 0.27165329456329346
Batch 34/64 loss: 0.27571725845336914
Batch 35/64 loss: 0.2782517075538635
Batch 36/64 loss: 0.2734295129776001
Batch 37/64 loss: 0.2785753607749939
Batch 38/64 loss: 0.27489328384399414
Batch 39/64 loss: 0.2809153199195862
Batch 40/64 loss: 0.27865487337112427
Batch 41/64 loss: 0.27475208044052124
Batch 42/64 loss: 0.2689470648765564
Batch 43/64 loss: 0.2796429395675659
Batch 44/64 loss: 0.2796022295951843
Batch 45/64 loss: 0.2751663327217102
Batch 46/64 loss: 0.27610623836517334
Batch 47/64 loss: 0.280434250831604
Batch 48/64 loss: 0.2786663770675659
Batch 49/64 loss: 0.2695009112358093
Batch 50/64 loss: 0.283456027507782
Batch 51/64 loss: 0.2751970887184143
Batch 52/64 loss: 0.2695746421813965
Batch 53/64 loss: 0.2764185070991516
Batch 54/64 loss: 0.26877129077911377
Batch 55/64 loss: 0.2752598524093628
Batch 56/64 loss: 0.2717457413673401
Batch 57/64 loss: 0.27388250827789307
Batch 58/64 loss: 0.27374857664108276
Batch 59/64 loss: 0.28315579891204834
Batch 60/64 loss: 0.27649348974227905
Batch 61/64 loss: 0.280911922454834
Batch 62/64 loss: 0.27980881929397583
Batch 63/64 loss: 0.27631962299346924
Batch 64/64 loss: 0.27339446544647217
Epoch 19  Train loss: 0.2758930061377731  Val loss: 0.28803518322325244
Epoch 20
-------------------------------
Batch 1/64 loss: 0.2696126699447632
Batch 2/64 loss: 0.27056193351745605
Batch 3/64 loss: 0.26978790760040283
Batch 4/64 loss: 0.2749342918395996
Batch 5/64 loss: 0.2807495594024658
Batch 6/64 loss: 0.2777872085571289
Batch 7/64 loss: 0.2715098261833191
Batch 8/64 loss: 0.28041601181030273
Batch 9/64 loss: 0.267072856426239
Batch 10/64 loss: 0.27424192428588867
Batch 11/64 loss: 0.27545666694641113
Batch 12/64 loss: 0.28233909606933594
Batch 13/64 loss: 0.27643418312072754
Batch 14/64 loss: 0.26767849922180176
Batch 15/64 loss: 0.2735726833343506
Batch 16/64 loss: 0.2746788263320923
Batch 17/64 loss: 0.2702590227127075
Batch 18/64 loss: 0.2812255620956421
Batch 19/64 loss: 0.26367974281311035
Batch 20/64 loss: 0.279371976852417
Batch 21/64 loss: 0.27522432804107666
Batch 22/64 loss: 0.2881782054901123
Batch 23/64 loss: 0.2717580795288086
Batch 24/64 loss: 0.27319687604904175
Batch 25/64 loss: 0.27460235357284546
Batch 26/64 loss: 0.27453017234802246
Batch 27/64 loss: 0.2714085578918457
Batch 28/64 loss: 0.2755785584449768
Batch 29/64 loss: 0.2658348083496094
Batch 30/64 loss: 0.2751612663269043
Batch 31/64 loss: 0.27800846099853516
Batch 32/64 loss: 0.27404874563217163
Batch 33/64 loss: 0.27177172899246216
Batch 34/64 loss: 0.27167385816574097
Batch 35/64 loss: 0.2614585757255554
Batch 36/64 loss: 0.27452999353408813
Batch 37/64 loss: 0.2687839865684509
Batch 38/64 loss: 0.2645360827445984
Batch 39/64 loss: 0.2712383270263672
Batch 40/64 loss: 0.27559494972229004
Batch 41/64 loss: 0.2684546113014221
Batch 42/64 loss: 0.27435195446014404
Batch 43/64 loss: 0.2743232250213623
Batch 44/64 loss: 0.27920466661453247
Batch 45/64 loss: 0.2687336802482605
Batch 46/64 loss: 0.27876734733581543
Batch 47/64 loss: 0.27942800521850586
Batch 48/64 loss: 0.2758070230484009
Batch 49/64 loss: 0.2701001763343811
Batch 50/64 loss: 0.2721940279006958
Batch 51/64 loss: 0.2730178236961365
Batch 52/64 loss: 0.2725006341934204
Batch 53/64 loss: 0.2677866816520691
Batch 54/64 loss: 0.2778882384300232
Batch 55/64 loss: 0.2735656499862671
Batch 56/64 loss: 0.27560335397720337
Batch 57/64 loss: 0.27094531059265137
Batch 58/64 loss: 0.28518009185791016
Batch 59/64 loss: 0.27573585510253906
Batch 60/64 loss: 0.2680014371871948
Batch 61/64 loss: 0.2817944288253784
Batch 62/64 loss: 0.2730843424797058
Batch 63/64 loss: 0.28566765785217285
Batch 64/64 loss: 0.2669273614883423
Epoch 20  Train loss: 0.2738952173906214  Val loss: 0.28628825977495853
Epoch 21
-------------------------------
Batch 1/64 loss: 0.2789095640182495
Batch 2/64 loss: 0.27568697929382324
Batch 3/64 loss: 0.2738276720046997
Batch 4/64 loss: 0.2751510739326477
Batch 5/64 loss: 0.2752859592437744
Batch 6/64 loss: 0.27030742168426514
Batch 7/64 loss: 0.28737831115722656
Batch 8/64 loss: 0.27321410179138184
Batch 9/64 loss: 0.2719329595565796
Batch 10/64 loss: 0.2826085090637207
Batch 11/64 loss: 0.27368003129959106
Batch 12/64 loss: 0.2599726915359497
Batch 13/64 loss: 0.2708902359008789
Batch 14/64 loss: 0.2678108215332031
Batch 15/64 loss: 0.2710410952568054
Batch 16/64 loss: 0.27397751808166504
Batch 17/64 loss: 0.2640659213066101
Batch 18/64 loss: 0.27658069133758545
Batch 19/64 loss: 0.2689934968948364
Batch 20/64 loss: 0.2717849016189575
Batch 21/64 loss: 0.2660301923751831
Batch 22/64 loss: 0.2732139825820923
Batch 23/64 loss: 0.27602648735046387
Batch 24/64 loss: 0.2800642251968384
Batch 25/64 loss: 0.2722036838531494
Batch 26/64 loss: 0.2745738625526428
Batch 27/64 loss: 0.26724082231521606
Batch 28/64 loss: 0.275532603263855
Batch 29/64 loss: 0.2665443420410156
Batch 30/64 loss: 0.2714017629623413
Batch 31/64 loss: 0.2634437084197998
Batch 32/64 loss: 0.28017377853393555
Batch 33/64 loss: 0.27685797214508057
Batch 34/64 loss: 0.2805163264274597
Batch 35/64 loss: 0.2633167505264282
Batch 36/64 loss: 0.26990437507629395
Batch 37/64 loss: 0.2644941806793213
Batch 38/64 loss: 0.27166450023651123
Batch 39/64 loss: 0.2738298177719116
Batch 40/64 loss: 0.2646653652191162
Batch 41/64 loss: 0.2695436477661133
Batch 42/64 loss: 0.2732999324798584
Batch 43/64 loss: 0.269498348236084
Batch 44/64 loss: 0.2761530876159668
Batch 45/64 loss: 0.26827168464660645
Batch 46/64 loss: 0.26286977529525757
Batch 47/64 loss: 0.27811670303344727
Batch 48/64 loss: 0.26876431703567505
Batch 49/64 loss: 0.26811206340789795
Batch 50/64 loss: 0.27138984203338623
Batch 51/64 loss: 0.2711002826690674
Batch 52/64 loss: 0.26236188411712646
Batch 53/64 loss: 0.2707710266113281
Batch 54/64 loss: 0.2708073854446411
Batch 55/64 loss: 0.27252477407455444
Batch 56/64 loss: 0.26970410346984863
Batch 57/64 loss: 0.2646743059158325
Batch 58/64 loss: 0.27237939834594727
Batch 59/64 loss: 0.2752957344055176
Batch 60/64 loss: 0.27313196659088135
Batch 61/64 loss: 0.2621333599090576
Batch 62/64 loss: 0.2777777910232544
Batch 63/64 loss: 0.2785518169403076
Batch 64/64 loss: 0.27084940671920776
Epoch 21  Train loss: 0.2717673565827164  Val loss: 0.2812977324646363
Saving best model, epoch: 21
Epoch 22
-------------------------------
Batch 1/64 loss: 0.2666515111923218
Batch 2/64 loss: 0.2680702209472656
Batch 3/64 loss: 0.2787235975265503
Batch 4/64 loss: 0.2713305354118347
Batch 5/64 loss: 0.2697514295578003
Batch 6/64 loss: 0.2767331600189209
Batch 7/64 loss: 0.2779476046562195
Batch 8/64 loss: 0.2758296728134155
Batch 9/64 loss: 0.2637699842453003
Batch 10/64 loss: 0.27402687072753906
Batch 11/64 loss: 0.2743062973022461
Batch 12/64 loss: 0.2738105058670044
Batch 13/64 loss: 0.2646142840385437
Batch 14/64 loss: 0.2743816375732422
Batch 15/64 loss: 0.26486122608184814
Batch 16/64 loss: 0.2661864757537842
Batch 17/64 loss: 0.2577582001686096
Batch 18/64 loss: 0.2668660283088684
Batch 19/64 loss: 0.2723824977874756
Batch 20/64 loss: 0.27193957567214966
Batch 21/64 loss: 0.2789638042449951
Batch 22/64 loss: 0.269321084022522
Batch 23/64 loss: 0.264809787273407
Batch 24/64 loss: 0.26335835456848145
Batch 25/64 loss: 0.2660132646560669
Batch 26/64 loss: 0.26131105422973633
Batch 27/64 loss: 0.2726346254348755
Batch 28/64 loss: 0.2649691104888916
Batch 29/64 loss: 0.26468074321746826
Batch 30/64 loss: 0.27219802141189575
Batch 31/64 loss: 0.2642393112182617
Batch 32/64 loss: 0.26988035440444946
Batch 33/64 loss: 0.2741643190383911
Batch 34/64 loss: 0.2737743854522705
Batch 35/64 loss: 0.2719542384147644
Batch 36/64 loss: 0.2714991569519043
Batch 37/64 loss: 0.2701914310455322
Batch 38/64 loss: 0.26964980363845825
Batch 39/64 loss: 0.2746758460998535
Batch 40/64 loss: 0.2738282084465027
Batch 41/64 loss: 0.26711034774780273
Batch 42/64 loss: 0.26229727268218994
Batch 43/64 loss: 0.26569902896881104
Batch 44/64 loss: 0.27548325061798096
Batch 45/64 loss: 0.2734556794166565
Batch 46/64 loss: 0.27246659994125366
Batch 47/64 loss: 0.26922607421875
Batch 48/64 loss: 0.26858723163604736
Batch 49/64 loss: 0.27346324920654297
Batch 50/64 loss: 0.2671862840652466
Batch 51/64 loss: 0.26888787746429443
Batch 52/64 loss: 0.27443480491638184
Batch 53/64 loss: 0.26301318407058716
Batch 54/64 loss: 0.2720816135406494
Batch 55/64 loss: 0.27424073219299316
Batch 56/64 loss: 0.2682682275772095
Batch 57/64 loss: 0.2629575729370117
Batch 58/64 loss: 0.2689887285232544
Batch 59/64 loss: 0.2690465450286865
Batch 60/64 loss: 0.26822173595428467
Batch 61/64 loss: 0.27138662338256836
Batch 62/64 loss: 0.2743874788284302
Batch 63/64 loss: 0.2674403190612793
Batch 64/64 loss: 0.2740796208381653
Epoch 22  Train loss: 0.26989722976497577  Val loss: 0.276318098261594
Saving best model, epoch: 22
Epoch 23
-------------------------------
Batch 1/64 loss: 0.2661629915237427
Batch 2/64 loss: 0.26820504665374756
Batch 3/64 loss: 0.2756814956665039
Batch 4/64 loss: 0.28307217359542847
Batch 5/64 loss: 0.2573091983795166
Batch 6/64 loss: 0.2702265977859497
Batch 7/64 loss: 0.2717634439468384
Batch 8/64 loss: 0.26502931118011475
Batch 9/64 loss: 0.2711048126220703
Batch 10/64 loss: 0.2792150378227234
Batch 11/64 loss: 0.26803499460220337
Batch 12/64 loss: 0.26850730180740356
Batch 13/64 loss: 0.26856303215026855
Batch 14/64 loss: 0.26150137186050415
Batch 15/64 loss: 0.2693706750869751
Batch 16/64 loss: 0.25923120975494385
Batch 17/64 loss: 0.2669748067855835
Batch 18/64 loss: 0.26685988903045654
Batch 19/64 loss: 0.2656809091567993
Batch 20/64 loss: 0.2668372392654419
Batch 21/64 loss: 0.27166879177093506
Batch 22/64 loss: 0.2722485065460205
Batch 23/64 loss: 0.2696000337600708
Batch 24/64 loss: 0.2679966688156128
Batch 25/64 loss: 0.26290297508239746
Batch 26/64 loss: 0.2710554599761963
Batch 27/64 loss: 0.25683218240737915
Batch 28/64 loss: 0.265521764755249
Batch 29/64 loss: 0.2652742862701416
Batch 30/64 loss: 0.2585960626602173
Batch 31/64 loss: 0.26385951042175293
Batch 32/64 loss: 0.2675091624259949
Batch 33/64 loss: 0.2688002586364746
Batch 34/64 loss: 0.2676272392272949
Batch 35/64 loss: 0.2665122151374817
Batch 36/64 loss: 0.26773643493652344
Batch 37/64 loss: 0.26713937520980835
Batch 38/64 loss: 0.27080297470092773
Batch 39/64 loss: 0.2673739194869995
Batch 40/64 loss: 0.26594579219818115
Batch 41/64 loss: 0.26708531379699707
Batch 42/64 loss: 0.26946544647216797
Batch 43/64 loss: 0.26618170738220215
Batch 44/64 loss: 0.2667783498764038
Batch 45/64 loss: 0.268363356590271
Batch 46/64 loss: 0.26195967197418213
Batch 47/64 loss: 0.265405535697937
Batch 48/64 loss: 0.26489710807800293
Batch 49/64 loss: 0.26431936025619507
Batch 50/64 loss: 0.26591384410858154
Batch 51/64 loss: 0.2728273868560791
Batch 52/64 loss: 0.2661660313606262
Batch 53/64 loss: 0.27643799781799316
Batch 54/64 loss: 0.2722470760345459
Batch 55/64 loss: 0.2646903991699219
Batch 56/64 loss: 0.2720438838005066
Batch 57/64 loss: 0.26389777660369873
Batch 58/64 loss: 0.2597047686576843
Batch 59/64 loss: 0.26421070098876953
Batch 60/64 loss: 0.26577961444854736
Batch 61/64 loss: 0.2522702217102051
Batch 62/64 loss: 0.26782190799713135
Batch 63/64 loss: 0.2722792625427246
Batch 64/64 loss: 0.26297301054000854
Epoch 23  Train loss: 0.26717351581536086  Val loss: 0.275038617378248
Saving best model, epoch: 23
Epoch 24
-------------------------------
Batch 1/64 loss: 0.2702738642692566
Batch 2/64 loss: 0.2697831392288208
Batch 3/64 loss: 0.2625482678413391
Batch 4/64 loss: 0.26600968837738037
Batch 5/64 loss: 0.2632383108139038
Batch 6/64 loss: 0.2698488235473633
Batch 7/64 loss: 0.26734912395477295
Batch 8/64 loss: 0.2585601806640625
Batch 9/64 loss: 0.2785841226577759
Batch 10/64 loss: 0.273828387260437
Batch 11/64 loss: 0.2725433111190796
Batch 12/64 loss: 0.2568330764770508
Batch 13/64 loss: 0.2642311453819275
Batch 14/64 loss: 0.2740362882614136
Batch 15/64 loss: 0.27546530961990356
Batch 16/64 loss: 0.26531219482421875
Batch 17/64 loss: 0.2656579613685608
Batch 18/64 loss: 0.25836843252182007
Batch 19/64 loss: 0.2676553726196289
Batch 20/64 loss: 0.26118946075439453
Batch 21/64 loss: 0.26553452014923096
Batch 22/64 loss: 0.26398128271102905
Batch 23/64 loss: 0.2646559476852417
Batch 24/64 loss: 0.26627981662750244
Batch 25/64 loss: 0.2562050223350525
Batch 26/64 loss: 0.260079562664032
Batch 27/64 loss: 0.25938743352890015
Batch 28/64 loss: 0.26668524742126465
Batch 29/64 loss: 0.2667248845100403
Batch 30/64 loss: 0.26492840051651
Batch 31/64 loss: 0.259519100189209
Batch 32/64 loss: 0.2565615773200989
Batch 33/64 loss: 0.26916754245758057
Batch 34/64 loss: 0.25582873821258545
Batch 35/64 loss: 0.2653343081474304
Batch 36/64 loss: 0.27380305528640747
Batch 37/64 loss: 0.26222771406173706
Batch 38/64 loss: 0.26831042766571045
Batch 39/64 loss: 0.26319730281829834
Batch 40/64 loss: 0.2672325372695923
Batch 41/64 loss: 0.2632564306259155
Batch 42/64 loss: 0.2671905755996704
Batch 43/64 loss: 0.2604246735572815
Batch 44/64 loss: 0.2678218483924866
Batch 45/64 loss: 0.25786852836608887
Batch 46/64 loss: 0.25728142261505127
Batch 47/64 loss: 0.2692294716835022
Batch 48/64 loss: 0.26525819301605225
Batch 49/64 loss: 0.2656676173210144
Batch 50/64 loss: 0.2690194249153137
Batch 51/64 loss: 0.27043819427490234
Batch 52/64 loss: 0.2605588436126709
Batch 53/64 loss: 0.2658463716506958
Batch 54/64 loss: 0.2622631788253784
Batch 55/64 loss: 0.2698519229888916
Batch 56/64 loss: 0.263019323348999
Batch 57/64 loss: 0.25183331966400146
Batch 58/64 loss: 0.2673828601837158
Batch 59/64 loss: 0.2621645927429199
Batch 60/64 loss: 0.25673556327819824
Batch 61/64 loss: 0.27058207988739014
Batch 62/64 loss: 0.2644304633140564
Batch 63/64 loss: 0.2573084831237793
Batch 64/64 loss: 0.2663424611091614
Epoch 24  Train loss: 0.2648180566582025  Val loss: 0.2705129139202157
Saving best model, epoch: 24
Epoch 25
-------------------------------
Batch 1/64 loss: 0.2656072974205017
Batch 2/64 loss: 0.25521212816238403
Batch 3/64 loss: 0.27227169275283813
Batch 4/64 loss: 0.2553313970565796
Batch 5/64 loss: 0.2615957260131836
Batch 6/64 loss: 0.2601788640022278
Batch 7/64 loss: 0.260226309299469
Batch 8/64 loss: 0.26271766424179077
Batch 9/64 loss: 0.2617083787918091
Batch 10/64 loss: 0.2600317597389221
Batch 11/64 loss: 0.25358426570892334
Batch 12/64 loss: 0.26089078187942505
Batch 13/64 loss: 0.2566766142845154
Batch 14/64 loss: 0.26605474948883057
Batch 15/64 loss: 0.2672305107116699
Batch 16/64 loss: 0.2631049156188965
Batch 17/64 loss: 0.2711018919944763
Batch 18/64 loss: 0.2610349655151367
Batch 19/64 loss: 0.2619660496711731
Batch 20/64 loss: 0.2626129388809204
Batch 21/64 loss: 0.2617466449737549
Batch 22/64 loss: 0.2583654522895813
Batch 23/64 loss: 0.2605326175689697
Batch 24/64 loss: 0.2596130967140198
Batch 25/64 loss: 0.251947283744812
Batch 26/64 loss: 0.27600932121276855
Batch 27/64 loss: 0.26035141944885254
Batch 28/64 loss: 0.2672460079193115
Batch 29/64 loss: 0.2688620686531067
Batch 30/64 loss: 0.2667827606201172
Batch 31/64 loss: 0.2603767514228821
Batch 32/64 loss: 0.2718167304992676
Batch 33/64 loss: 0.25342386960983276
Batch 34/64 loss: 0.2549017667770386
Batch 35/64 loss: 0.2767763137817383
Batch 36/64 loss: 0.26952415704727173
Batch 37/64 loss: 0.26958727836608887
Batch 38/64 loss: 0.2575458288192749
Batch 39/64 loss: 0.2714199423789978
Batch 40/64 loss: 0.26347917318344116
Batch 41/64 loss: 0.2636997699737549
Batch 42/64 loss: 0.25745177268981934
Batch 43/64 loss: 0.2579694986343384
Batch 44/64 loss: 0.25565004348754883
Batch 45/64 loss: 0.2613677978515625
Batch 46/64 loss: 0.2618875503540039
Batch 47/64 loss: 0.26543599367141724
Batch 48/64 loss: 0.25367337465286255
Batch 49/64 loss: 0.2668447494506836
Batch 50/64 loss: 0.27370768785476685
Batch 51/64 loss: 0.2594112157821655
Batch 52/64 loss: 0.2596409320831299
Batch 53/64 loss: 0.25825047492980957
Batch 54/64 loss: 0.2628405690193176
Batch 55/64 loss: 0.2622705101966858
Batch 56/64 loss: 0.2677909731864929
Batch 57/64 loss: 0.2555888891220093
Batch 58/64 loss: 0.2607266902923584
Batch 59/64 loss: 0.25528669357299805
Batch 60/64 loss: 0.2672642469406128
Batch 61/64 loss: 0.2604912519454956
Batch 62/64 loss: 0.26543617248535156
Batch 63/64 loss: 0.2667122483253479
Batch 64/64 loss: 0.267453670501709
Epoch 25  Train loss: 0.26257939993166457  Val loss: 0.2731688616611704
Epoch 26
-------------------------------
Batch 1/64 loss: 0.26398324966430664
Batch 2/64 loss: 0.26035749912261963
Batch 3/64 loss: 0.2589876651763916
Batch 4/64 loss: 0.26503288745880127
Batch 5/64 loss: 0.25080597400665283
Batch 6/64 loss: 0.26421117782592773
Batch 7/64 loss: 0.2612190246582031
Batch 8/64 loss: 0.2655695676803589
Batch 9/64 loss: 0.25517207384109497
Batch 10/64 loss: 0.257316529750824
Batch 11/64 loss: 0.25646549463272095
Batch 12/64 loss: 0.2562946677207947
Batch 13/64 loss: 0.26538705825805664
Batch 14/64 loss: 0.25050586462020874
Batch 15/64 loss: 0.2576104402542114
Batch 16/64 loss: 0.2602936029434204
Batch 17/64 loss: 0.2546125650405884
Batch 18/64 loss: 0.2616789937019348
Batch 19/64 loss: 0.25343358516693115
Batch 20/64 loss: 0.25438129901885986
Batch 21/64 loss: 0.270055890083313
Batch 22/64 loss: 0.25887417793273926
Batch 23/64 loss: 0.2699737548828125
Batch 24/64 loss: 0.2736419439315796
Batch 25/64 loss: 0.25966912508010864
Batch 26/64 loss: 0.2614750266075134
Batch 27/64 loss: 0.26901960372924805
Batch 28/64 loss: 0.25958824157714844
Batch 29/64 loss: 0.26663243770599365
Batch 30/64 loss: 0.2588917016983032
Batch 31/64 loss: 0.26094698905944824
Batch 32/64 loss: 0.25915879011154175
Batch 33/64 loss: 0.258716344833374
Batch 34/64 loss: 0.2602905035018921
Batch 35/64 loss: 0.2613493800163269
Batch 36/64 loss: 0.25600916147232056
Batch 37/64 loss: 0.24997401237487793
Batch 38/64 loss: 0.24661916494369507
Batch 39/64 loss: 0.2558250427246094
Batch 40/64 loss: 0.25628983974456787
Batch 41/64 loss: 0.2627936601638794
Batch 42/64 loss: 0.2604355812072754
Batch 43/64 loss: 0.2615630030632019
Batch 44/64 loss: 0.2545693516731262
Batch 45/64 loss: 0.2606663107872009
Batch 46/64 loss: 0.25506591796875
Batch 47/64 loss: 0.26027345657348633
Batch 48/64 loss: 0.2679567337036133
Batch 49/64 loss: 0.27068108320236206
Batch 50/64 loss: 0.26126664876937866
Batch 51/64 loss: 0.24745285511016846
Batch 52/64 loss: 0.2552602291107178
Batch 53/64 loss: 0.2581818103790283
Batch 54/64 loss: 0.2567809820175171
Batch 55/64 loss: 0.2567172050476074
Batch 56/64 loss: 0.2623025178909302
Batch 57/64 loss: 0.2731417417526245
Batch 58/64 loss: 0.26983511447906494
Batch 59/64 loss: 0.26131755113601685
Batch 60/64 loss: 0.25920403003692627
Batch 61/64 loss: 0.26582348346710205
Batch 62/64 loss: 0.27351635694503784
Batch 63/64 loss: 0.2569156289100647
Batch 64/64 loss: 0.24844706058502197
Epoch 26  Train loss: 0.26014708864922614  Val loss: 0.2655125573738334
Saving best model, epoch: 26
Epoch 27
-------------------------------
Batch 1/64 loss: 0.2607492208480835
Batch 2/64 loss: 0.28362810611724854
Batch 3/64 loss: 0.2461310625076294
Batch 4/64 loss: 0.26283204555511475
Batch 5/64 loss: 0.25822699069976807
Batch 6/64 loss: 0.2534586191177368
Batch 7/64 loss: 0.26174795627593994
Batch 8/64 loss: 0.2666865587234497
Batch 9/64 loss: 0.2627849578857422
Batch 10/64 loss: 0.25297683477401733
Batch 11/64 loss: 0.25085800886154175
Batch 12/64 loss: 0.2558819651603699
Batch 13/64 loss: 0.27445781230926514
Batch 14/64 loss: 0.2556818127632141
Batch 15/64 loss: 0.2605322003364563
Batch 16/64 loss: 0.2607409358024597
Batch 17/64 loss: 0.2708287835121155
Batch 18/64 loss: 0.25676995515823364
Batch 19/64 loss: 0.2618356943130493
Batch 20/64 loss: 0.26525479555130005
Batch 21/64 loss: 0.26582813262939453
Batch 22/64 loss: 0.2574273943901062
Batch 23/64 loss: 0.25311386585235596
Batch 24/64 loss: 0.2654554843902588
Batch 25/64 loss: 0.2592684030532837
Batch 26/64 loss: 0.26258641481399536
Batch 27/64 loss: 0.25857436656951904
Batch 28/64 loss: 0.2637227773666382
Batch 29/64 loss: 0.25318747758865356
Batch 30/64 loss: 0.2564607262611389
Batch 31/64 loss: 0.27128851413726807
Batch 32/64 loss: 0.25594663619995117
Batch 33/64 loss: 0.26331210136413574
Batch 34/64 loss: 0.2599305510520935
Batch 35/64 loss: 0.272741436958313
Batch 36/64 loss: 0.25539982318878174
Batch 37/64 loss: 0.25039762258529663
Batch 38/64 loss: 0.2534449100494385
Batch 39/64 loss: 0.26043450832366943
Batch 40/64 loss: 0.2617691159248352
Batch 41/64 loss: 0.2574475407600403
Batch 42/64 loss: 0.25981634855270386
Batch 43/64 loss: 0.25742805004119873
Batch 44/64 loss: 0.26011431217193604
Batch 45/64 loss: 0.25886887311935425
Batch 46/64 loss: 0.26079225540161133
Batch 47/64 loss: 0.2631973624229431
Batch 48/64 loss: 0.2571711540222168
Batch 49/64 loss: 0.26039981842041016
Batch 50/64 loss: 0.24704980850219727
Batch 51/64 loss: 0.26158273220062256
Batch 52/64 loss: 0.26521867513656616
Batch 53/64 loss: 0.2608301639556885
Batch 54/64 loss: 0.26039963960647583
Batch 55/64 loss: 0.2569936513900757
Batch 56/64 loss: 0.2525097131729126
Batch 57/64 loss: 0.25972914695739746
Batch 58/64 loss: 0.25464898347854614
Batch 59/64 loss: 0.2603999376296997
Batch 60/64 loss: 0.25657910108566284
Batch 61/64 loss: 0.25217074155807495
Batch 62/64 loss: 0.266157865524292
Batch 63/64 loss: 0.24872875213623047
Batch 64/64 loss: 0.2557380795478821
Epoch 27  Train loss: 0.25964537671968047  Val loss: 0.262182983745824
Saving best model, epoch: 27
Epoch 28
-------------------------------
Batch 1/64 loss: 0.2637143135070801
Batch 2/64 loss: 0.25093209743499756
Batch 3/64 loss: 0.26271235942840576
Batch 4/64 loss: 0.2477063536643982
Batch 5/64 loss: 0.2539621591567993
Batch 6/64 loss: 0.2571953535079956
Batch 7/64 loss: 0.25304603576660156
Batch 8/64 loss: 0.24519610404968262
Batch 9/64 loss: 0.2609628438949585
Batch 10/64 loss: 0.24977070093154907
Batch 11/64 loss: 0.2573089599609375
Batch 12/64 loss: 0.25338882207870483
Batch 13/64 loss: 0.2515321969985962
Batch 14/64 loss: 0.25517940521240234
Batch 15/64 loss: 0.2607298493385315
Batch 16/64 loss: 0.25642263889312744
Batch 17/64 loss: 0.26587724685668945
Batch 18/64 loss: 0.2587021589279175
Batch 19/64 loss: 0.26036274433135986
Batch 20/64 loss: 0.26263201236724854
Batch 21/64 loss: 0.25871503353118896
Batch 22/64 loss: 0.26821279525756836
Batch 23/64 loss: 0.26480942964553833
Batch 24/64 loss: 0.24607527256011963
Batch 25/64 loss: 0.2589418292045593
Batch 26/64 loss: 0.2541416883468628
Batch 27/64 loss: 0.25655078887939453
Batch 28/64 loss: 0.25705671310424805
Batch 29/64 loss: 0.24886107444763184
Batch 30/64 loss: 0.25288838148117065
Batch 31/64 loss: 0.2576466202735901
Batch 32/64 loss: 0.25484275817871094
Batch 33/64 loss: 0.2572331428527832
Batch 34/64 loss: 0.24249780178070068
Batch 35/64 loss: 0.25964272022247314
Batch 36/64 loss: 0.252491295337677
Batch 37/64 loss: 0.25443965196609497
Batch 38/64 loss: 0.2568971514701843
Batch 39/64 loss: 0.2586631774902344
Batch 40/64 loss: 0.26095134019851685
Batch 41/64 loss: 0.24861353635787964
Batch 42/64 loss: 0.25231069326400757
Batch 43/64 loss: 0.24925971031188965
Batch 44/64 loss: 0.25673288106918335
Batch 45/64 loss: 0.24294155836105347
Batch 46/64 loss: 0.26196831464767456
Batch 47/64 loss: 0.2579299807548523
Batch 48/64 loss: 0.24443942308425903
Batch 49/64 loss: 0.24937641620635986
Batch 50/64 loss: 0.25560033321380615
Batch 51/64 loss: 0.2585555911064148
Batch 52/64 loss: 0.26369667053222656
Batch 53/64 loss: 0.25647515058517456
Batch 54/64 loss: 0.25653183460235596
Batch 55/64 loss: 0.25722718238830566
Batch 56/64 loss: 0.25393903255462646
Batch 57/64 loss: 0.2553335428237915
Batch 58/64 loss: 0.2552231550216675
Batch 59/64 loss: 0.2511502504348755
Batch 60/64 loss: 0.24592465162277222
Batch 61/64 loss: 0.24816405773162842
Batch 62/64 loss: 0.25223493576049805
Batch 63/64 loss: 0.252713680267334
Batch 64/64 loss: 0.24843764305114746
Epoch 28  Train loss: 0.2550833543141683  Val loss: 0.2635895673352009
Epoch 29
-------------------------------
Batch 1/64 loss: 0.24902307987213135
Batch 2/64 loss: 0.2499876618385315
Batch 3/64 loss: 0.2639879584312439
Batch 4/64 loss: 0.2556401491165161
Batch 5/64 loss: 0.24652814865112305
Batch 6/64 loss: 0.2729318141937256
Batch 7/64 loss: 0.2466733455657959
Batch 8/64 loss: 0.2435092329978943
Batch 9/64 loss: 0.24370694160461426
Batch 10/64 loss: 0.24401438236236572
Batch 11/64 loss: 0.2701749801635742
Batch 12/64 loss: 0.2561511993408203
Batch 13/64 loss: 0.26195228099823
Batch 14/64 loss: 0.26395463943481445
Batch 15/64 loss: 0.25865161418914795
Batch 16/64 loss: 0.24398481845855713
Batch 17/64 loss: 0.2519669532775879
Batch 18/64 loss: 0.2425643801689148
Batch 19/64 loss: 0.2563730478286743
Batch 20/64 loss: 0.24715352058410645
Batch 21/64 loss: 0.2476845383644104
Batch 22/64 loss: 0.2591897249221802
Batch 23/64 loss: 0.256436824798584
Batch 24/64 loss: 0.25944483280181885
Batch 25/64 loss: 0.25324881076812744
Batch 26/64 loss: 0.24921202659606934
Batch 27/64 loss: 0.26012909412384033
Batch 28/64 loss: 0.24333173036575317
Batch 29/64 loss: 0.2516295909881592
Batch 30/64 loss: 0.252848744392395
Batch 31/64 loss: 0.2519518733024597
Batch 32/64 loss: 0.2554992437362671
Batch 33/64 loss: 0.2541428804397583
Batch 34/64 loss: 0.25619179010391235
Batch 35/64 loss: 0.25320130586624146
Batch 36/64 loss: 0.2502387762069702
Batch 37/64 loss: 0.2459423542022705
Batch 38/64 loss: 0.2481544017791748
Batch 39/64 loss: 0.24691736698150635
Batch 40/64 loss: 0.25838279724121094
Batch 41/64 loss: 0.2422693967819214
Batch 42/64 loss: 0.256189227104187
Batch 43/64 loss: 0.24160706996917725
Batch 44/64 loss: 0.2559424042701721
Batch 45/64 loss: 0.2526369094848633
Batch 46/64 loss: 0.25298619270324707
Batch 47/64 loss: 0.25025302171707153
Batch 48/64 loss: 0.2522176504135132
Batch 49/64 loss: 0.2434086799621582
Batch 50/64 loss: 0.24868524074554443
Batch 51/64 loss: 0.24928796291351318
Batch 52/64 loss: 0.25133228302001953
Batch 53/64 loss: 0.25829267501831055
Batch 54/64 loss: 0.2534559369087219
Batch 55/64 loss: 0.25780773162841797
Batch 56/64 loss: 0.2554968595504761
Batch 57/64 loss: 0.25188255310058594
Batch 58/64 loss: 0.25032973289489746
Batch 59/64 loss: 0.25701904296875
Batch 60/64 loss: 0.25816404819488525
Batch 61/64 loss: 0.2455369234085083
Batch 62/64 loss: 0.26381975412368774
Batch 63/64 loss: 0.2550697326660156
Batch 64/64 loss: 0.2500852346420288
Epoch 29  Train loss: 0.2527680598053278  Val loss: 0.26277296076115875
Epoch 30
-------------------------------
Batch 1/64 loss: 0.25208258628845215
Batch 2/64 loss: 0.249284029006958
Batch 3/64 loss: 0.24779236316680908
Batch 4/64 loss: 0.25307774543762207
Batch 5/64 loss: 0.2520805597305298
Batch 6/64 loss: 0.24935674667358398
Batch 7/64 loss: 0.24969565868377686
Batch 8/64 loss: 0.25721240043640137
Batch 9/64 loss: 0.2638763189315796
Batch 10/64 loss: 0.24865782260894775
Batch 11/64 loss: 0.25374847650527954
Batch 12/64 loss: 0.2542898654937744
Batch 13/64 loss: 0.2544264793395996
Batch 14/64 loss: 0.25339770317077637
Batch 15/64 loss: 0.24938088655471802
Batch 16/64 loss: 0.2530406713485718
Batch 17/64 loss: 0.24643385410308838
Batch 18/64 loss: 0.257099986076355
Batch 19/64 loss: 0.2460440993309021
Batch 20/64 loss: 0.24955153465270996
Batch 21/64 loss: 0.2461380958557129
Batch 22/64 loss: 0.24724960327148438
Batch 23/64 loss: 0.24570775032043457
Batch 24/64 loss: 0.2503206729888916
Batch 25/64 loss: 0.249114990234375
Batch 26/64 loss: 0.2504499554634094
Batch 27/64 loss: 0.24705779552459717
Batch 28/64 loss: 0.2477283477783203
Batch 29/64 loss: 0.24587643146514893
Batch 30/64 loss: 0.24414008855819702
Batch 31/64 loss: 0.24361705780029297
Batch 32/64 loss: 0.26009464263916016
Batch 33/64 loss: 0.23956263065338135
Batch 34/64 loss: 0.23999828100204468
Batch 35/64 loss: 0.2491394281387329
Batch 36/64 loss: 0.24656444787979126
Batch 37/64 loss: 0.2532894015312195
Batch 38/64 loss: 0.24840128421783447
Batch 39/64 loss: 0.2486589550971985
Batch 40/64 loss: 0.2523345351219177
Batch 41/64 loss: 0.2501436471939087
Batch 42/64 loss: 0.244581937789917
Batch 43/64 loss: 0.24949824810028076
Batch 44/64 loss: 0.24011939764022827
Batch 45/64 loss: 0.25004005432128906
Batch 46/64 loss: 0.253012478351593
Batch 47/64 loss: 0.25680720806121826
Batch 48/64 loss: 0.2654423713684082
Batch 49/64 loss: 0.2446906566619873
Batch 50/64 loss: 0.2537882924079895
Batch 51/64 loss: 0.2532765865325928
Batch 52/64 loss: 0.25052380561828613
Batch 53/64 loss: 0.2542955279350281
Batch 54/64 loss: 0.2404165267944336
Batch 55/64 loss: 0.2470824122428894
Batch 56/64 loss: 0.24865341186523438
Batch 57/64 loss: 0.2427213191986084
Batch 58/64 loss: 0.26621830463409424
Batch 59/64 loss: 0.24609804153442383
Batch 60/64 loss: 0.24741137027740479
Batch 61/64 loss: 0.24352788925170898
Batch 62/64 loss: 0.25123071670532227
Batch 63/64 loss: 0.2518341541290283
Batch 64/64 loss: 0.2593526840209961
Epoch 30  Train loss: 0.2500690675249287  Val loss: 0.261398415385243
Saving best model, epoch: 30
Epoch 31
-------------------------------
Batch 1/64 loss: 0.2507493495941162
Batch 2/64 loss: 0.2496623992919922
Batch 3/64 loss: 0.2433004379272461
Batch 4/64 loss: 0.2518995404243469
Batch 5/64 loss: 0.24373149871826172
Batch 6/64 loss: 0.24311447143554688
Batch 7/64 loss: 0.23979800939559937
Batch 8/64 loss: 0.24351167678833008
Batch 9/64 loss: 0.23706293106079102
Batch 10/64 loss: 0.2564525604248047
Batch 11/64 loss: 0.24077939987182617
Batch 12/64 loss: 0.24304211139678955
Batch 13/64 loss: 0.24996352195739746
Batch 14/64 loss: 0.2483121156692505
Batch 15/64 loss: 0.2505373954772949
Batch 16/64 loss: 0.2512798309326172
Batch 17/64 loss: 0.24856889247894287
Batch 18/64 loss: 0.24367785453796387
Batch 19/64 loss: 0.24904727935791016
Batch 20/64 loss: 0.25531256198883057
Batch 21/64 loss: 0.25517797470092773
Batch 22/64 loss: 0.25123298168182373
Batch 23/64 loss: 0.2455134391784668
Batch 24/64 loss: 0.2534521222114563
Batch 25/64 loss: 0.2641473412513733
Batch 26/64 loss: 0.2522582411766052
Batch 27/64 loss: 0.25672781467437744
Batch 28/64 loss: 0.26631563901901245
Batch 29/64 loss: 0.25616925954818726
Batch 30/64 loss: 0.2644023895263672
Batch 31/64 loss: 0.24237453937530518
Batch 32/64 loss: 0.2596345543861389
Batch 33/64 loss: 0.24953699111938477
Batch 34/64 loss: 0.2452443242073059
Batch 35/64 loss: 0.2449612021446228
Batch 36/64 loss: 0.23948317766189575
Batch 37/64 loss: 0.2484074831008911
Batch 38/64 loss: 0.25352048873901367
Batch 39/64 loss: 0.2629038691520691
Batch 40/64 loss: 0.24165403842926025
Batch 41/64 loss: 0.254319429397583
Batch 42/64 loss: 0.24985271692276
Batch 43/64 loss: 0.24188727140426636
Batch 44/64 loss: 0.258280873298645
Batch 45/64 loss: 0.25848329067230225
Batch 46/64 loss: 0.24580132961273193
Batch 47/64 loss: 0.2392416000366211
Batch 48/64 loss: 0.25150012969970703
Batch 49/64 loss: 0.2406848669052124
Batch 50/64 loss: 0.24466657638549805
Batch 51/64 loss: 0.22994160652160645
Batch 52/64 loss: 0.2518026828765869
Batch 53/64 loss: 0.24329698085784912
Batch 54/64 loss: 0.252785325050354
Batch 55/64 loss: 0.24674713611602783
Batch 56/64 loss: 0.25003767013549805
Batch 57/64 loss: 0.24870753288269043
Batch 58/64 loss: 0.2556794285774231
Batch 59/64 loss: 0.251708984375
Batch 60/64 loss: 0.2423720359802246
Batch 61/64 loss: 0.24904108047485352
Batch 62/64 loss: 0.2525080442428589
Batch 63/64 loss: 0.24170005321502686
Batch 64/64 loss: 0.24263542890548706
Epoch 31  Train loss: 0.24903443023270252  Val loss: 0.2583878458160715
Saving best model, epoch: 31
Epoch 32
-------------------------------
Batch 1/64 loss: 0.24945902824401855
Batch 2/64 loss: 0.24543380737304688
Batch 3/64 loss: 0.23759925365447998
Batch 4/64 loss: 0.2532461881637573
Batch 5/64 loss: 0.234511137008667
Batch 6/64 loss: 0.24468708038330078
Batch 7/64 loss: 0.24271351099014282
Batch 8/64 loss: 0.23799747228622437
Batch 9/64 loss: 0.24706566333770752
Batch 10/64 loss: 0.24881768226623535
Batch 11/64 loss: 0.24944400787353516
Batch 12/64 loss: 0.2471914291381836
Batch 13/64 loss: 0.25091779232025146
Batch 14/64 loss: 0.2454858422279358
Batch 15/64 loss: 0.24583768844604492
Batch 16/64 loss: 0.24127936363220215
Batch 17/64 loss: 0.2459806203842163
Batch 18/64 loss: 0.23958897590637207
Batch 19/64 loss: 0.24246591329574585
Batch 20/64 loss: 0.23696434497833252
Batch 21/64 loss: 0.25427764654159546
Batch 22/64 loss: 0.2553086280822754
Batch 23/64 loss: 0.2416597604751587
Batch 24/64 loss: 0.24691170454025269
Batch 25/64 loss: 0.24457120895385742
Batch 26/64 loss: 0.2514832019805908
Batch 27/64 loss: 0.24837201833724976
Batch 28/64 loss: 0.2557549476623535
Batch 29/64 loss: 0.2405126690864563
Batch 30/64 loss: 0.24219095706939697
Batch 31/64 loss: 0.24298977851867676
Batch 32/64 loss: 0.243108868598938
Batch 33/64 loss: 0.23918581008911133
Batch 34/64 loss: 0.23899942636489868
Batch 35/64 loss: 0.2539978623390198
Batch 36/64 loss: 0.24518489837646484
Batch 37/64 loss: 0.2533921003341675
Batch 38/64 loss: 0.24740904569625854
Batch 39/64 loss: 0.24885088205337524
Batch 40/64 loss: 0.25142544507980347
Batch 41/64 loss: 0.259951651096344
Batch 42/64 loss: 0.24816948175430298
Batch 43/64 loss: 0.2358461618423462
Batch 44/64 loss: 0.2501336932182312
Batch 45/64 loss: 0.25196778774261475
Batch 46/64 loss: 0.23696887493133545
Batch 47/64 loss: 0.24932992458343506
Batch 48/64 loss: 0.2436509132385254
Batch 49/64 loss: 0.23641014099121094
Batch 50/64 loss: 0.24847698211669922
Batch 51/64 loss: 0.2515801191329956
Batch 52/64 loss: 0.23998093605041504
Batch 53/64 loss: 0.238594651222229
Batch 54/64 loss: 0.24994570016860962
Batch 55/64 loss: 0.2426479458808899
Batch 56/64 loss: 0.24849247932434082
Batch 57/64 loss: 0.23796731233596802
Batch 58/64 loss: 0.24726206064224243
Batch 59/64 loss: 0.24937093257904053
Batch 60/64 loss: 0.24744492769241333
Batch 61/64 loss: 0.24272418022155762
Batch 62/64 loss: 0.241837739944458
Batch 63/64 loss: 0.23671960830688477
Batch 64/64 loss: 0.255012571811676
Epoch 32  Train loss: 0.24563148699554743  Val loss: 0.25448526365240826
Saving best model, epoch: 32
Epoch 33
-------------------------------
Batch 1/64 loss: 0.24223601818084717
Batch 2/64 loss: 0.24680542945861816
Batch 3/64 loss: 0.23606455326080322
Batch 4/64 loss: 0.2521054148674011
Batch 5/64 loss: 0.2413853406906128
Batch 6/64 loss: 0.24588274955749512
Batch 7/64 loss: 0.24460721015930176
Batch 8/64 loss: 0.2391834259033203
Batch 9/64 loss: 0.23755741119384766
Batch 10/64 loss: 0.2462538480758667
Batch 11/64 loss: 0.24367719888687134
Batch 12/64 loss: 0.24544626474380493
Batch 13/64 loss: 0.24890202283859253
Batch 14/64 loss: 0.24499034881591797
Batch 15/64 loss: 0.24266260862350464
Batch 16/64 loss: 0.24817711114883423
Batch 17/64 loss: 0.24596911668777466
Batch 18/64 loss: 0.24783581495285034
Batch 19/64 loss: 0.2532324194908142
Batch 20/64 loss: 0.24527692794799805
Batch 21/64 loss: 0.2489720582962036
Batch 22/64 loss: 0.24627912044525146
Batch 23/64 loss: 0.24364590644836426
Batch 24/64 loss: 0.23402827978134155
Batch 25/64 loss: 0.24817347526550293
Batch 26/64 loss: 0.24132871627807617
Batch 27/64 loss: 0.24737977981567383
Batch 28/64 loss: 0.24350041151046753
Batch 29/64 loss: 0.24459433555603027
Batch 30/64 loss: 0.23685330152511597
Batch 31/64 loss: 0.24782896041870117
Batch 32/64 loss: 0.24885165691375732
Batch 33/64 loss: 0.24737763404846191
Batch 34/64 loss: 0.24593055248260498
Batch 35/64 loss: 0.24454426765441895
Batch 36/64 loss: 0.23896121978759766
Batch 37/64 loss: 0.23205715417861938
Batch 38/64 loss: 0.23824214935302734
Batch 39/64 loss: 0.24728763103485107
Batch 40/64 loss: 0.2493060827255249
Batch 41/64 loss: 0.24535274505615234
Batch 42/64 loss: 0.2389141321182251
Batch 43/64 loss: 0.24631744623184204
Batch 44/64 loss: 0.2368561029434204
Batch 45/64 loss: 0.24546033143997192
Batch 46/64 loss: 0.23767435550689697
Batch 47/64 loss: 0.24945026636123657
Batch 48/64 loss: 0.2402186393737793
Batch 49/64 loss: 0.24152857065200806
Batch 50/64 loss: 0.2299061417579651
Batch 51/64 loss: 0.24531090259552002
Batch 52/64 loss: 0.24249768257141113
Batch 53/64 loss: 0.2568255066871643
Batch 54/64 loss: 0.2471315860748291
Batch 55/64 loss: 0.24183642864227295
Batch 56/64 loss: 0.24301522970199585
Batch 57/64 loss: 0.23869001865386963
Batch 58/64 loss: 0.22891336679458618
Batch 59/64 loss: 0.2383434772491455
Batch 60/64 loss: 0.24254965782165527
Batch 61/64 loss: 0.2370622158050537
Batch 62/64 loss: 0.23737263679504395
Batch 63/64 loss: 0.24385130405426025
Batch 64/64 loss: 0.2528309226036072
Epoch 33  Train loss: 0.24342111162110872  Val loss: 0.25203133202910016
Saving best model, epoch: 33
Epoch 34
-------------------------------
Batch 1/64 loss: 0.24670517444610596
Batch 2/64 loss: 0.2344425916671753
Batch 3/64 loss: 0.23889410495758057
Batch 4/64 loss: 0.2546083927154541
Batch 5/64 loss: 0.24661779403686523
Batch 6/64 loss: 0.2291170358657837
Batch 7/64 loss: 0.2472507357597351
Batch 8/64 loss: 0.2447444200515747
Batch 9/64 loss: 0.24625909328460693
Batch 10/64 loss: 0.24291443824768066
Batch 11/64 loss: 0.2347714900970459
Batch 12/64 loss: 0.24195396900177002
Batch 13/64 loss: 0.24216127395629883
Batch 14/64 loss: 0.23924946784973145
Batch 15/64 loss: 0.24181586503982544
Batch 16/64 loss: 0.24498701095581055
Batch 17/64 loss: 0.2328486442565918
Batch 18/64 loss: 0.23775815963745117
Batch 19/64 loss: 0.23265445232391357
Batch 20/64 loss: 0.2472686767578125
Batch 21/64 loss: 0.24305248260498047
Batch 22/64 loss: 0.233262836933136
Batch 23/64 loss: 0.23019778728485107
Batch 24/64 loss: 0.23548924922943115
Batch 25/64 loss: 0.2334778904914856
Batch 26/64 loss: 0.24089717864990234
Batch 27/64 loss: 0.23939162492752075
Batch 28/64 loss: 0.2411109209060669
Batch 29/64 loss: 0.23758697509765625
Batch 30/64 loss: 0.24067968130111694
Batch 31/64 loss: 0.23298108577728271
Batch 32/64 loss: 0.24004513025283813
Batch 33/64 loss: 0.2533451318740845
Batch 34/64 loss: 0.23584306240081787
Batch 35/64 loss: 0.23228955268859863
Batch 36/64 loss: 0.22694659233093262
Batch 37/64 loss: 0.23615717887878418
Batch 38/64 loss: 0.242453932762146
Batch 39/64 loss: 0.23242181539535522
Batch 40/64 loss: 0.24232876300811768
Batch 41/64 loss: 0.2361307144165039
Batch 42/64 loss: 0.23176205158233643
Batch 43/64 loss: 0.23991751670837402
Batch 44/64 loss: 0.24210882186889648
Batch 45/64 loss: 0.2320348024368286
Batch 46/64 loss: 0.2422780990600586
Batch 47/64 loss: 0.2335023283958435
Batch 48/64 loss: 0.24047088623046875
Batch 49/64 loss: 0.2549785375595093
Batch 50/64 loss: 0.24405592679977417
Batch 51/64 loss: 0.24867033958435059
Batch 52/64 loss: 0.23003816604614258
Batch 53/64 loss: 0.2364412546157837
Batch 54/64 loss: 0.24722027778625488
Batch 55/64 loss: 0.23684155941009521
Batch 56/64 loss: 0.2362040877342224
Batch 57/64 loss: 0.24177521467208862
Batch 58/64 loss: 0.2541412115097046
Batch 59/64 loss: 0.24024999141693115
Batch 60/64 loss: 0.23125946521759033
Batch 61/64 loss: 0.240412175655365
Batch 62/64 loss: 0.24330341815948486
Batch 63/64 loss: 0.23264896869659424
Batch 64/64 loss: 0.2348729968070984
Epoch 34  Train loss: 0.23952285846074423  Val loss: 0.24930977739419313
Saving best model, epoch: 34
Epoch 35
-------------------------------
Batch 1/64 loss: 0.2351720929145813
Batch 2/64 loss: 0.24141323566436768
Batch 3/64 loss: 0.2363823652267456
Batch 4/64 loss: 0.23411929607391357
Batch 5/64 loss: 0.2450990080833435
Batch 6/64 loss: 0.2294551134109497
Batch 7/64 loss: 0.23417538404464722
Batch 8/64 loss: 0.23410135507583618
Batch 9/64 loss: 0.23837929964065552
Batch 10/64 loss: 0.24434643983840942
Batch 11/64 loss: 0.23874062299728394
Batch 12/64 loss: 0.23703491687774658
Batch 13/64 loss: 0.24648499488830566
Batch 14/64 loss: 0.24147111177444458
Batch 15/64 loss: 0.23714375495910645
Batch 16/64 loss: 0.23858952522277832
Batch 17/64 loss: 0.2305506467819214
Batch 18/64 loss: 0.23524802923202515
Batch 19/64 loss: 0.23304831981658936
Batch 20/64 loss: 0.23074662685394287
Batch 21/64 loss: 0.23494184017181396
Batch 22/64 loss: 0.23976564407348633
Batch 23/64 loss: 0.23519492149353027
Batch 24/64 loss: 0.23998641967773438
Batch 25/64 loss: 0.2325814962387085
Batch 26/64 loss: 0.23895275592803955
Batch 27/64 loss: 0.2286667823791504
Batch 28/64 loss: 0.23173069953918457
Batch 29/64 loss: 0.23449605703353882
Batch 30/64 loss: 0.24081206321716309
Batch 31/64 loss: 0.2513957619667053
Batch 32/64 loss: 0.23980367183685303
Batch 33/64 loss: 0.23593300580978394
Batch 34/64 loss: 0.24951457977294922
Batch 35/64 loss: 0.23445117473602295
Batch 36/64 loss: 0.22987860441207886
Batch 37/64 loss: 0.24771356582641602
Batch 38/64 loss: 0.22793638706207275
Batch 39/64 loss: 0.22678512334823608
Batch 40/64 loss: 0.23530066013336182
Batch 41/64 loss: 0.23567456007003784
Batch 42/64 loss: 0.24200016260147095
Batch 43/64 loss: 0.23848140239715576
Batch 44/64 loss: 0.24125444889068604
Batch 45/64 loss: 0.23448526859283447
Batch 46/64 loss: 0.22256016731262207
Batch 47/64 loss: 0.2532460689544678
Batch 48/64 loss: 0.23106765747070312
Batch 49/64 loss: 0.22845858335494995
Batch 50/64 loss: 0.23556023836135864
Batch 51/64 loss: 0.24112898111343384
Batch 52/64 loss: 0.22163152694702148
Batch 53/64 loss: 0.2288152575492859
Batch 54/64 loss: 0.23466455936431885
Batch 55/64 loss: 0.22633469104766846
Batch 56/64 loss: 0.22977763414382935
Batch 57/64 loss: 0.23352372646331787
Batch 58/64 loss: 0.2292248010635376
Batch 59/64 loss: 0.2494257092475891
Batch 60/64 loss: 0.2380872368812561
Batch 61/64 loss: 0.2364882230758667
Batch 62/64 loss: 0.23107939958572388
Batch 63/64 loss: 0.2395075559616089
Batch 64/64 loss: 0.22767603397369385
Epoch 35  Train loss: 0.2360906390582814  Val loss: 0.24873080982785045
Saving best model, epoch: 35
Epoch 36
-------------------------------
Batch 1/64 loss: 0.23342269659042358
Batch 2/64 loss: 0.23378121852874756
Batch 3/64 loss: 0.23266905546188354
Batch 4/64 loss: 0.23233211040496826
Batch 5/64 loss: 0.2292405366897583
Batch 6/64 loss: 0.23569273948669434
Batch 7/64 loss: 0.23298025131225586
Batch 8/64 loss: 0.24742496013641357
Batch 9/64 loss: 0.23806989192962646
Batch 10/64 loss: 0.24027103185653687
Batch 11/64 loss: 0.23972976207733154
Batch 12/64 loss: 0.22657722234725952
Batch 13/64 loss: 0.23345673084259033
Batch 14/64 loss: 0.23993313312530518
Batch 15/64 loss: 0.23811805248260498
Batch 16/64 loss: 0.23346054553985596
Batch 17/64 loss: 0.22858619689941406
Batch 18/64 loss: 0.22335922718048096
Batch 19/64 loss: 0.2322688102722168
Batch 20/64 loss: 0.23523753881454468
Batch 21/64 loss: 0.23328924179077148
Batch 22/64 loss: 0.24392473697662354
Batch 23/64 loss: 0.22639435529708862
Batch 24/64 loss: 0.23125320672988892
Batch 25/64 loss: 0.23775720596313477
Batch 26/64 loss: 0.2388368844985962
Batch 27/64 loss: 0.2395835518836975
Batch 28/64 loss: 0.2228173017501831
Batch 29/64 loss: 0.23116779327392578
Batch 30/64 loss: 0.24013280868530273
Batch 31/64 loss: 0.22356963157653809
Batch 32/64 loss: 0.2496166229248047
Batch 33/64 loss: 0.2269301414489746
Batch 34/64 loss: 0.23272550106048584
Batch 35/64 loss: 0.23054873943328857
Batch 36/64 loss: 0.23415225744247437
Batch 37/64 loss: 0.22333943843841553
Batch 38/64 loss: 0.24161267280578613
Batch 39/64 loss: 0.23321425914764404
Batch 40/64 loss: 0.22973692417144775
Batch 41/64 loss: 0.2240924835205078
Batch 42/64 loss: 0.2247544527053833
Batch 43/64 loss: 0.2216101884841919
Batch 44/64 loss: 0.23403632640838623
Batch 45/64 loss: 0.24030089378356934
Batch 46/64 loss: 0.24066555500030518
Batch 47/64 loss: 0.23766767978668213
Batch 48/64 loss: 0.24023163318634033
Batch 49/64 loss: 0.2450941801071167
Batch 50/64 loss: 0.23749256134033203
Batch 51/64 loss: 0.23783129453659058
Batch 52/64 loss: 0.2503966689109802
Batch 53/64 loss: 0.22586286067962646
Batch 54/64 loss: 0.23299986124038696
Batch 55/64 loss: 0.23434853553771973
Batch 56/64 loss: 0.23159819841384888
Batch 57/64 loss: 0.2226288914680481
Batch 58/64 loss: 0.22197318077087402
Batch 59/64 loss: 0.23738479614257812
Batch 60/64 loss: 0.21726351976394653
Batch 61/64 loss: 0.2328500747680664
Batch 62/64 loss: 0.23645812273025513
Batch 63/64 loss: 0.23311978578567505
Batch 64/64 loss: 0.228346586227417
Epoch 36  Train loss: 0.23355508505129347  Val loss: 0.24166074835557708
Saving best model, epoch: 36
Epoch 37
-------------------------------
Batch 1/64 loss: 0.23406606912612915
Batch 2/64 loss: 0.23007488250732422
Batch 3/64 loss: 0.22757810354232788
Batch 4/64 loss: 0.22504109144210815
Batch 5/64 loss: 0.22769665718078613
Batch 6/64 loss: 0.2184523344039917
Batch 7/64 loss: 0.22418874502182007
Batch 8/64 loss: 0.22250747680664062
Batch 9/64 loss: 0.23724472522735596
Batch 10/64 loss: 0.21951782703399658
Batch 11/64 loss: 0.23625928163528442
Batch 12/64 loss: 0.22873246669769287
Batch 13/64 loss: 0.2451028823852539
Batch 14/64 loss: 0.2267671823501587
Batch 15/64 loss: 0.2375185489654541
Batch 16/64 loss: 0.2328682541847229
Batch 17/64 loss: 0.23181414604187012
Batch 18/64 loss: 0.23216819763183594
Batch 19/64 loss: 0.22362589836120605
Batch 20/64 loss: 0.23612385988235474
Batch 21/64 loss: 0.22233128547668457
Batch 22/64 loss: 0.23408794403076172
Batch 23/64 loss: 0.2432694435119629
Batch 24/64 loss: 0.23976850509643555
Batch 25/64 loss: 0.22220462560653687
Batch 26/64 loss: 0.22140955924987793
Batch 27/64 loss: 0.23310130834579468
Batch 28/64 loss: 0.24222004413604736
Batch 29/64 loss: 0.23361480236053467
Batch 30/64 loss: 0.23390185832977295
Batch 31/64 loss: 0.2217845320701599
Batch 32/64 loss: 0.22645896673202515
Batch 33/64 loss: 0.22433209419250488
Batch 34/64 loss: 0.2306753396987915
Batch 35/64 loss: 0.22919106483459473
Batch 36/64 loss: 0.2278057336807251
Batch 37/64 loss: 0.23543238639831543
Batch 38/64 loss: 0.22297674417495728
Batch 39/64 loss: 0.24134397506713867
Batch 40/64 loss: 0.23014187812805176
Batch 41/64 loss: 0.22949159145355225
Batch 42/64 loss: 0.22888851165771484
Batch 43/64 loss: 0.23081213235855103
Batch 44/64 loss: 0.24522173404693604
Batch 45/64 loss: 0.2311701774597168
Batch 46/64 loss: 0.22707229852676392
Batch 47/64 loss: 0.22812682390213013
Batch 48/64 loss: 0.24261236190795898
Batch 49/64 loss: 0.23288869857788086
Batch 50/64 loss: 0.23531264066696167
Batch 51/64 loss: 0.22598105669021606
Batch 52/64 loss: 0.23576772212982178
Batch 53/64 loss: 0.22337186336517334
Batch 54/64 loss: 0.23609447479248047
Batch 55/64 loss: 0.23080384731292725
Batch 56/64 loss: 0.231886088848114
Batch 57/64 loss: 0.2365124225616455
Batch 58/64 loss: 0.23501968383789062
Batch 59/64 loss: 0.22936654090881348
Batch 60/64 loss: 0.2179703712463379
Batch 61/64 loss: 0.22851622104644775
Batch 62/64 loss: 0.23464035987854004
Batch 63/64 loss: 0.21539688110351562
Batch 64/64 loss: 0.22928404808044434
Epoch 37  Train loss: 0.23056138450024175  Val loss: 0.2407017073680445
Saving best model, epoch: 37
Epoch 38
-------------------------------
Batch 1/64 loss: 0.2314136028289795
Batch 2/64 loss: 0.22723275423049927
Batch 3/64 loss: 0.23787766695022583
Batch 4/64 loss: 0.22634893655776978
Batch 5/64 loss: 0.22473371028900146
Batch 6/64 loss: 0.22289955615997314
Batch 7/64 loss: 0.22618985176086426
Batch 8/64 loss: 0.2275276780128479
Batch 9/64 loss: 0.2366727590560913
Batch 10/64 loss: 0.23278087377548218
Batch 11/64 loss: 0.22768259048461914
Batch 12/64 loss: 0.22759610414505005
Batch 13/64 loss: 0.23101437091827393
Batch 14/64 loss: 0.21625220775604248
Batch 15/64 loss: 0.2223641276359558
Batch 16/64 loss: 0.23961246013641357
Batch 17/64 loss: 0.23442625999450684
Batch 18/64 loss: 0.23134386539459229
Batch 19/64 loss: 0.22628623247146606
Batch 20/64 loss: 0.23388493061065674
Batch 21/64 loss: 0.23212170600891113
Batch 22/64 loss: 0.22798240184783936
Batch 23/64 loss: 0.22449922561645508
Batch 24/64 loss: 0.2346402406692505
Batch 25/64 loss: 0.22046256065368652
Batch 26/64 loss: 0.2350832223892212
Batch 27/64 loss: 0.2198643684387207
Batch 28/64 loss: 0.2291727066040039
Batch 29/64 loss: 0.21657538414001465
Batch 30/64 loss: 0.22852301597595215
Batch 31/64 loss: 0.24103474617004395
Batch 32/64 loss: 0.21473383903503418
Batch 33/64 loss: 0.22096562385559082
Batch 34/64 loss: 0.22763067483901978
Batch 35/64 loss: 0.22504746913909912
Batch 36/64 loss: 0.22892361879348755
Batch 37/64 loss: 0.23161184787750244
Batch 38/64 loss: 0.22818565368652344
Batch 39/64 loss: 0.23890310525894165
Batch 40/64 loss: 0.23760497570037842
Batch 41/64 loss: 0.2333030104637146
Batch 42/64 loss: 0.24003833532333374
Batch 43/64 loss: 0.23219150304794312
Batch 44/64 loss: 0.224473237991333
Batch 45/64 loss: 0.21858274936676025
Batch 46/64 loss: 0.2271789312362671
Batch 47/64 loss: 0.22696107625961304
Batch 48/64 loss: 0.22870802879333496
Batch 49/64 loss: 0.2252953052520752
Batch 50/64 loss: 0.23081588745117188
Batch 51/64 loss: 0.22211319208145142
Batch 52/64 loss: 0.21549224853515625
Batch 53/64 loss: 0.2167242169380188
Batch 54/64 loss: 0.22253721952438354
Batch 55/64 loss: 0.22349488735198975
Batch 56/64 loss: 0.22445392608642578
Batch 57/64 loss: 0.22275882959365845
Batch 58/64 loss: 0.22805970907211304
Batch 59/64 loss: 0.2322896122932434
Batch 60/64 loss: 0.22634226083755493
Batch 61/64 loss: 0.22505128383636475
Batch 62/64 loss: 0.24190747737884521
Batch 63/64 loss: 0.21869111061096191
Batch 64/64 loss: 0.2282097339630127
Epoch 38  Train loss: 0.22786397279477588  Val loss: 0.23984568840039963
Saving best model, epoch: 38
Epoch 39
-------------------------------
Batch 1/64 loss: 0.22477656602859497
Batch 2/64 loss: 0.23508059978485107
Batch 3/64 loss: 0.2107938528060913
Batch 4/64 loss: 0.22091412544250488
Batch 5/64 loss: 0.2273087501525879
Batch 6/64 loss: 0.22409534454345703
Batch 7/64 loss: 0.20531052350997925
Batch 8/64 loss: 0.22677528858184814
Batch 9/64 loss: 0.2330470085144043
Batch 10/64 loss: 0.2180943489074707
Batch 11/64 loss: 0.22740983963012695
Batch 12/64 loss: 0.22872602939605713
Batch 13/64 loss: 0.21875810623168945
Batch 14/64 loss: 0.2190713882446289
Batch 15/64 loss: 0.22029876708984375
Batch 16/64 loss: 0.22536176443099976
Batch 17/64 loss: 0.2253168821334839
Batch 18/64 loss: 0.22199499607086182
Batch 19/64 loss: 0.2295650839805603
Batch 20/64 loss: 0.22980034351348877
Batch 21/64 loss: 0.22894179821014404
Batch 22/64 loss: 0.2243279218673706
Batch 23/64 loss: 0.23683148622512817
Batch 24/64 loss: 0.21484410762786865
Batch 25/64 loss: 0.22530066967010498
Batch 26/64 loss: 0.21732985973358154
Batch 27/64 loss: 0.21651685237884521
Batch 28/64 loss: 0.22656965255737305
Batch 29/64 loss: 0.2274472713470459
Batch 30/64 loss: 0.219679594039917
Batch 31/64 loss: 0.24392783641815186
Batch 32/64 loss: 0.22669970989227295
Batch 33/64 loss: 0.21814405918121338
Batch 34/64 loss: 0.23282015323638916
Batch 35/64 loss: 0.22592943906784058
Batch 36/64 loss: 0.22105717658996582
Batch 37/64 loss: 0.2235255241394043
Batch 38/64 loss: 0.23078584671020508
Batch 39/64 loss: 0.22319072484970093
Batch 40/64 loss: 0.2341964840888977
Batch 41/64 loss: 0.22419780492782593
Batch 42/64 loss: 0.228643536567688
Batch 43/64 loss: 0.2109203338623047
Batch 44/64 loss: 0.21337759494781494
Batch 45/64 loss: 0.2307887077331543
Batch 46/64 loss: 0.2117992639541626
Batch 47/64 loss: 0.22843658924102783
Batch 48/64 loss: 0.23569214344024658
Batch 49/64 loss: 0.2405974268913269
Batch 50/64 loss: 0.22560656070709229
Batch 51/64 loss: 0.23184406757354736
Batch 52/64 loss: 0.22129523754119873
Batch 53/64 loss: 0.2262234091758728
Batch 54/64 loss: 0.22817444801330566
Batch 55/64 loss: 0.21917611360549927
Batch 56/64 loss: 0.22357821464538574
Batch 57/64 loss: 0.2301884889602661
Batch 58/64 loss: 0.2131636142730713
Batch 59/64 loss: 0.22501134872436523
Batch 60/64 loss: 0.22218704223632812
Batch 61/64 loss: 0.22114241123199463
Batch 62/64 loss: 0.21621352434158325
Batch 63/64 loss: 0.2315184473991394
Batch 64/64 loss: 0.22046935558319092
Epoch 39  Train loss: 0.22455990781971052  Val loss: 0.23361528821007901
Saving best model, epoch: 39
Epoch 40
-------------------------------
Batch 1/64 loss: 0.22702807188034058
Batch 2/64 loss: 0.21898198127746582
Batch 3/64 loss: 0.230310320854187
Batch 4/64 loss: 0.23186904191970825
Batch 5/64 loss: 0.21414971351623535
Batch 6/64 loss: 0.2289043664932251
Batch 7/64 loss: 0.22685664892196655
Batch 8/64 loss: 0.2221928834915161
Batch 9/64 loss: 0.22550982236862183
Batch 10/64 loss: 0.22944295406341553
Batch 11/64 loss: 0.22244101762771606
Batch 12/64 loss: 0.22543638944625854
Batch 13/64 loss: 0.22856420278549194
Batch 14/64 loss: 0.2281399965286255
Batch 15/64 loss: 0.22501206398010254
Batch 16/64 loss: 0.22627675533294678
Batch 17/64 loss: 0.22204720973968506
Batch 18/64 loss: 0.22332990169525146
Batch 19/64 loss: 0.21444064378738403
Batch 20/64 loss: 0.23141974210739136
Batch 21/64 loss: 0.22327333688735962
Batch 22/64 loss: 0.21700674295425415
Batch 23/64 loss: 0.22651636600494385
Batch 24/64 loss: 0.22156047821044922
Batch 25/64 loss: 0.22920042276382446
Batch 26/64 loss: 0.2192678451538086
Batch 27/64 loss: 0.2169068455696106
Batch 28/64 loss: 0.22854357957839966
Batch 29/64 loss: 0.21559011936187744
Batch 30/64 loss: 0.22432738542556763
Batch 31/64 loss: 0.2215193510055542
Batch 32/64 loss: 0.216056227684021
Batch 33/64 loss: 0.22341251373291016
Batch 34/64 loss: 0.2215512990951538
Batch 35/64 loss: 0.21739274263381958
Batch 36/64 loss: 0.22218692302703857
Batch 37/64 loss: 0.22108721733093262
Batch 38/64 loss: 0.2227407693862915
Batch 39/64 loss: 0.2175990343093872
Batch 40/64 loss: 0.21640753746032715
Batch 41/64 loss: 0.22200405597686768
Batch 42/64 loss: 0.22121703624725342
Batch 43/64 loss: 0.22712165117263794
Batch 44/64 loss: 0.22693967819213867
Batch 45/64 loss: 0.22937870025634766
Batch 46/64 loss: 0.229081392288208
Batch 47/64 loss: 0.21078699827194214
Batch 48/64 loss: 0.22157937288284302
Batch 49/64 loss: 0.2259582281112671
Batch 50/64 loss: 0.22938764095306396
Batch 51/64 loss: 0.21901977062225342
Batch 52/64 loss: 0.23463809490203857
Batch 53/64 loss: 0.21348261833190918
Batch 54/64 loss: 0.23479944467544556
Batch 55/64 loss: 0.21940255165100098
Batch 56/64 loss: 0.22856497764587402
Batch 57/64 loss: 0.22855281829833984
Batch 58/64 loss: 0.23142647743225098
Batch 59/64 loss: 0.2139924168586731
Batch 60/64 loss: 0.21812117099761963
Batch 61/64 loss: 0.21807348728179932
Batch 62/64 loss: 0.2113547921180725
Batch 63/64 loss: 0.21916735172271729
Batch 64/64 loss: 0.2314792275428772
Epoch 40  Train loss: 0.22324957824220845  Val loss: 0.23678639582342298
Epoch 41
-------------------------------
Batch 1/64 loss: 0.22064179182052612
Batch 2/64 loss: 0.23914885520935059
Batch 3/64 loss: 0.2227497100830078
Batch 4/64 loss: 0.21851599216461182
Batch 5/64 loss: 0.2149006724357605
Batch 6/64 loss: 0.2207239866256714
Batch 7/64 loss: 0.2218349575996399
Batch 8/64 loss: 0.22668015956878662
Batch 9/64 loss: 0.2199711799621582
Batch 10/64 loss: 0.2194594144821167
Batch 11/64 loss: 0.235845685005188
Batch 12/64 loss: 0.2283003330230713
Batch 13/64 loss: 0.22211575508117676
Batch 14/64 loss: 0.21925556659698486
Batch 15/64 loss: 0.21557772159576416
Batch 16/64 loss: 0.22608673572540283
Batch 17/64 loss: 0.22095084190368652
Batch 18/64 loss: 0.22857415676116943
Batch 19/64 loss: 0.223088800907135
Batch 20/64 loss: 0.22367936372756958
Batch 21/64 loss: 0.21281325817108154
Batch 22/64 loss: 0.21984630823135376
Batch 23/64 loss: 0.211683452129364
Batch 24/64 loss: 0.21692579984664917
Batch 25/64 loss: 0.22418510913848877
Batch 26/64 loss: 0.21749979257583618
Batch 27/64 loss: 0.23250627517700195
Batch 28/64 loss: 0.22314822673797607
Batch 29/64 loss: 0.21331119537353516
Batch 30/64 loss: 0.20505613088607788
Batch 31/64 loss: 0.21370863914489746
Batch 32/64 loss: 0.21789300441741943
Batch 33/64 loss: 0.21399885416030884
Batch 34/64 loss: 0.20777374505996704
Batch 35/64 loss: 0.21145951747894287
Batch 36/64 loss: 0.21242743730545044
Batch 37/64 loss: 0.2140134572982788
Batch 38/64 loss: 0.2210768461227417
Batch 39/64 loss: 0.22840648889541626
Batch 40/64 loss: 0.2038598656654358
Batch 41/64 loss: 0.2081214189529419
Batch 42/64 loss: 0.2170141339302063
Batch 43/64 loss: 0.22098761796951294
Batch 44/64 loss: 0.2302863597869873
Batch 45/64 loss: 0.2155919075012207
Batch 46/64 loss: 0.22295290231704712
Batch 47/64 loss: 0.20766669511795044
Batch 48/64 loss: 0.2186516523361206
Batch 49/64 loss: 0.20993071794509888
Batch 50/64 loss: 0.22262543439865112
Batch 51/64 loss: 0.21812039613723755
Batch 52/64 loss: 0.22180438041687012
Batch 53/64 loss: 0.22092515230178833
Batch 54/64 loss: 0.20859861373901367
Batch 55/64 loss: 0.21898221969604492
Batch 56/64 loss: 0.2158045768737793
Batch 57/64 loss: 0.2246595025062561
Batch 58/64 loss: 0.22863495349884033
Batch 59/64 loss: 0.22038960456848145
Batch 60/64 loss: 0.21657902002334595
Batch 61/64 loss: 0.2202824354171753
Batch 62/64 loss: 0.22006064653396606
Batch 63/64 loss: 0.2116398811340332
Batch 64/64 loss: 0.21831655502319336
Epoch 41  Train loss: 0.21919596428964652  Val loss: 0.22563314130625775
Saving best model, epoch: 41
Epoch 42
-------------------------------
Batch 1/64 loss: 0.21233415603637695
Batch 2/64 loss: 0.2149452567100525
Batch 3/64 loss: 0.21834039688110352
Batch 4/64 loss: 0.23008239269256592
Batch 5/64 loss: 0.21974515914916992
Batch 6/64 loss: 0.2173939347267151
Batch 7/64 loss: 0.22087311744689941
Batch 8/64 loss: 0.21271395683288574
Batch 9/64 loss: 0.22096216678619385
Batch 10/64 loss: 0.21330863237380981
Batch 11/64 loss: 0.21465706825256348
Batch 12/64 loss: 0.21954047679901123
Batch 13/64 loss: 0.2188408374786377
Batch 14/64 loss: 0.21887874603271484
Batch 15/64 loss: 0.22343754768371582
Batch 16/64 loss: 0.21445846557617188
Batch 17/64 loss: 0.22515815496444702
Batch 18/64 loss: 0.21779179573059082
Batch 19/64 loss: 0.22290319204330444
Batch 20/64 loss: 0.22981226444244385
Batch 21/64 loss: 0.21924662590026855
Batch 22/64 loss: 0.21531105041503906
Batch 23/64 loss: 0.22033387422561646
Batch 24/64 loss: 0.21205216646194458
Batch 25/64 loss: 0.2188345193862915
Batch 26/64 loss: 0.22183680534362793
Batch 27/64 loss: 0.21538448333740234
Batch 28/64 loss: 0.21578097343444824
Batch 29/64 loss: 0.22644388675689697
Batch 30/64 loss: 0.20946359634399414
Batch 31/64 loss: 0.2131558656692505
Batch 32/64 loss: 0.2082405686378479
Batch 33/64 loss: 0.2275923490524292
Batch 34/64 loss: 0.2088257074356079
Batch 35/64 loss: 0.22460079193115234
Batch 36/64 loss: 0.210796058177948
Batch 37/64 loss: 0.21953904628753662
Batch 38/64 loss: 0.23186445236206055
Batch 39/64 loss: 0.2160407304763794
Batch 40/64 loss: 0.22186875343322754
Batch 41/64 loss: 0.209547221660614
Batch 42/64 loss: 0.21049928665161133
Batch 43/64 loss: 0.2167224884033203
Batch 44/64 loss: 0.22001934051513672
Batch 45/64 loss: 0.22247135639190674
Batch 46/64 loss: 0.21610307693481445
Batch 47/64 loss: 0.21760082244873047
Batch 48/64 loss: 0.21350538730621338
Batch 49/64 loss: 0.22375166416168213
Batch 50/64 loss: 0.21013468503952026
Batch 51/64 loss: 0.2227877378463745
Batch 52/64 loss: 0.22300606966018677
Batch 53/64 loss: 0.22246664762496948
Batch 54/64 loss: 0.21248513460159302
Batch 55/64 loss: 0.2163228988647461
Batch 56/64 loss: 0.22991418838500977
Batch 57/64 loss: 0.2155367136001587
Batch 58/64 loss: 0.2192198634147644
Batch 59/64 loss: 0.20682066679000854
Batch 60/64 loss: 0.2108810544013977
Batch 61/64 loss: 0.2113645076751709
Batch 62/64 loss: 0.20794403553009033
Batch 63/64 loss: 0.2152422070503235
Batch 64/64 loss: 0.2130315899848938
Epoch 42  Train loss: 0.21768644346910365  Val loss: 0.21930479798529975
Saving best model, epoch: 42
Epoch 43
-------------------------------
Batch 1/64 loss: 0.20825517177581787
Batch 2/64 loss: 0.20999252796173096
Batch 3/64 loss: 0.21317851543426514
Batch 4/64 loss: 0.2183072566986084
Batch 5/64 loss: 0.21056044101715088
Batch 6/64 loss: 0.2267472743988037
Batch 7/64 loss: 0.21660631895065308
Batch 8/64 loss: 0.21387934684753418
Batch 9/64 loss: 0.2097078561782837
Batch 10/64 loss: 0.20535945892333984
Batch 11/64 loss: 0.2218378186225891
Batch 12/64 loss: 0.222631573677063
Batch 13/64 loss: 0.21087646484375
Batch 14/64 loss: 0.21640139818191528
Batch 15/64 loss: 0.21779799461364746
Batch 16/64 loss: 0.2109355926513672
Batch 17/64 loss: 0.21592742204666138
Batch 18/64 loss: 0.20993822813034058
Batch 19/64 loss: 0.20825600624084473
Batch 20/64 loss: 0.20956766605377197
Batch 21/64 loss: 0.21619582176208496
Batch 22/64 loss: 0.2145988941192627
Batch 23/64 loss: 0.2137523889541626
Batch 24/64 loss: 0.213070809841156
Batch 25/64 loss: 0.21486037969589233
Batch 26/64 loss: 0.21930336952209473
Batch 27/64 loss: 0.22278302907943726
Batch 28/64 loss: 0.22276759147644043
Batch 29/64 loss: 0.21490716934204102
Batch 30/64 loss: 0.20418477058410645
Batch 31/64 loss: 0.213905930519104
Batch 32/64 loss: 0.21170783042907715
Batch 33/64 loss: 0.2033754587173462
Batch 34/64 loss: 0.20556879043579102
Batch 35/64 loss: 0.21048951148986816
Batch 36/64 loss: 0.21958494186401367
Batch 37/64 loss: 0.2045220136642456
Batch 38/64 loss: 0.2036343812942505
Batch 39/64 loss: 0.20640701055526733
Batch 40/64 loss: 0.2093040943145752
Batch 41/64 loss: 0.20909744501113892
Batch 42/64 loss: 0.2041967511177063
Batch 43/64 loss: 0.2053869366645813
Batch 44/64 loss: 0.2065889835357666
Batch 45/64 loss: 0.20209980010986328
Batch 46/64 loss: 0.20704680681228638
Batch 47/64 loss: 0.20284318923950195
Batch 48/64 loss: 0.22189778089523315
Batch 49/64 loss: 0.2065601348876953
Batch 50/64 loss: 0.21224045753479004
Batch 51/64 loss: 0.20264595746994019
Batch 52/64 loss: 0.22255533933639526
Batch 53/64 loss: 0.21563851833343506
Batch 54/64 loss: 0.22200226783752441
Batch 55/64 loss: 0.2058534026145935
Batch 56/64 loss: 0.21834886074066162
Batch 57/64 loss: 0.20874595642089844
Batch 58/64 loss: 0.21523940563201904
Batch 59/64 loss: 0.21780991554260254
Batch 60/64 loss: 0.21870100498199463
Batch 61/64 loss: 0.21464335918426514
Batch 62/64 loss: 0.22162103652954102
Batch 63/64 loss: 0.21897214651107788
Batch 64/64 loss: 0.21281135082244873
Epoch 43  Train loss: 0.21273776456421498  Val loss: 0.22246529781531632
Epoch 44
-------------------------------
Batch 1/64 loss: 0.22007238864898682
Batch 2/64 loss: 0.21638274192810059
Batch 3/64 loss: 0.20736050605773926
Batch 4/64 loss: 0.21416974067687988
Batch 5/64 loss: 0.2140793800354004
Batch 6/64 loss: 0.20445376634597778
Batch 7/64 loss: 0.2024911642074585
Batch 8/64 loss: 0.2191256284713745
Batch 9/64 loss: 0.21273815631866455
Batch 10/64 loss: 0.21853232383728027
Batch 11/64 loss: 0.20951604843139648
Batch 12/64 loss: 0.20466375350952148
Batch 13/64 loss: 0.21039092540740967
Batch 14/64 loss: 0.2110128402709961
Batch 15/64 loss: 0.21643835306167603
Batch 16/64 loss: 0.2154148817062378
Batch 17/64 loss: 0.21695423126220703
Batch 18/64 loss: 0.20811843872070312
Batch 19/64 loss: 0.21963387727737427
Batch 20/64 loss: 0.20789122581481934
Batch 21/64 loss: 0.2171403169631958
Batch 22/64 loss: 0.22314298152923584
Batch 23/64 loss: 0.22685039043426514
Batch 24/64 loss: 0.21159464120864868
Batch 25/64 loss: 0.21066135168075562
Batch 26/64 loss: 0.21019673347473145
Batch 27/64 loss: 0.21579289436340332
Batch 28/64 loss: 0.2043246030807495
Batch 29/64 loss: 0.21547818183898926
Batch 30/64 loss: 0.2116193175315857
Batch 31/64 loss: 0.22781622409820557
Batch 32/64 loss: 0.20498359203338623
Batch 33/64 loss: 0.20407938957214355
Batch 34/64 loss: 0.2066013216972351
Batch 35/64 loss: 0.22318953275680542
Batch 36/64 loss: 0.21435445547103882
Batch 37/64 loss: 0.20929455757141113
Batch 38/64 loss: 0.22018688917160034
Batch 39/64 loss: 0.21725130081176758
Batch 40/64 loss: 0.20052587985992432
Batch 41/64 loss: 0.21175909042358398
Batch 42/64 loss: 0.212044358253479
Batch 43/64 loss: 0.2043432593345642
Batch 44/64 loss: 0.20864510536193848
Batch 45/64 loss: 0.2089982032775879
Batch 46/64 loss: 0.21067702770233154
Batch 47/64 loss: 0.21344292163848877
Batch 48/64 loss: 0.214039146900177
Batch 49/64 loss: 0.20271658897399902
Batch 50/64 loss: 0.20648610591888428
Batch 51/64 loss: 0.21849042177200317
Batch 52/64 loss: 0.21733009815216064
Batch 53/64 loss: 0.21616190671920776
Batch 54/64 loss: 0.2026381492614746
Batch 55/64 loss: 0.19555997848510742
Batch 56/64 loss: 0.21295952796936035
Batch 57/64 loss: 0.2093779444694519
Batch 58/64 loss: 0.20414358377456665
Batch 59/64 loss: 0.22013020515441895
Batch 60/64 loss: 0.21372932195663452
Batch 61/64 loss: 0.20953184366226196
Batch 62/64 loss: 0.21090304851531982
Batch 63/64 loss: 0.2071821093559265
Batch 64/64 loss: 0.20784306526184082
Epoch 44  Train loss: 0.21194819095087986  Val loss: 0.2275220981168583
Epoch 45
-------------------------------
Batch 1/64 loss: 0.2175050973892212
Batch 2/64 loss: 0.20177865028381348
Batch 3/64 loss: 0.21697258949279785
Batch 4/64 loss: 0.2031211256980896
Batch 5/64 loss: 0.22050374746322632
Batch 6/64 loss: 0.2134537696838379
Batch 7/64 loss: 0.2087295651435852
Batch 8/64 loss: 0.21605044603347778
Batch 9/64 loss: 0.19307327270507812
Batch 10/64 loss: 0.20167326927185059
Batch 11/64 loss: 0.2192642092704773
Batch 12/64 loss: 0.22187399864196777
Batch 13/64 loss: 0.21134167909622192
Batch 14/64 loss: 0.2152729034423828
Batch 15/64 loss: 0.2081897258758545
Batch 16/64 loss: 0.19338852167129517
Batch 17/64 loss: 0.20485824346542358
Batch 18/64 loss: 0.20365959405899048
Batch 19/64 loss: 0.20310592651367188
Batch 20/64 loss: 0.19475102424621582
Batch 21/64 loss: 0.1925312876701355
Batch 22/64 loss: 0.21192574501037598
Batch 23/64 loss: 0.203965425491333
Batch 24/64 loss: 0.21181392669677734
Batch 25/64 loss: 0.21363872289657593
Batch 26/64 loss: 0.20751750469207764
Batch 27/64 loss: 0.21772676706314087
Batch 28/64 loss: 0.195198655128479
Batch 29/64 loss: 0.20929217338562012
Batch 30/64 loss: 0.1953486204147339
Batch 31/64 loss: 0.20293426513671875
Batch 32/64 loss: 0.20750856399536133
Batch 33/64 loss: 0.21161353588104248
Batch 34/64 loss: 0.21807515621185303
Batch 35/64 loss: 0.2044091820716858
Batch 36/64 loss: 0.20021498203277588
Batch 37/64 loss: 0.21292227506637573
Batch 38/64 loss: 0.2072790265083313
Batch 39/64 loss: 0.1986013650894165
Batch 40/64 loss: 0.2208382487297058
Batch 41/64 loss: 0.21540814638137817
Batch 42/64 loss: 0.20815837383270264
Batch 43/64 loss: 0.20702719688415527
Batch 44/64 loss: 0.21574193239212036
Batch 45/64 loss: 0.20983552932739258
Batch 46/64 loss: 0.2146638035774231
Batch 47/64 loss: 0.2092348337173462
Batch 48/64 loss: 0.21666258573532104
Batch 49/64 loss: 0.21272271871566772
Batch 50/64 loss: 0.21881455183029175
Batch 51/64 loss: 0.19592887163162231
Batch 52/64 loss: 0.19949769973754883
Batch 53/64 loss: 0.1998952031135559
Batch 54/64 loss: 0.19584941864013672
Batch 55/64 loss: 0.20183676481246948
Batch 56/64 loss: 0.201729416847229
Batch 57/64 loss: 0.20738816261291504
Batch 58/64 loss: 0.20945966243743896
Batch 59/64 loss: 0.21392667293548584
Batch 60/64 loss: 0.20406574010849
Batch 61/64 loss: 0.20083588361740112
Batch 62/64 loss: 0.20551151037216187
Batch 63/64 loss: 0.20801198482513428
Batch 64/64 loss: 0.21687161922454834
Epoch 45  Train loss: 0.20779267713135363  Val loss: 0.2129704456558752
Saving best model, epoch: 45
Epoch 46
-------------------------------
Batch 1/64 loss: 0.2019639015197754
Batch 2/64 loss: 0.20372146368026733
Batch 3/64 loss: 0.18980062007904053
Batch 4/64 loss: 0.1976078748703003
Batch 5/64 loss: 0.19697856903076172
Batch 6/64 loss: 0.2072293758392334
Batch 7/64 loss: 0.21198368072509766
Batch 8/64 loss: 0.21247661113739014
Batch 9/64 loss: 0.21632176637649536
Batch 10/64 loss: 0.2308673858642578
Batch 11/64 loss: 0.20559442043304443
Batch 12/64 loss: 0.20808017253875732
Batch 13/64 loss: 0.20459920167922974
Batch 14/64 loss: 0.20541369915008545
Batch 15/64 loss: 0.20903587341308594
Batch 16/64 loss: 0.19794297218322754
Batch 17/64 loss: 0.2001245617866516
Batch 18/64 loss: 0.19461238384246826
Batch 19/64 loss: 0.219457745552063
Batch 20/64 loss: 0.21798157691955566
Batch 21/64 loss: 0.2104412317276001
Batch 22/64 loss: 0.2189692258834839
Batch 23/64 loss: 0.20752519369125366
Batch 24/64 loss: 0.205865740776062
Batch 25/64 loss: 0.20764553546905518
Batch 26/64 loss: 0.20367646217346191
Batch 27/64 loss: 0.20191240310668945
Batch 28/64 loss: 0.19801247119903564
Batch 29/64 loss: 0.20802652835845947
Batch 30/64 loss: 0.20651185512542725
Batch 31/64 loss: 0.20092415809631348
Batch 32/64 loss: 0.2037489414215088
Batch 33/64 loss: 0.203086256980896
Batch 34/64 loss: 0.2085038423538208
Batch 35/64 loss: 0.20989245176315308
Batch 36/64 loss: 0.21300333738327026
Batch 37/64 loss: 0.2010183334350586
Batch 38/64 loss: 0.20180833339691162
Batch 39/64 loss: 0.21225273609161377
Batch 40/64 loss: 0.206107497215271
Batch 41/64 loss: 0.21186280250549316
Batch 42/64 loss: 0.20289242267608643
Batch 43/64 loss: 0.20650243759155273
Batch 44/64 loss: 0.20982158184051514
Batch 45/64 loss: 0.1928694248199463
Batch 46/64 loss: 0.20535385608673096
Batch 47/64 loss: 0.19765573740005493
Batch 48/64 loss: 0.20125305652618408
Batch 49/64 loss: 0.2039462924003601
Batch 50/64 loss: 0.20294713973999023
Batch 51/64 loss: 0.20386719703674316
Batch 52/64 loss: 0.19930332899093628
Batch 53/64 loss: 0.1984882354736328
Batch 54/64 loss: 0.20148515701293945
Batch 55/64 loss: 0.20455074310302734
Batch 56/64 loss: 0.2039056420326233
Batch 57/64 loss: 0.19711124897003174
Batch 58/64 loss: 0.20716917514801025
Batch 59/64 loss: 0.19606781005859375
Batch 60/64 loss: 0.19468998908996582
Batch 61/64 loss: 0.2131190299987793
Batch 62/64 loss: 0.20226740837097168
Batch 63/64 loss: 0.20374810695648193
Batch 64/64 loss: 0.20449280738830566
Epoch 46  Train loss: 0.20509762857474534  Val loss: 0.21530990870957523
Epoch 47
-------------------------------
Batch 1/64 loss: 0.19370430707931519
Batch 2/64 loss: 0.19811445474624634
Batch 3/64 loss: 0.20896673202514648
Batch 4/64 loss: 0.2084188461303711
Batch 5/64 loss: 0.19517529010772705
Batch 6/64 loss: 0.21520280838012695
Batch 7/64 loss: 0.19878947734832764
Batch 8/64 loss: 0.20793473720550537
Batch 9/64 loss: 0.20153653621673584
Batch 10/64 loss: 0.2018236517906189
Batch 11/64 loss: 0.19813990592956543
Batch 12/64 loss: 0.19948649406433105
Batch 13/64 loss: 0.1973065733909607
Batch 14/64 loss: 0.20966410636901855
Batch 15/64 loss: 0.19052469730377197
Batch 16/64 loss: 0.19519591331481934
Batch 17/64 loss: 0.19268298149108887
Batch 18/64 loss: 0.19985485076904297
Batch 19/64 loss: 0.19761782884597778
Batch 20/64 loss: 0.20280110836029053
Batch 21/64 loss: 0.20268458127975464
Batch 22/64 loss: 0.20152509212493896
Batch 23/64 loss: 0.19255274534225464
Batch 24/64 loss: 0.19521558284759521
Batch 25/64 loss: 0.2005355954170227
Batch 26/64 loss: 0.20526039600372314
Batch 27/64 loss: 0.2001994252204895
Batch 28/64 loss: 0.19823938608169556
Batch 29/64 loss: 0.19258904457092285
Batch 30/64 loss: 0.20455360412597656
Batch 31/64 loss: 0.19571208953857422
Batch 32/64 loss: 0.1920928955078125
Batch 33/64 loss: 0.1952528953552246
Batch 34/64 loss: 0.1923275589942932
Batch 35/64 loss: 0.21399855613708496
Batch 36/64 loss: 0.19008058309555054
Batch 37/64 loss: 0.2030961513519287
Batch 38/64 loss: 0.19969117641448975
Batch 39/64 loss: 0.20755577087402344
Batch 40/64 loss: 0.2078617811203003
Batch 41/64 loss: 0.19659662246704102
Batch 42/64 loss: 0.20285087823867798
Batch 43/64 loss: 0.20764833688735962
Batch 44/64 loss: 0.20589840412139893
Batch 45/64 loss: 0.18743371963500977
Batch 46/64 loss: 0.20696288347244263
Batch 47/64 loss: 0.2006535530090332
Batch 48/64 loss: 0.20701807737350464
Batch 49/64 loss: 0.20550721883773804
Batch 50/64 loss: 0.20418328046798706
Batch 51/64 loss: 0.20294064283370972
Batch 52/64 loss: 0.20281749963760376
Batch 53/64 loss: 0.20959079265594482
Batch 54/64 loss: 0.20021063089370728
Batch 55/64 loss: 0.2035718560218811
Batch 56/64 loss: 0.2083876132965088
Batch 57/64 loss: 0.19778597354888916
Batch 58/64 loss: 0.209089994430542
Batch 59/64 loss: 0.19516021013259888
Batch 60/64 loss: 0.19531452655792236
Batch 61/64 loss: 0.1985255479812622
Batch 62/64 loss: 0.21556496620178223
Batch 63/64 loss: 0.21569228172302246
Batch 64/64 loss: 0.20562338829040527
Epoch 47  Train loss: 0.20137394923789828  Val loss: 0.21666751694433467
Epoch 48
-------------------------------
Batch 1/64 loss: 0.19990551471710205
Batch 2/64 loss: 0.2087031602859497
Batch 3/64 loss: 0.1947181224822998
Batch 4/64 loss: 0.1989954113960266
Batch 5/64 loss: 0.1996750831604004
Batch 6/64 loss: 0.20029902458190918
Batch 7/64 loss: 0.21098828315734863
Batch 8/64 loss: 0.21220111846923828
Batch 9/64 loss: 0.20272552967071533
Batch 10/64 loss: 0.20867526531219482
Batch 11/64 loss: 0.2039879560470581
Batch 12/64 loss: 0.20286595821380615
Batch 13/64 loss: 0.19800472259521484
Batch 14/64 loss: 0.20251846313476562
Batch 15/64 loss: 0.19727152585983276
Batch 16/64 loss: 0.19433796405792236
Batch 17/64 loss: 0.20293140411376953
Batch 18/64 loss: 0.2069130539894104
Batch 19/64 loss: 0.19636565446853638
Batch 20/64 loss: 0.19793343544006348
Batch 21/64 loss: 0.2009490728378296
Batch 22/64 loss: 0.20608091354370117
Batch 23/64 loss: 0.19509553909301758
Batch 24/64 loss: 0.19167983531951904
Batch 25/64 loss: 0.2043370008468628
Batch 26/64 loss: 0.20585882663726807
Batch 27/64 loss: 0.20367151498794556
Batch 28/64 loss: 0.20581459999084473
Batch 29/64 loss: 0.20936715602874756
Batch 30/64 loss: 0.1974368691444397
Batch 31/64 loss: 0.2055983543395996
Batch 32/64 loss: 0.1923733949661255
Batch 33/64 loss: 0.2066354751586914
Batch 34/64 loss: 0.20615416765213013
Batch 35/64 loss: 0.20281720161437988
Batch 36/64 loss: 0.19480431079864502
Batch 37/64 loss: 0.19812333583831787
Batch 38/64 loss: 0.21383023262023926
Batch 39/64 loss: 0.20579403638839722
Batch 40/64 loss: 0.19405829906463623
Batch 41/64 loss: 0.2039615511894226
Batch 42/64 loss: 0.1883859634399414
Batch 43/64 loss: 0.19838368892669678
Batch 44/64 loss: 0.20026826858520508
Batch 45/64 loss: 0.19611328840255737
Batch 46/64 loss: 0.1988723874092102
Batch 47/64 loss: 0.2053561806678772
Batch 48/64 loss: 0.1863926649093628
Batch 49/64 loss: 0.18863928318023682
Batch 50/64 loss: 0.20040762424468994
Batch 51/64 loss: 0.18853890895843506
Batch 52/64 loss: 0.187483549118042
Batch 53/64 loss: 0.19848036766052246
Batch 54/64 loss: 0.2206888198852539
Batch 55/64 loss: 0.19869667291641235
Batch 56/64 loss: 0.20267772674560547
Batch 57/64 loss: 0.19746172428131104
Batch 58/64 loss: 0.20524364709854126
Batch 59/64 loss: 0.20888185501098633
Batch 60/64 loss: 0.20043063163757324
Batch 61/64 loss: 0.19938433170318604
Batch 62/64 loss: 0.20106983184814453
Batch 63/64 loss: 0.19807195663452148
Batch 64/64 loss: 0.20297527313232422
Epoch 48  Train loss: 0.20088814380122166  Val loss: 0.21456697019924412
Epoch 49
-------------------------------
Batch 1/64 loss: 0.19882452487945557
Batch 2/64 loss: 0.18212270736694336
Batch 3/64 loss: 0.21072959899902344
Batch 4/64 loss: 0.19372260570526123
Batch 5/64 loss: 0.20491909980773926
Batch 6/64 loss: 0.20845073461532593
Batch 7/64 loss: 0.19271695613861084
Batch 8/64 loss: 0.19898617267608643
Batch 9/64 loss: 0.19409900903701782
Batch 10/64 loss: 0.19615554809570312
Batch 11/64 loss: 0.21225714683532715
Batch 12/64 loss: 0.1889204978942871
Batch 13/64 loss: 0.209588885307312
Batch 14/64 loss: 0.19935870170593262
Batch 15/64 loss: 0.2204914093017578
Batch 16/64 loss: 0.2002260684967041
Batch 17/64 loss: 0.19278991222381592
Batch 18/64 loss: 0.19215762615203857
Batch 19/64 loss: 0.19613313674926758
Batch 20/64 loss: 0.20329350233078003
Batch 21/64 loss: 0.20317566394805908
Batch 22/64 loss: 0.1901821494102478
Batch 23/64 loss: 0.20186465978622437
Batch 24/64 loss: 0.19583290815353394
Batch 25/64 loss: 0.19919347763061523
Batch 26/64 loss: 0.20152270793914795
Batch 27/64 loss: 0.20707309246063232
Batch 28/64 loss: 0.2041885256767273
Batch 29/64 loss: 0.19875967502593994
Batch 30/64 loss: 0.20135629177093506
Batch 31/64 loss: 0.1963942050933838
Batch 32/64 loss: 0.20835071802139282
Batch 33/64 loss: 0.21071135997772217
Batch 34/64 loss: 0.1892791986465454
Batch 35/64 loss: 0.2086637020111084
Batch 36/64 loss: 0.18972623348236084
Batch 37/64 loss: 0.19741463661193848
Batch 38/64 loss: 0.1905210018157959
Batch 39/64 loss: 0.19704318046569824
Batch 40/64 loss: 0.19606554508209229
Batch 41/64 loss: 0.19583773612976074
Batch 42/64 loss: 0.188321053981781
Batch 43/64 loss: 0.20171040296554565
Batch 44/64 loss: 0.20010972023010254
Batch 45/64 loss: 0.18998920917510986
Batch 46/64 loss: 0.19518601894378662
Batch 47/64 loss: 0.19550538063049316
Batch 48/64 loss: 0.1994253396987915
Batch 49/64 loss: 0.21104609966278076
Batch 50/64 loss: 0.19828367233276367
Batch 51/64 loss: 0.19108325242996216
Batch 52/64 loss: 0.19128650426864624
Batch 53/64 loss: 0.20649117231369019
Batch 54/64 loss: 0.18851667642593384
Batch 55/64 loss: 0.19254803657531738
Batch 56/64 loss: 0.18977689743041992
Batch 57/64 loss: 0.19885635375976562
Batch 58/64 loss: 0.1947873830795288
Batch 59/64 loss: 0.1964019536972046
Batch 60/64 loss: 0.20361900329589844
Batch 61/64 loss: 0.2135445475578308
Batch 62/64 loss: 0.1898040771484375
Batch 63/64 loss: 0.19383615255355835
Batch 64/64 loss: 0.19237053394317627
Epoch 49  Train loss: 0.19848638936585072  Val loss: 0.2052773761585406
Saving best model, epoch: 49
Epoch 50
-------------------------------
Batch 1/64 loss: 0.19003820419311523
Batch 2/64 loss: 0.17972832918167114
Batch 3/64 loss: 0.1905478835105896
Batch 4/64 loss: 0.19589197635650635
Batch 5/64 loss: 0.1887032389640808
Batch 6/64 loss: 0.20577239990234375
Batch 7/64 loss: 0.19078963994979858
Batch 8/64 loss: 0.1923900842666626
Batch 9/64 loss: 0.19947659969329834
Batch 10/64 loss: 0.20619678497314453
Batch 11/64 loss: 0.19287657737731934
Batch 12/64 loss: 0.18419206142425537
Batch 13/64 loss: 0.19557350873947144
Batch 14/64 loss: 0.2022889256477356
Batch 15/64 loss: 0.18746733665466309
Batch 16/64 loss: 0.20684897899627686
Batch 17/64 loss: 0.18702369928359985
Batch 18/64 loss: 0.1912754774093628
Batch 19/64 loss: 0.1946704387664795
Batch 20/64 loss: 0.18692326545715332
Batch 21/64 loss: 0.18196415901184082
Batch 22/64 loss: 0.181382954120636
Batch 23/64 loss: 0.20378947257995605
Batch 24/64 loss: 0.1923907995223999
Batch 25/64 loss: 0.18805938959121704
Batch 26/64 loss: 0.20163559913635254
Batch 27/64 loss: 0.19167304039001465
Batch 28/64 loss: 0.20119547843933105
Batch 29/64 loss: 0.19563591480255127
Batch 30/64 loss: 0.18694627285003662
Batch 31/64 loss: 0.20450901985168457
Batch 32/64 loss: 0.20408588647842407
Batch 33/64 loss: 0.20348870754241943
Batch 34/64 loss: 0.21139323711395264
Batch 35/64 loss: 0.2016984224319458
Batch 36/64 loss: 0.197858989238739
Batch 37/64 loss: 0.19748008251190186
Batch 38/64 loss: 0.18816637992858887
Batch 39/64 loss: 0.19281035661697388
Batch 40/64 loss: 0.2116190791130066
Batch 41/64 loss: 0.19080525636672974
Batch 42/64 loss: 0.19522982835769653
Batch 43/64 loss: 0.18673241138458252
Batch 44/64 loss: 0.18878936767578125
Batch 45/64 loss: 0.19140625
Batch 46/64 loss: 0.20411014556884766
Batch 47/64 loss: 0.21049141883850098
Batch 48/64 loss: 0.20543789863586426
Batch 49/64 loss: 0.19901680946350098
Batch 50/64 loss: 0.19933414459228516
Batch 51/64 loss: 0.20206218957901
Batch 52/64 loss: 0.18626868724822998
Batch 53/64 loss: 0.1947529911994934
Batch 54/64 loss: 0.18940579891204834
Batch 55/64 loss: 0.2002784013748169
Batch 56/64 loss: 0.19980096817016602
Batch 57/64 loss: 0.21064555644989014
Batch 58/64 loss: 0.1794642210006714
Batch 59/64 loss: 0.19652748107910156
Batch 60/64 loss: 0.19912004470825195
Batch 61/64 loss: 0.18100804090499878
Batch 62/64 loss: 0.19916260242462158
Batch 63/64 loss: 0.20281708240509033
Batch 64/64 loss: 0.19113820791244507
Epoch 50  Train loss: 0.1954898808516708  Val loss: 0.20925311861988605
Epoch 51
-------------------------------
Batch 1/64 loss: 0.18568354845046997
Batch 2/64 loss: 0.19074785709381104
Batch 3/64 loss: 0.19136416912078857
Batch 4/64 loss: 0.19846075773239136
Batch 5/64 loss: 0.20400023460388184
Batch 6/64 loss: 0.1937110424041748
Batch 7/64 loss: 0.1945568323135376
Batch 8/64 loss: 0.2125159502029419
Batch 9/64 loss: 0.19071471691131592
Batch 10/64 loss: 0.1933230757713318
Batch 11/64 loss: 0.20013773441314697
Batch 12/64 loss: 0.1964329481124878
Batch 13/64 loss: 0.1797611117362976
Batch 14/64 loss: 0.18156832456588745
Batch 15/64 loss: 0.18645405769348145
Batch 16/64 loss: 0.1783280372619629
Batch 17/64 loss: 0.19338595867156982
Batch 18/64 loss: 0.20561957359313965
Batch 19/64 loss: 0.21491914987564087
Batch 20/64 loss: 0.191364586353302
Batch 21/64 loss: 0.19399338960647583
Batch 22/64 loss: 0.1766381859779358
Batch 23/64 loss: 0.19353079795837402
Batch 24/64 loss: 0.1920313835144043
Batch 25/64 loss: 0.20474612712860107
Batch 26/64 loss: 0.18295669555664062
Batch 27/64 loss: 0.1906413435935974
Batch 28/64 loss: 0.1876785159111023
Batch 29/64 loss: 0.1971588134765625
Batch 30/64 loss: 0.20299196243286133
Batch 31/64 loss: 0.19246625900268555
Batch 32/64 loss: 0.18777525424957275
Batch 33/64 loss: 0.17683309316635132
Batch 34/64 loss: 0.19999951124191284
Batch 35/64 loss: 0.17966699600219727
Batch 36/64 loss: 0.1852436065673828
Batch 37/64 loss: 0.2028104066848755
Batch 38/64 loss: 0.17946678400039673
Batch 39/64 loss: 0.19995200634002686
Batch 40/64 loss: 0.19746780395507812
Batch 41/64 loss: 0.19186615943908691
Batch 42/64 loss: 0.19218194484710693
Batch 43/64 loss: 0.20225167274475098
Batch 44/64 loss: 0.19351649284362793
Batch 45/64 loss: 0.1963338851928711
Batch 46/64 loss: 0.20969903469085693
Batch 47/64 loss: 0.17873764038085938
Batch 48/64 loss: 0.18435120582580566
Batch 49/64 loss: 0.18281590938568115
Batch 50/64 loss: 0.18869054317474365
Batch 51/64 loss: 0.18489807844161987
Batch 52/64 loss: 0.19326180219650269
Batch 53/64 loss: 0.2026602029800415
Batch 54/64 loss: 0.1904551386833191
Batch 55/64 loss: 0.19102227687835693
Batch 56/64 loss: 0.19342255592346191
Batch 57/64 loss: 0.18560397624969482
Batch 58/64 loss: 0.18051695823669434
Batch 59/64 loss: 0.1919468641281128
Batch 60/64 loss: 0.18198251724243164
Batch 61/64 loss: 0.18431192636489868
Batch 62/64 loss: 0.19239425659179688
Batch 63/64 loss: 0.1845291256904602
Batch 64/64 loss: 0.1963394284248352
Epoch 51  Train loss: 0.1918714249835295  Val loss: 0.2011296208371821
Saving best model, epoch: 51
Epoch 52
-------------------------------
Batch 1/64 loss: 0.2005220651626587
Batch 2/64 loss: 0.19143182039260864
Batch 3/64 loss: 0.19085466861724854
Batch 4/64 loss: 0.19320166110992432
Batch 5/64 loss: 0.18951189517974854
Batch 6/64 loss: 0.185477614402771
Batch 7/64 loss: 0.18993133306503296
Batch 8/64 loss: 0.18587809801101685
Batch 9/64 loss: 0.18712544441223145
Batch 10/64 loss: 0.17397648096084595
Batch 11/64 loss: 0.18382304906845093
Batch 12/64 loss: 0.18567407131195068
Batch 13/64 loss: 0.18918192386627197
Batch 14/64 loss: 0.1841137409210205
Batch 15/64 loss: 0.20305442810058594
Batch 16/64 loss: 0.19863343238830566
Batch 17/64 loss: 0.19942474365234375
Batch 18/64 loss: 0.19305622577667236
Batch 19/64 loss: 0.19691526889801025
Batch 20/64 loss: 0.20066094398498535
Batch 21/64 loss: 0.17642676830291748
Batch 22/64 loss: 0.17752325534820557
Batch 23/64 loss: 0.19909942150115967
Batch 24/64 loss: 0.18744254112243652
Batch 25/64 loss: 0.18256044387817383
Batch 26/64 loss: 0.1820235252380371
Batch 27/64 loss: 0.20097386837005615
Batch 28/64 loss: 0.18181824684143066
Batch 29/64 loss: 0.18116247653961182
Batch 30/64 loss: 0.18038618564605713
Batch 31/64 loss: 0.19179648160934448
Batch 32/64 loss: 0.190291166305542
Batch 33/64 loss: 0.21607351303100586
Batch 34/64 loss: 0.18588531017303467
Batch 35/64 loss: 0.18976175785064697
Batch 36/64 loss: 0.1801152229309082
Batch 37/64 loss: 0.19291901588439941
Batch 38/64 loss: 0.19387829303741455
Batch 39/64 loss: 0.19490396976470947
Batch 40/64 loss: 0.19418954849243164
Batch 41/64 loss: 0.1844085454940796
Batch 42/64 loss: 0.20173919200897217
Batch 43/64 loss: 0.19159841537475586
Batch 44/64 loss: 0.1766592264175415
Batch 45/64 loss: 0.19414228200912476
Batch 46/64 loss: 0.18917512893676758
Batch 47/64 loss: 0.1978479027748108
Batch 48/64 loss: 0.19447576999664307
Batch 49/64 loss: 0.18832826614379883
Batch 50/64 loss: 0.1844651699066162
Batch 51/64 loss: 0.20516222715377808
Batch 52/64 loss: 0.1860538125038147
Batch 53/64 loss: 0.19759583473205566
Batch 54/64 loss: 0.18136680126190186
Batch 55/64 loss: 0.18788504600524902
Batch 56/64 loss: 0.18948757648468018
Batch 57/64 loss: 0.17824804782867432
Batch 58/64 loss: 0.18548810482025146
Batch 59/64 loss: 0.20069289207458496
Batch 60/64 loss: 0.19333314895629883
Batch 61/64 loss: 0.19683492183685303
Batch 62/64 loss: 0.18635153770446777
Batch 63/64 loss: 0.1970481276512146
Batch 64/64 loss: 0.1814812421798706
Epoch 52  Train loss: 0.1902145698958752  Val loss: 0.19930066992736764
Saving best model, epoch: 52
Epoch 53
-------------------------------
Batch 1/64 loss: 0.19822925329208374
Batch 2/64 loss: 0.18511033058166504
Batch 3/64 loss: 0.177590012550354
Batch 4/64 loss: 0.18855571746826172
Batch 5/64 loss: 0.18974483013153076
Batch 6/64 loss: 0.17906039953231812
Batch 7/64 loss: 0.18746644258499146
Batch 8/64 loss: 0.19086945056915283
Batch 9/64 loss: 0.186695396900177
Batch 10/64 loss: 0.18739092350006104
Batch 11/64 loss: 0.17867755889892578
Batch 12/64 loss: 0.20176160335540771
Batch 13/64 loss: 0.21072864532470703
Batch 14/64 loss: 0.17911911010742188
Batch 15/64 loss: 0.19391965866088867
Batch 16/64 loss: 0.18595588207244873
Batch 17/64 loss: 0.19951754808425903
Batch 18/64 loss: 0.19154834747314453
Batch 19/64 loss: 0.1897639036178589
Batch 20/64 loss: 0.20126700401306152
Batch 21/64 loss: 0.1867157220840454
Batch 22/64 loss: 0.19631308317184448
Batch 23/64 loss: 0.195909321308136
Batch 24/64 loss: 0.1761123538017273
Batch 25/64 loss: 0.18847030401229858
Batch 26/64 loss: 0.19574642181396484
Batch 27/64 loss: 0.1798686385154724
Batch 28/64 loss: 0.1846979856491089
Batch 29/64 loss: 0.18290764093399048
Batch 30/64 loss: 0.1745433807373047
Batch 31/64 loss: 0.17713487148284912
Batch 32/64 loss: 0.17753517627716064
Batch 33/64 loss: 0.19589602947235107
Batch 34/64 loss: 0.18182671070098877
Batch 35/64 loss: 0.1779279112815857
Batch 36/64 loss: 0.18868732452392578
Batch 37/64 loss: 0.18098187446594238
Batch 38/64 loss: 0.18403536081314087
Batch 39/64 loss: 0.1900947093963623
Batch 40/64 loss: 0.1921718716621399
Batch 41/64 loss: 0.17479956150054932
Batch 42/64 loss: 0.19082677364349365
Batch 43/64 loss: 0.18839234113693237
Batch 44/64 loss: 0.19216156005859375
Batch 45/64 loss: 0.1822637915611267
Batch 46/64 loss: 0.19144922494888306
Batch 47/64 loss: 0.18980306386947632
Batch 48/64 loss: 0.18145018815994263
Batch 49/64 loss: 0.1915290355682373
Batch 50/64 loss: 0.1872425079345703
Batch 51/64 loss: 0.1947951316833496
Batch 52/64 loss: 0.1806110143661499
Batch 53/64 loss: 0.19207501411437988
Batch 54/64 loss: 0.19016355276107788
Batch 55/64 loss: 0.1924426555633545
Batch 56/64 loss: 0.20422351360321045
Batch 57/64 loss: 0.18765103816986084
Batch 58/64 loss: 0.18515324592590332
Batch 59/64 loss: 0.19355380535125732
Batch 60/64 loss: 0.18783658742904663
Batch 61/64 loss: 0.1944512128829956
Batch 62/64 loss: 0.2034703493118286
Batch 63/64 loss: 0.20002895593643188
Batch 64/64 loss: 0.18501520156860352
Epoch 53  Train loss: 0.1886381841173359  Val loss: 0.1985764673485379
Saving best model, epoch: 53
Epoch 54
-------------------------------
Batch 1/64 loss: 0.19800662994384766
Batch 2/64 loss: 0.18366169929504395
Batch 3/64 loss: 0.18030506372451782
Batch 4/64 loss: 0.17469561100006104
Batch 5/64 loss: 0.17140787839889526
Batch 6/64 loss: 0.17581123113632202
Batch 7/64 loss: 0.18001198768615723
Batch 8/64 loss: 0.18821412324905396
Batch 9/64 loss: 0.17314010858535767
Batch 10/64 loss: 0.18675923347473145
Batch 11/64 loss: 0.19485431909561157
Batch 12/64 loss: 0.20122087001800537
Batch 13/64 loss: 0.1920660138130188
Batch 14/64 loss: 0.19511061906814575
Batch 15/64 loss: 0.17829537391662598
Batch 16/64 loss: 0.1884588599205017
Batch 17/64 loss: 0.18770146369934082
Batch 18/64 loss: 0.19784873723983765
Batch 19/64 loss: 0.19278395175933838
Batch 20/64 loss: 0.18465566635131836
Batch 21/64 loss: 0.17755728960037231
Batch 22/64 loss: 0.18904352188110352
Batch 23/64 loss: 0.18105077743530273
Batch 24/64 loss: 0.18033277988433838
Batch 25/64 loss: 0.195085346698761
Batch 26/64 loss: 0.17673170566558838
Batch 27/64 loss: 0.18088281154632568
Batch 28/64 loss: 0.18237823247909546
Batch 29/64 loss: 0.19261997938156128
Batch 30/64 loss: 0.1829695701599121
Batch 31/64 loss: 0.18991589546203613
Batch 32/64 loss: 0.1941664218902588
Batch 33/64 loss: 0.18330252170562744
Batch 34/64 loss: 0.20210671424865723
Batch 35/64 loss: 0.17663192749023438
Batch 36/64 loss: 0.17365562915802002
Batch 37/64 loss: 0.19201236963272095
Batch 38/64 loss: 0.18662512302398682
Batch 39/64 loss: 0.17230087518692017
Batch 40/64 loss: 0.19093036651611328
Batch 41/64 loss: 0.17178618907928467
Batch 42/64 loss: 0.20392799377441406
Batch 43/64 loss: 0.18847274780273438
Batch 44/64 loss: 0.1807761788368225
Batch 45/64 loss: 0.17727899551391602
Batch 46/64 loss: 0.19777345657348633
Batch 47/64 loss: 0.1762438416481018
Batch 48/64 loss: 0.18724727630615234
Batch 49/64 loss: 0.18237793445587158
Batch 50/64 loss: 0.18065810203552246
Batch 51/64 loss: 0.18828481435775757
Batch 52/64 loss: 0.18029403686523438
Batch 53/64 loss: 0.18445134162902832
Batch 54/64 loss: 0.17578202486038208
Batch 55/64 loss: 0.1844855546951294
Batch 56/64 loss: 0.18230938911437988
Batch 57/64 loss: 0.1865708827972412
Batch 58/64 loss: 0.17843353748321533
Batch 59/64 loss: 0.17951279878616333
Batch 60/64 loss: 0.17295807600021362
Batch 61/64 loss: 0.17740559577941895
Batch 62/64 loss: 0.18020546436309814
Batch 63/64 loss: 0.20571410655975342
Batch 64/64 loss: 0.17826730012893677
Epoch 54  Train loss: 0.1848150606248893  Val loss: 0.19820907189673984
Saving best model, epoch: 54
Epoch 55
-------------------------------
Batch 1/64 loss: 0.17872726917266846
Batch 2/64 loss: 0.18793904781341553
Batch 3/64 loss: 0.18896424770355225
Batch 4/64 loss: 0.17955291271209717
Batch 5/64 loss: 0.17611056566238403
Batch 6/64 loss: 0.1911252737045288
Batch 7/64 loss: 0.1834017038345337
Batch 8/64 loss: 0.18972915410995483
Batch 9/64 loss: 0.18100249767303467
Batch 10/64 loss: 0.18271303176879883
Batch 11/64 loss: 0.18321412801742554
Batch 12/64 loss: 0.1843029260635376
Batch 13/64 loss: 0.1830344796180725
Batch 14/64 loss: 0.1825270652770996
Batch 15/64 loss: 0.19179576635360718
Batch 16/64 loss: 0.19298136234283447
Batch 17/64 loss: 0.17728972434997559
Batch 18/64 loss: 0.19730710983276367
Batch 19/64 loss: 0.17837387323379517
Batch 20/64 loss: 0.1819852590560913
Batch 21/64 loss: 0.17099368572235107
Batch 22/64 loss: 0.1803680658340454
Batch 23/64 loss: 0.19001799821853638
Batch 24/64 loss: 0.17052781581878662
Batch 25/64 loss: 0.18144828081130981
Batch 26/64 loss: 0.1793910264968872
Batch 27/64 loss: 0.16782212257385254
Batch 28/64 loss: 0.16990679502487183
Batch 29/64 loss: 0.16497588157653809
Batch 30/64 loss: 0.1775493025779724
Batch 31/64 loss: 0.17663133144378662
Batch 32/64 loss: 0.18465197086334229
Batch 33/64 loss: 0.1896212100982666
Batch 34/64 loss: 0.18293261528015137
Batch 35/64 loss: 0.18975073099136353
Batch 36/64 loss: 0.18847739696502686
Batch 37/64 loss: 0.18568706512451172
Batch 38/64 loss: 0.20555543899536133
Batch 39/64 loss: 0.17900192737579346
Batch 40/64 loss: 0.1889098882675171
Batch 41/64 loss: 0.1778612732887268
Batch 42/64 loss: 0.17959415912628174
Batch 43/64 loss: 0.1916600465774536
Batch 44/64 loss: 0.18245112895965576
Batch 45/64 loss: 0.18115472793579102
Batch 46/64 loss: 0.1765758991241455
Batch 47/64 loss: 0.17009282112121582
Batch 48/64 loss: 0.17369961738586426
Batch 49/64 loss: 0.17858076095581055
Batch 50/64 loss: 0.1852506399154663
Batch 51/64 loss: 0.18562078475952148
Batch 52/64 loss: 0.18530797958374023
Batch 53/64 loss: 0.1766883134841919
Batch 54/64 loss: 0.17332559823989868
Batch 55/64 loss: 0.1970347762107849
Batch 56/64 loss: 0.1837480068206787
Batch 57/64 loss: 0.19644123315811157
Batch 58/64 loss: 0.1870390772819519
Batch 59/64 loss: 0.18859833478927612
Batch 60/64 loss: 0.1853623390197754
Batch 61/64 loss: 0.1737939715385437
Batch 62/64 loss: 0.17732572555541992
Batch 63/64 loss: 0.18918555974960327
Batch 64/64 loss: 0.17052185535430908
Epoch 55  Train loss: 0.1825973665013033  Val loss: 0.19330231430604286
Saving best model, epoch: 55
Epoch 56
-------------------------------
Batch 1/64 loss: 0.18037450313568115
Batch 2/64 loss: 0.17647749185562134
Batch 3/64 loss: 0.1783839464187622
Batch 4/64 loss: 0.17740333080291748
Batch 5/64 loss: 0.18424326181411743
Batch 6/64 loss: 0.17506587505340576
Batch 7/64 loss: 0.1776077151298523
Batch 8/64 loss: 0.18428540229797363
Batch 9/64 loss: 0.1789119839668274
Batch 10/64 loss: 0.1939411163330078
Batch 11/64 loss: 0.17725467681884766
Batch 12/64 loss: 0.17991983890533447
Batch 13/64 loss: 0.18442034721374512
Batch 14/64 loss: 0.19237536191940308
Batch 15/64 loss: 0.1709449291229248
Batch 16/64 loss: 0.17729222774505615
Batch 17/64 loss: 0.1911773681640625
Batch 18/64 loss: 0.17761611938476562
Batch 19/64 loss: 0.16948014497756958
Batch 20/64 loss: 0.1826741099357605
Batch 21/64 loss: 0.18117713928222656
Batch 22/64 loss: 0.18089759349822998
Batch 23/64 loss: 0.17745596170425415
Batch 24/64 loss: 0.1694040298461914
Batch 25/64 loss: 0.1872878074645996
Batch 26/64 loss: 0.18084102869033813
Batch 27/64 loss: 0.17074167728424072
Batch 28/64 loss: 0.18555426597595215
Batch 29/64 loss: 0.18211090564727783
Batch 30/64 loss: 0.18074607849121094
Batch 31/64 loss: 0.18252062797546387
Batch 32/64 loss: 0.18621408939361572
Batch 33/64 loss: 0.1747007966041565
Batch 34/64 loss: 0.18441998958587646
Batch 35/64 loss: 0.19271790981292725
Batch 36/64 loss: 0.17673474550247192
Batch 37/64 loss: 0.17879188060760498
Batch 38/64 loss: 0.17119914293289185
Batch 39/64 loss: 0.17800450325012207
Batch 40/64 loss: 0.17702525854110718
Batch 41/64 loss: 0.17884516716003418
Batch 42/64 loss: 0.17747420072555542
Batch 43/64 loss: 0.17747819423675537
Batch 44/64 loss: 0.16369003057479858
Batch 45/64 loss: 0.17936795949935913
Batch 46/64 loss: 0.18694907426834106
Batch 47/64 loss: 0.1809631586074829
Batch 48/64 loss: 0.19320416450500488
Batch 49/64 loss: 0.19404220581054688
Batch 50/64 loss: 0.18242263793945312
Batch 51/64 loss: 0.17539441585540771
Batch 52/64 loss: 0.17881238460540771
Batch 53/64 loss: 0.18222647905349731
Batch 54/64 loss: 0.17838019132614136
Batch 55/64 loss: 0.1709611415863037
Batch 56/64 loss: 0.187935471534729
Batch 57/64 loss: 0.18509340286254883
Batch 58/64 loss: 0.19181382656097412
Batch 59/64 loss: 0.1879839301109314
Batch 60/64 loss: 0.17794162034988403
Batch 61/64 loss: 0.1825302243232727
Batch 62/64 loss: 0.18270796537399292
Batch 63/64 loss: 0.17771708965301514
Batch 64/64 loss: 0.17895424365997314
Epoch 56  Train loss: 0.18065169886046764  Val loss: 0.1848876349295128
Saving best model, epoch: 56
Epoch 57
-------------------------------
Batch 1/64 loss: 0.1885778307914734
Batch 2/64 loss: 0.18231558799743652
Batch 3/64 loss: 0.17016667127609253
Batch 4/64 loss: 0.18841320276260376
Batch 5/64 loss: 0.17451012134552002
Batch 6/64 loss: 0.1809920072555542
Batch 7/64 loss: 0.18693780899047852
Batch 8/64 loss: 0.16737043857574463
Batch 9/64 loss: 0.19559109210968018
Batch 10/64 loss: 0.17942845821380615
Batch 11/64 loss: 0.16499972343444824
Batch 12/64 loss: 0.18610155582427979
Batch 13/64 loss: 0.19398808479309082
Batch 14/64 loss: 0.18337029218673706
Batch 15/64 loss: 0.18846380710601807
Batch 16/64 loss: 0.18054670095443726
Batch 17/64 loss: 0.18665122985839844
Batch 18/64 loss: 0.18324607610702515
Batch 19/64 loss: 0.17368990182876587
Batch 20/64 loss: 0.1737527847290039
Batch 21/64 loss: 0.17138856649398804
Batch 22/64 loss: 0.17464357614517212
Batch 23/64 loss: 0.17970752716064453
Batch 24/64 loss: 0.16810059547424316
Batch 25/64 loss: 0.17347854375839233
Batch 26/64 loss: 0.19348180294036865
Batch 27/64 loss: 0.16165602207183838
Batch 28/64 loss: 0.18060564994812012
Batch 29/64 loss: 0.18260407447814941
Batch 30/64 loss: 0.17356783151626587
Batch 31/64 loss: 0.1744140386581421
Batch 32/64 loss: 0.17052245140075684
Batch 33/64 loss: 0.17554986476898193
Batch 34/64 loss: 0.18327033519744873
Batch 35/64 loss: 0.18136584758758545
Batch 36/64 loss: 0.17732000350952148
Batch 37/64 loss: 0.18874549865722656
Batch 38/64 loss: 0.17840969562530518
Batch 39/64 loss: 0.1745273470878601
Batch 40/64 loss: 0.16010761260986328
Batch 41/64 loss: 0.17939996719360352
Batch 42/64 loss: 0.17179495096206665
Batch 43/64 loss: 0.18374085426330566
Batch 44/64 loss: 0.1895437240600586
Batch 45/64 loss: 0.18057990074157715
Batch 46/64 loss: 0.16550171375274658
Batch 47/64 loss: 0.16731923818588257
Batch 48/64 loss: 0.18249666690826416
Batch 49/64 loss: 0.1675114631652832
Batch 50/64 loss: 0.17714285850524902
Batch 51/64 loss: 0.17276263236999512
Batch 52/64 loss: 0.1702226996421814
Batch 53/64 loss: 0.18499523401260376
Batch 54/64 loss: 0.17488324642181396
Batch 55/64 loss: 0.17396283149719238
Batch 56/64 loss: 0.17173433303833008
Batch 57/64 loss: 0.16758811473846436
Batch 58/64 loss: 0.1702287197113037
Batch 59/64 loss: 0.16678857803344727
Batch 60/64 loss: 0.17316138744354248
Batch 61/64 loss: 0.16632890701293945
Batch 62/64 loss: 0.17867732048034668
Batch 63/64 loss: 0.17227184772491455
Batch 64/64 loss: 0.17331922054290771
Epoch 57  Train loss: 0.1771169704549453  Val loss: 0.18802091469060106
Epoch 58
-------------------------------
Batch 1/64 loss: 0.1790604591369629
Batch 2/64 loss: 0.167294442653656
Batch 3/64 loss: 0.1698189377784729
Batch 4/64 loss: 0.17038506269454956
Batch 5/64 loss: 0.18264788389205933
Batch 6/64 loss: 0.16639864444732666
Batch 7/64 loss: 0.1594962477684021
Batch 8/64 loss: 0.1706686019897461
Batch 9/64 loss: 0.16515547037124634
Batch 10/64 loss: 0.1840183138847351
Batch 11/64 loss: 0.1708470582962036
Batch 12/64 loss: 0.1945720911026001
Batch 13/64 loss: 0.17514175176620483
Batch 14/64 loss: 0.18368446826934814
Batch 15/64 loss: 0.19764000177383423
Batch 16/64 loss: 0.17740124464035034
Batch 17/64 loss: 0.1721329689025879
Batch 18/64 loss: 0.18551170825958252
Batch 19/64 loss: 0.18179595470428467
Batch 20/64 loss: 0.17595338821411133
Batch 21/64 loss: 0.18896949291229248
Batch 22/64 loss: 0.15751951932907104
Batch 23/64 loss: 0.1819627285003662
Batch 24/64 loss: 0.18234169483184814
Batch 25/64 loss: 0.16335350275039673
Batch 26/64 loss: 0.17851895093917847
Batch 27/64 loss: 0.18035709857940674
Batch 28/64 loss: 0.1779959797859192
Batch 29/64 loss: 0.1736125946044922
Batch 30/64 loss: 0.17348778247833252
Batch 31/64 loss: 0.1765451431274414
Batch 32/64 loss: 0.17526686191558838
Batch 33/64 loss: 0.1716577410697937
Batch 34/64 loss: 0.17012375593185425
Batch 35/64 loss: 0.1919441819190979
Batch 36/64 loss: 0.17514407634735107
Batch 37/64 loss: 0.17547744512557983
Batch 38/64 loss: 0.1813424825668335
Batch 39/64 loss: 0.16329729557037354
Batch 40/64 loss: 0.18275916576385498
Batch 41/64 loss: 0.17525196075439453
Batch 42/64 loss: 0.1653568148612976
Batch 43/64 loss: 0.1711377501487732
Batch 44/64 loss: 0.1866511106491089
Batch 45/64 loss: 0.1741178035736084
Batch 46/64 loss: 0.17494958639144897
Batch 47/64 loss: 0.1592416763305664
Batch 48/64 loss: 0.17311638593673706
Batch 49/64 loss: 0.19185125827789307
Batch 50/64 loss: 0.18185007572174072
Batch 51/64 loss: 0.1703985333442688
Batch 52/64 loss: 0.17256247997283936
Batch 53/64 loss: 0.17821580171585083
Batch 54/64 loss: 0.1695212721824646
Batch 55/64 loss: 0.17243623733520508
Batch 56/64 loss: 0.16980725526809692
Batch 57/64 loss: 0.1669449806213379
Batch 58/64 loss: 0.16061937808990479
Batch 59/64 loss: 0.1692832112312317
Batch 60/64 loss: 0.17212873697280884
Batch 61/64 loss: 0.1739104986190796
Batch 62/64 loss: 0.15684086084365845
Batch 63/64 loss: 0.17297911643981934
Batch 64/64 loss: 0.17591232061386108
Epoch 58  Train loss: 0.1747828897307901  Val loss: 0.18272025048527932
Saving best model, epoch: 58
Epoch 59
-------------------------------
Batch 1/64 loss: 0.17252427339553833
Batch 2/64 loss: 0.1601659059524536
Batch 3/64 loss: 0.18441593647003174
Batch 4/64 loss: 0.17953920364379883
Batch 5/64 loss: 0.17680561542510986
Batch 6/64 loss: 0.17316877841949463
Batch 7/64 loss: 0.18256163597106934
Batch 8/64 loss: 0.1610357165336609
Batch 9/64 loss: 0.18902337551116943
Batch 10/64 loss: 0.17784368991851807
Batch 11/64 loss: 0.183529794216156
Batch 12/64 loss: 0.175448477268219
Batch 13/64 loss: 0.17261111736297607
Batch 14/64 loss: 0.1798756718635559
Batch 15/64 loss: 0.18554627895355225
Batch 16/64 loss: 0.16693508625030518
Batch 17/64 loss: 0.15955114364624023
Batch 18/64 loss: 0.17423182725906372
Batch 19/64 loss: 0.1679973602294922
Batch 20/64 loss: 0.18142282962799072
Batch 21/64 loss: 0.16372787952423096
Batch 22/64 loss: 0.18187600374221802
Batch 23/64 loss: 0.17081183195114136
Batch 24/64 loss: 0.16791045665740967
Batch 25/64 loss: 0.16933006048202515
Batch 26/64 loss: 0.17199474573135376
Batch 27/64 loss: 0.18095028400421143
Batch 28/64 loss: 0.1727582812309265
Batch 29/64 loss: 0.1756553053855896
Batch 30/64 loss: 0.17112892866134644
Batch 31/64 loss: 0.15765786170959473
Batch 32/64 loss: 0.1727246642112732
Batch 33/64 loss: 0.16652792692184448
Batch 34/64 loss: 0.18095970153808594
Batch 35/64 loss: 0.15918952226638794
Batch 36/64 loss: 0.1861269474029541
Batch 37/64 loss: 0.17345601320266724
Batch 38/64 loss: 0.16660994291305542
Batch 39/64 loss: 0.18401861190795898
Batch 40/64 loss: 0.15705174207687378
Batch 41/64 loss: 0.1737719178199768
Batch 42/64 loss: 0.17359179258346558
Batch 43/64 loss: 0.182104229927063
Batch 44/64 loss: 0.18301057815551758
Batch 45/64 loss: 0.17415571212768555
Batch 46/64 loss: 0.16376334428787231
Batch 47/64 loss: 0.15489333868026733
Batch 48/64 loss: 0.17453265190124512
Batch 49/64 loss: 0.16321533918380737
Batch 50/64 loss: 0.17223715782165527
Batch 51/64 loss: 0.17179638147354126
Batch 52/64 loss: 0.1857190728187561
Batch 53/64 loss: 0.18718379735946655
Batch 54/64 loss: 0.1707400679588318
Batch 55/64 loss: 0.16670948266983032
Batch 56/64 loss: 0.16392701864242554
Batch 57/64 loss: 0.1877981424331665
Batch 58/64 loss: 0.18069303035736084
Batch 59/64 loss: 0.1776619553565979
Batch 60/64 loss: 0.17305344343185425
Batch 61/64 loss: 0.17954283952713013
Batch 62/64 loss: 0.16776472330093384
Batch 63/64 loss: 0.1690899133682251
Batch 64/64 loss: 0.17518997192382812
Epoch 59  Train loss: 0.1735380210128485  Val loss: 0.1826169621903462
Saving best model, epoch: 59
Epoch 60
-------------------------------
Batch 1/64 loss: 0.17436116933822632
Batch 2/64 loss: 0.1714324951171875
Batch 3/64 loss: 0.1942201852798462
Batch 4/64 loss: 0.17760032415390015
Batch 5/64 loss: 0.17865455150604248
Batch 6/64 loss: 0.17782020568847656
Batch 7/64 loss: 0.17324304580688477
Batch 8/64 loss: 0.1581299901008606
Batch 9/64 loss: 0.17536526918411255
Batch 10/64 loss: 0.15966475009918213
Batch 11/64 loss: 0.17465174198150635
Batch 12/64 loss: 0.16836410760879517
Batch 13/64 loss: 0.17719531059265137
Batch 14/64 loss: 0.17913389205932617
Batch 15/64 loss: 0.17654722929000854
Batch 16/64 loss: 0.16104638576507568
Batch 17/64 loss: 0.16761887073516846
Batch 18/64 loss: 0.18742835521697998
Batch 19/64 loss: 0.15974849462509155
Batch 20/64 loss: 0.1830478310585022
Batch 21/64 loss: 0.16607916355133057
Batch 22/64 loss: 0.1744149923324585
Batch 23/64 loss: 0.17200249433517456
Batch 24/64 loss: 0.16699695587158203
Batch 25/64 loss: 0.16478729248046875
Batch 26/64 loss: 0.1732032299041748
Batch 27/64 loss: 0.17964929342269897
Batch 28/64 loss: 0.169447124004364
Batch 29/64 loss: 0.17347514629364014
Batch 30/64 loss: 0.16901010274887085
Batch 31/64 loss: 0.16708385944366455
Batch 32/64 loss: 0.1654796004295349
Batch 33/64 loss: 0.17756760120391846
Batch 34/64 loss: 0.17776358127593994
Batch 35/64 loss: 0.1684412956237793
Batch 36/64 loss: 0.16744965314865112
Batch 37/64 loss: 0.16902828216552734
Batch 38/64 loss: 0.18555569648742676
Batch 39/64 loss: 0.18050897121429443
Batch 40/64 loss: 0.16436231136322021
Batch 41/64 loss: 0.17051362991333008
Batch 42/64 loss: 0.1822676658630371
Batch 43/64 loss: 0.16466522216796875
Batch 44/64 loss: 0.17277681827545166
Batch 45/64 loss: 0.177379310131073
Batch 46/64 loss: 0.1610652208328247
Batch 47/64 loss: 0.16558218002319336
Batch 48/64 loss: 0.17864596843719482
Batch 49/64 loss: 0.1722387671470642
Batch 50/64 loss: 0.17321562767028809
Batch 51/64 loss: 0.16941726207733154
Batch 52/64 loss: 0.1722317934036255
Batch 53/64 loss: 0.17576539516448975
Batch 54/64 loss: 0.16885197162628174
Batch 55/64 loss: 0.1758905053138733
Batch 56/64 loss: 0.182728111743927
Batch 57/64 loss: 0.16600626707077026
Batch 58/64 loss: 0.17829465866088867
Batch 59/64 loss: 0.16717404127120972
Batch 60/64 loss: 0.15947014093399048
Batch 61/64 loss: 0.17039763927459717
Batch 62/64 loss: 0.15499019622802734
Batch 63/64 loss: 0.16785019636154175
Batch 64/64 loss: 0.17596131563186646
Epoch 60  Train loss: 0.17203090868744195  Val loss: 0.18208522628672755
Saving best model, epoch: 60
Epoch 61
-------------------------------
Batch 1/64 loss: 0.18203890323638916
Batch 2/64 loss: 0.17623400688171387
Batch 3/64 loss: 0.15842676162719727
Batch 4/64 loss: 0.1726561188697815
Batch 5/64 loss: 0.1625218391418457
Batch 6/64 loss: 0.16258931159973145
Batch 7/64 loss: 0.1600879430770874
Batch 8/64 loss: 0.16648757457733154
Batch 9/64 loss: 0.16221100091934204
Batch 10/64 loss: 0.16557371616363525
Batch 11/64 loss: 0.1733991503715515
Batch 12/64 loss: 0.15809547901153564
Batch 13/64 loss: 0.1653624176979065
Batch 14/64 loss: 0.170343279838562
Batch 15/64 loss: 0.1707361936569214
Batch 16/64 loss: 0.16560184955596924
Batch 17/64 loss: 0.1854810118675232
Batch 18/64 loss: 0.16921979188919067
Batch 19/64 loss: 0.1687830686569214
Batch 20/64 loss: 0.17278331518173218
Batch 21/64 loss: 0.15450119972229004
Batch 22/64 loss: 0.18047654628753662
Batch 23/64 loss: 0.16678881645202637
Batch 24/64 loss: 0.1651160717010498
Batch 25/64 loss: 0.19348609447479248
Batch 26/64 loss: 0.17030024528503418
Batch 27/64 loss: 0.17026257514953613
Batch 28/64 loss: 0.16305077075958252
Batch 29/64 loss: 0.16459739208221436
Batch 30/64 loss: 0.15480810403823853
Batch 31/64 loss: 0.16310179233551025
Batch 32/64 loss: 0.16703331470489502
Batch 33/64 loss: 0.17609381675720215
Batch 34/64 loss: 0.16029441356658936
Batch 35/64 loss: 0.16577517986297607
Batch 36/64 loss: 0.17876315116882324
Batch 37/64 loss: 0.17430508136749268
Batch 38/64 loss: 0.1806238889694214
Batch 39/64 loss: 0.18895411491394043
Batch 40/64 loss: 0.16817528009414673
Batch 41/64 loss: 0.17961138486862183
Batch 42/64 loss: 0.16589957475662231
Batch 43/64 loss: 0.1574687361717224
Batch 44/64 loss: 0.17184895277023315
Batch 45/64 loss: 0.16810619831085205
Batch 46/64 loss: 0.1546667218208313
Batch 47/64 loss: 0.16427040100097656
Batch 48/64 loss: 0.174902081489563
Batch 49/64 loss: 0.17223215103149414
Batch 50/64 loss: 0.16162604093551636
Batch 51/64 loss: 0.17325401306152344
Batch 52/64 loss: 0.17373985052108765
Batch 53/64 loss: 0.1717512607574463
Batch 54/64 loss: 0.16775858402252197
Batch 55/64 loss: 0.18856298923492432
Batch 56/64 loss: 0.1683846116065979
Batch 57/64 loss: 0.1695958375930786
Batch 58/64 loss: 0.17667627334594727
Batch 59/64 loss: 0.16358542442321777
Batch 60/64 loss: 0.17106503248214722
Batch 61/64 loss: 0.16456621885299683
Batch 62/64 loss: 0.16788476705551147
Batch 63/64 loss: 0.17213129997253418
Batch 64/64 loss: 0.15804481506347656
Epoch 61  Train loss: 0.16930608001409791  Val loss: 0.1813892564413064
Saving best model, epoch: 61
Epoch 62
-------------------------------
Batch 1/64 loss: 0.16587817668914795
Batch 2/64 loss: 0.16348576545715332
Batch 3/64 loss: 0.17298197746276855
Batch 4/64 loss: 0.16349279880523682
Batch 5/64 loss: 0.17714393138885498
Batch 6/64 loss: 0.1784868836402893
Batch 7/64 loss: 0.1779005527496338
Batch 8/64 loss: 0.1693974733352661
Batch 9/64 loss: 0.18191885948181152
Batch 10/64 loss: 0.18871474266052246
Batch 11/64 loss: 0.17107897996902466
Batch 12/64 loss: 0.18063479661941528
Batch 13/64 loss: 0.16205787658691406
Batch 14/64 loss: 0.1554517149925232
Batch 15/64 loss: 0.16018915176391602
Batch 16/64 loss: 0.17828595638275146
Batch 17/64 loss: 0.15621739625930786
Batch 18/64 loss: 0.17194724082946777
Batch 19/64 loss: 0.1524287462234497
Batch 20/64 loss: 0.16146188974380493
Batch 21/64 loss: 0.17393726110458374
Batch 22/64 loss: 0.17150390148162842
Batch 23/64 loss: 0.1617388129234314
Batch 24/64 loss: 0.17629075050354004
Batch 25/64 loss: 0.16968172788619995
Batch 26/64 loss: 0.1628420352935791
Batch 27/64 loss: 0.19041597843170166
Batch 28/64 loss: 0.1642516851425171
Batch 29/64 loss: 0.1678764820098877
Batch 30/64 loss: 0.1594928503036499
Batch 31/64 loss: 0.1587916612625122
Batch 32/64 loss: 0.1581641435623169
Batch 33/64 loss: 0.16682302951812744
Batch 34/64 loss: 0.1496192216873169
Batch 35/64 loss: 0.15529465675354004
Batch 36/64 loss: 0.16980427503585815
Batch 37/64 loss: 0.16940635442733765
Batch 38/64 loss: 0.17481017112731934
Batch 39/64 loss: 0.16985267400741577
Batch 40/64 loss: 0.16124367713928223
Batch 41/64 loss: 0.16576498746871948
Batch 42/64 loss: 0.15761691331863403
Batch 43/64 loss: 0.17258191108703613
Batch 44/64 loss: 0.17591536045074463
Batch 45/64 loss: 0.1772581934928894
Batch 46/64 loss: 0.16448986530303955
Batch 47/64 loss: 0.1680881381034851
Batch 48/64 loss: 0.1788957118988037
Batch 49/64 loss: 0.16436100006103516
Batch 50/64 loss: 0.16979777812957764
Batch 51/64 loss: 0.15595859289169312
Batch 52/64 loss: 0.16731315851211548
Batch 53/64 loss: 0.1618441939353943
Batch 54/64 loss: 0.15567153692245483
Batch 55/64 loss: 0.1683022379875183
Batch 56/64 loss: 0.17026329040527344
Batch 57/64 loss: 0.16476422548294067
Batch 58/64 loss: 0.15467143058776855
Batch 59/64 loss: 0.15999585390090942
Batch 60/64 loss: 0.1790245771408081
Batch 61/64 loss: 0.16777968406677246
Batch 62/64 loss: 0.1717107892036438
Batch 63/64 loss: 0.17900687456130981
Batch 64/64 loss: 0.17124074697494507
Epoch 62  Train loss: 0.16785102942410637  Val loss: 0.1807478875229039
Saving best model, epoch: 62
Epoch 63
-------------------------------
Batch 1/64 loss: 0.16480690240859985
Batch 2/64 loss: 0.16635143756866455
Batch 3/64 loss: 0.17150717973709106
Batch 4/64 loss: 0.15920084714889526
Batch 5/64 loss: 0.1507003903388977
Batch 6/64 loss: 0.1726698875427246
Batch 7/64 loss: 0.16575473546981812
Batch 8/64 loss: 0.15368950366973877
Batch 9/64 loss: 0.18535983562469482
Batch 10/64 loss: 0.16222846508026123
Batch 11/64 loss: 0.16197115182876587
Batch 12/64 loss: 0.16403836011886597
Batch 13/64 loss: 0.15571045875549316
Batch 14/64 loss: 0.17620539665222168
Batch 15/64 loss: 0.1708407998085022
Batch 16/64 loss: 0.16237801313400269
Batch 17/64 loss: 0.17088091373443604
Batch 18/64 loss: 0.16714775562286377
Batch 19/64 loss: 0.1672547459602356
Batch 20/64 loss: 0.16679561138153076
Batch 21/64 loss: 0.17201119661331177
Batch 22/64 loss: 0.1539074182510376
Batch 23/64 loss: 0.17095965147018433
Batch 24/64 loss: 0.17902696132659912
Batch 25/64 loss: 0.1671631932258606
Batch 26/64 loss: 0.16866523027420044
Batch 27/64 loss: 0.15431880950927734
Batch 28/64 loss: 0.159928560256958
Batch 29/64 loss: 0.16651654243469238
Batch 30/64 loss: 0.16316872835159302
Batch 31/64 loss: 0.16733694076538086
Batch 32/64 loss: 0.16551703214645386
Batch 33/64 loss: 0.16602712869644165
Batch 34/64 loss: 0.175528883934021
Batch 35/64 loss: 0.16724878549575806
Batch 36/64 loss: 0.17945659160614014
Batch 37/64 loss: 0.18123221397399902
Batch 38/64 loss: 0.18711787462234497
Batch 39/64 loss: 0.17163556814193726
Batch 40/64 loss: 0.15954208374023438
Batch 41/64 loss: 0.1626841425895691
Batch 42/64 loss: 0.1708921194076538
Batch 43/64 loss: 0.17550110816955566
Batch 44/64 loss: 0.16041111946105957
Batch 45/64 loss: 0.16015273332595825
Batch 46/64 loss: 0.170376718044281
Batch 47/64 loss: 0.16374796628952026
Batch 48/64 loss: 0.1697148084640503
Batch 49/64 loss: 0.16731363534927368
Batch 50/64 loss: 0.1670810580253601
Batch 51/64 loss: 0.15710103511810303
Batch 52/64 loss: 0.1707848310470581
Batch 53/64 loss: 0.15892142057418823
Batch 54/64 loss: 0.16951334476470947
Batch 55/64 loss: 0.15355467796325684
Batch 56/64 loss: 0.16111284494400024
Batch 57/64 loss: 0.1549440622329712
Batch 58/64 loss: 0.1762353777885437
Batch 59/64 loss: 0.1729779839515686
Batch 60/64 loss: 0.19053912162780762
Batch 61/64 loss: 0.16897642612457275
Batch 62/64 loss: 0.17091190814971924
Batch 63/64 loss: 0.17321497201919556
Batch 64/64 loss: 0.16887569427490234
Epoch 63  Train loss: 0.1672960309421315  Val loss: 0.17704270262898447
Saving best model, epoch: 63
Epoch 64
-------------------------------
Batch 1/64 loss: 0.15791535377502441
Batch 2/64 loss: 0.17198705673217773
Batch 3/64 loss: 0.1523890495300293
Batch 4/64 loss: 0.16715103387832642
Batch 5/64 loss: 0.16250014305114746
Batch 6/64 loss: 0.17194640636444092
Batch 7/64 loss: 0.16158920526504517
Batch 8/64 loss: 0.1616230010986328
Batch 9/64 loss: 0.16146951913833618
Batch 10/64 loss: 0.16271215677261353
Batch 11/64 loss: 0.1516391634941101
Batch 12/64 loss: 0.1675683856010437
Batch 13/64 loss: 0.15596795082092285
Batch 14/64 loss: 0.16874605417251587
Batch 15/64 loss: 0.1512790322303772
Batch 16/64 loss: 0.14939188957214355
Batch 17/64 loss: 0.16627538204193115
Batch 18/64 loss: 0.16008293628692627
Batch 19/64 loss: 0.1572176218032837
Batch 20/64 loss: 0.15905117988586426
Batch 21/64 loss: 0.17544960975646973
Batch 22/64 loss: 0.15723085403442383
Batch 23/64 loss: 0.16815298795700073
Batch 24/64 loss: 0.17297571897506714
Batch 25/64 loss: 0.16423535346984863
Batch 26/64 loss: 0.14920467138290405
Batch 27/64 loss: 0.17845499515533447
Batch 28/64 loss: 0.15705645084381104
Batch 29/64 loss: 0.16853368282318115
Batch 30/64 loss: 0.18008631467819214
Batch 31/64 loss: 0.16642433404922485
Batch 32/64 loss: 0.17224830389022827
Batch 33/64 loss: 0.1556013822555542
Batch 34/64 loss: 0.1770973801612854
Batch 35/64 loss: 0.15951043367385864
Batch 36/64 loss: 0.16245287656784058
Batch 37/64 loss: 0.16324388980865479
Batch 38/64 loss: 0.16428500413894653
Batch 39/64 loss: 0.15933269262313843
Batch 40/64 loss: 0.16237521171569824
Batch 41/64 loss: 0.15090715885162354
Batch 42/64 loss: 0.16683542728424072
Batch 43/64 loss: 0.17307734489440918
Batch 44/64 loss: 0.16907864809036255
Batch 45/64 loss: 0.16883009672164917
Batch 46/64 loss: 0.1542854905128479
Batch 47/64 loss: 0.1588568091392517
Batch 48/64 loss: 0.15788346529006958
Batch 49/64 loss: 0.15265750885009766
Batch 50/64 loss: 0.1671227216720581
Batch 51/64 loss: 0.15139871835708618
Batch 52/64 loss: 0.16267460584640503
Batch 53/64 loss: 0.16529405117034912
Batch 54/64 loss: 0.1656399965286255
Batch 55/64 loss: 0.16683340072631836
Batch 56/64 loss: 0.15760576725006104
Batch 57/64 loss: 0.1692805290222168
Batch 58/64 loss: 0.14246970415115356
Batch 59/64 loss: 0.16215121746063232
Batch 60/64 loss: 0.15882611274719238
Batch 61/64 loss: 0.16358602046966553
Batch 62/64 loss: 0.15150833129882812
Batch 63/64 loss: 0.15548455715179443
Batch 64/64 loss: 0.15882021188735962
Epoch 64  Train loss: 0.1622565884216159  Val loss: 0.1717690276526094
Saving best model, epoch: 64
Epoch 65
-------------------------------
Batch 1/64 loss: 0.1398717164993286
Batch 2/64 loss: 0.16665244102478027
Batch 3/64 loss: 0.15989422798156738
Batch 4/64 loss: 0.16732728481292725
Batch 5/64 loss: 0.1562734842300415
Batch 6/64 loss: 0.16939526796340942
Batch 7/64 loss: 0.15580880641937256
Batch 8/64 loss: 0.15393203496932983
Batch 9/64 loss: 0.1545403003692627
Batch 10/64 loss: 0.17701011896133423
Batch 11/64 loss: 0.15461111068725586
Batch 12/64 loss: 0.17710095643997192
Batch 13/64 loss: 0.1728479266166687
Batch 14/64 loss: 0.16223621368408203
Batch 15/64 loss: 0.1649463176727295
Batch 16/64 loss: 0.1761431097984314
Batch 17/64 loss: 0.1655369997024536
Batch 18/64 loss: 0.1566154956817627
Batch 19/64 loss: 0.14611226320266724
Batch 20/64 loss: 0.15201616287231445
Batch 21/64 loss: 0.15869760513305664
Batch 22/64 loss: 0.15964895486831665
Batch 23/64 loss: 0.17605429887771606
Batch 24/64 loss: 0.16583991050720215
Batch 25/64 loss: 0.16744649410247803
Batch 26/64 loss: 0.1707208752632141
Batch 27/64 loss: 0.15420383214950562
Batch 28/64 loss: 0.15300822257995605
Batch 29/64 loss: 0.152471125125885
Batch 30/64 loss: 0.14807432889938354
Batch 31/64 loss: 0.15328389406204224
Batch 32/64 loss: 0.15766727924346924
Batch 33/64 loss: 0.16226696968078613
Batch 34/64 loss: 0.15434378385543823
Batch 35/64 loss: 0.1523042917251587
Batch 36/64 loss: 0.1601760983467102
Batch 37/64 loss: 0.152751624584198
Batch 38/64 loss: 0.16249769926071167
Batch 39/64 loss: 0.1594749093055725
Batch 40/64 loss: 0.15579688549041748
Batch 41/64 loss: 0.1600438952445984
Batch 42/64 loss: 0.17371928691864014
Batch 43/64 loss: 0.15780770778656006
Batch 44/64 loss: 0.16771233081817627
Batch 45/64 loss: 0.14907509088516235
Batch 46/64 loss: 0.1695033311843872
Batch 47/64 loss: 0.148851215839386
Batch 48/64 loss: 0.16391170024871826
Batch 49/64 loss: 0.16291075944900513
Batch 50/64 loss: 0.15813958644866943
Batch 51/64 loss: 0.1581517457962036
Batch 52/64 loss: 0.157690167427063
Batch 53/64 loss: 0.147161602973938
Batch 54/64 loss: 0.1592826247215271
Batch 55/64 loss: 0.15595638751983643
Batch 56/64 loss: 0.14287197589874268
Batch 57/64 loss: 0.1614254117012024
Batch 58/64 loss: 0.1670764684677124
Batch 59/64 loss: 0.16244018077850342
Batch 60/64 loss: 0.15677624940872192
Batch 61/64 loss: 0.16563951969146729
Batch 62/64 loss: 0.1601264476776123
Batch 63/64 loss: 0.1482234001159668
Batch 64/64 loss: 0.1557619571685791
Epoch 65  Train loss: 0.15963795886320226  Val loss: 0.17894630354294663
Epoch 66
-------------------------------
Batch 1/64 loss: 0.1678786277770996
Batch 2/64 loss: 0.15499985218048096
Batch 3/64 loss: 0.15769809484481812
Batch 4/64 loss: 0.15543919801712036
Batch 5/64 loss: 0.16147726774215698
Batch 6/64 loss: 0.14761006832122803
Batch 7/64 loss: 0.1669396162033081
Batch 8/64 loss: 0.14909225702285767
Batch 9/64 loss: 0.1506863236427307
Batch 10/64 loss: 0.15675848722457886
Batch 11/64 loss: 0.15513527393341064
Batch 12/64 loss: 0.14972972869873047
Batch 13/64 loss: 0.15614384412765503
Batch 14/64 loss: 0.16270935535430908
Batch 15/64 loss: 0.17401957511901855
Batch 16/64 loss: 0.17136770486831665
Batch 17/64 loss: 0.16121196746826172
Batch 18/64 loss: 0.16066449880599976
Batch 19/64 loss: 0.1558123230934143
Batch 20/64 loss: 0.16277027130126953
Batch 21/64 loss: 0.15604203939437866
Batch 22/64 loss: 0.15461742877960205
Batch 23/64 loss: 0.16659307479858398
Batch 24/64 loss: 0.1536598801612854
Batch 25/64 loss: 0.1541234254837036
Batch 26/64 loss: 0.1578485369682312
Batch 27/64 loss: 0.1771807074546814
Batch 28/64 loss: 0.1633702516555786
Batch 29/64 loss: 0.16616475582122803
Batch 30/64 loss: 0.1666245460510254
Batch 31/64 loss: 0.16173690557479858
Batch 32/64 loss: 0.18138301372528076
Batch 33/64 loss: 0.17469561100006104
Batch 34/64 loss: 0.15768367052078247
Batch 35/64 loss: 0.15030032396316528
Batch 36/64 loss: 0.15022790431976318
Batch 37/64 loss: 0.16317957639694214
Batch 38/64 loss: 0.15149641036987305
Batch 39/64 loss: 0.176735520362854
Batch 40/64 loss: 0.16553890705108643
Batch 41/64 loss: 0.1490514874458313
Batch 42/64 loss: 0.16187196969985962
Batch 43/64 loss: 0.14976012706756592
Batch 44/64 loss: 0.17029237747192383
Batch 45/64 loss: 0.1508120894432068
Batch 46/64 loss: 0.16123807430267334
Batch 47/64 loss: 0.15423566102981567
Batch 48/64 loss: 0.16780924797058105
Batch 49/64 loss: 0.16139239072799683
Batch 50/64 loss: 0.15796810388565063
Batch 51/64 loss: 0.15334868431091309
Batch 52/64 loss: 0.1716252565383911
Batch 53/64 loss: 0.16277939081192017
Batch 54/64 loss: 0.177556574344635
Batch 55/64 loss: 0.16757118701934814
Batch 56/64 loss: 0.15995752811431885
Batch 57/64 loss: 0.15934550762176514
Batch 58/64 loss: 0.16979104280471802
Batch 59/64 loss: 0.14848148822784424
Batch 60/64 loss: 0.15863382816314697
Batch 61/64 loss: 0.1583741307258606
Batch 62/64 loss: 0.1544756293296814
Batch 63/64 loss: 0.16014081239700317
Batch 64/64 loss: 0.1569051742553711
Epoch 66  Train loss: 0.16049471836464077  Val loss: 0.17349771515200638
Epoch 67
-------------------------------
Batch 1/64 loss: 0.14800918102264404
Batch 2/64 loss: 0.16900181770324707
Batch 3/64 loss: 0.15962988138198853
Batch 4/64 loss: 0.15887105464935303
Batch 5/64 loss: 0.1434694528579712
Batch 6/64 loss: 0.1455320119857788
Batch 7/64 loss: 0.1652388572692871
Batch 8/64 loss: 0.1561192274093628
Batch 9/64 loss: 0.16638875007629395
Batch 10/64 loss: 0.1611066460609436
Batch 11/64 loss: 0.14422476291656494
Batch 12/64 loss: 0.1511858105659485
Batch 13/64 loss: 0.1541622281074524
Batch 14/64 loss: 0.15478438138961792
Batch 15/64 loss: 0.16288906335830688
Batch 16/64 loss: 0.15791314840316772
Batch 17/64 loss: 0.15989387035369873
Batch 18/64 loss: 0.1725841760635376
Batch 19/64 loss: 0.1541900634765625
Batch 20/64 loss: 0.16634255647659302
Batch 21/64 loss: 0.1593315601348877
Batch 22/64 loss: 0.15311288833618164
Batch 23/64 loss: 0.14038419723510742
Batch 24/64 loss: 0.14839524030685425
Batch 25/64 loss: 0.15572059154510498
Batch 26/64 loss: 0.16085386276245117
Batch 27/64 loss: 0.16296148300170898
Batch 28/64 loss: 0.1547449231147766
Batch 29/64 loss: 0.1588265299797058
Batch 30/64 loss: 0.1619943380355835
Batch 31/64 loss: 0.1586567759513855
Batch 32/64 loss: 0.17652708292007446
Batch 33/64 loss: 0.1672247052192688
Batch 34/64 loss: 0.15700900554656982
Batch 35/64 loss: 0.16135895252227783
Batch 36/64 loss: 0.1631394624710083
Batch 37/64 loss: 0.14977920055389404
Batch 38/64 loss: 0.14546382427215576
Batch 39/64 loss: 0.1722346544265747
Batch 40/64 loss: 0.17616558074951172
Batch 41/64 loss: 0.16964209079742432
Batch 42/64 loss: 0.1504315733909607
Batch 43/64 loss: 0.1425982117652893
Batch 44/64 loss: 0.14225506782531738
Batch 45/64 loss: 0.17023873329162598
Batch 46/64 loss: 0.15845632553100586
Batch 47/64 loss: 0.15154755115509033
Batch 48/64 loss: 0.1656903624534607
Batch 49/64 loss: 0.1482229232788086
Batch 50/64 loss: 0.15791022777557373
Batch 51/64 loss: 0.1562889814376831
Batch 52/64 loss: 0.15145069360733032
Batch 53/64 loss: 0.15420114994049072
Batch 54/64 loss: 0.14990687370300293
Batch 55/64 loss: 0.1517345905303955
Batch 56/64 loss: 0.15851706266403198
Batch 57/64 loss: 0.1512056589126587
Batch 58/64 loss: 0.15559488534927368
Batch 59/64 loss: 0.17463535070419312
Batch 60/64 loss: 0.1425207257270813
Batch 61/64 loss: 0.16243398189544678
Batch 62/64 loss: 0.15281200408935547
Batch 63/64 loss: 0.16637420654296875
Batch 64/64 loss: 0.15412664413452148
Epoch 67  Train loss: 0.15742252574247473  Val loss: 0.16728867575065376
Saving best model, epoch: 67
Epoch 68
-------------------------------
Batch 1/64 loss: 0.1531902551651001
Batch 2/64 loss: 0.15181618928909302
Batch 3/64 loss: 0.144844651222229
Batch 4/64 loss: 0.15379935503005981
Batch 5/64 loss: 0.15371555089950562
Batch 6/64 loss: 0.16364896297454834
Batch 7/64 loss: 0.15332436561584473
Batch 8/64 loss: 0.14837396144866943
Batch 9/64 loss: 0.15858280658721924
Batch 10/64 loss: 0.155925452709198
Batch 11/64 loss: 0.15385276079177856
Batch 12/64 loss: 0.17044878005981445
Batch 13/64 loss: 0.1525336503982544
Batch 14/64 loss: 0.15635275840759277
Batch 15/64 loss: 0.153120756149292
Batch 16/64 loss: 0.1490580439567566
Batch 17/64 loss: 0.14809858798980713
Batch 18/64 loss: 0.1531631350517273
Batch 19/64 loss: 0.1542673110961914
Batch 20/64 loss: 0.15336263179779053
Batch 21/64 loss: 0.15485942363739014
Batch 22/64 loss: 0.15934443473815918
Batch 23/64 loss: 0.16529768705368042
Batch 24/64 loss: 0.15402644872665405
Batch 25/64 loss: 0.1585390567779541
Batch 26/64 loss: 0.14849084615707397
Batch 27/64 loss: 0.15554487705230713
Batch 28/64 loss: 0.15525054931640625
Batch 29/64 loss: 0.14522093534469604
Batch 30/64 loss: 0.15536493062973022
Batch 31/64 loss: 0.14734482765197754
Batch 32/64 loss: 0.14557373523712158
Batch 33/64 loss: 0.15689921379089355
Batch 34/64 loss: 0.14332211017608643
Batch 35/64 loss: 0.15359920263290405
Batch 36/64 loss: 0.1595047116279602
Batch 37/64 loss: 0.15067893266677856
Batch 38/64 loss: 0.16067969799041748
Batch 39/64 loss: 0.15478110313415527
Batch 40/64 loss: 0.14905738830566406
Batch 41/64 loss: 0.14819806814193726
Batch 42/64 loss: 0.15938806533813477
Batch 43/64 loss: 0.1644381880760193
Batch 44/64 loss: 0.15759634971618652
Batch 45/64 loss: 0.15510553121566772
Batch 46/64 loss: 0.15413391590118408
Batch 47/64 loss: 0.16389226913452148
Batch 48/64 loss: 0.15413206815719604
Batch 49/64 loss: 0.16911917924880981
Batch 50/64 loss: 0.15572881698608398
Batch 51/64 loss: 0.16479098796844482
Batch 52/64 loss: 0.16396337747573853
Batch 53/64 loss: 0.1604195237159729
Batch 54/64 loss: 0.15255767107009888
Batch 55/64 loss: 0.15147477388381958
Batch 56/64 loss: 0.1351819634437561
Batch 57/64 loss: 0.15139389038085938
Batch 58/64 loss: 0.1578434705734253
Batch 59/64 loss: 0.16190016269683838
Batch 60/64 loss: 0.1511940360069275
Batch 61/64 loss: 0.1532973051071167
Batch 62/64 loss: 0.1585107445716858
Batch 63/64 loss: 0.17496782541275024
Batch 64/64 loss: 0.17783188819885254
Epoch 68  Train loss: 0.15541117144565955  Val loss: 0.1654356134306524
Saving best model, epoch: 68
Epoch 69
-------------------------------
Batch 1/64 loss: 0.1531052589416504
Batch 2/64 loss: 0.15056490898132324
Batch 3/64 loss: 0.14445310831069946
Batch 4/64 loss: 0.15938091278076172
Batch 5/64 loss: 0.14698421955108643
Batch 6/64 loss: 0.1600557565689087
Batch 7/64 loss: 0.15875506401062012
Batch 8/64 loss: 0.15672403573989868
Batch 9/64 loss: 0.16698694229125977
Batch 10/64 loss: 0.17598426342010498
Batch 11/64 loss: 0.13997673988342285
Batch 12/64 loss: 0.1541966199874878
Batch 13/64 loss: 0.14806270599365234
Batch 14/64 loss: 0.15725332498550415
Batch 15/64 loss: 0.1444607973098755
Batch 16/64 loss: 0.14840376377105713
Batch 17/64 loss: 0.1417781114578247
Batch 18/64 loss: 0.15174049139022827
Batch 19/64 loss: 0.14285415410995483
Batch 20/64 loss: 0.1737789511680603
Batch 21/64 loss: 0.14755511283874512
Batch 22/64 loss: 0.1788313388824463
Batch 23/64 loss: 0.15913647413253784
Batch 24/64 loss: 0.1556451916694641
Batch 25/64 loss: 0.15158545970916748
Batch 26/64 loss: 0.1519259214401245
Batch 27/64 loss: 0.1592181921005249
Batch 28/64 loss: 0.15803349018096924
Batch 29/64 loss: 0.14451837539672852
Batch 30/64 loss: 0.15258628129959106
Batch 31/64 loss: 0.1555483341217041
Batch 32/64 loss: 0.15236949920654297
Batch 33/64 loss: 0.1532772183418274
Batch 34/64 loss: 0.15257805585861206
Batch 35/64 loss: 0.15415418148040771
Batch 36/64 loss: 0.15846455097198486
Batch 37/64 loss: 0.15508019924163818
Batch 38/64 loss: 0.15350842475891113
Batch 39/64 loss: 0.14953196048736572
Batch 40/64 loss: 0.14773207902908325
Batch 41/64 loss: 0.1526658535003662
Batch 42/64 loss: 0.15437448024749756
Batch 43/64 loss: 0.14011281728744507
Batch 44/64 loss: 0.15097808837890625
Batch 45/64 loss: 0.14538109302520752
Batch 46/64 loss: 0.14691734313964844
Batch 47/64 loss: 0.1620781421661377
Batch 48/64 loss: 0.16469484567642212
Batch 49/64 loss: 0.15873974561691284
Batch 50/64 loss: 0.1462080478668213
Batch 51/64 loss: 0.15044552087783813
Batch 52/64 loss: 0.16796839237213135
Batch 53/64 loss: 0.14264732599258423
Batch 54/64 loss: 0.1560598611831665
Batch 55/64 loss: 0.15435373783111572
Batch 56/64 loss: 0.15309679508209229
Batch 57/64 loss: 0.16024082899093628
Batch 58/64 loss: 0.1745445728302002
Batch 59/64 loss: 0.15301436185836792
Batch 60/64 loss: 0.15714198350906372
Batch 61/64 loss: 0.1457679271697998
Batch 62/64 loss: 0.15311062335968018
Batch 63/64 loss: 0.16031062602996826
Batch 64/64 loss: 0.15676558017730713
Epoch 69  Train loss: 0.15427776738709095  Val loss: 0.1698326241109789
Epoch 70
-------------------------------
Batch 1/64 loss: 0.15362465381622314
Batch 2/64 loss: 0.1412995457649231
Batch 3/64 loss: 0.1516571044921875
Batch 4/64 loss: 0.14903497695922852
Batch 5/64 loss: 0.1596510410308838
Batch 6/64 loss: 0.14808839559555054
Batch 7/64 loss: 0.15278387069702148
Batch 8/64 loss: 0.1540488600730896
Batch 9/64 loss: 0.14818888902664185
Batch 10/64 loss: 0.1500331163406372
Batch 11/64 loss: 0.15360796451568604
Batch 12/64 loss: 0.1552051305770874
Batch 13/64 loss: 0.1670604944229126
Batch 14/64 loss: 0.15952813625335693
Batch 15/64 loss: 0.14801806211471558
Batch 16/64 loss: 0.1553691029548645
Batch 17/64 loss: 0.156579852104187
Batch 18/64 loss: 0.13902413845062256
Batch 19/64 loss: 0.14259850978851318
Batch 20/64 loss: 0.15161126852035522
Batch 21/64 loss: 0.14477908611297607
Batch 22/64 loss: 0.14455896615982056
Batch 23/64 loss: 0.14304065704345703
Batch 24/64 loss: 0.14008420705795288
Batch 25/64 loss: 0.15282058715820312
Batch 26/64 loss: 0.14742302894592285
Batch 27/64 loss: 0.15052932500839233
Batch 28/64 loss: 0.1573823094367981
Batch 29/64 loss: 0.15825563669204712
Batch 30/64 loss: 0.1627519130706787
Batch 31/64 loss: 0.1621190309524536
Batch 32/64 loss: 0.14850878715515137
Batch 33/64 loss: 0.16600662469863892
Batch 34/64 loss: 0.14783990383148193
Batch 35/64 loss: 0.14782500267028809
Batch 36/64 loss: 0.16243362426757812
Batch 37/64 loss: 0.14848864078521729
Batch 38/64 loss: 0.14955377578735352
Batch 39/64 loss: 0.1451331377029419
Batch 40/64 loss: 0.1460774540901184
Batch 41/64 loss: 0.15824264287948608
Batch 42/64 loss: 0.1430070400238037
Batch 43/64 loss: 0.15643411874771118
Batch 44/64 loss: 0.1377859115600586
Batch 45/64 loss: 0.1548243761062622
Batch 46/64 loss: 0.14555734395980835
Batch 47/64 loss: 0.1517123579978943
Batch 48/64 loss: 0.14737802743911743
Batch 49/64 loss: 0.14257043600082397
Batch 50/64 loss: 0.1617521047592163
Batch 51/64 loss: 0.14292693138122559
Batch 52/64 loss: 0.13682973384857178
Batch 53/64 loss: 0.16709572076797485
Batch 54/64 loss: 0.14637672901153564
Batch 55/64 loss: 0.15662074089050293
Batch 56/64 loss: 0.14543265104293823
Batch 57/64 loss: 0.16913962364196777
Batch 58/64 loss: 0.14232933521270752
Batch 59/64 loss: 0.1292257308959961
Batch 60/64 loss: 0.13991224765777588
Batch 61/64 loss: 0.16057860851287842
Batch 62/64 loss: 0.15031635761260986
Batch 63/64 loss: 0.1424194574356079
Batch 64/64 loss: 0.13917535543441772
Epoch 70  Train loss: 0.15051724793864232  Val loss: 0.16199970900807595
Saving best model, epoch: 70
Epoch 71
-------------------------------
Batch 1/64 loss: 0.14292901754379272
Batch 2/64 loss: 0.16153788566589355
Batch 3/64 loss: 0.14920204877853394
Batch 4/64 loss: 0.1447000503540039
Batch 5/64 loss: 0.1531994342803955
Batch 6/64 loss: 0.14899873733520508
Batch 7/64 loss: 0.16753637790679932
Batch 8/64 loss: 0.1337132453918457
Batch 9/64 loss: 0.1422911286354065
Batch 10/64 loss: 0.1441907286643982
Batch 11/64 loss: 0.1584082841873169
Batch 12/64 loss: 0.15167665481567383
Batch 13/64 loss: 0.13596391677856445
Batch 14/64 loss: 0.15449118614196777
Batch 15/64 loss: 0.15105831623077393
Batch 16/64 loss: 0.1525404453277588
Batch 17/64 loss: 0.15536564588546753
Batch 18/64 loss: 0.14256244897842407
Batch 19/64 loss: 0.16086030006408691
Batch 20/64 loss: 0.15086901187896729
Batch 21/64 loss: 0.142442524433136
Batch 22/64 loss: 0.13884049654006958
Batch 23/64 loss: 0.1659996509552002
Batch 24/64 loss: 0.1471906304359436
Batch 25/64 loss: 0.155534565448761
Batch 26/64 loss: 0.15558022260665894
Batch 27/64 loss: 0.1376781463623047
Batch 28/64 loss: 0.15499794483184814
Batch 29/64 loss: 0.16135263442993164
Batch 30/64 loss: 0.17178601026535034
Batch 31/64 loss: 0.15676385164260864
Batch 32/64 loss: 0.15045839548110962
Batch 33/64 loss: 0.16332346200942993
Batch 34/64 loss: 0.16317152976989746
Batch 35/64 loss: 0.14353013038635254
Batch 36/64 loss: 0.13914847373962402
Batch 37/64 loss: 0.1426708698272705
Batch 38/64 loss: 0.14897847175598145
Batch 39/64 loss: 0.15797489881515503
Batch 40/64 loss: 0.1460423469543457
Batch 41/64 loss: 0.14570128917694092
Batch 42/64 loss: 0.16237688064575195
Batch 43/64 loss: 0.15444892644882202
Batch 44/64 loss: 0.16054916381835938
Batch 45/64 loss: 0.15052032470703125
Batch 46/64 loss: 0.1531352996826172
Batch 47/64 loss: 0.14782452583312988
Batch 48/64 loss: 0.13473254442214966
Batch 49/64 loss: 0.14632117748260498
Batch 50/64 loss: 0.1478966474533081
Batch 51/64 loss: 0.16181904077529907
Batch 52/64 loss: 0.14783579111099243
Batch 53/64 loss: 0.15518373250961304
Batch 54/64 loss: 0.1445446014404297
Batch 55/64 loss: 0.1533569097518921
Batch 56/64 loss: 0.1487848162651062
Batch 57/64 loss: 0.15340667963027954
Batch 58/64 loss: 0.14154702425003052
Batch 59/64 loss: 0.14225459098815918
Batch 60/64 loss: 0.1452115774154663
Batch 61/64 loss: 0.13385623693466187
Batch 62/64 loss: 0.14563632011413574
Batch 63/64 loss: 0.15566009283065796
Batch 64/64 loss: 0.15718650817871094
Epoch 71  Train loss: 0.15052634033502316  Val loss: 0.16211012569080105
Epoch 72
-------------------------------
Batch 1/64 loss: 0.153680682182312
Batch 2/64 loss: 0.14622074365615845
Batch 3/64 loss: 0.13966959714889526
Batch 4/64 loss: 0.14313745498657227
Batch 5/64 loss: 0.15497499704360962
Batch 6/64 loss: 0.14242255687713623
Batch 7/64 loss: 0.14507216215133667
Batch 8/64 loss: 0.13275295495986938
Batch 9/64 loss: 0.1356000304222107
Batch 10/64 loss: 0.14165759086608887
Batch 11/64 loss: 0.15361148118972778
Batch 12/64 loss: 0.13809287548065186
Batch 13/64 loss: 0.15509992837905884
Batch 14/64 loss: 0.14333021640777588
Batch 15/64 loss: 0.14587372541427612
Batch 16/64 loss: 0.15698152780532837
Batch 17/64 loss: 0.16103899478912354
Batch 18/64 loss: 0.14381074905395508
Batch 19/64 loss: 0.16140532493591309
Batch 20/64 loss: 0.1422891616821289
Batch 21/64 loss: 0.15381181240081787
Batch 22/64 loss: 0.14907783269882202
Batch 23/64 loss: 0.1451423168182373
Batch 24/64 loss: 0.1425178050994873
Batch 25/64 loss: 0.16072291135787964
Batch 26/64 loss: 0.15889954566955566
Batch 27/64 loss: 0.15278953313827515
Batch 28/64 loss: 0.1419561505317688
Batch 29/64 loss: 0.1445174217224121
Batch 30/64 loss: 0.14244449138641357
Batch 31/64 loss: 0.14693289995193481
Batch 32/64 loss: 0.15000712871551514
Batch 33/64 loss: 0.14759254455566406
Batch 34/64 loss: 0.1507173776626587
Batch 35/64 loss: 0.16283392906188965
Batch 36/64 loss: 0.15822720527648926
Batch 37/64 loss: 0.1421995759010315
Batch 38/64 loss: 0.13465774059295654
Batch 39/64 loss: 0.15693241357803345
Batch 40/64 loss: 0.17915523052215576
Batch 41/64 loss: 0.13681626319885254
Batch 42/64 loss: 0.15115869045257568
Batch 43/64 loss: 0.15439105033874512
Batch 44/64 loss: 0.14270293712615967
Batch 45/64 loss: 0.15347057580947876
Batch 46/64 loss: 0.1509827971458435
Batch 47/64 loss: 0.15808963775634766
Batch 48/64 loss: 0.1424245834350586
Batch 49/64 loss: 0.1568862795829773
Batch 50/64 loss: 0.1372692584991455
Batch 51/64 loss: 0.1451295018196106
Batch 52/64 loss: 0.16857922077178955
Batch 53/64 loss: 0.16562002897262573
Batch 54/64 loss: 0.1440800428390503
Batch 55/64 loss: 0.14755117893218994
Batch 56/64 loss: 0.1422520875930786
Batch 57/64 loss: 0.1519574522972107
Batch 58/64 loss: 0.16136634349822998
Batch 59/64 loss: 0.1517302393913269
Batch 60/64 loss: 0.13835126161575317
Batch 61/64 loss: 0.15856611728668213
Batch 62/64 loss: 0.14486610889434814
Batch 63/64 loss: 0.16275548934936523
Batch 64/64 loss: 0.1348123550415039
Epoch 72  Train loss: 0.14942690250920315  Val loss: 0.16106449697435515
Saving best model, epoch: 72
Epoch 73
-------------------------------
Batch 1/64 loss: 0.1450270414352417
Batch 2/64 loss: 0.17958885431289673
Batch 3/64 loss: 0.1523815393447876
Batch 4/64 loss: 0.15377795696258545
Batch 5/64 loss: 0.1499888300895691
Batch 6/64 loss: 0.15213221311569214
Batch 7/64 loss: 0.13824856281280518
Batch 8/64 loss: 0.14934378862380981
Batch 9/64 loss: 0.14734768867492676
Batch 10/64 loss: 0.15193068981170654
Batch 11/64 loss: 0.16307753324508667
Batch 12/64 loss: 0.16717565059661865
Batch 13/64 loss: 0.14123928546905518
Batch 14/64 loss: 0.15215545892715454
Batch 15/64 loss: 0.15224504470825195
Batch 16/64 loss: 0.14112430810928345
Batch 17/64 loss: 0.14619338512420654
Batch 18/64 loss: 0.13833528757095337
Batch 19/64 loss: 0.15376514196395874
Batch 20/64 loss: 0.16061055660247803
Batch 21/64 loss: 0.16604363918304443
Batch 22/64 loss: 0.1506749987602234
Batch 23/64 loss: 0.15319478511810303
Batch 24/64 loss: 0.1610558032989502
Batch 25/64 loss: 0.14043182134628296
Batch 26/64 loss: 0.15332388877868652
Batch 27/64 loss: 0.14007949829101562
Batch 28/64 loss: 0.15140682458877563
Batch 29/64 loss: 0.14356684684753418
Batch 30/64 loss: 0.15600740909576416
Batch 31/64 loss: 0.14667141437530518
Batch 32/64 loss: 0.1528332233428955
Batch 33/64 loss: 0.16043096780776978
Batch 34/64 loss: 0.14378511905670166
Batch 35/64 loss: 0.14657920598983765
Batch 36/64 loss: 0.15801334381103516
Batch 37/64 loss: 0.14922428131103516
Batch 38/64 loss: 0.14787441492080688
Batch 39/64 loss: 0.14511042833328247
Batch 40/64 loss: 0.13650715351104736
Batch 41/64 loss: 0.15408742427825928
Batch 42/64 loss: 0.14475446939468384
Batch 43/64 loss: 0.1397496461868286
Batch 44/64 loss: 0.14295679330825806
Batch 45/64 loss: 0.14080601930618286
Batch 46/64 loss: 0.14883965253829956
Batch 47/64 loss: 0.1586177945137024
Batch 48/64 loss: 0.14150136709213257
Batch 49/64 loss: 0.14997535943984985
Batch 50/64 loss: 0.14532256126403809
Batch 51/64 loss: 0.14897572994232178
Batch 52/64 loss: 0.1441846489906311
Batch 53/64 loss: 0.13554233312606812
Batch 54/64 loss: 0.13471245765686035
Batch 55/64 loss: 0.15082818269729614
Batch 56/64 loss: 0.1721847653388977
Batch 57/64 loss: 0.15393561124801636
Batch 58/64 loss: 0.13765650987625122
Batch 59/64 loss: 0.1371152400970459
Batch 60/64 loss: 0.1468486189842224
Batch 61/64 loss: 0.14879542589187622
Batch 62/64 loss: 0.14903771877288818
Batch 63/64 loss: 0.1453385353088379
Batch 64/64 loss: 0.1476578712463379
Epoch 73  Train loss: 0.1493492416307038  Val loss: 0.15651842265604288
Saving best model, epoch: 73
Epoch 74
-------------------------------
Batch 1/64 loss: 0.13878345489501953
Batch 2/64 loss: 0.14742213487625122
Batch 3/64 loss: 0.155798077583313
Batch 4/64 loss: 0.14243870973587036
Batch 5/64 loss: 0.1616002917289734
Batch 6/64 loss: 0.14526253938674927
Batch 7/64 loss: 0.13272255659103394
Batch 8/64 loss: 0.16063696146011353
Batch 9/64 loss: 0.14364910125732422
Batch 10/64 loss: 0.17459440231323242
Batch 11/64 loss: 0.1402735710144043
Batch 12/64 loss: 0.15589165687561035
Batch 13/64 loss: 0.15412753820419312
Batch 14/64 loss: 0.1376948356628418
Batch 15/64 loss: 0.14943057298660278
Batch 16/64 loss: 0.14246410131454468
Batch 17/64 loss: 0.14104807376861572
Batch 18/64 loss: 0.1479024887084961
Batch 19/64 loss: 0.14362967014312744
Batch 20/64 loss: 0.12663227319717407
Batch 21/64 loss: 0.13183116912841797
Batch 22/64 loss: 0.13852471113204956
Batch 23/64 loss: 0.13261330127716064
Batch 24/64 loss: 0.14355891942977905
Batch 25/64 loss: 0.14118742942810059
Batch 26/64 loss: 0.15294885635375977
Batch 27/64 loss: 0.12922710180282593
Batch 28/64 loss: 0.13087421655654907
Batch 29/64 loss: 0.1628706455230713
Batch 30/64 loss: 0.133020281791687
Batch 31/64 loss: 0.15994417667388916
Batch 32/64 loss: 0.14262717962265015
Batch 33/64 loss: 0.14053523540496826
Batch 34/64 loss: 0.14293557405471802
Batch 35/64 loss: 0.14521878957748413
Batch 36/64 loss: 0.14854305982589722
Batch 37/64 loss: 0.1424732804298401
Batch 38/64 loss: 0.13601213693618774
Batch 39/64 loss: 0.13392537832260132
Batch 40/64 loss: 0.13833695650100708
Batch 41/64 loss: 0.16515636444091797
Batch 42/64 loss: 0.15868115425109863
Batch 43/64 loss: 0.1489381194114685
Batch 44/64 loss: 0.14525586366653442
Batch 45/64 loss: 0.14575082063674927
Batch 46/64 loss: 0.13963156938552856
Batch 47/64 loss: 0.15014135837554932
Batch 48/64 loss: 0.1463320255279541
Batch 49/64 loss: 0.14836126565933228
Batch 50/64 loss: 0.1418548822402954
Batch 51/64 loss: 0.1461331844329834
Batch 52/64 loss: 0.1469849944114685
Batch 53/64 loss: 0.13927119970321655
Batch 54/64 loss: 0.1347244381904602
Batch 55/64 loss: 0.14443200826644897
Batch 56/64 loss: 0.1453160047531128
Batch 57/64 loss: 0.14133518934249878
Batch 58/64 loss: 0.14022743701934814
Batch 59/64 loss: 0.14293265342712402
Batch 60/64 loss: 0.13749539852142334
Batch 61/64 loss: 0.15249794721603394
Batch 62/64 loss: 0.15194177627563477
Batch 63/64 loss: 0.13492822647094727
Batch 64/64 loss: 0.15986180305480957
Epoch 74  Train loss: 0.14490088855519015  Val loss: 0.16331811254376807
Epoch 75
-------------------------------
Batch 1/64 loss: 0.13216227293014526
Batch 2/64 loss: 0.15441268682479858
Batch 3/64 loss: 0.15147161483764648
Batch 4/64 loss: 0.14315837621688843
Batch 5/64 loss: 0.14219379425048828
Batch 6/64 loss: 0.14707213640213013
Batch 7/64 loss: 0.14852946996688843
Batch 8/64 loss: 0.14223623275756836
Batch 9/64 loss: 0.1490023136138916
Batch 10/64 loss: 0.15329289436340332
Batch 11/64 loss: 0.15201354026794434
Batch 12/64 loss: 0.1351865530014038
Batch 13/64 loss: 0.13684839010238647
Batch 14/64 loss: 0.14715981483459473
Batch 15/64 loss: 0.1372370719909668
Batch 16/64 loss: 0.1399068832397461
Batch 17/64 loss: 0.14600884914398193
Batch 18/64 loss: 0.14008498191833496
Batch 19/64 loss: 0.1377173662185669
Batch 20/64 loss: 0.1373119354248047
Batch 21/64 loss: 0.15670567750930786
Batch 22/64 loss: 0.15247845649719238
Batch 23/64 loss: 0.14739811420440674
Batch 24/64 loss: 0.14339792728424072
Batch 25/64 loss: 0.1494065523147583
Batch 26/64 loss: 0.15118813514709473
Batch 27/64 loss: 0.13980042934417725
Batch 28/64 loss: 0.1302015781402588
Batch 29/64 loss: 0.1554916501045227
Batch 30/64 loss: 0.15321695804595947
Batch 31/64 loss: 0.1410638689994812
Batch 32/64 loss: 0.13886260986328125
Batch 33/64 loss: 0.1422121524810791
Batch 34/64 loss: 0.13505315780639648
Batch 35/64 loss: 0.1457000970840454
Batch 36/64 loss: 0.13837194442749023
Batch 37/64 loss: 0.13672411441802979
Batch 38/64 loss: 0.12985128164291382
Batch 39/64 loss: 0.13833129405975342
Batch 40/64 loss: 0.13314104080200195
Batch 41/64 loss: 0.1527193784713745
Batch 42/64 loss: 0.1572808027267456
Batch 43/64 loss: 0.14029145240783691
Batch 44/64 loss: 0.13975375890731812
Batch 45/64 loss: 0.136236310005188
Batch 46/64 loss: 0.13305139541625977
Batch 47/64 loss: 0.13904684782028198
Batch 48/64 loss: 0.1503557562828064
Batch 49/64 loss: 0.13818097114562988
Batch 50/64 loss: 0.15273642539978027
Batch 51/64 loss: 0.13966816663742065
Batch 52/64 loss: 0.15672814846038818
Batch 53/64 loss: 0.14078080654144287
Batch 54/64 loss: 0.151885986328125
Batch 55/64 loss: 0.1549738049507141
Batch 56/64 loss: 0.16208279132843018
Batch 57/64 loss: 0.1339598298072815
Batch 58/64 loss: 0.14270222187042236
Batch 59/64 loss: 0.14828556776046753
Batch 60/64 loss: 0.1486847996711731
Batch 61/64 loss: 0.13226830959320068
Batch 62/64 loss: 0.12465858459472656
Batch 63/64 loss: 0.15612447261810303
Batch 64/64 loss: 0.15135997533798218
Epoch 75  Train loss: 0.14396205158794628  Val loss: 0.15487993623792512
Saving best model, epoch: 75
Epoch 76
-------------------------------
Batch 1/64 loss: 0.14306312799453735
Batch 2/64 loss: 0.15467029809951782
Batch 3/64 loss: 0.13978898525238037
Batch 4/64 loss: 0.1342182159423828
Batch 5/64 loss: 0.13043349981307983
Batch 6/64 loss: 0.1371326446533203
Batch 7/64 loss: 0.14566165208816528
Batch 8/64 loss: 0.14476841688156128
Batch 9/64 loss: 0.15042054653167725
Batch 10/64 loss: 0.14800995588302612
Batch 11/64 loss: 0.1448594331741333
Batch 12/64 loss: 0.13748586177825928
Batch 13/64 loss: 0.14638406038284302
Batch 14/64 loss: 0.15563786029815674
Batch 15/64 loss: 0.15365755558013916
Batch 16/64 loss: 0.13643383979797363
Batch 17/64 loss: 0.15949475765228271
Batch 18/64 loss: 0.1421043872833252
Batch 19/64 loss: 0.1500711441040039
Batch 20/64 loss: 0.14851272106170654
Batch 21/64 loss: 0.14049625396728516
Batch 22/64 loss: 0.14701294898986816
Batch 23/64 loss: 0.14041459560394287
Batch 24/64 loss: 0.14867281913757324
Batch 25/64 loss: 0.15306782722473145
Batch 26/64 loss: 0.1416165828704834
Batch 27/64 loss: 0.14478129148483276
Batch 28/64 loss: 0.14176291227340698
Batch 29/64 loss: 0.13470804691314697
Batch 30/64 loss: 0.1403551697731018
Batch 31/64 loss: 0.1516658067703247
Batch 32/64 loss: 0.1382218599319458
Batch 33/64 loss: 0.14776360988616943
Batch 34/64 loss: 0.13932359218597412
Batch 35/64 loss: 0.15554451942443848
Batch 36/64 loss: 0.14184927940368652
Batch 37/64 loss: 0.1400042176246643
Batch 38/64 loss: 0.15030699968338013
Batch 39/64 loss: 0.13604217767715454
Batch 40/64 loss: 0.1493086814880371
Batch 41/64 loss: 0.15211808681488037
Batch 42/64 loss: 0.14796805381774902
Batch 43/64 loss: 0.14484089612960815
Batch 44/64 loss: 0.1479206681251526
Batch 45/64 loss: 0.13840794563293457
Batch 46/64 loss: 0.13785213232040405
Batch 47/64 loss: 0.14243590831756592
Batch 48/64 loss: 0.1368112564086914
Batch 49/64 loss: 0.1277989149093628
Batch 50/64 loss: 0.14671051502227783
Batch 51/64 loss: 0.14978504180908203
Batch 52/64 loss: 0.14974868297576904
Batch 53/64 loss: 0.14825505018234253
Batch 54/64 loss: 0.12476092576980591
Batch 55/64 loss: 0.14185988903045654
Batch 56/64 loss: 0.14267981052398682
Batch 57/64 loss: 0.14202523231506348
Batch 58/64 loss: 0.13928264379501343
Batch 59/64 loss: 0.1416063904762268
Batch 60/64 loss: 0.1321125030517578
Batch 61/64 loss: 0.13768792152404785
Batch 62/64 loss: 0.1415833830833435
Batch 63/64 loss: 0.1424485445022583
Batch 64/64 loss: 0.13180220127105713
Epoch 76  Train loss: 0.1433926306518854  Val loss: 0.1539346407778894
Saving best model, epoch: 76
Epoch 77
-------------------------------
Batch 1/64 loss: 0.13548344373703003
Batch 2/64 loss: 0.13169395923614502
Batch 3/64 loss: 0.13917207717895508
Batch 4/64 loss: 0.1440334916114807
Batch 5/64 loss: 0.13496637344360352
Batch 6/64 loss: 0.1371772289276123
Batch 7/64 loss: 0.15063118934631348
Batch 8/64 loss: 0.14224636554718018
Batch 9/64 loss: 0.1438460350036621
Batch 10/64 loss: 0.13596487045288086
Batch 11/64 loss: 0.1272227168083191
Batch 12/64 loss: 0.13014042377471924
Batch 13/64 loss: 0.14582431316375732
Batch 14/64 loss: 0.13326501846313477
Batch 15/64 loss: 0.14136886596679688
Batch 16/64 loss: 0.13317179679870605
Batch 17/64 loss: 0.14167344570159912
Batch 18/64 loss: 0.13988977670669556
Batch 19/64 loss: 0.13990825414657593
Batch 20/64 loss: 0.1318373680114746
Batch 21/64 loss: 0.13932478427886963
Batch 22/64 loss: 0.14920103549957275
Batch 23/64 loss: 0.12888705730438232
Batch 24/64 loss: 0.12817180156707764
Batch 25/64 loss: 0.13748329877853394
Batch 26/64 loss: 0.14114904403686523
Batch 27/64 loss: 0.13369321823120117
Batch 28/64 loss: 0.13620108366012573
Batch 29/64 loss: 0.12990719079971313
Batch 30/64 loss: 0.13718748092651367
Batch 31/64 loss: 0.13758373260498047
Batch 32/64 loss: 0.13048386573791504
Batch 33/64 loss: 0.12978094816207886
Batch 34/64 loss: 0.14309418201446533
Batch 35/64 loss: 0.1435183882713318
Batch 36/64 loss: 0.13805562257766724
Batch 37/64 loss: 0.1463785171508789
Batch 38/64 loss: 0.13971686363220215
Batch 39/64 loss: 0.12844860553741455
Batch 40/64 loss: 0.14555513858795166
Batch 41/64 loss: 0.1480005979537964
Batch 42/64 loss: 0.14737671613693237
Batch 43/64 loss: 0.14398527145385742
Batch 44/64 loss: 0.14643502235412598
Batch 45/64 loss: 0.1463150978088379
Batch 46/64 loss: 0.14345431327819824
Batch 47/64 loss: 0.16550707817077637
Batch 48/64 loss: 0.14554011821746826
Batch 49/64 loss: 0.13468360900878906
Batch 50/64 loss: 0.1538318395614624
Batch 51/64 loss: 0.1446819305419922
Batch 52/64 loss: 0.1586843729019165
Batch 53/64 loss: 0.13203215599060059
Batch 54/64 loss: 0.13851457834243774
Batch 55/64 loss: 0.14297765493392944
Batch 56/64 loss: 0.13878709077835083
Batch 57/64 loss: 0.13483083248138428
Batch 58/64 loss: 0.13127613067626953
Batch 59/64 loss: 0.13645124435424805
Batch 60/64 loss: 0.14912092685699463
Batch 61/64 loss: 0.12765192985534668
Batch 62/64 loss: 0.14077651500701904
Batch 63/64 loss: 0.13714063167572021
Batch 64/64 loss: 0.143218994140625
Epoch 77  Train loss: 0.13958915729148716  Val loss: 0.15616316975596845
Epoch 78
-------------------------------
Batch 1/64 loss: 0.13706880807876587
Batch 2/64 loss: 0.13894832134246826
Batch 3/64 loss: 0.14007306098937988
Batch 4/64 loss: 0.13666099309921265
Batch 5/64 loss: 0.1450774073600769
Batch 6/64 loss: 0.133234441280365
Batch 7/64 loss: 0.139812171459198
Batch 8/64 loss: 0.1420394778251648
Batch 9/64 loss: 0.13876879215240479
Batch 10/64 loss: 0.1392725110054016
Batch 11/64 loss: 0.1612604260444641
Batch 12/64 loss: 0.14133822917938232
Batch 13/64 loss: 0.1362917423248291
Batch 14/64 loss: 0.14980554580688477
Batch 15/64 loss: 0.14862191677093506
Batch 16/64 loss: 0.1419852375984192
Batch 17/64 loss: 0.13804954290390015
Batch 18/64 loss: 0.1295502781867981
Batch 19/64 loss: 0.14202040433883667
Batch 20/64 loss: 0.14468121528625488
Batch 21/64 loss: 0.12858539819717407
Batch 22/64 loss: 0.13729655742645264
Batch 23/64 loss: 0.1554061770439148
Batch 24/64 loss: 0.15532726049423218
Batch 25/64 loss: 0.15458828210830688
Batch 26/64 loss: 0.13550829887390137
Batch 27/64 loss: 0.13457435369491577
Batch 28/64 loss: 0.1343734860420227
Batch 29/64 loss: 0.13528192043304443
Batch 30/64 loss: 0.1585041880607605
Batch 31/64 loss: 0.13785052299499512
Batch 32/64 loss: 0.1366228461265564
Batch 33/64 loss: 0.1437472701072693
Batch 34/64 loss: 0.1338944435119629
Batch 35/64 loss: 0.13503962755203247
Batch 36/64 loss: 0.13805150985717773
Batch 37/64 loss: 0.1620522141456604
Batch 38/64 loss: 0.15363174676895142
Batch 39/64 loss: 0.13433605432510376
Batch 40/64 loss: 0.13375341892242432
Batch 41/64 loss: 0.15240871906280518
Batch 42/64 loss: 0.13845866918563843
Batch 43/64 loss: 0.13362514972686768
Batch 44/64 loss: 0.15327036380767822
Batch 45/64 loss: 0.1252000331878662
Batch 46/64 loss: 0.13751381635665894
Batch 47/64 loss: 0.1451132893562317
Batch 48/64 loss: 0.1468348503112793
Batch 49/64 loss: 0.13786041736602783
Batch 50/64 loss: 0.13904297351837158
Batch 51/64 loss: 0.1330432891845703
Batch 52/64 loss: 0.13484758138656616
Batch 53/64 loss: 0.14906048774719238
Batch 54/64 loss: 0.14954960346221924
Batch 55/64 loss: 0.13479524850845337
Batch 56/64 loss: 0.14073079824447632
Batch 57/64 loss: 0.13332313299179077
Batch 58/64 loss: 0.1501637101173401
Batch 59/64 loss: 0.13054072856903076
Batch 60/64 loss: 0.13909775018692017
Batch 61/64 loss: 0.14161336421966553
Batch 62/64 loss: 0.12912046909332275
Batch 63/64 loss: 0.14329254627227783
Batch 64/64 loss: 0.12413758039474487
Epoch 78  Train loss: 0.1407779808137931  Val loss: 0.14986887051887118
Saving best model, epoch: 78
Epoch 79
-------------------------------
Batch 1/64 loss: 0.13478338718414307
Batch 2/64 loss: 0.13774704933166504
Batch 3/64 loss: 0.1343989372253418
Batch 4/64 loss: 0.1479872465133667
Batch 5/64 loss: 0.12492877244949341
Batch 6/64 loss: 0.13613831996917725
Batch 7/64 loss: 0.1345909833908081
Batch 8/64 loss: 0.15719008445739746
Batch 9/64 loss: 0.11987733840942383
Batch 10/64 loss: 0.12962853908538818
Batch 11/64 loss: 0.1347130537033081
Batch 12/64 loss: 0.12449371814727783
Batch 13/64 loss: 0.1383407711982727
Batch 14/64 loss: 0.133231520652771
Batch 15/64 loss: 0.12333160638809204
Batch 16/64 loss: 0.14267665147781372
Batch 17/64 loss: 0.12908029556274414
Batch 18/64 loss: 0.14145421981811523
Batch 19/64 loss: 0.13307911157608032
Batch 20/64 loss: 0.15545767545700073
Batch 21/64 loss: 0.15479522943496704
Batch 22/64 loss: 0.1324068307876587
Batch 23/64 loss: 0.13200825452804565
Batch 24/64 loss: 0.12871873378753662
Batch 25/64 loss: 0.13580995798110962
Batch 26/64 loss: 0.13577985763549805
Batch 27/64 loss: 0.15031683444976807
Batch 28/64 loss: 0.12445485591888428
Batch 29/64 loss: 0.14419668912887573
Batch 30/64 loss: 0.13450664281845093
Batch 31/64 loss: 0.12672924995422363
Batch 32/64 loss: 0.15257209539413452
Batch 33/64 loss: 0.147150456905365
Batch 34/64 loss: 0.12790971994400024
Batch 35/64 loss: 0.12926268577575684
Batch 36/64 loss: 0.13616108894348145
Batch 37/64 loss: 0.12953954935073853
Batch 38/64 loss: 0.1405928134918213
Batch 39/64 loss: 0.12172961235046387
Batch 40/64 loss: 0.12968778610229492
Batch 41/64 loss: 0.12725675106048584
Batch 42/64 loss: 0.14587634801864624
Batch 43/64 loss: 0.14983439445495605
Batch 44/64 loss: 0.13285517692565918
Batch 45/64 loss: 0.14813196659088135
Batch 46/64 loss: 0.1471894383430481
Batch 47/64 loss: 0.13555216789245605
Batch 48/64 loss: 0.13295501470565796
Batch 49/64 loss: 0.1293814778327942
Batch 50/64 loss: 0.1369788646697998
Batch 51/64 loss: 0.16109097003936768
Batch 52/64 loss: 0.13920259475708008
Batch 53/64 loss: 0.14049285650253296
Batch 54/64 loss: 0.1304851770401001
Batch 55/64 loss: 0.15019607543945312
Batch 56/64 loss: 0.14547783136367798
Batch 57/64 loss: 0.13860821723937988
Batch 58/64 loss: 0.14312690496444702
Batch 59/64 loss: 0.13225674629211426
Batch 60/64 loss: 0.15028762817382812
Batch 61/64 loss: 0.12606126070022583
Batch 62/64 loss: 0.1264680027961731
Batch 63/64 loss: 0.15070617198944092
Batch 64/64 loss: 0.13452482223510742
Epoch 79  Train loss: 0.13723645303763596  Val loss: 0.15249174567022683
Epoch 80
-------------------------------
Batch 1/64 loss: 0.15102940797805786
Batch 2/64 loss: 0.13057953119277954
Batch 3/64 loss: 0.12518233060836792
Batch 4/64 loss: 0.14662009477615356
Batch 5/64 loss: 0.12886905670166016
Batch 6/64 loss: 0.1436406970024109
Batch 7/64 loss: 0.1549054980278015
Batch 8/64 loss: 0.13200485706329346
Batch 9/64 loss: 0.13066470623016357
Batch 10/64 loss: 0.13035207986831665
Batch 11/64 loss: 0.14224117994308472
Batch 12/64 loss: 0.13487470149993896
Batch 13/64 loss: 0.14808684587478638
Batch 14/64 loss: 0.13556361198425293
Batch 15/64 loss: 0.13342273235321045
Batch 16/64 loss: 0.1334325671195984
Batch 17/64 loss: 0.14491629600524902
Batch 18/64 loss: 0.12851053476333618
Batch 19/64 loss: 0.14421159029006958
Batch 20/64 loss: 0.13909286260604858
Batch 21/64 loss: 0.13179230690002441
Batch 22/64 loss: 0.1290302276611328
Batch 23/64 loss: 0.1290072202682495
Batch 24/64 loss: 0.12840914726257324
Batch 25/64 loss: 0.14353376626968384
Batch 26/64 loss: 0.12499821186065674
Batch 27/64 loss: 0.13163363933563232
Batch 28/64 loss: 0.12782061100006104
Batch 29/64 loss: 0.13993418216705322
Batch 30/64 loss: 0.1339927315711975
Batch 31/64 loss: 0.12396878004074097
Batch 32/64 loss: 0.13829314708709717
Batch 33/64 loss: 0.1346222162246704
Batch 34/64 loss: 0.12837105989456177
Batch 35/64 loss: 0.13454866409301758
Batch 36/64 loss: 0.13719230890274048
Batch 37/64 loss: 0.1381164789199829
Batch 38/64 loss: 0.14492177963256836
Batch 39/64 loss: 0.15613579750061035
Batch 40/64 loss: 0.15357381105422974
Batch 41/64 loss: 0.1484675407409668
Batch 42/64 loss: 0.13786274194717407
Batch 43/64 loss: 0.12033689022064209
Batch 44/64 loss: 0.1384029984474182
Batch 45/64 loss: 0.1441587209701538
Batch 46/64 loss: 0.1310417652130127
Batch 47/64 loss: 0.1366361379623413
Batch 48/64 loss: 0.13473635911941528
Batch 49/64 loss: 0.1240835189819336
Batch 50/64 loss: 0.14419925212860107
Batch 51/64 loss: 0.1406792402267456
Batch 52/64 loss: 0.13168025016784668
Batch 53/64 loss: 0.14274102449417114
Batch 54/64 loss: 0.14243942499160767
Batch 55/64 loss: 0.14658969640731812
Batch 56/64 loss: 0.14573854207992554
Batch 57/64 loss: 0.12993890047073364
Batch 58/64 loss: 0.1295682191848755
Batch 59/64 loss: 0.14024442434310913
Batch 60/64 loss: 0.13934803009033203
Batch 61/64 loss: 0.1282196044921875
Batch 62/64 loss: 0.13679659366607666
Batch 63/64 loss: 0.13695824146270752
Batch 64/64 loss: 0.15303665399551392
Epoch 80  Train loss: 0.13699988827985876  Val loss: 0.14978343535125052
Saving best model, epoch: 80
Epoch 81
-------------------------------
Batch 1/64 loss: 0.13198179006576538
Batch 2/64 loss: 0.139135479927063
Batch 3/64 loss: 0.1364213228225708
Batch 4/64 loss: 0.13627254962921143
Batch 5/64 loss: 0.14106392860412598
Batch 6/64 loss: 0.13385236263275146
Batch 7/64 loss: 0.12346351146697998
Batch 8/64 loss: 0.13202422857284546
Batch 9/64 loss: 0.1446056365966797
Batch 10/64 loss: 0.1504509449005127
Batch 11/64 loss: 0.13364678621292114
Batch 12/64 loss: 0.13366079330444336
Batch 13/64 loss: 0.12707972526550293
Batch 14/64 loss: 0.12700533866882324
Batch 15/64 loss: 0.1340477466583252
Batch 16/64 loss: 0.134715735912323
Batch 17/64 loss: 0.1231192946434021
Batch 18/64 loss: 0.13794296979904175
Batch 19/64 loss: 0.1287158727645874
Batch 20/64 loss: 0.1172264814376831
Batch 21/64 loss: 0.1372380256652832
Batch 22/64 loss: 0.13980334997177124
Batch 23/64 loss: 0.1368492841720581
Batch 24/64 loss: 0.1368585228919983
Batch 25/64 loss: 0.1279091238975525
Batch 26/64 loss: 0.12767380475997925
Batch 27/64 loss: 0.1228601336479187
Batch 28/64 loss: 0.12096714973449707
Batch 29/64 loss: 0.13861960172653198
Batch 30/64 loss: 0.12306082248687744
Batch 31/64 loss: 0.14869481325149536
Batch 32/64 loss: 0.14174723625183105
Batch 33/64 loss: 0.12679696083068848
Batch 34/64 loss: 0.13130658864974976
Batch 35/64 loss: 0.12604230642318726
Batch 36/64 loss: 0.1351630687713623
Batch 37/64 loss: 0.15977460145950317
Batch 38/64 loss: 0.12212657928466797
Batch 39/64 loss: 0.1292676329612732
Batch 40/64 loss: 0.14920014142990112
Batch 41/64 loss: 0.13430780172348022
Batch 42/64 loss: 0.13642090559005737
Batch 43/64 loss: 0.14940762519836426
Batch 44/64 loss: 0.14377343654632568
Batch 45/64 loss: 0.13559859991073608
Batch 46/64 loss: 0.12096476554870605
Batch 47/64 loss: 0.13066112995147705
Batch 48/64 loss: 0.12904417514801025
Batch 49/64 loss: 0.14264756441116333
Batch 50/64 loss: 0.12406861782073975
Batch 51/64 loss: 0.13652992248535156
Batch 52/64 loss: 0.12464636564254761
Batch 53/64 loss: 0.1366729736328125
Batch 54/64 loss: 0.1408732533454895
Batch 55/64 loss: 0.13253557682037354
Batch 56/64 loss: 0.12031924724578857
Batch 57/64 loss: 0.13941621780395508
Batch 58/64 loss: 0.12011444568634033
Batch 59/64 loss: 0.14146161079406738
Batch 60/64 loss: 0.15263330936431885
Batch 61/64 loss: 0.13715863227844238
Batch 62/64 loss: 0.1362859606742859
Batch 63/64 loss: 0.133192777633667
Batch 64/64 loss: 0.13737553358078003
Epoch 81  Train loss: 0.1341201382524827  Val loss: 0.14917353563701985
Saving best model, epoch: 81
Epoch 82
-------------------------------
Batch 1/64 loss: 0.13888412714004517
Batch 2/64 loss: 0.13261067867279053
Batch 3/64 loss: 0.12046664953231812
Batch 4/64 loss: 0.1328960657119751
Batch 5/64 loss: 0.13313651084899902
Batch 6/64 loss: 0.1376238465309143
Batch 7/64 loss: 0.13558530807495117
Batch 8/64 loss: 0.12478625774383545
Batch 9/64 loss: 0.13358885049819946
Batch 10/64 loss: 0.1426965594291687
Batch 11/64 loss: 0.13357627391815186
Batch 12/64 loss: 0.14385050535202026
Batch 13/64 loss: 0.13041913509368896
Batch 14/64 loss: 0.11919432878494263
Batch 15/64 loss: 0.133459210395813
Batch 16/64 loss: 0.14723283052444458
Batch 17/64 loss: 0.13770663738250732
Batch 18/64 loss: 0.11166632175445557
Batch 19/64 loss: 0.1309400200843811
Batch 20/64 loss: 0.12265932559967041
Batch 21/64 loss: 0.12004691362380981
Batch 22/64 loss: 0.1335078477859497
Batch 23/64 loss: 0.13662797212600708
Batch 24/64 loss: 0.12819629907608032
Batch 25/64 loss: 0.1434665322303772
Batch 26/64 loss: 0.13167345523834229
Batch 27/64 loss: 0.1328294277191162
Batch 28/64 loss: 0.12495607137680054
Batch 29/64 loss: 0.14548492431640625
Batch 30/64 loss: 0.1292303204536438
Batch 31/64 loss: 0.1362343430519104
Batch 32/64 loss: 0.13209569454193115
Batch 33/64 loss: 0.12497788667678833
Batch 34/64 loss: 0.131430983543396
Batch 35/64 loss: 0.12279623746871948
Batch 36/64 loss: 0.13001948595046997
Batch 37/64 loss: 0.12048041820526123
Batch 38/64 loss: 0.1392093300819397
Batch 39/64 loss: 0.13231468200683594
Batch 40/64 loss: 0.1294494867324829
Batch 41/64 loss: 0.134779155254364
Batch 42/64 loss: 0.14888453483581543
Batch 43/64 loss: 0.14276397228240967
Batch 44/64 loss: 0.12817615270614624
Batch 45/64 loss: 0.12292128801345825
Batch 46/64 loss: 0.1343013048171997
Batch 47/64 loss: 0.13223397731781006
Batch 48/64 loss: 0.1285756230354309
Batch 49/64 loss: 0.12818503379821777
Batch 50/64 loss: 0.13736069202423096
Batch 51/64 loss: 0.12046545743942261
Batch 52/64 loss: 0.13445723056793213
Batch 53/64 loss: 0.12394452095031738
Batch 54/64 loss: 0.12480926513671875
Batch 55/64 loss: 0.11907655000686646
Batch 56/64 loss: 0.1291642189025879
Batch 57/64 loss: 0.12800192832946777
Batch 58/64 loss: 0.12492996454238892
Batch 59/64 loss: 0.12736785411834717
Batch 60/64 loss: 0.1280239224433899
Batch 61/64 loss: 0.13405346870422363
Batch 62/64 loss: 0.1349009871482849
Batch 63/64 loss: 0.14583593606948853
Batch 64/64 loss: 0.13936251401901245
Epoch 82  Train loss: 0.1315410616351109  Val loss: 0.15045146044996596
Epoch 83
-------------------------------
Batch 1/64 loss: 0.1313772201538086
Batch 2/64 loss: 0.13044989109039307
Batch 3/64 loss: 0.1230309009552002
Batch 4/64 loss: 0.1471455693244934
Batch 5/64 loss: 0.1300486922264099
Batch 6/64 loss: 0.12804663181304932
Batch 7/64 loss: 0.1353113055229187
Batch 8/64 loss: 0.12057173252105713
Batch 9/64 loss: 0.13322913646697998
Batch 10/64 loss: 0.13646942377090454
Batch 11/64 loss: 0.13168519735336304
Batch 12/64 loss: 0.13848716020584106
Batch 13/64 loss: 0.13927960395812988
Batch 14/64 loss: 0.13410288095474243
Batch 15/64 loss: 0.12915825843811035
Batch 16/64 loss: 0.13608258962631226
Batch 17/64 loss: 0.11207783222198486
Batch 18/64 loss: 0.1316967010498047
Batch 19/64 loss: 0.12383073568344116
Batch 20/64 loss: 0.1297932267189026
Batch 21/64 loss: 0.1341080665588379
Batch 22/64 loss: 0.14199531078338623
Batch 23/64 loss: 0.15376567840576172
Batch 24/64 loss: 0.149483323097229
Batch 25/64 loss: 0.12447994947433472
Batch 26/64 loss: 0.1251124143600464
Batch 27/64 loss: 0.12124252319335938
Batch 28/64 loss: 0.1210484504699707
Batch 29/64 loss: 0.13645780086517334
Batch 30/64 loss: 0.13642030954360962
Batch 31/64 loss: 0.12141752243041992
Batch 32/64 loss: 0.13892072439193726
Batch 33/64 loss: 0.1228337287902832
Batch 34/64 loss: 0.13698983192443848
Batch 35/64 loss: 0.1282086968421936
Batch 36/64 loss: 0.12527477741241455
Batch 37/64 loss: 0.1258862018585205
Batch 38/64 loss: 0.1361328363418579
Batch 39/64 loss: 0.1234443187713623
Batch 40/64 loss: 0.1261976957321167
Batch 41/64 loss: 0.13632112741470337
Batch 42/64 loss: 0.12887251377105713
Batch 43/64 loss: 0.11181586980819702
Batch 44/64 loss: 0.1278122067451477
Batch 45/64 loss: 0.13785654306411743
Batch 46/64 loss: 0.10983502864837646
Batch 47/64 loss: 0.134443461894989
Batch 48/64 loss: 0.13831067085266113
Batch 49/64 loss: 0.15033143758773804
Batch 50/64 loss: 0.11885297298431396
Batch 51/64 loss: 0.12555968761444092
Batch 52/64 loss: 0.13151425123214722
Batch 53/64 loss: 0.1355133056640625
Batch 54/64 loss: 0.12567764520645142
Batch 55/64 loss: 0.13387489318847656
Batch 56/64 loss: 0.14678716659545898
Batch 57/64 loss: 0.13477575778961182
Batch 58/64 loss: 0.13054490089416504
Batch 59/64 loss: 0.1328587532043457
Batch 60/64 loss: 0.14094161987304688
Batch 61/64 loss: 0.13086163997650146
Batch 62/64 loss: 0.13366007804870605
Batch 63/64 loss: 0.14436352252960205
Batch 64/64 loss: 0.11770129203796387
Epoch 83  Train loss: 0.13146597450854733  Val loss: 0.15402304491226615
Epoch 84
-------------------------------
Batch 1/64 loss: 0.12382107973098755
Batch 2/64 loss: 0.11994832754135132
Batch 3/64 loss: 0.11584484577178955
Batch 4/64 loss: 0.11597275733947754
Batch 5/64 loss: 0.13059961795806885
Batch 6/64 loss: 0.13443052768707275
Batch 7/64 loss: 0.14408338069915771
Batch 8/64 loss: 0.13320451974868774
Batch 9/64 loss: 0.13210242986679077
Batch 10/64 loss: 0.14324796199798584
Batch 11/64 loss: 0.11627739667892456
Batch 12/64 loss: 0.12361729145050049
Batch 13/64 loss: 0.14082133769989014
Batch 14/64 loss: 0.1268283724784851
Batch 15/64 loss: 0.13135993480682373
Batch 16/64 loss: 0.1281794309616089
Batch 17/64 loss: 0.12369740009307861
Batch 18/64 loss: 0.13552755117416382
Batch 19/64 loss: 0.12337285280227661
Batch 20/64 loss: 0.14496666193008423
Batch 21/64 loss: 0.1369231939315796
Batch 22/64 loss: 0.13073444366455078
Batch 23/64 loss: 0.14216166734695435
Batch 24/64 loss: 0.13894623517990112
Batch 25/64 loss: 0.13909083604812622
Batch 26/64 loss: 0.13204282522201538
Batch 27/64 loss: 0.1251019835472107
Batch 28/64 loss: 0.13212692737579346
Batch 29/64 loss: 0.1297687292098999
Batch 30/64 loss: 0.13727813959121704
Batch 31/64 loss: 0.11709803342819214
Batch 32/64 loss: 0.133009672164917
Batch 33/64 loss: 0.1345611810684204
Batch 34/64 loss: 0.135820209980011
Batch 35/64 loss: 0.1425454020500183
Batch 36/64 loss: 0.1276882290840149
Batch 37/64 loss: 0.1402980089187622
Batch 38/64 loss: 0.12727057933807373
Batch 39/64 loss: 0.1348562240600586
Batch 40/64 loss: 0.1297844648361206
Batch 41/64 loss: 0.125477135181427
Batch 42/64 loss: 0.14009630680084229
Batch 43/64 loss: 0.12603306770324707
Batch 44/64 loss: 0.12678611278533936
Batch 45/64 loss: 0.11429810523986816
Batch 46/64 loss: 0.1279650330543518
Batch 47/64 loss: 0.13415849208831787
Batch 48/64 loss: 0.13622969388961792
Batch 49/64 loss: 0.1327880620956421
Batch 50/64 loss: 0.12268829345703125
Batch 51/64 loss: 0.1251298189163208
Batch 52/64 loss: 0.13490831851959229
Batch 53/64 loss: 0.12047076225280762
Batch 54/64 loss: 0.13812041282653809
Batch 55/64 loss: 0.13076019287109375
Batch 56/64 loss: 0.13339579105377197
Batch 57/64 loss: 0.12236571311950684
Batch 58/64 loss: 0.12705236673355103
Batch 59/64 loss: 0.12609535455703735
Batch 60/64 loss: 0.12795215845108032
Batch 61/64 loss: 0.12255072593688965
Batch 62/64 loss: 0.1314886212348938
Batch 63/64 loss: 0.1349455714225769
Batch 64/64 loss: 0.11799865961074829
Epoch 84  Train loss: 0.13027867873509724  Val loss: 0.14384848760165708
Saving best model, epoch: 84
Epoch 85
-------------------------------
Batch 1/64 loss: 0.1357700228691101
Batch 2/64 loss: 0.1253969669342041
Batch 3/64 loss: 0.130479633808136
Batch 4/64 loss: 0.14029008150100708
Batch 5/64 loss: 0.11279278993606567
Batch 6/64 loss: 0.1290595531463623
Batch 7/64 loss: 0.14510595798492432
Batch 8/64 loss: 0.12037092447280884
Batch 9/64 loss: 0.12232261896133423
Batch 10/64 loss: 0.12337356805801392
Batch 11/64 loss: 0.13465535640716553
Batch 12/64 loss: 0.12633401155471802
Batch 13/64 loss: 0.12725746631622314
Batch 14/64 loss: 0.12380415201187134
Batch 15/64 loss: 0.1523420214653015
Batch 16/64 loss: 0.12081938982009888
Batch 17/64 loss: 0.13535308837890625
Batch 18/64 loss: 0.13371241092681885
Batch 19/64 loss: 0.1354120969772339
Batch 20/64 loss: 0.12344866991043091
Batch 21/64 loss: 0.1454574465751648
Batch 22/64 loss: 0.12952065467834473
Batch 23/64 loss: 0.1462121605873108
Batch 24/64 loss: 0.12541460990905762
Batch 25/64 loss: 0.14198839664459229
Batch 26/64 loss: 0.12203198671340942
Batch 27/64 loss: 0.12363928556442261
Batch 28/64 loss: 0.12230753898620605
Batch 29/64 loss: 0.1309989094734192
Batch 30/64 loss: 0.11261427402496338
Batch 31/64 loss: 0.1149933934211731
Batch 32/64 loss: 0.11638844013214111
Batch 33/64 loss: 0.11942124366760254
Batch 34/64 loss: 0.15544605255126953
Batch 35/64 loss: 0.1293712854385376
Batch 36/64 loss: 0.11353272199630737
Batch 37/64 loss: 0.1429152488708496
Batch 38/64 loss: 0.12025594711303711
Batch 39/64 loss: 0.12309038639068604
Batch 40/64 loss: 0.1185346245765686
Batch 41/64 loss: 0.1382465362548828
Batch 42/64 loss: 0.1316770315170288
Batch 43/64 loss: 0.131148099899292
Batch 44/64 loss: 0.13204658031463623
Batch 45/64 loss: 0.1407942771911621
Batch 46/64 loss: 0.14294779300689697
Batch 47/64 loss: 0.12663865089416504
Batch 48/64 loss: 0.13286197185516357
Batch 49/64 loss: 0.12535768747329712
Batch 50/64 loss: 0.14142245054244995
Batch 51/64 loss: 0.14078748226165771
Batch 52/64 loss: 0.1368398666381836
Batch 53/64 loss: 0.12345361709594727
Batch 54/64 loss: 0.12555694580078125
Batch 55/64 loss: 0.14094996452331543
Batch 56/64 loss: 0.13125407695770264
Batch 57/64 loss: 0.1340436339378357
Batch 58/64 loss: 0.12072134017944336
Batch 59/64 loss: 0.13447117805480957
Batch 60/64 loss: 0.13871312141418457
Batch 61/64 loss: 0.12336361408233643
Batch 62/64 loss: 0.12352663278579712
Batch 63/64 loss: 0.1457187533378601
Batch 64/64 loss: 0.12778067588806152
Epoch 85  Train loss: 0.13036254434024586  Val loss: 0.14127200657559424
Saving best model, epoch: 85
Epoch 86
-------------------------------
Batch 1/64 loss: 0.13868045806884766
Batch 2/64 loss: 0.1367625594139099
Batch 3/64 loss: 0.13689148426055908
Batch 4/64 loss: 0.1226797103881836
Batch 5/64 loss: 0.12122386693954468
Batch 6/64 loss: 0.14293807744979858
Batch 7/64 loss: 0.13702136278152466
Batch 8/64 loss: 0.11991715431213379
Batch 9/64 loss: 0.12315869331359863
Batch 10/64 loss: 0.11810833215713501
Batch 11/64 loss: 0.13267648220062256
Batch 12/64 loss: 0.12594932317733765
Batch 13/64 loss: 0.1317940354347229
Batch 14/64 loss: 0.12215328216552734
Batch 15/64 loss: 0.12448716163635254
Batch 16/64 loss: 0.12717348337173462
Batch 17/64 loss: 0.13086408376693726
Batch 18/64 loss: 0.12036097049713135
Batch 19/64 loss: 0.12185204029083252
Batch 20/64 loss: 0.12798011302947998
Batch 21/64 loss: 0.1501438021659851
Batch 22/64 loss: 0.1337965726852417
Batch 23/64 loss: 0.128678560256958
Batch 24/64 loss: 0.12879341840744019
Batch 25/64 loss: 0.11884498596191406
Batch 26/64 loss: 0.12805253267288208
Batch 27/64 loss: 0.12261337041854858
Batch 28/64 loss: 0.11931085586547852
Batch 29/64 loss: 0.12569093704223633
Batch 30/64 loss: 0.13166236877441406
Batch 31/64 loss: 0.1433059573173523
Batch 32/64 loss: 0.1154710054397583
Batch 33/64 loss: 0.13341450691223145
Batch 34/64 loss: 0.1152387261390686
Batch 35/64 loss: 0.11885344982147217
Batch 36/64 loss: 0.12156057357788086
Batch 37/64 loss: 0.14241725206375122
Batch 38/64 loss: 0.14259731769561768
Batch 39/64 loss: 0.1287524700164795
Batch 40/64 loss: 0.13994652032852173
Batch 41/64 loss: 0.1327342987060547
Batch 42/64 loss: 0.14052718877792358
Batch 43/64 loss: 0.13260900974273682
Batch 44/64 loss: 0.12516361474990845
Batch 45/64 loss: 0.12953555583953857
Batch 46/64 loss: 0.12610137462615967
Batch 47/64 loss: 0.13138025999069214
Batch 48/64 loss: 0.11880308389663696
Batch 49/64 loss: 0.12640798091888428
Batch 50/64 loss: 0.14227294921875
Batch 51/64 loss: 0.12944084405899048
Batch 52/64 loss: 0.1137964129447937
Batch 53/64 loss: 0.11381691694259644
Batch 54/64 loss: 0.1202738881111145
Batch 55/64 loss: 0.13674026727676392
Batch 56/64 loss: 0.13012313842773438
Batch 57/64 loss: 0.11511802673339844
Batch 58/64 loss: 0.12877172231674194
Batch 59/64 loss: 0.13490092754364014
Batch 60/64 loss: 0.13041454553604126
Batch 61/64 loss: 0.12954449653625488
Batch 62/64 loss: 0.12205278873443604
Batch 63/64 loss: 0.12757688760757446
Batch 64/64 loss: 0.13103550672531128
Epoch 86  Train loss: 0.12844236341177248  Val loss: 0.14013799031575522
Saving best model, epoch: 86
Epoch 87
-------------------------------
Batch 1/64 loss: 0.12907791137695312
Batch 2/64 loss: 0.12628602981567383
Batch 3/64 loss: 0.1276426911354065
Batch 4/64 loss: 0.1393219232559204
Batch 5/64 loss: 0.13014572858810425
Batch 6/64 loss: 0.1259283423423767
Batch 7/64 loss: 0.12832307815551758
Batch 8/64 loss: 0.12747251987457275
Batch 9/64 loss: 0.11924707889556885
Batch 10/64 loss: 0.13364636898040771
Batch 11/64 loss: 0.13259774446487427
Batch 12/64 loss: 0.1235763430595398
Batch 13/64 loss: 0.12137424945831299
Batch 14/64 loss: 0.13570749759674072
Batch 15/64 loss: 0.13571584224700928
Batch 16/64 loss: 0.12112963199615479
Batch 17/64 loss: 0.13887804746627808
Batch 18/64 loss: 0.1223762035369873
Batch 19/64 loss: 0.11935478448867798
Batch 20/64 loss: 0.12731832265853882
Batch 21/64 loss: 0.12045961618423462
Batch 22/64 loss: 0.1308959722518921
Batch 23/64 loss: 0.12207043170928955
Batch 24/64 loss: 0.1350172758102417
Batch 25/64 loss: 0.13842391967773438
Batch 26/64 loss: 0.12884819507598877
Batch 27/64 loss: 0.138469398021698
Batch 28/64 loss: 0.13152670860290527
Batch 29/64 loss: 0.1235000491142273
Batch 30/64 loss: 0.1225963830947876
Batch 31/64 loss: 0.13941919803619385
Batch 32/64 loss: 0.11211436986923218
Batch 33/64 loss: 0.12930291891098022
Batch 34/64 loss: 0.11924290657043457
Batch 35/64 loss: 0.12992233037948608
Batch 36/64 loss: 0.15315651893615723
Batch 37/64 loss: 0.14158600568771362
Batch 38/64 loss: 0.12062418460845947
Batch 39/64 loss: 0.12443220615386963
Batch 40/64 loss: 0.10726571083068848
Batch 41/64 loss: 0.12135475873947144
Batch 42/64 loss: 0.13295114040374756
Batch 43/64 loss: 0.12641888856887817
Batch 44/64 loss: 0.12148386240005493
Batch 45/64 loss: 0.10969096422195435
Batch 46/64 loss: 0.12469065189361572
Batch 47/64 loss: 0.13140946626663208
Batch 48/64 loss: 0.12533217668533325
Batch 49/64 loss: 0.11359930038452148
Batch 50/64 loss: 0.12374109029769897
Batch 51/64 loss: 0.10839825868606567
Batch 52/64 loss: 0.12344920635223389
Batch 53/64 loss: 0.12177109718322754
Batch 54/64 loss: 0.1169886589050293
Batch 55/64 loss: 0.12426978349685669
Batch 56/64 loss: 0.11725789308547974
Batch 57/64 loss: 0.11729496717453003
Batch 58/64 loss: 0.1314079761505127
Batch 59/64 loss: 0.12647032737731934
Batch 60/64 loss: 0.13506227731704712
Batch 61/64 loss: 0.13483166694641113
Batch 62/64 loss: 0.11423128843307495
Batch 63/64 loss: 0.13482141494750977
Batch 64/64 loss: 0.1322508454322815
Epoch 87  Train loss: 0.12658995904174505  Val loss: 0.14305100551585562
Epoch 88
-------------------------------
Batch 1/64 loss: 0.12502729892730713
Batch 2/64 loss: 0.12757039070129395
Batch 3/64 loss: 0.11486977338790894
Batch 4/64 loss: 0.13193529844284058
Batch 5/64 loss: 0.12372362613677979
Batch 6/64 loss: 0.11295676231384277
Batch 7/64 loss: 0.12147653102874756
Batch 8/64 loss: 0.12995320558547974
Batch 9/64 loss: 0.14436984062194824
Batch 10/64 loss: 0.110481858253479
Batch 11/64 loss: 0.12395697832107544
Batch 12/64 loss: 0.1335963010787964
Batch 13/64 loss: 0.1218804121017456
Batch 14/64 loss: 0.11890727281570435
Batch 15/64 loss: 0.11164182424545288
Batch 16/64 loss: 0.128953754901886
Batch 17/64 loss: 0.11367285251617432
Batch 18/64 loss: 0.12802928686141968
Batch 19/64 loss: 0.12842518091201782
Batch 20/64 loss: 0.12337839603424072
Batch 21/64 loss: 0.12664872407913208
Batch 22/64 loss: 0.1406388282775879
Batch 23/64 loss: 0.1304769515991211
Batch 24/64 loss: 0.12597894668579102
Batch 25/64 loss: 0.1307591199874878
Batch 26/64 loss: 0.12210202217102051
Batch 27/64 loss: 0.1277889609336853
Batch 28/64 loss: 0.13134980201721191
Batch 29/64 loss: 0.12608391046524048
Batch 30/64 loss: 0.1181488037109375
Batch 31/64 loss: 0.12134456634521484
Batch 32/64 loss: 0.1231275200843811
Batch 33/64 loss: 0.14279383420944214
Batch 34/64 loss: 0.134088397026062
Batch 35/64 loss: 0.13822424411773682
Batch 36/64 loss: 0.1495506763458252
Batch 37/64 loss: 0.12989330291748047
Batch 38/64 loss: 0.12261319160461426
Batch 39/64 loss: 0.12095463275909424
Batch 40/64 loss: 0.1272047758102417
Batch 41/64 loss: 0.12383776903152466
Batch 42/64 loss: 0.12259042263031006
Batch 43/64 loss: 0.11778074502944946
Batch 44/64 loss: 0.13471674919128418
Batch 45/64 loss: 0.14298135042190552
Batch 46/64 loss: 0.12816286087036133
Batch 47/64 loss: 0.13375818729400635
Batch 48/64 loss: 0.11466479301452637
Batch 49/64 loss: 0.133122980594635
Batch 50/64 loss: 0.11717647314071655
Batch 51/64 loss: 0.13093078136444092
Batch 52/64 loss: 0.12597328424453735
Batch 53/64 loss: 0.11871635913848877
Batch 54/64 loss: 0.14429998397827148
Batch 55/64 loss: 0.12386476993560791
Batch 56/64 loss: 0.11272889375686646
Batch 57/64 loss: 0.10665631294250488
Batch 58/64 loss: 0.12316781282424927
Batch 59/64 loss: 0.1226242184638977
Batch 60/64 loss: 0.12627965211868286
Batch 61/64 loss: 0.14726698398590088
Batch 62/64 loss: 0.13068795204162598
Batch 63/64 loss: 0.11997926235198975
Batch 64/64 loss: 0.12760716676712036
Epoch 88  Train loss: 0.12646669850629919  Val loss: 0.13686057034227037
Saving best model, epoch: 88
Epoch 89
-------------------------------
Batch 1/64 loss: 0.12081944942474365
Batch 2/64 loss: 0.12580204010009766
Batch 3/64 loss: 0.13638699054718018
Batch 4/64 loss: 0.1182870864868164
Batch 5/64 loss: 0.10981869697570801
Batch 6/64 loss: 0.12248915433883667
Batch 7/64 loss: 0.10791784524917603
Batch 8/64 loss: 0.12473887205123901
Batch 9/64 loss: 0.11641597747802734
Batch 10/64 loss: 0.11918938159942627
Batch 11/64 loss: 0.11178779602050781
Batch 12/64 loss: 0.12367695569992065
Batch 13/64 loss: 0.1353214979171753
Batch 14/64 loss: 0.1251927614212036
Batch 15/64 loss: 0.11740374565124512
Batch 16/64 loss: 0.12541109323501587
Batch 17/64 loss: 0.1195911169052124
Batch 18/64 loss: 0.1248544454574585
Batch 19/64 loss: 0.12977153062820435
Batch 20/64 loss: 0.13218235969543457
Batch 21/64 loss: 0.13519734144210815
Batch 22/64 loss: 0.1162412166595459
Batch 23/64 loss: 0.11585110425949097
Batch 24/64 loss: 0.11621719598770142
Batch 25/64 loss: 0.12211191654205322
Batch 26/64 loss: 0.12405258417129517
Batch 27/64 loss: 0.12531572580337524
Batch 28/64 loss: 0.13626688718795776
Batch 29/64 loss: 0.1148756742477417
Batch 30/64 loss: 0.1256105899810791
Batch 31/64 loss: 0.12391328811645508
Batch 32/64 loss: 0.12156760692596436
Batch 33/64 loss: 0.13408678770065308
Batch 34/64 loss: 0.15503418445587158
Batch 35/64 loss: 0.1367606520652771
Batch 36/64 loss: 0.11007100343704224
Batch 37/64 loss: 0.12076294422149658
Batch 38/64 loss: 0.13168716430664062
Batch 39/64 loss: 0.1278870701789856
Batch 40/64 loss: 0.11849731206893921
Batch 41/64 loss: 0.1339077353477478
Batch 42/64 loss: 0.13059979677200317
Batch 43/64 loss: 0.11552244424819946
Batch 44/64 loss: 0.13872045278549194
Batch 45/64 loss: 0.11871218681335449
Batch 46/64 loss: 0.12791025638580322
Batch 47/64 loss: 0.12051016092300415
Batch 48/64 loss: 0.12750399112701416
Batch 49/64 loss: 0.11650311946868896
Batch 50/64 loss: 0.1283373236656189
Batch 51/64 loss: 0.11229586601257324
Batch 52/64 loss: 0.12108480930328369
Batch 53/64 loss: 0.14644575119018555
Batch 54/64 loss: 0.13316452503204346
Batch 55/64 loss: 0.1321779489517212
Batch 56/64 loss: 0.11689984798431396
Batch 57/64 loss: 0.1307762861251831
Batch 58/64 loss: 0.13523739576339722
Batch 59/64 loss: 0.1207929253578186
Batch 60/64 loss: 0.11342722177505493
Batch 61/64 loss: 0.11532992124557495
Batch 62/64 loss: 0.12101298570632935
Batch 63/64 loss: 0.12663769721984863
Batch 64/64 loss: 0.1287516951560974
Epoch 89  Train loss: 0.12453555242688048  Val loss: 0.1413022449745755
Epoch 90
-------------------------------
Batch 1/64 loss: 0.13025939464569092
Batch 2/64 loss: 0.1293047070503235
Batch 3/64 loss: 0.12798172235488892
Batch 4/64 loss: 0.13580232858657837
Batch 5/64 loss: 0.10594433546066284
Batch 6/64 loss: 0.13781023025512695
Batch 7/64 loss: 0.138008713722229
Batch 8/64 loss: 0.11416685581207275
Batch 9/64 loss: 0.12279045581817627
Batch 10/64 loss: 0.13055038452148438
Batch 11/64 loss: 0.12874191999435425
Batch 12/64 loss: 0.1352728009223938
Batch 13/64 loss: 0.12097477912902832
Batch 14/64 loss: 0.13351589441299438
Batch 15/64 loss: 0.11462557315826416
Batch 16/64 loss: 0.12116867303848267
Batch 17/64 loss: 0.11269527673721313
Batch 18/64 loss: 0.11981946229934692
Batch 19/64 loss: 0.11724245548248291
Batch 20/64 loss: 0.12164008617401123
Batch 21/64 loss: 0.12591367959976196
Batch 22/64 loss: 0.13502466678619385
Batch 23/64 loss: 0.12609773874282837
Batch 24/64 loss: 0.13484454154968262
Batch 25/64 loss: 0.10877072811126709
Batch 26/64 loss: 0.14007025957107544
Batch 27/64 loss: 0.1066591739654541
Batch 28/64 loss: 0.13863515853881836
Batch 29/64 loss: 0.11953508853912354
Batch 30/64 loss: 0.13207805156707764
Batch 31/64 loss: 0.10585188865661621
Batch 32/64 loss: 0.13486605882644653
Batch 33/64 loss: 0.13820409774780273
Batch 34/64 loss: 0.13460582494735718
Batch 35/64 loss: 0.11762923002243042
Batch 36/64 loss: 0.12711715698242188
Batch 37/64 loss: 0.12890291213989258
Batch 38/64 loss: 0.11977815628051758
Batch 39/64 loss: 0.11765849590301514
Batch 40/64 loss: 0.11328303813934326
Batch 41/64 loss: 0.12043589353561401
Batch 42/64 loss: 0.11393064260482788
Batch 43/64 loss: 0.11737746000289917
Batch 44/64 loss: 0.12917804718017578
Batch 45/64 loss: 0.14699214696884155
Batch 46/64 loss: 0.10467362403869629
Batch 47/64 loss: 0.10879069566726685
Batch 48/64 loss: 0.11066627502441406
Batch 49/64 loss: 0.10768330097198486
Batch 50/64 loss: 0.13175880908966064
Batch 51/64 loss: 0.1170722246170044
Batch 52/64 loss: 0.11968648433685303
Batch 53/64 loss: 0.11265432834625244
Batch 54/64 loss: 0.12479853630065918
Batch 55/64 loss: 0.12059909105300903
Batch 56/64 loss: 0.11903047561645508
Batch 57/64 loss: 0.12456601858139038
Batch 58/64 loss: 0.11664474010467529
Batch 59/64 loss: 0.10271310806274414
Batch 60/64 loss: 0.11542993783950806
Batch 61/64 loss: 0.13723111152648926
Batch 62/64 loss: 0.13449740409851074
Batch 63/64 loss: 0.12048876285552979
Batch 64/64 loss: 0.10978645086288452
Epoch 90  Train loss: 0.12302872082766364  Val loss: 0.13733002676586925
Epoch 91
-------------------------------
Batch 1/64 loss: 0.11372357606887817
Batch 2/64 loss: 0.12430346012115479
Batch 3/64 loss: 0.12594306468963623
Batch 4/64 loss: 0.13350951671600342
Batch 5/64 loss: 0.11966627836227417
Batch 6/64 loss: 0.11625468730926514
Batch 7/64 loss: 0.1247137188911438
Batch 8/64 loss: 0.11734706163406372
Batch 9/64 loss: 0.111569344997406
Batch 10/64 loss: 0.10474538803100586
Batch 11/64 loss: 0.1246720552444458
Batch 12/64 loss: 0.12461715936660767
Batch 13/64 loss: 0.1312219500541687
Batch 14/64 loss: 0.1330244541168213
Batch 15/64 loss: 0.1082870364189148
Batch 16/64 loss: 0.1428607702255249
Batch 17/64 loss: 0.12792694568634033
Batch 18/64 loss: 0.12066102027893066
Batch 19/64 loss: 0.11376112699508667
Batch 20/64 loss: 0.13211160898208618
Batch 21/64 loss: 0.12153208255767822
Batch 22/64 loss: 0.144214928150177
Batch 23/64 loss: 0.11377078294754028
Batch 24/64 loss: 0.12863540649414062
Batch 25/64 loss: 0.13594788312911987
Batch 26/64 loss: 0.12476283311843872
Batch 27/64 loss: 0.11281812191009521
Batch 28/64 loss: 0.123740553855896
Batch 29/64 loss: 0.1402723789215088
Batch 30/64 loss: 0.1264076828956604
Batch 31/64 loss: 0.12817436456680298
Batch 32/64 loss: 0.1224091649055481
Batch 33/64 loss: 0.11109668016433716
Batch 34/64 loss: 0.13168668746948242
Batch 35/64 loss: 0.11235147714614868
Batch 36/64 loss: 0.1214590072631836
Batch 37/64 loss: 0.11221373081207275
Batch 38/64 loss: 0.11416059732437134
Batch 39/64 loss: 0.11142748594284058
Batch 40/64 loss: 0.13284242153167725
Batch 41/64 loss: 0.12663054466247559
Batch 42/64 loss: 0.11829882860183716
Batch 43/64 loss: 0.11204463243484497
Batch 44/64 loss: 0.12447404861450195
Batch 45/64 loss: 0.1261688470840454
Batch 46/64 loss: 0.11746245622634888
Batch 47/64 loss: 0.12294399738311768
Batch 48/64 loss: 0.11935293674468994
Batch 49/64 loss: 0.12174475193023682
Batch 50/64 loss: 0.11565577983856201
Batch 51/64 loss: 0.13509082794189453
Batch 52/64 loss: 0.11744356155395508
Batch 53/64 loss: 0.11752694845199585
Batch 54/64 loss: 0.12089866399765015
Batch 55/64 loss: 0.12319713830947876
Batch 56/64 loss: 0.12060463428497314
Batch 57/64 loss: 0.11460620164871216
Batch 58/64 loss: 0.11175447702407837
Batch 59/64 loss: 0.12281787395477295
Batch 60/64 loss: 0.12930774688720703
Batch 61/64 loss: 0.1300523281097412
Batch 62/64 loss: 0.11350804567337036
Batch 63/64 loss: 0.1287682056427002
Batch 64/64 loss: 0.11993157863616943
Epoch 91  Train loss: 0.12237092794156541  Val loss: 0.14140362858362623
Epoch 92
-------------------------------
Batch 1/64 loss: 0.12138265371322632
Batch 2/64 loss: 0.12985581159591675
Batch 3/64 loss: 0.10783636569976807
Batch 4/64 loss: 0.12059712409973145
Batch 5/64 loss: 0.14094597101211548
Batch 6/64 loss: 0.11128872632980347
Batch 7/64 loss: 0.1111440658569336
Batch 8/64 loss: 0.11731559038162231
Batch 9/64 loss: 0.13189423084259033
Batch 10/64 loss: 0.14110201597213745
Batch 11/64 loss: 0.109993577003479
Batch 12/64 loss: 0.13716435432434082
Batch 13/64 loss: 0.11503338813781738
Batch 14/64 loss: 0.1201697587966919
Batch 15/64 loss: 0.12455636262893677
Batch 16/64 loss: 0.11313718557357788
Batch 17/64 loss: 0.12188845872879028
Batch 18/64 loss: 0.12631767988204956
Batch 19/64 loss: 0.11951565742492676
Batch 20/64 loss: 0.11786103248596191
Batch 21/64 loss: 0.12749159336090088
Batch 22/64 loss: 0.11235463619232178
Batch 23/64 loss: 0.11829042434692383
Batch 24/64 loss: 0.11284387111663818
Batch 25/64 loss: 0.12241119146347046
Batch 26/64 loss: 0.1056179404258728
Batch 27/64 loss: 0.11772716045379639
Batch 28/64 loss: 0.12002730369567871
Batch 29/64 loss: 0.12297648191452026
Batch 30/64 loss: 0.12317001819610596
Batch 31/64 loss: 0.11444127559661865
Batch 32/64 loss: 0.12860536575317383
Batch 33/64 loss: 0.12865793704986572
Batch 34/64 loss: 0.12317448854446411
Batch 35/64 loss: 0.10948026180267334
Batch 36/64 loss: 0.1153673529624939
Batch 37/64 loss: 0.1255146861076355
Batch 38/64 loss: 0.1192595362663269
Batch 39/64 loss: 0.12320190668106079
Batch 40/64 loss: 0.14227890968322754
Batch 41/64 loss: 0.12604832649230957
Batch 42/64 loss: 0.10842388868331909
Batch 43/64 loss: 0.11204415559768677
Batch 44/64 loss: 0.1252673864364624
Batch 45/64 loss: 0.12301003932952881
Batch 46/64 loss: 0.11653029918670654
Batch 47/64 loss: 0.12807250022888184
Batch 48/64 loss: 0.1363835334777832
Batch 49/64 loss: 0.13188838958740234
Batch 50/64 loss: 0.1158406138420105
Batch 51/64 loss: 0.14463567733764648
Batch 52/64 loss: 0.10830444097518921
Batch 53/64 loss: 0.13019436597824097
Batch 54/64 loss: 0.11596542596817017
Batch 55/64 loss: 0.1118851900100708
Batch 56/64 loss: 0.13238203525543213
Batch 57/64 loss: 0.14294487237930298
Batch 58/64 loss: 0.12043869495391846
Batch 59/64 loss: 0.12937921285629272
Batch 60/64 loss: 0.12560617923736572
Batch 61/64 loss: 0.10290646553039551
Batch 62/64 loss: 0.11555337905883789
Batch 63/64 loss: 0.13095885515213013
Batch 64/64 loss: 0.11843407154083252
Epoch 92  Train loss: 0.12196677385591993  Val loss: 0.1322679638452956
Saving best model, epoch: 92
Epoch 93
-------------------------------
Batch 1/64 loss: 0.11280465126037598
Batch 2/64 loss: 0.11159640550613403
Batch 3/64 loss: 0.11470437049865723
Batch 4/64 loss: 0.12053626775741577
Batch 5/64 loss: 0.12139511108398438
Batch 6/64 loss: 0.12260317802429199
Batch 7/64 loss: 0.11719965934753418
Batch 8/64 loss: 0.116061270236969
Batch 9/64 loss: 0.12520986795425415
Batch 10/64 loss: 0.11144781112670898
Batch 11/64 loss: 0.11849600076675415
Batch 12/64 loss: 0.11608254909515381
Batch 13/64 loss: 0.11072778701782227
Batch 14/64 loss: 0.10780954360961914
Batch 15/64 loss: 0.12017172574996948
Batch 16/64 loss: 0.12149173021316528
Batch 17/64 loss: 0.11301100254058838
Batch 18/64 loss: 0.11063432693481445
Batch 19/64 loss: 0.12267643213272095
Batch 20/64 loss: 0.12380659580230713
Batch 21/64 loss: 0.10892617702484131
Batch 22/64 loss: 0.11614537239074707
Batch 23/64 loss: 0.11566263437271118
Batch 24/64 loss: 0.10380429029464722
Batch 25/64 loss: 0.12437629699707031
Batch 26/64 loss: 0.10974717140197754
Batch 27/64 loss: 0.10521256923675537
Batch 28/64 loss: 0.13367033004760742
Batch 29/64 loss: 0.1262715458869934
Batch 30/64 loss: 0.11318683624267578
Batch 31/64 loss: 0.10815489292144775
Batch 32/64 loss: 0.11672204732894897
Batch 33/64 loss: 0.10933226346969604
Batch 34/64 loss: 0.11812162399291992
Batch 35/64 loss: 0.14921867847442627
Batch 36/64 loss: 0.13262218236923218
Batch 37/64 loss: 0.10627156496047974
Batch 38/64 loss: 0.12262076139450073
Batch 39/64 loss: 0.11662381887435913
Batch 40/64 loss: 0.121024489402771
Batch 41/64 loss: 0.12493401765823364
Batch 42/64 loss: 0.11473637819290161
Batch 43/64 loss: 0.10573852062225342
Batch 44/64 loss: 0.13438934087753296
Batch 45/64 loss: 0.11024713516235352
Batch 46/64 loss: 0.11215388774871826
Batch 47/64 loss: 0.1270599365234375
Batch 48/64 loss: 0.11859828233718872
Batch 49/64 loss: 0.12559980154037476
Batch 50/64 loss: 0.11747884750366211
Batch 51/64 loss: 0.13261133432388306
Batch 52/64 loss: 0.11338049173355103
Batch 53/64 loss: 0.11103242635726929
Batch 54/64 loss: 0.11285793781280518
Batch 55/64 loss: 0.11192810535430908
Batch 56/64 loss: 0.10453146696090698
Batch 57/64 loss: 0.11087965965270996
Batch 58/64 loss: 0.1293032169342041
Batch 59/64 loss: 0.13079941272735596
Batch 60/64 loss: 0.11550265550613403
Batch 61/64 loss: 0.12638527154922485
Batch 62/64 loss: 0.1097116470336914
Batch 63/64 loss: 0.12948817014694214
Batch 64/64 loss: 0.12766140699386597
Epoch 93  Train loss: 0.11798079738429948  Val loss: 0.13263637994982533
Epoch 94
-------------------------------
Batch 1/64 loss: 0.11366897821426392
Batch 2/64 loss: 0.10605865716934204
Batch 3/64 loss: 0.11975818872451782
Batch 4/64 loss: 0.11052495241165161
Batch 5/64 loss: 0.11563748121261597
Batch 6/64 loss: 0.11680412292480469
Batch 7/64 loss: 0.11899137496948242
Batch 8/64 loss: 0.11154603958129883
Batch 9/64 loss: 0.12947118282318115
Batch 10/64 loss: 0.11423367261886597
Batch 11/64 loss: 0.11473846435546875
Batch 12/64 loss: 0.10611832141876221
Batch 13/64 loss: 0.12392741441726685
Batch 14/64 loss: 0.11753910779953003
Batch 15/64 loss: 0.13450688123703003
Batch 16/64 loss: 0.126787006855011
Batch 17/64 loss: 0.1249535083770752
Batch 18/64 loss: 0.1326543092727661
Batch 19/64 loss: 0.1270182728767395
Batch 20/64 loss: 0.1220177412033081
Batch 21/64 loss: 0.13478732109069824
Batch 22/64 loss: 0.12458944320678711
Batch 23/64 loss: 0.12858796119689941
Batch 24/64 loss: 0.11720824241638184
Batch 25/64 loss: 0.10520970821380615
Batch 26/64 loss: 0.10327857732772827
Batch 27/64 loss: 0.11468029022216797
Batch 28/64 loss: 0.11376076936721802
Batch 29/64 loss: 0.1041136384010315
Batch 30/64 loss: 0.12895983457565308
Batch 31/64 loss: 0.11545920372009277
Batch 32/64 loss: 0.12404119968414307
Batch 33/64 loss: 0.11862874031066895
Batch 34/64 loss: 0.11002600193023682
Batch 35/64 loss: 0.10909807682037354
Batch 36/64 loss: 0.12128943204879761
Batch 37/64 loss: 0.1138964295387268
Batch 38/64 loss: 0.11857283115386963
Batch 39/64 loss: 0.11853748559951782
Batch 40/64 loss: 0.10988831520080566
Batch 41/64 loss: 0.12241238355636597
Batch 42/64 loss: 0.13091540336608887
Batch 43/64 loss: 0.11092489957809448
Batch 44/64 loss: 0.12484884262084961
Batch 45/64 loss: 0.12157762050628662
Batch 46/64 loss: 0.1360124945640564
Batch 47/64 loss: 0.13793009519577026
Batch 48/64 loss: 0.12357187271118164
Batch 49/64 loss: 0.1358429193496704
Batch 50/64 loss: 0.11753755807876587
Batch 51/64 loss: 0.1203729510307312
Batch 52/64 loss: 0.12256324291229248
Batch 53/64 loss: 0.12811380624771118
Batch 54/64 loss: 0.11156558990478516
Batch 55/64 loss: 0.12337732315063477
Batch 56/64 loss: 0.12261199951171875
Batch 57/64 loss: 0.13451802730560303
Batch 58/64 loss: 0.12342715263366699
Batch 59/64 loss: 0.12300056219100952
Batch 60/64 loss: 0.10393685102462769
Batch 61/64 loss: 0.12390118837356567
Batch 62/64 loss: 0.12771356105804443
Batch 63/64 loss: 0.10904085636138916
Batch 64/64 loss: 0.12527680397033691
Epoch 94  Train loss: 0.12001951348547842  Val loss: 0.13498841733047642
Epoch 95
-------------------------------
Batch 1/64 loss: 0.12558144330978394
Batch 2/64 loss: 0.11666250228881836
Batch 3/64 loss: 0.11421144008636475
Batch 4/64 loss: 0.12328231334686279
Batch 5/64 loss: 0.12412464618682861
Batch 6/64 loss: 0.1125568151473999
Batch 7/64 loss: 0.11577093601226807
Batch 8/64 loss: 0.11284852027893066
Batch 9/64 loss: 0.11904966831207275
Batch 10/64 loss: 0.11892485618591309
Batch 11/64 loss: 0.11693978309631348
Batch 12/64 loss: 0.11579376459121704
Batch 13/64 loss: 0.1323331594467163
Batch 14/64 loss: 0.11830979585647583
Batch 15/64 loss: 0.12848573923110962
Batch 16/64 loss: 0.12461447715759277
Batch 17/64 loss: 0.11730819940567017
Batch 18/64 loss: 0.11354190111160278
Batch 19/64 loss: 0.11614000797271729
Batch 20/64 loss: 0.1028825044631958
Batch 21/64 loss: 0.10984176397323608
Batch 22/64 loss: 0.12285864353179932
Batch 23/64 loss: 0.11748164892196655
Batch 24/64 loss: 0.13201314210891724
Batch 25/64 loss: 0.11974138021469116
Batch 26/64 loss: 0.12331885099411011
Batch 27/64 loss: 0.10716861486434937
Batch 28/64 loss: 0.12330859899520874
Batch 29/64 loss: 0.12539255619049072
Batch 30/64 loss: 0.12036722898483276
Batch 31/64 loss: 0.13520348072052002
Batch 32/64 loss: 0.1355133056640625
Batch 33/64 loss: 0.11426663398742676
Batch 34/64 loss: 0.12927323579788208
Batch 35/64 loss: 0.12085878849029541
Batch 36/64 loss: 0.11845171451568604
Batch 37/64 loss: 0.12293606996536255
Batch 38/64 loss: 0.12590879201889038
Batch 39/64 loss: 0.11550992727279663
Batch 40/64 loss: 0.112926185131073
Batch 41/64 loss: 0.12388831377029419
Batch 42/64 loss: 0.1215355396270752
Batch 43/64 loss: 0.12502247095108032
Batch 44/64 loss: 0.1289428472518921
Batch 45/64 loss: 0.11399579048156738
Batch 46/64 loss: 0.12648844718933105
Batch 47/64 loss: 0.1196526288986206
Batch 48/64 loss: 0.11778175830841064
Batch 49/64 loss: 0.11933767795562744
Batch 50/64 loss: 0.12597918510437012
Batch 51/64 loss: 0.11812156438827515
Batch 52/64 loss: 0.12216031551361084
Batch 53/64 loss: 0.1361004114151001
Batch 54/64 loss: 0.11654436588287354
Batch 55/64 loss: 0.12374258041381836
Batch 56/64 loss: 0.11794793605804443
Batch 57/64 loss: 0.11054021120071411
Batch 58/64 loss: 0.11052697896957397
Batch 59/64 loss: 0.12993073463439941
Batch 60/64 loss: 0.10485875606536865
Batch 61/64 loss: 0.12242180109024048
Batch 62/64 loss: 0.12554413080215454
Batch 63/64 loss: 0.11271893978118896
Batch 64/64 loss: 0.10281533002853394
Epoch 95  Train loss: 0.12004075167225857  Val loss: 0.1317902371236139
Saving best model, epoch: 95
Epoch 96
-------------------------------
Batch 1/64 loss: 0.12146246433258057
Batch 2/64 loss: 0.11312234401702881
Batch 3/64 loss: 0.1321128010749817
Batch 4/64 loss: 0.12231254577636719
Batch 5/64 loss: 0.12904894351959229
Batch 6/64 loss: 0.12324512004852295
Batch 7/64 loss: 0.10867911577224731
Batch 8/64 loss: 0.102203369140625
Batch 9/64 loss: 0.12520039081573486
Batch 10/64 loss: 0.11768937110900879
Batch 11/64 loss: 0.12015670537948608
Batch 12/64 loss: 0.102012038230896
Batch 13/64 loss: 0.10758894681930542
Batch 14/64 loss: 0.10875797271728516
Batch 15/64 loss: 0.1189224123954773
Batch 16/64 loss: 0.13236618041992188
Batch 17/64 loss: 0.1277121901512146
Batch 18/64 loss: 0.10658848285675049
Batch 19/64 loss: 0.1146116852760315
Batch 20/64 loss: 0.12208247184753418
Batch 21/64 loss: 0.10223263502120972
Batch 22/64 loss: 0.11746209859848022
Batch 23/64 loss: 0.11689668893814087
Batch 24/64 loss: 0.1156761646270752
Batch 25/64 loss: 0.12096893787384033
Batch 26/64 loss: 0.12050819396972656
Batch 27/64 loss: 0.11478722095489502
Batch 28/64 loss: 0.11853587627410889
Batch 29/64 loss: 0.11178523302078247
Batch 30/64 loss: 0.1185997724533081
Batch 31/64 loss: 0.13117176294326782
Batch 32/64 loss: 0.11801737546920776
Batch 33/64 loss: 0.11134201288223267
Batch 34/64 loss: 0.12993508577346802
Batch 35/64 loss: 0.11319684982299805
Batch 36/64 loss: 0.10717535018920898
Batch 37/64 loss: 0.12513208389282227
Batch 38/64 loss: 0.12158364057540894
Batch 39/64 loss: 0.11964893341064453
Batch 40/64 loss: 0.10968047380447388
Batch 41/64 loss: 0.12019526958465576
Batch 42/64 loss: 0.11627703905105591
Batch 43/64 loss: 0.11652672290802002
Batch 44/64 loss: 0.11967289447784424
Batch 45/64 loss: 0.11807477474212646
Batch 46/64 loss: 0.1298156976699829
Batch 47/64 loss: 0.12015998363494873
Batch 48/64 loss: 0.10829049348831177
Batch 49/64 loss: 0.14197474718093872
Batch 50/64 loss: 0.11604940891265869
Batch 51/64 loss: 0.11649888753890991
Batch 52/64 loss: 0.11019951105117798
Batch 53/64 loss: 0.13741666078567505
Batch 54/64 loss: 0.130531907081604
Batch 55/64 loss: 0.13311827182769775
Batch 56/64 loss: 0.12054622173309326
Batch 57/64 loss: 0.11238741874694824
Batch 58/64 loss: 0.12181466817855835
Batch 59/64 loss: 0.11128199100494385
Batch 60/64 loss: 0.11395490169525146
Batch 61/64 loss: 0.11821955442428589
Batch 62/64 loss: 0.12138968706130981
Batch 63/64 loss: 0.11640673875808716
Batch 64/64 loss: 0.10032433271408081
Epoch 96  Train loss: 0.11837268460030649  Val loss: 0.13708124578613595
Epoch 97
-------------------------------
Batch 1/64 loss: 0.14329636096954346
Batch 2/64 loss: 0.12197643518447876
Batch 3/64 loss: 0.13653302192687988
Batch 4/64 loss: 0.12808173894882202
Batch 5/64 loss: 0.11096346378326416
Batch 6/64 loss: 0.12387204170227051
Batch 7/64 loss: 0.11942297220230103
Batch 8/64 loss: 0.13201969861984253
Batch 9/64 loss: 0.14183062314987183
Batch 10/64 loss: 0.12757664918899536
Batch 11/64 loss: 0.1124567985534668
Batch 12/64 loss: 0.11464476585388184
Batch 13/64 loss: 0.13426053524017334
Batch 14/64 loss: 0.10540390014648438
Batch 15/64 loss: 0.11751353740692139
Batch 16/64 loss: 0.10585200786590576
Batch 17/64 loss: 0.1219104528427124
Batch 18/64 loss: 0.12262403964996338
Batch 19/64 loss: 0.12107622623443604
Batch 20/64 loss: 0.11524277925491333
Batch 21/64 loss: 0.11206769943237305
Batch 22/64 loss: 0.12355095148086548
Batch 23/64 loss: 0.12578940391540527
Batch 24/64 loss: 0.12689757347106934
Batch 25/64 loss: 0.11134916543960571
Batch 26/64 loss: 0.12733012437820435
Batch 27/64 loss: 0.11616069078445435
Batch 28/64 loss: 0.11617964506149292
Batch 29/64 loss: 0.11426949501037598
Batch 30/64 loss: 0.11286383867263794
Batch 31/64 loss: 0.12486064434051514
Batch 32/64 loss: 0.11393117904663086
Batch 33/64 loss: 0.11564230918884277
Batch 34/64 loss: 0.12494349479675293
Batch 35/64 loss: 0.113880455493927
Batch 36/64 loss: 0.10615098476409912
Batch 37/64 loss: 0.1135568618774414
Batch 38/64 loss: 0.135209321975708
Batch 39/64 loss: 0.12213873863220215
Batch 40/64 loss: 0.11551612615585327
Batch 41/64 loss: 0.12197232246398926
Batch 42/64 loss: 0.11519408226013184
Batch 43/64 loss: 0.12842804193496704
Batch 44/64 loss: 0.12681496143341064
Batch 45/64 loss: 0.11627137660980225
Batch 46/64 loss: 0.13081759214401245
Batch 47/64 loss: 0.12045067548751831
Batch 48/64 loss: 0.12016183137893677
Batch 49/64 loss: 0.10999280214309692
Batch 50/64 loss: 0.11654627323150635
Batch 51/64 loss: 0.11498099565505981
Batch 52/64 loss: 0.1136581301689148
Batch 53/64 loss: 0.1253575086593628
Batch 54/64 loss: 0.1084831953048706
Batch 55/64 loss: 0.10650217533111572
Batch 56/64 loss: 0.11399787664413452
Batch 57/64 loss: 0.12774819135665894
Batch 58/64 loss: 0.10404562950134277
Batch 59/64 loss: 0.12081789970397949
Batch 60/64 loss: 0.1338375210762024
Batch 61/64 loss: 0.14223182201385498
Batch 62/64 loss: 0.11999678611755371
Batch 63/64 loss: 0.12136095762252808
Batch 64/64 loss: 0.11229020357131958
Epoch 97  Train loss: 0.12035659691866707  Val loss: 0.13943694260521852
Epoch 98
-------------------------------
Batch 1/64 loss: 0.13571494817733765
Batch 2/64 loss: 0.11312514543533325
Batch 3/64 loss: 0.10613042116165161
Batch 4/64 loss: 0.10767340660095215
Batch 5/64 loss: 0.11038804054260254
Batch 6/64 loss: 0.12010526657104492
Batch 7/64 loss: 0.11396360397338867
Batch 8/64 loss: 0.11440253257751465
Batch 9/64 loss: 0.12757736444473267
Batch 10/64 loss: 0.13390201330184937
Batch 11/64 loss: 0.11866223812103271
Batch 12/64 loss: 0.11822080612182617
Batch 13/64 loss: 0.10827171802520752
Batch 14/64 loss: 0.11181771755218506
Batch 15/64 loss: 0.12929320335388184
Batch 16/64 loss: 0.12167847156524658
Batch 17/64 loss: 0.11415815353393555
Batch 18/64 loss: 0.12412416934967041
Batch 19/64 loss: 0.12600648403167725
Batch 20/64 loss: 0.11820399761199951
Batch 21/64 loss: 0.12003934383392334
Batch 22/64 loss: 0.10193294286727905
Batch 23/64 loss: 0.11842381954193115
Batch 24/64 loss: 0.10303932428359985
Batch 25/64 loss: 0.11291676759719849
Batch 26/64 loss: 0.11885702610015869
Batch 27/64 loss: 0.11404234170913696
Batch 28/64 loss: 0.11663305759429932
Batch 29/64 loss: 0.10257166624069214
Batch 30/64 loss: 0.1216273307800293
Batch 31/64 loss: 0.12756484746932983
Batch 32/64 loss: 0.11735403537750244
Batch 33/64 loss: 0.11881673336029053
Batch 34/64 loss: 0.11855655908584595
Batch 35/64 loss: 0.10905295610427856
Batch 36/64 loss: 0.11288255453109741
Batch 37/64 loss: 0.1097070574760437
Batch 38/64 loss: 0.10499250888824463
Batch 39/64 loss: 0.11297750473022461
Batch 40/64 loss: 0.11471682786941528
Batch 41/64 loss: 0.10093027353286743
Batch 42/64 loss: 0.10979306697845459
Batch 43/64 loss: 0.11951547861099243
Batch 44/64 loss: 0.12407428026199341
Batch 45/64 loss: 0.12323927879333496
Batch 46/64 loss: 0.11561673879623413
Batch 47/64 loss: 0.1288306713104248
Batch 48/64 loss: 0.11412829160690308
Batch 49/64 loss: 0.11931765079498291
Batch 50/64 loss: 0.12064480781555176
Batch 51/64 loss: 0.11650228500366211
Batch 52/64 loss: 0.11460047960281372
Batch 53/64 loss: 0.12032347917556763
Batch 54/64 loss: 0.12064474821090698
Batch 55/64 loss: 0.12242472171783447
Batch 56/64 loss: 0.13446485996246338
Batch 57/64 loss: 0.1043136715888977
Batch 58/64 loss: 0.10247457027435303
Batch 59/64 loss: 0.10077619552612305
Batch 60/64 loss: 0.11157602071762085
Batch 61/64 loss: 0.12212979793548584
Batch 62/64 loss: 0.1226205825805664
Batch 63/64 loss: 0.11876410245895386
Batch 64/64 loss: 0.10440903902053833
Epoch 98  Train loss: 0.11633157239240759  Val loss: 0.12661995121703526
Saving best model, epoch: 98
Epoch 99
-------------------------------
Batch 1/64 loss: 0.1191299557685852
Batch 2/64 loss: 0.09423202276229858
Batch 3/64 loss: 0.10700339078903198
Batch 4/64 loss: 0.1183929443359375
Batch 5/64 loss: 0.10654401779174805
Batch 6/64 loss: 0.11586856842041016
Batch 7/64 loss: 0.11524993181228638
Batch 8/64 loss: 0.11508321762084961
Batch 9/64 loss: 0.12996113300323486
Batch 10/64 loss: 0.11305344104766846
Batch 11/64 loss: 0.10701370239257812
Batch 12/64 loss: 0.11564737558364868
Batch 13/64 loss: 0.12875330448150635
Batch 14/64 loss: 0.12809991836547852
Batch 15/64 loss: 0.12177419662475586
Batch 16/64 loss: 0.10651940107345581
Batch 17/64 loss: 0.11339247226715088
Batch 18/64 loss: 0.09903931617736816
Batch 19/64 loss: 0.11420786380767822
Batch 20/64 loss: 0.13417649269104004
Batch 21/64 loss: 0.12613797187805176
Batch 22/64 loss: 0.10369294881820679
Batch 23/64 loss: 0.11736506223678589
Batch 24/64 loss: 0.10734987258911133
Batch 25/64 loss: 0.11885493993759155
Batch 26/64 loss: 0.12051820755004883
Batch 27/64 loss: 0.10523194074630737
Batch 28/64 loss: 0.12210953235626221
Batch 29/64 loss: 0.13408410549163818
Batch 30/64 loss: 0.11260402202606201
Batch 31/64 loss: 0.10664242506027222
Batch 32/64 loss: 0.12092059850692749
Batch 33/64 loss: 0.1058618426322937
Batch 34/64 loss: 0.10990375280380249
Batch 35/64 loss: 0.11624020338058472
Batch 36/64 loss: 0.1229621171951294
Batch 37/64 loss: 0.11184018850326538
Batch 38/64 loss: 0.11235940456390381
Batch 39/64 loss: 0.11923444271087646
Batch 40/64 loss: 0.10619473457336426
Batch 41/64 loss: 0.12098884582519531
Batch 42/64 loss: 0.11432743072509766
Batch 43/64 loss: 0.10975277423858643
Batch 44/64 loss: 0.10225528478622437
Batch 45/64 loss: 0.10849058628082275
Batch 46/64 loss: 0.11031007766723633
Batch 47/64 loss: 0.125230073928833
Batch 48/64 loss: 0.14613878726959229
Batch 49/64 loss: 0.11451601982116699
Batch 50/64 loss: 0.11671942472457886
Batch 51/64 loss: 0.11139994859695435
Batch 52/64 loss: 0.1335512399673462
Batch 53/64 loss: 0.10977411270141602
Batch 54/64 loss: 0.11788880825042725
Batch 55/64 loss: 0.1251542568206787
Batch 56/64 loss: 0.11710202693939209
Batch 57/64 loss: 0.11534780263900757
Batch 58/64 loss: 0.11150866746902466
Batch 59/64 loss: 0.10856884717941284
Batch 60/64 loss: 0.11826837062835693
Batch 61/64 loss: 0.11841875314712524
Batch 62/64 loss: 0.1422353982925415
Batch 63/64 loss: 0.10574012994766235
Batch 64/64 loss: 0.10615700483322144
Epoch 99  Train loss: 0.1158675514015497  Val loss: 0.12999405197261535
Epoch 100
-------------------------------
Batch 1/64 loss: 0.1230286955833435
Batch 2/64 loss: 0.11811685562133789
Batch 3/64 loss: 0.11108243465423584
Batch 4/64 loss: 0.12460851669311523
Batch 5/64 loss: 0.1115877628326416
Batch 6/64 loss: 0.10824781656265259
Batch 7/64 loss: 0.10937708616256714
Batch 8/64 loss: 0.11088615655899048
Batch 9/64 loss: 0.11874890327453613
Batch 10/64 loss: 0.11135977506637573
Batch 11/64 loss: 0.11090582609176636
Batch 12/64 loss: 0.10108917951583862
Batch 13/64 loss: 0.10979646444320679
Batch 14/64 loss: 0.11277872323989868
Batch 15/64 loss: 0.12311381101608276
Batch 16/64 loss: 0.10572439432144165
Batch 17/64 loss: 0.10522806644439697
Batch 18/64 loss: 0.12478494644165039
Batch 19/64 loss: 0.10401856899261475
Batch 20/64 loss: 0.13477206230163574
Batch 21/64 loss: 0.12632620334625244
Batch 22/64 loss: 0.12133216857910156
Batch 23/64 loss: 0.11440396308898926
Batch 24/64 loss: 0.11899799108505249
Batch 25/64 loss: 0.11727547645568848
Batch 26/64 loss: 0.11696124076843262
Batch 27/64 loss: 0.12237906455993652
Batch 28/64 loss: 0.10892283916473389
Batch 29/64 loss: 0.10554248094558716
Batch 30/64 loss: 0.10827445983886719
Batch 31/64 loss: 0.10564577579498291
Batch 32/64 loss: 0.11099117994308472
Batch 33/64 loss: 0.12313950061798096
Batch 34/64 loss: 0.11057460308074951
Batch 35/64 loss: 0.10167771577835083
Batch 36/64 loss: 0.11915278434753418
Batch 37/64 loss: 0.12454801797866821
Batch 38/64 loss: 0.09119540452957153
Batch 39/64 loss: 0.1253480315208435
Batch 40/64 loss: 0.10419386625289917
Batch 41/64 loss: 0.10470974445343018
Batch 42/64 loss: 0.11835026741027832
Batch 43/64 loss: 0.11690670251846313
Batch 44/64 loss: 0.12164980173110962
Batch 45/64 loss: 0.1237567663192749
Batch 46/64 loss: 0.11061346530914307
Batch 47/64 loss: 0.11699825525283813
Batch 48/64 loss: 0.12129092216491699
Batch 49/64 loss: 0.130540132522583
Batch 50/64 loss: 0.12091833353042603
Batch 51/64 loss: 0.11639213562011719
Batch 52/64 loss: 0.11301535367965698
Batch 53/64 loss: 0.12355804443359375
Batch 54/64 loss: 0.11114645004272461
Batch 55/64 loss: 0.1323533058166504
Batch 56/64 loss: 0.10723137855529785
Batch 57/64 loss: 0.11363393068313599
Batch 58/64 loss: 0.11738628149032593
Batch 59/64 loss: 0.11328995227813721
Batch 60/64 loss: 0.10692167282104492
Batch 61/64 loss: 0.11737579107284546
Batch 62/64 loss: 0.12627792358398438
Batch 63/64 loss: 0.10002768039703369
Batch 64/64 loss: 0.10756939649581909
Epoch 100  Train loss: 0.11484172788320803  Val loss: 0.13173826248785064
Epoch 101
-------------------------------
Batch 1/64 loss: 0.11532145738601685
Batch 2/64 loss: 0.11798596382141113
Batch 3/64 loss: 0.12195515632629395
Batch 4/64 loss: 0.111095130443573
Batch 5/64 loss: 0.10937076807022095
Batch 6/64 loss: 0.1273854374885559
Batch 7/64 loss: 0.11079460382461548
Batch 8/64 loss: 0.10867971181869507
Batch 9/64 loss: 0.10171228647232056
Batch 10/64 loss: 0.10092514753341675
Batch 11/64 loss: 0.11918210983276367
Batch 12/64 loss: 0.11179548501968384
Batch 13/64 loss: 0.11303210258483887
Batch 14/64 loss: 0.11474895477294922
Batch 15/64 loss: 0.11247354745864868
Batch 16/64 loss: 0.11405199766159058
Batch 17/64 loss: 0.10388171672821045
Batch 18/64 loss: 0.10686933994293213
Batch 19/64 loss: 0.12748360633850098
Batch 20/64 loss: 0.13098639249801636
Batch 21/64 loss: 0.11282241344451904
Batch 22/64 loss: 0.11919510364532471
Batch 23/64 loss: 0.10380226373672485
Batch 24/64 loss: 0.10552167892456055
Batch 25/64 loss: 0.10930180549621582
Batch 26/64 loss: 0.11044967174530029
Batch 27/64 loss: 0.12283790111541748
Batch 28/64 loss: 0.12679851055145264
Batch 29/64 loss: 0.1051020622253418
Batch 30/64 loss: 0.1332516074180603
Batch 31/64 loss: 0.11534625291824341
Batch 32/64 loss: 0.10357731580734253
Batch 33/64 loss: 0.11738723516464233
Batch 34/64 loss: 0.11760258674621582
Batch 35/64 loss: 0.11896759271621704
Batch 36/64 loss: 0.12437999248504639
Batch 37/64 loss: 0.10808563232421875
Batch 38/64 loss: 0.11340588331222534
Batch 39/64 loss: 0.1108742356300354
Batch 40/64 loss: 0.11392676830291748
Batch 41/64 loss: 0.1270996332168579
Batch 42/64 loss: 0.11076927185058594
Batch 43/64 loss: 0.1119508147239685
Batch 44/64 loss: 0.11954480409622192
Batch 45/64 loss: 0.1111343502998352
Batch 46/64 loss: 0.11680543422698975
Batch 47/64 loss: 0.10944664478302002
Batch 48/64 loss: 0.10898768901824951
Batch 49/64 loss: 0.12977463006973267
Batch 50/64 loss: 0.12574994564056396
Batch 51/64 loss: 0.11314314603805542
Batch 52/64 loss: 0.10344624519348145
Batch 53/64 loss: 0.11074268817901611
Batch 54/64 loss: 0.1115921139717102
Batch 55/64 loss: 0.11439907550811768
Batch 56/64 loss: 0.11328709125518799
Batch 57/64 loss: 0.12491583824157715
Batch 58/64 loss: 0.131128191947937
Batch 59/64 loss: 0.10816067457199097
Batch 60/64 loss: 0.11253249645233154
Batch 61/64 loss: 0.10162347555160522
Batch 62/64 loss: 0.1050528883934021
Batch 63/64 loss: 0.10235744714736938
Batch 64/64 loss: 0.10835886001586914
Epoch 101  Train loss: 0.11415343004114488  Val loss: 0.13181327052952088
Epoch 102
-------------------------------
Batch 1/64 loss: 0.11413103342056274
Batch 2/64 loss: 0.10225588083267212
Batch 3/64 loss: 0.12971055507659912
Batch 4/64 loss: 0.11917722225189209
Batch 5/64 loss: 0.09581351280212402
Batch 6/64 loss: 0.10410439968109131
Batch 7/64 loss: 0.09829723834991455
Batch 8/64 loss: 0.11369967460632324
Batch 9/64 loss: 0.10687917470932007
Batch 10/64 loss: 0.1162421703338623
Batch 11/64 loss: 0.1070130467414856
Batch 12/64 loss: 0.11312443017959595
Batch 13/64 loss: 0.10740876197814941
Batch 14/64 loss: 0.11517179012298584
Batch 15/64 loss: 0.1226651668548584
Batch 16/64 loss: 0.11217957735061646
Batch 17/64 loss: 0.11317473649978638
Batch 18/64 loss: 0.1335662603378296
Batch 19/64 loss: 0.10272210836410522
Batch 20/64 loss: 0.10432004928588867
Batch 21/64 loss: 0.11275613307952881
Batch 22/64 loss: 0.10636794567108154
Batch 23/64 loss: 0.11830908060073853
Batch 24/64 loss: 0.09983313083648682
Batch 25/64 loss: 0.12267690896987915
Batch 26/64 loss: 0.11606311798095703
Batch 27/64 loss: 0.12791508436203003
Batch 28/64 loss: 0.12157756090164185
Batch 29/64 loss: 0.10788005590438843
Batch 30/64 loss: 0.11690032482147217
Batch 31/64 loss: 0.1146078109741211
Batch 32/64 loss: 0.10960119962692261
Batch 33/64 loss: 0.12617236375808716
Batch 34/64 loss: 0.10161358118057251
Batch 35/64 loss: 0.10951268672943115
Batch 36/64 loss: 0.09808611869812012
Batch 37/64 loss: 0.10899943113327026
Batch 38/64 loss: 0.12897098064422607
Batch 39/64 loss: 0.11419451236724854
Batch 40/64 loss: 0.12632805109024048
Batch 41/64 loss: 0.10447824001312256
Batch 42/64 loss: 0.11057025194168091
Batch 43/64 loss: 0.09914600849151611
Batch 44/64 loss: 0.09900379180908203
Batch 45/64 loss: 0.10312449932098389
Batch 46/64 loss: 0.1066586971282959
Batch 47/64 loss: 0.10631310939788818
Batch 48/64 loss: 0.11344653367996216
Batch 49/64 loss: 0.11438357830047607
Batch 50/64 loss: 0.11160171031951904
Batch 51/64 loss: 0.10744160413742065
Batch 52/64 loss: 0.11732792854309082
Batch 53/64 loss: 0.1115342378616333
Batch 54/64 loss: 0.1294727325439453
Batch 55/64 loss: 0.11428987979888916
Batch 56/64 loss: 0.10487866401672363
Batch 57/64 loss: 0.10806262493133545
Batch 58/64 loss: 0.10964572429656982
Batch 59/64 loss: 0.11730855703353882
Batch 60/64 loss: 0.10595434904098511
Batch 61/64 loss: 0.10949856042861938
Batch 62/64 loss: 0.12307614088058472
Batch 63/64 loss: 0.11350107192993164
Batch 64/64 loss: 0.12670093774795532
Epoch 102  Train loss: 0.11224732656104892  Val loss: 0.12506743927591854
Saving best model, epoch: 102
Epoch 103
-------------------------------
Batch 1/64 loss: 0.1160893440246582
Batch 2/64 loss: 0.10605520009994507
Batch 3/64 loss: 0.1093512773513794
Batch 4/64 loss: 0.1068044900894165
Batch 5/64 loss: 0.10857892036437988
Batch 6/64 loss: 0.11504960060119629
Batch 7/64 loss: 0.10106450319290161
Batch 8/64 loss: 0.11354285478591919
Batch 9/64 loss: 0.09550440311431885
Batch 10/64 loss: 0.11210894584655762
Batch 11/64 loss: 0.10921716690063477
Batch 12/64 loss: 0.09896087646484375
Batch 13/64 loss: 0.11356467008590698
Batch 14/64 loss: 0.11687785387039185
Batch 15/64 loss: 0.11153542995452881
Batch 16/64 loss: 0.10051286220550537
Batch 17/64 loss: 0.1184471845626831
Batch 18/64 loss: 0.10798501968383789
Batch 19/64 loss: 0.10017287731170654
Batch 20/64 loss: 0.12334024906158447
Batch 21/64 loss: 0.10895687341690063
Batch 22/64 loss: 0.12227654457092285
Batch 23/64 loss: 0.11334604024887085
Batch 24/64 loss: 0.12603193521499634
Batch 25/64 loss: 0.10755980014801025
Batch 26/64 loss: 0.10863786935806274
Batch 27/64 loss: 0.1076364517211914
Batch 28/64 loss: 0.11118537187576294
Batch 29/64 loss: 0.10574281215667725
Batch 30/64 loss: 0.10881853103637695
Batch 31/64 loss: 0.10550075769424438
Batch 32/64 loss: 0.10901987552642822
Batch 33/64 loss: 0.11789464950561523
Batch 34/64 loss: 0.10841506719589233
Batch 35/64 loss: 0.11307728290557861
Batch 36/64 loss: 0.10409349203109741
Batch 37/64 loss: 0.1079789400100708
Batch 38/64 loss: 0.10972428321838379
Batch 39/64 loss: 0.10602104663848877
Batch 40/64 loss: 0.11950314044952393
Batch 41/64 loss: 0.11868464946746826
Batch 42/64 loss: 0.10968440771102905
Batch 43/64 loss: 0.11881792545318604
Batch 44/64 loss: 0.10908329486846924
Batch 45/64 loss: 0.10995054244995117
Batch 46/64 loss: 0.11354678869247437
Batch 47/64 loss: 0.11216723918914795
Batch 48/64 loss: 0.11017602682113647
Batch 49/64 loss: 0.11910748481750488
Batch 50/64 loss: 0.12307173013687134
Batch 51/64 loss: 0.11202859878540039
Batch 52/64 loss: 0.11541098356246948
Batch 53/64 loss: 0.1179153323173523
Batch 54/64 loss: 0.10836982727050781
Batch 55/64 loss: 0.12009531259536743
Batch 56/64 loss: 0.12528502941131592
Batch 57/64 loss: 0.10295045375823975
Batch 58/64 loss: 0.1155286431312561
Batch 59/64 loss: 0.11887073516845703
Batch 60/64 loss: 0.10912132263183594
Batch 61/64 loss: 0.1385657787322998
Batch 62/64 loss: 0.10080349445343018
Batch 63/64 loss: 0.11666655540466309
Batch 64/64 loss: 0.12541645765304565
Epoch 103  Train loss: 0.11209645481670603  Val loss: 0.13621282147378036
Epoch 104
-------------------------------
Batch 1/64 loss: 0.12144237756729126
Batch 2/64 loss: 0.10324859619140625
Batch 3/64 loss: 0.11709117889404297
Batch 4/64 loss: 0.11745792627334595
Batch 5/64 loss: 0.10997170209884644
Batch 6/64 loss: 0.10642528533935547
Batch 7/64 loss: 0.09611165523529053
Batch 8/64 loss: 0.11667525768280029
Batch 9/64 loss: 0.09706968069076538
Batch 10/64 loss: 0.1167149543762207
Batch 11/64 loss: 0.11542117595672607
Batch 12/64 loss: 0.10478132963180542
Batch 13/64 loss: 0.10844659805297852
Batch 14/64 loss: 0.12381154298782349
Batch 15/64 loss: 0.0946875810623169
Batch 16/64 loss: 0.12202227115631104
Batch 17/64 loss: 0.1023741364479065
Batch 18/64 loss: 0.11293751001358032
Batch 19/64 loss: 0.11911451816558838
Batch 20/64 loss: 0.13487333059310913
Batch 21/64 loss: 0.1195191740989685
Batch 22/64 loss: 0.11881428956985474
Batch 23/64 loss: 0.09750068187713623
Batch 24/64 loss: 0.10814732313156128
Batch 25/64 loss: 0.11601078510284424
Batch 26/64 loss: 0.12409484386444092
Batch 27/64 loss: 0.10902047157287598
Batch 28/64 loss: 0.1063154935836792
Batch 29/64 loss: 0.11227720975875854
Batch 30/64 loss: 0.09541714191436768
Batch 31/64 loss: 0.1116485595703125
Batch 32/64 loss: 0.11785030364990234
Batch 33/64 loss: 0.10177910327911377
Batch 34/64 loss: 0.10894155502319336
Batch 35/64 loss: 0.11696350574493408
Batch 36/64 loss: 0.09966105222702026
Batch 37/64 loss: 0.10068398714065552
Batch 38/64 loss: 0.10561412572860718
Batch 39/64 loss: 0.11872220039367676
Batch 40/64 loss: 0.1133338212966919
Batch 41/64 loss: 0.1240963339805603
Batch 42/64 loss: 0.10649251937866211
Batch 43/64 loss: 0.11079496145248413
Batch 44/64 loss: 0.10318487882614136
Batch 45/64 loss: 0.1127355694770813
Batch 46/64 loss: 0.10493969917297363
Batch 47/64 loss: 0.12063270807266235
Batch 48/64 loss: 0.11896586418151855
Batch 49/64 loss: 0.10640907287597656
Batch 50/64 loss: 0.10404038429260254
Batch 51/64 loss: 0.11301404237747192
Batch 52/64 loss: 0.10856205224990845
Batch 53/64 loss: 0.1117403507232666
Batch 54/64 loss: 0.10433536767959595
Batch 55/64 loss: 0.11746305227279663
Batch 56/64 loss: 0.13246029615402222
Batch 57/64 loss: 0.10604459047317505
Batch 58/64 loss: 0.1213422417640686
Batch 59/64 loss: 0.10096895694732666
Batch 60/64 loss: 0.121193528175354
Batch 61/64 loss: 0.12255674600601196
Batch 62/64 loss: 0.11822140216827393
Batch 63/64 loss: 0.104836106300354
Batch 64/64 loss: 0.1151077151298523
Epoch 104  Train loss: 0.1117545999732672  Val loss: 0.12345472582427088
Saving best model, epoch: 104
Epoch 105
-------------------------------
Batch 1/64 loss: 0.10917973518371582
Batch 2/64 loss: 0.10980820655822754
Batch 3/64 loss: 0.10107523202896118
Batch 4/64 loss: 0.1440584659576416
Batch 5/64 loss: 0.11753135919570923
Batch 6/64 loss: 0.12325912714004517
Batch 7/64 loss: 0.09646183252334595
Batch 8/64 loss: 0.1178663969039917
Batch 9/64 loss: 0.12082767486572266
Batch 10/64 loss: 0.10768747329711914
Batch 11/64 loss: 0.11085999011993408
Batch 12/64 loss: 0.10800755023956299
Batch 13/64 loss: 0.1021583080291748
Batch 14/64 loss: 0.11260372400283813
Batch 15/64 loss: 0.09672212600708008
Batch 16/64 loss: 0.09876418113708496
Batch 17/64 loss: 0.11059969663619995
Batch 18/64 loss: 0.11961674690246582
Batch 19/64 loss: 0.11350405216217041
Batch 20/64 loss: 0.10478979349136353
Batch 21/64 loss: 0.12704765796661377
Batch 22/64 loss: 0.1076970100402832
Batch 23/64 loss: 0.11777418851852417
Batch 24/64 loss: 0.11536568403244019
Batch 25/64 loss: 0.10412847995758057
Batch 26/64 loss: 0.11253297328948975
Batch 27/64 loss: 0.1138957142829895
Batch 28/64 loss: 0.12638550996780396
Batch 29/64 loss: 0.11116218566894531
Batch 30/64 loss: 0.10057765245437622
Batch 31/64 loss: 0.10253018140792847
Batch 32/64 loss: 0.13569217920303345
Batch 33/64 loss: 0.12018442153930664
Batch 34/64 loss: 0.09959828853607178
Batch 35/64 loss: 0.11344277858734131
Batch 36/64 loss: 0.10989433526992798
Batch 37/64 loss: 0.10333162546157837
Batch 38/64 loss: 0.1080898642539978
Batch 39/64 loss: 0.10333359241485596
Batch 40/64 loss: 0.09516072273254395
Batch 41/64 loss: 0.10614168643951416
Batch 42/64 loss: 0.11583322286605835
Batch 43/64 loss: 0.10901463031768799
Batch 44/64 loss: 0.11192190647125244
Batch 45/64 loss: 0.11095190048217773
Batch 46/64 loss: 0.09556961059570312
Batch 47/64 loss: 0.1121944785118103
Batch 48/64 loss: 0.1017569899559021
Batch 49/64 loss: 0.09798175096511841
Batch 50/64 loss: 0.09674191474914551
Batch 51/64 loss: 0.11394226551055908
Batch 52/64 loss: 0.10569113492965698
Batch 53/64 loss: 0.11852872371673584
Batch 54/64 loss: 0.1085885763168335
Batch 55/64 loss: 0.12054896354675293
Batch 56/64 loss: 0.11225980520248413
Batch 57/64 loss: 0.11317634582519531
Batch 58/64 loss: 0.10860216617584229
Batch 59/64 loss: 0.11145353317260742
Batch 60/64 loss: 0.1073656678199768
Batch 61/64 loss: 0.09707260131835938
Batch 62/64 loss: 0.11044138669967651
Batch 63/64 loss: 0.11057621240615845
Batch 64/64 loss: 0.11481618881225586
Epoch 105  Train loss: 0.11036351895799824  Val loss: 0.12628628564454436
Epoch 106
-------------------------------
Batch 1/64 loss: 0.11609941720962524
Batch 2/64 loss: 0.11442738771438599
Batch 3/64 loss: 0.09974789619445801
Batch 4/64 loss: 0.10681802034378052
Batch 5/64 loss: 0.11357879638671875
Batch 6/64 loss: 0.10823231935501099
Batch 7/64 loss: 0.1078336238861084
Batch 8/64 loss: 0.12355756759643555
Batch 9/64 loss: 0.10634970664978027
Batch 10/64 loss: 0.11881661415100098
Batch 11/64 loss: 0.11011219024658203
Batch 12/64 loss: 0.1137169599533081
Batch 13/64 loss: 0.09682339429855347
Batch 14/64 loss: 0.11502420902252197
Batch 15/64 loss: 0.11581283807754517
Batch 16/64 loss: 0.11327815055847168
Batch 17/64 loss: 0.09249734878540039
Batch 18/64 loss: 0.09868031740188599
Batch 19/64 loss: 0.1161048412322998
Batch 20/64 loss: 0.10035395622253418
Batch 21/64 loss: 0.1198996901512146
Batch 22/64 loss: 0.11816620826721191
Batch 23/64 loss: 0.11435329914093018
Batch 24/64 loss: 0.11245942115783691
Batch 25/64 loss: 0.11661618947982788
Batch 26/64 loss: 0.10371315479278564
Batch 27/64 loss: 0.11445504426956177
Batch 28/64 loss: 0.11216753721237183
Batch 29/64 loss: 0.11177575588226318
Batch 30/64 loss: 0.10352432727813721
Batch 31/64 loss: 0.11842525005340576
Batch 32/64 loss: 0.09147334098815918
Batch 33/64 loss: 0.10014647245407104
Batch 34/64 loss: 0.10125577449798584
Batch 35/64 loss: 0.09724456071853638
Batch 36/64 loss: 0.10159069299697876
Batch 37/64 loss: 0.11073505878448486
Batch 38/64 loss: 0.10244059562683105
Batch 39/64 loss: 0.09754180908203125
Batch 40/64 loss: 0.11698740720748901
Batch 41/64 loss: 0.11001265048980713
Batch 42/64 loss: 0.11560720205307007
Batch 43/64 loss: 0.10830199718475342
Batch 44/64 loss: 0.11409139633178711
Batch 45/64 loss: 0.1269984245300293
Batch 46/64 loss: 0.09415590763092041
Batch 47/64 loss: 0.11553943157196045
Batch 48/64 loss: 0.1176912784576416
Batch 49/64 loss: 0.11000120639801025
Batch 50/64 loss: 0.1086086630821228
Batch 51/64 loss: 0.10473662614822388
Batch 52/64 loss: 0.11432325839996338
Batch 53/64 loss: 0.09987533092498779
Batch 54/64 loss: 0.1056528091430664
Batch 55/64 loss: 0.10334783792495728
Batch 56/64 loss: 0.0920146107673645
Batch 57/64 loss: 0.11951398849487305
Batch 58/64 loss: 0.12521332502365112
Batch 59/64 loss: 0.10482025146484375
Batch 60/64 loss: 0.11280685663223267
Batch 61/64 loss: 0.10314357280731201
Batch 62/64 loss: 0.11390703916549683
Batch 63/64 loss: 0.10131579637527466
Batch 64/64 loss: 0.11177515983581543
Epoch 106  Train loss: 0.10915055648953306  Val loss: 0.12443369053483419
Epoch 107
-------------------------------
Batch 1/64 loss: 0.1098182201385498
Batch 2/64 loss: 0.10364413261413574
Batch 3/64 loss: 0.10571438074111938
Batch 4/64 loss: 0.1369428038597107
Batch 5/64 loss: 0.11722922325134277
Batch 6/64 loss: 0.11138302087783813
Batch 7/64 loss: 0.10798907279968262
Batch 8/64 loss: 0.10215204954147339
Batch 9/64 loss: 0.11096864938735962
Batch 10/64 loss: 0.12365114688873291
Batch 11/64 loss: 0.09760540723800659
Batch 12/64 loss: 0.1107589602470398
Batch 13/64 loss: 0.09925377368927002
Batch 14/64 loss: 0.10504287481307983
Batch 15/64 loss: 0.1090167760848999
Batch 16/64 loss: 0.11694842576980591
Batch 17/64 loss: 0.10870856046676636
Batch 18/64 loss: 0.11999034881591797
Batch 19/64 loss: 0.11135411262512207
Batch 20/64 loss: 0.09892487525939941
Batch 21/64 loss: 0.11057573556900024
Batch 22/64 loss: 0.10727083683013916
Batch 23/64 loss: 0.09550857543945312
Batch 24/64 loss: 0.11125689744949341
Batch 25/64 loss: 0.09421253204345703
Batch 26/64 loss: 0.12644773721694946
Batch 27/64 loss: 0.10491979122161865
Batch 28/64 loss: 0.11720693111419678
Batch 29/64 loss: 0.12225598096847534
Batch 30/64 loss: 0.10418164730072021
Batch 31/64 loss: 0.1155017614364624
Batch 32/64 loss: 0.11435514688491821
Batch 33/64 loss: 0.11204648017883301
Batch 34/64 loss: 0.11256074905395508
Batch 35/64 loss: 0.10681182146072388
Batch 36/64 loss: 0.10563421249389648
Batch 37/64 loss: 0.09791958332061768
Batch 38/64 loss: 0.1023252010345459
Batch 39/64 loss: 0.10100644826889038
Batch 40/64 loss: 0.12022513151168823
Batch 41/64 loss: 0.11464494466781616
Batch 42/64 loss: 0.11386334896087646
Batch 43/64 loss: 0.10054081678390503
Batch 44/64 loss: 0.11642658710479736
Batch 45/64 loss: 0.112987220287323
Batch 46/64 loss: 0.1186608076095581
Batch 47/64 loss: 0.11052942276000977
Batch 48/64 loss: 0.10634785890579224
Batch 49/64 loss: 0.10507184267044067
Batch 50/64 loss: 0.1281377077102661
Batch 51/64 loss: 0.11663365364074707
Batch 52/64 loss: 0.09515774250030518
Batch 53/64 loss: 0.09998995065689087
Batch 54/64 loss: 0.11790680885314941
Batch 55/64 loss: 0.10546374320983887
Batch 56/64 loss: 0.1098179817199707
Batch 57/64 loss: 0.12297701835632324
Batch 58/64 loss: 0.1025192141532898
Batch 59/64 loss: 0.12132805585861206
Batch 60/64 loss: 0.10444968938827515
Batch 61/64 loss: 0.12816965579986572
Batch 62/64 loss: 0.12100327014923096
Batch 63/64 loss: 0.11447107791900635
Batch 64/64 loss: 0.11542820930480957
Epoch 107  Train loss: 0.11079199360866172  Val loss: 0.13112671924210906
Epoch 108
-------------------------------
Batch 1/64 loss: 0.11080741882324219
Batch 2/64 loss: 0.10366714000701904
Batch 3/64 loss: 0.11418169736862183
Batch 4/64 loss: 0.1040717363357544
Batch 5/64 loss: 0.11618518829345703
Batch 6/64 loss: 0.11435890197753906
Batch 7/64 loss: 0.10595089197158813
Batch 8/64 loss: 0.09983700513839722
Batch 9/64 loss: 0.12337273359298706
Batch 10/64 loss: 0.11284160614013672
Batch 11/64 loss: 0.11953824758529663
Batch 12/64 loss: 0.10209006071090698
Batch 13/64 loss: 0.10686957836151123
Batch 14/64 loss: 0.10262596607208252
Batch 15/64 loss: 0.1070285439491272
Batch 16/64 loss: 0.11299949884414673
Batch 17/64 loss: 0.10466539859771729
Batch 18/64 loss: 0.11325597763061523
Batch 19/64 loss: 0.12089300155639648
Batch 20/64 loss: 0.10670173168182373
Batch 21/64 loss: 0.123859703540802
Batch 22/64 loss: 0.11661767959594727
Batch 23/64 loss: 0.10749644041061401
Batch 24/64 loss: 0.11525094509124756
Batch 25/64 loss: 0.1084975004196167
Batch 26/64 loss: 0.11101573705673218
Batch 27/64 loss: 0.10348391532897949
Batch 28/64 loss: 0.11522412300109863
Batch 29/64 loss: 0.11599242687225342
Batch 30/64 loss: 0.09605586528778076
Batch 31/64 loss: 0.10873639583587646
Batch 32/64 loss: 0.10174012184143066
Batch 33/64 loss: 0.10852038860321045
Batch 34/64 loss: 0.10124218463897705
Batch 35/64 loss: 0.09774351119995117
Batch 36/64 loss: 0.11864709854125977
Batch 37/64 loss: 0.1197737455368042
Batch 38/64 loss: 0.12588775157928467
Batch 39/64 loss: 0.11827194690704346
Batch 40/64 loss: 0.11604338884353638
Batch 41/64 loss: 0.11457085609436035
Batch 42/64 loss: 0.10627573728561401
Batch 43/64 loss: 0.10123777389526367
Batch 44/64 loss: 0.09586608409881592
Batch 45/64 loss: 0.105324387550354
Batch 46/64 loss: 0.10227411985397339
Batch 47/64 loss: 0.10849267244338989
Batch 48/64 loss: 0.09760779142379761
Batch 49/64 loss: 0.11708778142929077
Batch 50/64 loss: 0.097187340259552
Batch 51/64 loss: 0.12173086404800415
Batch 52/64 loss: 0.12379759550094604
Batch 53/64 loss: 0.11812317371368408
Batch 54/64 loss: 0.10177743434906006
Batch 55/64 loss: 0.11789745092391968
Batch 56/64 loss: 0.13409912586212158
Batch 57/64 loss: 0.11084520816802979
Batch 58/64 loss: 0.11572813987731934
Batch 59/64 loss: 0.10080152750015259
Batch 60/64 loss: 0.10415482521057129
Batch 61/64 loss: 0.1114313006401062
Batch 62/64 loss: 0.12069272994995117
Batch 63/64 loss: 0.11878925561904907
Batch 64/64 loss: 0.12015527486801147
Epoch 108  Train loss: 0.11086984826069252  Val loss: 0.12615275239616736
Epoch 109
-------------------------------
Batch 1/64 loss: 0.10604238510131836
Batch 2/64 loss: 0.10292375087738037
Batch 3/64 loss: 0.09982454776763916
Batch 4/64 loss: 0.11002743244171143
Batch 5/64 loss: 0.1086188554763794
Batch 6/64 loss: 0.11550599336624146
Batch 7/64 loss: 0.10734200477600098
Batch 8/64 loss: 0.10231447219848633
Batch 9/64 loss: 0.09178835153579712
Batch 10/64 loss: 0.10589909553527832
Batch 11/64 loss: 0.1064113974571228
Batch 12/64 loss: 0.11369454860687256
Batch 13/64 loss: 0.108772873878479
Batch 14/64 loss: 0.10419493913650513
Batch 15/64 loss: 0.10628610849380493
Batch 16/64 loss: 0.10954618453979492
Batch 17/64 loss: 0.1060795783996582
Batch 18/64 loss: 0.11028385162353516
Batch 19/64 loss: 0.10106784105300903
Batch 20/64 loss: 0.10561275482177734
Batch 21/64 loss: 0.10943138599395752
Batch 22/64 loss: 0.10013628005981445
Batch 23/64 loss: 0.11022228002548218
Batch 24/64 loss: 0.12159943580627441
Batch 25/64 loss: 0.10822802782058716
Batch 26/64 loss: 0.1312691569328308
Batch 27/64 loss: 0.11846327781677246
Batch 28/64 loss: 0.09564542770385742
Batch 29/64 loss: 0.09844917058944702
Batch 30/64 loss: 0.10345542430877686
Batch 31/64 loss: 0.10770255327224731
Batch 32/64 loss: 0.11528396606445312
Batch 33/64 loss: 0.10595589876174927
Batch 34/64 loss: 0.10140907764434814
Batch 35/64 loss: 0.11144429445266724
Batch 36/64 loss: 0.10724002122879028
Batch 37/64 loss: 0.10456377267837524
Batch 38/64 loss: 0.1408851146697998
Batch 39/64 loss: 0.12058264017105103
Batch 40/64 loss: 0.11148416996002197
Batch 41/64 loss: 0.11706358194351196
Batch 42/64 loss: 0.10730636119842529
Batch 43/64 loss: 0.1039050817489624
Batch 44/64 loss: 0.10172057151794434
Batch 45/64 loss: 0.10492664575576782
Batch 46/64 loss: 0.107571542263031
Batch 47/64 loss: 0.12081968784332275
Batch 48/64 loss: 0.10187715291976929
Batch 49/64 loss: 0.1224067211151123
Batch 50/64 loss: 0.11070418357849121
Batch 51/64 loss: 0.10310840606689453
Batch 52/64 loss: 0.09352165460586548
Batch 53/64 loss: 0.10389316082000732
Batch 54/64 loss: 0.11314356327056885
Batch 55/64 loss: 0.10367631912231445
Batch 56/64 loss: 0.11504381895065308
Batch 57/64 loss: 0.10868632793426514
Batch 58/64 loss: 0.10993754863739014
Batch 59/64 loss: 0.10889619588851929
Batch 60/64 loss: 0.11906087398529053
Batch 61/64 loss: 0.09574031829833984
Batch 62/64 loss: 0.09395134449005127
Batch 63/64 loss: 0.1165398359298706
Batch 64/64 loss: 0.12522590160369873
Epoch 109  Train loss: 0.10859778301388609  Val loss: 0.12243884597037666
Saving best model, epoch: 109
Epoch 110
-------------------------------
Batch 1/64 loss: 0.11285912990570068
Batch 2/64 loss: 0.10652327537536621
Batch 3/64 loss: 0.10964089632034302
Batch 4/64 loss: 0.11124235391616821
Batch 5/64 loss: 0.12358558177947998
Batch 6/64 loss: 0.11267799139022827
Batch 7/64 loss: 0.09532272815704346
Batch 8/64 loss: 0.10766756534576416
Batch 9/64 loss: 0.11439508199691772
Batch 10/64 loss: 0.10342007875442505
Batch 11/64 loss: 0.10741662979125977
Batch 12/64 loss: 0.1152159571647644
Batch 13/64 loss: 0.11538875102996826
Batch 14/64 loss: 0.10263943672180176
Batch 15/64 loss: 0.10159564018249512
Batch 16/64 loss: 0.10594981908798218
Batch 17/64 loss: 0.10861682891845703
Batch 18/64 loss: 0.10983723402023315
Batch 19/64 loss: 0.11299806833267212
Batch 20/64 loss: 0.0935257077217102
Batch 21/64 loss: 0.11118519306182861
Batch 22/64 loss: 0.10217589139938354
Batch 23/64 loss: 0.10109680891036987
Batch 24/64 loss: 0.1018485426902771
Batch 25/64 loss: 0.10158705711364746
Batch 26/64 loss: 0.09621042013168335
Batch 27/64 loss: 0.10796141624450684
Batch 28/64 loss: 0.10341465473175049
Batch 29/64 loss: 0.1078406572341919
Batch 30/64 loss: 0.10809630155563354
Batch 31/64 loss: 0.10232323408126831
Batch 32/64 loss: 0.09849154949188232
Batch 33/64 loss: 0.10911625623703003
Batch 34/64 loss: 0.1000627875328064
Batch 35/64 loss: 0.11070889234542847
Batch 36/64 loss: 0.10956019163131714
Batch 37/64 loss: 0.10982924699783325
Batch 38/64 loss: 0.11286211013793945
Batch 39/64 loss: 0.10421699285507202
Batch 40/64 loss: 0.10014885663986206
Batch 41/64 loss: 0.10115450620651245
Batch 42/64 loss: 0.10242021083831787
Batch 43/64 loss: 0.1096947193145752
Batch 44/64 loss: 0.11683624982833862
Batch 45/64 loss: 0.10354608297348022
Batch 46/64 loss: 0.09908366203308105
Batch 47/64 loss: 0.12680453062057495
Batch 48/64 loss: 0.11064314842224121
Batch 49/64 loss: 0.09977966547012329
Batch 50/64 loss: 0.1245880126953125
Batch 51/64 loss: 0.1113542914390564
Batch 52/64 loss: 0.11676245927810669
Batch 53/64 loss: 0.08662444353103638
Batch 54/64 loss: 0.09195894002914429
Batch 55/64 loss: 0.10167926549911499
Batch 56/64 loss: 0.11526131629943848
Batch 57/64 loss: 0.10282087326049805
Batch 58/64 loss: 0.10777950286865234
Batch 59/64 loss: 0.10740584135055542
Batch 60/64 loss: 0.11686766147613525
Batch 61/64 loss: 0.12169283628463745
Batch 62/64 loss: 0.11406660079956055
Batch 63/64 loss: 0.12661194801330566
Batch 64/64 loss: 0.10234254598617554
Epoch 110  Train loss: 0.10763058031306547  Val loss: 0.12469513227849482
Epoch 111
-------------------------------
Batch 1/64 loss: 0.12714457511901855
Batch 2/64 loss: 0.11819076538085938
Batch 3/64 loss: 0.10421359539031982
Batch 4/64 loss: 0.11801105737686157
Batch 5/64 loss: 0.11668086051940918
Batch 6/64 loss: 0.11105644702911377
Batch 7/64 loss: 0.11738485097885132
Batch 8/64 loss: 0.0956692099571228
Batch 9/64 loss: 0.10368090867996216
Batch 10/64 loss: 0.12957501411437988
Batch 11/64 loss: 0.10179561376571655
Batch 12/64 loss: 0.09289312362670898
Batch 13/64 loss: 0.09085923433303833
Batch 14/64 loss: 0.10769921541213989
Batch 15/64 loss: 0.10789084434509277
Batch 16/64 loss: 0.09788459539413452
Batch 17/64 loss: 0.12023162841796875
Batch 18/64 loss: 0.11442607641220093
Batch 19/64 loss: 0.10898333787918091
Batch 20/64 loss: 0.10366851091384888
Batch 21/64 loss: 0.10602569580078125
Batch 22/64 loss: 0.128573477268219
Batch 23/64 loss: 0.11747926473617554
Batch 24/64 loss: 0.10888516902923584
Batch 25/64 loss: 0.10900735855102539
Batch 26/64 loss: 0.10260426998138428
Batch 27/64 loss: 0.10363614559173584
Batch 28/64 loss: 0.10361379384994507
Batch 29/64 loss: 0.11376398801803589
Batch 30/64 loss: 0.11659598350524902
Batch 31/64 loss: 0.1126168966293335
Batch 32/64 loss: 0.11060118675231934
Batch 33/64 loss: 0.11019814014434814
Batch 34/64 loss: 0.10795563459396362
Batch 35/64 loss: 0.1026354432106018
Batch 36/64 loss: 0.09206753969192505
Batch 37/64 loss: 0.11421549320220947
Batch 38/64 loss: 0.1316431760787964
Batch 39/64 loss: 0.11050111055374146
Batch 40/64 loss: 0.10087209939956665
Batch 41/64 loss: 0.09616774320602417
Batch 42/64 loss: 0.11133319139480591
Batch 43/64 loss: 0.11278676986694336
Batch 44/64 loss: 0.11013150215148926
Batch 45/64 loss: 0.10538589954376221
Batch 46/64 loss: 0.09639281034469604
Batch 47/64 loss: 0.110076904296875
Batch 48/64 loss: 0.09893590211868286
Batch 49/64 loss: 0.11262387037277222
Batch 50/64 loss: 0.09901994466781616
Batch 51/64 loss: 0.10328894853591919
Batch 52/64 loss: 0.11659228801727295
Batch 53/64 loss: 0.10640370845794678
Batch 54/64 loss: 0.09380978345870972
Batch 55/64 loss: 0.10396736860275269
Batch 56/64 loss: 0.09817147254943848
Batch 57/64 loss: 0.10294359922409058
Batch 58/64 loss: 0.10526818037033081
Batch 59/64 loss: 0.10451602935791016
Batch 60/64 loss: 0.09148192405700684
Batch 61/64 loss: 0.10068732500076294
Batch 62/64 loss: 0.10815936326980591
Batch 63/64 loss: 0.11026114225387573
Batch 64/64 loss: 0.09056735038757324
Epoch 111  Train loss: 0.10757274534188065  Val loss: 0.1236452394744375
Epoch 112
-------------------------------
Batch 1/64 loss: 0.11163729429244995
Batch 2/64 loss: 0.10517734289169312
Batch 3/64 loss: 0.10830473899841309
Batch 4/64 loss: 0.0972902774810791
Batch 5/64 loss: 0.10151660442352295
Batch 6/64 loss: 0.08752179145812988
Batch 7/64 loss: 0.10377860069274902
Batch 8/64 loss: 0.11804366111755371
Batch 9/64 loss: 0.11948364973068237
Batch 10/64 loss: 0.11455279588699341
Batch 11/64 loss: 0.10097622871398926
Batch 12/64 loss: 0.09853547811508179
Batch 13/64 loss: 0.11533385515213013
Batch 14/64 loss: 0.10513114929199219
Batch 15/64 loss: 0.1030571460723877
Batch 16/64 loss: 0.11354756355285645
Batch 17/64 loss: 0.1136828064918518
Batch 18/64 loss: 0.09235817193984985
Batch 19/64 loss: 0.10388451814651489
Batch 20/64 loss: 0.10810428857803345
Batch 21/64 loss: 0.09710061550140381
Batch 22/64 loss: 0.1094006896018982
Batch 23/64 loss: 0.09971058368682861
Batch 24/64 loss: 0.1095132827758789
Batch 25/64 loss: 0.10709375143051147
Batch 26/64 loss: 0.10362094640731812
Batch 27/64 loss: 0.10952746868133545
Batch 28/64 loss: 0.10185086727142334
Batch 29/64 loss: 0.11358988285064697
Batch 30/64 loss: 0.10883361101150513
Batch 31/64 loss: 0.09476786851882935
Batch 32/64 loss: 0.09458416700363159
Batch 33/64 loss: 0.10674828290939331
Batch 34/64 loss: 0.10255211591720581
Batch 35/64 loss: 0.11457669734954834
Batch 36/64 loss: 0.10393410921096802
Batch 37/64 loss: 0.10250687599182129
Batch 38/64 loss: 0.10901379585266113
Batch 39/64 loss: 0.11064767837524414
Batch 40/64 loss: 0.10527557134628296
Batch 41/64 loss: 0.11026889085769653
Batch 42/64 loss: 0.09653884172439575
Batch 43/64 loss: 0.10971266031265259
Batch 44/64 loss: 0.10106468200683594
Batch 45/64 loss: 0.1017681360244751
Batch 46/64 loss: 0.11240613460540771
Batch 47/64 loss: 0.10019052028656006
Batch 48/64 loss: 0.11215424537658691
Batch 49/64 loss: 0.10820478200912476
Batch 50/64 loss: 0.10560715198516846
Batch 51/64 loss: 0.1090930700302124
Batch 52/64 loss: 0.10949981212615967
Batch 53/64 loss: 0.10093766450881958
Batch 54/64 loss: 0.10042250156402588
Batch 55/64 loss: 0.09840661287307739
Batch 56/64 loss: 0.10456645488739014
Batch 57/64 loss: 0.11528801918029785
Batch 58/64 loss: 0.11512726545333862
Batch 59/64 loss: 0.1099439263343811
Batch 60/64 loss: 0.1046065092086792
Batch 61/64 loss: 0.09594994783401489
Batch 62/64 loss: 0.10620880126953125
Batch 63/64 loss: 0.11142241954803467
Batch 64/64 loss: 0.10283398628234863
Epoch 112  Train loss: 0.10577696257946538  Val loss: 0.12501334427148617
Epoch 113
-------------------------------
Batch 1/64 loss: 0.09844905138015747
Batch 2/64 loss: 0.10782116651535034
Batch 3/64 loss: 0.10367250442504883
Batch 4/64 loss: 0.10002589225769043
Batch 5/64 loss: 0.09311157464981079
Batch 6/64 loss: 0.09763163328170776
Batch 7/64 loss: 0.09529381990432739
Batch 8/64 loss: 0.11244815587997437
Batch 9/64 loss: 0.10259228944778442
Batch 10/64 loss: 0.10999882221221924
Batch 11/64 loss: 0.10707372426986694
Batch 12/64 loss: 0.11345171928405762
Batch 13/64 loss: 0.11674785614013672
Batch 14/64 loss: 0.11953175067901611
Batch 15/64 loss: 0.10613685846328735
Batch 16/64 loss: 0.11104762554168701
Batch 17/64 loss: 0.09491622447967529
Batch 18/64 loss: 0.10328817367553711
Batch 19/64 loss: 0.10449707508087158
Batch 20/64 loss: 0.10744661092758179
Batch 21/64 loss: 0.11683481931686401
Batch 22/64 loss: 0.11858803033828735
Batch 23/64 loss: 0.10788750648498535
Batch 24/64 loss: 0.10594886541366577
Batch 25/64 loss: 0.10556119680404663
Batch 26/64 loss: 0.10957509279251099
Batch 27/64 loss: 0.10001951456069946
Batch 28/64 loss: 0.1046288013458252
Batch 29/64 loss: 0.11333733797073364
Batch 30/64 loss: 0.10969972610473633
Batch 31/64 loss: 0.10755962133407593
Batch 32/64 loss: 0.09480655193328857
Batch 33/64 loss: 0.09607058763504028
Batch 34/64 loss: 0.10189485549926758
Batch 35/64 loss: 0.11350905895233154
Batch 36/64 loss: 0.1045261025428772
Batch 37/64 loss: 0.10540765523910522
Batch 38/64 loss: 0.11158859729766846
Batch 39/64 loss: 0.10829228162765503
Batch 40/64 loss: 0.10718458890914917
Batch 41/64 loss: 0.1173674464225769
Batch 42/64 loss: 0.11414551734924316
Batch 43/64 loss: 0.10691171884536743
Batch 44/64 loss: 0.10776680707931519
Batch 45/64 loss: 0.09351986646652222
Batch 46/64 loss: 0.10905611515045166
Batch 47/64 loss: 0.09955811500549316
Batch 48/64 loss: 0.09172391891479492
Batch 49/64 loss: 0.09185093641281128
Batch 50/64 loss: 0.11257344484329224
Batch 51/64 loss: 0.10987889766693115
Batch 52/64 loss: 0.09052443504333496
Batch 53/64 loss: 0.09944164752960205
Batch 54/64 loss: 0.11898136138916016
Batch 55/64 loss: 0.09762728214263916
Batch 56/64 loss: 0.10268557071685791
Batch 57/64 loss: 0.10447967052459717
Batch 58/64 loss: 0.10971331596374512
Batch 59/64 loss: 0.12503403425216675
Batch 60/64 loss: 0.11073321104049683
Batch 61/64 loss: 0.10085815191268921
Batch 62/64 loss: 0.10500377416610718
Batch 63/64 loss: 0.12211853265762329
Batch 64/64 loss: 0.10954111814498901
Epoch 113  Train loss: 0.10622451656004962  Val loss: 0.11802271838040695
Saving best model, epoch: 113
Epoch 114
-------------------------------
Batch 1/64 loss: 0.11254870891571045
Batch 2/64 loss: 0.11115539073944092
Batch 3/64 loss: 0.10605859756469727
Batch 4/64 loss: 0.10106182098388672
Batch 5/64 loss: 0.1082913875579834
Batch 6/64 loss: 0.09137874841690063
Batch 7/64 loss: 0.10217982530593872
Batch 8/64 loss: 0.10469430685043335
Batch 9/64 loss: 0.1174514889717102
Batch 10/64 loss: 0.09546661376953125
Batch 11/64 loss: 0.11658036708831787
Batch 12/64 loss: 0.10149580240249634
Batch 13/64 loss: 0.09600436687469482
Batch 14/64 loss: 0.10559016466140747
Batch 15/64 loss: 0.09278833866119385
Batch 16/64 loss: 0.10822772979736328
Batch 17/64 loss: 0.10071772336959839
Batch 18/64 loss: 0.09834539890289307
Batch 19/64 loss: 0.10307955741882324
Batch 20/64 loss: 0.1056562066078186
Batch 21/64 loss: 0.11486291885375977
Batch 22/64 loss: 0.09446698427200317
Batch 23/64 loss: 0.11474788188934326
Batch 24/64 loss: 0.1188732385635376
Batch 25/64 loss: 0.10396599769592285
Batch 26/64 loss: 0.09613466262817383
Batch 27/64 loss: 0.10277342796325684
Batch 28/64 loss: 0.09893035888671875
Batch 29/64 loss: 0.10924452543258667
Batch 30/64 loss: 0.10223913192749023
Batch 31/64 loss: 0.10497123003005981
Batch 32/64 loss: 0.09426665306091309
Batch 33/64 loss: 0.0955573320388794
Batch 34/64 loss: 0.09767574071884155
Batch 35/64 loss: 0.09100759029388428
Batch 36/64 loss: 0.10526353120803833
Batch 37/64 loss: 0.0931733250617981
Batch 38/64 loss: 0.101590096950531
Batch 39/64 loss: 0.10854864120483398
Batch 40/64 loss: 0.08720153570175171
Batch 41/64 loss: 0.11538189649581909
Batch 42/64 loss: 0.09171909093856812
Batch 43/64 loss: 0.0944746732711792
Batch 44/64 loss: 0.09282422065734863
Batch 45/64 loss: 0.09722131490707397
Batch 46/64 loss: 0.10310351848602295
Batch 47/64 loss: 0.10135316848754883
Batch 48/64 loss: 0.0922170877456665
Batch 49/64 loss: 0.10978710651397705
Batch 50/64 loss: 0.09359341859817505
Batch 51/64 loss: 0.11240601539611816
Batch 52/64 loss: 0.09928518533706665
Batch 53/64 loss: 0.09890300035476685
Batch 54/64 loss: 0.10552293062210083
Batch 55/64 loss: 0.10153710842132568
Batch 56/64 loss: 0.1194920539855957
Batch 57/64 loss: 0.1209789514541626
Batch 58/64 loss: 0.10733234882354736
Batch 59/64 loss: 0.10804325342178345
Batch 60/64 loss: 0.11748695373535156
Batch 61/64 loss: 0.10327780246734619
Batch 62/64 loss: 0.09486228227615356
Batch 63/64 loss: 0.12100988626480103
Batch 64/64 loss: 0.10983854532241821
Epoch 114  Train loss: 0.10350524748072905  Val loss: 0.11819581723295126
Epoch 115
-------------------------------
Batch 1/64 loss: 0.10275393724441528
Batch 2/64 loss: 0.09635502099990845
Batch 3/64 loss: 0.09559106826782227
Batch 4/64 loss: 0.10385251045227051
Batch 5/64 loss: 0.10808992385864258
Batch 6/64 loss: 0.11773407459259033
Batch 7/64 loss: 0.09863299131393433
Batch 8/64 loss: 0.09801948070526123
Batch 9/64 loss: 0.10474401712417603
Batch 10/64 loss: 0.10694658756256104
Batch 11/64 loss: 0.10264706611633301
Batch 12/64 loss: 0.10444492101669312
Batch 13/64 loss: 0.09559494256973267
Batch 14/64 loss: 0.09500151872634888
Batch 15/64 loss: 0.09209215641021729
Batch 16/64 loss: 0.11563056707382202
Batch 17/64 loss: 0.10134291648864746
Batch 18/64 loss: 0.10708224773406982
Batch 19/64 loss: 0.09366488456726074
Batch 20/64 loss: 0.1144561767578125
Batch 21/64 loss: 0.09874773025512695
Batch 22/64 loss: 0.1203107237815857
Batch 23/64 loss: 0.09202945232391357
Batch 24/64 loss: 0.11020195484161377
Batch 25/64 loss: 0.0996849536895752
Batch 26/64 loss: 0.10672241449356079
Batch 27/64 loss: 0.09846079349517822
Batch 28/64 loss: 0.10018086433410645
Batch 29/64 loss: 0.10281282663345337
Batch 30/64 loss: 0.1011161208152771
Batch 31/64 loss: 0.09126061201095581
Batch 32/64 loss: 0.09048199653625488
Batch 33/64 loss: 0.11335235834121704
Batch 34/64 loss: 0.1055486798286438
Batch 35/64 loss: 0.10976427793502808
Batch 36/64 loss: 0.10773366689682007
Batch 37/64 loss: 0.09787178039550781
Batch 38/64 loss: 0.10992610454559326
Batch 39/64 loss: 0.10608541965484619
Batch 40/64 loss: 0.10141849517822266
Batch 41/64 loss: 0.09321916103363037
Batch 42/64 loss: 0.11452394723892212
Batch 43/64 loss: 0.10500353574752808
Batch 44/64 loss: 0.1052560806274414
Batch 45/64 loss: 0.09937596321105957
Batch 46/64 loss: 0.09285485744476318
Batch 47/64 loss: 0.11061275005340576
Batch 48/64 loss: 0.10008025169372559
Batch 49/64 loss: 0.11921119689941406
Batch 50/64 loss: 0.11217063665390015
Batch 51/64 loss: 0.085285484790802
Batch 52/64 loss: 0.12355011701583862
Batch 53/64 loss: 0.0894622802734375
Batch 54/64 loss: 0.09732919931411743
Batch 55/64 loss: 0.12067610025405884
Batch 56/64 loss: 0.10045754909515381
Batch 57/64 loss: 0.09312927722930908
Batch 58/64 loss: 0.09929466247558594
Batch 59/64 loss: 0.09764736890792847
Batch 60/64 loss: 0.1129220724105835
Batch 61/64 loss: 0.08791661262512207
Batch 62/64 loss: 0.10970938205718994
Batch 63/64 loss: 0.10702371597290039
Batch 64/64 loss: 0.09678071737289429
Epoch 115  Train loss: 0.10302252512352139  Val loss: 0.11952511346626937
Epoch 116
-------------------------------
Batch 1/64 loss: 0.10184919834136963
Batch 2/64 loss: 0.09449249505996704
Batch 3/64 loss: 0.0981910228729248
Batch 4/64 loss: 0.10246694087982178
Batch 5/64 loss: 0.1064385175704956
Batch 6/64 loss: 0.10761535167694092
Batch 7/64 loss: 0.10995858907699585
Batch 8/64 loss: 0.10343897342681885
Batch 9/64 loss: 0.09600687026977539
Batch 10/64 loss: 0.09530746936798096
Batch 11/64 loss: 0.08721429109573364
Batch 12/64 loss: 0.09629631042480469
Batch 13/64 loss: 0.09113967418670654
Batch 14/64 loss: 0.09723103046417236
Batch 15/64 loss: 0.09528195858001709
Batch 16/64 loss: 0.09646338224411011
Batch 17/64 loss: 0.10758030414581299
Batch 18/64 loss: 0.1071205735206604
Batch 19/64 loss: 0.10327160358428955
Batch 20/64 loss: 0.09901678562164307
Batch 21/64 loss: 0.1115947961807251
Batch 22/64 loss: 0.10359680652618408
Batch 23/64 loss: 0.12158870697021484
Batch 24/64 loss: 0.08846038579940796
Batch 25/64 loss: 0.10557854175567627
Batch 26/64 loss: 0.08187580108642578
Batch 27/64 loss: 0.09078460931777954
Batch 28/64 loss: 0.09621661901473999
Batch 29/64 loss: 0.10950207710266113
Batch 30/64 loss: 0.10708719491958618
Batch 31/64 loss: 0.11276137828826904
Batch 32/64 loss: 0.09839761257171631
Batch 33/64 loss: 0.10704433917999268
Batch 34/64 loss: 0.09652197360992432
Batch 35/64 loss: 0.09203606843948364
Batch 36/64 loss: 0.09908831119537354
Batch 37/64 loss: 0.09909391403198242
Batch 38/64 loss: 0.09306001663208008
Batch 39/64 loss: 0.11195826530456543
Batch 40/64 loss: 0.10068607330322266
Batch 41/64 loss: 0.11042594909667969
Batch 42/64 loss: 0.10962116718292236
Batch 43/64 loss: 0.10101789236068726
Batch 44/64 loss: 0.09566032886505127
Batch 45/64 loss: 0.10285907983779907
Batch 46/64 loss: 0.09636098146438599
Batch 47/64 loss: 0.10063827037811279
Batch 48/64 loss: 0.10042989253997803
Batch 49/64 loss: 0.09636187553405762
Batch 50/64 loss: 0.12129449844360352
Batch 51/64 loss: 0.11503618955612183
Batch 52/64 loss: 0.09571337699890137
Batch 53/64 loss: 0.10138273239135742
Batch 54/64 loss: 0.1203051209449768
Batch 55/64 loss: 0.0929635763168335
Batch 56/64 loss: 0.10516548156738281
Batch 57/64 loss: 0.1074216365814209
Batch 58/64 loss: 0.09519398212432861
Batch 59/64 loss: 0.10901659727096558
Batch 60/64 loss: 0.10714131593704224
Batch 61/64 loss: 0.11003631353378296
Batch 62/64 loss: 0.10084813833236694
Batch 63/64 loss: 0.10890007019042969
Batch 64/64 loss: 0.0882420539855957
Epoch 116  Train loss: 0.10185554261301077  Val loss: 0.11909403444565449
Epoch 117
-------------------------------
Batch 1/64 loss: 0.10578310489654541
Batch 2/64 loss: 0.10093188285827637
Batch 3/64 loss: 0.09522408246994019
Batch 4/64 loss: 0.11093288660049438
Batch 5/64 loss: 0.12511789798736572
Batch 6/64 loss: 0.11696642637252808
Batch 7/64 loss: 0.09691578149795532
Batch 8/64 loss: 0.09886699914932251
Batch 9/64 loss: 0.09779655933380127
Batch 10/64 loss: 0.099590003490448
Batch 11/64 loss: 0.099373459815979
Batch 12/64 loss: 0.09963196516036987
Batch 13/64 loss: 0.11057186126708984
Batch 14/64 loss: 0.13120436668395996
Batch 15/64 loss: 0.10389906167984009
Batch 16/64 loss: 0.08945006132125854
Batch 17/64 loss: 0.1187710165977478
Batch 18/64 loss: 0.1018218994140625
Batch 19/64 loss: 0.0864831805229187
Batch 20/64 loss: 0.09451687335968018
Batch 21/64 loss: 0.10057955980300903
Batch 22/64 loss: 0.10844159126281738
Batch 23/64 loss: 0.09766590595245361
Batch 24/64 loss: 0.10625714063644409
Batch 25/64 loss: 0.11303931474685669
Batch 26/64 loss: 0.10118722915649414
Batch 27/64 loss: 0.10514700412750244
Batch 28/64 loss: 0.08818882703781128
Batch 29/64 loss: 0.10758852958679199
Batch 30/64 loss: 0.10554766654968262
Batch 31/64 loss: 0.10897397994995117
Batch 32/64 loss: 0.09367108345031738
Batch 33/64 loss: 0.10575568675994873
Batch 34/64 loss: 0.12437254190444946
Batch 35/64 loss: 0.09819221496582031
Batch 36/64 loss: 0.11175084114074707
Batch 37/64 loss: 0.09916424751281738
Batch 38/64 loss: 0.10881257057189941
Batch 39/64 loss: 0.12218332290649414
Batch 40/64 loss: 0.10368329286575317
Batch 41/64 loss: 0.10404247045516968
Batch 42/64 loss: 0.08538293838500977
Batch 43/64 loss: 0.10833066701889038
Batch 44/64 loss: 0.11330908536911011
Batch 45/64 loss: 0.12655788660049438
Batch 46/64 loss: 0.10090512037277222
Batch 47/64 loss: 0.08819389343261719
Batch 48/64 loss: 0.09808582067489624
Batch 49/64 loss: 0.11421889066696167
Batch 50/64 loss: 0.09067052602767944
Batch 51/64 loss: 0.09553587436676025
Batch 52/64 loss: 0.10904085636138916
Batch 53/64 loss: 0.10058128833770752
Batch 54/64 loss: 0.10069453716278076
Batch 55/64 loss: 0.09676986932754517
Batch 56/64 loss: 0.10124897956848145
Batch 57/64 loss: 0.08508318662643433
Batch 58/64 loss: 0.09946388006210327
Batch 59/64 loss: 0.11047834157943726
Batch 60/64 loss: 0.10078740119934082
Batch 61/64 loss: 0.0979112982749939
Batch 62/64 loss: 0.1141434907913208
Batch 63/64 loss: 0.1001131534576416
Batch 64/64 loss: 0.09879559278488159
Epoch 117  Train loss: 0.10368153838550344  Val loss: 0.12587585481991062
Epoch 118
-------------------------------
Batch 1/64 loss: 0.10493123531341553
Batch 2/64 loss: 0.08968150615692139
Batch 3/64 loss: 0.100105881690979
Batch 4/64 loss: 0.10862118005752563
Batch 5/64 loss: 0.09959661960601807
Batch 6/64 loss: 0.106991708278656
Batch 7/64 loss: 0.11073416471481323
Batch 8/64 loss: 0.09766238927841187
Batch 9/64 loss: 0.09228098392486572
Batch 10/64 loss: 0.11354142427444458
Batch 11/64 loss: 0.10584455728530884
Batch 12/64 loss: 0.10385113954544067
Batch 13/64 loss: 0.08979213237762451
Batch 14/64 loss: 0.10563462972640991
Batch 15/64 loss: 0.1044277548789978
Batch 16/64 loss: 0.10730361938476562
Batch 17/64 loss: 0.10434788465499878
Batch 18/64 loss: 0.10259699821472168
Batch 19/64 loss: 0.10928785800933838
Batch 20/64 loss: 0.10676306486129761
Batch 21/64 loss: 0.09930163621902466
Batch 22/64 loss: 0.10402697324752808
Batch 23/64 loss: 0.1057782769203186
Batch 24/64 loss: 0.086525559425354
Batch 25/64 loss: 0.10069465637207031
Batch 26/64 loss: 0.09864801168441772
Batch 27/64 loss: 0.0900987982749939
Batch 28/64 loss: 0.08852958679199219
Batch 29/64 loss: 0.10589563846588135
Batch 30/64 loss: 0.10229110717773438
Batch 31/64 loss: 0.09460097551345825
Batch 32/64 loss: 0.11895990371704102
Batch 33/64 loss: 0.09871566295623779
Batch 34/64 loss: 0.09611719846725464
Batch 35/64 loss: 0.11626118421554565
Batch 36/64 loss: 0.08903175592422485
Batch 37/64 loss: 0.08623838424682617
Batch 38/64 loss: 0.08587992191314697
Batch 39/64 loss: 0.1102340817451477
Batch 40/64 loss: 0.09794080257415771
Batch 41/64 loss: 0.1059388518333435
Batch 42/64 loss: 0.09340900182723999
Batch 43/64 loss: 0.10068196058273315
Batch 44/64 loss: 0.10350567102432251
Batch 45/64 loss: 0.09045600891113281
Batch 46/64 loss: 0.09411174058914185
Batch 47/64 loss: 0.10165625810623169
Batch 48/64 loss: 0.09434163570404053
Batch 49/64 loss: 0.11333638429641724
Batch 50/64 loss: 0.10179257392883301
Batch 51/64 loss: 0.1083117127418518
Batch 52/64 loss: 0.09408974647521973
Batch 53/64 loss: 0.12499141693115234
Batch 54/64 loss: 0.08663290739059448
Batch 55/64 loss: 0.10086828470230103
Batch 56/64 loss: 0.09069424867630005
Batch 57/64 loss: 0.09733712673187256
Batch 58/64 loss: 0.0969085693359375
Batch 59/64 loss: 0.10164815187454224
Batch 60/64 loss: 0.11616647243499756
Batch 61/64 loss: 0.08828973770141602
Batch 62/64 loss: 0.11567068099975586
Batch 63/64 loss: 0.09226459264755249
Batch 64/64 loss: 0.10090011358261108
Epoch 118  Train loss: 0.10083993205837175  Val loss: 0.11614914960467938
Saving best model, epoch: 118
Epoch 119
-------------------------------
Batch 1/64 loss: 0.10648852586746216
Batch 2/64 loss: 0.09544175863265991
Batch 3/64 loss: 0.08993250131607056
Batch 4/64 loss: 0.0973658561706543
Batch 5/64 loss: 0.09439080953598022
Batch 6/64 loss: 0.09313398599624634
Batch 7/64 loss: 0.10849648714065552
Batch 8/64 loss: 0.1036226749420166
Batch 9/64 loss: 0.09974133968353271
Batch 10/64 loss: 0.10246604681015015
Batch 11/64 loss: 0.0939909815788269
Batch 12/64 loss: 0.10747295618057251
Batch 13/64 loss: 0.09528201818466187
Batch 14/64 loss: 0.11695271730422974
Batch 15/64 loss: 0.09712296724319458
Batch 16/64 loss: 0.09794306755065918
Batch 17/64 loss: 0.10097908973693848
Batch 18/64 loss: 0.10651469230651855
Batch 19/64 loss: 0.10033285617828369
Batch 20/64 loss: 0.08670175075531006
Batch 21/64 loss: 0.09716498851776123
Batch 22/64 loss: 0.10180747509002686
Batch 23/64 loss: 0.09250617027282715
Batch 24/64 loss: 0.10034626722335815
Batch 25/64 loss: 0.08413857221603394
Batch 26/64 loss: 0.08578300476074219
Batch 27/64 loss: 0.10729557275772095
Batch 28/64 loss: 0.11011570692062378
Batch 29/64 loss: 0.10404378175735474
Batch 30/64 loss: 0.10340732336044312
Batch 31/64 loss: 0.10997313261032104
Batch 32/64 loss: 0.08952564001083374
Batch 33/64 loss: 0.10389095544815063
Batch 34/64 loss: 0.1002429723739624
Batch 35/64 loss: 0.10701006650924683
Batch 36/64 loss: 0.11336064338684082
Batch 37/64 loss: 0.09081298112869263
Batch 38/64 loss: 0.10186076164245605
Batch 39/64 loss: 0.0926523208618164
Batch 40/64 loss: 0.09033560752868652
Batch 41/64 loss: 0.10189610719680786
Batch 42/64 loss: 0.09974169731140137
Batch 43/64 loss: 0.10912054777145386
Batch 44/64 loss: 0.10048216581344604
Batch 45/64 loss: 0.09519439935684204
Batch 46/64 loss: 0.09876728057861328
Batch 47/64 loss: 0.09806817770004272
Batch 48/64 loss: 0.08826863765716553
Batch 49/64 loss: 0.09596538543701172
Batch 50/64 loss: 0.10731768608093262
Batch 51/64 loss: 0.08986908197402954
Batch 52/64 loss: 0.09025955200195312
Batch 53/64 loss: 0.0960550308227539
Batch 54/64 loss: 0.08947998285293579
Batch 55/64 loss: 0.11105561256408691
Batch 56/64 loss: 0.10966116189956665
Batch 57/64 loss: 0.0954439640045166
Batch 58/64 loss: 0.11009716987609863
Batch 59/64 loss: 0.10375857353210449
Batch 60/64 loss: 0.09996998310089111
Batch 61/64 loss: 0.10338747501373291
Batch 62/64 loss: 0.10889601707458496
Batch 63/64 loss: 0.10305178165435791
Batch 64/64 loss: 0.09732621908187866
Epoch 119  Train loss: 0.0997560653032041  Val loss: 0.11742682493839067
Epoch 120
-------------------------------
Batch 1/64 loss: 0.09554588794708252
Batch 2/64 loss: 0.10536861419677734
Batch 3/64 loss: 0.09308648109436035
Batch 4/64 loss: 0.11137986183166504
Batch 5/64 loss: 0.10589051246643066
Batch 6/64 loss: 0.10374552011489868
Batch 7/64 loss: 0.09605258703231812
Batch 8/64 loss: 0.09969604015350342
Batch 9/64 loss: 0.09357434511184692
Batch 10/64 loss: 0.09387803077697754
Batch 11/64 loss: 0.09179568290710449
Batch 12/64 loss: 0.1064683198928833
Batch 13/64 loss: 0.09542781114578247
Batch 14/64 loss: 0.10073268413543701
Batch 15/64 loss: 0.08813387155532837
Batch 16/64 loss: 0.09409129619598389
Batch 17/64 loss: 0.08400368690490723
Batch 18/64 loss: 0.0956382155418396
Batch 19/64 loss: 0.10074561834335327
Batch 20/64 loss: 0.10313695669174194
Batch 21/64 loss: 0.0919371247291565
Batch 22/64 loss: 0.09896320104598999
Batch 23/64 loss: 0.09470033645629883
Batch 24/64 loss: 0.0986815094947815
Batch 25/64 loss: 0.0963791012763977
Batch 26/64 loss: 0.09308570623397827
Batch 27/64 loss: 0.10268521308898926
Batch 28/64 loss: 0.09517264366149902
Batch 29/64 loss: 0.11255627870559692
Batch 30/64 loss: 0.09365922212600708
Batch 31/64 loss: 0.09305500984191895
Batch 32/64 loss: 0.11119478940963745
Batch 33/64 loss: 0.10086119174957275
Batch 34/64 loss: 0.11121463775634766
Batch 35/64 loss: 0.10025757551193237
Batch 36/64 loss: 0.10224127769470215
Batch 37/64 loss: 0.094382643699646
Batch 38/64 loss: 0.10644328594207764
Batch 39/64 loss: 0.09974151849746704
Batch 40/64 loss: 0.10109710693359375
Batch 41/64 loss: 0.10441255569458008
Batch 42/64 loss: 0.12156271934509277
Batch 43/64 loss: 0.09026312828063965
Batch 44/64 loss: 0.09916430711746216
Batch 45/64 loss: 0.09437710046768188
Batch 46/64 loss: 0.09622359275817871
Batch 47/64 loss: 0.10706734657287598
Batch 48/64 loss: 0.10291969776153564
Batch 49/64 loss: 0.10689431428909302
Batch 50/64 loss: 0.100990891456604
Batch 51/64 loss: 0.10346883535385132
Batch 52/64 loss: 0.09301644563674927
Batch 53/64 loss: 0.09745252132415771
Batch 54/64 loss: 0.11795657873153687
Batch 55/64 loss: 0.09246045351028442
Batch 56/64 loss: 0.09904968738555908
Batch 57/64 loss: 0.09527468681335449
Batch 58/64 loss: 0.10596603155136108
Batch 59/64 loss: 0.10290831327438354
Batch 60/64 loss: 0.10063815116882324
Batch 61/64 loss: 0.09889191389083862
Batch 62/64 loss: 0.10761117935180664
Batch 63/64 loss: 0.10002797842025757
Batch 64/64 loss: 0.10310488939285278
Epoch 120  Train loss: 0.0999627999230927  Val loss: 0.1193208659637425
Epoch 121
-------------------------------
Batch 1/64 loss: 0.09192025661468506
Batch 2/64 loss: 0.0904504656791687
Batch 3/64 loss: 0.09797877073287964
Batch 4/64 loss: 0.09887629747390747
Batch 5/64 loss: 0.09682387113571167
Batch 6/64 loss: 0.10909849405288696
Batch 7/64 loss: 0.10285383462905884
Batch 8/64 loss: 0.09096002578735352
Batch 9/64 loss: 0.0919879674911499
Batch 10/64 loss: 0.10083556175231934
Batch 11/64 loss: 0.09432077407836914
Batch 12/64 loss: 0.10467857122421265
Batch 13/64 loss: 0.09668195247650146
Batch 14/64 loss: 0.10514622926712036
Batch 15/64 loss: 0.08746159076690674
Batch 16/64 loss: 0.09568381309509277
Batch 17/64 loss: 0.08632385730743408
Batch 18/64 loss: 0.11072689294815063
Batch 19/64 loss: 0.10256808996200562
Batch 20/64 loss: 0.1019364595413208
Batch 21/64 loss: 0.09861284494400024
Batch 22/64 loss: 0.09434819221496582
Batch 23/64 loss: 0.09729540348052979
Batch 24/64 loss: 0.10018360614776611
Batch 25/64 loss: 0.1052977442741394
Batch 26/64 loss: 0.10001158714294434
Batch 27/64 loss: 0.10914433002471924
Batch 28/64 loss: 0.10169357061386108
Batch 29/64 loss: 0.1090201735496521
Batch 30/64 loss: 0.10008645057678223
Batch 31/64 loss: 0.1053396463394165
Batch 32/64 loss: 0.09871858358383179
Batch 33/64 loss: 0.10307097434997559
Batch 34/64 loss: 0.10714608430862427
Batch 35/64 loss: 0.11041039228439331
Batch 36/64 loss: 0.0976707935333252
Batch 37/64 loss: 0.10592710971832275
Batch 38/64 loss: 0.08847129344940186
Batch 39/64 loss: 0.12039762735366821
Batch 40/64 loss: 0.09799528121948242
Batch 41/64 loss: 0.08829247951507568
Batch 42/64 loss: 0.10592931509017944
Batch 43/64 loss: 0.08781450986862183
Batch 44/64 loss: 0.10555344820022583
Batch 45/64 loss: 0.09616303443908691
Batch 46/64 loss: 0.10708951950073242
Batch 47/64 loss: 0.08736538887023926
Batch 48/64 loss: 0.09647881984710693
Batch 49/64 loss: 0.09513217210769653
Batch 50/64 loss: 0.0994005799293518
Batch 51/64 loss: 0.09560191631317139
Batch 52/64 loss: 0.1012653112411499
Batch 53/64 loss: 0.10373270511627197
Batch 54/64 loss: 0.09707117080688477
Batch 55/64 loss: 0.10586303472518921
Batch 56/64 loss: 0.08819860219955444
Batch 57/64 loss: 0.09943503141403198
Batch 58/64 loss: 0.11935138702392578
Batch 59/64 loss: 0.09360915422439575
Batch 60/64 loss: 0.09134000539779663
Batch 61/64 loss: 0.09613269567489624
Batch 62/64 loss: 0.10765361785888672
Batch 63/64 loss: 0.09514117240905762
Batch 64/64 loss: 0.09186989068984985
Epoch 121  Train loss: 0.09946153654771693  Val loss: 0.11207551644839782
Saving best model, epoch: 121
Epoch 122
-------------------------------
Batch 1/64 loss: 0.09159380197525024
Batch 2/64 loss: 0.0908961296081543
Batch 3/64 loss: 0.08940768241882324
Batch 4/64 loss: 0.0998275876045227
Batch 5/64 loss: 0.08865070343017578
Batch 6/64 loss: 0.09662824869155884
Batch 7/64 loss: 0.09249913692474365
Batch 8/64 loss: 0.09724831581115723
Batch 9/64 loss: 0.1036037802696228
Batch 10/64 loss: 0.08521139621734619
Batch 11/64 loss: 0.10130655765533447
Batch 12/64 loss: 0.09471255540847778
Batch 13/64 loss: 0.0974321961402893
Batch 14/64 loss: 0.09178721904754639
Batch 15/64 loss: 0.09799724817276001
Batch 16/64 loss: 0.10846710205078125
Batch 17/64 loss: 0.09797537326812744
Batch 18/64 loss: 0.09761744737625122
Batch 19/64 loss: 0.09641557931900024
Batch 20/64 loss: 0.10289639234542847
Batch 21/64 loss: 0.09055757522583008
Batch 22/64 loss: 0.09271609783172607
Batch 23/64 loss: 0.09907972812652588
Batch 24/64 loss: 0.10147202014923096
Batch 25/64 loss: 0.09464520215988159
Batch 26/64 loss: 0.10188227891921997
Batch 27/64 loss: 0.08899348974227905
Batch 28/64 loss: 0.112030029296875
Batch 29/64 loss: 0.09063118696212769
Batch 30/64 loss: 0.10089230537414551
Batch 31/64 loss: 0.09987151622772217
Batch 32/64 loss: 0.09214180707931519
Batch 33/64 loss: 0.10009711980819702
Batch 34/64 loss: 0.09369641542434692
Batch 35/64 loss: 0.09994369745254517
Batch 36/64 loss: 0.09349358081817627
Batch 37/64 loss: 0.10579657554626465
Batch 38/64 loss: 0.10371363162994385
Batch 39/64 loss: 0.10145986080169678
Batch 40/64 loss: 0.11302107572555542
Batch 41/64 loss: 0.09218263626098633
Batch 42/64 loss: 0.10741537809371948
Batch 43/64 loss: 0.09686613082885742
Batch 44/64 loss: 0.11581289768218994
Batch 45/64 loss: 0.09535670280456543
Batch 46/64 loss: 0.08469688892364502
Batch 47/64 loss: 0.11524122953414917
Batch 48/64 loss: 0.111300528049469
Batch 49/64 loss: 0.09840524196624756
Batch 50/64 loss: 0.1031334400177002
Batch 51/64 loss: 0.10355305671691895
Batch 52/64 loss: 0.10723841190338135
Batch 53/64 loss: 0.10738682746887207
Batch 54/64 loss: 0.10319793224334717
Batch 55/64 loss: 0.09006750583648682
Batch 56/64 loss: 0.09400677680969238
Batch 57/64 loss: 0.09641605615615845
Batch 58/64 loss: 0.1060950756072998
Batch 59/64 loss: 0.10815870761871338
Batch 60/64 loss: 0.0965186357498169
Batch 61/64 loss: 0.10957014560699463
Batch 62/64 loss: 0.08936154842376709
Batch 63/64 loss: 0.09486174583435059
Batch 64/64 loss: 0.09068691730499268
Epoch 122  Train loss: 0.09871636605730244  Val loss: 0.1157295013621091
Epoch 123
-------------------------------
Batch 1/64 loss: 0.10933029651641846
Batch 2/64 loss: 0.08102905750274658
Batch 3/64 loss: 0.1045604944229126
Batch 4/64 loss: 0.10786867141723633
Batch 5/64 loss: 0.09117263555526733
Batch 6/64 loss: 0.1057201623916626
Batch 7/64 loss: 0.08807218074798584
Batch 8/64 loss: 0.10425859689712524
Batch 9/64 loss: 0.10269725322723389
Batch 10/64 loss: 0.10461795330047607
Batch 11/64 loss: 0.08867722749710083
Batch 12/64 loss: 0.09232091903686523
Batch 13/64 loss: 0.09132444858551025
Batch 14/64 loss: 0.08942723274230957
Batch 15/64 loss: 0.09781581163406372
Batch 16/64 loss: 0.10623615980148315
Batch 17/64 loss: 0.08572077751159668
Batch 18/64 loss: 0.10057544708251953
Batch 19/64 loss: 0.08530259132385254
Batch 20/64 loss: 0.0900193452835083
Batch 21/64 loss: 0.0874444842338562
Batch 22/64 loss: 0.09514695405960083
Batch 23/64 loss: 0.08857595920562744
Batch 24/64 loss: 0.10334634780883789
Batch 25/64 loss: 0.09813940525054932
Batch 26/64 loss: 0.09602701663970947
Batch 27/64 loss: 0.09406697750091553
Batch 28/64 loss: 0.09268903732299805
Batch 29/64 loss: 0.09845435619354248
Batch 30/64 loss: 0.10103636980056763
Batch 31/64 loss: 0.08044761419296265
Batch 32/64 loss: 0.08955490589141846
Batch 33/64 loss: 0.09942704439163208
Batch 34/64 loss: 0.08212774991989136
Batch 35/64 loss: 0.09714198112487793
Batch 36/64 loss: 0.10784256458282471
Batch 37/64 loss: 0.08812958002090454
Batch 38/64 loss: 0.0945744514465332
Batch 39/64 loss: 0.1163715124130249
Batch 40/64 loss: 0.10074669122695923
Batch 41/64 loss: 0.09221088886260986
Batch 42/64 loss: 0.09138727188110352
Batch 43/64 loss: 0.09014803171157837
Batch 44/64 loss: 0.09985959529876709
Batch 45/64 loss: 0.09733980894088745
Batch 46/64 loss: 0.11858373880386353
Batch 47/64 loss: 0.09133970737457275
Batch 48/64 loss: 0.09384578466415405
Batch 49/64 loss: 0.10444140434265137
Batch 50/64 loss: 0.09088373184204102
Batch 51/64 loss: 0.1028057336807251
Batch 52/64 loss: 0.10217458009719849
Batch 53/64 loss: 0.10446012020111084
Batch 54/64 loss: 0.10214447975158691
Batch 55/64 loss: 0.09734237194061279
Batch 56/64 loss: 0.09722429513931274
Batch 57/64 loss: 0.10549992322921753
Batch 58/64 loss: 0.09728807210922241
Batch 59/64 loss: 0.09705257415771484
Batch 60/64 loss: 0.09969401359558105
Batch 61/64 loss: 0.09969222545623779
Batch 62/64 loss: 0.10362493991851807
Batch 63/64 loss: 0.09991413354873657
Batch 64/64 loss: 0.10177767276763916
Epoch 123  Train loss: 0.09715025798947204  Val loss: 0.11736095443214338
Epoch 124
-------------------------------
Batch 1/64 loss: 0.0990491509437561
Batch 2/64 loss: 0.10740196704864502
Batch 3/64 loss: 0.09667408466339111
Batch 4/64 loss: 0.10525703430175781
Batch 5/64 loss: 0.10687828063964844
Batch 6/64 loss: 0.10348838567733765
Batch 7/64 loss: 0.10157901048660278
Batch 8/64 loss: 0.09228146076202393
Batch 9/64 loss: 0.08762598037719727
Batch 10/64 loss: 0.09461027383804321
Batch 11/64 loss: 0.08863306045532227
Batch 12/64 loss: 0.0941544771194458
Batch 13/64 loss: 0.09464031457901001
Batch 14/64 loss: 0.09624946117401123
Batch 15/64 loss: 0.08516371250152588
Batch 16/64 loss: 0.0892641544342041
Batch 17/64 loss: 0.09558022022247314
Batch 18/64 loss: 0.09869253635406494
Batch 19/64 loss: 0.09285271167755127
Batch 20/64 loss: 0.08694964647293091
Batch 21/64 loss: 0.0958871841430664
Batch 22/64 loss: 0.10360169410705566
Batch 23/64 loss: 0.09529876708984375
Batch 24/64 loss: 0.10246402025222778
Batch 25/64 loss: 0.09557348489761353
Batch 26/64 loss: 0.09768921136856079
Batch 27/64 loss: 0.09260016679763794
Batch 28/64 loss: 0.1052512526512146
Batch 29/64 loss: 0.10005450248718262
Batch 30/64 loss: 0.08501565456390381
Batch 31/64 loss: 0.10330265760421753
Batch 32/64 loss: 0.09932565689086914
Batch 33/64 loss: 0.10219645500183105
Batch 34/64 loss: 0.1014205813407898
Batch 35/64 loss: 0.10960757732391357
Batch 36/64 loss: 0.1029006838798523
Batch 37/64 loss: 0.10352236032485962
Batch 38/64 loss: 0.09795629978179932
Batch 39/64 loss: 0.10204249620437622
Batch 40/64 loss: 0.09250521659851074
Batch 41/64 loss: 0.08569234609603882
Batch 42/64 loss: 0.11098891496658325
Batch 43/64 loss: 0.10914897918701172
Batch 44/64 loss: 0.10645067691802979
Batch 45/64 loss: 0.10249483585357666
Batch 46/64 loss: 0.09709179401397705
Batch 47/64 loss: 0.08806878328323364
Batch 48/64 loss: 0.10231047868728638
Batch 49/64 loss: 0.1071312427520752
Batch 50/64 loss: 0.1084485650062561
Batch 51/64 loss: 0.08730560541152954
Batch 52/64 loss: 0.09653830528259277
Batch 53/64 loss: 0.10283160209655762
Batch 54/64 loss: 0.0951891541481018
Batch 55/64 loss: 0.09247726202011108
Batch 56/64 loss: 0.09763014316558838
Batch 57/64 loss: 0.10120749473571777
Batch 58/64 loss: 0.10277330875396729
Batch 59/64 loss: 0.11127209663391113
Batch 60/64 loss: 0.10579091310501099
Batch 61/64 loss: 0.09600365161895752
Batch 62/64 loss: 0.09777408838272095
Batch 63/64 loss: 0.10359621047973633
Batch 64/64 loss: 0.09067100286483765
Epoch 124  Train loss: 0.09853273022408579  Val loss: 0.11921028337118142
Epoch 125
-------------------------------
Batch 1/64 loss: 0.10489678382873535
Batch 2/64 loss: 0.09485816955566406
Batch 3/64 loss: 0.09765613079071045
Batch 4/64 loss: 0.08547002077102661
Batch 5/64 loss: 0.09022170305252075
Batch 6/64 loss: 0.1064600944519043
Batch 7/64 loss: 0.08835101127624512
Batch 8/64 loss: 0.08763593435287476
Batch 9/64 loss: 0.08681148290634155
Batch 10/64 loss: 0.10412061214447021
Batch 11/64 loss: 0.0989580750465393
Batch 12/64 loss: 0.0919613242149353
Batch 13/64 loss: 0.08701705932617188
Batch 14/64 loss: 0.10769331455230713
Batch 15/64 loss: 0.09223008155822754
Batch 16/64 loss: 0.08605104684829712
Batch 17/64 loss: 0.10681343078613281
Batch 18/64 loss: 0.09806805849075317
Batch 19/64 loss: 0.09287703037261963
Batch 20/64 loss: 0.09730041027069092
Batch 21/64 loss: 0.09462672472000122
Batch 22/64 loss: 0.09313774108886719
Batch 23/64 loss: 0.09053045511245728
Batch 24/64 loss: 0.09127843379974365
Batch 25/64 loss: 0.10715645551681519
Batch 26/64 loss: 0.09680217504501343
Batch 27/64 loss: 0.09942156076431274
Batch 28/64 loss: 0.09101051092147827
Batch 29/64 loss: 0.11778247356414795
Batch 30/64 loss: 0.0963582992553711
Batch 31/64 loss: 0.10353034734725952
Batch 32/64 loss: 0.0915454626083374
Batch 33/64 loss: 0.08673876523971558
Batch 34/64 loss: 0.10468190908432007
Batch 35/64 loss: 0.09183496236801147
Batch 36/64 loss: 0.10018253326416016
Batch 37/64 loss: 0.11327552795410156
Batch 38/64 loss: 0.09943312406539917
Batch 39/64 loss: 0.09628945589065552
Batch 40/64 loss: 0.10696148872375488
Batch 41/64 loss: 0.10267478227615356
Batch 42/64 loss: 0.09596776962280273
Batch 43/64 loss: 0.1125069260597229
Batch 44/64 loss: 0.10638612508773804
Batch 45/64 loss: 0.09325003623962402
Batch 46/64 loss: 0.0998924970626831
Batch 47/64 loss: 0.10288375616073608
Batch 48/64 loss: 0.10508954524993896
Batch 49/64 loss: 0.11275708675384521
Batch 50/64 loss: 0.0946192741394043
Batch 51/64 loss: 0.09077578783035278
Batch 52/64 loss: 0.10647571086883545
Batch 53/64 loss: 0.09885823726654053
Batch 54/64 loss: 0.08723747730255127
Batch 55/64 loss: 0.09658807516098022
Batch 56/64 loss: 0.09400850534439087
Batch 57/64 loss: 0.10485142469406128
Batch 58/64 loss: 0.10721945762634277
Batch 59/64 loss: 0.09781700372695923
Batch 60/64 loss: 0.10298097133636475
Batch 61/64 loss: 0.08907461166381836
Batch 62/64 loss: 0.09369337558746338
Batch 63/64 loss: 0.11088579893112183
Batch 64/64 loss: 0.09297722578048706
Epoch 125  Train loss: 0.09810599743151198  Val loss: 0.11374134460265693
Epoch 126
-------------------------------
Batch 1/64 loss: 0.09564489126205444
Batch 2/64 loss: 0.09740197658538818
Batch 3/64 loss: 0.1060713529586792
Batch 4/64 loss: 0.07754069566726685
Batch 5/64 loss: 0.09047126770019531
Batch 6/64 loss: 0.10449981689453125
Batch 7/64 loss: 0.09277009963989258
Batch 8/64 loss: 0.08739036321640015
Batch 9/64 loss: 0.09613513946533203
Batch 10/64 loss: 0.10395938158035278
Batch 11/64 loss: 0.08907729387283325
Batch 12/64 loss: 0.1046951413154602
Batch 13/64 loss: 0.1044430136680603
Batch 14/64 loss: 0.10548162460327148
Batch 15/64 loss: 0.08456146717071533
Batch 16/64 loss: 0.0937756896018982
Batch 17/64 loss: 0.09659451246261597
Batch 18/64 loss: 0.09307599067687988
Batch 19/64 loss: 0.1024276614189148
Batch 20/64 loss: 0.1082613468170166
Batch 21/64 loss: 0.09407740831375122
Batch 22/64 loss: 0.10685062408447266
Batch 23/64 loss: 0.09721529483795166
Batch 24/64 loss: 0.10736870765686035
Batch 25/64 loss: 0.10588854551315308
Batch 26/64 loss: 0.08973652124404907
Batch 27/64 loss: 0.07805269956588745
Batch 28/64 loss: 0.10063517093658447
Batch 29/64 loss: 0.10455119609832764
Batch 30/64 loss: 0.09596776962280273
Batch 31/64 loss: 0.08936834335327148
Batch 32/64 loss: 0.0976518988609314
Batch 33/64 loss: 0.09334254264831543
Batch 34/64 loss: 0.09058666229248047
Batch 35/64 loss: 0.085396409034729
Batch 36/64 loss: 0.10589754581451416
Batch 37/64 loss: 0.0909777283668518
Batch 38/64 loss: 0.09083420038223267
Batch 39/64 loss: 0.09007823467254639
Batch 40/64 loss: 0.10560369491577148
Batch 41/64 loss: 0.09262120723724365
Batch 42/64 loss: 0.10855603218078613
Batch 43/64 loss: 0.0916140079498291
Batch 44/64 loss: 0.08408868312835693
Batch 45/64 loss: 0.08204710483551025
Batch 46/64 loss: 0.09115594625473022
Batch 47/64 loss: 0.09943193197250366
Batch 48/64 loss: 0.1045638918876648
Batch 49/64 loss: 0.09923189878463745
Batch 50/64 loss: 0.09625345468521118
Batch 51/64 loss: 0.10080403089523315
Batch 52/64 loss: 0.09747445583343506
Batch 53/64 loss: 0.11983096599578857
Batch 54/64 loss: 0.09362000226974487
Batch 55/64 loss: 0.09428894519805908
Batch 56/64 loss: 0.09900844097137451
Batch 57/64 loss: 0.10135877132415771
Batch 58/64 loss: 0.08745032548904419
Batch 59/64 loss: 0.09458047151565552
Batch 60/64 loss: 0.09269940853118896
Batch 61/64 loss: 0.09284794330596924
Batch 62/64 loss: 0.10417133569717407
Batch 63/64 loss: 0.09145200252532959
Batch 64/64 loss: 0.09323221445083618
Epoch 126  Train loss: 0.09636761324078429  Val loss: 0.1152041753133138
Epoch 127
-------------------------------
Batch 1/64 loss: 0.10236245393753052
Batch 2/64 loss: 0.09397238492965698
Batch 3/64 loss: 0.08202016353607178
Batch 4/64 loss: 0.08600926399230957
Batch 5/64 loss: 0.08802968263626099
Batch 6/64 loss: 0.10368967056274414
Batch 7/64 loss: 0.07370495796203613
Batch 8/64 loss: 0.09126830101013184
Batch 9/64 loss: 0.09073406457901001
Batch 10/64 loss: 0.08745080232620239
Batch 11/64 loss: 0.0897364616394043
Batch 12/64 loss: 0.09969305992126465
Batch 13/64 loss: 0.09946131706237793
Batch 14/64 loss: 0.09975045919418335
Batch 15/64 loss: 0.08892637491226196
Batch 16/64 loss: 0.09681713581085205
Batch 17/64 loss: 0.08861654996871948
Batch 18/64 loss: 0.10021930932998657
Batch 19/64 loss: 0.08726894855499268
Batch 20/64 loss: 0.09485399723052979
Batch 21/64 loss: 0.09009665250778198
Batch 22/64 loss: 0.09315979480743408
Batch 23/64 loss: 0.08442473411560059
Batch 24/64 loss: 0.10007703304290771
Batch 25/64 loss: 0.11052680015563965
Batch 26/64 loss: 0.10764920711517334
Batch 27/64 loss: 0.08124160766601562
Batch 28/64 loss: 0.09237819910049438
Batch 29/64 loss: 0.10149329900741577
Batch 30/64 loss: 0.10634690523147583
Batch 31/64 loss: 0.11121290922164917
Batch 32/64 loss: 0.10557913780212402
Batch 33/64 loss: 0.08317327499389648
Batch 34/64 loss: 0.09493005275726318
Batch 35/64 loss: 0.11405295133590698
Batch 36/64 loss: 0.10196912288665771
Batch 37/64 loss: 0.09077984094619751
Batch 38/64 loss: 0.09089946746826172
Batch 39/64 loss: 0.11022812128067017
Batch 40/64 loss: 0.10168677568435669
Batch 41/64 loss: 0.09340417385101318
Batch 42/64 loss: 0.08386504650115967
Batch 43/64 loss: 0.09088116884231567
Batch 44/64 loss: 0.10054653882980347
Batch 45/64 loss: 0.09775280952453613
Batch 46/64 loss: 0.12096697092056274
Batch 47/64 loss: 0.1134943962097168
Batch 48/64 loss: 0.10433965921401978
Batch 49/64 loss: 0.08344590663909912
Batch 50/64 loss: 0.090462327003479
Batch 51/64 loss: 0.09392529726028442
Batch 52/64 loss: 0.08965647220611572
Batch 53/64 loss: 0.08916181325912476
Batch 54/64 loss: 0.10003173351287842
Batch 55/64 loss: 0.11779665946960449
Batch 56/64 loss: 0.10038495063781738
Batch 57/64 loss: 0.10452455282211304
Batch 58/64 loss: 0.09981954097747803
Batch 59/64 loss: 0.10866594314575195
Batch 60/64 loss: 0.08978670835494995
Batch 61/64 loss: 0.09499436616897583
Batch 62/64 loss: 0.0989331603050232
Batch 63/64 loss: 0.10361146926879883
Batch 64/64 loss: 0.1031196117401123
Epoch 127  Train loss: 0.09669462933259852  Val loss: 0.11257297005440361
Epoch 128
-------------------------------
Batch 1/64 loss: 0.11160475015640259
Batch 2/64 loss: 0.10929363965988159
Batch 3/64 loss: 0.0844373106956482
Batch 4/64 loss: 0.10410106182098389
Batch 5/64 loss: 0.09275442361831665
Batch 6/64 loss: 0.09540730714797974
Batch 7/64 loss: 0.08786702156066895
Batch 8/64 loss: 0.09427130222320557
Batch 9/64 loss: 0.0944368839263916
Batch 10/64 loss: 0.09349322319030762
Batch 11/64 loss: 0.09849554300308228
Batch 12/64 loss: 0.10622823238372803
Batch 13/64 loss: 0.09668493270874023
Batch 14/64 loss: 0.09825253486633301
Batch 15/64 loss: 0.09177154302597046
Batch 16/64 loss: 0.10151106119155884
Batch 17/64 loss: 0.10872262716293335
Batch 18/64 loss: 0.09029197692871094
Batch 19/64 loss: 0.10090792179107666
Batch 20/64 loss: 0.1001061201095581
Batch 21/64 loss: 0.0933845043182373
Batch 22/64 loss: 0.09316277503967285
Batch 23/64 loss: 0.11497354507446289
Batch 24/64 loss: 0.1060948371887207
Batch 25/64 loss: 0.09767472743988037
Batch 26/64 loss: 0.09997916221618652
Batch 27/64 loss: 0.1052560806274414
Batch 28/64 loss: 0.11686241626739502
Batch 29/64 loss: 0.08712267875671387
Batch 30/64 loss: 0.08478093147277832
Batch 31/64 loss: 0.0871201753616333
Batch 32/64 loss: 0.10085487365722656
Batch 33/64 loss: 0.09573805332183838
Batch 34/64 loss: 0.11116141080856323
Batch 35/64 loss: 0.0932651162147522
Batch 36/64 loss: 0.09454154968261719
Batch 37/64 loss: 0.09937334060668945
Batch 38/64 loss: 0.09414494037628174
Batch 39/64 loss: 0.10929113626480103
Batch 40/64 loss: 0.1009528636932373
Batch 41/64 loss: 0.09652799367904663
Batch 42/64 loss: 0.0979539155960083
Batch 43/64 loss: 0.1099010705947876
Batch 44/64 loss: 0.09102445840835571
Batch 45/64 loss: 0.0936058759689331
Batch 46/64 loss: 0.09068083763122559
Batch 47/64 loss: 0.09187877178192139
Batch 48/64 loss: 0.10753673315048218
Batch 49/64 loss: 0.08062911033630371
Batch 50/64 loss: 0.09899848699569702
Batch 51/64 loss: 0.09375447034835815
Batch 52/64 loss: 0.10438412427902222
Batch 53/64 loss: 0.10595762729644775
Batch 54/64 loss: 0.0998227596282959
Batch 55/64 loss: 0.09166145324707031
Batch 56/64 loss: 0.09859734773635864
Batch 57/64 loss: 0.08174240589141846
Batch 58/64 loss: 0.08487212657928467
Batch 59/64 loss: 0.09368818998336792
Batch 60/64 loss: 0.09566020965576172
Batch 61/64 loss: 0.079140305519104
Batch 62/64 loss: 0.10936838388442993
Batch 63/64 loss: 0.09361350536346436
Batch 64/64 loss: 0.093425452709198
Epoch 128  Train loss: 0.09737166727290433  Val loss: 0.1107198905289378
Saving best model, epoch: 128
Epoch 129
-------------------------------
Batch 1/64 loss: 0.10761773586273193
Batch 2/64 loss: 0.08025556802749634
Batch 3/64 loss: 0.0905943512916565
Batch 4/64 loss: 0.07686728239059448
Batch 5/64 loss: 0.0950770378112793
Batch 6/64 loss: 0.09681588411331177
Batch 7/64 loss: 0.08422774076461792
Batch 8/64 loss: 0.08922082185745239
Batch 9/64 loss: 0.09177333116531372
Batch 10/64 loss: 0.10556161403656006
Batch 11/64 loss: 0.08910888433456421
Batch 12/64 loss: 0.08356928825378418
Batch 13/64 loss: 0.08731859922409058
Batch 14/64 loss: 0.0917128324508667
Batch 15/64 loss: 0.09396421909332275
Batch 16/64 loss: 0.09242039918899536
Batch 17/64 loss: 0.10379970073699951
Batch 18/64 loss: 0.09359067678451538
Batch 19/64 loss: 0.10849976539611816
Batch 20/64 loss: 0.09165680408477783
Batch 21/64 loss: 0.08713746070861816
Batch 22/64 loss: 0.08857464790344238
Batch 23/64 loss: 0.08938908576965332
Batch 24/64 loss: 0.08213478326797485
Batch 25/64 loss: 0.07212549448013306
Batch 26/64 loss: 0.08150458335876465
Batch 27/64 loss: 0.10122400522232056
Batch 28/64 loss: 0.09838861227035522
Batch 29/64 loss: 0.08891373872756958
Batch 30/64 loss: 0.09149926900863647
Batch 31/64 loss: 0.0950208306312561
Batch 32/64 loss: 0.10992145538330078
Batch 33/64 loss: 0.08362066745758057
Batch 34/64 loss: 0.09268373250961304
Batch 35/64 loss: 0.11907625198364258
Batch 36/64 loss: 0.08514350652694702
Batch 37/64 loss: 0.08502030372619629
Batch 38/64 loss: 0.09015977382659912
Batch 39/64 loss: 0.08766353130340576
Batch 40/64 loss: 0.10524320602416992
Batch 41/64 loss: 0.09457719326019287
Batch 42/64 loss: 0.10885941982269287
Batch 43/64 loss: 0.0938493013381958
Batch 44/64 loss: 0.09270912408828735
Batch 45/64 loss: 0.09867256879806519
Batch 46/64 loss: 0.09348708391189575
Batch 47/64 loss: 0.0992887020111084
Batch 48/64 loss: 0.08796977996826172
Batch 49/64 loss: 0.0906287431716919
Batch 50/64 loss: 0.09187096357345581
Batch 51/64 loss: 0.0943763256072998
Batch 52/64 loss: 0.0901263952255249
Batch 53/64 loss: 0.10113829374313354
Batch 54/64 loss: 0.09598934650421143
Batch 55/64 loss: 0.10828471183776855
Batch 56/64 loss: 0.09053707122802734
Batch 57/64 loss: 0.09151387214660645
Batch 58/64 loss: 0.09234434366226196
Batch 59/64 loss: 0.11583983898162842
Batch 60/64 loss: 0.09503650665283203
Batch 61/64 loss: 0.10029172897338867
Batch 62/64 loss: 0.08732843399047852
Batch 63/64 loss: 0.09548437595367432
Batch 64/64 loss: 0.0972476601600647
Epoch 129  Train loss: 0.09366646818086213  Val loss: 0.11227661842333082
Epoch 130
-------------------------------
Batch 1/64 loss: 0.11476504802703857
Batch 2/64 loss: 0.09792673587799072
Batch 3/64 loss: 0.08477222919464111
Batch 4/64 loss: 0.09143978357315063
Batch 5/64 loss: 0.12409597635269165
Batch 6/64 loss: 0.0873686671257019
Batch 7/64 loss: 0.0813596248626709
Batch 8/64 loss: 0.08723759651184082
Batch 9/64 loss: 0.09573185443878174
Batch 10/64 loss: 0.08979862928390503
Batch 11/64 loss: 0.08859223127365112
Batch 12/64 loss: 0.08378630876541138
Batch 13/64 loss: 0.07913327217102051
Batch 14/64 loss: 0.0862693190574646
Batch 15/64 loss: 0.10137468576431274
Batch 16/64 loss: 0.09923046827316284
Batch 17/64 loss: 0.08916628360748291
Batch 18/64 loss: 0.10308223962783813
Batch 19/64 loss: 0.08924120664596558
Batch 20/64 loss: 0.07992160320281982
Batch 21/64 loss: 0.09379702806472778
Batch 22/64 loss: 0.09425562620162964
Batch 23/64 loss: 0.10029458999633789
Batch 24/64 loss: 0.09418725967407227
Batch 25/64 loss: 0.08891743421554565
Batch 26/64 loss: 0.09190970659255981
Batch 27/64 loss: 0.11305195093154907
Batch 28/64 loss: 0.08439528942108154
Batch 29/64 loss: 0.09928405284881592
Batch 30/64 loss: 0.10478591918945312
Batch 31/64 loss: 0.09308826923370361
Batch 32/64 loss: 0.0975145697593689
Batch 33/64 loss: 0.10492801666259766
Batch 34/64 loss: 0.0971304178237915
Batch 35/64 loss: 0.09286820888519287
Batch 36/64 loss: 0.1001860499382019
Batch 37/64 loss: 0.10122191905975342
Batch 38/64 loss: 0.09254127740859985
Batch 39/64 loss: 0.09065884351730347
Batch 40/64 loss: 0.08069658279418945
Batch 41/64 loss: 0.0910903811454773
Batch 42/64 loss: 0.09891623258590698
Batch 43/64 loss: 0.1026526689529419
Batch 44/64 loss: 0.10350114107131958
Batch 45/64 loss: 0.09128224849700928
Batch 46/64 loss: 0.10622459650039673
Batch 47/64 loss: 0.09205037355422974
Batch 48/64 loss: 0.10191571712493896
Batch 49/64 loss: 0.09681302309036255
Batch 50/64 loss: 0.08852291107177734
Batch 51/64 loss: 0.08524149656295776
Batch 52/64 loss: 0.07635629177093506
Batch 53/64 loss: 0.09704655408859253
Batch 54/64 loss: 0.08365786075592041
Batch 55/64 loss: 0.08701831102371216
Batch 56/64 loss: 0.10205298662185669
Batch 57/64 loss: 0.09774625301361084
Batch 58/64 loss: 0.08994472026824951
Batch 59/64 loss: 0.10707497596740723
Batch 60/64 loss: 0.08864700794219971
Batch 61/64 loss: 0.10546004772186279
Batch 62/64 loss: 0.09704345464706421
Batch 63/64 loss: 0.08595061302185059
Batch 64/64 loss: 0.08544743061065674
Epoch 130  Train loss: 0.09427925044415043  Val loss: 0.11067024211293643
Saving best model, epoch: 130
Epoch 131
-------------------------------
Batch 1/64 loss: 0.09004676342010498
Batch 2/64 loss: 0.08857548236846924
Batch 3/64 loss: 0.09182912111282349
Batch 4/64 loss: 0.09205067157745361
Batch 5/64 loss: 0.0916031002998352
Batch 6/64 loss: 0.09086906909942627
Batch 7/64 loss: 0.08528047800064087
Batch 8/64 loss: 0.08292245864868164
Batch 9/64 loss: 0.10419410467147827
Batch 10/64 loss: 0.10488682985305786
Batch 11/64 loss: 0.09848636388778687
Batch 12/64 loss: 0.09560650587081909
Batch 13/64 loss: 0.09619647264480591
Batch 14/64 loss: 0.09816265106201172
Batch 15/64 loss: 0.10029900074005127
Batch 16/64 loss: 0.08753395080566406
Batch 17/64 loss: 0.08868920803070068
Batch 18/64 loss: 0.1062391996383667
Batch 19/64 loss: 0.10128480195999146
Batch 20/64 loss: 0.09310388565063477
Batch 21/64 loss: 0.1020892858505249
Batch 22/64 loss: 0.09218788146972656
Batch 23/64 loss: 0.09709376096725464
Batch 24/64 loss: 0.09827762842178345
Batch 25/64 loss: 0.09581291675567627
Batch 26/64 loss: 0.0906379222869873
Batch 27/64 loss: 0.08888930082321167
Batch 28/64 loss: 0.09644103050231934
Batch 29/64 loss: 0.08327794075012207
Batch 30/64 loss: 0.09732639789581299
Batch 31/64 loss: 0.0932232141494751
Batch 32/64 loss: 0.08582907915115356
Batch 33/64 loss: 0.10171651840209961
Batch 34/64 loss: 0.10646706819534302
Batch 35/64 loss: 0.09371870756149292
Batch 36/64 loss: 0.0983918309211731
Batch 37/64 loss: 0.0884549617767334
Batch 38/64 loss: 0.09842491149902344
Batch 39/64 loss: 0.10121750831604004
Batch 40/64 loss: 0.10041218996047974
Batch 41/64 loss: 0.09243905544281006
Batch 42/64 loss: 0.09475749731063843
Batch 43/64 loss: 0.10854381322860718
Batch 44/64 loss: 0.09049296379089355
Batch 45/64 loss: 0.08883130550384521
Batch 46/64 loss: 0.09345895051956177
Batch 47/64 loss: 0.0995640754699707
Batch 48/64 loss: 0.10090029239654541
Batch 49/64 loss: 0.09826743602752686
Batch 50/64 loss: 0.10444444417953491
Batch 51/64 loss: 0.09782958030700684
Batch 52/64 loss: 0.09044289588928223
Batch 53/64 loss: 0.09634828567504883
Batch 54/64 loss: 0.09799587726593018
Batch 55/64 loss: 0.07771360874176025
Batch 56/64 loss: 0.09479516744613647
Batch 57/64 loss: 0.09621888399124146
Batch 58/64 loss: 0.08384203910827637
Batch 59/64 loss: 0.11895221471786499
Batch 60/64 loss: 0.10190171003341675
Batch 61/64 loss: 0.08841407299041748
Batch 62/64 loss: 0.10218864679336548
Batch 63/64 loss: 0.0873100757598877
Batch 64/64 loss: 0.09763586521148682
Epoch 131  Train loss: 0.09531968574897916  Val loss: 0.11256529600759552
Epoch 132
-------------------------------
Batch 1/64 loss: 0.09300780296325684
Batch 2/64 loss: 0.10642498731613159
Batch 3/64 loss: 0.10199970006942749
Batch 4/64 loss: 0.10522902011871338
Batch 5/64 loss: 0.09015226364135742
Batch 6/64 loss: 0.0854838490486145
Batch 7/64 loss: 0.09273946285247803
Batch 8/64 loss: 0.07421201467514038
Batch 9/64 loss: 0.08068591356277466
Batch 10/64 loss: 0.08614814281463623
Batch 11/64 loss: 0.08260136842727661
Batch 12/64 loss: 0.08762800693511963
Batch 13/64 loss: 0.08850729465484619
Batch 14/64 loss: 0.10101211071014404
Batch 15/64 loss: 0.09077262878417969
Batch 16/64 loss: 0.09386312961578369
Batch 17/64 loss: 0.09386318922042847
Batch 18/64 loss: 0.08690345287322998
Batch 19/64 loss: 0.09079056978225708
Batch 20/64 loss: 0.09083229303359985
Batch 21/64 loss: 0.08600616455078125
Batch 22/64 loss: 0.08999115228652954
Batch 23/64 loss: 0.09380519390106201
Batch 24/64 loss: 0.08913475275039673
Batch 25/64 loss: 0.09372347593307495
Batch 26/64 loss: 0.09341943264007568
Batch 27/64 loss: 0.08137369155883789
Batch 28/64 loss: 0.08650410175323486
Batch 29/64 loss: 0.1150277853012085
Batch 30/64 loss: 0.11020565032958984
Batch 31/64 loss: 0.09759736061096191
Batch 32/64 loss: 0.09997022151947021
Batch 33/64 loss: 0.0844656229019165
Batch 34/64 loss: 0.08337044715881348
Batch 35/64 loss: 0.09942662715911865
Batch 36/64 loss: 0.09087657928466797
Batch 37/64 loss: 0.08678001165390015
Batch 38/64 loss: 0.09956872463226318
Batch 39/64 loss: 0.08484971523284912
Batch 40/64 loss: 0.0842100977897644
Batch 41/64 loss: 0.09201258420944214
Batch 42/64 loss: 0.09036308526992798
Batch 43/64 loss: 0.0973130464553833
Batch 44/64 loss: 0.08648920059204102
Batch 45/64 loss: 0.0973849892616272
Batch 46/64 loss: 0.09876936674118042
Batch 47/64 loss: 0.10000842809677124
Batch 48/64 loss: 0.08800745010375977
Batch 49/64 loss: 0.1000097393989563
Batch 50/64 loss: 0.09732991456985474
Batch 51/64 loss: 0.11256164312362671
Batch 52/64 loss: 0.10706895589828491
Batch 53/64 loss: 0.09315729141235352
Batch 54/64 loss: 0.09288185834884644
Batch 55/64 loss: 0.09476727247238159
Batch 56/64 loss: 0.10570555925369263
Batch 57/64 loss: 0.08584314584732056
Batch 58/64 loss: 0.10024374723434448
Batch 59/64 loss: 0.09280836582183838
Batch 60/64 loss: 0.08661174774169922
Batch 61/64 loss: 0.08365190029144287
Batch 62/64 loss: 0.08710992336273193
Batch 63/64 loss: 0.10833990573883057
Batch 64/64 loss: 0.08190631866455078
Epoch 132  Train loss: 0.0930671822791006  Val loss: 0.11060102657763819
Saving best model, epoch: 132
Epoch 133
-------------------------------
Batch 1/64 loss: 0.10718750953674316
Batch 2/64 loss: 0.10083043575286865
Batch 3/64 loss: 0.0884811282157898
Batch 4/64 loss: 0.08844107389450073
Batch 5/64 loss: 0.09278684854507446
Batch 6/64 loss: 0.10663574934005737
Batch 7/64 loss: 0.09497499465942383
Batch 8/64 loss: 0.08748441934585571
Batch 9/64 loss: 0.09386765956878662
Batch 10/64 loss: 0.08133870363235474
Batch 11/64 loss: 0.09797513484954834
Batch 12/64 loss: 0.08317124843597412
Batch 13/64 loss: 0.09101676940917969
Batch 14/64 loss: 0.09049183130264282
Batch 15/64 loss: 0.10633820295333862
Batch 16/64 loss: 0.09428238868713379
Batch 17/64 loss: 0.08590817451477051
Batch 18/64 loss: 0.09391164779663086
Batch 19/64 loss: 0.09136998653411865
Batch 20/64 loss: 0.09451019763946533
Batch 21/64 loss: 0.08235585689544678
Batch 22/64 loss: 0.0904313325881958
Batch 23/64 loss: 0.08702421188354492
Batch 24/64 loss: 0.08609861135482788
Batch 25/64 loss: 0.08479243516921997
Batch 26/64 loss: 0.09780502319335938
Batch 27/64 loss: 0.08025681972503662
Batch 28/64 loss: 0.09928858280181885
Batch 29/64 loss: 0.09516668319702148
Batch 30/64 loss: 0.08325010538101196
Batch 31/64 loss: 0.09117060899734497
Batch 32/64 loss: 0.09676468372344971
Batch 33/64 loss: 0.08702373504638672
Batch 34/64 loss: 0.09858298301696777
Batch 35/64 loss: 0.08096373081207275
Batch 36/64 loss: 0.08506691455841064
Batch 37/64 loss: 0.09370261430740356
Batch 38/64 loss: 0.08379006385803223
Batch 39/64 loss: 0.0860101580619812
Batch 40/64 loss: 0.09386950731277466
Batch 41/64 loss: 0.10554593801498413
Batch 42/64 loss: 0.10133397579193115
Batch 43/64 loss: 0.08618921041488647
Batch 44/64 loss: 0.08700346946716309
Batch 45/64 loss: 0.084736168384552
Batch 46/64 loss: 0.09836781024932861
Batch 47/64 loss: 0.08466124534606934
Batch 48/64 loss: 0.09756118059158325
Batch 49/64 loss: 0.08669662475585938
Batch 50/64 loss: 0.09125041961669922
Batch 51/64 loss: 0.10911524295806885
Batch 52/64 loss: 0.09253835678100586
Batch 53/64 loss: 0.08889490365982056
Batch 54/64 loss: 0.10236227512359619
Batch 55/64 loss: 0.08613133430480957
Batch 56/64 loss: 0.09887552261352539
Batch 57/64 loss: 0.08891832828521729
Batch 58/64 loss: 0.08955121040344238
Batch 59/64 loss: 0.1088489294052124
Batch 60/64 loss: 0.11365729570388794
Batch 61/64 loss: 0.09655505418777466
Batch 62/64 loss: 0.0961301326751709
Batch 63/64 loss: 0.09076189994812012
Batch 64/64 loss: 0.08689922094345093
Epoch 133  Train loss: 0.09263183860217823  Val loss: 0.11226017040895023
Epoch 134
-------------------------------
Batch 1/64 loss: 0.08455878496170044
Batch 2/64 loss: 0.0921517014503479
Batch 3/64 loss: 0.08532094955444336
Batch 4/64 loss: 0.08358609676361084
Batch 5/64 loss: 0.08324861526489258
Batch 6/64 loss: 0.08885782957077026
Batch 7/64 loss: 0.07876640558242798
Batch 8/64 loss: 0.09859836101531982
Batch 9/64 loss: 0.09664309024810791
Batch 10/64 loss: 0.08391380310058594
Batch 11/64 loss: 0.0918726921081543
Batch 12/64 loss: 0.08139097690582275
Batch 13/64 loss: 0.09673804044723511
Batch 14/64 loss: 0.1095508337020874
Batch 15/64 loss: 0.0895645022392273
Batch 16/64 loss: 0.08635848760604858
Batch 17/64 loss: 0.08971071243286133
Batch 18/64 loss: 0.09780210256576538
Batch 19/64 loss: 0.09842437505722046
Batch 20/64 loss: 0.09085237979888916
Batch 21/64 loss: 0.09098488092422485
Batch 22/64 loss: 0.0982658863067627
Batch 23/64 loss: 0.0858500599861145
Batch 24/64 loss: 0.08626854419708252
Batch 25/64 loss: 0.07393044233322144
Batch 26/64 loss: 0.08635485172271729
Batch 27/64 loss: 0.1007012128829956
Batch 28/64 loss: 0.08956319093704224
Batch 29/64 loss: 0.10652673244476318
Batch 30/64 loss: 0.07911831140518188
Batch 31/64 loss: 0.09390193223953247
Batch 32/64 loss: 0.09403860569000244
Batch 33/64 loss: 0.08376419544219971
Batch 34/64 loss: 0.09402787685394287
Batch 35/64 loss: 0.10102349519729614
Batch 36/64 loss: 0.09902715682983398
Batch 37/64 loss: 0.07693499326705933
Batch 38/64 loss: 0.09102219343185425
Batch 39/64 loss: 0.08425366878509521
Batch 40/64 loss: 0.08067977428436279
Batch 41/64 loss: 0.08862614631652832
Batch 42/64 loss: 0.08574485778808594
Batch 43/64 loss: 0.08846688270568848
Batch 44/64 loss: 0.07989001274108887
Batch 45/64 loss: 0.09681260585784912
Batch 46/64 loss: 0.09538543224334717
Batch 47/64 loss: 0.09127539396286011
Batch 48/64 loss: 0.0932205319404602
Batch 49/64 loss: 0.09023833274841309
Batch 50/64 loss: 0.10864311456680298
Batch 51/64 loss: 0.08312278985977173
Batch 52/64 loss: 0.08382230997085571
Batch 53/64 loss: 0.09568518400192261
Batch 54/64 loss: 0.09720128774642944
Batch 55/64 loss: 0.09872812032699585
Batch 56/64 loss: 0.09178197383880615
Batch 57/64 loss: 0.0940350890159607
Batch 58/64 loss: 0.11103945970535278
Batch 59/64 loss: 0.08031463623046875
Batch 60/64 loss: 0.09011518955230713
Batch 61/64 loss: 0.10039234161376953
Batch 62/64 loss: 0.0909881591796875
Batch 63/64 loss: 0.09120631217956543
Batch 64/64 loss: 0.10183781385421753
Epoch 134  Train loss: 0.0910943258042429  Val loss: 0.10864600610896893
Saving best model, epoch: 134
Epoch 135
-------------------------------
Batch 1/64 loss: 0.0762144923210144
Batch 2/64 loss: 0.08951389789581299
Batch 3/64 loss: 0.09381037950515747
Batch 4/64 loss: 0.07336556911468506
Batch 5/64 loss: 0.08040934801101685
Batch 6/64 loss: 0.08908367156982422
Batch 7/64 loss: 0.08984959125518799
Batch 8/64 loss: 0.08531486988067627
Batch 9/64 loss: 0.10583341121673584
Batch 10/64 loss: 0.08917129039764404
Batch 11/64 loss: 0.11597597599029541
Batch 12/64 loss: 0.10133963823318481
Batch 13/64 loss: 0.08379465341567993
Batch 14/64 loss: 0.10400283336639404
Batch 15/64 loss: 0.08886778354644775
Batch 16/64 loss: 0.0959157943725586
Batch 17/64 loss: 0.1027982234954834
Batch 18/64 loss: 0.09414219856262207
Batch 19/64 loss: 0.0772542953491211
Batch 20/64 loss: 0.08928388357162476
Batch 21/64 loss: 0.09144473075866699
Batch 22/64 loss: 0.10151749849319458
Batch 23/64 loss: 0.08835077285766602
Batch 24/64 loss: 0.09607678651809692
Batch 25/64 loss: 0.08432239294052124
Batch 26/64 loss: 0.08120453357696533
Batch 27/64 loss: 0.09426248073577881
Batch 28/64 loss: 0.0947069525718689
Batch 29/64 loss: 0.08584702014923096
Batch 30/64 loss: 0.09308493137359619
Batch 31/64 loss: 0.09185397624969482
Batch 32/64 loss: 0.08985936641693115
Batch 33/64 loss: 0.0881684422492981
Batch 34/64 loss: 0.08445686101913452
Batch 35/64 loss: 0.09471601247787476
Batch 36/64 loss: 0.09075212478637695
Batch 37/64 loss: 0.08849269151687622
Batch 38/64 loss: 0.09609121084213257
Batch 39/64 loss: 0.110251784324646
Batch 40/64 loss: 0.09385091066360474
Batch 41/64 loss: 0.09213495254516602
Batch 42/64 loss: 0.08928686380386353
Batch 43/64 loss: 0.09206569194793701
Batch 44/64 loss: 0.08801496028900146
Batch 45/64 loss: 0.10293704271316528
Batch 46/64 loss: 0.07941281795501709
Batch 47/64 loss: 0.09353262186050415
Batch 48/64 loss: 0.08266925811767578
Batch 49/64 loss: 0.08947676420211792
Batch 50/64 loss: 0.09621477127075195
Batch 51/64 loss: 0.08165228366851807
Batch 52/64 loss: 0.10001450777053833
Batch 53/64 loss: 0.09494167566299438
Batch 54/64 loss: 0.10420817136764526
Batch 55/64 loss: 0.08943682909011841
Batch 56/64 loss: 0.08408832550048828
Batch 57/64 loss: 0.08418023586273193
Batch 58/64 loss: 0.08934247493743896
Batch 59/64 loss: 0.07291543483734131
Batch 60/64 loss: 0.08611631393432617
Batch 61/64 loss: 0.09354126453399658
Batch 62/64 loss: 0.09206032752990723
Batch 63/64 loss: 0.08923017978668213
Batch 64/64 loss: 0.09111475944519043
Epoch 135  Train loss: 0.09099706855474733  Val loss: 0.10664091425663008
Saving best model, epoch: 135
Epoch 136
-------------------------------
Batch 1/64 loss: 0.08558273315429688
Batch 2/64 loss: 0.10463285446166992
Batch 3/64 loss: 0.09588885307312012
Batch 4/64 loss: 0.08729583024978638
Batch 5/64 loss: 0.09493613243103027
Batch 6/64 loss: 0.08088523149490356
Batch 7/64 loss: 0.09036505222320557
Batch 8/64 loss: 0.08150243759155273
Batch 9/64 loss: 0.08929282426834106
Batch 10/64 loss: 0.08120220899581909
Batch 11/64 loss: 0.09097927808761597
Batch 12/64 loss: 0.07688331604003906
Batch 13/64 loss: 0.08600080013275146
Batch 14/64 loss: 0.08145356178283691
Batch 15/64 loss: 0.0916968584060669
Batch 16/64 loss: 0.10103720426559448
Batch 17/64 loss: 0.08803266286849976
Batch 18/64 loss: 0.10253894329071045
Batch 19/64 loss: 0.1006622314453125
Batch 20/64 loss: 0.09636306762695312
Batch 21/64 loss: 0.08844864368438721
Batch 22/64 loss: 0.0837554931640625
Batch 23/64 loss: 0.09212744235992432
Batch 24/64 loss: 0.08519959449768066
Batch 25/64 loss: 0.08133995532989502
Batch 26/64 loss: 0.09530413150787354
Batch 27/64 loss: 0.09139561653137207
Batch 28/64 loss: 0.08320063352584839
Batch 29/64 loss: 0.08657783269882202
Batch 30/64 loss: 0.08795279264450073
Batch 31/64 loss: 0.0897291898727417
Batch 32/64 loss: 0.0788416862487793
Batch 33/64 loss: 0.09134215116500854
Batch 34/64 loss: 0.07806038856506348
Batch 35/64 loss: 0.09404098987579346
Batch 36/64 loss: 0.08858370780944824
Batch 37/64 loss: 0.09104812145233154
Batch 38/64 loss: 0.0849689245223999
Batch 39/64 loss: 0.09697657823562622
Batch 40/64 loss: 0.07534372806549072
Batch 41/64 loss: 0.08091485500335693
Batch 42/64 loss: 0.09171038866043091
Batch 43/64 loss: 0.09090030193328857
Batch 44/64 loss: 0.09547650814056396
Batch 45/64 loss: 0.08594262599945068
Batch 46/64 loss: 0.07030987739562988
Batch 47/64 loss: 0.09136569499969482
Batch 48/64 loss: 0.09614646434783936
Batch 49/64 loss: 0.08372628688812256
Batch 50/64 loss: 0.09022247791290283
Batch 51/64 loss: 0.07608413696289062
Batch 52/64 loss: 0.08133107423782349
Batch 53/64 loss: 0.10738849639892578
Batch 54/64 loss: 0.09533846378326416
Batch 55/64 loss: 0.10233825445175171
Batch 56/64 loss: 0.10198962688446045
Batch 57/64 loss: 0.08861243724822998
Batch 58/64 loss: 0.08599984645843506
Batch 59/64 loss: 0.09347409009933472
Batch 60/64 loss: 0.08222436904907227
Batch 61/64 loss: 0.08996343612670898
Batch 62/64 loss: 0.1107972264289856
Batch 63/64 loss: 0.08752447366714478
Batch 64/64 loss: 0.09627068042755127
Epoch 136  Train loss: 0.08946594677719416  Val loss: 0.11515410270068244
Epoch 137
-------------------------------
Batch 1/64 loss: 0.0911603569984436
Batch 2/64 loss: 0.09872746467590332
Batch 3/64 loss: 0.08931010961532593
Batch 4/64 loss: 0.09537124633789062
Batch 5/64 loss: 0.09684193134307861
Batch 6/64 loss: 0.08571457862854004
Batch 7/64 loss: 0.10111486911773682
Batch 8/64 loss: 0.09299767017364502
Batch 9/64 loss: 0.08146780729293823
Batch 10/64 loss: 0.09970200061798096
Batch 11/64 loss: 0.09122872352600098
Batch 12/64 loss: 0.07682013511657715
Batch 13/64 loss: 0.0946197509765625
Batch 14/64 loss: 0.10307514667510986
Batch 15/64 loss: 0.11612361669540405
Batch 16/64 loss: 0.08288633823394775
Batch 17/64 loss: 0.08906674385070801
Batch 18/64 loss: 0.0875089168548584
Batch 19/64 loss: 0.0854291319847107
Batch 20/64 loss: 0.09030526876449585
Batch 21/64 loss: 0.08545607328414917
Batch 22/64 loss: 0.08944112062454224
Batch 23/64 loss: 0.10275071859359741
Batch 24/64 loss: 0.09050261974334717
Batch 25/64 loss: 0.0922694206237793
Batch 26/64 loss: 0.0856291651725769
Batch 27/64 loss: 0.10344785451889038
Batch 28/64 loss: 0.09413713216781616
Batch 29/64 loss: 0.08769387006759644
Batch 30/64 loss: 0.08085471391677856
Batch 31/64 loss: 0.09753966331481934
Batch 32/64 loss: 0.10073471069335938
Batch 33/64 loss: 0.0967981219291687
Batch 34/64 loss: 0.09086096286773682
Batch 35/64 loss: 0.08660972118377686
Batch 36/64 loss: 0.09179025888442993
Batch 37/64 loss: 0.08902162313461304
Batch 38/64 loss: 0.08514809608459473
Batch 39/64 loss: 0.08898675441741943
Batch 40/64 loss: 0.07814997434616089
Batch 41/64 loss: 0.11178332567214966
Batch 42/64 loss: 0.09530997276306152
Batch 43/64 loss: 0.09085804224014282
Batch 44/64 loss: 0.0883830189704895
Batch 45/64 loss: 0.10513734817504883
Batch 46/64 loss: 0.08041441440582275
Batch 47/64 loss: 0.09271782636642456
Batch 48/64 loss: 0.08614683151245117
Batch 49/64 loss: 0.08750253915786743
Batch 50/64 loss: 0.0834774374961853
Batch 51/64 loss: 0.09118247032165527
Batch 52/64 loss: 0.07992750406265259
Batch 53/64 loss: 0.08031129837036133
Batch 54/64 loss: 0.09020936489105225
Batch 55/64 loss: 0.10161048173904419
Batch 56/64 loss: 0.08213520050048828
Batch 57/64 loss: 0.0875391960144043
Batch 58/64 loss: 0.08186030387878418
Batch 59/64 loss: 0.0962073802947998
Batch 60/64 loss: 0.08586335182189941
Batch 61/64 loss: 0.09231090545654297
Batch 62/64 loss: 0.07881087064743042
Batch 63/64 loss: 0.08154690265655518
Batch 64/64 loss: 0.08527541160583496
Epoch 137  Train loss: 0.09070583418303844  Val loss: 0.10984933151002602
Epoch 138
-------------------------------
Batch 1/64 loss: 0.07620429992675781
Batch 2/64 loss: 0.08872580528259277
Batch 3/64 loss: 0.08813846111297607
Batch 4/64 loss: 0.10181176662445068
Batch 5/64 loss: 0.08743953704833984
Batch 6/64 loss: 0.08833831548690796
Batch 7/64 loss: 0.083041250705719
Batch 8/64 loss: 0.08858507871627808
Batch 9/64 loss: 0.08645039796829224
Batch 10/64 loss: 0.09196507930755615
Batch 11/64 loss: 0.09229177236557007
Batch 12/64 loss: 0.08772861957550049
Batch 13/64 loss: 0.08935916423797607
Batch 14/64 loss: 0.08869326114654541
Batch 15/64 loss: 0.07831239700317383
Batch 16/64 loss: 0.10938876867294312
Batch 17/64 loss: 0.0703885555267334
Batch 18/64 loss: 0.0761113166809082
Batch 19/64 loss: 0.0942685604095459
Batch 20/64 loss: 0.08901363611221313
Batch 21/64 loss: 0.08950358629226685
Batch 22/64 loss: 0.09700286388397217
Batch 23/64 loss: 0.07326531410217285
Batch 24/64 loss: 0.09164893627166748
Batch 25/64 loss: 0.07385832071304321
Batch 26/64 loss: 0.09144842624664307
Batch 27/64 loss: 0.08552747964859009
Batch 28/64 loss: 0.10584056377410889
Batch 29/64 loss: 0.08469939231872559
Batch 30/64 loss: 0.07954663038253784
Batch 31/64 loss: 0.08972787857055664
Batch 32/64 loss: 0.11048060655593872
Batch 33/64 loss: 0.09112274646759033
Batch 34/64 loss: 0.09837120771408081
Batch 35/64 loss: 0.07701146602630615
Batch 36/64 loss: 0.09482395648956299
Batch 37/64 loss: 0.0861092209815979
Batch 38/64 loss: 0.0869753360748291
Batch 39/64 loss: 0.09083563089370728
Batch 40/64 loss: 0.07878291606903076
Batch 41/64 loss: 0.10239160060882568
Batch 42/64 loss: 0.08659559488296509
Batch 43/64 loss: 0.08721393346786499
Batch 44/64 loss: 0.09511786699295044
Batch 45/64 loss: 0.09501928091049194
Batch 46/64 loss: 0.08501893281936646
Batch 47/64 loss: 0.10269403457641602
Batch 48/64 loss: 0.08214879035949707
Batch 49/64 loss: 0.10998821258544922
Batch 50/64 loss: 0.08600205183029175
Batch 51/64 loss: 0.08137452602386475
Batch 52/64 loss: 0.07281947135925293
Batch 53/64 loss: 0.08040344715118408
Batch 54/64 loss: 0.08184874057769775
Batch 55/64 loss: 0.0920630693435669
Batch 56/64 loss: 0.07431304454803467
Batch 57/64 loss: 0.07618886232376099
Batch 58/64 loss: 0.07634121179580688
Batch 59/64 loss: 0.086983323097229
Batch 60/64 loss: 0.08300584554672241
Batch 61/64 loss: 0.07046276330947876
Batch 62/64 loss: 0.09269362688064575
Batch 63/64 loss: 0.09646159410476685
Batch 64/64 loss: 0.09368419647216797
Epoch 138  Train loss: 0.08784708228765749  Val loss: 0.11416859192536868
Epoch 139
-------------------------------
Batch 1/64 loss: 0.07814812660217285
Batch 2/64 loss: 0.08664119243621826
Batch 3/64 loss: 0.08296942710876465
Batch 4/64 loss: 0.08981943130493164
Batch 5/64 loss: 0.07443976402282715
Batch 6/64 loss: 0.09764748811721802
Batch 7/64 loss: 0.08598780632019043
Batch 8/64 loss: 0.0905274748802185
Batch 9/64 loss: 0.08623772859573364
Batch 10/64 loss: 0.0811164379119873
Batch 11/64 loss: 0.07754397392272949
Batch 12/64 loss: 0.09955215454101562
Batch 13/64 loss: 0.08107304573059082
Batch 14/64 loss: 0.09871065616607666
Batch 15/64 loss: 0.08724868297576904
Batch 16/64 loss: 0.06941699981689453
Batch 17/64 loss: 0.08578890562057495
Batch 18/64 loss: 0.08700889348983765
Batch 19/64 loss: 0.0996938943862915
Batch 20/64 loss: 0.08109927177429199
Batch 21/64 loss: 0.08152544498443604
Batch 22/64 loss: 0.08594226837158203
Batch 23/64 loss: 0.09035128355026245
Batch 24/64 loss: 0.09151709079742432
Batch 25/64 loss: 0.079537034034729
Batch 26/64 loss: 0.09468644857406616
Batch 27/64 loss: 0.09952473640441895
Batch 28/64 loss: 0.0872260332107544
Batch 29/64 loss: 0.08345407247543335
Batch 30/64 loss: 0.0894126296043396
Batch 31/64 loss: 0.08447980880737305
Batch 32/64 loss: 0.10541999340057373
Batch 33/64 loss: 0.08261001110076904
Batch 34/64 loss: 0.09315323829650879
Batch 35/64 loss: 0.08350884914398193
Batch 36/64 loss: 0.09763568639755249
Batch 37/64 loss: 0.09493470191955566
Batch 38/64 loss: 0.08607381582260132
Batch 39/64 loss: 0.0888676643371582
Batch 40/64 loss: 0.08572989702224731
Batch 41/64 loss: 0.0841018557548523
Batch 42/64 loss: 0.09248363971710205
Batch 43/64 loss: 0.08904284238815308
Batch 44/64 loss: 0.08129233121871948
Batch 45/64 loss: 0.07813584804534912
Batch 46/64 loss: 0.08746755123138428
Batch 47/64 loss: 0.09080439805984497
Batch 48/64 loss: 0.07513618469238281
Batch 49/64 loss: 0.09572696685791016
Batch 50/64 loss: 0.08436441421508789
Batch 51/64 loss: 0.09060269594192505
Batch 52/64 loss: 0.08894705772399902
Batch 53/64 loss: 0.07878953218460083
Batch 54/64 loss: 0.09124308824539185
Batch 55/64 loss: 0.08578646183013916
Batch 56/64 loss: 0.09486889839172363
Batch 57/64 loss: 0.07879990339279175
Batch 58/64 loss: 0.08861643075942993
Batch 59/64 loss: 0.0893281102180481
Batch 60/64 loss: 0.0905609130859375
Batch 61/64 loss: 0.08691513538360596
Batch 62/64 loss: 0.09988021850585938
Batch 63/64 loss: 0.10556650161743164
Batch 64/64 loss: 0.09990495443344116
Epoch 139  Train loss: 0.08799453740026436  Val loss: 0.10649162940552964
Saving best model, epoch: 139
Epoch 140
-------------------------------
Batch 1/64 loss: 0.08973157405853271
Batch 2/64 loss: 0.09285569190979004
Batch 3/64 loss: 0.0774308443069458
Batch 4/64 loss: 0.08142781257629395
Batch 5/64 loss: 0.08152854442596436
Batch 6/64 loss: 0.09223657846450806
Batch 7/64 loss: 0.083579421043396
Batch 8/64 loss: 0.08761739730834961
Batch 9/64 loss: 0.09188544750213623
Batch 10/64 loss: 0.07973015308380127
Batch 11/64 loss: 0.0853356122970581
Batch 12/64 loss: 0.0927802324295044
Batch 13/64 loss: 0.07678776979446411
Batch 14/64 loss: 0.0812104344367981
Batch 15/64 loss: 0.09272497892379761
Batch 16/64 loss: 0.08830171823501587
Batch 17/64 loss: 0.08216094970703125
Batch 18/64 loss: 0.08607566356658936
Batch 19/64 loss: 0.0854266881942749
Batch 20/64 loss: 0.09029972553253174
Batch 21/64 loss: 0.08778154850006104
Batch 22/64 loss: 0.08536404371261597
Batch 23/64 loss: 0.07790440320968628
Batch 24/64 loss: 0.08799290657043457
Batch 25/64 loss: 0.09038078784942627
Batch 26/64 loss: 0.09083038568496704
Batch 27/64 loss: 0.08603405952453613
Batch 28/64 loss: 0.09211134910583496
Batch 29/64 loss: 0.07713615894317627
Batch 30/64 loss: 0.0885496735572815
Batch 31/64 loss: 0.09484326839447021
Batch 32/64 loss: 0.08728927373886108
Batch 33/64 loss: 0.10238897800445557
Batch 34/64 loss: 0.09499675035476685
Batch 35/64 loss: 0.09987044334411621
Batch 36/64 loss: 0.08821266889572144
Batch 37/64 loss: 0.07755565643310547
Batch 38/64 loss: 0.08497029542922974
Batch 39/64 loss: 0.08200830221176147
Batch 40/64 loss: 0.08573192358016968
Batch 41/64 loss: 0.08625984191894531
Batch 42/64 loss: 0.10950803756713867
Batch 43/64 loss: 0.09597361087799072
Batch 44/64 loss: 0.09965980052947998
Batch 45/64 loss: 0.08340370655059814
Batch 46/64 loss: 0.09835255146026611
Batch 47/64 loss: 0.0847618579864502
Batch 48/64 loss: 0.07811927795410156
Batch 49/64 loss: 0.0966038703918457
Batch 50/64 loss: 0.09993398189544678
Batch 51/64 loss: 0.0902336835861206
Batch 52/64 loss: 0.07925176620483398
Batch 53/64 loss: 0.08574843406677246
Batch 54/64 loss: 0.09883272647857666
Batch 55/64 loss: 0.080799400806427
Batch 56/64 loss: 0.08811026811599731
Batch 57/64 loss: 0.09464865922927856
Batch 58/64 loss: 0.08362013101577759
Batch 59/64 loss: 0.08447909355163574
Batch 60/64 loss: 0.08067142963409424
Batch 61/64 loss: 0.082516610622406
Batch 62/64 loss: 0.09828197956085205
Batch 63/64 loss: 0.09366941452026367
Batch 64/64 loss: 0.08368730545043945
Epoch 140  Train loss: 0.08811428593654258  Val loss: 0.11394559231001078
Epoch 141
-------------------------------
Batch 1/64 loss: 0.09146726131439209
Batch 2/64 loss: 0.08693426847457886
Batch 3/64 loss: 0.0992472767829895
Batch 4/64 loss: 0.07810688018798828
Batch 5/64 loss: 0.08314120769500732
Batch 6/64 loss: 0.07184326648712158
Batch 7/64 loss: 0.09670192003250122
Batch 8/64 loss: 0.07611817121505737
Batch 9/64 loss: 0.091311514377594
Batch 10/64 loss: 0.08294546604156494
Batch 11/64 loss: 0.09454154968261719
Batch 12/64 loss: 0.07580530643463135
Batch 13/64 loss: 0.08749634027481079
Batch 14/64 loss: 0.1000518798828125
Batch 15/64 loss: 0.09282201528549194
Batch 16/64 loss: 0.0821574330329895
Batch 17/64 loss: 0.08117961883544922
Batch 18/64 loss: 0.08667439222335815
Batch 19/64 loss: 0.09379357099533081
Batch 20/64 loss: 0.10508960485458374
Batch 21/64 loss: 0.07895845174789429
Batch 22/64 loss: 0.0895116925239563
Batch 23/64 loss: 0.08090144395828247
Batch 24/64 loss: 0.09270060062408447
Batch 25/64 loss: 0.0821300745010376
Batch 26/64 loss: 0.09338337182998657
Batch 27/64 loss: 0.09300887584686279
Batch 28/64 loss: 0.09083175659179688
Batch 29/64 loss: 0.09581345319747925
Batch 30/64 loss: 0.09603148698806763
Batch 31/64 loss: 0.08010780811309814
Batch 32/64 loss: 0.09966564178466797
Batch 33/64 loss: 0.08919590711593628
Batch 34/64 loss: 0.08257710933685303
Batch 35/64 loss: 0.08973526954650879
Batch 36/64 loss: 0.0894317626953125
Batch 37/64 loss: 0.06854778528213501
Batch 38/64 loss: 0.08599328994750977
Batch 39/64 loss: 0.0816153883934021
Batch 40/64 loss: 0.09209710359573364
Batch 41/64 loss: 0.07959800958633423
Batch 42/64 loss: 0.09733545780181885
Batch 43/64 loss: 0.09279686212539673
Batch 44/64 loss: 0.08245891332626343
Batch 45/64 loss: 0.0767785906791687
Batch 46/64 loss: 0.08815199136734009
Batch 47/64 loss: 0.08761328458786011
Batch 48/64 loss: 0.09485721588134766
Batch 49/64 loss: 0.09511774778366089
Batch 50/64 loss: 0.08278858661651611
Batch 51/64 loss: 0.07634449005126953
Batch 52/64 loss: 0.07288467884063721
Batch 53/64 loss: 0.07666951417922974
Batch 54/64 loss: 0.0745689868927002
Batch 55/64 loss: 0.08683121204376221
Batch 56/64 loss: 0.09223973751068115
Batch 57/64 loss: 0.0864524245262146
Batch 58/64 loss: 0.08695977926254272
Batch 59/64 loss: 0.09318709373474121
Batch 60/64 loss: 0.08699339628219604
Batch 61/64 loss: 0.08775782585144043
Batch 62/64 loss: 0.07934850454330444
Batch 63/64 loss: 0.09150981903076172
Batch 64/64 loss: 0.08536171913146973
Epoch 141  Train loss: 0.08694796281702379  Val loss: 0.10676538473142382
Epoch 142
-------------------------------
Batch 1/64 loss: 0.0708993673324585
Batch 2/64 loss: 0.07973343133926392
Batch 3/64 loss: 0.08257496356964111
Batch 4/64 loss: 0.08112025260925293
Batch 5/64 loss: 0.08157134056091309
Batch 6/64 loss: 0.07619589567184448
Batch 7/64 loss: 0.07779860496520996
Batch 8/64 loss: 0.07510900497436523
Batch 9/64 loss: 0.08468073606491089
Batch 10/64 loss: 0.05850797891616821
Batch 11/64 loss: 0.10323441028594971
Batch 12/64 loss: 0.08386027812957764
Batch 13/64 loss: 0.08088821172714233
Batch 14/64 loss: 0.09182995557785034
Batch 15/64 loss: 0.09220468997955322
Batch 16/64 loss: 0.09088033437728882
Batch 17/64 loss: 0.08961039781570435
Batch 18/64 loss: 0.0790715217590332
Batch 19/64 loss: 0.08471441268920898
Batch 20/64 loss: 0.09585434198379517
Batch 21/64 loss: 0.07692456245422363
Batch 22/64 loss: 0.08491116762161255
Batch 23/64 loss: 0.09714466333389282
Batch 24/64 loss: 0.08072274923324585
Batch 25/64 loss: 0.08430814743041992
Batch 26/64 loss: 0.07606315612792969
Batch 27/64 loss: 0.09135276079177856
Batch 28/64 loss: 0.09000617265701294
Batch 29/64 loss: 0.08615517616271973
Batch 30/64 loss: 0.08721810579299927
Batch 31/64 loss: 0.10272109508514404
Batch 32/64 loss: 0.08075934648513794
Batch 33/64 loss: 0.09250366687774658
Batch 34/64 loss: 0.09985506534576416
Batch 35/64 loss: 0.0845632553100586
Batch 36/64 loss: 0.09092915058135986
Batch 37/64 loss: 0.09128212928771973
Batch 38/64 loss: 0.08351987600326538
Batch 39/64 loss: 0.08559733629226685
Batch 40/64 loss: 0.0883028507232666
Batch 41/64 loss: 0.09364819526672363
Batch 42/64 loss: 0.09242761135101318
Batch 43/64 loss: 0.09217917919158936
Batch 44/64 loss: 0.07255756855010986
Batch 45/64 loss: 0.10490703582763672
Batch 46/64 loss: 0.08864009380340576
Batch 47/64 loss: 0.09667527675628662
Batch 48/64 loss: 0.0861290693283081
Batch 49/64 loss: 0.08316576480865479
Batch 50/64 loss: 0.09316796064376831
Batch 51/64 loss: 0.08253735303878784
Batch 52/64 loss: 0.08759599924087524
Batch 53/64 loss: 0.07706862688064575
Batch 54/64 loss: 0.0934327244758606
Batch 55/64 loss: 0.08773374557495117
Batch 56/64 loss: 0.08129411935806274
Batch 57/64 loss: 0.08696293830871582
Batch 58/64 loss: 0.08441483974456787
Batch 59/64 loss: 0.0884740948677063
Batch 60/64 loss: 0.09607267379760742
Batch 61/64 loss: 0.10428082942962646
Batch 62/64 loss: 0.08209240436553955
Batch 63/64 loss: 0.08822071552276611
Batch 64/64 loss: 0.07100099325180054
Epoch 142  Train loss: 0.0864644726117452  Val loss: 0.10442862703218493
Saving best model, epoch: 142
Epoch 143
-------------------------------
Batch 1/64 loss: 0.0944717526435852
Batch 2/64 loss: 0.0892290472984314
Batch 3/64 loss: 0.07436555624008179
Batch 4/64 loss: 0.07079261541366577
Batch 5/64 loss: 0.09543216228485107
Batch 6/64 loss: 0.08554279804229736
Batch 7/64 loss: 0.07792991399765015
Batch 8/64 loss: 0.08138275146484375
Batch 9/64 loss: 0.09542787075042725
Batch 10/64 loss: 0.08877646923065186
Batch 11/64 loss: 0.071461021900177
Batch 12/64 loss: 0.0768357515335083
Batch 13/64 loss: 0.08030492067337036
Batch 14/64 loss: 0.08510726690292358
Batch 15/64 loss: 0.09869140386581421
Batch 16/64 loss: 0.08583939075469971
Batch 17/64 loss: 0.08736294507980347
Batch 18/64 loss: 0.08507084846496582
Batch 19/64 loss: 0.08616089820861816
Batch 20/64 loss: 0.09055846929550171
Batch 21/64 loss: 0.08299076557159424
Batch 22/64 loss: 0.08926725387573242
Batch 23/64 loss: 0.08081412315368652
Batch 24/64 loss: 0.06756055355072021
Batch 25/64 loss: 0.07892429828643799
Batch 26/64 loss: 0.07508742809295654
Batch 27/64 loss: 0.07187837362289429
Batch 28/64 loss: 0.09134900569915771
Batch 29/64 loss: 0.0940636396408081
Batch 30/64 loss: 0.08406412601470947
Batch 31/64 loss: 0.08633363246917725
Batch 32/64 loss: 0.08688086271286011
Batch 33/64 loss: 0.08061879873275757
Batch 34/64 loss: 0.08032190799713135
Batch 35/64 loss: 0.08031928539276123
Batch 36/64 loss: 0.08306843042373657
Batch 37/64 loss: 0.08678120374679565
Batch 38/64 loss: 0.08712387084960938
Batch 39/64 loss: 0.07472670078277588
Batch 40/64 loss: 0.09318053722381592
Batch 41/64 loss: 0.07192403078079224
Batch 42/64 loss: 0.08319747447967529
Batch 43/64 loss: 0.06967860460281372
Batch 44/64 loss: 0.09099578857421875
Batch 45/64 loss: 0.0908193588256836
Batch 46/64 loss: 0.0897902250289917
Batch 47/64 loss: 0.08802425861358643
Batch 48/64 loss: 0.08210164308547974
Batch 49/64 loss: 0.09493410587310791
Batch 50/64 loss: 0.09689772129058838
Batch 51/64 loss: 0.08416914939880371
Batch 52/64 loss: 0.08983665704727173
Batch 53/64 loss: 0.0823858380317688
Batch 54/64 loss: 0.08905941247940063
Batch 55/64 loss: 0.09035050868988037
Batch 56/64 loss: 0.09358656406402588
Batch 57/64 loss: 0.10835707187652588
Batch 58/64 loss: 0.08355844020843506
Batch 59/64 loss: 0.09354519844055176
Batch 60/64 loss: 0.08370864391326904
Batch 61/64 loss: 0.08597838878631592
Batch 62/64 loss: 0.0801476240158081
Batch 63/64 loss: 0.07896745204925537
Batch 64/64 loss: 0.08154040575027466
Epoch 143  Train loss: 0.08500812731537165  Val loss: 0.10745476918531857
Epoch 144
-------------------------------
Batch 1/64 loss: 0.07640296220779419
Batch 2/64 loss: 0.08296167850494385
Batch 3/64 loss: 0.08345812559127808
Batch 4/64 loss: 0.09490323066711426
Batch 5/64 loss: 0.07761484384536743
Batch 6/64 loss: 0.08382511138916016
Batch 7/64 loss: 0.07193797826766968
Batch 8/64 loss: 0.08233153820037842
Batch 9/64 loss: 0.0858035683631897
Batch 10/64 loss: 0.09832870960235596
Batch 11/64 loss: 0.08359843492507935
Batch 12/64 loss: 0.07929050922393799
Batch 13/64 loss: 0.06088322401046753
Batch 14/64 loss: 0.07621622085571289
Batch 15/64 loss: 0.08092409372329712
Batch 16/64 loss: 0.08366125822067261
Batch 17/64 loss: 0.08345657587051392
Batch 18/64 loss: 0.09669864177703857
Batch 19/64 loss: 0.07407796382904053
Batch 20/64 loss: 0.09189736843109131
Batch 21/64 loss: 0.09169584512710571
Batch 22/64 loss: 0.08611643314361572
Batch 23/64 loss: 0.0829351544380188
Batch 24/64 loss: 0.08821165561676025
Batch 25/64 loss: 0.08167219161987305
Batch 26/64 loss: 0.07958364486694336
Batch 27/64 loss: 0.08096307516098022
Batch 28/64 loss: 0.08320486545562744
Batch 29/64 loss: 0.08138030767440796
Batch 30/64 loss: 0.0773889422416687
Batch 31/64 loss: 0.08378088474273682
Batch 32/64 loss: 0.08700865507125854
Batch 33/64 loss: 0.07491093873977661
Batch 34/64 loss: 0.07614094018936157
Batch 35/64 loss: 0.08601641654968262
Batch 36/64 loss: 0.0787995457649231
Batch 37/64 loss: 0.07377880811691284
Batch 38/64 loss: 0.10349607467651367
Batch 39/64 loss: 0.08279073238372803
Batch 40/64 loss: 0.08362597227096558
Batch 41/64 loss: 0.07379329204559326
Batch 42/64 loss: 0.09278368949890137
Batch 43/64 loss: 0.09961777925491333
Batch 44/64 loss: 0.08259111642837524
Batch 45/64 loss: 0.09545242786407471
Batch 46/64 loss: 0.0776747465133667
Batch 47/64 loss: 0.07803833484649658
Batch 48/64 loss: 0.09402388334274292
Batch 49/64 loss: 0.10310328006744385
Batch 50/64 loss: 0.0779259204864502
Batch 51/64 loss: 0.09385120868682861
Batch 52/64 loss: 0.09242302179336548
Batch 53/64 loss: 0.08710134029388428
Batch 54/64 loss: 0.07469362020492554
Batch 55/64 loss: 0.09643948078155518
Batch 56/64 loss: 0.08725380897521973
Batch 57/64 loss: 0.09153103828430176
Batch 58/64 loss: 0.07611995935440063
Batch 59/64 loss: 0.07724916934967041
Batch 60/64 loss: 0.07151871919631958
Batch 61/64 loss: 0.07350152730941772
Batch 62/64 loss: 0.07879042625427246
Batch 63/64 loss: 0.09445011615753174
Batch 64/64 loss: 0.10844689607620239
Epoch 144  Train loss: 0.08412605804555556  Val loss: 0.10632629554296277
Epoch 145
-------------------------------
Batch 1/64 loss: 0.08677530288696289
Batch 2/64 loss: 0.08446598052978516
Batch 3/64 loss: 0.08251899480819702
Batch 4/64 loss: 0.07845568656921387
Batch 5/64 loss: 0.11285912990570068
Batch 6/64 loss: 0.08823555707931519
Batch 7/64 loss: 0.07436877489089966
Batch 8/64 loss: 0.07871228456497192
Batch 9/64 loss: 0.08471989631652832
Batch 10/64 loss: 0.07478898763656616
Batch 11/64 loss: 0.08089196681976318
Batch 12/64 loss: 0.08530330657958984
Batch 13/64 loss: 0.08769232034683228
Batch 14/64 loss: 0.08739972114562988
Batch 15/64 loss: 0.08130168914794922
Batch 16/64 loss: 0.09064781665802002
Batch 17/64 loss: 0.08025074005126953
Batch 18/64 loss: 0.07617026567459106
Batch 19/64 loss: 0.07286900281906128
Batch 20/64 loss: 0.08814054727554321
Batch 21/64 loss: 0.07513058185577393
Batch 22/64 loss: 0.07726603746414185
Batch 23/64 loss: 0.07941305637359619
Batch 24/64 loss: 0.0798882246017456
Batch 25/64 loss: 0.08205127716064453
Batch 26/64 loss: 0.07301527261734009
Batch 27/64 loss: 0.07902532815933228
Batch 28/64 loss: 0.07191306352615356
Batch 29/64 loss: 0.09270280599594116
Batch 30/64 loss: 0.0829693078994751
Batch 31/64 loss: 0.07451766729354858
Batch 32/64 loss: 0.08438599109649658
Batch 33/64 loss: 0.08297193050384521
Batch 34/64 loss: 0.0847165584564209
Batch 35/64 loss: 0.07337307929992676
Batch 36/64 loss: 0.08802181482315063
Batch 37/64 loss: 0.09910750389099121
Batch 38/64 loss: 0.08128643035888672
Batch 39/64 loss: 0.07329946756362915
Batch 40/64 loss: 0.08246272802352905
Batch 41/64 loss: 0.08358478546142578
Batch 42/64 loss: 0.08225077390670776
Batch 43/64 loss: 0.07074570655822754
Batch 44/64 loss: 0.0791020393371582
Batch 45/64 loss: 0.07120406627655029
Batch 46/64 loss: 0.0881493091583252
Batch 47/64 loss: 0.0868573784828186
Batch 48/64 loss: 0.08131253719329834
Batch 49/64 loss: 0.07524394989013672
Batch 50/64 loss: 0.08263552188873291
Batch 51/64 loss: 0.08039027452468872
Batch 52/64 loss: 0.07821035385131836
Batch 53/64 loss: 0.09019756317138672
Batch 54/64 loss: 0.06956547498703003
Batch 55/64 loss: 0.08663344383239746
Batch 56/64 loss: 0.08570468425750732
Batch 57/64 loss: 0.09231078624725342
Batch 58/64 loss: 0.08977079391479492
Batch 59/64 loss: 0.07572698593139648
Batch 60/64 loss: 0.09636455774307251
Batch 61/64 loss: 0.08936554193496704
Batch 62/64 loss: 0.08779621124267578
Batch 63/64 loss: 0.07628333568572998
Batch 64/64 loss: 0.08288025856018066
Epoch 145  Train loss: 0.0824416057736266  Val loss: 0.10582692864834238
Epoch 146
-------------------------------
Batch 1/64 loss: 0.09402382373809814
Batch 2/64 loss: 0.07170730829238892
Batch 3/64 loss: 0.0707482099533081
Batch 4/64 loss: 0.0736883282661438
Batch 5/64 loss: 0.08042794466018677
Batch 6/64 loss: 0.08773809671401978
Batch 7/64 loss: 0.09052145481109619
Batch 8/64 loss: 0.08473074436187744
Batch 9/64 loss: 0.08566796779632568
Batch 10/64 loss: 0.08886897563934326
Batch 11/64 loss: 0.0749703049659729
Batch 12/64 loss: 0.08343487977981567
Batch 13/64 loss: 0.09349924325942993
Batch 14/64 loss: 0.07789850234985352
Batch 15/64 loss: 0.07015830278396606
Batch 16/64 loss: 0.09734505414962769
Batch 17/64 loss: 0.09024035930633545
Batch 18/64 loss: 0.08537238836288452
Batch 19/64 loss: 0.0738142728805542
Batch 20/64 loss: 0.07332855463027954
Batch 21/64 loss: 0.08357453346252441
Batch 22/64 loss: 0.07907688617706299
Batch 23/64 loss: 0.08752584457397461
Batch 24/64 loss: 0.08809995651245117
Batch 25/64 loss: 0.07961732149124146
Batch 26/64 loss: 0.0779954195022583
Batch 27/64 loss: 0.08148884773254395
Batch 28/64 loss: 0.07965290546417236
Batch 29/64 loss: 0.07177746295928955
Batch 30/64 loss: 0.0783568024635315
Batch 31/64 loss: 0.07945847511291504
Batch 32/64 loss: 0.08549749851226807
Batch 33/64 loss: 0.08645009994506836
Batch 34/64 loss: 0.08627188205718994
Batch 35/64 loss: 0.09280270338058472
Batch 36/64 loss: 0.08188027143478394
Batch 37/64 loss: 0.08345562219619751
Batch 38/64 loss: 0.10173571109771729
Batch 39/64 loss: 0.07011735439300537
Batch 40/64 loss: 0.08962017297744751
Batch 41/64 loss: 0.08841508626937866
Batch 42/64 loss: 0.09276950359344482
Batch 43/64 loss: 0.07650560140609741
Batch 44/64 loss: 0.0854729413986206
Batch 45/64 loss: 0.09558272361755371
Batch 46/64 loss: 0.08247727155685425
Batch 47/64 loss: 0.09984499216079712
Batch 48/64 loss: 0.08498650789260864
Batch 49/64 loss: 0.08104449510574341
Batch 50/64 loss: 0.07560783624649048
Batch 51/64 loss: 0.08505380153656006
Batch 52/64 loss: 0.08649146556854248
Batch 53/64 loss: 0.07597655057907104
Batch 54/64 loss: 0.08856779336929321
Batch 55/64 loss: 0.0940137505531311
Batch 56/64 loss: 0.09057831764221191
Batch 57/64 loss: 0.06948310136795044
Batch 58/64 loss: 0.09123408794403076
Batch 59/64 loss: 0.08414065837860107
Batch 60/64 loss: 0.08409816026687622
Batch 61/64 loss: 0.08052754402160645
Batch 62/64 loss: 0.09923017024993896
Batch 63/64 loss: 0.09929025173187256
Batch 64/64 loss: 0.07340490818023682
Epoch 146  Train loss: 0.08406410638023826  Val loss: 0.1101799668725004
Epoch 147
-------------------------------
Batch 1/64 loss: 0.07912468910217285
Batch 2/64 loss: 0.07566708326339722
Batch 3/64 loss: 0.08673965930938721
Batch 4/64 loss: 0.07197684049606323
Batch 5/64 loss: 0.0854882001876831
Batch 6/64 loss: 0.10022246837615967
Batch 7/64 loss: 0.09149909019470215
Batch 8/64 loss: 0.08351314067840576
Batch 9/64 loss: 0.08298438787460327
Batch 10/64 loss: 0.09213817119598389
Batch 11/64 loss: 0.0899541974067688
Batch 12/64 loss: 0.08338671922683716
Batch 13/64 loss: 0.08709251880645752
Batch 14/64 loss: 0.0861196517944336
Batch 15/64 loss: 0.07865852117538452
Batch 16/64 loss: 0.07534819841384888
Batch 17/64 loss: 0.08835935592651367
Batch 18/64 loss: 0.08462721109390259
Batch 19/64 loss: 0.07973623275756836
Batch 20/64 loss: 0.07176458835601807
Batch 21/64 loss: 0.0690159797668457
Batch 22/64 loss: 0.06274867057800293
Batch 23/64 loss: 0.07080972194671631
Batch 24/64 loss: 0.0719950795173645
Batch 25/64 loss: 0.09802055358886719
Batch 26/64 loss: 0.08618748188018799
Batch 27/64 loss: 0.08027499914169312
Batch 28/64 loss: 0.08653807640075684
Batch 29/64 loss: 0.08342695236206055
Batch 30/64 loss: 0.07436448335647583
Batch 31/64 loss: 0.0818791389465332
Batch 32/64 loss: 0.07789993286132812
Batch 33/64 loss: 0.08087378740310669
Batch 34/64 loss: 0.08469176292419434
Batch 35/64 loss: 0.07699120044708252
Batch 36/64 loss: 0.07871681451797485
Batch 37/64 loss: 0.07121992111206055
Batch 38/64 loss: 0.07963693141937256
Batch 39/64 loss: 0.0996624231338501
Batch 40/64 loss: 0.08635860681533813
Batch 41/64 loss: 0.0874980092048645
Batch 42/64 loss: 0.07671546936035156
Batch 43/64 loss: 0.09415137767791748
Batch 44/64 loss: 0.09171253442764282
Batch 45/64 loss: 0.07971757650375366
Batch 46/64 loss: 0.07505863904953003
Batch 47/64 loss: 0.0752488374710083
Batch 48/64 loss: 0.07649403810501099
Batch 49/64 loss: 0.07562011480331421
Batch 50/64 loss: 0.09501039981842041
Batch 51/64 loss: 0.08056819438934326
Batch 52/64 loss: 0.08702808618545532
Batch 53/64 loss: 0.0775827169418335
Batch 54/64 loss: 0.07448309659957886
Batch 55/64 loss: 0.084148108959198
Batch 56/64 loss: 0.08749961853027344
Batch 57/64 loss: 0.07862591743469238
Batch 58/64 loss: 0.08964574337005615
Batch 59/64 loss: 0.08980345726013184
Batch 60/64 loss: 0.08483850955963135
Batch 61/64 loss: 0.09095358848571777
Batch 62/64 loss: 0.07523024082183838
Batch 63/64 loss: 0.0887136459350586
Batch 64/64 loss: 0.08835554122924805
Epoch 147  Train loss: 0.08248357679329667  Val loss: 0.10691075288143355
Epoch 148
-------------------------------
Batch 1/64 loss: 0.08311665058135986
Batch 2/64 loss: 0.08050733804702759
Batch 3/64 loss: 0.07792091369628906
Batch 4/64 loss: 0.0797472596168518
Batch 5/64 loss: 0.08460801839828491
Batch 6/64 loss: 0.06923681497573853
Batch 7/64 loss: 0.07514238357543945
Batch 8/64 loss: 0.08192670345306396
Batch 9/64 loss: 0.07083266973495483
Batch 10/64 loss: 0.09310746192932129
Batch 11/64 loss: 0.08420717716217041
Batch 12/64 loss: 0.08870244026184082
Batch 13/64 loss: 0.0842665433883667
Batch 14/64 loss: 0.08408427238464355
Batch 15/64 loss: 0.0826941728591919
Batch 16/64 loss: 0.08338308334350586
Batch 17/64 loss: 0.08175867795944214
Batch 18/64 loss: 0.0940437912940979
Batch 19/64 loss: 0.08243393898010254
Batch 20/64 loss: 0.07808160781860352
Batch 21/64 loss: 0.08310884237289429
Batch 22/64 loss: 0.06599509716033936
Batch 23/64 loss: 0.09986484050750732
Batch 24/64 loss: 0.09929788112640381
Batch 25/64 loss: 0.07920312881469727
Batch 26/64 loss: 0.09273606538772583
Batch 27/64 loss: 0.07865011692047119
Batch 28/64 loss: 0.0767824649810791
Batch 29/64 loss: 0.08352243900299072
Batch 30/64 loss: 0.08780533075332642
Batch 31/64 loss: 0.09375947713851929
Batch 32/64 loss: 0.10517925024032593
Batch 33/64 loss: 0.09741246700286865
Batch 34/64 loss: 0.08401864767074585
Batch 35/64 loss: 0.07705074548721313
Batch 36/64 loss: 0.08137327432632446
Batch 37/64 loss: 0.07516628503799438
Batch 38/64 loss: 0.08381688594818115
Batch 39/64 loss: 0.08819061517715454
Batch 40/64 loss: 0.08298659324645996
Batch 41/64 loss: 0.09258729219436646
Batch 42/64 loss: 0.08364534378051758
Batch 43/64 loss: 0.07402914762496948
Batch 44/64 loss: 0.08056396245956421
Batch 45/64 loss: 0.08435600996017456
Batch 46/64 loss: 0.08416104316711426
Batch 47/64 loss: 0.09144032001495361
Batch 48/64 loss: 0.07995873689651489
Batch 49/64 loss: 0.07504791021347046
Batch 50/64 loss: 0.08727741241455078
Batch 51/64 loss: 0.09543716907501221
Batch 52/64 loss: 0.07568210363388062
Batch 53/64 loss: 0.08042764663696289
Batch 54/64 loss: 0.06257385015487671
Batch 55/64 loss: 0.09704262018203735
Batch 56/64 loss: 0.09130948781967163
Batch 57/64 loss: 0.08678305149078369
Batch 58/64 loss: 0.08046066761016846
Batch 59/64 loss: 0.07076060771942139
Batch 60/64 loss: 0.09501278400421143
Batch 61/64 loss: 0.08565622568130493
Batch 62/64 loss: 0.07786589860916138
Batch 63/64 loss: 0.07853186130523682
Batch 64/64 loss: 0.07036644220352173
Epoch 148  Train loss: 0.0834369937578837  Val loss: 0.1028899249342299
Saving best model, epoch: 148
Epoch 149
-------------------------------
Batch 1/64 loss: 0.07406049966812134
Batch 2/64 loss: 0.08301490545272827
Batch 3/64 loss: 0.08719593286514282
Batch 4/64 loss: 0.08261549472808838
Batch 5/64 loss: 0.08302348852157593
Batch 6/64 loss: 0.0715366005897522
Batch 7/64 loss: 0.0863649845123291
Batch 8/64 loss: 0.07336980104446411
Batch 9/64 loss: 0.0756678581237793
Batch 10/64 loss: 0.08428966999053955
Batch 11/64 loss: 0.07960683107376099
Batch 12/64 loss: 0.08373266458511353
Batch 13/64 loss: 0.08138519525527954
Batch 14/64 loss: 0.08662539720535278
Batch 15/64 loss: 0.07495307922363281
Batch 16/64 loss: 0.082569420337677
Batch 17/64 loss: 0.0819472074508667
Batch 18/64 loss: 0.09022951126098633
Batch 19/64 loss: 0.08146029710769653
Batch 20/64 loss: 0.08025228977203369
Batch 21/64 loss: 0.08430677652359009
Batch 22/64 loss: 0.0982561707496643
Batch 23/64 loss: 0.06966078281402588
Batch 24/64 loss: 0.08203154802322388
Batch 25/64 loss: 0.09086740016937256
Batch 26/64 loss: 0.10308372974395752
Batch 27/64 loss: 0.07676714658737183
Batch 28/64 loss: 0.08120161294937134
Batch 29/64 loss: 0.07823854684829712
Batch 30/64 loss: 0.07494324445724487
Batch 31/64 loss: 0.0881190299987793
Batch 32/64 loss: 0.07522058486938477
Batch 33/64 loss: 0.07035231590270996
Batch 34/64 loss: 0.08898401260375977
Batch 35/64 loss: 0.07135355472564697
Batch 36/64 loss: 0.09182858467102051
Batch 37/64 loss: 0.06514763832092285
Batch 38/64 loss: 0.07542067766189575
Batch 39/64 loss: 0.08525854349136353
Batch 40/64 loss: 0.07553666830062866
Batch 41/64 loss: 0.08532243967056274
Batch 42/64 loss: 0.10210990905761719
Batch 43/64 loss: 0.07479321956634521
Batch 44/64 loss: 0.07378244400024414
Batch 45/64 loss: 0.0788084864616394
Batch 46/64 loss: 0.08535313606262207
Batch 47/64 loss: 0.08245885372161865
Batch 48/64 loss: 0.08265125751495361
Batch 49/64 loss: 0.08822447061538696
Batch 50/64 loss: 0.0794716477394104
Batch 51/64 loss: 0.08022594451904297
Batch 52/64 loss: 0.0875130295753479
Batch 53/64 loss: 0.08297979831695557
Batch 54/64 loss: 0.09771966934204102
Batch 55/64 loss: 0.07486820220947266
Batch 56/64 loss: 0.07126045227050781
Batch 57/64 loss: 0.08283078670501709
Batch 58/64 loss: 0.09143245220184326
Batch 59/64 loss: 0.07929104566574097
Batch 60/64 loss: 0.07321196794509888
Batch 61/64 loss: 0.07283514738082886
Batch 62/64 loss: 0.08106207847595215
Batch 63/64 loss: 0.07379615306854248
Batch 64/64 loss: 0.09281301498413086
Epoch 149  Train loss: 0.08160144394519282  Val loss: 0.10297044036314659
Epoch 150
-------------------------------
Batch 1/64 loss: 0.08206355571746826
Batch 2/64 loss: 0.07877457141876221
Batch 3/64 loss: 0.07199138402938843
Batch 4/64 loss: 0.08843737840652466
Batch 5/64 loss: 0.08782386779785156
Batch 6/64 loss: 0.07100933790206909
Batch 7/64 loss: 0.06637001037597656
Batch 8/64 loss: 0.07473301887512207
Batch 9/64 loss: 0.08755576610565186
Batch 10/64 loss: 0.09927564859390259
Batch 11/64 loss: 0.09632551670074463
Batch 12/64 loss: 0.07850039005279541
Batch 13/64 loss: 0.07682615518569946
Batch 14/64 loss: 0.07801806926727295
Batch 15/64 loss: 0.07323002815246582
Batch 16/64 loss: 0.08398234844207764
Batch 17/64 loss: 0.08103251457214355
Batch 18/64 loss: 0.07731056213378906
Batch 19/64 loss: 0.08228832483291626
Batch 20/64 loss: 0.08548581600189209
Batch 21/64 loss: 0.07893025875091553
Batch 22/64 loss: 0.07391321659088135
Batch 23/64 loss: 0.07382583618164062
Batch 24/64 loss: 0.08960407972335815
Batch 25/64 loss: 0.08034324645996094
Batch 26/64 loss: 0.08242940902709961
Batch 27/64 loss: 0.07739466428756714
Batch 28/64 loss: 0.0788424015045166
Batch 29/64 loss: 0.08153998851776123
Batch 30/64 loss: 0.08298122882843018
Batch 31/64 loss: 0.08812278509140015
Batch 32/64 loss: 0.08547908067703247
Batch 33/64 loss: 0.07598042488098145
Batch 34/64 loss: 0.07736390829086304
Batch 35/64 loss: 0.07368242740631104
Batch 36/64 loss: 0.07825863361358643
Batch 37/64 loss: 0.0767403244972229
Batch 38/64 loss: 0.08231371641159058
Batch 39/64 loss: 0.07677370309829712
Batch 40/64 loss: 0.07520425319671631
Batch 41/64 loss: 0.0820317268371582
Batch 42/64 loss: 0.08746147155761719
Batch 43/64 loss: 0.0785527229309082
Batch 44/64 loss: 0.07303762435913086
Batch 45/64 loss: 0.07373994588851929
Batch 46/64 loss: 0.10229140520095825
Batch 47/64 loss: 0.08061695098876953
Batch 48/64 loss: 0.07117462158203125
Batch 49/64 loss: 0.06496685743331909
Batch 50/64 loss: 0.08116263151168823
Batch 51/64 loss: 0.08593946695327759
Batch 52/64 loss: 0.0853608250617981
Batch 53/64 loss: 0.09038662910461426
Batch 54/64 loss: 0.08713209629058838
Batch 55/64 loss: 0.07932573556900024
Batch 56/64 loss: 0.07716542482376099
Batch 57/64 loss: 0.07014530897140503
Batch 58/64 loss: 0.09876763820648193
Batch 59/64 loss: 0.08239996433258057
Batch 60/64 loss: 0.07494992017745972
Batch 61/64 loss: 0.0803835391998291
Batch 62/64 loss: 0.09579223394393921
Batch 63/64 loss: 0.08041834831237793
Batch 64/64 loss: 0.09416848421096802
Epoch 150  Train loss: 0.08101313418033076  Val loss: 0.10420419014606279
Epoch 151
-------------------------------
Batch 1/64 loss: 0.07744228839874268
Batch 2/64 loss: 0.07737410068511963
Batch 3/64 loss: 0.08785498142242432
Batch 4/64 loss: 0.06976807117462158
Batch 5/64 loss: 0.07067984342575073
Batch 6/64 loss: 0.07142484188079834
Batch 7/64 loss: 0.0805101990699768
Batch 8/64 loss: 0.07616806030273438
Batch 9/64 loss: 0.06777089834213257
Batch 10/64 loss: 0.08603912591934204
Batch 11/64 loss: 0.07491481304168701
Batch 12/64 loss: 0.08127331733703613
Batch 13/64 loss: 0.07449370622634888
Batch 14/64 loss: 0.06909036636352539
Batch 15/64 loss: 0.0891345739364624
Batch 16/64 loss: 0.09169244766235352
Batch 17/64 loss: 0.07474863529205322
Batch 18/64 loss: 0.08518266677856445
Batch 19/64 loss: 0.07550430297851562
Batch 20/64 loss: 0.0789867639541626
Batch 21/64 loss: 0.06955212354660034
Batch 22/64 loss: 0.09888124465942383
Batch 23/64 loss: 0.0877724289894104
Batch 24/64 loss: 0.08614885807037354
Batch 25/64 loss: 0.06910383701324463
Batch 26/64 loss: 0.07201546430587769
Batch 27/64 loss: 0.08531439304351807
Batch 28/64 loss: 0.08251738548278809
Batch 29/64 loss: 0.07674634456634521
Batch 30/64 loss: 0.07931250333786011
Batch 31/64 loss: 0.07451552152633667
Batch 32/64 loss: 0.07448512315750122
Batch 33/64 loss: 0.09770286083221436
Batch 34/64 loss: 0.07791906595230103
Batch 35/64 loss: 0.07645297050476074
Batch 36/64 loss: 0.0783810019493103
Batch 37/64 loss: 0.09535688161849976
Batch 38/64 loss: 0.09052371978759766
Batch 39/64 loss: 0.07968324422836304
Batch 40/64 loss: 0.07109677791595459
Batch 41/64 loss: 0.06819671392440796
Batch 42/64 loss: 0.07609069347381592
Batch 43/64 loss: 0.08574551343917847
Batch 44/64 loss: 0.08393043279647827
Batch 45/64 loss: 0.08274585008621216
Batch 46/64 loss: 0.07785093784332275
Batch 47/64 loss: 0.06493330001831055
Batch 48/64 loss: 0.07225430011749268
Batch 49/64 loss: 0.08060252666473389
Batch 50/64 loss: 0.06448507308959961
Batch 51/64 loss: 0.08304524421691895
Batch 52/64 loss: 0.08222252130508423
Batch 53/64 loss: 0.07665950059890747
Batch 54/64 loss: 0.07420551776885986
Batch 55/64 loss: 0.07404583692550659
Batch 56/64 loss: 0.09401381015777588
Batch 57/64 loss: 0.0665397047996521
Batch 58/64 loss: 0.0633116364479065
Batch 59/64 loss: 0.07910871505737305
Batch 60/64 loss: 0.08697372674942017
Batch 61/64 loss: 0.08399271965026855
Batch 62/64 loss: 0.0795479416847229
Batch 63/64 loss: 0.0850324034690857
Batch 64/64 loss: 0.07289034128189087
Epoch 151  Train loss: 0.07880373538709154  Val loss: 0.10138043710046618
Saving best model, epoch: 151
Epoch 152
-------------------------------
Batch 1/64 loss: 0.07290709018707275
Batch 2/64 loss: 0.06432312726974487
Batch 3/64 loss: 0.07859593629837036
Batch 4/64 loss: 0.0853503942489624
Batch 5/64 loss: 0.07240468263626099
Batch 6/64 loss: 0.07564091682434082
Batch 7/64 loss: 0.07873976230621338
Batch 8/64 loss: 0.09994226694107056
Batch 9/64 loss: 0.06994736194610596
Batch 10/64 loss: 0.07944351434707642
Batch 11/64 loss: 0.08020055294036865
Batch 12/64 loss: 0.09185326099395752
Batch 13/64 loss: 0.0799412727355957
Batch 14/64 loss: 0.06690669059753418
Batch 15/64 loss: 0.06951326131820679
Batch 16/64 loss: 0.08029931783676147
Batch 17/64 loss: 0.09360599517822266
Batch 18/64 loss: 0.0851210355758667
Batch 19/64 loss: 0.07251518964767456
Batch 20/64 loss: 0.08368343114852905
Batch 21/64 loss: 0.07975101470947266
Batch 22/64 loss: 0.08702832460403442
Batch 23/64 loss: 0.07502490282058716
Batch 24/64 loss: 0.08427727222442627
Batch 25/64 loss: 0.0737963318824768
Batch 26/64 loss: 0.06927239894866943
Batch 27/64 loss: 0.07825404405593872
Batch 28/64 loss: 0.08818554878234863
Batch 29/64 loss: 0.07613146305084229
Batch 30/64 loss: 0.06653600931167603
Batch 31/64 loss: 0.07176530361175537
Batch 32/64 loss: 0.08939659595489502
Batch 33/64 loss: 0.08012259006500244
Batch 34/64 loss: 0.06879281997680664
Batch 35/64 loss: 0.07509708404541016
Batch 36/64 loss: 0.07832598686218262
Batch 37/64 loss: 0.07826244831085205
Batch 38/64 loss: 0.06952160596847534
Batch 39/64 loss: 0.07974451780319214
Batch 40/64 loss: 0.07022511959075928
Batch 41/64 loss: 0.09606397151947021
Batch 42/64 loss: 0.08215487003326416
Batch 43/64 loss: 0.06895983219146729
Batch 44/64 loss: 0.08355492353439331
Batch 45/64 loss: 0.08579397201538086
Batch 46/64 loss: 0.07797086238861084
Batch 47/64 loss: 0.07258355617523193
Batch 48/64 loss: 0.08353501558303833
Batch 49/64 loss: 0.0796893835067749
Batch 50/64 loss: 0.07140195369720459
Batch 51/64 loss: 0.0811234712600708
Batch 52/64 loss: 0.07743716239929199
Batch 53/64 loss: 0.1010400652885437
Batch 54/64 loss: 0.06571060419082642
Batch 55/64 loss: 0.08232671022415161
Batch 56/64 loss: 0.057226479053497314
Batch 57/64 loss: 0.07426655292510986
Batch 58/64 loss: 0.08289003372192383
Batch 59/64 loss: 0.07844215631484985
Batch 60/64 loss: 0.06941789388656616
Batch 61/64 loss: 0.06523823738098145
Batch 62/64 loss: 0.0776781439781189
Batch 63/64 loss: 0.07115435600280762
Batch 64/64 loss: 0.07952439785003662
Epoch 152  Train loss: 0.07789411685046027  Val loss: 0.10380033836331974
Epoch 153
-------------------------------
Batch 1/64 loss: 0.07382261753082275
Batch 2/64 loss: 0.07222509384155273
Batch 3/64 loss: 0.07884001731872559
Batch 4/64 loss: 0.07268750667572021
Batch 5/64 loss: 0.0697932243347168
Batch 6/64 loss: 0.07507383823394775
Batch 7/64 loss: 0.09214025735855103
Batch 8/64 loss: 0.08012545108795166
Batch 9/64 loss: 0.08853256702423096
Batch 10/64 loss: 0.07449018955230713
Batch 11/64 loss: 0.07276862859725952
Batch 12/64 loss: 0.0714755654335022
Batch 13/64 loss: 0.07999038696289062
Batch 14/64 loss: 0.07966452836990356
Batch 15/64 loss: 0.06546306610107422
Batch 16/64 loss: 0.07189297676086426
Batch 17/64 loss: 0.07042711973190308
Batch 18/64 loss: 0.0677000880241394
Batch 19/64 loss: 0.08246862888336182
Batch 20/64 loss: 0.08820760250091553
Batch 21/64 loss: 0.07638168334960938
Batch 22/64 loss: 0.07940924167633057
Batch 23/64 loss: 0.08336824178695679
Batch 24/64 loss: 0.07918834686279297
Batch 25/64 loss: 0.07313549518585205
Batch 26/64 loss: 0.07443267107009888
Batch 27/64 loss: 0.07088476419448853
Batch 28/64 loss: 0.06607484817504883
Batch 29/64 loss: 0.071685791015625
Batch 30/64 loss: 0.0777062177658081
Batch 31/64 loss: 0.08089315891265869
Batch 32/64 loss: 0.07657492160797119
Batch 33/64 loss: 0.057305872440338135
Batch 34/64 loss: 0.09221488237380981
Batch 35/64 loss: 0.07877236604690552
Batch 36/64 loss: 0.09108930826187134
Batch 37/64 loss: 0.09980249404907227
Batch 38/64 loss: 0.0686025619506836
Batch 39/64 loss: 0.07108902931213379
Batch 40/64 loss: 0.08013474941253662
Batch 41/64 loss: 0.08578550815582275
Batch 42/64 loss: 0.0809248685836792
Batch 43/64 loss: 0.08391386270523071
Batch 44/64 loss: 0.0725448727607727
Batch 45/64 loss: 0.08347153663635254
Batch 46/64 loss: 0.0693291425704956
Batch 47/64 loss: 0.08388131856918335
Batch 48/64 loss: 0.0732298493385315
Batch 49/64 loss: 0.10325372219085693
Batch 50/64 loss: 0.07950359582901001
Batch 51/64 loss: 0.07467478513717651
Batch 52/64 loss: 0.07199102640151978
Batch 53/64 loss: 0.06857126951217651
Batch 54/64 loss: 0.08795720338821411
Batch 55/64 loss: 0.08802908658981323
Batch 56/64 loss: 0.07348769903182983
Batch 57/64 loss: 0.08223772048950195
Batch 58/64 loss: 0.07995116710662842
Batch 59/64 loss: 0.0803523063659668
Batch 60/64 loss: 0.0813189148902893
Batch 61/64 loss: 0.07769769430160522
Batch 62/64 loss: 0.07821214199066162
Batch 63/64 loss: 0.0758097767829895
Batch 64/64 loss: 0.07711011171340942
Epoch 153  Train loss: 0.07796859063354193  Val loss: 0.10673317446331798
Epoch 154
-------------------------------
Batch 1/64 loss: 0.08817946910858154
Batch 2/64 loss: 0.08594280481338501
Batch 3/64 loss: 0.09547829627990723
Batch 4/64 loss: 0.06760728359222412
Batch 5/64 loss: 0.07383424043655396
Batch 6/64 loss: 0.0739738941192627
Batch 7/64 loss: 0.07849162817001343
Batch 8/64 loss: 0.06489956378936768
Batch 9/64 loss: 0.08174598217010498
Batch 10/64 loss: 0.07205229997634888
Batch 11/64 loss: 0.0827593207359314
Batch 12/64 loss: 0.07592684030532837
Batch 13/64 loss: 0.0728769302368164
Batch 14/64 loss: 0.0800027847290039
Batch 15/64 loss: 0.08028817176818848
Batch 16/64 loss: 0.07212036848068237
Batch 17/64 loss: 0.07758182287216187
Batch 18/64 loss: 0.07228153944015503
Batch 19/64 loss: 0.08031797409057617
Batch 20/64 loss: 0.08353638648986816
Batch 21/64 loss: 0.0848921537399292
Batch 22/64 loss: 0.07437717914581299
Batch 23/64 loss: 0.07721555233001709
Batch 24/64 loss: 0.08202093839645386
Batch 25/64 loss: 0.09463638067245483
Batch 26/64 loss: 0.08985131978988647
Batch 27/64 loss: 0.0729522705078125
Batch 28/64 loss: 0.07522088289260864
Batch 29/64 loss: 0.0674942135810852
Batch 30/64 loss: 0.07290053367614746
Batch 31/64 loss: 0.07391297817230225
Batch 32/64 loss: 0.0798453688621521
Batch 33/64 loss: 0.07728338241577148
Batch 34/64 loss: 0.07927018404006958
Batch 35/64 loss: 0.09154951572418213
Batch 36/64 loss: 0.084877610206604
Batch 37/64 loss: 0.07497763633728027
Batch 38/64 loss: 0.09587609767913818
Batch 39/64 loss: 0.07224172353744507
Batch 40/64 loss: 0.08803170919418335
Batch 41/64 loss: 0.07786601781845093
Batch 42/64 loss: 0.08092045783996582
Batch 43/64 loss: 0.06866222620010376
Batch 44/64 loss: 0.06906944513320923
Batch 45/64 loss: 0.06999772787094116
Batch 46/64 loss: 0.07724106311798096
Batch 47/64 loss: 0.08305317163467407
Batch 48/64 loss: 0.07546103000640869
Batch 49/64 loss: 0.08580672740936279
Batch 50/64 loss: 0.09032011032104492
Batch 51/64 loss: 0.08115208148956299
Batch 52/64 loss: 0.07251507043838501
Batch 53/64 loss: 0.06913924217224121
Batch 54/64 loss: 0.08087402582168579
Batch 55/64 loss: 0.06749558448791504
Batch 56/64 loss: 0.07147705554962158
Batch 57/64 loss: 0.07588666677474976
Batch 58/64 loss: 0.07503557205200195
Batch 59/64 loss: 0.0809820294380188
Batch 60/64 loss: 0.07876604795455933
Batch 61/64 loss: 0.07937741279602051
Batch 62/64 loss: 0.0722271203994751
Batch 63/64 loss: 0.07876253128051758
Batch 64/64 loss: 0.07329082489013672
Epoch 154  Train loss: 0.07821775324204389  Val loss: 0.09679480578071882
Saving best model, epoch: 154
Epoch 155
-------------------------------
Batch 1/64 loss: 0.07764911651611328
Batch 2/64 loss: 0.07641828060150146
Batch 3/64 loss: 0.08049708604812622
Batch 4/64 loss: 0.07517707347869873
Batch 5/64 loss: 0.07757627964019775
Batch 6/64 loss: 0.08112716674804688
Batch 7/64 loss: 0.08119606971740723
Batch 8/64 loss: 0.07616710662841797
Batch 9/64 loss: 0.08173215389251709
Batch 10/64 loss: 0.08536416292190552
Batch 11/64 loss: 0.08126914501190186
Batch 12/64 loss: 0.07438695430755615
Batch 13/64 loss: 0.08160769939422607
Batch 14/64 loss: 0.07390421628952026
Batch 15/64 loss: 0.08021152019500732
Batch 16/64 loss: 0.07968676090240479
Batch 17/64 loss: 0.07343411445617676
Batch 18/64 loss: 0.07741475105285645
Batch 19/64 loss: 0.07390928268432617
Batch 20/64 loss: 0.06158137321472168
Batch 21/64 loss: 0.09452593326568604
Batch 22/64 loss: 0.07993841171264648
Batch 23/64 loss: 0.07896310091018677
Batch 24/64 loss: 0.08507251739501953
Batch 25/64 loss: 0.07599914073944092
Batch 26/64 loss: 0.08330768346786499
Batch 27/64 loss: 0.0711674690246582
Batch 28/64 loss: 0.07285988330841064
Batch 29/64 loss: 0.08445537090301514
Batch 30/64 loss: 0.08560854196548462
Batch 31/64 loss: 0.08276140689849854
Batch 32/64 loss: 0.08020490407943726
Batch 33/64 loss: 0.08506888151168823
Batch 34/64 loss: 0.07345551252365112
Batch 35/64 loss: 0.09281367063522339
Batch 36/64 loss: 0.0759047269821167
Batch 37/64 loss: 0.072437584400177
Batch 38/64 loss: 0.07813799381256104
Batch 39/64 loss: 0.0890623927116394
Batch 40/64 loss: 0.06800127029418945
Batch 41/64 loss: 0.08280569314956665
Batch 42/64 loss: 0.07565903663635254
Batch 43/64 loss: 0.06904095411300659
Batch 44/64 loss: 0.08607834577560425
Batch 45/64 loss: 0.0776834487915039
Batch 46/64 loss: 0.08338135480880737
Batch 47/64 loss: 0.06997126340866089
Batch 48/64 loss: 0.062052130699157715
Batch 49/64 loss: 0.0705413818359375
Batch 50/64 loss: 0.06479555368423462
Batch 51/64 loss: 0.07482755184173584
Batch 52/64 loss: 0.062250494956970215
Batch 53/64 loss: 0.07527053356170654
Batch 54/64 loss: 0.07699060440063477
Batch 55/64 loss: 0.08030086755752563
Batch 56/64 loss: 0.0889747142791748
Batch 57/64 loss: 0.06834715604782104
Batch 58/64 loss: 0.06094151735305786
Batch 59/64 loss: 0.08374148607254028
Batch 60/64 loss: 0.08310359716415405
Batch 61/64 loss: 0.09022557735443115
Batch 62/64 loss: 0.07553893327713013
Batch 63/64 loss: 0.07436811923980713
Batch 64/64 loss: 0.08009880781173706
Epoch 155  Train loss: 0.07775719422920077  Val loss: 0.10399246502578054
Epoch 156
-------------------------------
Batch 1/64 loss: 0.06603312492370605
Batch 2/64 loss: 0.07736039161682129
Batch 3/64 loss: 0.07802802324295044
Batch 4/64 loss: 0.08607625961303711
Batch 5/64 loss: 0.08988934755325317
Batch 6/64 loss: 0.07586264610290527
Batch 7/64 loss: 0.06866210699081421
Batch 8/64 loss: 0.09186375141143799
Batch 9/64 loss: 0.08726847171783447
Batch 10/64 loss: 0.06722384691238403
Batch 11/64 loss: 0.07078635692596436
Batch 12/64 loss: 0.08573770523071289
Batch 13/64 loss: 0.07485014200210571
Batch 14/64 loss: 0.07000404596328735
Batch 15/64 loss: 0.07870012521743774
Batch 16/64 loss: 0.06349742412567139
Batch 17/64 loss: 0.08027738332748413
Batch 18/64 loss: 0.08633410930633545
Batch 19/64 loss: 0.08045512437820435
Batch 20/64 loss: 0.07357174158096313
Batch 21/64 loss: 0.0888933539390564
Batch 22/64 loss: 0.07705426216125488
Batch 23/64 loss: 0.0767812728881836
Batch 24/64 loss: 0.06800127029418945
Batch 25/64 loss: 0.07978594303131104
Batch 26/64 loss: 0.0856170654296875
Batch 27/64 loss: 0.07615476846694946
Batch 28/64 loss: 0.06965464353561401
Batch 29/64 loss: 0.06964129209518433
Batch 30/64 loss: 0.07172125577926636
Batch 31/64 loss: 0.08047497272491455
Batch 32/64 loss: 0.06538403034210205
Batch 33/64 loss: 0.061175405979156494
Batch 34/64 loss: 0.0742110013961792
Batch 35/64 loss: 0.07383346557617188
Batch 36/64 loss: 0.06372159719467163
Batch 37/64 loss: 0.06589627265930176
Batch 38/64 loss: 0.06973487138748169
Batch 39/64 loss: 0.07116049528121948
Batch 40/64 loss: 0.0684579610824585
Batch 41/64 loss: 0.07679873704910278
Batch 42/64 loss: 0.08788955211639404
Batch 43/64 loss: 0.08532208204269409
Batch 44/64 loss: 0.08466088771820068
Batch 45/64 loss: 0.07950228452682495
Batch 46/64 loss: 0.07478982210159302
Batch 47/64 loss: 0.07541924715042114
Batch 48/64 loss: 0.07168722152709961
Batch 49/64 loss: 0.06990480422973633
Batch 50/64 loss: 0.08730030059814453
Batch 51/64 loss: 0.06650739908218384
Batch 52/64 loss: 0.08020025491714478
Batch 53/64 loss: 0.07558661699295044
Batch 54/64 loss: 0.07261538505554199
Batch 55/64 loss: 0.08147644996643066
Batch 56/64 loss: 0.08045428991317749
Batch 57/64 loss: 0.07671284675598145
Batch 58/64 loss: 0.09670865535736084
Batch 59/64 loss: 0.08942794799804688
Batch 60/64 loss: 0.07491618394851685
Batch 61/64 loss: 0.0730363130569458
Batch 62/64 loss: 0.072623610496521
Batch 63/64 loss: 0.09360969066619873
Batch 64/64 loss: 0.06599146127700806
Epoch 156  Train loss: 0.07665069033117855  Val loss: 0.10086285351068293
Epoch 157
-------------------------------
Batch 1/64 loss: 0.07937192916870117
Batch 2/64 loss: 0.07788324356079102
Batch 3/64 loss: 0.06598371267318726
Batch 4/64 loss: 0.069194495677948
Batch 5/64 loss: 0.07124096155166626
Batch 6/64 loss: 0.06923067569732666
Batch 7/64 loss: 0.07556533813476562
Batch 8/64 loss: 0.09326469898223877
Batch 9/64 loss: 0.06734931468963623
Batch 10/64 loss: 0.07325208187103271
Batch 11/64 loss: 0.07689756155014038
Batch 12/64 loss: 0.07860159873962402
Batch 13/64 loss: 0.06913512945175171
Batch 14/64 loss: 0.08132809400558472
Batch 15/64 loss: 0.07246816158294678
Batch 16/64 loss: 0.08405691385269165
Batch 17/64 loss: 0.07498031854629517
Batch 18/64 loss: 0.06760847568511963
Batch 19/64 loss: 0.08962804079055786
Batch 20/64 loss: 0.09474647045135498
Batch 21/64 loss: 0.07438355684280396
Batch 22/64 loss: 0.07303637266159058
Batch 23/64 loss: 0.07068157196044922
Batch 24/64 loss: 0.06891292333602905
Batch 25/64 loss: 0.0782504677772522
Batch 26/64 loss: 0.07688593864440918
Batch 27/64 loss: 0.0661698579788208
Batch 28/64 loss: 0.07585042715072632
Batch 29/64 loss: 0.07095766067504883
Batch 30/64 loss: 0.07191312313079834
Batch 31/64 loss: 0.06863409280776978
Batch 32/64 loss: 0.06547212600708008
Batch 33/64 loss: 0.0908089280128479
Batch 34/64 loss: 0.07588374614715576
Batch 35/64 loss: 0.0769268274307251
Batch 36/64 loss: 0.0840374231338501
Batch 37/64 loss: 0.07730698585510254
Batch 38/64 loss: 0.06201118230819702
Batch 39/64 loss: 0.0688866376876831
Batch 40/64 loss: 0.06925761699676514
Batch 41/64 loss: 0.07805860042572021
Batch 42/64 loss: 0.07187849283218384
Batch 43/64 loss: 0.07120674848556519
Batch 44/64 loss: 0.0856093168258667
Batch 45/64 loss: 0.06919288635253906
Batch 46/64 loss: 0.06857627630233765
Batch 47/64 loss: 0.07346874475479126
Batch 48/64 loss: 0.06935340166091919
Batch 49/64 loss: 0.09414732456207275
Batch 50/64 loss: 0.0640031099319458
Batch 51/64 loss: 0.08181673288345337
Batch 52/64 loss: 0.06822216510772705
Batch 53/64 loss: 0.0737336277961731
Batch 54/64 loss: 0.07727766036987305
Batch 55/64 loss: 0.09101539850234985
Batch 56/64 loss: 0.07863140106201172
Batch 57/64 loss: 0.06950336694717407
Batch 58/64 loss: 0.08320152759552002
Batch 59/64 loss: 0.08629411458969116
Batch 60/64 loss: 0.07222509384155273
Batch 61/64 loss: 0.0623401403427124
Batch 62/64 loss: 0.07787150144577026
Batch 63/64 loss: 0.0920790433883667
Batch 64/64 loss: 0.06725883483886719
Epoch 157  Train loss: 0.07542283114264993  Val loss: 0.1002227242050302
Epoch 158
-------------------------------
Batch 1/64 loss: 0.0689091682434082
Batch 2/64 loss: 0.07618343830108643
Batch 3/64 loss: 0.07593494653701782
Batch 4/64 loss: 0.0814850926399231
Batch 5/64 loss: 0.08368653059005737
Batch 6/64 loss: 0.07483184337615967
Batch 7/64 loss: 0.07374602556228638
Batch 8/64 loss: 0.07970958948135376
Batch 9/64 loss: 0.07741200923919678
Batch 10/64 loss: 0.0757519006729126
Batch 11/64 loss: 0.0870969295501709
Batch 12/64 loss: 0.06788718700408936
Batch 13/64 loss: 0.09318852424621582
Batch 14/64 loss: 0.07183647155761719
Batch 15/64 loss: 0.06859052181243896
Batch 16/64 loss: 0.06921035051345825
Batch 17/64 loss: 0.06826251745223999
Batch 18/64 loss: 0.07081669569015503
Batch 19/64 loss: 0.06569832563400269
Batch 20/64 loss: 0.06113159656524658
Batch 21/64 loss: 0.06714189052581787
Batch 22/64 loss: 0.0671730637550354
Batch 23/64 loss: 0.0654991865158081
Batch 24/64 loss: 0.06847560405731201
Batch 25/64 loss: 0.06802773475646973
Batch 26/64 loss: 0.07829856872558594
Batch 27/64 loss: 0.07096344232559204
Batch 28/64 loss: 0.07883554697036743
Batch 29/64 loss: 0.05338644981384277
Batch 30/64 loss: 0.07455664873123169
Batch 31/64 loss: 0.07098674774169922
Batch 32/64 loss: 0.0759773850440979
Batch 33/64 loss: 0.08142238855361938
Batch 34/64 loss: 0.08581072092056274
Batch 35/64 loss: 0.07077336311340332
Batch 36/64 loss: 0.07643240690231323
Batch 37/64 loss: 0.06793594360351562
Batch 38/64 loss: 0.08109152317047119
Batch 39/64 loss: 0.06633216142654419
Batch 40/64 loss: 0.07645273208618164
Batch 41/64 loss: 0.06975746154785156
Batch 42/64 loss: 0.0821942687034607
Batch 43/64 loss: 0.09614181518554688
Batch 44/64 loss: 0.08859455585479736
Batch 45/64 loss: 0.08174479007720947
Batch 46/64 loss: 0.08831125497817993
Batch 47/64 loss: 0.08516675233840942
Batch 48/64 loss: 0.07216668128967285
Batch 49/64 loss: 0.0805935263633728
Batch 50/64 loss: 0.09968376159667969
Batch 51/64 loss: 0.08951902389526367
Batch 52/64 loss: 0.0925145149230957
Batch 53/64 loss: 0.07828587293624878
Batch 54/64 loss: 0.07849055528640747
Batch 55/64 loss: 0.07563155889511108
Batch 56/64 loss: 0.07516288757324219
Batch 57/64 loss: 0.08358407020568848
Batch 58/64 loss: 0.08134406805038452
Batch 59/64 loss: 0.07036322355270386
Batch 60/64 loss: 0.07322418689727783
Batch 61/64 loss: 0.08027362823486328
Batch 62/64 loss: 0.08159589767456055
Batch 63/64 loss: 0.08624690771102905
Batch 64/64 loss: 0.06987076997756958
Epoch 158  Train loss: 0.07654803944569008  Val loss: 0.1010696496750481
Epoch 159
-------------------------------
Batch 1/64 loss: 0.06979227066040039
Batch 2/64 loss: 0.08325767517089844
Batch 3/64 loss: 0.07995223999023438
Batch 4/64 loss: 0.07843911647796631
Batch 5/64 loss: 0.06079661846160889
Batch 6/64 loss: 0.06388640403747559
Batch 7/64 loss: 0.062414705753326416
Batch 8/64 loss: 0.08153021335601807
Batch 9/64 loss: 0.09560346603393555
Batch 10/64 loss: 0.09108191728591919
Batch 11/64 loss: 0.0821717381477356
Batch 12/64 loss: 0.07294261455535889
Batch 13/64 loss: 0.08271461725234985
Batch 14/64 loss: 0.07031774520874023
Batch 15/64 loss: 0.09054046869277954
Batch 16/64 loss: 0.09492725133895874
Batch 17/64 loss: 0.08567744493484497
Batch 18/64 loss: 0.07045549154281616
Batch 19/64 loss: 0.07475107908248901
Batch 20/64 loss: 0.08379131555557251
Batch 21/64 loss: 0.07891982793807983
Batch 22/64 loss: 0.08164292573928833
Batch 23/64 loss: 0.07868504524230957
Batch 24/64 loss: 0.09617120027542114
Batch 25/64 loss: 0.08165407180786133
Batch 26/64 loss: 0.062233686447143555
Batch 27/64 loss: 0.08949804306030273
Batch 28/64 loss: 0.06728506088256836
Batch 29/64 loss: 0.07210361957550049
Batch 30/64 loss: 0.06984472274780273
Batch 31/64 loss: 0.07610547542572021
Batch 32/64 loss: 0.0855669379234314
Batch 33/64 loss: 0.075442373752594
Batch 34/64 loss: 0.0853455662727356
Batch 35/64 loss: 0.08093786239624023
Batch 36/64 loss: 0.08991396427154541
Batch 37/64 loss: 0.08305937051773071
Batch 38/64 loss: 0.07600754499435425
Batch 39/64 loss: 0.06998777389526367
Batch 40/64 loss: 0.07077538967132568
Batch 41/64 loss: 0.0779719352722168
Batch 42/64 loss: 0.06677758693695068
Batch 43/64 loss: 0.07003110647201538
Batch 44/64 loss: 0.07219386100769043
Batch 45/64 loss: 0.07134139537811279
Batch 46/64 loss: 0.08339416980743408
Batch 47/64 loss: 0.06701374053955078
Batch 48/64 loss: 0.07805776596069336
Batch 49/64 loss: 0.09575378894805908
Batch 50/64 loss: 0.07564246654510498
Batch 51/64 loss: 0.062483012676239014
Batch 52/64 loss: 0.0759781002998352
Batch 53/64 loss: 0.07918953895568848
Batch 54/64 loss: 0.08276671171188354
Batch 55/64 loss: 0.06896984577178955
Batch 56/64 loss: 0.07743918895721436
Batch 57/64 loss: 0.06519711017608643
Batch 58/64 loss: 0.08157074451446533
Batch 59/64 loss: 0.06765574216842651
Batch 60/64 loss: 0.06739532947540283
Batch 61/64 loss: 0.06660318374633789
Batch 62/64 loss: 0.0663144588470459
Batch 63/64 loss: 0.07541108131408691
Batch 64/64 loss: 0.0979192852973938
Epoch 159  Train loss: 0.0770951558561886  Val loss: 0.0997850649135629
Epoch 160
-------------------------------
Batch 1/64 loss: 0.07898867130279541
Batch 2/64 loss: 0.07293856143951416
Batch 3/64 loss: 0.06755948066711426
Batch 4/64 loss: 0.07205629348754883
Batch 5/64 loss: 0.07922416925430298
Batch 6/64 loss: 0.0831727385520935
Batch 7/64 loss: 0.08539235591888428
Batch 8/64 loss: 0.08459174633026123
Batch 9/64 loss: 0.07700377702713013
Batch 10/64 loss: 0.07226181030273438
Batch 11/64 loss: 0.0780298113822937
Batch 12/64 loss: 0.06464236974716187
Batch 13/64 loss: 0.07449334859848022
Batch 14/64 loss: 0.06405782699584961
Batch 15/64 loss: 0.0656440258026123
Batch 16/64 loss: 0.08737415075302124
Batch 17/64 loss: 0.06029230356216431
Batch 18/64 loss: 0.0792776346206665
Batch 19/64 loss: 0.06491351127624512
Batch 20/64 loss: 0.08262902498245239
Batch 21/64 loss: 0.05950915813446045
Batch 22/64 loss: 0.07716310024261475
Batch 23/64 loss: 0.06628221273422241
Batch 24/64 loss: 0.0752263069152832
Batch 25/64 loss: 0.06862980127334595
Batch 26/64 loss: 0.07151705026626587
Batch 27/64 loss: 0.0763314962387085
Batch 28/64 loss: 0.07151079177856445
Batch 29/64 loss: 0.06464064121246338
Batch 30/64 loss: 0.06811320781707764
Batch 31/64 loss: 0.07139647006988525
Batch 32/64 loss: 0.06816917657852173
Batch 33/64 loss: 0.07534027099609375
Batch 34/64 loss: 0.0723084807395935
Batch 35/64 loss: 0.0745401382446289
Batch 36/64 loss: 0.08500897884368896
Batch 37/64 loss: 0.07012051343917847
Batch 38/64 loss: 0.07319110631942749
Batch 39/64 loss: 0.07588237524032593
Batch 40/64 loss: 0.08011925220489502
Batch 41/64 loss: 0.0665208101272583
Batch 42/64 loss: 0.07752805948257446
Batch 43/64 loss: 0.07436186075210571
Batch 44/64 loss: 0.08255457878112793
Batch 45/64 loss: 0.07183289527893066
Batch 46/64 loss: 0.07712757587432861
Batch 47/64 loss: 0.06984776258468628
Batch 48/64 loss: 0.07686787843704224
Batch 49/64 loss: 0.07616245746612549
Batch 50/64 loss: 0.0628923773765564
Batch 51/64 loss: 0.06849902868270874
Batch 52/64 loss: 0.08705657720565796
Batch 53/64 loss: 0.07325798273086548
Batch 54/64 loss: 0.07898324728012085
Batch 55/64 loss: 0.06616687774658203
Batch 56/64 loss: 0.07139068841934204
Batch 57/64 loss: 0.06424945592880249
Batch 58/64 loss: 0.0719289779663086
Batch 59/64 loss: 0.06471139192581177
Batch 60/64 loss: 0.0949169397354126
Batch 61/64 loss: 0.08246642351150513
Batch 62/64 loss: 0.067402184009552
Batch 63/64 loss: 0.08579516410827637
Batch 64/64 loss: 0.08403158187866211
Epoch 160  Train loss: 0.07396218075471765  Val loss: 0.10174660174707367
Epoch 161
-------------------------------
Batch 1/64 loss: 0.0685989260673523
Batch 2/64 loss: 0.07372128963470459
Batch 3/64 loss: 0.0865594744682312
Batch 4/64 loss: 0.07057106494903564
Batch 5/64 loss: 0.06539803743362427
Batch 6/64 loss: 0.07447940111160278
Batch 7/64 loss: 0.07794952392578125
Batch 8/64 loss: 0.06577420234680176
Batch 9/64 loss: 0.06901520490646362
Batch 10/64 loss: 0.07361626625061035
Batch 11/64 loss: 0.06814718246459961
Batch 12/64 loss: 0.1032518744468689
Batch 13/64 loss: 0.08581256866455078
Batch 14/64 loss: 0.06778669357299805
Batch 15/64 loss: 0.08046543598175049
Batch 16/64 loss: 0.08324819803237915
Batch 17/64 loss: 0.07798725366592407
Batch 18/64 loss: 0.0682188868522644
Batch 19/64 loss: 0.07744646072387695
Batch 20/64 loss: 0.07816958427429199
Batch 21/64 loss: 0.09844112396240234
Batch 22/64 loss: 0.07079237699508667
Batch 23/64 loss: 0.0857664942741394
Batch 24/64 loss: 0.06985557079315186
Batch 25/64 loss: 0.06740796566009521
Batch 26/64 loss: 0.06963884830474854
Batch 27/64 loss: 0.07352256774902344
Batch 28/64 loss: 0.08831721544265747
Batch 29/64 loss: 0.07424181699752808
Batch 30/64 loss: 0.06626039743423462
Batch 31/64 loss: 0.07738804817199707
Batch 32/64 loss: 0.07593125104904175
Batch 33/64 loss: 0.06127583980560303
Batch 34/64 loss: 0.07393962144851685
Batch 35/64 loss: 0.08577120304107666
Batch 36/64 loss: 0.06648898124694824
Batch 37/64 loss: 0.06646192073822021
Batch 38/64 loss: 0.07835662364959717
Batch 39/64 loss: 0.08916127681732178
Batch 40/64 loss: 0.06734174489974976
Batch 41/64 loss: 0.09151250123977661
Batch 42/64 loss: 0.06296682357788086
Batch 43/64 loss: 0.07045763731002808
Batch 44/64 loss: 0.07120811939239502
Batch 45/64 loss: 0.07971400022506714
Batch 46/64 loss: 0.06347191333770752
Batch 47/64 loss: 0.06886333227157593
Batch 48/64 loss: 0.06141245365142822
Batch 49/64 loss: 0.06923139095306396
Batch 50/64 loss: 0.06166476011276245
Batch 51/64 loss: 0.07681834697723389
Batch 52/64 loss: 0.07654672861099243
Batch 53/64 loss: 0.06242161989212036
Batch 54/64 loss: 0.09580355882644653
Batch 55/64 loss: 0.07372444868087769
Batch 56/64 loss: 0.06945264339447021
Batch 57/64 loss: 0.06954050064086914
Batch 58/64 loss: 0.07163441181182861
Batch 59/64 loss: 0.06946098804473877
Batch 60/64 loss: 0.07639020681381226
Batch 61/64 loss: 0.07069987058639526
Batch 62/64 loss: 0.0844581127166748
Batch 63/64 loss: 0.06907951831817627
Batch 64/64 loss: 0.0823289155960083
Epoch 161  Train loss: 0.07452327831118714  Val loss: 0.0998058474760285
Epoch 162
-------------------------------
Batch 1/64 loss: 0.06438136100769043
Batch 2/64 loss: 0.056108713150024414
Batch 3/64 loss: 0.06711351871490479
Batch 4/64 loss: 0.06711363792419434
Batch 5/64 loss: 0.07456314563751221
Batch 6/64 loss: 0.05682104825973511
Batch 7/64 loss: 0.06335699558258057
Batch 8/64 loss: 0.0658147931098938
Batch 9/64 loss: 0.08520197868347168
Batch 10/64 loss: 0.07234323024749756
Batch 11/64 loss: 0.0781402587890625
Batch 12/64 loss: 0.07043272256851196
Batch 13/64 loss: 0.07460302114486694
Batch 14/64 loss: 0.07794225215911865
Batch 15/64 loss: 0.0828777551651001
Batch 16/64 loss: 0.07593631744384766
Batch 17/64 loss: 0.07635515928268433
Batch 18/64 loss: 0.07463723421096802
Batch 19/64 loss: 0.057732462882995605
Batch 20/64 loss: 0.07254135608673096
Batch 21/64 loss: 0.07558244466781616
Batch 22/64 loss: 0.07839596271514893
Batch 23/64 loss: 0.06521928310394287
Batch 24/64 loss: 0.07832276821136475
Batch 25/64 loss: 0.08055460453033447
Batch 26/64 loss: 0.06499803066253662
Batch 27/64 loss: 0.06735885143280029
Batch 28/64 loss: 0.08041709661483765
Batch 29/64 loss: 0.10397195816040039
Batch 30/64 loss: 0.07294577360153198
Batch 31/64 loss: 0.0717354416847229
Batch 32/64 loss: 0.07407784461975098
Batch 33/64 loss: 0.0773119330406189
Batch 34/64 loss: 0.07442688941955566
Batch 35/64 loss: 0.0822870135307312
Batch 36/64 loss: 0.08732593059539795
Batch 37/64 loss: 0.07873129844665527
Batch 38/64 loss: 0.07628142833709717
Batch 39/64 loss: 0.0880005955696106
Batch 40/64 loss: 0.05774778127670288
Batch 41/64 loss: 0.060872554779052734
Batch 42/64 loss: 0.06793475151062012
Batch 43/64 loss: 0.07998377084732056
Batch 44/64 loss: 0.062061607837677
Batch 45/64 loss: 0.08691930770874023
Batch 46/64 loss: 0.07956558465957642
Batch 47/64 loss: 0.07307624816894531
Batch 48/64 loss: 0.0680803656578064
Batch 49/64 loss: 0.06931179761886597
Batch 50/64 loss: 0.06357306241989136
Batch 51/64 loss: 0.06894063949584961
Batch 52/64 loss: 0.06071573495864868
Batch 53/64 loss: 0.07407152652740479
Batch 54/64 loss: 0.07566326856613159
Batch 55/64 loss: 0.07296562194824219
Batch 56/64 loss: 0.07915574312210083
Batch 57/64 loss: 0.05988353490829468
Batch 58/64 loss: 0.06165111064910889
Batch 59/64 loss: 0.07541954517364502
Batch 60/64 loss: 0.0738726258277893
Batch 61/64 loss: 0.07227873802185059
Batch 62/64 loss: 0.07440394163131714
Batch 63/64 loss: 0.06807935237884521
Batch 64/64 loss: 0.0655943751335144
Epoch 162  Train loss: 0.07258644875358133  Val loss: 0.09829386812714777
Epoch 163
-------------------------------
Batch 1/64 loss: 0.0675816535949707
Batch 2/64 loss: 0.06072723865509033
Batch 3/64 loss: 0.07787644863128662
Batch 4/64 loss: 0.06119561195373535
Batch 5/64 loss: 0.09152966737747192
Batch 6/64 loss: 0.0713958740234375
Batch 7/64 loss: 0.06739163398742676
Batch 8/64 loss: 0.06285202503204346
Batch 9/64 loss: 0.07142454385757446
Batch 10/64 loss: 0.06459707021713257
Batch 11/64 loss: 0.07561361789703369
Batch 12/64 loss: 0.07039719820022583
Batch 13/64 loss: 0.06951147317886353
Batch 14/64 loss: 0.07310140132904053
Batch 15/64 loss: 0.06398546695709229
Batch 16/64 loss: 0.07499051094055176
Batch 17/64 loss: 0.06138366460800171
Batch 18/64 loss: 0.08080703020095825
Batch 19/64 loss: 0.07730996608734131
Batch 20/64 loss: 0.09201812744140625
Batch 21/64 loss: 0.06964480876922607
Batch 22/64 loss: 0.06723147630691528
Batch 23/64 loss: 0.0640861988067627
Batch 24/64 loss: 0.07951927185058594
Batch 25/64 loss: 0.0735885500907898
Batch 26/64 loss: 0.0779578685760498
Batch 27/64 loss: 0.07009685039520264
Batch 28/64 loss: 0.07405072450637817
Batch 29/64 loss: 0.0776817798614502
Batch 30/64 loss: 0.07522141933441162
Batch 31/64 loss: 0.07040244340896606
Batch 32/64 loss: 0.07812482118606567
Batch 33/64 loss: 0.08033525943756104
Batch 34/64 loss: 0.07562685012817383
Batch 35/64 loss: 0.06008249521255493
Batch 36/64 loss: 0.06715601682662964
Batch 37/64 loss: 0.06686240434646606
Batch 38/64 loss: 0.06981420516967773
Batch 39/64 loss: 0.0658639669418335
Batch 40/64 loss: 0.07912397384643555
Batch 41/64 loss: 0.0771748423576355
Batch 42/64 loss: 0.07757973670959473
Batch 43/64 loss: 0.07851463556289673
Batch 44/64 loss: 0.07463198900222778
Batch 45/64 loss: 0.06970977783203125
Batch 46/64 loss: 0.08384037017822266
Batch 47/64 loss: 0.07818758487701416
Batch 48/64 loss: 0.09099090099334717
Batch 49/64 loss: 0.08438438177108765
Batch 50/64 loss: 0.08089011907577515
Batch 51/64 loss: 0.06584227085113525
Batch 52/64 loss: 0.07761013507843018
Batch 53/64 loss: 0.07098615169525146
Batch 54/64 loss: 0.08626389503479004
Batch 55/64 loss: 0.06480348110198975
Batch 56/64 loss: 0.08286207914352417
Batch 57/64 loss: 0.07965302467346191
Batch 58/64 loss: 0.07153433561325073
Batch 59/64 loss: 0.08675849437713623
Batch 60/64 loss: 0.06264972686767578
Batch 61/64 loss: 0.0740671157836914
Batch 62/64 loss: 0.08116793632507324
Batch 63/64 loss: 0.07355272769927979
Batch 64/64 loss: 0.05767601728439331
Epoch 163  Train loss: 0.07364822485867668  Val loss: 0.09748361856257383
Epoch 164
-------------------------------
Batch 1/64 loss: 0.06650805473327637
Batch 2/64 loss: 0.08177679777145386
Batch 3/64 loss: 0.06765788793563843
Batch 4/64 loss: 0.07670998573303223
Batch 5/64 loss: 0.06698143482208252
Batch 6/64 loss: 0.07376337051391602
Batch 7/64 loss: 0.08224648237228394
Batch 8/64 loss: 0.07333195209503174
Batch 9/64 loss: 0.0655636191368103
Batch 10/64 loss: 0.07137137651443481
Batch 11/64 loss: 0.07933437824249268
Batch 12/64 loss: 0.06047242879867554
Batch 13/64 loss: 0.05615115165710449
Batch 14/64 loss: 0.07457661628723145
Batch 15/64 loss: 0.06099885702133179
Batch 16/64 loss: 0.058409929275512695
Batch 17/64 loss: 0.05799448490142822
Batch 18/64 loss: 0.06944185495376587
Batch 19/64 loss: 0.07131469249725342
Batch 20/64 loss: 0.07146680355072021
Batch 21/64 loss: 0.06251287460327148
Batch 22/64 loss: 0.0642862319946289
Batch 23/64 loss: 0.05888146162033081
Batch 24/64 loss: 0.07721441984176636
Batch 25/64 loss: 0.06082046031951904
Batch 26/64 loss: 0.065601646900177
Batch 27/64 loss: 0.08232343196868896
Batch 28/64 loss: 0.06594228744506836
Batch 29/64 loss: 0.07673883438110352
Batch 30/64 loss: 0.08810651302337646
Batch 31/64 loss: 0.06352448463439941
Batch 32/64 loss: 0.07104754447937012
Batch 33/64 loss: 0.07456505298614502
Batch 34/64 loss: 0.0667804479598999
Batch 35/64 loss: 0.080802321434021
Batch 36/64 loss: 0.06665325164794922
Batch 37/64 loss: 0.06386333703994751
Batch 38/64 loss: 0.07529675960540771
Batch 39/64 loss: 0.07758057117462158
Batch 40/64 loss: 0.07384943962097168
Batch 41/64 loss: 0.06453979015350342
Batch 42/64 loss: 0.0796588659286499
Batch 43/64 loss: 0.09445148706436157
Batch 44/64 loss: 0.07990342378616333
Batch 45/64 loss: 0.06396263837814331
Batch 46/64 loss: 0.08918106555938721
Batch 47/64 loss: 0.06921011209487915
Batch 48/64 loss: 0.06433767080307007
Batch 49/64 loss: 0.07850426435470581
Batch 50/64 loss: 0.06809401512145996
Batch 51/64 loss: 0.07423126697540283
Batch 52/64 loss: 0.09577101469039917
Batch 53/64 loss: 0.06491893529891968
Batch 54/64 loss: 0.06816208362579346
Batch 55/64 loss: 0.08327800035476685
Batch 56/64 loss: 0.06484609842300415
Batch 57/64 loss: 0.07830691337585449
Batch 58/64 loss: 0.08629268407821655
Batch 59/64 loss: 0.061629533767700195
Batch 60/64 loss: 0.07827609777450562
Batch 61/64 loss: 0.061409950256347656
Batch 62/64 loss: 0.07935953140258789
Batch 63/64 loss: 0.06826364994049072
Batch 64/64 loss: 0.08541405200958252
Epoch 164  Train loss: 0.07189197119544534  Val loss: 0.09830205055446559
Epoch 165
-------------------------------
Batch 1/64 loss: 0.08578526973724365
Batch 2/64 loss: 0.07688730955123901
Batch 3/64 loss: 0.07595348358154297
Batch 4/64 loss: 0.060955822467803955
Batch 5/64 loss: 0.07958459854125977
Batch 6/64 loss: 0.09617525339126587
Batch 7/64 loss: 0.06462603807449341
Batch 8/64 loss: 0.06602752208709717
Batch 9/64 loss: 0.0760188102722168
Batch 10/64 loss: 0.0712507963180542
Batch 11/64 loss: 0.06110215187072754
Batch 12/64 loss: 0.0580446720123291
Batch 13/64 loss: 0.06204044818878174
Batch 14/64 loss: 0.06379431486129761
Batch 15/64 loss: 0.07761859893798828
Batch 16/64 loss: 0.07743978500366211
Batch 17/64 loss: 0.08042943477630615
Batch 18/64 loss: 0.07941794395446777
Batch 19/64 loss: 0.06818562746047974
Batch 20/64 loss: 0.06041455268859863
Batch 21/64 loss: 0.07154160737991333
Batch 22/64 loss: 0.08198279142379761
Batch 23/64 loss: 0.07965213060379028
Batch 24/64 loss: 0.06580603122711182
Batch 25/64 loss: 0.0675574541091919
Batch 26/64 loss: 0.052836060523986816
Batch 27/64 loss: 0.061037659645080566
Batch 28/64 loss: 0.06913447380065918
Batch 29/64 loss: 0.06799149513244629
Batch 30/64 loss: 0.06783819198608398
Batch 31/64 loss: 0.06507623195648193
Batch 32/64 loss: 0.0650414228439331
Batch 33/64 loss: 0.06865108013153076
Batch 34/64 loss: 0.06771039962768555
Batch 35/64 loss: 0.06409740447998047
Batch 36/64 loss: 0.06965130567550659
Batch 37/64 loss: 0.059326171875
Batch 38/64 loss: 0.07186436653137207
Batch 39/64 loss: 0.07968854904174805
Batch 40/64 loss: 0.06921517848968506
Batch 41/64 loss: 0.06871223449707031
Batch 42/64 loss: 0.0769643783569336
Batch 43/64 loss: 0.06570106744766235
Batch 44/64 loss: 0.07071352005004883
Batch 45/64 loss: 0.07001382112503052
Batch 46/64 loss: 0.06789571046829224
Batch 47/64 loss: 0.0716177225112915
Batch 48/64 loss: 0.07726889848709106
Batch 49/64 loss: 0.06371992826461792
Batch 50/64 loss: 0.0640331506729126
Batch 51/64 loss: 0.06937849521636963
Batch 52/64 loss: 0.07612454891204834
Batch 53/64 loss: 0.07539969682693481
Batch 54/64 loss: 0.0796317458152771
Batch 55/64 loss: 0.06722873449325562
Batch 56/64 loss: 0.06444251537322998
Batch 57/64 loss: 0.06855523586273193
Batch 58/64 loss: 0.0617251992225647
Batch 59/64 loss: 0.07580024003982544
Batch 60/64 loss: 0.06506860256195068
Batch 61/64 loss: 0.07393276691436768
Batch 62/64 loss: 0.06606298685073853
Batch 63/64 loss: 0.08409911394119263
Batch 64/64 loss: 0.06552392244338989
Epoch 165  Train loss: 0.07028526581969916  Val loss: 0.09983275047282583
Epoch 166
-------------------------------
Batch 1/64 loss: 0.0721350908279419
Batch 2/64 loss: 0.07718425989151001
Batch 3/64 loss: 0.05340796709060669
Batch 4/64 loss: 0.07011604309082031
Batch 5/64 loss: 0.06995207071304321
Batch 6/64 loss: 0.07995772361755371
Batch 7/64 loss: 0.0692063570022583
Batch 8/64 loss: 0.06590020656585693
Batch 9/64 loss: 0.07846587896347046
Batch 10/64 loss: 0.06259113550186157
Batch 11/64 loss: 0.0730212926864624
Batch 12/64 loss: 0.05721616744995117
Batch 13/64 loss: 0.06725764274597168
Batch 14/64 loss: 0.06314665079116821
Batch 15/64 loss: 0.06162136793136597
Batch 16/64 loss: 0.05915343761444092
Batch 17/64 loss: 0.0701446533203125
Batch 18/64 loss: 0.08056753873825073
Batch 19/64 loss: 0.0741073489189148
Batch 20/64 loss: 0.085837721824646
Batch 21/64 loss: 0.06943017244338989
Batch 22/64 loss: 0.06869244575500488
Batch 23/64 loss: 0.07238435745239258
Batch 24/64 loss: 0.0797656774520874
Batch 25/64 loss: 0.0668562650680542
Batch 26/64 loss: 0.07328194379806519
Batch 27/64 loss: 0.07039821147918701
Batch 28/64 loss: 0.08622270822525024
Batch 29/64 loss: 0.0756758451461792
Batch 30/64 loss: 0.06885945796966553
Batch 31/64 loss: 0.0781664252281189
Batch 32/64 loss: 0.0772850513458252
Batch 33/64 loss: 0.09054279327392578
Batch 34/64 loss: 0.0796816349029541
Batch 35/64 loss: 0.07944989204406738
Batch 36/64 loss: 0.08030855655670166
Batch 37/64 loss: 0.08220672607421875
Batch 38/64 loss: 0.07163584232330322
Batch 39/64 loss: 0.0720752477645874
Batch 40/64 loss: 0.07503479719161987
Batch 41/64 loss: 0.08345645666122437
Batch 42/64 loss: 0.08083385229110718
Batch 43/64 loss: 0.06867390871047974
Batch 44/64 loss: 0.07112890481948853
Batch 45/64 loss: 0.06636536121368408
Batch 46/64 loss: 0.06808996200561523
Batch 47/64 loss: 0.08561372756958008
Batch 48/64 loss: 0.07415169477462769
Batch 49/64 loss: 0.08609747886657715
Batch 50/64 loss: 0.07052016258239746
Batch 51/64 loss: 0.07447832822799683
Batch 52/64 loss: 0.0738188624382019
Batch 53/64 loss: 0.08312463760375977
Batch 54/64 loss: 0.0678095817565918
Batch 55/64 loss: 0.06579279899597168
Batch 56/64 loss: 0.06714904308319092
Batch 57/64 loss: 0.06139475107192993
Batch 58/64 loss: 0.08679771423339844
Batch 59/64 loss: 0.08139592409133911
Batch 60/64 loss: 0.08339530229568481
Batch 61/64 loss: 0.07491344213485718
Batch 62/64 loss: 0.05994313955307007
Batch 63/64 loss: 0.07964897155761719
Batch 64/64 loss: 0.08298683166503906
Epoch 166  Train loss: 0.07350241156185375  Val loss: 0.09925454916413297
Epoch 167
-------------------------------
Batch 1/64 loss: 0.06572592258453369
Batch 2/64 loss: 0.06473928689956665
Batch 3/64 loss: 0.07013273239135742
Batch 4/64 loss: 0.06767892837524414
Batch 5/64 loss: 0.07299470901489258
Batch 6/64 loss: 0.07073259353637695
Batch 7/64 loss: 0.07418054342269897
Batch 8/64 loss: 0.06652975082397461
Batch 9/64 loss: 0.0735892653465271
Batch 10/64 loss: 0.06661123037338257
Batch 11/64 loss: 0.06169635057449341
Batch 12/64 loss: 0.05695432424545288
Batch 13/64 loss: 0.07116824388504028
Batch 14/64 loss: 0.05780184268951416
Batch 15/64 loss: 0.08342361450195312
Batch 16/64 loss: 0.07860535383224487
Batch 17/64 loss: 0.06894415616989136
Batch 18/64 loss: 0.07903015613555908
Batch 19/64 loss: 0.08113288879394531
Batch 20/64 loss: 0.059780776500701904
Batch 21/64 loss: 0.07152068614959717
Batch 22/64 loss: 0.06367450952529907
Batch 23/64 loss: 0.07569044828414917
Batch 24/64 loss: 0.07923382520675659
Batch 25/64 loss: 0.06451749801635742
Batch 26/64 loss: 0.06481868028640747
Batch 27/64 loss: 0.08188903331756592
Batch 28/64 loss: 0.0826568603515625
Batch 29/64 loss: 0.06872141361236572
Batch 30/64 loss: 0.0651780366897583
Batch 31/64 loss: 0.07673400640487671
Batch 32/64 loss: 0.0720636248588562
Batch 33/64 loss: 0.06895864009857178
Batch 34/64 loss: 0.06836837530136108
Batch 35/64 loss: 0.061104416847229004
Batch 36/64 loss: 0.058709025382995605
Batch 37/64 loss: 0.07191663980484009
Batch 38/64 loss: 0.07963693141937256
Batch 39/64 loss: 0.06920182704925537
Batch 40/64 loss: 0.07486921548843384
Batch 41/64 loss: 0.07115525007247925
Batch 42/64 loss: 0.08327794075012207
Batch 43/64 loss: 0.08444321155548096
Batch 44/64 loss: 0.07911664247512817
Batch 45/64 loss: 0.06891566514968872
Batch 46/64 loss: 0.06963473558425903
Batch 47/64 loss: 0.0730399489402771
Batch 48/64 loss: 0.06880110502243042
Batch 49/64 loss: 0.06475478410720825
Batch 50/64 loss: 0.07421481609344482
Batch 51/64 loss: 0.0773734450340271
Batch 52/64 loss: 0.07440364360809326
Batch 53/64 loss: 0.06712406873703003
Batch 54/64 loss: 0.08639192581176758
Batch 55/64 loss: 0.07040458917617798
Batch 56/64 loss: 0.07860016822814941
Batch 57/64 loss: 0.07410383224487305
Batch 58/64 loss: 0.06379944086074829
Batch 59/64 loss: 0.06653749942779541
Batch 60/64 loss: 0.08457529544830322
Batch 61/64 loss: 0.08659613132476807
Batch 62/64 loss: 0.07547909021377563
Batch 63/64 loss: 0.08392143249511719
Batch 64/64 loss: 0.06314057111740112
Epoch 167  Train loss: 0.07192057184144562  Val loss: 0.1000303976314584
Epoch 168
-------------------------------
Batch 1/64 loss: 0.06729555130004883
Batch 2/64 loss: 0.07864296436309814
Batch 3/64 loss: 0.06651520729064941
Batch 4/64 loss: 0.06105858087539673
Batch 5/64 loss: 0.07605212926864624
Batch 6/64 loss: 0.07010161876678467
Batch 7/64 loss: 0.07803481817245483
Batch 8/64 loss: 0.05382978916168213
Batch 9/64 loss: 0.061409950256347656
Batch 10/64 loss: 0.06138867139816284
Batch 11/64 loss: 0.06505143642425537
Batch 12/64 loss: 0.06774908304214478
Batch 13/64 loss: 0.05477285385131836
Batch 14/64 loss: 0.07134628295898438
Batch 15/64 loss: 0.06648290157318115
Batch 16/64 loss: 0.08181917667388916
Batch 17/64 loss: 0.05807697772979736
Batch 18/64 loss: 0.06581920385360718
Batch 19/64 loss: 0.052751898765563965
Batch 20/64 loss: 0.06517761945724487
Batch 21/64 loss: 0.05805349349975586
Batch 22/64 loss: 0.06149011850357056
Batch 23/64 loss: 0.06677573919296265
Batch 24/64 loss: 0.05125528573989868
Batch 25/64 loss: 0.06584632396697998
Batch 26/64 loss: 0.07280254364013672
Batch 27/64 loss: 0.06553071737289429
Batch 28/64 loss: 0.06773757934570312
Batch 29/64 loss: 0.08738583326339722
Batch 30/64 loss: 0.06874531507492065
Batch 31/64 loss: 0.06713122129440308
Batch 32/64 loss: 0.06998920440673828
Batch 33/64 loss: 0.05993759632110596
Batch 34/64 loss: 0.06563270092010498
Batch 35/64 loss: 0.07564449310302734
Batch 36/64 loss: 0.07289361953735352
Batch 37/64 loss: 0.07615858316421509
Batch 38/64 loss: 0.06544733047485352
Batch 39/64 loss: 0.07958221435546875
Batch 40/64 loss: 0.06981682777404785
Batch 41/64 loss: 0.07497775554656982
Batch 42/64 loss: 0.06408506631851196
Batch 43/64 loss: 0.0844728946685791
Batch 44/64 loss: 0.061773717403411865
Batch 45/64 loss: 0.0659339427947998
Batch 46/64 loss: 0.07924747467041016
Batch 47/64 loss: 0.0750800371170044
Batch 48/64 loss: 0.08088165521621704
Batch 49/64 loss: 0.06738227605819702
Batch 50/64 loss: 0.07556474208831787
Batch 51/64 loss: 0.06426620483398438
Batch 52/64 loss: 0.08136308193206787
Batch 53/64 loss: 0.06863802671432495
Batch 54/64 loss: 0.07197535037994385
Batch 55/64 loss: 0.07021540403366089
Batch 56/64 loss: 0.07197529077529907
Batch 57/64 loss: 0.07476550340652466
Batch 58/64 loss: 0.0760728120803833
Batch 59/64 loss: 0.07466793060302734
Batch 60/64 loss: 0.07017767429351807
Batch 61/64 loss: 0.07410240173339844
Batch 62/64 loss: 0.0755721926689148
Batch 63/64 loss: 0.07347935438156128
Batch 64/64 loss: 0.06621748208999634
Epoch 168  Train loss: 0.06929517423405367  Val loss: 0.10254231167003461
Epoch 169
-------------------------------
Batch 1/64 loss: 0.06633627414703369
Batch 2/64 loss: 0.07973545789718628
Batch 3/64 loss: 0.05894505977630615
Batch 4/64 loss: 0.06945019960403442
Batch 5/64 loss: 0.0664067268371582
Batch 6/64 loss: 0.06950891017913818
Batch 7/64 loss: 0.06301271915435791
Batch 8/64 loss: 0.07174599170684814
Batch 9/64 loss: 0.06107908487319946
Batch 10/64 loss: 0.05453556776046753
Batch 11/64 loss: 0.06561172008514404
Batch 12/64 loss: 0.06501156091690063
Batch 13/64 loss: 0.08061575889587402
Batch 14/64 loss: 0.0821075439453125
Batch 15/64 loss: 0.07545185089111328
Batch 16/64 loss: 0.07298922538757324
Batch 17/64 loss: 0.06640946865081787
Batch 18/64 loss: 0.06553244590759277
Batch 19/64 loss: 0.0787387490272522
Batch 20/64 loss: 0.07486903667449951
Batch 21/64 loss: 0.07577431201934814
Batch 22/64 loss: 0.06900382041931152
Batch 23/64 loss: 0.07277381420135498
Batch 24/64 loss: 0.06950622797012329
Batch 25/64 loss: 0.0685276985168457
Batch 26/64 loss: 0.0709233283996582
Batch 27/64 loss: 0.07213586568832397
Batch 28/64 loss: 0.0739591121673584
Batch 29/64 loss: 0.06519031524658203
Batch 30/64 loss: 0.07769417762756348
Batch 31/64 loss: 0.06360411643981934
Batch 32/64 loss: 0.058626532554626465
Batch 33/64 loss: 0.07439953088760376
Batch 34/64 loss: 0.07709383964538574
Batch 35/64 loss: 0.06904858350753784
Batch 36/64 loss: 0.09195858240127563
Batch 37/64 loss: 0.060150742530822754
Batch 38/64 loss: 0.06417399644851685
Batch 39/64 loss: 0.06556332111358643
Batch 40/64 loss: 0.06980633735656738
Batch 41/64 loss: 0.06769871711730957
Batch 42/64 loss: 0.06619793176651001
Batch 43/64 loss: 0.07022994756698608
Batch 44/64 loss: 0.07616400718688965
Batch 45/64 loss: 0.08447444438934326
Batch 46/64 loss: 0.0740932822227478
Batch 47/64 loss: 0.05948060750961304
Batch 48/64 loss: 0.0770026445388794
Batch 49/64 loss: 0.07593947649002075
Batch 50/64 loss: 0.057332396507263184
Batch 51/64 loss: 0.0763084888458252
Batch 52/64 loss: 0.058604419231414795
Batch 53/64 loss: 0.06299048662185669
Batch 54/64 loss: 0.06593793630599976
Batch 55/64 loss: 0.07505667209625244
Batch 56/64 loss: 0.06518584489822388
Batch 57/64 loss: 0.06336754560470581
Batch 58/64 loss: 0.07236611843109131
Batch 59/64 loss: 0.07870149612426758
Batch 60/64 loss: 0.06607598066329956
Batch 61/64 loss: 0.07812178134918213
Batch 62/64 loss: 0.05923354625701904
Batch 63/64 loss: 0.07538950443267822
Batch 64/64 loss: 0.07240921258926392
Epoch 169  Train loss: 0.06993361244014665  Val loss: 0.0967174588200153
Saving best model, epoch: 169
Epoch 170
-------------------------------
Batch 1/64 loss: 0.07353049516677856
Batch 2/64 loss: 0.06438016891479492
Batch 3/64 loss: 0.05972695350646973
Batch 4/64 loss: 0.07182043790817261
Batch 5/64 loss: 0.09698033332824707
Batch 6/64 loss: 0.07232844829559326
Batch 7/64 loss: 0.06507009267807007
Batch 8/64 loss: 0.07148110866546631
Batch 9/64 loss: 0.06543469429016113
Batch 10/64 loss: 0.07348424196243286
Batch 11/64 loss: 0.0670543909072876
Batch 12/64 loss: 0.07025140523910522
Batch 13/64 loss: 0.0752784013748169
Batch 14/64 loss: 0.07143270969390869
Batch 15/64 loss: 0.0716702938079834
Batch 16/64 loss: 0.06591945886611938
Batch 17/64 loss: 0.06905210018157959
Batch 18/64 loss: 0.07088077068328857
Batch 19/64 loss: 0.055969059467315674
Batch 20/64 loss: 0.06732231378555298
Batch 21/64 loss: 0.07570725679397583
Batch 22/64 loss: 0.06534790992736816
Batch 23/64 loss: 0.06828176975250244
Batch 24/64 loss: 0.06902468204498291
Batch 25/64 loss: 0.060265958309173584
Batch 26/64 loss: 0.06977438926696777
Batch 27/64 loss: 0.06433063745498657
Batch 28/64 loss: 0.07197219133377075
Batch 29/64 loss: 0.06459993124008179
Batch 30/64 loss: 0.06210285425186157
Batch 31/64 loss: 0.06032341718673706
Batch 32/64 loss: 0.06295377016067505
Batch 33/64 loss: 0.06450939178466797
Batch 34/64 loss: 0.067294180393219
Batch 35/64 loss: 0.06667280197143555
Batch 36/64 loss: 0.08288991451263428
Batch 37/64 loss: 0.06240212917327881
Batch 38/64 loss: 0.07323908805847168
Batch 39/64 loss: 0.0665130615234375
Batch 40/64 loss: 0.05867362022399902
Batch 41/64 loss: 0.061252713203430176
Batch 42/64 loss: 0.05945652723312378
Batch 43/64 loss: 0.06340622901916504
Batch 44/64 loss: 0.05135488510131836
Batch 45/64 loss: 0.07485103607177734
Batch 46/64 loss: 0.07077932357788086
Batch 47/64 loss: 0.07045912742614746
Batch 48/64 loss: 0.06938153505325317
Batch 49/64 loss: 0.05993151664733887
Batch 50/64 loss: 0.06143927574157715
Batch 51/64 loss: 0.06370151042938232
Batch 52/64 loss: 0.0585329532623291
Batch 53/64 loss: 0.07153671979904175
Batch 54/64 loss: 0.07255184650421143
Batch 55/64 loss: 0.05550813674926758
Batch 56/64 loss: 0.06760841608047485
Batch 57/64 loss: 0.05668085813522339
Batch 58/64 loss: 0.08220136165618896
Batch 59/64 loss: 0.07023561000823975
Batch 60/64 loss: 0.06924682855606079
Batch 61/64 loss: 0.06721925735473633
Batch 62/64 loss: 0.07560992240905762
Batch 63/64 loss: 0.07346862554550171
Batch 64/64 loss: 0.07704412937164307
Epoch 170  Train loss: 0.06776696700675815  Val loss: 0.09771884778111252
Epoch 171
-------------------------------
Batch 1/64 loss: 0.060837745666503906
Batch 2/64 loss: 0.06445115804672241
Batch 3/64 loss: 0.057840168476104736
Batch 4/64 loss: 0.08342117071151733
Batch 5/64 loss: 0.06567037105560303
Batch 6/64 loss: 0.051917433738708496
Batch 7/64 loss: 0.0593569278717041
Batch 8/64 loss: 0.05796414613723755
Batch 9/64 loss: 0.07801574468612671
Batch 10/64 loss: 0.08048462867736816
Batch 11/64 loss: 0.06550097465515137
Batch 12/64 loss: 0.08022016286849976
Batch 13/64 loss: 0.07066243886947632
Batch 14/64 loss: 0.06571024656295776
Batch 15/64 loss: 0.06695055961608887
Batch 16/64 loss: 0.061734914779663086
Batch 17/64 loss: 0.08000069856643677
Batch 18/64 loss: 0.07224196195602417
Batch 19/64 loss: 0.06970828771591187
Batch 20/64 loss: 0.08139002323150635
Batch 21/64 loss: 0.06843888759613037
Batch 22/64 loss: 0.06723147630691528
Batch 23/64 loss: 0.06513857841491699
Batch 24/64 loss: 0.08030945062637329
Batch 25/64 loss: 0.060844361782073975
Batch 26/64 loss: 0.07242923974990845
Batch 27/64 loss: 0.06514918804168701
Batch 28/64 loss: 0.07171452045440674
Batch 29/64 loss: 0.06882256269454956
Batch 30/64 loss: 0.0706748366355896
Batch 31/64 loss: 0.07772260904312134
Batch 32/64 loss: 0.0545884370803833
Batch 33/64 loss: 0.061449646949768066
Batch 34/64 loss: 0.0768272876739502
Batch 35/64 loss: 0.07171940803527832
Batch 36/64 loss: 0.07538419961929321
Batch 37/64 loss: 0.06637305021286011
Batch 38/64 loss: 0.07032966613769531
Batch 39/64 loss: 0.061534762382507324
Batch 40/64 loss: 0.07391703128814697
Batch 41/64 loss: 0.07089626789093018
Batch 42/64 loss: 0.07020729780197144
Batch 43/64 loss: 0.05844193696975708
Batch 44/64 loss: 0.07896411418914795
Batch 45/64 loss: 0.05026066303253174
Batch 46/64 loss: 0.07227504253387451
Batch 47/64 loss: 0.07494527101516724
Batch 48/64 loss: 0.06726658344268799
Batch 49/64 loss: 0.07056766748428345
Batch 50/64 loss: 0.05812644958496094
Batch 51/64 loss: 0.0726502537727356
Batch 52/64 loss: 0.058002769947052
Batch 53/64 loss: 0.06208765506744385
Batch 54/64 loss: 0.06778168678283691
Batch 55/64 loss: 0.06332719326019287
Batch 56/64 loss: 0.06446075439453125
Batch 57/64 loss: 0.0686333179473877
Batch 58/64 loss: 0.07194042205810547
Batch 59/64 loss: 0.0677645206451416
Batch 60/64 loss: 0.06798535585403442
Batch 61/64 loss: 0.08402979373931885
Batch 62/64 loss: 0.07873654365539551
Batch 63/64 loss: 0.08205991983413696
Batch 64/64 loss: 0.05874830484390259
Epoch 171  Train loss: 0.0687082615553164  Val loss: 0.10273379660963602
Epoch 172
-------------------------------
Batch 1/64 loss: 0.055232882499694824
Batch 2/64 loss: 0.060753703117370605
Batch 3/64 loss: 0.08022743463516235
Batch 4/64 loss: 0.0692475438117981
Batch 5/64 loss: 0.08282727003097534
Batch 6/64 loss: 0.07322967052459717
Batch 7/64 loss: 0.06925046443939209
Batch 8/64 loss: 0.06108283996582031
Batch 9/64 loss: 0.07467454671859741
Batch 10/64 loss: 0.05930286645889282
Batch 11/64 loss: 0.06153839826583862
Batch 12/64 loss: 0.07242590188980103
Batch 13/64 loss: 0.06143641471862793
Batch 14/64 loss: 0.06618386507034302
Batch 15/64 loss: 0.08153700828552246
Batch 16/64 loss: 0.06569796800613403
Batch 17/64 loss: 0.06404364109039307
Batch 18/64 loss: 0.061934828758239746
Batch 19/64 loss: 0.0749427080154419
Batch 20/64 loss: 0.05907487869262695
Batch 21/64 loss: 0.06534761190414429
Batch 22/64 loss: 0.06525373458862305
Batch 23/64 loss: 0.09282338619232178
Batch 24/64 loss: 0.08653485774993896
Batch 25/64 loss: 0.06938773393630981
Batch 26/64 loss: 0.06278175115585327
Batch 27/64 loss: 0.07855188846588135
Batch 28/64 loss: 0.07160723209381104
Batch 29/64 loss: 0.07076495885848999
Batch 30/64 loss: 0.08314931392669678
Batch 31/64 loss: 0.06418478488922119
Batch 32/64 loss: 0.08571922779083252
Batch 33/64 loss: 0.069568932056427
Batch 34/64 loss: 0.07261496782302856
Batch 35/64 loss: 0.06466662883758545
Batch 36/64 loss: 0.07133066654205322
Batch 37/64 loss: 0.06451761722564697
Batch 38/64 loss: 0.0953105092048645
Batch 39/64 loss: 0.06923145055770874
Batch 40/64 loss: 0.0889083743095398
Batch 41/64 loss: 0.06832700967788696
Batch 42/64 loss: 0.07643640041351318
Batch 43/64 loss: 0.06438517570495605
Batch 44/64 loss: 0.07627558708190918
Batch 45/64 loss: 0.07900649309158325
Batch 46/64 loss: 0.06848353147506714
Batch 47/64 loss: 0.07035160064697266
Batch 48/64 loss: 0.070412278175354
Batch 49/64 loss: 0.07775723934173584
Batch 50/64 loss: 0.05918687582015991
Batch 51/64 loss: 0.06156128644943237
Batch 52/64 loss: 0.06782305240631104
Batch 53/64 loss: 0.05873364210128784
Batch 54/64 loss: 0.07033860683441162
Batch 55/64 loss: 0.07259762287139893
Batch 56/64 loss: 0.07312029600143433
Batch 57/64 loss: 0.05098181962966919
Batch 58/64 loss: 0.06386858224868774
Batch 59/64 loss: 0.06162071228027344
Batch 60/64 loss: 0.07638639211654663
Batch 61/64 loss: 0.08101814985275269
Batch 62/64 loss: 0.059948086738586426
Batch 63/64 loss: 0.06538617610931396
Batch 64/64 loss: 0.0664791464805603
Epoch 172  Train loss: 0.07012966964759079  Val loss: 0.09887260260041227
Epoch 173
-------------------------------
Batch 1/64 loss: 0.05451422929763794
Batch 2/64 loss: 0.05832570791244507
Batch 3/64 loss: 0.07130205631256104
Batch 4/64 loss: 0.059534430503845215
Batch 5/64 loss: 0.06944561004638672
Batch 6/64 loss: 0.05700713396072388
Batch 7/64 loss: 0.0914534330368042
Batch 8/64 loss: 0.08117717504501343
Batch 9/64 loss: 0.06170523166656494
Batch 10/64 loss: 0.05811440944671631
Batch 11/64 loss: 0.07323801517486572
Batch 12/64 loss: 0.06488102674484253
Batch 13/64 loss: 0.06812697649002075
Batch 14/64 loss: 0.06225323677062988
Batch 15/64 loss: 0.06796389818191528
Batch 16/64 loss: 0.06290698051452637
Batch 17/64 loss: 0.06095850467681885
Batch 18/64 loss: 0.04769301414489746
Batch 19/64 loss: 0.06152421236038208
Batch 20/64 loss: 0.06064426898956299
Batch 21/64 loss: 0.0626068115234375
Batch 22/64 loss: 0.07375705242156982
Batch 23/64 loss: 0.053271472454071045
Batch 24/64 loss: 0.08030873537063599
Batch 25/64 loss: 0.06908339262008667
Batch 26/64 loss: 0.06956732273101807
Batch 27/64 loss: 0.07051557302474976
Batch 28/64 loss: 0.06434804201126099
Batch 29/64 loss: 0.08793991804122925
Batch 30/64 loss: 0.060058414936065674
Batch 31/64 loss: 0.07177817821502686
Batch 32/64 loss: 0.07358568906784058
Batch 33/64 loss: 0.06795746088027954
Batch 34/64 loss: 0.059152305126190186
Batch 35/64 loss: 0.06768929958343506
Batch 36/64 loss: 0.07044363021850586
Batch 37/64 loss: 0.06651127338409424
Batch 38/64 loss: 0.06347841024398804
Batch 39/64 loss: 0.06745219230651855
Batch 40/64 loss: 0.06294286251068115
Batch 41/64 loss: 0.06962013244628906
Batch 42/64 loss: 0.07710367441177368
Batch 43/64 loss: 0.061391592025756836
Batch 44/64 loss: 0.07844126224517822
Batch 45/64 loss: 0.06297457218170166
Batch 46/64 loss: 0.08471691608428955
Batch 47/64 loss: 0.07544046640396118
Batch 48/64 loss: 0.057431161403656006
Batch 49/64 loss: 0.07975059747695923
Batch 50/64 loss: 0.07312262058258057
Batch 51/64 loss: 0.06490278244018555
Batch 52/64 loss: 0.06981074810028076
Batch 53/64 loss: 0.06273311376571655
Batch 54/64 loss: 0.0682639479637146
Batch 55/64 loss: 0.05730360746383667
Batch 56/64 loss: 0.07511073350906372
Batch 57/64 loss: 0.06190091371536255
Batch 58/64 loss: 0.06008434295654297
Batch 59/64 loss: 0.06919538974761963
Batch 60/64 loss: 0.06078273057937622
Batch 61/64 loss: 0.06605041027069092
Batch 62/64 loss: 0.0653645396232605
Batch 63/64 loss: 0.06836670637130737
Batch 64/64 loss: 0.07143306732177734
Epoch 173  Train loss: 0.06711616235620835  Val loss: 0.09921828782845199
Epoch 174
-------------------------------
Batch 1/64 loss: 0.05799853801727295
Batch 2/64 loss: 0.061119794845581055
Batch 3/64 loss: 0.0641552209854126
Batch 4/64 loss: 0.06184309720993042
Batch 5/64 loss: 0.0856364369392395
Batch 6/64 loss: 0.06445121765136719
Batch 7/64 loss: 0.06134045124053955
Batch 8/64 loss: 0.05645471811294556
Batch 9/64 loss: 0.07626843452453613
Batch 10/64 loss: 0.0842943787574768
Batch 11/64 loss: 0.06307494640350342
Batch 12/64 loss: 0.0586012601852417
Batch 13/64 loss: 0.056143760681152344
Batch 14/64 loss: 0.04963099956512451
Batch 15/64 loss: 0.06072789430618286
Batch 16/64 loss: 0.06514966487884521
Batch 17/64 loss: 0.05432426929473877
Batch 18/64 loss: 0.06835746765136719
Batch 19/64 loss: 0.06912988424301147
Batch 20/64 loss: 0.05663406848907471
Batch 21/64 loss: 0.06892859935760498
Batch 22/64 loss: 0.05644255876541138
Batch 23/64 loss: 0.06387472152709961
Batch 24/64 loss: 0.0588458776473999
Batch 25/64 loss: 0.058049023151397705
Batch 26/64 loss: 0.0603337287902832
Batch 27/64 loss: 0.09194165468215942
Batch 28/64 loss: 0.06372708082199097
Batch 29/64 loss: 0.0608367919921875
Batch 30/64 loss: 0.056476891040802
Batch 31/64 loss: 0.05749255418777466
Batch 32/64 loss: 0.08812481164932251
Batch 33/64 loss: 0.05756884813308716
Batch 34/64 loss: 0.0687859058380127
Batch 35/64 loss: 0.07005804777145386
Batch 36/64 loss: 0.07009094953536987
Batch 37/64 loss: 0.07268214225769043
Batch 38/64 loss: 0.06152445077896118
Batch 39/64 loss: 0.06634622812271118
Batch 40/64 loss: 0.05759328603744507
Batch 41/64 loss: 0.0661810040473938
Batch 42/64 loss: 0.07307553291320801
Batch 43/64 loss: 0.07642221450805664
Batch 44/64 loss: 0.07552731037139893
Batch 45/64 loss: 0.048683762550354004
Batch 46/64 loss: 0.0805368423461914
Batch 47/64 loss: 0.06140708923339844
Batch 48/64 loss: 0.05510509014129639
Batch 49/64 loss: 0.06391739845275879
Batch 50/64 loss: 0.05838054418563843
Batch 51/64 loss: 0.06460452079772949
Batch 52/64 loss: 0.05934298038482666
Batch 53/64 loss: 0.07015550136566162
Batch 54/64 loss: 0.05996584892272949
Batch 55/64 loss: 0.05691885948181152
Batch 56/64 loss: 0.06335097551345825
Batch 57/64 loss: 0.07089841365814209
Batch 58/64 loss: 0.06568491458892822
Batch 59/64 loss: 0.07554483413696289
Batch 60/64 loss: 0.06983047723770142
Batch 61/64 loss: 0.07077240943908691
Batch 62/64 loss: 0.07359063625335693
Batch 63/64 loss: 0.0594601035118103
Batch 64/64 loss: 0.07591044902801514
Epoch 174  Train loss: 0.0652760903040568  Val loss: 0.09722805760570408
Epoch 175
-------------------------------
Batch 1/64 loss: 0.06588947772979736
Batch 2/64 loss: 0.068450927734375
Batch 3/64 loss: 0.060270845890045166
Batch 4/64 loss: 0.05964529514312744
Batch 5/64 loss: 0.06444954872131348
Batch 6/64 loss: 0.061975836753845215
Batch 7/64 loss: 0.07334321737289429
Batch 8/64 loss: 0.06268924474716187
Batch 9/64 loss: 0.059906840324401855
Batch 10/64 loss: 0.05053466558456421
Batch 11/64 loss: 0.06537973880767822
Batch 12/64 loss: 0.06912213563919067
Batch 13/64 loss: 0.0676005482673645
Batch 14/64 loss: 0.06544190645217896
Batch 15/64 loss: 0.06669449806213379
Batch 16/64 loss: 0.06548929214477539
Batch 17/64 loss: 0.0653543472290039
Batch 18/64 loss: 0.06643414497375488
Batch 19/64 loss: 0.06636059284210205
Batch 20/64 loss: 0.06518018245697021
Batch 21/64 loss: 0.07470780611038208
Batch 22/64 loss: 0.06514638662338257
Batch 23/64 loss: 0.054256558418273926
Batch 24/64 loss: 0.06148415803909302
Batch 25/64 loss: 0.0592494010925293
Batch 26/64 loss: 0.06367331743240356
Batch 27/64 loss: 0.053187787532806396
Batch 28/64 loss: 0.06052619218826294
Batch 29/64 loss: 0.059259653091430664
Batch 30/64 loss: 0.0650937557220459
Batch 31/64 loss: 0.06297141313552856
Batch 32/64 loss: 0.05727863311767578
Batch 33/64 loss: 0.06991839408874512
Batch 34/64 loss: 0.06167334318161011
Batch 35/64 loss: 0.06888729333877563
Batch 36/64 loss: 0.05012089014053345
Batch 37/64 loss: 0.07602787017822266
Batch 38/64 loss: 0.06025242805480957
Batch 39/64 loss: 0.05549746751785278
Batch 40/64 loss: 0.0667847990989685
Batch 41/64 loss: 0.06455200910568237
Batch 42/64 loss: 0.07011908292770386
Batch 43/64 loss: 0.06155812740325928
Batch 44/64 loss: 0.05946415662765503
Batch 45/64 loss: 0.06436645984649658
Batch 46/64 loss: 0.06334209442138672
Batch 47/64 loss: 0.06101423501968384
Batch 48/64 loss: 0.05813407897949219
Batch 49/64 loss: 0.07209253311157227
Batch 50/64 loss: 0.06687808036804199
Batch 51/64 loss: 0.05663388967514038
Batch 52/64 loss: 0.05712682008743286
Batch 53/64 loss: 0.056313514709472656
Batch 54/64 loss: 0.08829712867736816
Batch 55/64 loss: 0.07399892807006836
Batch 56/64 loss: 0.07075154781341553
Batch 57/64 loss: 0.08960074186325073
Batch 58/64 loss: 0.06428968906402588
Batch 59/64 loss: 0.07778537273406982
Batch 60/64 loss: 0.07785505056381226
Batch 61/64 loss: 0.06466704607009888
Batch 62/64 loss: 0.052454233169555664
Batch 63/64 loss: 0.07495516538619995
Batch 64/64 loss: 0.06922018527984619
Epoch 175  Train loss: 0.06485295622956519  Val loss: 0.0960741073814864
Saving best model, epoch: 175
Epoch 176
-------------------------------
Batch 1/64 loss: 0.08015340566635132
Batch 2/64 loss: 0.05961209535598755
Batch 3/64 loss: 0.09006071090698242
Batch 4/64 loss: 0.08489334583282471
Batch 5/64 loss: 0.05952078104019165
Batch 6/64 loss: 0.061012446880340576
Batch 7/64 loss: 0.06789880990982056
Batch 8/64 loss: 0.053910017013549805
Batch 9/64 loss: 0.06274914741516113
Batch 10/64 loss: 0.07073688507080078
Batch 11/64 loss: 0.05768454074859619
Batch 12/64 loss: 0.0579184889793396
Batch 13/64 loss: 0.06820344924926758
Batch 14/64 loss: 0.06718325614929199
Batch 15/64 loss: 0.057941973209381104
Batch 16/64 loss: 0.06617814302444458
Batch 17/64 loss: 0.055117130279541016
Batch 18/64 loss: 0.06664454936981201
Batch 19/64 loss: 0.05377012491226196
Batch 20/64 loss: 0.058167099952697754
Batch 21/64 loss: 0.06553435325622559
Batch 22/64 loss: 0.04941511154174805
Batch 23/64 loss: 0.05383610725402832
Batch 24/64 loss: 0.0743071436882019
Batch 25/64 loss: 0.05423814058303833
Batch 26/64 loss: 0.06812185049057007
Batch 27/64 loss: 0.06934821605682373
Batch 28/64 loss: 0.05805516242980957
Batch 29/64 loss: 0.057185590267181396
Batch 30/64 loss: 0.07378160953521729
Batch 31/64 loss: 0.0755009651184082
Batch 32/64 loss: 0.06580543518066406
Batch 33/64 loss: 0.07589453458786011
Batch 34/64 loss: 0.05787837505340576
Batch 35/64 loss: 0.06684571504592896
Batch 36/64 loss: 0.06049984693527222
Batch 37/64 loss: 0.05375254154205322
Batch 38/64 loss: 0.06883430480957031
Batch 39/64 loss: 0.06449174880981445
Batch 40/64 loss: 0.07506054639816284
Batch 41/64 loss: 0.05173850059509277
Batch 42/64 loss: 0.07201945781707764
Batch 43/64 loss: 0.06054192781448364
Batch 44/64 loss: 0.05545079708099365
Batch 45/64 loss: 0.0680687427520752
Batch 46/64 loss: 0.046482205390930176
Batch 47/64 loss: 0.07093822956085205
Batch 48/64 loss: 0.06287193298339844
Batch 49/64 loss: 0.057367920875549316
Batch 50/64 loss: 0.0653156042098999
Batch 51/64 loss: 0.06287658214569092
Batch 52/64 loss: 0.05961048603057861
Batch 53/64 loss: 0.06865072250366211
Batch 54/64 loss: 0.06039249897003174
Batch 55/64 loss: 0.06219667196273804
Batch 56/64 loss: 0.07520830631256104
Batch 57/64 loss: 0.08015787601470947
Batch 58/64 loss: 0.06796145439147949
Batch 59/64 loss: 0.0720711350440979
Batch 60/64 loss: 0.05482947826385498
Batch 61/64 loss: 0.0728985071182251
Batch 62/64 loss: 0.05992215871810913
Batch 63/64 loss: 0.07200455665588379
Batch 64/64 loss: 0.06090831756591797
Epoch 176  Train loss: 0.06451765789705165  Val loss: 0.09734818103796837
Epoch 177
-------------------------------
Batch 1/64 loss: 0.055807292461395264
Batch 2/64 loss: 0.07014840841293335
Batch 3/64 loss: 0.052896857261657715
Batch 4/64 loss: 0.06666874885559082
Batch 5/64 loss: 0.07162845134735107
Batch 6/64 loss: 0.07194703817367554
Batch 7/64 loss: 0.06670564413070679
Batch 8/64 loss: 0.0650472640991211
Batch 9/64 loss: 0.07700538635253906
Batch 10/64 loss: 0.0606040358543396
Batch 11/64 loss: 0.07524663209915161
Batch 12/64 loss: 0.06856250762939453
Batch 13/64 loss: 0.07574927806854248
Batch 14/64 loss: 0.07826358079910278
Batch 15/64 loss: 0.06337827444076538
Batch 16/64 loss: 0.059754908084869385
Batch 17/64 loss: 0.06338244676589966
Batch 18/64 loss: 0.07797837257385254
Batch 19/64 loss: 0.05279982089996338
Batch 20/64 loss: 0.07449424266815186
Batch 21/64 loss: 0.0596889853477478
Batch 22/64 loss: 0.07936638593673706
Batch 23/64 loss: 0.06792265176773071
Batch 24/64 loss: 0.05955260992050171
Batch 25/64 loss: 0.05999642610549927
Batch 26/64 loss: 0.0597681999206543
Batch 27/64 loss: 0.07915931940078735
Batch 28/64 loss: 0.06518536806106567
Batch 29/64 loss: 0.06004691123962402
Batch 30/64 loss: 0.06441754102706909
Batch 31/64 loss: 0.06915253400802612
Batch 32/64 loss: 0.054896533489227295
Batch 33/64 loss: 0.06583088636398315
Batch 34/64 loss: 0.07300758361816406
Batch 35/64 loss: 0.0621495246887207
Batch 36/64 loss: 0.060951948165893555
Batch 37/64 loss: 0.06400096416473389
Batch 38/64 loss: 0.06080406904220581
Batch 39/64 loss: 0.05752319097518921
Batch 40/64 loss: 0.06321382522583008
Batch 41/64 loss: 0.06325262784957886
Batch 42/64 loss: 0.08286267518997192
Batch 43/64 loss: 0.07321929931640625
Batch 44/64 loss: 0.060775935649871826
Batch 45/64 loss: 0.07187336683273315
Batch 46/64 loss: 0.06346750259399414
Batch 47/64 loss: 0.06993526220321655
Batch 48/64 loss: 0.0759018063545227
Batch 49/64 loss: 0.08537960052490234
Batch 50/64 loss: 0.06956833600997925
Batch 51/64 loss: 0.07824540138244629
Batch 52/64 loss: 0.05864232778549194
Batch 53/64 loss: 0.06701439619064331
Batch 54/64 loss: 0.07710140943527222
Batch 55/64 loss: 0.06078827381134033
Batch 56/64 loss: 0.07007622718811035
Batch 57/64 loss: 0.08708345890045166
Batch 58/64 loss: 0.06328201293945312
Batch 59/64 loss: 0.06503009796142578
Batch 60/64 loss: 0.057853639125823975
Batch 61/64 loss: 0.07149779796600342
Batch 62/64 loss: 0.06444841623306274
Batch 63/64 loss: 0.06228107213973999
Batch 64/64 loss: 0.07166826725006104
Epoch 177  Train loss: 0.0672633222505158  Val loss: 0.09608440976781943
Epoch 178
-------------------------------
Batch 1/64 loss: 0.05883669853210449
Batch 2/64 loss: 0.051632046699523926
Batch 3/64 loss: 0.06550836563110352
Batch 4/64 loss: 0.05603605508804321
Batch 5/64 loss: 0.06497365236282349
Batch 6/64 loss: 0.05699640512466431
Batch 7/64 loss: 0.07687419652938843
Batch 8/64 loss: 0.08476734161376953
Batch 9/64 loss: 0.07010716199874878
Batch 10/64 loss: 0.06392574310302734
Batch 11/64 loss: 0.05765265226364136
Batch 12/64 loss: 0.058694660663604736
Batch 13/64 loss: 0.06023770570755005
Batch 14/64 loss: 0.059892356395721436
Batch 15/64 loss: 0.06276547908782959
Batch 16/64 loss: 0.06832939386367798
Batch 17/64 loss: 0.059792280197143555
Batch 18/64 loss: 0.07468324899673462
Batch 19/64 loss: 0.07351058721542358
Batch 20/64 loss: 0.06579995155334473
Batch 21/64 loss: 0.05855393409729004
Batch 22/64 loss: 0.05969691276550293
Batch 23/64 loss: 0.06690728664398193
Batch 24/64 loss: 0.04750180244445801
Batch 25/64 loss: 0.0775766372680664
Batch 26/64 loss: 0.059899747371673584
Batch 27/64 loss: 0.06875967979431152
Batch 28/64 loss: 0.05825197696685791
Batch 29/64 loss: 0.06625741720199585
Batch 30/64 loss: 0.06192028522491455
Batch 31/64 loss: 0.05777144432067871
Batch 32/64 loss: 0.06755918264389038
Batch 33/64 loss: 0.06375521421432495
Batch 34/64 loss: 0.05696702003479004
Batch 35/64 loss: 0.06866610050201416
Batch 36/64 loss: 0.04885077476501465
Batch 37/64 loss: 0.05524563789367676
Batch 38/64 loss: 0.056922733783721924
Batch 39/64 loss: 0.05994778871536255
Batch 40/64 loss: 0.06481212377548218
Batch 41/64 loss: 0.056789934635162354
Batch 42/64 loss: 0.06376266479492188
Batch 43/64 loss: 0.05948096513748169
Batch 44/64 loss: 0.06398195028305054
Batch 45/64 loss: 0.052273571491241455
Batch 46/64 loss: 0.06778109073638916
Batch 47/64 loss: 0.07465648651123047
Batch 48/64 loss: 0.05734175443649292
Batch 49/64 loss: 0.05308663845062256
Batch 50/64 loss: 0.062041282653808594
Batch 51/64 loss: 0.06842559576034546
Batch 52/64 loss: 0.07544165849685669
Batch 53/64 loss: 0.07650202512741089
Batch 54/64 loss: 0.08030366897583008
Batch 55/64 loss: 0.058074235916137695
Batch 56/64 loss: 0.08017861843109131
Batch 57/64 loss: 0.06479448080062866
Batch 58/64 loss: 0.06946897506713867
Batch 59/64 loss: 0.060894131660461426
Batch 60/64 loss: 0.06567275524139404
Batch 61/64 loss: 0.06234031915664673
Batch 62/64 loss: 0.06434082984924316
Batch 63/64 loss: 0.06204932928085327
Batch 64/64 loss: 0.06096172332763672
Epoch 178  Train loss: 0.06372147354425169  Val loss: 0.09789022403894011
Epoch 179
-------------------------------
Batch 1/64 loss: 0.0764775276184082
Batch 2/64 loss: 0.06683218479156494
Batch 3/64 loss: 0.06328952312469482
Batch 4/64 loss: 0.0832597017288208
Batch 5/64 loss: 0.05772507190704346
Batch 6/64 loss: 0.06047886610031128
Batch 7/64 loss: 0.06937307119369507
Batch 8/64 loss: 0.0760653018951416
Batch 9/64 loss: 0.05780625343322754
Batch 10/64 loss: 0.05647611618041992
Batch 11/64 loss: 0.07523703575134277
Batch 12/64 loss: 0.06460732221603394
Batch 13/64 loss: 0.053137362003326416
Batch 14/64 loss: 0.06036019325256348
Batch 15/64 loss: 0.06992393732070923
Batch 16/64 loss: 0.05201292037963867
Batch 17/64 loss: 0.058242082595825195
Batch 18/64 loss: 0.05619978904724121
Batch 19/64 loss: 0.07241463661193848
Batch 20/64 loss: 0.06237882375717163
Batch 21/64 loss: 0.06002449989318848
Batch 22/64 loss: 0.06289166212081909
Batch 23/64 loss: 0.05833017826080322
Batch 24/64 loss: 0.06505709886550903
Batch 25/64 loss: 0.06522423028945923
Batch 26/64 loss: 0.05736863613128662
Batch 27/64 loss: 0.05662107467651367
Batch 28/64 loss: 0.07185918092727661
Batch 29/64 loss: 0.07625150680541992
Batch 30/64 loss: 0.07009661197662354
Batch 31/64 loss: 0.05802953243255615
Batch 32/64 loss: 0.057710230350494385
Batch 33/64 loss: 0.055027782917022705
Batch 34/64 loss: 0.0576934814453125
Batch 35/64 loss: 0.05545461177825928
Batch 36/64 loss: 0.06949567794799805
Batch 37/64 loss: 0.050717711448669434
Batch 38/64 loss: 0.05233639478683472
Batch 39/64 loss: 0.07432013750076294
Batch 40/64 loss: 0.059453725814819336
Batch 41/64 loss: 0.051037371158599854
Batch 42/64 loss: 0.09016573429107666
Batch 43/64 loss: 0.05698835849761963
Batch 44/64 loss: 0.06528526544570923
Batch 45/64 loss: 0.06110739707946777
Batch 46/64 loss: 0.07827752828598022
Batch 47/64 loss: 0.06043452024459839
Batch 48/64 loss: 0.06775331497192383
Batch 49/64 loss: 0.05462062358856201
Batch 50/64 loss: 0.09001988172531128
Batch 51/64 loss: 0.059591054916381836
Batch 52/64 loss: 0.07026958465576172
Batch 53/64 loss: 0.06931442022323608
Batch 54/64 loss: 0.07924997806549072
Batch 55/64 loss: 0.05465906858444214
Batch 56/64 loss: 0.0696374773979187
Batch 57/64 loss: 0.061625003814697266
Batch 58/64 loss: 0.05943793058395386
Batch 59/64 loss: 0.057221293449401855
Batch 60/64 loss: 0.07190805673599243
Batch 61/64 loss: 0.053685545921325684
Batch 62/64 loss: 0.06397920846939087
Batch 63/64 loss: 0.06012296676635742
Batch 64/64 loss: 0.06557011604309082
Epoch 179  Train loss: 0.06402871935975318  Val loss: 0.09546313048228365
Saving best model, epoch: 179
Epoch 180
-------------------------------
Batch 1/64 loss: 0.05336260795593262
Batch 2/64 loss: 0.08279907703399658
Batch 3/64 loss: 0.07280725240707397
Batch 4/64 loss: 0.07622653245925903
Batch 5/64 loss: 0.06499207019805908
Batch 6/64 loss: 0.05361354351043701
Batch 7/64 loss: 0.06409817934036255
Batch 8/64 loss: 0.0550045371055603
Batch 9/64 loss: 0.06535422801971436
Batch 10/64 loss: 0.062480390071868896
Batch 11/64 loss: 0.05887949466705322
Batch 12/64 loss: 0.059523046016693115
Batch 13/64 loss: 0.06818753480911255
Batch 14/64 loss: 0.06143689155578613
Batch 15/64 loss: 0.07787102460861206
Batch 16/64 loss: 0.061588287353515625
Batch 17/64 loss: 0.06290996074676514
Batch 18/64 loss: 0.07122611999511719
Batch 19/64 loss: 0.07646673917770386
Batch 20/64 loss: 0.06146740913391113
Batch 21/64 loss: 0.06560486555099487
Batch 22/64 loss: 0.05746960639953613
Batch 23/64 loss: 0.06820112466812134
Batch 24/64 loss: 0.06482619047164917
Batch 25/64 loss: 0.05997186899185181
Batch 26/64 loss: 0.055471062660217285
Batch 27/64 loss: 0.05777251720428467
Batch 28/64 loss: 0.07643508911132812
Batch 29/64 loss: 0.04672807455062866
Batch 30/64 loss: 0.0544123649597168
Batch 31/64 loss: 0.06253468990325928
Batch 32/64 loss: 0.06420129537582397
Batch 33/64 loss: 0.055376529693603516
Batch 34/64 loss: 0.06247520446777344
Batch 35/64 loss: 0.07842588424682617
Batch 36/64 loss: 0.08874285221099854
Batch 37/64 loss: 0.07872951030731201
Batch 38/64 loss: 0.06426817178726196
Batch 39/64 loss: 0.07290405035018921
Batch 40/64 loss: 0.06581109762191772
Batch 41/64 loss: 0.056508421897888184
Batch 42/64 loss: 0.06366854906082153
Batch 43/64 loss: 0.08696925640106201
Batch 44/64 loss: 0.05575227737426758
Batch 45/64 loss: 0.06598007678985596
Batch 46/64 loss: 0.07131588459014893
Batch 47/64 loss: 0.06053590774536133
Batch 48/64 loss: 0.06132614612579346
Batch 49/64 loss: 0.06006830930709839
Batch 50/64 loss: 0.07058990001678467
Batch 51/64 loss: 0.05998718738555908
Batch 52/64 loss: 0.07124519348144531
Batch 53/64 loss: 0.06532800197601318
Batch 54/64 loss: 0.06369554996490479
Batch 55/64 loss: 0.06855356693267822
Batch 56/64 loss: 0.05861377716064453
Batch 57/64 loss: 0.06405794620513916
Batch 58/64 loss: 0.059601545333862305
Batch 59/64 loss: 0.0545727014541626
Batch 60/64 loss: 0.07249683141708374
Batch 61/64 loss: 0.05296605825424194
Batch 62/64 loss: 0.04632693529129028
Batch 63/64 loss: 0.06399756669998169
Batch 64/64 loss: 0.06103938817977905
Epoch 180  Train loss: 0.06447990758746278  Val loss: 0.09666178705766029
Epoch 181
-------------------------------
Batch 1/64 loss: 0.06703424453735352
Batch 2/64 loss: 0.070057213306427
Batch 3/64 loss: 0.06191301345825195
Batch 4/64 loss: 0.06457948684692383
Batch 5/64 loss: 0.06171441078186035
Batch 6/64 loss: 0.07629817724227905
Batch 7/64 loss: 0.05333530902862549
Batch 8/64 loss: 0.06889033317565918
Batch 9/64 loss: 0.046862900257110596
Batch 10/64 loss: 0.06245589256286621
Batch 11/64 loss: 0.06435215473175049
Batch 12/64 loss: 0.05794084072113037
Batch 13/64 loss: 0.05963784456253052
Batch 14/64 loss: 0.07373380661010742
Batch 15/64 loss: 0.050495803356170654
Batch 16/64 loss: 0.05829298496246338
Batch 17/64 loss: 0.06334292888641357
Batch 18/64 loss: 0.05589568614959717
Batch 19/64 loss: 0.07041013240814209
Batch 20/64 loss: 0.05234724283218384
Batch 21/64 loss: 0.06872951984405518
Batch 22/64 loss: 0.06280946731567383
Batch 23/64 loss: 0.05576449632644653
Batch 24/64 loss: 0.05496472120285034
Batch 25/64 loss: 0.06998234987258911
Batch 26/64 loss: 0.07048988342285156
Batch 27/64 loss: 0.052845120429992676
Batch 28/64 loss: 0.07879209518432617
Batch 29/64 loss: 0.06833970546722412
Batch 30/64 loss: 0.07014095783233643
Batch 31/64 loss: 0.054159343242645264
Batch 32/64 loss: 0.06899881362915039
Batch 33/64 loss: 0.06559216976165771
Batch 34/64 loss: 0.06550979614257812
Batch 35/64 loss: 0.06436359882354736
Batch 36/64 loss: 0.054990172386169434
Batch 37/64 loss: 0.07788121700286865
Batch 38/64 loss: 0.07318258285522461
Batch 39/64 loss: 0.07228755950927734
Batch 40/64 loss: 0.07023131847381592
Batch 41/64 loss: 0.06041848659515381
Batch 42/64 loss: 0.053523361682891846
Batch 43/64 loss: 0.06449449062347412
Batch 44/64 loss: 0.058984339237213135
Batch 45/64 loss: 0.06798595190048218
Batch 46/64 loss: 0.06413930654525757
Batch 47/64 loss: 0.053979456424713135
Batch 48/64 loss: 0.04757505655288696
Batch 49/64 loss: 0.05823469161987305
Batch 50/64 loss: 0.061960816383361816
Batch 51/64 loss: 0.05238962173461914
Batch 52/64 loss: 0.06585854291915894
Batch 53/64 loss: 0.05628305673599243
Batch 54/64 loss: 0.05903518199920654
Batch 55/64 loss: 0.06260454654693604
Batch 56/64 loss: 0.059763312339782715
Batch 57/64 loss: 0.05553281307220459
Batch 58/64 loss: 0.05863142013549805
Batch 59/64 loss: 0.06690752506256104
Batch 60/64 loss: 0.05692183971405029
Batch 61/64 loss: 0.08007824420928955
Batch 62/64 loss: 0.06170123815536499
Batch 63/64 loss: 0.06837385892868042
Batch 64/64 loss: 0.05793982744216919
Epoch 181  Train loss: 0.0627212129387201  Val loss: 0.09469725585885064
Saving best model, epoch: 181
Epoch 182
-------------------------------
Batch 1/64 loss: 0.05713951587677002
Batch 2/64 loss: 0.05894726514816284
Batch 3/64 loss: 0.07075315713882446
Batch 4/64 loss: 0.05916076898574829
Batch 5/64 loss: 0.05171215534210205
Batch 6/64 loss: 0.07732093334197998
Batch 7/64 loss: 0.058519065380096436
Batch 8/64 loss: 0.05958491563796997
Batch 9/64 loss: 0.06728285551071167
Batch 10/64 loss: 0.06750255823135376
Batch 11/64 loss: 0.06483584642410278
Batch 12/64 loss: 0.08310896158218384
Batch 13/64 loss: 0.057970643043518066
Batch 14/64 loss: 0.05674874782562256
Batch 15/64 loss: 0.054963648319244385
Batch 16/64 loss: 0.0677289366722107
Batch 17/64 loss: 0.05876559019088745
Batch 18/64 loss: 0.05168575048446655
Batch 19/64 loss: 0.06796681880950928
Batch 20/64 loss: 0.06887322664260864
Batch 21/64 loss: 0.059401869773864746
Batch 22/64 loss: 0.05367636680603027
Batch 23/64 loss: 0.06991034746170044
Batch 24/64 loss: 0.0690951943397522
Batch 25/64 loss: 0.048348069190979004
Batch 26/64 loss: 0.06316220760345459
Batch 27/64 loss: 0.06486266851425171
Batch 28/64 loss: 0.05825561285018921
Batch 29/64 loss: 0.0468982458114624
Batch 30/64 loss: 0.05451864004135132
Batch 31/64 loss: 0.052610576152801514
Batch 32/64 loss: 0.0690762996673584
Batch 33/64 loss: 0.05603909492492676
Batch 34/64 loss: 0.06137681007385254
Batch 35/64 loss: 0.05146700143814087
Batch 36/64 loss: 0.06380373239517212
Batch 37/64 loss: 0.06720620393753052
Batch 38/64 loss: 0.07238787412643433
Batch 39/64 loss: 0.06264835596084595
Batch 40/64 loss: 0.05492877960205078
Batch 41/64 loss: 0.05404418706893921
Batch 42/64 loss: 0.06696611642837524
Batch 43/64 loss: 0.055971384048461914
Batch 44/64 loss: 0.06919264793395996
Batch 45/64 loss: 0.05467665195465088
Batch 46/64 loss: 0.07288962602615356
Batch 47/64 loss: 0.045276761054992676
Batch 48/64 loss: 0.0657452940940857
Batch 49/64 loss: 0.051688432693481445
Batch 50/64 loss: 0.0627666711807251
Batch 51/64 loss: 0.0542791485786438
Batch 52/64 loss: 0.05621671676635742
Batch 53/64 loss: 0.062209248542785645
Batch 54/64 loss: 0.0664604902267456
Batch 55/64 loss: 0.0565030574798584
Batch 56/64 loss: 0.06527101993560791
Batch 57/64 loss: 0.06064796447753906
Batch 58/64 loss: 0.05532580614089966
Batch 59/64 loss: 0.05895030498504639
Batch 60/64 loss: 0.0633421540260315
Batch 61/64 loss: 0.05844968557357788
Batch 62/64 loss: 0.07256853580474854
Batch 63/64 loss: 0.06143641471862793
Batch 64/64 loss: 0.06381332874298096
Epoch 182  Train loss: 0.06116052749110203  Val loss: 0.09413719464003835
Saving best model, epoch: 182
Epoch 183
-------------------------------
Batch 1/64 loss: 0.05653959512710571
Batch 2/64 loss: 0.06497883796691895
Batch 3/64 loss: 0.06889349222183228
Batch 4/64 loss: 0.06735944747924805
Batch 5/64 loss: 0.05816805362701416
Batch 6/64 loss: 0.06903672218322754
Batch 7/64 loss: 0.05691462755203247
Batch 8/64 loss: 0.06805551052093506
Batch 9/64 loss: 0.05483531951904297
Batch 10/64 loss: 0.06778448820114136
Batch 11/64 loss: 0.046510398387908936
Batch 12/64 loss: 0.06056183576583862
Batch 13/64 loss: 0.05911761522293091
Batch 14/64 loss: 0.052594900131225586
Batch 15/64 loss: 0.05162346363067627
Batch 16/64 loss: 0.07100945711135864
Batch 17/64 loss: 0.047766685485839844
Batch 18/64 loss: 0.06890326738357544
Batch 19/64 loss: 0.055585384368896484
Batch 20/64 loss: 0.06324541568756104
Batch 21/64 loss: 0.051966190338134766
Batch 22/64 loss: 0.05798465013504028
Batch 23/64 loss: 0.06351977586746216
Batch 24/64 loss: 0.05581784248352051
Batch 25/64 loss: 0.05434352159500122
Batch 26/64 loss: 0.07460677623748779
Batch 27/64 loss: 0.0636259913444519
Batch 28/64 loss: 0.06308227777481079
Batch 29/64 loss: 0.0713157057762146
Batch 30/64 loss: 0.07514923810958862
Batch 31/64 loss: 0.06638777256011963
Batch 32/64 loss: 0.06121283769607544
Batch 33/64 loss: 0.05155664682388306
Batch 34/64 loss: 0.07183480262756348
Batch 35/64 loss: 0.0647134780883789
Batch 36/64 loss: 0.06381231546401978
Batch 37/64 loss: 0.059085845947265625
Batch 38/64 loss: 0.04852914810180664
Batch 39/64 loss: 0.06485462188720703
Batch 40/64 loss: 0.06733357906341553
Batch 41/64 loss: 0.05648702383041382
Batch 42/64 loss: 0.044121742248535156
Batch 43/64 loss: 0.05816781520843506
Batch 44/64 loss: 0.05349808931350708
Batch 45/64 loss: 0.05092728137969971
Batch 46/64 loss: 0.055637359619140625
Batch 47/64 loss: 0.050943970680236816
Batch 48/64 loss: 0.053029775619506836
Batch 49/64 loss: 0.06809282302856445
Batch 50/64 loss: 0.07325667142868042
Batch 51/64 loss: 0.0700264573097229
Batch 52/64 loss: 0.06952404975891113
Batch 53/64 loss: 0.06311243772506714
Batch 54/64 loss: 0.050700247287750244
Batch 55/64 loss: 0.053772032260894775
Batch 56/64 loss: 0.05831331014633179
Batch 57/64 loss: 0.05600130558013916
Batch 58/64 loss: 0.0632902979850769
Batch 59/64 loss: 0.05789589881896973
Batch 60/64 loss: 0.0737575888633728
Batch 61/64 loss: 0.060842931270599365
Batch 62/64 loss: 0.07591480016708374
Batch 63/64 loss: 0.06532979011535645
Batch 64/64 loss: 0.06634587049484253
Epoch 183  Train loss: 0.061060715890398215  Val loss: 0.09731303539472758
Epoch 184
-------------------------------
Batch 1/64 loss: 0.07447928190231323
Batch 2/64 loss: 0.05931514501571655
Batch 3/64 loss: 0.057936251163482666
Batch 4/64 loss: 0.0566827654838562
Batch 5/64 loss: 0.06732034683227539
Batch 6/64 loss: 0.057654619216918945
Batch 7/64 loss: 0.0606113076210022
Batch 8/64 loss: 0.061409711837768555
Batch 9/64 loss: 0.057014286518096924
Batch 10/64 loss: 0.05581945180892944
Batch 11/64 loss: 0.045291781425476074
Batch 12/64 loss: 0.05673712491989136
Batch 13/64 loss: 0.06523525714874268
Batch 14/64 loss: 0.06734508275985718
Batch 15/64 loss: 0.05982518196105957
Batch 16/64 loss: 0.060576558113098145
Batch 17/64 loss: 0.05831432342529297
Batch 18/64 loss: 0.0743408203125
Batch 19/64 loss: 0.05086404085159302
Batch 20/64 loss: 0.05115312337875366
Batch 21/64 loss: 0.05075967311859131
Batch 22/64 loss: 0.053329408168792725
Batch 23/64 loss: 0.04746145009994507
Batch 24/64 loss: 0.05425703525543213
Batch 25/64 loss: 0.05704474449157715
Batch 26/64 loss: 0.06445097923278809
Batch 27/64 loss: 0.05896097421646118
Batch 28/64 loss: 0.06287360191345215
Batch 29/64 loss: 0.05623382329940796
Batch 30/64 loss: 0.06782609224319458
Batch 31/64 loss: 0.06803715229034424
Batch 32/64 loss: 0.05217111110687256
Batch 33/64 loss: 0.05665630102157593
Batch 34/64 loss: 0.061054885387420654
Batch 35/64 loss: 0.06304466724395752
Batch 36/64 loss: 0.060634732246398926
Batch 37/64 loss: 0.06763648986816406
Batch 38/64 loss: 0.06368577480316162
Batch 39/64 loss: 0.06265830993652344
Batch 40/64 loss: 0.05576157569885254
Batch 41/64 loss: 0.060978591442108154
Batch 42/64 loss: 0.06885147094726562
Batch 43/64 loss: 0.07208377122879028
Batch 44/64 loss: 0.06196141242980957
Batch 45/64 loss: 0.07108736038208008
Batch 46/64 loss: 0.05871087312698364
Batch 47/64 loss: 0.05286514759063721
Batch 48/64 loss: 0.06697434186935425
Batch 49/64 loss: 0.06167477369308472
Batch 50/64 loss: 0.07681196928024292
Batch 51/64 loss: 0.06281280517578125
Batch 52/64 loss: 0.05205738544464111
Batch 53/64 loss: 0.07379293441772461
Batch 54/64 loss: 0.05885767936706543
Batch 55/64 loss: 0.06020963191986084
Batch 56/64 loss: 0.06142270565032959
Batch 57/64 loss: 0.06420308351516724
Batch 58/64 loss: 0.053472280502319336
Batch 59/64 loss: 0.05990242958068848
Batch 60/64 loss: 0.06483584642410278
Batch 61/64 loss: 0.06535804271697998
Batch 62/64 loss: 0.07662653923034668
Batch 63/64 loss: 0.06061720848083496
Batch 64/64 loss: 0.07123535871505737
Epoch 184  Train loss: 0.06120878504771812  Val loss: 0.09374420524053148
Saving best model, epoch: 184
Epoch 185
-------------------------------
Batch 1/64 loss: 0.05610388517379761
Batch 2/64 loss: 0.06266045570373535
Batch 3/64 loss: 0.07118165493011475
Batch 4/64 loss: 0.07076311111450195
Batch 5/64 loss: 0.07499158382415771
Batch 6/64 loss: 0.07141566276550293
Batch 7/64 loss: 0.06129765510559082
Batch 8/64 loss: 0.06319856643676758
Batch 9/64 loss: 0.07233387231826782
Batch 10/64 loss: 0.06828874349594116
Batch 11/64 loss: 0.05696547031402588
Batch 12/64 loss: 0.06005239486694336
Batch 13/64 loss: 0.06836622953414917
Batch 14/64 loss: 0.056600868701934814
Batch 15/64 loss: 0.05430680513381958
Batch 16/64 loss: 0.05467236042022705
Batch 17/64 loss: 0.05743813514709473
Batch 18/64 loss: 0.06377744674682617
Batch 19/64 loss: 0.052864134311676025
Batch 20/64 loss: 0.06598633527755737
Batch 21/64 loss: 0.0652514100074768
Batch 22/64 loss: 0.05564677715301514
Batch 23/64 loss: 0.052420854568481445
Batch 24/64 loss: 0.04879885911941528
Batch 25/64 loss: 0.05577331781387329
Batch 26/64 loss: 0.06426888704299927
Batch 27/64 loss: 0.062156081199645996
Batch 28/64 loss: 0.048835039138793945
Batch 29/64 loss: 0.05618470907211304
Batch 30/64 loss: 0.06375807523727417
Batch 31/64 loss: 0.05465507507324219
Batch 32/64 loss: 0.0523870587348938
Batch 33/64 loss: 0.057553648948669434
Batch 34/64 loss: 0.04970735311508179
Batch 35/64 loss: 0.050722360610961914
Batch 36/64 loss: 0.04844236373901367
Batch 37/64 loss: 0.06222212314605713
Batch 38/64 loss: 0.04902017116546631
Batch 39/64 loss: 0.08646154403686523
Batch 40/64 loss: 0.06395208835601807
Batch 41/64 loss: 0.06038409471511841
Batch 42/64 loss: 0.0537455677986145
Batch 43/64 loss: 0.05815708637237549
Batch 44/64 loss: 0.05824810266494751
Batch 45/64 loss: 0.07929074764251709
Batch 46/64 loss: 0.06500899791717529
Batch 47/64 loss: 0.05592775344848633
Batch 48/64 loss: 0.05406594276428223
Batch 49/64 loss: 0.05485832691192627
Batch 50/64 loss: 0.0661545991897583
Batch 51/64 loss: 0.06651616096496582
Batch 52/64 loss: 0.062275826930999756
Batch 53/64 loss: 0.05629563331604004
Batch 54/64 loss: 0.05532217025756836
Batch 55/64 loss: 0.06821650266647339
Batch 56/64 loss: 0.06030714511871338
Batch 57/64 loss: 0.056863367557525635
Batch 58/64 loss: 0.05677914619445801
Batch 59/64 loss: 0.062364280223846436
Batch 60/64 loss: 0.054598331451416016
Batch 61/64 loss: 0.06499254703521729
Batch 62/64 loss: 0.058801114559173584
Batch 63/64 loss: 0.0644637942314148
Batch 64/64 loss: 0.06058323383331299
Epoch 185  Train loss: 0.06040090719858805  Val loss: 0.09389049326840955
Epoch 186
-------------------------------
Batch 1/64 loss: 0.06385695934295654
Batch 2/64 loss: 0.05085194110870361
Batch 3/64 loss: 0.06097888946533203
Batch 4/64 loss: 0.04323023557662964
Batch 5/64 loss: 0.06153076887130737
Batch 6/64 loss: 0.07749032974243164
Batch 7/64 loss: 0.06627929210662842
Batch 8/64 loss: 0.07662773132324219
Batch 9/64 loss: 0.06901377439498901
Batch 10/64 loss: 0.05666017532348633
Batch 11/64 loss: 0.06357467174530029
Batch 12/64 loss: 0.05871981382369995
Batch 13/64 loss: 0.06279754638671875
Batch 14/64 loss: 0.04929584264755249
Batch 15/64 loss: 0.04910832643508911
Batch 16/64 loss: 0.05685621500015259
Batch 17/64 loss: 0.06588727235794067
Batch 18/64 loss: 0.052899718284606934
Batch 19/64 loss: 0.05158597230911255
Batch 20/64 loss: 0.0744583010673523
Batch 21/64 loss: 0.05957299470901489
Batch 22/64 loss: 0.06290227174758911
Batch 23/64 loss: 0.04685240983963013
Batch 24/64 loss: 0.06511688232421875
Batch 25/64 loss: 0.062170982360839844
Batch 26/64 loss: 0.07012701034545898
Batch 27/64 loss: 0.05211043357849121
Batch 28/64 loss: 0.07403385639190674
Batch 29/64 loss: 0.05274343490600586
Batch 30/64 loss: 0.06599605083465576
Batch 31/64 loss: 0.06101405620574951
Batch 32/64 loss: 0.06497269868850708
Batch 33/64 loss: 0.0537988543510437
Batch 34/64 loss: 0.056487858295440674
Batch 35/64 loss: 0.05942976474761963
Batch 36/64 loss: 0.053382039070129395
Batch 37/64 loss: 0.05826163291931152
Batch 38/64 loss: 0.06325304508209229
Batch 39/64 loss: 0.04721379280090332
Batch 40/64 loss: 0.06326836347579956
Batch 41/64 loss: 0.06038999557495117
Batch 42/64 loss: 0.05580949783325195
Batch 43/64 loss: 0.05285441875457764
Batch 44/64 loss: 0.0605587363243103
Batch 45/64 loss: 0.05843663215637207
Batch 46/64 loss: 0.059962332248687744
Batch 47/64 loss: 0.05639314651489258
Batch 48/64 loss: 0.061416804790496826
Batch 49/64 loss: 0.05755019187927246
Batch 50/64 loss: 0.06081831455230713
Batch 51/64 loss: 0.06438720226287842
Batch 52/64 loss: 0.07093364000320435
Batch 53/64 loss: 0.06513214111328125
Batch 54/64 loss: 0.06035435199737549
Batch 55/64 loss: 0.06903517246246338
Batch 56/64 loss: 0.06465351581573486
Batch 57/64 loss: 0.0611535906791687
Batch 58/64 loss: 0.05935871601104736
Batch 59/64 loss: 0.06624287366867065
Batch 60/64 loss: 0.058404743671417236
Batch 61/64 loss: 0.06890606880187988
Batch 62/64 loss: 0.06022769212722778
Batch 63/64 loss: 0.078701913356781
Batch 64/64 loss: 0.05532187223434448
Epoch 186  Train loss: 0.06082486755707685  Val loss: 0.09510205660489007
Epoch 187
-------------------------------
Batch 1/64 loss: 0.05548018217086792
Batch 2/64 loss: 0.05077844858169556
Batch 3/64 loss: 0.05724579095840454
Batch 4/64 loss: 0.04533308744430542
Batch 5/64 loss: 0.06246495246887207
Batch 6/64 loss: 0.05683833360671997
Batch 7/64 loss: 0.08224856853485107
Batch 8/64 loss: 0.07145130634307861
Batch 9/64 loss: 0.048363685607910156
Batch 10/64 loss: 0.05075216293334961
Batch 11/64 loss: 0.054334044456481934
Batch 12/64 loss: 0.053398847579956055
Batch 13/64 loss: 0.0574989914894104
Batch 14/64 loss: 0.06576371192932129
Batch 15/64 loss: 0.04593479633331299
Batch 16/64 loss: 0.0530165433883667
Batch 17/64 loss: 0.05059647560119629
Batch 18/64 loss: 0.04898768663406372
Batch 19/64 loss: 0.04723292589187622
Batch 20/64 loss: 0.04715919494628906
Batch 21/64 loss: 0.04574006795883179
Batch 22/64 loss: 0.05451405048370361
Batch 23/64 loss: 0.05825930833816528
Batch 24/64 loss: 0.06443029642105103
Batch 25/64 loss: 0.052889883518218994
Batch 26/64 loss: 0.04989194869995117
Batch 27/64 loss: 0.06643694639205933
Batch 28/64 loss: 0.06130427122116089
Batch 29/64 loss: 0.06891417503356934
Batch 30/64 loss: 0.06457769870758057
Batch 31/64 loss: 0.05545663833618164
Batch 32/64 loss: 0.05150175094604492
Batch 33/64 loss: 0.06628751754760742
Batch 34/64 loss: 0.062660813331604
Batch 35/64 loss: 0.05876600742340088
Batch 36/64 loss: 0.0713508129119873
Batch 37/64 loss: 0.0448189377784729
Batch 38/64 loss: 0.07409530878067017
Batch 39/64 loss: 0.05710911750793457
Batch 40/64 loss: 0.043427884578704834
Batch 41/64 loss: 0.0667610764503479
Batch 42/64 loss: 0.048006534576416016
Batch 43/64 loss: 0.06574463844299316
Batch 44/64 loss: 0.059369683265686035
Batch 45/64 loss: 0.06151610612869263
Batch 46/64 loss: 0.05749499797821045
Batch 47/64 loss: 0.05479693412780762
Batch 48/64 loss: 0.06250119209289551
Batch 49/64 loss: 0.05438810586929321
Batch 50/64 loss: 0.06321090459823608
Batch 51/64 loss: 0.06074810028076172
Batch 52/64 loss: 0.05014848709106445
Batch 53/64 loss: 0.06909006834030151
Batch 54/64 loss: 0.05639773607254028
Batch 55/64 loss: 0.05491495132446289
Batch 56/64 loss: 0.06029558181762695
Batch 57/64 loss: 0.04651820659637451
Batch 58/64 loss: 0.06408870220184326
Batch 59/64 loss: 0.06931978464126587
Batch 60/64 loss: 0.07092994451522827
Batch 61/64 loss: 0.06975889205932617
Batch 62/64 loss: 0.07047861814498901
Batch 63/64 loss: 0.05871027708053589
Batch 64/64 loss: 0.06099206209182739
Epoch 187  Train loss: 0.05832543910718432  Val loss: 0.09369211733546044
Saving best model, epoch: 187
Epoch 188
-------------------------------
Batch 1/64 loss: 0.05262547731399536
Batch 2/64 loss: 0.05686765909194946
Batch 3/64 loss: 0.04262334108352661
Batch 4/64 loss: 0.06615769863128662
Batch 5/64 loss: 0.05612528324127197
Batch 6/64 loss: 0.06797224283218384
Batch 7/64 loss: 0.05894213914871216
Batch 8/64 loss: 0.05289435386657715
Batch 9/64 loss: 0.05216944217681885
Batch 10/64 loss: 0.06581968069076538
Batch 11/64 loss: 0.06948250532150269
Batch 12/64 loss: 0.05856060981750488
Batch 13/64 loss: 0.0647965669631958
Batch 14/64 loss: 0.06781059503555298
Batch 15/64 loss: 0.06084245443344116
Batch 16/64 loss: 0.06639641523361206
Batch 17/64 loss: 0.060730159282684326
Batch 18/64 loss: 0.061894893646240234
Batch 19/64 loss: 0.05403900146484375
Batch 20/64 loss: 0.059004902839660645
Batch 21/64 loss: 0.06000471115112305
Batch 22/64 loss: 0.06542080640792847
Batch 23/64 loss: 0.05613189935684204
Batch 24/64 loss: 0.04930907487869263
Batch 25/64 loss: 0.05397909879684448
Batch 26/64 loss: 0.06036210060119629
Batch 27/64 loss: 0.05459475517272949
Batch 28/64 loss: 0.06486356258392334
Batch 29/64 loss: 0.053679049015045166
Batch 30/64 loss: 0.059256136417388916
Batch 31/64 loss: 0.0693010687828064
Batch 32/64 loss: 0.05996131896972656
Batch 33/64 loss: 0.06315606832504272
Batch 34/64 loss: 0.056584715843200684
Batch 35/64 loss: 0.061109066009521484
Batch 36/64 loss: 0.052222609519958496
Batch 37/64 loss: 0.04538220167160034
Batch 38/64 loss: 0.05326110124588013
Batch 39/64 loss: 0.07086247205734253
Batch 40/64 loss: 0.049627721309661865
Batch 41/64 loss: 0.06015259027481079
Batch 42/64 loss: 0.05955040454864502
Batch 43/64 loss: 0.05898481607437134
Batch 44/64 loss: 0.06694215536117554
Batch 45/64 loss: 0.06580191850662231
Batch 46/64 loss: 0.06347489356994629
Batch 47/64 loss: 0.06984162330627441
Batch 48/64 loss: 0.06839406490325928
Batch 49/64 loss: 0.06488800048828125
Batch 50/64 loss: 0.04656761884689331
Batch 51/64 loss: 0.06995183229446411
Batch 52/64 loss: 0.06690758466720581
Batch 53/64 loss: 0.054410457611083984
Batch 54/64 loss: 0.06346213817596436
Batch 55/64 loss: 0.051274657249450684
Batch 56/64 loss: 0.050766050815582275
Batch 57/64 loss: 0.05752754211425781
Batch 58/64 loss: 0.0653073787689209
Batch 59/64 loss: 0.06102687120437622
Batch 60/64 loss: 0.0688127875328064
Batch 61/64 loss: 0.061163246631622314
Batch 62/64 loss: 0.053201496601104736
Batch 63/64 loss: 0.06742715835571289
Batch 64/64 loss: 0.05146372318267822
Epoch 188  Train loss: 0.059753098674848966  Val loss: 0.09414907788083315
Epoch 189
-------------------------------
Batch 1/64 loss: 0.05405569076538086
Batch 2/64 loss: 0.05482882261276245
Batch 3/64 loss: 0.059492647647857666
Batch 4/64 loss: 0.053709447383880615
Batch 5/64 loss: 0.05598968267440796
Batch 6/64 loss: 0.05081719160079956
Batch 7/64 loss: 0.05921882390975952
Batch 8/64 loss: 0.05139172077178955
Batch 9/64 loss: 0.06425988674163818
Batch 10/64 loss: 0.05681997537612915
Batch 11/64 loss: 0.06482440233230591
Batch 12/64 loss: 0.0625421404838562
Batch 13/64 loss: 0.04745739698410034
Batch 14/64 loss: 0.04758012294769287
Batch 15/64 loss: 0.05856132507324219
Batch 16/64 loss: 0.05111968517303467
Batch 17/64 loss: 0.05139428377151489
Batch 18/64 loss: 0.06174647808074951
Batch 19/64 loss: 0.05138051509857178
Batch 20/64 loss: 0.057164132595062256
Batch 21/64 loss: 0.08202743530273438
Batch 22/64 loss: 0.06296855211257935
Batch 23/64 loss: 0.05115509033203125
Batch 24/64 loss: 0.06738936901092529
Batch 25/64 loss: 0.05266070365905762
Batch 26/64 loss: 0.06603699922561646
Batch 27/64 loss: 0.060679078102111816
Batch 28/64 loss: 0.054448723793029785
Batch 29/64 loss: 0.06194615364074707
Batch 30/64 loss: 0.0665665864944458
Batch 31/64 loss: 0.05151474475860596
Batch 32/64 loss: 0.05379629135131836
Batch 33/64 loss: 0.05784153938293457
Batch 34/64 loss: 0.06515651941299438
Batch 35/64 loss: 0.061376750469207764
Batch 36/64 loss: 0.048354506492614746
Batch 37/64 loss: 0.05585843324661255
Batch 38/64 loss: 0.060141026973724365
Batch 39/64 loss: 0.046435654163360596
Batch 40/64 loss: 0.06759834289550781
Batch 41/64 loss: 0.06983375549316406
Batch 42/64 loss: 0.05838567018508911
Batch 43/64 loss: 0.04683572053909302
Batch 44/64 loss: 0.052336156368255615
Batch 45/64 loss: 0.057856082916259766
Batch 46/64 loss: 0.05819666385650635
Batch 47/64 loss: 0.05426985025405884
Batch 48/64 loss: 0.06059664487838745
Batch 49/64 loss: 0.057486534118652344
Batch 50/64 loss: 0.057331085205078125
Batch 51/64 loss: 0.05622220039367676
Batch 52/64 loss: 0.06340181827545166
Batch 53/64 loss: 0.0435483455657959
Batch 54/64 loss: 0.052972257137298584
Batch 55/64 loss: 0.05601125955581665
Batch 56/64 loss: 0.04623734951019287
Batch 57/64 loss: 0.058946430683135986
Batch 58/64 loss: 0.0574306845664978
Batch 59/64 loss: 0.054935574531555176
Batch 60/64 loss: 0.05754572153091431
Batch 61/64 loss: 0.05876755714416504
Batch 62/64 loss: 0.05862671136856079
Batch 63/64 loss: 0.07679438591003418
Batch 64/64 loss: 0.0672106146812439
Epoch 189  Train loss: 0.057620130099502266  Val loss: 0.09415934360314071
Epoch 190
-------------------------------
Batch 1/64 loss: 0.041680335998535156
Batch 2/64 loss: 0.04676342010498047
Batch 3/64 loss: 0.0500224232673645
Batch 4/64 loss: 0.05293375253677368
Batch 5/64 loss: 0.058823466300964355
Batch 6/64 loss: 0.060882568359375
Batch 7/64 loss: 0.05769491195678711
Batch 8/64 loss: 0.04977375268936157
Batch 9/64 loss: 0.05078178644180298
Batch 10/64 loss: 0.052604496479034424
Batch 11/64 loss: 0.06091123819351196
Batch 12/64 loss: 0.044133543968200684
Batch 13/64 loss: 0.05631202459335327
Batch 14/64 loss: 0.06366562843322754
Batch 15/64 loss: 0.05965012311935425
Batch 16/64 loss: 0.050493836402893066
Batch 17/64 loss: 0.0694621205329895
Batch 18/64 loss: 0.050890982151031494
Batch 19/64 loss: 0.06689471006393433
Batch 20/64 loss: 0.05510061979293823
Batch 21/64 loss: 0.050728559494018555
Batch 22/64 loss: 0.06525659561157227
Batch 23/64 loss: 0.05698448419570923
Batch 24/64 loss: 0.07382470369338989
Batch 25/64 loss: 0.05368328094482422
Batch 26/64 loss: 0.07473242282867432
Batch 27/64 loss: 0.04380542039871216
Batch 28/64 loss: 0.0596659779548645
Batch 29/64 loss: 0.06231158971786499
Batch 30/64 loss: 0.054456114768981934
Batch 31/64 loss: 0.04849112033843994
Batch 32/64 loss: 0.07103937864303589
Batch 33/64 loss: 0.04894375801086426
Batch 34/64 loss: 0.05443525314331055
Batch 35/64 loss: 0.05402946472167969
Batch 36/64 loss: 0.061743080615997314
Batch 37/64 loss: 0.061854422092437744
Batch 38/64 loss: 0.04815471172332764
Batch 39/64 loss: 0.05957639217376709
Batch 40/64 loss: 0.06368589401245117
Batch 41/64 loss: 0.05846667289733887
Batch 42/64 loss: 0.06852108240127563
Batch 43/64 loss: 0.04820704460144043
Batch 44/64 loss: 0.051991164684295654
Batch 45/64 loss: 0.06114298105239868
Batch 46/64 loss: 0.059117913246154785
Batch 47/64 loss: 0.05441451072692871
Batch 48/64 loss: 0.055914998054504395
Batch 49/64 loss: 0.04884171485900879
Batch 50/64 loss: 0.05504751205444336
Batch 51/64 loss: 0.06196838617324829
Batch 52/64 loss: 0.06261104345321655
Batch 53/64 loss: 0.05532163381576538
Batch 54/64 loss: 0.04964303970336914
Batch 55/64 loss: 0.07047033309936523
Batch 56/64 loss: 0.053672075271606445
Batch 57/64 loss: 0.05396580696105957
Batch 58/64 loss: 0.051533639430999756
Batch 59/64 loss: 0.05618935823440552
Batch 60/64 loss: 0.05328494310379028
Batch 61/64 loss: 0.04947477579116821
Batch 62/64 loss: 0.056039273738861084
Batch 63/64 loss: 0.050086140632629395
Batch 64/64 loss: 0.04620373249053955
Epoch 190  Train loss: 0.05627383858549829  Val loss: 0.09073072774303738
Saving best model, epoch: 190
Epoch 191
-------------------------------
Batch 1/64 loss: 0.056990087032318115
Batch 2/64 loss: 0.05130112171173096
Batch 3/64 loss: 0.052269816398620605
Batch 4/64 loss: 0.047760605812072754
Batch 5/64 loss: 0.045449793338775635
Batch 6/64 loss: 0.06064033508300781
Batch 7/64 loss: 0.04873824119567871
Batch 8/64 loss: 0.06064105033874512
Batch 9/64 loss: 0.05079782009124756
Batch 10/64 loss: 0.06629908084869385
Batch 11/64 loss: 0.04967218637466431
Batch 12/64 loss: 0.0608365535736084
Batch 13/64 loss: 0.06700420379638672
Batch 14/64 loss: 0.0609244704246521
Batch 15/64 loss: 0.04699552059173584
Batch 16/64 loss: 0.05582219362258911
Batch 17/64 loss: 0.06769436597824097
Batch 18/64 loss: 0.04949748516082764
Batch 19/64 loss: 0.051506221294403076
Batch 20/64 loss: 0.056024909019470215
Batch 21/64 loss: 0.05082511901855469
Batch 22/64 loss: 0.0503542423248291
Batch 23/64 loss: 0.05246692895889282
Batch 24/64 loss: 0.04765969514846802
Batch 25/64 loss: 0.05412018299102783
Batch 26/64 loss: 0.053244709968566895
Batch 27/64 loss: 0.05958998203277588
Batch 28/64 loss: 0.05185621976852417
Batch 29/64 loss: 0.05688518285751343
Batch 30/64 loss: 0.06074190139770508
Batch 31/64 loss: 0.06545531749725342
Batch 32/64 loss: 0.061019957065582275
Batch 33/64 loss: 0.08162397146224976
Batch 34/64 loss: 0.05187565088272095
Batch 35/64 loss: 0.04920238256454468
Batch 36/64 loss: 0.06991350650787354
Batch 37/64 loss: 0.056049346923828125
Batch 38/64 loss: 0.06359535455703735
Batch 39/64 loss: 0.05731689929962158
Batch 40/64 loss: 0.05566149950027466
Batch 41/64 loss: 0.05546337366104126
Batch 42/64 loss: 0.0527457594871521
Batch 43/64 loss: 0.048659443855285645
Batch 44/64 loss: 0.057474732398986816
Batch 45/64 loss: 0.060119032859802246
Batch 46/64 loss: 0.04819774627685547
Batch 47/64 loss: 0.0646926760673523
Batch 48/64 loss: 0.0803297758102417
Batch 49/64 loss: 0.04713696241378784
Batch 50/64 loss: 0.0600508451461792
Batch 51/64 loss: 0.05487465858459473
Batch 52/64 loss: 0.051746487617492676
Batch 53/64 loss: 0.06716865301132202
Batch 54/64 loss: 0.058511972427368164
Batch 55/64 loss: 0.04733043909072876
Batch 56/64 loss: 0.0572819709777832
Batch 57/64 loss: 0.05752688646316528
Batch 58/64 loss: 0.075802743434906
Batch 59/64 loss: 0.04877990484237671
Batch 60/64 loss: 0.0626441240310669
Batch 61/64 loss: 0.05431485176086426
Batch 62/64 loss: 0.06531846523284912
Batch 63/64 loss: 0.06329691410064697
Batch 64/64 loss: 0.0541001558303833
Epoch 191  Train loss: 0.05704106115827373  Val loss: 0.09291115679691747
Epoch 192
-------------------------------
Batch 1/64 loss: 0.05308562517166138
Batch 2/64 loss: 0.06098693609237671
Batch 3/64 loss: 0.06110072135925293
Batch 4/64 loss: 0.07146435976028442
Batch 5/64 loss: 0.04637026786804199
Batch 6/64 loss: 0.04172778129577637
Batch 7/64 loss: 0.054785847663879395
Batch 8/64 loss: 0.0609203577041626
Batch 9/64 loss: 0.06763780117034912
Batch 10/64 loss: 0.060401320457458496
Batch 11/64 loss: 0.057879865169525146
Batch 12/64 loss: 0.047437071800231934
Batch 13/64 loss: 0.06809788942337036
Batch 14/64 loss: 0.05926483869552612
Batch 15/64 loss: 0.05971425771713257
Batch 16/64 loss: 0.04473435878753662
Batch 17/64 loss: 0.056646108627319336
Batch 18/64 loss: 0.05015629529953003
Batch 19/64 loss: 0.0675157904624939
Batch 20/64 loss: 0.05194050073623657
Batch 21/64 loss: 0.06542706489562988
Batch 22/64 loss: 0.05613285303115845
Batch 23/64 loss: 0.06891024112701416
Batch 24/64 loss: 0.05957961082458496
Batch 25/64 loss: 0.05250716209411621
Batch 26/64 loss: 0.06238478422164917
Batch 27/64 loss: 0.061726927757263184
Batch 28/64 loss: 0.0491374135017395
Batch 29/64 loss: 0.05059313774108887
Batch 30/64 loss: 0.05432283878326416
Batch 31/64 loss: 0.050927162170410156
Batch 32/64 loss: 0.059658825397491455
Batch 33/64 loss: 0.048626482486724854
Batch 34/64 loss: 0.05898547172546387
Batch 35/64 loss: 0.0617411732673645
Batch 36/64 loss: 0.06363570690155029
Batch 37/64 loss: 0.07934081554412842
Batch 38/64 loss: 0.05424857139587402
Batch 39/64 loss: 0.07163947820663452
Batch 40/64 loss: 0.05665498971939087
Batch 41/64 loss: 0.06141287088394165
Batch 42/64 loss: 0.07418739795684814
Batch 43/64 loss: 0.054509878158569336
Batch 44/64 loss: 0.08156478404998779
Batch 45/64 loss: 0.0720100998878479
Batch 46/64 loss: 0.06665629148483276
Batch 47/64 loss: 0.05519866943359375
Batch 48/64 loss: 0.06649911403656006
Batch 49/64 loss: 0.06617683172225952
Batch 50/64 loss: 0.05203014612197876
Batch 51/64 loss: 0.061049580574035645
Batch 52/64 loss: 0.05945420265197754
Batch 53/64 loss: 0.050902605056762695
Batch 54/64 loss: 0.05732768774032593
Batch 55/64 loss: 0.058409929275512695
Batch 56/64 loss: 0.05223876237869263
Batch 57/64 loss: 0.05447030067443848
Batch 58/64 loss: 0.05870068073272705
Batch 59/64 loss: 0.06398153305053711
Batch 60/64 loss: 0.05423414707183838
Batch 61/64 loss: 0.04682350158691406
Batch 62/64 loss: 0.05227077007293701
Batch 63/64 loss: 0.06343269348144531
Batch 64/64 loss: 0.04812377691268921
Epoch 192  Train loss: 0.05878720026390225  Val loss: 0.09122821453101036
Epoch 193
-------------------------------
Batch 1/64 loss: 0.047391295433044434
Batch 2/64 loss: 0.06625699996948242
Batch 3/64 loss: 0.04612928628921509
Batch 4/64 loss: 0.051798999309539795
Batch 5/64 loss: 0.05539363622665405
Batch 6/64 loss: 0.05576038360595703
Batch 7/64 loss: 0.05980950593948364
Batch 8/64 loss: 0.04855769872665405
Batch 9/64 loss: 0.04341423511505127
Batch 10/64 loss: 0.05971658229827881
Batch 11/64 loss: 0.05593550205230713
Batch 12/64 loss: 0.042194485664367676
Batch 13/64 loss: 0.06218832731246948
Batch 14/64 loss: 0.06511616706848145
Batch 15/64 loss: 0.05094677209854126
Batch 16/64 loss: 0.04957860708236694
Batch 17/64 loss: 0.05186265707015991
Batch 18/64 loss: 0.052678048610687256
Batch 19/64 loss: 0.06602901220321655
Batch 20/64 loss: 0.04984503984451294
Batch 21/64 loss: 0.057552337646484375
Batch 22/64 loss: 0.053130149841308594
Batch 23/64 loss: 0.061102986335754395
Batch 24/64 loss: 0.047992587089538574
Batch 25/64 loss: 0.054544925689697266
Batch 26/64 loss: 0.05174529552459717
Batch 27/64 loss: 0.05012655258178711
Batch 28/64 loss: 0.051156580448150635
Batch 29/64 loss: 0.055348098278045654
Batch 30/64 loss: 0.05231368541717529
Batch 31/64 loss: 0.048337459564208984
Batch 32/64 loss: 0.06078833341598511
Batch 33/64 loss: 0.07032644748687744
Batch 34/64 loss: 0.058716416358947754
Batch 35/64 loss: 0.06172192096710205
Batch 36/64 loss: 0.04909229278564453
Batch 37/64 loss: 0.057560741901397705
Batch 38/64 loss: 0.07365596294403076
Batch 39/64 loss: 0.04763394594192505
Batch 40/64 loss: 0.0546567440032959
Batch 41/64 loss: 0.0648619532585144
Batch 42/64 loss: 0.05275362730026245
Batch 43/64 loss: 0.06432288885116577
Batch 44/64 loss: 0.057841956615448
Batch 45/64 loss: 0.06694066524505615
Batch 46/64 loss: 0.05773794651031494
Batch 47/64 loss: 0.06161981821060181
Batch 48/64 loss: 0.051405131816864014
Batch 49/64 loss: 0.09108293056488037
Batch 50/64 loss: 0.05202442407608032
Batch 51/64 loss: 0.07149368524551392
Batch 52/64 loss: 0.049173593521118164
Batch 53/64 loss: 0.05591237545013428
Batch 54/64 loss: 0.055274367332458496
Batch 55/64 loss: 0.0448535680770874
Batch 56/64 loss: 0.04441630840301514
Batch 57/64 loss: 0.06483471393585205
Batch 58/64 loss: 0.05335676670074463
Batch 59/64 loss: 0.06695389747619629
Batch 60/64 loss: 0.04358917474746704
Batch 61/64 loss: 0.060075998306274414
Batch 62/64 loss: 0.07451045513153076
Batch 63/64 loss: 0.057830810546875
Batch 64/64 loss: 0.05978626012802124
Epoch 193  Train loss: 0.056561779742147406  Val loss: 0.0945911071554492
Epoch 194
-------------------------------
Batch 1/64 loss: 0.04519158601760864
Batch 2/64 loss: 0.049382150173187256
Batch 3/64 loss: 0.04899454116821289
Batch 4/64 loss: 0.056538522243499756
Batch 5/64 loss: 0.0640144944190979
Batch 6/64 loss: 0.059885382652282715
Batch 7/64 loss: 0.04960054159164429
Batch 8/64 loss: 0.05170559883117676
Batch 9/64 loss: 0.06689876317977905
Batch 10/64 loss: 0.053316712379455566
Batch 11/64 loss: 0.05460160970687866
Batch 12/64 loss: 0.05695730447769165
Batch 13/64 loss: 0.04550325870513916
Batch 14/64 loss: 0.04276728630065918
Batch 15/64 loss: 0.05194354057312012
Batch 16/64 loss: 0.04601418972015381
Batch 17/64 loss: 0.044114649295806885
Batch 18/64 loss: 0.06441336870193481
Batch 19/64 loss: 0.05650961399078369
Batch 20/64 loss: 0.04953014850616455
Batch 21/64 loss: 0.04537159204483032
Batch 22/64 loss: 0.06169086694717407
Batch 23/64 loss: 0.04366248846054077
Batch 24/64 loss: 0.06958913803100586
Batch 25/64 loss: 0.03826463222503662
Batch 26/64 loss: 0.0487821102142334
Batch 27/64 loss: 0.058354735374450684
Batch 28/64 loss: 0.04912722110748291
Batch 29/64 loss: 0.07447880506515503
Batch 30/64 loss: 0.06770557165145874
Batch 31/64 loss: 0.055390357971191406
Batch 32/64 loss: 0.05332136154174805
Batch 33/64 loss: 0.05227547883987427
Batch 34/64 loss: 0.06661993265151978
Batch 35/64 loss: 0.0508347749710083
Batch 36/64 loss: 0.059819936752319336
Batch 37/64 loss: 0.05067169666290283
Batch 38/64 loss: 0.06413930654525757
Batch 39/64 loss: 0.052863359451293945
Batch 40/64 loss: 0.05998861789703369
Batch 41/64 loss: 0.0469738245010376
Batch 42/64 loss: 0.06057393550872803
Batch 43/64 loss: 0.06027662754058838
Batch 44/64 loss: 0.05042058229446411
Batch 45/64 loss: 0.05539679527282715
Batch 46/64 loss: 0.0496252179145813
Batch 47/64 loss: 0.04955852031707764
Batch 48/64 loss: 0.06310296058654785
Batch 49/64 loss: 0.06162130832672119
Batch 50/64 loss: 0.06371235847473145
Batch 51/64 loss: 0.051596760749816895
Batch 52/64 loss: 0.05941808223724365
Batch 53/64 loss: 0.05432373285293579
Batch 54/64 loss: 0.05344736576080322
Batch 55/64 loss: 0.06635379791259766
Batch 56/64 loss: 0.04471480846405029
Batch 57/64 loss: 0.09118527173995972
Batch 58/64 loss: 0.06218165159225464
Batch 59/64 loss: 0.07042586803436279
Batch 60/64 loss: 0.05048412084579468
Batch 61/64 loss: 0.05287212133407593
Batch 62/64 loss: 0.04911518096923828
Batch 63/64 loss: 0.06512600183486938
Batch 64/64 loss: 0.0566861629486084
Epoch 194  Train loss: 0.05577814532261269  Val loss: 0.09259932434435972
Epoch 195
-------------------------------
Batch 1/64 loss: 0.05160391330718994
Batch 2/64 loss: 0.04786413908004761
Batch 3/64 loss: 0.04700356721878052
Batch 4/64 loss: 0.054148197174072266
Batch 5/64 loss: 0.04968452453613281
Batch 6/64 loss: 0.052365779876708984
Batch 7/64 loss: 0.056474149227142334
Batch 8/64 loss: 0.05861341953277588
Batch 9/64 loss: 0.050150275230407715
Batch 10/64 loss: 0.047467589378356934
Batch 11/64 loss: 0.05556541681289673
Batch 12/64 loss: 0.06067115068435669
Batch 13/64 loss: 0.052165210247039795
Batch 14/64 loss: 0.043373823165893555
Batch 15/64 loss: 0.05474042892456055
Batch 16/64 loss: 0.06393826007843018
Batch 17/64 loss: 0.04665696620941162
Batch 18/64 loss: 0.05605578422546387
Batch 19/64 loss: 0.048123955726623535
Batch 20/64 loss: 0.055790066719055176
Batch 21/64 loss: 0.04927128553390503
Batch 22/64 loss: 0.05089879035949707
Batch 23/64 loss: 0.05085533857345581
Batch 24/64 loss: 0.05178821086883545
Batch 25/64 loss: 0.0745084285736084
Batch 26/64 loss: 0.06763535737991333
Batch 27/64 loss: 0.05401003360748291
Batch 28/64 loss: 0.05013835430145264
Batch 29/64 loss: 0.05902290344238281
Batch 30/64 loss: 0.04814869165420532
Batch 31/64 loss: 0.06190788745880127
Batch 32/64 loss: 0.04973524808883667
Batch 33/64 loss: 0.05379980802536011
Batch 34/64 loss: 0.04672199487686157
Batch 35/64 loss: 0.06700789928436279
Batch 36/64 loss: 0.04843473434448242
Batch 37/64 loss: 0.055245935916900635
Batch 38/64 loss: 0.06835687160491943
Batch 39/64 loss: 0.048814594745635986
Batch 40/64 loss: 0.06867599487304688
Batch 41/64 loss: 0.04565328359603882
Batch 42/64 loss: 0.05524259805679321
Batch 43/64 loss: 0.05622142553329468
Batch 44/64 loss: 0.04746520519256592
Batch 45/64 loss: 0.059519052505493164
Batch 46/64 loss: 0.05038273334503174
Batch 47/64 loss: 0.06099843978881836
Batch 48/64 loss: 0.053366780281066895
Batch 49/64 loss: 0.05725198984146118
Batch 50/64 loss: 0.05162173509597778
Batch 51/64 loss: 0.06343269348144531
Batch 52/64 loss: 0.04715585708618164
Batch 53/64 loss: 0.052335500717163086
Batch 54/64 loss: 0.05438774824142456
Batch 55/64 loss: 0.06185537576675415
Batch 56/64 loss: 0.060477614402770996
Batch 57/64 loss: 0.047947466373443604
Batch 58/64 loss: 0.05386686325073242
Batch 59/64 loss: 0.058841049671173096
Batch 60/64 loss: 0.0625881552696228
Batch 61/64 loss: 0.060102105140686035
Batch 62/64 loss: 0.07734280824661255
Batch 63/64 loss: 0.04780000448226929
Batch 64/64 loss: 0.05856812000274658
Epoch 195  Train loss: 0.05501517735275568  Val loss: 0.09196897634525888
Epoch 196
-------------------------------
Batch 1/64 loss: 0.049824059009552
Batch 2/64 loss: 0.045031845569610596
Batch 3/64 loss: 0.05621039867401123
Batch 4/64 loss: 0.04955416917800903
Batch 5/64 loss: 0.048966288566589355
Batch 6/64 loss: 0.05468106269836426
Batch 7/64 loss: 0.062007248401641846
Batch 8/64 loss: 0.05512726306915283
Batch 9/64 loss: 0.05637657642364502
Batch 10/64 loss: 0.05149340629577637
Batch 11/64 loss: 0.06045329570770264
Batch 12/64 loss: 0.048596858978271484
Batch 13/64 loss: 0.04263359308242798
Batch 14/64 loss: 0.0321236252784729
Batch 15/64 loss: 0.05274975299835205
Batch 16/64 loss: 0.036136507987976074
Batch 17/64 loss: 0.06662935018539429
Batch 18/64 loss: 0.055823683738708496
Batch 19/64 loss: 0.04521334171295166
Batch 20/64 loss: 0.05401945114135742
Batch 21/64 loss: 0.05054515600204468
Batch 22/64 loss: 0.047981977462768555
Batch 23/64 loss: 0.05606234073638916
Batch 24/64 loss: 0.05519676208496094
Batch 25/64 loss: 0.043688416481018066
Batch 26/64 loss: 0.05232119560241699
Batch 27/64 loss: 0.06743103265762329
Batch 28/64 loss: 0.05635988712310791
Batch 29/64 loss: 0.07107961177825928
Batch 30/64 loss: 0.06008565425872803
Batch 31/64 loss: 0.04741781949996948
Batch 32/64 loss: 0.05363965034484863
Batch 33/64 loss: 0.06024634838104248
Batch 34/64 loss: 0.04287552833557129
Batch 35/64 loss: 0.04800426959991455
Batch 36/64 loss: 0.05427193641662598
Batch 37/64 loss: 0.06379765272140503
Batch 38/64 loss: 0.05214560031890869
Batch 39/64 loss: 0.05278658866882324
Batch 40/64 loss: 0.06014508008956909
Batch 41/64 loss: 0.04250842332839966
Batch 42/64 loss: 0.06837064027786255
Batch 43/64 loss: 0.06605267524719238
Batch 44/64 loss: 0.04933953285217285
Batch 45/64 loss: 0.04905283451080322
Batch 46/64 loss: 0.058892011642456055
Batch 47/64 loss: 0.04338574409484863
Batch 48/64 loss: 0.06342631578445435
Batch 49/64 loss: 0.06309127807617188
Batch 50/64 loss: 0.06737321615219116
Batch 51/64 loss: 0.056002914905548096
Batch 52/64 loss: 0.05768394470214844
Batch 53/64 loss: 0.06931114196777344
Batch 54/64 loss: 0.07953357696533203
Batch 55/64 loss: 0.06355452537536621
Batch 56/64 loss: 0.047733962535858154
Batch 57/64 loss: 0.06042593717575073
Batch 58/64 loss: 0.05479156970977783
Batch 59/64 loss: 0.05836474895477295
Batch 60/64 loss: 0.06403636932373047
Batch 61/64 loss: 0.06188708543777466
Batch 62/64 loss: 0.061937570571899414
Batch 63/64 loss: 0.049192607402801514
Batch 64/64 loss: 0.050401508808135986
Epoch 196  Train loss: 0.055113475229225904  Val loss: 0.0920697848002116
Epoch 197
-------------------------------
Batch 1/64 loss: 0.06280851364135742
Batch 2/64 loss: 0.061696767807006836
Batch 3/64 loss: 0.044293761253356934
Batch 4/64 loss: 0.05243968963623047
Batch 5/64 loss: 0.038102805614471436
Batch 6/64 loss: 0.055813610553741455
Batch 7/64 loss: 0.054698824882507324
Batch 8/64 loss: 0.04435616731643677
Batch 9/64 loss: 0.051213860511779785
Batch 10/64 loss: 0.04907989501953125
Batch 11/64 loss: 0.04910832643508911
Batch 12/64 loss: 0.04976344108581543
Batch 13/64 loss: 0.05513399839401245
Batch 14/64 loss: 0.042185187339782715
Batch 15/64 loss: 0.05888563394546509
Batch 16/64 loss: 0.038475871086120605
Batch 17/64 loss: 0.04517847299575806
Batch 18/64 loss: 0.04821592569351196
Batch 19/64 loss: 0.05084902048110962
Batch 20/64 loss: 0.047771334648132324
Batch 21/64 loss: 0.04930919408798218
Batch 22/64 loss: 0.052000463008880615
Batch 23/64 loss: 0.06563389301300049
Batch 24/64 loss: 0.06752115488052368
Batch 25/64 loss: 0.039153456687927246
Batch 26/64 loss: 0.047552406787872314
Batch 27/64 loss: 0.0499722957611084
Batch 28/64 loss: 0.039231717586517334
Batch 29/64 loss: 0.03933745622634888
Batch 30/64 loss: 0.05620872974395752
Batch 31/64 loss: 0.05191302299499512
Batch 32/64 loss: 0.04633450508117676
Batch 33/64 loss: 0.060768842697143555
Batch 34/64 loss: 0.06271374225616455
Batch 35/64 loss: 0.04819369316101074
Batch 36/64 loss: 0.04923701286315918
Batch 37/64 loss: 0.05057418346405029
Batch 38/64 loss: 0.04693460464477539
Batch 39/64 loss: 0.04855084419250488
Batch 40/64 loss: 0.05393052101135254
Batch 41/64 loss: 0.03937792778015137
Batch 42/64 loss: 0.04320037364959717
Batch 43/64 loss: 0.06813114881515503
Batch 44/64 loss: 0.06672948598861694
Batch 45/64 loss: 0.050489842891693115
Batch 46/64 loss: 0.069283127784729
Batch 47/64 loss: 0.05299800634384155
Batch 48/64 loss: 0.056669414043426514
Batch 49/64 loss: 0.07899826765060425
Batch 50/64 loss: 0.05665856599807739
Batch 51/64 loss: 0.058764517307281494
Batch 52/64 loss: 0.05560290813446045
Batch 53/64 loss: 0.05288255214691162
Batch 54/64 loss: 0.05645143985748291
Batch 55/64 loss: 0.054026901721954346
Batch 56/64 loss: 0.05042058229446411
Batch 57/64 loss: 0.0618513822555542
Batch 58/64 loss: 0.05425989627838135
Batch 59/64 loss: 0.04871648550033569
Batch 60/64 loss: 0.06257027387619019
Batch 61/64 loss: 0.05718636512756348
Batch 62/64 loss: 0.044936537742614746
Batch 63/64 loss: 0.044722020626068115
Batch 64/64 loss: 0.047071754932403564
Epoch 197  Train loss: 0.052476465702056885  Val loss: 0.09490278619261541
Epoch 198
-------------------------------
Batch 1/64 loss: 0.053138911724090576
Batch 2/64 loss: 0.050382375717163086
Batch 3/64 loss: 0.04910576343536377
Batch 4/64 loss: 0.04586005210876465
Batch 5/64 loss: 0.058422207832336426
Batch 6/64 loss: 0.06012803316116333
Batch 7/64 loss: 0.0476040244102478
Batch 8/64 loss: 0.05427199602127075
Batch 9/64 loss: 0.05202591419219971
Batch 10/64 loss: 0.06768053770065308
Batch 11/64 loss: 0.05415356159210205
Batch 12/64 loss: 0.060516536235809326
Batch 13/64 loss: 0.05243748426437378
Batch 14/64 loss: 0.051088035106658936
Batch 15/64 loss: 0.05409431457519531
Batch 16/64 loss: 0.05955696105957031
Batch 17/64 loss: 0.050748348236083984
Batch 18/64 loss: 0.05301821231842041
Batch 19/64 loss: 0.04811203479766846
Batch 20/64 loss: 0.059648752212524414
Batch 21/64 loss: 0.04911583662033081
Batch 22/64 loss: 0.0589141845703125
Batch 23/64 loss: 0.041710078716278076
Batch 24/64 loss: 0.05634117126464844
Batch 25/64 loss: 0.06276267766952515
Batch 26/64 loss: 0.05947554111480713
Batch 27/64 loss: 0.04646706581115723
Batch 28/64 loss: 0.04860496520996094
Batch 29/64 loss: 0.05353653430938721
Batch 30/64 loss: 0.05608713626861572
Batch 31/64 loss: 0.041327595710754395
Batch 32/64 loss: 0.06983321905136108
Batch 33/64 loss: 0.06627964973449707
Batch 34/64 loss: 0.054714202880859375
Batch 35/64 loss: 0.055778682231903076
Batch 36/64 loss: 0.051186978816986084
Batch 37/64 loss: 0.07062888145446777
Batch 38/64 loss: 0.042283594608306885
Batch 39/64 loss: 0.04806715250015259
Batch 40/64 loss: 0.06653648614883423
Batch 41/64 loss: 0.0643848180770874
Batch 42/64 loss: 0.0461348295211792
Batch 43/64 loss: 0.053085923194885254
Batch 44/64 loss: 0.05156177282333374
Batch 45/64 loss: 0.06868427991867065
Batch 46/64 loss: 0.056121766567230225
Batch 47/64 loss: 0.04969954490661621
Batch 48/64 loss: 0.04748409986495972
Batch 49/64 loss: 0.04124808311462402
Batch 50/64 loss: 0.051028668880462646
Batch 51/64 loss: 0.045411527156829834
Batch 52/64 loss: 0.056409358978271484
Batch 53/64 loss: 0.06481438875198364
Batch 54/64 loss: 0.059582650661468506
Batch 55/64 loss: 0.05408906936645508
Batch 56/64 loss: 0.0476914644241333
Batch 57/64 loss: 0.04495137929916382
Batch 58/64 loss: 0.05166977643966675
Batch 59/64 loss: 0.04053151607513428
Batch 60/64 loss: 0.05890333652496338
Batch 61/64 loss: 0.05370461940765381
Batch 62/64 loss: 0.054095447063446045
Batch 63/64 loss: 0.058119893074035645
Batch 64/64 loss: 0.04725527763366699
Epoch 198  Train loss: 0.05390580962685978  Val loss: 0.09170561844540626
Epoch 199
-------------------------------
Batch 1/64 loss: 0.049414753913879395
Batch 2/64 loss: 0.061129212379455566
Batch 3/64 loss: 0.03610336780548096
Batch 4/64 loss: 0.048773229122161865
Batch 5/64 loss: 0.03285020589828491
Batch 6/64 loss: 0.052370309829711914
Batch 7/64 loss: 0.06228208541870117
Batch 8/64 loss: 0.05553710460662842
Batch 9/64 loss: 0.05223584175109863
Batch 10/64 loss: 0.04323291778564453
Batch 11/64 loss: 0.05488872528076172
Batch 12/64 loss: 0.04388153553009033
Batch 13/64 loss: 0.03765195608139038
Batch 14/64 loss: 0.059741854667663574
Batch 15/64 loss: 0.049714863300323486
Batch 16/64 loss: 0.07281452417373657
Batch 17/64 loss: 0.042171597480773926
Batch 18/64 loss: 0.05699044466018677
Batch 19/64 loss: 0.06638246774673462
Batch 20/64 loss: 0.04684668779373169
Batch 21/64 loss: 0.047925710678100586
Batch 22/64 loss: 0.04351240396499634
Batch 23/64 loss: 0.061627984046936035
Batch 24/64 loss: 0.049778103828430176
Batch 25/64 loss: 0.06002342700958252
Batch 26/64 loss: 0.05425029993057251
Batch 27/64 loss: 0.06173717975616455
Batch 28/64 loss: 0.05684942007064819
Batch 29/64 loss: 0.0543365478515625
Batch 30/64 loss: 0.053161442279815674
Batch 31/64 loss: 0.05028039216995239
Batch 32/64 loss: 0.05579650402069092
Batch 33/64 loss: 0.04840201139450073
Batch 34/64 loss: 0.0506553053855896
Batch 35/64 loss: 0.04361456632614136
Batch 36/64 loss: 0.053506672382354736
Batch 37/64 loss: 0.0564311146736145
Batch 38/64 loss: 0.07434946298599243
Batch 39/64 loss: 0.057230472564697266
Batch 40/64 loss: 0.06058847904205322
Batch 41/64 loss: 0.050492942333221436
Batch 42/64 loss: 0.04432415962219238
Batch 43/64 loss: 0.04880046844482422
Batch 44/64 loss: 0.05641680955886841
Batch 45/64 loss: 0.057947397232055664
Batch 46/64 loss: 0.045630812644958496
Batch 47/64 loss: 0.04038912057876587
Batch 48/64 loss: 0.04443824291229248
Batch 49/64 loss: 0.04890739917755127
Batch 50/64 loss: 0.05737251043319702
Batch 51/64 loss: 0.053869009017944336
Batch 52/64 loss: 0.04816406965255737
Batch 53/64 loss: 0.06534707546234131
Batch 54/64 loss: 0.05295675992965698
Batch 55/64 loss: 0.04661160707473755
Batch 56/64 loss: 0.06735217571258545
Batch 57/64 loss: 0.04734694957733154
Batch 58/64 loss: 0.055709004402160645
Batch 59/64 loss: 0.05929243564605713
Batch 60/64 loss: 0.04136252403259277
Batch 61/64 loss: 0.05017596483230591
Batch 62/64 loss: 0.04646468162536621
Batch 63/64 loss: 0.04340904951095581
Batch 64/64 loss: 0.0657721757888794
Epoch 199  Train loss: 0.05241068998972575  Val loss: 0.09067586873405169
Saving best model, epoch: 199
Epoch 200
-------------------------------
Batch 1/64 loss: 0.05003774166107178
Batch 2/64 loss: 0.048495709896087646
Batch 3/64 loss: 0.06094050407409668
Batch 4/64 loss: 0.055749714374542236
Batch 5/64 loss: 0.03894972801208496
Batch 6/64 loss: 0.04836165904998779
Batch 7/64 loss: 0.042432963848114014
Batch 8/64 loss: 0.05653727054595947
Batch 9/64 loss: 0.055187225341796875
Batch 10/64 loss: 0.05464744567871094
Batch 11/64 loss: 0.04756706953048706
Batch 12/64 loss: 0.060089290142059326
Batch 13/64 loss: 0.0598410964012146
Batch 14/64 loss: 0.047288715839385986
Batch 15/64 loss: 0.05299663543701172
Batch 16/64 loss: 0.04723989963531494
Batch 17/64 loss: 0.048127830028533936
Batch 18/64 loss: 0.04646831750869751
Batch 19/64 loss: 0.050154805183410645
Batch 20/64 loss: 0.06327188014984131
Batch 21/64 loss: 0.06391310691833496
Batch 22/64 loss: 0.051964640617370605
Batch 23/64 loss: 0.06399959325790405
Batch 24/64 loss: 0.04443281888961792
Batch 25/64 loss: 0.045560240745544434
Batch 26/64 loss: 0.04236304759979248
Batch 27/64 loss: 0.05077415704727173
Batch 28/64 loss: 0.046485185623168945
Batch 29/64 loss: 0.05831187963485718
Batch 30/64 loss: 0.05215734243392944
Batch 31/64 loss: 0.03726702928543091
Batch 32/64 loss: 0.05063652992248535
Batch 33/64 loss: 0.05697816610336304
Batch 34/64 loss: 0.06575286388397217
Batch 35/64 loss: 0.058491051197052
Batch 36/64 loss: 0.049556732177734375
Batch 37/64 loss: 0.053808391094207764
Batch 38/64 loss: 0.0570986270904541
Batch 39/64 loss: 0.060971856117248535
Batch 40/64 loss: 0.056255996227264404
Batch 41/64 loss: 0.06741142272949219
Batch 42/64 loss: 0.059411585330963135
Batch 43/64 loss: 0.06376546621322632
Batch 44/64 loss: 0.06125366687774658
Batch 45/64 loss: 0.05294227600097656
Batch 46/64 loss: 0.057291269302368164
Batch 47/64 loss: 0.05645936727523804
Batch 48/64 loss: 0.05172121524810791
Batch 49/64 loss: 0.04092133045196533
Batch 50/64 loss: 0.06284213066101074
Batch 51/64 loss: 0.05191230773925781
Batch 52/64 loss: 0.05270189046859741
Batch 53/64 loss: 0.05035662651062012
Batch 54/64 loss: 0.052127838134765625
Batch 55/64 loss: 0.06283372640609741
Batch 56/64 loss: 0.0492938756942749
Batch 57/64 loss: 0.04983454942703247
Batch 58/64 loss: 0.04562801122665405
Batch 59/64 loss: 0.04974019527435303
Batch 60/64 loss: 0.04589277505874634
Batch 61/64 loss: 0.05913519859313965
Batch 62/64 loss: 0.03820931911468506
Batch 63/64 loss: 0.06330299377441406
Batch 64/64 loss: 0.04134088754653931
Epoch 200  Train loss: 0.052975050374573354  Val loss: 0.08978131997216608
Saving best model, epoch: 200
Epoch 201
-------------------------------
Batch 1/64 loss: 0.04892325401306152
Batch 2/64 loss: 0.059653282165527344
Batch 3/64 loss: 0.05773293972015381
Batch 4/64 loss: 0.05895853042602539
Batch 5/64 loss: 0.036000847816467285
Batch 6/64 loss: 0.05001944303512573
Batch 7/64 loss: 0.03276419639587402
Batch 8/64 loss: 0.06544393301010132
Batch 9/64 loss: 0.048220157623291016
Batch 10/64 loss: 0.04609590768814087
Batch 11/64 loss: 0.056883156299591064
Batch 12/64 loss: 0.04408621788024902
Batch 13/64 loss: 0.04757857322692871
Batch 14/64 loss: 0.0403287410736084
Batch 15/64 loss: 0.043022751808166504
Batch 16/64 loss: 0.0359686017036438
Batch 17/64 loss: 0.05037999153137207
Batch 18/64 loss: 0.043646395206451416
Batch 19/64 loss: 0.04791259765625
Batch 20/64 loss: 0.05326986312866211
Batch 21/64 loss: 0.05406773090362549
Batch 22/64 loss: 0.06822872161865234
Batch 23/64 loss: 0.06182432174682617
Batch 24/64 loss: 0.04776853322982788
Batch 25/64 loss: 0.052909255027770996
Batch 26/64 loss: 0.05040872097015381
Batch 27/64 loss: 0.056752920150756836
Batch 28/64 loss: 0.04638713598251343
Batch 29/64 loss: 0.057266950607299805
Batch 30/64 loss: 0.05743443965911865
Batch 31/64 loss: 0.05358123779296875
Batch 32/64 loss: 0.0649004578590393
Batch 33/64 loss: 0.05986583232879639
Batch 34/64 loss: 0.0501483678817749
Batch 35/64 loss: 0.06022578477859497
Batch 36/64 loss: 0.06611168384552002
Batch 37/64 loss: 0.053304433822631836
Batch 38/64 loss: 0.06005018949508667
Batch 39/64 loss: 0.06051677465438843
Batch 40/64 loss: 0.06822574138641357
Batch 41/64 loss: 0.058302879333496094
Batch 42/64 loss: 0.05176419019699097
Batch 43/64 loss: 0.04784989356994629
Batch 44/64 loss: 0.042720913887023926
Batch 45/64 loss: 0.05248367786407471
Batch 46/64 loss: 0.05704587697982788
Batch 47/64 loss: 0.04894912242889404
Batch 48/64 loss: 0.04770022630691528
Batch 49/64 loss: 0.058086514472961426
Batch 50/64 loss: 0.05284321308135986
Batch 51/64 loss: 0.060711026191711426
Batch 52/64 loss: 0.045830488204956055
Batch 53/64 loss: 0.04958629608154297
Batch 54/64 loss: 0.0489497184753418
Batch 55/64 loss: 0.07453036308288574
Batch 56/64 loss: 0.04987823963165283
Batch 57/64 loss: 0.06054067611694336
Batch 58/64 loss: 0.05248606204986572
Batch 59/64 loss: 0.053475379943847656
Batch 60/64 loss: 0.04568582773208618
Batch 61/64 loss: 0.054236412048339844
Batch 62/64 loss: 0.06483018398284912
Batch 63/64 loss: 0.05616199970245361
Batch 64/64 loss: 0.04935801029205322
Epoch 201  Train loss: 0.05315351065467386  Val loss: 0.09119573266235824
Epoch 202
-------------------------------
Batch 1/64 loss: 0.03958064317703247
Batch 2/64 loss: 0.04762530326843262
Batch 3/64 loss: 0.04256939888000488
Batch 4/64 loss: 0.05311846733093262
Batch 5/64 loss: 0.04410606622695923
Batch 6/64 loss: 0.037188708782196045
Batch 7/64 loss: 0.04763597249984741
Batch 8/64 loss: 0.06534057855606079
Batch 9/64 loss: 0.04813408851623535
Batch 10/64 loss: 0.06800949573516846
Batch 11/64 loss: 0.041114091873168945
Batch 12/64 loss: 0.05800366401672363
Batch 13/64 loss: 0.04737573862075806
Batch 14/64 loss: 0.0541919469833374
Batch 15/64 loss: 0.04634535312652588
Batch 16/64 loss: 0.04530423879623413
Batch 17/64 loss: 0.04827374219894409
Batch 18/64 loss: 0.046859681606292725
Batch 19/64 loss: 0.06107276678085327
Batch 20/64 loss: 0.05225473642349243
Batch 21/64 loss: 0.054515302181243896
Batch 22/64 loss: 0.046703040599823
Batch 23/64 loss: 0.048255860805511475
Batch 24/64 loss: 0.0465395450592041
Batch 25/64 loss: 0.04775893688201904
Batch 26/64 loss: 0.0492129921913147
Batch 27/64 loss: 0.06507271528244019
Batch 28/64 loss: 0.0643351674079895
Batch 29/64 loss: 0.060537099838256836
Batch 30/64 loss: 0.06779241561889648
Batch 31/64 loss: 0.0631062388420105
Batch 32/64 loss: 0.05176454782485962
Batch 33/64 loss: 0.05141949653625488
Batch 34/64 loss: 0.06775075197219849
Batch 35/64 loss: 0.04879021644592285
Batch 36/64 loss: 0.07114052772521973
Batch 37/64 loss: 0.060530126094818115
Batch 38/64 loss: 0.04173898696899414
Batch 39/64 loss: 0.0586208701133728
Batch 40/64 loss: 0.05861508846282959
Batch 41/64 loss: 0.05177807807922363
Batch 42/64 loss: 0.05201679468154907
Batch 43/64 loss: 0.049578726291656494
Batch 44/64 loss: 0.05819368362426758
Batch 45/64 loss: 0.06298404932022095
Batch 46/64 loss: 0.0528680682182312
Batch 47/64 loss: 0.052948832511901855
Batch 48/64 loss: 0.0465049147605896
Batch 49/64 loss: 0.061762452125549316
Batch 50/64 loss: 0.04752039909362793
Batch 51/64 loss: 0.04910844564437866
Batch 52/64 loss: 0.05957973003387451
Batch 53/64 loss: 0.06172442436218262
Batch 54/64 loss: 0.05712181329727173
Batch 55/64 loss: 0.06611406803131104
Batch 56/64 loss: 0.0593448281288147
Batch 57/64 loss: 0.03891170024871826
Batch 58/64 loss: 0.04479795694351196
Batch 59/64 loss: 0.0568547248840332
Batch 60/64 loss: 0.05220693349838257
Batch 61/64 loss: 0.05064743757247925
Batch 62/64 loss: 0.052151262760162354
Batch 63/64 loss: 0.06240791082382202
Batch 64/64 loss: 0.04907423257827759
Epoch 202  Train loss: 0.05336843168034273  Val loss: 0.09180526245910277
Epoch 203
-------------------------------
Batch 1/64 loss: 0.0566936731338501
Batch 2/64 loss: 0.06161093711853027
Batch 3/64 loss: 0.037148118019104004
Batch 4/64 loss: 0.05951404571533203
Batch 5/64 loss: 0.04397803544998169
Batch 6/64 loss: 0.058153748512268066
Batch 7/64 loss: 0.045945167541503906
Batch 8/64 loss: 0.0484466552734375
Batch 9/64 loss: 0.05053800344467163
Batch 10/64 loss: 0.05771350860595703
Batch 11/64 loss: 0.05559331178665161
Batch 12/64 loss: 0.050503313541412354
Batch 13/64 loss: 0.04900646209716797
Batch 14/64 loss: 0.04369622468948364
Batch 15/64 loss: 0.0469212532043457
Batch 16/64 loss: 0.0483546257019043
Batch 17/64 loss: 0.04537910223007202
Batch 18/64 loss: 0.04622483253479004
Batch 19/64 loss: 0.0449751615524292
Batch 20/64 loss: 0.06919407844543457
Batch 21/64 loss: 0.04963427782058716
Batch 22/64 loss: 0.05210369825363159
Batch 23/64 loss: 0.05394184589385986
Batch 24/64 loss: 0.048828125
Batch 25/64 loss: 0.05549389123916626
Batch 26/64 loss: 0.04088735580444336
Batch 27/64 loss: 0.05070209503173828
Batch 28/64 loss: 0.046670615673065186
Batch 29/64 loss: 0.03540217876434326
Batch 30/64 loss: 0.04784506559371948
Batch 31/64 loss: 0.053964972496032715
Batch 32/64 loss: 0.04453057050704956
Batch 33/64 loss: 0.05285322666168213
Batch 34/64 loss: 0.042227327823638916
Batch 35/64 loss: 0.05167675018310547
Batch 36/64 loss: 0.04857301712036133
Batch 37/64 loss: 0.04128539562225342
Batch 38/64 loss: 0.06272029876708984
Batch 39/64 loss: 0.0544358491897583
Batch 40/64 loss: 0.03960144519805908
Batch 41/64 loss: 0.045990824699401855
Batch 42/64 loss: 0.05157738924026489
Batch 43/64 loss: 0.055664241313934326
Batch 44/64 loss: 0.049158692359924316
Batch 45/64 loss: 0.04669296741485596
Batch 46/64 loss: 0.056610167026519775
Batch 47/64 loss: 0.0464898943901062
Batch 48/64 loss: 0.05152308940887451
Batch 49/64 loss: 0.04898649454116821
Batch 50/64 loss: 0.048021554946899414
Batch 51/64 loss: 0.04058682918548584
Batch 52/64 loss: 0.04069113731384277
Batch 53/64 loss: 0.04927206039428711
Batch 54/64 loss: 0.06466615200042725
Batch 55/64 loss: 0.06123054027557373
Batch 56/64 loss: 0.04447662830352783
Batch 57/64 loss: 0.052799105644226074
Batch 58/64 loss: 0.047541260719299316
Batch 59/64 loss: 0.0561366081237793
Batch 60/64 loss: 0.04899036884307861
Batch 61/64 loss: 0.04882389307022095
Batch 62/64 loss: 0.07109540700912476
Batch 63/64 loss: 0.06943047046661377
Batch 64/64 loss: 0.05624532699584961
Epoch 203  Train loss: 0.05069189071655274  Val loss: 0.09371850785520888
Epoch 204
-------------------------------
Batch 1/64 loss: 0.0772855281829834
Batch 2/64 loss: 0.04952442646026611
Batch 3/64 loss: 0.032698988914489746
Batch 4/64 loss: 0.0518147349357605
Batch 5/64 loss: 0.034723639488220215
Batch 6/64 loss: 0.05049020051956177
Batch 7/64 loss: 0.06045329570770264
Batch 8/64 loss: 0.049629926681518555
Batch 9/64 loss: 0.036000967025756836
Batch 10/64 loss: 0.06658422946929932
Batch 11/64 loss: 0.04739135503768921
Batch 12/64 loss: 0.04124492406845093
Batch 13/64 loss: 0.044475674629211426
Batch 14/64 loss: 0.047184109687805176
Batch 15/64 loss: 0.049291133880615234
Batch 16/64 loss: 0.04889547824859619
Batch 17/64 loss: 0.05290478467941284
Batch 18/64 loss: 0.03171819448471069
Batch 19/64 loss: 0.04342782497406006
Batch 20/64 loss: 0.0596432089805603
Batch 21/64 loss: 0.05252891778945923
Batch 22/64 loss: 0.05451112985610962
Batch 23/64 loss: 0.03942859172821045
Batch 24/64 loss: 0.03378015756607056
Batch 25/64 loss: 0.0561443567276001
Batch 26/64 loss: 0.06772232055664062
Batch 27/64 loss: 0.04497629404067993
Batch 28/64 loss: 0.04756033420562744
Batch 29/64 loss: 0.03450113534927368
Batch 30/64 loss: 0.049833595752716064
Batch 31/64 loss: 0.041369080543518066
Batch 32/64 loss: 0.05624490976333618
Batch 33/64 loss: 0.04464876651763916
Batch 34/64 loss: 0.05442720651626587
Batch 35/64 loss: 0.05968517065048218
Batch 36/64 loss: 0.049048662185668945
Batch 37/64 loss: 0.045058488845825195
Batch 38/64 loss: 0.061318039894104004
Batch 39/64 loss: 0.04946482181549072
Batch 40/64 loss: 0.05611300468444824
Batch 41/64 loss: 0.06237959861755371
Batch 42/64 loss: 0.051416993141174316
Batch 43/64 loss: 0.04580652713775635
Batch 44/64 loss: 0.04494524002075195
Batch 45/64 loss: 0.04921305179595947
Batch 46/64 loss: 0.04216736555099487
Batch 47/64 loss: 0.04568326473236084
Batch 48/64 loss: 0.05204594135284424
Batch 49/64 loss: 0.04100841283798218
Batch 50/64 loss: 0.04449528455734253
Batch 51/64 loss: 0.05934023857116699
Batch 52/64 loss: 0.052917420864105225
Batch 53/64 loss: 0.04495859146118164
Batch 54/64 loss: 0.058234214782714844
Batch 55/64 loss: 0.05324387550354004
Batch 56/64 loss: 0.05912590026855469
Batch 57/64 loss: 0.05531364679336548
Batch 58/64 loss: 0.05283099412918091
Batch 59/64 loss: 0.041286349296569824
Batch 60/64 loss: 0.0445554256439209
Batch 61/64 loss: 0.06068849563598633
Batch 62/64 loss: 0.059578120708465576
Batch 63/64 loss: 0.04205667972564697
Batch 64/64 loss: 0.04803752899169922
Epoch 204  Train loss: 0.049773606132058536  Val loss: 0.09065576906466402
Epoch 205
-------------------------------
Batch 1/64 loss: 0.04126542806625366
Batch 2/64 loss: 0.06587904691696167
Batch 3/64 loss: 0.04231119155883789
Batch 4/64 loss: 0.04959148168563843
Batch 5/64 loss: 0.035539329051971436
Batch 6/64 loss: 0.04403114318847656
Batch 7/64 loss: 0.03212618827819824
Batch 8/64 loss: 0.04356718063354492
Batch 9/64 loss: 0.04287010431289673
Batch 10/64 loss: 0.04956018924713135
Batch 11/64 loss: 0.05116868019104004
Batch 12/64 loss: 0.04686027765274048
Batch 13/64 loss: 0.04422718286514282
Batch 14/64 loss: 0.04385721683502197
Batch 15/64 loss: 0.05183011293411255
Batch 16/64 loss: 0.035019874572753906
Batch 17/64 loss: 0.0668073296546936
Batch 18/64 loss: 0.05081874132156372
Batch 19/64 loss: 0.04099595546722412
Batch 20/64 loss: 0.052119433879852295
Batch 21/64 loss: 0.04644864797592163
Batch 22/64 loss: 0.049079179763793945
Batch 23/64 loss: 0.05485564470291138
Batch 24/64 loss: 0.047408878803253174
Batch 25/64 loss: 0.047973811626434326
Batch 26/64 loss: 0.04315590858459473
Batch 27/64 loss: 0.06104910373687744
Batch 28/64 loss: 0.03726893663406372
Batch 29/64 loss: 0.042573511600494385
Batch 30/64 loss: 0.05721980333328247
Batch 31/64 loss: 0.06565916538238525
Batch 32/64 loss: 0.046346426010131836
Batch 33/64 loss: 0.053815603256225586
Batch 34/64 loss: 0.04802381992340088
Batch 35/64 loss: 0.04160797595977783
Batch 36/64 loss: 0.04629582166671753
Batch 37/64 loss: 0.038714051246643066
Batch 38/64 loss: 0.04938948154449463
Batch 39/64 loss: 0.042231082916259766
Batch 40/64 loss: 0.05495297908782959
Batch 41/64 loss: 0.05158841609954834
Batch 42/64 loss: 0.047942280769348145
Batch 43/64 loss: 0.06706660985946655
Batch 44/64 loss: 0.038510262966156006
Batch 45/64 loss: 0.038499653339385986
Batch 46/64 loss: 0.04705941677093506
Batch 47/64 loss: 0.056772470474243164
Batch 48/64 loss: 0.052612483501434326
Batch 49/64 loss: 0.04553651809692383
Batch 50/64 loss: 0.049751460552215576
Batch 51/64 loss: 0.0682421326637268
Batch 52/64 loss: 0.04284745454788208
Batch 53/64 loss: 0.051793694496154785
Batch 54/64 loss: 0.06471073627471924
Batch 55/64 loss: 0.06940799951553345
Batch 56/64 loss: 0.04432481527328491
Batch 57/64 loss: 0.048418402671813965
Batch 58/64 loss: 0.03731125593185425
Batch 59/64 loss: 0.03661704063415527
Batch 60/64 loss: 0.06773519515991211
Batch 61/64 loss: 0.056415557861328125
Batch 62/64 loss: 0.05409669876098633
Batch 63/64 loss: 0.05901145935058594
Batch 64/64 loss: 0.04748988151550293
Epoch 205  Train loss: 0.049198515274945426  Val loss: 0.09103053541937235
Epoch 206
-------------------------------
Batch 1/64 loss: 0.04922127723693848
Batch 2/64 loss: 0.04193645715713501
Batch 3/64 loss: 0.04352694749832153
Batch 4/64 loss: 0.05158436298370361
Batch 5/64 loss: 0.03753948211669922
Batch 6/64 loss: 0.04463392496109009
Batch 7/64 loss: 0.05569875240325928
Batch 8/64 loss: 0.05273646116256714
Batch 9/64 loss: 0.051931023597717285
Batch 10/64 loss: 0.049805283546447754
Batch 11/64 loss: 0.04696303606033325
Batch 12/64 loss: 0.04571598768234253
Batch 13/64 loss: 0.04712224006652832
Batch 14/64 loss: 0.047417521476745605
Batch 15/64 loss: 0.0461888313293457
Batch 16/64 loss: 0.04015851020812988
Batch 17/64 loss: 0.05040246248245239
Batch 18/64 loss: 0.051892638206481934
Batch 19/64 loss: 0.052931368350982666
Batch 20/64 loss: 0.04576849937438965
Batch 21/64 loss: 0.050985634326934814
Batch 22/64 loss: 0.04755669832229614
Batch 23/64 loss: 0.058676183223724365
Batch 24/64 loss: 0.04106247425079346
Batch 25/64 loss: 0.04933589696884155
Batch 26/64 loss: 0.040773987770080566
Batch 27/64 loss: 0.054186224937438965
Batch 28/64 loss: 0.04516106843948364
Batch 29/64 loss: 0.05381864309310913
Batch 30/64 loss: 0.04307323694229126
Batch 31/64 loss: 0.04675966501235962
Batch 32/64 loss: 0.04537588357925415
Batch 33/64 loss: 0.03841888904571533
Batch 34/64 loss: 0.05538332462310791
Batch 35/64 loss: 0.040444374084472656
Batch 36/64 loss: 0.05094355344772339
Batch 37/64 loss: 0.04004102945327759
Batch 38/64 loss: 0.040015578269958496
Batch 39/64 loss: 0.04170083999633789
Batch 40/64 loss: 0.053874433040618896
Batch 41/64 loss: 0.044628918170928955
Batch 42/64 loss: 0.06248891353607178
Batch 43/64 loss: 0.0617293119430542
Batch 44/64 loss: 0.05084353685379028
Batch 45/64 loss: 0.055058956146240234
Batch 46/64 loss: 0.05087500810623169
Batch 47/64 loss: 0.04315471649169922
Batch 48/64 loss: 0.056589603424072266
Batch 49/64 loss: 0.042872607707977295
Batch 50/64 loss: 0.042050957679748535
Batch 51/64 loss: 0.05114638805389404
Batch 52/64 loss: 0.049937546253204346
Batch 53/64 loss: 0.04436975717544556
Batch 54/64 loss: 0.03909003734588623
Batch 55/64 loss: 0.051049888134002686
Batch 56/64 loss: 0.04177635908126831
Batch 57/64 loss: 0.05174452066421509
Batch 58/64 loss: 0.052112579345703125
Batch 59/64 loss: 0.04203301668167114
Batch 60/64 loss: 0.038466036319732666
Batch 61/64 loss: 0.05146259069442749
Batch 62/64 loss: 0.05315852165222168
Batch 63/64 loss: 0.044028520584106445
Batch 64/64 loss: 0.039564311504364014
Epoch 206  Train loss: 0.047703595488679175  Val loss: 0.08986899779014981
Epoch 207
-------------------------------
Batch 1/64 loss: 0.03939211368560791
Batch 2/64 loss: 0.053318023681640625
Batch 3/64 loss: 0.040162622928619385
Batch 4/64 loss: 0.04617589712142944
Batch 5/64 loss: 0.04230844974517822
Batch 6/64 loss: 0.051816701889038086
Batch 7/64 loss: 0.04772120714187622
Batch 8/64 loss: 0.05199664831161499
Batch 9/64 loss: 0.05139118432998657
Batch 10/64 loss: 0.049971938133239746
Batch 11/64 loss: 0.06678903102874756
Batch 12/64 loss: 0.05417931079864502
Batch 13/64 loss: 0.0642436146736145
Batch 14/64 loss: 0.057292163372039795
Batch 15/64 loss: 0.05955374240875244
Batch 16/64 loss: 0.05260497331619263
Batch 17/64 loss: 0.05833721160888672
Batch 18/64 loss: 0.03323042392730713
Batch 19/64 loss: 0.0443955659866333
Batch 20/64 loss: 0.0495305061340332
Batch 21/64 loss: 0.051692962646484375
Batch 22/64 loss: 0.039563775062561035
Batch 23/64 loss: 0.05105942487716675
Batch 24/64 loss: 0.041875243186950684
Batch 25/64 loss: 0.04251760244369507
Batch 26/64 loss: 0.05434262752532959
Batch 27/64 loss: 0.05021774768829346
Batch 28/64 loss: 0.04181814193725586
Batch 29/64 loss: 0.050554633140563965
Batch 30/64 loss: 0.05399960279464722
Batch 31/64 loss: 0.03973102569580078
Batch 32/64 loss: 0.05769878625869751
Batch 33/64 loss: 0.03950333595275879
Batch 34/64 loss: 0.05260443687438965
Batch 35/64 loss: 0.04216885566711426
Batch 36/64 loss: 0.061338186264038086
Batch 37/64 loss: 0.044282495975494385
Batch 38/64 loss: 0.06085473299026489
Batch 39/64 loss: 0.05236631631851196
Batch 40/64 loss: 0.04390096664428711
Batch 41/64 loss: 0.058885157108306885
Batch 42/64 loss: 0.046285390853881836
Batch 43/64 loss: 0.04735541343688965
Batch 44/64 loss: 0.04458218812942505
Batch 45/64 loss: 0.040675461292266846
Batch 46/64 loss: 0.03951537609100342
Batch 47/64 loss: 0.058348655700683594
Batch 48/64 loss: 0.04010552167892456
Batch 49/64 loss: 0.05066943168640137
Batch 50/64 loss: 0.05372399091720581
Batch 51/64 loss: 0.04379415512084961
Batch 52/64 loss: 0.060015857219696045
Batch 53/64 loss: 0.05253922939300537
Batch 54/64 loss: 0.05617135763168335
Batch 55/64 loss: 0.051052212715148926
Batch 56/64 loss: 0.05138963460922241
Batch 57/64 loss: 0.05038803815841675
Batch 58/64 loss: 0.046427786350250244
Batch 59/64 loss: 0.050816476345062256
Batch 60/64 loss: 0.043890297412872314
Batch 61/64 loss: 0.03564870357513428
Batch 62/64 loss: 0.03879249095916748
Batch 63/64 loss: 0.07204592227935791
Batch 64/64 loss: 0.043633103370666504
Epoch 207  Train loss: 0.04944856166839599  Val loss: 0.09066857095436542
Epoch 208
-------------------------------
Batch 1/64 loss: 0.033346474170684814
Batch 2/64 loss: 0.048073530197143555
Batch 3/64 loss: 0.049139976501464844
Batch 4/64 loss: 0.05013537406921387
Batch 5/64 loss: 0.042690396308898926
Batch 6/64 loss: 0.038648009300231934
Batch 7/64 loss: 0.04321753978729248
Batch 8/64 loss: 0.05597519874572754
Batch 9/64 loss: 0.06381070613861084
Batch 10/64 loss: 0.03892481327056885
Batch 11/64 loss: 0.04295462369918823
Batch 12/64 loss: 0.05326282978057861
Batch 13/64 loss: 0.054728031158447266
Batch 14/64 loss: 0.03691256046295166
Batch 15/64 loss: 0.0354311466217041
Batch 16/64 loss: 0.05079984664916992
Batch 17/64 loss: 0.04624974727630615
Batch 18/64 loss: 0.03082883358001709
Batch 19/64 loss: 0.05722934007644653
Batch 20/64 loss: 0.04788315296173096
Batch 21/64 loss: 0.042430102825164795
Batch 22/64 loss: 0.05363506078720093
Batch 23/64 loss: 0.047416627407073975
Batch 24/64 loss: 0.043363988399505615
Batch 25/64 loss: 0.05281686782836914
Batch 26/64 loss: 0.0435638427734375
Batch 27/64 loss: 0.04461824893951416
Batch 28/64 loss: 0.0574491024017334
Batch 29/64 loss: 0.05483102798461914
Batch 30/64 loss: 0.04979449510574341
Batch 31/64 loss: 0.040395259857177734
Batch 32/64 loss: 0.05558812618255615
Batch 33/64 loss: 0.06219977140426636
Batch 34/64 loss: 0.04783439636230469
Batch 35/64 loss: 0.04653531312942505
Batch 36/64 loss: 0.044558584690093994
Batch 37/64 loss: 0.053334712982177734
Batch 38/64 loss: 0.039576172828674316
Batch 39/64 loss: 0.04924887418746948
Batch 40/64 loss: 0.05440986156463623
Batch 41/64 loss: 0.049237072467803955
Batch 42/64 loss: 0.05644106864929199
Batch 43/64 loss: 0.03947269916534424
Batch 44/64 loss: 0.050165653228759766
Batch 45/64 loss: 0.052993059158325195
Batch 46/64 loss: 0.039089083671569824
Batch 47/64 loss: 0.04666173458099365
Batch 48/64 loss: 0.05700808763504028
Batch 49/64 loss: 0.04253894090652466
Batch 50/64 loss: 0.05555534362792969
Batch 51/64 loss: 0.06128406524658203
Batch 52/64 loss: 0.053081512451171875
Batch 53/64 loss: 0.05368906259536743
Batch 54/64 loss: 0.03701210021972656
Batch 55/64 loss: 0.04999357461929321
Batch 56/64 loss: 0.05434519052505493
Batch 57/64 loss: 0.05163031816482544
Batch 58/64 loss: 0.056325674057006836
Batch 59/64 loss: 0.04803234338760376
Batch 60/64 loss: 0.06089299917221069
Batch 61/64 loss: 0.05276656150817871
Batch 62/64 loss: 0.056152284145355225
Batch 63/64 loss: 0.03447854518890381
Batch 64/64 loss: 0.04329472780227661
Epoch 208  Train loss: 0.04855153864505244  Val loss: 0.0905342737014351
Epoch 209
-------------------------------
Batch 1/64 loss: 0.0481264591217041
Batch 2/64 loss: 0.05559694766998291
Batch 3/64 loss: 0.04160356521606445
Batch 4/64 loss: 0.03505396842956543
Batch 5/64 loss: 0.03377974033355713
Batch 6/64 loss: 0.04302304983139038
Batch 7/64 loss: 0.047284066677093506
Batch 8/64 loss: 0.035726070404052734
Batch 9/64 loss: 0.057764530181884766
Batch 10/64 loss: 0.05414825677871704
Batch 11/64 loss: 0.034522056579589844
Batch 12/64 loss: 0.039793968200683594
Batch 13/64 loss: 0.03587716817855835
Batch 14/64 loss: 0.04174089431762695
Batch 15/64 loss: 0.041474223136901855
Batch 16/64 loss: 0.04080796241760254
Batch 17/64 loss: 0.035317182540893555
Batch 18/64 loss: 0.027380645275115967
Batch 19/64 loss: 0.06379109621047974
Batch 20/64 loss: 0.05322664976119995
Batch 21/64 loss: 0.03664100170135498
Batch 22/64 loss: 0.04004007577896118
Batch 23/64 loss: 0.06223726272583008
Batch 24/64 loss: 0.06575584411621094
Batch 25/64 loss: 0.042382895946502686
Batch 26/64 loss: 0.04408073425292969
Batch 27/64 loss: 0.05946493148803711
Batch 28/64 loss: 0.04668712615966797
Batch 29/64 loss: 0.05251652002334595
Batch 30/64 loss: 0.049314260482788086
Batch 31/64 loss: 0.048471271991729736
Batch 32/64 loss: 0.05146455764770508
Batch 33/64 loss: 0.05129730701446533
Batch 34/64 loss: 0.045672833919525146
Batch 35/64 loss: 0.038432419300079346
Batch 36/64 loss: 0.04494345188140869
Batch 37/64 loss: 0.04326909780502319
Batch 38/64 loss: 0.03548073768615723
Batch 39/64 loss: 0.056592702865600586
Batch 40/64 loss: 0.03886014223098755
Batch 41/64 loss: 0.0456966757774353
Batch 42/64 loss: 0.046172916889190674
Batch 43/64 loss: 0.05191296339035034
Batch 44/64 loss: 0.04114037752151489
Batch 45/64 loss: 0.03483378887176514
Batch 46/64 loss: 0.05581474304199219
Batch 47/64 loss: 0.05263221263885498
Batch 48/64 loss: 0.04620003700256348
Batch 49/64 loss: 0.06551581621170044
Batch 50/64 loss: 0.051158368587493896
Batch 51/64 loss: 0.040746867656707764
Batch 52/64 loss: 0.0588763952255249
Batch 53/64 loss: 0.056624650955200195
Batch 54/64 loss: 0.0590859055519104
Batch 55/64 loss: 0.0607563853263855
Batch 56/64 loss: 0.07486218214035034
Batch 57/64 loss: 0.04169309139251709
Batch 58/64 loss: 0.058557748794555664
Batch 59/64 loss: 0.05544853210449219
Batch 60/64 loss: 0.06278455257415771
Batch 61/64 loss: 0.047614336013793945
Batch 62/64 loss: 0.0537945032119751
Batch 63/64 loss: 0.06765663623809814
Batch 64/64 loss: 0.049530029296875
Epoch 209  Train loss: 0.048413660947014305  Val loss: 0.09103402075488952
Epoch 210
-------------------------------
Batch 1/64 loss: 0.049655914306640625
Batch 2/64 loss: 0.05122882127761841
Batch 3/64 loss: 0.03537100553512573
Batch 4/64 loss: 0.05343139171600342
Batch 5/64 loss: 0.03986656665802002
Batch 6/64 loss: 0.04461878538131714
Batch 7/64 loss: 0.06184995174407959
Batch 8/64 loss: 0.042534589767456055
Batch 9/64 loss: 0.047708749771118164
Batch 10/64 loss: 0.0374031662940979
Batch 11/64 loss: 0.04153335094451904
Batch 12/64 loss: 0.05596965551376343
Batch 13/64 loss: 0.03621608018875122
Batch 14/64 loss: 0.040663182735443115
Batch 15/64 loss: 0.04065680503845215
Batch 16/64 loss: 0.050630807876586914
Batch 17/64 loss: 0.04285454750061035
Batch 18/64 loss: 0.051602303981781006
Batch 19/64 loss: 0.04533183574676514
Batch 20/64 loss: 0.03551781177520752
Batch 21/64 loss: 0.051665306091308594
Batch 22/64 loss: 0.051475465297698975
Batch 23/64 loss: 0.05855172872543335
Batch 24/64 loss: 0.051219165325164795
Batch 25/64 loss: 0.043413519859313965
Batch 26/64 loss: 0.044710397720336914
Batch 27/64 loss: 0.06649947166442871
Batch 28/64 loss: 0.04778897762298584
Batch 29/64 loss: 0.046125173568725586
Batch 30/64 loss: 0.049343883991241455
Batch 31/64 loss: 0.04542732238769531
Batch 32/64 loss: 0.045345962047576904
Batch 33/64 loss: 0.04586309194564819
Batch 34/64 loss: 0.04243314266204834
Batch 35/64 loss: 0.041607797145843506
Batch 36/64 loss: 0.0473821759223938
Batch 37/64 loss: 0.043139874935150146
Batch 38/64 loss: 0.03835487365722656
Batch 39/64 loss: 0.04698556661605835
Batch 40/64 loss: 0.054636597633361816
Batch 41/64 loss: 0.04642230272293091
Batch 42/64 loss: 0.054564714431762695
Batch 43/64 loss: 0.053431153297424316
Batch 44/64 loss: 0.0487595796585083
Batch 45/64 loss: 0.040592312812805176
Batch 46/64 loss: 0.04666149616241455
Batch 47/64 loss: 0.04984980821609497
Batch 48/64 loss: 0.0513002872467041
Batch 49/64 loss: 0.04771566390991211
Batch 50/64 loss: 0.039856016635894775
Batch 51/64 loss: 0.04137003421783447
Batch 52/64 loss: 0.04252934455871582
Batch 53/64 loss: 0.03770148754119873
Batch 54/64 loss: 0.055336058139801025
Batch 55/64 loss: 0.052886247634887695
Batch 56/64 loss: 0.05285423994064331
Batch 57/64 loss: 0.07274669408798218
Batch 58/64 loss: 0.06249094009399414
Batch 59/64 loss: 0.0682840347290039
Batch 60/64 loss: 0.057930171489715576
Batch 61/64 loss: 0.03973865509033203
Batch 62/64 loss: 0.05856621265411377
Batch 63/64 loss: 0.04001492261886597
Batch 64/64 loss: 0.05073142051696777
Epoch 210  Train loss: 0.04809828646042768  Val loss: 0.09064800346020571
Epoch 211
-------------------------------
Batch 1/64 loss: 0.05053776502609253
Batch 2/64 loss: 0.03506439924240112
Batch 3/64 loss: 0.04721885919570923
Batch 4/64 loss: 0.03278428316116333
Batch 5/64 loss: 0.042868852615356445
Batch 6/64 loss: 0.04578298330307007
Batch 7/64 loss: 0.06176567077636719
Batch 8/64 loss: 0.05139642953872681
Batch 9/64 loss: 0.04188704490661621
Batch 10/64 loss: 0.05219399929046631
Batch 11/64 loss: 0.030344247817993164
Batch 12/64 loss: 0.04449564218521118
Batch 13/64 loss: 0.049855828285217285
Batch 14/64 loss: 0.05016505718231201
Batch 15/64 loss: 0.052860140800476074
Batch 16/64 loss: 0.043522655963897705
Batch 17/64 loss: 0.035024821758270264
Batch 18/64 loss: 0.04641753435134888
Batch 19/64 loss: 0.04298233985900879
Batch 20/64 loss: 0.04774510860443115
Batch 21/64 loss: 0.045809030532836914
Batch 22/64 loss: 0.03456771373748779
Batch 23/64 loss: 0.039837539196014404
Batch 24/64 loss: 0.052490174770355225
Batch 25/64 loss: 0.04916149377822876
Batch 26/64 loss: 0.04783588647842407
Batch 27/64 loss: 0.04771089553833008
Batch 28/64 loss: 0.037897348403930664
Batch 29/64 loss: 0.05833679437637329
Batch 30/64 loss: 0.053031325340270996
Batch 31/64 loss: 0.04320591688156128
Batch 32/64 loss: 0.04010814428329468
Batch 33/64 loss: 0.03694552183151245
Batch 34/64 loss: 0.057285428047180176
Batch 35/64 loss: 0.047447919845581055
Batch 36/64 loss: 0.0501214861869812
Batch 37/64 loss: 0.05010092258453369
Batch 38/64 loss: 0.06909734010696411
Batch 39/64 loss: 0.04805350303649902
Batch 40/64 loss: 0.04529052972793579
Batch 41/64 loss: 0.05673408508300781
Batch 42/64 loss: 0.05136716365814209
Batch 43/64 loss: 0.05497455596923828
Batch 44/64 loss: 0.03877228498458862
Batch 45/64 loss: 0.06840455532073975
Batch 46/64 loss: 0.04979550838470459
Batch 47/64 loss: 0.04573696851730347
Batch 48/64 loss: 0.04413199424743652
Batch 49/64 loss: 0.052068233489990234
Batch 50/64 loss: 0.05345606803894043
Batch 51/64 loss: 0.03947383165359497
Batch 52/64 loss: 0.05173152685165405
Batch 53/64 loss: 0.045530617237091064
Batch 54/64 loss: 0.04538369178771973
Batch 55/64 loss: 0.05834782123565674
Batch 56/64 loss: 0.04060029983520508
Batch 57/64 loss: 0.04363715648651123
Batch 58/64 loss: 0.05288100242614746
Batch 59/64 loss: 0.04764753580093384
Batch 60/64 loss: 0.03540623188018799
Batch 61/64 loss: 0.04313540458679199
Batch 62/64 loss: 0.046327948570251465
Batch 63/64 loss: 0.05805385112762451
Batch 64/64 loss: 0.04098916053771973
Epoch 211  Train loss: 0.0472719809588264  Val loss: 0.09017489927331197
Epoch 212
-------------------------------
Batch 1/64 loss: 0.04040241241455078
Batch 2/64 loss: 0.04410821199417114
Batch 3/64 loss: 0.053585052490234375
Batch 4/64 loss: 0.032766878604888916
Batch 5/64 loss: 0.052636027336120605
Batch 6/64 loss: 0.04290097951889038
Batch 7/64 loss: 0.05621302127838135
Batch 8/64 loss: 0.044329047203063965
Batch 9/64 loss: 0.04751992225646973
Batch 10/64 loss: 0.05162018537521362
Batch 11/64 loss: 0.04619628190994263
Batch 12/64 loss: 0.04800128936767578
Batch 13/64 loss: 0.06313824653625488
Batch 14/64 loss: 0.05144566297531128
Batch 15/64 loss: 0.03694945573806763
Batch 16/64 loss: 0.04694253206253052
Batch 17/64 loss: 0.04455995559692383
Batch 18/64 loss: 0.03480023145675659
Batch 19/64 loss: 0.029101192951202393
Batch 20/64 loss: 0.04283404350280762
Batch 21/64 loss: 0.04479682445526123
Batch 22/64 loss: 0.034245848655700684
Batch 23/64 loss: 0.04187875986099243
Batch 24/64 loss: 0.051290035247802734
Batch 25/64 loss: 0.0446627140045166
Batch 26/64 loss: 0.03463560342788696
Batch 27/64 loss: 0.051738858222961426
Batch 28/64 loss: 0.053457438945770264
Batch 29/64 loss: 0.049707889556884766
Batch 30/64 loss: 0.04237496852874756
Batch 31/64 loss: 0.04690319299697876
Batch 32/64 loss: 0.03736913204193115
Batch 33/64 loss: 0.04294842481613159
Batch 34/64 loss: 0.04347330331802368
Batch 35/64 loss: 0.056185901165008545
Batch 36/64 loss: 0.02949702739715576
Batch 37/64 loss: 0.05037039518356323
Batch 38/64 loss: 0.0429224967956543
Batch 39/64 loss: 0.04049795866012573
Batch 40/64 loss: 0.04944807291030884
Batch 41/64 loss: 0.050934672355651855
Batch 42/64 loss: 0.051788926124572754
Batch 43/64 loss: 0.046125948429107666
Batch 44/64 loss: 0.03752171993255615
Batch 45/64 loss: 0.03993040323257446
Batch 46/64 loss: 0.05040460824966431
Batch 47/64 loss: 0.05239367485046387
Batch 48/64 loss: 0.03846609592437744
Batch 49/64 loss: 0.0524633526802063
Batch 50/64 loss: 0.047224581241607666
Batch 51/64 loss: 0.04168587923049927
Batch 52/64 loss: 0.042015254497528076
Batch 53/64 loss: 0.04630565643310547
Batch 54/64 loss: 0.04658883810043335
Batch 55/64 loss: 0.047065138816833496
Batch 56/64 loss: 0.054315268993377686
Batch 57/64 loss: 0.043914735317230225
Batch 58/64 loss: 0.05101656913757324
Batch 59/64 loss: 0.052647531032562256
Batch 60/64 loss: 0.048935770988464355
Batch 61/64 loss: 0.04183149337768555
Batch 62/64 loss: 0.039985477924346924
Batch 63/64 loss: 0.04345273971557617
Batch 64/64 loss: 0.042617857456207275
Epoch 212  Train loss: 0.04544993265002382  Val loss: 0.09352310744347851
Epoch 213
-------------------------------
Batch 1/64 loss: 0.044504523277282715
Batch 2/64 loss: 0.03922295570373535
Batch 3/64 loss: 0.06923586130142212
Batch 4/64 loss: 0.03345346450805664
Batch 5/64 loss: 0.03729480504989624
Batch 6/64 loss: 0.041480839252471924
Batch 7/64 loss: 0.045544445514678955
Batch 8/64 loss: 0.04764014482498169
Batch 9/64 loss: 0.035723328590393066
Batch 10/64 loss: 0.03414011001586914
Batch 11/64 loss: 0.047622501850128174
Batch 12/64 loss: 0.040856122970581055
Batch 13/64 loss: 0.0525364875793457
Batch 14/64 loss: 0.05102437734603882
Batch 15/64 loss: 0.04644966125488281
Batch 16/64 loss: 0.03497314453125
Batch 17/64 loss: 0.03320854902267456
Batch 18/64 loss: 0.031786203384399414
Batch 19/64 loss: 0.03881573677062988
Batch 20/64 loss: 0.03423357009887695
Batch 21/64 loss: 0.047851383686065674
Batch 22/64 loss: 0.045004427433013916
Batch 23/64 loss: 0.0558774471282959
Batch 24/64 loss: 0.03509491682052612
Batch 25/64 loss: 0.049416303634643555
Batch 26/64 loss: 0.03332000970840454
Batch 27/64 loss: 0.04743236303329468
Batch 28/64 loss: 0.03678750991821289
Batch 29/64 loss: 0.03819894790649414
Batch 30/64 loss: 0.044313907623291016
Batch 31/64 loss: 0.04553210735321045
Batch 32/64 loss: 0.05329751968383789
Batch 33/64 loss: 0.05660438537597656
Batch 34/64 loss: 0.0355832576751709
Batch 35/64 loss: 0.048809587955474854
Batch 36/64 loss: 0.06015676259994507
Batch 37/64 loss: 0.029718220233917236
Batch 38/64 loss: 0.05805391073226929
Batch 39/64 loss: 0.04958522319793701
Batch 40/64 loss: 0.03612548112869263
Batch 41/64 loss: 0.05861163139343262
Batch 42/64 loss: 0.04097980260848999
Batch 43/64 loss: 0.044609248638153076
Batch 44/64 loss: 0.049612343311309814
Batch 45/64 loss: 0.04507458209991455
Batch 46/64 loss: 0.04587233066558838
Batch 47/64 loss: 0.03819692134857178
Batch 48/64 loss: 0.049682438373565674
Batch 49/64 loss: 0.04518324136734009
Batch 50/64 loss: 0.03767818212509155
Batch 51/64 loss: 0.05224192142486572
Batch 52/64 loss: 0.044629693031311035
Batch 53/64 loss: 0.047474801540374756
Batch 54/64 loss: 0.03801548480987549
Batch 55/64 loss: 0.05446577072143555
Batch 56/64 loss: 0.040437400341033936
Batch 57/64 loss: 0.05629372596740723
Batch 58/64 loss: 0.04841107130050659
Batch 59/64 loss: 0.0458681583404541
Batch 60/64 loss: 0.03537952899932861
Batch 61/64 loss: 0.04535704851150513
Batch 62/64 loss: 0.03366398811340332
Batch 63/64 loss: 0.04852515459060669
Batch 64/64 loss: 0.04299408197402954
Epoch 213  Train loss: 0.044314047635770314  Val loss: 0.08951172058524955
Saving best model, epoch: 213
Epoch 214
-------------------------------
Batch 1/64 loss: 0.03295743465423584
Batch 2/64 loss: 0.033552706241607666
Batch 3/64 loss: 0.05239129066467285
Batch 4/64 loss: 0.04638141393661499
Batch 5/64 loss: 0.03715932369232178
Batch 6/64 loss: 0.030947387218475342
Batch 7/64 loss: 0.03473222255706787
Batch 8/64 loss: 0.04129791259765625
Batch 9/64 loss: 0.03585243225097656
Batch 10/64 loss: 0.04165714979171753
Batch 11/64 loss: 0.03826141357421875
Batch 12/64 loss: 0.025763750076293945
Batch 13/64 loss: 0.04542684555053711
Batch 14/64 loss: 0.04111373424530029
Batch 15/64 loss: 0.05573701858520508
Batch 16/64 loss: 0.03653359413146973
Batch 17/64 loss: 0.04903066158294678
Batch 18/64 loss: 0.040142834186553955
Batch 19/64 loss: 0.03832441568374634
Batch 20/64 loss: 0.04811513423919678
Batch 21/64 loss: 0.055175065994262695
Batch 22/64 loss: 0.045577287673950195
Batch 23/64 loss: 0.0490572452545166
Batch 24/64 loss: 0.04773592948913574
Batch 25/64 loss: 0.03763848543167114
Batch 26/64 loss: 0.045429348945617676
Batch 27/64 loss: 0.038396477699279785
Batch 28/64 loss: 0.043984830379486084
Batch 29/64 loss: 0.04576098918914795
Batch 30/64 loss: 0.040207087993621826
Batch 31/64 loss: 0.03950810432434082
Batch 32/64 loss: 0.0403369665145874
Batch 33/64 loss: 0.04680359363555908
Batch 34/64 loss: 0.047962069511413574
Batch 35/64 loss: 0.04686015844345093
Batch 36/64 loss: 0.05044776201248169
Batch 37/64 loss: 0.05511671304702759
Batch 38/64 loss: 0.0554051399230957
Batch 39/64 loss: 0.05409902334213257
Batch 40/64 loss: 0.05402606725692749
Batch 41/64 loss: 0.03687375783920288
Batch 42/64 loss: 0.0595855712890625
Batch 43/64 loss: 0.04075455665588379
Batch 44/64 loss: 0.06236523389816284
Batch 45/64 loss: 0.0515289306640625
Batch 46/64 loss: 0.04707932472229004
Batch 47/64 loss: 0.04280853271484375
Batch 48/64 loss: 0.043609559535980225
Batch 49/64 loss: 0.04453456401824951
Batch 50/64 loss: 0.04619699716567993
Batch 51/64 loss: 0.04424893856048584
Batch 52/64 loss: 0.046021461486816406
Batch 53/64 loss: 0.04791820049285889
Batch 54/64 loss: 0.034723639488220215
Batch 55/64 loss: 0.053005874156951904
Batch 56/64 loss: 0.05228853225708008
Batch 57/64 loss: 0.05092275142669678
Batch 58/64 loss: 0.047429025173187256
Batch 59/64 loss: 0.05754047632217407
Batch 60/64 loss: 0.038825392723083496
Batch 61/64 loss: 0.03696340322494507
Batch 62/64 loss: 0.04413723945617676
Batch 63/64 loss: 0.053070664405822754
Batch 64/64 loss: 0.049738943576812744
Epoch 214  Train loss: 0.044935589911890964  Val loss: 0.0911913655467869
Epoch 215
-------------------------------
Batch 1/64 loss: 0.05034208297729492
Batch 2/64 loss: 0.0545271635055542
Batch 3/64 loss: 0.03961426019668579
Batch 4/64 loss: 0.051665306091308594
Batch 5/64 loss: 0.03802001476287842
Batch 6/64 loss: 0.057047367095947266
Batch 7/64 loss: 0.036321818828582764
Batch 8/64 loss: 0.05817955732345581
Batch 9/64 loss: 0.029372572898864746
Batch 10/64 loss: 0.04646939039230347
Batch 11/64 loss: 0.04059958457946777
Batch 12/64 loss: 0.06112426519393921
Batch 13/64 loss: 0.0409693717956543
Batch 14/64 loss: 0.04294818639755249
Batch 15/64 loss: 0.0350341796875
Batch 16/64 loss: 0.0401233434677124
Batch 17/64 loss: 0.03086555004119873
Batch 18/64 loss: 0.031977176666259766
Batch 19/64 loss: 0.032060444355010986
Batch 20/64 loss: 0.043834686279296875
Batch 21/64 loss: 0.0431370735168457
Batch 22/64 loss: 0.041403114795684814
Batch 23/64 loss: 0.04926353693008423
Batch 24/64 loss: 0.03848564624786377
Batch 25/64 loss: 0.034230172634124756
Batch 26/64 loss: 0.045949339866638184
Batch 27/64 loss: 0.03099304437637329
Batch 28/64 loss: 0.06499487161636353
Batch 29/64 loss: 0.040744781494140625
Batch 30/64 loss: 0.03306162357330322
Batch 31/64 loss: 0.0590587854385376
Batch 32/64 loss: 0.04095590114593506
Batch 33/64 loss: 0.04297429323196411
Batch 34/64 loss: 0.04078686237335205
Batch 35/64 loss: 0.04104208946228027
Batch 36/64 loss: 0.04047673940658569
Batch 37/64 loss: 0.04492759704589844
Batch 38/64 loss: 0.045586466789245605
Batch 39/64 loss: 0.05946880578994751
Batch 40/64 loss: 0.04622209072113037
Batch 41/64 loss: 0.03540283441543579
Batch 42/64 loss: 0.044692039489746094
Batch 43/64 loss: 0.04909563064575195
Batch 44/64 loss: 0.0348319411277771
Batch 45/64 loss: 0.04218566417694092
Batch 46/64 loss: 0.049517035484313965
Batch 47/64 loss: 0.04658859968185425
Batch 48/64 loss: 0.04383265972137451
Batch 49/64 loss: 0.05522489547729492
Batch 50/64 loss: 0.03090459108352661
Batch 51/64 loss: 0.042356014251708984
Batch 52/64 loss: 0.03536790609359741
Batch 53/64 loss: 0.040789902210235596
Batch 54/64 loss: 0.053774118423461914
Batch 55/64 loss: 0.04695558547973633
Batch 56/64 loss: 0.04523003101348877
Batch 57/64 loss: 0.05162346363067627
Batch 58/64 loss: 0.04810541868209839
Batch 59/64 loss: 0.05453455448150635
Batch 60/64 loss: 0.034637391567230225
Batch 61/64 loss: 0.03751569986343384
Batch 62/64 loss: 0.04443073272705078
Batch 63/64 loss: 0.04351276159286499
Batch 64/64 loss: 0.03632080554962158
Epoch 215  Train loss: 0.04365818407021317  Val loss: 0.09142591494465202
Epoch 216
-------------------------------
Batch 1/64 loss: 0.0673668384552002
Batch 2/64 loss: 0.03719520568847656
Batch 3/64 loss: 0.05412393808364868
Batch 4/64 loss: 0.0435214638710022
Batch 5/64 loss: 0.05258345603942871
Batch 6/64 loss: 0.05243539810180664
Batch 7/64 loss: 0.0356869101524353
Batch 8/64 loss: 0.039967238903045654
Batch 9/64 loss: 0.05410146713256836
Batch 10/64 loss: 0.047176241874694824
Batch 11/64 loss: 0.03660249710083008
Batch 12/64 loss: 0.04788327217102051
Batch 13/64 loss: 0.03964763879776001
Batch 14/64 loss: 0.032743990421295166
Batch 15/64 loss: 0.03266596794128418
Batch 16/64 loss: 0.046129584312438965
Batch 17/64 loss: 0.042093873023986816
Batch 18/64 loss: 0.0506935715675354
Batch 19/64 loss: 0.03996860980987549
Batch 20/64 loss: 0.037693917751312256
Batch 21/64 loss: 0.04646766185760498
Batch 22/64 loss: 0.046962738037109375
Batch 23/64 loss: 0.04529529809951782
Batch 24/64 loss: 0.04036968946456909
Batch 25/64 loss: 0.05987745523452759
Batch 26/64 loss: 0.06605309247970581
Batch 27/64 loss: 0.046435654163360596
Batch 28/64 loss: 0.04608345031738281
Batch 29/64 loss: 0.045855045318603516
Batch 30/64 loss: 0.051311373710632324
Batch 31/64 loss: 0.040197670459747314
Batch 32/64 loss: 0.037404000759124756
Batch 33/64 loss: 0.04292714595794678
Batch 34/64 loss: 0.04932427406311035
Batch 35/64 loss: 0.03249680995941162
Batch 36/64 loss: 0.0455775260925293
Batch 37/64 loss: 0.035666823387145996
Batch 38/64 loss: 0.04465121030807495
Batch 39/64 loss: 0.03736239671707153
Batch 40/64 loss: 0.05197376012802124
Batch 41/64 loss: 0.04150688648223877
Batch 42/64 loss: 0.038136065006256104
Batch 43/64 loss: 0.045016705989837646
Batch 44/64 loss: 0.04222238063812256
Batch 45/64 loss: 0.034492671489715576
Batch 46/64 loss: 0.057944297790527344
Batch 47/64 loss: 0.040264010429382324
Batch 48/64 loss: 0.04571402072906494
Batch 49/64 loss: 0.04319202899932861
Batch 50/64 loss: 0.03678393363952637
Batch 51/64 loss: 0.04012703895568848
Batch 52/64 loss: 0.03107309341430664
Batch 53/64 loss: 0.043673157691955566
Batch 54/64 loss: 0.05157172679901123
Batch 55/64 loss: 0.0459897518157959
Batch 56/64 loss: 0.041961848735809326
Batch 57/64 loss: 0.03770577907562256
Batch 58/64 loss: 0.03273409605026245
Batch 59/64 loss: 0.03656882047653198
Batch 60/64 loss: 0.06208443641662598
Batch 61/64 loss: 0.038060665130615234
Batch 62/64 loss: 0.04180872440338135
Batch 63/64 loss: 0.04434305429458618
Batch 64/64 loss: 0.060443222522735596
Epoch 216  Train loss: 0.044249125789193544  Val loss: 0.0874105102008151
Saving best model, epoch: 216
Epoch 217
-------------------------------
Batch 1/64 loss: 0.03783285617828369
Batch 2/64 loss: 0.03302574157714844
Batch 3/64 loss: 0.05693727731704712
Batch 4/64 loss: 0.04505389928817749
Batch 5/64 loss: 0.04885458946228027
Batch 6/64 loss: 0.03387737274169922
Batch 7/64 loss: 0.03738683462142944
Batch 8/64 loss: 0.030164003372192383
Batch 9/64 loss: 0.0342332124710083
Batch 10/64 loss: 0.04969066381454468
Batch 11/64 loss: 0.04361051321029663
Batch 12/64 loss: 0.040830135345458984
Batch 13/64 loss: 0.06013023853302002
Batch 14/64 loss: 0.048782944679260254
Batch 15/64 loss: 0.04301697015762329
Batch 16/64 loss: 0.03998565673828125
Batch 17/64 loss: 0.0493701696395874
Batch 18/64 loss: 0.05531775951385498
Batch 19/64 loss: 0.03438669443130493
Batch 20/64 loss: 0.060378968715667725
Batch 21/64 loss: 0.04019659757614136
Batch 22/64 loss: 0.05007284879684448
Batch 23/64 loss: 0.034251868724823
Batch 24/64 loss: 0.02709507942199707
Batch 25/64 loss: 0.04091775417327881
Batch 26/64 loss: 0.03699624538421631
Batch 27/64 loss: 0.04371070861816406
Batch 28/64 loss: 0.04800701141357422
Batch 29/64 loss: 0.045213162899017334
Batch 30/64 loss: 0.036938607692718506
Batch 31/64 loss: 0.046268463134765625
Batch 32/64 loss: 0.04430687427520752
Batch 33/64 loss: 0.04593491554260254
Batch 34/64 loss: 0.04047131538391113
Batch 35/64 loss: 0.04655963182449341
Batch 36/64 loss: 0.04332810640335083
Batch 37/64 loss: 0.034497857093811035
Batch 38/64 loss: 0.03487586975097656
Batch 39/64 loss: 0.04288297891616821
Batch 40/64 loss: 0.05196845531463623
Batch 41/64 loss: 0.04633033275604248
Batch 42/64 loss: 0.05049610137939453
Batch 43/64 loss: 0.04560369253158569
Batch 44/64 loss: 0.05402487516403198
Batch 45/64 loss: 0.039910972118377686
Batch 46/64 loss: 0.03994393348693848
Batch 47/64 loss: 0.04050016403198242
Batch 48/64 loss: 0.049123167991638184
Batch 49/64 loss: 0.04109776020050049
Batch 50/64 loss: 0.04241842031478882
Batch 51/64 loss: 0.02886795997619629
Batch 52/64 loss: 0.05850476026535034
Batch 53/64 loss: 0.0406339168548584
Batch 54/64 loss: 0.04033172130584717
Batch 55/64 loss: 0.03278648853302002
Batch 56/64 loss: 0.04804104566574097
Batch 57/64 loss: 0.039999425411224365
Batch 58/64 loss: 0.03992122411727905
Batch 59/64 loss: 0.0648757815361023
Batch 60/64 loss: 0.03232389688491821
Batch 61/64 loss: 0.04361116886138916
Batch 62/64 loss: 0.054075658321380615
Batch 63/64 loss: 0.0424501895904541
Batch 64/64 loss: 0.03749358654022217
Epoch 217  Train loss: 0.04331535217808742  Val loss: 0.09145090280939214
Epoch 218
-------------------------------
Batch 1/64 loss: 0.05782055854797363
Batch 2/64 loss: 0.0334397554397583
Batch 3/64 loss: 0.04481983184814453
Batch 4/64 loss: 0.04180663824081421
Batch 5/64 loss: 0.030755281448364258
Batch 6/64 loss: 0.03082895278930664
Batch 7/64 loss: 0.04888182878494263
Batch 8/64 loss: 0.036881208419799805
Batch 9/64 loss: 0.06129711866378784
Batch 10/64 loss: 0.046237826347351074
Batch 11/64 loss: 0.04743778705596924
Batch 12/64 loss: 0.05647820234298706
Batch 13/64 loss: 0.04481780529022217
Batch 14/64 loss: 0.04140937328338623
Batch 15/64 loss: 0.05055958032608032
Batch 16/64 loss: 0.03421872854232788
Batch 17/64 loss: 0.042639732360839844
Batch 18/64 loss: 0.0385708212852478
Batch 19/64 loss: 0.04649454355239868
Batch 20/64 loss: 0.04063349962234497
Batch 21/64 loss: 0.041519880294799805
Batch 22/64 loss: 0.04073166847229004
Batch 23/64 loss: 0.03518635034561157
Batch 24/64 loss: 0.03740137815475464
Batch 25/64 loss: 0.0424344539642334
Batch 26/64 loss: 0.04564189910888672
Batch 27/64 loss: 0.04294389486312866
Batch 28/64 loss: 0.03052312135696411
Batch 29/64 loss: 0.030091524124145508
Batch 30/64 loss: 0.049947381019592285
Batch 31/64 loss: 0.04506182670593262
Batch 32/64 loss: 0.038001298904418945
Batch 33/64 loss: 0.04020506143569946
Batch 34/64 loss: 0.03945314884185791
Batch 35/64 loss: 0.02417808771133423
Batch 36/64 loss: 0.044102609157562256
Batch 37/64 loss: 0.052017152309417725
Batch 38/64 loss: 0.05588865280151367
Batch 39/64 loss: 0.034731507301330566
Batch 40/64 loss: 0.03565400838851929
Batch 41/64 loss: 0.03720712661743164
Batch 42/64 loss: 0.03789550065994263
Batch 43/64 loss: 0.03788340091705322
Batch 44/64 loss: 0.04393279552459717
Batch 45/64 loss: 0.05094349384307861
Batch 46/64 loss: 0.03354811668395996
Batch 47/64 loss: 0.03497689962387085
Batch 48/64 loss: 0.037532925605773926
Batch 49/64 loss: 0.04269295930862427
Batch 50/64 loss: 0.04095488786697388
Batch 51/64 loss: 0.035732150077819824
Batch 52/64 loss: 0.04618889093399048
Batch 53/64 loss: 0.04233521223068237
Batch 54/64 loss: 0.04659658670425415
Batch 55/64 loss: 0.03484398126602173
Batch 56/64 loss: 0.05836951732635498
Batch 57/64 loss: 0.03653609752655029
Batch 58/64 loss: 0.05440104007720947
Batch 59/64 loss: 0.05033844709396362
Batch 60/64 loss: 0.04412829875946045
Batch 61/64 loss: 0.05832850933074951
Batch 62/64 loss: 0.043919384479522705
Batch 63/64 loss: 0.034082233905792236
Batch 64/64 loss: 0.03787064552307129
Epoch 218  Train loss: 0.0422512222738827  Val loss: 0.08973623018494177
Epoch 219
-------------------------------
Batch 1/64 loss: 0.03211551904678345
Batch 2/64 loss: 0.03800523281097412
Batch 3/64 loss: 0.0445217490196228
Batch 4/64 loss: 0.036509931087493896
Batch 5/64 loss: 0.04793518781661987
Batch 6/64 loss: 0.05451643466949463
Batch 7/64 loss: 0.03608578443527222
Batch 8/64 loss: 0.03952443599700928
Batch 9/64 loss: 0.0391465425491333
Batch 10/64 loss: 0.030746817588806152
Batch 11/64 loss: 0.05252861976623535
Batch 12/64 loss: 0.043601155281066895
Batch 13/64 loss: 0.047528862953186035
Batch 14/64 loss: 0.03075587749481201
Batch 15/64 loss: 0.03507411479949951
Batch 16/64 loss: 0.03872883319854736
Batch 17/64 loss: 0.03742307424545288
Batch 18/64 loss: 0.028535425662994385
Batch 19/64 loss: 0.049823641777038574
Batch 20/64 loss: 0.05430710315704346
Batch 21/64 loss: 0.048490703105926514
Batch 22/64 loss: 0.061139822006225586
Batch 23/64 loss: 0.044880032539367676
Batch 24/64 loss: 0.040650248527526855
Batch 25/64 loss: 0.029013395309448242
Batch 26/64 loss: 0.038636744022369385
Batch 27/64 loss: 0.034177541732788086
Batch 28/64 loss: 0.04280215501785278
Batch 29/64 loss: 0.042346179485321045
Batch 30/64 loss: 0.06475687026977539
Batch 31/64 loss: 0.04843324422836304
Batch 32/64 loss: 0.05565190315246582
Batch 33/64 loss: 0.04881101846694946
Batch 34/64 loss: 0.037407517433166504
Batch 35/64 loss: 0.0530015230178833
Batch 36/64 loss: 0.03206276893615723
Batch 37/64 loss: 0.042164623737335205
Batch 38/64 loss: 0.047506511211395264
Batch 39/64 loss: 0.0493849515914917
Batch 40/64 loss: 0.045082032680511475
Batch 41/64 loss: 0.029450595378875732
Batch 42/64 loss: 0.03717374801635742
Batch 43/64 loss: 0.04054909944534302
Batch 44/64 loss: 0.03999757766723633
Batch 45/64 loss: 0.04274719953536987
Batch 46/64 loss: 0.040290117263793945
Batch 47/64 loss: 0.03208357095718384
Batch 48/64 loss: 0.03211599588394165
Batch 49/64 loss: 0.028256654739379883
Batch 50/64 loss: 0.041503310203552246
Batch 51/64 loss: 0.04088097810745239
Batch 52/64 loss: 0.06755495071411133
Batch 53/64 loss: 0.041677236557006836
Batch 54/64 loss: 0.04753834009170532
Batch 55/64 loss: 0.049666643142700195
Batch 56/64 loss: 0.07611393928527832
Batch 57/64 loss: 0.0447084903717041
Batch 58/64 loss: 0.03993344306945801
Batch 59/64 loss: 0.04299294948577881
Batch 60/64 loss: 0.051017045974731445
Batch 61/64 loss: 0.034786105155944824
Batch 62/64 loss: 0.042501866817474365
Batch 63/64 loss: 0.03390765190124512
Batch 64/64 loss: 0.04741138219833374
Epoch 219  Train loss: 0.042930512568529915  Val loss: 0.08742391120936863
Epoch 220
-------------------------------
Batch 1/64 loss: 0.03867650032043457
Batch 2/64 loss: 0.056452929973602295
Batch 3/64 loss: 0.027088403701782227
Batch 4/64 loss: 0.04142880439758301
Batch 5/64 loss: 0.03213620185852051
Batch 6/64 loss: 0.03253382444381714
Batch 7/64 loss: 0.03796476125717163
Batch 8/64 loss: 0.032666683197021484
Batch 9/64 loss: 0.04195296764373779
Batch 10/64 loss: 0.04149818420410156
Batch 11/64 loss: 0.024902939796447754
Batch 12/64 loss: 0.036587655544281006
Batch 13/64 loss: 0.05770021677017212
Batch 14/64 loss: 0.045714735984802246
Batch 15/64 loss: 0.04128652811050415
Batch 16/64 loss: 0.04953598976135254
Batch 17/64 loss: 0.03361409902572632
Batch 18/64 loss: 0.03896516561508179
Batch 19/64 loss: 0.04184609651565552
Batch 20/64 loss: 0.04207080602645874
Batch 21/64 loss: 0.03084540367126465
Batch 22/64 loss: 0.0428166389465332
Batch 23/64 loss: 0.047598421573638916
Batch 24/64 loss: 0.03308439254760742
Batch 25/64 loss: 0.028681695461273193
Batch 26/64 loss: 0.03706389665603638
Batch 27/64 loss: 0.04857468605041504
Batch 28/64 loss: 0.05093348026275635
Batch 29/64 loss: 0.03352928161621094
Batch 30/64 loss: 0.03694075345993042
Batch 31/64 loss: 0.03272181749343872
Batch 32/64 loss: 0.04020202159881592
Batch 33/64 loss: 0.04223799705505371
Batch 34/64 loss: 0.03630650043487549
Batch 35/64 loss: 0.04179346561431885
Batch 36/64 loss: 0.036062538623809814
Batch 37/64 loss: 0.04466569423675537
Batch 38/64 loss: 0.03511953353881836
Batch 39/64 loss: 0.05235171318054199
Batch 40/64 loss: 0.03906404972076416
Batch 41/64 loss: 0.025213658809661865
Batch 42/64 loss: 0.04998910427093506
Batch 43/64 loss: 0.04023897647857666
Batch 44/64 loss: 0.043414413928985596
Batch 45/64 loss: 0.049424171447753906
Batch 46/64 loss: 0.06740164756774902
Batch 47/64 loss: 0.04803478717803955
Batch 48/64 loss: 0.059473633766174316
Batch 49/64 loss: 0.02837657928466797
Batch 50/64 loss: 0.04284048080444336
Batch 51/64 loss: 0.05548501014709473
Batch 52/64 loss: 0.03882598876953125
Batch 53/64 loss: 0.03689461946487427
Batch 54/64 loss: 0.039865732192993164
Batch 55/64 loss: 0.037859320640563965
Batch 56/64 loss: 0.046303510665893555
Batch 57/64 loss: 0.03477776050567627
Batch 58/64 loss: 0.035056114196777344
Batch 59/64 loss: 0.04288601875305176
Batch 60/64 loss: 0.03295588493347168
Batch 61/64 loss: 0.05127960443496704
Batch 62/64 loss: 0.047209858894348145
Batch 63/64 loss: 0.038423776626586914
Batch 64/64 loss: 0.025039196014404297
Epoch 220  Train loss: 0.0407251376731723  Val loss: 0.08912078396151565
Epoch 221
-------------------------------
Batch 1/64 loss: 0.040402114391326904
Batch 2/64 loss: 0.04154479503631592
Batch 3/64 loss: 0.036468565464019775
Batch 4/64 loss: 0.04230690002441406
Batch 5/64 loss: 0.04239392280578613
Batch 6/64 loss: 0.0399591326713562
Batch 7/64 loss: 0.05777573585510254
Batch 8/64 loss: 0.033441007137298584
Batch 9/64 loss: 0.04194742441177368
Batch 10/64 loss: 0.032517433166503906
Batch 11/64 loss: 0.03084707260131836
Batch 12/64 loss: 0.03522205352783203
Batch 13/64 loss: 0.04295581579208374
Batch 14/64 loss: 0.042373478412628174
Batch 15/64 loss: 0.03273344039916992
Batch 16/64 loss: 0.03802359104156494
Batch 17/64 loss: 0.03744620084762573
Batch 18/64 loss: 0.045821547508239746
Batch 19/64 loss: 0.03752034902572632
Batch 20/64 loss: 0.037369370460510254
Batch 21/64 loss: 0.025154471397399902
Batch 22/64 loss: 0.045529305934906006
Batch 23/64 loss: 0.046816229820251465
Batch 24/64 loss: 0.036180317401885986
Batch 25/64 loss: 0.04521834850311279
Batch 26/64 loss: 0.0324329137802124
Batch 27/64 loss: 0.04414635896682739
Batch 28/64 loss: 0.059382081031799316
Batch 29/64 loss: 0.03562331199645996
Batch 30/64 loss: 0.053526341915130615
Batch 31/64 loss: 0.03582143783569336
Batch 32/64 loss: 0.04654419422149658
Batch 33/64 loss: 0.03650927543640137
Batch 34/64 loss: 0.04254335165023804
Batch 35/64 loss: 0.03774058818817139
Batch 36/64 loss: 0.05099517107009888
Batch 37/64 loss: 0.040290236473083496
Batch 38/64 loss: 0.052650272846221924
Batch 39/64 loss: 0.04952394962310791
Batch 40/64 loss: 0.05275970697402954
Batch 41/64 loss: 0.044454753398895264
Batch 42/64 loss: 0.031676650047302246
Batch 43/64 loss: 0.02649635076522827
Batch 44/64 loss: 0.049140334129333496
Batch 45/64 loss: 0.04416477680206299
Batch 46/64 loss: 0.05017411708831787
Batch 47/64 loss: 0.031391918659210205
Batch 48/64 loss: 0.03335803747177124
Batch 49/64 loss: 0.027473390102386475
Batch 50/64 loss: 0.03318941593170166
Batch 51/64 loss: 0.03421604633331299
Batch 52/64 loss: 0.03530013561248779
Batch 53/64 loss: 0.03777909278869629
Batch 54/64 loss: 0.042495906352996826
Batch 55/64 loss: 0.04036760330200195
Batch 56/64 loss: 0.06796455383300781
Batch 57/64 loss: 0.04352813959121704
Batch 58/64 loss: 0.039574265480041504
Batch 59/64 loss: 0.02799403667449951
Batch 60/64 loss: 0.05901777744293213
Batch 61/64 loss: 0.05475050210952759
Batch 62/64 loss: 0.036127448081970215
Batch 63/64 loss: 0.03982746601104736
Batch 64/64 loss: 0.04583919048309326
Epoch 221  Train loss: 0.04114980276893167  Val loss: 0.08993219776251882
Epoch 222
-------------------------------
Batch 1/64 loss: 0.052751779556274414
Batch 2/64 loss: 0.041381239891052246
Batch 3/64 loss: 0.0302659273147583
Batch 4/64 loss: 0.04185587167739868
Batch 5/64 loss: 0.028629839420318604
Batch 6/64 loss: 0.04475224018096924
Batch 7/64 loss: 0.03509879112243652
Batch 8/64 loss: 0.031128764152526855
Batch 9/64 loss: 0.03742349147796631
Batch 10/64 loss: 0.04426848888397217
Batch 11/64 loss: 0.033447265625
Batch 12/64 loss: 0.04729723930358887
Batch 13/64 loss: 0.04004460573196411
Batch 14/64 loss: 0.035737454891204834
Batch 15/64 loss: 0.05084824562072754
Batch 16/64 loss: 0.039052486419677734
Batch 17/64 loss: 0.042716026306152344
Batch 18/64 loss: 0.03942763805389404
Batch 19/64 loss: 0.040440380573272705
Batch 20/64 loss: 0.030977606773376465
Batch 21/64 loss: 0.043349266052246094
Batch 22/64 loss: 0.028118014335632324
Batch 23/64 loss: 0.04110884666442871
Batch 24/64 loss: 0.04567086696624756
Batch 25/64 loss: 0.03920412063598633
Batch 26/64 loss: 0.035481810569763184
Batch 27/64 loss: 0.044465720653533936
Batch 28/64 loss: 0.047702908515930176
Batch 29/64 loss: 0.039983272552490234
Batch 30/64 loss: 0.0397796630859375
Batch 31/64 loss: 0.05320841073989868
Batch 32/64 loss: 0.04080355167388916
Batch 33/64 loss: 0.04113197326660156
Batch 34/64 loss: 0.03632986545562744
Batch 35/64 loss: 0.029415130615234375
Batch 36/64 loss: 0.049335598945617676
Batch 37/64 loss: 0.046332597732543945
Batch 38/64 loss: 0.04271960258483887
Batch 39/64 loss: 0.04028564691543579
Batch 40/64 loss: 0.03305244445800781
Batch 41/64 loss: 0.04006868600845337
Batch 42/64 loss: 0.043636441230773926
Batch 43/64 loss: 0.03669232130050659
Batch 44/64 loss: 0.05283772945404053
Batch 45/64 loss: 0.037214577198028564
Batch 46/64 loss: 0.0576859712600708
Batch 47/64 loss: 0.040891945362091064
Batch 48/64 loss: 0.052197396755218506
Batch 49/64 loss: 0.04078441858291626
Batch 50/64 loss: 0.032277822494506836
Batch 51/64 loss: 0.03334677219390869
Batch 52/64 loss: 0.039849698543548584
Batch 53/64 loss: 0.03259468078613281
Batch 54/64 loss: 0.05196201801300049
Batch 55/64 loss: 0.039339661598205566
Batch 56/64 loss: 0.04407548904418945
Batch 57/64 loss: 0.05184459686279297
Batch 58/64 loss: 0.05658066272735596
Batch 59/64 loss: 0.04191625118255615
Batch 60/64 loss: 0.0357094407081604
Batch 61/64 loss: 0.04139000177383423
Batch 62/64 loss: 0.033625006675720215
Batch 63/64 loss: 0.04439288377761841
Batch 64/64 loss: 0.03454554080963135
Epoch 222  Train loss: 0.040970169796663174  Val loss: 0.09134776506227316
Epoch 223
-------------------------------
Batch 1/64 loss: 0.024702847003936768
Batch 2/64 loss: 0.03506821393966675
Batch 3/64 loss: 0.05044788122177124
Batch 4/64 loss: 0.027169108390808105
Batch 5/64 loss: 0.05123347043991089
Batch 6/64 loss: 0.028748631477355957
Batch 7/64 loss: 0.05133652687072754
Batch 8/64 loss: 0.043023645877838135
Batch 9/64 loss: 0.037195444107055664
Batch 10/64 loss: 0.03501158952713013
Batch 11/64 loss: 0.02770894765853882
Batch 12/64 loss: 0.04048895835876465
Batch 13/64 loss: 0.024448037147521973
Batch 14/64 loss: 0.03121006488800049
Batch 15/64 loss: 0.03396189212799072
Batch 16/64 loss: 0.04502546787261963
Batch 17/64 loss: 0.029351234436035156
Batch 18/64 loss: 0.03607940673828125
Batch 19/64 loss: 0.03295087814331055
Batch 20/64 loss: 0.0381922721862793
Batch 21/64 loss: 0.031000912189483643
Batch 22/64 loss: 0.0380672812461853
Batch 23/64 loss: 0.026405692100524902
Batch 24/64 loss: 0.040625035762786865
Batch 25/64 loss: 0.05131196975708008
Batch 26/64 loss: 0.04082345962524414
Batch 27/64 loss: 0.04948228597640991
Batch 28/64 loss: 0.033042967319488525
Batch 29/64 loss: 0.05136829614639282
Batch 30/64 loss: 0.055912911891937256
Batch 31/64 loss: 0.05170798301696777
Batch 32/64 loss: 0.050200045108795166
Batch 33/64 loss: 0.04503297805786133
Batch 34/64 loss: 0.05710643529891968
Batch 35/64 loss: 0.05755198001861572
Batch 36/64 loss: 0.0522119402885437
Batch 37/64 loss: 0.04383814334869385
Batch 38/64 loss: 0.04577058553695679
Batch 39/64 loss: 0.03713613748550415
Batch 40/64 loss: 0.03661102056503296
Batch 41/64 loss: 0.052229881286621094
Batch 42/64 loss: 0.031528592109680176
Batch 43/64 loss: 0.04145234823226929
Batch 44/64 loss: 0.05001235008239746
Batch 45/64 loss: 0.04876410961151123
Batch 46/64 loss: 0.0302850604057312
Batch 47/64 loss: 0.04662114381790161
Batch 48/64 loss: 0.03832286596298218
Batch 49/64 loss: 0.0354955792427063
Batch 50/64 loss: 0.036177098751068115
Batch 51/64 loss: 0.037800133228302
Batch 52/64 loss: 0.028206825256347656
Batch 53/64 loss: 0.04187697172164917
Batch 54/64 loss: 0.03292441368103027
Batch 55/64 loss: 0.046600282192230225
Batch 56/64 loss: 0.049712300300598145
Batch 57/64 loss: 0.0369839072227478
Batch 58/64 loss: 0.0326610803604126
Batch 59/64 loss: 0.04625064134597778
Batch 60/64 loss: 0.05980873107910156
Batch 61/64 loss: 0.0414198637008667
Batch 62/64 loss: 0.048328518867492676
Batch 63/64 loss: 0.049661874771118164
Batch 64/64 loss: 0.026305735111236572
Epoch 223  Train loss: 0.040806532607359045  Val loss: 0.08892643021554061
Epoch 224
-------------------------------
Batch 1/64 loss: 0.03222435712814331
Batch 2/64 loss: 0.036755383014678955
Batch 3/64 loss: 0.03520900011062622
Batch 4/64 loss: 0.030309021472930908
Batch 5/64 loss: 0.047014594078063965
Batch 6/64 loss: 0.025576293468475342
Batch 7/64 loss: 0.0367700457572937
Batch 8/64 loss: 0.03721433877944946
Batch 9/64 loss: 0.03709721565246582
Batch 10/64 loss: 0.03977543115615845
Batch 11/64 loss: 0.03167736530303955
Batch 12/64 loss: 0.049818336963653564
Batch 13/64 loss: 0.032470643520355225
Batch 14/64 loss: 0.03965258598327637
Batch 15/64 loss: 0.027193665504455566
Batch 16/64 loss: 0.03290325403213501
Batch 17/64 loss: 0.04686868190765381
Batch 18/64 loss: 0.04938960075378418
Batch 19/64 loss: 0.03382313251495361
Batch 20/64 loss: 0.03928864002227783
Batch 21/64 loss: 0.048343658447265625
Batch 22/64 loss: 0.029494881629943848
Batch 23/64 loss: 0.04735475778579712
Batch 24/64 loss: 0.035793960094451904
Batch 25/64 loss: 0.026776134967803955
Batch 26/64 loss: 0.03658360242843628
Batch 27/64 loss: 0.03850525617599487
Batch 28/64 loss: 0.03474503755569458
Batch 29/64 loss: 0.05166900157928467
Batch 30/64 loss: 0.036424458026885986
Batch 31/64 loss: 0.048088669776916504
Batch 32/64 loss: 0.052779436111450195
Batch 33/64 loss: 0.03828418254852295
Batch 34/64 loss: 0.037647128105163574
Batch 35/64 loss: 0.043449461460113525
Batch 36/64 loss: 0.05072951316833496
Batch 37/64 loss: 0.04477643966674805
Batch 38/64 loss: 0.0336642861366272
Batch 39/64 loss: 0.03597736358642578
Batch 40/64 loss: 0.039634525775909424
Batch 41/64 loss: 0.029377400875091553
Batch 42/64 loss: 0.025484025478363037
Batch 43/64 loss: 0.049064695835113525
Batch 44/64 loss: 0.03916281461715698
Batch 45/64 loss: 0.04281771183013916
Batch 46/64 loss: 0.04544186592102051
Batch 47/64 loss: 0.038286566734313965
Batch 48/64 loss: 0.03897923231124878
Batch 49/64 loss: 0.03640472888946533
Batch 50/64 loss: 0.030647754669189453
Batch 51/64 loss: 0.036437928676605225
Batch 52/64 loss: 0.04363924264907837
Batch 53/64 loss: 0.0399327278137207
Batch 54/64 loss: 0.04379051923751831
Batch 55/64 loss: 0.02746051549911499
Batch 56/64 loss: 0.07028800249099731
Batch 57/64 loss: 0.047719478607177734
Batch 58/64 loss: 0.03925997018814087
Batch 59/64 loss: 0.05249190330505371
Batch 60/64 loss: 0.04751443862915039
Batch 61/64 loss: 0.04469031095504761
Batch 62/64 loss: 0.03631889820098877
Batch 63/64 loss: 0.038552284240722656
Batch 64/64 loss: 0.036328673362731934
Epoch 224  Train loss: 0.03957275082083309  Val loss: 0.09282651062273897
Epoch 225
-------------------------------
Batch 1/64 loss: 0.03444713354110718
Batch 2/64 loss: 0.04141741991043091
Batch 3/64 loss: 0.03146761655807495
Batch 4/64 loss: 0.03762692213058472
Batch 5/64 loss: 0.04864394664764404
Batch 6/64 loss: 0.057705700397491455
Batch 7/64 loss: 0.03003871440887451
Batch 8/64 loss: 0.037612318992614746
Batch 9/64 loss: 0.023756086826324463
Batch 10/64 loss: 0.0365220308303833
Batch 11/64 loss: 0.03705179691314697
Batch 12/64 loss: 0.03990119695663452
Batch 13/64 loss: 0.05273246765136719
Batch 14/64 loss: 0.025535881519317627
Batch 15/64 loss: 0.02868729829788208
Batch 16/64 loss: 0.031730055809020996
Batch 17/64 loss: 0.04253089427947998
Batch 18/64 loss: 0.041548967361450195
Batch 19/64 loss: 0.04134202003479004
Batch 20/64 loss: 0.027781009674072266
Batch 21/64 loss: 0.03736835718154907
Batch 22/64 loss: 0.04511106014251709
Batch 23/64 loss: 0.029502689838409424
Batch 24/64 loss: 0.04133099317550659
Batch 25/64 loss: 0.031407713890075684
Batch 26/64 loss: 0.035235464572906494
Batch 27/64 loss: 0.038256824016571045
Batch 28/64 loss: 0.04236173629760742
Batch 29/64 loss: 0.037969887256622314
Batch 30/64 loss: 0.0367586612701416
Batch 31/64 loss: 0.033744990825653076
Batch 32/64 loss: 0.024447381496429443
Batch 33/64 loss: 0.03556090593338013
Batch 34/64 loss: 0.03390371799468994
Batch 35/64 loss: 0.04401141405105591
Batch 36/64 loss: 0.04459327459335327
Batch 37/64 loss: 0.02652263641357422
Batch 38/64 loss: 0.031093358993530273
Batch 39/64 loss: 0.0359264612197876
Batch 40/64 loss: 0.05564844608306885
Batch 41/64 loss: 0.05717575550079346
Batch 42/64 loss: 0.04384821653366089
Batch 43/64 loss: 0.039348483085632324
Batch 44/64 loss: 0.032478392124176025
Batch 45/64 loss: 0.043673574924468994
Batch 46/64 loss: 0.03887301683425903
Batch 47/64 loss: 0.03648197650909424
Batch 48/64 loss: 0.040267348289489746
Batch 49/64 loss: 0.034268081188201904
Batch 50/64 loss: 0.08366018533706665
Batch 51/64 loss: 0.04326748847961426
Batch 52/64 loss: 0.03923523426055908
Batch 53/64 loss: 0.041561245918273926
Batch 54/64 loss: 0.03965187072753906
Batch 55/64 loss: 0.036359548568725586
Batch 56/64 loss: 0.034647583961486816
Batch 57/64 loss: 0.02932596206665039
Batch 58/64 loss: 0.049193739891052246
Batch 59/64 loss: 0.03705573081970215
Batch 60/64 loss: 0.048386216163635254
Batch 61/64 loss: 0.05092507600784302
Batch 62/64 loss: 0.04916954040527344
Batch 63/64 loss: 0.02886056900024414
Batch 64/64 loss: 0.05019080638885498
Epoch 225  Train loss: 0.0392814650255091  Val loss: 0.08936807646374523
Epoch 226
-------------------------------
Batch 1/64 loss: 0.02051544189453125
Batch 2/64 loss: 0.029528558254241943
Batch 3/64 loss: 0.040224552154541016
Batch 4/64 loss: 0.04981487989425659
Batch 5/64 loss: 0.029841184616088867
Batch 6/64 loss: 0.027212142944335938
Batch 7/64 loss: 0.0407329797744751
Batch 8/64 loss: 0.04534667730331421
Batch 9/64 loss: 0.0360146164894104
Batch 10/64 loss: 0.028060853481292725
Batch 11/64 loss: 0.04356682300567627
Batch 12/64 loss: 0.027406036853790283
Batch 13/64 loss: 0.050015270709991455
Batch 14/64 loss: 0.02822089195251465
Batch 15/64 loss: 0.04866218566894531
Batch 16/64 loss: 0.039173245429992676
Batch 17/64 loss: 0.0329703688621521
Batch 18/64 loss: 0.04635578393936157
Batch 19/64 loss: 0.028234124183654785
Batch 20/64 loss: 0.027841031551361084
Batch 21/64 loss: 0.04168093204498291
Batch 22/64 loss: 0.028277993202209473
Batch 23/64 loss: 0.02363598346710205
Batch 24/64 loss: 0.035385727882385254
Batch 25/64 loss: 0.022040724754333496
Batch 26/64 loss: 0.03411835432052612
Batch 27/64 loss: 0.03387492895126343
Batch 28/64 loss: 0.02972233295440674
Batch 29/64 loss: 0.03719949722290039
Batch 30/64 loss: 0.05499064922332764
Batch 31/64 loss: 0.04981827735900879
Batch 32/64 loss: 0.036228835582733154
Batch 33/64 loss: 0.03621751070022583
Batch 34/64 loss: 0.053773462772369385
Batch 35/64 loss: 0.03753077983856201
Batch 36/64 loss: 0.03381139039993286
Batch 37/64 loss: 0.03300577402114868
Batch 38/64 loss: 0.030343234539031982
Batch 39/64 loss: 0.04373687505722046
Batch 40/64 loss: 0.03348541259765625
Batch 41/64 loss: 0.04221057891845703
Batch 42/64 loss: 0.02991652488708496
Batch 43/64 loss: 0.0532492995262146
Batch 44/64 loss: 0.061575889587402344
Batch 45/64 loss: 0.0483630895614624
Batch 46/64 loss: 0.05057007074356079
Batch 47/64 loss: 0.05401641130447388
Batch 48/64 loss: 0.047967374324798584
Batch 49/64 loss: 0.046989619731903076
Batch 50/64 loss: 0.05284523963928223
Batch 51/64 loss: 0.038563072681427
Batch 52/64 loss: 0.045115768909454346
Batch 53/64 loss: 0.04668337106704712
Batch 54/64 loss: 0.04466712474822998
Batch 55/64 loss: 0.032204508781433105
Batch 56/64 loss: 0.036944031715393066
Batch 57/64 loss: 0.04357796907424927
Batch 58/64 loss: 0.04135119915008545
Batch 59/64 loss: 0.04340362548828125
Batch 60/64 loss: 0.03757607936859131
Batch 61/64 loss: 0.03619104623794556
Batch 62/64 loss: 0.04631209373474121
Batch 63/64 loss: 0.051621079444885254
Batch 64/64 loss: 0.044972240924835205
Epoch 226  Train loss: 0.039439381571377025  Val loss: 0.09203964738092062
Epoch 227
-------------------------------
Batch 1/64 loss: 0.03486663103103638
Batch 2/64 loss: 0.051471829414367676
Batch 3/64 loss: 0.037558138370513916
Batch 4/64 loss: 0.04182076454162598
Batch 5/64 loss: 0.050502121448516846
Batch 6/64 loss: 0.024500668048858643
Batch 7/64 loss: 0.03403139114379883
Batch 8/64 loss: 0.02728646993637085
Batch 9/64 loss: 0.03614377975463867
Batch 10/64 loss: 0.026774168014526367
Batch 11/64 loss: 0.03920280933380127
Batch 12/64 loss: 0.03557974100112915
Batch 13/64 loss: 0.039292097091674805
Batch 14/64 loss: 0.03657722473144531
Batch 15/64 loss: 0.03876030445098877
Batch 16/64 loss: 0.04119449853897095
Batch 17/64 loss: 0.03246903419494629
Batch 18/64 loss: 0.04902535676956177
Batch 19/64 loss: 0.040019333362579346
Batch 20/64 loss: 0.035204172134399414
Batch 21/64 loss: 0.03961384296417236
Batch 22/64 loss: 0.02391064167022705
Batch 23/64 loss: 0.049958765506744385
Batch 24/64 loss: 0.036099910736083984
Batch 25/64 loss: 0.04122579097747803
Batch 26/64 loss: 0.03543972969055176
Batch 27/64 loss: 0.03728979825973511
Batch 28/64 loss: 0.046625614166259766
Batch 29/64 loss: 0.03373551368713379
Batch 30/64 loss: 0.027637779712677002
Batch 31/64 loss: 0.03117978572845459
Batch 32/64 loss: 0.023942112922668457
Batch 33/64 loss: 0.048179030418395996
Batch 34/64 loss: 0.037850141525268555
Batch 35/64 loss: 0.040678977966308594
Batch 36/64 loss: 0.030449628829956055
Batch 37/64 loss: 0.03488647937774658
Batch 38/64 loss: 0.026844143867492676
Batch 39/64 loss: 0.03581637144088745
Batch 40/64 loss: 0.04307377338409424
Batch 41/64 loss: 0.03255420923233032
Batch 42/64 loss: 0.05013465881347656
Batch 43/64 loss: 0.02942556142807007
Batch 44/64 loss: 0.06015026569366455
Batch 45/64 loss: 0.03525424003601074
Batch 46/64 loss: 0.04300248622894287
Batch 47/64 loss: 0.037555038928985596
Batch 48/64 loss: 0.051605224609375
Batch 49/64 loss: 0.04816138744354248
Batch 50/64 loss: 0.032428860664367676
Batch 51/64 loss: 0.03007262945175171
Batch 52/64 loss: 0.04957425594329834
Batch 53/64 loss: 0.045857250690460205
Batch 54/64 loss: 0.0297621488571167
Batch 55/64 loss: 0.04135632514953613
Batch 56/64 loss: 0.05178457498550415
Batch 57/64 loss: 0.04054129123687744
Batch 58/64 loss: 0.03976738452911377
Batch 59/64 loss: 0.03426218032836914
Batch 60/64 loss: 0.0406264066696167
Batch 61/64 loss: 0.03605860471725464
Batch 62/64 loss: 0.041797935962677
Batch 63/64 loss: 0.03421562910079956
Batch 64/64 loss: 0.04889005422592163
Epoch 227  Train loss: 0.03842093266692816  Val loss: 0.08946239395239919
Epoch 228
-------------------------------
Batch 1/64 loss: 0.027853548526763916
Batch 2/64 loss: 0.04469907283782959
Batch 3/64 loss: 0.03377127647399902
Batch 4/64 loss: 0.021352410316467285
Batch 5/64 loss: 0.03888815641403198
Batch 6/64 loss: 0.029864907264709473
Batch 7/64 loss: 0.03839695453643799
Batch 8/64 loss: 0.056314170360565186
Batch 9/64 loss: 0.06231123208999634
Batch 10/64 loss: 0.03382575511932373
Batch 11/64 loss: 0.037103116512298584
Batch 12/64 loss: 0.03074324131011963
Batch 13/64 loss: 0.02857387065887451
Batch 14/64 loss: 0.04614555835723877
Batch 15/64 loss: 0.03848719596862793
Batch 16/64 loss: 0.03994244337081909
Batch 17/64 loss: 0.048131465911865234
Batch 18/64 loss: 0.037214577198028564
Batch 19/64 loss: 0.033308207988739014
Batch 20/64 loss: 0.03808856010437012
Batch 21/64 loss: 0.04408717155456543
Batch 22/64 loss: 0.04747509956359863
Batch 23/64 loss: 0.03577464818954468
Batch 24/64 loss: 0.04230296611785889
Batch 25/64 loss: 0.035053253173828125
Batch 26/64 loss: 0.03718924522399902
Batch 27/64 loss: 0.03041243553161621
Batch 28/64 loss: 0.03180837631225586
Batch 29/64 loss: 0.034741222858428955
Batch 30/64 loss: 0.0423770546913147
Batch 31/64 loss: 0.05354928970336914
Batch 32/64 loss: 0.03503692150115967
Batch 33/64 loss: 0.04137587547302246
Batch 34/64 loss: 0.04113125801086426
Batch 35/64 loss: 0.04785430431365967
Batch 36/64 loss: 0.031600117683410645
Batch 37/64 loss: 0.034071922302246094
Batch 38/64 loss: 0.04106724262237549
Batch 39/64 loss: 0.04836392402648926
Batch 40/64 loss: 0.03568923473358154
Batch 41/64 loss: 0.04000723361968994
Batch 42/64 loss: 0.04058706760406494
Batch 43/64 loss: 0.03965681791305542
Batch 44/64 loss: 0.027554750442504883
Batch 45/64 loss: 0.03410828113555908
Batch 46/64 loss: 0.032884061336517334
Batch 47/64 loss: 0.04218149185180664
Batch 48/64 loss: 0.02536720037460327
Batch 49/64 loss: 0.044263601303100586
Batch 50/64 loss: 0.041723549365997314
Batch 51/64 loss: 0.05637925863265991
Batch 52/64 loss: 0.04399913549423218
Batch 53/64 loss: 0.04162740707397461
Batch 54/64 loss: 0.027412354946136475
Batch 55/64 loss: 0.0449489951133728
Batch 56/64 loss: 0.04774051904678345
Batch 57/64 loss: 0.035357773303985596
Batch 58/64 loss: 0.03678244352340698
Batch 59/64 loss: 0.0429224967956543
Batch 60/64 loss: 0.04301154613494873
Batch 61/64 loss: 0.02667015790939331
Batch 62/64 loss: 0.036345481872558594
Batch 63/64 loss: 0.03811907768249512
Batch 64/64 loss: 0.03487050533294678
Epoch 228  Train loss: 0.03877303319818833  Val loss: 0.08910850929640413
Epoch 229
-------------------------------
Batch 1/64 loss: 0.02941828966140747
Batch 2/64 loss: 0.034436702728271484
Batch 3/64 loss: 0.04680216312408447
Batch 4/64 loss: 0.048186659812927246
Batch 5/64 loss: 0.03367072343826294
Batch 6/64 loss: 0.037959158420562744
Batch 7/64 loss: 0.027658402919769287
Batch 8/64 loss: 0.030607223510742188
Batch 9/64 loss: 0.03882467746734619
Batch 10/64 loss: 0.039710164070129395
Batch 11/64 loss: 0.028431057929992676
Batch 12/64 loss: 0.04397165775299072
Batch 13/64 loss: 0.04718250036239624
Batch 14/64 loss: 0.05374777317047119
Batch 15/64 loss: 0.03527706861495972
Batch 16/64 loss: 0.031956255435943604
Batch 17/64 loss: 0.032897233963012695
Batch 18/64 loss: 0.032192349433898926
Batch 19/64 loss: 0.03528928756713867
Batch 20/64 loss: 0.039578139781951904
Batch 21/64 loss: 0.03491026163101196
Batch 22/64 loss: 0.04408681392669678
Batch 23/64 loss: 0.039574265480041504
Batch 24/64 loss: 0.03362298011779785
Batch 25/64 loss: 0.04611140489578247
Batch 26/64 loss: 0.04818516969680786
Batch 27/64 loss: 0.03029865026473999
Batch 28/64 loss: 0.034873247146606445
Batch 29/64 loss: 0.038529813289642334
Batch 30/64 loss: 0.04298591613769531
Batch 31/64 loss: 0.04496502876281738
Batch 32/64 loss: 0.03408271074295044
Batch 33/64 loss: 0.04807102680206299
Batch 34/64 loss: 0.03775864839553833
Batch 35/64 loss: 0.0267333984375
Batch 36/64 loss: 0.039980530738830566
Batch 37/64 loss: 0.02272617816925049
Batch 38/64 loss: 0.03187984228134155
Batch 39/64 loss: 0.03500080108642578
Batch 40/64 loss: 0.04635477066040039
Batch 41/64 loss: 0.037905335426330566
Batch 42/64 loss: 0.03958690166473389
Batch 43/64 loss: 0.034117817878723145
Batch 44/64 loss: 0.031441330909729004
Batch 45/64 loss: 0.03990066051483154
Batch 46/64 loss: 0.04229533672332764
Batch 47/64 loss: 0.03534901142120361
Batch 48/64 loss: 0.04313051700592041
Batch 49/64 loss: 0.040161073207855225
Batch 50/64 loss: 0.04049426317214966
Batch 51/64 loss: 0.031898677349090576
Batch 52/64 loss: 0.03466850519180298
Batch 53/64 loss: 0.0370335578918457
Batch 54/64 loss: 0.02609848976135254
Batch 55/64 loss: 0.032615840435028076
Batch 56/64 loss: 0.03211021423339844
Batch 57/64 loss: 0.032032907009124756
Batch 58/64 loss: 0.030405700206756592
Batch 59/64 loss: 0.03588289022445679
Batch 60/64 loss: 0.03185552358627319
Batch 61/64 loss: 0.04759645462036133
Batch 62/64 loss: 0.02824580669403076
Batch 63/64 loss: 0.05084317922592163
Batch 64/64 loss: 0.03713101148605347
Epoch 229  Train loss: 0.037208583541944916  Val loss: 0.09055610252000212
Epoch 230
-------------------------------
Batch 1/64 loss: 0.02676689624786377
Batch 2/64 loss: 0.02824234962463379
Batch 3/64 loss: 0.027472376823425293
Batch 4/64 loss: 0.05067974328994751
Batch 5/64 loss: 0.041167378425598145
Batch 6/64 loss: 0.03254324197769165
Batch 7/64 loss: 0.030211210250854492
Batch 8/64 loss: 0.03081613779067993
Batch 9/64 loss: 0.03570723533630371
Batch 10/64 loss: 0.041546642780303955
Batch 11/64 loss: 0.041349947452545166
Batch 12/64 loss: 0.040104806423187256
Batch 13/64 loss: 0.039229393005371094
Batch 14/64 loss: 0.031077861785888672
Batch 15/64 loss: 0.03468066453933716
Batch 16/64 loss: 0.026871442794799805
Batch 17/64 loss: 0.041664183139801025
Batch 18/64 loss: 0.033431053161621094
Batch 19/64 loss: 0.029580295085906982
Batch 20/64 loss: 0.03378570079803467
Batch 21/64 loss: 0.025920748710632324
Batch 22/64 loss: 0.03350687026977539
Batch 23/64 loss: 0.03689974546432495
Batch 24/64 loss: 0.028312623500823975
Batch 25/64 loss: 0.03690463304519653
Batch 26/64 loss: 0.0309678316116333
Batch 27/64 loss: 0.04054594039916992
Batch 28/64 loss: 0.041462838649749756
Batch 29/64 loss: 0.036478519439697266
Batch 30/64 loss: 0.02548116445541382
Batch 31/64 loss: 0.028706908226013184
Batch 32/64 loss: 0.027116894721984863
Batch 33/64 loss: 0.03146868944168091
Batch 34/64 loss: 0.03362011909484863
Batch 35/64 loss: 0.034640491008758545
Batch 36/64 loss: 0.04631072282791138
Batch 37/64 loss: 0.04172724485397339
Batch 38/64 loss: 0.03227734565734863
Batch 39/64 loss: 0.030854403972625732
Batch 40/64 loss: 0.046601712703704834
Batch 41/64 loss: 0.04530131816864014
Batch 42/64 loss: 0.053875088691711426
Batch 43/64 loss: 0.04064542055130005
Batch 44/64 loss: 0.04182255268096924
Batch 45/64 loss: 0.04592829942703247
Batch 46/64 loss: 0.03268367052078247
Batch 47/64 loss: 0.03333228826522827
Batch 48/64 loss: 0.02700221538543701
Batch 49/64 loss: 0.03492790460586548
Batch 50/64 loss: 0.05325502157211304
Batch 51/64 loss: 0.03961390256881714
Batch 52/64 loss: 0.04026222229003906
Batch 53/64 loss: 0.032256364822387695
Batch 54/64 loss: 0.02945631742477417
Batch 55/64 loss: 0.04486960172653198
Batch 56/64 loss: 0.03357255458831787
Batch 57/64 loss: 0.040727436542510986
Batch 58/64 loss: 0.053015053272247314
Batch 59/64 loss: 0.05416285991668701
Batch 60/64 loss: 0.03398120403289795
Batch 61/64 loss: 0.03491532802581787
Batch 62/64 loss: 0.03387373685836792
Batch 63/64 loss: 0.030293166637420654
Batch 64/64 loss: 0.05665856599807739
Epoch 230  Train loss: 0.036690219009623805  Val loss: 0.08727020824078432
Saving best model, epoch: 230
Epoch 231
-------------------------------
Batch 1/64 loss: 0.03344237804412842
Batch 2/64 loss: 0.03425765037536621
Batch 3/64 loss: 0.03944993019104004
Batch 4/64 loss: 0.023322224617004395
Batch 5/64 loss: 0.036598026752471924
Batch 6/64 loss: 0.041143953800201416
Batch 7/64 loss: 0.03696721792221069
Batch 8/64 loss: 0.029804110527038574
Batch 9/64 loss: 0.042147159576416016
Batch 10/64 loss: 0.041629672050476074
Batch 11/64 loss: 0.033849239349365234
Batch 12/64 loss: 0.02840203046798706
Batch 13/64 loss: 0.04333853721618652
Batch 14/64 loss: 0.041306138038635254
Batch 15/64 loss: 0.02494126558303833
Batch 16/64 loss: 0.028495311737060547
Batch 17/64 loss: 0.04787087440490723
Batch 18/64 loss: 0.0327913761138916
Batch 19/64 loss: 0.040538907051086426
Batch 20/64 loss: 0.029484331607818604
Batch 21/64 loss: 0.035562217235565186
Batch 22/64 loss: 0.02809584140777588
Batch 23/64 loss: 0.03659576177597046
Batch 24/64 loss: 0.03423339128494263
Batch 25/64 loss: 0.036309659481048584
Batch 26/64 loss: 0.033210933208465576
Batch 27/64 loss: 0.03801947832107544
Batch 28/64 loss: 0.04290574789047241
Batch 29/64 loss: 0.053313612937927246
Batch 30/64 loss: 0.05494284629821777
Batch 31/64 loss: 0.0480228066444397
Batch 32/64 loss: 0.046030640602111816
Batch 33/64 loss: 0.031327784061431885
Batch 34/64 loss: 0.04214835166931152
Batch 35/64 loss: 0.029370248317718506
Batch 36/64 loss: 0.035364747047424316
Batch 37/64 loss: 0.034742116928100586
Batch 38/64 loss: 0.04381030797958374
Batch 39/64 loss: 0.03490304946899414
Batch 40/64 loss: 0.02683281898498535
Batch 41/64 loss: 0.0540008544921875
Batch 42/64 loss: 0.031405866146087646
Batch 43/64 loss: 0.029060065746307373
Batch 44/64 loss: 0.03967750072479248
Batch 45/64 loss: 0.02763843536376953
Batch 46/64 loss: 0.04531055688858032
Batch 47/64 loss: 0.039985477924346924
Batch 48/64 loss: 0.04733264446258545
Batch 49/64 loss: 0.028029263019561768
Batch 50/64 loss: 0.03357553482055664
Batch 51/64 loss: 0.0432511568069458
Batch 52/64 loss: 0.045658767223358154
Batch 53/64 loss: 0.044436752796173096
Batch 54/64 loss: 0.05321246385574341
Batch 55/64 loss: 0.061213910579681396
Batch 56/64 loss: 0.05184859037399292
Batch 57/64 loss: 0.028923213481903076
Batch 58/64 loss: 0.03107595443725586
Batch 59/64 loss: 0.04305243492126465
Batch 60/64 loss: 0.03870576620101929
Batch 61/64 loss: 0.058856964111328125
Batch 62/64 loss: 0.040293216705322266
Batch 63/64 loss: 0.02803480625152588
Batch 64/64 loss: 0.0402485728263855
Epoch 231  Train loss: 0.03843591096354466  Val loss: 0.08790982117767596
Epoch 232
-------------------------------
Batch 1/64 loss: 0.038502514362335205
Batch 2/64 loss: 0.03667038679122925
Batch 3/64 loss: 0.021676063537597656
Batch 4/64 loss: 0.024125754833221436
Batch 5/64 loss: 0.03505641222000122
Batch 6/64 loss: 0.03694242238998413
Batch 7/64 loss: 0.03263401985168457
Batch 8/64 loss: 0.028325557708740234
Batch 9/64 loss: 0.026973605155944824
Batch 10/64 loss: 0.031536996364593506
Batch 11/64 loss: 0.036220669746398926
Batch 12/64 loss: 0.03956526517868042
Batch 13/64 loss: 0.041718900203704834
Batch 14/64 loss: 0.04267168045043945
Batch 15/64 loss: 0.018703699111938477
Batch 16/64 loss: 0.03590798377990723
Batch 17/64 loss: 0.03242027759552002
Batch 18/64 loss: 0.03268200159072876
Batch 19/64 loss: 0.038726627826690674
Batch 20/64 loss: 0.051586270332336426
Batch 21/64 loss: 0.038998961448669434
Batch 22/64 loss: 0.023045480251312256
Batch 23/64 loss: 0.035666823387145996
Batch 24/64 loss: 0.04338061809539795
Batch 25/64 loss: 0.028359174728393555
Batch 26/64 loss: 0.03491783142089844
Batch 27/64 loss: 0.03480410575866699
Batch 28/64 loss: 0.040132999420166016
Batch 29/64 loss: 0.023945927619934082
Batch 30/64 loss: 0.0319901704788208
Batch 31/64 loss: 0.03650778532028198
Batch 32/64 loss: 0.027081727981567383
Batch 33/64 loss: 0.03317993879318237
Batch 34/64 loss: 0.03453660011291504
Batch 35/64 loss: 0.0281563401222229
Batch 36/64 loss: 0.04637426137924194
Batch 37/64 loss: 0.03095906972885132
Batch 38/64 loss: 0.031323134899139404
Batch 39/64 loss: 0.030736982822418213
Batch 40/64 loss: 0.051118433475494385
Batch 41/64 loss: 0.04701507091522217
Batch 42/64 loss: 0.06481742858886719
Batch 43/64 loss: 0.036638617515563965
Batch 44/64 loss: 0.04545700550079346
Batch 45/64 loss: 0.04246068000793457
Batch 46/64 loss: 0.0423504114151001
Batch 47/64 loss: 0.04852402210235596
Batch 48/64 loss: 0.03109675645828247
Batch 49/64 loss: 0.02883172035217285
Batch 50/64 loss: 0.052368760108947754
Batch 51/64 loss: 0.028582394123077393
Batch 52/64 loss: 0.04853355884552002
Batch 53/64 loss: 0.05628162622451782
Batch 54/64 loss: 0.0321466326713562
Batch 55/64 loss: 0.025588512420654297
Batch 56/64 loss: 0.0415339469909668
Batch 57/64 loss: 0.0388527512550354
Batch 58/64 loss: 0.03881490230560303
Batch 59/64 loss: 0.04550516605377197
Batch 60/64 loss: 0.03445786237716675
Batch 61/64 loss: 0.04636991024017334
Batch 62/64 loss: 0.032333970069885254
Batch 63/64 loss: 0.02915722131729126
Batch 64/64 loss: 0.04569798707962036
Epoch 232  Train loss: 0.036703621873668595  Val loss: 0.08726496282721713
Saving best model, epoch: 232
Epoch 233
-------------------------------
Batch 1/64 loss: 0.027142882347106934
Batch 2/64 loss: 0.036475956439971924
Batch 3/64 loss: 0.02994561195373535
Batch 4/64 loss: 0.02356618642807007
Batch 5/64 loss: 0.02384316921234131
Batch 6/64 loss: 0.033202171325683594
Batch 7/64 loss: 0.02319389581680298
Batch 8/64 loss: 0.0425531268119812
Batch 9/64 loss: 0.030059635639190674
Batch 10/64 loss: 0.0388677716255188
Batch 11/64 loss: 0.03935307264328003
Batch 12/64 loss: 0.01944178342819214
Batch 13/64 loss: 0.03321349620819092
Batch 14/64 loss: 0.037876784801483154
Batch 15/64 loss: 0.03797358274459839
Batch 16/64 loss: 0.03194248676300049
Batch 17/64 loss: 0.03378826379776001
Batch 18/64 loss: 0.038657307624816895
Batch 19/64 loss: 0.02167034149169922
Batch 20/64 loss: 0.038397252559661865
Batch 21/64 loss: 0.04652124643325806
Batch 22/64 loss: 0.03729921579360962
Batch 23/64 loss: 0.041617631912231445
Batch 24/64 loss: 0.03246808052062988
Batch 25/64 loss: 0.03293132781982422
Batch 26/64 loss: 0.03443777561187744
Batch 27/64 loss: 0.027168214321136475
Batch 28/64 loss: 0.039944469928741455
Batch 29/64 loss: 0.03547471761703491
Batch 30/64 loss: 0.028986752033233643
Batch 31/64 loss: 0.03057628870010376
Batch 32/64 loss: 0.040297627449035645
Batch 33/64 loss: 0.03864443302154541
Batch 34/64 loss: 0.024461030960083008
Batch 35/64 loss: 0.03936666250228882
Batch 36/64 loss: 0.028047919273376465
Batch 37/64 loss: 0.045029282569885254
Batch 38/64 loss: 0.02431619167327881
Batch 39/64 loss: 0.025714874267578125
Batch 40/64 loss: 0.05061239004135132
Batch 41/64 loss: 0.030575811862945557
Batch 42/64 loss: 0.04199576377868652
Batch 43/64 loss: 0.03558051586151123
Batch 44/64 loss: 0.046912550926208496
Batch 45/64 loss: 0.03821361064910889
Batch 46/64 loss: 0.025398671627044678
Batch 47/64 loss: 0.03613179922103882
Batch 48/64 loss: 0.045274317264556885
Batch 49/64 loss: 0.02539271116256714
Batch 50/64 loss: 0.05114006996154785
Batch 51/64 loss: 0.02594238519668579
Batch 52/64 loss: 0.03823673725128174
Batch 53/64 loss: 0.042677879333496094
Batch 54/64 loss: 0.040450870990753174
Batch 55/64 loss: 0.023774147033691406
Batch 56/64 loss: 0.03567999601364136
Batch 57/64 loss: 0.03409469127655029
Batch 58/64 loss: 0.025898396968841553
Batch 59/64 loss: 0.035488665103912354
Batch 60/64 loss: 0.04445832967758179
Batch 61/64 loss: 0.04896265268325806
Batch 62/64 loss: 0.04318505525588989
Batch 63/64 loss: 0.04269474744796753
Batch 64/64 loss: 0.036763668060302734
Epoch 233  Train loss: 0.034993161407171514  Val loss: 0.08980909164009225
Epoch 234
-------------------------------
Batch 1/64 loss: 0.03752797842025757
Batch 2/64 loss: 0.03678041696548462
Batch 3/64 loss: 0.03542637825012207
Batch 4/64 loss: 0.030806899070739746
Batch 5/64 loss: 0.02861088514328003
Batch 6/64 loss: 0.02473437786102295
Batch 7/64 loss: 0.037978410720825195
Batch 8/64 loss: 0.04677581787109375
Batch 9/64 loss: 0.029775619506835938
Batch 10/64 loss: 0.051362574100494385
Batch 11/64 loss: 0.045410215854644775
Batch 12/64 loss: 0.023529529571533203
Batch 13/64 loss: 0.02717411518096924
Batch 14/64 loss: 0.0315171480178833
Batch 15/64 loss: 0.03706008195877075
Batch 16/64 loss: 0.03910154104232788
Batch 17/64 loss: 0.027400672435760498
Batch 18/64 loss: 0.0382724404335022
Batch 19/64 loss: 0.037491440773010254
Batch 20/64 loss: 0.03337538242340088
Batch 21/64 loss: 0.02346217632293701
Batch 22/64 loss: 0.03483688831329346
Batch 23/64 loss: 0.02815467119216919
Batch 24/64 loss: 0.018657207489013672
Batch 25/64 loss: 0.04723101854324341
Batch 26/64 loss: 0.06587541103363037
Batch 27/64 loss: 0.029686331748962402
Batch 28/64 loss: 0.0346413254737854
Batch 29/64 loss: 0.0349884033203125
Batch 30/64 loss: 0.037297606468200684
Batch 31/64 loss: 0.03572738170623779
Batch 32/64 loss: 0.03897380828857422
Batch 33/64 loss: 0.035169124603271484
Batch 34/64 loss: 0.026800215244293213
Batch 35/64 loss: 0.06163835525512695
Batch 36/64 loss: 0.03499823808670044
Batch 37/64 loss: 0.033469438552856445
Batch 38/64 loss: 0.034840166568756104
Batch 39/64 loss: 0.028227627277374268
Batch 40/64 loss: 0.037020087242126465
Batch 41/64 loss: 0.040218353271484375
Batch 42/64 loss: 0.04815220832824707
Batch 43/64 loss: 0.028331518173217773
Batch 44/64 loss: 0.023392140865325928
Batch 45/64 loss: 0.03407788276672363
Batch 46/64 loss: 0.03963148593902588
Batch 47/64 loss: 0.02548849582672119
Batch 48/64 loss: 0.04725825786590576
Batch 49/64 loss: 0.031044065952301025
Batch 50/64 loss: 0.041769564151763916
Batch 51/64 loss: 0.03947043418884277
Batch 52/64 loss: 0.03350949287414551
Batch 53/64 loss: 0.04882997274398804
Batch 54/64 loss: 0.043556809425354004
Batch 55/64 loss: 0.04456436634063721
Batch 56/64 loss: 0.0411609411239624
Batch 57/64 loss: 0.043326497077941895
Batch 58/64 loss: 0.04343545436859131
Batch 59/64 loss: 0.03875458240509033
Batch 60/64 loss: 0.03628122806549072
Batch 61/64 loss: 0.03229653835296631
Batch 62/64 loss: 0.026107728481292725
Batch 63/64 loss: 0.039053142070770264
Batch 64/64 loss: 0.04508465528488159
Epoch 234  Train loss: 0.036475797026765115  Val loss: 0.08841453712830429
Epoch 235
-------------------------------
Batch 1/64 loss: 0.020280122756958008
Batch 2/64 loss: 0.031760573387145996
Batch 3/64 loss: 0.049473702907562256
Batch 4/64 loss: 0.03666001558303833
Batch 5/64 loss: 0.037265658378601074
Batch 6/64 loss: 0.02954840660095215
Batch 7/64 loss: 0.02722644805908203
Batch 8/64 loss: 0.02549302577972412
Batch 9/64 loss: 0.036070168018341064
Batch 10/64 loss: 0.04958456754684448
Batch 11/64 loss: 0.025330960750579834
Batch 12/64 loss: 0.04127269983291626
Batch 13/64 loss: 0.04046928882598877
Batch 14/64 loss: 0.024795472621917725
Batch 15/64 loss: 0.034272074699401855
Batch 16/64 loss: 0.02963346242904663
Batch 17/64 loss: 0.024195730686187744
Batch 18/64 loss: 0.0263405442237854
Batch 19/64 loss: 0.023021280765533447
Batch 20/64 loss: 0.038479506969451904
Batch 21/64 loss: 0.02168893814086914
Batch 22/64 loss: 0.032464802265167236
Batch 23/64 loss: 0.033725738525390625
Batch 24/64 loss: 0.037436604499816895
Batch 25/64 loss: 0.03807878494262695
Batch 26/64 loss: 0.03646957874298096
Batch 27/64 loss: 0.025468826293945312
Batch 28/64 loss: 0.033010125160217285
Batch 29/64 loss: 0.02417999505996704
Batch 30/64 loss: 0.030109167098999023
Batch 31/64 loss: 0.0415416955947876
Batch 32/64 loss: 0.04479503631591797
Batch 33/64 loss: 0.02493751049041748
Batch 34/64 loss: 0.0407177209854126
Batch 35/64 loss: 0.026954829692840576
Batch 36/64 loss: 0.02108132839202881
Batch 37/64 loss: 0.0342944860458374
Batch 38/64 loss: 0.027286529541015625
Batch 39/64 loss: 0.03494071960449219
Batch 40/64 loss: 0.042823195457458496
Batch 41/64 loss: 0.03128457069396973
Batch 42/64 loss: 0.032358646392822266
Batch 43/64 loss: 0.028854668140411377
Batch 44/64 loss: 0.03329503536224365
Batch 45/64 loss: 0.04573947191238403
Batch 46/64 loss: 0.046185076236724854
Batch 47/64 loss: 0.03842538595199585
Batch 48/64 loss: 0.033939361572265625
Batch 49/64 loss: 0.04226422309875488
Batch 50/64 loss: 0.028073549270629883
Batch 51/64 loss: 0.04586440324783325
Batch 52/64 loss: 0.04037290811538696
Batch 53/64 loss: 0.02664041519165039
Batch 54/64 loss: 0.04014033079147339
Batch 55/64 loss: 0.03708779811859131
Batch 56/64 loss: 0.050697386264801025
Batch 57/64 loss: 0.02588057518005371
Batch 58/64 loss: 0.0542910099029541
Batch 59/64 loss: 0.03053051233291626
Batch 60/64 loss: 0.0433163046836853
Batch 61/64 loss: 0.023545265197753906
Batch 62/64 loss: 0.029819607734680176
Batch 63/64 loss: 0.048392295837402344
Batch 64/64 loss: 0.024408936500549316
Epoch 235  Train loss: 0.03417231101615756  Val loss: 0.08894785388638474
Epoch 236
-------------------------------
Batch 1/64 loss: 0.02687966823577881
Batch 2/64 loss: 0.03164708614349365
Batch 3/64 loss: 0.026822924613952637
Batch 4/64 loss: 0.030667662620544434
Batch 5/64 loss: 0.03776448965072632
Batch 6/64 loss: 0.020708441734313965
Batch 7/64 loss: 0.04807561635971069
Batch 8/64 loss: 0.03850299119949341
Batch 9/64 loss: 0.036472201347351074
Batch 10/64 loss: 0.02985703945159912
Batch 11/64 loss: 0.04981589317321777
Batch 12/64 loss: 0.020851731300354004
Batch 13/64 loss: 0.023326754570007324
Batch 14/64 loss: 0.027411699295043945
Batch 15/64 loss: 0.02790623903274536
Batch 16/64 loss: 0.04228055477142334
Batch 17/64 loss: 0.02409571409225464
Batch 18/64 loss: 0.03210479021072388
Batch 19/64 loss: 0.01584792137145996
Batch 20/64 loss: 0.014919698238372803
Batch 21/64 loss: 0.03575032949447632
Batch 22/64 loss: 0.032542288303375244
Batch 23/64 loss: 0.030880987644195557
Batch 24/64 loss: 0.017203032970428467
Batch 25/64 loss: 0.02721273899078369
Batch 26/64 loss: 0.030094385147094727
Batch 27/64 loss: 0.0288926362991333
Batch 28/64 loss: 0.033532917499542236
Batch 29/64 loss: 0.026808619499206543
Batch 30/64 loss: 0.021489977836608887
Batch 31/64 loss: 0.049392104148864746
Batch 32/64 loss: 0.04004448652267456
Batch 33/64 loss: 0.036921918392181396
Batch 34/64 loss: 0.028191208839416504
Batch 35/64 loss: 0.02265554666519165
Batch 36/64 loss: 0.0399208664894104
Batch 37/64 loss: 0.03221869468688965
Batch 38/64 loss: 0.03446674346923828
Batch 39/64 loss: 0.04089385271072388
Batch 40/64 loss: 0.030385971069335938
Batch 41/64 loss: 0.035201311111450195
Batch 42/64 loss: 0.02941727638244629
Batch 43/64 loss: 0.03203892707824707
Batch 44/64 loss: 0.02474963665008545
Batch 45/64 loss: 0.040825068950653076
Batch 46/64 loss: 0.03097432851791382
Batch 47/64 loss: 0.03444570302963257
Batch 48/64 loss: 0.03284114599227905
Batch 49/64 loss: 0.04498136043548584
Batch 50/64 loss: 0.04110187292098999
Batch 51/64 loss: 0.04010009765625
Batch 52/64 loss: 0.027921676635742188
Batch 53/64 loss: 0.04540431499481201
Batch 54/64 loss: 0.043534040451049805
Batch 55/64 loss: 0.04746687412261963
Batch 56/64 loss: 0.042149901390075684
Batch 57/64 loss: 0.023973464965820312
Batch 58/64 loss: 0.03695333003997803
Batch 59/64 loss: 0.04586106538772583
Batch 60/64 loss: 0.033337295055389404
Batch 61/64 loss: 0.048836350440979004
Batch 62/64 loss: 0.03785288333892822
Batch 63/64 loss: 0.034668922424316406
Batch 64/64 loss: 0.04497218132019043
Epoch 236  Train loss: 0.03347175542046042  Val loss: 0.0878459895189685
Epoch 237
-------------------------------
Batch 1/64 loss: 0.0377659797668457
Batch 2/64 loss: 0.029943346977233887
Batch 3/64 loss: 0.031879961490631104
Batch 4/64 loss: 0.03173786401748657
Batch 5/64 loss: 0.03889954090118408
Batch 6/64 loss: 0.02925938367843628
Batch 7/64 loss: 0.037051618099212646
Batch 8/64 loss: 0.05532705783843994
Batch 9/64 loss: 0.032596051692962646
Batch 10/64 loss: 0.0323183536529541
Batch 11/64 loss: 0.022654175758361816
Batch 12/64 loss: 0.03813356161117554
Batch 13/64 loss: 0.03156745433807373
Batch 14/64 loss: 0.03508049249649048
Batch 15/64 loss: 0.019615650177001953
Batch 16/64 loss: 0.036099135875701904
Batch 17/64 loss: 0.030255615711212158
Batch 18/64 loss: 0.020273208618164062
Batch 19/64 loss: 0.029458284378051758
Batch 20/64 loss: 0.03335404396057129
Batch 21/64 loss: 0.029805004596710205
Batch 22/64 loss: 0.030442476272583008
Batch 23/64 loss: 0.04411691427230835
Batch 24/64 loss: 0.03100895881652832
Batch 25/64 loss: 0.028880000114440918
Batch 26/64 loss: 0.041495323181152344
Batch 27/64 loss: 0.03162491321563721
Batch 28/64 loss: 0.04285275936126709
Batch 29/64 loss: 0.04421031475067139
Batch 30/64 loss: 0.025988101959228516
Batch 31/64 loss: 0.04885399341583252
Batch 32/64 loss: 0.02566981315612793
Batch 33/64 loss: 0.02715170383453369
Batch 34/64 loss: 0.04022669792175293
Batch 35/64 loss: 0.030307114124298096
Batch 36/64 loss: 0.04499882459640503
Batch 37/64 loss: 0.02304518222808838
Batch 38/64 loss: 0.025825202465057373
Batch 39/64 loss: 0.02437227964401245
Batch 40/64 loss: 0.039370059967041016
Batch 41/64 loss: 0.038816630840301514
Batch 42/64 loss: 0.02434760332107544
Batch 43/64 loss: 0.03581392765045166
Batch 44/64 loss: 0.025975406169891357
Batch 45/64 loss: 0.027783334255218506
Batch 46/64 loss: 0.03416210412979126
Batch 47/64 loss: 0.030000269412994385
Batch 48/64 loss: 0.0372617244720459
Batch 49/64 loss: 0.04044365882873535
Batch 50/64 loss: 0.037676453590393066
Batch 51/64 loss: 0.025089025497436523
Batch 52/64 loss: 0.037921369075775146
Batch 53/64 loss: 0.031151413917541504
Batch 54/64 loss: 0.03566765785217285
Batch 55/64 loss: 0.03569704294204712
Batch 56/64 loss: 0.03715360164642334
Batch 57/64 loss: 0.028533518314361572
Batch 58/64 loss: 0.022149264812469482
Batch 59/64 loss: 0.02014338970184326
Batch 60/64 loss: 0.02579176425933838
Batch 61/64 loss: 0.035742759704589844
Batch 62/64 loss: 0.03195750713348389
Batch 63/64 loss: 0.032952308654785156
Batch 64/64 loss: 0.03817933797836304
Epoch 237  Train loss: 0.03285306131138521  Val loss: 0.08929355775367763
Epoch 238
-------------------------------
Batch 1/64 loss: 0.040527522563934326
Batch 2/64 loss: 0.04257553815841675
Batch 3/64 loss: 0.04460787773132324
Batch 4/64 loss: 0.026881098747253418
Batch 5/64 loss: 0.029075026512145996
Batch 6/64 loss: 0.02873551845550537
Batch 7/64 loss: 0.027350664138793945
Batch 8/64 loss: 0.024172663688659668
Batch 9/64 loss: 0.03728771209716797
Batch 10/64 loss: 0.016670823097229004
Batch 11/64 loss: 0.02959728240966797
Batch 12/64 loss: 0.02957993745803833
Batch 13/64 loss: 0.023956477642059326
Batch 14/64 loss: 0.02904224395751953
Batch 15/64 loss: 0.058473825454711914
Batch 16/64 loss: 0.021081149578094482
Batch 17/64 loss: 0.04028886556625366
Batch 18/64 loss: 0.029532015323638916
Batch 19/64 loss: 0.03337651491165161
Batch 20/64 loss: 0.039541006088256836
Batch 21/64 loss: 0.04022258520126343
Batch 22/64 loss: 0.028743863105773926
Batch 23/64 loss: 0.03172314167022705
Batch 24/64 loss: 0.05267995595932007
Batch 25/64 loss: 0.04374086856842041
Batch 26/64 loss: 0.0357394814491272
Batch 27/64 loss: 0.03272521495819092
Batch 28/64 loss: 0.019353032112121582
Batch 29/64 loss: 0.02966243028640747
Batch 30/64 loss: 0.027383089065551758
Batch 31/64 loss: 0.031565308570861816
Batch 32/64 loss: 0.027196407318115234
Batch 33/64 loss: 0.02779865264892578
Batch 34/64 loss: 0.03274095058441162
Batch 35/64 loss: 0.025112390518188477
Batch 36/64 loss: 0.03368961811065674
Batch 37/64 loss: 0.02430903911590576
Batch 38/64 loss: 0.036807477474212646
Batch 39/64 loss: 0.038716912269592285
Batch 40/64 loss: 0.026173710823059082
Batch 41/64 loss: 0.025146186351776123
Batch 42/64 loss: 0.029945075511932373
Batch 43/64 loss: 0.040511250495910645
Batch 44/64 loss: 0.041216254234313965
Batch 45/64 loss: 0.031137406826019287
Batch 46/64 loss: 0.04781639575958252
Batch 47/64 loss: 0.03166818618774414
Batch 48/64 loss: 0.03125
Batch 49/64 loss: 0.03284579515457153
Batch 50/64 loss: 0.03772038221359253
Batch 51/64 loss: 0.027884602546691895
Batch 52/64 loss: 0.027689814567565918
Batch 53/64 loss: 0.03061056137084961
Batch 54/64 loss: 0.028619050979614258
Batch 55/64 loss: 0.05025959014892578
Batch 56/64 loss: 0.038976848125457764
Batch 57/64 loss: 0.024793505668640137
Batch 58/64 loss: 0.0399928092956543
Batch 59/64 loss: 0.030594348907470703
Batch 60/64 loss: 0.03663647174835205
Batch 61/64 loss: 0.053265392780303955
Batch 62/64 loss: 0.04115098714828491
Batch 63/64 loss: 0.037889063358306885
Batch 64/64 loss: 0.045978665351867676
Epoch 238  Train loss: 0.03373399014566459  Val loss: 0.08780140831708089
Epoch 239
-------------------------------
Batch 1/64 loss: 0.021474242210388184
Batch 2/64 loss: 0.0367775559425354
Batch 3/64 loss: 0.03274482488632202
Batch 4/64 loss: 0.04001671075820923
Batch 5/64 loss: 0.03516894578933716
Batch 6/64 loss: 0.025028526782989502
Batch 7/64 loss: 0.029925167560577393
Batch 8/64 loss: 0.03291189670562744
Batch 9/64 loss: 0.01790034770965576
Batch 10/64 loss: 0.03962045907974243
Batch 11/64 loss: 0.042752981185913086
Batch 12/64 loss: 0.022930920124053955
Batch 13/64 loss: 0.03630727529525757
Batch 14/64 loss: 0.03642469644546509
Batch 15/64 loss: 0.030126214027404785
Batch 16/64 loss: 0.021313846111297607
Batch 17/64 loss: 0.04171115159988403
Batch 18/64 loss: 0.03102266788482666
Batch 19/64 loss: 0.040695905685424805
Batch 20/64 loss: 0.031577885150909424
Batch 21/64 loss: 0.027872025966644287
Batch 22/64 loss: 0.025124728679656982
Batch 23/64 loss: 0.028180241584777832
Batch 24/64 loss: 0.025706112384796143
Batch 25/64 loss: 0.015303254127502441
Batch 26/64 loss: 0.03340744972229004
Batch 27/64 loss: 0.03123319149017334
Batch 28/64 loss: 0.0337371826171875
Batch 29/64 loss: 0.02105051279067993
Batch 30/64 loss: 0.0414770245552063
Batch 31/64 loss: 0.03668928146362305
Batch 32/64 loss: 0.03607618808746338
Batch 33/64 loss: 0.02338510751724243
Batch 34/64 loss: 0.03136175870895386
Batch 35/64 loss: 0.038206398487091064
Batch 36/64 loss: 0.02635258436203003
Batch 37/64 loss: 0.029368162155151367
Batch 38/64 loss: 0.02448403835296631
Batch 39/64 loss: 0.02770942449569702
Batch 40/64 loss: 0.030040860176086426
Batch 41/64 loss: 0.016809940338134766
Batch 42/64 loss: 0.02755904197692871
Batch 43/64 loss: 0.034996867179870605
Batch 44/64 loss: 0.016453564167022705
Batch 45/64 loss: 0.020085573196411133
Batch 46/64 loss: 0.04090923070907593
Batch 47/64 loss: 0.029522061347961426
Batch 48/64 loss: 0.03724569082260132
Batch 49/64 loss: 0.03690522909164429
Batch 50/64 loss: 0.032936811447143555
Batch 51/64 loss: 0.04269373416900635
Batch 52/64 loss: 0.03953444957733154
Batch 53/64 loss: 0.039932966232299805
Batch 54/64 loss: 0.03170651197433472
Batch 55/64 loss: 0.03267425298690796
Batch 56/64 loss: 0.03877913951873779
Batch 57/64 loss: 0.04548907279968262
Batch 58/64 loss: 0.03812241554260254
Batch 59/64 loss: 0.039604902267456055
Batch 60/64 loss: 0.04288792610168457
Batch 61/64 loss: 0.029345810413360596
Batch 62/64 loss: 0.029040217399597168
Batch 63/64 loss: 0.050354182720184326
Batch 64/64 loss: 0.026933014392852783
Epoch 239  Train loss: 0.032109601591147625  Val loss: 0.08736993684801449
Epoch 240
-------------------------------
Batch 1/64 loss: 0.031314849853515625
Batch 2/64 loss: 0.026180267333984375
Batch 3/64 loss: 0.029427707195281982
Batch 4/64 loss: 0.05172759294509888
Batch 5/64 loss: 0.028281211853027344
Batch 6/64 loss: 0.03418165445327759
Batch 7/64 loss: 0.030066370964050293
Batch 8/64 loss: 0.038420021533966064
Batch 9/64 loss: 0.034197211265563965
Batch 10/64 loss: 0.02436929941177368
Batch 11/64 loss: 0.018973052501678467
Batch 12/64 loss: 0.031024456024169922
Batch 13/64 loss: 0.02380502223968506
Batch 14/64 loss: 0.028213441371917725
Batch 15/64 loss: 0.03464710712432861
Batch 16/64 loss: 0.03311634063720703
Batch 17/64 loss: 0.03355449438095093
Batch 18/64 loss: 0.045655786991119385
Batch 19/64 loss: 0.025144994258880615
Batch 20/64 loss: 0.022252142429351807
Batch 21/64 loss: 0.04260718822479248
Batch 22/64 loss: 0.02405637502670288
Batch 23/64 loss: 0.02250802516937256
Batch 24/64 loss: 0.028638064861297607
Batch 25/64 loss: 0.046867191791534424
Batch 26/64 loss: 0.042608678340911865
Batch 27/64 loss: 0.04745137691497803
Batch 28/64 loss: 0.03661459684371948
Batch 29/64 loss: 0.03038322925567627
Batch 30/64 loss: 0.026709258556365967
Batch 31/64 loss: 0.0254971981048584
Batch 32/64 loss: 0.03342938423156738
Batch 33/64 loss: 0.0379369854927063
Batch 34/64 loss: 0.02989649772644043
Batch 35/64 loss: 0.046005070209503174
Batch 36/64 loss: 0.027136683464050293
Batch 37/64 loss: 0.020547986030578613
Batch 38/64 loss: 0.040816664695739746
Batch 39/64 loss: 0.04425078630447388
Batch 40/64 loss: 0.042949020862579346
Batch 41/64 loss: 0.018513500690460205
Batch 42/64 loss: 0.029195547103881836
Batch 43/64 loss: 0.02921539545059204
Batch 44/64 loss: 0.02927422523498535
Batch 45/64 loss: 0.02836388349533081
Batch 46/64 loss: 0.030047297477722168
Batch 47/64 loss: 0.03346443176269531
Batch 48/64 loss: 0.0280001163482666
Batch 49/64 loss: 0.03343230485916138
Batch 50/64 loss: 0.027911365032196045
Batch 51/64 loss: 0.03130406141281128
Batch 52/64 loss: 0.05044996738433838
Batch 53/64 loss: 0.037993550300598145
Batch 54/64 loss: 0.030777573585510254
Batch 55/64 loss: 0.03229403495788574
Batch 56/64 loss: 0.027178645133972168
Batch 57/64 loss: 0.040013909339904785
Batch 58/64 loss: 0.026708364486694336
Batch 59/64 loss: 0.025720834732055664
Batch 60/64 loss: 0.022178590297698975
Batch 61/64 loss: 0.018774747848510742
Batch 62/64 loss: 0.04663276672363281
Batch 63/64 loss: 0.03821003437042236
Batch 64/64 loss: 0.044728100299835205
Epoch 240  Train loss: 0.03248101192362168  Val loss: 0.08865555987734974
Epoch 241
-------------------------------
Batch 1/64 loss: 0.028928816318511963
Batch 2/64 loss: 0.038353800773620605
Batch 3/64 loss: 0.034777700901031494
Batch 4/64 loss: 0.02431267499923706
Batch 5/64 loss: 0.03284263610839844
Batch 6/64 loss: 0.021346092224121094
Batch 7/64 loss: 0.028038203716278076
Batch 8/64 loss: 0.04650145769119263
Batch 9/64 loss: 0.02523815631866455
Batch 10/64 loss: 0.04380643367767334
Batch 11/64 loss: 0.026559293270111084
Batch 12/64 loss: 0.03140699863433838
Batch 13/64 loss: 0.02123028039932251
Batch 14/64 loss: 0.047289133071899414
Batch 15/64 loss: 0.02926701307296753
Batch 16/64 loss: 0.029362261295318604
Batch 17/64 loss: 0.022873759269714355
Batch 18/64 loss: 0.02476811408996582
Batch 19/64 loss: 0.03737974166870117
Batch 20/64 loss: 0.02972543239593506
Batch 21/64 loss: 0.023787617683410645
Batch 22/64 loss: 0.02909719944000244
Batch 23/64 loss: 0.03517431020736694
Batch 24/64 loss: 0.05488705635070801
Batch 25/64 loss: 0.034835755825042725
Batch 26/64 loss: 0.03817039728164673
Batch 27/64 loss: 0.029324233531951904
Batch 28/64 loss: 0.029956519603729248
Batch 29/64 loss: 0.03051745891571045
Batch 30/64 loss: 0.03254896402359009
Batch 31/64 loss: 0.03888845443725586
Batch 32/64 loss: 0.036548078060150146
Batch 33/64 loss: 0.04068666696548462
Batch 34/64 loss: 0.03455245494842529
Batch 35/64 loss: 0.01891392469406128
Batch 36/64 loss: 0.02372431755065918
Batch 37/64 loss: 0.04010510444641113
Batch 38/64 loss: 0.024428069591522217
Batch 39/64 loss: 0.044751882553100586
Batch 40/64 loss: 0.03622192144393921
Batch 41/64 loss: 0.05128830671310425
Batch 42/64 loss: 0.03401428461074829
Batch 43/64 loss: 0.028484225273132324
Batch 44/64 loss: 0.03449702262878418
Batch 45/64 loss: 0.038057029247283936
Batch 46/64 loss: 0.032462358474731445
Batch 47/64 loss: 0.030405879020690918
Batch 48/64 loss: 0.058404624462127686
Batch 49/64 loss: 0.04490816593170166
Batch 50/64 loss: 0.035972654819488525
Batch 51/64 loss: 0.03706848621368408
Batch 52/64 loss: 0.04642409086227417
Batch 53/64 loss: 0.04641848802566528
Batch 54/64 loss: 0.040698349475860596
Batch 55/64 loss: 0.034082770347595215
Batch 56/64 loss: 0.037728309631347656
Batch 57/64 loss: 0.030239760875701904
Batch 58/64 loss: 0.0491676926612854
Batch 59/64 loss: 0.03941088914871216
Batch 60/64 loss: 0.04054075479507446
Batch 61/64 loss: 0.029935002326965332
Batch 62/64 loss: 0.037221670150756836
Batch 63/64 loss: 0.050084054470062256
Batch 64/64 loss: 0.02403128147125244
Epoch 241  Train loss: 0.03492810539170808  Val loss: 0.08963535208882335
Epoch 242
-------------------------------
Batch 1/64 loss: 0.03130429983139038
Batch 2/64 loss: 0.03794968128204346
Batch 3/64 loss: 0.035114288330078125
Batch 4/64 loss: 0.01964282989501953
Batch 5/64 loss: 0.0328291654586792
Batch 6/64 loss: 0.03880077600479126
Batch 7/64 loss: 0.0250931978225708
Batch 8/64 loss: 0.03587895631790161
Batch 9/64 loss: 0.01791703701019287
Batch 10/64 loss: 0.01990610361099243
Batch 11/64 loss: 0.04588949680328369
Batch 12/64 loss: 0.03514581918716431
Batch 13/64 loss: 0.02474266290664673
Batch 14/64 loss: 0.030301928520202637
Batch 15/64 loss: 0.030898571014404297
Batch 16/64 loss: 0.03270810842514038
Batch 17/64 loss: 0.041781485080718994
Batch 18/64 loss: 0.06300914287567139
Batch 19/64 loss: 0.043467938899993896
Batch 20/64 loss: 0.02913886308670044
Batch 21/64 loss: 0.020619630813598633
Batch 22/64 loss: 0.01329505443572998
Batch 23/64 loss: 0.024656713008880615
Batch 24/64 loss: 0.027081787586212158
Batch 25/64 loss: 0.03665059804916382
Batch 26/64 loss: 0.04157179594039917
Batch 27/64 loss: 0.02978891134262085
Batch 28/64 loss: 0.020788908004760742
Batch 29/64 loss: 0.02766650915145874
Batch 30/64 loss: 0.025226950645446777
Batch 31/64 loss: 0.030924499034881592
Batch 32/64 loss: 0.03690516948699951
Batch 33/64 loss: 0.055450618267059326
Batch 34/64 loss: 0.022555410861968994
Batch 35/64 loss: 0.02682936191558838
Batch 36/64 loss: 0.03664994239807129
Batch 37/64 loss: 0.0237463116645813
Batch 38/64 loss: 0.021378517150878906
Batch 39/64 loss: 0.025911331176757812
Batch 40/64 loss: 0.02597910165786743
Batch 41/64 loss: 0.034952402114868164
Batch 42/64 loss: 0.03404909372329712
Batch 43/64 loss: 0.015260577201843262
Batch 44/64 loss: 0.029169142246246338
Batch 45/64 loss: 0.04650163650512695
Batch 46/64 loss: 0.03294295072555542
Batch 47/64 loss: 0.02222996950149536
Batch 48/64 loss: 0.034219324588775635
Batch 49/64 loss: 0.03855752944946289
Batch 50/64 loss: 0.023865818977355957
Batch 51/64 loss: 0.03803694248199463
Batch 52/64 loss: 0.036514103412628174
Batch 53/64 loss: 0.03952288627624512
Batch 54/64 loss: 0.028121888637542725
Batch 55/64 loss: 0.029614686965942383
Batch 56/64 loss: 0.04022866487503052
Batch 57/64 loss: 0.025824427604675293
Batch 58/64 loss: 0.03225785493850708
Batch 59/64 loss: 0.037087976932525635
Batch 60/64 loss: 0.01655346155166626
Batch 61/64 loss: 0.02351909875869751
Batch 62/64 loss: 0.0349583625793457
Batch 63/64 loss: 0.04352879524230957
Batch 64/64 loss: 0.0385056734085083
Epoch 242  Train loss: 0.03155442078908285  Val loss: 0.08930856235248526
Epoch 243
-------------------------------
Batch 1/64 loss: 0.021123290061950684
Batch 2/64 loss: 0.02222740650177002
Batch 3/64 loss: 0.026035010814666748
Batch 4/64 loss: 0.02528280019760132
Batch 5/64 loss: 0.02198207378387451
Batch 6/64 loss: 0.02297341823577881
Batch 7/64 loss: 0.0318257212638855
Batch 8/64 loss: 0.02668595314025879
Batch 9/64 loss: 0.0285797119140625
Batch 10/64 loss: 0.02103954553604126
Batch 11/64 loss: 0.023664772510528564
Batch 12/64 loss: 0.02596336603164673
Batch 13/64 loss: 0.018244147300720215
Batch 14/64 loss: 0.043865323066711426
Batch 15/64 loss: 0.039908647537231445
Batch 16/64 loss: 0.03745412826538086
Batch 17/64 loss: 0.02362966537475586
Batch 18/64 loss: 0.01861262321472168
Batch 19/64 loss: 0.01506662368774414
Batch 20/64 loss: 0.03211832046508789
Batch 21/64 loss: 0.028831005096435547
Batch 22/64 loss: 0.025069117546081543
Batch 23/64 loss: 0.030075252056121826
Batch 24/64 loss: 0.019272267818450928
Batch 25/64 loss: 0.034669578075408936
Batch 26/64 loss: 0.01999586820602417
Batch 27/64 loss: 0.021277010440826416
Batch 28/64 loss: 0.053342580795288086
Batch 29/64 loss: 0.031066596508026123
Batch 30/64 loss: 0.03395038843154907
Batch 31/64 loss: 0.02325892448425293
Batch 32/64 loss: 0.03832364082336426
Batch 33/64 loss: 0.018099308013916016
Batch 34/64 loss: 0.03164559602737427
Batch 35/64 loss: 0.025296330451965332
Batch 36/64 loss: 0.026212692260742188
Batch 37/64 loss: 0.029309749603271484
Batch 38/64 loss: 0.029484212398529053
Batch 39/64 loss: 0.031598448753356934
Batch 40/64 loss: 0.02750098705291748
Batch 41/64 loss: 0.04257476329803467
Batch 42/64 loss: 0.02719169855117798
Batch 43/64 loss: 0.04766249656677246
Batch 44/64 loss: 0.04029214382171631
Batch 45/64 loss: 0.012075304985046387
Batch 46/64 loss: 0.031272947788238525
Batch 47/64 loss: 0.0387723445892334
Batch 48/64 loss: 0.04073888063430786
Batch 49/64 loss: 0.018556714057922363
Batch 50/64 loss: 0.02859485149383545
Batch 51/64 loss: 0.02920854091644287
Batch 52/64 loss: 0.021829605102539062
Batch 53/64 loss: 0.03747075796127319
Batch 54/64 loss: 0.04203826189041138
Batch 55/64 loss: 0.03855431079864502
Batch 56/64 loss: 0.031581223011016846
Batch 57/64 loss: 0.024875998497009277
Batch 58/64 loss: 0.031538188457489014
Batch 59/64 loss: 0.027001500129699707
Batch 60/64 loss: 0.03248220682144165
Batch 61/64 loss: 0.04014420509338379
Batch 62/64 loss: 0.02820485830307007
Batch 63/64 loss: 0.030273795127868652
Batch 64/64 loss: 0.035813987255096436
Epoch 243  Train loss: 0.02940169713076423  Val loss: 0.08812376314012456
Epoch 244
-------------------------------
Batch 1/64 loss: 0.02583324909210205
Batch 2/64 loss: 0.03585082292556763
Batch 3/64 loss: 0.027155756950378418
Batch 4/64 loss: 0.025484681129455566
Batch 5/64 loss: 0.033665239810943604
Batch 6/64 loss: 0.029899299144744873
Batch 7/64 loss: 0.02175295352935791
Batch 8/64 loss: 0.023633718490600586
Batch 9/64 loss: 0.024870991706848145
Batch 10/64 loss: 0.043807029724121094
Batch 11/64 loss: 0.02543199062347412
Batch 12/64 loss: 0.026659727096557617
Batch 13/64 loss: 0.038278162479400635
Batch 14/64 loss: 0.03022634983062744
Batch 15/64 loss: 0.026325762271881104
Batch 16/64 loss: 0.028723716735839844
Batch 17/64 loss: 0.018389463424682617
Batch 18/64 loss: 0.041892290115356445
Batch 19/64 loss: 0.03661543130874634
Batch 20/64 loss: 0.03289914131164551
Batch 21/64 loss: 0.042742013931274414
Batch 22/64 loss: 0.028343617916107178
Batch 23/64 loss: 0.02934342622756958
Batch 24/64 loss: 0.026343703269958496
Batch 25/64 loss: 0.03425395488739014
Batch 26/64 loss: 0.03694117069244385
Batch 27/64 loss: 0.029153645038604736
Batch 28/64 loss: 0.030202150344848633
Batch 29/64 loss: 0.024928748607635498
Batch 30/64 loss: 0.03107696771621704
Batch 31/64 loss: 0.03497314453125
Batch 32/64 loss: 0.024723947048187256
Batch 33/64 loss: 0.029245197772979736
Batch 34/64 loss: 0.03747069835662842
Batch 35/64 loss: 0.03738582134246826
Batch 36/64 loss: 0.04159891605377197
Batch 37/64 loss: 0.01839733123779297
Batch 38/64 loss: 0.028320014476776123
Batch 39/64 loss: 0.02699601650238037
Batch 40/64 loss: 0.03475135564804077
Batch 41/64 loss: 0.03342539072036743
Batch 42/64 loss: 0.027590632438659668
Batch 43/64 loss: 0.044302165508270264
Batch 44/64 loss: 0.03798884153366089
Batch 45/64 loss: 0.030299484729766846
Batch 46/64 loss: 0.02679288387298584
Batch 47/64 loss: 0.032800912857055664
Batch 48/64 loss: 0.03641033172607422
Batch 49/64 loss: 0.04816251993179321
Batch 50/64 loss: 0.03436523675918579
Batch 51/64 loss: 0.03885084390640259
Batch 52/64 loss: 0.029337823390960693
Batch 53/64 loss: 0.028633058071136475
Batch 54/64 loss: 0.02446126937866211
Batch 55/64 loss: 0.037667810916900635
Batch 56/64 loss: 0.02165907621383667
Batch 57/64 loss: 0.03209948539733887
Batch 58/64 loss: 0.019614815711975098
Batch 59/64 loss: 0.03368330001831055
Batch 60/64 loss: 0.032520592212677
Batch 61/64 loss: 0.037573397159576416
Batch 62/64 loss: 0.035059213638305664
Batch 63/64 loss: 0.03623676300048828
Batch 64/64 loss: 0.033278703689575195
Epoch 244  Train loss: 0.03151501954770556  Val loss: 0.09220192645423601
Epoch 245
-------------------------------
Batch 1/64 loss: 0.02934277057647705
Batch 2/64 loss: 0.03061455488204956
Batch 3/64 loss: 0.02576202154159546
Batch 4/64 loss: 0.019492030143737793
Batch 5/64 loss: 0.023268043994903564
Batch 6/64 loss: 0.03445994853973389
Batch 7/64 loss: 0.0233229398727417
Batch 8/64 loss: 0.03870123624801636
Batch 9/64 loss: 0.018203914165496826
Batch 10/64 loss: 0.0465468168258667
Batch 11/64 loss: 0.025381803512573242
Batch 12/64 loss: 0.022824466228485107
Batch 13/64 loss: 0.019434094429016113
Batch 14/64 loss: 0.02520143985748291
Batch 15/64 loss: 0.020381569862365723
Batch 16/64 loss: 0.03644120693206787
Batch 17/64 loss: 0.018281280994415283
Batch 18/64 loss: 0.018202245235443115
Batch 19/64 loss: 0.0261954665184021
Batch 20/64 loss: 0.03164112567901611
Batch 21/64 loss: 0.03199470043182373
Batch 22/64 loss: 0.032378196716308594
Batch 23/64 loss: 0.021325528621673584
Batch 24/64 loss: 0.020669102668762207
Batch 25/64 loss: 0.03890770673751831
Batch 26/64 loss: 0.021456658840179443
Batch 27/64 loss: 0.02095639705657959
Batch 28/64 loss: 0.03382265567779541
Batch 29/64 loss: 0.019321799278259277
Batch 30/64 loss: 0.02017998695373535
Batch 31/64 loss: 0.016128838062286377
Batch 32/64 loss: 0.04758715629577637
Batch 33/64 loss: 0.0251157283782959
Batch 34/64 loss: 0.03091329336166382
Batch 35/64 loss: 0.022447407245635986
Batch 36/64 loss: 0.024200618267059326
Batch 37/64 loss: 0.023027896881103516
Batch 38/64 loss: 0.04024207592010498
Batch 39/64 loss: 0.04112505912780762
Batch 40/64 loss: 0.02528315782546997
Batch 41/64 loss: 0.03381502628326416
Batch 42/64 loss: 0.0335773229598999
Batch 43/64 loss: 0.03537660837173462
Batch 44/64 loss: 0.025908589363098145
Batch 45/64 loss: 0.029940485954284668
Batch 46/64 loss: 0.02911829948425293
Batch 47/64 loss: 0.018439531326293945
Batch 48/64 loss: 0.05360615253448486
Batch 49/64 loss: 0.02830559015274048
Batch 50/64 loss: 0.060910165309906006
Batch 51/64 loss: 0.029316246509552002
Batch 52/64 loss: 0.049811720848083496
Batch 53/64 loss: 0.037386178970336914
Batch 54/64 loss: 0.05127763748168945
Batch 55/64 loss: 0.02471768856048584
Batch 56/64 loss: 0.026902437210083008
Batch 57/64 loss: 0.05037277936935425
Batch 58/64 loss: 0.027521491050720215
Batch 59/64 loss: 0.045445024967193604
Batch 60/64 loss: 0.025281190872192383
Batch 61/64 loss: 0.03302145004272461
Batch 62/64 loss: 0.03648346662521362
Batch 63/64 loss: 0.032185912132263184
Batch 64/64 loss: 0.04656505584716797
Epoch 245  Train loss: 0.03050082711612477  Val loss: 0.08957449963821988
Epoch 246
-------------------------------
Batch 1/64 loss: 0.03338181972503662
Batch 2/64 loss: 0.03366649150848389
Batch 3/64 loss: 0.038551151752471924
Batch 4/64 loss: 0.0328747034072876
Batch 5/64 loss: 0.04674595594406128
Batch 6/64 loss: 0.02786165475845337
Batch 7/64 loss: 0.03176283836364746
Batch 8/64 loss: 0.02485215663909912
Batch 9/64 loss: 0.029695749282836914
Batch 10/64 loss: 0.03755486011505127
Batch 11/64 loss: 0.028301239013671875
Batch 12/64 loss: 0.04905730485916138
Batch 13/64 loss: 0.02815014123916626
Batch 14/64 loss: 0.02446925640106201
Batch 15/64 loss: 0.018880188465118408
Batch 16/64 loss: 0.03284621238708496
Batch 17/64 loss: 0.035904765129089355
Batch 18/64 loss: 0.031127452850341797
Batch 19/64 loss: 0.022488951683044434
Batch 20/64 loss: 0.007944345474243164
Batch 21/64 loss: 0.02632981538772583
Batch 22/64 loss: 0.039062321186065674
Batch 23/64 loss: 0.05290752649307251
Batch 24/64 loss: 0.031135857105255127
Batch 25/64 loss: 0.02954477071762085
Batch 26/64 loss: 0.025135934352874756
Batch 27/64 loss: 0.03743171691894531
Batch 28/64 loss: 0.04055190086364746
Batch 29/64 loss: 0.03653407096862793
Batch 30/64 loss: 0.04091060161590576
Batch 31/64 loss: 0.0333176851272583
Batch 32/64 loss: 0.02955949306488037
Batch 33/64 loss: 0.026820659637451172
Batch 34/64 loss: 0.02712571620941162
Batch 35/64 loss: 0.0337560772895813
Batch 36/64 loss: 0.018902957439422607
Batch 37/64 loss: 0.02518463134765625
Batch 38/64 loss: 0.02519392967224121
Batch 39/64 loss: 0.026965618133544922
Batch 40/64 loss: 0.026702165603637695
Batch 41/64 loss: 0.018884003162384033
Batch 42/64 loss: 0.02761256694793701
Batch 43/64 loss: 0.022912263870239258
Batch 44/64 loss: 0.02439558506011963
Batch 45/64 loss: 0.02870422601699829
Batch 46/64 loss: 0.016940295696258545
Batch 47/64 loss: 0.0313681960105896
Batch 48/64 loss: 0.01965731382369995
Batch 49/64 loss: 0.041037559509277344
Batch 50/64 loss: 0.03065580129623413
Batch 51/64 loss: 0.037431955337524414
Batch 52/64 loss: 0.03563129901885986
Batch 53/64 loss: 0.0422135591506958
Batch 54/64 loss: 0.033704400062561035
Batch 55/64 loss: 0.042654991149902344
Batch 56/64 loss: 0.02752608060836792
Batch 57/64 loss: 0.025705814361572266
Batch 58/64 loss: 0.02988201379776001
Batch 59/64 loss: 0.03391456604003906
Batch 60/64 loss: 0.03305119276046753
Batch 61/64 loss: 0.03910642862319946
Batch 62/64 loss: 0.02614814043045044
Batch 63/64 loss: 0.028704941272735596
Batch 64/64 loss: 0.027131319046020508
Epoch 246  Train loss: 0.03086089994393143  Val loss: 0.08979131347944647
Epoch 247
-------------------------------
Batch 1/64 loss: 0.02324777841567993
Batch 2/64 loss: 0.018633782863616943
Batch 3/64 loss: 0.02674388885498047
Batch 4/64 loss: 0.01697063446044922
Batch 5/64 loss: 0.016938984394073486
Batch 6/64 loss: 0.017155766487121582
Batch 7/64 loss: 0.010701417922973633
Batch 8/64 loss: 0.0193253755569458
Batch 9/64 loss: 0.024099647998809814
Batch 10/64 loss: 0.028974592685699463
Batch 11/64 loss: 0.03402841091156006
Batch 12/64 loss: 0.021910667419433594
Batch 13/64 loss: 0.036247968673706055
Batch 14/64 loss: 0.03827637434005737
Batch 15/64 loss: 0.011593401432037354
Batch 16/64 loss: 0.017724037170410156
Batch 17/64 loss: 0.02448868751525879
Batch 18/64 loss: 0.018512606620788574
Batch 19/64 loss: 0.014636814594268799
Batch 20/64 loss: 0.03295886516571045
Batch 21/64 loss: 0.04364943504333496
Batch 22/64 loss: 0.030688583850860596
Batch 23/64 loss: 0.026605844497680664
Batch 24/64 loss: 0.028968214988708496
Batch 25/64 loss: 0.025120139122009277
Batch 26/64 loss: 0.03733724355697632
Batch 27/64 loss: 0.03328657150268555
Batch 28/64 loss: 0.028199195861816406
Batch 29/64 loss: 0.022118568420410156
Batch 30/64 loss: 0.019611895084381104
Batch 31/64 loss: 0.03468126058578491
Batch 32/64 loss: 0.05128920078277588
Batch 33/64 loss: 0.03369879722595215
Batch 34/64 loss: 0.02288520336151123
Batch 35/64 loss: 0.0311279296875
Batch 36/64 loss: 0.04110443592071533
Batch 37/64 loss: 0.03154182434082031
Batch 38/64 loss: 0.027669668197631836
Batch 39/64 loss: 0.03397536277770996
Batch 40/64 loss: 0.02673250436782837
Batch 41/64 loss: 0.019831418991088867
Batch 42/64 loss: 0.029498755931854248
Batch 43/64 loss: 0.033817827701568604
Batch 44/64 loss: 0.01662623882293701
Batch 45/64 loss: 0.027754664421081543
Batch 46/64 loss: 0.013794898986816406
Batch 47/64 loss: 0.024192452430725098
Batch 48/64 loss: 0.022924602031707764
Batch 49/64 loss: 0.02316385507583618
Batch 50/64 loss: 0.026267647743225098
Batch 51/64 loss: 0.02563631534576416
Batch 52/64 loss: 0.013583004474639893
Batch 53/64 loss: 0.043814778327941895
Batch 54/64 loss: 0.04257488250732422
Batch 55/64 loss: 0.013840734958648682
Batch 56/64 loss: 0.037389934062957764
Batch 57/64 loss: 0.025967121124267578
Batch 58/64 loss: 0.04325509071350098
Batch 59/64 loss: 0.026128113269805908
Batch 60/64 loss: 0.030355989933013916
Batch 61/64 loss: 0.0276908278465271
Batch 62/64 loss: 0.03896772861480713
Batch 63/64 loss: 0.024518132209777832
Batch 64/64 loss: 0.01560664176940918
Epoch 247  Train loss: 0.027086456149232153  Val loss: 0.08792468116865125
Epoch 248
-------------------------------
Batch 1/64 loss: 0.013479650020599365
Batch 2/64 loss: 0.019501566886901855
Batch 3/64 loss: 0.019080698490142822
Batch 4/64 loss: 0.01772671937942505
Batch 5/64 loss: 0.016064763069152832
Batch 6/64 loss: 0.04465740919113159
Batch 7/64 loss: 0.026232421398162842
Batch 8/64 loss: 0.016620516777038574
Batch 9/64 loss: 0.022197365760803223
Batch 10/64 loss: 0.035660505294799805
Batch 11/64 loss: 0.017160356044769287
Batch 12/64 loss: 0.017312347888946533
Batch 13/64 loss: 0.03612154722213745
Batch 14/64 loss: 0.019023597240447998
Batch 15/64 loss: 0.029066920280456543
Batch 16/64 loss: 0.01988673210144043
Batch 17/64 loss: 0.03345799446105957
Batch 18/64 loss: 0.017692506313323975
Batch 19/64 loss: 0.033291637897491455
Batch 20/64 loss: 0.019517958164215088
Batch 21/64 loss: 0.01651817560195923
Batch 22/64 loss: 0.03355640172958374
Batch 23/64 loss: 0.024579167366027832
Batch 24/64 loss: 0.026238322257995605
Batch 25/64 loss: 0.03730058670043945
Batch 26/64 loss: 0.026776552200317383
Batch 27/64 loss: 0.03588581085205078
Batch 28/64 loss: 0.020711243152618408
Batch 29/64 loss: 0.031540632247924805
Batch 30/64 loss: 0.0422673225402832
Batch 31/64 loss: 0.024671077728271484
Batch 32/64 loss: 0.03925514221191406
Batch 33/64 loss: 0.03245401382446289
Batch 34/64 loss: 0.03578746318817139
Batch 35/64 loss: 0.027441740036010742
Batch 36/64 loss: 0.03542107343673706
Batch 37/64 loss: 0.03353071212768555
Batch 38/64 loss: 0.021983444690704346
Batch 39/64 loss: 0.023836374282836914
Batch 40/64 loss: 0.020986616611480713
Batch 41/64 loss: 0.03257519006729126
Batch 42/64 loss: 0.02978438138961792
Batch 43/64 loss: 0.0346454381942749
Batch 44/64 loss: 0.024788737297058105
Batch 45/64 loss: 0.025691866874694824
Batch 46/64 loss: 0.03294473886489868
Batch 47/64 loss: 0.027470707893371582
Batch 48/64 loss: 0.029019415378570557
Batch 49/64 loss: 0.028737902641296387
Batch 50/64 loss: 0.046084582805633545
Batch 51/64 loss: 0.04321712255477905
Batch 52/64 loss: 0.023003339767456055
Batch 53/64 loss: 0.048316240310668945
Batch 54/64 loss: 0.025772809982299805
Batch 55/64 loss: 0.02528858184814453
Batch 56/64 loss: 0.033605337142944336
Batch 57/64 loss: 0.03443276882171631
Batch 58/64 loss: 0.03272062540054321
Batch 59/64 loss: 0.03291153907775879
Batch 60/64 loss: 0.03309226036071777
Batch 61/64 loss: 0.018986821174621582
Batch 62/64 loss: 0.027293741703033447
Batch 63/64 loss: 0.028798699378967285
Batch 64/64 loss: 0.026186645030975342
Epoch 248  Train loss: 0.028287339911741368  Val loss: 0.08775013193641741
Epoch 249
-------------------------------
Batch 1/64 loss: 0.02256143093109131
Batch 2/64 loss: 0.022797584533691406
Batch 3/64 loss: 0.025377988815307617
Batch 4/64 loss: 0.014696717262268066
Batch 5/64 loss: 0.022378981113433838
Batch 6/64 loss: 0.025731325149536133
Batch 7/64 loss: 0.02843165397644043
Batch 8/64 loss: 0.012906312942504883
Batch 9/64 loss: 0.03471094369888306
Batch 10/64 loss: 0.05128002166748047
Batch 11/64 loss: 0.017194747924804688
Batch 12/64 loss: 0.030392885208129883
Batch 13/64 loss: 0.022366821765899658
Batch 14/64 loss: 0.027560174465179443
Batch 15/64 loss: 0.019937634468078613
Batch 16/64 loss: 0.023538172245025635
Batch 17/64 loss: 0.03738278150558472
Batch 18/64 loss: 0.022119104862213135
Batch 19/64 loss: 0.019903123378753662
Batch 20/64 loss: 0.01969844102859497
Batch 21/64 loss: 0.03005540370941162
Batch 22/64 loss: 0.013799548149108887
Batch 23/64 loss: 0.029512226581573486
Batch 24/64 loss: 0.03348726034164429
Batch 25/64 loss: 0.039101243019104004
Batch 26/64 loss: 0.034297823905944824
Batch 27/64 loss: 0.017857849597930908
Batch 28/64 loss: 0.02366691827774048
Batch 29/64 loss: 0.03126758337020874
Batch 30/64 loss: 0.040614962577819824
Batch 31/64 loss: 0.018726348876953125
Batch 32/64 loss: 0.02162182331085205
Batch 33/64 loss: 0.02713143825531006
Batch 34/64 loss: 0.02801424264907837
Batch 35/64 loss: 0.03206372261047363
Batch 36/64 loss: 0.013689577579498291
Batch 37/64 loss: 0.03809601068496704
Batch 38/64 loss: 0.03335976600646973
Batch 39/64 loss: 0.025577068328857422
Batch 40/64 loss: 0.028753221035003662
Batch 41/64 loss: 0.03480345010757446
Batch 42/64 loss: 0.025483369827270508
Batch 43/64 loss: 0.03277808427810669
Batch 44/64 loss: 0.03206634521484375
Batch 45/64 loss: 0.024721860885620117
Batch 46/64 loss: 0.038024306297302246
Batch 47/64 loss: 0.01573503017425537
Batch 48/64 loss: 0.032889604568481445
Batch 49/64 loss: 0.028419554233551025
Batch 50/64 loss: 0.021049439907073975
Batch 51/64 loss: 0.02504897117614746
Batch 52/64 loss: 0.027497529983520508
Batch 53/64 loss: 0.02654588222503662
Batch 54/64 loss: 0.018830537796020508
Batch 55/64 loss: 0.03123795986175537
Batch 56/64 loss: 0.028076767921447754
Batch 57/64 loss: 0.019673645496368408
Batch 58/64 loss: 0.017860233783721924
Batch 59/64 loss: 0.04032731056213379
Batch 60/64 loss: 0.027370333671569824
Batch 61/64 loss: 0.03270280361175537
Batch 62/64 loss: 0.03508025407791138
Batch 63/64 loss: 0.026282668113708496
Batch 64/64 loss: 0.02025383710861206
Epoch 249  Train loss: 0.0269703091359606  Val loss: 0.0869076030770528
Saving best model, epoch: 249
Epoch 250
-------------------------------
Batch 1/64 loss: 0.026100218296051025
Batch 2/64 loss: 0.03072679042816162
Batch 3/64 loss: 0.013978421688079834
Batch 4/64 loss: 0.008406221866607666
Batch 5/64 loss: 0.021839916706085205
Batch 6/64 loss: 0.031047344207763672
Batch 7/64 loss: 0.023460566997528076
Batch 8/64 loss: 0.025052785873413086
Batch 9/64 loss: 0.022129058837890625
Batch 10/64 loss: 0.031979918479919434
Batch 11/64 loss: 0.029826641082763672
Batch 12/64 loss: 0.01779770851135254
Batch 13/64 loss: 0.01947420835494995
Batch 14/64 loss: 0.04885202646255493
Batch 15/64 loss: 0.03286576271057129
Batch 16/64 loss: 0.020453155040740967
Batch 17/64 loss: 0.022501707077026367
Batch 18/64 loss: 0.04178732633590698
Batch 19/64 loss: 0.017154216766357422
Batch 20/64 loss: 0.031806766986846924
Batch 21/64 loss: 0.0209578275680542
Batch 22/64 loss: 0.01845228672027588
Batch 23/64 loss: 0.026174187660217285
Batch 24/64 loss: 0.03117990493774414
Batch 25/64 loss: 0.022998452186584473
Batch 26/64 loss: 0.030612409114837646
Batch 27/64 loss: 0.03790402412414551
Batch 28/64 loss: 0.03008025884628296
Batch 29/64 loss: 0.018962085247039795
Batch 30/64 loss: 0.035284221172332764
Batch 31/64 loss: 0.034198760986328125
Batch 32/64 loss: 0.02485203742980957
Batch 33/64 loss: 0.04908311367034912
Batch 34/64 loss: 0.029286444187164307
Batch 35/64 loss: 0.031925201416015625
Batch 36/64 loss: 0.02391284704208374
Batch 37/64 loss: 0.03118264675140381
Batch 38/64 loss: 0.041416704654693604
Batch 39/64 loss: 0.03160637617111206
Batch 40/64 loss: 0.02746284008026123
Batch 41/64 loss: 0.04484456777572632
Batch 42/64 loss: 0.036180973052978516
Batch 43/64 loss: 0.024117469787597656
Batch 44/64 loss: 0.029960572719573975
Batch 45/64 loss: 0.023281216621398926
Batch 46/64 loss: 0.02026212215423584
Batch 47/64 loss: 0.01725590229034424
Batch 48/64 loss: 0.021284937858581543
Batch 49/64 loss: 0.011888384819030762
Batch 50/64 loss: 0.041660308837890625
Batch 51/64 loss: 0.019155383110046387
Batch 52/64 loss: 0.01800537109375
Batch 53/64 loss: 0.033943355083465576
Batch 54/64 loss: 0.02620035409927368
Batch 55/64 loss: 0.024242103099822998
Batch 56/64 loss: 0.029874801635742188
Batch 57/64 loss: 0.028788864612579346
Batch 58/64 loss: 0.030279695987701416
Batch 59/64 loss: 0.03293865919113159
Batch 60/64 loss: 0.02875131368637085
Batch 61/64 loss: 0.029796183109283447
Batch 62/64 loss: 0.026304423809051514
Batch 63/64 loss: 0.02282625436782837
Batch 64/64 loss: 0.03261077404022217
Epoch 250  Train loss: 0.02762470105115105  Val loss: 0.09084450133477699
Epoch 251
-------------------------------
Batch 1/64 loss: 0.03259110450744629
Batch 2/64 loss: 0.028581976890563965
Batch 3/64 loss: 0.02833014726638794
Batch 4/64 loss: 0.030189156532287598
Batch 5/64 loss: 0.03126060962677002
Batch 6/64 loss: 0.014574706554412842
Batch 7/64 loss: 0.016361594200134277
Batch 8/64 loss: 0.02790236473083496
Batch 9/64 loss: 0.035048604011535645
Batch 10/64 loss: 0.022932887077331543
Batch 11/64 loss: 0.014204084873199463
Batch 12/64 loss: 0.03734093904495239
Batch 13/64 loss: 0.02757859230041504
Batch 14/64 loss: 0.017916321754455566
Batch 15/64 loss: 0.026372134685516357
Batch 16/64 loss: 0.04101604223251343
Batch 17/64 loss: 0.027926862239837646
Batch 18/64 loss: 0.016059935092926025
Batch 19/64 loss: 0.02632749080657959
Batch 20/64 loss: 0.023450911045074463
Batch 21/64 loss: 0.042221665382385254
Batch 22/64 loss: 0.025365233421325684
Batch 23/64 loss: 0.021261930465698242
Batch 24/64 loss: 0.03169846534729004
Batch 25/64 loss: 0.03137469291687012
Batch 26/64 loss: 0.04116678237915039
Batch 27/64 loss: 0.029891014099121094
Batch 28/64 loss: 0.03473186492919922
Batch 29/64 loss: 0.023540496826171875
Batch 30/64 loss: 0.0242578387260437
Batch 31/64 loss: 0.02508366107940674
Batch 32/64 loss: 0.013083338737487793
Batch 33/64 loss: 0.041030704975128174
Batch 34/64 loss: 0.02669668197631836
Batch 35/64 loss: 0.030250251293182373
Batch 36/64 loss: 0.025336921215057373
Batch 37/64 loss: 0.030768394470214844
Batch 38/64 loss: 0.016022324562072754
Batch 39/64 loss: 0.02372044324874878
Batch 40/64 loss: 0.02267080545425415
Batch 41/64 loss: 0.027568578720092773
Batch 42/64 loss: 0.02969580888748169
Batch 43/64 loss: 0.038691043853759766
Batch 44/64 loss: 0.030781030654907227
Batch 45/64 loss: 0.03646695613861084
Batch 46/64 loss: 0.03495347499847412
Batch 47/64 loss: 0.025874197483062744
Batch 48/64 loss: 0.03159499168395996
Batch 49/64 loss: 0.03616410493850708
Batch 50/64 loss: 0.029535293579101562
Batch 51/64 loss: 0.03117203712463379
Batch 52/64 loss: 0.019761323928833008
Batch 53/64 loss: 0.018572509288787842
Batch 54/64 loss: 0.03474533557891846
Batch 55/64 loss: 0.026300668716430664
Batch 56/64 loss: 0.03384435176849365
Batch 57/64 loss: 0.026987969875335693
Batch 58/64 loss: 0.055301785469055176
Batch 59/64 loss: 0.03220927715301514
Batch 60/64 loss: 0.020440280437469482
Batch 61/64 loss: 0.018717467784881592
Batch 62/64 loss: 0.038339972496032715
Batch 63/64 loss: 0.018722951412200928
Batch 64/64 loss: 0.023645877838134766
Epoch 251  Train loss: 0.028240247801238415  Val loss: 0.08786730217360139
Epoch 252
-------------------------------
Batch 1/64 loss: 0.021894335746765137
Batch 2/64 loss: 0.021839141845703125
Batch 3/64 loss: 0.01651686429977417
Batch 4/64 loss: 0.015011131763458252
Batch 5/64 loss: 0.029670417308807373
Batch 6/64 loss: 0.03333765268325806
Batch 7/64 loss: 0.021744966506958008
Batch 8/64 loss: 0.026304781436920166
Batch 9/64 loss: 0.05012744665145874
Batch 10/64 loss: 0.03559112548828125
Batch 11/64 loss: 0.01468890905380249
Batch 12/64 loss: 0.018470168113708496
Batch 13/64 loss: 0.013794422149658203
Batch 14/64 loss: 0.026446819305419922
Batch 15/64 loss: 0.02840554714202881
Batch 16/64 loss: 0.015350520610809326
Batch 17/64 loss: 0.03317999839782715
Batch 18/64 loss: 0.025553762912750244
Batch 19/64 loss: 0.03643643856048584
Batch 20/64 loss: 0.01214754581451416
Batch 21/64 loss: 0.02683490514755249
Batch 22/64 loss: 0.03108382225036621
Batch 23/64 loss: 0.03119051456451416
Batch 24/64 loss: 0.037480175495147705
Batch 25/64 loss: 0.03600031137466431
Batch 26/64 loss: 0.038999855518341064
Batch 27/64 loss: 0.02451843023300171
Batch 28/64 loss: 0.032437026500701904
Batch 29/64 loss: 0.0224229097366333
Batch 30/64 loss: 0.0328330397605896
Batch 31/64 loss: 0.01704704761505127
Batch 32/64 loss: 0.025896906852722168
Batch 33/64 loss: 0.025798261165618896
Batch 34/64 loss: 0.03321647644042969
Batch 35/64 loss: 0.013095855712890625
Batch 36/64 loss: 0.02465689182281494
Batch 37/64 loss: 0.03185635805130005
Batch 38/64 loss: 0.028661727905273438
Batch 39/64 loss: 0.021669864654541016
Batch 40/64 loss: 0.03410220146179199
Batch 41/64 loss: 0.016680479049682617
Batch 42/64 loss: 0.028742194175720215
Batch 43/64 loss: 0.018790245056152344
Batch 44/64 loss: 0.01339876651763916
Batch 45/64 loss: 0.03482335805892944
Batch 46/64 loss: 0.021478235721588135
Batch 47/64 loss: 0.026880741119384766
Batch 48/64 loss: 0.018131613731384277
Batch 49/64 loss: 0.040121495723724365
Batch 50/64 loss: 0.018512487411499023
Batch 51/64 loss: 0.034297823905944824
Batch 52/64 loss: 0.02139115333557129
Batch 53/64 loss: 0.028630971908569336
Batch 54/64 loss: 0.0366060733795166
Batch 55/64 loss: 0.02249467372894287
Batch 56/64 loss: 0.018785953521728516
Batch 57/64 loss: 0.02810347080230713
Batch 58/64 loss: 0.03681492805480957
Batch 59/64 loss: 0.027015268802642822
Batch 60/64 loss: 0.042745113372802734
Batch 61/64 loss: 0.026150405406951904
Batch 62/64 loss: 0.02442711591720581
Batch 63/64 loss: 0.037411272525787354
Batch 64/64 loss: 0.027820825576782227
Epoch 252  Train loss: 0.02681747511321423  Val loss: 0.0878735270696817
Epoch 253
-------------------------------
Batch 1/64 loss: 0.024134933948516846
Batch 2/64 loss: 0.020171046257019043
Batch 3/64 loss: 0.03034532070159912
Batch 4/64 loss: 0.018764853477478027
Batch 5/64 loss: 0.016159534454345703
Batch 6/64 loss: 0.01412200927734375
Batch 7/64 loss: 0.01021045446395874
Batch 8/64 loss: 0.02051854133605957
Batch 9/64 loss: 0.02217310667037964
Batch 10/64 loss: 0.023414313793182373
Batch 11/64 loss: 0.029093623161315918
Batch 12/64 loss: 0.023269712924957275
Batch 13/64 loss: 0.02792072296142578
Batch 14/64 loss: 0.027616500854492188
Batch 15/64 loss: 0.0324898362159729
Batch 16/64 loss: 0.027962207794189453
Batch 17/64 loss: 0.026696443557739258
Batch 18/64 loss: 0.03362685441970825
Batch 19/64 loss: 0.036064863204956055
Batch 20/64 loss: 0.024039626121520996
Batch 21/64 loss: 0.0261765718460083
Batch 22/64 loss: 0.030588626861572266
Batch 23/64 loss: 0.02939075231552124
Batch 24/64 loss: 0.038291335105895996
Batch 25/64 loss: 0.03512537479400635
Batch 26/64 loss: 0.0286865234375
Batch 27/64 loss: 0.02636408805847168
Batch 28/64 loss: 0.013416409492492676
Batch 29/64 loss: 0.03128623962402344
Batch 30/64 loss: 0.03799480199813843
Batch 31/64 loss: 0.023754596710205078
Batch 32/64 loss: 0.04797488451004028
Batch 33/64 loss: 0.030766606330871582
Batch 34/64 loss: 0.028628170490264893
Batch 35/64 loss: 0.02673715353012085
Batch 36/64 loss: 0.03215545415878296
Batch 37/64 loss: 0.04192131757736206
Batch 38/64 loss: 0.01936328411102295
Batch 39/64 loss: 0.022817134857177734
Batch 40/64 loss: 0.026637256145477295
Batch 41/64 loss: 0.026956796646118164
Batch 42/64 loss: 0.024265170097351074
Batch 43/64 loss: 0.04456716775894165
Batch 44/64 loss: 0.024855375289916992
Batch 45/64 loss: 0.020681381225585938
Batch 46/64 loss: 0.020710885524749756
Batch 47/64 loss: 0.03084719181060791
Batch 48/64 loss: 0.041837215423583984
Batch 49/64 loss: 0.03408634662628174
Batch 50/64 loss: 0.020100712776184082
Batch 51/64 loss: 0.04115021228790283
Batch 52/64 loss: 0.018685221672058105
Batch 53/64 loss: 0.023948967456817627
Batch 54/64 loss: 0.015666484832763672
Batch 55/64 loss: 0.03284192085266113
Batch 56/64 loss: 0.05514371395111084
Batch 57/64 loss: 0.018474102020263672
Batch 58/64 loss: 0.039713263511657715
Batch 59/64 loss: 0.014366865158081055
Batch 60/64 loss: 0.019990205764770508
Batch 61/64 loss: 0.028508782386779785
Batch 62/64 loss: 0.03084564208984375
Batch 63/64 loss: 0.024753689765930176
Batch 64/64 loss: 0.035443007946014404
Epoch 253  Train loss: 0.02770902993632298  Val loss: 0.08826296272146743
Epoch 254
-------------------------------
Batch 1/64 loss: 0.025726318359375
Batch 2/64 loss: 0.02263021469116211
Batch 3/64 loss: 0.024175047874450684
Batch 4/64 loss: 0.01591050624847412
Batch 5/64 loss: 0.011493027210235596
Batch 6/64 loss: 0.01836007833480835
Batch 7/64 loss: 0.01631951332092285
Batch 8/64 loss: 0.0201951265335083
Batch 9/64 loss: 0.015163540840148926
Batch 10/64 loss: 0.03625386953353882
Batch 11/64 loss: 0.0195578932762146
Batch 12/64 loss: 0.026234984397888184
Batch 13/64 loss: 0.03126102685928345
Batch 14/64 loss: 0.02012181282043457
Batch 15/64 loss: 0.023405015468597412
Batch 16/64 loss: 0.02428436279296875
Batch 17/64 loss: 0.0324518084526062
Batch 18/64 loss: 0.04695779085159302
Batch 19/64 loss: 0.024487197399139404
Batch 20/64 loss: 0.028881192207336426
Batch 21/64 loss: 0.03226703405380249
Batch 22/64 loss: 0.04073852300643921
Batch 23/64 loss: 0.014630675315856934
Batch 24/64 loss: 0.020168423652648926
Batch 25/64 loss: 0.024115920066833496
Batch 26/64 loss: 0.020139575004577637
Batch 27/64 loss: 0.032670021057128906
Batch 28/64 loss: 0.034555912017822266
Batch 29/64 loss: 0.022035658359527588
Batch 30/64 loss: 0.033192574977874756
Batch 31/64 loss: 0.03257179260253906
Batch 32/64 loss: 0.0329289436340332
Batch 33/64 loss: 0.007091343402862549
Batch 34/64 loss: 0.02841240167617798
Batch 35/64 loss: 0.016575515270233154
Batch 36/64 loss: 0.03506124019622803
Batch 37/64 loss: 0.026244819164276123
Batch 38/64 loss: 0.015620291233062744
Batch 39/64 loss: 0.02825629711151123
Batch 40/64 loss: 0.027928829193115234
Batch 41/64 loss: 0.02517467737197876
Batch 42/64 loss: 0.03326350450515747
Batch 43/64 loss: 0.03300696611404419
Batch 44/64 loss: 0.04447287321090698
Batch 45/64 loss: 0.031304121017456055
Batch 46/64 loss: 0.02126896381378174
Batch 47/64 loss: 0.018651902675628662
Batch 48/64 loss: 0.025495827198028564
Batch 49/64 loss: 0.02360767126083374
Batch 50/64 loss: 0.03385329246520996
Batch 51/64 loss: 0.02904653549194336
Batch 52/64 loss: 0.024143636226654053
Batch 53/64 loss: 0.02753770351409912
Batch 54/64 loss: 0.0182228684425354
Batch 55/64 loss: 0.0243532657623291
Batch 56/64 loss: 0.01879328489303589
Batch 57/64 loss: 0.028037309646606445
Batch 58/64 loss: 0.033971965312957764
Batch 59/64 loss: 0.024575233459472656
Batch 60/64 loss: 0.027040958404541016
Batch 61/64 loss: 0.014680862426757812
Batch 62/64 loss: 0.027223944664001465
Batch 63/64 loss: 0.022732973098754883
Batch 64/64 loss: 0.018580913543701172
Epoch 254  Train loss: 0.025623092464372225  Val loss: 0.08842416645325336
Epoch 255
-------------------------------
Batch 1/64 loss: 0.015099942684173584
Batch 2/64 loss: 0.016310393810272217
Batch 3/64 loss: 0.021083950996398926
Batch 4/64 loss: 0.024175584316253662
Batch 5/64 loss: 0.013453483581542969
Batch 6/64 loss: 0.03234386444091797
Batch 7/64 loss: 0.024540960788726807
Batch 8/64 loss: 0.03031134605407715
Batch 9/64 loss: 0.03686946630477905
Batch 10/64 loss: 0.021274805068969727
Batch 11/64 loss: 0.04004561901092529
Batch 12/64 loss: 0.02240508794784546
Batch 13/64 loss: 0.03298079967498779
Batch 14/64 loss: 0.01890653371810913
Batch 15/64 loss: 0.014238893985748291
Batch 16/64 loss: 0.026464462280273438
Batch 17/64 loss: 0.01654714345932007
Batch 18/64 loss: 0.020319223403930664
Batch 19/64 loss: 0.041110336780548096
Batch 20/64 loss: 0.013216614723205566
Batch 21/64 loss: 0.03210246562957764
Batch 22/64 loss: 0.02074265480041504
Batch 23/64 loss: 0.021666765213012695
Batch 24/64 loss: 0.02826857566833496
Batch 25/64 loss: 0.019429504871368408
Batch 26/64 loss: 0.023666560649871826
Batch 27/64 loss: 0.027993738651275635
Batch 28/64 loss: 0.013293981552124023
Batch 29/64 loss: 0.019092917442321777
Batch 30/64 loss: 0.03063303232192993
Batch 31/64 loss: 0.03532588481903076
Batch 32/64 loss: 0.027333378791809082
Batch 33/64 loss: 0.014892160892486572
Batch 34/64 loss: 0.02379518747329712
Batch 35/64 loss: 0.032462894916534424
Batch 36/64 loss: 0.021946966648101807
Batch 37/64 loss: 0.022095680236816406
Batch 38/64 loss: 0.012264132499694824
Batch 39/64 loss: 0.02833479642868042
Batch 40/64 loss: 0.023566484451293945
Batch 41/64 loss: 0.03593301773071289
Batch 42/64 loss: 0.021341443061828613
Batch 43/64 loss: 0.03002035617828369
Batch 44/64 loss: 0.02176874876022339
Batch 45/64 loss: 0.06176948547363281
Batch 46/64 loss: 0.015777766704559326
Batch 47/64 loss: 0.030119776725769043
Batch 48/64 loss: 0.02137213945388794
Batch 49/64 loss: 0.03126394748687744
Batch 50/64 loss: 0.024439752101898193
Batch 51/64 loss: 0.025783836841583252
Batch 52/64 loss: 0.029945671558380127
Batch 53/64 loss: 0.025797724723815918
Batch 54/64 loss: 0.01808995008468628
Batch 55/64 loss: 0.011234045028686523
Batch 56/64 loss: 0.028416812419891357
Batch 57/64 loss: 0.02133774757385254
Batch 58/64 loss: 0.015475571155548096
Batch 59/64 loss: 0.04066067934036255
Batch 60/64 loss: 0.036041855812072754
Batch 61/64 loss: 0.03541529178619385
Batch 62/64 loss: 0.02226322889328003
Batch 63/64 loss: 0.021639883518218994
Batch 64/64 loss: 0.020021498203277588
Epoch 255  Train loss: 0.025122056989108816  Val loss: 0.08914936387661807
Epoch 256
-------------------------------
Batch 1/64 loss: 0.01776421070098877
Batch 2/64 loss: 0.017676949501037598
Batch 3/64 loss: 0.03174793720245361
Batch 4/64 loss: 0.02046787738800049
Batch 5/64 loss: 0.02432572841644287
Batch 6/64 loss: 0.019551753997802734
Batch 7/64 loss: 0.010949373245239258
Batch 8/64 loss: 0.01921236515045166
Batch 9/64 loss: 0.0275917649269104
Batch 10/64 loss: 0.026781737804412842
Batch 11/64 loss: 0.02841198444366455
Batch 12/64 loss: 0.01672738790512085
Batch 13/64 loss: 0.02609848976135254
Batch 14/64 loss: 0.022514760494232178
Batch 15/64 loss: 0.011070609092712402
Batch 16/64 loss: 0.03087681531906128
Batch 17/64 loss: 0.02196323871612549
Batch 18/64 loss: 0.015518784523010254
Batch 19/64 loss: 0.020765602588653564
Batch 20/64 loss: 0.017228662967681885
Batch 21/64 loss: 0.024805963039398193
Batch 22/64 loss: 0.025169432163238525
Batch 23/64 loss: 0.05161947011947632
Batch 24/64 loss: 0.02066349983215332
Batch 25/64 loss: 0.03486776351928711
Batch 26/64 loss: 0.017511606216430664
Batch 27/64 loss: 0.014539718627929688
Batch 28/64 loss: 0.02371978759765625
Batch 29/64 loss: 0.02422398328781128
Batch 30/64 loss: 0.020777344703674316
Batch 31/64 loss: 0.0224916934967041
Batch 32/64 loss: 0.015589416027069092
Batch 33/64 loss: 0.020747780799865723
Batch 34/64 loss: 0.03480982780456543
Batch 35/64 loss: 0.02809661626815796
Batch 36/64 loss: 0.021409809589385986
Batch 37/64 loss: 0.018010497093200684
Batch 38/64 loss: 0.013396084308624268
Batch 39/64 loss: 0.032081782817840576
Batch 40/64 loss: 0.028085947036743164
Batch 41/64 loss: 0.02612704038619995
Batch 42/64 loss: 0.03962010145187378
Batch 43/64 loss: 0.021828770637512207
Batch 44/64 loss: 0.025339961051940918
Batch 45/64 loss: 0.030696332454681396
Batch 46/64 loss: 0.034024953842163086
Batch 47/64 loss: 0.03925621509552002
Batch 48/64 loss: 0.029088377952575684
Batch 49/64 loss: 0.02573227882385254
Batch 50/64 loss: 0.014545679092407227
Batch 51/64 loss: 0.024171948432922363
Batch 52/64 loss: 0.025082707405090332
Batch 53/64 loss: 0.015781760215759277
Batch 54/64 loss: 0.025234699249267578
Batch 55/64 loss: 0.021960854530334473
Batch 56/64 loss: 0.02652597427368164
Batch 57/64 loss: 0.03380924463272095
Batch 58/64 loss: 0.04128921031951904
Batch 59/64 loss: 0.02857106924057007
Batch 60/64 loss: 0.023122727870941162
Batch 61/64 loss: 0.02804619073867798
Batch 62/64 loss: 0.0348241925239563
Batch 63/64 loss: 0.01621919870376587
Batch 64/64 loss: 0.010022163391113281
Epoch 256  Train loss: 0.024443610509236654  Val loss: 0.08796325541034188
Epoch 257
-------------------------------
Batch 1/64 loss: 0.02047199010848999
Batch 2/64 loss: 0.025647282600402832
Batch 3/64 loss: 0.016136586666107178
Batch 4/64 loss: 0.023799777030944824
Batch 5/64 loss: 0.028254032135009766
Batch 6/64 loss: 0.019098341464996338
Batch 7/64 loss: 0.02698284387588501
Batch 8/64 loss: 0.028165042400360107
Batch 9/64 loss: 0.025172531604766846
Batch 10/64 loss: 0.034489214420318604
Batch 11/64 loss: 0.016634047031402588
Batch 12/64 loss: 0.03692054748535156
Batch 13/64 loss: 0.04708850383758545
Batch 14/64 loss: 0.014208972454071045
Batch 15/64 loss: 0.01898294687271118
Batch 16/64 loss: 0.014976322650909424
Batch 17/64 loss: 0.0265272855758667
Batch 18/64 loss: 0.02628880739212036
Batch 19/64 loss: 0.02501368522644043
Batch 20/64 loss: 0.01628953218460083
Batch 21/64 loss: 0.03591543436050415
Batch 22/64 loss: 0.03329908847808838
Batch 23/64 loss: 0.007927536964416504
Batch 24/64 loss: 0.035958290100097656
Batch 25/64 loss: 0.029053926467895508
Batch 26/64 loss: 0.029304563999176025
Batch 27/64 loss: 0.03139042854309082
Batch 28/64 loss: 0.026951730251312256
Batch 29/64 loss: 0.024085640907287598
Batch 30/64 loss: 0.03309941291809082
Batch 31/64 loss: 0.02118462324142456
Batch 32/64 loss: 0.0305178165435791
Batch 33/64 loss: 0.019622802734375
Batch 34/64 loss: 0.01952815055847168
Batch 35/64 loss: 0.032765090465545654
Batch 36/64 loss: 0.028891146183013916
Batch 37/64 loss: 0.0416722297668457
Batch 38/64 loss: 0.021177470684051514
Batch 39/64 loss: 0.022182226181030273
Batch 40/64 loss: 0.026862680912017822
Batch 41/64 loss: 0.031376779079437256
Batch 42/64 loss: 0.015633583068847656
Batch 43/64 loss: 0.014356017112731934
Batch 44/64 loss: 0.022466301918029785
Batch 45/64 loss: 0.03455239534378052
Batch 46/64 loss: 0.02813786268234253
Batch 47/64 loss: 0.01854562759399414
Batch 48/64 loss: 0.027134060859680176
Batch 49/64 loss: 0.03696799278259277
Batch 50/64 loss: 0.029147863388061523
Batch 51/64 loss: 0.02321469783782959
Batch 52/64 loss: 0.01890772581100464
Batch 53/64 loss: 0.018025338649749756
Batch 54/64 loss: 0.01688253879547119
Batch 55/64 loss: 0.042362332344055176
Batch 56/64 loss: 0.023871779441833496
Batch 57/64 loss: 0.032857298851013184
Batch 58/64 loss: 0.035080909729003906
Batch 59/64 loss: 0.016968369483947754
Batch 60/64 loss: 0.030673563480377197
Batch 61/64 loss: 0.024356961250305176
Batch 62/64 loss: 0.025448083877563477
Batch 63/64 loss: 0.022063493728637695
Batch 64/64 loss: 0.0067833662033081055
Epoch 257  Train loss: 0.02567306172614004  Val loss: 0.08608295417733208
Saving best model, epoch: 257
Epoch 258
-------------------------------
Batch 1/64 loss: 0.02297079563140869
Batch 2/64 loss: 0.017087340354919434
Batch 3/64 loss: 0.02266407012939453
Batch 4/64 loss: 0.028282880783081055
Batch 5/64 loss: 0.03890347480773926
Batch 6/64 loss: 0.017725706100463867
Batch 7/64 loss: 0.010350227355957031
Batch 8/64 loss: 0.012951672077178955
Batch 9/64 loss: 0.012409985065460205
Batch 10/64 loss: 0.03042900562286377
Batch 11/64 loss: 0.015181362628936768
Batch 12/64 loss: 0.019836068153381348
Batch 13/64 loss: 0.02908843755722046
Batch 14/64 loss: 0.009557902812957764
Batch 15/64 loss: 0.026420116424560547
Batch 16/64 loss: 0.019247770309448242
Batch 17/64 loss: 0.012643754482269287
Batch 18/64 loss: 0.041501522064208984
Batch 19/64 loss: 0.026152372360229492
Batch 20/64 loss: 0.027373790740966797
Batch 21/64 loss: 0.024425148963928223
Batch 22/64 loss: 0.01951122283935547
Batch 23/64 loss: 0.02254927158355713
Batch 24/64 loss: 0.03447532653808594
Batch 25/64 loss: 0.01689624786376953
Batch 26/64 loss: 0.020344972610473633
Batch 27/64 loss: 0.019122779369354248
Batch 28/64 loss: 0.03427988290786743
Batch 29/64 loss: 0.03239482641220093
Batch 30/64 loss: 0.023267626762390137
Batch 31/64 loss: 0.03891134262084961
Batch 32/64 loss: 0.030520915985107422
Batch 33/64 loss: 0.04151123762130737
Batch 34/64 loss: 0.04203140735626221
Batch 35/64 loss: 0.02329564094543457
Batch 36/64 loss: 0.018344879150390625
Batch 37/64 loss: 0.019637703895568848
Batch 38/64 loss: 0.025868892669677734
Batch 39/64 loss: 0.032659053802490234
Batch 40/64 loss: 0.019040226936340332
Batch 41/64 loss: 0.021861135959625244
Batch 42/64 loss: 0.02764904499053955
Batch 43/64 loss: 0.02572476863861084
Batch 44/64 loss: 0.015917181968688965
Batch 45/64 loss: 0.018544256687164307
Batch 46/64 loss: 0.015842676162719727
Batch 47/64 loss: 0.032911717891693115
Batch 48/64 loss: 0.025277316570281982
Batch 49/64 loss: 0.023997187614440918
Batch 50/64 loss: 0.024211466312408447
Batch 51/64 loss: 0.016011834144592285
Batch 52/64 loss: 0.04012817144393921
Batch 53/64 loss: 0.01324087381362915
Batch 54/64 loss: 0.02317601442337036
Batch 55/64 loss: 0.03543412685394287
Batch 56/64 loss: 0.025173068046569824
Batch 57/64 loss: 0.03966188430786133
Batch 58/64 loss: 0.028906702995300293
Batch 59/64 loss: 0.02522188425064087
Batch 60/64 loss: 0.030609548091888428
Batch 61/64 loss: 0.03972005844116211
Batch 62/64 loss: 0.01872086524963379
Batch 63/64 loss: 0.01343989372253418
Batch 64/64 loss: 0.025640010833740234
Epoch 258  Train loss: 0.024791820376527076  Val loss: 0.08974632712983593
Epoch 259
-------------------------------
Batch 1/64 loss: 0.026104092597961426
Batch 2/64 loss: 0.015026628971099854
Batch 3/64 loss: 0.019950449466705322
Batch 4/64 loss: 0.03281557559967041
Batch 5/64 loss: 0.012577056884765625
Batch 6/64 loss: 0.036570727825164795
Batch 7/64 loss: 0.017819464206695557
Batch 8/64 loss: 0.013647675514221191
Batch 9/64 loss: 0.006636381149291992
Batch 10/64 loss: 0.027333319187164307
Batch 11/64 loss: 0.01970529556274414
Batch 12/64 loss: 0.013119816780090332
Batch 13/64 loss: 0.047133803367614746
Batch 14/64 loss: 0.027152061462402344
Batch 15/64 loss: 0.03528189659118652
Batch 16/64 loss: 0.019198179244995117
Batch 17/64 loss: 0.03452783823013306
Batch 18/64 loss: 0.014044344425201416
Batch 19/64 loss: 0.0035987496376037598
Batch 20/64 loss: 0.02652186155319214
Batch 21/64 loss: 0.02124232053756714
Batch 22/64 loss: 0.021245956420898438
Batch 23/64 loss: 0.026525497436523438
Batch 24/64 loss: 0.02782660722732544
Batch 25/64 loss: 0.02198868989944458
Batch 26/64 loss: 0.02048414945602417
Batch 27/64 loss: 0.02014857530593872
Batch 28/64 loss: 0.020509004592895508
Batch 29/64 loss: 0.022264719009399414
Batch 30/64 loss: 0.033725738525390625
Batch 31/64 loss: 0.028781533241271973
Batch 32/64 loss: 0.015388727188110352
Batch 33/64 loss: 0.02381277084350586
Batch 34/64 loss: 0.02398669719696045
Batch 35/64 loss: 0.019192636013031006
Batch 36/64 loss: 0.024890422821044922
Batch 37/64 loss: 0.03304535150527954
Batch 38/64 loss: 0.023479342460632324
Batch 39/64 loss: 0.03597921133041382
Batch 40/64 loss: 0.015573322772979736
Batch 41/64 loss: 0.015626966953277588
Batch 42/64 loss: 0.03447282314300537
Batch 43/64 loss: 0.011173367500305176
Batch 44/64 loss: 0.03801417350769043
Batch 45/64 loss: 0.03316861391067505
Batch 46/64 loss: 0.049733757972717285
Batch 47/64 loss: 0.029892027378082275
Batch 48/64 loss: 0.016817986965179443
Batch 49/64 loss: 0.02560710906982422
Batch 50/64 loss: 0.03982645273208618
Batch 51/64 loss: 0.015334844589233398
Batch 52/64 loss: 0.019454121589660645
Batch 53/64 loss: 0.024293601512908936
Batch 54/64 loss: 0.05830800533294678
Batch 55/64 loss: 0.015455901622772217
Batch 56/64 loss: 0.0228918194770813
Batch 57/64 loss: 0.021059393882751465
Batch 58/64 loss: 0.025928258895874023
Batch 59/64 loss: 0.021478593349456787
Batch 60/64 loss: 0.027464568614959717
Batch 61/64 loss: 0.0324326753616333
Batch 62/64 loss: 0.020901858806610107
Batch 63/64 loss: 0.026517152786254883
Batch 64/64 loss: 0.038922905921936035
Epoch 259  Train loss: 0.0248455332774742  Val loss: 0.0889104416280268
Epoch 260
-------------------------------
Batch 1/64 loss: 0.016614556312561035
Batch 2/64 loss: 0.014149844646453857
Batch 3/64 loss: 0.01898956298828125
Batch 4/64 loss: 0.023376762866973877
Batch 5/64 loss: 0.0165901780128479
Batch 6/64 loss: 0.021844983100891113
Batch 7/64 loss: 0.01007223129272461
Batch 8/64 loss: 0.020374655723571777
Batch 9/64 loss: 0.033113181591033936
Batch 10/64 loss: 0.011844098567962646
Batch 11/64 loss: 0.03912150859832764
Batch 12/64 loss: 0.0181235671043396
Batch 13/64 loss: 0.012248337268829346
Batch 14/64 loss: 0.010033249855041504
Batch 15/64 loss: 0.02646392583847046
Batch 16/64 loss: 0.029306888580322266
Batch 17/64 loss: 0.024255335330963135
Batch 18/64 loss: 0.02386605739593506
Batch 19/64 loss: 0.01351851224899292
Batch 20/64 loss: 0.013306856155395508
Batch 21/64 loss: 0.01691645383834839
Batch 22/64 loss: 0.015053331851959229
Batch 23/64 loss: 0.022889375686645508
Batch 24/64 loss: 0.027051270008087158
Batch 25/64 loss: 0.017507195472717285
Batch 26/64 loss: 0.026023685932159424
Batch 27/64 loss: 0.03361964225769043
Batch 28/64 loss: 0.018828749656677246
Batch 29/64 loss: 0.01105409860610962
Batch 30/64 loss: 0.02006518840789795
Batch 31/64 loss: 0.01459425687789917
Batch 32/64 loss: 0.0279044508934021
Batch 33/64 loss: 0.01643747091293335
Batch 34/64 loss: 0.016626715660095215
Batch 35/64 loss: 0.012676119804382324
Batch 36/64 loss: 0.02933347225189209
Batch 37/64 loss: 0.04288935661315918
Batch 38/64 loss: 0.042806267738342285
Batch 39/64 loss: 0.04327291250228882
Batch 40/64 loss: 0.0344807505607605
Batch 41/64 loss: 0.03255033493041992
Batch 42/64 loss: 0.006022214889526367
Batch 43/64 loss: 0.021747231483459473
Batch 44/64 loss: 0.017938077449798584
Batch 45/64 loss: 0.034916698932647705
Batch 46/64 loss: 0.034619033336639404
Batch 47/64 loss: 0.024407923221588135
Batch 48/64 loss: 0.039887964725494385
Batch 49/64 loss: 0.014651238918304443
Batch 50/64 loss: 0.026615679264068604
Batch 51/64 loss: 0.024836421012878418
Batch 52/64 loss: 0.01687026023864746
Batch 53/64 loss: 0.031054377555847168
Batch 54/64 loss: 0.027901411056518555
Batch 55/64 loss: 0.03978914022445679
Batch 56/64 loss: 0.02603018283843994
Batch 57/64 loss: 0.023455440998077393
Batch 58/64 loss: 0.03876328468322754
Batch 59/64 loss: 0.014582812786102295
Batch 60/64 loss: 0.024163365364074707
Batch 61/64 loss: 0.01735907793045044
Batch 62/64 loss: 0.031188666820526123
Batch 63/64 loss: 0.014385461807250977
Batch 64/64 loss: 0.0201224684715271
Epoch 260  Train loss: 0.023310952326830697  Val loss: 0.08894344928748009
Epoch 261
-------------------------------
Batch 1/64 loss: 0.011965572834014893
Batch 2/64 loss: 0.019954800605773926
Batch 3/64 loss: 0.017054498195648193
Batch 4/64 loss: 0.021510004997253418
Batch 5/64 loss: 0.016157090663909912
Batch 6/64 loss: 0.013675093650817871
Batch 7/64 loss: 0.02573305368423462
Batch 8/64 loss: 0.032772958278656006
Batch 9/64 loss: 0.01708388328552246
Batch 10/64 loss: 0.033893883228302
Batch 11/64 loss: 0.021669864654541016
Batch 12/64 loss: 0.003071606159210205
Batch 13/64 loss: 0.010234713554382324
Batch 14/64 loss: 0.014649152755737305
Batch 15/64 loss: 0.015673816204071045
Batch 16/64 loss: 0.017424285411834717
Batch 17/64 loss: 0.023925364017486572
Batch 18/64 loss: 0.022701621055603027
Batch 19/64 loss: 0.020989537239074707
Batch 20/64 loss: 0.021082639694213867
Batch 21/64 loss: 0.008797943592071533
Batch 22/64 loss: 0.009302735328674316
Batch 23/64 loss: 0.011225759983062744
Batch 24/64 loss: 0.011159896850585938
Batch 25/64 loss: 0.008124828338623047
Batch 26/64 loss: 0.01281052827835083
Batch 27/64 loss: 0.020854055881500244
Batch 28/64 loss: 0.009271502494812012
Batch 29/64 loss: 0.0178835391998291
Batch 30/64 loss: 0.021296381950378418
Batch 31/64 loss: 0.025067389011383057
Batch 32/64 loss: 0.0054738521575927734
Batch 33/64 loss: 0.030811011791229248
Batch 34/64 loss: 0.026931703090667725
Batch 35/64 loss: 0.023608803749084473
Batch 36/64 loss: 0.029070615768432617
Batch 37/64 loss: 0.01048266887664795
Batch 38/64 loss: 0.018252015113830566
Batch 39/64 loss: 0.017252862453460693
Batch 40/64 loss: 0.013825774192810059
Batch 41/64 loss: 0.018660545349121094
Batch 42/64 loss: 0.04490506649017334
Batch 43/64 loss: 0.016406655311584473
Batch 44/64 loss: 0.028732001781463623
Batch 45/64 loss: 0.02959573268890381
Batch 46/64 loss: 0.019056200981140137
Batch 47/64 loss: 0.016753196716308594
Batch 48/64 loss: 0.028060972690582275
Batch 49/64 loss: 0.02979046106338501
Batch 50/64 loss: 0.019298970699310303
Batch 51/64 loss: 0.022501051425933838
Batch 52/64 loss: 0.016919851303100586
Batch 53/64 loss: 0.019884109497070312
Batch 54/64 loss: 0.022953331470489502
Batch 55/64 loss: 0.010726869106292725
Batch 56/64 loss: 0.02479422092437744
Batch 57/64 loss: 0.029402315616607666
Batch 58/64 loss: 0.019206643104553223
Batch 59/64 loss: 0.018137753009796143
Batch 60/64 loss: 0.026858389377593994
Batch 61/64 loss: 0.041319847106933594
Batch 62/64 loss: 0.0451318621635437
Batch 63/64 loss: 0.03301405906677246
Batch 64/64 loss: 0.03849691152572632
Epoch 261  Train loss: 0.020764079982159185  Val loss: 0.08539813040048395
Saving best model, epoch: 261
Epoch 262
-------------------------------
Batch 1/64 loss: 0.010213196277618408
Batch 2/64 loss: 0.006739020347595215
Batch 3/64 loss: 0.01932084560394287
Batch 4/64 loss: 0.01455622911453247
Batch 5/64 loss: 0.013026535511016846
Batch 6/64 loss: 0.006674408912658691
Batch 7/64 loss: 0.03473752737045288
Batch 8/64 loss: 0.05168813467025757
Batch 9/64 loss: 0.025554239749908447
Batch 10/64 loss: 0.028412997722625732
Batch 11/64 loss: 0.015337467193603516
Batch 12/64 loss: 0.04687654972076416
Batch 13/64 loss: 0.02543950080871582
Batch 14/64 loss: 0.02227860689163208
Batch 15/64 loss: 0.013601541519165039
Batch 16/64 loss: 0.011609435081481934
Batch 17/64 loss: 0.01030278205871582
Batch 18/64 loss: 0.020958781242370605
Batch 19/64 loss: 0.031027257442474365
Batch 20/64 loss: 0.009524345397949219
Batch 21/64 loss: 0.015850484371185303
Batch 22/64 loss: 0.011093497276306152
Batch 23/64 loss: 0.01880878210067749
Batch 24/64 loss: 0.01957803964614868
Batch 25/64 loss: 0.0288848876953125
Batch 26/64 loss: 0.021883070468902588
Batch 27/64 loss: 0.035522520542144775
Batch 28/64 loss: 0.012270987033843994
Batch 29/64 loss: 0.027841687202453613
Batch 30/64 loss: 0.014727890491485596
Batch 31/64 loss: 0.024541735649108887
Batch 32/64 loss: 0.020815551280975342
Batch 33/64 loss: 0.017663657665252686
Batch 34/64 loss: 0.019812941551208496
Batch 35/64 loss: 0.02074110507965088
Batch 36/64 loss: 0.015678882598876953
Batch 37/64 loss: 0.02711433172225952
Batch 38/64 loss: 0.012053906917572021
Batch 39/64 loss: 0.03138601779937744
Batch 40/64 loss: 0.04457634687423706
Batch 41/64 loss: 0.014772534370422363
Batch 42/64 loss: 0.039179861545562744
Batch 43/64 loss: 0.025873422622680664
Batch 44/64 loss: 0.0298193097114563
Batch 45/64 loss: 0.022358596324920654
Batch 46/64 loss: 0.026768803596496582
Batch 47/64 loss: 0.030442237854003906
Batch 48/64 loss: 0.017484426498413086
Batch 49/64 loss: 0.018092334270477295
Batch 50/64 loss: 0.02005523443222046
Batch 51/64 loss: 0.025590062141418457
Batch 52/64 loss: 0.030755460262298584
Batch 53/64 loss: 0.01726686954498291
Batch 54/64 loss: 0.03362703323364258
Batch 55/64 loss: 0.02419304847717285
Batch 56/64 loss: 0.018652737140655518
Batch 57/64 loss: 0.02444404363632202
Batch 58/64 loss: 0.013799011707305908
Batch 59/64 loss: 0.03366577625274658
Batch 60/64 loss: 0.023553669452667236
Batch 61/64 loss: 0.023183345794677734
Batch 62/64 loss: 0.02785170078277588
Batch 63/64 loss: 0.024254679679870605
Batch 64/64 loss: 0.025862395763397217
Epoch 262  Train loss: 0.02264794859231687  Val loss: 0.08983040552368689
Epoch 263
-------------------------------
Batch 1/64 loss: 0.0271989107131958
Batch 2/64 loss: 0.006256103515625
Batch 3/64 loss: 0.018086254596710205
Batch 4/64 loss: 0.01682567596435547
Batch 5/64 loss: 0.022184431552886963
Batch 6/64 loss: 0.033261895179748535
Batch 7/64 loss: 0.018150508403778076
Batch 8/64 loss: 0.019864320755004883
Batch 9/64 loss: 0.034995198249816895
Batch 10/64 loss: 0.02254462242126465
Batch 11/64 loss: 0.024361371994018555
Batch 12/64 loss: 0.025201082229614258
Batch 13/64 loss: 0.027280807495117188
Batch 14/64 loss: 0.01867920160293579
Batch 15/64 loss: 0.022855103015899658
Batch 16/64 loss: 0.01611793041229248
Batch 17/64 loss: 0.02226877212524414
Batch 18/64 loss: -0.000567317008972168
Batch 19/64 loss: 0.019622623920440674
Batch 20/64 loss: 0.033468425273895264
Batch 21/64 loss: 0.018751323223114014
Batch 22/64 loss: 0.012977182865142822
Batch 23/64 loss: 0.027706265449523926
Batch 24/64 loss: 0.019680559635162354
Batch 25/64 loss: 0.009822547435760498
Batch 26/64 loss: 0.03181147575378418
Batch 27/64 loss: 0.030952751636505127
Batch 28/64 loss: 0.013156533241271973
Batch 29/64 loss: 0.03920948505401611
Batch 30/64 loss: 0.01258474588394165
Batch 31/64 loss: 0.00892573595046997
Batch 32/64 loss: 0.013074994087219238
Batch 33/64 loss: 0.02701425552368164
Batch 34/64 loss: 0.022234737873077393
Batch 35/64 loss: 0.029647767543792725
Batch 36/64 loss: 0.028806865215301514
Batch 37/64 loss: 0.025633692741394043
Batch 38/64 loss: 0.02080714702606201
Batch 39/64 loss: 0.019627392292022705
Batch 40/64 loss: 0.021386027336120605
Batch 41/64 loss: 0.025717496871948242
Batch 42/64 loss: 0.015950500965118408
Batch 43/64 loss: 0.017111480236053467
Batch 44/64 loss: 0.028710663318634033
Batch 45/64 loss: 0.025366663932800293
Batch 46/64 loss: 0.0357246994972229
Batch 47/64 loss: 0.0231592059135437
Batch 48/64 loss: 0.024553537368774414
Batch 49/64 loss: 0.035244882106781006
Batch 50/64 loss: 0.019512474536895752
Batch 51/64 loss: 0.008224964141845703
Batch 52/64 loss: 0.020659148693084717
Batch 53/64 loss: 0.011836111545562744
Batch 54/64 loss: 0.020293891429901123
Batch 55/64 loss: 0.01699960231781006
Batch 56/64 loss: 0.02175891399383545
Batch 57/64 loss: 0.023775935173034668
Batch 58/64 loss: 0.03452569246292114
Batch 59/64 loss: 0.008878171443939209
Batch 60/64 loss: 0.01342308521270752
Batch 61/64 loss: 0.023484766483306885
Batch 62/64 loss: 0.02982628345489502
Batch 63/64 loss: 0.034934818744659424
Batch 64/64 loss: 0.017191171646118164
Epoch 263  Train loss: 0.02188288277270747  Val loss: 0.08690044191694751
Epoch 264
-------------------------------
Batch 1/64 loss: 0.045811593532562256
Batch 2/64 loss: 0.02086430788040161
Batch 3/64 loss: 0.014155268669128418
Batch 4/64 loss: 0.026710033416748047
Batch 5/64 loss: 0.02826511859893799
Batch 6/64 loss: 0.023138999938964844
Batch 7/64 loss: 0.016743481159210205
Batch 8/64 loss: 0.015296697616577148
Batch 9/64 loss: 0.01691526174545288
Batch 10/64 loss: 0.016354262828826904
Batch 11/64 loss: 0.018827617168426514
Batch 12/64 loss: 0.04805201292037964
Batch 13/64 loss: 0.020782828330993652
Batch 14/64 loss: 0.01621013879776001
Batch 15/64 loss: 0.019115924835205078
Batch 16/64 loss: 0.010508596897125244
Batch 17/64 loss: 0.02687162160873413
Batch 18/64 loss: 0.01371312141418457
Batch 19/64 loss: 0.022391438484191895
Batch 20/64 loss: 0.014992952346801758
Batch 21/64 loss: 0.03468829393386841
Batch 22/64 loss: 0.01693040132522583
Batch 23/64 loss: 0.022423982620239258
Batch 24/64 loss: 0.027000069618225098
Batch 25/64 loss: 0.017781555652618408
Batch 26/64 loss: 0.016371190547943115
Batch 27/64 loss: 0.029146969318389893
Batch 28/64 loss: 0.016182541847229004
Batch 29/64 loss: 0.0395127534866333
Batch 30/64 loss: 0.021733880043029785
Batch 31/64 loss: 0.021815001964569092
Batch 32/64 loss: 0.01754051446914673
Batch 33/64 loss: 0.0287020206451416
Batch 34/64 loss: 0.021950185298919678
Batch 35/64 loss: 0.025139808654785156
Batch 36/64 loss: 0.022448062896728516
Batch 37/64 loss: 0.012217044830322266
Batch 38/64 loss: 0.00922173261642456
Batch 39/64 loss: 0.013071656227111816
Batch 40/64 loss: 0.02316373586654663
Batch 41/64 loss: 0.041052401065826416
Batch 42/64 loss: 0.020678341388702393
Batch 43/64 loss: 0.024236083030700684
Batch 44/64 loss: 0.01852130889892578
Batch 45/64 loss: 0.02259540557861328
Batch 46/64 loss: 0.04614681005477905
Batch 47/64 loss: 0.006925463676452637
Batch 48/64 loss: 0.011470198631286621
Batch 49/64 loss: 0.019920527935028076
Batch 50/64 loss: 0.013390302658081055
Batch 51/64 loss: 0.018117964267730713
Batch 52/64 loss: 0.021735012531280518
Batch 53/64 loss: 0.019308388233184814
Batch 54/64 loss: 0.021308183670043945
Batch 55/64 loss: 0.008393406867980957
Batch 56/64 loss: 0.008334696292877197
Batch 57/64 loss: 0.020902514457702637
Batch 58/64 loss: 0.033735454082489014
Batch 59/64 loss: 0.019978642463684082
Batch 60/64 loss: 0.016995251178741455
Batch 61/64 loss: 0.019809722900390625
Batch 62/64 loss: 0.013176560401916504
Batch 63/64 loss: 0.013677656650543213
Batch 64/64 loss: 0.027939319610595703
Epoch 264  Train loss: 0.021241215163586187  Val loss: 0.08712106103339966
Epoch 265
-------------------------------
Batch 1/64 loss: 0.013595104217529297
Batch 2/64 loss: 0.02609926462173462
Batch 3/64 loss: 0.03340214490890503
Batch 4/64 loss: 0.006592214107513428
Batch 5/64 loss: 0.0313534140586853
Batch 6/64 loss: 0.015163183212280273
Batch 7/64 loss: 0.01392674446105957
Batch 8/64 loss: 0.008916974067687988
Batch 9/64 loss: 0.010777175426483154
Batch 10/64 loss: 0.0025446414947509766
Batch 11/64 loss: 0.00716787576675415
Batch 12/64 loss: 0.013937234878540039
Batch 13/64 loss: 0.011607646942138672
Batch 14/64 loss: 0.025800466537475586
Batch 15/64 loss: 0.0216522216796875
Batch 16/64 loss: 0.022486329078674316
Batch 17/64 loss: 0.026702404022216797
Batch 18/64 loss: 0.01637899875640869
Batch 19/64 loss: 0.01154237985610962
Batch 20/64 loss: 0.016333341598510742
Batch 21/64 loss: 0.00787508487701416
Batch 22/64 loss: 0.020769715309143066
Batch 23/64 loss: 0.03588593006134033
Batch 24/64 loss: 0.013669908046722412
Batch 25/64 loss: 0.016516387462615967
Batch 26/64 loss: 0.02129817008972168
Batch 27/64 loss: 0.024194777011871338
Batch 28/64 loss: 0.018834590911865234
Batch 29/64 loss: 0.018420100212097168
Batch 30/64 loss: 0.01845097541809082
Batch 31/64 loss: 0.03350567817687988
Batch 32/64 loss: 0.005435049533843994
Batch 33/64 loss: 0.01867198944091797
Batch 34/64 loss: 0.021558761596679688
Batch 35/64 loss: 0.013769090175628662
Batch 36/64 loss: 0.015638232231140137
Batch 37/64 loss: 0.03173565864562988
Batch 38/64 loss: 0.022551238536834717
Batch 39/64 loss: 0.0341109037399292
Batch 40/64 loss: 0.009337902069091797
Batch 41/64 loss: 0.018874824047088623
Batch 42/64 loss: 0.020444810390472412
Batch 43/64 loss: 0.008033990859985352
Batch 44/64 loss: 0.016310036182403564
Batch 45/64 loss: 0.008691072463989258
Batch 46/64 loss: 0.016937971115112305
Batch 47/64 loss: 0.006704509258270264
Batch 48/64 loss: 0.02201104164123535
Batch 49/64 loss: 0.0248568058013916
Batch 50/64 loss: 0.024737119674682617
Batch 51/64 loss: 0.01779264211654663
Batch 52/64 loss: 0.03609299659729004
Batch 53/64 loss: 0.017127275466918945
Batch 54/64 loss: 0.017483532428741455
Batch 55/64 loss: 0.03313249349594116
Batch 56/64 loss: 0.007672667503356934
Batch 57/64 loss: 0.014545023441314697
Batch 58/64 loss: 0.029301345348358154
Batch 59/64 loss: 0.026709437370300293
Batch 60/64 loss: 0.03742748498916626
Batch 61/64 loss: 0.029645204544067383
Batch 62/64 loss: 0.03284430503845215
Batch 63/64 loss: 0.029879212379455566
Batch 64/64 loss: 0.012526869773864746
Epoch 265  Train loss: 0.019527229140786563  Val loss: 0.08874709929797248
Epoch 266
-------------------------------
Batch 1/64 loss: 0.016999661922454834
Batch 2/64 loss: 0.0238037109375
Batch 3/64 loss: 0.010073363780975342
Batch 4/64 loss: 0.01101994514465332
Batch 5/64 loss: 0.022051572799682617
Batch 6/64 loss: 0.03279674053192139
Batch 7/64 loss: 0.006412804126739502
Batch 8/64 loss: 0.012877702713012695
Batch 9/64 loss: 0.010794103145599365
Batch 10/64 loss: 0.012462973594665527
Batch 11/64 loss: 0.02060920000076294
Batch 12/64 loss: 0.015427112579345703
Batch 13/64 loss: 0.027938246726989746
Batch 14/64 loss: 0.014614224433898926
Batch 15/64 loss: 0.012465596199035645
Batch 16/64 loss: 0.012842297554016113
Batch 17/64 loss: 0.02050316333770752
Batch 18/64 loss: 0.015096724033355713
Batch 19/64 loss: 0.017408788204193115
Batch 20/64 loss: 0.026916086673736572
Batch 21/64 loss: 0.013487637042999268
Batch 22/64 loss: 0.037738680839538574
Batch 23/64 loss: 0.008756160736083984
Batch 24/64 loss: 0.011841833591461182
Batch 25/64 loss: 0.01957303285598755
Batch 26/64 loss: 0.022512316703796387
Batch 27/64 loss: 0.02920633554458618
Batch 28/64 loss: 0.012880504131317139
Batch 29/64 loss: 0.017206311225891113
Batch 30/64 loss: 0.017039597034454346
Batch 31/64 loss: 0.015198230743408203
Batch 32/64 loss: 0.021025657653808594
Batch 33/64 loss: 0.019938647747039795
Batch 34/64 loss: 0.01440286636352539
Batch 35/64 loss: 0.022100985050201416
Batch 36/64 loss: 0.001576066017150879
Batch 37/64 loss: 0.024745702743530273
Batch 38/64 loss: 0.027842044830322266
Batch 39/64 loss: 0.016592741012573242
Batch 40/64 loss: 0.026141762733459473
Batch 41/64 loss: 0.025091886520385742
Batch 42/64 loss: 0.011327505111694336
Batch 43/64 loss: 0.014202713966369629
Batch 44/64 loss: 0.028985917568206787
Batch 45/64 loss: 0.021007418632507324
Batch 46/64 loss: 0.033795058727264404
Batch 47/64 loss: 0.01804208755493164
Batch 48/64 loss: 0.01259768009185791
Batch 49/64 loss: 0.02594590187072754
Batch 50/64 loss: 0.018302559852600098
Batch 51/64 loss: 0.014596998691558838
Batch 52/64 loss: 0.026206374168395996
Batch 53/64 loss: 0.01312410831451416
Batch 54/64 loss: 0.023128747940063477
Batch 55/64 loss: 0.015762805938720703
Batch 56/64 loss: 0.039874255657196045
Batch 57/64 loss: 0.023024797439575195
Batch 58/64 loss: 0.0200045108795166
Batch 59/64 loss: 0.028464436531066895
Batch 60/64 loss: 0.013517975807189941
Batch 61/64 loss: 0.030168473720550537
Batch 62/64 loss: 0.012220203876495361
Batch 63/64 loss: 0.023009657859802246
Batch 64/64 loss: 0.027835488319396973
Epoch 266  Train loss: 0.019391369819641114  Val loss: 0.08751307074556645
Epoch 267
-------------------------------
Batch 1/64 loss: 0.004853367805480957
Batch 2/64 loss: 0.017365634441375732
Batch 3/64 loss: 0.028198659420013428
Batch 4/64 loss: 0.021194040775299072
Batch 5/64 loss: 0.022722959518432617
Batch 6/64 loss: 0.016903221607208252
Batch 7/64 loss: 0.011258304119110107
Batch 8/64 loss: 0.023030877113342285
Batch 9/64 loss: 0.014691591262817383
Batch 10/64 loss: 0.0204276442527771
Batch 11/64 loss: 0.018749773502349854
Batch 12/64 loss: 0.009509861469268799
Batch 13/64 loss: 0.020402908325195312
Batch 14/64 loss: 0.028783559799194336
Batch 15/64 loss: 0.009127378463745117
Batch 16/64 loss: 0.015155434608459473
Batch 17/64 loss: 0.026400506496429443
Batch 18/64 loss: 0.011155009269714355
Batch 19/64 loss: 0.017653584480285645
Batch 20/64 loss: 0.02331066131591797
Batch 21/64 loss: 0.015033483505249023
Batch 22/64 loss: 0.010501623153686523
Batch 23/64 loss: 0.02969503402709961
Batch 24/64 loss: 0.03171968460083008
Batch 25/64 loss: 0.027435302734375
Batch 26/64 loss: 0.009613633155822754
Batch 27/64 loss: 0.01246786117553711
Batch 28/64 loss: 0.008985042572021484
Batch 29/64 loss: 0.05061995983123779
Batch 30/64 loss: 0.023111701011657715
Batch 31/64 loss: 0.0164373517036438
Batch 32/64 loss: 0.009253442287445068
Batch 33/64 loss: 0.006461143493652344
Batch 34/64 loss: 0.015311002731323242
Batch 35/64 loss: 0.014905452728271484
Batch 36/64 loss: 0.02590155601501465
Batch 37/64 loss: 0.014874756336212158
Batch 38/64 loss: 0.025148987770080566
Batch 39/64 loss: 0.02065342664718628
Batch 40/64 loss: 0.015681028366088867
Batch 41/64 loss: 0.016446590423583984
Batch 42/64 loss: 0.01930457353591919
Batch 43/64 loss: 0.028068184852600098
Batch 44/64 loss: 0.013314127922058105
Batch 45/64 loss: 0.00570523738861084
Batch 46/64 loss: 0.021824777126312256
Batch 47/64 loss: 0.0335652232170105
Batch 48/64 loss: 0.030667543411254883
Batch 49/64 loss: 0.017665624618530273
Batch 50/64 loss: 0.014229834079742432
Batch 51/64 loss: 0.010648012161254883
Batch 52/64 loss: 0.012143254280090332
Batch 53/64 loss: 0.01946049928665161
Batch 54/64 loss: -0.0016408562660217285
Batch 55/64 loss: 0.03727966547012329
Batch 56/64 loss: 0.022966504096984863
Batch 57/64 loss: 0.020626306533813477
Batch 58/64 loss: 0.020216166973114014
Batch 59/64 loss: 0.021033167839050293
Batch 60/64 loss: 0.01753842830657959
Batch 61/64 loss: 0.04522395133972168
Batch 62/64 loss: 0.022584199905395508
Batch 63/64 loss: 0.008396685123443604
Batch 64/64 loss: 0.02390080690383911
Epoch 267  Train loss: 0.019135682489357743  Val loss: 0.08716682552062359
Epoch 268
-------------------------------
Batch 1/64 loss: 0.014318108558654785
Batch 2/64 loss: 0.017049193382263184
Batch 3/64 loss: 0.02950495481491089
Batch 4/64 loss: 0.019835174083709717
Batch 5/64 loss: 0.013043105602264404
Batch 6/64 loss: 0.01870262622833252
Batch 7/64 loss: 0.019424855709075928
Batch 8/64 loss: 0.012632668018341064
Batch 9/64 loss: 0.015500903129577637
Batch 10/64 loss: 0.02577131986618042
Batch 11/64 loss: 0.00809621810913086
Batch 12/64 loss: 0.00969994068145752
Batch 13/64 loss: 0.01398855447769165
Batch 14/64 loss: 0.02089560031890869
Batch 15/64 loss: 0.010001420974731445
Batch 16/64 loss: 0.019106030464172363
Batch 17/64 loss: 0.03487342596054077
Batch 18/64 loss: 0.03761029243469238
Batch 19/64 loss: 0.011912226676940918
Batch 20/64 loss: 0.010389208793640137
Batch 21/64 loss: 0.032747745513916016
Batch 22/64 loss: 0.018351852893829346
Batch 23/64 loss: 0.016313672065734863
Batch 24/64 loss: 0.022775471210479736
Batch 25/64 loss: 0.00885707139968872
Batch 26/64 loss: 0.015055179595947266
Batch 27/64 loss: 0.00744318962097168
Batch 28/64 loss: 0.027005493640899658
Batch 29/64 loss: 0.01007986068725586
Batch 30/64 loss: 0.02417999505996704
Batch 31/64 loss: 0.027365505695343018
Batch 32/64 loss: 0.010628104209899902
Batch 33/64 loss: 0.023395419120788574
Batch 34/64 loss: 0.039471983909606934
Batch 35/64 loss: 0.010509908199310303
Batch 36/64 loss: 0.014770090579986572
Batch 37/64 loss: 0.03140002489089966
Batch 38/64 loss: 0.030723094940185547
Batch 39/64 loss: 0.02264004945755005
Batch 40/64 loss: 0.025567591190338135
Batch 41/64 loss: 0.03276228904724121
Batch 42/64 loss: 0.026543259620666504
Batch 43/64 loss: 0.008524715900421143
Batch 44/64 loss: 0.02324819564819336
Batch 45/64 loss: 0.029057025909423828
Batch 46/64 loss: 0.023649096488952637
Batch 47/64 loss: 0.030906081199645996
Batch 48/64 loss: 0.02941572666168213
Batch 49/64 loss: 0.022065460681915283
Batch 50/64 loss: 0.021496057510375977
Batch 51/64 loss: 0.01296013593673706
Batch 52/64 loss: 0.04317963123321533
Batch 53/64 loss: 0.03680408000946045
Batch 54/64 loss: 0.02580726146697998
Batch 55/64 loss: 0.023405492305755615
Batch 56/64 loss: 0.0243149995803833
Batch 57/64 loss: 0.024221062660217285
Batch 58/64 loss: 0.018210768699645996
Batch 59/64 loss: 0.019044220447540283
Batch 60/64 loss: 0.020159244537353516
Batch 61/64 loss: 0.025252103805541992
Batch 62/64 loss: 0.022598683834075928
Batch 63/64 loss: 0.030808627605438232
Batch 64/64 loss: 0.0162198543548584
Epoch 268  Train loss: 0.021462529313330557  Val loss: 0.08649722653156294
Epoch 269
-------------------------------
Batch 1/64 loss: 0.021361052989959717
Batch 2/64 loss: 0.021584272384643555
Batch 3/64 loss: 0.017199575901031494
Batch 4/64 loss: 0.022268950939178467
Batch 5/64 loss: 0.025679707527160645
Batch 6/64 loss: 0.023365318775177002
Batch 7/64 loss: 0.009141087532043457
Batch 8/64 loss: 0.015905380249023438
Batch 9/64 loss: 0.0061991214752197266
Batch 10/64 loss: 0.02335602045059204
Batch 11/64 loss: 0.03296089172363281
Batch 12/64 loss: 0.005189299583435059
Batch 13/64 loss: 0.016339778900146484
Batch 14/64 loss: 0.01642543077468872
Batch 15/64 loss: 0.009514570236206055
Batch 16/64 loss: 0.004123091697692871
Batch 17/64 loss: 0.0033445358276367188
Batch 18/64 loss: 0.003484666347503662
Batch 19/64 loss: 0.0184481143951416
Batch 20/64 loss: 0.01469123363494873
Batch 21/64 loss: 0.0008187294006347656
Batch 22/64 loss: 0.014659702777862549
Batch 23/64 loss: 0.013363897800445557
Batch 24/64 loss: 0.015866339206695557
Batch 25/64 loss: 0.019701659679412842
Batch 26/64 loss: 0.021455347537994385
Batch 27/64 loss: 0.024337828159332275
Batch 28/64 loss: 0.013803422451019287
Batch 29/64 loss: 0.006019115447998047
Batch 30/64 loss: 0.00604856014251709
Batch 31/64 loss: 0.02867227792739868
Batch 32/64 loss: 0.0070455074310302734
Batch 33/64 loss: 0.021671831607818604
Batch 34/64 loss: 0.00934666395187378
Batch 35/64 loss: 0.02441418170928955
Batch 36/64 loss: 0.01982903480529785
Batch 37/64 loss: 0.013458013534545898
Batch 38/64 loss: 0.01524662971496582
Batch 39/64 loss: 0.014212489128112793
Batch 40/64 loss: 0.014124572277069092
Batch 41/64 loss: 0.02300131320953369
Batch 42/64 loss: 0.01882988214492798
Batch 43/64 loss: 0.020515918731689453
Batch 44/64 loss: 0.015710771083831787
Batch 45/64 loss: 0.04185086488723755
Batch 46/64 loss: 0.021925508975982666
Batch 47/64 loss: 0.02502197027206421
Batch 48/64 loss: 0.043192505836486816
Batch 49/64 loss: 0.032034873962402344
Batch 50/64 loss: 0.023052573204040527
Batch 51/64 loss: 0.020191848278045654
Batch 52/64 loss: 0.023084819316864014
Batch 53/64 loss: 0.020715296268463135
Batch 54/64 loss: 0.009368896484375
Batch 55/64 loss: 0.02971780300140381
Batch 56/64 loss: 0.012757837772369385
Batch 57/64 loss: 0.02087092399597168
Batch 58/64 loss: 0.008327722549438477
Batch 59/64 loss: 0.024406492710113525
Batch 60/64 loss: 0.006277918815612793
Batch 61/64 loss: 0.018814921379089355
Batch 62/64 loss: 0.01864224672317505
Batch 63/64 loss: 0.015100538730621338
Batch 64/64 loss: 0.03071153163909912
Epoch 269  Train loss: 0.017743137303520652  Val loss: 0.0871214811334905
Epoch 270
-------------------------------
Batch 1/64 loss: 0.00817561149597168
Batch 2/64 loss: 0.01176309585571289
Batch 3/64 loss: 0.005504310131072998
Batch 4/64 loss: 0.018265962600708008
Batch 5/64 loss: 0.010790824890136719
Batch 6/64 loss: 0.007009625434875488
Batch 7/64 loss: 0.013449907302856445
Batch 8/64 loss: 0.020935237407684326
Batch 9/64 loss: 0.004301190376281738
Batch 10/64 loss: 0.0024560093879699707
Batch 11/64 loss: 0.013078570365905762
Batch 12/64 loss: 0.009597837924957275
Batch 13/64 loss: 0.015189826488494873
Batch 14/64 loss: 0.002200186252593994
Batch 15/64 loss: 0.021156132221221924
Batch 16/64 loss: 0.008487343788146973
Batch 17/64 loss: 0.003000915050506592
Batch 18/64 loss: 0.01142871379852295
Batch 19/64 loss: 0.009301841259002686
Batch 20/64 loss: 0.031200528144836426
Batch 21/64 loss: 0.007469892501831055
Batch 22/64 loss: 0.03762316703796387
Batch 23/64 loss: 0.022497177124023438
Batch 24/64 loss: 0.023554205894470215
Batch 25/64 loss: 0.020833849906921387
Batch 26/64 loss: 0.025805652141571045
Batch 27/64 loss: 0.014190971851348877
Batch 28/64 loss: 0.016777098178863525
Batch 29/64 loss: 0.02189260721206665
Batch 30/64 loss: 0.010771453380584717
Batch 31/64 loss: 0.0032618045806884766
Batch 32/64 loss: 0.018812179565429688
Batch 33/64 loss: 0.022204816341400146
Batch 34/64 loss: 0.023122429847717285
Batch 35/64 loss: 0.02747434377670288
Batch 36/64 loss: 0.007430672645568848
Batch 37/64 loss: 0.009664177894592285
Batch 38/64 loss: 0.04160881042480469
Batch 39/64 loss: 0.01977682113647461
Batch 40/64 loss: 0.018493950366973877
Batch 41/64 loss: 0.025231599807739258
Batch 42/64 loss: 0.02610182762145996
Batch 43/64 loss: 0.020669102668762207
Batch 44/64 loss: 0.03992551565170288
Batch 45/64 loss: 0.03440749645233154
Batch 46/64 loss: 0.022357940673828125
Batch 47/64 loss: 0.022000610828399658
Batch 48/64 loss: 0.021567821502685547
Batch 49/64 loss: 0.008046150207519531
Batch 50/64 loss: 0.014752805233001709
Batch 51/64 loss: 0.008182883262634277
Batch 52/64 loss: 0.016332268714904785
Batch 53/64 loss: 0.03822481632232666
Batch 54/64 loss: 0.009401917457580566
Batch 55/64 loss: 0.018984079360961914
Batch 56/64 loss: 0.02506190538406372
Batch 57/64 loss: 0.023377418518066406
Batch 58/64 loss: 0.016485095024108887
Batch 59/64 loss: 0.02414005994796753
Batch 60/64 loss: 0.03506159782409668
Batch 61/64 loss: 0.027619600296020508
Batch 62/64 loss: 0.01617145538330078
Batch 63/64 loss: 0.013087868690490723
Batch 64/64 loss: 0.022756576538085938
Epoch 270  Train loss: 0.017957945430980008  Val loss: 0.08835724559436549
Epoch 271
-------------------------------
Batch 1/64 loss: 0.0126914381980896
Batch 2/64 loss: 0.009208619594573975
Batch 3/64 loss: 0.008792579174041748
Batch 4/64 loss: 0.015461266040802002
Batch 5/64 loss: 0.022125303745269775
Batch 6/64 loss: 0.023156821727752686
Batch 7/64 loss: 0.01864868402481079
Batch 8/64 loss: 0.01777470111846924
Batch 9/64 loss: 0.019357681274414062
Batch 10/64 loss: 0.012877225875854492
Batch 11/64 loss: 0.005309939384460449
Batch 12/64 loss: 0.01808828115463257
Batch 13/64 loss: 0.01832723617553711
Batch 14/64 loss: 0.037101805210113525
Batch 15/64 loss: 0.0053618550300598145
Batch 16/64 loss: 0.019773006439208984
Batch 17/64 loss: 0.020429134368896484
Batch 18/64 loss: 0.020482778549194336
Batch 19/64 loss: 0.01043248176574707
Batch 20/64 loss: 0.018620193004608154
Batch 21/64 loss: 0.0131913423538208
Batch 22/64 loss: 0.014328360557556152
Batch 23/64 loss: 0.011467397212982178
Batch 24/64 loss: 0.001150369644165039
Batch 25/64 loss: 0.04018896818161011
Batch 26/64 loss: 0.023159325122833252
Batch 27/64 loss: 0.020263254642486572
Batch 28/64 loss: 0.05061119794845581
Batch 29/64 loss: 0.017667293548583984
Batch 30/64 loss: 0.02102065086364746
Batch 31/64 loss: 0.023309051990509033
Batch 32/64 loss: 0.009025633335113525
Batch 33/64 loss: 0.02381432056427002
Batch 34/64 loss: 0.013091027736663818
Batch 35/64 loss: 0.022241413593292236
Batch 36/64 loss: 0.026430904865264893
Batch 37/64 loss: 0.04092365503311157
Batch 38/64 loss: 0.018879830837249756
Batch 39/64 loss: 0.01572507619857788
Batch 40/64 loss: 0.012426018714904785
Batch 41/64 loss: 0.013209044933319092
Batch 42/64 loss: 0.016775310039520264
Batch 43/64 loss: 0.019397079944610596
Batch 44/64 loss: 0.014627218246459961
Batch 45/64 loss: 0.020283520221710205
Batch 46/64 loss: 0.026536285877227783
Batch 47/64 loss: 0.018208682537078857
Batch 48/64 loss: 0.02966928482055664
Batch 49/64 loss: 0.02024662494659424
Batch 50/64 loss: 0.01783442497253418
Batch 51/64 loss: 0.00636136531829834
Batch 52/64 loss: 0.0212860107421875
Batch 53/64 loss: 0.0262143611907959
Batch 54/64 loss: 0.01830923557281494
Batch 55/64 loss: 0.008281230926513672
Batch 56/64 loss: 0.0037115812301635742
Batch 57/64 loss: 0.0176008939743042
Batch 58/64 loss: 0.01800215244293213
Batch 59/64 loss: 0.01379173994064331
Batch 60/64 loss: 0.007836580276489258
Batch 61/64 loss: 0.016563713550567627
Batch 62/64 loss: 0.032323479652404785
Batch 63/64 loss: 0.032784104347229004
Batch 64/64 loss: 0.02441859245300293
Epoch 271  Train loss: 0.018683984232883828  Val loss: 0.0915772181605965
Epoch 272
-------------------------------
Batch 1/64 loss: 0.015284299850463867
Batch 2/64 loss: 0.016406536102294922
Batch 3/64 loss: 0.014194607734680176
Batch 4/64 loss: 0.006449460983276367
Batch 5/64 loss: 0.010559439659118652
Batch 6/64 loss: 0.006848394870758057
Batch 7/64 loss: 0.010240674018859863
Batch 8/64 loss: 0.006310999393463135
Batch 9/64 loss: 0.01678931713104248
Batch 10/64 loss: 0.009681105613708496
Batch 11/64 loss: 0.010400712490081787
Batch 12/64 loss: 0.020598649978637695
Batch 13/64 loss: 0.0026223063468933105
Batch 14/64 loss: 0.008428633213043213
Batch 15/64 loss: 0.03530460596084595
Batch 16/64 loss: 0.009955644607543945
Batch 17/64 loss: 0.017739176750183105
Batch 18/64 loss: 0.019345223903656006
Batch 19/64 loss: 0.01606673002243042
Batch 20/64 loss: 0.009839057922363281
Batch 21/64 loss: 0.0040400028228759766
Batch 22/64 loss: 0.006296396255493164
Batch 23/64 loss: 0.0159723162651062
Batch 24/64 loss: 0.026416301727294922
Batch 25/64 loss: 0.008697092533111572
Batch 26/64 loss: 0.018438339233398438
Batch 27/64 loss: 0.01951014995574951
Batch 28/64 loss: 0.0401684045791626
Batch 29/64 loss: 0.024123787879943848
Batch 30/64 loss: 0.013921737670898438
Batch 31/64 loss: 0.018528997898101807
Batch 32/64 loss: 0.02406972646713257
Batch 33/64 loss: 0.012988507747650146
Batch 34/64 loss: 0.01602458953857422
Batch 35/64 loss: 0.014405310153961182
Batch 36/64 loss: 0.016264915466308594
Batch 37/64 loss: 0.017831921577453613
Batch 38/64 loss: 0.019090235233306885
Batch 39/64 loss: 0.0032945871353149414
Batch 40/64 loss: 0.03229767084121704
Batch 41/64 loss: 0.014143288135528564
Batch 42/64 loss: 0.019134044647216797
Batch 43/64 loss: 0.013750255107879639
Batch 44/64 loss: 0.02505350112915039
Batch 45/64 loss: 0.02145785093307495
Batch 46/64 loss: 0.02565300464630127
Batch 47/64 loss: 0.025834321975708008
Batch 48/64 loss: 0.030453741550445557
Batch 49/64 loss: 0.014869213104248047
Batch 50/64 loss: 0.010526001453399658
Batch 51/64 loss: 0.005789697170257568
Batch 52/64 loss: 0.018713831901550293
Batch 53/64 loss: 0.013694167137145996
Batch 54/64 loss: 0.023198843002319336
Batch 55/64 loss: 0.008894383907318115
Batch 56/64 loss: 0.028447747230529785
Batch 57/64 loss: 0.024282872676849365
Batch 58/64 loss: 0.02281808853149414
Batch 59/64 loss: 0.025416076183319092
Batch 60/64 loss: 0.03056812286376953
Batch 61/64 loss: 0.012444555759429932
Batch 62/64 loss: 0.018006324768066406
Batch 63/64 loss: 0.021959662437438965
Batch 64/64 loss: 0.041811585426330566
Epoch 272  Train loss: 0.01728493886835435  Val loss: 0.08744796793075771
Epoch 273
-------------------------------
Batch 1/64 loss: 0.02020418643951416
Batch 2/64 loss: 0.00819176435470581
Batch 3/64 loss: 0.02069026231765747
Batch 4/64 loss: 0.022358715534210205
Batch 5/64 loss: 0.008573949337005615
Batch 6/64 loss: 0.017355501651763916
Batch 7/64 loss: 0.01296842098236084
Batch 8/64 loss: 0.002200782299041748
Batch 9/64 loss: 0.028185129165649414
Batch 10/64 loss: 0.0043541789054870605
Batch 11/64 loss: 0.018099188804626465
Batch 12/64 loss: 0.0011107325553894043
Batch 13/64 loss: 0.003238976001739502
Batch 14/64 loss: 0.015167653560638428
Batch 15/64 loss: 0.0153275728225708
Batch 16/64 loss: 0.01855522394180298
Batch 17/64 loss: 0.017170608043670654
Batch 18/64 loss: 0.015379071235656738
Batch 19/64 loss: 0.02281796932220459
Batch 20/64 loss: 0.02279043197631836
Batch 21/64 loss: 0.01299959421157837
Batch 22/64 loss: 0.014899909496307373
Batch 23/64 loss: 0.012768328189849854
Batch 24/64 loss: 0.013233661651611328
Batch 25/64 loss: 0.002210855484008789
Batch 26/64 loss: 0.019825756549835205
Batch 27/64 loss: 0.016851603984832764
Batch 28/64 loss: 0.004330992698669434
Batch 29/64 loss: 0.010124802589416504
Batch 30/64 loss: 0.022876858711242676
Batch 31/64 loss: 0.011008262634277344
Batch 32/64 loss: 0.00874936580657959
Batch 33/64 loss: 0.0075563788414001465
Batch 34/64 loss: 0.020575761795043945
Batch 35/64 loss: 0.00481945276260376
Batch 36/64 loss: 0.026642799377441406
Batch 37/64 loss: 0.003043949604034424
Batch 38/64 loss: 0.015318632125854492
Batch 39/64 loss: 0.014939367771148682
Batch 40/64 loss: 0.027189671993255615
Batch 41/64 loss: 0.004922986030578613
Batch 42/64 loss: 0.012589216232299805
Batch 43/64 loss: 0.017811298370361328
Batch 44/64 loss: 0.009492278099060059
Batch 45/64 loss: 0.009048163890838623
Batch 46/64 loss: 0.026883304119110107
Batch 47/64 loss: 0.013077914714813232
Batch 48/64 loss: 0.02159702777862549
Batch 49/64 loss: 0.026205778121948242
Batch 50/64 loss: 0.013536214828491211
Batch 51/64 loss: 0.0042084455490112305
Batch 52/64 loss: 0.010317385196685791
Batch 53/64 loss: 0.014581441879272461
Batch 54/64 loss: 0.03486192226409912
Batch 55/64 loss: 0.006487488746643066
Batch 56/64 loss: 0.020624220371246338
Batch 57/64 loss: 0.017163753509521484
Batch 58/64 loss: 0.030255794525146484
Batch 59/64 loss: 0.048230648040771484
Batch 60/64 loss: 0.023804962635040283
Batch 61/64 loss: 0.0061533451080322266
Batch 62/64 loss: 0.007040739059448242
Batch 63/64 loss: 0.02943742275238037
Batch 64/64 loss: 0.01894509792327881
Epoch 273  Train loss: 0.01548622589485318  Val loss: 0.0910875823899233
Epoch 274
-------------------------------
Batch 1/64 loss: 0.009308993816375732
Batch 2/64 loss: 0.00959765911102295
Batch 3/64 loss: 0.015111088752746582
Batch 4/64 loss: 0.013678252696990967
Batch 5/64 loss: 0.023926198482513428
Batch 6/64 loss: 0.015099704265594482
Batch 7/64 loss: 0.02873927354812622
Batch 8/64 loss: 0.005702793598175049
Batch 9/64 loss: 0.010721802711486816
Batch 10/64 loss: 0.004310488700866699
Batch 11/64 loss: 0.022410035133361816
Batch 12/64 loss: 0.004738807678222656
Batch 13/64 loss: 0.011663317680358887
Batch 14/64 loss: 0.011460185050964355
Batch 15/64 loss: 0.007371485233306885
Batch 16/64 loss: 0.01784414052963257
Batch 17/64 loss: 0.006249666213989258
Batch 18/64 loss: 0.01731640100479126
Batch 19/64 loss: 0.03339183330535889
Batch 20/64 loss: 0.012377917766571045
Batch 21/64 loss: 0.015662968158721924
Batch 22/64 loss: 0.024998486042022705
Batch 23/64 loss: 0.001669764518737793
Batch 24/64 loss: 0.01448148488998413
Batch 25/64 loss: 0.013688325881958008
Batch 26/64 loss: 0.00948786735534668
Batch 27/64 loss: 0.009046852588653564
Batch 28/64 loss: 0.018278002738952637
Batch 29/64 loss: 0.024581074714660645
Batch 30/64 loss: 0.0094757080078125
Batch 31/64 loss: 0.008239805698394775
Batch 32/64 loss: 0.014815151691436768
Batch 33/64 loss: 0.020489037036895752
Batch 34/64 loss: 0.030137956142425537
Batch 35/64 loss: 0.02145969867706299
Batch 36/64 loss: 0.00715482234954834
Batch 37/64 loss: 0.026813983917236328
Batch 38/64 loss: 0.01606982946395874
Batch 39/64 loss: 0.014841973781585693
Batch 40/64 loss: 0.02229386568069458
Batch 41/64 loss: 0.018266797065734863
Batch 42/64 loss: 0.010004639625549316
Batch 43/64 loss: 0.008817911148071289
Batch 44/64 loss: 0.0008462071418762207
Batch 45/64 loss: 0.0055435895919799805
Batch 46/64 loss: 0.03233271837234497
Batch 47/64 loss: 0.003631591796875
Batch 48/64 loss: 0.017964959144592285
Batch 49/64 loss: 0.02490377426147461
Batch 50/64 loss: 0.014337301254272461
Batch 51/64 loss: 0.019580483436584473
Batch 52/64 loss: 0.040396690368652344
Batch 53/64 loss: 0.01693594455718994
Batch 54/64 loss: 0.020349442958831787
Batch 55/64 loss: 0.007884860038757324
Batch 56/64 loss: 0.023704349994659424
Batch 57/64 loss: 0.022897541522979736
Batch 58/64 loss: 0.010393977165222168
Batch 59/64 loss: 0.03712725639343262
Batch 60/64 loss: 0.014593005180358887
Batch 61/64 loss: 0.014072418212890625
Batch 62/64 loss: 0.005687832832336426
Batch 63/64 loss: 0.0116654634475708
Batch 64/64 loss: 0.00932610034942627
Epoch 274  Train loss: 0.015586510359072218  Val loss: 0.08752231561031538
Epoch 275
-------------------------------
Batch 1/64 loss: 0.008436918258666992
Batch 2/64 loss: 0.031561315059661865
Batch 3/64 loss: 0.007685542106628418
Batch 4/64 loss: 0.014396190643310547
Batch 5/64 loss: 0.011999785900115967
Batch 6/64 loss: 0.020737528800964355
Batch 7/64 loss: 0.014429450035095215
Batch 8/64 loss: 0.01553565263748169
Batch 9/64 loss: 0.02493947744369507
Batch 10/64 loss: 0.014447569847106934
Batch 11/64 loss: 0.017796874046325684
Batch 12/64 loss: 0.019610345363616943
Batch 13/64 loss: 0.007236957550048828
Batch 14/64 loss: 0.0113944411277771
Batch 15/64 loss: 0.02079981565475464
Batch 16/64 loss: 0.03099769353866577
Batch 17/64 loss: 0.02286696434020996
Batch 18/64 loss: 0.017100095748901367
Batch 19/64 loss: 0.001695394515991211
Batch 20/64 loss: 0.019105851650238037
Batch 21/64 loss: 0.014619588851928711
Batch 22/64 loss: 0.011467516422271729
Batch 23/64 loss: 0.02708280086517334
Batch 24/64 loss: 0.012567877769470215
Batch 25/64 loss: 0.021191954612731934
Batch 26/64 loss: 0.017355680465698242
Batch 27/64 loss: 0.01661217212677002
Batch 28/64 loss: 0.02184903621673584
Batch 29/64 loss: 0.00871199369430542
Batch 30/64 loss: 0.02132624387741089
Batch 31/64 loss: 0.019258856773376465
Batch 32/64 loss: 0.015513718128204346
Batch 33/64 loss: 0.027292907238006592
Batch 34/64 loss: 0.008609652519226074
Batch 35/64 loss: 0.028788983821868896
Batch 36/64 loss: 0.019786477088928223
Batch 37/64 loss: 0.018231689929962158
Batch 38/64 loss: 0.02037811279296875
Batch 39/64 loss: 0.028545022010803223
Batch 40/64 loss: 0.014920294284820557
Batch 41/64 loss: 0.01764589548110962
Batch 42/64 loss: 0.018222570419311523
Batch 43/64 loss: 0.006862163543701172
Batch 44/64 loss: 0.01273888349533081
Batch 45/64 loss: 0.015067636966705322
Batch 46/64 loss: 0.019492268562316895
Batch 47/64 loss: 0.01256716251373291
Batch 48/64 loss: 0.023710012435913086
Batch 49/64 loss: 0.01698446273803711
Batch 50/64 loss: 0.007605195045471191
Batch 51/64 loss: 0.005203425884246826
Batch 52/64 loss: 0.015491724014282227
Batch 53/64 loss: 0.006743788719177246
Batch 54/64 loss: 0.02170884609222412
Batch 55/64 loss: 0.025899291038513184
Batch 56/64 loss: 0.03312712907791138
Batch 57/64 loss: 0.02264571189880371
Batch 58/64 loss: 0.018506765365600586
Batch 59/64 loss: 0.019006669521331787
Batch 60/64 loss: 0.013978123664855957
Batch 61/64 loss: 0.023846149444580078
Batch 62/64 loss: 0.012823402881622314
Batch 63/64 loss: 0.015909016132354736
Batch 64/64 loss: 0.04266023635864258
Epoch 275  Train loss: 0.017641818289663276  Val loss: 0.08822413183159844
Epoch 276
-------------------------------
Batch 1/64 loss: 0.018643736839294434
Batch 2/64 loss: 0.017763495445251465
Batch 3/64 loss: 0.023838400840759277
Batch 4/64 loss: 0.028508663177490234
Batch 5/64 loss: 0.012394189834594727
Batch 6/64 loss: 0.014480173587799072
Batch 7/64 loss: 0.02452826499938965
Batch 8/64 loss: 0.01504892110824585
Batch 9/64 loss: 0.006480097770690918
Batch 10/64 loss: 0.014877915382385254
Batch 11/64 loss: 0.003969311714172363
Batch 12/64 loss: 0.028090178966522217
Batch 13/64 loss: 0.021326720714569092
Batch 14/64 loss: 0.030956149101257324
Batch 15/64 loss: 0.00822216272354126
Batch 16/64 loss: 0.008138418197631836
Batch 17/64 loss: 0.01746547222137451
Batch 18/64 loss: 0.022389769554138184
Batch 19/64 loss: 0.003518342971801758
Batch 20/64 loss: 0.01692795753479004
Batch 21/64 loss: 0.01637047529220581
Batch 22/64 loss: -0.0010619163513183594
Batch 23/64 loss: 0.010889649391174316
Batch 24/64 loss: 0.019994020462036133
Batch 25/64 loss: 0.013941645622253418
Batch 26/64 loss: 0.00942850112915039
Batch 27/64 loss: 0.033210039138793945
Batch 28/64 loss: 0.017748594284057617
Batch 29/64 loss: 0.017119884490966797
Batch 30/64 loss: 0.030328035354614258
Batch 31/64 loss: 0.05508369207382202
Batch 32/64 loss: 0.012078702449798584
Batch 33/64 loss: 0.015241742134094238
Batch 34/64 loss: 0.01670694351196289
Batch 35/64 loss: 0.019253015518188477
Batch 36/64 loss: 0.012822091579437256
Batch 37/64 loss: 0.0115242600440979
Batch 38/64 loss: 0.010856688022613525
Batch 39/64 loss: 0.01275414228439331
Batch 40/64 loss: 0.023701131343841553
Batch 41/64 loss: 0.02402675151824951
Batch 42/64 loss: 0.018644750118255615
Batch 43/64 loss: 0.02690601348876953
Batch 44/64 loss: 0.01973778009414673
Batch 45/64 loss: 0.021733999252319336
Batch 46/64 loss: 0.0005483627319335938
Batch 47/64 loss: 0.008894920349121094
Batch 48/64 loss: 0.023548424243927002
Batch 49/64 loss: 0.017658770084381104
Batch 50/64 loss: 0.0096779465675354
Batch 51/64 loss: 0.014952600002288818
Batch 52/64 loss: 0.015165746212005615
Batch 53/64 loss: 0.020027220249176025
Batch 54/64 loss: 0.024378955364227295
Batch 55/64 loss: 0.028055250644683838
Batch 56/64 loss: 0.023430824279785156
Batch 57/64 loss: 0.012772142887115479
Batch 58/64 loss: 0.027604281902313232
Batch 59/64 loss: 0.022791624069213867
Batch 60/64 loss: 0.024638116359710693
Batch 61/64 loss: 0.020945847034454346
Batch 62/64 loss: 0.013508081436157227
Batch 63/64 loss: 0.01285475492477417
Batch 64/64 loss: 0.023375868797302246
Epoch 276  Train loss: 0.01797011366077498  Val loss: 0.08715310457236168
Epoch 277
-------------------------------
Batch 1/64 loss: 0.0011031627655029297
Batch 2/64 loss: 0.028383493423461914
Batch 3/64 loss: 0.012884557247161865
Batch 4/64 loss: 0.010221421718597412
Batch 5/64 loss: 0.005426287651062012
Batch 6/64 loss: 0.002150118350982666
Batch 7/64 loss: 0.010206878185272217
Batch 8/64 loss: 0.0003833770751953125
Batch 9/64 loss: 0.007203340530395508
Batch 10/64 loss: 0.01956254243850708
Batch 11/64 loss: 0.015453219413757324
Batch 12/64 loss: 0.0179290771484375
Batch 13/64 loss: 0.021546006202697754
Batch 14/64 loss: 0.013084888458251953
Batch 15/64 loss: 0.020696401596069336
Batch 16/64 loss: 0.027393698692321777
Batch 17/64 loss: 0.011419534683227539
Batch 18/64 loss: 0.02162933349609375
Batch 19/64 loss: 0.013832151889801025
Batch 20/64 loss: 0.029878556728363037
Batch 21/64 loss: 0.0026581287384033203
Batch 22/64 loss: 0.021084368228912354
Batch 23/64 loss: 0.01358109712600708
Batch 24/64 loss: 0.037484049797058105
Batch 25/64 loss: 0.015709280967712402
Batch 26/64 loss: 0.00862663984298706
Batch 27/64 loss: 0.013394176959991455
Batch 28/64 loss: 0.022914648056030273
Batch 29/64 loss: 0.007932782173156738
Batch 30/64 loss: 0.005477488040924072
Batch 31/64 loss: 0.004088103771209717
Batch 32/64 loss: 0.016386091709136963
Batch 33/64 loss: 0.010535836219787598
Batch 34/64 loss: 0.017239749431610107
Batch 35/64 loss: 0.01428002119064331
Batch 36/64 loss: 0.017439305782318115
Batch 37/64 loss: 0.01280587911605835
Batch 38/64 loss: 0.027930617332458496
Batch 39/64 loss: 0.014288663864135742
Batch 40/64 loss: 0.004659533500671387
Batch 41/64 loss: 0.020005464553833008
Batch 42/64 loss: 0.02159583568572998
Batch 43/64 loss: 0.0004917383193969727
Batch 44/64 loss: 0.011309564113616943
Batch 45/64 loss: 0.0001774430274963379
Batch 46/64 loss: 0.023036479949951172
Batch 47/64 loss: 0.02733314037322998
Batch 48/64 loss: 0.018869340419769287
Batch 49/64 loss: 0.011597394943237305
Batch 50/64 loss: 0.02604621648788452
Batch 51/64 loss: 0.01413261890411377
Batch 52/64 loss: 0.016483724117279053
Batch 53/64 loss: 0.016534805297851562
Batch 54/64 loss: 0.01230621337890625
Batch 55/64 loss: 0.015810608863830566
Batch 56/64 loss: 0.019167423248291016
Batch 57/64 loss: 0.01122581958770752
Batch 58/64 loss: 0.014039218425750732
Batch 59/64 loss: 0.014471828937530518
Batch 60/64 loss: 0.024478256702423096
Batch 61/64 loss: 0.013388395309448242
Batch 62/64 loss: 0.013849139213562012
Batch 63/64 loss: 0.023087739944458008
Batch 64/64 loss: 0.014587700366973877
Epoch 277  Train loss: 0.015047587366665111  Val loss: 0.08998812617305219
Epoch 278
-------------------------------
Batch 1/64 loss: 0.004004955291748047
Batch 2/64 loss: 0.02013838291168213
Batch 3/64 loss: 0.003548860549926758
Batch 4/64 loss: 0.0030440688133239746
Batch 5/64 loss: 0.007513284683227539
Batch 6/64 loss: 0.009481668472290039
Batch 7/64 loss: 0.017798304557800293
Batch 8/64 loss: 0.019821226596832275
Batch 9/64 loss: 0.017668068408966064
Batch 10/64 loss: 0.031794071197509766
Batch 11/64 loss: 0.01040428876876831
Batch 12/64 loss: 0.008929133415222168
Batch 13/64 loss: 0.01676720380783081
Batch 14/64 loss: 0.0189741849899292
Batch 15/64 loss: 0.017264068126678467
Batch 16/64 loss: 0.001743912696838379
Batch 17/64 loss: 0.010444402694702148
Batch 18/64 loss: 0.007614612579345703
Batch 19/64 loss: -0.00048464536666870117
Batch 20/64 loss: 0.0128556489944458
Batch 21/64 loss: 0.020209848880767822
Batch 22/64 loss: 0.018486499786376953
Batch 23/64 loss: 0.012179255485534668
Batch 24/64 loss: 0.008100628852844238
Batch 25/64 loss: 0.013236701488494873
Batch 26/64 loss: 0.013515830039978027
Batch 27/64 loss: 0.020982563495635986
Batch 28/64 loss: 0.016419529914855957
Batch 29/64 loss: 0.019156992435455322
Batch 30/64 loss: 0.012949943542480469
Batch 31/64 loss: 0.004229366779327393
Batch 32/64 loss: 0.006168663501739502
Batch 33/64 loss: 0.015967845916748047
Batch 34/64 loss: 0.0037276744842529297
Batch 35/64 loss: 0.017337262630462646
Batch 36/64 loss: 0.0035505294799804688
Batch 37/64 loss: 0.017718076705932617
Batch 38/64 loss: 0.01426088809967041
Batch 39/64 loss: 0.028830349445343018
Batch 40/64 loss: 0.010279536247253418
Batch 41/64 loss: 0.008801102638244629
Batch 42/64 loss: 0.015844225883483887
Batch 43/64 loss: 0.002832353115081787
Batch 44/64 loss: 0.014828681945800781
Batch 45/64 loss: 0.01591801643371582
Batch 46/64 loss: 0.018854856491088867
Batch 47/64 loss: 0.022865891456604004
Batch 48/64 loss: 0.0146445631980896
Batch 49/64 loss: 0.024101555347442627
Batch 50/64 loss: 0.03235846757888794
Batch 51/64 loss: 0.017513811588287354
Batch 52/64 loss: 0.02240443229675293
Batch 53/64 loss: 0.012739837169647217
Batch 54/64 loss: 0.01595097780227661
Batch 55/64 loss: 0.019423246383666992
Batch 56/64 loss: 0.023617267608642578
Batch 57/64 loss: 0.009339213371276855
Batch 58/64 loss: -0.00211179256439209
Batch 59/64 loss: 0.02040541172027588
Batch 60/64 loss: 0.02397763729095459
Batch 61/64 loss: 0.015399575233459473
Batch 62/64 loss: 0.03833746910095215
Batch 63/64 loss: -0.00012600421905517578
Batch 64/64 loss: 0.010499954223632812
Epoch 278  Train loss: 0.014312618853999119  Val loss: 0.08573493637989477
Epoch 279
-------------------------------
Batch 1/64 loss: 0.004242300987243652
Batch 2/64 loss: 0.009159624576568604
Batch 3/64 loss: 0.010384619235992432
Batch 4/64 loss: 0.0071527957916259766
Batch 5/64 loss: 0.0063207149505615234
Batch 6/64 loss: 0.004024624824523926
Batch 7/64 loss: 0.016882777214050293
Batch 8/64 loss: 0.011891722679138184
Batch 9/64 loss: 0.007800877094268799
Batch 10/64 loss: 0.009966135025024414
Batch 11/64 loss: -0.004367172718048096
Batch 12/64 loss: 0.007926106452941895
Batch 13/64 loss: 0.009003221988677979
Batch 14/64 loss: 0.006582021713256836
Batch 15/64 loss: 0.01332855224609375
Batch 16/64 loss: 0.0013127923011779785
Batch 17/64 loss: 0.02577918767929077
Batch 18/64 loss: 0.011866390705108643
Batch 19/64 loss: 0.03283339738845825
Batch 20/64 loss: 0.03826349973678589
Batch 21/64 loss: 0.004832565784454346
Batch 22/64 loss: 0.012931466102600098
Batch 23/64 loss: 0.0225641131401062
Batch 24/64 loss: 0.012890875339508057
Batch 25/64 loss: 0.0029082894325256348
Batch 26/64 loss: 0.026227295398712158
Batch 27/64 loss: 0.013772010803222656
Batch 28/64 loss: 0.013245999813079834
Batch 29/64 loss: 0.008721768856048584
Batch 30/64 loss: 0.0212288498878479
Batch 31/64 loss: 0.027773916721343994
Batch 32/64 loss: 0.012823820114135742
Batch 33/64 loss: 0.015445053577423096
Batch 34/64 loss: 0.014197826385498047
Batch 35/64 loss: 0.021289348602294922
Batch 36/64 loss: 0.01386028528213501
Batch 37/64 loss: 0.008729040622711182
Batch 38/64 loss: 0.017136216163635254
Batch 39/64 loss: 0.003454923629760742
Batch 40/64 loss: 0.003494560718536377
Batch 41/64 loss: 0.0062239766120910645
Batch 42/64 loss: 0.0005055069923400879
Batch 43/64 loss: 0.030866146087646484
Batch 44/64 loss: 0.027225732803344727
Batch 45/64 loss: 0.02122730016708374
Batch 46/64 loss: 0.023368418216705322
Batch 47/64 loss: 0.012699246406555176
Batch 48/64 loss: 0.020969390869140625
Batch 49/64 loss: 0.013404667377471924
Batch 50/64 loss: 0.02866828441619873
Batch 51/64 loss: 0.019693732261657715
Batch 52/64 loss: 0.025541961193084717
Batch 53/64 loss: 0.023433685302734375
Batch 54/64 loss: 0.022355377674102783
Batch 55/64 loss: 0.006296098232269287
Batch 56/64 loss: 0.01770007610321045
Batch 57/64 loss: 0.010362088680267334
Batch 58/64 loss: 0.015472888946533203
Batch 59/64 loss: 0.03152608871459961
Batch 60/64 loss: 0.026396334171295166
Batch 61/64 loss: 0.0060613155364990234
Batch 62/64 loss: 0.008099079132080078
Batch 63/64 loss: 0.014255225658416748
Batch 64/64 loss: 0.016998469829559326
Epoch 279  Train loss: 0.014635041414522657  Val loss: 0.08797806071251937
Epoch 280
-------------------------------
Batch 1/64 loss: 0.011071562767028809
Batch 2/64 loss: 0.01145702600479126
Batch 3/64 loss: 0.012525081634521484
Batch 4/64 loss: 0.010109543800354004
Batch 5/64 loss: 0.012147963047027588
Batch 6/64 loss: -0.000607609748840332
Batch 7/64 loss: 0.020480096340179443
Batch 8/64 loss: 0.019757986068725586
Batch 9/64 loss: 0.0015562176704406738
Batch 10/64 loss: 0.008092939853668213
Batch 11/64 loss: 0.02882826328277588
Batch 12/64 loss: -0.001940608024597168
Batch 13/64 loss: 0.015276789665222168
Batch 14/64 loss: 0.003177940845489502
Batch 15/64 loss: 0.03026372194290161
Batch 16/64 loss: 0.028532207012176514
Batch 17/64 loss: 0.01573842763900757
Batch 18/64 loss: 0.012454688549041748
Batch 19/64 loss: 0.020995140075683594
Batch 20/64 loss: -0.0005519986152648926
Batch 21/64 loss: 0.006526529788970947
Batch 22/64 loss: 0.01860368251800537
Batch 23/64 loss: 0.026722609996795654
Batch 24/64 loss: 0.012985825538635254
Batch 25/64 loss: 0.02320164442062378
Batch 26/64 loss: 0.015417933464050293
Batch 27/64 loss: 0.013494610786437988
Batch 28/64 loss: 0.03696143627166748
Batch 29/64 loss: 0.014134466648101807
Batch 30/64 loss: 0.008232295513153076
Batch 31/64 loss: 0.011486232280731201
Batch 32/64 loss: 0.014066755771636963
Batch 33/64 loss: 0.024740934371948242
Batch 34/64 loss: 0.007581233978271484
Batch 35/64 loss: 0.02008521556854248
Batch 36/64 loss: 0.005967617034912109
Batch 37/64 loss: 0.00877612829208374
Batch 38/64 loss: 0.009081661701202393
Batch 39/64 loss: 0.012628376483917236
Batch 40/64 loss: 0.016469836235046387
Batch 41/64 loss: 0.0158349871635437
Batch 42/64 loss: 0.002148151397705078
Batch 43/64 loss: 0.013202309608459473
Batch 44/64 loss: 0.010816693305969238
Batch 45/64 loss: 0.01069408655166626
Batch 46/64 loss: 0.010073542594909668
Batch 47/64 loss: 0.019134104251861572
Batch 48/64 loss: 0.03587746620178223
Batch 49/64 loss: 0.014678239822387695
Batch 50/64 loss: 0.013366401195526123
Batch 51/64 loss: 0.015311360359191895
Batch 52/64 loss: 0.021044015884399414
Batch 53/64 loss: 0.02758103609085083
Batch 54/64 loss: 0.0028705596923828125
Batch 55/64 loss: 0.003080010414123535
Batch 56/64 loss: 0.009899020195007324
Batch 57/64 loss: 0.004298686981201172
Batch 58/64 loss: 0.008575022220611572
Batch 59/64 loss: 0.005420207977294922
Batch 60/64 loss: 0.0006420612335205078
Batch 61/64 loss: -0.00016057491302490234
Batch 62/64 loss: 0.009979724884033203
Batch 63/64 loss: 0.01866525411605835
Batch 64/64 loss: 0.02015995979309082
Epoch 280  Train loss: 0.013406788134107403  Val loss: 0.08806737345928178
Epoch 281
-------------------------------
Batch 1/64 loss: 0.0031485557556152344
Batch 2/64 loss: 0.0007011294364929199
Batch 3/64 loss: 0.006529390811920166
Batch 4/64 loss: 0.021425724029541016
Batch 5/64 loss: 0.008546769618988037
Batch 6/64 loss: 0.0007843375205993652
Batch 7/64 loss: -0.0013735294342041016
Batch 8/64 loss: 0.01707702875137329
Batch 9/64 loss: 0.010406255722045898
Batch 10/64 loss: 0.023196697235107422
Batch 11/64 loss: 0.021283507347106934
Batch 12/64 loss: 0.021673083305358887
Batch 13/64 loss: 0.006754636764526367
Batch 14/64 loss: 0.008138656616210938
Batch 15/64 loss: 0.010349571704864502
Batch 16/64 loss: 0.008724987506866455
Batch 17/64 loss: 0.04489248991012573
Batch 18/64 loss: 0.02454608678817749
Batch 19/64 loss: 0.0025862455368041992
Batch 20/64 loss: 0.022230803966522217
Batch 21/64 loss: 0.007382452487945557
Batch 22/64 loss: 0.012159585952758789
Batch 23/64 loss: 0.011357486248016357
Batch 24/64 loss: 0.009131014347076416
Batch 25/64 loss: 0.024173498153686523
Batch 26/64 loss: 0.016272008419036865
Batch 27/64 loss: 0.015822410583496094
Batch 28/64 loss: 0.017547309398651123
Batch 29/64 loss: 0.018070578575134277
Batch 30/64 loss: 0.002744138240814209
Batch 31/64 loss: 0.009432375431060791
Batch 32/64 loss: 0.020403146743774414
Batch 33/64 loss: 0.017843008041381836
Batch 34/64 loss: 0.02368748188018799
Batch 35/64 loss: -0.004042506217956543
Batch 36/64 loss: -0.006536602973937988
Batch 37/64 loss: 0.007491767406463623
Batch 38/64 loss: 0.017270147800445557
Batch 39/64 loss: 0.014247596263885498
Batch 40/64 loss: -0.002955913543701172
Batch 41/64 loss: 0.006782948970794678
Batch 42/64 loss: 0.011194467544555664
Batch 43/64 loss: 0.012502849102020264
Batch 44/64 loss: 0.013502180576324463
Batch 45/64 loss: 0.014144539833068848
Batch 46/64 loss: 0.019752442836761475
Batch 47/64 loss: 0.005033254623413086
Batch 48/64 loss: 0.01606428623199463
Batch 49/64 loss: 0.0023491978645324707
Batch 50/64 loss: 0.017397046089172363
Batch 51/64 loss: 0.0230141282081604
Batch 52/64 loss: 0.00541609525680542
Batch 53/64 loss: 0.00025260448455810547
Batch 54/64 loss: 0.012454032897949219
Batch 55/64 loss: 0.01900261640548706
Batch 56/64 loss: -0.0015231966972351074
Batch 57/64 loss: 0.016823947429656982
Batch 58/64 loss: 0.029001832008361816
Batch 59/64 loss: 0.027880072593688965
Batch 60/64 loss: 0.0030066967010498047
Batch 61/64 loss: 0.020749330520629883
Batch 62/64 loss: 0.012038886547088623
Batch 63/64 loss: 0.01696634292602539
Batch 64/64 loss: 0.025066137313842773
Epoch 281  Train loss: 0.012795758714862899  Val loss: 0.090346614724582
Epoch 282
-------------------------------
Batch 1/64 loss: 0.004170656204223633
Batch 2/64 loss: 0.00015431642532348633
Batch 3/64 loss: 0.013852953910827637
Batch 4/64 loss: -0.001013636589050293
Batch 5/64 loss: 0.01928502321243286
Batch 6/64 loss: 0.0178372859954834
Batch 7/64 loss: 0.007848918437957764
Batch 8/64 loss: 0.009980440139770508
Batch 9/64 loss: -0.0027832984924316406
Batch 10/64 loss: -0.001520693302154541
Batch 11/64 loss: 0.0008224844932556152
Batch 12/64 loss: 0.016691386699676514
Batch 13/64 loss: 0.0054468512535095215
Batch 14/64 loss: 0.02580314874649048
Batch 15/64 loss: 0.028060078620910645
Batch 16/64 loss: 0.010268986225128174
Batch 17/64 loss: -0.003934919834136963
Batch 18/64 loss: 0.010959923267364502
Batch 19/64 loss: 0.013367235660552979
Batch 20/64 loss: 0.008966982364654541
Batch 21/64 loss: 0.002064824104309082
Batch 22/64 loss: 0.02607142925262451
Batch 23/64 loss: 0.019332706928253174
Batch 24/64 loss: 0.007201015949249268
Batch 25/64 loss: 0.026840686798095703
Batch 26/64 loss: 0.015800178050994873
Batch 27/64 loss: 0.018463313579559326
Batch 28/64 loss: 0.01108860969543457
Batch 29/64 loss: 0.03021836280822754
Batch 30/64 loss: 0.02362591028213501
Batch 31/64 loss: 0.011570096015930176
Batch 32/64 loss: 0.03144478797912598
Batch 33/64 loss: 0.03423386812210083
Batch 34/64 loss: 0.007160663604736328
Batch 35/64 loss: 0.010415256023406982
Batch 36/64 loss: 0.013194084167480469
Batch 37/64 loss: 0.019460320472717285
Batch 38/64 loss: 0.0077114105224609375
Batch 39/64 loss: 0.006615877151489258
Batch 40/64 loss: 0.026820838451385498
Batch 41/64 loss: 0.016497313976287842
Batch 42/64 loss: 0.0049111247062683105
Batch 43/64 loss: 0.009888410568237305
Batch 44/64 loss: 0.014530718326568604
Batch 45/64 loss: 0.010792016983032227
Batch 46/64 loss: 0.024449288845062256
Batch 47/64 loss: 0.0220034122467041
Batch 48/64 loss: -0.0012273192405700684
Batch 49/64 loss: 0.014548242092132568
Batch 50/64 loss: 0.009310483932495117
Batch 51/64 loss: -0.003935277462005615
Batch 52/64 loss: 0.005935490131378174
Batch 53/64 loss: 0.0153542160987854
Batch 54/64 loss: 0.014521896839141846
Batch 55/64 loss: 0.013929545879364014
Batch 56/64 loss: 0.016697227954864502
Batch 57/64 loss: 0.02290475368499756
Batch 58/64 loss: 0.03001779317855835
Batch 59/64 loss: 0.016116678714752197
Batch 60/64 loss: 0.025331556797027588
Batch 61/64 loss: 0.01281648874282837
Batch 62/64 loss: 0.024893999099731445
Batch 63/64 loss: 0.03101128339767456
Batch 64/64 loss: 0.015703916549682617
Epoch 282  Train loss: 0.014065500334197399  Val loss: 0.09033025765337076
Epoch 283
-------------------------------
Batch 1/64 loss: 0.01055830717086792
Batch 2/64 loss: 0.00233614444732666
Batch 3/64 loss: 0.0046416521072387695
Batch 4/64 loss: 0.0071830153465271
Batch 5/64 loss: 0.021767020225524902
Batch 6/64 loss: 0.004075467586517334
Batch 7/64 loss: -0.000986933708190918
Batch 8/64 loss: 0.04824894666671753
Batch 9/64 loss: 0.01047980785369873
Batch 10/64 loss: 0.008841216564178467
Batch 11/64 loss: 0.00670778751373291
Batch 12/64 loss: 0.0053841471672058105
Batch 13/64 loss: 0.007344484329223633
Batch 14/64 loss: -0.0052912235260009766
Batch 15/64 loss: 0.016536593437194824
Batch 16/64 loss: 0.02054905891418457
Batch 17/64 loss: 0.02960413694381714
Batch 18/64 loss: 0.012542009353637695
Batch 19/64 loss: 0.0006814002990722656
Batch 20/64 loss: 0.006558537483215332
Batch 21/64 loss: 0.011936962604522705
Batch 22/64 loss: 0.011950552463531494
Batch 23/64 loss: 0.015940070152282715
Batch 24/64 loss: 0.03705155849456787
Batch 25/64 loss: 0.005822300910949707
Batch 26/64 loss: 0.014648973941802979
Batch 27/64 loss: 0.003467857837677002
Batch 28/64 loss: 0.005617201328277588
Batch 29/64 loss: 0.014864504337310791
Batch 30/64 loss: 0.0006778240203857422
Batch 31/64 loss: 0.015779435634613037
Batch 32/64 loss: 0.006600797176361084
Batch 33/64 loss: 0.028524458408355713
Batch 34/64 loss: 0.008954763412475586
Batch 35/64 loss: 0.0064237117767333984
Batch 36/64 loss: 0.03493976593017578
Batch 37/64 loss: 0.01271200180053711
Batch 38/64 loss: 0.011090397834777832
Batch 39/64 loss: 0.0051424503326416016
Batch 40/64 loss: 0.02512061595916748
Batch 41/64 loss: 0.008086323738098145
Batch 42/64 loss: 0.004306793212890625
Batch 43/64 loss: 0.01754140853881836
Batch 44/64 loss: 0.014365673065185547
Batch 45/64 loss: 0.003827989101409912
Batch 46/64 loss: 0.0030588507652282715
Batch 47/64 loss: 0.0055294036865234375
Batch 48/64 loss: 0.01384645700454712
Batch 49/64 loss: 1.7523765563964844e-05
Batch 50/64 loss: 0.010839462280273438
Batch 51/64 loss: 0.00719606876373291
Batch 52/64 loss: 0.0026784539222717285
Batch 53/64 loss: 0.040756285190582275
Batch 54/64 loss: 0.0222894549369812
Batch 55/64 loss: 0.014895737171173096
Batch 56/64 loss: 0.008547663688659668
Batch 57/64 loss: 0.02271449565887451
Batch 58/64 loss: 0.00361788272857666
Batch 59/64 loss: 0.039873361587524414
Batch 60/64 loss: 0.0216941237449646
Batch 61/64 loss: 0.010291099548339844
Batch 62/64 loss: 0.003209710121154785
Batch 63/64 loss: 0.010687649250030518
Batch 64/64 loss: 0.019274115562438965
Epoch 283  Train loss: 0.012538905704722684  Val loss: 0.08768253596787601
Epoch 284
-------------------------------
Batch 1/64 loss: 0.0010869503021240234
Batch 2/64 loss: 0.02021312713623047
Batch 3/64 loss: 0.01480722427368164
Batch 4/64 loss: -0.0034491419792175293
Batch 5/64 loss: 0.013080477714538574
Batch 6/64 loss: 0.018317341804504395
Batch 7/64 loss: 0.0144575834274292
Batch 8/64 loss: 0.0076326727867126465
Batch 9/64 loss: 0.021281063556671143
Batch 10/64 loss: 0.009428560733795166
Batch 11/64 loss: 0.007252335548400879
Batch 12/64 loss: 0.005460977554321289
Batch 13/64 loss: 0.006240189075469971
Batch 14/64 loss: 0.011701226234436035
Batch 15/64 loss: 0.007255196571350098
Batch 16/64 loss: 0.007002294063568115
Batch 17/64 loss: 0.006242573261260986
Batch 18/64 loss: 0.006662607192993164
Batch 19/64 loss: 0.0023802518844604492
Batch 20/64 loss: 0.0073239803314208984
Batch 21/64 loss: 0.00952225923538208
Batch 22/64 loss: -0.0002727508544921875
Batch 23/64 loss: 0.01637589931488037
Batch 24/64 loss: 0.02145540714263916
Batch 25/64 loss: 0.01360630989074707
Batch 26/64 loss: -0.002172708511352539
Batch 27/64 loss: 0.011718034744262695
Batch 28/64 loss: 0.023535311222076416
Batch 29/64 loss: 0.023326635360717773
Batch 30/64 loss: 0.011850059032440186
Batch 31/64 loss: 0.010224759578704834
Batch 32/64 loss: 0.010396778583526611
Batch 33/64 loss: 0.0172804594039917
Batch 34/64 loss: 0.015942752361297607
Batch 35/64 loss: 0.010753750801086426
Batch 36/64 loss: 0.027880609035491943
Batch 37/64 loss: 0.01746213436126709
Batch 38/64 loss: 0.004146575927734375
Batch 39/64 loss: 0.0018918514251708984
Batch 40/64 loss: 0.006927073001861572
Batch 41/64 loss: 0.003574371337890625
Batch 42/64 loss: 0.005464375019073486
Batch 43/64 loss: 0.01494210958480835
Batch 44/64 loss: 0.007410228252410889
Batch 45/64 loss: 0.01238161325454712
Batch 46/64 loss: 0.006622493267059326
Batch 47/64 loss: 0.02753204107284546
Batch 48/64 loss: 0.024572014808654785
Batch 49/64 loss: 0.011963367462158203
Batch 50/64 loss: 0.021719694137573242
Batch 51/64 loss: 0.015143096446990967
Batch 52/64 loss: 0.022089600563049316
Batch 53/64 loss: 0.0009160041809082031
Batch 54/64 loss: 0.012235403060913086
Batch 55/64 loss: 0.015729129314422607
Batch 56/64 loss: -0.0002694129943847656
Batch 57/64 loss: 0.026374459266662598
Batch 58/64 loss: 0.0023579001426696777
Batch 59/64 loss: 0.014634847640991211
Batch 60/64 loss: 0.009569108486175537
Batch 61/64 loss: 0.005361795425415039
Batch 62/64 loss: 0.0038360953330993652
Batch 63/64 loss: 0.024044573307037354
Batch 64/64 loss: 0.016344189643859863
Epoch 284  Train loss: 0.011555948911928663  Val loss: 0.0874367434134598
Epoch 285
-------------------------------
Batch 1/64 loss: 0.029720306396484375
Batch 2/64 loss: 0.009083211421966553
Batch 3/64 loss: 0.01596933603286743
Batch 4/64 loss: 0.014173269271850586
Batch 5/64 loss: -0.00160139799118042
Batch 6/64 loss: 0.009747028350830078
Batch 7/64 loss: 0.009851038455963135
Batch 8/64 loss: 0.02117002010345459
Batch 9/64 loss: 0.0035633444786071777
Batch 10/64 loss: 0.016511201858520508
Batch 11/64 loss: 0.030012905597686768
Batch 12/64 loss: 0.012515366077423096
Batch 13/64 loss: -0.0019500255584716797
Batch 14/64 loss: 0.0135878324508667
Batch 15/64 loss: 0.015555977821350098
Batch 16/64 loss: 0.023037075996398926
Batch 17/64 loss: 0.005746603012084961
Batch 18/64 loss: 0.0076029300689697266
Batch 19/64 loss: 0.0146331787109375
Batch 20/64 loss: 0.018763601779937744
Batch 21/64 loss: 0.010775446891784668
Batch 22/64 loss: 0.012429893016815186
Batch 23/64 loss: 0.01987367868423462
Batch 24/64 loss: 0.007426917552947998
Batch 25/64 loss: 0.00906825065612793
Batch 26/64 loss: -0.002160966396331787
Batch 27/64 loss: 0.012740135192871094
Batch 28/64 loss: 0.01610696315765381
Batch 29/64 loss: 0.011812448501586914
Batch 30/64 loss: 0.01459360122680664
Batch 31/64 loss: 0.0005709528923034668
Batch 32/64 loss: 0.007377266883850098
Batch 33/64 loss: 0.012190401554107666
Batch 34/64 loss: 0.02087688446044922
Batch 35/64 loss: 0.011623978614807129
Batch 36/64 loss: 0.009263455867767334
Batch 37/64 loss: 0.04543942213058472
Batch 38/64 loss: 0.01412290334701538
Batch 39/64 loss: 0.009249687194824219
Batch 40/64 loss: 0.026003479957580566
Batch 41/64 loss: 0.0038650035858154297
Batch 42/64 loss: 0.006520509719848633
Batch 43/64 loss: 0.013121068477630615
Batch 44/64 loss: 0.012875139713287354
Batch 45/64 loss: 0.009646713733673096
Batch 46/64 loss: 0.025417804718017578
Batch 47/64 loss: 0.009869098663330078
Batch 48/64 loss: 0.012133240699768066
Batch 49/64 loss: 0.023882925510406494
Batch 50/64 loss: 0.019026339054107666
Batch 51/64 loss: 0.013291001319885254
Batch 52/64 loss: 0.012144744396209717
Batch 53/64 loss: 0.008778095245361328
Batch 54/64 loss: 0.013087093830108643
Batch 55/64 loss: 0.02467954158782959
Batch 56/64 loss: 0.004555344581604004
Batch 57/64 loss: 0.0015196800231933594
Batch 58/64 loss: 0.0276334285736084
Batch 59/64 loss: 0.023897528648376465
Batch 60/64 loss: 0.00998079776763916
Batch 61/64 loss: 0.010888457298278809
Batch 62/64 loss: 0.01096886396408081
Batch 63/64 loss: 0.027727067470550537
Batch 64/64 loss: 0.010645151138305664
Epoch 285  Train loss: 0.013656015022128236  Val loss: 0.08789948909143402
Epoch 286
-------------------------------
Batch 1/64 loss: 0.02146250009536743
Batch 2/64 loss: 0.003949880599975586
Batch 3/64 loss: -0.0001055002212524414
Batch 4/64 loss: 0.006634652614593506
Batch 5/64 loss: -0.001911163330078125
Batch 6/64 loss: -0.0004443526268005371
Batch 7/64 loss: 0.013031721115112305
Batch 8/64 loss: 0.015124857425689697
Batch 9/64 loss: -0.00010633468627929688
Batch 10/64 loss: 0.01853477954864502
Batch 11/64 loss: 0.010808944702148438
Batch 12/64 loss: 0.005827605724334717
Batch 13/64 loss: 0.005814731121063232
Batch 14/64 loss: -0.004223763942718506
Batch 15/64 loss: 0.016584813594818115
Batch 16/64 loss: 0.02109682559967041
Batch 17/64 loss: 0.0034350156784057617
Batch 18/64 loss: 0.00831460952758789
Batch 19/64 loss: 0.01833409070968628
Batch 20/64 loss: 0.028908967971801758
Batch 21/64 loss: 0.000148773193359375
Batch 22/64 loss: 0.015455663204193115
Batch 23/64 loss: 0.011945962905883789
Batch 24/64 loss: 0.01881963014602661
Batch 25/64 loss: 0.005248069763183594
Batch 26/64 loss: 0.015428662300109863
Batch 27/64 loss: 0.025205492973327637
Batch 28/64 loss: 0.022122442722320557
Batch 29/64 loss: 0.02220284938812256
Batch 30/64 loss: 0.0023389458656311035
Batch 31/64 loss: 0.013972103595733643
Batch 32/64 loss: 0.0049825310707092285
Batch 33/64 loss: 0.012765228748321533
Batch 34/64 loss: 0.02113264799118042
Batch 35/64 loss: 0.01769125461578369
Batch 36/64 loss: 0.010339021682739258
Batch 37/64 loss: 0.02249211072921753
Batch 38/64 loss: 0.0004937648773193359
Batch 39/64 loss: 0.01330709457397461
Batch 40/64 loss: 0.02022385597229004
Batch 41/64 loss: 0.01755809783935547
Batch 42/64 loss: 0.0131264328956604
Batch 43/64 loss: 0.019236981868743896
Batch 44/64 loss: 0.02414947748184204
Batch 45/64 loss: 0.00624316930770874
Batch 46/64 loss: 0.006493568420410156
Batch 47/64 loss: 0.01705646514892578
Batch 48/64 loss: 0.011421263217926025
Batch 49/64 loss: 0.01684701442718506
Batch 50/64 loss: 0.022248685359954834
Batch 51/64 loss: 0.008484363555908203
Batch 52/64 loss: 0.00743412971496582
Batch 53/64 loss: 0.01824551820755005
Batch 54/64 loss: 0.004528164863586426
Batch 55/64 loss: 0.0018169283866882324
Batch 56/64 loss: 0.002665996551513672
Batch 57/64 loss: 0.004468083381652832
Batch 58/64 loss: 0.011582434177398682
Batch 59/64 loss: 0.02014791965484619
Batch 60/64 loss: 0.0026931166648864746
Batch 61/64 loss: 0.014318764209747314
Batch 62/64 loss: 0.021435976028442383
Batch 63/64 loss: -0.0033310651779174805
Batch 64/64 loss: 0.017856180667877197
Epoch 286  Train loss: 0.011759256615358241  Val loss: 0.08766942950048807
Epoch 287
-------------------------------
Batch 1/64 loss: 0.0010103583335876465
Batch 2/64 loss: -0.0004996657371520996
Batch 3/64 loss: 0.008630335330963135
Batch 4/64 loss: 0.001325070858001709
Batch 5/64 loss: 0.0076721906661987305
Batch 6/64 loss: 0.00033146142959594727
Batch 7/64 loss: 0.005082249641418457
Batch 8/64 loss: 0.022978484630584717
Batch 9/64 loss: 0.002703249454498291
Batch 10/64 loss: 0.020236611366271973
Batch 11/64 loss: -0.001611948013305664
Batch 12/64 loss: 0.010221600532531738
Batch 13/64 loss: -0.005879759788513184
Batch 14/64 loss: 0.019839346408843994
Batch 15/64 loss: 0.012850821018218994
Batch 16/64 loss: 0.004471898078918457
Batch 17/64 loss: 0.023456990718841553
Batch 18/64 loss: 0.016657590866088867
Batch 19/64 loss: 0.010707736015319824
Batch 20/64 loss: 0.008099198341369629
Batch 21/64 loss: 0.029057323932647705
Batch 22/64 loss: 0.015437006950378418
Batch 23/64 loss: 0.008773505687713623
Batch 24/64 loss: 0.005551815032958984
Batch 25/64 loss: 0.01043546199798584
Batch 26/64 loss: 0.001252293586730957
Batch 27/64 loss: 0.007673740386962891
Batch 28/64 loss: 0.010386347770690918
Batch 29/64 loss: 0.005847275257110596
Batch 30/64 loss: 0.01358121633529663
Batch 31/64 loss: 0.022167861461639404
Batch 32/64 loss: 0.009585738182067871
Batch 33/64 loss: 0.00954127311706543
Batch 34/64 loss: 0.00691908597946167
Batch 35/64 loss: 0.01148378849029541
Batch 36/64 loss: 0.0065482258796691895
Batch 37/64 loss: 0.008689701557159424
Batch 38/64 loss: 0.012833714485168457
Batch 39/64 loss: 0.022503256797790527
Batch 40/64 loss: 0.012437999248504639
Batch 41/64 loss: 0.012952029705047607
Batch 42/64 loss: 0.0070732831954956055
Batch 43/64 loss: -0.0030151009559631348
Batch 44/64 loss: 0.0036467909812927246
Batch 45/64 loss: 0.016045570373535156
Batch 46/64 loss: 0.031514883041381836
Batch 47/64 loss: 0.03569769859313965
Batch 48/64 loss: -0.0023708343505859375
Batch 49/64 loss: 0.006996452808380127
Batch 50/64 loss: -0.003804326057434082
Batch 51/64 loss: 0.00666731595993042
Batch 52/64 loss: 0.0114096999168396
Batch 53/64 loss: 0.021874666213989258
Batch 54/64 loss: 0.014309167861938477
Batch 55/64 loss: 0.012674570083618164
Batch 56/64 loss: 0.010204970836639404
Batch 57/64 loss: 0.017528653144836426
Batch 58/64 loss: 0.021383821964263916
Batch 59/64 loss: 0.005504965782165527
Batch 60/64 loss: 0.010368406772613525
Batch 61/64 loss: 0.00975579023361206
Batch 62/64 loss: 0.020300328731536865
Batch 63/64 loss: 0.0177609920501709
Batch 64/64 loss: 0.009479880332946777
Epoch 287  Train loss: 0.010832629951776243  Val loss: 0.09012693658317487
Epoch 288
-------------------------------
Batch 1/64 loss: 0.027144789695739746
Batch 2/64 loss: 0.024536430835723877
Batch 3/64 loss: 0.005571246147155762
Batch 4/64 loss: 0.018077969551086426
Batch 5/64 loss: 0.010226607322692871
Batch 6/64 loss: 0.018461108207702637
Batch 7/64 loss: 0.012427449226379395
Batch 8/64 loss: 0.013124227523803711
Batch 9/64 loss: -0.004675686359405518
Batch 10/64 loss: -0.0026419758796691895
Batch 11/64 loss: -0.006570875644683838
Batch 12/64 loss: 0.011193335056304932
Batch 13/64 loss: 0.01915144920349121
Batch 14/64 loss: 0.0010263323783874512
Batch 15/64 loss: -0.002088785171508789
Batch 16/64 loss: 0.008329510688781738
Batch 17/64 loss: 0.015957117080688477
Batch 18/64 loss: 0.0035074353218078613
Batch 19/64 loss: 0.005916118621826172
Batch 20/64 loss: 0.01633477210998535
Batch 21/64 loss: 0.004816412925720215
Batch 22/64 loss: 0.007407665252685547
Batch 23/64 loss: 0.007370293140411377
Batch 24/64 loss: -0.006022751331329346
Batch 25/64 loss: 0.0010666847229003906
Batch 26/64 loss: 0.0036320090293884277
Batch 27/64 loss: 0.022541344165802002
Batch 28/64 loss: 0.01632988452911377
Batch 29/64 loss: 0.0072293877601623535
Batch 30/64 loss: -0.004220545291900635
Batch 31/64 loss: 0.022011756896972656
Batch 32/64 loss: -0.007961452007293701
Batch 33/64 loss: 0.021462857723236084
Batch 34/64 loss: 0.004816770553588867
Batch 35/64 loss: 0.002298116683959961
Batch 36/64 loss: 0.0023932456970214844
Batch 37/64 loss: 0.0036696195602416992
Batch 38/64 loss: 0.009198665618896484
Batch 39/64 loss: 0.009700119495391846
Batch 40/64 loss: 0.012706279754638672
Batch 41/64 loss: 0.0018146634101867676
Batch 42/64 loss: 0.01340019702911377
Batch 43/64 loss: 0.01553422212600708
Batch 44/64 loss: 0.0019352436065673828
Batch 45/64 loss: 0.011577308177947998
Batch 46/64 loss: 0.005386948585510254
Batch 47/64 loss: 0.006388604640960693
Batch 48/64 loss: 0.029908418655395508
Batch 49/64 loss: 0.014783740043640137
Batch 50/64 loss: 0.008287191390991211
Batch 51/64 loss: -8.147954940795898e-05
Batch 52/64 loss: 0.009199738502502441
Batch 53/64 loss: 0.013348221778869629
Batch 54/64 loss: 0.018029093742370605
Batch 55/64 loss: 0.002965867519378662
Batch 56/64 loss: 0.02158796787261963
Batch 57/64 loss: 0.015558719635009766
Batch 58/64 loss: 0.027350306510925293
Batch 59/64 loss: 0.018081486225128174
Batch 60/64 loss: 0.01777595281600952
Batch 61/64 loss: 0.006081581115722656
Batch 62/64 loss: 0.018881022930145264
Batch 63/64 loss: 0.009980380535125732
Batch 64/64 loss: 0.013547122478485107
Epoch 288  Train loss: 0.00993554007773306  Val loss: 0.09010498896497222
Epoch 289
-------------------------------
Batch 1/64 loss: 0.008033514022827148
Batch 2/64 loss: 0.0324978232383728
Batch 3/64 loss: -0.006296396255493164
Batch 4/64 loss: 0.011078596115112305
Batch 5/64 loss: 0.03284353017807007
Batch 6/64 loss: -0.00033974647521972656
Batch 7/64 loss: 0.018566370010375977
Batch 8/64 loss: 0.004093647003173828
Batch 9/64 loss: 0.005662739276885986
Batch 10/64 loss: 0.01429736614227295
Batch 11/64 loss: 0.011292755603790283
Batch 12/64 loss: 0.0030039548873901367
Batch 13/64 loss: 0.017321288585662842
Batch 14/64 loss: 0.006567716598510742
Batch 15/64 loss: 0.005989253520965576
Batch 16/64 loss: 0.0023102760314941406
Batch 17/64 loss: 0.008598625659942627
Batch 18/64 loss: 0.03354078531265259
Batch 19/64 loss: 0.004097580909729004
Batch 20/64 loss: 0.005980134010314941
Batch 21/64 loss: 0.020542263984680176
Batch 22/64 loss: 0.010881006717681885
Batch 23/64 loss: 0.01834118366241455
Batch 24/64 loss: 0.017143309116363525
Batch 25/64 loss: 0.014415562152862549
Batch 26/64 loss: 0.02078026533126831
Batch 27/64 loss: 0.005253314971923828
Batch 28/64 loss: 0.0248449444770813
Batch 29/64 loss: -0.001356363296508789
Batch 30/64 loss: 0.0015999078750610352
Batch 31/64 loss: 0.002671658992767334
Batch 32/64 loss: 0.006462752819061279
Batch 33/64 loss: 0.0014477968215942383
Batch 34/64 loss: 0.005197346210479736
Batch 35/64 loss: 0.02559077739715576
Batch 36/64 loss: 0.014396429061889648
Batch 37/64 loss: 0.014865756034851074
Batch 38/64 loss: -0.0051871538162231445
Batch 39/64 loss: 0.011746644973754883
Batch 40/64 loss: 0.014331221580505371
Batch 41/64 loss: 0.0012482404708862305
Batch 42/64 loss: 0.0107041597366333
Batch 43/64 loss: -0.003853738307952881
Batch 44/64 loss: 0.005996108055114746
Batch 45/64 loss: 0.002726912498474121
Batch 46/64 loss: 0.01868593692779541
Batch 47/64 loss: 0.009492933750152588
Batch 48/64 loss: 0.006443977355957031
Batch 49/64 loss: 0.0030202865600585938
Batch 50/64 loss: -0.004354953765869141
Batch 51/64 loss: 0.006775319576263428
Batch 52/64 loss: 0.011081516742706299
Batch 53/64 loss: 0.0022807717323303223
Batch 54/64 loss: 0.004642903804779053
Batch 55/64 loss: -0.00247114896774292
Batch 56/64 loss: 0.0324249267578125
Batch 57/64 loss: 0.0199282169342041
Batch 58/64 loss: 0.008678734302520752
Batch 59/64 loss: 0.005186140537261963
Batch 60/64 loss: 0.005171716213226318
Batch 61/64 loss: 0.006441056728363037
Batch 62/64 loss: 0.013777494430541992
Batch 63/64 loss: 0.0036486387252807617
Batch 64/64 loss: 0.006884515285491943
Epoch 289  Train loss: 0.009661929046406465  Val loss: 0.08994019256834312
Epoch 290
-------------------------------
Batch 1/64 loss: -0.001532912254333496
Batch 2/64 loss: -0.010186553001403809
Batch 3/64 loss: -0.00910806655883789
Batch 4/64 loss: 0.012890815734863281
Batch 5/64 loss: -0.0033068060874938965
Batch 6/64 loss: -0.0010028481483459473
Batch 7/64 loss: 0.0015867352485656738
Batch 8/64 loss: -0.005964159965515137
Batch 9/64 loss: 0.003784775733947754
Batch 10/64 loss: 0.0042531490325927734
Batch 11/64 loss: 0.032305359840393066
Batch 12/64 loss: 0.001686692237854004
Batch 13/64 loss: 0.01729828119277954
Batch 14/64 loss: 0.0011365413665771484
Batch 15/64 loss: 0.0008397102355957031
Batch 16/64 loss: 0.004766225814819336
Batch 17/64 loss: 0.005701839923858643
Batch 18/64 loss: 0.0007238388061523438
Batch 19/64 loss: 0.010544776916503906
Batch 20/64 loss: 0.0044522881507873535
Batch 21/64 loss: -0.0008426904678344727
Batch 22/64 loss: -0.004446804523468018
Batch 23/64 loss: 0.008684635162353516
Batch 24/64 loss: 0.0038716793060302734
Batch 25/64 loss: 0.00615769624710083
Batch 26/64 loss: 0.005575239658355713
Batch 27/64 loss: 0.003633737564086914
Batch 28/64 loss: 0.016255855560302734
Batch 29/64 loss: -0.001726388931274414
Batch 30/64 loss: 0.020833373069763184
Batch 31/64 loss: 0.011087596416473389
Batch 32/64 loss: 0.0005404353141784668
Batch 33/64 loss: 0.005990266799926758
Batch 34/64 loss: 0.003107011318206787
Batch 35/64 loss: 0.003699779510498047
Batch 36/64 loss: -0.0018615126609802246
Batch 37/64 loss: 0.012086987495422363
Batch 38/64 loss: 0.0056681036949157715
Batch 39/64 loss: 0.022421300411224365
Batch 40/64 loss: 0.004224836826324463
Batch 41/64 loss: 0.013403356075286865
Batch 42/64 loss: 0.015621185302734375
Batch 43/64 loss: 0.006236910820007324
Batch 44/64 loss: 0.01673269271850586
Batch 45/64 loss: 0.006700336933135986
Batch 46/64 loss: 0.0013093352317810059
Batch 47/64 loss: 0.009276390075683594
Batch 48/64 loss: 0.0153733491897583
Batch 49/64 loss: 0.0077443718910217285
Batch 50/64 loss: 0.005730271339416504
Batch 51/64 loss: -0.002100229263305664
Batch 52/64 loss: 0.005380988121032715
Batch 53/64 loss: 0.01910388469696045
Batch 54/64 loss: 0.012267708778381348
Batch 55/64 loss: 0.006552696228027344
Batch 56/64 loss: 0.013073265552520752
Batch 57/64 loss: 0.013956546783447266
Batch 58/64 loss: 0.005472302436828613
Batch 59/64 loss: 0.009788930416107178
Batch 60/64 loss: 0.010808706283569336
Batch 61/64 loss: -0.0005993843078613281
Batch 62/64 loss: 0.020670771598815918
Batch 63/64 loss: 0.0023952722549438477
Batch 64/64 loss: 0.01989912986755371
Epoch 290  Train loss: 0.006676938487034218  Val loss: 0.08809562708504011
Epoch 291
-------------------------------
Batch 1/64 loss: -0.0062157511711120605
Batch 2/64 loss: 0.011066794395446777
Batch 3/64 loss: 0.00960475206375122
Batch 4/64 loss: 0.0037800073623657227
Batch 5/64 loss: 0.01739203929901123
Batch 6/64 loss: -0.004569888114929199
Batch 7/64 loss: 0.008583545684814453
Batch 8/64 loss: 0.006341516971588135
Batch 9/64 loss: -0.0014875531196594238
Batch 10/64 loss: 0.004880845546722412
Batch 11/64 loss: 0.0001551508903503418
Batch 12/64 loss: -0.0023040771484375
Batch 13/64 loss: 0.0003153681755065918
Batch 14/64 loss: 0.011621952056884766
Batch 15/64 loss: 0.0038354992866516113
Batch 16/64 loss: -0.01093226671218872
Batch 17/64 loss: 0.007172822952270508
Batch 18/64 loss: 0.007941901683807373
Batch 19/64 loss: 0.00035953521728515625
Batch 20/64 loss: 0.007068812847137451
Batch 21/64 loss: 0.003009796142578125
Batch 22/64 loss: 0.010941863059997559
Batch 23/64 loss: 0.012782514095306396
Batch 24/64 loss: 0.01436007022857666
Batch 25/64 loss: 0.012704670429229736
Batch 26/64 loss: 0.02832847833633423
Batch 27/64 loss: 0.010245203971862793
Batch 28/64 loss: 0.008866071701049805
Batch 29/64 loss: 0.01040184497833252
Batch 30/64 loss: 0.0016293525695800781
Batch 31/64 loss: 0.01080465316772461
Batch 32/64 loss: 0.022361934185028076
Batch 33/64 loss: 0.011957108974456787
Batch 34/64 loss: -0.0013660788536071777
Batch 35/64 loss: -0.005308389663696289
Batch 36/64 loss: 0.00999075174331665
Batch 37/64 loss: 0.015042245388031006
Batch 38/64 loss: 0.004175424575805664
Batch 39/64 loss: 0.026810765266418457
Batch 40/64 loss: 0.0065833330154418945
Batch 41/64 loss: 0.023925483226776123
Batch 42/64 loss: 0.0037273168563842773
Batch 43/64 loss: -0.0008748769760131836
Batch 44/64 loss: 0.016075849533081055
Batch 45/64 loss: 0.007644772529602051
Batch 46/64 loss: -0.0006860494613647461
Batch 47/64 loss: 0.020976781845092773
Batch 48/64 loss: 0.017032623291015625
Batch 49/64 loss: -0.007755756378173828
Batch 50/64 loss: 0.017894268035888672
Batch 51/64 loss: 0.023587822914123535
Batch 52/64 loss: -0.014175891876220703
Batch 53/64 loss: 0.017238914966583252
Batch 54/64 loss: 0.012395620346069336
Batch 55/64 loss: 0.018760979175567627
Batch 56/64 loss: 0.011245012283325195
Batch 57/64 loss: 0.014573872089385986
Batch 58/64 loss: 0.036910831928253174
Batch 59/64 loss: 0.009754300117492676
Batch 60/64 loss: 0.01827293634414673
Batch 61/64 loss: 0.015174031257629395
Batch 62/64 loss: 0.024480223655700684
Batch 63/64 loss: 0.0063073039054870605
Batch 64/64 loss: 0.02303534746170044
Epoch 291  Train loss: 0.009391301987217921  Val loss: 0.08898227075530901
Epoch 292
-------------------------------
Batch 1/64 loss: 0.019050955772399902
Batch 2/64 loss: 0.000330507755279541
Batch 3/64 loss: 0.027904808521270752
Batch 4/64 loss: 0.014588594436645508
Batch 5/64 loss: -0.007676661014556885
Batch 6/64 loss: 0.00194549560546875
Batch 7/64 loss: 0.017816245555877686
Batch 8/64 loss: -0.000998079776763916
Batch 9/64 loss: 0.011369109153747559
Batch 10/64 loss: -0.0028847455978393555
Batch 11/64 loss: 0.008496999740600586
Batch 12/64 loss: 0.005525052547454834
Batch 13/64 loss: 0.0067179203033447266
Batch 14/64 loss: 0.025314807891845703
Batch 15/64 loss: 0.004093587398529053
Batch 16/64 loss: -0.003769397735595703
Batch 17/64 loss: 0.012881159782409668
Batch 18/64 loss: 0.00401073694229126
Batch 19/64 loss: 0.0007059574127197266
Batch 20/64 loss: -0.00010788440704345703
Batch 21/64 loss: 0.011566340923309326
Batch 22/64 loss: 0.011748433113098145
Batch 23/64 loss: 0.003954529762268066
Batch 24/64 loss: 0.002393960952758789
Batch 25/64 loss: 0.004091143608093262
Batch 26/64 loss: 0.0065427422523498535
Batch 27/64 loss: 0.03474003076553345
Batch 28/64 loss: 0.007607579231262207
Batch 29/64 loss: -0.0015636682510375977
Batch 30/64 loss: 0.008671045303344727
Batch 31/64 loss: 0.004769742488861084
Batch 32/64 loss: 0.03731149435043335
Batch 33/64 loss: 0.00856626033782959
Batch 34/64 loss: 0.009015083312988281
Batch 35/64 loss: 0.022483110427856445
Batch 36/64 loss: 0.009752750396728516
Batch 37/64 loss: 0.016111791133880615
Batch 38/64 loss: 0.0030633211135864258
Batch 39/64 loss: 0.018826067447662354
Batch 40/64 loss: 0.016043663024902344
Batch 41/64 loss: 0.012954950332641602
Batch 42/64 loss: 0.026857495307922363
Batch 43/64 loss: 0.016065895557403564
Batch 44/64 loss: 0.00885474681854248
Batch 45/64 loss: 0.008474230766296387
Batch 46/64 loss: 0.002053558826446533
Batch 47/64 loss: 0.006782710552215576
Batch 48/64 loss: 0.008987367153167725
Batch 49/64 loss: 0.010162889957427979
Batch 50/64 loss: 0.010450541973114014
Batch 51/64 loss: 0.0067560672760009766
Batch 52/64 loss: 0.0032814741134643555
Batch 53/64 loss: 0.004577279090881348
Batch 54/64 loss: 0.00834512710571289
Batch 55/64 loss: 0.00791168212890625
Batch 56/64 loss: 0.009071111679077148
Batch 57/64 loss: 0.014894545078277588
Batch 58/64 loss: 0.004139065742492676
Batch 59/64 loss: 0.004476785659790039
Batch 60/64 loss: 0.0022350549697875977
Batch 61/64 loss: 0.026113390922546387
Batch 62/64 loss: -0.0029501914978027344
Batch 63/64 loss: -0.005245089530944824
Batch 64/64 loss: 0.019294381141662598
Epoch 292  Train loss: 0.009266385377622118  Val loss: 0.08696602689441536
Epoch 293
-------------------------------
Batch 1/64 loss: 0.00933915376663208
Batch 2/64 loss: 0.008304357528686523
Batch 3/64 loss: -0.0005921721458435059
Batch 4/64 loss: -0.0071961283683776855
Batch 5/64 loss: 0.022417545318603516
Batch 6/64 loss: 0.019120097160339355
Batch 7/64 loss: 0.002076387405395508
Batch 8/64 loss: -0.0027837157249450684
Batch 9/64 loss: 0.0010914802551269531
Batch 10/64 loss: 0.013427793979644775
Batch 11/64 loss: -0.0019989609718322754
Batch 12/64 loss: 0.009702026844024658
Batch 13/64 loss: 0.0001392960548400879
Batch 14/64 loss: 0.002115011215209961
Batch 15/64 loss: 0.011122465133666992
Batch 16/64 loss: -0.0005195736885070801
Batch 17/64 loss: 0.023712754249572754
Batch 18/64 loss: 0.00707167387008667
Batch 19/64 loss: 0.00567936897277832
Batch 20/64 loss: 0.01578819751739502
Batch 21/64 loss: 0.0029234886169433594
Batch 22/64 loss: 0.014170408248901367
Batch 23/64 loss: 0.02531832456588745
Batch 24/64 loss: 0.0049097537994384766
Batch 25/64 loss: 0.012625813484191895
Batch 26/64 loss: 0.005750060081481934
Batch 27/64 loss: -0.00198209285736084
Batch 28/64 loss: -0.0017486810684204102
Batch 29/64 loss: 0.00443875789642334
Batch 30/64 loss: 0.006325125694274902
Batch 31/64 loss: 0.012005031108856201
Batch 32/64 loss: 0.028566718101501465
Batch 33/64 loss: -0.004411280155181885
Batch 34/64 loss: 0.004315018653869629
Batch 35/64 loss: 0.016755104064941406
Batch 36/64 loss: 0.0018116235733032227
Batch 37/64 loss: 0.007295012474060059
Batch 38/64 loss: 0.005056560039520264
Batch 39/64 loss: 0.020374417304992676
Batch 40/64 loss: 0.021289288997650146
Batch 41/64 loss: 0.012920260429382324
Batch 42/64 loss: 0.008949637413024902
Batch 43/64 loss: 0.022318482398986816
Batch 44/64 loss: 0.002441883087158203
Batch 45/64 loss: 0.0020411014556884766
Batch 46/64 loss: 0.008631467819213867
Batch 47/64 loss: 0.013126790523529053
Batch 48/64 loss: 0.0017296671867370605
Batch 49/64 loss: 0.008231639862060547
Batch 50/64 loss: 0.011137843132019043
Batch 51/64 loss: 0.0005068778991699219
Batch 52/64 loss: 0.009241759777069092
Batch 53/64 loss: 0.00906306505203247
Batch 54/64 loss: -0.007336676120758057
Batch 55/64 loss: 0.006972372531890869
Batch 56/64 loss: -0.0027505159378051758
Batch 57/64 loss: 0.005474388599395752
Batch 58/64 loss: -3.2782554626464844e-05
Batch 59/64 loss: 0.0067961812019348145
Batch 60/64 loss: -0.01002657413482666
Batch 61/64 loss: 0.0045768022537231445
Batch 62/64 loss: 0.026579439640045166
Batch 63/64 loss: 0.013506531715393066
Batch 64/64 loss: 0.012163400650024414
Epoch 293  Train loss: 0.007639650270050647  Val loss: 0.09075825894411486
Epoch 294
-------------------------------
Batch 1/64 loss: -0.002764582633972168
Batch 2/64 loss: 0.0009884238243103027
Batch 3/64 loss: 0.00260007381439209
Batch 4/64 loss: 0.029271364212036133
Batch 5/64 loss: 0.008903205394744873
Batch 6/64 loss: -0.00417637825012207
Batch 7/64 loss: 0.006001770496368408
Batch 8/64 loss: 0.014188706874847412
Batch 9/64 loss: -0.006323277950286865
Batch 10/64 loss: 0.010220527648925781
Batch 11/64 loss: -0.007817506790161133
Batch 12/64 loss: 0.015397250652313232
Batch 13/64 loss: -0.00818336009979248
Batch 14/64 loss: 0.005744874477386475
Batch 15/64 loss: 0.003498077392578125
Batch 16/64 loss: 0.0012462139129638672
Batch 17/64 loss: 0.002659618854522705
Batch 18/64 loss: 0.020594239234924316
Batch 19/64 loss: 0.00018006563186645508
Batch 20/64 loss: 0.0153273344039917
Batch 21/64 loss: 0.014119625091552734
Batch 22/64 loss: 0.004433691501617432
Batch 23/64 loss: 0.0030798912048339844
Batch 24/64 loss: 0.009254932403564453
Batch 25/64 loss: 0.006491959095001221
Batch 26/64 loss: -0.0035422444343566895
Batch 27/64 loss: 0.020390629768371582
Batch 28/64 loss: -0.0012255311012268066
Batch 29/64 loss: 0.021855592727661133
Batch 30/64 loss: -0.009088397026062012
Batch 31/64 loss: -0.010878443717956543
Batch 32/64 loss: 0.00987100601196289
Batch 33/64 loss: 0.004484832286834717
Batch 34/64 loss: 0.010209977626800537
Batch 35/64 loss: -0.0012230277061462402
Batch 36/64 loss: 0.0018468499183654785
Batch 37/64 loss: 0.0037103891372680664
Batch 38/64 loss: -0.0006270408630371094
Batch 39/64 loss: 0.010239660739898682
Batch 40/64 loss: -0.0029467344284057617
Batch 41/64 loss: -0.005681812763214111
Batch 42/64 loss: -0.003958940505981445
Batch 43/64 loss: -0.0011521577835083008
Batch 44/64 loss: 0.016829848289489746
Batch 45/64 loss: 0.0001118779182434082
Batch 46/64 loss: -0.0014209151268005371
Batch 47/64 loss: 0.011988162994384766
Batch 48/64 loss: 0.01468658447265625
Batch 49/64 loss: 0.0006862878799438477
Batch 50/64 loss: -0.002873539924621582
Batch 51/64 loss: 0.020458996295928955
Batch 52/64 loss: 0.0055370330810546875
Batch 53/64 loss: 0.0015860795974731445
Batch 54/64 loss: 0.010032117366790771
Batch 55/64 loss: -0.000989079475402832
Batch 56/64 loss: 0.0003447532653808594
Batch 57/64 loss: -0.0016134381294250488
Batch 58/64 loss: 0.002682805061340332
Batch 59/64 loss: -0.001531839370727539
Batch 60/64 loss: 0.024182140827178955
Batch 61/64 loss: 0.015421688556671143
Batch 62/64 loss: 0.011568069458007812
Batch 63/64 loss: 0.030306577682495117
Batch 64/64 loss: 0.017013072967529297
Epoch 294  Train loss: 0.005615299823237401  Val loss: 0.0880697123783151
Epoch 295
-------------------------------
Batch 1/64 loss: 0.008517980575561523
Batch 2/64 loss: 0.013544559478759766
Batch 3/64 loss: 0.0007361173629760742
Batch 4/64 loss: 0.0021512508392333984
Batch 5/64 loss: 0.015194177627563477
Batch 6/64 loss: 0.0017211437225341797
Batch 7/64 loss: 0.0003783106803894043
Batch 8/64 loss: 0.001506030559539795
Batch 9/64 loss: -0.0032464265823364258
Batch 10/64 loss: 0.025042176246643066
Batch 11/64 loss: 0.005820930004119873
Batch 12/64 loss: 0.03943657875061035
Batch 13/64 loss: 0.00676339864730835
Batch 14/64 loss: 0.00620579719543457
Batch 15/64 loss: 0.0008120536804199219
Batch 16/64 loss: 0.009380221366882324
Batch 17/64 loss: 0.0020203590393066406
Batch 18/64 loss: 0.0046929121017456055
Batch 19/64 loss: 0.008328020572662354
Batch 20/64 loss: 0.004265844821929932
Batch 21/64 loss: 0.01682579517364502
Batch 22/64 loss: -0.0013251304626464844
Batch 23/64 loss: 0.00365293025970459
Batch 24/64 loss: -0.00974583625793457
Batch 25/64 loss: -0.0003151893615722656
Batch 26/64 loss: 0.007435202598571777
Batch 27/64 loss: 0.006133973598480225
Batch 28/64 loss: 0.007212936878204346
Batch 29/64 loss: 0.003675699234008789
Batch 30/64 loss: 0.003813624382019043
Batch 31/64 loss: 0.004305243492126465
Batch 32/64 loss: 0.00464320182800293
Batch 33/64 loss: 0.006191372871398926
Batch 34/64 loss: 0.016418516635894775
Batch 35/64 loss: 0.008507966995239258
Batch 36/64 loss: 0.0020714998245239258
Batch 37/64 loss: -0.0014455318450927734
Batch 38/64 loss: 0.018931448459625244
Batch 39/64 loss: 0.024564146995544434
Batch 40/64 loss: 0.00747913122177124
Batch 41/64 loss: 0.021403074264526367
Batch 42/64 loss: 0.014703333377838135
Batch 43/64 loss: -0.005218505859375
Batch 44/64 loss: 0.012884140014648438
Batch 45/64 loss: -0.006935238838195801
Batch 46/64 loss: 0.008987247943878174
Batch 47/64 loss: 0.0009837746620178223
Batch 48/64 loss: 0.010857105255126953
Batch 49/64 loss: 0.004950881004333496
Batch 50/64 loss: 0.0008650422096252441
Batch 51/64 loss: 0.0005152225494384766
Batch 52/64 loss: 0.01977914571762085
Batch 53/64 loss: 0.004447579383850098
Batch 54/64 loss: -0.0041713714599609375
Batch 55/64 loss: 0.01922452449798584
Batch 56/64 loss: -0.013715505599975586
Batch 57/64 loss: 0.003256380558013916
Batch 58/64 loss: 0.014755785465240479
Batch 59/64 loss: 0.006524860858917236
Batch 60/64 loss: 0.011509895324707031
Batch 61/64 loss: -0.012354075908660889
Batch 62/64 loss: -0.006133913993835449
Batch 63/64 loss: 0.020305991172790527
Batch 64/64 loss: 0.005594074726104736
Epoch 295  Train loss: 0.006492915574242087  Val loss: 0.08918860483005694
Epoch 296
-------------------------------
Batch 1/64 loss: -0.010381579399108887
Batch 2/64 loss: -0.009147584438323975
Batch 3/64 loss: 0.012454748153686523
Batch 4/64 loss: 0.008066952228546143
Batch 5/64 loss: -0.004786074161529541
Batch 6/64 loss: 0.012721776962280273
Batch 7/64 loss: -0.008751869201660156
Batch 8/64 loss: 0.0015935897827148438
Batch 9/64 loss: 0.014049649238586426
Batch 10/64 loss: 0.005559027194976807
Batch 11/64 loss: 0.0072113871574401855
Batch 12/64 loss: 0.014629542827606201
Batch 13/64 loss: -0.0017558932304382324
Batch 14/64 loss: 0.007224619388580322
Batch 15/64 loss: -0.01046741008758545
Batch 16/64 loss: -0.0053168535232543945
Batch 17/64 loss: 0.005634903907775879
Batch 18/64 loss: -0.004300236701965332
Batch 19/64 loss: -0.008524715900421143
Batch 20/64 loss: 0.0063307881355285645
Batch 21/64 loss: 0.010738551616668701
Batch 22/64 loss: 0.0006331801414489746
Batch 23/64 loss: 0.01190263032913208
Batch 24/64 loss: -0.007466435432434082
Batch 25/64 loss: 0.018103837966918945
Batch 26/64 loss: 0.0066522955894470215
Batch 27/64 loss: 0.00567859411239624
Batch 28/64 loss: 0.0043059587478637695
Batch 29/64 loss: 0.0013604164123535156
Batch 30/64 loss: 0.004064738750457764
Batch 31/64 loss: 0.006073951721191406
Batch 32/64 loss: 0.0038425326347351074
Batch 33/64 loss: 0.021942853927612305
Batch 34/64 loss: 0.0052220821380615234
Batch 35/64 loss: 0.018040180206298828
Batch 36/64 loss: 0.00463491678237915
Batch 37/64 loss: 0.0291978120803833
Batch 38/64 loss: 0.009785115718841553
Batch 39/64 loss: 0.0015207529067993164
Batch 40/64 loss: 0.025470197200775146
Batch 41/64 loss: -0.005603373050689697
Batch 42/64 loss: 0.004701972007751465
Batch 43/64 loss: 0.01161116361618042
Batch 44/64 loss: 0.01825505495071411
Batch 45/64 loss: 0.015348374843597412
Batch 46/64 loss: 0.007706761360168457
Batch 47/64 loss: -0.0018428564071655273
Batch 48/64 loss: 0.024022996425628662
Batch 49/64 loss: 0.0031859874725341797
Batch 50/64 loss: 0.011083364486694336
Batch 51/64 loss: -0.0028489232063293457
Batch 52/64 loss: 0.0009888410568237305
Batch 53/64 loss: 0.00024700164794921875
Batch 54/64 loss: -0.004877686500549316
Batch 55/64 loss: 0.010361254215240479
Batch 56/64 loss: -0.009421825408935547
Batch 57/64 loss: 0.0024170279502868652
Batch 58/64 loss: 0.007683277130126953
Batch 59/64 loss: 0.003237426280975342
Batch 60/64 loss: 0.001033186912536621
Batch 61/64 loss: 0.007598400115966797
Batch 62/64 loss: 0.009025275707244873
Batch 63/64 loss: -0.011680006980895996
Batch 64/64 loss: 0.00228118896484375
Epoch 296  Train loss: 0.004983412050733379  Val loss: 0.0889495254791889
Epoch 297
-------------------------------
Batch 1/64 loss: -0.0025255680084228516
Batch 2/64 loss: 0.004998743534088135
Batch 3/64 loss: -0.001476883888244629
Batch 4/64 loss: -0.003144979476928711
Batch 5/64 loss: 0.0221482515335083
Batch 6/64 loss: -0.009065210819244385
Batch 7/64 loss: 0.0025956034660339355
Batch 8/64 loss: -0.012511730194091797
Batch 9/64 loss: 0.005707144737243652
Batch 10/64 loss: 0.023195326328277588
Batch 11/64 loss: -0.004379868507385254
Batch 12/64 loss: 0.015473008155822754
Batch 13/64 loss: 0.006120145320892334
Batch 14/64 loss: 0.006955146789550781
Batch 15/64 loss: 0.004882752895355225
Batch 16/64 loss: 0.0033853650093078613
Batch 17/64 loss: -0.0007919669151306152
Batch 18/64 loss: 0.015116214752197266
Batch 19/64 loss: 0.0020298361778259277
Batch 20/64 loss: 0.006018638610839844
Batch 21/64 loss: -0.001961231231689453
Batch 22/64 loss: 0.004314124584197998
Batch 23/64 loss: 0.010422706604003906
Batch 24/64 loss: 0.015901625156402588
Batch 25/64 loss: 0.018767237663269043
Batch 26/64 loss: 0.0038039684295654297
Batch 27/64 loss: 0.01152801513671875
Batch 28/64 loss: 0.006211638450622559
Batch 29/64 loss: 0.016938447952270508
Batch 30/64 loss: -0.004740476608276367
Batch 31/64 loss: 0.0190160870552063
Batch 32/64 loss: 0.02213430404663086
Batch 33/64 loss: -0.003442704677581787
Batch 34/64 loss: 0.00034105777740478516
Batch 35/64 loss: -0.004096388816833496
Batch 36/64 loss: 0.003537118434906006
Batch 37/64 loss: 0.005195140838623047
Batch 38/64 loss: 0.0013686418533325195
Batch 39/64 loss: 0.01884061098098755
Batch 40/64 loss: -0.00048297643661499023
Batch 41/64 loss: 0.0007147789001464844
Batch 42/64 loss: 0.008288264274597168
Batch 43/64 loss: 0.005269229412078857
Batch 44/64 loss: 0.007033348083496094
Batch 45/64 loss: 0.014972209930419922
Batch 46/64 loss: 0.012852251529693604
Batch 47/64 loss: 0.018208086490631104
Batch 48/64 loss: 0.02082836627960205
Batch 49/64 loss: 0.008313775062561035
Batch 50/64 loss: 0.01899087429046631
Batch 51/64 loss: 0.004764199256896973
Batch 52/64 loss: 0.0033089518547058105
Batch 53/64 loss: -0.005269467830657959
Batch 54/64 loss: -0.0002574920654296875
Batch 55/64 loss: 0.009678483009338379
Batch 56/64 loss: 0.00462496280670166
Batch 57/64 loss: 0.027240514755249023
Batch 58/64 loss: -0.0036098361015319824
Batch 59/64 loss: 0.004954874515533447
Batch 60/64 loss: 0.021372318267822266
Batch 61/64 loss: 0.0059316158294677734
Batch 62/64 loss: 0.010463356971740723
Batch 63/64 loss: -0.001176595687866211
Batch 64/64 loss: -6.276369094848633e-05
Epoch 297  Train loss: 0.0066788535492092954  Val loss: 0.08817489532260961
Epoch 298
-------------------------------
Batch 1/64 loss: -0.00617903470993042
Batch 2/64 loss: 0.008214235305786133
Batch 3/64 loss: -0.0034012198448181152
Batch 4/64 loss: -0.0032950639724731445
Batch 5/64 loss: -0.0012860298156738281
Batch 6/64 loss: 0.024152398109436035
Batch 7/64 loss: 0.00777280330657959
Batch 8/64 loss: 0.0036849379539489746
Batch 9/64 loss: 0.0017299652099609375
Batch 10/64 loss: 0.01639688014984131
Batch 11/64 loss: 0.0055204033851623535
Batch 12/64 loss: 0.004574239253997803
Batch 13/64 loss: -0.0009882450103759766
Batch 14/64 loss: -0.0020159482955932617
Batch 15/64 loss: 0.00019276142120361328
Batch 16/64 loss: 0.015922486782073975
Batch 17/64 loss: -0.009692847728729248
Batch 18/64 loss: -0.0042896270751953125
Batch 19/64 loss: 0.001704573631286621
Batch 20/64 loss: 0.0006479024887084961
Batch 21/64 loss: -0.0048310160636901855
Batch 22/64 loss: -0.011003494262695312
Batch 23/64 loss: 0.0032652616500854492
Batch 24/64 loss: 0.019961237907409668
Batch 25/64 loss: 0.0037348270416259766
Batch 26/64 loss: 0.001977384090423584
Batch 27/64 loss: 0.010568797588348389
Batch 28/64 loss: 0.019539356231689453
Batch 29/64 loss: -0.0017195343971252441
Batch 30/64 loss: 0.015204071998596191
Batch 31/64 loss: -0.00829458236694336
Batch 32/64 loss: 0.002285182476043701
Batch 33/64 loss: 0.008075952529907227
Batch 34/64 loss: 0.004982471466064453
Batch 35/64 loss: 0.011982500553131104
Batch 36/64 loss: 0.014471232891082764
Batch 37/64 loss: 0.02197861671447754
Batch 38/64 loss: 0.01442784070968628
Batch 39/64 loss: 0.01914733648300171
Batch 40/64 loss: 0.01317363977432251
Batch 41/64 loss: 0.01694357395172119
Batch 42/64 loss: -0.002695441246032715
Batch 43/64 loss: 0.011447250843048096
Batch 44/64 loss: 0.012863397598266602
Batch 45/64 loss: -0.006435990333557129
Batch 46/64 loss: -0.0010828971862792969
Batch 47/64 loss: 0.02170342206954956
Batch 48/64 loss: -0.007104456424713135
Batch 49/64 loss: 0.0021643638610839844
Batch 50/64 loss: 0.011563122272491455
Batch 51/64 loss: 0.008509457111358643
Batch 52/64 loss: 0.008880019187927246
Batch 53/64 loss: 0.017852067947387695
Batch 54/64 loss: 0.009660720825195312
Batch 55/64 loss: 0.027211546897888184
Batch 56/64 loss: 0.0018686652183532715
Batch 57/64 loss: 0.002341151237487793
Batch 58/64 loss: 0.016887903213500977
Batch 59/64 loss: 0.0012697577476501465
Batch 60/64 loss: -0.005505800247192383
Batch 61/64 loss: 0.0030091404914855957
Batch 62/64 loss: 0.00483250617980957
Batch 63/64 loss: 0.012640655040740967
Batch 64/64 loss: -8.916854858398438e-05
Epoch 298  Train loss: 0.006071371190688189  Val loss: 0.08915014447215497
Epoch 299
-------------------------------
Batch 1/64 loss: -0.003797173500061035
Batch 2/64 loss: 0.005815386772155762
Batch 3/64 loss: -0.002099931240081787
Batch 4/64 loss: 0.014876127243041992
Batch 5/64 loss: -0.001503586769104004
Batch 6/64 loss: -0.003282010555267334
Batch 7/64 loss: 0.017383992671966553
Batch 8/64 loss: 0.007295668125152588
Batch 9/64 loss: -0.0026618242263793945
Batch 10/64 loss: -0.012048602104187012
Batch 11/64 loss: -0.004844546318054199
Batch 12/64 loss: -0.00036078691482543945
Batch 13/64 loss: 0.02385956048965454
Batch 14/64 loss: 0.00012004375457763672
Batch 15/64 loss: -0.011210858821868896
Batch 16/64 loss: -0.00042831897735595703
Batch 17/64 loss: -0.01273488998413086
Batch 18/64 loss: -0.0043631792068481445
Batch 19/64 loss: -0.0057152509689331055
Batch 20/64 loss: 0.011188626289367676
Batch 21/64 loss: -0.00558394193649292
Batch 22/64 loss: -0.009300768375396729
Batch 23/64 loss: 0.011114239692687988
Batch 24/64 loss: -0.000657200813293457
Batch 25/64 loss: 0.0071563720703125
Batch 26/64 loss: -0.0006131529808044434
Batch 27/64 loss: -0.0083656907081604
Batch 28/64 loss: 0.005765795707702637
Batch 29/64 loss: 0.007934749126434326
Batch 30/64 loss: 0.00833195447921753
Batch 31/64 loss: 0.0076865553855896
Batch 32/64 loss: 0.01616847515106201
Batch 33/64 loss: 0.008866488933563232
Batch 34/64 loss: 0.0045629143714904785
Batch 35/64 loss: 0.013591289520263672
Batch 36/64 loss: 0.01897495985031128
Batch 37/64 loss: 0.004391133785247803
Batch 38/64 loss: 0.0055106282234191895
Batch 39/64 loss: -0.0015258193016052246
Batch 40/64 loss: 0.015257835388183594
Batch 41/64 loss: 0.00724339485168457
Batch 42/64 loss: -0.0027083754539489746
Batch 43/64 loss: 0.00894862413406372
Batch 44/64 loss: 0.0036191940307617188
Batch 45/64 loss: 0.003304779529571533
Batch 46/64 loss: 0.013708710670471191
Batch 47/64 loss: 0.00622403621673584
Batch 48/64 loss: 0.015141010284423828
Batch 49/64 loss: -0.0012704730033874512
Batch 50/64 loss: 0.021261394023895264
Batch 51/64 loss: 0.003371715545654297
Batch 52/64 loss: 0.021740615367889404
Batch 53/64 loss: 0.011034786701202393
Batch 54/64 loss: -0.00046372413635253906
Batch 55/64 loss: -0.0001289844512939453
Batch 56/64 loss: 0.01201862096786499
Batch 57/64 loss: 0.03375673294067383
Batch 58/64 loss: 0.012888312339782715
Batch 59/64 loss: 0.008247733116149902
Batch 60/64 loss: -0.008927702903747559
Batch 61/64 loss: 0.0045201778411865234
Batch 62/64 loss: -0.0025322437286376953
Batch 63/64 loss: -0.0026166439056396484
Batch 64/64 loss: 0.01959061622619629
Epoch 299  Train loss: 0.004828704572191426  Val loss: 0.0912094372244635
Epoch 300
-------------------------------
Batch 1/64 loss: -0.0003827810287475586
Batch 2/64 loss: -0.005207717418670654
Batch 3/64 loss: 0.0064781904220581055
Batch 4/64 loss: 0.0064525604248046875
Batch 5/64 loss: 0.014893531799316406
Batch 6/64 loss: 0.008947372436523438
Batch 7/64 loss: -0.006757855415344238
Batch 8/64 loss: 0.008421242237091064
Batch 9/64 loss: 0.015312790870666504
Batch 10/64 loss: 0.002438783645629883
Batch 11/64 loss: 0.015248775482177734
Batch 12/64 loss: 0.0039809346199035645
Batch 13/64 loss: 0.0021457672119140625
Batch 14/64 loss: 0.011934041976928711
Batch 15/64 loss: 0.00926274061203003
Batch 16/64 loss: -0.005853176116943359
Batch 17/64 loss: 0.005447804927825928
Batch 18/64 loss: 0.007646024227142334
Batch 19/64 loss: -0.0009770393371582031
Batch 20/64 loss: 0.023082494735717773
Batch 21/64 loss: 0.0060024261474609375
Batch 22/64 loss: 0.0033695101737976074
Batch 23/64 loss: -0.004280030727386475
Batch 24/64 loss: 0.0015243291854858398
Batch 25/64 loss: 0.01492762565612793
Batch 26/64 loss: 0.004430532455444336
Batch 27/64 loss: 0.008578598499298096
Batch 28/64 loss: 0.014634907245635986
Batch 29/64 loss: 0.01886683702468872
Batch 30/64 loss: 0.004745006561279297
Batch 31/64 loss: 0.00012958049774169922
Batch 32/64 loss: 0.004487156867980957
Batch 33/64 loss: -4.3451786041259766e-05
Batch 34/64 loss: 0.004850864410400391
Batch 35/64 loss: 0.005659997463226318
Batch 36/64 loss: 0.0053160786628723145
Batch 37/64 loss: 0.006492912769317627
Batch 38/64 loss: 0.02105104923248291
Batch 39/64 loss: 0.014416813850402832
Batch 40/64 loss: -0.003593921661376953
Batch 41/64 loss: 0.008188009262084961
Batch 42/64 loss: -0.0037897825241088867
Batch 43/64 loss: 0.013498842716217041
Batch 44/64 loss: 0.0033723115921020508
Batch 45/64 loss: -0.005351841449737549
Batch 46/64 loss: 0.007935762405395508
Batch 47/64 loss: 0.0083884596824646
Batch 48/64 loss: 0.007445693016052246
Batch 49/64 loss: 0.010633468627929688
Batch 50/64 loss: 0.00830841064453125
Batch 51/64 loss: 0.01848471164703369
Batch 52/64 loss: 0.012527823448181152
Batch 53/64 loss: 0.005654692649841309
Batch 54/64 loss: 0.004220902919769287
Batch 55/64 loss: 0.003494739532470703
Batch 56/64 loss: 0.01175767183303833
Batch 57/64 loss: 0.018142402172088623
Batch 58/64 loss: -0.004508793354034424
Batch 59/64 loss: 0.0242118239402771
Batch 60/64 loss: -0.003632068634033203
Batch 61/64 loss: -0.004662454128265381
Batch 62/64 loss: -0.00993281602859497
Batch 63/64 loss: 0.03051811456680298
Batch 64/64 loss: 0.014370083808898926
Epoch 300  Train loss: 0.006741411078209971  Val loss: 0.09006875712437318
Epoch 301
-------------------------------
Batch 1/64 loss: 0.003865957260131836
Batch 2/64 loss: 0.023890316486358643
Batch 3/64 loss: 0.000706791877746582
Batch 4/64 loss: 0.013273656368255615
Batch 5/64 loss: 0.0052585601806640625
Batch 6/64 loss: -0.0032724738121032715
Batch 7/64 loss: 0.0036112070083618164
Batch 8/64 loss: 0.0076874494552612305
Batch 9/64 loss: 0.0064653754234313965
Batch 10/64 loss: 0.0007707476615905762
Batch 11/64 loss: 0.0048980712890625
Batch 12/64 loss: -0.013080894947052002
Batch 13/64 loss: -0.0010135769844055176
Batch 14/64 loss: 0.010238170623779297
Batch 15/64 loss: 0.013622283935546875
Batch 16/64 loss: 0.018848836421966553
Batch 17/64 loss: 0.013439357280731201
Batch 18/64 loss: 0.0051801204681396484
Batch 19/64 loss: -0.0022652149200439453
Batch 20/64 loss: 0.009401798248291016
Batch 21/64 loss: 0.013438761234283447
Batch 22/64 loss: 0.0025998353958129883
Batch 23/64 loss: -0.005101501941680908
Batch 24/64 loss: 0.00041240453720092773
Batch 25/64 loss: -0.006958901882171631
Batch 26/64 loss: 0.0029116272926330566
Batch 27/64 loss: 0.01939702033996582
Batch 28/64 loss: -0.012021660804748535
Batch 29/64 loss: -0.0033500194549560547
Batch 30/64 loss: 0.004467368125915527
Batch 31/64 loss: 0.008865177631378174
Batch 32/64 loss: 0.009241640567779541
Batch 33/64 loss: -0.004472672939300537
Batch 34/64 loss: 0.004427671432495117
Batch 35/64 loss: 0.01831042766571045
Batch 36/64 loss: 0.004557251930236816
Batch 37/64 loss: -0.007623195648193359
Batch 38/64 loss: -0.0037017464637756348
Batch 39/64 loss: 0.0147322416305542
Batch 40/64 loss: 0.014750897884368896
Batch 41/64 loss: -0.009298264980316162
Batch 42/64 loss: 0.009260892868041992
Batch 43/64 loss: 0.011282861232757568
Batch 44/64 loss: -0.002208709716796875
Batch 45/64 loss: -0.004136204719543457
Batch 46/64 loss: -0.0033459067344665527
Batch 47/64 loss: 0.010606884956359863
Batch 48/64 loss: 0.005844712257385254
Batch 49/64 loss: -0.005049347877502441
Batch 50/64 loss: 0.013028740882873535
Batch 51/64 loss: -0.011506438255310059
Batch 52/64 loss: 0.005022823810577393
Batch 53/64 loss: -0.004927992820739746
Batch 54/64 loss: 0.010752379894256592
Batch 55/64 loss: 0.013951301574707031
Batch 56/64 loss: -0.00513005256652832
Batch 57/64 loss: 0.020978450775146484
Batch 58/64 loss: 0.011473476886749268
Batch 59/64 loss: 0.010003805160522461
Batch 60/64 loss: 0.01774495840072632
Batch 61/64 loss: 0.019130706787109375
Batch 62/64 loss: 0.008284211158752441
Batch 63/64 loss: 0.003059208393096924
Batch 64/64 loss: 0.02961677312850952
Epoch 301  Train loss: 0.005387360675662172  Val loss: 0.0896361890117737
Epoch 302
-------------------------------
Batch 1/64 loss: -0.009595930576324463
Batch 2/64 loss: 0.0019074678421020508
Batch 3/64 loss: 0.005665004253387451
Batch 4/64 loss: 0.013331472873687744
Batch 5/64 loss: 0.008422017097473145
Batch 6/64 loss: -0.006808578968048096
Batch 7/64 loss: -0.006180286407470703
Batch 8/64 loss: -0.008139550685882568
Batch 9/64 loss: 0.0011708736419677734
Batch 10/64 loss: 0.009298980236053467
Batch 11/64 loss: 0.005187690258026123
Batch 12/64 loss: 0.02734774351119995
Batch 13/64 loss: 0.01325160264968872
Batch 14/64 loss: 0.0039823055267333984
Batch 15/64 loss: 0.005189836025238037
Batch 16/64 loss: 0.0034439563751220703
Batch 17/64 loss: 0.0062294602394104
Batch 18/64 loss: 0.009771287441253662
Batch 19/64 loss: 0.004190683364868164
Batch 20/64 loss: 0.007123887538909912
Batch 21/64 loss: 0.00981229543685913
Batch 22/64 loss: 0.0008934736251831055
Batch 23/64 loss: 0.017744779586791992
Batch 24/64 loss: 0.012823343276977539
Batch 25/64 loss: 0.019425153732299805
Batch 26/64 loss: -0.0018728375434875488
Batch 27/64 loss: 0.0001964569091796875
Batch 28/64 loss: 0.0037300586700439453
Batch 29/64 loss: -0.010014057159423828
Batch 30/64 loss: 0.00047004222869873047
Batch 31/64 loss: -0.006596565246582031
Batch 32/64 loss: 0.002814352512359619
Batch 33/64 loss: -0.0036157965660095215
Batch 34/64 loss: 0.0037530064582824707
Batch 35/64 loss: 0.010988116264343262
Batch 36/64 loss: -0.00353395938873291
Batch 37/64 loss: 0.007363438606262207
Batch 38/64 loss: 0.02382063865661621
Batch 39/64 loss: 0.003899097442626953
Batch 40/64 loss: 0.005553185939788818
Batch 41/64 loss: 0.0005949735641479492
Batch 42/64 loss: 0.006019949913024902
Batch 43/64 loss: 0.01879119873046875
Batch 44/64 loss: 0.0191422700881958
Batch 45/64 loss: 0.0018028616905212402
Batch 46/64 loss: 0.005334019660949707
Batch 47/64 loss: 4.982948303222656e-05
Batch 48/64 loss: -0.0007069706916809082
Batch 49/64 loss: -0.0012152791023254395
Batch 50/64 loss: 0.006440699100494385
Batch 51/64 loss: 0.017319858074188232
Batch 52/64 loss: 0.000965416431427002
Batch 53/64 loss: -0.006601154804229736
Batch 54/64 loss: 0.004414200782775879
Batch 55/64 loss: 0.0139809250831604
Batch 56/64 loss: 0.0018197298049926758
Batch 57/64 loss: 0.0036830902099609375
Batch 58/64 loss: 0.016141176223754883
Batch 59/64 loss: 0.0230637788772583
Batch 60/64 loss: -0.0005172491073608398
Batch 61/64 loss: 0.001614212989807129
Batch 62/64 loss: 0.014905214309692383
Batch 63/64 loss: 0.019057631492614746
Batch 64/64 loss: 0.011049985885620117
Epoch 302  Train loss: 0.005754227731742111  Val loss: 0.09344989880663422
Epoch 303
-------------------------------
Batch 1/64 loss: -0.0060386061668396
Batch 2/64 loss: -0.005845844745635986
Batch 3/64 loss: 0.005056619644165039
Batch 4/64 loss: 0.004330039024353027
Batch 5/64 loss: -0.003936648368835449
Batch 6/64 loss: 0.026607632637023926
Batch 7/64 loss: 0.019450068473815918
Batch 8/64 loss: 0.00846785306930542
Batch 9/64 loss: 0.00657200813293457
Batch 10/64 loss: -0.014533519744873047
Batch 11/64 loss: -0.01593172550201416
Batch 12/64 loss: 0.014547765254974365
Batch 13/64 loss: 0.0013357996940612793
Batch 14/64 loss: 0.0010666847229003906
Batch 15/64 loss: 0.016571879386901855
Batch 16/64 loss: -0.009852290153503418
Batch 17/64 loss: 0.0014989376068115234
Batch 18/64 loss: -0.0011203289031982422
Batch 19/64 loss: 0.024695873260498047
Batch 20/64 loss: 0.006563842296600342
Batch 21/64 loss: -0.005229353904724121
Batch 22/64 loss: 0.00951308012008667
Batch 23/64 loss: -0.001850724220275879
Batch 24/64 loss: 0.0014301538467407227
Batch 25/64 loss: -0.006395697593688965
Batch 26/64 loss: 0.0036321282386779785
Batch 27/64 loss: -0.0016214847564697266
Batch 28/64 loss: 0.0002480149269104004
Batch 29/64 loss: 0.0009998679161071777
Batch 30/64 loss: 0.010015785694122314
Batch 31/64 loss: 0.004945814609527588
Batch 32/64 loss: -0.006163537502288818
Batch 33/64 loss: 0.006905257701873779
Batch 34/64 loss: -0.003252565860748291
Batch 35/64 loss: -0.0027660727500915527
Batch 36/64 loss: 0.002466917037963867
Batch 37/64 loss: 0.00460439920425415
Batch 38/64 loss: -0.006979584693908691
Batch 39/64 loss: 0.00607985258102417
Batch 40/64 loss: -0.014996469020843506
Batch 41/64 loss: -0.006317019462585449
Batch 42/64 loss: -0.005099058151245117
Batch 43/64 loss: -0.0074727535247802734
Batch 44/64 loss: -0.012165367603302002
Batch 45/64 loss: 0.00022608041763305664
Batch 46/64 loss: -0.006529033184051514
Batch 47/64 loss: 0.014305710792541504
Batch 48/64 loss: -0.0009695887565612793
Batch 49/64 loss: -0.004693269729614258
Batch 50/64 loss: -0.004875123500823975
Batch 51/64 loss: 0.024063825607299805
Batch 52/64 loss: 0.006906390190124512
Batch 53/64 loss: 0.0039376020431518555
Batch 54/64 loss: 0.006991982460021973
Batch 55/64 loss: 0.010373890399932861
Batch 56/64 loss: 0.0038724541664123535
Batch 57/64 loss: -0.0037420988082885742
Batch 58/64 loss: 0.014661967754364014
Batch 59/64 loss: -0.0018374919891357422
Batch 60/64 loss: -0.0001970529556274414
Batch 61/64 loss: 0.004946768283843994
Batch 62/64 loss: 0.013629317283630371
Batch 63/64 loss: 0.006041049957275391
Batch 64/64 loss: 0.0006799101829528809
Epoch 303  Train loss: 0.0021593872238607967  Val loss: 0.08930270143390931
Epoch 304
-------------------------------
Batch 1/64 loss: 0.0038462281227111816
Batch 2/64 loss: -0.002605617046356201
Batch 3/64 loss: -0.0055991411209106445
Batch 4/64 loss: -0.007867097854614258
Batch 5/64 loss: 0.011948347091674805
Batch 6/64 loss: 0.003987908363342285
Batch 7/64 loss: -0.0018566250801086426
Batch 8/64 loss: 0.016196131706237793
Batch 9/64 loss: 0.001982390880584717
Batch 10/64 loss: 0.012324929237365723
Batch 11/64 loss: 0.003163576126098633
Batch 12/64 loss: -0.011022508144378662
Batch 13/64 loss: 0.006816864013671875
Batch 14/64 loss: -0.007056236267089844
Batch 15/64 loss: 0.00718235969543457
Batch 16/64 loss: -0.003896772861480713
Batch 17/64 loss: 0.01175999641418457
Batch 18/64 loss: 0.022370457649230957
Batch 19/64 loss: 0.0007423162460327148
Batch 20/64 loss: 0.02364516258239746
Batch 21/64 loss: 0.019330978393554688
Batch 22/64 loss: 0.011795878410339355
Batch 23/64 loss: 0.004764139652252197
Batch 24/64 loss: 0.013512253761291504
Batch 25/64 loss: -0.003549516201019287
Batch 26/64 loss: -0.005061924457550049
Batch 27/64 loss: -0.00760263204574585
Batch 28/64 loss: -0.0024949312210083008
Batch 29/64 loss: 0.002816140651702881
Batch 30/64 loss: 0.005699217319488525
Batch 31/64 loss: 0.0059926509857177734
Batch 32/64 loss: -0.007819116115570068
Batch 33/64 loss: -0.008212268352508545
Batch 34/64 loss: 0.004692256450653076
Batch 35/64 loss: 0.008010149002075195
Batch 36/64 loss: 0.0017311573028564453
Batch 37/64 loss: 0.0049446821212768555
Batch 38/64 loss: 0.014822840690612793
Batch 39/64 loss: -0.0018943548202514648
Batch 40/64 loss: -0.0069626569747924805
Batch 41/64 loss: -0.011455416679382324
Batch 42/64 loss: -0.011231184005737305
Batch 43/64 loss: 0.003908693790435791
Batch 44/64 loss: 0.011591553688049316
Batch 45/64 loss: 0.028718113899230957
Batch 46/64 loss: 0.002234935760498047
Batch 47/64 loss: 0.018337905406951904
Batch 48/64 loss: 0.0006651878356933594
Batch 49/64 loss: -0.009219765663146973
Batch 50/64 loss: 0.00408017635345459
Batch 51/64 loss: 0.019450485706329346
Batch 52/64 loss: -0.00496518611907959
Batch 53/64 loss: 0.005708575248718262
Batch 54/64 loss: 0.028055667877197266
Batch 55/64 loss: 0.004259824752807617
Batch 56/64 loss: 0.01285630464553833
Batch 57/64 loss: 0.0007465481758117676
Batch 58/64 loss: -0.004558563232421875
Batch 59/64 loss: -0.016792118549346924
Batch 60/64 loss: -0.003487527370452881
Batch 61/64 loss: -0.010975062847137451
Batch 62/64 loss: -0.0022867321968078613
Batch 63/64 loss: -0.0007651448249816895
Batch 64/64 loss: 0.011530876159667969
Epoch 304  Train loss: 0.0033584791071274702  Val loss: 0.0889465497531432
Epoch 305
-------------------------------
Batch 1/64 loss: 0.006055057048797607
Batch 2/64 loss: -0.005891025066375732
Batch 3/64 loss: 0.03375136852264404
Batch 4/64 loss: 0.01613539457321167
Batch 5/64 loss: -0.005173802375793457
Batch 6/64 loss: 0.00022774934768676758
Batch 7/64 loss: 0.0013456940650939941
Batch 8/64 loss: 0.0008297562599182129
Batch 9/64 loss: -0.0018712282180786133
Batch 10/64 loss: -0.014277100563049316
Batch 11/64 loss: -0.004408419132232666
Batch 12/64 loss: -0.0016595125198364258
Batch 13/64 loss: 0.02509462833404541
Batch 14/64 loss: 0.005076766014099121
Batch 15/64 loss: 0.0019447803497314453
Batch 16/64 loss: 0.02949124574661255
Batch 17/64 loss: 0.007400035858154297
Batch 18/64 loss: 0.008737325668334961
Batch 19/64 loss: 0.004243373870849609
Batch 20/64 loss: 0.00508195161819458
Batch 21/64 loss: -0.0041446685791015625
Batch 22/64 loss: 0.0001366734504699707
Batch 23/64 loss: 0.0015859603881835938
Batch 24/64 loss: 0.012924909591674805
Batch 25/64 loss: -0.001794278621673584
Batch 26/64 loss: 0.001470804214477539
Batch 27/64 loss: -0.0032025575637817383
Batch 28/64 loss: -0.012037932872772217
Batch 29/64 loss: 0.0018496513366699219
Batch 30/64 loss: -0.007403254508972168
Batch 31/64 loss: 0.0037173032760620117
Batch 32/64 loss: 0.009051382541656494
Batch 33/64 loss: 0.0020421743392944336
Batch 34/64 loss: -0.0008033514022827148
Batch 35/64 loss: 0.005104780197143555
Batch 36/64 loss: -0.00655442476272583
Batch 37/64 loss: 0.0029279589653015137
Batch 38/64 loss: -0.005539238452911377
Batch 39/64 loss: 0.0053375959396362305
Batch 40/64 loss: 0.013550221920013428
Batch 41/64 loss: 0.004015028476715088
Batch 42/64 loss: -0.000133514404296875
Batch 43/64 loss: 0.004197061061859131
Batch 44/64 loss: -0.0052384138107299805
Batch 45/64 loss: 0.003101050853729248
Batch 46/64 loss: 0.01216137409210205
Batch 47/64 loss: 0.02450418472290039
Batch 48/64 loss: -0.010922670364379883
Batch 49/64 loss: 0.007017612457275391
Batch 50/64 loss: -0.0036510229110717773
Batch 51/64 loss: 0.0008006691932678223
Batch 52/64 loss: -0.004412531852722168
Batch 53/64 loss: -0.005329132080078125
Batch 54/64 loss: -0.0016450881958007812
Batch 55/64 loss: -0.004357695579528809
Batch 56/64 loss: 0.01560819149017334
Batch 57/64 loss: -0.0057343244552612305
Batch 58/64 loss: 0.0029723644256591797
Batch 59/64 loss: -0.00801694393157959
Batch 60/64 loss: 0.002692282199859619
Batch 61/64 loss: 0.00041419267654418945
Batch 62/64 loss: 0.030757367610931396
Batch 63/64 loss: 0.002309262752532959
Batch 64/64 loss: 0.014737188816070557
Epoch 305  Train loss: 0.003176720703349394  Val loss: 0.08894880203037328
Epoch 306
-------------------------------
Batch 1/64 loss: -0.009992420673370361
Batch 2/64 loss: -0.003921806812286377
Batch 3/64 loss: 0.009842872619628906
Batch 4/64 loss: -0.018084943294525146
Batch 5/64 loss: 0.012859642505645752
Batch 6/64 loss: -0.007290184497833252
Batch 7/64 loss: 0.0006118416786193848
Batch 8/64 loss: 0.010866761207580566
Batch 9/64 loss: 0.0023043155670166016
Batch 10/64 loss: -0.0022050142288208008
Batch 11/64 loss: -0.008126914501190186
Batch 12/64 loss: -0.007530272006988525
Batch 13/64 loss: 0.005057156085968018
Batch 14/64 loss: -0.006130039691925049
Batch 15/64 loss: -0.0012419819831848145
Batch 16/64 loss: -0.004462301731109619
Batch 17/64 loss: -0.004504799842834473
Batch 18/64 loss: 0.004960477352142334
Batch 19/64 loss: 0.009268879890441895
Batch 20/64 loss: 0.009014248847961426
Batch 21/64 loss: -0.005512237548828125
Batch 22/64 loss: -0.002003788948059082
Batch 23/64 loss: 0.008709609508514404
Batch 24/64 loss: 0.00401073694229126
Batch 25/64 loss: 0.014970898628234863
Batch 26/64 loss: 0.009405016899108887
Batch 27/64 loss: -0.012633025646209717
Batch 28/64 loss: -0.012375712394714355
Batch 29/64 loss: -0.0013267993927001953
Batch 30/64 loss: -0.007467865943908691
Batch 31/64 loss: 0.005672931671142578
Batch 32/64 loss: 0.004988193511962891
Batch 33/64 loss: 0.0019966959953308105
Batch 34/64 loss: 0.00041753053665161133
Batch 35/64 loss: -1.52587890625e-05
Batch 36/64 loss: -0.0031105875968933105
Batch 37/64 loss: -0.007687926292419434
Batch 38/64 loss: -0.008519649505615234
Batch 39/64 loss: -0.0073732733726501465
Batch 40/64 loss: 0.007570385932922363
Batch 41/64 loss: 0.007708907127380371
Batch 42/64 loss: -0.003631591796875
Batch 43/64 loss: 0.00798720121383667
Batch 44/64 loss: 0.017496943473815918
Batch 45/64 loss: 0.020978808403015137
Batch 46/64 loss: 0.014534294605255127
Batch 47/64 loss: 0.009053230285644531
Batch 48/64 loss: 0.008434832096099854
Batch 49/64 loss: 0.022022604942321777
Batch 50/64 loss: -0.004692137241363525
Batch 51/64 loss: 0.02131885290145874
Batch 52/64 loss: -0.002041935920715332
Batch 53/64 loss: 0.01389533281326294
Batch 54/64 loss: -0.008063018321990967
Batch 55/64 loss: 0.012456059455871582
Batch 56/64 loss: 0.0013513565063476562
Batch 57/64 loss: -0.0002778768539428711
Batch 58/64 loss: 0.021951019763946533
Batch 59/64 loss: -0.0026264190673828125
Batch 60/64 loss: -0.013383686542510986
Batch 61/64 loss: 0.0005555152893066406
Batch 62/64 loss: -0.0068035125732421875
Batch 63/64 loss: -0.005570113658905029
Batch 64/64 loss: 0.0052073001861572266
Epoch 306  Train loss: 0.0018442593368829465  Val loss: 0.08968669217067077
Epoch 307
-------------------------------
Batch 1/64 loss: -0.0057566165924072266
Batch 2/64 loss: -0.011397063732147217
Batch 3/64 loss: -0.00665205717086792
Batch 4/64 loss: -0.006490468978881836
Batch 5/64 loss: -0.006742238998413086
Batch 6/64 loss: -0.007482171058654785
Batch 7/64 loss: -0.006633877754211426
Batch 8/64 loss: 0.006624579429626465
Batch 9/64 loss: 0.00048041343688964844
Batch 10/64 loss: 0.007722616195678711
Batch 11/64 loss: -0.0009150505065917969
Batch 12/64 loss: 0.008221864700317383
Batch 13/64 loss: -0.003124713897705078
Batch 14/64 loss: 0.022082149982452393
Batch 15/64 loss: 0.0020334720611572266
Batch 16/64 loss: -0.0018801689147949219
Batch 17/64 loss: 0.00508880615234375
Batch 18/64 loss: 0.0005225539207458496
Batch 19/64 loss: -0.00889962911605835
Batch 20/64 loss: -0.006624937057495117
Batch 21/64 loss: 0.00594860315322876
Batch 22/64 loss: 0.014814913272857666
Batch 23/64 loss: -0.0008655786514282227
Batch 24/64 loss: -0.006072044372558594
Batch 25/64 loss: -0.005208253860473633
Batch 26/64 loss: -0.005304098129272461
Batch 27/64 loss: 0.006560206413269043
Batch 28/64 loss: 0.01315981149673462
Batch 29/64 loss: -0.0009145736694335938
Batch 30/64 loss: 0.005260109901428223
Batch 31/64 loss: 0.0072013139724731445
Batch 32/64 loss: 0.008290529251098633
Batch 33/64 loss: 0.007764041423797607
Batch 34/64 loss: -0.009607017040252686
Batch 35/64 loss: -0.015265107154846191
Batch 36/64 loss: -0.017939746379852295
Batch 37/64 loss: 0.014118015766143799
Batch 38/64 loss: 0.013215780258178711
Batch 39/64 loss: -0.004277646541595459
Batch 40/64 loss: 0.009196043014526367
Batch 41/64 loss: 0.0015699267387390137
Batch 42/64 loss: -0.002558588981628418
Batch 43/64 loss: 0.0069563984870910645
Batch 44/64 loss: -0.0029123425483703613
Batch 45/64 loss: -0.0036743879318237305
Batch 46/64 loss: 0.004404544830322266
Batch 47/64 loss: 0.00710219144821167
Batch 48/64 loss: 0.007177591323852539
Batch 49/64 loss: 0.003206789493560791
Batch 50/64 loss: -0.0017830133438110352
Batch 51/64 loss: 0.003828287124633789
Batch 52/64 loss: -0.006670892238616943
Batch 53/64 loss: 0.009267151355743408
Batch 54/64 loss: -0.0011488795280456543
Batch 55/64 loss: 0.02129596471786499
Batch 56/64 loss: 0.013882040977478027
Batch 57/64 loss: -0.0034992098808288574
Batch 58/64 loss: -0.005645692348480225
Batch 59/64 loss: 0.01509183645248413
Batch 60/64 loss: -0.006370663642883301
Batch 61/64 loss: 0.00023472309112548828
Batch 62/64 loss: -0.009766578674316406
Batch 63/64 loss: 0.011567771434783936
Batch 64/64 loss: -0.0035578012466430664
Epoch 307  Train loss: 0.0012414020650527056  Val loss: 0.0900849837208122
Epoch 308
-------------------------------
Batch 1/64 loss: 0.006172597408294678
Batch 2/64 loss: -0.0029022693634033203
Batch 3/64 loss: -0.00977027416229248
Batch 4/64 loss: 0.0006938576698303223
Batch 5/64 loss: -0.00764620304107666
Batch 6/64 loss: -0.002824842929840088
Batch 7/64 loss: -0.0008819103240966797
Batch 8/64 loss: 0.010522246360778809
Batch 9/64 loss: 0.0044005513191223145
Batch 10/64 loss: -0.0014353394508361816
Batch 11/64 loss: -0.004062950611114502
Batch 12/64 loss: -0.0029277801513671875
Batch 13/64 loss: 0.014592111110687256
Batch 14/64 loss: -0.006997883319854736
Batch 15/64 loss: 0.009132862091064453
Batch 16/64 loss: 0.02271890640258789
Batch 17/64 loss: 0.005722761154174805
Batch 18/64 loss: 0.0047528743743896484
Batch 19/64 loss: -0.0075890421867370605
Batch 20/64 loss: 0.007293343544006348
Batch 21/64 loss: -0.016436457633972168
Batch 22/64 loss: 0.00556027889251709
Batch 23/64 loss: 0.036960482597351074
Batch 24/64 loss: 0.004427492618560791
Batch 25/64 loss: -0.014553844928741455
Batch 26/64 loss: 0.008828520774841309
Batch 27/64 loss: 0.023777425289154053
Batch 28/64 loss: 0.010657906532287598
Batch 29/64 loss: -0.0012356042861938477
Batch 30/64 loss: 0.011240243911743164
Batch 31/64 loss: 0.009449362754821777
Batch 32/64 loss: -0.0051863789558410645
Batch 33/64 loss: -0.0023908615112304688
Batch 34/64 loss: 0.005513966083526611
Batch 35/64 loss: 0.016104459762573242
Batch 36/64 loss: -0.006690025329589844
Batch 37/64 loss: 0.0019347071647644043
Batch 38/64 loss: -0.002899646759033203
Batch 39/64 loss: 0.007640361785888672
Batch 40/64 loss: 0.005654871463775635
Batch 41/64 loss: 0.001565694808959961
Batch 42/64 loss: 0.006663680076599121
Batch 43/64 loss: -0.0016691088676452637
Batch 44/64 loss: -0.005513131618499756
Batch 45/64 loss: 0.008592844009399414
Batch 46/64 loss: -0.006760537624359131
Batch 47/64 loss: 0.003832578659057617
Batch 48/64 loss: 0.03255492448806763
Batch 49/64 loss: -0.0030649304389953613
Batch 50/64 loss: 0.0020799636840820312
Batch 51/64 loss: 0.007381021976470947
Batch 52/64 loss: 0.011168181896209717
Batch 53/64 loss: -0.0015588998794555664
Batch 54/64 loss: 0.009707033634185791
Batch 55/64 loss: 0.0098227858543396
Batch 56/64 loss: 0.004647731781005859
Batch 57/64 loss: -0.007529556751251221
Batch 58/64 loss: 0.0006763935089111328
Batch 59/64 loss: 0.0042994022369384766
Batch 60/64 loss: -0.0021764636039733887
Batch 61/64 loss: -0.003985166549682617
Batch 62/64 loss: -0.012071371078491211
Batch 63/64 loss: 0.008730173110961914
Batch 64/64 loss: 0.01682192087173462
Epoch 308  Train loss: 0.003409106824912277  Val loss: 0.08960970509093243
Epoch 309
-------------------------------
Batch 1/64 loss: 0.003719925880432129
Batch 2/64 loss: 0.004782497882843018
Batch 3/64 loss: -0.0041304826736450195
Batch 4/64 loss: -0.007015526294708252
Batch 5/64 loss: -0.007112383842468262
Batch 6/64 loss: -0.0020385384559631348
Batch 7/64 loss: 0.00790560245513916
Batch 8/64 loss: -0.0016309022903442383
Batch 9/64 loss: -0.01029670238494873
Batch 10/64 loss: -0.01012563705444336
Batch 11/64 loss: -0.012947618961334229
Batch 12/64 loss: -0.00655972957611084
Batch 13/64 loss: 0.010311007499694824
Batch 14/64 loss: -0.00991743803024292
Batch 15/64 loss: -0.0010456442832946777
Batch 16/64 loss: 0.009398698806762695
Batch 17/64 loss: -0.01057523488998413
Batch 18/64 loss: -0.0007131099700927734
Batch 19/64 loss: 0.005608320236206055
Batch 20/64 loss: 0.008076071739196777
Batch 21/64 loss: 0.008746206760406494
Batch 22/64 loss: 0.008661389350891113
Batch 23/64 loss: 0.004619956016540527
Batch 24/64 loss: 0.009531915187835693
Batch 25/64 loss: -0.0008931159973144531
Batch 26/64 loss: -0.0005092024803161621
Batch 27/64 loss: -0.004580795764923096
Batch 28/64 loss: -0.008709847927093506
Batch 29/64 loss: -0.021095871925354004
Batch 30/64 loss: 0.009486913681030273
Batch 31/64 loss: 0.002343893051147461
Batch 32/64 loss: 0.010205388069152832
Batch 33/64 loss: 0.005265593528747559
Batch 34/64 loss: -0.003497183322906494
Batch 35/64 loss: 0.007945239543914795
Batch 36/64 loss: -0.00043398141860961914
Batch 37/64 loss: -0.004384636878967285
Batch 38/64 loss: 0.013436734676361084
Batch 39/64 loss: -0.007587432861328125
Batch 40/64 loss: 0.0043637752532958984
Batch 41/64 loss: 0.002504408359527588
Batch 42/64 loss: -0.010804712772369385
Batch 43/64 loss: -0.0002949833869934082
Batch 44/64 loss: 0.013401389122009277
Batch 45/64 loss: 0.005210816860198975
Batch 46/64 loss: -0.002074301242828369
Batch 47/64 loss: -0.006578922271728516
Batch 48/64 loss: 0.02084904909133911
Batch 49/64 loss: 0.01941835880279541
Batch 50/64 loss: -0.0013205409049987793
Batch 51/64 loss: 0.0013858675956726074
Batch 52/64 loss: -0.0031098127365112305
Batch 53/64 loss: -0.002003610134124756
Batch 54/64 loss: 0.0051514506340026855
Batch 55/64 loss: -0.002293527126312256
Batch 56/64 loss: 0.004295408725738525
Batch 57/64 loss: -0.004658937454223633
Batch 58/64 loss: 0.0012350082397460938
Batch 59/64 loss: 0.0017151236534118652
Batch 60/64 loss: -0.0037106871604919434
Batch 61/64 loss: 0.01595240831375122
Batch 62/64 loss: 0.0005524158477783203
Batch 63/64 loss: 0.01585286855697632
Batch 64/64 loss: 0.009641945362091064
Epoch 309  Train loss: 0.0012002213328492408  Val loss: 0.09036705952739388
Epoch 310
-------------------------------
Batch 1/64 loss: 0.01895827054977417
Batch 2/64 loss: -0.002490401268005371
Batch 3/64 loss: 0.004207015037536621
Batch 4/64 loss: 0.008518576622009277
Batch 5/64 loss: 0.013127565383911133
Batch 6/64 loss: 0.002396523952484131
Batch 7/64 loss: -0.004732370376586914
Batch 8/64 loss: 0.0032415390014648438
Batch 9/64 loss: -0.008541643619537354
Batch 10/64 loss: 0.008880376815795898
Batch 11/64 loss: 0.0019515156745910645
Batch 12/64 loss: 0.0074408650398254395
Batch 13/64 loss: -0.0014401674270629883
Batch 14/64 loss: 0.00016182661056518555
Batch 15/64 loss: -0.004123508930206299
Batch 16/64 loss: 0.008349001407623291
Batch 17/64 loss: -0.01209878921508789
Batch 18/64 loss: -0.007023632526397705
Batch 19/64 loss: 0.010968565940856934
Batch 20/64 loss: -0.004065096378326416
Batch 21/64 loss: -0.006126463413238525
Batch 22/64 loss: 0.0006170868873596191
Batch 23/64 loss: -0.002483844757080078
Batch 24/64 loss: 0.014373958110809326
Batch 25/64 loss: -0.003678739070892334
Batch 26/64 loss: 0.006248116493225098
Batch 27/64 loss: 0.0014342665672302246
Batch 28/64 loss: -0.002518773078918457
Batch 29/64 loss: 0.01714468002319336
Batch 30/64 loss: 0.01151585578918457
Batch 31/64 loss: 0.003970146179199219
Batch 32/64 loss: 0.009332537651062012
Batch 33/64 loss: -0.0003514885902404785
Batch 34/64 loss: -0.004586160182952881
Batch 35/64 loss: 0.013440847396850586
Batch 36/64 loss: 0.007886886596679688
Batch 37/64 loss: -0.015582799911499023
Batch 38/64 loss: -0.003960967063903809
Batch 39/64 loss: 0.0010094642639160156
Batch 40/64 loss: 0.0074002742767333984
Batch 41/64 loss: -0.005938827991485596
Batch 42/64 loss: 0.0036107301712036133
Batch 43/64 loss: -0.008034288883209229
Batch 44/64 loss: 0.0011401772499084473
Batch 45/64 loss: -0.0009744167327880859
Batch 46/64 loss: -6.74128532409668e-05
Batch 47/64 loss: 0.005423903465270996
Batch 48/64 loss: 0.002554655075073242
Batch 49/64 loss: 0.00962209701538086
Batch 50/64 loss: 0.010424375534057617
Batch 51/64 loss: 0.003992795944213867
Batch 52/64 loss: -0.007752060890197754
Batch 53/64 loss: 0.0021178722381591797
Batch 54/64 loss: -0.004383981227874756
Batch 55/64 loss: 0.011407136917114258
Batch 56/64 loss: 0.01575911045074463
Batch 57/64 loss: -0.0023493170738220215
Batch 58/64 loss: -0.005302071571350098
Batch 59/64 loss: -0.010012984275817871
Batch 60/64 loss: -0.002918720245361328
Batch 61/64 loss: 0.013318538665771484
Batch 62/64 loss: -0.0038357973098754883
Batch 63/64 loss: 0.0013567209243774414
Batch 64/64 loss: 0.009025096893310547
Epoch 310  Train loss: 0.0021129093918145873  Val loss: 0.0897123737843176
Epoch 311
-------------------------------
Batch 1/64 loss: -0.007104277610778809
Batch 2/64 loss: -0.006072998046875
Batch 3/64 loss: -0.0012235045433044434
Batch 4/64 loss: -0.007556319236755371
Batch 5/64 loss: -0.014593541622161865
Batch 6/64 loss: -0.00029838085174560547
Batch 7/64 loss: -0.013096213340759277
Batch 8/64 loss: -0.0006968379020690918
Batch 9/64 loss: -0.01248323917388916
Batch 10/64 loss: -0.0003789663314819336
Batch 11/64 loss: 0.007157564163208008
Batch 12/64 loss: 0.010723888874053955
Batch 13/64 loss: -0.0030267834663391113
Batch 14/64 loss: 0.004709362983703613
Batch 15/64 loss: 0.009432315826416016
Batch 16/64 loss: -0.0008777379989624023
Batch 17/64 loss: 0.0033330321311950684
Batch 18/64 loss: 0.009566903114318848
Batch 19/64 loss: 0.015377402305603027
Batch 20/64 loss: 0.024606823921203613
Batch 21/64 loss: 0.009067416191101074
Batch 22/64 loss: 0.009816408157348633
Batch 23/64 loss: 0.0019518136978149414
Batch 24/64 loss: 0.00012946128845214844
Batch 25/64 loss: -0.000452578067779541
Batch 26/64 loss: 0.011099398136138916
Batch 27/64 loss: -0.010736346244812012
Batch 28/64 loss: -0.011067748069763184
Batch 29/64 loss: -0.0075983405113220215
Batch 30/64 loss: 0.004803657531738281
Batch 31/64 loss: -0.002560853958129883
Batch 32/64 loss: -0.01001131534576416
Batch 33/64 loss: -0.007925748825073242
Batch 34/64 loss: 0.013923943042755127
Batch 35/64 loss: -0.0002422332763671875
Batch 36/64 loss: 0.007597506046295166
Batch 37/64 loss: 0.004215598106384277
Batch 38/64 loss: 0.013405203819274902
Batch 39/64 loss: 0.003531932830810547
Batch 40/64 loss: -0.009662330150604248
Batch 41/64 loss: -0.0006172657012939453
Batch 42/64 loss: -0.00858694314956665
Batch 43/64 loss: -0.002823173999786377
Batch 44/64 loss: 0.0012880563735961914
Batch 45/64 loss: 0.007504403591156006
Batch 46/64 loss: -0.002985835075378418
Batch 47/64 loss: 0.016601502895355225
Batch 48/64 loss: 0.00027239322662353516
Batch 49/64 loss: 0.008349180221557617
Batch 50/64 loss: 0.0067331790924072266
Batch 51/64 loss: 0.013150811195373535
Batch 52/64 loss: 0.013254940509796143
Batch 53/64 loss: -0.011958599090576172
Batch 54/64 loss: -0.006104946136474609
Batch 55/64 loss: 0.00430673360824585
Batch 56/64 loss: 0.002573847770690918
Batch 57/64 loss: -0.005656599998474121
Batch 58/64 loss: 0.0010619759559631348
Batch 59/64 loss: 0.008723318576812744
Batch 60/64 loss: 0.016445159912109375
Batch 61/64 loss: -0.016575515270233154
Batch 62/64 loss: 0.02748692035675049
Batch 63/64 loss: -0.0040395259857177734
Batch 64/64 loss: 0.009787440299987793
Epoch 311  Train loss: 0.001765144105051078  Val loss: 0.08850041431250032
Epoch 312
-------------------------------
Batch 1/64 loss: -0.00508195161819458
Batch 2/64 loss: 0.001903533935546875
Batch 3/64 loss: 0.01877540349960327
Batch 4/64 loss: 0.002112090587615967
Batch 5/64 loss: -0.009681224822998047
Batch 6/64 loss: -0.00324934720993042
Batch 7/64 loss: 0.035408616065979004
Batch 8/64 loss: -0.003854095935821533
Batch 9/64 loss: -0.009793519973754883
Batch 10/64 loss: -0.0003094673156738281
Batch 11/64 loss: 0.021369576454162598
Batch 12/64 loss: -0.0005443692207336426
Batch 13/64 loss: -0.005761921405792236
Batch 14/64 loss: -0.003966867923736572
Batch 15/64 loss: -0.0015397071838378906
Batch 16/64 loss: 0.009499430656433105
Batch 17/64 loss: -0.0027022361755371094
Batch 18/64 loss: -0.006982386112213135
Batch 19/64 loss: -0.0058945417404174805
Batch 20/64 loss: 0.005615055561065674
Batch 21/64 loss: -0.00634843111038208
Batch 22/64 loss: -0.004038035869598389
Batch 23/64 loss: -0.0031969547271728516
Batch 24/64 loss: -0.005026519298553467
Batch 25/64 loss: -0.009856700897216797
Batch 26/64 loss: -0.00646132230758667
Batch 27/64 loss: 0.004538238048553467
Batch 28/64 loss: 0.02248924970626831
Batch 29/64 loss: -0.001911163330078125
Batch 30/64 loss: -0.0022060275077819824
Batch 31/64 loss: 0.004625976085662842
Batch 32/64 loss: -0.0004245638847351074
Batch 33/64 loss: -0.00015419721603393555
Batch 34/64 loss: -0.010715484619140625
Batch 35/64 loss: 0.009356200695037842
Batch 36/64 loss: 0.004602253437042236
Batch 37/64 loss: -0.004983305931091309
Batch 38/64 loss: -0.004147171974182129
Batch 39/64 loss: 0.005167901515960693
Batch 40/64 loss: 0.004214167594909668
Batch 41/64 loss: 0.015046954154968262
Batch 42/64 loss: 0.011220216751098633
Batch 43/64 loss: 0.012010693550109863
Batch 44/64 loss: 0.012016236782073975
Batch 45/64 loss: 0.008701205253601074
Batch 46/64 loss: 0.006592869758605957
Batch 47/64 loss: 0.004266500473022461
Batch 48/64 loss: 0.006931722164154053
Batch 49/64 loss: -0.0034835338592529297
Batch 50/64 loss: -0.0041925907135009766
Batch 51/64 loss: 0.02593594789505005
Batch 52/64 loss: -0.0005953311920166016
Batch 53/64 loss: 0.006322205066680908
Batch 54/64 loss: 0.007505059242248535
Batch 55/64 loss: 0.00045800209045410156
Batch 56/64 loss: -0.009150028228759766
Batch 57/64 loss: 0.008203446865081787
Batch 58/64 loss: -0.012719273567199707
Batch 59/64 loss: -0.010841965675354004
Batch 60/64 loss: -0.012187838554382324
Batch 61/64 loss: -0.0067348480224609375
Batch 62/64 loss: -0.0160103440284729
Batch 63/64 loss: -0.00039517879486083984
Batch 64/64 loss: -0.009434103965759277
Epoch 312  Train loss: 0.0011399329877367207  Val loss: 0.08871649712631383
Epoch 313
-------------------------------
Batch 1/64 loss: 0.0036936402320861816
Batch 2/64 loss: -0.02178168296813965
Batch 3/64 loss: 0.013016223907470703
Batch 4/64 loss: -0.011861443519592285
Batch 5/64 loss: 0.0005626082420349121
Batch 6/64 loss: -0.011298835277557373
Batch 7/64 loss: 0.00208359956741333
Batch 8/64 loss: 0.006623268127441406
Batch 9/64 loss: -0.005344033241271973
Batch 10/64 loss: -0.015958189964294434
Batch 11/64 loss: 0.005281507968902588
Batch 12/64 loss: 0.012809991836547852
Batch 13/64 loss: -0.00793236494064331
Batch 14/64 loss: 0.01215064525604248
Batch 15/64 loss: -0.01801210641860962
Batch 16/64 loss: 0.05076122283935547
Batch 17/64 loss: -0.014748215675354004
Batch 18/64 loss: -0.010105729103088379
Batch 19/64 loss: -0.005461573600769043
Batch 20/64 loss: -0.004443168640136719
Batch 21/64 loss: -0.009243965148925781
Batch 22/64 loss: -0.006474196910858154
Batch 23/64 loss: 0.005295395851135254
Batch 24/64 loss: -0.00403439998626709
Batch 25/64 loss: -0.0021467208862304688
Batch 26/64 loss: 0.0036140084266662598
Batch 27/64 loss: -0.0001252889633178711
Batch 28/64 loss: -0.010903239250183105
Batch 29/64 loss: -0.006582796573638916
Batch 30/64 loss: -0.004020392894744873
Batch 31/64 loss: 0.0014952421188354492
Batch 32/64 loss: -0.004376769065856934
Batch 33/64 loss: -0.001754164695739746
Batch 34/64 loss: -0.012004375457763672
Batch 35/64 loss: 0.009222984313964844
Batch 36/64 loss: -0.0025320053100585938
Batch 37/64 loss: 0.0017290115356445312
Batch 38/64 loss: 0.010529518127441406
Batch 39/64 loss: 0.003091156482696533
Batch 40/64 loss: 0.002155601978302002
Batch 41/64 loss: 0.0064501166343688965
Batch 42/64 loss: 0.008327007293701172
Batch 43/64 loss: 0.014312446117401123
Batch 44/64 loss: 0.004954993724822998
Batch 45/64 loss: 0.00883030891418457
Batch 46/64 loss: 0.00014418363571166992
Batch 47/64 loss: -0.009883642196655273
Batch 48/64 loss: 0.005009353160858154
Batch 49/64 loss: -0.006320476531982422
Batch 50/64 loss: -0.0042577385902404785
Batch 51/64 loss: -0.0018687248229980469
Batch 52/64 loss: -0.004246771335601807
Batch 53/64 loss: 0.0010756254196166992
Batch 54/64 loss: 0.00567626953125
Batch 55/64 loss: 0.01252591609954834
Batch 56/64 loss: -0.005270063877105713
Batch 57/64 loss: -0.011596202850341797
Batch 58/64 loss: -0.0038186311721801758
Batch 59/64 loss: -0.0030902624130249023
Batch 60/64 loss: 0.005815744400024414
Batch 61/64 loss: 0.0034095048904418945
Batch 62/64 loss: 0.013908684253692627
Batch 63/64 loss: -0.012052536010742188
Batch 64/64 loss: -0.010816991329193115
Epoch 313  Train loss: -0.0004252183671091117  Val loss: 0.08967152944545156
Epoch 314
-------------------------------
Batch 1/64 loss: -0.02250969409942627
Batch 2/64 loss: -0.0007699131965637207
Batch 3/64 loss: 0.00205838680267334
Batch 4/64 loss: 0.00966864824295044
Batch 5/64 loss: 0.012151479721069336
Batch 6/64 loss: -0.01295173168182373
Batch 7/64 loss: 0.005116522312164307
Batch 8/64 loss: 0.0006644725799560547
Batch 9/64 loss: 0.007278800010681152
Batch 10/64 loss: -0.001962602138519287
Batch 11/64 loss: -0.011033773422241211
Batch 12/64 loss: -0.0022470951080322266
Batch 13/64 loss: 0.016388118267059326
Batch 14/64 loss: 0.0006326436996459961
Batch 15/64 loss: 0.004797041416168213
Batch 16/64 loss: 0.009226858615875244
Batch 17/64 loss: 0.0029975175857543945
Batch 18/64 loss: -0.0034651756286621094
Batch 19/64 loss: -0.012369751930236816
Batch 20/64 loss: -0.005423128604888916
Batch 21/64 loss: 0.0029407739639282227
Batch 22/64 loss: -0.009178996086120605
Batch 23/64 loss: -0.02040719985961914
Batch 24/64 loss: -0.0158841609954834
Batch 25/64 loss: -0.004411160945892334
Batch 26/64 loss: -0.007508814334869385
Batch 27/64 loss: 0.030441224575042725
Batch 28/64 loss: -0.004065513610839844
Batch 29/64 loss: 0.004382133483886719
Batch 30/64 loss: -0.006785869598388672
Batch 31/64 loss: -0.002078711986541748
Batch 32/64 loss: 0.005324065685272217
Batch 33/64 loss: 0.0072174072265625
Batch 34/64 loss: -0.009378910064697266
Batch 35/64 loss: -0.007808327674865723
Batch 36/64 loss: 0.010897040367126465
Batch 37/64 loss: -0.014672160148620605
Batch 38/64 loss: 0.0017788410186767578
Batch 39/64 loss: 0.006471872329711914
Batch 40/64 loss: 0.00047469139099121094
Batch 41/64 loss: -0.013609766960144043
Batch 42/64 loss: 0.008054614067077637
Batch 43/64 loss: 0.016461074352264404
Batch 44/64 loss: 0.006190299987792969
Batch 45/64 loss: 0.0026906728744506836
Batch 46/64 loss: -0.009160876274108887
Batch 47/64 loss: -0.0018069744110107422
Batch 48/64 loss: 0.0037713050842285156
Batch 49/64 loss: 0.0012952089309692383
Batch 50/64 loss: -0.01313471794128418
Batch 51/64 loss: -0.0034720301628112793
Batch 52/64 loss: 0.003226757049560547
Batch 53/64 loss: -0.004155874252319336
Batch 54/64 loss: -0.0030643343925476074
Batch 55/64 loss: -0.004436016082763672
Batch 56/64 loss: 0.008567750453948975
Batch 57/64 loss: -2.9027462005615234e-05
Batch 58/64 loss: 0.006364405155181885
Batch 59/64 loss: -0.00579148530960083
Batch 60/64 loss: -0.009944438934326172
Batch 61/64 loss: 0.009358227252960205
Batch 62/64 loss: 0.010850131511688232
Batch 63/64 loss: -0.009329617023468018
Batch 64/64 loss: 0.004821419715881348
Epoch 314  Train loss: -0.0004940046983606675  Val loss: 0.08814932144794267
Epoch 315
-------------------------------
Batch 1/64 loss: 0.015635132789611816
Batch 2/64 loss: 0.0058547258377075195
Batch 3/64 loss: -0.0032458901405334473
Batch 4/64 loss: -0.008550405502319336
Batch 5/64 loss: 0.02402198314666748
Batch 6/64 loss: -0.009304702281951904
Batch 7/64 loss: -0.014106392860412598
Batch 8/64 loss: -0.012282311916351318
Batch 9/64 loss: 0.000521242618560791
Batch 10/64 loss: -0.01334768533706665
Batch 11/64 loss: -0.0160483717918396
Batch 12/64 loss: 0.0001575946807861328
Batch 13/64 loss: -0.0066301822662353516
Batch 14/64 loss: 0.0015494823455810547
Batch 15/64 loss: -0.007631242275238037
Batch 16/64 loss: -0.005984663963317871
Batch 17/64 loss: -0.020638108253479004
Batch 18/64 loss: -0.012899458408355713
Batch 19/64 loss: -0.004233062267303467
Batch 20/64 loss: 0.007651209831237793
Batch 21/64 loss: -0.004845917224884033
Batch 22/64 loss: -0.008416891098022461
Batch 23/64 loss: -0.009325504302978516
Batch 24/64 loss: 0.006932735443115234
Batch 25/64 loss: -0.006411552429199219
Batch 26/64 loss: -0.00862652063369751
Batch 27/64 loss: -0.016455471515655518
Batch 28/64 loss: 0.008837878704071045
Batch 29/64 loss: 0.004905998706817627
Batch 30/64 loss: -0.005902886390686035
Batch 31/64 loss: -0.002733945846557617
Batch 32/64 loss: -0.008116185665130615
Batch 33/64 loss: 0.008180439472198486
Batch 34/64 loss: -0.002024531364440918
Batch 35/64 loss: -0.004174172878265381
Batch 36/64 loss: 0.006821990013122559
Batch 37/64 loss: -0.0024352669715881348
Batch 38/64 loss: 0.009372830390930176
Batch 39/64 loss: 0.0005571842193603516
Batch 40/64 loss: -0.0056386590003967285
Batch 41/64 loss: -0.0048635005950927734
Batch 42/64 loss: -0.004905402660369873
Batch 43/64 loss: 0.034413278102874756
Batch 44/64 loss: 0.0037665367126464844
Batch 45/64 loss: 0.01704120635986328
Batch 46/64 loss: 0.019179701805114746
Batch 47/64 loss: 0.007810533046722412
Batch 48/64 loss: 0.008590638637542725
Batch 49/64 loss: -0.008813977241516113
Batch 50/64 loss: 0.0009233951568603516
Batch 51/64 loss: -0.0034999847412109375
Batch 52/64 loss: -0.002099454402923584
Batch 53/64 loss: -0.014870166778564453
Batch 54/64 loss: 0.012913405895233154
Batch 55/64 loss: -0.002549886703491211
Batch 56/64 loss: -0.0068915486335754395
Batch 57/64 loss: -0.0016913414001464844
Batch 58/64 loss: -0.0038267970085144043
Batch 59/64 loss: -0.009504318237304688
Batch 60/64 loss: 0.02327948808670044
Batch 61/64 loss: 0.006260514259338379
Batch 62/64 loss: -0.0080488920211792
Batch 63/64 loss: -0.001590132713317871
Batch 64/64 loss: 0.0027260184288024902
Epoch 315  Train loss: -0.0008775175786485859  Val loss: 0.0895588438945128
Epoch 316
-------------------------------
Batch 1/64 loss: -0.00510406494140625
Batch 2/64 loss: -0.015899658203125
Batch 3/64 loss: -0.005541741847991943
Batch 4/64 loss: -0.002137124538421631
Batch 5/64 loss: -0.009482324123382568
Batch 6/64 loss: -0.01655113697052002
Batch 7/64 loss: -0.003293752670288086
Batch 8/64 loss: 0.005236387252807617
Batch 9/64 loss: -0.002258002758026123
Batch 10/64 loss: -0.005553066730499268
Batch 11/64 loss: -0.009405732154846191
Batch 12/64 loss: -0.009891152381896973
Batch 13/64 loss: -0.003218531608581543
Batch 14/64 loss: -0.009861886501312256
Batch 15/64 loss: -0.011376023292541504
Batch 16/64 loss: -0.008790969848632812
Batch 17/64 loss: 0.0037430524826049805
Batch 18/64 loss: -0.007416784763336182
Batch 19/64 loss: -0.012217819690704346
Batch 20/64 loss: -0.004813730716705322
Batch 21/64 loss: 0.0005224347114562988
Batch 22/64 loss: 0.024105846881866455
Batch 23/64 loss: -0.0020903944969177246
Batch 24/64 loss: 0.016800999641418457
Batch 25/64 loss: -0.001911759376525879
Batch 26/64 loss: -0.00031512975692749023
Batch 27/64 loss: 0.0010914206504821777
Batch 28/64 loss: 0.0042473673820495605
Batch 29/64 loss: 0.008717060089111328
Batch 30/64 loss: -0.01102745532989502
Batch 31/64 loss: -0.009676992893218994
Batch 32/64 loss: -0.007345736026763916
Batch 33/64 loss: 0.024317264556884766
Batch 34/64 loss: 0.012607932090759277
Batch 35/64 loss: -0.010931015014648438
Batch 36/64 loss: -0.005435943603515625
Batch 37/64 loss: 0.0031751394271850586
Batch 38/64 loss: 0.0023604631423950195
Batch 39/64 loss: -0.007072925567626953
Batch 40/64 loss: -0.008837878704071045
Batch 41/64 loss: -0.0036101341247558594
Batch 42/64 loss: -0.016146540641784668
Batch 43/64 loss: -0.0014448761940002441
Batch 44/64 loss: -0.0026017427444458008
Batch 45/64 loss: -0.006188869476318359
Batch 46/64 loss: -0.0021929144859313965
Batch 47/64 loss: -0.008780956268310547
Batch 48/64 loss: -0.005591630935668945
Batch 49/64 loss: -0.0018480420112609863
Batch 50/64 loss: -0.01755279302597046
Batch 51/64 loss: 0.008451998233795166
Batch 52/64 loss: -0.01229029893875122
Batch 53/64 loss: 0.0005847811698913574
Batch 54/64 loss: -0.0036655664443969727
Batch 55/64 loss: -0.01102060079574585
Batch 56/64 loss: 0.006058037281036377
Batch 57/64 loss: -0.0015816688537597656
Batch 58/64 loss: 0.0031165480613708496
Batch 59/64 loss: 0.011962413787841797
Batch 60/64 loss: 0.007926702499389648
Batch 61/64 loss: 0.010978102684020996
Batch 62/64 loss: 0.0029082894325256348
Batch 63/64 loss: 0.005238711833953857
Batch 64/64 loss: -0.00608062744140625
Epoch 316  Train loss: -0.0022334884194766774  Val loss: 0.09042386254903786
Epoch 317
-------------------------------
Batch 1/64 loss: 0.0006632208824157715
Batch 2/64 loss: -0.020239055156707764
Batch 3/64 loss: -0.016713619232177734
Batch 4/64 loss: -0.01777350902557373
Batch 5/64 loss: -0.013857781887054443
Batch 6/64 loss: -0.008712947368621826
Batch 7/64 loss: 0.008152425289154053
Batch 8/64 loss: 0.0353124737739563
Batch 9/64 loss: -0.015622615814208984
Batch 10/64 loss: -0.014579534530639648
Batch 11/64 loss: -0.0016317963600158691
Batch 12/64 loss: -0.0011559724807739258
Batch 13/64 loss: -0.0071620941162109375
Batch 14/64 loss: -0.004431545734405518
Batch 15/64 loss: 0.0004353523254394531
Batch 16/64 loss: 0.007888257503509521
Batch 17/64 loss: 0.0009214878082275391
Batch 18/64 loss: -0.002382993698120117
Batch 19/64 loss: 0.003219127655029297
Batch 20/64 loss: -0.005745530128479004
Batch 21/64 loss: -0.01844078302383423
Batch 22/64 loss: -0.009343624114990234
Batch 23/64 loss: 0.003339529037475586
Batch 24/64 loss: -0.0016759634017944336
Batch 25/64 loss: 0.004531264305114746
Batch 26/64 loss: -0.01278012990951538
Batch 27/64 loss: -0.0013749003410339355
Batch 28/64 loss: -0.0016607046127319336
Batch 29/64 loss: -0.006797134876251221
Batch 30/64 loss: -0.0064206719398498535
Batch 31/64 loss: 0.027694761753082275
Batch 32/64 loss: -0.0009688735008239746
Batch 33/64 loss: 0.0023216605186462402
Batch 34/64 loss: -0.0076819658279418945
Batch 35/64 loss: -0.0068367719650268555
Batch 36/64 loss: 0.009842216968536377
Batch 37/64 loss: 0.010699212551116943
Batch 38/64 loss: -0.00989675521850586
Batch 39/64 loss: -0.01013040542602539
Batch 40/64 loss: 0.005251169204711914
Batch 41/64 loss: -0.004042387008666992
Batch 42/64 loss: -0.012452960014343262
Batch 43/64 loss: -0.00885772705078125
Batch 44/64 loss: 0.015230953693389893
Batch 45/64 loss: -0.0034613609313964844
Batch 46/64 loss: -0.006021678447723389
Batch 47/64 loss: 0.0020105838775634766
Batch 48/64 loss: -0.011414051055908203
Batch 49/64 loss: -0.010873734951019287
Batch 50/64 loss: -0.008460760116577148
Batch 51/64 loss: 0.01042109727859497
Batch 52/64 loss: -0.011878252029418945
Batch 53/64 loss: -0.0034990310668945312
Batch 54/64 loss: -0.0018928050994873047
Batch 55/64 loss: -0.0014797449111938477
Batch 56/64 loss: 0.004679560661315918
Batch 57/64 loss: -0.005732238292694092
Batch 58/64 loss: -0.0001983642578125
Batch 59/64 loss: -0.004853010177612305
Batch 60/64 loss: -0.008330881595611572
Batch 61/64 loss: -0.019412517547607422
Batch 62/64 loss: 0.00014132261276245117
Batch 63/64 loss: 0.014334380626678467
Batch 64/64 loss: 0.0052558183670043945
Epoch 317  Train loss: -0.0027583884257896276  Val loss: 0.09205684096542831
Epoch 318
-------------------------------
Batch 1/64 loss: -0.014891266822814941
Batch 2/64 loss: 0.015015661716461182
Batch 3/64 loss: 0.00494009256362915
Batch 4/64 loss: -0.015718460083007812
Batch 5/64 loss: -0.015252530574798584
Batch 6/64 loss: -0.004869818687438965
Batch 7/64 loss: -0.001037001609802246
Batch 8/64 loss: 0.004051566123962402
Batch 9/64 loss: 0.009833216667175293
Batch 10/64 loss: -0.01759326457977295
Batch 11/64 loss: -0.022731363773345947
Batch 12/64 loss: 0.005204141139984131
Batch 13/64 loss: -5.829334259033203e-05
Batch 14/64 loss: -0.00385439395904541
Batch 15/64 loss: -0.0079917311668396
Batch 16/64 loss: -0.00667726993560791
Batch 17/64 loss: 0.004958510398864746
Batch 18/64 loss: -0.012613534927368164
Batch 19/64 loss: -0.00428849458694458
Batch 20/64 loss: 0.007211089134216309
Batch 21/64 loss: -0.01389157772064209
Batch 22/64 loss: -0.007153749465942383
Batch 23/64 loss: -0.017187774181365967
Batch 24/64 loss: -0.009717285633087158
Batch 25/64 loss: -0.006134629249572754
Batch 26/64 loss: -0.0036377310752868652
Batch 27/64 loss: -0.015283465385437012
Batch 28/64 loss: 0.0045972466468811035
Batch 29/64 loss: 0.012382268905639648
Batch 30/64 loss: 0.008010387420654297
Batch 31/64 loss: 0.0144728422164917
Batch 32/64 loss: -0.0025739669799804688
Batch 33/64 loss: -0.008673608303070068
Batch 34/64 loss: -0.004649162292480469
Batch 35/64 loss: 0.008821666240692139
Batch 36/64 loss: -0.0013971924781799316
Batch 37/64 loss: -0.0037581920623779297
Batch 38/64 loss: 0.015084981918334961
Batch 39/64 loss: 0.015561819076538086
Batch 40/64 loss: 0.0078200101852417
Batch 41/64 loss: 0.007150888442993164
Batch 42/64 loss: -0.00512850284576416
Batch 43/64 loss: 0.008498191833496094
Batch 44/64 loss: 0.010251104831695557
Batch 45/64 loss: -0.009543776512145996
Batch 46/64 loss: 0.024397432804107666
Batch 47/64 loss: 0.0065860748291015625
Batch 48/64 loss: 0.0038869380950927734
Batch 49/64 loss: -0.002202630043029785
Batch 50/64 loss: -0.009980499744415283
Batch 51/64 loss: 0.0017813444137573242
Batch 52/64 loss: -0.010393381118774414
Batch 53/64 loss: 0.010322093963623047
Batch 54/64 loss: 0.003603637218475342
Batch 55/64 loss: 0.0046073198318481445
Batch 56/64 loss: 0.013061344623565674
Batch 57/64 loss: 0.0027025938034057617
Batch 58/64 loss: -0.010898351669311523
Batch 59/64 loss: 0.00043761730194091797
Batch 60/64 loss: 0.0012331604957580566
Batch 61/64 loss: 0.0004152059555053711
Batch 62/64 loss: -0.011856675148010254
Batch 63/64 loss: -0.0069724321365356445
Batch 64/64 loss: -0.006947934627532959
Epoch 318  Train loss: -0.0008929021218243768  Val loss: 0.0892552473291089
Epoch 319
-------------------------------
Batch 1/64 loss: -0.003848731517791748
Batch 2/64 loss: -0.01678365468978882
Batch 3/64 loss: -0.008758842945098877
Batch 4/64 loss: -0.010214447975158691
Batch 5/64 loss: 0.009840190410614014
Batch 6/64 loss: -0.006199002265930176
Batch 7/64 loss: -0.011788666248321533
Batch 8/64 loss: 0.002241849899291992
Batch 9/64 loss: -0.0029935240745544434
Batch 10/64 loss: -0.004840731620788574
Batch 11/64 loss: 0.01455599069595337
Batch 12/64 loss: -0.0029481053352355957
Batch 13/64 loss: -0.01508861780166626
Batch 14/64 loss: -0.011040985584259033
Batch 15/64 loss: 0.010247886180877686
Batch 16/64 loss: -0.014009952545166016
Batch 17/64 loss: -0.006698489189147949
Batch 18/64 loss: -0.01690971851348877
Batch 19/64 loss: -0.0024486184120178223
Batch 20/64 loss: -0.011493086814880371
Batch 21/64 loss: -0.010680317878723145
Batch 22/64 loss: -0.010158300399780273
Batch 23/64 loss: -0.02833801507949829
Batch 24/64 loss: -0.007218122482299805
Batch 25/64 loss: -0.01348865032196045
Batch 26/64 loss: -0.003077268600463867
Batch 27/64 loss: -0.02245485782623291
Batch 28/64 loss: -0.0023612380027770996
Batch 29/64 loss: -0.004060208797454834
Batch 30/64 loss: -0.0019751787185668945
Batch 31/64 loss: -0.006679534912109375
Batch 32/64 loss: -0.008455753326416016
Batch 33/64 loss: 0.020685970783233643
Batch 34/64 loss: -0.008679568767547607
Batch 35/64 loss: -0.004929661750793457
Batch 36/64 loss: 0.005215585231781006
Batch 37/64 loss: 0.0010468363761901855
Batch 38/64 loss: 0.008816838264465332
Batch 39/64 loss: 0.001305699348449707
Batch 40/64 loss: -0.01896798610687256
Batch 41/64 loss: 0.0031555891036987305
Batch 42/64 loss: 0.0045435428619384766
Batch 43/64 loss: 0.0012184977531433105
Batch 44/64 loss: 0.0008299946784973145
Batch 45/64 loss: 0.000914454460144043
Batch 46/64 loss: 0.0011677145957946777
Batch 47/64 loss: -0.0029937028884887695
Batch 48/64 loss: 0.0023339390754699707
Batch 49/64 loss: -0.014119207859039307
Batch 50/64 loss: -0.019331037998199463
Batch 51/64 loss: 0.0011257529258728027
Batch 52/64 loss: 0.010005354881286621
Batch 53/64 loss: -0.004673361778259277
Batch 54/64 loss: -0.0040544867515563965
Batch 55/64 loss: -0.0052825212478637695
Batch 56/64 loss: -0.009438753128051758
Batch 57/64 loss: -0.00473254919052124
Batch 58/64 loss: 0.011747360229492188
Batch 59/64 loss: 0.023433923721313477
Batch 60/64 loss: 0.0047550201416015625
Batch 61/64 loss: 0.020141661167144775
Batch 62/64 loss: 0.00354158878326416
Batch 63/64 loss: 0.0059814453125
Batch 64/64 loss: -0.003062725067138672
Epoch 319  Train loss: -0.003069173588472254  Val loss: 0.09133299539998635
Epoch 320
-------------------------------
Batch 1/64 loss: -0.016537606716156006
Batch 2/64 loss: 0.006049215793609619
Batch 3/64 loss: -0.005319416522979736
Batch 4/64 loss: 0.00966191291809082
Batch 5/64 loss: 0.009140074253082275
Batch 6/64 loss: -0.0028492212295532227
Batch 7/64 loss: -0.01285785436630249
Batch 8/64 loss: -0.005388617515563965
Batch 9/64 loss: -0.02850949764251709
Batch 10/64 loss: 0.0006362199783325195
Batch 11/64 loss: 0.00494837760925293
Batch 12/64 loss: -0.014236927032470703
Batch 13/64 loss: -0.016640841960906982
Batch 14/64 loss: 0.012259900569915771
Batch 15/64 loss: -0.0017859935760498047
Batch 16/64 loss: -0.007282555103302002
Batch 17/64 loss: 0.00029414892196655273
Batch 18/64 loss: -0.009401142597198486
Batch 19/64 loss: -0.007671713829040527
Batch 20/64 loss: -0.011807739734649658
Batch 21/64 loss: 0.004025757312774658
Batch 22/64 loss: -0.006004691123962402
Batch 23/64 loss: 0.014170229434967041
Batch 24/64 loss: -0.0064733028411865234
Batch 25/64 loss: -0.009555637836456299
Batch 26/64 loss: 0.007073581218719482
Batch 27/64 loss: -0.0062639713287353516
Batch 28/64 loss: -0.004976212978363037
Batch 29/64 loss: -0.007976531982421875
Batch 30/64 loss: -0.00024700164794921875
Batch 31/64 loss: 0.010211944580078125
Batch 32/64 loss: -0.0084306001663208
Batch 33/64 loss: -0.008259832859039307
Batch 34/64 loss: -0.010624468326568604
Batch 35/64 loss: 0.0004532933235168457
Batch 36/64 loss: -0.0024625062942504883
Batch 37/64 loss: 0.005783796310424805
Batch 38/64 loss: 0.008388161659240723
Batch 39/64 loss: -0.002399265766143799
Batch 40/64 loss: -0.010132551193237305
Batch 41/64 loss: -0.011608421802520752
Batch 42/64 loss: 0.005308628082275391
Batch 43/64 loss: -0.0222243070602417
Batch 44/64 loss: -0.01257944107055664
Batch 45/64 loss: 0.0068514347076416016
Batch 46/64 loss: 0.01387929916381836
Batch 47/64 loss: -0.000929713249206543
Batch 48/64 loss: -0.006043791770935059
Batch 49/64 loss: 0.006716728210449219
Batch 50/64 loss: -0.001641392707824707
Batch 51/64 loss: -0.0014355182647705078
Batch 52/64 loss: 0.01350480318069458
Batch 53/64 loss: 0.007683753967285156
Batch 54/64 loss: 0.013888657093048096
Batch 55/64 loss: -0.012515664100646973
Batch 56/64 loss: 0.004781603813171387
Batch 57/64 loss: -0.0053021907806396484
Batch 58/64 loss: -0.006570398807525635
Batch 59/64 loss: -0.014930486679077148
Batch 60/64 loss: -0.004237353801727295
Batch 61/64 loss: 0.0009022355079650879
Batch 62/64 loss: 0.015334248542785645
Batch 63/64 loss: -0.0038799047470092773
Batch 64/64 loss: -0.0018766522407531738
Epoch 320  Train loss: -0.0023130003143759337  Val loss: 0.08928559899739794
Epoch 321
-------------------------------
Batch 1/64 loss: -2.765655517578125e-05
Batch 2/64 loss: -0.0030528903007507324
Batch 3/64 loss: 0.0011131763458251953
Batch 4/64 loss: 0.0005098581314086914
Batch 5/64 loss: -0.0006093382835388184
Batch 6/64 loss: 0.0013926029205322266
Batch 7/64 loss: -0.013303160667419434
Batch 8/64 loss: -0.004473447799682617
Batch 9/64 loss: -0.0013467669486999512
Batch 10/64 loss: -0.0014371871948242188
Batch 11/64 loss: 0.01623845100402832
Batch 12/64 loss: 0.007075786590576172
Batch 13/64 loss: 0.016148090362548828
Batch 14/64 loss: -0.004278361797332764
Batch 15/64 loss: -0.005789637565612793
Batch 16/64 loss: 0.014000475406646729
Batch 17/64 loss: -0.002306342124938965
Batch 18/64 loss: -0.009835898876190186
Batch 19/64 loss: 0.009244263172149658
Batch 20/64 loss: 0.008251428604125977
Batch 21/64 loss: -0.012975692749023438
Batch 22/64 loss: 0.0017847418785095215
Batch 23/64 loss: -0.0013099908828735352
Batch 24/64 loss: 0.0041043758392333984
Batch 25/64 loss: -0.005263149738311768
Batch 26/64 loss: -0.02427297830581665
Batch 27/64 loss: -0.02004563808441162
Batch 28/64 loss: -0.011116266250610352
Batch 29/64 loss: -0.010132968425750732
Batch 30/64 loss: -0.019734680652618408
Batch 31/64 loss: -0.02335953712463379
Batch 32/64 loss: 0.007191061973571777
Batch 33/64 loss: -0.01695305109024048
Batch 34/64 loss: 0.00903254747390747
Batch 35/64 loss: -0.011117637157440186
Batch 36/64 loss: -0.007518768310546875
Batch 37/64 loss: 0.0009874701499938965
Batch 38/64 loss: -0.0048694610595703125
Batch 39/64 loss: -0.006922006607055664
Batch 40/64 loss: 0.0012875795364379883
Batch 41/64 loss: -0.0042459964752197266
Batch 42/64 loss: -0.023100435733795166
Batch 43/64 loss: -0.002460300922393799
Batch 44/64 loss: -0.0009131431579589844
Batch 45/64 loss: -0.007357001304626465
Batch 46/64 loss: -0.01741206645965576
Batch 47/64 loss: -0.0016420483589172363
Batch 48/64 loss: 0.00678175687789917
Batch 49/64 loss: 0.00010627508163452148
Batch 50/64 loss: 0.0009945034980773926
Batch 51/64 loss: 0.010016202926635742
Batch 52/64 loss: 0.009734153747558594
Batch 53/64 loss: -0.011877775192260742
Batch 54/64 loss: -0.0021208524703979492
Batch 55/64 loss: -0.007932066917419434
Batch 56/64 loss: -0.00921320915222168
Batch 57/64 loss: -0.0029855966567993164
Batch 58/64 loss: 0.006840646266937256
Batch 59/64 loss: -0.0016931891441345215
Batch 60/64 loss: 0.0076743364334106445
Batch 61/64 loss: 0.008629977703094482
Batch 62/64 loss: -0.011543512344360352
Batch 63/64 loss: 0.013731718063354492
Batch 64/64 loss: 0.0003229975700378418
Epoch 321  Train loss: -0.0025637016576879165  Val loss: 0.09407007059280816
Epoch 322
-------------------------------
Batch 1/64 loss: 0.007142484188079834
Batch 2/64 loss: -0.008233428001403809
Batch 3/64 loss: 0.00048041343688964844
Batch 4/64 loss: 0.008693575859069824
Batch 5/64 loss: -0.008787989616394043
Batch 6/64 loss: -0.016288340091705322
Batch 7/64 loss: -0.009388864040374756
Batch 8/64 loss: 0.010063230991363525
Batch 9/64 loss: 0.000642240047454834
Batch 10/64 loss: -0.020837903022766113
Batch 11/64 loss: 0.008601486682891846
Batch 12/64 loss: -0.02110767364501953
Batch 13/64 loss: 0.015949785709381104
Batch 14/64 loss: -0.011230826377868652
Batch 15/64 loss: -0.013373076915740967
Batch 16/64 loss: -0.004444718360900879
Batch 17/64 loss: -0.012814521789550781
Batch 18/64 loss: -0.013388872146606445
Batch 19/64 loss: -0.0020945072174072266
Batch 20/64 loss: -0.01215893030166626
Batch 21/64 loss: 0.0013402700424194336
Batch 22/64 loss: 0.0050871968269348145
Batch 23/64 loss: -0.008789539337158203
Batch 24/64 loss: 0.00014966726303100586
Batch 25/64 loss: -0.014463663101196289
Batch 26/64 loss: -0.005401194095611572
Batch 27/64 loss: -0.015143275260925293
Batch 28/64 loss: -0.01315605640411377
Batch 29/64 loss: -0.0036545991897583008
Batch 30/64 loss: -0.015988945960998535
Batch 31/64 loss: -0.011411011219024658
Batch 32/64 loss: 0.0006857514381408691
Batch 33/64 loss: -0.003175079822540283
Batch 34/64 loss: -0.007684111595153809
Batch 35/64 loss: 0.00054168701171875
Batch 36/64 loss: -0.013626933097839355
Batch 37/64 loss: -0.0187186598777771
Batch 38/64 loss: -0.015291392803192139
Batch 39/64 loss: -0.015727877616882324
Batch 40/64 loss: -0.005470931529998779
Batch 41/64 loss: -0.0035439729690551758
Batch 42/64 loss: -0.010318398475646973
Batch 43/64 loss: 0.0013289451599121094
Batch 44/64 loss: -0.018991827964782715
Batch 45/64 loss: -0.009117543697357178
Batch 46/64 loss: -0.003157198429107666
Batch 47/64 loss: -0.00982046127319336
Batch 48/64 loss: 0.00018554925918579102
Batch 49/64 loss: -0.014132916927337646
Batch 50/64 loss: -0.01407921314239502
Batch 51/64 loss: -0.0072629451751708984
Batch 52/64 loss: 0.01203155517578125
Batch 53/64 loss: 0.0072512030601501465
Batch 54/64 loss: -0.007640659809112549
Batch 55/64 loss: 0.01010817289352417
Batch 56/64 loss: 3.5762786865234375e-07
Batch 57/64 loss: -0.004397034645080566
Batch 58/64 loss: -0.004122376441955566
Batch 59/64 loss: 0.008264243602752686
Batch 60/64 loss: 0.012969374656677246
Batch 61/64 loss: -0.011313199996948242
Batch 62/64 loss: 0.012630999088287354
Batch 63/64 loss: -0.011189520359039307
Batch 64/64 loss: -0.0003209710121154785
Epoch 322  Train loss: -0.005129925176209095  Val loss: 0.09205259756533961
Epoch 323
-------------------------------
Batch 1/64 loss: -0.007019996643066406
Batch 2/64 loss: -0.01011425256729126
Batch 3/64 loss: 0.0008239150047302246
Batch 4/64 loss: -0.020416975021362305
Batch 5/64 loss: -0.003890097141265869
Batch 6/64 loss: -0.02191835641860962
Batch 7/64 loss: 0.020124316215515137
Batch 8/64 loss: -0.009117484092712402
Batch 9/64 loss: -0.005435466766357422
Batch 10/64 loss: -0.011460065841674805
Batch 11/64 loss: -0.007845699787139893
Batch 12/64 loss: -0.013847947120666504
Batch 13/64 loss: -0.01974010467529297
Batch 14/64 loss: -0.008494913578033447
Batch 15/64 loss: -0.019122540950775146
Batch 16/64 loss: -0.014238834381103516
Batch 17/64 loss: 0.008382081985473633
Batch 18/64 loss: -0.007404208183288574
Batch 19/64 loss: -0.0021930336952209473
Batch 20/64 loss: -0.003286600112915039
Batch 21/64 loss: -0.006568014621734619
Batch 22/64 loss: -0.00935053825378418
Batch 23/64 loss: -0.005287110805511475
Batch 24/64 loss: -0.014316797256469727
Batch 25/64 loss: -0.012746930122375488
Batch 26/64 loss: -0.004103541374206543
Batch 27/64 loss: -0.007286190986633301
Batch 28/64 loss: -0.00043582916259765625
Batch 29/64 loss: -0.005347609519958496
Batch 30/64 loss: -0.0177270770072937
Batch 31/64 loss: -0.005398929119110107
Batch 32/64 loss: -0.011798262596130371
Batch 33/64 loss: 0.0017520785331726074
Batch 34/64 loss: -0.004701733589172363
Batch 35/64 loss: 0.004309892654418945
Batch 36/64 loss: -0.017006218433380127
Batch 37/64 loss: -0.010490894317626953
Batch 38/64 loss: -0.013229131698608398
Batch 39/64 loss: -0.005890071392059326
Batch 40/64 loss: -0.012862324714660645
Batch 41/64 loss: -0.013218939304351807
Batch 42/64 loss: 0.004872441291809082
Batch 43/64 loss: -0.022796273231506348
Batch 44/64 loss: -0.0027890801429748535
Batch 45/64 loss: -0.01911783218383789
Batch 46/64 loss: -0.009183645248413086
Batch 47/64 loss: -0.014635860919952393
Batch 48/64 loss: -0.003773927688598633
Batch 49/64 loss: -0.016013026237487793
Batch 50/64 loss: -0.0009268522262573242
Batch 51/64 loss: -0.003577291965484619
Batch 52/64 loss: 0.029966413974761963
Batch 53/64 loss: -0.014277935028076172
Batch 54/64 loss: 0.0170438289642334
Batch 55/64 loss: 0.011086821556091309
Batch 56/64 loss: 0.0004869699478149414
Batch 57/64 loss: 2.682209014892578e-06
Batch 58/64 loss: 0.01024395227432251
Batch 59/64 loss: -0.0006005764007568359
Batch 60/64 loss: -0.013585329055786133
Batch 61/64 loss: -0.005323946475982666
Batch 62/64 loss: 0.0021294355392456055
Batch 63/64 loss: -0.0032359957695007324
Batch 64/64 loss: -0.011859774589538574
Epoch 323  Train loss: -0.0061305144253899066  Val loss: 0.09124721325549882
Epoch 324
-------------------------------
Batch 1/64 loss: 0.004694104194641113
Batch 2/64 loss: -0.007439017295837402
Batch 3/64 loss: -7.826089859008789e-05
Batch 4/64 loss: 0.002205967903137207
Batch 5/64 loss: -0.014673948287963867
Batch 6/64 loss: -0.017714381217956543
Batch 7/64 loss: -0.009268701076507568
Batch 8/64 loss: -0.015781402587890625
Batch 9/64 loss: 0.010261833667755127
Batch 10/64 loss: -0.002196669578552246
Batch 11/64 loss: 0.01449573040008545
Batch 12/64 loss: -8.797645568847656e-05
Batch 13/64 loss: -0.016602158546447754
Batch 14/64 loss: -0.0003497600555419922
Batch 15/64 loss: -0.0053863525390625
Batch 16/64 loss: 0.0016421079635620117
Batch 17/64 loss: -0.008569180965423584
Batch 18/64 loss: -0.00876915454864502
Batch 19/64 loss: -0.010280370712280273
Batch 20/64 loss: -0.0025724172592163086
Batch 21/64 loss: -0.0039827823638916016
Batch 22/64 loss: -0.0077056884765625
Batch 23/64 loss: -0.013248324394226074
Batch 24/64 loss: -0.015322089195251465
Batch 25/64 loss: -0.004098474979400635
Batch 26/64 loss: 0.006073296070098877
Batch 27/64 loss: -0.02101689577102661
Batch 28/64 loss: 0.0024656057357788086
Batch 29/64 loss: 0.014000535011291504
Batch 30/64 loss: 0.007339000701904297
Batch 31/64 loss: -0.008403003215789795
Batch 32/64 loss: -0.004180729389190674
Batch 33/64 loss: -0.009376287460327148
Batch 34/64 loss: 0.0008159875869750977
Batch 35/64 loss: 0.003916501998901367
Batch 36/64 loss: 0.011423707008361816
Batch 37/64 loss: -0.012477457523345947
Batch 38/64 loss: 0.0058217644691467285
Batch 39/64 loss: -0.009024858474731445
Batch 40/64 loss: -0.012341201305389404
Batch 41/64 loss: -0.014471113681793213
Batch 42/64 loss: -0.006105482578277588
Batch 43/64 loss: -0.0129508376121521
Batch 44/64 loss: 0.0013321638107299805
Batch 45/64 loss: -0.007074952125549316
Batch 46/64 loss: -0.00869739055633545
Batch 47/64 loss: -0.013298511505126953
Batch 48/64 loss: 0.004350900650024414
Batch 49/64 loss: -0.005540609359741211
Batch 50/64 loss: -0.0036663413047790527
Batch 51/64 loss: -0.009171605110168457
Batch 52/64 loss: -0.00778573751449585
Batch 53/64 loss: -0.0008590221405029297
Batch 54/64 loss: 0.006822049617767334
Batch 55/64 loss: -0.006292462348937988
Batch 56/64 loss: -0.003206193447113037
Batch 57/64 loss: 0.014122366905212402
Batch 58/64 loss: -0.006502628326416016
Batch 59/64 loss: -0.009331703186035156
Batch 60/64 loss: -0.002093195915222168
Batch 61/64 loss: -0.00892496109008789
Batch 62/64 loss: 0.005183219909667969
Batch 63/64 loss: -0.004718303680419922
Batch 64/64 loss: 0.0067855119705200195
Epoch 324  Train loss: -0.003915021466273888  Val loss: 0.08979349201897166
Epoch 325
-------------------------------
Batch 1/64 loss: -0.023995697498321533
Batch 2/64 loss: -0.02819359302520752
Batch 3/64 loss: -0.006916940212249756
Batch 4/64 loss: -0.025015950202941895
Batch 5/64 loss: -0.021384775638580322
Batch 6/64 loss: -0.028734445571899414
Batch 7/64 loss: -0.023421823978424072
Batch 8/64 loss: -0.01470249891281128
Batch 9/64 loss: -0.015211105346679688
Batch 10/64 loss: 0.0009210705757141113
Batch 11/64 loss: -0.018979310989379883
Batch 12/64 loss: 0.009984910488128662
Batch 13/64 loss: 0.004599452018737793
Batch 14/64 loss: 1.7702579498291016e-05
Batch 15/64 loss: -0.0024080276489257812
Batch 16/64 loss: -0.004075765609741211
Batch 17/64 loss: -0.015039920806884766
Batch 18/64 loss: -0.0032462477684020996
Batch 19/64 loss: -0.014249145984649658
Batch 20/64 loss: -0.00950247049331665
Batch 21/64 loss: -0.0035548806190490723
Batch 22/64 loss: -0.008533179759979248
Batch 23/64 loss: -0.004984676837921143
Batch 24/64 loss: 0.0014244914054870605
Batch 25/64 loss: 0.009141623973846436
Batch 26/64 loss: -0.0003495216369628906
Batch 27/64 loss: -0.0048410892486572266
Batch 28/64 loss: 0.013263940811157227
Batch 29/64 loss: 0.003364086151123047
Batch 30/64 loss: -0.010600745677947998
Batch 31/64 loss: -0.019430160522460938
Batch 32/64 loss: -0.010707437992095947
Batch 33/64 loss: -0.009300410747528076
Batch 34/64 loss: -0.005842685699462891
Batch 35/64 loss: -0.006263256072998047
Batch 36/64 loss: 0.006604671478271484
Batch 37/64 loss: -0.0021308064460754395
Batch 38/64 loss: -0.006851077079772949
Batch 39/64 loss: 0.004262387752532959
Batch 40/64 loss: -0.021423697471618652
Batch 41/64 loss: -0.013427734375
Batch 42/64 loss: -0.005891740322113037
Batch 43/64 loss: -0.014195144176483154
Batch 44/64 loss: -0.011715173721313477
Batch 45/64 loss: -0.010645031929016113
Batch 46/64 loss: 0.010104060173034668
Batch 47/64 loss: -0.01080310344696045
Batch 48/64 loss: 0.007249712944030762
Batch 49/64 loss: -0.0027437210083007812
Batch 50/64 loss: -0.00014084577560424805
Batch 51/64 loss: 0.0022573471069335938
Batch 52/64 loss: 0.003368377685546875
Batch 53/64 loss: -0.003603518009185791
Batch 54/64 loss: -0.01474541425704956
Batch 55/64 loss: -0.002810180187225342
Batch 56/64 loss: -0.0020658373832702637
Batch 57/64 loss: -9.864568710327148e-05
Batch 58/64 loss: 0.010665297508239746
Batch 59/64 loss: -0.010226130485534668
Batch 60/64 loss: -0.013955473899841309
Batch 61/64 loss: -0.0023239850997924805
Batch 62/64 loss: 0.011514663696289062
Batch 63/64 loss: -0.007548332214355469
Batch 64/64 loss: -0.0029697418212890625
Epoch 325  Train loss: -0.006279448901905733  Val loss: 0.08907745732474573
Epoch 326
-------------------------------
Batch 1/64 loss: -0.012468397617340088
Batch 2/64 loss: 0.008714437484741211
Batch 3/64 loss: -0.027398645877838135
Batch 4/64 loss: -0.016782402992248535
Batch 5/64 loss: -0.002368032932281494
Batch 6/64 loss: -0.007850408554077148
Batch 7/64 loss: -0.011484920978546143
Batch 8/64 loss: 0.021541476249694824
Batch 9/64 loss: -0.00603330135345459
Batch 10/64 loss: -0.006133913993835449
Batch 11/64 loss: -0.003453552722930908
Batch 12/64 loss: -0.0044564008712768555
Batch 13/64 loss: -0.018975496292114258
Batch 14/64 loss: -0.017001807689666748
Batch 15/64 loss: -0.016790926456451416
Batch 16/64 loss: -0.011893272399902344
Batch 17/64 loss: 0.003336191177368164
Batch 18/64 loss: -0.008027315139770508
Batch 19/64 loss: -0.011473536491394043
Batch 20/64 loss: -0.0008952021598815918
Batch 21/64 loss: 0.0029010772705078125
Batch 22/64 loss: 0.004113256931304932
Batch 23/64 loss: -0.00800091028213501
Batch 24/64 loss: -0.012434840202331543
Batch 25/64 loss: -0.019083499908447266
Batch 26/64 loss: 0.007648587226867676
Batch 27/64 loss: -0.00866544246673584
Batch 28/64 loss: 0.006197750568389893
Batch 29/64 loss: -0.016169250011444092
Batch 30/64 loss: -0.011727631092071533
Batch 31/64 loss: -0.0014511346817016602
Batch 32/64 loss: -0.007462143898010254
Batch 33/64 loss: -0.01382589340209961
Batch 34/64 loss: -0.0039004087448120117
Batch 35/64 loss: -0.006182253360748291
Batch 36/64 loss: 0.007908821105957031
Batch 37/64 loss: -0.00655895471572876
Batch 38/64 loss: 0.0021469593048095703
Batch 39/64 loss: 0.00616532564163208
Batch 40/64 loss: -0.009368181228637695
Batch 41/64 loss: -0.020180344581604004
Batch 42/64 loss: 0.003325045108795166
Batch 43/64 loss: -0.004887700080871582
Batch 44/64 loss: -0.009088099002838135
Batch 45/64 loss: -0.00986713171005249
Batch 46/64 loss: 0.0005290508270263672
Batch 47/64 loss: -0.008270084857940674
Batch 48/64 loss: -0.020877718925476074
Batch 49/64 loss: -0.003966629505157471
Batch 50/64 loss: -0.0053174495697021484
Batch 51/64 loss: -0.0036798715591430664
Batch 52/64 loss: -0.006666421890258789
Batch 53/64 loss: -0.013806819915771484
Batch 54/64 loss: -0.014963030815124512
Batch 55/64 loss: -0.018037617206573486
Batch 56/64 loss: -0.008717060089111328
Batch 57/64 loss: -0.010143637657165527
Batch 58/64 loss: -0.0029590725898742676
Batch 59/64 loss: -0.0002003312110900879
Batch 60/64 loss: -0.0024909377098083496
Batch 61/64 loss: 0.017287373542785645
Batch 62/64 loss: 0.023047208786010742
Batch 63/64 loss: -0.004622280597686768
Batch 64/64 loss: -0.005491971969604492
Epoch 326  Train loss: -0.005746144874423158  Val loss: 0.09096767361631099
Epoch 327
-------------------------------
Batch 1/64 loss: -0.01660621166229248
Batch 2/64 loss: 0.002450227737426758
Batch 3/64 loss: -0.011944711208343506
Batch 4/64 loss: -0.01486515998840332
Batch 5/64 loss: -0.008311569690704346
Batch 6/64 loss: -0.007970094680786133
Batch 7/64 loss: -0.01587355136871338
Batch 8/64 loss: -0.016265451908111572
Batch 9/64 loss: -0.006028294563293457
Batch 10/64 loss: -0.00046825408935546875
Batch 11/64 loss: -0.019581258296966553
Batch 12/64 loss: -0.0058100223541259766
Batch 13/64 loss: -0.009101450443267822
Batch 14/64 loss: -0.018727540969848633
Batch 15/64 loss: -0.00558704137802124
Batch 16/64 loss: -0.007425129413604736
Batch 17/64 loss: -0.01496279239654541
Batch 18/64 loss: 0.00024688243865966797
Batch 19/64 loss: -0.012170493602752686
Batch 20/64 loss: -0.006069302558898926
Batch 21/64 loss: -0.006608843803405762
Batch 22/64 loss: -0.016389548778533936
Batch 23/64 loss: -0.022418618202209473
Batch 24/64 loss: 0.004139304161071777
Batch 25/64 loss: -0.0063817501068115234
Batch 26/64 loss: 0.0012391209602355957
Batch 27/64 loss: -0.003994762897491455
Batch 28/64 loss: -0.014860033988952637
Batch 29/64 loss: -0.008434057235717773
Batch 30/64 loss: -0.019534051418304443
Batch 31/64 loss: -0.0038123726844787598
Batch 32/64 loss: -0.00960928201675415
Batch 33/64 loss: -0.0026395320892333984
Batch 34/64 loss: -0.003197193145751953
Batch 35/64 loss: -0.0038564205169677734
Batch 36/64 loss: -0.01819896697998047
Batch 37/64 loss: -0.01167827844619751
Batch 38/64 loss: 0.013984322547912598
Batch 39/64 loss: -0.017172157764434814
Batch 40/64 loss: 0.0035189390182495117
Batch 41/64 loss: -0.0058078765869140625
Batch 42/64 loss: -0.0019404292106628418
Batch 43/64 loss: -0.012696385383605957
Batch 44/64 loss: -0.009969830513000488
Batch 45/64 loss: -0.0021584033966064453
Batch 46/64 loss: -0.018691718578338623
Batch 47/64 loss: -0.004843652248382568
Batch 48/64 loss: 0.000761568546295166
Batch 49/64 loss: -0.002888917922973633
Batch 50/64 loss: -0.021401822566986084
Batch 51/64 loss: -0.005351901054382324
Batch 52/64 loss: -0.014634251594543457
Batch 53/64 loss: 0.004150271415710449
Batch 54/64 loss: -0.015318989753723145
Batch 55/64 loss: -0.003948509693145752
Batch 56/64 loss: -0.0065985918045043945
Batch 57/64 loss: -0.0038263797760009766
Batch 58/64 loss: -0.010569572448730469
Batch 59/64 loss: -0.00309908390045166
Batch 60/64 loss: 0.013464808464050293
Batch 61/64 loss: 0.003563225269317627
Batch 62/64 loss: -0.0005106329917907715
Batch 63/64 loss: -0.015788912773132324
Batch 64/64 loss: 0.007890582084655762
Epoch 327  Train loss: -0.007422171854505352  Val loss: 0.09047849870629326
Epoch 328
-------------------------------
Batch 1/64 loss: -0.0015826225280761719
Batch 2/64 loss: -0.01618105173110962
Batch 3/64 loss: -0.020594358444213867
Batch 4/64 loss: -0.01413118839263916
Batch 5/64 loss: -0.006624042987823486
Batch 6/64 loss: -0.015938162803649902
Batch 7/64 loss: -0.005591452121734619
Batch 8/64 loss: -0.011050164699554443
Batch 9/64 loss: 0.00618743896484375
Batch 10/64 loss: -0.005610823631286621
Batch 11/64 loss: 0.0021520256996154785
Batch 12/64 loss: -0.009911656379699707
Batch 13/64 loss: 0.02104365825653076
Batch 14/64 loss: -0.00711977481842041
Batch 15/64 loss: -0.0068645477294921875
Batch 16/64 loss: -0.0044841766357421875
Batch 17/64 loss: 0.0004112720489501953
Batch 18/64 loss: -0.010739326477050781
Batch 19/64 loss: -0.0037397146224975586
Batch 20/64 loss: -0.005788207054138184
Batch 21/64 loss: -0.007750272750854492
Batch 22/64 loss: -0.006748080253601074
Batch 23/64 loss: -0.015638351440429688
Batch 24/64 loss: -0.0038539767265319824
Batch 25/64 loss: -0.012150585651397705
Batch 26/64 loss: 0.0013592243194580078
Batch 27/64 loss: -0.01310509443283081
Batch 28/64 loss: -0.00943458080291748
Batch 29/64 loss: -0.009600639343261719
Batch 30/64 loss: 5.888938903808594e-05
Batch 31/64 loss: -0.005151629447937012
Batch 32/64 loss: -0.0005931258201599121
Batch 33/64 loss: -0.00904625654220581
Batch 34/64 loss: 0.0033054351806640625
Batch 35/64 loss: 0.0004311800003051758
Batch 36/64 loss: -0.006953239440917969
Batch 37/64 loss: -0.004126012325286865
Batch 38/64 loss: -0.016599714756011963
Batch 39/64 loss: -0.0048770904541015625
Batch 40/64 loss: -0.011216998100280762
Batch 41/64 loss: -0.009580492973327637
Batch 42/64 loss: 0.00989753007888794
Batch 43/64 loss: 0.005868852138519287
Batch 44/64 loss: -0.013178586959838867
Batch 45/64 loss: 0.0007213950157165527
Batch 46/64 loss: -0.009249091148376465
Batch 47/64 loss: -0.010519683361053467
Batch 48/64 loss: -0.0033321380615234375
Batch 49/64 loss: -0.013093531131744385
Batch 50/64 loss: -0.010236382484436035
Batch 51/64 loss: -0.009894311428070068
Batch 52/64 loss: 0.017528176307678223
Batch 53/64 loss: -0.009735941886901855
Batch 54/64 loss: 0.006234109401702881
Batch 55/64 loss: -0.008643627166748047
Batch 56/64 loss: -0.015851736068725586
Batch 57/64 loss: 0.005555987358093262
Batch 58/64 loss: -0.018166303634643555
Batch 59/64 loss: -0.007104635238647461
Batch 60/64 loss: 0.005257725715637207
Batch 61/64 loss: -0.0064705610275268555
Batch 62/64 loss: -0.012611627578735352
Batch 63/64 loss: -0.017359614372253418
Batch 64/64 loss: -0.0012705326080322266
Epoch 328  Train loss: -0.005847296995275161  Val loss: 0.09026971100941557
Epoch 329
-------------------------------
Batch 1/64 loss: 0.0038607120513916016
Batch 2/64 loss: 0.0032665133476257324
Batch 3/64 loss: 0.001835465431213379
Batch 4/64 loss: 0.006037533283233643
Batch 5/64 loss: 0.006231546401977539
Batch 6/64 loss: 0.016781866550445557
Batch 7/64 loss: -0.011575460433959961
Batch 8/64 loss: -0.0050598978996276855
Batch 9/64 loss: -0.005914032459259033
Batch 10/64 loss: -0.010520100593566895
Batch 11/64 loss: -0.014472126960754395
Batch 12/64 loss: -0.00864940881729126
Batch 13/64 loss: 0.003964662551879883
Batch 14/64 loss: -0.012570619583129883
Batch 15/64 loss: -0.007225692272186279
Batch 16/64 loss: -0.0017375946044921875
Batch 17/64 loss: -0.01232612133026123
Batch 18/64 loss: -0.005614876747131348
Batch 19/64 loss: 0.001871943473815918
Batch 20/64 loss: -0.018199801445007324
Batch 21/64 loss: 0.009263575077056885
Batch 22/64 loss: -0.023861706256866455
Batch 23/64 loss: -0.018668413162231445
Batch 24/64 loss: -0.0104140043258667
Batch 25/64 loss: -0.00816655158996582
Batch 26/64 loss: -0.007494807243347168
Batch 27/64 loss: 0.02012324333190918
Batch 28/64 loss: -0.002314746379852295
Batch 29/64 loss: 0.0012743473052978516
Batch 30/64 loss: -0.007335484027862549
Batch 31/64 loss: -0.007307171821594238
Batch 32/64 loss: -0.020870327949523926
Batch 33/64 loss: -0.009240508079528809
Batch 34/64 loss: -0.011009573936462402
Batch 35/64 loss: -0.004586756229400635
Batch 36/64 loss: 0.0010184049606323242
Batch 37/64 loss: -0.007040739059448242
Batch 38/64 loss: -0.004422783851623535
Batch 39/64 loss: -0.010924875736236572
Batch 40/64 loss: -0.006405472755432129
Batch 41/64 loss: 0.005789756774902344
Batch 42/64 loss: -0.02604854106903076
Batch 43/64 loss: -0.008792996406555176
Batch 44/64 loss: -0.013922452926635742
Batch 45/64 loss: -0.012746095657348633
Batch 46/64 loss: -0.005063831806182861
Batch 47/64 loss: 0.0007843375205993652
Batch 48/64 loss: -0.005121946334838867
Batch 49/64 loss: -0.005012989044189453
Batch 50/64 loss: -0.018747687339782715
Batch 51/64 loss: -0.013573527336120605
Batch 52/64 loss: -0.019295930862426758
Batch 53/64 loss: 0.02363055944442749
Batch 54/64 loss: -0.0029625892639160156
Batch 55/64 loss: -0.004627227783203125
Batch 56/64 loss: 0.00019615888595581055
Batch 57/64 loss: -0.005562484264373779
Batch 58/64 loss: -0.012152552604675293
Batch 59/64 loss: -0.0009413957595825195
Batch 60/64 loss: -0.00498509407043457
Batch 61/64 loss: -0.013184785842895508
Batch 62/64 loss: -0.01227867603302002
Batch 63/64 loss: -0.010596036911010742
Batch 64/64 loss: -0.0006112456321716309
Epoch 329  Train loss: -0.005710969485488593  Val loss: 0.09013673427588341
Epoch 330
-------------------------------
Batch 1/64 loss: 0.01151132583618164
Batch 2/64 loss: -0.013223409652709961
Batch 3/64 loss: -0.011800050735473633
Batch 4/64 loss: 0.007198214530944824
Batch 5/64 loss: -0.003806889057159424
Batch 6/64 loss: -0.0269584059715271
Batch 7/64 loss: 0.003934383392333984
Batch 8/64 loss: -0.01671159267425537
Batch 9/64 loss: -0.02548980712890625
Batch 10/64 loss: -0.006206989288330078
Batch 11/64 loss: -0.005667448043823242
Batch 12/64 loss: -0.017025887966156006
Batch 13/64 loss: -0.02418595552444458
Batch 14/64 loss: 0.02289646863937378
Batch 15/64 loss: -0.010538816452026367
Batch 16/64 loss: 0.01172095537185669
Batch 17/64 loss: -0.009910166263580322
Batch 18/64 loss: -0.007819652557373047
Batch 19/64 loss: -0.014155805110931396
Batch 20/64 loss: -0.013065159320831299
Batch 21/64 loss: -0.009210467338562012
Batch 22/64 loss: -0.010575950145721436
Batch 23/64 loss: -0.004118382930755615
Batch 24/64 loss: -0.025457441806793213
Batch 25/64 loss: -0.009589970111846924
Batch 26/64 loss: -0.023195087909698486
Batch 27/64 loss: -0.0024830102920532227
Batch 28/64 loss: -0.010577261447906494
Batch 29/64 loss: -0.01884835958480835
Batch 30/64 loss: -0.008478760719299316
Batch 31/64 loss: -0.0053501129150390625
Batch 32/64 loss: -0.011298060417175293
Batch 33/64 loss: -0.006659150123596191
Batch 34/64 loss: 0.004652440547943115
Batch 35/64 loss: -0.018896102905273438
Batch 36/64 loss: -0.032113075256347656
Batch 37/64 loss: -0.0009693503379821777
Batch 38/64 loss: -0.0064585208892822266
Batch 39/64 loss: -0.018837451934814453
Batch 40/64 loss: -0.0025485754013061523
Batch 41/64 loss: -0.01958465576171875
Batch 42/64 loss: 0.005320906639099121
Batch 43/64 loss: -0.015258193016052246
Batch 44/64 loss: -0.014122545719146729
Batch 45/64 loss: -0.017309606075286865
Batch 46/64 loss: 0.008675694465637207
Batch 47/64 loss: -0.017993152141571045
Batch 48/64 loss: -0.01314091682434082
Batch 49/64 loss: -0.012127041816711426
Batch 50/64 loss: -0.020537614822387695
Batch 51/64 loss: -0.012161314487457275
Batch 52/64 loss: -0.01341921091079712
Batch 53/64 loss: 0.018038272857666016
Batch 54/64 loss: -0.01224517822265625
Batch 55/64 loss: -0.012898445129394531
Batch 56/64 loss: 0.0060645341873168945
Batch 57/64 loss: 0.0043299198150634766
Batch 58/64 loss: 0.010697007179260254
Batch 59/64 loss: -0.00640636682510376
Batch 60/64 loss: -0.01934194564819336
Batch 61/64 loss: -0.012103855609893799
Batch 62/64 loss: -0.016160547733306885
Batch 63/64 loss: -0.013967931270599365
Batch 64/64 loss: -0.003959059715270996
Epoch 330  Train loss: -0.008924530534183277  Val loss: 0.08883566032979906
Epoch 331
-------------------------------
Batch 1/64 loss: -0.028592705726623535
Batch 2/64 loss: -0.023917675018310547
Batch 3/64 loss: -0.012442946434020996
Batch 4/64 loss: -0.026703596115112305
Batch 5/64 loss: -0.009835779666900635
Batch 6/64 loss: -0.01747232675552368
Batch 7/64 loss: -0.013484954833984375
Batch 8/64 loss: -0.007762730121612549
Batch 9/64 loss: -0.02502673864364624
Batch 10/64 loss: -0.009412884712219238
Batch 11/64 loss: -0.02021080255508423
Batch 12/64 loss: -0.007148683071136475
Batch 13/64 loss: -0.01776885986328125
Batch 14/64 loss: -0.021795034408569336
Batch 15/64 loss: -0.022425949573516846
Batch 16/64 loss: -0.01474696397781372
Batch 17/64 loss: -0.0032994747161865234
Batch 18/64 loss: 0.0019093751907348633
Batch 19/64 loss: -0.006914675235748291
Batch 20/64 loss: 0.00787806510925293
Batch 21/64 loss: -0.02090167999267578
Batch 22/64 loss: -0.024582386016845703
Batch 23/64 loss: -0.013424873352050781
Batch 24/64 loss: -0.01709514856338501
Batch 25/64 loss: -0.0008771419525146484
Batch 26/64 loss: -0.005054056644439697
Batch 27/64 loss: -0.0060332417488098145
Batch 28/64 loss: 0.004436075687408447
Batch 29/64 loss: -0.016877412796020508
Batch 30/64 loss: -0.011002004146575928
Batch 31/64 loss: -0.0064966678619384766
Batch 32/64 loss: 0.0008745789527893066
Batch 33/64 loss: 0.003865957260131836
Batch 34/64 loss: -0.009781301021575928
Batch 35/64 loss: -0.005348145961761475
Batch 36/64 loss: -0.006100118160247803
Batch 37/64 loss: -0.014171421527862549
Batch 38/64 loss: -0.011950552463531494
Batch 39/64 loss: -0.012547731399536133
Batch 40/64 loss: 0.024663925170898438
Batch 41/64 loss: 0.0014474987983703613
Batch 42/64 loss: 0.0010868310928344727
Batch 43/64 loss: -0.005915820598602295
Batch 44/64 loss: 0.01287698745727539
Batch 45/64 loss: 0.012431740760803223
Batch 46/64 loss: 0.011388063430786133
Batch 47/64 loss: -0.016199350357055664
Batch 48/64 loss: 0.004842162132263184
Batch 49/64 loss: -0.0034596920013427734
Batch 50/64 loss: -0.015148639678955078
Batch 51/64 loss: -0.017102360725402832
Batch 52/64 loss: 0.006591498851776123
Batch 53/64 loss: -0.006949305534362793
Batch 54/64 loss: -0.013715744018554688
Batch 55/64 loss: 0.00932222604751587
Batch 56/64 loss: 0.006988942623138428
Batch 57/64 loss: -0.006092965602874756
Batch 58/64 loss: -0.004932880401611328
Batch 59/64 loss: -0.013763427734375
Batch 60/64 loss: 0.0020121335983276367
Batch 61/64 loss: 0.004968523979187012
Batch 62/64 loss: -0.008172750473022461
Batch 63/64 loss: -0.007269144058227539
Batch 64/64 loss: -0.009106695652008057
Epoch 331  Train loss: -0.007516457754022935  Val loss: 0.09243091602915342
Epoch 332
-------------------------------
Batch 1/64 loss: 0.011505484580993652
Batch 2/64 loss: -0.01199418306350708
Batch 3/64 loss: 0.008590400218963623
Batch 4/64 loss: -0.001001894474029541
Batch 5/64 loss: -0.011709511280059814
Batch 6/64 loss: -0.014004945755004883
Batch 7/64 loss: -0.015742242336273193
Batch 8/64 loss: -0.014565825462341309
Batch 9/64 loss: 0.0008220076560974121
Batch 10/64 loss: 0.02439272403717041
Batch 11/64 loss: 0.01424187421798706
Batch 12/64 loss: 0.022038638591766357
Batch 13/64 loss: -0.003727436065673828
Batch 14/64 loss: -0.005117654800415039
Batch 15/64 loss: 0.0025184154510498047
Batch 16/64 loss: 0.002858579158782959
Batch 17/64 loss: -0.013253211975097656
Batch 18/64 loss: -0.009908854961395264
Batch 19/64 loss: 0.002239227294921875
Batch 20/64 loss: -0.009585261344909668
Batch 21/64 loss: -0.00874018669128418
Batch 22/64 loss: -0.019688844680786133
Batch 23/64 loss: -0.004519522190093994
Batch 24/64 loss: 0.006043434143066406
Batch 25/64 loss: -0.017234504222869873
Batch 26/64 loss: -0.016716718673706055
Batch 27/64 loss: -0.0005315542221069336
Batch 28/64 loss: -0.019526958465576172
Batch 29/64 loss: -0.005420804023742676
Batch 30/64 loss: -0.0076372623443603516
Batch 31/64 loss: -0.01015925407409668
Batch 32/64 loss: -0.012722253799438477
Batch 33/64 loss: -0.014955222606658936
Batch 34/64 loss: -0.01074141263961792
Batch 35/64 loss: -0.010399281978607178
Batch 36/64 loss: -0.02008110284805298
Batch 37/64 loss: -0.007676482200622559
Batch 38/64 loss: -0.010297596454620361
Batch 39/64 loss: -0.016992807388305664
Batch 40/64 loss: -0.012973368167877197
Batch 41/64 loss: 0.011383473873138428
Batch 42/64 loss: -0.020002782344818115
Batch 43/64 loss: -0.015018165111541748
Batch 44/64 loss: -0.011954545974731445
Batch 45/64 loss: 0.016485631465911865
Batch 46/64 loss: -0.009016752243041992
Batch 47/64 loss: -0.014411628246307373
Batch 48/64 loss: -0.0029296875
Batch 49/64 loss: -0.010892093181610107
Batch 50/64 loss: -0.0008918046951293945
Batch 51/64 loss: -0.01306915283203125
Batch 52/64 loss: -0.008832216262817383
Batch 53/64 loss: -0.00232541561126709
Batch 54/64 loss: -0.019665300846099854
Batch 55/64 loss: -0.002905130386352539
Batch 56/64 loss: -0.007399141788482666
Batch 57/64 loss: 0.0018617510795593262
Batch 58/64 loss: 0.005456209182739258
Batch 59/64 loss: -0.009052515029907227
Batch 60/64 loss: -0.006965458393096924
Batch 61/64 loss: -0.010123848915100098
Batch 62/64 loss: -0.018754124641418457
Batch 63/64 loss: -0.013375639915466309
Batch 64/64 loss: 0.0033661723136901855
Epoch 332  Train loss: -0.006309789536046047  Val loss: 0.09116567184834956
Epoch 333
-------------------------------
Batch 1/64 loss: 0.002285897731781006
Batch 2/64 loss: -0.008912861347198486
Batch 3/64 loss: 0.007494330406188965
Batch 4/64 loss: 0.022650837898254395
Batch 5/64 loss: 0.007570505142211914
Batch 6/64 loss: -0.0036931633949279785
Batch 7/64 loss: 0.008438408374786377
Batch 8/64 loss: 0.003148496150970459
Batch 9/64 loss: -0.010776400566101074
Batch 10/64 loss: -0.021043479442596436
Batch 11/64 loss: -0.004724740982055664
Batch 12/64 loss: 0.017920315265655518
Batch 13/64 loss: 0.0021430253982543945
Batch 14/64 loss: -0.007660031318664551
Batch 15/64 loss: -0.007489323616027832
Batch 16/64 loss: -0.011944115161895752
Batch 17/64 loss: -0.016531765460968018
Batch 18/64 loss: -0.0020008087158203125
Batch 19/64 loss: 0.000616610050201416
Batch 20/64 loss: -0.007120013236999512
Batch 21/64 loss: 0.0044866204261779785
Batch 22/64 loss: -0.003373265266418457
Batch 23/64 loss: -0.0035468339920043945
Batch 24/64 loss: -0.012025117874145508
Batch 25/64 loss: -0.02030181884765625
Batch 26/64 loss: -0.008468866348266602
Batch 27/64 loss: -0.01505434513092041
Batch 28/64 loss: -0.00216519832611084
Batch 29/64 loss: -0.020035505294799805
Batch 30/64 loss: -0.013470590114593506
Batch 31/64 loss: -0.0039994120597839355
Batch 32/64 loss: -0.01399242877960205
Batch 33/64 loss: -0.014143824577331543
Batch 34/64 loss: 0.005702972412109375
Batch 35/64 loss: -0.003229379653930664
Batch 36/64 loss: -0.006955265998840332
Batch 37/64 loss: -0.0056639909744262695
Batch 38/64 loss: -0.012560009956359863
Batch 39/64 loss: -0.02149862051010132
Batch 40/64 loss: -0.013675987720489502
Batch 41/64 loss: -0.011640369892120361
Batch 42/64 loss: -0.021054983139038086
Batch 43/64 loss: -0.014140307903289795
Batch 44/64 loss: -0.013001322746276855
Batch 45/64 loss: -0.01211613416671753
Batch 46/64 loss: -0.012819111347198486
Batch 47/64 loss: -0.005620598793029785
Batch 48/64 loss: -0.015459299087524414
Batch 49/64 loss: -0.008295416831970215
Batch 50/64 loss: -0.01081174612045288
Batch 51/64 loss: -0.004668176174163818
Batch 52/64 loss: -0.011357545852661133
Batch 53/64 loss: 0.0016795992851257324
Batch 54/64 loss: -0.010740458965301514
Batch 55/64 loss: -0.016093969345092773
Batch 56/64 loss: -0.014104068279266357
Batch 57/64 loss: 0.001999974250793457
Batch 58/64 loss: -0.004056334495544434
Batch 59/64 loss: -0.008681654930114746
Batch 60/64 loss: -0.019992351531982422
Batch 61/64 loss: -0.014631986618041992
Batch 62/64 loss: -0.017128050327301025
Batch 63/64 loss: -0.004090845584869385
Batch 64/64 loss: -0.018389880657196045
Epoch 333  Train loss: -0.007438693560805975  Val loss: 0.08776910546719004
Epoch 334
-------------------------------
Batch 1/64 loss: -0.02006518840789795
Batch 2/64 loss: -0.02674025297164917
Batch 3/64 loss: -9.66787338256836e-05
Batch 4/64 loss: -0.010649502277374268
Batch 5/64 loss: -0.017814576625823975
Batch 6/64 loss: 0.006228625774383545
Batch 7/64 loss: -0.018504738807678223
Batch 8/64 loss: -0.01689326763153076
Batch 9/64 loss: -0.028953135013580322
Batch 10/64 loss: -0.01731044054031372
Batch 11/64 loss: -0.010837197303771973
Batch 12/64 loss: -0.016120970249176025
Batch 13/64 loss: -0.019072532653808594
Batch 14/64 loss: -0.007584214210510254
Batch 15/64 loss: -0.009815514087677002
Batch 16/64 loss: -0.015167236328125
Batch 17/64 loss: -0.012231051921844482
Batch 18/64 loss: -0.014952480792999268
Batch 19/64 loss: -9.006261825561523e-05
Batch 20/64 loss: -0.00868833065032959
Batch 21/64 loss: -0.011508285999298096
Batch 22/64 loss: -0.016359806060791016
Batch 23/64 loss: -0.026886940002441406
Batch 24/64 loss: -0.00191575288772583
Batch 25/64 loss: -0.01278984546661377
Batch 26/64 loss: -0.016868948936462402
Batch 27/64 loss: 0.00820237398147583
Batch 28/64 loss: -0.011505484580993652
Batch 29/64 loss: -0.01110231876373291
Batch 30/64 loss: -0.012056410312652588
Batch 31/64 loss: -0.005582153797149658
Batch 32/64 loss: -0.017808139324188232
Batch 33/64 loss: 0.002943575382232666
Batch 34/64 loss: 0.01425933837890625
Batch 35/64 loss: -0.01089334487915039
Batch 36/64 loss: -0.0242043137550354
Batch 37/64 loss: -0.013030648231506348
Batch 38/64 loss: -0.005637764930725098
Batch 39/64 loss: -0.007750511169433594
Batch 40/64 loss: -0.019881784915924072
Batch 41/64 loss: 0.007026553153991699
Batch 42/64 loss: -0.01593303680419922
Batch 43/64 loss: -0.01529693603515625
Batch 44/64 loss: -0.02285182476043701
Batch 45/64 loss: -0.018087446689605713
Batch 46/64 loss: -0.01839578151702881
Batch 47/64 loss: -0.01480865478515625
Batch 48/64 loss: 0.017530202865600586
Batch 49/64 loss: 0.006740391254425049
Batch 50/64 loss: -0.012559890747070312
Batch 51/64 loss: -0.010617971420288086
Batch 52/64 loss: 0.0007973313331604004
Batch 53/64 loss: -0.014145612716674805
Batch 54/64 loss: -0.013263344764709473
Batch 55/64 loss: -0.0065228939056396484
Batch 56/64 loss: -0.006908774375915527
Batch 57/64 loss: -0.007393777370452881
Batch 58/64 loss: -0.0014274120330810547
Batch 59/64 loss: -0.016834795475006104
Batch 60/64 loss: 0.010837793350219727
Batch 61/64 loss: -0.02889573574066162
Batch 62/64 loss: -0.011133015155792236
Batch 63/64 loss: 0.008964955806732178
Batch 64/64 loss: -0.0230063796043396
Epoch 334  Train loss: -0.010449731349945068  Val loss: 0.09249740961900692
Epoch 335
-------------------------------
Batch 1/64 loss: -0.0017077922821044922
Batch 2/64 loss: -0.009134531021118164
Batch 3/64 loss: -0.008775651454925537
Batch 4/64 loss: 0.011647045612335205
Batch 5/64 loss: -0.010951697826385498
Batch 6/64 loss: 0.006441950798034668
Batch 7/64 loss: -0.014439284801483154
Batch 8/64 loss: -0.0012748241424560547
Batch 9/64 loss: -0.002521216869354248
Batch 10/64 loss: -0.015975892543792725
Batch 11/64 loss: -0.01977384090423584
Batch 12/64 loss: -0.022696852684020996
Batch 13/64 loss: -0.00726550817489624
Batch 14/64 loss: -0.02200186252593994
Batch 15/64 loss: -0.0070740580558776855
Batch 16/64 loss: -0.014820456504821777
Batch 17/64 loss: -0.0012548565864562988
Batch 18/64 loss: 0.0036122798919677734
Batch 19/64 loss: -0.013561606407165527
Batch 20/64 loss: -0.005945920944213867
Batch 21/64 loss: -0.020141124725341797
Batch 22/64 loss: -0.013026833534240723
Batch 23/64 loss: -0.0226629376411438
Batch 24/64 loss: -0.017958223819732666
Batch 25/64 loss: -0.012297570705413818
Batch 26/64 loss: -0.011725187301635742
Batch 27/64 loss: -0.015487134456634521
Batch 28/64 loss: -0.017912745475769043
Batch 29/64 loss: -0.023972928524017334
Batch 30/64 loss: -0.011580705642700195
Batch 31/64 loss: -0.015231311321258545
Batch 32/64 loss: -0.0062108635902404785
Batch 33/64 loss: -0.02004861831665039
Batch 34/64 loss: 0.00036215782165527344
Batch 35/64 loss: -0.019329488277435303
Batch 36/64 loss: -0.0046193599700927734
Batch 37/64 loss: -0.01837027072906494
Batch 38/64 loss: -0.00468289852142334
Batch 39/64 loss: -0.004837214946746826
Batch 40/64 loss: -0.018150806427001953
Batch 41/64 loss: -0.027287185192108154
Batch 42/64 loss: -0.008157730102539062
Batch 43/64 loss: -0.004795372486114502
Batch 44/64 loss: -0.016141057014465332
Batch 45/64 loss: -0.020862162113189697
Batch 46/64 loss: -0.015206694602966309
Batch 47/64 loss: -0.007143735885620117
Batch 48/64 loss: -0.020061254501342773
Batch 49/64 loss: -0.01727747917175293
Batch 50/64 loss: -0.016948580741882324
Batch 51/64 loss: -0.014452993869781494
Batch 52/64 loss: 0.0023238062858581543
Batch 53/64 loss: 0.012275516986846924
Batch 54/64 loss: -0.013192355632781982
Batch 55/64 loss: 0.009307682514190674
Batch 56/64 loss: -0.002439141273498535
Batch 57/64 loss: -0.024289190769195557
Batch 58/64 loss: 0.004642784595489502
Batch 59/64 loss: -0.008983492851257324
Batch 60/64 loss: -0.0013995170593261719
Batch 61/64 loss: 0.007058978080749512
Batch 62/64 loss: -0.010501861572265625
Batch 63/64 loss: 0.006561219692230225
Batch 64/64 loss: -0.007754623889923096
Epoch 335  Train loss: -0.009853245931513169  Val loss: 0.09409766815782003
Epoch 336
-------------------------------
Batch 1/64 loss: 0.013733506202697754
Batch 2/64 loss: -0.011989831924438477
Batch 3/64 loss: -0.017523527145385742
Batch 4/64 loss: -0.005143702030181885
Batch 5/64 loss: -0.001381218433380127
Batch 6/64 loss: -0.0021271705627441406
Batch 7/64 loss: 0.0026662349700927734
Batch 8/64 loss: -0.013371765613555908
Batch 9/64 loss: -0.021171867847442627
Batch 10/64 loss: -0.00038051605224609375
Batch 11/64 loss: -0.003636956214904785
Batch 12/64 loss: -0.015176057815551758
Batch 13/64 loss: -0.01636981964111328
Batch 14/64 loss: -0.009918808937072754
Batch 15/64 loss: -0.022082030773162842
Batch 16/64 loss: -0.007800281047821045
Batch 17/64 loss: -0.009840667247772217
Batch 18/64 loss: 0.004573166370391846
Batch 19/64 loss: -0.002494215965270996
Batch 20/64 loss: -0.017859935760498047
Batch 21/64 loss: -0.006815075874328613
Batch 22/64 loss: -0.003882884979248047
Batch 23/64 loss: -0.015117466449737549
Batch 24/64 loss: 0.0020005106925964355
Batch 25/64 loss: -0.004776656627655029
Batch 26/64 loss: -0.017168402671813965
Batch 27/64 loss: -0.027181267738342285
Batch 28/64 loss: 0.010236501693725586
Batch 29/64 loss: -0.01693117618560791
Batch 30/64 loss: -0.007177472114562988
Batch 31/64 loss: -0.01090472936630249
Batch 32/64 loss: -0.008198559284210205
Batch 33/64 loss: -0.010762274265289307
Batch 34/64 loss: -0.015569329261779785
Batch 35/64 loss: -0.018584609031677246
Batch 36/64 loss: -0.00876682996749878
Batch 37/64 loss: -0.005854904651641846
Batch 38/64 loss: -0.013149380683898926
Batch 39/64 loss: -0.002178192138671875
Batch 40/64 loss: 0.009617924690246582
Batch 41/64 loss: -0.018406391143798828
Batch 42/64 loss: -0.016820907592773438
Batch 43/64 loss: -0.014366447925567627
Batch 44/64 loss: -0.0034455060958862305
Batch 45/64 loss: -0.013870060443878174
Batch 46/64 loss: -0.009642541408538818
Batch 47/64 loss: 0.012202620506286621
Batch 48/64 loss: 0.003985941410064697
Batch 49/64 loss: 0.0008291006088256836
Batch 50/64 loss: -0.018856525421142578
Batch 51/64 loss: -0.007921099662780762
Batch 52/64 loss: -0.007447302341461182
Batch 53/64 loss: 0.00948178768157959
Batch 54/64 loss: -0.005629837512969971
Batch 55/64 loss: 0.0025031566619873047
Batch 56/64 loss: -0.017173349857330322
Batch 57/64 loss: 0.007904589176177979
Batch 58/64 loss: -0.0031018853187561035
Batch 59/64 loss: 0.00558924674987793
Batch 60/64 loss: 0.012541770935058594
Batch 61/64 loss: 2.0742416381835938e-05
Batch 62/64 loss: -0.013555705547332764
Batch 63/64 loss: -0.00555109977722168
Batch 64/64 loss: -0.006642758846282959
Epoch 336  Train loss: -0.006810533532909319  Val loss: 0.09184773271436134
Epoch 337
-------------------------------
Batch 1/64 loss: 0.0023215413093566895
Batch 2/64 loss: -0.00928419828414917
Batch 3/64 loss: 0.004780173301696777
Batch 4/64 loss: -0.006481766700744629
Batch 5/64 loss: -0.008318483829498291
Batch 6/64 loss: -0.004071354866027832
Batch 7/64 loss: -0.010914742946624756
Batch 8/64 loss: -0.007870733737945557
Batch 9/64 loss: -0.01506340503692627
Batch 10/64 loss: -0.0250779390335083
Batch 11/64 loss: -0.013268709182739258
Batch 12/64 loss: -0.004956364631652832
Batch 13/64 loss: 0.0074651241302490234
Batch 14/64 loss: -0.007771849632263184
Batch 15/64 loss: -0.021637439727783203
Batch 16/64 loss: -0.0037487149238586426
Batch 17/64 loss: -0.014507710933685303
Batch 18/64 loss: -0.01679295301437378
Batch 19/64 loss: -0.003646373748779297
Batch 20/64 loss: -0.021345078945159912
Batch 21/64 loss: -0.013467133045196533
Batch 22/64 loss: -0.0198671817779541
Batch 23/64 loss: -0.019503474235534668
Batch 24/64 loss: -0.02726137638092041
Batch 25/64 loss: -0.013395905494689941
Batch 26/64 loss: -0.004046618938446045
Batch 27/64 loss: -0.007368206977844238
Batch 28/64 loss: -0.009060859680175781
Batch 29/64 loss: 0.0014089345932006836
Batch 30/64 loss: -0.008104801177978516
Batch 31/64 loss: -0.012104988098144531
Batch 32/64 loss: -0.004749178886413574
Batch 33/64 loss: -0.005541086196899414
Batch 34/64 loss: -0.009048521518707275
Batch 35/64 loss: -0.01812690496444702
Batch 36/64 loss: -0.004325151443481445
Batch 37/64 loss: -0.012425720691680908
Batch 38/64 loss: 0.0012216567993164062
Batch 39/64 loss: -0.005980372428894043
Batch 40/64 loss: -0.0021806955337524414
Batch 41/64 loss: -0.010500788688659668
Batch 42/64 loss: -0.012462139129638672
Batch 43/64 loss: -0.013761281967163086
Batch 44/64 loss: -0.01805335283279419
Batch 45/64 loss: -0.0009055137634277344
Batch 46/64 loss: -0.017838716506958008
Batch 47/64 loss: -0.014393866062164307
Batch 48/64 loss: -0.023351013660430908
Batch 49/64 loss: -0.01599937677383423
Batch 50/64 loss: -0.0011393427848815918
Batch 51/64 loss: -0.017633438110351562
Batch 52/64 loss: -0.020490825176239014
Batch 53/64 loss: 0.018297672271728516
Batch 54/64 loss: 0.0037160515785217285
Batch 55/64 loss: -0.0015909671783447266
Batch 56/64 loss: -0.010351836681365967
Batch 57/64 loss: -0.01205986738204956
Batch 58/64 loss: -0.0035451650619506836
Batch 59/64 loss: -0.00873422622680664
Batch 60/64 loss: -0.010617196559906006
Batch 61/64 loss: -0.02517223358154297
Batch 62/64 loss: -0.01058042049407959
Batch 63/64 loss: -0.006492733955383301
Batch 64/64 loss: -0.013839006423950195
Epoch 337  Train loss: -0.009696602353862688  Val loss: 0.09163518664763146
Epoch 338
-------------------------------
Batch 1/64 loss: -0.014483392238616943
Batch 2/64 loss: -0.0060843825340271
Batch 3/64 loss: 0.006278872489929199
Batch 4/64 loss: 0.0002148151397705078
Batch 5/64 loss: -0.007794201374053955
Batch 6/64 loss: -0.016772568225860596
Batch 7/64 loss: -0.0035322904586791992
Batch 8/64 loss: -0.010866940021514893
Batch 9/64 loss: -0.007419407367706299
Batch 10/64 loss: 0.010391652584075928
Batch 11/64 loss: -0.003177046775817871
Batch 12/64 loss: -0.01066899299621582
Batch 13/64 loss: -0.026010572910308838
Batch 14/64 loss: -0.010894238948822021
Batch 15/64 loss: -0.00642162561416626
Batch 16/64 loss: -0.02742600440979004
Batch 17/64 loss: -0.010789752006530762
Batch 18/64 loss: -0.01635080575942993
Batch 19/64 loss: -0.018514573574066162
Batch 20/64 loss: -0.007952630519866943
Batch 21/64 loss: -0.01831376552581787
Batch 22/64 loss: -0.021890997886657715
Batch 23/64 loss: -0.021349787712097168
Batch 24/64 loss: -0.023293137550354004
Batch 25/64 loss: -0.005392849445343018
Batch 26/64 loss: -0.019853413105010986
Batch 27/64 loss: -0.019103586673736572
Batch 28/64 loss: -0.02556854486465454
Batch 29/64 loss: -0.016752243041992188
Batch 30/64 loss: -0.010063767433166504
Batch 31/64 loss: -0.019811272621154785
Batch 32/64 loss: -0.01142740249633789
Batch 33/64 loss: -0.02030038833618164
Batch 34/64 loss: -0.0022949576377868652
Batch 35/64 loss: -0.013837754726409912
Batch 36/64 loss: -0.019526541233062744
Batch 37/64 loss: -0.005857229232788086
Batch 38/64 loss: 0.009484171867370605
Batch 39/64 loss: -0.021073579788208008
Batch 40/64 loss: -0.014191031455993652
Batch 41/64 loss: -0.014166057109832764
Batch 42/64 loss: -0.011844396591186523
Batch 43/64 loss: -0.026555359363555908
Batch 44/64 loss: -0.0038083791732788086
Batch 45/64 loss: -0.03039795160293579
Batch 46/64 loss: -0.008461952209472656
Batch 47/64 loss: -0.011902570724487305
Batch 48/64 loss: -0.019283771514892578
Batch 49/64 loss: -0.009544730186462402
Batch 50/64 loss: -0.005928933620452881
Batch 51/64 loss: 0.0030400753021240234
Batch 52/64 loss: -0.019083261489868164
Batch 53/64 loss: -0.012025952339172363
Batch 54/64 loss: -0.018386662006378174
Batch 55/64 loss: -0.014180898666381836
Batch 56/64 loss: -0.001059114933013916
Batch 57/64 loss: -0.0009542703628540039
Batch 58/64 loss: -0.005766391754150391
Batch 59/64 loss: -0.020893335342407227
Batch 60/64 loss: -0.002454221248626709
Batch 61/64 loss: -0.004375040531158447
Batch 62/64 loss: -0.006914198398590088
Batch 63/64 loss: -0.022804856300354004
Batch 64/64 loss: -0.013320863246917725
Epoch 338  Train loss: -0.012022510463116216  Val loss: 0.09178867352377508
Epoch 339
-------------------------------
Batch 1/64 loss: -0.007431626319885254
Batch 2/64 loss: -0.009862184524536133
Batch 3/64 loss: -0.007243812084197998
Batch 4/64 loss: -0.01405256986618042
Batch 5/64 loss: -0.022274911403656006
Batch 6/64 loss: -0.001156628131866455
Batch 7/64 loss: -0.020790815353393555
Batch 8/64 loss: -0.013505220413208008
Batch 9/64 loss: -0.01802140474319458
Batch 10/64 loss: -0.008437514305114746
Batch 11/64 loss: -0.006196260452270508
Batch 12/64 loss: -0.019987821578979492
Batch 13/64 loss: -0.00793600082397461
Batch 14/64 loss: -0.026082634925842285
Batch 15/64 loss: -0.01817154884338379
Batch 16/64 loss: -0.017792701721191406
Batch 17/64 loss: -0.01547706127166748
Batch 18/64 loss: -0.019481778144836426
Batch 19/64 loss: -0.02019047737121582
Batch 20/64 loss: -0.028794825077056885
Batch 21/64 loss: -0.02662593126296997
Batch 22/64 loss: -0.011519670486450195
Batch 23/64 loss: -0.011851072311401367
Batch 24/64 loss: -0.021049201488494873
Batch 25/64 loss: -0.02129143476486206
Batch 26/64 loss: -0.004351615905761719
Batch 27/64 loss: -0.02900993824005127
Batch 28/64 loss: -0.010757625102996826
Batch 29/64 loss: 0.004571318626403809
Batch 30/64 loss: -0.014053761959075928
Batch 31/64 loss: -0.026919245719909668
Batch 32/64 loss: -0.014811873435974121
Batch 33/64 loss: -0.01201319694519043
Batch 34/64 loss: -0.01651865243911743
Batch 35/64 loss: -0.008177757263183594
Batch 36/64 loss: -0.025131583213806152
Batch 37/64 loss: -0.01766371726989746
Batch 38/64 loss: -0.02328181266784668
Batch 39/64 loss: 0.002943098545074463
Batch 40/64 loss: -0.012990891933441162
Batch 41/64 loss: 0.0011984705924987793
Batch 42/64 loss: -0.002643883228302002
Batch 43/64 loss: -0.001706242561340332
Batch 44/64 loss: 0.0019297003746032715
Batch 45/64 loss: -0.017788827419281006
Batch 46/64 loss: 0.009333193302154541
Batch 47/64 loss: -0.011359095573425293
Batch 48/64 loss: -0.004722714424133301
Batch 49/64 loss: -0.012820005416870117
Batch 50/64 loss: -0.01979362964630127
Batch 51/64 loss: 0.00506967306137085
Batch 52/64 loss: 0.007427990436553955
Batch 53/64 loss: -0.008611679077148438
Batch 54/64 loss: -0.0168914794921875
Batch 55/64 loss: -0.00896531343460083
Batch 56/64 loss: -0.014207780361175537
Batch 57/64 loss: -0.012653768062591553
Batch 58/64 loss: -0.00455784797668457
Batch 59/64 loss: -0.01530313491821289
Batch 60/64 loss: -0.015419721603393555
Batch 61/64 loss: -0.004573225975036621
Batch 62/64 loss: -0.02322453260421753
Batch 63/64 loss: -0.003212571144104004
Batch 64/64 loss: -0.002507030963897705
Epoch 339  Train loss: -0.012215985270107494  Val loss: 0.0898226421723251
Epoch 340
-------------------------------
Batch 1/64 loss: -0.017396628856658936
Batch 2/64 loss: -0.009959220886230469
Batch 3/64 loss: -0.00916379690170288
Batch 4/64 loss: -0.007471680641174316
Batch 5/64 loss: -0.020796537399291992
Batch 6/64 loss: -0.016581475734710693
Batch 7/64 loss: -0.005443096160888672
Batch 8/64 loss: -0.018672585487365723
Batch 9/64 loss: -0.02118861675262451
Batch 10/64 loss: -0.03295016288757324
Batch 11/64 loss: -0.01178675889968872
Batch 12/64 loss: -0.01225501298904419
Batch 13/64 loss: -0.02848029136657715
Batch 14/64 loss: -0.01517951488494873
Batch 15/64 loss: -0.0018473267555236816
Batch 16/64 loss: -0.013343095779418945
Batch 17/64 loss: -0.004192709922790527
Batch 18/64 loss: -0.014393448829650879
Batch 19/64 loss: -0.013859748840332031
Batch 20/64 loss: -0.01897454261779785
Batch 21/64 loss: -0.005934417247772217
Batch 22/64 loss: 0.008104860782623291
Batch 23/64 loss: -0.024114906787872314
Batch 24/64 loss: -0.008419632911682129
Batch 25/64 loss: -0.020956039428710938
Batch 26/64 loss: -0.006776690483093262
Batch 27/64 loss: -0.021514952182769775
Batch 28/64 loss: 0.004015803337097168
Batch 29/64 loss: -0.009784698486328125
Batch 30/64 loss: -0.022163808345794678
Batch 31/64 loss: -0.012972235679626465
Batch 32/64 loss: -0.0093575119972229
Batch 33/64 loss: -0.011010885238647461
Batch 34/64 loss: -0.015732526779174805
Batch 35/64 loss: -0.005376458168029785
Batch 36/64 loss: -0.0004851222038269043
Batch 37/64 loss: -0.0031626224517822266
Batch 38/64 loss: -0.014883577823638916
Batch 39/64 loss: -0.01092374324798584
Batch 40/64 loss: -0.003579556941986084
Batch 41/64 loss: -0.0326998233795166
Batch 42/64 loss: 0.0031059980392456055
Batch 43/64 loss: -0.012816667556762695
Batch 44/64 loss: -0.009072601795196533
Batch 45/64 loss: -0.00367128849029541
Batch 46/64 loss: -0.005961239337921143
Batch 47/64 loss: -0.001627206802368164
Batch 48/64 loss: 0.0033544301986694336
Batch 49/64 loss: -0.0056122541427612305
Batch 50/64 loss: -0.018076777458190918
Batch 51/64 loss: -0.021275579929351807
Batch 52/64 loss: -0.008663356304168701
Batch 53/64 loss: -0.02107161283493042
Batch 54/64 loss: -0.010380089282989502
Batch 55/64 loss: -0.02038019895553589
Batch 56/64 loss: -0.00019884109497070312
Batch 57/64 loss: -0.029850244522094727
Batch 58/64 loss: -0.0025667548179626465
Batch 59/64 loss: -0.0012689828872680664
Batch 60/64 loss: -0.017450690269470215
Batch 61/64 loss: -0.017192721366882324
Batch 62/64 loss: -0.016328394412994385
Batch 63/64 loss: -0.019693851470947266
Batch 64/64 loss: -0.004323363304138184
Epoch 340  Train loss: -0.011946764646791945  Val loss: 0.09078947764491707
Epoch 341
-------------------------------
Batch 1/64 loss: -0.0053157806396484375
Batch 2/64 loss: -0.014109194278717041
Batch 3/64 loss: 0.010867178440093994
Batch 4/64 loss: -0.013973116874694824
Batch 5/64 loss: -0.02288341522216797
Batch 6/64 loss: -0.017330288887023926
Batch 7/64 loss: 0.0040566325187683105
Batch 8/64 loss: -0.011977672576904297
Batch 9/64 loss: -0.007975876331329346
Batch 10/64 loss: -0.007444143295288086
Batch 11/64 loss: -0.01589345932006836
Batch 12/64 loss: -0.02150547504425049
Batch 13/64 loss: -0.01790708303451538
Batch 14/64 loss: -0.014549791812896729
Batch 15/64 loss: -0.017888903617858887
Batch 16/64 loss: -0.012111246585845947
Batch 17/64 loss: -0.019519925117492676
Batch 18/64 loss: -0.0077904462814331055
Batch 19/64 loss: -0.008257806301116943
Batch 20/64 loss: -0.019060909748077393
Batch 21/64 loss: -0.024262428283691406
Batch 22/64 loss: -0.023527145385742188
Batch 23/64 loss: -0.0026903748512268066
Batch 24/64 loss: -0.02118140459060669
Batch 25/64 loss: -0.011330485343933105
Batch 26/64 loss: 0.00515294075012207
Batch 27/64 loss: -0.0010097622871398926
Batch 28/64 loss: -0.015427172183990479
Batch 29/64 loss: -0.017266273498535156
Batch 30/64 loss: -0.02381843328475952
Batch 31/64 loss: -0.022316038608551025
Batch 32/64 loss: -0.011469602584838867
Batch 33/64 loss: 0.0009348392486572266
Batch 34/64 loss: -0.01964670419692993
Batch 35/64 loss: -0.006256818771362305
Batch 36/64 loss: -0.014056265354156494
Batch 37/64 loss: 0.0218544602394104
Batch 38/64 loss: -0.018604755401611328
Batch 39/64 loss: -0.009292006492614746
Batch 40/64 loss: -0.0065648555755615234
Batch 41/64 loss: 0.00016897916793823242
Batch 42/64 loss: -0.02659386396408081
Batch 43/64 loss: -0.003823399543762207
Batch 44/64 loss: -0.027290761470794678
Batch 45/64 loss: -0.017444729804992676
Batch 46/64 loss: -0.00904238224029541
Batch 47/64 loss: -0.005057334899902344
Batch 48/64 loss: -0.0098915696144104
Batch 49/64 loss: 0.015109002590179443
Batch 50/64 loss: -0.00984346866607666
Batch 51/64 loss: -0.008091092109680176
Batch 52/64 loss: -0.0037726759910583496
Batch 53/64 loss: -0.006012141704559326
Batch 54/64 loss: -0.0184171199798584
Batch 55/64 loss: -0.01567322015762329
Batch 56/64 loss: -0.005627155303955078
Batch 57/64 loss: 0.005254924297332764
Batch 58/64 loss: -0.015112638473510742
Batch 59/64 loss: -0.011179029941558838
Batch 60/64 loss: -0.02024245262145996
Batch 61/64 loss: 0.0007959604263305664
Batch 62/64 loss: -0.006733894348144531
Batch 63/64 loss: -0.011208951473236084
Batch 64/64 loss: -9.715557098388672e-06
Epoch 341  Train loss: -0.010526828438627954  Val loss: 0.09105752423866507
Epoch 342
-------------------------------
Batch 1/64 loss: 0.0076751708984375
Batch 2/64 loss: -0.02216780185699463
Batch 3/64 loss: -0.008923470973968506
Batch 4/64 loss: -0.011748731136322021
Batch 5/64 loss: -0.004273355007171631
Batch 6/64 loss: -0.015073895454406738
Batch 7/64 loss: -0.015616357326507568
Batch 8/64 loss: -0.017635881900787354
Batch 9/64 loss: -0.009769022464752197
Batch 10/64 loss: -0.015871763229370117
Batch 11/64 loss: -0.022440195083618164
Batch 12/64 loss: -0.009648680686950684
Batch 13/64 loss: -0.005883216857910156
Batch 14/64 loss: -0.014619708061218262
Batch 15/64 loss: -0.013433456420898438
Batch 16/64 loss: 0.014972507953643799
Batch 17/64 loss: -0.026972591876983643
Batch 18/64 loss: 0.0035286545753479004
Batch 19/64 loss: -0.02284836769104004
Batch 20/64 loss: -0.007595658302307129
Batch 21/64 loss: -0.005639076232910156
Batch 22/64 loss: -0.016666293144226074
Batch 23/64 loss: -0.016611576080322266
Batch 24/64 loss: -0.01872539520263672
Batch 25/64 loss: -0.031970202922821045
Batch 26/64 loss: -0.020582735538482666
Batch 27/64 loss: -0.01185840368270874
Batch 28/64 loss: -0.012532532215118408
Batch 29/64 loss: -0.011746346950531006
Batch 30/64 loss: -0.004242599010467529
Batch 31/64 loss: -0.01033163070678711
Batch 32/64 loss: -0.015341341495513916
Batch 33/64 loss: -0.0052492618560791016
Batch 34/64 loss: -0.014892518520355225
Batch 35/64 loss: -0.020294129848480225
Batch 36/64 loss: -0.017816245555877686
Batch 37/64 loss: 0.0018408894538879395
Batch 38/64 loss: -0.012846887111663818
Batch 39/64 loss: -0.01346886157989502
Batch 40/64 loss: -0.008958101272583008
Batch 41/64 loss: -0.006319642066955566
Batch 42/64 loss: 0.0002205967903137207
Batch 43/64 loss: -0.031804561614990234
Batch 44/64 loss: -0.011629164218902588
Batch 45/64 loss: -0.0013921856880187988
Batch 46/64 loss: -0.01894199848175049
Batch 47/64 loss: -0.017559170722961426
Batch 48/64 loss: -0.012176334857940674
Batch 49/64 loss: -0.030524075031280518
Batch 50/64 loss: -0.002392709255218506
Batch 51/64 loss: -0.01957803964614868
Batch 52/64 loss: -0.011481881141662598
Batch 53/64 loss: -0.009654998779296875
Batch 54/64 loss: -0.01590108871459961
Batch 55/64 loss: 0.007305145263671875
Batch 56/64 loss: 0.008938372135162354
Batch 57/64 loss: -0.004795432090759277
Batch 58/64 loss: 0.007772326469421387
Batch 59/64 loss: -0.013724565505981445
Batch 60/64 loss: -0.00513070821762085
Batch 61/64 loss: 0.002820730209350586
Batch 62/64 loss: -0.003033578395843506
Batch 63/64 loss: -0.002468585968017578
Batch 64/64 loss: -0.0010452866554260254
Epoch 342  Train loss: -0.010643366037630567  Val loss: 0.08934458178752885
Epoch 343
-------------------------------
Batch 1/64 loss: -0.001225292682647705
Batch 2/64 loss: -0.004367053508758545
Batch 3/64 loss: -0.022085130214691162
Batch 4/64 loss: -0.011996567249298096
Batch 5/64 loss: -0.01994842290878296
Batch 6/64 loss: -0.022883296012878418
Batch 7/64 loss: 0.0032430291175842285
Batch 8/64 loss: -0.009569048881530762
Batch 9/64 loss: -0.016739845275878906
Batch 10/64 loss: -0.020633578300476074
Batch 11/64 loss: -0.020126283168792725
Batch 12/64 loss: -0.00878077745437622
Batch 13/64 loss: -0.010220706462860107
Batch 14/64 loss: -0.0268743634223938
Batch 15/64 loss: -0.023557722568511963
Batch 16/64 loss: -0.017757773399353027
Batch 17/64 loss: -0.02332472801208496
Batch 18/64 loss: 0.007569253444671631
Batch 19/64 loss: -0.012499809265136719
Batch 20/64 loss: 0.004368126392364502
Batch 21/64 loss: -0.020964503288269043
Batch 22/64 loss: -0.021145224571228027
Batch 23/64 loss: -0.014531195163726807
Batch 24/64 loss: 0.007653951644897461
Batch 25/64 loss: -0.006060957908630371
Batch 26/64 loss: -0.01367795467376709
Batch 27/64 loss: -0.008552253246307373
Batch 28/64 loss: -0.019948840141296387
Batch 29/64 loss: -0.008621037006378174
Batch 30/64 loss: -0.014585554599761963
Batch 31/64 loss: 0.00021028518676757812
Batch 32/64 loss: -0.009173035621643066
Batch 33/64 loss: -0.00899428129196167
Batch 34/64 loss: -0.01640796661376953
Batch 35/64 loss: -0.007462561130523682
Batch 36/64 loss: -0.014461994171142578
Batch 37/64 loss: -0.0030345916748046875
Batch 38/64 loss: -0.006967782974243164
Batch 39/64 loss: -0.012254118919372559
Batch 40/64 loss: -0.014484405517578125
Batch 41/64 loss: -0.01586329936981201
Batch 42/64 loss: -0.023295462131500244
Batch 43/64 loss: -0.014544308185577393
Batch 44/64 loss: 0.027976036071777344
Batch 45/64 loss: -0.01938939094543457
Batch 46/64 loss: -0.005735814571380615
Batch 47/64 loss: 0.023264765739440918
Batch 48/64 loss: -0.013118743896484375
Batch 49/64 loss: -0.0011135339736938477
Batch 50/64 loss: -0.014239907264709473
Batch 51/64 loss: -0.004365682601928711
Batch 52/64 loss: -0.014498770236968994
Batch 53/64 loss: -0.0073664188385009766
Batch 54/64 loss: 0.0031589865684509277
Batch 55/64 loss: 0.0024206042289733887
Batch 56/64 loss: -0.00829768180847168
Batch 57/64 loss: -0.006925463676452637
Batch 58/64 loss: 0.009748101234436035
Batch 59/64 loss: -0.00530397891998291
Batch 60/64 loss: 0.005048632621765137
Batch 61/64 loss: -0.015895366668701172
Batch 62/64 loss: -0.010918617248535156
Batch 63/64 loss: -0.0017890334129333496
Batch 64/64 loss: -0.010415077209472656
Epoch 343  Train loss: -0.009250661438586666  Val loss: 0.09238582992881435
Epoch 344
-------------------------------
Batch 1/64 loss: -0.020495057106018066
Batch 2/64 loss: -0.017355740070343018
Batch 3/64 loss: -0.024989724159240723
Batch 4/64 loss: 0.00447160005569458
Batch 5/64 loss: -0.01000910997390747
Batch 6/64 loss: 0.0054090023040771484
Batch 7/64 loss: -0.019271671772003174
Batch 8/64 loss: -0.020598948001861572
Batch 9/64 loss: -0.0073084235191345215
Batch 10/64 loss: 0.004439115524291992
Batch 11/64 loss: -0.008340716361999512
Batch 12/64 loss: -0.005758106708526611
Batch 13/64 loss: -0.016026854515075684
Batch 14/64 loss: -0.0004082322120666504
Batch 15/64 loss: 0.011241376399993896
Batch 16/64 loss: -0.007498323917388916
Batch 17/64 loss: -0.007401406764984131
Batch 18/64 loss: 0.008411705493927002
Batch 19/64 loss: -0.02097010612487793
Batch 20/64 loss: -0.015704751014709473
Batch 21/64 loss: -0.023757338523864746
Batch 22/64 loss: -0.013266444206237793
Batch 23/64 loss: -0.014103889465332031
Batch 24/64 loss: -0.02142918109893799
Batch 25/64 loss: -0.018479585647583008
Batch 26/64 loss: -0.012784957885742188
Batch 27/64 loss: -0.02657628059387207
Batch 28/64 loss: -0.022499799728393555
Batch 29/64 loss: -0.014988601207733154
Batch 30/64 loss: -0.017044126987457275
Batch 31/64 loss: -0.003137528896331787
Batch 32/64 loss: -0.014999866485595703
Batch 33/64 loss: -0.024465620517730713
Batch 34/64 loss: -0.013911545276641846
Batch 35/64 loss: -0.007921814918518066
Batch 36/64 loss: 0.00559079647064209
Batch 37/64 loss: -0.013174951076507568
Batch 38/64 loss: -0.010313153266906738
Batch 39/64 loss: -0.008233070373535156
Batch 40/64 loss: -0.006856858730316162
Batch 41/64 loss: 0.005217909812927246
Batch 42/64 loss: -0.010660409927368164
Batch 43/64 loss: -0.014195024967193604
Batch 44/64 loss: -0.003388345241546631
Batch 45/64 loss: 0.009748399257659912
Batch 46/64 loss: -0.0198858380317688
Batch 47/64 loss: -0.007828593254089355
Batch 48/64 loss: -0.008451998233795166
Batch 49/64 loss: -0.010944366455078125
Batch 50/64 loss: 0.005997002124786377
Batch 51/64 loss: -0.013208866119384766
Batch 52/64 loss: 0.01093214750289917
Batch 53/64 loss: -0.013871252536773682
Batch 54/64 loss: -0.009106278419494629
Batch 55/64 loss: -0.025007307529449463
Batch 56/64 loss: -0.006419777870178223
Batch 57/64 loss: -0.01726001501083374
Batch 58/64 loss: -0.007304668426513672
Batch 59/64 loss: -0.011461615562438965
Batch 60/64 loss: -0.007157087326049805
Batch 61/64 loss: -0.017404139041900635
Batch 62/64 loss: -0.016360759735107422
Batch 63/64 loss: -0.028976917266845703
Batch 64/64 loss: -0.01934570074081421
Epoch 344  Train loss: -0.010698435587041519  Val loss: 0.09053106426783034
Epoch 345
-------------------------------
Batch 1/64 loss: -0.030952751636505127
Batch 2/64 loss: -0.028829514980316162
Batch 3/64 loss: -0.02673184871673584
Batch 4/64 loss: -0.016980230808258057
Batch 5/64 loss: -0.007993876934051514
Batch 6/64 loss: -0.022266089916229248
Batch 7/64 loss: -0.020953774452209473
Batch 8/64 loss: -0.021046698093414307
Batch 9/64 loss: -0.014671683311462402
Batch 10/64 loss: -0.00010216236114501953
Batch 11/64 loss: -0.020133137702941895
Batch 12/64 loss: -0.023476898670196533
Batch 13/64 loss: -0.011462032794952393
Batch 14/64 loss: -0.011426329612731934
Batch 15/64 loss: -0.008875072002410889
Batch 16/64 loss: -0.009073138236999512
Batch 17/64 loss: -0.010894715785980225
Batch 18/64 loss: 0.0169827938079834
Batch 19/64 loss: -0.012662172317504883
Batch 20/64 loss: -0.003707408905029297
Batch 21/64 loss: 0.007938683032989502
Batch 22/64 loss: -0.006521344184875488
Batch 23/64 loss: -0.0113525390625
Batch 24/64 loss: 0.0016901493072509766
Batch 25/64 loss: -0.003060579299926758
Batch 26/64 loss: -0.016006112098693848
Batch 27/64 loss: -0.015092015266418457
Batch 28/64 loss: -0.003856062889099121
Batch 29/64 loss: -0.0040169358253479
Batch 30/64 loss: -0.005013585090637207
Batch 31/64 loss: -0.014820218086242676
Batch 32/64 loss: -0.007749974727630615
Batch 33/64 loss: -0.006218373775482178
Batch 34/64 loss: -0.006269574165344238
Batch 35/64 loss: -0.01712977886199951
Batch 36/64 loss: -0.018328368663787842
Batch 37/64 loss: 0.005376279354095459
Batch 38/64 loss: -0.005120813846588135
Batch 39/64 loss: -0.015705406665802002
Batch 40/64 loss: -0.02208554744720459
Batch 41/64 loss: -0.01899874210357666
Batch 42/64 loss: -0.006972193717956543
Batch 43/64 loss: -0.005245685577392578
Batch 44/64 loss: 0.015032291412353516
Batch 45/64 loss: -0.02054762840270996
Batch 46/64 loss: -0.01913917064666748
Batch 47/64 loss: -0.018680036067962646
Batch 48/64 loss: -0.011158466339111328
Batch 49/64 loss: -0.013355851173400879
Batch 50/64 loss: -0.013103008270263672
Batch 51/64 loss: -0.011146068572998047
Batch 52/64 loss: -0.008238077163696289
Batch 53/64 loss: 0.0024353861808776855
Batch 54/64 loss: -0.017902016639709473
Batch 55/64 loss: -0.009129106998443604
Batch 56/64 loss: -0.02776426076889038
Batch 57/64 loss: -0.010053396224975586
Batch 58/64 loss: -0.012420117855072021
Batch 59/64 loss: -0.013415336608886719
Batch 60/64 loss: -0.004359722137451172
Batch 61/64 loss: -0.005010247230529785
Batch 62/64 loss: 0.009754955768585205
Batch 63/64 loss: 0.0057830810546875
Batch 64/64 loss: -0.011194586753845215
Epoch 345  Train loss: -0.010519658350477032  Val loss: 0.09031303969445507
Epoch 346
-------------------------------
Batch 1/64 loss: -0.012909293174743652
Batch 2/64 loss: -0.027656078338623047
Batch 3/64 loss: -0.017335057258605957
Batch 4/64 loss: -0.018300354480743408
Batch 5/64 loss: -0.030042171478271484
Batch 6/64 loss: -0.013889431953430176
Batch 7/64 loss: -0.024775981903076172
Batch 8/64 loss: -0.014912605285644531
Batch 9/64 loss: -0.018691062927246094
Batch 10/64 loss: -0.01613229513168335
Batch 11/64 loss: -0.01694422960281372
Batch 12/64 loss: -0.023037374019622803
Batch 13/64 loss: -0.017469823360443115
Batch 14/64 loss: -0.012632131576538086
Batch 15/64 loss: -0.018789708614349365
Batch 16/64 loss: -0.003118753433227539
Batch 17/64 loss: -0.024003267288208008
Batch 18/64 loss: -0.024271845817565918
Batch 19/64 loss: -0.018378794193267822
Batch 20/64 loss: -0.02137279510498047
Batch 21/64 loss: -0.0075234174728393555
Batch 22/64 loss: -0.015987038612365723
Batch 23/64 loss: -0.015460491180419922
Batch 24/64 loss: -0.015535831451416016
Batch 25/64 loss: -0.011559665203094482
Batch 26/64 loss: -0.007981419563293457
Batch 27/64 loss: -0.017610013484954834
Batch 28/64 loss: -0.012561202049255371
Batch 29/64 loss: -5.614757537841797e-05
Batch 30/64 loss: -0.009695291519165039
Batch 31/64 loss: -0.006916224956512451
Batch 32/64 loss: -0.00836247205734253
Batch 33/64 loss: -0.023823440074920654
Batch 34/64 loss: -0.009499967098236084
Batch 35/64 loss: -0.00020956993103027344
Batch 36/64 loss: -0.017272531986236572
Batch 37/64 loss: -0.020523011684417725
Batch 38/64 loss: -0.007169842720031738
Batch 39/64 loss: -0.016763925552368164
Batch 40/64 loss: -0.015021920204162598
Batch 41/64 loss: -0.009762167930603027
Batch 42/64 loss: -0.022475063800811768
Batch 43/64 loss: -0.023439764976501465
Batch 44/64 loss: -0.0013312697410583496
Batch 45/64 loss: -0.002611994743347168
Batch 46/64 loss: -0.012081623077392578
Batch 47/64 loss: -0.01287907361984253
Batch 48/64 loss: -0.020197510719299316
Batch 49/64 loss: 0.0024753212928771973
Batch 50/64 loss: -0.016566693782806396
Batch 51/64 loss: -0.015151262283325195
Batch 52/64 loss: 0.00509035587310791
Batch 53/64 loss: -0.020664334297180176
Batch 54/64 loss: -0.018511056900024414
Batch 55/64 loss: -0.005621016025543213
Batch 56/64 loss: -0.008866071701049805
Batch 57/64 loss: -0.009945929050445557
Batch 58/64 loss: -0.01726830005645752
Batch 59/64 loss: -0.016602635383605957
Batch 60/64 loss: -0.023424804210662842
Batch 61/64 loss: -0.008439600467681885
Batch 62/64 loss: -0.007714986801147461
Batch 63/64 loss: -0.01036924123764038
Batch 64/64 loss: -0.002908170223236084
Epoch 346  Train loss: -0.01400370714711208  Val loss: 0.09059273500213098
Epoch 347
-------------------------------
Batch 1/64 loss: -0.02443253993988037
Batch 2/64 loss: -0.00993269681930542
Batch 3/64 loss: -0.019478440284729004
Batch 4/64 loss: -0.00927436351776123
Batch 5/64 loss: -0.02435213327407837
Batch 6/64 loss: -0.012383580207824707
Batch 7/64 loss: -0.013946175575256348
Batch 8/64 loss: -0.0048781633377075195
Batch 9/64 loss: -0.006788730621337891
Batch 10/64 loss: -0.020325660705566406
Batch 11/64 loss: -0.022663235664367676
Batch 12/64 loss: -0.02307027578353882
Batch 13/64 loss: -0.01890474557876587
Batch 14/64 loss: -0.028440475463867188
Batch 15/64 loss: 0.01590120792388916
Batch 16/64 loss: -0.008858680725097656
Batch 17/64 loss: -0.024818122386932373
Batch 18/64 loss: -0.0076912641525268555
Batch 19/64 loss: -0.01171332597732544
Batch 20/64 loss: -0.013409435749053955
Batch 21/64 loss: -0.00786358118057251
Batch 22/64 loss: -0.004041850566864014
Batch 23/64 loss: -0.019740939140319824
Batch 24/64 loss: -0.007876813411712646
Batch 25/64 loss: -0.017017006874084473
Batch 26/64 loss: -0.014451026916503906
Batch 27/64 loss: -0.00806969404220581
Batch 28/64 loss: -0.01261293888092041
Batch 29/64 loss: -0.018850505352020264
Batch 30/64 loss: -0.022854745388031006
Batch 31/64 loss: -0.006017029285430908
Batch 32/64 loss: -0.008425235748291016
Batch 33/64 loss: -0.0034865140914916992
Batch 34/64 loss: -0.0042285919189453125
Batch 35/64 loss: -0.021353185176849365
Batch 36/64 loss: -0.023478806018829346
Batch 37/64 loss: -0.022698581218719482
Batch 38/64 loss: -0.013148188591003418
Batch 39/64 loss: -0.020610332489013672
Batch 40/64 loss: -0.002474546432495117
Batch 41/64 loss: -0.024364471435546875
Batch 42/64 loss: -0.009584665298461914
Batch 43/64 loss: -0.02137172222137451
Batch 44/64 loss: 0.0076277852058410645
Batch 45/64 loss: -0.022722303867340088
Batch 46/64 loss: -0.007365882396697998
Batch 47/64 loss: -0.01937568187713623
Batch 48/64 loss: -0.012399852275848389
Batch 49/64 loss: -0.014772653579711914
Batch 50/64 loss: -0.00880509614944458
Batch 51/64 loss: -0.01776254177093506
Batch 52/64 loss: -0.011998176574707031
Batch 53/64 loss: -0.006022214889526367
Batch 54/64 loss: 0.0011508464813232422
Batch 55/64 loss: -0.022773265838623047
Batch 56/64 loss: -0.02579033374786377
Batch 57/64 loss: -0.018358349800109863
Batch 58/64 loss: -0.01668161153793335
Batch 59/64 loss: -0.016300439834594727
Batch 60/64 loss: -0.0011296868324279785
Batch 61/64 loss: -0.019848942756652832
Batch 62/64 loss: -0.03169369697570801
Batch 63/64 loss: -0.004584252834320068
Batch 64/64 loss: -0.01741969585418701
Epoch 347  Train loss: -0.013909849933549469  Val loss: 0.09054839426709205
Epoch 348
-------------------------------
Batch 1/64 loss: -0.01025247573852539
Batch 2/64 loss: -0.01998192071914673
Batch 3/64 loss: -0.022610843181610107
Batch 4/64 loss: -0.006952643394470215
Batch 5/64 loss: -0.037357449531555176
Batch 6/64 loss: -0.021190166473388672
Batch 7/64 loss: -0.01907259225845337
Batch 8/64 loss: -0.023832499980926514
Batch 9/64 loss: -0.016634106636047363
Batch 10/64 loss: -0.012407839298248291
Batch 11/64 loss: -0.006218552589416504
Batch 12/64 loss: -0.011004507541656494
Batch 13/64 loss: -0.005052924156188965
Batch 14/64 loss: -0.027608871459960938
Batch 15/64 loss: -0.01570892333984375
Batch 16/64 loss: -0.016767680644989014
Batch 17/64 loss: -0.014336347579956055
Batch 18/64 loss: -0.004462301731109619
Batch 19/64 loss: -0.002802252769470215
Batch 20/64 loss: -0.016964077949523926
Batch 21/64 loss: -0.018407583236694336
Batch 22/64 loss: -0.01626908779144287
Batch 23/64 loss: -0.00932455062866211
Batch 24/64 loss: -0.00702899694442749
Batch 25/64 loss: 0.0036240816116333008
Batch 26/64 loss: 0.008895158767700195
Batch 27/64 loss: -0.02863156795501709
Batch 28/64 loss: -0.027632474899291992
Batch 29/64 loss: -0.016316652297973633
Batch 30/64 loss: -0.01023101806640625
Batch 31/64 loss: -0.02875077724456787
Batch 32/64 loss: -0.0038633346557617188
Batch 33/64 loss: -0.006527841091156006
Batch 34/64 loss: -0.008408963680267334
Batch 35/64 loss: -0.019544124603271484
Batch 36/64 loss: -0.01763051748275757
Batch 37/64 loss: -0.017567157745361328
Batch 38/64 loss: -0.027020037174224854
Batch 39/64 loss: 0.002267301082611084
Batch 40/64 loss: -0.006210267543792725
Batch 41/64 loss: -0.014795958995819092
Batch 42/64 loss: -0.024122953414916992
Batch 43/64 loss: -0.01461029052734375
Batch 44/64 loss: -0.011692404747009277
Batch 45/64 loss: -0.009534358978271484
Batch 46/64 loss: -0.003895103931427002
Batch 47/64 loss: -0.019141674041748047
Batch 48/64 loss: -0.011288583278656006
Batch 49/64 loss: -0.014189720153808594
Batch 50/64 loss: -0.013594210147857666
Batch 51/64 loss: -0.009447157382965088
Batch 52/64 loss: -0.02082502841949463
Batch 53/64 loss: -0.009737014770507812
Batch 54/64 loss: -0.019477009773254395
Batch 55/64 loss: -0.011477470397949219
Batch 56/64 loss: -0.02177196741104126
Batch 57/64 loss: -0.028325438499450684
Batch 58/64 loss: -0.003809988498687744
Batch 59/64 loss: 0.002984464168548584
Batch 60/64 loss: -0.018789470195770264
Batch 61/64 loss: -0.013427972793579102
Batch 62/64 loss: -0.00022268295288085938
Batch 63/64 loss: -0.011508822441101074
Batch 64/64 loss: -0.013721704483032227
Epoch 348  Train loss: -0.01378493309020996  Val loss: 0.09121527868447844
Epoch 349
-------------------------------
Batch 1/64 loss: -0.021272003650665283
Batch 2/64 loss: -0.006975412368774414
Batch 3/64 loss: -0.01425778865814209
Batch 4/64 loss: -0.030598163604736328
Batch 5/64 loss: -0.029754161834716797
Batch 6/64 loss: -0.006213486194610596
Batch 7/64 loss: -0.020070314407348633
Batch 8/64 loss: -0.01980876922607422
Batch 9/64 loss: -0.02653902769088745
Batch 10/64 loss: -0.01995903253555298
Batch 11/64 loss: -0.009135007858276367
Batch 12/64 loss: 0.002932906150817871
Batch 13/64 loss: -0.025836706161499023
Batch 14/64 loss: -0.02050912380218506
Batch 15/64 loss: -0.015837013721466064
Batch 16/64 loss: -0.009734749794006348
Batch 17/64 loss: -0.02801334857940674
Batch 18/64 loss: -0.01843208074569702
Batch 19/64 loss: -0.03273087739944458
Batch 20/64 loss: -0.017084717750549316
Batch 21/64 loss: -0.022626936435699463
Batch 22/64 loss: -0.016701817512512207
Batch 23/64 loss: -0.019784212112426758
Batch 24/64 loss: -0.02590179443359375
Batch 25/64 loss: -0.024714231491088867
Batch 26/64 loss: -0.02633345127105713
Batch 27/64 loss: -0.03335714340209961
Batch 28/64 loss: -0.033544301986694336
Batch 29/64 loss: -0.013321638107299805
Batch 30/64 loss: -0.02761167287826538
Batch 31/64 loss: -0.028241097927093506
Batch 32/64 loss: 0.0014292001724243164
Batch 33/64 loss: -0.03068375587463379
Batch 34/64 loss: -0.019376695156097412
Batch 35/64 loss: -0.027072608470916748
Batch 36/64 loss: -0.024546682834625244
Batch 37/64 loss: -0.006652712821960449
Batch 38/64 loss: -0.02793729305267334
Batch 39/64 loss: -0.014658093452453613
Batch 40/64 loss: -0.011967360973358154
Batch 41/64 loss: -0.003951013088226318
Batch 42/64 loss: -0.0068160295486450195
Batch 43/64 loss: -0.006972908973693848
Batch 44/64 loss: -0.010061025619506836
Batch 45/64 loss: -0.02301234006881714
Batch 46/64 loss: -0.005198240280151367
Batch 47/64 loss: -0.021342754364013672
Batch 48/64 loss: -0.004067182540893555
Batch 49/64 loss: -0.020517468452453613
Batch 50/64 loss: -0.021579861640930176
Batch 51/64 loss: 0.0013380050659179688
Batch 52/64 loss: 0.0048781633377075195
Batch 53/64 loss: -0.02034902572631836
Batch 54/64 loss: -0.01706331968307495
Batch 55/64 loss: -0.018763601779937744
Batch 56/64 loss: -0.024771392345428467
Batch 57/64 loss: -0.0021333694458007812
Batch 58/64 loss: -0.030620694160461426
Batch 59/64 loss: -0.021957814693450928
Batch 60/64 loss: -0.01881265640258789
Batch 61/64 loss: -0.03128457069396973
Batch 62/64 loss: -0.024487018585205078
Batch 63/64 loss: -0.005467653274536133
Batch 64/64 loss: -0.02102220058441162
Epoch 349  Train loss: -0.01807393887463738  Val loss: 0.09043392917954225
Epoch 350
-------------------------------
Batch 1/64 loss: -0.02878814935684204
Batch 2/64 loss: -0.02143871784210205
Batch 3/64 loss: -0.0036422014236450195
Batch 4/64 loss: -0.017539680004119873
Batch 5/64 loss: -0.02002662420272827
Batch 6/64 loss: -0.024068832397460938
Batch 7/64 loss: -0.00645369291305542
Batch 8/64 loss: -0.02442699670791626
Batch 9/64 loss: -0.003927290439605713
Batch 10/64 loss: -0.01399838924407959
Batch 11/64 loss: -0.0007852315902709961
Batch 12/64 loss: -0.012433886528015137
Batch 13/64 loss: -0.007784426212310791
Batch 14/64 loss: -0.023756563663482666
Batch 15/64 loss: -0.022864878177642822
Batch 16/64 loss: -0.01043093204498291
Batch 17/64 loss: -0.01649874448776245
Batch 18/64 loss: -0.02997732162475586
Batch 19/64 loss: -0.03149998188018799
Batch 20/64 loss: -0.022365450859069824
Batch 21/64 loss: -0.02628391981124878
Batch 22/64 loss: -0.024124622344970703
Batch 23/64 loss: -0.015032708644866943
Batch 24/64 loss: 0.012683331966400146
Batch 25/64 loss: -0.01124352216720581
Batch 26/64 loss: -0.029141366481781006
Batch 27/64 loss: -0.018630504608154297
Batch 28/64 loss: -0.023955941200256348
Batch 29/64 loss: -0.01283174753189087
Batch 30/64 loss: -0.02781575918197632
Batch 31/64 loss: -0.014733731746673584
Batch 32/64 loss: -0.013640940189361572
Batch 33/64 loss: -0.018671929836273193
Batch 34/64 loss: 0.00759124755859375
Batch 35/64 loss: -0.02877706289291382
Batch 36/64 loss: -9.09566879272461e-05
Batch 37/64 loss: -0.02558058500289917
Batch 38/64 loss: 0.001920938491821289
Batch 39/64 loss: -0.011019766330718994
Batch 40/64 loss: 0.0009572505950927734
Batch 41/64 loss: 0.0077402591705322266
Batch 42/64 loss: -0.023784518241882324
Batch 43/64 loss: -0.018550515174865723
Batch 44/64 loss: -0.01736617088317871
Batch 45/64 loss: -0.016348659992218018
Batch 46/64 loss: -0.015680789947509766
Batch 47/64 loss: -0.00401759147644043
Batch 48/64 loss: -0.009894311428070068
Batch 49/64 loss: -0.0135536789894104
Batch 50/64 loss: -0.006115436553955078
Batch 51/64 loss: -0.008557617664337158
Batch 52/64 loss: -0.006887316703796387
Batch 53/64 loss: 0.00457453727722168
Batch 54/64 loss: -0.011395573616027832
Batch 55/64 loss: 0.0018067359924316406
Batch 56/64 loss: -0.017130255699157715
Batch 57/64 loss: -0.02487504482269287
Batch 58/64 loss: 0.004448056221008301
Batch 59/64 loss: -0.022256791591644287
Batch 60/64 loss: -0.011969208717346191
Batch 61/64 loss: -0.016966581344604492
Batch 62/64 loss: -0.0198824405670166
Batch 63/64 loss: -0.024736344814300537
Batch 64/64 loss: -0.014334559440612793
Epoch 350  Train loss: -0.014169105361489688  Val loss: 0.0916557277191136
Epoch 351
-------------------------------
Batch 1/64 loss: -0.018740415573120117
Batch 2/64 loss: -0.019912004470825195
Batch 3/64 loss: -0.03761136531829834
Batch 4/64 loss: -0.0301893949508667
Batch 5/64 loss: -0.020321369171142578
Batch 6/64 loss: -0.027495086193084717
Batch 7/64 loss: -0.025287747383117676
Batch 8/64 loss: -0.01997077465057373
Batch 9/64 loss: -0.02334308624267578
Batch 10/64 loss: -0.022978544235229492
Batch 11/64 loss: -0.028306961059570312
Batch 12/64 loss: -0.016446292400360107
Batch 13/64 loss: -0.01879417896270752
Batch 14/64 loss: -0.025340914726257324
Batch 15/64 loss: -0.019361555576324463
Batch 16/64 loss: -0.022845327854156494
Batch 17/64 loss: -0.02520298957824707
Batch 18/64 loss: -0.020050048828125
Batch 19/64 loss: -0.00926518440246582
Batch 20/64 loss: -0.0117950439453125
Batch 21/64 loss: -0.014000475406646729
Batch 22/64 loss: -0.02552741765975952
Batch 23/64 loss: -0.009747743606567383
Batch 24/64 loss: -0.015102624893188477
Batch 25/64 loss: -0.016411185264587402
Batch 26/64 loss: 0.006732583045959473
Batch 27/64 loss: -0.015507221221923828
Batch 28/64 loss: -0.012241005897521973
Batch 29/64 loss: -0.011215806007385254
Batch 30/64 loss: -0.00814056396484375
Batch 31/64 loss: -0.02042257785797119
Batch 32/64 loss: -0.028233885765075684
Batch 33/64 loss: -0.03361386060714722
Batch 34/64 loss: 0.0085640549659729
Batch 35/64 loss: -0.016823232173919678
Batch 36/64 loss: -0.0032896995544433594
Batch 37/64 loss: -0.010923147201538086
Batch 38/64 loss: -0.020658433437347412
Batch 39/64 loss: -0.02198415994644165
Batch 40/64 loss: -0.0073577165603637695
Batch 41/64 loss: -0.026586592197418213
Batch 42/64 loss: -0.01603013277053833
Batch 43/64 loss: -0.019695281982421875
Batch 44/64 loss: -0.02454698085784912
Batch 45/64 loss: -0.014117538928985596
Batch 46/64 loss: 0.006786167621612549
Batch 47/64 loss: -0.028658390045166016
Batch 48/64 loss: -0.021759629249572754
Batch 49/64 loss: -0.013311386108398438
Batch 50/64 loss: -0.02191317081451416
Batch 51/64 loss: -0.011115729808807373
Batch 52/64 loss: -0.023934006690979004
Batch 53/64 loss: -0.02937030792236328
Batch 54/64 loss: -0.0055863261222839355
Batch 55/64 loss: -0.016129136085510254
Batch 56/64 loss: -0.019030392169952393
Batch 57/64 loss: -0.01828855276107788
Batch 58/64 loss: 0.006004154682159424
Batch 59/64 loss: -0.007846355438232422
Batch 60/64 loss: -0.009169578552246094
Batch 61/64 loss: -0.002583742141723633
Batch 62/64 loss: -0.013611197471618652
Batch 63/64 loss: -0.004131317138671875
Batch 64/64 loss: -0.0057718753814697266
Epoch 351  Train loss: -0.016597909553378237  Val loss: 0.09390854139098596
Epoch 352
-------------------------------
Batch 1/64 loss: -0.024628877639770508
Batch 2/64 loss: -0.02623152732849121
Batch 3/64 loss: -0.012807786464691162
Batch 4/64 loss: -0.024111509323120117
Batch 5/64 loss: -0.019199073314666748
Batch 6/64 loss: -0.011839091777801514
Batch 7/64 loss: -0.02536672353744507
Batch 8/64 loss: -0.042856574058532715
Batch 9/64 loss: -0.026717960834503174
Batch 10/64 loss: -0.027804851531982422
Batch 11/64 loss: -0.0033516883850097656
Batch 12/64 loss: -0.01943027973175049
Batch 13/64 loss: -0.026309192180633545
Batch 14/64 loss: -0.023346006870269775
Batch 15/64 loss: -0.016263902187347412
Batch 16/64 loss: -0.023392677307128906
Batch 17/64 loss: -0.02452796697616577
Batch 18/64 loss: -0.001495361328125
Batch 19/64 loss: -0.018690764904022217
Batch 20/64 loss: -0.03123915195465088
Batch 21/64 loss: -0.03278923034667969
Batch 22/64 loss: -0.020293831825256348
Batch 23/64 loss: -0.03051358461380005
Batch 24/64 loss: -0.033399224281311035
Batch 25/64 loss: -0.028090059757232666
Batch 26/64 loss: 0.00890803337097168
Batch 27/64 loss: -0.01390308141708374
Batch 28/64 loss: -0.018917322158813477
Batch 29/64 loss: -0.016801774501800537
Batch 30/64 loss: -0.025749504566192627
Batch 31/64 loss: -0.024750947952270508
Batch 32/64 loss: -0.012370467185974121
Batch 33/64 loss: -0.006861090660095215
Batch 34/64 loss: -0.004650294780731201
Batch 35/64 loss: -0.00047791004180908203
Batch 36/64 loss: -0.021610915660858154
Batch 37/64 loss: -0.009011447429656982
Batch 38/64 loss: -0.0011698007583618164
Batch 39/64 loss: -0.017340540885925293
Batch 40/64 loss: -0.0058144330978393555
Batch 41/64 loss: 0.0033536553382873535
Batch 42/64 loss: -0.003993868827819824
Batch 43/64 loss: -0.007120311260223389
Batch 44/64 loss: -0.005919754505157471
Batch 45/64 loss: -0.008915305137634277
Batch 46/64 loss: -0.01215904951095581
Batch 47/64 loss: -0.02270209789276123
Batch 48/64 loss: 0.0017322301864624023
Batch 49/64 loss: -0.003991961479187012
Batch 50/64 loss: -0.006700098514556885
Batch 51/64 loss: -0.03121316432952881
Batch 52/64 loss: -0.010936200618743896
Batch 53/64 loss: -0.007224917411804199
Batch 54/64 loss: -0.027139008045196533
Batch 55/64 loss: -0.019822418689727783
Batch 56/64 loss: -0.023063063621520996
Batch 57/64 loss: -0.015816986560821533
Batch 58/64 loss: -0.018505096435546875
Batch 59/64 loss: -0.008984267711639404
Batch 60/64 loss: -0.023579299449920654
Batch 61/64 loss: -0.016536951065063477
Batch 62/64 loss: -0.028731703758239746
Batch 63/64 loss: -0.0167427659034729
Batch 64/64 loss: -0.03202104568481445
Epoch 352  Train loss: -0.017003083696552353  Val loss: 0.09019923537867176
Epoch 353
-------------------------------
Batch 1/64 loss: -0.04166817665100098
Batch 2/64 loss: -0.029665708541870117
Batch 3/64 loss: -0.016820549964904785
Batch 4/64 loss: -0.012601613998413086
Batch 5/64 loss: -0.019842445850372314
Batch 6/64 loss: -0.02145671844482422
Batch 7/64 loss: -0.013379335403442383
Batch 8/64 loss: -0.017987072467803955
Batch 9/64 loss: -0.030936121940612793
Batch 10/64 loss: -0.01525658369064331
Batch 11/64 loss: -0.025382041931152344
Batch 12/64 loss: -0.01301276683807373
Batch 13/64 loss: -0.010462403297424316
Batch 14/64 loss: -0.028005123138427734
Batch 15/64 loss: -0.020542681217193604
Batch 16/64 loss: -0.010894179344177246
Batch 17/64 loss: -0.021594345569610596
Batch 18/64 loss: -0.01698547601699829
Batch 19/64 loss: -0.04053306579589844
Batch 20/64 loss: -0.019358336925506592
Batch 21/64 loss: -0.020714223384857178
Batch 22/64 loss: -0.019161701202392578
Batch 23/64 loss: -0.018657267093658447
Batch 24/64 loss: -0.021580219268798828
Batch 25/64 loss: -0.019038915634155273
Batch 26/64 loss: -0.017121553421020508
Batch 27/64 loss: -0.017680585384368896
Batch 28/64 loss: -0.008873999118804932
Batch 29/64 loss: -0.01925826072692871
Batch 30/64 loss: -0.016036808490753174
Batch 31/64 loss: -0.025071442127227783
Batch 32/64 loss: -0.0021064281463623047
Batch 33/64 loss: -0.027222633361816406
Batch 34/64 loss: -0.020994603633880615
Batch 35/64 loss: -0.03272604942321777
Batch 36/64 loss: -0.0036079883575439453
Batch 37/64 loss: 0.005339384078979492
Batch 38/64 loss: -0.0012249350547790527
Batch 39/64 loss: -0.018823087215423584
Batch 40/64 loss: -0.016054630279541016
Batch 41/64 loss: -0.012577295303344727
Batch 42/64 loss: -0.018353164196014404
Batch 43/64 loss: -0.017548322677612305
Batch 44/64 loss: -0.015147387981414795
Batch 45/64 loss: 0.0037984848022460938
Batch 46/64 loss: -0.030578196048736572
Batch 47/64 loss: -0.0040618181228637695
Batch 48/64 loss: -0.00951540470123291
Batch 49/64 loss: 0.0054901838302612305
Batch 50/64 loss: -0.012439250946044922
Batch 51/64 loss: -0.001961648464202881
Batch 52/64 loss: -0.019875824451446533
Batch 53/64 loss: -0.026490211486816406
Batch 54/64 loss: -0.020045340061187744
Batch 55/64 loss: -0.019099712371826172
Batch 56/64 loss: -0.01975691318511963
Batch 57/64 loss: -0.014882981777191162
Batch 58/64 loss: -0.02577197551727295
Batch 59/64 loss: -0.007027983665466309
Batch 60/64 loss: -0.026243209838867188
Batch 61/64 loss: -0.012121617794036865
Batch 62/64 loss: -0.02462005615234375
Batch 63/64 loss: 0.0018894672393798828
Batch 64/64 loss: -0.01955866813659668
Epoch 353  Train loss: -0.017076233321545172  Val loss: 0.0937024137818117
Epoch 354
-------------------------------
Batch 1/64 loss: -0.01731175184249878
Batch 2/64 loss: -0.02740532159805298
Batch 3/64 loss: -0.021041035652160645
Batch 4/64 loss: -0.011349678039550781
Batch 5/64 loss: -0.016404926776885986
Batch 6/64 loss: -0.025585591793060303
Batch 7/64 loss: -0.02582329511642456
Batch 8/64 loss: -0.013728976249694824
Batch 9/64 loss: -0.03217124938964844
Batch 10/64 loss: -0.013905763626098633
Batch 11/64 loss: -0.01897674798965454
Batch 12/64 loss: -0.024523437023162842
Batch 13/64 loss: -0.028946757316589355
Batch 14/64 loss: -0.017578482627868652
Batch 15/64 loss: -0.027446985244750977
Batch 16/64 loss: -0.015864908695220947
Batch 17/64 loss: 0.000531315803527832
Batch 18/64 loss: -0.02131485939025879
Batch 19/64 loss: -0.02030956745147705
Batch 20/64 loss: -0.01819026470184326
Batch 21/64 loss: -0.006147503852844238
Batch 22/64 loss: -0.020702898502349854
Batch 23/64 loss: -0.020897328853607178
Batch 24/64 loss: -0.024720191955566406
Batch 25/64 loss: -0.006867885589599609
Batch 26/64 loss: -0.019832372665405273
Batch 27/64 loss: -0.01699042320251465
Batch 28/64 loss: -0.016373813152313232
Batch 29/64 loss: -0.017597556114196777
Batch 30/64 loss: -0.012376725673675537
Batch 31/64 loss: -0.011592447757720947
Batch 32/64 loss: -0.021754324436187744
Batch 33/64 loss: -0.029415130615234375
Batch 34/64 loss: -0.01847982406616211
Batch 35/64 loss: -0.024356424808502197
Batch 36/64 loss: -0.021169006824493408
Batch 37/64 loss: -0.006686091423034668
Batch 38/64 loss: -0.0018235445022583008
Batch 39/64 loss: -0.007504105567932129
Batch 40/64 loss: -0.018738746643066406
Batch 41/64 loss: -0.016227543354034424
Batch 42/64 loss: -0.0159304141998291
Batch 43/64 loss: -0.02346205711364746
Batch 44/64 loss: -0.013236463069915771
Batch 45/64 loss: -0.025650978088378906
Batch 46/64 loss: -0.016672372817993164
Batch 47/64 loss: -0.01915717124938965
Batch 48/64 loss: -0.007890880107879639
Batch 49/64 loss: -0.009067952632904053
Batch 50/64 loss: -0.02832120656967163
Batch 51/64 loss: -0.007355451583862305
Batch 52/64 loss: -0.015844643115997314
Batch 53/64 loss: 0.0038793087005615234
Batch 54/64 loss: -0.006006419658660889
Batch 55/64 loss: -0.012689828872680664
Batch 56/64 loss: -0.009126782417297363
Batch 57/64 loss: -0.009802699089050293
Batch 58/64 loss: -0.023413538932800293
Batch 59/64 loss: -0.007147014141082764
Batch 60/64 loss: -0.01922297477722168
Batch 61/64 loss: 0.002789437770843506
Batch 62/64 loss: -0.02180379629135132
Batch 63/64 loss: -0.018230319023132324
Batch 64/64 loss: -0.01806575059890747
Epoch 354  Train loss: -0.01654142281588386  Val loss: 0.0909664589924501
Epoch 355
-------------------------------
Batch 1/64 loss: -0.009097278118133545
Batch 2/64 loss: -0.02031540870666504
Batch 3/64 loss: -0.0022614002227783203
Batch 4/64 loss: -0.033189475536346436
Batch 5/64 loss: -0.017045974731445312
Batch 6/64 loss: -0.01944369077682495
Batch 7/64 loss: -0.02510279417037964
Batch 8/64 loss: -0.014017581939697266
Batch 9/64 loss: -0.027621686458587646
Batch 10/64 loss: -0.017839550971984863
Batch 11/64 loss: -0.02652871608734131
Batch 12/64 loss: -0.016769886016845703
Batch 13/64 loss: -0.03389596939086914
Batch 14/64 loss: -0.023146510124206543
Batch 15/64 loss: -0.020598649978637695
Batch 16/64 loss: -0.03724658489227295
Batch 17/64 loss: -0.029188215732574463
Batch 18/64 loss: -0.019147872924804688
Batch 19/64 loss: 0.0005328655242919922
Batch 20/64 loss: -0.02381080389022827
Batch 21/64 loss: -0.017771005630493164
Batch 22/64 loss: -0.01120603084564209
Batch 23/64 loss: -0.02825343608856201
Batch 24/64 loss: -0.006482720375061035
Batch 25/64 loss: -0.024438917636871338
Batch 26/64 loss: -0.007816314697265625
Batch 27/64 loss: -0.018006861209869385
Batch 28/64 loss: -0.013304948806762695
Batch 29/64 loss: -0.032175540924072266
Batch 30/64 loss: -0.034840285778045654
Batch 31/64 loss: -0.026723861694335938
Batch 32/64 loss: -0.03832399845123291
Batch 33/64 loss: -0.026378393173217773
Batch 34/64 loss: -0.03473663330078125
Batch 35/64 loss: -0.0008525848388671875
Batch 36/64 loss: -0.011529803276062012
Batch 37/64 loss: 0.0002670884132385254
Batch 38/64 loss: -0.01824420690536499
Batch 39/64 loss: -0.011554300785064697
Batch 40/64 loss: -0.021110117435455322
Batch 41/64 loss: -0.004832625389099121
Batch 42/64 loss: -0.0008247494697570801
Batch 43/64 loss: -0.028798580169677734
Batch 44/64 loss: -0.014220118522644043
Batch 45/64 loss: -0.024816393852233887
Batch 46/64 loss: 0.006270885467529297
Batch 47/64 loss: -0.03391528129577637
Batch 48/64 loss: 0.0006355643272399902
Batch 49/64 loss: -0.01980280876159668
Batch 50/64 loss: -0.004836738109588623
Batch 51/64 loss: -0.01555323600769043
Batch 52/64 loss: 6.848573684692383e-05
Batch 53/64 loss: -0.025163769721984863
Batch 54/64 loss: -0.014617979526519775
Batch 55/64 loss: -0.02947211265563965
Batch 56/64 loss: -0.030479490756988525
Batch 57/64 loss: -0.015578508377075195
Batch 58/64 loss: -0.0002771615982055664
Batch 59/64 loss: -0.0012966394424438477
Batch 60/64 loss: -0.011859714984893799
Batch 61/64 loss: 0.002326071262359619
Batch 62/64 loss: 0.0009319186210632324
Batch 63/64 loss: -0.02252054214477539
Batch 64/64 loss: -0.0168609619140625
Epoch 355  Train loss: -0.01726270282969755  Val loss: 0.09174226628955696
Epoch 356
-------------------------------
Batch 1/64 loss: -0.026909351348876953
Batch 2/64 loss: -0.03710860013961792
Batch 3/64 loss: -0.032647132873535156
Batch 4/64 loss: -0.026735365390777588
Batch 5/64 loss: -0.029951930046081543
Batch 6/64 loss: -0.02837526798248291
Batch 7/64 loss: -0.02188318967819214
Batch 8/64 loss: -0.020417094230651855
Batch 9/64 loss: -0.00600355863571167
Batch 10/64 loss: 0.0060765743255615234
Batch 11/64 loss: -0.026037395000457764
Batch 12/64 loss: -0.022627055644989014
Batch 13/64 loss: -0.01405191421508789
Batch 14/64 loss: -0.020376086235046387
Batch 15/64 loss: -0.00496065616607666
Batch 16/64 loss: -0.020883262157440186
Batch 17/64 loss: -0.009121417999267578
Batch 18/64 loss: -0.00922536849975586
Batch 19/64 loss: -0.010135471820831299
Batch 20/64 loss: -0.003778815269470215
Batch 21/64 loss: -0.015192627906799316
Batch 22/64 loss: 0.010713934898376465
Batch 23/64 loss: -0.007677733898162842
Batch 24/64 loss: -0.029481112957000732
Batch 25/64 loss: -0.013081133365631104
Batch 26/64 loss: -0.026398539543151855
Batch 27/64 loss: -0.003087162971496582
Batch 28/64 loss: -0.012770116329193115
Batch 29/64 loss: -0.02482539415359497
Batch 30/64 loss: -0.019802570343017578
Batch 31/64 loss: -0.013477504253387451
Batch 32/64 loss: -0.025042474269866943
Batch 33/64 loss: -0.02682816982269287
Batch 34/64 loss: -0.01588273048400879
Batch 35/64 loss: -0.013825654983520508
Batch 36/64 loss: -0.014099538326263428
Batch 37/64 loss: -0.025445282459259033
Batch 38/64 loss: -0.026989877223968506
Batch 39/64 loss: -0.014522731304168701
Batch 40/64 loss: -0.017091035842895508
Batch 41/64 loss: -0.027667701244354248
Batch 42/64 loss: -0.02326071262359619
Batch 43/64 loss: -0.012826144695281982
Batch 44/64 loss: -0.02381157875061035
Batch 45/64 loss: -0.023576855659484863
Batch 46/64 loss: -0.026996612548828125
Batch 47/64 loss: -0.023258209228515625
Batch 48/64 loss: -0.02397400140762329
Batch 49/64 loss: -0.04313284158706665
Batch 50/64 loss: -0.024459779262542725
Batch 51/64 loss: -0.02109062671661377
Batch 52/64 loss: -0.020282089710235596
Batch 53/64 loss: -0.00667262077331543
Batch 54/64 loss: -0.032361626625061035
Batch 55/64 loss: -0.00470423698425293
Batch 56/64 loss: -0.017352402210235596
Batch 57/64 loss: -0.016895413398742676
Batch 58/64 loss: -0.004601120948791504
Batch 59/64 loss: -0.009648740291595459
Batch 60/64 loss: -0.01990199089050293
Batch 61/64 loss: -0.007739067077636719
Batch 62/64 loss: -0.01520383358001709
Batch 63/64 loss: -0.015729069709777832
Batch 64/64 loss: -0.011996448040008545
Epoch 356  Train loss: -0.01810359884710873  Val loss: 0.09198646959160611
Epoch 357
-------------------------------
Batch 1/64 loss: -0.00896143913269043
Batch 2/64 loss: -0.021619319915771484
Batch 3/64 loss: -0.02650618553161621
Batch 4/64 loss: -0.022667288780212402
Batch 5/64 loss: -0.03062361478805542
Batch 6/64 loss: -0.018679380416870117
Batch 7/64 loss: -0.02581840753555298
Batch 8/64 loss: -0.014283061027526855
Batch 9/64 loss: -0.017128586769104004
Batch 10/64 loss: -0.013933420181274414
Batch 11/64 loss: -0.01416015625
Batch 12/64 loss: -0.017769217491149902
Batch 13/64 loss: 0.011608719825744629
Batch 14/64 loss: -0.0203438401222229
Batch 15/64 loss: -0.03321564197540283
Batch 16/64 loss: 0.00041365623474121094
Batch 17/64 loss: -0.020500659942626953
Batch 18/64 loss: -0.03259921073913574
Batch 19/64 loss: -0.03538525104522705
Batch 20/64 loss: -0.02660191059112549
Batch 21/64 loss: -0.017514824867248535
Batch 22/64 loss: -0.012573838233947754
Batch 23/64 loss: -0.02314084768295288
Batch 24/64 loss: -0.022181391716003418
Batch 25/64 loss: -0.010004818439483643
Batch 26/64 loss: -0.010242283344268799
Batch 27/64 loss: -0.02271866798400879
Batch 28/64 loss: -0.009924173355102539
Batch 29/64 loss: -0.025450527667999268
Batch 30/64 loss: -0.03960973024368286
Batch 31/64 loss: -0.026051878929138184
Batch 32/64 loss: -0.019978225231170654
Batch 33/64 loss: -0.024316489696502686
Batch 34/64 loss: -0.011584758758544922
Batch 35/64 loss: -0.02770686149597168
Batch 36/64 loss: -0.02089214324951172
Batch 37/64 loss: -0.014671802520751953
Batch 38/64 loss: -0.012063443660736084
Batch 39/64 loss: -0.020094335079193115
Batch 40/64 loss: -0.008362650871276855
Batch 41/64 loss: -0.01777440309524536
Batch 42/64 loss: -0.015665113925933838
Batch 43/64 loss: -0.006623625755310059
Batch 44/64 loss: -0.015545368194580078
Batch 45/64 loss: -0.01842188835144043
Batch 46/64 loss: -0.01915043592453003
Batch 47/64 loss: -0.009312450885772705
Batch 48/64 loss: -0.02360302209854126
Batch 49/64 loss: -0.019735753536224365
Batch 50/64 loss: -0.023453116416931152
Batch 51/64 loss: -0.021427392959594727
Batch 52/64 loss: -0.03011530637741089
Batch 53/64 loss: -0.021219849586486816
Batch 54/64 loss: -0.03041619062423706
Batch 55/64 loss: -0.022316396236419678
Batch 56/64 loss: -0.010344266891479492
Batch 57/64 loss: -0.02278655767440796
Batch 58/64 loss: -0.012723088264465332
Batch 59/64 loss: -0.01052713394165039
Batch 60/64 loss: -0.028160929679870605
Batch 61/64 loss: -0.01807534694671631
Batch 62/64 loss: -0.023910105228424072
Batch 63/64 loss: -0.026295840740203857
Batch 64/64 loss: -0.03070586919784546
Epoch 357  Train loss: -0.019427308148028805  Val loss: 0.09401780203036017
Epoch 358
-------------------------------
Batch 1/64 loss: -0.00699615478515625
Batch 2/64 loss: -0.02397465705871582
Batch 3/64 loss: -0.02757185697555542
Batch 4/64 loss: -0.007890105247497559
Batch 5/64 loss: -0.0371556282043457
Batch 6/64 loss: -0.024149775505065918
Batch 7/64 loss: -0.02767390012741089
Batch 8/64 loss: -0.031200766563415527
Batch 9/64 loss: -0.027868449687957764
Batch 10/64 loss: -0.035489559173583984
Batch 11/64 loss: -0.027066171169281006
Batch 12/64 loss: -0.03238755464553833
Batch 13/64 loss: -0.02757120132446289
Batch 14/64 loss: -0.0340384840965271
Batch 15/64 loss: -0.029863715171813965
Batch 16/64 loss: -0.023484647274017334
Batch 17/64 loss: -0.021817922592163086
Batch 18/64 loss: -0.009943246841430664
Batch 19/64 loss: -0.024356365203857422
Batch 20/64 loss: -0.021973073482513428
Batch 21/64 loss: -0.01588517427444458
Batch 22/64 loss: -0.030299663543701172
Batch 23/64 loss: -0.03244340419769287
Batch 24/64 loss: 0.00987178087234497
Batch 25/64 loss: -0.007621645927429199
Batch 26/64 loss: -0.02047264575958252
Batch 27/64 loss: -0.009759783744812012
Batch 28/64 loss: -0.01743757724761963
Batch 29/64 loss: -0.0162431001663208
Batch 30/64 loss: -0.008463680744171143
Batch 31/64 loss: -0.020599722862243652
Batch 32/64 loss: -0.01575493812561035
Batch 33/64 loss: -0.016097545623779297
Batch 34/64 loss: -0.007882535457611084
Batch 35/64 loss: -0.018119215965270996
Batch 36/64 loss: -0.00044667720794677734
Batch 37/64 loss: -0.011806249618530273
Batch 38/64 loss: -0.020289599895477295
Batch 39/64 loss: -0.02000117301940918
Batch 40/64 loss: -0.026014328002929688
Batch 41/64 loss: -0.023025095462799072
Batch 42/64 loss: -0.01650691032409668
Batch 43/64 loss: -0.03761577606201172
Batch 44/64 loss: -0.01003408432006836
Batch 45/64 loss: -0.006926119327545166
Batch 46/64 loss: -0.01115041971206665
Batch 47/64 loss: -0.02910029888153076
Batch 48/64 loss: -0.02707308530807495
Batch 49/64 loss: -0.016273975372314453
Batch 50/64 loss: -0.027902185916900635
Batch 51/64 loss: -0.018638551235198975
Batch 52/64 loss: -0.02695983648300171
Batch 53/64 loss: -0.021075427532196045
Batch 54/64 loss: -0.01463615894317627
Batch 55/64 loss: -0.027293920516967773
Batch 56/64 loss: -0.015769243240356445
Batch 57/64 loss: -0.0179978609085083
Batch 58/64 loss: -0.00953972339630127
Batch 59/64 loss: -0.03051018714904785
Batch 60/64 loss: -0.009656012058258057
Batch 61/64 loss: -0.033016204833984375
Batch 62/64 loss: -0.0282096266746521
Batch 63/64 loss: -0.02382493019104004
Batch 64/64 loss: -0.006323039531707764
Epoch 358  Train loss: -0.020324988692414527  Val loss: 0.09154392702063334
Epoch 359
-------------------------------
Batch 1/64 loss: -0.031550824642181396
Batch 2/64 loss: -0.009151279926300049
Batch 3/64 loss: -0.03539574146270752
Batch 4/64 loss: -0.01687180995941162
Batch 5/64 loss: -0.0075647830963134766
Batch 6/64 loss: -0.027065038681030273
Batch 7/64 loss: -0.02551037073135376
Batch 8/64 loss: -0.03026270866394043
Batch 9/64 loss: -0.02918553352355957
Batch 10/64 loss: -0.015491306781768799
Batch 11/64 loss: -0.014939665794372559
Batch 12/64 loss: -0.027513086795806885
Batch 13/64 loss: -0.013738632202148438
Batch 14/64 loss: 0.0052727460861206055
Batch 15/64 loss: -0.009716331958770752
Batch 16/64 loss: -0.025634467601776123
Batch 17/64 loss: -0.012130975723266602
Batch 18/64 loss: -0.029119491577148438
Batch 19/64 loss: -0.01063472032546997
Batch 20/64 loss: -0.026427626609802246
Batch 21/64 loss: -0.014365732669830322
Batch 22/64 loss: -0.029054701328277588
Batch 23/64 loss: -0.020981669425964355
Batch 24/64 loss: -0.014353513717651367
Batch 25/64 loss: -0.013913094997406006
Batch 26/64 loss: -0.029356420040130615
Batch 27/64 loss: -0.030520081520080566
Batch 28/64 loss: -0.007807254791259766
Batch 29/64 loss: -0.015290379524230957
Batch 30/64 loss: -0.01629948616027832
Batch 31/64 loss: -0.015708506107330322
Batch 32/64 loss: -0.01289975643157959
Batch 33/64 loss: -0.03387439250946045
Batch 34/64 loss: -0.03183150291442871
Batch 35/64 loss: -0.013633489608764648
Batch 36/64 loss: -0.014640092849731445
Batch 37/64 loss: -0.03004169464111328
Batch 38/64 loss: -0.004098653793334961
Batch 39/64 loss: -0.015880227088928223
Batch 40/64 loss: -0.01670736074447632
Batch 41/64 loss: -0.016083240509033203
Batch 42/64 loss: -0.021182775497436523
Batch 43/64 loss: -0.024828791618347168
Batch 44/64 loss: -0.017628908157348633
Batch 45/64 loss: -0.0016291141510009766
Batch 46/64 loss: -0.00721508264541626
Batch 47/64 loss: -0.023220181465148926
Batch 48/64 loss: -0.01267153024673462
Batch 49/64 loss: -0.009047627449035645
Batch 50/64 loss: -0.03705412149429321
Batch 51/64 loss: -0.02886718511581421
Batch 52/64 loss: -0.03706258535385132
Batch 53/64 loss: -0.018722236156463623
Batch 54/64 loss: -0.01217496395111084
Batch 55/64 loss: -0.022086679935455322
Batch 56/64 loss: -0.006318449974060059
Batch 57/64 loss: -0.01395118236541748
Batch 58/64 loss: -0.026244044303894043
Batch 59/64 loss: -0.018569350242614746
Batch 60/64 loss: -0.016734004020690918
Batch 61/64 loss: -0.015627801418304443
Batch 62/64 loss: -0.014888107776641846
Batch 63/64 loss: -0.02372288703918457
Batch 64/64 loss: 0.009939372539520264
Epoch 359  Train loss: -0.01869750560498705  Val loss: 0.093029314095212
Epoch 360
-------------------------------
Batch 1/64 loss: -0.03774583339691162
Batch 2/64 loss: -0.030607283115386963
Batch 3/64 loss: -0.015531837940216064
Batch 4/64 loss: -0.027371764183044434
Batch 5/64 loss: -0.0024309754371643066
Batch 6/64 loss: -0.01761913299560547
Batch 7/64 loss: -0.011894583702087402
Batch 8/64 loss: -0.008659899234771729
Batch 9/64 loss: -0.014190077781677246
Batch 10/64 loss: -0.020366966724395752
Batch 11/64 loss: -0.016187965869903564
Batch 12/64 loss: -0.02261894941329956
Batch 13/64 loss: -0.021990180015563965
Batch 14/64 loss: -0.02590620517730713
Batch 15/64 loss: -0.0056092143058776855
Batch 16/64 loss: -0.01230698823928833
Batch 17/64 loss: -0.017782390117645264
Batch 18/64 loss: -0.01608937978744507
Batch 19/64 loss: -0.021790027618408203
Batch 20/64 loss: -0.023148834705352783
Batch 21/64 loss: -0.02636188268661499
Batch 22/64 loss: 0.014229118824005127
Batch 23/64 loss: -0.03234589099884033
Batch 24/64 loss: -0.025288105010986328
Batch 25/64 loss: -0.017385244369506836
Batch 26/64 loss: -0.028727352619171143
Batch 27/64 loss: -0.024485886096954346
Batch 28/64 loss: -0.016907572746276855
Batch 29/64 loss: -0.02490699291229248
Batch 30/64 loss: -0.01564079523086548
Batch 31/64 loss: -0.01264733076095581
Batch 32/64 loss: -0.023270606994628906
Batch 33/64 loss: -0.024378836154937744
Batch 34/64 loss: 0.0003314018249511719
Batch 35/64 loss: -0.036182284355163574
Batch 36/64 loss: -0.021691560745239258
Batch 37/64 loss: -0.031216323375701904
Batch 38/64 loss: -0.016194820404052734
Batch 39/64 loss: -0.019320964813232422
Batch 40/64 loss: -0.012143135070800781
Batch 41/64 loss: -0.021014153957366943
Batch 42/64 loss: -0.007925868034362793
Batch 43/64 loss: 0.006546378135681152
Batch 44/64 loss: -0.024876534938812256
Batch 45/64 loss: -0.030286133289337158
Batch 46/64 loss: -0.015091657638549805
Batch 47/64 loss: -0.010334193706512451
Batch 48/64 loss: -0.026241779327392578
Batch 49/64 loss: -0.01356053352355957
Batch 50/64 loss: -0.02428525686264038
Batch 51/64 loss: -0.001322329044342041
Batch 52/64 loss: -0.03039371967315674
Batch 53/64 loss: -0.013539791107177734
Batch 54/64 loss: -0.017762184143066406
Batch 55/64 loss: -0.01983410120010376
Batch 56/64 loss: -0.019669651985168457
Batch 57/64 loss: -0.01720881462097168
Batch 58/64 loss: -0.0030912160873413086
Batch 59/64 loss: 0.015001177787780762
Batch 60/64 loss: -0.017480134963989258
Batch 61/64 loss: -0.016202688217163086
Batch 62/64 loss: -0.030010700225830078
Batch 63/64 loss: -0.022507190704345703
Batch 64/64 loss: -0.018651187419891357
Epoch 360  Train loss: -0.017873929762372783  Val loss: 0.09439788262049358
Epoch 361
-------------------------------
Batch 1/64 loss: -0.007092773914337158
Batch 2/64 loss: -0.01903688907623291
Batch 3/64 loss: -0.021534979343414307
Batch 4/64 loss: 0.005681514739990234
Batch 5/64 loss: -0.026390135288238525
Batch 6/64 loss: -0.006879687309265137
Batch 7/64 loss: -0.007728874683380127
Batch 8/64 loss: -0.008021831512451172
Batch 9/64 loss: -0.019573569297790527
Batch 10/64 loss: -0.004643499851226807
Batch 11/64 loss: -0.021591544151306152
Batch 12/64 loss: -0.024469614028930664
Batch 13/64 loss: -0.0045410990715026855
Batch 14/64 loss: -0.0008757114410400391
Batch 15/64 loss: -0.020480573177337646
Batch 16/64 loss: -0.019529521465301514
Batch 17/64 loss: -0.02543717622756958
Batch 18/64 loss: -0.020373761653900146
Batch 19/64 loss: -0.00661623477935791
Batch 20/64 loss: -0.029513955116271973
Batch 21/64 loss: -0.02289414405822754
Batch 22/64 loss: -0.017289400100708008
Batch 23/64 loss: -0.015809953212738037
Batch 24/64 loss: -0.026823997497558594
Batch 25/64 loss: -0.02093672752380371
Batch 26/64 loss: -0.019641757011413574
Batch 27/64 loss: -0.006068766117095947
Batch 28/64 loss: -0.006315648555755615
Batch 29/64 loss: -0.017634212970733643
Batch 30/64 loss: -0.015171170234680176
Batch 31/64 loss: -0.026760101318359375
Batch 32/64 loss: 0.00500112771987915
Batch 33/64 loss: 0.014375507831573486
Batch 34/64 loss: 0.002742469310760498
Batch 35/64 loss: -0.008908450603485107
Batch 36/64 loss: -0.013586878776550293
Batch 37/64 loss: -0.01947951316833496
Batch 38/64 loss: -0.0249403715133667
Batch 39/64 loss: -0.030492305755615234
Batch 40/64 loss: -0.02033621072769165
Batch 41/64 loss: -0.005076050758361816
Batch 42/64 loss: -0.01949387788772583
Batch 43/64 loss: -0.021855831146240234
Batch 44/64 loss: -0.02703636884689331
Batch 45/64 loss: -0.0240744948387146
Batch 46/64 loss: -0.022899627685546875
Batch 47/64 loss: -0.025673210620880127
Batch 48/64 loss: -0.03478193283081055
Batch 49/64 loss: -0.0184938907623291
Batch 50/64 loss: -0.013825297355651855
Batch 51/64 loss: -0.02525198459625244
Batch 52/64 loss: -0.02681708335876465
Batch 53/64 loss: -0.010854899883270264
Batch 54/64 loss: -0.029192745685577393
Batch 55/64 loss: -0.03131812810897827
Batch 56/64 loss: -0.012211620807647705
Batch 57/64 loss: -0.007014214992523193
Batch 58/64 loss: -0.004647731781005859
Batch 59/64 loss: -0.027172744274139404
Batch 60/64 loss: -0.025728821754455566
Batch 61/64 loss: -0.012622952461242676
Batch 62/64 loss: -0.02580881118774414
Batch 63/64 loss: -0.019576609134674072
Batch 64/64 loss: 0.004654824733734131
Epoch 361  Train loss: -0.016432286010069005  Val loss: 0.09231599643058383
Epoch 362
-------------------------------
Batch 1/64 loss: -0.008975744247436523
Batch 2/64 loss: -0.025009870529174805
Batch 3/64 loss: -0.019293546676635742
Batch 4/64 loss: -0.02384096384048462
Batch 5/64 loss: -0.009842276573181152
Batch 6/64 loss: -0.019775032997131348
Batch 7/64 loss: -0.014080524444580078
Batch 8/64 loss: -0.024529337882995605
Batch 9/64 loss: -0.014290094375610352
Batch 10/64 loss: -0.023280560970306396
Batch 11/64 loss: -0.018032550811767578
Batch 12/64 loss: -0.027308881282806396
Batch 13/64 loss: -0.02111595869064331
Batch 14/64 loss: -0.022639751434326172
Batch 15/64 loss: -0.023844122886657715
Batch 16/64 loss: -0.02370297908782959
Batch 17/64 loss: -0.02928370237350464
Batch 18/64 loss: -0.017196714878082275
Batch 19/64 loss: -0.02033895254135132
Batch 20/64 loss: -0.03297388553619385
Batch 21/64 loss: -0.025690197944641113
Batch 22/64 loss: -0.018521606922149658
Batch 23/64 loss: -0.03941696882247925
Batch 24/64 loss: -0.03470879793167114
Batch 25/64 loss: -0.010250568389892578
Batch 26/64 loss: -0.011637091636657715
Batch 27/64 loss: -0.015381693840026855
Batch 28/64 loss: -0.03159993886947632
Batch 29/64 loss: -0.02709561586380005
Batch 30/64 loss: -0.027595341205596924
Batch 31/64 loss: -0.008022427558898926
Batch 32/64 loss: -0.022630929946899414
Batch 33/64 loss: -0.022914230823516846
Batch 34/64 loss: -0.025814414024353027
Batch 35/64 loss: -0.01663726568222046
Batch 36/64 loss: -0.02438884973526001
Batch 37/64 loss: -0.004919946193695068
Batch 38/64 loss: -0.0007437467575073242
Batch 39/64 loss: -0.01107865571975708
Batch 40/64 loss: -0.02974879741668701
Batch 41/64 loss: -0.023041486740112305
Batch 42/64 loss: -0.020333051681518555
Batch 43/64 loss: -0.010153651237487793
Batch 44/64 loss: -0.01883620023727417
Batch 45/64 loss: -0.01743030548095703
Batch 46/64 loss: -0.013734519481658936
Batch 47/64 loss: -0.02376413345336914
Batch 48/64 loss: -0.016128361225128174
Batch 49/64 loss: -0.0007425546646118164
Batch 50/64 loss: -0.021075785160064697
Batch 51/64 loss: -0.013614773750305176
Batch 52/64 loss: -0.018732070922851562
Batch 53/64 loss: -0.011232197284698486
Batch 54/64 loss: -0.02166128158569336
Batch 55/64 loss: -0.008193135261535645
Batch 56/64 loss: -0.01999586820602417
Batch 57/64 loss: -0.028918743133544922
Batch 58/64 loss: -0.032354116439819336
Batch 59/64 loss: -0.024364054203033447
Batch 60/64 loss: -0.024409949779510498
Batch 61/64 loss: -0.01569211483001709
Batch 62/64 loss: -0.0019993185997009277
Batch 63/64 loss: -0.008650243282318115
Batch 64/64 loss: -0.030098259449005127
Epoch 362  Train loss: -0.019541712134492163  Val loss: 0.09246787172822199
Epoch 363
-------------------------------
Batch 1/64 loss: -0.025417625904083252
Batch 2/64 loss: -0.012298583984375
Batch 3/64 loss: -0.01676785945892334
Batch 4/64 loss: -0.013745248317718506
Batch 5/64 loss: -0.027492523193359375
Batch 6/64 loss: -0.031867384910583496
Batch 7/64 loss: -0.007637977600097656
Batch 8/64 loss: -0.023681282997131348
Batch 9/64 loss: -0.017756938934326172
Batch 10/64 loss: -0.017547905445098877
Batch 11/64 loss: -0.026381254196166992
Batch 12/64 loss: -0.016388297080993652
Batch 13/64 loss: -0.008625388145446777
Batch 14/64 loss: -0.02844977378845215
Batch 15/64 loss: -0.03215986490249634
Batch 16/64 loss: -0.023844599723815918
Batch 17/64 loss: -0.013511419296264648
Batch 18/64 loss: -0.003345668315887451
Batch 19/64 loss: -0.016729295253753662
Batch 20/64 loss: -0.036255061626434326
Batch 21/64 loss: -0.01833486557006836
Batch 22/64 loss: -0.02627694606781006
Batch 23/64 loss: -0.013140261173248291
Batch 24/64 loss: -0.007148146629333496
Batch 25/64 loss: -0.02203679084777832
Batch 26/64 loss: -0.027210593223571777
Batch 27/64 loss: -0.008192896842956543
Batch 28/64 loss: -0.0255088210105896
Batch 29/64 loss: -0.012613415718078613
Batch 30/64 loss: -0.013073146343231201
Batch 31/64 loss: -0.018466174602508545
Batch 32/64 loss: -0.033940672874450684
Batch 33/64 loss: -0.03514719009399414
Batch 34/64 loss: -0.03546494245529175
Batch 35/64 loss: -0.012178540229797363
Batch 36/64 loss: 0.002196073532104492
Batch 37/64 loss: -0.02621150016784668
Batch 38/64 loss: -0.013002574443817139
Batch 39/64 loss: -0.021237730979919434
Batch 40/64 loss: -0.028904259204864502
Batch 41/64 loss: -0.03202342987060547
Batch 42/64 loss: -0.00816875696182251
Batch 43/64 loss: -0.03240001201629639
Batch 44/64 loss: -0.01383441686630249
Batch 45/64 loss: -0.02853405475616455
Batch 46/64 loss: -0.022742092609405518
Batch 47/64 loss: -0.02952408790588379
Batch 48/64 loss: -0.012178421020507812
Batch 49/64 loss: -0.009035050868988037
Batch 50/64 loss: -0.006578922271728516
Batch 51/64 loss: -0.025643646717071533
Batch 52/64 loss: -0.021036207675933838
Batch 53/64 loss: -0.011811137199401855
Batch 54/64 loss: -0.015924394130706787
Batch 55/64 loss: -0.012514948844909668
Batch 56/64 loss: -0.023458778858184814
Batch 57/64 loss: -0.0019416213035583496
Batch 58/64 loss: -0.01564842462539673
Batch 59/64 loss: -0.008229613304138184
Batch 60/64 loss: -0.019942164421081543
Batch 61/64 loss: -0.015093207359313965
Batch 62/64 loss: -0.01567143201828003
Batch 63/64 loss: -0.029520630836486816
Batch 64/64 loss: -0.01919400691986084
Epoch 363  Train loss: -0.01916311067693374  Val loss: 0.0932759394760394
Epoch 364
-------------------------------
Batch 1/64 loss: -0.0027101635932922363
Batch 2/64 loss: -0.028383195400238037
Batch 3/64 loss: -0.024031758308410645
Batch 4/64 loss: -0.02433568239212036
Batch 5/64 loss: -0.026047110557556152
Batch 6/64 loss: -0.004057943820953369
Batch 7/64 loss: -0.03948301076889038
Batch 8/64 loss: -0.02284795045852661
Batch 9/64 loss: -0.030876755714416504
Batch 10/64 loss: -0.005207479000091553
Batch 11/64 loss: -0.010971605777740479
Batch 12/64 loss: -0.014428853988647461
Batch 13/64 loss: -0.013975858688354492
Batch 14/64 loss: -0.018456995487213135
Batch 15/64 loss: -0.02327120304107666
Batch 16/64 loss: -0.026974022388458252
Batch 17/64 loss: -0.009157121181488037
Batch 18/64 loss: -0.02068173885345459
Batch 19/64 loss: -0.02435147762298584
Batch 20/64 loss: -0.01735842227935791
Batch 21/64 loss: -0.023064613342285156
Batch 22/64 loss: -0.020642518997192383
Batch 23/64 loss: -0.010744810104370117
Batch 24/64 loss: -0.020313680171966553
Batch 25/64 loss: -0.026776373386383057
Batch 26/64 loss: -0.0007138252258300781
Batch 27/64 loss: -0.022961735725402832
Batch 28/64 loss: -0.020584464073181152
Batch 29/64 loss: -0.030230402946472168
Batch 30/64 loss: -0.006441891193389893
Batch 31/64 loss: -0.011382341384887695
Batch 32/64 loss: -0.006016731262207031
Batch 33/64 loss: -0.017007887363433838
Batch 34/64 loss: -0.014991044998168945
Batch 35/64 loss: -0.02861964702606201
Batch 36/64 loss: -0.015416920185089111
Batch 37/64 loss: -0.012972116470336914
Batch 38/64 loss: -0.030286669731140137
Batch 39/64 loss: -0.01822894811630249
Batch 40/64 loss: -0.023073256015777588
Batch 41/64 loss: -0.04554867744445801
Batch 42/64 loss: -0.00879514217376709
Batch 43/64 loss: 3.8743019104003906e-05
Batch 44/64 loss: 0.02236652374267578
Batch 45/64 loss: -0.0018502473831176758
Batch 46/64 loss: -0.030353069305419922
Batch 47/64 loss: -0.03633749485015869
Batch 48/64 loss: -0.026714563369750977
Batch 49/64 loss: -0.031254589557647705
Batch 50/64 loss: -0.012038171291351318
Batch 51/64 loss: -0.013806343078613281
Batch 52/64 loss: -0.013553202152252197
Batch 53/64 loss: -0.015198051929473877
Batch 54/64 loss: -0.023244500160217285
Batch 55/64 loss: -0.019928574562072754
Batch 56/64 loss: -0.023444533348083496
Batch 57/64 loss: -0.03714859485626221
Batch 58/64 loss: -0.024351954460144043
Batch 59/64 loss: -0.03200340270996094
Batch 60/64 loss: -0.019034147262573242
Batch 61/64 loss: -0.007762610912322998
Batch 62/64 loss: -0.0067182183265686035
Batch 63/64 loss: -0.028713464736938477
Batch 64/64 loss: -0.013803243637084961
Epoch 364  Train loss: -0.018726665833417106  Val loss: 0.09463259157855895
Epoch 365
-------------------------------
Batch 1/64 loss: 0.0059397220611572266
Batch 2/64 loss: -0.003828108310699463
Batch 3/64 loss: -0.02134263515472412
Batch 4/64 loss: -0.024351179599761963
Batch 5/64 loss: -0.023584842681884766
Batch 6/64 loss: -0.031976521015167236
Batch 7/64 loss: -0.0271567702293396
Batch 8/64 loss: -0.01581263542175293
Batch 9/64 loss: -0.026113390922546387
Batch 10/64 loss: -0.018637239933013916
Batch 11/64 loss: -0.02952873706817627
Batch 12/64 loss: -0.03747028112411499
Batch 13/64 loss: -0.006781458854675293
Batch 14/64 loss: -0.025382936000823975
Batch 15/64 loss: -0.02854311466217041
Batch 16/64 loss: -0.02335226535797119
Batch 17/64 loss: -0.016472697257995605
Batch 18/64 loss: -0.023321986198425293
Batch 19/64 loss: -0.023700594902038574
Batch 20/64 loss: -0.0237845778465271
Batch 21/64 loss: -0.02826005220413208
Batch 22/64 loss: -0.0281713604927063
Batch 23/64 loss: -0.0127449631690979
Batch 24/64 loss: -0.037721216678619385
Batch 25/64 loss: -0.009619176387786865
Batch 26/64 loss: -0.024119436740875244
Batch 27/64 loss: -0.012105584144592285
Batch 28/64 loss: -0.014020979404449463
Batch 29/64 loss: -0.018071651458740234
Batch 30/64 loss: -0.02593100070953369
Batch 31/64 loss: -0.025982201099395752
Batch 32/64 loss: -0.026443958282470703
Batch 33/64 loss: -0.035028696060180664
Batch 34/64 loss: -0.021279335021972656
Batch 35/64 loss: -0.025168299674987793
Batch 36/64 loss: -0.036938369274139404
Batch 37/64 loss: -0.02108532190322876
Batch 38/64 loss: -0.011608719825744629
Batch 39/64 loss: -0.02771049737930298
Batch 40/64 loss: -0.021362364292144775
Batch 41/64 loss: -0.02154719829559326
Batch 42/64 loss: -0.033502161502838135
Batch 43/64 loss: -0.02345794439315796
Batch 44/64 loss: -0.010805845260620117
Batch 45/64 loss: -0.028576195240020752
Batch 46/64 loss: 0.0022121667861938477
Batch 47/64 loss: -0.0017829537391662598
Batch 48/64 loss: -0.01566380262374878
Batch 49/64 loss: -0.025614500045776367
Batch 50/64 loss: -0.014893889427185059
Batch 51/64 loss: -0.008817434310913086
Batch 52/64 loss: -0.026205599308013916
Batch 53/64 loss: -0.020548522472381592
Batch 54/64 loss: -0.018165171146392822
Batch 55/64 loss: -0.013661384582519531
Batch 56/64 loss: -0.029226481914520264
Batch 57/64 loss: -0.01913607120513916
Batch 58/64 loss: -0.01790320873260498
Batch 59/64 loss: -0.015854060649871826
Batch 60/64 loss: -0.032457828521728516
Batch 61/64 loss: -0.011211633682250977
Batch 62/64 loss: -0.029255032539367676
Batch 63/64 loss: -0.017762362957000732
Batch 64/64 loss: 0.0037078857421875
Epoch 365  Train loss: -0.02070010409635656  Val loss: 0.09114145413297149
Epoch 366
-------------------------------
Batch 1/64 loss: -0.04473984241485596
Batch 2/64 loss: -0.022913575172424316
Batch 3/64 loss: -0.006092727184295654
Batch 4/64 loss: -0.01699817180633545
Batch 5/64 loss: -0.012433350086212158
Batch 6/64 loss: -0.021556973457336426
Batch 7/64 loss: -0.01935797929763794
Batch 8/64 loss: -0.01607799530029297
Batch 9/64 loss: -0.02158939838409424
Batch 10/64 loss: -0.024263620376586914
Batch 11/64 loss: -0.015735089778900146
Batch 12/64 loss: -0.02419489622116089
Batch 13/64 loss: -0.013926506042480469
Batch 14/64 loss: -0.03285253047943115
Batch 15/64 loss: -0.02429717779159546
Batch 16/64 loss: -0.013149619102478027
Batch 17/64 loss: -0.02827632427215576
Batch 18/64 loss: -0.021601557731628418
Batch 19/64 loss: -0.020044326782226562
Batch 20/64 loss: -0.022941112518310547
Batch 21/64 loss: -0.024795830249786377
Batch 22/64 loss: -0.020667433738708496
Batch 23/64 loss: 0.0025025606155395508
Batch 24/64 loss: -0.03853178024291992
Batch 25/64 loss: -0.0037454962730407715
Batch 26/64 loss: -0.02049320936203003
Batch 27/64 loss: -0.027141988277435303
Batch 28/64 loss: -0.018867790699005127
Batch 29/64 loss: -0.015445232391357422
Batch 30/64 loss: -0.026986360549926758
Batch 31/64 loss: -0.013562560081481934
Batch 32/64 loss: -0.018956899642944336
Batch 33/64 loss: -0.023677408695220947
Batch 34/64 loss: -0.01852017641067505
Batch 35/64 loss: -0.022694826126098633
Batch 36/64 loss: -0.0035538077354431152
Batch 37/64 loss: -0.016120314598083496
Batch 38/64 loss: -0.04174387454986572
Batch 39/64 loss: -0.017355382442474365
Batch 40/64 loss: -0.03605014085769653
Batch 41/64 loss: -0.02278512716293335
Batch 42/64 loss: -0.026568353176116943
Batch 43/64 loss: -0.01241755485534668
Batch 44/64 loss: -0.03672516345977783
Batch 45/64 loss: -0.018835604190826416
Batch 46/64 loss: -0.029106438159942627
Batch 47/64 loss: -0.023315250873565674
Batch 48/64 loss: -0.030097246170043945
Batch 49/64 loss: -0.019841790199279785
Batch 50/64 loss: -0.03012627363204956
Batch 51/64 loss: -0.02193361520767212
Batch 52/64 loss: -0.019760608673095703
Batch 53/64 loss: -0.01344919204711914
Batch 54/64 loss: -0.03564482927322388
Batch 55/64 loss: -0.032237708568573
Batch 56/64 loss: -0.016354620456695557
Batch 57/64 loss: -0.030394673347473145
Batch 58/64 loss: -0.02548384666442871
Batch 59/64 loss: -0.025267183780670166
Batch 60/64 loss: -0.011543691158294678
Batch 61/64 loss: -0.03338807821273804
Batch 62/64 loss: -0.012830197811126709
Batch 63/64 loss: -0.0016315579414367676
Batch 64/64 loss: 0.013519525527954102
Epoch 366  Train loss: -0.021161563723695046  Val loss: 0.09314970605561823
Epoch 367
-------------------------------
Batch 1/64 loss: -0.024016618728637695
Batch 2/64 loss: -0.0341833233833313
Batch 3/64 loss: -0.025658130645751953
Batch 4/64 loss: -0.019690752029418945
Batch 5/64 loss: 0.010218560695648193
Batch 6/64 loss: -0.023958325386047363
Batch 7/64 loss: -0.02534031867980957
Batch 8/64 loss: -0.02423638105392456
Batch 9/64 loss: -0.004590511322021484
Batch 10/64 loss: -0.03140747547149658
Batch 11/64 loss: -0.009089767932891846
Batch 12/64 loss: -0.015598177909851074
Batch 13/64 loss: -0.014047324657440186
Batch 14/64 loss: -0.014700174331665039
Batch 15/64 loss: -0.018724679946899414
Batch 16/64 loss: -0.018395304679870605
Batch 17/64 loss: -0.003101944923400879
Batch 18/64 loss: -0.008345067501068115
Batch 19/64 loss: -0.0139542818069458
Batch 20/64 loss: -0.03276163339614868
Batch 21/64 loss: -0.009163379669189453
Batch 22/64 loss: -0.014469563961029053
Batch 23/64 loss: -0.03384894132614136
Batch 24/64 loss: -0.031600773334503174
Batch 25/64 loss: -0.007295489311218262
Batch 26/64 loss: -0.02640324831008911
Batch 27/64 loss: -0.02163243293762207
Batch 28/64 loss: -0.02430546283721924
Batch 29/64 loss: 0.008053958415985107
Batch 30/64 loss: -0.025384485721588135
Batch 31/64 loss: -0.020819485187530518
Batch 32/64 loss: -0.002245485782623291
Batch 33/64 loss: -0.029858529567718506
Batch 34/64 loss: -0.017441093921661377
Batch 35/64 loss: -0.01418757438659668
Batch 36/64 loss: -0.02792489528656006
Batch 37/64 loss: -0.01653057336807251
Batch 38/64 loss: -0.014166593551635742
Batch 39/64 loss: -0.020175814628601074
Batch 40/64 loss: -0.008925020694732666
Batch 41/64 loss: -0.019498229026794434
Batch 42/64 loss: -0.028539061546325684
Batch 43/64 loss: -0.021075010299682617
Batch 44/64 loss: -0.012765884399414062
Batch 45/64 loss: -0.016600430011749268
Batch 46/64 loss: -0.017121434211730957
Batch 47/64 loss: -0.02699202299118042
Batch 48/64 loss: -0.023735642433166504
Batch 49/64 loss: -0.0036435723304748535
Batch 50/64 loss: -0.022604763507843018
Batch 51/64 loss: -0.03142523765563965
Batch 52/64 loss: -0.023458123207092285
Batch 53/64 loss: -0.015815258026123047
Batch 54/64 loss: -0.018788635730743408
Batch 55/64 loss: -0.027844250202178955
Batch 56/64 loss: -0.018972575664520264
Batch 57/64 loss: -0.012880086898803711
Batch 58/64 loss: -0.011155903339385986
Batch 59/64 loss: -0.026906073093414307
Batch 60/64 loss: -0.02635890245437622
Batch 61/64 loss: -0.02659320831298828
Batch 62/64 loss: -0.020389258861541748
Batch 63/64 loss: 0.003352820873260498
Batch 64/64 loss: -0.012193858623504639
Epoch 367  Train loss: -0.018335100482491887  Val loss: 0.09238124754011016
Epoch 368
-------------------------------
Batch 1/64 loss: -0.02991318702697754
Batch 2/64 loss: -0.04140287637710571
Batch 3/64 loss: -0.029413878917694092
Batch 4/64 loss: -0.011867165565490723
Batch 5/64 loss: -0.024861156940460205
Batch 6/64 loss: -0.03011918067932129
Batch 7/64 loss: -0.015501081943511963
Batch 8/64 loss: -0.02218186855316162
Batch 9/64 loss: -0.020352423191070557
Batch 10/64 loss: -0.02223825454711914
Batch 11/64 loss: -0.018726825714111328
Batch 12/64 loss: -0.029773235321044922
Batch 13/64 loss: -0.036110639572143555
Batch 14/64 loss: -0.029627561569213867
Batch 15/64 loss: -0.03397798538208008
Batch 16/64 loss: -0.011765122413635254
Batch 17/64 loss: -0.02808403968811035
Batch 18/64 loss: -0.02299278974533081
Batch 19/64 loss: -0.01819223165512085
Batch 20/64 loss: -0.025876402854919434
Batch 21/64 loss: -0.01961284875869751
Batch 22/64 loss: -0.0347633957862854
Batch 23/64 loss: -0.017238140106201172
Batch 24/64 loss: -0.03850489854812622
Batch 25/64 loss: -0.026076853275299072
Batch 26/64 loss: -0.008218765258789062
Batch 27/64 loss: -0.023363232612609863
Batch 28/64 loss: -0.007645010948181152
Batch 29/64 loss: 0.0036090612411499023
Batch 30/64 loss: -0.005421042442321777
Batch 31/64 loss: -0.0018374919891357422
Batch 32/64 loss: -0.03260070085525513
Batch 33/64 loss: -0.027469098567962646
Batch 34/64 loss: -0.031575679779052734
Batch 35/64 loss: -0.0236966609954834
Batch 36/64 loss: -0.02840954065322876
Batch 37/64 loss: -0.03743642568588257
Batch 38/64 loss: -0.013195276260375977
Batch 39/64 loss: -0.015977203845977783
Batch 40/64 loss: -0.011761188507080078
Batch 41/64 loss: -0.002166450023651123
Batch 42/64 loss: -0.0199815034866333
Batch 43/64 loss: -0.015488147735595703
Batch 44/64 loss: -0.03207153081893921
Batch 45/64 loss: -0.022562026977539062
Batch 46/64 loss: -0.02069026231765747
Batch 47/64 loss: -0.01741093397140503
Batch 48/64 loss: -0.017771124839782715
Batch 49/64 loss: -0.0130842924118042
Batch 50/64 loss: -0.029706835746765137
Batch 51/64 loss: -0.023527681827545166
Batch 52/64 loss: -0.03847706317901611
Batch 53/64 loss: -0.006022095680236816
Batch 54/64 loss: -0.012020647525787354
Batch 55/64 loss: -0.031891703605651855
Batch 56/64 loss: -0.031123995780944824
Batch 57/64 loss: -0.020489990711212158
Batch 58/64 loss: -0.017492294311523438
Batch 59/64 loss: -0.0342290997505188
Batch 60/64 loss: -0.02613961696624756
Batch 61/64 loss: -0.026738107204437256
Batch 62/64 loss: -0.021598339080810547
Batch 63/64 loss: -0.010688304901123047
Batch 64/64 loss: -0.0026261210441589355
Epoch 368  Train loss: -0.02192178730871163  Val loss: 0.09264971320981422
Epoch 369
-------------------------------
Batch 1/64 loss: -0.025837182998657227
Batch 2/64 loss: -0.043351590633392334
Batch 3/64 loss: -0.036566853523254395
Batch 4/64 loss: -0.024526357650756836
Batch 5/64 loss: -0.0196225643157959
Batch 6/64 loss: -0.014134526252746582
Batch 7/64 loss: -0.02426856756210327
Batch 8/64 loss: -0.03137946128845215
Batch 9/64 loss: -0.028471171855926514
Batch 10/64 loss: -0.023088395595550537
Batch 11/64 loss: -0.028313279151916504
Batch 12/64 loss: -0.017653703689575195
Batch 13/64 loss: -0.02776271104812622
Batch 14/64 loss: -0.0361858606338501
Batch 15/64 loss: -0.03166019916534424
Batch 16/64 loss: -0.026728808879852295
Batch 17/64 loss: -0.021434485912322998
Batch 18/64 loss: -0.03889882564544678
Batch 19/64 loss: -0.02282959222793579
Batch 20/64 loss: -0.011225402355194092
Batch 21/64 loss: -0.013412117958068848
Batch 22/64 loss: -0.03316545486450195
Batch 23/64 loss: -0.021794259548187256
Batch 24/64 loss: -0.026659250259399414
Batch 25/64 loss: -0.015929102897644043
Batch 26/64 loss: -0.03279221057891846
Batch 27/64 loss: -0.03369152545928955
Batch 28/64 loss: -0.04041457176208496
Batch 29/64 loss: 0.0015804171562194824
Batch 30/64 loss: -0.013697206974029541
Batch 31/64 loss: -0.005859851837158203
Batch 32/64 loss: -0.029328644275665283
Batch 33/64 loss: -0.01549607515335083
Batch 34/64 loss: -0.03604847192764282
Batch 35/64 loss: -0.027752399444580078
Batch 36/64 loss: -0.026029586791992188
Batch 37/64 loss: -0.027706801891326904
Batch 38/64 loss: -0.02537250518798828
Batch 39/64 loss: -0.014471948146820068
Batch 40/64 loss: -0.028006553649902344
Batch 41/64 loss: -0.02057856321334839
Batch 42/64 loss: -0.0229300856590271
Batch 43/64 loss: -0.006185948848724365
Batch 44/64 loss: -0.023152172565460205
Batch 45/64 loss: -0.03537547588348389
Batch 46/64 loss: -0.019736826419830322
Batch 47/64 loss: -0.019721686840057373
Batch 48/64 loss: -0.033740341663360596
Batch 49/64 loss: -0.005328714847564697
Batch 50/64 loss: -0.017806410789489746
Batch 51/64 loss: 0.01703202724456787
Batch 52/64 loss: -0.022424042224884033
Batch 53/64 loss: 0.0053029656410217285
Batch 54/64 loss: -0.013314247131347656
Batch 55/64 loss: 0.008886754512786865
Batch 56/64 loss: -0.005071461200714111
Batch 57/64 loss: -0.015551269054412842
Batch 58/64 loss: -0.02508401870727539
Batch 59/64 loss: -0.010419249534606934
Batch 60/64 loss: -0.008241474628448486
Batch 61/64 loss: -0.014811813831329346
Batch 62/64 loss: -0.01652318239212036
Batch 63/64 loss: -0.02699875831604004
Batch 64/64 loss: -0.013087093830108643
Epoch 369  Train loss: -0.02104434476179235  Val loss: 0.09239919283955368
Epoch 370
-------------------------------
Batch 1/64 loss: -0.022956907749176025
Batch 2/64 loss: -0.026538550853729248
Batch 3/64 loss: -0.01614534854888916
Batch 4/64 loss: -0.014309167861938477
Batch 5/64 loss: -0.03479933738708496
Batch 6/64 loss: -0.027128100395202637
Batch 7/64 loss: -0.031104981899261475
Batch 8/64 loss: -0.006997108459472656
Batch 9/64 loss: -0.004323601722717285
Batch 10/64 loss: -0.028594970703125
Batch 11/64 loss: -0.025249719619750977
Batch 12/64 loss: -0.01517188549041748
Batch 13/64 loss: -0.018819808959960938
Batch 14/64 loss: 0.0019555091857910156
Batch 15/64 loss: -0.030258655548095703
Batch 16/64 loss: -0.02483898401260376
Batch 17/64 loss: -0.02727264165878296
Batch 18/64 loss: -0.022855937480926514
Batch 19/64 loss: -0.033322930335998535
Batch 20/64 loss: -0.012417316436767578
Batch 21/64 loss: -0.03166717290878296
Batch 22/64 loss: -0.022482872009277344
Batch 23/64 loss: -0.024481356143951416
Batch 24/64 loss: -0.03862345218658447
Batch 25/64 loss: -0.020513296127319336
Batch 26/64 loss: -0.041160762310028076
Batch 27/64 loss: -0.031746625900268555
Batch 28/64 loss: -0.027853429317474365
Batch 29/64 loss: -0.01834237575531006
Batch 30/64 loss: -0.0409771203994751
Batch 31/64 loss: -0.023508548736572266
Batch 32/64 loss: -0.007977724075317383
Batch 33/64 loss: -0.03449910879135132
Batch 34/64 loss: -0.02556788921356201
Batch 35/64 loss: -0.02626669406890869
Batch 36/64 loss: -0.0351068377494812
Batch 37/64 loss: -0.019841313362121582
Batch 38/64 loss: -0.030331790447235107
Batch 39/64 loss: -0.016544759273529053
Batch 40/64 loss: -0.014903545379638672
Batch 41/64 loss: -0.017347514629364014
Batch 42/64 loss: -0.027288079261779785
Batch 43/64 loss: -0.03197896480560303
Batch 44/64 loss: -0.020692169666290283
Batch 45/64 loss: -0.016573786735534668
Batch 46/64 loss: -0.02358168363571167
Batch 47/64 loss: -0.04057896137237549
Batch 48/64 loss: -0.0143815279006958
Batch 49/64 loss: -0.03181833028793335
Batch 50/64 loss: -0.03331702947616577
Batch 51/64 loss: -0.023885667324066162
Batch 52/64 loss: -0.027980566024780273
Batch 53/64 loss: -0.020537972450256348
Batch 54/64 loss: -0.016535162925720215
Batch 55/64 loss: -0.02744448184967041
Batch 56/64 loss: -0.009883105754852295
Batch 57/64 loss: -0.011988282203674316
Batch 58/64 loss: -0.0013639330863952637
Batch 59/64 loss: -0.028644800186157227
Batch 60/64 loss: -0.01683562994003296
Batch 61/64 loss: -0.0138511061668396
Batch 62/64 loss: -0.019154906272888184
Batch 63/64 loss: -0.028115153312683105
Batch 64/64 loss: -0.006378471851348877
Epoch 370  Train loss: -0.022935055517682844  Val loss: 0.09205758776451714
Epoch 371
-------------------------------
Batch 1/64 loss: -0.017947793006896973
Batch 2/64 loss: -0.023586809635162354
Batch 3/64 loss: -0.004992842674255371
Batch 4/64 loss: -0.02737605571746826
Batch 5/64 loss: -0.0336032509803772
Batch 6/64 loss: -0.02039569616317749
Batch 7/64 loss: -0.0214693546295166
Batch 8/64 loss: -0.025133907794952393
Batch 9/64 loss: -0.033254384994506836
Batch 10/64 loss: -0.033907413482666016
Batch 11/64 loss: -0.027194559574127197
Batch 12/64 loss: -0.015892982482910156
Batch 13/64 loss: -0.015085875988006592
Batch 14/64 loss: -0.026520073413848877
Batch 15/64 loss: -0.031039178371429443
Batch 16/64 loss: -0.023632943630218506
Batch 17/64 loss: -0.016234397888183594
Batch 18/64 loss: -0.029047369956970215
Batch 19/64 loss: -0.04178255796432495
Batch 20/64 loss: -0.032148540019989014
Batch 21/64 loss: -0.03220182657241821
Batch 22/64 loss: -0.037393808364868164
Batch 23/64 loss: -0.04066592454910278
Batch 24/64 loss: -0.025642216205596924
Batch 25/64 loss: -0.025831103324890137
Batch 26/64 loss: -0.013039231300354004
Batch 27/64 loss: -0.006385445594787598
Batch 28/64 loss: -0.0234372615814209
Batch 29/64 loss: -0.021812081336975098
Batch 30/64 loss: 0.002309441566467285
Batch 31/64 loss: -0.02799886465072632
Batch 32/64 loss: 0.0006339550018310547
Batch 33/64 loss: 0.002107977867126465
Batch 34/64 loss: -0.0169680118560791
Batch 35/64 loss: -0.019537031650543213
Batch 36/64 loss: -0.020009636878967285
Batch 37/64 loss: -0.011210083961486816
Batch 38/64 loss: -0.019612908363342285
Batch 39/64 loss: -0.020099759101867676
Batch 40/64 loss: -0.006705880165100098
Batch 41/64 loss: -0.02995365858078003
Batch 42/64 loss: -0.03437262773513794
Batch 43/64 loss: -0.00716853141784668
Batch 44/64 loss: -0.006195187568664551
Batch 45/64 loss: -0.021467089653015137
Batch 46/64 loss: -0.02269965410232544
Batch 47/64 loss: -0.012041926383972168
Batch 48/64 loss: -0.024610042572021484
Batch 49/64 loss: -0.00909876823425293
Batch 50/64 loss: -0.02204686403274536
Batch 51/64 loss: -0.02289748191833496
Batch 52/64 loss: -0.0032073259353637695
Batch 53/64 loss: -0.010955095291137695
Batch 54/64 loss: -0.023126542568206787
Batch 55/64 loss: -0.024775922298431396
Batch 56/64 loss: -0.022960662841796875
Batch 57/64 loss: -0.008325636386871338
Batch 58/64 loss: -0.01905655860900879
Batch 59/64 loss: -0.008313655853271484
Batch 60/64 loss: -0.028471767902374268
Batch 61/64 loss: -0.027302324771881104
Batch 62/64 loss: -0.025094270706176758
Batch 63/64 loss: -0.03018021583557129
Batch 64/64 loss: -0.009186983108520508
Epoch 371  Train loss: -0.02062678804584578  Val loss: 0.09312163349689077
Epoch 372
-------------------------------
Batch 1/64 loss: -0.04030656814575195
Batch 2/64 loss: -0.025392770767211914
Batch 3/64 loss: -0.007947087287902832
Batch 4/64 loss: 0.0017438530921936035
Batch 5/64 loss: -0.018551886081695557
Batch 6/64 loss: -0.029898762702941895
Batch 7/64 loss: -0.02011817693710327
Batch 8/64 loss: 0.009618222713470459
Batch 9/64 loss: 0.0015864968299865723
Batch 10/64 loss: -0.02952277660369873
Batch 11/64 loss: -0.02146965265274048
Batch 12/64 loss: -0.02358531951904297
Batch 13/64 loss: -0.03592759370803833
Batch 14/64 loss: -0.03255397081375122
Batch 15/64 loss: -0.01271212100982666
Batch 16/64 loss: -0.009107232093811035
Batch 17/64 loss: -0.005543112754821777
Batch 18/64 loss: -0.032005369663238525
Batch 19/64 loss: -0.01814478635787964
Batch 20/64 loss: 0.010263562202453613
Batch 21/64 loss: -0.0063245296478271484
Batch 22/64 loss: -0.0027714967727661133
Batch 23/64 loss: -0.03350859880447388
Batch 24/64 loss: -0.018078267574310303
Batch 25/64 loss: -0.02890104055404663
Batch 26/64 loss: -0.02432352304458618
Batch 27/64 loss: -0.019077837467193604
Batch 28/64 loss: -0.03024578094482422
Batch 29/64 loss: -0.00845479965209961
Batch 30/64 loss: -0.009105503559112549
Batch 31/64 loss: -0.01906132698059082
Batch 32/64 loss: -0.01149129867553711
Batch 33/64 loss: -0.018419086933135986
Batch 34/64 loss: -0.006041884422302246
Batch 35/64 loss: -0.020061135292053223
Batch 36/64 loss: -0.020691752433776855
Batch 37/64 loss: -0.02001631259918213
Batch 38/64 loss: -0.033969879150390625
Batch 39/64 loss: -0.026222169399261475
Batch 40/64 loss: -0.006035029888153076
Batch 41/64 loss: -0.035821735858917236
Batch 42/64 loss: -0.035652756690979004
Batch 43/64 loss: -0.023642003536224365
Batch 44/64 loss: -0.021990597248077393
Batch 45/64 loss: -0.02686631679534912
Batch 46/64 loss: -0.00941544771194458
Batch 47/64 loss: -0.010340571403503418
Batch 48/64 loss: -0.017626821994781494
Batch 49/64 loss: -0.017153441905975342
Batch 50/64 loss: -0.012607157230377197
Batch 51/64 loss: -0.03272974491119385
Batch 52/64 loss: -0.007319390773773193
Batch 53/64 loss: -0.03494685888290405
Batch 54/64 loss: -0.03276801109313965
Batch 55/64 loss: -0.022100389003753662
Batch 56/64 loss: -0.024421095848083496
Batch 57/64 loss: -0.03013920783996582
Batch 58/64 loss: -0.023542821407318115
Batch 59/64 loss: -0.023929238319396973
Batch 60/64 loss: -0.01886838674545288
Batch 61/64 loss: -0.025025367736816406
Batch 62/64 loss: -0.030601978302001953
Batch 63/64 loss: -0.028978407382965088
Batch 64/64 loss: -0.01445162296295166
Epoch 372  Train loss: -0.01976004348081701  Val loss: 0.0926112954559195
Epoch 373
-------------------------------
Batch 1/64 loss: -0.020241081714630127
Batch 2/64 loss: -0.030629396438598633
Batch 3/64 loss: -0.02577883005142212
Batch 4/64 loss: -0.03192859888076782
Batch 5/64 loss: -0.025342226028442383
Batch 6/64 loss: -0.02769017219543457
Batch 7/64 loss: -0.010999202728271484
Batch 8/64 loss: -0.037061214447021484
Batch 9/64 loss: -0.0034164786338806152
Batch 10/64 loss: -0.041000545024871826
Batch 11/64 loss: -0.030804574489593506
Batch 12/64 loss: -0.03522998094558716
Batch 13/64 loss: -0.02647155523300171
Batch 14/64 loss: -0.026912331581115723
Batch 15/64 loss: -0.0289914608001709
Batch 16/64 loss: -0.008803963661193848
Batch 17/64 loss: -0.019815146923065186
Batch 18/64 loss: -0.033161163330078125
Batch 19/64 loss: -0.02993232011795044
Batch 20/64 loss: -0.02825009822845459
Batch 21/64 loss: -0.03582829236984253
Batch 22/64 loss: -0.03352254629135132
Batch 23/64 loss: -0.017584562301635742
Batch 24/64 loss: -0.00465470552444458
Batch 25/64 loss: -0.014027893543243408
Batch 26/64 loss: -0.03060370683670044
Batch 27/64 loss: -0.02955472469329834
Batch 28/64 loss: -0.028498411178588867
Batch 29/64 loss: -0.029921412467956543
Batch 30/64 loss: -0.026377320289611816
Batch 31/64 loss: -0.02739083766937256
Batch 32/64 loss: -0.03871983289718628
Batch 33/64 loss: -0.028387367725372314
Batch 34/64 loss: -0.03346329927444458
Batch 35/64 loss: -0.02010244131088257
Batch 36/64 loss: -0.04230046272277832
Batch 37/64 loss: -0.01187962293624878
Batch 38/64 loss: -0.02623128890991211
Batch 39/64 loss: -0.007056832313537598
Batch 40/64 loss: -0.03160184621810913
Batch 41/64 loss: -0.01292729377746582
Batch 42/64 loss: -0.036633193492889404
Batch 43/64 loss: -0.01823437213897705
Batch 44/64 loss: -0.02374410629272461
Batch 45/64 loss: -0.03381919860839844
Batch 46/64 loss: -0.03412926197052002
Batch 47/64 loss: -0.019229352474212646
Batch 48/64 loss: -0.01893848180770874
Batch 49/64 loss: -0.033383071422576904
Batch 50/64 loss: -0.011766672134399414
Batch 51/64 loss: -0.0019412040710449219
Batch 52/64 loss: -0.026988983154296875
Batch 53/64 loss: -0.02953648567199707
Batch 54/64 loss: -0.03182780742645264
Batch 55/64 loss: -0.016541600227355957
Batch 56/64 loss: -0.021886706352233887
Batch 57/64 loss: -0.021448850631713867
Batch 58/64 loss: -0.033869802951812744
Batch 59/64 loss: -0.028262674808502197
Batch 60/64 loss: -0.013355016708374023
Batch 61/64 loss: -0.022728562355041504
Batch 62/64 loss: -0.031798601150512695
Batch 63/64 loss: -0.014169454574584961
Batch 64/64 loss: -0.03010958433151245
Epoch 373  Train loss: -0.02509663829616472  Val loss: 0.09233033861900933
Epoch 374
-------------------------------
Batch 1/64 loss: -0.02769482135772705
Batch 2/64 loss: -0.026912391185760498
Batch 3/64 loss: -0.02657395601272583
Batch 4/64 loss: -0.03238332271575928
Batch 5/64 loss: -0.027526795864105225
Batch 6/64 loss: -0.009852766990661621
Batch 7/64 loss: -0.014359652996063232
Batch 8/64 loss: -0.033037543296813965
Batch 9/64 loss: -0.01877683401107788
Batch 10/64 loss: -0.034474968910217285
Batch 11/64 loss: -0.03238123655319214
Batch 12/64 loss: -0.021758854389190674
Batch 13/64 loss: -0.03435218334197998
Batch 14/64 loss: -0.01582658290863037
Batch 15/64 loss: -0.029561758041381836
Batch 16/64 loss: -0.03791159391403198
Batch 17/64 loss: -0.024235844612121582
Batch 18/64 loss: -0.015008985996246338
Batch 19/64 loss: -0.021706044673919678
Batch 20/64 loss: -0.024561822414398193
Batch 21/64 loss: -0.030528128147125244
Batch 22/64 loss: -0.034075021743774414
Batch 23/64 loss: -0.031701087951660156
Batch 24/64 loss: -0.02716583013534546
Batch 25/64 loss: -0.018734514713287354
Batch 26/64 loss: -0.03671640157699585
Batch 27/64 loss: -0.028043925762176514
Batch 28/64 loss: -0.03134948015213013
Batch 29/64 loss: -0.018829524517059326
Batch 30/64 loss: -0.027521967887878418
Batch 31/64 loss: -0.03902137279510498
Batch 32/64 loss: 0.0020813345909118652
Batch 33/64 loss: -0.031121671199798584
Batch 34/64 loss: 0.027303576469421387
Batch 35/64 loss: -0.020111560821533203
Batch 36/64 loss: -0.01847553253173828
Batch 37/64 loss: -0.017192542552947998
Batch 38/64 loss: 0.0027518868446350098
Batch 39/64 loss: -0.021600723266601562
Batch 40/64 loss: -0.029607295989990234
Batch 41/64 loss: -0.0270463228225708
Batch 42/64 loss: -0.020411908626556396
Batch 43/64 loss: -0.0264776349067688
Batch 44/64 loss: -0.03325474262237549
Batch 45/64 loss: -0.02130216360092163
Batch 46/64 loss: -0.029826045036315918
Batch 47/64 loss: -0.026204824447631836
Batch 48/64 loss: -0.029161036014556885
Batch 49/64 loss: -0.017916321754455566
Batch 50/64 loss: -0.04032176733016968
Batch 51/64 loss: -0.015153050422668457
Batch 52/64 loss: -0.027132928371429443
Batch 53/64 loss: -0.03135347366333008
Batch 54/64 loss: -0.016391098499298096
Batch 55/64 loss: -0.040112972259521484
Batch 56/64 loss: -0.015280485153198242
Batch 57/64 loss: -0.020742356777191162
Batch 58/64 loss: -0.031946778297424316
Batch 59/64 loss: -0.028606951236724854
Batch 60/64 loss: -0.021191060543060303
Batch 61/64 loss: -0.008982241153717041
Batch 62/64 loss: -0.02337747812271118
Batch 63/64 loss: -0.001055300235748291
Batch 64/64 loss: -0.0011793971061706543
Epoch 374  Train loss: -0.023414764451045615  Val loss: 0.09469301839874372
Epoch 375
-------------------------------
Batch 1/64 loss: -0.02345496416091919
Batch 2/64 loss: -0.013052284717559814
Batch 3/64 loss: -0.020999610424041748
Batch 4/64 loss: -0.03821009397506714
Batch 5/64 loss: -0.019660651683807373
Batch 6/64 loss: -0.0207974910736084
Batch 7/64 loss: -0.028665602207183838
Batch 8/64 loss: -0.033367037773132324
Batch 9/64 loss: -0.027527987957000732
Batch 10/64 loss: -0.018936514854431152
Batch 11/64 loss: -0.03328132629394531
Batch 12/64 loss: -0.03748476505279541
Batch 13/64 loss: -0.039425015449523926
Batch 14/64 loss: -0.02478182315826416
Batch 15/64 loss: -0.033299148082733154
Batch 16/64 loss: -0.03878599405288696
Batch 17/64 loss: -0.020613133907318115
Batch 18/64 loss: -0.025226235389709473
Batch 19/64 loss: -0.02028292417526245
Batch 20/64 loss: -0.011106431484222412
Batch 21/64 loss: -0.0259857177734375
Batch 22/64 loss: -0.027613461017608643
Batch 23/64 loss: -0.024470090866088867
Batch 24/64 loss: -0.0069272518157958984
Batch 25/64 loss: -0.020586013793945312
Batch 26/64 loss: -0.03330951929092407
Batch 27/64 loss: -0.01618725061416626
Batch 28/64 loss: -0.022818148136138916
Batch 29/64 loss: -0.021258294582366943
Batch 30/64 loss: -0.010255396366119385
Batch 31/64 loss: -0.03144890069961548
Batch 32/64 loss: -0.037965476512908936
Batch 33/64 loss: -0.02300870418548584
Batch 34/64 loss: -0.005126237869262695
Batch 35/64 loss: -0.024238407611846924
Batch 36/64 loss: -0.022975564002990723
Batch 37/64 loss: -0.03215491771697998
Batch 38/64 loss: -0.03362715244293213
Batch 39/64 loss: -0.03974604606628418
Batch 40/64 loss: -0.017999768257141113
Batch 41/64 loss: -0.022111356258392334
Batch 42/64 loss: -0.03307199478149414
Batch 43/64 loss: -0.01870870590209961
Batch 44/64 loss: -0.018299639225006104
Batch 45/64 loss: -0.027025938034057617
Batch 46/64 loss: -0.03512769937515259
Batch 47/64 loss: -0.024104654788970947
Batch 48/64 loss: -0.03083258867263794
Batch 49/64 loss: -0.011645317077636719
Batch 50/64 loss: -0.030980587005615234
Batch 51/64 loss: -0.032100558280944824
Batch 52/64 loss: -0.037241578102111816
Batch 53/64 loss: -0.03854942321777344
Batch 54/64 loss: -0.030175983905792236
Batch 55/64 loss: -0.020924806594848633
Batch 56/64 loss: -0.018106400966644287
Batch 57/64 loss: -0.03261524438858032
Batch 58/64 loss: -0.04286879301071167
Batch 59/64 loss: -0.03709644079208374
Batch 60/64 loss: -0.0031360387802124023
Batch 61/64 loss: -0.018463969230651855
Batch 62/64 loss: -0.023949503898620605
Batch 63/64 loss: -0.031651973724365234
Batch 64/64 loss: -0.009417176246643066
Epoch 375  Train loss: -0.025608053861879834  Val loss: 0.09215682601600987
Epoch 376
-------------------------------
Batch 1/64 loss: -0.04797947406768799
Batch 2/64 loss: -0.025806844234466553
Batch 3/64 loss: -0.0008940696716308594
Batch 4/64 loss: -0.02030271291732788
Batch 5/64 loss: -0.03441917896270752
Batch 6/64 loss: -0.03342992067337036
Batch 7/64 loss: -0.020118117332458496
Batch 8/64 loss: -0.02619040012359619
Batch 9/64 loss: -0.0341181755065918
Batch 10/64 loss: -0.03002852201461792
Batch 11/64 loss: -0.02389746904373169
Batch 12/64 loss: 0.003808736801147461
Batch 13/64 loss: -0.040239691734313965
Batch 14/64 loss: -0.027864575386047363
Batch 15/64 loss: -0.030343711376190186
Batch 16/64 loss: -0.025734663009643555
Batch 17/64 loss: -0.026466965675354004
Batch 18/64 loss: -0.02301192283630371
Batch 19/64 loss: -0.0019083619117736816
Batch 20/64 loss: -0.04248851537704468
Batch 21/64 loss: -0.03365236520767212
Batch 22/64 loss: -0.006568968296051025
Batch 23/64 loss: -0.0284956693649292
Batch 24/64 loss: -0.03095841407775879
Batch 25/64 loss: -0.032090067863464355
Batch 26/64 loss: -0.024605512619018555
Batch 27/64 loss: -0.013648688793182373
Batch 28/64 loss: -0.03221327066421509
Batch 29/64 loss: -0.01635611057281494
Batch 30/64 loss: -0.005890488624572754
Batch 31/64 loss: -0.02167332172393799
Batch 32/64 loss: -0.014636993408203125
Batch 33/64 loss: -0.0076667070388793945
Batch 34/64 loss: -0.021068930625915527
Batch 35/64 loss: -0.032024502754211426
Batch 36/64 loss: -0.02583557367324829
Batch 37/64 loss: -0.02104973793029785
Batch 38/64 loss: -0.0151824951171875
Batch 39/64 loss: -0.030641615390777588
Batch 40/64 loss: -0.03304100036621094
Batch 41/64 loss: -0.03317904472351074
Batch 42/64 loss: -0.022261202335357666
Batch 43/64 loss: -0.03226006031036377
Batch 44/64 loss: -0.026482760906219482
Batch 45/64 loss: -0.019299209117889404
Batch 46/64 loss: -0.02410811185836792
Batch 47/64 loss: -0.016712665557861328
Batch 48/64 loss: -0.03273957967758179
Batch 49/64 loss: -0.028652429580688477
Batch 50/64 loss: -0.017331480979919434
Batch 51/64 loss: -0.03328216075897217
Batch 52/64 loss: -0.020077109336853027
Batch 53/64 loss: -0.017909765243530273
Batch 54/64 loss: -0.022515177726745605
Batch 55/64 loss: -0.038643479347229004
Batch 56/64 loss: -0.03650069236755371
Batch 57/64 loss: -0.03265887498855591
Batch 58/64 loss: -0.035849571228027344
Batch 59/64 loss: -0.04635727405548096
Batch 60/64 loss: -0.034428536891937256
Batch 61/64 loss: -0.021175861358642578
Batch 62/64 loss: -0.022680461406707764
Batch 63/64 loss: -0.019768714904785156
Batch 64/64 loss: -0.00553584098815918
Epoch 376  Train loss: -0.025062919130512312  Val loss: 0.09242862975064832
Epoch 377
-------------------------------
Batch 1/64 loss: -0.03895533084869385
Batch 2/64 loss: -0.022317111492156982
Batch 3/64 loss: -0.037318646907806396
Batch 4/64 loss: -0.03402745723724365
Batch 5/64 loss: -0.03585696220397949
Batch 6/64 loss: -0.02810680866241455
Batch 7/64 loss: -0.02425849437713623
Batch 8/64 loss: -0.030725061893463135
Batch 9/64 loss: -0.03751015663146973
Batch 10/64 loss: -0.03841567039489746
Batch 11/64 loss: -0.03181743621826172
Batch 12/64 loss: -0.013927340507507324
Batch 13/64 loss: -0.04349350929260254
Batch 14/64 loss: -0.03661549091339111
Batch 15/64 loss: -0.0373191237449646
Batch 16/64 loss: -0.03972327709197998
Batch 17/64 loss: -0.03080916404724121
Batch 18/64 loss: -0.024150073528289795
Batch 19/64 loss: -0.033741116523742676
Batch 20/64 loss: -0.027519822120666504
Batch 21/64 loss: -0.009796321392059326
Batch 22/64 loss: -0.04025059938430786
Batch 23/64 loss: -0.023062169551849365
Batch 24/64 loss: -0.03201258182525635
Batch 25/64 loss: -0.03288751840591431
Batch 26/64 loss: -0.033432602882385254
Batch 27/64 loss: -0.02632725238800049
Batch 28/64 loss: -0.03011554479598999
Batch 29/64 loss: -0.01750117540359497
Batch 30/64 loss: -0.020892024040222168
Batch 31/64 loss: -0.022617995738983154
Batch 32/64 loss: -0.02772200107574463
Batch 33/64 loss: -0.040511250495910645
Batch 34/64 loss: -0.027389943599700928
Batch 35/64 loss: -0.013634264469146729
Batch 36/64 loss: -0.023768901824951172
Batch 37/64 loss: -0.03725355863571167
Batch 38/64 loss: -0.02277660369873047
Batch 39/64 loss: -0.02296978235244751
Batch 40/64 loss: -0.023589491844177246
Batch 41/64 loss: -0.03635185956954956
Batch 42/64 loss: -0.025557994842529297
Batch 43/64 loss: -0.023612916469573975
Batch 44/64 loss: -0.012749671936035156
Batch 45/64 loss: -0.023667991161346436
Batch 46/64 loss: -0.014434635639190674
Batch 47/64 loss: -0.012943744659423828
Batch 48/64 loss: -0.017775237560272217
Batch 49/64 loss: -0.022650957107543945
Batch 50/64 loss: -0.019750654697418213
Batch 51/64 loss: -0.00956737995147705
Batch 52/64 loss: -0.01620805263519287
Batch 53/64 loss: -0.03012007474899292
Batch 54/64 loss: -0.025662004947662354
Batch 55/64 loss: -0.0355304479598999
Batch 56/64 loss: -0.019232451915740967
Batch 57/64 loss: -0.014592468738555908
Batch 58/64 loss: -0.007329106330871582
Batch 59/64 loss: -0.0058509111404418945
Batch 60/64 loss: -0.03178173303604126
Batch 61/64 loss: -0.024975836277008057
Batch 62/64 loss: -0.02861166000366211
Batch 63/64 loss: -0.008502840995788574
Batch 64/64 loss: -0.02398538589477539
Epoch 377  Train loss: -0.026048146042169307  Val loss: 0.09241504685575609
Epoch 378
-------------------------------
Batch 1/64 loss: -0.01749342679977417
Batch 2/64 loss: -0.018520832061767578
Batch 3/64 loss: -0.03317981958389282
Batch 4/64 loss: -0.035377442836761475
Batch 5/64 loss: -0.015513062477111816
Batch 6/64 loss: -0.033758699893951416
Batch 7/64 loss: -0.03425443172454834
Batch 8/64 loss: -0.0413668155670166
Batch 9/64 loss: -0.03878277540206909
Batch 10/64 loss: -0.027152657508850098
Batch 11/64 loss: -0.03493911027908325
Batch 12/64 loss: -0.01676112413406372
Batch 13/64 loss: -0.02100437879562378
Batch 14/64 loss: -0.028415560722351074
Batch 15/64 loss: -0.007938563823699951
Batch 16/64 loss: -0.042496681213378906
Batch 17/64 loss: -0.034302711486816406
Batch 18/64 loss: -0.033006250858306885
Batch 19/64 loss: -0.037277936935424805
Batch 20/64 loss: -0.028977394104003906
Batch 21/64 loss: -0.031430602073669434
Batch 22/64 loss: 0.00998610258102417
Batch 23/64 loss: -0.028731942176818848
Batch 24/64 loss: -0.03043496608734131
Batch 25/64 loss: -0.03347671031951904
Batch 26/64 loss: -0.02794712781906128
Batch 27/64 loss: -0.033621013164520264
Batch 28/64 loss: -0.030323028564453125
Batch 29/64 loss: -0.02343916893005371
Batch 30/64 loss: -0.017803847789764404
Batch 31/64 loss: -0.03332972526550293
Batch 32/64 loss: -0.012104392051696777
Batch 33/64 loss: -0.019858241081237793
Batch 34/64 loss: -0.031355440616607666
Batch 35/64 loss: -0.013448596000671387
Batch 36/64 loss: -0.02529919147491455
Batch 37/64 loss: -0.0245475172996521
Batch 38/64 loss: -0.024681687355041504
Batch 39/64 loss: -0.024272799491882324
Batch 40/64 loss: -0.01682215929031372
Batch 41/64 loss: -0.022728264331817627
Batch 42/64 loss: -0.008129119873046875
Batch 43/64 loss: -0.03022855520248413
Batch 44/64 loss: -0.01656264066696167
Batch 45/64 loss: -0.03982186317443848
Batch 46/64 loss: -0.023056864738464355
Batch 47/64 loss: -0.046341001987457275
Batch 48/64 loss: -0.013859748840332031
Batch 49/64 loss: -0.02781081199645996
Batch 50/64 loss: -0.03555178642272949
Batch 51/64 loss: -0.028620123863220215
Batch 52/64 loss: -0.034177184104919434
Batch 53/64 loss: -0.0016894340515136719
Batch 54/64 loss: -0.032612383365631104
Batch 55/64 loss: -0.015806615352630615
Batch 56/64 loss: -0.024861693382263184
Batch 57/64 loss: -0.03161585330963135
Batch 58/64 loss: -0.03853607177734375
Batch 59/64 loss: -0.024515211582183838
Batch 60/64 loss: -0.026823103427886963
Batch 61/64 loss: -0.025425970554351807
Batch 62/64 loss: -0.017886877059936523
Batch 63/64 loss: -0.01789712905883789
Batch 64/64 loss: -0.020545542240142822
Epoch 378  Train loss: -0.02593614423976225  Val loss: 0.09178632538752868
Epoch 379
-------------------------------
Batch 1/64 loss: -0.038359761238098145
Batch 2/64 loss: -0.02921271324157715
Batch 3/64 loss: -0.013381242752075195
Batch 4/64 loss: -0.027736186981201172
Batch 5/64 loss: -0.02516627311706543
Batch 6/64 loss: -0.017930150032043457
Batch 7/64 loss: -0.010865747928619385
Batch 8/64 loss: -0.03701150417327881
Batch 9/64 loss: -0.03568297624588013
Batch 10/64 loss: -0.030879974365234375
Batch 11/64 loss: -0.04598003625869751
Batch 12/64 loss: -0.02703040838241577
Batch 13/64 loss: -0.041358888149261475
Batch 14/64 loss: -0.02712160348892212
Batch 15/64 loss: -0.016526460647583008
Batch 16/64 loss: -0.019643783569335938
Batch 17/64 loss: -0.027493834495544434
Batch 18/64 loss: -0.04803198575973511
Batch 19/64 loss: -0.02999722957611084
Batch 20/64 loss: -0.019392669200897217
Batch 21/64 loss: -0.03955650329589844
Batch 22/64 loss: -0.027566194534301758
Batch 23/64 loss: -0.02284419536590576
Batch 24/64 loss: -0.029327690601348877
Batch 25/64 loss: -0.04134625196456909
Batch 26/64 loss: -0.02153754234313965
Batch 27/64 loss: -0.042301058769226074
Batch 28/64 loss: -0.028254210948944092
Batch 29/64 loss: -0.04086625576019287
Batch 30/64 loss: -0.036464691162109375
Batch 31/64 loss: -0.0398329496383667
Batch 32/64 loss: -0.03391897678375244
Batch 33/64 loss: -0.03359043598175049
Batch 34/64 loss: -0.025082290172576904
Batch 35/64 loss: -0.024377942085266113
Batch 36/64 loss: -0.036213576793670654
Batch 37/64 loss: -0.02540534734725952
Batch 38/64 loss: -0.019971370697021484
Batch 39/64 loss: -0.025960683822631836
Batch 40/64 loss: -0.026166677474975586
Batch 41/64 loss: -0.03254050016403198
Batch 42/64 loss: -0.02362602949142456
Batch 43/64 loss: -0.026301145553588867
Batch 44/64 loss: -0.023355424404144287
Batch 45/64 loss: -0.043971240520477295
Batch 46/64 loss: -0.03391444683074951
Batch 47/64 loss: -0.029718756675720215
Batch 48/64 loss: -0.02364182472229004
Batch 49/64 loss: -0.023798346519470215
Batch 50/64 loss: -0.038629770278930664
Batch 51/64 loss: -0.016730844974517822
Batch 52/64 loss: -0.02496170997619629
Batch 53/64 loss: -0.025599002838134766
Batch 54/64 loss: -0.024918556213378906
Batch 55/64 loss: -0.022496342658996582
Batch 56/64 loss: -0.02649068832397461
Batch 57/64 loss: -0.019419074058532715
Batch 58/64 loss: -0.020656466484069824
Batch 59/64 loss: -0.014419376850128174
Batch 60/64 loss: -0.015480458736419678
Batch 61/64 loss: -0.014844894409179688
Batch 62/64 loss: -0.03645014762878418
Batch 63/64 loss: -0.015096187591552734
Batch 64/64 loss: -0.026093482971191406
Epoch 379  Train loss: -0.028015994090659947  Val loss: 0.0921030224803387
Epoch 380
-------------------------------
Batch 1/64 loss: -0.01845484972000122
Batch 2/64 loss: -0.015099525451660156
Batch 3/64 loss: -0.016973674297332764
Batch 4/64 loss: -0.04172384738922119
Batch 5/64 loss: -0.032385945320129395
Batch 6/64 loss: -0.03257596492767334
Batch 7/64 loss: -0.03176772594451904
Batch 8/64 loss: -0.019007086753845215
Batch 9/64 loss: -0.0355987548828125
Batch 10/64 loss: -0.022066593170166016
Batch 11/64 loss: -0.03679502010345459
Batch 12/64 loss: -0.024411499500274658
Batch 13/64 loss: -0.017092466354370117
Batch 14/64 loss: -0.020360469818115234
Batch 15/64 loss: -0.031764328479766846
Batch 16/64 loss: -0.0212438702583313
Batch 17/64 loss: -0.00362241268157959
Batch 18/64 loss: -0.028666973114013672
Batch 19/64 loss: -0.039544641971588135
Batch 20/64 loss: -0.0047574639320373535
Batch 21/64 loss: -0.015088677406311035
Batch 22/64 loss: -0.010762214660644531
Batch 23/64 loss: -0.03224605321884155
Batch 24/64 loss: -0.03025078773498535
Batch 25/64 loss: -0.03126335144042969
Batch 26/64 loss: -0.01911306381225586
Batch 27/64 loss: -0.022713959217071533
Batch 28/64 loss: -0.009118914604187012
Batch 29/64 loss: -0.044551074504852295
Batch 30/64 loss: -0.032245755195617676
Batch 31/64 loss: -0.02439594268798828
Batch 32/64 loss: -0.01802009344100952
Batch 33/64 loss: -0.02780240774154663
Batch 34/64 loss: -0.028280138969421387
Batch 35/64 loss: -0.015421748161315918
Batch 36/64 loss: -0.031095683574676514
Batch 37/64 loss: -0.026186645030975342
Batch 38/64 loss: -0.014595270156860352
Batch 39/64 loss: -0.01964247226715088
Batch 40/64 loss: -0.022805392742156982
Batch 41/64 loss: -0.02513718605041504
Batch 42/64 loss: -0.03552079200744629
Batch 43/64 loss: -0.027587413787841797
Batch 44/64 loss: -0.02534306049346924
Batch 45/64 loss: -0.022652626037597656
Batch 46/64 loss: -0.019548118114471436
Batch 47/64 loss: -0.020987331867218018
Batch 48/64 loss: -0.013289093971252441
Batch 49/64 loss: -0.008872628211975098
Batch 50/64 loss: -0.03169524669647217
Batch 51/64 loss: -0.024393975734710693
Batch 52/64 loss: -0.026094436645507812
Batch 53/64 loss: -0.027285277843475342
Batch 54/64 loss: -0.02006697654724121
Batch 55/64 loss: -0.03901219367980957
Batch 56/64 loss: -0.03640174865722656
Batch 57/64 loss: -0.037163496017456055
Batch 58/64 loss: -0.030788064002990723
Batch 59/64 loss: -0.03175067901611328
Batch 60/64 loss: -0.025252223014831543
Batch 61/64 loss: -0.023871302604675293
Batch 62/64 loss: -0.03520464897155762
Batch 63/64 loss: -0.014374136924743652
Batch 64/64 loss: -0.016033709049224854
Epoch 380  Train loss: -0.024844403126660516  Val loss: 0.09305599977060691
Epoch 381
-------------------------------
Batch 1/64 loss: -0.016872763633728027
Batch 2/64 loss: -0.027669191360473633
Batch 3/64 loss: -0.031028449535369873
Batch 4/64 loss: -0.03876817226409912
Batch 5/64 loss: -0.03665238618850708
Batch 6/64 loss: -0.02176809310913086
Batch 7/64 loss: -0.017115652561187744
Batch 8/64 loss: -0.04000121355056763
Batch 9/64 loss: -0.017612457275390625
Batch 10/64 loss: -0.03272712230682373
Batch 11/64 loss: -0.043882906436920166
Batch 12/64 loss: -0.022331416606903076
Batch 13/64 loss: -0.03849208354949951
Batch 14/64 loss: -0.02934211492538452
Batch 15/64 loss: -0.02664083242416382
Batch 16/64 loss: -0.023699164390563965
Batch 17/64 loss: -0.0035828351974487305
Batch 18/64 loss: -0.03152233362197876
Batch 19/64 loss: -0.032131314277648926
Batch 20/64 loss: -0.02406024932861328
Batch 21/64 loss: -0.03228241205215454
Batch 22/64 loss: -0.01431286334991455
Batch 23/64 loss: -0.030906319618225098
Batch 24/64 loss: 0.000311434268951416
Batch 25/64 loss: -0.027654051780700684
Batch 26/64 loss: -0.040821373462677
Batch 27/64 loss: -0.028589487075805664
Batch 28/64 loss: -0.029036641120910645
Batch 29/64 loss: -0.026410400867462158
Batch 30/64 loss: -0.02211707830429077
Batch 31/64 loss: -0.03891938924789429
Batch 32/64 loss: -0.03293198347091675
Batch 33/64 loss: -0.023264169692993164
Batch 34/64 loss: -0.01738971471786499
Batch 35/64 loss: -0.009917020797729492
Batch 36/64 loss: -0.021640539169311523
Batch 37/64 loss: -0.033291399478912354
Batch 38/64 loss: -0.038712382316589355
Batch 39/64 loss: -0.04083222150802612
Batch 40/64 loss: -0.02550715208053589
Batch 41/64 loss: -0.0252913236618042
Batch 42/64 loss: -0.026994287967681885
Batch 43/64 loss: -0.025523364543914795
Batch 44/64 loss: -0.015273988246917725
Batch 45/64 loss: -0.03665405511856079
Batch 46/64 loss: -0.02461409568786621
Batch 47/64 loss: -0.028924882411956787
Batch 48/64 loss: -0.017929375171661377
Batch 49/64 loss: -0.0357474684715271
Batch 50/64 loss: -0.020338773727416992
Batch 51/64 loss: -0.043727874755859375
Batch 52/64 loss: -0.01442575454711914
Batch 53/64 loss: -0.012584924697875977
Batch 54/64 loss: -0.026059508323669434
Batch 55/64 loss: -0.03218787908554077
Batch 56/64 loss: -0.027762532234191895
Batch 57/64 loss: -0.0011211633682250977
Batch 58/64 loss: -0.030888676643371582
Batch 59/64 loss: -0.031075000762939453
Batch 60/64 loss: -0.02537459135055542
Batch 61/64 loss: -0.009621381759643555
Batch 62/64 loss: -0.02923572063446045
Batch 63/64 loss: -0.03056412935256958
Batch 64/64 loss: -0.01930248737335205
Epoch 381  Train loss: -0.026298408414803298  Val loss: 0.09301150234294511
Epoch 382
-------------------------------
Batch 1/64 loss: -0.048657000064849854
Batch 2/64 loss: -0.03018975257873535
Batch 3/64 loss: -0.025726377964019775
Batch 4/64 loss: -0.04206353425979614
Batch 5/64 loss: -0.039285123348236084
Batch 6/64 loss: -0.01954895257949829
Batch 7/64 loss: -0.028991341590881348
Batch 8/64 loss: -0.029648244380950928
Batch 9/64 loss: -0.022751688957214355
Batch 10/64 loss: -0.0382230281829834
Batch 11/64 loss: -0.0243607759475708
Batch 12/64 loss: -0.02102905511856079
Batch 13/64 loss: -0.02337557077407837
Batch 14/64 loss: -0.03316587209701538
Batch 15/64 loss: -0.030742645263671875
Batch 16/64 loss: -0.014991343021392822
Batch 17/64 loss: -0.025951743125915527
Batch 18/64 loss: -0.026874661445617676
Batch 19/64 loss: -0.015392959117889404
Batch 20/64 loss: -0.006521940231323242
Batch 21/64 loss: -0.033095479011535645
Batch 22/64 loss: -0.03660053014755249
Batch 23/64 loss: -0.017277300357818604
Batch 24/64 loss: -0.021039485931396484
Batch 25/64 loss: -0.029104113578796387
Batch 26/64 loss: -0.024198055267333984
Batch 27/64 loss: -0.028821825981140137
Batch 28/64 loss: -0.02204054594039917
Batch 29/64 loss: -0.039119720458984375
Batch 30/64 loss: -0.036510348320007324
Batch 31/64 loss: -0.026054203510284424
Batch 32/64 loss: -0.02226889133453369
Batch 33/64 loss: -0.0019201040267944336
Batch 34/64 loss: -0.01591789722442627
Batch 35/64 loss: -0.020851731300354004
Batch 36/64 loss: -0.03943711519241333
Batch 37/64 loss: -0.0010437965393066406
Batch 38/64 loss: -0.020455658435821533
Batch 39/64 loss: -0.022824406623840332
Batch 40/64 loss: -0.0310552716255188
Batch 41/64 loss: -0.03159576654434204
Batch 42/64 loss: -0.034401535987854004
Batch 43/64 loss: -0.019023418426513672
Batch 44/64 loss: -0.03389322757720947
Batch 45/64 loss: -0.021802961826324463
Batch 46/64 loss: -0.01190042495727539
Batch 47/64 loss: -0.007281303405761719
Batch 48/64 loss: -0.026355624198913574
Batch 49/64 loss: -0.037912607192993164
Batch 50/64 loss: -0.025753438472747803
Batch 51/64 loss: -0.0026802420616149902
Batch 52/64 loss: -0.037011682987213135
Batch 53/64 loss: -0.03272891044616699
Batch 54/64 loss: -0.026906847953796387
Batch 55/64 loss: -0.022541046142578125
Batch 56/64 loss: -0.02666652202606201
Batch 57/64 loss: -0.005095779895782471
Batch 58/64 loss: -0.0279388427734375
Batch 59/64 loss: -0.01819479465484619
Batch 60/64 loss: -0.029216885566711426
Batch 61/64 loss: -0.003269016742706299
Batch 62/64 loss: -0.0277557373046875
Batch 63/64 loss: -0.02159363031387329
Batch 64/64 loss: -0.025301575660705566
Epoch 382  Train loss: -0.0249039140402102  Val loss: 0.09371736676422592
Epoch 383
-------------------------------
Batch 1/64 loss: -0.030010342597961426
Batch 2/64 loss: -0.03524380922317505
Batch 3/64 loss: -0.027831196784973145
Batch 4/64 loss: -0.02421700954437256
Batch 5/64 loss: -0.02157801389694214
Batch 6/64 loss: -0.03460049629211426
Batch 7/64 loss: -0.027696728706359863
Batch 8/64 loss: -0.023798346519470215
Batch 9/64 loss: -0.04354125261306763
Batch 10/64 loss: -0.02727687358856201
Batch 11/64 loss: -0.03137671947479248
Batch 12/64 loss: -0.030870795249938965
Batch 13/64 loss: -0.02604520320892334
Batch 14/64 loss: -0.01766836643218994
Batch 15/64 loss: -0.04125314950942993
Batch 16/64 loss: -0.02343881130218506
Batch 17/64 loss: -0.03529250621795654
Batch 18/64 loss: -0.01714003086090088
Batch 19/64 loss: -0.035310447216033936
Batch 20/64 loss: -0.02364635467529297
Batch 21/64 loss: -0.02381378412246704
Batch 22/64 loss: -0.020267963409423828
Batch 23/64 loss: -0.035398900508880615
Batch 24/64 loss: -0.01994144916534424
Batch 25/64 loss: -0.02063298225402832
Batch 26/64 loss: -0.021375179290771484
Batch 27/64 loss: -0.02901381254196167
Batch 28/64 loss: -0.021887242794036865
Batch 29/64 loss: -0.03637993335723877
Batch 30/64 loss: -0.0035919547080993652
Batch 31/64 loss: -0.024672746658325195
Batch 32/64 loss: -0.02613234519958496
Batch 33/64 loss: -0.0298614501953125
Batch 34/64 loss: -0.03447139263153076
Batch 35/64 loss: -0.016084253787994385
Batch 36/64 loss: -0.01803457736968994
Batch 37/64 loss: -0.030155837535858154
Batch 38/64 loss: -0.04463362693786621
Batch 39/64 loss: -0.043652236461639404
Batch 40/64 loss: -0.02927267551422119
Batch 41/64 loss: -0.02082061767578125
Batch 42/64 loss: -0.02679675817489624
Batch 43/64 loss: -0.015992045402526855
Batch 44/64 loss: -0.019883811473846436
Batch 45/64 loss: -0.005119025707244873
Batch 46/64 loss: -0.023276090621948242
Batch 47/64 loss: -0.029118835926055908
Batch 48/64 loss: -0.028186917304992676
Batch 49/64 loss: -0.02052748203277588
Batch 50/64 loss: -0.03364378213882446
Batch 51/64 loss: -0.051326870918273926
Batch 52/64 loss: -0.016636192798614502
Batch 53/64 loss: -0.002581298351287842
Batch 54/64 loss: -0.029925823211669922
Batch 55/64 loss: -0.031053543090820312
Batch 56/64 loss: -0.028780043125152588
Batch 57/64 loss: -0.04117029905319214
Batch 58/64 loss: -0.0071489810943603516
Batch 59/64 loss: -0.03726702928543091
Batch 60/64 loss: -0.029529571533203125
Batch 61/64 loss: -0.0446619987487793
Batch 62/64 loss: -0.038238346576690674
Batch 63/64 loss: -0.03659701347351074
Batch 64/64 loss: -0.03396111726760864
Epoch 383  Train loss: -0.027464533553403968  Val loss: 0.09432474321516109
Epoch 384
-------------------------------
Batch 1/64 loss: -0.03820079565048218
Batch 2/64 loss: 0.0010755658149719238
Batch 3/64 loss: -0.03634876012802124
Batch 4/64 loss: -0.03306668996810913
Batch 5/64 loss: -0.04057127237319946
Batch 6/64 loss: -0.028691768646240234
Batch 7/64 loss: -0.03608286380767822
Batch 8/64 loss: -0.02732837200164795
Batch 9/64 loss: -0.044030725955963135
Batch 10/64 loss: -0.032554447650909424
Batch 11/64 loss: -0.023622512817382812
Batch 12/64 loss: -0.018547654151916504
Batch 13/64 loss: -0.009795546531677246
Batch 14/64 loss: -0.017763137817382812
Batch 15/64 loss: -0.013536453247070312
Batch 16/64 loss: -0.03059917688369751
Batch 17/64 loss: -0.018877387046813965
Batch 18/64 loss: -0.020371079444885254
Batch 19/64 loss: -0.038380324840545654
Batch 20/64 loss: -0.01886516809463501
Batch 21/64 loss: -0.03666520118713379
Batch 22/64 loss: -0.028506875038146973
Batch 23/64 loss: -0.034609436988830566
Batch 24/64 loss: -0.022534489631652832
Batch 25/64 loss: -0.02490997314453125
Batch 26/64 loss: -0.044781625270843506
Batch 27/64 loss: -0.03890180587768555
Batch 28/64 loss: -0.0016602277755737305
Batch 29/64 loss: -0.033189356327056885
Batch 30/64 loss: -0.03672480583190918
Batch 31/64 loss: -0.021690845489501953
Batch 32/64 loss: -0.045964598655700684
Batch 33/64 loss: -0.022939324378967285
Batch 34/64 loss: -0.013338327407836914
Batch 35/64 loss: -0.02845168113708496
Batch 36/64 loss: -0.033202409744262695
Batch 37/64 loss: -0.0276067852973938
Batch 38/64 loss: -0.028304994106292725
Batch 39/64 loss: -0.025428831577301025
Batch 40/64 loss: -0.030672430992126465
Batch 41/64 loss: -0.013506054878234863
Batch 42/64 loss: -0.025960803031921387
Batch 43/64 loss: -0.02257901430130005
Batch 44/64 loss: -0.033791959285736084
Batch 45/64 loss: -0.02689945697784424
Batch 46/64 loss: -0.024714231491088867
Batch 47/64 loss: -0.007893502712249756
Batch 48/64 loss: -0.02665841579437256
Batch 49/64 loss: -0.01613396406173706
Batch 50/64 loss: -0.0367889404296875
Batch 51/64 loss: -0.025881946086883545
Batch 52/64 loss: -0.014949917793273926
Batch 53/64 loss: -0.013327360153198242
Batch 54/64 loss: -0.041267216205596924
Batch 55/64 loss: -0.02249431610107422
Batch 56/64 loss: -0.027623653411865234
Batch 57/64 loss: -0.021595120429992676
Batch 58/64 loss: -0.008269906044006348
Batch 59/64 loss: -0.0249977707862854
Batch 60/64 loss: -0.008692502975463867
Batch 61/64 loss: -0.023748934268951416
Batch 62/64 loss: -0.032655954360961914
Batch 63/64 loss: -0.02914118766784668
Batch 64/64 loss: -0.02045261859893799
Epoch 384  Train loss: -0.02590045788708855  Val loss: 0.09264526334415187
Epoch 385
-------------------------------
Batch 1/64 loss: -0.03304398059844971
Batch 2/64 loss: -0.03455871343612671
Batch 3/64 loss: -0.041947245597839355
Batch 4/64 loss: -0.01100701093673706
Batch 5/64 loss: -0.03564023971557617
Batch 6/64 loss: -0.012928247451782227
Batch 7/64 loss: -0.036431074142456055
Batch 8/64 loss: -0.0461193323135376
Batch 9/64 loss: -0.04187750816345215
Batch 10/64 loss: -0.022905468940734863
Batch 11/64 loss: -0.025420963764190674
Batch 12/64 loss: -0.02380812168121338
Batch 13/64 loss: -0.03439438343048096
Batch 14/64 loss: -0.03149747848510742
Batch 15/64 loss: -0.029444575309753418
Batch 16/64 loss: -0.027122855186462402
Batch 17/64 loss: -0.03236633539199829
Batch 18/64 loss: -0.03604310750961304
Batch 19/64 loss: -0.0322725772857666
Batch 20/64 loss: -0.02674245834350586
Batch 21/64 loss: -0.036106228828430176
Batch 22/64 loss: -0.018587827682495117
Batch 23/64 loss: -0.05360066890716553
Batch 24/64 loss: -0.012556314468383789
Batch 25/64 loss: -0.03221803903579712
Batch 26/64 loss: -0.021317899227142334
Batch 27/64 loss: -0.0407140851020813
Batch 28/64 loss: -0.012426495552062988
Batch 29/64 loss: -0.0247957706451416
Batch 30/64 loss: -0.036602020263671875
Batch 31/64 loss: -0.04365730285644531
Batch 32/64 loss: -0.03181272745132446
Batch 33/64 loss: -0.02333509922027588
Batch 34/64 loss: -0.032449960708618164
Batch 35/64 loss: -0.0325312614440918
Batch 36/64 loss: -0.02848154306411743
Batch 37/64 loss: -0.017559528350830078
Batch 38/64 loss: -0.03328526020050049
Batch 39/64 loss: -0.031279802322387695
Batch 40/64 loss: -0.03018927574157715
Batch 41/64 loss: -0.02726423740386963
Batch 42/64 loss: -0.022082030773162842
Batch 43/64 loss: 0.007458925247192383
Batch 44/64 loss: -0.032318115234375
Batch 45/64 loss: -0.021754860877990723
Batch 46/64 loss: -0.04281109571456909
Batch 47/64 loss: -0.025522172451019287
Batch 48/64 loss: -0.0240820050239563
Batch 49/64 loss: -0.04156839847564697
Batch 50/64 loss: -0.029427528381347656
Batch 51/64 loss: -0.0319097638130188
Batch 52/64 loss: -0.03144329786300659
Batch 53/64 loss: -0.020530521869659424
Batch 54/64 loss: -0.025744855403900146
Batch 55/64 loss: -0.03758883476257324
Batch 56/64 loss: -0.029069960117340088
Batch 57/64 loss: -0.015803217887878418
Batch 58/64 loss: -0.024904966354370117
Batch 59/64 loss: -0.027224421501159668
Batch 60/64 loss: -0.028243958950042725
Batch 61/64 loss: -0.0206148624420166
Batch 62/64 loss: -0.014870762825012207
Batch 63/64 loss: -0.034515380859375
Batch 64/64 loss: -0.03334540128707886
Epoch 385  Train loss: -0.02879881040722716  Val loss: 0.09230258292758588
Epoch 386
-------------------------------
Batch 1/64 loss: -0.03703010082244873
Batch 2/64 loss: -0.03606933355331421
Batch 3/64 loss: -0.012604236602783203
Batch 4/64 loss: 0.001448512077331543
Batch 5/64 loss: -0.00018274784088134766
Batch 6/64 loss: -0.04760277271270752
Batch 7/64 loss: -0.032792747020721436
Batch 8/64 loss: -0.02040231227874756
Batch 9/64 loss: -0.018407702445983887
Batch 10/64 loss: -0.012560129165649414
Batch 11/64 loss: -0.04161018133163452
Batch 12/64 loss: -0.02673637866973877
Batch 13/64 loss: -0.01873648166656494
Batch 14/64 loss: -0.034668922424316406
Batch 15/64 loss: -0.023452460765838623
Batch 16/64 loss: -0.023675143718719482
Batch 17/64 loss: -0.030246078968048096
Batch 18/64 loss: -0.028761863708496094
Batch 19/64 loss: -0.020448744297027588
Batch 20/64 loss: -0.03364795446395874
Batch 21/64 loss: -0.008175134658813477
Batch 22/64 loss: -0.01802647113800049
Batch 23/64 loss: -0.016405940055847168
Batch 24/64 loss: -0.046207308769226074
Batch 25/64 loss: -0.015131175518035889
Batch 26/64 loss: -0.02659428119659424
Batch 27/64 loss: -0.03591233491897583
Batch 28/64 loss: -0.019272446632385254
Batch 29/64 loss: -0.0090101957321167
Batch 30/64 loss: -0.030695199966430664
Batch 31/64 loss: -0.03564023971557617
Batch 32/64 loss: -0.02082061767578125
Batch 33/64 loss: -0.028311729431152344
Batch 34/64 loss: -0.033318519592285156
Batch 35/64 loss: -0.04021477699279785
Batch 36/64 loss: -0.03188490867614746
Batch 37/64 loss: -0.024910569190979004
Batch 38/64 loss: -0.03196215629577637
Batch 39/64 loss: -0.03241223096847534
Batch 40/64 loss: -0.01933610439300537
Batch 41/64 loss: -0.016394317150115967
Batch 42/64 loss: -0.024265587329864502
Batch 43/64 loss: -0.04376089572906494
Batch 44/64 loss: -0.03107321262359619
Batch 45/64 loss: -0.04856067895889282
Batch 46/64 loss: -0.0350489616394043
Batch 47/64 loss: -0.027409493923187256
Batch 48/64 loss: -0.0276908278465271
Batch 49/64 loss: -0.037724316120147705
Batch 50/64 loss: -0.007607221603393555
Batch 51/64 loss: -0.03229647874832153
Batch 52/64 loss: -0.030193686485290527
Batch 53/64 loss: -0.01847100257873535
Batch 54/64 loss: -0.030226409435272217
Batch 55/64 loss: -0.019679665565490723
Batch 56/64 loss: -0.033139586448669434
Batch 57/64 loss: -0.03653895854949951
Batch 58/64 loss: -0.03041607141494751
Batch 59/64 loss: -0.045377492904663086
Batch 60/64 loss: -0.024120211601257324
Batch 61/64 loss: -0.023559629917144775
Batch 62/64 loss: -0.0346907377243042
Batch 63/64 loss: -0.02843928337097168
Batch 64/64 loss: -0.0380367636680603
Epoch 386  Train loss: -0.027257136036367976  Val loss: 0.09370536718171896
Epoch 387
-------------------------------
Batch 1/64 loss: -0.03421628475189209
Batch 2/64 loss: -0.029074013233184814
Batch 3/64 loss: -0.015221118927001953
Batch 4/64 loss: -0.037862300872802734
Batch 5/64 loss: -0.0410686731338501
Batch 6/64 loss: -0.030658602714538574
Batch 7/64 loss: -0.03650331497192383
Batch 8/64 loss: -0.03156846761703491
Batch 9/64 loss: -0.05847436189651489
Batch 10/64 loss: -0.027930498123168945
Batch 11/64 loss: -0.010387659072875977
Batch 12/64 loss: -0.03126394748687744
Batch 13/64 loss: -0.032076358795166016
Batch 14/64 loss: -0.04219484329223633
Batch 15/64 loss: -0.03300619125366211
Batch 16/64 loss: -0.036411166191101074
Batch 17/64 loss: -0.004550933837890625
Batch 18/64 loss: -0.018851816654205322
Batch 19/64 loss: -0.02995765209197998
Batch 20/64 loss: -0.043196916580200195
Batch 21/64 loss: -0.03537696599960327
Batch 22/64 loss: -0.03430241346359253
Batch 23/64 loss: -0.03269582986831665
Batch 24/64 loss: -0.04229867458343506
Batch 25/64 loss: -0.021344542503356934
Batch 26/64 loss: -0.029706835746765137
Batch 27/64 loss: -0.023729920387268066
Batch 28/64 loss: -0.0250701904296875
Batch 29/64 loss: -0.044483065605163574
Batch 30/64 loss: -0.028249144554138184
Batch 31/64 loss: -0.03649419546127319
Batch 32/64 loss: -0.027393817901611328
Batch 33/64 loss: -0.029318928718566895
Batch 34/64 loss: -0.026446223258972168
Batch 35/64 loss: -0.034120798110961914
Batch 36/64 loss: -0.03284269571304321
Batch 37/64 loss: -0.018909931182861328
Batch 38/64 loss: -0.020959138870239258
Batch 39/64 loss: -0.03424018621444702
Batch 40/64 loss: -0.0407223105430603
Batch 41/64 loss: -0.028738319873809814
Batch 42/64 loss: -0.026546478271484375
Batch 43/64 loss: -0.011202812194824219
Batch 44/64 loss: -0.03532230854034424
Batch 45/64 loss: -0.030947506427764893
Batch 46/64 loss: -0.03585696220397949
Batch 47/64 loss: -0.022318601608276367
Batch 48/64 loss: -0.014957845211029053
Batch 49/64 loss: -0.04255819320678711
Batch 50/64 loss: -0.02046877145767212
Batch 51/64 loss: -0.03472316265106201
Batch 52/64 loss: -0.02718043327331543
Batch 53/64 loss: -0.041643500328063965
Batch 54/64 loss: -0.014900565147399902
Batch 55/64 loss: -0.02458178997039795
Batch 56/64 loss: -0.014017879962921143
Batch 57/64 loss: -0.004397451877593994
Batch 58/64 loss: -0.021778404712677002
Batch 59/64 loss: -0.03089660406112671
Batch 60/64 loss: -0.028934836387634277
Batch 61/64 loss: -0.013322532176971436
Batch 62/64 loss: -0.026424765586853027
Batch 63/64 loss: -0.022159039974212646
Batch 64/64 loss: -0.04146379232406616
Epoch 387  Train loss: -0.028990706509234857  Val loss: 0.09285009706143252
Epoch 388
-------------------------------
Batch 1/64 loss: -0.03621798753738403
Batch 2/64 loss: -0.021271467208862305
Batch 3/64 loss: -0.025515496730804443
Batch 4/64 loss: -0.04084277153015137
Batch 5/64 loss: -0.04021197557449341
Batch 6/64 loss: -0.03067147731781006
Batch 7/64 loss: -0.01859438419342041
Batch 8/64 loss: -0.033336520195007324
Batch 9/64 loss: -0.0341227650642395
Batch 10/64 loss: -0.04305863380432129
Batch 11/64 loss: -0.03635561466217041
Batch 12/64 loss: -0.03309440612792969
Batch 13/64 loss: -0.013955235481262207
Batch 14/64 loss: -0.03613865375518799
Batch 15/64 loss: -0.013753175735473633
Batch 16/64 loss: -0.016968965530395508
Batch 17/64 loss: -0.02989363670349121
Batch 18/64 loss: -0.027092039585113525
Batch 19/64 loss: -0.029053926467895508
Batch 20/64 loss: -0.03267216682434082
Batch 21/64 loss: -0.017984628677368164
Batch 22/64 loss: -0.03035414218902588
Batch 23/64 loss: -0.03507411479949951
Batch 24/64 loss: -0.01607835292816162
Batch 25/64 loss: -0.02984476089477539
Batch 26/64 loss: -0.0220639705657959
Batch 27/64 loss: -0.021922588348388672
Batch 28/64 loss: -0.02844381332397461
Batch 29/64 loss: -0.03502839803695679
Batch 30/64 loss: -0.04249536991119385
Batch 31/64 loss: -0.025017142295837402
Batch 32/64 loss: -0.02888023853302002
Batch 33/64 loss: -0.02256488800048828
Batch 34/64 loss: -0.025905728340148926
Batch 35/64 loss: -0.04008358716964722
Batch 36/64 loss: -0.031066596508026123
Batch 37/64 loss: -0.04629284143447876
Batch 38/64 loss: -0.03836566209793091
Batch 39/64 loss: -0.0358467698097229
Batch 40/64 loss: -0.00905919075012207
Batch 41/64 loss: -0.023441076278686523
Batch 42/64 loss: -0.04946953058242798
Batch 43/64 loss: -0.02029520273208618
Batch 44/64 loss: -0.0265161395072937
Batch 45/64 loss: -0.03583097457885742
Batch 46/64 loss: -0.029160618782043457
Batch 47/64 loss: -0.03588670492172241
Batch 48/64 loss: -0.03283798694610596
Batch 49/64 loss: -0.02662038803100586
Batch 50/64 loss: -0.0359610915184021
Batch 51/64 loss: -0.03308701515197754
Batch 52/64 loss: -0.019578099250793457
Batch 53/64 loss: -0.023871660232543945
Batch 54/64 loss: -0.030344903469085693
Batch 55/64 loss: -0.04024636745452881
Batch 56/64 loss: -0.03549140691757202
Batch 57/64 loss: -0.039237916469573975
Batch 58/64 loss: -0.04297059774398804
Batch 59/64 loss: -0.019043922424316406
Batch 60/64 loss: -0.03882110118865967
Batch 61/64 loss: -0.020520448684692383
Batch 62/64 loss: -0.03732329607009888
Batch 63/64 loss: -0.028674840927124023
Batch 64/64 loss: -0.016322731971740723
Epoch 388  Train loss: -0.030002626250771915  Val loss: 0.09305038136714922
Epoch 389
-------------------------------
Batch 1/64 loss: -0.027788877487182617
Batch 2/64 loss: -0.0365261435508728
Batch 3/64 loss: -0.029682159423828125
Batch 4/64 loss: -0.03275489807128906
Batch 5/64 loss: -0.04864835739135742
Batch 6/64 loss: -0.02601844072341919
Batch 7/64 loss: -0.03286468982696533
Batch 8/64 loss: -0.04181331396102905
Batch 9/64 loss: -0.03432267904281616
Batch 10/64 loss: -0.042222559452056885
Batch 11/64 loss: -0.04553288221359253
Batch 12/64 loss: -0.0410158634185791
Batch 13/64 loss: -0.0294649600982666
Batch 14/64 loss: -0.027987003326416016
Batch 15/64 loss: -0.026879549026489258
Batch 16/64 loss: -0.038665831089019775
Batch 17/64 loss: -0.03968203067779541
Batch 18/64 loss: -0.03243303298950195
Batch 19/64 loss: -0.027549147605895996
Batch 20/64 loss: -0.0325351357460022
Batch 21/64 loss: -0.03547239303588867
Batch 22/64 loss: -0.052565038204193115
Batch 23/64 loss: -0.026347219944000244
Batch 24/64 loss: -0.025748610496520996
Batch 25/64 loss: -0.02335059642791748
Batch 26/64 loss: -0.02506887912750244
Batch 27/64 loss: -0.04314863681793213
Batch 28/64 loss: -0.02120339870452881
Batch 29/64 loss: -0.026283562183380127
Batch 30/64 loss: -0.03248751163482666
Batch 31/64 loss: -0.005881965160369873
Batch 32/64 loss: -0.02984565496444702
Batch 33/64 loss: -0.0384557843208313
Batch 34/64 loss: -0.03147304058074951
Batch 35/64 loss: -0.025492191314697266
Batch 36/64 loss: -0.025935828685760498
Batch 37/64 loss: -0.042861342430114746
Batch 38/64 loss: -0.032272160053253174
Batch 39/64 loss: -0.027642786502838135
Batch 40/64 loss: -0.00865030288696289
Batch 41/64 loss: -0.027069389820098877
Batch 42/64 loss: -0.02674168348312378
Batch 43/64 loss: -0.034603238105773926
Batch 44/64 loss: -0.026498138904571533
Batch 45/64 loss: -0.024553894996643066
Batch 46/64 loss: -0.022385060787200928
Batch 47/64 loss: -0.005949616432189941
Batch 48/64 loss: -0.02352827787399292
Batch 49/64 loss: -0.021136224269866943
Batch 50/64 loss: -0.03339970111846924
Batch 51/64 loss: -0.04128098487854004
Batch 52/64 loss: -0.03278225660324097
Batch 53/64 loss: -0.03243893384933472
Batch 54/64 loss: -0.027173876762390137
Batch 55/64 loss: -0.03008294105529785
Batch 56/64 loss: -0.035359978675842285
Batch 57/64 loss: -0.021302759647369385
Batch 58/64 loss: -0.039832353591918945
Batch 59/64 loss: -0.022147834300994873
Batch 60/64 loss: -0.02715480327606201
Batch 61/64 loss: -0.0347517728805542
Batch 62/64 loss: -0.031020760536193848
Batch 63/64 loss: -0.017872095108032227
Batch 64/64 loss: -0.027444303035736084
Epoch 389  Train loss: -0.030340772750330907  Val loss: 0.09287425455768493
Epoch 390
-------------------------------
Batch 1/64 loss: -0.03957366943359375
Batch 2/64 loss: -0.04331916570663452
Batch 3/64 loss: -0.03800767660140991
Batch 4/64 loss: -0.017722606658935547
Batch 5/64 loss: -0.011551201343536377
Batch 6/64 loss: -0.030479907989501953
Batch 7/64 loss: -0.04014134407043457
Batch 8/64 loss: -0.030667662620544434
Batch 9/64 loss: -0.04572486877441406
Batch 10/64 loss: -0.026923418045043945
Batch 11/64 loss: -0.012913346290588379
Batch 12/64 loss: -0.016370415687561035
Batch 13/64 loss: -0.021298766136169434
Batch 14/64 loss: -0.026843369007110596
Batch 15/64 loss: -0.03024214506149292
Batch 16/64 loss: -0.02262657880783081
Batch 17/64 loss: -0.030309319496154785
Batch 18/64 loss: -0.02160191535949707
Batch 19/64 loss: -0.031645894050598145
Batch 20/64 loss: -0.037679433822631836
Batch 21/64 loss: -0.025025606155395508
Batch 22/64 loss: -0.03114694356918335
Batch 23/64 loss: -0.041030824184417725
Batch 24/64 loss: -0.015580415725708008
Batch 25/64 loss: -0.04448282718658447
Batch 26/64 loss: -0.013018965721130371
Batch 27/64 loss: -0.030287623405456543
Batch 28/64 loss: -0.025744736194610596
Batch 29/64 loss: -0.02687549591064453
Batch 30/64 loss: -0.012608170509338379
Batch 31/64 loss: -0.030797839164733887
Batch 32/64 loss: -0.03307473659515381
Batch 33/64 loss: -0.023782074451446533
Batch 34/64 loss: -0.024846792221069336
Batch 35/64 loss: -0.0232200026512146
Batch 36/64 loss: -0.026809334754943848
Batch 37/64 loss: -0.034816622734069824
Batch 38/64 loss: -0.016826987266540527
Batch 39/64 loss: -0.034398794174194336
Batch 40/64 loss: -0.027758419513702393
Batch 41/64 loss: -0.007932066917419434
Batch 42/64 loss: -0.020730555057525635
Batch 43/64 loss: -0.015433251857757568
Batch 44/64 loss: -0.02342236042022705
Batch 45/64 loss: -0.03489863872528076
Batch 46/64 loss: -0.04110842943191528
Batch 47/64 loss: -0.02396225929260254
Batch 48/64 loss: -0.02483522891998291
Batch 49/64 loss: -0.020210742950439453
Batch 50/64 loss: -0.022637486457824707
Batch 51/64 loss: -0.035125017166137695
Batch 52/64 loss: -0.022120893001556396
Batch 53/64 loss: -0.03987777233123779
Batch 54/64 loss: -0.031184852123260498
Batch 55/64 loss: -0.018939852714538574
Batch 56/64 loss: -0.010701894760131836
Batch 57/64 loss: -0.028805851936340332
Batch 58/64 loss: -0.02790820598602295
Batch 59/64 loss: -0.014371752738952637
Batch 60/64 loss: -0.03679877519607544
Batch 61/64 loss: -0.03754919767379761
Batch 62/64 loss: -0.03470015525817871
Batch 63/64 loss: -0.0268934965133667
Batch 64/64 loss: -0.02001190185546875
Epoch 390  Train loss: -0.027183271856868967  Val loss: 0.09513007017345362
Epoch 391
-------------------------------
Batch 1/64 loss: -0.03097844123840332
Batch 2/64 loss: -0.039644062519073486
Batch 3/64 loss: -0.04043847322463989
Batch 4/64 loss: -0.015984952449798584
Batch 5/64 loss: -0.03429442644119263
Batch 6/64 loss: -0.040662527084350586
Batch 7/64 loss: -0.037708938121795654
Batch 8/64 loss: -0.046691179275512695
Batch 9/64 loss: -0.04373610019683838
Batch 10/64 loss: -0.02380317449569702
Batch 11/64 loss: -0.0051381587982177734
Batch 12/64 loss: -0.025878727436065674
Batch 13/64 loss: -0.030734717845916748
Batch 14/64 loss: -0.028672635555267334
Batch 15/64 loss: -0.03126758337020874
Batch 16/64 loss: -0.034787893295288086
Batch 17/64 loss: -0.038740456104278564
Batch 18/64 loss: -0.03778356313705444
Batch 19/64 loss: -0.00911557674407959
Batch 20/64 loss: -0.03123706579208374
Batch 21/64 loss: -0.007576346397399902
Batch 22/64 loss: -0.026916086673736572
Batch 23/64 loss: -0.024187803268432617
Batch 24/64 loss: -0.02953505516052246
Batch 25/64 loss: -0.032313525676727295
Batch 26/64 loss: -0.038117170333862305
Batch 27/64 loss: -0.035776615142822266
Batch 28/64 loss: -0.016247987747192383
Batch 29/64 loss: -0.015952467918395996
Batch 30/64 loss: -0.032013893127441406
Batch 31/64 loss: -0.02841353416442871
Batch 32/64 loss: -0.04237788915634155
Batch 33/64 loss: -0.0424652099609375
Batch 34/64 loss: -0.018880128860473633
Batch 35/64 loss: -0.007624566555023193
Batch 36/64 loss: -0.029600799083709717
Batch 37/64 loss: -0.04037576913833618
Batch 38/64 loss: -0.032949626445770264
Batch 39/64 loss: -0.04398757219314575
Batch 40/64 loss: -0.024392426013946533
Batch 41/64 loss: -0.032793283462524414
Batch 42/64 loss: -0.04027211666107178
Batch 43/64 loss: -0.021860122680664062
Batch 44/64 loss: -0.04018157720565796
Batch 45/64 loss: -0.03682398796081543
Batch 46/64 loss: -0.026333212852478027
Batch 47/64 loss: -0.03490680456161499
Batch 48/64 loss: -0.023683547973632812
Batch 49/64 loss: -0.02533435821533203
Batch 50/64 loss: -0.04167908430099487
Batch 51/64 loss: -0.04088163375854492
Batch 52/64 loss: -0.023308277130126953
Batch 53/64 loss: -0.035712480545043945
Batch 54/64 loss: -0.027629971504211426
Batch 55/64 loss: -0.04148995876312256
Batch 56/64 loss: -0.044722557067871094
Batch 57/64 loss: -0.03343641757965088
Batch 58/64 loss: -0.02906721830368042
Batch 59/64 loss: -0.0263216495513916
Batch 60/64 loss: -0.028985321521759033
Batch 61/64 loss: -0.020587146282196045
Batch 62/64 loss: -0.039194583892822266
Batch 63/64 loss: -0.030373454093933105
Batch 64/64 loss: -0.04523533582687378
Epoch 391  Train loss: -0.031004053237391454  Val loss: 0.09536020247796967
Epoch 392
-------------------------------
Batch 1/64 loss: -0.02535712718963623
Batch 2/64 loss: -0.0507625937461853
Batch 3/64 loss: -0.025588035583496094
Batch 4/64 loss: -0.0501629114151001
Batch 5/64 loss: -0.03470194339752197
Batch 6/64 loss: -0.026959896087646484
Batch 7/64 loss: -0.034295856952667236
Batch 8/64 loss: -0.02914893627166748
Batch 9/64 loss: -0.03758347034454346
Batch 10/64 loss: -0.031533658504486084
Batch 11/64 loss: -0.027161478996276855
Batch 12/64 loss: -0.04445338249206543
Batch 13/64 loss: -0.019574880599975586
Batch 14/64 loss: -0.038823604583740234
Batch 15/64 loss: -0.033638179302215576
Batch 16/64 loss: -0.020426690578460693
Batch 17/64 loss: -0.04960554838180542
Batch 18/64 loss: -0.0002532005310058594
Batch 19/64 loss: -0.03323805332183838
Batch 20/64 loss: -0.02668476104736328
Batch 21/64 loss: -0.02223283052444458
Batch 22/64 loss: -0.021367788314819336
Batch 23/64 loss: -0.03530055284500122
Batch 24/64 loss: -0.02731931209564209
Batch 25/64 loss: -0.02356773614883423
Batch 26/64 loss: -0.031398236751556396
Batch 27/64 loss: -0.03346759080886841
Batch 28/64 loss: -0.020139753818511963
Batch 29/64 loss: -0.03291738033294678
Batch 30/64 loss: -0.020837903022766113
Batch 31/64 loss: -0.03349250555038452
Batch 32/64 loss: -0.022188842296600342
Batch 33/64 loss: -0.033507466316223145
Batch 34/64 loss: -0.029686927795410156
Batch 35/64 loss: -0.04015457630157471
Batch 36/64 loss: -0.04094940423965454
Batch 37/64 loss: -0.04207348823547363
Batch 38/64 loss: -0.040951550006866455
Batch 39/64 loss: -0.014514148235321045
Batch 40/64 loss: -0.032365381717681885
Batch 41/64 loss: -0.02812296152114868
Batch 42/64 loss: -0.021264195442199707
Batch 43/64 loss: -0.026647448539733887
Batch 44/64 loss: -0.03943854570388794
Batch 45/64 loss: -0.036959826946258545
Batch 46/64 loss: -0.030852437019348145
Batch 47/64 loss: -0.04240274429321289
Batch 48/64 loss: -0.03825068473815918
Batch 49/64 loss: -0.04323577880859375
Batch 50/64 loss: -0.04033428430557251
Batch 51/64 loss: -0.04188191890716553
Batch 52/64 loss: -0.031527817249298096
Batch 53/64 loss: -0.029621779918670654
Batch 54/64 loss: -0.038289785385131836
Batch 55/64 loss: -0.0068585872650146484
Batch 56/64 loss: -0.03184396028518677
Batch 57/64 loss: -0.03335464000701904
Batch 58/64 loss: -0.008638381958007812
Batch 59/64 loss: -0.03987079858779907
Batch 60/64 loss: -0.029816031455993652
Batch 61/64 loss: -0.02336442470550537
Batch 62/64 loss: -0.021574556827545166
Batch 63/64 loss: -0.03257298469543457
Batch 64/64 loss: -0.026088595390319824
Epoch 392  Train loss: -0.03097535067913579  Val loss: 0.09345124144734386
Epoch 393
-------------------------------
Batch 1/64 loss: -0.024842023849487305
Batch 2/64 loss: -0.03717350959777832
Batch 3/64 loss: -0.03283977508544922
Batch 4/64 loss: -0.03273719549179077
Batch 5/64 loss: -0.0459669828414917
Batch 6/64 loss: -0.0545351505279541
Batch 7/64 loss: -0.025111377239227295
Batch 8/64 loss: -0.04632675647735596
Batch 9/64 loss: -0.021767616271972656
Batch 10/64 loss: -0.03547263145446777
Batch 11/64 loss: -0.03308027982711792
Batch 12/64 loss: -0.05109000205993652
Batch 13/64 loss: -0.04949831962585449
Batch 14/64 loss: -0.035565316677093506
Batch 15/64 loss: -0.04064089059829712
Batch 16/64 loss: -0.04941529035568237
Batch 17/64 loss: -0.04499328136444092
Batch 18/64 loss: -0.03905665874481201
Batch 19/64 loss: -0.031183242797851562
Batch 20/64 loss: -0.033692240715026855
Batch 21/64 loss: -0.041609883308410645
Batch 22/64 loss: -0.03333669900894165
Batch 23/64 loss: -0.02100205421447754
Batch 24/64 loss: -0.03718233108520508
Batch 25/64 loss: -0.009942173957824707
Batch 26/64 loss: -0.027867376804351807
Batch 27/64 loss: -0.029222607612609863
Batch 28/64 loss: -0.04367971420288086
Batch 29/64 loss: -0.02669459581375122
Batch 30/64 loss: -0.00868690013885498
Batch 31/64 loss: -0.027925312519073486
Batch 32/64 loss: -0.040322065353393555
Batch 33/64 loss: -0.04259306192398071
Batch 34/64 loss: -0.03545665740966797
Batch 35/64 loss: -0.03951442241668701
Batch 36/64 loss: -0.016706526279449463
Batch 37/64 loss: -0.02373027801513672
Batch 38/64 loss: -0.03401458263397217
Batch 39/64 loss: -0.011942863464355469
Batch 40/64 loss: -0.012605607509613037
Batch 41/64 loss: -0.025313973426818848
Batch 42/64 loss: -0.03595411777496338
Batch 43/64 loss: -0.04270809888839722
Batch 44/64 loss: -0.0369420051574707
Batch 45/64 loss: -0.011328518390655518
Batch 46/64 loss: -0.03613203763961792
Batch 47/64 loss: -0.012779295444488525
Batch 48/64 loss: -0.033640921115875244
Batch 49/64 loss: -0.041775524616241455
Batch 50/64 loss: -0.00958406925201416
Batch 51/64 loss: -0.03226602077484131
Batch 52/64 loss: -0.03279316425323486
Batch 53/64 loss: -0.023286163806915283
Batch 54/64 loss: -0.0009975433349609375
Batch 55/64 loss: -0.019024908542633057
Batch 56/64 loss: -0.03904503583908081
Batch 57/64 loss: -0.029353678226470947
Batch 58/64 loss: -0.02017056941986084
Batch 59/64 loss: -0.03735250234603882
Batch 60/64 loss: -0.03389233350753784
Batch 61/64 loss: -0.03554809093475342
Batch 62/64 loss: -0.037971556186676025
Batch 63/64 loss: -0.020807266235351562
Batch 64/64 loss: -0.03173130750656128
Epoch 393  Train loss: -0.0313959236238517  Val loss: 0.09339090249792407
Epoch 394
-------------------------------
Batch 1/64 loss: -0.03341639041900635
Batch 2/64 loss: -0.04420357942581177
Batch 3/64 loss: -0.008447229862213135
Batch 4/64 loss: -0.03454160690307617
Batch 5/64 loss: -0.035783588886260986
Batch 6/64 loss: -0.03108614683151245
Batch 7/64 loss: -0.05291694402694702
Batch 8/64 loss: -0.0255700945854187
Batch 9/64 loss: -0.031885385513305664
Batch 10/64 loss: -0.03530895709991455
Batch 11/64 loss: -0.03904139995574951
Batch 12/64 loss: -0.03417861461639404
Batch 13/64 loss: -0.04482626914978027
Batch 14/64 loss: -0.03293251991271973
Batch 15/64 loss: -0.05076664686203003
Batch 16/64 loss: -0.05779904127120972
Batch 17/64 loss: -0.046448469161987305
Batch 18/64 loss: -0.050198912620544434
Batch 19/64 loss: -0.03398162126541138
Batch 20/64 loss: -0.03395634889602661
Batch 21/64 loss: -0.04453098773956299
Batch 22/64 loss: -0.03153812885284424
Batch 23/64 loss: -0.03845030069351196
Batch 24/64 loss: -0.02822202444076538
Batch 25/64 loss: -0.03718268871307373
Batch 26/64 loss: -0.04898548126220703
Batch 27/64 loss: -0.03229367733001709
Batch 28/64 loss: -0.020681262016296387
Batch 29/64 loss: -0.04684633016586304
Batch 30/64 loss: -0.030403614044189453
Batch 31/64 loss: -0.037052810192108154
Batch 32/64 loss: -0.041442692279815674
Batch 33/64 loss: -0.0320354700088501
Batch 34/64 loss: -0.02924501895904541
Batch 35/64 loss: -0.018835902214050293
Batch 36/64 loss: -0.031167149543762207
Batch 37/64 loss: -0.02852994203567505
Batch 38/64 loss: -0.015155673027038574
Batch 39/64 loss: -0.030922293663024902
Batch 40/64 loss: -0.02922201156616211
Batch 41/64 loss: -0.0303155779838562
Batch 42/64 loss: -0.02969259023666382
Batch 43/64 loss: -0.025561094284057617
Batch 44/64 loss: -0.050362467765808105
Batch 45/64 loss: -0.036370813846588135
Batch 46/64 loss: -0.010265827178955078
Batch 47/64 loss: -0.037094175815582275
Batch 48/64 loss: -0.015846192836761475
Batch 49/64 loss: -0.04300427436828613
Batch 50/64 loss: -0.015825092792510986
Batch 51/64 loss: -0.04513883590698242
Batch 52/64 loss: -0.04905116558074951
Batch 53/64 loss: -0.038232266902923584
Batch 54/64 loss: -0.027449846267700195
Batch 55/64 loss: -0.03679567575454712
Batch 56/64 loss: -0.0362241268157959
Batch 57/64 loss: -0.023126423358917236
Batch 58/64 loss: -0.03488534688949585
Batch 59/64 loss: -0.02562159299850464
Batch 60/64 loss: -0.020011723041534424
Batch 61/64 loss: -0.029562175273895264
Batch 62/64 loss: -0.02264028787612915
Batch 63/64 loss: -0.02686929702758789
Batch 64/64 loss: -0.019349098205566406
Epoch 394  Train loss: -0.03348222620346967  Val loss: 0.09451480030603834
Epoch 395
-------------------------------
Batch 1/64 loss: -0.03841620683670044
Batch 2/64 loss: -0.04162156581878662
Batch 3/64 loss: -0.03311556577682495
Batch 4/64 loss: -0.029826760292053223
Batch 5/64 loss: -0.0470123291015625
Batch 6/64 loss: -0.017752885818481445
Batch 7/64 loss: -0.02097475528717041
Batch 8/64 loss: -0.03874176740646362
Batch 9/64 loss: -0.03865623474121094
Batch 10/64 loss: -0.02427273988723755
Batch 11/64 loss: -0.04049795866012573
Batch 12/64 loss: -0.03987264633178711
Batch 13/64 loss: -0.04110836982727051
Batch 14/64 loss: -0.03382980823516846
Batch 15/64 loss: -0.04317891597747803
Batch 16/64 loss: -0.04391348361968994
Batch 17/64 loss: -0.0385967493057251
Batch 18/64 loss: -0.041217148303985596
Batch 19/64 loss: -0.03344053030014038
Batch 20/64 loss: -0.022597193717956543
Batch 21/64 loss: -0.04012411832809448
Batch 22/64 loss: -0.01014930009841919
Batch 23/64 loss: -0.03856372833251953
Batch 24/64 loss: -0.03629422187805176
Batch 25/64 loss: -0.052261948585510254
Batch 26/64 loss: -0.02336716651916504
Batch 27/64 loss: -0.04378914833068848
Batch 28/64 loss: -0.018908798694610596
Batch 29/64 loss: -0.017687499523162842
Batch 30/64 loss: -0.032848060131073
Batch 31/64 loss: -0.03289496898651123
Batch 32/64 loss: -0.039577603340148926
Batch 33/64 loss: -0.036475181579589844
Batch 34/64 loss: -0.02204650640487671
Batch 35/64 loss: -0.01950359344482422
Batch 36/64 loss: -0.016553640365600586
Batch 37/64 loss: -0.03242015838623047
Batch 38/64 loss: -0.028188109397888184
Batch 39/64 loss: -0.023929178714752197
Batch 40/64 loss: -0.01772254705429077
Batch 41/64 loss: -0.032918572425842285
Batch 42/64 loss: -0.0379752516746521
Batch 43/64 loss: -0.04502153396606445
Batch 44/64 loss: -0.026185929775238037
Batch 45/64 loss: -0.039504289627075195
Batch 46/64 loss: -0.03674447536468506
Batch 47/64 loss: -0.0065122246742248535
Batch 48/64 loss: -0.03532099723815918
Batch 49/64 loss: -0.02306842803955078
Batch 50/64 loss: -0.020443081855773926
Batch 51/64 loss: -0.027227044105529785
Batch 52/64 loss: -0.04532676935195923
Batch 53/64 loss: -0.040214478969573975
Batch 54/64 loss: -0.02731800079345703
Batch 55/64 loss: -0.0453948974609375
Batch 56/64 loss: -0.019900083541870117
Batch 57/64 loss: -0.03156852722167969
Batch 58/64 loss: -0.0383906364440918
Batch 59/64 loss: -0.038071513175964355
Batch 60/64 loss: -0.03164482116699219
Batch 61/64 loss: -0.038963139057159424
Batch 62/64 loss: -0.030231237411499023
Batch 63/64 loss: -0.031649231910705566
Batch 64/64 loss: -0.028967618942260742
Epoch 395  Train loss: -0.03236501918119543  Val loss: 0.09681643735092531
Epoch 396
-------------------------------
Batch 1/64 loss: -0.028239786624908447
Batch 2/64 loss: -0.02495121955871582
Batch 3/64 loss: -0.04092562198638916
Batch 4/64 loss: -0.036605238914489746
Batch 5/64 loss: -0.03035891056060791
Batch 6/64 loss: -0.015588819980621338
Batch 7/64 loss: -0.046566545963287354
Batch 8/64 loss: -0.04166370630264282
Batch 9/64 loss: -0.05020791292190552
Batch 10/64 loss: -0.01998305320739746
Batch 11/64 loss: -0.02885723114013672
Batch 12/64 loss: -0.0321844220161438
Batch 13/64 loss: -0.02890002727508545
Batch 14/64 loss: -0.028638720512390137
Batch 15/64 loss: -0.02514469623565674
Batch 16/64 loss: -0.021710336208343506
Batch 17/64 loss: -0.03277921676635742
Batch 18/64 loss: -0.037290990352630615
Batch 19/64 loss: -0.04019385576248169
Batch 20/64 loss: -0.040509939193725586
Batch 21/64 loss: -0.015818119049072266
Batch 22/64 loss: -0.03981959819793701
Batch 23/64 loss: -0.03491568565368652
Batch 24/64 loss: -0.04095184803009033
Batch 25/64 loss: -0.045623064041137695
Batch 26/64 loss: -0.004827380180358887
Batch 27/64 loss: -0.025779902935028076
Batch 28/64 loss: -0.036351144313812256
Batch 29/64 loss: -0.03991556167602539
Batch 30/64 loss: -0.03289651870727539
Batch 31/64 loss: -0.03763258457183838
Batch 32/64 loss: -0.027861475944519043
Batch 33/64 loss: -0.041481971740722656
Batch 34/64 loss: -0.04412645101547241
Batch 35/64 loss: -0.03079015016555786
Batch 36/64 loss: -0.03599780797958374
Batch 37/64 loss: -0.011359035968780518
Batch 38/64 loss: -0.041261255741119385
Batch 39/64 loss: -0.036544859409332275
Batch 40/64 loss: -0.024730801582336426
Batch 41/64 loss: -0.019740402698516846
Batch 42/64 loss: -0.02880185842514038
Batch 43/64 loss: -0.019064664840698242
Batch 44/64 loss: -0.01724112033843994
Batch 45/64 loss: -0.02599722146987915
Batch 46/64 loss: -0.04622673988342285
Batch 47/64 loss: -0.027503371238708496
Batch 48/64 loss: -0.03448688983917236
Batch 49/64 loss: -0.018984735012054443
Batch 50/64 loss: -0.039302825927734375
Batch 51/64 loss: -0.03275579214096069
Batch 52/64 loss: -0.030400097370147705
Batch 53/64 loss: -0.030084729194641113
Batch 54/64 loss: -0.03070777654647827
Batch 55/64 loss: -0.0298120379447937
Batch 56/64 loss: -0.03105330467224121
Batch 57/64 loss: -0.042765021324157715
Batch 58/64 loss: -0.013064682483673096
Batch 59/64 loss: -0.03587353229522705
Batch 60/64 loss: -0.0335773229598999
Batch 61/64 loss: -0.03658503293991089
Batch 62/64 loss: -0.03786945343017578
Batch 63/64 loss: -0.01673412322998047
Batch 64/64 loss: -0.03995513916015625
Epoch 396  Train loss: -0.03150720876805922  Val loss: 0.09357662790829373
Epoch 397
-------------------------------
Batch 1/64 loss: -0.03055405616760254
Batch 2/64 loss: -0.05350494384765625
Batch 3/64 loss: -0.04392588138580322
Batch 4/64 loss: -0.039287447929382324
Batch 5/64 loss: -0.02428436279296875
Batch 6/64 loss: -0.04065662622451782
Batch 7/64 loss: -0.03961622714996338
Batch 8/64 loss: -0.006582140922546387
Batch 9/64 loss: -0.04737740755081177
Batch 10/64 loss: -0.041710495948791504
Batch 11/64 loss: -0.03846710920333862
Batch 12/64 loss: -0.037046730518341064
Batch 13/64 loss: -0.04275345802307129
Batch 14/64 loss: -0.030949890613555908
Batch 15/64 loss: -0.05323511362075806
Batch 16/64 loss: -0.04647493362426758
Batch 17/64 loss: -0.0487101674079895
Batch 18/64 loss: -0.05065804719924927
Batch 19/64 loss: -0.03311896324157715
Batch 20/64 loss: -0.039214372634887695
Batch 21/64 loss: -0.03315919637680054
Batch 22/64 loss: -0.04389238357543945
Batch 23/64 loss: -0.05202627182006836
Batch 24/64 loss: -0.03598833084106445
Batch 25/64 loss: -0.045609891414642334
Batch 26/64 loss: -0.045158207416534424
Batch 27/64 loss: -0.04081183671951294
Batch 28/64 loss: -0.04174315929412842
Batch 29/64 loss: -0.027043581008911133
Batch 30/64 loss: -0.02851814031600952
Batch 31/64 loss: -0.03154897689819336
Batch 32/64 loss: -0.03237956762313843
Batch 33/64 loss: -0.04044163227081299
Batch 34/64 loss: -0.04409903287887573
Batch 35/64 loss: -0.04393875598907471
Batch 36/64 loss: -0.039575159549713135
Batch 37/64 loss: -0.031048059463500977
Batch 38/64 loss: -0.04954826831817627
Batch 39/64 loss: -0.029246747493743896
Batch 40/64 loss: -0.0038428306579589844
Batch 41/64 loss: -0.022828340530395508
Batch 42/64 loss: -0.047992050647735596
Batch 43/64 loss: -0.025027930736541748
Batch 44/64 loss: -0.03200972080230713
Batch 45/64 loss: 0.015713989734649658
Batch 46/64 loss: -0.025737106800079346
Batch 47/64 loss: 0.0011898279190063477
Batch 48/64 loss: -0.015874266624450684
Batch 49/64 loss: -0.024001479148864746
Batch 50/64 loss: -0.026728451251983643
Batch 51/64 loss: -0.03391927480697632
Batch 52/64 loss: -0.0399131178855896
Batch 53/64 loss: -0.029676198959350586
Batch 54/64 loss: -0.03283888101577759
Batch 55/64 loss: -0.029308617115020752
Batch 56/64 loss: -0.022953033447265625
Batch 57/64 loss: -0.039186179637908936
Batch 58/64 loss: -0.03196918964385986
Batch 59/64 loss: -0.047168195247650146
Batch 60/64 loss: -0.0432887077331543
Batch 61/64 loss: -0.020724177360534668
Batch 62/64 loss: -0.03750264644622803
Batch 63/64 loss: -0.029906630516052246
Batch 64/64 loss: -0.030512332916259766
Epoch 397  Train loss: -0.03438875347960229  Val loss: 0.09442342208423156
Epoch 398
-------------------------------
Batch 1/64 loss: -0.051963627338409424
Batch 2/64 loss: -0.017899811267852783
Batch 3/64 loss: -0.0453299880027771
Batch 4/64 loss: -0.050852298736572266
Batch 5/64 loss: -0.040440499782562256
Batch 6/64 loss: -0.04402923583984375
Batch 7/64 loss: -0.026443660259246826
Batch 8/64 loss: -0.04977715015411377
Batch 9/64 loss: -0.027457475662231445
Batch 10/64 loss: -0.028595685958862305
Batch 11/64 loss: -0.04092204570770264
Batch 12/64 loss: -0.04141378402709961
Batch 13/64 loss: -0.0368228554725647
Batch 14/64 loss: -0.0368962287902832
Batch 15/64 loss: -0.04533994197845459
Batch 16/64 loss: -0.008344471454620361
Batch 17/64 loss: -0.037854552268981934
Batch 18/64 loss: -0.04485130310058594
Batch 19/64 loss: -0.016173481941223145
Batch 20/64 loss: -0.05238199234008789
Batch 21/64 loss: -0.03990626335144043
Batch 22/64 loss: -0.05830115079879761
Batch 23/64 loss: -0.033698081970214844
Batch 24/64 loss: -0.038146793842315674
Batch 25/64 loss: -0.04871559143066406
Batch 26/64 loss: -0.021966993808746338
Batch 27/64 loss: -0.03843933343887329
Batch 28/64 loss: -0.040541112422943115
Batch 29/64 loss: -0.0555804967880249
Batch 30/64 loss: -0.03914165496826172
Batch 31/64 loss: -0.03277701139450073
Batch 32/64 loss: -0.029904484748840332
Batch 33/64 loss: -0.03711414337158203
Batch 34/64 loss: -0.0472484827041626
Batch 35/64 loss: -0.04456913471221924
Batch 36/64 loss: -0.027245521545410156
Batch 37/64 loss: -0.044178903102874756
Batch 38/64 loss: -0.04576277732849121
Batch 39/64 loss: -0.0198897123336792
Batch 40/64 loss: -0.040633976459503174
Batch 41/64 loss: -0.04050189256668091
Batch 42/64 loss: -0.009762406349182129
Batch 43/64 loss: -0.02490532398223877
Batch 44/64 loss: -0.039347708225250244
Batch 45/64 loss: -0.04302966594696045
Batch 46/64 loss: -0.027346014976501465
Batch 47/64 loss: -0.03345894813537598
Batch 48/64 loss: -0.02435249090194702
Batch 49/64 loss: -0.026517868041992188
Batch 50/64 loss: -0.027824819087982178
Batch 51/64 loss: -0.019075214862823486
Batch 52/64 loss: -0.018727421760559082
Batch 53/64 loss: 0.0007519721984863281
Batch 54/64 loss: -0.011317074298858643
Batch 55/64 loss: -0.0236741304397583
Batch 56/64 loss: -0.013357341289520264
Batch 57/64 loss: -0.035468220710754395
Batch 58/64 loss: -0.023255646228790283
Batch 59/64 loss: -0.032938241958618164
Batch 60/64 loss: -0.026593923568725586
Batch 61/64 loss: -0.03806793689727783
Batch 62/64 loss: -0.02916574478149414
Batch 63/64 loss: -0.037449538707733154
Batch 64/64 loss: -0.0206260085105896
Epoch 398  Train loss: -0.033700499113868264  Val loss: 0.09439955769535602
Epoch 399
-------------------------------
Batch 1/64 loss: -0.03820693492889404
Batch 2/64 loss: -0.01950591802597046
Batch 3/64 loss: -0.03823971748352051
Batch 4/64 loss: -0.039406538009643555
Batch 5/64 loss: -0.040188491344451904
Batch 6/64 loss: -0.037311434745788574
Batch 7/64 loss: -0.02966618537902832
Batch 8/64 loss: -0.04442530870437622
Batch 9/64 loss: -0.03034418821334839
Batch 10/64 loss: -0.036820828914642334
Batch 11/64 loss: -0.04311680793762207
Batch 12/64 loss: -0.03712356090545654
Batch 13/64 loss: -0.042098283767700195
Batch 14/64 loss: -0.02372974157333374
Batch 15/64 loss: -0.035085439682006836
Batch 16/64 loss: -0.030211329460144043
Batch 17/64 loss: -0.029524266719818115
Batch 18/64 loss: -0.02773880958557129
Batch 19/64 loss: -0.03793603181838989
Batch 20/64 loss: -0.05422711372375488
Batch 21/64 loss: -0.019537627696990967
Batch 22/64 loss: -0.03181058168411255
Batch 23/64 loss: -0.043762266635894775
Batch 24/64 loss: -0.05718708038330078
Batch 25/64 loss: -0.03346830606460571
Batch 26/64 loss: -0.03225052356719971
Batch 27/64 loss: -0.04242348670959473
Batch 28/64 loss: -0.03413653373718262
Batch 29/64 loss: -0.04006659984588623
Batch 30/64 loss: -0.04024416208267212
Batch 31/64 loss: -0.0331341028213501
Batch 32/64 loss: -0.019861996173858643
Batch 33/64 loss: 0.0036324262619018555
Batch 34/64 loss: 0.0005351901054382324
Batch 35/64 loss: -0.028045237064361572
Batch 36/64 loss: -0.024474382400512695
Batch 37/64 loss: -0.01865255832672119
Batch 38/64 loss: -0.04111748933792114
Batch 39/64 loss: -0.03818696737289429
Batch 40/64 loss: -0.019393742084503174
Batch 41/64 loss: -0.014853239059448242
Batch 42/64 loss: -0.01830768585205078
Batch 43/64 loss: -0.043222904205322266
Batch 44/64 loss: -0.03125828504562378
Batch 45/64 loss: -0.03233635425567627
Batch 46/64 loss: -0.022496700286865234
Batch 47/64 loss: -0.01588970422744751
Batch 48/64 loss: -0.021972835063934326
Batch 49/64 loss: -0.033109068870544434
Batch 50/64 loss: -0.018190979957580566
Batch 51/64 loss: -0.02375626564025879
Batch 52/64 loss: -0.03262394666671753
Batch 53/64 loss: -0.040805160999298096
Batch 54/64 loss: -0.03849226236343384
Batch 55/64 loss: -0.03687244653701782
Batch 56/64 loss: -0.028299570083618164
Batch 57/64 loss: -0.02018052339553833
Batch 58/64 loss: -0.030431687831878662
Batch 59/64 loss: -0.03942883014678955
Batch 60/64 loss: -0.04255789518356323
Batch 61/64 loss: -0.03553074598312378
Batch 62/64 loss: -0.026864707469940186
Batch 63/64 loss: -0.036919474601745605
Batch 64/64 loss: -0.02864670753479004
Epoch 399  Train loss: -0.031598106085085405  Val loss: 0.0968631737420649
Epoch 400
-------------------------------
Batch 1/64 loss: -0.032028257846832275
Batch 2/64 loss: -0.024242162704467773
Batch 3/64 loss: -0.020012855529785156
Batch 4/64 loss: -0.04892849922180176
Batch 5/64 loss: -0.0402148962020874
Batch 6/64 loss: -0.0337597131729126
Batch 7/64 loss: -0.04608875513076782
Batch 8/64 loss: -0.03851276636123657
Batch 9/64 loss: -0.029347360134124756
Batch 10/64 loss: -0.04140651226043701
Batch 11/64 loss: -0.04080390930175781
Batch 12/64 loss: -0.047846317291259766
Batch 13/64 loss: -0.03980785608291626
Batch 14/64 loss: -0.0377887487411499
Batch 15/64 loss: -0.0426783561706543
Batch 16/64 loss: -0.05197286605834961
Batch 17/64 loss: -0.00054931640625
Batch 18/64 loss: -0.03411132097244263
Batch 19/64 loss: -0.04578959941864014
Batch 20/64 loss: -0.038989365100860596
Batch 21/64 loss: -0.042700231075286865
Batch 22/64 loss: -0.030808091163635254
Batch 23/64 loss: -0.04438507556915283
Batch 24/64 loss: -0.0438423752784729
Batch 25/64 loss: -0.033837854862213135
Batch 26/64 loss: -0.03670549392700195
Batch 27/64 loss: -0.030545949935913086
Batch 28/64 loss: -0.009229898452758789
Batch 29/64 loss: -0.01755368709564209
Batch 30/64 loss: -0.025574207305908203
Batch 31/64 loss: -0.03906375169754028
Batch 32/64 loss: -0.030778825283050537
Batch 33/64 loss: -0.04508388042449951
Batch 34/64 loss: -0.03579521179199219
Batch 35/64 loss: -0.024456501007080078
Batch 36/64 loss: -0.05122882127761841
Batch 37/64 loss: -0.029679536819458008
Batch 38/64 loss: -0.02095508575439453
Batch 39/64 loss: -0.041861772537231445
Batch 40/64 loss: -0.028009235858917236
Batch 41/64 loss: -0.03642314672470093
Batch 42/64 loss: -0.017528831958770752
Batch 43/64 loss: -0.03552877902984619
Batch 44/64 loss: -0.030331790447235107
Batch 45/64 loss: -0.025957703590393066
Batch 46/64 loss: -0.033591628074645996
Batch 47/64 loss: -0.014328300952911377
Batch 48/64 loss: -0.032657623291015625
Batch 49/64 loss: -0.024512529373168945
Batch 50/64 loss: -0.047923922538757324
Batch 51/64 loss: -0.02434098720550537
Batch 52/64 loss: -0.03122025728225708
Batch 53/64 loss: -0.028777360916137695
Batch 54/64 loss: -0.04465991258621216
Batch 55/64 loss: -0.02931123971939087
Batch 56/64 loss: -0.038885414600372314
Batch 57/64 loss: -0.034121572971343994
Batch 58/64 loss: -0.029722929000854492
Batch 59/64 loss: -0.037764668464660645
Batch 60/64 loss: -0.03661298751831055
Batch 61/64 loss: -0.030045270919799805
Batch 62/64 loss: -0.045185625553131104
Batch 63/64 loss: -0.03232687711715698
Batch 64/64 loss: -0.009362876415252686
Epoch 400  Train loss: -0.03365892452352187  Val loss: 0.09538839936666063
Epoch 401
-------------------------------
Batch 1/64 loss: -0.03947460651397705
Batch 2/64 loss: -0.02275872230529785
Batch 3/64 loss: -0.014396965503692627
Batch 4/64 loss: -0.03521519899368286
Batch 5/64 loss: -0.0296209454536438
Batch 6/64 loss: -0.05039620399475098
Batch 7/64 loss: -0.046306610107421875
Batch 8/64 loss: -0.047444939613342285
Batch 9/64 loss: -0.03569108247756958
Batch 10/64 loss: -0.03959238529205322
Batch 11/64 loss: -0.026344358921051025
Batch 12/64 loss: -0.030439376831054688
Batch 13/64 loss: -0.033728182315826416
Batch 14/64 loss: -0.03465944528579712
Batch 15/64 loss: -0.02805805206298828
Batch 16/64 loss: -0.040466248989105225
Batch 17/64 loss: -0.04096466302871704
Batch 18/64 loss: -0.036871135234832764
Batch 19/64 loss: -0.04563283920288086
Batch 20/64 loss: -0.03862124681472778
Batch 21/64 loss: -0.03003591299057007
Batch 22/64 loss: -0.033214569091796875
Batch 23/64 loss: -0.03463643789291382
Batch 24/64 loss: -0.04900217056274414
Batch 25/64 loss: -0.04756075143814087
Batch 26/64 loss: -0.04347586631774902
Batch 27/64 loss: -0.004967689514160156
Batch 28/64 loss: -0.046585798263549805
Batch 29/64 loss: -0.04037117958068848
Batch 30/64 loss: -0.01778167486190796
Batch 31/64 loss: -0.028707504272460938
Batch 32/64 loss: -0.03588402271270752
Batch 33/64 loss: -0.04145944118499756
Batch 34/64 loss: -0.03891158103942871
Batch 35/64 loss: -0.03292691707611084
Batch 36/64 loss: -0.03174746036529541
Batch 37/64 loss: -0.04057198762893677
Batch 38/64 loss: -0.03763836622238159
Batch 39/64 loss: -0.053749680519104004
Batch 40/64 loss: -0.022862672805786133
Batch 41/64 loss: -0.01686251163482666
Batch 42/64 loss: -0.024213194847106934
Batch 43/64 loss: -0.01782923936843872
Batch 44/64 loss: -0.03916370868682861
Batch 45/64 loss: -0.02954655885696411
Batch 46/64 loss: -0.036596477031707764
Batch 47/64 loss: -0.04863518476486206
Batch 48/64 loss: -0.03160804510116577
Batch 49/64 loss: -0.03455007076263428
Batch 50/64 loss: -0.020075976848602295
Batch 51/64 loss: -0.04347330331802368
Batch 52/64 loss: -0.03757137060165405
Batch 53/64 loss: -0.02893054485321045
Batch 54/64 loss: -0.025627732276916504
Batch 55/64 loss: -0.024712443351745605
Batch 56/64 loss: -0.032290875911712646
Batch 57/64 loss: -0.0326005220413208
Batch 58/64 loss: -0.02433234453201294
Batch 59/64 loss: -0.046652793884277344
Batch 60/64 loss: -0.04134261608123779
Batch 61/64 loss: -0.021601557731628418
Batch 62/64 loss: -0.03876793384552002
Batch 63/64 loss: -0.028659403324127197
Batch 64/64 loss: -0.030903518199920654
Epoch 401  Train loss: -0.034158383397495044  Val loss: 0.09548840776751541
Epoch 402
-------------------------------
Batch 1/64 loss: -0.03934049606323242
Batch 2/64 loss: -0.062211573123931885
Batch 3/64 loss: -0.036567509174346924
Batch 4/64 loss: -0.028253257274627686
Batch 5/64 loss: -0.04843682050704956
Batch 6/64 loss: -0.03782302141189575
Batch 7/64 loss: -0.038641929626464844
Batch 8/64 loss: -0.04911959171295166
Batch 9/64 loss: -0.03131455183029175
Batch 10/64 loss: -0.045092880725860596
Batch 11/64 loss: -0.044471919536590576
Batch 12/64 loss: -0.04424941539764404
Batch 13/64 loss: -0.04547107219696045
Batch 14/64 loss: -0.046392083168029785
Batch 15/64 loss: -0.04813253879547119
Batch 16/64 loss: -0.028971970081329346
Batch 17/64 loss: -0.03792637586593628
Batch 18/64 loss: -0.054700255393981934
Batch 19/64 loss: -0.04519319534301758
Batch 20/64 loss: -0.03148293495178223
Batch 21/64 loss: -0.017551183700561523
Batch 22/64 loss: -0.02223968505859375
Batch 23/64 loss: -0.033104002475738525
Batch 24/64 loss: -0.04408526420593262
Batch 25/64 loss: -0.03685945272445679
Batch 26/64 loss: -0.04214656352996826
Batch 27/64 loss: -0.03834223747253418
Batch 28/64 loss: -0.021515607833862305
Batch 29/64 loss: -0.03455233573913574
Batch 30/64 loss: -0.034739911556243896
Batch 31/64 loss: -0.03154629468917847
Batch 32/64 loss: -0.042389094829559326
Batch 33/64 loss: -0.033115923404693604
Batch 34/64 loss: -0.05258125066757202
Batch 35/64 loss: -0.03614157438278198
Batch 36/64 loss: -0.030685722827911377
Batch 37/64 loss: -0.02489006519317627
Batch 38/64 loss: -0.040631771087646484
Batch 39/64 loss: -0.030154824256896973
Batch 40/64 loss: -0.05555260181427002
Batch 41/64 loss: -0.033483147621154785
Batch 42/64 loss: -0.032377421855926514
Batch 43/64 loss: -0.0326613187789917
Batch 44/64 loss: -0.03554427623748779
Batch 45/64 loss: -0.02669590711593628
Batch 46/64 loss: -0.028209686279296875
Batch 47/64 loss: -0.03373825550079346
Batch 48/64 loss: -0.02877908945083618
Batch 49/64 loss: -0.0551108717918396
Batch 50/64 loss: -0.03170371055603027
Batch 51/64 loss: -0.047548770904541016
Batch 52/64 loss: -0.03578364849090576
Batch 53/64 loss: -0.033561527729034424
Batch 54/64 loss: -0.03912895917892456
Batch 55/64 loss: -0.04407835006713867
Batch 56/64 loss: -0.03899484872817993
Batch 57/64 loss: -0.0418662428855896
Batch 58/64 loss: -0.03588038682937622
Batch 59/64 loss: -0.03163021802902222
Batch 60/64 loss: 0.007200896739959717
Batch 61/64 loss: -0.016843199729919434
Batch 62/64 loss: -0.023782193660736084
Batch 63/64 loss: -0.04318970441818237
Batch 64/64 loss: -0.020390033721923828
Epoch 402  Train loss: -0.036475327435661765  Val loss: 0.09458274939625534
Epoch 403
-------------------------------
Batch 1/64 loss: -0.04409116506576538
Batch 2/64 loss: -0.03523749113082886
Batch 3/64 loss: -0.037583231925964355
Batch 4/64 loss: -0.04052245616912842
Batch 5/64 loss: -0.039807021617889404
Batch 6/64 loss: -0.044731318950653076
Batch 7/64 loss: -0.03142130374908447
Batch 8/64 loss: -0.04799383878707886
Batch 9/64 loss: -0.05814659595489502
Batch 10/64 loss: -0.03649425506591797
Batch 11/64 loss: -0.0463031530380249
Batch 12/64 loss: -0.038722991943359375
Batch 13/64 loss: -0.029559850692749023
Batch 14/64 loss: -0.022684812545776367
Batch 15/64 loss: -0.015496015548706055
Batch 16/64 loss: -0.03609156608581543
Batch 17/64 loss: -0.034432411193847656
Batch 18/64 loss: -0.03469836711883545
Batch 19/64 loss: -0.03458297252655029
Batch 20/64 loss: -0.031155943870544434
Batch 21/64 loss: -0.024423956871032715
Batch 22/64 loss: -0.042517900466918945
Batch 23/64 loss: -0.052204430103302
Batch 24/64 loss: 0.0034922361373901367
Batch 25/64 loss: -0.044453442096710205
Batch 26/64 loss: -0.04247826337814331
Batch 27/64 loss: -0.0381622314453125
Batch 28/64 loss: -0.03674185276031494
Batch 29/64 loss: -0.025115787982940674
Batch 30/64 loss: -0.03387361764907837
Batch 31/64 loss: -0.03404593467712402
Batch 32/64 loss: -0.04642641544342041
Batch 33/64 loss: -0.03313028812408447
Batch 34/64 loss: -0.021906793117523193
Batch 35/64 loss: -0.035153746604919434
Batch 36/64 loss: -0.03468865156173706
Batch 37/64 loss: -0.041934192180633545
Batch 38/64 loss: -0.024149060249328613
Batch 39/64 loss: -0.031392812728881836
Batch 40/64 loss: -0.02771693468093872
Batch 41/64 loss: -0.027816057205200195
Batch 42/64 loss: -0.04934227466583252
Batch 43/64 loss: -0.04915344715118408
Batch 44/64 loss: -0.03045332431793213
Batch 45/64 loss: -0.034255146980285645
Batch 46/64 loss: -0.0440792441368103
Batch 47/64 loss: -0.039023756980895996
Batch 48/64 loss: -0.04075753688812256
Batch 49/64 loss: -0.06040388345718384
Batch 50/64 loss: -0.049267590045928955
Batch 51/64 loss: -0.02851271629333496
Batch 52/64 loss: -0.030002355575561523
Batch 53/64 loss: -0.02273792028427124
Batch 54/64 loss: -0.03545743227005005
Batch 55/64 loss: -0.024067163467407227
Batch 56/64 loss: -0.03366124629974365
Batch 57/64 loss: -0.03548628091812134
Batch 58/64 loss: -0.04610788822174072
Batch 59/64 loss: -0.01987820863723755
Batch 60/64 loss: -0.01681685447692871
Batch 61/64 loss: -0.04292804002761841
Batch 62/64 loss: -0.04552459716796875
Batch 63/64 loss: -0.04696166515350342
Batch 64/64 loss: -0.03245025873184204
Epoch 403  Train loss: -0.03582452021393121  Val loss: 0.09482540789338731
Epoch 404
-------------------------------
Batch 1/64 loss: -0.0558091402053833
Batch 2/64 loss: -0.04771256446838379
Batch 3/64 loss: -0.012375056743621826
Batch 4/64 loss: -0.041289567947387695
Batch 5/64 loss: -0.04792428016662598
Batch 6/64 loss: -0.02254354953765869
Batch 7/64 loss: -0.05331528186798096
Batch 8/64 loss: -0.037920355796813965
Batch 9/64 loss: -0.03501474857330322
Batch 10/64 loss: -0.03717398643493652
Batch 11/64 loss: -0.03369641304016113
Batch 12/64 loss: -0.03448033332824707
Batch 13/64 loss: -0.025670349597930908
Batch 14/64 loss: -0.02563166618347168
Batch 15/64 loss: -0.04873979091644287
Batch 16/64 loss: -0.03217935562133789
Batch 17/64 loss: -0.03852623701095581
Batch 18/64 loss: -0.021624445915222168
Batch 19/64 loss: -0.03811752796173096
Batch 20/64 loss: -0.021441638469696045
Batch 21/64 loss: -0.029002904891967773
Batch 22/64 loss: -0.04150390625
Batch 23/64 loss: -0.03236973285675049
Batch 24/64 loss: -0.031378328800201416
Batch 25/64 loss: -0.033452630043029785
Batch 26/64 loss: -0.027974367141723633
Batch 27/64 loss: -0.03383612632751465
Batch 28/64 loss: -0.04581671953201294
Batch 29/64 loss: -0.03147101402282715
Batch 30/64 loss: -0.029475688934326172
Batch 31/64 loss: -0.021097302436828613
Batch 32/64 loss: -0.04246079921722412
Batch 33/64 loss: -0.03408628702163696
Batch 34/64 loss: -0.01749587059020996
Batch 35/64 loss: -0.04959326982498169
Batch 36/64 loss: -0.028821170330047607
Batch 37/64 loss: -0.03701746463775635
Batch 38/64 loss: -0.0383228063583374
Batch 39/64 loss: -0.019475936889648438
Batch 40/64 loss: -0.029355287551879883
Batch 41/64 loss: -0.03663313388824463
Batch 42/64 loss: -0.03173333406448364
Batch 43/64 loss: -0.04910534620285034
Batch 44/64 loss: -0.01506662368774414
Batch 45/64 loss: -0.042104899883270264
Batch 46/64 loss: -0.03329157829284668
Batch 47/64 loss: -0.05609554052352905
Batch 48/64 loss: -0.04621022939682007
Batch 49/64 loss: -0.036051154136657715
Batch 50/64 loss: -0.027318298816680908
Batch 51/64 loss: -0.03005129098892212
Batch 52/64 loss: -0.026393234729766846
Batch 53/64 loss: -0.010845422744750977
Batch 54/64 loss: -0.0363774299621582
Batch 55/64 loss: -0.03303098678588867
Batch 56/64 loss: -0.039036571979522705
Batch 57/64 loss: -0.05439591407775879
Batch 58/64 loss: -0.03692704439163208
Batch 59/64 loss: -0.019697189331054688
Batch 60/64 loss: -0.03393900394439697
Batch 61/64 loss: -0.03583705425262451
Batch 62/64 loss: -0.048094749450683594
Batch 63/64 loss: -0.030202090740203857
Batch 64/64 loss: -0.03502291440963745
Epoch 404  Train loss: -0.03450822292589674  Val loss: 0.09376014344061363
Epoch 405
-------------------------------
Batch 1/64 loss: -0.03456145524978638
Batch 2/64 loss: -0.04633450508117676
Batch 3/64 loss: -0.04533737897872925
Batch 4/64 loss: -0.0459824800491333
Batch 5/64 loss: -0.03835493326187134
Batch 6/64 loss: -0.05293840169906616
Batch 7/64 loss: -0.04865455627441406
Batch 8/64 loss: -0.03455168008804321
Batch 9/64 loss: -0.05119603872299194
Batch 10/64 loss: -0.0338749885559082
Batch 11/64 loss: -0.034360408782958984
Batch 12/64 loss: -0.048207879066467285
Batch 13/64 loss: -0.042346954345703125
Batch 14/64 loss: -0.05741089582443237
Batch 15/64 loss: -0.012283027172088623
Batch 16/64 loss: -0.023312389850616455
Batch 17/64 loss: -0.020933866500854492
Batch 18/64 loss: -0.01925945281982422
Batch 19/64 loss: -0.036054372787475586
Batch 20/64 loss: -0.04317504167556763
Batch 21/64 loss: -0.051795244216918945
Batch 22/64 loss: -0.03167116641998291
Batch 23/64 loss: -0.046147704124450684
Batch 24/64 loss: -0.03824126720428467
Batch 25/64 loss: -0.01490640640258789
Batch 26/64 loss: -0.03848844766616821
Batch 27/64 loss: -0.043422698974609375
Batch 28/64 loss: -0.03518325090408325
Batch 29/64 loss: -0.03623396158218384
Batch 30/64 loss: -0.03389042615890503
Batch 31/64 loss: -0.037172675132751465
Batch 32/64 loss: -0.037367820739746094
Batch 33/64 loss: -0.05104184150695801
Batch 34/64 loss: -0.03289681673049927
Batch 35/64 loss: -0.03211331367492676
Batch 36/64 loss: -0.04333829879760742
Batch 37/64 loss: -0.026644647121429443
Batch 38/64 loss: -0.05454510450363159
Batch 39/64 loss: -0.03229856491088867
Batch 40/64 loss: -0.029049336910247803
Batch 41/64 loss: -0.019949913024902344
Batch 42/64 loss: -0.030762672424316406
Batch 43/64 loss: -0.03624987602233887
Batch 44/64 loss: -0.021999597549438477
Batch 45/64 loss: -0.006605029106140137
Batch 46/64 loss: -0.023470044136047363
Batch 47/64 loss: -0.04194897413253784
Batch 48/64 loss: -0.03805786371231079
Batch 49/64 loss: -0.02933812141418457
Batch 50/64 loss: -0.042958855628967285
Batch 51/64 loss: -0.037744343280792236
Batch 52/64 loss: -0.042856454849243164
Batch 53/64 loss: -0.016512930393218994
Batch 54/64 loss: -0.04298067092895508
Batch 55/64 loss: -0.03969496488571167
Batch 56/64 loss: -0.03464847803115845
Batch 57/64 loss: -0.03779935836791992
Batch 58/64 loss: -0.04066348075866699
Batch 59/64 loss: -0.03260529041290283
Batch 60/64 loss: -0.03206133842468262
Batch 61/64 loss: -0.0484461784362793
Batch 62/64 loss: -0.044229984283447266
Batch 63/64 loss: -0.03901338577270508
Batch 64/64 loss: -0.039335668087005615
Epoch 405  Train loss: -0.036481243255091646  Val loss: 0.09462470778894588
Epoch 406
-------------------------------
Batch 1/64 loss: -7.49826431274414e-05
Batch 2/64 loss: -0.04262751340866089
Batch 3/64 loss: -0.0419846773147583
Batch 4/64 loss: -0.03225928544998169
Batch 5/64 loss: -0.04311561584472656
Batch 6/64 loss: -0.04919850826263428
Batch 7/64 loss: -0.03167426586151123
Batch 8/64 loss: -0.036920905113220215
Batch 9/64 loss: -0.03234833478927612
Batch 10/64 loss: -0.05395042896270752
Batch 11/64 loss: -0.02639639377593994
Batch 12/64 loss: -0.03540545701980591
Batch 13/64 loss: -0.04264330863952637
Batch 14/64 loss: -0.027025341987609863
Batch 15/64 loss: -0.037545204162597656
Batch 16/64 loss: -0.04491162300109863
Batch 17/64 loss: -0.017964303493499756
Batch 18/64 loss: -0.04279470443725586
Batch 19/64 loss: -0.04166090488433838
Batch 20/64 loss: -0.029641568660736084
Batch 21/64 loss: -0.03628349304199219
Batch 22/64 loss: -0.030288398265838623
Batch 23/64 loss: -0.04197502136230469
Batch 24/64 loss: -0.02458411455154419
Batch 25/64 loss: -0.040176451206207275
Batch 26/64 loss: -0.04018354415893555
Batch 27/64 loss: -0.03156006336212158
Batch 28/64 loss: -0.03255617618560791
Batch 29/64 loss: -0.013826847076416016
Batch 30/64 loss: -0.04447627067565918
Batch 31/64 loss: -0.05039554834365845
Batch 32/64 loss: -0.010112285614013672
Batch 33/64 loss: -0.038897931575775146
Batch 34/64 loss: -0.045068323612213135
Batch 35/64 loss: -0.020404696464538574
Batch 36/64 loss: -0.00443422794342041
Batch 37/64 loss: -0.03685957193374634
Batch 38/64 loss: -0.011961698532104492
Batch 39/64 loss: -0.04886817932128906
Batch 40/64 loss: -0.012326836585998535
Batch 41/64 loss: -0.038860201835632324
Batch 42/64 loss: -0.021733760833740234
Batch 43/64 loss: -0.043415367603302
Batch 44/64 loss: -0.04352593421936035
Batch 45/64 loss: -0.04071325063705444
Batch 46/64 loss: -0.042221665382385254
Batch 47/64 loss: -0.015048384666442871
Batch 48/64 loss: -0.03533536195755005
Batch 49/64 loss: -0.017044544219970703
Batch 50/64 loss: -0.021540403366088867
Batch 51/64 loss: -0.03348135948181152
Batch 52/64 loss: -0.02375626564025879
Batch 53/64 loss: -0.02537059783935547
Batch 54/64 loss: -0.03321117162704468
Batch 55/64 loss: -0.025837481021881104
Batch 56/64 loss: -0.04257655143737793
Batch 57/64 loss: -0.020513415336608887
Batch 58/64 loss: -0.02760636806488037
Batch 59/64 loss: -0.030669808387756348
Batch 60/64 loss: -0.04938375949859619
Batch 61/64 loss: -0.04275137186050415
Batch 62/64 loss: -0.0458378791809082
Batch 63/64 loss: -0.03837019205093384
Batch 64/64 loss: -0.007946252822875977
Epoch 406  Train loss: -0.03278663205165489  Val loss: 0.09566723358180515
Epoch 407
-------------------------------
Batch 1/64 loss: -0.05156564712524414
Batch 2/64 loss: -0.04953700304031372
Batch 3/64 loss: -0.04689896106719971
Batch 4/64 loss: -0.03626662492752075
Batch 5/64 loss: -0.018514394760131836
Batch 6/64 loss: -0.03261953592300415
Batch 7/64 loss: -0.046503663063049316
Batch 8/64 loss: -0.05450248718261719
Batch 9/64 loss: -0.022923946380615234
Batch 10/64 loss: -0.03873121738433838
Batch 11/64 loss: -0.046039044857025146
Batch 12/64 loss: -0.011519432067871094
Batch 13/64 loss: -0.032491326332092285
Batch 14/64 loss: -0.0386541485786438
Batch 15/64 loss: -0.04719686508178711
Batch 16/64 loss: 0.014562726020812988
Batch 17/64 loss: -0.026954352855682373
Batch 18/64 loss: -0.03431856632232666
Batch 19/64 loss: -0.040228962898254395
Batch 20/64 loss: -0.053211092948913574
Batch 21/64 loss: -0.0403478741645813
Batch 22/64 loss: -0.019858479499816895
Batch 23/64 loss: -0.0365176796913147
Batch 24/64 loss: -0.043238043785095215
Batch 25/64 loss: -0.03077256679534912
Batch 26/64 loss: -0.03913223743438721
Batch 27/64 loss: -0.025394201278686523
Batch 28/64 loss: -0.03551685810089111
Batch 29/64 loss: -0.03011155128479004
Batch 30/64 loss: -0.031838297843933105
Batch 31/64 loss: -0.021859586238861084
Batch 32/64 loss: -0.031134426593780518
Batch 33/64 loss: -0.025977015495300293
Batch 34/64 loss: -0.03384023904800415
Batch 35/64 loss: -0.033552587032318115
Batch 36/64 loss: -0.022679269313812256
Batch 37/64 loss: -0.03436434268951416
Batch 38/64 loss: -0.011668860912322998
Batch 39/64 loss: -0.03301960229873657
Batch 40/64 loss: -0.04218798875808716
Batch 41/64 loss: -0.030559301376342773
Batch 42/64 loss: -0.042701900005340576
Batch 43/64 loss: -0.04005694389343262
Batch 44/64 loss: -0.029253244400024414
Batch 45/64 loss: -0.04970937967300415
Batch 46/64 loss: -0.03319692611694336
Batch 47/64 loss: -0.016508042812347412
Batch 48/64 loss: -0.02195453643798828
Batch 49/64 loss: -0.03861832618713379
Batch 50/64 loss: -0.046922922134399414
Batch 51/64 loss: -0.03777122497558594
Batch 52/64 loss: -0.049300432205200195
Batch 53/64 loss: -0.05479896068572998
Batch 54/64 loss: -0.039892375469207764
Batch 55/64 loss: -0.04130285978317261
Batch 56/64 loss: -0.01413041353225708
Batch 57/64 loss: -0.031384825706481934
Batch 58/64 loss: -0.04384124279022217
Batch 59/64 loss: -0.028218328952789307
Batch 60/64 loss: -0.04422903060913086
Batch 61/64 loss: -0.03185427188873291
Batch 62/64 loss: -0.022048354148864746
Batch 63/64 loss: -0.036518216133117676
Batch 64/64 loss: -0.03464305400848389
Epoch 407  Train loss: -0.03431969577190923  Val loss: 0.09565835740558061
Epoch 408
-------------------------------
Batch 1/64 loss: -0.02990436553955078
Batch 2/64 loss: -0.03148782253265381
Batch 3/64 loss: -0.027278482913970947
Batch 4/64 loss: -0.03505808115005493
Batch 5/64 loss: -0.05019956827163696
Batch 6/64 loss: -0.032912611961364746
Batch 7/64 loss: -0.05531841516494751
Batch 8/64 loss: -0.039112210273742676
Batch 9/64 loss: -0.04528731107711792
Batch 10/64 loss: -0.03331887722015381
Batch 11/64 loss: -0.051254093647003174
Batch 12/64 loss: -0.028834819793701172
Batch 13/64 loss: -0.04120779037475586
Batch 14/64 loss: -0.035646796226501465
Batch 15/64 loss: -0.020148634910583496
Batch 16/64 loss: -0.03440123796463013
Batch 17/64 loss: -0.04094213247299194
Batch 18/64 loss: -0.012886404991149902
Batch 19/64 loss: -0.03021538257598877
Batch 20/64 loss: -0.031032681465148926
Batch 21/64 loss: -0.033617615699768066
Batch 22/64 loss: -0.03311091661453247
Batch 23/64 loss: -0.027387797832489014
Batch 24/64 loss: -0.034256815910339355
Batch 25/64 loss: -0.022261202335357666
Batch 26/64 loss: -0.04374760389328003
Batch 27/64 loss: -0.04076892137527466
Batch 28/64 loss: -0.02491748332977295
Batch 29/64 loss: -0.03866410255432129
Batch 30/64 loss: -0.04099440574645996
Batch 31/64 loss: -0.026806354522705078
Batch 32/64 loss: -0.045809388160705566
Batch 33/64 loss: -0.02966684103012085
Batch 34/64 loss: -0.039009809494018555
Batch 35/64 loss: -0.03885167837142944
Batch 36/64 loss: -0.04035615921020508
Batch 37/64 loss: -0.04498100280761719
Batch 38/64 loss: -0.03594714403152466
Batch 39/64 loss: -0.03357267379760742
Batch 40/64 loss: -0.023486673831939697
Batch 41/64 loss: -0.025651931762695312
Batch 42/64 loss: -0.050739169120788574
Batch 43/64 loss: -0.05051511526107788
Batch 44/64 loss: -0.03608208894729614
Batch 45/64 loss: -0.026480495929718018
Batch 46/64 loss: -0.029096603393554688
Batch 47/64 loss: -0.03861492872238159
Batch 48/64 loss: -0.0352783203125
Batch 49/64 loss: -0.05006927251815796
Batch 50/64 loss: -0.03704601526260376
Batch 51/64 loss: -0.04476785659790039
Batch 52/64 loss: -0.030752182006835938
Batch 53/64 loss: -0.03411459922790527
Batch 54/64 loss: -0.046003878116607666
Batch 55/64 loss: -0.03418773412704468
Batch 56/64 loss: -0.02716726064682007
Batch 57/64 loss: -0.021325886249542236
Batch 58/64 loss: -0.02415931224822998
Batch 59/64 loss: -0.02667313814163208
Batch 60/64 loss: -0.038886070251464844
Batch 61/64 loss: -0.04194706678390503
Batch 62/64 loss: -0.03674805164337158
Batch 63/64 loss: -0.047063350677490234
Batch 64/64 loss: -0.022940635681152344
Epoch 408  Train loss: -0.03537628416921578  Val loss: 0.09563008329712648
Epoch 409
-------------------------------
Batch 1/64 loss: -0.045772016048431396
Batch 2/64 loss: -0.03565186262130737
Batch 3/64 loss: -0.0222090482711792
Batch 4/64 loss: -0.042927682399749756
Batch 5/64 loss: -0.04374438524246216
Batch 6/64 loss: -0.042993247509002686
Batch 7/64 loss: -0.04255872964859009
Batch 8/64 loss: -0.05367720127105713
Batch 9/64 loss: -0.0232735276222229
Batch 10/64 loss: -0.024828791618347168
Batch 11/64 loss: -0.04417937994003296
Batch 12/64 loss: -0.025526821613311768
Batch 13/64 loss: -0.021507322788238525
Batch 14/64 loss: -0.038822710514068604
Batch 15/64 loss: -0.03981602191925049
Batch 16/64 loss: -0.030887603759765625
Batch 17/64 loss: -0.04129284620285034
Batch 18/64 loss: -0.041651129722595215
Batch 19/64 loss: -0.024198412895202637
Batch 20/64 loss: -0.041963934898376465
Batch 21/64 loss: -0.043473899364471436
Batch 22/64 loss: -0.04886007308959961
Batch 23/64 loss: -0.01397174596786499
Batch 24/64 loss: -0.03293734788894653
Batch 25/64 loss: -0.04725152254104614
Batch 26/64 loss: -0.044160544872283936
Batch 27/64 loss: -0.03941071033477783
Batch 28/64 loss: -0.03712970018386841
Batch 29/64 loss: -0.04552769660949707
Batch 30/64 loss: -0.03493630886077881
Batch 31/64 loss: -0.025100350379943848
Batch 32/64 loss: -0.03279834985733032
Batch 33/64 loss: -0.03370636701583862
Batch 34/64 loss: -0.024119973182678223
Batch 35/64 loss: -0.028752446174621582
Batch 36/64 loss: -0.029656410217285156
Batch 37/64 loss: -0.03126239776611328
Batch 38/64 loss: -0.03284919261932373
Batch 39/64 loss: -0.051843106746673584
Batch 40/64 loss: -0.03530728816986084
Batch 41/64 loss: -0.03576481342315674
Batch 42/64 loss: -0.05193972587585449
Batch 43/64 loss: -0.02964705228805542
Batch 44/64 loss: -0.04255390167236328
Batch 45/64 loss: -0.0366780161857605
Batch 46/64 loss: -0.018708467483520508
Batch 47/64 loss: -0.0445103645324707
Batch 48/64 loss: -0.022384464740753174
Batch 49/64 loss: -0.03987550735473633
Batch 50/64 loss: -0.034749627113342285
Batch 51/64 loss: -0.023235201835632324
Batch 52/64 loss: -0.029814481735229492
Batch 53/64 loss: -0.029308557510375977
Batch 54/64 loss: -0.03748118877410889
Batch 55/64 loss: -0.04302626848220825
Batch 56/64 loss: -0.04287993907928467
Batch 57/64 loss: -0.02802520990371704
Batch 58/64 loss: -0.043002426624298096
Batch 59/64 loss: -0.04988831281661987
Batch 60/64 loss: -0.03473156690597534
Batch 61/64 loss: -0.0234220027923584
Batch 62/64 loss: -0.04932808876037598
Batch 63/64 loss: -0.03131824731826782
Batch 64/64 loss: -0.03831225633621216
Epoch 409  Train loss: -0.03594581543230543  Val loss: 0.09595182957927796
Epoch 410
-------------------------------
Batch 1/64 loss: -0.03419315814971924
Batch 2/64 loss: -0.04410058259963989
Batch 3/64 loss: -0.03253507614135742
Batch 4/64 loss: -0.02946341037750244
Batch 5/64 loss: -0.039950549602508545
Batch 6/64 loss: -0.018626093864440918
Batch 7/64 loss: -0.03924095630645752
Batch 8/64 loss: -0.03070932626724243
Batch 9/64 loss: -0.03770577907562256
Batch 10/64 loss: -0.043080270290374756
Batch 11/64 loss: -0.037135958671569824
Batch 12/64 loss: -0.04694831371307373
Batch 13/64 loss: -0.06168770790100098
Batch 14/64 loss: -0.023689985275268555
Batch 15/64 loss: -0.03834962844848633
Batch 16/64 loss: -0.03795582056045532
Batch 17/64 loss: -0.017023146152496338
Batch 18/64 loss: -0.03781694173812866
Batch 19/64 loss: -0.02555018663406372
Batch 20/64 loss: -0.028597354888916016
Batch 21/64 loss: -0.0383148193359375
Batch 22/64 loss: -0.042673349380493164
Batch 23/64 loss: -0.034722864627838135
Batch 24/64 loss: -0.03170597553253174
Batch 25/64 loss: -0.02026432752609253
Batch 26/64 loss: -0.017437219619750977
Batch 27/64 loss: -0.018519580364227295
Batch 28/64 loss: -0.026195108890533447
Batch 29/64 loss: -0.02369934320449829
Batch 30/64 loss: -0.03863930702209473
Batch 31/64 loss: -0.045265018939971924
Batch 32/64 loss: -0.004646599292755127
Batch 33/64 loss: -0.04413115978240967
Batch 34/64 loss: -0.02798682451248169
Batch 35/64 loss: -0.04771268367767334
Batch 36/64 loss: -0.04774057865142822
Batch 37/64 loss: -0.038179755210876465
Batch 38/64 loss: -0.031053662300109863
Batch 39/64 loss: -0.03736644983291626
Batch 40/64 loss: -0.03314852714538574
Batch 41/64 loss: -0.04633021354675293
Batch 42/64 loss: -0.045825064182281494
Batch 43/64 loss: -0.05011075735092163
Batch 44/64 loss: -0.049570322036743164
Batch 45/64 loss: -0.029301822185516357
Batch 46/64 loss: -0.0356174111366272
Batch 47/64 loss: -0.04331851005554199
Batch 48/64 loss: -0.027223706245422363
Batch 49/64 loss: -0.03689396381378174
Batch 50/64 loss: -0.03148055076599121
Batch 51/64 loss: -0.042322635650634766
Batch 52/64 loss: -0.03404116630554199
Batch 53/64 loss: -0.03243589401245117
Batch 54/64 loss: -0.04740053415298462
Batch 55/64 loss: -0.04083383083343506
Batch 56/64 loss: -0.029310941696166992
Batch 57/64 loss: -0.02237790822982788
Batch 58/64 loss: -0.02314150333404541
Batch 59/64 loss: -0.0287477970123291
Batch 60/64 loss: -0.03106778860092163
Batch 61/64 loss: -0.04395526647567749
Batch 62/64 loss: -0.01903390884399414
Batch 63/64 loss: -0.0453527569770813
Batch 64/64 loss: -0.03991514444351196
Epoch 410  Train loss: -0.034814023737813914  Val loss: 0.09595207434749276
Epoch 411
-------------------------------
Batch 1/64 loss: -0.043016254901885986
Batch 2/64 loss: -0.05023318529129028
Batch 3/64 loss: -0.04496645927429199
Batch 4/64 loss: -0.040107786655426025
Batch 5/64 loss: -0.03625345230102539
Batch 6/64 loss: -0.04507613182067871
Batch 7/64 loss: -0.0542757511138916
Batch 8/64 loss: -0.044373273849487305
Batch 9/64 loss: -0.051020026206970215
Batch 10/64 loss: -0.03371196985244751
Batch 11/64 loss: -0.05021548271179199
Batch 12/64 loss: -0.06398016214370728
Batch 13/64 loss: -0.03363823890686035
Batch 14/64 loss: -0.055158019065856934
Batch 15/64 loss: -0.006244182586669922
Batch 16/64 loss: -0.03941607475280762
Batch 17/64 loss: -0.049212098121643066
Batch 18/64 loss: -0.024496495723724365
Batch 19/64 loss: -0.03896188735961914
Batch 20/64 loss: -0.04353147745132446
Batch 21/64 loss: -0.04111981391906738
Batch 22/64 loss: -0.0310211181640625
Batch 23/64 loss: -0.044237494468688965
Batch 24/64 loss: -0.044547080993652344
Batch 25/64 loss: -0.02956181764602661
Batch 26/64 loss: -0.03017258644104004
Batch 27/64 loss: -0.033605098724365234
Batch 28/64 loss: -0.02756035327911377
Batch 29/64 loss: -0.02731800079345703
Batch 30/64 loss: -0.022460341453552246
Batch 31/64 loss: -0.029062271118164062
Batch 32/64 loss: -0.020497679710388184
Batch 33/64 loss: -0.028710663318634033
Batch 34/64 loss: -0.021296560764312744
Batch 35/64 loss: -0.02681553363800049
Batch 36/64 loss: -0.018084049224853516
Batch 37/64 loss: -0.026671171188354492
Batch 38/64 loss: -0.03550982475280762
Batch 39/64 loss: -0.03187358379364014
Batch 40/64 loss: -0.03604775667190552
Batch 41/64 loss: -0.046140313148498535
Batch 42/64 loss: -0.01722884178161621
Batch 43/64 loss: -0.03919553756713867
Batch 44/64 loss: -0.03265190124511719
Batch 45/64 loss: -0.037760138511657715
Batch 46/64 loss: -0.033052921295166016
Batch 47/64 loss: -0.04104018211364746
Batch 48/64 loss: -0.014432191848754883
Batch 49/64 loss: -0.04070562124252319
Batch 50/64 loss: -0.04135406017303467
Batch 51/64 loss: -0.04350966215133667
Batch 52/64 loss: -0.04035294055938721
Batch 53/64 loss: -0.03167402744293213
Batch 54/64 loss: -0.019875764846801758
Batch 55/64 loss: -0.04367929697036743
Batch 56/64 loss: -0.04648745059967041
Batch 57/64 loss: -0.03505474328994751
Batch 58/64 loss: -0.020632624626159668
Batch 59/64 loss: -0.04430735111236572
Batch 60/64 loss: -0.03997337818145752
Batch 61/64 loss: -0.03456282615661621
Batch 62/64 loss: -0.039871811866760254
Batch 63/64 loss: -0.010788202285766602
Batch 64/64 loss: -0.00647658109664917
Epoch 411  Train loss: -0.03534510486266192  Val loss: 0.09742520519138612
Epoch 412
-------------------------------
Batch 1/64 loss: -0.046329498291015625
Batch 2/64 loss: -0.028271734714508057
Batch 3/64 loss: -0.038613975048065186
Batch 4/64 loss: -0.042122602462768555
Batch 5/64 loss: -0.031161844730377197
Batch 6/64 loss: -0.022422730922698975
Batch 7/64 loss: -0.022459983825683594
Batch 8/64 loss: -0.025093436241149902
Batch 9/64 loss: -0.028398096561431885
Batch 10/64 loss: -0.021057307720184326
Batch 11/64 loss: -0.03712594509124756
Batch 12/64 loss: -0.05250048637390137
Batch 13/64 loss: -0.029196500778198242
Batch 14/64 loss: -0.03822171688079834
Batch 15/64 loss: -0.014926731586456299
Batch 16/64 loss: -0.037457942962646484
Batch 17/64 loss: -0.04941558837890625
Batch 18/64 loss: -0.02317368984222412
Batch 19/64 loss: -0.02567315101623535
Batch 20/64 loss: -0.03016364574432373
Batch 21/64 loss: -0.041387081146240234
Batch 22/64 loss: -0.05552518367767334
Batch 23/64 loss: -0.03860348463058472
Batch 24/64 loss: -0.041903018951416016
Batch 25/64 loss: -0.04995095729827881
Batch 26/64 loss: -0.05530291795730591
Batch 27/64 loss: -0.028361976146697998
Batch 28/64 loss: -0.038090646266937256
Batch 29/64 loss: -0.04226750135421753
Batch 30/64 loss: -0.04370236396789551
Batch 31/64 loss: -0.04388612508773804
Batch 32/64 loss: -0.030409395694732666
Batch 33/64 loss: -0.045623958110809326
Batch 34/64 loss: -0.034923672676086426
Batch 35/64 loss: -0.04645413160324097
Batch 36/64 loss: -0.03683924674987793
Batch 37/64 loss: -0.033212900161743164
Batch 38/64 loss: -0.048150479793548584
Batch 39/64 loss: -0.04897385835647583
Batch 40/64 loss: -0.019436776638031006
Batch 41/64 loss: -0.029932856559753418
Batch 42/64 loss: -0.026379108428955078
Batch 43/64 loss: -0.03122657537460327
Batch 44/64 loss: -0.033749163150787354
Batch 45/64 loss: -0.029227614402770996
Batch 46/64 loss: -0.04087775945663452
Batch 47/64 loss: -0.028242290019989014
Batch 48/64 loss: -0.027179241180419922
Batch 49/64 loss: -0.04205727577209473
Batch 50/64 loss: -0.02016448974609375
Batch 51/64 loss: -0.046022891998291016
Batch 52/64 loss: -0.036787331104278564
Batch 53/64 loss: -0.03964507579803467
Batch 54/64 loss: -0.03920644521713257
Batch 55/64 loss: -0.048356592655181885
Batch 56/64 loss: -0.024690866470336914
Batch 57/64 loss: -0.0009264349937438965
Batch 58/64 loss: -0.04008960723876953
Batch 59/64 loss: -0.009012341499328613
Batch 60/64 loss: -0.02938520908355713
Batch 61/64 loss: -0.01489347219467163
Batch 62/64 loss: -0.0287400484085083
Batch 63/64 loss: -0.006067812442779541
Batch 64/64 loss: -0.022747337818145752
Epoch 412  Train loss: -0.03383079673729691  Val loss: 0.09664715053289617
Epoch 413
-------------------------------
Batch 1/64 loss: -0.0400087833404541
Batch 2/64 loss: -0.03851151466369629
Batch 3/64 loss: -0.015424609184265137
Batch 4/64 loss: -0.029908418655395508
Batch 5/64 loss: -0.031858980655670166
Batch 6/64 loss: -0.048070430755615234
Batch 7/64 loss: -0.0444568395614624
Batch 8/64 loss: -0.039989352226257324
Batch 9/64 loss: -0.03405636548995972
Batch 10/64 loss: -0.04635840654373169
Batch 11/64 loss: -0.05036193132400513
Batch 12/64 loss: -0.048564791679382324
Batch 13/64 loss: -0.04446256160736084
Batch 14/64 loss: -0.04387247562408447
Batch 15/64 loss: -0.030842185020446777
Batch 16/64 loss: -0.043420374393463135
Batch 17/64 loss: -0.04357177019119263
Batch 18/64 loss: -0.03872889280319214
Batch 19/64 loss: -0.042760491371154785
Batch 20/64 loss: -0.04105520248413086
Batch 21/64 loss: -0.03516346216201782
Batch 22/64 loss: -0.04693436622619629
Batch 23/64 loss: -0.043905675411224365
Batch 24/64 loss: -0.02387160062789917
Batch 25/64 loss: -0.03938204050064087
Batch 26/64 loss: -0.03334939479827881
Batch 27/64 loss: -0.0343356728553772
Batch 28/64 loss: -0.012409806251525879
Batch 29/64 loss: -0.04392421245574951
Batch 30/64 loss: -0.04328775405883789
Batch 31/64 loss: -0.04897260665893555
Batch 32/64 loss: -0.04392802715301514
Batch 33/64 loss: -0.028194189071655273
Batch 34/64 loss: -0.037676870822906494
Batch 35/64 loss: -0.03870666027069092
Batch 36/64 loss: -0.012546062469482422
Batch 37/64 loss: -0.01569068431854248
Batch 38/64 loss: -0.047165870666503906
Batch 39/64 loss: -0.029389500617980957
Batch 40/64 loss: -0.03875291347503662
Batch 41/64 loss: -0.04712092876434326
Batch 42/64 loss: -0.02225404977798462
Batch 43/64 loss: -0.052651286125183105
Batch 44/64 loss: -0.047343909740448
Batch 45/64 loss: -0.05111473798751831
Batch 46/64 loss: -0.05234730243682861
Batch 47/64 loss: -0.033126115798950195
Batch 48/64 loss: -0.02787262201309204
Batch 49/64 loss: -0.03780090808868408
Batch 50/64 loss: -0.019940078258514404
Batch 51/64 loss: -0.03680109977722168
Batch 52/64 loss: -0.03416299819946289
Batch 53/64 loss: -0.03391498327255249
Batch 54/64 loss: -0.02480292320251465
Batch 55/64 loss: -0.01989084482192993
Batch 56/64 loss: -0.03421223163604736
Batch 57/64 loss: -0.057439446449279785
Batch 58/64 loss: -0.03802913427352905
Batch 59/64 loss: -0.04054856300354004
Batch 60/64 loss: -0.046141982078552246
Batch 61/64 loss: -0.04194551706314087
Batch 62/64 loss: -0.03574270009994507
Batch 63/64 loss: -0.04083371162414551
Batch 64/64 loss: -0.04116833209991455
Epoch 413  Train loss: -0.03765938936495313  Val loss: 0.0952170095902538
Epoch 414
-------------------------------
Batch 1/64 loss: -0.05985558032989502
Batch 2/64 loss: -0.037225961685180664
Batch 3/64 loss: -0.035151004791259766
Batch 4/64 loss: -0.016516387462615967
Batch 5/64 loss: -0.03690415620803833
Batch 6/64 loss: -0.062353432178497314
Batch 7/64 loss: -0.04948544502258301
Batch 8/64 loss: -0.03172487020492554
Batch 9/64 loss: -0.04439646005630493
Batch 10/64 loss: -0.042563676834106445
Batch 11/64 loss: -0.039945781230926514
Batch 12/64 loss: -0.045311152935028076
Batch 13/64 loss: -0.029464781284332275
Batch 14/64 loss: -0.03096163272857666
Batch 15/64 loss: -0.032697081565856934
Batch 16/64 loss: -0.04957538843154907
Batch 17/64 loss: -0.05716139078140259
Batch 18/64 loss: -0.04057776927947998
Batch 19/64 loss: -0.049669623374938965
Batch 20/64 loss: -0.04230809211730957
Batch 21/64 loss: -0.03182166814804077
Batch 22/64 loss: -0.04282224178314209
Batch 23/64 loss: -0.055306434631347656
Batch 24/64 loss: -0.03839331865310669
Batch 25/64 loss: -0.033301472663879395
Batch 26/64 loss: -0.03709244728088379
Batch 27/64 loss: -0.030846357345581055
Batch 28/64 loss: -0.039031803607940674
Batch 29/64 loss: -0.026916861534118652
Batch 30/64 loss: -0.04187047481536865
Batch 31/64 loss: -0.04167681932449341
Batch 32/64 loss: -0.03171885013580322
Batch 33/64 loss: -0.014835774898529053
Batch 34/64 loss: -0.02152848243713379
Batch 35/64 loss: -0.04118514060974121
Batch 36/64 loss: -0.024443328380584717
Batch 37/64 loss: -0.05195438861846924
Batch 38/64 loss: -0.038498640060424805
Batch 39/64 loss: -0.04149883985519409
Batch 40/64 loss: -0.05122727155685425
Batch 41/64 loss: -0.025427579879760742
Batch 42/64 loss: -0.05532276630401611
Batch 43/64 loss: -0.04138839244842529
Batch 44/64 loss: -0.04627460241317749
Batch 45/64 loss: -0.040442585945129395
Batch 46/64 loss: -0.04231464862823486
Batch 47/64 loss: -0.04404062032699585
Batch 48/64 loss: -0.03864198923110962
Batch 49/64 loss: -0.0419234037399292
Batch 50/64 loss: -0.049321115016937256
Batch 51/64 loss: -0.039873361587524414
Batch 52/64 loss: -0.04172033071517944
Batch 53/64 loss: -0.02267742156982422
Batch 54/64 loss: -0.04199415445327759
Batch 55/64 loss: -0.02851712703704834
Batch 56/64 loss: -0.04961937665939331
Batch 57/64 loss: -0.031223416328430176
Batch 58/64 loss: -0.03039497137069702
Batch 59/64 loss: -0.027579009532928467
Batch 60/64 loss: -0.02336782217025757
Batch 61/64 loss: -0.018426060676574707
Batch 62/64 loss: -0.013968288898468018
Batch 63/64 loss: -0.02788829803466797
Batch 64/64 loss: -0.0528140664100647
Epoch 414  Train loss: -0.038145540041082046  Val loss: 0.09616180793526247
Epoch 415
-------------------------------
Batch 1/64 loss: -0.03520822525024414
Batch 2/64 loss: -0.05236238241195679
Batch 3/64 loss: -0.05639892816543579
Batch 4/64 loss: -0.056149423122406006
Batch 5/64 loss: -0.05265510082244873
Batch 6/64 loss: -0.038497209548950195
Batch 7/64 loss: -0.044086992740631104
Batch 8/64 loss: -0.04279637336730957
Batch 9/64 loss: -0.03591585159301758
Batch 10/64 loss: -0.04638189077377319
Batch 11/64 loss: -0.043736040592193604
Batch 12/64 loss: -0.04342377185821533
Batch 13/64 loss: -0.05195373296737671
Batch 14/64 loss: -0.03337085247039795
Batch 15/64 loss: -0.036022305488586426
Batch 16/64 loss: -0.05967116355895996
Batch 17/64 loss: -0.03475987911224365
Batch 18/64 loss: -0.06164127588272095
Batch 19/64 loss: -0.023768842220306396
Batch 20/64 loss: -0.04335516691207886
Batch 21/64 loss: -0.050621092319488525
Batch 22/64 loss: -0.04237639904022217
Batch 23/64 loss: -0.040688276290893555
Batch 24/64 loss: -0.02359175682067871
Batch 25/64 loss: -0.04146355390548706
Batch 26/64 loss: -0.032269835472106934
Batch 27/64 loss: -0.04906105995178223
Batch 28/64 loss: -0.04132300615310669
Batch 29/64 loss: -0.0559234619140625
Batch 30/64 loss: -0.053207993507385254
Batch 31/64 loss: -0.040858447551727295
Batch 32/64 loss: -0.05183589458465576
Batch 33/64 loss: -0.03137505054473877
Batch 34/64 loss: -0.03687548637390137
Batch 35/64 loss: -0.05404102802276611
Batch 36/64 loss: -0.027529656887054443
Batch 37/64 loss: -0.03786063194274902
Batch 38/64 loss: -0.04476523399353027
Batch 39/64 loss: -0.027247846126556396
Batch 40/64 loss: -0.03716123104095459
Batch 41/64 loss: -0.04031574726104736
Batch 42/64 loss: -0.03179210424423218
Batch 43/64 loss: -0.046312689781188965
Batch 44/64 loss: -0.029132843017578125
Batch 45/64 loss: -0.04472911357879639
Batch 46/64 loss: -0.037368178367614746
Batch 47/64 loss: -0.045189857482910156
Batch 48/64 loss: -0.04173225164413452
Batch 49/64 loss: -0.031446635723114014
Batch 50/64 loss: -0.05036747455596924
Batch 51/64 loss: -0.029486119747161865
Batch 52/64 loss: -0.04668271541595459
Batch 53/64 loss: -0.026400744915008545
Batch 54/64 loss: -0.029911518096923828
Batch 55/64 loss: -0.039971232414245605
Batch 56/64 loss: -0.03795492649078369
Batch 57/64 loss: -0.05916917324066162
Batch 58/64 loss: -0.03166431188583374
Batch 59/64 loss: -0.04641306400299072
Batch 60/64 loss: -0.02439296245574951
Batch 61/64 loss: 0.002940654754638672
Batch 62/64 loss: -0.03754568099975586
Batch 63/64 loss: -0.02775418758392334
Batch 64/64 loss: -0.036952197551727295
Epoch 415  Train loss: -0.04035669611949547  Val loss: 0.09459679642903436
Epoch 416
-------------------------------
Batch 1/64 loss: -0.03188866376876831
Batch 2/64 loss: -0.03867155313491821
Batch 3/64 loss: -0.03911381959915161
Batch 4/64 loss: -0.0507817268371582
Batch 5/64 loss: -0.054754674434661865
Batch 6/64 loss: -0.03911864757537842
Batch 7/64 loss: -0.048348188400268555
Batch 8/64 loss: -0.04036509990692139
Batch 9/64 loss: -0.03929013013839722
Batch 10/64 loss: -0.03397834300994873
Batch 11/64 loss: -0.05299580097198486
Batch 12/64 loss: -0.05220842361450195
Batch 13/64 loss: -0.0492822527885437
Batch 14/64 loss: -0.028906404972076416
Batch 15/64 loss: -0.049754440784454346
Batch 16/64 loss: -0.026875078678131104
Batch 17/64 loss: -0.022315382957458496
Batch 18/64 loss: -0.04329085350036621
Batch 19/64 loss: -0.027564048767089844
Batch 20/64 loss: -0.014687299728393555
Batch 21/64 loss: -0.037091732025146484
Batch 22/64 loss: -0.040084242820739746
Batch 23/64 loss: -0.0396767258644104
Batch 24/64 loss: -0.04840660095214844
Batch 25/64 loss: -0.03591728210449219
Batch 26/64 loss: -0.0426945686340332
Batch 27/64 loss: -0.0429387092590332
Batch 28/64 loss: -0.034458816051483154
Batch 29/64 loss: -0.025275766849517822
Batch 30/64 loss: -0.04088407754898071
Batch 31/64 loss: -0.042941153049468994
Batch 32/64 loss: -0.03882557153701782
Batch 33/64 loss: -0.03062140941619873
Batch 34/64 loss: -0.03306502103805542
Batch 35/64 loss: -0.03436833620071411
Batch 36/64 loss: -0.03938770294189453
Batch 37/64 loss: -0.012224793434143066
Batch 38/64 loss: -0.024680793285369873
Batch 39/64 loss: -0.04014396667480469
Batch 40/64 loss: -0.030144810676574707
Batch 41/64 loss: -0.045671284198760986
Batch 42/64 loss: -0.03473782539367676
Batch 43/64 loss: -0.044114530086517334
Batch 44/64 loss: -0.03964787721633911
Batch 45/64 loss: -0.02091127634048462
Batch 46/64 loss: -0.042887985706329346
Batch 47/64 loss: -0.024767041206359863
Batch 48/64 loss: -0.03948777914047241
Batch 49/64 loss: -0.03229570388793945
Batch 50/64 loss: -0.029015183448791504
Batch 51/64 loss: -0.03924453258514404
Batch 52/64 loss: -0.030158936977386475
Batch 53/64 loss: -0.03864431381225586
Batch 54/64 loss: -0.01975691318511963
Batch 55/64 loss: -0.03692680597305298
Batch 56/64 loss: -0.04084491729736328
Batch 57/64 loss: -0.03851085901260376
Batch 58/64 loss: -0.032362937927246094
Batch 59/64 loss: -0.032090187072753906
Batch 60/64 loss: -0.04089921712875366
Batch 61/64 loss: -0.03951150178909302
Batch 62/64 loss: -0.030238449573516846
Batch 63/64 loss: -0.0449143648147583
Batch 64/64 loss: -0.041640639305114746
Epoch 416  Train loss: -0.03681402066174676  Val loss: 0.0952334950879677
Epoch 417
-------------------------------
Batch 1/64 loss: -0.048455893993377686
Batch 2/64 loss: -0.038769304752349854
Batch 3/64 loss: -0.031779468059539795
Batch 4/64 loss: -0.03204178810119629
Batch 5/64 loss: -0.047230541706085205
Batch 6/64 loss: -0.042951107025146484
Batch 7/64 loss: -0.04633444547653198
Batch 8/64 loss: -0.04020488262176514
Batch 9/64 loss: -0.025713562965393066
Batch 10/64 loss: -0.05336105823516846
Batch 11/64 loss: -0.045175909996032715
Batch 12/64 loss: -0.04167759418487549
Batch 13/64 loss: -0.023755013942718506
Batch 14/64 loss: -0.0413704514503479
Batch 15/64 loss: -0.05318105220794678
Batch 16/64 loss: -0.03068089485168457
Batch 17/64 loss: -0.045738816261291504
Batch 18/64 loss: -0.05659282207489014
Batch 19/64 loss: -0.03688526153564453
Batch 20/64 loss: -0.05729144811630249
Batch 21/64 loss: -0.022243082523345947
Batch 22/64 loss: -0.06007581949234009
Batch 23/64 loss: -0.04771304130554199
Batch 24/64 loss: -0.04356181621551514
Batch 25/64 loss: -0.043110787868499756
Batch 26/64 loss: -0.04055368900299072
Batch 27/64 loss: -0.04586637020111084
Batch 28/64 loss: -0.038530707359313965
Batch 29/64 loss: -0.052542030811309814
Batch 30/64 loss: -0.03821772336959839
Batch 31/64 loss: -0.045378804206848145
Batch 32/64 loss: -0.058357179164886475
Batch 33/64 loss: -0.04367470741271973
Batch 34/64 loss: -0.03821885585784912
Batch 35/64 loss: -0.042949140071868896
Batch 36/64 loss: -0.042999863624572754
Batch 37/64 loss: -0.05004161596298218
Batch 38/64 loss: -0.03781086206436157
Batch 39/64 loss: -0.030412375926971436
Batch 40/64 loss: -0.03805398941040039
Batch 41/64 loss: -0.0277712345123291
Batch 42/64 loss: -0.042001962661743164
Batch 43/64 loss: -0.029662013053894043
Batch 44/64 loss: -0.04728543758392334
Batch 45/64 loss: -0.036048829555511475
Batch 46/64 loss: -0.05074441432952881
Batch 47/64 loss: -0.0388256311416626
Batch 48/64 loss: -0.040023088455200195
Batch 49/64 loss: -0.03727996349334717
Batch 50/64 loss: -0.03627508878707886
Batch 51/64 loss: -0.013230621814727783
Batch 52/64 loss: -0.03358328342437744
Batch 53/64 loss: -0.03442347049713135
Batch 54/64 loss: -0.04387855529785156
Batch 55/64 loss: -0.049127936363220215
Batch 56/64 loss: -0.0380476713180542
Batch 57/64 loss: -0.038268089294433594
Batch 58/64 loss: -0.03433018922805786
Batch 59/64 loss: -0.04642844200134277
Batch 60/64 loss: -0.053053200244903564
Batch 61/64 loss: -0.039006829261779785
Batch 62/64 loss: -0.02928626537322998
Batch 63/64 loss: -0.03959155082702637
Batch 64/64 loss: -0.02914106845855713
Epoch 417  Train loss: -0.04077699371412689  Val loss: 0.09680111199310146
Epoch 418
-------------------------------
Batch 1/64 loss: -0.04510015249252319
Batch 2/64 loss: -0.06386643648147583
Batch 3/64 loss: -0.034300386905670166
Batch 4/64 loss: -0.019199073314666748
Batch 5/64 loss: -0.041525840759277344
Batch 6/64 loss: -0.04757082462310791
Batch 7/64 loss: -0.035418808460235596
Batch 8/64 loss: -0.041596412658691406
Batch 9/64 loss: -0.0455780029296875
Batch 10/64 loss: -0.01711130142211914
Batch 11/64 loss: -0.04309225082397461
Batch 12/64 loss: -0.0457228422164917
Batch 13/64 loss: -0.04245460033416748
Batch 14/64 loss: -0.04264640808105469
Batch 15/64 loss: -0.04900592565536499
Batch 16/64 loss: -0.04555630683898926
Batch 17/64 loss: -0.028777241706848145
Batch 18/64 loss: -0.04033619165420532
Batch 19/64 loss: -0.06084674596786499
Batch 20/64 loss: -0.0473862886428833
Batch 21/64 loss: -0.04646563529968262
Batch 22/64 loss: -0.042471468448638916
Batch 23/64 loss: -0.024018287658691406
Batch 24/64 loss: -0.04360908269882202
Batch 25/64 loss: -0.04711848497390747
Batch 26/64 loss: -0.047020137310028076
Batch 27/64 loss: -0.027981460094451904
Batch 28/64 loss: -0.02495729923248291
Batch 29/64 loss: -0.0439186692237854
Batch 30/64 loss: -0.0439755916595459
Batch 31/64 loss: -0.045293569564819336
Batch 32/64 loss: -0.03722965717315674
Batch 33/64 loss: -0.036205828189849854
Batch 34/64 loss: -0.022547125816345215
Batch 35/64 loss: -0.04412466287612915
Batch 36/64 loss: -0.03932291269302368
Batch 37/64 loss: -0.04094290733337402
Batch 38/64 loss: -0.028211116790771484
Batch 39/64 loss: -0.053159117698669434
Batch 40/64 loss: -0.02434670925140381
Batch 41/64 loss: -0.03841841220855713
Batch 42/64 loss: -0.011634767055511475
Batch 43/64 loss: -0.045665621757507324
Batch 44/64 loss: -0.03916656970977783
Batch 45/64 loss: -0.04022413492202759
Batch 46/64 loss: -0.03876638412475586
Batch 47/64 loss: -0.04114329814910889
Batch 48/64 loss: -0.041882216930389404
Batch 49/64 loss: -0.03680378198623657
Batch 50/64 loss: -0.01922684907913208
Batch 51/64 loss: -0.020242273807525635
Batch 52/64 loss: -0.04873615503311157
Batch 53/64 loss: -0.035239577293395996
Batch 54/64 loss: -0.048419833183288574
Batch 55/64 loss: -0.05374789237976074
Batch 56/64 loss: -0.050418317317962646
Batch 57/64 loss: -0.04228925704956055
Batch 58/64 loss: -0.03375208377838135
Batch 59/64 loss: -0.05037111043930054
Batch 60/64 loss: -0.030672669410705566
Batch 61/64 loss: -0.03491884469985962
Batch 62/64 loss: -0.036187171936035156
Batch 63/64 loss: -0.03090500831604004
Batch 64/64 loss: -0.0006540417671203613
Epoch 418  Train loss: -0.03873465926039452  Val loss: 0.09777075912534576
Epoch 419
-------------------------------
Batch 1/64 loss: -0.034601449966430664
Batch 2/64 loss: -0.05094510316848755
Batch 3/64 loss: -0.03754091262817383
Batch 4/64 loss: -0.03764498233795166
Batch 5/64 loss: -0.04928511381149292
Batch 6/64 loss: -0.02393287420272827
Batch 7/64 loss: -0.03804880380630493
Batch 8/64 loss: -0.018480420112609863
Batch 9/64 loss: -0.04385906457901001
Batch 10/64 loss: -0.04229736328125
Batch 11/64 loss: -0.04226100444793701
Batch 12/64 loss: -0.027949094772338867
Batch 13/64 loss: -0.04911792278289795
Batch 14/64 loss: -0.05349093675613403
Batch 15/64 loss: -0.02044898271560669
Batch 16/64 loss: -0.04143780469894409
Batch 17/64 loss: -0.043255507946014404
Batch 18/64 loss: -0.0304945707321167
Batch 19/64 loss: -0.02258509397506714
Batch 20/64 loss: -0.04066145420074463
Batch 21/64 loss: -0.01969611644744873
Batch 22/64 loss: -0.023311257362365723
Batch 23/64 loss: -0.03240644931793213
Batch 24/64 loss: -0.029846608638763428
Batch 25/64 loss: -0.04067474603652954
Batch 26/64 loss: -0.022432148456573486
Batch 27/64 loss: -0.011927664279937744
Batch 28/64 loss: -0.04467284679412842
Batch 29/64 loss: -0.05081456899642944
Batch 30/64 loss: -0.03460502624511719
Batch 31/64 loss: -0.04351741075515747
Batch 32/64 loss: -0.03545433282852173
Batch 33/64 loss: -0.026468753814697266
Batch 34/64 loss: -0.036559104919433594
Batch 35/64 loss: -0.05123943090438843
Batch 36/64 loss: -0.02399986982345581
Batch 37/64 loss: -0.05109935998916626
Batch 38/64 loss: -0.02928096055984497
Batch 39/64 loss: -0.03286713361740112
Batch 40/64 loss: -0.03651309013366699
Batch 41/64 loss: -0.02290177345275879
Batch 42/64 loss: 0.01797306537628174
Batch 43/64 loss: -0.03512471914291382
Batch 44/64 loss: -0.01717609167098999
Batch 45/64 loss: -0.014683067798614502
Batch 46/64 loss: -0.018884778022766113
Batch 47/64 loss: -0.0451621413230896
Batch 48/64 loss: -0.03552889823913574
Batch 49/64 loss: -0.020755648612976074
Batch 50/64 loss: -0.02096956968307495
Batch 51/64 loss: -0.026161611080169678
Batch 52/64 loss: -0.045103490352630615
Batch 53/64 loss: -0.03767049312591553
Batch 54/64 loss: -0.02984774112701416
Batch 55/64 loss: -0.029689133167266846
Batch 56/64 loss: -0.03714829683303833
Batch 57/64 loss: -0.03747665882110596
Batch 58/64 loss: -0.04778498411178589
Batch 59/64 loss: -0.03704637289047241
Batch 60/64 loss: -0.03131139278411865
Batch 61/64 loss: -0.031085968017578125
Batch 62/64 loss: -0.051270484924316406
Batch 63/64 loss: -0.048019587993621826
Batch 64/64 loss: -0.015543341636657715
Epoch 419  Train loss: -0.033571963684231626  Val loss: 0.09536248691303213
Epoch 420
-------------------------------
Batch 1/64 loss: -0.042482197284698486
Batch 2/64 loss: -0.04283398389816284
Batch 3/64 loss: -0.0482332706451416
Batch 4/64 loss: -0.027089059352874756
Batch 5/64 loss: -0.039853572845458984
Batch 6/64 loss: -0.04615694284439087
Batch 7/64 loss: -0.04318112134933472
Batch 8/64 loss: -0.04779428243637085
Batch 9/64 loss: -0.024580180644989014
Batch 10/64 loss: -0.04082846641540527
Batch 11/64 loss: -0.036716341972351074
Batch 12/64 loss: -0.028046131134033203
Batch 13/64 loss: -0.05730921030044556
Batch 14/64 loss: -0.051306068897247314
Batch 15/64 loss: -0.04323238134384155
Batch 16/64 loss: -0.039306461811065674
Batch 17/64 loss: -0.03595459461212158
Batch 18/64 loss: -0.03409755229949951
Batch 19/64 loss: -0.01850283145904541
Batch 20/64 loss: -0.045815885066986084
Batch 21/64 loss: -0.048005640506744385
Batch 22/64 loss: -0.05103558301925659
Batch 23/64 loss: -0.013359248638153076
Batch 24/64 loss: -0.028646767139434814
Batch 25/64 loss: -0.029229044914245605
Batch 26/64 loss: -0.04790544509887695
Batch 27/64 loss: -0.045879244804382324
Batch 28/64 loss: -0.03513967990875244
Batch 29/64 loss: -0.035810112953186035
Batch 30/64 loss: -0.03607112169265747
Batch 31/64 loss: -0.04317939281463623
Batch 32/64 loss: -0.029354333877563477
Batch 33/64 loss: -0.027256131172180176
Batch 34/64 loss: -0.023224174976348877
Batch 35/64 loss: -0.0440254807472229
Batch 36/64 loss: -0.056936800479888916
Batch 37/64 loss: -0.03739321231842041
Batch 38/64 loss: -0.031198620796203613
Batch 39/64 loss: -0.05185443162918091
Batch 40/64 loss: -0.0329890251159668
Batch 41/64 loss: -0.04080462455749512
Batch 42/64 loss: -0.0225335955619812
Batch 43/64 loss: -0.019595861434936523
Batch 44/64 loss: -0.04674410820007324
Batch 45/64 loss: -0.04458588361740112
Batch 46/64 loss: -0.03542745113372803
Batch 47/64 loss: -0.024465739727020264
Batch 48/64 loss: -0.05446690320968628
Batch 49/64 loss: -0.04792344570159912
Batch 50/64 loss: -0.04126262664794922
Batch 51/64 loss: -0.03435099124908447
Batch 52/64 loss: -0.05152475833892822
Batch 53/64 loss: -0.03583788871765137
Batch 54/64 loss: -0.03824138641357422
Batch 55/64 loss: -0.03947025537490845
Batch 56/64 loss: -0.033571600914001465
Batch 57/64 loss: -0.05249232053756714
Batch 58/64 loss: -0.03203552961349487
Batch 59/64 loss: -0.03252530097961426
Batch 60/64 loss: -0.031084001064300537
Batch 61/64 loss: -0.04693102836608887
Batch 62/64 loss: -0.04838752746582031
Batch 63/64 loss: -0.04163169860839844
Batch 64/64 loss: -0.044534921646118164
Epoch 420  Train loss: -0.038762505849202475  Val loss: 0.09399029254094023
Epoch 421
-------------------------------
Batch 1/64 loss: -0.06612628698348999
Batch 2/64 loss: -0.03787374496459961
Batch 3/64 loss: -0.03165489435195923
Batch 4/64 loss: -0.033342599868774414
Batch 5/64 loss: -0.02689182758331299
Batch 6/64 loss: -0.06788349151611328
Batch 7/64 loss: -0.04998445510864258
Batch 8/64 loss: -0.03069394826889038
Batch 9/64 loss: -0.05242270231246948
Batch 10/64 loss: -0.035633742809295654
Batch 11/64 loss: -0.05031025409698486
Batch 12/64 loss: -0.03838646411895752
Batch 13/64 loss: -0.03641384840011597
Batch 14/64 loss: -0.04300469160079956
Batch 15/64 loss: -0.03994643688201904
Batch 16/64 loss: -0.04788869619369507
Batch 17/64 loss: -0.055612921714782715
Batch 18/64 loss: -0.03161501884460449
Batch 19/64 loss: -0.04546177387237549
Batch 20/64 loss: -0.042942702770233154
Batch 21/64 loss: -0.04599404335021973
Batch 22/64 loss: -0.041506409645080566
Batch 23/64 loss: -0.04476332664489746
Batch 24/64 loss: -0.035411298274993896
Batch 25/64 loss: -0.0531008243560791
Batch 26/64 loss: -0.03897339105606079
Batch 27/64 loss: -0.05991482734680176
Batch 28/64 loss: -0.05021899938583374
Batch 29/64 loss: -0.04540979862213135
Batch 30/64 loss: -0.031598448753356934
Batch 31/64 loss: -0.03958767652511597
Batch 32/64 loss: -0.03404796123504639
Batch 33/64 loss: -0.06551969051361084
Batch 34/64 loss: -0.04599499702453613
Batch 35/64 loss: -0.03173422813415527
Batch 36/64 loss: -0.04955339431762695
Batch 37/64 loss: -0.04312419891357422
Batch 38/64 loss: -0.03456723690032959
Batch 39/64 loss: -0.014490604400634766
Batch 40/64 loss: -0.029361963272094727
Batch 41/64 loss: -0.03629100322723389
Batch 42/64 loss: -0.044176340103149414
Batch 43/64 loss: -0.04586595296859741
Batch 44/64 loss: -0.014881610870361328
Batch 45/64 loss: -0.03425353765487671
Batch 46/64 loss: -0.046588778495788574
Batch 47/64 loss: -0.038020193576812744
Batch 48/64 loss: -0.042690277099609375
Batch 49/64 loss: -0.026260673999786377
Batch 50/64 loss: -0.04079228639602661
Batch 51/64 loss: -0.0318913459777832
Batch 52/64 loss: -0.03145456314086914
Batch 53/64 loss: -0.03630101680755615
Batch 54/64 loss: -0.043182969093322754
Batch 55/64 loss: -0.03904581069946289
Batch 56/64 loss: -0.04289335012435913
Batch 57/64 loss: -0.03923827409744263
Batch 58/64 loss: -0.03468608856201172
Batch 59/64 loss: -0.0231054425239563
Batch 60/64 loss: -0.04416543245315552
Batch 61/64 loss: -0.045642197132110596
Batch 62/64 loss: -0.03596317768096924
Batch 63/64 loss: -0.04684615135192871
Batch 64/64 loss: -0.025884807109832764
Epoch 421  Train loss: -0.04051159059300142  Val loss: 0.09561606022910155
Epoch 422
-------------------------------
Batch 1/64 loss: -0.04670917987823486
Batch 2/64 loss: -0.05138176679611206
Batch 3/64 loss: -0.04689764976501465
Batch 4/64 loss: -0.03377574682235718
Batch 5/64 loss: -0.04500985145568848
Batch 6/64 loss: -0.045935630798339844
Batch 7/64 loss: -0.0368342399597168
Batch 8/64 loss: -0.031491756439208984
Batch 9/64 loss: -0.04380613565444946
Batch 10/64 loss: -0.0466884970664978
Batch 11/64 loss: -0.05226635932922363
Batch 12/64 loss: -0.030753672122955322
Batch 13/64 loss: -0.04453152418136597
Batch 14/64 loss: -0.05279886722564697
Batch 15/64 loss: -0.02253854274749756
Batch 16/64 loss: -0.02486330270767212
Batch 17/64 loss: -0.023243188858032227
Batch 18/64 loss: -0.03543168306350708
Batch 19/64 loss: -0.053202927112579346
Batch 20/64 loss: -0.04959779977798462
Batch 21/64 loss: -0.038958966732025146
Batch 22/64 loss: -0.0524144172668457
Batch 23/64 loss: -0.02340012788772583
Batch 24/64 loss: -0.0594523549079895
Batch 25/64 loss: -0.026469111442565918
Batch 26/64 loss: -0.03295910358428955
Batch 27/64 loss: -0.04083883762359619
Batch 28/64 loss: -0.052202045917510986
Batch 29/64 loss: -0.042441487312316895
Batch 30/64 loss: -0.012690603733062744
Batch 31/64 loss: -0.04140424728393555
Batch 32/64 loss: -0.050752460956573486
Batch 33/64 loss: -0.04349428415298462
Batch 34/64 loss: -0.034278154373168945
Batch 35/64 loss: -0.0399283766746521
Batch 36/64 loss: -0.01017308235168457
Batch 37/64 loss: -0.032252728939056396
Batch 38/64 loss: -0.0361788272857666
Batch 39/64 loss: -0.0400846004486084
Batch 40/64 loss: -0.024993538856506348
Batch 41/64 loss: -0.03405320644378662
Batch 42/64 loss: -0.03738415241241455
Batch 43/64 loss: -0.03068488836288452
Batch 44/64 loss: -0.03445887565612793
Batch 45/64 loss: -0.05152595043182373
Batch 46/64 loss: -0.01130986213684082
Batch 47/64 loss: -0.018133699893951416
Batch 48/64 loss: -0.04536592960357666
Batch 49/64 loss: -0.04643893241882324
Batch 50/64 loss: -0.03165125846862793
Batch 51/64 loss: -0.03315615653991699
Batch 52/64 loss: -0.01976931095123291
Batch 53/64 loss: -0.02965676784515381
Batch 54/64 loss: -0.022102713584899902
Batch 55/64 loss: -0.02779322862625122
Batch 56/64 loss: -0.040439486503601074
Batch 57/64 loss: -0.043313562870025635
Batch 58/64 loss: -0.04951894283294678
Batch 59/64 loss: -0.04202866554260254
Batch 60/64 loss: -0.03803735971450806
Batch 61/64 loss: -0.06574445962905884
Batch 62/64 loss: -0.03179657459259033
Batch 63/64 loss: -0.054527223110198975
Batch 64/64 loss: -0.037801265716552734
Epoch 422  Train loss: -0.037966554305132696  Val loss: 0.09591256957693198
Epoch 423
-------------------------------
Batch 1/64 loss: -0.04486948251724243
Batch 2/64 loss: -0.06353181600570679
Batch 3/64 loss: -0.05836153030395508
Batch 4/64 loss: -0.03246128559112549
Batch 5/64 loss: -0.04921013116836548
Batch 6/64 loss: -0.06436294317245483
Batch 7/64 loss: -0.055187225341796875
Batch 8/64 loss: -0.054721295833587646
Batch 9/64 loss: -0.054331421852111816
Batch 10/64 loss: -0.038475751876831055
Batch 11/64 loss: -0.055258333683013916
Batch 12/64 loss: -0.02398902177810669
Batch 13/64 loss: -0.051790595054626465
Batch 14/64 loss: -0.03978008031845093
Batch 15/64 loss: -0.041446030139923096
Batch 16/64 loss: -0.02864360809326172
Batch 17/64 loss: -0.022395610809326172
Batch 18/64 loss: -0.04632139205932617
Batch 19/64 loss: -0.026562094688415527
Batch 20/64 loss: -0.05729401111602783
Batch 21/64 loss: -0.05309271812438965
Batch 22/64 loss: -0.02703779935836792
Batch 23/64 loss: -0.028492450714111328
Batch 24/64 loss: -0.03687477111816406
Batch 25/64 loss: -0.03692972660064697
Batch 26/64 loss: -0.05153179168701172
Batch 27/64 loss: -0.05266869068145752
Batch 28/64 loss: -0.039327144622802734
Batch 29/64 loss: -0.02482849359512329
Batch 30/64 loss: -0.059290528297424316
Batch 31/64 loss: -0.04519164562225342
Batch 32/64 loss: -0.04111301898956299
Batch 33/64 loss: -0.04871177673339844
Batch 34/64 loss: -0.039370834827423096
Batch 35/64 loss: -0.03993546962738037
Batch 36/64 loss: -0.05298894643783569
Batch 37/64 loss: -0.05147814750671387
Batch 38/64 loss: -0.04189270734786987
Batch 39/64 loss: -0.04082441329956055
Batch 40/64 loss: -0.03588998317718506
Batch 41/64 loss: -0.05677366256713867
Batch 42/64 loss: -0.01934957504272461
Batch 43/64 loss: -0.04046463966369629
Batch 44/64 loss: -0.05031692981719971
Batch 45/64 loss: -0.03393864631652832
Batch 46/64 loss: -0.026288330554962158
Batch 47/64 loss: -0.04223775863647461
Batch 48/64 loss: -0.028395354747772217
Batch 49/64 loss: -0.013077735900878906
Batch 50/64 loss: -0.02860814332962036
Batch 51/64 loss: -0.03958868980407715
Batch 52/64 loss: -0.035150885581970215
Batch 53/64 loss: -0.06057095527648926
Batch 54/64 loss: -0.04227721691131592
Batch 55/64 loss: -0.04811781644821167
Batch 56/64 loss: -0.03452014923095703
Batch 57/64 loss: -0.0509149432182312
Batch 58/64 loss: -0.02223348617553711
Batch 59/64 loss: -0.04311227798461914
Batch 60/64 loss: -0.041424334049224854
Batch 61/64 loss: -0.03889375925064087
Batch 62/64 loss: -0.04623645544052124
Batch 63/64 loss: -0.04615068435668945
Batch 64/64 loss: -0.04543507099151611
Epoch 423  Train loss: -0.04202640711092481  Val loss: 0.0973868064863985
Epoch 424
-------------------------------
Batch 1/64 loss: -0.033505260944366455
Batch 2/64 loss: -0.030656874179840088
Batch 3/64 loss: -0.05350011587142944
Batch 4/64 loss: -0.026579618453979492
Batch 5/64 loss: -0.050370097160339355
Batch 6/64 loss: -0.04415130615234375
Batch 7/64 loss: -0.03749191761016846
Batch 8/64 loss: -0.05849051475524902
Batch 9/64 loss: -0.0511249303817749
Batch 10/64 loss: -0.02706623077392578
Batch 11/64 loss: -0.04615223407745361
Batch 12/64 loss: -0.03601634502410889
Batch 13/64 loss: -0.060698509216308594
Batch 14/64 loss: -0.04443717002868652
Batch 15/64 loss: -0.04868364334106445
Batch 16/64 loss: -0.06006193161010742
Batch 17/64 loss: -0.04609477519989014
Batch 18/64 loss: -0.04124325513839722
Batch 19/64 loss: -0.050118446350097656
Batch 20/64 loss: -0.05170673131942749
Batch 21/64 loss: -0.06129467487335205
Batch 22/64 loss: -0.044125914573669434
Batch 23/64 loss: -0.03814059495925903
Batch 24/64 loss: -0.028549373149871826
Batch 25/64 loss: -0.010364830493927002
Batch 26/64 loss: -0.03735458850860596
Batch 27/64 loss: -0.05500233173370361
Batch 28/64 loss: -0.050945937633514404
Batch 29/64 loss: -0.055101871490478516
Batch 30/64 loss: -0.04703879356384277
Batch 31/64 loss: -0.022333920001983643
Batch 32/64 loss: -0.042999982833862305
Batch 33/64 loss: -0.043936967849731445
Batch 34/64 loss: -0.051768362522125244
Batch 35/64 loss: -0.03134793043136597
Batch 36/64 loss: -0.044608473777770996
Batch 37/64 loss: -0.057969093322753906
Batch 38/64 loss: -0.04470205307006836
Batch 39/64 loss: -0.031767189502716064
Batch 40/64 loss: -0.02920675277709961
Batch 41/64 loss: -0.04660993814468384
Batch 42/64 loss: -0.02654421329498291
Batch 43/64 loss: -0.04307836294174194
Batch 44/64 loss: -0.015800952911376953
Batch 45/64 loss: -0.04029428958892822
Batch 46/64 loss: -0.04016423225402832
Batch 47/64 loss: -0.040134429931640625
Batch 48/64 loss: -0.0471152663230896
Batch 49/64 loss: -0.046201109886169434
Batch 50/64 loss: -0.04318755865097046
Batch 51/64 loss: -0.03436601161956787
Batch 52/64 loss: -0.04332607984542847
Batch 53/64 loss: -0.046885013580322266
Batch 54/64 loss: -0.05350232124328613
Batch 55/64 loss: -0.04090076684951782
Batch 56/64 loss: -0.043110549449920654
Batch 57/64 loss: -0.03828108310699463
Batch 58/64 loss: -0.043631792068481445
Batch 59/64 loss: -0.04354095458984375
Batch 60/64 loss: -0.029325783252716064
Batch 61/64 loss: -0.05655205249786377
Batch 62/64 loss: -0.04580587148666382
Batch 63/64 loss: -0.012343227863311768
Batch 64/64 loss: -0.04148286581039429
Epoch 424  Train loss: -0.04201605577094882  Val loss: 0.09741196415268678
Epoch 425
-------------------------------
Batch 1/64 loss: -0.039537787437438965
Batch 2/64 loss: -0.03291630744934082
Batch 3/64 loss: -0.03253066539764404
Batch 4/64 loss: -0.04732769727706909
Batch 5/64 loss: -0.05080366134643555
Batch 6/64 loss: -0.05815160274505615
Batch 7/64 loss: -0.03374123573303223
Batch 8/64 loss: -0.03903341293334961
Batch 9/64 loss: -0.024165630340576172
Batch 10/64 loss: -0.04690515995025635
Batch 11/64 loss: -0.03125232458114624
Batch 12/64 loss: -0.029993534088134766
Batch 13/64 loss: -0.03423607349395752
Batch 14/64 loss: -0.038133323192596436
Batch 15/64 loss: -0.04754585027694702
Batch 16/64 loss: -0.03621792793273926
Batch 17/64 loss: -0.01786351203918457
Batch 18/64 loss: -0.033539414405822754
Batch 19/64 loss: -0.04697680473327637
Batch 20/64 loss: -0.04070615768432617
Batch 21/64 loss: -0.02779209613800049
Batch 22/64 loss: -0.04073375463485718
Batch 23/64 loss: -0.02980935573577881
Batch 24/64 loss: -0.04080933332443237
Batch 25/64 loss: -0.03306311368942261
Batch 26/64 loss: -0.044145166873931885
Batch 27/64 loss: -0.03953814506530762
Batch 28/64 loss: -0.037730395793914795
Batch 29/64 loss: -0.03437846899032593
Batch 30/64 loss: -0.03337061405181885
Batch 31/64 loss: -0.0503574013710022
Batch 32/64 loss: -0.053536295890808105
Batch 33/64 loss: -0.045818865299224854
Batch 34/64 loss: -0.03972905874252319
Batch 35/64 loss: -0.0521504282951355
Batch 36/64 loss: -0.019688725471496582
Batch 37/64 loss: -0.03671562671661377
Batch 38/64 loss: -0.03217005729675293
Batch 39/64 loss: -0.03731429576873779
Batch 40/64 loss: -0.014899611473083496
Batch 41/64 loss: -0.053893327713012695
Batch 42/64 loss: -0.04306793212890625
Batch 43/64 loss: -0.0466151237487793
Batch 44/64 loss: -0.050324440002441406
Batch 45/64 loss: -0.03338664770126343
Batch 46/64 loss: -0.05118203163146973
Batch 47/64 loss: -0.04426699876785278
Batch 48/64 loss: -0.05054342746734619
Batch 49/64 loss: -0.04735046625137329
Batch 50/64 loss: -0.047920942306518555
Batch 51/64 loss: -0.041913628578186035
Batch 52/64 loss: -0.030851125717163086
Batch 53/64 loss: -0.02147763967514038
Batch 54/64 loss: -0.01858609914779663
Batch 55/64 loss: -0.022340118885040283
Batch 56/64 loss: -0.042904436588287354
Batch 57/64 loss: -0.04613834619522095
Batch 58/64 loss: -0.020875215530395508
Batch 59/64 loss: -0.04282611608505249
Batch 60/64 loss: -0.037642717361450195
Batch 61/64 loss: -0.047952890396118164
Batch 62/64 loss: -0.024708926677703857
Batch 63/64 loss: -0.01721435785293579
Batch 64/64 loss: -0.03300905227661133
Epoch 425  Train loss: -0.03786774336122999  Val loss: 0.09727461354429369
Epoch 426
-------------------------------
Batch 1/64 loss: -0.039449989795684814
Batch 2/64 loss: -0.04894500970840454
Batch 3/64 loss: -0.031607627868652344
Batch 4/64 loss: -0.033381760120391846
Batch 5/64 loss: -0.04037553071975708
Batch 6/64 loss: -0.05476880073547363
Batch 7/64 loss: -0.0517880916595459
Batch 8/64 loss: -0.04938715696334839
Batch 9/64 loss: -0.054776549339294434
Batch 10/64 loss: -0.05082392692565918
Batch 11/64 loss: -0.04672658443450928
Batch 12/64 loss: -0.026064634323120117
Batch 13/64 loss: -0.03048241138458252
Batch 14/64 loss: -0.052494823932647705
Batch 15/64 loss: -0.04200482368469238
Batch 16/64 loss: -0.035100579261779785
Batch 17/64 loss: -0.048490941524505615
Batch 18/64 loss: -0.022002995014190674
Batch 19/64 loss: -0.05425894260406494
Batch 20/64 loss: -0.0025296807289123535
Batch 21/64 loss: -0.041181087493896484
Batch 22/64 loss: -0.033071041107177734
Batch 23/64 loss: -0.05169719457626343
Batch 24/64 loss: -0.02252054214477539
Batch 25/64 loss: -0.05591636896133423
Batch 26/64 loss: -0.043985724449157715
Batch 27/64 loss: -0.04005509614944458
Batch 28/64 loss: -0.029713690280914307
Batch 29/64 loss: -0.05216801166534424
Batch 30/64 loss: -0.04781860113143921
Batch 31/64 loss: -0.05796802043914795
Batch 32/64 loss: -0.05329006910324097
Batch 33/64 loss: -0.03728175163269043
Batch 34/64 loss: -0.0390741229057312
Batch 35/64 loss: -0.03113013505935669
Batch 36/64 loss: -0.053371965885162354
Batch 37/64 loss: -0.03136897087097168
Batch 38/64 loss: -0.040520668029785156
Batch 39/64 loss: -0.05111861228942871
Batch 40/64 loss: -0.04763096570968628
Batch 41/64 loss: -0.04212141036987305
Batch 42/64 loss: -0.049309492111206055
Batch 43/64 loss: -0.035154521465301514
Batch 44/64 loss: -0.044077157974243164
Batch 45/64 loss: -0.019297659397125244
Batch 46/64 loss: -0.0464097261428833
Batch 47/64 loss: -0.06039327383041382
Batch 48/64 loss: -0.011489391326904297
Batch 49/64 loss: -0.06105852127075195
Batch 50/64 loss: -0.029469192028045654
Batch 51/64 loss: -0.037271201610565186
Batch 52/64 loss: -0.035999178886413574
Batch 53/64 loss: -0.04634135961532593
Batch 54/64 loss: -0.04607123136520386
Batch 55/64 loss: -0.026033520698547363
Batch 56/64 loss: -0.04052448272705078
Batch 57/64 loss: -0.03186619281768799
Batch 58/64 loss: -0.0489119291305542
Batch 59/64 loss: -0.025639712810516357
Batch 60/64 loss: -0.056049466133117676
Batch 61/64 loss: -0.05023294687271118
Batch 62/64 loss: -0.05458807945251465
Batch 63/64 loss: -0.031842708587646484
Batch 64/64 loss: -0.053363680839538574
Epoch 426  Train loss: -0.041514017535190954  Val loss: 0.09859252877251799
Epoch 427
-------------------------------
Batch 1/64 loss: -0.042844414710998535
Batch 2/64 loss: -0.03798168897628784
Batch 3/64 loss: -0.03285020589828491
Batch 4/64 loss: -0.05495649576187134
Batch 5/64 loss: -0.03423810005187988
Batch 6/64 loss: -0.04158198833465576
Batch 7/64 loss: -0.04004281759262085
Batch 8/64 loss: -0.059141337871551514
Batch 9/64 loss: -0.028096318244934082
Batch 10/64 loss: -0.02902311086654663
Batch 11/64 loss: -0.03503084182739258
Batch 12/64 loss: -0.04912757873535156
Batch 13/64 loss: -0.04944968223571777
Batch 14/64 loss: -0.06381785869598389
Batch 15/64 loss: -0.05923563241958618
Batch 16/64 loss: -0.06147128343582153
Batch 17/64 loss: -0.06253194808959961
Batch 18/64 loss: -0.0546269416809082
Batch 19/64 loss: -0.04661679267883301
Batch 20/64 loss: -0.050927937030792236
Batch 21/64 loss: -0.03650355339050293
Batch 22/64 loss: -0.04195064306259155
Batch 23/64 loss: -0.03565037250518799
Batch 24/64 loss: -0.0364069938659668
Batch 25/64 loss: -0.04151952266693115
Batch 26/64 loss: -0.037124037742614746
Batch 27/64 loss: -0.04032289981842041
Batch 28/64 loss: -0.014315962791442871
Batch 29/64 loss: -0.05420506000518799
Batch 30/64 loss: -0.033634305000305176
Batch 31/64 loss: -0.03733319044113159
Batch 32/64 loss: -0.04865545034408569
Batch 33/64 loss: -0.058076322078704834
Batch 34/64 loss: -0.04536128044128418
Batch 35/64 loss: -0.03756880760192871
Batch 36/64 loss: -0.03655916452407837
Batch 37/64 loss: -0.01587855815887451
Batch 38/64 loss: -0.03655332326889038
Batch 39/64 loss: -0.050829529762268066
Batch 40/64 loss: -0.03914940357208252
Batch 41/64 loss: -0.048924803733825684
Batch 42/64 loss: -0.032730698585510254
Batch 43/64 loss: -0.04576516151428223
Batch 44/64 loss: -0.059722185134887695
Batch 45/64 loss: -0.04555058479309082
Batch 46/64 loss: -0.03802210092544556
Batch 47/64 loss: -0.04549562931060791
Batch 48/64 loss: -0.03380382061004639
Batch 49/64 loss: -0.04778718948364258
Batch 50/64 loss: -0.03961646556854248
Batch 51/64 loss: -0.05833232402801514
Batch 52/64 loss: -0.048879027366638184
Batch 53/64 loss: -0.044205546379089355
Batch 54/64 loss: -0.054007768630981445
Batch 55/64 loss: -0.04330623149871826
Batch 56/64 loss: -0.0390278697013855
Batch 57/64 loss: -0.035144031047821045
Batch 58/64 loss: -0.04716145992279053
Batch 59/64 loss: -0.03498256206512451
Batch 60/64 loss: -0.04141736030578613
Batch 61/64 loss: -0.023987531661987305
Batch 62/64 loss: -0.044728994369506836
Batch 63/64 loss: -0.027705609798431396
Batch 64/64 loss: -0.046784937381744385
Epoch 427  Train loss: -0.042769961029875514  Val loss: 0.09621508752357509
Epoch 428
-------------------------------
Batch 1/64 loss: -0.04600334167480469
Batch 2/64 loss: -0.05323612689971924
Batch 3/64 loss: -0.039673805236816406
Batch 4/64 loss: -0.04596996307373047
Batch 5/64 loss: -0.0522235631942749
Batch 6/64 loss: -0.03842461109161377
Batch 7/64 loss: -0.04003345966339111
Batch 8/64 loss: -0.058689117431640625
Batch 9/64 loss: -0.034435272216796875
Batch 10/64 loss: -0.04726684093475342
Batch 11/64 loss: -0.0454941987991333
Batch 12/64 loss: -0.048539817333221436
Batch 13/64 loss: -0.04822266101837158
Batch 14/64 loss: -0.05917859077453613
Batch 15/64 loss: -0.04821276664733887
Batch 16/64 loss: -0.05457782745361328
Batch 17/64 loss: -0.043712735176086426
Batch 18/64 loss: -0.051137566566467285
Batch 19/64 loss: -0.03912836313247681
Batch 20/64 loss: -0.034622013568878174
Batch 21/64 loss: -0.04457902908325195
Batch 22/64 loss: -0.04538440704345703
Batch 23/64 loss: -0.05320960283279419
Batch 24/64 loss: -0.06488305330276489
Batch 25/64 loss: -0.03674745559692383
Batch 26/64 loss: -0.029802918434143066
Batch 27/64 loss: -0.04063767194747925
Batch 28/64 loss: -0.0569494366645813
Batch 29/64 loss: -0.037534475326538086
Batch 30/64 loss: -0.05975526571273804
Batch 31/64 loss: -0.05407291650772095
Batch 32/64 loss: -0.02631378173828125
Batch 33/64 loss: -0.051988959312438965
Batch 34/64 loss: -0.03850436210632324
Batch 35/64 loss: -0.041746675968170166
Batch 36/64 loss: -0.03820383548736572
Batch 37/64 loss: -0.03334897756576538
Batch 38/64 loss: -0.03711134195327759
Batch 39/64 loss: -0.021438777446746826
Batch 40/64 loss: -0.04880964756011963
Batch 41/64 loss: -0.04434847831726074
Batch 42/64 loss: -0.04396843910217285
Batch 43/64 loss: -0.06645244359970093
Batch 44/64 loss: -0.044847846031188965
Batch 45/64 loss: -0.03696465492248535
Batch 46/64 loss: -0.02960360050201416
Batch 47/64 loss: -0.031992435455322266
Batch 48/64 loss: -0.04278045892715454
Batch 49/64 loss: -0.04543179273605347
Batch 50/64 loss: -0.020241975784301758
Batch 51/64 loss: -0.03616046905517578
Batch 52/64 loss: -0.057437777519226074
Batch 53/64 loss: -0.048557400703430176
Batch 54/64 loss: -0.03773009777069092
Batch 55/64 loss: -0.05018270015716553
Batch 56/64 loss: -0.05201530456542969
Batch 57/64 loss: -0.03299403190612793
Batch 58/64 loss: -0.052357017993927
Batch 59/64 loss: -0.026560425758361816
Batch 60/64 loss: -0.05105412006378174
Batch 61/64 loss: -0.05652201175689697
Batch 62/64 loss: -0.04374504089355469
Batch 63/64 loss: -0.027461469173431396
Batch 64/64 loss: -0.04394012689590454
Epoch 428  Train loss: -0.043955612883848305  Val loss: 0.09826670725321032
Epoch 429
-------------------------------
Batch 1/64 loss: -0.05343317985534668
Batch 2/64 loss: -0.06783407926559448
Batch 3/64 loss: -0.05336540937423706
Batch 4/64 loss: -0.05608654022216797
Batch 5/64 loss: -0.03383338451385498
Batch 6/64 loss: -0.05924844741821289
Batch 7/64 loss: -0.048300743103027344
Batch 8/64 loss: -0.045332908630371094
Batch 9/64 loss: -0.0453110933303833
Batch 10/64 loss: -0.05623656511306763
Batch 11/64 loss: -0.06464499235153198
Batch 12/64 loss: -0.05376434326171875
Batch 13/64 loss: -0.04482460021972656
Batch 14/64 loss: -0.045308828353881836
Batch 15/64 loss: -0.052457571029663086
Batch 16/64 loss: -0.030481696128845215
Batch 17/64 loss: -0.05248326063156128
Batch 18/64 loss: -0.05426734685897827
Batch 19/64 loss: -0.053872764110565186
Batch 20/64 loss: -0.047742247581481934
Batch 21/64 loss: -0.03069162368774414
Batch 22/64 loss: -0.056365013122558594
Batch 23/64 loss: -0.03511327505111694
Batch 24/64 loss: -0.05069094896316528
Batch 25/64 loss: -0.03716409206390381
Batch 26/64 loss: -0.04394656419754028
Batch 27/64 loss: -0.05022245645523071
Batch 28/64 loss: -0.026509761810302734
Batch 29/64 loss: -0.04690897464752197
Batch 30/64 loss: -0.04999136924743652
Batch 31/64 loss: -0.030036330223083496
Batch 32/64 loss: -0.03416609764099121
Batch 33/64 loss: -0.04501771926879883
Batch 34/64 loss: -0.05352067947387695
Batch 35/64 loss: -0.04036986827850342
Batch 36/64 loss: -0.048032939434051514
Batch 37/64 loss: -0.04095274209976196
Batch 38/64 loss: -0.041004061698913574
Batch 39/64 loss: -0.04442620277404785
Batch 40/64 loss: -0.04876375198364258
Batch 41/64 loss: -0.0440140962600708
Batch 42/64 loss: -0.0489463210105896
Batch 43/64 loss: -0.04802674055099487
Batch 44/64 loss: -0.040381431579589844
Batch 45/64 loss: -0.03866058588027954
Batch 46/64 loss: -0.05356937646865845
Batch 47/64 loss: -0.04627537727355957
Batch 48/64 loss: -0.05147939920425415
Batch 49/64 loss: -0.021658658981323242
Batch 50/64 loss: -0.030938804149627686
Batch 51/64 loss: -0.05290955305099487
Batch 52/64 loss: -0.04978674650192261
Batch 53/64 loss: -0.03645741939544678
Batch 54/64 loss: -0.0407562255859375
Batch 55/64 loss: -0.05422884225845337
Batch 56/64 loss: -0.05265939235687256
Batch 57/64 loss: -0.039493024349212646
Batch 58/64 loss: -0.04535496234893799
Batch 59/64 loss: -0.046638309955596924
Batch 60/64 loss: -0.04629170894622803
Batch 61/64 loss: -0.03398340940475464
Batch 62/64 loss: -0.03730607032775879
Batch 63/64 loss: -0.04398733377456665
Batch 64/64 loss: -0.03379666805267334
Epoch 429  Train loss: -0.04551961982951445  Val loss: 0.09608514837382995
Epoch 430
-------------------------------
Batch 1/64 loss: -0.05726522207260132
Batch 2/64 loss: -0.05332547426223755
Batch 3/64 loss: -0.05390191078186035
Batch 4/64 loss: -0.053569018840789795
Batch 5/64 loss: -0.05558896064758301
Batch 6/64 loss: -0.043594300746917725
Batch 7/64 loss: -0.042952656745910645
Batch 8/64 loss: -0.04463362693786621
Batch 9/64 loss: -0.04673027992248535
Batch 10/64 loss: -0.05226278305053711
Batch 11/64 loss: -0.06462091207504272
Batch 12/64 loss: -0.045570433139801025
Batch 13/64 loss: -0.04321831464767456
Batch 14/64 loss: -0.05529224872589111
Batch 15/64 loss: -0.048200130462646484
Batch 16/64 loss: -0.04564875364303589
Batch 17/64 loss: -0.028582274913787842
Batch 18/64 loss: -0.039604127407073975
Batch 19/64 loss: -0.05130040645599365
Batch 20/64 loss: -0.006645619869232178
Batch 21/64 loss: -0.036489665508270264
Batch 22/64 loss: -0.042510986328125
Batch 23/64 loss: -0.04820120334625244
Batch 24/64 loss: -0.04913198947906494
Batch 25/64 loss: -0.049486398696899414
Batch 26/64 loss: -0.06214702129364014
Batch 27/64 loss: -0.04445427656173706
Batch 28/64 loss: -0.03236496448516846
Batch 29/64 loss: -0.05842792987823486
Batch 30/64 loss: -0.0376514196395874
Batch 31/64 loss: -0.021798431873321533
Batch 32/64 loss: -0.039718687534332275
Batch 33/64 loss: -0.06106811761856079
Batch 34/64 loss: -0.04863333702087402
Batch 35/64 loss: -0.026694178581237793
Batch 36/64 loss: -0.033834218978881836
Batch 37/64 loss: -0.05065411329269409
Batch 38/64 loss: -0.055553555488586426
Batch 39/64 loss: -0.036831021308898926
Batch 40/64 loss: -0.04846227169036865
Batch 41/64 loss: -0.05388152599334717
Batch 42/64 loss: -0.04634350538253784
Batch 43/64 loss: -0.03277111053466797
Batch 44/64 loss: -0.05091661214828491
Batch 45/64 loss: -0.04079693555831909
Batch 46/64 loss: -0.0412101149559021
Batch 47/64 loss: -0.04283195734024048
Batch 48/64 loss: -0.053137898445129395
Batch 49/64 loss: -0.02670466899871826
Batch 50/64 loss: -0.048001110553741455
Batch 51/64 loss: -0.049942612648010254
Batch 52/64 loss: -0.04738330841064453
Batch 53/64 loss: -0.0398712158203125
Batch 54/64 loss: -0.033474504947662354
Batch 55/64 loss: -0.03321230411529541
Batch 56/64 loss: -0.04086047410964966
Batch 57/64 loss: -0.03976857662200928
Batch 58/64 loss: -0.04756784439086914
Batch 59/64 loss: -0.044014573097229004
Batch 60/64 loss: -0.04110509157180786
Batch 61/64 loss: -0.05655938386917114
Batch 62/64 loss: -0.05175817012786865
Batch 63/64 loss: -0.011209547519683838
Batch 64/64 loss: -0.04815042018890381
Epoch 430  Train loss: -0.04433030754912133  Val loss: 0.09611626507080707
Epoch 431
-------------------------------
Batch 1/64 loss: -0.030994713306427002
Batch 2/64 loss: -0.03401291370391846
Batch 3/64 loss: -0.04978454113006592
Batch 4/64 loss: -0.05513036251068115
Batch 5/64 loss: -0.02349841594696045
Batch 6/64 loss: -0.04548448324203491
Batch 7/64 loss: -0.04131639003753662
Batch 8/64 loss: -0.04526352882385254
Batch 9/64 loss: -0.03241467475891113
Batch 10/64 loss: -0.04224860668182373
Batch 11/64 loss: -0.06466865539550781
Batch 12/64 loss: -0.06450486183166504
Batch 13/64 loss: -0.05357944965362549
Batch 14/64 loss: -0.04569101333618164
Batch 15/64 loss: -0.05467420816421509
Batch 16/64 loss: -0.04887819290161133
Batch 17/64 loss: -0.056027352809906006
Batch 18/64 loss: -0.056269049644470215
Batch 19/64 loss: -0.05053114891052246
Batch 20/64 loss: -0.046527087688446045
Batch 21/64 loss: -0.04545915126800537
Batch 22/64 loss: -0.048000335693359375
Batch 23/64 loss: -0.04522484540939331
Batch 24/64 loss: -0.04917943477630615
Batch 25/64 loss: -0.05128026008605957
Batch 26/64 loss: -0.023249804973602295
Batch 27/64 loss: -0.03960949182510376
Batch 28/64 loss: -0.04104059934616089
Batch 29/64 loss: -0.061969876289367676
Batch 30/64 loss: -0.04554861783981323
Batch 31/64 loss: -0.05438232421875
Batch 32/64 loss: -0.0286368727684021
Batch 33/64 loss: -0.0432208776473999
Batch 34/64 loss: -0.041114985942840576
Batch 35/64 loss: -0.04125106334686279
Batch 36/64 loss: -0.042823612689971924
Batch 37/64 loss: -0.04538142681121826
Batch 38/64 loss: -0.032758116722106934
Batch 39/64 loss: -0.04717552661895752
Batch 40/64 loss: -0.04177790880203247
Batch 41/64 loss: -0.04675513505935669
Batch 42/64 loss: -0.053627967834472656
Batch 43/64 loss: -0.046218276023864746
Batch 44/64 loss: -0.03896099328994751
Batch 45/64 loss: -0.04140251874923706
Batch 46/64 loss: -0.055886685848236084
Batch 47/64 loss: -0.04872363805770874
Batch 48/64 loss: -0.029239773750305176
Batch 49/64 loss: -0.029269874095916748
Batch 50/64 loss: -0.03905630111694336
Batch 51/64 loss: -0.02457374334335327
Batch 52/64 loss: -0.035838961601257324
Batch 53/64 loss: -0.03957664966583252
Batch 54/64 loss: -0.03433418273925781
Batch 55/64 loss: -0.038776397705078125
Batch 56/64 loss: -0.061277568340301514
Batch 57/64 loss: -0.053534746170043945
Batch 58/64 loss: -0.03445029258728027
Batch 59/64 loss: -0.03990250825881958
Batch 60/64 loss: -0.055146992206573486
Batch 61/64 loss: -0.027588307857513428
Batch 62/64 loss: -0.03768116235733032
Batch 63/64 loss: -0.04334491491317749
Batch 64/64 loss: -0.033822834491729736
Epoch 431  Train loss: -0.043782266682269526  Val loss: 0.09873608973427736
Epoch 432
-------------------------------
Batch 1/64 loss: -0.05355173349380493
Batch 2/64 loss: -0.06063437461853027
Batch 3/64 loss: -0.061258912086486816
Batch 4/64 loss: -0.04837411642074585
Batch 5/64 loss: -0.05438715219497681
Batch 6/64 loss: -0.04272425174713135
Batch 7/64 loss: -0.05513876676559448
Batch 8/64 loss: -0.04760313034057617
Batch 9/64 loss: -0.0669240951538086
Batch 10/64 loss: -0.03367912769317627
Batch 11/64 loss: -0.06084775924682617
Batch 12/64 loss: -0.055435001850128174
Batch 13/64 loss: -0.02609783411026001
Batch 14/64 loss: -0.04409235715866089
Batch 15/64 loss: -0.04678499698638916
Batch 16/64 loss: -0.05828070640563965
Batch 17/64 loss: -0.05131888389587402
Batch 18/64 loss: -0.05528569221496582
Batch 19/64 loss: -0.041705191135406494
Batch 20/64 loss: -0.045885443687438965
Batch 21/64 loss: -0.0515781044960022
Batch 22/64 loss: -0.04217338562011719
Batch 23/64 loss: -0.05260825157165527
Batch 24/64 loss: -0.04506814479827881
Batch 25/64 loss: -0.04495131969451904
Batch 26/64 loss: -0.04304760694503784
Batch 27/64 loss: -0.05919992923736572
Batch 28/64 loss: -0.04997450113296509
Batch 29/64 loss: -0.0431903600692749
Batch 30/64 loss: -0.04455745220184326
Batch 31/64 loss: -0.046675801277160645
Batch 32/64 loss: -0.03513133525848389
Batch 33/64 loss: -0.022806823253631592
Batch 34/64 loss: -0.04472017288208008
Batch 35/64 loss: -0.036959052085876465
Batch 36/64 loss: -0.041918933391571045
Batch 37/64 loss: -0.053726375102996826
Batch 38/64 loss: -0.05849117040634155
Batch 39/64 loss: -0.044069886207580566
Batch 40/64 loss: -0.05576509237289429
Batch 41/64 loss: -0.03612840175628662
Batch 42/64 loss: -0.05071115493774414
Batch 43/64 loss: -0.01819711923599243
Batch 44/64 loss: -0.05394548177719116
Batch 45/64 loss: -0.04615992307662964
Batch 46/64 loss: -0.046532630920410156
Batch 47/64 loss: -0.027721881866455078
Batch 48/64 loss: -0.01733928918838501
Batch 49/64 loss: -0.055563509464263916
Batch 50/64 loss: -0.0407833456993103
Batch 51/64 loss: -0.047181010246276855
Batch 52/64 loss: -0.05147898197174072
Batch 53/64 loss: -0.04026484489440918
Batch 54/64 loss: -0.05201125144958496
Batch 55/64 loss: -0.028530538082122803
Batch 56/64 loss: -0.05414849519729614
Batch 57/64 loss: -0.042030930519104004
Batch 58/64 loss: -0.03592967987060547
Batch 59/64 loss: -0.04776132106781006
Batch 60/64 loss: -0.022133708000183105
Batch 61/64 loss: -0.04842936992645264
Batch 62/64 loss: -0.03685605525970459
Batch 63/64 loss: -0.04359310865402222
Batch 64/64 loss: -0.05364638566970825
Epoch 432  Train loss: -0.04565160858864878  Val loss: 0.09577420978611688
Epoch 433
-------------------------------
Batch 1/64 loss: -0.04752010107040405
Batch 2/64 loss: -0.05334138870239258
Batch 3/64 loss: -0.03665214776992798
Batch 4/64 loss: -0.05077022314071655
Batch 5/64 loss: -0.035471200942993164
Batch 6/64 loss: -0.04986327886581421
Batch 7/64 loss: -0.04923528432846069
Batch 8/64 loss: -0.042272090911865234
Batch 9/64 loss: -0.041685640811920166
Batch 10/64 loss: -0.052667319774627686
Batch 11/64 loss: -0.022825777530670166
Batch 12/64 loss: -0.041484951972961426
Batch 13/64 loss: -0.05827361345291138
Batch 14/64 loss: -0.050039827823638916
Batch 15/64 loss: -0.052906334400177
Batch 16/64 loss: -0.03546875715255737
Batch 17/64 loss: -0.0513649582862854
Batch 18/64 loss: -0.056760311126708984
Batch 19/64 loss: -0.05767381191253662
Batch 20/64 loss: -0.048672497272491455
Batch 21/64 loss: -0.05128711462020874
Batch 22/64 loss: -0.0528186559677124
Batch 23/64 loss: -0.04605233669281006
Batch 24/64 loss: -0.04937875270843506
Batch 25/64 loss: -0.038057029247283936
Batch 26/64 loss: -0.030674397945404053
Batch 27/64 loss: -0.01710379123687744
Batch 28/64 loss: -0.035404086112976074
Batch 29/64 loss: -0.05146080255508423
Batch 30/64 loss: -0.0433235764503479
Batch 31/64 loss: -0.04489278793334961
Batch 32/64 loss: -0.05761665105819702
Batch 33/64 loss: -0.0413936972618103
Batch 34/64 loss: -0.05455887317657471
Batch 35/64 loss: -0.04901015758514404
Batch 36/64 loss: -0.05746138095855713
Batch 37/64 loss: -0.04225355386734009
Batch 38/64 loss: -0.04853755235671997
Batch 39/64 loss: -0.03493911027908325
Batch 40/64 loss: -0.04329574108123779
Batch 41/64 loss: -0.04217571020126343
Batch 42/64 loss: -0.05264192819595337
Batch 43/64 loss: -0.05263012647628784
Batch 44/64 loss: -0.033518314361572266
Batch 45/64 loss: -0.0356099009513855
Batch 46/64 loss: -0.02217918634414673
Batch 47/64 loss: -0.04189199209213257
Batch 48/64 loss: -0.039720118045806885
Batch 49/64 loss: -0.043476998805999756
Batch 50/64 loss: -0.04672795534133911
Batch 51/64 loss: -0.041347503662109375
Batch 52/64 loss: -0.03321266174316406
Batch 53/64 loss: -0.049196720123291016
Batch 54/64 loss: -0.04349195957183838
Batch 55/64 loss: -0.041005730628967285
Batch 56/64 loss: -0.036984920501708984
Batch 57/64 loss: -0.02611255645751953
Batch 58/64 loss: -0.03505206108093262
Batch 59/64 loss: -0.023903250694274902
Batch 60/64 loss: -0.04118829965591431
Batch 61/64 loss: -0.04604506492614746
Batch 62/64 loss: -0.02715986967086792
Batch 63/64 loss: -0.04187822341918945
Batch 64/64 loss: -0.06249535083770752
Epoch 433  Train loss: -0.04342732756745581  Val loss: 0.09741219867955368
Epoch 434
-------------------------------
Batch 1/64 loss: -0.04098165035247803
Batch 2/64 loss: -0.05136913061141968
Batch 3/64 loss: -0.04926586151123047
Batch 4/64 loss: -0.054803669452667236
Batch 5/64 loss: -0.053214967250823975
Batch 6/64 loss: -0.05427271127700806
Batch 7/64 loss: -0.04569268226623535
Batch 8/64 loss: -0.0530695915222168
Batch 9/64 loss: -0.048874855041503906
Batch 10/64 loss: -0.03255182504653931
Batch 11/64 loss: -0.04137367010116577
Batch 12/64 loss: -0.051842570304870605
Batch 13/64 loss: -0.04828906059265137
Batch 14/64 loss: -0.04773128032684326
Batch 15/64 loss: -0.024426639080047607
Batch 16/64 loss: -0.03895527124404907
Batch 17/64 loss: -0.06539803743362427
Batch 18/64 loss: -0.0563197135925293
Batch 19/64 loss: -0.044121503829956055
Batch 20/64 loss: -0.07121288776397705
Batch 21/64 loss: -0.04894763231277466
Batch 22/64 loss: -0.03411310911178589
Batch 23/64 loss: -0.025101780891418457
Batch 24/64 loss: -0.05422067642211914
Batch 25/64 loss: -0.04194080829620361
Batch 26/64 loss: -0.057742178440093994
Batch 27/64 loss: -0.05520439147949219
Batch 28/64 loss: -0.038522541522979736
Batch 29/64 loss: -0.03694730997085571
Batch 30/64 loss: -0.05104571580886841
Batch 31/64 loss: -0.054561078548431396
Batch 32/64 loss: -0.05923646688461304
Batch 33/64 loss: -0.04351043701171875
Batch 34/64 loss: -0.04270124435424805
Batch 35/64 loss: -0.008592545986175537
Batch 36/64 loss: -0.04348355531692505
Batch 37/64 loss: -0.023944973945617676
Batch 38/64 loss: -0.0428234338760376
Batch 39/64 loss: -0.040882885456085205
Batch 40/64 loss: -0.039533793926239014
Batch 41/64 loss: -0.03774738311767578
Batch 42/64 loss: -0.03850299119949341
Batch 43/64 loss: -0.024891972541809082
Batch 44/64 loss: -0.053446829319000244
Batch 45/64 loss: -0.051900625228881836
Batch 46/64 loss: -0.03409641981124878
Batch 47/64 loss: -0.03999418020248413
Batch 48/64 loss: -0.04442685842514038
Batch 49/64 loss: -0.03517460823059082
Batch 50/64 loss: -0.046159207820892334
Batch 51/64 loss: -0.04547548294067383
Batch 52/64 loss: -0.048983097076416016
Batch 53/64 loss: -0.03146570920944214
Batch 54/64 loss: -0.05176246166229248
Batch 55/64 loss: -0.04416471719741821
Batch 56/64 loss: -0.02575993537902832
Batch 57/64 loss: -0.04369240999221802
Batch 58/64 loss: -0.03311753273010254
Batch 59/64 loss: -0.04562324285507202
Batch 60/64 loss: -0.04597049951553345
Batch 61/64 loss: -0.0470733642578125
Batch 62/64 loss: -0.04688584804534912
Batch 63/64 loss: -0.05867135524749756
Batch 64/64 loss: -0.05358529090881348
Epoch 434  Train loss: -0.04442352687611299  Val loss: 0.09742727291952703
Epoch 435
-------------------------------
Batch 1/64 loss: -0.04368323087692261
Batch 2/64 loss: -0.03375214338302612
Batch 3/64 loss: -0.03562057018280029
Batch 4/64 loss: -0.05083745718002319
Batch 5/64 loss: -0.05266261100769043
Batch 6/64 loss: -0.03276872634887695
Batch 7/64 loss: -0.042933106422424316
Batch 8/64 loss: -0.02983260154724121
Batch 9/64 loss: -0.04924660921096802
Batch 10/64 loss: -0.05753248929977417
Batch 11/64 loss: -0.05436122417449951
Batch 12/64 loss: -0.042910099029541016
Batch 13/64 loss: -0.047486186027526855
Batch 14/64 loss: -0.030150413513183594
Batch 15/64 loss: -0.05330967903137207
Batch 16/64 loss: -0.047062814235687256
Batch 17/64 loss: -0.062370359897613525
Batch 18/64 loss: -0.04170125722885132
Batch 19/64 loss: -0.034025728702545166
Batch 20/64 loss: -0.0264132022857666
Batch 21/64 loss: -0.040162622928619385
Batch 22/64 loss: -0.04635047912597656
Batch 23/64 loss: -0.03877377510070801
Batch 24/64 loss: -0.05937397480010986
Batch 25/64 loss: -0.04502338171005249
Batch 26/64 loss: -0.05064117908477783
Batch 27/64 loss: -0.03890681266784668
Batch 28/64 loss: -0.05234479904174805
Batch 29/64 loss: -0.040029048919677734
Batch 30/64 loss: -0.0722651481628418
Batch 31/64 loss: -0.055139362812042236
Batch 32/64 loss: -0.047099530696868896
Batch 33/64 loss: -0.04629606008529663
Batch 34/64 loss: -0.056154489517211914
Batch 35/64 loss: -0.033502280712127686
Batch 36/64 loss: -0.0538100004196167
Batch 37/64 loss: -0.05937463045120239
Batch 38/64 loss: -0.05050903558731079
Batch 39/64 loss: -0.05789685249328613
Batch 40/64 loss: -0.03167051076889038
Batch 41/64 loss: -0.04816323518753052
Batch 42/64 loss: -0.03478729724884033
Batch 43/64 loss: -0.043078064918518066
Batch 44/64 loss: -0.03947991132736206
Batch 45/64 loss: -0.041660308837890625
Batch 46/64 loss: -0.052151501178741455
Batch 47/64 loss: -0.04457736015319824
Batch 48/64 loss: -0.05379891395568848
Batch 49/64 loss: -0.0394519567489624
Batch 50/64 loss: -0.048935115337371826
Batch 51/64 loss: -0.04898160696029663
Batch 52/64 loss: -0.042924344539642334
Batch 53/64 loss: -0.04077601432800293
Batch 54/64 loss: -0.0385822057723999
Batch 55/64 loss: -0.053730547428131104
Batch 56/64 loss: -0.04354751110076904
Batch 57/64 loss: -0.03651386499404907
Batch 58/64 loss: -0.04624950885772705
Batch 59/64 loss: -0.03263378143310547
Batch 60/64 loss: -0.04486268758773804
Batch 61/64 loss: -0.051113009452819824
Batch 62/64 loss: -0.0358043909072876
Batch 63/64 loss: -0.054831504821777344
Batch 64/64 loss: -0.045987069606781006
Epoch 435  Train loss: -0.04541348125420365  Val loss: 0.09748177532477886
Epoch 436
-------------------------------
Batch 1/64 loss: -0.028073549270629883
Batch 2/64 loss: -0.02386915683746338
Batch 3/64 loss: -0.052236199378967285
Batch 4/64 loss: -0.03587472438812256
Batch 5/64 loss: -0.05036735534667969
Batch 6/64 loss: -0.039676547050476074
Batch 7/64 loss: -0.027078211307525635
Batch 8/64 loss: -0.022345304489135742
Batch 9/64 loss: -0.03567361831665039
Batch 10/64 loss: -0.030672848224639893
Batch 11/64 loss: -0.0359615683555603
Batch 12/64 loss: -0.047852516174316406
Batch 13/64 loss: -0.055606842041015625
Batch 14/64 loss: -0.02590310573577881
Batch 15/64 loss: -0.048194825649261475
Batch 16/64 loss: -0.052568793296813965
Batch 17/64 loss: -0.05069541931152344
Batch 18/64 loss: -0.033405601978302
Batch 19/64 loss: -0.03947043418884277
Batch 20/64 loss: -0.05420774221420288
Batch 21/64 loss: -0.04455125331878662
Batch 22/64 loss: -0.04861736297607422
Batch 23/64 loss: -0.03824913501739502
Batch 24/64 loss: -0.047225356101989746
Batch 25/64 loss: -0.046419739723205566
Batch 26/64 loss: -0.05572652816772461
Batch 27/64 loss: -0.04140937328338623
Batch 28/64 loss: -0.0543980598449707
Batch 29/64 loss: -0.02852952480316162
Batch 30/64 loss: -0.05497545003890991
Batch 31/64 loss: -0.05148613452911377
Batch 32/64 loss: -0.05141860246658325
Batch 33/64 loss: -0.036488115787506104
Batch 34/64 loss: -0.04943382740020752
Batch 35/64 loss: -0.05370664596557617
Batch 36/64 loss: -0.04938381910324097
Batch 37/64 loss: -0.04198944568634033
Batch 38/64 loss: -0.0699567198753357
Batch 39/64 loss: -0.05003213882446289
Batch 40/64 loss: -0.043963074684143066
Batch 41/64 loss: -0.05399829149246216
Batch 42/64 loss: -0.039052605628967285
Batch 43/64 loss: -0.05446046590805054
Batch 44/64 loss: -0.04080897569656372
Batch 45/64 loss: -0.060304105281829834
Batch 46/64 loss: -0.03408414125442505
Batch 47/64 loss: -0.05883306264877319
Batch 48/64 loss: -0.04112398624420166
Batch 49/64 loss: -0.05755394697189331
Batch 50/64 loss: -0.0479472279548645
Batch 51/64 loss: -0.05055516958236694
Batch 52/64 loss: -0.052385568618774414
Batch 53/64 loss: -0.05560564994812012
Batch 54/64 loss: -0.05009770393371582
Batch 55/64 loss: -0.0469517707824707
Batch 56/64 loss: -0.031878888607025146
Batch 57/64 loss: -0.04667842388153076
Batch 58/64 loss: -0.02209240198135376
Batch 59/64 loss: -0.04370397329330444
Batch 60/64 loss: -0.047818541526794434
Batch 61/64 loss: -0.049943625926971436
Batch 62/64 loss: -0.058001697063446045
Batch 63/64 loss: -0.03685307502746582
Batch 64/64 loss: -0.04836660623550415
Epoch 436  Train loss: -0.04493651647193759  Val loss: 0.09811401531049066
Epoch 437
-------------------------------
Batch 1/64 loss: -0.0346185564994812
Batch 2/64 loss: -0.015131473541259766
Batch 3/64 loss: -0.032483816146850586
Batch 4/64 loss: -0.04067254066467285
Batch 5/64 loss: -0.045984089374542236
Batch 6/64 loss: -0.03337150812149048
Batch 7/64 loss: -0.042832791805267334
Batch 8/64 loss: -0.041627466678619385
Batch 9/64 loss: -0.059990882873535156
Batch 10/64 loss: -0.04244893789291382
Batch 11/64 loss: -0.035067975521087646
Batch 12/64 loss: -0.043105125427246094
Batch 13/64 loss: -0.05336505174636841
Batch 14/64 loss: -0.053090691566467285
Batch 15/64 loss: -0.04087412357330322
Batch 16/64 loss: -0.051879942417144775
Batch 17/64 loss: -0.04315441846847534
Batch 18/64 loss: -0.052424490451812744
Batch 19/64 loss: -0.04932665824890137
Batch 20/64 loss: -0.041138529777526855
Batch 21/64 loss: -0.047154903411865234
Batch 22/64 loss: -0.06132990121841431
Batch 23/64 loss: -0.029346346855163574
Batch 24/64 loss: -0.05885136127471924
Batch 25/64 loss: -0.050898849964141846
Batch 26/64 loss: -0.05574929714202881
Batch 27/64 loss: -0.049156904220581055
Batch 28/64 loss: -0.033285677433013916
Batch 29/64 loss: -0.05379652976989746
Batch 30/64 loss: -0.04421257972717285
Batch 31/64 loss: -0.06237989664077759
Batch 32/64 loss: -0.03331863880157471
Batch 33/64 loss: -0.04280877113342285
Batch 34/64 loss: -0.054763853549957275
Batch 35/64 loss: -0.04579734802246094
Batch 36/64 loss: -0.036546170711517334
Batch 37/64 loss: -0.05355459451675415
Batch 38/64 loss: -0.060749173164367676
Batch 39/64 loss: -0.0271112322807312
Batch 40/64 loss: -0.047391653060913086
Batch 41/64 loss: -0.06042921543121338
Batch 42/64 loss: -0.02147209644317627
Batch 43/64 loss: -0.03524380922317505
Batch 44/64 loss: -0.05597543716430664
Batch 45/64 loss: -0.030191540718078613
Batch 46/64 loss: -0.026534557342529297
Batch 47/64 loss: -0.052905142307281494
Batch 48/64 loss: -0.05615264177322388
Batch 49/64 loss: -0.04526960849761963
Batch 50/64 loss: -0.041896939277648926
Batch 51/64 loss: -0.05543029308319092
Batch 52/64 loss: -0.06100320816040039
Batch 53/64 loss: -0.03992033004760742
Batch 54/64 loss: -0.051808059215545654
Batch 55/64 loss: -0.03745770454406738
Batch 56/64 loss: -0.049750685691833496
Batch 57/64 loss: -0.0450289249420166
Batch 58/64 loss: -0.04970282316207886
Batch 59/64 loss: -0.04439043998718262
Batch 60/64 loss: -0.043931663036346436
Batch 61/64 loss: -0.06095409393310547
Batch 62/64 loss: -0.042453110218048096
Batch 63/64 loss: -0.04805862903594971
Batch 64/64 loss: -0.020768702030181885
Epoch 437  Train loss: -0.045056160524779676  Val loss: 0.09770888192547146
Epoch 438
-------------------------------
Batch 1/64 loss: -0.04125028848648071
Batch 2/64 loss: -0.06037783622741699
Batch 3/64 loss: -0.04172849655151367
Batch 4/64 loss: -0.05455678701400757
Batch 5/64 loss: -0.04379218816757202
Batch 6/64 loss: -0.0497593879699707
Batch 7/64 loss: -0.04596853256225586
Batch 8/64 loss: -0.05947983264923096
Batch 9/64 loss: -0.05263704061508179
Batch 10/64 loss: -0.0426369309425354
Batch 11/64 loss: -0.05350381135940552
Batch 12/64 loss: -0.04272538423538208
Batch 13/64 loss: -0.037958621978759766
Batch 14/64 loss: -0.05310177803039551
Batch 15/64 loss: -0.034355998039245605
Batch 16/64 loss: -0.06362098455429077
Batch 17/64 loss: -0.054186105728149414
Batch 18/64 loss: -0.039005815982818604
Batch 19/64 loss: -0.048847854137420654
Batch 20/64 loss: -0.05618208646774292
Batch 21/64 loss: -0.04591524600982666
Batch 22/64 loss: -0.060472190380096436
Batch 23/64 loss: -0.05756819248199463
Batch 24/64 loss: -0.03165304660797119
Batch 25/64 loss: -0.047282397747039795
Batch 26/64 loss: -0.055371105670928955
Batch 27/64 loss: -0.051831066608428955
Batch 28/64 loss: -0.036103665828704834
Batch 29/64 loss: -0.022235214710235596
Batch 30/64 loss: -0.032818496227264404
Batch 31/64 loss: -0.03702270984649658
Batch 32/64 loss: -0.03976082801818848
Batch 33/64 loss: -0.05208832025527954
Batch 34/64 loss: -0.05129373073577881
Batch 35/64 loss: -0.0593111515045166
Batch 36/64 loss: -0.03275233507156372
Batch 37/64 loss: -0.058575332164764404
Batch 38/64 loss: -0.05283689498901367
Batch 39/64 loss: -0.051663756370544434
Batch 40/64 loss: -0.049547433853149414
Batch 41/64 loss: -0.05298209190368652
Batch 42/64 loss: -0.05524778366088867
Batch 43/64 loss: -0.038268446922302246
Batch 44/64 loss: -0.038223862648010254
Batch 45/64 loss: -0.05262935161590576
Batch 46/64 loss: -0.026934504508972168
Batch 47/64 loss: -0.04221141338348389
Batch 48/64 loss: -0.02845168113708496
Batch 49/64 loss: -0.04077523946762085
Batch 50/64 loss: -0.04439711570739746
Batch 51/64 loss: -0.02905350923538208
Batch 52/64 loss: -0.04763096570968628
Batch 53/64 loss: -0.05306822061538696
Batch 54/64 loss: -0.053645431995391846
Batch 55/64 loss: -0.05456352233886719
Batch 56/64 loss: -0.05512857437133789
Batch 57/64 loss: -0.03773045539855957
Batch 58/64 loss: -0.052484333515167236
Batch 59/64 loss: -0.05174058675765991
Batch 60/64 loss: -0.06304734945297241
Batch 61/64 loss: -0.02790975570678711
Batch 62/64 loss: -0.055034101009368896
Batch 63/64 loss: -0.0470125675201416
Batch 64/64 loss: -0.06844007968902588
Epoch 438  Train loss: -0.04707889884125953  Val loss: 0.0979156109065944
Epoch 439
-------------------------------
Batch 1/64 loss: -0.056616246700286865
Batch 2/64 loss: -0.0607297420501709
Batch 3/64 loss: -0.06324183940887451
Batch 4/64 loss: -0.0532035231590271
Batch 5/64 loss: -0.03155785799026489
Batch 6/64 loss: -0.0557633638381958
Batch 7/64 loss: -0.03810381889343262
Batch 8/64 loss: -0.04645109176635742
Batch 9/64 loss: -0.04504793882369995
Batch 10/64 loss: -0.06704938411712646
Batch 11/64 loss: -0.023716747760772705
Batch 12/64 loss: -0.0558929443359375
Batch 13/64 loss: -0.054592907428741455
Batch 14/64 loss: -0.04448467493057251
Batch 15/64 loss: -0.04729336500167847
Batch 16/64 loss: -0.048507511615753174
Batch 17/64 loss: -0.03024768829345703
Batch 18/64 loss: -0.036310672760009766
Batch 19/64 loss: -0.036773622035980225
Batch 20/64 loss: -0.036071598529815674
Batch 21/64 loss: -0.05697786808013916
Batch 22/64 loss: -0.052053868770599365
Batch 23/64 loss: -0.057598233222961426
Batch 24/64 loss: -0.04162687063217163
Batch 25/64 loss: -0.057541489601135254
Batch 26/64 loss: -0.034512221813201904
Batch 27/64 loss: -0.049063801765441895
Batch 28/64 loss: -0.05271810293197632
Batch 29/64 loss: -0.03394657373428345
Batch 30/64 loss: -0.050476133823394775
Batch 31/64 loss: -0.03914397954940796
Batch 32/64 loss: -0.0420377254486084
Batch 33/64 loss: -0.0561671257019043
Batch 34/64 loss: -0.06063646078109741
Batch 35/64 loss: -0.05381518602371216
Batch 36/64 loss: -0.05189168453216553
Batch 37/64 loss: -0.05570286512374878
Batch 38/64 loss: -0.01902693510055542
Batch 39/64 loss: -0.0589146614074707
Batch 40/64 loss: -0.035930335521698
Batch 41/64 loss: -0.024449169635772705
Batch 42/64 loss: -0.05560159683227539
Batch 43/64 loss: -0.03854775428771973
Batch 44/64 loss: -0.0558474063873291
Batch 45/64 loss: -0.04795992374420166
Batch 46/64 loss: -0.018947184085845947
Batch 47/64 loss: -0.05974394083023071
Batch 48/64 loss: -0.049126267433166504
Batch 49/64 loss: -0.06325995922088623
Batch 50/64 loss: -0.03774726390838623
Batch 51/64 loss: -0.05817079544067383
Batch 52/64 loss: -0.05835080146789551
Batch 53/64 loss: -0.054855406284332275
Batch 54/64 loss: -0.06570851802825928
Batch 55/64 loss: -0.04905271530151367
Batch 56/64 loss: -0.03159821033477783
Batch 57/64 loss: -0.05157780647277832
Batch 58/64 loss: -0.04628843069076538
Batch 59/64 loss: -0.04831838607788086
Batch 60/64 loss: -0.03326857089996338
Batch 61/64 loss: -0.034461259841918945
Batch 62/64 loss: -0.03915572166442871
Batch 63/64 loss: -0.03882342576980591
Batch 64/64 loss: -0.04522538185119629
Epoch 439  Train loss: -0.04684263865152995  Val loss: 0.09664415411932771
Epoch 440
-------------------------------
Batch 1/64 loss: -0.038009703159332275
Batch 2/64 loss: -0.04709964990615845
Batch 3/64 loss: -0.022821247577667236
Batch 4/64 loss: -0.037736713886260986
Batch 5/64 loss: -0.06907486915588379
Batch 6/64 loss: -0.0546497106552124
Batch 7/64 loss: -0.02852451801300049
Batch 8/64 loss: -0.020850956439971924
Batch 9/64 loss: -0.03241169452667236
Batch 10/64 loss: -0.01845252513885498
Batch 11/64 loss: -0.034304678440093994
Batch 12/64 loss: -0.060901641845703125
Batch 13/64 loss: -0.06475329399108887
Batch 14/64 loss: -0.0313035249710083
Batch 15/64 loss: -0.03126579523086548
Batch 16/64 loss: -0.037403404712677
Batch 17/64 loss: -0.059561312198638916
Batch 18/64 loss: -0.04734694957733154
Batch 19/64 loss: -0.036970317363739014
Batch 20/64 loss: -0.04809826612472534
Batch 21/64 loss: -0.047596275806427
Batch 22/64 loss: -0.06265866756439209
Batch 23/64 loss: -0.06103157997131348
Batch 24/64 loss: -0.0422404408454895
Batch 25/64 loss: -0.05809968709945679
Batch 26/64 loss: -0.04298388957977295
Batch 27/64 loss: -0.05808281898498535
Batch 28/64 loss: -0.04231685400009155
Batch 29/64 loss: -0.049386680126190186
Batch 30/64 loss: -0.05509132146835327
Batch 31/64 loss: -0.055501341819763184
Batch 32/64 loss: -0.05065351724624634
Batch 33/64 loss: -0.0443120002746582
Batch 34/64 loss: -0.012072861194610596
Batch 35/64 loss: -0.020276188850402832
Batch 36/64 loss: -0.05080997943878174
Batch 37/64 loss: -0.03936779499053955
Batch 38/64 loss: -0.04725611209869385
Batch 39/64 loss: -0.03969311714172363
Batch 40/64 loss: -0.038668930530548096
Batch 41/64 loss: -0.04221475124359131
Batch 42/64 loss: -0.05206429958343506
Batch 43/64 loss: -0.05510431528091431
Batch 44/64 loss: -0.033992648124694824
Batch 45/64 loss: -0.03740715980529785
Batch 46/64 loss: -0.054144442081451416
Batch 47/64 loss: -0.03954720497131348
Batch 48/64 loss: -0.0409923791885376
Batch 49/64 loss: -0.037404656410217285
Batch 50/64 loss: -0.03580087423324585
Batch 51/64 loss: -0.04146534204483032
Batch 52/64 loss: -0.04229933023452759
Batch 53/64 loss: -0.04557579755783081
Batch 54/64 loss: -0.045984506607055664
Batch 55/64 loss: -0.04805433750152588
Batch 56/64 loss: -0.042411625385284424
Batch 57/64 loss: -0.03345006704330444
Batch 58/64 loss: -0.027024507522583008
Batch 59/64 loss: -0.04934501647949219
Batch 60/64 loss: -0.038294851779937744
Batch 61/64 loss: -0.03090989589691162
Batch 62/64 loss: -0.03453707695007324
Batch 63/64 loss: -0.050626933574676514
Batch 64/64 loss: -0.031971275806427
Epoch 440  Train loss: -0.04270229503220203  Val loss: 0.09874482970057484
Epoch 441
-------------------------------
Batch 1/64 loss: -0.04376429319381714
Batch 2/64 loss: -0.02200937271118164
Batch 3/64 loss: -0.04988962411880493
Batch 4/64 loss: -0.048988938331604004
Batch 5/64 loss: -0.04244464635848999
Batch 6/64 loss: -0.04079216718673706
Batch 7/64 loss: -0.03528684377670288
Batch 8/64 loss: -0.040656983852386475
Batch 9/64 loss: -0.043676137924194336
Batch 10/64 loss: -0.038710951805114746
Batch 11/64 loss: -0.030280232429504395
Batch 12/64 loss: -0.03360241651535034
Batch 13/64 loss: -0.05905652046203613
Batch 14/64 loss: -0.038987159729003906
Batch 15/64 loss: -0.03513467311859131
Batch 16/64 loss: -0.049951791763305664
Batch 17/64 loss: -0.06341385841369629
Batch 18/64 loss: -0.04440063238143921
Batch 19/64 loss: -0.05112254619598389
Batch 20/64 loss: -0.03280836343765259
Batch 21/64 loss: -0.05745333433151245
Batch 22/64 loss: -0.055053770542144775
Batch 23/64 loss: -0.048104703426361084
Batch 24/64 loss: -0.06012427806854248
Batch 25/64 loss: -0.030736982822418213
Batch 26/64 loss: -0.022198140621185303
Batch 27/64 loss: -0.024805665016174316
Batch 28/64 loss: -0.060504794120788574
Batch 29/64 loss: -0.05557072162628174
Batch 30/64 loss: -0.061655402183532715
Batch 31/64 loss: -0.04798483848571777
Batch 32/64 loss: -0.0610119104385376
Batch 33/64 loss: -0.0560002326965332
Batch 34/64 loss: -0.036865174770355225
Batch 35/64 loss: -0.0492134690284729
Batch 36/64 loss: -0.06104779243469238
Batch 37/64 loss: -0.05196356773376465
Batch 38/64 loss: -0.04414474964141846
Batch 39/64 loss: -0.04012417793273926
Batch 40/64 loss: -0.04070544242858887
Batch 41/64 loss: -0.03373080492019653
Batch 42/64 loss: -0.01618516445159912
Batch 43/64 loss: -0.028824150562286377
Batch 44/64 loss: -0.04973781108856201
Batch 45/64 loss: -0.05018198490142822
Batch 46/64 loss: -0.03545808792114258
Batch 47/64 loss: -0.04692882299423218
Batch 48/64 loss: -0.04519641399383545
Batch 49/64 loss: -0.03640717267990112
Batch 50/64 loss: -0.052705228328704834
Batch 51/64 loss: -0.051003098487854004
Batch 52/64 loss: -0.041951537132263184
Batch 53/64 loss: -0.05295437574386597
Batch 54/64 loss: -0.044388771057128906
Batch 55/64 loss: -0.06439846754074097
Batch 56/64 loss: -0.05867409706115723
Batch 57/64 loss: -0.04310709238052368
Batch 58/64 loss: -0.043293118476867676
Batch 59/64 loss: -0.03441166877746582
Batch 60/64 loss: -0.057656288146972656
Batch 61/64 loss: -0.02221059799194336
Batch 62/64 loss: -0.03489804267883301
Batch 63/64 loss: -0.041674017906188965
Batch 64/64 loss: -0.04023456573486328
Epoch 441  Train loss: -0.04433568692674824  Val loss: 0.09807605616415489
Epoch 442
-------------------------------
Batch 1/64 loss: -0.048673272132873535
Batch 2/64 loss: -0.0625564455986023
Batch 3/64 loss: -0.05267989635467529
Batch 4/64 loss: -0.039134442806243896
Batch 5/64 loss: -0.04328477382659912
Batch 6/64 loss: -0.06206578016281128
Batch 7/64 loss: -0.038643717765808105
Batch 8/64 loss: -0.06616944074630737
Batch 9/64 loss: -0.04675281047821045
Batch 10/64 loss: -0.054792582988739014
Batch 11/64 loss: -0.0561833381652832
Batch 12/64 loss: -0.06250041723251343
Batch 13/64 loss: -0.057299792766571045
Batch 14/64 loss: -0.04806649684906006
Batch 15/64 loss: -0.05510544776916504
Batch 16/64 loss: -0.05923044681549072
Batch 17/64 loss: -0.05093955993652344
Batch 18/64 loss: -0.06323975324630737
Batch 19/64 loss: -0.0392913818359375
Batch 20/64 loss: -0.050972819328308105
Batch 21/64 loss: -0.05758333206176758
Batch 22/64 loss: -0.05202513933181763
Batch 23/64 loss: -0.03454422950744629
Batch 24/64 loss: -0.049927353858947754
Batch 25/64 loss: -0.05341184139251709
Batch 26/64 loss: -0.05672872066497803
Batch 27/64 loss: -0.05335724353790283
Batch 28/64 loss: -0.04780995845794678
Batch 29/64 loss: -0.04146683216094971
Batch 30/64 loss: -0.04329836368560791
Batch 31/64 loss: -0.011961638927459717
Batch 32/64 loss: -0.05750948190689087
Batch 33/64 loss: -0.04922020435333252
Batch 34/64 loss: -0.044966816902160645
Batch 35/64 loss: -0.04036509990692139
Batch 36/64 loss: -0.04308539628982544
Batch 37/64 loss: -0.027073144912719727
Batch 38/64 loss: -0.048676490783691406
Batch 39/64 loss: -0.03889673948287964
Batch 40/64 loss: -0.03813141584396362
Batch 41/64 loss: -0.05616492033004761
Batch 42/64 loss: -0.043512940406799316
Batch 43/64 loss: -0.027916550636291504
Batch 44/64 loss: -0.05599784851074219
Batch 45/64 loss: -0.036510348320007324
Batch 46/64 loss: -0.038288414478302
Batch 47/64 loss: -0.05969882011413574
Batch 48/64 loss: -0.04710620641708374
Batch 49/64 loss: -0.06528007984161377
Batch 50/64 loss: -0.02976149320602417
Batch 51/64 loss: -0.03915095329284668
Batch 52/64 loss: -0.039227962493896484
Batch 53/64 loss: -0.03754234313964844
Batch 54/64 loss: -0.03152132034301758
Batch 55/64 loss: -0.035892486572265625
Batch 56/64 loss: -0.043451130390167236
Batch 57/64 loss: -0.041446566581726074
Batch 58/64 loss: -0.03996694087982178
Batch 59/64 loss: -0.05138552188873291
Batch 60/64 loss: -0.04264569282531738
Batch 61/64 loss: -0.02258533239364624
Batch 62/64 loss: -0.04531657695770264
Batch 63/64 loss: -0.03321361541748047
Batch 64/64 loss: -0.013986051082611084
Epoch 442  Train loss: -0.04583052022784364  Val loss: 0.09914584237685319
Epoch 443
-------------------------------
Batch 1/64 loss: -0.04020047187805176
Batch 2/64 loss: -0.04800248146057129
Batch 3/64 loss: -0.047394633293151855
Batch 4/64 loss: -0.05388522148132324
Batch 5/64 loss: -0.04263961315155029
Batch 6/64 loss: -0.02309262752532959
Batch 7/64 loss: -0.05131751298904419
Batch 8/64 loss: -0.07368236780166626
Batch 9/64 loss: -0.04532003402709961
Batch 10/64 loss: -0.0602419376373291
Batch 11/64 loss: -0.04218107461929321
Batch 12/64 loss: -0.045545756816864014
Batch 13/64 loss: -0.04970860481262207
Batch 14/64 loss: -0.04426681995391846
Batch 15/64 loss: -0.06673973798751831
Batch 16/64 loss: -0.05091547966003418
Batch 17/64 loss: -0.0604436993598938
Batch 18/64 loss: -0.04418891668319702
Batch 19/64 loss: -0.04795265197753906
Batch 20/64 loss: -0.040427982807159424
Batch 21/64 loss: -0.04492229223251343
Batch 22/64 loss: -0.05523574352264404
Batch 23/64 loss: -0.05152863264083862
Batch 24/64 loss: -0.04771745204925537
Batch 25/64 loss: -0.035550832748413086
Batch 26/64 loss: -0.02765178680419922
Batch 27/64 loss: -0.05220097303390503
Batch 28/64 loss: -0.04108285903930664
Batch 29/64 loss: -0.046544790267944336
Batch 30/64 loss: -0.03475844860076904
Batch 31/64 loss: -0.06403446197509766
Batch 32/64 loss: -0.05050688982009888
Batch 33/64 loss: -0.05042529106140137
Batch 34/64 loss: -0.04591643810272217
Batch 35/64 loss: -0.03510463237762451
Batch 36/64 loss: -0.06842619180679321
Batch 37/64 loss: -0.04968571662902832
Batch 38/64 loss: -0.06115990877151489
Batch 39/64 loss: -0.048807621002197266
Batch 40/64 loss: -0.046939969062805176
Batch 41/64 loss: -0.06302332878112793
Batch 42/64 loss: -0.03773236274719238
Batch 43/64 loss: -0.05662965774536133
Batch 44/64 loss: -0.046369194984436035
Batch 45/64 loss: -0.050790488719940186
Batch 46/64 loss: -0.05080902576446533
Batch 47/64 loss: -0.04539918899536133
Batch 48/64 loss: -0.02979755401611328
Batch 49/64 loss: -0.04553532600402832
Batch 50/64 loss: -0.05603039264678955
Batch 51/64 loss: -0.037837207317352295
Batch 52/64 loss: -0.05763816833496094
Batch 53/64 loss: -0.060407936573028564
Batch 54/64 loss: -0.04894816875457764
Batch 55/64 loss: -0.05737602710723877
Batch 56/64 loss: -0.033551692962646484
Batch 57/64 loss: -0.05154919624328613
Batch 58/64 loss: -0.03937655687332153
Batch 59/64 loss: -0.039760053157806396
Batch 60/64 loss: -0.06756281852722168
Batch 61/64 loss: -0.05676370859146118
Batch 62/64 loss: -0.04482918977737427
Batch 63/64 loss: -0.053720057010650635
Batch 64/64 loss: -0.04698663949966431
Epoch 443  Train loss: -0.04867491511737599  Val loss: 0.09908002328217234
Epoch 444
-------------------------------
Batch 1/64 loss: -0.05882704257965088
Batch 2/64 loss: -0.05594038963317871
Batch 3/64 loss: -0.041110217571258545
Batch 4/64 loss: -0.03678685426712036
Batch 5/64 loss: -0.04778337478637695
Batch 6/64 loss: -0.054043710231781006
Batch 7/64 loss: -0.06707394123077393
Batch 8/64 loss: -0.05805337429046631
Batch 9/64 loss: -0.05067896842956543
Batch 10/64 loss: -0.058587849140167236
Batch 11/64 loss: -0.05224490165710449
Batch 12/64 loss: -0.03664076328277588
Batch 13/64 loss: -0.04539692401885986
Batch 14/64 loss: -0.049480974674224854
Batch 15/64 loss: -0.06454956531524658
Batch 16/64 loss: -0.051227688789367676
Batch 17/64 loss: -0.04208105802536011
Batch 18/64 loss: -0.05367368459701538
Batch 19/64 loss: -0.05781739950180054
Batch 20/64 loss: -0.03918856382369995
Batch 21/64 loss: -0.04806661605834961
Batch 22/64 loss: -0.049974918365478516
Batch 23/64 loss: -0.04240339994430542
Batch 24/64 loss: -0.04068422317504883
Batch 25/64 loss: -0.05120640993118286
Batch 26/64 loss: -0.055474162101745605
Batch 27/64 loss: -0.05576735734939575
Batch 28/64 loss: -0.04839414358139038
Batch 29/64 loss: -0.034234583377838135
Batch 30/64 loss: -0.021572589874267578
Batch 31/64 loss: -0.040808796882629395
Batch 32/64 loss: -0.045456886291503906
Batch 33/64 loss: -0.03717637062072754
Batch 34/64 loss: -0.030172109603881836
Batch 35/64 loss: -0.03875249624252319
Batch 36/64 loss: -0.0400465726852417
Batch 37/64 loss: -0.046471595764160156
Batch 38/64 loss: -0.04163855314254761
Batch 39/64 loss: -0.03783464431762695
Batch 40/64 loss: -0.051844894886016846
Batch 41/64 loss: -0.0599057674407959
Batch 42/64 loss: -0.052592337131500244
Batch 43/64 loss: -0.0466465950012207
Batch 44/64 loss: -0.0431668758392334
Batch 45/64 loss: -0.0631328821182251
Batch 46/64 loss: -0.048708438873291016
Batch 47/64 loss: -0.025362133979797363
Batch 48/64 loss: -0.036179423332214355
Batch 49/64 loss: -0.05358731746673584
Batch 50/64 loss: -0.04680657386779785
Batch 51/64 loss: -0.04732447862625122
Batch 52/64 loss: -0.052214622497558594
Batch 53/64 loss: -0.0550956130027771
Batch 54/64 loss: -0.03715479373931885
Batch 55/64 loss: -0.031013131141662598
Batch 56/64 loss: -0.03799480199813843
Batch 57/64 loss: -0.04568451642990112
Batch 58/64 loss: -0.0198746919631958
Batch 59/64 loss: -0.045600950717926025
Batch 60/64 loss: -0.05444622039794922
Batch 61/64 loss: -0.033430397510528564
Batch 62/64 loss: -0.05180048942565918
Batch 63/64 loss: -0.03334850072860718
Batch 64/64 loss: -0.04688173532485962
Epoch 444  Train loss: -0.04604551254534254  Val loss: 0.09742413148847233
Epoch 445
-------------------------------
Batch 1/64 loss: -0.05818378925323486
Batch 2/64 loss: -0.059875428676605225
Batch 3/64 loss: -0.040972352027893066
Batch 4/64 loss: -0.06077486276626587
Batch 5/64 loss: -0.06077539920806885
Batch 6/64 loss: -0.03837382793426514
Batch 7/64 loss: -0.04379373788833618
Batch 8/64 loss: -0.047159552574157715
Batch 9/64 loss: -0.057085633277893066
Batch 10/64 loss: -0.041182100772857666
Batch 11/64 loss: -0.05647224187850952
Batch 12/64 loss: -0.03554415702819824
Batch 13/64 loss: -0.04919940233230591
Batch 14/64 loss: -0.04215282201766968
Batch 15/64 loss: -0.03147280216217041
Batch 16/64 loss: -0.045919954776763916
Batch 17/64 loss: -0.056907832622528076
Batch 18/64 loss: -0.03235745429992676
Batch 19/64 loss: -0.05482137203216553
Batch 20/64 loss: -0.05627799034118652
Batch 21/64 loss: -0.030520141124725342
Batch 22/64 loss: -0.04536867141723633
Batch 23/64 loss: -0.06458306312561035
Batch 24/64 loss: -0.056899845600128174
Batch 25/64 loss: -0.028297603130340576
Batch 26/64 loss: -0.05800747871398926
Batch 27/64 loss: -0.034860849380493164
Batch 28/64 loss: -0.02102106809616089
Batch 29/64 loss: -0.05534476041793823
Batch 30/64 loss: -0.06223231554031372
Batch 31/64 loss: -0.053432345390319824
Batch 32/64 loss: -0.04641568660736084
Batch 33/64 loss: -0.050908565521240234
Batch 34/64 loss: -0.05233919620513916
Batch 35/64 loss: -0.026515483856201172
Batch 36/64 loss: -0.0544283390045166
Batch 37/64 loss: -0.02109968662261963
Batch 38/64 loss: -0.050671935081481934
Batch 39/64 loss: -0.06636166572570801
Batch 40/64 loss: -0.04306471347808838
Batch 41/64 loss: -0.04533219337463379
Batch 42/64 loss: -0.04527425765991211
Batch 43/64 loss: -0.05243182182312012
Batch 44/64 loss: -0.05435389280319214
Batch 45/64 loss: -0.06139028072357178
Batch 46/64 loss: -0.03104257583618164
Batch 47/64 loss: -0.020414113998413086
Batch 48/64 loss: -0.057363271713256836
Batch 49/64 loss: -0.053000450134277344
Batch 50/64 loss: -0.04130524396896362
Batch 51/64 loss: -0.06210005283355713
Batch 52/64 loss: -0.041872262954711914
Batch 53/64 loss: -0.05450177192687988
Batch 54/64 loss: -0.025719165802001953
Batch 55/64 loss: -0.05044662952423096
Batch 56/64 loss: -0.05467057228088379
Batch 57/64 loss: -0.04822254180908203
Batch 58/64 loss: -0.03290867805480957
Batch 59/64 loss: -0.052939414978027344
Batch 60/64 loss: -0.04940056800842285
Batch 61/64 loss: -0.012368321418762207
Batch 62/64 loss: -0.05768018960952759
Batch 63/64 loss: -0.041146934032440186
Batch 64/64 loss: -0.062263548374176025
Epoch 445  Train loss: -0.04681190579545264  Val loss: 0.09764314066503466
Epoch 446
-------------------------------
Batch 1/64 loss: -0.04389321804046631
Batch 2/64 loss: -0.05877697467803955
Batch 3/64 loss: -0.05538874864578247
Batch 4/64 loss: -0.05049550533294678
Batch 5/64 loss: -0.038780391216278076
Batch 6/64 loss: -0.055096209049224854
Batch 7/64 loss: -0.06624341011047363
Batch 8/64 loss: -0.0521431565284729
Batch 9/64 loss: -0.05290043354034424
Batch 10/64 loss: -0.06625765562057495
Batch 11/64 loss: -0.048624277114868164
Batch 12/64 loss: -0.05067342519760132
Batch 13/64 loss: -0.029125511646270752
Batch 14/64 loss: -0.06441903114318848
Batch 15/64 loss: -0.07589185237884521
Batch 16/64 loss: -0.05471014976501465
Batch 17/64 loss: -0.057364463806152344
Batch 18/64 loss: -0.06078755855560303
Batch 19/64 loss: -0.04633688926696777
Batch 20/64 loss: -0.047037363052368164
Batch 21/64 loss: -0.05235731601715088
Batch 22/64 loss: -0.0539669394493103
Batch 23/64 loss: -0.047320008277893066
Batch 24/64 loss: -0.0315399169921875
Batch 25/64 loss: -0.06362932920455933
Batch 26/64 loss: -0.037857651710510254
Batch 27/64 loss: -0.03860074281692505
Batch 28/64 loss: -0.04777193069458008
Batch 29/64 loss: -0.06002402305603027
Batch 30/64 loss: -0.0516124963760376
Batch 31/64 loss: -0.04553711414337158
Batch 32/64 loss: -0.04510307312011719
Batch 33/64 loss: -0.041684508323669434
Batch 34/64 loss: -0.05745208263397217
Batch 35/64 loss: -0.05222874879837036
Batch 36/64 loss: -0.04430359601974487
Batch 37/64 loss: -0.019047081470489502
Batch 38/64 loss: -0.023288607597351074
Batch 39/64 loss: -0.05255484580993652
Batch 40/64 loss: -0.05243396759033203
Batch 41/64 loss: -0.05491924285888672
Batch 42/64 loss: -0.051342546939849854
Batch 43/64 loss: -0.03988415002822876
Batch 44/64 loss: -0.05115616321563721
Batch 45/64 loss: -0.04212766885757446
Batch 46/64 loss: -0.03928506374359131
Batch 47/64 loss: -0.04910773038864136
Batch 48/64 loss: -0.03230088949203491
Batch 49/64 loss: -0.058109819889068604
Batch 50/64 loss: -0.04096883535385132
Batch 51/64 loss: -0.023313581943511963
Batch 52/64 loss: -0.05002403259277344
Batch 53/64 loss: -0.0402684211730957
Batch 54/64 loss: -0.055252909660339355
Batch 55/64 loss: -0.0294264554977417
Batch 56/64 loss: -0.05516195297241211
Batch 57/64 loss: -0.06767147779464722
Batch 58/64 loss: -0.048006653785705566
Batch 59/64 loss: -0.05608755350112915
Batch 60/64 loss: -0.040308594703674316
Batch 61/64 loss: -0.06116008758544922
Batch 62/64 loss: -0.06432801485061646
Batch 63/64 loss: -0.043074846267700195
Batch 64/64 loss: -0.042742908000946045
Epoch 446  Train loss: -0.048919279668845385  Val loss: 0.09960301512295437
Epoch 447
-------------------------------
Batch 1/64 loss: -0.045982301235198975
Batch 2/64 loss: -0.06290924549102783
Batch 3/64 loss: -0.051443636417388916
Batch 4/64 loss: -0.05457127094268799
Batch 5/64 loss: -0.06997150182723999
Batch 6/64 loss: -0.0416717529296875
Batch 7/64 loss: -0.060727059841156006
Batch 8/64 loss: -0.053054630756378174
Batch 9/64 loss: -0.07634937763214111
Batch 10/64 loss: -0.0297623872756958
Batch 11/64 loss: -0.062245070934295654
Batch 12/64 loss: -0.05460655689239502
Batch 13/64 loss: -0.06214761734008789
Batch 14/64 loss: -0.05882573127746582
Batch 15/64 loss: -0.0495184063911438
Batch 16/64 loss: -0.04839897155761719
Batch 17/64 loss: -0.05780142545700073
Batch 18/64 loss: -0.04673105478286743
Batch 19/64 loss: -0.033227503299713135
Batch 20/64 loss: -0.06354296207427979
Batch 21/64 loss: -0.05599701404571533
Batch 22/64 loss: -0.06319940090179443
Batch 23/64 loss: -0.06923156976699829
Batch 24/64 loss: -0.04853510856628418
Batch 25/64 loss: -0.0413743257522583
Batch 26/64 loss: -0.05433386564254761
Batch 27/64 loss: -0.04983222484588623
Batch 28/64 loss: -0.05223369598388672
Batch 29/64 loss: -0.06148296594619751
Batch 30/64 loss: -0.061021506786346436
Batch 31/64 loss: -0.04359930753707886
Batch 32/64 loss: -0.051006019115448
Batch 33/64 loss: -0.047095656394958496
Batch 34/64 loss: -0.06062006950378418
Batch 35/64 loss: -0.04581773281097412
Batch 36/64 loss: -0.04148399829864502
Batch 37/64 loss: -0.051155924797058105
Batch 38/64 loss: -0.03613358736038208
Batch 39/64 loss: -0.048468947410583496
Batch 40/64 loss: -0.06967955827713013
Batch 41/64 loss: -0.05170708894729614
Batch 42/64 loss: -0.048875510692596436
Batch 43/64 loss: -0.05764460563659668
Batch 44/64 loss: -0.04788362979888916
Batch 45/64 loss: -0.04084575176239014
Batch 46/64 loss: -0.05145907402038574
Batch 47/64 loss: -0.03902578353881836
Batch 48/64 loss: -0.04439789056777954
Batch 49/64 loss: -0.05783194303512573
Batch 50/64 loss: -0.057897329330444336
Batch 51/64 loss: -0.057317376136779785
Batch 52/64 loss: -0.032444894313812256
Batch 53/64 loss: -0.04022037982940674
Batch 54/64 loss: -0.05028039216995239
Batch 55/64 loss: -0.05908989906311035
Batch 56/64 loss: -0.03743499517440796
Batch 57/64 loss: -0.04840350151062012
Batch 58/64 loss: -0.05438333749771118
Batch 59/64 loss: -0.036129772663116455
Batch 60/64 loss: -0.04294031858444214
Batch 61/64 loss: -0.04995012283325195
Batch 62/64 loss: -0.04316055774688721
Batch 63/64 loss: -0.004274904727935791
Batch 64/64 loss: -0.051747262477874756
Epoch 447  Train loss: -0.05063843843983669  Val loss: 0.0979521856275211
Epoch 448
-------------------------------
Batch 1/64 loss: -0.053689658641815186
Batch 2/64 loss: -0.058490633964538574
Batch 3/64 loss: -0.03285342454910278
Batch 4/64 loss: -0.04033905267715454
Batch 5/64 loss: -0.06483089923858643
Batch 6/64 loss: -0.06541532278060913
Batch 7/64 loss: -0.05630683898925781
Batch 8/64 loss: -0.037872493267059326
Batch 9/64 loss: -0.05707907676696777
Batch 10/64 loss: -0.052495598793029785
Batch 11/64 loss: -0.037616729736328125
Batch 12/64 loss: -0.05190926790237427
Batch 13/64 loss: -0.05842667818069458
Batch 14/64 loss: -0.05830138921737671
Batch 15/64 loss: -0.054978370666503906
Batch 16/64 loss: -0.05952697992324829
Batch 17/64 loss: -0.04607725143432617
Batch 18/64 loss: -0.03522229194641113
Batch 19/64 loss: -0.054855525493621826
Batch 20/64 loss: -0.06224346160888672
Batch 21/64 loss: -0.06147259473800659
Batch 22/64 loss: -0.03944265842437744
Batch 23/64 loss: -0.06287503242492676
Batch 24/64 loss: -0.05689799785614014
Batch 25/64 loss: -0.07378047704696655
Batch 26/64 loss: -0.030766308307647705
Batch 27/64 loss: -0.06423401832580566
Batch 28/64 loss: -0.047212064266204834
Batch 29/64 loss: -0.06965786218643188
Batch 30/64 loss: -0.04937171936035156
Batch 31/64 loss: -0.06587421894073486
Batch 32/64 loss: -0.057035088539123535
Batch 33/64 loss: -0.05142688751220703
Batch 34/64 loss: -0.037365734577178955
Batch 35/64 loss: -0.07135426998138428
Batch 36/64 loss: -0.04854708909988403
Batch 37/64 loss: -0.05628925561904907
Batch 38/64 loss: -0.06291782855987549
Batch 39/64 loss: -0.0497664213180542
Batch 40/64 loss: -0.05933356285095215
Batch 41/64 loss: -0.06229370832443237
Batch 42/64 loss: -0.05395609140396118
Batch 43/64 loss: -0.049142301082611084
Batch 44/64 loss: -0.032955050468444824
Batch 45/64 loss: -0.051560163497924805
Batch 46/64 loss: -0.06242650747299194
Batch 47/64 loss: -0.04281729459762573
Batch 48/64 loss: -0.044861435890197754
Batch 49/64 loss: -0.0310249924659729
Batch 50/64 loss: -0.06527507305145264
Batch 51/64 loss: -0.05088144540786743
Batch 52/64 loss: -0.05235403776168823
Batch 53/64 loss: -0.028586924076080322
Batch 54/64 loss: -0.05547940731048584
Batch 55/64 loss: -0.038287341594696045
Batch 56/64 loss: -0.05867433547973633
Batch 57/64 loss: -0.04955899715423584
Batch 58/64 loss: -0.025083541870117188
Batch 59/64 loss: -0.032810330390930176
Batch 60/64 loss: -0.038925349712371826
Batch 61/64 loss: -0.05581122636795044
Batch 62/64 loss: -0.030126750469207764
Batch 63/64 loss: -0.04623913764953613
Batch 64/64 loss: -0.017998218536376953
Epoch 448  Train loss: -0.05061619048025094  Val loss: 0.09826064396559987
Epoch 449
-------------------------------
Batch 1/64 loss: -0.07178246974945068
Batch 2/64 loss: -0.06018620729446411
Batch 3/64 loss: -0.03850299119949341
Batch 4/64 loss: -0.04552638530731201
Batch 5/64 loss: -0.04375112056732178
Batch 6/64 loss: -0.03402531147003174
Batch 7/64 loss: -0.030404090881347656
Batch 8/64 loss: -0.043149352073669434
Batch 9/64 loss: -0.04933100938796997
Batch 10/64 loss: -0.0464167594909668
Batch 11/64 loss: -0.03469705581665039
Batch 12/64 loss: -0.06987303495407104
Batch 13/64 loss: -0.035536348819732666
Batch 14/64 loss: -0.04983067512512207
Batch 15/64 loss: -0.04366642236709595
Batch 16/64 loss: -0.0525779128074646
Batch 17/64 loss: -0.058866262435913086
Batch 18/64 loss: -0.05268204212188721
Batch 19/64 loss: -0.06637167930603027
Batch 20/64 loss: -0.02893298864364624
Batch 21/64 loss: -0.04633378982543945
Batch 22/64 loss: -0.04451704025268555
Batch 23/64 loss: -0.04575377702713013
Batch 24/64 loss: -0.04615539312362671
Batch 25/64 loss: -0.0493127703666687
Batch 26/64 loss: -0.043633103370666504
Batch 27/64 loss: -0.04604542255401611
Batch 28/64 loss: -0.03834265470504761
Batch 29/64 loss: -0.04272305965423584
Batch 30/64 loss: -0.04078632593154907
Batch 31/64 loss: -0.04293680191040039
Batch 32/64 loss: -0.05198311805725098
Batch 33/64 loss: -0.05887150764465332
Batch 34/64 loss: -0.027796149253845215
Batch 35/64 loss: -0.03030550479888916
Batch 36/64 loss: -0.06291085481643677
Batch 37/64 loss: -0.03689122200012207
Batch 38/64 loss: -0.03531903028488159
Batch 39/64 loss: -0.03601408004760742
Batch 40/64 loss: -0.040239691734313965
Batch 41/64 loss: -0.06243133544921875
Batch 42/64 loss: -0.053000807762145996
Batch 43/64 loss: -0.06097990274429321
Batch 44/64 loss: -0.051837921142578125
Batch 45/64 loss: -0.042921245098114014
Batch 46/64 loss: -0.042348265647888184
Batch 47/64 loss: -0.0381128191947937
Batch 48/64 loss: -0.046440839767456055
Batch 49/64 loss: -0.06053096055984497
Batch 50/64 loss: -0.05329883098602295
Batch 51/64 loss: -0.05691540241241455
Batch 52/64 loss: -0.0571436882019043
Batch 53/64 loss: -0.051701366901397705
Batch 54/64 loss: -0.04985237121582031
Batch 55/64 loss: -0.05903208255767822
Batch 56/64 loss: -0.061049699783325195
Batch 57/64 loss: -0.030608057975769043
Batch 58/64 loss: -0.05138969421386719
Batch 59/64 loss: -0.05087769031524658
Batch 60/64 loss: -0.06696414947509766
Batch 61/64 loss: -0.043459415435791016
Batch 62/64 loss: -0.06361669301986694
Batch 63/64 loss: -0.05210024118423462
Batch 64/64 loss: -0.05590212345123291
Epoch 449  Train loss: -0.04818072926764395  Val loss: 0.09755673195488264
Epoch 450
-------------------------------
Batch 1/64 loss: -0.0657198429107666
Batch 2/64 loss: -0.05463862419128418
Batch 3/64 loss: -0.05107557773590088
Batch 4/64 loss: -0.050395071506500244
Batch 5/64 loss: -0.04326730966567993
Batch 6/64 loss: -0.04874527454376221
Batch 7/64 loss: -0.06252115964889526
Batch 8/64 loss: -0.06653422117233276
Batch 9/64 loss: -0.0696725845336914
Batch 10/64 loss: -0.06738841533660889
Batch 11/64 loss: -0.07233595848083496
Batch 12/64 loss: -0.053911030292510986
Batch 13/64 loss: -0.05402946472167969
Batch 14/64 loss: -0.05794954299926758
Batch 15/64 loss: -0.05871373414993286
Batch 16/64 loss: -0.04360306262969971
Batch 17/64 loss: -0.058328986167907715
Batch 18/64 loss: -0.03600996732711792
Batch 19/64 loss: -0.04065680503845215
Batch 20/64 loss: -0.012030601501464844
Batch 21/64 loss: -0.06747972965240479
Batch 22/64 loss: -0.05470973253250122
Batch 23/64 loss: -0.03481245040893555
Batch 24/64 loss: -0.03939199447631836
Batch 25/64 loss: -0.051043033599853516
Batch 26/64 loss: -0.05949896574020386
Batch 27/64 loss: -0.03851896524429321
Batch 28/64 loss: -0.05506598949432373
Batch 29/64 loss: -0.04466652870178223
Batch 30/64 loss: -0.00574946403503418
Batch 31/64 loss: -0.05027592182159424
Batch 32/64 loss: -0.038042426109313965
Batch 33/64 loss: -0.04512852430343628
Batch 34/64 loss: -0.03702843189239502
Batch 35/64 loss: -0.07687383890151978
Batch 36/64 loss: -0.06322437524795532
Batch 37/64 loss: -0.047042250633239746
Batch 38/64 loss: -0.05921512842178345
Batch 39/64 loss: -0.047143757343292236
Batch 40/64 loss: -0.049840569496154785
Batch 41/64 loss: -0.05649751424789429
Batch 42/64 loss: -0.0488855242729187
Batch 43/64 loss: -0.05847287178039551
Batch 44/64 loss: -0.07128381729125977
Batch 45/64 loss: -0.038712501525878906
Batch 46/64 loss: -0.058299124240875244
Batch 47/64 loss: -0.05567610263824463
Batch 48/64 loss: -0.05888313055038452
Batch 49/64 loss: -0.05506277084350586
Batch 50/64 loss: -0.05704808235168457
Batch 51/64 loss: -0.05372023582458496
Batch 52/64 loss: -0.047527015209198
Batch 53/64 loss: -0.03477144241333008
Batch 54/64 loss: -0.035639941692352295
Batch 55/64 loss: -0.03956383466720581
Batch 56/64 loss: -0.0434916615486145
Batch 57/64 loss: -0.05803638696670532
Batch 58/64 loss: -0.04964947700500488
Batch 59/64 loss: -0.04911541938781738
Batch 60/64 loss: -0.05413711071014404
Batch 61/64 loss: -0.054520487785339355
Batch 62/64 loss: -0.046391308307647705
Batch 63/64 loss: -0.03204631805419922
Batch 64/64 loss: -0.05508017539978027
Epoch 450  Train loss: -0.050682690564323875  Val loss: 0.09834313617948814
Epoch 451
-------------------------------
Batch 1/64 loss: -0.04926532506942749
Batch 2/64 loss: -0.03582733869552612
Batch 3/64 loss: -0.05652588605880737
Batch 4/64 loss: -0.037358641624450684
Batch 5/64 loss: -0.06829267740249634
Batch 6/64 loss: -0.07045942544937134
Batch 7/64 loss: -0.06238633394241333
Batch 8/64 loss: -0.03850555419921875
Batch 9/64 loss: -0.04761767387390137
Batch 10/64 loss: -0.039115309715270996
Batch 11/64 loss: -0.05351918935775757
Batch 12/64 loss: -0.04231637716293335
Batch 13/64 loss: -0.05778294801712036
Batch 14/64 loss: -0.044406771659851074
Batch 15/64 loss: -0.056837618350982666
Batch 16/64 loss: -0.038752079010009766
Batch 17/64 loss: -0.06931531429290771
Batch 18/64 loss: -0.05762636661529541
Batch 19/64 loss: -0.03796201944351196
Batch 20/64 loss: -0.059865593910217285
Batch 21/64 loss: -0.0406990647315979
Batch 22/64 loss: -0.047004878520965576
Batch 23/64 loss: -0.04967755079269409
Batch 24/64 loss: -0.04444319009780884
Batch 25/64 loss: -0.05228227376937866
Batch 26/64 loss: -0.024002373218536377
Batch 27/64 loss: -0.041436076164245605
Batch 28/64 loss: -0.062162160873413086
Batch 29/64 loss: -0.02362072467803955
Batch 30/64 loss: -0.04561799764633179
Batch 31/64 loss: -0.05794137716293335
Batch 32/64 loss: -0.03432661294937134
Batch 33/64 loss: -0.0520704984664917
Batch 34/64 loss: -0.04106992483139038
Batch 35/64 loss: -0.05111396312713623
Batch 36/64 loss: -0.029108881950378418
Batch 37/64 loss: -0.03560590744018555
Batch 38/64 loss: -0.0454714298248291
Batch 39/64 loss: -0.05709576606750488
Batch 40/64 loss: -0.05918467044830322
Batch 41/64 loss: -0.05845761299133301
Batch 42/64 loss: -0.05406045913696289
Batch 43/64 loss: -0.05411398410797119
Batch 44/64 loss: -0.056365132331848145
Batch 45/64 loss: -0.07058870792388916
Batch 46/64 loss: -0.04536217451095581
Batch 47/64 loss: -0.035921692848205566
Batch 48/64 loss: -0.035022735595703125
Batch 49/64 loss: -0.0422094464302063
Batch 50/64 loss: -0.05015599727630615
Batch 51/64 loss: -0.0271148681640625
Batch 52/64 loss: -0.052847445011138916
Batch 53/64 loss: -0.0539247989654541
Batch 54/64 loss: -0.05654764175415039
Batch 55/64 loss: -0.04989290237426758
Batch 56/64 loss: -0.04663628339767456
Batch 57/64 loss: -0.06476753950119019
Batch 58/64 loss: -0.05906212329864502
Batch 59/64 loss: -0.06146097183227539
Batch 60/64 loss: -0.06170177459716797
Batch 61/64 loss: -0.04472905397415161
Batch 62/64 loss: -0.06204324960708618
Batch 63/64 loss: -0.03235936164855957
Batch 64/64 loss: -0.07554805278778076
Epoch 451  Train loss: -0.049406757074243884  Val loss: 0.09852473248321165
Epoch 452
-------------------------------
Batch 1/64 loss: -0.04845535755157471
Batch 2/64 loss: -0.06640762090682983
Batch 3/64 loss: -0.06702065467834473
Batch 4/64 loss: -0.055506110191345215
Batch 5/64 loss: -0.05430757999420166
Batch 6/64 loss: -0.04140514135360718
Batch 7/64 loss: -0.057279348373413086
Batch 8/64 loss: -0.05377471446990967
Batch 9/64 loss: -0.06064552068710327
Batch 10/64 loss: -0.05307728052139282
Batch 11/64 loss: -0.03579217195510864
Batch 12/64 loss: -0.053986966609954834
Batch 13/64 loss: -0.04463726282119751
Batch 14/64 loss: -0.051783084869384766
Batch 15/64 loss: -0.029736757278442383
Batch 16/64 loss: -0.04952800273895264
Batch 17/64 loss: -0.04336780309677124
Batch 18/64 loss: -0.06541544198989868
Batch 19/64 loss: -0.04849964380264282
Batch 20/64 loss: -0.06313443183898926
Batch 21/64 loss: -0.06649357080459595
Batch 22/64 loss: -0.06536537408828735
Batch 23/64 loss: -0.07304561138153076
Batch 24/64 loss: -0.05870109796524048
Batch 25/64 loss: -0.054406821727752686
Batch 26/64 loss: -0.06389820575714111
Batch 27/64 loss: -0.07054847478866577
Batch 28/64 loss: -0.03806912899017334
Batch 29/64 loss: -0.06230056285858154
Batch 30/64 loss: -0.0695260763168335
Batch 31/64 loss: -0.06105649471282959
Batch 32/64 loss: -0.02465122938156128
Batch 33/64 loss: -0.054764628410339355
Batch 34/64 loss: -0.053556859493255615
Batch 35/64 loss: -0.053755760192871094
Batch 36/64 loss: -0.04973727464675903
Batch 37/64 loss: -0.04732811450958252
Batch 38/64 loss: -0.03451603651046753
Batch 39/64 loss: -0.04157114028930664
Batch 40/64 loss: -0.047736406326293945
Batch 41/64 loss: -0.03689604997634888
Batch 42/64 loss: -0.043293774127960205
Batch 43/64 loss: -0.02916741371154785
Batch 44/64 loss: -0.055738985538482666
Batch 45/64 loss: -0.0503578782081604
Batch 46/64 loss: -0.025502026081085205
Batch 47/64 loss: -0.026708126068115234
Batch 48/64 loss: -0.05581545829772949
Batch 49/64 loss: -0.04564487934112549
Batch 50/64 loss: -0.0530397891998291
Batch 51/64 loss: -0.05069601535797119
Batch 52/64 loss: -0.06425827741622925
Batch 53/64 loss: -0.045623064041137695
Batch 54/64 loss: -0.04035979509353638
Batch 55/64 loss: -0.029794812202453613
Batch 56/64 loss: -0.04317879676818848
Batch 57/64 loss: -0.04145544767379761
Batch 58/64 loss: -0.011023104190826416
Batch 59/64 loss: -0.03895068168640137
Batch 60/64 loss: -0.04802107810974121
Batch 61/64 loss: -0.05334669351577759
Batch 62/64 loss: -0.05274200439453125
Batch 63/64 loss: -0.02746748924255371
Batch 64/64 loss: -0.04246318340301514
Epoch 452  Train loss: -0.04918774576748119  Val loss: 0.09672765879286933
Epoch 453
-------------------------------
Batch 1/64 loss: -0.05724447965621948
Batch 2/64 loss: -0.038237035274505615
Batch 3/64 loss: -0.03741812705993652
Batch 4/64 loss: -0.04095268249511719
Batch 5/64 loss: -0.04191625118255615
Batch 6/64 loss: -0.05137145519256592
Batch 7/64 loss: -0.04424387216567993
Batch 8/64 loss: -0.0514754056930542
Batch 9/64 loss: -0.05663013458251953
Batch 10/64 loss: -0.03895604610443115
Batch 11/64 loss: -0.044926583766937256
Batch 12/64 loss: -0.05791133642196655
Batch 13/64 loss: -0.035047292709350586
Batch 14/64 loss: -0.04595679044723511
Batch 15/64 loss: -0.04900771379470825
Batch 16/64 loss: -0.030979812145233154
Batch 17/64 loss: -0.03508949279785156
Batch 18/64 loss: -0.05620032548904419
Batch 19/64 loss: -0.040061354637145996
Batch 20/64 loss: -0.054383814334869385
Batch 21/64 loss: -0.05464577674865723
Batch 22/64 loss: -0.042411208152770996
Batch 23/64 loss: -0.047547221183776855
Batch 24/64 loss: -0.040213942527770996
Batch 25/64 loss: -0.05895829200744629
Batch 26/64 loss: -0.06725418567657471
Batch 27/64 loss: -0.05530345439910889
Batch 28/64 loss: -0.05857384204864502
Batch 29/64 loss: -0.055620551109313965
Batch 30/64 loss: -0.040449678897857666
Batch 31/64 loss: -0.028281688690185547
Batch 32/64 loss: -0.03495281934738159
Batch 33/64 loss: -0.061531901359558105
Batch 34/64 loss: -0.04900538921356201
Batch 35/64 loss: -0.06296718120574951
Batch 36/64 loss: -0.04744267463684082
Batch 37/64 loss: -0.04229605197906494
Batch 38/64 loss: -0.040775954723358154
Batch 39/64 loss: -0.059151291847229004
Batch 40/64 loss: -0.03705704212188721
Batch 41/64 loss: -0.04003405570983887
Batch 42/64 loss: -0.06310909986495972
Batch 43/64 loss: -0.06578278541564941
Batch 44/64 loss: -0.0712881088256836
Batch 45/64 loss: -0.041580915451049805
Batch 46/64 loss: -0.07189106941223145
Batch 47/64 loss: -0.05501222610473633
Batch 48/64 loss: -0.057251691818237305
Batch 49/64 loss: -0.06324511766433716
Batch 50/64 loss: -0.07007193565368652
Batch 51/64 loss: -0.03461813926696777
Batch 52/64 loss: -0.060996413230895996
Batch 53/64 loss: -0.046831369400024414
Batch 54/64 loss: -0.07334208488464355
Batch 55/64 loss: -0.052684783935546875
Batch 56/64 loss: -0.027375638484954834
Batch 57/64 loss: -0.05389857292175293
Batch 58/64 loss: -0.0619083046913147
Batch 59/64 loss: -0.036701738834381104
Batch 60/64 loss: -0.05577528476715088
Batch 61/64 loss: -0.044557392597198486
Batch 62/64 loss: -0.03418755531311035
Batch 63/64 loss: -0.06569939851760864
Batch 64/64 loss: -0.057256460189819336
Epoch 453  Train loss: -0.04993311657625086  Val loss: 0.10079250220990263
Epoch 454
-------------------------------
Batch 1/64 loss: -0.06178545951843262
Batch 2/64 loss: -0.05054372549057007
Batch 3/64 loss: -0.06464660167694092
Batch 4/64 loss: -0.07060343027114868
Batch 5/64 loss: -0.047025978565216064
Batch 6/64 loss: -0.05605888366699219
Batch 7/64 loss: -0.056506335735321045
Batch 8/64 loss: -0.0434001088142395
Batch 9/64 loss: -0.04262351989746094
Batch 10/64 loss: -0.05580151081085205
Batch 11/64 loss: -0.062493085861206055
Batch 12/64 loss: -0.056497931480407715
Batch 13/64 loss: -0.052560627460479736
Batch 14/64 loss: -0.04982072114944458
Batch 15/64 loss: -0.06397932767868042
Batch 16/64 loss: -0.04198801517486572
Batch 17/64 loss: -0.059805989265441895
Batch 18/64 loss: -0.04350399971008301
Batch 19/64 loss: -0.06005501747131348
Batch 20/64 loss: -0.05214923620223999
Batch 21/64 loss: -0.030225515365600586
Batch 22/64 loss: -0.055489301681518555
Batch 23/64 loss: -0.04947686195373535
Batch 24/64 loss: -0.07077473402023315
Batch 25/64 loss: -0.053475379943847656
Batch 26/64 loss: -0.06024235486984253
Batch 27/64 loss: -0.06023764610290527
Batch 28/64 loss: -0.040177345275878906
Batch 29/64 loss: -0.055625855922698975
Batch 30/64 loss: -0.03952687978744507
Batch 31/64 loss: -0.05172586441040039
Batch 32/64 loss: -0.058478355407714844
Batch 33/64 loss: -0.057384192943573
Batch 34/64 loss: -0.05606198310852051
Batch 35/64 loss: -0.016941308975219727
Batch 36/64 loss: -0.035714149475097656
Batch 37/64 loss: -0.05857837200164795
Batch 38/64 loss: -0.0393376350402832
Batch 39/64 loss: -0.042378783226013184
Batch 40/64 loss: -0.05921107530593872
Batch 41/64 loss: -0.07172393798828125
Batch 42/64 loss: -0.05280256271362305
Batch 43/64 loss: -0.05711352825164795
Batch 44/64 loss: -0.05818057060241699
Batch 45/64 loss: -0.05423074960708618
Batch 46/64 loss: -0.0507887601852417
Batch 47/64 loss: -0.04302108287811279
Batch 48/64 loss: -0.041506409645080566
Batch 49/64 loss: -0.045500218868255615
Batch 50/64 loss: -0.032776713371276855
Batch 51/64 loss: -0.06300139427185059
Batch 52/64 loss: -0.04737424850463867
Batch 53/64 loss: -0.049156010150909424
Batch 54/64 loss: -0.03389531373977661
Batch 55/64 loss: -0.060882508754730225
Batch 56/64 loss: -0.053338050842285156
Batch 57/64 loss: -0.040506839752197266
Batch 58/64 loss: -0.054476141929626465
Batch 59/64 loss: -0.05072760581970215
Batch 60/64 loss: -0.040865302085876465
Batch 61/64 loss: -0.06213313341140747
Batch 62/64 loss: -0.04233074188232422
Batch 63/64 loss: -0.06479740142822266
Batch 64/64 loss: -0.034134864807128906
Epoch 454  Train loss: -0.051445387858970494  Val loss: 0.1002737537692093
Epoch 455
-------------------------------
Batch 1/64 loss: -0.050728559494018555
Batch 2/64 loss: -0.06319117546081543
Batch 3/64 loss: -0.03042691946029663
Batch 4/64 loss: -0.05358046293258667
Batch 5/64 loss: -0.04041963815689087
Batch 6/64 loss: -0.049427807331085205
Batch 7/64 loss: -0.040464580059051514
Batch 8/64 loss: -0.04745107889175415
Batch 9/64 loss: -0.06305938959121704
Batch 10/64 loss: -0.05486702919006348
Batch 11/64 loss: -0.05968821048736572
Batch 12/64 loss: -0.05068486928939819
Batch 13/64 loss: -0.06136608123779297
Batch 14/64 loss: -0.060018301010131836
Batch 15/64 loss: -0.03989815711975098
Batch 16/64 loss: -0.07546669244766235
Batch 17/64 loss: -0.049518704414367676
Batch 18/64 loss: -0.06120270490646362
Batch 19/64 loss: -0.04865610599517822
Batch 20/64 loss: -0.054708123207092285
Batch 21/64 loss: -0.06566798686981201
Batch 22/64 loss: -0.05256927013397217
Batch 23/64 loss: -0.03865504264831543
Batch 24/64 loss: -0.048390984535217285
Batch 25/64 loss: -0.056502699851989746
Batch 26/64 loss: -0.0496826171875
Batch 27/64 loss: -0.054027676582336426
Batch 28/64 loss: -0.061090707778930664
Batch 29/64 loss: -0.04614537954330444
Batch 30/64 loss: -0.03501415252685547
Batch 31/64 loss: -0.05808669328689575
Batch 32/64 loss: -0.053152620792388916
Batch 33/64 loss: -0.04092860221862793
Batch 34/64 loss: -0.06037569046020508
Batch 35/64 loss: -0.05099809169769287
Batch 36/64 loss: -0.07190346717834473
Batch 37/64 loss: -0.04079526662826538
Batch 38/64 loss: -0.05514490604400635
Batch 39/64 loss: -0.05485427379608154
Batch 40/64 loss: -0.05430412292480469
Batch 41/64 loss: -0.05928695201873779
Batch 42/64 loss: -0.05414462089538574
Batch 43/64 loss: -0.05583071708679199
Batch 44/64 loss: -0.05493319034576416
Batch 45/64 loss: -0.046070754528045654
Batch 46/64 loss: -0.06247299909591675
Batch 47/64 loss: -0.05219292640686035
Batch 48/64 loss: -0.062100768089294434
Batch 49/64 loss: -0.04342532157897949
Batch 50/64 loss: -0.06243044137954712
Batch 51/64 loss: -0.04544377326965332
Batch 52/64 loss: -0.03712809085845947
Batch 53/64 loss: -0.055153846740722656
Batch 54/64 loss: -0.05959463119506836
Batch 55/64 loss: -0.04662764072418213
Batch 56/64 loss: -0.04421967267990112
Batch 57/64 loss: -0.0170862078666687
Batch 58/64 loss: -0.059181034564971924
Batch 59/64 loss: -0.06049281358718872
Batch 60/64 loss: -0.059174954891204834
Batch 61/64 loss: -0.040961265563964844
Batch 62/64 loss: -0.059780657291412354
Batch 63/64 loss: -0.050653696060180664
Batch 64/64 loss: -0.03326648473739624
Epoch 455  Train loss: -0.052022771508085965  Val loss: 0.09993378157468186
Epoch 456
-------------------------------
Batch 1/64 loss: -0.06777536869049072
Batch 2/64 loss: -0.040398359298706055
Batch 3/64 loss: -0.04568350315093994
Batch 4/64 loss: -0.06365013122558594
Batch 5/64 loss: -0.048865675926208496
Batch 6/64 loss: -0.048644185066223145
Batch 7/64 loss: -0.06183439493179321
Batch 8/64 loss: -0.04544776678085327
Batch 9/64 loss: -0.06539130210876465
Batch 10/64 loss: -0.055660367012023926
Batch 11/64 loss: -0.044878482818603516
Batch 12/64 loss: -0.04330551624298096
Batch 13/64 loss: -0.04991215467453003
Batch 14/64 loss: -0.05383944511413574
Batch 15/64 loss: -0.057924509048461914
Batch 16/64 loss: -0.03748053312301636
Batch 17/64 loss: -0.0460125207901001
Batch 18/64 loss: -0.06591033935546875
Batch 19/64 loss: -0.05660980939865112
Batch 20/64 loss: -0.05011749267578125
Batch 21/64 loss: -0.04687345027923584
Batch 22/64 loss: -0.0590287446975708
Batch 23/64 loss: -0.06956839561462402
Batch 24/64 loss: -0.06726831197738647
Batch 25/64 loss: -0.05179703235626221
Batch 26/64 loss: -0.05390441417694092
Batch 27/64 loss: -0.0452420711517334
Batch 28/64 loss: -0.059438228607177734
Batch 29/64 loss: -0.06964719295501709
Batch 30/64 loss: -0.05114436149597168
Batch 31/64 loss: -0.06297630071640015
Batch 32/64 loss: -0.055851519107818604
Batch 33/64 loss: -0.06334185600280762
Batch 34/64 loss: -0.05265915393829346
Batch 35/64 loss: -0.06117302179336548
Batch 36/64 loss: -0.05071437358856201
Batch 37/64 loss: -0.04944878816604614
Batch 38/64 loss: -0.04502791166305542
Batch 39/64 loss: -0.060575127601623535
Batch 40/64 loss: -0.03236955404281616
Batch 41/64 loss: -0.03625059127807617
Batch 42/64 loss: -0.037375032901763916
Batch 43/64 loss: -0.07034242153167725
Batch 44/64 loss: -0.04194068908691406
Batch 45/64 loss: -0.048419058322906494
Batch 46/64 loss: -0.03953349590301514
Batch 47/64 loss: -0.05873233079910278
Batch 48/64 loss: -0.062478721141815186
Batch 49/64 loss: -0.04754507541656494
Batch 50/64 loss: -0.05769610404968262
Batch 51/64 loss: -0.038289546966552734
Batch 52/64 loss: -0.04505199193954468
Batch 53/64 loss: -0.038063883781433105
Batch 54/64 loss: -0.0560954213142395
Batch 55/64 loss: -0.039016783237457275
Batch 56/64 loss: -0.037399888038635254
Batch 57/64 loss: -0.04489928483963013
Batch 58/64 loss: -0.06251424551010132
Batch 59/64 loss: -0.05984318256378174
Batch 60/64 loss: -0.027374982833862305
Batch 61/64 loss: -0.025180518627166748
Batch 62/64 loss: -0.05776709318161011
Batch 63/64 loss: -0.061226069927215576
Batch 64/64 loss: -0.03185683488845825
Epoch 456  Train loss: -0.05136189343882542  Val loss: 0.09858384001296001
Epoch 457
-------------------------------
Batch 1/64 loss: -0.04227083921432495
Batch 2/64 loss: -0.04066038131713867
Batch 3/64 loss: -0.050144195556640625
Batch 4/64 loss: -0.06884628534317017
Batch 5/64 loss: -0.06173807382583618
Batch 6/64 loss: -0.04170882701873779
Batch 7/64 loss: -0.0527958869934082
Batch 8/64 loss: -0.038604557514190674
Batch 9/64 loss: -0.043329834938049316
Batch 10/64 loss: -0.05380356311798096
Batch 11/64 loss: -0.058046936988830566
Batch 12/64 loss: -0.06281071901321411
Batch 13/64 loss: -0.06686973571777344
Batch 14/64 loss: -0.06512618064880371
Batch 15/64 loss: -0.04596585035324097
Batch 16/64 loss: -0.042926907539367676
Batch 17/64 loss: -0.05416274070739746
Batch 18/64 loss: -0.07187247276306152
Batch 19/64 loss: -0.06204867362976074
Batch 20/64 loss: -0.035609424114227295
Batch 21/64 loss: -0.05572819709777832
Batch 22/64 loss: -0.0462033748626709
Batch 23/64 loss: -0.05304110050201416
Batch 24/64 loss: -0.06629723310470581
Batch 25/64 loss: -0.04581993818283081
Batch 26/64 loss: -0.06315857172012329
Batch 27/64 loss: -0.04200565814971924
Batch 28/64 loss: -0.06079214811325073
Batch 29/64 loss: -0.06207507848739624
Batch 30/64 loss: -0.039108335971832275
Batch 31/64 loss: -0.05035126209259033
Batch 32/64 loss: -0.05185520648956299
Batch 33/64 loss: -0.04434126615524292
Batch 34/64 loss: -0.044948458671569824
Batch 35/64 loss: -0.03581690788269043
Batch 36/64 loss: -0.03945612907409668
Batch 37/64 loss: -0.040305495262145996
Batch 38/64 loss: -0.060469090938568115
Batch 39/64 loss: -0.05013376474380493
Batch 40/64 loss: -0.05159139633178711
Batch 41/64 loss: -0.059676527976989746
Batch 42/64 loss: -0.05092740058898926
Batch 43/64 loss: -0.0519983172416687
Batch 44/64 loss: -0.04218769073486328
Batch 45/64 loss: -0.048496365547180176
Batch 46/64 loss: -0.06516152620315552
Batch 47/64 loss: -0.0533214807510376
Batch 48/64 loss: -0.059387922286987305
Batch 49/64 loss: -0.04715639352798462
Batch 50/64 loss: -0.05092930793762207
Batch 51/64 loss: -0.058072447776794434
Batch 52/64 loss: -0.047338008880615234
Batch 53/64 loss: -0.05268365144729614
Batch 54/64 loss: -0.06147289276123047
Batch 55/64 loss: -0.03932827711105347
Batch 56/64 loss: -0.05619746446609497
Batch 57/64 loss: -0.0543789267539978
Batch 58/64 loss: -0.05874943733215332
Batch 59/64 loss: -0.05386674404144287
Batch 60/64 loss: -0.040452420711517334
Batch 61/64 loss: -0.04848378896713257
Batch 62/64 loss: -0.048899173736572266
Batch 63/64 loss: -0.052959978580474854
Batch 64/64 loss: -0.05631279945373535
Epoch 457  Train loss: -0.05187766972710105  Val loss: 0.09898499852603244
Epoch 458
-------------------------------
Batch 1/64 loss: -0.06388038396835327
Batch 2/64 loss: -0.039627015590667725
Batch 3/64 loss: -0.05504798889160156
Batch 4/64 loss: -0.06187331676483154
Batch 5/64 loss: -0.058014631271362305
Batch 6/64 loss: -0.06939125061035156
Batch 7/64 loss: -0.07008427381515503
Batch 8/64 loss: -0.049869418144226074
Batch 9/64 loss: -0.042768657207489014
Batch 10/64 loss: -0.06396329402923584
Batch 11/64 loss: -0.053093135356903076
Batch 12/64 loss: -0.0442429780960083
Batch 13/64 loss: -0.05624377727508545
Batch 14/64 loss: -0.07096856832504272
Batch 15/64 loss: -0.03913372755050659
Batch 16/64 loss: -0.06542497873306274
Batch 17/64 loss: -0.03647875785827637
Batch 18/64 loss: -0.049453139305114746
Batch 19/64 loss: -0.038278818130493164
Batch 20/64 loss: -0.034799814224243164
Batch 21/64 loss: -0.06808185577392578
Batch 22/64 loss: -0.06616568565368652
Batch 23/64 loss: -0.03522986173629761
Batch 24/64 loss: -0.048364341259002686
Batch 25/64 loss: -0.04574817419052124
Batch 26/64 loss: -0.04419809579849243
Batch 27/64 loss: -0.057555556297302246
Batch 28/64 loss: -0.029537856578826904
Batch 29/64 loss: -0.03624814748764038
Batch 30/64 loss: -0.034001708030700684
Batch 31/64 loss: -0.04812479019165039
Batch 32/64 loss: -0.061428725719451904
Batch 33/64 loss: -0.04067075252532959
Batch 34/64 loss: -0.03980612754821777
Batch 35/64 loss: -0.04361933469772339
Batch 36/64 loss: -0.06620162725448608
Batch 37/64 loss: -0.060835182666778564
Batch 38/64 loss: -0.05609869956970215
Batch 39/64 loss: -0.05571568012237549
Batch 40/64 loss: -0.0763738751411438
Batch 41/64 loss: -0.06316971778869629
Batch 42/64 loss: -0.060566842555999756
Batch 43/64 loss: -0.03347659111022949
Batch 44/64 loss: -0.05564790964126587
Batch 45/64 loss: -0.04971730709075928
Batch 46/64 loss: -0.04535180330276489
Batch 47/64 loss: -0.053659796714782715
Batch 48/64 loss: -0.037960052490234375
Batch 49/64 loss: -0.07182228565216064
Batch 50/64 loss: -0.047783613204956055
Batch 51/64 loss: -0.05889946222305298
Batch 52/64 loss: -0.054558515548706055
Batch 53/64 loss: -0.03679537773132324
Batch 54/64 loss: -0.052871763706207275
Batch 55/64 loss: -0.04805368185043335
Batch 56/64 loss: -0.045350611209869385
Batch 57/64 loss: -0.037310779094696045
Batch 58/64 loss: -0.03633904457092285
Batch 59/64 loss: -0.058546364307403564
Batch 60/64 loss: -0.060242652893066406
Batch 61/64 loss: -0.052776455879211426
Batch 62/64 loss: -0.05840027332305908
Batch 63/64 loss: -0.055598318576812744
Batch 64/64 loss: -0.048655569553375244
Epoch 458  Train loss: -0.05157701805526135  Val loss: 0.09869348716080394
Epoch 459
-------------------------------
Batch 1/64 loss: -0.06087273359298706
Batch 2/64 loss: -0.0530858039855957
Batch 3/64 loss: -0.048087120056152344
Batch 4/64 loss: -0.06061369180679321
Batch 5/64 loss: -0.06079912185668945
Batch 6/64 loss: -0.05520898103713989
Batch 7/64 loss: -0.06262952089309692
Batch 8/64 loss: -0.048874855041503906
Batch 9/64 loss: -0.06936872005462646
Batch 10/64 loss: -0.059058964252471924
Batch 11/64 loss: -0.06252413988113403
Batch 12/64 loss: -0.06159853935241699
Batch 13/64 loss: -0.05647265911102295
Batch 14/64 loss: -0.05822563171386719
Batch 15/64 loss: -0.04511141777038574
Batch 16/64 loss: -0.05712413787841797
Batch 17/64 loss: -0.05332005023956299
Batch 18/64 loss: -0.045516133308410645
Batch 19/64 loss: -0.04330176115036011
Batch 20/64 loss: -0.06140923500061035
Batch 21/64 loss: -0.03156048059463501
Batch 22/64 loss: -0.04624664783477783
Batch 23/64 loss: -0.06882274150848389
Batch 24/64 loss: -0.06863731145858765
Batch 25/64 loss: -0.05864739418029785
Batch 26/64 loss: -0.05956512689590454
Batch 27/64 loss: -0.03488272428512573
Batch 28/64 loss: -0.061014413833618164
Batch 29/64 loss: -0.0487513542175293
Batch 30/64 loss: -0.05610150098800659
Batch 31/64 loss: -0.05641573667526245
Batch 32/64 loss: -0.04526066780090332
Batch 33/64 loss: -0.0566638708114624
Batch 34/64 loss: -0.06096959114074707
Batch 35/64 loss: -0.060908734798431396
Batch 36/64 loss: -0.06226319074630737
Batch 37/64 loss: -0.06696295738220215
Batch 38/64 loss: -0.058138370513916016
Batch 39/64 loss: -0.06747478246688843
Batch 40/64 loss: -0.06817775964736938
Batch 41/64 loss: -0.035901427268981934
Batch 42/64 loss: -0.03663867712020874
Batch 43/64 loss: -0.06026124954223633
Batch 44/64 loss: -0.045059263706207275
Batch 45/64 loss: -0.06270742416381836
Batch 46/64 loss: -0.047838687896728516
Batch 47/64 loss: -0.055220603942871094
Batch 48/64 loss: -0.0568317174911499
Batch 49/64 loss: -0.05043822526931763
Batch 50/64 loss: -0.04614967107772827
Batch 51/64 loss: -0.042327821254730225
Batch 52/64 loss: -0.049587786197662354
Batch 53/64 loss: -0.048793792724609375
Batch 54/64 loss: -0.05767267942428589
Batch 55/64 loss: -0.06778126955032349
Batch 56/64 loss: -0.04074907302856445
Batch 57/64 loss: -0.035576581954956055
Batch 58/64 loss: -0.04356825351715088
Batch 59/64 loss: -0.05578511953353882
Batch 60/64 loss: -0.027053356170654297
Batch 61/64 loss: -0.05093449354171753
Batch 62/64 loss: -0.0525246262550354
Batch 63/64 loss: -0.05111289024353027
Batch 64/64 loss: -0.036158621311187744
Epoch 459  Train loss: -0.05346356443330354  Val loss: 0.09927032800884181
Epoch 460
-------------------------------
Batch 1/64 loss: -0.03224897384643555
Batch 2/64 loss: -0.0369337797164917
Batch 3/64 loss: -0.048267245292663574
Batch 4/64 loss: -0.05297982692718506
Batch 5/64 loss: -0.038473546504974365
Batch 6/64 loss: -0.05291247367858887
Batch 7/64 loss: -0.0527535080909729
Batch 8/64 loss: -0.04029512405395508
Batch 9/64 loss: -0.05076611042022705
Batch 10/64 loss: -0.01709127426147461
Batch 11/64 loss: -0.04905891418457031
Batch 12/64 loss: -0.04661291837692261
Batch 13/64 loss: -0.0523032546043396
Batch 14/64 loss: -0.058207154273986816
Batch 15/64 loss: -0.02854830026626587
Batch 16/64 loss: -0.04996770620346069
Batch 17/64 loss: -0.05892300605773926
Batch 18/64 loss: -0.04128444194793701
Batch 19/64 loss: -0.021691858768463135
Batch 20/64 loss: -0.04568839073181152
Batch 21/64 loss: -0.0346604585647583
Batch 22/64 loss: -0.03569662570953369
Batch 23/64 loss: -0.06363803148269653
Batch 24/64 loss: -0.05099189281463623
Batch 25/64 loss: -0.05099362134933472
Batch 26/64 loss: -0.07287043333053589
Batch 27/64 loss: -0.05966603755950928
Batch 28/64 loss: -0.04710948467254639
Batch 29/64 loss: -0.05317002534866333
Batch 30/64 loss: -0.046131432056427
Batch 31/64 loss: -0.05502915382385254
Batch 32/64 loss: -0.04912424087524414
Batch 33/64 loss: -0.06266945600509644
Batch 34/64 loss: -0.046417832374572754
Batch 35/64 loss: -0.03588992357254028
Batch 36/64 loss: -0.055457234382629395
Batch 37/64 loss: -0.05043816566467285
Batch 38/64 loss: -0.04808175563812256
Batch 39/64 loss: -0.05375915765762329
Batch 40/64 loss: -0.043718576431274414
Batch 41/64 loss: -0.045307159423828125
Batch 42/64 loss: -0.06702965497970581
Batch 43/64 loss: -0.06602108478546143
Batch 44/64 loss: -0.04300457239151001
Batch 45/64 loss: -0.04587215185165405
Batch 46/64 loss: -0.05311006307601929
Batch 47/64 loss: -0.057684242725372314
Batch 48/64 loss: -0.057691216468811035
Batch 49/64 loss: -0.06668084859848022
Batch 50/64 loss: -0.045232295989990234
Batch 51/64 loss: -0.0676184892654419
Batch 52/64 loss: -0.061683058738708496
Batch 53/64 loss: -0.0628817081451416
Batch 54/64 loss: -0.054722607135772705
Batch 55/64 loss: -0.07201159000396729
Batch 56/64 loss: -0.05241227149963379
Batch 57/64 loss: -0.060796916484832764
Batch 58/64 loss: -0.040213584899902344
Batch 59/64 loss: -0.048719584941864014
Batch 60/64 loss: -0.06365883350372314
Batch 61/64 loss: -0.04754912853240967
Batch 62/64 loss: -0.056448400020599365
Batch 63/64 loss: -0.04336714744567871
Batch 64/64 loss: -0.052828073501586914
Epoch 460  Train loss: -0.05035072962443034  Val loss: 0.10136345223462868
Epoch 461
-------------------------------
Batch 1/64 loss: -0.05492806434631348
Batch 2/64 loss: -0.037567198276519775
Batch 3/64 loss: -0.05760776996612549
Batch 4/64 loss: -0.029599308967590332
Batch 5/64 loss: -0.059214115142822266
Batch 6/64 loss: -0.06478708982467651
Batch 7/64 loss: -0.029223620891571045
Batch 8/64 loss: -0.06632494926452637
Batch 9/64 loss: -0.06573635339736938
Batch 10/64 loss: -0.07545322179794312
Batch 11/64 loss: -0.05916321277618408
Batch 12/64 loss: -0.046530187129974365
Batch 13/64 loss: -0.04204946756362915
Batch 14/64 loss: -0.04891467094421387
Batch 15/64 loss: -0.059876084327697754
Batch 16/64 loss: -0.025615930557250977
Batch 17/64 loss: -0.06316238641738892
Batch 18/64 loss: -0.0455472469329834
Batch 19/64 loss: -0.05292987823486328
Batch 20/64 loss: -0.05102252960205078
Batch 21/64 loss: -0.039683640003204346
Batch 22/64 loss: -0.03867805004119873
Batch 23/64 loss: -0.07186275720596313
Batch 24/64 loss: -0.06069666147232056
Batch 25/64 loss: -0.06658995151519775
Batch 26/64 loss: -0.05702412128448486
Batch 27/64 loss: -0.055670320987701416
Batch 28/64 loss: -0.04400390386581421
Batch 29/64 loss: -0.04188501834869385
Batch 30/64 loss: -0.04614901542663574
Batch 31/64 loss: -0.0557553768157959
Batch 32/64 loss: -0.05061274766921997
Batch 33/64 loss: -0.06378698348999023
Batch 34/64 loss: -0.07355040311813354
Batch 35/64 loss: -0.041550517082214355
Batch 36/64 loss: -0.04165756702423096
Batch 37/64 loss: -0.05343925952911377
Batch 38/64 loss: -0.0324515700340271
Batch 39/64 loss: -0.053069889545440674
Batch 40/64 loss: -0.056051433086395264
Batch 41/64 loss: -0.06754326820373535
Batch 42/64 loss: -0.04568982124328613
Batch 43/64 loss: -0.05216634273529053
Batch 44/64 loss: -0.04049253463745117
Batch 45/64 loss: -0.04707062244415283
Batch 46/64 loss: -0.06292635202407837
Batch 47/64 loss: -0.06463038921356201
Batch 48/64 loss: -0.06867140531539917
Batch 49/64 loss: -0.051506221294403076
Batch 50/64 loss: -0.047748863697052
Batch 51/64 loss: -0.04230523109436035
Batch 52/64 loss: -0.03213244676589966
Batch 53/64 loss: -0.07165157794952393
Batch 54/64 loss: -0.04472154378890991
Batch 55/64 loss: -0.008857369422912598
Batch 56/64 loss: -0.03926360607147217
Batch 57/64 loss: -0.06380224227905273
Batch 58/64 loss: -0.03536045551300049
Batch 59/64 loss: -0.050323426723480225
Batch 60/64 loss: -0.04497188329696655
Batch 61/64 loss: -0.039417803287506104
Batch 62/64 loss: -0.06548058986663818
Batch 63/64 loss: -0.06200826168060303
Batch 64/64 loss: -0.035439252853393555
Epoch 461  Train loss: -0.05105481054268631  Val loss: 0.09845161192195932
Epoch 462
-------------------------------
Batch 1/64 loss: -0.0596698522567749
Batch 2/64 loss: -0.06370007991790771
Batch 3/64 loss: -0.037191927433013916
Batch 4/64 loss: -0.03698527812957764
Batch 5/64 loss: -0.06362718343734741
Batch 6/64 loss: -0.04238790273666382
Batch 7/64 loss: -0.05249661207199097
Batch 8/64 loss: -0.04858940839767456
Batch 9/64 loss: -0.04425346851348877
Batch 10/64 loss: -0.05848163366317749
Batch 11/64 loss: -0.025090813636779785
Batch 12/64 loss: -0.054882049560546875
Batch 13/64 loss: -0.062125325202941895
Batch 14/64 loss: -0.05609792470932007
Batch 15/64 loss: -0.0611303448677063
Batch 16/64 loss: -0.013223528861999512
Batch 17/64 loss: -0.06433898210525513
Batch 18/64 loss: -0.06137579679489136
Batch 19/64 loss: -0.05213725566864014
Batch 20/64 loss: -0.0628015398979187
Batch 21/64 loss: -0.06499373912811279
Batch 22/64 loss: -0.04597669839859009
Batch 23/64 loss: -0.05498778820037842
Batch 24/64 loss: -0.05655711889266968
Batch 25/64 loss: -0.050642311573028564
Batch 26/64 loss: -0.047866761684417725
Batch 27/64 loss: -0.04125678539276123
Batch 28/64 loss: -0.06713247299194336
Batch 29/64 loss: -0.04331845045089722
Batch 30/64 loss: -0.018195509910583496
Batch 31/64 loss: -0.06342220306396484
Batch 32/64 loss: -0.05642968416213989
Batch 33/64 loss: -0.041129350662231445
Batch 34/64 loss: -0.06398177146911621
Batch 35/64 loss: -0.051274120807647705
Batch 36/64 loss: -0.06823360919952393
Batch 37/64 loss: -0.05403292179107666
Batch 38/64 loss: -0.04658859968185425
Batch 39/64 loss: -0.05920696258544922
Batch 40/64 loss: -0.0716710090637207
Batch 41/64 loss: -0.06744754314422607
Batch 42/64 loss: -0.05023699998855591
Batch 43/64 loss: -0.03449732065200806
Batch 44/64 loss: -0.04602313041687012
Batch 45/64 loss: -0.06060683727264404
Batch 46/64 loss: -0.036850154399871826
Batch 47/64 loss: -0.0619732141494751
Batch 48/64 loss: -0.05854213237762451
Batch 49/64 loss: -0.058975815773010254
Batch 50/64 loss: -0.04565960168838501
Batch 51/64 loss: -0.05718320608139038
Batch 52/64 loss: -0.058019042015075684
Batch 53/64 loss: -0.06221604347229004
Batch 54/64 loss: -0.04188114404678345
Batch 55/64 loss: -0.05275273323059082
Batch 56/64 loss: -0.05617570877075195
Batch 57/64 loss: -0.04775816202163696
Batch 58/64 loss: -0.06798362731933594
Batch 59/64 loss: -0.022040069103240967
Batch 60/64 loss: -0.05961203575134277
Batch 61/64 loss: -0.047676682472229004
Batch 62/64 loss: -0.049324989318847656
Batch 63/64 loss: -0.06649386882781982
Batch 64/64 loss: -0.054753899574279785
Epoch 462  Train loss: -0.05236831786585789  Val loss: 0.09984782491762613
Epoch 463
-------------------------------
Batch 1/64 loss: -0.06912219524383545
Batch 2/64 loss: -0.05681329965591431
Batch 3/64 loss: -0.06944453716278076
Batch 4/64 loss: -0.04689228534698486
Batch 5/64 loss: -0.060451388359069824
Batch 6/64 loss: -0.060752809047698975
Batch 7/64 loss: -0.05506712198257446
Batch 8/64 loss: -0.04335951805114746
Batch 9/64 loss: -0.04825407266616821
Batch 10/64 loss: -0.06143677234649658
Batch 11/64 loss: -0.03690379858016968
Batch 12/64 loss: -0.05100303888320923
Batch 13/64 loss: -0.05003619194030762
Batch 14/64 loss: -0.050688087940216064
Batch 15/64 loss: -0.049467504024505615
Batch 16/64 loss: -0.05000460147857666
Batch 17/64 loss: -0.05204594135284424
Batch 18/64 loss: -0.0749559998512268
Batch 19/64 loss: -0.05374646186828613
Batch 20/64 loss: -0.061215221881866455
Batch 21/64 loss: -0.05935794115066528
Batch 22/64 loss: -0.07564926147460938
Batch 23/64 loss: -0.0644921064376831
Batch 24/64 loss: -0.07714629173278809
Batch 25/64 loss: -0.06942242383956909
Batch 26/64 loss: -0.04429352283477783
Batch 27/64 loss: -0.04201245307922363
Batch 28/64 loss: -0.05367487668991089
Batch 29/64 loss: -0.06660568714141846
Batch 30/64 loss: -0.06414574384689331
Batch 31/64 loss: -0.05933433771133423
Batch 32/64 loss: -0.05064362287521362
Batch 33/64 loss: -0.056687355041503906
Batch 34/64 loss: -0.06896209716796875
Batch 35/64 loss: -0.07336384057998657
Batch 36/64 loss: -0.06871914863586426
Batch 37/64 loss: -0.03829169273376465
Batch 38/64 loss: -0.055433034896850586
Batch 39/64 loss: -0.0401381254196167
Batch 40/64 loss: -0.03992903232574463
Batch 41/64 loss: -0.03423035144805908
Batch 42/64 loss: -0.0517040491104126
Batch 43/64 loss: -0.047797203063964844
Batch 44/64 loss: -0.07156598567962646
Batch 45/64 loss: -0.03825640678405762
Batch 46/64 loss: -0.05080759525299072
Batch 47/64 loss: -0.04824155569076538
Batch 48/64 loss: -0.06359684467315674
Batch 49/64 loss: -0.045641303062438965
Batch 50/64 loss: -0.046975791454315186
Batch 51/64 loss: -0.07292401790618896
Batch 52/64 loss: -0.053191304206848145
Batch 53/64 loss: -0.055185019969940186
Batch 54/64 loss: -0.06372380256652832
Batch 55/64 loss: -0.03974926471710205
Batch 56/64 loss: -0.05796217918395996
Batch 57/64 loss: -0.0446704626083374
Batch 58/64 loss: -0.04753941297531128
Batch 59/64 loss: -0.0558774471282959
Batch 60/64 loss: -0.05128300189971924
Batch 61/64 loss: -0.03202652931213379
Batch 62/64 loss: -0.030843913555145264
Batch 63/64 loss: -0.03589987754821777
Batch 64/64 loss: -0.052362799644470215
Epoch 463  Train loss: -0.054100844906825644  Val loss: 0.09899891610817402
Epoch 464
-------------------------------
Batch 1/64 loss: -0.04030036926269531
Batch 2/64 loss: -0.06529343128204346
Batch 3/64 loss: -0.04392039775848389
Batch 4/64 loss: -0.056143879890441895
Batch 5/64 loss: -0.05743759870529175
Batch 6/64 loss: -0.05225270986557007
Batch 7/64 loss: -0.03498542308807373
Batch 8/64 loss: -0.060834527015686035
Batch 9/64 loss: -0.0666424036026001
Batch 10/64 loss: -0.05862689018249512
Batch 11/64 loss: -0.047420382499694824
Batch 12/64 loss: -0.06386315822601318
Batch 13/64 loss: -0.06409883499145508
Batch 14/64 loss: -0.04880040884017944
Batch 15/64 loss: -0.0669291615486145
Batch 16/64 loss: -0.05578756332397461
Batch 17/64 loss: -0.02790510654449463
Batch 18/64 loss: -0.05324709415435791
Batch 19/64 loss: -0.0541345477104187
Batch 20/64 loss: -0.06529533863067627
Batch 21/64 loss: -0.03852486610412598
Batch 22/64 loss: -0.050536274909973145
Batch 23/64 loss: -0.0390201210975647
Batch 24/64 loss: -0.03905826807022095
Batch 25/64 loss: -0.07688283920288086
Batch 26/64 loss: -0.05023825168609619
Batch 27/64 loss: -0.048304975032806396
Batch 28/64 loss: -0.0693100094795227
Batch 29/64 loss: -0.03275442123413086
Batch 30/64 loss: -0.06953179836273193
Batch 31/64 loss: -0.05508166551589966
Batch 32/64 loss: -0.05447512865066528
Batch 33/64 loss: -0.06163012981414795
Batch 34/64 loss: -0.027982115745544434
Batch 35/64 loss: -0.05069935321807861
Batch 36/64 loss: -0.04750978946685791
Batch 37/64 loss: -0.05280435085296631
Batch 38/64 loss: -0.06496703624725342
Batch 39/64 loss: -0.05102962255477905
Batch 40/64 loss: -0.04999929666519165
Batch 41/64 loss: -0.05960047245025635
Batch 42/64 loss: -0.06126129627227783
Batch 43/64 loss: -0.07300853729248047
Batch 44/64 loss: -0.06703484058380127
Batch 45/64 loss: -0.042330801486968994
Batch 46/64 loss: -0.030367493629455566
Batch 47/64 loss: -0.03837764263153076
Batch 48/64 loss: -0.05723261833190918
Batch 49/64 loss: -0.05081582069396973
Batch 50/64 loss: -0.05312228202819824
Batch 51/64 loss: -0.047587037086486816
Batch 52/64 loss: -0.042060017585754395
Batch 53/64 loss: -0.06013643741607666
Batch 54/64 loss: -0.055836617946624756
Batch 55/64 loss: -0.0381547212600708
Batch 56/64 loss: -0.059076905250549316
Batch 57/64 loss: -0.05593615770339966
Batch 58/64 loss: -0.046462059020996094
Batch 59/64 loss: -0.05164867639541626
Batch 60/64 loss: -0.05109357833862305
Batch 61/64 loss: -0.026547551155090332
Batch 62/64 loss: -0.0604170560836792
Batch 63/64 loss: -0.05242729187011719
Batch 64/64 loss: -0.05344289541244507
Epoch 464  Train loss: -0.052311805884043376  Val loss: 0.09978864672257728
Epoch 465
-------------------------------
Batch 1/64 loss: -0.05610358715057373
Batch 2/64 loss: -0.05513966083526611
Batch 3/64 loss: -0.06010836362838745
Batch 4/64 loss: -0.0782618522644043
Batch 5/64 loss: -0.057630717754364014
Batch 6/64 loss: -0.060197532176971436
Batch 7/64 loss: -0.06284785270690918
Batch 8/64 loss: -0.030149340629577637
Batch 9/64 loss: -0.05087614059448242
Batch 10/64 loss: -0.06040215492248535
Batch 11/64 loss: -0.04049640893936157
Batch 12/64 loss: -0.036939144134521484
Batch 13/64 loss: -0.058524489402770996
Batch 14/64 loss: -0.06224977970123291
Batch 15/64 loss: -0.04849398136138916
Batch 16/64 loss: -0.037446796894073486
Batch 17/64 loss: -0.06384384632110596
Batch 18/64 loss: -0.05570608377456665
Batch 19/64 loss: -0.06005221605300903
Batch 20/64 loss: -0.05141627788543701
Batch 21/64 loss: -0.04395449161529541
Batch 22/64 loss: -0.05578768253326416
Batch 23/64 loss: -0.05699276924133301
Batch 24/64 loss: -0.06727313995361328
Batch 25/64 loss: -0.03370380401611328
Batch 26/64 loss: -0.05596888065338135
Batch 27/64 loss: -0.06355196237564087
Batch 28/64 loss: -0.02776390314102173
Batch 29/64 loss: -0.061606645584106445
Batch 30/64 loss: -0.050744593143463135
Batch 31/64 loss: -0.04882395267486572
Batch 32/64 loss: -0.04999476671218872
Batch 33/64 loss: -0.06582635641098022
Batch 34/64 loss: -0.04125797748565674
Batch 35/64 loss: -0.06662797927856445
Batch 36/64 loss: -0.06776118278503418
Batch 37/64 loss: -0.05481767654418945
Batch 38/64 loss: -0.06016749143600464
Batch 39/64 loss: -0.054031431674957275
Batch 40/64 loss: -0.04333984851837158
Batch 41/64 loss: -0.06628817319869995
Batch 42/64 loss: -0.05368804931640625
Batch 43/64 loss: -0.054352521896362305
Batch 44/64 loss: -0.05310070514678955
Batch 45/64 loss: -0.04837733507156372
Batch 46/64 loss: -0.06039494276046753
Batch 47/64 loss: -0.05082911252975464
Batch 48/64 loss: -0.0442885160446167
Batch 49/64 loss: -0.02523094415664673
Batch 50/64 loss: -0.06473731994628906
Batch 51/64 loss: -0.05798441171646118
Batch 52/64 loss: -0.06934565305709839
Batch 53/64 loss: -0.062660813331604
Batch 54/64 loss: -0.03563070297241211
Batch 55/64 loss: -0.07402557134628296
Batch 56/64 loss: -0.05021977424621582
Batch 57/64 loss: -0.053998470306396484
Batch 58/64 loss: -0.05475682020187378
Batch 59/64 loss: -0.05577731132507324
Batch 60/64 loss: -0.044668614864349365
Batch 61/64 loss: -0.04431641101837158
Batch 62/64 loss: -0.05892825126647949
Batch 63/64 loss: -0.04455673694610596
Batch 64/64 loss: -0.06430816650390625
Epoch 465  Train loss: -0.05385523216397155  Val loss: 0.10033051992200084
Epoch 466
-------------------------------
Batch 1/64 loss: -0.05057138204574585
Batch 2/64 loss: -0.05077004432678223
Batch 3/64 loss: -0.04334312677383423
Batch 4/64 loss: -0.05674922466278076
Batch 5/64 loss: -0.057765841484069824
Batch 6/64 loss: -0.05292230844497681
Batch 7/64 loss: -0.05360835790634155
Batch 8/64 loss: -0.06451696157455444
Batch 9/64 loss: -0.0580136775970459
Batch 10/64 loss: -0.04151129722595215
Batch 11/64 loss: -0.056725502014160156
Batch 12/64 loss: -0.055759310722351074
Batch 13/64 loss: -0.05953240394592285
Batch 14/64 loss: -0.02823120355606079
Batch 15/64 loss: -0.029877662658691406
Batch 16/64 loss: -0.06706565618515015
Batch 17/64 loss: -0.06227767467498779
Batch 18/64 loss: -0.050852179527282715
Batch 19/64 loss: -0.05491596460342407
Batch 20/64 loss: -0.0527271032333374
Batch 21/64 loss: -0.05407595634460449
Batch 22/64 loss: -0.06653136014938354
Batch 23/64 loss: -0.04701876640319824
Batch 24/64 loss: -0.03764617443084717
Batch 25/64 loss: -0.05396395921707153
Batch 26/64 loss: -0.0532536506652832
Batch 27/64 loss: -0.06340980529785156
Batch 28/64 loss: -0.055622875690460205
Batch 29/64 loss: -0.043755948543548584
Batch 30/64 loss: -0.04078429937362671
Batch 31/64 loss: -0.06517624855041504
Batch 32/64 loss: -0.0586545467376709
Batch 33/64 loss: -0.0483514666557312
Batch 34/64 loss: -0.06529092788696289
Batch 35/64 loss: -0.0668298602104187
Batch 36/64 loss: -0.040317416191101074
Batch 37/64 loss: -0.06966716051101685
Batch 38/64 loss: -0.057962000370025635
Batch 39/64 loss: -0.07136309146881104
Batch 40/64 loss: -0.044276416301727295
Batch 41/64 loss: -0.06362831592559814
Batch 42/64 loss: -0.06868386268615723
Batch 43/64 loss: -0.0602259635925293
Batch 44/64 loss: -0.07423746585845947
Batch 45/64 loss: -0.01904928684234619
Batch 46/64 loss: -0.07622802257537842
Batch 47/64 loss: -0.07370215654373169
Batch 48/64 loss: -0.06128150224685669
Batch 49/64 loss: -0.05006891489028931
Batch 50/64 loss: -0.042719244956970215
Batch 51/64 loss: -0.06400036811828613
Batch 52/64 loss: -0.04776257276535034
Batch 53/64 loss: -0.042356908321380615
Batch 54/64 loss: -0.039817214012145996
Batch 55/64 loss: -0.025443732738494873
Batch 56/64 loss: -0.06916403770446777
Batch 57/64 loss: -0.03404355049133301
Batch 58/64 loss: -0.06546258926391602
Batch 59/64 loss: -0.05018800497055054
Batch 60/64 loss: -0.0470503568649292
Batch 61/64 loss: -0.05536186695098877
Batch 62/64 loss: -0.04877901077270508
Batch 63/64 loss: -0.06776154041290283
Batch 64/64 loss: -0.05101650953292847
Epoch 466  Train loss: -0.05391321860107721  Val loss: 0.0997625753232294
Epoch 467
-------------------------------
Batch 1/64 loss: -0.039385974407196045
Batch 2/64 loss: -0.04809194803237915
Batch 3/64 loss: -0.05926698446273804
Batch 4/64 loss: -0.06425076723098755
Batch 5/64 loss: -0.06816685199737549
Batch 6/64 loss: -0.048069119453430176
Batch 7/64 loss: -0.05084967613220215
Batch 8/64 loss: -0.061228930950164795
Batch 9/64 loss: -0.07483577728271484
Batch 10/64 loss: -0.03733837604522705
Batch 11/64 loss: -0.07256579399108887
Batch 12/64 loss: -0.0678243637084961
Batch 13/64 loss: -0.05883359909057617
Batch 14/64 loss: -0.054843902587890625
Batch 15/64 loss: -0.03524136543273926
Batch 16/64 loss: -0.0572507381439209
Batch 17/64 loss: -0.03275465965270996
Batch 18/64 loss: -0.06123363971710205
Batch 19/64 loss: -0.0482558012008667
Batch 20/64 loss: -0.062354445457458496
Batch 21/64 loss: -0.07197314500808716
Batch 22/64 loss: -0.04931062459945679
Batch 23/64 loss: -0.06158292293548584
Batch 24/64 loss: -0.04811882972717285
Batch 25/64 loss: -0.03671497106552124
Batch 26/64 loss: -0.05398857593536377
Batch 27/64 loss: -0.056709229946136475
Batch 28/64 loss: -0.05873602628707886
Batch 29/64 loss: -0.03606116771697998
Batch 30/64 loss: -0.04707688093185425
Batch 31/64 loss: -0.04627341032028198
Batch 32/64 loss: -0.05890697240829468
Batch 33/64 loss: -0.06203538179397583
Batch 34/64 loss: -0.04035806655883789
Batch 35/64 loss: -0.043125927448272705
Batch 36/64 loss: -0.06657862663269043
Batch 37/64 loss: -0.0662682056427002
Batch 38/64 loss: -0.038424789905548096
Batch 39/64 loss: -0.06022846698760986
Batch 40/64 loss: -0.054899752140045166
Batch 41/64 loss: -0.04570269584655762
Batch 42/64 loss: -0.05251950025558472
Batch 43/64 loss: -0.053549349308013916
Batch 44/64 loss: -0.06669366359710693
Batch 45/64 loss: -0.06348562240600586
Batch 46/64 loss: -0.058777570724487305
Batch 47/64 loss: -0.05926942825317383
Batch 48/64 loss: -0.06164729595184326
Batch 49/64 loss: -0.03874194622039795
Batch 50/64 loss: -0.035515427589416504
Batch 51/64 loss: -0.04619652032852173
Batch 52/64 loss: -0.06185448169708252
Batch 53/64 loss: -0.045126259326934814
Batch 54/64 loss: -0.056405842304229736
Batch 55/64 loss: -0.06767600774765015
Batch 56/64 loss: -0.04779183864593506
Batch 57/64 loss: -0.037510454654693604
Batch 58/64 loss: -0.06088954210281372
Batch 59/64 loss: -0.0316925048828125
Batch 60/64 loss: -0.057000577449798584
Batch 61/64 loss: -0.05409961938858032
Batch 62/64 loss: -0.07018446922302246
Batch 63/64 loss: -0.057775676250457764
Batch 64/64 loss: -0.06019514799118042
Epoch 467  Train loss: -0.05388654657438689  Val loss: 0.09731444398971767
Epoch 468
-------------------------------
Batch 1/64 loss: -0.04886674880981445
Batch 2/64 loss: -0.05879640579223633
Batch 3/64 loss: -0.05558061599731445
Batch 4/64 loss: -0.07023614645004272
Batch 5/64 loss: -0.055570125579833984
Batch 6/64 loss: -0.07269090414047241
Batch 7/64 loss: -0.033097147941589355
Batch 8/64 loss: -0.07336926460266113
Batch 9/64 loss: -0.07094520330429077
Batch 10/64 loss: -0.055738985538482666
Batch 11/64 loss: -0.06024599075317383
Batch 12/64 loss: -0.06673216819763184
Batch 13/64 loss: -0.04561132192611694
Batch 14/64 loss: -0.05727994441986084
Batch 15/64 loss: -0.03157544136047363
Batch 16/64 loss: -0.05450558662414551
Batch 17/64 loss: -0.0594293475151062
Batch 18/64 loss: -0.07449787855148315
Batch 19/64 loss: -0.05603969097137451
Batch 20/64 loss: -0.04921180009841919
Batch 21/64 loss: -0.05462813377380371
Batch 22/64 loss: -0.05172169208526611
Batch 23/64 loss: -0.037366390228271484
Batch 24/64 loss: -0.06745094060897827
Batch 25/64 loss: -0.07525604963302612
Batch 26/64 loss: -0.06231600046157837
Batch 27/64 loss: -0.022297382354736328
Batch 28/64 loss: -0.04019761085510254
Batch 29/64 loss: -0.05586796998977661
Batch 30/64 loss: -0.06907522678375244
Batch 31/64 loss: -0.047788918018341064
Batch 32/64 loss: -0.05868023633956909
Batch 33/64 loss: -0.0675652027130127
Batch 34/64 loss: -0.02946949005126953
Batch 35/64 loss: -0.04592108726501465
Batch 36/64 loss: -0.04474437236785889
Batch 37/64 loss: -0.06459230184555054
Batch 38/64 loss: -0.07043194770812988
Batch 39/64 loss: -0.0674782395362854
Batch 40/64 loss: -0.0494387149810791
Batch 41/64 loss: -0.04032379388809204
Batch 42/64 loss: -0.035187482833862305
Batch 43/64 loss: -0.07491838932037354
Batch 44/64 loss: -0.062298715114593506
Batch 45/64 loss: -0.054599881172180176
Batch 46/64 loss: -0.0646544098854065
Batch 47/64 loss: -0.04945218563079834
Batch 48/64 loss: -0.05730545520782471
Batch 49/64 loss: -0.06703841686248779
Batch 50/64 loss: -0.07634907960891724
Batch 51/64 loss: -0.05027848482131958
Batch 52/64 loss: -0.049669861793518066
Batch 53/64 loss: -0.06429159641265869
Batch 54/64 loss: -0.06223404407501221
Batch 55/64 loss: -0.028310537338256836
Batch 56/64 loss: -0.044988393783569336
Batch 57/64 loss: -0.0648851990699768
Batch 58/64 loss: -0.062244296073913574
Batch 59/64 loss: -0.04053908586502075
Batch 60/64 loss: -0.06895864009857178
Batch 61/64 loss: -0.06900054216384888
Batch 62/64 loss: -0.06996321678161621
Batch 63/64 loss: -0.049617886543273926
Batch 64/64 loss: -0.04663032293319702
Epoch 468  Train loss: -0.056068877846586936  Val loss: 0.09982511636727455
Epoch 469
-------------------------------
Batch 1/64 loss: -0.06698983907699585
Batch 2/64 loss: -0.06749558448791504
Batch 3/64 loss: -0.07143598794937134
Batch 4/64 loss: -0.06827199459075928
Batch 5/64 loss: -0.06502389907836914
Batch 6/64 loss: -0.06577622890472412
Batch 7/64 loss: -0.06699913740158081
Batch 8/64 loss: -0.05992317199707031
Batch 9/64 loss: -0.057872653007507324
Batch 10/64 loss: -0.06729739904403687
Batch 11/64 loss: -0.04590028524398804
Batch 12/64 loss: -0.057568252086639404
Batch 13/64 loss: -0.0416717529296875
Batch 14/64 loss: -0.04894822835922241
Batch 15/64 loss: -0.058024823665618896
Batch 16/64 loss: -0.06854301691055298
Batch 17/64 loss: -0.056407809257507324
Batch 18/64 loss: -0.038886845111846924
Batch 19/64 loss: -0.07351559400558472
Batch 20/64 loss: -0.0623553991317749
Batch 21/64 loss: -0.07893991470336914
Batch 22/64 loss: -0.05983096361160278
Batch 23/64 loss: -0.05859827995300293
Batch 24/64 loss: -0.041552603244781494
Batch 25/64 loss: -0.049317240715026855
Batch 26/64 loss: -0.05409848690032959
Batch 27/64 loss: -0.051186561584472656
Batch 28/64 loss: -0.0537877082824707
Batch 29/64 loss: -0.060328900814056396
Batch 30/64 loss: -0.0503230094909668
Batch 31/64 loss: -0.06715887784957886
Batch 32/64 loss: -0.06474423408508301
Batch 33/64 loss: -0.06871724128723145
Batch 34/64 loss: -0.06275761127471924
Batch 35/64 loss: -0.046658337116241455
Batch 36/64 loss: -0.05396181344985962
Batch 37/64 loss: -0.053571999073028564
Batch 38/64 loss: -0.051123738288879395
Batch 39/64 loss: -0.06981456279754639
Batch 40/64 loss: -0.05023980140686035
Batch 41/64 loss: -0.059288620948791504
Batch 42/64 loss: -0.06614381074905396
Batch 43/64 loss: -0.03508126735687256
Batch 44/64 loss: -0.05139201879501343
Batch 45/64 loss: -0.06524813175201416
Batch 46/64 loss: -0.051334500312805176
Batch 47/64 loss: -0.0563051700592041
Batch 48/64 loss: -0.07083547115325928
Batch 49/64 loss: -0.00998997688293457
Batch 50/64 loss: -0.03922116756439209
Batch 51/64 loss: -0.02776426076889038
Batch 52/64 loss: -0.06714212894439697
Batch 53/64 loss: -0.051087796688079834
Batch 54/64 loss: -0.04457777738571167
Batch 55/64 loss: -0.054171085357666016
Batch 56/64 loss: -0.060555458068847656
Batch 57/64 loss: -0.05855518579483032
Batch 58/64 loss: -0.055567026138305664
Batch 59/64 loss: -0.004042625427246094
Batch 60/64 loss: -0.044165849685668945
Batch 61/64 loss: -0.042078495025634766
Batch 62/64 loss: -0.06791907548904419
Batch 63/64 loss: -0.06161874532699585
Batch 64/64 loss: -0.050567030906677246
Epoch 469  Train loss: -0.055492246384714165  Val loss: 0.09858538358891543
Epoch 470
-------------------------------
Batch 1/64 loss: -0.04950845241546631
Batch 2/64 loss: -0.0610811710357666
Batch 3/64 loss: -0.061436474323272705
Batch 4/64 loss: -0.06492960453033447
Batch 5/64 loss: -0.04932647943496704
Batch 6/64 loss: -0.057960569858551025
Batch 7/64 loss: -0.07592260837554932
Batch 8/64 loss: -0.048139989376068115
Batch 9/64 loss: -0.06180310249328613
Batch 10/64 loss: -0.05599290132522583
Batch 11/64 loss: -0.06404829025268555
Batch 12/64 loss: -0.06796342134475708
Batch 13/64 loss: -0.06646651029586792
Batch 14/64 loss: -0.05100560188293457
Batch 15/64 loss: -0.057704031467437744
Batch 16/64 loss: -0.07500565052032471
Batch 17/64 loss: -0.04318517446517944
Batch 18/64 loss: -0.06535911560058594
Batch 19/64 loss: -0.07259565591812134
Batch 20/64 loss: -0.045572638511657715
Batch 21/64 loss: -0.0414695143699646
Batch 22/64 loss: -0.06942808628082275
Batch 23/64 loss: -0.06635373830795288
Batch 24/64 loss: -0.04224759340286255
Batch 25/64 loss: -0.07141506671905518
Batch 26/64 loss: -0.03421908617019653
Batch 27/64 loss: -0.06858843564987183
Batch 28/64 loss: -0.05498164892196655
Batch 29/64 loss: -0.055140674114227295
Batch 30/64 loss: -0.05451840162277222
Batch 31/64 loss: -0.059471070766448975
Batch 32/64 loss: -0.05909329652786255
Batch 33/64 loss: -0.058666229248046875
Batch 34/64 loss: -0.03196972608566284
Batch 35/64 loss: -0.06464189291000366
Batch 36/64 loss: -0.06522935628890991
Batch 37/64 loss: -0.06586360931396484
Batch 38/64 loss: -0.052704453468322754
Batch 39/64 loss: -0.05294305086135864
Batch 40/64 loss: -0.04061782360076904
Batch 41/64 loss: -0.07369434833526611
Batch 42/64 loss: -0.052256882190704346
Batch 43/64 loss: -0.04278939962387085
Batch 44/64 loss: -0.04476737976074219
Batch 45/64 loss: -0.05775439739227295
Batch 46/64 loss: -0.06288933753967285
Batch 47/64 loss: -0.0439031720161438
Batch 48/64 loss: -0.0647011399269104
Batch 49/64 loss: -0.047817885875701904
Batch 50/64 loss: -0.05996042490005493
Batch 51/64 loss: -0.06529384851455688
Batch 52/64 loss: -0.07779616117477417
Batch 53/64 loss: -0.06730717420578003
Batch 54/64 loss: -0.06400775909423828
Batch 55/64 loss: -0.07400637865066528
Batch 56/64 loss: -0.04906731843948364
Batch 57/64 loss: -0.04300117492675781
Batch 58/64 loss: -0.06256633996963501
Batch 59/64 loss: -0.05669277906417847
Batch 60/64 loss: -0.05197727680206299
Batch 61/64 loss: -0.049826741218566895
Batch 62/64 loss: -0.05403929948806763
Batch 63/64 loss: -0.07293832302093506
Batch 64/64 loss: -0.05701667070388794
Epoch 470  Train loss: -0.0578256885210673  Val loss: 0.10045952321737493
Epoch 471
-------------------------------
Batch 1/64 loss: -0.06265765428543091
Batch 2/64 loss: -0.04276639223098755
Batch 3/64 loss: -0.043724238872528076
Batch 4/64 loss: -0.053272783756256104
Batch 5/64 loss: -0.0524897575378418
Batch 6/64 loss: -0.056361377239227295
Batch 7/64 loss: -0.07086127996444702
Batch 8/64 loss: -0.03324800729751587
Batch 9/64 loss: -0.0626366138458252
Batch 10/64 loss: -0.07552140951156616
Batch 11/64 loss: -0.04819846153259277
Batch 12/64 loss: -0.04855167865753174
Batch 13/64 loss: -0.06869697570800781
Batch 14/64 loss: -0.059122562408447266
Batch 15/64 loss: -0.07739454507827759
Batch 16/64 loss: -0.05527764558792114
Batch 17/64 loss: -0.05901116132736206
Batch 18/64 loss: -0.03952699899673462
Batch 19/64 loss: -0.06424093246459961
Batch 20/64 loss: -0.05671191215515137
Batch 21/64 loss: -0.0687340497970581
Batch 22/64 loss: -0.03533750772476196
Batch 23/64 loss: -0.05653411149978638
Batch 24/64 loss: -0.07467448711395264
Batch 25/64 loss: -0.07324421405792236
Batch 26/64 loss: -0.05697154998779297
Batch 27/64 loss: -0.0781676173210144
Batch 28/64 loss: -0.04522973299026489
Batch 29/64 loss: -0.08206450939178467
Batch 30/64 loss: -0.051090776920318604
Batch 31/64 loss: -0.07412469387054443
Batch 32/64 loss: -0.049864351749420166
Batch 33/64 loss: -0.063812255859375
Batch 34/64 loss: -0.06516444683074951
Batch 35/64 loss: -0.050282299518585205
Batch 36/64 loss: -0.0450553297996521
Batch 37/64 loss: -0.05289113521575928
Batch 38/64 loss: -0.06220734119415283
Batch 39/64 loss: -0.06807750463485718
Batch 40/64 loss: -0.05187183618545532
Batch 41/64 loss: -0.058913588523864746
Batch 42/64 loss: -0.06262671947479248
Batch 43/64 loss: -0.061432600021362305
Batch 44/64 loss: -0.051412105560302734
Batch 45/64 loss: -0.05124884843826294
Batch 46/64 loss: -0.0393601655960083
Batch 47/64 loss: -0.059111833572387695
Batch 48/64 loss: -0.058722853660583496
Batch 49/64 loss: -0.07481217384338379
Batch 50/64 loss: -0.07354128360748291
Batch 51/64 loss: -0.05795407295227051
Batch 52/64 loss: -0.04130101203918457
Batch 53/64 loss: -0.06907206773757935
Batch 54/64 loss: -0.042086005210876465
Batch 55/64 loss: -0.0641179084777832
Batch 56/64 loss: -0.05256998538970947
Batch 57/64 loss: -0.06653988361358643
Batch 58/64 loss: -0.05625784397125244
Batch 59/64 loss: -0.06460261344909668
Batch 60/64 loss: -0.06812512874603271
Batch 61/64 loss: -0.06278711557388306
Batch 62/64 loss: -0.06019413471221924
Batch 63/64 loss: -0.0634307861328125
Batch 64/64 loss: -0.06587731838226318
Epoch 471  Train loss: -0.058749817399417655  Val loss: 0.10088052085994445
Epoch 472
-------------------------------
Batch 1/64 loss: -0.05663114786148071
Batch 2/64 loss: -0.0713733434677124
Batch 3/64 loss: -0.06479012966156006
Batch 4/64 loss: -0.0676567554473877
Batch 5/64 loss: -0.0511629581451416
Batch 6/64 loss: -0.05321317911148071
Batch 7/64 loss: -0.0674811601638794
Batch 8/64 loss: -0.059996843338012695
Batch 9/64 loss: -0.06966692209243774
Batch 10/64 loss: -0.062032461166381836
Batch 11/64 loss: -0.04140836000442505
Batch 12/64 loss: -0.04947972297668457
Batch 13/64 loss: -0.06530952453613281
Batch 14/64 loss: -0.05345141887664795
Batch 15/64 loss: -0.06885969638824463
Batch 16/64 loss: -0.07258570194244385
Batch 17/64 loss: -0.06082433462142944
Batch 18/64 loss: -0.06758922338485718
Batch 19/64 loss: -0.07196015119552612
Batch 20/64 loss: -0.06221652030944824
Batch 21/64 loss: -0.06357604265213013
Batch 22/64 loss: -0.07559096813201904
Batch 23/64 loss: -0.03394430875778198
Batch 24/64 loss: -0.049618542194366455
Batch 25/64 loss: -0.053526461124420166
Batch 26/64 loss: -0.063090980052948
Batch 27/64 loss: -0.030471324920654297
Batch 28/64 loss: -0.05828148126602173
Batch 29/64 loss: -0.06454557180404663
Batch 30/64 loss: -0.06612712144851685
Batch 31/64 loss: -0.06943392753601074
Batch 32/64 loss: -0.05790609121322632
Batch 33/64 loss: -0.0753774642944336
Batch 34/64 loss: -0.06351935863494873
Batch 35/64 loss: -0.06390750408172607
Batch 36/64 loss: -0.07274532318115234
Batch 37/64 loss: -0.07088720798492432
Batch 38/64 loss: -0.05969792604446411
Batch 39/64 loss: -0.07110649347305298
Batch 40/64 loss: -0.06641030311584473
Batch 41/64 loss: -0.062488436698913574
Batch 42/64 loss: -0.05383354425430298
Batch 43/64 loss: -0.05542105436325073
Batch 44/64 loss: -0.07285851240158081
Batch 45/64 loss: -0.06429040431976318
Batch 46/64 loss: -0.05301100015640259
Batch 47/64 loss: -0.06275677680969238
Batch 48/64 loss: -0.07445693016052246
Batch 49/64 loss: -0.06002211570739746
Batch 50/64 loss: -0.05994594097137451
Batch 51/64 loss: -0.05726999044418335
Batch 52/64 loss: -0.060871422290802
Batch 53/64 loss: -0.05469387769699097
Batch 54/64 loss: -0.04821741580963135
Batch 55/64 loss: -0.06375986337661743
Batch 56/64 loss: -0.06247204542160034
Batch 57/64 loss: -0.04668456315994263
Batch 58/64 loss: -0.06160241365432739
Batch 59/64 loss: -0.03895670175552368
Batch 60/64 loss: -0.04203742742538452
Batch 61/64 loss: -0.07379293441772461
Batch 62/64 loss: -0.04932612180709839
Batch 63/64 loss: -0.06448304653167725
Batch 64/64 loss: -0.039301395416259766
Epoch 472  Train loss: -0.060238157534131816  Val loss: 0.10136544724919952
Epoch 473
-------------------------------
Batch 1/64 loss: -0.0674511194229126
Batch 2/64 loss: -0.07274883985519409
Batch 3/64 loss: -0.06139814853668213
Batch 4/64 loss: -0.0500677227973938
Batch 5/64 loss: -0.027216553688049316
Batch 6/64 loss: -0.07483500242233276
Batch 7/64 loss: -0.056668639183044434
Batch 8/64 loss: -0.07264667749404907
Batch 9/64 loss: -0.06375038623809814
Batch 10/64 loss: -0.05823582410812378
Batch 11/64 loss: -0.06353819370269775
Batch 12/64 loss: -0.04969930648803711
Batch 13/64 loss: -0.05088621377944946
Batch 14/64 loss: -0.08024972677230835
Batch 15/64 loss: -0.053174614906311035
Batch 16/64 loss: -0.07339918613433838
Batch 17/64 loss: -0.03950101137161255
Batch 18/64 loss: -0.056617558002471924
Batch 19/64 loss: -0.06366264820098877
Batch 20/64 loss: -0.049072980880737305
Batch 21/64 loss: -0.058788001537323
Batch 22/64 loss: -0.06286996603012085
Batch 23/64 loss: -0.06735348701477051
Batch 24/64 loss: -0.07627648115158081
Batch 25/64 loss: -0.06220996379852295
Batch 26/64 loss: -0.06352609395980835
Batch 27/64 loss: -0.04566192626953125
Batch 28/64 loss: -0.06633317470550537
Batch 29/64 loss: -0.03006845712661743
Batch 30/64 loss: -0.061851680278778076
Batch 31/64 loss: -0.06311827898025513
Batch 32/64 loss: -0.05077219009399414
Batch 33/64 loss: -0.05048525333404541
Batch 34/64 loss: -0.05882072448730469
Batch 35/64 loss: -0.07244563102722168
Batch 36/64 loss: -0.05975484848022461
Batch 37/64 loss: -0.06859052181243896
Batch 38/64 loss: -0.06724530458450317
Batch 39/64 loss: -0.050004780292510986
Batch 40/64 loss: -0.05777394771575928
Batch 41/64 loss: -0.07349139451980591
Batch 42/64 loss: -0.07103472948074341
Batch 43/64 loss: -0.06511843204498291
Batch 44/64 loss: -0.05452728271484375
Batch 45/64 loss: -0.07269704341888428
Batch 46/64 loss: -0.062043607234954834
Batch 47/64 loss: -0.06858950853347778
Batch 48/64 loss: -0.05357944965362549
Batch 49/64 loss: -0.05409562587738037
Batch 50/64 loss: -0.04163408279418945
Batch 51/64 loss: -0.06774032115936279
Batch 52/64 loss: -0.06056356430053711
Batch 53/64 loss: -0.040908217430114746
Batch 54/64 loss: -0.05823260545730591
Batch 55/64 loss: -0.07352906465530396
Batch 56/64 loss: -0.03631997108459473
Batch 57/64 loss: -0.053816139698028564
Batch 58/64 loss: -0.05348193645477295
Batch 59/64 loss: -0.05129116773605347
Batch 60/64 loss: -0.03885620832443237
Batch 61/64 loss: -0.042636334896087646
Batch 62/64 loss: -0.0510408878326416
Batch 63/64 loss: -0.05127429962158203
Batch 64/64 loss: -0.042813003063201904
Epoch 473  Train loss: -0.05815502264920403  Val loss: 0.09973948046923503
Epoch 474
-------------------------------
Batch 1/64 loss: -0.0725327730178833
Batch 2/64 loss: -0.047852277755737305
Batch 3/64 loss: -0.052702486515045166
Batch 4/64 loss: -0.06463193893432617
Batch 5/64 loss: -0.04850345849990845
Batch 6/64 loss: -0.044130802154541016
Batch 7/64 loss: -0.038398921489715576
Batch 8/64 loss: -0.054458439350128174
Batch 9/64 loss: -0.05835956335067749
Batch 10/64 loss: -0.06207764148712158
Batch 11/64 loss: -0.05188077688217163
Batch 12/64 loss: -0.044742584228515625
Batch 13/64 loss: -0.06249898672103882
Batch 14/64 loss: -0.06313848495483398
Batch 15/64 loss: -0.04308575391769409
Batch 16/64 loss: -0.04371589422225952
Batch 17/64 loss: -0.07183128595352173
Batch 18/64 loss: -0.037359416484832764
Batch 19/64 loss: -0.049691200256347656
Batch 20/64 loss: -0.0555872917175293
Batch 21/64 loss: -0.06150531768798828
Batch 22/64 loss: -0.05990785360336304
Batch 23/64 loss: -0.0320131778717041
Batch 24/64 loss: -0.06095761060714722
Batch 25/64 loss: -0.055385470390319824
Batch 26/64 loss: -0.05897641181945801
Batch 27/64 loss: -0.06234121322631836
Batch 28/64 loss: -0.026453495025634766
Batch 29/64 loss: -0.0409809947013855
Batch 30/64 loss: -0.060283362865448
Batch 31/64 loss: -0.04718095064163208
Batch 32/64 loss: -0.05711948871612549
Batch 33/64 loss: -0.03861856460571289
Batch 34/64 loss: -0.05231708288192749
Batch 35/64 loss: -0.023644089698791504
Batch 36/64 loss: -0.05192220211029053
Batch 37/64 loss: -0.018453001976013184
Batch 38/64 loss: -0.015365123748779297
Batch 39/64 loss: -0.04821193218231201
Batch 40/64 loss: -0.061440467834472656
Batch 41/64 loss: -0.05198109149932861
Batch 42/64 loss: -0.059985339641571045
Batch 43/64 loss: -0.04150545597076416
Batch 44/64 loss: -0.03928804397583008
Batch 45/64 loss: -0.0556219220161438
Batch 46/64 loss: -0.06608778238296509
Batch 47/64 loss: -0.06760287284851074
Batch 48/64 loss: -0.05869102478027344
Batch 49/64 loss: -0.05226486921310425
Batch 50/64 loss: -0.047833383083343506
Batch 51/64 loss: -0.05380237102508545
Batch 52/64 loss: -0.06106299161911011
Batch 53/64 loss: -0.03819394111633301
Batch 54/64 loss: -0.039502084255218506
Batch 55/64 loss: -0.06745874881744385
Batch 56/64 loss: -0.05626499652862549
Batch 57/64 loss: -0.06661814451217651
Batch 58/64 loss: -0.016668975353240967
Batch 59/64 loss: -0.05095183849334717
Batch 60/64 loss: -0.06175363063812256
Batch 61/64 loss: -0.06037956476211548
Batch 62/64 loss: -0.03984367847442627
Batch 63/64 loss: -0.057900965213775635
Batch 64/64 loss: -0.05402040481567383
Epoch 474  Train loss: -0.05101234211641199  Val loss: 0.09900393027210563
Epoch 475
-------------------------------
Batch 1/64 loss: -0.07558584213256836
Batch 2/64 loss: -0.05933058261871338
Batch 3/64 loss: -0.04015171527862549
Batch 4/64 loss: -0.06848502159118652
Batch 5/64 loss: -0.06519967317581177
Batch 6/64 loss: -0.042796313762664795
Batch 7/64 loss: -0.06385594606399536
Batch 8/64 loss: -0.03050011396408081
Batch 9/64 loss: -0.06395184993743896
Batch 10/64 loss: -0.06672519445419312
Batch 11/64 loss: -0.07367217540740967
Batch 12/64 loss: -0.05361908674240112
Batch 13/64 loss: -0.062159061431884766
Batch 14/64 loss: -0.03753453493118286
Batch 15/64 loss: -0.029320478439331055
Batch 16/64 loss: -0.04871237277984619
Batch 17/64 loss: -0.04091709852218628
Batch 18/64 loss: -0.04169785976409912
Batch 19/64 loss: -0.06279122829437256
Batch 20/64 loss: -0.06977760791778564
Batch 21/64 loss: -0.05619746446609497
Batch 22/64 loss: -0.04591411352157593
Batch 23/64 loss: -0.056771278381347656
Batch 24/64 loss: -0.05300450325012207
Batch 25/64 loss: -0.06634265184402466
Batch 26/64 loss: -0.041630446910858154
Batch 27/64 loss: -0.028184831142425537
Batch 28/64 loss: -0.06457275152206421
Batch 29/64 loss: -0.06846970319747925
Batch 30/64 loss: -0.05072319507598877
Batch 31/64 loss: -0.06525146961212158
Batch 32/64 loss: -0.0662696361541748
Batch 33/64 loss: -0.05764412879943848
Batch 34/64 loss: -0.05245941877365112
Batch 35/64 loss: -0.06119334697723389
Batch 36/64 loss: -0.056336820125579834
Batch 37/64 loss: -0.06483322381973267
Batch 38/64 loss: -0.05349373817443848
Batch 39/64 loss: -0.060043513774871826
Batch 40/64 loss: -0.03472226858139038
Batch 41/64 loss: -0.06181401014328003
Batch 42/64 loss: -0.057389140129089355
Batch 43/64 loss: -0.054262638092041016
Batch 44/64 loss: -0.05648988485336304
Batch 45/64 loss: -0.06441932916641235
Batch 46/64 loss: -0.06687551736831665
Batch 47/64 loss: -0.04302835464477539
Batch 48/64 loss: -0.04971623420715332
Batch 49/64 loss: -0.03619062900543213
Batch 50/64 loss: -0.04823589324951172
Batch 51/64 loss: -0.055760324001312256
Batch 52/64 loss: -0.07110267877578735
Batch 53/64 loss: -0.032945215702056885
Batch 54/64 loss: -0.05867326259613037
Batch 55/64 loss: -0.0503653883934021
Batch 56/64 loss: -0.048190951347351074
Batch 57/64 loss: -0.07617759704589844
Batch 58/64 loss: -0.061917901039123535
Batch 59/64 loss: -0.04468107223510742
Batch 60/64 loss: -0.06069988012313843
Batch 61/64 loss: -0.04474806785583496
Batch 62/64 loss: -0.05972111225128174
Batch 63/64 loss: -0.05553793907165527
Batch 64/64 loss: -0.055428922176361084
Epoch 475  Train loss: -0.05492324665480969  Val loss: 0.10028604195290006
Epoch 476
-------------------------------
Batch 1/64 loss: -0.05949169397354126
Batch 2/64 loss: -0.07802760601043701
Batch 3/64 loss: -0.05665779113769531
Batch 4/64 loss: -0.06984525918960571
Batch 5/64 loss: -0.05173051357269287
Batch 6/64 loss: -0.0602419376373291
Batch 7/64 loss: -0.08038610219955444
Batch 8/64 loss: -0.06749606132507324
Batch 9/64 loss: -0.07199496030807495
Batch 10/64 loss: -0.06373792886734009
Batch 11/64 loss: -0.05816906690597534
Batch 12/64 loss: -0.05730879306793213
Batch 13/64 loss: -0.04120326042175293
Batch 14/64 loss: -0.066841721534729
Batch 15/64 loss: -0.05560934543609619
Batch 16/64 loss: -0.047068119049072266
Batch 17/64 loss: -0.055391132831573486
Batch 18/64 loss: -0.055765509605407715
Batch 19/64 loss: -0.06818372011184692
Batch 20/64 loss: -0.03769195079803467
Batch 21/64 loss: -0.06093496084213257
Batch 22/64 loss: -0.07051748037338257
Batch 23/64 loss: -0.059736669063568115
Batch 24/64 loss: -0.06275069713592529
Batch 25/64 loss: -0.06345903873443604
Batch 26/64 loss: -0.07451832294464111
Batch 27/64 loss: -0.05564248561859131
Batch 28/64 loss: -0.04938364028930664
Batch 29/64 loss: -0.06862694025039673
Batch 30/64 loss: -0.05536121129989624
Batch 31/64 loss: -0.057708024978637695
Batch 32/64 loss: -0.05240505933761597
Batch 33/64 loss: -0.049497246742248535
Batch 34/64 loss: -0.06474471092224121
Batch 35/64 loss: -0.06854921579360962
Batch 36/64 loss: -0.05618774890899658
Batch 37/64 loss: -0.07958447933197021
Batch 38/64 loss: -0.05046921968460083
Batch 39/64 loss: -0.051548004150390625
Batch 40/64 loss: -0.06687730550765991
Batch 41/64 loss: -0.06405586004257202
Batch 42/64 loss: -0.05609869956970215
Batch 43/64 loss: -0.06269049644470215
Batch 44/64 loss: -0.06805598735809326
Batch 45/64 loss: -0.04970669746398926
Batch 46/64 loss: -0.035564959049224854
Batch 47/64 loss: -0.06609666347503662
Batch 48/64 loss: -0.057872772216796875
Batch 49/64 loss: -0.05612027645111084
Batch 50/64 loss: -0.05690741539001465
Batch 51/64 loss: -0.05654329061508179
Batch 52/64 loss: -0.05305194854736328
Batch 53/64 loss: -0.06259548664093018
Batch 54/64 loss: -0.07305711507797241
Batch 55/64 loss: -0.03719806671142578
Batch 56/64 loss: -0.03761112689971924
Batch 57/64 loss: -0.05186796188354492
Batch 58/64 loss: -0.06068432331085205
Batch 59/64 loss: -0.04251307249069214
Batch 60/64 loss: -0.06899315118789673
Batch 61/64 loss: -0.051838040351867676
Batch 62/64 loss: -0.07580524682998657
Batch 63/64 loss: -0.06120777130126953
Batch 64/64 loss: -0.0463680624961853
Epoch 476  Train loss: -0.05901580254236857  Val loss: 0.10017178886125178
Epoch 477
-------------------------------
Batch 1/64 loss: -0.06214892864227295
Batch 2/64 loss: -0.06224548816680908
Batch 3/64 loss: -0.05552095174789429
Batch 4/64 loss: -0.06645160913467407
Batch 5/64 loss: -0.07064145803451538
Batch 6/64 loss: -0.06171351671218872
Batch 7/64 loss: -0.06612074375152588
Batch 8/64 loss: -0.06443268060684204
Batch 9/64 loss: -0.07497477531433105
Batch 10/64 loss: -0.04606735706329346
Batch 11/64 loss: -0.053787291049957275
Batch 12/64 loss: -0.07966411113739014
Batch 13/64 loss: -0.059223830699920654
Batch 14/64 loss: -0.07265251874923706
Batch 15/64 loss: -0.02099740505218506
Batch 16/64 loss: -0.06510788202285767
Batch 17/64 loss: -0.07340890169143677
Batch 18/64 loss: -0.05905580520629883
Batch 19/64 loss: -0.05746924877166748
Batch 20/64 loss: -0.0682905912399292
Batch 21/64 loss: -0.06158357858657837
Batch 22/64 loss: -0.06792342662811279
Batch 23/64 loss: -0.05190020799636841
Batch 24/64 loss: -0.0482715368270874
Batch 25/64 loss: -0.06382685899734497
Batch 26/64 loss: -0.07429999113082886
Batch 27/64 loss: -0.06958591938018799
Batch 28/64 loss: -0.054450809955596924
Batch 29/64 loss: -0.06568753719329834
Batch 30/64 loss: -0.04578322172164917
Batch 31/64 loss: -0.07310819625854492
Batch 32/64 loss: -0.0734788179397583
Batch 33/64 loss: -0.0463104248046875
Batch 34/64 loss: -0.03574681282043457
Batch 35/64 loss: -0.051068246364593506
Batch 36/64 loss: -0.06573158502578735
Batch 37/64 loss: -0.034288644790649414
Batch 38/64 loss: -0.06203430891036987
Batch 39/64 loss: -0.061035335063934326
Batch 40/64 loss: -0.07088184356689453
Batch 41/64 loss: -0.045290589332580566
Batch 42/64 loss: -0.05891525745391846
Batch 43/64 loss: -0.060618042945861816
Batch 44/64 loss: -0.05387735366821289
Batch 45/64 loss: -0.06520771980285645
Batch 46/64 loss: -0.057202279567718506
Batch 47/64 loss: -0.05325126647949219
Batch 48/64 loss: -0.06452465057373047
Batch 49/64 loss: -0.06773453950881958
Batch 50/64 loss: -0.07087743282318115
Batch 51/64 loss: -0.04900699853897095
Batch 52/64 loss: -0.06930643320083618
Batch 53/64 loss: -0.051535844802856445
Batch 54/64 loss: -0.0685495138168335
Batch 55/64 loss: -0.05950725078582764
Batch 56/64 loss: -0.06605809926986694
Batch 57/64 loss: -0.07040172815322876
Batch 58/64 loss: -0.06648141145706177
Batch 59/64 loss: -0.04391169548034668
Batch 60/64 loss: -0.03672206401824951
Batch 61/64 loss: -0.06527531147003174
Batch 62/64 loss: -0.051675498485565186
Batch 63/64 loss: -0.0625605583190918
Batch 64/64 loss: -0.0655105710029602
Epoch 477  Train loss: -0.05999367634455363  Val loss: 0.10154982510301255
Epoch 478
-------------------------------
Batch 1/64 loss: -0.0612339973449707
Batch 2/64 loss: -0.0678911805152893
Batch 3/64 loss: -0.055619001388549805
Batch 4/64 loss: -0.06065034866333008
Batch 5/64 loss: -0.083396315574646
Batch 6/64 loss: -0.08917498588562012
Batch 7/64 loss: -0.07766455411911011
Batch 8/64 loss: -0.06304562091827393
Batch 9/64 loss: -0.049756765365600586
Batch 10/64 loss: -0.03420591354370117
Batch 11/64 loss: -0.04910874366760254
Batch 12/64 loss: -0.05683654546737671
Batch 13/64 loss: -0.05032628774642944
Batch 14/64 loss: -0.05565530061721802
Batch 15/64 loss: -0.06445461511611938
Batch 16/64 loss: -0.058560729026794434
Batch 17/64 loss: -0.06562650203704834
Batch 18/64 loss: -0.051106393337249756
Batch 19/64 loss: -0.06152355670928955
Batch 20/64 loss: -0.03686845302581787
Batch 21/64 loss: -0.07140779495239258
Batch 22/64 loss: -0.056772470474243164
Batch 23/64 loss: -0.056863605976104736
Batch 24/64 loss: -0.05256778001785278
Batch 25/64 loss: -0.0489199161529541
Batch 26/64 loss: -0.06496310234069824
Batch 27/64 loss: -0.06639862060546875
Batch 28/64 loss: -0.06174647808074951
Batch 29/64 loss: -0.04574316740036011
Batch 30/64 loss: -0.052898287773132324
Batch 31/64 loss: -0.03080737590789795
Batch 32/64 loss: -0.05321913957595825
Batch 33/64 loss: -0.0654146671295166
Batch 34/64 loss: -0.05788111686706543
Batch 35/64 loss: -0.06255185604095459
Batch 36/64 loss: -0.03708571195602417
Batch 37/64 loss: -0.08010464906692505
Batch 38/64 loss: -0.061409711837768555
Batch 39/64 loss: -0.053269386291503906
Batch 40/64 loss: -0.0656808614730835
Batch 41/64 loss: -0.05984312295913696
Batch 42/64 loss: -0.05073082447052002
Batch 43/64 loss: -0.07004356384277344
Batch 44/64 loss: -0.05144071578979492
Batch 45/64 loss: -0.05910778045654297
Batch 46/64 loss: -0.05989563465118408
Batch 47/64 loss: -0.055163443088531494
Batch 48/64 loss: -0.03344285488128662
Batch 49/64 loss: -0.052919864654541016
Batch 50/64 loss: -0.046739280223846436
Batch 51/64 loss: -0.047830939292907715
Batch 52/64 loss: -0.04654216766357422
Batch 53/64 loss: -0.053109824657440186
Batch 54/64 loss: -0.0724872350692749
Batch 55/64 loss: -0.07094359397888184
Batch 56/64 loss: -0.061535775661468506
Batch 57/64 loss: -0.06816405057907104
Batch 58/64 loss: -0.05813056230545044
Batch 59/64 loss: -0.030031561851501465
Batch 60/64 loss: -0.05401819944381714
Batch 61/64 loss: -0.06981098651885986
Batch 62/64 loss: -0.07265228033065796
Batch 63/64 loss: -0.06540900468826294
Batch 64/64 loss: -0.03411656618118286
Epoch 478  Train loss: -0.05763125022252401  Val loss: 0.09958756359172441
Epoch 479
-------------------------------
Batch 1/64 loss: -0.06597751379013062
Batch 2/64 loss: -0.07736212015151978
Batch 3/64 loss: -0.06666779518127441
Batch 4/64 loss: -0.07264286279678345
Batch 5/64 loss: -0.0743253231048584
Batch 6/64 loss: -0.06117171049118042
Batch 7/64 loss: -0.06733822822570801
Batch 8/64 loss: -0.05873918533325195
Batch 9/64 loss: -0.07207334041595459
Batch 10/64 loss: -0.07755738496780396
Batch 11/64 loss: -0.07728999853134155
Batch 12/64 loss: -0.05522274971008301
Batch 13/64 loss: -0.05219566822052002
Batch 14/64 loss: -0.05699557065963745
Batch 15/64 loss: -0.07752871513366699
Batch 16/64 loss: -0.0558658242225647
Batch 17/64 loss: -0.058552086353302
Batch 18/64 loss: -0.05740463733673096
Batch 19/64 loss: -0.05514180660247803
Batch 20/64 loss: -0.018730878829956055
Batch 21/64 loss: -0.06809109449386597
Batch 22/64 loss: -0.06438851356506348
Batch 23/64 loss: -0.0444340705871582
Batch 24/64 loss: -0.054324448108673096
Batch 25/64 loss: -0.06790262460708618
Batch 26/64 loss: -0.05453026294708252
Batch 27/64 loss: -0.06171673536300659
Batch 28/64 loss: -0.07043808698654175
Batch 29/64 loss: -0.06419956684112549
Batch 30/64 loss: -0.07782965898513794
Batch 31/64 loss: -0.06150054931640625
Batch 32/64 loss: -0.06088554859161377
Batch 33/64 loss: -0.06122171878814697
Batch 34/64 loss: -0.041127800941467285
Batch 35/64 loss: -0.05653274059295654
Batch 36/64 loss: -0.043472886085510254
Batch 37/64 loss: -0.06393086910247803
Batch 38/64 loss: -0.06209003925323486
Batch 39/64 loss: -0.05880659818649292
Batch 40/64 loss: -0.020339667797088623
Batch 41/64 loss: -0.06840652227401733
Batch 42/64 loss: -0.07441306114196777
Batch 43/64 loss: -0.06143534183502197
Batch 44/64 loss: -0.05319148302078247
Batch 45/64 loss: -0.053847670555114746
Batch 46/64 loss: -0.0644615888595581
Batch 47/64 loss: -0.061198651790618896
Batch 48/64 loss: -0.059193193912506104
Batch 49/64 loss: -0.07216238975524902
Batch 50/64 loss: -0.06952476501464844
Batch 51/64 loss: -0.0841553807258606
Batch 52/64 loss: -0.05170542001724243
Batch 53/64 loss: -0.07717323303222656
Batch 54/64 loss: -0.06006205081939697
Batch 55/64 loss: -0.050770580768585205
Batch 56/64 loss: -0.07323449850082397
Batch 57/64 loss: -0.05487889051437378
Batch 58/64 loss: -0.0643695592880249
Batch 59/64 loss: -0.05906039476394653
Batch 60/64 loss: -0.061340510845184326
Batch 61/64 loss: -0.04465818405151367
Batch 62/64 loss: -0.054819583892822266
Batch 63/64 loss: -0.057098209857940674
Batch 64/64 loss: -0.054620444774627686
Epoch 479  Train loss: -0.06099879811791813  Val loss: 0.1008933819446367
Epoch 480
-------------------------------
Batch 1/64 loss: -0.07464849948883057
Batch 2/64 loss: -0.07214921712875366
Batch 3/64 loss: -0.06637799739837646
Batch 4/64 loss: -0.0579148530960083
Batch 5/64 loss: -0.06430131196975708
Batch 6/64 loss: -0.06480878591537476
Batch 7/64 loss: -0.056960463523864746
Batch 8/64 loss: -0.06761008501052856
Batch 9/64 loss: -0.05902659893035889
Batch 10/64 loss: -0.03865307569503784
Batch 11/64 loss: -0.06712698936462402
Batch 12/64 loss: -0.06926697492599487
Batch 13/64 loss: -0.05244433879852295
Batch 14/64 loss: -0.05686992406845093
Batch 15/64 loss: -0.060379862785339355
Batch 16/64 loss: -0.0678107738494873
Batch 17/64 loss: -0.07788807153701782
Batch 18/64 loss: -0.055151939392089844
Batch 19/64 loss: -0.038823723793029785
Batch 20/64 loss: -0.0525781512260437
Batch 21/64 loss: -0.052795350551605225
Batch 22/64 loss: -0.04963415861129761
Batch 23/64 loss: -0.0625079870223999
Batch 24/64 loss: -0.06001991033554077
Batch 25/64 loss: -0.05884432792663574
Batch 26/64 loss: -0.04875314235687256
Batch 27/64 loss: -0.05638694763183594
Batch 28/64 loss: -0.04239779710769653
Batch 29/64 loss: -0.03623849153518677
Batch 30/64 loss: -0.07248330116271973
Batch 31/64 loss: -0.04674088954925537
Batch 32/64 loss: -0.07259434461593628
Batch 33/64 loss: -0.06919926404953003
Batch 34/64 loss: -0.0533595085144043
Batch 35/64 loss: -0.06430339813232422
Batch 36/64 loss: -0.02443838119506836
Batch 37/64 loss: -0.050986528396606445
Batch 38/64 loss: -0.0749097466468811
Batch 39/64 loss: -0.06584692001342773
Batch 40/64 loss: -0.08051127195358276
Batch 41/64 loss: -0.06834453344345093
Batch 42/64 loss: -0.0723496675491333
Batch 43/64 loss: -0.0588531494140625
Batch 44/64 loss: -0.04915356636047363
Batch 45/64 loss: -0.04429149627685547
Batch 46/64 loss: -0.06237435340881348
Batch 47/64 loss: -0.05154263973236084
Batch 48/64 loss: -0.07310926914215088
Batch 49/64 loss: -0.04947018623352051
Batch 50/64 loss: -0.064339280128479
Batch 51/64 loss: -0.073497474193573
Batch 52/64 loss: -0.07135277986526489
Batch 53/64 loss: -0.03647029399871826
Batch 54/64 loss: -0.05102342367172241
Batch 55/64 loss: -0.072063148021698
Batch 56/64 loss: -0.07338839769363403
Batch 57/64 loss: -0.04842841625213623
Batch 58/64 loss: -0.048440635204315186
Batch 59/64 loss: -0.07821518182754517
Batch 60/64 loss: -0.061980366706848145
Batch 61/64 loss: -0.060366928577423096
Batch 62/64 loss: -0.05504113435745239
Batch 63/64 loss: -0.058881163597106934
Batch 64/64 loss: -0.06478261947631836
Epoch 480  Train loss: -0.059534239301494526  Val loss: 0.1004899004890337
Epoch 481
-------------------------------
Batch 1/64 loss: -0.070770263671875
Batch 2/64 loss: -0.06187248229980469
Batch 3/64 loss: -0.06071573495864868
Batch 4/64 loss: -0.06052374839782715
Batch 5/64 loss: -0.05145496129989624
Batch 6/64 loss: -0.07007777690887451
Batch 7/64 loss: -0.060064077377319336
Batch 8/64 loss: -0.06513553857803345
Batch 9/64 loss: -0.0705193281173706
Batch 10/64 loss: -0.04645669460296631
Batch 11/64 loss: -0.059868693351745605
Batch 12/64 loss: -0.0749998688697815
Batch 13/64 loss: -0.061360716819763184
Batch 14/64 loss: -0.06686323881149292
Batch 15/64 loss: -0.06630337238311768
Batch 16/64 loss: -0.07026660442352295
Batch 17/64 loss: -0.07486540079116821
Batch 18/64 loss: -0.0701296329498291
Batch 19/64 loss: -0.05483090877532959
Batch 20/64 loss: -0.07026088237762451
Batch 21/64 loss: -0.0674867033958435
Batch 22/64 loss: -0.08421701192855835
Batch 23/64 loss: -0.0725477933883667
Batch 24/64 loss: -0.07729935646057129
Batch 25/64 loss: -0.054811954498291016
Batch 26/64 loss: -0.038255393505096436
Batch 27/64 loss: -0.05091893672943115
Batch 28/64 loss: -0.07522284984588623
Batch 29/64 loss: -0.06994813680648804
Batch 30/64 loss: -0.07696831226348877
Batch 31/64 loss: -0.0660364031791687
Batch 32/64 loss: -0.07670211791992188
Batch 33/64 loss: -0.048501670360565186
Batch 34/64 loss: -0.06119173765182495
Batch 35/64 loss: -0.04899531602859497
Batch 36/64 loss: -0.06416386365890503
Batch 37/64 loss: -0.05046892166137695
Batch 38/64 loss: -0.05823242664337158
Batch 39/64 loss: -0.047092437744140625
Batch 40/64 loss: -0.0618557333946228
Batch 41/64 loss: -0.03189873695373535
Batch 42/64 loss: -0.05340468883514404
Batch 43/64 loss: -0.0712546706199646
Batch 44/64 loss: -0.06185692548751831
Batch 45/64 loss: -0.0523073673248291
Batch 46/64 loss: -0.07266223430633545
Batch 47/64 loss: -0.04007965326309204
Batch 48/64 loss: -0.04198777675628662
Batch 49/64 loss: -0.06394177675247192
Batch 50/64 loss: -0.0556180477142334
Batch 51/64 loss: -0.06096804141998291
Batch 52/64 loss: -0.059658169746398926
Batch 53/64 loss: -0.04822194576263428
Batch 54/64 loss: -0.038443565368652344
Batch 55/64 loss: -0.047368764877319336
Batch 56/64 loss: -0.04689210653305054
Batch 57/64 loss: -0.05522799491882324
Batch 58/64 loss: -0.029294133186340332
Batch 59/64 loss: -0.05941969156265259
Batch 60/64 loss: -0.038853347301483154
Batch 61/64 loss: -0.051023900508880615
Batch 62/64 loss: -0.06153839826583862
Batch 63/64 loss: -0.036409854888916016
Batch 64/64 loss: -0.04581934213638306
Epoch 481  Train loss: -0.05883848316529218  Val loss: 0.10129443394769098
Epoch 482
-------------------------------
Batch 1/64 loss: -0.057865262031555176
Batch 2/64 loss: -0.05593764781951904
Batch 3/64 loss: -0.07275962829589844
Batch 4/64 loss: -0.03626912832260132
Batch 5/64 loss: -0.057814598083496094
Batch 6/64 loss: -0.061416029930114746
Batch 7/64 loss: -0.0566178560256958
Batch 8/64 loss: -0.04218626022338867
Batch 9/64 loss: -0.04930388927459717
Batch 10/64 loss: -0.05094277858734131
Batch 11/64 loss: -0.0631975531578064
Batch 12/64 loss: -0.06325268745422363
Batch 13/64 loss: -0.062029898166656494
Batch 14/64 loss: -0.07399952411651611
Batch 15/64 loss: -0.06840616464614868
Batch 16/64 loss: -0.050219953060150146
Batch 17/64 loss: -0.057920634746551514
Batch 18/64 loss: -0.06425994634628296
Batch 19/64 loss: -0.06173986196517944
Batch 20/64 loss: -0.03908735513687134
Batch 21/64 loss: -0.06202739477157593
Batch 22/64 loss: -0.060622990131378174
Batch 23/64 loss: -0.0723692774772644
Batch 24/64 loss: -0.053869009017944336
Batch 25/64 loss: -0.05414736270904541
Batch 26/64 loss: -0.051913321018218994
Batch 27/64 loss: -0.0538751482963562
Batch 28/64 loss: -0.07313883304595947
Batch 29/64 loss: -0.06859588623046875
Batch 30/64 loss: -0.052440106868743896
Batch 31/64 loss: -0.06658071279525757
Batch 32/64 loss: -0.06900924444198608
Batch 33/64 loss: -0.05167597532272339
Batch 34/64 loss: -0.05083721876144409
Batch 35/64 loss: -0.06145882606506348
Batch 36/64 loss: -0.05391263961791992
Batch 37/64 loss: -0.05590522289276123
Batch 38/64 loss: -0.051578402519226074
Batch 39/64 loss: -0.04321932792663574
Batch 40/64 loss: -0.06194645166397095
Batch 41/64 loss: -0.06944906711578369
Batch 42/64 loss: -0.07051283121109009
Batch 43/64 loss: -0.06733173131942749
Batch 44/64 loss: -0.05475282669067383
Batch 45/64 loss: -0.06497633457183838
Batch 46/64 loss: -0.063850998878479
Batch 47/64 loss: -0.05336636304855347
Batch 48/64 loss: -0.05594325065612793
Batch 49/64 loss: -0.05566602945327759
Batch 50/64 loss: -0.06051748991012573
Batch 51/64 loss: -0.05624544620513916
Batch 52/64 loss: -0.05757880210876465
Batch 53/64 loss: -0.045541465282440186
Batch 54/64 loss: -0.06787443161010742
Batch 55/64 loss: -0.04680681228637695
Batch 56/64 loss: -0.059946417808532715
Batch 57/64 loss: -0.07168161869049072
Batch 58/64 loss: -0.05237191915512085
Batch 59/64 loss: -0.06522232294082642
Batch 60/64 loss: -0.04468274116516113
Batch 61/64 loss: -0.07481038570404053
Batch 62/64 loss: -0.04460042715072632
Batch 63/64 loss: -0.06228232383728027
Batch 64/64 loss: -0.05649209022521973
Epoch 482  Train loss: -0.05839578217151118  Val loss: 0.1031723702486438
Epoch 483
-------------------------------
Batch 1/64 loss: -0.0651082992553711
Batch 2/64 loss: -0.06193089485168457
Batch 3/64 loss: -0.07891720533370972
Batch 4/64 loss: -0.07963639497756958
Batch 5/64 loss: -0.03848230838775635
Batch 6/64 loss: -0.06670790910720825
Batch 7/64 loss: -0.0636640191078186
Batch 8/64 loss: -0.07097369432449341
Batch 9/64 loss: -0.07392781972885132
Batch 10/64 loss: -0.07174783945083618
Batch 11/64 loss: -0.06673890352249146
Batch 12/64 loss: -0.0739126205444336
Batch 13/64 loss: -0.07237714529037476
Batch 14/64 loss: -0.057849764823913574
Batch 15/64 loss: -0.06905019283294678
Batch 16/64 loss: -0.03818774223327637
Batch 17/64 loss: -0.06238293647766113
Batch 18/64 loss: -0.056812822818756104
Batch 19/64 loss: -0.019939184188842773
Batch 20/64 loss: -0.05233711004257202
Batch 21/64 loss: -0.07032114267349243
Batch 22/64 loss: -0.06595700979232788
Batch 23/64 loss: -0.047663331031799316
Batch 24/64 loss: -0.05154353380203247
Batch 25/64 loss: -0.04324519634246826
Batch 26/64 loss: -0.05496853590011597
Batch 27/64 loss: -0.06659144163131714
Batch 28/64 loss: -0.04317021369934082
Batch 29/64 loss: -0.0828256607055664
Batch 30/64 loss: -0.04503345489501953
Batch 31/64 loss: -0.07168185710906982
Batch 32/64 loss: -0.05818355083465576
Batch 33/64 loss: -0.05514633655548096
Batch 34/64 loss: -0.05985742807388306
Batch 35/64 loss: -0.05763310194015503
Batch 36/64 loss: -0.04378032684326172
Batch 37/64 loss: -0.04914224147796631
Batch 38/64 loss: -0.05487263202667236
Batch 39/64 loss: -0.06890976428985596
Batch 40/64 loss: -0.052602946758270264
Batch 41/64 loss: -0.06934881210327148
Batch 42/64 loss: -0.04909783601760864
Batch 43/64 loss: -0.0594901442527771
Batch 44/64 loss: -0.06483805179595947
Batch 45/64 loss: -0.04884594678878784
Batch 46/64 loss: -0.07365864515304565
Batch 47/64 loss: -0.06343251466751099
Batch 48/64 loss: -0.055786073207855225
Batch 49/64 loss: -0.06633245944976807
Batch 50/64 loss: -0.02140176296234131
Batch 51/64 loss: -0.05179119110107422
Batch 52/64 loss: -0.06216263771057129
Batch 53/64 loss: -0.05731093883514404
Batch 54/64 loss: -0.06310480833053589
Batch 55/64 loss: -0.06100356578826904
Batch 56/64 loss: -0.04540109634399414
Batch 57/64 loss: -0.062238454818725586
Batch 58/64 loss: -0.057107388973236084
Batch 59/64 loss: -0.04064583778381348
Batch 60/64 loss: -0.05553901195526123
Batch 61/64 loss: -0.03428983688354492
Batch 62/64 loss: -0.05440855026245117
Batch 63/64 loss: -0.04852128028869629
Batch 64/64 loss: -0.04345524311065674
Epoch 483  Train loss: -0.05775941633710674  Val loss: 0.10289839719169329
Epoch 484
-------------------------------
Batch 1/64 loss: -0.07163602113723755
Batch 2/64 loss: -0.07471692562103271
Batch 3/64 loss: -0.07115453481674194
Batch 4/64 loss: -0.050896644592285156
Batch 5/64 loss: -0.04588741064071655
Batch 6/64 loss: -0.0681847333908081
Batch 7/64 loss: -0.07048702239990234
Batch 8/64 loss: -0.06772691011428833
Batch 9/64 loss: -0.07647162675857544
Batch 10/64 loss: -0.06282806396484375
Batch 11/64 loss: -0.05670708417892456
Batch 12/64 loss: -0.04306596517562866
Batch 13/64 loss: -0.04005223512649536
Batch 14/64 loss: -0.058465003967285156
Batch 15/64 loss: -0.050330519676208496
Batch 16/64 loss: -0.07436984777450562
Batch 17/64 loss: -0.07057833671569824
Batch 18/64 loss: -0.06452912092208862
Batch 19/64 loss: -0.05975067615509033
Batch 20/64 loss: -0.07147669792175293
Batch 21/64 loss: -0.07762068510055542
Batch 22/64 loss: -0.02646338939666748
Batch 23/64 loss: -0.03442007303237915
Batch 24/64 loss: -0.04542750120162964
Batch 25/64 loss: -0.07528644800186157
Batch 26/64 loss: -0.05029851198196411
Batch 27/64 loss: -0.033058881759643555
Batch 28/64 loss: -0.06857705116271973
Batch 29/64 loss: -0.07311922311782837
Batch 30/64 loss: -0.07875984907150269
Batch 31/64 loss: -0.06530845165252686
Batch 32/64 loss: -0.062228381633758545
Batch 33/64 loss: -0.046965181827545166
Batch 34/64 loss: -0.05649775266647339
Batch 35/64 loss: -0.060427188873291016
Batch 36/64 loss: -0.0708460807800293
Batch 37/64 loss: -0.08120125532150269
Batch 38/64 loss: -0.061271607875823975
Batch 39/64 loss: -0.052031874656677246
Batch 40/64 loss: -0.07219088077545166
Batch 41/64 loss: -0.042546868324279785
Batch 42/64 loss: -0.08229124546051025
Batch 43/64 loss: -0.06892991065979004
Batch 44/64 loss: -0.051082730293273926
Batch 45/64 loss: -0.05706918239593506
Batch 46/64 loss: -0.05880439281463623
Batch 47/64 loss: -0.08234250545501709
Batch 48/64 loss: -0.05507504940032959
Batch 49/64 loss: -0.07041561603546143
Batch 50/64 loss: -0.06915479898452759
Batch 51/64 loss: -0.07447463274002075
Batch 52/64 loss: -0.05573803186416626
Batch 53/64 loss: -0.05539977550506592
Batch 54/64 loss: -0.05842149257659912
Batch 55/64 loss: -0.05600672960281372
Batch 56/64 loss: -0.06629782915115356
Batch 57/64 loss: -0.05300438404083252
Batch 58/64 loss: -0.06074631214141846
Batch 59/64 loss: -0.06202667951583862
Batch 60/64 loss: -0.049102723598480225
Batch 61/64 loss: -0.05134427547454834
Batch 62/64 loss: -0.06158846616744995
Batch 63/64 loss: -0.060999274253845215
Batch 64/64 loss: -0.04092520475387573
Epoch 484  Train loss: -0.06078231311311909  Val loss: 0.10405574897720232
Epoch 485
-------------------------------
Batch 1/64 loss: -0.07243752479553223
Batch 2/64 loss: -0.0556943416595459
Batch 3/64 loss: -0.06083482503890991
Batch 4/64 loss: -0.06819897890090942
Batch 5/64 loss: -0.07868397235870361
Batch 6/64 loss: -0.06183886528015137
Batch 7/64 loss: -0.0425381064414978
Batch 8/64 loss: -0.03193986415863037
Batch 9/64 loss: -0.07637381553649902
Batch 10/64 loss: -0.05784869194030762
Batch 11/64 loss: -0.05767667293548584
Batch 12/64 loss: -0.04433107376098633
Batch 13/64 loss: -0.04885071516036987
Batch 14/64 loss: -0.041080355644226074
Batch 15/64 loss: -0.03818565607070923
Batch 16/64 loss: -0.029562532901763916
Batch 17/64 loss: -0.05670958757400513
Batch 18/64 loss: -0.05269014835357666
Batch 19/64 loss: -0.07565701007843018
Batch 20/64 loss: -0.04771876335144043
Batch 21/64 loss: -0.04700273275375366
Batch 22/64 loss: -0.05948948860168457
Batch 23/64 loss: -0.0692375898361206
Batch 24/64 loss: -0.05605107545852661
Batch 25/64 loss: -0.06295442581176758
Batch 26/64 loss: -0.034629225730895996
Batch 27/64 loss: -0.06925356388092041
Batch 28/64 loss: -0.06910628080368042
Batch 29/64 loss: -0.07194077968597412
Batch 30/64 loss: -0.06077522039413452
Batch 31/64 loss: -0.05738532543182373
Batch 32/64 loss: -0.06204789876937866
Batch 33/64 loss: -0.0727682113647461
Batch 34/64 loss: -0.05335593223571777
Batch 35/64 loss: -0.07247447967529297
Batch 36/64 loss: -0.06567728519439697
Batch 37/64 loss: -0.0646672248840332
Batch 38/64 loss: -0.06971460580825806
Batch 39/64 loss: -0.06767076253890991
Batch 40/64 loss: -0.05878567695617676
Batch 41/64 loss: -0.05092954635620117
Batch 42/64 loss: -0.06871670484542847
Batch 43/64 loss: -0.029533743858337402
Batch 44/64 loss: -0.06363224983215332
Batch 45/64 loss: -0.048764944076538086
Batch 46/64 loss: -0.07442033290863037
Batch 47/64 loss: -0.05351680517196655
Batch 48/64 loss: -0.06355243921279907
Batch 49/64 loss: -0.0597761869430542
Batch 50/64 loss: -0.06292015314102173
Batch 51/64 loss: -0.056761324405670166
Batch 52/64 loss: -0.06876420974731445
Batch 53/64 loss: -0.07241964340209961
Batch 54/64 loss: -0.05193901062011719
Batch 55/64 loss: -0.08527308702468872
Batch 56/64 loss: -0.07026112079620361
Batch 57/64 loss: -0.06949615478515625
Batch 58/64 loss: -0.05822467803955078
Batch 59/64 loss: -0.05689668655395508
Batch 60/64 loss: -0.06659621000289917
Batch 61/64 loss: -0.06481349468231201
Batch 62/64 loss: -0.04528886079788208
Batch 63/64 loss: -0.08012455701828003
Batch 64/64 loss: -0.05072253942489624
Epoch 485  Train loss: -0.059678483710569497  Val loss: 0.10098444288948558
Epoch 486
-------------------------------
Batch 1/64 loss: -0.07747399806976318
Batch 2/64 loss: -0.05677157640457153
Batch 3/64 loss: -0.04107308387756348
Batch 4/64 loss: -0.05184692144393921
Batch 5/64 loss: -0.07500475645065308
Batch 6/64 loss: -0.07367867231369019
Batch 7/64 loss: -0.055818796157836914
Batch 8/64 loss: -0.07712143659591675
Batch 9/64 loss: -0.07799464464187622
Batch 10/64 loss: -0.055253565311431885
Batch 11/64 loss: -0.08279961347579956
Batch 12/64 loss: -0.07825958728790283
Batch 13/64 loss: -0.06807088851928711
Batch 14/64 loss: -0.060293614864349365
Batch 15/64 loss: -0.08099031448364258
Batch 16/64 loss: -0.059735894203186035
Batch 17/64 loss: -0.06170588731765747
Batch 18/64 loss: -0.05708664655685425
Batch 19/64 loss: -0.06557071208953857
Batch 20/64 loss: -0.05095946788787842
Batch 21/64 loss: -0.05926918983459473
Batch 22/64 loss: -0.06257444620132446
Batch 23/64 loss: -0.06887698173522949
Batch 24/64 loss: -0.0740668773651123
Batch 25/64 loss: -0.07654309272766113
Batch 26/64 loss: -0.029303371906280518
Batch 27/64 loss: -0.06784301996231079
Batch 28/64 loss: -0.07294857501983643
Batch 29/64 loss: -0.06069761514663696
Batch 30/64 loss: -0.08056014776229858
Batch 31/64 loss: -0.06679940223693848
Batch 32/64 loss: -0.06750547885894775
Batch 33/64 loss: -0.05791705846786499
Batch 34/64 loss: -0.05838823318481445
Batch 35/64 loss: -0.0488467812538147
Batch 36/64 loss: -0.04459559917449951
Batch 37/64 loss: -0.05799823999404907
Batch 38/64 loss: -0.05473834276199341
Batch 39/64 loss: -0.05747324228286743
Batch 40/64 loss: -0.08049243688583374
Batch 41/64 loss: -0.06198924779891968
Batch 42/64 loss: -0.06399297714233398
Batch 43/64 loss: -0.07665866613388062
Batch 44/64 loss: -0.05219733715057373
Batch 45/64 loss: -7.092952728271484e-05
Batch 46/64 loss: -0.0539436936378479
Batch 47/64 loss: -0.04419386386871338
Batch 48/64 loss: -0.03967410326004028
Batch 49/64 loss: -0.05074423551559448
Batch 50/64 loss: -0.04392218589782715
Batch 51/64 loss: -0.05523574352264404
Batch 52/64 loss: -0.045606911182403564
Batch 53/64 loss: -0.05278623104095459
Batch 54/64 loss: -0.05337262153625488
Batch 55/64 loss: -0.07503032684326172
Batch 56/64 loss: -0.050024330615997314
Batch 57/64 loss: -0.05653423070907593
Batch 58/64 loss: -0.052625298500061035
Batch 59/64 loss: -0.028350889682769775
Batch 60/64 loss: -0.06254374980926514
Batch 61/64 loss: -0.03797566890716553
Batch 62/64 loss: -0.07204729318618774
Batch 63/64 loss: -0.05429387092590332
Batch 64/64 loss: -0.02403426170349121
Epoch 486  Train loss: -0.05893001275904038  Val loss: 0.09954340986369811
Epoch 487
-------------------------------
Batch 1/64 loss: -0.040612637996673584
Batch 2/64 loss: -0.06522303819656372
Batch 3/64 loss: -0.05429697036743164
Batch 4/64 loss: -0.07281321287155151
Batch 5/64 loss: -0.07151305675506592
Batch 6/64 loss: -0.07574272155761719
Batch 7/64 loss: -0.06427198648452759
Batch 8/64 loss: -0.04749804735183716
Batch 9/64 loss: -0.0739564299583435
Batch 10/64 loss: -0.052050113677978516
Batch 11/64 loss: -0.07980197668075562
Batch 12/64 loss: -0.06362372636795044
Batch 13/64 loss: -0.07215631008148193
Batch 14/64 loss: -0.06625044345855713
Batch 15/64 loss: -0.08139657974243164
Batch 16/64 loss: -0.07237404584884644
Batch 17/64 loss: -0.04092508554458618
Batch 18/64 loss: -0.06516999006271362
Batch 19/64 loss: -0.06671196222305298
Batch 20/64 loss: -0.0665045976638794
Batch 21/64 loss: -0.03298985958099365
Batch 22/64 loss: -0.06181371212005615
Batch 23/64 loss: -0.06033003330230713
Batch 24/64 loss: -0.07109534740447998
Batch 25/64 loss: -0.0381242036819458
Batch 26/64 loss: -0.0629960298538208
Batch 27/64 loss: -0.05321460962295532
Batch 28/64 loss: -0.0745844841003418
Batch 29/64 loss: -0.06379389762878418
Batch 30/64 loss: -0.06289815902709961
Batch 31/64 loss: -0.0651940107345581
Batch 32/64 loss: -0.05177652835845947
Batch 33/64 loss: -0.06480979919433594
Batch 34/64 loss: -0.06551045179367065
Batch 35/64 loss: -0.06566733121871948
Batch 36/64 loss: -0.06708341836929321
Batch 37/64 loss: -0.05588406324386597
Batch 38/64 loss: -0.07654434442520142
Batch 39/64 loss: -0.04333198070526123
Batch 40/64 loss: -0.050166964530944824
Batch 41/64 loss: -0.05097907781600952
Batch 42/64 loss: -0.07369029521942139
Batch 43/64 loss: -0.055351316928863525
Batch 44/64 loss: -0.05636775493621826
Batch 45/64 loss: -0.05586206912994385
Batch 46/64 loss: -0.04556882381439209
Batch 47/64 loss: -0.04012089967727661
Batch 48/64 loss: -0.051489412784576416
Batch 49/64 loss: -0.0460013747215271
Batch 50/64 loss: -0.05994057655334473
Batch 51/64 loss: -0.06669658422470093
Batch 52/64 loss: -0.04453885555267334
Batch 53/64 loss: -0.04463869333267212
Batch 54/64 loss: -0.06409090757369995
Batch 55/64 loss: -0.05052739381790161
Batch 56/64 loss: -0.06477481126785278
Batch 57/64 loss: -0.06235802173614502
Batch 58/64 loss: -0.07600635290145874
Batch 59/64 loss: -0.05727124214172363
Batch 60/64 loss: -0.07454341650009155
Batch 61/64 loss: -0.037406742572784424
Batch 62/64 loss: -0.07393181324005127
Batch 63/64 loss: -0.04546940326690674
Batch 64/64 loss: -0.04798763990402222
Epoch 487  Train loss: -0.059832450689054004  Val loss: 0.10006036156231594
Epoch 488
-------------------------------
Batch 1/64 loss: -0.05424100160598755
Batch 2/64 loss: -0.06249892711639404
Batch 3/64 loss: -0.08502686023712158
Batch 4/64 loss: -0.06971001625061035
Batch 5/64 loss: -0.06528300046920776
Batch 6/64 loss: -0.05768388509750366
Batch 7/64 loss: -0.06136476993560791
Batch 8/64 loss: -0.07250446081161499
Batch 9/64 loss: -0.057929396629333496
Batch 10/64 loss: -0.05991584062576294
Batch 11/64 loss: -0.05395728349685669
Batch 12/64 loss: -0.04820752143859863
Batch 13/64 loss: -0.0677105188369751
Batch 14/64 loss: -0.053206682205200195
Batch 15/64 loss: -0.0630178451538086
Batch 16/64 loss: -0.032183706760406494
Batch 17/64 loss: -0.07860147953033447
Batch 18/64 loss: -0.07770663499832153
Batch 19/64 loss: -0.050351738929748535
Batch 20/64 loss: -0.06377625465393066
Batch 21/64 loss: -0.0852404236793518
Batch 22/64 loss: -0.060221970081329346
Batch 23/64 loss: -0.0550684928894043
Batch 24/64 loss: -0.06354022026062012
Batch 25/64 loss: -0.05613070726394653
Batch 26/64 loss: -0.060709595680236816
Batch 27/64 loss: -0.06717407703399658
Batch 28/64 loss: -0.05088263750076294
Batch 29/64 loss: -0.055645763874053955
Batch 30/64 loss: -0.06894004344940186
Batch 31/64 loss: -0.06392669677734375
Batch 32/64 loss: -0.06614494323730469
Batch 33/64 loss: -0.07023012638092041
Batch 34/64 loss: -0.059668660163879395
Batch 35/64 loss: -0.04475516080856323
Batch 36/64 loss: -0.048905372619628906
Batch 37/64 loss: -0.058926165103912354
Batch 38/64 loss: -0.07211840152740479
Batch 39/64 loss: -0.05752694606781006
Batch 40/64 loss: -0.052678048610687256
Batch 41/64 loss: -0.048364102840423584
Batch 42/64 loss: -0.05860257148742676
Batch 43/64 loss: -0.04051542282104492
Batch 44/64 loss: -0.047396957874298096
Batch 45/64 loss: -0.06765127182006836
Batch 46/64 loss: -0.06826448440551758
Batch 47/64 loss: -0.056093037128448486
Batch 48/64 loss: -0.060065507888793945
Batch 49/64 loss: -0.06230485439300537
Batch 50/64 loss: -0.05697464942932129
Batch 51/64 loss: -0.07542872428894043
Batch 52/64 loss: -0.06303977966308594
Batch 53/64 loss: -0.0765182375907898
Batch 54/64 loss: -0.04500550031661987
Batch 55/64 loss: -0.05363643169403076
Batch 56/64 loss: -0.06930625438690186
Batch 57/64 loss: -0.07186353206634521
Batch 58/64 loss: -0.05179309844970703
Batch 59/64 loss: -0.05740588903427124
Batch 60/64 loss: -0.04296886920928955
Batch 61/64 loss: -0.053547680377960205
Batch 62/64 loss: -0.04622960090637207
Batch 63/64 loss: -0.07662785053253174
Batch 64/64 loss: -0.05848979949951172
Epoch 488  Train loss: -0.06034170880037196  Val loss: 0.10368728924453054
Epoch 489
-------------------------------
Batch 1/64 loss: -0.08040273189544678
Batch 2/64 loss: -0.03806149959564209
Batch 3/64 loss: -0.0683777928352356
Batch 4/64 loss: -0.0621337890625
Batch 5/64 loss: -0.057646334171295166
Batch 6/64 loss: -0.06336790323257446
Batch 7/64 loss: -0.07348239421844482
Batch 8/64 loss: -0.07170659303665161
Batch 9/64 loss: -0.06139111518859863
Batch 10/64 loss: -0.06544595956802368
Batch 11/64 loss: -0.039422690868377686
Batch 12/64 loss: -0.07210075855255127
Batch 13/64 loss: -0.042363762855529785
Batch 14/64 loss: -0.06229346990585327
Batch 15/64 loss: -0.06375688314437866
Batch 16/64 loss: -0.061388611793518066
Batch 17/64 loss: -0.05818462371826172
Batch 18/64 loss: -0.03779071569442749
Batch 19/64 loss: -0.04174607992172241
Batch 20/64 loss: -0.06082421541213989
Batch 21/64 loss: -0.04815173149108887
Batch 22/64 loss: -0.020387887954711914
Batch 23/64 loss: -0.06697392463684082
Batch 24/64 loss: -0.05521714687347412
Batch 25/64 loss: -0.06882625818252563
Batch 26/64 loss: -0.05878955125808716
Batch 27/64 loss: -0.055624186992645264
Batch 28/64 loss: -0.056732773780822754
Batch 29/64 loss: -0.06128215789794922
Batch 30/64 loss: -0.03087395429611206
Batch 31/64 loss: -0.062474846839904785
Batch 32/64 loss: -0.07339900732040405
Batch 33/64 loss: -0.057652950286865234
Batch 34/64 loss: -0.05574887990951538
Batch 35/64 loss: -0.06431102752685547
Batch 36/64 loss: -0.06714117527008057
Batch 37/64 loss: -0.06107974052429199
Batch 38/64 loss: -0.06911402940750122
Batch 39/64 loss: -0.017260074615478516
Batch 40/64 loss: -0.06995666027069092
Batch 41/64 loss: -0.07697618007659912
Batch 42/64 loss: -0.07007777690887451
Batch 43/64 loss: -0.0835612416267395
Batch 44/64 loss: -0.07107478380203247
Batch 45/64 loss: -0.05875086784362793
Batch 46/64 loss: -0.05251789093017578
Batch 47/64 loss: -0.05798029899597168
Batch 48/64 loss: -0.06538641452789307
Batch 49/64 loss: -0.06662696599960327
Batch 50/64 loss: -0.058092713356018066
Batch 51/64 loss: -0.06558316946029663
Batch 52/64 loss: -0.07574117183685303
Batch 53/64 loss: -0.058481037616729736
Batch 54/64 loss: -0.06717002391815186
Batch 55/64 loss: -0.042360544204711914
Batch 56/64 loss: -0.07284891605377197
Batch 57/64 loss: -0.05543851852416992
Batch 58/64 loss: -0.03833341598510742
Batch 59/64 loss: -0.05417996644973755
Batch 60/64 loss: -0.06777840852737427
Batch 61/64 loss: -0.06962412595748901
Batch 62/64 loss: -0.0650249719619751
Batch 63/64 loss: -0.08241891860961914
Batch 64/64 loss: -0.05789917707443237
Epoch 489  Train loss: -0.05995825248606065  Val loss: 0.09980579585963507
Epoch 490
-------------------------------
Batch 1/64 loss: -0.0769510269165039
Batch 2/64 loss: -0.04200929403305054
Batch 3/64 loss: -0.05452197790145874
Batch 4/64 loss: -0.06883478164672852
Batch 5/64 loss: -0.053330183029174805
Batch 6/64 loss: -0.0684729814529419
Batch 7/64 loss: -0.06631201505661011
Batch 8/64 loss: -0.0750001072883606
Batch 9/64 loss: -0.051612794399261475
Batch 10/64 loss: -0.05426448583602905
Batch 11/64 loss: -0.07220149040222168
Batch 12/64 loss: -0.07308566570281982
Batch 13/64 loss: -0.060421645641326904
Batch 14/64 loss: -0.07399928569793701
Batch 15/64 loss: -0.06547510623931885
Batch 16/64 loss: -0.0692940354347229
Batch 17/64 loss: -0.06584632396697998
Batch 18/64 loss: -0.05182051658630371
Batch 19/64 loss: -0.07377660274505615
Batch 20/64 loss: -0.07017266750335693
Batch 21/64 loss: -0.04464995861053467
Batch 22/64 loss: -0.04851865768432617
Batch 23/64 loss: -0.06461244821548462
Batch 24/64 loss: -0.05650472640991211
Batch 25/64 loss: -0.04289686679840088
Batch 26/64 loss: -0.06647288799285889
Batch 27/64 loss: -0.058946192264556885
Batch 28/64 loss: -0.06734824180603027
Batch 29/64 loss: -0.0523759126663208
Batch 30/64 loss: -0.0742485523223877
Batch 31/64 loss: -0.025638103485107422
Batch 32/64 loss: -0.07848119735717773
Batch 33/64 loss: -0.06294244527816772
Batch 34/64 loss: -0.06685113906860352
Batch 35/64 loss: -0.04495114088058472
Batch 36/64 loss: -0.05260348320007324
Batch 37/64 loss: -0.07457274198532104
Batch 38/64 loss: -0.05185091495513916
Batch 39/64 loss: -0.06784963607788086
Batch 40/64 loss: -0.06080085039138794
Batch 41/64 loss: -0.05717211961746216
Batch 42/64 loss: -0.05664849281311035
Batch 43/64 loss: -0.04762005805969238
Batch 44/64 loss: -0.045305490493774414
Batch 45/64 loss: -0.057953476905822754
Batch 46/64 loss: -0.05921220779418945
Batch 47/64 loss: -0.04668700695037842
Batch 48/64 loss: -0.04775971174240112
Batch 49/64 loss: -0.06160169839859009
Batch 50/64 loss: -0.06411951780319214
Batch 51/64 loss: -0.06855666637420654
Batch 52/64 loss: -0.061852216720581055
Batch 53/64 loss: -0.05731600522994995
Batch 54/64 loss: -0.045246899127960205
Batch 55/64 loss: -0.05708378553390503
Batch 56/64 loss: -0.047522127628326416
Batch 57/64 loss: -0.03539156913757324
Batch 58/64 loss: -0.05791693925857544
Batch 59/64 loss: -0.06879067420959473
Batch 60/64 loss: -0.07107937335968018
Batch 61/64 loss: -0.06591558456420898
Batch 62/64 loss: -0.05643320083618164
Batch 63/64 loss: -0.05142688751220703
Batch 64/64 loss: -0.0435945987701416
Epoch 490  Train loss: -0.059165909711052386  Val loss: 0.10196619898183239
Epoch 491
-------------------------------
Batch 1/64 loss: -0.05693638324737549
Batch 2/64 loss: -0.0714188814163208
Batch 3/64 loss: -0.06879371404647827
Batch 4/64 loss: -0.05864924192428589
Batch 5/64 loss: -0.07618457078933716
Batch 6/64 loss: -0.08581870794296265
Batch 7/64 loss: -0.04790341854095459
Batch 8/64 loss: -0.06888043880462646
Batch 9/64 loss: -0.07023817300796509
Batch 10/64 loss: -0.07718312740325928
Batch 11/64 loss: -0.03641301393508911
Batch 12/64 loss: -0.08325588703155518
Batch 13/64 loss: -0.070037841796875
Batch 14/64 loss: -0.0678708553314209
Batch 15/64 loss: -0.0819857120513916
Batch 16/64 loss: -0.05445319414138794
Batch 17/64 loss: -0.06707918643951416
Batch 18/64 loss: -0.07772302627563477
Batch 19/64 loss: -0.06985557079315186
Batch 20/64 loss: -0.06634515523910522
Batch 21/64 loss: -0.06014961004257202
Batch 22/64 loss: -0.06152474880218506
Batch 23/64 loss: -0.06318056583404541
Batch 24/64 loss: -0.07143968343734741
Batch 25/64 loss: -0.02933257818222046
Batch 26/64 loss: -0.054956793785095215
Batch 27/64 loss: -0.061550021171569824
Batch 28/64 loss: -0.06571131944656372
Batch 29/64 loss: -0.06520134210586548
Batch 30/64 loss: -0.07086348533630371
Batch 31/64 loss: -0.06897592544555664
Batch 32/64 loss: -0.07110273838043213
Batch 33/64 loss: -0.03684729337692261
Batch 34/64 loss: -0.06236433982849121
Batch 35/64 loss: -0.07114845514297485
Batch 36/64 loss: -0.0481988787651062
Batch 37/64 loss: -0.07339799404144287
Batch 38/64 loss: -0.06332099437713623
Batch 39/64 loss: -0.06898659467697144
Batch 40/64 loss: -0.07940304279327393
Batch 41/64 loss: -0.06226181983947754
Batch 42/64 loss: -0.04820197820663452
Batch 43/64 loss: -0.07361960411071777
Batch 44/64 loss: -0.06453144550323486
Batch 45/64 loss: -0.0455625057220459
Batch 46/64 loss: -0.05757969617843628
Batch 47/64 loss: -0.06506764888763428
Batch 48/64 loss: -0.057369112968444824
Batch 49/64 loss: -0.07029283046722412
Batch 50/64 loss: -0.053632259368896484
Batch 51/64 loss: -0.04455745220184326
Batch 52/64 loss: -0.04342639446258545
Batch 53/64 loss: -0.03827404975891113
Batch 54/64 loss: -0.05618560314178467
Batch 55/64 loss: -0.052365899085998535
Batch 56/64 loss: -0.06149935722351074
Batch 57/64 loss: -0.06338143348693848
Batch 58/64 loss: -0.06413280963897705
Batch 59/64 loss: -0.06834018230438232
Batch 60/64 loss: -0.06082385778427124
Batch 61/64 loss: -0.0656387209892273
Batch 62/64 loss: -0.05407434701919556
Batch 63/64 loss: -0.0622556209564209
Batch 64/64 loss: -0.05776113271713257
Epoch 491  Train loss: -0.06244828210157507  Val loss: 0.09982959921007714
Epoch 492
-------------------------------
Batch 1/64 loss: -0.07890212535858154
Batch 2/64 loss: -0.07996082305908203
Batch 3/64 loss: -0.05410349369049072
Batch 4/64 loss: -0.06958651542663574
Batch 5/64 loss: -0.05740475654602051
Batch 6/64 loss: -0.06936079263687134
Batch 7/64 loss: -0.04197484254837036
Batch 8/64 loss: -0.0701678991317749
Batch 9/64 loss: -0.05079692602157593
Batch 10/64 loss: -0.045195937156677246
Batch 11/64 loss: -0.04649472236633301
Batch 12/64 loss: -0.07228994369506836
Batch 13/64 loss: -0.06070071458816528
Batch 14/64 loss: -0.0716555118560791
Batch 15/64 loss: -0.08214294910430908
Batch 16/64 loss: -0.061544954776763916
Batch 17/64 loss: -0.06814861297607422
Batch 18/64 loss: -0.05883049964904785
Batch 19/64 loss: -0.0712958574295044
Batch 20/64 loss: -0.058444201946258545
Batch 21/64 loss: -0.04773610830307007
Batch 22/64 loss: -0.05790978670120239
Batch 23/64 loss: -0.06092649698257446
Batch 24/64 loss: -0.05279552936553955
Batch 25/64 loss: -0.067524254322052
Batch 26/64 loss: -0.05534160137176514
Batch 27/64 loss: -0.049054503440856934
Batch 28/64 loss: -0.05874454975128174
Batch 29/64 loss: -0.05122941732406616
Batch 30/64 loss: -0.03998136520385742
Batch 31/64 loss: -0.05806213617324829
Batch 32/64 loss: -0.07025724649429321
Batch 33/64 loss: -0.07879722118377686
Batch 34/64 loss: -0.06033933162689209
Batch 35/64 loss: -0.06693035364151001
Batch 36/64 loss: -0.05672776699066162
Batch 37/64 loss: -0.05734610557556152
Batch 38/64 loss: -0.07191681861877441
Batch 39/64 loss: -0.07523059844970703
Batch 40/64 loss: -0.07417166233062744
Batch 41/64 loss: -0.05773645639419556
Batch 42/64 loss: -0.07109951972961426
Batch 43/64 loss: -0.021542787551879883
Batch 44/64 loss: -0.06284749507904053
Batch 45/64 loss: -0.07492095232009888
Batch 46/64 loss: -0.07164734601974487
Batch 47/64 loss: -0.06320077180862427
Batch 48/64 loss: -0.07293140888214111
Batch 49/64 loss: -0.058096230030059814
Batch 50/64 loss: -0.059090614318847656
Batch 51/64 loss: -0.07716673612594604
Batch 52/64 loss: -0.06211507320404053
Batch 53/64 loss: -0.07408887147903442
Batch 54/64 loss: -0.0695618987083435
Batch 55/64 loss: -0.050121307373046875
Batch 56/64 loss: -0.08053910732269287
Batch 57/64 loss: -0.06220740079879761
Batch 58/64 loss: -0.04518282413482666
Batch 59/64 loss: -0.06362569332122803
Batch 60/64 loss: -0.06219106912612915
Batch 61/64 loss: -0.03987079858779907
Batch 62/64 loss: -0.050137341022491455
Batch 63/64 loss: -0.07369107007980347
Batch 64/64 loss: -0.0548703670501709
Epoch 492  Train loss: -0.06187906639248717  Val loss: 0.10148343597490762
Epoch 493
-------------------------------
Batch 1/64 loss: -0.08223462104797363
Batch 2/64 loss: -0.06956863403320312
Batch 3/64 loss: -0.054179608821868896
Batch 4/64 loss: -0.03556877374649048
Batch 5/64 loss: -0.07043474912643433
Batch 6/64 loss: -0.0689663290977478
Batch 7/64 loss: -0.043956100940704346
Batch 8/64 loss: -0.0670396089553833
Batch 9/64 loss: -0.05797433853149414
Batch 10/64 loss: -0.06064045429229736
Batch 11/64 loss: -0.06949830055236816
Batch 12/64 loss: -0.05495345592498779
Batch 13/64 loss: -0.0709998607635498
Batch 14/64 loss: -0.0831146240234375
Batch 15/64 loss: -0.046094655990600586
Batch 16/64 loss: -0.0456051230430603
Batch 17/64 loss: -0.05004239082336426
Batch 18/64 loss: -0.045470595359802246
Batch 19/64 loss: -0.06824791431427002
Batch 20/64 loss: -0.07119530439376831
Batch 21/64 loss: -0.05866736173629761
Batch 22/64 loss: -0.04607999324798584
Batch 23/64 loss: -0.06447041034698486
Batch 24/64 loss: -0.06901323795318604
Batch 25/64 loss: -0.06329786777496338
Batch 26/64 loss: -0.046651363372802734
Batch 27/64 loss: -0.07449686527252197
Batch 28/64 loss: -0.05244934558868408
Batch 29/64 loss: -0.07174563407897949
Batch 30/64 loss: -0.0643010139465332
Batch 31/64 loss: -0.05487781763076782
Batch 32/64 loss: -0.07434612512588501
Batch 33/64 loss: -0.08082330226898193
Batch 34/64 loss: -0.06008577346801758
Batch 35/64 loss: -0.07201111316680908
Batch 36/64 loss: -0.0658603310585022
Batch 37/64 loss: -0.06101781129837036
Batch 38/64 loss: -0.056469619274139404
Batch 39/64 loss: -0.06124460697174072
Batch 40/64 loss: -0.065807044506073
Batch 41/64 loss: -0.0655369758605957
Batch 42/64 loss: -0.04348939657211304
Batch 43/64 loss: -0.07504421472549438
Batch 44/64 loss: -0.08440268039703369
Batch 45/64 loss: -0.07540398836135864
Batch 46/64 loss: -0.05448693037033081
Batch 47/64 loss: -0.06683260202407837
Batch 48/64 loss: -0.05782055854797363
Batch 49/64 loss: -0.0715150237083435
Batch 50/64 loss: -0.043628573417663574
Batch 51/64 loss: -0.09311884641647339
Batch 52/64 loss: -0.05956625938415527
Batch 53/64 loss: -0.05848050117492676
Batch 54/64 loss: -0.05491971969604492
Batch 55/64 loss: -0.05913221836090088
Batch 56/64 loss: -0.0379447340965271
Batch 57/64 loss: -0.049292683601379395
Batch 58/64 loss: -0.05674320459365845
Batch 59/64 loss: -0.045524001121520996
Batch 60/64 loss: -0.06684672832489014
Batch 61/64 loss: -0.06695389747619629
Batch 62/64 loss: -0.05325371026992798
Batch 63/64 loss: -0.06968516111373901
Batch 64/64 loss: -0.03729361295700073
Epoch 493  Train loss: -0.06144462590124093  Val loss: 0.10190315631656713
Epoch 494
-------------------------------
Batch 1/64 loss: -0.06923550367355347
Batch 2/64 loss: -0.07937312126159668
Batch 3/64 loss: -0.05196315050125122
Batch 4/64 loss: -0.045784175395965576
Batch 5/64 loss: -0.07477760314941406
Batch 6/64 loss: -0.04120355844497681
Batch 7/64 loss: -0.0661320686340332
Batch 8/64 loss: -0.0562555193901062
Batch 9/64 loss: -0.06960833072662354
Batch 10/64 loss: -0.05900830030441284
Batch 11/64 loss: -0.06134974956512451
Batch 12/64 loss: -0.06789571046829224
Batch 13/64 loss: -0.05657440423965454
Batch 14/64 loss: -0.06493282318115234
Batch 15/64 loss: -0.0571061372756958
Batch 16/64 loss: -0.07025575637817383
Batch 17/64 loss: -0.07442331314086914
Batch 18/64 loss: -0.07699769735336304
Batch 19/64 loss: -0.06288403272628784
Batch 20/64 loss: -0.07831823825836182
Batch 21/64 loss: -0.06612402200698853
Batch 22/64 loss: -0.056399762630462646
Batch 23/64 loss: -0.0620800256729126
Batch 24/64 loss: -0.07054120302200317
Batch 25/64 loss: -0.08070653676986694
Batch 26/64 loss: -0.06155198812484741
Batch 27/64 loss: -0.07011860609054565
Batch 28/64 loss: -0.04949021339416504
Batch 29/64 loss: -0.06415557861328125
Batch 30/64 loss: -0.07239389419555664
Batch 31/64 loss: -0.07432067394256592
Batch 32/64 loss: -0.0674130916595459
Batch 33/64 loss: -0.06038260459899902
Batch 34/64 loss: -0.07507359981536865
Batch 35/64 loss: -0.07688426971435547
Batch 36/64 loss: -0.07793092727661133
Batch 37/64 loss: -0.04150897264480591
Batch 38/64 loss: -0.07113176584243774
Batch 39/64 loss: -0.019781947135925293
Batch 40/64 loss: -0.060613274574279785
Batch 41/64 loss: -0.05073338747024536
Batch 42/64 loss: -0.07243937253952026
Batch 43/64 loss: -0.07595646381378174
Batch 44/64 loss: -0.05897313356399536
Batch 45/64 loss: -0.046735167503356934
Batch 46/64 loss: -0.0714614987373352
Batch 47/64 loss: -0.05957603454589844
Batch 48/64 loss: -0.06246328353881836
Batch 49/64 loss: -0.07205605506896973
Batch 50/64 loss: -0.056546926498413086
Batch 51/64 loss: -0.06227082014083862
Batch 52/64 loss: -0.06052863597869873
Batch 53/64 loss: -0.04069924354553223
Batch 54/64 loss: -0.06217724084854126
Batch 55/64 loss: -0.05848044157028198
Batch 56/64 loss: -0.05901026725769043
Batch 57/64 loss: -0.05827176570892334
Batch 58/64 loss: -0.06476545333862305
Batch 59/64 loss: -0.05587261915206909
Batch 60/64 loss: -0.06195169687271118
Batch 61/64 loss: -0.07002103328704834
Batch 62/64 loss: -0.05998659133911133
Batch 63/64 loss: -0.06843996047973633
Batch 64/64 loss: -0.07459425926208496
Epoch 494  Train loss: -0.0631850186516257  Val loss: 0.10149837031806867
Epoch 495
-------------------------------
Batch 1/64 loss: -0.05315077304840088
Batch 2/64 loss: -0.0459437370300293
Batch 3/64 loss: -0.07077765464782715
Batch 4/64 loss: -0.03940492868423462
Batch 5/64 loss: -0.08115506172180176
Batch 6/64 loss: -0.0691940188407898
Batch 7/64 loss: -0.07361268997192383
Batch 8/64 loss: -0.08139073848724365
Batch 9/64 loss: -0.057816505432128906
Batch 10/64 loss: -0.06324934959411621
Batch 11/64 loss: -0.07704460620880127
Batch 12/64 loss: -0.07274830341339111
Batch 13/64 loss: -0.08894038200378418
Batch 14/64 loss: -0.06133580207824707
Batch 15/64 loss: -0.038088321685791016
Batch 16/64 loss: -0.0887380838394165
Batch 17/64 loss: -0.06773817539215088
Batch 18/64 loss: -0.07908213138580322
Batch 19/64 loss: -0.06695646047592163
Batch 20/64 loss: -0.07359659671783447
Batch 21/64 loss: -0.07480734586715698
Batch 22/64 loss: -0.04580330848693848
Batch 23/64 loss: -0.08163577318191528
Batch 24/64 loss: -0.07806837558746338
Batch 25/64 loss: -0.06352227926254272
Batch 26/64 loss: -0.07500326633453369
Batch 27/64 loss: -0.051659464836120605
Batch 28/64 loss: -0.07427334785461426
Batch 29/64 loss: -0.08002698421478271
Batch 30/64 loss: -0.03818321228027344
Batch 31/64 loss: -0.06625932455062866
Batch 32/64 loss: -0.07414346933364868
Batch 33/64 loss: -0.06492161750793457
Batch 34/64 loss: -0.0727418065071106
Batch 35/64 loss: -0.06687474250793457
Batch 36/64 loss: -0.05009490251541138
Batch 37/64 loss: -0.08028233051300049
Batch 38/64 loss: -0.06801199913024902
Batch 39/64 loss: -0.07530868053436279
Batch 40/64 loss: -0.03234696388244629
Batch 41/64 loss: -0.03739488124847412
Batch 42/64 loss: -0.0684276819229126
Batch 43/64 loss: -0.050983965396881104
Batch 44/64 loss: -0.0459364652633667
Batch 45/64 loss: -0.07491308450698853
Batch 46/64 loss: -0.06491315364837646
Batch 47/64 loss: -0.041764140129089355
Batch 48/64 loss: -0.0588870644569397
Batch 49/64 loss: -0.053972601890563965
Batch 50/64 loss: -0.07475697994232178
Batch 51/64 loss: -0.058249473571777344
Batch 52/64 loss: -0.07196539640426636
Batch 53/64 loss: -0.06532144546508789
Batch 54/64 loss: -0.05794203281402588
Batch 55/64 loss: -0.05578577518463135
Batch 56/64 loss: -0.057129621505737305
Batch 57/64 loss: -0.0794450044631958
Batch 58/64 loss: -0.07571661472320557
Batch 59/64 loss: -0.0699273943901062
Batch 60/64 loss: -0.03925049304962158
Batch 61/64 loss: -0.06799912452697754
Batch 62/64 loss: -0.05787980556488037
Batch 63/64 loss: -0.05329054594039917
Batch 64/64 loss: -0.06103247404098511
Epoch 495  Train loss: -0.06418134301316504  Val loss: 0.10272741112922065
Epoch 496
-------------------------------
Batch 1/64 loss: -0.04616576433181763
Batch 2/64 loss: -0.07482999563217163
Batch 3/64 loss: -0.051952362060546875
Batch 4/64 loss: -0.0732303261756897
Batch 5/64 loss: -0.07602947950363159
Batch 6/64 loss: -0.06425946950912476
Batch 7/64 loss: -0.06550157070159912
Batch 8/64 loss: -0.08013474941253662
Batch 9/64 loss: -0.07762491703033447
Batch 10/64 loss: -0.06918948888778687
Batch 11/64 loss: -0.07563817501068115
Batch 12/64 loss: -0.06444275379180908
Batch 13/64 loss: -0.06630271673202515
Batch 14/64 loss: -0.04427897930145264
Batch 15/64 loss: -0.06703531742095947
Batch 16/64 loss: -0.062333106994628906
Batch 17/64 loss: -0.0807884931564331
Batch 18/64 loss: -0.07733666896820068
Batch 19/64 loss: -0.053176939487457275
Batch 20/64 loss: -0.0807567834854126
Batch 21/64 loss: -0.0664517879486084
Batch 22/64 loss: -0.04527348279953003
Batch 23/64 loss: -0.059851229190826416
Batch 24/64 loss: -0.07098191976547241
Batch 25/64 loss: -0.05883830785751343
Batch 26/64 loss: -0.07736200094223022
Batch 27/64 loss: -0.07136350870132446
Batch 28/64 loss: -0.07856607437133789
Batch 29/64 loss: -0.07599008083343506
Batch 30/64 loss: -0.052510738372802734
Batch 31/64 loss: -0.0641985535621643
Batch 32/64 loss: -0.07425820827484131
Batch 33/64 loss: -0.057210445404052734
Batch 34/64 loss: -0.0707392692565918
Batch 35/64 loss: -0.07248938083648682
Batch 36/64 loss: -0.06336081027984619
Batch 37/64 loss: -0.045296311378479004
Batch 38/64 loss: -0.080474853515625
Batch 39/64 loss: -0.0672031044960022
Batch 40/64 loss: -0.07639002799987793
Batch 41/64 loss: -0.062385499477386475
Batch 42/64 loss: -0.07744085788726807
Batch 43/64 loss: -0.07214975357055664
Batch 44/64 loss: -0.05481696128845215
Batch 45/64 loss: -0.05813896656036377
Batch 46/64 loss: -0.06895327568054199
Batch 47/64 loss: -0.06418496370315552
Batch 48/64 loss: -0.05470740795135498
Batch 49/64 loss: -0.07534921169281006
Batch 50/64 loss: -0.05803227424621582
Batch 51/64 loss: -0.06017887592315674
Batch 52/64 loss: -0.06468832492828369
Batch 53/64 loss: -0.05919599533081055
Batch 54/64 loss: -0.07008892297744751
Batch 55/64 loss: -0.07241225242614746
Batch 56/64 loss: -0.0398259162902832
Batch 57/64 loss: -0.05883246660232544
Batch 58/64 loss: -0.049440979957580566
Batch 59/64 loss: -0.0641547441482544
Batch 60/64 loss: -0.05378139019012451
Batch 61/64 loss: -0.06569069623947144
Batch 62/64 loss: -0.061589598655700684
Batch 63/64 loss: -0.050626158714294434
Batch 64/64 loss: -0.05571448802947998
Epoch 496  Train loss: -0.06491403158973245  Val loss: 0.10451025487631048
Epoch 497
-------------------------------
Batch 1/64 loss: -0.058296263217926025
Batch 2/64 loss: -0.07852661609649658
Batch 3/64 loss: -0.07818609476089478
Batch 4/64 loss: -0.0860787034034729
Batch 5/64 loss: -0.07321763038635254
Batch 6/64 loss: -0.0627889633178711
Batch 7/64 loss: -0.07532423734664917
Batch 8/64 loss: -0.06835001707077026
Batch 9/64 loss: -0.0762484073638916
Batch 10/64 loss: -0.07857054471969604
Batch 11/64 loss: -0.0758327841758728
Batch 12/64 loss: -0.0678219199180603
Batch 13/64 loss: -0.0585094690322876
Batch 14/64 loss: -0.06180250644683838
Batch 15/64 loss: -0.06356924772262573
Batch 16/64 loss: -0.07414579391479492
Batch 17/64 loss: -0.08898556232452393
Batch 18/64 loss: -0.06565380096435547
Batch 19/64 loss: -0.03548717498779297
Batch 20/64 loss: -0.06621289253234863
Batch 21/64 loss: -0.06122183799743652
Batch 22/64 loss: -0.06526529788970947
Batch 23/64 loss: -0.04830288887023926
Batch 24/64 loss: -0.05267226696014404
Batch 25/64 loss: -0.07837402820587158
Batch 26/64 loss: -0.0685727596282959
Batch 27/64 loss: -0.03409808874130249
Batch 28/64 loss: -0.06730198860168457
Batch 29/64 loss: -0.07114017009735107
Batch 30/64 loss: -0.0695149302482605
Batch 31/64 loss: -0.06755483150482178
Batch 32/64 loss: -0.08379805088043213
Batch 33/64 loss: -0.05502593517303467
Batch 34/64 loss: -0.081032395362854
Batch 35/64 loss: -0.08078217506408691
Batch 36/64 loss: -0.0562860369682312
Batch 37/64 loss: -0.03714686632156372
Batch 38/64 loss: -0.05324828624725342
Batch 39/64 loss: -0.05941176414489746
Batch 40/64 loss: -0.0617825984954834
Batch 41/64 loss: -0.06306606531143188
Batch 42/64 loss: -0.0598941445350647
Batch 43/64 loss: -0.05119514465332031
Batch 44/64 loss: -0.05162620544433594
Batch 45/64 loss: -0.08071190118789673
Batch 46/64 loss: -0.053636908531188965
Batch 47/64 loss: -0.07505172491073608
Batch 48/64 loss: -0.05756646394729614
Batch 49/64 loss: -0.06159013509750366
Batch 50/64 loss: -0.062253475189208984
Batch 51/64 loss: -0.08041423559188843
Batch 52/64 loss: -0.08872115612030029
Batch 53/64 loss: -0.06063234806060791
Batch 54/64 loss: -0.06796199083328247
Batch 55/64 loss: -0.07795321941375732
Batch 56/64 loss: -0.054660677909851074
Batch 57/64 loss: -0.061788856983184814
Batch 58/64 loss: -0.055676817893981934
Batch 59/64 loss: -0.05104929208755493
Batch 60/64 loss: -0.06263643503189087
Batch 61/64 loss: -0.0728522539138794
Batch 62/64 loss: -0.08798366785049438
Batch 63/64 loss: -0.06579530239105225
Batch 64/64 loss: -0.06944352388381958
Epoch 497  Train loss: -0.06592851596720078  Val loss: 0.10240364218085903
Epoch 498
-------------------------------
Batch 1/64 loss: -0.0795392394065857
Batch 2/64 loss: -0.0649874210357666
Batch 3/64 loss: -0.06298547983169556
Batch 4/64 loss: -0.075755774974823
Batch 5/64 loss: -0.06651037931442261
Batch 6/64 loss: -0.08647775650024414
Batch 7/64 loss: -0.07333129644393921
Batch 8/64 loss: -0.06860452890396118
Batch 9/64 loss: -0.040777623653411865
Batch 10/64 loss: -0.07728803157806396
Batch 11/64 loss: -0.07286053895950317
Batch 12/64 loss: -0.06489944458007812
Batch 13/64 loss: -0.07577496767044067
Batch 14/64 loss: -0.04973578453063965
Batch 15/64 loss: -0.0784531831741333
Batch 16/64 loss: -0.0895267128944397
Batch 17/64 loss: -0.07850956916809082
Batch 18/64 loss: -0.07052099704742432
Batch 19/64 loss: -0.08476686477661133
Batch 20/64 loss: -0.0827951431274414
Batch 21/64 loss: -0.06928503513336182
Batch 22/64 loss: -0.0438266396522522
Batch 23/64 loss: -0.05000978708267212
Batch 24/64 loss: -0.06223106384277344
Batch 25/64 loss: -0.03860229253768921
Batch 26/64 loss: -0.07464057207107544
Batch 27/64 loss: -0.05551856756210327
Batch 28/64 loss: -0.056745946407318115
Batch 29/64 loss: -0.07076990604400635
Batch 30/64 loss: -0.03975409269332886
Batch 31/64 loss: -0.03251290321350098
Batch 32/64 loss: -0.07462835311889648
Batch 33/64 loss: -0.06066220998764038
Batch 34/64 loss: -0.07486367225646973
Batch 35/64 loss: -0.04429209232330322
Batch 36/64 loss: -0.06761997938156128
Batch 37/64 loss: -0.07861870527267456
Batch 38/64 loss: -0.07462507486343384
Batch 39/64 loss: -0.0638166069984436
Batch 40/64 loss: -0.04282081127166748
Batch 41/64 loss: -0.07063323259353638
Batch 42/64 loss: -0.06374841928482056
Batch 43/64 loss: -0.05788373947143555
Batch 44/64 loss: -0.05837053060531616
Batch 45/64 loss: -0.06697899103164673
Batch 46/64 loss: -0.04376727342605591
Batch 47/64 loss: -0.05260753631591797
Batch 48/64 loss: -0.0610119104385376
Batch 49/64 loss: -0.05786263942718506
Batch 50/64 loss: -0.07016229629516602
Batch 51/64 loss: -0.06989449262619019
Batch 52/64 loss: -0.051694273948669434
Batch 53/64 loss: -0.03862440586090088
Batch 54/64 loss: -0.06158590316772461
Batch 55/64 loss: -0.0594639778137207
Batch 56/64 loss: -0.055435776710510254
Batch 57/64 loss: -0.052115678787231445
Batch 58/64 loss: -0.060077130794525146
Batch 59/64 loss: -0.05217146873474121
Batch 60/64 loss: -0.03601270914077759
Batch 61/64 loss: -0.06307172775268555
Batch 62/64 loss: -0.05633598566055298
Batch 63/64 loss: -0.06019306182861328
Batch 64/64 loss: -0.030928850173950195
Epoch 498  Train loss: -0.06216227213541667  Val loss: 0.10188235900656055
Epoch 499
-------------------------------
Batch 1/64 loss: -0.07097744941711426
Batch 2/64 loss: -0.0482025146484375
Batch 3/64 loss: -0.03935432434082031
Batch 4/64 loss: -0.0454447865486145
Batch 5/64 loss: -0.035316526889801025
Batch 6/64 loss: -0.08207273483276367
Batch 7/64 loss: -0.05724453926086426
Batch 8/64 loss: -0.03520536422729492
Batch 9/64 loss: -0.036377906799316406
Batch 10/64 loss: -0.05451542139053345
Batch 11/64 loss: -0.06722718477249146
Batch 12/64 loss: -0.056920409202575684
Batch 13/64 loss: -0.06440925598144531
Batch 14/64 loss: -0.04977291822433472
Batch 15/64 loss: -0.052408456802368164
Batch 16/64 loss: -0.05576455593109131
Batch 17/64 loss: -0.05391967296600342
Batch 18/64 loss: -0.06547671556472778
Batch 19/64 loss: -0.06245851516723633
Batch 20/64 loss: -0.06927049160003662
Batch 21/64 loss: -0.06888777017593384
Batch 22/64 loss: -0.07475829124450684
Batch 23/64 loss: -0.064456045627594
Batch 24/64 loss: -0.06951963901519775
Batch 25/64 loss: -0.07026505470275879
Batch 26/64 loss: -0.06572985649108887
Batch 27/64 loss: -0.07805490493774414
Batch 28/64 loss: -0.05071890354156494
Batch 29/64 loss: -0.04274332523345947
Batch 30/64 loss: -0.04628884792327881
Batch 31/64 loss: -0.0684659481048584
Batch 32/64 loss: -0.058751463890075684
Batch 33/64 loss: -0.06318140029907227
Batch 34/64 loss: -0.06759452819824219
Batch 35/64 loss: -0.07974696159362793
Batch 36/64 loss: -0.06308233737945557
Batch 37/64 loss: -0.05519843101501465
Batch 38/64 loss: -0.06894201040267944
Batch 39/64 loss: -0.05435371398925781
Batch 40/64 loss: -0.04491019248962402
Batch 41/64 loss: -0.07257133722305298
Batch 42/64 loss: -0.06368190050125122
Batch 43/64 loss: -0.061493873596191406
Batch 44/64 loss: -0.05729198455810547
Batch 45/64 loss: -0.09117227792739868
Batch 46/64 loss: -0.07301253080368042
Batch 47/64 loss: -0.08227944374084473
Batch 48/64 loss: -0.07339996099472046
Batch 49/64 loss: -0.07502973079681396
Batch 50/64 loss: -0.06333065032958984
Batch 51/64 loss: -0.04866349697113037
Batch 52/64 loss: -0.05729562044143677
Batch 53/64 loss: -0.03476506471633911
Batch 54/64 loss: -0.057933509349823
Batch 55/64 loss: -0.06865048408508301
Batch 56/64 loss: -0.06431347131729126
Batch 57/64 loss: -0.07029110193252563
Batch 58/64 loss: -0.08265578746795654
Batch 59/64 loss: -0.059770941734313965
Batch 60/64 loss: -0.06302964687347412
Batch 61/64 loss: -0.0649670958518982
Batch 62/64 loss: -0.07167601585388184
Batch 63/64 loss: -0.05163896083831787
Batch 64/64 loss: -0.030450820922851562
Epoch 499  Train loss: -0.061015566657571234  Val loss: 0.10322076901537446
Epoch 500
-------------------------------
Batch 1/64 loss: -0.07020890712738037
Batch 2/64 loss: -0.021683037281036377
Batch 3/64 loss: -0.07192808389663696
Batch 4/64 loss: -0.055524230003356934
Batch 5/64 loss: -0.05701327323913574
Batch 6/64 loss: -0.08637416362762451
Batch 7/64 loss: -0.0643567442893982
Batch 8/64 loss: -0.07427799701690674
Batch 9/64 loss: -0.057301342487335205
Batch 10/64 loss: -0.0481448769569397
Batch 11/64 loss: -0.04754137992858887
Batch 12/64 loss: -0.04591834545135498
Batch 13/64 loss: -0.05953705310821533
Batch 14/64 loss: -0.06904828548431396
Batch 15/64 loss: -0.05847197771072388
Batch 16/64 loss: -0.0726277232170105
Batch 17/64 loss: -0.027574121952056885
Batch 18/64 loss: -0.07774972915649414
Batch 19/64 loss: -0.05300372838973999
Batch 20/64 loss: -0.04989898204803467
Batch 21/64 loss: -0.053925275802612305
Batch 22/64 loss: -0.0511898398399353
Batch 23/64 loss: -0.07434886693954468
Batch 24/64 loss: -0.046772658824920654
Batch 25/64 loss: -0.05919289588928223
Batch 26/64 loss: -0.07140946388244629
Batch 27/64 loss: -0.08452236652374268
Batch 28/64 loss: -0.08359009027481079
Batch 29/64 loss: -0.07247602939605713
Batch 30/64 loss: -0.05258452892303467
Batch 31/64 loss: -0.056374311447143555
Batch 32/64 loss: -0.05425834655761719
Batch 33/64 loss: -0.0446852445602417
Batch 34/64 loss: -0.05158787965774536
Batch 35/64 loss: -0.05997598171234131
Batch 36/64 loss: -0.05796527862548828
Batch 37/64 loss: -0.048415303230285645
Batch 38/64 loss: -0.04985237121582031
Batch 39/64 loss: -0.07077854871749878
Batch 40/64 loss: -0.067332923412323
Batch 41/64 loss: -0.0541955828666687
Batch 42/64 loss: -0.053620755672454834
Batch 43/64 loss: -0.06105804443359375
Batch 44/64 loss: -0.06894415616989136
Batch 45/64 loss: -0.07348710298538208
Batch 46/64 loss: -0.02075868844985962
Batch 47/64 loss: -0.04650026559829712
Batch 48/64 loss: -0.057731032371520996
Batch 49/64 loss: -0.05803263187408447
Batch 50/64 loss: -0.02766883373260498
Batch 51/64 loss: -0.044019877910614014
Batch 52/64 loss: -0.06850379705429077
Batch 53/64 loss: -0.046697258949279785
Batch 54/64 loss: -0.0651196837425232
Batch 55/64 loss: -0.055734992027282715
Batch 56/64 loss: -0.07027560472488403
Batch 57/64 loss: -0.05512368679046631
Batch 58/64 loss: -0.06876468658447266
Batch 59/64 loss: -0.04343348741531372
Batch 60/64 loss: -0.07895958423614502
Batch 61/64 loss: -0.08366811275482178
Batch 62/64 loss: -0.06602728366851807
Batch 63/64 loss: -0.07302141189575195
Batch 64/64 loss: -0.06237649917602539
Epoch 500  Train loss: -0.05909884116228889  Val loss: 0.10244393676416981
SLIC undersegmentation error: 0.09655395189003434
SLIC inter-cluster variation: 0.08782060068082356
SLIC number of superpixels: 38426
SLIC superpixels per image: 132.04810996563575
Model loaded
Test metrics:
0.08101656060038563 0.2853676975945017 11.771134642447006 tensor(0.1615, dtype=torch.float64) 0.7124215658846569 2.5744174893089804 50042
Inference time: 0.004283594511628561 seconds
Relabeled undersegmentation error: 0.0851683848797251
Relabeled inter-cluster variation: 0.0396568321636227
Relabeled mean superpixels count: 442.7147766323024
Original mean superpixels count: 171.96563573883162
Done!
Job id: 420582
Job id: 422805
Job id: 422845
