Started preprocessing dataset
Number of training samples: 2040
Number of validation samples: 582
Number of testing samples: 291
Using cuda device
Epoch 1
-------------------------------
Batch 1/64 loss: 0.3918294906616211
Batch 2/64 loss: 0.32290834188461304
Batch 3/64 loss: 0.32194089889526367
Batch 4/64 loss: 0.29082977771759033
Batch 5/64 loss: 0.28064364194869995
Batch 6/64 loss: 0.2788110375404358
Batch 7/64 loss: 0.26784223318099976
Batch 8/64 loss: 0.2663946747779846
Batch 9/64 loss: 0.26205873489379883
Batch 10/64 loss: 0.2685176134109497
Batch 11/64 loss: 0.25543296337127686
Batch 12/64 loss: 0.2573535442352295
Batch 13/64 loss: 0.2543177008628845
Batch 14/64 loss: 0.25212836265563965
Batch 15/64 loss: 0.256987988948822
Batch 16/64 loss: 0.24388539791107178
Batch 17/64 loss: 0.24712097644805908
Batch 18/64 loss: 0.24545931816101074
Batch 19/64 loss: 0.23233449459075928
Batch 20/64 loss: 0.22594046592712402
Batch 21/64 loss: 0.22503119707107544
Batch 22/64 loss: 0.23081129789352417
Batch 23/64 loss: 0.2347591519355774
Batch 24/64 loss: 0.23086488246917725
Batch 25/64 loss: 0.2158799171447754
Batch 26/64 loss: 0.22716331481933594
Batch 27/64 loss: 0.22260278463363647
Batch 28/64 loss: 0.22886431217193604
Batch 29/64 loss: 0.23434734344482422
Batch 30/64 loss: 0.2192530632019043
Batch 31/64 loss: 0.2139650583267212
Batch 32/64 loss: 0.2085949182510376
Batch 33/64 loss: 0.20601117610931396
Batch 34/64 loss: 0.2082725167274475
Batch 35/64 loss: 0.21411579847335815
Batch 36/64 loss: 0.21165096759796143
Batch 37/64 loss: 0.2175368070602417
Batch 38/64 loss: 0.21665722131729126
Batch 39/64 loss: 0.20689928531646729
Batch 40/64 loss: 0.21625185012817383
Batch 41/64 loss: 0.19196224212646484
Batch 42/64 loss: 0.2033909559249878
Batch 43/64 loss: 0.21705901622772217
Batch 44/64 loss: 0.18896764516830444
Batch 45/64 loss: 0.18608510494232178
Batch 46/64 loss: 0.1921234130859375
Batch 47/64 loss: 0.1949673295021057
Batch 48/64 loss: 0.1968880295753479
Batch 49/64 loss: 0.19246399402618408
Batch 50/64 loss: 0.18876397609710693
Batch 51/64 loss: 0.19227004051208496
Batch 52/64 loss: 0.1940762996673584
Batch 53/64 loss: 0.19995403289794922
Batch 54/64 loss: 0.1845475435256958
Batch 55/64 loss: 0.1818915605545044
Batch 56/64 loss: 0.20243597030639648
Batch 57/64 loss: 0.19087278842926025
Batch 58/64 loss: 0.17654043436050415
Batch 59/64 loss: 0.18877369165420532
Batch 60/64 loss: 0.1794668436050415
Batch 61/64 loss: 0.19397211074829102
Batch 62/64 loss: 0.1856222152709961
Batch 63/64 loss: 0.17137134075164795
Batch 64/64 loss: 0.17338472604751587
Epoch 1  Train loss: 0.2248905983625674  Val loss: 0.21074610477460617
Saving best model, epoch: 1
Epoch 2
-------------------------------
Batch 1/64 loss: 0.17950189113616943
Batch 2/64 loss: 0.1716570258140564
Batch 3/64 loss: 0.17008012533187866
Batch 4/64 loss: 0.18096059560775757
Batch 5/64 loss: 0.17573457956314087
Batch 6/64 loss: 0.18282407522201538
Batch 7/64 loss: 0.15140950679779053
Batch 8/64 loss: 0.16560935974121094
Batch 9/64 loss: 0.17389899492263794
Batch 10/64 loss: 0.16994714736938477
Batch 11/64 loss: 0.1669113039970398
Batch 12/64 loss: 0.18474537134170532
Batch 13/64 loss: 0.18170428276062012
Batch 14/64 loss: 0.1645161509513855
Batch 15/64 loss: 0.16497719287872314
Batch 16/64 loss: 0.1893405318260193
Batch 17/64 loss: 0.17331302165985107
Batch 18/64 loss: 0.1591622233390808
Batch 19/64 loss: 0.1646067500114441
Batch 20/64 loss: 0.16181612014770508
Batch 21/64 loss: 0.1701728105545044
Batch 22/64 loss: 0.17078649997711182
Batch 23/64 loss: 0.19005054235458374
Batch 24/64 loss: 0.17347514629364014
Batch 25/64 loss: 0.1762220859527588
Batch 26/64 loss: 0.1588989496231079
Batch 27/64 loss: 0.14717090129852295
Batch 28/64 loss: 0.1607041358947754
Batch 29/64 loss: 0.15901821851730347
Batch 30/64 loss: 0.1534414291381836
Batch 31/64 loss: 0.15611737966537476
Batch 32/64 loss: 0.17615211009979248
Batch 33/64 loss: 0.1706889271736145
Batch 34/64 loss: 0.15190309286117554
Batch 35/64 loss: 0.15847933292388916
Batch 36/64 loss: 0.14999961853027344
Batch 37/64 loss: 0.16805845499038696
Batch 38/64 loss: 0.1475834846496582
Batch 39/64 loss: 0.16296708583831787
Batch 40/64 loss: 0.16436302661895752
Batch 41/64 loss: 0.15476524829864502
Batch 42/64 loss: 0.12779772281646729
Batch 43/64 loss: 0.1459299921989441
Batch 44/64 loss: 0.16360175609588623
Batch 45/64 loss: 0.15123796463012695
Batch 46/64 loss: 0.1465315818786621
Batch 47/64 loss: 0.14768081903457642
Batch 48/64 loss: 0.15554964542388916
Batch 49/64 loss: 0.15024596452713013
Batch 50/64 loss: 0.143055260181427
Batch 51/64 loss: 0.15755730867385864
Batch 52/64 loss: 0.13991773128509521
Batch 53/64 loss: 0.15222787857055664
Batch 54/64 loss: 0.14528560638427734
Batch 55/64 loss: 0.14413654804229736
Batch 56/64 loss: 0.16929161548614502
Batch 57/64 loss: 0.15535855293273926
Batch 58/64 loss: 0.154654860496521
Batch 59/64 loss: 0.13741183280944824
Batch 60/64 loss: 0.14106464385986328
Batch 61/64 loss: 0.13841968774795532
Batch 62/64 loss: 0.15219265222549438
Batch 63/64 loss: 0.14671587944030762
Batch 64/64 loss: 0.1396484375
Epoch 2  Train loss: 0.16038175003201355  Val loss: 0.15781704400413224
Saving best model, epoch: 2
Epoch 3
-------------------------------
Batch 1/64 loss: 0.13492661714553833
Batch 2/64 loss: 0.15429669618606567
Batch 3/64 loss: 0.1376989483833313
Batch 4/64 loss: 0.14781492948532104
Batch 5/64 loss: 0.14763343334197998
Batch 6/64 loss: 0.15760105848312378
Batch 7/64 loss: 0.14373993873596191
Batch 8/64 loss: 0.1439274549484253
Batch 9/64 loss: 0.15809935331344604
Batch 10/64 loss: 0.13457202911376953
Batch 11/64 loss: 0.1353742480278015
Batch 12/64 loss: 0.1660996675491333
Batch 13/64 loss: 0.1249164342880249
Batch 14/64 loss: 0.14739447832107544
Batch 15/64 loss: 0.15106523036956787
Batch 16/64 loss: 0.1408231258392334
Batch 17/64 loss: 0.1383036971092224
Batch 18/64 loss: 0.16848689317703247
Batch 19/64 loss: 0.1468970775604248
Batch 20/64 loss: 0.14424824714660645
Batch 21/64 loss: 0.13836896419525146
Batch 22/64 loss: 0.14014029502868652
Batch 23/64 loss: 0.15383023023605347
Batch 24/64 loss: 0.12546312808990479
Batch 25/64 loss: 0.13825690746307373
Batch 26/64 loss: 0.13094699382781982
Batch 27/64 loss: 0.13942617177963257
Batch 28/64 loss: 0.1328643560409546
Batch 29/64 loss: 0.14356935024261475
Batch 30/64 loss: 0.1322016716003418
Batch 31/64 loss: 0.13828623294830322
Batch 32/64 loss: 0.12798786163330078
Batch 33/64 loss: 0.1305222511291504
Batch 34/64 loss: 0.14299333095550537
Batch 35/64 loss: 0.135637104511261
Batch 36/64 loss: 0.14133012294769287
Batch 37/64 loss: 0.12594741582870483
Batch 38/64 loss: 0.1469407081604004
Batch 39/64 loss: 0.12748479843139648
Batch 40/64 loss: 0.1394345760345459
Batch 41/64 loss: 0.11648595333099365
Batch 42/64 loss: 0.15299564599990845
Batch 43/64 loss: 0.14000606536865234
Batch 44/64 loss: 0.13330566883087158
Batch 45/64 loss: 0.15139275789260864
Batch 46/64 loss: 0.12883460521697998
Batch 47/64 loss: 0.12902885675430298
Batch 48/64 loss: 0.13046085834503174
Batch 49/64 loss: 0.12873339653015137
Batch 50/64 loss: 0.11711734533309937
Batch 51/64 loss: 0.13847076892852783
Batch 52/64 loss: 0.12807250022888184
Batch 53/64 loss: 0.11773693561553955
Batch 54/64 loss: 0.12074846029281616
Batch 55/64 loss: 0.12486326694488525
Batch 56/64 loss: 0.13059550523757935
Batch 57/64 loss: 0.12213551998138428
Batch 58/64 loss: 0.12211710214614868
Batch 59/64 loss: 0.11490750312805176
Batch 60/64 loss: 0.11705124378204346
Batch 61/64 loss: 0.13199788331985474
Batch 62/64 loss: 0.13845932483673096
Batch 63/64 loss: 0.13958740234375
Batch 64/64 loss: 0.12718737125396729
Epoch 3  Train loss: 0.1368791706421796  Val loss: 0.13122512692028715
Saving best model, epoch: 3
Epoch 4
-------------------------------
Batch 1/64 loss: 0.10844707489013672
Batch 2/64 loss: 0.11993741989135742
Batch 3/64 loss: 0.11634117364883423
Batch 4/64 loss: 0.11133670806884766
Batch 5/64 loss: 0.12540537118911743
Batch 6/64 loss: 0.11615383625030518
Batch 7/64 loss: 0.12621194124221802
Batch 8/64 loss: 0.11813521385192871
Batch 9/64 loss: 0.12294679880142212
Batch 10/64 loss: 0.13508731126785278
Batch 11/64 loss: 0.12981778383255005
Batch 12/64 loss: 0.11237955093383789
Batch 13/64 loss: 0.10750436782836914
Batch 14/64 loss: 0.11022382974624634
Batch 15/64 loss: 0.12999582290649414
Batch 16/64 loss: 0.13590079545974731
Batch 17/64 loss: 0.11978363990783691
Batch 18/64 loss: 0.10691285133361816
Batch 19/64 loss: 0.11536037921905518
Batch 20/64 loss: 0.12859588861465454
Batch 21/64 loss: 0.13261479139328003
Batch 22/64 loss: 0.1112523078918457
Batch 23/64 loss: 0.12221670150756836
Batch 24/64 loss: 0.11912953853607178
Batch 25/64 loss: 0.11272978782653809
Batch 26/64 loss: 0.09758371114730835
Batch 27/64 loss: 0.12871122360229492
Batch 28/64 loss: 0.1118849515914917
Batch 29/64 loss: 0.09861528873443604
Batch 30/64 loss: 0.11135733127593994
Batch 31/64 loss: 0.11381888389587402
Batch 32/64 loss: 0.10551261901855469
Batch 33/64 loss: 0.12121176719665527
Batch 34/64 loss: 0.1052253246307373
Batch 35/64 loss: 0.09962981939315796
Batch 36/64 loss: 0.1049845814704895
Batch 37/64 loss: 0.0933074951171875
Batch 38/64 loss: 0.10302484035491943
Batch 39/64 loss: 0.1039586067199707
Batch 40/64 loss: 0.11544269323348999
Batch 41/64 loss: 0.11528337001800537
Batch 42/64 loss: 0.1202353835105896
Batch 43/64 loss: 0.10296231508255005
Batch 44/64 loss: 0.12905102968215942
Batch 45/64 loss: 0.10460031032562256
Batch 46/64 loss: 0.10346424579620361
Batch 47/64 loss: 0.10324692726135254
Batch 48/64 loss: 0.09519743919372559
Batch 49/64 loss: 0.09652191400527954
Batch 50/64 loss: 0.08089649677276611
Batch 51/64 loss: 0.10692423582077026
Batch 52/64 loss: 0.09101980924606323
Batch 53/64 loss: 0.11543893814086914
Batch 54/64 loss: 0.09944707155227661
Batch 55/64 loss: 0.11553174257278442
Batch 56/64 loss: 0.11803007125854492
Batch 57/64 loss: 0.11982196569442749
Batch 58/64 loss: 0.10309290885925293
Batch 59/64 loss: 0.0947832465171814
Batch 60/64 loss: 0.08680164813995361
Batch 61/64 loss: 0.1103355884552002
Batch 62/64 loss: 0.10207700729370117
Batch 63/64 loss: 0.09216177463531494
Batch 64/64 loss: 0.11496245861053467
Epoch 4  Train loss: 0.11140136952493705  Val loss: 0.11539535485592085
Saving best model, epoch: 4
Epoch 5
-------------------------------
Batch 1/64 loss: 0.10341000556945801
Batch 2/64 loss: 0.09074956178665161
Batch 3/64 loss: 0.11710405349731445
Batch 4/64 loss: 0.10456174612045288
Batch 5/64 loss: 0.07344180345535278
Batch 6/64 loss: 0.09826689958572388
Batch 7/64 loss: 0.08890831470489502
Batch 8/64 loss: 0.10311305522918701
Batch 9/64 loss: 0.09669429063796997
Batch 10/64 loss: 0.10882854461669922
Batch 11/64 loss: 0.09388083219528198
Batch 12/64 loss: 0.09335547685623169
Batch 13/64 loss: 0.08978068828582764
Batch 14/64 loss: 0.09533065557479858
Batch 15/64 loss: 0.0840003490447998
Batch 16/64 loss: 0.09520173072814941
Batch 17/64 loss: 0.08371686935424805
Batch 18/64 loss: 0.10806530714035034
Batch 19/64 loss: 0.09473627805709839
Batch 20/64 loss: 0.10700607299804688
Batch 21/64 loss: 0.06716811656951904
Batch 22/64 loss: 0.11648648977279663
Batch 23/64 loss: 0.10498261451721191
Batch 24/64 loss: 0.08137309551239014
Batch 25/64 loss: 0.0799717903137207
Batch 26/64 loss: 0.089496910572052
Batch 27/64 loss: 0.0881354808807373
Batch 28/64 loss: 0.0909186601638794
Batch 29/64 loss: 0.09441685676574707
Batch 30/64 loss: 0.11836832761764526
Batch 31/64 loss: 0.10069024562835693
Batch 32/64 loss: 0.09072250127792358
Batch 33/64 loss: 0.07756650447845459
Batch 34/64 loss: 0.08315807580947876
Batch 35/64 loss: 0.10484552383422852
Batch 36/64 loss: 0.08115482330322266
Batch 37/64 loss: 0.08836841583251953
Batch 38/64 loss: 0.09400057792663574
Batch 39/64 loss: 0.08563679456710815
Batch 40/64 loss: 0.09987527132034302
Batch 41/64 loss: 0.10408949851989746
Batch 42/64 loss: 0.0861891508102417
Batch 43/64 loss: 0.07646733522415161
Batch 44/64 loss: 0.07646268606185913
Batch 45/64 loss: 0.09324461221694946
Batch 46/64 loss: 0.08963817358016968
Batch 47/64 loss: 0.07383060455322266
Batch 48/64 loss: 0.06520134210586548
Batch 49/64 loss: 0.0820353627204895
Batch 50/64 loss: 0.0728691816329956
Batch 51/64 loss: 0.09115564823150635
Batch 52/64 loss: 0.08532601594924927
Batch 53/64 loss: 0.0885048508644104
Batch 54/64 loss: 0.08918207883834839
Batch 55/64 loss: 0.0848771333694458
Batch 56/64 loss: 0.07682639360427856
Batch 57/64 loss: 0.08263552188873291
Batch 58/64 loss: 0.08533042669296265
Batch 59/64 loss: 0.05544281005859375
Batch 60/64 loss: 0.08261960744857788
Batch 61/64 loss: 0.07449907064437866
Batch 62/64 loss: 0.07585585117340088
Batch 63/64 loss: 0.06955134868621826
Batch 64/64 loss: 0.0900580883026123
Epoch 5  Train loss: 0.0893626334620457  Val loss: 0.09061997289100464
Saving best model, epoch: 5
Epoch 6
-------------------------------
Batch 1/64 loss: 0.06719666719436646
Batch 2/64 loss: 0.06423825025558472
Batch 3/64 loss: 0.08380734920501709
Batch 4/64 loss: 0.07072359323501587
Batch 5/64 loss: 0.09502047300338745
Batch 6/64 loss: 0.07101714611053467
Batch 7/64 loss: 0.08434951305389404
Batch 8/64 loss: 0.07257318496704102
Batch 9/64 loss: 0.06421256065368652
Batch 10/64 loss: 0.08682769536972046
Batch 11/64 loss: 0.07538026571273804
Batch 12/64 loss: 0.07590514421463013
Batch 13/64 loss: 0.0807342529296875
Batch 14/64 loss: 0.0732804536819458
Batch 15/64 loss: 0.07862699031829834
Batch 16/64 loss: 0.07182556390762329
Batch 17/64 loss: 0.07454895973205566
Batch 18/64 loss: 0.07072341442108154
Batch 19/64 loss: 0.06042814254760742
Batch 20/64 loss: 0.06395983695983887
Batch 21/64 loss: 0.07908707857131958
Batch 22/64 loss: 0.057198405265808105
Batch 23/64 loss: 0.06423962116241455
Batch 24/64 loss: 0.04086267948150635
Batch 25/64 loss: 0.07111889123916626
Batch 26/64 loss: 0.08270043134689331
Batch 27/64 loss: 0.06841111183166504
Batch 28/64 loss: 0.05741262435913086
Batch 29/64 loss: 0.051116108894348145
Batch 30/64 loss: 0.06678277254104614
Batch 31/64 loss: 0.07452040910720825
Batch 32/64 loss: 0.06916195154190063
Batch 33/64 loss: 0.05004233121871948
Batch 34/64 loss: 0.056002259254455566
Batch 35/64 loss: 0.06483376026153564
Batch 36/64 loss: 0.08302026987075806
Batch 37/64 loss: 0.06998324394226074
Batch 38/64 loss: 0.057604432106018066
Batch 39/64 loss: 0.07610177993774414
Batch 40/64 loss: 0.07341653108596802
Batch 41/64 loss: 0.065396249294281
Batch 42/64 loss: 0.07166087627410889
Batch 43/64 loss: 0.0818827748298645
Batch 44/64 loss: 0.09672760963439941
Batch 45/64 loss: 0.055374205112457275
Batch 46/64 loss: 0.047158002853393555
Batch 47/64 loss: 0.045467495918273926
Batch 48/64 loss: 0.07428675889968872
Batch 49/64 loss: 0.05572652816772461
Batch 50/64 loss: 0.05462944507598877
Batch 51/64 loss: 0.06653064489364624
Batch 52/64 loss: 0.07418358325958252
Batch 53/64 loss: 0.05927729606628418
Batch 54/64 loss: 0.08015096187591553
Batch 55/64 loss: 0.059763550758361816
Batch 56/64 loss: 0.054712891578674316
Batch 57/64 loss: 0.04559445381164551
Batch 58/64 loss: 0.07996535301208496
Batch 59/64 loss: 0.050254881381988525
Batch 60/64 loss: 0.07103472948074341
Batch 61/64 loss: 0.04370397329330444
Batch 62/64 loss: 0.04877251386642456
Batch 63/64 loss: 0.03195840120315552
Batch 64/64 loss: 0.021778345108032227
Epoch 6  Train loss: 0.06634577395869236  Val loss: 0.07882497626071944
Saving best model, epoch: 6
Epoch 7
-------------------------------
Batch 1/64 loss: 0.02537769079208374
Batch 2/64 loss: 0.03984475135803223
Batch 3/64 loss: 0.06087315082550049
Batch 4/64 loss: 0.051654279232025146
Batch 5/64 loss: 0.04037219285964966
Batch 6/64 loss: 0.05578017234802246
Batch 7/64 loss: 0.05221480131149292
Batch 8/64 loss: 0.04507720470428467
Batch 9/64 loss: 0.06649690866470337
Batch 10/64 loss: 0.05133473873138428
Batch 11/64 loss: 0.0413934588432312
Batch 12/64 loss: 0.05779320001602173
Batch 13/64 loss: 0.054041147232055664
Batch 14/64 loss: 0.057178616523742676
Batch 15/64 loss: 0.027191638946533203
Batch 16/64 loss: 0.028336763381958008
Batch 17/64 loss: 0.04783087968826294
Batch 18/64 loss: 0.06842255592346191
Batch 19/64 loss: 0.027867555618286133
Batch 20/64 loss: 0.043525755405426025
Batch 21/64 loss: 0.03700369596481323
Batch 22/64 loss: 0.07278001308441162
Batch 23/64 loss: 0.030129611492156982
Batch 24/64 loss: 0.07173919677734375
Batch 25/64 loss: 0.028887927532196045
Batch 26/64 loss: 0.042433977127075195
Batch 27/64 loss: 0.015723466873168945
Batch 28/64 loss: 0.01587510108947754
Batch 29/64 loss: 0.032940030097961426
Batch 30/64 loss: 0.023807406425476074
Batch 31/64 loss: 0.0313262939453125
Batch 32/64 loss: 0.011622846126556396
Batch 33/64 loss: 0.0324479341506958
Batch 34/64 loss: 0.0317608118057251
Batch 35/64 loss: 0.04881840944290161
Batch 36/64 loss: 0.029517650604248047
Batch 37/64 loss: 0.03365558385848999
Batch 38/64 loss: 0.01606154441833496
Batch 39/64 loss: 0.018609166145324707
Batch 40/64 loss: 0.04212707281112671
Batch 41/64 loss: 0.029635608196258545
Batch 42/64 loss: 0.013140261173248291
Batch 43/64 loss: 0.028875768184661865
Batch 44/64 loss: 0.051579952239990234
Batch 45/64 loss: 0.022730708122253418
Batch 46/64 loss: 0.012594282627105713
Batch 47/64 loss: 0.012415170669555664
Batch 48/64 loss: 0.02962207794189453
Batch 49/64 loss: 0.029296159744262695
Batch 50/64 loss: 0.017891526222229004
Batch 51/64 loss: 0.011321842670440674
Batch 52/64 loss: -0.001365363597869873
Batch 53/64 loss: 0.042461514472961426
Batch 54/64 loss: 0.01454627513885498
Batch 55/64 loss: 0.026452362537384033
Batch 56/64 loss: 0.00871741771697998
Batch 57/64 loss: 0.03653985261917114
Batch 58/64 loss: 0.02600860595703125
Batch 59/64 loss: 0.028470337390899658
Batch 60/64 loss: 0.004734396934509277
Batch 61/64 loss: -0.0021671056747436523
Batch 62/64 loss: 0.004489481449127197
Batch 63/64 loss: 0.030494630336761475
Batch 64/64 loss: 0.04626142978668213
Epoch 7  Train loss: 0.03330285549163818  Val loss: 0.07152527365897529
Saving best model, epoch: 7
Epoch 8
-------------------------------
Batch 1/64 loss: 0.017975986003875732
Batch 2/64 loss: 0.005040526390075684
Batch 3/64 loss: 0.030546188354492188
Batch 4/64 loss: 0.014512419700622559
Batch 5/64 loss: 0.043697476387023926
Batch 6/64 loss: 0.008666396141052246
Batch 7/64 loss: 0.02140820026397705
Batch 8/64 loss: 0.006976485252380371
Batch 9/64 loss: -0.005303621292114258
Batch 10/64 loss: 0.008997201919555664
Batch 11/64 loss: -0.0005998611450195312
Batch 12/64 loss: 0.02671980857849121
Batch 13/64 loss: 0.001141667366027832
Batch 14/64 loss: 0.01160341501235962
Batch 15/64 loss: 0.01058274507522583
Batch 16/64 loss: 0.003815293312072754
Batch 17/64 loss: 0.015331625938415527
Batch 18/64 loss: 0.017605841159820557
Batch 19/64 loss: -0.020423173904418945
Batch 20/64 loss: 0.0008239150047302246
Batch 21/64 loss: -0.03215646743774414
Batch 22/64 loss: -0.004864871501922607
Batch 23/64 loss: 0.002398550510406494
Batch 24/64 loss: 0.009645342826843262
Batch 25/64 loss: -0.0031132102012634277
Batch 26/64 loss: 0.02245539426803589
Batch 27/64 loss: -0.026310265064239502
Batch 28/64 loss: -0.0031696557998657227
Batch 29/64 loss: -0.006569802761077881
Batch 30/64 loss: -0.005233466625213623
Batch 31/64 loss: 0.014746487140655518
Batch 32/64 loss: -0.0007246136665344238
Batch 33/64 loss: -0.01299351453781128
Batch 34/64 loss: -0.023746609687805176
Batch 35/64 loss: -0.009781122207641602
Batch 36/64 loss: -0.011062860488891602
Batch 37/64 loss: 0.0008954405784606934
Batch 38/64 loss: 0.0185202956199646
Batch 39/64 loss: 0.0050688982009887695
Batch 40/64 loss: 0.02231663465499878
Batch 41/64 loss: -0.0015017390251159668
Batch 42/64 loss: -0.03308075666427612
Batch 43/64 loss: 0.010811209678649902
Batch 44/64 loss: -0.0025277137756347656
Batch 45/64 loss: -0.022219836711883545
Batch 46/64 loss: 0.00035119056701660156
Batch 47/64 loss: -0.022935092449188232
Batch 48/64 loss: 0.01817852258682251
Batch 49/64 loss: 0.009359240531921387
Batch 50/64 loss: -0.009642481803894043
Batch 51/64 loss: 0.0012878775596618652
Batch 52/64 loss: 0.01114654541015625
Batch 53/64 loss: 0.010288596153259277
Batch 54/64 loss: -0.02747821807861328
Batch 55/64 loss: -0.01708090305328369
Batch 56/64 loss: -0.024575531482696533
Batch 57/64 loss: -0.03919702768325806
Batch 58/64 loss: -0.0055201053619384766
Batch 59/64 loss: -0.02571260929107666
Batch 60/64 loss: 0.03128772974014282
Batch 61/64 loss: 0.007449150085449219
Batch 62/64 loss: -0.014615774154663086
Batch 63/64 loss: -0.021942734718322754
Batch 64/64 loss: -0.026670336723327637
Epoch 8  Train loss: -0.00019504462971406826  Val loss: 0.0400694006497098
Saving best model, epoch: 8
Epoch 9
-------------------------------
Batch 1/64 loss: -0.035285353660583496
Batch 2/64 loss: -0.006805062294006348
Batch 3/64 loss: -0.00809401273727417
Batch 4/64 loss: -0.000171661376953125
Batch 5/64 loss: -0.038997530937194824
Batch 6/64 loss: 0.007301032543182373
Batch 7/64 loss: -0.012562155723571777
Batch 8/64 loss: -0.0040662288665771484
Batch 9/64 loss: -0.01758408546447754
Batch 10/64 loss: -0.019468188285827637
Batch 11/64 loss: -0.023721277713775635
Batch 12/64 loss: -0.04674345254898071
Batch 13/64 loss: -0.04256206750869751
Batch 14/64 loss: -0.022739768028259277
Batch 15/64 loss: -0.0040547847747802734
Batch 16/64 loss: -0.028737664222717285
Batch 17/64 loss: 0.017301321029663086
Batch 18/64 loss: -0.018853068351745605
Batch 19/64 loss: -0.028268158435821533
Batch 20/64 loss: 0.0007524490356445312
Batch 21/64 loss: 0.013786256313323975
Batch 22/64 loss: -0.003519117832183838
Batch 23/64 loss: 0.013285040855407715
Batch 24/64 loss: -0.0002773404121398926
Batch 25/64 loss: -0.004051029682159424
Batch 26/64 loss: -0.0496554970741272
Batch 27/64 loss: -0.01276099681854248
Batch 28/64 loss: -0.018183350563049316
Batch 29/64 loss: -0.016285061836242676
Batch 30/64 loss: -0.03144460916519165
Batch 31/64 loss: -0.019827961921691895
Batch 32/64 loss: -0.023193001747131348
Batch 33/64 loss: -0.027645766735076904
Batch 34/64 loss: -0.01734340190887451
Batch 35/64 loss: -0.013169586658477783
Batch 36/64 loss: -0.030881285667419434
Batch 37/64 loss: -0.01686793565750122
Batch 38/64 loss: -0.04756307601928711
Batch 39/64 loss: -0.02255147695541382
Batch 40/64 loss: -0.015115320682525635
Batch 41/64 loss: -0.041789352893829346
Batch 42/64 loss: -0.05300873517990112
Batch 43/64 loss: -0.03977388143539429
Batch 44/64 loss: -0.008351743221282959
Batch 45/64 loss: -0.02771657705307007
Batch 46/64 loss: -0.024507999420166016
Batch 47/64 loss: -0.04067867994308472
Batch 48/64 loss: -0.03438091278076172
Batch 49/64 loss: -0.027495861053466797
Batch 50/64 loss: -0.027778685092926025
Batch 51/64 loss: -0.0022638440132141113
Batch 52/64 loss: -0.02137017250061035
Batch 53/64 loss: -0.021093666553497314
Batch 54/64 loss: -0.03333956003189087
Batch 55/64 loss: -0.04956376552581787
Batch 56/64 loss: -0.029718339443206787
Batch 57/64 loss: -0.01886272430419922
Batch 58/64 loss: -0.028264284133911133
Batch 59/64 loss: -0.049104154109954834
Batch 60/64 loss: -0.022439420223236084
Batch 61/64 loss: -0.019568324089050293
Batch 62/64 loss: -0.023617267608642578
Batch 63/64 loss: -0.03951239585876465
Batch 64/64 loss: -0.02071624994277954
Epoch 9  Train loss: -0.02158998858694937  Val loss: -0.0016849665707329294
Saving best model, epoch: 9
Epoch 10
-------------------------------
Batch 1/64 loss: -0.04128921031951904
Batch 2/64 loss: -0.04712170362472534
Batch 3/64 loss: -0.016456902027130127
Batch 4/64 loss: -0.044457852840423584
Batch 5/64 loss: -0.060250818729400635
Batch 6/64 loss: -0.02910745143890381
Batch 7/64 loss: -0.05192208290100098
Batch 8/64 loss: -0.03701275587081909
Batch 9/64 loss: -0.04271572828292847
Batch 10/64 loss: -0.010418057441711426
Batch 11/64 loss: -0.018618404865264893
Batch 12/64 loss: -0.03957498073577881
Batch 13/64 loss: -0.04629158973693848
Batch 14/64 loss: -0.03963494300842285
Batch 15/64 loss: -0.05782109498977661
Batch 16/64 loss: -0.05615031719207764
Batch 17/64 loss: -0.052233755588531494
Batch 18/64 loss: -0.04818379878997803
Batch 19/64 loss: -0.047473251819610596
Batch 20/64 loss: -0.045236170291900635
Batch 21/64 loss: -0.048962414264678955
Batch 22/64 loss: -0.015435993671417236
Batch 23/64 loss: -0.016106128692626953
Batch 24/64 loss: -0.038687288761138916
Batch 25/64 loss: -0.04008060693740845
Batch 26/64 loss: -0.06060594320297241
Batch 27/64 loss: -0.026318252086639404
Batch 28/64 loss: -0.05549067258834839
Batch 29/64 loss: -0.041435062885284424
Batch 30/64 loss: -0.03324085474014282
Batch 31/64 loss: -0.06312859058380127
Batch 32/64 loss: -0.017976820468902588
Batch 33/64 loss: -0.0523642897605896
Batch 34/64 loss: -0.041589319705963135
Batch 35/64 loss: -0.05869925022125244
Batch 36/64 loss: -0.05632823705673218
Batch 37/64 loss: -0.06224697828292847
Batch 38/64 loss: -0.04002350568771362
Batch 39/64 loss: -0.04903876781463623
Batch 40/64 loss: -0.017482995986938477
Batch 41/64 loss: -0.041759610176086426
Batch 42/64 loss: -0.043848514556884766
Batch 43/64 loss: -0.030526399612426758
Batch 44/64 loss: -0.04697519540786743
Batch 45/64 loss: -0.02668595314025879
Batch 46/64 loss: -0.03707611560821533
Batch 47/64 loss: -0.04905897378921509
Batch 48/64 loss: -0.07518750429153442
Batch 49/64 loss: -0.051819801330566406
Batch 50/64 loss: -0.025805771350860596
Batch 51/64 loss: -0.06322157382965088
Batch 52/64 loss: -0.04810285568237305
Batch 53/64 loss: -0.06168478727340698
Batch 54/64 loss: -0.020313143730163574
Batch 55/64 loss: -0.0389026403427124
Batch 56/64 loss: -0.03839373588562012
Batch 57/64 loss: -0.05405908823013306
Batch 58/64 loss: -0.06945580244064331
Batch 59/64 loss: -0.044794678688049316
Batch 60/64 loss: 0.002958536148071289
Batch 61/64 loss: -0.03854340314865112
Batch 62/64 loss: -0.0630117654800415
Batch 63/64 loss: -0.03436470031738281
Batch 64/64 loss: -0.027597904205322266
Epoch 10  Train loss: -0.0421732509837431  Val loss: -0.014031594561547348
Saving best model, epoch: 10
Epoch 11
-------------------------------
Batch 1/64 loss: -0.06901848316192627
Batch 2/64 loss: -0.06317061185836792
Batch 3/64 loss: -0.04778575897216797
Batch 4/64 loss: -0.05077707767486572
Batch 5/64 loss: -0.041465163230895996
Batch 6/64 loss: -0.05262625217437744
Batch 7/64 loss: -0.022217154502868652
Batch 8/64 loss: -0.07921230792999268
Batch 9/64 loss: -0.036575376987457275
Batch 10/64 loss: -0.0397220253944397
Batch 11/64 loss: -0.010125041007995605
Batch 12/64 loss: -0.03491806983947754
Batch 13/64 loss: -0.03678452968597412
Batch 14/64 loss: -0.054712653160095215
Batch 15/64 loss: -0.07351738214492798
Batch 16/64 loss: -0.03963881731033325
Batch 17/64 loss: -0.04446554183959961
Batch 18/64 loss: -0.03705418109893799
Batch 19/64 loss: -0.025065481662750244
Batch 20/64 loss: -0.06387209892272949
Batch 21/64 loss: -0.03506171703338623
Batch 22/64 loss: -0.049087703227996826
Batch 23/64 loss: -0.056745290756225586
Batch 24/64 loss: -0.05292212963104248
Batch 25/64 loss: -0.06650185585021973
Batch 26/64 loss: -0.07808804512023926
Batch 27/64 loss: -0.07069885730743408
Batch 28/64 loss: -0.05286252498626709
Batch 29/64 loss: -0.012414932250976562
Batch 30/64 loss: -0.05272519588470459
Batch 31/64 loss: -0.05934172868728638
Batch 32/64 loss: -0.02765488624572754
Batch 33/64 loss: -0.06782138347625732
Batch 34/64 loss: -0.06396830081939697
Batch 35/64 loss: -0.05707794427871704
Batch 36/64 loss: -0.027362406253814697
Batch 37/64 loss: -0.03800606727600098
Batch 38/64 loss: -0.04457592964172363
Batch 39/64 loss: -0.06650650501251221
Batch 40/64 loss: -0.07616358995437622
Batch 41/64 loss: -0.05615276098251343
Batch 42/64 loss: -0.06441992521286011
Batch 43/64 loss: -0.036077141761779785
Batch 44/64 loss: -0.046463072299957275
Batch 45/64 loss: -0.07104402780532837
Batch 46/64 loss: -0.03650873899459839
Batch 47/64 loss: -0.0751386284828186
Batch 48/64 loss: -0.07425159215927124
Batch 49/64 loss: -0.05224275588989258
Batch 50/64 loss: -0.04634577035903931
Batch 51/64 loss: -0.04431021213531494
Batch 52/64 loss: -0.04596567153930664
Batch 53/64 loss: -0.07186245918273926
Batch 54/64 loss: -0.05064266920089722
Batch 55/64 loss: -0.025444984436035156
Batch 56/64 loss: -0.07728737592697144
Batch 57/64 loss: -0.03937971591949463
Batch 58/64 loss: -0.046021342277526855
Batch 59/64 loss: -0.058476388454437256
Batch 60/64 loss: -0.06122910976409912
Batch 61/64 loss: -0.06194502115249634
Batch 62/64 loss: -0.05034071207046509
Batch 63/64 loss: -0.07522177696228027
Batch 64/64 loss: -0.09553408622741699
Epoch 11  Train loss: -0.052027190900316427  Val loss: -0.008072686973715975
Epoch 12
-------------------------------
Batch 1/64 loss: -0.05480128526687622
Batch 2/64 loss: -0.05051064491271973
Batch 3/64 loss: -0.05341249704360962
Batch 4/64 loss: -0.05952739715576172
Batch 5/64 loss: -0.052844464778900146
Batch 6/64 loss: -0.0789029598236084
Batch 7/64 loss: -0.035625338554382324
Batch 8/64 loss: -0.046190500259399414
Batch 9/64 loss: -0.05704379081726074
Batch 10/64 loss: -0.05911612510681152
Batch 11/64 loss: -0.05827075242996216
Batch 12/64 loss: -0.08743762969970703
Batch 13/64 loss: -0.0805513858795166
Batch 14/64 loss: -0.06836849451065063
Batch 15/64 loss: -0.049901723861694336
Batch 16/64 loss: -0.10400128364562988
Batch 17/64 loss: -0.029417097568511963
Batch 18/64 loss: -0.06040006875991821
Batch 19/64 loss: -0.08572518825531006
Batch 20/64 loss: -0.05001789331436157
Batch 21/64 loss: -0.038595616817474365
Batch 22/64 loss: -0.02149665355682373
Batch 23/64 loss: -0.07494992017745972
Batch 24/64 loss: -0.08495914936065674
Batch 25/64 loss: -0.06313294172286987
Batch 26/64 loss: -0.05966466665267944
Batch 27/64 loss: -0.05904579162597656
Batch 28/64 loss: -0.06380289793014526
Batch 29/64 loss: -0.06327855587005615
Batch 30/64 loss: -0.07731419801712036
Batch 31/64 loss: -0.08600640296936035
Batch 32/64 loss: -0.06909328699111938
Batch 33/64 loss: -0.03949624300003052
Batch 34/64 loss: -0.06011444330215454
Batch 35/64 loss: -0.07430106401443481
Batch 36/64 loss: -0.047540128231048584
Batch 37/64 loss: -0.05440187454223633
Batch 38/64 loss: -0.060775041580200195
Batch 39/64 loss: -0.06724011898040771
Batch 40/64 loss: -0.06010091304779053
Batch 41/64 loss: -0.08153301477432251
Batch 42/64 loss: -0.047028541564941406
Batch 43/64 loss: -0.08317673206329346
Batch 44/64 loss: -0.06593620777130127
Batch 45/64 loss: -0.07151526212692261
Batch 46/64 loss: -0.07898104190826416
Batch 47/64 loss: -0.06332135200500488
Batch 48/64 loss: -0.05736464262008667
Batch 49/64 loss: -0.03907763957977295
Batch 50/64 loss: -0.020068764686584473
Batch 51/64 loss: -0.054038286209106445
Batch 52/64 loss: -0.04000258445739746
Batch 53/64 loss: -0.05395108461380005
Batch 54/64 loss: -0.0860099196434021
Batch 55/64 loss: -0.1134786605834961
Batch 56/64 loss: -0.06671899557113647
Batch 57/64 loss: -0.07621991634368896
Batch 58/64 loss: -0.030488431453704834
Batch 59/64 loss: -0.025015532970428467
Batch 60/64 loss: -0.08568805456161499
Batch 61/64 loss: -0.08277499675750732
Batch 62/64 loss: -0.044979631900787354
Batch 63/64 loss: -0.06817448139190674
Batch 64/64 loss: -0.08430254459381104
Epoch 12  Train loss: -0.06190034688687792  Val loss: 0.03666024273613474
Epoch 13
-------------------------------
Batch 1/64 loss: -0.0701589584350586
Batch 2/64 loss: -0.028093457221984863
Batch 3/64 loss: -0.07078999280929565
Batch 4/64 loss: -0.05614328384399414
Batch 5/64 loss: -0.07765990495681763
Batch 6/64 loss: -0.10578924417495728
Batch 7/64 loss: -0.058603644371032715
Batch 8/64 loss: -0.07605266571044922
Batch 9/64 loss: -0.07594156265258789
Batch 10/64 loss: -0.040697693824768066
Batch 11/64 loss: -0.04623144865036011
Batch 12/64 loss: -0.09956061840057373
Batch 13/64 loss: -0.05874592065811157
Batch 14/64 loss: -0.08199739456176758
Batch 15/64 loss: -0.07490479946136475
Batch 16/64 loss: -0.05369746685028076
Batch 17/64 loss: -0.06548136472702026
Batch 18/64 loss: -0.06553232669830322
Batch 19/64 loss: -0.08311748504638672
Batch 20/64 loss: -0.03753006458282471
Batch 21/64 loss: -0.06899654865264893
Batch 22/64 loss: -0.06774133443832397
Batch 23/64 loss: -0.07175099849700928
Batch 24/64 loss: -0.03887069225311279
Batch 25/64 loss: -0.06128638982772827
Batch 26/64 loss: -0.07498204708099365
Batch 27/64 loss: -0.07735133171081543
Batch 28/64 loss: -0.09372800588607788
Batch 29/64 loss: -0.061725080013275146
Batch 30/64 loss: -0.08277738094329834
Batch 31/64 loss: -0.08842778205871582
Batch 32/64 loss: -0.0842599868774414
Batch 33/64 loss: -0.09692472219467163
Batch 34/64 loss: -0.04561781883239746
Batch 35/64 loss: -0.060965776443481445
Batch 36/64 loss: -0.08070123195648193
Batch 37/64 loss: -0.039995431900024414
Batch 38/64 loss: -0.06525206565856934
Batch 39/64 loss: -0.06525659561157227
Batch 40/64 loss: -0.06958705186843872
Batch 41/64 loss: -0.06270599365234375
Batch 42/64 loss: -0.09039461612701416
Batch 43/64 loss: -0.068104088306427
Batch 44/64 loss: -0.010251522064208984
Batch 45/64 loss: -0.05792593955993652
Batch 46/64 loss: -0.06214702129364014
Batch 47/64 loss: -0.07005774974822998
Batch 48/64 loss: -0.04789644479751587
Batch 49/64 loss: -0.08490443229675293
Batch 50/64 loss: -0.10552424192428589
Batch 51/64 loss: -0.07706058025360107
Batch 52/64 loss: -0.08153390884399414
Batch 53/64 loss: -0.03153353929519653
Batch 54/64 loss: -0.05498552322387695
Batch 55/64 loss: -0.09069275856018066
Batch 56/64 loss: -0.06525546312332153
Batch 57/64 loss: -0.09767824411392212
Batch 58/64 loss: -0.08389890193939209
Batch 59/64 loss: -0.1014089584350586
Batch 60/64 loss: -0.04428541660308838
Batch 61/64 loss: -0.08696722984313965
Batch 62/64 loss: -0.07681500911712646
Batch 63/64 loss: -0.0681678056716919
Batch 64/64 loss: -0.05485260486602783
Epoch 13  Train loss: -0.06877274372998406  Val loss: -0.043445838071226664
Saving best model, epoch: 13
Epoch 14
-------------------------------
Batch 1/64 loss: -0.05751800537109375
Batch 2/64 loss: -0.09020054340362549
Batch 3/64 loss: -0.08280658721923828
Batch 4/64 loss: -0.06987220048904419
Batch 5/64 loss: -0.06835240125656128
Batch 6/64 loss: -0.04458886384963989
Batch 7/64 loss: -0.09028732776641846
Batch 8/64 loss: -0.07609152793884277
Batch 9/64 loss: -0.05189239978790283
Batch 10/64 loss: -0.06929081678390503
Batch 11/64 loss: -0.07103395462036133
Batch 12/64 loss: -0.08121955394744873
Batch 13/64 loss: -0.08028632402420044
Batch 14/64 loss: -0.09775686264038086
Batch 15/64 loss: -0.0935102105140686
Batch 16/64 loss: -0.05212479829788208
Batch 17/64 loss: -0.08975470066070557
Batch 18/64 loss: -0.0975809097290039
Batch 19/64 loss: -0.054592788219451904
Batch 20/64 loss: -0.09456974267959595
Batch 21/64 loss: -0.07438457012176514
Batch 22/64 loss: -0.08521556854248047
Batch 23/64 loss: -0.07647264003753662
Batch 24/64 loss: -0.06716334819793701
Batch 25/64 loss: -0.054289817810058594
Batch 26/64 loss: -0.08183276653289795
Batch 27/64 loss: -0.09814947843551636
Batch 28/64 loss: -0.08659422397613525
Batch 29/64 loss: -0.084145188331604
Batch 30/64 loss: -0.0722777247428894
Batch 31/64 loss: -0.07536494731903076
Batch 32/64 loss: -0.07978230714797974
Batch 33/64 loss: -0.07952392101287842
Batch 34/64 loss: -0.06300985813140869
Batch 35/64 loss: -0.08234643936157227
Batch 36/64 loss: -0.07297229766845703
Batch 37/64 loss: -0.03739893436431885
Batch 38/64 loss: -0.0401119589805603
Batch 39/64 loss: -0.08079928159713745
Batch 40/64 loss: -0.060157179832458496
Batch 41/64 loss: -0.07127624750137329
Batch 42/64 loss: -0.06437230110168457
Batch 43/64 loss: -0.04586142301559448
Batch 44/64 loss: -0.04636859893798828
Batch 45/64 loss: -0.11251175403594971
Batch 46/64 loss: -0.06185650825500488
Batch 47/64 loss: -0.057336628437042236
Batch 48/64 loss: -0.054127275943756104
Batch 49/64 loss: -0.058895230293273926
Batch 50/64 loss: -0.06628376245498657
Batch 51/64 loss: -0.06977498531341553
Batch 52/64 loss: -0.09690773487091064
Batch 53/64 loss: -0.05118793249130249
Batch 54/64 loss: -0.05072629451751709
Batch 55/64 loss: -0.062087416648864746
Batch 56/64 loss: -0.06662052869796753
Batch 57/64 loss: -0.07591158151626587
Batch 58/64 loss: -0.08186483383178711
Batch 59/64 loss: -0.07462900876998901
Batch 60/64 loss: -0.044432878494262695
Batch 61/64 loss: -0.0952829122543335
Batch 62/64 loss: -0.06295645236968994
Batch 63/64 loss: -0.09270858764648438
Batch 64/64 loss: -0.07677274942398071
Epoch 14  Train loss: -0.07195111232645371  Val loss: -0.029485847327307736
Epoch 15
-------------------------------
Batch 1/64 loss: -0.05480462312698364
Batch 2/64 loss: -0.06558805704116821
Batch 3/64 loss: -0.07924914360046387
Batch 4/64 loss: -0.08148729801177979
Batch 5/64 loss: -0.05955225229263306
Batch 6/64 loss: -0.04404902458190918
Batch 7/64 loss: -0.08346867561340332
Batch 8/64 loss: -0.08187830448150635
Batch 9/64 loss: -0.08492743968963623
Batch 10/64 loss: -0.07389914989471436
Batch 11/64 loss: -0.09038043022155762
Batch 12/64 loss: -0.110348641872406
Batch 13/64 loss: -0.09067326784133911
Batch 14/64 loss: -0.061318039894104004
Batch 15/64 loss: -0.07876861095428467
Batch 16/64 loss: -0.06683039665222168
Batch 17/64 loss: -0.06335818767547607
Batch 18/64 loss: -0.06565308570861816
Batch 19/64 loss: -0.07682961225509644
Batch 20/64 loss: -0.05854976177215576
Batch 21/64 loss: -0.09450274705886841
Batch 22/64 loss: -0.09729653596878052
Batch 23/64 loss: -0.08830660581588745
Batch 24/64 loss: -0.07220858335494995
Batch 25/64 loss: -0.06930279731750488
Batch 26/64 loss: -0.08920824527740479
Batch 27/64 loss: -0.06131565570831299
Batch 28/64 loss: -0.03131788969039917
Batch 29/64 loss: -0.07110148668289185
Batch 30/64 loss: -0.09701991081237793
Batch 31/64 loss: -0.11575782299041748
Batch 32/64 loss: -0.09262222051620483
Batch 33/64 loss: -0.07155793905258179
Batch 34/64 loss: -0.08276140689849854
Batch 35/64 loss: -0.07129079103469849
Batch 36/64 loss: -0.07325100898742676
Batch 37/64 loss: -0.06744909286499023
Batch 38/64 loss: -0.060219645500183105
Batch 39/64 loss: -0.07228755950927734
Batch 40/64 loss: -0.054534077644348145
Batch 41/64 loss: -0.07540690898895264
Batch 42/64 loss: -0.07881999015808105
Batch 43/64 loss: -0.07028210163116455
Batch 44/64 loss: -0.0827530026435852
Batch 45/64 loss: -0.028880536556243896
Batch 46/64 loss: -0.06641387939453125
Batch 47/64 loss: -0.0801464319229126
Batch 48/64 loss: -0.07321149110794067
Batch 49/64 loss: -0.08499598503112793
Batch 50/64 loss: -0.06947320699691772
Batch 51/64 loss: -0.10666167736053467
Batch 52/64 loss: -0.09485507011413574
Batch 53/64 loss: -0.07412540912628174
Batch 54/64 loss: -0.07847976684570312
Batch 55/64 loss: -0.07640647888183594
Batch 56/64 loss: -0.05913823843002319
Batch 57/64 loss: -0.07019925117492676
Batch 58/64 loss: -0.06977659463882446
Batch 59/64 loss: -0.07682079076766968
Batch 60/64 loss: -0.09420454502105713
Batch 61/64 loss: -0.06868547201156616
Batch 62/64 loss: -0.09677928686141968
Batch 63/64 loss: -0.10085070133209229
Batch 64/64 loss: -0.08549082279205322
Epoch 15  Train loss: -0.07602213270523969  Val loss: -0.038278147117378786
Epoch 16
-------------------------------
Batch 1/64 loss: -0.08968466520309448
Batch 2/64 loss: -0.051244378089904785
Batch 3/64 loss: -0.09422034025192261
Batch 4/64 loss: -0.06528043746948242
Batch 5/64 loss: -0.08395713567733765
Batch 6/64 loss: -0.0520821213722229
Batch 7/64 loss: -0.07206547260284424
Batch 8/64 loss: -0.06380569934844971
Batch 9/64 loss: -0.08506333827972412
Batch 10/64 loss: -0.08515667915344238
Batch 11/64 loss: -0.09568268060684204
Batch 12/64 loss: -0.0892522931098938
Batch 13/64 loss: -0.08395183086395264
Batch 14/64 loss: -0.12015682458877563
Batch 15/64 loss: -0.06558007001876831
Batch 16/64 loss: -0.06255590915679932
Batch 17/64 loss: -0.08156782388687134
Batch 18/64 loss: -0.07718032598495483
Batch 19/64 loss: -0.10385090112686157
Batch 20/64 loss: -0.061783790588378906
Batch 21/64 loss: -0.09551584720611572
Batch 22/64 loss: -0.08083963394165039
Batch 23/64 loss: -0.07863092422485352
Batch 24/64 loss: -0.1053343415260315
Batch 25/64 loss: -0.06672859191894531
Batch 26/64 loss: -0.07976102828979492
Batch 27/64 loss: -0.09804672002792358
Batch 28/64 loss: -0.093375563621521
Batch 29/64 loss: -0.07808172702789307
Batch 30/64 loss: -0.07501637935638428
Batch 31/64 loss: -0.1052786111831665
Batch 32/64 loss: -0.11359506845474243
Batch 33/64 loss: -0.08975732326507568
Batch 34/64 loss: -0.0928804874420166
Batch 35/64 loss: -0.07270282506942749
Batch 36/64 loss: -0.09951764345169067
Batch 37/64 loss: -0.07658195495605469
Batch 38/64 loss: -0.10489153861999512
Batch 39/64 loss: -0.06821787357330322
Batch 40/64 loss: -0.05626410245895386
Batch 41/64 loss: -0.10903263092041016
Batch 42/64 loss: -0.09510040283203125
Batch 43/64 loss: -0.06305217742919922
Batch 44/64 loss: -0.07658672332763672
Batch 45/64 loss: -0.07208728790283203
Batch 46/64 loss: -0.07333099842071533
Batch 47/64 loss: -0.09920018911361694
Batch 48/64 loss: -0.0916295051574707
Batch 49/64 loss: -0.0640251636505127
Batch 50/64 loss: -0.06989675760269165
Batch 51/64 loss: -0.09478181600570679
Batch 52/64 loss: -0.0931198000907898
Batch 53/64 loss: -0.10181647539138794
Batch 54/64 loss: -0.07802814245223999
Batch 55/64 loss: -0.07840347290039062
Batch 56/64 loss: -0.10601603984832764
Batch 57/64 loss: -0.09104013442993164
Batch 58/64 loss: -0.08032315969467163
Batch 59/64 loss: -0.08617383241653442
Batch 60/64 loss: -0.10529041290283203
Batch 61/64 loss: -0.10321897268295288
Batch 62/64 loss: -0.07537740468978882
Batch 63/64 loss: -0.08834892511367798
Batch 64/64 loss: -0.09724658727645874
Epoch 16  Train loss: -0.08445421597536873  Val loss: -0.03777048473095976
Epoch 17
-------------------------------
Batch 1/64 loss: -0.039781272411346436
Batch 2/64 loss: -0.11147499084472656
Batch 3/64 loss: -0.10765540599822998
Batch 4/64 loss: -0.1093059778213501
Batch 5/64 loss: -0.11902189254760742
Batch 6/64 loss: -0.09385472536087036
Batch 7/64 loss: -0.11258590221405029
Batch 8/64 loss: -0.0863046646118164
Batch 9/64 loss: -0.07612806558609009
Batch 10/64 loss: -0.06454300880432129
Batch 11/64 loss: -0.09881079196929932
Batch 12/64 loss: -0.10073214769363403
Batch 13/64 loss: -0.08992540836334229
Batch 14/64 loss: -0.10079193115234375
Batch 15/64 loss: -0.08783829212188721
Batch 16/64 loss: -0.06843072175979614
Batch 17/64 loss: -0.07753294706344604
Batch 18/64 loss: -0.09962767362594604
Batch 19/64 loss: -0.10490965843200684
Batch 20/64 loss: -0.11612516641616821
Batch 21/64 loss: -0.08234763145446777
Batch 22/64 loss: -0.10346680879592896
Batch 23/64 loss: -0.0904586911201477
Batch 24/64 loss: -0.08469712734222412
Batch 25/64 loss: -0.10820198059082031
Batch 26/64 loss: -0.11197769641876221
Batch 27/64 loss: -0.09744727611541748
Batch 28/64 loss: -0.10641950368881226
Batch 29/64 loss: -0.08315849304199219
Batch 30/64 loss: -0.08996957540512085
Batch 31/64 loss: -0.05848658084869385
Batch 32/64 loss: -0.11109542846679688
Batch 33/64 loss: -0.1049160361289978
Batch 34/64 loss: -0.10359001159667969
Batch 35/64 loss: -0.09811502695083618
Batch 36/64 loss: -0.1213538646697998
Batch 37/64 loss: -0.08894622325897217
Batch 38/64 loss: -0.12643569707870483
Batch 39/64 loss: -0.09740710258483887
Batch 40/64 loss: -0.0874587893486023
Batch 41/64 loss: -0.12147492170333862
Batch 42/64 loss: -0.09055984020233154
Batch 43/64 loss: -0.07870310544967651
Batch 44/64 loss: -0.0772504210472107
Batch 45/64 loss: -0.08358949422836304
Batch 46/64 loss: -0.06007140874862671
Batch 47/64 loss: -0.08590304851531982
Batch 48/64 loss: -0.09047329425811768
Batch 49/64 loss: -0.0684654712677002
Batch 50/64 loss: -0.07543671131134033
Batch 51/64 loss: -0.10724306106567383
Batch 52/64 loss: -0.10256761312484741
Batch 53/64 loss: -0.09160149097442627
Batch 54/64 loss: -0.07484972476959229
Batch 55/64 loss: -0.08720481395721436
Batch 56/64 loss: -0.0745692253112793
Batch 57/64 loss: -0.09535104036331177
Batch 58/64 loss: -0.06484335660934448
Batch 59/64 loss: -0.09307682514190674
Batch 60/64 loss: -0.09101498126983643
Batch 61/64 loss: -0.07506924867630005
Batch 62/64 loss: -0.04435795545578003
Batch 63/64 loss: -0.1093064546585083
Batch 64/64 loss: -0.0928199291229248
Epoch 17  Train loss: -0.09151260619070016  Val loss: -0.05538865217228526
Saving best model, epoch: 17
Epoch 18
-------------------------------
Batch 1/64 loss: -0.09922969341278076
Batch 2/64 loss: -0.08391261100769043
Batch 3/64 loss: -0.09264504909515381
Batch 4/64 loss: -0.09538531303405762
Batch 5/64 loss: -0.09378647804260254
Batch 6/64 loss: -0.10234463214874268
Batch 7/64 loss: -0.1089792251586914
Batch 8/64 loss: -0.09130501747131348
Batch 9/64 loss: -0.1052548885345459
Batch 10/64 loss: -0.10448646545410156
Batch 11/64 loss: -0.09393483400344849
Batch 12/64 loss: -0.09726607799530029
Batch 13/64 loss: -0.1086389422416687
Batch 14/64 loss: -0.09788084030151367
Batch 15/64 loss: -0.10110992193222046
Batch 16/64 loss: -0.08618652820587158
Batch 17/64 loss: -0.09520125389099121
Batch 18/64 loss: -0.1094316840171814
Batch 19/64 loss: -0.09624171257019043
Batch 20/64 loss: -0.12876981496810913
Batch 21/64 loss: -0.08877170085906982
Batch 22/64 loss: -0.06021881103515625
Batch 23/64 loss: -0.09675955772399902
Batch 24/64 loss: -0.10438483953475952
Batch 25/64 loss: -0.12909454107284546
Batch 26/64 loss: -0.07787299156188965
Batch 27/64 loss: -0.08016806840896606
Batch 28/64 loss: -0.10631442070007324
Batch 29/64 loss: -0.04785686731338501
Batch 30/64 loss: -0.11831861734390259
Batch 31/64 loss: -0.06815510988235474
Batch 32/64 loss: -0.04956471920013428
Batch 33/64 loss: -0.08994615077972412
Batch 34/64 loss: -0.08932632207870483
Batch 35/64 loss: -0.10495483875274658
Batch 36/64 loss: -0.11471593379974365
Batch 37/64 loss: -0.08109474182128906
Batch 38/64 loss: -0.07883834838867188
Batch 39/64 loss: -0.12192046642303467
Batch 40/64 loss: -0.10002404451370239
Batch 41/64 loss: -0.06466364860534668
Batch 42/64 loss: -0.08721691370010376
Batch 43/64 loss: -0.09358090162277222
Batch 44/64 loss: -0.07185876369476318
Batch 45/64 loss: -0.09987705945968628
Batch 46/64 loss: -0.08443719148635864
Batch 47/64 loss: -0.09027087688446045
Batch 48/64 loss: -0.09782737493515015
Batch 49/64 loss: -0.08795166015625
Batch 50/64 loss: -0.07246667146682739
Batch 51/64 loss: -0.0845831036567688
Batch 52/64 loss: -0.09787571430206299
Batch 53/64 loss: -0.11160069704055786
Batch 54/64 loss: -0.07331949472427368
Batch 55/64 loss: -0.10436826944351196
Batch 56/64 loss: -0.06791704893112183
Batch 57/64 loss: -0.10651195049285889
Batch 58/64 loss: -0.09586870670318604
Batch 59/64 loss: -0.1018572449684143
Batch 60/64 loss: -0.09363323450088501
Batch 61/64 loss: -0.09296095371246338
Batch 62/64 loss: -0.09890151023864746
Batch 63/64 loss: -0.11243569850921631
Batch 64/64 loss: -0.09876471757888794
Epoch 18  Train loss: -0.093589808660395  Val loss: -0.02830274244354353
Epoch 19
-------------------------------
Batch 1/64 loss: -0.10058891773223877
Batch 2/64 loss: -0.11367589235305786
Batch 3/64 loss: -0.13664251565933228
Batch 4/64 loss: -0.11615931987762451
Batch 5/64 loss: -0.1046636700630188
Batch 6/64 loss: -0.09212249517440796
Batch 7/64 loss: -0.11587178707122803
Batch 8/64 loss: -0.0761408805847168
Batch 9/64 loss: -0.09953612089157104
Batch 10/64 loss: -0.12001907825469971
Batch 11/64 loss: -0.10563504695892334
Batch 12/64 loss: -0.13445836305618286
Batch 13/64 loss: -0.09948110580444336
Batch 14/64 loss: -0.11636161804199219
Batch 15/64 loss: -0.09643363952636719
Batch 16/64 loss: -0.07361888885498047
Batch 17/64 loss: -0.0754886269569397
Batch 18/64 loss: -0.1064254641532898
Batch 19/64 loss: -0.10207772254943848
Batch 20/64 loss: -0.10333025455474854
Batch 21/64 loss: -0.08367466926574707
Batch 22/64 loss: -0.11236447095870972
Batch 23/64 loss: -0.11783146858215332
Batch 24/64 loss: -0.11046284437179565
Batch 25/64 loss: -0.09223484992980957
Batch 26/64 loss: -0.09416884183883667
Batch 27/64 loss: -0.10322308540344238
Batch 28/64 loss: -0.10717928409576416
Batch 29/64 loss: -0.10155743360519409
Batch 30/64 loss: -0.07263380289077759
Batch 31/64 loss: -0.08516967296600342
Batch 32/64 loss: -0.08418142795562744
Batch 33/64 loss: -0.09308189153671265
Batch 34/64 loss: -0.08095788955688477
Batch 35/64 loss: -0.08203768730163574
Batch 36/64 loss: -0.09756255149841309
Batch 37/64 loss: -0.09485924243927002
Batch 38/64 loss: -0.06114250421524048
Batch 39/64 loss: -0.10678279399871826
Batch 40/64 loss: -0.07433772087097168
Batch 41/64 loss: -0.08832621574401855
Batch 42/64 loss: -0.09427034854888916
Batch 43/64 loss: -0.07310378551483154
Batch 44/64 loss: -0.07869231700897217
Batch 45/64 loss: -0.08916223049163818
Batch 46/64 loss: -0.08602619171142578
Batch 47/64 loss: -0.07794797420501709
Batch 48/64 loss: -0.07194852828979492
Batch 49/64 loss: -0.10440796613693237
Batch 50/64 loss: -0.09030264616012573
Batch 51/64 loss: -0.08717852830886841
Batch 52/64 loss: -0.09705859422683716
Batch 53/64 loss: -0.0815805196762085
Batch 54/64 loss: -0.08185648918151855
Batch 55/64 loss: -0.10494720935821533
Batch 56/64 loss: -0.11504954099655151
Batch 57/64 loss: -0.10660922527313232
Batch 58/64 loss: -0.1213064193725586
Batch 59/64 loss: -0.14137232303619385
Batch 60/64 loss: -0.10751223564147949
Batch 61/64 loss: -0.10521465539932251
Batch 62/64 loss: -0.09530234336853027
Batch 63/64 loss: -0.09205460548400879
Batch 64/64 loss: -0.09120160341262817
Epoch 19  Train loss: -0.09731462960149728  Val loss: -0.059817747561792325
Saving best model, epoch: 19
Epoch 20
-------------------------------
Batch 1/64 loss: -0.09461873769760132
Batch 2/64 loss: -0.10089457035064697
Batch 3/64 loss: -0.09987890720367432
Batch 4/64 loss: -0.06109309196472168
Batch 5/64 loss: -0.13371646404266357
Batch 6/64 loss: -0.11416751146316528
Batch 7/64 loss: -0.10785776376724243
Batch 8/64 loss: -0.11947113275527954
Batch 9/64 loss: -0.11959713697433472
Batch 10/64 loss: -0.09818971157073975
Batch 11/64 loss: -0.12597453594207764
Batch 12/64 loss: -0.10624849796295166
Batch 13/64 loss: -0.06581771373748779
Batch 14/64 loss: -0.09214764833450317
Batch 15/64 loss: -0.10461187362670898
Batch 16/64 loss: -0.11442840099334717
Batch 17/64 loss: -0.10539758205413818
Batch 18/64 loss: -0.08257293701171875
Batch 19/64 loss: -0.09801685810089111
Batch 20/64 loss: -0.11101818084716797
Batch 21/64 loss: -0.11988621950149536
Batch 22/64 loss: -0.11625844240188599
Batch 23/64 loss: -0.11220657825469971
Batch 24/64 loss: -0.10663342475891113
Batch 25/64 loss: -0.09027218818664551
Batch 26/64 loss: -0.10912275314331055
Batch 27/64 loss: -0.11027777194976807
Batch 28/64 loss: -0.0771593451499939
Batch 29/64 loss: -0.08804279565811157
Batch 30/64 loss: -0.0796162486076355
Batch 31/64 loss: -0.09772723913192749
Batch 32/64 loss: -0.09420853853225708
Batch 33/64 loss: -0.10294985771179199
Batch 34/64 loss: -0.11746937036514282
Batch 35/64 loss: -0.09867089986801147
Batch 36/64 loss: -0.08102911710739136
Batch 37/64 loss: -0.09662115573883057
Batch 38/64 loss: -0.11032062768936157
Batch 39/64 loss: -0.08319813013076782
Batch 40/64 loss: -0.09576535224914551
Batch 41/64 loss: -0.1048479676246643
Batch 42/64 loss: -0.09968984127044678
Batch 43/64 loss: -0.06671738624572754
Batch 44/64 loss: -0.07891613245010376
Batch 45/64 loss: -0.1170966625213623
Batch 46/64 loss: -0.0982363224029541
Batch 47/64 loss: -0.09156453609466553
Batch 48/64 loss: -0.10760200023651123
Batch 49/64 loss: -0.12640464305877686
Batch 50/64 loss: -0.09464365243911743
Batch 51/64 loss: -0.1230994462966919
Batch 52/64 loss: -0.05948084592819214
Batch 53/64 loss: -0.11462199687957764
Batch 54/64 loss: -0.1118515133857727
Batch 55/64 loss: -0.10856157541275024
Batch 56/64 loss: -0.10614234209060669
Batch 57/64 loss: -0.1252284049987793
Batch 58/64 loss: -0.12997019290924072
Batch 59/64 loss: -0.12281912565231323
Batch 60/64 loss: -0.11365693807601929
Batch 61/64 loss: -0.10869789123535156
Batch 62/64 loss: -0.09985071420669556
Batch 63/64 loss: -0.1018097996711731
Batch 64/64 loss: -0.12186992168426514
Epoch 20  Train loss: -0.10268341466492298  Val loss: -0.07082841855144173
Saving best model, epoch: 20
Epoch 21
-------------------------------
Batch 1/64 loss: -0.08306074142456055
Batch 2/64 loss: -0.11598879098892212
Batch 3/64 loss: -0.11889475584030151
Batch 4/64 loss: -0.0956147313117981
Batch 5/64 loss: -0.10830378532409668
Batch 6/64 loss: -0.08186477422714233
Batch 7/64 loss: -0.09271883964538574
Batch 8/64 loss: -0.11725562810897827
Batch 9/64 loss: -0.06888699531555176
Batch 10/64 loss: -0.10264497995376587
Batch 11/64 loss: -0.10069715976715088
Batch 12/64 loss: -0.09985685348510742
Batch 13/64 loss: -0.13294613361358643
Batch 14/64 loss: -0.08767151832580566
Batch 15/64 loss: -0.09671461582183838
Batch 16/64 loss: -0.10053783655166626
Batch 17/64 loss: -0.09875565767288208
Batch 18/64 loss: -0.07620787620544434
Batch 19/64 loss: -0.11136919260025024
Batch 20/64 loss: -0.09365367889404297
Batch 21/64 loss: -0.09625166654586792
Batch 22/64 loss: -0.1210634708404541
Batch 23/64 loss: -0.1266307234764099
Batch 24/64 loss: -0.09526216983795166
Batch 25/64 loss: -0.12432920932769775
Batch 26/64 loss: -0.12012797594070435
Batch 27/64 loss: -0.11487019062042236
Batch 28/64 loss: -0.120841383934021
Batch 29/64 loss: -0.12551355361938477
Batch 30/64 loss: -0.0960463285446167
Batch 31/64 loss: -0.11498439311981201
Batch 32/64 loss: -0.11849451065063477
Batch 33/64 loss: -0.09133034944534302
Batch 34/64 loss: -0.10312104225158691
Batch 35/64 loss: -0.06370854377746582
Batch 36/64 loss: -0.09842163324356079
Batch 37/64 loss: -0.11746704578399658
Batch 38/64 loss: -0.0996248722076416
Batch 39/64 loss: -0.11604678630828857
Batch 40/64 loss: -0.08610391616821289
Batch 41/64 loss: -0.10619837045669556
Batch 42/64 loss: -0.10290825366973877
Batch 43/64 loss: -0.08403325080871582
Batch 44/64 loss: -0.1087881326675415
Batch 45/64 loss: -0.08943831920623779
Batch 46/64 loss: -0.10059726238250732
Batch 47/64 loss: -0.0766640305519104
Batch 48/64 loss: -0.12202626466751099
Batch 49/64 loss: -0.09393954277038574
Batch 50/64 loss: -0.07981544733047485
Batch 51/64 loss: -0.09970742464065552
Batch 52/64 loss: -0.09185677766799927
Batch 53/64 loss: -0.10059446096420288
Batch 54/64 loss: -0.09786105155944824
Batch 55/64 loss: -0.08369404077529907
Batch 56/64 loss: -0.10842418670654297
Batch 57/64 loss: -0.11846548318862915
Batch 58/64 loss: -0.10634785890579224
Batch 59/64 loss: -0.07618314027786255
Batch 60/64 loss: -0.0866442322731018
Batch 61/64 loss: -0.11236423254013062
Batch 62/64 loss: -0.08434337377548218
Batch 63/64 loss: -0.11090821027755737
Batch 64/64 loss: -0.12185221910476685
Epoch 21  Train loss: -0.10144434232337803  Val loss: -0.037039979012151766
Epoch 22
-------------------------------
Batch 1/64 loss: -0.11289018392562866
Batch 2/64 loss: -0.11307722330093384
Batch 3/64 loss: -0.09098917245864868
Batch 4/64 loss: -0.0934332013130188
Batch 5/64 loss: -0.0830422043800354
Batch 6/64 loss: -0.08636093139648438
Batch 7/64 loss: -0.10312920808792114
Batch 8/64 loss: -0.10550642013549805
Batch 9/64 loss: -0.07635772228240967
Batch 10/64 loss: -0.09842032194137573
Batch 11/64 loss: -0.10558319091796875
Batch 12/64 loss: -0.08897238969802856
Batch 13/64 loss: -0.09720826148986816
Batch 14/64 loss: -0.09947836399078369
Batch 15/64 loss: -0.1246328353881836
Batch 16/64 loss: -0.12322127819061279
Batch 17/64 loss: -0.0990067720413208
Batch 18/64 loss: -0.08095645904541016
Batch 19/64 loss: -0.08111143112182617
Batch 20/64 loss: -0.079487144947052
Batch 21/64 loss: -0.07464438676834106
Batch 22/64 loss: -0.07849347591400146
Batch 23/64 loss: -0.08572852611541748
Batch 24/64 loss: -0.11212813854217529
Batch 25/64 loss: -0.10097640752792358
Batch 26/64 loss: -0.08260738849639893
Batch 27/64 loss: -0.10999417304992676
Batch 28/64 loss: -0.13348424434661865
Batch 29/64 loss: -0.10597407817840576
Batch 30/64 loss: -0.09517717361450195
Batch 31/64 loss: -0.08881878852844238
Batch 32/64 loss: -0.10381990671157837
Batch 33/64 loss: -0.11573618650436401
Batch 34/64 loss: -0.09451138973236084
Batch 35/64 loss: -0.09185701608657837
Batch 36/64 loss: -0.11733222007751465
Batch 37/64 loss: -0.10566699504852295
Batch 38/64 loss: -0.12579256296157837
Batch 39/64 loss: -0.09567379951477051
Batch 40/64 loss: -0.1056179404258728
Batch 41/64 loss: -0.1256730556488037
Batch 42/64 loss: -0.11075341701507568
Batch 43/64 loss: -0.10444951057434082
Batch 44/64 loss: -0.09469461441040039
Batch 45/64 loss: -0.1137310266494751
Batch 46/64 loss: -0.1322176456451416
Batch 47/64 loss: -0.11142301559448242
Batch 48/64 loss: -0.06519800424575806
Batch 49/64 loss: -0.10927706956863403
Batch 50/64 loss: -0.11337894201278687
Batch 51/64 loss: -0.09293609857559204
Batch 52/64 loss: -0.09723985195159912
Batch 53/64 loss: -0.10857760906219482
Batch 54/64 loss: -0.09959208965301514
Batch 55/64 loss: -0.13002163171768188
Batch 56/64 loss: -0.1195327639579773
Batch 57/64 loss: -0.08477884531021118
Batch 58/64 loss: -0.13000428676605225
Batch 59/64 loss: -0.12870573997497559
Batch 60/64 loss: -0.09168577194213867
Batch 61/64 loss: -0.12399107217788696
Batch 62/64 loss: -0.08240079879760742
Batch 63/64 loss: -0.1147950291633606
Batch 64/64 loss: -0.06497383117675781
Epoch 22  Train loss: -0.10197157298817354  Val loss: -0.06381484384798922
Epoch 23
-------------------------------
Batch 1/64 loss: -0.08395612239837646
Batch 2/64 loss: -0.07406926155090332
Batch 3/64 loss: -0.0942186713218689
Batch 4/64 loss: -0.11511838436126709
Batch 5/64 loss: -0.08251816034317017
Batch 6/64 loss: -0.11867892742156982
Batch 7/64 loss: -0.11945557594299316
Batch 8/64 loss: -0.11456078290939331
Batch 9/64 loss: -0.11150515079498291
Batch 10/64 loss: -0.10520684719085693
Batch 11/64 loss: -0.11454635858535767
Batch 12/64 loss: -0.09899556636810303
Batch 13/64 loss: -0.1093629002571106
Batch 14/64 loss: -0.09604823589324951
Batch 15/64 loss: -0.09812003374099731
Batch 16/64 loss: -0.1232839822769165
Batch 17/64 loss: -0.11460226774215698
Batch 18/64 loss: -0.08506464958190918
Batch 19/64 loss: -0.10423517227172852
Batch 20/64 loss: -0.10123920440673828
Batch 21/64 loss: -0.1014256477355957
Batch 22/64 loss: -0.09966129064559937
Batch 23/64 loss: -0.11447328329086304
Batch 24/64 loss: -0.12186729907989502
Batch 25/64 loss: -0.14455610513687134
Batch 26/64 loss: -0.10317754745483398
Batch 27/64 loss: -0.11149603128433228
Batch 28/64 loss: -0.11138856410980225
Batch 29/64 loss: -0.09715777635574341
Batch 30/64 loss: -0.10014241933822632
Batch 31/64 loss: -0.1243029236793518
Batch 32/64 loss: -0.10654252767562866
Batch 33/64 loss: -0.095986008644104
Batch 34/64 loss: -0.09137964248657227
Batch 35/64 loss: -0.13776135444641113
Batch 36/64 loss: -0.12625539302825928
Batch 37/64 loss: -0.11544132232666016
Batch 38/64 loss: -0.1215827465057373
Batch 39/64 loss: -0.10939604043960571
Batch 40/64 loss: -0.12061703205108643
Batch 41/64 loss: -0.1012340784072876
Batch 42/64 loss: -0.10352236032485962
Batch 43/64 loss: -0.09330582618713379
Batch 44/64 loss: -0.12184590101242065
Batch 45/64 loss: -0.130587637424469
Batch 46/64 loss: -0.10681742429733276
Batch 47/64 loss: -0.10657680034637451
Batch 48/64 loss: -0.11911648511886597
Batch 49/64 loss: -0.13011348247528076
Batch 50/64 loss: -0.11202293634414673
Batch 51/64 loss: -0.07975965738296509
Batch 52/64 loss: -0.10220199823379517
Batch 53/64 loss: -0.10149717330932617
Batch 54/64 loss: -0.08958470821380615
Batch 55/64 loss: -0.10391151905059814
Batch 56/64 loss: -0.07506221532821655
Batch 57/64 loss: -0.09894907474517822
Batch 58/64 loss: -0.09588402509689331
Batch 59/64 loss: -0.11915916204452515
Batch 60/64 loss: -0.10567593574523926
Batch 61/64 loss: -0.1004830002784729
Batch 62/64 loss: -0.12496191263198853
Batch 63/64 loss: -0.10553395748138428
Batch 64/64 loss: -0.12638914585113525
Epoch 23  Train loss: -0.10732543281480378  Val loss: -0.06559999783833821
Epoch 24
-------------------------------
Batch 1/64 loss: -0.11568522453308105
Batch 2/64 loss: -0.11492574214935303
Batch 3/64 loss: -0.11366438865661621
Batch 4/64 loss: -0.11477464437484741
Batch 5/64 loss: -0.092212975025177
Batch 6/64 loss: -0.09922868013381958
Batch 7/64 loss: -0.12414288520812988
Batch 8/64 loss: -0.08926212787628174
Batch 9/64 loss: -0.1005786657333374
Batch 10/64 loss: -0.1156584620475769
Batch 11/64 loss: -0.09537291526794434
Batch 12/64 loss: -0.11413764953613281
Batch 13/64 loss: -0.11225312948226929
Batch 14/64 loss: -0.09125417470932007
Batch 15/64 loss: -0.11549091339111328
Batch 16/64 loss: -0.11680585145950317
Batch 17/64 loss: -0.12600314617156982
Batch 18/64 loss: -0.11292469501495361
Batch 19/64 loss: -0.09243452548980713
Batch 20/64 loss: -0.11497753858566284
Batch 21/64 loss: -0.10570782423019409
Batch 22/64 loss: -0.10331147909164429
Batch 23/64 loss: -0.11529922485351562
Batch 24/64 loss: -0.10433971881866455
Batch 25/64 loss: -0.12454032897949219
Batch 26/64 loss: -0.10321152210235596
Batch 27/64 loss: -0.11346709728240967
Batch 28/64 loss: -0.11232876777648926
Batch 29/64 loss: -0.10205620527267456
Batch 30/64 loss: -0.13128995895385742
Batch 31/64 loss: -0.12021666765213013
Batch 32/64 loss: -0.12534606456756592
Batch 33/64 loss: -0.12290829420089722
Batch 34/64 loss: -0.10038864612579346
Batch 35/64 loss: -0.12940925359725952
Batch 36/64 loss: -0.10761427879333496
Batch 37/64 loss: -0.11358684301376343
Batch 38/64 loss: -0.1287158727645874
Batch 39/64 loss: -0.11413824558258057
Batch 40/64 loss: -0.1284772753715515
Batch 41/64 loss: -0.1148759126663208
Batch 42/64 loss: -0.11229050159454346
Batch 43/64 loss: -0.1250779628753662
Batch 44/64 loss: -0.11043977737426758
Batch 45/64 loss: -0.09438270330429077
Batch 46/64 loss: -0.1128200888633728
Batch 47/64 loss: -0.1023527979850769
Batch 48/64 loss: -0.12085217237472534
Batch 49/64 loss: -0.11156302690505981
Batch 50/64 loss: -0.11095792055130005
Batch 51/64 loss: -0.13071131706237793
Batch 52/64 loss: -0.11742424964904785
Batch 53/64 loss: -0.13142669200897217
Batch 54/64 loss: -0.11854255199432373
Batch 55/64 loss: -0.10954678058624268
Batch 56/64 loss: -0.13412994146347046
Batch 57/64 loss: -0.143021821975708
Batch 58/64 loss: -0.12838530540466309
Batch 59/64 loss: -0.12023210525512695
Batch 60/64 loss: -0.09077370166778564
Batch 61/64 loss: -0.1458095908164978
Batch 62/64 loss: -0.11179119348526001
Batch 63/64 loss: -0.11847829818725586
Batch 64/64 loss: -0.0855298638343811
Epoch 24  Train loss: -0.11385373671849569  Val loss: -0.08532890400935694
Saving best model, epoch: 24
Epoch 25
-------------------------------
Batch 1/64 loss: -0.10691535472869873
Batch 2/64 loss: -0.09641140699386597
Batch 3/64 loss: -0.11928468942642212
Batch 4/64 loss: -0.12998932600021362
Batch 5/64 loss: -0.11605691909790039
Batch 6/64 loss: -0.12132591009140015
Batch 7/64 loss: -0.09424006938934326
Batch 8/64 loss: -0.12034726142883301
Batch 9/64 loss: -0.11545741558074951
Batch 10/64 loss: -0.09776484966278076
Batch 11/64 loss: -0.12067866325378418
Batch 12/64 loss: -0.10494363307952881
Batch 13/64 loss: -0.13537102937698364
Batch 14/64 loss: -0.1319013237953186
Batch 15/64 loss: -0.09991180896759033
Batch 16/64 loss: -0.1306309700012207
Batch 17/64 loss: -0.1295587420463562
Batch 18/64 loss: -0.11726534366607666
Batch 19/64 loss: -0.1110612154006958
Batch 20/64 loss: -0.11991983652114868
Batch 21/64 loss: -0.14029204845428467
Batch 22/64 loss: -0.14897751808166504
Batch 23/64 loss: -0.10743367671966553
Batch 24/64 loss: -0.12261080741882324
Batch 25/64 loss: -0.09375900030136108
Batch 26/64 loss: -0.09324091672897339
Batch 27/64 loss: -0.09387803077697754
Batch 28/64 loss: -0.09641432762145996
Batch 29/64 loss: -0.11597496271133423
Batch 30/64 loss: -0.09742707014083862
Batch 31/64 loss: -0.09816920757293701
Batch 32/64 loss: -0.11962109804153442
Batch 33/64 loss: -0.11680340766906738
Batch 34/64 loss: -0.10989511013031006
Batch 35/64 loss: -0.10746312141418457
Batch 36/64 loss: -0.12629061937332153
Batch 37/64 loss: -0.12312573194503784
Batch 38/64 loss: -0.07868421077728271
Batch 39/64 loss: -0.1225893497467041
Batch 40/64 loss: -0.1126030683517456
Batch 41/64 loss: -0.11551356315612793
Batch 42/64 loss: -0.10148614645004272
Batch 43/64 loss: -0.1404091715812683
Batch 44/64 loss: -0.1278085708618164
Batch 45/64 loss: -0.11108118295669556
Batch 46/64 loss: -0.10709738731384277
Batch 47/64 loss: -0.12206977605819702
Batch 48/64 loss: -0.10926693677902222
Batch 49/64 loss: -0.13052713871002197
Batch 50/64 loss: -0.12180709838867188
Batch 51/64 loss: -0.12398350238800049
Batch 52/64 loss: -0.11332899332046509
Batch 53/64 loss: -0.10802513360977173
Batch 54/64 loss: -0.08185762166976929
Batch 55/64 loss: -0.11364150047302246
Batch 56/64 loss: -0.1312551498413086
Batch 57/64 loss: -0.11241865158081055
Batch 58/64 loss: -0.057827532291412354
Batch 59/64 loss: -0.1263168454170227
Batch 60/64 loss: -0.16331088542938232
Batch 61/64 loss: -0.11334836483001709
Batch 62/64 loss: -0.09537345170974731
Batch 63/64 loss: -0.11889398097991943
Batch 64/64 loss: -0.06875759363174438
Epoch 25  Train loss: -0.11360793464324054  Val loss: -0.07623447257628556
Epoch 26
-------------------------------
Batch 1/64 loss: -0.09554791450500488
Batch 2/64 loss: -0.12036687135696411
Batch 3/64 loss: -0.11575615406036377
Batch 4/64 loss: -0.11614304780960083
Batch 5/64 loss: -0.10162210464477539
Batch 6/64 loss: -0.13151663541793823
Batch 7/64 loss: -0.1256653070449829
Batch 8/64 loss: -0.10009187459945679
Batch 9/64 loss: -0.10868537425994873
Batch 10/64 loss: -0.09019708633422852
Batch 11/64 loss: -0.08719116449356079
Batch 12/64 loss: -0.1197967529296875
Batch 13/64 loss: -0.11618959903717041
Batch 14/64 loss: -0.128204345703125
Batch 15/64 loss: -0.10638809204101562
Batch 16/64 loss: -0.13239407539367676
Batch 17/64 loss: -0.10968410968780518
Batch 18/64 loss: -0.1194603443145752
Batch 19/64 loss: -0.12582993507385254
Batch 20/64 loss: -0.11268121004104614
Batch 21/64 loss: -0.09645754098892212
Batch 22/64 loss: -0.13218754529953003
Batch 23/64 loss: -0.11452978849411011
Batch 24/64 loss: -0.14792460203170776
Batch 25/64 loss: -0.11922192573547363
Batch 26/64 loss: -0.09827685356140137
Batch 27/64 loss: -0.14571738243103027
Batch 28/64 loss: -0.09285050630569458
Batch 29/64 loss: -0.1341099739074707
Batch 30/64 loss: -0.1174466609954834
Batch 31/64 loss: -0.0845186710357666
Batch 32/64 loss: -0.08164548873901367
Batch 33/64 loss: -0.09897112846374512
Batch 34/64 loss: -0.1161203384399414
Batch 35/64 loss: -0.1519884467124939
Batch 36/64 loss: -0.1076362133026123
Batch 37/64 loss: -0.06959062814712524
Batch 38/64 loss: -0.12005984783172607
Batch 39/64 loss: -0.1305241584777832
Batch 40/64 loss: -0.10367405414581299
Batch 41/64 loss: -0.14823389053344727
Batch 42/64 loss: -0.11094915866851807
Batch 43/64 loss: -0.1341966986656189
Batch 44/64 loss: -0.13822579383850098
Batch 45/64 loss: -0.11483091115951538
Batch 46/64 loss: -0.12741893529891968
Batch 47/64 loss: -0.13784289360046387
Batch 48/64 loss: -0.11790913343429565
Batch 49/64 loss: -0.12705272436141968
Batch 50/64 loss: -0.09563708305358887
Batch 51/64 loss: -0.11148196458816528
Batch 52/64 loss: -0.10700327157974243
Batch 53/64 loss: -0.14312517642974854
Batch 54/64 loss: -0.11379921436309814
Batch 55/64 loss: -0.12907850742340088
Batch 56/64 loss: -0.1194337010383606
Batch 57/64 loss: -0.12856101989746094
Batch 58/64 loss: -0.13140517473220825
Batch 59/64 loss: -0.11286681890487671
Batch 60/64 loss: -0.11539298295974731
Batch 61/64 loss: -0.09903466701507568
Batch 62/64 loss: -0.13129782676696777
Batch 63/64 loss: -0.13348394632339478
Batch 64/64 loss: -0.14400029182434082
Epoch 26  Train loss: -0.11706863478118298  Val loss: -0.07651693796374134
Epoch 27
-------------------------------
Batch 1/64 loss: -0.12431639432907104
Batch 2/64 loss: -0.11835455894470215
Batch 3/64 loss: -0.12459456920623779
Batch 4/64 loss: -0.09103250503540039
Batch 5/64 loss: -0.10432946681976318
Batch 6/64 loss: -0.10443812608718872
Batch 7/64 loss: -0.12599027156829834
Batch 8/64 loss: -0.12992727756500244
Batch 9/64 loss: -0.13165342807769775
Batch 10/64 loss: -0.13852143287658691
Batch 11/64 loss: -0.1320173740386963
Batch 12/64 loss: -0.13903820514678955
Batch 13/64 loss: -0.10951822996139526
Batch 14/64 loss: -0.12573498487472534
Batch 15/64 loss: -0.10858505964279175
Batch 16/64 loss: -0.1132810115814209
Batch 17/64 loss: -0.13348865509033203
Batch 18/64 loss: -0.12399113178253174
Batch 19/64 loss: -0.10382497310638428
Batch 20/64 loss: -0.13436859846115112
Batch 21/64 loss: -0.09514808654785156
Batch 22/64 loss: -0.1488335132598877
Batch 23/64 loss: -0.15128165483474731
Batch 24/64 loss: -0.12413632869720459
Batch 25/64 loss: -0.114249587059021
Batch 26/64 loss: -0.14872771501541138
Batch 27/64 loss: -0.1198354959487915
Batch 28/64 loss: -0.11607110500335693
Batch 29/64 loss: -0.10203349590301514
Batch 30/64 loss: -0.06876134872436523
Batch 31/64 loss: -0.1375185251235962
Batch 32/64 loss: -0.13472914695739746
Batch 33/64 loss: -0.12596774101257324
Batch 34/64 loss: -0.1305493712425232
Batch 35/64 loss: -0.10831904411315918
Batch 36/64 loss: -0.1178019642829895
Batch 37/64 loss: -0.11939972639083862
Batch 38/64 loss: -0.15298902988433838
Batch 39/64 loss: -0.14690721035003662
Batch 40/64 loss: -0.10980677604675293
Batch 41/64 loss: -0.10641998052597046
Batch 42/64 loss: -0.10166221857070923
Batch 43/64 loss: -0.12312126159667969
Batch 44/64 loss: -0.13517916202545166
Batch 45/64 loss: -0.09666240215301514
Batch 46/64 loss: -0.11456459760665894
Batch 47/64 loss: -0.11515367031097412
Batch 48/64 loss: -0.14844352006912231
Batch 49/64 loss: -0.11541628837585449
Batch 50/64 loss: -0.11738860607147217
Batch 51/64 loss: -0.12712275981903076
Batch 52/64 loss: -0.11796504259109497
Batch 53/64 loss: -0.14532196521759033
Batch 54/64 loss: -0.1336551308631897
Batch 55/64 loss: -0.14272809028625488
Batch 56/64 loss: -0.1112709641456604
Batch 57/64 loss: -0.14773797988891602
Batch 58/64 loss: -0.1062246561050415
Batch 59/64 loss: -0.13047468662261963
Batch 60/64 loss: -0.10808026790618896
Batch 61/64 loss: -0.13690584897994995
Batch 62/64 loss: -0.11368608474731445
Batch 63/64 loss: -0.13518065214157104
Batch 64/64 loss: -0.1437944769859314
Epoch 27  Train loss: -0.12279662454829496  Val loss: -0.09059143865231387
Saving best model, epoch: 27
Epoch 28
-------------------------------
Batch 1/64 loss: -0.13518434762954712
Batch 2/64 loss: -0.1282031536102295
Batch 3/64 loss: -0.14005637168884277
Batch 4/64 loss: -0.10907554626464844
Batch 5/64 loss: -0.129194438457489
Batch 6/64 loss: -0.13329792022705078
Batch 7/64 loss: -0.13995736837387085
Batch 8/64 loss: -0.1221964955329895
Batch 9/64 loss: -0.16294419765472412
Batch 10/64 loss: -0.13182806968688965
Batch 11/64 loss: -0.10511481761932373
Batch 12/64 loss: -0.12574642896652222
Batch 13/64 loss: -0.10440325736999512
Batch 14/64 loss: -0.11926466226577759
Batch 15/64 loss: -0.14100933074951172
Batch 16/64 loss: -0.11822891235351562
Batch 17/64 loss: -0.1387956738471985
Batch 18/64 loss: -0.12261813879013062
Batch 19/64 loss: -0.1295921802520752
Batch 20/64 loss: -0.12632465362548828
Batch 21/64 loss: -0.11853998899459839
Batch 22/64 loss: -0.1139596700668335
Batch 23/64 loss: -0.11342215538024902
Batch 24/64 loss: -0.1430770754814148
Batch 25/64 loss: -0.10950243473052979
Batch 26/64 loss: -0.13721251487731934
Batch 27/64 loss: -0.11630767583847046
Batch 28/64 loss: -0.13890111446380615
Batch 29/64 loss: -0.1499587893486023
Batch 30/64 loss: -0.08325999975204468
Batch 31/64 loss: -0.13554418087005615
Batch 32/64 loss: -0.13004052639007568
Batch 33/64 loss: -0.10267674922943115
Batch 34/64 loss: -0.13818740844726562
Batch 35/64 loss: -0.15962785482406616
Batch 36/64 loss: -0.12682557106018066
Batch 37/64 loss: -0.13055098056793213
Batch 38/64 loss: -0.10963410139083862
Batch 39/64 loss: -0.13799703121185303
Batch 40/64 loss: -0.12421107292175293
Batch 41/64 loss: -0.13304662704467773
Batch 42/64 loss: -0.1186438798904419
Batch 43/64 loss: -0.14759033918380737
Batch 44/64 loss: -0.11960339546203613
Batch 45/64 loss: -0.1319853663444519
Batch 46/64 loss: -0.1564502716064453
Batch 47/64 loss: -0.12513577938079834
Batch 48/64 loss: -0.12163615226745605
Batch 49/64 loss: -0.10929369926452637
Batch 50/64 loss: -0.13442128896713257
Batch 51/64 loss: -0.13377690315246582
Batch 52/64 loss: -0.1091800332069397
Batch 53/64 loss: -0.12119138240814209
Batch 54/64 loss: -0.1316625475883484
Batch 55/64 loss: -0.11149007081985474
Batch 56/64 loss: -0.1246105432510376
Batch 57/64 loss: -0.12499403953552246
Batch 58/64 loss: -0.09536099433898926
Batch 59/64 loss: -0.11899787187576294
Batch 60/64 loss: -0.11196720600128174
Batch 61/64 loss: -0.14091843366622925
Batch 62/64 loss: -0.13367372751235962
Batch 63/64 loss: -0.14408749341964722
Batch 64/64 loss: -0.13523811101913452
Epoch 28  Train loss: -0.12680187435711132  Val loss: -0.08419803737365093
Epoch 29
-------------------------------
Batch 1/64 loss: -0.09089314937591553
Batch 2/64 loss: -0.13557171821594238
Batch 3/64 loss: -0.15543997287750244
Batch 4/64 loss: -0.12228429317474365
Batch 5/64 loss: -0.14542371034622192
Batch 6/64 loss: -0.1430262327194214
Batch 7/64 loss: -0.1279183030128479
Batch 8/64 loss: -0.1638038158416748
Batch 9/64 loss: -0.1286180019378662
Batch 10/64 loss: -0.11582928895950317
Batch 11/64 loss: -0.13713455200195312
Batch 12/64 loss: -0.13288700580596924
Batch 13/64 loss: -0.124839186668396
Batch 14/64 loss: -0.14303874969482422
Batch 15/64 loss: -0.12119019031524658
Batch 16/64 loss: -0.10318797826766968
Batch 17/64 loss: -0.10051453113555908
Batch 18/64 loss: -0.12187188863754272
Batch 19/64 loss: -0.11909681558609009
Batch 20/64 loss: -0.12014508247375488
Batch 21/64 loss: -0.12783277034759521
Batch 22/64 loss: -0.09090429544448853
Batch 23/64 loss: -0.11805850267410278
Batch 24/64 loss: -0.11934137344360352
Batch 25/64 loss: -0.15280473232269287
Batch 26/64 loss: -0.13179481029510498
Batch 27/64 loss: -0.13230788707733154
Batch 28/64 loss: -0.12351483106613159
Batch 29/64 loss: -0.16828948259353638
Batch 30/64 loss: -0.11205953359603882
Batch 31/64 loss: -0.13029813766479492
Batch 32/64 loss: -0.12394183874130249
Batch 33/64 loss: -0.11596769094467163
Batch 34/64 loss: -0.1062002182006836
Batch 35/64 loss: -0.1235802173614502
Batch 36/64 loss: -0.11981213092803955
Batch 37/64 loss: -0.10946959257125854
Batch 38/64 loss: -0.11845529079437256
Batch 39/64 loss: -0.12732642889022827
Batch 40/64 loss: -0.1074407696723938
Batch 41/64 loss: -0.12407368421554565
Batch 42/64 loss: -0.14761954545974731
Batch 43/64 loss: -0.11208480596542358
Batch 44/64 loss: -0.11721605062484741
Batch 45/64 loss: -0.11808401346206665
Batch 46/64 loss: -0.12099391222000122
Batch 47/64 loss: -0.10301560163497925
Batch 48/64 loss: -0.11187338829040527
Batch 49/64 loss: -0.13507330417633057
Batch 50/64 loss: -0.11504387855529785
Batch 51/64 loss: -0.14120173454284668
Batch 52/64 loss: -0.10327160358428955
Batch 53/64 loss: -0.1359860897064209
Batch 54/64 loss: -0.12297457456588745
Batch 55/64 loss: -0.11987525224685669
Batch 56/64 loss: -0.1202542781829834
Batch 57/64 loss: -0.15322893857955933
Batch 58/64 loss: -0.12174081802368164
Batch 59/64 loss: -0.12917351722717285
Batch 60/64 loss: -0.10136401653289795
Batch 61/64 loss: -0.12506836652755737
Batch 62/64 loss: -0.13357174396514893
Batch 63/64 loss: -0.10639697313308716
Batch 64/64 loss: -0.14421707391738892
Epoch 29  Train loss: -0.12454061017316931  Val loss: -0.08028168080188974
Epoch 30
-------------------------------
Batch 1/64 loss: -0.11696082353591919
Batch 2/64 loss: -0.14876306056976318
Batch 3/64 loss: -0.1398114562034607
Batch 4/64 loss: -0.13808190822601318
Batch 5/64 loss: -0.10967010259628296
Batch 6/64 loss: -0.11793923377990723
Batch 7/64 loss: -0.1594541072845459
Batch 8/64 loss: -0.13317859172821045
Batch 9/64 loss: -0.12270563840866089
Batch 10/64 loss: -0.14149624109268188
Batch 11/64 loss: -0.14972913265228271
Batch 12/64 loss: -0.1634436845779419
Batch 13/64 loss: -0.10657775402069092
Batch 14/64 loss: -0.1381281614303589
Batch 15/64 loss: -0.12923282384872437
Batch 16/64 loss: -0.11142623424530029
Batch 17/64 loss: -0.14185965061187744
Batch 18/64 loss: -0.12103313207626343
Batch 19/64 loss: -0.125432550907135
Batch 20/64 loss: -0.13599270582199097
Batch 21/64 loss: -0.1333174705505371
Batch 22/64 loss: -0.11008262634277344
Batch 23/64 loss: -0.12050575017929077
Batch 24/64 loss: -0.13837623596191406
Batch 25/64 loss: -0.12105119228363037
Batch 26/64 loss: -0.1311858892440796
Batch 27/64 loss: -0.14003539085388184
Batch 28/64 loss: -0.11680662631988525
Batch 29/64 loss: -0.12755465507507324
Batch 30/64 loss: -0.13733285665512085
Batch 31/64 loss: -0.1261380910873413
Batch 32/64 loss: -0.14665907621383667
Batch 33/64 loss: -0.13098102807998657
Batch 34/64 loss: -0.15089702606201172
Batch 35/64 loss: -0.08561575412750244
Batch 36/64 loss: -0.12379932403564453
Batch 37/64 loss: -0.0883895754814148
Batch 38/64 loss: -0.14058685302734375
Batch 39/64 loss: -0.107513427734375
Batch 40/64 loss: -0.15235745906829834
Batch 41/64 loss: -0.12727481126785278
Batch 42/64 loss: -0.14016765356063843
Batch 43/64 loss: -0.1434863805770874
Batch 44/64 loss: -0.11967974901199341
Batch 45/64 loss: -0.14842456579208374
Batch 46/64 loss: -0.11429411172866821
Batch 47/64 loss: -0.1068418025970459
Batch 48/64 loss: -0.09046179056167603
Batch 49/64 loss: -0.12849634885787964
Batch 50/64 loss: -0.09976625442504883
Batch 51/64 loss: -0.1493646502494812
Batch 52/64 loss: -0.11769700050354004
Batch 53/64 loss: -0.12652552127838135
Batch 54/64 loss: -0.13807201385498047
Batch 55/64 loss: -0.1263042688369751
Batch 56/64 loss: -0.13800835609436035
Batch 57/64 loss: -0.12581437826156616
Batch 58/64 loss: -0.14600014686584473
Batch 59/64 loss: -0.15945571660995483
Batch 60/64 loss: -0.11406552791595459
Batch 61/64 loss: -0.14145493507385254
Batch 62/64 loss: -0.1276111602783203
Batch 63/64 loss: -0.12392503023147583
Batch 64/64 loss: -0.1220363974571228
Epoch 30  Train loss: -0.1290168278357562  Val loss: -0.10567554124851816
Saving best model, epoch: 30
Epoch 31
-------------------------------
Batch 1/64 loss: -0.11148977279663086
Batch 2/64 loss: -0.12271082401275635
Batch 3/64 loss: -0.12858307361602783
Batch 4/64 loss: -0.1327277421951294
Batch 5/64 loss: -0.128781259059906
Batch 6/64 loss: -0.13746601343154907
Batch 7/64 loss: -0.1302417516708374
Batch 8/64 loss: -0.14516687393188477
Batch 9/64 loss: -0.1693912148475647
Batch 10/64 loss: -0.13906413316726685
Batch 11/64 loss: -0.12211203575134277
Batch 12/64 loss: -0.1658867597579956
Batch 13/64 loss: -0.13452857732772827
Batch 14/64 loss: -0.14588987827301025
Batch 15/64 loss: -0.1428436040878296
Batch 16/64 loss: -0.13924109935760498
Batch 17/64 loss: -0.12994635105133057
Batch 18/64 loss: -0.11022871732711792
Batch 19/64 loss: -0.11606109142303467
Batch 20/64 loss: -0.10735833644866943
Batch 21/64 loss: -0.15027838945388794
Batch 22/64 loss: -0.11222916841506958
Batch 23/64 loss: -0.13409793376922607
Batch 24/64 loss: -0.13656800985336304
Batch 25/64 loss: -0.12018537521362305
Batch 26/64 loss: -0.1114722490310669
Batch 27/64 loss: -0.14661890268325806
Batch 28/64 loss: -0.13625258207321167
Batch 29/64 loss: -0.14100778102874756
Batch 30/64 loss: -0.15516400337219238
Batch 31/64 loss: -0.13331514596939087
Batch 32/64 loss: -0.13953787088394165
Batch 33/64 loss: -0.11638844013214111
Batch 34/64 loss: -0.11888706684112549
Batch 35/64 loss: -0.1054641604423523
Batch 36/64 loss: -0.15285450220108032
Batch 37/64 loss: -0.11577355861663818
Batch 38/64 loss: -0.13618028163909912
Batch 39/64 loss: -0.15189224481582642
Batch 40/64 loss: -0.11641740798950195
Batch 41/64 loss: -0.12261223793029785
Batch 42/64 loss: -0.12851804494857788
Batch 43/64 loss: -0.14704978466033936
Batch 44/64 loss: -0.12694960832595825
Batch 45/64 loss: -0.1201019287109375
Batch 46/64 loss: -0.15931713581085205
Batch 47/64 loss: -0.1272890567779541
Batch 48/64 loss: -0.1292799711227417
Batch 49/64 loss: -0.11524468660354614
Batch 50/64 loss: -0.11859959363937378
Batch 51/64 loss: -0.1635686159133911
Batch 52/64 loss: -0.11392992734909058
Batch 53/64 loss: -0.15312498807907104
Batch 54/64 loss: -0.12501680850982666
Batch 55/64 loss: -0.14962291717529297
Batch 56/64 loss: -0.1250678300857544
Batch 57/64 loss: -0.1311771273612976
Batch 58/64 loss: -0.14182782173156738
Batch 59/64 loss: -0.11547452211380005
Batch 60/64 loss: -0.14139634370803833
Batch 61/64 loss: -0.12648868560791016
Batch 62/64 loss: -0.1207207441329956
Batch 63/64 loss: -0.12528318166732788
Batch 64/64 loss: -0.13819855451583862
Epoch 31  Train loss: -0.13210375940098482  Val loss: -0.09655310915098157
Epoch 32
-------------------------------
Batch 1/64 loss: -0.1106078028678894
Batch 2/64 loss: -0.12971365451812744
Batch 3/64 loss: -0.13131719827651978
Batch 4/64 loss: -0.11062479019165039
Batch 5/64 loss: -0.15419983863830566
Batch 6/64 loss: -0.11893415451049805
Batch 7/64 loss: -0.1434834599494934
Batch 8/64 loss: -0.14375340938568115
Batch 9/64 loss: -0.12987273931503296
Batch 10/64 loss: -0.1224212646484375
Batch 11/64 loss: -0.12661802768707275
Batch 12/64 loss: -0.14646363258361816
Batch 13/64 loss: -0.14723044633865356
Batch 14/64 loss: -0.14142680168151855
Batch 15/64 loss: -0.16166961193084717
Batch 16/64 loss: -0.12762701511383057
Batch 17/64 loss: -0.16007381677627563
Batch 18/64 loss: -0.11204278469085693
Batch 19/64 loss: -0.1149168610572815
Batch 20/64 loss: -0.16330301761627197
Batch 21/64 loss: -0.1520809531211853
Batch 22/64 loss: -0.12913250923156738
Batch 23/64 loss: -0.12887609004974365
Batch 24/64 loss: -0.12715280055999756
Batch 25/64 loss: -0.10855495929718018
Batch 26/64 loss: -0.14739251136779785
Batch 27/64 loss: -0.1496497392654419
Batch 28/64 loss: -0.12000894546508789
Batch 29/64 loss: -0.14424967765808105
Batch 30/64 loss: -0.13006776571273804
Batch 31/64 loss: -0.1311800479888916
Batch 32/64 loss: -0.14870774745941162
Batch 33/64 loss: -0.12340444326400757
Batch 34/64 loss: -0.13580387830734253
Batch 35/64 loss: -0.1454368233680725
Batch 36/64 loss: -0.14986026287078857
Batch 37/64 loss: -0.14801561832427979
Batch 38/64 loss: -0.14537209272384644
Batch 39/64 loss: -0.13995760679244995
Batch 40/64 loss: -0.14932072162628174
Batch 41/64 loss: -0.12249618768692017
Batch 42/64 loss: -0.1478825807571411
Batch 43/64 loss: -0.12935692071914673
Batch 44/64 loss: -0.11712807416915894
Batch 45/64 loss: -0.12545466423034668
Batch 46/64 loss: -0.10673093795776367
Batch 47/64 loss: -0.13790935277938843
Batch 48/64 loss: -0.13483387231826782
Batch 49/64 loss: -0.1460047960281372
Batch 50/64 loss: -0.14019089937210083
Batch 51/64 loss: -0.11579710245132446
Batch 52/64 loss: -0.09269064664840698
Batch 53/64 loss: -0.13214820623397827
Batch 54/64 loss: -0.11735481023788452
Batch 55/64 loss: -0.13418829441070557
Batch 56/64 loss: -0.11980772018432617
Batch 57/64 loss: -0.13975751399993896
Batch 58/64 loss: -0.13344520330429077
Batch 59/64 loss: -0.13316017389297485
Batch 60/64 loss: -0.15916049480438232
Batch 61/64 loss: -0.09495693445205688
Batch 62/64 loss: -0.11665809154510498
Batch 63/64 loss: -0.1193496584892273
Batch 64/64 loss: -0.15474402904510498
Epoch 32  Train loss: -0.1330673989127664  Val loss: -0.09206925225012082
Epoch 33
-------------------------------
Batch 1/64 loss: -0.11006689071655273
Batch 2/64 loss: -0.14860659837722778
Batch 3/64 loss: -0.10553538799285889
Batch 4/64 loss: -0.11107546091079712
Batch 5/64 loss: -0.15157753229141235
Batch 6/64 loss: -0.15062928199768066
Batch 7/64 loss: -0.12742388248443604
Batch 8/64 loss: -0.13023430109024048
Batch 9/64 loss: -0.11951088905334473
Batch 10/64 loss: -0.1107097864151001
Batch 11/64 loss: -0.14537596702575684
Batch 12/64 loss: -0.14096707105636597
Batch 13/64 loss: -0.11043083667755127
Batch 14/64 loss: -0.14631807804107666
Batch 15/64 loss: -0.15246230363845825
Batch 16/64 loss: -0.13304120302200317
Batch 17/64 loss: -0.13418203592300415
Batch 18/64 loss: -0.13125401735305786
Batch 19/64 loss: -0.15602779388427734
Batch 20/64 loss: -0.15785950422286987
Batch 21/64 loss: -0.11615383625030518
Batch 22/64 loss: -0.15429437160491943
Batch 23/64 loss: -0.152360737323761
Batch 24/64 loss: -0.1409149169921875
Batch 25/64 loss: -0.1124526858329773
Batch 26/64 loss: -0.14610356092453003
Batch 27/64 loss: -0.15447723865509033
Batch 28/64 loss: -0.14706379175186157
Batch 29/64 loss: -0.1610305905342102
Batch 30/64 loss: -0.12044894695281982
Batch 31/64 loss: -0.14583802223205566
Batch 32/64 loss: -0.12367528676986694
Batch 33/64 loss: -0.1277204155921936
Batch 34/64 loss: -0.12111032009124756
Batch 35/64 loss: -0.15045523643493652
Batch 36/64 loss: -0.1335064172744751
Batch 37/64 loss: -0.11705869436264038
Batch 38/64 loss: -0.13445985317230225
Batch 39/64 loss: -0.1307550072669983
Batch 40/64 loss: -0.14558660984039307
Batch 41/64 loss: -0.12841427326202393
Batch 42/64 loss: -0.1287340521812439
Batch 43/64 loss: -0.15407991409301758
Batch 44/64 loss: -0.13893693685531616
Batch 45/64 loss: -0.1397261619567871
Batch 46/64 loss: -0.1265513300895691
Batch 47/64 loss: -0.14041471481323242
Batch 48/64 loss: -0.15088528394699097
Batch 49/64 loss: -0.14053189754486084
Batch 50/64 loss: -0.1479894518852234
Batch 51/64 loss: -0.13434553146362305
Batch 52/64 loss: -0.12443947792053223
Batch 53/64 loss: -0.13257277011871338
Batch 54/64 loss: -0.1245877742767334
Batch 55/64 loss: -0.12208539247512817
Batch 56/64 loss: -0.10406351089477539
Batch 57/64 loss: -0.11914271116256714
Batch 58/64 loss: -0.14719587564468384
Batch 59/64 loss: -0.1261168122291565
Batch 60/64 loss: -0.13218450546264648
Batch 61/64 loss: -0.15045827627182007
Batch 62/64 loss: -0.15671586990356445
Batch 63/64 loss: -0.13180989027023315
Batch 64/64 loss: -0.11577343940734863
Epoch 33  Train loss: -0.1348638095107733  Val loss: -0.10556098194056769
Epoch 34
-------------------------------
Batch 1/64 loss: -0.12466806173324585
Batch 2/64 loss: -0.14428365230560303
Batch 3/64 loss: -0.09866827726364136
Batch 4/64 loss: -0.13330906629562378
Batch 5/64 loss: -0.1324145793914795
Batch 6/64 loss: -0.13640481233596802
Batch 7/64 loss: -0.1265401840209961
Batch 8/64 loss: -0.13825303316116333
Batch 9/64 loss: -0.12482154369354248
Batch 10/64 loss: -0.15309494733810425
Batch 11/64 loss: -0.14950037002563477
Batch 12/64 loss: -0.13571786880493164
Batch 13/64 loss: -0.1542990803718567
Batch 14/64 loss: -0.13653451204299927
Batch 15/64 loss: -0.1614924669265747
Batch 16/64 loss: -0.14791089296340942
Batch 17/64 loss: -0.12996745109558105
Batch 18/64 loss: -0.14943504333496094
Batch 19/64 loss: -0.11763525009155273
Batch 20/64 loss: -0.1255984902381897
Batch 21/64 loss: -0.12180256843566895
Batch 22/64 loss: -0.16657811403274536
Batch 23/64 loss: -0.13018101453781128
Batch 24/64 loss: -0.1396346092224121
Batch 25/64 loss: -0.1313645839691162
Batch 26/64 loss: -0.1465204954147339
Batch 27/64 loss: -0.1372712254524231
Batch 28/64 loss: -0.11895859241485596
Batch 29/64 loss: -0.1744779348373413
Batch 30/64 loss: -0.15630042552947998
Batch 31/64 loss: -0.15079796314239502
Batch 32/64 loss: -0.14430564641952515
Batch 33/64 loss: -0.11666929721832275
Batch 34/64 loss: -0.13490629196166992
Batch 35/64 loss: -0.15722912549972534
Batch 36/64 loss: -0.12119388580322266
Batch 37/64 loss: -0.12643963098526
Batch 38/64 loss: -0.1387425661087036
Batch 39/64 loss: -0.12378567457199097
Batch 40/64 loss: -0.1170075535774231
Batch 41/64 loss: -0.14805102348327637
Batch 42/64 loss: -0.15324771404266357
Batch 43/64 loss: -0.1443278193473816
Batch 44/64 loss: -0.06763428449630737
Batch 45/64 loss: -0.12941354513168335
Batch 46/64 loss: -0.1517423391342163
Batch 47/64 loss: -0.11231476068496704
Batch 48/64 loss: -0.13105672597885132
Batch 49/64 loss: -0.12261366844177246
Batch 50/64 loss: -0.14885592460632324
Batch 51/64 loss: -0.12452250719070435
Batch 52/64 loss: -0.15495437383651733
Batch 53/64 loss: -0.12284016609191895
Batch 54/64 loss: -0.11795163154602051
Batch 55/64 loss: -0.13491815328598022
Batch 56/64 loss: -0.1501399278640747
Batch 57/64 loss: -0.10917019844055176
Batch 58/64 loss: -0.15363085269927979
Batch 59/64 loss: -0.137864351272583
Batch 60/64 loss: -0.1282225251197815
Batch 61/64 loss: -0.154205322265625
Batch 62/64 loss: -0.12677592039108276
Batch 63/64 loss: -0.13068604469299316
Batch 64/64 loss: -0.1378486156463623
Epoch 34  Train loss: -0.13542341998979157  Val loss: -0.10060490816319521
Epoch 35
-------------------------------
Batch 1/64 loss: -0.14335215091705322
Batch 2/64 loss: -0.13354599475860596
Batch 3/64 loss: -0.1302403211593628
Batch 4/64 loss: -0.1175122857093811
Batch 5/64 loss: -0.12769782543182373
Batch 6/64 loss: -0.15105050802230835
Batch 7/64 loss: -0.160572350025177
Batch 8/64 loss: -0.1327747106552124
Batch 9/64 loss: -0.12249630689620972
Batch 10/64 loss: -0.14275497198104858
Batch 11/64 loss: -0.15218985080718994
Batch 12/64 loss: -0.141862690448761
Batch 13/64 loss: -0.14760178327560425
Batch 14/64 loss: -0.13553285598754883
Batch 15/64 loss: -0.1433059573173523
Batch 16/64 loss: -0.12126708030700684
Batch 17/64 loss: -0.15429967641830444
Batch 18/64 loss: -0.14573264122009277
Batch 19/64 loss: -0.14037823677062988
Batch 20/64 loss: -0.13228464126586914
Batch 21/64 loss: -0.09830957651138306
Batch 22/64 loss: -0.14792168140411377
Batch 23/64 loss: -0.10139799118041992
Batch 24/64 loss: -0.15879708528518677
Batch 25/64 loss: -0.1557592749595642
Batch 26/64 loss: -0.15814602375030518
Batch 27/64 loss: -0.11105859279632568
Batch 28/64 loss: -0.12863963842391968
Batch 29/64 loss: -0.14276206493377686
Batch 30/64 loss: -0.1480286717414856
Batch 31/64 loss: -0.170798659324646
Batch 32/64 loss: -0.136491060256958
Batch 33/64 loss: -0.11432015895843506
Batch 34/64 loss: -0.12824517488479614
Batch 35/64 loss: -0.13629937171936035
Batch 36/64 loss: -0.14197206497192383
Batch 37/64 loss: -0.1357288360595703
Batch 38/64 loss: -0.14874792098999023
Batch 39/64 loss: -0.11353206634521484
Batch 40/64 loss: -0.1348358392715454
Batch 41/64 loss: -0.1346912384033203
Batch 42/64 loss: -0.17675817012786865
Batch 43/64 loss: -0.1628362536430359
Batch 44/64 loss: -0.14490938186645508
Batch 45/64 loss: -0.11694735288619995
Batch 46/64 loss: -0.14762824773788452
Batch 47/64 loss: -0.15329355001449585
Batch 48/64 loss: -0.1564505696296692
Batch 49/64 loss: -0.12134110927581787
Batch 50/64 loss: -0.1313965916633606
Batch 51/64 loss: -0.13608777523040771
Batch 52/64 loss: -0.14679980278015137
Batch 53/64 loss: -0.12214392423629761
Batch 54/64 loss: -0.14061731100082397
Batch 55/64 loss: -0.14585107564926147
Batch 56/64 loss: -0.10890251398086548
Batch 57/64 loss: -0.14073717594146729
Batch 58/64 loss: -0.12669169902801514
Batch 59/64 loss: -0.11242902278900146
Batch 60/64 loss: -0.15697771310806274
Batch 61/64 loss: -0.1398603320121765
Batch 62/64 loss: -0.14211881160736084
Batch 63/64 loss: -0.14610016345977783
Batch 64/64 loss: -0.13261252641677856
Epoch 35  Train loss: -0.13802782390631882  Val loss: -0.10717656522272379
Saving best model, epoch: 35
Epoch 36
-------------------------------
Batch 1/64 loss: -0.14643967151641846
Batch 2/64 loss: -0.1478678584098816
Batch 3/64 loss: -0.13142108917236328
Batch 4/64 loss: -0.15158140659332275
Batch 5/64 loss: -0.15788650512695312
Batch 6/64 loss: -0.14110541343688965
Batch 7/64 loss: -0.14772659540176392
Batch 8/64 loss: -0.15243792533874512
Batch 9/64 loss: -0.12368923425674438
Batch 10/64 loss: -0.11329782009124756
Batch 11/64 loss: -0.1506941318511963
Batch 12/64 loss: -0.1439138650894165
Batch 13/64 loss: -0.1158221960067749
Batch 14/64 loss: -0.11219704151153564
Batch 15/64 loss: -0.1364203691482544
Batch 16/64 loss: -0.14579039812088013
Batch 17/64 loss: -0.10019439458847046
Batch 18/64 loss: -0.13009989261627197
Batch 19/64 loss: -0.14454859495162964
Batch 20/64 loss: -0.15053105354309082
Batch 21/64 loss: -0.12443047761917114
Batch 22/64 loss: -0.12929272651672363
Batch 23/64 loss: -0.13609445095062256
Batch 24/64 loss: -0.1258070468902588
Batch 25/64 loss: -0.1439359188079834
Batch 26/64 loss: -0.1618615984916687
Batch 27/64 loss: -0.13825225830078125
Batch 28/64 loss: -0.13836610317230225
Batch 29/64 loss: -0.11857658624649048
Batch 30/64 loss: -0.1407226324081421
Batch 31/64 loss: -0.1515568494796753
Batch 32/64 loss: -0.14510118961334229
Batch 33/64 loss: -0.16095823049545288
Batch 34/64 loss: -0.14536947011947632
Batch 35/64 loss: -0.15013480186462402
Batch 36/64 loss: -0.1337534785270691
Batch 37/64 loss: -0.13310086727142334
Batch 38/64 loss: -0.15732264518737793
Batch 39/64 loss: -0.13801860809326172
Batch 40/64 loss: -0.14676976203918457
Batch 41/64 loss: -0.16644608974456787
Batch 42/64 loss: -0.13371092081069946
Batch 43/64 loss: -0.12696486711502075
Batch 44/64 loss: -0.13031959533691406
Batch 45/64 loss: -0.14455437660217285
Batch 46/64 loss: -0.13475149869918823
Batch 47/64 loss: -0.14412277936935425
Batch 48/64 loss: -0.1170005202293396
Batch 49/64 loss: -0.1443263292312622
Batch 50/64 loss: -0.15077245235443115
Batch 51/64 loss: -0.14481323957443237
Batch 52/64 loss: -0.14423024654388428
Batch 53/64 loss: -0.14899206161499023
Batch 54/64 loss: -0.16017210483551025
Batch 55/64 loss: -0.15208053588867188
Batch 56/64 loss: -0.12251412868499756
Batch 57/64 loss: -0.14924883842468262
Batch 58/64 loss: -0.11441999673843384
Batch 59/64 loss: -0.15407228469848633
Batch 60/64 loss: -0.1554352045059204
Batch 61/64 loss: -0.15789616107940674
Batch 62/64 loss: -0.15575122833251953
Batch 63/64 loss: -0.10185611248016357
Batch 64/64 loss: -0.12904691696166992
Epoch 36  Train loss: -0.1398325948154225  Val loss: -0.07545521353528262
Epoch 37
-------------------------------
Batch 1/64 loss: -0.14942246675491333
Batch 2/64 loss: -0.12898296117782593
Batch 3/64 loss: -0.1414097547531128
Batch 4/64 loss: -0.13910222053527832
Batch 5/64 loss: -0.11742067337036133
Batch 6/64 loss: -0.13613581657409668
Batch 7/64 loss: -0.14020371437072754
Batch 8/64 loss: -0.1299089789390564
Batch 9/64 loss: -0.1255120038986206
Batch 10/64 loss: -0.14236772060394287
Batch 11/64 loss: -0.12220418453216553
Batch 12/64 loss: -0.1500474214553833
Batch 13/64 loss: -0.12177741527557373
Batch 14/64 loss: -0.16750526428222656
Batch 15/64 loss: -0.07539689540863037
Batch 16/64 loss: -0.1353691816329956
Batch 17/64 loss: -0.1579594612121582
Batch 18/64 loss: -0.11132597923278809
Batch 19/64 loss: -0.14113664627075195
Batch 20/64 loss: -0.13941025733947754
Batch 21/64 loss: -0.12692928314208984
Batch 22/64 loss: -0.13124144077301025
Batch 23/64 loss: -0.11298280954360962
Batch 24/64 loss: -0.12051427364349365
Batch 25/64 loss: -0.127862811088562
Batch 26/64 loss: -0.1279633641242981
Batch 27/64 loss: -0.12827461957931519
Batch 28/64 loss: -0.14681315422058105
Batch 29/64 loss: -0.12774670124053955
Batch 30/64 loss: -0.12193191051483154
Batch 31/64 loss: -0.13116806745529175
Batch 32/64 loss: -0.14211952686309814
Batch 33/64 loss: -0.11412382125854492
Batch 34/64 loss: -0.11727017164230347
Batch 35/64 loss: -0.15160971879959106
Batch 36/64 loss: -0.12156009674072266
Batch 37/64 loss: -0.1322237253189087
Batch 38/64 loss: -0.13519048690795898
Batch 39/64 loss: -0.12310242652893066
Batch 40/64 loss: -0.14157068729400635
Batch 41/64 loss: -0.14551740884780884
Batch 42/64 loss: -0.1659376621246338
Batch 43/64 loss: -0.12187957763671875
Batch 44/64 loss: -0.15199732780456543
Batch 45/64 loss: -0.1427018642425537
Batch 46/64 loss: -0.1511296033859253
Batch 47/64 loss: -0.11180883646011353
Batch 48/64 loss: -0.15439677238464355
Batch 49/64 loss: -0.1563812494277954
Batch 50/64 loss: -0.1547302007675171
Batch 51/64 loss: -0.15245503187179565
Batch 52/64 loss: -0.14369070529937744
Batch 53/64 loss: -0.14886921644210815
Batch 54/64 loss: -0.14670616388320923
Batch 55/64 loss: -0.15008848905563354
Batch 56/64 loss: -0.141252338886261
Batch 57/64 loss: -0.13821125030517578
Batch 58/64 loss: -0.14849668741226196
Batch 59/64 loss: -0.14169251918792725
Batch 60/64 loss: -0.1417006254196167
Batch 61/64 loss: -0.13468831777572632
Batch 62/64 loss: -0.15816336870193481
Batch 63/64 loss: -0.13386833667755127
Batch 64/64 loss: -0.15999865531921387
Epoch 37  Train loss: -0.1366456574084712  Val loss: -0.11775590279667648
Saving best model, epoch: 37
Epoch 38
-------------------------------
Batch 1/64 loss: -0.1543196439743042
Batch 2/64 loss: -0.15347415208816528
Batch 3/64 loss: -0.16751313209533691
Batch 4/64 loss: -0.15143972635269165
Batch 5/64 loss: -0.16114556789398193
Batch 6/64 loss: -0.13444340229034424
Batch 7/64 loss: -0.149261474609375
Batch 8/64 loss: -0.13890540599822998
Batch 9/64 loss: -0.13815170526504517
Batch 10/64 loss: -0.1346750259399414
Batch 11/64 loss: -0.15006643533706665
Batch 12/64 loss: -0.15713179111480713
Batch 13/64 loss: -0.12509441375732422
Batch 14/64 loss: -0.1526012420654297
Batch 15/64 loss: -0.12927502393722534
Batch 16/64 loss: -0.1378849744796753
Batch 17/64 loss: -0.12099605798721313
Batch 18/64 loss: -0.1332414746284485
Batch 19/64 loss: -0.14858520030975342
Batch 20/64 loss: -0.14613991975784302
Batch 21/64 loss: -0.12527161836624146
Batch 22/64 loss: -0.14063727855682373
Batch 23/64 loss: -0.1647205352783203
Batch 24/64 loss: -0.13927710056304932
Batch 25/64 loss: -0.1454032063484192
Batch 26/64 loss: -0.13344919681549072
Batch 27/64 loss: -0.1693131923675537
Batch 28/64 loss: -0.11680471897125244
Batch 29/64 loss: -0.17196840047836304
Batch 30/64 loss: -0.15321487188339233
Batch 31/64 loss: -0.12778830528259277
Batch 32/64 loss: -0.15993279218673706
Batch 33/64 loss: -0.16168278455734253
Batch 34/64 loss: -0.15740197896957397
Batch 35/64 loss: -0.13523387908935547
Batch 36/64 loss: -0.13045161962509155
Batch 37/64 loss: -0.15383636951446533
Batch 38/64 loss: -0.13860094547271729
Batch 39/64 loss: -0.15257906913757324
Batch 40/64 loss: -0.1819186806678772
Batch 41/64 loss: -0.14427310228347778
Batch 42/64 loss: -0.16085302829742432
Batch 43/64 loss: -0.14053285121917725
Batch 44/64 loss: -0.14409148693084717
Batch 45/64 loss: -0.1442701816558838
Batch 46/64 loss: -0.13158565759658813
Batch 47/64 loss: -0.1315189003944397
Batch 48/64 loss: -0.16829383373260498
Batch 49/64 loss: -0.1136736273765564
Batch 50/64 loss: -0.15661609172821045
Batch 51/64 loss: -0.1462203860282898
Batch 52/64 loss: -0.12922906875610352
Batch 53/64 loss: -0.14663827419281006
Batch 54/64 loss: -0.13244402408599854
Batch 55/64 loss: -0.13815420866012573
Batch 56/64 loss: -0.13043934106826782
Batch 57/64 loss: -0.16594964265823364
Batch 58/64 loss: -0.14591360092163086
Batch 59/64 loss: -0.1465226411819458
Batch 60/64 loss: -0.15799206495285034
Batch 61/64 loss: -0.1304943561553955
Batch 62/64 loss: -0.12331897020339966
Batch 63/64 loss: -0.15113097429275513
Batch 64/64 loss: -0.12720364332199097
Epoch 38  Train loss: -0.14461790370006186  Val loss: -0.06334394421364434
Epoch 39
-------------------------------
Batch 1/64 loss: -0.16314810514450073
Batch 2/64 loss: -0.12798547744750977
Batch 3/64 loss: -0.12051689624786377
Batch 4/64 loss: -0.1457371711730957
Batch 5/64 loss: -0.14706993103027344
Batch 6/64 loss: -0.160392165184021
Batch 7/64 loss: -0.11766397953033447
Batch 8/64 loss: -0.11871135234832764
Batch 9/64 loss: -0.15041524171829224
Batch 10/64 loss: -0.15274560451507568
Batch 11/64 loss: -0.13501501083374023
Batch 12/64 loss: -0.16615617275238037
Batch 13/64 loss: -0.12947702407836914
Batch 14/64 loss: -0.13756966590881348
Batch 15/64 loss: -0.13268691301345825
Batch 16/64 loss: -0.128398597240448
Batch 17/64 loss: -0.1530388593673706
Batch 18/64 loss: -0.13862240314483643
Batch 19/64 loss: -0.13716578483581543
Batch 20/64 loss: -0.13733714818954468
Batch 21/64 loss: -0.13688677549362183
Batch 22/64 loss: -0.13442081212997437
Batch 23/64 loss: -0.15051090717315674
Batch 24/64 loss: -0.15067875385284424
Batch 25/64 loss: -0.14619344472885132
Batch 26/64 loss: -0.1341056227684021
Batch 27/64 loss: -0.14865273237228394
Batch 28/64 loss: -0.14622902870178223
Batch 29/64 loss: -0.13007813692092896
Batch 30/64 loss: -0.1516786813735962
Batch 31/64 loss: -0.12352478504180908
Batch 32/64 loss: -0.1563279628753662
Batch 33/64 loss: -0.14780175685882568
Batch 34/64 loss: -0.13788574934005737
Batch 35/64 loss: -0.10617625713348389
Batch 36/64 loss: -0.13344937562942505
Batch 37/64 loss: -0.16839855909347534
Batch 38/64 loss: -0.16214025020599365
Batch 39/64 loss: -0.1479320526123047
Batch 40/64 loss: -0.1429293155670166
Batch 41/64 loss: -0.15196329355239868
Batch 42/64 loss: -0.15568113327026367
Batch 43/64 loss: -0.13756680488586426
Batch 44/64 loss: -0.168712317943573
Batch 45/64 loss: -0.11877214908599854
Batch 46/64 loss: -0.14031660556793213
Batch 47/64 loss: -0.14112776517868042
Batch 48/64 loss: -0.1500348448753357
Batch 49/64 loss: -0.16052567958831787
Batch 50/64 loss: -0.14992296695709229
Batch 51/64 loss: -0.16404426097869873
Batch 52/64 loss: -0.1414412260055542
Batch 53/64 loss: -0.18003827333450317
Batch 54/64 loss: -0.14762228727340698
Batch 55/64 loss: -0.1491776704788208
Batch 56/64 loss: -0.14164865016937256
Batch 57/64 loss: -0.15335267782211304
Batch 58/64 loss: -0.14770710468292236
Batch 59/64 loss: -0.1595221757888794
Batch 60/64 loss: -0.12609100341796875
Batch 61/64 loss: -0.1363658905029297
Batch 62/64 loss: -0.16028720140457153
Batch 63/64 loss: -0.13851535320281982
Batch 64/64 loss: -0.12652885913848877
Epoch 39  Train loss: -0.1438617795121436  Val loss: -0.1272560296599398
Saving best model, epoch: 39
Epoch 40
-------------------------------
Batch 1/64 loss: -0.1325744390487671
Batch 2/64 loss: -0.16015499830245972
Batch 3/64 loss: -0.13597840070724487
Batch 4/64 loss: -0.14525294303894043
Batch 5/64 loss: -0.14937478303909302
Batch 6/64 loss: -0.1439218521118164
Batch 7/64 loss: -0.14745736122131348
Batch 8/64 loss: -0.12807023525238037
Batch 9/64 loss: -0.1472247838973999
Batch 10/64 loss: -0.12931698560714722
Batch 11/64 loss: -0.14337974786758423
Batch 12/64 loss: -0.15465855598449707
Batch 13/64 loss: -0.1490529179573059
Batch 14/64 loss: -0.15809094905853271
Batch 15/64 loss: -0.15620911121368408
Batch 16/64 loss: -0.16088569164276123
Batch 17/64 loss: -0.15587502717971802
Batch 18/64 loss: -0.16604995727539062
Batch 19/64 loss: -0.15001940727233887
Batch 20/64 loss: -0.1493244767189026
Batch 21/64 loss: -0.1417635679244995
Batch 22/64 loss: -0.13465815782546997
Batch 23/64 loss: -0.17173343896865845
Batch 24/64 loss: -0.15291523933410645
Batch 25/64 loss: -0.15400493144989014
Batch 26/64 loss: -0.17218250036239624
Batch 27/64 loss: -0.15211701393127441
Batch 28/64 loss: -0.14279532432556152
Batch 29/64 loss: -0.13253110647201538
Batch 30/64 loss: -0.1365668773651123
Batch 31/64 loss: -0.13873988389968872
Batch 32/64 loss: -0.15516549348831177
Batch 33/64 loss: -0.13739162683486938
Batch 34/64 loss: -0.15456080436706543
Batch 35/64 loss: -0.15198582410812378
Batch 36/64 loss: -0.14785897731781006
Batch 37/64 loss: -0.18141204118728638
Batch 38/64 loss: -0.1703892946243286
Batch 39/64 loss: -0.14932125806808472
Batch 40/64 loss: -0.16276395320892334
Batch 41/64 loss: -0.16954505443572998
Batch 42/64 loss: -0.15870708227157593
Batch 43/64 loss: -0.13955241441726685
Batch 44/64 loss: -0.16289734840393066
Batch 45/64 loss: -0.1637004017829895
Batch 46/64 loss: -0.14261579513549805
Batch 47/64 loss: -0.13435900211334229
Batch 48/64 loss: -0.15646809339523315
Batch 49/64 loss: -0.18089944124221802
Batch 50/64 loss: -0.14394450187683105
Batch 51/64 loss: -0.16394048929214478
Batch 52/64 loss: -0.13825702667236328
Batch 53/64 loss: -0.1493830680847168
Batch 54/64 loss: -0.14182299375534058
Batch 55/64 loss: -0.15978127717971802
Batch 56/64 loss: -0.13826334476470947
Batch 57/64 loss: -0.16722965240478516
Batch 58/64 loss: -0.14524871110916138
Batch 59/64 loss: -0.11601698398590088
Batch 60/64 loss: -0.14942532777786255
Batch 61/64 loss: -0.14341861009597778
Batch 62/64 loss: -0.12711578607559204
Batch 63/64 loss: -0.15118181705474854
Batch 64/64 loss: -0.14831560850143433
Epoch 40  Train loss: -0.14994103324179556  Val loss: -0.1270048020221933
Epoch 41
-------------------------------
Batch 1/64 loss: -0.13836008310317993
Batch 2/64 loss: -0.15872079133987427
Batch 3/64 loss: -0.189531147480011
Batch 4/64 loss: -0.14505475759506226
Batch 5/64 loss: -0.1710718274116516
Batch 6/64 loss: -0.160783052444458
Batch 7/64 loss: -0.1263793706893921
Batch 8/64 loss: -0.16392648220062256
Batch 9/64 loss: -0.17064416408538818
Batch 10/64 loss: -0.13564014434814453
Batch 11/64 loss: -0.14579910039901733
Batch 12/64 loss: -0.16187608242034912
Batch 13/64 loss: -0.14157819747924805
Batch 14/64 loss: -0.17587196826934814
Batch 15/64 loss: -0.17392325401306152
Batch 16/64 loss: -0.15316718816757202
Batch 17/64 loss: -0.13581836223602295
Batch 18/64 loss: -0.16038548946380615
Batch 19/64 loss: -0.14753562211990356
Batch 20/64 loss: -0.1360321044921875
Batch 21/64 loss: -0.1334558129310608
Batch 22/64 loss: -0.12034523487091064
Batch 23/64 loss: -0.1342078447341919
Batch 24/64 loss: -0.12015402317047119
Batch 25/64 loss: -0.16125881671905518
Batch 26/64 loss: -0.15387916564941406
Batch 27/64 loss: -0.15075606107711792
Batch 28/64 loss: -0.12962967157363892
Batch 29/64 loss: -0.15781480073928833
Batch 30/64 loss: -0.12361198663711548
Batch 31/64 loss: -0.15922141075134277
Batch 32/64 loss: -0.16310787200927734
Batch 33/64 loss: -0.13059264421463013
Batch 34/64 loss: -0.14912807941436768
Batch 35/64 loss: -0.1367623209953308
Batch 36/64 loss: -0.1595481038093567
Batch 37/64 loss: -0.1398909091949463
Batch 38/64 loss: -0.1613059639930725
Batch 39/64 loss: -0.1464846134185791
Batch 40/64 loss: -0.14618772268295288
Batch 41/64 loss: -0.12012612819671631
Batch 42/64 loss: -0.148249089717865
Batch 43/64 loss: -0.15169036388397217
Batch 44/64 loss: -0.14761465787887573
Batch 45/64 loss: -0.1342373490333557
Batch 46/64 loss: -0.1562204360961914
Batch 47/64 loss: -0.1759844422340393
Batch 48/64 loss: -0.14518290758132935
Batch 49/64 loss: -0.13484376668930054
Batch 50/64 loss: -0.12799274921417236
Batch 51/64 loss: -0.13714879751205444
Batch 52/64 loss: -0.149156391620636
Batch 53/64 loss: -0.15162301063537598
Batch 54/64 loss: -0.13708001375198364
Batch 55/64 loss: -0.1829422116279602
Batch 56/64 loss: -0.1508101224899292
Batch 57/64 loss: -0.15143954753875732
Batch 58/64 loss: -0.1280374526977539
Batch 59/64 loss: -0.1389402151107788
Batch 60/64 loss: -0.1356797218322754
Batch 61/64 loss: -0.16399800777435303
Batch 62/64 loss: -0.14715087413787842
Batch 63/64 loss: -0.12797784805297852
Batch 64/64 loss: -0.16464251279830933
Epoch 41  Train loss: -0.14803216060002644  Val loss: -0.1212591436720386
Epoch 42
-------------------------------
Batch 1/64 loss: -0.16938984394073486
Batch 2/64 loss: -0.16076719760894775
Batch 3/64 loss: -0.146736741065979
Batch 4/64 loss: -0.1564427614212036
Batch 5/64 loss: -0.11912387609481812
Batch 6/64 loss: -0.13722646236419678
Batch 7/64 loss: -0.15425246953964233
Batch 8/64 loss: -0.14288657903671265
Batch 9/64 loss: -0.13748568296432495
Batch 10/64 loss: -0.11448138952255249
Batch 11/64 loss: -0.12765681743621826
Batch 12/64 loss: -0.1562567949295044
Batch 13/64 loss: -0.13258051872253418
Batch 14/64 loss: -0.16443538665771484
Batch 15/64 loss: -0.15209680795669556
Batch 16/64 loss: -0.14989423751831055
Batch 17/64 loss: -0.15832942724227905
Batch 18/64 loss: -0.14527803659439087
Batch 19/64 loss: -0.15552973747253418
Batch 20/64 loss: -0.13793033361434937
Batch 21/64 loss: -0.15194827318191528
Batch 22/64 loss: -0.13805288076400757
Batch 23/64 loss: -0.14386248588562012
Batch 24/64 loss: -0.17182183265686035
Batch 25/64 loss: -0.1666225790977478
Batch 26/64 loss: -0.15242791175842285
Batch 27/64 loss: -0.14952290058135986
Batch 28/64 loss: -0.16145408153533936
Batch 29/64 loss: -0.12240761518478394
Batch 30/64 loss: -0.14409494400024414
Batch 31/64 loss: -0.15968596935272217
Batch 32/64 loss: -0.1410433053970337
Batch 33/64 loss: -0.15605735778808594
Batch 34/64 loss: -0.14577239751815796
Batch 35/64 loss: -0.1502588391304016
Batch 36/64 loss: -0.16678810119628906
Batch 37/64 loss: -0.1427190899848938
Batch 38/64 loss: -0.15831345319747925
Batch 39/64 loss: -0.15917587280273438
Batch 40/64 loss: -0.13485383987426758
Batch 41/64 loss: -0.13808763027191162
Batch 42/64 loss: -0.14569896459579468
Batch 43/64 loss: -0.14901697635650635
Batch 44/64 loss: -0.13849341869354248
Batch 45/64 loss: -0.14184582233428955
Batch 46/64 loss: -0.16481316089630127
Batch 47/64 loss: -0.14603924751281738
Batch 48/64 loss: -0.14067119359970093
Batch 49/64 loss: -0.1429843306541443
Batch 50/64 loss: -0.15137726068496704
Batch 51/64 loss: -0.16209334135055542
Batch 52/64 loss: -0.13823652267456055
Batch 53/64 loss: -0.1579188108444214
Batch 54/64 loss: -0.12575602531433105
Batch 55/64 loss: -0.1472240686416626
Batch 56/64 loss: -0.14843904972076416
Batch 57/64 loss: -0.15434765815734863
Batch 58/64 loss: -0.14870333671569824
Batch 59/64 loss: -0.14390522241592407
Batch 60/64 loss: -0.15106302499771118
Batch 61/64 loss: -0.16341185569763184
Batch 62/64 loss: -0.1459275484085083
Batch 63/64 loss: -0.1623133420944214
Batch 64/64 loss: -0.12254154682159424
Epoch 42  Train loss: -0.1480147263583015  Val loss: -0.09164312574052319
Epoch 43
-------------------------------
Batch 1/64 loss: -0.1366139054298401
Batch 2/64 loss: -0.1576492190361023
Batch 3/64 loss: -0.15330564975738525
Batch 4/64 loss: -0.1455056071281433
Batch 5/64 loss: -0.1671130657196045
Batch 6/64 loss: -0.10819673538208008
Batch 7/64 loss: -0.1435021162033081
Batch 8/64 loss: -0.15455996990203857
Batch 9/64 loss: -0.1447044014930725
Batch 10/64 loss: -0.15196317434310913
Batch 11/64 loss: -0.15754938125610352
Batch 12/64 loss: -0.17485475540161133
Batch 13/64 loss: -0.14323151111602783
Batch 14/64 loss: -0.15636605024337769
Batch 15/64 loss: -0.15604513883590698
Batch 16/64 loss: -0.14323300123214722
Batch 17/64 loss: -0.15340226888656616
Batch 18/64 loss: -0.17795252799987793
Batch 19/64 loss: -0.15046441555023193
Batch 20/64 loss: -0.15167200565338135
Batch 21/64 loss: -0.16324615478515625
Batch 22/64 loss: -0.1618196964263916
Batch 23/64 loss: -0.1674971580505371
Batch 24/64 loss: -0.14968574047088623
Batch 25/64 loss: -0.14234232902526855
Batch 26/64 loss: -0.16878873109817505
Batch 27/64 loss: -0.16248393058776855
Batch 28/64 loss: -0.15335917472839355
Batch 29/64 loss: -0.16567033529281616
Batch 30/64 loss: -0.14821290969848633
Batch 31/64 loss: -0.16341054439544678
Batch 32/64 loss: -0.1323193907737732
Batch 33/64 loss: -0.13082093000411987
Batch 34/64 loss: -0.15982943773269653
Batch 35/64 loss: -0.120968759059906
Batch 36/64 loss: -0.1493244767189026
Batch 37/64 loss: -0.13208317756652832
Batch 38/64 loss: -0.13596469163894653
Batch 39/64 loss: -0.15589654445648193
Batch 40/64 loss: -0.1409679651260376
Batch 41/64 loss: -0.12167632579803467
Batch 42/64 loss: -0.1800626516342163
Batch 43/64 loss: -0.14491045475006104
Batch 44/64 loss: -0.15327751636505127
Batch 45/64 loss: -0.14577001333236694
Batch 46/64 loss: -0.14660155773162842
Batch 47/64 loss: -0.14791816473007202
Batch 48/64 loss: -0.1259341835975647
Batch 49/64 loss: -0.1638505458831787
Batch 50/64 loss: -0.13343673944473267
Batch 51/64 loss: -0.13834983110427856
Batch 52/64 loss: -0.16346687078475952
Batch 53/64 loss: -0.15145671367645264
Batch 54/64 loss: -0.15148359537124634
Batch 55/64 loss: -0.15620696544647217
Batch 56/64 loss: -0.18038475513458252
Batch 57/64 loss: -0.14680159091949463
Batch 58/64 loss: -0.1565920114517212
Batch 59/64 loss: -0.11220544576644897
Batch 60/64 loss: -0.1488611102104187
Batch 61/64 loss: -0.14618730545043945
Batch 62/64 loss: -0.15941905975341797
Batch 63/64 loss: -0.14998161792755127
Batch 64/64 loss: -0.16913443803787231
Epoch 43  Train loss: -0.15034137777253692  Val loss: -0.12253162725684569
Epoch 44
-------------------------------
Batch 1/64 loss: -0.1550527811050415
Batch 2/64 loss: -0.17933624982833862
Batch 3/64 loss: -0.15489250421524048
Batch 4/64 loss: -0.16337800025939941
Batch 5/64 loss: -0.13282853364944458
Batch 6/64 loss: -0.15800940990447998
Batch 7/64 loss: -0.13913023471832275
Batch 8/64 loss: -0.16460388898849487
Batch 9/64 loss: -0.16154110431671143
Batch 10/64 loss: -0.1346297264099121
Batch 11/64 loss: -0.14779305458068848
Batch 12/64 loss: -0.1480831503868103
Batch 13/64 loss: -0.14570331573486328
Batch 14/64 loss: -0.13535654544830322
Batch 15/64 loss: -0.1403118371963501
Batch 16/64 loss: -0.1423322558403015
Batch 17/64 loss: -0.1369817852973938
Batch 18/64 loss: -0.1694003939628601
Batch 19/64 loss: -0.16071367263793945
Batch 20/64 loss: -0.15472543239593506
Batch 21/64 loss: -0.14575636386871338
Batch 22/64 loss: -0.15707671642303467
Batch 23/64 loss: -0.16087740659713745
Batch 24/64 loss: -0.15463536977767944
Batch 25/64 loss: -0.15807437896728516
Batch 26/64 loss: -0.1684672236442566
Batch 27/64 loss: -0.10685449838638306
Batch 28/64 loss: -0.1566908359527588
Batch 29/64 loss: -0.1611311435699463
Batch 30/64 loss: -0.1451258659362793
Batch 31/64 loss: -0.17150425910949707
Batch 32/64 loss: -0.15009349584579468
Batch 33/64 loss: -0.16969311237335205
Batch 34/64 loss: -0.14039498567581177
Batch 35/64 loss: -0.1411035656929016
Batch 36/64 loss: -0.1603984832763672
Batch 37/64 loss: -0.12906503677368164
Batch 38/64 loss: -0.15495067834854126
Batch 39/64 loss: -0.15437358617782593
Batch 40/64 loss: -0.1471010446548462
Batch 41/64 loss: -0.13199222087860107
Batch 42/64 loss: -0.11900186538696289
Batch 43/64 loss: -0.14906185865402222
Batch 44/64 loss: -0.16331619024276733
Batch 45/64 loss: -0.15274649858474731
Batch 46/64 loss: -0.1426539421081543
Batch 47/64 loss: -0.14492732286453247
Batch 48/64 loss: -0.1443697214126587
Batch 49/64 loss: -0.15332525968551636
Batch 50/64 loss: -0.16424572467803955
Batch 51/64 loss: -0.14937615394592285
Batch 52/64 loss: -0.1546177864074707
Batch 53/64 loss: -0.15203869342803955
Batch 54/64 loss: -0.14788174629211426
Batch 55/64 loss: -0.1502034068107605
Batch 56/64 loss: -0.1581745147705078
Batch 57/64 loss: -0.12120616436004639
Batch 58/64 loss: -0.1409996747970581
Batch 59/64 loss: -0.1544298529624939
Batch 60/64 loss: -0.16586637496948242
Batch 61/64 loss: -0.16189879179000854
Batch 62/64 loss: -0.13910388946533203
Batch 63/64 loss: -0.15534818172454834
Batch 64/64 loss: -0.14504903554916382
Epoch 44  Train loss: -0.15033324769898956  Val loss: -0.11715937848762958
Epoch 45
-------------------------------
Batch 1/64 loss: -0.14130252599716187
Batch 2/64 loss: -0.14959430694580078
Batch 3/64 loss: -0.1685846447944641
Batch 4/64 loss: -0.18796968460083008
Batch 5/64 loss: -0.16719233989715576
Batch 6/64 loss: -0.14106488227844238
Batch 7/64 loss: -0.14367878437042236
Batch 8/64 loss: -0.17823171615600586
Batch 9/64 loss: -0.1670817732810974
Batch 10/64 loss: -0.1407063603401184
Batch 11/64 loss: -0.15611666440963745
Batch 12/64 loss: -0.1372150182723999
Batch 13/64 loss: -0.14980071783065796
Batch 14/64 loss: -0.1438106894493103
Batch 15/64 loss: -0.1499006748199463
Batch 16/64 loss: -0.15434688329696655
Batch 17/64 loss: -0.15165746212005615
Batch 18/64 loss: -0.15080100297927856
Batch 19/64 loss: -0.11981254816055298
Batch 20/64 loss: -0.16779911518096924
Batch 21/64 loss: -0.1432133913040161
Batch 22/64 loss: -0.1583777666091919
Batch 23/64 loss: -0.16802042722702026
Batch 24/64 loss: -0.1453160047531128
Batch 25/64 loss: -0.16995441913604736
Batch 26/64 loss: -0.12226128578186035
Batch 27/64 loss: -0.14298784732818604
Batch 28/64 loss: -0.15323543548583984
Batch 29/64 loss: -0.16336214542388916
Batch 30/64 loss: -0.1648327112197876
Batch 31/64 loss: -0.15849679708480835
Batch 32/64 loss: -0.14920592308044434
Batch 33/64 loss: -0.17551404237747192
Batch 34/64 loss: -0.14771562814712524
Batch 35/64 loss: -0.15619564056396484
Batch 36/64 loss: -0.1708139181137085
Batch 37/64 loss: -0.1256963014602661
Batch 38/64 loss: -0.14378130435943604
Batch 39/64 loss: -0.13985413312911987
Batch 40/64 loss: -0.1718379259109497
Batch 41/64 loss: -0.1627178192138672
Batch 42/64 loss: -0.15715420246124268
Batch 43/64 loss: -0.16968506574630737
Batch 44/64 loss: -0.15893787145614624
Batch 45/64 loss: -0.14713239669799805
Batch 46/64 loss: -0.1627432107925415
Batch 47/64 loss: -0.16737431287765503
Batch 48/64 loss: -0.1619134545326233
Batch 49/64 loss: -0.17646509408950806
Batch 50/64 loss: -0.14898836612701416
Batch 51/64 loss: -0.182839035987854
Batch 52/64 loss: -0.1612035036087036
Batch 53/64 loss: -0.15837103128433228
Batch 54/64 loss: -0.14832448959350586
Batch 55/64 loss: -0.14025849103927612
Batch 56/64 loss: -0.15173739194869995
Batch 57/64 loss: -0.1578693389892578
Batch 58/64 loss: -0.16565299034118652
Batch 59/64 loss: -0.16380000114440918
Batch 60/64 loss: -0.1595020294189453
Batch 61/64 loss: -0.172183096408844
Batch 62/64 loss: -0.17583835124969482
Batch 63/64 loss: -0.1224401593208313
Batch 64/64 loss: -0.16900759935379028
Epoch 45  Train loss: -0.15587809015722837  Val loss: -0.10886425824509453
Epoch 46
-------------------------------
Batch 1/64 loss: -0.15215468406677246
Batch 2/64 loss: -0.14541828632354736
Batch 3/64 loss: -0.1356547474861145
Batch 4/64 loss: -0.16355431079864502
Batch 5/64 loss: -0.15486055612564087
Batch 6/64 loss: -0.17189174890518188
Batch 7/64 loss: -0.14947623014450073
Batch 8/64 loss: -0.17332112789154053
Batch 9/64 loss: -0.15301179885864258
Batch 10/64 loss: -0.13099706172943115
Batch 11/64 loss: -0.13434231281280518
Batch 12/64 loss: -0.1556473970413208
Batch 13/64 loss: -0.12407022714614868
Batch 14/64 loss: -0.19073808193206787
Batch 15/64 loss: -0.156450092792511
Batch 16/64 loss: -0.1700497269630432
Batch 17/64 loss: -0.15070831775665283
Batch 18/64 loss: -0.15016484260559082
Batch 19/64 loss: -0.18254327774047852
Batch 20/64 loss: -0.16687244176864624
Batch 21/64 loss: -0.1569068431854248
Batch 22/64 loss: -0.15307796001434326
Batch 23/64 loss: -0.15245497226715088
Batch 24/64 loss: -0.171586275100708
Batch 25/64 loss: -0.1515215039253235
Batch 26/64 loss: -0.1665867567062378
Batch 27/64 loss: -0.1627497673034668
Batch 28/64 loss: -0.1612567901611328
Batch 29/64 loss: -0.1692185401916504
Batch 30/64 loss: -0.15613877773284912
Batch 31/64 loss: -0.1599634885787964
Batch 32/64 loss: -0.14854228496551514
Batch 33/64 loss: -0.17291146516799927
Batch 34/64 loss: -0.1393038034439087
Batch 35/64 loss: -0.1730060577392578
Batch 36/64 loss: -0.16354495286941528
Batch 37/64 loss: -0.13980168104171753
Batch 38/64 loss: -0.14770686626434326
Batch 39/64 loss: -0.1545962691307068
Batch 40/64 loss: -0.17939281463623047
Batch 41/64 loss: -0.13850921392440796
Batch 42/64 loss: -0.16776037216186523
Batch 43/64 loss: -0.17268794775009155
Batch 44/64 loss: -0.18089234828948975
Batch 45/64 loss: -0.16432273387908936
Batch 46/64 loss: -0.16978740692138672
Batch 47/64 loss: -0.17398416996002197
Batch 48/64 loss: -0.14412707090377808
Batch 49/64 loss: -0.17643356323242188
Batch 50/64 loss: -0.13197851181030273
Batch 51/64 loss: -0.1640615463256836
Batch 52/64 loss: -0.1810740828514099
Batch 53/64 loss: -0.182620108127594
Batch 54/64 loss: -0.17209333181381226
Batch 55/64 loss: -0.15686380863189697
Batch 56/64 loss: -0.15333843231201172
Batch 57/64 loss: -0.16223609447479248
Batch 58/64 loss: -0.16569280624389648
Batch 59/64 loss: -0.16967350244522095
Batch 60/64 loss: -0.15259677171707153
Batch 61/64 loss: -0.1729242205619812
Batch 62/64 loss: -0.14468199014663696
Batch 63/64 loss: -0.16336560249328613
Batch 64/64 loss: -0.17101269960403442
Epoch 46  Train loss: -0.1597201616156335  Val loss: -0.13126817212481678
Saving best model, epoch: 46
Epoch 47
-------------------------------
Batch 1/64 loss: -0.14805370569229126
Batch 2/64 loss: -0.1429535150527954
Batch 3/64 loss: -0.15666663646697998
Batch 4/64 loss: -0.13674426078796387
Batch 5/64 loss: -0.17306309938430786
Batch 6/64 loss: -0.16548019647598267
Batch 7/64 loss: -0.15459507703781128
Batch 8/64 loss: -0.1808624267578125
Batch 9/64 loss: -0.18605923652648926
Batch 10/64 loss: -0.11510509252548218
Batch 11/64 loss: -0.13872677087783813
Batch 12/64 loss: -0.149519145488739
Batch 13/64 loss: -0.17939263582229614
Batch 14/64 loss: -0.16745615005493164
Batch 15/64 loss: -0.17619824409484863
Batch 16/64 loss: -0.16109979152679443
Batch 17/64 loss: -0.16898292303085327
Batch 18/64 loss: -0.15097039937973022
Batch 19/64 loss: -0.18619871139526367
Batch 20/64 loss: -0.16414731740951538
Batch 21/64 loss: -0.1437661051750183
Batch 22/64 loss: -0.17649292945861816
Batch 23/64 loss: -0.16234976053237915
Batch 24/64 loss: -0.1811257004737854
Batch 25/64 loss: -0.15261882543563843
Batch 26/64 loss: -0.17057305574417114
Batch 27/64 loss: -0.15021663904190063
Batch 28/64 loss: -0.14138340950012207
Batch 29/64 loss: -0.1569608449935913
Batch 30/64 loss: -0.16312861442565918
Batch 31/64 loss: -0.16978049278259277
Batch 32/64 loss: -0.16790646314620972
Batch 33/64 loss: -0.15810716152191162
Batch 34/64 loss: -0.15440410375595093
Batch 35/64 loss: -0.15861642360687256
Batch 36/64 loss: -0.16770422458648682
Batch 37/64 loss: -0.1674867868423462
Batch 38/64 loss: -0.15432608127593994
Batch 39/64 loss: -0.17784953117370605
Batch 40/64 loss: -0.16710436344146729
Batch 41/64 loss: -0.17225754261016846
Batch 42/64 loss: -0.17134058475494385
Batch 43/64 loss: -0.16125798225402832
Batch 44/64 loss: -0.16485613584518433
Batch 45/64 loss: -0.17364507913589478
Batch 46/64 loss: -0.15874606370925903
Batch 47/64 loss: -0.16381561756134033
Batch 48/64 loss: -0.1851276159286499
Batch 49/64 loss: -0.13559859991073608
Batch 50/64 loss: -0.14156508445739746
Batch 51/64 loss: -0.16789114475250244
Batch 52/64 loss: -0.17668205499649048
Batch 53/64 loss: -0.1713438630104065
Batch 54/64 loss: -0.12900972366333008
Batch 55/64 loss: -0.16855686902999878
Batch 56/64 loss: -0.1795704960823059
Batch 57/64 loss: -0.1610662341117859
Batch 58/64 loss: -0.163382887840271
Batch 59/64 loss: -0.1515985131263733
Batch 60/64 loss: -0.16965484619140625
Batch 61/64 loss: -0.17065370082855225
Batch 62/64 loss: -0.15114587545394897
Batch 63/64 loss: -0.17199015617370605
Batch 64/64 loss: -0.10674911737442017
Epoch 47  Train loss: -0.1613332605829426  Val loss: -0.11414739479314011
Epoch 48
-------------------------------
Batch 1/64 loss: -0.16598445177078247
Batch 2/64 loss: -0.16784131526947021
Batch 3/64 loss: -0.17035222053527832
Batch 4/64 loss: -0.16842025518417358
Batch 5/64 loss: -0.14338815212249756
Batch 6/64 loss: -0.16512000560760498
Batch 7/64 loss: -0.16803622245788574
Batch 8/64 loss: -0.13944268226623535
Batch 9/64 loss: -0.17133420705795288
Batch 10/64 loss: -0.14747047424316406
Batch 11/64 loss: -0.16516214609146118
Batch 12/64 loss: -0.16451996564865112
Batch 13/64 loss: -0.178122878074646
Batch 14/64 loss: -0.17064052820205688
Batch 15/64 loss: -0.15645891427993774
Batch 16/64 loss: -0.1927608847618103
Batch 17/64 loss: -0.16971993446350098
Batch 18/64 loss: -0.17072468996047974
Batch 19/64 loss: -0.17181819677352905
Batch 20/64 loss: -0.12986457347869873
Batch 21/64 loss: -0.17008477449417114
Batch 22/64 loss: -0.1588597297668457
Batch 23/64 loss: -0.12035709619522095
Batch 24/64 loss: -0.16876542568206787
Batch 25/64 loss: -0.16860949993133545
Batch 26/64 loss: -0.18890917301177979
Batch 27/64 loss: -0.156335711479187
Batch 28/64 loss: -0.1777302622795105
Batch 29/64 loss: -0.16531163454055786
Batch 30/64 loss: -0.17135697603225708
Batch 31/64 loss: -0.16221559047698975
Batch 32/64 loss: -0.15100771188735962
Batch 33/64 loss: -0.15861159563064575
Batch 34/64 loss: -0.16922128200531006
Batch 35/64 loss: -0.16179698705673218
Batch 36/64 loss: -0.14608514308929443
Batch 37/64 loss: -0.17203199863433838
Batch 38/64 loss: -0.13900470733642578
Batch 39/64 loss: -0.1557968258857727
Batch 40/64 loss: -0.17939960956573486
Batch 41/64 loss: -0.13597851991653442
Batch 42/64 loss: -0.15825504064559937
Batch 43/64 loss: -0.1533856987953186
Batch 44/64 loss: -0.14313912391662598
Batch 45/64 loss: -0.17587006092071533
Batch 46/64 loss: -0.15796715021133423
Batch 47/64 loss: -0.17170637845993042
Batch 48/64 loss: -0.16597825288772583
Batch 49/64 loss: -0.18216699361801147
Batch 50/64 loss: -0.17886418104171753
Batch 51/64 loss: -0.15623390674591064
Batch 52/64 loss: -0.156261146068573
Batch 53/64 loss: -0.14362043142318726
Batch 54/64 loss: -0.1356821060180664
Batch 55/64 loss: -0.1644795536994934
Batch 56/64 loss: -0.17491185665130615
Batch 57/64 loss: -0.15834516286849976
Batch 58/64 loss: -0.1615085005760193
Batch 59/64 loss: -0.16175270080566406
Batch 60/64 loss: -0.17661643028259277
Batch 61/64 loss: -0.16084033250808716
Batch 62/64 loss: -0.17323070764541626
Batch 63/64 loss: -0.16030681133270264
Batch 64/64 loss: -0.17601341009140015
Epoch 48  Train loss: -0.1624750672602186  Val loss: -0.1286115113812214
Epoch 49
-------------------------------
Batch 1/64 loss: -0.17595058679580688
Batch 2/64 loss: -0.1524164080619812
Batch 3/64 loss: -0.182284414768219
Batch 4/64 loss: -0.1807725429534912
Batch 5/64 loss: -0.1512250304222107
Batch 6/64 loss: -0.1932544708251953
Batch 7/64 loss: -0.16370362043380737
Batch 8/64 loss: -0.14486241340637207
Batch 9/64 loss: -0.15108203887939453
Batch 10/64 loss: -0.13546651601791382
Batch 11/64 loss: -0.15936636924743652
Batch 12/64 loss: -0.1555236577987671
Batch 13/64 loss: -0.15776079893112183
Batch 14/64 loss: -0.1529686450958252
Batch 15/64 loss: -0.17088395357131958
Batch 16/64 loss: -0.16679775714874268
Batch 17/64 loss: -0.14566856622695923
Batch 18/64 loss: -0.17847657203674316
Batch 19/64 loss: -0.15948712825775146
Batch 20/64 loss: -0.14196985960006714
Batch 21/64 loss: -0.14082551002502441
Batch 22/64 loss: -0.18102812767028809
Batch 23/64 loss: -0.17012649774551392
Batch 24/64 loss: -0.14613991975784302
Batch 25/64 loss: -0.1580866575241089
Batch 26/64 loss: -0.17632943391799927
Batch 27/64 loss: -0.1383812427520752
Batch 28/64 loss: -0.16065597534179688
Batch 29/64 loss: -0.19084417819976807
Batch 30/64 loss: -0.16729497909545898
Batch 31/64 loss: -0.14005929231643677
Batch 32/64 loss: -0.1510189175605774
Batch 33/64 loss: -0.14354193210601807
Batch 34/64 loss: -0.1531694531440735
Batch 35/64 loss: -0.193120539188385
Batch 36/64 loss: -0.13925635814666748
Batch 37/64 loss: -0.1614300012588501
Batch 38/64 loss: -0.16435527801513672
Batch 39/64 loss: -0.1453176736831665
Batch 40/64 loss: -0.16023564338684082
Batch 41/64 loss: -0.17440003156661987
Batch 42/64 loss: -0.16507649421691895
Batch 43/64 loss: -0.14649081230163574
Batch 44/64 loss: -0.16006362438201904
Batch 45/64 loss: -0.16542398929595947
Batch 46/64 loss: -0.17738693952560425
Batch 47/64 loss: -0.15356308221817017
Batch 48/64 loss: -0.1847018003463745
Batch 49/64 loss: -0.14883267879486084
Batch 50/64 loss: -0.18065738677978516
Batch 51/64 loss: -0.17018777132034302
Batch 52/64 loss: -0.16809314489364624
Batch 53/64 loss: -0.16869515180587769
Batch 54/64 loss: -0.15620684623718262
Batch 55/64 loss: -0.19439882040023804
Batch 56/64 loss: -0.17085164785385132
Batch 57/64 loss: -0.15077364444732666
Batch 58/64 loss: -0.16678857803344727
Batch 59/64 loss: -0.1752924919128418
Batch 60/64 loss: -0.1906050443649292
Batch 61/64 loss: -0.1542237401008606
Batch 62/64 loss: -0.16031092405319214
Batch 63/64 loss: -0.16114264726638794
Batch 64/64 loss: -0.16161054372787476
Epoch 49  Train loss: -0.16261198684280995  Val loss: -0.12341039225817546
Epoch 50
-------------------------------
Batch 1/64 loss: -0.14023983478546143
Batch 2/64 loss: -0.1568097472190857
Batch 3/64 loss: -0.1518288254737854
Batch 4/64 loss: -0.15906864404678345
Batch 5/64 loss: -0.16908657550811768
Batch 6/64 loss: -0.17721980810165405
Batch 7/64 loss: -0.16556334495544434
Batch 8/64 loss: -0.16434425115585327
Batch 9/64 loss: -0.16911989450454712
Batch 10/64 loss: -0.1564183235168457
Batch 11/64 loss: -0.16385459899902344
Batch 12/64 loss: -0.18118804693222046
Batch 13/64 loss: -0.1580914855003357
Batch 14/64 loss: -0.16633355617523193
Batch 15/64 loss: -0.16579413414001465
Batch 16/64 loss: -0.15948385000228882
Batch 17/64 loss: -0.16019552946090698
Batch 18/64 loss: -0.17134946584701538
Batch 19/64 loss: -0.18258917331695557
Batch 20/64 loss: -0.14082801342010498
Batch 21/64 loss: -0.1503123641014099
Batch 22/64 loss: -0.1506853699684143
Batch 23/64 loss: -0.16376948356628418
Batch 24/64 loss: -0.17389100790023804
Batch 25/64 loss: -0.16567468643188477
Batch 26/64 loss: -0.16249656677246094
Batch 27/64 loss: -0.14468592405319214
Batch 28/64 loss: -0.18072569370269775
Batch 29/64 loss: -0.15154403448104858
Batch 30/64 loss: -0.1747124195098877
Batch 31/64 loss: -0.16301226615905762
Batch 32/64 loss: -0.1725367307662964
Batch 33/64 loss: -0.16767871379852295
Batch 34/64 loss: -0.1688464879989624
Batch 35/64 loss: -0.15874743461608887
Batch 36/64 loss: -0.15201544761657715
Batch 37/64 loss: -0.14156097173690796
Batch 38/64 loss: -0.15414130687713623
Batch 39/64 loss: -0.16239029169082642
Batch 40/64 loss: -0.16723817586898804
Batch 41/64 loss: -0.15047836303710938
Batch 42/64 loss: -0.14881110191345215
Batch 43/64 loss: -0.164273202419281
Batch 44/64 loss: -0.15928977727890015
Batch 45/64 loss: -0.17031311988830566
Batch 46/64 loss: -0.16964632272720337
Batch 47/64 loss: -0.1648229956626892
Batch 48/64 loss: -0.17204737663269043
Batch 49/64 loss: -0.19136428833007812
Batch 50/64 loss: -0.17374426126480103
Batch 51/64 loss: -0.17617791891098022
Batch 52/64 loss: -0.15869563817977905
Batch 53/64 loss: -0.14868515729904175
Batch 54/64 loss: -0.15738976001739502
Batch 55/64 loss: -0.1675623655319214
Batch 56/64 loss: -0.16028785705566406
Batch 57/64 loss: -0.17794889211654663
Batch 58/64 loss: -0.175132155418396
Batch 59/64 loss: -0.17622160911560059
Batch 60/64 loss: -0.15023106336593628
Batch 61/64 loss: -0.18729394674301147
Batch 62/64 loss: -0.15383613109588623
Batch 63/64 loss: -0.1607767939567566
Batch 64/64 loss: -0.16541600227355957
Epoch 50  Train loss: -0.1635319934171789  Val loss: -0.13744344907937592
Saving best model, epoch: 50
Epoch 51
-------------------------------
Batch 1/64 loss: -0.15756148099899292
Batch 2/64 loss: -0.16849225759506226
Batch 3/64 loss: -0.17766225337982178
Batch 4/64 loss: -0.16422945261001587
Batch 5/64 loss: -0.1788657307624817
Batch 6/64 loss: -0.15911030769348145
Batch 7/64 loss: -0.1684839129447937
Batch 8/64 loss: -0.16358870267868042
Batch 9/64 loss: -0.17175912857055664
Batch 10/64 loss: -0.14360201358795166
Batch 11/64 loss: -0.15318715572357178
Batch 12/64 loss: -0.16298234462738037
Batch 13/64 loss: -0.1442546844482422
Batch 14/64 loss: -0.1716824769973755
Batch 15/64 loss: -0.19587290287017822
Batch 16/64 loss: -0.15501952171325684
Batch 17/64 loss: -0.12986207008361816
Batch 18/64 loss: -0.15838724374771118
Batch 19/64 loss: -0.16799920797348022
Batch 20/64 loss: -0.16978859901428223
Batch 21/64 loss: -0.1713981032371521
Batch 22/64 loss: -0.1585216522216797
Batch 23/64 loss: -0.1582951545715332
Batch 24/64 loss: -0.1544567346572876
Batch 25/64 loss: -0.14328104257583618
Batch 26/64 loss: -0.1860339641571045
Batch 27/64 loss: -0.1477997899055481
Batch 28/64 loss: -0.17187082767486572
Batch 29/64 loss: -0.14644724130630493
Batch 30/64 loss: -0.16995060443878174
Batch 31/64 loss: -0.17520540952682495
Batch 32/64 loss: -0.17378497123718262
Batch 33/64 loss: -0.16536366939544678
Batch 34/64 loss: -0.17077642679214478
Batch 35/64 loss: -0.16085785627365112
Batch 36/64 loss: -0.1427655816078186
Batch 37/64 loss: -0.1440322995185852
Batch 38/64 loss: -0.174821138381958
Batch 39/64 loss: -0.16012883186340332
Batch 40/64 loss: -0.16487634181976318
Batch 41/64 loss: -0.1565287709236145
Batch 42/64 loss: -0.18663984537124634
Batch 43/64 loss: -0.14296209812164307
Batch 44/64 loss: -0.16820555925369263
Batch 45/64 loss: -0.11250805854797363
Batch 46/64 loss: -0.15645134449005127
Batch 47/64 loss: -0.14890849590301514
Batch 48/64 loss: -0.16779041290283203
Batch 49/64 loss: -0.1604071855545044
Batch 50/64 loss: -0.17603325843811035
Batch 51/64 loss: -0.1708238124847412
Batch 52/64 loss: -0.1504892110824585
Batch 53/64 loss: -0.17396676540374756
Batch 54/64 loss: -0.1650170087814331
Batch 55/64 loss: -0.15295928716659546
Batch 56/64 loss: -0.16799914836883545
Batch 57/64 loss: -0.15851974487304688
Batch 58/64 loss: -0.13041508197784424
Batch 59/64 loss: -0.14713150262832642
Batch 60/64 loss: -0.1688227653503418
Batch 61/64 loss: -0.18298321962356567
Batch 62/64 loss: -0.18699562549591064
Batch 63/64 loss: -0.16805320978164673
Batch 64/64 loss: -0.18411743640899658
Epoch 51  Train loss: -0.1622236639845605  Val loss: -0.13336699443174801
Epoch 52
-------------------------------
Batch 1/64 loss: -0.1560056209564209
Batch 2/64 loss: -0.1769121289253235
Batch 3/64 loss: -0.16925299167633057
Batch 4/64 loss: -0.1595866084098816
Batch 5/64 loss: -0.1850874423980713
Batch 6/64 loss: -0.15590506792068481
Batch 7/64 loss: -0.1716265082359314
Batch 8/64 loss: -0.16090339422225952
Batch 9/64 loss: -0.17804193496704102
Batch 10/64 loss: -0.16869843006134033
Batch 11/64 loss: -0.16665875911712646
Batch 12/64 loss: -0.172926664352417
Batch 13/64 loss: -0.17084848880767822
Batch 14/64 loss: -0.17034167051315308
Batch 15/64 loss: -0.1479048728942871
Batch 16/64 loss: -0.17151808738708496
Batch 17/64 loss: -0.15998578071594238
Batch 18/64 loss: -0.14735984802246094
Batch 19/64 loss: -0.15188288688659668
Batch 20/64 loss: -0.18043065071105957
Batch 21/64 loss: -0.15973228216171265
Batch 22/64 loss: -0.1715564727783203
Batch 23/64 loss: -0.17358767986297607
Batch 24/64 loss: -0.1778775453567505
Batch 25/64 loss: -0.13911134004592896
Batch 26/64 loss: -0.16604948043823242
Batch 27/64 loss: -0.15635710954666138
Batch 28/64 loss: -0.18520230054855347
Batch 29/64 loss: -0.17707204818725586
Batch 30/64 loss: -0.17867004871368408
Batch 31/64 loss: -0.1739060878753662
Batch 32/64 loss: -0.17490392923355103
Batch 33/64 loss: -0.17923128604888916
Batch 34/64 loss: -0.12950509786605835
Batch 35/64 loss: -0.14653372764587402
Batch 36/64 loss: -0.15490466356277466
Batch 37/64 loss: -0.17345267534255981
Batch 38/64 loss: -0.1660786271095276
Batch 39/64 loss: -0.16135871410369873
Batch 40/64 loss: -0.1647486686706543
Batch 41/64 loss: -0.18760180473327637
Batch 42/64 loss: -0.15840208530426025
Batch 43/64 loss: -0.1383906602859497
Batch 44/64 loss: -0.15458565950393677
Batch 45/64 loss: -0.1782820224761963
Batch 46/64 loss: -0.17537736892700195
Batch 47/64 loss: -0.16815614700317383
Batch 48/64 loss: -0.1634150743484497
Batch 49/64 loss: -0.17218202352523804
Batch 50/64 loss: -0.16547667980194092
Batch 51/64 loss: -0.165793776512146
Batch 52/64 loss: -0.15622073411941528
Batch 53/64 loss: -0.1679762601852417
Batch 54/64 loss: -0.18752574920654297
Batch 55/64 loss: -0.16991931200027466
Batch 56/64 loss: -0.1696988344192505
Batch 57/64 loss: -0.16889292001724243
Batch 58/64 loss: -0.1933932900428772
Batch 59/64 loss: -0.19499576091766357
Batch 60/64 loss: -0.16977989673614502
Batch 61/64 loss: -0.1726083755493164
Batch 62/64 loss: -0.14708834886550903
Batch 63/64 loss: -0.16857284307479858
Batch 64/64 loss: -0.1876128911972046
Epoch 52  Train loss: -0.16732177126641368  Val loss: -0.1340330851446722
Epoch 53
-------------------------------
Batch 1/64 loss: -0.1781773567199707
Batch 2/64 loss: -0.18166077136993408
Batch 3/64 loss: -0.17136085033416748
Batch 4/64 loss: -0.18350672721862793
Batch 5/64 loss: -0.16016024351119995
Batch 6/64 loss: -0.1613917350769043
Batch 7/64 loss: -0.14724862575531006
Batch 8/64 loss: -0.17487967014312744
Batch 9/64 loss: -0.18413758277893066
Batch 10/64 loss: -0.19395500421524048
Batch 11/64 loss: -0.1702500581741333
Batch 12/64 loss: -0.16811597347259521
Batch 13/64 loss: -0.16029763221740723
Batch 14/64 loss: -0.1469840407371521
Batch 15/64 loss: -0.1631394624710083
Batch 16/64 loss: -0.18582868576049805
Batch 17/64 loss: -0.13624078035354614
Batch 18/64 loss: -0.17302435636520386
Batch 19/64 loss: -0.15452539920806885
Batch 20/64 loss: -0.173409104347229
Batch 21/64 loss: -0.1539859175682068
Batch 22/64 loss: -0.15459519624710083
Batch 23/64 loss: -0.1873754858970642
Batch 24/64 loss: -0.17629265785217285
Batch 25/64 loss: -0.15421587228775024
Batch 26/64 loss: -0.1507524847984314
Batch 27/64 loss: -0.17626118659973145
Batch 28/64 loss: -0.1890992522239685
Batch 29/64 loss: -0.1970447301864624
Batch 30/64 loss: -0.17713075876235962
Batch 31/64 loss: -0.1815686821937561
Batch 32/64 loss: -0.17201435565948486
Batch 33/64 loss: -0.17090260982513428
Batch 34/64 loss: -0.17432254552841187
Batch 35/64 loss: -0.16163939237594604
Batch 36/64 loss: -0.1615283489227295
Batch 37/64 loss: -0.18122833967208862
Batch 38/64 loss: -0.16577357053756714
Batch 39/64 loss: -0.17477315664291382
Batch 40/64 loss: -0.17188912630081177
Batch 41/64 loss: -0.1674187183380127
Batch 42/64 loss: -0.14445364475250244
Batch 43/64 loss: -0.16427791118621826
Batch 44/64 loss: -0.18525946140289307
Batch 45/64 loss: -0.15696346759796143
Batch 46/64 loss: -0.15269720554351807
Batch 47/64 loss: -0.14782118797302246
Batch 48/64 loss: -0.17842596769332886
Batch 49/64 loss: -0.18606406450271606
Batch 50/64 loss: -0.1560552716255188
Batch 51/64 loss: -0.150496244430542
Batch 52/64 loss: -0.17731428146362305
Batch 53/64 loss: -0.17655009031295776
Batch 54/64 loss: -0.1632368564605713
Batch 55/64 loss: -0.1849791407585144
Batch 56/64 loss: -0.1504662036895752
Batch 57/64 loss: -0.1739499568939209
Batch 58/64 loss: -0.16637742519378662
Batch 59/64 loss: -0.16574829816818237
Batch 60/64 loss: -0.14863508939743042
Batch 61/64 loss: -0.16914981603622437
Batch 62/64 loss: -0.16572237014770508
Batch 63/64 loss: -0.18220603466033936
Batch 64/64 loss: -0.1676785945892334
Epoch 53  Train loss: -0.1684818099526798  Val loss: -0.13479916234196665
Epoch 54
-------------------------------
Batch 1/64 loss: -0.17793315649032593
Batch 2/64 loss: -0.17009389400482178
Batch 3/64 loss: -0.1617581844329834
Batch 4/64 loss: -0.16037112474441528
Batch 5/64 loss: -0.16715508699417114
Batch 6/64 loss: -0.1619734764099121
Batch 7/64 loss: -0.17667347192764282
Batch 8/64 loss: -0.17471694946289062
Batch 9/64 loss: -0.19270992279052734
Batch 10/64 loss: -0.174169659614563
Batch 11/64 loss: -0.1462310552597046
Batch 12/64 loss: -0.14815783500671387
Batch 13/64 loss: -0.1678452491760254
Batch 14/64 loss: -0.17841875553131104
Batch 15/64 loss: -0.16971862316131592
Batch 16/64 loss: -0.18327295780181885
Batch 17/64 loss: -0.19596946239471436
Batch 18/64 loss: -0.17884236574172974
Batch 19/64 loss: -0.14319199323654175
Batch 20/64 loss: -0.166606605052948
Batch 21/64 loss: -0.1829434037208557
Batch 22/64 loss: -0.17096030712127686
Batch 23/64 loss: -0.16346842050552368
Batch 24/64 loss: -0.18424707651138306
Batch 25/64 loss: -0.17066431045532227
Batch 26/64 loss: -0.17039036750793457
Batch 27/64 loss: -0.1642686128616333
Batch 28/64 loss: -0.15490597486495972
Batch 29/64 loss: -0.17431795597076416
Batch 30/64 loss: -0.179979145526886
Batch 31/64 loss: -0.18719452619552612
Batch 32/64 loss: -0.16004377603530884
Batch 33/64 loss: -0.19067561626434326
Batch 34/64 loss: -0.17207622528076172
Batch 35/64 loss: -0.1759849190711975
Batch 36/64 loss: -0.14827418327331543
Batch 37/64 loss: -0.18117743730545044
Batch 38/64 loss: -0.15246719121932983
Batch 39/64 loss: -0.16135621070861816
Batch 40/64 loss: -0.17073875665664673
Batch 41/64 loss: -0.17271029949188232
Batch 42/64 loss: -0.18295443058013916
Batch 43/64 loss: -0.16750848293304443
Batch 44/64 loss: -0.14738374948501587
Batch 45/64 loss: -0.15260159969329834
Batch 46/64 loss: -0.17242282629013062
Batch 47/64 loss: -0.18323737382888794
Batch 48/64 loss: -0.162187397480011
Batch 49/64 loss: -0.19600754976272583
Batch 50/64 loss: -0.1847522258758545
Batch 51/64 loss: -0.18164515495300293
Batch 52/64 loss: -0.17722105979919434
Batch 53/64 loss: -0.16281133890151978
Batch 54/64 loss: -0.17362940311431885
Batch 55/64 loss: -0.18975603580474854
Batch 56/64 loss: -0.1739540696144104
Batch 57/64 loss: -0.14662694931030273
Batch 58/64 loss: -0.17688977718353271
Batch 59/64 loss: -0.15752792358398438
Batch 60/64 loss: -0.1800137162208557
Batch 61/64 loss: -0.1698865294456482
Batch 62/64 loss: -0.17237651348114014
Batch 63/64 loss: -0.14179331064224243
Batch 64/64 loss: -0.16634124517440796
Epoch 54  Train loss: -0.1703936925121382  Val loss: -0.1210711721292476
Epoch 55
-------------------------------
Batch 1/64 loss: -0.17357707023620605
Batch 2/64 loss: -0.155927836894989
Batch 3/64 loss: -0.16481411457061768
Batch 4/64 loss: -0.18417143821716309
Batch 5/64 loss: -0.17332333326339722
Batch 6/64 loss: -0.15990608930587769
Batch 7/64 loss: -0.15694177150726318
Batch 8/64 loss: -0.1849387288093567
Batch 9/64 loss: -0.16920089721679688
Batch 10/64 loss: -0.17059874534606934
Batch 11/64 loss: -0.17081016302108765
Batch 12/64 loss: -0.1899477243423462
Batch 13/64 loss: -0.15428996086120605
Batch 14/64 loss: -0.1748294234275818
Batch 15/64 loss: -0.18947887420654297
Batch 16/64 loss: -0.1910945177078247
Batch 17/64 loss: -0.1840844750404358
Batch 18/64 loss: -0.1755378246307373
Batch 19/64 loss: -0.1746290922164917
Batch 20/64 loss: -0.18521130084991455
Batch 21/64 loss: -0.17284399271011353
Batch 22/64 loss: -0.15422087907791138
Batch 23/64 loss: -0.18493223190307617
Batch 24/64 loss: -0.1380150318145752
Batch 25/64 loss: -0.1600247025489807
Batch 26/64 loss: -0.15531641244888306
Batch 27/64 loss: -0.17010420560836792
Batch 28/64 loss: -0.17091935873031616
Batch 29/64 loss: -0.17144787311553955
Batch 30/64 loss: -0.17210644483566284
Batch 31/64 loss: -0.1466427445411682
Batch 32/64 loss: -0.15600508451461792
Batch 33/64 loss: -0.1671849489212036
Batch 34/64 loss: -0.12977492809295654
Batch 35/64 loss: -0.1642066240310669
Batch 36/64 loss: -0.16055214405059814
Batch 37/64 loss: -0.15111231803894043
Batch 38/64 loss: -0.1586781144142151
Batch 39/64 loss: -0.14851850271224976
Batch 40/64 loss: -0.17875123023986816
Batch 41/64 loss: -0.1629052758216858
Batch 42/64 loss: -0.16323834657669067
Batch 43/64 loss: -0.14992988109588623
Batch 44/64 loss: -0.17597293853759766
Batch 45/64 loss: -0.17414313554763794
Batch 46/64 loss: -0.1283886432647705
Batch 47/64 loss: -0.16052192449569702
Batch 48/64 loss: -0.16515880823135376
Batch 49/64 loss: -0.17741841077804565
Batch 50/64 loss: -0.18589091300964355
Batch 51/64 loss: -0.16693735122680664
Batch 52/64 loss: -0.15096890926361084
Batch 53/64 loss: -0.17682451009750366
Batch 54/64 loss: -0.1829547882080078
Batch 55/64 loss: -0.16254717111587524
Batch 56/64 loss: -0.18194985389709473
Batch 57/64 loss: -0.18506765365600586
Batch 58/64 loss: -0.17710250616073608
Batch 59/64 loss: -0.152568519115448
Batch 60/64 loss: -0.17453312873840332
Batch 61/64 loss: -0.1706797480583191
Batch 62/64 loss: -0.15803009271621704
Batch 63/64 loss: -0.16996198892593384
Batch 64/64 loss: -0.16014182567596436
Epoch 55  Train loss: -0.16734858073440254  Val loss: -0.13344838692969882
Epoch 56
-------------------------------
Batch 1/64 loss: -0.15487748384475708
Batch 2/64 loss: -0.18172186613082886
Batch 3/64 loss: -0.13410133123397827
Batch 4/64 loss: -0.15594512224197388
Batch 5/64 loss: -0.16852807998657227
Batch 6/64 loss: -0.1690363883972168
Batch 7/64 loss: -0.16993361711502075
Batch 8/64 loss: -0.1629272699356079
Batch 9/64 loss: -0.17074382305145264
Batch 10/64 loss: -0.1576632261276245
Batch 11/64 loss: -0.17617541551589966
Batch 12/64 loss: -0.1555463671684265
Batch 13/64 loss: -0.17807292938232422
Batch 14/64 loss: -0.17811352014541626
Batch 15/64 loss: -0.14996755123138428
Batch 16/64 loss: -0.17640912532806396
Batch 17/64 loss: -0.16345804929733276
Batch 18/64 loss: -0.1648450493812561
Batch 19/64 loss: -0.1500040888786316
Batch 20/64 loss: -0.18311768770217896
Batch 21/64 loss: -0.18206000328063965
Batch 22/64 loss: -0.16019195318222046
Batch 23/64 loss: -0.1740904450416565
Batch 24/64 loss: -0.1667541265487671
Batch 25/64 loss: -0.17271900177001953
Batch 26/64 loss: -0.17817598581314087
Batch 27/64 loss: -0.17061352729797363
Batch 28/64 loss: -0.17436885833740234
Batch 29/64 loss: -0.1717061996459961
Batch 30/64 loss: -0.1805201768875122
Batch 31/64 loss: -0.17447513341903687
Batch 32/64 loss: -0.16915106773376465
Batch 33/64 loss: -0.17445135116577148
Batch 34/64 loss: -0.18792861700057983
Batch 35/64 loss: -0.18238258361816406
Batch 36/64 loss: -0.15603238344192505
Batch 37/64 loss: -0.1574224829673767
Batch 38/64 loss: -0.16959220170974731
Batch 39/64 loss: -0.15244543552398682
Batch 40/64 loss: -0.1618747115135193
Batch 41/64 loss: -0.16495347023010254
Batch 42/64 loss: -0.16839265823364258
Batch 43/64 loss: -0.1826450228691101
Batch 44/64 loss: -0.16222697496414185
Batch 45/64 loss: -0.18209099769592285
Batch 46/64 loss: -0.18581247329711914
Batch 47/64 loss: -0.17484909296035767
Batch 48/64 loss: -0.17206859588623047
Batch 49/64 loss: -0.184198260307312
Batch 50/64 loss: -0.17439305782318115
Batch 51/64 loss: -0.17273485660552979
Batch 52/64 loss: -0.18060094118118286
Batch 53/64 loss: -0.1630854606628418
Batch 54/64 loss: -0.17912739515304565
Batch 55/64 loss: -0.18046706914901733
Batch 56/64 loss: -0.15473514795303345
Batch 57/64 loss: -0.14977014064788818
Batch 58/64 loss: -0.16569089889526367
Batch 59/64 loss: -0.16902416944503784
Batch 60/64 loss: -0.17268776893615723
Batch 61/64 loss: -0.174585223197937
Batch 62/64 loss: -0.1574133038520813
Batch 63/64 loss: -0.16385996341705322
Batch 64/64 loss: -0.1873731017112732
Epoch 56  Train loss: -0.16931901188457713  Val loss: -0.13291921423063247
Epoch 57
-------------------------------
Batch 1/64 loss: -0.1817786693572998
Batch 2/64 loss: -0.16590356826782227
Batch 3/64 loss: -0.13449853658676147
Batch 4/64 loss: -0.16558563709259033
Batch 5/64 loss: -0.1558464765548706
Batch 6/64 loss: -0.1669725775718689
Batch 7/64 loss: -0.1688464879989624
Batch 8/64 loss: -0.19276964664459229
Batch 9/64 loss: -0.17042279243469238
Batch 10/64 loss: -0.17414963245391846
Batch 11/64 loss: -0.16697227954864502
Batch 12/64 loss: -0.15944623947143555
Batch 13/64 loss: -0.18061363697052002
Batch 14/64 loss: -0.1796913743019104
Batch 15/64 loss: -0.14664745330810547
Batch 16/64 loss: -0.16382288932800293
Batch 17/64 loss: -0.1514783501625061
Batch 18/64 loss: -0.17772167921066284
Batch 19/64 loss: -0.16374272108078003
Batch 20/64 loss: -0.1612910032272339
Batch 21/64 loss: -0.19129443168640137
Batch 22/64 loss: -0.13779979944229126
Batch 23/64 loss: -0.17835962772369385
Batch 24/64 loss: -0.16443604230880737
Batch 25/64 loss: -0.16972529888153076
Batch 26/64 loss: -0.15432626008987427
Batch 27/64 loss: -0.17764729261398315
Batch 28/64 loss: -0.19354665279388428
Batch 29/64 loss: -0.18634182214736938
Batch 30/64 loss: -0.17366182804107666
Batch 31/64 loss: -0.15978366136550903
Batch 32/64 loss: -0.1478327512741089
Batch 33/64 loss: -0.14735478162765503
Batch 34/64 loss: -0.16425883769989014
Batch 35/64 loss: -0.12737709283828735
Batch 36/64 loss: -0.16024738550186157
Batch 37/64 loss: -0.156602680683136
Batch 38/64 loss: -0.1856621503829956
Batch 39/64 loss: -0.14906668663024902
Batch 40/64 loss: -0.17236852645874023
Batch 41/64 loss: -0.17075693607330322
Batch 42/64 loss: -0.16369444131851196
Batch 43/64 loss: -0.17748457193374634
Batch 44/64 loss: -0.16198033094406128
Batch 45/64 loss: -0.16481173038482666
Batch 46/64 loss: -0.1756502389907837
Batch 47/64 loss: -0.15328800678253174
Batch 48/64 loss: -0.16738682985305786
Batch 49/64 loss: -0.14519226551055908
Batch 50/64 loss: -0.14516723155975342
Batch 51/64 loss: -0.1691843867301941
Batch 52/64 loss: -0.17520582675933838
Batch 53/64 loss: -0.1813996434211731
Batch 54/64 loss: -0.16960793733596802
Batch 55/64 loss: -0.15874338150024414
Batch 56/64 loss: -0.15135729312896729
Batch 57/64 loss: -0.169550359249115
Batch 58/64 loss: -0.1571727991104126
Batch 59/64 loss: -0.17258518934249878
Batch 60/64 loss: -0.1552436351776123
Batch 61/64 loss: -0.18215632438659668
Batch 62/64 loss: -0.15862512588500977
Batch 63/64 loss: -0.17428886890411377
Batch 64/64 loss: -0.1649075746536255
Epoch 57  Train loss: -0.16549191054175882  Val loss: -0.12901176189638905
Epoch 58
-------------------------------
Batch 1/64 loss: -0.15083104372024536
Batch 2/64 loss: -0.1698981523513794
Batch 3/64 loss: -0.1605370044708252
Batch 4/64 loss: -0.18268167972564697
Batch 5/64 loss: -0.16560786962509155
Batch 6/64 loss: -0.1387670636177063
Batch 7/64 loss: -0.16781848669052124
Batch 8/64 loss: -0.16686135530471802
Batch 9/64 loss: -0.17093932628631592
Batch 10/64 loss: -0.16021162271499634
Batch 11/64 loss: -0.1753285527229309
Batch 12/64 loss: -0.1781674027442932
Batch 13/64 loss: -0.17404943704605103
Batch 14/64 loss: -0.16877156496047974
Batch 15/64 loss: -0.1776270866394043
Batch 16/64 loss: -0.11228233575820923
Batch 17/64 loss: -0.17932987213134766
Batch 18/64 loss: -0.14944475889205933
Batch 19/64 loss: -0.16928303241729736
Batch 20/64 loss: -0.17006337642669678
Batch 21/64 loss: -0.16972827911376953
Batch 22/64 loss: -0.16942447423934937
Batch 23/64 loss: -0.166296124458313
Batch 24/64 loss: -0.16795647144317627
Batch 25/64 loss: -0.16595572233200073
Batch 26/64 loss: -0.18469005823135376
Batch 27/64 loss: -0.17282772064208984
Batch 28/64 loss: -0.1467035412788391
Batch 29/64 loss: -0.18590641021728516
Batch 30/64 loss: -0.16608989238739014
Batch 31/64 loss: -0.13861751556396484
Batch 32/64 loss: -0.17203229665756226
Batch 33/64 loss: -0.15995317697525024
Batch 34/64 loss: -0.17549479007720947
Batch 35/64 loss: -0.15596526861190796
Batch 36/64 loss: -0.15939247608184814
Batch 37/64 loss: -0.18804699182510376
Batch 38/64 loss: -0.1598910093307495
Batch 39/64 loss: -0.18141454458236694
Batch 40/64 loss: -0.1718117594718933
Batch 41/64 loss: -0.14646172523498535
Batch 42/64 loss: -0.17928951978683472
Batch 43/64 loss: -0.16413533687591553
Batch 44/64 loss: -0.17949455976486206
Batch 45/64 loss: -0.1758897304534912
Batch 46/64 loss: -0.18928253650665283
Batch 47/64 loss: -0.18743109703063965
Batch 48/64 loss: -0.1738377809524536
Batch 49/64 loss: -0.15042322874069214
Batch 50/64 loss: -0.1463075876235962
Batch 51/64 loss: -0.17276233434677124
Batch 52/64 loss: -0.14800786972045898
Batch 53/64 loss: -0.17298609018325806
Batch 54/64 loss: -0.16756975650787354
Batch 55/64 loss: -0.1947920322418213
Batch 56/64 loss: -0.19635009765625
Batch 57/64 loss: -0.18871384859085083
Batch 58/64 loss: -0.18472075462341309
Batch 59/64 loss: -0.17127180099487305
Batch 60/64 loss: -0.17474544048309326
Batch 61/64 loss: -0.1801382303237915
Batch 62/64 loss: -0.18666613101959229
Batch 63/64 loss: -0.17483079433441162
Batch 64/64 loss: -0.20639652013778687
Epoch 58  Train loss: -0.16953106230380488  Val loss: -0.13741126420981287
Epoch 59
-------------------------------
Batch 1/64 loss: -0.19171428680419922
Batch 2/64 loss: -0.17680776119232178
Batch 3/64 loss: -0.15730822086334229
Batch 4/64 loss: -0.1593639850616455
Batch 5/64 loss: -0.17373019456863403
Batch 6/64 loss: -0.17679595947265625
Batch 7/64 loss: -0.17198306322097778
Batch 8/64 loss: -0.14817774295806885
Batch 9/64 loss: -0.16352635622024536
Batch 10/64 loss: -0.17944324016571045
Batch 11/64 loss: -0.17742455005645752
Batch 12/64 loss: -0.16639047861099243
Batch 13/64 loss: -0.16922402381896973
Batch 14/64 loss: -0.16645491123199463
Batch 15/64 loss: -0.16391485929489136
Batch 16/64 loss: -0.1645667552947998
Batch 17/64 loss: -0.1655634641647339
Batch 18/64 loss: -0.18024492263793945
Batch 19/64 loss: -0.1589394211769104
Batch 20/64 loss: -0.16014337539672852
Batch 21/64 loss: -0.1656232476234436
Batch 22/64 loss: -0.17264723777770996
Batch 23/64 loss: -0.17861735820770264
Batch 24/64 loss: -0.1453263759613037
Batch 25/64 loss: -0.1799747347831726
Batch 26/64 loss: -0.16859740018844604
Batch 27/64 loss: -0.17687088251113892
Batch 28/64 loss: -0.16114753484725952
Batch 29/64 loss: -0.18466615676879883
Batch 30/64 loss: -0.15774625539779663
Batch 31/64 loss: -0.18233293294906616
Batch 32/64 loss: -0.17114269733428955
Batch 33/64 loss: -0.17554157972335815
Batch 34/64 loss: -0.1625012755393982
Batch 35/64 loss: -0.1842327117919922
Batch 36/64 loss: -0.17985689640045166
Batch 37/64 loss: -0.17152196168899536
Batch 38/64 loss: -0.17964094877243042
Batch 39/64 loss: -0.17430007457733154
Batch 40/64 loss: -0.17927610874176025
Batch 41/64 loss: -0.15598946809768677
Batch 42/64 loss: -0.16871291399002075
Batch 43/64 loss: -0.17183315753936768
Batch 44/64 loss: -0.18254804611206055
Batch 45/64 loss: -0.1971501111984253
Batch 46/64 loss: -0.17836534976959229
Batch 47/64 loss: -0.17645037174224854
Batch 48/64 loss: -0.17805618047714233
Batch 49/64 loss: -0.1421276330947876
Batch 50/64 loss: -0.15175247192382812
Batch 51/64 loss: -0.18088281154632568
Batch 52/64 loss: -0.18326407670974731
Batch 53/64 loss: -0.15960347652435303
Batch 54/64 loss: -0.18544191122055054
Batch 55/64 loss: -0.1782628893852234
Batch 56/64 loss: -0.17769628763198853
Batch 57/64 loss: -0.16613107919692993
Batch 58/64 loss: -0.18512439727783203
Batch 59/64 loss: -0.18350297212600708
Batch 60/64 loss: -0.1851556897163391
Batch 61/64 loss: -0.19810116291046143
Batch 62/64 loss: -0.169061541557312
Batch 63/64 loss: -0.17558056116104126
Batch 64/64 loss: -0.1844194531440735
Epoch 59  Train loss: -0.17242966422847672  Val loss: -0.1306938184905298
Epoch 60
-------------------------------
Batch 1/64 loss: -0.15873348712921143
Batch 2/64 loss: -0.15100997686386108
Batch 3/64 loss: -0.17962968349456787
Batch 4/64 loss: -0.1900026798248291
Batch 5/64 loss: -0.16582274436950684
Batch 6/64 loss: -0.18933629989624023
Batch 7/64 loss: -0.16063475608825684
Batch 8/64 loss: -0.18589752912521362
Batch 9/64 loss: -0.16846764087677002
Batch 10/64 loss: -0.19146591424942017
Batch 11/64 loss: -0.17056196928024292
Batch 12/64 loss: -0.1975829005241394
Batch 13/64 loss: -0.182864248752594
Batch 14/64 loss: -0.20735061168670654
Batch 15/64 loss: -0.17131710052490234
Batch 16/64 loss: -0.19029873609542847
Batch 17/64 loss: -0.18784970045089722
Batch 18/64 loss: -0.1761026382446289
Batch 19/64 loss: -0.17775297164916992
Batch 20/64 loss: -0.19045734405517578
Batch 21/64 loss: -0.1795433759689331
Batch 22/64 loss: -0.18002665042877197
Batch 23/64 loss: -0.16877931356430054
Batch 24/64 loss: -0.1908046007156372
Batch 25/64 loss: -0.15814852714538574
Batch 26/64 loss: -0.14933550357818604
Batch 27/64 loss: -0.18433833122253418
Batch 28/64 loss: -0.1769484281539917
Batch 29/64 loss: -0.17275649309158325
Batch 30/64 loss: -0.17969465255737305
Batch 31/64 loss: -0.17030572891235352
Batch 32/64 loss: -0.15215849876403809
Batch 33/64 loss: -0.15810710191726685
Batch 34/64 loss: -0.15823572874069214
Batch 35/64 loss: -0.16712063550949097
Batch 36/64 loss: -0.18873310089111328
Batch 37/64 loss: -0.17291337251663208
Batch 38/64 loss: -0.19370341300964355
Batch 39/64 loss: -0.17412418127059937
Batch 40/64 loss: -0.15714794397354126
Batch 41/64 loss: -0.19120997190475464
Batch 42/64 loss: -0.18814325332641602
Batch 43/64 loss: -0.1597362756729126
Batch 44/64 loss: -0.15113574266433716
Batch 45/64 loss: -0.15346604585647583
Batch 46/64 loss: -0.14646488428115845
Batch 47/64 loss: -0.1659715175628662
Batch 48/64 loss: -0.18176811933517456
Batch 49/64 loss: -0.1659676432609558
Batch 50/64 loss: -0.177440345287323
Batch 51/64 loss: -0.18900573253631592
Batch 52/64 loss: -0.13933944702148438
Batch 53/64 loss: -0.18330281972885132
Batch 54/64 loss: -0.18315988779067993
Batch 55/64 loss: -0.1756901741027832
Batch 56/64 loss: -0.15149492025375366
Batch 57/64 loss: -0.16069167852401733
Batch 58/64 loss: -0.16645389795303345
Batch 59/64 loss: -0.1804085373878479
Batch 60/64 loss: -0.16845548152923584
Batch 61/64 loss: -0.17681878805160522
Batch 62/64 loss: -0.18276703357696533
Batch 63/64 loss: -0.16919881105422974
Batch 64/64 loss: -0.18750381469726562
Epoch 60  Train loss: -0.17372209231058758  Val loss: -0.14481304314537966
Saving best model, epoch: 60
Epoch 61
-------------------------------
Batch 1/64 loss: -0.17138230800628662
Batch 2/64 loss: -0.18251270055770874
Batch 3/64 loss: -0.17506146430969238
Batch 4/64 loss: -0.17103499174118042
Batch 5/64 loss: -0.1663583517074585
Batch 6/64 loss: -0.16806364059448242
Batch 7/64 loss: -0.18725448846817017
Batch 8/64 loss: -0.17565292119979858
Batch 9/64 loss: -0.13929027318954468
Batch 10/64 loss: -0.15523487329483032
Batch 11/64 loss: -0.19251799583435059
Batch 12/64 loss: -0.19269710779190063
Batch 13/64 loss: -0.176751971244812
Batch 14/64 loss: -0.18371593952178955
Batch 15/64 loss: -0.1537952423095703
Batch 16/64 loss: -0.18927901983261108
Batch 17/64 loss: -0.16310685873031616
Batch 18/64 loss: -0.16224533319473267
Batch 19/64 loss: -0.18011397123336792
Batch 20/64 loss: -0.16565752029418945
Batch 21/64 loss: -0.18034660816192627
Batch 22/64 loss: -0.15544146299362183
Batch 23/64 loss: -0.1925748586654663
Batch 24/64 loss: -0.15073800086975098
Batch 25/64 loss: -0.17238152027130127
Batch 26/64 loss: -0.1779981255531311
Batch 27/64 loss: -0.18106645345687866
Batch 28/64 loss: -0.18321716785430908
Batch 29/64 loss: -0.19061458110809326
Batch 30/64 loss: -0.18542426824569702
Batch 31/64 loss: -0.1870514154434204
Batch 32/64 loss: -0.15675699710845947
Batch 33/64 loss: -0.1347641944885254
Batch 34/64 loss: -0.18836909532546997
Batch 35/64 loss: -0.17766624689102173
Batch 36/64 loss: -0.1822819709777832
Batch 37/64 loss: -0.1758265495300293
Batch 38/64 loss: -0.14497476816177368
Batch 39/64 loss: -0.17979609966278076
Batch 40/64 loss: -0.1689671277999878
Batch 41/64 loss: -0.1757335662841797
Batch 42/64 loss: -0.15562641620635986
Batch 43/64 loss: -0.16964536905288696
Batch 44/64 loss: -0.17320877313613892
Batch 45/64 loss: -0.18558549880981445
Batch 46/64 loss: -0.18414193391799927
Batch 47/64 loss: -0.19603562355041504
Batch 48/64 loss: -0.19308888912200928
Batch 49/64 loss: -0.1816973090171814
Batch 50/64 loss: -0.16301167011260986
Batch 51/64 loss: -0.18485385179519653
Batch 52/64 loss: -0.14111965894699097
Batch 53/64 loss: -0.17522317171096802
Batch 54/64 loss: -0.19641703367233276
Batch 55/64 loss: -0.18373775482177734
Batch 56/64 loss: -0.18487930297851562
Batch 57/64 loss: -0.1860663890838623
Batch 58/64 loss: -0.15938055515289307
Batch 59/64 loss: -0.18639010190963745
Batch 60/64 loss: -0.17674481868743896
Batch 61/64 loss: -0.1733904480934143
Batch 62/64 loss: -0.17809438705444336
Batch 63/64 loss: -0.17745059728622437
Batch 64/64 loss: -0.20877790451049805
Epoch 61  Train loss: -0.17499750642215503  Val loss: -0.13680313440532618
Epoch 62
-------------------------------
Batch 1/64 loss: -0.1942136287689209
Batch 2/64 loss: -0.19204235076904297
Batch 3/64 loss: -0.19175738096237183
Batch 4/64 loss: -0.1637207269668579
Batch 5/64 loss: -0.1708897352218628
Batch 6/64 loss: -0.12323731184005737
Batch 7/64 loss: -0.16697001457214355
Batch 8/64 loss: -0.15854400396347046
Batch 9/64 loss: -0.1779313087463379
Batch 10/64 loss: -0.18026942014694214
Batch 11/64 loss: -0.19245219230651855
Batch 12/64 loss: -0.17686855792999268
Batch 13/64 loss: -0.1810297966003418
Batch 14/64 loss: -0.18765908479690552
Batch 15/64 loss: -0.16267138719558716
Batch 16/64 loss: -0.18095779418945312
Batch 17/64 loss: -0.15329229831695557
Batch 18/64 loss: -0.16132867336273193
Batch 19/64 loss: -0.19149982929229736
Batch 20/64 loss: -0.1587005853652954
Batch 21/64 loss: -0.15071165561676025
Batch 22/64 loss: -0.1982109546661377
Batch 23/64 loss: -0.18679851293563843
Batch 24/64 loss: -0.19488471746444702
Batch 25/64 loss: -0.17828452587127686
Batch 26/64 loss: -0.15932416915893555
Batch 27/64 loss: -0.2012811303138733
Batch 28/64 loss: -0.17301642894744873
Batch 29/64 loss: -0.18076658248901367
Batch 30/64 loss: -0.18222320079803467
Batch 31/64 loss: -0.17109441757202148
Batch 32/64 loss: -0.1743113398551941
Batch 33/64 loss: -0.17979127168655396
Batch 34/64 loss: -0.1975482702255249
Batch 35/64 loss: -0.17030620574951172
Batch 36/64 loss: -0.18820184469223022
Batch 37/64 loss: -0.19042491912841797
Batch 38/64 loss: -0.17381179332733154
Batch 39/64 loss: -0.1687946319580078
Batch 40/64 loss: -0.17568808794021606
Batch 41/64 loss: -0.159925639629364
Batch 42/64 loss: -0.18894994258880615
Batch 43/64 loss: -0.1824532151222229
Batch 44/64 loss: -0.16731971502304077
Batch 45/64 loss: -0.17460477352142334
Batch 46/64 loss: -0.18039655685424805
Batch 47/64 loss: -0.19720280170440674
Batch 48/64 loss: -0.17752385139465332
Batch 49/64 loss: -0.17953377962112427
Batch 50/64 loss: -0.1597622036933899
Batch 51/64 loss: -0.1770680546760559
Batch 52/64 loss: -0.16176801919937134
Batch 53/64 loss: -0.17946630716323853
Batch 54/64 loss: -0.16993510723114014
Batch 55/64 loss: -0.14727264642715454
Batch 56/64 loss: -0.1558687686920166
Batch 57/64 loss: -0.16717588901519775
Batch 58/64 loss: -0.1578931212425232
Batch 59/64 loss: -0.1840030550956726
Batch 60/64 loss: -0.17327678203582764
Batch 61/64 loss: -0.15550875663757324
Batch 62/64 loss: -0.1790183186531067
Batch 63/64 loss: -0.12798160314559937
Batch 64/64 loss: -0.2084817886352539
Epoch 62  Train loss: -0.17445930967143938  Val loss: -0.14750672574715107
Saving best model, epoch: 62
Epoch 63
-------------------------------
Batch 1/64 loss: -0.17017459869384766
Batch 2/64 loss: -0.1669626235961914
Batch 3/64 loss: -0.1849943995475769
Batch 4/64 loss: -0.17465955018997192
Batch 5/64 loss: -0.19910603761672974
Batch 6/64 loss: -0.15980589389801025
Batch 7/64 loss: -0.16333824396133423
Batch 8/64 loss: -0.18453043699264526
Batch 9/64 loss: -0.16502773761749268
Batch 10/64 loss: -0.14683836698532104
Batch 11/64 loss: -0.18550217151641846
Batch 12/64 loss: -0.17800867557525635
Batch 13/64 loss: -0.18089938163757324
Batch 14/64 loss: -0.1746525764465332
Batch 15/64 loss: -0.16042494773864746
Batch 16/64 loss: -0.19262927770614624
Batch 17/64 loss: -0.18526995182037354
Batch 18/64 loss: -0.17881298065185547
Batch 19/64 loss: -0.1647629737854004
Batch 20/64 loss: -0.18830156326293945
Batch 21/64 loss: -0.17999792098999023
Batch 22/64 loss: -0.15335392951965332
Batch 23/64 loss: -0.15649425983428955
Batch 24/64 loss: -0.17711752653121948
Batch 25/64 loss: -0.2067803144454956
Batch 26/64 loss: -0.169661283493042
Batch 27/64 loss: -0.18255746364593506
Batch 28/64 loss: -0.20746713876724243
Batch 29/64 loss: -0.18447524309158325
Batch 30/64 loss: -0.1882869005203247
Batch 31/64 loss: -0.1917467713356018
Batch 32/64 loss: -0.2042366862297058
Batch 33/64 loss: -0.19166427850723267
Batch 34/64 loss: -0.1728215217590332
Batch 35/64 loss: -0.15879720449447632
Batch 36/64 loss: -0.1939711570739746
Batch 37/64 loss: -0.17186498641967773
Batch 38/64 loss: -0.17424827814102173
Batch 39/64 loss: -0.1902376413345337
Batch 40/64 loss: -0.18427342176437378
Batch 41/64 loss: -0.19462990760803223
Batch 42/64 loss: -0.16356146335601807
Batch 43/64 loss: -0.2048768401145935
Batch 44/64 loss: -0.16800427436828613
Batch 45/64 loss: -0.2062353491783142
Batch 46/64 loss: -0.1775699257850647
Batch 47/64 loss: -0.15613532066345215
Batch 48/64 loss: -0.10919469594955444
Batch 49/64 loss: -0.16965657472610474
Batch 50/64 loss: -0.19415080547332764
Batch 51/64 loss: -0.18416732549667358
Batch 52/64 loss: -0.1866316795349121
Batch 53/64 loss: -0.14455705881118774
Batch 54/64 loss: -0.1842471957206726
Batch 55/64 loss: -0.16569310426712036
Batch 56/64 loss: -0.16489452123641968
Batch 57/64 loss: -0.18395036458969116
Batch 58/64 loss: -0.19193035364151
Batch 59/64 loss: -0.15887212753295898
Batch 60/64 loss: -0.19272643327713013
Batch 61/64 loss: -0.19633817672729492
Batch 62/64 loss: -0.17864787578582764
Batch 63/64 loss: -0.18089640140533447
Batch 64/64 loss: -0.17978060245513916
Epoch 63  Train loss: -0.17783779677222758  Val loss: -0.13874366439085237
Epoch 64
-------------------------------
Batch 1/64 loss: -0.16549289226531982
Batch 2/64 loss: -0.1798950433731079
Batch 3/64 loss: -0.1736851930618286
Batch 4/64 loss: -0.1523439884185791
Batch 5/64 loss: -0.17422252893447876
Batch 6/64 loss: -0.17560064792633057
Batch 7/64 loss: -0.17725825309753418
Batch 8/64 loss: -0.156954824924469
Batch 9/64 loss: -0.16101431846618652
Batch 10/64 loss: -0.18685054779052734
Batch 11/64 loss: -0.18244671821594238
Batch 12/64 loss: -0.17256760597229004
Batch 13/64 loss: -0.18621355295181274
Batch 14/64 loss: -0.20020651817321777
Batch 15/64 loss: -0.1790880560874939
Batch 16/64 loss: -0.16262286901474
Batch 17/64 loss: -0.17178606986999512
Batch 18/64 loss: -0.1565726399421692
Batch 19/64 loss: -0.17211884260177612
Batch 20/64 loss: -0.1604151725769043
Batch 21/64 loss: -0.19704151153564453
Batch 22/64 loss: -0.16979730129241943
Batch 23/64 loss: -0.18264198303222656
Batch 24/64 loss: -0.1929013729095459
Batch 25/64 loss: -0.1502629518508911
Batch 26/64 loss: -0.17161357402801514
Batch 27/64 loss: -0.19450533390045166
Batch 28/64 loss: -0.16730576753616333
Batch 29/64 loss: -0.18695300817489624
Batch 30/64 loss: -0.17344224452972412
Batch 31/64 loss: -0.1564670205116272
Batch 32/64 loss: -0.16951340436935425
Batch 33/64 loss: -0.16620421409606934
Batch 34/64 loss: -0.16238147020339966
Batch 35/64 loss: -0.1781163215637207
Batch 36/64 loss: -0.1982135772705078
Batch 37/64 loss: -0.16237682104110718
Batch 38/64 loss: -0.1684536337852478
Batch 39/64 loss: -0.17770731449127197
Batch 40/64 loss: -0.19230622053146362
Batch 41/64 loss: -0.16976433992385864
Batch 42/64 loss: -0.1907309889793396
Batch 43/64 loss: -0.19422286748886108
Batch 44/64 loss: -0.152357280254364
Batch 45/64 loss: -0.19133925437927246
Batch 46/64 loss: -0.16456562280654907
Batch 47/64 loss: -0.1685194969177246
Batch 48/64 loss: -0.18998008966445923
Batch 49/64 loss: -0.15607362985610962
Batch 50/64 loss: -0.17872470617294312
Batch 51/64 loss: -0.1854954957962036
Batch 52/64 loss: -0.16700983047485352
Batch 53/64 loss: -0.15283703804016113
Batch 54/64 loss: -0.19895964860916138
Batch 55/64 loss: -0.192399263381958
Batch 56/64 loss: -0.17359668016433716
Batch 57/64 loss: -0.1956614851951599
Batch 58/64 loss: -0.19427895545959473
Batch 59/64 loss: -0.20142877101898193
Batch 60/64 loss: -0.1845545768737793
Batch 61/64 loss: -0.16898518800735474
Batch 62/64 loss: -0.1927468180656433
Batch 63/64 loss: -0.1744297742843628
Batch 64/64 loss: -0.18804335594177246
Epoch 64  Train loss: -0.17639616704454608  Val loss: -0.14559545222016954
Epoch 65
-------------------------------
Batch 1/64 loss: -0.20166850090026855
Batch 2/64 loss: -0.16659760475158691
Batch 3/64 loss: -0.1894645094871521
Batch 4/64 loss: -0.1933962106704712
Batch 5/64 loss: -0.1551920771598816
Batch 6/64 loss: -0.19342130422592163
Batch 7/64 loss: -0.19127756357192993
Batch 8/64 loss: -0.17885005474090576
Batch 9/64 loss: -0.18960607051849365
Batch 10/64 loss: -0.17775410413742065
Batch 11/64 loss: -0.15994346141815186
Batch 12/64 loss: -0.1742558479309082
Batch 13/64 loss: -0.1907508373260498
Batch 14/64 loss: -0.17974597215652466
Batch 15/64 loss: -0.21446657180786133
Batch 16/64 loss: -0.18650513887405396
Batch 17/64 loss: -0.19326788187026978
Batch 18/64 loss: -0.19078123569488525
Batch 19/64 loss: -0.17632746696472168
Batch 20/64 loss: -0.17230290174484253
Batch 21/64 loss: -0.19756364822387695
Batch 22/64 loss: -0.19406956434249878
Batch 23/64 loss: -0.17294257879257202
Batch 24/64 loss: -0.18977689743041992
Batch 25/64 loss: -0.17849701642990112
Batch 26/64 loss: -0.18792223930358887
Batch 27/64 loss: -0.1815779209136963
Batch 28/64 loss: -0.18691366910934448
Batch 29/64 loss: -0.18666237592697144
Batch 30/64 loss: -0.19353461265563965
Batch 31/64 loss: -0.19096648693084717
Batch 32/64 loss: -0.19438248872756958
Batch 33/64 loss: -0.18602007627487183
Batch 34/64 loss: -0.200708270072937
Batch 35/64 loss: -0.192940354347229
Batch 36/64 loss: -0.17029017210006714
Batch 37/64 loss: -0.17316550016403198
Batch 38/64 loss: -0.1684836745262146
Batch 39/64 loss: -0.18819165229797363
Batch 40/64 loss: -0.18707334995269775
Batch 41/64 loss: -0.19281011819839478
Batch 42/64 loss: -0.18995070457458496
Batch 43/64 loss: -0.19052362442016602
Batch 44/64 loss: -0.17033874988555908
Batch 45/64 loss: -0.18321865797042847
Batch 46/64 loss: -0.16920340061187744
Batch 47/64 loss: -0.21863031387329102
Batch 48/64 loss: -0.1846710443496704
Batch 49/64 loss: -0.18620890378952026
Batch 50/64 loss: -0.18553966283798218
Batch 51/64 loss: -0.17060327529907227
Batch 52/64 loss: -0.15388989448547363
Batch 53/64 loss: -0.15780603885650635
Batch 54/64 loss: -0.14273500442504883
Batch 55/64 loss: -0.18125557899475098
Batch 56/64 loss: -0.1801052689552307
Batch 57/64 loss: -0.16389763355255127
Batch 58/64 loss: -0.18036943674087524
Batch 59/64 loss: -0.1577460765838623
Batch 60/64 loss: -0.1663932204246521
Batch 61/64 loss: -0.18714332580566406
Batch 62/64 loss: -0.17755728960037231
Batch 63/64 loss: -0.17511844635009766
Batch 64/64 loss: -0.2083648443222046
Epoch 65  Train loss: -0.18226270067925546  Val loss: -0.15173487667365582
Saving best model, epoch: 65
Epoch 66
-------------------------------
Batch 1/64 loss: -0.17843478918075562
Batch 2/64 loss: -0.19159960746765137
Batch 3/64 loss: -0.21119099855422974
Batch 4/64 loss: -0.1722649335861206
Batch 5/64 loss: -0.18573325872421265
Batch 6/64 loss: -0.16476011276245117
Batch 7/64 loss: -0.14042437076568604
Batch 8/64 loss: -0.1960979700088501
Batch 9/64 loss: -0.19427967071533203
Batch 10/64 loss: -0.18122494220733643
Batch 11/64 loss: -0.1939767599105835
Batch 12/64 loss: -0.18030548095703125
Batch 13/64 loss: -0.17351603507995605
Batch 14/64 loss: -0.18088650703430176
Batch 15/64 loss: -0.19280540943145752
Batch 16/64 loss: -0.17741388082504272
Batch 17/64 loss: -0.17392659187316895
Batch 18/64 loss: -0.1642189621925354
Batch 19/64 loss: -0.16970306634902954
Batch 20/64 loss: -0.1852673888206482
Batch 21/64 loss: -0.18246370553970337
Batch 22/64 loss: -0.18207240104675293
Batch 23/64 loss: -0.18844389915466309
Batch 24/64 loss: -0.19611036777496338
Batch 25/64 loss: -0.21383625268936157
Batch 26/64 loss: -0.16631996631622314
Batch 27/64 loss: -0.19245201349258423
Batch 28/64 loss: -0.17733359336853027
Batch 29/64 loss: -0.1951972246170044
Batch 30/64 loss: -0.20214450359344482
Batch 31/64 loss: -0.18306946754455566
Batch 32/64 loss: -0.19934481382369995
Batch 33/64 loss: -0.18939071893692017
Batch 34/64 loss: -0.18477535247802734
Batch 35/64 loss: -0.17691385746002197
Batch 36/64 loss: -0.18785983324050903
Batch 37/64 loss: -0.18623536825180054
Batch 38/64 loss: -0.17867326736450195
Batch 39/64 loss: -0.1779232621192932
Batch 40/64 loss: -0.16105735301971436
Batch 41/64 loss: -0.18575716018676758
Batch 42/64 loss: -0.18404459953308105
Batch 43/64 loss: -0.15959006547927856
Batch 44/64 loss: -0.19209468364715576
Batch 45/64 loss: -0.17284619808197021
Batch 46/64 loss: -0.16524243354797363
Batch 47/64 loss: -0.18717998266220093
Batch 48/64 loss: -0.172296404838562
Batch 49/64 loss: -0.17208433151245117
Batch 50/64 loss: -0.1899106502532959
Batch 51/64 loss: -0.18373709917068481
Batch 52/64 loss: -0.18386614322662354
Batch 53/64 loss: -0.1419869065284729
Batch 54/64 loss: -0.18027335405349731
Batch 55/64 loss: -0.17722439765930176
Batch 56/64 loss: -0.19661813974380493
Batch 57/64 loss: -0.16894829273223877
Batch 58/64 loss: -0.17946261167526245
Batch 59/64 loss: -0.16024857759475708
Batch 60/64 loss: -0.19748938083648682
Batch 61/64 loss: -0.19036191701889038
Batch 62/64 loss: -0.1829758882522583
Batch 63/64 loss: -0.19303375482559204
Batch 64/64 loss: -0.19604086875915527
Epoch 66  Train loss: -0.18186590718288048  Val loss: -0.1530768564066936
Saving best model, epoch: 66
Epoch 67
-------------------------------
Batch 1/64 loss: -0.20285534858703613
Batch 2/64 loss: -0.19361406564712524
Batch 3/64 loss: -0.18196654319763184
Batch 4/64 loss: -0.17793768644332886
Batch 5/64 loss: -0.20668458938598633
Batch 6/64 loss: -0.1616610288619995
Batch 7/64 loss: -0.19786453247070312
Batch 8/64 loss: -0.19406330585479736
Batch 9/64 loss: -0.16894614696502686
Batch 10/64 loss: -0.16810357570648193
Batch 11/64 loss: -0.1784144639968872
Batch 12/64 loss: -0.1839192509651184
Batch 13/64 loss: -0.17270684242248535
Batch 14/64 loss: -0.16347813606262207
Batch 15/64 loss: -0.1877148151397705
Batch 16/64 loss: -0.2105773687362671
Batch 17/64 loss: -0.1644895076751709
Batch 18/64 loss: -0.1821049451828003
Batch 19/64 loss: -0.19975340366363525
Batch 20/64 loss: -0.18204277753829956
Batch 21/64 loss: -0.1703316569328308
Batch 22/64 loss: -0.17799800634384155
Batch 23/64 loss: -0.19867640733718872
Batch 24/64 loss: -0.18373942375183105
Batch 25/64 loss: -0.17699509859085083
Batch 26/64 loss: -0.18037545680999756
Batch 27/64 loss: -0.18311601877212524
Batch 28/64 loss: -0.18808996677398682
Batch 29/64 loss: -0.18886065483093262
Batch 30/64 loss: -0.1381407380104065
Batch 31/64 loss: -0.17856037616729736
Batch 32/64 loss: -0.1719728708267212
Batch 33/64 loss: -0.178941011428833
Batch 34/64 loss: -0.1980602741241455
Batch 35/64 loss: -0.18233859539031982
Batch 36/64 loss: -0.18346363306045532
Batch 37/64 loss: -0.18707221746444702
Batch 38/64 loss: -0.19156700372695923
Batch 39/64 loss: -0.18174374103546143
Batch 40/64 loss: -0.17922240495681763
Batch 41/64 loss: -0.18728888034820557
Batch 42/64 loss: -0.1819838285446167
Batch 43/64 loss: -0.18680816888809204
Batch 44/64 loss: -0.15614354610443115
Batch 45/64 loss: -0.17771995067596436
Batch 46/64 loss: -0.17649519443511963
Batch 47/64 loss: -0.1689854860305786
Batch 48/64 loss: -0.17381954193115234
Batch 49/64 loss: -0.1860882043838501
Batch 50/64 loss: -0.20489251613616943
Batch 51/64 loss: -0.18745934963226318
Batch 52/64 loss: -0.2008339762687683
Batch 53/64 loss: -0.20595085620880127
Batch 54/64 loss: -0.18533313274383545
Batch 55/64 loss: -0.18762648105621338
Batch 56/64 loss: -0.192796528339386
Batch 57/64 loss: -0.20309853553771973
Batch 58/64 loss: -0.16782116889953613
Batch 59/64 loss: -0.16079062223434448
Batch 60/64 loss: -0.18022960424423218
Batch 61/64 loss: -0.16754388809204102
Batch 62/64 loss: -0.1729145050048828
Batch 63/64 loss: -0.19086480140686035
Batch 64/64 loss: -0.1862848997116089
Epoch 67  Train loss: -0.1826096679650101  Val loss: -0.15079591630660383
Epoch 68
-------------------------------
Batch 1/64 loss: -0.1952204704284668
Batch 2/64 loss: -0.18724358081817627
Batch 3/64 loss: -0.18863826990127563
Batch 4/64 loss: -0.17062997817993164
Batch 5/64 loss: -0.19788992404937744
Batch 6/64 loss: -0.18536537885665894
Batch 7/64 loss: -0.18110054731369019
Batch 8/64 loss: -0.1569458246231079
Batch 9/64 loss: -0.20301765203475952
Batch 10/64 loss: -0.19125986099243164
Batch 11/64 loss: -0.14862555265426636
Batch 12/64 loss: -0.19149845838546753
Batch 13/64 loss: -0.15965914726257324
Batch 14/64 loss: -0.1656699776649475
Batch 15/64 loss: -0.19040530920028687
Batch 16/64 loss: -0.20333993434906006
Batch 17/64 loss: -0.17812323570251465
Batch 18/64 loss: -0.18986618518829346
Batch 19/64 loss: -0.1886240839958191
Batch 20/64 loss: -0.1868261694908142
Batch 21/64 loss: -0.18412655591964722
Batch 22/64 loss: -0.1652427315711975
Batch 23/64 loss: -0.18633151054382324
Batch 24/64 loss: -0.15963250398635864
Batch 25/64 loss: -0.18832498788833618
Batch 26/64 loss: -0.16366684436798096
Batch 27/64 loss: -0.19540798664093018
Batch 28/64 loss: -0.16642040014266968
Batch 29/64 loss: -0.16590267419815063
Batch 30/64 loss: -0.18831568956375122
Batch 31/64 loss: -0.20715564489364624
Batch 32/64 loss: -0.1790183186531067
Batch 33/64 loss: -0.17823898792266846
Batch 34/64 loss: -0.18477284908294678
Batch 35/64 loss: -0.17849516868591309
Batch 36/64 loss: -0.1814846396446228
Batch 37/64 loss: -0.19258970022201538
Batch 38/64 loss: -0.19477832317352295
Batch 39/64 loss: -0.1869564652442932
Batch 40/64 loss: -0.19235455989837646
Batch 41/64 loss: -0.1746707558631897
Batch 42/64 loss: -0.18887656927108765
Batch 43/64 loss: -0.1791577935218811
Batch 44/64 loss: -0.18777954578399658
Batch 45/64 loss: -0.16513687372207642
Batch 46/64 loss: -0.15693235397338867
Batch 47/64 loss: -0.18663281202316284
Batch 48/64 loss: -0.18208545446395874
Batch 49/64 loss: -0.19108480215072632
Batch 50/64 loss: -0.18053394556045532
Batch 51/64 loss: -0.19643139839172363
Batch 52/64 loss: -0.21037065982818604
Batch 53/64 loss: -0.17602694034576416
Batch 54/64 loss: -0.17966318130493164
Batch 55/64 loss: -0.18059879541397095
Batch 56/64 loss: -0.18278813362121582
Batch 57/64 loss: -0.19493836164474487
Batch 58/64 loss: -0.16821706295013428
Batch 59/64 loss: -0.17040830850601196
Batch 60/64 loss: -0.2033144235610962
Batch 61/64 loss: -0.16929686069488525
Batch 62/64 loss: -0.15958738327026367
Batch 63/64 loss: -0.18726485967636108
Batch 64/64 loss: -0.17796611785888672
Epoch 68  Train loss: -0.1820303991729138  Val loss: -0.15894548507900172
Saving best model, epoch: 68
Epoch 69
-------------------------------
Batch 1/64 loss: -0.14744335412979126
Batch 2/64 loss: -0.18444108963012695
Batch 3/64 loss: -0.1831788420677185
Batch 4/64 loss: -0.18027031421661377
Batch 5/64 loss: -0.19818240404129028
Batch 6/64 loss: -0.18765604496002197
Batch 7/64 loss: -0.1969388723373413
Batch 8/64 loss: -0.18783247470855713
Batch 9/64 loss: -0.16653400659561157
Batch 10/64 loss: -0.19504451751708984
Batch 11/64 loss: -0.1807321310043335
Batch 12/64 loss: -0.18838155269622803
Batch 13/64 loss: -0.20353567600250244
Batch 14/64 loss: -0.21696722507476807
Batch 15/64 loss: -0.1765691637992859
Batch 16/64 loss: -0.19758647680282593
Batch 17/64 loss: -0.18088853359222412
Batch 18/64 loss: -0.2027900218963623
Batch 19/64 loss: -0.1865835189819336
Batch 20/64 loss: -0.1983688473701477
Batch 21/64 loss: -0.1774519681930542
Batch 22/64 loss: -0.18975704908370972
Batch 23/64 loss: -0.18032097816467285
Batch 24/64 loss: -0.18365079164505005
Batch 25/64 loss: -0.17705202102661133
Batch 26/64 loss: -0.16263210773468018
Batch 27/64 loss: -0.17715930938720703
Batch 28/64 loss: -0.18916261196136475
Batch 29/64 loss: -0.19330066442489624
Batch 30/64 loss: -0.18550759553909302
Batch 31/64 loss: -0.17607706785202026
Batch 32/64 loss: -0.17346185445785522
Batch 33/64 loss: -0.19205373525619507
Batch 34/64 loss: -0.18883895874023438
Batch 35/64 loss: -0.18278086185455322
Batch 36/64 loss: -0.1869649887084961
Batch 37/64 loss: -0.20320343971252441
Batch 38/64 loss: -0.1795496940612793
Batch 39/64 loss: -0.17264926433563232
Batch 40/64 loss: -0.18439441919326782
Batch 41/64 loss: -0.16614645719528198
Batch 42/64 loss: -0.19284266233444214
Batch 43/64 loss: -0.18332380056381226
Batch 44/64 loss: -0.1644025444984436
Batch 45/64 loss: -0.17292970418930054
Batch 46/64 loss: -0.1741754412651062
Batch 47/64 loss: -0.17618131637573242
Batch 48/64 loss: -0.17606031894683838
Batch 49/64 loss: -0.18004906177520752
Batch 50/64 loss: -0.17096740007400513
Batch 51/64 loss: -0.20066404342651367
Batch 52/64 loss: -0.17805421352386475
Batch 53/64 loss: -0.19546300172805786
Batch 54/64 loss: -0.17983990907669067
Batch 55/64 loss: -0.1840875744819641
Batch 56/64 loss: -0.18163758516311646
Batch 57/64 loss: -0.18313336372375488
Batch 58/64 loss: -0.15397679805755615
Batch 59/64 loss: -0.17508816719055176
Batch 60/64 loss: -0.18800634145736694
Batch 61/64 loss: -0.19699424505233765
Batch 62/64 loss: -0.19322490692138672
Batch 63/64 loss: -0.1895902156829834
Batch 64/64 loss: -0.19757288694381714
Epoch 69  Train loss: -0.18385746175167608  Val loss: -0.14149486875206335
Epoch 70
-------------------------------
Batch 1/64 loss: -0.19983655214309692
Batch 2/64 loss: -0.17606472969055176
Batch 3/64 loss: -0.15300357341766357
Batch 4/64 loss: -0.1955161690711975
Batch 5/64 loss: -0.19534790515899658
Batch 6/64 loss: -0.18290770053863525
Batch 7/64 loss: -0.18048083782196045
Batch 8/64 loss: -0.18589359521865845
Batch 9/64 loss: -0.18475806713104248
Batch 10/64 loss: -0.1712045669555664
Batch 11/64 loss: -0.18885284662246704
Batch 12/64 loss: -0.17549514770507812
Batch 13/64 loss: -0.16964799165725708
Batch 14/64 loss: -0.18104326725006104
Batch 15/64 loss: -0.16454869508743286
Batch 16/64 loss: -0.18346750736236572
Batch 17/64 loss: -0.18693971633911133
Batch 18/64 loss: -0.19904381036758423
Batch 19/64 loss: -0.18164968490600586
Batch 20/64 loss: -0.1741911768913269
Batch 21/64 loss: -0.16708600521087646
Batch 22/64 loss: -0.17517447471618652
Batch 23/64 loss: -0.18304342031478882
Batch 24/64 loss: -0.18466728925704956
Batch 25/64 loss: -0.1881941556930542
Batch 26/64 loss: -0.17469358444213867
Batch 27/64 loss: -0.18553709983825684
Batch 28/64 loss: -0.172840416431427
Batch 29/64 loss: -0.1567366123199463
Batch 30/64 loss: -0.16592323780059814
Batch 31/64 loss: -0.18178778886795044
Batch 32/64 loss: -0.17416304349899292
Batch 33/64 loss: -0.2022138237953186
Batch 34/64 loss: -0.19433951377868652
Batch 35/64 loss: -0.2038751244544983
Batch 36/64 loss: -0.20052504539489746
Batch 37/64 loss: -0.19764059782028198
Batch 38/64 loss: -0.1690616011619568
Batch 39/64 loss: -0.18613505363464355
Batch 40/64 loss: -0.19228816032409668
Batch 41/64 loss: -0.20127207040786743
Batch 42/64 loss: -0.18088555335998535
Batch 43/64 loss: -0.17512667179107666
Batch 44/64 loss: -0.19847464561462402
Batch 45/64 loss: -0.18257451057434082
Batch 46/64 loss: -0.18403244018554688
Batch 47/64 loss: -0.1826765537261963
Batch 48/64 loss: -0.17986702919006348
Batch 49/64 loss: -0.2052718997001648
Batch 50/64 loss: -0.17969810962677002
Batch 51/64 loss: -0.18746167421340942
Batch 52/64 loss: -0.18675541877746582
Batch 53/64 loss: -0.1628483533859253
Batch 54/64 loss: -0.19973838329315186
Batch 55/64 loss: -0.19019901752471924
Batch 56/64 loss: -0.19578152894973755
Batch 57/64 loss: -0.14066803455352783
Batch 58/64 loss: -0.1912379264831543
Batch 59/64 loss: -0.18282842636108398
Batch 60/64 loss: -0.18460357189178467
Batch 61/64 loss: -0.171750009059906
Batch 62/64 loss: -0.18036746978759766
Batch 63/64 loss: -0.213395357131958
Batch 64/64 loss: -0.18250501155853271
Epoch 70  Train loss: -0.18321902751922609  Val loss: -0.1661752286235901
Saving best model, epoch: 70
Epoch 71
-------------------------------
Batch 1/64 loss: -0.20808464288711548
Batch 2/64 loss: -0.1903473138809204
Batch 3/64 loss: -0.17228162288665771
Batch 4/64 loss: -0.18584930896759033
Batch 5/64 loss: -0.1604219675064087
Batch 6/64 loss: -0.16742420196533203
Batch 7/64 loss: -0.1886950135231018
Batch 8/64 loss: -0.17975765466690063
Batch 9/64 loss: -0.18863892555236816
Batch 10/64 loss: -0.20407938957214355
Batch 11/64 loss: -0.20145946741104126
Batch 12/64 loss: -0.20033389329910278
Batch 13/64 loss: -0.17817699909210205
Batch 14/64 loss: -0.18996554613113403
Batch 15/64 loss: -0.19168078899383545
Batch 16/64 loss: -0.18589729070663452
Batch 17/64 loss: -0.17461931705474854
Batch 18/64 loss: -0.20188885927200317
Batch 19/64 loss: -0.1855948567390442
Batch 20/64 loss: -0.21045470237731934
Batch 21/64 loss: -0.18356263637542725
Batch 22/64 loss: -0.1909666657447815
Batch 23/64 loss: -0.20265638828277588
Batch 24/64 loss: -0.1881699562072754
Batch 25/64 loss: -0.19835585355758667
Batch 26/64 loss: -0.18054896593093872
Batch 27/64 loss: -0.21233206987380981
Batch 28/64 loss: -0.19813638925552368
Batch 29/64 loss: -0.19908982515335083
Batch 30/64 loss: -0.20998096466064453
Batch 31/64 loss: -0.18998867273330688
Batch 32/64 loss: -0.2033776044845581
Batch 33/64 loss: -0.18505549430847168
Batch 34/64 loss: -0.19966542720794678
Batch 35/64 loss: -0.19552147388458252
Batch 36/64 loss: -0.1868489384651184
Batch 37/64 loss: -0.1725916862487793
Batch 38/64 loss: -0.19545912742614746
Batch 39/64 loss: -0.16899973154067993
Batch 40/64 loss: -0.1853475570678711
Batch 41/64 loss: -0.19200164079666138
Batch 42/64 loss: -0.21246659755706787
Batch 43/64 loss: -0.19652307033538818
Batch 44/64 loss: -0.18782252073287964
Batch 45/64 loss: -0.1959647536277771
Batch 46/64 loss: -0.19740307331085205
Batch 47/64 loss: -0.16704028844833374
Batch 48/64 loss: -0.174565851688385
Batch 49/64 loss: -0.18000322580337524
Batch 50/64 loss: -0.17717337608337402
Batch 51/64 loss: -0.1802118420600891
Batch 52/64 loss: -0.19859856367111206
Batch 53/64 loss: -0.1920313835144043
Batch 54/64 loss: -0.18466830253601074
Batch 55/64 loss: -0.18365788459777832
Batch 56/64 loss: -0.17865222692489624
Batch 57/64 loss: -0.17609739303588867
Batch 58/64 loss: -0.16501545906066895
Batch 59/64 loss: -0.19629979133605957
Batch 60/64 loss: -0.17430782318115234
Batch 61/64 loss: -0.19641625881195068
Batch 62/64 loss: -0.16852974891662598
Batch 63/64 loss: -0.19413459300994873
Batch 64/64 loss: -0.15492641925811768
Epoch 71  Train loss: -0.18820529685300938  Val loss: -0.11206396748519845
Epoch 72
-------------------------------
Batch 1/64 loss: -0.18425273895263672
Batch 2/64 loss: -0.19031721353530884
Batch 3/64 loss: -0.1942082643508911
Batch 4/64 loss: -0.19386029243469238
Batch 5/64 loss: -0.205541729927063
Batch 6/64 loss: -0.1999349594116211
Batch 7/64 loss: -0.1734158992767334
Batch 8/64 loss: -0.18679946660995483
Batch 9/64 loss: -0.18654227256774902
Batch 10/64 loss: -0.19173866510391235
Batch 11/64 loss: -0.17197012901306152
Batch 12/64 loss: -0.18521499633789062
Batch 13/64 loss: -0.196272075176239
Batch 14/64 loss: -0.17409074306488037
Batch 15/64 loss: -0.1758384108543396
Batch 16/64 loss: -0.1919451355934143
Batch 17/64 loss: -0.17857736349105835
Batch 18/64 loss: -0.180647611618042
Batch 19/64 loss: -0.20168936252593994
Batch 20/64 loss: -0.18436861038208008
Batch 21/64 loss: -0.1939416527748108
Batch 22/64 loss: -0.17501074075698853
Batch 23/64 loss: -0.20480507612228394
Batch 24/64 loss: -0.18012768030166626
Batch 25/64 loss: -0.18700188398361206
Batch 26/64 loss: -0.19329190254211426
Batch 27/64 loss: -0.17151784896850586
Batch 28/64 loss: -0.16639304161071777
Batch 29/64 loss: -0.18193721771240234
Batch 30/64 loss: -0.19064974784851074
Batch 31/64 loss: -0.17951828241348267
Batch 32/64 loss: -0.18434977531433105
Batch 33/64 loss: -0.19585001468658447
Batch 34/64 loss: -0.18615812063217163
Batch 35/64 loss: -0.16089332103729248
Batch 36/64 loss: -0.18018978834152222
Batch 37/64 loss: -0.16486430168151855
Batch 38/64 loss: -0.17331045866012573
Batch 39/64 loss: -0.19314825534820557
Batch 40/64 loss: -0.18500977754592896
Batch 41/64 loss: -0.2052760124206543
Batch 42/64 loss: -0.1645001769065857
Batch 43/64 loss: -0.19004476070404053
Batch 44/64 loss: -0.21687984466552734
Batch 45/64 loss: -0.18763643503189087
Batch 46/64 loss: -0.17958217859268188
Batch 47/64 loss: -0.1839991807937622
Batch 48/64 loss: -0.18971872329711914
Batch 49/64 loss: -0.18942785263061523
Batch 50/64 loss: -0.17316436767578125
Batch 51/64 loss: -0.18896931409835815
Batch 52/64 loss: -0.17617183923721313
Batch 53/64 loss: -0.20660418272018433
Batch 54/64 loss: -0.2096378207206726
Batch 55/64 loss: -0.19607746601104736
Batch 56/64 loss: -0.20180141925811768
Batch 57/64 loss: -0.18393391370773315
Batch 58/64 loss: -0.19157534837722778
Batch 59/64 loss: -0.18101686239242554
Batch 60/64 loss: -0.18693006038665771
Batch 61/64 loss: -0.1896544098854065
Batch 62/64 loss: -0.18642187118530273
Batch 63/64 loss: -0.2062036395072937
Batch 64/64 loss: -0.17260730266571045
Epoch 72  Train loss: -0.18682161546220966  Val loss: -0.12140626596011657
Epoch 73
-------------------------------
Batch 1/64 loss: -0.18384134769439697
Batch 2/64 loss: -0.19719350337982178
Batch 3/64 loss: -0.1952894926071167
Batch 4/64 loss: -0.17384791374206543
Batch 5/64 loss: -0.16058003902435303
Batch 6/64 loss: -0.20345056056976318
Batch 7/64 loss: -0.18281793594360352
Batch 8/64 loss: -0.17808234691619873
Batch 9/64 loss: -0.20543473958969116
Batch 10/64 loss: -0.20307600498199463
Batch 11/64 loss: -0.17958641052246094
Batch 12/64 loss: -0.18793052434921265
Batch 13/64 loss: -0.19803011417388916
Batch 14/64 loss: -0.17899245023727417
Batch 15/64 loss: -0.1566469669342041
Batch 16/64 loss: -0.19165682792663574
Batch 17/64 loss: -0.17906951904296875
Batch 18/64 loss: -0.19057047367095947
Batch 19/64 loss: -0.1987079381942749
Batch 20/64 loss: -0.20033466815948486
Batch 21/64 loss: -0.195634126663208
Batch 22/64 loss: -0.1756608486175537
Batch 23/64 loss: -0.18651586771011353
Batch 24/64 loss: -0.1781836748123169
Batch 25/64 loss: -0.1874142289161682
Batch 26/64 loss: -0.1746864914894104
Batch 27/64 loss: -0.18002575635910034
Batch 28/64 loss: -0.20318293571472168
Batch 29/64 loss: -0.1897297501564026
Batch 30/64 loss: -0.1855604648590088
Batch 31/64 loss: -0.1873902678489685
Batch 32/64 loss: -0.1916453242301941
Batch 33/64 loss: -0.19416993856430054
Batch 34/64 loss: -0.18716347217559814
Batch 35/64 loss: -0.1744595766067505
Batch 36/64 loss: -0.1818997859954834
Batch 37/64 loss: -0.17860960960388184
Batch 38/64 loss: -0.18357586860656738
Batch 39/64 loss: -0.1794668436050415
Batch 40/64 loss: -0.18348169326782227
Batch 41/64 loss: -0.17420369386672974
Batch 42/64 loss: -0.18652641773223877
Batch 43/64 loss: -0.17776983976364136
Batch 44/64 loss: -0.16706979274749756
Batch 45/64 loss: -0.21144676208496094
Batch 46/64 loss: -0.15577161312103271
Batch 47/64 loss: -0.18740200996398926
Batch 48/64 loss: -0.17803072929382324
Batch 49/64 loss: -0.17697548866271973
Batch 50/64 loss: -0.17783188819885254
Batch 51/64 loss: -0.18526887893676758
Batch 52/64 loss: -0.1972954273223877
Batch 53/64 loss: -0.20945769548416138
Batch 54/64 loss: -0.20783394575119019
Batch 55/64 loss: -0.172116219997406
Batch 56/64 loss: -0.1969088912010193
Batch 57/64 loss: -0.17133831977844238
Batch 58/64 loss: -0.18784785270690918
Batch 59/64 loss: -0.2075204849243164
Batch 60/64 loss: -0.1841350793838501
Batch 61/64 loss: -0.16998296976089478
Batch 62/64 loss: -0.1853131651878357
Batch 63/64 loss: -0.18332040309906006
Batch 64/64 loss: -0.19014060497283936
Epoch 73  Train loss: -0.18568736197901706  Val loss: -0.15900754785209997
Epoch 74
-------------------------------
Batch 1/64 loss: -0.18216311931610107
Batch 2/64 loss: -0.20623427629470825
Batch 3/64 loss: -0.20279550552368164
Batch 4/64 loss: -0.16397613286972046
Batch 5/64 loss: -0.18844211101531982
Batch 6/64 loss: -0.19249391555786133
Batch 7/64 loss: -0.18313753604888916
Batch 8/64 loss: -0.1636614203453064
Batch 9/64 loss: -0.18688637018203735
Batch 10/64 loss: -0.19015192985534668
Batch 11/64 loss: -0.21027284860610962
Batch 12/64 loss: -0.1794494390487671
Batch 13/64 loss: -0.18495672941207886
Batch 14/64 loss: -0.1677069067955017
Batch 15/64 loss: -0.2022213339805603
Batch 16/64 loss: -0.16513216495513916
Batch 17/64 loss: -0.19984906911849976
Batch 18/64 loss: -0.19022119045257568
Batch 19/64 loss: -0.17732900381088257
Batch 20/64 loss: -0.1927233338356018
Batch 21/64 loss: -0.19292199611663818
Batch 22/64 loss: -0.19415581226348877
Batch 23/64 loss: -0.17991018295288086
Batch 24/64 loss: -0.18992197513580322
Batch 25/64 loss: -0.19253623485565186
Batch 26/64 loss: -0.19480746984481812
Batch 27/64 loss: -0.17879128456115723
Batch 28/64 loss: -0.18778502941131592
Batch 29/64 loss: -0.1577223539352417
Batch 30/64 loss: -0.18255311250686646
Batch 31/64 loss: -0.19609707593917847
Batch 32/64 loss: -0.19596320390701294
Batch 33/64 loss: -0.1923728585243225
Batch 34/64 loss: -0.18967056274414062
Batch 35/64 loss: -0.17325162887573242
Batch 36/64 loss: -0.19510281085968018
Batch 37/64 loss: -0.1904279589653015
Batch 38/64 loss: -0.15404736995697021
Batch 39/64 loss: -0.18922704458236694
Batch 40/64 loss: -0.18993210792541504
Batch 41/64 loss: -0.18111968040466309
Batch 42/64 loss: -0.17477071285247803
Batch 43/64 loss: -0.21337467432022095
Batch 44/64 loss: -0.21101582050323486
Batch 45/64 loss: -0.18693137168884277
Batch 46/64 loss: -0.18882322311401367
Batch 47/64 loss: -0.16968834400177002
Batch 48/64 loss: -0.20077931880950928
Batch 49/64 loss: -0.18368494510650635
Batch 50/64 loss: -0.16917073726654053
Batch 51/64 loss: -0.19404733180999756
Batch 52/64 loss: -0.1818808913230896
Batch 53/64 loss: -0.20472240447998047
Batch 54/64 loss: -0.1886165738105774
Batch 55/64 loss: -0.18992727994918823
Batch 56/64 loss: -0.20410430431365967
Batch 57/64 loss: -0.1880916953086853
Batch 58/64 loss: -0.1941368579864502
Batch 59/64 loss: -0.18849587440490723
Batch 60/64 loss: -0.1965399980545044
Batch 61/64 loss: -0.18799340724945068
Batch 62/64 loss: -0.177382230758667
Batch 63/64 loss: -0.20145869255065918
Batch 64/64 loss: -0.20876502990722656
Epoch 74  Train loss: -0.18792678515116373  Val loss: -0.15582279261854506
Epoch 75
-------------------------------
Batch 1/64 loss: -0.2082383632659912
Batch 2/64 loss: -0.18561750650405884
Batch 3/64 loss: -0.2030543088912964
Batch 4/64 loss: -0.18298566341400146
Batch 5/64 loss: -0.20528334379196167
Batch 6/64 loss: -0.19327616691589355
Batch 7/64 loss: -0.17784810066223145
Batch 8/64 loss: -0.21720564365386963
Batch 9/64 loss: -0.15849971771240234
Batch 10/64 loss: -0.20933401584625244
Batch 11/64 loss: -0.182564377784729
Batch 12/64 loss: -0.20077627897262573
Batch 13/64 loss: -0.19269472360610962
Batch 14/64 loss: -0.1751554012298584
Batch 15/64 loss: -0.18612760305404663
Batch 16/64 loss: -0.1967090368270874
Batch 17/64 loss: -0.18591147661209106
Batch 18/64 loss: -0.20228934288024902
Batch 19/64 loss: -0.19753825664520264
Batch 20/64 loss: -0.1785128116607666
Batch 21/64 loss: -0.18288737535476685
Batch 22/64 loss: -0.18279075622558594
Batch 23/64 loss: -0.1895706057548523
Batch 24/64 loss: -0.2043142318725586
Batch 25/64 loss: -0.1820065975189209
Batch 26/64 loss: -0.2078918218612671
Batch 27/64 loss: -0.18465590476989746
Batch 28/64 loss: -0.20144253969192505
Batch 29/64 loss: -0.19459128379821777
Batch 30/64 loss: -0.20226424932479858
Batch 31/64 loss: -0.19210761785507202
Batch 32/64 loss: -0.19162583351135254
Batch 33/64 loss: -0.17972254753112793
Batch 34/64 loss: -0.18640804290771484
Batch 35/64 loss: -0.19784867763519287
Batch 36/64 loss: -0.19404375553131104
Batch 37/64 loss: -0.187788724899292
Batch 38/64 loss: -0.19852900505065918
Batch 39/64 loss: -0.18717330694198608
Batch 40/64 loss: -0.19505548477172852
Batch 41/64 loss: -0.1920473575592041
Batch 42/64 loss: -0.2040502429008484
Batch 43/64 loss: -0.2012900710105896
Batch 44/64 loss: -0.16597282886505127
Batch 45/64 loss: -0.1728951334953308
Batch 46/64 loss: -0.2047368288040161
Batch 47/64 loss: -0.18053662776947021
Batch 48/64 loss: -0.1957920789718628
Batch 49/64 loss: -0.20835256576538086
Batch 50/64 loss: -0.1898505687713623
Batch 51/64 loss: -0.20145803689956665
Batch 52/64 loss: -0.18975108861923218
Batch 53/64 loss: -0.17640411853790283
Batch 54/64 loss: -0.19800251722335815
Batch 55/64 loss: -0.20053601264953613
Batch 56/64 loss: -0.1976432204246521
Batch 57/64 loss: -0.2033388614654541
Batch 58/64 loss: -0.16095054149627686
Batch 59/64 loss: -0.20446473360061646
Batch 60/64 loss: -0.20860803127288818
Batch 61/64 loss: -0.1730494499206543
Batch 62/64 loss: -0.20969223976135254
Batch 63/64 loss: -0.19271600246429443
Batch 64/64 loss: -0.20472151041030884
Epoch 75  Train loss: -0.19243954186346016  Val loss: -0.16919395214913227
Saving best model, epoch: 75
Epoch 76
-------------------------------
Batch 1/64 loss: -0.18801891803741455
Batch 2/64 loss: -0.21102452278137207
Batch 3/64 loss: -0.2141484022140503
Batch 4/64 loss: -0.20895236730575562
Batch 5/64 loss: -0.2044229507446289
Batch 6/64 loss: -0.20519578456878662
Batch 7/64 loss: -0.20407003164291382
Batch 8/64 loss: -0.2036333680152893
Batch 9/64 loss: -0.19672447443008423
Batch 10/64 loss: -0.19791525602340698
Batch 11/64 loss: -0.19624733924865723
Batch 12/64 loss: -0.19090259075164795
Batch 13/64 loss: -0.20764243602752686
Batch 14/64 loss: -0.1943928599357605
Batch 15/64 loss: -0.19860553741455078
Batch 16/64 loss: -0.19935590028762817
Batch 17/64 loss: -0.18460410833358765
Batch 18/64 loss: -0.1787855625152588
Batch 19/64 loss: -0.18408113718032837
Batch 20/64 loss: -0.2011844515800476
Batch 21/64 loss: -0.19765079021453857
Batch 22/64 loss: -0.2157157063484192
Batch 23/64 loss: -0.15446686744689941
Batch 24/64 loss: -0.1901896595954895
Batch 25/64 loss: -0.19339591264724731
Batch 26/64 loss: -0.19919127225875854
Batch 27/64 loss: -0.2021007537841797
Batch 28/64 loss: -0.2030206322669983
Batch 29/64 loss: -0.20411312580108643
Batch 30/64 loss: -0.16047358512878418
Batch 31/64 loss: -0.19238120317459106
Batch 32/64 loss: -0.18821680545806885
Batch 33/64 loss: -0.18689954280853271
Batch 34/64 loss: -0.19750750064849854
Batch 35/64 loss: -0.20199590921401978
Batch 36/64 loss: -0.19402897357940674
Batch 37/64 loss: -0.18632489442825317
Batch 38/64 loss: -0.1895330548286438
Batch 39/64 loss: -0.19538235664367676
Batch 40/64 loss: -0.1962968111038208
Batch 41/64 loss: -0.20492208003997803
Batch 42/64 loss: -0.19786524772644043
Batch 43/64 loss: -0.18688690662384033
Batch 44/64 loss: -0.20576566457748413
Batch 45/64 loss: -0.21211189031600952
Batch 46/64 loss: -0.21313929557800293
Batch 47/64 loss: -0.19857311248779297
Batch 48/64 loss: -0.19768553972244263
Batch 49/64 loss: -0.20141077041625977
Batch 50/64 loss: -0.16137254238128662
Batch 51/64 loss: -0.2002497911453247
Batch 52/64 loss: -0.20055919885635376
Batch 53/64 loss: -0.19355028867721558
Batch 54/64 loss: -0.19671040773391724
Batch 55/64 loss: -0.1682690978050232
Batch 56/64 loss: -0.19711172580718994
Batch 57/64 loss: -0.18422675132751465
Batch 58/64 loss: -0.17661654949188232
Batch 59/64 loss: -0.17479801177978516
Batch 60/64 loss: -0.1844831109046936
Batch 61/64 loss: -0.19582116603851318
Batch 62/64 loss: -0.20260238647460938
Batch 63/64 loss: -0.2043590545654297
Batch 64/64 loss: -0.19210129976272583
Epoch 76  Train loss: -0.1948542732818454  Val loss: -0.14329605721116476
Epoch 77
-------------------------------
Batch 1/64 loss: -0.2005831003189087
Batch 2/64 loss: -0.18288689851760864
Batch 3/64 loss: -0.16821563243865967
Batch 4/64 loss: -0.20084035396575928
Batch 5/64 loss: -0.1828153133392334
Batch 6/64 loss: -0.18528395891189575
Batch 7/64 loss: -0.198611319065094
Batch 8/64 loss: -0.20123296976089478
Batch 9/64 loss: -0.20705252885818481
Batch 10/64 loss: -0.1881272792816162
Batch 11/64 loss: -0.1984184980392456
Batch 12/64 loss: -0.2241618037223816
Batch 13/64 loss: -0.199893057346344
Batch 14/64 loss: -0.20778876543045044
Batch 15/64 loss: -0.20860034227371216
Batch 16/64 loss: -0.20334339141845703
Batch 17/64 loss: -0.2164534330368042
Batch 18/64 loss: -0.2093905806541443
Batch 19/64 loss: -0.1734849214553833
Batch 20/64 loss: -0.19707709550857544
Batch 21/64 loss: -0.19031208753585815
Batch 22/64 loss: -0.2112339735031128
Batch 23/64 loss: -0.18736326694488525
Batch 24/64 loss: -0.211683452129364
Batch 25/64 loss: -0.200791597366333
Batch 26/64 loss: -0.20711123943328857
Batch 27/64 loss: -0.18412435054779053
Batch 28/64 loss: -0.21934598684310913
Batch 29/64 loss: -0.22176873683929443
Batch 30/64 loss: -0.18264883756637573
Batch 31/64 loss: -0.18922144174575806
Batch 32/64 loss: -0.19167518615722656
Batch 33/64 loss: -0.17706561088562012
Batch 34/64 loss: -0.18545585870742798
Batch 35/64 loss: -0.18435156345367432
Batch 36/64 loss: -0.20416700839996338
Batch 37/64 loss: -0.20674389600753784
Batch 38/64 loss: -0.20255506038665771
Batch 39/64 loss: -0.20839720964431763
Batch 40/64 loss: -0.18108969926834106
Batch 41/64 loss: -0.2124481201171875
Batch 42/64 loss: -0.18815791606903076
Batch 43/64 loss: -0.21221637725830078
Batch 44/64 loss: -0.16263282299041748
Batch 45/64 loss: -0.1954001784324646
Batch 46/64 loss: -0.173323392868042
Batch 47/64 loss: -0.2010299563407898
Batch 48/64 loss: -0.197149395942688
Batch 49/64 loss: -0.19732534885406494
Batch 50/64 loss: -0.18955343961715698
Batch 51/64 loss: -0.17524534463882446
Batch 52/64 loss: -0.196050763130188
Batch 53/64 loss: -0.19942164421081543
Batch 54/64 loss: -0.19232416152954102
Batch 55/64 loss: -0.19099915027618408
Batch 56/64 loss: -0.21182745695114136
Batch 57/64 loss: -0.18533509969711304
Batch 58/64 loss: -0.19094634056091309
Batch 59/64 loss: -0.19567441940307617
Batch 60/64 loss: -0.1946665644645691
Batch 61/64 loss: -0.1994715929031372
Batch 62/64 loss: -0.1753697395324707
Batch 63/64 loss: -0.19601112604141235
Batch 64/64 loss: -0.21622872352600098
Epoch 77  Train loss: -0.19601755609699323  Val loss: -0.1659820536567583
Epoch 78
-------------------------------
Batch 1/64 loss: -0.18942439556121826
Batch 2/64 loss: -0.1822190284729004
Batch 3/64 loss: -0.2075788974761963
Batch 4/64 loss: -0.2054957151412964
Batch 5/64 loss: -0.1969311237335205
Batch 6/64 loss: -0.18837064504623413
Batch 7/64 loss: -0.19996178150177002
Batch 8/64 loss: -0.19255977869033813
Batch 9/64 loss: -0.19081801176071167
Batch 10/64 loss: -0.1778920292854309
Batch 11/64 loss: -0.1876300573348999
Batch 12/64 loss: -0.19589018821716309
Batch 13/64 loss: -0.17497676610946655
Batch 14/64 loss: -0.17522859573364258
Batch 15/64 loss: -0.2055705189704895
Batch 16/64 loss: -0.18633079528808594
Batch 17/64 loss: -0.20383012294769287
Batch 18/64 loss: -0.18663859367370605
Batch 19/64 loss: -0.20943450927734375
Batch 20/64 loss: -0.19527894258499146
Batch 21/64 loss: -0.19996792078018188
Batch 22/64 loss: -0.1960875391960144
Batch 23/64 loss: -0.2014980912208557
Batch 24/64 loss: -0.1657276153564453
Batch 25/64 loss: -0.2004745602607727
Batch 26/64 loss: -0.19951248168945312
Batch 27/64 loss: -0.1960735321044922
Batch 28/64 loss: -0.219710111618042
Batch 29/64 loss: -0.20568621158599854
Batch 30/64 loss: -0.17461282014846802
Batch 31/64 loss: -0.20825618505477905
Batch 32/64 loss: -0.203030526638031
Batch 33/64 loss: -0.20718765258789062
Batch 34/64 loss: -0.1815946102142334
Batch 35/64 loss: -0.20058846473693848
Batch 36/64 loss: -0.1476207971572876
Batch 37/64 loss: -0.21098250150680542
Batch 38/64 loss: -0.19385796785354614
Batch 39/64 loss: -0.1990875005722046
Batch 40/64 loss: -0.2163340449333191
Batch 41/64 loss: -0.20526301860809326
Batch 42/64 loss: -0.2040475606918335
Batch 43/64 loss: -0.21549075841903687
Batch 44/64 loss: -0.21837365627288818
Batch 45/64 loss: -0.19391417503356934
Batch 46/64 loss: -0.2040284276008606
Batch 47/64 loss: -0.18229323625564575
Batch 48/64 loss: -0.1954745650291443
Batch 49/64 loss: -0.2069000005722046
Batch 50/64 loss: -0.19150972366333008
Batch 51/64 loss: -0.19617170095443726
Batch 52/64 loss: -0.19443833827972412
Batch 53/64 loss: -0.19996756315231323
Batch 54/64 loss: -0.21799373626708984
Batch 55/64 loss: -0.20054328441619873
Batch 56/64 loss: -0.20800477266311646
Batch 57/64 loss: -0.19708621501922607
Batch 58/64 loss: -0.19912338256835938
Batch 59/64 loss: -0.18884968757629395
Batch 60/64 loss: -0.17533475160598755
Batch 61/64 loss: -0.1928820013999939
Batch 62/64 loss: -0.18405842781066895
Batch 63/64 loss: -0.20251351594924927
Batch 64/64 loss: -0.1961168646812439
Epoch 78  Train loss: -0.19609885145636166  Val loss: -0.15970881984815566
Epoch 79
-------------------------------
Batch 1/64 loss: -0.18171674013137817
Batch 2/64 loss: -0.18636858463287354
Batch 3/64 loss: -0.20588910579681396
Batch 4/64 loss: -0.22438251972198486
Batch 5/64 loss: -0.21548771858215332
Batch 6/64 loss: -0.2081221342086792
Batch 7/64 loss: -0.20832818746566772
Batch 8/64 loss: -0.20256870985031128
Batch 9/64 loss: -0.18989628553390503
Batch 10/64 loss: -0.19516319036483765
Batch 11/64 loss: -0.19693267345428467
Batch 12/64 loss: -0.1870887279510498
Batch 13/64 loss: -0.18344920873641968
Batch 14/64 loss: -0.19106996059417725
Batch 15/64 loss: -0.18917548656463623
Batch 16/64 loss: -0.1835351586341858
Batch 17/64 loss: -0.21591001749038696
Batch 18/64 loss: -0.2155657410621643
Batch 19/64 loss: -0.2107338309288025
Batch 20/64 loss: -0.21230000257492065
Batch 21/64 loss: -0.1982816457748413
Batch 22/64 loss: -0.21486014127731323
Batch 23/64 loss: -0.19933712482452393
Batch 24/64 loss: -0.17852675914764404
Batch 25/64 loss: -0.17799782752990723
Batch 26/64 loss: -0.20400822162628174
Batch 27/64 loss: -0.1869000792503357
Batch 28/64 loss: -0.1839035153388977
Batch 29/64 loss: -0.22810614109039307
Batch 30/64 loss: -0.19986021518707275
Batch 31/64 loss: -0.165072500705719
Batch 32/64 loss: -0.20583868026733398
Batch 33/64 loss: -0.1844068169593811
Batch 34/64 loss: -0.20706653594970703
Batch 35/64 loss: -0.20419859886169434
Batch 36/64 loss: -0.2125105857849121
Batch 37/64 loss: -0.21183699369430542
Batch 38/64 loss: -0.20944178104400635
Batch 39/64 loss: -0.18332123756408691
Batch 40/64 loss: -0.2164306640625
Batch 41/64 loss: -0.1855027675628662
Batch 42/64 loss: -0.19137132167816162
Batch 43/64 loss: -0.22479557991027832
Batch 44/64 loss: -0.19732487201690674
Batch 45/64 loss: -0.21759897470474243
Batch 46/64 loss: -0.20992708206176758
Batch 47/64 loss: -0.2125377655029297
Batch 48/64 loss: -0.2165822982788086
Batch 49/64 loss: -0.21102076768875122
Batch 50/64 loss: -0.18359357118606567
Batch 51/64 loss: -0.16783618927001953
Batch 52/64 loss: -0.17498594522476196
Batch 53/64 loss: -0.19979214668273926
Batch 54/64 loss: -0.19337064027786255
Batch 55/64 loss: -0.21277272701263428
Batch 56/64 loss: -0.1983703374862671
Batch 57/64 loss: -0.18743407726287842
Batch 58/64 loss: -0.2136961817741394
Batch 59/64 loss: -0.17947626113891602
Batch 60/64 loss: -0.16671574115753174
Batch 61/64 loss: -0.16422349214553833
Batch 62/64 loss: -0.2026439905166626
Batch 63/64 loss: -0.1921125054359436
Batch 64/64 loss: -0.18589919805526733
Epoch 79  Train loss: -0.19794038721159393  Val loss: -0.18165933225572722
Saving best model, epoch: 79
Epoch 80
-------------------------------
Batch 1/64 loss: -0.2025526762008667
Batch 2/64 loss: -0.21264111995697021
Batch 3/64 loss: -0.19717079401016235
Batch 4/64 loss: -0.19183123111724854
Batch 5/64 loss: -0.19377130270004272
Batch 6/64 loss: -0.20916444063186646
Batch 7/64 loss: -0.18592047691345215
Batch 8/64 loss: -0.2096550464630127
Batch 9/64 loss: -0.21817362308502197
Batch 10/64 loss: -0.1895022988319397
Batch 11/64 loss: -0.2086571455001831
Batch 12/64 loss: -0.164650559425354
Batch 13/64 loss: -0.1983467936515808
Batch 14/64 loss: -0.18625569343566895
Batch 15/64 loss: -0.20006489753723145
Batch 16/64 loss: -0.20719289779663086
Batch 17/64 loss: -0.21455800533294678
Batch 18/64 loss: -0.21422863006591797
Batch 19/64 loss: -0.2218780517578125
Batch 20/64 loss: -0.17727279663085938
Batch 21/64 loss: -0.2090054154396057
Batch 22/64 loss: -0.1917625069618225
Batch 23/64 loss: -0.20441192388534546
Batch 24/64 loss: -0.1996605396270752
Batch 25/64 loss: -0.21366578340530396
Batch 26/64 loss: -0.22261059284210205
Batch 27/64 loss: -0.2029968500137329
Batch 28/64 loss: -0.21790355443954468
Batch 29/64 loss: -0.22508203983306885
Batch 30/64 loss: -0.20540910959243774
Batch 31/64 loss: -0.20872724056243896
Batch 32/64 loss: -0.20529896020889282
Batch 33/64 loss: -0.19989609718322754
Batch 34/64 loss: -0.20131361484527588
Batch 35/64 loss: -0.17889654636383057
Batch 36/64 loss: -0.211530864238739
Batch 37/64 loss: -0.1920490860939026
Batch 38/64 loss: -0.2099819779396057
Batch 39/64 loss: -0.20666146278381348
Batch 40/64 loss: -0.1951887011528015
Batch 41/64 loss: -0.20954668521881104
Batch 42/64 loss: -0.190437912940979
Batch 43/64 loss: -0.2187037467956543
Batch 44/64 loss: -0.20422613620758057
Batch 45/64 loss: -0.18669915199279785
Batch 46/64 loss: -0.18750625848770142
Batch 47/64 loss: -0.1904645562171936
Batch 48/64 loss: -0.20786052942276
Batch 49/64 loss: -0.18894803524017334
Batch 50/64 loss: -0.2117648720741272
Batch 51/64 loss: -0.20851069688796997
Batch 52/64 loss: -0.19458186626434326
Batch 53/64 loss: -0.18933236598968506
Batch 54/64 loss: -0.18983709812164307
Batch 55/64 loss: -0.2031121850013733
Batch 56/64 loss: -0.19846183061599731
Batch 57/64 loss: -0.2276487946510315
Batch 58/64 loss: -0.2164890170097351
Batch 59/64 loss: -0.19533371925354004
Batch 60/64 loss: -0.20576560497283936
Batch 61/64 loss: -0.17675530910491943
Batch 62/64 loss: -0.1979997158050537
Batch 63/64 loss: -0.18746668100357056
Batch 64/64 loss: -0.2238977551460266
Epoch 80  Train loss: -0.2017398813191582  Val loss: -0.1828641481825576
Saving best model, epoch: 80
Epoch 81
-------------------------------
Batch 1/64 loss: -0.19015663862228394
Batch 2/64 loss: -0.20130252838134766
Batch 3/64 loss: -0.19946962594985962
Batch 4/64 loss: -0.1917058229446411
Batch 5/64 loss: -0.18055886030197144
Batch 6/64 loss: -0.2151603102684021
Batch 7/64 loss: -0.18871277570724487
Batch 8/64 loss: -0.21896111965179443
Batch 9/64 loss: -0.21165788173675537
Batch 10/64 loss: -0.20219635963439941
Batch 11/64 loss: -0.22311776876449585
Batch 12/64 loss: -0.20123350620269775
Batch 13/64 loss: -0.22002875804901123
Batch 14/64 loss: -0.18394988775253296
Batch 15/64 loss: -0.20954561233520508
Batch 16/64 loss: -0.21763354539871216
Batch 17/64 loss: -0.22389352321624756
Batch 18/64 loss: -0.2211642861366272
Batch 19/64 loss: -0.21288228034973145
Batch 20/64 loss: -0.2122427225112915
Batch 21/64 loss: -0.21935725212097168
Batch 22/64 loss: -0.19454753398895264
Batch 23/64 loss: -0.20948296785354614
Batch 24/64 loss: -0.2029399275779724
Batch 25/64 loss: -0.20374608039855957
Batch 26/64 loss: -0.20346379280090332
Batch 27/64 loss: -0.21105992794036865
Batch 28/64 loss: -0.2100379467010498
Batch 29/64 loss: -0.20623785257339478
Batch 30/64 loss: -0.22732794284820557
Batch 31/64 loss: -0.1705804467201233
Batch 32/64 loss: -0.1850486397743225
Batch 33/64 loss: -0.20367729663848877
Batch 34/64 loss: -0.17228734493255615
Batch 35/64 loss: -0.21534943580627441
Batch 36/64 loss: -0.19013643264770508
Batch 37/64 loss: -0.20730137825012207
Batch 38/64 loss: -0.2050994634628296
Batch 39/64 loss: -0.19722753763198853
Batch 40/64 loss: -0.18227851390838623
Batch 41/64 loss: -0.19497311115264893
Batch 42/64 loss: -0.21132469177246094
Batch 43/64 loss: -0.20473384857177734
Batch 44/64 loss: -0.185435950756073
Batch 45/64 loss: -0.20518851280212402
Batch 46/64 loss: -0.2018735408782959
Batch 47/64 loss: -0.18117308616638184
Batch 48/64 loss: -0.19764351844787598
Batch 49/64 loss: -0.15650701522827148
Batch 50/64 loss: -0.20326721668243408
Batch 51/64 loss: -0.1798686981201172
Batch 52/64 loss: -0.2013247013092041
Batch 53/64 loss: -0.18624722957611084
Batch 54/64 loss: -0.19083315134048462
Batch 55/64 loss: -0.2030709981918335
Batch 56/64 loss: -0.2110060453414917
Batch 57/64 loss: -0.20298027992248535
Batch 58/64 loss: -0.1983606219291687
Batch 59/64 loss: -0.17577415704727173
Batch 60/64 loss: -0.2024896740913391
Batch 61/64 loss: -0.18346697092056274
Batch 62/64 loss: -0.1840084195137024
Batch 63/64 loss: -0.1885930299758911
Batch 64/64 loss: -0.19150912761688232
Epoch 81  Train loss: -0.19978882892459046  Val loss: -0.15137554566884778
Epoch 82
-------------------------------
Batch 1/64 loss: -0.1899493932723999
Batch 2/64 loss: -0.19832921028137207
Batch 3/64 loss: -0.20542997121810913
Batch 4/64 loss: -0.2095949649810791
Batch 5/64 loss: -0.18696415424346924
Batch 6/64 loss: -0.1888856291770935
Batch 7/64 loss: -0.17406201362609863
Batch 8/64 loss: -0.1960892677307129
Batch 9/64 loss: -0.19502687454223633
Batch 10/64 loss: -0.20087885856628418
Batch 11/64 loss: -0.19733715057373047
Batch 12/64 loss: -0.20837730169296265
Batch 13/64 loss: -0.2008259892463684
Batch 14/64 loss: -0.1870126724243164
Batch 15/64 loss: -0.1842605471611023
Batch 16/64 loss: -0.21140074729919434
Batch 17/64 loss: -0.21638154983520508
Batch 18/64 loss: -0.20027625560760498
Batch 19/64 loss: -0.20069164037704468
Batch 20/64 loss: -0.18787896633148193
Batch 21/64 loss: -0.22475624084472656
Batch 22/64 loss: -0.19905245304107666
Batch 23/64 loss: -0.22044140100479126
Batch 24/64 loss: -0.2079041600227356
Batch 25/64 loss: -0.18655109405517578
Batch 26/64 loss: -0.18085122108459473
Batch 27/64 loss: -0.21232390403747559
Batch 28/64 loss: -0.183089017868042
Batch 29/64 loss: -0.20324021577835083
Batch 30/64 loss: -0.2015278935432434
Batch 31/64 loss: -0.2175755500793457
Batch 32/64 loss: -0.19104957580566406
Batch 33/64 loss: -0.20115309953689575
Batch 34/64 loss: -0.2206329107284546
Batch 35/64 loss: -0.18771475553512573
Batch 36/64 loss: -0.219199538230896
Batch 37/64 loss: -0.22246801853179932
Batch 38/64 loss: -0.2190127968788147
Batch 39/64 loss: -0.20780271291732788
Batch 40/64 loss: -0.20436573028564453
Batch 41/64 loss: -0.18805831670761108
Batch 42/64 loss: -0.2035810947418213
Batch 43/64 loss: -0.19312238693237305
Batch 44/64 loss: -0.20450276136398315
Batch 45/64 loss: -0.19148242473602295
Batch 46/64 loss: -0.2121390700340271
Batch 47/64 loss: -0.2095385193824768
Batch 48/64 loss: -0.191999614238739
Batch 49/64 loss: -0.19271796941757202
Batch 50/64 loss: -0.18863976001739502
Batch 51/64 loss: -0.19732940196990967
Batch 52/64 loss: -0.20590513944625854
Batch 53/64 loss: -0.2119712233543396
Batch 54/64 loss: -0.21128857135772705
Batch 55/64 loss: -0.21793365478515625
Batch 56/64 loss: -0.2036285400390625
Batch 57/64 loss: -0.21341639757156372
Batch 58/64 loss: -0.1981339454650879
Batch 59/64 loss: -0.18521320819854736
Batch 60/64 loss: -0.21286725997924805
Batch 61/64 loss: -0.2174314260482788
Batch 62/64 loss: -0.19903993606567383
Batch 63/64 loss: -0.1979919672012329
Batch 64/64 loss: -0.2273697853088379
Epoch 82  Train loss: -0.20186392746719659  Val loss: -0.18138183924750365
Epoch 83
-------------------------------
Batch 1/64 loss: -0.21302133798599243
Batch 2/64 loss: -0.19330626726150513
Batch 3/64 loss: -0.1918758749961853
Batch 4/64 loss: -0.20589590072631836
Batch 5/64 loss: -0.21896910667419434
Batch 6/64 loss: -0.21319162845611572
Batch 7/64 loss: -0.21314233541488647
Batch 8/64 loss: -0.19552844762802124
Batch 9/64 loss: -0.21066713333129883
Batch 10/64 loss: -0.21382957696914673
Batch 11/64 loss: -0.20403194427490234
Batch 12/64 loss: -0.22981572151184082
Batch 13/64 loss: -0.20644140243530273
Batch 14/64 loss: -0.21371901035308838
Batch 15/64 loss: -0.2024042010307312
Batch 16/64 loss: -0.21828889846801758
Batch 17/64 loss: -0.19965827465057373
Batch 18/64 loss: -0.1957768201828003
Batch 19/64 loss: -0.21452778577804565
Batch 20/64 loss: -0.19870364665985107
Batch 21/64 loss: -0.18021708726882935
Batch 22/64 loss: -0.19904321432113647
Batch 23/64 loss: -0.17071431875228882
Batch 24/64 loss: -0.17114990949630737
Batch 25/64 loss: -0.1785886287689209
Batch 26/64 loss: -0.2004651427268982
Batch 27/64 loss: -0.19095975160598755
Batch 28/64 loss: -0.21017122268676758
Batch 29/64 loss: -0.18474340438842773
Batch 30/64 loss: -0.20875060558319092
Batch 31/64 loss: -0.19649392366409302
Batch 32/64 loss: -0.22453540563583374
Batch 33/64 loss: -0.2110697627067566
Batch 34/64 loss: -0.19774353504180908
Batch 35/64 loss: -0.1776118278503418
Batch 36/64 loss: -0.21368706226348877
Batch 37/64 loss: -0.19476252794265747
Batch 38/64 loss: -0.21245735883712769
Batch 39/64 loss: -0.19911587238311768
Batch 40/64 loss: -0.20582538843154907
Batch 41/64 loss: -0.21317458152770996
Batch 42/64 loss: -0.18767917156219482
Batch 43/64 loss: -0.1926422119140625
Batch 44/64 loss: -0.20693862438201904
Batch 45/64 loss: -0.19312578439712524
Batch 46/64 loss: -0.154879629611969
Batch 47/64 loss: -0.2134532928466797
Batch 48/64 loss: -0.18909037113189697
Batch 49/64 loss: -0.20571446418762207
Batch 50/64 loss: -0.209175705909729
Batch 51/64 loss: -0.20453858375549316
Batch 52/64 loss: -0.20927822589874268
Batch 53/64 loss: -0.1976090669631958
Batch 54/64 loss: -0.20512866973876953
Batch 55/64 loss: -0.19229036569595337
Batch 56/64 loss: -0.21478784084320068
Batch 57/64 loss: -0.20300966501235962
Batch 58/64 loss: -0.19949519634246826
Batch 59/64 loss: -0.18994873762130737
Batch 60/64 loss: -0.16075998544692993
Batch 61/64 loss: -0.18038010597229004
Batch 62/64 loss: -0.21643131971359253
Batch 63/64 loss: -0.18618899583816528
Batch 64/64 loss: -0.21781516075134277
Epoch 83  Train loss: -0.2003134624630797  Val loss: -0.15856406348677435
Epoch 84
-------------------------------
Batch 1/64 loss: -0.19557082653045654
Batch 2/64 loss: -0.18460583686828613
Batch 3/64 loss: -0.20493102073669434
Batch 4/64 loss: -0.188279390335083
Batch 5/64 loss: -0.1939905285835266
Batch 6/64 loss: -0.21463221311569214
Batch 7/64 loss: -0.1942082643508911
Batch 8/64 loss: -0.19165253639221191
Batch 9/64 loss: -0.18200242519378662
Batch 10/64 loss: -0.21579033136367798
Batch 11/64 loss: -0.2099841833114624
Batch 12/64 loss: -0.21366500854492188
Batch 13/64 loss: -0.20088541507720947
Batch 14/64 loss: -0.21358734369277954
Batch 15/64 loss: -0.20337539911270142
Batch 16/64 loss: -0.22252213954925537
Batch 17/64 loss: -0.2057567834854126
Batch 18/64 loss: -0.22055566310882568
Batch 19/64 loss: -0.2110787034034729
Batch 20/64 loss: -0.20608747005462646
Batch 21/64 loss: -0.20128083229064941
Batch 22/64 loss: -0.20468008518218994
Batch 23/64 loss: -0.2180166244506836
Batch 24/64 loss: -0.20746827125549316
Batch 25/64 loss: -0.22978150844573975
Batch 26/64 loss: -0.1944427490234375
Batch 27/64 loss: -0.20891940593719482
Batch 28/64 loss: -0.22067272663116455
Batch 29/64 loss: -0.2122681736946106
Batch 30/64 loss: -0.19962161779403687
Batch 31/64 loss: -0.19891691207885742
Batch 32/64 loss: -0.20904016494750977
Batch 33/64 loss: -0.21569430828094482
Batch 34/64 loss: -0.19376736879348755
Batch 35/64 loss: -0.21505266427993774
Batch 36/64 loss: -0.19256335496902466
Batch 37/64 loss: -0.1845201849937439
Batch 38/64 loss: -0.20152640342712402
Batch 39/64 loss: -0.20502907037734985
Batch 40/64 loss: -0.22830194234848022
Batch 41/64 loss: -0.19093942642211914
Batch 42/64 loss: -0.220658540725708
Batch 43/64 loss: -0.19955050945281982
Batch 44/64 loss: -0.19666439294815063
Batch 45/64 loss: -0.21092385053634644
Batch 46/64 loss: -0.2309187650680542
Batch 47/64 loss: -0.22194576263427734
Batch 48/64 loss: -0.20922386646270752
Batch 49/64 loss: -0.20911628007888794
Batch 50/64 loss: -0.21139311790466309
Batch 51/64 loss: -0.2212820053100586
Batch 52/64 loss: -0.19772154092788696
Batch 53/64 loss: -0.16327422857284546
Batch 54/64 loss: -0.21596121788024902
Batch 55/64 loss: -0.18350398540496826
Batch 56/64 loss: -0.1976432204246521
Batch 57/64 loss: -0.2094653844833374
Batch 58/64 loss: -0.20082664489746094
Batch 59/64 loss: -0.20634669065475464
Batch 60/64 loss: -0.2028946876525879
Batch 61/64 loss: -0.14919453859329224
Batch 62/64 loss: -0.1744939684867859
Batch 63/64 loss: -0.2059539556503296
Batch 64/64 loss: -0.17447495460510254
Epoch 84  Train loss: -0.20353691998650045  Val loss: -0.14985470980713048
Epoch 85
-------------------------------
Batch 1/64 loss: -0.1908847689628601
Batch 2/64 loss: -0.19607090950012207
Batch 3/64 loss: -0.19561755657196045
Batch 4/64 loss: -0.2005566954612732
Batch 5/64 loss: -0.19635730981826782
Batch 6/64 loss: -0.16631227731704712
Batch 7/64 loss: -0.19415801763534546
Batch 8/64 loss: -0.19866645336151123
Batch 9/64 loss: -0.21439659595489502
Batch 10/64 loss: -0.2113940715789795
Batch 11/64 loss: -0.19917255640029907
Batch 12/64 loss: -0.1912705898284912
Batch 13/64 loss: -0.1992618441581726
Batch 14/64 loss: -0.19194942712783813
Batch 15/64 loss: -0.18369418382644653
Batch 16/64 loss: -0.20542776584625244
Batch 17/64 loss: -0.19833117723464966
Batch 18/64 loss: -0.2017318606376648
Batch 19/64 loss: -0.19968634843826294
Batch 20/64 loss: -0.21194475889205933
Batch 21/64 loss: -0.1801277995109558
Batch 22/64 loss: -0.22377300262451172
Batch 23/64 loss: -0.20619606971740723
Batch 24/64 loss: -0.20877337455749512
Batch 25/64 loss: -0.22932255268096924
Batch 26/64 loss: -0.2033507227897644
Batch 27/64 loss: -0.21421897411346436
Batch 28/64 loss: -0.2199876308441162
Batch 29/64 loss: -0.20536547899246216
Batch 30/64 loss: -0.20779961347579956
Batch 31/64 loss: -0.2031307816505432
Batch 32/64 loss: -0.2173480987548828
Batch 33/64 loss: -0.20198476314544678
Batch 34/64 loss: -0.22831356525421143
Batch 35/64 loss: -0.1925492286682129
Batch 36/64 loss: -0.20216375589370728
Batch 37/64 loss: -0.209430992603302
Batch 38/64 loss: -0.2295582890510559
Batch 39/64 loss: -0.19993984699249268
Batch 40/64 loss: -0.21371984481811523
Batch 41/64 loss: -0.18603748083114624
Batch 42/64 loss: -0.20859098434448242
Batch 43/64 loss: -0.19963675737380981
Batch 44/64 loss: -0.20959407091140747
Batch 45/64 loss: -0.18363821506500244
Batch 46/64 loss: -0.18349218368530273
Batch 47/64 loss: -0.19280201196670532
Batch 48/64 loss: -0.17415225505828857
Batch 49/64 loss: -0.1982775330543518
Batch 50/64 loss: -0.16118913888931274
Batch 51/64 loss: -0.19107329845428467
Batch 52/64 loss: -0.19795143604278564
Batch 53/64 loss: -0.15422862768173218
Batch 54/64 loss: -0.21849387884140015
Batch 55/64 loss: -0.19844114780426025
Batch 56/64 loss: -0.16909098625183105
Batch 57/64 loss: -0.20163536071777344
Batch 58/64 loss: -0.17640674114227295
Batch 59/64 loss: -0.20991671085357666
Batch 60/64 loss: -0.20752239227294922
Batch 61/64 loss: -0.17183160781860352
Batch 62/64 loss: -0.18301832675933838
Batch 63/64 loss: -0.2153063416481018
Batch 64/64 loss: -0.20291352272033691
Epoch 85  Train loss: -0.19903454406588686  Val loss: -0.16791473180567687
Epoch 86
-------------------------------
Batch 1/64 loss: -0.19749969244003296
Batch 2/64 loss: -0.199779212474823
Batch 3/64 loss: -0.20956206321716309
Batch 4/64 loss: -0.20634019374847412
Batch 5/64 loss: -0.22274112701416016
Batch 6/64 loss: -0.20447874069213867
Batch 7/64 loss: -0.20563489198684692
Batch 8/64 loss: -0.19095230102539062
Batch 9/64 loss: -0.18010658025741577
Batch 10/64 loss: -0.21679288148880005
Batch 11/64 loss: -0.19632840156555176
Batch 12/64 loss: -0.19258582592010498
Batch 13/64 loss: -0.20798581838607788
Batch 14/64 loss: -0.20404154062271118
Batch 15/64 loss: -0.2098177671432495
Batch 16/64 loss: -0.20440876483917236
Batch 17/64 loss: -0.19936144351959229
Batch 18/64 loss: -0.18661820888519287
Batch 19/64 loss: -0.21476751565933228
Batch 20/64 loss: -0.17280447483062744
Batch 21/64 loss: -0.19343692064285278
Batch 22/64 loss: -0.20960038900375366
Batch 23/64 loss: -0.2033853530883789
Batch 24/64 loss: -0.1971505880355835
Batch 25/64 loss: -0.18601495027542114
Batch 26/64 loss: -0.19791066646575928
Batch 27/64 loss: -0.20027941465377808
Batch 28/64 loss: -0.20609331130981445
Batch 29/64 loss: -0.20365238189697266
Batch 30/64 loss: -0.20779001712799072
Batch 31/64 loss: -0.20096111297607422
Batch 32/64 loss: -0.20634007453918457
Batch 33/64 loss: -0.22206461429595947
Batch 34/64 loss: -0.20840203762054443
Batch 35/64 loss: -0.19893813133239746
Batch 36/64 loss: -0.18893563747406006
Batch 37/64 loss: -0.19550305604934692
Batch 38/64 loss: -0.20270705223083496
Batch 39/64 loss: -0.20151150226593018
Batch 40/64 loss: -0.19096523523330688
Batch 41/64 loss: -0.20420879125595093
Batch 42/64 loss: -0.21271252632141113
Batch 43/64 loss: -0.18284523487091064
Batch 44/64 loss: -0.18788087368011475
Batch 45/64 loss: -0.19978868961334229
Batch 46/64 loss: -0.19076251983642578
Batch 47/64 loss: -0.20444995164871216
Batch 48/64 loss: -0.1973792314529419
Batch 49/64 loss: -0.18819475173950195
Batch 50/64 loss: -0.21542704105377197
Batch 51/64 loss: -0.18718528747558594
Batch 52/64 loss: -0.21374988555908203
Batch 53/64 loss: -0.20663034915924072
Batch 54/64 loss: -0.20448732376098633
Batch 55/64 loss: -0.21035218238830566
Batch 56/64 loss: -0.2153158187866211
Batch 57/64 loss: -0.21118175983428955
Batch 58/64 loss: -0.2096133828163147
Batch 59/64 loss: -0.19289273023605347
Batch 60/64 loss: -0.22376656532287598
Batch 61/64 loss: -0.2019692063331604
Batch 62/64 loss: -0.18349790573120117
Batch 63/64 loss: -0.1874331831932068
Batch 64/64 loss: -0.2134295105934143
Epoch 86  Train loss: -0.2013497602705862  Val loss: -0.17051953671314463
Epoch 87
-------------------------------
Batch 1/64 loss: -0.20597326755523682
Batch 2/64 loss: -0.20317459106445312
Batch 3/64 loss: -0.2133980393409729
Batch 4/64 loss: -0.20734107494354248
Batch 5/64 loss: -0.22563385963439941
Batch 6/64 loss: -0.2077791690826416
Batch 7/64 loss: -0.22259563207626343
Batch 8/64 loss: -0.23117595911026
Batch 9/64 loss: -0.19943487644195557
Batch 10/64 loss: -0.2120334506034851
Batch 11/64 loss: -0.2025800347328186
Batch 12/64 loss: -0.19988369941711426
Batch 13/64 loss: -0.1758149266242981
Batch 14/64 loss: -0.21763169765472412
Batch 15/64 loss: -0.18765389919281006
Batch 16/64 loss: -0.20406359434127808
Batch 17/64 loss: -0.19542014598846436
Batch 18/64 loss: -0.18888336420059204
Batch 19/64 loss: -0.22828614711761475
Batch 20/64 loss: -0.19625091552734375
Batch 21/64 loss: -0.20537662506103516
Batch 22/64 loss: -0.1807084083557129
Batch 23/64 loss: -0.19068652391433716
Batch 24/64 loss: -0.19459789991378784
Batch 25/64 loss: -0.19730716943740845
Batch 26/64 loss: -0.1923832893371582
Batch 27/64 loss: -0.20703446865081787
Batch 28/64 loss: -0.20362788438796997
Batch 29/64 loss: -0.20866858959197998
Batch 30/64 loss: -0.18828755617141724
Batch 31/64 loss: -0.20328879356384277
Batch 32/64 loss: -0.20688366889953613
Batch 33/64 loss: -0.20580291748046875
Batch 34/64 loss: -0.18022936582565308
Batch 35/64 loss: -0.19343024492263794
Batch 36/64 loss: -0.1813652515411377
Batch 37/64 loss: -0.18342137336730957
Batch 38/64 loss: -0.17017757892608643
Batch 39/64 loss: -0.2163715362548828
Batch 40/64 loss: -0.20792412757873535
Batch 41/64 loss: -0.1992785930633545
Batch 42/64 loss: -0.1964380145072937
Batch 43/64 loss: -0.2086261510848999
Batch 44/64 loss: -0.19923174381256104
Batch 45/64 loss: -0.20863139629364014
Batch 46/64 loss: -0.21592241525650024
Batch 47/64 loss: -0.1963040828704834
Batch 48/64 loss: -0.20706665515899658
Batch 49/64 loss: -0.19632601737976074
Batch 50/64 loss: -0.19975370168685913
Batch 51/64 loss: -0.1871209740638733
Batch 52/64 loss: -0.17419791221618652
Batch 53/64 loss: -0.22243642807006836
Batch 54/64 loss: -0.20738178491592407
Batch 55/64 loss: -0.21699810028076172
Batch 56/64 loss: -0.18504661321640015
Batch 57/64 loss: -0.19056743383407593
Batch 58/64 loss: -0.1874380111694336
Batch 59/64 loss: -0.2115473747253418
Batch 60/64 loss: -0.21970981359481812
Batch 61/64 loss: -0.19491422176361084
Batch 62/64 loss: -0.20720809698104858
Batch 63/64 loss: -0.1953548789024353
Batch 64/64 loss: -0.2280309796333313
Epoch 87  Train loss: -0.2014291022338119  Val loss: -0.16148903791847097
Epoch 88
-------------------------------
Batch 1/64 loss: -0.20675891637802124
Batch 2/64 loss: -0.2106298804283142
Batch 3/64 loss: -0.19820666313171387
Batch 4/64 loss: -0.19948667287826538
Batch 5/64 loss: -0.20983362197875977
Batch 6/64 loss: -0.19785499572753906
Batch 7/64 loss: -0.20741313695907593
Batch 8/64 loss: -0.19669640064239502
Batch 9/64 loss: -0.19154804944992065
Batch 10/64 loss: -0.19110655784606934
Batch 11/64 loss: -0.16288793087005615
Batch 12/64 loss: -0.20279216766357422
Batch 13/64 loss: -0.20047491788864136
Batch 14/64 loss: -0.2042776346206665
Batch 15/64 loss: -0.18327820301055908
Batch 16/64 loss: -0.21199029684066772
Batch 17/64 loss: -0.18732398748397827
Batch 18/64 loss: -0.2088477611541748
Batch 19/64 loss: -0.19913864135742188
Batch 20/64 loss: -0.1999286413192749
Batch 21/64 loss: -0.21920645236968994
Batch 22/64 loss: -0.19546246528625488
Batch 23/64 loss: -0.19595879316329956
Batch 24/64 loss: -0.20348072052001953
Batch 25/64 loss: -0.20310628414154053
Batch 26/64 loss: -0.17303508520126343
Batch 27/64 loss: -0.20822447538375854
Batch 28/64 loss: -0.22007215023040771
Batch 29/64 loss: -0.20747512578964233
Batch 30/64 loss: -0.1983121633529663
Batch 31/64 loss: -0.21920692920684814
Batch 32/64 loss: -0.18759429454803467
Batch 33/64 loss: -0.20132243633270264
Batch 34/64 loss: -0.18631517887115479
Batch 35/64 loss: -0.20809292793273926
Batch 36/64 loss: -0.21762710809707642
Batch 37/64 loss: -0.1997206211090088
Batch 38/64 loss: -0.2197137475013733
Batch 39/64 loss: -0.22074288129806519
Batch 40/64 loss: -0.20771610736846924
Batch 41/64 loss: -0.2175489068031311
Batch 42/64 loss: -0.20217329263687134
Batch 43/64 loss: -0.21500808000564575
Batch 44/64 loss: -0.20652443170547485
Batch 45/64 loss: -0.2128162384033203
Batch 46/64 loss: -0.20423024892807007
Batch 47/64 loss: -0.20896202325820923
Batch 48/64 loss: -0.21934688091278076
Batch 49/64 loss: -0.20430505275726318
Batch 50/64 loss: -0.22048354148864746
Batch 51/64 loss: -0.1794801950454712
Batch 52/64 loss: -0.21744626760482788
Batch 53/64 loss: -0.2004128098487854
Batch 54/64 loss: -0.22186297178268433
Batch 55/64 loss: -0.21949470043182373
Batch 56/64 loss: -0.2059888243675232
Batch 57/64 loss: -0.19225406646728516
Batch 58/64 loss: -0.18902522325515747
Batch 59/64 loss: -0.22048181295394897
Batch 60/64 loss: -0.20092839002609253
Batch 61/64 loss: -0.1871507167816162
Batch 62/64 loss: -0.20589971542358398
Batch 63/64 loss: -0.21222710609436035
Batch 64/64 loss: -0.2036615014076233
Epoch 88  Train loss: -0.20360248860190897  Val loss: -0.15931843891995878
Epoch 89
-------------------------------
Batch 1/64 loss: -0.1974571943283081
Batch 2/64 loss: -0.21913892030715942
Batch 3/64 loss: -0.21069830656051636
Batch 4/64 loss: -0.2048177719116211
Batch 5/64 loss: -0.20552879571914673
Batch 6/64 loss: -0.2040461301803589
Batch 7/64 loss: -0.20989322662353516
Batch 8/64 loss: -0.18519556522369385
Batch 9/64 loss: -0.19715183973312378
Batch 10/64 loss: -0.19757670164108276
Batch 11/64 loss: -0.18506371974945068
Batch 12/64 loss: -0.199085533618927
Batch 13/64 loss: -0.21970438957214355
Batch 14/64 loss: -0.2285059690475464
Batch 15/64 loss: -0.17394667863845825
Batch 16/64 loss: -0.20085179805755615
Batch 17/64 loss: -0.18698865175247192
Batch 18/64 loss: -0.20226502418518066
Batch 19/64 loss: -0.2081781029701233
Batch 20/64 loss: -0.19059544801712036
Batch 21/64 loss: -0.22173309326171875
Batch 22/64 loss: -0.19982457160949707
Batch 23/64 loss: -0.21263718605041504
Batch 24/64 loss: -0.208249032497406
Batch 25/64 loss: -0.20594775676727295
Batch 26/64 loss: -0.20872312784194946
Batch 27/64 loss: -0.20068275928497314
Batch 28/64 loss: -0.17996525764465332
Batch 29/64 loss: -0.2141830325126648
Batch 30/64 loss: -0.2091105580329895
Batch 31/64 loss: -0.22106444835662842
Batch 32/64 loss: -0.23301321268081665
Batch 33/64 loss: -0.21166127920150757
Batch 34/64 loss: -0.2058035135269165
Batch 35/64 loss: -0.20539945363998413
Batch 36/64 loss: -0.2173151969909668
Batch 37/64 loss: -0.22383683919906616
Batch 38/64 loss: -0.18987399339675903
Batch 39/64 loss: -0.21804529428482056
Batch 40/64 loss: -0.209694504737854
Batch 41/64 loss: -0.21183443069458008
Batch 42/64 loss: -0.1971275806427002
Batch 43/64 loss: -0.21332114934921265
Batch 44/64 loss: -0.2009652853012085
Batch 45/64 loss: -0.19391846656799316
Batch 46/64 loss: -0.18285208940505981
Batch 47/64 loss: -0.1577804684638977
Batch 48/64 loss: -0.21112984418869019
Batch 49/64 loss: -0.2079237699508667
Batch 50/64 loss: -0.21443164348602295
Batch 51/64 loss: -0.20831835269927979
Batch 52/64 loss: -0.206581711769104
Batch 53/64 loss: -0.23803073167800903
Batch 54/64 loss: -0.20869648456573486
Batch 55/64 loss: -0.21079611778259277
Batch 56/64 loss: -0.2079000473022461
Batch 57/64 loss: -0.20485037565231323
Batch 58/64 loss: -0.19908279180526733
Batch 59/64 loss: -0.1909605860710144
Batch 60/64 loss: -0.19837021827697754
Batch 61/64 loss: -0.23218274116516113
Batch 62/64 loss: -0.2247070074081421
Batch 63/64 loss: -0.22600698471069336
Batch 64/64 loss: -0.2152579426765442
Epoch 89  Train loss: -0.20600260729883232  Val loss: -0.16206792171058787
Epoch 90
-------------------------------
Batch 1/64 loss: -0.19377177953720093
Batch 2/64 loss: -0.19018125534057617
Batch 3/64 loss: -0.21016395092010498
Batch 4/64 loss: -0.2133353352546692
Batch 5/64 loss: -0.22391986846923828
Batch 6/64 loss: -0.20903998613357544
Batch 7/64 loss: -0.2161717414855957
Batch 8/64 loss: -0.1946568489074707
Batch 9/64 loss: -0.20915508270263672
Batch 10/64 loss: -0.19022756814956665
Batch 11/64 loss: -0.20845317840576172
Batch 12/64 loss: -0.20689642429351807
Batch 13/64 loss: -0.20216000080108643
Batch 14/64 loss: -0.21754908561706543
Batch 15/64 loss: -0.21206164360046387
Batch 16/64 loss: -0.22162532806396484
Batch 17/64 loss: -0.23315495252609253
Batch 18/64 loss: -0.20275866985321045
Batch 19/64 loss: -0.20857512950897217
Batch 20/64 loss: -0.20960474014282227
Batch 21/64 loss: -0.22725409269332886
Batch 22/64 loss: -0.2296677827835083
Batch 23/64 loss: -0.23272180557250977
Batch 24/64 loss: -0.19149965047836304
Batch 25/64 loss: -0.20775765180587769
Batch 26/64 loss: -0.18693989515304565
Batch 27/64 loss: -0.23552286624908447
Batch 28/64 loss: -0.20467805862426758
Batch 29/64 loss: -0.22092485427856445
Batch 30/64 loss: -0.2005968689918518
Batch 31/64 loss: -0.24297094345092773
Batch 32/64 loss: -0.19741755723953247
Batch 33/64 loss: -0.19709616899490356
Batch 34/64 loss: -0.2036866545677185
Batch 35/64 loss: -0.2178436517715454
Batch 36/64 loss: -0.2109704613685608
Batch 37/64 loss: -0.22226262092590332
Batch 38/64 loss: -0.20518463850021362
Batch 39/64 loss: -0.2133767008781433
Batch 40/64 loss: -0.20788872241973877
Batch 41/64 loss: -0.22285562753677368
Batch 42/64 loss: -0.21636968851089478
Batch 43/64 loss: -0.21650201082229614
Batch 44/64 loss: -0.20675170421600342
Batch 45/64 loss: -0.216516375541687
Batch 46/64 loss: -0.22689419984817505
Batch 47/64 loss: -0.20034128427505493
Batch 48/64 loss: -0.20867139101028442
Batch 49/64 loss: -0.22679877281188965
Batch 50/64 loss: -0.21218568086624146
Batch 51/64 loss: -0.21130287647247314
Batch 52/64 loss: -0.22667407989501953
Batch 53/64 loss: -0.2113945484161377
Batch 54/64 loss: -0.20885366201400757
Batch 55/64 loss: -0.21540570259094238
Batch 56/64 loss: -0.1894763708114624
Batch 57/64 loss: -0.19282156229019165
Batch 58/64 loss: -0.20141756534576416
Batch 59/64 loss: -0.22496432065963745
Batch 60/64 loss: -0.21162182092666626
Batch 61/64 loss: -0.2021259069442749
Batch 62/64 loss: -0.22486728429794312
Batch 63/64 loss: -0.20853334665298462
Batch 64/64 loss: -0.2429673671722412
Epoch 90  Train loss: -0.2116909101897595  Val loss: -0.19327319528638703
Saving best model, epoch: 90
Epoch 91
-------------------------------
Batch 1/64 loss: -0.20432829856872559
Batch 2/64 loss: -0.22806674242019653
Batch 3/64 loss: -0.20870959758758545
Batch 4/64 loss: -0.22600537538528442
Batch 5/64 loss: -0.20535969734191895
Batch 6/64 loss: -0.21333885192871094
Batch 7/64 loss: -0.19465339183807373
Batch 8/64 loss: -0.19643044471740723
Batch 9/64 loss: -0.20273929834365845
Batch 10/64 loss: -0.21327722072601318
Batch 11/64 loss: -0.22339344024658203
Batch 12/64 loss: -0.22735148668289185
Batch 13/64 loss: -0.207508385181427
Batch 14/64 loss: -0.22809690237045288
Batch 15/64 loss: -0.19188940525054932
Batch 16/64 loss: -0.19005423784255981
Batch 17/64 loss: -0.229996919631958
Batch 18/64 loss: -0.2127818465232849
Batch 19/64 loss: -0.20876801013946533
Batch 20/64 loss: -0.1952095627784729
Batch 21/64 loss: -0.23510998487472534
Batch 22/64 loss: -0.22432047128677368
Batch 23/64 loss: -0.19087457656860352
Batch 24/64 loss: -0.2162262201309204
Batch 25/64 loss: -0.20460140705108643
Batch 26/64 loss: -0.2181033492088318
Batch 27/64 loss: -0.19974380731582642
Batch 28/64 loss: -0.22335636615753174
Batch 29/64 loss: -0.2092214822769165
Batch 30/64 loss: -0.21813172101974487
Batch 31/64 loss: -0.18010342121124268
Batch 32/64 loss: -0.19470655918121338
Batch 33/64 loss: -0.2102147340774536
Batch 34/64 loss: -0.19845116138458252
Batch 35/64 loss: -0.21175789833068848
Batch 36/64 loss: -0.22839736938476562
Batch 37/64 loss: -0.22800838947296143
Batch 38/64 loss: -0.20500630140304565
Batch 39/64 loss: -0.22070837020874023
Batch 40/64 loss: -0.21858644485473633
Batch 41/64 loss: -0.21622860431671143
Batch 42/64 loss: -0.1870570182800293
Batch 43/64 loss: -0.20661956071853638
Batch 44/64 loss: -0.22370684146881104
Batch 45/64 loss: -0.21952694654464722
Batch 46/64 loss: -0.22084969282150269
Batch 47/64 loss: -0.21271753311157227
Batch 48/64 loss: -0.20223355293273926
Batch 49/64 loss: -0.2025774121284485
Batch 50/64 loss: -0.21612554788589478
Batch 51/64 loss: -0.19314706325531006
Batch 52/64 loss: -0.19906175136566162
Batch 53/64 loss: -0.21056509017944336
Batch 54/64 loss: -0.22413313388824463
Batch 55/64 loss: -0.2103162407875061
Batch 56/64 loss: -0.21396881341934204
Batch 57/64 loss: -0.18685561418533325
Batch 58/64 loss: -0.18940454721450806
Batch 59/64 loss: -0.20721155405044556
Batch 60/64 loss: -0.2024834156036377
Batch 61/64 loss: -0.19798994064331055
Batch 62/64 loss: -0.20907622575759888
Batch 63/64 loss: -0.22479552030563354
Batch 64/64 loss: -0.21010422706604004
Epoch 91  Train loss: -0.20984814026776483  Val loss: -0.183485019862447
Epoch 92
-------------------------------
Batch 1/64 loss: -0.2139567732810974
Batch 2/64 loss: -0.22317498922348022
Batch 3/64 loss: -0.2161775827407837
Batch 4/64 loss: -0.21467936038970947
Batch 5/64 loss: -0.20215439796447754
Batch 6/64 loss: -0.2162703275680542
Batch 7/64 loss: -0.21291178464889526
Batch 8/64 loss: -0.20345747470855713
Batch 9/64 loss: -0.22235983610153198
Batch 10/64 loss: -0.22717750072479248
Batch 11/64 loss: -0.20327448844909668
Batch 12/64 loss: -0.19602346420288086
Batch 13/64 loss: -0.1980137825012207
Batch 14/64 loss: -0.2291470766067505
Batch 15/64 loss: -0.21297091245651245
Batch 16/64 loss: -0.18786340951919556
Batch 17/64 loss: -0.21647334098815918
Batch 18/64 loss: -0.1941014528274536
Batch 19/64 loss: -0.2204534411430359
Batch 20/64 loss: -0.2142425775527954
Batch 21/64 loss: -0.2080707550048828
Batch 22/64 loss: -0.19534432888031006
Batch 23/64 loss: -0.18049311637878418
Batch 24/64 loss: -0.2190570831298828
Batch 25/64 loss: -0.19787448644638062
Batch 26/64 loss: -0.21103417873382568
Batch 27/64 loss: -0.2127310037612915
Batch 28/64 loss: -0.21102404594421387
Batch 29/64 loss: -0.20343977212905884
Batch 30/64 loss: -0.20048081874847412
Batch 31/64 loss: -0.20288896560668945
Batch 32/64 loss: -0.18208807706832886
Batch 33/64 loss: -0.20498543977737427
Batch 34/64 loss: -0.2244417667388916
Batch 35/64 loss: -0.19930893182754517
Batch 36/64 loss: -0.17374080419540405
Batch 37/64 loss: -0.2097693681716919
Batch 38/64 loss: -0.182561457157135
Batch 39/64 loss: -0.20300531387329102
Batch 40/64 loss: -0.19144099950790405
Batch 41/64 loss: -0.2144557237625122
Batch 42/64 loss: -0.19361263513565063
Batch 43/64 loss: -0.19638842344284058
Batch 44/64 loss: -0.2283276915550232
Batch 45/64 loss: -0.19950371980667114
Batch 46/64 loss: -0.2122948169708252
Batch 47/64 loss: -0.19626885652542114
Batch 48/64 loss: -0.19044053554534912
Batch 49/64 loss: -0.19412761926651
Batch 50/64 loss: -0.19287806749343872
Batch 51/64 loss: -0.20908188819885254
Batch 52/64 loss: -0.21001464128494263
Batch 53/64 loss: -0.2166842222213745
Batch 54/64 loss: -0.2002321481704712
Batch 55/64 loss: -0.21225357055664062
Batch 56/64 loss: -0.17770159244537354
Batch 57/64 loss: -0.20800858736038208
Batch 58/64 loss: -0.15752923488616943
Batch 59/64 loss: -0.20783287286758423
Batch 60/64 loss: -0.20689094066619873
Batch 61/64 loss: -0.20000600814819336
Batch 62/64 loss: -0.19879913330078125
Batch 63/64 loss: -0.2108548879623413
Batch 64/64 loss: -0.20946192741394043
Epoch 92  Train loss: -0.20439135607551126  Val loss: -0.14784243221545137
Epoch 93
-------------------------------
Batch 1/64 loss: -0.20565223693847656
Batch 2/64 loss: -0.2086222767829895
Batch 3/64 loss: -0.23078703880310059
Batch 4/64 loss: -0.2044161558151245
Batch 5/64 loss: -0.21187108755111694
Batch 6/64 loss: -0.22765815258026123
Batch 7/64 loss: -0.2176448106765747
Batch 8/64 loss: -0.21971780061721802
Batch 9/64 loss: -0.19943737983703613
Batch 10/64 loss: -0.19864106178283691
Batch 11/64 loss: -0.204179048538208
Batch 12/64 loss: -0.2030881643295288
Batch 13/64 loss: -0.20243656635284424
Batch 14/64 loss: -0.21012920141220093
Batch 15/64 loss: -0.19222986698150635
Batch 16/64 loss: -0.19845861196517944
Batch 17/64 loss: -0.22690826654434204
Batch 18/64 loss: -0.2234399914741516
Batch 19/64 loss: -0.2054992914199829
Batch 20/64 loss: -0.22986090183258057
Batch 21/64 loss: -0.20137786865234375
Batch 22/64 loss: -0.19615286588668823
Batch 23/64 loss: -0.20334231853485107
Batch 24/64 loss: -0.1969926357269287
Batch 25/64 loss: -0.19841045141220093
Batch 26/64 loss: -0.22262990474700928
Batch 27/64 loss: -0.2131480574607849
Batch 28/64 loss: -0.20517975091934204
Batch 29/64 loss: -0.20917147397994995
Batch 30/64 loss: -0.20851808786392212
Batch 31/64 loss: -0.20700347423553467
Batch 32/64 loss: -0.22601252794265747
Batch 33/64 loss: -0.21984261274337769
Batch 34/64 loss: -0.2010013461112976
Batch 35/64 loss: -0.21041911840438843
Batch 36/64 loss: -0.2058669924736023
Batch 37/64 loss: -0.21268773078918457
Batch 38/64 loss: -0.18516921997070312
Batch 39/64 loss: -0.19000369310379028
Batch 40/64 loss: -0.18075543642044067
Batch 41/64 loss: -0.18710291385650635
Batch 42/64 loss: -0.20575058460235596
Batch 43/64 loss: -0.20194441080093384
Batch 44/64 loss: -0.2010188102722168
Batch 45/64 loss: -0.210027813911438
Batch 46/64 loss: -0.22163963317871094
Batch 47/64 loss: -0.2004871964454651
Batch 48/64 loss: -0.20734935998916626
Batch 49/64 loss: -0.2090216875076294
Batch 50/64 loss: -0.21309030055999756
Batch 51/64 loss: -0.2321549654006958
Batch 52/64 loss: -0.18407833576202393
Batch 53/64 loss: -0.22120428085327148
Batch 54/64 loss: -0.2225266695022583
Batch 55/64 loss: -0.1922244429588318
Batch 56/64 loss: -0.198622465133667
Batch 57/64 loss: -0.22090160846710205
Batch 58/64 loss: -0.22374379634857178
Batch 59/64 loss: -0.21352720260620117
Batch 60/64 loss: -0.21991431713104248
Batch 61/64 loss: -0.20517945289611816
Batch 62/64 loss: -0.20463240146636963
Batch 63/64 loss: -0.2323845624923706
Batch 64/64 loss: -0.23883897066116333
Epoch 93  Train loss: -0.20897286148632274  Val loss: -0.1810986135423798
Epoch 94
-------------------------------
Batch 1/64 loss: -0.2134612798690796
Batch 2/64 loss: -0.20969319343566895
Batch 3/64 loss: -0.2236342430114746
Batch 4/64 loss: -0.20108556747436523
Batch 5/64 loss: -0.21485590934753418
Batch 6/64 loss: -0.21663689613342285
Batch 7/64 loss: -0.20735347270965576
Batch 8/64 loss: -0.23592215776443481
Batch 9/64 loss: -0.21603244543075562
Batch 10/64 loss: -0.20901185274124146
Batch 11/64 loss: -0.2165898084640503
Batch 12/64 loss: -0.21473771333694458
Batch 13/64 loss: -0.17968106269836426
Batch 14/64 loss: -0.202589213848114
Batch 15/64 loss: -0.22190213203430176
Batch 16/64 loss: -0.20512419939041138
Batch 17/64 loss: -0.2225854992866516
Batch 18/64 loss: -0.21326851844787598
Batch 19/64 loss: -0.225053608417511
Batch 20/64 loss: -0.19956201314926147
Batch 21/64 loss: -0.20545697212219238
Batch 22/64 loss: -0.19809013605117798
Batch 23/64 loss: -0.2281738519668579
Batch 24/64 loss: -0.17047852277755737
Batch 25/64 loss: -0.1690990924835205
Batch 26/64 loss: -0.19201862812042236
Batch 27/64 loss: -0.21005183458328247
Batch 28/64 loss: -0.22877579927444458
Batch 29/64 loss: -0.20226764678955078
Batch 30/64 loss: -0.19583088159561157
Batch 31/64 loss: -0.21041399240493774
Batch 32/64 loss: -0.21336990594863892
Batch 33/64 loss: -0.21761810779571533
Batch 34/64 loss: -0.21557295322418213
Batch 35/64 loss: -0.19454777240753174
Batch 36/64 loss: -0.205535888671875
Batch 37/64 loss: -0.2189394235610962
Batch 38/64 loss: -0.2231196165084839
Batch 39/64 loss: -0.1819765567779541
Batch 40/64 loss: -0.21812093257904053
Batch 41/64 loss: -0.20030415058135986
Batch 42/64 loss: -0.23481154441833496
Batch 43/64 loss: -0.20018059015274048
Batch 44/64 loss: -0.21254682540893555
Batch 45/64 loss: -0.22467374801635742
Batch 46/64 loss: -0.19185465574264526
Batch 47/64 loss: -0.18972373008728027
Batch 48/64 loss: -0.23225849866867065
Batch 49/64 loss: -0.20943337678909302
Batch 50/64 loss: -0.213034987449646
Batch 51/64 loss: -0.213822603225708
Batch 52/64 loss: -0.22792690992355347
Batch 53/64 loss: -0.18979555368423462
Batch 54/64 loss: -0.19514095783233643
Batch 55/64 loss: -0.22304373979568481
Batch 56/64 loss: -0.20915424823760986
Batch 57/64 loss: -0.2016594409942627
Batch 58/64 loss: -0.23014622926712036
Batch 59/64 loss: -0.19721567630767822
Batch 60/64 loss: -0.21040493249893188
Batch 61/64 loss: -0.21516811847686768
Batch 62/64 loss: -0.20304298400878906
Batch 63/64 loss: -0.209233820438385
Batch 64/64 loss: -0.22581076622009277
Epoch 94  Train loss: -0.2094458776361802  Val loss: -0.1668603831959754
Epoch 95
-------------------------------
Batch 1/64 loss: -0.23560690879821777
Batch 2/64 loss: -0.20313167572021484
Batch 3/64 loss: -0.1971021294593811
Batch 4/64 loss: -0.22581696510314941
Batch 5/64 loss: -0.20029854774475098
Batch 6/64 loss: -0.22991979122161865
Batch 7/64 loss: -0.22170990705490112
Batch 8/64 loss: -0.20466482639312744
Batch 9/64 loss: -0.20881319046020508
Batch 10/64 loss: -0.20616018772125244
Batch 11/64 loss: -0.2336212396621704
Batch 12/64 loss: -0.1996048092842102
Batch 13/64 loss: -0.19796639680862427
Batch 14/64 loss: -0.21569466590881348
Batch 15/64 loss: -0.21890074014663696
Batch 16/64 loss: -0.2283540964126587
Batch 17/64 loss: -0.2228899598121643
Batch 18/64 loss: -0.20949149131774902
Batch 19/64 loss: -0.19966328144073486
Batch 20/64 loss: -0.19690775871276855
Batch 21/64 loss: -0.21413010358810425
Batch 22/64 loss: -0.2063547968864441
Batch 23/64 loss: -0.22103792428970337
Batch 24/64 loss: -0.21848446130752563
Batch 25/64 loss: -0.22183239459991455
Batch 26/64 loss: -0.21889209747314453
Batch 27/64 loss: -0.23017090559005737
Batch 28/64 loss: -0.2054418921470642
Batch 29/64 loss: -0.20775991678237915
Batch 30/64 loss: -0.22409963607788086
Batch 31/64 loss: -0.21895664930343628
Batch 32/64 loss: -0.22298669815063477
Batch 33/64 loss: -0.20974719524383545
Batch 34/64 loss: -0.21345847845077515
Batch 35/64 loss: -0.20301562547683716
Batch 36/64 loss: -0.18345153331756592
Batch 37/64 loss: -0.2161870002746582
Batch 38/64 loss: -0.21788936853408813
Batch 39/64 loss: -0.21639353036880493
Batch 40/64 loss: -0.2268754243850708
Batch 41/64 loss: -0.2239304780960083
Batch 42/64 loss: -0.2128756046295166
Batch 43/64 loss: -0.21347582340240479
Batch 44/64 loss: -0.22745811939239502
Batch 45/64 loss: -0.20035487413406372
Batch 46/64 loss: -0.20875555276870728
Batch 47/64 loss: -0.23507791757583618
Batch 48/64 loss: -0.21793699264526367
Batch 49/64 loss: -0.22380602359771729
Batch 50/64 loss: -0.21353000402450562
Batch 51/64 loss: -0.1996062994003296
Batch 52/64 loss: -0.16154342889785767
Batch 53/64 loss: -0.2204635739326477
Batch 54/64 loss: -0.2282336950302124
Batch 55/64 loss: -0.21976113319396973
Batch 56/64 loss: -0.2048020362854004
Batch 57/64 loss: -0.2167065143585205
Batch 58/64 loss: -0.22276699542999268
Batch 59/64 loss: -0.22692769765853882
Batch 60/64 loss: -0.22077447175979614
Batch 61/64 loss: -0.183579683303833
Batch 62/64 loss: -0.19191640615463257
Batch 63/64 loss: -0.2283206582069397
Batch 64/64 loss: -0.21315371990203857
Epoch 95  Train loss: -0.2135835839252846  Val loss: -0.17065618742782226
Epoch 96
-------------------------------
Batch 1/64 loss: -0.22063350677490234
Batch 2/64 loss: -0.20373684167861938
Batch 3/64 loss: -0.2015974521636963
Batch 4/64 loss: -0.22702717781066895
Batch 5/64 loss: -0.20736795663833618
Batch 6/64 loss: -0.21903175115585327
Batch 7/64 loss: -0.19807034730911255
Batch 8/64 loss: -0.22433871030807495
Batch 9/64 loss: -0.20412588119506836
Batch 10/64 loss: -0.20734649896621704
Batch 11/64 loss: -0.19251245260238647
Batch 12/64 loss: -0.21354931592941284
Batch 13/64 loss: -0.1973138451576233
Batch 14/64 loss: -0.1926175355911255
Batch 15/64 loss: -0.21366316080093384
Batch 16/64 loss: -0.20798444747924805
Batch 17/64 loss: -0.21458590030670166
Batch 18/64 loss: -0.21398711204528809
Batch 19/64 loss: -0.20242244005203247
Batch 20/64 loss: -0.20525586605072021
Batch 21/64 loss: -0.2012803554534912
Batch 22/64 loss: -0.23553764820098877
Batch 23/64 loss: -0.2277146577835083
Batch 24/64 loss: -0.2209363579750061
Batch 25/64 loss: -0.20684188604354858
Batch 26/64 loss: -0.21099352836608887
Batch 27/64 loss: -0.233725905418396
Batch 28/64 loss: -0.20748811960220337
Batch 29/64 loss: -0.2136087417602539
Batch 30/64 loss: -0.2198103666305542
Batch 31/64 loss: -0.20265382528305054
Batch 32/64 loss: -0.20866703987121582
Batch 33/64 loss: -0.23199361562728882
Batch 34/64 loss: -0.1921483874320984
Batch 35/64 loss: -0.21011412143707275
Batch 36/64 loss: -0.21387195587158203
Batch 37/64 loss: -0.21308618783950806
Batch 38/64 loss: -0.1833907961845398
Batch 39/64 loss: -0.2227545976638794
Batch 40/64 loss: -0.20657610893249512
Batch 41/64 loss: -0.21349143981933594
Batch 42/64 loss: -0.2225170135498047
Batch 43/64 loss: -0.21374374628067017
Batch 44/64 loss: -0.1926087737083435
Batch 45/64 loss: -0.21275317668914795
Batch 46/64 loss: -0.21690189838409424
Batch 47/64 loss: -0.2235262393951416
Batch 48/64 loss: -0.21716225147247314
Batch 49/64 loss: -0.23376715183258057
Batch 50/64 loss: -0.23441511392593384
Batch 51/64 loss: -0.2018604278564453
Batch 52/64 loss: -0.20796269178390503
Batch 53/64 loss: -0.2334275245666504
Batch 54/64 loss: -0.22059834003448486
Batch 55/64 loss: -0.21820580959320068
Batch 56/64 loss: -0.21010082960128784
Batch 57/64 loss: -0.23484671115875244
Batch 58/64 loss: -0.22208964824676514
Batch 59/64 loss: -0.19797277450561523
Batch 60/64 loss: -0.23043352365493774
Batch 61/64 loss: -0.22908121347427368
Batch 62/64 loss: -0.21705949306488037
Batch 63/64 loss: -0.21963751316070557
Batch 64/64 loss: -0.1911049485206604
Epoch 96  Train loss: -0.2132996771849838  Val loss: -0.1887592744991132
Epoch 97
-------------------------------
Batch 1/64 loss: -0.1864582896232605
Batch 2/64 loss: -0.2280227541923523
Batch 3/64 loss: -0.2122427225112915
Batch 4/64 loss: -0.21127116680145264
Batch 5/64 loss: -0.23071956634521484
Batch 6/64 loss: -0.2253737449645996
Batch 7/64 loss: -0.21289879083633423
Batch 8/64 loss: -0.22518116235733032
Batch 9/64 loss: -0.20949256420135498
Batch 10/64 loss: -0.22785496711730957
Batch 11/64 loss: -0.22514277696609497
Batch 12/64 loss: -0.21148240566253662
Batch 13/64 loss: -0.2254197597503662
Batch 14/64 loss: -0.23605448007583618
Batch 15/64 loss: -0.2219454050064087
Batch 16/64 loss: -0.19661486148834229
Batch 17/64 loss: -0.23662590980529785
Batch 18/64 loss: -0.20093929767608643
Batch 19/64 loss: -0.19644445180892944
Batch 20/64 loss: -0.22112315893173218
Batch 21/64 loss: -0.2266552448272705
Batch 22/64 loss: -0.21664667129516602
Batch 23/64 loss: -0.22102993726730347
Batch 24/64 loss: -0.2065466046333313
Batch 25/64 loss: -0.2011430263519287
Batch 26/64 loss: -0.22191005945205688
Batch 27/64 loss: -0.2205127477645874
Batch 28/64 loss: -0.2188509702682495
Batch 29/64 loss: -0.19693350791931152
Batch 30/64 loss: -0.20615482330322266
Batch 31/64 loss: -0.21243029832839966
Batch 32/64 loss: -0.20040345191955566
Batch 33/64 loss: -0.2157745361328125
Batch 34/64 loss: -0.18746602535247803
Batch 35/64 loss: -0.21468377113342285
Batch 36/64 loss: -0.21764272451400757
Batch 37/64 loss: -0.23011213541030884
Batch 38/64 loss: -0.19843560457229614
Batch 39/64 loss: -0.20853149890899658
Batch 40/64 loss: -0.20738792419433594
Batch 41/64 loss: -0.21282529830932617
Batch 42/64 loss: -0.21391361951828003
Batch 43/64 loss: -0.2261715531349182
Batch 44/64 loss: -0.22186481952667236
Batch 45/64 loss: -0.20773816108703613
Batch 46/64 loss: -0.2274101972579956
Batch 47/64 loss: -0.22055107355117798
Batch 48/64 loss: -0.19431668519973755
Batch 49/64 loss: -0.23124319314956665
Batch 50/64 loss: -0.20554393529891968
Batch 51/64 loss: -0.21056431531906128
Batch 52/64 loss: -0.21163678169250488
Batch 53/64 loss: -0.21966028213500977
Batch 54/64 loss: -0.20912033319473267
Batch 55/64 loss: -0.22898578643798828
Batch 56/64 loss: -0.1948152780532837
Batch 57/64 loss: -0.21007364988327026
Batch 58/64 loss: -0.2216324806213379
Batch 59/64 loss: -0.21873825788497925
Batch 60/64 loss: -0.2272183895111084
Batch 61/64 loss: -0.24536532163619995
Batch 62/64 loss: -0.21283859014511108
Batch 63/64 loss: -0.2245965600013733
Batch 64/64 loss: -0.21741485595703125
Epoch 97  Train loss: -0.21537953732060452  Val loss: -0.18609424780324563
Epoch 98
-------------------------------
Batch 1/64 loss: -0.19571250677108765
Batch 2/64 loss: -0.22962182760238647
Batch 3/64 loss: -0.21041107177734375
Batch 4/64 loss: -0.22887253761291504
Batch 5/64 loss: -0.23030370473861694
Batch 6/64 loss: -0.22072327136993408
Batch 7/64 loss: -0.21607917547225952
Batch 8/64 loss: -0.21646565198898315
Batch 9/64 loss: -0.2291429042816162
Batch 10/64 loss: -0.2248910665512085
Batch 11/64 loss: -0.18911725282669067
Batch 12/64 loss: -0.22802454233169556
Batch 13/64 loss: -0.2187480330467224
Batch 14/64 loss: -0.21982824802398682
Batch 15/64 loss: -0.23124849796295166
Batch 16/64 loss: -0.2421283721923828
Batch 17/64 loss: -0.21091049909591675
Batch 18/64 loss: -0.22814583778381348
Batch 19/64 loss: -0.20815473794937134
Batch 20/64 loss: -0.2181086540222168
Batch 21/64 loss: -0.20296716690063477
Batch 22/64 loss: -0.19770216941833496
Batch 23/64 loss: -0.22623419761657715
Batch 24/64 loss: -0.23177194595336914
Batch 25/64 loss: -0.2237868309020996
Batch 26/64 loss: -0.2081538438796997
Batch 27/64 loss: -0.22306203842163086
Batch 28/64 loss: -0.22800934314727783
Batch 29/64 loss: -0.21237558126449585
Batch 30/64 loss: -0.2036130428314209
Batch 31/64 loss: -0.16926097869873047
Batch 32/64 loss: -0.20335960388183594
Batch 33/64 loss: -0.22483563423156738
Batch 34/64 loss: -0.22370260953903198
Batch 35/64 loss: -0.22427862882614136
Batch 36/64 loss: -0.2340456247329712
Batch 37/64 loss: -0.24458229541778564
Batch 38/64 loss: -0.2211281657218933
Batch 39/64 loss: -0.21220743656158447
Batch 40/64 loss: -0.2048790454864502
Batch 41/64 loss: -0.24665892124176025
Batch 42/64 loss: -0.22311484813690186
Batch 43/64 loss: -0.22760707139968872
Batch 44/64 loss: -0.2268642783164978
Batch 45/64 loss: -0.22446578741073608
Batch 46/64 loss: -0.23422110080718994
Batch 47/64 loss: -0.22393393516540527
Batch 48/64 loss: -0.22739332914352417
Batch 49/64 loss: -0.22610682249069214
Batch 50/64 loss: -0.21284419298171997
Batch 51/64 loss: -0.22293812036514282
Batch 52/64 loss: -0.18152618408203125
Batch 53/64 loss: -0.20481503009796143
Batch 54/64 loss: -0.23240625858306885
Batch 55/64 loss: -0.2082662582397461
Batch 56/64 loss: -0.19757121801376343
Batch 57/64 loss: -0.2152811884880066
Batch 58/64 loss: -0.19928514957427979
Batch 59/64 loss: -0.20820695161819458
Batch 60/64 loss: -0.20786166191101074
Batch 61/64 loss: -0.20799791812896729
Batch 62/64 loss: -0.23108011484146118
Batch 63/64 loss: -0.19582825899124146
Batch 64/64 loss: -0.20845234394073486
Epoch 98  Train loss: -0.21739934799717922  Val loss: -0.1818934113708968
Epoch 99
-------------------------------
Batch 1/64 loss: -0.20205533504486084
Batch 2/64 loss: -0.22606605291366577
Batch 3/64 loss: -0.19476062059402466
Batch 4/64 loss: -0.225117027759552
Batch 5/64 loss: -0.21318787336349487
Batch 6/64 loss: -0.22078633308410645
Batch 7/64 loss: -0.21282607316970825
Batch 8/64 loss: -0.2218506932258606
Batch 9/64 loss: -0.2151806354522705
Batch 10/64 loss: -0.20611917972564697
Batch 11/64 loss: -0.23096811771392822
Batch 12/64 loss: -0.23282098770141602
Batch 13/64 loss: -0.2247176170349121
Batch 14/64 loss: -0.2038525938987732
Batch 15/64 loss: -0.2338014841079712
Batch 16/64 loss: -0.23230350017547607
Batch 17/64 loss: -0.2026994824409485
Batch 18/64 loss: -0.22198188304901123
Batch 19/64 loss: -0.2186110019683838
Batch 20/64 loss: -0.22556233406066895
Batch 21/64 loss: -0.21217530965805054
Batch 22/64 loss: -0.2024720311164856
Batch 23/64 loss: -0.22755789756774902
Batch 24/64 loss: -0.2033989429473877
Batch 25/64 loss: -0.22599774599075317
Batch 26/64 loss: -0.20305609703063965
Batch 27/64 loss: -0.2279704213142395
Batch 28/64 loss: -0.2175532579421997
Batch 29/64 loss: -0.22178059816360474
Batch 30/64 loss: -0.21908044815063477
Batch 31/64 loss: -0.21834540367126465
Batch 32/64 loss: -0.20110517740249634
Batch 33/64 loss: -0.19274985790252686
Batch 34/64 loss: -0.23142695426940918
Batch 35/64 loss: -0.23181945085525513
Batch 36/64 loss: -0.20728826522827148
Batch 37/64 loss: -0.23211050033569336
Batch 38/64 loss: -0.24206054210662842
Batch 39/64 loss: -0.22421860694885254
Batch 40/64 loss: -0.24002152681350708
Batch 41/64 loss: -0.21509718894958496
Batch 42/64 loss: -0.23801225423812866
Batch 43/64 loss: -0.22035229206085205
Batch 44/64 loss: -0.22710120677947998
Batch 45/64 loss: -0.2157934308052063
Batch 46/64 loss: -0.20917725563049316
Batch 47/64 loss: -0.21522992849349976
Batch 48/64 loss: -0.2124563455581665
Batch 49/64 loss: -0.2249065637588501
Batch 50/64 loss: -0.22738754749298096
Batch 51/64 loss: -0.22528845071792603
Batch 52/64 loss: -0.21196579933166504
Batch 53/64 loss: -0.20837122201919556
Batch 54/64 loss: -0.19612473249435425
Batch 55/64 loss: -0.21878403425216675
Batch 56/64 loss: -0.21272218227386475
Batch 57/64 loss: -0.18940943479537964
Batch 58/64 loss: -0.21239042282104492
Batch 59/64 loss: -0.22398978471755981
Batch 60/64 loss: -0.23435211181640625
Batch 61/64 loss: -0.20178961753845215
Batch 62/64 loss: -0.22026950120925903
Batch 63/64 loss: -0.1965160369873047
Batch 64/64 loss: -0.2003920078277588
Epoch 99  Train loss: -0.21733708568647797  Val loss: -0.15907499532109684
Epoch 100
-------------------------------
Batch 1/64 loss: -0.18298280239105225
Batch 2/64 loss: -0.2238876223564148
Batch 3/64 loss: -0.21174132823944092
Batch 4/64 loss: -0.220569908618927
Batch 5/64 loss: -0.21752411127090454
Batch 6/64 loss: -0.20466649532318115
Batch 7/64 loss: -0.22981488704681396
Batch 8/64 loss: -0.2057458758354187
Batch 9/64 loss: -0.23327159881591797
Batch 10/64 loss: -0.20204859972000122
Batch 11/64 loss: -0.18266749382019043
Batch 12/64 loss: -0.2149108648300171
Batch 13/64 loss: -0.20866024494171143
Batch 14/64 loss: -0.2236006259918213
Batch 15/64 loss: -0.2263454794883728
Batch 16/64 loss: -0.22214895486831665
Batch 17/64 loss: -0.22362345457077026
Batch 18/64 loss: -0.1961512565612793
Batch 19/64 loss: -0.1852281093597412
Batch 20/64 loss: -0.21836990118026733
Batch 21/64 loss: -0.19838565587997437
Batch 22/64 loss: -0.20590418577194214
Batch 23/64 loss: -0.19659340381622314
Batch 24/64 loss: -0.21429121494293213
Batch 25/64 loss: -0.23010927438735962
Batch 26/64 loss: -0.21720480918884277
Batch 27/64 loss: -0.23145288228988647
Batch 28/64 loss: -0.17957931756973267
Batch 29/64 loss: -0.23624420166015625
Batch 30/64 loss: -0.21038001775741577
Batch 31/64 loss: -0.2101944088935852
Batch 32/64 loss: -0.21862465143203735
Batch 33/64 loss: -0.19757550954818726
Batch 34/64 loss: -0.21186411380767822
Batch 35/64 loss: -0.2166462540626526
Batch 36/64 loss: -0.2390187382698059
Batch 37/64 loss: -0.22281485795974731
Batch 38/64 loss: -0.20904541015625
Batch 39/64 loss: -0.21003413200378418
Batch 40/64 loss: -0.22391760349273682
Batch 41/64 loss: -0.22687172889709473
Batch 42/64 loss: -0.20997434854507446
Batch 43/64 loss: -0.23034876585006714
Batch 44/64 loss: -0.22678178548812866
Batch 45/64 loss: -0.22383511066436768
Batch 46/64 loss: -0.22105395793914795
Batch 47/64 loss: -0.2223203182220459
Batch 48/64 loss: -0.19791972637176514
Batch 49/64 loss: -0.2317664623260498
Batch 50/64 loss: -0.21999400854110718
Batch 51/64 loss: -0.22890502214431763
Batch 52/64 loss: -0.22652018070220947
Batch 53/64 loss: -0.22630834579467773
Batch 54/64 loss: -0.22519391775131226
Batch 55/64 loss: -0.21922385692596436
Batch 56/64 loss: -0.225860595703125
Batch 57/64 loss: -0.21757125854492188
Batch 58/64 loss: -0.21742743253707886
Batch 59/64 loss: -0.22478175163269043
Batch 60/64 loss: -0.19833976030349731
Batch 61/64 loss: -0.22515350580215454
Batch 62/64 loss: -0.22684675455093384
Batch 63/64 loss: -0.21904689073562622
Batch 64/64 loss: -0.22923731803894043
Epoch 100  Train loss: -0.21612256835488713  Val loss: -0.177920113519295
Epoch 101
-------------------------------
Batch 1/64 loss: -0.2144336700439453
Batch 2/64 loss: -0.22996169328689575
Batch 3/64 loss: -0.21223676204681396
Batch 4/64 loss: -0.2077423334121704
Batch 5/64 loss: -0.2316422462463379
Batch 6/64 loss: -0.20665621757507324
Batch 7/64 loss: -0.216436505317688
Batch 8/64 loss: -0.2086976170539856
Batch 9/64 loss: -0.2401731014251709
Batch 10/64 loss: -0.20814239978790283
Batch 11/64 loss: -0.18864518404006958
Batch 12/64 loss: -0.23081791400909424
Batch 13/64 loss: -0.23497122526168823
Batch 14/64 loss: -0.22167623043060303
Batch 15/64 loss: -0.2332422137260437
Batch 16/64 loss: -0.2214207649230957
Batch 17/64 loss: -0.21286463737487793
Batch 18/64 loss: -0.18527436256408691
Batch 19/64 loss: -0.22122913599014282
Batch 20/64 loss: -0.1653064489364624
Batch 21/64 loss: -0.24008643627166748
Batch 22/64 loss: -0.2240939736366272
Batch 23/64 loss: -0.19026023149490356
Batch 24/64 loss: -0.19329124689102173
Batch 25/64 loss: -0.22456872463226318
Batch 26/64 loss: -0.19900333881378174
Batch 27/64 loss: -0.2262181043624878
Batch 28/64 loss: -0.22263842821121216
Batch 29/64 loss: -0.24412262439727783
Batch 30/64 loss: -0.2135365605354309
Batch 31/64 loss: -0.20305156707763672
Batch 32/64 loss: -0.21044683456420898
Batch 33/64 loss: -0.20881032943725586
Batch 34/64 loss: -0.21239274740219116
Batch 35/64 loss: -0.23153609037399292
Batch 36/64 loss: -0.2169625163078308
Batch 37/64 loss: -0.23466980457305908
Batch 38/64 loss: -0.2286882996559143
Batch 39/64 loss: -0.233018696308136
Batch 40/64 loss: -0.22710061073303223
Batch 41/64 loss: -0.22312217950820923
Batch 42/64 loss: -0.21938133239746094
Batch 43/64 loss: -0.19119805097579956
Batch 44/64 loss: -0.2150312066078186
Batch 45/64 loss: -0.24634778499603271
Batch 46/64 loss: -0.22590899467468262
Batch 47/64 loss: -0.23014581203460693
Batch 48/64 loss: -0.21936893463134766
Batch 49/64 loss: -0.23765647411346436
Batch 50/64 loss: -0.2299472689628601
Batch 51/64 loss: -0.2249041199684143
Batch 52/64 loss: -0.2282552719116211
Batch 53/64 loss: -0.2276638150215149
Batch 54/64 loss: -0.2309817671775818
Batch 55/64 loss: -0.23003524541854858
Batch 56/64 loss: -0.214585542678833
Batch 57/64 loss: -0.22687971591949463
Batch 58/64 loss: -0.22588086128234863
Batch 59/64 loss: -0.23169803619384766
Batch 60/64 loss: -0.20387673377990723
Batch 61/64 loss: -0.23232132196426392
Batch 62/64 loss: -0.23769235610961914
Batch 63/64 loss: -0.23697882890701294
Batch 64/64 loss: -0.24282830953598022
Epoch 101  Train loss: -0.22036157986697028  Val loss: -0.20310665364937275
Saving best model, epoch: 101
Epoch 102
-------------------------------
Batch 1/64 loss: -0.22569721937179565
Batch 2/64 loss: -0.23667526245117188
Batch 3/64 loss: -0.2299012541770935
Batch 4/64 loss: -0.21440458297729492
Batch 5/64 loss: -0.23619353771209717
Batch 6/64 loss: -0.20314913988113403
Batch 7/64 loss: -0.22124135494232178
Batch 8/64 loss: -0.22302711009979248
Batch 9/64 loss: -0.21009212732315063
Batch 10/64 loss: -0.22231817245483398
Batch 11/64 loss: -0.2356742024421692
Batch 12/64 loss: -0.21089500188827515
Batch 13/64 loss: -0.22147029638290405
Batch 14/64 loss: -0.22865670919418335
Batch 15/64 loss: -0.20771187543869019
Batch 16/64 loss: -0.24084925651550293
Batch 17/64 loss: -0.2313653826713562
Batch 18/64 loss: -0.20723778009414673
Batch 19/64 loss: -0.24149543046951294
Batch 20/64 loss: -0.2214444875717163
Batch 21/64 loss: -0.2217952013015747
Batch 22/64 loss: -0.21936005353927612
Batch 23/64 loss: -0.20785659551620483
Batch 24/64 loss: -0.2307835817337036
Batch 25/64 loss: -0.23408925533294678
Batch 26/64 loss: -0.22257161140441895
Batch 27/64 loss: -0.220689594745636
Batch 28/64 loss: -0.22704166173934937
Batch 29/64 loss: -0.226002037525177
Batch 30/64 loss: -0.234239399433136
Batch 31/64 loss: -0.2013302445411682
Batch 32/64 loss: -0.21525901556015015
Batch 33/64 loss: -0.2224712371826172
Batch 34/64 loss: -0.23709475994110107
Batch 35/64 loss: -0.22160476446151733
Batch 36/64 loss: -0.21731609106063843
Batch 37/64 loss: -0.22621464729309082
Batch 38/64 loss: -0.21582114696502686
Batch 39/64 loss: -0.2286568284034729
Batch 40/64 loss: -0.20714938640594482
Batch 41/64 loss: -0.21491873264312744
Batch 42/64 loss: -0.22834259271621704
Batch 43/64 loss: -0.21340686082839966
Batch 44/64 loss: -0.23044824600219727
Batch 45/64 loss: -0.23271620273590088
Batch 46/64 loss: -0.22104895114898682
Batch 47/64 loss: -0.20982128381729126
Batch 48/64 loss: -0.24081462621688843
Batch 49/64 loss: -0.19818413257598877
Batch 50/64 loss: -0.22203701734542847
Batch 51/64 loss: -0.23935437202453613
Batch 52/64 loss: -0.2233559489250183
Batch 53/64 loss: -0.22559964656829834
Batch 54/64 loss: -0.2190793752670288
Batch 55/64 loss: -0.24620962142944336
Batch 56/64 loss: -0.22847449779510498
Batch 57/64 loss: -0.23318171501159668
Batch 58/64 loss: -0.21353912353515625
Batch 59/64 loss: -0.23488986492156982
Batch 60/64 loss: -0.2247631549835205
Batch 61/64 loss: -0.18878942728042603
Batch 62/64 loss: -0.2411174774169922
Batch 63/64 loss: -0.21384233236312866
Batch 64/64 loss: -0.21236497163772583
Epoch 102  Train loss: -0.22290284283020917  Val loss: -0.1951713633701154
Epoch 103
-------------------------------
Batch 1/64 loss: -0.21786069869995117
Batch 2/64 loss: -0.25015759468078613
Batch 3/64 loss: -0.21329885721206665
Batch 4/64 loss: -0.23559683561325073
Batch 5/64 loss: -0.2327284812927246
Batch 6/64 loss: -0.22744327783584595
Batch 7/64 loss: -0.23274755477905273
Batch 8/64 loss: -0.2302986979484558
Batch 9/64 loss: -0.22294670343399048
Batch 10/64 loss: -0.2234211564064026
Batch 11/64 loss: -0.23388665914535522
Batch 12/64 loss: -0.2313213348388672
Batch 13/64 loss: -0.1993820071220398
Batch 14/64 loss: -0.2027767300605774
Batch 15/64 loss: -0.21668100357055664
Batch 16/64 loss: -0.2431730031967163
Batch 17/64 loss: -0.22788900136947632
Batch 18/64 loss: -0.19791072607040405
Batch 19/64 loss: -0.22391021251678467
Batch 20/64 loss: -0.22475945949554443
Batch 21/64 loss: -0.22393572330474854
Batch 22/64 loss: -0.232810378074646
Batch 23/64 loss: -0.2287839651107788
Batch 24/64 loss: -0.2471458911895752
Batch 25/64 loss: -0.2241639494895935
Batch 26/64 loss: -0.2302032709121704
Batch 27/64 loss: -0.22956383228302002
Batch 28/64 loss: -0.23588520288467407
Batch 29/64 loss: -0.223643958568573
Batch 30/64 loss: -0.21008574962615967
Batch 31/64 loss: -0.23337745666503906
Batch 32/64 loss: -0.19953608512878418
Batch 33/64 loss: -0.2367764711380005
Batch 34/64 loss: -0.23114049434661865
Batch 35/64 loss: -0.1952885389328003
Batch 36/64 loss: -0.21644610166549683
Batch 37/64 loss: -0.22373950481414795
Batch 38/64 loss: -0.22267091274261475
Batch 39/64 loss: -0.2179538607597351
Batch 40/64 loss: -0.23459172248840332
Batch 41/64 loss: -0.22631704807281494
Batch 42/64 loss: -0.21474075317382812
Batch 43/64 loss: -0.2252286672592163
Batch 44/64 loss: -0.23069524765014648
Batch 45/64 loss: -0.22367769479751587
Batch 46/64 loss: -0.23046022653579712
Batch 47/64 loss: -0.2223023772239685
Batch 48/64 loss: -0.21424007415771484
Batch 49/64 loss: -0.22207236289978027
Batch 50/64 loss: -0.22067052125930786
Batch 51/64 loss: -0.22019028663635254
Batch 52/64 loss: -0.21879076957702637
Batch 53/64 loss: -0.2221163511276245
Batch 54/64 loss: -0.21647429466247559
Batch 55/64 loss: -0.21098458766937256
Batch 56/64 loss: -0.22999560832977295
Batch 57/64 loss: -0.22762268781661987
Batch 58/64 loss: -0.21198147535324097
Batch 59/64 loss: -0.22743487358093262
Batch 60/64 loss: -0.20297342538833618
Batch 61/64 loss: -0.227514386177063
Batch 62/64 loss: -0.22362244129180908
Batch 63/64 loss: -0.22567778825759888
Batch 64/64 loss: -0.23905915021896362
Epoch 103  Train loss: -0.22370213924669752  Val loss: -0.17738966884481946
Epoch 104
-------------------------------
Batch 1/64 loss: -0.22544652223587036
Batch 2/64 loss: -0.20826232433319092
Batch 3/64 loss: -0.2401747703552246
Batch 4/64 loss: -0.23783177137374878
Batch 5/64 loss: -0.22414827346801758
Batch 6/64 loss: -0.22223693132400513
Batch 7/64 loss: -0.21792691946029663
Batch 8/64 loss: -0.21858203411102295
Batch 9/64 loss: -0.2249177098274231
Batch 10/64 loss: -0.23902112245559692
Batch 11/64 loss: -0.20417428016662598
Batch 12/64 loss: -0.21492183208465576
Batch 13/64 loss: -0.23079067468643188
Batch 14/64 loss: -0.23926877975463867
Batch 15/64 loss: -0.21907293796539307
Batch 16/64 loss: -0.24858039617538452
Batch 17/64 loss: -0.23054265975952148
Batch 18/64 loss: -0.23662930727005005
Batch 19/64 loss: -0.23250621557235718
Batch 20/64 loss: -0.2239220142364502
Batch 21/64 loss: -0.22102361917495728
Batch 22/64 loss: -0.21701812744140625
Batch 23/64 loss: -0.2018725872039795
Batch 24/64 loss: -0.2160019874572754
Batch 25/64 loss: -0.22406762838363647
Batch 26/64 loss: -0.2167116403579712
Batch 27/64 loss: -0.22015494108200073
Batch 28/64 loss: -0.2105008363723755
Batch 29/64 loss: -0.2310255765914917
Batch 30/64 loss: -0.2235153317451477
Batch 31/64 loss: -0.2100541591644287
Batch 32/64 loss: -0.23549425601959229
Batch 33/64 loss: -0.22548937797546387
Batch 34/64 loss: -0.21562141180038452
Batch 35/64 loss: -0.21576571464538574
Batch 36/64 loss: -0.22680556774139404
Batch 37/64 loss: -0.19717419147491455
Batch 38/64 loss: -0.2294444441795349
Batch 39/64 loss: -0.22168588638305664
Batch 40/64 loss: -0.22717493772506714
Batch 41/64 loss: -0.22548091411590576
Batch 42/64 loss: -0.21584057807922363
Batch 43/64 loss: -0.1834130883216858
Batch 44/64 loss: -0.19567972421646118
Batch 45/64 loss: -0.21721595525741577
Batch 46/64 loss: -0.2146509289741516
Batch 47/64 loss: -0.19152891635894775
Batch 48/64 loss: -0.21561199426651
Batch 49/64 loss: -0.21359330415725708
Batch 50/64 loss: -0.21974611282348633
Batch 51/64 loss: -0.2119770646095276
Batch 52/64 loss: -0.22447961568832397
Batch 53/64 loss: -0.22322499752044678
Batch 54/64 loss: -0.21726059913635254
Batch 55/64 loss: -0.21659845113754272
Batch 56/64 loss: -0.1936667561531067
Batch 57/64 loss: -0.23119378089904785
Batch 58/64 loss: -0.23361575603485107
Batch 59/64 loss: -0.18783575296401978
Batch 60/64 loss: -0.20276415348052979
Batch 61/64 loss: -0.2206987738609314
Batch 62/64 loss: -0.22345679998397827
Batch 63/64 loss: -0.1918688416481018
Batch 64/64 loss: -0.21107721328735352
Epoch 104  Train loss: -0.21890620343825395  Val loss: -0.1935643688919618
Epoch 105
-------------------------------
Batch 1/64 loss: -0.19294631481170654
Batch 2/64 loss: -0.22533416748046875
Batch 3/64 loss: -0.2136249542236328
Batch 4/64 loss: -0.20631647109985352
Batch 5/64 loss: -0.2184206247329712
Batch 6/64 loss: -0.19353961944580078
Batch 7/64 loss: -0.2180216908454895
Batch 8/64 loss: -0.22736871242523193
Batch 9/64 loss: -0.21834486722946167
Batch 10/64 loss: -0.23456180095672607
Batch 11/64 loss: -0.21731126308441162
Batch 12/64 loss: -0.20867806673049927
Batch 13/64 loss: -0.21273356676101685
Batch 14/64 loss: -0.22904908657073975
Batch 15/64 loss: -0.23738080263137817
Batch 16/64 loss: -0.22812891006469727
Batch 17/64 loss: -0.22495537996292114
Batch 18/64 loss: -0.21027332544326782
Batch 19/64 loss: -0.23256278038024902
Batch 20/64 loss: -0.22716820240020752
Batch 21/64 loss: -0.22559762001037598
Batch 22/64 loss: -0.18187952041625977
Batch 23/64 loss: -0.21340793371200562
Batch 24/64 loss: -0.2129773497581482
Batch 25/64 loss: -0.2177918553352356
Batch 26/64 loss: -0.2285425066947937
Batch 27/64 loss: -0.2288060188293457
Batch 28/64 loss: -0.20855969190597534
Batch 29/64 loss: -0.22696322202682495
Batch 30/64 loss: -0.2327873706817627
Batch 31/64 loss: -0.23262369632720947
Batch 32/64 loss: -0.22642266750335693
Batch 33/64 loss: -0.22634613513946533
Batch 34/64 loss: -0.24324005842208862
Batch 35/64 loss: -0.2316598892211914
Batch 36/64 loss: -0.24536842107772827
Batch 37/64 loss: -0.2244478464126587
Batch 38/64 loss: -0.20828795433044434
Batch 39/64 loss: -0.23084932565689087
Batch 40/64 loss: -0.19478750228881836
Batch 41/64 loss: -0.2410528063774109
Batch 42/64 loss: -0.23212426900863647
Batch 43/64 loss: -0.21465307474136353
Batch 44/64 loss: -0.19609349966049194
Batch 45/64 loss: -0.23103076219558716
Batch 46/64 loss: -0.21576601266860962
Batch 47/64 loss: -0.2112681269645691
Batch 48/64 loss: -0.21211183071136475
Batch 49/64 loss: -0.18950605392456055
Batch 50/64 loss: -0.22409921884536743
Batch 51/64 loss: -0.20401275157928467
Batch 52/64 loss: -0.18760263919830322
Batch 53/64 loss: -0.23572301864624023
Batch 54/64 loss: -0.22248601913452148
Batch 55/64 loss: -0.22819435596466064
Batch 56/64 loss: -0.21763992309570312
Batch 57/64 loss: -0.2179018259048462
Batch 58/64 loss: -0.23335111141204834
Batch 59/64 loss: -0.21733123064041138
Batch 60/64 loss: -0.22421270608901978
Batch 61/64 loss: -0.1916959285736084
Batch 62/64 loss: -0.2296292781829834
Batch 63/64 loss: -0.2065272331237793
Batch 64/64 loss: -0.22434008121490479
Epoch 105  Train loss: -0.21914252440134685  Val loss: -0.18185289618895226
Epoch 106
-------------------------------
Batch 1/64 loss: -0.23092114925384521
Batch 2/64 loss: -0.2190539836883545
Batch 3/64 loss: -0.21831077337265015
Batch 4/64 loss: -0.22111505270004272
Batch 5/64 loss: -0.21551787853240967
Batch 6/64 loss: -0.24298423528671265
Batch 7/64 loss: -0.2275344729423523
Batch 8/64 loss: -0.22255927324295044
Batch 9/64 loss: -0.24524807929992676
Batch 10/64 loss: -0.22726655006408691
Batch 11/64 loss: -0.2057124376296997
Batch 12/64 loss: -0.22613608837127686
Batch 13/64 loss: -0.2138596773147583
Batch 14/64 loss: -0.22321629524230957
Batch 15/64 loss: -0.20274096727371216
Batch 16/64 loss: -0.22811269760131836
Batch 17/64 loss: -0.23684799671173096
Batch 18/64 loss: -0.21149063110351562
Batch 19/64 loss: -0.2175247073173523
Batch 20/64 loss: -0.22007697820663452
Batch 21/64 loss: -0.20851397514343262
Batch 22/64 loss: -0.2258315086364746
Batch 23/64 loss: -0.2249051332473755
Batch 24/64 loss: -0.20417523384094238
Batch 25/64 loss: -0.22576862573623657
Batch 26/64 loss: -0.2128448486328125
Batch 27/64 loss: -0.21370983123779297
Batch 28/64 loss: -0.21957820653915405
Batch 29/64 loss: -0.1944878101348877
Batch 30/64 loss: -0.20720523595809937
Batch 31/64 loss: -0.2199188470840454
Batch 32/64 loss: -0.222603440284729
Batch 33/64 loss: -0.21001112461090088
Batch 34/64 loss: -0.23149609565734863
Batch 35/64 loss: -0.21370148658752441
Batch 36/64 loss: -0.215226411819458
Batch 37/64 loss: -0.22869205474853516
Batch 38/64 loss: -0.23714035749435425
Batch 39/64 loss: -0.20799368619918823
Batch 40/64 loss: -0.1979910135269165
Batch 41/64 loss: -0.21203285455703735
Batch 42/64 loss: -0.2321305274963379
Batch 43/64 loss: -0.22884035110473633
Batch 44/64 loss: -0.2223966121673584
Batch 45/64 loss: -0.23714685440063477
Batch 46/64 loss: -0.22129327058792114
Batch 47/64 loss: -0.2064906358718872
Batch 48/64 loss: -0.21089261770248413
Batch 49/64 loss: -0.2230120301246643
Batch 50/64 loss: -0.19487202167510986
Batch 51/64 loss: -0.20533275604248047
Batch 52/64 loss: -0.2297661304473877
Batch 53/64 loss: -0.2249813675880432
Batch 54/64 loss: -0.2188364863395691
Batch 55/64 loss: -0.2177736759185791
Batch 56/64 loss: -0.21543264389038086
Batch 57/64 loss: -0.20915961265563965
Batch 58/64 loss: -0.20930105447769165
Batch 59/64 loss: -0.21269452571868896
Batch 60/64 loss: -0.22844183444976807
Batch 61/64 loss: -0.23191440105438232
Batch 62/64 loss: -0.20579588413238525
Batch 63/64 loss: -0.22058582305908203
Batch 64/64 loss: -0.2333923578262329
Epoch 106  Train loss: -0.2191716563467886  Val loss: -0.19675478656677037
Epoch 107
-------------------------------
Batch 1/64 loss: -0.2248389720916748
Batch 2/64 loss: -0.21674197912216187
Batch 3/64 loss: -0.21819067001342773
Batch 4/64 loss: -0.232011079788208
Batch 5/64 loss: -0.22338628768920898
Batch 6/64 loss: -0.22989362478256226
Batch 7/64 loss: -0.2390710711479187
Batch 8/64 loss: -0.23082077503204346
Batch 9/64 loss: -0.23354822397232056
Batch 10/64 loss: -0.2252514362335205
Batch 11/64 loss: -0.23412519693374634
Batch 12/64 loss: -0.2309582233428955
Batch 13/64 loss: -0.22377640008926392
Batch 14/64 loss: -0.23181617259979248
Batch 15/64 loss: -0.21061205863952637
Batch 16/64 loss: -0.22154968976974487
Batch 17/64 loss: -0.23476964235305786
Batch 18/64 loss: -0.21564728021621704
Batch 19/64 loss: -0.2271011471748352
Batch 20/64 loss: -0.22726112604141235
Batch 21/64 loss: -0.21070802211761475
Batch 22/64 loss: -0.2245376706123352
Batch 23/64 loss: -0.25347375869750977
Batch 24/64 loss: -0.2210187315940857
Batch 25/64 loss: -0.21497225761413574
Batch 26/64 loss: -0.23729795217514038
Batch 27/64 loss: -0.22512221336364746
Batch 28/64 loss: -0.22456228733062744
Batch 29/64 loss: -0.22304439544677734
Batch 30/64 loss: -0.23564857244491577
Batch 31/64 loss: -0.23077666759490967
Batch 32/64 loss: -0.23894637823104858
Batch 33/64 loss: -0.22874873876571655
Batch 34/64 loss: -0.2199110984802246
Batch 35/64 loss: -0.23295950889587402
Batch 36/64 loss: -0.24388772249221802
Batch 37/64 loss: -0.21475255489349365
Batch 38/64 loss: -0.22477936744689941
Batch 39/64 loss: -0.21217817068099976
Batch 40/64 loss: -0.20556390285491943
Batch 41/64 loss: -0.1862773895263672
Batch 42/64 loss: -0.22881650924682617
Batch 43/64 loss: -0.2503262162208557
Batch 44/64 loss: -0.22176527976989746
Batch 45/64 loss: -0.230022132396698
Batch 46/64 loss: -0.21369469165802002
Batch 47/64 loss: -0.23675024509429932
Batch 48/64 loss: -0.18305325508117676
Batch 49/64 loss: -0.2090815305709839
Batch 50/64 loss: -0.23361235857009888
Batch 51/64 loss: -0.22380369901657104
Batch 52/64 loss: -0.23034042119979858
Batch 53/64 loss: -0.21269863843917847
Batch 54/64 loss: -0.23324483633041382
Batch 55/64 loss: -0.227095365524292
Batch 56/64 loss: -0.2228192687034607
Batch 57/64 loss: -0.22276771068572998
Batch 58/64 loss: -0.22771179676055908
Batch 59/64 loss: -0.22619789838790894
Batch 60/64 loss: -0.23303323984146118
Batch 61/64 loss: -0.22571659088134766
Batch 62/64 loss: -0.21269690990447998
Batch 63/64 loss: -0.21461427211761475
Batch 64/64 loss: -0.19239169359207153
Epoch 107  Train loss: -0.22438737341001921  Val loss: -0.19911001085006086
Epoch 108
-------------------------------
Batch 1/64 loss: -0.2024921178817749
Batch 2/64 loss: -0.22326695919036865
Batch 3/64 loss: -0.21883803606033325
Batch 4/64 loss: -0.19632649421691895
Batch 5/64 loss: -0.21903353929519653
Batch 6/64 loss: -0.23144793510437012
Batch 7/64 loss: -0.22624284029006958
Batch 8/64 loss: -0.22530579566955566
Batch 9/64 loss: -0.22982102632522583
Batch 10/64 loss: -0.23452043533325195
Batch 11/64 loss: -0.24390029907226562
Batch 12/64 loss: -0.23155224323272705
Batch 13/64 loss: -0.2300603985786438
Batch 14/64 loss: -0.22248786687850952
Batch 15/64 loss: -0.22928041219711304
Batch 16/64 loss: -0.2331562638282776
Batch 17/64 loss: -0.21855014562606812
Batch 18/64 loss: -0.23454535007476807
Batch 19/64 loss: -0.2082003355026245
Batch 20/64 loss: -0.23579412698745728
Batch 21/64 loss: -0.21310997009277344
Batch 22/64 loss: -0.22253835201263428
Batch 23/64 loss: -0.21287494897842407
Batch 24/64 loss: -0.23830974102020264
Batch 25/64 loss: -0.2335963249206543
Batch 26/64 loss: -0.24668961763381958
Batch 27/64 loss: -0.20985662937164307
Batch 28/64 loss: -0.21984761953353882
Batch 29/64 loss: -0.2331194281578064
Batch 30/64 loss: -0.24069559574127197
Batch 31/64 loss: -0.239654541015625
Batch 32/64 loss: -0.23328596353530884
Batch 33/64 loss: -0.24728703498840332
Batch 34/64 loss: -0.2298595905303955
Batch 35/64 loss: -0.23091959953308105
Batch 36/64 loss: -0.21845513582229614
Batch 37/64 loss: -0.2330845594406128
Batch 38/64 loss: -0.24176156520843506
Batch 39/64 loss: -0.2391495704650879
Batch 40/64 loss: -0.22858315706253052
Batch 41/64 loss: -0.22633665800094604
Batch 42/64 loss: -0.24355024099349976
Batch 43/64 loss: -0.2163088321685791
Batch 44/64 loss: -0.23382437229156494
Batch 45/64 loss: -0.23231381177902222
Batch 46/64 loss: -0.23959285020828247
Batch 47/64 loss: -0.21714162826538086
Batch 48/64 loss: -0.2262357473373413
Batch 49/64 loss: -0.22234272956848145
Batch 50/64 loss: -0.2179354429244995
Batch 51/64 loss: -0.21873587369918823
Batch 52/64 loss: -0.22145777940750122
Batch 53/64 loss: -0.22659564018249512
Batch 54/64 loss: -0.24955981969833374
Batch 55/64 loss: -0.22575390338897705
Batch 56/64 loss: -0.23882412910461426
Batch 57/64 loss: -0.22676897048950195
Batch 58/64 loss: -0.2189267873764038
Batch 59/64 loss: -0.20944076776504517
Batch 60/64 loss: -0.22282731533050537
Batch 61/64 loss: -0.22626954317092896
Batch 62/64 loss: -0.21329939365386963
Batch 63/64 loss: -0.22908711433410645
Batch 64/64 loss: -0.21309936046600342
Epoch 108  Train loss: -0.2269875362807629  Val loss: -0.1920103840401902
Epoch 109
-------------------------------
Batch 1/64 loss: -0.2417130470275879
Batch 2/64 loss: -0.215224027633667
Batch 3/64 loss: -0.23220223188400269
Batch 4/64 loss: -0.20812255144119263
Batch 5/64 loss: -0.23382437229156494
Batch 6/64 loss: -0.22675752639770508
Batch 7/64 loss: -0.22483736276626587
Batch 8/64 loss: -0.2191540002822876
Batch 9/64 loss: -0.24198037385940552
Batch 10/64 loss: -0.2124117612838745
Batch 11/64 loss: -0.23982524871826172
Batch 12/64 loss: -0.23768013715744019
Batch 13/64 loss: -0.21766865253448486
Batch 14/64 loss: -0.21970736980438232
Batch 15/64 loss: -0.23089665174484253
Batch 16/64 loss: -0.22436517477035522
Batch 17/64 loss: -0.2236059308052063
Batch 18/64 loss: -0.18922358751296997
Batch 19/64 loss: -0.2350863814353943
Batch 20/64 loss: -0.2170376181602478
Batch 21/64 loss: -0.22233647108078003
Batch 22/64 loss: -0.21797555685043335
Batch 23/64 loss: -0.23801040649414062
Batch 24/64 loss: -0.2185981273651123
Batch 25/64 loss: -0.240850567817688
Batch 26/64 loss: -0.23354732990264893
Batch 27/64 loss: -0.21217471361160278
Batch 28/64 loss: -0.2146207094192505
Batch 29/64 loss: -0.22866249084472656
Batch 30/64 loss: -0.22979390621185303
Batch 31/64 loss: -0.23571157455444336
Batch 32/64 loss: -0.2413485050201416
Batch 33/64 loss: -0.20544898509979248
Batch 34/64 loss: -0.22313517332077026
Batch 35/64 loss: -0.22784805297851562
Batch 36/64 loss: -0.22552305459976196
Batch 37/64 loss: -0.18953275680541992
Batch 38/64 loss: -0.2517843246459961
Batch 39/64 loss: -0.2202940583229065
Batch 40/64 loss: -0.2084597945213318
Batch 41/64 loss: -0.2276155948638916
Batch 42/64 loss: -0.2299550175666809
Batch 43/64 loss: -0.22461271286010742
Batch 44/64 loss: -0.240073561668396
Batch 45/64 loss: -0.22525477409362793
Batch 46/64 loss: -0.23264563083648682
Batch 47/64 loss: -0.2306661605834961
Batch 48/64 loss: -0.23273158073425293
Batch 49/64 loss: -0.23431611061096191
Batch 50/64 loss: -0.22718238830566406
Batch 51/64 loss: -0.24419277906417847
Batch 52/64 loss: -0.2239292860031128
Batch 53/64 loss: -0.2004166841506958
Batch 54/64 loss: -0.24927634000778198
Batch 55/64 loss: -0.21487683057785034
Batch 56/64 loss: -0.23408234119415283
Batch 57/64 loss: -0.22574567794799805
Batch 58/64 loss: -0.2199079394340515
Batch 59/64 loss: -0.22310376167297363
Batch 60/64 loss: -0.24700844287872314
Batch 61/64 loss: -0.2124742865562439
Batch 62/64 loss: -0.23049306869506836
Batch 63/64 loss: -0.24383234977722168
Batch 64/64 loss: -0.22602438926696777
Epoch 109  Train loss: -0.22621007339627136  Val loss: -0.20291233165157618
Epoch 110
-------------------------------
Batch 1/64 loss: -0.24645370244979858
Batch 2/64 loss: -0.23509055376052856
Batch 3/64 loss: -0.23238587379455566
Batch 4/64 loss: -0.2180401086807251
Batch 5/64 loss: -0.23309749364852905
Batch 6/64 loss: -0.21881961822509766
Batch 7/64 loss: -0.23763501644134521
Batch 8/64 loss: -0.2042427659034729
Batch 9/64 loss: -0.2169344425201416
Batch 10/64 loss: -0.21776920557022095
Batch 11/64 loss: -0.2344123125076294
Batch 12/64 loss: -0.23708295822143555
Batch 13/64 loss: -0.24332791566848755
Batch 14/64 loss: -0.23267757892608643
Batch 15/64 loss: -0.21822404861450195
Batch 16/64 loss: -0.21962499618530273
Batch 17/64 loss: -0.220802903175354
Batch 18/64 loss: -0.23889964818954468
Batch 19/64 loss: -0.22886931896209717
Batch 20/64 loss: -0.23205381631851196
Batch 21/64 loss: -0.22015392780303955
Batch 22/64 loss: -0.23394399881362915
Batch 23/64 loss: -0.21917575597763062
Batch 24/64 loss: -0.1990324854850769
Batch 25/64 loss: -0.2390207052230835
Batch 26/64 loss: -0.2391974925994873
Batch 27/64 loss: -0.22299033403396606
Batch 28/64 loss: -0.23504048585891724
Batch 29/64 loss: -0.2128450870513916
Batch 30/64 loss: -0.21420204639434814
Batch 31/64 loss: -0.2132326364517212
Batch 32/64 loss: -0.2196972370147705
Batch 33/64 loss: -0.22264522314071655
Batch 34/64 loss: -0.23537516593933105
Batch 35/64 loss: -0.23503464460372925
Batch 36/64 loss: -0.23225051164627075
Batch 37/64 loss: -0.2184343934059143
Batch 38/64 loss: -0.20380502939224243
Batch 39/64 loss: -0.21124231815338135
Batch 40/64 loss: -0.24678051471710205
Batch 41/64 loss: -0.21562594175338745
Batch 42/64 loss: -0.24604076147079468
Batch 43/64 loss: -0.19944226741790771
Batch 44/64 loss: -0.23867875337600708
Batch 45/64 loss: -0.21930241584777832
Batch 46/64 loss: -0.2235279679298401
Batch 47/64 loss: -0.1968303918838501
Batch 48/64 loss: -0.24528735876083374
Batch 49/64 loss: -0.23464667797088623
Batch 50/64 loss: -0.20445382595062256
Batch 51/64 loss: -0.23053979873657227
Batch 52/64 loss: -0.2418147325515747
Batch 53/64 loss: -0.23728501796722412
Batch 54/64 loss: -0.19703316688537598
Batch 55/64 loss: -0.2328813076019287
Batch 56/64 loss: -0.24292325973510742
Batch 57/64 loss: -0.22631502151489258
Batch 58/64 loss: -0.23608040809631348
Batch 59/64 loss: -0.20654016733169556
Batch 60/64 loss: -0.2286587357521057
Batch 61/64 loss: -0.2266332507133484
Batch 62/64 loss: -0.2424573302268982
Batch 63/64 loss: -0.22728365659713745
Batch 64/64 loss: -0.2323606014251709
Epoch 110  Train loss: -0.2261191676644718  Val loss: -0.19788163531686842
Epoch 111
-------------------------------
Batch 1/64 loss: -0.24357104301452637
Batch 2/64 loss: -0.2220233678817749
Batch 3/64 loss: -0.2318844199180603
Batch 4/64 loss: -0.23626810312271118
Batch 5/64 loss: -0.22536492347717285
Batch 6/64 loss: -0.23663455247879028
Batch 7/64 loss: -0.24047964811325073
Batch 8/64 loss: -0.23069071769714355
Batch 9/64 loss: -0.23799192905426025
Batch 10/64 loss: -0.22193002700805664
Batch 11/64 loss: -0.23298364877700806
Batch 12/64 loss: -0.23439371585845947
Batch 13/64 loss: -0.24469059705734253
Batch 14/64 loss: -0.2586677670478821
Batch 15/64 loss: -0.23224598169326782
Batch 16/64 loss: -0.24326050281524658
Batch 17/64 loss: -0.2190793752670288
Batch 18/64 loss: -0.19713830947875977
Batch 19/64 loss: -0.2289179563522339
Batch 20/64 loss: -0.2208232879638672
Batch 21/64 loss: -0.23642081022262573
Batch 22/64 loss: -0.24003905057907104
Batch 23/64 loss: -0.19574451446533203
Batch 24/64 loss: -0.20435059070587158
Batch 25/64 loss: -0.1950998306274414
Batch 26/64 loss: -0.2187865972518921
Batch 27/64 loss: -0.2334822416305542
Batch 28/64 loss: -0.22560065984725952
Batch 29/64 loss: -0.2202165126800537
Batch 30/64 loss: -0.23915010690689087
Batch 31/64 loss: -0.23892343044281006
Batch 32/64 loss: -0.23508518934249878
Batch 33/64 loss: -0.23582780361175537
Batch 34/64 loss: -0.2277899980545044
Batch 35/64 loss: -0.23167955875396729
Batch 36/64 loss: -0.23221290111541748
Batch 37/64 loss: -0.23413634300231934
Batch 38/64 loss: -0.24242126941680908
Batch 39/64 loss: -0.21469086408615112
Batch 40/64 loss: -0.21250581741333008
Batch 41/64 loss: -0.23877614736557007
Batch 42/64 loss: -0.2475677728652954
Batch 43/64 loss: -0.2290564775466919
Batch 44/64 loss: -0.2547926902770996
Batch 45/64 loss: -0.23816204071044922
Batch 46/64 loss: -0.23436379432678223
Batch 47/64 loss: -0.23597508668899536
Batch 48/64 loss: -0.23481935262680054
Batch 49/64 loss: -0.2346147894859314
Batch 50/64 loss: -0.23450762033462524
Batch 51/64 loss: -0.23058313131332397
Batch 52/64 loss: -0.24567961692810059
Batch 53/64 loss: -0.22769200801849365
Batch 54/64 loss: -0.2189044952392578
Batch 55/64 loss: -0.21136897802352905
Batch 56/64 loss: -0.21784114837646484
Batch 57/64 loss: -0.24084246158599854
Batch 58/64 loss: -0.2215164303779602
Batch 59/64 loss: -0.2226189374923706
Batch 60/64 loss: -0.22289198637008667
Batch 61/64 loss: -0.21555745601654053
Batch 62/64 loss: -0.22032934427261353
Batch 63/64 loss: -0.2303222417831421
Batch 64/64 loss: -0.22986602783203125
Epoch 111  Train loss: -0.22955901950013405  Val loss: -0.19533384665590792
Epoch 112
-------------------------------
Batch 1/64 loss: -0.23722195625305176
Batch 2/64 loss: -0.21179014444351196
Batch 3/64 loss: -0.21509701013565063
Batch 4/64 loss: -0.21642160415649414
Batch 5/64 loss: -0.24405527114868164
Batch 6/64 loss: -0.25409960746765137
Batch 7/64 loss: -0.2465478777885437
Batch 8/64 loss: -0.21351975202560425
Batch 9/64 loss: -0.2470109462738037
Batch 10/64 loss: -0.2223752737045288
Batch 11/64 loss: -0.22067421674728394
Batch 12/64 loss: -0.2160930633544922
Batch 13/64 loss: -0.23494207859039307
Batch 14/64 loss: -0.2274869680404663
Batch 15/64 loss: -0.2410908341407776
Batch 16/64 loss: -0.22556406259536743
Batch 17/64 loss: -0.24609756469726562
Batch 18/64 loss: -0.21945524215698242
Batch 19/64 loss: -0.21596509218215942
Batch 20/64 loss: -0.22687411308288574
Batch 21/64 loss: -0.20704686641693115
Batch 22/64 loss: -0.23107004165649414
Batch 23/64 loss: -0.24153518676757812
Batch 24/64 loss: -0.2376742959022522
Batch 25/64 loss: -0.24359190464019775
Batch 26/64 loss: -0.2284071445465088
Batch 27/64 loss: -0.24739694595336914
Batch 28/64 loss: -0.240373432636261
Batch 29/64 loss: -0.24106842279434204
Batch 30/64 loss: -0.23409497737884521
Batch 31/64 loss: -0.23209810256958008
Batch 32/64 loss: -0.2600020170211792
Batch 33/64 loss: -0.24134457111358643
Batch 34/64 loss: -0.26086676120758057
Batch 35/64 loss: -0.2478015422821045
Batch 36/64 loss: -0.235160231590271
Batch 37/64 loss: -0.21342778205871582
Batch 38/64 loss: -0.22433722019195557
Batch 39/64 loss: -0.24073898792266846
Batch 40/64 loss: -0.228709876537323
Batch 41/64 loss: -0.24123209714889526
Batch 42/64 loss: -0.20986288785934448
Batch 43/64 loss: -0.21209126710891724
Batch 44/64 loss: -0.22965556383132935
Batch 45/64 loss: -0.23997801542282104
Batch 46/64 loss: -0.2281266450881958
Batch 47/64 loss: -0.23805749416351318
Batch 48/64 loss: -0.22750121355056763
Batch 49/64 loss: -0.2274366021156311
Batch 50/64 loss: -0.22326332330703735
Batch 51/64 loss: -0.23977386951446533
Batch 52/64 loss: -0.21822452545166016
Batch 53/64 loss: -0.2533412575721741
Batch 54/64 loss: -0.25899267196655273
Batch 55/64 loss: -0.24817150831222534
Batch 56/64 loss: -0.24296432733535767
Batch 57/64 loss: -0.22685039043426514
Batch 58/64 loss: -0.226490318775177
Batch 59/64 loss: -0.22968757152557373
Batch 60/64 loss: -0.23618876934051514
Batch 61/64 loss: -0.21628117561340332
Batch 62/64 loss: -0.22909903526306152
Batch 63/64 loss: -0.205657958984375
Batch 64/64 loss: -0.18822717666625977
Epoch 112  Train loss: -0.23167416254679363  Val loss: -0.1960971732319835
Epoch 113
-------------------------------
Batch 1/64 loss: -0.2188253402709961
Batch 2/64 loss: -0.23896747827529907
Batch 3/64 loss: -0.2334737777709961
Batch 4/64 loss: -0.24095690250396729
Batch 5/64 loss: -0.22677600383758545
Batch 6/64 loss: -0.20885217189788818
Batch 7/64 loss: -0.23758327960968018
Batch 8/64 loss: -0.2349987030029297
Batch 9/64 loss: -0.22177505493164062
Batch 10/64 loss: -0.2095470428466797
Batch 11/64 loss: -0.24488699436187744
Batch 12/64 loss: -0.24519681930541992
Batch 13/64 loss: -0.21908676624298096
Batch 14/64 loss: -0.21702778339385986
Batch 15/64 loss: -0.2277073860168457
Batch 16/64 loss: -0.23214006423950195
Batch 17/64 loss: -0.24362778663635254
Batch 18/64 loss: -0.24020349979400635
Batch 19/64 loss: -0.23068004846572876
Batch 20/64 loss: -0.22265690565109253
Batch 21/64 loss: -0.2322792410850525
Batch 22/64 loss: -0.23204350471496582
Batch 23/64 loss: -0.23906171321868896
Batch 24/64 loss: -0.24028950929641724
Batch 25/64 loss: -0.2256404161453247
Batch 26/64 loss: -0.2238193154335022
Batch 27/64 loss: -0.2323664426803589
Batch 28/64 loss: -0.23776960372924805
Batch 29/64 loss: -0.22699010372161865
Batch 30/64 loss: -0.24479937553405762
Batch 31/64 loss: -0.2131912112236023
Batch 32/64 loss: -0.24290424585342407
Batch 33/64 loss: -0.22886359691619873
Batch 34/64 loss: -0.2283393144607544
Batch 35/64 loss: -0.233631432056427
Batch 36/64 loss: -0.24596285820007324
Batch 37/64 loss: -0.22328829765319824
Batch 38/64 loss: -0.23263955116271973
Batch 39/64 loss: -0.2268238067626953
Batch 40/64 loss: -0.23521995544433594
Batch 41/64 loss: -0.22692561149597168
Batch 42/64 loss: -0.23514628410339355
Batch 43/64 loss: -0.21362054347991943
Batch 44/64 loss: -0.25005239248275757
Batch 45/64 loss: -0.23376566171646118
Batch 46/64 loss: -0.24235868453979492
Batch 47/64 loss: -0.24861550331115723
Batch 48/64 loss: -0.2310543656349182
Batch 49/64 loss: -0.23330837488174438
Batch 50/64 loss: -0.23914724588394165
Batch 51/64 loss: -0.22424936294555664
Batch 52/64 loss: -0.24145185947418213
Batch 53/64 loss: -0.2357807159423828
Batch 54/64 loss: -0.22754508256912231
Batch 55/64 loss: -0.23073256015777588
Batch 56/64 loss: -0.20567327737808228
Batch 57/64 loss: -0.22627580165863037
Batch 58/64 loss: -0.222395658493042
Batch 59/64 loss: -0.22187036275863647
Batch 60/64 loss: -0.2284238338470459
Batch 61/64 loss: -0.2287270426750183
Batch 62/64 loss: -0.2317865490913391
Batch 63/64 loss: -0.2232707142829895
Batch 64/64 loss: -0.23222529888153076
Epoch 113  Train loss: -0.23085866301667457  Val loss: -0.2058174841592402
Saving best model, epoch: 113
Epoch 114
-------------------------------
Batch 1/64 loss: -0.2353842854499817
Batch 2/64 loss: -0.24093568325042725
Batch 3/64 loss: -0.24108803272247314
Batch 4/64 loss: -0.23445427417755127
Batch 5/64 loss: -0.23229312896728516
Batch 6/64 loss: -0.238825261592865
Batch 7/64 loss: -0.24027496576309204
Batch 8/64 loss: -0.22522175312042236
Batch 9/64 loss: -0.21099793910980225
Batch 10/64 loss: -0.22972065210342407
Batch 11/64 loss: -0.23487168550491333
Batch 12/64 loss: -0.2375551462173462
Batch 13/64 loss: -0.23755133152008057
Batch 14/64 loss: -0.24537968635559082
Batch 15/64 loss: -0.24899828433990479
Batch 16/64 loss: -0.2308223843574524
Batch 17/64 loss: -0.23838073015213013
Batch 18/64 loss: -0.23898202180862427
Batch 19/64 loss: -0.2128002643585205
Batch 20/64 loss: -0.2377520203590393
Batch 21/64 loss: -0.23225748538970947
Batch 22/64 loss: -0.2361559271812439
Batch 23/64 loss: -0.23688161373138428
Batch 24/64 loss: -0.25158774852752686
Batch 25/64 loss: -0.21347576379776
Batch 26/64 loss: -0.2240743637084961
Batch 27/64 loss: -0.23137319087982178
Batch 28/64 loss: -0.23810911178588867
Batch 29/64 loss: -0.22194021940231323
Batch 30/64 loss: -0.22932100296020508
Batch 31/64 loss: -0.23017752170562744
Batch 32/64 loss: -0.24643856287002563
Batch 33/64 loss: -0.24030983448028564
Batch 34/64 loss: -0.22074615955352783
Batch 35/64 loss: -0.22964388132095337
Batch 36/64 loss: -0.22647029161453247
Batch 37/64 loss: -0.2318437099456787
Batch 38/64 loss: -0.23367911577224731
Batch 39/64 loss: -0.21104580163955688
Batch 40/64 loss: -0.2200961709022522
Batch 41/64 loss: -0.22614753246307373
Batch 42/64 loss: -0.20436930656433105
Batch 43/64 loss: -0.22247666120529175
Batch 44/64 loss: -0.21695047616958618
Batch 45/64 loss: -0.23321497440338135
Batch 46/64 loss: -0.22259312868118286
Batch 47/64 loss: -0.21260124444961548
Batch 48/64 loss: -0.21992629766464233
Batch 49/64 loss: -0.2283461093902588
Batch 50/64 loss: -0.22519421577453613
Batch 51/64 loss: -0.2276681661605835
Batch 52/64 loss: -0.24112427234649658
Batch 53/64 loss: -0.2426002025604248
Batch 54/64 loss: -0.22750073671340942
Batch 55/64 loss: -0.2350480556488037
Batch 56/64 loss: -0.23465847969055176
Batch 57/64 loss: -0.2501598596572876
Batch 58/64 loss: -0.22685325145721436
Batch 59/64 loss: -0.23303008079528809
Batch 60/64 loss: -0.23937463760375977
Batch 61/64 loss: -0.22037672996520996
Batch 62/64 loss: -0.2349497675895691
Batch 63/64 loss: -0.2151249647140503
Batch 64/64 loss: -0.2335498332977295
Epoch 114  Train loss: -0.23079793780457739  Val loss: -0.20652382173079395
Saving best model, epoch: 114
Epoch 115
-------------------------------
Batch 1/64 loss: -0.24172478914260864
Batch 2/64 loss: -0.24875032901763916
Batch 3/64 loss: -0.23028749227523804
Batch 4/64 loss: -0.23595011234283447
Batch 5/64 loss: -0.23408830165863037
Batch 6/64 loss: -0.2475096583366394
Batch 7/64 loss: -0.22690188884735107
Batch 8/64 loss: -0.24173617362976074
Batch 9/64 loss: -0.22563928365707397
Batch 10/64 loss: -0.2381124496459961
Batch 11/64 loss: -0.23648536205291748
Batch 12/64 loss: -0.19634193181991577
Batch 13/64 loss: -0.22947508096694946
Batch 14/64 loss: -0.24627965688705444
Batch 15/64 loss: -0.2208411693572998
Batch 16/64 loss: -0.24481827020645142
Batch 17/64 loss: -0.22174549102783203
Batch 18/64 loss: -0.22374218702316284
Batch 19/64 loss: -0.2176181674003601
Batch 20/64 loss: -0.23754191398620605
Batch 21/64 loss: -0.2181277871131897
Batch 22/64 loss: -0.23579871654510498
Batch 23/64 loss: -0.23641562461853027
Batch 24/64 loss: -0.2205519676208496
Batch 25/64 loss: -0.22809946537017822
Batch 26/64 loss: -0.24411195516586304
Batch 27/64 loss: -0.22998732328414917
Batch 28/64 loss: -0.2225116491317749
Batch 29/64 loss: -0.24551236629486084
Batch 30/64 loss: -0.23500722646713257
Batch 31/64 loss: -0.22135335206985474
Batch 32/64 loss: -0.23305243253707886
Batch 33/64 loss: -0.2555558681488037
Batch 34/64 loss: -0.21979773044586182
Batch 35/64 loss: -0.22367912530899048
Batch 36/64 loss: -0.21440523862838745
Batch 37/64 loss: -0.25580936670303345
Batch 38/64 loss: -0.2417026162147522
Batch 39/64 loss: -0.21934425830841064
Batch 40/64 loss: -0.24305272102355957
Batch 41/64 loss: -0.23051691055297852
Batch 42/64 loss: -0.23876506090164185
Batch 43/64 loss: -0.236488938331604
Batch 44/64 loss: -0.24005353450775146
Batch 45/64 loss: -0.23910152912139893
Batch 46/64 loss: -0.2066645622253418
Batch 47/64 loss: -0.21654444932937622
Batch 48/64 loss: -0.2132096290588379
Batch 49/64 loss: -0.23237591981887817
Batch 50/64 loss: -0.21489036083221436
Batch 51/64 loss: -0.20928752422332764
Batch 52/64 loss: -0.1935673952102661
Batch 53/64 loss: -0.2367624044418335
Batch 54/64 loss: -0.22183895111083984
Batch 55/64 loss: -0.23267078399658203
Batch 56/64 loss: -0.22307145595550537
Batch 57/64 loss: -0.23382651805877686
Batch 58/64 loss: -0.21086764335632324
Batch 59/64 loss: -0.21285539865493774
Batch 60/64 loss: -0.23377609252929688
Batch 61/64 loss: -0.21903270483016968
Batch 62/64 loss: -0.2205764651298523
Batch 63/64 loss: -0.23735755681991577
Batch 64/64 loss: -0.2287309169769287
Epoch 115  Train loss: -0.22925672905117858  Val loss: -0.1996911479435426
Epoch 116
-------------------------------
Batch 1/64 loss: -0.23459279537200928
Batch 2/64 loss: -0.23461860418319702
Batch 3/64 loss: -0.24233216047286987
Batch 4/64 loss: -0.2341822385787964
Batch 5/64 loss: -0.24137848615646362
Batch 6/64 loss: -0.23841679096221924
Batch 7/64 loss: -0.23522961139678955
Batch 8/64 loss: -0.26113343238830566
Batch 9/64 loss: -0.2340889573097229
Batch 10/64 loss: -0.21606779098510742
Batch 11/64 loss: -0.22903811931610107
Batch 12/64 loss: -0.2304234504699707
Batch 13/64 loss: -0.2231559157371521
Batch 14/64 loss: -0.23515069484710693
Batch 15/64 loss: -0.21945929527282715
Batch 16/64 loss: -0.22762137651443481
Batch 17/64 loss: -0.23108631372451782
Batch 18/64 loss: -0.2048969268798828
Batch 19/64 loss: -0.2109980583190918
Batch 20/64 loss: -0.22131305932998657
Batch 21/64 loss: -0.24424415826797485
Batch 22/64 loss: -0.25835520029067993
Batch 23/64 loss: -0.21924841403961182
Batch 24/64 loss: -0.23669123649597168
Batch 25/64 loss: -0.22289323806762695
Batch 26/64 loss: -0.22383064031600952
Batch 27/64 loss: -0.25253844261169434
Batch 28/64 loss: -0.23619449138641357
Batch 29/64 loss: -0.22286081314086914
Batch 30/64 loss: -0.215587317943573
Batch 31/64 loss: -0.23315119743347168
Batch 32/64 loss: -0.2571631073951721
Batch 33/64 loss: -0.2481294870376587
Batch 34/64 loss: -0.23649287223815918
Batch 35/64 loss: -0.23472750186920166
Batch 36/64 loss: -0.20021158456802368
Batch 37/64 loss: -0.25224024057388306
Batch 38/64 loss: -0.2394375205039978
Batch 39/64 loss: -0.17613846063613892
Batch 40/64 loss: -0.20974689722061157
Batch 41/64 loss: -0.2120363712310791
Batch 42/64 loss: -0.23302596807479858
Batch 43/64 loss: -0.21820765733718872
Batch 44/64 loss: -0.24347931146621704
Batch 45/64 loss: -0.21702051162719727
Batch 46/64 loss: -0.22578197717666626
Batch 47/64 loss: -0.22333920001983643
Batch 48/64 loss: -0.24832451343536377
Batch 49/64 loss: -0.19856441020965576
Batch 50/64 loss: -0.2320430874824524
Batch 51/64 loss: -0.2350480556488037
Batch 52/64 loss: -0.23859798908233643
Batch 53/64 loss: -0.24375009536743164
Batch 54/64 loss: -0.25981128215789795
Batch 55/64 loss: -0.23195451498031616
Batch 56/64 loss: -0.24279367923736572
Batch 57/64 loss: -0.21447181701660156
Batch 58/64 loss: -0.24225205183029175
Batch 59/64 loss: -0.23700213432312012
Batch 60/64 loss: -0.23032450675964355
Batch 61/64 loss: -0.22361135482788086
Batch 62/64 loss: -0.23368030786514282
Batch 63/64 loss: -0.24900662899017334
Batch 64/64 loss: -0.2386842966079712
Epoch 116  Train loss: -0.23118756939383114  Val loss: -0.19722379646760083
Epoch 117
-------------------------------
Batch 1/64 loss: -0.22789233922958374
Batch 2/64 loss: -0.2188209891319275
Batch 3/64 loss: -0.23521757125854492
Batch 4/64 loss: -0.24578237533569336
Batch 5/64 loss: -0.2224021553993225
Batch 6/64 loss: -0.22140604257583618
Batch 7/64 loss: -0.2310534119606018
Batch 8/64 loss: -0.24525481462478638
Batch 9/64 loss: -0.2370668649673462
Batch 10/64 loss: -0.23887044191360474
Batch 11/64 loss: -0.2094477415084839
Batch 12/64 loss: -0.24606388807296753
Batch 13/64 loss: -0.24310028553009033
Batch 14/64 loss: -0.21998727321624756
Batch 15/64 loss: -0.23470056056976318
Batch 16/64 loss: -0.233562171459198
Batch 17/64 loss: -0.23298412561416626
Batch 18/64 loss: -0.2413698434829712
Batch 19/64 loss: -0.2330741286277771
Batch 20/64 loss: -0.2460229992866516
Batch 21/64 loss: -0.21721720695495605
Batch 22/64 loss: -0.2247244119644165
Batch 23/64 loss: -0.21861237287521362
Batch 24/64 loss: -0.2428884506225586
Batch 25/64 loss: -0.236289381980896
Batch 26/64 loss: -0.2294883131980896
Batch 27/64 loss: -0.2301337718963623
Batch 28/64 loss: -0.216516375541687
Batch 29/64 loss: -0.24764472246170044
Batch 30/64 loss: -0.23679792881011963
Batch 31/64 loss: -0.24260324239730835
Batch 32/64 loss: -0.24193423986434937
Batch 33/64 loss: -0.23543065786361694
Batch 34/64 loss: -0.2341884970664978
Batch 35/64 loss: -0.2221270203590393
Batch 36/64 loss: -0.2335774302482605
Batch 37/64 loss: -0.23857784271240234
Batch 38/64 loss: -0.22247689962387085
Batch 39/64 loss: -0.24471867084503174
Batch 40/64 loss: -0.2430306077003479
Batch 41/64 loss: -0.21814018487930298
Batch 42/64 loss: -0.2120092511177063
Batch 43/64 loss: -0.21362507343292236
Batch 44/64 loss: -0.23622822761535645
Batch 45/64 loss: -0.22270911931991577
Batch 46/64 loss: -0.23002421855926514
Batch 47/64 loss: -0.2571384906768799
Batch 48/64 loss: -0.22675204277038574
Batch 49/64 loss: -0.24406194686889648
Batch 50/64 loss: -0.23126858472824097
Batch 51/64 loss: -0.212723970413208
Batch 52/64 loss: -0.23783564567565918
Batch 53/64 loss: -0.23794549703598022
Batch 54/64 loss: -0.2284955382347107
Batch 55/64 loss: -0.23269414901733398
Batch 56/64 loss: -0.20168662071228027
Batch 57/64 loss: -0.2261486053466797
Batch 58/64 loss: -0.2377687692642212
Batch 59/64 loss: -0.2129186987876892
Batch 60/64 loss: -0.22600984573364258
Batch 61/64 loss: -0.22507035732269287
Batch 62/64 loss: -0.24431520700454712
Batch 63/64 loss: -0.2357160449028015
Batch 64/64 loss: -0.21588796377182007
Epoch 117  Train loss: -0.2311570216627682  Val loss: -0.18303009877909499
Epoch 118
-------------------------------
Batch 1/64 loss: -0.2238338589668274
Batch 2/64 loss: -0.21724236011505127
Batch 3/64 loss: -0.23036181926727295
Batch 4/64 loss: -0.22143876552581787
Batch 5/64 loss: -0.2293022871017456
Batch 6/64 loss: -0.22231042385101318
Batch 7/64 loss: -0.24055802822113037
Batch 8/64 loss: -0.22763842344284058
Batch 9/64 loss: -0.24799132347106934
Batch 10/64 loss: -0.2476292848587036
Batch 11/64 loss: -0.23721522092819214
Batch 12/64 loss: -0.24268484115600586
Batch 13/64 loss: -0.24109530448913574
Batch 14/64 loss: -0.23432451486587524
Batch 15/64 loss: -0.2431674599647522
Batch 16/64 loss: -0.2423977255821228
Batch 17/64 loss: -0.24908125400543213
Batch 18/64 loss: -0.23175424337387085
Batch 19/64 loss: -0.2451556921005249
Batch 20/64 loss: -0.22624939680099487
Batch 21/64 loss: -0.24707841873168945
Batch 22/64 loss: -0.23888033628463745
Batch 23/64 loss: -0.22717279195785522
Batch 24/64 loss: -0.2481784224510193
Batch 25/64 loss: -0.2375197410583496
Batch 26/64 loss: -0.21450567245483398
Batch 27/64 loss: -0.2552129030227661
Batch 28/64 loss: -0.23914849758148193
Batch 29/64 loss: -0.24266809225082397
Batch 30/64 loss: -0.21501123905181885
Batch 31/64 loss: -0.2371077537536621
Batch 32/64 loss: -0.22759610414505005
Batch 33/64 loss: -0.24827337265014648
Batch 34/64 loss: -0.2455543875694275
Batch 35/64 loss: -0.21711581945419312
Batch 36/64 loss: -0.23874211311340332
Batch 37/64 loss: -0.22718876600265503
Batch 38/64 loss: -0.21836239099502563
Batch 39/64 loss: -0.23847609758377075
Batch 40/64 loss: -0.24179011583328247
Batch 41/64 loss: -0.2276773452758789
Batch 42/64 loss: -0.2409244179725647
Batch 43/64 loss: -0.22510749101638794
Batch 44/64 loss: -0.24551630020141602
Batch 45/64 loss: -0.22054678201675415
Batch 46/64 loss: -0.2227352261543274
Batch 47/64 loss: -0.23862791061401367
Batch 48/64 loss: -0.23459380865097046
Batch 49/64 loss: -0.2295326590538025
Batch 50/64 loss: -0.24782586097717285
Batch 51/64 loss: -0.2541011571884155
Batch 52/64 loss: -0.23830217123031616
Batch 53/64 loss: -0.239277184009552
Batch 54/64 loss: -0.25220930576324463
Batch 55/64 loss: -0.2221735119819641
Batch 56/64 loss: -0.23731470108032227
Batch 57/64 loss: -0.24175065755844116
Batch 58/64 loss: -0.20831966400146484
Batch 59/64 loss: -0.22108447551727295
Batch 60/64 loss: -0.22843801975250244
Batch 61/64 loss: -0.23694777488708496
Batch 62/64 loss: -0.25263917446136475
Batch 63/64 loss: -0.1951414942741394
Batch 64/64 loss: -0.2358243465423584
Epoch 118  Train loss: -0.2344262057659673  Val loss: -0.20309033836286092
Epoch 119
-------------------------------
Batch 1/64 loss: -0.22982418537139893
Batch 2/64 loss: -0.23366129398345947
Batch 3/64 loss: -0.2378634810447693
Batch 4/64 loss: -0.20363545417785645
Batch 5/64 loss: -0.23154139518737793
Batch 6/64 loss: -0.22699260711669922
Batch 7/64 loss: -0.23437011241912842
Batch 8/64 loss: -0.23089617490768433
Batch 9/64 loss: -0.24743276834487915
Batch 10/64 loss: -0.24234694242477417
Batch 11/64 loss: -0.2362992763519287
Batch 12/64 loss: -0.23633211851119995
Batch 13/64 loss: -0.2350136637687683
Batch 14/64 loss: -0.23201197385787964
Batch 15/64 loss: -0.22758442163467407
Batch 16/64 loss: -0.22935909032821655
Batch 17/64 loss: -0.22713220119476318
Batch 18/64 loss: -0.19610464572906494
Batch 19/64 loss: -0.23186379671096802
Batch 20/64 loss: -0.24370259046554565
Batch 21/64 loss: -0.2160237431526184
Batch 22/64 loss: -0.23266690969467163
Batch 23/64 loss: -0.24184530973434448
Batch 24/64 loss: -0.23635601997375488
Batch 25/64 loss: -0.23960578441619873
Batch 26/64 loss: -0.21769237518310547
Batch 27/64 loss: -0.23505401611328125
Batch 28/64 loss: -0.23278272151947021
Batch 29/64 loss: -0.2448822259902954
Batch 30/64 loss: -0.239343523979187
Batch 31/64 loss: -0.22719568014144897
Batch 32/64 loss: -0.21414965391159058
Batch 33/64 loss: -0.23615849018096924
Batch 34/64 loss: -0.24533116817474365
Batch 35/64 loss: -0.22446614503860474
Batch 36/64 loss: -0.21174287796020508
Batch 37/64 loss: -0.2461363673210144
Batch 38/64 loss: -0.2517954111099243
Batch 39/64 loss: -0.2519306540489197
Batch 40/64 loss: -0.2562112808227539
Batch 41/64 loss: -0.2182328701019287
Batch 42/64 loss: -0.23196983337402344
Batch 43/64 loss: -0.24887162446975708
Batch 44/64 loss: -0.24391412734985352
Batch 45/64 loss: -0.2633016109466553
Batch 46/64 loss: -0.21221530437469482
Batch 47/64 loss: -0.23487305641174316
Batch 48/64 loss: -0.22797781229019165
Batch 49/64 loss: -0.2435455322265625
Batch 50/64 loss: -0.25224030017852783
Batch 51/64 loss: -0.24048179388046265
Batch 52/64 loss: -0.2608741521835327
Batch 53/64 loss: -0.23762226104736328
Batch 54/64 loss: -0.22685927152633667
Batch 55/64 loss: -0.22935903072357178
Batch 56/64 loss: -0.23750311136245728
Batch 57/64 loss: -0.2506083846092224
Batch 58/64 loss: -0.2181606888771057
Batch 59/64 loss: -0.23469913005828857
Batch 60/64 loss: -0.23680973052978516
Batch 61/64 loss: -0.23982936143875122
Batch 62/64 loss: -0.24393409490585327
Batch 63/64 loss: -0.25194406509399414
Batch 64/64 loss: -0.21993017196655273
Epoch 119  Train loss: -0.23476256202248966  Val loss: -0.2091722914443393
Saving best model, epoch: 119
Epoch 120
-------------------------------
Batch 1/64 loss: -0.24791157245635986
Batch 2/64 loss: -0.23636972904205322
Batch 3/64 loss: -0.23024028539657593
Batch 4/64 loss: -0.24688774347305298
Batch 5/64 loss: -0.23494160175323486
Batch 6/64 loss: -0.2070939540863037
Batch 7/64 loss: -0.2405238151550293
Batch 8/64 loss: -0.2259851098060608
Batch 9/64 loss: -0.24069547653198242
Batch 10/64 loss: -0.21691977977752686
Batch 11/64 loss: -0.2346503734588623
Batch 12/64 loss: -0.2484828233718872
Batch 13/64 loss: -0.23738640546798706
Batch 14/64 loss: -0.2293214201927185
Batch 15/64 loss: -0.2366616129875183
Batch 16/64 loss: -0.2428959608078003
Batch 17/64 loss: -0.23606735467910767
Batch 18/64 loss: -0.22618317604064941
Batch 19/64 loss: -0.23231369256973267
Batch 20/64 loss: -0.23254424333572388
Batch 21/64 loss: -0.21703898906707764
Batch 22/64 loss: -0.20147299766540527
Batch 23/64 loss: -0.25127893686294556
Batch 24/64 loss: -0.2475089430809021
Batch 25/64 loss: -0.2288212776184082
Batch 26/64 loss: -0.24301999807357788
Batch 27/64 loss: -0.2321556806564331
Batch 28/64 loss: -0.2481447458267212
Batch 29/64 loss: -0.22791528701782227
Batch 30/64 loss: -0.2202947735786438
Batch 31/64 loss: -0.24673831462860107
Batch 32/64 loss: -0.23621487617492676
Batch 33/64 loss: -0.2393054962158203
Batch 34/64 loss: -0.23225855827331543
Batch 35/64 loss: -0.24340182542800903
Batch 36/64 loss: -0.24079811573028564
Batch 37/64 loss: -0.24154865741729736
Batch 38/64 loss: -0.229445219039917
Batch 39/64 loss: -0.24364149570465088
Batch 40/64 loss: -0.22446691989898682
Batch 41/64 loss: -0.219682514667511
Batch 42/64 loss: -0.25321483612060547
Batch 43/64 loss: -0.24973928928375244
Batch 44/64 loss: -0.24237513542175293
Batch 45/64 loss: -0.22052937746047974
Batch 46/64 loss: -0.24306583404541016
Batch 47/64 loss: -0.23993682861328125
Batch 48/64 loss: -0.2502342462539673
Batch 49/64 loss: -0.2483348846435547
Batch 50/64 loss: -0.2384345531463623
Batch 51/64 loss: -0.23289138078689575
Batch 52/64 loss: -0.2500290274620056
Batch 53/64 loss: -0.2237403392791748
Batch 54/64 loss: -0.24324750900268555
Batch 55/64 loss: -0.22486352920532227
Batch 56/64 loss: -0.24556785821914673
Batch 57/64 loss: -0.24134135246276855
Batch 58/64 loss: -0.2354610562324524
Batch 59/64 loss: -0.22877496480941772
Batch 60/64 loss: -0.23409271240234375
Batch 61/64 loss: -0.2415880560874939
Batch 62/64 loss: -0.2560722231864929
Batch 63/64 loss: -0.24042528867721558
Batch 64/64 loss: -0.24707406759262085
Epoch 120  Train loss: -0.23636855821983488  Val loss: -0.2061564920284494
Epoch 121
-------------------------------
Batch 1/64 loss: -0.2462664246559143
Batch 2/64 loss: -0.24930119514465332
Batch 3/64 loss: -0.24002820253372192
Batch 4/64 loss: -0.25156235694885254
Batch 5/64 loss: -0.2294657826423645
Batch 6/64 loss: -0.2124241590499878
Batch 7/64 loss: -0.22615289688110352
Batch 8/64 loss: -0.25944072008132935
Batch 9/64 loss: -0.2319471836090088
Batch 10/64 loss: -0.23784315586090088
Batch 11/64 loss: -0.26504600048065186
Batch 12/64 loss: -0.22015857696533203
Batch 13/64 loss: -0.24205738306045532
Batch 14/64 loss: -0.22447288036346436
Batch 15/64 loss: -0.2489669919013977
Batch 16/64 loss: -0.24535638093948364
Batch 17/64 loss: -0.22136855125427246
Batch 18/64 loss: -0.24400389194488525
Batch 19/64 loss: -0.22877627611160278
Batch 20/64 loss: -0.24612104892730713
Batch 21/64 loss: -0.24830114841461182
Batch 22/64 loss: -0.23771953582763672
Batch 23/64 loss: -0.24576234817504883
Batch 24/64 loss: -0.22158735990524292
Batch 25/64 loss: -0.2540794610977173
Batch 26/64 loss: -0.2496318221092224
Batch 27/64 loss: -0.2288464903831482
Batch 28/64 loss: -0.24123775959014893
Batch 29/64 loss: -0.25068503618240356
Batch 30/64 loss: -0.23903357982635498
Batch 31/64 loss: -0.23325049877166748
Batch 32/64 loss: -0.21567445993423462
Batch 33/64 loss: -0.23517686128616333
Batch 34/64 loss: -0.24762558937072754
Batch 35/64 loss: -0.22394829988479614
Batch 36/64 loss: -0.2233636975288391
Batch 37/64 loss: -0.23936426639556885
Batch 38/64 loss: -0.24615508317947388
Batch 39/64 loss: -0.24357229471206665
Batch 40/64 loss: -0.23892265558242798
Batch 41/64 loss: -0.22698628902435303
Batch 42/64 loss: -0.24761039018630981
Batch 43/64 loss: -0.24003660678863525
Batch 44/64 loss: -0.2264268398284912
Batch 45/64 loss: -0.24631589651107788
Batch 46/64 loss: -0.22673767805099487
Batch 47/64 loss: -0.229469895362854
Batch 48/64 loss: -0.226789653301239
Batch 49/64 loss: -0.2566072344779968
Batch 50/64 loss: -0.22794437408447266
Batch 51/64 loss: -0.2323285937309265
Batch 52/64 loss: -0.24380463361740112
Batch 53/64 loss: -0.24896204471588135
Batch 54/64 loss: -0.231714129447937
Batch 55/64 loss: -0.2337433099746704
Batch 56/64 loss: -0.2564932107925415
Batch 57/64 loss: -0.22249484062194824
Batch 58/64 loss: -0.1977757215499878
Batch 59/64 loss: -0.24049293994903564
Batch 60/64 loss: -0.21546506881713867
Batch 61/64 loss: -0.22714662551879883
Batch 62/64 loss: -0.23965543508529663
Batch 63/64 loss: -0.2477409839630127
Batch 64/64 loss: -0.2293427586555481
Epoch 121  Train loss: -0.2368540826965781  Val loss: -0.20829810268690496
Epoch 122
-------------------------------
Batch 1/64 loss: -0.23998326063156128
Batch 2/64 loss: -0.23780322074890137
Batch 3/64 loss: -0.24324703216552734
Batch 4/64 loss: -0.22177940607070923
Batch 5/64 loss: -0.2132120132446289
Batch 6/64 loss: -0.21942895650863647
Batch 7/64 loss: -0.2482704520225525
Batch 8/64 loss: -0.2354448437690735
Batch 9/64 loss: -0.22806358337402344
Batch 10/64 loss: -0.2403625249862671
Batch 11/64 loss: -0.242822527885437
Batch 12/64 loss: -0.2365679144859314
Batch 13/64 loss: -0.2246553897857666
Batch 14/64 loss: -0.2519304156303406
Batch 15/64 loss: -0.24425429105758667
Batch 16/64 loss: -0.2379615306854248
Batch 17/64 loss: -0.22157907485961914
Batch 18/64 loss: -0.24459540843963623
Batch 19/64 loss: -0.2232431173324585
Batch 20/64 loss: -0.22918158769607544
Batch 21/64 loss: -0.2593798041343689
Batch 22/64 loss: -0.23721033334732056
Batch 23/64 loss: -0.23803138732910156
Batch 24/64 loss: -0.2490825653076172
Batch 25/64 loss: -0.23149371147155762
Batch 26/64 loss: -0.2360857129096985
Batch 27/64 loss: -0.2387651801109314
Batch 28/64 loss: -0.23985004425048828
Batch 29/64 loss: -0.2219630479812622
Batch 30/64 loss: -0.24182885885238647
Batch 31/64 loss: -0.2271549105644226
Batch 32/64 loss: -0.23574906587600708
Batch 33/64 loss: -0.23835039138793945
Batch 34/64 loss: -0.24209743738174438
Batch 35/64 loss: -0.24817651510238647
Batch 36/64 loss: -0.217759370803833
Batch 37/64 loss: -0.25316929817199707
Batch 38/64 loss: -0.22798681259155273
Batch 39/64 loss: -0.21618133783340454
Batch 40/64 loss: -0.211955726146698
Batch 41/64 loss: -0.24606794118881226
Batch 42/64 loss: -0.23867881298065186
Batch 43/64 loss: -0.24436676502227783
Batch 44/64 loss: -0.21397560834884644
Batch 45/64 loss: -0.2451310157775879
Batch 46/64 loss: -0.2372448444366455
Batch 47/64 loss: -0.2519156336784363
Batch 48/64 loss: -0.22202080488204956
Batch 49/64 loss: -0.24272900819778442
Batch 50/64 loss: -0.23678851127624512
Batch 51/64 loss: -0.25101619958877563
Batch 52/64 loss: -0.2295975685119629
Batch 53/64 loss: -0.25880295038223267
Batch 54/64 loss: -0.21986019611358643
Batch 55/64 loss: -0.24960333108901978
Batch 56/64 loss: -0.23675072193145752
Batch 57/64 loss: -0.2188776135444641
Batch 58/64 loss: -0.24417495727539062
Batch 59/64 loss: -0.24222981929779053
Batch 60/64 loss: -0.21904122829437256
Batch 61/64 loss: -0.2443079948425293
Batch 62/64 loss: -0.24409770965576172
Batch 63/64 loss: -0.25188761949539185
Batch 64/64 loss: -0.25817394256591797
Epoch 122  Train loss: -0.23654047648111978  Val loss: -0.19549425811702034
Epoch 123
-------------------------------
Batch 1/64 loss: -0.2248755693435669
Batch 2/64 loss: -0.24495458602905273
Batch 3/64 loss: -0.2552913427352905
Batch 4/64 loss: -0.23474174737930298
Batch 5/64 loss: -0.2274375557899475
Batch 6/64 loss: -0.22661566734313965
Batch 7/64 loss: -0.25061798095703125
Batch 8/64 loss: -0.24680721759796143
Batch 9/64 loss: -0.2369706630706787
Batch 10/64 loss: -0.2399963140487671
Batch 11/64 loss: -0.21723616123199463
Batch 12/64 loss: -0.2251114845275879
Batch 13/64 loss: -0.23896431922912598
Batch 14/64 loss: -0.24197256565093994
Batch 15/64 loss: -0.2468942403793335
Batch 16/64 loss: -0.24498915672302246
Batch 17/64 loss: -0.24142789840698242
Batch 18/64 loss: -0.24027180671691895
Batch 19/64 loss: -0.25877463817596436
Batch 20/64 loss: -0.24057424068450928
Batch 21/64 loss: -0.2402554750442505
Batch 22/64 loss: -0.25217151641845703
Batch 23/64 loss: -0.22297346591949463
Batch 24/64 loss: -0.260006844997406
Batch 25/64 loss: -0.23187410831451416
Batch 26/64 loss: -0.24384653568267822
Batch 27/64 loss: -0.2474767565727234
Batch 28/64 loss: -0.24198925495147705
Batch 29/64 loss: -0.25950366258621216
Batch 30/64 loss: -0.2484695315361023
Batch 31/64 loss: -0.2334652543067932
Batch 32/64 loss: -0.24411463737487793
Batch 33/64 loss: -0.23920732736587524
Batch 34/64 loss: -0.23039692640304565
Batch 35/64 loss: -0.247283935546875
Batch 36/64 loss: -0.24685722589492798
Batch 37/64 loss: -0.2395668625831604
Batch 38/64 loss: -0.25757575035095215
Batch 39/64 loss: -0.2423924207687378
Batch 40/64 loss: -0.24497568607330322
Batch 41/64 loss: -0.2420446276664734
Batch 42/64 loss: -0.23872095346450806
Batch 43/64 loss: -0.23921626806259155
Batch 44/64 loss: -0.24270105361938477
Batch 45/64 loss: -0.21998465061187744
Batch 46/64 loss: -0.2355787754058838
Batch 47/64 loss: -0.2600620985031128
Batch 48/64 loss: -0.23044335842132568
Batch 49/64 loss: -0.24985575675964355
Batch 50/64 loss: -0.2393801212310791
Batch 51/64 loss: -0.23030853271484375
Batch 52/64 loss: -0.23026823997497559
Batch 53/64 loss: -0.20722860097885132
Batch 54/64 loss: -0.23118847608566284
Batch 55/64 loss: -0.2456064224243164
Batch 56/64 loss: -0.2377694845199585
Batch 57/64 loss: -0.24390101432800293
Batch 58/64 loss: -0.22711175680160522
Batch 59/64 loss: -0.22360742092132568
Batch 60/64 loss: -0.2521226406097412
Batch 61/64 loss: -0.23632335662841797
Batch 62/64 loss: -0.24358701705932617
Batch 63/64 loss: -0.23219573497772217
Batch 64/64 loss: -0.23724597692489624
Epoch 123  Train loss: -0.23962461457532994  Val loss: -0.21009101429346091
Saving best model, epoch: 123
Epoch 124
-------------------------------
Batch 1/64 loss: -0.23238801956176758
Batch 2/64 loss: -0.25077205896377563
Batch 3/64 loss: -0.23862314224243164
Batch 4/64 loss: -0.23769408464431763
Batch 5/64 loss: -0.25458163022994995
Batch 6/64 loss: -0.2510693669319153
Batch 7/64 loss: -0.22705423831939697
Batch 8/64 loss: -0.21930217742919922
Batch 9/64 loss: -0.24288654327392578
Batch 10/64 loss: -0.2419745922088623
Batch 11/64 loss: -0.23603671789169312
Batch 12/64 loss: -0.24524128437042236
Batch 13/64 loss: -0.21877527236938477
Batch 14/64 loss: -0.24511325359344482
Batch 15/64 loss: -0.2367193102836609
Batch 16/64 loss: -0.21667617559432983
Batch 17/64 loss: -0.24884843826293945
Batch 18/64 loss: -0.21874982118606567
Batch 19/64 loss: -0.2300257682800293
Batch 20/64 loss: -0.2325463891029358
Batch 21/64 loss: -0.22612839937210083
Batch 22/64 loss: -0.22427576780319214
Batch 23/64 loss: -0.22568482160568237
Batch 24/64 loss: -0.2183314561843872
Batch 25/64 loss: -0.21387475728988647
Batch 26/64 loss: -0.23307842016220093
Batch 27/64 loss: -0.21458572149276733
Batch 28/64 loss: -0.2282198667526245
Batch 29/64 loss: -0.22791635990142822
Batch 30/64 loss: -0.2203139066696167
Batch 31/64 loss: -0.23206841945648193
Batch 32/64 loss: -0.1934647560119629
Batch 33/64 loss: -0.20323026180267334
Batch 34/64 loss: -0.2097233533859253
Batch 35/64 loss: -0.2361164689064026
Batch 36/64 loss: -0.1863141655921936
Batch 37/64 loss: -0.2338244915008545
Batch 38/64 loss: -0.2314281463623047
Batch 39/64 loss: -0.23993349075317383
Batch 40/64 loss: -0.21890735626220703
Batch 41/64 loss: -0.23876625299453735
Batch 42/64 loss: -0.20979827642440796
Batch 43/64 loss: -0.24041956663131714
Batch 44/64 loss: -0.22395002841949463
Batch 45/64 loss: -0.23373502492904663
Batch 46/64 loss: -0.24245965480804443
Batch 47/64 loss: -0.22203493118286133
Batch 48/64 loss: -0.2534259557723999
Batch 49/64 loss: -0.24052834510803223
Batch 50/64 loss: -0.23523211479187012
Batch 51/64 loss: -0.22970998287200928
Batch 52/64 loss: -0.22438716888427734
Batch 53/64 loss: -0.2399502992630005
Batch 54/64 loss: -0.21625208854675293
Batch 55/64 loss: -0.22753453254699707
Batch 56/64 loss: -0.24690109491348267
Batch 57/64 loss: -0.24922043085098267
Batch 58/64 loss: -0.24864685535430908
Batch 59/64 loss: -0.2547834515571594
Batch 60/64 loss: -0.22061491012573242
Batch 61/64 loss: -0.23929715156555176
Batch 62/64 loss: -0.2445213794708252
Batch 63/64 loss: -0.24404972791671753
Batch 64/64 loss: -0.244221031665802
Epoch 124  Train loss: -0.2314020967951008  Val loss: -0.20057324056363188
Epoch 125
-------------------------------
Batch 1/64 loss: -0.2284826636314392
Batch 2/64 loss: -0.2265281081199646
Batch 3/64 loss: -0.24099743366241455
Batch 4/64 loss: -0.2573888301849365
Batch 5/64 loss: -0.24085450172424316
Batch 6/64 loss: -0.23628485202789307
Batch 7/64 loss: -0.24348503351211548
Batch 8/64 loss: -0.2393069863319397
Batch 9/64 loss: -0.25771498680114746
Batch 10/64 loss: -0.2454463243484497
Batch 11/64 loss: -0.25550544261932373
Batch 12/64 loss: -0.22979313135147095
Batch 13/64 loss: -0.25151026248931885
Batch 14/64 loss: -0.25136542320251465
Batch 15/64 loss: -0.24482423067092896
Batch 16/64 loss: -0.2426142692565918
Batch 17/64 loss: -0.24000835418701172
Batch 18/64 loss: -0.23023009300231934
Batch 19/64 loss: -0.24203777313232422
Batch 20/64 loss: -0.2456609010696411
Batch 21/64 loss: -0.22193127870559692
Batch 22/64 loss: -0.2371789813041687
Batch 23/64 loss: -0.2511575222015381
Batch 24/64 loss: -0.23516559600830078
Batch 25/64 loss: -0.2577511668205261
Batch 26/64 loss: -0.23727631568908691
Batch 27/64 loss: -0.2358115315437317
Batch 28/64 loss: -0.2424364686012268
Batch 29/64 loss: -0.23388218879699707
Batch 30/64 loss: -0.23654508590698242
Batch 31/64 loss: -0.2492755651473999
Batch 32/64 loss: -0.20992803573608398
Batch 33/64 loss: -0.24930834770202637
Batch 34/64 loss: -0.23982548713684082
Batch 35/64 loss: -0.23637139797210693
Batch 36/64 loss: -0.22164559364318848
Batch 37/64 loss: -0.24610304832458496
Batch 38/64 loss: -0.2408396601676941
Batch 39/64 loss: -0.24855762720108032
Batch 40/64 loss: -0.22429704666137695
Batch 41/64 loss: -0.2114708423614502
Batch 42/64 loss: -0.24550622701644897
Batch 43/64 loss: -0.2168826460838318
Batch 44/64 loss: -0.2437816858291626
Batch 45/64 loss: -0.24549025297164917
Batch 46/64 loss: -0.23009270429611206
Batch 47/64 loss: -0.243066668510437
Batch 48/64 loss: -0.2446218729019165
Batch 49/64 loss: -0.2240997552871704
Batch 50/64 loss: -0.23409926891326904
Batch 51/64 loss: -0.23370057344436646
Batch 52/64 loss: -0.24917125701904297
Batch 53/64 loss: -0.23081517219543457
Batch 54/64 loss: -0.2306758165359497
Batch 55/64 loss: -0.24397575855255127
Batch 56/64 loss: -0.22896963357925415
Batch 57/64 loss: -0.21794575452804565
Batch 58/64 loss: -0.24240583181381226
Batch 59/64 loss: -0.22983431816101074
Batch 60/64 loss: -0.2439700961112976
Batch 61/64 loss: -0.2556961178779602
Batch 62/64 loss: -0.24936974048614502
Batch 63/64 loss: -0.23929286003112793
Batch 64/64 loss: -0.25226449966430664
Epoch 125  Train loss: -0.23889350236630907  Val loss: -0.20899174754152594
Epoch 126
-------------------------------
Batch 1/64 loss: -0.24163216352462769
Batch 2/64 loss: -0.2369617223739624
Batch 3/64 loss: -0.2285451889038086
Batch 4/64 loss: -0.23593318462371826
Batch 5/64 loss: -0.22942078113555908
Batch 6/64 loss: -0.24113672971725464
Batch 7/64 loss: -0.22372829914093018
Batch 8/64 loss: -0.22524988651275635
Batch 9/64 loss: -0.22715699672698975
Batch 10/64 loss: -0.24156725406646729
Batch 11/64 loss: -0.2513166069984436
Batch 12/64 loss: -0.23665636777877808
Batch 13/64 loss: -0.23154133558273315
Batch 14/64 loss: -0.23951303958892822
Batch 15/64 loss: -0.24339580535888672
Batch 16/64 loss: -0.24782907962799072
Batch 17/64 loss: -0.22428911924362183
Batch 18/64 loss: -0.2315305471420288
Batch 19/64 loss: -0.21627771854400635
Batch 20/64 loss: -0.22303670644760132
Batch 21/64 loss: -0.2202470302581787
Batch 22/64 loss: -0.24349987506866455
Batch 23/64 loss: -0.2353278398513794
Batch 24/64 loss: -0.223777174949646
Batch 25/64 loss: -0.23247241973876953
Batch 26/64 loss: -0.2441716194152832
Batch 27/64 loss: -0.23514008522033691
Batch 28/64 loss: -0.2362154722213745
Batch 29/64 loss: -0.24013841152191162
Batch 30/64 loss: -0.2508409023284912
Batch 31/64 loss: -0.22984951734542847
Batch 32/64 loss: -0.23575395345687866
Batch 33/64 loss: -0.2296844720840454
Batch 34/64 loss: -0.21488863229751587
Batch 35/64 loss: -0.2573177218437195
Batch 36/64 loss: -0.236474871635437
Batch 37/64 loss: -0.2272202968597412
Batch 38/64 loss: -0.24620074033737183
Batch 39/64 loss: -0.23184698820114136
Batch 40/64 loss: -0.23241353034973145
Batch 41/64 loss: -0.22238504886627197
Batch 42/64 loss: -0.2067406177520752
Batch 43/64 loss: -0.2523955702781677
Batch 44/64 loss: -0.22867119312286377
Batch 45/64 loss: -0.2516864538192749
Batch 46/64 loss: -0.2224779725074768
Batch 47/64 loss: -0.2424410581588745
Batch 48/64 loss: -0.24392813444137573
Batch 49/64 loss: -0.25422942638397217
Batch 50/64 loss: -0.22210931777954102
Batch 51/64 loss: -0.24943047761917114
Batch 52/64 loss: -0.23540568351745605
Batch 53/64 loss: -0.24627816677093506
Batch 54/64 loss: -0.2627606987953186
Batch 55/64 loss: -0.22171109914779663
Batch 56/64 loss: -0.23317253589630127
Batch 57/64 loss: -0.2292589545249939
Batch 58/64 loss: -0.249193012714386
Batch 59/64 loss: -0.22959494590759277
Batch 60/64 loss: -0.24732106924057007
Batch 61/64 loss: -0.23187607526779175
Batch 62/64 loss: -0.23124432563781738
Batch 63/64 loss: -0.24045145511627197
Batch 64/64 loss: -0.2517606019973755
Epoch 126  Train loss: -0.235667197377074  Val loss: -0.2091533342997233
Epoch 127
-------------------------------
Batch 1/64 loss: -0.24151378870010376
Batch 2/64 loss: -0.23859673738479614
Batch 3/64 loss: -0.24642401933670044
Batch 4/64 loss: -0.24172264337539673
Batch 5/64 loss: -0.24078845977783203
Batch 6/64 loss: -0.24609923362731934
Batch 7/64 loss: -0.24827933311462402
Batch 8/64 loss: -0.23883521556854248
Batch 9/64 loss: -0.24550926685333252
Batch 10/64 loss: -0.22684037685394287
Batch 11/64 loss: -0.24795252084732056
Batch 12/64 loss: -0.2430891990661621
Batch 13/64 loss: -0.2438366413116455
Batch 14/64 loss: -0.22747009992599487
Batch 15/64 loss: -0.2330067753791809
Batch 16/64 loss: -0.23815304040908813
Batch 17/64 loss: -0.24218231439590454
Batch 18/64 loss: -0.2529703378677368
Batch 19/64 loss: -0.23997366428375244
Batch 20/64 loss: -0.2371997833251953
Batch 21/64 loss: -0.2324528694152832
Batch 22/64 loss: -0.23876327276229858
Batch 23/64 loss: -0.2513374090194702
Batch 24/64 loss: -0.23640549182891846
Batch 25/64 loss: -0.23739206790924072
Batch 26/64 loss: -0.22165316343307495
Batch 27/64 loss: -0.23730093240737915
Batch 28/64 loss: -0.24880588054656982
Batch 29/64 loss: -0.255290150642395
Batch 30/64 loss: -0.23649847507476807
Batch 31/64 loss: -0.2583281993865967
Batch 32/64 loss: -0.23715078830718994
Batch 33/64 loss: -0.25842612981796265
Batch 34/64 loss: -0.23226487636566162
Batch 35/64 loss: -0.24300169944763184
Batch 36/64 loss: -0.24673080444335938
Batch 37/64 loss: -0.22204208374023438
Batch 38/64 loss: -0.23071223497390747
Batch 39/64 loss: -0.25360798835754395
Batch 40/64 loss: -0.2356320023536682
Batch 41/64 loss: -0.2503851652145386
Batch 42/64 loss: -0.24693506956100464
Batch 43/64 loss: -0.2590932250022888
Batch 44/64 loss: -0.2220238447189331
Batch 45/64 loss: -0.23714429140090942
Batch 46/64 loss: -0.23334509134292603
Batch 47/64 loss: -0.2453961968421936
Batch 48/64 loss: -0.25118881464004517
Batch 49/64 loss: -0.2216660976409912
Batch 50/64 loss: -0.23977911472320557
Batch 51/64 loss: -0.243536114692688
Batch 52/64 loss: -0.24071550369262695
Batch 53/64 loss: -0.24774789810180664
Batch 54/64 loss: -0.21356844902038574
Batch 55/64 loss: -0.23528754711151123
Batch 56/64 loss: -0.24645143747329712
Batch 57/64 loss: -0.2343066930770874
Batch 58/64 loss: -0.2311861515045166
Batch 59/64 loss: -0.2342354655265808
Batch 60/64 loss: -0.2480984926223755
Batch 61/64 loss: -0.21680593490600586
Batch 62/64 loss: -0.24556958675384521
Batch 63/64 loss: -0.23547178506851196
Batch 64/64 loss: -0.24316740036010742
Epoch 127  Train loss: -0.23994593713797774  Val loss: -0.2198827672250492
Saving best model, epoch: 127
Epoch 128
-------------------------------
Batch 1/64 loss: -0.25710034370422363
Batch 2/64 loss: -0.23931258916854858
Batch 3/64 loss: -0.25490403175354004
Batch 4/64 loss: -0.24252218008041382
Batch 5/64 loss: -0.24827104806900024
Batch 6/64 loss: -0.2450718879699707
Batch 7/64 loss: -0.21796202659606934
Batch 8/64 loss: -0.25039684772491455
Batch 9/64 loss: -0.24480551481246948
Batch 10/64 loss: -0.2518778443336487
Batch 11/64 loss: -0.1920098066329956
Batch 12/64 loss: -0.25210142135620117
Batch 13/64 loss: -0.24013924598693848
Batch 14/64 loss: -0.2419220209121704
Batch 15/64 loss: -0.24226540327072144
Batch 16/64 loss: -0.24375855922698975
Batch 17/64 loss: -0.24696028232574463
Batch 18/64 loss: -0.2478773593902588
Batch 19/64 loss: -0.21868109703063965
Batch 20/64 loss: -0.2509737014770508
Batch 21/64 loss: -0.2444852590560913
Batch 22/64 loss: -0.2457815408706665
Batch 23/64 loss: -0.24776744842529297
Batch 24/64 loss: -0.2576568126678467
Batch 25/64 loss: -0.21215581893920898
Batch 26/64 loss: -0.21712154150009155
Batch 27/64 loss: -0.2521817088127136
Batch 28/64 loss: -0.24780750274658203
Batch 29/64 loss: -0.24892961978912354
Batch 30/64 loss: -0.24744141101837158
Batch 31/64 loss: -0.24499094486236572
Batch 32/64 loss: -0.24342131614685059
Batch 33/64 loss: -0.23569941520690918
Batch 34/64 loss: -0.24855315685272217
Batch 35/64 loss: -0.2539486885070801
Batch 36/64 loss: -0.22829383611679077
Batch 37/64 loss: -0.24889349937438965
Batch 38/64 loss: -0.25530415773391724
Batch 39/64 loss: -0.2329561710357666
Batch 40/64 loss: -0.2628217935562134
Batch 41/64 loss: -0.2455214262008667
Batch 42/64 loss: -0.25801509618759155
Batch 43/64 loss: -0.25997865200042725
Batch 44/64 loss: -0.2471606731414795
Batch 45/64 loss: -0.24445593357086182
Batch 46/64 loss: -0.25171613693237305
Batch 47/64 loss: -0.2504822015762329
Batch 48/64 loss: -0.2315845489501953
Batch 49/64 loss: -0.21816188097000122
Batch 50/64 loss: -0.24008482694625854
Batch 51/64 loss: -0.2538325786590576
Batch 52/64 loss: -0.2615278959274292
Batch 53/64 loss: -0.24152827262878418
Batch 54/64 loss: -0.25139617919921875
Batch 55/64 loss: -0.2526254653930664
Batch 56/64 loss: -0.23721706867218018
Batch 57/64 loss: -0.2527446746826172
Batch 58/64 loss: -0.2544199824333191
Batch 59/64 loss: -0.2412051558494568
Batch 60/64 loss: -0.21500349044799805
Batch 61/64 loss: -0.24063998460769653
Batch 62/64 loss: -0.24851632118225098
Batch 63/64 loss: -0.2439529299736023
Batch 64/64 loss: -0.26999127864837646
Epoch 128  Train loss: -0.2439119951397765  Val loss: -0.2180361135309095
Epoch 129
-------------------------------
Batch 1/64 loss: -0.23077386617660522
Batch 2/64 loss: -0.22564786672592163
Batch 3/64 loss: -0.23581397533416748
Batch 4/64 loss: -0.24531906843185425
Batch 5/64 loss: -0.2440730333328247
Batch 6/64 loss: -0.24486881494522095
Batch 7/64 loss: -0.24544209241867065
Batch 8/64 loss: -0.25498348474502563
Batch 9/64 loss: -0.23502308130264282
Batch 10/64 loss: -0.23707163333892822
Batch 11/64 loss: -0.2421916127204895
Batch 12/64 loss: -0.22625035047531128
Batch 13/64 loss: -0.25381308794021606
Batch 14/64 loss: -0.23923927545547485
Batch 15/64 loss: -0.2471906542778015
Batch 16/64 loss: -0.23479288816452026
Batch 17/64 loss: -0.2374122142791748
Batch 18/64 loss: -0.2451840043067932
Batch 19/64 loss: -0.23359405994415283
Batch 20/64 loss: -0.23550260066986084
Batch 21/64 loss: -0.23213475942611694
Batch 22/64 loss: -0.2585994601249695
Batch 23/64 loss: -0.24943751096725464
Batch 24/64 loss: -0.24565845727920532
Batch 25/64 loss: -0.2471563220024109
Batch 26/64 loss: -0.25006961822509766
Batch 27/64 loss: -0.226334810256958
Batch 28/64 loss: -0.25283145904541016
Batch 29/64 loss: -0.24031084775924683
Batch 30/64 loss: -0.2172195315361023
Batch 31/64 loss: -0.24865281581878662
Batch 32/64 loss: -0.2307426929473877
Batch 33/64 loss: -0.26883256435394287
Batch 34/64 loss: -0.2510913610458374
Batch 35/64 loss: -0.24539077281951904
Batch 36/64 loss: -0.25005650520324707
Batch 37/64 loss: -0.2417217493057251
Batch 38/64 loss: -0.2484046220779419
Batch 39/64 loss: -0.23365473747253418
Batch 40/64 loss: -0.2519261837005615
Batch 41/64 loss: -0.23370105028152466
Batch 42/64 loss: -0.23956501483917236
Batch 43/64 loss: -0.22968542575836182
Batch 44/64 loss: -0.24202418327331543
Batch 45/64 loss: -0.25502997636795044
Batch 46/64 loss: -0.22340333461761475
Batch 47/64 loss: -0.25779712200164795
Batch 48/64 loss: -0.2545226216316223
Batch 49/64 loss: -0.2535899877548218
Batch 50/64 loss: -0.2544128894805908
Batch 51/64 loss: -0.23992103338241577
Batch 52/64 loss: -0.2344309687614441
Batch 53/64 loss: -0.21195542812347412
Batch 54/64 loss: -0.24244016408920288
Batch 55/64 loss: -0.2494117021560669
Batch 56/64 loss: -0.2267422080039978
Batch 57/64 loss: -0.2174052596092224
Batch 58/64 loss: -0.25514185428619385
Batch 59/64 loss: -0.2445881962776184
Batch 60/64 loss: -0.2503434419631958
Batch 61/64 loss: -0.2519254684448242
Batch 62/64 loss: -0.23421066999435425
Batch 63/64 loss: -0.23986554145812988
Batch 64/64 loss: -0.23842614889144897
Epoch 129  Train loss: -0.2416525113816355  Val loss: -0.19183804632462176
Epoch 130
-------------------------------
Batch 1/64 loss: -0.2258216142654419
Batch 2/64 loss: -0.22674614191055298
Batch 3/64 loss: -0.24590587615966797
Batch 4/64 loss: -0.23746412992477417
Batch 5/64 loss: -0.2390238642692566
Batch 6/64 loss: -0.24579274654388428
Batch 7/64 loss: -0.22690117359161377
Batch 8/64 loss: -0.23877286911010742
Batch 9/64 loss: -0.23566830158233643
Batch 10/64 loss: -0.2560216784477234
Batch 11/64 loss: -0.23878008127212524
Batch 12/64 loss: -0.24525529146194458
Batch 13/64 loss: -0.2375476360321045
Batch 14/64 loss: -0.2472548484802246
Batch 15/64 loss: -0.2225170135498047
Batch 16/64 loss: -0.2474668025970459
Batch 17/64 loss: -0.2495412826538086
Batch 18/64 loss: -0.24974894523620605
Batch 19/64 loss: -0.22398442029953003
Batch 20/64 loss: -0.25073468685150146
Batch 21/64 loss: -0.2278226613998413
Batch 22/64 loss: -0.2018185257911682
Batch 23/64 loss: -0.23920100927352905
Batch 24/64 loss: -0.23836129903793335
Batch 25/64 loss: -0.2409154176712036
Batch 26/64 loss: -0.24951744079589844
Batch 27/64 loss: -0.23384177684783936
Batch 28/64 loss: -0.2342579960823059
Batch 29/64 loss: -0.23882806301116943
Batch 30/64 loss: -0.23687958717346191
Batch 31/64 loss: -0.24933916330337524
Batch 32/64 loss: -0.221230149269104
Batch 33/64 loss: -0.23894578218460083
Batch 34/64 loss: -0.22751390933990479
Batch 35/64 loss: -0.2299870252609253
Batch 36/64 loss: -0.2439408302307129
Batch 37/64 loss: -0.25330471992492676
Batch 38/64 loss: -0.24716877937316895
Batch 39/64 loss: -0.22636938095092773
Batch 40/64 loss: -0.2601906657218933
Batch 41/64 loss: -0.2273426651954651
Batch 42/64 loss: -0.25095415115356445
Batch 43/64 loss: -0.24586236476898193
Batch 44/64 loss: -0.2420472502708435
Batch 45/64 loss: -0.2357574701309204
Batch 46/64 loss: -0.2422008514404297
Batch 47/64 loss: -0.2616159915924072
Batch 48/64 loss: -0.24023640155792236
Batch 49/64 loss: -0.2427271008491516
Batch 50/64 loss: -0.24599337577819824
Batch 51/64 loss: -0.257986843585968
Batch 52/64 loss: -0.224420964717865
Batch 53/64 loss: -0.2489844560623169
Batch 54/64 loss: -0.23623383045196533
Batch 55/64 loss: -0.23228079080581665
Batch 56/64 loss: -0.22929006814956665
Batch 57/64 loss: -0.2298070192337036
Batch 58/64 loss: -0.2537938356399536
Batch 59/64 loss: -0.23630362749099731
Batch 60/64 loss: -0.246529221534729
Batch 61/64 loss: -0.2539198398590088
Batch 62/64 loss: -0.2294495701789856
Batch 63/64 loss: -0.23704856634140015
Batch 64/64 loss: -0.2246379852294922
Epoch 130  Train loss: -0.23921016992307176  Val loss: -0.2145894024380294
Epoch 131
-------------------------------
Batch 1/64 loss: -0.21741336584091187
Batch 2/64 loss: -0.25277483463287354
Batch 3/64 loss: -0.22663354873657227
Batch 4/64 loss: -0.2380441427230835
Batch 5/64 loss: -0.2725095748901367
Batch 6/64 loss: -0.2490931749343872
Batch 7/64 loss: -0.24480777978897095
Batch 8/64 loss: -0.2483365535736084
Batch 9/64 loss: -0.21432089805603027
Batch 10/64 loss: -0.24997329711914062
Batch 11/64 loss: -0.24675917625427246
Batch 12/64 loss: -0.2326493263244629
Batch 13/64 loss: -0.2557481527328491
Batch 14/64 loss: -0.24529659748077393
Batch 15/64 loss: -0.23558366298675537
Batch 16/64 loss: -0.22823303937911987
Batch 17/64 loss: -0.24823153018951416
Batch 18/64 loss: -0.2312430739402771
Batch 19/64 loss: -0.23872792720794678
Batch 20/64 loss: -0.2507915496826172
Batch 21/64 loss: -0.2493973970413208
Batch 22/64 loss: -0.23035317659378052
Batch 23/64 loss: -0.2359243631362915
Batch 24/64 loss: -0.2524542808532715
Batch 25/64 loss: -0.2525327205657959
Batch 26/64 loss: -0.2617560029029846
Batch 27/64 loss: -0.25218623876571655
Batch 28/64 loss: -0.24893146753311157
Batch 29/64 loss: -0.24681246280670166
Batch 30/64 loss: -0.2524205446243286
Batch 31/64 loss: -0.24635767936706543
Batch 32/64 loss: -0.24186396598815918
Batch 33/64 loss: -0.24220681190490723
Batch 34/64 loss: -0.23099517822265625
Batch 35/64 loss: -0.25712335109710693
Batch 36/64 loss: -0.25998932123184204
Batch 37/64 loss: -0.2575647830963135
Batch 38/64 loss: -0.21582460403442383
Batch 39/64 loss: -0.2420472502708435
Batch 40/64 loss: -0.24356257915496826
Batch 41/64 loss: -0.24501758813858032
Batch 42/64 loss: -0.23703783750534058
Batch 43/64 loss: -0.22848349809646606
Batch 44/64 loss: -0.24990051984786987
Batch 45/64 loss: -0.25143009424209595
Batch 46/64 loss: -0.2447655200958252
Batch 47/64 loss: -0.24633550643920898
Batch 48/64 loss: -0.24425679445266724
Batch 49/64 loss: -0.2427988052368164
Batch 50/64 loss: -0.2549833059310913
Batch 51/64 loss: -0.264662504196167
Batch 52/64 loss: -0.23599034547805786
Batch 53/64 loss: -0.22781330347061157
Batch 54/64 loss: -0.26164358854293823
Batch 55/64 loss: -0.24026548862457275
Batch 56/64 loss: -0.2287764549255371
Batch 57/64 loss: -0.22198641300201416
Batch 58/64 loss: -0.25852036476135254
Batch 59/64 loss: -0.2531893849372864
Batch 60/64 loss: -0.228057861328125
Batch 61/64 loss: -0.2333473563194275
Batch 62/64 loss: -0.24222618341445923
Batch 63/64 loss: -0.22449463605880737
Batch 64/64 loss: -0.21763557195663452
Epoch 131  Train loss: -0.24277153599495982  Val loss: -0.21298574254275188
Epoch 132
-------------------------------
Batch 1/64 loss: -0.25757765769958496
Batch 2/64 loss: -0.24380117654800415
Batch 3/64 loss: -0.25929147005081177
Batch 4/64 loss: -0.25151002407073975
Batch 5/64 loss: -0.2091488242149353
Batch 6/64 loss: -0.24854493141174316
Batch 7/64 loss: -0.24783766269683838
Batch 8/64 loss: -0.22810685634613037
Batch 9/64 loss: -0.2681746482849121
Batch 10/64 loss: -0.250474214553833
Batch 11/64 loss: -0.24642223119735718
Batch 12/64 loss: -0.25470614433288574
Batch 13/64 loss: -0.24864625930786133
Batch 14/64 loss: -0.2517061233520508
Batch 15/64 loss: -0.24855166673660278
Batch 16/64 loss: -0.25597965717315674
Batch 17/64 loss: -0.2487022876739502
Batch 18/64 loss: -0.22289139032363892
Batch 19/64 loss: -0.25507622957229614
Batch 20/64 loss: -0.2249031662940979
Batch 21/64 loss: -0.2403944730758667
Batch 22/64 loss: -0.25442051887512207
Batch 23/64 loss: -0.23394787311553955
Batch 24/64 loss: -0.2727136015892029
Batch 25/64 loss: -0.26101773977279663
Batch 26/64 loss: -0.2354569435119629
Batch 27/64 loss: -0.2393742799758911
Batch 28/64 loss: -0.2443828582763672
Batch 29/64 loss: -0.24766510725021362
Batch 30/64 loss: -0.23989826440811157
Batch 31/64 loss: -0.24430549144744873
Batch 32/64 loss: -0.25565874576568604
Batch 33/64 loss: -0.23736929893493652
Batch 34/64 loss: -0.24951225519180298
Batch 35/64 loss: -0.24597805738449097
Batch 36/64 loss: -0.25300705432891846
Batch 37/64 loss: -0.21946626901626587
Batch 38/64 loss: -0.2456282377243042
Batch 39/64 loss: -0.22584199905395508
Batch 40/64 loss: -0.2379368543624878
Batch 41/64 loss: -0.23860299587249756
Batch 42/64 loss: -0.24526959657669067
Batch 43/64 loss: -0.24466991424560547
Batch 44/64 loss: -0.23703289031982422
Batch 45/64 loss: -0.23858624696731567
Batch 46/64 loss: -0.2250627875328064
Batch 47/64 loss: -0.2565540075302124
Batch 48/64 loss: -0.24296438694000244
Batch 49/64 loss: -0.23318088054656982
Batch 50/64 loss: -0.2472395896911621
Batch 51/64 loss: -0.24136751890182495
Batch 52/64 loss: -0.24760615825653076
Batch 53/64 loss: -0.21600520610809326
Batch 54/64 loss: -0.23961365222930908
Batch 55/64 loss: -0.25106775760650635
Batch 56/64 loss: -0.2419365644454956
Batch 57/64 loss: -0.24858754873275757
Batch 58/64 loss: -0.24453836679458618
Batch 59/64 loss: -0.24803733825683594
Batch 60/64 loss: -0.230124831199646
Batch 61/64 loss: -0.24914300441741943
Batch 62/64 loss: -0.25525182485580444
Batch 63/64 loss: -0.2293539047241211
Batch 64/64 loss: -0.2286909818649292
Epoch 132  Train loss: -0.24359754908318612  Val loss: -0.21915238583620472
Epoch 133
-------------------------------
Batch 1/64 loss: -0.23340588808059692
Batch 2/64 loss: -0.24234873056411743
Batch 3/64 loss: -0.24074792861938477
Batch 4/64 loss: -0.24906158447265625
Batch 5/64 loss: -0.24228274822235107
Batch 6/64 loss: -0.25795602798461914
Batch 7/64 loss: -0.2582542896270752
Batch 8/64 loss: -0.24108541011810303
Batch 9/64 loss: -0.24076181650161743
Batch 10/64 loss: -0.2507336139678955
Batch 11/64 loss: -0.237862229347229
Batch 12/64 loss: -0.24834996461868286
Batch 13/64 loss: -0.24934160709381104
Batch 14/64 loss: -0.2253042459487915
Batch 15/64 loss: -0.253271222114563
Batch 16/64 loss: -0.2417287826538086
Batch 17/64 loss: -0.23952555656433105
Batch 18/64 loss: -0.2531213164329529
Batch 19/64 loss: -0.2442278265953064
Batch 20/64 loss: -0.23911988735198975
Batch 21/64 loss: -0.2618106007575989
Batch 22/64 loss: -0.23284757137298584
Batch 23/64 loss: -0.2098139524459839
Batch 24/64 loss: -0.25693070888519287
Batch 25/64 loss: -0.2555239200592041
Batch 26/64 loss: -0.22998899221420288
Batch 27/64 loss: -0.2494436502456665
Batch 28/64 loss: -0.22823452949523926
Batch 29/64 loss: -0.22439414262771606
Batch 30/64 loss: -0.23466312885284424
Batch 31/64 loss: -0.2393677830696106
Batch 32/64 loss: -0.2421160340309143
Batch 33/64 loss: -0.23193562030792236
Batch 34/64 loss: -0.22140836715698242
Batch 35/64 loss: -0.2480757236480713
Batch 36/64 loss: -0.24941325187683105
Batch 37/64 loss: -0.2426247000694275
Batch 38/64 loss: -0.24301910400390625
Batch 39/64 loss: -0.24841934442520142
Batch 40/64 loss: -0.2469959855079651
Batch 41/64 loss: -0.2525063157081604
Batch 42/64 loss: -0.25284355878829956
Batch 43/64 loss: -0.24707818031311035
Batch 44/64 loss: -0.22205591201782227
Batch 45/64 loss: -0.2300506830215454
Batch 46/64 loss: -0.24733197689056396
Batch 47/64 loss: -0.23820555210113525
Batch 48/64 loss: -0.23144876956939697
Batch 49/64 loss: -0.2611422538757324
Batch 50/64 loss: -0.23325306177139282
Batch 51/64 loss: -0.24505555629730225
Batch 52/64 loss: -0.22636085748672485
Batch 53/64 loss: -0.2540571689605713
Batch 54/64 loss: -0.24817371368408203
Batch 55/64 loss: -0.2485189437866211
Batch 56/64 loss: -0.22981256246566772
Batch 57/64 loss: -0.24633020162582397
Batch 58/64 loss: -0.23558378219604492
Batch 59/64 loss: -0.24491196870803833
Batch 60/64 loss: -0.24895811080932617
Batch 61/64 loss: -0.25975626707077026
Batch 62/64 loss: -0.2559604048728943
Batch 63/64 loss: -0.2667781114578247
Batch 64/64 loss: -0.23847228288650513
Epoch 133  Train loss: -0.24298895550709146  Val loss: -0.22045048938174427
Saving best model, epoch: 133
Epoch 134
-------------------------------
Batch 1/64 loss: -0.2464374303817749
Batch 2/64 loss: -0.25455451011657715
Batch 3/64 loss: -0.25699132680892944
Batch 4/64 loss: -0.23544561862945557
Batch 5/64 loss: -0.23398756980895996
Batch 6/64 loss: -0.25366729497909546
Batch 7/64 loss: -0.24765372276306152
Batch 8/64 loss: -0.25500112771987915
Batch 9/64 loss: -0.24658715724945068
Batch 10/64 loss: -0.2538794279098511
Batch 11/64 loss: -0.26010894775390625
Batch 12/64 loss: -0.23019319772720337
Batch 13/64 loss: -0.25727808475494385
Batch 14/64 loss: -0.23035532236099243
Batch 15/64 loss: -0.24335527420043945
Batch 16/64 loss: -0.23142099380493164
Batch 17/64 loss: -0.2237679362297058
Batch 18/64 loss: -0.24832558631896973
Batch 19/64 loss: -0.24256771802902222
Batch 20/64 loss: -0.24408406019210815
Batch 21/64 loss: -0.2484636902809143
Batch 22/64 loss: -0.2541060447692871
Batch 23/64 loss: -0.24379265308380127
Batch 24/64 loss: -0.22294974327087402
Batch 25/64 loss: -0.23791557550430298
Batch 26/64 loss: -0.26096755266189575
Batch 27/64 loss: -0.26406270265579224
Batch 28/64 loss: -0.23820900917053223
Batch 29/64 loss: -0.2322523593902588
Batch 30/64 loss: -0.24720042943954468
Batch 31/64 loss: -0.248954176902771
Batch 32/64 loss: -0.2501729726791382
Batch 33/64 loss: -0.2342991828918457
Batch 34/64 loss: -0.23725980520248413
Batch 35/64 loss: -0.2471718192100525
Batch 36/64 loss: -0.24078643321990967
Batch 37/64 loss: -0.24632883071899414
Batch 38/64 loss: -0.24160480499267578
Batch 39/64 loss: -0.25514721870422363
Batch 40/64 loss: -0.236270010471344
Batch 41/64 loss: -0.22464358806610107
Batch 42/64 loss: -0.22667992115020752
Batch 43/64 loss: -0.25866150856018066
Batch 44/64 loss: -0.24374264478683472
Batch 45/64 loss: -0.25311708450317383
Batch 46/64 loss: -0.2564331293106079
Batch 47/64 loss: -0.24139130115509033
Batch 48/64 loss: -0.23875761032104492
Batch 49/64 loss: -0.2455701231956482
Batch 50/64 loss: -0.2615029811859131
Batch 51/64 loss: -0.22588121891021729
Batch 52/64 loss: -0.24483239650726318
Batch 53/64 loss: -0.2511637806892395
Batch 54/64 loss: -0.2590118646621704
Batch 55/64 loss: -0.2267930507659912
Batch 56/64 loss: -0.21621090173721313
Batch 57/64 loss: -0.22313791513442993
Batch 58/64 loss: -0.2336127758026123
Batch 59/64 loss: -0.2546648383140564
Batch 60/64 loss: -0.23707282543182373
Batch 61/64 loss: -0.262409508228302
Batch 62/64 loss: -0.2317272424697876
Batch 63/64 loss: -0.2507334351539612
Batch 64/64 loss: -0.23873698711395264
Epoch 134  Train loss: -0.24361383073470172  Val loss: -0.22043880072655955
Epoch 135
-------------------------------
Batch 1/64 loss: -0.24209648370742798
Batch 2/64 loss: -0.256666362285614
Batch 3/64 loss: -0.2552441954612732
Batch 4/64 loss: -0.23262429237365723
Batch 5/64 loss: -0.25576281547546387
Batch 6/64 loss: -0.25179624557495117
Batch 7/64 loss: -0.2503095269203186
Batch 8/64 loss: -0.24798864126205444
Batch 9/64 loss: -0.24961495399475098
Batch 10/64 loss: -0.24146616458892822
Batch 11/64 loss: -0.2435615062713623
Batch 12/64 loss: -0.25626254081726074
Batch 13/64 loss: -0.2680200934410095
Batch 14/64 loss: -0.24430495500564575
Batch 15/64 loss: -0.2549387812614441
Batch 16/64 loss: -0.23524868488311768
Batch 17/64 loss: -0.23984968662261963
Batch 18/64 loss: -0.2673107981681824
Batch 19/64 loss: -0.24591481685638428
Batch 20/64 loss: -0.25138795375823975
Batch 21/64 loss: -0.24108409881591797
Batch 22/64 loss: -0.2391890287399292
Batch 23/64 loss: -0.26910412311553955
Batch 24/64 loss: -0.25173449516296387
Batch 25/64 loss: -0.25939100980758667
Batch 26/64 loss: -0.24873626232147217
Batch 27/64 loss: -0.24678558111190796
Batch 28/64 loss: -0.24791783094406128
Batch 29/64 loss: -0.23831361532211304
Batch 30/64 loss: -0.22163265943527222
Batch 31/64 loss: -0.2506333589553833
Batch 32/64 loss: -0.2510228157043457
Batch 33/64 loss: -0.25824403762817383
Batch 34/64 loss: -0.24312305450439453
Batch 35/64 loss: -0.254033625125885
Batch 36/64 loss: -0.23989039659500122
Batch 37/64 loss: -0.24911588430404663
Batch 38/64 loss: -0.24509310722351074
Batch 39/64 loss: -0.26929646730422974
Batch 40/64 loss: -0.24601048231124878
Batch 41/64 loss: -0.2473224401473999
Batch 42/64 loss: -0.2350931167602539
Batch 43/64 loss: -0.2366037368774414
Batch 44/64 loss: -0.2657133936882019
Batch 45/64 loss: -0.2264748215675354
Batch 46/64 loss: -0.2530393600463867
Batch 47/64 loss: -0.25884127616882324
Batch 48/64 loss: -0.24247628450393677
Batch 49/64 loss: -0.2544741630554199
Batch 50/64 loss: -0.2313578724861145
Batch 51/64 loss: -0.24617540836334229
Batch 52/64 loss: -0.251284122467041
Batch 53/64 loss: -0.2500910758972168
Batch 54/64 loss: -0.2454524040222168
Batch 55/64 loss: -0.22022318840026855
Batch 56/64 loss: -0.24646848440170288
Batch 57/64 loss: -0.2431318759918213
Batch 58/64 loss: -0.25526899099349976
Batch 59/64 loss: -0.2518172264099121
Batch 60/64 loss: -0.2492307424545288
Batch 61/64 loss: -0.22222894430160522
Batch 62/64 loss: -0.24218428134918213
Batch 63/64 loss: -0.2581726908683777
Batch 64/64 loss: -0.24315869808197021
Epoch 135  Train loss: -0.2474700605168062  Val loss: -0.2293375796059153
Saving best model, epoch: 135
Epoch 136
-------------------------------
Batch 1/64 loss: -0.24655073881149292
Batch 2/64 loss: -0.24270117282867432
Batch 3/64 loss: -0.25470346212387085
Batch 4/64 loss: -0.263710081577301
Batch 5/64 loss: -0.2520536184310913
Batch 6/64 loss: -0.24520844221115112
Batch 7/64 loss: -0.2399461269378662
Batch 8/64 loss: -0.2496541142463684
Batch 9/64 loss: -0.23834460973739624
Batch 10/64 loss: -0.2653595209121704
Batch 11/64 loss: -0.24540340900421143
Batch 12/64 loss: -0.24720323085784912
Batch 13/64 loss: -0.259968638420105
Batch 14/64 loss: -0.23621857166290283
Batch 15/64 loss: -0.23469394445419312
Batch 16/64 loss: -0.24238801002502441
Batch 17/64 loss: -0.2607004642486572
Batch 18/64 loss: -0.2396641969680786
Batch 19/64 loss: -0.24518394470214844
Batch 20/64 loss: -0.23802608251571655
Batch 21/64 loss: -0.23736780881881714
Batch 22/64 loss: -0.22635102272033691
Batch 23/64 loss: -0.24499905109405518
Batch 24/64 loss: -0.23934447765350342
Batch 25/64 loss: -0.2508890628814697
Batch 26/64 loss: -0.21113580465316772
Batch 27/64 loss: -0.2460317611694336
Batch 28/64 loss: -0.2659778594970703
Batch 29/64 loss: -0.27123767137527466
Batch 30/64 loss: -0.25308001041412354
Batch 31/64 loss: -0.24752241373062134
Batch 32/64 loss: -0.2413882613182068
Batch 33/64 loss: -0.25168365240097046
Batch 34/64 loss: -0.22098934650421143
Batch 35/64 loss: -0.2615417242050171
Batch 36/64 loss: -0.2656242847442627
Batch 37/64 loss: -0.24632340669631958
Batch 38/64 loss: -0.2503776550292969
Batch 39/64 loss: -0.24513006210327148
Batch 40/64 loss: -0.27106159925460815
Batch 41/64 loss: -0.23732435703277588
Batch 42/64 loss: -0.2596200108528137
Batch 43/64 loss: -0.23053181171417236
Batch 44/64 loss: -0.2573080062866211
Batch 45/64 loss: -0.24347108602523804
Batch 46/64 loss: -0.24477225542068481
Batch 47/64 loss: -0.26160728931427
Batch 48/64 loss: -0.24470752477645874
Batch 49/64 loss: -0.25697559118270874
Batch 50/64 loss: -0.2647736668586731
Batch 51/64 loss: -0.2382286787033081
Batch 52/64 loss: -0.25953078269958496
Batch 53/64 loss: -0.24829423427581787
Batch 54/64 loss: -0.24687087535858154
Batch 55/64 loss: -0.23314249515533447
Batch 56/64 loss: -0.25814563035964966
Batch 57/64 loss: -0.25611042976379395
Batch 58/64 loss: -0.218214213848114
Batch 59/64 loss: -0.243613600730896
Batch 60/64 loss: -0.2483653426170349
Batch 61/64 loss: -0.24244928359985352
Batch 62/64 loss: -0.2515331506729126
Batch 63/64 loss: -0.23262882232666016
Batch 64/64 loss: -0.23857104778289795
Epoch 136  Train loss: -0.24710410576240688  Val loss: -0.21318784509737468
Epoch 137
-------------------------------
Batch 1/64 loss: -0.2558016777038574
Batch 2/64 loss: -0.2562747001647949
Batch 3/64 loss: -0.2563731074333191
Batch 4/64 loss: -0.25831377506256104
Batch 5/64 loss: -0.24280285835266113
Batch 6/64 loss: -0.25782477855682373
Batch 7/64 loss: -0.25821274518966675
Batch 8/64 loss: -0.24173492193222046
Batch 9/64 loss: -0.2522423267364502
Batch 10/64 loss: -0.25444912910461426
Batch 11/64 loss: -0.2578885555267334
Batch 12/64 loss: -0.2580823302268982
Batch 13/64 loss: -0.2626364827156067
Batch 14/64 loss: -0.24349623918533325
Batch 15/64 loss: -0.2385360598564148
Batch 16/64 loss: -0.25670695304870605
Batch 17/64 loss: -0.25323909521102905
Batch 18/64 loss: -0.2559788227081299
Batch 19/64 loss: -0.25032705068588257
Batch 20/64 loss: -0.24932342767715454
Batch 21/64 loss: -0.24946534633636475
Batch 22/64 loss: -0.2545323371887207
Batch 23/64 loss: -0.24372780323028564
Batch 24/64 loss: -0.22514450550079346
Batch 25/64 loss: -0.26265883445739746
Batch 26/64 loss: -0.2341923713684082
Batch 27/64 loss: -0.22410619258880615
Batch 28/64 loss: -0.24828392267227173
Batch 29/64 loss: -0.24411308765411377
Batch 30/64 loss: -0.2520887851715088
Batch 31/64 loss: -0.258872389793396
Batch 32/64 loss: -0.24149203300476074
Batch 33/64 loss: -0.23028647899627686
Batch 34/64 loss: -0.24280273914337158
Batch 35/64 loss: -0.25463950634002686
Batch 36/64 loss: -0.24962055683135986
Batch 37/64 loss: -0.25769591331481934
Batch 38/64 loss: -0.2532086968421936
Batch 39/64 loss: -0.23104214668273926
Batch 40/64 loss: -0.25274866819381714
Batch 41/64 loss: -0.2727932333946228
Batch 42/64 loss: -0.25249576568603516
Batch 43/64 loss: -0.2466362714767456
Batch 44/64 loss: -0.2587318420410156
Batch 45/64 loss: -0.2570957541465759
Batch 46/64 loss: -0.26978862285614014
Batch 47/64 loss: -0.22441846132278442
Batch 48/64 loss: -0.24773061275482178
Batch 49/64 loss: -0.25356394052505493
Batch 50/64 loss: -0.24592351913452148
Batch 51/64 loss: -0.2796286940574646
Batch 52/64 loss: -0.23802483081817627
Batch 53/64 loss: -0.24467426538467407
Batch 54/64 loss: -0.262986421585083
Batch 55/64 loss: -0.2357659935951233
Batch 56/64 loss: -0.24994981288909912
Batch 57/64 loss: -0.2581130266189575
Batch 58/64 loss: -0.24363481998443604
Batch 59/64 loss: -0.2536433935165405
Batch 60/64 loss: -0.2604159712791443
Batch 61/64 loss: -0.23758387565612793
Batch 62/64 loss: -0.2526592016220093
Batch 63/64 loss: -0.25287723541259766
Batch 64/64 loss: -0.26138824224472046
Epoch 137  Train loss: -0.2504493192130444  Val loss: -0.22488236939374523
Epoch 138
-------------------------------
Batch 1/64 loss: -0.2549171447753906
Batch 2/64 loss: -0.24489784240722656
Batch 3/64 loss: -0.25331783294677734
Batch 4/64 loss: -0.2463970184326172
Batch 5/64 loss: -0.24997341632843018
Batch 6/64 loss: -0.24925166368484497
Batch 7/64 loss: -0.2597780227661133
Batch 8/64 loss: -0.25560951232910156
Batch 9/64 loss: -0.2624053955078125
Batch 10/64 loss: -0.23671633005142212
Batch 11/64 loss: -0.24746960401535034
Batch 12/64 loss: -0.23538029193878174
Batch 13/64 loss: -0.2569620609283447
Batch 14/64 loss: -0.2252386212348938
Batch 15/64 loss: -0.24864083528518677
Batch 16/64 loss: -0.23746895790100098
Batch 17/64 loss: -0.2505407929420471
Batch 18/64 loss: -0.25463104248046875
Batch 19/64 loss: -0.2588730454444885
Batch 20/64 loss: -0.24777400493621826
Batch 21/64 loss: -0.23839932680130005
Batch 22/64 loss: -0.24101144075393677
Batch 23/64 loss: -0.23440539836883545
Batch 24/64 loss: -0.25555580854415894
Batch 25/64 loss: -0.25900954008102417
Batch 26/64 loss: -0.23656904697418213
Batch 27/64 loss: -0.25798165798187256
Batch 28/64 loss: -0.2358437180519104
Batch 29/64 loss: -0.25696784257888794
Batch 30/64 loss: -0.2416548728942871
Batch 31/64 loss: -0.2588047385215759
Batch 32/64 loss: -0.2540166974067688
Batch 33/64 loss: -0.22797179222106934
Batch 34/64 loss: -0.24771875143051147
Batch 35/64 loss: -0.24584698677062988
Batch 36/64 loss: -0.2352287769317627
Batch 37/64 loss: -0.24939191341400146
Batch 38/64 loss: -0.22995972633361816
Batch 39/64 loss: -0.26310813426971436
Batch 40/64 loss: -0.2414255142211914
Batch 41/64 loss: -0.2347879409790039
Batch 42/64 loss: -0.25110137462615967
Batch 43/64 loss: -0.2522803544998169
Batch 44/64 loss: -0.22575509548187256
Batch 45/64 loss: -0.24096429347991943
Batch 46/64 loss: -0.24783796072006226
Batch 47/64 loss: -0.24609804153442383
Batch 48/64 loss: -0.2513244152069092
Batch 49/64 loss: -0.24198222160339355
Batch 50/64 loss: -0.23768603801727295
Batch 51/64 loss: -0.2463335394859314
Batch 52/64 loss: -0.25375860929489136
Batch 53/64 loss: -0.2523660659790039
Batch 54/64 loss: -0.24572134017944336
Batch 55/64 loss: -0.2368772029876709
Batch 56/64 loss: -0.24279391765594482
Batch 57/64 loss: -0.26425445079803467
Batch 58/64 loss: -0.25366270542144775
Batch 59/64 loss: -0.2692553400993347
Batch 60/64 loss: -0.2390272617340088
Batch 61/64 loss: -0.2355089783668518
Batch 62/64 loss: -0.2236407995223999
Batch 63/64 loss: -0.23572266101837158
Batch 64/64 loss: -0.23374176025390625
Epoch 138  Train loss: -0.24613587529051537  Val loss: -0.2095956734775268
Epoch 139
-------------------------------
Batch 1/64 loss: -0.23151487112045288
Batch 2/64 loss: -0.23380911350250244
Batch 3/64 loss: -0.23153221607208252
Batch 4/64 loss: -0.2422378659248352
Batch 5/64 loss: -0.24494928121566772
Batch 6/64 loss: -0.2375311255455017
Batch 7/64 loss: -0.22020649909973145
Batch 8/64 loss: -0.2521095871925354
Batch 9/64 loss: -0.2460041046142578
Batch 10/64 loss: -0.22726893424987793
Batch 11/64 loss: -0.2446972131729126
Batch 12/64 loss: -0.24809139966964722
Batch 13/64 loss: -0.23980748653411865
Batch 14/64 loss: -0.25616371631622314
Batch 15/64 loss: -0.25748932361602783
Batch 16/64 loss: -0.24908554553985596
Batch 17/64 loss: -0.25785577297210693
Batch 18/64 loss: -0.2578429579734802
Batch 19/64 loss: -0.2610158920288086
Batch 20/64 loss: -0.24804246425628662
Batch 21/64 loss: -0.24679237604141235
Batch 22/64 loss: -0.2548673152923584
Batch 23/64 loss: -0.2523241639137268
Batch 24/64 loss: -0.24018633365631104
Batch 25/64 loss: -0.2528138756752014
Batch 26/64 loss: -0.25712984800338745
Batch 27/64 loss: -0.23148632049560547
Batch 28/64 loss: -0.25710415840148926
Batch 29/64 loss: -0.24166429042816162
Batch 30/64 loss: -0.24736160039901733
Batch 31/64 loss: -0.2524735927581787
Batch 32/64 loss: -0.26005202531814575
Batch 33/64 loss: -0.2383788824081421
Batch 34/64 loss: -0.24407708644866943
Batch 35/64 loss: -0.23620235919952393
Batch 36/64 loss: -0.25732147693634033
Batch 37/64 loss: -0.25336432456970215
Batch 38/64 loss: -0.23433417081832886
Batch 39/64 loss: -0.2444329857826233
Batch 40/64 loss: -0.22082412242889404
Batch 41/64 loss: -0.2203192114830017
Batch 42/64 loss: -0.21651822328567505
Batch 43/64 loss: -0.23138374090194702
Batch 44/64 loss: -0.2509421706199646
Batch 45/64 loss: -0.22261542081832886
Batch 46/64 loss: -0.2495955228805542
Batch 47/64 loss: -0.27278846502304077
Batch 48/64 loss: -0.24782896041870117
Batch 49/64 loss: -0.2402108907699585
Batch 50/64 loss: -0.22751891613006592
Batch 51/64 loss: -0.2587922215461731
Batch 52/64 loss: -0.25011616945266724
Batch 53/64 loss: -0.2406741976737976
Batch 54/64 loss: -0.24120467901229858
Batch 55/64 loss: -0.23766964673995972
Batch 56/64 loss: -0.23069024085998535
Batch 57/64 loss: -0.25303059816360474
Batch 58/64 loss: -0.257351279258728
Batch 59/64 loss: -0.2545155882835388
Batch 60/64 loss: -0.24389702081680298
Batch 61/64 loss: -0.24772334098815918
Batch 62/64 loss: -0.25481319427490234
Batch 63/64 loss: -0.24812930822372437
Batch 64/64 loss: -0.2405637502670288
Epoch 139  Train loss: -0.244536447057537  Val loss: -0.22553329123664148
Epoch 140
-------------------------------
Batch 1/64 loss: -0.2529033422470093
Batch 2/64 loss: -0.2506856322288513
Batch 3/64 loss: -0.24009692668914795
Batch 4/64 loss: -0.24252498149871826
Batch 5/64 loss: -0.2492457628250122
Batch 6/64 loss: -0.23959535360336304
Batch 7/64 loss: -0.24965006113052368
Batch 8/64 loss: -0.2516050338745117
Batch 9/64 loss: -0.24019306898117065
Batch 10/64 loss: -0.22133123874664307
Batch 11/64 loss: -0.24050581455230713
Batch 12/64 loss: -0.24076461791992188
Batch 13/64 loss: -0.2564326524734497
Batch 14/64 loss: -0.25895798206329346
Batch 15/64 loss: -0.2571289539337158
Batch 16/64 loss: -0.25825703144073486
Batch 17/64 loss: -0.24099481105804443
Batch 18/64 loss: -0.22192370891571045
Batch 19/64 loss: -0.23595976829528809
Batch 20/64 loss: -0.23719340562820435
Batch 21/64 loss: -0.2534058094024658
Batch 22/64 loss: -0.2615674138069153
Batch 23/64 loss: -0.24541038274765015
Batch 24/64 loss: -0.26150256395339966
Batch 25/64 loss: -0.26196545362472534
Batch 26/64 loss: -0.2502814531326294
Batch 27/64 loss: -0.22339481115341187
Batch 28/64 loss: -0.24153262376785278
Batch 29/64 loss: -0.256719172000885
Batch 30/64 loss: -0.239882230758667
Batch 31/64 loss: -0.2415286898612976
Batch 32/64 loss: -0.2352219820022583
Batch 33/64 loss: -0.26058781147003174
Batch 34/64 loss: -0.23108702898025513
Batch 35/64 loss: -0.2428419589996338
Batch 36/64 loss: -0.2559202313423157
Batch 37/64 loss: -0.25703805685043335
Batch 38/64 loss: -0.22330880165100098
Batch 39/64 loss: -0.2324587106704712
Batch 40/64 loss: -0.24817228317260742
Batch 41/64 loss: -0.215576171875
Batch 42/64 loss: -0.23695993423461914
Batch 43/64 loss: -0.22432571649551392
Batch 44/64 loss: -0.23883599042892456
Batch 45/64 loss: -0.2399991750717163
Batch 46/64 loss: -0.2436145544052124
Batch 47/64 loss: -0.23235493898391724
Batch 48/64 loss: -0.22896629571914673
Batch 49/64 loss: -0.24523383378982544
Batch 50/64 loss: -0.22352242469787598
Batch 51/64 loss: -0.2223215103149414
Batch 52/64 loss: -0.2454867959022522
Batch 53/64 loss: -0.24409526586532593
Batch 54/64 loss: -0.22429299354553223
Batch 55/64 loss: -0.2420671582221985
Batch 56/64 loss: -0.23565983772277832
Batch 57/64 loss: -0.24268019199371338
Batch 58/64 loss: -0.24464893341064453
Batch 59/64 loss: -0.24389171600341797
Batch 60/64 loss: -0.23743605613708496
Batch 61/64 loss: -0.25486433506011963
Batch 62/64 loss: -0.23845505714416504
Batch 63/64 loss: -0.24794375896453857
Batch 64/64 loss: -0.2522541880607605
Epoch 140  Train loss: -0.24238746516844806  Val loss: -0.2122061848640442
Epoch 141
-------------------------------
Batch 1/64 loss: -0.26053398847579956
Batch 2/64 loss: -0.236139714717865
Batch 3/64 loss: -0.2566680312156677
Batch 4/64 loss: -0.22934281826019287
Batch 5/64 loss: -0.23657959699630737
Batch 6/64 loss: -0.24271368980407715
Batch 7/64 loss: -0.255412220954895
Batch 8/64 loss: -0.23210418224334717
Batch 9/64 loss: -0.2545468807220459
Batch 10/64 loss: -0.23615741729736328
Batch 11/64 loss: -0.22725695371627808
Batch 12/64 loss: -0.24609804153442383
Batch 13/64 loss: -0.25530970096588135
Batch 14/64 loss: -0.2376079559326172
Batch 15/64 loss: -0.24311089515686035
Batch 16/64 loss: -0.25136786699295044
Batch 17/64 loss: -0.2567969560623169
Batch 18/64 loss: -0.24276280403137207
Batch 19/64 loss: -0.2564568519592285
Batch 20/64 loss: -0.25261175632476807
Batch 21/64 loss: -0.2617947459220886
Batch 22/64 loss: -0.25285887718200684
Batch 23/64 loss: -0.23417556285858154
Batch 24/64 loss: -0.23933804035186768
Batch 25/64 loss: -0.23137301206588745
Batch 26/64 loss: -0.24239778518676758
Batch 27/64 loss: -0.24710237979888916
Batch 28/64 loss: -0.2018660306930542
Batch 29/64 loss: -0.24795019626617432
Batch 30/64 loss: -0.24553674459457397
Batch 31/64 loss: -0.23952984809875488
Batch 32/64 loss: -0.24892425537109375
Batch 33/64 loss: -0.24576729536056519
Batch 34/64 loss: -0.24866288900375366
Batch 35/64 loss: -0.2549694776535034
Batch 36/64 loss: -0.2541320323944092
Batch 37/64 loss: -0.2615605592727661
Batch 38/64 loss: -0.24525392055511475
Batch 39/64 loss: -0.23960506916046143
Batch 40/64 loss: -0.2364119291305542
Batch 41/64 loss: -0.22223365306854248
Batch 42/64 loss: -0.24884003400802612
Batch 43/64 loss: -0.2501523494720459
Batch 44/64 loss: -0.22158896923065186
Batch 45/64 loss: -0.2587857246398926
Batch 46/64 loss: -0.22433704137802124
Batch 47/64 loss: -0.22816354036331177
Batch 48/64 loss: -0.23768234252929688
Batch 49/64 loss: -0.2732057571411133
Batch 50/64 loss: -0.25326597690582275
Batch 51/64 loss: -0.24906563758850098
Batch 52/64 loss: -0.2459191083908081
Batch 53/64 loss: -0.24704456329345703
Batch 54/64 loss: -0.25317269563674927
Batch 55/64 loss: -0.25567907094955444
Batch 56/64 loss: -0.23794662952423096
Batch 57/64 loss: -0.240953266620636
Batch 58/64 loss: -0.2405555248260498
Batch 59/64 loss: -0.23850977420806885
Batch 60/64 loss: -0.25897216796875
Batch 61/64 loss: -0.2460346817970276
Batch 62/64 loss: -0.25708281993865967
Batch 63/64 loss: -0.2518357038497925
Batch 64/64 loss: -0.205968976020813
Epoch 141  Train loss: -0.2444594939549764  Val loss: -0.21847373882110177
Epoch 142
-------------------------------
Batch 1/64 loss: -0.26724570989608765
Batch 2/64 loss: -0.24971705675125122
Batch 3/64 loss: -0.24369925260543823
Batch 4/64 loss: -0.249869704246521
Batch 5/64 loss: -0.2393488883972168
Batch 6/64 loss: -0.2443881630897522
Batch 7/64 loss: -0.2583237290382385
Batch 8/64 loss: -0.23842620849609375
Batch 9/64 loss: -0.24040937423706055
Batch 10/64 loss: -0.26803553104400635
Batch 11/64 loss: -0.2290564775466919
Batch 12/64 loss: -0.23398327827453613
Batch 13/64 loss: -0.24238067865371704
Batch 14/64 loss: -0.24529045820236206
Batch 15/64 loss: -0.26088637113571167
Batch 16/64 loss: -0.25271791219711304
Batch 17/64 loss: -0.23775041103363037
Batch 18/64 loss: -0.23889952898025513
Batch 19/64 loss: -0.23852336406707764
Batch 20/64 loss: -0.2451479434967041
Batch 21/64 loss: -0.2548253536224365
Batch 22/64 loss: -0.24761152267456055
Batch 23/64 loss: -0.23346734046936035
Batch 24/64 loss: -0.25593554973602295
Batch 25/64 loss: -0.2538983225822449
Batch 26/64 loss: -0.20474708080291748
Batch 27/64 loss: -0.2551093101501465
Batch 28/64 loss: -0.2508264183998108
Batch 29/64 loss: -0.2504606246948242
Batch 30/64 loss: -0.2465207576751709
Batch 31/64 loss: -0.2481323480606079
Batch 32/64 loss: -0.24716460704803467
Batch 33/64 loss: -0.21844059228897095
Batch 34/64 loss: -0.2500886917114258
Batch 35/64 loss: -0.22115886211395264
Batch 36/64 loss: -0.24928593635559082
Batch 37/64 loss: -0.2380967140197754
Batch 38/64 loss: -0.2552734613418579
Batch 39/64 loss: -0.25129884481430054
Batch 40/64 loss: -0.2553666830062866
Batch 41/64 loss: -0.2458091378211975
Batch 42/64 loss: -0.2415255904197693
Batch 43/64 loss: -0.25134581327438354
Batch 44/64 loss: -0.23730987310409546
Batch 45/64 loss: -0.2552356719970703
Batch 46/64 loss: -0.22544485330581665
Batch 47/64 loss: -0.24683868885040283
Batch 48/64 loss: -0.26004576683044434
Batch 49/64 loss: -0.25165432691574097
Batch 50/64 loss: -0.24652689695358276
Batch 51/64 loss: -0.24232959747314453
Batch 52/64 loss: -0.22995150089263916
Batch 53/64 loss: -0.2438042163848877
Batch 54/64 loss: -0.2521682381629944
Batch 55/64 loss: -0.24051916599273682
Batch 56/64 loss: -0.2612076997756958
Batch 57/64 loss: -0.2548877000808716
Batch 58/64 loss: -0.22196239233016968
Batch 59/64 loss: -0.2643383741378784
Batch 60/64 loss: -0.24339115619659424
Batch 61/64 loss: -0.25713348388671875
Batch 62/64 loss: -0.22699123620986938
Batch 63/64 loss: -0.25603652000427246
Batch 64/64 loss: -0.2592509984970093
Epoch 142  Train loss: -0.24568949353461172  Val loss: -0.21399308848626836
Epoch 143
-------------------------------
Batch 1/64 loss: -0.2521122097969055
Batch 2/64 loss: -0.25642889738082886
Batch 3/64 loss: -0.2464524507522583
Batch 4/64 loss: -0.2545604109764099
Batch 5/64 loss: -0.2645074129104614
Batch 6/64 loss: -0.24906575679779053
Batch 7/64 loss: -0.23133593797683716
Batch 8/64 loss: -0.23240894079208374
Batch 9/64 loss: -0.2552989721298218
Batch 10/64 loss: -0.2560679316520691
Batch 11/64 loss: -0.25966668128967285
Batch 12/64 loss: -0.24235135316848755
Batch 13/64 loss: -0.259213387966156
Batch 14/64 loss: -0.24970871210098267
Batch 15/64 loss: -0.24272829294204712
Batch 16/64 loss: -0.26208311319351196
Batch 17/64 loss: -0.24771863222122192
Batch 18/64 loss: -0.23753678798675537
Batch 19/64 loss: -0.25011777877807617
Batch 20/64 loss: -0.24850046634674072
Batch 21/64 loss: -0.251559853553772
Batch 22/64 loss: -0.255279541015625
Batch 23/64 loss: -0.26516348123550415
Batch 24/64 loss: -0.25356531143188477
Batch 25/64 loss: -0.26229482889175415
Batch 26/64 loss: -0.2536523938179016
Batch 27/64 loss: -0.221169114112854
Batch 28/64 loss: -0.25352632999420166
Batch 29/64 loss: -0.2593093514442444
Batch 30/64 loss: -0.2539886236190796
Batch 31/64 loss: -0.26141470670700073
Batch 32/64 loss: -0.25324326753616333
Batch 33/64 loss: -0.25090491771698
Batch 34/64 loss: -0.2452661395072937
Batch 35/64 loss: -0.25037050247192383
Batch 36/64 loss: -0.2615913152694702
Batch 37/64 loss: -0.24749767780303955
Batch 38/64 loss: -0.24991118907928467
Batch 39/64 loss: -0.25290095806121826
Batch 40/64 loss: -0.2527652382850647
Batch 41/64 loss: -0.24753308296203613
Batch 42/64 loss: -0.24563276767730713
Batch 43/64 loss: -0.2683084011077881
Batch 44/64 loss: -0.23983800411224365
Batch 45/64 loss: -0.2336827516555786
Batch 46/64 loss: -0.24320733547210693
Batch 47/64 loss: -0.2601109743118286
Batch 48/64 loss: -0.2480989694595337
Batch 49/64 loss: -0.23146772384643555
Batch 50/64 loss: -0.22722822427749634
Batch 51/64 loss: -0.2564878463745117
Batch 52/64 loss: -0.2233891487121582
Batch 53/64 loss: -0.2502506971359253
Batch 54/64 loss: -0.23718154430389404
Batch 55/64 loss: -0.25169485807418823
Batch 56/64 loss: -0.24810177087783813
Batch 57/64 loss: -0.2412276268005371
Batch 58/64 loss: -0.23541414737701416
Batch 59/64 loss: -0.24217909574508667
Batch 60/64 loss: -0.2364041805267334
Batch 61/64 loss: -0.24030333757400513
Batch 62/64 loss: -0.24737870693206787
Batch 63/64 loss: -0.24688935279846191
Batch 64/64 loss: -0.22724175453186035
Epoch 143  Train loss: -0.24824597227807138  Val loss: -0.21859824595992097
Epoch 144
-------------------------------
Batch 1/64 loss: -0.2529822587966919
Batch 2/64 loss: -0.24285757541656494
Batch 3/64 loss: -0.2472141981124878
Batch 4/64 loss: -0.26279330253601074
Batch 5/64 loss: -0.26202183961868286
Batch 6/64 loss: -0.24714219570159912
Batch 7/64 loss: -0.2447909116744995
Batch 8/64 loss: -0.22806710004806519
Batch 9/64 loss: -0.24129986763000488
Batch 10/64 loss: -0.23231607675552368
Batch 11/64 loss: -0.26617956161499023
Batch 12/64 loss: -0.22653794288635254
Batch 13/64 loss: -0.26527833938598633
Batch 14/64 loss: -0.2603171467781067
Batch 15/64 loss: -0.21857202053070068
Batch 16/64 loss: -0.26247328519821167
Batch 17/64 loss: -0.24369585514068604
Batch 18/64 loss: -0.256705641746521
Batch 19/64 loss: -0.2613673210144043
Batch 20/64 loss: -0.2265910506248474
Batch 21/64 loss: -0.2545948028564453
Batch 22/64 loss: -0.24781030416488647
Batch 23/64 loss: -0.24153238534927368
Batch 24/64 loss: -0.24738025665283203
Batch 25/64 loss: -0.25051552057266235
Batch 26/64 loss: -0.243699312210083
Batch 27/64 loss: -0.2599998712539673
Batch 28/64 loss: -0.25753307342529297
Batch 29/64 loss: -0.26015257835388184
Batch 30/64 loss: -0.25544798374176025
Batch 31/64 loss: -0.24633938074111938
Batch 32/64 loss: -0.2486029863357544
Batch 33/64 loss: -0.24917280673980713
Batch 34/64 loss: -0.25035953521728516
Batch 35/64 loss: -0.23371410369873047
Batch 36/64 loss: -0.25780439376831055
Batch 37/64 loss: -0.24107056856155396
Batch 38/64 loss: -0.23794114589691162
Batch 39/64 loss: -0.2563152313232422
Batch 40/64 loss: -0.24362832307815552
Batch 41/64 loss: -0.2520751357078552
Batch 42/64 loss: -0.23347651958465576
Batch 43/64 loss: -0.2575252056121826
Batch 44/64 loss: -0.23386287689208984
Batch 45/64 loss: -0.25842320919036865
Batch 46/64 loss: -0.2413712739944458
Batch 47/64 loss: -0.24709999561309814
Batch 48/64 loss: -0.21940332651138306
Batch 49/64 loss: -0.2688373327255249
Batch 50/64 loss: -0.2296290397644043
Batch 51/64 loss: -0.2530890107154846
Batch 52/64 loss: -0.24848413467407227
Batch 53/64 loss: -0.2510095238685608
Batch 54/64 loss: -0.2601581811904907
Batch 55/64 loss: -0.25449419021606445
Batch 56/64 loss: -0.2557463049888611
Batch 57/64 loss: -0.2363007664680481
Batch 58/64 loss: -0.2555232644081116
Batch 59/64 loss: -0.25419580936431885
Batch 60/64 loss: -0.2242792844772339
Batch 61/64 loss: -0.26422107219696045
Batch 62/64 loss: -0.2427017092704773
Batch 63/64 loss: -0.2356136441230774
Batch 64/64 loss: -0.25776708126068115
Epoch 144  Train loss: -0.2479006145514694  Val loss: -0.2274858203950207
Epoch 145
-------------------------------
Batch 1/64 loss: -0.24344635009765625
Batch 2/64 loss: -0.24285811185836792
Batch 3/64 loss: -0.26248687505722046
Batch 4/64 loss: -0.24051690101623535
Batch 5/64 loss: -0.26737648248672485
Batch 6/64 loss: -0.2608060836791992
Batch 7/64 loss: -0.22990822792053223
Batch 8/64 loss: -0.2642061710357666
Batch 9/64 loss: -0.24954861402511597
Batch 10/64 loss: -0.2541888952255249
Batch 11/64 loss: -0.23379695415496826
Batch 12/64 loss: -0.2269207239151001
Batch 13/64 loss: -0.2506636381149292
Batch 14/64 loss: -0.24898916482925415
Batch 15/64 loss: -0.24503451585769653
Batch 16/64 loss: -0.2389703392982483
Batch 17/64 loss: -0.24746865034103394
Batch 18/64 loss: -0.2700200080871582
Batch 19/64 loss: -0.2469862699508667
Batch 20/64 loss: -0.20880645513534546
Batch 21/64 loss: -0.23360633850097656
Batch 22/64 loss: -0.26411378383636475
Batch 23/64 loss: -0.24337387084960938
Batch 24/64 loss: -0.2497779130935669
Batch 25/64 loss: -0.24240708351135254
Batch 26/64 loss: -0.2578287124633789
Batch 27/64 loss: -0.2468726634979248
Batch 28/64 loss: -0.2575916647911072
Batch 29/64 loss: -0.22280406951904297
Batch 30/64 loss: -0.25391167402267456
Batch 31/64 loss: -0.25184154510498047
Batch 32/64 loss: -0.23995548486709595
Batch 33/64 loss: -0.2444722056388855
Batch 34/64 loss: -0.22427690029144287
Batch 35/64 loss: -0.26117217540740967
Batch 36/64 loss: -0.23626744747161865
Batch 37/64 loss: -0.235953688621521
Batch 38/64 loss: -0.23910611867904663
Batch 39/64 loss: -0.26075875759124756
Batch 40/64 loss: -0.2587094306945801
Batch 41/64 loss: -0.24527591466903687
Batch 42/64 loss: -0.242919921875
Batch 43/64 loss: -0.2520817518234253
Batch 44/64 loss: -0.24162304401397705
Batch 45/64 loss: -0.2578052282333374
Batch 46/64 loss: -0.24724292755126953
Batch 47/64 loss: -0.2000749111175537
Batch 48/64 loss: -0.24848300218582153
Batch 49/64 loss: -0.22517746686935425
Batch 50/64 loss: -0.2532052993774414
Batch 51/64 loss: -0.24877488613128662
Batch 52/64 loss: -0.24457156658172607
Batch 53/64 loss: -0.24628698825836182
Batch 54/64 loss: -0.22781902551651
Batch 55/64 loss: -0.24678850173950195
Batch 56/64 loss: -0.2467707395553589
Batch 57/64 loss: -0.24839520454406738
Batch 58/64 loss: -0.24887406826019287
Batch 59/64 loss: -0.24628496170043945
Batch 60/64 loss: -0.2434544563293457
Batch 61/64 loss: -0.24750065803527832
Batch 62/64 loss: -0.25783848762512207
Batch 63/64 loss: -0.25814080238342285
Batch 64/64 loss: -0.25895893573760986
Epoch 145  Train loss: -0.24592015649758134  Val loss: -0.21678159552341475
Epoch 146
-------------------------------
Batch 1/64 loss: -0.24224597215652466
Batch 2/64 loss: -0.2561435103416443
Batch 3/64 loss: -0.2459355592727661
Batch 4/64 loss: -0.26563334465026855
Batch 5/64 loss: -0.24860423803329468
Batch 6/64 loss: -0.2490406036376953
Batch 7/64 loss: -0.24818259477615356
Batch 8/64 loss: -0.21419274806976318
Batch 9/64 loss: -0.23095828294754028
Batch 10/64 loss: -0.2512773275375366
Batch 11/64 loss: -0.25156521797180176
Batch 12/64 loss: -0.2413182258605957
Batch 13/64 loss: -0.2199845314025879
Batch 14/64 loss: -0.24979400634765625
Batch 15/64 loss: -0.2703173756599426
Batch 16/64 loss: -0.2538638114929199
Batch 17/64 loss: -0.2413783073425293
Batch 18/64 loss: -0.24050551652908325
Batch 19/64 loss: -0.2247447371482849
Batch 20/64 loss: -0.23242682218551636
Batch 21/64 loss: -0.2333800196647644
Batch 22/64 loss: -0.24130332469940186
Batch 23/64 loss: -0.23285531997680664
Batch 24/64 loss: -0.23785126209259033
Batch 25/64 loss: -0.25131893157958984
Batch 26/64 loss: -0.24512887001037598
Batch 27/64 loss: -0.22391772270202637
Batch 28/64 loss: -0.24492669105529785
Batch 29/64 loss: -0.24217945337295532
Batch 30/64 loss: -0.2513095736503601
Batch 31/64 loss: -0.25071680545806885
Batch 32/64 loss: -0.2548079490661621
Batch 33/64 loss: -0.24403870105743408
Batch 34/64 loss: -0.23120373487472534
Batch 35/64 loss: -0.2524113655090332
Batch 36/64 loss: -0.23487484455108643
Batch 37/64 loss: -0.2477644681930542
Batch 38/64 loss: -0.23667925596237183
Batch 39/64 loss: -0.2393096685409546
Batch 40/64 loss: -0.2384483814239502
Batch 41/64 loss: -0.23759961128234863
Batch 42/64 loss: -0.24362671375274658
Batch 43/64 loss: -0.2394145131111145
Batch 44/64 loss: -0.25490802526474
Batch 45/64 loss: -0.2376871109008789
Batch 46/64 loss: -0.23504304885864258
Batch 47/64 loss: -0.26114726066589355
Batch 48/64 loss: -0.24766528606414795
Batch 49/64 loss: -0.24962031841278076
Batch 50/64 loss: -0.24910563230514526
Batch 51/64 loss: -0.2467198371887207
Batch 52/64 loss: -0.23051750659942627
Batch 53/64 loss: -0.23159760236740112
Batch 54/64 loss: -0.22604340314865112
Batch 55/64 loss: -0.2371354103088379
Batch 56/64 loss: -0.2544177174568176
Batch 57/64 loss: -0.22514665126800537
Batch 58/64 loss: -0.24062269926071167
Batch 59/64 loss: -0.2147965431213379
Batch 60/64 loss: -0.24016481637954712
Batch 61/64 loss: -0.24952900409698486
Batch 62/64 loss: -0.23866623640060425
Batch 63/64 loss: -0.2276337742805481
Batch 64/64 loss: -0.2531566023826599
Epoch 146  Train loss: -0.24190141570334342  Val loss: -0.2164853909580978
Epoch 147
-------------------------------
Batch 1/64 loss: -0.2346634864807129
Batch 2/64 loss: -0.24026167392730713
Batch 3/64 loss: -0.24854451417922974
Batch 4/64 loss: -0.2505221366882324
Batch 5/64 loss: -0.2526872158050537
Batch 6/64 loss: -0.2510594129562378
Batch 7/64 loss: -0.2603476643562317
Batch 8/64 loss: -0.2340589165687561
Batch 9/64 loss: -0.2631574273109436
Batch 10/64 loss: -0.2565060257911682
Batch 11/64 loss: -0.23669826984405518
Batch 12/64 loss: -0.24686497449874878
Batch 13/64 loss: -0.2534565329551697
Batch 14/64 loss: -0.22754859924316406
Batch 15/64 loss: -0.23014718294143677
Batch 16/64 loss: -0.24366480112075806
Batch 17/64 loss: -0.244421124458313
Batch 18/64 loss: -0.2405543327331543
Batch 19/64 loss: -0.24980288743972778
Batch 20/64 loss: -0.2542833685874939
Batch 21/64 loss: -0.26681554317474365
Batch 22/64 loss: -0.25957322120666504
Batch 23/64 loss: -0.2529524564743042
Batch 24/64 loss: -0.24138987064361572
Batch 25/64 loss: -0.23662322759628296
Batch 26/64 loss: -0.24052536487579346
Batch 27/64 loss: -0.25599443912506104
Batch 28/64 loss: -0.25123536586761475
Batch 29/64 loss: -0.23851937055587769
Batch 30/64 loss: -0.24503093957901
Batch 31/64 loss: -0.23871582746505737
Batch 32/64 loss: -0.2522449493408203
Batch 33/64 loss: -0.254986047744751
Batch 34/64 loss: -0.24000191688537598
Batch 35/64 loss: -0.21820586919784546
Batch 36/64 loss: -0.24426573514938354
Batch 37/64 loss: -0.253032922744751
Batch 38/64 loss: -0.26062166690826416
Batch 39/64 loss: -0.22601574659347534
Batch 40/64 loss: -0.2529575228691101
Batch 41/64 loss: -0.22691738605499268
Batch 42/64 loss: -0.24583536386489868
Batch 43/64 loss: -0.2501986026763916
Batch 44/64 loss: -0.26044368743896484
Batch 45/64 loss: -0.21431154012680054
Batch 46/64 loss: -0.24872153997421265
Batch 47/64 loss: -0.2462085485458374
Batch 48/64 loss: -0.25117766857147217
Batch 49/64 loss: -0.23854601383209229
Batch 50/64 loss: -0.233934223651886
Batch 51/64 loss: -0.22124457359313965
Batch 52/64 loss: -0.24861979484558105
Batch 53/64 loss: -0.2528895139694214
Batch 54/64 loss: -0.25680285692214966
Batch 55/64 loss: -0.24990171194076538
Batch 56/64 loss: -0.2501918077468872
Batch 57/64 loss: -0.24282103776931763
Batch 58/64 loss: -0.2545170783996582
Batch 59/64 loss: -0.2379114031791687
Batch 60/64 loss: -0.2539847493171692
Batch 61/64 loss: -0.24880975484848022
Batch 62/64 loss: -0.24074167013168335
Batch 63/64 loss: -0.23247027397155762
Batch 64/64 loss: -0.2758440375328064
Epoch 147  Train loss: -0.2456947824534248  Val loss: -0.23151860142901182
Saving best model, epoch: 147
Epoch 148
-------------------------------
Batch 1/64 loss: -0.24419903755187988
Batch 2/64 loss: -0.2575530409812927
Batch 3/64 loss: -0.2509550452232361
Batch 4/64 loss: -0.26910310983657837
Batch 5/64 loss: -0.24317866563796997
Batch 6/64 loss: -0.24642306566238403
Batch 7/64 loss: -0.25399160385131836
Batch 8/64 loss: -0.25115257501602173
Batch 9/64 loss: -0.2661818861961365
Batch 10/64 loss: -0.2571820616722107
Batch 11/64 loss: -0.259233295917511
Batch 12/64 loss: -0.2593667507171631
Batch 13/64 loss: -0.2608208656311035
Batch 14/64 loss: -0.2587631344795227
Batch 15/64 loss: -0.2519422173500061
Batch 16/64 loss: -0.25882548093795776
Batch 17/64 loss: -0.2702014446258545
Batch 18/64 loss: -0.2627297639846802
Batch 19/64 loss: -0.24488401412963867
Batch 20/64 loss: -0.2583281397819519
Batch 21/64 loss: -0.22037434577941895
Batch 22/64 loss: -0.2574193477630615
Batch 23/64 loss: -0.2598109245300293
Batch 24/64 loss: -0.2588738203048706
Batch 25/64 loss: -0.2634503245353699
Batch 26/64 loss: -0.26137423515319824
Batch 27/64 loss: -0.2554687261581421
Batch 28/64 loss: -0.2707664370536804
Batch 29/64 loss: -0.25135111808776855
Batch 30/64 loss: -0.251767098903656
Batch 31/64 loss: -0.2651355266571045
Batch 32/64 loss: -0.255979061126709
Batch 33/64 loss: -0.2524569630622864
Batch 34/64 loss: -0.2575315833091736
Batch 35/64 loss: -0.22079849243164062
Batch 36/64 loss: -0.2627577781677246
Batch 37/64 loss: -0.24751341342926025
Batch 38/64 loss: -0.23838984966278076
Batch 39/64 loss: -0.2520289421081543
Batch 40/64 loss: -0.26752227544784546
Batch 41/64 loss: -0.24728327989578247
Batch 42/64 loss: -0.2514575719833374
Batch 43/64 loss: -0.2657267451286316
Batch 44/64 loss: -0.2714345455169678
Batch 45/64 loss: -0.24197626113891602
Batch 46/64 loss: -0.25801801681518555
Batch 47/64 loss: -0.2571946978569031
Batch 48/64 loss: -0.2637009620666504
Batch 49/64 loss: -0.26733994483947754
Batch 50/64 loss: -0.2461288571357727
Batch 51/64 loss: -0.24322819709777832
Batch 52/64 loss: -0.257999062538147
Batch 53/64 loss: -0.24544227123260498
Batch 54/64 loss: -0.25197815895080566
Batch 55/64 loss: -0.2510281205177307
Batch 56/64 loss: -0.23411285877227783
Batch 57/64 loss: -0.2521020770072937
Batch 58/64 loss: -0.261752724647522
Batch 59/64 loss: -0.25157397985458374
Batch 60/64 loss: -0.2469525933265686
Batch 61/64 loss: -0.2584075927734375
Batch 62/64 loss: -0.24538564682006836
Batch 63/64 loss: -0.2486792802810669
Batch 64/64 loss: -0.22837769985198975
Epoch 148  Train loss: -0.25389760288537716  Val loss: -0.2272713309301134
Epoch 149
-------------------------------
Batch 1/64 loss: -0.25931888818740845
Batch 2/64 loss: -0.24612915515899658
Batch 3/64 loss: -0.23776209354400635
Batch 4/64 loss: -0.2652725577354431
Batch 5/64 loss: -0.2556770443916321
Batch 6/64 loss: -0.2641943097114563
Batch 7/64 loss: -0.26456761360168457
Batch 8/64 loss: -0.2700626850128174
Batch 9/64 loss: -0.2388668656349182
Batch 10/64 loss: -0.2185060977935791
Batch 11/64 loss: -0.27178776264190674
Batch 12/64 loss: -0.25974470376968384
Batch 13/64 loss: -0.27319085597991943
Batch 14/64 loss: -0.24974745512008667
Batch 15/64 loss: -0.25114625692367554
Batch 16/64 loss: -0.2530311942100525
Batch 17/64 loss: -0.24166476726531982
Batch 18/64 loss: -0.24496334791183472
Batch 19/64 loss: -0.2501453757286072
Batch 20/64 loss: -0.24419426918029785
Batch 21/64 loss: -0.261264443397522
Batch 22/64 loss: -0.2564387321472168
Batch 23/64 loss: -0.23364675045013428
Batch 24/64 loss: -0.2577976584434509
Batch 25/64 loss: -0.2476586103439331
Batch 26/64 loss: -0.26691603660583496
Batch 27/64 loss: -0.2283179759979248
Batch 28/64 loss: -0.24261105060577393
Batch 29/64 loss: -0.24410468339920044
Batch 30/64 loss: -0.25034844875335693
Batch 31/64 loss: -0.23705071210861206
Batch 32/64 loss: -0.25647151470184326
Batch 33/64 loss: -0.2514379620552063
Batch 34/64 loss: -0.2636984586715698
Batch 35/64 loss: -0.25325149297714233
Batch 36/64 loss: -0.24486607313156128
Batch 37/64 loss: -0.2437680959701538
Batch 38/64 loss: -0.2629656195640564
Batch 39/64 loss: -0.24841588735580444
Batch 40/64 loss: -0.23378276824951172
Batch 41/64 loss: -0.2516220211982727
Batch 42/64 loss: -0.2558985948562622
Batch 43/64 loss: -0.22721922397613525
Batch 44/64 loss: -0.2566739320755005
Batch 45/64 loss: -0.21494293212890625
Batch 46/64 loss: -0.24673521518707275
Batch 47/64 loss: -0.23083752393722534
Batch 48/64 loss: -0.23601508140563965
Batch 49/64 loss: -0.2540580630302429
Batch 50/64 loss: -0.23938018083572388
Batch 51/64 loss: -0.26060664653778076
Batch 52/64 loss: -0.24365836381912231
Batch 53/64 loss: -0.2326527237892151
Batch 54/64 loss: -0.24498343467712402
Batch 55/64 loss: -0.24841082096099854
Batch 56/64 loss: -0.23688828945159912
Batch 57/64 loss: -0.23429352045059204
Batch 58/64 loss: -0.25502365827560425
Batch 59/64 loss: -0.255237340927124
Batch 60/64 loss: -0.2637077569961548
Batch 61/64 loss: -0.25336748361587524
Batch 62/64 loss: -0.2480887770652771
Batch 63/64 loss: -0.24228250980377197
Batch 64/64 loss: -0.2652587890625
Epoch 149  Train loss: -0.24904022684284285  Val loss: -0.21761724137768304
Epoch 150
-------------------------------
Batch 1/64 loss: -0.252968430519104
Batch 2/64 loss: -0.2669994831085205
Batch 3/64 loss: -0.26777374744415283
Batch 4/64 loss: -0.2606053352355957
Batch 5/64 loss: -0.25944358110427856
Batch 6/64 loss: -0.2576882839202881
Batch 7/64 loss: -0.26354897022247314
Batch 8/64 loss: -0.24925553798675537
Batch 9/64 loss: -0.2667602300643921
Batch 10/64 loss: -0.2562899589538574
Batch 11/64 loss: -0.272558331489563
Batch 12/64 loss: -0.25625741481781006
Batch 13/64 loss: -0.25550544261932373
Batch 14/64 loss: -0.25626564025878906
Batch 15/64 loss: -0.26625317335128784
Batch 16/64 loss: -0.2473616600036621
Batch 17/64 loss: -0.2600463628768921
Batch 18/64 loss: -0.24352848529815674
Batch 19/64 loss: -0.2494664192199707
Batch 20/64 loss: -0.2642442584037781
Batch 21/64 loss: -0.2571728229522705
Batch 22/64 loss: -0.24484479427337646
Batch 23/64 loss: -0.2615179419517517
Batch 24/64 loss: -0.22631478309631348
Batch 25/64 loss: -0.24401211738586426
Batch 26/64 loss: -0.2681306004524231
Batch 27/64 loss: -0.25660115480422974
Batch 28/64 loss: -0.2719097137451172
Batch 29/64 loss: -0.24701672792434692
Batch 30/64 loss: -0.2681490182876587
Batch 31/64 loss: -0.2552102208137512
Batch 32/64 loss: -0.27083665132522583
Batch 33/64 loss: -0.2518782615661621
Batch 34/64 loss: -0.23823171854019165
Batch 35/64 loss: -0.2582750916481018
Batch 36/64 loss: -0.26129817962646484
Batch 37/64 loss: -0.2541106343269348
Batch 38/64 loss: -0.22621870040893555
Batch 39/64 loss: -0.24393099546432495
Batch 40/64 loss: -0.2540982961654663
Batch 41/64 loss: -0.24736863374710083
Batch 42/64 loss: -0.26167720556259155
Batch 43/64 loss: -0.2610445022583008
Batch 44/64 loss: -0.24964332580566406
Batch 45/64 loss: -0.2505400776863098
Batch 46/64 loss: -0.2461194396018982
Batch 47/64 loss: -0.2573848366737366
Batch 48/64 loss: -0.24387192726135254
Batch 49/64 loss: -0.23728126287460327
Batch 50/64 loss: -0.25028663873672485
Batch 51/64 loss: -0.25575679540634155
Batch 52/64 loss: -0.24548906087875366
Batch 53/64 loss: -0.24351471662521362
Batch 54/64 loss: -0.2464541792869568
Batch 55/64 loss: -0.24805784225463867
Batch 56/64 loss: -0.2360566258430481
Batch 57/64 loss: -0.2282261848449707
Batch 58/64 loss: -0.2504171133041382
Batch 59/64 loss: -0.24927568435668945
Batch 60/64 loss: -0.24613994359970093
Batch 61/64 loss: -0.25172632932662964
Batch 62/64 loss: -0.26515042781829834
Batch 63/64 loss: -0.2580263614654541
Batch 64/64 loss: -0.257484495639801
Epoch 150  Train loss: -0.25341492797814164  Val loss: -0.2033634120246389
Epoch 151
-------------------------------
Batch 1/64 loss: -0.23630696535110474
Batch 2/64 loss: -0.25071561336517334
Batch 3/64 loss: -0.2635362148284912
Batch 4/64 loss: -0.23933470249176025
Batch 5/64 loss: -0.2526884078979492
Batch 6/64 loss: -0.24641883373260498
Batch 7/64 loss: -0.24439769983291626
Batch 8/64 loss: -0.23956239223480225
Batch 9/64 loss: -0.26296424865722656
Batch 10/64 loss: -0.2552379369735718
Batch 11/64 loss: -0.2607874870300293
Batch 12/64 loss: -0.24605441093444824
Batch 13/64 loss: -0.27265483140945435
Batch 14/64 loss: -0.26372528076171875
Batch 15/64 loss: -0.2421247959136963
Batch 16/64 loss: -0.2520090341567993
Batch 17/64 loss: -0.2552233338356018
Batch 18/64 loss: -0.25226444005966187
Batch 19/64 loss: -0.2606264352798462
Batch 20/64 loss: -0.24309754371643066
Batch 21/64 loss: -0.24684381484985352
Batch 22/64 loss: -0.2541379928588867
Batch 23/64 loss: -0.2653971314430237
Batch 24/64 loss: -0.2487170696258545
Batch 25/64 loss: -0.25466614961624146
Batch 26/64 loss: -0.2406800389289856
Batch 27/64 loss: -0.2519491910934448
Batch 28/64 loss: -0.25807297229766846
Batch 29/64 loss: -0.21491080522537231
Batch 30/64 loss: -0.23882097005844116
Batch 31/64 loss: -0.23854589462280273
Batch 32/64 loss: -0.24000155925750732
Batch 33/64 loss: -0.23993390798568726
Batch 34/64 loss: -0.24842065572738647
Batch 35/64 loss: -0.2485750913619995
Batch 36/64 loss: -0.2315332293510437
Batch 37/64 loss: -0.22874057292938232
Batch 38/64 loss: -0.24077171087265015
Batch 39/64 loss: -0.2283650040626526
Batch 40/64 loss: -0.24189388751983643
Batch 41/64 loss: -0.24879395961761475
Batch 42/64 loss: -0.25662219524383545
Batch 43/64 loss: -0.24709993600845337
Batch 44/64 loss: -0.24599099159240723
Batch 45/64 loss: -0.25553858280181885
Batch 46/64 loss: -0.2535547614097595
Batch 47/64 loss: -0.24364358186721802
Batch 48/64 loss: -0.26188963651657104
Batch 49/64 loss: -0.25234490633010864
Batch 50/64 loss: -0.2264513373374939
Batch 51/64 loss: -0.23331129550933838
Batch 52/64 loss: -0.23399507999420166
Batch 53/64 loss: -0.24346894025802612
Batch 54/64 loss: -0.25420284271240234
Batch 55/64 loss: -0.25146484375
Batch 56/64 loss: -0.24840843677520752
Batch 57/64 loss: -0.2647241950035095
Batch 58/64 loss: -0.24771541357040405
Batch 59/64 loss: -0.2513260841369629
Batch 60/64 loss: -0.2508559226989746
Batch 61/64 loss: -0.26243311166763306
Batch 62/64 loss: -0.2561933398246765
Batch 63/64 loss: -0.25268077850341797
Batch 64/64 loss: -0.22798609733581543
Epoch 151  Train loss: -0.24806914516523773  Val loss: -0.22543921503414402
Epoch 152
-------------------------------
Batch 1/64 loss: -0.23497295379638672
Batch 2/64 loss: -0.243402361869812
Batch 3/64 loss: -0.24877679347991943
Batch 4/64 loss: -0.26119816303253174
Batch 5/64 loss: -0.2618224620819092
Batch 6/64 loss: -0.24595904350280762
Batch 7/64 loss: -0.246457040309906
Batch 8/64 loss: -0.2681669592857361
Batch 9/64 loss: -0.22252345085144043
Batch 10/64 loss: -0.24875575304031372
Batch 11/64 loss: -0.27150559425354004
Batch 12/64 loss: -0.25521183013916016
Batch 13/64 loss: -0.24718326330184937
Batch 14/64 loss: -0.21153855323791504
Batch 15/64 loss: -0.25624215602874756
Batch 16/64 loss: -0.2438163161277771
Batch 17/64 loss: -0.2551558017730713
Batch 18/64 loss: -0.2490134835243225
Batch 19/64 loss: -0.2629116177558899
Batch 20/64 loss: -0.24817264080047607
Batch 21/64 loss: -0.2597673535346985
Batch 22/64 loss: -0.24374043941497803
Batch 23/64 loss: -0.24590599536895752
Batch 24/64 loss: -0.24802732467651367
Batch 25/64 loss: -0.24591928720474243
Batch 26/64 loss: -0.2450762391090393
Batch 27/64 loss: -0.25326329469680786
Batch 28/64 loss: -0.23845982551574707
Batch 29/64 loss: -0.26880472898483276
Batch 30/64 loss: -0.23575109243392944
Batch 31/64 loss: -0.2583177089691162
Batch 32/64 loss: -0.2567242980003357
Batch 33/64 loss: -0.2535855174064636
Batch 34/64 loss: -0.24160343408584595
Batch 35/64 loss: -0.2368476390838623
Batch 36/64 loss: -0.25599944591522217
Batch 37/64 loss: -0.24365973472595215
Batch 38/64 loss: -0.2485600709915161
Batch 39/64 loss: -0.25608396530151367
Batch 40/64 loss: -0.27382850646972656
Batch 41/64 loss: -0.2386784553527832
Batch 42/64 loss: -0.23535221815109253
Batch 43/64 loss: -0.2595011591911316
Batch 44/64 loss: -0.23668015003204346
Batch 45/64 loss: -0.25769615173339844
Batch 46/64 loss: -0.24632269144058228
Batch 47/64 loss: -0.2580459713935852
Batch 48/64 loss: -0.2422037124633789
Batch 49/64 loss: -0.2588713765144348
Batch 50/64 loss: -0.2741199731826782
Batch 51/64 loss: -0.26363372802734375
Batch 52/64 loss: -0.2586907148361206
Batch 53/64 loss: -0.2682981491088867
Batch 54/64 loss: -0.2533828020095825
Batch 55/64 loss: -0.24000418186187744
Batch 56/64 loss: -0.2447359561920166
Batch 57/64 loss: -0.2514273524284363
Batch 58/64 loss: -0.2519795298576355
Batch 59/64 loss: -0.25166404247283936
Batch 60/64 loss: -0.2666977643966675
Batch 61/64 loss: -0.25204938650131226
Batch 62/64 loss: -0.25584298372268677
Batch 63/64 loss: -0.2634845972061157
Batch 64/64 loss: -0.24933773279190063
Epoch 152  Train loss: -0.2511227998079038  Val loss: -0.2235415942480474
Epoch 153
-------------------------------
Batch 1/64 loss: -0.25444555282592773
Batch 2/64 loss: -0.2707369923591614
Batch 3/64 loss: -0.24109572172164917
Batch 4/64 loss: -0.2599698305130005
Batch 5/64 loss: -0.26389890909194946
Batch 6/64 loss: -0.24732732772827148
Batch 7/64 loss: -0.2529795169830322
Batch 8/64 loss: -0.2583199739456177
Batch 9/64 loss: -0.26897430419921875
Batch 10/64 loss: -0.24866914749145508
Batch 11/64 loss: -0.25680047273635864
Batch 12/64 loss: -0.26783132553100586
Batch 13/64 loss: -0.2564624547958374
Batch 14/64 loss: -0.2466309666633606
Batch 15/64 loss: -0.25540339946746826
Batch 16/64 loss: -0.25663864612579346
Batch 17/64 loss: -0.262819766998291
Batch 18/64 loss: -0.27351945638656616
Batch 19/64 loss: -0.23003923892974854
Batch 20/64 loss: -0.25147175788879395
Batch 21/64 loss: -0.25476664304733276
Batch 22/64 loss: -0.2679120898246765
Batch 23/64 loss: -0.26977288722991943
Batch 24/64 loss: -0.27220553159713745
Batch 25/64 loss: -0.2608788013458252
Batch 26/64 loss: -0.22894829511642456
Batch 27/64 loss: -0.2537577152252197
Batch 28/64 loss: -0.257790207862854
Batch 29/64 loss: -0.25575804710388184
Batch 30/64 loss: -0.24708139896392822
Batch 31/64 loss: -0.25824475288391113
Batch 32/64 loss: -0.23519474267959595
Batch 33/64 loss: -0.26654475927352905
Batch 34/64 loss: -0.24728596210479736
Batch 35/64 loss: -0.24729275703430176
Batch 36/64 loss: -0.2540987730026245
Batch 37/64 loss: -0.2463740110397339
Batch 38/64 loss: -0.2684183716773987
Batch 39/64 loss: -0.2540366053581238
Batch 40/64 loss: -0.23914200067520142
Batch 41/64 loss: -0.24673426151275635
Batch 42/64 loss: -0.24340176582336426
Batch 43/64 loss: -0.23071950674057007
Batch 44/64 loss: -0.25708991289138794
Batch 45/64 loss: -0.260046124458313
Batch 46/64 loss: -0.25245052576065063
Batch 47/64 loss: -0.22170358896255493
Batch 48/64 loss: -0.25508439540863037
Batch 49/64 loss: -0.2636801600456238
Batch 50/64 loss: -0.2548748254776001
Batch 51/64 loss: -0.25211256742477417
Batch 52/64 loss: -0.2635088562965393
Batch 53/64 loss: -0.2682380676269531
Batch 54/64 loss: -0.25746363401412964
Batch 55/64 loss: -0.24520456790924072
Batch 56/64 loss: -0.2524179220199585
Batch 57/64 loss: -0.24396342039108276
Batch 58/64 loss: -0.27072978019714355
Batch 59/64 loss: -0.2565730810165405
Batch 60/64 loss: -0.2313385009765625
Batch 61/64 loss: -0.24062073230743408
Batch 62/64 loss: -0.26595568656921387
Batch 63/64 loss: -0.2528635263442993
Batch 64/64 loss: -0.24939048290252686
Epoch 153  Train loss: -0.2538565864749983  Val loss: -0.22262826285411402
Epoch 154
-------------------------------
Batch 1/64 loss: -0.25617051124572754
Batch 2/64 loss: -0.26691538095474243
Batch 3/64 loss: -0.25571197271347046
Batch 4/64 loss: -0.2632228136062622
Batch 5/64 loss: -0.2540830373764038
Batch 6/64 loss: -0.25673073530197144
Batch 7/64 loss: -0.2511984705924988
Batch 8/64 loss: -0.24062389135360718
Batch 9/64 loss: -0.24906307458877563
Batch 10/64 loss: -0.24980437755584717
Batch 11/64 loss: -0.26059532165527344
Batch 12/64 loss: -0.26187407970428467
Batch 13/64 loss: -0.2618809938430786
Batch 14/64 loss: -0.23876559734344482
Batch 15/64 loss: -0.23318862915039062
Batch 16/64 loss: -0.2406640648841858
Batch 17/64 loss: -0.2459251880645752
Batch 18/64 loss: -0.2476101517677307
Batch 19/64 loss: -0.270962119102478
Batch 20/64 loss: -0.26230233907699585
Batch 21/64 loss: -0.26584911346435547
Batch 22/64 loss: -0.2679925560951233
Batch 23/64 loss: -0.2620425224304199
Batch 24/64 loss: -0.24592018127441406
Batch 25/64 loss: -0.2615640163421631
Batch 26/64 loss: -0.24558943510055542
Batch 27/64 loss: -0.2550821304321289
Batch 28/64 loss: -0.2589414715766907
Batch 29/64 loss: -0.2721153497695923
Batch 30/64 loss: -0.26249396800994873
Batch 31/64 loss: -0.25280773639678955
Batch 32/64 loss: -0.2622412443161011
Batch 33/64 loss: -0.25711262226104736
Batch 34/64 loss: -0.27406489849090576
Batch 35/64 loss: -0.22959840297698975
Batch 36/64 loss: -0.23965030908584595
Batch 37/64 loss: -0.24267619848251343
Batch 38/64 loss: -0.25937390327453613
Batch 39/64 loss: -0.26394206285476685
Batch 40/64 loss: -0.2599979043006897
Batch 41/64 loss: -0.2637782692909241
Batch 42/64 loss: -0.2560325860977173
Batch 43/64 loss: -0.23685598373413086
Batch 44/64 loss: -0.2395671010017395
Batch 45/64 loss: -0.2571343183517456
Batch 46/64 loss: -0.25645148754119873
Batch 47/64 loss: -0.24354463815689087
Batch 48/64 loss: -0.2518489956855774
Batch 49/64 loss: -0.2626062035560608
Batch 50/64 loss: -0.244964599609375
Batch 51/64 loss: -0.25089216232299805
Batch 52/64 loss: -0.2506129741668701
Batch 53/64 loss: -0.26418161392211914
Batch 54/64 loss: -0.2632313370704651
Batch 55/64 loss: -0.24665391445159912
Batch 56/64 loss: -0.2327147126197815
Batch 57/64 loss: -0.2574297785758972
Batch 58/64 loss: -0.26275354623794556
Batch 59/64 loss: -0.2743304967880249
Batch 60/64 loss: -0.23322689533233643
Batch 61/64 loss: -0.2583949565887451
Batch 62/64 loss: -0.24969887733459473
Batch 63/64 loss: -0.2493211030960083
Batch 64/64 loss: -0.2484757900238037
Epoch 154  Train loss: -0.25410089679792813  Val loss: -0.22728214046799441
Epoch 155
-------------------------------
Batch 1/64 loss: -0.2621448040008545
Batch 2/64 loss: -0.26299750804901123
Batch 3/64 loss: -0.257470965385437
Batch 4/64 loss: -0.278110146522522
Batch 5/64 loss: -0.24361515045166016
Batch 6/64 loss: -0.23426014184951782
Batch 7/64 loss: -0.2527086138725281
Batch 8/64 loss: -0.24496710300445557
Batch 9/64 loss: -0.26229894161224365
Batch 10/64 loss: -0.24185693264007568
Batch 11/64 loss: -0.2494293451309204
Batch 12/64 loss: -0.2752933204174042
Batch 13/64 loss: -0.27219337224960327
Batch 14/64 loss: -0.24934762716293335
Batch 15/64 loss: -0.26452475786209106
Batch 16/64 loss: -0.22786962985992432
Batch 17/64 loss: -0.2552744150161743
Batch 18/64 loss: -0.25360846519470215
Batch 19/64 loss: -0.25768160820007324
Batch 20/64 loss: -0.23700755834579468
Batch 21/64 loss: -0.24877381324768066
Batch 22/64 loss: -0.23554885387420654
Batch 23/64 loss: -0.25073760747909546
Batch 24/64 loss: -0.2599000930786133
Batch 25/64 loss: -0.2659314274787903
Batch 26/64 loss: -0.2532312870025635
Batch 27/64 loss: -0.25147610902786255
Batch 28/64 loss: -0.24790942668914795
Batch 29/64 loss: -0.2463754415512085
Batch 30/64 loss: -0.25141727924346924
Batch 31/64 loss: -0.24376815557479858
Batch 32/64 loss: -0.23609596490859985
Batch 33/64 loss: -0.253909707069397
Batch 34/64 loss: -0.26710212230682373
Batch 35/64 loss: -0.2412700057029724
Batch 36/64 loss: -0.2638617157936096
Batch 37/64 loss: -0.26033759117126465
Batch 38/64 loss: -0.22953587770462036
Batch 39/64 loss: -0.2548883557319641
Batch 40/64 loss: -0.26713597774505615
Batch 41/64 loss: -0.2543145418167114
Batch 42/64 loss: -0.2577449083328247
Batch 43/64 loss: -0.25957101583480835
Batch 44/64 loss: -0.2502008080482483
Batch 45/64 loss: -0.24891430139541626
Batch 46/64 loss: -0.260944664478302
Batch 47/64 loss: -0.24389392137527466
Batch 48/64 loss: -0.25490081310272217
Batch 49/64 loss: -0.2675580382347107
Batch 50/64 loss: -0.24417203664779663
Batch 51/64 loss: -0.24003547430038452
Batch 52/64 loss: -0.23940664529800415
Batch 53/64 loss: -0.24470406770706177
Batch 54/64 loss: -0.2591503858566284
Batch 55/64 loss: -0.24656808376312256
Batch 56/64 loss: -0.2321474552154541
Batch 57/64 loss: -0.2461313009262085
Batch 58/64 loss: -0.24813193082809448
Batch 59/64 loss: -0.24892973899841309
Batch 60/64 loss: -0.2598700523376465
Batch 61/64 loss: -0.24135321378707886
Batch 62/64 loss: -0.23372334241867065
Batch 63/64 loss: -0.25012004375457764
Batch 64/64 loss: -0.2576449513435364
Epoch 155  Train loss: -0.25157000340667424  Val loss: -0.2170330480611611
Epoch 156
-------------------------------
Batch 1/64 loss: -0.26915621757507324
Batch 2/64 loss: -0.25444477796554565
Batch 3/64 loss: -0.24061131477355957
Batch 4/64 loss: -0.2789956033229828
Batch 5/64 loss: -0.2537940740585327
Batch 6/64 loss: -0.24601995944976807
Batch 7/64 loss: -0.26469218730926514
Batch 8/64 loss: -0.25230491161346436
Batch 9/64 loss: -0.25366687774658203
Batch 10/64 loss: -0.2553809881210327
Batch 11/64 loss: -0.2553732395172119
Batch 12/64 loss: -0.26822179555892944
Batch 13/64 loss: -0.2540092468261719
Batch 14/64 loss: -0.26495128870010376
Batch 15/64 loss: -0.2681310176849365
Batch 16/64 loss: -0.26648032665252686
Batch 17/64 loss: -0.2667052149772644
Batch 18/64 loss: -0.24429583549499512
Batch 19/64 loss: -0.26476943492889404
Batch 20/64 loss: -0.27012449502944946
Batch 21/64 loss: -0.2670667767524719
Batch 22/64 loss: -0.2631596326828003
Batch 23/64 loss: -0.2584264874458313
Batch 24/64 loss: -0.2454344630241394
Batch 25/64 loss: -0.267009973526001
Batch 26/64 loss: -0.2589993476867676
Batch 27/64 loss: -0.25862252712249756
Batch 28/64 loss: -0.26325923204421997
Batch 29/64 loss: -0.2679099440574646
Batch 30/64 loss: -0.2702394723892212
Batch 31/64 loss: -0.2554081678390503
Batch 32/64 loss: -0.25259941816329956
Batch 33/64 loss: -0.26695334911346436
Batch 34/64 loss: -0.24818050861358643
Batch 35/64 loss: -0.26169365644454956
Batch 36/64 loss: -0.2429901361465454
Batch 37/64 loss: -0.24321424961090088
Batch 38/64 loss: -0.2570763826370239
Batch 39/64 loss: -0.25769639015197754
Batch 40/64 loss: -0.27228450775146484
Batch 41/64 loss: -0.25986772775650024
Batch 42/64 loss: -0.26009660959243774
Batch 43/64 loss: -0.25816452503204346
Batch 44/64 loss: -0.2558608055114746
Batch 45/64 loss: -0.2666587233543396
Batch 46/64 loss: -0.27129173278808594
Batch 47/64 loss: -0.26915794610977173
Batch 48/64 loss: -0.2606128454208374
Batch 49/64 loss: -0.2591540813446045
Batch 50/64 loss: -0.24012666940689087
Batch 51/64 loss: -0.23888975381851196
Batch 52/64 loss: -0.2542232275009155
Batch 53/64 loss: -0.24337482452392578
Batch 54/64 loss: -0.2573738098144531
Batch 55/64 loss: -0.24422848224639893
Batch 56/64 loss: -0.26459360122680664
Batch 57/64 loss: -0.249700129032135
Batch 58/64 loss: -0.261244535446167
Batch 59/64 loss: -0.23985892534255981
Batch 60/64 loss: -0.25170624256134033
Batch 61/64 loss: -0.24181914329528809
Batch 62/64 loss: -0.2530348300933838
Batch 63/64 loss: -0.26873070001602173
Batch 64/64 loss: -0.24193865060806274
Epoch 156  Train loss: -0.25759336925020404  Val loss: -0.2371697714648296
Saving best model, epoch: 156
Epoch 157
-------------------------------
Batch 1/64 loss: -0.2586217522621155
Batch 2/64 loss: -0.24750715494155884
Batch 3/64 loss: -0.25578582286834717
Batch 4/64 loss: -0.2566048502922058
Batch 5/64 loss: -0.27342790365219116
Batch 6/64 loss: -0.256591260433197
Batch 7/64 loss: -0.26498866081237793
Batch 8/64 loss: -0.24319100379943848
Batch 9/64 loss: -0.26386862993240356
Batch 10/64 loss: -0.2594398260116577
Batch 11/64 loss: -0.2583308219909668
Batch 12/64 loss: -0.24410521984100342
Batch 13/64 loss: -0.25039881467819214
Batch 14/64 loss: -0.26212847232818604
Batch 15/64 loss: -0.2509382963180542
Batch 16/64 loss: -0.2842591404914856
Batch 17/64 loss: -0.2621462941169739
Batch 18/64 loss: -0.2753757834434509
Batch 19/64 loss: -0.2606998682022095
Batch 20/64 loss: -0.2583159804344177
Batch 21/64 loss: -0.26049453020095825
Batch 22/64 loss: -0.25304651260375977
Batch 23/64 loss: -0.228399395942688
Batch 24/64 loss: -0.2509644031524658
Batch 25/64 loss: -0.25399214029312134
Batch 26/64 loss: -0.23818570375442505
Batch 27/64 loss: -0.24733662605285645
Batch 28/64 loss: -0.23016834259033203
Batch 29/64 loss: -0.2635303735733032
Batch 30/64 loss: -0.26175427436828613
Batch 31/64 loss: -0.26740074157714844
Batch 32/64 loss: -0.25618648529052734
Batch 33/64 loss: -0.25874006748199463
Batch 34/64 loss: -0.2679176926612854
Batch 35/64 loss: -0.25218838453292847
Batch 36/64 loss: -0.2427765130996704
Batch 37/64 loss: -0.2632976770401001
Batch 38/64 loss: -0.2631028890609741
Batch 39/64 loss: -0.2734437584877014
Batch 40/64 loss: -0.24964439868927002
Batch 41/64 loss: -0.2651480436325073
Batch 42/64 loss: -0.2544475793838501
Batch 43/64 loss: -0.24763303995132446
Batch 44/64 loss: -0.2589429020881653
Batch 45/64 loss: -0.2585027813911438
Batch 46/64 loss: -0.2550775408744812
Batch 47/64 loss: -0.25535112619400024
Batch 48/64 loss: -0.27078455686569214
Batch 49/64 loss: -0.25037872791290283
Batch 50/64 loss: -0.25174570083618164
Batch 51/64 loss: -0.2621668577194214
Batch 52/64 loss: -0.27052998542785645
Batch 53/64 loss: -0.2430100440979004
Batch 54/64 loss: -0.2660449147224426
Batch 55/64 loss: -0.253878116607666
Batch 56/64 loss: -0.2574213743209839
Batch 57/64 loss: -0.23451250791549683
Batch 58/64 loss: -0.2482069730758667
Batch 59/64 loss: -0.252943217754364
Batch 60/64 loss: -0.26780444383621216
Batch 61/64 loss: -0.264537513256073
Batch 62/64 loss: -0.2529473304748535
Batch 63/64 loss: -0.26467055082321167
Batch 64/64 loss: -0.24237483739852905
Epoch 157  Train loss: -0.25659236744338393  Val loss: -0.23758222413636565
Saving best model, epoch: 157
Epoch 158
-------------------------------
Batch 1/64 loss: -0.24588966369628906
Batch 2/64 loss: -0.25194019079208374
Batch 3/64 loss: -0.24205416440963745
Batch 4/64 loss: -0.25036823749542236
Batch 5/64 loss: -0.2660033106803894
Batch 6/64 loss: -0.25856852531433105
Batch 7/64 loss: -0.23774266242980957
Batch 8/64 loss: -0.2502593398094177
Batch 9/64 loss: -0.25090402364730835
Batch 10/64 loss: -0.26662111282348633
Batch 11/64 loss: -0.2653679847717285
Batch 12/64 loss: -0.252566933631897
Batch 13/64 loss: -0.24256592988967896
Batch 14/64 loss: -0.2462249994277954
Batch 15/64 loss: -0.2690044045448303
Batch 16/64 loss: -0.26080042123794556
Batch 17/64 loss: -0.25266027450561523
Batch 18/64 loss: -0.27116644382476807
Batch 19/64 loss: -0.24551647901535034
Batch 20/64 loss: -0.24262875318527222
Batch 21/64 loss: -0.2318781614303589
Batch 22/64 loss: -0.25907015800476074
Batch 23/64 loss: -0.25063246488571167
Batch 24/64 loss: -0.24294596910476685
Batch 25/64 loss: -0.2635611891746521
Batch 26/64 loss: -0.26695626974105835
Batch 27/64 loss: -0.2548104524612427
Batch 28/64 loss: -0.25186604261398315
Batch 29/64 loss: -0.26747769117355347
Batch 30/64 loss: -0.24755936861038208
Batch 31/64 loss: -0.23155546188354492
Batch 32/64 loss: -0.25094789266586304
Batch 33/64 loss: -0.20823150873184204
Batch 34/64 loss: -0.25354981422424316
Batch 35/64 loss: -0.26442742347717285
Batch 36/64 loss: -0.24319779872894287
Batch 37/64 loss: -0.2660205364227295
Batch 38/64 loss: -0.2548355460166931
Batch 39/64 loss: -0.25170445442199707
Batch 40/64 loss: -0.24933022260665894
Batch 41/64 loss: -0.24765342473983765
Batch 42/64 loss: -0.25856220722198486
Batch 43/64 loss: -0.26067548990249634
Batch 44/64 loss: -0.2523465156555176
Batch 45/64 loss: -0.2542306184768677
Batch 46/64 loss: -0.23516827821731567
Batch 47/64 loss: -0.2785096764564514
Batch 48/64 loss: -0.26493269205093384
Batch 49/64 loss: -0.2615165710449219
Batch 50/64 loss: -0.2528337240219116
Batch 51/64 loss: -0.25465619564056396
Batch 52/64 loss: -0.25337696075439453
Batch 53/64 loss: -0.2525063157081604
Batch 54/64 loss: -0.25743740797042847
Batch 55/64 loss: -0.26039040088653564
Batch 56/64 loss: -0.25611335039138794
Batch 57/64 loss: -0.2514533996582031
Batch 58/64 loss: -0.25037771463394165
Batch 59/64 loss: -0.24482542276382446
Batch 60/64 loss: -0.2475130558013916
Batch 61/64 loss: -0.2385627031326294
Batch 62/64 loss: -0.24187076091766357
Batch 63/64 loss: -0.25261610746383667
Batch 64/64 loss: -0.2644599676132202
Epoch 158  Train loss: -0.25264135295269535  Val loss: -0.1988473575549437
Epoch 159
-------------------------------
Batch 1/64 loss: -0.245638906955719
Batch 2/64 loss: -0.2649378776550293
Batch 3/64 loss: -0.24902784824371338
Batch 4/64 loss: -0.23999738693237305
Batch 5/64 loss: -0.2504727840423584
Batch 6/64 loss: -0.24242013692855835
Batch 7/64 loss: -0.2405821681022644
Batch 8/64 loss: -0.2614234685897827
Batch 9/64 loss: -0.25733697414398193
Batch 10/64 loss: -0.25628262758255005
Batch 11/64 loss: -0.2547304630279541
Batch 12/64 loss: -0.23911434412002563
Batch 13/64 loss: -0.25288230180740356
Batch 14/64 loss: -0.26067423820495605
Batch 15/64 loss: -0.2567552328109741
Batch 16/64 loss: -0.25839632749557495
Batch 17/64 loss: -0.2541773319244385
Batch 18/64 loss: -0.25013166666030884
Batch 19/64 loss: -0.26323938369750977
Batch 20/64 loss: -0.2552250623703003
Batch 21/64 loss: -0.25919294357299805
Batch 22/64 loss: -0.25724130868911743
Batch 23/64 loss: -0.2545672655105591
Batch 24/64 loss: -0.24509847164154053
Batch 25/64 loss: -0.2494925856590271
Batch 26/64 loss: -0.2444220781326294
Batch 27/64 loss: -0.2556190490722656
Batch 28/64 loss: -0.24698561429977417
Batch 29/64 loss: -0.24966657161712646
Batch 30/64 loss: -0.2575477361679077
Batch 31/64 loss: -0.26220017671585083
Batch 32/64 loss: -0.23920506238937378
Batch 33/64 loss: -0.24556845426559448
Batch 34/64 loss: -0.23696529865264893
Batch 35/64 loss: -0.25251340866088867
Batch 36/64 loss: -0.25620049238204956
Batch 37/64 loss: -0.2645503282546997
Batch 38/64 loss: -0.24194443225860596
Batch 39/64 loss: -0.24862587451934814
Batch 40/64 loss: -0.2661092281341553
Batch 41/64 loss: -0.2668002247810364
Batch 42/64 loss: -0.24257713556289673
Batch 43/64 loss: -0.24935483932495117
Batch 44/64 loss: -0.2594139575958252
Batch 45/64 loss: -0.26308250427246094
Batch 46/64 loss: -0.26218128204345703
Batch 47/64 loss: -0.2596072554588318
Batch 48/64 loss: -0.24940872192382812
Batch 49/64 loss: -0.24893856048583984
Batch 50/64 loss: -0.24296247959136963
Batch 51/64 loss: -0.2577056884765625
Batch 52/64 loss: -0.2503014802932739
Batch 53/64 loss: -0.22978723049163818
Batch 54/64 loss: -0.23791873455047607
Batch 55/64 loss: -0.2620798349380493
Batch 56/64 loss: -0.2753678560256958
Batch 57/64 loss: -0.2588720917701721
Batch 58/64 loss: -0.26032716035842896
Batch 59/64 loss: -0.2716448903083801
Batch 60/64 loss: -0.25880444049835205
Batch 61/64 loss: -0.2566889524459839
Batch 62/64 loss: -0.239105224609375
Batch 63/64 loss: -0.25811612606048584
Batch 64/64 loss: -0.26695770025253296
Epoch 159  Train loss: -0.25330867229723464  Val loss: -0.2406509571878361
Saving best model, epoch: 159
Epoch 160
-------------------------------
Batch 1/64 loss: -0.2664960026741028
Batch 2/64 loss: -0.25648409128189087
Batch 3/64 loss: -0.2668353319168091
Batch 4/64 loss: -0.2606123685836792
Batch 5/64 loss: -0.2390230894088745
Batch 6/64 loss: -0.26671135425567627
Batch 7/64 loss: -0.260276198387146
Batch 8/64 loss: -0.25581830739974976
Batch 9/64 loss: -0.24244320392608643
Batch 10/64 loss: -0.2423396110534668
Batch 11/64 loss: -0.2604275941848755
Batch 12/64 loss: -0.26514482498168945
Batch 13/64 loss: -0.24402111768722534
Batch 14/64 loss: -0.25489264726638794
Batch 15/64 loss: -0.22942572832107544
Batch 16/64 loss: -0.268526554107666
Batch 17/64 loss: -0.2450154423713684
Batch 18/64 loss: -0.26934289932250977
Batch 19/64 loss: -0.2738669514656067
Batch 20/64 loss: -0.2695266008377075
Batch 21/64 loss: -0.23068511486053467
Batch 22/64 loss: -0.260811448097229
Batch 23/64 loss: -0.257246732711792
Batch 24/64 loss: -0.2630521059036255
Batch 25/64 loss: -0.255215585231781
Batch 26/64 loss: -0.2599806785583496
Batch 27/64 loss: -0.259682297706604
Batch 28/64 loss: -0.2636801600456238
Batch 29/64 loss: -0.26164549589157104
Batch 30/64 loss: -0.2519538402557373
Batch 31/64 loss: -0.25095564126968384
Batch 32/64 loss: -0.2797538638114929
Batch 33/64 loss: -0.25564414262771606
Batch 34/64 loss: -0.2586829662322998
Batch 35/64 loss: -0.258961021900177
Batch 36/64 loss: -0.24607175588607788
Batch 37/64 loss: -0.25883251428604126
Batch 38/64 loss: -0.2552258372306824
Batch 39/64 loss: -0.2562209367752075
Batch 40/64 loss: -0.2634850740432739
Batch 41/64 loss: -0.25828254222869873
Batch 42/64 loss: -0.23055773973464966
Batch 43/64 loss: -0.2349158525466919
Batch 44/64 loss: -0.2663875222206116
Batch 45/64 loss: -0.2547493577003479
Batch 46/64 loss: -0.2607569098472595
Batch 47/64 loss: -0.259297251701355
Batch 48/64 loss: -0.2511802315711975
Batch 49/64 loss: -0.25809359550476074
Batch 50/64 loss: -0.25237518548965454
Batch 51/64 loss: -0.26091206073760986
Batch 52/64 loss: -0.22640973329544067
Batch 53/64 loss: -0.2610653042793274
Batch 54/64 loss: -0.25635308027267456
Batch 55/64 loss: -0.2501869797706604
Batch 56/64 loss: -0.23541259765625
Batch 57/64 loss: -0.24711167812347412
Batch 58/64 loss: -0.25398343801498413
Batch 59/64 loss: -0.2484087347984314
Batch 60/64 loss: -0.26035523414611816
Batch 61/64 loss: -0.26297396421432495
Batch 62/64 loss: -0.2645946145057678
Batch 63/64 loss: -0.24486654996871948
Batch 64/64 loss: -0.268024742603302
Epoch 160  Train loss: -0.2554551505574993  Val loss: -0.21576310904165313
Epoch 161
-------------------------------
Batch 1/64 loss: -0.27632173895835876
Batch 2/64 loss: -0.26804232597351074
Batch 3/64 loss: -0.2541534900665283
Batch 4/64 loss: -0.22698533535003662
Batch 5/64 loss: -0.25805920362472534
Batch 6/64 loss: -0.2616981267929077
Batch 7/64 loss: -0.2486499547958374
Batch 8/64 loss: -0.24255895614624023
Batch 9/64 loss: -0.24244463443756104
Batch 10/64 loss: -0.2571513056755066
Batch 11/64 loss: -0.2447555661201477
Batch 12/64 loss: -0.26565074920654297
Batch 13/64 loss: -0.2498178482055664
Batch 14/64 loss: -0.24850863218307495
Batch 15/64 loss: -0.26610636711120605
Batch 16/64 loss: -0.2647086977958679
Batch 17/64 loss: -0.26149630546569824
Batch 18/64 loss: -0.2563472390174866
Batch 19/64 loss: -0.26623690128326416
Batch 20/64 loss: -0.24794769287109375
Batch 21/64 loss: -0.2488565444946289
Batch 22/64 loss: -0.26986610889434814
Batch 23/64 loss: -0.2603722810745239
Batch 24/64 loss: -0.26124298572540283
Batch 25/64 loss: -0.2696226239204407
Batch 26/64 loss: -0.2715856432914734
Batch 27/64 loss: -0.26228588819503784
Batch 28/64 loss: -0.2319381833076477
Batch 29/64 loss: -0.25087881088256836
Batch 30/64 loss: -0.25469160079956055
Batch 31/64 loss: -0.2598555088043213
Batch 32/64 loss: -0.20493507385253906
Batch 33/64 loss: -0.25901520252227783
Batch 34/64 loss: -0.26536285877227783
Batch 35/64 loss: -0.2705901265144348
Batch 36/64 loss: -0.25333285331726074
Batch 37/64 loss: -0.2643563747406006
Batch 38/64 loss: -0.25132662057876587
Batch 39/64 loss: -0.23958253860473633
Batch 40/64 loss: -0.2562040090560913
Batch 41/64 loss: -0.26801377534866333
Batch 42/64 loss: -0.22673094272613525
Batch 43/64 loss: -0.26805615425109863
Batch 44/64 loss: -0.21936243772506714
Batch 45/64 loss: -0.2430659532546997
Batch 46/64 loss: -0.23690640926361084
Batch 47/64 loss: -0.26363372802734375
Batch 48/64 loss: -0.26164066791534424
Batch 49/64 loss: -0.25128597021102905
Batch 50/64 loss: -0.2581740617752075
Batch 51/64 loss: -0.27038782835006714
Batch 52/64 loss: -0.24560141563415527
Batch 53/64 loss: -0.24939119815826416
Batch 54/64 loss: -0.26107412576675415
Batch 55/64 loss: -0.2621779441833496
Batch 56/64 loss: -0.2629721164703369
Batch 57/64 loss: -0.2663288116455078
Batch 58/64 loss: -0.24696743488311768
Batch 59/64 loss: -0.27359873056411743
Batch 60/64 loss: -0.25357306003570557
Batch 61/64 loss: -0.2399388551712036
Batch 62/64 loss: -0.25658488273620605
Batch 63/64 loss: -0.25253647565841675
Batch 64/64 loss: -0.26103585958480835
Epoch 161  Train loss: -0.25485996522155463  Val loss: -0.23053031606772512
Epoch 162
-------------------------------
Batch 1/64 loss: -0.23925381898880005
Batch 2/64 loss: -0.2480211853981018
Batch 3/64 loss: -0.27481067180633545
Batch 4/64 loss: -0.25703132152557373
Batch 5/64 loss: -0.25323808193206787
Batch 6/64 loss: -0.25266319513320923
Batch 7/64 loss: -0.26254481077194214
Batch 8/64 loss: -0.25395041704177856
Batch 9/64 loss: -0.23462283611297607
Batch 10/64 loss: -0.227061927318573
Batch 11/64 loss: -0.25115108489990234
Batch 12/64 loss: -0.24361485242843628
Batch 13/64 loss: -0.25161468982696533
Batch 14/64 loss: -0.2562710642814636
Batch 15/64 loss: -0.2400137186050415
Batch 16/64 loss: -0.2526775598526001
Batch 17/64 loss: -0.24720311164855957
Batch 18/64 loss: -0.2692532539367676
Batch 19/64 loss: -0.26167988777160645
Batch 20/64 loss: -0.26017338037490845
Batch 21/64 loss: -0.2490665316581726
Batch 22/64 loss: -0.2582990527153015
Batch 23/64 loss: -0.24064409732818604
Batch 24/64 loss: -0.2459733486175537
Batch 25/64 loss: -0.23051106929779053
Batch 26/64 loss: -0.2349451184272766
Batch 27/64 loss: -0.25390487909317017
Batch 28/64 loss: -0.26101046800613403
Batch 29/64 loss: -0.2375827431678772
Batch 30/64 loss: -0.24496608972549438
Batch 31/64 loss: -0.24903583526611328
Batch 32/64 loss: -0.2641666531562805
Batch 33/64 loss: -0.24050021171569824
Batch 34/64 loss: -0.2421441674232483
Batch 35/64 loss: -0.2626522183418274
Batch 36/64 loss: -0.2547784447669983
Batch 37/64 loss: -0.2486339807510376
Batch 38/64 loss: -0.24741852283477783
Batch 39/64 loss: -0.2325233817100525
Batch 40/64 loss: -0.23907685279846191
Batch 41/64 loss: -0.2441844940185547
Batch 42/64 loss: -0.2668536901473999
Batch 43/64 loss: -0.254841685295105
Batch 44/64 loss: -0.2561987638473511
Batch 45/64 loss: -0.2670172452926636
Batch 46/64 loss: -0.23529481887817383
Batch 47/64 loss: -0.2603047490119934
Batch 48/64 loss: -0.2556665539741516
Batch 49/64 loss: -0.24972671270370483
Batch 50/64 loss: -0.24355030059814453
Batch 51/64 loss: -0.25869786739349365
Batch 52/64 loss: -0.23688554763793945
Batch 53/64 loss: -0.26576006412506104
Batch 54/64 loss: -0.27495187520980835
Batch 55/64 loss: -0.24839460849761963
Batch 56/64 loss: -0.259035587310791
Batch 57/64 loss: -0.2314060926437378
Batch 58/64 loss: -0.25940459966659546
Batch 59/64 loss: -0.23408842086791992
Batch 60/64 loss: -0.24987322092056274
Batch 61/64 loss: -0.24620270729064941
Batch 62/64 loss: -0.25903159379959106
Batch 63/64 loss: -0.260046124458313
Batch 64/64 loss: -0.25927358865737915
Epoch 162  Train loss: -0.25076952237708894  Val loss: -0.22579010593932108
Epoch 163
-------------------------------
Batch 1/64 loss: -0.23544639348983765
Batch 2/64 loss: -0.2601374387741089
Batch 3/64 loss: -0.2696496844291687
Batch 4/64 loss: -0.26288777589797974
Batch 5/64 loss: -0.25385797023773193
Batch 6/64 loss: -0.25725769996643066
Batch 7/64 loss: -0.2655680179595947
Batch 8/64 loss: -0.25441640615463257
Batch 9/64 loss: -0.27461177110671997
Batch 10/64 loss: -0.2661527395248413
Batch 11/64 loss: -0.25063228607177734
Batch 12/64 loss: -0.26241958141326904
Batch 13/64 loss: -0.24926519393920898
Batch 14/64 loss: -0.2665673494338989
Batch 15/64 loss: -0.23429745435714722
Batch 16/64 loss: -0.2683742046356201
Batch 17/64 loss: -0.2640168070793152
Batch 18/64 loss: -0.23803609609603882
Batch 19/64 loss: -0.2673124074935913
Batch 20/64 loss: -0.2692621350288391
Batch 21/64 loss: -0.2653273940086365
Batch 22/64 loss: -0.27129995822906494
Batch 23/64 loss: -0.24795103073120117
Batch 24/64 loss: -0.25899481773376465
Batch 25/64 loss: -0.26141494512557983
Batch 26/64 loss: -0.26273179054260254
Batch 27/64 loss: -0.23985207080841064
Batch 28/64 loss: -0.2665412425994873
Batch 29/64 loss: -0.2670251727104187
Batch 30/64 loss: -0.26495248079299927
Batch 31/64 loss: -0.26273953914642334
Batch 32/64 loss: -0.2616639733314514
Batch 33/64 loss: -0.2514466643333435
Batch 34/64 loss: -0.25053179264068604
Batch 35/64 loss: -0.2614107131958008
Batch 36/64 loss: -0.26401054859161377
Batch 37/64 loss: -0.26239073276519775
Batch 38/64 loss: -0.2577056288719177
Batch 39/64 loss: -0.24910664558410645
Batch 40/64 loss: -0.25535744428634644
Batch 41/64 loss: -0.2528418302536011
Batch 42/64 loss: -0.2586812973022461
Batch 43/64 loss: -0.26553529500961304
Batch 44/64 loss: -0.23519551753997803
Batch 45/64 loss: -0.26541370153427124
Batch 46/64 loss: -0.26061463356018066
Batch 47/64 loss: -0.26051610708236694
Batch 48/64 loss: -0.2538456320762634
Batch 49/64 loss: -0.24002283811569214
Batch 50/64 loss: -0.26039373874664307
Batch 51/64 loss: -0.2518106698989868
Batch 52/64 loss: -0.25849610567092896
Batch 53/64 loss: -0.2637676000595093
Batch 54/64 loss: -0.22802984714508057
Batch 55/64 loss: -0.2502248287200928
Batch 56/64 loss: -0.25121647119522095
Batch 57/64 loss: -0.24459415674209595
Batch 58/64 loss: -0.2619783282279968
Batch 59/64 loss: -0.23922467231750488
Batch 60/64 loss: -0.2428492307662964
Batch 61/64 loss: -0.24921584129333496
Batch 62/64 loss: -0.2523092031478882
Batch 63/64 loss: -0.2595611810684204
Batch 64/64 loss: -0.2605332136154175
Epoch 163  Train loss: -0.25653902175379734  Val loss: -0.23419749941612847
Epoch 164
-------------------------------
Batch 1/64 loss: -0.2500194311141968
Batch 2/64 loss: -0.2638291120529175
Batch 3/64 loss: -0.25970953702926636
Batch 4/64 loss: -0.2596200108528137
Batch 5/64 loss: -0.2690168023109436
Batch 6/64 loss: -0.266298770904541
Batch 7/64 loss: -0.24378037452697754
Batch 8/64 loss: -0.26436418294906616
Batch 9/64 loss: -0.2510796785354614
Batch 10/64 loss: -0.26165616512298584
Batch 11/64 loss: -0.257260799407959
Batch 12/64 loss: -0.25412559509277344
Batch 13/64 loss: -0.26826655864715576
Batch 14/64 loss: -0.24121439456939697
Batch 15/64 loss: -0.2630489468574524
Batch 16/64 loss: -0.2503848075866699
Batch 17/64 loss: -0.2499164342880249
Batch 18/64 loss: -0.25273895263671875
Batch 19/64 loss: -0.2705180048942566
Batch 20/64 loss: -0.2433452606201172
Batch 21/64 loss: -0.24878805875778198
Batch 22/64 loss: -0.2588246464729309
Batch 23/64 loss: -0.2287920117378235
Batch 24/64 loss: -0.24063169956207275
Batch 25/64 loss: -0.2528609037399292
Batch 26/64 loss: -0.251537024974823
Batch 27/64 loss: -0.25775378942489624
Batch 28/64 loss: -0.2555583715438843
Batch 29/64 loss: -0.2603273391723633
Batch 30/64 loss: -0.2611197829246521
Batch 31/64 loss: -0.2498791217803955
Batch 32/64 loss: -0.2595902681350708
Batch 33/64 loss: -0.24312198162078857
Batch 34/64 loss: -0.24129605293273926
Batch 35/64 loss: -0.26709938049316406
Batch 36/64 loss: -0.2692703604698181
Batch 37/64 loss: -0.2550584077835083
Batch 38/64 loss: -0.2536700367927551
Batch 39/64 loss: -0.2321549654006958
Batch 40/64 loss: -0.2603294849395752
Batch 41/64 loss: -0.2743961811065674
Batch 42/64 loss: -0.25227975845336914
Batch 43/64 loss: -0.24790799617767334
Batch 44/64 loss: -0.26153677701950073
Batch 45/64 loss: -0.2532559633255005
Batch 46/64 loss: -0.2576514482498169
Batch 47/64 loss: -0.23761624097824097
Batch 48/64 loss: -0.25296807289123535
Batch 49/64 loss: -0.24500834941864014
Batch 50/64 loss: -0.2645522952079773
Batch 51/64 loss: -0.27049654722213745
Batch 52/64 loss: -0.2528870105743408
Batch 53/64 loss: -0.2697798013687134
Batch 54/64 loss: -0.26473909616470337
Batch 55/64 loss: -0.24200814962387085
Batch 56/64 loss: -0.2621244788169861
Batch 57/64 loss: -0.264581561088562
Batch 58/64 loss: -0.23910599946975708
Batch 59/64 loss: -0.2491053342819214
Batch 60/64 loss: -0.247067391872406
Batch 61/64 loss: -0.2592981457710266
Batch 62/64 loss: -0.23716312646865845
Batch 63/64 loss: -0.22967058420181274
Batch 64/64 loss: -0.24311810731887817
Epoch 164  Train loss: -0.25420229645336373  Val loss: -0.20069364993432953
Epoch 165
-------------------------------
Batch 1/64 loss: -0.24485689401626587
Batch 2/64 loss: -0.23790216445922852
Batch 3/64 loss: -0.24736034870147705
Batch 4/64 loss: -0.2437484860420227
Batch 5/64 loss: -0.24802279472351074
Batch 6/64 loss: -0.25751709938049316
Batch 7/64 loss: -0.23809897899627686
Batch 8/64 loss: -0.23553913831710815
Batch 9/64 loss: -0.25356417894363403
Batch 10/64 loss: -0.2609555125236511
Batch 11/64 loss: -0.2663693428039551
Batch 12/64 loss: -0.2617093324661255
Batch 13/64 loss: -0.23782670497894287
Batch 14/64 loss: -0.23210233449935913
Batch 15/64 loss: -0.25727468729019165
Batch 16/64 loss: -0.2513657808303833
Batch 17/64 loss: -0.2512427568435669
Batch 18/64 loss: -0.2581218481063843
Batch 19/64 loss: -0.2686595916748047
Batch 20/64 loss: -0.2505275011062622
Batch 21/64 loss: -0.2601194977760315
Batch 22/64 loss: -0.2442697286605835
Batch 23/64 loss: -0.25174134969711304
Batch 24/64 loss: -0.260870099067688
Batch 25/64 loss: -0.24670350551605225
Batch 26/64 loss: -0.24253559112548828
Batch 27/64 loss: -0.27103155851364136
Batch 28/64 loss: -0.2762755751609802
Batch 29/64 loss: -0.253670334815979
Batch 30/64 loss: -0.2521229386329651
Batch 31/64 loss: -0.2635296583175659
Batch 32/64 loss: -0.23199355602264404
Batch 33/64 loss: -0.26183950901031494
Batch 34/64 loss: -0.2676280736923218
Batch 35/64 loss: -0.251259446144104
Batch 36/64 loss: -0.25251954793930054
Batch 37/64 loss: -0.2586362361907959
Batch 38/64 loss: -0.25930845737457275
Batch 39/64 loss: -0.25613200664520264
Batch 40/64 loss: -0.24557644128799438
Batch 41/64 loss: -0.2600587010383606
Batch 42/64 loss: -0.24000108242034912
Batch 43/64 loss: -0.259560227394104
Batch 44/64 loss: -0.2468765377998352
Batch 45/64 loss: -0.26106059551239014
Batch 46/64 loss: -0.24775344133377075
Batch 47/64 loss: -0.26188063621520996
Batch 48/64 loss: -0.25340867042541504
Batch 49/64 loss: -0.249006986618042
Batch 50/64 loss: -0.2667649984359741
Batch 51/64 loss: -0.2526301145553589
Batch 52/64 loss: -0.257931649684906
Batch 53/64 loss: -0.22479510307312012
Batch 54/64 loss: -0.25103819370269775
Batch 55/64 loss: -0.26025885343551636
Batch 56/64 loss: -0.2524026036262512
Batch 57/64 loss: -0.25776422023773193
Batch 58/64 loss: -0.24586862325668335
Batch 59/64 loss: -0.25951898097991943
Batch 60/64 loss: -0.2616836428642273
Batch 61/64 loss: -0.2606339454650879
Batch 62/64 loss: -0.2664588689804077
Batch 63/64 loss: -0.257232666015625
Batch 64/64 loss: -0.2330402135848999
Epoch 165  Train loss: -0.25317487202438654  Val loss: -0.22414208482630885
Epoch 166
-------------------------------
Batch 1/64 loss: -0.2539600729942322
Batch 2/64 loss: -0.2728368043899536
Batch 3/64 loss: -0.2577558755874634
Batch 4/64 loss: -0.24457651376724243
Batch 5/64 loss: -0.2546449899673462
Batch 6/64 loss: -0.25367116928100586
Batch 7/64 loss: -0.2468520998954773
Batch 8/64 loss: -0.2504922151565552
Batch 9/64 loss: -0.23434430360794067
Batch 10/64 loss: -0.24077880382537842
Batch 11/64 loss: -0.2546919584274292
Batch 12/64 loss: -0.2650860548019409
Batch 13/64 loss: -0.23630696535110474
Batch 14/64 loss: -0.2673783302307129
Batch 15/64 loss: -0.25283384323120117
Batch 16/64 loss: -0.25077205896377563
Batch 17/64 loss: -0.2609267830848694
Batch 18/64 loss: -0.25567781925201416
Batch 19/64 loss: -0.2565428614616394
Batch 20/64 loss: -0.2540247440338135
Batch 21/64 loss: -0.2594428062438965
Batch 22/64 loss: -0.23341655731201172
Batch 23/64 loss: -0.2524069547653198
Batch 24/64 loss: -0.2420777678489685
Batch 25/64 loss: -0.24595588445663452
Batch 26/64 loss: -0.2603551149368286
Batch 27/64 loss: -0.2549612522125244
Batch 28/64 loss: -0.26548004150390625
Batch 29/64 loss: -0.25813573598861694
Batch 30/64 loss: -0.25278782844543457
Batch 31/64 loss: -0.25602298974990845
Batch 32/64 loss: -0.26921355724334717
Batch 33/64 loss: -0.26872456073760986
Batch 34/64 loss: -0.2632943391799927
Batch 35/64 loss: -0.26211708784103394
Batch 36/64 loss: -0.25990426540374756
Batch 37/64 loss: -0.25429898500442505
Batch 38/64 loss: -0.2602579593658447
Batch 39/64 loss: -0.24193155765533447
Batch 40/64 loss: -0.2599848508834839
Batch 41/64 loss: -0.2558354139328003
Batch 42/64 loss: -0.26423174142837524
Batch 43/64 loss: -0.24964964389801025
Batch 44/64 loss: -0.26355624198913574
Batch 45/64 loss: -0.2643918991088867
Batch 46/64 loss: -0.2670621871948242
Batch 47/64 loss: -0.2484862208366394
Batch 48/64 loss: -0.2584625482559204
Batch 49/64 loss: -0.23762553930282593
Batch 50/64 loss: -0.25476354360580444
Batch 51/64 loss: -0.2528427839279175
Batch 52/64 loss: -0.2509356141090393
Batch 53/64 loss: -0.25790834426879883
Batch 54/64 loss: -0.2513766288757324
Batch 55/64 loss: -0.23379695415496826
Batch 56/64 loss: -0.2755765914916992
Batch 57/64 loss: -0.25280964374542236
Batch 58/64 loss: -0.2681955099105835
Batch 59/64 loss: -0.2628428339958191
Batch 60/64 loss: -0.2486611008644104
Batch 61/64 loss: -0.2732444405555725
Batch 62/64 loss: -0.26434212923049927
Batch 63/64 loss: -0.2512471675872803
Batch 64/64 loss: -0.27053606510162354
Epoch 166  Train loss: -0.25561837074803373  Val loss: -0.2250512225931043
Epoch 167
-------------------------------
Batch 1/64 loss: -0.25236034393310547
Batch 2/64 loss: -0.2536189556121826
Batch 3/64 loss: -0.2407991886138916
Batch 4/64 loss: -0.259372353553772
Batch 5/64 loss: -0.24896717071533203
Batch 6/64 loss: -0.24820250272750854
Batch 7/64 loss: -0.26634275913238525
Batch 8/64 loss: -0.2629566192626953
Batch 9/64 loss: -0.24585217237472534
Batch 10/64 loss: -0.24897170066833496
Batch 11/64 loss: -0.2765679359436035
Batch 12/64 loss: -0.2487342357635498
Batch 13/64 loss: -0.2481815218925476
Batch 14/64 loss: -0.27570557594299316
Batch 15/64 loss: -0.27876392006874084
Batch 16/64 loss: -0.2532479763031006
Batch 17/64 loss: -0.25397247076034546
Batch 18/64 loss: -0.24947166442871094
Batch 19/64 loss: -0.2696651220321655
Batch 20/64 loss: -0.2602287530899048
Batch 21/64 loss: -0.26212871074676514
Batch 22/64 loss: -0.23720353841781616
Batch 23/64 loss: -0.26271528005599976
Batch 24/64 loss: -0.2820553779602051
Batch 25/64 loss: -0.22922229766845703
Batch 26/64 loss: -0.25356370210647583
Batch 27/64 loss: -0.2738686203956604
Batch 28/64 loss: -0.26995158195495605
Batch 29/64 loss: -0.2630350589752197
Batch 30/64 loss: -0.25798285007476807
Batch 31/64 loss: -0.2533966302871704
Batch 32/64 loss: -0.2732153534889221
Batch 33/64 loss: -0.23326218128204346
Batch 34/64 loss: -0.2605780363082886
Batch 35/64 loss: -0.25280702114105225
Batch 36/64 loss: -0.2602296471595764
Batch 37/64 loss: -0.25754958391189575
Batch 38/64 loss: -0.2700735926628113
Batch 39/64 loss: -0.26094377040863037
Batch 40/64 loss: -0.27036595344543457
Batch 41/64 loss: -0.26905280351638794
Batch 42/64 loss: -0.25126945972442627
Batch 43/64 loss: -0.2604341506958008
Batch 44/64 loss: -0.26306581497192383
Batch 45/64 loss: -0.2642521262168884
Batch 46/64 loss: -0.2658761143684387
Batch 47/64 loss: -0.2611517906188965
Batch 48/64 loss: -0.24447143077850342
Batch 49/64 loss: -0.2762029767036438
Batch 50/64 loss: -0.2566251754760742
Batch 51/64 loss: -0.2659623622894287
Batch 52/64 loss: -0.2593653202056885
Batch 53/64 loss: -0.255124032497406
Batch 54/64 loss: -0.26550233364105225
Batch 55/64 loss: -0.23384130001068115
Batch 56/64 loss: -0.23106443881988525
Batch 57/64 loss: -0.26678532361984253
Batch 58/64 loss: -0.259005069732666
Batch 59/64 loss: -0.25894981622695923
Batch 60/64 loss: -0.2523968815803528
Batch 61/64 loss: -0.2493804693222046
Batch 62/64 loss: -0.2543656826019287
Batch 63/64 loss: -0.26000338792800903
Batch 64/64 loss: -0.25681614875793457
Epoch 167  Train loss: -0.2579277192845064  Val loss: -0.23612879457342664
Epoch 168
-------------------------------
Batch 1/64 loss: -0.2613067626953125
Batch 2/64 loss: -0.2655617594718933
Batch 3/64 loss: -0.26555299758911133
Batch 4/64 loss: -0.2765771746635437
Batch 5/64 loss: -0.273165225982666
Batch 6/64 loss: -0.2574375867843628
Batch 7/64 loss: -0.26599979400634766
Batch 8/64 loss: -0.24004226922988892
Batch 9/64 loss: -0.2654581069946289
Batch 10/64 loss: -0.268680214881897
Batch 11/64 loss: -0.25380927324295044
Batch 12/64 loss: -0.2558935880661011
Batch 13/64 loss: -0.2663103938102722
Batch 14/64 loss: -0.25882118940353394
Batch 15/64 loss: -0.25938260555267334
Batch 16/64 loss: -0.23233425617218018
Batch 17/64 loss: -0.24711239337921143
Batch 18/64 loss: -0.2563048005104065
Batch 19/64 loss: -0.2606205940246582
Batch 20/64 loss: -0.2550480365753174
Batch 21/64 loss: -0.27162784337997437
Batch 22/64 loss: -0.2580159902572632
Batch 23/64 loss: -0.2759547233581543
Batch 24/64 loss: -0.2705318331718445
Batch 25/64 loss: -0.26264750957489014
Batch 26/64 loss: -0.261857271194458
Batch 27/64 loss: -0.2767382860183716
Batch 28/64 loss: -0.26623010635375977
Batch 29/64 loss: -0.2546643018722534
Batch 30/64 loss: -0.26282578706741333
Batch 31/64 loss: -0.27853837609291077
Batch 32/64 loss: -0.240337073802948
Batch 33/64 loss: -0.23526078462600708
Batch 34/64 loss: -0.26142674684524536
Batch 35/64 loss: -0.2695252299308777
Batch 36/64 loss: -0.26545625925064087
Batch 37/64 loss: -0.26347100734710693
Batch 38/64 loss: -0.2524533271789551
Batch 39/64 loss: -0.2626669406890869
Batch 40/64 loss: -0.25888413190841675
Batch 41/64 loss: -0.24383044242858887
Batch 42/64 loss: -0.26233750581741333
Batch 43/64 loss: -0.2501353621482849
Batch 44/64 loss: -0.2517547607421875
Batch 45/64 loss: -0.2455536127090454
Batch 46/64 loss: -0.2559484839439392
Batch 47/64 loss: -0.25362420082092285
Batch 48/64 loss: -0.24612563848495483
Batch 49/64 loss: -0.26301252841949463
Batch 50/64 loss: -0.26043981313705444
Batch 51/64 loss: -0.2501208782196045
Batch 52/64 loss: -0.2633398771286011
Batch 53/64 loss: -0.2635265588760376
Batch 54/64 loss: -0.2726888656616211
Batch 55/64 loss: -0.2637217044830322
Batch 56/64 loss: -0.2528557777404785
Batch 57/64 loss: -0.24829965829849243
Batch 58/64 loss: -0.2575685977935791
Batch 59/64 loss: -0.26465904712677
Batch 60/64 loss: -0.26936256885528564
Batch 61/64 loss: -0.2661290764808655
Batch 62/64 loss: -0.2679694890975952
Batch 63/64 loss: -0.26313817501068115
Batch 64/64 loss: -0.2536330223083496
Epoch 168  Train loss: -0.2597788226370718  Val loss: -0.22604552884282114
Epoch 169
-------------------------------
Batch 1/64 loss: -0.27103322744369507
Batch 2/64 loss: -0.2650004029273987
Batch 3/64 loss: -0.25395405292510986
Batch 4/64 loss: -0.2715997099876404
Batch 5/64 loss: -0.2653099298477173
Batch 6/64 loss: -0.2431807518005371
Batch 7/64 loss: -0.2596060037612915
Batch 8/64 loss: -0.25989240407943726
Batch 9/64 loss: -0.25307613611221313
Batch 10/64 loss: -0.2520582675933838
Batch 11/64 loss: -0.27391523122787476
Batch 12/64 loss: -0.25612831115722656
Batch 13/64 loss: -0.25643378496170044
Batch 14/64 loss: -0.27053409814834595
Batch 15/64 loss: -0.2555985450744629
Batch 16/64 loss: -0.25598156452178955
Batch 17/64 loss: -0.2530651092529297
Batch 18/64 loss: -0.2626502513885498
Batch 19/64 loss: -0.25985288619995117
Batch 20/64 loss: -0.24756717681884766
Batch 21/64 loss: -0.25535517930984497
Batch 22/64 loss: -0.24855029582977295
Batch 23/64 loss: -0.2545693516731262
Batch 24/64 loss: -0.26290082931518555
Batch 25/64 loss: -0.27204430103302
Batch 26/64 loss: -0.26266777515411377
Batch 27/64 loss: -0.27316510677337646
Batch 28/64 loss: -0.2581649422645569
Batch 29/64 loss: -0.24475795030593872
Batch 30/64 loss: -0.2301180362701416
Batch 31/64 loss: -0.2752918004989624
Batch 32/64 loss: -0.260232150554657
Batch 33/64 loss: -0.23999518156051636
Batch 34/64 loss: -0.25346869230270386
Batch 35/64 loss: -0.2609856128692627
Batch 36/64 loss: -0.26776617765426636
Batch 37/64 loss: -0.2616516351699829
Batch 38/64 loss: -0.2396828532218933
Batch 39/64 loss: -0.24293208122253418
Batch 40/64 loss: -0.26341116428375244
Batch 41/64 loss: -0.2618509531021118
Batch 42/64 loss: -0.27212804555892944
Batch 43/64 loss: -0.2607552409172058
Batch 44/64 loss: -0.23898351192474365
Batch 45/64 loss: -0.2694305181503296
Batch 46/64 loss: -0.26613372564315796
Batch 47/64 loss: -0.2657756805419922
Batch 48/64 loss: -0.2600249648094177
Batch 49/64 loss: -0.24846172332763672
Batch 50/64 loss: -0.2695661783218384
Batch 51/64 loss: -0.2538248300552368
Batch 52/64 loss: -0.2647676467895508
Batch 53/64 loss: -0.24846696853637695
Batch 54/64 loss: -0.25698769092559814
Batch 55/64 loss: -0.25985032320022583
Batch 56/64 loss: -0.26487189531326294
Batch 57/64 loss: -0.2625730037689209
Batch 58/64 loss: -0.24729830026626587
Batch 59/64 loss: -0.2528145909309387
Batch 60/64 loss: -0.26021289825439453
Batch 61/64 loss: -0.25823134183883667
Batch 62/64 loss: -0.2576609253883362
Batch 63/64 loss: -0.2571846842765808
Batch 64/64 loss: -0.2651969790458679
Epoch 169  Train loss: -0.25836709550782744  Val loss: -0.23099893352010406
Epoch 170
-------------------------------
Batch 1/64 loss: -0.24526947736740112
Batch 2/64 loss: -0.2772817015647888
Batch 3/64 loss: -0.2712821960449219
Batch 4/64 loss: -0.265811562538147
Batch 5/64 loss: -0.24549907445907593
Batch 6/64 loss: -0.2596580982208252
Batch 7/64 loss: -0.25643640756607056
Batch 8/64 loss: -0.27602720260620117
Batch 9/64 loss: -0.2553097605705261
Batch 10/64 loss: -0.2616935968399048
Batch 11/64 loss: -0.2702316641807556
Batch 12/64 loss: -0.23960167169570923
Batch 13/64 loss: -0.2602355480194092
Batch 14/64 loss: -0.2670060396194458
Batch 15/64 loss: -0.27078908681869507
Batch 16/64 loss: -0.2512608766555786
Batch 17/64 loss: -0.25303053855895996
Batch 18/64 loss: -0.2746204137802124
Batch 19/64 loss: -0.25340503454208374
Batch 20/64 loss: -0.25178200006484985
Batch 21/64 loss: -0.2691584825515747
Batch 22/64 loss: -0.26235586404800415
Batch 23/64 loss: -0.2444295883178711
Batch 24/64 loss: -0.2591157555580139
Batch 25/64 loss: -0.2508280277252197
Batch 26/64 loss: -0.2467508316040039
Batch 27/64 loss: -0.26901471614837646
Batch 28/64 loss: -0.2580721974372864
Batch 29/64 loss: -0.26065123081207275
Batch 30/64 loss: -0.23460078239440918
Batch 31/64 loss: -0.2740805745124817
Batch 32/64 loss: -0.2450094223022461
Batch 33/64 loss: -0.2558751106262207
Batch 34/64 loss: -0.2515479326248169
Batch 35/64 loss: -0.26816171407699585
Batch 36/64 loss: -0.2574695944786072
Batch 37/64 loss: -0.25820738077163696
Batch 38/64 loss: -0.26907432079315186
Batch 39/64 loss: -0.26674652099609375
Batch 40/64 loss: -0.26117533445358276
Batch 41/64 loss: -0.2700278162956238
Batch 42/64 loss: -0.2566477656364441
Batch 43/64 loss: -0.26904118061065674
Batch 44/64 loss: -0.25397008657455444
Batch 45/64 loss: -0.24454104900360107
Batch 46/64 loss: -0.2605298161506653
Batch 47/64 loss: -0.2508354187011719
Batch 48/64 loss: -0.2765125632286072
Batch 49/64 loss: -0.2360450029373169
Batch 50/64 loss: -0.26465272903442383
Batch 51/64 loss: -0.2604571580886841
Batch 52/64 loss: -0.2509195804595947
Batch 53/64 loss: -0.264270544052124
Batch 54/64 loss: -0.2652495503425598
Batch 55/64 loss: -0.2746883034706116
Batch 56/64 loss: -0.24911260604858398
Batch 57/64 loss: -0.23719948530197144
Batch 58/64 loss: -0.2547065019607544
Batch 59/64 loss: -0.2653849124908447
Batch 60/64 loss: -0.2453474998474121
Batch 61/64 loss: -0.2600865960121155
Batch 62/64 loss: -0.25624215602874756
Batch 63/64 loss: -0.25338423252105713
Batch 64/64 loss: -0.26864171028137207
Epoch 170  Train loss: -0.2586649595522413  Val loss: -0.2157591904971198
Epoch 171
-------------------------------
Batch 1/64 loss: -0.2662743330001831
Batch 2/64 loss: -0.2545509338378906
Batch 3/64 loss: -0.27904003858566284
Batch 4/64 loss: -0.23598051071166992
Batch 5/64 loss: -0.2631097435951233
Batch 6/64 loss: -0.24880945682525635
Batch 7/64 loss: -0.26676827669143677
Batch 8/64 loss: -0.2551364302635193
Batch 9/64 loss: -0.25610554218292236
Batch 10/64 loss: -0.250487744808197
Batch 11/64 loss: -0.2536129951477051
Batch 12/64 loss: -0.27020561695098877
Batch 13/64 loss: -0.2637472152709961
Batch 14/64 loss: -0.25501590967178345
Batch 15/64 loss: -0.2384577989578247
Batch 16/64 loss: -0.266346275806427
Batch 17/64 loss: -0.2705382704734802
Batch 18/64 loss: -0.25470972061157227
Batch 19/64 loss: -0.27593958377838135
Batch 20/64 loss: -0.2671767473220825
Batch 21/64 loss: -0.2590581774711609
Batch 22/64 loss: -0.26090359687805176
Batch 23/64 loss: -0.2603127956390381
Batch 24/64 loss: -0.2642415761947632
Batch 25/64 loss: -0.24702608585357666
Batch 26/64 loss: -0.2706339359283447
Batch 27/64 loss: -0.2641564607620239
Batch 28/64 loss: -0.24777376651763916
Batch 29/64 loss: -0.2690204381942749
Batch 30/64 loss: -0.27946051955223083
Batch 31/64 loss: -0.24651920795440674
Batch 32/64 loss: -0.259937584400177
Batch 33/64 loss: -0.27017277479171753
Batch 34/64 loss: -0.26558661460876465
Batch 35/64 loss: -0.24825096130371094
Batch 36/64 loss: -0.26000553369522095
Batch 37/64 loss: -0.2679562568664551
Batch 38/64 loss: -0.2521148920059204
Batch 39/64 loss: -0.24862664937973022
Batch 40/64 loss: -0.24008172750473022
Batch 41/64 loss: -0.26346898078918457
Batch 42/64 loss: -0.24450910091400146
Batch 43/64 loss: -0.2570269703865051
Batch 44/64 loss: -0.24304908514022827
Batch 45/64 loss: -0.2673330307006836
Batch 46/64 loss: -0.26896005868911743
Batch 47/64 loss: -0.26041269302368164
Batch 48/64 loss: -0.2700319290161133
Batch 49/64 loss: -0.2546663284301758
Batch 50/64 loss: -0.24915659427642822
Batch 51/64 loss: -0.2490786910057068
Batch 52/64 loss: -0.2493267059326172
Batch 53/64 loss: -0.266981840133667
Batch 54/64 loss: -0.26056933403015137
Batch 55/64 loss: -0.25721949338912964
Batch 56/64 loss: -0.25853151082992554
Batch 57/64 loss: -0.26140207052230835
Batch 58/64 loss: -0.2791164517402649
Batch 59/64 loss: -0.2522449493408203
Batch 60/64 loss: -0.26899564266204834
Batch 61/64 loss: -0.2657153010368347
Batch 62/64 loss: -0.23943793773651123
Batch 63/64 loss: -0.24791741371154785
Batch 64/64 loss: -0.2588185667991638
Epoch 171  Train loss: -0.25887245080050303  Val loss: -0.22913532732278621
Epoch 172
-------------------------------
Batch 1/64 loss: -0.25620216131210327
Batch 2/64 loss: -0.2612910270690918
Batch 3/64 loss: -0.24001502990722656
Batch 4/64 loss: -0.25606095790863037
Batch 5/64 loss: -0.25354743003845215
Batch 6/64 loss: -0.2679761052131653
Batch 7/64 loss: -0.2712099552154541
Batch 8/64 loss: -0.26367026567459106
Batch 9/64 loss: -0.2664608359336853
Batch 10/64 loss: -0.26128411293029785
Batch 11/64 loss: -0.25184476375579834
Batch 12/64 loss: -0.24787628650665283
Batch 13/64 loss: -0.2585892677307129
Batch 14/64 loss: -0.25274789333343506
Batch 15/64 loss: -0.2507728338241577
Batch 16/64 loss: -0.2646915316581726
Batch 17/64 loss: -0.26129376888275146
Batch 18/64 loss: -0.25865620374679565
Batch 19/64 loss: -0.2596648931503296
Batch 20/64 loss: -0.2633318305015564
Batch 21/64 loss: -0.26210081577301025
Batch 22/64 loss: -0.24632418155670166
Batch 23/64 loss: -0.26450443267822266
Batch 24/64 loss: -0.2657802104949951
Batch 25/64 loss: -0.2542440891265869
Batch 26/64 loss: -0.25611186027526855
Batch 27/64 loss: -0.2576388716697693
Batch 28/64 loss: -0.2543265223503113
Batch 29/64 loss: -0.26340681314468384
Batch 30/64 loss: -0.2585527300834656
Batch 31/64 loss: -0.25729846954345703
Batch 32/64 loss: -0.2694300413131714
Batch 33/64 loss: -0.2582339644432068
Batch 34/64 loss: -0.2576674818992615
Batch 35/64 loss: -0.24377423524856567
Batch 36/64 loss: -0.2626703977584839
Batch 37/64 loss: -0.2612643837928772
Batch 38/64 loss: -0.2601170539855957
Batch 39/64 loss: -0.25262296199798584
Batch 40/64 loss: -0.26568901538848877
Batch 41/64 loss: -0.26221001148223877
Batch 42/64 loss: -0.2562353014945984
Batch 43/64 loss: -0.24708741903305054
Batch 44/64 loss: -0.2600439786911011
Batch 45/64 loss: -0.24879246950149536
Batch 46/64 loss: -0.24377566576004028
Batch 47/64 loss: -0.23786914348602295
Batch 48/64 loss: -0.2552887797355652
Batch 49/64 loss: -0.2617630362510681
Batch 50/64 loss: -0.2561076283454895
Batch 51/64 loss: -0.26247596740722656
Batch 52/64 loss: -0.26259946823120117
Batch 53/64 loss: -0.27084892988204956
Batch 54/64 loss: -0.25593048334121704
Batch 55/64 loss: -0.25437700748443604
Batch 56/64 loss: -0.24573445320129395
Batch 57/64 loss: -0.254976749420166
Batch 58/64 loss: -0.2468159794807434
Batch 59/64 loss: -0.24543702602386475
Batch 60/64 loss: -0.25892627239227295
Batch 61/64 loss: -0.2442752718925476
Batch 62/64 loss: -0.2501789331436157
Batch 63/64 loss: -0.2657410502433777
Batch 64/64 loss: -0.253818154335022
Epoch 172  Train loss: -0.25689098554499007  Val loss: -0.2238261503042634
Epoch 173
-------------------------------
Batch 1/64 loss: -0.26558077335357666
Batch 2/64 loss: -0.26486122608184814
Batch 3/64 loss: -0.26119017601013184
Batch 4/64 loss: -0.2677265405654907
Batch 5/64 loss: -0.24682921171188354
Batch 6/64 loss: -0.25100499391555786
Batch 7/64 loss: -0.25163692235946655
Batch 8/64 loss: -0.2513939142227173
Batch 9/64 loss: -0.24492913484573364
Batch 10/64 loss: -0.26114779710769653
Batch 11/64 loss: -0.26235830783843994
Batch 12/64 loss: -0.2565634250640869
Batch 13/64 loss: -0.26019352674484253
Batch 14/64 loss: -0.2636697292327881
Batch 15/64 loss: -0.26937586069107056
Batch 16/64 loss: -0.2610477805137634
Batch 17/64 loss: -0.2581480145454407
Batch 18/64 loss: -0.2721647024154663
Batch 19/64 loss: -0.25884515047073364
Batch 20/64 loss: -0.2423723340034485
Batch 21/64 loss: -0.25299447774887085
Batch 22/64 loss: -0.2698514461517334
Batch 23/64 loss: -0.25991618633270264
Batch 24/64 loss: -0.26883816719055176
Batch 25/64 loss: -0.2601921558380127
Batch 26/64 loss: -0.2463955283164978
Batch 27/64 loss: -0.26205629110336304
Batch 28/64 loss: -0.25453299283981323
Batch 29/64 loss: -0.2649332284927368
Batch 30/64 loss: -0.2573457956314087
Batch 31/64 loss: -0.23959213495254517
Batch 32/64 loss: -0.24785882234573364
Batch 33/64 loss: -0.2493349313735962
Batch 34/64 loss: -0.24387085437774658
Batch 35/64 loss: -0.24998313188552856
Batch 36/64 loss: -0.26906728744506836
Batch 37/64 loss: -0.2572000026702881
Batch 38/64 loss: -0.25731945037841797
Batch 39/64 loss: -0.2648303508758545
Batch 40/64 loss: -0.2455432415008545
Batch 41/64 loss: -0.2491544485092163
Batch 42/64 loss: -0.26164281368255615
Batch 43/64 loss: -0.262764036655426
Batch 44/64 loss: -0.2530490756034851
Batch 45/64 loss: -0.25176429748535156
Batch 46/64 loss: -0.2604185938835144
Batch 47/64 loss: -0.2611599564552307
Batch 48/64 loss: -0.24907690286636353
Batch 49/64 loss: -0.27452731132507324
Batch 50/64 loss: -0.26003819704055786
Batch 51/64 loss: -0.26087623834609985
Batch 52/64 loss: -0.25281625986099243
Batch 53/64 loss: -0.2581138610839844
Batch 54/64 loss: -0.26379477977752686
Batch 55/64 loss: -0.2655564546585083
Batch 56/64 loss: -0.26560747623443604
Batch 57/64 loss: -0.2532476782798767
Batch 58/64 loss: -0.2341199517250061
Batch 59/64 loss: -0.25436121225357056
Batch 60/64 loss: -0.2733888626098633
Batch 61/64 loss: -0.260287344455719
Batch 62/64 loss: -0.2662491798400879
Batch 63/64 loss: -0.2576090097427368
Batch 64/64 loss: -0.25020653009414673
Epoch 173  Train loss: -0.25772509551515765  Val loss: -0.23228370324033232
Epoch 174
-------------------------------
Batch 1/64 loss: -0.2506222724914551
Batch 2/64 loss: -0.2450547218322754
Batch 3/64 loss: -0.26664257049560547
Batch 4/64 loss: -0.2649405002593994
Batch 5/64 loss: -0.2607850432395935
Batch 6/64 loss: -0.2621345520019531
Batch 7/64 loss: -0.2412867546081543
Batch 8/64 loss: -0.26215964555740356
Batch 9/64 loss: -0.2616826295852661
Batch 10/64 loss: -0.25539302825927734
Batch 11/64 loss: -0.26148509979248047
Batch 12/64 loss: -0.26505333185195923
Batch 13/64 loss: -0.24915605783462524
Batch 14/64 loss: -0.2687492370605469
Batch 15/64 loss: -0.25533872842788696
Batch 16/64 loss: -0.2666500210762024
Batch 17/64 loss: -0.2739250063896179
Batch 18/64 loss: -0.2442563772201538
Batch 19/64 loss: -0.2768440246582031
Batch 20/64 loss: -0.26857221126556396
Batch 21/64 loss: -0.25816208124160767
Batch 22/64 loss: -0.2501704692840576
Batch 23/64 loss: -0.2599020004272461
Batch 24/64 loss: -0.24456799030303955
Batch 25/64 loss: -0.2707347273826599
Batch 26/64 loss: -0.24045872688293457
Batch 27/64 loss: -0.2556830644607544
Batch 28/64 loss: -0.268760621547699
Batch 29/64 loss: -0.25827616453170776
Batch 30/64 loss: -0.2519261837005615
Batch 31/64 loss: -0.2425384521484375
Batch 32/64 loss: -0.26043009757995605
Batch 33/64 loss: -0.2498602271080017
Batch 34/64 loss: -0.2545047998428345
Batch 35/64 loss: -0.26327234506607056
Batch 36/64 loss: -0.25265079736709595
Batch 37/64 loss: -0.26153111457824707
Batch 38/64 loss: -0.2632477879524231
Batch 39/64 loss: -0.2510017156600952
Batch 40/64 loss: -0.2522333860397339
Batch 41/64 loss: -0.2568645477294922
Batch 42/64 loss: -0.24653494358062744
Batch 43/64 loss: -0.24427783489227295
Batch 44/64 loss: -0.27350497245788574
Batch 45/64 loss: -0.2645326256752014
Batch 46/64 loss: -0.2685224413871765
Batch 47/64 loss: -0.2549726963043213
Batch 48/64 loss: -0.2576310634613037
Batch 49/64 loss: -0.2559853792190552
Batch 50/64 loss: -0.2479008436203003
Batch 51/64 loss: -0.26155394315719604
Batch 52/64 loss: -0.2718948721885681
Batch 53/64 loss: -0.24961143732070923
Batch 54/64 loss: -0.2584238648414612
Batch 55/64 loss: -0.2584373354911804
Batch 56/64 loss: -0.27136898040771484
Batch 57/64 loss: -0.2558397054672241
Batch 58/64 loss: -0.28002333641052246
Batch 59/64 loss: -0.26080507040023804
Batch 60/64 loss: -0.2681923508644104
Batch 61/64 loss: -0.2564329504966736
Batch 62/64 loss: -0.26453226804733276
Batch 63/64 loss: -0.2503954768180847
Batch 64/64 loss: -0.25833749771118164
Epoch 174  Train loss: -0.25855116283192353  Val loss: -0.2328502584978477
Epoch 175
-------------------------------
Batch 1/64 loss: -0.25470101833343506
Batch 2/64 loss: -0.2679917812347412
Batch 3/64 loss: -0.25944632291793823
Batch 4/64 loss: -0.26160478591918945
Batch 5/64 loss: -0.24862676858901978
Batch 6/64 loss: -0.2565321922302246
Batch 7/64 loss: -0.2552575469017029
Batch 8/64 loss: -0.272377073764801
Batch 9/64 loss: -0.260550320148468
Batch 10/64 loss: -0.26678210496902466
Batch 11/64 loss: -0.25098997354507446
Batch 12/64 loss: -0.2695106267929077
Batch 13/64 loss: -0.27319979667663574
Batch 14/64 loss: -0.2708666920661926
Batch 15/64 loss: -0.2723711133003235
Batch 16/64 loss: -0.2798788547515869
Batch 17/64 loss: -0.2699158787727356
Batch 18/64 loss: -0.27390754222869873
Batch 19/64 loss: -0.25227826833724976
Batch 20/64 loss: -0.26998603343963623
Batch 21/64 loss: -0.27547651529312134
Batch 22/64 loss: -0.2666943073272705
Batch 23/64 loss: -0.25832027196884155
Batch 24/64 loss: -0.27332478761672974
Batch 25/64 loss: -0.2710403800010681
Batch 26/64 loss: -0.24825561046600342
Batch 27/64 loss: -0.2675785422325134
Batch 28/64 loss: -0.27686870098114014
Batch 29/64 loss: -0.24918383359909058
Batch 30/64 loss: -0.2708359956741333
Batch 31/64 loss: -0.25845158100128174
Batch 32/64 loss: -0.2614935636520386
Batch 33/64 loss: -0.2709364891052246
Batch 34/64 loss: -0.2670785188674927
Batch 35/64 loss: -0.2379213571548462
Batch 36/64 loss: -0.25676947832107544
Batch 37/64 loss: -0.24858146905899048
Batch 38/64 loss: -0.27090126276016235
Batch 39/64 loss: -0.2517096996307373
Batch 40/64 loss: -0.2483331561088562
Batch 41/64 loss: -0.2665729522705078
Batch 42/64 loss: -0.26286351680755615
Batch 43/64 loss: -0.26201581954956055
Batch 44/64 loss: -0.26695603132247925
Batch 45/64 loss: -0.26532429456710815
Batch 46/64 loss: -0.28038525581359863
Batch 47/64 loss: -0.25681692361831665
Batch 48/64 loss: -0.2667727470397949
Batch 49/64 loss: -0.25158411264419556
Batch 50/64 loss: -0.2851320207118988
Batch 51/64 loss: -0.24869447946548462
Batch 52/64 loss: -0.26345133781433105
Batch 53/64 loss: -0.2684730291366577
Batch 54/64 loss: -0.2615776062011719
Batch 55/64 loss: -0.2688519358634949
Batch 56/64 loss: -0.2719617486000061
Batch 57/64 loss: -0.2774236500263214
Batch 58/64 loss: -0.24878758192062378
Batch 59/64 loss: -0.26861512660980225
Batch 60/64 loss: -0.26081621646881104
Batch 61/64 loss: -0.24450039863586426
Batch 62/64 loss: -0.2620859146118164
Batch 63/64 loss: -0.26174992322921753
Batch 64/64 loss: -0.27441269159317017
Epoch 175  Train loss: -0.26343140952727373  Val loss: -0.23535352003123752
Epoch 176
-------------------------------
Batch 1/64 loss: -0.2551840543746948
Batch 2/64 loss: -0.260189414024353
Batch 3/64 loss: -0.2535110116004944
Batch 4/64 loss: -0.24108856916427612
Batch 5/64 loss: -0.26412278413772583
Batch 6/64 loss: -0.2739582657814026
Batch 7/64 loss: -0.2714475393295288
Batch 8/64 loss: -0.24221199750900269
Batch 9/64 loss: -0.26630669832229614
Batch 10/64 loss: -0.23632866144180298
Batch 11/64 loss: -0.25048959255218506
Batch 12/64 loss: -0.26222455501556396
Batch 13/64 loss: -0.2818065881729126
Batch 14/64 loss: -0.2720201015472412
Batch 15/64 loss: -0.2683231234550476
Batch 16/64 loss: -0.25532066822052
Batch 17/64 loss: -0.2871330976486206
Batch 18/64 loss: -0.2600024938583374
Batch 19/64 loss: -0.2635939121246338
Batch 20/64 loss: -0.2498374581336975
Batch 21/64 loss: -0.2626083493232727
Batch 22/64 loss: -0.2694357633590698
Batch 23/64 loss: -0.2608789801597595
Batch 24/64 loss: -0.2568286061286926
Batch 25/64 loss: -0.2690098285675049
Batch 26/64 loss: -0.24973320960998535
Batch 27/64 loss: -0.27251845598220825
Batch 28/64 loss: -0.25353801250457764
Batch 29/64 loss: -0.25986939668655396
Batch 30/64 loss: -0.24819135665893555
Batch 31/64 loss: -0.24742352962493896
Batch 32/64 loss: -0.2564097046852112
Batch 33/64 loss: -0.24819809198379517
Batch 34/64 loss: -0.24870336055755615
Batch 35/64 loss: -0.2478424310684204
Batch 36/64 loss: -0.24616336822509766
Batch 37/64 loss: -0.2503170967102051
Batch 38/64 loss: -0.25006240606307983
Batch 39/64 loss: -0.25825923681259155
Batch 40/64 loss: -0.25273680686950684
Batch 41/64 loss: -0.24770253896713257
Batch 42/64 loss: -0.2539941668510437
Batch 43/64 loss: -0.2515460252761841
Batch 44/64 loss: -0.2465595006942749
Batch 45/64 loss: -0.2454385757446289
Batch 46/64 loss: -0.24323582649230957
Batch 47/64 loss: -0.24236053228378296
Batch 48/64 loss: -0.24792397022247314
Batch 49/64 loss: -0.2719973921775818
Batch 50/64 loss: -0.25904691219329834
Batch 51/64 loss: -0.2694789171218872
Batch 52/64 loss: -0.24967312812805176
Batch 53/64 loss: -0.24867314100265503
Batch 54/64 loss: -0.22549575567245483
Batch 55/64 loss: -0.2544245719909668
Batch 56/64 loss: -0.24821007251739502
Batch 57/64 loss: -0.2621626853942871
Batch 58/64 loss: -0.23989850282669067
Batch 59/64 loss: -0.2565128803253174
Batch 60/64 loss: -0.2743692994117737
Batch 61/64 loss: -0.26944196224212646
Batch 62/64 loss: -0.2503795027732849
Batch 63/64 loss: -0.2640848755836487
Batch 64/64 loss: -0.24818360805511475
Epoch 176  Train loss: -0.2561972865871355  Val loss: -0.20570536843689857
Epoch 177
-------------------------------
Batch 1/64 loss: -0.2578303813934326
Batch 2/64 loss: -0.24344587326049805
Batch 3/64 loss: -0.2677009105682373
Batch 4/64 loss: -0.2605903148651123
Batch 5/64 loss: -0.23185455799102783
Batch 6/64 loss: -0.26328957080841064
Batch 7/64 loss: -0.24888908863067627
Batch 8/64 loss: -0.2563248872756958
Batch 9/64 loss: -0.2767970561981201
Batch 10/64 loss: -0.2612428665161133
Batch 11/64 loss: -0.2683500051498413
Batch 12/64 loss: -0.2700883150100708
Batch 13/64 loss: -0.2504124045372009
Batch 14/64 loss: -0.2510952353477478
Batch 15/64 loss: -0.25941479206085205
Batch 16/64 loss: -0.25864869356155396
Batch 17/64 loss: -0.2502812147140503
Batch 18/64 loss: -0.26333045959472656
Batch 19/64 loss: -0.23894858360290527
Batch 20/64 loss: -0.22787928581237793
Batch 21/64 loss: -0.2620304226875305
Batch 22/64 loss: -0.2553892135620117
Batch 23/64 loss: -0.2627091407775879
Batch 24/64 loss: -0.24838149547576904
Batch 25/64 loss: -0.2607715129852295
Batch 26/64 loss: -0.2663334608078003
Batch 27/64 loss: -0.24162554740905762
Batch 28/64 loss: -0.26643431186676025
Batch 29/64 loss: -0.256322979927063
Batch 30/64 loss: -0.2511499524116516
Batch 31/64 loss: -0.2438441514968872
Batch 32/64 loss: -0.2582625150680542
Batch 33/64 loss: -0.25099754333496094
Batch 34/64 loss: -0.270912230014801
Batch 35/64 loss: -0.26643985509872437
Batch 36/64 loss: -0.24616235494613647
Batch 37/64 loss: -0.24139761924743652
Batch 38/64 loss: -0.2582250237464905
Batch 39/64 loss: -0.27206605672836304
Batch 40/64 loss: -0.2526841163635254
Batch 41/64 loss: -0.26460689306259155
Batch 42/64 loss: -0.26885873079299927
Batch 43/64 loss: -0.2655702233314514
Batch 44/64 loss: -0.2472386360168457
Batch 45/64 loss: -0.26712435483932495
Batch 46/64 loss: -0.252971351146698
Batch 47/64 loss: -0.24772900342941284
Batch 48/64 loss: -0.26335573196411133
Batch 49/64 loss: -0.23025918006896973
Batch 50/64 loss: -0.24951684474945068
Batch 51/64 loss: -0.2614070177078247
Batch 52/64 loss: -0.2392812967300415
Batch 53/64 loss: -0.23437529802322388
Batch 54/64 loss: -0.25993919372558594
Batch 55/64 loss: -0.2358260154724121
Batch 56/64 loss: -0.24089878797531128
Batch 57/64 loss: -0.2344515323638916
Batch 58/64 loss: -0.2319490909576416
Batch 59/64 loss: -0.23233717679977417
Batch 60/64 loss: -0.22358077764511108
Batch 61/64 loss: -0.25496888160705566
Batch 62/64 loss: -0.2558252215385437
Batch 63/64 loss: -0.243172287940979
Batch 64/64 loss: -0.24613064527511597
Epoch 177  Train loss: -0.25299443941490324  Val loss: -0.21515617890865943
Epoch 178
-------------------------------
Batch 1/64 loss: -0.25565773248672485
Batch 2/64 loss: -0.2584049701690674
Batch 3/64 loss: -0.25217580795288086
Batch 4/64 loss: -0.2519266605377197
Batch 5/64 loss: -0.25497007369995117
Batch 6/64 loss: -0.26121532917022705
Batch 7/64 loss: -0.2346121072769165
Batch 8/64 loss: -0.2437422275543213
Batch 9/64 loss: -0.24957704544067383
Batch 10/64 loss: -0.24774527549743652
Batch 11/64 loss: -0.24530333280563354
Batch 12/64 loss: -0.24851447343826294
Batch 13/64 loss: -0.2596293091773987
Batch 14/64 loss: -0.254691481590271
Batch 15/64 loss: -0.2597910761833191
Batch 16/64 loss: -0.2600204348564148
Batch 17/64 loss: -0.259670615196228
Batch 18/64 loss: -0.27002084255218506
Batch 19/64 loss: -0.20914697647094727
Batch 20/64 loss: -0.24334704875946045
Batch 21/64 loss: -0.264953076839447
Batch 22/64 loss: -0.23024815320968628
Batch 23/64 loss: -0.2617166042327881
Batch 24/64 loss: -0.24511516094207764
Batch 25/64 loss: -0.25546908378601074
Batch 26/64 loss: -0.25788426399230957
Batch 27/64 loss: -0.24815893173217773
Batch 28/64 loss: -0.2634916305541992
Batch 29/64 loss: -0.2637619972229004
Batch 30/64 loss: -0.22967535257339478
Batch 31/64 loss: -0.2643281817436218
Batch 32/64 loss: -0.2614443302154541
Batch 33/64 loss: -0.24595898389816284
Batch 34/64 loss: -0.2649692893028259
Batch 35/64 loss: -0.25840723514556885
Batch 36/64 loss: -0.2709938883781433
Batch 37/64 loss: -0.26576632261276245
Batch 38/64 loss: -0.23335760831832886
Batch 39/64 loss: -0.25938665866851807
Batch 40/64 loss: -0.27928537130355835
Batch 41/64 loss: -0.26688969135284424
Batch 42/64 loss: -0.25085926055908203
Batch 43/64 loss: -0.253517746925354
Batch 44/64 loss: -0.26199495792388916
Batch 45/64 loss: -0.25863462686538696
Batch 46/64 loss: -0.24770087003707886
Batch 47/64 loss: -0.25631386041641235
Batch 48/64 loss: -0.280051052570343
Batch 49/64 loss: -0.23626309633255005
Batch 50/64 loss: -0.2594088912010193
Batch 51/64 loss: -0.2475690245628357
Batch 52/64 loss: -0.2423255443572998
Batch 53/64 loss: -0.2540816068649292
Batch 54/64 loss: -0.25517791509628296
Batch 55/64 loss: -0.2514614462852478
Batch 56/64 loss: -0.25774550437927246
Batch 57/64 loss: -0.25451040267944336
Batch 58/64 loss: -0.2627946138381958
Batch 59/64 loss: -0.2720212936401367
Batch 60/64 loss: -0.270733118057251
Batch 61/64 loss: -0.26390981674194336
Batch 62/64 loss: -0.2498587965965271
Batch 63/64 loss: -0.24907642602920532
Batch 64/64 loss: -0.2573370933532715
Epoch 178  Train loss: -0.25475195809906603  Val loss: -0.2294787735873481
Epoch 179
-------------------------------
Batch 1/64 loss: -0.2510746121406555
Batch 2/64 loss: -0.25025033950805664
Batch 3/64 loss: -0.2556493282318115
Batch 4/64 loss: -0.23850220441818237
Batch 5/64 loss: -0.26356053352355957
Batch 6/64 loss: -0.2563822269439697
Batch 7/64 loss: -0.27533113956451416
Batch 8/64 loss: -0.25832599401474
Batch 9/64 loss: -0.25781047344207764
Batch 10/64 loss: -0.2650200128555298
Batch 11/64 loss: -0.26264292001724243
Batch 12/64 loss: -0.272908091545105
Batch 13/64 loss: -0.2723480463027954
Batch 14/64 loss: -0.2780059576034546
Batch 15/64 loss: -0.2661427855491638
Batch 16/64 loss: -0.2597905993461609
Batch 17/64 loss: -0.2749420404434204
Batch 18/64 loss: -0.2690289616584778
Batch 19/64 loss: -0.2509431838989258
Batch 20/64 loss: -0.23917323350906372
Batch 21/64 loss: -0.27041447162628174
Batch 22/64 loss: -0.26175957918167114
Batch 23/64 loss: -0.2537463307380676
Batch 24/64 loss: -0.25828641653060913
Batch 25/64 loss: -0.24931758642196655
Batch 26/64 loss: -0.26916933059692383
Batch 27/64 loss: -0.2395927906036377
Batch 28/64 loss: -0.2636168599128723
Batch 29/64 loss: -0.2632477879524231
Batch 30/64 loss: -0.2602716088294983
Batch 31/64 loss: -0.2520332336425781
Batch 32/64 loss: -0.24925947189331055
Batch 33/64 loss: -0.26467251777648926
Batch 34/64 loss: -0.2415294051170349
Batch 35/64 loss: -0.25247591733932495
Batch 36/64 loss: -0.2620546817779541
Batch 37/64 loss: -0.2557169795036316
Batch 38/64 loss: -0.2645752429962158
Batch 39/64 loss: -0.2545791268348694
Batch 40/64 loss: -0.2575814127922058
Batch 41/64 loss: -0.2767831087112427
Batch 42/64 loss: -0.25566160678863525
Batch 43/64 loss: -0.2618424892425537
Batch 44/64 loss: -0.22593927383422852
Batch 45/64 loss: -0.2544214725494385
Batch 46/64 loss: -0.24300628900527954
Batch 47/64 loss: -0.25869935750961304
Batch 48/64 loss: -0.2598186731338501
Batch 49/64 loss: -0.2656160593032837
Batch 50/64 loss: -0.25636303424835205
Batch 51/64 loss: -0.270080029964447
Batch 52/64 loss: -0.2636450529098511
Batch 53/64 loss: -0.25422024726867676
Batch 54/64 loss: -0.24395036697387695
Batch 55/64 loss: -0.25535911321640015
Batch 56/64 loss: -0.2415410280227661
Batch 57/64 loss: -0.2507474422454834
Batch 58/64 loss: -0.26046180725097656
Batch 59/64 loss: -0.26313352584838867
Batch 60/64 loss: -0.21227401494979858
Batch 61/64 loss: -0.24629241228103638
Batch 62/64 loss: -0.26674985885620117
Batch 63/64 loss: -0.2633028030395508
Batch 64/64 loss: -0.2671571373939514
Epoch 179  Train loss: -0.2574432997142567  Val loss: -0.2276409139338228
Epoch 180
-------------------------------
Batch 1/64 loss: -0.2458195686340332
Batch 2/64 loss: -0.26065850257873535
Batch 3/64 loss: -0.25388801097869873
Batch 4/64 loss: -0.2693546414375305
Batch 5/64 loss: -0.2665463089942932
Batch 6/64 loss: -0.25104206800460815
Batch 7/64 loss: -0.27407318353652954
Batch 8/64 loss: -0.2717365622520447
Batch 9/64 loss: -0.2593894600868225
Batch 10/64 loss: -0.2516714334487915
Batch 11/64 loss: -0.25639772415161133
Batch 12/64 loss: -0.26180773973464966
Batch 13/64 loss: -0.25466787815093994
Batch 14/64 loss: -0.2621726393699646
Batch 15/64 loss: -0.26689058542251587
Batch 16/64 loss: -0.2589925527572632
Batch 17/64 loss: -0.26315176486968994
Batch 18/64 loss: -0.2839920222759247
Batch 19/64 loss: -0.24913567304611206
Batch 20/64 loss: -0.26618921756744385
Batch 21/64 loss: -0.2429746389389038
Batch 22/64 loss: -0.2524358034133911
Batch 23/64 loss: -0.24488627910614014
Batch 24/64 loss: -0.24886822700500488
Batch 25/64 loss: -0.2548254132270813
Batch 26/64 loss: -0.26183021068573
Batch 27/64 loss: -0.23232638835906982
Batch 28/64 loss: -0.2434825897216797
Batch 29/64 loss: -0.25411027669906616
Batch 30/64 loss: -0.2638614773750305
Batch 31/64 loss: -0.25633710622787476
Batch 32/64 loss: -0.26351892948150635
Batch 33/64 loss: -0.2557259202003479
Batch 34/64 loss: -0.24689507484436035
Batch 35/64 loss: -0.25520312786102295
Batch 36/64 loss: -0.2733025550842285
Batch 37/64 loss: -0.24913311004638672
Batch 38/64 loss: -0.2588195204734802
Batch 39/64 loss: -0.2572266459465027
Batch 40/64 loss: -0.25382256507873535
Batch 41/64 loss: -0.2724737524986267
Batch 42/64 loss: -0.2693192958831787
Batch 43/64 loss: -0.2583792209625244
Batch 44/64 loss: -0.2709338068962097
Batch 45/64 loss: -0.2642509341239929
Batch 46/64 loss: -0.26717090606689453
Batch 47/64 loss: -0.25078487396240234
Batch 48/64 loss: -0.25358259677886963
Batch 49/64 loss: -0.2616175413131714
Batch 50/64 loss: -0.22328627109527588
Batch 51/64 loss: -0.25287485122680664
Batch 52/64 loss: -0.24307531118392944
Batch 53/64 loss: -0.24924105405807495
Batch 54/64 loss: -0.2623720169067383
Batch 55/64 loss: -0.2347317337989807
Batch 56/64 loss: -0.2607377767562866
Batch 57/64 loss: -0.26947855949401855
Batch 58/64 loss: -0.2532203197479248
Batch 59/64 loss: -0.24267899990081787
Batch 60/64 loss: -0.2656230330467224
Batch 61/64 loss: -0.24897754192352295
Batch 62/64 loss: -0.2633312940597534
Batch 63/64 loss: -0.23154932260513306
Batch 64/64 loss: -0.2504720091819763
Epoch 180  Train loss: -0.2565444457764719  Val loss: -0.2305868982859084
Epoch 181
-------------------------------
Batch 1/64 loss: -0.26213473081588745
Batch 2/64 loss: -0.27685844898223877
Batch 3/64 loss: -0.23955798149108887
Batch 4/64 loss: -0.2551723122596741
Batch 5/64 loss: -0.2657250165939331
Batch 6/64 loss: -0.2646533250808716
Batch 7/64 loss: -0.2627881169319153
Batch 8/64 loss: -0.2538447380065918
Batch 9/64 loss: -0.24255573749542236
Batch 10/64 loss: -0.2545759081840515
Batch 11/64 loss: -0.2513156533241272
Batch 12/64 loss: -0.263161838054657
Batch 13/64 loss: -0.26132315397262573
Batch 14/64 loss: -0.2604864835739136
Batch 15/64 loss: -0.25345373153686523
Batch 16/64 loss: -0.26580530405044556
Batch 17/64 loss: -0.25728821754455566
Batch 18/64 loss: -0.2521733045578003
Batch 19/64 loss: -0.2679840922355652
Batch 20/64 loss: -0.24795210361480713
Batch 21/64 loss: -0.24510884284973145
Batch 22/64 loss: -0.255301833152771
Batch 23/64 loss: -0.2542980909347534
Batch 24/64 loss: -0.2633306384086609
Batch 25/64 loss: -0.2539817690849304
Batch 26/64 loss: -0.2627675533294678
Batch 27/64 loss: -0.2579540014266968
Batch 28/64 loss: -0.25253134965896606
Batch 29/64 loss: -0.25150740146636963
Batch 30/64 loss: -0.2582852244377136
Batch 31/64 loss: -0.26390260457992554
Batch 32/64 loss: -0.24858081340789795
Batch 33/64 loss: -0.2712005376815796
Batch 34/64 loss: -0.25155532360076904
Batch 35/64 loss: -0.2498490810394287
Batch 36/64 loss: -0.26605963706970215
Batch 37/64 loss: -0.257310152053833
Batch 38/64 loss: -0.24011510610580444
Batch 39/64 loss: -0.25018441677093506
Batch 40/64 loss: -0.25679004192352295
Batch 41/64 loss: -0.2541855573654175
Batch 42/64 loss: -0.268446147441864
Batch 43/64 loss: -0.2668876647949219
Batch 44/64 loss: -0.2496798038482666
Batch 45/64 loss: -0.2566244602203369
Batch 46/64 loss: -0.23469531536102295
Batch 47/64 loss: -0.26004958152770996
Batch 48/64 loss: -0.2571430802345276
Batch 49/64 loss: -0.26294493675231934
Batch 50/64 loss: -0.26771968603134155
Batch 51/64 loss: -0.27221915125846863
Batch 52/64 loss: -0.27081263065338135
Batch 53/64 loss: -0.246859610080719
Batch 54/64 loss: -0.27357423305511475
Batch 55/64 loss: -0.26369863748550415
Batch 56/64 loss: -0.2524818778038025
Batch 57/64 loss: -0.2472519874572754
Batch 58/64 loss: -0.2546202540397644
Batch 59/64 loss: -0.2559015154838562
Batch 60/64 loss: -0.25137561559677124
Batch 61/64 loss: -0.25874990224838257
Batch 62/64 loss: -0.2500340938568115
Batch 63/64 loss: -0.2577863931655884
Batch 64/64 loss: -0.2650192379951477
Epoch 181  Train loss: -0.2572851950047063  Val loss: -0.22808312693822017
Epoch 182
-------------------------------
Batch 1/64 loss: -0.24868559837341309
Batch 2/64 loss: -0.24088573455810547
Batch 3/64 loss: -0.24634712934494019
Batch 4/64 loss: -0.24247026443481445
Batch 5/64 loss: -0.23906809091567993
Batch 6/64 loss: -0.23859673738479614
Batch 7/64 loss: -0.2585561275482178
Batch 8/64 loss: -0.2233954668045044
Batch 9/64 loss: -0.22709238529205322
Batch 10/64 loss: -0.2540454864501953
Batch 11/64 loss: -0.24975359439849854
Batch 12/64 loss: -0.252143919467926
Batch 13/64 loss: -0.2502725124359131
Batch 14/64 loss: -0.252224862575531
Batch 15/64 loss: -0.2526226043701172
Batch 16/64 loss: -0.2553837299346924
Batch 17/64 loss: -0.26451897621154785
Batch 18/64 loss: -0.24605214595794678
Batch 19/64 loss: -0.2684478759765625
Batch 20/64 loss: -0.27713829278945923
Batch 21/64 loss: -0.2677296996116638
Batch 22/64 loss: -0.24725127220153809
Batch 23/64 loss: -0.258807897567749
Batch 24/64 loss: -0.25851500034332275
Batch 25/64 loss: -0.25547629594802856
Batch 26/64 loss: -0.25939249992370605
Batch 27/64 loss: -0.2618473768234253
Batch 28/64 loss: -0.25966811180114746
Batch 29/64 loss: -0.2796453833580017
Batch 30/64 loss: -0.2609730362892151
Batch 31/64 loss: -0.2674188017845154
Batch 32/64 loss: -0.2714915871620178
Batch 33/64 loss: -0.2799440622329712
Batch 34/64 loss: -0.2481091022491455
Batch 35/64 loss: -0.23772168159484863
Batch 36/64 loss: -0.26241445541381836
Batch 37/64 loss: -0.2520514726638794
Batch 38/64 loss: -0.2623080611228943
Batch 39/64 loss: -0.2630821466445923
Batch 40/64 loss: -0.24138092994689941
Batch 41/64 loss: -0.2619026303291321
Batch 42/64 loss: -0.23738056421279907
Batch 43/64 loss: -0.2592829465866089
Batch 44/64 loss: -0.2570904493331909
Batch 45/64 loss: -0.24024629592895508
Batch 46/64 loss: -0.27383559942245483
Batch 47/64 loss: -0.25327038764953613
Batch 48/64 loss: -0.2578515410423279
Batch 49/64 loss: -0.24373477697372437
Batch 50/64 loss: -0.25193119049072266
Batch 51/64 loss: -0.26348167657852173
Batch 52/64 loss: -0.2503756880760193
Batch 53/64 loss: -0.2658504247665405
Batch 54/64 loss: -0.24297606945037842
Batch 55/64 loss: -0.26096946001052856
Batch 56/64 loss: -0.2646564841270447
Batch 57/64 loss: -0.2744210958480835
Batch 58/64 loss: -0.269237220287323
Batch 59/64 loss: -0.2560378909111023
Batch 60/64 loss: -0.2703568935394287
Batch 61/64 loss: -0.2730710506439209
Batch 62/64 loss: -0.26944518089294434
Batch 63/64 loss: -0.27054572105407715
Batch 64/64 loss: -0.26311349868774414
Epoch 182  Train loss: -0.2564426160326191  Val loss: -0.23839444167835197
Epoch 183
-------------------------------
Batch 1/64 loss: -0.261124849319458
Batch 2/64 loss: -0.24573230743408203
Batch 3/64 loss: -0.26068586111068726
Batch 4/64 loss: -0.24804317951202393
Batch 5/64 loss: -0.25690335035324097
Batch 6/64 loss: -0.270133912563324
Batch 7/64 loss: -0.2692531943321228
Batch 8/64 loss: -0.27354860305786133
Batch 9/64 loss: -0.26426059007644653
Batch 10/64 loss: -0.26210540533065796
Batch 11/64 loss: -0.26472795009613037
Batch 12/64 loss: -0.2417777180671692
Batch 13/64 loss: -0.25608742237091064
Batch 14/64 loss: -0.2610633373260498
Batch 15/64 loss: -0.2713271975517273
Batch 16/64 loss: -0.2663725018501282
Batch 17/64 loss: -0.26039958000183105
Batch 18/64 loss: -0.26408547163009644
Batch 19/64 loss: -0.25716423988342285
Batch 20/64 loss: -0.27943581342697144
Batch 21/64 loss: -0.2675430178642273
Batch 22/64 loss: -0.2471940517425537
Batch 23/64 loss: -0.2747288942337036
Batch 24/64 loss: -0.2520703077316284
Batch 25/64 loss: -0.27641311287879944
Batch 26/64 loss: -0.266648530960083
Batch 27/64 loss: -0.2638896703720093
Batch 28/64 loss: -0.2573919892311096
Batch 29/64 loss: -0.2556416988372803
Batch 30/64 loss: -0.26780813932418823
Batch 31/64 loss: -0.2520240545272827
Batch 32/64 loss: -0.2632889747619629
Batch 33/64 loss: -0.26526087522506714
Batch 34/64 loss: -0.2588861584663391
Batch 35/64 loss: -0.2518535852432251
Batch 36/64 loss: -0.2740728259086609
Batch 37/64 loss: -0.2580764889717102
Batch 38/64 loss: -0.27464866638183594
Batch 39/64 loss: -0.2787501811981201
Batch 40/64 loss: -0.26294606924057007
Batch 41/64 loss: -0.2683892846107483
Batch 42/64 loss: -0.26266682147979736
Batch 43/64 loss: -0.2609589695930481
Batch 44/64 loss: -0.2446274757385254
Batch 45/64 loss: -0.26288551092147827
Batch 46/64 loss: -0.2601592540740967
Batch 47/64 loss: -0.26980817317962646
Batch 48/64 loss: -0.2708280682563782
Batch 49/64 loss: -0.26461392641067505
Batch 50/64 loss: -0.2751314640045166
Batch 51/64 loss: -0.2649382948875427
Batch 52/64 loss: -0.25193852186203003
Batch 53/64 loss: -0.26792222261428833
Batch 54/64 loss: -0.27396124601364136
Batch 55/64 loss: -0.27018415927886963
Batch 56/64 loss: -0.26491332054138184
Batch 57/64 loss: -0.2624783515930176
Batch 58/64 loss: -0.26549363136291504
Batch 59/64 loss: -0.26329678297042847
Batch 60/64 loss: -0.27185553312301636
Batch 61/64 loss: -0.27057385444641113
Batch 62/64 loss: -0.2708359360694885
Batch 63/64 loss: -0.2591210603713989
Batch 64/64 loss: -0.26584553718566895
Epoch 183  Train loss: -0.26353467913234935  Val loss: -0.20635811221558614
Epoch 184
-------------------------------
Batch 1/64 loss: -0.26711297035217285
Batch 2/64 loss: -0.2486119270324707
Batch 3/64 loss: -0.2758256793022156
Batch 4/64 loss: -0.2552117109298706
Batch 5/64 loss: -0.25598418712615967
Batch 6/64 loss: -0.26392704248428345
Batch 7/64 loss: -0.2731729745864868
Batch 8/64 loss: -0.23982352018356323
Batch 9/64 loss: -0.2626420259475708
Batch 10/64 loss: -0.2650580406188965
Batch 11/64 loss: -0.2715580463409424
Batch 12/64 loss: -0.2785642743110657
Batch 13/64 loss: -0.2722207307815552
Batch 14/64 loss: -0.2578018307685852
Batch 15/64 loss: -0.2649569511413574
Batch 16/64 loss: -0.2712830901145935
Batch 17/64 loss: -0.2577033042907715
Batch 18/64 loss: -0.2476218342781067
Batch 19/64 loss: -0.23622483015060425
Batch 20/64 loss: -0.26801055669784546
Batch 21/64 loss: -0.26593077182769775
Batch 22/64 loss: -0.2656470537185669
Batch 23/64 loss: -0.27082544565200806
Batch 24/64 loss: -0.2620607018470764
Batch 25/64 loss: -0.26183927059173584
Batch 26/64 loss: -0.26256197690963745
Batch 27/64 loss: -0.2676105499267578
Batch 28/64 loss: -0.263508677482605
Batch 29/64 loss: -0.25942468643188477
Batch 30/64 loss: -0.26346874237060547
Batch 31/64 loss: -0.25929391384124756
Batch 32/64 loss: -0.25297844409942627
Batch 33/64 loss: -0.26636838912963867
Batch 34/64 loss: -0.2710988521575928
Batch 35/64 loss: -0.25878840684890747
Batch 36/64 loss: -0.26095902919769287
Batch 37/64 loss: -0.27237236499786377
Batch 38/64 loss: -0.26950085163116455
Batch 39/64 loss: -0.27081215381622314
Batch 40/64 loss: -0.25831037759780884
Batch 41/64 loss: -0.2615169286727905
Batch 42/64 loss: -0.2651209831237793
Batch 43/64 loss: -0.250188946723938
Batch 44/64 loss: -0.26600921154022217
Batch 45/64 loss: -0.25682181119918823
Batch 46/64 loss: -0.268110990524292
Batch 47/64 loss: -0.28519657254219055
Batch 48/64 loss: -0.2686009407043457
Batch 49/64 loss: -0.27396607398986816
Batch 50/64 loss: -0.2796505093574524
Batch 51/64 loss: -0.2862790524959564
Batch 52/64 loss: -0.261783242225647
Batch 53/64 loss: -0.2569662928581238
Batch 54/64 loss: -0.2668485641479492
Batch 55/64 loss: -0.26633429527282715
Batch 56/64 loss: -0.27471792697906494
Batch 57/64 loss: -0.2657884955406189
Batch 58/64 loss: -0.2706140875816345
Batch 59/64 loss: -0.2695204019546509
Batch 60/64 loss: -0.26831531524658203
Batch 61/64 loss: -0.2796064615249634
Batch 62/64 loss: -0.2833997905254364
Batch 63/64 loss: -0.28461721539497375
Batch 64/64 loss: -0.2675827145576477
Epoch 184  Train loss: -0.26552686013427435  Val loss: -0.24680535060023934
Saving best model, epoch: 184
Epoch 185
-------------------------------
Batch 1/64 loss: -0.2721649408340454
Batch 2/64 loss: -0.2736161947250366
Batch 3/64 loss: -0.2790186405181885
Batch 4/64 loss: -0.27088111639022827
Batch 5/64 loss: -0.2826097011566162
Batch 6/64 loss: -0.2719533443450928
Batch 7/64 loss: -0.2702293395996094
Batch 8/64 loss: -0.26629602909088135
Batch 9/64 loss: -0.27239662408828735
Batch 10/64 loss: -0.25732117891311646
Batch 11/64 loss: -0.2592681646347046
Batch 12/64 loss: -0.24316120147705078
Batch 13/64 loss: -0.2576311230659485
Batch 14/64 loss: -0.263755738735199
Batch 15/64 loss: -0.24643903970718384
Batch 16/64 loss: -0.2585710287094116
Batch 17/64 loss: -0.2834906280040741
Batch 18/64 loss: -0.27053940296173096
Batch 19/64 loss: -0.2523922920227051
Batch 20/64 loss: -0.27557921409606934
Batch 21/64 loss: -0.2803604304790497
Batch 22/64 loss: -0.2471749186515808
Batch 23/64 loss: -0.27100855112075806
Batch 24/64 loss: -0.26656556129455566
Batch 25/64 loss: -0.26568835973739624
Batch 26/64 loss: -0.269062876701355
Batch 27/64 loss: -0.24681472778320312
Batch 28/64 loss: -0.26432573795318604
Batch 29/64 loss: -0.2779419422149658
Batch 30/64 loss: -0.2717782258987427
Batch 31/64 loss: -0.27137577533721924
Batch 32/64 loss: -0.2698199152946472
Batch 33/64 loss: -0.2741643190383911
Batch 34/64 loss: -0.2628936171531677
Batch 35/64 loss: -0.25101739168167114
Batch 36/64 loss: -0.27465301752090454
Batch 37/64 loss: -0.2584218978881836
Batch 38/64 loss: -0.27338409423828125
Batch 39/64 loss: -0.2642160654067993
Batch 40/64 loss: -0.2516525387763977
Batch 41/64 loss: -0.2479063868522644
Batch 42/64 loss: -0.2691667079925537
Batch 43/64 loss: -0.27330660820007324
Batch 44/64 loss: -0.2596138119697571
Batch 45/64 loss: -0.2645469903945923
Batch 46/64 loss: -0.26582711935043335
Batch 47/64 loss: -0.259906530380249
Batch 48/64 loss: -0.2590712308883667
Batch 49/64 loss: -0.26020169258117676
Batch 50/64 loss: -0.268984317779541
Batch 51/64 loss: -0.23998969793319702
Batch 52/64 loss: -0.2727315425872803
Batch 53/64 loss: -0.2630576491355896
Batch 54/64 loss: -0.27325886487960815
Batch 55/64 loss: -0.2608087658882141
Batch 56/64 loss: -0.27318280935287476
Batch 57/64 loss: -0.2523449659347534
Batch 58/64 loss: -0.2496660351753235
Batch 59/64 loss: -0.2556418180465698
Batch 60/64 loss: -0.25389033555984497
Batch 61/64 loss: -0.27510344982147217
Batch 62/64 loss: -0.2598608136177063
Batch 63/64 loss: -0.2567692995071411
Batch 64/64 loss: -0.2677572965621948
Epoch 185  Train loss: -0.2643967109567979  Val loss: -0.23592569983701936
Epoch 186
-------------------------------
Batch 1/64 loss: -0.272183358669281
Batch 2/64 loss: -0.27178603410720825
Batch 3/64 loss: -0.2580759525299072
Batch 4/64 loss: -0.25610417127609253
Batch 5/64 loss: -0.27501922845840454
Batch 6/64 loss: -0.2778909206390381
Batch 7/64 loss: -0.27746233344078064
Batch 8/64 loss: -0.25093603134155273
Batch 9/64 loss: -0.2808316946029663
Batch 10/64 loss: -0.24938642978668213
Batch 11/64 loss: -0.260736346244812
Batch 12/64 loss: -0.2676292657852173
Batch 13/64 loss: -0.27363884449005127
Batch 14/64 loss: -0.25723934173583984
Batch 15/64 loss: -0.2719915509223938
Batch 16/64 loss: -0.2689284682273865
Batch 17/64 loss: -0.2595038414001465
Batch 18/64 loss: -0.2638706564903259
Batch 19/64 loss: -0.24466019868850708
Batch 20/64 loss: -0.28152990341186523
Batch 21/64 loss: -0.2674567699432373
Batch 22/64 loss: -0.254879355430603
Batch 23/64 loss: -0.2678302526473999
Batch 24/64 loss: -0.2629941701889038
Batch 25/64 loss: -0.2588846683502197
Batch 26/64 loss: -0.2583867311477661
Batch 27/64 loss: -0.26565414667129517
Batch 28/64 loss: -0.2616461515426636
Batch 29/64 loss: -0.22245573997497559
Batch 30/64 loss: -0.2461848258972168
Batch 31/64 loss: -0.25543421506881714
Batch 32/64 loss: -0.2506076693534851
Batch 33/64 loss: -0.2552602291107178
Batch 34/64 loss: -0.2705153226852417
Batch 35/64 loss: -0.2642752528190613
Batch 36/64 loss: -0.2545957565307617
Batch 37/64 loss: -0.2668262720108032
Batch 38/64 loss: -0.26980412006378174
Batch 39/64 loss: -0.2627556324005127
Batch 40/64 loss: -0.26873207092285156
Batch 41/64 loss: -0.24755102396011353
Batch 42/64 loss: -0.2646704912185669
Batch 43/64 loss: -0.25816571712493896
Batch 44/64 loss: -0.26957225799560547
Batch 45/64 loss: -0.24635589122772217
Batch 46/64 loss: -0.2574703097343445
Batch 47/64 loss: -0.27070891857147217
Batch 48/64 loss: -0.2777993679046631
Batch 49/64 loss: -0.2736653685569763
Batch 50/64 loss: -0.25935816764831543
Batch 51/64 loss: -0.2646603584289551
Batch 52/64 loss: -0.26956820487976074
Batch 53/64 loss: -0.2659638524055481
Batch 54/64 loss: -0.2585828900337219
Batch 55/64 loss: -0.27132439613342285
Batch 56/64 loss: -0.26731157302856445
Batch 57/64 loss: -0.26397305727005005
Batch 58/64 loss: -0.2500154376029968
Batch 59/64 loss: -0.25205838680267334
Batch 60/64 loss: -0.28177160024642944
Batch 61/64 loss: -0.275287389755249
Batch 62/64 loss: -0.27194976806640625
Batch 63/64 loss: -0.28053444623947144
Batch 64/64 loss: -0.27638164162635803
Epoch 186  Train loss: -0.2636892397029727  Val loss: -0.23488422562576242
Epoch 187
-------------------------------
Batch 1/64 loss: -0.272156298160553
Batch 2/64 loss: -0.2804623544216156
Batch 3/64 loss: -0.2770063281059265
Batch 4/64 loss: -0.28054285049438477
Batch 5/64 loss: -0.2726924419403076
Batch 6/64 loss: -0.2792797088623047
Batch 7/64 loss: -0.26030993461608887
Batch 8/64 loss: -0.2508774995803833
Batch 9/64 loss: -0.26890742778778076
Batch 10/64 loss: -0.2666693925857544
Batch 11/64 loss: -0.2672743797302246
Batch 12/64 loss: -0.2679724097251892
Batch 13/64 loss: -0.27114027738571167
Batch 14/64 loss: -0.26191246509552
Batch 15/64 loss: -0.26714277267456055
Batch 16/64 loss: -0.2528066039085388
Batch 17/64 loss: -0.2640039920806885
Batch 18/64 loss: -0.26286834478378296
Batch 19/64 loss: -0.2683602571487427
Batch 20/64 loss: -0.2638202905654907
Batch 21/64 loss: -0.26420146226882935
Batch 22/64 loss: -0.25242316722869873
Batch 23/64 loss: -0.26524919271469116
Batch 24/64 loss: -0.2669523358345032
Batch 25/64 loss: -0.25613123178482056
Batch 26/64 loss: -0.2545779347419739
Batch 27/64 loss: -0.2502167820930481
Batch 28/64 loss: -0.250659704208374
Batch 29/64 loss: -0.2596917152404785
Batch 30/64 loss: -0.253711462020874
Batch 31/64 loss: -0.2575254440307617
Batch 32/64 loss: -0.2672504186630249
Batch 33/64 loss: -0.2625517249107361
Batch 34/64 loss: -0.2526389956474304
Batch 35/64 loss: -0.26832109689712524
Batch 36/64 loss: -0.276839941740036
Batch 37/64 loss: -0.23896753787994385
Batch 38/64 loss: -0.27558255195617676
Batch 39/64 loss: -0.2618083953857422
Batch 40/64 loss: -0.27438223361968994
Batch 41/64 loss: -0.25714462995529175
Batch 42/64 loss: -0.2658839821815491
Batch 43/64 loss: -0.2636718153953552
Batch 44/64 loss: -0.26441842317581177
Batch 45/64 loss: -0.26570552587509155
Batch 46/64 loss: -0.25860416889190674
Batch 47/64 loss: -0.2588574290275574
Batch 48/64 loss: -0.27230149507522583
Batch 49/64 loss: -0.2538679242134094
Batch 50/64 loss: -0.230707049369812
Batch 51/64 loss: -0.26825714111328125
Batch 52/64 loss: -0.2632647752761841
Batch 53/64 loss: -0.2692527770996094
Batch 54/64 loss: -0.2645074129104614
Batch 55/64 loss: -0.27270036935806274
Batch 56/64 loss: -0.2619233727455139
Batch 57/64 loss: -0.2683255672454834
Batch 58/64 loss: -0.27793967723846436
Batch 59/64 loss: -0.26361095905303955
Batch 60/64 loss: -0.2337653636932373
Batch 61/64 loss: -0.27050769329071045
Batch 62/64 loss: -0.27476412057876587
Batch 63/64 loss: -0.27113866806030273
Batch 64/64 loss: -0.261817991733551
Epoch 187  Train loss: -0.2637391873434478  Val loss: -0.24226262827509457
Epoch 188
-------------------------------
Batch 1/64 loss: -0.25720763206481934
Batch 2/64 loss: -0.2658078670501709
Batch 3/64 loss: -0.2777211368083954
Batch 4/64 loss: -0.2705363631248474
Batch 5/64 loss: -0.25951212644577026
Batch 6/64 loss: -0.2641376852989197
Batch 7/64 loss: -0.2789977192878723
Batch 8/64 loss: -0.2755686640739441
Batch 9/64 loss: -0.26418614387512207
Batch 10/64 loss: -0.2806340456008911
Batch 11/64 loss: -0.2742580771446228
Batch 12/64 loss: -0.27605730295181274
Batch 13/64 loss: -0.2589743137359619
Batch 14/64 loss: -0.2648794651031494
Batch 15/64 loss: -0.27134817838668823
Batch 16/64 loss: -0.2561032176017761
Batch 17/64 loss: -0.2544589042663574
Batch 18/64 loss: -0.2699164152145386
Batch 19/64 loss: -0.2533000111579895
Batch 20/64 loss: -0.2699573040008545
Batch 21/64 loss: -0.26629912853240967
Batch 22/64 loss: -0.2687336206436157
Batch 23/64 loss: -0.2648214101791382
Batch 24/64 loss: -0.2662612199783325
Batch 25/64 loss: -0.2682824730873108
Batch 26/64 loss: -0.27353328466415405
Batch 27/64 loss: -0.2706947326660156
Batch 28/64 loss: -0.27586066722869873
Batch 29/64 loss: -0.2679014801979065
Batch 30/64 loss: -0.253822922706604
Batch 31/64 loss: -0.26576846837997437
Batch 32/64 loss: -0.25796836614608765
Batch 33/64 loss: -0.2740233540534973
Batch 34/64 loss: -0.2737917900085449
Batch 35/64 loss: -0.2695046663284302
Batch 36/64 loss: -0.2472859025001526
Batch 37/64 loss: -0.26536107063293457
Batch 38/64 loss: -0.2788363993167877
Batch 39/64 loss: -0.24315530061721802
Batch 40/64 loss: -0.26718252897262573
Batch 41/64 loss: -0.26573479175567627
Batch 42/64 loss: -0.2787860035896301
Batch 43/64 loss: -0.265489399433136
Batch 44/64 loss: -0.2677716016769409
Batch 45/64 loss: -0.2773333191871643
Batch 46/64 loss: -0.27132391929626465
Batch 47/64 loss: -0.2602687478065491
Batch 48/64 loss: -0.2749117612838745
Batch 49/64 loss: -0.272513747215271
Batch 50/64 loss: -0.2705771327018738
Batch 51/64 loss: -0.26429659128189087
Batch 52/64 loss: -0.2675575017929077
Batch 53/64 loss: -0.23434555530548096
Batch 54/64 loss: -0.26658016443252563
Batch 55/64 loss: -0.2653743624687195
Batch 56/64 loss: -0.26753944158554077
Batch 57/64 loss: -0.25272631645202637
Batch 58/64 loss: -0.2654604911804199
Batch 59/64 loss: -0.29366856813430786
Batch 60/64 loss: -0.2563936114311218
Batch 61/64 loss: -0.27716702222824097
Batch 62/64 loss: -0.2646154761314392
Batch 63/64 loss: -0.2901883125305176
Batch 64/64 loss: -0.27802515029907227
Epoch 188  Train loss: -0.26732225979075713  Val loss: -0.23277562365089496
Epoch 189
-------------------------------
Batch 1/64 loss: -0.23579150438308716
Batch 2/64 loss: -0.2737250328063965
Batch 3/64 loss: -0.2719990015029907
Batch 4/64 loss: -0.23298990726470947
Batch 5/64 loss: -0.25441974401474
Batch 6/64 loss: -0.2688342332839966
Batch 7/64 loss: -0.27471452951431274
Batch 8/64 loss: -0.28181782364845276
Batch 9/64 loss: -0.2429291009902954
Batch 10/64 loss: -0.26329028606414795
Batch 11/64 loss: -0.26508718729019165
Batch 12/64 loss: -0.2648186683654785
Batch 13/64 loss: -0.23623818159103394
Batch 14/64 loss: -0.26546210050582886
Batch 15/64 loss: -0.26900196075439453
Batch 16/64 loss: -0.25853651762008667
Batch 17/64 loss: -0.2519218921661377
Batch 18/64 loss: -0.2678912281990051
Batch 19/64 loss: -0.2776428461074829
Batch 20/64 loss: -0.2624390721321106
Batch 21/64 loss: -0.2650015950202942
Batch 22/64 loss: -0.2613159418106079
Batch 23/64 loss: -0.24668550491333008
Batch 24/64 loss: -0.25933200120925903
Batch 25/64 loss: -0.2573722004890442
Batch 26/64 loss: -0.26832908391952515
Batch 27/64 loss: -0.25117790699005127
Batch 28/64 loss: -0.2597821354866028
Batch 29/64 loss: -0.27520883083343506
Batch 30/64 loss: -0.26884984970092773
Batch 31/64 loss: -0.2718290090560913
Batch 32/64 loss: -0.26147180795669556
Batch 33/64 loss: -0.2645533084869385
Batch 34/64 loss: -0.2633519172668457
Batch 35/64 loss: -0.26782041788101196
Batch 36/64 loss: -0.2681487202644348
Batch 37/64 loss: -0.2482297420501709
Batch 38/64 loss: -0.2596450448036194
Batch 39/64 loss: -0.2681868076324463
Batch 40/64 loss: -0.277488112449646
Batch 41/64 loss: -0.26544463634490967
Batch 42/64 loss: -0.2493380308151245
Batch 43/64 loss: -0.27705079317092896
Batch 44/64 loss: -0.27219313383102417
Batch 45/64 loss: -0.2690393924713135
Batch 46/64 loss: -0.25009942054748535
Batch 47/64 loss: -0.26980501413345337
Batch 48/64 loss: -0.2579166889190674
Batch 49/64 loss: -0.2621645927429199
Batch 50/64 loss: -0.23757129907608032
Batch 51/64 loss: -0.26673394441604614
Batch 52/64 loss: -0.2569361925125122
Batch 53/64 loss: -0.25883620977401733
Batch 54/64 loss: -0.26225554943084717
Batch 55/64 loss: -0.261269748210907
Batch 56/64 loss: -0.23683172464370728
Batch 57/64 loss: -0.26879793405532837
Batch 58/64 loss: -0.2701594829559326
Batch 59/64 loss: -0.25887036323547363
Batch 60/64 loss: -0.26371365785598755
Batch 61/64 loss: -0.2615586519241333
Batch 62/64 loss: -0.27410125732421875
Batch 63/64 loss: -0.24867737293243408
Batch 64/64 loss: -0.27274298667907715
Epoch 189  Train loss: -0.26176083274916107  Val loss: -0.22528410285608874
Epoch 190
-------------------------------
Batch 1/64 loss: -0.24441015720367432
Batch 2/64 loss: -0.27408862113952637
Batch 3/64 loss: -0.28153911232948303
Batch 4/64 loss: -0.2808389663696289
Batch 5/64 loss: -0.2813096046447754
Batch 6/64 loss: -0.2566158175468445
Batch 7/64 loss: -0.2587597966194153
Batch 8/64 loss: -0.2615622878074646
Batch 9/64 loss: -0.2620043158531189
Batch 10/64 loss: -0.2556949257850647
Batch 11/64 loss: -0.27153146266937256
Batch 12/64 loss: -0.27829986810684204
Batch 13/64 loss: -0.25182610750198364
Batch 14/64 loss: -0.2736017107963562
Batch 15/64 loss: -0.2726989984512329
Batch 16/64 loss: -0.27065110206604004
Batch 17/64 loss: -0.25163501501083374
Batch 18/64 loss: -0.2612265348434448
Batch 19/64 loss: -0.2705008387565613
Batch 20/64 loss: -0.24504518508911133
Batch 21/64 loss: -0.2687162160873413
Batch 22/64 loss: -0.25962090492248535
Batch 23/64 loss: -0.26076674461364746
Batch 24/64 loss: -0.26043981313705444
Batch 25/64 loss: -0.2680535912513733
Batch 26/64 loss: -0.25676095485687256
Batch 27/64 loss: -0.26229190826416016
Batch 28/64 loss: -0.2843555510044098
Batch 29/64 loss: -0.274421751499176
Batch 30/64 loss: -0.26996707916259766
Batch 31/64 loss: -0.23690485954284668
Batch 32/64 loss: -0.257343053817749
Batch 33/64 loss: -0.2655966877937317
Batch 34/64 loss: -0.2507411241531372
Batch 35/64 loss: -0.2588304281234741
Batch 36/64 loss: -0.25427448749542236
Batch 37/64 loss: -0.24933254718780518
Batch 38/64 loss: -0.2535768747329712
Batch 39/64 loss: -0.2589207887649536
Batch 40/64 loss: -0.2655583620071411
Batch 41/64 loss: -0.24890869855880737
Batch 42/64 loss: -0.27579784393310547
Batch 43/64 loss: -0.26140159368515015
Batch 44/64 loss: -0.24769651889801025
Batch 45/64 loss: -0.24567711353302002
Batch 46/64 loss: -0.27419841289520264
Batch 47/64 loss: -0.2590574026107788
Batch 48/64 loss: -0.26539021730422974
Batch 49/64 loss: -0.2681235074996948
Batch 50/64 loss: -0.27465319633483887
Batch 51/64 loss: -0.26526880264282227
Batch 52/64 loss: -0.2616392970085144
Batch 53/64 loss: -0.255365252494812
Batch 54/64 loss: -0.27611029148101807
Batch 55/64 loss: -0.2572002410888672
Batch 56/64 loss: -0.270763635635376
Batch 57/64 loss: -0.266878604888916
Batch 58/64 loss: -0.26594340801239014
Batch 59/64 loss: -0.27307820320129395
Batch 60/64 loss: -0.2668870687484741
Batch 61/64 loss: -0.2394167184829712
Batch 62/64 loss: -0.2642322778701782
Batch 63/64 loss: -0.2674214839935303
Batch 64/64 loss: -0.27179521322250366
Epoch 190  Train loss: -0.2631414958074981  Val loss: -0.24061325673794828
Epoch 191
-------------------------------
Batch 1/64 loss: -0.2764047384262085
Batch 2/64 loss: -0.2806430459022522
Batch 3/64 loss: -0.2362375259399414
Batch 4/64 loss: -0.26715636253356934
Batch 5/64 loss: -0.2707715630531311
Batch 6/64 loss: -0.26400208473205566
Batch 7/64 loss: -0.25679707527160645
Batch 8/64 loss: -0.26968634128570557
Batch 9/64 loss: -0.2695745825767517
Batch 10/64 loss: -0.27926063537597656
Batch 11/64 loss: -0.25790250301361084
Batch 12/64 loss: -0.2780196964740753
Batch 13/64 loss: -0.26342976093292236
Batch 14/64 loss: -0.25938695669174194
Batch 15/64 loss: -0.2500889301300049
Batch 16/64 loss: -0.2678772211074829
Batch 17/64 loss: -0.27273666858673096
Batch 18/64 loss: -0.27189165353775024
Batch 19/64 loss: -0.27658146619796753
Batch 20/64 loss: -0.2730693817138672
Batch 21/64 loss: -0.2732812762260437
Batch 22/64 loss: -0.26782745122909546
Batch 23/64 loss: -0.2600693106651306
Batch 24/64 loss: -0.25767189264297485
Batch 25/64 loss: -0.27382993698120117
Batch 26/64 loss: -0.2627837061882019
Batch 27/64 loss: -0.2656524181365967
Batch 28/64 loss: -0.2602313756942749
Batch 29/64 loss: -0.2661651372909546
Batch 30/64 loss: -0.2685477137565613
Batch 31/64 loss: -0.2881494462490082
Batch 32/64 loss: -0.26780205965042114
Batch 33/64 loss: -0.2719239592552185
Batch 34/64 loss: -0.2756696343421936
Batch 35/64 loss: -0.27237361669540405
Batch 36/64 loss: -0.2535626292228699
Batch 37/64 loss: -0.2699885964393616
Batch 38/64 loss: -0.25806689262390137
Batch 39/64 loss: -0.28319984674453735
Batch 40/64 loss: -0.2718033194541931
Batch 41/64 loss: -0.26730799674987793
Batch 42/64 loss: -0.26357877254486084
Batch 43/64 loss: -0.2697408199310303
Batch 44/64 loss: -0.27204060554504395
Batch 45/64 loss: -0.2736297845840454
Batch 46/64 loss: -0.27685368061065674
Batch 47/64 loss: -0.2771534323692322
Batch 48/64 loss: -0.2832793593406677
Batch 49/64 loss: -0.26644426584243774
Batch 50/64 loss: -0.26269179582595825
Batch 51/64 loss: -0.24250972270965576
Batch 52/64 loss: -0.2602382302284241
Batch 53/64 loss: -0.26659828424453735
Batch 54/64 loss: -0.2695245146751404
Batch 55/64 loss: -0.2762925624847412
Batch 56/64 loss: -0.2587103843688965
Batch 57/64 loss: -0.26221972703933716
Batch 58/64 loss: -0.2700929641723633
Batch 59/64 loss: -0.25183069705963135
Batch 60/64 loss: -0.25219833850860596
Batch 61/64 loss: -0.2629285454750061
Batch 62/64 loss: -0.2516489028930664
Batch 63/64 loss: -0.24567663669586182
Batch 64/64 loss: -0.25483137369155884
Epoch 191  Train loss: -0.2664224622296352  Val loss: -0.23021160163420581
Epoch 192
-------------------------------
Batch 1/64 loss: -0.25419509410858154
Batch 2/64 loss: -0.24498742818832397
Batch 3/64 loss: -0.2521904706954956
Batch 4/64 loss: -0.2493422031402588
Batch 5/64 loss: -0.2451971173286438
Batch 6/64 loss: -0.26366740465164185
Batch 7/64 loss: -0.266099214553833
Batch 8/64 loss: -0.2545781135559082
Batch 9/64 loss: -0.2632548213005066
Batch 10/64 loss: -0.2698290944099426
Batch 11/64 loss: -0.26096677780151367
Batch 12/64 loss: -0.2679954171180725
Batch 13/64 loss: -0.25268030166625977
Batch 14/64 loss: -0.2528398633003235
Batch 15/64 loss: -0.26875656843185425
Batch 16/64 loss: -0.2631005048751831
Batch 17/64 loss: -0.266598105430603
Batch 18/64 loss: -0.26182520389556885
Batch 19/64 loss: -0.27049458026885986
Batch 20/64 loss: -0.2652369737625122
Batch 21/64 loss: -0.2593347430229187
Batch 22/64 loss: -0.27882498502731323
Batch 23/64 loss: -0.2769775986671448
Batch 24/64 loss: -0.26751935482025146
Batch 25/64 loss: -0.27695101499557495
Batch 26/64 loss: -0.24766016006469727
Batch 27/64 loss: -0.27959391474723816
Batch 28/64 loss: -0.26982998847961426
Batch 29/64 loss: -0.25400352478027344
Batch 30/64 loss: -0.2670016884803772
Batch 31/64 loss: -0.2613537311553955
Batch 32/64 loss: -0.27292776107788086
Batch 33/64 loss: -0.27357620000839233
Batch 34/64 loss: -0.2386465072631836
Batch 35/64 loss: -0.25992971658706665
Batch 36/64 loss: -0.2505371570587158
Batch 37/64 loss: -0.24178004264831543
Batch 38/64 loss: -0.2753574848175049
Batch 39/64 loss: -0.2715337872505188
Batch 40/64 loss: -0.27072417736053467
Batch 41/64 loss: -0.2670901417732239
Batch 42/64 loss: -0.27192211151123047
Batch 43/64 loss: -0.2571897506713867
Batch 44/64 loss: -0.2756025195121765
Batch 45/64 loss: -0.2674364447593689
Batch 46/64 loss: -0.2666481137275696
Batch 47/64 loss: -0.2493211030960083
Batch 48/64 loss: -0.25834983587265015
Batch 49/64 loss: -0.2626104950904846
Batch 50/64 loss: -0.2676544785499573
Batch 51/64 loss: -0.2661934494972229
Batch 52/64 loss: -0.26070332527160645
Batch 53/64 loss: -0.27184635400772095
Batch 54/64 loss: -0.26205259561538696
Batch 55/64 loss: -0.26833248138427734
Batch 56/64 loss: -0.27416473627090454
Batch 57/64 loss: -0.26551055908203125
Batch 58/64 loss: -0.26128488779067993
Batch 59/64 loss: -0.2605516314506531
Batch 60/64 loss: -0.2788499593734741
Batch 61/64 loss: -0.2803664803504944
Batch 62/64 loss: -0.2697497010231018
Batch 63/64 loss: -0.2699396014213562
Batch 64/64 loss: -0.265411913394928
Epoch 192  Train loss: -0.26384829002268173  Val loss: -0.2342661879316638
Epoch 193
-------------------------------
Batch 1/64 loss: -0.2548196315765381
Batch 2/64 loss: -0.2587074041366577
Batch 3/64 loss: -0.27649804949760437
Batch 4/64 loss: -0.26185786724090576
Batch 5/64 loss: -0.2623928189277649
Batch 6/64 loss: -0.2646522521972656
Batch 7/64 loss: -0.268191933631897
Batch 8/64 loss: -0.27678442001342773
Batch 9/64 loss: -0.26241010427474976
Batch 10/64 loss: -0.26253819465637207
Batch 11/64 loss: -0.2758401036262512
Batch 12/64 loss: -0.2610064148902893
Batch 13/64 loss: -0.2550175189971924
Batch 14/64 loss: -0.2681068181991577
Batch 15/64 loss: -0.27575743198394775
Batch 16/64 loss: -0.27096718549728394
Batch 17/64 loss: -0.26574432849884033
Batch 18/64 loss: -0.2596103549003601
Batch 19/64 loss: -0.2613932490348816
Batch 20/64 loss: -0.2707902789115906
Batch 21/64 loss: -0.25862395763397217
Batch 22/64 loss: -0.27967455983161926
Batch 23/64 loss: -0.2407500147819519
Batch 24/64 loss: -0.273193359375
Batch 25/64 loss: -0.2743617296218872
Batch 26/64 loss: -0.2776174545288086
Batch 27/64 loss: -0.2677508592605591
Batch 28/64 loss: -0.2768935561180115
Batch 29/64 loss: -0.25757938623428345
Batch 30/64 loss: -0.27124541997909546
Batch 31/64 loss: -0.26295673847198486
Batch 32/64 loss: -0.25595951080322266
Batch 33/64 loss: -0.28168362379074097
Batch 34/64 loss: -0.2651140093803406
Batch 35/64 loss: -0.25151675939559937
Batch 36/64 loss: -0.2777109742164612
Batch 37/64 loss: -0.278415322303772
Batch 38/64 loss: -0.2643548250198364
Batch 39/64 loss: -0.27719253301620483
Batch 40/64 loss: -0.2632412910461426
Batch 41/64 loss: -0.271045446395874
Batch 42/64 loss: -0.2745822072029114
Batch 43/64 loss: -0.2776039242744446
Batch 44/64 loss: -0.26789069175720215
Batch 45/64 loss: -0.27229344844818115
Batch 46/64 loss: -0.27582162618637085
Batch 47/64 loss: -0.2759772837162018
Batch 48/64 loss: -0.27314066886901855
Batch 49/64 loss: -0.26652657985687256
Batch 50/64 loss: -0.25973010063171387
Batch 51/64 loss: -0.23480039834976196
Batch 52/64 loss: -0.27538973093032837
Batch 53/64 loss: -0.26316356658935547
Batch 54/64 loss: -0.2695605158805847
Batch 55/64 loss: -0.27223843336105347
Batch 56/64 loss: -0.26377224922180176
Batch 57/64 loss: -0.26755988597869873
Batch 58/64 loss: -0.2596396803855896
Batch 59/64 loss: -0.26753324270248413
Batch 60/64 loss: -0.264512836933136
Batch 61/64 loss: -0.2564179301261902
Batch 62/64 loss: -0.2570422291755676
Batch 63/64 loss: -0.2512105107307434
Batch 64/64 loss: -0.27961915731430054
Epoch 193  Train loss: -0.26660533021478094  Val loss: -0.23874198755447806
Epoch 194
-------------------------------
Batch 1/64 loss: -0.2723127007484436
Batch 2/64 loss: -0.24868792295455933
Batch 3/64 loss: -0.2731664180755615
Batch 4/64 loss: -0.26147037744522095
Batch 5/64 loss: -0.2849513292312622
Batch 6/64 loss: -0.2775362730026245
Batch 7/64 loss: -0.2703756093978882
Batch 8/64 loss: -0.24571585655212402
Batch 9/64 loss: -0.25930774211883545
Batch 10/64 loss: -0.2783091962337494
Batch 11/64 loss: -0.2569708824157715
Batch 12/64 loss: -0.257058322429657
Batch 13/64 loss: -0.2685976028442383
Batch 14/64 loss: -0.26984286308288574
Batch 15/64 loss: -0.25453102588653564
Batch 16/64 loss: -0.25156867504119873
Batch 17/64 loss: -0.27668899297714233
Batch 18/64 loss: -0.2810204029083252
Batch 19/64 loss: -0.26430022716522217
Batch 20/64 loss: -0.2571316361427307
Batch 21/64 loss: -0.2618345618247986
Batch 22/64 loss: -0.2531774640083313
Batch 23/64 loss: -0.27496498823165894
Batch 24/64 loss: -0.25557005405426025
Batch 25/64 loss: -0.26704859733581543
Batch 26/64 loss: -0.2593855857849121
Batch 27/64 loss: -0.26350218057632446
Batch 28/64 loss: -0.2702087163925171
Batch 29/64 loss: -0.24473696947097778
Batch 30/64 loss: -0.25033146142959595
Batch 31/64 loss: -0.2517990469932556
Batch 32/64 loss: -0.27755337953567505
Batch 33/64 loss: -0.26512181758880615
Batch 34/64 loss: -0.272869348526001
Batch 35/64 loss: -0.2707582712173462
Batch 36/64 loss: -0.2580147385597229
Batch 37/64 loss: -0.24007368087768555
Batch 38/64 loss: -0.25616830587387085
Batch 39/64 loss: -0.27712592482566833
Batch 40/64 loss: -0.2739967703819275
Batch 41/64 loss: -0.26504653692245483
Batch 42/64 loss: -0.26288050413131714
Batch 43/64 loss: -0.2769870162010193
Batch 44/64 loss: -0.26127076148986816
Batch 45/64 loss: -0.2396453619003296
Batch 46/64 loss: -0.27547788619995117
Batch 47/64 loss: -0.2646060585975647
Batch 48/64 loss: -0.2618279457092285
Batch 49/64 loss: -0.2650863528251648
Batch 50/64 loss: -0.2492542266845703
Batch 51/64 loss: -0.24925637245178223
Batch 52/64 loss: -0.24259698390960693
Batch 53/64 loss: -0.25294435024261475
Batch 54/64 loss: -0.24301940202713013
Batch 55/64 loss: -0.26144593954086304
Batch 56/64 loss: -0.27454066276550293
Batch 57/64 loss: -0.2707148790359497
Batch 58/64 loss: -0.2301589846611023
Batch 59/64 loss: -0.22589945793151855
Batch 60/64 loss: -0.236564040184021
Batch 61/64 loss: -0.25374066829681396
Batch 62/64 loss: -0.26444828510284424
Batch 63/64 loss: -0.2536332607269287
Batch 64/64 loss: -0.23802703619003296
Epoch 194  Train loss: -0.26060160211488315  Val loss: -0.2266149395929579
Epoch 195
-------------------------------
Batch 1/64 loss: -0.27709466218948364
Batch 2/64 loss: -0.25708532333374023
Batch 3/64 loss: -0.27342671155929565
Batch 4/64 loss: -0.24046868085861206
Batch 5/64 loss: -0.23501455783843994
Batch 6/64 loss: -0.2296379804611206
Batch 7/64 loss: -0.2564426064491272
Batch 8/64 loss: -0.25267231464385986
Batch 9/64 loss: -0.25846725702285767
Batch 10/64 loss: -0.2583053708076477
Batch 11/64 loss: -0.263716459274292
Batch 12/64 loss: -0.2690630555152893
Batch 13/64 loss: -0.2594081163406372
Batch 14/64 loss: -0.26767194271087646
Batch 15/64 loss: -0.273007333278656
Batch 16/64 loss: -0.27811890840530396
Batch 17/64 loss: -0.2657012343406677
Batch 18/64 loss: -0.24780124425888062
Batch 19/64 loss: -0.25897836685180664
Batch 20/64 loss: -0.2634560465812683
Batch 21/64 loss: -0.26548200845718384
Batch 22/64 loss: -0.2518271207809448
Batch 23/64 loss: -0.2565956115722656
Batch 24/64 loss: -0.26803213357925415
Batch 25/64 loss: -0.2502184510231018
Batch 26/64 loss: -0.2576422691345215
Batch 27/64 loss: -0.2587343454360962
Batch 28/64 loss: -0.2687184810638428
Batch 29/64 loss: -0.2681587338447571
Batch 30/64 loss: -0.2752816677093506
Batch 31/64 loss: -0.263655424118042
Batch 32/64 loss: -0.26926302909851074
Batch 33/64 loss: -0.2810189425945282
Batch 34/64 loss: -0.2754948139190674
Batch 35/64 loss: -0.2621029019355774
Batch 36/64 loss: -0.27539700269699097
Batch 37/64 loss: -0.255585253238678
Batch 38/64 loss: -0.2733945846557617
Batch 39/64 loss: -0.2655957341194153
Batch 40/64 loss: -0.2507238984107971
Batch 41/64 loss: -0.2727504372596741
Batch 42/64 loss: -0.28025540709495544
Batch 43/64 loss: -0.2622264623641968
Batch 44/64 loss: -0.25547879934310913
Batch 45/64 loss: -0.27120864391326904
Batch 46/64 loss: -0.2659488916397095
Batch 47/64 loss: -0.2730717062950134
Batch 48/64 loss: -0.27506023645401
Batch 49/64 loss: -0.25319015979766846
Batch 50/64 loss: -0.2796561121940613
Batch 51/64 loss: -0.2691788673400879
Batch 52/64 loss: -0.2725546360015869
Batch 53/64 loss: -0.276397168636322
Batch 54/64 loss: -0.26017194986343384
Batch 55/64 loss: -0.2875727713108063
Batch 56/64 loss: -0.2731061577796936
Batch 57/64 loss: -0.24819320440292358
Batch 58/64 loss: -0.25580090284347534
Batch 59/64 loss: -0.26566076278686523
Batch 60/64 loss: -0.271206259727478
Batch 61/64 loss: -0.27066051959991455
Batch 62/64 loss: -0.27727895975112915
Batch 63/64 loss: -0.2665823698043823
Batch 64/64 loss: -0.27183008193969727
Epoch 195  Train loss: -0.26455756121990726  Val loss: -0.24374459310085914
Epoch 196
-------------------------------
Batch 1/64 loss: -0.2666568160057068
Batch 2/64 loss: -0.253681480884552
Batch 3/64 loss: -0.2727203369140625
Batch 4/64 loss: -0.2696499228477478
Batch 5/64 loss: -0.2626977562904358
Batch 6/64 loss: -0.26916593313217163
Batch 7/64 loss: -0.2633180618286133
Batch 8/64 loss: -0.2632966637611389
Batch 9/64 loss: -0.26049840450286865
Batch 10/64 loss: -0.24945873022079468
Batch 11/64 loss: -0.2716805934906006
Batch 12/64 loss: -0.2670466899871826
Batch 13/64 loss: -0.271314799785614
Batch 14/64 loss: -0.2789510190486908
Batch 15/64 loss: -0.28207266330718994
Batch 16/64 loss: -0.2771819531917572
Batch 17/64 loss: -0.2669656276702881
Batch 18/64 loss: -0.26842325925827026
Batch 19/64 loss: -0.2734141945838928
Batch 20/64 loss: -0.2781984508037567
Batch 21/64 loss: -0.2708420753479004
Batch 22/64 loss: -0.2543376088142395
Batch 23/64 loss: -0.2771655321121216
Batch 24/64 loss: -0.2753605544567108
Batch 25/64 loss: -0.27319151163101196
Batch 26/64 loss: -0.27160531282424927
Batch 27/64 loss: -0.2726568579673767
Batch 28/64 loss: -0.27295225858688354
Batch 29/64 loss: -0.27075546979904175
Batch 30/64 loss: -0.2730579972267151
Batch 31/64 loss: -0.2723348140716553
Batch 32/64 loss: -0.28284937143325806
Batch 33/64 loss: -0.26180821657180786
Batch 34/64 loss: -0.26885294914245605
Batch 35/64 loss: -0.26888173818588257
Batch 36/64 loss: -0.2804158926010132
Batch 37/64 loss: -0.26274752616882324
Batch 38/64 loss: -0.27977633476257324
Batch 39/64 loss: -0.26196980476379395
Batch 40/64 loss: -0.2489963173866272
Batch 41/64 loss: -0.2653775215148926
Batch 42/64 loss: -0.2623997926712036
Batch 43/64 loss: -0.27057236433029175
Batch 44/64 loss: -0.26080524921417236
Batch 45/64 loss: -0.2521183490753174
Batch 46/64 loss: -0.26195448637008667
Batch 47/64 loss: -0.2791286110877991
Batch 48/64 loss: -0.24914240837097168
Batch 49/64 loss: -0.27268487215042114
Batch 50/64 loss: -0.25679516792297363
Batch 51/64 loss: -0.29009518027305603
Batch 52/64 loss: -0.2723935842514038
Batch 53/64 loss: -0.2551157474517822
Batch 54/64 loss: -0.27077585458755493
Batch 55/64 loss: -0.27006685733795166
Batch 56/64 loss: -0.26324886083602905
Batch 57/64 loss: -0.27798494696617126
Batch 58/64 loss: -0.2784543037414551
Batch 59/64 loss: -0.27539825439453125
Batch 60/64 loss: -0.2694765329360962
Batch 61/64 loss: -0.2565733790397644
Batch 62/64 loss: -0.26960092782974243
Batch 63/64 loss: -0.2629624009132385
Batch 64/64 loss: -0.26917535066604614
Epoch 196  Train loss: -0.2684228810609556  Val loss: -0.24035522331486864
Epoch 197
-------------------------------
Batch 1/64 loss: -0.2590276002883911
Batch 2/64 loss: -0.2708146572113037
Batch 3/64 loss: -0.2660423517227173
Batch 4/64 loss: -0.27108752727508545
Batch 5/64 loss: -0.2724113464355469
Batch 6/64 loss: -0.25551605224609375
Batch 7/64 loss: -0.24604111909866333
Batch 8/64 loss: -0.26801902055740356
Batch 9/64 loss: -0.26787787675857544
Batch 10/64 loss: -0.26293349266052246
Batch 11/64 loss: -0.2606886625289917
Batch 12/64 loss: -0.27794092893600464
Batch 13/64 loss: -0.273501455783844
Batch 14/64 loss: -0.2687234878540039
Batch 15/64 loss: -0.2536315321922302
Batch 16/64 loss: -0.25633811950683594
Batch 17/64 loss: -0.25852155685424805
Batch 18/64 loss: -0.262157678604126
Batch 19/64 loss: -0.27328795194625854
Batch 20/64 loss: -0.26374197006225586
Batch 21/64 loss: -0.26172906160354614
Batch 22/64 loss: -0.26018214225769043
Batch 23/64 loss: -0.27150624990463257
Batch 24/64 loss: -0.26196497678756714
Batch 25/64 loss: -0.26191431283950806
Batch 26/64 loss: -0.25912708044052124
Batch 27/64 loss: -0.24550467729568481
Batch 28/64 loss: -0.2538285255432129
Batch 29/64 loss: -0.2585071921348572
Batch 30/64 loss: -0.26364564895629883
Batch 31/64 loss: -0.2669258117675781
Batch 32/64 loss: -0.2567652463912964
Batch 33/64 loss: -0.24158227443695068
Batch 34/64 loss: -0.24832391738891602
Batch 35/64 loss: -0.2737044095993042
Batch 36/64 loss: -0.2577677369117737
Batch 37/64 loss: -0.2822561264038086
Batch 38/64 loss: -0.26035380363464355
Batch 39/64 loss: -0.26416224241256714
Batch 40/64 loss: -0.2655600905418396
Batch 41/64 loss: -0.2680938243865967
Batch 42/64 loss: -0.2634981870651245
Batch 43/64 loss: -0.2672240734100342
Batch 44/64 loss: -0.26395124197006226
Batch 45/64 loss: -0.25117915868759155
Batch 46/64 loss: -0.2557457685470581
Batch 47/64 loss: -0.27306485176086426
Batch 48/64 loss: -0.27653512358665466
Batch 49/64 loss: -0.2577783465385437
Batch 50/64 loss: -0.263843297958374
Batch 51/64 loss: -0.2722484767436981
Batch 52/64 loss: -0.2529204487800598
Batch 53/64 loss: -0.2741512358188629
Batch 54/64 loss: -0.2745550274848938
Batch 55/64 loss: -0.27527445554733276
Batch 56/64 loss: -0.26473724842071533
Batch 57/64 loss: -0.23871266841888428
Batch 58/64 loss: -0.25434839725494385
Batch 59/64 loss: -0.26826703548431396
Batch 60/64 loss: -0.28287646174430847
Batch 61/64 loss: -0.27783095836639404
Batch 62/64 loss: -0.2500970959663391
Batch 63/64 loss: -0.2337968349456787
Batch 64/64 loss: -0.24539756774902344
Epoch 197  Train loss: -0.26271991075253953  Val loss: -0.22115405584938339
Epoch 198
-------------------------------
Batch 1/64 loss: -0.2580488324165344
Batch 2/64 loss: -0.2284693717956543
Batch 3/64 loss: -0.26022058725357056
Batch 4/64 loss: -0.2632284164428711
Batch 5/64 loss: -0.2593316435813904
Batch 6/64 loss: -0.25351905822753906
Batch 7/64 loss: -0.22821295261383057
Batch 8/64 loss: -0.2563983201980591
Batch 9/64 loss: -0.25786882638931274
Batch 10/64 loss: -0.24802237749099731
Batch 11/64 loss: -0.27258729934692383
Batch 12/64 loss: -0.26039379835128784
Batch 13/64 loss: -0.27038103342056274
Batch 14/64 loss: -0.27176594734191895
Batch 15/64 loss: -0.25902432203292847
Batch 16/64 loss: -0.25606536865234375
Batch 17/64 loss: -0.26216375827789307
Batch 18/64 loss: -0.2736203074455261
Batch 19/64 loss: -0.2653542757034302
Batch 20/64 loss: -0.2674823999404907
Batch 21/64 loss: -0.2765822410583496
Batch 22/64 loss: -0.273088276386261
Batch 23/64 loss: -0.27753162384033203
Batch 24/64 loss: -0.2642524242401123
Batch 25/64 loss: -0.2705399990081787
Batch 26/64 loss: -0.25564879179000854
Batch 27/64 loss: -0.2782405912876129
Batch 28/64 loss: -0.2740594744682312
Batch 29/64 loss: -0.2802131175994873
Batch 30/64 loss: -0.2809410095214844
Batch 31/64 loss: -0.2721874713897705
Batch 32/64 loss: -0.2630046010017395
Batch 33/64 loss: -0.28356534242630005
Batch 34/64 loss: -0.26788848638534546
Batch 35/64 loss: -0.28197911381721497
Batch 36/64 loss: -0.2799754738807678
Batch 37/64 loss: -0.2728922963142395
Batch 38/64 loss: -0.27047109603881836
Batch 39/64 loss: -0.2626168131828308
Batch 40/64 loss: -0.2702687978744507
Batch 41/64 loss: -0.2526300549507141
Batch 42/64 loss: -0.26094377040863037
Batch 43/64 loss: -0.26548337936401367
Batch 44/64 loss: -0.2664859890937805
Batch 45/64 loss: -0.2640443444252014
Batch 46/64 loss: -0.25335609912872314
Batch 47/64 loss: -0.2613025903701782
Batch 48/64 loss: -0.26680487394332886
Batch 49/64 loss: -0.26306605339050293
Batch 50/64 loss: -0.26022422313690186
Batch 51/64 loss: -0.2523113489151001
Batch 52/64 loss: -0.25384461879730225
Batch 53/64 loss: -0.24270987510681152
Batch 54/64 loss: -0.25261110067367554
Batch 55/64 loss: -0.2727309465408325
Batch 56/64 loss: -0.25596362352371216
Batch 57/64 loss: -0.2567121386528015
Batch 58/64 loss: -0.27665287256240845
Batch 59/64 loss: -0.2660825252532959
Batch 60/64 loss: -0.27238714694976807
Batch 61/64 loss: -0.25844812393188477
Batch 62/64 loss: -0.2679632306098938
Batch 63/64 loss: -0.2681594491004944
Batch 64/64 loss: -0.2729886770248413
Epoch 198  Train loss: -0.2642159345103245  Val loss: -0.23824985494318696
Epoch 199
-------------------------------
Batch 1/64 loss: -0.27639609575271606
Batch 2/64 loss: -0.2710185647010803
Batch 3/64 loss: -0.2666783928871155
Batch 4/64 loss: -0.2608315944671631
Batch 5/64 loss: -0.27104413509368896
Batch 6/64 loss: -0.26055973768234253
Batch 7/64 loss: -0.2588386535644531
Batch 8/64 loss: -0.25490719079971313
Batch 9/64 loss: -0.2640073895454407
Batch 10/64 loss: -0.2825009226799011
Batch 11/64 loss: -0.2837628424167633
Batch 12/64 loss: -0.253322958946228
Batch 13/64 loss: -0.26759952306747437
Batch 14/64 loss: -0.2631474733352661
Batch 15/64 loss: -0.2563592195510864
Batch 16/64 loss: -0.23894524574279785
Batch 17/64 loss: -0.2752058506011963
Batch 18/64 loss: -0.26672810316085815
Batch 19/64 loss: -0.24536550045013428
Batch 20/64 loss: -0.2705601453781128
Batch 21/64 loss: -0.26085078716278076
Batch 22/64 loss: -0.27679726481437683
Batch 23/64 loss: -0.2809443473815918
Batch 24/64 loss: -0.2804322838783264
Batch 25/64 loss: -0.2655310034751892
Batch 26/64 loss: -0.28007635474205017
Batch 27/64 loss: -0.27261048555374146
Batch 28/64 loss: -0.2747229337692261
Batch 29/64 loss: -0.27110421657562256
Batch 30/64 loss: -0.27344250679016113
Batch 31/64 loss: -0.281227707862854
Batch 32/64 loss: -0.2733246684074402
Batch 33/64 loss: -0.2752766013145447
Batch 34/64 loss: -0.2616667151451111
Batch 35/64 loss: -0.27121490240097046
Batch 36/64 loss: -0.2609969973564148
Batch 37/64 loss: -0.26347142457962036
Batch 38/64 loss: -0.2673265337944031
Batch 39/64 loss: -0.27910804748535156
Batch 40/64 loss: -0.26932215690612793
Batch 41/64 loss: -0.26976442337036133
Batch 42/64 loss: -0.2741926312446594
Batch 43/64 loss: -0.2717403173446655
Batch 44/64 loss: -0.2552807331085205
Batch 45/64 loss: -0.27408620715141296
Batch 46/64 loss: -0.26083123683929443
Batch 47/64 loss: -0.27153539657592773
Batch 48/64 loss: -0.2728425860404968
Batch 49/64 loss: -0.26224321126937866
Batch 50/64 loss: -0.2784586548805237
Batch 51/64 loss: -0.2796722650527954
Batch 52/64 loss: -0.2619680166244507
Batch 53/64 loss: -0.2770712375640869
Batch 54/64 loss: -0.27543890476226807
Batch 55/64 loss: -0.2709118127822876
Batch 56/64 loss: -0.2756393849849701
Batch 57/64 loss: -0.27211183309555054
Batch 58/64 loss: -0.28022411465644836
Batch 59/64 loss: -0.25640588998794556
Batch 60/64 loss: -0.26926207542419434
Batch 61/64 loss: -0.27010512351989746
Batch 62/64 loss: -0.28479471802711487
Batch 63/64 loss: -0.2767033576965332
Batch 64/64 loss: -0.2717118263244629
Epoch 199  Train loss: -0.2693693408779069  Val loss: -0.24648540700014515
Epoch 200
-------------------------------
Batch 1/64 loss: -0.27141666412353516
Batch 2/64 loss: -0.2556028366088867
Batch 3/64 loss: -0.26540201902389526
Batch 4/64 loss: -0.27432334423065186
Batch 5/64 loss: -0.25488507747650146
Batch 6/64 loss: -0.2741413712501526
Batch 7/64 loss: -0.27127164602279663
Batch 8/64 loss: -0.27452218532562256
Batch 9/64 loss: -0.2759864330291748
Batch 10/64 loss: -0.27093756198883057
Batch 11/64 loss: -0.2772033214569092
Batch 12/64 loss: -0.2564314603805542
Batch 13/64 loss: -0.25835496187210083
Batch 14/64 loss: -0.2779955267906189
Batch 15/64 loss: -0.26614099740982056
Batch 16/64 loss: -0.26806753873825073
Batch 17/64 loss: -0.25826990604400635
Batch 18/64 loss: -0.26781153678894043
Batch 19/64 loss: -0.2692067623138428
Batch 20/64 loss: -0.2675613760948181
Batch 21/64 loss: -0.2600995898246765
Batch 22/64 loss: -0.2630479335784912
Batch 23/64 loss: -0.2541053295135498
Batch 24/64 loss: -0.2715754508972168
Batch 25/64 loss: -0.2623368501663208
Batch 26/64 loss: -0.25286781787872314
Batch 27/64 loss: -0.2759701609611511
Batch 28/64 loss: -0.2630044221878052
Batch 29/64 loss: -0.26307952404022217
Batch 30/64 loss: -0.2723081111907959
Batch 31/64 loss: -0.2708512544631958
Batch 32/64 loss: -0.26480722427368164
Batch 33/64 loss: -0.2753097414970398
Batch 34/64 loss: -0.2551204562187195
Batch 35/64 loss: -0.259723961353302
Batch 36/64 loss: -0.2702721357345581
Batch 37/64 loss: -0.27096444368362427
Batch 38/64 loss: -0.27018511295318604
Batch 39/64 loss: -0.27191829681396484
Batch 40/64 loss: -0.26445239782333374
Batch 41/64 loss: -0.26531553268432617
Batch 42/64 loss: -0.27421316504478455
Batch 43/64 loss: -0.2371203899383545
Batch 44/64 loss: -0.2585136890411377
Batch 45/64 loss: -0.2597740888595581
Batch 46/64 loss: -0.2598021626472473
Batch 47/64 loss: -0.2774904668331146
Batch 48/64 loss: -0.26956021785736084
Batch 49/64 loss: -0.2714499235153198
Batch 50/64 loss: -0.26778626441955566
Batch 51/64 loss: -0.2484591007232666
Batch 52/64 loss: -0.286064475774765
Batch 53/64 loss: -0.2644105553627014
Batch 54/64 loss: -0.26814889907836914
Batch 55/64 loss: -0.2681173086166382
Batch 56/64 loss: -0.25570350885391235
Batch 57/64 loss: -0.27280789613723755
Batch 58/64 loss: -0.2643689513206482
Batch 59/64 loss: -0.2602652311325073
Batch 60/64 loss: -0.2521037459373474
Batch 61/64 loss: -0.26200515031814575
Batch 62/64 loss: -0.2584472894668579
Batch 63/64 loss: -0.2626684308052063
Batch 64/64 loss: -0.26135218143463135
Epoch 200  Train loss: -0.26553950309753416  Val loss: -0.2395884980450791
Epoch 201
-------------------------------
Batch 1/64 loss: -0.267153799533844
Batch 2/64 loss: -0.26747316122055054
Batch 3/64 loss: -0.2810467481613159
Batch 4/64 loss: -0.25754034519195557
Batch 5/64 loss: -0.26056963205337524
Batch 6/64 loss: -0.27056455612182617
Batch 7/64 loss: -0.273395836353302
Batch 8/64 loss: -0.26003289222717285
Batch 9/64 loss: -0.271178662776947
Batch 10/64 loss: -0.2733801007270813
Batch 11/64 loss: -0.2476043701171875
Batch 12/64 loss: -0.2497987151145935
Batch 13/64 loss: -0.2575194835662842
Batch 14/64 loss: -0.2703239917755127
Batch 15/64 loss: -0.26118040084838867
Batch 16/64 loss: -0.2682541012763977
Batch 17/64 loss: -0.25468164682388306
Batch 18/64 loss: -0.27873194217681885
Batch 19/64 loss: -0.2578946352005005
Batch 20/64 loss: -0.2646228075027466
Batch 21/64 loss: -0.2599785327911377
Batch 22/64 loss: -0.28062891960144043
Batch 23/64 loss: -0.25477373600006104
Batch 24/64 loss: -0.26501572132110596
Batch 25/64 loss: -0.2690966725349426
Batch 26/64 loss: -0.27158108353614807
Batch 27/64 loss: -0.25313401222229004
Batch 28/64 loss: -0.2815456986427307
Batch 29/64 loss: -0.28271836042404175
Batch 30/64 loss: -0.27250194549560547
Batch 31/64 loss: -0.26877361536026
Batch 32/64 loss: -0.25823718309402466
Batch 33/64 loss: -0.2830743193626404
Batch 34/64 loss: -0.2896515727043152
Batch 35/64 loss: -0.2554367780685425
Batch 36/64 loss: -0.27263712882995605
Batch 37/64 loss: -0.2517179250717163
Batch 38/64 loss: -0.22321641445159912
Batch 39/64 loss: -0.2704528570175171
Batch 40/64 loss: -0.2609660029411316
Batch 41/64 loss: -0.261127769947052
Batch 42/64 loss: -0.2660658359527588
Batch 43/64 loss: -0.258500874042511
Batch 44/64 loss: -0.26098889112472534
Batch 45/64 loss: -0.27044349908828735
Batch 46/64 loss: -0.2633568048477173
Batch 47/64 loss: -0.26760244369506836
Batch 48/64 loss: -0.26247525215148926
Batch 49/64 loss: -0.27626872062683105
Batch 50/64 loss: -0.2732568383216858
Batch 51/64 loss: -0.2578887939453125
Batch 52/64 loss: -0.2809719443321228
Batch 53/64 loss: -0.2710813879966736
Batch 54/64 loss: -0.27263951301574707
Batch 55/64 loss: -0.26820337772369385
Batch 56/64 loss: -0.2595174312591553
Batch 57/64 loss: -0.26522672176361084
Batch 58/64 loss: -0.24425309896469116
Batch 59/64 loss: -0.2657819986343384
Batch 60/64 loss: -0.23719370365142822
Batch 61/64 loss: -0.26974397897720337
Batch 62/64 loss: -0.28044313192367554
Batch 63/64 loss: -0.26101571321487427
Batch 64/64 loss: -0.25483500957489014
Epoch 201  Train loss: -0.26514918102937585  Val loss: -0.22225797749876566
Epoch 202
-------------------------------
Batch 1/64 loss: -0.27749061584472656
Batch 2/64 loss: -0.26466310024261475
Batch 3/64 loss: -0.2672250270843506
Batch 4/64 loss: -0.24650108814239502
Batch 5/64 loss: -0.2751498222351074
Batch 6/64 loss: -0.27067697048187256
Batch 7/64 loss: -0.26354122161865234
Batch 8/64 loss: -0.2597948908805847
Batch 9/64 loss: -0.2561248540878296
Batch 10/64 loss: -0.25896507501602173
Batch 11/64 loss: -0.2697907090187073
Batch 12/64 loss: -0.2731204628944397
Batch 13/64 loss: -0.28473299741744995
Batch 14/64 loss: -0.2623588442802429
Batch 15/64 loss: -0.21771061420440674
Batch 16/64 loss: -0.2852228581905365
Batch 17/64 loss: -0.2752298414707184
Batch 18/64 loss: -0.2492261528968811
Batch 19/64 loss: -0.25396066904067993
Batch 20/64 loss: -0.2801559567451477
Batch 21/64 loss: -0.2756175994873047
Batch 22/64 loss: -0.2563323378562927
Batch 23/64 loss: -0.2590177059173584
Batch 24/64 loss: -0.27957379817962646
Batch 25/64 loss: -0.261504590511322
Batch 26/64 loss: -0.2600102424621582
Batch 27/64 loss: -0.28228771686553955
Batch 28/64 loss: -0.2712319493293762
Batch 29/64 loss: -0.2570638656616211
Batch 30/64 loss: -0.2655339241027832
Batch 31/64 loss: -0.26794904470443726
Batch 32/64 loss: -0.2684946060180664
Batch 33/64 loss: -0.2566564679145813
Batch 34/64 loss: -0.26940590143203735
Batch 35/64 loss: -0.25386136770248413
Batch 36/64 loss: -0.2700155973434448
Batch 37/64 loss: -0.2500041723251343
Batch 38/64 loss: -0.26836031675338745
Batch 39/64 loss: -0.25671184062957764
Batch 40/64 loss: -0.26496392488479614
Batch 41/64 loss: -0.2642368674278259
Batch 42/64 loss: -0.26257210969924927
Batch 43/64 loss: -0.26442885398864746
Batch 44/64 loss: -0.27326422929763794
Batch 45/64 loss: -0.2662729024887085
Batch 46/64 loss: -0.2509147524833679
Batch 47/64 loss: -0.2546210289001465
Batch 48/64 loss: -0.27402716875076294
Batch 49/64 loss: -0.2697545289993286
Batch 50/64 loss: -0.27286070585250854
Batch 51/64 loss: -0.2619154453277588
Batch 52/64 loss: -0.2763764262199402
Batch 53/64 loss: -0.2781064510345459
Batch 54/64 loss: -0.27328479290008545
Batch 55/64 loss: -0.2727181911468506
Batch 56/64 loss: -0.2596592903137207
Batch 57/64 loss: -0.26365894079208374
Batch 58/64 loss: -0.24509602785110474
Batch 59/64 loss: -0.26351308822631836
Batch 60/64 loss: -0.2667195796966553
Batch 61/64 loss: -0.2510002851486206
Batch 62/64 loss: -0.25382888317108154
Batch 63/64 loss: -0.25351059436798096
Batch 64/64 loss: -0.24962174892425537
Epoch 202  Train loss: -0.2642477834925932  Val loss: -0.20067745098952985
Epoch 203
-------------------------------
Batch 1/64 loss: -0.24771207571029663
Batch 2/64 loss: -0.2634098529815674
Batch 3/64 loss: -0.2627184987068176
Batch 4/64 loss: -0.24549442529678345
Batch 5/64 loss: -0.2551735043525696
Batch 6/64 loss: -0.265061616897583
Batch 7/64 loss: -0.2656949758529663
Batch 8/64 loss: -0.2648335099220276
Batch 9/64 loss: -0.24351590871810913
Batch 10/64 loss: -0.2617524266242981
Batch 11/64 loss: -0.2601587176322937
Batch 12/64 loss: -0.2845528721809387
Batch 13/64 loss: -0.2665526866912842
Batch 14/64 loss: -0.26576781272888184
Batch 15/64 loss: -0.2564859390258789
Batch 16/64 loss: -0.23574090003967285
Batch 17/64 loss: -0.2298504114151001
Batch 18/64 loss: -0.24619567394256592
Batch 19/64 loss: -0.2595359683036804
Batch 20/64 loss: -0.2679826021194458
Batch 21/64 loss: -0.2638707160949707
Batch 22/64 loss: -0.26755446195602417
Batch 23/64 loss: -0.2705257534980774
Batch 24/64 loss: -0.2601373791694641
Batch 25/64 loss: -0.2566078305244446
Batch 26/64 loss: -0.2796782851219177
Batch 27/64 loss: -0.25542277097702026
Batch 28/64 loss: -0.2693406939506531
Batch 29/64 loss: -0.2661105990409851
Batch 30/64 loss: -0.2542889714241028
Batch 31/64 loss: -0.27928149700164795
Batch 32/64 loss: -0.2843266427516937
Batch 33/64 loss: -0.25158822536468506
Batch 34/64 loss: -0.2594003677368164
Batch 35/64 loss: -0.2604280114173889
Batch 36/64 loss: -0.26044905185699463
Batch 37/64 loss: -0.2506304979324341
Batch 38/64 loss: -0.2800177335739136
Batch 39/64 loss: -0.2541150450706482
Batch 40/64 loss: -0.25554484128952026
Batch 41/64 loss: -0.24608349800109863
Batch 42/64 loss: -0.26547181606292725
Batch 43/64 loss: -0.2464645504951477
Batch 44/64 loss: -0.2650291919708252
Batch 45/64 loss: -0.25766611099243164
Batch 46/64 loss: -0.25894057750701904
Batch 47/64 loss: -0.2716500163078308
Batch 48/64 loss: -0.25701117515563965
Batch 49/64 loss: -0.26818084716796875
Batch 50/64 loss: -0.26358675956726074
Batch 51/64 loss: -0.2687349319458008
Batch 52/64 loss: -0.26616793870925903
Batch 53/64 loss: -0.26601213216781616
Batch 54/64 loss: -0.2699928283691406
Batch 55/64 loss: -0.2787579596042633
Batch 56/64 loss: -0.26202714443206787
Batch 57/64 loss: -0.24585336446762085
Batch 58/64 loss: -0.2781611680984497
Batch 59/64 loss: -0.2611185908317566
Batch 60/64 loss: -0.2407674789428711
Batch 61/64 loss: -0.2530341148376465
Batch 62/64 loss: -0.27350252866744995
Batch 63/64 loss: -0.26544785499572754
Batch 64/64 loss: -0.25807952880859375
Epoch 203  Train loss: -0.2611879526400099  Val loss: -0.22898338422742495
Epoch 204
-------------------------------
Batch 1/64 loss: -0.26886677742004395
Batch 2/64 loss: -0.2595144510269165
Batch 3/64 loss: -0.25602638721466064
Batch 4/64 loss: -0.27156370878219604
Batch 5/64 loss: -0.27205145359039307
Batch 6/64 loss: -0.2580353617668152
Batch 7/64 loss: -0.2590603232383728
Batch 8/64 loss: -0.24665695428848267
Batch 9/64 loss: -0.27375829219818115
Batch 10/64 loss: -0.2637125253677368
Batch 11/64 loss: -0.2826825678348541
Batch 12/64 loss: -0.2624121904373169
Batch 13/64 loss: -0.2652190327644348
Batch 14/64 loss: -0.26291126012802124
Batch 15/64 loss: -0.2542586326599121
Batch 16/64 loss: -0.25938087701797485
Batch 17/64 loss: -0.2485663890838623
Batch 18/64 loss: -0.26823925971984863
Batch 19/64 loss: -0.2700803279876709
Batch 20/64 loss: -0.2657439112663269
Batch 21/64 loss: -0.25893664360046387
Batch 22/64 loss: -0.26530420780181885
Batch 23/64 loss: -0.26353657245635986
Batch 24/64 loss: -0.2732294201850891
Batch 25/64 loss: -0.25997549295425415
Batch 26/64 loss: -0.2707368731498718
Batch 27/64 loss: -0.258628249168396
Batch 28/64 loss: -0.2684280276298523
Batch 29/64 loss: -0.2638031244277954
Batch 30/64 loss: -0.2568219304084778
Batch 31/64 loss: -0.27161091566085815
Batch 32/64 loss: -0.27160757780075073
Batch 33/64 loss: -0.2612614035606384
Batch 34/64 loss: -0.26985621452331543
Batch 35/64 loss: -0.24447757005691528
Batch 36/64 loss: -0.27277642488479614
Batch 37/64 loss: -0.25905847549438477
Batch 38/64 loss: -0.2772587537765503
Batch 39/64 loss: -0.26425665616989136
Batch 40/64 loss: -0.268130898475647
Batch 41/64 loss: -0.23942089080810547
Batch 42/64 loss: -0.2581731677055359
Batch 43/64 loss: -0.24926888942718506
Batch 44/64 loss: -0.26181870698928833
Batch 45/64 loss: -0.27080440521240234
Batch 46/64 loss: -0.25338447093963623
Batch 47/64 loss: -0.25502246618270874
Batch 48/64 loss: -0.25967586040496826
Batch 49/64 loss: -0.26564013957977295
Batch 50/64 loss: -0.25882601737976074
Batch 51/64 loss: -0.2742716670036316
Batch 52/64 loss: -0.27722787857055664
Batch 53/64 loss: -0.24235910177230835
Batch 54/64 loss: -0.27248358726501465
Batch 55/64 loss: -0.26893824338912964
Batch 56/64 loss: -0.2677983045578003
Batch 57/64 loss: -0.27930229902267456
Batch 58/64 loss: -0.24737030267715454
Batch 59/64 loss: -0.27038151025772095
Batch 60/64 loss: -0.2697839140892029
Batch 61/64 loss: -0.26543354988098145
Batch 62/64 loss: -0.27130377292633057
Batch 63/64 loss: -0.2504003643989563
Batch 64/64 loss: -0.24763202667236328
Epoch 204  Train loss: -0.26326666112039604  Val loss: -0.23961622731382495
Epoch 205
-------------------------------
Batch 1/64 loss: -0.2562345266342163
Batch 2/64 loss: -0.24227595329284668
Batch 3/64 loss: -0.24983632564544678
Batch 4/64 loss: -0.2580534815788269
Batch 5/64 loss: -0.2677454352378845
Batch 6/64 loss: -0.2770630121231079
Batch 7/64 loss: -0.25514066219329834
Batch 8/64 loss: -0.24876099824905396
Batch 9/64 loss: -0.24874866008758545
Batch 10/64 loss: -0.2725651264190674
Batch 11/64 loss: -0.26564115285873413
Batch 12/64 loss: -0.2617189884185791
Batch 13/64 loss: -0.2726946473121643
Batch 14/64 loss: -0.2910833954811096
Batch 15/64 loss: -0.27389705181121826
Batch 16/64 loss: -0.26916295289993286
Batch 17/64 loss: -0.2699548006057739
Batch 18/64 loss: -0.2606942653656006
Batch 19/64 loss: -0.2787187695503235
Batch 20/64 loss: -0.2736679017543793
Batch 21/64 loss: -0.265081524848938
Batch 22/64 loss: -0.26719051599502563
Batch 23/64 loss: -0.2665700912475586
Batch 24/64 loss: -0.2643257975578308
Batch 25/64 loss: -0.2505109906196594
Batch 26/64 loss: -0.25995922088623047
Batch 27/64 loss: -0.2560347318649292
Batch 28/64 loss: -0.2563149333000183
Batch 29/64 loss: -0.256941556930542
Batch 30/64 loss: -0.27624475955963135
Batch 31/64 loss: -0.2783374786376953
Batch 32/64 loss: -0.24440103769302368
Batch 33/64 loss: -0.26225554943084717
Batch 34/64 loss: -0.2661117911338806
Batch 35/64 loss: -0.2595762610435486
Batch 36/64 loss: -0.25146812200546265
Batch 37/64 loss: -0.241912841796875
Batch 38/64 loss: -0.254597008228302
Batch 39/64 loss: -0.2726694345474243
Batch 40/64 loss: -0.2494211196899414
Batch 41/64 loss: -0.2585642337799072
Batch 42/64 loss: -0.25393491983413696
Batch 43/64 loss: -0.257393479347229
Batch 44/64 loss: -0.2569504976272583
Batch 45/64 loss: -0.28014808893203735
Batch 46/64 loss: -0.2515134811401367
Batch 47/64 loss: -0.26440829038619995
Batch 48/64 loss: -0.25331389904022217
Batch 49/64 loss: -0.28083741664886475
Batch 50/64 loss: -0.26781266927719116
Batch 51/64 loss: -0.2648054361343384
Batch 52/64 loss: -0.2605018615722656
Batch 53/64 loss: -0.2588244676589966
Batch 54/64 loss: -0.2524425983428955
Batch 55/64 loss: -0.27201521396636963
Batch 56/64 loss: -0.26689743995666504
Batch 57/64 loss: -0.27120208740234375
Batch 58/64 loss: -0.26673072576522827
Batch 59/64 loss: -0.2675674557685852
Batch 60/64 loss: -0.2588694095611572
Batch 61/64 loss: -0.278399795293808
Batch 62/64 loss: -0.2674649953842163
Batch 63/64 loss: -0.27294111251831055
Batch 64/64 loss: -0.26422613859176636
Epoch 205  Train loss: -0.2631418361383326  Val loss: -0.2360325773147373
Epoch 206
-------------------------------
Batch 1/64 loss: -0.2581469416618347
Batch 2/64 loss: -0.27445143461227417
Batch 3/64 loss: -0.2643554210662842
Batch 4/64 loss: -0.26975345611572266
Batch 5/64 loss: -0.27113544940948486
Batch 6/64 loss: -0.26615869998931885
Batch 7/64 loss: -0.2821662127971649
Batch 8/64 loss: -0.26219528913497925
Batch 9/64 loss: -0.27142828702926636
Batch 10/64 loss: -0.27343374490737915
Batch 11/64 loss: -0.25184166431427
Batch 12/64 loss: -0.27001410722732544
Batch 13/64 loss: -0.2618178129196167
Batch 14/64 loss: -0.27546048164367676
Batch 15/64 loss: -0.259274959564209
Batch 16/64 loss: -0.2698936462402344
Batch 17/64 loss: -0.2614904046058655
Batch 18/64 loss: -0.2706507444381714
Batch 19/64 loss: -0.28687959909439087
Batch 20/64 loss: -0.2675396800041199
Batch 21/64 loss: -0.2539544105529785
Batch 22/64 loss: -0.26554763317108154
Batch 23/64 loss: -0.26990824937820435
Batch 24/64 loss: -0.2651669979095459
Batch 25/64 loss: -0.271186888217926
Batch 26/64 loss: -0.26725155115127563
Batch 27/64 loss: -0.2456950545310974
Batch 28/64 loss: -0.2706126570701599
Batch 29/64 loss: -0.2713192105293274
Batch 30/64 loss: -0.2598661184310913
Batch 31/64 loss: -0.27217769622802734
Batch 32/64 loss: -0.2838444709777832
Batch 33/64 loss: -0.2647474408149719
Batch 34/64 loss: -0.26571375131607056
Batch 35/64 loss: -0.2634314298629761
Batch 36/64 loss: -0.2767161428928375
Batch 37/64 loss: -0.2602829337120056
Batch 38/64 loss: -0.27073824405670166
Batch 39/64 loss: -0.2747490406036377
Batch 40/64 loss: -0.2508077025413513
Batch 41/64 loss: -0.27136749029159546
Batch 42/64 loss: -0.27277708053588867
Batch 43/64 loss: -0.25112760066986084
Batch 44/64 loss: -0.2605173587799072
Batch 45/64 loss: -0.2589725852012634
Batch 46/64 loss: -0.2724284529685974
Batch 47/64 loss: -0.24331563711166382
Batch 48/64 loss: -0.26729416847229004
Batch 49/64 loss: -0.2577016353607178
Batch 50/64 loss: -0.2606027126312256
Batch 51/64 loss: -0.26772648096084595
Batch 52/64 loss: -0.2696135640144348
Batch 53/64 loss: -0.28311657905578613
Batch 54/64 loss: -0.2641063928604126
Batch 55/64 loss: -0.2716064453125
Batch 56/64 loss: -0.2790929079055786
Batch 57/64 loss: -0.254541277885437
Batch 58/64 loss: -0.25078821182250977
Batch 59/64 loss: -0.25319576263427734
Batch 60/64 loss: -0.2751373052597046
Batch 61/64 loss: -0.28605660796165466
Batch 62/64 loss: -0.25602054595947266
Batch 63/64 loss: -0.2753133177757263
Batch 64/64 loss: -0.2750299870967865
Epoch 206  Train loss: -0.26667448270554633  Val loss: -0.24407368365841633
Epoch 207
-------------------------------
Batch 1/64 loss: -0.26995527744293213
Batch 2/64 loss: -0.2624800205230713
Batch 3/64 loss: -0.2639129161834717
Batch 4/64 loss: -0.2554633617401123
Batch 5/64 loss: -0.2734440565109253
Batch 6/64 loss: -0.2640841007232666
Batch 7/64 loss: -0.27716749906539917
Batch 8/64 loss: -0.2630712389945984
Batch 9/64 loss: -0.25941240787506104
Batch 10/64 loss: -0.2736937999725342
Batch 11/64 loss: -0.27317148447036743
Batch 12/64 loss: -0.2712252140045166
Batch 13/64 loss: -0.27324604988098145
Batch 14/64 loss: -0.27390235662460327
Batch 15/64 loss: -0.2739010751247406
Batch 16/64 loss: -0.26168978214263916
Batch 17/64 loss: -0.2687404155731201
Batch 18/64 loss: -0.27292531728744507
Batch 19/64 loss: -0.26381397247314453
Batch 20/64 loss: -0.27664679288864136
Batch 21/64 loss: -0.2741812467575073
Batch 22/64 loss: -0.2596437931060791
Batch 23/64 loss: -0.2625986933708191
Batch 24/64 loss: -0.2746625542640686
Batch 25/64 loss: -0.27217137813568115
Batch 26/64 loss: -0.26562070846557617
Batch 27/64 loss: -0.2708149552345276
Batch 28/64 loss: -0.2509363889694214
Batch 29/64 loss: -0.2749582529067993
Batch 30/64 loss: -0.2397022247314453
Batch 31/64 loss: -0.2634090781211853
Batch 32/64 loss: -0.2864527702331543
Batch 33/64 loss: -0.25600939989089966
Batch 34/64 loss: -0.2585031986236572
Batch 35/64 loss: -0.2686206102371216
Batch 36/64 loss: -0.25870203971862793
Batch 37/64 loss: -0.2663182020187378
Batch 38/64 loss: -0.2634791135787964
Batch 39/64 loss: -0.2631741166114807
Batch 40/64 loss: -0.24833130836486816
Batch 41/64 loss: -0.2740510106086731
Batch 42/64 loss: -0.2781499922275543
Batch 43/64 loss: -0.2697756290435791
Batch 44/64 loss: -0.2684791088104248
Batch 45/64 loss: -0.2671383023262024
Batch 46/64 loss: -0.2543461322784424
Batch 47/64 loss: -0.2553192377090454
Batch 48/64 loss: -0.27077221870422363
Batch 49/64 loss: -0.25988447666168213
Batch 50/64 loss: -0.2700079083442688
Batch 51/64 loss: -0.23683345317840576
Batch 52/64 loss: -0.27127933502197266
Batch 53/64 loss: -0.2715040445327759
Batch 54/64 loss: -0.26009100675582886
Batch 55/64 loss: -0.2761804163455963
Batch 56/64 loss: -0.2695040702819824
Batch 57/64 loss: -0.2662436366081238
Batch 58/64 loss: -0.2644385099411011
Batch 59/64 loss: -0.27762743830680847
Batch 60/64 loss: -0.26138079166412354
Batch 61/64 loss: -0.23704439401626587
Batch 62/64 loss: -0.2635883688926697
Batch 63/64 loss: -0.26196521520614624
Batch 64/64 loss: -0.25945597887039185
Epoch 207  Train loss: -0.2655754330111485  Val loss: -0.23445597403647564
Epoch 208
-------------------------------
Batch 1/64 loss: -0.2758467197418213
Batch 2/64 loss: -0.2735212743282318
Batch 3/64 loss: -0.28134143352508545
Batch 4/64 loss: -0.2674301862716675
Batch 5/64 loss: -0.2596736550331116
Batch 6/64 loss: -0.27563178539276123
Batch 7/64 loss: -0.2636338472366333
Batch 8/64 loss: -0.2741262912750244
Batch 9/64 loss: -0.25817394256591797
Batch 10/64 loss: -0.27768146991729736
Batch 11/64 loss: -0.2587251663208008
Batch 12/64 loss: -0.2745971083641052
Batch 13/64 loss: -0.26999038457870483
Batch 14/64 loss: -0.2519047260284424
Batch 15/64 loss: -0.27181142568588257
Batch 16/64 loss: -0.2719508409500122
Batch 17/64 loss: -0.259935200214386
Batch 18/64 loss: -0.2779790759086609
Batch 19/64 loss: -0.267548143863678
Batch 20/64 loss: -0.2652548551559448
Batch 21/64 loss: -0.26967889070510864
Batch 22/64 loss: -0.26172876358032227
Batch 23/64 loss: -0.26224273443222046
Batch 24/64 loss: -0.2563280463218689
Batch 25/64 loss: -0.27908527851104736
Batch 26/64 loss: -0.27114832401275635
Batch 27/64 loss: -0.26398026943206787
Batch 28/64 loss: -0.2599796652793884
Batch 29/64 loss: -0.26319485902786255
Batch 30/64 loss: -0.26124829053878784
Batch 31/64 loss: -0.28381145000457764
Batch 32/64 loss: -0.27527832984924316
Batch 33/64 loss: -0.2688565254211426
Batch 34/64 loss: -0.26053404808044434
Batch 35/64 loss: -0.2641603350639343
Batch 36/64 loss: -0.27506810426712036
Batch 37/64 loss: -0.25693273544311523
Batch 38/64 loss: -0.2764803171157837
Batch 39/64 loss: -0.2494823932647705
Batch 40/64 loss: -0.2622985243797302
Batch 41/64 loss: -0.26255565881729126
Batch 42/64 loss: -0.2825767993927002
Batch 43/64 loss: -0.2660127282142639
Batch 44/64 loss: -0.27589714527130127
Batch 45/64 loss: -0.2702575922012329
Batch 46/64 loss: -0.2709817886352539
Batch 47/64 loss: -0.25236058235168457
Batch 48/64 loss: -0.26059699058532715
Batch 49/64 loss: -0.2646244168281555
Batch 50/64 loss: -0.2635418176651001
Batch 51/64 loss: -0.2639652490615845
Batch 52/64 loss: -0.27408158779144287
Batch 53/64 loss: -0.28152722120285034
Batch 54/64 loss: -0.25904643535614014
Batch 55/64 loss: -0.27756625413894653
Batch 56/64 loss: -0.24926036596298218
Batch 57/64 loss: -0.2771961987018585
Batch 58/64 loss: -0.27182459831237793
Batch 59/64 loss: -0.2734435200691223
Batch 60/64 loss: -0.27741071581840515
Batch 61/64 loss: -0.28216850757598877
Batch 62/64 loss: -0.2790970206260681
Batch 63/64 loss: -0.25529634952545166
Batch 64/64 loss: -0.2714281678199768
Epoch 208  Train loss: -0.2681276248950584  Val loss: -0.24876352089786857
Saving best model, epoch: 208
Epoch 209
-------------------------------
Batch 1/64 loss: -0.27037978172302246
Batch 2/64 loss: -0.25912541151046753
Batch 3/64 loss: -0.2593644857406616
Batch 4/64 loss: -0.25824427604675293
Batch 5/64 loss: -0.2801091969013214
Batch 6/64 loss: -0.2817925810813904
Batch 7/64 loss: -0.28053751587867737
Batch 8/64 loss: -0.26313692331314087
Batch 9/64 loss: -0.267792284488678
Batch 10/64 loss: -0.2627948522567749
Batch 11/64 loss: -0.26741838455200195
Batch 12/64 loss: -0.25144052505493164
Batch 13/64 loss: -0.27725544571876526
Batch 14/64 loss: -0.2787226438522339
Batch 15/64 loss: -0.2690232992172241
Batch 16/64 loss: -0.2783123552799225
Batch 17/64 loss: -0.2719517946243286
Batch 18/64 loss: -0.2716982364654541
Batch 19/64 loss: -0.266390323638916
Batch 20/64 loss: -0.2707837224006653
Batch 21/64 loss: -0.2599549889564514
Batch 22/64 loss: -0.2526552677154541
Batch 23/64 loss: -0.2674434781074524
Batch 24/64 loss: -0.2706880569458008
Batch 25/64 loss: -0.2575783133506775
Batch 26/64 loss: -0.27242302894592285
Batch 27/64 loss: -0.27851414680480957
Batch 28/64 loss: -0.2766726613044739
Batch 29/64 loss: -0.27048516273498535
Batch 30/64 loss: -0.24167507886886597
Batch 31/64 loss: -0.28026920557022095
Batch 32/64 loss: -0.2774001359939575
Batch 33/64 loss: -0.27362340688705444
Batch 34/64 loss: -0.2808470129966736
Batch 35/64 loss: -0.26252782344818115
Batch 36/64 loss: -0.26260048151016235
Batch 37/64 loss: -0.25667351484298706
Batch 38/64 loss: -0.2827596366405487
Batch 39/64 loss: -0.2520483732223511
Batch 40/64 loss: -0.2800758481025696
Batch 41/64 loss: -0.2801116704940796
Batch 42/64 loss: -0.26710712909698486
Batch 43/64 loss: -0.2722288966178894
Batch 44/64 loss: -0.2656620740890503
Batch 45/64 loss: -0.255507230758667
Batch 46/64 loss: -0.25315630435943604
Batch 47/64 loss: -0.2498706579208374
Batch 48/64 loss: -0.27837926149368286
Batch 49/64 loss: -0.2562558650970459
Batch 50/64 loss: -0.27033185958862305
Batch 51/64 loss: -0.26905250549316406
Batch 52/64 loss: -0.26513683795928955
Batch 53/64 loss: -0.2710378170013428
Batch 54/64 loss: -0.27097976207733154
Batch 55/64 loss: -0.2711591124534607
Batch 56/64 loss: -0.2743597626686096
Batch 57/64 loss: -0.28450632095336914
Batch 58/64 loss: -0.2677309513092041
Batch 59/64 loss: -0.2706764340400696
Batch 60/64 loss: -0.27397722005844116
Batch 61/64 loss: -0.25570857524871826
Batch 62/64 loss: -0.24919795989990234
Batch 63/64 loss: -0.2634196877479553
Batch 64/64 loss: -0.26963603496551514
Epoch 209  Train loss: -0.267936793495627  Val loss: -0.20667901542997852
Epoch 210
-------------------------------
Batch 1/64 loss: -0.2569456696510315
Batch 2/64 loss: -0.269292950630188
Batch 3/64 loss: -0.26036179065704346
Batch 4/64 loss: -0.25555557012557983
Batch 5/64 loss: -0.2662416100502014
Batch 6/64 loss: -0.2774675488471985
Batch 7/64 loss: -0.25603628158569336
Batch 8/64 loss: -0.26716333627700806
Batch 9/64 loss: -0.24745237827301025
Batch 10/64 loss: -0.27829211950302124
Batch 11/64 loss: -0.2602233290672302
Batch 12/64 loss: -0.25889670848846436
Batch 13/64 loss: -0.269115686416626
Batch 14/64 loss: -0.2483246922492981
Batch 15/64 loss: -0.28335222601890564
Batch 16/64 loss: -0.26951515674591064
Batch 17/64 loss: -0.25678980350494385
Batch 18/64 loss: -0.2685436010360718
Batch 19/64 loss: -0.2801257371902466
Batch 20/64 loss: -0.2559163570404053
Batch 21/64 loss: -0.2728036046028137
Batch 22/64 loss: -0.26135820150375366
Batch 23/64 loss: -0.2744280695915222
Batch 24/64 loss: -0.27655214071273804
Batch 25/64 loss: -0.28181737661361694
Batch 26/64 loss: -0.2795942425727844
Batch 27/64 loss: -0.2446955442428589
Batch 28/64 loss: -0.28592732548713684
Batch 29/64 loss: -0.2632427215576172
Batch 30/64 loss: -0.27147841453552246
Batch 31/64 loss: -0.2613934874534607
Batch 32/64 loss: -0.26413875818252563
Batch 33/64 loss: -0.284353107213974
Batch 34/64 loss: -0.27599090337753296
Batch 35/64 loss: -0.2689645290374756
Batch 36/64 loss: -0.2592838406562805
Batch 37/64 loss: -0.27018898725509644
Batch 38/64 loss: -0.27391910552978516
Batch 39/64 loss: -0.2726033329963684
Batch 40/64 loss: -0.23081082105636597
Batch 41/64 loss: -0.26227569580078125
Batch 42/64 loss: -0.2719096541404724
Batch 43/64 loss: -0.25179004669189453
Batch 44/64 loss: -0.2614452838897705
Batch 45/64 loss: -0.2615099549293518
Batch 46/64 loss: -0.27604109048843384
Batch 47/64 loss: -0.26894986629486084
Batch 48/64 loss: -0.2742655873298645
Batch 49/64 loss: -0.2622690796852112
Batch 50/64 loss: -0.26029324531555176
Batch 51/64 loss: -0.2607150673866272
Batch 52/64 loss: -0.2811022400856018
Batch 53/64 loss: -0.2594107985496521
Batch 54/64 loss: -0.27499663829803467
Batch 55/64 loss: -0.27763915061950684
Batch 56/64 loss: -0.2845512330532074
Batch 57/64 loss: -0.2908816635608673
Batch 58/64 loss: -0.27439743280410767
Batch 59/64 loss: -0.26610493659973145
Batch 60/64 loss: -0.27163177728652954
Batch 61/64 loss: -0.25122612714767456
Batch 62/64 loss: -0.27460211515426636
Batch 63/64 loss: -0.2706935405731201
Batch 64/64 loss: -0.2612670063972473
Epoch 210  Train loss: -0.26735387526306453  Val loss: -0.23470957787176178
Epoch 211
-------------------------------
Batch 1/64 loss: -0.25992047786712646
Batch 2/64 loss: -0.2635902762413025
Batch 3/64 loss: -0.28243356943130493
Batch 4/64 loss: -0.28320688009262085
Batch 5/64 loss: -0.2658645510673523
Batch 6/64 loss: -0.27956444025039673
Batch 7/64 loss: -0.25748705863952637
Batch 8/64 loss: -0.2856442332267761
Batch 9/64 loss: -0.26440542936325073
Batch 10/64 loss: -0.25387656688690186
Batch 11/64 loss: -0.27545517683029175
Batch 12/64 loss: -0.2553524971008301
Batch 13/64 loss: -0.26734524965286255
Batch 14/64 loss: -0.2890391945838928
Batch 15/64 loss: -0.2765711545944214
Batch 16/64 loss: -0.2764089107513428
Batch 17/64 loss: -0.26726770401000977
Batch 18/64 loss: -0.27633798122406006
Batch 19/64 loss: -0.2676829695701599
Batch 20/64 loss: -0.2704702615737915
Batch 21/64 loss: -0.2702767848968506
Batch 22/64 loss: -0.2791784405708313
Batch 23/64 loss: -0.2485140562057495
Batch 24/64 loss: -0.26688051223754883
Batch 25/64 loss: -0.2643476128578186
Batch 26/64 loss: -0.27091318368911743
Batch 27/64 loss: -0.27031219005584717
Batch 28/64 loss: -0.2676352858543396
Batch 29/64 loss: -0.2634051442146301
Batch 30/64 loss: -0.2749404311180115
Batch 31/64 loss: -0.25822150707244873
Batch 32/64 loss: -0.249126136302948
Batch 33/64 loss: -0.2740020155906677
Batch 34/64 loss: -0.252200722694397
Batch 35/64 loss: -0.2688513398170471
Batch 36/64 loss: -0.2674823999404907
Batch 37/64 loss: -0.26506417989730835
Batch 38/64 loss: -0.26896023750305176
Batch 39/64 loss: -0.2590891122817993
Batch 40/64 loss: -0.28280171751976013
Batch 41/64 loss: -0.26434326171875
Batch 42/64 loss: -0.2676770091056824
Batch 43/64 loss: -0.2851998805999756
Batch 44/64 loss: -0.2541959285736084
Batch 45/64 loss: -0.2692018747329712
Batch 46/64 loss: -0.2635119557380676
Batch 47/64 loss: -0.258431613445282
Batch 48/64 loss: -0.26125508546829224
Batch 49/64 loss: -0.26949918270111084
Batch 50/64 loss: -0.2578909397125244
Batch 51/64 loss: -0.2520390748977661
Batch 52/64 loss: -0.2638741135597229
Batch 53/64 loss: -0.27554622292518616
Batch 54/64 loss: -0.2555125951766968
Batch 55/64 loss: -0.2710745334625244
Batch 56/64 loss: -0.2785574793815613
Batch 57/64 loss: -0.27332016825675964
Batch 58/64 loss: -0.25159329175949097
Batch 59/64 loss: -0.2609485387802124
Batch 60/64 loss: -0.28011155128479004
Batch 61/64 loss: -0.26864564418792725
Batch 62/64 loss: -0.25233161449432373
Batch 63/64 loss: -0.2732720375061035
Batch 64/64 loss: -0.27505743503570557
Epoch 211  Train loss: -0.2675208512474509  Val loss: -0.232584692358561
Epoch 212
-------------------------------
Batch 1/64 loss: -0.2688101530075073
Batch 2/64 loss: -0.24854683876037598
Batch 3/64 loss: -0.2522023916244507
Batch 4/64 loss: -0.2816470265388489
Batch 5/64 loss: -0.259731650352478
Batch 6/64 loss: -0.28624147176742554
Batch 7/64 loss: -0.24994349479675293
Batch 8/64 loss: -0.26799994707107544
Batch 9/64 loss: -0.2565367817878723
Batch 10/64 loss: -0.27380841970443726
Batch 11/64 loss: -0.2681318521499634
Batch 12/64 loss: -0.2656068205833435
Batch 13/64 loss: -0.2585090398788452
Batch 14/64 loss: -0.25541234016418457
Batch 15/64 loss: -0.2732906937599182
Batch 16/64 loss: -0.2677507996559143
Batch 17/64 loss: -0.2569009065628052
Batch 18/64 loss: -0.2560179829597473
Batch 19/64 loss: -0.26525449752807617
Batch 20/64 loss: -0.2655719518661499
Batch 21/64 loss: -0.2569435238838196
Batch 22/64 loss: -0.2690812945365906
Batch 23/64 loss: -0.2658902406692505
Batch 24/64 loss: -0.27428963780403137
Batch 25/64 loss: -0.26745468378067017
Batch 26/64 loss: -0.2700156569480896
Batch 27/64 loss: -0.2678227424621582
Batch 28/64 loss: -0.27351754903793335
Batch 29/64 loss: -0.272122323513031
Batch 30/64 loss: -0.26907771825790405
Batch 31/64 loss: -0.2612232565879822
Batch 32/64 loss: -0.27641913294792175
Batch 33/64 loss: -0.26338350772857666
Batch 34/64 loss: -0.27167677879333496
Batch 35/64 loss: -0.2634090185165405
Batch 36/64 loss: -0.2640799880027771
Batch 37/64 loss: -0.2524574398994446
Batch 38/64 loss: -0.26057523488998413
Batch 39/64 loss: -0.2579331398010254
Batch 40/64 loss: -0.26908230781555176
Batch 41/64 loss: -0.2644798755645752
Batch 42/64 loss: -0.27356278896331787
Batch 43/64 loss: -0.26706796884536743
Batch 44/64 loss: -0.25286537408828735
Batch 45/64 loss: -0.25488102436065674
Batch 46/64 loss: -0.25964921712875366
Batch 47/64 loss: -0.2613913416862488
Batch 48/64 loss: -0.2759498357772827
Batch 49/64 loss: -0.27608877420425415
Batch 50/64 loss: -0.28095996379852295
Batch 51/64 loss: -0.26378870010375977
Batch 52/64 loss: -0.2691396474838257
Batch 53/64 loss: -0.26602256298065186
Batch 54/64 loss: -0.25313693284988403
Batch 55/64 loss: -0.2854139506816864
Batch 56/64 loss: -0.2715689539909363
Batch 57/64 loss: -0.2622763514518738
Batch 58/64 loss: -0.2606896162033081
Batch 59/64 loss: -0.26253896951675415
Batch 60/64 loss: -0.24957340955734253
Batch 61/64 loss: -0.25293833017349243
Batch 62/64 loss: -0.26820188760757446
Batch 63/64 loss: -0.25755590200424194
Batch 64/64 loss: -0.25505417585372925
Epoch 212  Train loss: -0.26486905485975976  Val loss: -0.24027317741892182
Epoch 213
-------------------------------
Batch 1/64 loss: -0.2730144262313843
Batch 2/64 loss: -0.2631795406341553
Batch 3/64 loss: -0.2597576379776001
Batch 4/64 loss: -0.2717280387878418
Batch 5/64 loss: -0.2656477093696594
Batch 6/64 loss: -0.27074766159057617
Batch 7/64 loss: -0.26986420154571533
Batch 8/64 loss: -0.2672075629234314
Batch 9/64 loss: -0.26595836877822876
Batch 10/64 loss: -0.26489514112472534
Batch 11/64 loss: -0.2727200984954834
Batch 12/64 loss: -0.27069056034088135
Batch 13/64 loss: -0.25456976890563965
Batch 14/64 loss: -0.2646214962005615
Batch 15/64 loss: -0.2627965807914734
Batch 16/64 loss: -0.2742713689804077
Batch 17/64 loss: -0.2526055574417114
Batch 18/64 loss: -0.23966538906097412
Batch 19/64 loss: -0.24880248308181763
Batch 20/64 loss: -0.2470223307609558
Batch 21/64 loss: -0.23587560653686523
Batch 22/64 loss: -0.2621215581893921
Batch 23/64 loss: -0.23659002780914307
Batch 24/64 loss: -0.26317739486694336
Batch 25/64 loss: -0.23560667037963867
Batch 26/64 loss: -0.2681382894515991
Batch 27/64 loss: -0.26928001642227173
Batch 28/64 loss: -0.2802755534648895
Batch 29/64 loss: -0.26640230417251587
Batch 30/64 loss: -0.2551652193069458
Batch 31/64 loss: -0.27031469345092773
Batch 32/64 loss: -0.2639304995536804
Batch 33/64 loss: -0.24969041347503662
Batch 34/64 loss: -0.2785624861717224
Batch 35/64 loss: -0.2691248655319214
Batch 36/64 loss: -0.26224392652511597
Batch 37/64 loss: -0.25599026679992676
Batch 38/64 loss: -0.26442521810531616
Batch 39/64 loss: -0.2579762935638428
Batch 40/64 loss: -0.2686372399330139
Batch 41/64 loss: -0.2570473551750183
Batch 42/64 loss: -0.2549833059310913
Batch 43/64 loss: -0.27712249755859375
Batch 44/64 loss: -0.26877760887145996
Batch 45/64 loss: -0.26205241680145264
Batch 46/64 loss: -0.2690650224685669
Batch 47/64 loss: -0.26040756702423096
Batch 48/64 loss: -0.27698570489883423
Batch 49/64 loss: -0.27378952503204346
Batch 50/64 loss: -0.2597962021827698
Batch 51/64 loss: -0.26631200313568115
Batch 52/64 loss: -0.2616559863090515
Batch 53/64 loss: -0.25568175315856934
Batch 54/64 loss: -0.2379472255706787
Batch 55/64 loss: -0.26144057512283325
Batch 56/64 loss: -0.26292043924331665
Batch 57/64 loss: -0.2554510235786438
Batch 58/64 loss: -0.2521560788154602
Batch 59/64 loss: -0.2572287321090698
Batch 60/64 loss: -0.2712542414665222
Batch 61/64 loss: -0.25430285930633545
Batch 62/64 loss: -0.2747972011566162
Batch 63/64 loss: -0.26149022579193115
Batch 64/64 loss: -0.2721449136734009
Epoch 213  Train loss: -0.2621187247482001  Val loss: -0.23136540458784072
Epoch 214
-------------------------------
Batch 1/64 loss: -0.2671562433242798
Batch 2/64 loss: -0.26603513956069946
Batch 3/64 loss: -0.2677452564239502
Batch 4/64 loss: -0.275290846824646
Batch 5/64 loss: -0.2706540822982788
Batch 6/64 loss: -0.2854190766811371
Batch 7/64 loss: -0.2551913857460022
Batch 8/64 loss: -0.2656092643737793
Batch 9/64 loss: -0.2718345522880554
Batch 10/64 loss: -0.2802729606628418
Batch 11/64 loss: -0.25107163190841675
Batch 12/64 loss: -0.27128463983535767
Batch 13/64 loss: -0.27017951011657715
Batch 14/64 loss: -0.2708200216293335
Batch 15/64 loss: -0.25798487663269043
Batch 16/64 loss: -0.2766605019569397
Batch 17/64 loss: -0.28036361932754517
Batch 18/64 loss: -0.2747069001197815
Batch 19/64 loss: -0.2657064199447632
Batch 20/64 loss: -0.2717333436012268
Batch 21/64 loss: -0.25785136222839355
Batch 22/64 loss: -0.2729865312576294
Batch 23/64 loss: -0.2692772150039673
Batch 24/64 loss: -0.25641167163848877
Batch 25/64 loss: -0.2636027932167053
Batch 26/64 loss: -0.2706853747367859
Batch 27/64 loss: -0.2564131021499634
Batch 28/64 loss: -0.2540707588195801
Batch 29/64 loss: -0.2684518098831177
Batch 30/64 loss: -0.26904553174972534
Batch 31/64 loss: -0.2522333860397339
Batch 32/64 loss: -0.2725563049316406
Batch 33/64 loss: -0.2737928628921509
Batch 34/64 loss: -0.27050602436065674
Batch 35/64 loss: -0.2681959867477417
Batch 36/64 loss: -0.25411170721054077
Batch 37/64 loss: -0.2796523869037628
Batch 38/64 loss: -0.2734960913658142
Batch 39/64 loss: -0.2523089647293091
Batch 40/64 loss: -0.2634415030479431
Batch 41/64 loss: -0.26095157861709595
Batch 42/64 loss: -0.27212661504745483
Batch 43/64 loss: -0.26621758937835693
Batch 44/64 loss: -0.25887739658355713
Batch 45/64 loss: -0.26103508472442627
Batch 46/64 loss: -0.26326918601989746
Batch 47/64 loss: -0.2515885829925537
Batch 48/64 loss: -0.25801414251327515
Batch 49/64 loss: -0.2795872092247009
Batch 50/64 loss: -0.25992053747177124
Batch 51/64 loss: -0.2653372287750244
Batch 52/64 loss: -0.2749079465866089
Batch 53/64 loss: -0.2611083984375
Batch 54/64 loss: -0.26637470722198486
Batch 55/64 loss: -0.26398974657058716
Batch 56/64 loss: -0.25986433029174805
Batch 57/64 loss: -0.2740158140659332
Batch 58/64 loss: -0.26172834634780884
Batch 59/64 loss: -0.26657986640930176
Batch 60/64 loss: -0.2821112275123596
Batch 61/64 loss: -0.2547813057899475
Batch 62/64 loss: -0.2652263641357422
Batch 63/64 loss: -0.2804220914840698
Batch 64/64 loss: -0.2546820044517517
Epoch 214  Train loss: -0.2665703284974192  Val loss: -0.24285971626792988
Epoch 215
-------------------------------
Batch 1/64 loss: -0.26698607206344604
Batch 2/64 loss: -0.2635122537612915
Batch 3/64 loss: -0.25765520334243774
Batch 4/64 loss: -0.27171486616134644
Batch 5/64 loss: -0.26251643896102905
Batch 6/64 loss: -0.2817051112651825
Batch 7/64 loss: -0.27633756399154663
Batch 8/64 loss: -0.27031052112579346
Batch 9/64 loss: -0.26365435123443604
Batch 10/64 loss: -0.2585669159889221
Batch 11/64 loss: -0.27384305000305176
Batch 12/64 loss: -0.2716658115386963
Batch 13/64 loss: -0.2578774094581604
Batch 14/64 loss: -0.2711232900619507
Batch 15/64 loss: -0.2648113965988159
Batch 16/64 loss: -0.2827630639076233
Batch 17/64 loss: -0.2631596326828003
Batch 18/64 loss: -0.2496235966682434
Batch 19/64 loss: -0.2729240655899048
Batch 20/64 loss: -0.27645379304885864
Batch 21/64 loss: -0.27558815479278564
Batch 22/64 loss: -0.27324581146240234
Batch 23/64 loss: -0.2651277780532837
Batch 24/64 loss: -0.26986634731292725
Batch 25/64 loss: -0.2661173939704895
Batch 26/64 loss: -0.2706547975540161
Batch 27/64 loss: -0.2716256380081177
Batch 28/64 loss: -0.27141624689102173
Batch 29/64 loss: -0.27243709564208984
Batch 30/64 loss: -0.25536197423934937
Batch 31/64 loss: -0.27886301279067993
Batch 32/64 loss: -0.2816382050514221
Batch 33/64 loss: -0.25894856452941895
Batch 34/64 loss: -0.2628302574157715
Batch 35/64 loss: -0.27653399109840393
Batch 36/64 loss: -0.25714659690856934
Batch 37/64 loss: -0.27518534660339355
Batch 38/64 loss: -0.2680027484893799
Batch 39/64 loss: -0.2742322087287903
Batch 40/64 loss: -0.2805558145046234
Batch 41/64 loss: -0.258611798286438
Batch 42/64 loss: -0.2691652178764343
Batch 43/64 loss: -0.26416587829589844
Batch 44/64 loss: -0.2738354206085205
Batch 45/64 loss: -0.23831403255462646
Batch 46/64 loss: -0.2736624479293823
Batch 47/64 loss: -0.26594817638397217
Batch 48/64 loss: -0.2611522674560547
Batch 49/64 loss: -0.2508615255355835
Batch 50/64 loss: -0.2656596302986145
Batch 51/64 loss: -0.2596972584724426
Batch 52/64 loss: -0.25263673067092896
Batch 53/64 loss: -0.2817119359970093
Batch 54/64 loss: -0.24980676174163818
Batch 55/64 loss: -0.2690421938896179
Batch 56/64 loss: -0.2735401391983032
Batch 57/64 loss: -0.26458364725112915
Batch 58/64 loss: -0.26903975009918213
Batch 59/64 loss: -0.2738999128341675
Batch 60/64 loss: -0.26509320735931396
Batch 61/64 loss: -0.2732136845588684
Batch 62/64 loss: -0.2679738998413086
Batch 63/64 loss: -0.2578314542770386
Batch 64/64 loss: -0.26741957664489746
Epoch 215  Train loss: -0.26733480855530384  Val loss: -0.23841624182114488
Epoch 216
-------------------------------
Batch 1/64 loss: -0.2669222950935364
Batch 2/64 loss: -0.2710738778114319
Batch 3/64 loss: -0.25883281230926514
Batch 4/64 loss: -0.2754475474357605
Batch 5/64 loss: -0.27581626176834106
Batch 6/64 loss: -0.266215443611145
Batch 7/64 loss: -0.2760094106197357
Batch 8/64 loss: -0.26676762104034424
Batch 9/64 loss: -0.27514320611953735
Batch 10/64 loss: -0.2624208331108093
Batch 11/64 loss: -0.26784050464630127
Batch 12/64 loss: -0.27892667055130005
Batch 13/64 loss: -0.28822755813598633
Batch 14/64 loss: -0.25536268949508667
Batch 15/64 loss: -0.23946261405944824
Batch 16/64 loss: -0.28263527154922485
Batch 17/64 loss: -0.27673667669296265
Batch 18/64 loss: -0.2703268527984619
Batch 19/64 loss: -0.272205114364624
Batch 20/64 loss: -0.2720900774002075
Batch 21/64 loss: -0.2727435231208801
Batch 22/64 loss: -0.27753186225891113
Batch 23/64 loss: -0.2666347622871399
Batch 24/64 loss: -0.26565802097320557
Batch 25/64 loss: -0.2647057771682739
Batch 26/64 loss: -0.2765224874019623
Batch 27/64 loss: -0.284246027469635
Batch 28/64 loss: -0.2655559778213501
Batch 29/64 loss: -0.2573796510696411
Batch 30/64 loss: -0.2686201333999634
Batch 31/64 loss: -0.2688361406326294
Batch 32/64 loss: -0.2704140543937683
Batch 33/64 loss: -0.2561379671096802
Batch 34/64 loss: -0.2734416723251343
Batch 35/64 loss: -0.2613554000854492
Batch 36/64 loss: -0.2676812410354614
Batch 37/64 loss: -0.25303202867507935
Batch 38/64 loss: -0.2739133834838867
Batch 39/64 loss: -0.2771885097026825
Batch 40/64 loss: -0.2565285563468933
Batch 41/64 loss: -0.2643275856971741
Batch 42/64 loss: -0.28353196382522583
Batch 43/64 loss: -0.2659071683883667
Batch 44/64 loss: -0.26815521717071533
Batch 45/64 loss: -0.276413232088089
Batch 46/64 loss: -0.2666146159172058
Batch 47/64 loss: -0.2659335732460022
Batch 48/64 loss: -0.2727598547935486
Batch 49/64 loss: -0.2657812237739563
Batch 50/64 loss: -0.278433233499527
Batch 51/64 loss: -0.25781548023223877
Batch 52/64 loss: -0.2737506628036499
Batch 53/64 loss: -0.27527397871017456
Batch 54/64 loss: -0.27692946791648865
Batch 55/64 loss: -0.27314281463623047
Batch 56/64 loss: -0.27300506830215454
Batch 57/64 loss: -0.25342637300491333
Batch 58/64 loss: -0.2692556381225586
Batch 59/64 loss: -0.2536003589630127
Batch 60/64 loss: -0.26373541355133057
Batch 61/64 loss: -0.27616629004478455
Batch 62/64 loss: -0.2627728581428528
Batch 63/64 loss: -0.28251713514328003
Batch 64/64 loss: -0.27108234167099
Epoch 216  Train loss: -0.2691631604643429  Val loss: -0.25512310094440105
Saving best model, epoch: 216
Epoch 217
-------------------------------
Batch 1/64 loss: -0.27451038360595703
Batch 2/64 loss: -0.27795279026031494
Batch 3/64 loss: -0.26455390453338623
Batch 4/64 loss: -0.26290565729141235
Batch 5/64 loss: -0.2662374973297119
Batch 6/64 loss: -0.254543662071228
Batch 7/64 loss: -0.2671780586242676
Batch 8/64 loss: -0.280061274766922
Batch 9/64 loss: -0.27305537462234497
Batch 10/64 loss: -0.268477201461792
Batch 11/64 loss: -0.24715834856033325
Batch 12/64 loss: -0.281981885433197
Batch 13/64 loss: -0.27224302291870117
Batch 14/64 loss: -0.26793062686920166
Batch 15/64 loss: -0.26158833503723145
Batch 16/64 loss: -0.2677924633026123
Batch 17/64 loss: -0.2577093839645386
Batch 18/64 loss: -0.24687433242797852
Batch 19/64 loss: -0.27238595485687256
Batch 20/64 loss: -0.26180481910705566
Batch 21/64 loss: -0.2744392156600952
Batch 22/64 loss: -0.27355271577835083
Batch 23/64 loss: -0.2672930359840393
Batch 24/64 loss: -0.26414257287979126
Batch 25/64 loss: -0.27151334285736084
Batch 26/64 loss: -0.26349252462387085
Batch 27/64 loss: -0.2889910936355591
Batch 28/64 loss: -0.264093816280365
Batch 29/64 loss: -0.264462947845459
Batch 30/64 loss: -0.2904854416847229
Batch 31/64 loss: -0.2640845775604248
Batch 32/64 loss: -0.25951188802719116
Batch 33/64 loss: -0.27606332302093506
Batch 34/64 loss: -0.2707630395889282
Batch 35/64 loss: -0.24946355819702148
Batch 36/64 loss: -0.2749094069004059
Batch 37/64 loss: -0.27948328852653503
Batch 38/64 loss: -0.27599531412124634
Batch 39/64 loss: -0.2677513360977173
Batch 40/64 loss: -0.2517396807670593
Batch 41/64 loss: -0.2564592957496643
Batch 42/64 loss: -0.2619727849960327
Batch 43/64 loss: -0.26229310035705566
Batch 44/64 loss: -0.26029717922210693
Batch 45/64 loss: -0.265697717666626
Batch 46/64 loss: -0.2613038420677185
Batch 47/64 loss: -0.2623324990272522
Batch 48/64 loss: -0.2643311619758606
Batch 49/64 loss: -0.2672640085220337
Batch 50/64 loss: -0.24827009439468384
Batch 51/64 loss: -0.2742573022842407
Batch 52/64 loss: -0.26368409395217896
Batch 53/64 loss: -0.28130435943603516
Batch 54/64 loss: -0.2648405432701111
Batch 55/64 loss: -0.2808723747730255
Batch 56/64 loss: -0.264188289642334
Batch 57/64 loss: -0.2718486785888672
Batch 58/64 loss: -0.285958468914032
Batch 59/64 loss: -0.2688124179840088
Batch 60/64 loss: -0.26487916707992554
Batch 61/64 loss: -0.2831255793571472
Batch 62/64 loss: -0.262717604637146
Batch 63/64 loss: -0.28096649050712585
Batch 64/64 loss: -0.27658191323280334
Epoch 217  Train loss: -0.2679888719437169  Val loss: -0.25029853037542493
Epoch 218
-------------------------------
Batch 1/64 loss: -0.26118171215057373
Batch 2/64 loss: -0.26921141147613525
Batch 3/64 loss: -0.2716010808944702
Batch 4/64 loss: -0.2673887014389038
Batch 5/64 loss: -0.26108217239379883
Batch 6/64 loss: -0.27957475185394287
Batch 7/64 loss: -0.2636379599571228
Batch 8/64 loss: -0.28668251633644104
Batch 9/64 loss: -0.2739397883415222
Batch 10/64 loss: -0.25928354263305664
Batch 11/64 loss: -0.27870407700538635
Batch 12/64 loss: -0.2760441303253174
Batch 13/64 loss: -0.2662086486816406
Batch 14/64 loss: -0.2707774043083191
Batch 15/64 loss: -0.2622378468513489
Batch 16/64 loss: -0.2623358964920044
Batch 17/64 loss: -0.27429628372192383
Batch 18/64 loss: -0.2545722723007202
Batch 19/64 loss: -0.2707613706588745
Batch 20/64 loss: -0.2721782326698303
Batch 21/64 loss: -0.267198383808136
Batch 22/64 loss: -0.26583337783813477
Batch 23/64 loss: -0.2495417594909668
Batch 24/64 loss: -0.2683393955230713
Batch 25/64 loss: -0.2626466155052185
Batch 26/64 loss: -0.2749324142932892
Batch 27/64 loss: -0.2616112232208252
Batch 28/64 loss: -0.27341002225875854
Batch 29/64 loss: -0.26759815216064453
Batch 30/64 loss: -0.2738388180732727
Batch 31/64 loss: -0.27877315878868103
Batch 32/64 loss: -0.2557048201560974
Batch 33/64 loss: -0.2737545669078827
Batch 34/64 loss: -0.268421471118927
Batch 35/64 loss: -0.2764928936958313
Batch 36/64 loss: -0.2484542727470398
Batch 37/64 loss: -0.2730233073234558
Batch 38/64 loss: -0.2698090076446533
Batch 39/64 loss: -0.25105077028274536
Batch 40/64 loss: -0.28137195110321045
Batch 41/64 loss: -0.2561013698577881
Batch 42/64 loss: -0.2534260153770447
Batch 43/64 loss: -0.28018516302108765
Batch 44/64 loss: -0.2616451382637024
Batch 45/64 loss: -0.2495235800743103
Batch 46/64 loss: -0.27095097303390503
Batch 47/64 loss: -0.2685176730155945
Batch 48/64 loss: -0.25428080558776855
Batch 49/64 loss: -0.2694953680038452
Batch 50/64 loss: -0.2550384998321533
Batch 51/64 loss: -0.2662651538848877
Batch 52/64 loss: -0.2653507590293884
Batch 53/64 loss: -0.2485141158103943
Batch 54/64 loss: -0.2652217149734497
Batch 55/64 loss: -0.2737760543823242
Batch 56/64 loss: -0.2645522356033325
Batch 57/64 loss: -0.2536616921424866
Batch 58/64 loss: -0.2711196541786194
Batch 59/64 loss: -0.2577133774757385
Batch 60/64 loss: -0.27045923471450806
Batch 61/64 loss: -0.2652045488357544
Batch 62/64 loss: -0.28163933753967285
Batch 63/64 loss: -0.27833521366119385
Batch 64/64 loss: -0.2657501697540283
Epoch 218  Train loss: -0.2667262194203395  Val loss: -0.23028157011340164
Epoch 219
-------------------------------
Batch 1/64 loss: -0.26801854372024536
Batch 2/64 loss: -0.26455438137054443
Batch 3/64 loss: -0.26531273126602173
Batch 4/64 loss: -0.2721651792526245
Batch 5/64 loss: -0.2771857976913452
Batch 6/64 loss: -0.28237491846084595
Batch 7/64 loss: -0.2729610204696655
Batch 8/64 loss: -0.27046966552734375
Batch 9/64 loss: -0.26912379264831543
Batch 10/64 loss: -0.2703569531440735
Batch 11/64 loss: -0.26520395278930664
Batch 12/64 loss: -0.26754528284072876
Batch 13/64 loss: -0.2866605520248413
Batch 14/64 loss: -0.2634361982345581
Batch 15/64 loss: -0.2701725959777832
Batch 16/64 loss: -0.2890504002571106
Batch 17/64 loss: -0.25682079792022705
Batch 18/64 loss: -0.2649118900299072
Batch 19/64 loss: -0.25783711671829224
Batch 20/64 loss: -0.2791426181793213
Batch 21/64 loss: -0.2744453549385071
Batch 22/64 loss: -0.27944523096084595
Batch 23/64 loss: -0.25947463512420654
Batch 24/64 loss: -0.2740328311920166
Batch 25/64 loss: -0.2796096205711365
Batch 26/64 loss: -0.2679193615913391
Batch 27/64 loss: -0.2648155689239502
Batch 28/64 loss: -0.25756704807281494
Batch 29/64 loss: -0.25582313537597656
Batch 30/64 loss: -0.26005423069000244
Batch 31/64 loss: -0.2790457010269165
Batch 32/64 loss: -0.24453586339950562
Batch 33/64 loss: -0.26657676696777344
Batch 34/64 loss: -0.27231794595718384
Batch 35/64 loss: -0.24505281448364258
Batch 36/64 loss: -0.2658400535583496
Batch 37/64 loss: -0.2552633285522461
Batch 38/64 loss: -0.2611929178237915
Batch 39/64 loss: -0.2404056191444397
Batch 40/64 loss: -0.24766427278518677
Batch 41/64 loss: -0.2622053623199463
Batch 42/64 loss: -0.2556162476539612
Batch 43/64 loss: -0.2562716007232666
Batch 44/64 loss: -0.2525445818901062
Batch 45/64 loss: -0.2757846415042877
Batch 46/64 loss: -0.2603197693824768
Batch 47/64 loss: -0.2458585500717163
Batch 48/64 loss: -0.27104371786117554
Batch 49/64 loss: -0.26730066537857056
Batch 50/64 loss: -0.24750244617462158
Batch 51/64 loss: -0.25748753547668457
Batch 52/64 loss: -0.2577834725379944
Batch 53/64 loss: -0.2641791105270386
Batch 54/64 loss: -0.2720513939857483
Batch 55/64 loss: -0.25925350189208984
Batch 56/64 loss: -0.2741524577140808
Batch 57/64 loss: -0.258018434047699
Batch 58/64 loss: -0.2633192539215088
Batch 59/64 loss: -0.26027578115463257
Batch 60/64 loss: -0.27012866735458374
Batch 61/64 loss: -0.2659130096435547
Batch 62/64 loss: -0.2684794068336487
Batch 63/64 loss: -0.2790546417236328
Batch 64/64 loss: -0.2597224712371826
Epoch 219  Train loss: -0.2651564359664917  Val loss: -0.23271546023817816
Epoch 220
-------------------------------
Batch 1/64 loss: -0.27202916145324707
Batch 2/64 loss: -0.27357590198516846
Batch 3/64 loss: -0.2806280851364136
Batch 4/64 loss: -0.25557905435562134
Batch 5/64 loss: -0.27490416169166565
Batch 6/64 loss: -0.2680983543395996
Batch 7/64 loss: -0.24735301733016968
Batch 8/64 loss: -0.2522119879722595
Batch 9/64 loss: -0.2507009506225586
Batch 10/64 loss: -0.2621533274650574
Batch 11/64 loss: -0.2646865248680115
Batch 12/64 loss: -0.26217353343963623
Batch 13/64 loss: -0.2589111328125
Batch 14/64 loss: -0.264498233795166
Batch 15/64 loss: -0.2618318796157837
Batch 16/64 loss: -0.25130122900009155
Batch 17/64 loss: -0.24795204401016235
Batch 18/64 loss: -0.2718774080276489
Batch 19/64 loss: -0.2660011649131775
Batch 20/64 loss: -0.2761651873588562
Batch 21/64 loss: -0.2627655267715454
Batch 22/64 loss: -0.25791776180267334
Batch 23/64 loss: -0.2602747678756714
Batch 24/64 loss: -0.25488895177841187
Batch 25/64 loss: -0.2592017650604248
Batch 26/64 loss: -0.260778546333313
Batch 27/64 loss: -0.2566017508506775
Batch 28/64 loss: -0.2519020438194275
Batch 29/64 loss: -0.2549034357070923
Batch 30/64 loss: -0.2505560517311096
Batch 31/64 loss: -0.2701345682144165
Batch 32/64 loss: -0.26820558309555054
Batch 33/64 loss: -0.2547900676727295
Batch 34/64 loss: -0.24808937311172485
Batch 35/64 loss: -0.26169055700302124
Batch 36/64 loss: -0.26316505670547485
Batch 37/64 loss: -0.24965733289718628
Batch 38/64 loss: -0.2559179663658142
Batch 39/64 loss: -0.26490938663482666
Batch 40/64 loss: -0.2564014196395874
Batch 41/64 loss: -0.26269930601119995
Batch 42/64 loss: -0.2606063485145569
Batch 43/64 loss: -0.27416980266571045
Batch 44/64 loss: -0.2581157684326172
Batch 45/64 loss: -0.2688578963279724
Batch 46/64 loss: -0.28411489725112915
Batch 47/64 loss: -0.25533515214920044
Batch 48/64 loss: -0.26160186529159546
Batch 49/64 loss: -0.27149009704589844
Batch 50/64 loss: -0.2743561267852783
Batch 51/64 loss: -0.27379703521728516
Batch 52/64 loss: -0.27751296758651733
Batch 53/64 loss: -0.268818736076355
Batch 54/64 loss: -0.26841235160827637
Batch 55/64 loss: -0.27811843156814575
Batch 56/64 loss: -0.27563679218292236
Batch 57/64 loss: -0.26376235485076904
Batch 58/64 loss: -0.2766403257846832
Batch 59/64 loss: -0.2602430582046509
Batch 60/64 loss: -0.2647029161453247
Batch 61/64 loss: -0.270752489566803
Batch 62/64 loss: -0.2706107497215271
Batch 63/64 loss: -0.26607394218444824
Batch 64/64 loss: -0.2715504765510559
Epoch 220  Train loss: -0.26377218073489617  Val loss: -0.24520787463565052
Epoch 221
-------------------------------
Batch 1/64 loss: -0.27530500292778015
Batch 2/64 loss: -0.25979340076446533
Batch 3/64 loss: -0.26881617307662964
Batch 4/64 loss: -0.2728685736656189
Batch 5/64 loss: -0.27082204818725586
Batch 6/64 loss: -0.2695677876472473
Batch 7/64 loss: -0.28021490573883057
Batch 8/64 loss: -0.28103089332580566
Batch 9/64 loss: -0.2686834931373596
Batch 10/64 loss: -0.2666349411010742
Batch 11/64 loss: -0.2638448476791382
Batch 12/64 loss: -0.2721356153488159
Batch 13/64 loss: -0.25040483474731445
Batch 14/64 loss: -0.24716120958328247
Batch 15/64 loss: -0.2524663209915161
Batch 16/64 loss: -0.2691059112548828
Batch 17/64 loss: -0.26848286390304565
Batch 18/64 loss: -0.275762677192688
Batch 19/64 loss: -0.2685508131980896
Batch 20/64 loss: -0.2738584280014038
Batch 21/64 loss: -0.26062101125717163
Batch 22/64 loss: -0.26271533966064453
Batch 23/64 loss: -0.26007533073425293
Batch 24/64 loss: -0.2602205276489258
Batch 25/64 loss: -0.2640472650527954
Batch 26/64 loss: -0.2593510150909424
Batch 27/64 loss: -0.27142494916915894
Batch 28/64 loss: -0.2596249580383301
Batch 29/64 loss: -0.25079137086868286
Batch 30/64 loss: -0.2405962347984314
Batch 31/64 loss: -0.27108168601989746
Batch 32/64 loss: -0.26929378509521484
Batch 33/64 loss: -0.2439613938331604
Batch 34/64 loss: -0.26447033882141113
Batch 35/64 loss: -0.2454872727394104
Batch 36/64 loss: -0.2488788366317749
Batch 37/64 loss: -0.24601960182189941
Batch 38/64 loss: -0.25348520278930664
Batch 39/64 loss: -0.2399991750717163
Batch 40/64 loss: -0.23944902420043945
Batch 41/64 loss: -0.26801061630249023
Batch 42/64 loss: -0.2365381121635437
Batch 43/64 loss: -0.23394978046417236
Batch 44/64 loss: -0.24678361415863037
Batch 45/64 loss: -0.25465625524520874
Batch 46/64 loss: -0.25665736198425293
Batch 47/64 loss: -0.25196439027786255
Batch 48/64 loss: -0.2569853663444519
Batch 49/64 loss: -0.27147698402404785
Batch 50/64 loss: -0.26811283826828003
Batch 51/64 loss: -0.2520262598991394
Batch 52/64 loss: -0.2507796287536621
Batch 53/64 loss: -0.23429018259048462
Batch 54/64 loss: -0.25478339195251465
Batch 55/64 loss: -0.23747652769088745
Batch 56/64 loss: -0.2475452423095703
Batch 57/64 loss: -0.25342023372650146
Batch 58/64 loss: -0.27201247215270996
Batch 59/64 loss: -0.25691545009613037
Batch 60/64 loss: -0.2577459216117859
Batch 61/64 loss: -0.25293493270874023
Batch 62/64 loss: -0.24376684427261353
Batch 63/64 loss: -0.25407832860946655
Batch 64/64 loss: -0.2600693106651306
Epoch 221  Train loss: -0.2584324357556362  Val loss: -0.23279722040051856
Epoch 222
-------------------------------
Batch 1/64 loss: -0.2683091163635254
Batch 2/64 loss: -0.26132845878601074
Batch 3/64 loss: -0.25395524501800537
Batch 4/64 loss: -0.2711212635040283
Batch 5/64 loss: -0.2583876848220825
Batch 6/64 loss: -0.2682374119758606
Batch 7/64 loss: -0.26222729682922363
Batch 8/64 loss: -0.24762237071990967
Batch 9/64 loss: -0.24804329872131348
Batch 10/64 loss: -0.2591625452041626
Batch 11/64 loss: -0.26441895961761475
Batch 12/64 loss: -0.25293248891830444
Batch 13/64 loss: -0.26124465465545654
Batch 14/64 loss: -0.26652854681015015
Batch 15/64 loss: -0.2589382529258728
Batch 16/64 loss: -0.2745627760887146
Batch 17/64 loss: -0.27683818340301514
Batch 18/64 loss: -0.25229257345199585
Batch 19/64 loss: -0.26378941535949707
Batch 20/64 loss: -0.2623579502105713
Batch 21/64 loss: -0.2636963725090027
Batch 22/64 loss: -0.2708962559700012
Batch 23/64 loss: -0.2692034840583801
Batch 24/64 loss: -0.26976150274276733
Batch 25/64 loss: -0.25059032440185547
Batch 26/64 loss: -0.26027607917785645
Batch 27/64 loss: -0.2636035680770874
Batch 28/64 loss: -0.2604289650917053
Batch 29/64 loss: -0.2586650848388672
Batch 30/64 loss: -0.26703304052352905
Batch 31/64 loss: -0.2669193148612976
Batch 32/64 loss: -0.25364744663238525
Batch 33/64 loss: -0.25282758474349976
Batch 34/64 loss: -0.272061824798584
Batch 35/64 loss: -0.2772893011569977
Batch 36/64 loss: -0.26276516914367676
Batch 37/64 loss: -0.27594971656799316
Batch 38/64 loss: -0.26821255683898926
Batch 39/64 loss: -0.2578021287918091
Batch 40/64 loss: -0.27453333139419556
Batch 41/64 loss: -0.25802499055862427
Batch 42/64 loss: -0.27186697721481323
Batch 43/64 loss: -0.2761196494102478
Batch 44/64 loss: -0.2737826704978943
Batch 45/64 loss: -0.24626779556274414
Batch 46/64 loss: -0.26730072498321533
Batch 47/64 loss: -0.27672845125198364
Batch 48/64 loss: -0.26426005363464355
Batch 49/64 loss: -0.2719111442565918
Batch 50/64 loss: -0.27435213327407837
Batch 51/64 loss: -0.28007596731185913
Batch 52/64 loss: -0.2740267515182495
Batch 53/64 loss: -0.2616080045700073
Batch 54/64 loss: -0.2665103077888489
Batch 55/64 loss: -0.2486674189567566
Batch 56/64 loss: -0.26622384786605835
Batch 57/64 loss: -0.2673664093017578
Batch 58/64 loss: -0.271354615688324
Batch 59/64 loss: -0.2713695764541626
Batch 60/64 loss: -0.26135969161987305
Batch 61/64 loss: -0.2761983871459961
Batch 62/64 loss: -0.2742193937301636
Batch 63/64 loss: -0.2699180841445923
Batch 64/64 loss: -0.2659280300140381
Epoch 222  Train loss: -0.265057578273848  Val loss: -0.21121739501395995
Epoch 223
-------------------------------
Batch 1/64 loss: -0.2550625205039978
Batch 2/64 loss: -0.27047550678253174
Batch 3/64 loss: -0.2711825370788574
Batch 4/64 loss: -0.26926612854003906
Batch 5/64 loss: -0.2416955828666687
Batch 6/64 loss: -0.2676393985748291
Batch 7/64 loss: -0.25276559591293335
Batch 8/64 loss: -0.2659232020378113
Batch 9/64 loss: -0.2726770043373108
Batch 10/64 loss: -0.27846089005470276
Batch 11/64 loss: -0.276943176984787
Batch 12/64 loss: -0.26919227838516235
Batch 13/64 loss: -0.2627848982810974
Batch 14/64 loss: -0.2653690576553345
Batch 15/64 loss: -0.26195675134658813
Batch 16/64 loss: -0.2702329754829407
Batch 17/64 loss: -0.2632390260696411
Batch 18/64 loss: -0.2698742747306824
Batch 19/64 loss: -0.27395808696746826
Batch 20/64 loss: -0.2812349796295166
Batch 21/64 loss: -0.2643612027168274
Batch 22/64 loss: -0.25921326875686646
Batch 23/64 loss: -0.2770407795906067
Batch 24/64 loss: -0.264778733253479
Batch 25/64 loss: -0.2766103148460388
Batch 26/64 loss: -0.2773449122905731
Batch 27/64 loss: -0.2645648717880249
Batch 28/64 loss: -0.28120994567871094
Batch 29/64 loss: -0.27904894948005676
Batch 30/64 loss: -0.25643473863601685
Batch 31/64 loss: -0.27056407928466797
Batch 32/64 loss: -0.2841656506061554
Batch 33/64 loss: -0.2646883726119995
Batch 34/64 loss: -0.26456165313720703
Batch 35/64 loss: -0.2596287727355957
Batch 36/64 loss: -0.27016210556030273
Batch 37/64 loss: -0.2829856872558594
Batch 38/64 loss: -0.27134233713150024
Batch 39/64 loss: -0.26092708110809326
Batch 40/64 loss: -0.2808753550052643
Batch 41/64 loss: -0.2665231227874756
Batch 42/64 loss: -0.2660704255104065
Batch 43/64 loss: -0.26874250173568726
Batch 44/64 loss: -0.263827383518219
Batch 45/64 loss: -0.2886650562286377
Batch 46/64 loss: -0.273912250995636
Batch 47/64 loss: -0.2441544532775879
Batch 48/64 loss: -0.2754747271537781
Batch 49/64 loss: -0.2720489501953125
Batch 50/64 loss: -0.2630883455276489
Batch 51/64 loss: -0.26928555965423584
Batch 52/64 loss: -0.27104389667510986
Batch 53/64 loss: -0.2723873257637024
Batch 54/64 loss: -0.2671170234680176
Batch 55/64 loss: -0.27730900049209595
Batch 56/64 loss: -0.2559208273887634
Batch 57/64 loss: -0.2830681800842285
Batch 58/64 loss: -0.27466481924057007
Batch 59/64 loss: -0.25670671463012695
Batch 60/64 loss: -0.2777179181575775
Batch 61/64 loss: -0.27303266525268555
Batch 62/64 loss: -0.262126088142395
Batch 63/64 loss: -0.25462597608566284
Batch 64/64 loss: -0.2680257558822632
Epoch 223  Train loss: -0.26868980725606284  Val loss: -0.23985209211041428
Epoch 224
-------------------------------
Batch 1/64 loss: -0.26346051692962646
Batch 2/64 loss: -0.26609039306640625
Batch 3/64 loss: -0.2605193257331848
Batch 4/64 loss: -0.2648274302482605
Batch 5/64 loss: -0.26428765058517456
Batch 6/64 loss: -0.26599788665771484
Batch 7/64 loss: -0.280549556016922
Batch 8/64 loss: -0.26809144020080566
Batch 9/64 loss: -0.267487108707428
Batch 10/64 loss: -0.25819194316864014
Batch 11/64 loss: -0.27227866649627686
Batch 12/64 loss: -0.2579081058502197
Batch 13/64 loss: -0.25836896896362305
Batch 14/64 loss: -0.27387624979019165
Batch 15/64 loss: -0.269595742225647
Batch 16/64 loss: -0.2596887946128845
Batch 17/64 loss: -0.2835004925727844
Batch 18/64 loss: -0.2672247886657715
Batch 19/64 loss: -0.26386499404907227
Batch 20/64 loss: -0.28105267882347107
Batch 21/64 loss: -0.28782665729522705
Batch 22/64 loss: -0.27052152156829834
Batch 23/64 loss: -0.2641727328300476
Batch 24/64 loss: -0.2604718804359436
Batch 25/64 loss: -0.2645801305770874
Batch 26/64 loss: -0.27391061186790466
Batch 27/64 loss: -0.2513275742530823
Batch 28/64 loss: -0.2729412913322449
Batch 29/64 loss: -0.2722651958465576
Batch 30/64 loss: -0.2567809820175171
Batch 31/64 loss: -0.27359244227409363
Batch 32/64 loss: -0.26203423738479614
Batch 33/64 loss: -0.2611318826675415
Batch 34/64 loss: -0.28811967372894287
Batch 35/64 loss: -0.26394331455230713
Batch 36/64 loss: -0.26171404123306274
Batch 37/64 loss: -0.2728375494480133
Batch 38/64 loss: -0.2777332067489624
Batch 39/64 loss: -0.26137202978134155
Batch 40/64 loss: -0.2673894762992859
Batch 41/64 loss: -0.2810835540294647
Batch 42/64 loss: -0.27310797572135925
Batch 43/64 loss: -0.27698779106140137
Batch 44/64 loss: -0.27458426356315613
Batch 45/64 loss: -0.2605378031730652
Batch 46/64 loss: -0.2628833055496216
Batch 47/64 loss: -0.28469595313072205
Batch 48/64 loss: -0.27273792028427124
Batch 49/64 loss: -0.28467974066734314
Batch 50/64 loss: -0.2676072120666504
Batch 51/64 loss: -0.2387791872024536
Batch 52/64 loss: -0.2699938416481018
Batch 53/64 loss: -0.2621501684188843
Batch 54/64 loss: -0.25472474098205566
Batch 55/64 loss: -0.28010329604148865
Batch 56/64 loss: -0.274097204208374
Batch 57/64 loss: -0.2682496905326843
Batch 58/64 loss: -0.25836342573165894
Batch 59/64 loss: -0.27194833755493164
Batch 60/64 loss: -0.2763623595237732
Batch 61/64 loss: -0.2749430537223816
Batch 62/64 loss: -0.26711583137512207
Batch 63/64 loss: -0.26925069093704224
Batch 64/64 loss: -0.27404940128326416
Epoch 224  Train loss: -0.2685812323701148  Val loss: -0.24556855811286218
Epoch 225
-------------------------------
Batch 1/64 loss: -0.27641814947128296
Batch 2/64 loss: -0.2701934576034546
Batch 3/64 loss: -0.2702453136444092
Batch 4/64 loss: -0.2767865061759949
Batch 5/64 loss: -0.2520869970321655
Batch 6/64 loss: -0.2877511978149414
Batch 7/64 loss: -0.2769775092601776
Batch 8/64 loss: -0.27425020933151245
Batch 9/64 loss: -0.2463449239730835
Batch 10/64 loss: -0.26738500595092773
Batch 11/64 loss: -0.2582378387451172
Batch 12/64 loss: -0.2725139260292053
Batch 13/64 loss: -0.26993030309677124
Batch 14/64 loss: -0.27469882369041443
Batch 15/64 loss: -0.2775253653526306
Batch 16/64 loss: -0.25750255584716797
Batch 17/64 loss: -0.2609265446662903
Batch 18/64 loss: -0.27741140127182007
Batch 19/64 loss: -0.27678006887435913
Batch 20/64 loss: -0.28741729259490967
Batch 21/64 loss: -0.2831076383590698
Batch 22/64 loss: -0.2851523756980896
Batch 23/64 loss: -0.27163565158843994
Batch 24/64 loss: -0.2673324942588806
Batch 25/64 loss: -0.2800822854042053
Batch 26/64 loss: -0.2900811433792114
Batch 27/64 loss: -0.2656065821647644
Batch 28/64 loss: -0.262919545173645
Batch 29/64 loss: -0.2685559391975403
Batch 30/64 loss: -0.2620599865913391
Batch 31/64 loss: -0.27567362785339355
Batch 32/64 loss: -0.2746744155883789
Batch 33/64 loss: -0.2676582336425781
Batch 34/64 loss: -0.2677159905433655
Batch 35/64 loss: -0.2803661525249481
Batch 36/64 loss: -0.2612682580947876
Batch 37/64 loss: -0.2617264986038208
Batch 38/64 loss: -0.26717132329940796
Batch 39/64 loss: -0.2714616656303406
Batch 40/64 loss: -0.2706071138381958
Batch 41/64 loss: -0.24787288904190063
Batch 42/64 loss: -0.25536614656448364
Batch 43/64 loss: -0.2681472897529602
Batch 44/64 loss: -0.2581806182861328
Batch 45/64 loss: -0.260919988155365
Batch 46/64 loss: -0.2732095718383789
Batch 47/64 loss: -0.2764952778816223
Batch 48/64 loss: -0.26871824264526367
Batch 49/64 loss: -0.27419376373291016
Batch 50/64 loss: -0.25532931089401245
Batch 51/64 loss: -0.2664688229560852
Batch 52/64 loss: -0.26751708984375
Batch 53/64 loss: -0.26962703466415405
Batch 54/64 loss: -0.26681220531463623
Batch 55/64 loss: -0.25363028049468994
Batch 56/64 loss: -0.25961196422576904
Batch 57/64 loss: -0.27135148644447327
Batch 58/64 loss: -0.262725293636322
Batch 59/64 loss: -0.26717591285705566
Batch 60/64 loss: -0.2626500129699707
Batch 61/64 loss: -0.27566540241241455
Batch 62/64 loss: -0.2869611978530884
Batch 63/64 loss: -0.27468040585517883
Batch 64/64 loss: -0.263005793094635
Epoch 225  Train loss: -0.26928321356866874  Val loss: -0.23066480913522727
Epoch 226
-------------------------------
Batch 1/64 loss: -0.2689017057418823
Batch 2/64 loss: -0.27176952362060547
Batch 3/64 loss: -0.27267807722091675
Batch 4/64 loss: -0.26685160398483276
Batch 5/64 loss: -0.27110427618026733
Batch 6/64 loss: -0.26705360412597656
Batch 7/64 loss: -0.2833179235458374
Batch 8/64 loss: -0.2807474136352539
Batch 9/64 loss: -0.26554131507873535
Batch 10/64 loss: -0.28726649284362793
Batch 11/64 loss: -0.27873727679252625
Batch 12/64 loss: -0.26552534103393555
Batch 13/64 loss: -0.26732200384140015
Batch 14/64 loss: -0.2668275237083435
Batch 15/64 loss: -0.271248996257782
Batch 16/64 loss: -0.27358168363571167
Batch 17/64 loss: -0.2836502194404602
Batch 18/64 loss: -0.27591416239738464
Batch 19/64 loss: -0.27484801411628723
Batch 20/64 loss: -0.282656192779541
Batch 21/64 loss: -0.27245891094207764
Batch 22/64 loss: -0.25623178482055664
Batch 23/64 loss: -0.2645236849784851
Batch 24/64 loss: -0.2476332187652588
Batch 25/64 loss: -0.25305020809173584
Batch 26/64 loss: -0.2659795880317688
Batch 27/64 loss: -0.2654799222946167
Batch 28/64 loss: -0.2822725176811218
Batch 29/64 loss: -0.2707796096801758
Batch 30/64 loss: -0.2796838879585266
Batch 31/64 loss: -0.2725527882575989
Batch 32/64 loss: -0.2772391438484192
Batch 33/64 loss: -0.28052854537963867
Batch 34/64 loss: -0.2529667615890503
Batch 35/64 loss: -0.25763416290283203
Batch 36/64 loss: -0.2712593674659729
Batch 37/64 loss: -0.2777112126350403
Batch 38/64 loss: -0.27372342348098755
Batch 39/64 loss: -0.2657238245010376
Batch 40/64 loss: -0.27588480710983276
Batch 41/64 loss: -0.2780620753765106
Batch 42/64 loss: -0.27625948190689087
Batch 43/64 loss: -0.27109968662261963
Batch 44/64 loss: -0.2600642442703247
Batch 45/64 loss: -0.2507031559944153
Batch 46/64 loss: -0.2717461585998535
Batch 47/64 loss: -0.2666127681732178
Batch 48/64 loss: -0.2732551097869873
Batch 49/64 loss: -0.28465136885643005
Batch 50/64 loss: -0.27922600507736206
Batch 51/64 loss: -0.2694854140281677
Batch 52/64 loss: -0.2602580785751343
Batch 53/64 loss: -0.2828618586063385
Batch 54/64 loss: -0.27963924407958984
Batch 55/64 loss: -0.2589380741119385
Batch 56/64 loss: -0.2608413100242615
Batch 57/64 loss: -0.27631235122680664
Batch 58/64 loss: -0.2780753970146179
Batch 59/64 loss: -0.2628359794616699
Batch 60/64 loss: -0.26235532760620117
Batch 61/64 loss: -0.27314668893814087
Batch 62/64 loss: -0.2592150568962097
Batch 63/64 loss: -0.2653771638870239
Batch 64/64 loss: -0.2674979567527771
Epoch 226  Train loss: -0.2704393127385308  Val loss: -0.24938866219569727
Epoch 227
-------------------------------
Batch 1/64 loss: -0.271589994430542
Batch 2/64 loss: -0.2769200801849365
Batch 3/64 loss: -0.2764962911605835
Batch 4/64 loss: -0.267519474029541
Batch 5/64 loss: -0.2682197093963623
Batch 6/64 loss: -0.27215033769607544
Batch 7/64 loss: -0.27233004570007324
Batch 8/64 loss: -0.28185853362083435
Batch 9/64 loss: -0.2707262635231018
Batch 10/64 loss: -0.2587360143661499
Batch 11/64 loss: -0.28076523542404175
Batch 12/64 loss: -0.2532137632369995
Batch 13/64 loss: -0.26057833433151245
Batch 14/64 loss: -0.27108776569366455
Batch 15/64 loss: -0.2699500322341919
Batch 16/64 loss: -0.2767298221588135
Batch 17/64 loss: -0.27700456976890564
Batch 18/64 loss: -0.2668495178222656
Batch 19/64 loss: -0.2660844326019287
Batch 20/64 loss: -0.2811744809150696
Batch 21/64 loss: -0.2738950252532959
Batch 22/64 loss: -0.2878239154815674
Batch 23/64 loss: -0.2699601650238037
Batch 24/64 loss: -0.2805050313472748
Batch 25/64 loss: -0.2712863087654114
Batch 26/64 loss: -0.2735012173652649
Batch 27/64 loss: -0.26699918508529663
Batch 28/64 loss: -0.27247023582458496
Batch 29/64 loss: -0.258131206035614
Batch 30/64 loss: -0.28627946972846985
Batch 31/64 loss: -0.25501978397369385
Batch 32/64 loss: -0.2812606692314148
Batch 33/64 loss: -0.2738829255104065
Batch 34/64 loss: -0.25709474086761475
Batch 35/64 loss: -0.26588648557662964
Batch 36/64 loss: -0.2671273350715637
Batch 37/64 loss: -0.2741064429283142
Batch 38/64 loss: -0.27992916107177734
Batch 39/64 loss: -0.2650189995765686
Batch 40/64 loss: -0.2849264144897461
Batch 41/64 loss: -0.2730977535247803
Batch 42/64 loss: -0.2782944440841675
Batch 43/64 loss: -0.26571017503738403
Batch 44/64 loss: -0.2756972908973694
Batch 45/64 loss: -0.27741333842277527
Batch 46/64 loss: -0.276406466960907
Batch 47/64 loss: -0.2647387981414795
Batch 48/64 loss: -0.2621285319328308
Batch 49/64 loss: -0.26135367155075073
Batch 50/64 loss: -0.27619773149490356
Batch 51/64 loss: -0.2782793939113617
Batch 52/64 loss: -0.2841988801956177
Batch 53/64 loss: -0.28160953521728516
Batch 54/64 loss: -0.2704817056655884
Batch 55/64 loss: -0.2611791491508484
Batch 56/64 loss: -0.2787669599056244
Batch 57/64 loss: -0.27027881145477295
Batch 58/64 loss: -0.26987940073013306
Batch 59/64 loss: -0.27687814831733704
Batch 60/64 loss: -0.276730477809906
Batch 61/64 loss: -0.26494383811950684
Batch 62/64 loss: -0.2749108672142029
Batch 63/64 loss: -0.2728458046913147
Batch 64/64 loss: -0.27064967155456543
Epoch 227  Train loss: -0.2720015348172655  Val loss: -0.23405429792568036
Epoch 228
-------------------------------
Batch 1/64 loss: -0.2825384736061096
Batch 2/64 loss: -0.2714036703109741
Batch 3/64 loss: -0.2505829334259033
Batch 4/64 loss: -0.28067612648010254
Batch 5/64 loss: -0.28488194942474365
Batch 6/64 loss: -0.2728538513183594
Batch 7/64 loss: -0.2591538429260254
Batch 8/64 loss: -0.2736758589744568
Batch 9/64 loss: -0.2701103687286377
Batch 10/64 loss: -0.2742763161659241
Batch 11/64 loss: -0.27037370204925537
Batch 12/64 loss: -0.27048206329345703
Batch 13/64 loss: -0.2763633131980896
Batch 14/64 loss: -0.267342209815979
Batch 15/64 loss: -0.2730560302734375
Batch 16/64 loss: -0.2790561318397522
Batch 17/64 loss: -0.27953675389289856
Batch 18/64 loss: -0.2717400789260864
Batch 19/64 loss: -0.2856307625770569
Batch 20/64 loss: -0.2764829993247986
Batch 21/64 loss: -0.24109452962875366
Batch 22/64 loss: -0.2705201506614685
Batch 23/64 loss: -0.284807026386261
Batch 24/64 loss: -0.266781747341156
Batch 25/64 loss: -0.2797113060951233
Batch 26/64 loss: -0.25674957036972046
Batch 27/64 loss: -0.26111942529678345
Batch 28/64 loss: -0.26590824127197266
Batch 29/64 loss: -0.2760394811630249
Batch 30/64 loss: -0.2717534303665161
Batch 31/64 loss: -0.25246375799179077
Batch 32/64 loss: -0.276361346244812
Batch 33/64 loss: -0.283146470785141
Batch 34/64 loss: -0.2734052538871765
Batch 35/64 loss: -0.27439790964126587
Batch 36/64 loss: -0.27273696660995483
Batch 37/64 loss: -0.27536511421203613
Batch 38/64 loss: -0.2604560852050781
Batch 39/64 loss: -0.2768412232398987
Batch 40/64 loss: -0.25451886653900146
Batch 41/64 loss: -0.2714037299156189
Batch 42/64 loss: -0.27108240127563477
Batch 43/64 loss: -0.28114786744117737
Batch 44/64 loss: -0.26294130086898804
Batch 45/64 loss: -0.2749422788619995
Batch 46/64 loss: -0.27393293380737305
Batch 47/64 loss: -0.26177340745925903
Batch 48/64 loss: -0.2661202549934387
Batch 49/64 loss: -0.2744901776313782
Batch 50/64 loss: -0.28157293796539307
Batch 51/64 loss: -0.2691822648048401
Batch 52/64 loss: -0.2571932077407837
Batch 53/64 loss: -0.26997989416122437
Batch 54/64 loss: -0.28483399748802185
Batch 55/64 loss: -0.2704353928565979
Batch 56/64 loss: -0.2659097909927368
Batch 57/64 loss: -0.2734774947166443
Batch 58/64 loss: -0.26811444759368896
Batch 59/64 loss: -0.2543575167655945
Batch 60/64 loss: -0.2580268383026123
Batch 61/64 loss: -0.2642502188682556
Batch 62/64 loss: -0.2841886281967163
Batch 63/64 loss: -0.270419716835022
Batch 64/64 loss: -0.25752145051956177
Epoch 228  Train loss: -0.2705460882654377  Val loss: -0.24098109686907215
Epoch 229
-------------------------------
Batch 1/64 loss: -0.2843930423259735
Batch 2/64 loss: -0.27845799922943115
Batch 3/64 loss: -0.26285433769226074
Batch 4/64 loss: -0.27415579557418823
Batch 5/64 loss: -0.2580186128616333
Batch 6/64 loss: -0.27101731300354004
Batch 7/64 loss: -0.25134652853012085
Batch 8/64 loss: -0.2753745913505554
Batch 9/64 loss: -0.26754260063171387
Batch 10/64 loss: -0.28377991914749146
Batch 11/64 loss: -0.2799975872039795
Batch 12/64 loss: -0.27970725297927856
Batch 13/64 loss: -0.262977659702301
Batch 14/64 loss: -0.26949262619018555
Batch 15/64 loss: -0.27049195766448975
Batch 16/64 loss: -0.2665964961051941
Batch 17/64 loss: -0.2758731245994568
Batch 18/64 loss: -0.2786974608898163
Batch 19/64 loss: -0.26549118757247925
Batch 20/64 loss: -0.2797570824623108
Batch 21/64 loss: -0.278864324092865
Batch 22/64 loss: -0.2750471532344818
Batch 23/64 loss: -0.2773471474647522
Batch 24/64 loss: -0.27893489599227905
Batch 25/64 loss: -0.2691571116447449
Batch 26/64 loss: -0.25670528411865234
Batch 27/64 loss: -0.2548633813858032
Batch 28/64 loss: -0.2628558278083801
Batch 29/64 loss: -0.25934797525405884
Batch 30/64 loss: -0.2740800976753235
Batch 31/64 loss: -0.2770153284072876
Batch 32/64 loss: -0.27538827061653137
Batch 33/64 loss: -0.2549772262573242
Batch 34/64 loss: -0.2729019522666931
Batch 35/64 loss: -0.28223103284835815
Batch 36/64 loss: -0.266748309135437
Batch 37/64 loss: -0.2480841875076294
Batch 38/64 loss: -0.2419893741607666
Batch 39/64 loss: -0.2675219178199768
Batch 40/64 loss: -0.2535248398780823
Batch 41/64 loss: -0.25945210456848145
Batch 42/64 loss: -0.25862443447113037
Batch 43/64 loss: -0.2652132511138916
Batch 44/64 loss: -0.27046024799346924
Batch 45/64 loss: -0.28387629985809326
Batch 46/64 loss: -0.2738218307495117
Batch 47/64 loss: -0.2666893005371094
Batch 48/64 loss: -0.2618969678878784
Batch 49/64 loss: -0.25877296924591064
Batch 50/64 loss: -0.26516449451446533
Batch 51/64 loss: -0.26378536224365234
Batch 52/64 loss: -0.2854781448841095
Batch 53/64 loss: -0.2866695523262024
Batch 54/64 loss: -0.26565635204315186
Batch 55/64 loss: -0.2818564474582672
Batch 56/64 loss: -0.27167853713035583
Batch 57/64 loss: -0.2763526141643524
Batch 58/64 loss: -0.28396692872047424
Batch 59/64 loss: -0.28022104501724243
Batch 60/64 loss: -0.2769430875778198
Batch 61/64 loss: -0.2725493311882019
Batch 62/64 loss: -0.28117460012435913
Batch 63/64 loss: -0.27670884132385254
Batch 64/64 loss: -0.2812695801258087
Epoch 229  Train loss: -0.27045605872191636  Val loss: -0.2460341766937492
Epoch 230
-------------------------------
Batch 1/64 loss: -0.2733651399612427
Batch 2/64 loss: -0.2606043815612793
Batch 3/64 loss: -0.2508370280265808
Batch 4/64 loss: -0.2792680263519287
Batch 5/64 loss: -0.2697627544403076
Batch 6/64 loss: -0.2906741499900818
Batch 7/64 loss: -0.27703335881233215
Batch 8/64 loss: -0.26520365476608276
Batch 9/64 loss: -0.28089067339897156
Batch 10/64 loss: -0.25850754976272583
Batch 11/64 loss: -0.2552300691604614
Batch 12/64 loss: -0.24813783168792725
Batch 13/64 loss: -0.2836725413799286
Batch 14/64 loss: -0.2773658037185669
Batch 15/64 loss: -0.27070093154907227
Batch 16/64 loss: -0.28240543603897095
Batch 17/64 loss: -0.2938784956932068
Batch 18/64 loss: -0.26700276136398315
Batch 19/64 loss: -0.2670270800590515
Batch 20/64 loss: -0.2685664892196655
Batch 21/64 loss: -0.26841551065444946
Batch 22/64 loss: -0.2640899419784546
Batch 23/64 loss: -0.27259546518325806
Batch 24/64 loss: -0.25922590494155884
Batch 25/64 loss: -0.2577320337295532
Batch 26/64 loss: -0.2727217674255371
Batch 27/64 loss: -0.27423036098480225
Batch 28/64 loss: -0.27122172713279724
Batch 29/64 loss: -0.2706749439239502
Batch 30/64 loss: -0.2624660134315491
Batch 31/64 loss: -0.2717757821083069
Batch 32/64 loss: -0.2745860517024994
Batch 33/64 loss: -0.24863970279693604
Batch 34/64 loss: -0.28071388602256775
Batch 35/64 loss: -0.27453964948654175
Batch 36/64 loss: -0.27998751401901245
Batch 37/64 loss: -0.27110379934310913
Batch 38/64 loss: -0.2796633541584015
Batch 39/64 loss: -0.2718666195869446
Batch 40/64 loss: -0.2675902843475342
Batch 41/64 loss: -0.26830774545669556
Batch 42/64 loss: -0.26240599155426025
Batch 43/64 loss: -0.2655687928199768
Batch 44/64 loss: -0.2726619243621826
Batch 45/64 loss: -0.2785821855068207
Batch 46/64 loss: -0.2839289605617523
Batch 47/64 loss: -0.2760358154773712
Batch 48/64 loss: -0.28147780895233154
Batch 49/64 loss: -0.2732534408569336
Batch 50/64 loss: -0.28068825602531433
Batch 51/64 loss: -0.2762611508369446
Batch 52/64 loss: -0.2592774033546448
Batch 53/64 loss: -0.2821069657802582
Batch 54/64 loss: -0.2719762325286865
Batch 55/64 loss: -0.27383488416671753
Batch 56/64 loss: -0.2767234146595001
Batch 57/64 loss: -0.2731732428073883
Batch 58/64 loss: -0.24684935808181763
Batch 59/64 loss: -0.2655033469200134
Batch 60/64 loss: -0.2812526822090149
Batch 61/64 loss: -0.2694588899612427
Batch 62/64 loss: -0.2682971954345703
Batch 63/64 loss: -0.27256473898887634
Batch 64/64 loss: -0.27048367261886597
Epoch 230  Train loss: -0.2710121984575309  Val loss: -0.24675516389899238
Epoch 231
-------------------------------
Batch 1/64 loss: -0.2624583840370178
Batch 2/64 loss: -0.2887139320373535
Batch 3/64 loss: -0.2545779347419739
Batch 4/64 loss: -0.26834577322006226
Batch 5/64 loss: -0.26167768239974976
Batch 6/64 loss: -0.27005916833877563
Batch 7/64 loss: -0.28391408920288086
Batch 8/64 loss: -0.2734551429748535
Batch 9/64 loss: -0.27938467264175415
Batch 10/64 loss: -0.2803230285644531
Batch 11/64 loss: -0.2639721632003784
Batch 12/64 loss: -0.2754818797111511
Batch 13/64 loss: -0.28012871742248535
Batch 14/64 loss: -0.27517732977867126
Batch 15/64 loss: -0.2736647129058838
Batch 16/64 loss: -0.27431201934814453
Batch 17/64 loss: -0.2811441719532013
Batch 18/64 loss: -0.2877756357192993
Batch 19/64 loss: -0.29279279708862305
Batch 20/64 loss: -0.266044020652771
Batch 21/64 loss: -0.26642394065856934
Batch 22/64 loss: -0.27784377336502075
Batch 23/64 loss: -0.2844715118408203
Batch 24/64 loss: -0.2674645185470581
Batch 25/64 loss: -0.277081698179245
Batch 26/64 loss: -0.26962733268737793
Batch 27/64 loss: -0.28978097438812256
Batch 28/64 loss: -0.28577369451522827
Batch 29/64 loss: -0.26882314682006836
Batch 30/64 loss: -0.27422600984573364
Batch 31/64 loss: -0.2871050238609314
Batch 32/64 loss: -0.2596549987792969
Batch 33/64 loss: -0.27938318252563477
Batch 34/64 loss: -0.27571266889572144
Batch 35/64 loss: -0.2857396900653839
Batch 36/64 loss: -0.29913565516471863
Batch 37/64 loss: -0.2736817002296448
Batch 38/64 loss: -0.27659934759140015
Batch 39/64 loss: -0.27473145723342896
Batch 40/64 loss: -0.26858824491500854
Batch 41/64 loss: -0.28108811378479004
Batch 42/64 loss: -0.2668478488922119
Batch 43/64 loss: -0.2744029760360718
Batch 44/64 loss: -0.27173876762390137
Batch 45/64 loss: -0.2805263102054596
Batch 46/64 loss: -0.23611867427825928
Batch 47/64 loss: -0.2739095687866211
Batch 48/64 loss: -0.28204312920570374
Batch 49/64 loss: -0.268504798412323
Batch 50/64 loss: -0.26551753282546997
Batch 51/64 loss: -0.26893699169158936
Batch 52/64 loss: -0.2690471410751343
Batch 53/64 loss: -0.27189332246780396
Batch 54/64 loss: -0.27882128953933716
Batch 55/64 loss: -0.2622639536857605
Batch 56/64 loss: -0.2703954577445984
Batch 57/64 loss: -0.2851242125034332
Batch 58/64 loss: -0.2775537371635437
Batch 59/64 loss: -0.27918529510498047
Batch 60/64 loss: -0.2823253870010376
Batch 61/64 loss: -0.27357804775238037
Batch 62/64 loss: -0.2366628646850586
Batch 63/64 loss: -0.27679139375686646
Batch 64/64 loss: -0.2609422206878662
Epoch 231  Train loss: -0.27394878911037074  Val loss: -0.24328058323090018
Epoch 232
-------------------------------
Batch 1/64 loss: -0.2772161364555359
Batch 2/64 loss: -0.26624220609664917
Batch 3/64 loss: -0.29631584882736206
Batch 4/64 loss: -0.2680070400238037
Batch 5/64 loss: -0.2690122723579407
Batch 6/64 loss: -0.2776152789592743
Batch 7/64 loss: -0.2690560221672058
Batch 8/64 loss: -0.26743990182876587
Batch 9/64 loss: -0.2626389265060425
Batch 10/64 loss: -0.2700962424278259
Batch 11/64 loss: -0.2866559624671936
Batch 12/64 loss: -0.28364884853363037
Batch 13/64 loss: -0.26293182373046875
Batch 14/64 loss: -0.27190589904785156
Batch 15/64 loss: -0.2756621539592743
Batch 16/64 loss: -0.2820553481578827
Batch 17/64 loss: -0.28611546754837036
Batch 18/64 loss: -0.2666977643966675
Batch 19/64 loss: -0.26807212829589844
Batch 20/64 loss: -0.2884238660335541
Batch 21/64 loss: -0.2624765634536743
Batch 22/64 loss: -0.2661004066467285
Batch 23/64 loss: -0.26601290702819824
Batch 24/64 loss: -0.2644030451774597
Batch 25/64 loss: -0.26552653312683105
Batch 26/64 loss: -0.2810577154159546
Batch 27/64 loss: -0.24365901947021484
Batch 28/64 loss: -0.26738440990448
Batch 29/64 loss: -0.26583319902420044
Batch 30/64 loss: -0.2730517089366913
Batch 31/64 loss: -0.2830984592437744
Batch 32/64 loss: -0.2748914957046509
Batch 33/64 loss: -0.2723805904388428
Batch 34/64 loss: -0.2517702579498291
Batch 35/64 loss: -0.27299439907073975
Batch 36/64 loss: -0.26224851608276367
Batch 37/64 loss: -0.26740020513534546
Batch 38/64 loss: -0.283907413482666
Batch 39/64 loss: -0.2754170596599579
Batch 40/64 loss: -0.2768784165382385
Batch 41/64 loss: -0.2796741724014282
Batch 42/64 loss: -0.26183974742889404
Batch 43/64 loss: -0.27585741877555847
Batch 44/64 loss: -0.27019017934799194
Batch 45/64 loss: -0.28084999322891235
Batch 46/64 loss: -0.26953649520874023
Batch 47/64 loss: -0.2788008451461792
Batch 48/64 loss: -0.2546253204345703
Batch 49/64 loss: -0.28321564197540283
Batch 50/64 loss: -0.278033047914505
Batch 51/64 loss: -0.27586841583251953
Batch 52/64 loss: -0.2779567837715149
Batch 53/64 loss: -0.276900053024292
Batch 54/64 loss: -0.2862566113471985
Batch 55/64 loss: -0.28701961040496826
Batch 56/64 loss: -0.2543381452560425
Batch 57/64 loss: -0.2749170958995819
Batch 58/64 loss: -0.2720433473587036
Batch 59/64 loss: -0.2854228615760803
Batch 60/64 loss: -0.2680385708808899
Batch 61/64 loss: -0.2588236331939697
Batch 62/64 loss: -0.28480246663093567
Batch 63/64 loss: -0.27884727716445923
Batch 64/64 loss: -0.2545185685157776
Epoch 232  Train loss: -0.2725811469788645  Val loss: -0.24934097335920302
Epoch 233
-------------------------------
Batch 1/64 loss: -0.2750728726387024
Batch 2/64 loss: -0.26688718795776367
Batch 3/64 loss: -0.26816326379776
Batch 4/64 loss: -0.2684338688850403
Batch 5/64 loss: -0.265131413936615
Batch 6/64 loss: -0.2883641719818115
Batch 7/64 loss: -0.28169190883636475
Batch 8/64 loss: -0.263866662979126
Batch 9/64 loss: -0.27173495292663574
Batch 10/64 loss: -0.2952578067779541
Batch 11/64 loss: -0.2701667547225952
Batch 12/64 loss: -0.28620457649230957
Batch 13/64 loss: -0.26359230279922485
Batch 14/64 loss: -0.253972589969635
Batch 15/64 loss: -0.26757168769836426
Batch 16/64 loss: -0.28212833404541016
Batch 17/64 loss: -0.2761029601097107
Batch 18/64 loss: -0.27858829498291016
Batch 19/64 loss: -0.2747091054916382
Batch 20/64 loss: -0.2827557921409607
Batch 21/64 loss: -0.27006208896636963
Batch 22/64 loss: -0.25820887088775635
Batch 23/64 loss: -0.27649450302124023
Batch 24/64 loss: -0.28929051756858826
Batch 25/64 loss: -0.281284898519516
Batch 26/64 loss: -0.2798759937286377
Batch 27/64 loss: -0.27852171659469604
Batch 28/64 loss: -0.26961785554885864
Batch 29/64 loss: -0.2612568140029907
Batch 30/64 loss: -0.24643445014953613
Batch 31/64 loss: -0.26676565408706665
Batch 32/64 loss: -0.2509387731552124
Batch 33/64 loss: -0.26923084259033203
Batch 34/64 loss: -0.2528077960014343
Batch 35/64 loss: -0.2734121084213257
Batch 36/64 loss: -0.2621796131134033
Batch 37/64 loss: -0.27252525091171265
Batch 38/64 loss: -0.27321726083755493
Batch 39/64 loss: -0.26011794805526733
Batch 40/64 loss: -0.2548207640647888
Batch 41/64 loss: -0.27719324827194214
Batch 42/64 loss: -0.26538825035095215
Batch 43/64 loss: -0.27798011898994446
Batch 44/64 loss: -0.2758603096008301
Batch 45/64 loss: -0.2753320336341858
Batch 46/64 loss: -0.26884615421295166
Batch 47/64 loss: -0.27628231048583984
Batch 48/64 loss: -0.26991981267929077
Batch 49/64 loss: -0.257111132144928
Batch 50/64 loss: -0.28824782371520996
Batch 51/64 loss: -0.2662273049354553
Batch 52/64 loss: -0.2701416015625
Batch 53/64 loss: -0.27078694105148315
Batch 54/64 loss: -0.2875923216342926
Batch 55/64 loss: -0.2808297872543335
Batch 56/64 loss: -0.27449214458465576
Batch 57/64 loss: -0.2707673907279968
Batch 58/64 loss: -0.2879456877708435
Batch 59/64 loss: -0.26429879665374756
Batch 60/64 loss: -0.2858453392982483
Batch 61/64 loss: -0.2668623924255371
Batch 62/64 loss: -0.27566441893577576
Batch 63/64 loss: -0.2634662389755249
Batch 64/64 loss: -0.265752911567688
Epoch 233  Train loss: -0.2717467682034362  Val loss: -0.2421824993546476
Epoch 234
-------------------------------
Batch 1/64 loss: -0.2809144854545593
Batch 2/64 loss: -0.2622259855270386
Batch 3/64 loss: -0.28507059812545776
Batch 4/64 loss: -0.2874033451080322
Batch 5/64 loss: -0.26345938444137573
Batch 6/64 loss: -0.27215060591697693
Batch 7/64 loss: -0.2874772846698761
Batch 8/64 loss: -0.27698737382888794
Batch 9/64 loss: -0.2743765711784363
Batch 10/64 loss: -0.25966793298721313
Batch 11/64 loss: -0.26106786727905273
Batch 12/64 loss: -0.26468873023986816
Batch 13/64 loss: -0.26589149236679077
Batch 14/64 loss: -0.2552121877670288
Batch 15/64 loss: -0.26142919063568115
Batch 16/64 loss: -0.2837819755077362
Batch 17/64 loss: -0.2670990228652954
Batch 18/64 loss: -0.26494288444519043
Batch 19/64 loss: -0.2688484191894531
Batch 20/64 loss: -0.23123323917388916
Batch 21/64 loss: -0.2816154956817627
Batch 22/64 loss: -0.2755773365497589
Batch 23/64 loss: -0.27732497453689575
Batch 24/64 loss: -0.2736152112483978
Batch 25/64 loss: -0.26947832107543945
Batch 26/64 loss: -0.27131181955337524
Batch 27/64 loss: -0.2814033031463623
Batch 28/64 loss: -0.2677249312400818
Batch 29/64 loss: -0.266417920589447
Batch 30/64 loss: -0.27190250158309937
Batch 31/64 loss: -0.26678144931793213
Batch 32/64 loss: -0.28560903668403625
Batch 33/64 loss: -0.25492846965789795
Batch 34/64 loss: -0.26035118103027344
Batch 35/64 loss: -0.26487118005752563
Batch 36/64 loss: -0.27583181858062744
Batch 37/64 loss: -0.2821124494075775
Batch 38/64 loss: -0.2750192880630493
Batch 39/64 loss: -0.2919837534427643
Batch 40/64 loss: -0.2596750855445862
Batch 41/64 loss: -0.2684333324432373
Batch 42/64 loss: -0.2980058491230011
Batch 43/64 loss: -0.2645116448402405
Batch 44/64 loss: -0.2807948589324951
Batch 45/64 loss: -0.2799818515777588
Batch 46/64 loss: -0.2757323086261749
Batch 47/64 loss: -0.25001049041748047
Batch 48/64 loss: -0.26365768909454346
Batch 49/64 loss: -0.26804524660110474
Batch 50/64 loss: -0.2730948030948639
Batch 51/64 loss: -0.26634281873703003
Batch 52/64 loss: -0.25776952505111694
Batch 53/64 loss: -0.24616354703903198
Batch 54/64 loss: -0.2785191535949707
Batch 55/64 loss: -0.26074784994125366
Batch 56/64 loss: -0.2863314151763916
Batch 57/64 loss: -0.2656739354133606
Batch 58/64 loss: -0.2764042615890503
Batch 59/64 loss: -0.2753869891166687
Batch 60/64 loss: -0.2791418433189392
Batch 61/64 loss: -0.25055742263793945
Batch 62/64 loss: -0.24018722772598267
Batch 63/64 loss: -0.26655352115631104
Batch 64/64 loss: -0.26868540048599243
Epoch 234  Train loss: -0.26982001159705366  Val loss: -0.22509232788151481
Epoch 235
-------------------------------
Batch 1/64 loss: -0.24797332286834717
Batch 2/64 loss: -0.2517330050468445
Batch 3/64 loss: -0.2679617404937744
Batch 4/64 loss: -0.25704121589660645
Batch 5/64 loss: -0.28264665603637695
Batch 6/64 loss: -0.27401670813560486
Batch 7/64 loss: -0.24885255098342896
Batch 8/64 loss: -0.23497498035430908
Batch 9/64 loss: -0.27320027351379395
Batch 10/64 loss: -0.26674801111221313
Batch 11/64 loss: -0.26844578981399536
Batch 12/64 loss: -0.2652214765548706
Batch 13/64 loss: -0.2756465971469879
Batch 14/64 loss: -0.26825863122940063
Batch 15/64 loss: -0.24226385354995728
Batch 16/64 loss: -0.27333712577819824
Batch 17/64 loss: -0.2651705741882324
Batch 18/64 loss: -0.26614290475845337
Batch 19/64 loss: -0.260617196559906
Batch 20/64 loss: -0.2737351059913635
Batch 21/64 loss: -0.2677527666091919
Batch 22/64 loss: -0.2609696388244629
Batch 23/64 loss: -0.28192639350891113
Batch 24/64 loss: -0.27248895168304443
Batch 25/64 loss: -0.2823096215724945
Batch 26/64 loss: -0.27069026231765747
Batch 27/64 loss: -0.26807522773742676
Batch 28/64 loss: -0.28007182478904724
Batch 29/64 loss: -0.2746215760707855
Batch 30/64 loss: -0.2673656940460205
Batch 31/64 loss: -0.2783464193344116
Batch 32/64 loss: -0.26531797647476196
Batch 33/64 loss: -0.2770593464374542
Batch 34/64 loss: -0.27608823776245117
Batch 35/64 loss: -0.27195513248443604
Batch 36/64 loss: -0.28121721744537354
Batch 37/64 loss: -0.2867555022239685
Batch 38/64 loss: -0.2660616636276245
Batch 39/64 loss: -0.2691713571548462
Batch 40/64 loss: -0.2790995240211487
Batch 41/64 loss: -0.2707598805427551
Batch 42/64 loss: -0.29128462076187134
Batch 43/64 loss: -0.2607101798057556
Batch 44/64 loss: -0.27261513471603394
Batch 45/64 loss: -0.2786979377269745
Batch 46/64 loss: -0.2757452130317688
Batch 47/64 loss: -0.26493799686431885
Batch 48/64 loss: -0.2651236653327942
Batch 49/64 loss: -0.25927627086639404
Batch 50/64 loss: -0.24988412857055664
Batch 51/64 loss: -0.28425541520118713
Batch 52/64 loss: -0.26640182733535767
Batch 53/64 loss: -0.2659262418746948
Batch 54/64 loss: -0.2689031958580017
Batch 55/64 loss: -0.2672101855278015
Batch 56/64 loss: -0.2810097336769104
Batch 57/64 loss: -0.2715802788734436
Batch 58/64 loss: -0.2667633891105652
Batch 59/64 loss: -0.2647305727005005
Batch 60/64 loss: -0.26035845279693604
Batch 61/64 loss: -0.26882559061050415
Batch 62/64 loss: -0.2647317051887512
Batch 63/64 loss: -0.27670231461524963
Batch 64/64 loss: -0.26759326457977295
Epoch 235  Train loss: -0.2688386028888179  Val loss: -0.23293553492457597
Epoch 236
-------------------------------
Batch 1/64 loss: -0.2808949053287506
Batch 2/64 loss: -0.2729584574699402
Batch 3/64 loss: -0.28449103236198425
Batch 4/64 loss: -0.27332502603530884
Batch 5/64 loss: -0.27190133929252625
Batch 6/64 loss: -0.278883159160614
Batch 7/64 loss: -0.2730932831764221
Batch 8/64 loss: -0.2621344327926636
Batch 9/64 loss: -0.2706977128982544
Batch 10/64 loss: -0.2578643560409546
Batch 11/64 loss: -0.2500634789466858
Batch 12/64 loss: -0.27664318680763245
Batch 13/64 loss: -0.28727927803993225
Batch 14/64 loss: -0.273374080657959
Batch 15/64 loss: -0.2876271605491638
Batch 16/64 loss: -0.26894575357437134
Batch 17/64 loss: -0.2692216634750366
Batch 18/64 loss: -0.2906239628791809
Batch 19/64 loss: -0.29400834441185
Batch 20/64 loss: -0.2674459218978882
Batch 21/64 loss: -0.2740493416786194
Batch 22/64 loss: -0.25783586502075195
Batch 23/64 loss: -0.26683884859085083
Batch 24/64 loss: -0.280392050743103
Batch 25/64 loss: -0.27249205112457275
Batch 26/64 loss: -0.2698580026626587
Batch 27/64 loss: -0.265014111995697
Batch 28/64 loss: -0.2709352374076843
Batch 29/64 loss: -0.2709384560585022
Batch 30/64 loss: -0.2682254910469055
Batch 31/64 loss: -0.26138585805892944
Batch 32/64 loss: -0.27466556429862976
Batch 33/64 loss: -0.2642326354980469
Batch 34/64 loss: -0.2808520197868347
Batch 35/64 loss: -0.2663158178329468
Batch 36/64 loss: -0.26304155588150024
Batch 37/64 loss: -0.2668485641479492
Batch 38/64 loss: -0.29164350032806396
Batch 39/64 loss: -0.27052515745162964
Batch 40/64 loss: -0.27562806010246277
Batch 41/64 loss: -0.26856285333633423
Batch 42/64 loss: -0.28478866815567017
Batch 43/64 loss: -0.27996858954429626
Batch 44/64 loss: -0.25839298963546753
Batch 45/64 loss: -0.2783135771751404
Batch 46/64 loss: -0.2640775442123413
Batch 47/64 loss: -0.2877180874347687
Batch 48/64 loss: -0.258381724357605
Batch 49/64 loss: -0.2715442180633545
Batch 50/64 loss: -0.26594090461730957
Batch 51/64 loss: -0.2721630334854126
Batch 52/64 loss: -0.2764691710472107
Batch 53/64 loss: -0.2884282171726227
Batch 54/64 loss: -0.2774474620819092
Batch 55/64 loss: -0.28455957770347595
Batch 56/64 loss: -0.26427561044692993
Batch 57/64 loss: -0.2844792604446411
Batch 58/64 loss: -0.2801172137260437
Batch 59/64 loss: -0.2777963876724243
Batch 60/64 loss: -0.2656708359718323
Batch 61/64 loss: -0.2801193594932556
Batch 62/64 loss: -0.27905508875846863
Batch 63/64 loss: -0.2717367112636566
Batch 64/64 loss: -0.2807261347770691
Epoch 236  Train loss: -0.2734705319591597  Val loss: -0.2364918306930778
Epoch 237
-------------------------------
Batch 1/64 loss: -0.2694814205169678
Batch 2/64 loss: -0.27649450302124023
Batch 3/64 loss: -0.2786339521408081
Batch 4/64 loss: -0.2782132923603058
Batch 5/64 loss: -0.28835365176200867
Batch 6/64 loss: -0.267694890499115
Batch 7/64 loss: -0.2882905900478363
Batch 8/64 loss: -0.2722415030002594
Batch 9/64 loss: -0.2818775773048401
Batch 10/64 loss: -0.2653900384902954
Batch 11/64 loss: -0.26117897033691406
Batch 12/64 loss: -0.27522194385528564
Batch 13/64 loss: -0.28578510880470276
Batch 14/64 loss: -0.2693100571632385
Batch 15/64 loss: -0.2818308174610138
Batch 16/64 loss: -0.2589479088783264
Batch 17/64 loss: -0.25976407527923584
Batch 18/64 loss: -0.26974934339523315
Batch 19/64 loss: -0.2741084396839142
Batch 20/64 loss: -0.2731693387031555
Batch 21/64 loss: -0.2655177712440491
Batch 22/64 loss: -0.2738036811351776
Batch 23/64 loss: -0.27457332611083984
Batch 24/64 loss: -0.26786744594573975
Batch 25/64 loss: -0.2728273272514343
Batch 26/64 loss: -0.26349693536758423
Batch 27/64 loss: -0.2771434485912323
Batch 28/64 loss: -0.25540298223495483
Batch 29/64 loss: -0.272149920463562
Batch 30/64 loss: -0.2595180869102478
Batch 31/64 loss: -0.2710687518119812
Batch 32/64 loss: -0.27777647972106934
Batch 33/64 loss: -0.26785385608673096
Batch 34/64 loss: -0.2667475938796997
Batch 35/64 loss: -0.278386652469635
Batch 36/64 loss: -0.261255145072937
Batch 37/64 loss: -0.27701789140701294
Batch 38/64 loss: -0.2624983787536621
Batch 39/64 loss: -0.265015184879303
Batch 40/64 loss: -0.26987606287002563
Batch 41/64 loss: -0.2762061655521393
Batch 42/64 loss: -0.27105003595352173
Batch 43/64 loss: -0.26861047744750977
Batch 44/64 loss: -0.2718654274940491
Batch 45/64 loss: -0.26678305864334106
Batch 46/64 loss: -0.2778410315513611
Batch 47/64 loss: -0.26050734519958496
Batch 48/64 loss: -0.26709043979644775
Batch 49/64 loss: -0.27894943952560425
Batch 50/64 loss: -0.27840983867645264
Batch 51/64 loss: -0.2651820182800293
Batch 52/64 loss: -0.28730905055999756
Batch 53/64 loss: -0.2726494073867798
Batch 54/64 loss: -0.2793414890766144
Batch 55/64 loss: -0.27126163244247437
Batch 56/64 loss: -0.25250905752182007
Batch 57/64 loss: -0.2437017560005188
Batch 58/64 loss: -0.23135417699813843
Batch 59/64 loss: -0.27381396293640137
Batch 60/64 loss: -0.2723039984703064
Batch 61/64 loss: -0.2700458765029907
Batch 62/64 loss: -0.28146064281463623
Batch 63/64 loss: -0.26985472440719604
Batch 64/64 loss: -0.2619965076446533
Epoch 237  Train loss: -0.27043345535502716  Val loss: -0.23319276399219158
Epoch 238
-------------------------------
Batch 1/64 loss: -0.275820255279541
Batch 2/64 loss: -0.277492880821228
Batch 3/64 loss: -0.27020591497421265
Batch 4/64 loss: -0.26312434673309326
Batch 5/64 loss: -0.27601587772369385
Batch 6/64 loss: -0.26646512746810913
Batch 7/64 loss: -0.2627379298210144
Batch 8/64 loss: -0.2860492467880249
Batch 9/64 loss: -0.2611064314842224
Batch 10/64 loss: -0.2759352922439575
Batch 11/64 loss: -0.25165754556655884
Batch 12/64 loss: -0.2789365351200104
Batch 13/64 loss: -0.27960866689682007
Batch 14/64 loss: -0.25721466541290283
Batch 15/64 loss: -0.2748800218105316
Batch 16/64 loss: -0.27885958552360535
Batch 17/64 loss: -0.2769520580768585
Batch 18/64 loss: -0.2762375473976135
Batch 19/64 loss: -0.2822135090827942
Batch 20/64 loss: -0.2814146876335144
Batch 21/64 loss: -0.28970247507095337
Batch 22/64 loss: -0.2889484763145447
Batch 23/64 loss: -0.2700749635696411
Batch 24/64 loss: -0.2643219232559204
Batch 25/64 loss: -0.28633105754852295
Batch 26/64 loss: -0.258308470249176
Batch 27/64 loss: -0.28318846225738525
Batch 28/64 loss: -0.26647186279296875
Batch 29/64 loss: -0.26558905839920044
Batch 30/64 loss: -0.26794981956481934
Batch 31/64 loss: -0.2679406404495239
Batch 32/64 loss: -0.2879472076892853
Batch 33/64 loss: -0.28522008657455444
Batch 34/64 loss: -0.2818755805492401
Batch 35/64 loss: -0.2644577622413635
Batch 36/64 loss: -0.2695310115814209
Batch 37/64 loss: -0.27970314025878906
Batch 38/64 loss: -0.2548432946205139
Batch 39/64 loss: -0.2676350474357605
Batch 40/64 loss: -0.26432955265045166
Batch 41/64 loss: -0.2840174436569214
Batch 42/64 loss: -0.27900218963623047
Batch 43/64 loss: -0.2584717273712158
Batch 44/64 loss: -0.2675180435180664
Batch 45/64 loss: -0.27221453189849854
Batch 46/64 loss: -0.2726367712020874
Batch 47/64 loss: -0.26678192615509033
Batch 48/64 loss: -0.26119565963745117
Batch 49/64 loss: -0.2722494602203369
Batch 50/64 loss: -0.2754136323928833
Batch 51/64 loss: -0.26979297399520874
Batch 52/64 loss: -0.26568835973739624
Batch 53/64 loss: -0.273607075214386
Batch 54/64 loss: -0.27591702342033386
Batch 55/64 loss: -0.2705167531967163
Batch 56/64 loss: -0.273764431476593
Batch 57/64 loss: -0.27147170901298523
Batch 58/64 loss: -0.26863640546798706
Batch 59/64 loss: -0.26565051078796387
Batch 60/64 loss: -0.2658619284629822
Batch 61/64 loss: -0.27619439363479614
Batch 62/64 loss: -0.26880407333374023
Batch 63/64 loss: -0.2704010605812073
Batch 64/64 loss: -0.27966371178627014
Epoch 238  Train loss: -0.272201629596598  Val loss: -0.24107714944688724
Epoch 239
-------------------------------
Batch 1/64 loss: -0.2725985050201416
Batch 2/64 loss: -0.27451640367507935
Batch 3/64 loss: -0.2629508972167969
Batch 4/64 loss: -0.25920021533966064
Batch 5/64 loss: -0.2673766613006592
Batch 6/64 loss: -0.2566283941268921
Batch 7/64 loss: -0.2665708065032959
Batch 8/64 loss: -0.2848264276981354
Batch 9/64 loss: -0.2697387933731079
Batch 10/64 loss: -0.28005093336105347
Batch 11/64 loss: -0.2684880495071411
Batch 12/64 loss: -0.273430198431015
Batch 13/64 loss: -0.2844356894493103
Batch 14/64 loss: -0.2888075113296509
Batch 15/64 loss: -0.26869910955429077
Batch 16/64 loss: -0.2841920256614685
Batch 17/64 loss: -0.25993412733078003
Batch 18/64 loss: -0.2584049105644226
Batch 19/64 loss: -0.2792651951313019
Batch 20/64 loss: -0.2534710764884949
Batch 21/64 loss: -0.2596943974494934
Batch 22/64 loss: -0.2706378698348999
Batch 23/64 loss: -0.265322208404541
Batch 24/64 loss: -0.24473083019256592
Batch 25/64 loss: -0.26263439655303955
Batch 26/64 loss: -0.2677538990974426
Batch 27/64 loss: -0.279258668422699
Batch 28/64 loss: -0.2709469795227051
Batch 29/64 loss: -0.2708340287208557
Batch 30/64 loss: -0.25949692726135254
Batch 31/64 loss: -0.2814885377883911
Batch 32/64 loss: -0.2614380121231079
Batch 33/64 loss: -0.27249324321746826
Batch 34/64 loss: -0.26146841049194336
Batch 35/64 loss: -0.2653994560241699
Batch 36/64 loss: -0.2661215662956238
Batch 37/64 loss: -0.26201069355010986
Batch 38/64 loss: -0.2707781195640564
Batch 39/64 loss: -0.2507627010345459
Batch 40/64 loss: -0.26638340950012207
Batch 41/64 loss: -0.28424569964408875
Batch 42/64 loss: -0.2846289873123169
Batch 43/64 loss: -0.2723821997642517
Batch 44/64 loss: -0.2733292579650879
Batch 45/64 loss: -0.2582109570503235
Batch 46/64 loss: -0.2893963158130646
Batch 47/64 loss: -0.2648821473121643
Batch 48/64 loss: -0.26386821269989014
Batch 49/64 loss: -0.2734321355819702
Batch 50/64 loss: -0.2718559503555298
Batch 51/64 loss: -0.2693485617637634
Batch 52/64 loss: -0.2870304584503174
Batch 53/64 loss: -0.2756498456001282
Batch 54/64 loss: -0.28930720686912537
Batch 55/64 loss: -0.2780408263206482
Batch 56/64 loss: -0.261621356010437
Batch 57/64 loss: -0.27279072999954224
Batch 58/64 loss: -0.2782513499259949
Batch 59/64 loss: -0.2724021077156067
Batch 60/64 loss: -0.27895426750183105
Batch 61/64 loss: -0.26937365531921387
Batch 62/64 loss: -0.2613942623138428
Batch 63/64 loss: -0.27701008319854736
Batch 64/64 loss: -0.2732934355735779
Epoch 239  Train loss: -0.2703626186239953  Val loss: -0.24486041212409632
Epoch 240
-------------------------------
Batch 1/64 loss: -0.2779942750930786
Batch 2/64 loss: -0.27966058254241943
Batch 3/64 loss: -0.27097010612487793
Batch 4/64 loss: -0.2756001055240631
Batch 5/64 loss: -0.2821658253669739
Batch 6/64 loss: -0.27537259459495544
Batch 7/64 loss: -0.28685784339904785
Batch 8/64 loss: -0.2694483995437622
Batch 9/64 loss: -0.2702292203903198
Batch 10/64 loss: -0.2721526622772217
Batch 11/64 loss: -0.25423663854599
Batch 12/64 loss: -0.2802882790565491
Batch 13/64 loss: -0.2652122974395752
Batch 14/64 loss: -0.2681463360786438
Batch 15/64 loss: -0.27348533272743225
Batch 16/64 loss: -0.2776488661766052
Batch 17/64 loss: -0.2676939070224762
Batch 18/64 loss: -0.27796176075935364
Batch 19/64 loss: -0.27494025230407715
Batch 20/64 loss: -0.2683812379837036
Batch 21/64 loss: -0.2850029170513153
Batch 22/64 loss: -0.2669561505317688
Batch 23/64 loss: -0.2711038589477539
Batch 24/64 loss: -0.27662917971611023
Batch 25/64 loss: -0.27973300218582153
Batch 26/64 loss: -0.2770087718963623
Batch 27/64 loss: -0.27652543783187866
Batch 28/64 loss: -0.27789783477783203
Batch 29/64 loss: -0.28408247232437134
Batch 30/64 loss: -0.286621630191803
Batch 31/64 loss: -0.2717101573944092
Batch 32/64 loss: -0.27253860235214233
Batch 33/64 loss: -0.2885070741176605
Batch 34/64 loss: -0.27513670921325684
Batch 35/64 loss: -0.26978588104248047
Batch 36/64 loss: -0.27777722477912903
Batch 37/64 loss: -0.27747130393981934
Batch 38/64 loss: -0.2781882882118225
Batch 39/64 loss: -0.27398359775543213
Batch 40/64 loss: -0.28114715218544006
Batch 41/64 loss: -0.2815953493118286
Batch 42/64 loss: -0.26533329486846924
Batch 43/64 loss: -0.27302825450897217
Batch 44/64 loss: -0.2798576354980469
Batch 45/64 loss: -0.2655102610588074
Batch 46/64 loss: -0.27528828382492065
Batch 47/64 loss: -0.2600259780883789
Batch 48/64 loss: -0.28881290555000305
Batch 49/64 loss: -0.2821899652481079
Batch 50/64 loss: -0.2572792172431946
Batch 51/64 loss: -0.2823794186115265
Batch 52/64 loss: -0.2689415216445923
Batch 53/64 loss: -0.2843693196773529
Batch 54/64 loss: -0.28229132294654846
Batch 55/64 loss: -0.28715288639068604
Batch 56/64 loss: -0.27000004053115845
Batch 57/64 loss: -0.2821931838989258
Batch 58/64 loss: -0.26904618740081787
Batch 59/64 loss: -0.27260124683380127
Batch 60/64 loss: -0.27673548460006714
Batch 61/64 loss: -0.276643842458725
Batch 62/64 loss: -0.271986722946167
Batch 63/64 loss: -0.2510119080543518
Batch 64/64 loss: -0.2921038866043091
Epoch 240  Train loss: -0.2751310731850418  Val loss: -0.2529043164040215
Epoch 241
-------------------------------
Batch 1/64 loss: -0.26638704538345337
Batch 2/64 loss: -0.27202141284942627
Batch 3/64 loss: -0.2662454843521118
Batch 4/64 loss: -0.2771652936935425
Batch 5/64 loss: -0.2832079827785492
Batch 6/64 loss: -0.2831628918647766
Batch 7/64 loss: -0.2609664797782898
Batch 8/64 loss: -0.2713979482650757
Batch 9/64 loss: -0.2608608603477478
Batch 10/64 loss: -0.2717912197113037
Batch 11/64 loss: -0.26990920305252075
Batch 12/64 loss: -0.28190457820892334
Batch 13/64 loss: -0.2653862237930298
Batch 14/64 loss: -0.25771772861480713
Batch 15/64 loss: -0.2667858600616455
Batch 16/64 loss: -0.28558149933815
Batch 17/64 loss: -0.26620960235595703
Batch 18/64 loss: -0.2700392007827759
Batch 19/64 loss: -0.27848684787750244
Batch 20/64 loss: -0.27237290143966675
Batch 21/64 loss: -0.277931809425354
Batch 22/64 loss: -0.2857077121734619
Batch 23/64 loss: -0.2850436568260193
Batch 24/64 loss: -0.27338093519210815
Batch 25/64 loss: -0.2727307677268982
Batch 26/64 loss: -0.28015410900115967
Batch 27/64 loss: -0.27171939611434937
Batch 28/64 loss: -0.2696762681007385
Batch 29/64 loss: -0.27770891785621643
Batch 30/64 loss: -0.27872583270072937
Batch 31/64 loss: -0.2693418860435486
Batch 32/64 loss: -0.2518388628959656
Batch 33/64 loss: -0.25683146715164185
Batch 34/64 loss: -0.26399779319763184
Batch 35/64 loss: -0.2621762752532959
Batch 36/64 loss: -0.25860047340393066
Batch 37/64 loss: -0.2666008472442627
Batch 38/64 loss: -0.2647923231124878
Batch 39/64 loss: -0.26457440853118896
Batch 40/64 loss: -0.27661585807800293
Batch 41/64 loss: -0.2611636519432068
Batch 42/64 loss: -0.28214168548583984
Batch 43/64 loss: -0.2580341100692749
Batch 44/64 loss: -0.26837992668151855
Batch 45/64 loss: -0.26564621925354004
Batch 46/64 loss: -0.2567998766899109
Batch 47/64 loss: -0.27671748399734497
Batch 48/64 loss: -0.26304441690444946
Batch 49/64 loss: -0.2735735774040222
Batch 50/64 loss: -0.26903200149536133
Batch 51/64 loss: -0.2663431763648987
Batch 52/64 loss: -0.26832544803619385
Batch 53/64 loss: -0.28085505962371826
Batch 54/64 loss: -0.28262102603912354
Batch 55/64 loss: -0.26124173402786255
Batch 56/64 loss: -0.27762937545776367
Batch 57/64 loss: -0.2796618342399597
Batch 58/64 loss: -0.2866189479827881
Batch 59/64 loss: -0.2728692591190338
Batch 60/64 loss: -0.27956706285476685
Batch 61/64 loss: -0.2798793315887451
Batch 62/64 loss: -0.28812211751937866
Batch 63/64 loss: -0.27542221546173096
Batch 64/64 loss: -0.2746884822845459
Epoch 241  Train loss: -0.27161499238481707  Val loss: -0.24845725994339513
Epoch 242
-------------------------------
Batch 1/64 loss: -0.2792133688926697
Batch 2/64 loss: -0.27807989716529846
Batch 3/64 loss: -0.26002728939056396
Batch 4/64 loss: -0.26966041326522827
Batch 5/64 loss: -0.27051806449890137
Batch 6/64 loss: -0.2694298028945923
Batch 7/64 loss: -0.28101274371147156
Batch 8/64 loss: -0.2693154811859131
Batch 9/64 loss: -0.26847779750823975
Batch 10/64 loss: -0.2706960439682007
Batch 11/64 loss: -0.2604717016220093
Batch 12/64 loss: -0.25266772508621216
Batch 13/64 loss: -0.29427412152290344
Batch 14/64 loss: -0.260221004486084
Batch 15/64 loss: -0.2857169508934021
Batch 16/64 loss: -0.2810510993003845
Batch 17/64 loss: -0.24733412265777588
Batch 18/64 loss: -0.28355810046195984
Batch 19/64 loss: -0.28149813413619995
Batch 20/64 loss: -0.2739788293838501
Batch 21/64 loss: -0.29360049962997437
Batch 22/64 loss: -0.28308814764022827
Batch 23/64 loss: -0.2893284261226654
Batch 24/64 loss: -0.28890275955200195
Batch 25/64 loss: -0.2740955948829651
Batch 26/64 loss: -0.290753573179245
Batch 27/64 loss: -0.27610519528388977
Batch 28/64 loss: -0.2644447088241577
Batch 29/64 loss: -0.2729264199733734
Batch 30/64 loss: -0.28384101390838623
Batch 31/64 loss: -0.2780037522315979
Batch 32/64 loss: -0.27643105387687683
Batch 33/64 loss: -0.26684582233428955
Batch 34/64 loss: -0.27892547845840454
Batch 35/64 loss: -0.27670401334762573
Batch 36/64 loss: -0.2794584035873413
Batch 37/64 loss: -0.2746812105178833
Batch 38/64 loss: -0.2575344443321228
Batch 39/64 loss: -0.28767627477645874
Batch 40/64 loss: -0.27068519592285156
Batch 41/64 loss: -0.26434755325317383
Batch 42/64 loss: -0.27195000648498535
Batch 43/64 loss: -0.2698671221733093
Batch 44/64 loss: -0.27799177169799805
Batch 45/64 loss: -0.27634939551353455
Batch 46/64 loss: -0.280935138463974
Batch 47/64 loss: -0.2779865860939026
Batch 48/64 loss: -0.2773544192314148
Batch 49/64 loss: -0.2777024209499359
Batch 50/64 loss: -0.27999091148376465
Batch 51/64 loss: -0.28462502360343933
Batch 52/64 loss: -0.287882536649704
Batch 53/64 loss: -0.26172196865081787
Batch 54/64 loss: -0.29000893235206604
Batch 55/64 loss: -0.2886632978916168
Batch 56/64 loss: -0.2916942834854126
Batch 57/64 loss: -0.27746066451072693
Batch 58/64 loss: -0.2741565704345703
Batch 59/64 loss: -0.25923842191696167
Batch 60/64 loss: -0.28721022605895996
Batch 61/64 loss: -0.27469611167907715
Batch 62/64 loss: -0.2846255898475647
Batch 63/64 loss: -0.2776939272880554
Batch 64/64 loss: -0.27621594071388245
Epoch 242  Train loss: -0.27611842377513063  Val loss: -0.25551603852268756
Saving best model, epoch: 242
Epoch 243
-------------------------------
Batch 1/64 loss: -0.2983105480670929
Batch 2/64 loss: -0.27741873264312744
Batch 3/64 loss: -0.27507415413856506
Batch 4/64 loss: -0.28005290031433105
Batch 5/64 loss: -0.29381924867630005
Batch 6/64 loss: -0.2894895672798157
Batch 7/64 loss: -0.27712124586105347
Batch 8/64 loss: -0.2728601098060608
Batch 9/64 loss: -0.27778226137161255
Batch 10/64 loss: -0.2861913740634918
Batch 11/64 loss: -0.26853376626968384
Batch 12/64 loss: -0.2816302180290222
Batch 13/64 loss: -0.27994388341903687
Batch 14/64 loss: -0.26939857006073
Batch 15/64 loss: -0.27395159006118774
Batch 16/64 loss: -0.2777060270309448
Batch 17/64 loss: -0.2701684832572937
Batch 18/64 loss: -0.26155799627304077
Batch 19/64 loss: -0.27480965852737427
Batch 20/64 loss: -0.28051066398620605
Batch 21/64 loss: -0.2743843197822571
Batch 22/64 loss: -0.2940937876701355
Batch 23/64 loss: -0.29449012875556946
Batch 24/64 loss: -0.2635021209716797
Batch 25/64 loss: -0.2747330367565155
Batch 26/64 loss: -0.27548477053642273
Batch 27/64 loss: -0.2862691879272461
Batch 28/64 loss: -0.27535760402679443
Batch 29/64 loss: -0.2805623710155487
Batch 30/64 loss: -0.2703225016593933
Batch 31/64 loss: -0.27412182092666626
Batch 32/64 loss: -0.2794211506843567
Batch 33/64 loss: -0.2760345935821533
Batch 34/64 loss: -0.2826036512851715
Batch 35/64 loss: -0.2829475402832031
Batch 36/64 loss: -0.275368332862854
Batch 37/64 loss: -0.2719256579875946
Batch 38/64 loss: -0.2838830351829529
Batch 39/64 loss: -0.2746084928512573
Batch 40/64 loss: -0.2880866527557373
Batch 41/64 loss: -0.2755342125892639
Batch 42/64 loss: -0.27277472615242004
Batch 43/64 loss: -0.2752302289009094
Batch 44/64 loss: -0.26957231760025024
Batch 45/64 loss: -0.2702615261077881
Batch 46/64 loss: -0.2600001096725464
Batch 47/64 loss: -0.2921581268310547
Batch 48/64 loss: -0.2729850113391876
Batch 49/64 loss: -0.2811729907989502
Batch 50/64 loss: -0.28421103954315186
Batch 51/64 loss: -0.2795528471469879
Batch 52/64 loss: -0.2813343107700348
Batch 53/64 loss: -0.2814953923225403
Batch 54/64 loss: -0.27112656831741333
Batch 55/64 loss: -0.2698967456817627
Batch 56/64 loss: -0.28435057401657104
Batch 57/64 loss: -0.2700634002685547
Batch 58/64 loss: -0.27619534730911255
Batch 59/64 loss: -0.27710145711898804
Batch 60/64 loss: -0.25426214933395386
Batch 61/64 loss: -0.2946207821369171
Batch 62/64 loss: -0.2775205969810486
Batch 63/64 loss: -0.27863162755966187
Batch 64/64 loss: -0.2625737190246582
Epoch 243  Train loss: -0.277513947206385  Val loss: -0.23875566385046312
Epoch 244
-------------------------------
Batch 1/64 loss: -0.2787035405635834
Batch 2/64 loss: -0.2717529535293579
Batch 3/64 loss: -0.2908337116241455
Batch 4/64 loss: -0.2728224992752075
Batch 5/64 loss: -0.2848778963088989
Batch 6/64 loss: -0.283966988325119
Batch 7/64 loss: -0.2841867208480835
Batch 8/64 loss: -0.2715179920196533
Batch 9/64 loss: -0.25772935152053833
Batch 10/64 loss: -0.27534452080726624
Batch 11/64 loss: -0.26437079906463623
Batch 12/64 loss: -0.28735512495040894
Batch 13/64 loss: -0.27701428532600403
Batch 14/64 loss: -0.2765146791934967
Batch 15/64 loss: -0.2705106735229492
Batch 16/64 loss: -0.269989550113678
Batch 17/64 loss: -0.27941709756851196
Batch 18/64 loss: -0.2794072926044464
Batch 19/64 loss: -0.2687872052192688
Batch 20/64 loss: -0.2815036177635193
Batch 21/64 loss: -0.2660733461380005
Batch 22/64 loss: -0.26986587047576904
Batch 23/64 loss: -0.2768629193305969
Batch 24/64 loss: -0.28223884105682373
Batch 25/64 loss: -0.28515708446502686
Batch 26/64 loss: -0.2768429219722748
Batch 27/64 loss: -0.27218127250671387
Batch 28/64 loss: -0.28294456005096436
Batch 29/64 loss: -0.27784043550491333
Batch 30/64 loss: -0.27573421597480774
Batch 31/64 loss: -0.26866650581359863
Batch 32/64 loss: -0.2747461199760437
Batch 33/64 loss: -0.2712022066116333
Batch 34/64 loss: -0.28112781047821045
Batch 35/64 loss: -0.2731616497039795
Batch 36/64 loss: -0.2754918932914734
Batch 37/64 loss: -0.2765977382659912
Batch 38/64 loss: -0.2824748754501343
Batch 39/64 loss: -0.27057111263275146
Batch 40/64 loss: -0.26928943395614624
Batch 41/64 loss: -0.2729101777076721
Batch 42/64 loss: -0.28249555826187134
Batch 43/64 loss: -0.27869123220443726
Batch 44/64 loss: -0.2699131369590759
Batch 45/64 loss: -0.2649492621421814
Batch 46/64 loss: -0.2877625823020935
Batch 47/64 loss: -0.27236276865005493
Batch 48/64 loss: -0.27288269996643066
Batch 49/64 loss: -0.2867904603481293
Batch 50/64 loss: -0.25847798585891724
Batch 51/64 loss: -0.2730318307876587
Batch 52/64 loss: -0.28075677156448364
Batch 53/64 loss: -0.2744811773300171
Batch 54/64 loss: -0.2638632655143738
Batch 55/64 loss: -0.2797483801841736
Batch 56/64 loss: -0.27227771282196045
Batch 57/64 loss: -0.26923108100891113
Batch 58/64 loss: -0.28021106123924255
Batch 59/64 loss: -0.27690666913986206
Batch 60/64 loss: -0.28492265939712524
Batch 61/64 loss: -0.2862465977668762
Batch 62/64 loss: -0.2757022976875305
Batch 63/64 loss: -0.2731170356273651
Batch 64/64 loss: -0.2725881338119507
Epoch 244  Train loss: -0.27573099323347505  Val loss: -0.22905848444122628
Epoch 245
-------------------------------
Batch 1/64 loss: -0.2821274399757385
Batch 2/64 loss: -0.2852106988430023
Batch 3/64 loss: -0.2738441824913025
Batch 4/64 loss: -0.2645152807235718
Batch 5/64 loss: -0.283875972032547
Batch 6/64 loss: -0.263810932636261
Batch 7/64 loss: -0.2622642517089844
Batch 8/64 loss: -0.26970943808555603
Batch 9/64 loss: -0.26405835151672363
Batch 10/64 loss: -0.26958972215652466
Batch 11/64 loss: -0.27358898520469666
Batch 12/64 loss: -0.2751089632511139
Batch 13/64 loss: -0.26629751920700073
Batch 14/64 loss: -0.27678894996643066
Batch 15/64 loss: -0.28003421425819397
Batch 16/64 loss: -0.281743586063385
Batch 17/64 loss: -0.25413984060287476
Batch 18/64 loss: -0.289709210395813
Batch 19/64 loss: -0.2885633111000061
Batch 20/64 loss: -0.27343952655792236
Batch 21/64 loss: -0.2563037872314453
Batch 22/64 loss: -0.26938551664352417
Batch 23/64 loss: -0.28824400901794434
Batch 24/64 loss: -0.2607787251472473
Batch 25/64 loss: -0.2485419511795044
Batch 26/64 loss: -0.27452993392944336
Batch 27/64 loss: -0.2618376612663269
Batch 28/64 loss: -0.28678080439567566
Batch 29/64 loss: -0.29218316078186035
Batch 30/64 loss: -0.2802644670009613
Batch 31/64 loss: -0.2739257216453552
Batch 32/64 loss: -0.2797136902809143
Batch 33/64 loss: -0.2553388476371765
Batch 34/64 loss: -0.2723572850227356
Batch 35/64 loss: -0.2796834111213684
Batch 36/64 loss: -0.26823127269744873
Batch 37/64 loss: -0.277739942073822
Batch 38/64 loss: -0.2882274389266968
Batch 39/64 loss: -0.28608375787734985
Batch 40/64 loss: -0.27632051706314087
Batch 41/64 loss: -0.28376254439353943
Batch 42/64 loss: -0.258425235748291
Batch 43/64 loss: -0.2717251777648926
Batch 44/64 loss: -0.2736815810203552
Batch 45/64 loss: -0.27257680892944336
Batch 46/64 loss: -0.27819252014160156
Batch 47/64 loss: -0.2856553792953491
Batch 48/64 loss: -0.27362188696861267
Batch 49/64 loss: -0.26450514793395996
Batch 50/64 loss: -0.2770301103591919
Batch 51/64 loss: -0.2784712612628937
Batch 52/64 loss: -0.2642461657524109
Batch 53/64 loss: -0.2587912678718567
Batch 54/64 loss: -0.27532050013542175
Batch 55/64 loss: -0.27991563081741333
Batch 56/64 loss: -0.26586997509002686
Batch 57/64 loss: -0.273873507976532
Batch 58/64 loss: -0.2728007733821869
Batch 59/64 loss: -0.2755628824234009
Batch 60/64 loss: -0.27508825063705444
Batch 61/64 loss: -0.283327579498291
Batch 62/64 loss: -0.2779178023338318
Batch 63/64 loss: -0.2802067995071411
Batch 64/64 loss: -0.28066277503967285
Epoch 245  Train loss: -0.27397581409005556  Val loss: -0.24468175959341304
Epoch 246
-------------------------------
Batch 1/64 loss: -0.25908273458480835
Batch 2/64 loss: -0.27491581439971924
Batch 3/64 loss: -0.27077382802963257
Batch 4/64 loss: -0.27658799290657043
Batch 5/64 loss: -0.2853749990463257
Batch 6/64 loss: -0.2669384479522705
Batch 7/64 loss: -0.26200342178344727
Batch 8/64 loss: -0.26586127281188965
Batch 9/64 loss: -0.28646671772003174
Batch 10/64 loss: -0.28325575590133667
Batch 11/64 loss: -0.2923574447631836
Batch 12/64 loss: -0.28311845660209656
Batch 13/64 loss: -0.2845923900604248
Batch 14/64 loss: -0.2800465226173401
Batch 15/64 loss: -0.27894333004951477
Batch 16/64 loss: -0.27050185203552246
Batch 17/64 loss: -0.2719014286994934
Batch 18/64 loss: -0.2774215042591095
Batch 19/64 loss: -0.2833041846752167
Batch 20/64 loss: -0.2821832299232483
Batch 21/64 loss: -0.26952242851257324
Batch 22/64 loss: -0.27205508947372437
Batch 23/64 loss: -0.25612807273864746
Batch 24/64 loss: -0.2734483480453491
Batch 25/64 loss: -0.2837825417518616
Batch 26/64 loss: -0.28659629821777344
Batch 27/64 loss: -0.2678767442703247
Batch 28/64 loss: -0.2866464853286743
Batch 29/64 loss: -0.2839711308479309
Batch 30/64 loss: -0.2618083953857422
Batch 31/64 loss: -0.2716900110244751
Batch 32/64 loss: -0.2853270173072815
Batch 33/64 loss: -0.2728114724159241
Batch 34/64 loss: -0.27483654022216797
Batch 35/64 loss: -0.2709342837333679
Batch 36/64 loss: -0.268796443939209
Batch 37/64 loss: -0.2780221104621887
Batch 38/64 loss: -0.2861964702606201
Batch 39/64 loss: -0.28117915987968445
Batch 40/64 loss: -0.2674019932746887
Batch 41/64 loss: -0.27410924434661865
Batch 42/64 loss: -0.2863510251045227
Batch 43/64 loss: -0.2880804240703583
Batch 44/64 loss: -0.28009724617004395
Batch 45/64 loss: -0.2527202367782593
Batch 46/64 loss: -0.27937182784080505
Batch 47/64 loss: -0.2628050446510315
Batch 48/64 loss: -0.2854900360107422
Batch 49/64 loss: -0.2858804166316986
Batch 50/64 loss: -0.28592154383659363
Batch 51/64 loss: -0.29007142782211304
Batch 52/64 loss: -0.2861446738243103
Batch 53/64 loss: -0.26261472702026367
Batch 54/64 loss: -0.26494187116622925
Batch 55/64 loss: -0.27873939275741577
Batch 56/64 loss: -0.27734842896461487
Batch 57/64 loss: -0.2454271912574768
Batch 58/64 loss: -0.28497910499572754
Batch 59/64 loss: -0.2764703631401062
Batch 60/64 loss: -0.28832170367240906
Batch 61/64 loss: -0.2733387351036072
Batch 62/64 loss: -0.2778335213661194
Batch 63/64 loss: -0.2558748722076416
Batch 64/64 loss: -0.28736644983291626
Epoch 246  Train loss: -0.2759705139141457  Val loss: -0.24686762885129737
Epoch 247
-------------------------------
Batch 1/64 loss: -0.28187525272369385
Batch 2/64 loss: -0.27897700667381287
Batch 3/64 loss: -0.2796645760536194
Batch 4/64 loss: -0.2762463688850403
Batch 5/64 loss: -0.27304530143737793
Batch 6/64 loss: -0.26166725158691406
Batch 7/64 loss: -0.26551109552383423
Batch 8/64 loss: -0.2718693017959595
Batch 9/64 loss: -0.27311384677886963
Batch 10/64 loss: -0.26054924726486206
Batch 11/64 loss: -0.28885412216186523
Batch 12/64 loss: -0.2798933982849121
Batch 13/64 loss: -0.26020169258117676
Batch 14/64 loss: -0.26741981506347656
Batch 15/64 loss: -0.2724635601043701
Batch 16/64 loss: -0.27587759494781494
Batch 17/64 loss: -0.2713102102279663
Batch 18/64 loss: -0.26224929094314575
Batch 19/64 loss: -0.2638664245605469
Batch 20/64 loss: -0.2688100337982178
Batch 21/64 loss: -0.2820035219192505
Batch 22/64 loss: -0.27331095933914185
Batch 23/64 loss: -0.26567959785461426
Batch 24/64 loss: -0.27106374502182007
Batch 25/64 loss: -0.2744154930114746
Batch 26/64 loss: -0.28389501571655273
Batch 27/64 loss: -0.285444974899292
Batch 28/64 loss: -0.28853586316108704
Batch 29/64 loss: -0.2845730781555176
Batch 30/64 loss: -0.2759912610054016
Batch 31/64 loss: -0.2853300869464874
Batch 32/64 loss: -0.27578258514404297
Batch 33/64 loss: -0.28712067008018494
Batch 34/64 loss: -0.27168309688568115
Batch 35/64 loss: -0.2689482569694519
Batch 36/64 loss: -0.23512494564056396
Batch 37/64 loss: -0.27301883697509766
Batch 38/64 loss: -0.27575868368148804
Batch 39/64 loss: -0.2884279191493988
Batch 40/64 loss: -0.2793692350387573
Batch 41/64 loss: -0.28487810492515564
Batch 42/64 loss: -0.28717076778411865
Batch 43/64 loss: -0.2711406946182251
Batch 44/64 loss: -0.2521982192993164
Batch 45/64 loss: -0.2990087866783142
Batch 46/64 loss: -0.2765352427959442
Batch 47/64 loss: -0.27634429931640625
Batch 48/64 loss: -0.26995187997817993
Batch 49/64 loss: -0.27570417523384094
Batch 50/64 loss: -0.2819955348968506
Batch 51/64 loss: -0.27718228101730347
Batch 52/64 loss: -0.26238012313842773
Batch 53/64 loss: -0.2705000638961792
Batch 54/64 loss: -0.2785290777683258
Batch 55/64 loss: -0.2723163366317749
Batch 56/64 loss: -0.2749108672142029
Batch 57/64 loss: -0.2717586159706116
Batch 58/64 loss: -0.27757370471954346
Batch 59/64 loss: -0.277976393699646
Batch 60/64 loss: -0.2767709493637085
Batch 61/64 loss: -0.2697363495826721
Batch 62/64 loss: -0.2828209400177002
Batch 63/64 loss: -0.2632564306259155
Batch 64/64 loss: -0.2647486925125122
Epoch 247  Train loss: -0.2743238364948946  Val loss: -0.24908918388111076
Epoch 248
-------------------------------
Batch 1/64 loss: -0.2691901922225952
Batch 2/64 loss: -0.28138434886932373
Batch 3/64 loss: -0.2752326726913452
Batch 4/64 loss: -0.2870256304740906
Batch 5/64 loss: -0.27622634172439575
Batch 6/64 loss: -0.27468788623809814
Batch 7/64 loss: -0.28378432989120483
Batch 8/64 loss: -0.2877468466758728
Batch 9/64 loss: -0.27778562903404236
Batch 10/64 loss: -0.25768613815307617
Batch 11/64 loss: -0.2805142104625702
Batch 12/64 loss: -0.28714078664779663
Batch 13/64 loss: -0.28595757484436035
Batch 14/64 loss: -0.2864265441894531
Batch 15/64 loss: -0.28844887018203735
Batch 16/64 loss: -0.27864396572113037
Batch 17/64 loss: -0.28828567266464233
Batch 18/64 loss: -0.268136203289032
Batch 19/64 loss: -0.2768231928348541
Batch 20/64 loss: -0.2719065546989441
Batch 21/64 loss: -0.2585848569869995
Batch 22/64 loss: -0.28388261795043945
Batch 23/64 loss: -0.2831442356109619
Batch 24/64 loss: -0.2508726119995117
Batch 25/64 loss: -0.2685505747795105
Batch 26/64 loss: -0.2742440104484558
Batch 27/64 loss: -0.27286624908447266
Batch 28/64 loss: -0.27973371744155884
Batch 29/64 loss: -0.26152437925338745
Batch 30/64 loss: -0.28849172592163086
Batch 31/64 loss: -0.2821058928966522
Batch 32/64 loss: -0.28620219230651855
Batch 33/64 loss: -0.27171748876571655
Batch 34/64 loss: -0.26842111349105835
Batch 35/64 loss: -0.2791885733604431
Batch 36/64 loss: -0.280413955450058
Batch 37/64 loss: -0.27725911140441895
Batch 38/64 loss: -0.28708159923553467
Batch 39/64 loss: -0.29350680112838745
Batch 40/64 loss: -0.2982326149940491
Batch 41/64 loss: -0.28313902020454407
Batch 42/64 loss: -0.2764238715171814
Batch 43/64 loss: -0.2824375629425049
Batch 44/64 loss: -0.27046632766723633
Batch 45/64 loss: -0.2720780372619629
Batch 46/64 loss: -0.2625858187675476
Batch 47/64 loss: -0.294525682926178
Batch 48/64 loss: -0.2888481616973877
Batch 49/64 loss: -0.28127872943878174
Batch 50/64 loss: -0.27312928438186646
Batch 51/64 loss: -0.27919965982437134
Batch 52/64 loss: -0.26560544967651367
Batch 53/64 loss: -0.27818429470062256
Batch 54/64 loss: -0.2769918739795685
Batch 55/64 loss: -0.28236305713653564
Batch 56/64 loss: -0.27128392457962036
Batch 57/64 loss: -0.2707030177116394
Batch 58/64 loss: -0.2816387414932251
Batch 59/64 loss: -0.2797608971595764
Batch 60/64 loss: -0.27914780378341675
Batch 61/64 loss: -0.27002525329589844
Batch 62/64 loss: -0.284646213054657
Batch 63/64 loss: -0.27408909797668457
Batch 64/64 loss: -0.2767248749732971
Epoch 248  Train loss: -0.2778847584537431  Val loss: -0.2552499702501133
Epoch 249
-------------------------------
Batch 1/64 loss: -0.2726360559463501
Batch 2/64 loss: -0.2665194272994995
Batch 3/64 loss: -0.27643245458602905
Batch 4/64 loss: -0.2504192590713501
Batch 5/64 loss: -0.2848602533340454
Batch 6/64 loss: -0.285359650850296
Batch 7/64 loss: -0.2849714457988739
Batch 8/64 loss: -0.2809629440307617
Batch 9/64 loss: -0.26099324226379395
Batch 10/64 loss: -0.2744668126106262
Batch 11/64 loss: -0.2831268012523651
Batch 12/64 loss: -0.2657819986343384
Batch 13/64 loss: -0.26714128255844116
Batch 14/64 loss: -0.27268993854522705
Batch 15/64 loss: -0.24185752868652344
Batch 16/64 loss: -0.2770223617553711
Batch 17/64 loss: -0.27449092268943787
Batch 18/64 loss: -0.2669483423233032
Batch 19/64 loss: -0.28610989451408386
Batch 20/64 loss: -0.28371137380599976
Batch 21/64 loss: -0.26594698429107666
Batch 22/64 loss: -0.2806810736656189
Batch 23/64 loss: -0.28354963660240173
Batch 24/64 loss: -0.25303226709365845
Batch 25/64 loss: -0.26884186267852783
Batch 26/64 loss: -0.28773874044418335
Batch 27/64 loss: -0.27006155252456665
Batch 28/64 loss: -0.2607406973838806
Batch 29/64 loss: -0.2792365849018097
Batch 30/64 loss: -0.2634570598602295
Batch 31/64 loss: -0.26782095432281494
Batch 32/64 loss: -0.27132242918014526
Batch 33/64 loss: -0.2716599106788635
Batch 34/64 loss: -0.2724868059158325
Batch 35/64 loss: -0.2872581481933594
Batch 36/64 loss: -0.27055907249450684
Batch 37/64 loss: -0.26069384813308716
Batch 38/64 loss: -0.25367218255996704
Batch 39/64 loss: -0.2639068365097046
Batch 40/64 loss: -0.2739782929420471
Batch 41/64 loss: -0.27960872650146484
Batch 42/64 loss: -0.2675579786300659
Batch 43/64 loss: -0.254733681678772
Batch 44/64 loss: -0.26916491985321045
Batch 45/64 loss: -0.2661229372024536
Batch 46/64 loss: -0.27848467230796814
Batch 47/64 loss: -0.2677430510520935
Batch 48/64 loss: -0.27327316999435425
Batch 49/64 loss: -0.28885042667388916
Batch 50/64 loss: -0.27575427293777466
Batch 51/64 loss: -0.2750822901725769
Batch 52/64 loss: -0.27569669485092163
Batch 53/64 loss: -0.27522850036621094
Batch 54/64 loss: -0.27336668968200684
Batch 55/64 loss: -0.2748885750770569
Batch 56/64 loss: -0.2608742117881775
Batch 57/64 loss: -0.27889999747276306
Batch 58/64 loss: -0.271531343460083
Batch 59/64 loss: -0.2793779969215393
Batch 60/64 loss: -0.2786073684692383
Batch 61/64 loss: -0.2752249240875244
Batch 62/64 loss: -0.28472900390625
Batch 63/64 loss: -0.271714448928833
Batch 64/64 loss: -0.27417659759521484
Epoch 249  Train loss: -0.27239678841011195  Val loss: -0.2511717273607287
Epoch 250
-------------------------------
Batch 1/64 loss: -0.26470857858657837
Batch 2/64 loss: -0.27767765522003174
Batch 3/64 loss: -0.286623477935791
Batch 4/64 loss: -0.2728295922279358
Batch 5/64 loss: -0.27339041233062744
Batch 6/64 loss: -0.27628010511398315
Batch 7/64 loss: -0.2818155884742737
Batch 8/64 loss: -0.27113616466522217
Batch 9/64 loss: -0.2763217091560364
Batch 10/64 loss: -0.2667502164840698
Batch 11/64 loss: -0.2717140316963196
Batch 12/64 loss: -0.27444589138031006
Batch 13/64 loss: -0.28620660305023193
Batch 14/64 loss: -0.28169679641723633
Batch 15/64 loss: -0.2687259912490845
Batch 16/64 loss: -0.28751933574676514
Batch 17/64 loss: -0.2795290946960449
Batch 18/64 loss: -0.26916396617889404
Batch 19/64 loss: -0.2676358222961426
Batch 20/64 loss: -0.2559429407119751
Batch 21/64 loss: -0.2743556499481201
Batch 22/64 loss: -0.2840406596660614
Batch 23/64 loss: -0.27314794063568115
Batch 24/64 loss: -0.2800966501235962
Batch 25/64 loss: -0.26822781562805176
Batch 26/64 loss: -0.2688245177268982
Batch 27/64 loss: -0.2897154688835144
Batch 28/64 loss: -0.2728508710861206
Batch 29/64 loss: -0.2800162136554718
Batch 30/64 loss: -0.27107739448547363
Batch 31/64 loss: -0.2718003988265991
Batch 32/64 loss: -0.26697778701782227
Batch 33/64 loss: -0.2745087742805481
Batch 34/64 loss: -0.2673795819282532
Batch 35/64 loss: -0.27050918340682983
Batch 36/64 loss: -0.2690926790237427
Batch 37/64 loss: -0.2829037606716156
Batch 38/64 loss: -0.26403963565826416
Batch 39/64 loss: -0.28012996912002563
Batch 40/64 loss: -0.2836523652076721
Batch 41/64 loss: -0.27418023347854614
Batch 42/64 loss: -0.28246960043907166
Batch 43/64 loss: -0.2619183659553528
Batch 44/64 loss: -0.2728844881057739
Batch 45/64 loss: -0.27942851185798645
Batch 46/64 loss: -0.27284693717956543
Batch 47/64 loss: -0.27735891938209534
Batch 48/64 loss: -0.26841193437576294
Batch 49/64 loss: -0.28593382239341736
Batch 50/64 loss: -0.2746003270149231
Batch 51/64 loss: -0.26797258853912354
Batch 52/64 loss: -0.2824486494064331
Batch 53/64 loss: -0.2777518630027771
Batch 54/64 loss: -0.2829701006412506
Batch 55/64 loss: -0.2730225920677185
Batch 56/64 loss: -0.26904696226119995
Batch 57/64 loss: -0.262820839881897
Batch 58/64 loss: -0.2758826017379761
Batch 59/64 loss: -0.2707502245903015
Batch 60/64 loss: -0.2650207281112671
Batch 61/64 loss: -0.2602083683013916
Batch 62/64 loss: -0.27122026681900024
Batch 63/64 loss: -0.2771371006965637
Batch 64/64 loss: -0.27078068256378174
Epoch 250  Train loss: -0.27408365221584546  Val loss: -0.24678448467320183
Epoch 251
-------------------------------
Batch 1/64 loss: -0.27311813831329346
Batch 2/64 loss: -0.2832087576389313
Batch 3/64 loss: -0.2883962392807007
Batch 4/64 loss: -0.26971685886383057
Batch 5/64 loss: -0.2673090696334839
Batch 6/64 loss: -0.25952595472335815
Batch 7/64 loss: -0.2809789776802063
Batch 8/64 loss: -0.2615654468536377
Batch 9/64 loss: -0.2753187417984009
Batch 10/64 loss: -0.28325724601745605
Batch 11/64 loss: -0.2626931071281433
Batch 12/64 loss: -0.27708131074905396
Batch 13/64 loss: -0.28011444211006165
Batch 14/64 loss: -0.26981961727142334
Batch 15/64 loss: -0.2851577401161194
Batch 16/64 loss: -0.2616257071495056
Batch 17/64 loss: -0.2743918299674988
Batch 18/64 loss: -0.2735651135444641
Batch 19/64 loss: -0.2775155007839203
Batch 20/64 loss: -0.2770031988620758
Batch 21/64 loss: -0.2717260718345642
Batch 22/64 loss: -0.28099918365478516
Batch 23/64 loss: -0.2706088423728943
Batch 24/64 loss: -0.2648268938064575
Batch 25/64 loss: -0.2776349186897278
Batch 26/64 loss: -0.27576616406440735
Batch 27/64 loss: -0.27462238073349
Batch 28/64 loss: -0.26218533515930176
Batch 29/64 loss: -0.26829224824905396
Batch 30/64 loss: -0.27465683221817017
Batch 31/64 loss: -0.26997315883636475
Batch 32/64 loss: -0.26664721965789795
Batch 33/64 loss: -0.28069961071014404
Batch 34/64 loss: -0.25615209341049194
Batch 35/64 loss: -0.2759542167186737
Batch 36/64 loss: -0.2823750376701355
Batch 37/64 loss: -0.28211545944213867
Batch 38/64 loss: -0.28887739777565
Batch 39/64 loss: -0.2749291658401489
Batch 40/64 loss: -0.28411078453063965
Batch 41/64 loss: -0.2874119281768799
Batch 42/64 loss: -0.2768995761871338
Batch 43/64 loss: -0.2761789560317993
Batch 44/64 loss: -0.2681567072868347
Batch 45/64 loss: -0.2811206579208374
Batch 46/64 loss: -0.2721712589263916
Batch 47/64 loss: -0.26939576864242554
Batch 48/64 loss: -0.2680593729019165
Batch 49/64 loss: -0.27881938219070435
Batch 50/64 loss: -0.2908828556537628
Batch 51/64 loss: -0.28048795461654663
Batch 52/64 loss: -0.2809762954711914
Batch 53/64 loss: -0.2660747170448303
Batch 54/64 loss: -0.28284725546836853
Batch 55/64 loss: -0.27490222454071045
Batch 56/64 loss: -0.2911752164363861
Batch 57/64 loss: -0.25805848836898804
Batch 58/64 loss: -0.25804585218429565
Batch 59/64 loss: -0.2650201916694641
Batch 60/64 loss: -0.2847256660461426
Batch 61/64 loss: -0.28754740953445435
Batch 62/64 loss: -0.27695024013519287
Batch 63/64 loss: -0.27914923429489136
Batch 64/64 loss: -0.2813985049724579
Epoch 251  Train loss: -0.2749901506246305  Val loss: -0.2584868014063622
Saving best model, epoch: 251
Epoch 252
-------------------------------
Batch 1/64 loss: -0.2673221230506897
Batch 2/64 loss: -0.2715265154838562
Batch 3/64 loss: -0.2866728901863098
Batch 4/64 loss: -0.2687087655067444
Batch 5/64 loss: -0.28246039152145386
Batch 6/64 loss: -0.2731546461582184
Batch 7/64 loss: -0.2792682647705078
Batch 8/64 loss: -0.27944546937942505
Batch 9/64 loss: -0.2758227586746216
Batch 10/64 loss: -0.26216578483581543
Batch 11/64 loss: -0.27790820598602295
Batch 12/64 loss: -0.2865847647190094
Batch 13/64 loss: -0.2747674286365509
Batch 14/64 loss: -0.2858504056930542
Batch 15/64 loss: -0.29120713472366333
Batch 16/64 loss: -0.2759884297847748
Batch 17/64 loss: -0.26629090309143066
Batch 18/64 loss: -0.2789633274078369
Batch 19/64 loss: -0.2723965048789978
Batch 20/64 loss: -0.27552711963653564
Batch 21/64 loss: -0.2789378762245178
Batch 22/64 loss: -0.26677119731903076
Batch 23/64 loss: -0.2581028342247009
Batch 24/64 loss: -0.2700841426849365
Batch 25/64 loss: -0.29440879821777344
Batch 26/64 loss: -0.26655733585357666
Batch 27/64 loss: -0.26787233352661133
Batch 28/64 loss: -0.26503974199295044
Batch 29/64 loss: -0.2636500597000122
Batch 30/64 loss: -0.2752333879470825
Batch 31/64 loss: -0.272652268409729
Batch 32/64 loss: -0.27540719509124756
Batch 33/64 loss: -0.26884764432907104
Batch 34/64 loss: -0.27304649353027344
Batch 35/64 loss: -0.28511714935302734
Batch 36/64 loss: -0.27689507603645325
Batch 37/64 loss: -0.278203547000885
Batch 38/64 loss: -0.28091099858283997
Batch 39/64 loss: -0.2701721787452698
Batch 40/64 loss: -0.27360808849334717
Batch 41/64 loss: -0.26555168628692627
Batch 42/64 loss: -0.2819933295249939
Batch 43/64 loss: -0.268649160861969
Batch 44/64 loss: -0.26775044202804565
Batch 45/64 loss: -0.26373523473739624
Batch 46/64 loss: -0.26258647441864014
Batch 47/64 loss: -0.27573999762535095
Batch 48/64 loss: -0.2531846761703491
Batch 49/64 loss: -0.2797242999076843
Batch 50/64 loss: -0.28224247694015503
Batch 51/64 loss: -0.26628541946411133
Batch 52/64 loss: -0.28728172183036804
Batch 53/64 loss: -0.26187217235565186
Batch 54/64 loss: -0.2736912965774536
Batch 55/64 loss: -0.2838830351829529
Batch 56/64 loss: -0.27379122376441956
Batch 57/64 loss: -0.27865707874298096
Batch 58/64 loss: -0.27268946170806885
Batch 59/64 loss: -0.28219038248062134
Batch 60/64 loss: -0.2837592661380768
Batch 61/64 loss: -0.27716580033302307
Batch 62/64 loss: -0.28646644949913025
Batch 63/64 loss: -0.2958958148956299
Batch 64/64 loss: -0.2627313733100891
Epoch 252  Train loss: -0.2747511468681635  Val loss: -0.2483094525091427
Epoch 253
-------------------------------
Batch 1/64 loss: -0.2867045998573303
Batch 2/64 loss: -0.2801070809364319
Batch 3/64 loss: -0.2727450728416443
Batch 4/64 loss: -0.27179646492004395
Batch 5/64 loss: -0.2734435200691223
Batch 6/64 loss: -0.28539252281188965
Batch 7/64 loss: -0.28191614151000977
Batch 8/64 loss: -0.28436118364334106
Batch 9/64 loss: -0.2684714198112488
Batch 10/64 loss: -0.2795833647251129
Batch 11/64 loss: -0.28401660919189453
Batch 12/64 loss: -0.2864912748336792
Batch 13/64 loss: -0.28427761793136597
Batch 14/64 loss: -0.25387895107269287
Batch 15/64 loss: -0.2893102467060089
Batch 16/64 loss: -0.26854342222213745
Batch 17/64 loss: -0.27206671237945557
Batch 18/64 loss: -0.27846282720565796
Batch 19/64 loss: -0.2739693820476532
Batch 20/64 loss: -0.28077176213264465
Batch 21/64 loss: -0.2706661820411682
Batch 22/64 loss: -0.27334219217300415
Batch 23/64 loss: -0.26135027408599854
Batch 24/64 loss: -0.26910728216171265
Batch 25/64 loss: -0.2898959517478943
Batch 26/64 loss: -0.28310948610305786
Batch 27/64 loss: -0.29345399141311646
Batch 28/64 loss: -0.284555047750473
Batch 29/64 loss: -0.28317394852638245
Batch 30/64 loss: -0.2827458381652832
Batch 31/64 loss: -0.2630026936531067
Batch 32/64 loss: -0.2698298692703247
Batch 33/64 loss: -0.2746553421020508
Batch 34/64 loss: -0.2857876420021057
Batch 35/64 loss: -0.2636597156524658
Batch 36/64 loss: -0.27590876817703247
Batch 37/64 loss: -0.27131742238998413
Batch 38/64 loss: -0.2785675823688507
Batch 39/64 loss: -0.25193560123443604
Batch 40/64 loss: -0.2718600034713745
Batch 41/64 loss: -0.27015984058380127
Batch 42/64 loss: -0.260095477104187
Batch 43/64 loss: -0.2826663851737976
Batch 44/64 loss: -0.2846773862838745
Batch 45/64 loss: -0.27159178256988525
Batch 46/64 loss: -0.2624722123146057
Batch 47/64 loss: -0.26828086376190186
Batch 48/64 loss: -0.28568682074546814
Batch 49/64 loss: -0.28688961267471313
Batch 50/64 loss: -0.27917760610580444
Batch 51/64 loss: -0.2691727876663208
Batch 52/64 loss: -0.2617366313934326
Batch 53/64 loss: -0.27816230058670044
Batch 54/64 loss: -0.27278047800064087
Batch 55/64 loss: -0.2638157606124878
Batch 56/64 loss: -0.2729993462562561
Batch 57/64 loss: -0.2704738974571228
Batch 58/64 loss: -0.27706795930862427
Batch 59/64 loss: -0.2672017812728882
Batch 60/64 loss: -0.2744670808315277
Batch 61/64 loss: -0.29005134105682373
Batch 62/64 loss: -0.2737940847873688
Batch 63/64 loss: -0.2865026593208313
Batch 64/64 loss: -0.2667984962463379
Epoch 253  Train loss: -0.2755177722257726  Val loss: -0.2452259770373708
Epoch 254
-------------------------------
Batch 1/64 loss: -0.2741349935531616
Batch 2/64 loss: -0.2722252309322357
Batch 3/64 loss: -0.2766357660293579
Batch 4/64 loss: -0.27271604537963867
Batch 5/64 loss: -0.2833256423473358
Batch 6/64 loss: -0.25424855947494507
Batch 7/64 loss: -0.2746913731098175
Batch 8/64 loss: -0.28366953134536743
Batch 9/64 loss: -0.2742895483970642
Batch 10/64 loss: -0.2774982750415802
Batch 11/64 loss: -0.25410473346710205
Batch 12/64 loss: -0.26634323596954346
Batch 13/64 loss: -0.28921693563461304
Batch 14/64 loss: -0.2705596089363098
Batch 15/64 loss: -0.28369957208633423
Batch 16/64 loss: -0.277990460395813
Batch 17/64 loss: -0.2793269455432892
Batch 18/64 loss: -0.2847520709037781
Batch 19/64 loss: -0.278747022151947
Batch 20/64 loss: -0.2795596122741699
Batch 21/64 loss: -0.2699524760246277
Batch 22/64 loss: -0.26953691244125366
Batch 23/64 loss: -0.2762898802757263
Batch 24/64 loss: -0.27768951654434204
Batch 25/64 loss: -0.2762112021446228
Batch 26/64 loss: -0.29100048542022705
Batch 27/64 loss: -0.27534162998199463
Batch 28/64 loss: -0.2686424255371094
Batch 29/64 loss: -0.2772313952445984
Batch 30/64 loss: -0.2650923728942871
Batch 31/64 loss: -0.2581714391708374
Batch 32/64 loss: -0.2656998038291931
Batch 33/64 loss: -0.2553892731666565
Batch 34/64 loss: -0.28186389803886414
Batch 35/64 loss: -0.28193360567092896
Batch 36/64 loss: -0.26603859663009644
Batch 37/64 loss: -0.2687368392944336
Batch 38/64 loss: -0.270757794380188
Batch 39/64 loss: -0.27183884382247925
Batch 40/64 loss: -0.26907408237457275
Batch 41/64 loss: -0.27119290828704834
Batch 42/64 loss: -0.2769707441329956
Batch 43/64 loss: -0.2572312355041504
Batch 44/64 loss: -0.26605260372161865
Batch 45/64 loss: -0.26768553256988525
Batch 46/64 loss: -0.2489100694656372
Batch 47/64 loss: -0.2792627513408661
Batch 48/64 loss: -0.2772436738014221
Batch 49/64 loss: -0.2807347774505615
Batch 50/64 loss: -0.2593473196029663
Batch 51/64 loss: -0.27522218227386475
Batch 52/64 loss: -0.27114593982696533
Batch 53/64 loss: -0.268208384513855
Batch 54/64 loss: -0.2617228627204895
Batch 55/64 loss: -0.2793566882610321
Batch 56/64 loss: -0.27374929189682007
Batch 57/64 loss: -0.2585991621017456
Batch 58/64 loss: -0.25888222455978394
Batch 59/64 loss: -0.2717043161392212
Batch 60/64 loss: -0.25270605087280273
Batch 61/64 loss: -0.2690443992614746
Batch 62/64 loss: -0.2634694576263428
Batch 63/64 loss: -0.27480822801589966
Batch 64/64 loss: -0.26691704988479614
Epoch 254  Train loss: -0.27149280356425864  Val loss: -0.24500647996299454
Epoch 255
-------------------------------
Batch 1/64 loss: -0.2737995386123657
Batch 2/64 loss: -0.275248646736145
Batch 3/64 loss: -0.26391100883483887
Batch 4/64 loss: -0.2818256616592407
Batch 5/64 loss: -0.2810613512992859
Batch 6/64 loss: -0.2840045690536499
Batch 7/64 loss: -0.2620011568069458
Batch 8/64 loss: -0.2845160961151123
Batch 9/64 loss: -0.2636229991912842
Batch 10/64 loss: -0.2758021652698517
Batch 11/64 loss: -0.27657657861709595
Batch 12/64 loss: -0.2652868628501892
Batch 13/64 loss: -0.2739371359348297
Batch 14/64 loss: -0.26623237133026123
Batch 15/64 loss: -0.2566337585449219
Batch 16/64 loss: -0.28275057673454285
Batch 17/64 loss: -0.27899035811424255
Batch 18/64 loss: -0.28461790084838867
Batch 19/64 loss: -0.27949392795562744
Batch 20/64 loss: -0.2592175006866455
Batch 21/64 loss: -0.2710731029510498
Batch 22/64 loss: -0.2703397274017334
Batch 23/64 loss: -0.2887571454048157
Batch 24/64 loss: -0.27624672651290894
Batch 25/64 loss: -0.2705279588699341
Batch 26/64 loss: -0.2727396488189697
Batch 27/64 loss: -0.2770734429359436
Batch 28/64 loss: -0.26201337575912476
Batch 29/64 loss: -0.283734530210495
Batch 30/64 loss: -0.2653959393501282
Batch 31/64 loss: -0.25998926162719727
Batch 32/64 loss: -0.2725643515586853
Batch 33/64 loss: -0.27459952235221863
Batch 34/64 loss: -0.27780377864837646
Batch 35/64 loss: -0.26971256732940674
Batch 36/64 loss: -0.2852109670639038
Batch 37/64 loss: -0.27817946672439575
Batch 38/64 loss: -0.26641517877578735
Batch 39/64 loss: -0.2690035104751587
Batch 40/64 loss: -0.27686333656311035
Batch 41/64 loss: -0.2619504928588867
Batch 42/64 loss: -0.28230851888656616
Batch 43/64 loss: -0.2766367793083191
Batch 44/64 loss: -0.26201558113098145
Batch 45/64 loss: -0.26181960105895996
Batch 46/64 loss: -0.2814628481864929
Batch 47/64 loss: -0.28460848331451416
Batch 48/64 loss: -0.27268752455711365
Batch 49/64 loss: -0.2813563644886017
Batch 50/64 loss: -0.2726162075996399
Batch 51/64 loss: -0.2750990390777588
Batch 52/64 loss: -0.2654355764389038
Batch 53/64 loss: -0.2695070505142212
Batch 54/64 loss: -0.27008020877838135
Batch 55/64 loss: -0.28072330355644226
Batch 56/64 loss: -0.27090340852737427
Batch 57/64 loss: -0.2799907922744751
Batch 58/64 loss: -0.27309125661849976
Batch 59/64 loss: -0.2709590792655945
Batch 60/64 loss: -0.27714940905570984
Batch 61/64 loss: -0.25457221269607544
Batch 62/64 loss: -0.2599492073059082
Batch 63/64 loss: -0.278562068939209
Batch 64/64 loss: -0.2836669981479645
Epoch 255  Train loss: -0.27316092498162214  Val loss: -0.20710780161762565
Epoch 256
-------------------------------
Batch 1/64 loss: -0.2757227420806885
Batch 2/64 loss: -0.26380473375320435
Batch 3/64 loss: -0.28141021728515625
Batch 4/64 loss: -0.26522892713546753
Batch 5/64 loss: -0.26894432306289673
Batch 6/64 loss: -0.2591492533683777
Batch 7/64 loss: -0.2621638774871826
Batch 8/64 loss: -0.26766228675842285
Batch 9/64 loss: -0.278809130191803
Batch 10/64 loss: -0.27243778109550476
Batch 11/64 loss: -0.26662707328796387
Batch 12/64 loss: -0.2764093279838562
Batch 13/64 loss: -0.274356484413147
Batch 14/64 loss: -0.276404470205307
Batch 15/64 loss: -0.2670484185218811
Batch 16/64 loss: -0.27998805046081543
Batch 17/64 loss: -0.26675111055374146
Batch 18/64 loss: -0.27911776304244995
Batch 19/64 loss: -0.2731986939907074
Batch 20/64 loss: -0.280681848526001
Batch 21/64 loss: -0.26206207275390625
Batch 22/64 loss: -0.25522953271865845
Batch 23/64 loss: -0.26798903942108154
Batch 24/64 loss: -0.2691054940223694
Batch 25/64 loss: -0.2693523168563843
Batch 26/64 loss: -0.28462672233581543
Batch 27/64 loss: -0.27371084690093994
Batch 28/64 loss: -0.2698770761489868
Batch 29/64 loss: -0.24219995737075806
Batch 30/64 loss: -0.28090912103652954
Batch 31/64 loss: -0.28603672981262207
Batch 32/64 loss: -0.2649161219596863
Batch 33/64 loss: -0.2603342533111572
Batch 34/64 loss: -0.27231937646865845
Batch 35/64 loss: -0.2796022891998291
Batch 36/64 loss: -0.2937846779823303
Batch 37/64 loss: -0.27596578001976013
Batch 38/64 loss: -0.2718544006347656
Batch 39/64 loss: -0.27282801270484924
Batch 40/64 loss: -0.28190964460372925
Batch 41/64 loss: -0.2799360156059265
Batch 42/64 loss: -0.2916397452354431
Batch 43/64 loss: -0.2858673930168152
Batch 44/64 loss: -0.2677662968635559
Batch 45/64 loss: -0.2789028286933899
Batch 46/64 loss: -0.2744486927986145
Batch 47/64 loss: -0.28168806433677673
Batch 48/64 loss: -0.2858702540397644
Batch 49/64 loss: -0.2904081344604492
Batch 50/64 loss: -0.2861109972000122
Batch 51/64 loss: -0.27785569429397583
Batch 52/64 loss: -0.2832567095756531
Batch 53/64 loss: -0.2896144390106201
Batch 54/64 loss: -0.27781587839126587
Batch 55/64 loss: -0.28211578726768494
Batch 56/64 loss: -0.28511685132980347
Batch 57/64 loss: -0.27324584126472473
Batch 58/64 loss: -0.28827184438705444
Batch 59/64 loss: -0.27354565262794495
Batch 60/64 loss: -0.28786492347717285
Batch 61/64 loss: -0.27879709005355835
Batch 62/64 loss: -0.2948516607284546
Batch 63/64 loss: -0.27198898792266846
Batch 64/64 loss: -0.28803420066833496
Epoch 256  Train loss: -0.27566330058901917  Val loss: -0.23845122462695406
Epoch 257
-------------------------------
Batch 1/64 loss: -0.2814639210700989
Batch 2/64 loss: -0.26936596632003784
Batch 3/64 loss: -0.2784078121185303
Batch 4/64 loss: -0.25927168130874634
Batch 5/64 loss: -0.27993831038475037
Batch 6/64 loss: -0.27172383666038513
Batch 7/64 loss: -0.27918949723243713
Batch 8/64 loss: -0.2708325982093811
Batch 9/64 loss: -0.2845538258552551
Batch 10/64 loss: -0.2828046381473541
Batch 11/64 loss: -0.28840649127960205
Batch 12/64 loss: -0.2803342640399933
Batch 13/64 loss: -0.27670639753341675
Batch 14/64 loss: -0.2709696888923645
Batch 15/64 loss: -0.2738637328147888
Batch 16/64 loss: -0.2862733006477356
Batch 17/64 loss: -0.270487904548645
Batch 18/64 loss: -0.2775614857673645
Batch 19/64 loss: -0.279540479183197
Batch 20/64 loss: -0.28137364983558655
Batch 21/64 loss: -0.2754982113838196
Batch 22/64 loss: -0.277643084526062
Batch 23/64 loss: -0.27972957491874695
Batch 24/64 loss: -0.28181642293930054
Batch 25/64 loss: -0.2717254161834717
Batch 26/64 loss: -0.2696479558944702
Batch 27/64 loss: -0.2774643301963806
Batch 28/64 loss: -0.27125978469848633
Batch 29/64 loss: -0.2802009880542755
Batch 30/64 loss: -0.2883687913417816
Batch 31/64 loss: -0.27784472703933716
Batch 32/64 loss: -0.2864844799041748
Batch 33/64 loss: -0.277559369802475
Batch 34/64 loss: -0.2959367632865906
Batch 35/64 loss: -0.27765077352523804
Batch 36/64 loss: -0.23733818531036377
Batch 37/64 loss: -0.2775924801826477
Batch 38/64 loss: -0.26785391569137573
Batch 39/64 loss: -0.2832268178462982
Batch 40/64 loss: -0.28323879837989807
Batch 41/64 loss: -0.2799091041088104
Batch 42/64 loss: -0.2880289554595947
Batch 43/64 loss: -0.2791597843170166
Batch 44/64 loss: -0.2696424126625061
Batch 45/64 loss: -0.2821926176548004
Batch 46/64 loss: -0.2762235999107361
Batch 47/64 loss: -0.28557440638542175
Batch 48/64 loss: -0.26918119192123413
Batch 49/64 loss: -0.28678667545318604
Batch 50/64 loss: -0.2780420482158661
Batch 51/64 loss: -0.28097420930862427
Batch 52/64 loss: -0.28388160467147827
Batch 53/64 loss: -0.25190722942352295
Batch 54/64 loss: -0.2810221016407013
Batch 55/64 loss: -0.2780161201953888
Batch 56/64 loss: -0.27719128131866455
Batch 57/64 loss: -0.2663995027542114
Batch 58/64 loss: -0.27589577436447144
Batch 59/64 loss: -0.27653032541275024
Batch 60/64 loss: -0.2734321355819702
Batch 61/64 loss: -0.26227396726608276
Batch 62/64 loss: -0.2595735192298889
Batch 63/64 loss: -0.29003334045410156
Batch 64/64 loss: -0.28809213638305664
Epoch 257  Train loss: -0.27684849196789313  Val loss: -0.2490662829982456
Epoch 258
-------------------------------
Batch 1/64 loss: -0.2877720594406128
Batch 2/64 loss: -0.27129149436950684
Batch 3/64 loss: -0.2870387136936188
Batch 4/64 loss: -0.2749703824520111
Batch 5/64 loss: -0.2765392065048218
Batch 6/64 loss: -0.2705000638961792
Batch 7/64 loss: -0.27603358030319214
Batch 8/64 loss: -0.27157026529312134
Batch 9/64 loss: -0.27754345536231995
Batch 10/64 loss: -0.2737430930137634
Batch 11/64 loss: -0.28204843401908875
Batch 12/64 loss: -0.2749851644039154
Batch 13/64 loss: -0.27212828397750854
Batch 14/64 loss: -0.2801235020160675
Batch 15/64 loss: -0.25785017013549805
Batch 16/64 loss: -0.26059597730636597
Batch 17/64 loss: -0.2892036736011505
Batch 18/64 loss: -0.27790725231170654
Batch 19/64 loss: -0.2679479122161865
Batch 20/64 loss: -0.2824908494949341
Batch 21/64 loss: -0.26373904943466187
Batch 22/64 loss: -0.27924269437789917
Batch 23/64 loss: -0.2820851802825928
Batch 24/64 loss: -0.2782188057899475
Batch 25/64 loss: -0.27664417028427124
Batch 26/64 loss: -0.27358368039131165
Batch 27/64 loss: -0.2812504768371582
Batch 28/64 loss: -0.2798458933830261
Batch 29/64 loss: -0.2771368622779846
Batch 30/64 loss: -0.28343868255615234
Batch 31/64 loss: -0.2823042869567871
Batch 32/64 loss: -0.2610272169113159
Batch 33/64 loss: -0.2704482078552246
Batch 34/64 loss: -0.2699453830718994
Batch 35/64 loss: -0.28208303451538086
Batch 36/64 loss: -0.2699841260910034
Batch 37/64 loss: -0.27436962723731995
Batch 38/64 loss: -0.29180222749710083
Batch 39/64 loss: -0.27408912777900696
Batch 40/64 loss: -0.26555609703063965
Batch 41/64 loss: -0.27341604232788086
Batch 42/64 loss: -0.26972323656082153
Batch 43/64 loss: -0.2842617630958557
Batch 44/64 loss: -0.2799294888973236
Batch 45/64 loss: -0.2626230716705322
Batch 46/64 loss: -0.27339082956314087
Batch 47/64 loss: -0.2897053360939026
Batch 48/64 loss: -0.2699131965637207
Batch 49/64 loss: -0.2764076590538025
Batch 50/64 loss: -0.27039235830307007
Batch 51/64 loss: -0.280977725982666
Batch 52/64 loss: -0.2660318613052368
Batch 53/64 loss: -0.25953829288482666
Batch 54/64 loss: -0.2963702082633972
Batch 55/64 loss: -0.2756146788597107
Batch 56/64 loss: -0.2634540796279907
Batch 57/64 loss: -0.2785513997077942
Batch 58/64 loss: -0.26209938526153564
Batch 59/64 loss: -0.27421504259109497
Batch 60/64 loss: -0.26261866092681885
Batch 61/64 loss: -0.2829427719116211
Batch 62/64 loss: -0.2830727994441986
Batch 63/64 loss: -0.27233049273490906
Batch 64/64 loss: -0.2906513512134552
Epoch 258  Train loss: -0.2753670158339482  Val loss: -0.24570610146342275
Epoch 259
-------------------------------
Batch 1/64 loss: -0.2853706479072571
Batch 2/64 loss: -0.2697634696960449
Batch 3/64 loss: -0.27235233783721924
Batch 4/64 loss: -0.28246062994003296
Batch 5/64 loss: -0.286575585603714
Batch 6/64 loss: -0.28348034620285034
Batch 7/64 loss: -0.2840222716331482
Batch 8/64 loss: -0.2692340612411499
Batch 9/64 loss: -0.28401005268096924
Batch 10/64 loss: -0.285466730594635
Batch 11/64 loss: -0.28413695096969604
Batch 12/64 loss: -0.2772364020347595
Batch 13/64 loss: -0.2654259204864502
Batch 14/64 loss: -0.2681093215942383
Batch 15/64 loss: -0.2780444920063019
Batch 16/64 loss: -0.27060467004776
Batch 17/64 loss: -0.27358347177505493
Batch 18/64 loss: -0.2721538543701172
Batch 19/64 loss: -0.25815796852111816
Batch 20/64 loss: -0.2792554199695587
Batch 21/64 loss: -0.2711057662963867
Batch 22/64 loss: -0.26900023221969604
Batch 23/64 loss: -0.2643898129463196
Batch 24/64 loss: -0.2641545534133911
Batch 25/64 loss: -0.26886165142059326
Batch 26/64 loss: -0.26940667629241943
Batch 27/64 loss: -0.2722066342830658
Batch 28/64 loss: -0.27459657192230225
Batch 29/64 loss: -0.2882029414176941
Batch 30/64 loss: -0.27125084400177
Batch 31/64 loss: -0.269489049911499
Batch 32/64 loss: -0.26792359352111816
Batch 33/64 loss: -0.2676464915275574
Batch 34/64 loss: -0.2785007357597351
Batch 35/64 loss: -0.2819189131259918
Batch 36/64 loss: -0.2808578312397003
Batch 37/64 loss: -0.26658135652542114
Batch 38/64 loss: -0.27399468421936035
Batch 39/64 loss: -0.26929545402526855
Batch 40/64 loss: -0.2697969675064087
Batch 41/64 loss: -0.27291762828826904
Batch 42/64 loss: -0.2833334803581238
Batch 43/64 loss: -0.2905316948890686
Batch 44/64 loss: -0.2835286259651184
Batch 45/64 loss: -0.28138697147369385
Batch 46/64 loss: -0.27758148312568665
Batch 47/64 loss: -0.2647862434387207
Batch 48/64 loss: -0.2718890309333801
Batch 49/64 loss: -0.28273922204971313
Batch 50/64 loss: -0.27911174297332764
Batch 51/64 loss: -0.27767831087112427
Batch 52/64 loss: -0.28133153915405273
Batch 53/64 loss: -0.27313047647476196
Batch 54/64 loss: -0.2637684941291809
Batch 55/64 loss: -0.2812812924385071
Batch 56/64 loss: -0.27249622344970703
Batch 57/64 loss: -0.2723997235298157
Batch 58/64 loss: -0.2778595983982086
Batch 59/64 loss: -0.2851566672325134
Batch 60/64 loss: -0.26721805334091187
Batch 61/64 loss: -0.2809884548187256
Batch 62/64 loss: -0.2737129330635071
Batch 63/64 loss: -0.2841081917285919
Batch 64/64 loss: -0.294851154088974
Epoch 259  Train loss: -0.27558744806869356  Val loss: -0.25606561485434726
Epoch 260
-------------------------------
Batch 1/64 loss: -0.2858399748802185
Batch 2/64 loss: -0.28126686811447144
Batch 3/64 loss: -0.27041882276535034
Batch 4/64 loss: -0.27756810188293457
Batch 5/64 loss: -0.275320827960968
Batch 6/64 loss: -0.27490097284317017
Batch 7/64 loss: -0.28637248277664185
Batch 8/64 loss: -0.2880610227584839
Batch 9/64 loss: -0.2931400239467621
Batch 10/64 loss: -0.29181623458862305
Batch 11/64 loss: -0.2825379967689514
Batch 12/64 loss: -0.27408528327941895
Batch 13/64 loss: -0.2859717905521393
Batch 14/64 loss: -0.2761191129684448
Batch 15/64 loss: -0.270047664642334
Batch 16/64 loss: -0.2705761790275574
Batch 17/64 loss: -0.26996713876724243
Batch 18/64 loss: -0.26307517290115356
Batch 19/64 loss: -0.27457618713378906
Batch 20/64 loss: -0.27091336250305176
Batch 21/64 loss: -0.27840283513069153
Batch 22/64 loss: -0.281257688999176
Batch 23/64 loss: -0.2841808497905731
Batch 24/64 loss: -0.27683597803115845
Batch 25/64 loss: -0.27334338426589966
Batch 26/64 loss: -0.2686690092086792
Batch 27/64 loss: -0.2568812370300293
Batch 28/64 loss: -0.25575655698776245
Batch 29/64 loss: -0.2780110239982605
Batch 30/64 loss: -0.2730098366737366
Batch 31/64 loss: -0.2735902667045593
Batch 32/64 loss: -0.26812314987182617
Batch 33/64 loss: -0.28089258074760437
Batch 34/64 loss: -0.263874351978302
Batch 35/64 loss: -0.26709359884262085
Batch 36/64 loss: -0.2659241557121277
Batch 37/64 loss: -0.272849977016449
Batch 38/64 loss: -0.26445531845092773
Batch 39/64 loss: -0.27838417887687683
Batch 40/64 loss: -0.2680620551109314
Batch 41/64 loss: -0.284628689289093
Batch 42/64 loss: -0.27252256870269775
Batch 43/64 loss: -0.2742924690246582
Batch 44/64 loss: -0.2795109152793884
Batch 45/64 loss: -0.2786473035812378
Batch 46/64 loss: -0.2845674455165863
Batch 47/64 loss: -0.26149433851242065
Batch 48/64 loss: -0.26493674516677856
Batch 49/64 loss: -0.27718281745910645
Batch 50/64 loss: -0.27362537384033203
Batch 51/64 loss: -0.26265639066696167
Batch 52/64 loss: -0.26316797733306885
Batch 53/64 loss: -0.27769261598587036
Batch 54/64 loss: -0.2647881507873535
Batch 55/64 loss: -0.26604795455932617
Batch 56/64 loss: -0.26441800594329834
Batch 57/64 loss: -0.2643846273422241
Batch 58/64 loss: -0.27080368995666504
Batch 59/64 loss: -0.2772815525531769
Batch 60/64 loss: -0.2751908600330353
Batch 61/64 loss: -0.276899516582489
Batch 62/64 loss: -0.27536895871162415
Batch 63/64 loss: -0.27415037155151367
Batch 64/64 loss: -0.28122830390930176
Epoch 260  Train loss: -0.27399770699295345  Val loss: -0.24958298255487815
Epoch 261
-------------------------------
Batch 1/64 loss: -0.26945459842681885
Batch 2/64 loss: -0.2594410181045532
Batch 3/64 loss: -0.26826179027557373
Batch 4/64 loss: -0.2709522247314453
Batch 5/64 loss: -0.27395471930503845
Batch 6/64 loss: -0.2656128406524658
Batch 7/64 loss: -0.27488285303115845
Batch 8/64 loss: -0.2714807987213135
Batch 9/64 loss: -0.2604372501373291
Batch 10/64 loss: -0.2650858163833618
Batch 11/64 loss: -0.26503294706344604
Batch 12/64 loss: -0.2555375099182129
Batch 13/64 loss: -0.24768733978271484
Batch 14/64 loss: -0.2666059136390686
Batch 15/64 loss: -0.2537863850593567
Batch 16/64 loss: -0.2613222002983093
Batch 17/64 loss: -0.27160394191741943
Batch 18/64 loss: -0.25118011236190796
Batch 19/64 loss: -0.2507671117782593
Batch 20/64 loss: -0.24952149391174316
Batch 21/64 loss: -0.2660371661186218
Batch 22/64 loss: -0.26268696784973145
Batch 23/64 loss: -0.26296472549438477
Batch 24/64 loss: -0.2674483060836792
Batch 25/64 loss: -0.2662505507469177
Batch 26/64 loss: -0.2696972191333771
Batch 27/64 loss: -0.2713492214679718
Batch 28/64 loss: -0.2558706998825073
Batch 29/64 loss: -0.26917564868927
Batch 30/64 loss: -0.2609771490097046
Batch 31/64 loss: -0.265575647354126
Batch 32/64 loss: -0.27920883893966675
Batch 33/64 loss: -0.2689880132675171
Batch 34/64 loss: -0.25864923000335693
Batch 35/64 loss: -0.27404654026031494
Batch 36/64 loss: -0.2758067846298218
Batch 37/64 loss: -0.26996713876724243
Batch 38/64 loss: -0.2735363841056824
Batch 39/64 loss: -0.2840280532836914
Batch 40/64 loss: -0.2752423882484436
Batch 41/64 loss: -0.26824742555618286
Batch 42/64 loss: -0.2827790081501007
Batch 43/64 loss: -0.27146080136299133
Batch 44/64 loss: -0.2661662697792053
Batch 45/64 loss: -0.26734572649002075
Batch 46/64 loss: -0.2674207091331482
Batch 47/64 loss: -0.27883580327033997
Batch 48/64 loss: -0.25106310844421387
Batch 49/64 loss: -0.2781020402908325
Batch 50/64 loss: -0.2518262267112732
Batch 51/64 loss: -0.2702990174293518
Batch 52/64 loss: -0.26131588220596313
Batch 53/64 loss: -0.26996010541915894
Batch 54/64 loss: -0.26700854301452637
Batch 55/64 loss: -0.27387571334838867
Batch 56/64 loss: -0.26953452825546265
Batch 57/64 loss: -0.27108287811279297
Batch 58/64 loss: -0.2664005160331726
Batch 59/64 loss: -0.2673884630203247
Batch 60/64 loss: -0.2779591679573059
Batch 61/64 loss: -0.2756229043006897
Batch 62/64 loss: -0.2759759724140167
Batch 63/64 loss: -0.265865683555603
Batch 64/64 loss: -0.28148770332336426
Epoch 261  Train loss: -0.2672434166365979  Val loss: -0.24141559854815506
Epoch 262
-------------------------------
Batch 1/64 loss: -0.27178794145584106
Batch 2/64 loss: -0.27431583404541016
Batch 3/64 loss: -0.28579050302505493
Batch 4/64 loss: -0.2813096046447754
Batch 5/64 loss: -0.27304673194885254
Batch 6/64 loss: -0.27523133158683777
Batch 7/64 loss: -0.26632267236709595
Batch 8/64 loss: -0.27730509638786316
Batch 9/64 loss: -0.27380478382110596
Batch 10/64 loss: -0.2844443917274475
Batch 11/64 loss: -0.2876124978065491
Batch 12/64 loss: -0.2614847421646118
Batch 13/64 loss: -0.2838839292526245
Batch 14/64 loss: -0.2723239064216614
Batch 15/64 loss: -0.28871840238571167
Batch 16/64 loss: -0.27295953035354614
Batch 17/64 loss: -0.27092820405960083
Batch 18/64 loss: -0.28702566027641296
Batch 19/64 loss: -0.27452871203422546
Batch 20/64 loss: -0.27510446310043335
Batch 21/64 loss: -0.2882140278816223
Batch 22/64 loss: -0.26499056816101074
Batch 23/64 loss: -0.295345664024353
Batch 24/64 loss: -0.27418094873428345
Batch 25/64 loss: -0.27529647946357727
Batch 26/64 loss: -0.27074289321899414
Batch 27/64 loss: -0.275263249874115
Batch 28/64 loss: -0.2858063578605652
Batch 29/64 loss: -0.2678499221801758
Batch 30/64 loss: -0.2732148766517639
Batch 31/64 loss: -0.27669256925582886
Batch 32/64 loss: -0.2795519530773163
Batch 33/64 loss: -0.2798444628715515
Batch 34/64 loss: -0.280119925737381
Batch 35/64 loss: -0.2746286988258362
Batch 36/64 loss: -0.26834940910339355
Batch 37/64 loss: -0.2728625535964966
Batch 38/64 loss: -0.2605612277984619
Batch 39/64 loss: -0.2807670533657074
Batch 40/64 loss: -0.29207339882850647
Batch 41/64 loss: -0.26600778102874756
Batch 42/64 loss: -0.2763703465461731
Batch 43/64 loss: -0.2652782201766968
Batch 44/64 loss: -0.28258588910102844
Batch 45/64 loss: -0.285001277923584
Batch 46/64 loss: -0.27759498357772827
Batch 47/64 loss: -0.25842154026031494
Batch 48/64 loss: -0.27325206995010376
Batch 49/64 loss: -0.27852001786231995
Batch 50/64 loss: -0.27911168336868286
Batch 51/64 loss: -0.29055795073509216
Batch 52/64 loss: -0.2747628390789032
Batch 53/64 loss: -0.27699387073516846
Batch 54/64 loss: -0.28411155939102173
Batch 55/64 loss: -0.27456218004226685
Batch 56/64 loss: -0.2739926278591156
Batch 57/64 loss: -0.25629663467407227
Batch 58/64 loss: -0.2819192409515381
Batch 59/64 loss: -0.28960394859313965
Batch 60/64 loss: -0.2849932014942169
Batch 61/64 loss: -0.280911922454834
Batch 62/64 loss: -0.29767224192619324
Batch 63/64 loss: -0.2694525718688965
Batch 64/64 loss: -0.2885828912258148
Epoch 262  Train loss: -0.27715603054738513  Val loss: -0.2345129906926368
Epoch 263
-------------------------------
Batch 1/64 loss: -0.28664499521255493
Batch 2/64 loss: -0.2782498002052307
Batch 3/64 loss: -0.2669909596443176
Batch 4/64 loss: -0.27063071727752686
Batch 5/64 loss: -0.28996700048446655
Batch 6/64 loss: -0.29184606671333313
Batch 7/64 loss: -0.25890958309173584
Batch 8/64 loss: -0.2742937207221985
Batch 9/64 loss: -0.27494433522224426
Batch 10/64 loss: -0.26456236839294434
Batch 11/64 loss: -0.26791638135910034
Batch 12/64 loss: -0.2663058638572693
Batch 13/64 loss: -0.26258617639541626
Batch 14/64 loss: -0.2720256745815277
Batch 15/64 loss: -0.2775334119796753
Batch 16/64 loss: -0.2548428177833557
Batch 17/64 loss: -0.27388995885849
Batch 18/64 loss: -0.27543142437934875
Batch 19/64 loss: -0.2775014042854309
Batch 20/64 loss: -0.2697131633758545
Batch 21/64 loss: -0.27284714579582214
Batch 22/64 loss: -0.25226783752441406
Batch 23/64 loss: -0.26831185817718506
Batch 24/64 loss: -0.2792322337627411
Batch 25/64 loss: -0.2653668522834778
Batch 26/64 loss: -0.2716516852378845
Batch 27/64 loss: -0.26888108253479004
Batch 28/64 loss: -0.27719879150390625
Batch 29/64 loss: -0.2698041796684265
Batch 30/64 loss: -0.2811241149902344
Batch 31/64 loss: -0.25734567642211914
Batch 32/64 loss: -0.2756956219673157
Batch 33/64 loss: -0.2879413962364197
Batch 34/64 loss: -0.286408394575119
Batch 35/64 loss: -0.29152214527130127
Batch 36/64 loss: -0.2819948196411133
Batch 37/64 loss: -0.2730366885662079
Batch 38/64 loss: -0.26098620891571045
Batch 39/64 loss: -0.28466588258743286
Batch 40/64 loss: -0.27085861563682556
Batch 41/64 loss: -0.27073514461517334
Batch 42/64 loss: -0.2819405198097229
Batch 43/64 loss: -0.27209535241127014
Batch 44/64 loss: -0.2664416432380676
Batch 45/64 loss: -0.2904443144798279
Batch 46/64 loss: -0.2618812918663025
Batch 47/64 loss: -0.2611461281776428
Batch 48/64 loss: -0.2659134268760681
Batch 49/64 loss: -0.28481924533843994
Batch 50/64 loss: -0.28335288166999817
Batch 51/64 loss: -0.29216235876083374
Batch 52/64 loss: -0.27536773681640625
Batch 53/64 loss: -0.2812819480895996
Batch 54/64 loss: -0.2735908627510071
Batch 55/64 loss: -0.2718043923377991
Batch 56/64 loss: -0.28121787309646606
Batch 57/64 loss: -0.2652292251586914
Batch 58/64 loss: -0.28008854389190674
Batch 59/64 loss: -0.28713294863700867
Batch 60/64 loss: -0.2803570628166199
Batch 61/64 loss: -0.27935895323753357
Batch 62/64 loss: -0.26799172163009644
Batch 63/64 loss: -0.2795383632183075
Batch 64/64 loss: -0.2732505798339844
Epoch 263  Train loss: -0.2743648145713058  Val loss: -0.2518532128678155
Epoch 264
-------------------------------
Batch 1/64 loss: -0.27079248428344727
Batch 2/64 loss: -0.2847370207309723
Batch 3/64 loss: -0.28256863355636597
Batch 4/64 loss: -0.2838588356971741
Batch 5/64 loss: -0.2863208055496216
Batch 6/64 loss: -0.2843017876148224
Batch 7/64 loss: -0.27557459473609924
Batch 8/64 loss: -0.2833453416824341
Batch 9/64 loss: -0.28277674317359924
Batch 10/64 loss: -0.27481985092163086
Batch 11/64 loss: -0.27421432733535767
Batch 12/64 loss: -0.27841851115226746
Batch 13/64 loss: -0.27542823553085327
Batch 14/64 loss: -0.2856743633747101
Batch 15/64 loss: -0.28878161311149597
Batch 16/64 loss: -0.26805055141448975
Batch 17/64 loss: -0.2868841290473938
Batch 18/64 loss: -0.276682049036026
Batch 19/64 loss: -0.2574501633644104
Batch 20/64 loss: -0.26905763149261475
Batch 21/64 loss: -0.2872408926486969
Batch 22/64 loss: -0.28055471181869507
Batch 23/64 loss: -0.27714741230010986
Batch 24/64 loss: -0.26553529500961304
Batch 25/64 loss: -0.27578526735305786
Batch 26/64 loss: -0.26285475492477417
Batch 27/64 loss: -0.2741352319717407
Batch 28/64 loss: -0.28176912665367126
Batch 29/64 loss: -0.2564234137535095
Batch 30/64 loss: -0.2777857482433319
Batch 31/64 loss: -0.277458131313324
Batch 32/64 loss: -0.2718003988265991
Batch 33/64 loss: -0.2722526788711548
Batch 34/64 loss: -0.2672215700149536
Batch 35/64 loss: -0.2761891484260559
Batch 36/64 loss: -0.2879294753074646
Batch 37/64 loss: -0.267178475856781
Batch 38/64 loss: -0.26299387216567993
Batch 39/64 loss: -0.2876526713371277
Batch 40/64 loss: -0.24784278869628906
Batch 41/64 loss: -0.2778441905975342
Batch 42/64 loss: -0.26578807830810547
Batch 43/64 loss: -0.27883467078208923
Batch 44/64 loss: -0.27562379837036133
Batch 45/64 loss: -0.27333149313926697
Batch 46/64 loss: -0.2673732042312622
Batch 47/64 loss: -0.2753242552280426
Batch 48/64 loss: -0.27525001764297485
Batch 49/64 loss: -0.271175742149353
Batch 50/64 loss: -0.2681856155395508
Batch 51/64 loss: -0.2552223801612854
Batch 52/64 loss: -0.26821625232696533
Batch 53/64 loss: -0.28027021884918213
Batch 54/64 loss: -0.2669382691383362
Batch 55/64 loss: -0.2795069217681885
Batch 56/64 loss: -0.27797025442123413
Batch 57/64 loss: -0.28211694955825806
Batch 58/64 loss: -0.26875460147857666
Batch 59/64 loss: -0.26764941215515137
Batch 60/64 loss: -0.26054686307907104
Batch 61/64 loss: -0.2819412052631378
Batch 62/64 loss: -0.27571195363998413
Batch 63/64 loss: -0.2856735587120056
Batch 64/64 loss: -0.2685618996620178
Epoch 264  Train loss: -0.27463780501309565  Val loss: -0.24269044747467303
Epoch 265
-------------------------------
Batch 1/64 loss: -0.2731373906135559
Batch 2/64 loss: -0.28290945291519165
Batch 3/64 loss: -0.27893906831741333
Batch 4/64 loss: -0.27566438913345337
Batch 5/64 loss: -0.28163373470306396
Batch 6/64 loss: -0.26618504524230957
Batch 7/64 loss: -0.2812536358833313
Batch 8/64 loss: -0.27990204095840454
Batch 9/64 loss: -0.28204333782196045
Batch 10/64 loss: -0.28554898500442505
Batch 11/64 loss: -0.2662656903266907
Batch 12/64 loss: -0.28521013259887695
Batch 13/64 loss: -0.27759623527526855
Batch 14/64 loss: -0.2679719924926758
Batch 15/64 loss: -0.27213481068611145
Batch 16/64 loss: -0.2861006259918213
Batch 17/64 loss: -0.2811824679374695
Batch 18/64 loss: -0.28671136498451233
Batch 19/64 loss: -0.2691025733947754
Batch 20/64 loss: -0.28811031579971313
Batch 21/64 loss: -0.2790670394897461
Batch 22/64 loss: -0.2836306393146515
Batch 23/64 loss: -0.28622448444366455
Batch 24/64 loss: -0.27679938077926636
Batch 25/64 loss: -0.2729622721672058
Batch 26/64 loss: -0.28410694003105164
Batch 27/64 loss: -0.28008413314819336
Batch 28/64 loss: -0.2778858542442322
Batch 29/64 loss: -0.2826273441314697
Batch 30/64 loss: -0.28246912360191345
Batch 31/64 loss: -0.2781861424446106
Batch 32/64 loss: -0.2853497266769409
Batch 33/64 loss: -0.29097604751586914
Batch 34/64 loss: -0.2812710404396057
Batch 35/64 loss: -0.27996617555618286
Batch 36/64 loss: -0.24521082639694214
Batch 37/64 loss: -0.2776334881782532
Batch 38/64 loss: -0.2809717059135437
Batch 39/64 loss: -0.30062025785446167
Batch 40/64 loss: -0.281103253364563
Batch 41/64 loss: -0.28456151485443115
Batch 42/64 loss: -0.28455787897109985
Batch 43/64 loss: -0.2699436545372009
Batch 44/64 loss: -0.2824741005897522
Batch 45/64 loss: -0.2935842275619507
Batch 46/64 loss: -0.2810644805431366
Batch 47/64 loss: -0.2807832956314087
Batch 48/64 loss: -0.28235259652137756
Batch 49/64 loss: -0.27681964635849
Batch 50/64 loss: -0.27220451831817627
Batch 51/64 loss: -0.2917322516441345
Batch 52/64 loss: -0.2849953770637512
Batch 53/64 loss: -0.2673095464706421
Batch 54/64 loss: -0.28344452381134033
Batch 55/64 loss: -0.29056912660598755
Batch 56/64 loss: -0.2767336070537567
Batch 57/64 loss: -0.2810666859149933
Batch 58/64 loss: -0.2866010069847107
Batch 59/64 loss: -0.2796391248703003
Batch 60/64 loss: -0.279729962348938
Batch 61/64 loss: -0.2742803692817688
Batch 62/64 loss: -0.28560686111450195
Batch 63/64 loss: -0.2901439368724823
Batch 64/64 loss: -0.29232388734817505
Epoch 265  Train loss: -0.2803794568660212  Val loss: -0.2536972384272572
Epoch 266
-------------------------------
Batch 1/64 loss: -0.2840903103351593
Batch 2/64 loss: -0.2872229218482971
Batch 3/64 loss: -0.2774479389190674
Batch 4/64 loss: -0.2883302867412567
Batch 5/64 loss: -0.2891467213630676
Batch 6/64 loss: -0.2607218027114868
Batch 7/64 loss: -0.27455615997314453
Batch 8/64 loss: -0.28227245807647705
Batch 9/64 loss: -0.26630890369415283
Batch 10/64 loss: -0.26780372858047485
Batch 11/64 loss: -0.27036404609680176
Batch 12/64 loss: -0.28759217262268066
Batch 13/64 loss: -0.2748032808303833
Batch 14/64 loss: -0.26980483531951904
Batch 15/64 loss: -0.2922874093055725
Batch 16/64 loss: -0.2727660536766052
Batch 17/64 loss: -0.2806709408760071
Batch 18/64 loss: -0.27156370878219604
Batch 19/64 loss: -0.2604826092720032
Batch 20/64 loss: -0.2816089391708374
Batch 21/64 loss: -0.2656515836715698
Batch 22/64 loss: -0.2710280418395996
Batch 23/64 loss: -0.2803436517715454
Batch 24/64 loss: -0.28054291009902954
Batch 25/64 loss: -0.2836020588874817
Batch 26/64 loss: -0.2777937948703766
Batch 27/64 loss: -0.2740302085876465
Batch 28/64 loss: -0.2780207395553589
Batch 29/64 loss: -0.26947516202926636
Batch 30/64 loss: -0.2627020478248596
Batch 31/64 loss: -0.2764584422111511
Batch 32/64 loss: -0.27588945627212524
Batch 33/64 loss: -0.28237271308898926
Batch 34/64 loss: -0.27846983075141907
Batch 35/64 loss: -0.2683953642845154
Batch 36/64 loss: -0.286663681268692
Batch 37/64 loss: -0.2624659538269043
Batch 38/64 loss: -0.2714160084724426
Batch 39/64 loss: -0.28039470314979553
Batch 40/64 loss: -0.2746403217315674
Batch 41/64 loss: -0.29051828384399414
Batch 42/64 loss: -0.2825812101364136
Batch 43/64 loss: -0.2791571617126465
Batch 44/64 loss: -0.2871295213699341
Batch 45/64 loss: -0.2858940362930298
Batch 46/64 loss: -0.28556108474731445
Batch 47/64 loss: -0.2802162170410156
Batch 48/64 loss: -0.2745620906352997
Batch 49/64 loss: -0.28439074754714966
Batch 50/64 loss: -0.29188817739486694
Batch 51/64 loss: -0.2734912633895874
Batch 52/64 loss: -0.2657594084739685
Batch 53/64 loss: -0.28627079725265503
Batch 54/64 loss: -0.2851862907409668
Batch 55/64 loss: -0.26439785957336426
Batch 56/64 loss: -0.29150110483169556
Batch 57/64 loss: -0.269344687461853
Batch 58/64 loss: -0.275607705116272
Batch 59/64 loss: -0.2924540042877197
Batch 60/64 loss: -0.2796391248703003
Batch 61/64 loss: -0.2776893377304077
Batch 62/64 loss: -0.2904626429080963
Batch 63/64 loss: -0.29156386852264404
Batch 64/64 loss: -0.2863325774669647
Epoch 266  Train loss: -0.27827792880581875  Val loss: -0.24374470596051298
Epoch 267
-------------------------------
Batch 1/64 loss: -0.2786449193954468
Batch 2/64 loss: -0.29580360651016235
Batch 3/64 loss: -0.281480073928833
Batch 4/64 loss: -0.2882488965988159
Batch 5/64 loss: -0.2789708077907562
Batch 6/64 loss: -0.27501142024993896
Batch 7/64 loss: -0.2762809991836548
Batch 8/64 loss: -0.28288179636001587
Batch 9/64 loss: -0.27182066440582275
Batch 10/64 loss: -0.2790318727493286
Batch 11/64 loss: -0.2864670157432556
Batch 12/64 loss: -0.292175829410553
Batch 13/64 loss: -0.2872675657272339
Batch 14/64 loss: -0.2810012698173523
Batch 15/64 loss: -0.2722405791282654
Batch 16/64 loss: -0.2880796492099762
Batch 17/64 loss: -0.2617652416229248
Batch 18/64 loss: -0.277940571308136
Batch 19/64 loss: -0.26410698890686035
Batch 20/64 loss: -0.27938276529312134
Batch 21/64 loss: -0.2895856499671936
Batch 22/64 loss: -0.28559309244155884
Batch 23/64 loss: -0.2801337242126465
Batch 24/64 loss: -0.2863125205039978
Batch 25/64 loss: -0.28546595573425293
Batch 26/64 loss: -0.27854230999946594
Batch 27/64 loss: -0.28149062395095825
Batch 28/64 loss: -0.2804678976535797
Batch 29/64 loss: -0.2783622145652771
Batch 30/64 loss: -0.28840458393096924
Batch 31/64 loss: -0.27474844455718994
Batch 32/64 loss: -0.2842918038368225
Batch 33/64 loss: -0.28315478563308716
Batch 34/64 loss: -0.27191847562789917
Batch 35/64 loss: -0.2846909165382385
Batch 36/64 loss: -0.2781282663345337
Batch 37/64 loss: -0.2641521096229553
Batch 38/64 loss: -0.2938718795776367
Batch 39/64 loss: -0.2854107618331909
Batch 40/64 loss: -0.2725719213485718
Batch 41/64 loss: -0.277359277009964
Batch 42/64 loss: -0.27731066942214966
Batch 43/64 loss: -0.282589852809906
Batch 44/64 loss: -0.2967693507671356
Batch 45/64 loss: -0.27031636238098145
Batch 46/64 loss: -0.285858154296875
Batch 47/64 loss: -0.275007426738739
Batch 48/64 loss: -0.29235294461250305
Batch 49/64 loss: -0.27795737981796265
Batch 50/64 loss: -0.2687535881996155
Batch 51/64 loss: -0.28787776827812195
Batch 52/64 loss: -0.2815866470336914
Batch 53/64 loss: -0.28725510835647583
Batch 54/64 loss: -0.2799033522605896
Batch 55/64 loss: -0.2941661477088928
Batch 56/64 loss: -0.2819984555244446
Batch 57/64 loss: -0.27353358268737793
Batch 58/64 loss: -0.275503933429718
Batch 59/64 loss: -0.2833135724067688
Batch 60/64 loss: -0.28147196769714355
Batch 61/64 loss: -0.2777952551841736
Batch 62/64 loss: -0.27601420879364014
Batch 63/64 loss: -0.2933714687824249
Batch 64/64 loss: -0.28874796628952026
Epoch 267  Train loss: -0.2811063203157163  Val loss: -0.2647305325544167
Saving best model, epoch: 267
Epoch 268
-------------------------------
Batch 1/64 loss: -0.28057312965393066
Batch 2/64 loss: -0.28449511528015137
Batch 3/64 loss: -0.2963738739490509
Batch 4/64 loss: -0.27510350942611694
Batch 5/64 loss: -0.27655133605003357
Batch 6/64 loss: -0.2785398066043854
Batch 7/64 loss: -0.2889428734779358
Batch 8/64 loss: -0.2881155014038086
Batch 9/64 loss: -0.2667621374130249
Batch 10/64 loss: -0.27892667055130005
Batch 11/64 loss: -0.28448787331581116
Batch 12/64 loss: -0.2897412180900574
Batch 13/64 loss: -0.27447378635406494
Batch 14/64 loss: -0.2868194282054901
Batch 15/64 loss: -0.28343912959098816
Batch 16/64 loss: -0.29415184259414673
Batch 17/64 loss: -0.28252047300338745
Batch 18/64 loss: -0.28363949060440063
Batch 19/64 loss: -0.28172221779823303
Batch 20/64 loss: -0.2794952988624573
Batch 21/64 loss: -0.28988412022590637
Batch 22/64 loss: -0.27887919545173645
Batch 23/64 loss: -0.28443872928619385
Batch 24/64 loss: -0.2739194631576538
Batch 25/64 loss: -0.2796940803527832
Batch 26/64 loss: -0.2831442058086395
Batch 27/64 loss: -0.26811057329177856
Batch 28/64 loss: -0.2800547480583191
Batch 29/64 loss: -0.26981377601623535
Batch 30/64 loss: -0.2797592878341675
Batch 31/64 loss: -0.2861451506614685
Batch 32/64 loss: -0.2850746512413025
Batch 33/64 loss: -0.27731817960739136
Batch 34/64 loss: -0.28912073373794556
Batch 35/64 loss: -0.28690579533576965
Batch 36/64 loss: -0.2688820958137512
Batch 37/64 loss: -0.2751988172531128
Batch 38/64 loss: -0.27359992265701294
Batch 39/64 loss: -0.2797602117061615
Batch 40/64 loss: -0.2743854522705078
Batch 41/64 loss: -0.2825049161911011
Batch 42/64 loss: -0.29040077328681946
Batch 43/64 loss: -0.27247077226638794
Batch 44/64 loss: -0.2721588611602783
Batch 45/64 loss: -0.27708977460861206
Batch 46/64 loss: -0.2713039517402649
Batch 47/64 loss: -0.27734071016311646
Batch 48/64 loss: -0.27637290954589844
Batch 49/64 loss: -0.28506916761398315
Batch 50/64 loss: -0.27791470289230347
Batch 51/64 loss: -0.2948574423789978
Batch 52/64 loss: -0.25773394107818604
Batch 53/64 loss: -0.28216490149497986
Batch 54/64 loss: -0.2792702317237854
Batch 55/64 loss: -0.27096128463745117
Batch 56/64 loss: -0.28529995679855347
Batch 57/64 loss: -0.2714594602584839
Batch 58/64 loss: -0.2706698179244995
Batch 59/64 loss: -0.282636821269989
Batch 60/64 loss: -0.27830082178115845
Batch 61/64 loss: -0.289798378944397
Batch 62/64 loss: -0.29081425070762634
Batch 63/64 loss: -0.29170113801956177
Batch 64/64 loss: -0.28041380643844604
Epoch 268  Train loss: -0.28043245871861777  Val loss: -0.2579578372211391
Epoch 269
-------------------------------
Batch 1/64 loss: -0.283091276884079
Batch 2/64 loss: -0.28215301036834717
Batch 3/64 loss: -0.2847142815589905
Batch 4/64 loss: -0.28864383697509766
Batch 5/64 loss: -0.29359862208366394
Batch 6/64 loss: -0.28051164746284485
Batch 7/64 loss: -0.28016185760498047
Batch 8/64 loss: -0.28203898668289185
Batch 9/64 loss: -0.278879851102829
Batch 10/64 loss: -0.271598219871521
Batch 11/64 loss: -0.2893351912498474
Batch 12/64 loss: -0.27379941940307617
Batch 13/64 loss: -0.28832095861434937
Batch 14/64 loss: -0.28382980823516846
Batch 15/64 loss: -0.28438907861709595
Batch 16/64 loss: -0.28409141302108765
Batch 17/64 loss: -0.27952831983566284
Batch 18/64 loss: -0.28197020292282104
Batch 19/64 loss: -0.27133888006210327
Batch 20/64 loss: -0.2819056212902069
Batch 21/64 loss: -0.2611098289489746
Batch 22/64 loss: -0.2851220667362213
Batch 23/64 loss: -0.26794159412384033
Batch 24/64 loss: -0.28187230229377747
Batch 25/64 loss: -0.2691617012023926
Batch 26/64 loss: -0.2883181869983673
Batch 27/64 loss: -0.2586197257041931
Batch 28/64 loss: -0.2716924548149109
Batch 29/64 loss: -0.2823667526245117
Batch 30/64 loss: -0.2644854784011841
Batch 31/64 loss: -0.277167409658432
Batch 32/64 loss: -0.2879270911216736
Batch 33/64 loss: -0.2825775146484375
Batch 34/64 loss: -0.2757137417793274
Batch 35/64 loss: -0.2758556306362152
Batch 36/64 loss: -0.27301084995269775
Batch 37/64 loss: -0.28719455003738403
Batch 38/64 loss: -0.2726636528968811
Batch 39/64 loss: -0.27223461866378784
Batch 40/64 loss: -0.2855542302131653
Batch 41/64 loss: -0.2746371030807495
Batch 42/64 loss: -0.26524168252944946
Batch 43/64 loss: -0.27541500329971313
Batch 44/64 loss: -0.287051796913147
Batch 45/64 loss: -0.27691471576690674
Batch 46/64 loss: -0.28942131996154785
Batch 47/64 loss: -0.274192214012146
Batch 48/64 loss: -0.2765137851238251
Batch 49/64 loss: -0.28009268641471863
Batch 50/64 loss: -0.27003467082977295
Batch 51/64 loss: -0.27684423327445984
Batch 52/64 loss: -0.27089130878448486
Batch 53/64 loss: -0.2848776578903198
Batch 54/64 loss: -0.276001513004303
Batch 55/64 loss: -0.27137625217437744
Batch 56/64 loss: -0.28792181611061096
Batch 57/64 loss: -0.2836427092552185
Batch 58/64 loss: -0.2738072872161865
Batch 59/64 loss: -0.28999555110931396
Batch 60/64 loss: -0.27134859561920166
Batch 61/64 loss: -0.2767430543899536
Batch 62/64 loss: -0.29308512806892395
Batch 63/64 loss: -0.26586103439331055
Batch 64/64 loss: -0.2900983691215515
Epoch 269  Train loss: -0.27880744723712697  Val loss: -0.2479534513761907
Epoch 270
-------------------------------
Batch 1/64 loss: -0.2876012921333313
Batch 2/64 loss: -0.2781832814216614
Batch 3/64 loss: -0.27800115942955017
Batch 4/64 loss: -0.2818219065666199
Batch 5/64 loss: -0.2814459204673767
Batch 6/64 loss: -0.2914574444293976
Batch 7/64 loss: -0.26917344331741333
Batch 8/64 loss: -0.2679096460342407
Batch 9/64 loss: -0.27846378087997437
Batch 10/64 loss: -0.2843119204044342
Batch 11/64 loss: -0.2824406921863556
Batch 12/64 loss: -0.2861945629119873
Batch 13/64 loss: -0.28020936250686646
Batch 14/64 loss: -0.2728790044784546
Batch 15/64 loss: -0.28665268421173096
Batch 16/64 loss: -0.2685425281524658
Batch 17/64 loss: -0.2665879726409912
Batch 18/64 loss: -0.27527403831481934
Batch 19/64 loss: -0.2714221775531769
Batch 20/64 loss: -0.28425973653793335
Batch 21/64 loss: -0.28419017791748047
Batch 22/64 loss: -0.27597755193710327
Batch 23/64 loss: -0.2887212038040161
Batch 24/64 loss: -0.27724772691726685
Batch 25/64 loss: -0.2820766568183899
Batch 26/64 loss: -0.27897927165031433
Batch 27/64 loss: -0.2838577330112457
Batch 28/64 loss: -0.2920691967010498
Batch 29/64 loss: -0.2823391556739807
Batch 30/64 loss: -0.2879287004470825
Batch 31/64 loss: -0.28474798798561096
Batch 32/64 loss: -0.28239285945892334
Batch 33/64 loss: -0.2872047424316406
Batch 34/64 loss: -0.2825927734375
Batch 35/64 loss: -0.2636891007423401
Batch 36/64 loss: -0.26796579360961914
Batch 37/64 loss: -0.26608502864837646
Batch 38/64 loss: -0.2722513675689697
Batch 39/64 loss: -0.2799341082572937
Batch 40/64 loss: -0.29155299067497253
Batch 41/64 loss: -0.28591614961624146
Batch 42/64 loss: -0.2788277268409729
Batch 43/64 loss: -0.2772117555141449
Batch 44/64 loss: -0.27170389890670776
Batch 45/64 loss: -0.287783682346344
Batch 46/64 loss: -0.2802475392818451
Batch 47/64 loss: -0.2747788429260254
Batch 48/64 loss: -0.28597456216812134
Batch 49/64 loss: -0.27072083950042725
Batch 50/64 loss: -0.280748188495636
Batch 51/64 loss: -0.28253763914108276
Batch 52/64 loss: -0.2871491014957428
Batch 53/64 loss: -0.28682076930999756
Batch 54/64 loss: -0.2668555974960327
Batch 55/64 loss: -0.2823651432991028
Batch 56/64 loss: -0.28932929039001465
Batch 57/64 loss: -0.289130300283432
Batch 58/64 loss: -0.29099589586257935
Batch 59/64 loss: -0.29758915305137634
Batch 60/64 loss: -0.28152939677238464
Batch 61/64 loss: -0.2970614433288574
Batch 62/64 loss: -0.27751025557518005
Batch 63/64 loss: -0.28758299350738525
Batch 64/64 loss: -0.2819545567035675
Epoch 270  Train loss: -0.2808858472926944  Val loss: -0.25239588018135517
Epoch 271
-------------------------------
Batch 1/64 loss: -0.3001498579978943
Batch 2/64 loss: -0.29526010155677795
Batch 3/64 loss: -0.28538161516189575
Batch 4/64 loss: -0.2830820083618164
Batch 5/64 loss: -0.2883298993110657
Batch 6/64 loss: -0.2830714285373688
Batch 7/64 loss: -0.28939980268478394
Batch 8/64 loss: -0.27848100662231445
Batch 9/64 loss: -0.28003284335136414
Batch 10/64 loss: -0.2838284969329834
Batch 11/64 loss: -0.2838505208492279
Batch 12/64 loss: -0.2916609048843384
Batch 13/64 loss: -0.293540894985199
Batch 14/64 loss: -0.2619144916534424
Batch 15/64 loss: -0.2790990471839905
Batch 16/64 loss: -0.2880510687828064
Batch 17/64 loss: -0.28003621101379395
Batch 18/64 loss: -0.2766706943511963
Batch 19/64 loss: -0.2766357362270355
Batch 20/64 loss: -0.28415447473526
Batch 21/64 loss: -0.2946743965148926
Batch 22/64 loss: -0.28012505173683167
Batch 23/64 loss: -0.2534971237182617
Batch 24/64 loss: -0.27107536792755127
Batch 25/64 loss: -0.2938931882381439
Batch 26/64 loss: -0.26165491342544556
Batch 27/64 loss: -0.26931437849998474
Batch 28/64 loss: -0.2980399429798126
Batch 29/64 loss: -0.2844478189945221
Batch 30/64 loss: -0.2955546975135803
Batch 31/64 loss: -0.269852876663208
Batch 32/64 loss: -0.28373968601226807
Batch 33/64 loss: -0.2793963849544525
Batch 34/64 loss: -0.276664674282074
Batch 35/64 loss: -0.28492242097854614
Batch 36/64 loss: -0.2708113193511963
Batch 37/64 loss: -0.28294456005096436
Batch 38/64 loss: -0.2763502597808838
Batch 39/64 loss: -0.27054375410079956
Batch 40/64 loss: -0.28413844108581543
Batch 41/64 loss: -0.2831115126609802
Batch 42/64 loss: -0.27910733222961426
Batch 43/64 loss: -0.2744184732437134
Batch 44/64 loss: -0.2725951075553894
Batch 45/64 loss: -0.2667410373687744
Batch 46/64 loss: -0.2791230082511902
Batch 47/64 loss: -0.28663650155067444
Batch 48/64 loss: -0.2842737138271332
Batch 49/64 loss: -0.29499301314353943
Batch 50/64 loss: -0.28037163615226746
Batch 51/64 loss: -0.29930630326271057
Batch 52/64 loss: -0.28427720069885254
Batch 53/64 loss: -0.2560776472091675
Batch 54/64 loss: -0.2790355980396271
Batch 55/64 loss: -0.26568400859832764
Batch 56/64 loss: -0.2738747000694275
Batch 57/64 loss: -0.2756987512111664
Batch 58/64 loss: -0.2818920910358429
Batch 59/64 loss: -0.28483444452285767
Batch 60/64 loss: -0.26805979013442993
Batch 61/64 loss: -0.27852579951286316
Batch 62/64 loss: -0.2847137451171875
Batch 63/64 loss: -0.28254640102386475
Batch 64/64 loss: -0.2698630094528198
Epoch 271  Train loss: -0.28051086173338047  Val loss: -0.24787764758178868
Epoch 272
-------------------------------
Batch 1/64 loss: -0.2910322844982147
Batch 2/64 loss: -0.2827473282814026
Batch 3/64 loss: -0.26867544651031494
Batch 4/64 loss: -0.2814346253871918
Batch 5/64 loss: -0.2631499171257019
Batch 6/64 loss: -0.2765059769153595
Batch 7/64 loss: -0.27636075019836426
Batch 8/64 loss: -0.2726585865020752
Batch 9/64 loss: -0.24969327449798584
Batch 10/64 loss: -0.2789239287376404
Batch 11/64 loss: -0.27558064460754395
Batch 12/64 loss: -0.27059459686279297
Batch 13/64 loss: -0.2816751003265381
Batch 14/64 loss: -0.27448683977127075
Batch 15/64 loss: -0.2743184566497803
Batch 16/64 loss: -0.27036917209625244
Batch 17/64 loss: -0.27670443058013916
Batch 18/64 loss: -0.27704983949661255
Batch 19/64 loss: -0.2802272439002991
Batch 20/64 loss: -0.28454479575157166
Batch 21/64 loss: -0.2748792767524719
Batch 22/64 loss: -0.2672288417816162
Batch 23/64 loss: -0.26665496826171875
Batch 24/64 loss: -0.26647651195526123
Batch 25/64 loss: -0.27756190299987793
Batch 26/64 loss: -0.2832690477371216
Batch 27/64 loss: -0.25727224349975586
Batch 28/64 loss: -0.2797292470932007
Batch 29/64 loss: -0.26462942361831665
Batch 30/64 loss: -0.27841728925704956
Batch 31/64 loss: -0.2841774821281433
Batch 32/64 loss: -0.28267866373062134
Batch 33/64 loss: -0.26854443550109863
Batch 34/64 loss: -0.28210747241973877
Batch 35/64 loss: -0.25496262311935425
Batch 36/64 loss: -0.2715134024620056
Batch 37/64 loss: -0.287991464138031
Batch 38/64 loss: -0.24990075826644897
Batch 39/64 loss: -0.2797184884548187
Batch 40/64 loss: -0.2667578458786011
Batch 41/64 loss: -0.28294968605041504
Batch 42/64 loss: -0.2874565124511719
Batch 43/64 loss: -0.2836862802505493
Batch 44/64 loss: -0.26911038160324097
Batch 45/64 loss: -0.28356871008872986
Batch 46/64 loss: -0.2701514959335327
Batch 47/64 loss: -0.2875893712043762
Batch 48/64 loss: -0.2705894708633423
Batch 49/64 loss: -0.2850225567817688
Batch 50/64 loss: -0.2809956669807434
Batch 51/64 loss: -0.279232919216156
Batch 52/64 loss: -0.2829025685787201
Batch 53/64 loss: -0.28982675075531006
Batch 54/64 loss: -0.28101247549057007
Batch 55/64 loss: -0.27755945920944214
Batch 56/64 loss: -0.2720991373062134
Batch 57/64 loss: -0.2725897431373596
Batch 58/64 loss: -0.2842239737510681
Batch 59/64 loss: -0.2835938334465027
Batch 60/64 loss: -0.2597743272781372
Batch 61/64 loss: -0.26357322931289673
Batch 62/64 loss: -0.26801377534866333
Batch 63/64 loss: -0.267536461353302
Batch 64/64 loss: -0.2872217297554016
Epoch 272  Train loss: -0.27528909351311476  Val loss: -0.25423001403251466
Epoch 273
-------------------------------
Batch 1/64 loss: -0.2829975485801697
Batch 2/64 loss: -0.28573310375213623
Batch 3/64 loss: -0.27867794036865234
Batch 4/64 loss: -0.29126253724098206
Batch 5/64 loss: -0.27668270468711853
Batch 6/64 loss: -0.28141850233078003
Batch 7/64 loss: -0.26974618434906006
Batch 8/64 loss: -0.2879565954208374
Batch 9/64 loss: -0.279339998960495
Batch 10/64 loss: -0.27207881212234497
Batch 11/64 loss: -0.278263121843338
Batch 12/64 loss: -0.2887752652168274
Batch 13/64 loss: -0.2849739193916321
Batch 14/64 loss: -0.2813034653663635
Batch 15/64 loss: -0.2781447768211365
Batch 16/64 loss: -0.26948773860931396
Batch 17/64 loss: -0.27512824535369873
Batch 18/64 loss: -0.2841118574142456
Batch 19/64 loss: -0.2701725363731384
Batch 20/64 loss: -0.2764241695404053
Batch 21/64 loss: -0.27614760398864746
Batch 22/64 loss: -0.2843100428581238
Batch 23/64 loss: -0.2829727828502655
Batch 24/64 loss: -0.27299565076828003
Batch 25/64 loss: -0.25674670934677124
Batch 26/64 loss: -0.27987760305404663
Batch 27/64 loss: -0.287960410118103
Batch 28/64 loss: -0.2774478793144226
Batch 29/64 loss: -0.275601863861084
Batch 30/64 loss: -0.275180459022522
Batch 31/64 loss: -0.28407326340675354
Batch 32/64 loss: -0.2745526432991028
Batch 33/64 loss: -0.2973008155822754
Batch 34/64 loss: -0.28333884477615356
Batch 35/64 loss: -0.291992723941803
Batch 36/64 loss: -0.27291637659072876
Batch 37/64 loss: -0.2765105962753296
Batch 38/64 loss: -0.28731000423431396
Batch 39/64 loss: -0.2756495475769043
Batch 40/64 loss: -0.2768316864967346
Batch 41/64 loss: -0.28132474422454834
Batch 42/64 loss: -0.27663612365722656
Batch 43/64 loss: -0.28800123929977417
Batch 44/64 loss: -0.27659159898757935
Batch 45/64 loss: -0.26967114210128784
Batch 46/64 loss: -0.27691352367401123
Batch 47/64 loss: -0.27948570251464844
Batch 48/64 loss: -0.2736032009124756
Batch 49/64 loss: -0.27309900522232056
Batch 50/64 loss: -0.2677474617958069
Batch 51/64 loss: -0.2746279835700989
Batch 52/64 loss: -0.2815364599227905
Batch 53/64 loss: -0.2653845548629761
Batch 54/64 loss: -0.2751850485801697
Batch 55/64 loss: -0.283557653427124
Batch 56/64 loss: -0.2781110405921936
Batch 57/64 loss: -0.26767498254776
Batch 58/64 loss: -0.2789217233657837
Batch 59/64 loss: -0.2761998176574707
Batch 60/64 loss: -0.26377153396606445
Batch 61/64 loss: -0.2776055932044983
Batch 62/64 loss: -0.2541521191596985
Batch 63/64 loss: -0.2765611410140991
Batch 64/64 loss: -0.2521962523460388
Epoch 273  Train loss: -0.2774573665039212  Val loss: -0.22731863346296488
Epoch 274
-------------------------------
Batch 1/64 loss: -0.2552343010902405
Batch 2/64 loss: -0.26664453744888306
Batch 3/64 loss: -0.2715466022491455
Batch 4/64 loss: -0.2682831883430481
Batch 5/64 loss: -0.27039945125579834
Batch 6/64 loss: -0.26627129316329956
Batch 7/64 loss: -0.26167428493499756
Batch 8/64 loss: -0.2807314693927765
Batch 9/64 loss: -0.26339811086654663
Batch 10/64 loss: -0.2601583003997803
Batch 11/64 loss: -0.2716309130191803
Batch 12/64 loss: -0.27775922417640686
Batch 13/64 loss: -0.2813606262207031
Batch 14/64 loss: -0.2708234190940857
Batch 15/64 loss: -0.2741016149520874
Batch 16/64 loss: -0.2667360305786133
Batch 17/64 loss: -0.2785353660583496
Batch 18/64 loss: -0.2786235809326172
Batch 19/64 loss: -0.26730310916900635
Batch 20/64 loss: -0.27324312925338745
Batch 21/64 loss: -0.2767813801765442
Batch 22/64 loss: -0.2769107222557068
Batch 23/64 loss: -0.271173894405365
Batch 24/64 loss: -0.2685696482658386
Batch 25/64 loss: -0.2729201316833496
Batch 26/64 loss: -0.2679864168167114
Batch 27/64 loss: -0.26816582679748535
Batch 28/64 loss: -0.27784407138824463
Batch 29/64 loss: -0.2716480493545532
Batch 30/64 loss: -0.2636762261390686
Batch 31/64 loss: -0.27428749203681946
Batch 32/64 loss: -0.26567381620407104
Batch 33/64 loss: -0.27443546056747437
Batch 34/64 loss: -0.27403324842453003
Batch 35/64 loss: -0.24609827995300293
Batch 36/64 loss: -0.26281994581222534
Batch 37/64 loss: -0.2633432149887085
Batch 38/64 loss: -0.25719505548477173
Batch 39/64 loss: -0.2729053497314453
Batch 40/64 loss: -0.27383923530578613
Batch 41/64 loss: -0.26949506998062134
Batch 42/64 loss: -0.2721884846687317
Batch 43/64 loss: -0.2646864652633667
Batch 44/64 loss: -0.2777746319770813
Batch 45/64 loss: -0.27472686767578125
Batch 46/64 loss: -0.2819575071334839
Batch 47/64 loss: -0.27169132232666016
Batch 48/64 loss: -0.26930367946624756
Batch 49/64 loss: -0.2744479477405548
Batch 50/64 loss: -0.2798336148262024
Batch 51/64 loss: -0.27375078201293945
Batch 52/64 loss: -0.25725042819976807
Batch 53/64 loss: -0.2802361249923706
Batch 54/64 loss: -0.28772157430648804
Batch 55/64 loss: -0.2795547842979431
Batch 56/64 loss: -0.2542657256126404
Batch 57/64 loss: -0.27321046590805054
Batch 58/64 loss: -0.27028146386146545
Batch 59/64 loss: -0.2742830216884613
Batch 60/64 loss: -0.2714620530605316
Batch 61/64 loss: -0.25891125202178955
Batch 62/64 loss: -0.27126944065093994
Batch 63/64 loss: -0.2573925852775574
Batch 64/64 loss: -0.28074389696121216
Epoch 274  Train loss: -0.27044736053429397  Val loss: -0.2551121508952269
Epoch 275
-------------------------------
Batch 1/64 loss: -0.2645998001098633
Batch 2/64 loss: -0.2826840579509735
Batch 3/64 loss: -0.2751450538635254
Batch 4/64 loss: -0.26777660846710205
Batch 5/64 loss: -0.27635037899017334
Batch 6/64 loss: -0.27957046031951904
Batch 7/64 loss: -0.2736046612262726
Batch 8/64 loss: -0.2871120572090149
Batch 9/64 loss: -0.2784479260444641
Batch 10/64 loss: -0.2671167850494385
Batch 11/64 loss: -0.2771659791469574
Batch 12/64 loss: -0.24732720851898193
Batch 13/64 loss: -0.28070777654647827
Batch 14/64 loss: -0.2807573676109314
Batch 15/64 loss: -0.265780508518219
Batch 16/64 loss: -0.28395456075668335
Batch 17/64 loss: -0.2831498384475708
Batch 18/64 loss: -0.2750225067138672
Batch 19/64 loss: -0.27117711305618286
Batch 20/64 loss: -0.2882179021835327
Batch 21/64 loss: -0.2703416347503662
Batch 22/64 loss: -0.28548067808151245
Batch 23/64 loss: -0.27002251148223877
Batch 24/64 loss: -0.28205379843711853
Batch 25/64 loss: -0.27680090069770813
Batch 26/64 loss: -0.2711476981639862
Batch 27/64 loss: -0.27653050422668457
Batch 28/64 loss: -0.2743774652481079
Batch 29/64 loss: -0.271680623292923
Batch 30/64 loss: -0.2869994640350342
Batch 31/64 loss: -0.2833250164985657
Batch 32/64 loss: -0.26967287063598633
Batch 33/64 loss: -0.2713658809661865
Batch 34/64 loss: -0.2907285690307617
Batch 35/64 loss: -0.2683970034122467
Batch 36/64 loss: -0.2770339250564575
Batch 37/64 loss: -0.2793247103691101
Batch 38/64 loss: -0.2765341103076935
Batch 39/64 loss: -0.2854469418525696
Batch 40/64 loss: -0.2799542248249054
Batch 41/64 loss: -0.2775452435016632
Batch 42/64 loss: -0.2658233642578125
Batch 43/64 loss: -0.28282225131988525
Batch 44/64 loss: -0.273298054933548
Batch 45/64 loss: -0.2759242355823517
Batch 46/64 loss: -0.2797200083732605
Batch 47/64 loss: -0.278740793466568
Batch 48/64 loss: -0.2786606550216675
Batch 49/64 loss: -0.28509634733200073
Batch 50/64 loss: -0.279174268245697
Batch 51/64 loss: -0.26986202597618103
Batch 52/64 loss: -0.26701509952545166
Batch 53/64 loss: -0.2813892066478729
Batch 54/64 loss: -0.2731679677963257
Batch 55/64 loss: -0.29129695892333984
Batch 56/64 loss: -0.2739386260509491
Batch 57/64 loss: -0.28660476207733154
Batch 58/64 loss: -0.2721157371997833
Batch 59/64 loss: -0.27646228671073914
Batch 60/64 loss: -0.27149683237075806
Batch 61/64 loss: -0.27792784571647644
Batch 62/64 loss: -0.28369414806365967
Batch 63/64 loss: -0.27693215012550354
Batch 64/64 loss: -0.2863369882106781
Epoch 275  Train loss: -0.2768054696859098  Val loss: -0.24187039100017743
Epoch 276
-------------------------------
Batch 1/64 loss: -0.28540417551994324
Batch 2/64 loss: -0.2856544554233551
Batch 3/64 loss: -0.28582870960235596
Batch 4/64 loss: -0.2859046459197998
Batch 5/64 loss: -0.2775935232639313
Batch 6/64 loss: -0.28166598081588745
Batch 7/64 loss: -0.27730679512023926
Batch 8/64 loss: -0.2869799733161926
Batch 9/64 loss: -0.28279101848602295
Batch 10/64 loss: -0.2794155478477478
Batch 11/64 loss: -0.2727906107902527
Batch 12/64 loss: -0.2904406785964966
Batch 13/64 loss: -0.2639739513397217
Batch 14/64 loss: -0.2787860929965973
Batch 15/64 loss: -0.29522883892059326
Batch 16/64 loss: -0.26941704750061035
Batch 17/64 loss: -0.271107017993927
Batch 18/64 loss: -0.28855934739112854
Batch 19/64 loss: -0.2794327139854431
Batch 20/64 loss: -0.2854239344596863
Batch 21/64 loss: -0.2859600782394409
Batch 22/64 loss: -0.2733861804008484
Batch 23/64 loss: -0.290546178817749
Batch 24/64 loss: -0.27735263109207153
Batch 25/64 loss: -0.2742476463317871
Batch 26/64 loss: -0.23080915212631226
Batch 27/64 loss: -0.2930148243904114
Batch 28/64 loss: -0.2618035078048706
Batch 29/64 loss: -0.2629321813583374
Batch 30/64 loss: -0.2772591710090637
Batch 31/64 loss: -0.2879852056503296
Batch 32/64 loss: -0.2749165892601013
Batch 33/64 loss: -0.26961594820022583
Batch 34/64 loss: -0.2736610174179077
Batch 35/64 loss: -0.2600610852241516
Batch 36/64 loss: -0.26845765113830566
Batch 37/64 loss: -0.2934989333152771
Batch 38/64 loss: -0.29003262519836426
Batch 39/64 loss: -0.28421691060066223
Batch 40/64 loss: -0.2869483530521393
Batch 41/64 loss: -0.28172147274017334
Batch 42/64 loss: -0.2878227233886719
Batch 43/64 loss: -0.28165578842163086
Batch 44/64 loss: -0.28358468413352966
Batch 45/64 loss: -0.2728568911552429
Batch 46/64 loss: -0.2747902274131775
Batch 47/64 loss: -0.2790337800979614
Batch 48/64 loss: -0.2892693281173706
Batch 49/64 loss: -0.2880462408065796
Batch 50/64 loss: -0.28603672981262207
Batch 51/64 loss: -0.28561756014823914
Batch 52/64 loss: -0.2896541357040405
Batch 53/64 loss: -0.27076244354248047
Batch 54/64 loss: -0.2799173891544342
Batch 55/64 loss: -0.2800406217575073
Batch 56/64 loss: -0.2799907922744751
Batch 57/64 loss: -0.28658536076545715
Batch 58/64 loss: -0.2691364288330078
Batch 59/64 loss: -0.2742117643356323
Batch 60/64 loss: -0.2757174074649811
Batch 61/64 loss: -0.2817964553833008
Batch 62/64 loss: -0.2842164933681488
Batch 63/64 loss: -0.28727757930755615
Batch 64/64 loss: -0.29239189624786377
Epoch 276  Train loss: -0.2796776023565554  Val loss: -0.24891305420406906
Epoch 277
-------------------------------
Batch 1/64 loss: -0.2897374629974365
Batch 2/64 loss: -0.2916942238807678
Batch 3/64 loss: -0.2783238887786865
Batch 4/64 loss: -0.2814435362815857
Batch 5/64 loss: -0.2701859474182129
Batch 6/64 loss: -0.2821395695209503
Batch 7/64 loss: -0.2889921963214874
Batch 8/64 loss: -0.29189997911453247
Batch 9/64 loss: -0.2872481644153595
Batch 10/64 loss: -0.2853303849697113
Batch 11/64 loss: -0.2741588354110718
Batch 12/64 loss: -0.2706867456436157
Batch 13/64 loss: -0.2814955711364746
Batch 14/64 loss: -0.2918224036693573
Batch 15/64 loss: -0.26434922218322754
Batch 16/64 loss: -0.28870248794555664
Batch 17/64 loss: -0.29130667448043823
Batch 18/64 loss: -0.2835174798965454
Batch 19/64 loss: -0.2846725285053253
Batch 20/64 loss: -0.2881327271461487
Batch 21/64 loss: -0.2861567437648773
Batch 22/64 loss: -0.2906208038330078
Batch 23/64 loss: -0.28831058740615845
Batch 24/64 loss: -0.29464149475097656
Batch 25/64 loss: -0.28574928641319275
Batch 26/64 loss: -0.2941863536834717
Batch 27/64 loss: -0.2833406925201416
Batch 28/64 loss: -0.2806202173233032
Batch 29/64 loss: -0.2925553321838379
Batch 30/64 loss: -0.2778618633747101
Batch 31/64 loss: -0.29217827320098877
Batch 32/64 loss: -0.27458542585372925
Batch 33/64 loss: -0.2784275710582733
Batch 34/64 loss: -0.2685849070549011
Batch 35/64 loss: -0.29290544986724854
Batch 36/64 loss: -0.27430203557014465
Batch 37/64 loss: -0.28213149309158325
Batch 38/64 loss: -0.2782846987247467
Batch 39/64 loss: -0.2809545695781708
Batch 40/64 loss: -0.2738840579986572
Batch 41/64 loss: -0.2758166193962097
Batch 42/64 loss: -0.26805734634399414
Batch 43/64 loss: -0.2734583914279938
Batch 44/64 loss: -0.28040361404418945
Batch 45/64 loss: -0.27849942445755005
Batch 46/64 loss: -0.27706313133239746
Batch 47/64 loss: -0.261934757232666
Batch 48/64 loss: -0.28582990169525146
Batch 49/64 loss: -0.27513831853866577
Batch 50/64 loss: -0.2715398669242859
Batch 51/64 loss: -0.27727240324020386
Batch 52/64 loss: -0.27306240797042847
Batch 53/64 loss: -0.27836716175079346
Batch 54/64 loss: -0.29309892654418945
Batch 55/64 loss: -0.2510616183280945
Batch 56/64 loss: -0.2860355079174042
Batch 57/64 loss: -0.2843630909919739
Batch 58/64 loss: -0.2641183137893677
Batch 59/64 loss: -0.2846580743789673
Batch 60/64 loss: -0.27663949131965637
Batch 61/64 loss: -0.2841707170009613
Batch 62/64 loss: -0.2684497833251953
Batch 63/64 loss: -0.2785707712173462
Batch 64/64 loss: -0.28299054503440857
Epoch 277  Train loss: -0.280721167606466  Val loss: -0.24711977575243133
Epoch 278
-------------------------------
Batch 1/64 loss: -0.2725073993206024
Batch 2/64 loss: -0.2760051190853119
Batch 3/64 loss: -0.27590471506118774
Batch 4/64 loss: -0.2653895616531372
Batch 5/64 loss: -0.2852855324745178
Batch 6/64 loss: -0.279069721698761
Batch 7/64 loss: -0.26919376850128174
Batch 8/64 loss: -0.2808406352996826
Batch 9/64 loss: -0.26846635341644287
Batch 10/64 loss: -0.28030967712402344
Batch 11/64 loss: -0.27527108788490295
Batch 12/64 loss: -0.29459717869758606
Batch 13/64 loss: -0.2713199853897095
Batch 14/64 loss: -0.2847864031791687
Batch 15/64 loss: -0.26917141675949097
Batch 16/64 loss: -0.291744589805603
Batch 17/64 loss: -0.27884021401405334
Batch 18/64 loss: -0.27719056606292725
Batch 19/64 loss: -0.28215479850769043
Batch 20/64 loss: -0.29487234354019165
Batch 21/64 loss: -0.29008224606513977
Batch 22/64 loss: -0.2956039607524872
Batch 23/64 loss: -0.27480238676071167
Batch 24/64 loss: -0.2820029854774475
Batch 25/64 loss: -0.2803422808647156
Batch 26/64 loss: -0.2849530577659607
Batch 27/64 loss: -0.2759833037853241
Batch 28/64 loss: -0.2902759313583374
Batch 29/64 loss: -0.2816069722175598
Batch 30/64 loss: -0.26429587602615356
Batch 31/64 loss: -0.27694350481033325
Batch 32/64 loss: -0.29000526666641235
Batch 33/64 loss: -0.2765681743621826
Batch 34/64 loss: -0.2863888740539551
Batch 35/64 loss: -0.277888685464859
Batch 36/64 loss: -0.2611621618270874
Batch 37/64 loss: -0.27887511253356934
Batch 38/64 loss: -0.27703791856765747
Batch 39/64 loss: -0.2771240472793579
Batch 40/64 loss: -0.27248573303222656
Batch 41/64 loss: -0.28406229615211487
Batch 42/64 loss: -0.27406740188598633
Batch 43/64 loss: -0.2680428624153137
Batch 44/64 loss: -0.2871049642562866
Batch 45/64 loss: -0.28304556012153625
Batch 46/64 loss: -0.27449721097946167
Batch 47/64 loss: -0.28280964493751526
Batch 48/64 loss: -0.277972936630249
Batch 49/64 loss: -0.28764522075653076
Batch 50/64 loss: -0.2733621597290039
Batch 51/64 loss: -0.28040117025375366
Batch 52/64 loss: -0.291279673576355
Batch 53/64 loss: -0.26997244358062744
Batch 54/64 loss: -0.27830013632774353
Batch 55/64 loss: -0.28685545921325684
Batch 56/64 loss: -0.27044326066970825
Batch 57/64 loss: -0.27628934383392334
Batch 58/64 loss: -0.2779771089553833
Batch 59/64 loss: -0.28520435094833374
Batch 60/64 loss: -0.26310497522354126
Batch 61/64 loss: -0.27737903594970703
Batch 62/64 loss: -0.2852324843406677
Batch 63/64 loss: -0.2751089334487915
Batch 64/64 loss: -0.29013490676879883
Epoch 278  Train loss: -0.279138970375061  Val loss: -0.24762033514960116
Epoch 279
-------------------------------
Batch 1/64 loss: -0.2814040780067444
Batch 2/64 loss: -0.2778558135032654
Batch 3/64 loss: -0.2652968168258667
Batch 4/64 loss: -0.2875416874885559
Batch 5/64 loss: -0.27567487955093384
Batch 6/64 loss: -0.2804984450340271
Batch 7/64 loss: -0.2738409638404846
Batch 8/64 loss: -0.2874898910522461
Batch 9/64 loss: -0.28508731722831726
Batch 10/64 loss: -0.2904464900493622
Batch 11/64 loss: -0.2909337282180786
Batch 12/64 loss: -0.27009016275405884
Batch 13/64 loss: -0.2734062671661377
Batch 14/64 loss: -0.2855190336704254
Batch 15/64 loss: -0.2840104103088379
Batch 16/64 loss: -0.2748214602470398
Batch 17/64 loss: -0.2911880612373352
Batch 18/64 loss: -0.2891587018966675
Batch 19/64 loss: -0.28617292642593384
Batch 20/64 loss: -0.28443437814712524
Batch 21/64 loss: -0.28224021196365356
Batch 22/64 loss: -0.28189322352409363
Batch 23/64 loss: -0.2854014039039612
Batch 24/64 loss: -0.28171032667160034
Batch 25/64 loss: -0.2929458022117615
Batch 26/64 loss: -0.2929638624191284
Batch 27/64 loss: -0.28512752056121826
Batch 28/64 loss: -0.2838309109210968
Batch 29/64 loss: -0.27971595525741577
Batch 30/64 loss: -0.2662280201911926
Batch 31/64 loss: -0.2855307459831238
Batch 32/64 loss: -0.2909623980522156
Batch 33/64 loss: -0.2745290994644165
Batch 34/64 loss: -0.2763359844684601
Batch 35/64 loss: -0.28680136799812317
Batch 36/64 loss: -0.2882396876811981
Batch 37/64 loss: -0.29013586044311523
Batch 38/64 loss: -0.2781939208507538
Batch 39/64 loss: -0.2768971621990204
Batch 40/64 loss: -0.2803698182106018
Batch 41/64 loss: -0.2481619119644165
Batch 42/64 loss: -0.29723575711250305
Batch 43/64 loss: -0.2829456329345703
Batch 44/64 loss: -0.28213146328926086
Batch 45/64 loss: -0.2783966064453125
Batch 46/64 loss: -0.2747683525085449
Batch 47/64 loss: -0.2829369902610779
Batch 48/64 loss: -0.27932125329971313
Batch 49/64 loss: -0.28994113206863403
Batch 50/64 loss: -0.29045262932777405
Batch 51/64 loss: -0.28326448798179626
Batch 52/64 loss: -0.2804260849952698
Batch 53/64 loss: -0.27579453587532043
Batch 54/64 loss: -0.26345300674438477
Batch 55/64 loss: -0.2710862159729004
Batch 56/64 loss: -0.27791091799736023
Batch 57/64 loss: -0.27214229106903076
Batch 58/64 loss: -0.28428879380226135
Batch 59/64 loss: -0.28143003582954407
Batch 60/64 loss: -0.2841547727584839
Batch 61/64 loss: -0.2816142439842224
Batch 62/64 loss: -0.2694370746612549
Batch 63/64 loss: -0.30034759640693665
Batch 64/64 loss: -0.27441978454589844
Epoch 279  Train loss: -0.2812922575894524  Val loss: -0.2588251257680126
Epoch 280
-------------------------------
Batch 1/64 loss: -0.2871019244194031
Batch 2/64 loss: -0.269565224647522
Batch 3/64 loss: -0.27700191736221313
Batch 4/64 loss: -0.28045105934143066
Batch 5/64 loss: -0.27509093284606934
Batch 6/64 loss: -0.2970975339412689
Batch 7/64 loss: -0.2955779433250427
Batch 8/64 loss: -0.27598732709884644
Batch 9/64 loss: -0.2880105674266815
Batch 10/64 loss: -0.28549474477767944
Batch 11/64 loss: -0.28410661220550537
Batch 12/64 loss: -0.2726045846939087
Batch 13/64 loss: -0.28097957372665405
Batch 14/64 loss: -0.28807854652404785
Batch 15/64 loss: -0.2788545489311218
Batch 16/64 loss: -0.27614879608154297
Batch 17/64 loss: -0.2884725332260132
Batch 18/64 loss: -0.27888578176498413
Batch 19/64 loss: -0.2779735326766968
Batch 20/64 loss: -0.2699460983276367
Batch 21/64 loss: -0.28173381090164185
Batch 22/64 loss: -0.2743551731109619
Batch 23/64 loss: -0.2881203293800354
Batch 24/64 loss: -0.27487272024154663
Batch 25/64 loss: -0.2925037443637848
Batch 26/64 loss: -0.28810256719589233
Batch 27/64 loss: -0.27959373593330383
Batch 28/64 loss: -0.2786467671394348
Batch 29/64 loss: -0.2780534327030182
Batch 30/64 loss: -0.27389657497406006
Batch 31/64 loss: -0.2850448489189148
Batch 32/64 loss: -0.27501416206359863
Batch 33/64 loss: -0.28067564964294434
Batch 34/64 loss: -0.29298198223114014
Batch 35/64 loss: -0.2876489460468292
Batch 36/64 loss: -0.28400954604148865
Batch 37/64 loss: -0.2892270088195801
Batch 38/64 loss: -0.28120553493499756
Batch 39/64 loss: -0.2853975296020508
Batch 40/64 loss: -0.29048630595207214
Batch 41/64 loss: -0.28055936098098755
Batch 42/64 loss: -0.28527015447616577
Batch 43/64 loss: -0.2845294177532196
Batch 44/64 loss: -0.28473472595214844
Batch 45/64 loss: -0.2863224148750305
Batch 46/64 loss: -0.2812415361404419
Batch 47/64 loss: -0.28517651557922363
Batch 48/64 loss: -0.2882680296897888
Batch 49/64 loss: -0.28291764855384827
Batch 50/64 loss: -0.2853577136993408
Batch 51/64 loss: -0.277190625667572
Batch 52/64 loss: -0.2643771171569824
Batch 53/64 loss: -0.28895941376686096
Batch 54/64 loss: -0.29468971490859985
Batch 55/64 loss: -0.27933213114738464
Batch 56/64 loss: -0.2919394373893738
Batch 57/64 loss: -0.29535406827926636
Batch 58/64 loss: -0.2817482054233551
Batch 59/64 loss: -0.29334956407546997
Batch 60/64 loss: -0.28176969289779663
Batch 61/64 loss: -0.2641078233718872
Batch 62/64 loss: -0.286590576171875
Batch 63/64 loss: -0.2815837562084198
Batch 64/64 loss: -0.27788829803466797
Epoch 280  Train loss: -0.28271036896051144  Val loss: -0.24478335990938535
Epoch 281
-------------------------------
Batch 1/64 loss: -0.28674694895744324
Batch 2/64 loss: -0.2829475998878479
Batch 3/64 loss: -0.29456979036331177
Batch 4/64 loss: -0.25943809747695923
Batch 5/64 loss: -0.28651756048202515
Batch 6/64 loss: -0.27839750051498413
Batch 7/64 loss: -0.2663101553916931
Batch 8/64 loss: -0.2828561067581177
Batch 9/64 loss: -0.28097689151763916
Batch 10/64 loss: -0.2862538993358612
Batch 11/64 loss: -0.28305917978286743
Batch 12/64 loss: -0.29393360018730164
Batch 13/64 loss: -0.2873110771179199
Batch 14/64 loss: -0.2934716045856476
Batch 15/64 loss: -0.28413715958595276
Batch 16/64 loss: -0.2765505909919739
Batch 17/64 loss: -0.2757686376571655
Batch 18/64 loss: -0.28623712062835693
Batch 19/64 loss: -0.2778128981590271
Batch 20/64 loss: -0.2702253460884094
Batch 21/64 loss: -0.27271246910095215
Batch 22/64 loss: -0.28949716687202454
Batch 23/64 loss: -0.28472471237182617
Batch 24/64 loss: -0.2728985548019409
Batch 25/64 loss: -0.28405267000198364
Batch 26/64 loss: -0.28585362434387207
Batch 27/64 loss: -0.2741256356239319
Batch 28/64 loss: -0.2761664390563965
Batch 29/64 loss: -0.2922743558883667
Batch 30/64 loss: -0.3007497191429138
Batch 31/64 loss: -0.28528526425361633
Batch 32/64 loss: -0.28316551446914673
Batch 33/64 loss: -0.275501012802124
Batch 34/64 loss: -0.26740163564682007
Batch 35/64 loss: -0.2834275960922241
Batch 36/64 loss: -0.27245163917541504
Batch 37/64 loss: -0.2720797061920166
Batch 38/64 loss: -0.27693018317222595
Batch 39/64 loss: -0.2834673225879669
Batch 40/64 loss: -0.2830578684806824
Batch 41/64 loss: -0.2866954207420349
Batch 42/64 loss: -0.26882606744766235
Batch 43/64 loss: -0.28148895502090454
Batch 44/64 loss: -0.28441429138183594
Batch 45/64 loss: -0.27116143703460693
Batch 46/64 loss: -0.27615970373153687
Batch 47/64 loss: -0.2826915979385376
Batch 48/64 loss: -0.2779528498649597
Batch 49/64 loss: -0.27718740701675415
Batch 50/64 loss: -0.2874104976654053
Batch 51/64 loss: -0.281740665435791
Batch 52/64 loss: -0.27622556686401367
Batch 53/64 loss: -0.289969801902771
Batch 54/64 loss: -0.2909659147262573
Batch 55/64 loss: -0.2836112678050995
Batch 56/64 loss: -0.2826513946056366
Batch 57/64 loss: -0.2957070469856262
Batch 58/64 loss: -0.2854483723640442
Batch 59/64 loss: -0.28662109375
Batch 60/64 loss: -0.2936754822731018
Batch 61/64 loss: -0.29189929366111755
Batch 62/64 loss: -0.2843669652938843
Batch 63/64 loss: -0.2856154143810272
Batch 64/64 loss: -0.2876932621002197
Epoch 281  Train loss: -0.2821584519217996  Val loss: -0.253972787525236
Epoch 282
-------------------------------
Batch 1/64 loss: -0.28066015243530273
Batch 2/64 loss: -0.29965099692344666
Batch 3/64 loss: -0.28250783681869507
Batch 4/64 loss: -0.27798667550086975
Batch 5/64 loss: -0.2843030095100403
Batch 6/64 loss: -0.2851395010948181
Batch 7/64 loss: -0.2795255780220032
Batch 8/64 loss: -0.2717991769313812
Batch 9/64 loss: -0.28580909967422485
Batch 10/64 loss: -0.27783626317977905
Batch 11/64 loss: -0.2879958748817444
Batch 12/64 loss: -0.2785405218601227
Batch 13/64 loss: -0.2728452682495117
Batch 14/64 loss: -0.29067644476890564
Batch 15/64 loss: -0.2855057120323181
Batch 16/64 loss: -0.2868461012840271
Batch 17/64 loss: -0.29007700085639954
Batch 18/64 loss: -0.28774434328079224
Batch 19/64 loss: -0.2946107089519501
Batch 20/64 loss: -0.30107054114341736
Batch 21/64 loss: -0.2823450267314911
Batch 22/64 loss: -0.27537280321121216
Batch 23/64 loss: -0.29598212242126465
Batch 24/64 loss: -0.27076417207717896
Batch 25/64 loss: -0.28650811314582825
Batch 26/64 loss: -0.28635168075561523
Batch 27/64 loss: -0.28337323665618896
Batch 28/64 loss: -0.2892709970474243
Batch 29/64 loss: -0.28439682722091675
Batch 30/64 loss: -0.2972375154495239
Batch 31/64 loss: -0.2925494909286499
Batch 32/64 loss: -0.2800953984260559
Batch 33/64 loss: -0.2894096374511719
Batch 34/64 loss: -0.2776038944721222
Batch 35/64 loss: -0.2796066403388977
Batch 36/64 loss: -0.2855876088142395
Batch 37/64 loss: -0.293769508600235
Batch 38/64 loss: -0.2823147475719452
Batch 39/64 loss: -0.2831498384475708
Batch 40/64 loss: -0.2707921862602234
Batch 41/64 loss: -0.28493499755859375
Batch 42/64 loss: -0.2797536253929138
Batch 43/64 loss: -0.28692036867141724
Batch 44/64 loss: -0.27448123693466187
Batch 45/64 loss: -0.29202163219451904
Batch 46/64 loss: -0.28688138723373413
Batch 47/64 loss: -0.27002274990081787
Batch 48/64 loss: -0.2913668751716614
Batch 49/64 loss: -0.2842565178871155
Batch 50/64 loss: -0.28512415289878845
Batch 51/64 loss: -0.27032023668289185
Batch 52/64 loss: -0.28166285157203674
Batch 53/64 loss: -0.274907648563385
Batch 54/64 loss: -0.27741363644599915
Batch 55/64 loss: -0.28717583417892456
Batch 56/64 loss: -0.2836834192276001
Batch 57/64 loss: -0.2648423910140991
Batch 58/64 loss: -0.27200400829315186
Batch 59/64 loss: -0.27965834736824036
Batch 60/64 loss: -0.2773839235305786
Batch 61/64 loss: -0.24754220247268677
Batch 62/64 loss: -0.2670891284942627
Batch 63/64 loss: -0.27450722455978394
Batch 64/64 loss: -0.27164027094841003
Epoch 282  Train loss: -0.282122303574693  Val loss: -0.2545482497854331
Epoch 283
-------------------------------
Batch 1/64 loss: -0.2751152217388153
Batch 2/64 loss: -0.28134387731552124
Batch 3/64 loss: -0.2777871787548065
Batch 4/64 loss: -0.28231197595596313
Batch 5/64 loss: -0.2795712649822235
Batch 6/64 loss: -0.2734828591346741
Batch 7/64 loss: -0.2689204216003418
Batch 8/64 loss: -0.2677127718925476
Batch 9/64 loss: -0.2817406952381134
Batch 10/64 loss: -0.2800849676132202
Batch 11/64 loss: -0.2685040831565857
Batch 12/64 loss: -0.27883607149124146
Batch 13/64 loss: -0.2803276479244232
Batch 14/64 loss: -0.28868651390075684
Batch 15/64 loss: -0.25748199224472046
Batch 16/64 loss: -0.28478914499282837
Batch 17/64 loss: -0.27924132347106934
Batch 18/64 loss: -0.261871874332428
Batch 19/64 loss: -0.26380449533462524
Batch 20/64 loss: -0.27794042229652405
Batch 21/64 loss: -0.29112759232521057
Batch 22/64 loss: -0.2909958064556122
Batch 23/64 loss: -0.27612000703811646
Batch 24/64 loss: -0.2756595015525818
Batch 25/64 loss: -0.28387516736984253
Batch 26/64 loss: -0.2741159200668335
Batch 27/64 loss: -0.2730642855167389
Batch 28/64 loss: -0.2722618579864502
Batch 29/64 loss: -0.2670903205871582
Batch 30/64 loss: -0.27969372272491455
Batch 31/64 loss: -0.26913273334503174
Batch 32/64 loss: -0.27288058400154114
Batch 33/64 loss: -0.2556132674217224
Batch 34/64 loss: -0.2760487198829651
Batch 35/64 loss: -0.2698490023612976
Batch 36/64 loss: -0.2768130898475647
Batch 37/64 loss: -0.27235960960388184
Batch 38/64 loss: -0.27973589301109314
Batch 39/64 loss: -0.24085968732833862
Batch 40/64 loss: -0.25916218757629395
Batch 41/64 loss: -0.2882155179977417
Batch 42/64 loss: -0.2769291400909424
Batch 43/64 loss: -0.28483468294143677
Batch 44/64 loss: -0.28633710741996765
Batch 45/64 loss: -0.2821004390716553
Batch 46/64 loss: -0.284781277179718
Batch 47/64 loss: -0.2887382209300995
Batch 48/64 loss: -0.28793126344680786
Batch 49/64 loss: -0.2826020121574402
Batch 50/64 loss: -0.28482410311698914
Batch 51/64 loss: -0.27874669432640076
Batch 52/64 loss: -0.2955586314201355
Batch 53/64 loss: -0.2880597710609436
Batch 54/64 loss: -0.2804950475692749
Batch 55/64 loss: -0.29281720519065857
Batch 56/64 loss: -0.26657843589782715
Batch 57/64 loss: -0.270222544670105
Batch 58/64 loss: -0.282206654548645
Batch 59/64 loss: -0.2938658595085144
Batch 60/64 loss: -0.30063557624816895
Batch 61/64 loss: -0.27410268783569336
Batch 62/64 loss: -0.2932344079017639
Batch 63/64 loss: -0.2933349907398224
Batch 64/64 loss: -0.27828311920166016
Epoch 283  Train loss: -0.27814711168700573  Val loss: -0.23174334023007004
Epoch 284
-------------------------------
Batch 1/64 loss: -0.27752685546875
Batch 2/64 loss: -0.2909504771232605
Batch 3/64 loss: -0.2762835621833801
Batch 4/64 loss: -0.2794676721096039
Batch 5/64 loss: -0.2779127061367035
Batch 6/64 loss: -0.2844998240470886
Batch 7/64 loss: -0.27852994203567505
Batch 8/64 loss: -0.2777937352657318
Batch 9/64 loss: -0.2744251787662506
Batch 10/64 loss: -0.2715091109275818
Batch 11/64 loss: -0.27135276794433594
Batch 12/64 loss: -0.2821182310581207
Batch 13/64 loss: -0.28445184230804443
Batch 14/64 loss: -0.2734399437904358
Batch 15/64 loss: -0.2800174951553345
Batch 16/64 loss: -0.2833789587020874
Batch 17/64 loss: -0.27369970083236694
Batch 18/64 loss: -0.2903285622596741
Batch 19/64 loss: -0.2946322560310364
Batch 20/64 loss: -0.2745320796966553
Batch 21/64 loss: -0.27464932203292847
Batch 22/64 loss: -0.27995389699935913
Batch 23/64 loss: -0.2813311219215393
Batch 24/64 loss: -0.28838545083999634
Batch 25/64 loss: -0.2694985866546631
Batch 26/64 loss: -0.2683720588684082
Batch 27/64 loss: -0.26792752742767334
Batch 28/64 loss: -0.26885080337524414
Batch 29/64 loss: -0.27210569381713867
Batch 30/64 loss: -0.2842349112033844
Batch 31/64 loss: -0.288621187210083
Batch 32/64 loss: -0.2673446536064148
Batch 33/64 loss: -0.29627639055252075
Batch 34/64 loss: -0.2864174246788025
Batch 35/64 loss: -0.28011584281921387
Batch 36/64 loss: -0.2894741892814636
Batch 37/64 loss: -0.27873122692108154
Batch 38/64 loss: -0.28013038635253906
Batch 39/64 loss: -0.28002798557281494
Batch 40/64 loss: -0.26430314779281616
Batch 41/64 loss: -0.2724868357181549
Batch 42/64 loss: -0.2974222004413605
Batch 43/64 loss: -0.2728828191757202
Batch 44/64 loss: -0.2766423225402832
Batch 45/64 loss: -0.2804974317550659
Batch 46/64 loss: -0.27572083473205566
Batch 47/64 loss: -0.2713816165924072
Batch 48/64 loss: -0.2626551389694214
Batch 49/64 loss: -0.2742648422718048
Batch 50/64 loss: -0.280108779668808
Batch 51/64 loss: -0.2857188582420349
Batch 52/64 loss: -0.26728594303131104
Batch 53/64 loss: -0.28259149193763733
Batch 54/64 loss: -0.2877160310745239
Batch 55/64 loss: -0.2848428189754486
Batch 56/64 loss: -0.2931825518608093
Batch 57/64 loss: -0.28457027673721313
Batch 58/64 loss: -0.28081685304641724
Batch 59/64 loss: -0.27891677618026733
Batch 60/64 loss: -0.2772085964679718
Batch 61/64 loss: -0.2735867500305176
Batch 62/64 loss: -0.26958751678466797
Batch 63/64 loss: -0.27012205123901367
Batch 64/64 loss: -0.2808229923248291
Epoch 284  Train loss: -0.27881457712136065  Val loss: -0.25494431466171424
Epoch 285
-------------------------------
Batch 1/64 loss: -0.2755599617958069
Batch 2/64 loss: -0.2935791611671448
Batch 3/64 loss: -0.27957722544670105
Batch 4/64 loss: -0.2882766127586365
Batch 5/64 loss: -0.28972098231315613
Batch 6/64 loss: -0.28309518098831177
Batch 7/64 loss: -0.289268434047699
Batch 8/64 loss: -0.2959514260292053
Batch 9/64 loss: -0.27895253896713257
Batch 10/64 loss: -0.28648674488067627
Batch 11/64 loss: -0.29722321033477783
Batch 12/64 loss: -0.2903517484664917
Batch 13/64 loss: -0.2858554422855377
Batch 14/64 loss: -0.2794691026210785
Batch 15/64 loss: -0.27182167768478394
Batch 16/64 loss: -0.27887678146362305
Batch 17/64 loss: -0.2661810517311096
Batch 18/64 loss: -0.2825654149055481
Batch 19/64 loss: -0.2828787565231323
Batch 20/64 loss: -0.27080315351486206
Batch 21/64 loss: -0.2866925597190857
Batch 22/64 loss: -0.27519690990448
Batch 23/64 loss: -0.2977825403213501
Batch 24/64 loss: -0.26061534881591797
Batch 25/64 loss: -0.2937011122703552
Batch 26/64 loss: -0.2861727774143219
Batch 27/64 loss: -0.2757778763771057
Batch 28/64 loss: -0.2746585011482239
Batch 29/64 loss: -0.274556040763855
Batch 30/64 loss: -0.2741481065750122
Batch 31/64 loss: -0.27570652961730957
Batch 32/64 loss: -0.28279584646224976
Batch 33/64 loss: -0.28078967332839966
Batch 34/64 loss: -0.2922740876674652
Batch 35/64 loss: -0.29157158732414246
Batch 36/64 loss: -0.27316001057624817
Batch 37/64 loss: -0.2747160494327545
Batch 38/64 loss: -0.2697693109512329
Batch 39/64 loss: -0.2838013470172882
Batch 40/64 loss: -0.2796386778354645
Batch 41/64 loss: -0.27053260803222656
Batch 42/64 loss: -0.27439290285110474
Batch 43/64 loss: -0.27862200140953064
Batch 44/64 loss: -0.27468550205230713
Batch 45/64 loss: -0.2758031189441681
Batch 46/64 loss: -0.281583309173584
Batch 47/64 loss: -0.30175769329071045
Batch 48/64 loss: -0.28076398372650146
Batch 49/64 loss: -0.27845048904418945
Batch 50/64 loss: -0.268878698348999
Batch 51/64 loss: -0.27853167057037354
Batch 52/64 loss: -0.27362316846847534
Batch 53/64 loss: -0.2840966284275055
Batch 54/64 loss: -0.28816771507263184
Batch 55/64 loss: -0.2800743877887726
Batch 56/64 loss: -0.2633863091468811
Batch 57/64 loss: -0.29543161392211914
Batch 58/64 loss: -0.2815937399864197
Batch 59/64 loss: -0.27650633454322815
Batch 60/64 loss: -0.26181095838546753
Batch 61/64 loss: -0.26632702350616455
Batch 62/64 loss: -0.28739017248153687
Batch 63/64 loss: -0.2773183584213257
Batch 64/64 loss: -0.26380467414855957
Epoch 285  Train loss: -0.28027610030828737  Val loss: -0.2539527434663674
Epoch 286
-------------------------------
Batch 1/64 loss: -0.29274362325668335
Batch 2/64 loss: -0.2888711988925934
Batch 3/64 loss: -0.27566084265708923
Batch 4/64 loss: -0.2892654538154602
Batch 5/64 loss: -0.274322509765625
Batch 6/64 loss: -0.2710453271865845
Batch 7/64 loss: -0.2737266719341278
Batch 8/64 loss: -0.2862987816333771
Batch 9/64 loss: -0.27421754598617554
Batch 10/64 loss: -0.2745581269264221
Batch 11/64 loss: -0.2855907082557678
Batch 12/64 loss: -0.29486364126205444
Batch 13/64 loss: -0.2896151542663574
Batch 14/64 loss: -0.29993534088134766
Batch 15/64 loss: -0.27593329548835754
Batch 16/64 loss: -0.2799583375453949
Batch 17/64 loss: -0.28214728832244873
Batch 18/64 loss: -0.27615681290626526
Batch 19/64 loss: -0.2769920229911804
Batch 20/64 loss: -0.2838484048843384
Batch 21/64 loss: -0.28853100538253784
Batch 22/64 loss: -0.281434565782547
Batch 23/64 loss: -0.3020973205566406
Batch 24/64 loss: -0.27700239419937134
Batch 25/64 loss: -0.27434444427490234
Batch 26/64 loss: -0.28292280435562134
Batch 27/64 loss: -0.2796548008918762
Batch 28/64 loss: -0.27764493227005005
Batch 29/64 loss: -0.2699824869632721
Batch 30/64 loss: -0.2700479030609131
Batch 31/64 loss: -0.2712801992893219
Batch 32/64 loss: -0.2762981653213501
Batch 33/64 loss: -0.2814429998397827
Batch 34/64 loss: -0.2724025845527649
Batch 35/64 loss: -0.2776164412498474
Batch 36/64 loss: -0.2708653211593628
Batch 37/64 loss: -0.2877296507358551
Batch 38/64 loss: -0.29200655221939087
Batch 39/64 loss: -0.27670323848724365
Batch 40/64 loss: -0.27924489974975586
Batch 41/64 loss: -0.28341615200042725
Batch 42/64 loss: -0.26849985122680664
Batch 43/64 loss: -0.2866339087486267
Batch 44/64 loss: -0.2612342834472656
Batch 45/64 loss: -0.2755841910839081
Batch 46/64 loss: -0.27985450625419617
Batch 47/64 loss: -0.2757580280303955
Batch 48/64 loss: -0.26378315687179565
Batch 49/64 loss: -0.2998928427696228
Batch 50/64 loss: -0.29965245723724365
Batch 51/64 loss: -0.25241488218307495
Batch 52/64 loss: -0.2738719582557678
Batch 53/64 loss: -0.2655060291290283
Batch 54/64 loss: -0.27244263887405396
Batch 55/64 loss: -0.27607420086860657
Batch 56/64 loss: -0.27128398418426514
Batch 57/64 loss: -0.2707751393318176
Batch 58/64 loss: -0.26807868480682373
Batch 59/64 loss: -0.2640218138694763
Batch 60/64 loss: -0.27542489767074585
Batch 61/64 loss: -0.2836306691169739
Batch 62/64 loss: -0.28534945845603943
Batch 63/64 loss: -0.28435564041137695
Batch 64/64 loss: -0.27399712800979614
Epoch 286  Train loss: -0.2788712316868352  Val loss: -0.26088538817114026
Epoch 287
-------------------------------
Batch 1/64 loss: -0.28442859649658203
Batch 2/64 loss: -0.2836300730705261
Batch 3/64 loss: -0.28342074155807495
Batch 4/64 loss: -0.2883458733558655
Batch 5/64 loss: -0.26983344554901123
Batch 6/64 loss: -0.2743237018585205
Batch 7/64 loss: -0.2879210114479065
Batch 8/64 loss: -0.2847343683242798
Batch 9/64 loss: -0.2820613980293274
Batch 10/64 loss: -0.28314998745918274
Batch 11/64 loss: -0.2744845151901245
Batch 12/64 loss: -0.28118783235549927
Batch 13/64 loss: -0.2909451723098755
Batch 14/64 loss: -0.2684670686721802
Batch 15/64 loss: -0.2726671099662781
Batch 16/64 loss: -0.2836233675479889
Batch 17/64 loss: -0.2880396544933319
Batch 18/64 loss: -0.2866899073123932
Batch 19/64 loss: -0.27187708020210266
Batch 20/64 loss: -0.29064837098121643
Batch 21/64 loss: -0.28094691038131714
Batch 22/64 loss: -0.2919451594352722
Batch 23/64 loss: -0.272342324256897
Batch 24/64 loss: -0.2853001356124878
Batch 25/64 loss: -0.2794892191886902
Batch 26/64 loss: -0.2824647128582001
Batch 27/64 loss: -0.2798618674278259
Batch 28/64 loss: -0.27197325229644775
Batch 29/64 loss: -0.2874528169631958
Batch 30/64 loss: -0.28589722514152527
Batch 31/64 loss: -0.2742670774459839
Batch 32/64 loss: -0.2802625000476837
Batch 33/64 loss: -0.2828682065010071
Batch 34/64 loss: -0.2752539813518524
Batch 35/64 loss: -0.2878645658493042
Batch 36/64 loss: -0.28434741497039795
Batch 37/64 loss: -0.27237018942832947
Batch 38/64 loss: -0.27724966406822205
Batch 39/64 loss: -0.2739727795124054
Batch 40/64 loss: -0.2706049680709839
Batch 41/64 loss: -0.2851887345314026
Batch 42/64 loss: -0.27656981348991394
Batch 43/64 loss: -0.2891414761543274
Batch 44/64 loss: -0.2812827229499817
Batch 45/64 loss: -0.2717117667198181
Batch 46/64 loss: -0.278283953666687
Batch 47/64 loss: -0.28789934515953064
Batch 48/64 loss: -0.2812137007713318
Batch 49/64 loss: -0.2876970171928406
Batch 50/64 loss: -0.2825290560722351
Batch 51/64 loss: -0.28360515832901
Batch 52/64 loss: -0.29349908232688904
Batch 53/64 loss: -0.2775931656360626
Batch 54/64 loss: -0.2730933427810669
Batch 55/64 loss: -0.2872045040130615
Batch 56/64 loss: -0.26835858821868896
Batch 57/64 loss: -0.2864455282688141
Batch 58/64 loss: -0.2686716318130493
Batch 59/64 loss: -0.2791613042354584
Batch 60/64 loss: -0.2845233082771301
Batch 61/64 loss: -0.2781660556793213
Batch 62/64 loss: -0.2639352083206177
Batch 63/64 loss: -0.27126795053482056
Batch 64/64 loss: -0.2717127799987793
Epoch 287  Train loss: -0.28028299995497163  Val loss: -0.24848025811906532
Epoch 288
-------------------------------
Batch 1/64 loss: -0.2873561382293701
Batch 2/64 loss: -0.2871108055114746
Batch 3/64 loss: -0.27538353204727173
Batch 4/64 loss: -0.28066810965538025
Batch 5/64 loss: -0.2827416956424713
Batch 6/64 loss: -0.28236863017082214
Batch 7/64 loss: -0.2830415666103363
Batch 8/64 loss: -0.29726213216781616
Batch 9/64 loss: -0.29060977697372437
Batch 10/64 loss: -0.2913587689399719
Batch 11/64 loss: -0.2784188687801361
Batch 12/64 loss: -0.28435570001602173
Batch 13/64 loss: -0.2813108563423157
Batch 14/64 loss: -0.2815133035182953
Batch 15/64 loss: -0.28663620352745056
Batch 16/64 loss: -0.29128164052963257
Batch 17/64 loss: -0.2875223159790039
Batch 18/64 loss: -0.2804184556007385
Batch 19/64 loss: -0.2799408733844757
Batch 20/64 loss: -0.2832302451133728
Batch 21/64 loss: -0.27206331491470337
Batch 22/64 loss: -0.27539506554603577
Batch 23/64 loss: -0.2703371047973633
Batch 24/64 loss: -0.286787211894989
Batch 25/64 loss: -0.286892831325531
Batch 26/64 loss: -0.2765933871269226
Batch 27/64 loss: -0.2889534831047058
Batch 28/64 loss: -0.28966495394706726
Batch 29/64 loss: -0.2829386293888092
Batch 30/64 loss: -0.2879146337509155
Batch 31/64 loss: -0.2755362391471863
Batch 32/64 loss: -0.289359986782074
Batch 33/64 loss: -0.2851344347000122
Batch 34/64 loss: -0.2889917492866516
Batch 35/64 loss: -0.286797434091568
Batch 36/64 loss: -0.2715601921081543
Batch 37/64 loss: -0.28138473629951477
Batch 38/64 loss: -0.28128528594970703
Batch 39/64 loss: -0.28536567091941833
Batch 40/64 loss: -0.2843317985534668
Batch 41/64 loss: -0.27676352858543396
Batch 42/64 loss: -0.2755509912967682
Batch 43/64 loss: -0.2654085159301758
Batch 44/64 loss: -0.2904144525527954
Batch 45/64 loss: -0.27122098207473755
Batch 46/64 loss: -0.28605586290359497
Batch 47/64 loss: -0.29317164421081543
Batch 48/64 loss: -0.28582221269607544
Batch 49/64 loss: -0.2907409965991974
Batch 50/64 loss: -0.28467774391174316
Batch 51/64 loss: -0.28329455852508545
Batch 52/64 loss: -0.29410451650619507
Batch 53/64 loss: -0.3006357252597809
Batch 54/64 loss: -0.2842564582824707
Batch 55/64 loss: -0.27988019585609436
Batch 56/64 loss: -0.28058212995529175
Batch 57/64 loss: -0.2810170650482178
Batch 58/64 loss: -0.2860081195831299
Batch 59/64 loss: -0.28911682963371277
Batch 60/64 loss: -0.28582388162612915
Batch 61/64 loss: -0.2996279001235962
Batch 62/64 loss: -0.29618406295776367
Batch 63/64 loss: -0.2857719361782074
Batch 64/64 loss: -0.2853044271469116
Epoch 288  Train loss: -0.2842341394985423  Val loss: -0.25233229656809386
Epoch 289
-------------------------------
Batch 1/64 loss: -0.28251779079437256
Batch 2/64 loss: -0.28096073865890503
Batch 3/64 loss: -0.2994348406791687
Batch 4/64 loss: -0.2742934226989746
Batch 5/64 loss: -0.2837398052215576
Batch 6/64 loss: -0.28866156935691833
Batch 7/64 loss: -0.2811700403690338
Batch 8/64 loss: -0.274240106344223
Batch 9/64 loss: -0.2885977029800415
Batch 10/64 loss: -0.2837778329849243
Batch 11/64 loss: -0.28346502780914307
Batch 12/64 loss: -0.29110294580459595
Batch 13/64 loss: -0.28627482056617737
Batch 14/64 loss: -0.2841261923313141
Batch 15/64 loss: -0.2754673361778259
Batch 16/64 loss: -0.2718684673309326
Batch 17/64 loss: -0.28478822112083435
Batch 18/64 loss: -0.28425729274749756
Batch 19/64 loss: -0.285309374332428
Batch 20/64 loss: -0.28008830547332764
Batch 21/64 loss: -0.2853003144264221
Batch 22/64 loss: -0.27711957693099976
Batch 23/64 loss: -0.28692349791526794
Batch 24/64 loss: -0.2882794141769409
Batch 25/64 loss: -0.2752287983894348
Batch 26/64 loss: -0.28546541929244995
Batch 27/64 loss: -0.27932417392730713
Batch 28/64 loss: -0.28201764822006226
Batch 29/64 loss: -0.27734702825546265
Batch 30/64 loss: -0.2876065671443939
Batch 31/64 loss: -0.28814780712127686
Batch 32/64 loss: -0.2816120386123657
Batch 33/64 loss: -0.2872617542743683
Batch 34/64 loss: -0.28256624937057495
Batch 35/64 loss: -0.28993040323257446
Batch 36/64 loss: -0.28641244769096375
Batch 37/64 loss: -0.2920740842819214
Batch 38/64 loss: -0.2864077091217041
Batch 39/64 loss: -0.28321829438209534
Batch 40/64 loss: -0.28871721029281616
Batch 41/64 loss: -0.27718183398246765
Batch 42/64 loss: -0.2776496410369873
Batch 43/64 loss: -0.2816612124443054
Batch 44/64 loss: -0.28351637721061707
Batch 45/64 loss: -0.27086806297302246
Batch 46/64 loss: -0.28451406955718994
Batch 47/64 loss: -0.26785075664520264
Batch 48/64 loss: -0.2821510136127472
Batch 49/64 loss: -0.2828191816806793
Batch 50/64 loss: -0.2950590252876282
Batch 51/64 loss: -0.28221502900123596
Batch 52/64 loss: -0.2842603921890259
Batch 53/64 loss: -0.2814762592315674
Batch 54/64 loss: -0.2857413589954376
Batch 55/64 loss: -0.2831098437309265
Batch 56/64 loss: -0.27653905749320984
Batch 57/64 loss: -0.26706892251968384
Batch 58/64 loss: -0.28445756435394287
Batch 59/64 loss: -0.28839462995529175
Batch 60/64 loss: -0.286929726600647
Batch 61/64 loss: -0.2832643389701843
Batch 62/64 loss: -0.27777186036109924
Batch 63/64 loss: -0.2641591429710388
Batch 64/64 loss: -0.2790345549583435
Epoch 289  Train loss: -0.2825574507900313  Val loss: -0.25665577621394414
Epoch 290
-------------------------------
Batch 1/64 loss: -0.2595440745353699
Batch 2/64 loss: -0.2511056065559387
Batch 3/64 loss: -0.257388174533844
Batch 4/64 loss: -0.27126508951187134
Batch 5/64 loss: -0.27394798398017883
Batch 6/64 loss: -0.29153481125831604
Batch 7/64 loss: -0.28338128328323364
Batch 8/64 loss: -0.2913222312927246
Batch 9/64 loss: -0.2878321409225464
Batch 10/64 loss: -0.29889458417892456
Batch 11/64 loss: -0.2787606418132782
Batch 12/64 loss: -0.2729816138744354
Batch 13/64 loss: -0.27347439527511597
Batch 14/64 loss: -0.28091245889663696
Batch 15/64 loss: -0.2735622525215149
Batch 16/64 loss: -0.2794538736343384
Batch 17/64 loss: -0.2635911703109741
Batch 18/64 loss: -0.2722063362598419
Batch 19/64 loss: -0.2794961929321289
Batch 20/64 loss: -0.26370227336883545
Batch 21/64 loss: -0.2715378701686859
Batch 22/64 loss: -0.2496854066848755
Batch 23/64 loss: -0.2710387706756592
Batch 24/64 loss: -0.2711308002471924
Batch 25/64 loss: -0.2700604796409607
Batch 26/64 loss: -0.27424076199531555
Batch 27/64 loss: -0.2741776704788208
Batch 28/64 loss: -0.2764138877391815
Batch 29/64 loss: -0.28112250566482544
Batch 30/64 loss: -0.25987744331359863
Batch 31/64 loss: -0.2668050527572632
Batch 32/64 loss: -0.272790789604187
Batch 33/64 loss: -0.2735600471496582
Batch 34/64 loss: -0.26202207803726196
Batch 35/64 loss: -0.26961368322372437
Batch 36/64 loss: -0.2744821012020111
Batch 37/64 loss: -0.2721399664878845
Batch 38/64 loss: -0.26672184467315674
Batch 39/64 loss: -0.24288630485534668
Batch 40/64 loss: -0.2769100069999695
Batch 41/64 loss: -0.26007044315338135
Batch 42/64 loss: -0.27176398038864136
Batch 43/64 loss: -0.2626236081123352
Batch 44/64 loss: -0.27233362197875977
Batch 45/64 loss: -0.26151204109191895
Batch 46/64 loss: -0.2735522985458374
Batch 47/64 loss: -0.25913161039352417
Batch 48/64 loss: -0.28243646025657654
Batch 49/64 loss: -0.27609783411026
Batch 50/64 loss: -0.27475231885910034
Batch 51/64 loss: -0.2534450888633728
Batch 52/64 loss: -0.27632302045822144
Batch 53/64 loss: -0.2734640836715698
Batch 54/64 loss: -0.25833427906036377
Batch 55/64 loss: -0.2697221040725708
Batch 56/64 loss: -0.2719295024871826
Batch 57/64 loss: -0.27444684505462646
Batch 58/64 loss: -0.2886728048324585
Batch 59/64 loss: -0.27878400683403015
Batch 60/64 loss: -0.27035027742385864
Batch 61/64 loss: -0.2697433829307556
Batch 62/64 loss: -0.2757015824317932
Batch 63/64 loss: -0.2664473056793213
Batch 64/64 loss: -0.2787618041038513
Epoch 290  Train loss: -0.27156524728326237  Val loss: -0.2396066012661072
Epoch 291
-------------------------------
Batch 1/64 loss: -0.2672620415687561
Batch 2/64 loss: -0.2733933627605438
Batch 3/64 loss: -0.28146636486053467
Batch 4/64 loss: -0.2648651599884033
Batch 5/64 loss: -0.2853175699710846
Batch 6/64 loss: -0.27392175793647766
Batch 7/64 loss: -0.281045138835907
Batch 8/64 loss: -0.2890421748161316
Batch 9/64 loss: -0.27535998821258545
Batch 10/64 loss: -0.2815183997154236
Batch 11/64 loss: -0.2760636806488037
Batch 12/64 loss: -0.27224957942962646
Batch 13/64 loss: -0.2699001729488373
Batch 14/64 loss: -0.27442511916160583
Batch 15/64 loss: -0.2751811444759369
Batch 16/64 loss: -0.28172534704208374
Batch 17/64 loss: -0.2636134624481201
Batch 18/64 loss: -0.2703480124473572
Batch 19/64 loss: -0.2898811101913452
Batch 20/64 loss: -0.2632575035095215
Batch 21/64 loss: -0.28477799892425537
Batch 22/64 loss: -0.2644723057746887
Batch 23/64 loss: -0.276035875082016
Batch 24/64 loss: -0.2752452790737152
Batch 25/64 loss: -0.25708281993865967
Batch 26/64 loss: -0.24765461683273315
Batch 27/64 loss: -0.26825976371765137
Batch 28/64 loss: -0.27389079332351685
Batch 29/64 loss: -0.24091219902038574
Batch 30/64 loss: -0.2718883156776428
Batch 31/64 loss: -0.27291133999824524
Batch 32/64 loss: -0.259970486164093
Batch 33/64 loss: -0.28572583198547363
Batch 34/64 loss: -0.2916707396507263
Batch 35/64 loss: -0.2710490822792053
Batch 36/64 loss: -0.27969446778297424
Batch 37/64 loss: -0.26239919662475586
Batch 38/64 loss: -0.2757120132446289
Batch 39/64 loss: -0.2715848982334137
Batch 40/64 loss: -0.2780998945236206
Batch 41/64 loss: -0.2632182836532593
Batch 42/64 loss: -0.2799767851829529
Batch 43/64 loss: -0.25804954767227173
Batch 44/64 loss: -0.27042466402053833
Batch 45/64 loss: -0.28406256437301636
Batch 46/64 loss: -0.27860790491104126
Batch 47/64 loss: -0.28161928057670593
Batch 48/64 loss: -0.2732795476913452
Batch 49/64 loss: -0.27929961681365967
Batch 50/64 loss: -0.2840961217880249
Batch 51/64 loss: -0.29115384817123413
Batch 52/64 loss: -0.2604830861091614
Batch 53/64 loss: -0.27180206775665283
Batch 54/64 loss: -0.28956329822540283
Batch 55/64 loss: -0.2792642116546631
Batch 56/64 loss: -0.2524033188819885
Batch 57/64 loss: -0.2892182767391205
Batch 58/64 loss: -0.2873684763908386
Batch 59/64 loss: -0.26602286100387573
Batch 60/64 loss: -0.264994740486145
Batch 61/64 loss: -0.29063111543655396
Batch 62/64 loss: -0.2761369049549103
Batch 63/64 loss: -0.2834727168083191
Batch 64/64 loss: -0.2726505994796753
Epoch 291  Train loss: -0.2741727403565949  Val loss: -0.2494310175430324
Epoch 292
-------------------------------
Batch 1/64 loss: -0.270842581987381
Batch 2/64 loss: -0.28210702538490295
Batch 3/64 loss: -0.28375643491744995
Batch 4/64 loss: -0.2843979001045227
Batch 5/64 loss: -0.28589800000190735
Batch 6/64 loss: -0.2682188153266907
Batch 7/64 loss: -0.28870660066604614
Batch 8/64 loss: -0.2834714353084564
Batch 9/64 loss: -0.28098607063293457
Batch 10/64 loss: -0.2832884192466736
Batch 11/64 loss: -0.27411067485809326
Batch 12/64 loss: -0.27174896001815796
Batch 13/64 loss: -0.26139843463897705
Batch 14/64 loss: -0.2882731854915619
Batch 15/64 loss: -0.28645581007003784
Batch 16/64 loss: -0.2653207778930664
Batch 17/64 loss: -0.2831017076969147
Batch 18/64 loss: -0.2908799648284912
Batch 19/64 loss: -0.2779446244239807
Batch 20/64 loss: -0.2746100425720215
Batch 21/64 loss: -0.2581784129142761
Batch 22/64 loss: -0.27673834562301636
Batch 23/64 loss: -0.2780383229255676
Batch 24/64 loss: -0.26532435417175293
Batch 25/64 loss: -0.2782464623451233
Batch 26/64 loss: -0.2725588083267212
Batch 27/64 loss: -0.2825300097465515
Batch 28/64 loss: -0.27996569871902466
Batch 29/64 loss: -0.277360737323761
Batch 30/64 loss: -0.27258366346359253
Batch 31/64 loss: -0.28348347544670105
Batch 32/64 loss: -0.2719174027442932
Batch 33/64 loss: -0.2913936972618103
Batch 34/64 loss: -0.27494847774505615
Batch 35/64 loss: -0.2704707980155945
Batch 36/64 loss: -0.27397218346595764
Batch 37/64 loss: -0.28725042939186096
Batch 38/64 loss: -0.27821123600006104
Batch 39/64 loss: -0.2724277675151825
Batch 40/64 loss: -0.28668466210365295
Batch 41/64 loss: -0.28242427110671997
Batch 42/64 loss: -0.2925744354724884
Batch 43/64 loss: -0.28663384914398193
Batch 44/64 loss: -0.2830581068992615
Batch 45/64 loss: -0.2783058285713196
Batch 46/64 loss: -0.2793794870376587
Batch 47/64 loss: -0.2796514928340912
Batch 48/64 loss: -0.27307671308517456
Batch 49/64 loss: -0.2896409034729004
Batch 50/64 loss: -0.29172658920288086
Batch 51/64 loss: -0.27940553426742554
Batch 52/64 loss: -0.2844330072402954
Batch 53/64 loss: -0.28797152638435364
Batch 54/64 loss: -0.2897037863731384
Batch 55/64 loss: -0.27379298210144043
Batch 56/64 loss: -0.2891279458999634
Batch 57/64 loss: -0.27496853470802307
Batch 58/64 loss: -0.2837761640548706
Batch 59/64 loss: -0.27974867820739746
Batch 60/64 loss: -0.2875465154647827
Batch 61/64 loss: -0.29591497778892517
Batch 62/64 loss: -0.2956032156944275
Batch 63/64 loss: -0.2942069172859192
Batch 64/64 loss: -0.2779320180416107
Epoch 292  Train loss: -0.28045369234739564  Val loss: -0.2578867067586106
Epoch 293
-------------------------------
Batch 1/64 loss: -0.29506704211235046
Batch 2/64 loss: -0.2873331606388092
Batch 3/64 loss: -0.2924209237098694
Batch 4/64 loss: -0.28904521465301514
Batch 5/64 loss: -0.27904820442199707
Batch 6/64 loss: -0.2905080020427704
Batch 7/64 loss: -0.2785717844963074
Batch 8/64 loss: -0.2638378143310547
Batch 9/64 loss: -0.2839863896369934
Batch 10/64 loss: -0.2915727198123932
Batch 11/64 loss: -0.28711697459220886
Batch 12/64 loss: -0.28378045558929443
Batch 13/64 loss: -0.2891462445259094
Batch 14/64 loss: -0.27676886320114136
Batch 15/64 loss: -0.2917526960372925
Batch 16/64 loss: -0.2907644212245941
Batch 17/64 loss: -0.279609739780426
Batch 18/64 loss: -0.26987865567207336
Batch 19/64 loss: -0.2774582505226135
Batch 20/64 loss: -0.28613904118537903
Batch 21/64 loss: -0.290737509727478
Batch 22/64 loss: -0.2606586217880249
Batch 23/64 loss: -0.2648661136627197
Batch 24/64 loss: -0.2809586822986603
Batch 25/64 loss: -0.27276402711868286
Batch 26/64 loss: -0.2877691984176636
Batch 27/64 loss: -0.2855771780014038
Batch 28/64 loss: -0.2905886173248291
Batch 29/64 loss: -0.286952942609787
Batch 30/64 loss: -0.2838935852050781
Batch 31/64 loss: -0.27876389026641846
Batch 32/64 loss: -0.2639427185058594
Batch 33/64 loss: -0.2872145175933838
Batch 34/64 loss: -0.27211129665374756
Batch 35/64 loss: -0.2572419047355652
Batch 36/64 loss: -0.28122541308403015
Batch 37/64 loss: -0.27868905663490295
Batch 38/64 loss: -0.2728177309036255
Batch 39/64 loss: -0.2769381105899811
Batch 40/64 loss: -0.27033019065856934
Batch 41/64 loss: -0.28610020875930786
Batch 42/64 loss: -0.292073518037796
Batch 43/64 loss: -0.29239386320114136
Batch 44/64 loss: -0.2889014780521393
Batch 45/64 loss: -0.2853999733924866
Batch 46/64 loss: -0.26726168394088745
Batch 47/64 loss: -0.2894192039966583
Batch 48/64 loss: -0.27825605869293213
Batch 49/64 loss: -0.2884373664855957
Batch 50/64 loss: -0.28698450326919556
Batch 51/64 loss: -0.2857501804828644
Batch 52/64 loss: -0.2787294387817383
Batch 53/64 loss: -0.2786121964454651
Batch 54/64 loss: -0.27827757596969604
Batch 55/64 loss: -0.26992660760879517
Batch 56/64 loss: -0.269625186920166
Batch 57/64 loss: -0.2736320495605469
Batch 58/64 loss: -0.28130903840065
Batch 59/64 loss: -0.2819409668445587
Batch 60/64 loss: -0.2785210609436035
Batch 61/64 loss: -0.2746391296386719
Batch 62/64 loss: -0.29390549659729004
Batch 63/64 loss: -0.2604837417602539
Batch 64/64 loss: -0.29037636518478394
Epoch 293  Train loss: -0.2808817365590264  Val loss: -0.2609893769742697
Epoch 294
-------------------------------
Batch 1/64 loss: -0.28369173407554626
Batch 2/64 loss: -0.28587663173675537
Batch 3/64 loss: -0.28909337520599365
Batch 4/64 loss: -0.28257691860198975
Batch 5/64 loss: -0.26870983839035034
Batch 6/64 loss: -0.2519094944000244
Batch 7/64 loss: -0.27951061725616455
Batch 8/64 loss: -0.2707669734954834
Batch 9/64 loss: -0.2826058864593506
Batch 10/64 loss: -0.28397494554519653
Batch 11/64 loss: -0.2865341901779175
Batch 12/64 loss: -0.28609031438827515
Batch 13/64 loss: -0.2716999053955078
Batch 14/64 loss: -0.27501267194747925
Batch 15/64 loss: -0.2994110584259033
Batch 16/64 loss: -0.2789878845214844
Batch 17/64 loss: -0.28794875741004944
Batch 18/64 loss: -0.26744401454925537
Batch 19/64 loss: -0.2760166823863983
Batch 20/64 loss: -0.28656166791915894
Batch 21/64 loss: -0.2832414507865906
Batch 22/64 loss: -0.2821997404098511
Batch 23/64 loss: -0.29118824005126953
Batch 24/64 loss: -0.28030744194984436
Batch 25/64 loss: -0.28253188729286194
Batch 26/64 loss: -0.2885977029800415
Batch 27/64 loss: -0.26702576875686646
Batch 28/64 loss: -0.27669021487236023
Batch 29/64 loss: -0.29286736249923706
Batch 30/64 loss: -0.2758352756500244
Batch 31/64 loss: -0.2850654423236847
Batch 32/64 loss: -0.2639332413673401
Batch 33/64 loss: -0.27510660886764526
Batch 34/64 loss: -0.26762205362319946
Batch 35/64 loss: -0.2803717851638794
Batch 36/64 loss: -0.2880972623825073
Batch 37/64 loss: -0.29842907190322876
Batch 38/64 loss: -0.2763776183128357
Batch 39/64 loss: -0.2745690941810608
Batch 40/64 loss: -0.29389524459838867
Batch 41/64 loss: -0.28796935081481934
Batch 42/64 loss: -0.2566758394241333
Batch 43/64 loss: -0.2888439893722534
Batch 44/64 loss: -0.2879343032836914
Batch 45/64 loss: -0.27940547466278076
Batch 46/64 loss: -0.2799798250198364
Batch 47/64 loss: -0.2795405983924866
Batch 48/64 loss: -0.28898361325263977
Batch 49/64 loss: -0.2871716022491455
Batch 50/64 loss: -0.2863054871559143
Batch 51/64 loss: -0.2805421054363251
Batch 52/64 loss: -0.2760540246963501
Batch 53/64 loss: -0.2822014093399048
Batch 54/64 loss: -0.27846023440361023
Batch 55/64 loss: -0.26421433687210083
Batch 56/64 loss: -0.27374976873397827
Batch 57/64 loss: -0.27906161546707153
Batch 58/64 loss: -0.2755284905433655
Batch 59/64 loss: -0.2740459144115448
Batch 60/64 loss: -0.2816411256790161
Batch 61/64 loss: -0.2732909917831421
Batch 62/64 loss: -0.2781619131565094
Batch 63/64 loss: -0.27701854705810547
Batch 64/64 loss: -0.26908278465270996
Epoch 294  Train loss: -0.2797955877640668  Val loss: -0.24092919003103197
Epoch 295
-------------------------------
Batch 1/64 loss: -0.2752510905265808
Batch 2/64 loss: -0.27526986598968506
Batch 3/64 loss: -0.27066922187805176
Batch 4/64 loss: -0.2808341383934021
Batch 5/64 loss: -0.2928800582885742
Batch 6/64 loss: -0.2872689962387085
Batch 7/64 loss: -0.2717297673225403
Batch 8/64 loss: -0.2847103476524353
Batch 9/64 loss: -0.2622615098953247
Batch 10/64 loss: -0.2604151964187622
Batch 11/64 loss: -0.27787935733795166
Batch 12/64 loss: -0.29214590787887573
Batch 13/64 loss: -0.28817933797836304
Batch 14/64 loss: -0.2716484069824219
Batch 15/64 loss: -0.2839598059654236
Batch 16/64 loss: -0.27844294905662537
Batch 17/64 loss: -0.2807154655456543
Batch 18/64 loss: -0.2807159423828125
Batch 19/64 loss: -0.25339770317077637
Batch 20/64 loss: -0.2840464115142822
Batch 21/64 loss: -0.2758750021457672
Batch 22/64 loss: -0.27558600902557373
Batch 23/64 loss: -0.2807978093624115
Batch 24/64 loss: -0.2562485933303833
Batch 25/64 loss: -0.28506484627723694
Batch 26/64 loss: -0.2629835605621338
Batch 27/64 loss: -0.2658984661102295
Batch 28/64 loss: -0.2740722894668579
Batch 29/64 loss: -0.29567041993141174
Batch 30/64 loss: -0.29465508460998535
Batch 31/64 loss: -0.2772994041442871
Batch 32/64 loss: -0.2910846173763275
Batch 33/64 loss: -0.28298184275627136
Batch 34/64 loss: -0.28480684757232666
Batch 35/64 loss: -0.26738160848617554
Batch 36/64 loss: -0.27577531337738037
Batch 37/64 loss: -0.27216440439224243
Batch 38/64 loss: -0.2945307493209839
Batch 39/64 loss: -0.28442949056625366
Batch 40/64 loss: -0.2781502902507782
Batch 41/64 loss: -0.26584213972091675
Batch 42/64 loss: -0.27994504570961
Batch 43/64 loss: -0.29847949743270874
Batch 44/64 loss: -0.26695895195007324
Batch 45/64 loss: -0.2832622528076172
Batch 46/64 loss: -0.27207672595977783
Batch 47/64 loss: -0.2820355296134949
Batch 48/64 loss: -0.2739225924015045
Batch 49/64 loss: -0.2757043242454529
Batch 50/64 loss: -0.2774387001991272
Batch 51/64 loss: -0.2628672122955322
Batch 52/64 loss: -0.2720147371292114
Batch 53/64 loss: -0.2851589322090149
Batch 54/64 loss: -0.28426259756088257
Batch 55/64 loss: -0.2800891697406769
Batch 56/64 loss: -0.27707117795944214
Batch 57/64 loss: -0.27641040086746216
Batch 58/64 loss: -0.271566778421402
Batch 59/64 loss: -0.2850256860256195
Batch 60/64 loss: -0.28460007905960083
Batch 61/64 loss: -0.2841985821723938
Batch 62/64 loss: -0.29419589042663574
Batch 63/64 loss: -0.27175527811050415
Batch 64/64 loss: -0.27278652787208557
Epoch 295  Train loss: -0.27823294599850973  Val loss: -0.2579815006747688
Epoch 296
-------------------------------
Batch 1/64 loss: -0.29022467136383057
Batch 2/64 loss: -0.2775799036026001
Batch 3/64 loss: -0.28884267807006836
Batch 4/64 loss: -0.27956870198249817
Batch 5/64 loss: -0.289194792509079
Batch 6/64 loss: -0.26772254705429077
Batch 7/64 loss: -0.2798878252506256
Batch 8/64 loss: -0.2805105447769165
Batch 9/64 loss: -0.28312572836875916
Batch 10/64 loss: -0.28285813331604004
Batch 11/64 loss: -0.2851405739784241
Batch 12/64 loss: -0.28911489248275757
Batch 13/64 loss: -0.2837795913219452
Batch 14/64 loss: -0.2861195504665375
Batch 15/64 loss: -0.28692692518234253
Batch 16/64 loss: -0.26846176385879517
Batch 17/64 loss: -0.2815018594264984
Batch 18/64 loss: -0.2844724655151367
Batch 19/64 loss: -0.27884599566459656
Batch 20/64 loss: -0.2870919704437256
Batch 21/64 loss: -0.27830567955970764
Batch 22/64 loss: -0.27942991256713867
Batch 23/64 loss: -0.2849852442741394
Batch 24/64 loss: -0.2821750044822693
Batch 25/64 loss: -0.2804640829563141
Batch 26/64 loss: -0.2752249538898468
Batch 27/64 loss: -0.25775402784347534
Batch 28/64 loss: -0.2784305810928345
Batch 29/64 loss: -0.28963226079940796
Batch 30/64 loss: -0.27575039863586426
Batch 31/64 loss: -0.27072685956954956
Batch 32/64 loss: -0.2881616950035095
Batch 33/64 loss: -0.29319581389427185
Batch 34/64 loss: -0.2821125388145447
Batch 35/64 loss: -0.27338388562202454
Batch 36/64 loss: -0.2671588063240051
Batch 37/64 loss: -0.2774537205696106
Batch 38/64 loss: -0.28200477361679077
Batch 39/64 loss: -0.27905532717704773
Batch 40/64 loss: -0.27129849791526794
Batch 41/64 loss: -0.2790011763572693
Batch 42/64 loss: -0.25903594493865967
Batch 43/64 loss: -0.2825477719306946
Batch 44/64 loss: -0.2828329801559448
Batch 45/64 loss: -0.2626059651374817
Batch 46/64 loss: -0.2595316767692566
Batch 47/64 loss: -0.27338752150535583
Batch 48/64 loss: -0.27724602818489075
Batch 49/64 loss: -0.2761644721031189
Batch 50/64 loss: -0.27732348442077637
Batch 51/64 loss: -0.2771090865135193
Batch 52/64 loss: -0.2705146074295044
Batch 53/64 loss: -0.2846064567565918
Batch 54/64 loss: -0.28910183906555176
Batch 55/64 loss: -0.2842162251472473
Batch 56/64 loss: -0.2750106453895569
Batch 57/64 loss: -0.2695692479610443
Batch 58/64 loss: -0.27080589532852173
Batch 59/64 loss: -0.27012649178504944
Batch 60/64 loss: -0.28452032804489136
Batch 61/64 loss: -0.28014013171195984
Batch 62/64 loss: -0.26746392250061035
Batch 63/64 loss: -0.27988049387931824
Batch 64/64 loss: -0.27423250675201416
Epoch 296  Train loss: -0.2785269325854732  Val loss: -0.24907193970434444
Epoch 297
-------------------------------
Batch 1/64 loss: -0.2719734311103821
Batch 2/64 loss: -0.2954614460468292
Batch 3/64 loss: -0.27317434549331665
Batch 4/64 loss: -0.28451377153396606
Batch 5/64 loss: -0.2721872627735138
Batch 6/64 loss: -0.2816673517227173
Batch 7/64 loss: -0.2882757782936096
Batch 8/64 loss: -0.27213114500045776
Batch 9/64 loss: -0.2740407884120941
Batch 10/64 loss: -0.2795135974884033
Batch 11/64 loss: -0.2822858691215515
Batch 12/64 loss: -0.27647316455841064
Batch 13/64 loss: -0.28218430280685425
Batch 14/64 loss: -0.28784239292144775
Batch 15/64 loss: -0.2755381464958191
Batch 16/64 loss: -0.2853410243988037
Batch 17/64 loss: -0.2744726538658142
Batch 18/64 loss: -0.27810412645339966
Batch 19/64 loss: -0.27432525157928467
Batch 20/64 loss: -0.27410030364990234
Batch 21/64 loss: -0.28830260038375854
Batch 22/64 loss: -0.29586368799209595
Batch 23/64 loss: -0.2832159996032715
Batch 24/64 loss: -0.27782392501831055
Batch 25/64 loss: -0.29416197538375854
Batch 26/64 loss: -0.2711407244205475
Batch 27/64 loss: -0.25821638107299805
Batch 28/64 loss: -0.2798443138599396
Batch 29/64 loss: -0.2922632098197937
Batch 30/64 loss: -0.2891075015068054
Batch 31/64 loss: -0.28729891777038574
Batch 32/64 loss: -0.2856489419937134
Batch 33/64 loss: -0.2893446683883667
Batch 34/64 loss: -0.2857614755630493
Batch 35/64 loss: -0.2996329963207245
Batch 36/64 loss: -0.27706632018089294
Batch 37/64 loss: -0.28723353147506714
Batch 38/64 loss: -0.2826530933380127
Batch 39/64 loss: -0.2821589708328247
Batch 40/64 loss: -0.2889482080936432
Batch 41/64 loss: -0.29377782344818115
Batch 42/64 loss: -0.2934810221195221
Batch 43/64 loss: -0.28015145659446716
Batch 44/64 loss: -0.28138041496276855
Batch 45/64 loss: -0.2690006494522095
Batch 46/64 loss: -0.2940281629562378
Batch 47/64 loss: -0.28803014755249023
Batch 48/64 loss: -0.282279372215271
Batch 49/64 loss: -0.27707502245903015
Batch 50/64 loss: -0.28832149505615234
Batch 51/64 loss: -0.29003363847732544
Batch 52/64 loss: -0.28633803129196167
Batch 53/64 loss: -0.29059240221977234
Batch 54/64 loss: -0.2814283072948456
Batch 55/64 loss: -0.27129003405570984
Batch 56/64 loss: -0.2908596396446228
Batch 57/64 loss: -0.2785998284816742
Batch 58/64 loss: -0.2815256416797638
Batch 59/64 loss: -0.27676886320114136
Batch 60/64 loss: -0.2908257842063904
Batch 61/64 loss: -0.2839345633983612
Batch 62/64 loss: -0.2851722538471222
Batch 63/64 loss: -0.2863296568393707
Batch 64/64 loss: -0.28465819358825684
Epoch 297  Train loss: -0.2828864541708254  Val loss: -0.255746027448333
Epoch 298
-------------------------------
Batch 1/64 loss: -0.27277231216430664
Batch 2/64 loss: -0.2790544033050537
Batch 3/64 loss: -0.281657338142395
Batch 4/64 loss: -0.2830166816711426
Batch 5/64 loss: -0.27638208866119385
Batch 6/64 loss: -0.285979300737381
Batch 7/64 loss: -0.29617929458618164
Batch 8/64 loss: -0.2794615626335144
Batch 9/64 loss: -0.28373029828071594
Batch 10/64 loss: -0.27580416202545166
Batch 11/64 loss: -0.2828529179096222
Batch 12/64 loss: -0.2908264994621277
Batch 13/64 loss: -0.2930307984352112
Batch 14/64 loss: -0.29643625020980835
Batch 15/64 loss: -0.2875794768333435
Batch 16/64 loss: -0.27613914012908936
Batch 17/64 loss: -0.2894548177719116
Batch 18/64 loss: -0.2731654942035675
Batch 19/64 loss: -0.2819633483886719
Batch 20/64 loss: -0.2875041961669922
Batch 21/64 loss: -0.2816723585128784
Batch 22/64 loss: -0.2776724398136139
Batch 23/64 loss: -0.28609129786491394
Batch 24/64 loss: -0.2718345522880554
Batch 25/64 loss: -0.2682013511657715
Batch 26/64 loss: -0.2863101065158844
Batch 27/64 loss: -0.28687193989753723
Batch 28/64 loss: -0.28902575373649597
Batch 29/64 loss: -0.28617119789123535
Batch 30/64 loss: -0.2760973572731018
Batch 31/64 loss: -0.29227858781814575
Batch 32/64 loss: -0.2853817641735077
Batch 33/64 loss: -0.2890886664390564
Batch 34/64 loss: -0.28893184661865234
Batch 35/64 loss: -0.2795518636703491
Batch 36/64 loss: -0.28967535495758057
Batch 37/64 loss: -0.29322880506515503
Batch 38/64 loss: -0.28512680530548096
Batch 39/64 loss: -0.2808438837528229
Batch 40/64 loss: -0.2760280966758728
Batch 41/64 loss: -0.2741316854953766
Batch 42/64 loss: -0.2920997738838196
Batch 43/64 loss: -0.2862035036087036
Batch 44/64 loss: -0.28618937730789185
Batch 45/64 loss: -0.2893698215484619
Batch 46/64 loss: -0.26566797494888306
Batch 47/64 loss: -0.2813798785209656
Batch 48/64 loss: -0.2745799720287323
Batch 49/64 loss: -0.27838802337646484
Batch 50/64 loss: -0.2734391391277313
Batch 51/64 loss: -0.275992751121521
Batch 52/64 loss: -0.2855342924594879
Batch 53/64 loss: -0.2833949029445648
Batch 54/64 loss: -0.270657479763031
Batch 55/64 loss: -0.2827587425708771
Batch 56/64 loss: -0.2867603003978729
Batch 57/64 loss: -0.2757389545440674
Batch 58/64 loss: -0.2806682586669922
Batch 59/64 loss: -0.27949461340904236
Batch 60/64 loss: -0.278644859790802
Batch 61/64 loss: -0.28480035066604614
Batch 62/64 loss: -0.2873997092247009
Batch 63/64 loss: -0.28609076142311096
Batch 64/64 loss: -0.2893372178077698
Epoch 298  Train loss: -0.2826582345308042  Val loss: -0.25962891529515847
Epoch 299
-------------------------------
Batch 1/64 loss: -0.27512630820274353
Batch 2/64 loss: -0.27220505475997925
Batch 3/64 loss: -0.2671346068382263
Batch 4/64 loss: -0.2867180407047272
Batch 5/64 loss: -0.28208673000335693
Batch 6/64 loss: -0.2835099697113037
Batch 7/64 loss: -0.26343750953674316
Batch 8/64 loss: -0.2739347517490387
Batch 9/64 loss: -0.2768345773220062
Batch 10/64 loss: -0.2836056351661682
Batch 11/64 loss: -0.2888822555541992
Batch 12/64 loss: -0.28721535205841064
Batch 13/64 loss: -0.29258742928504944
Batch 14/64 loss: -0.27847373485565186
Batch 15/64 loss: -0.26440680027008057
Batch 16/64 loss: -0.2785956561565399
Batch 17/64 loss: -0.2550565004348755
Batch 18/64 loss: -0.2771514654159546
Batch 19/64 loss: -0.2725229859352112
Batch 20/64 loss: -0.26959025859832764
Batch 21/64 loss: -0.26405251026153564
Batch 22/64 loss: -0.2694571912288666
Batch 23/64 loss: -0.2704469561576843
Batch 24/64 loss: -0.272475004196167
Batch 25/64 loss: -0.2799827456474304
Batch 26/64 loss: -0.27546465396881104
Batch 27/64 loss: -0.2721852958202362
Batch 28/64 loss: -0.25595974922180176
Batch 29/64 loss: -0.28669828176498413
Batch 30/64 loss: -0.286562442779541
Batch 31/64 loss: -0.2922964096069336
Batch 32/64 loss: -0.2868938446044922
Batch 33/64 loss: -0.2795712947845459
Batch 34/64 loss: -0.2591879367828369
Batch 35/64 loss: -0.285245418548584
Batch 36/64 loss: -0.2651243805885315
Batch 37/64 loss: -0.2738224267959595
Batch 38/64 loss: -0.28140440583229065
Batch 39/64 loss: -0.2840380072593689
Batch 40/64 loss: -0.28182822465896606
Batch 41/64 loss: -0.28109222650527954
Batch 42/64 loss: -0.29677605628967285
Batch 43/64 loss: -0.2969268262386322
Batch 44/64 loss: -0.28374889492988586
Batch 45/64 loss: -0.2703893184661865
Batch 46/64 loss: -0.2692345380783081
Batch 47/64 loss: -0.2683291435241699
Batch 48/64 loss: -0.26972925662994385
Batch 49/64 loss: -0.2659381628036499
Batch 50/64 loss: -0.2791469991207123
Batch 51/64 loss: -0.2795755863189697
Batch 52/64 loss: -0.2748783826828003
Batch 53/64 loss: -0.28932178020477295
Batch 54/64 loss: -0.28094273805618286
Batch 55/64 loss: -0.28545522689819336
Batch 56/64 loss: -0.26982712745666504
Batch 57/64 loss: -0.283973753452301
Batch 58/64 loss: -0.2884783446788788
Batch 59/64 loss: -0.2768847942352295
Batch 60/64 loss: -0.28532442450523376
Batch 61/64 loss: -0.2932698428630829
Batch 62/64 loss: -0.26868128776550293
Batch 63/64 loss: -0.2837623357772827
Batch 64/64 loss: -0.2805832624435425
Epoch 299  Train loss: -0.2777081929001154  Val loss: -0.24849575327843734
Epoch 300
-------------------------------
Batch 1/64 loss: -0.2773211598396301
Batch 2/64 loss: -0.2869766354560852
Batch 3/64 loss: -0.2846704125404358
Batch 4/64 loss: -0.2909432053565979
Batch 5/64 loss: -0.27718064188957214
Batch 6/64 loss: -0.2858833074569702
Batch 7/64 loss: -0.2951904535293579
Batch 8/64 loss: -0.27108073234558105
Batch 9/64 loss: -0.27427536249160767
Batch 10/64 loss: -0.28185105323791504
Batch 11/64 loss: -0.2816285192966461
Batch 12/64 loss: -0.28172969818115234
Batch 13/64 loss: -0.29895955324172974
Batch 14/64 loss: -0.2901645004749298
Batch 15/64 loss: -0.28358331322669983
Batch 16/64 loss: -0.2781546711921692
Batch 17/64 loss: -0.29348433017730713
Batch 18/64 loss: -0.2827479839324951
Batch 19/64 loss: -0.2876927852630615
Batch 20/64 loss: -0.27440983057022095
Batch 21/64 loss: -0.29648399353027344
Batch 22/64 loss: -0.2720702290534973
Batch 23/64 loss: -0.27525728940963745
Batch 24/64 loss: -0.26375651359558105
Batch 25/64 loss: -0.29025977849960327
Batch 26/64 loss: -0.2828837037086487
Batch 27/64 loss: -0.2855762243270874
Batch 28/64 loss: -0.2949465215206146
Batch 29/64 loss: -0.28931427001953125
Batch 30/64 loss: -0.28553998470306396
Batch 31/64 loss: -0.2810876965522766
Batch 32/64 loss: -0.2774595618247986
Batch 33/64 loss: -0.2858090400695801
Batch 34/64 loss: -0.2964407801628113
Batch 35/64 loss: -0.2821213901042938
Batch 36/64 loss: -0.29613885283470154
Batch 37/64 loss: -0.27576616406440735
Batch 38/64 loss: -0.2885957360267639
Batch 39/64 loss: -0.2875582277774811
Batch 40/64 loss: -0.2734214663505554
Batch 41/64 loss: -0.28878194093704224
Batch 42/64 loss: -0.3016294836997986
Batch 43/64 loss: -0.2881476879119873
Batch 44/64 loss: -0.2883296310901642
Batch 45/64 loss: -0.2883201539516449
Batch 46/64 loss: -0.2892189621925354
Batch 47/64 loss: -0.28077951073646545
Batch 48/64 loss: -0.27226686477661133
Batch 49/64 loss: -0.28408682346343994
Batch 50/64 loss: -0.2978477478027344
Batch 51/64 loss: -0.2953382730484009
Batch 52/64 loss: -0.2794469892978668
Batch 53/64 loss: -0.28960275650024414
Batch 54/64 loss: -0.2771308422088623
Batch 55/64 loss: -0.2838548421859741
Batch 56/64 loss: -0.2786822021007538
Batch 57/64 loss: -0.2740709185600281
Batch 58/64 loss: -0.28503236174583435
Batch 59/64 loss: -0.2760297954082489
Batch 60/64 loss: -0.2848568558692932
Batch 61/64 loss: -0.2682720422744751
Batch 62/64 loss: -0.2801092565059662
Batch 63/64 loss: -0.2764436602592468
Batch 64/64 loss: -0.2682730555534363
Epoch 300  Train loss: -0.28373176420436186  Val loss: -0.2645793406619239
Epoch 301
-------------------------------
Batch 1/64 loss: -0.2850751280784607
Batch 2/64 loss: -0.2840348482131958
Batch 3/64 loss: -0.2879856824874878
Batch 4/64 loss: -0.29251301288604736
Batch 5/64 loss: -0.2899780571460724
Batch 6/64 loss: -0.29446977376937866
Batch 7/64 loss: -0.2860541045665741
Batch 8/64 loss: -0.2836609482765198
Batch 9/64 loss: -0.2896120846271515
Batch 10/64 loss: -0.27987807989120483
Batch 11/64 loss: -0.28865647315979004
Batch 12/64 loss: -0.2909158170223236
Batch 13/64 loss: -0.2820923626422882
Batch 14/64 loss: -0.29254966974258423
Batch 15/64 loss: -0.29564613103866577
Batch 16/64 loss: -0.284769743680954
Batch 17/64 loss: -0.28288733959198
Batch 18/64 loss: -0.2841297388076782
Batch 19/64 loss: -0.2767631411552429
Batch 20/64 loss: -0.2750650644302368
Batch 21/64 loss: -0.29366326332092285
Batch 22/64 loss: -0.2939358949661255
Batch 23/64 loss: -0.27492284774780273
Batch 24/64 loss: -0.2864774465560913
Batch 25/64 loss: -0.2870979905128479
Batch 26/64 loss: -0.2817201614379883
Batch 27/64 loss: -0.28192153573036194
Batch 28/64 loss: -0.2927626371383667
Batch 29/64 loss: -0.29062619805336
Batch 30/64 loss: -0.2723875641822815
Batch 31/64 loss: -0.2726525664329529
Batch 32/64 loss: -0.2749135196208954
Batch 33/64 loss: -0.27479273080825806
Batch 34/64 loss: -0.2713015377521515
Batch 35/64 loss: -0.27526527643203735
Batch 36/64 loss: -0.28485915064811707
Batch 37/64 loss: -0.28345203399658203
Batch 38/64 loss: -0.2810315787792206
Batch 39/64 loss: -0.28664499521255493
Batch 40/64 loss: -0.2966219186782837
Batch 41/64 loss: -0.28598690032958984
Batch 42/64 loss: -0.281144917011261
Batch 43/64 loss: -0.28050628304481506
Batch 44/64 loss: -0.2989340126514435
Batch 45/64 loss: -0.2747006118297577
Batch 46/64 loss: -0.28686219453811646
Batch 47/64 loss: -0.2852230370044708
Batch 48/64 loss: -0.27791398763656616
Batch 49/64 loss: -0.284702330827713
Batch 50/64 loss: -0.28921693563461304
Batch 51/64 loss: -0.2759227752685547
Batch 52/64 loss: -0.27004295587539673
Batch 53/64 loss: -0.2870544493198395
Batch 54/64 loss: -0.27662330865859985
Batch 55/64 loss: -0.28762996196746826
Batch 56/64 loss: -0.27750760316848755
Batch 57/64 loss: -0.27313828468322754
Batch 58/64 loss: -0.2688049077987671
Batch 59/64 loss: -0.27816739678382874
Batch 60/64 loss: -0.28379279375076294
Batch 61/64 loss: -0.28463104367256165
Batch 62/64 loss: -0.2805024981498718
Batch 63/64 loss: -0.27456921339035034
Batch 64/64 loss: -0.2993898391723633
Epoch 301  Train loss: -0.28348092284857057  Val loss: -0.25416365255604906
Epoch 302
-------------------------------
Batch 1/64 loss: -0.2962682247161865
Batch 2/64 loss: -0.28067004680633545
Batch 3/64 loss: -0.2928614616394043
Batch 4/64 loss: -0.29329776763916016
Batch 5/64 loss: -0.26154762506484985
Batch 6/64 loss: -0.2665173411369324
Batch 7/64 loss: -0.28581446409225464
Batch 8/64 loss: -0.2895316481590271
Batch 9/64 loss: -0.2842565178871155
Batch 10/64 loss: -0.2932227551937103
Batch 11/64 loss: -0.2702644467353821
Batch 12/64 loss: -0.2774922847747803
Batch 13/64 loss: -0.2818324565887451
Batch 14/64 loss: -0.2945789098739624
Batch 15/64 loss: -0.2930845022201538
Batch 16/64 loss: -0.2921213209629059
Batch 17/64 loss: -0.2955002784729004
Batch 18/64 loss: -0.300742506980896
Batch 19/64 loss: -0.2920702397823334
Batch 20/64 loss: -0.2785184681415558
Batch 21/64 loss: -0.27363258600234985
Batch 22/64 loss: -0.28977417945861816
Batch 23/64 loss: -0.2858608365058899
Batch 24/64 loss: -0.27748581767082214
Batch 25/64 loss: -0.28815698623657227
Batch 26/64 loss: -0.29122453927993774
Batch 27/64 loss: -0.2839478850364685
Batch 28/64 loss: -0.2887755036354065
Batch 29/64 loss: -0.26312941312789917
Batch 30/64 loss: -0.2757718861103058
Batch 31/64 loss: -0.27839532494544983
Batch 32/64 loss: -0.29827070236206055
Batch 33/64 loss: -0.2884144186973572
Batch 34/64 loss: -0.2875033915042877
Batch 35/64 loss: -0.2848677635192871
Batch 36/64 loss: -0.2670218348503113
Batch 37/64 loss: -0.28047531843185425
Batch 38/64 loss: -0.29341596364974976
Batch 39/64 loss: -0.2775122821331024
Batch 40/64 loss: -0.2757957875728607
Batch 41/64 loss: -0.28428012132644653
Batch 42/64 loss: -0.28755998611450195
Batch 43/64 loss: -0.292365700006485
Batch 44/64 loss: -0.2813166081905365
Batch 45/64 loss: -0.29001301527023315
Batch 46/64 loss: -0.2897862195968628
Batch 47/64 loss: -0.275732159614563
Batch 48/64 loss: -0.2692275047302246
Batch 49/64 loss: -0.2905159592628479
Batch 50/64 loss: -0.28738903999328613
Batch 51/64 loss: -0.27843305468559265
Batch 52/64 loss: -0.2780798673629761
Batch 53/64 loss: -0.28216254711151123
Batch 54/64 loss: -0.268854558467865
Batch 55/64 loss: -0.2843891978263855
Batch 56/64 loss: -0.29523998498916626
Batch 57/64 loss: -0.26892194151878357
Batch 58/64 loss: -0.29031115770339966
Batch 59/64 loss: -0.2782173156738281
Batch 60/64 loss: -0.2845836281776428
Batch 61/64 loss: -0.2849137783050537
Batch 62/64 loss: -0.2815515398979187
Batch 63/64 loss: -0.283855676651001
Batch 64/64 loss: -0.27663367986679077
Epoch 302  Train loss: -0.2836831609408061  Val loss: -0.2699266434535128
Saving best model, epoch: 302
Epoch 303
-------------------------------
Batch 1/64 loss: -0.29367321729660034
Batch 2/64 loss: -0.29320743680000305
Batch 3/64 loss: -0.27515965700149536
Batch 4/64 loss: -0.27642983198165894
Batch 5/64 loss: -0.28302809596061707
Batch 6/64 loss: -0.2833916246891022
Batch 7/64 loss: -0.29283952713012695
Batch 8/64 loss: -0.2934008538722992
Batch 9/64 loss: -0.29836153984069824
Batch 10/64 loss: -0.290922075510025
Batch 11/64 loss: -0.2910064458847046
Batch 12/64 loss: -0.284035861492157
Batch 13/64 loss: -0.2815932631492615
Batch 14/64 loss: -0.28181928396224976
Batch 15/64 loss: -0.2950798571109772
Batch 16/64 loss: -0.2851577401161194
Batch 17/64 loss: -0.2846779227256775
Batch 18/64 loss: -0.2764825224876404
Batch 19/64 loss: -0.2801132798194885
Batch 20/64 loss: -0.2831859290599823
Batch 21/64 loss: -0.2853304147720337
Batch 22/64 loss: -0.29280924797058105
Batch 23/64 loss: -0.2875640392303467
Batch 24/64 loss: -0.2817458212375641
Batch 25/64 loss: -0.29453742504119873
Batch 26/64 loss: -0.27569258213043213
Batch 27/64 loss: -0.2768379747867584
Batch 28/64 loss: -0.2737504839897156
Batch 29/64 loss: -0.2743191123008728
Batch 30/64 loss: -0.28742265701293945
Batch 31/64 loss: -0.2858384847640991
Batch 32/64 loss: -0.2913115918636322
Batch 33/64 loss: -0.2821953296661377
Batch 34/64 loss: -0.2803446054458618
Batch 35/64 loss: -0.28861480951309204
Batch 36/64 loss: -0.27818700671195984
Batch 37/64 loss: -0.2883031368255615
Batch 38/64 loss: -0.28965842723846436
Batch 39/64 loss: -0.2748985290527344
Batch 40/64 loss: -0.2920474708080292
Batch 41/64 loss: -0.2830278277397156
Batch 42/64 loss: -0.29250913858413696
Batch 43/64 loss: -0.2958386540412903
Batch 44/64 loss: -0.28523391485214233
Batch 45/64 loss: -0.2696748971939087
Batch 46/64 loss: -0.28073370456695557
Batch 47/64 loss: -0.30248939990997314
Batch 48/64 loss: -0.2805464267730713
Batch 49/64 loss: -0.29213082790374756
Batch 50/64 loss: -0.291426420211792
Batch 51/64 loss: -0.3015025854110718
Batch 52/64 loss: -0.28818607330322266
Batch 53/64 loss: -0.2810179889202118
Batch 54/64 loss: -0.2832767963409424
Batch 55/64 loss: -0.27806538343429565
Batch 56/64 loss: -0.282442569732666
Batch 57/64 loss: -0.2828882336616516
Batch 58/64 loss: -0.29301413893699646
Batch 59/64 loss: -0.3021642565727234
Batch 60/64 loss: -0.2824886441230774
Batch 61/64 loss: -0.28975731134414673
Batch 62/64 loss: -0.2998921275138855
Batch 63/64 loss: -0.28508931398391724
Batch 64/64 loss: -0.290138304233551
Epoch 303  Train loss: -0.28621138007033103  Val loss: -0.2600424045959289
Epoch 304
-------------------------------
Batch 1/64 loss: -0.28628191351890564
Batch 2/64 loss: -0.2913387417793274
Batch 3/64 loss: -0.28391921520233154
Batch 4/64 loss: -0.28521907329559326
Batch 5/64 loss: -0.28528597950935364
Batch 6/64 loss: -0.2845851182937622
Batch 7/64 loss: -0.2954092025756836
Batch 8/64 loss: -0.28954654932022095
Batch 9/64 loss: -0.2753189206123352
Batch 10/64 loss: -0.28554150462150574
Batch 11/64 loss: -0.2968761920928955
Batch 12/64 loss: -0.26964855194091797
Batch 13/64 loss: -0.2864481508731842
Batch 14/64 loss: -0.2777553200721741
Batch 15/64 loss: -0.2800389528274536
Batch 16/64 loss: -0.2900794446468353
Batch 17/64 loss: -0.2840348482131958
Batch 18/64 loss: -0.28014427423477173
Batch 19/64 loss: -0.2795603573322296
Batch 20/64 loss: -0.2799190878868103
Batch 21/64 loss: -0.2682682275772095
Batch 22/64 loss: -0.2738438546657562
Batch 23/64 loss: -0.27445289492607117
Batch 24/64 loss: -0.2762634754180908
Batch 25/64 loss: -0.2803992033004761
Batch 26/64 loss: -0.28421109914779663
Batch 27/64 loss: -0.276367723941803
Batch 28/64 loss: -0.281747043132782
Batch 29/64 loss: -0.293702632188797
Batch 30/64 loss: -0.28567248582839966
Batch 31/64 loss: -0.2785675823688507
Batch 32/64 loss: -0.278734028339386
Batch 33/64 loss: -0.2882418632507324
Batch 34/64 loss: -0.29179731011390686
Batch 35/64 loss: -0.28720855712890625
Batch 36/64 loss: -0.2835463881492615
Batch 37/64 loss: -0.27664056420326233
Batch 38/64 loss: -0.2790961265563965
Batch 39/64 loss: -0.28079694509506226
Batch 40/64 loss: -0.28427064418792725
Batch 41/64 loss: -0.2905983030796051
Batch 42/64 loss: -0.28652244806289673
Batch 43/64 loss: -0.27570807933807373
Batch 44/64 loss: -0.2836751937866211
Batch 45/64 loss: -0.2760380506515503
Batch 46/64 loss: -0.28428226709365845
Batch 47/64 loss: -0.284832239151001
Batch 48/64 loss: -0.2928280234336853
Batch 49/64 loss: -0.2774884104728699
Batch 50/64 loss: -0.269972562789917
Batch 51/64 loss: -0.28987500071525574
Batch 52/64 loss: -0.2879391014575958
Batch 53/64 loss: -0.2793445587158203
Batch 54/64 loss: -0.27409628033638
Batch 55/64 loss: -0.2676845192909241
Batch 56/64 loss: -0.2784076929092407
Batch 57/64 loss: -0.29115599393844604
Batch 58/64 loss: -0.2835255265235901
Batch 59/64 loss: -0.2708621025085449
Batch 60/64 loss: -0.2821807265281677
Batch 61/64 loss: -0.274700790643692
Batch 62/64 loss: -0.27520960569381714
Batch 63/64 loss: -0.2857530117034912
Batch 64/64 loss: -0.2947160005569458
Epoch 304  Train loss: -0.2822666280409869  Val loss: -0.2518331142225626
Epoch 305
-------------------------------
Batch 1/64 loss: -0.2636902332305908
Batch 2/64 loss: -0.26533257961273193
Batch 3/64 loss: -0.2870255708694458
Batch 4/64 loss: -0.2872387766838074
Batch 5/64 loss: -0.27184465527534485
Batch 6/64 loss: -0.27993571758270264
Batch 7/64 loss: -0.269646018743515
Batch 8/64 loss: -0.279295951128006
Batch 9/64 loss: -0.28286394476890564
Batch 10/64 loss: -0.2775440514087677
Batch 11/64 loss: -0.283544659614563
Batch 12/64 loss: -0.2724630832672119
Batch 13/64 loss: -0.28511884808540344
Batch 14/64 loss: -0.2821415662765503
Batch 15/64 loss: -0.27515673637390137
Batch 16/64 loss: -0.2879733741283417
Batch 17/64 loss: -0.2917780578136444
Batch 18/64 loss: -0.26871320605278015
Batch 19/64 loss: -0.2906661629676819
Batch 20/64 loss: -0.2936372756958008
Batch 21/64 loss: -0.2602549195289612
Batch 22/64 loss: -0.286465585231781
Batch 23/64 loss: -0.27248939871788025
Batch 24/64 loss: -0.27289149165153503
Batch 25/64 loss: -0.2697151303291321
Batch 26/64 loss: -0.24832600355148315
Batch 27/64 loss: -0.2820242643356323
Batch 28/64 loss: -0.28130215406417847
Batch 29/64 loss: -0.28263452649116516
Batch 30/64 loss: -0.27900928258895874
Batch 31/64 loss: -0.272640198469162
Batch 32/64 loss: -0.28210389614105225
Batch 33/64 loss: -0.26826155185699463
Batch 34/64 loss: -0.2815472483634949
Batch 35/64 loss: -0.2738368809223175
Batch 36/64 loss: -0.2689093351364136
Batch 37/64 loss: -0.27435094118118286
Batch 38/64 loss: -0.2812095284461975
Batch 39/64 loss: -0.28080105781555176
Batch 40/64 loss: -0.2878108024597168
Batch 41/64 loss: -0.27872785925865173
Batch 42/64 loss: -0.27045750617980957
Batch 43/64 loss: -0.28807342052459717
Batch 44/64 loss: -0.2873535752296448
Batch 45/64 loss: -0.28326699137687683
Batch 46/64 loss: -0.28261831402778625
Batch 47/64 loss: -0.26268887519836426
Batch 48/64 loss: -0.26701217889785767
Batch 49/64 loss: -0.2690098285675049
Batch 50/64 loss: -0.2616461515426636
Batch 51/64 loss: -0.26491349935531616
Batch 52/64 loss: -0.27478280663490295
Batch 53/64 loss: -0.2845018804073334
Batch 54/64 loss: -0.2903527617454529
Batch 55/64 loss: -0.29485607147216797
Batch 56/64 loss: -0.2946021854877472
Batch 57/64 loss: -0.2839522957801819
Batch 58/64 loss: -0.2824348211288452
Batch 59/64 loss: -0.27683013677597046
Batch 60/64 loss: -0.28733354806900024
Batch 61/64 loss: -0.2813372015953064
Batch 62/64 loss: -0.2750808000564575
Batch 63/64 loss: -0.2842722535133362
Batch 64/64 loss: -0.2905357778072357
Epoch 305  Train loss: -0.2783717876555873  Val loss: -0.23983715674311845
Epoch 306
-------------------------------
Batch 1/64 loss: -0.27779507637023926
Batch 2/64 loss: -0.28688982129096985
Batch 3/64 loss: -0.2943096160888672
Batch 4/64 loss: -0.27914631366729736
Batch 5/64 loss: -0.26648014783859253
Batch 6/64 loss: -0.2725611925125122
Batch 7/64 loss: -0.27948278188705444
Batch 8/64 loss: -0.2847656011581421
Batch 9/64 loss: -0.27718284726142883
Batch 10/64 loss: -0.2809215784072876
Batch 11/64 loss: -0.28386443853378296
Batch 12/64 loss: -0.2771340012550354
Batch 13/64 loss: -0.2888343930244446
Batch 14/64 loss: -0.2905014753341675
Batch 15/64 loss: -0.29069948196411133
Batch 16/64 loss: -0.2870471775531769
Batch 17/64 loss: -0.2750605344772339
Batch 18/64 loss: -0.2872263193130493
Batch 19/64 loss: -0.28335335850715637
Batch 20/64 loss: -0.2897437810897827
Batch 21/64 loss: -0.28714635968208313
Batch 22/64 loss: -0.27885735034942627
Batch 23/64 loss: -0.28420865535736084
Batch 24/64 loss: -0.28558582067489624
Batch 25/64 loss: -0.28871583938598633
Batch 26/64 loss: -0.28150951862335205
Batch 27/64 loss: -0.27986061573028564
Batch 28/64 loss: -0.2724687159061432
Batch 29/64 loss: -0.2980346083641052
Batch 30/64 loss: -0.2770463824272156
Batch 31/64 loss: -0.2870345115661621
Batch 32/64 loss: -0.2850358188152313
Batch 33/64 loss: -0.2986396253108978
Batch 34/64 loss: -0.2912612557411194
Batch 35/64 loss: -0.29018768668174744
Batch 36/64 loss: -0.29664453864097595
Batch 37/64 loss: -0.2789258360862732
Batch 38/64 loss: -0.30196520686149597
Batch 39/64 loss: -0.28771811723709106
Batch 40/64 loss: -0.2802267074584961
Batch 41/64 loss: -0.27807241678237915
Batch 42/64 loss: -0.2788599729537964
Batch 43/64 loss: -0.29192453622817993
Batch 44/64 loss: -0.28334295749664307
Batch 45/64 loss: -0.2969346046447754
Batch 46/64 loss: -0.2862001061439514
Batch 47/64 loss: -0.2945994734764099
Batch 48/64 loss: -0.28944098949432373
Batch 49/64 loss: -0.28416234254837036
Batch 50/64 loss: -0.2639085054397583
Batch 51/64 loss: -0.2881270945072174
Batch 52/64 loss: -0.2761303782463074
Batch 53/64 loss: -0.2927548289299011
Batch 54/64 loss: -0.2759306728839874
Batch 55/64 loss: -0.2920152544975281
Batch 56/64 loss: -0.27203142642974854
Batch 57/64 loss: -0.268219530582428
Batch 58/64 loss: -0.2745707631111145
Batch 59/64 loss: -0.2880253493785858
Batch 60/64 loss: -0.2871338129043579
Batch 61/64 loss: -0.2754973769187927
Batch 62/64 loss: -0.2871181070804596
Batch 63/64 loss: -0.2902964949607849
Batch 64/64 loss: -0.29080885648727417
Epoch 306  Train loss: -0.284195713669646  Val loss: -0.24694099561455324
Epoch 307
-------------------------------
Batch 1/64 loss: -0.29064905643463135
Batch 2/64 loss: -0.27698376774787903
Batch 3/64 loss: -0.277605801820755
Batch 4/64 loss: -0.2746358811855316
Batch 5/64 loss: -0.29480308294296265
Batch 6/64 loss: -0.2765304446220398
Batch 7/64 loss: -0.28386014699935913
Batch 8/64 loss: -0.27484551072120667
Batch 9/64 loss: -0.2917124032974243
Batch 10/64 loss: -0.2881868779659271
Batch 11/64 loss: -0.2773403525352478
Batch 12/64 loss: -0.27856677770614624
Batch 13/64 loss: -0.27884167432785034
Batch 14/64 loss: -0.28191372752189636
Batch 15/64 loss: -0.266826331615448
Batch 16/64 loss: -0.28390878438949585
Batch 17/64 loss: -0.2882870137691498
Batch 18/64 loss: -0.2766508460044861
Batch 19/64 loss: -0.2876606583595276
Batch 20/64 loss: -0.2785348892211914
Batch 21/64 loss: -0.25785279273986816
Batch 22/64 loss: -0.27510887384414673
Batch 23/64 loss: -0.28349193930625916
Batch 24/64 loss: -0.28419938683509827
Batch 25/64 loss: -0.27464163303375244
Batch 26/64 loss: -0.2820664048194885
Batch 27/64 loss: -0.27217555046081543
Batch 28/64 loss: -0.25021272897720337
Batch 29/64 loss: -0.2771087884902954
Batch 30/64 loss: -0.2743278741836548
Batch 31/64 loss: -0.29266542196273804
Batch 32/64 loss: -0.28241199254989624
Batch 33/64 loss: -0.2869543135166168
Batch 34/64 loss: -0.2866116166114807
Batch 35/64 loss: -0.28581351041793823
Batch 36/64 loss: -0.28669947385787964
Batch 37/64 loss: -0.2746759057044983
Batch 38/64 loss: -0.2742226719856262
Batch 39/64 loss: -0.2872857451438904
Batch 40/64 loss: -0.28344032168388367
Batch 41/64 loss: -0.29672032594680786
Batch 42/64 loss: -0.2935575842857361
Batch 43/64 loss: -0.2888927161693573
Batch 44/64 loss: -0.2876638174057007
Batch 45/64 loss: -0.2847813069820404
Batch 46/64 loss: -0.29227256774902344
Batch 47/64 loss: -0.28224578499794006
Batch 48/64 loss: -0.2709798216819763
Batch 49/64 loss: -0.28964564204216003
Batch 50/64 loss: -0.29198598861694336
Batch 51/64 loss: -0.28428393602371216
Batch 52/64 loss: -0.28838980197906494
Batch 53/64 loss: -0.2833622097969055
Batch 54/64 loss: -0.28108465671539307
Batch 55/64 loss: -0.2878928780555725
Batch 56/64 loss: -0.27651292085647583
Batch 57/64 loss: -0.26472383737564087
Batch 58/64 loss: -0.2910712957382202
Batch 59/64 loss: -0.2916761040687561
Batch 60/64 loss: -0.27505069971084595
Batch 61/64 loss: -0.2797476649284363
Batch 62/64 loss: -0.27691879868507385
Batch 63/64 loss: -0.28744107484817505
Batch 64/64 loss: -0.2805957794189453
Epoch 307  Train loss: -0.281688772463331  Val loss: -0.25920883425322594
Epoch 308
-------------------------------
Batch 1/64 loss: -0.28789734840393066
Batch 2/64 loss: -0.2664061188697815
Batch 3/64 loss: -0.28042328357696533
Batch 4/64 loss: -0.2972533106803894
Batch 5/64 loss: -0.2760551869869232
Batch 6/64 loss: -0.29489782452583313
Batch 7/64 loss: -0.2808678150177002
Batch 8/64 loss: -0.2656491994857788
Batch 9/64 loss: -0.2711271047592163
Batch 10/64 loss: -0.28674784302711487
Batch 11/64 loss: -0.2719404399394989
Batch 12/64 loss: -0.2857933044433594
Batch 13/64 loss: -0.2958987355232239
Batch 14/64 loss: -0.28218874335289
Batch 15/64 loss: -0.2802472710609436
Batch 16/64 loss: -0.2892422676086426
Batch 17/64 loss: -0.2772093415260315
Batch 18/64 loss: -0.2742917835712433
Batch 19/64 loss: -0.2850116789340973
Batch 20/64 loss: -0.2869376242160797
Batch 21/64 loss: -0.2899971604347229
Batch 22/64 loss: -0.29106396436691284
Batch 23/64 loss: -0.28802019357681274
Batch 24/64 loss: -0.2863810062408447
Batch 25/64 loss: -0.2862790524959564
Batch 26/64 loss: -0.2798725366592407
Batch 27/64 loss: -0.2836415767669678
Batch 28/64 loss: -0.28116536140441895
Batch 29/64 loss: -0.2857053279876709
Batch 30/64 loss: -0.28597530722618103
Batch 31/64 loss: -0.2749122679233551
Batch 32/64 loss: -0.2976919412612915
Batch 33/64 loss: -0.2812251150608063
Batch 34/64 loss: -0.28911274671554565
Batch 35/64 loss: -0.28543999791145325
Batch 36/64 loss: -0.2727183699607849
Batch 37/64 loss: -0.2847764194011688
Batch 38/64 loss: -0.27366092801094055
Batch 39/64 loss: -0.2919681668281555
Batch 40/64 loss: -0.29192009568214417
Batch 41/64 loss: -0.27999284863471985
Batch 42/64 loss: -0.28505343198776245
Batch 43/64 loss: -0.2917603850364685
Batch 44/64 loss: -0.28560522198677063
Batch 45/64 loss: -0.2829962372779846
Batch 46/64 loss: -0.29751041531562805
Batch 47/64 loss: -0.283700168132782
Batch 48/64 loss: -0.29866790771484375
Batch 49/64 loss: -0.2858221232891083
Batch 50/64 loss: -0.28496894240379333
Batch 51/64 loss: -0.28185153007507324
Batch 52/64 loss: -0.2721529006958008
Batch 53/64 loss: -0.27608951926231384
Batch 54/64 loss: -0.2797701358795166
Batch 55/64 loss: -0.2872413396835327
Batch 56/64 loss: -0.2895263433456421
Batch 57/64 loss: -0.28453320264816284
Batch 58/64 loss: -0.27916082739830017
Batch 59/64 loss: -0.2724074125289917
Batch 60/64 loss: -0.29480433464050293
Batch 61/64 loss: -0.281490683555603
Batch 62/64 loss: -0.28803300857543945
Batch 63/64 loss: -0.28070467710494995
Batch 64/64 loss: -0.2642626166343689
Epoch 308  Train loss: -0.28360242072273706  Val loss: -0.25264986253685967
Epoch 309
-------------------------------
Batch 1/64 loss: -0.2887709438800812
Batch 2/64 loss: -0.2811332643032074
Batch 3/64 loss: -0.28706926107406616
Batch 4/64 loss: -0.28282177448272705
Batch 5/64 loss: -0.270376980304718
Batch 6/64 loss: -0.2885865569114685
Batch 7/64 loss: -0.2818140387535095
Batch 8/64 loss: -0.2899516522884369
Batch 9/64 loss: -0.2864001989364624
Batch 10/64 loss: -0.2732987403869629
Batch 11/64 loss: -0.29119640588760376
Batch 12/64 loss: -0.2848457396030426
Batch 13/64 loss: -0.2862277328968048
Batch 14/64 loss: -0.2760107219219208
Batch 15/64 loss: -0.2801492214202881
Batch 16/64 loss: -0.2885652780532837
Batch 17/64 loss: -0.28806740045547485
Batch 18/64 loss: -0.2886437177658081
Batch 19/64 loss: -0.2851540446281433
Batch 20/64 loss: -0.28543543815612793
Batch 21/64 loss: -0.29716092348098755
Batch 22/64 loss: -0.2769131362438202
Batch 23/64 loss: -0.30730581283569336
Batch 24/64 loss: -0.27676957845687866
Batch 25/64 loss: -0.28815698623657227
Batch 26/64 loss: -0.29115599393844604
Batch 27/64 loss: -0.28464701771736145
Batch 28/64 loss: -0.2830703556537628
Batch 29/64 loss: -0.2841639220714569
Batch 30/64 loss: -0.28298452496528625
Batch 31/64 loss: -0.2844325304031372
Batch 32/64 loss: -0.29321569204330444
Batch 33/64 loss: -0.2760584354400635
Batch 34/64 loss: -0.2793531119823456
Batch 35/64 loss: -0.28729021549224854
Batch 36/64 loss: -0.28455933928489685
Batch 37/64 loss: -0.28895097970962524
Batch 38/64 loss: -0.2775208353996277
Batch 39/64 loss: -0.28593599796295166
Batch 40/64 loss: -0.2839291989803314
Batch 41/64 loss: -0.2891634404659271
Batch 42/64 loss: -0.279899537563324
Batch 43/64 loss: -0.2877720594406128
Batch 44/64 loss: -0.27790367603302
Batch 45/64 loss: -0.28923043608665466
Batch 46/64 loss: -0.2898402214050293
Batch 47/64 loss: -0.2806541919708252
Batch 48/64 loss: -0.2797544598579407
Batch 49/64 loss: -0.27513551712036133
Batch 50/64 loss: -0.28340184688568115
Batch 51/64 loss: -0.27234959602355957
Batch 52/64 loss: -0.2712036073207855
Batch 53/64 loss: -0.2869739532470703
Batch 54/64 loss: -0.28127521276474
Batch 55/64 loss: -0.2857409119606018
Batch 56/64 loss: -0.2676613926887512
Batch 57/64 loss: -0.27684009075164795
Batch 58/64 loss: -0.2899166941642761
Batch 59/64 loss: -0.29384827613830566
Batch 60/64 loss: -0.27282023429870605
Batch 61/64 loss: -0.2753269672393799
Batch 62/64 loss: -0.2776065468788147
Batch 63/64 loss: -0.2827848792076111
Batch 64/64 loss: -0.29242247343063354
Epoch 309  Train loss: -0.28367865576463586  Val loss: -0.2541598443108326
Epoch 310
-------------------------------
Batch 1/64 loss: -0.27937251329421997
Batch 2/64 loss: -0.296495258808136
Batch 3/64 loss: -0.2843092679977417
Batch 4/64 loss: -0.285829097032547
Batch 5/64 loss: -0.26811468601226807
Batch 6/64 loss: -0.2884521186351776
Batch 7/64 loss: -0.2840816378593445
Batch 8/64 loss: -0.27812981605529785
Batch 9/64 loss: -0.2898927927017212
Batch 10/64 loss: -0.28664588928222656
Batch 11/64 loss: -0.27919429540634155
Batch 12/64 loss: -0.2719491124153137
Batch 13/64 loss: -0.29532161355018616
Batch 14/64 loss: -0.28431665897369385
Batch 15/64 loss: -0.29271918535232544
Batch 16/64 loss: -0.28017622232437134
Batch 17/64 loss: -0.2853841185569763
Batch 18/64 loss: -0.2918633222579956
Batch 19/64 loss: -0.2778482437133789
Batch 20/64 loss: -0.2870643734931946
Batch 21/64 loss: -0.29812178015708923
Batch 22/64 loss: -0.29980742931365967
Batch 23/64 loss: -0.2934996485710144
Batch 24/64 loss: -0.29420310258865356
Batch 25/64 loss: -0.2955811023712158
Batch 26/64 loss: -0.28526461124420166
Batch 27/64 loss: -0.28970080614089966
Batch 28/64 loss: -0.29433754086494446
Batch 29/64 loss: -0.3012202978134155
Batch 30/64 loss: -0.2856859266757965
Batch 31/64 loss: -0.28780442476272583
Batch 32/64 loss: -0.2752876877784729
Batch 33/64 loss: -0.29517829418182373
Batch 34/64 loss: -0.28998637199401855
Batch 35/64 loss: -0.2986212372779846
Batch 36/64 loss: -0.293880820274353
Batch 37/64 loss: -0.2924741506576538
Batch 38/64 loss: -0.2849229574203491
Batch 39/64 loss: -0.2941651940345764
Batch 40/64 loss: -0.2885524332523346
Batch 41/64 loss: -0.29529833793640137
Batch 42/64 loss: -0.275520384311676
Batch 43/64 loss: -0.27294886112213135
Batch 44/64 loss: -0.2814943194389343
Batch 45/64 loss: -0.2950030565261841
Batch 46/64 loss: -0.2903827428817749
Batch 47/64 loss: -0.2907762825489044
Batch 48/64 loss: -0.2989617586135864
Batch 49/64 loss: -0.28305327892303467
Batch 50/64 loss: -0.2826528251171112
Batch 51/64 loss: -0.27640968561172485
Batch 52/64 loss: -0.2874563932418823
Batch 53/64 loss: -0.2910219430923462
Batch 54/64 loss: -0.2884131371974945
Batch 55/64 loss: -0.2760425806045532
Batch 56/64 loss: -0.2883487939834595
Batch 57/64 loss: -0.2841549515724182
Batch 58/64 loss: -0.27677208185195923
Batch 59/64 loss: -0.2890247702598572
Batch 60/64 loss: -0.2987527549266815
Batch 61/64 loss: -0.28550422191619873
Batch 62/64 loss: -0.2749980390071869
Batch 63/64 loss: -0.29309165477752686
Batch 64/64 loss: -0.27211689949035645
Epoch 310  Train loss: -0.28714708346946566  Val loss: -0.264840763254264
Epoch 311
-------------------------------
Batch 1/64 loss: -0.28642067313194275
Batch 2/64 loss: -0.2784668803215027
Batch 3/64 loss: -0.2923841178417206
Batch 4/64 loss: -0.2926938831806183
Batch 5/64 loss: -0.2980386018753052
Batch 6/64 loss: -0.2893862724304199
Batch 7/64 loss: -0.28044015169143677
Batch 8/64 loss: -0.2790068984031677
Batch 9/64 loss: -0.2767464220523834
Batch 10/64 loss: -0.2829543352127075
Batch 11/64 loss: -0.283333957195282
Batch 12/64 loss: -0.2971954941749573
Batch 13/64 loss: -0.2866228222846985
Batch 14/64 loss: -0.2959851026535034
Batch 15/64 loss: -0.28662148118019104
Batch 16/64 loss: -0.27455437183380127
Batch 17/64 loss: -0.27264052629470825
Batch 18/64 loss: -0.28702300786972046
Batch 19/64 loss: -0.2853234112262726
Batch 20/64 loss: -0.277410626411438
Batch 21/64 loss: -0.2870636582374573
Batch 22/64 loss: -0.2882872223854065
Batch 23/64 loss: -0.2872602343559265
Batch 24/64 loss: -0.2820160388946533
Batch 25/64 loss: -0.2912145256996155
Batch 26/64 loss: -0.28250622749328613
Batch 27/64 loss: -0.2893086373806
Batch 28/64 loss: -0.3005388379096985
Batch 29/64 loss: -0.28397709131240845
Batch 30/64 loss: -0.282353013753891
Batch 31/64 loss: -0.299968957901001
Batch 32/64 loss: -0.27753978967666626
Batch 33/64 loss: -0.28449201583862305
Batch 34/64 loss: -0.27706509828567505
Batch 35/64 loss: -0.27786940336227417
Batch 36/64 loss: -0.297612726688385
Batch 37/64 loss: -0.2842089831829071
Batch 38/64 loss: -0.27488064765930176
Batch 39/64 loss: -0.28595227003097534
Batch 40/64 loss: -0.28013724088668823
Batch 41/64 loss: -0.26995849609375
Batch 42/64 loss: -0.2780100703239441
Batch 43/64 loss: -0.27737903594970703
Batch 44/64 loss: -0.28313371539115906
Batch 45/64 loss: -0.28649598360061646
Batch 46/64 loss: -0.2699238657951355
Batch 47/64 loss: -0.29261457920074463
Batch 48/64 loss: -0.28267502784729004
Batch 49/64 loss: -0.2586205005645752
Batch 50/64 loss: -0.2796010375022888
Batch 51/64 loss: -0.28264525532722473
Batch 52/64 loss: -0.2883484959602356
Batch 53/64 loss: -0.28547051548957825
Batch 54/64 loss: -0.28666430711746216
Batch 55/64 loss: -0.2832087278366089
Batch 56/64 loss: -0.2912020683288574
Batch 57/64 loss: -0.255085289478302
Batch 58/64 loss: -0.2889663875102997
Batch 59/64 loss: -0.27909448742866516
Batch 60/64 loss: -0.2779400944709778
Batch 61/64 loss: -0.2929269075393677
Batch 62/64 loss: -0.2860041856765747
Batch 63/64 loss: -0.28604406118392944
Batch 64/64 loss: -0.2937726378440857
Epoch 311  Train loss: -0.2839191252110051  Val loss: -0.25144423344700606
Epoch 312
-------------------------------
Batch 1/64 loss: -0.2810051143169403
Batch 2/64 loss: -0.28351929783821106
Batch 3/64 loss: -0.272144079208374
Batch 4/64 loss: -0.28426820039749146
Batch 5/64 loss: -0.2813032865524292
Batch 6/64 loss: -0.28821271657943726
Batch 7/64 loss: -0.286384254693985
Batch 8/64 loss: -0.2947808504104614
Batch 9/64 loss: -0.26876378059387207
Batch 10/64 loss: -0.2736921012401581
Batch 11/64 loss: -0.276003360748291
Batch 12/64 loss: -0.28488439321517944
Batch 13/64 loss: -0.2809981107711792
Batch 14/64 loss: -0.27337032556533813
Batch 15/64 loss: -0.2885144352912903
Batch 16/64 loss: -0.2696232199668884
Batch 17/64 loss: -0.29510295391082764
Batch 18/64 loss: -0.29144442081451416
Batch 19/64 loss: -0.29469045996665955
Batch 20/64 loss: -0.2986525297164917
Batch 21/64 loss: -0.2929471433162689
Batch 22/64 loss: -0.26400619745254517
Batch 23/64 loss: -0.28791344165802
Batch 24/64 loss: -0.28622594475746155
Batch 25/64 loss: -0.28889691829681396
Batch 26/64 loss: -0.29496195912361145
Batch 27/64 loss: -0.29989027976989746
Batch 28/64 loss: -0.2872883677482605
Batch 29/64 loss: -0.2803468108177185
Batch 30/64 loss: -0.2665894627571106
Batch 31/64 loss: -0.2932363748550415
Batch 32/64 loss: -0.2826058268547058
Batch 33/64 loss: -0.26982593536376953
Batch 34/64 loss: -0.2906915545463562
Batch 35/64 loss: -0.2838909924030304
Batch 36/64 loss: -0.2868903875350952
Batch 37/64 loss: -0.286006897687912
Batch 38/64 loss: -0.25948548316955566
Batch 39/64 loss: -0.2843022048473358
Batch 40/64 loss: -0.28722673654556274
Batch 41/64 loss: -0.293489933013916
Batch 42/64 loss: -0.28435927629470825
Batch 43/64 loss: -0.2816654443740845
Batch 44/64 loss: -0.2856210470199585
Batch 45/64 loss: -0.27365848422050476
Batch 46/64 loss: -0.28969472646713257
Batch 47/64 loss: -0.26174384355545044
Batch 48/64 loss: -0.2709434926509857
Batch 49/64 loss: -0.279142826795578
Batch 50/64 loss: -0.2793571949005127
Batch 51/64 loss: -0.28187453746795654
Batch 52/64 loss: -0.2843293845653534
Batch 53/64 loss: -0.2884620428085327
Batch 54/64 loss: -0.2729280889034271
Batch 55/64 loss: -0.2683136463165283
Batch 56/64 loss: -0.2825237214565277
Batch 57/64 loss: -0.2821175158023834
Batch 58/64 loss: -0.27050143480300903
Batch 59/64 loss: -0.2797604501247406
Batch 60/64 loss: -0.2728008031845093
Batch 61/64 loss: -0.29282498359680176
Batch 62/64 loss: -0.28519633412361145
Batch 63/64 loss: -0.2885621190071106
Batch 64/64 loss: -0.28430190682411194
Epoch 312  Train loss: -0.28241073839804703  Val loss: -0.25993888351515804
Epoch 313
-------------------------------
Batch 1/64 loss: -0.2730908989906311
Batch 2/64 loss: -0.28645622730255127
Batch 3/64 loss: -0.2745249569416046
Batch 4/64 loss: -0.29170915484428406
Batch 5/64 loss: -0.2890533208847046
Batch 6/64 loss: -0.2808134853839874
Batch 7/64 loss: -0.2981375753879547
Batch 8/64 loss: -0.2860167920589447
Batch 9/64 loss: -0.2929222583770752
Batch 10/64 loss: -0.2700246572494507
Batch 11/64 loss: -0.2610514163970947
Batch 12/64 loss: -0.2945411205291748
Batch 13/64 loss: -0.2884320020675659
Batch 14/64 loss: -0.29016467928886414
Batch 15/64 loss: -0.2861535847187042
Batch 16/64 loss: -0.2774698734283447
Batch 17/64 loss: -0.29294049739837646
Batch 18/64 loss: -0.2648407816886902
Batch 19/64 loss: -0.28784722089767456
Batch 20/64 loss: -0.29583901166915894
Batch 21/64 loss: -0.287702739238739
Batch 22/64 loss: -0.2990119457244873
Batch 23/64 loss: -0.2871066927909851
Batch 24/64 loss: -0.2711278796195984
Batch 25/64 loss: -0.28181251883506775
Batch 26/64 loss: -0.29524654150009155
Batch 27/64 loss: -0.28630226850509644
Batch 28/64 loss: -0.2951453626155853
Batch 29/64 loss: -0.2912216782569885
Batch 30/64 loss: -0.28607529401779175
Batch 31/64 loss: -0.28059858083724976
Batch 32/64 loss: -0.2803259491920471
Batch 33/64 loss: -0.28695356845855713
Batch 34/64 loss: -0.2880614101886749
Batch 35/64 loss: -0.28730714321136475
Batch 36/64 loss: -0.27113908529281616
Batch 37/64 loss: -0.2807348370552063
Batch 38/64 loss: -0.2733626961708069
Batch 39/64 loss: -0.28058111667633057
Batch 40/64 loss: -0.26948845386505127
Batch 41/64 loss: -0.2829841077327728
Batch 42/64 loss: -0.281746506690979
Batch 43/64 loss: -0.28370702266693115
Batch 44/64 loss: -0.28328683972358704
Batch 45/64 loss: -0.28167304396629333
Batch 46/64 loss: -0.2857165038585663
Batch 47/64 loss: -0.27359095215797424
Batch 48/64 loss: -0.27710866928100586
Batch 49/64 loss: -0.280758261680603
Batch 50/64 loss: -0.2801022231578827
Batch 51/64 loss: -0.286246657371521
Batch 52/64 loss: -0.2765677571296692
Batch 53/64 loss: -0.28698092699050903
Batch 54/64 loss: -0.28677549958229065
Batch 55/64 loss: -0.2844223082065582
Batch 56/64 loss: -0.2798524498939514
Batch 57/64 loss: -0.2850087285041809
Batch 58/64 loss: -0.2815975844860077
Batch 59/64 loss: -0.3008246421813965
Batch 60/64 loss: -0.29525429010391235
Batch 61/64 loss: -0.29708150029182434
Batch 62/64 loss: -0.2884148955345154
Batch 63/64 loss: -0.28510135412216187
Batch 64/64 loss: -0.29673123359680176
Epoch 313  Train loss: -0.28437155181286383  Val loss: -0.26397544923926547
Epoch 314
-------------------------------
Batch 1/64 loss: -0.28121381998062134
Batch 2/64 loss: -0.2851191759109497
Batch 3/64 loss: -0.28975528478622437
Batch 4/64 loss: -0.2875920534133911
Batch 5/64 loss: -0.2873874306678772
Batch 6/64 loss: -0.26928573846817017
Batch 7/64 loss: -0.29029327630996704
Batch 8/64 loss: -0.2768743634223938
Batch 9/64 loss: -0.29629576206207275
Batch 10/64 loss: -0.2863074839115143
Batch 11/64 loss: -0.282924085855484
Batch 12/64 loss: -0.29237404465675354
Batch 13/64 loss: -0.2764301896095276
Batch 14/64 loss: -0.28830206394195557
Batch 15/64 loss: -0.29243022203445435
Batch 16/64 loss: -0.2821323275566101
Batch 17/64 loss: -0.27403151988983154
Batch 18/64 loss: -0.29203298687934875
Batch 19/64 loss: -0.27139896154403687
Batch 20/64 loss: -0.2744734287261963
Batch 21/64 loss: -0.276955246925354
Batch 22/64 loss: -0.28242504596710205
Batch 23/64 loss: -0.27214640378952026
Batch 24/64 loss: -0.2819265127182007
Batch 25/64 loss: -0.296516478061676
Batch 26/64 loss: -0.2921668291091919
Batch 27/64 loss: -0.26917070150375366
Batch 28/64 loss: -0.2962501645088196
Batch 29/64 loss: -0.27740490436553955
Batch 30/64 loss: -0.27782732248306274
Batch 31/64 loss: -0.28937774896621704
Batch 32/64 loss: -0.2840903401374817
Batch 33/64 loss: -0.29124313592910767
Batch 34/64 loss: -0.277548611164093
Batch 35/64 loss: -0.2870936989784241
Batch 36/64 loss: -0.28065362572669983
Batch 37/64 loss: -0.28634873032569885
Batch 38/64 loss: -0.26541852951049805
Batch 39/64 loss: -0.2993204593658447
Batch 40/64 loss: -0.2797696590423584
Batch 41/64 loss: -0.283081978559494
Batch 42/64 loss: -0.29075539112091064
Batch 43/64 loss: -0.28502708673477173
Batch 44/64 loss: -0.276034414768219
Batch 45/64 loss: -0.2907556891441345
Batch 46/64 loss: -0.29415005445480347
Batch 47/64 loss: -0.2875113785266876
Batch 48/64 loss: -0.2863638997077942
Batch 49/64 loss: -0.286893367767334
Batch 50/64 loss: -0.282934308052063
Batch 51/64 loss: -0.27351075410842896
Batch 52/64 loss: -0.2790721654891968
Batch 53/64 loss: -0.28569430112838745
Batch 54/64 loss: -0.28799763321876526
Batch 55/64 loss: -0.2896619439125061
Batch 56/64 loss: -0.28981372714042664
Batch 57/64 loss: -0.2796611189842224
Batch 58/64 loss: -0.28224724531173706
Batch 59/64 loss: -0.2870209217071533
Batch 60/64 loss: -0.28742361068725586
Batch 61/64 loss: -0.2826130986213684
Batch 62/64 loss: -0.2895151972770691
Batch 63/64 loss: -0.28731927275657654
Batch 64/64 loss: -0.2883908152580261
Epoch 314  Train loss: -0.2842613339424133  Val loss: -0.25634509140683204
Epoch 315
-------------------------------
Batch 1/64 loss: -0.287932813167572
Batch 2/64 loss: -0.2947615683078766
Batch 3/64 loss: -0.28693687915802
Batch 4/64 loss: -0.300686240196228
Batch 5/64 loss: -0.2882236838340759
Batch 6/64 loss: -0.3012482225894928
Batch 7/64 loss: -0.28001174330711365
Batch 8/64 loss: -0.2884216010570526
Batch 9/64 loss: -0.2849491536617279
Batch 10/64 loss: -0.2728908061981201
Batch 11/64 loss: -0.288885235786438
Batch 12/64 loss: -0.27581787109375
Batch 13/64 loss: -0.28753334283828735
Batch 14/64 loss: -0.2884160876274109
Batch 15/64 loss: -0.2835052013397217
Batch 16/64 loss: -0.2808346450328827
Batch 17/64 loss: -0.2861960232257843
Batch 18/64 loss: -0.2771962881088257
Batch 19/64 loss: -0.28907638788223267
Batch 20/64 loss: -0.27975237369537354
Batch 21/64 loss: -0.2867586016654968
Batch 22/64 loss: -0.27062177658081055
Batch 23/64 loss: -0.2781314253807068
Batch 24/64 loss: -0.2733794152736664
Batch 25/64 loss: -0.2822667956352234
Batch 26/64 loss: -0.29372578859329224
Batch 27/64 loss: -0.2704636752605438
Batch 28/64 loss: -0.28973400592803955
Batch 29/64 loss: -0.29718396067619324
Batch 30/64 loss: -0.2870132327079773
Batch 31/64 loss: -0.29013651609420776
Batch 32/64 loss: -0.2839764356613159
Batch 33/64 loss: -0.2892189025878906
Batch 34/64 loss: -0.2662818431854248
Batch 35/64 loss: -0.2972729802131653
Batch 36/64 loss: -0.29233837127685547
Batch 37/64 loss: -0.2699267566204071
Batch 38/64 loss: -0.2716365456581116
Batch 39/64 loss: -0.28571879863739014
Batch 40/64 loss: -0.29841136932373047
Batch 41/64 loss: -0.27254718542099
Batch 42/64 loss: -0.277168869972229
Batch 43/64 loss: -0.2837924361228943
Batch 44/64 loss: -0.28326284885406494
Batch 45/64 loss: -0.2891271114349365
Batch 46/64 loss: -0.28766217827796936
Batch 47/64 loss: -0.2772196829319
Batch 48/64 loss: -0.29100149869918823
Batch 49/64 loss: -0.2754681706428528
Batch 50/64 loss: -0.2852586507797241
Batch 51/64 loss: -0.2858421504497528
Batch 52/64 loss: -0.2826114296913147
Batch 53/64 loss: -0.29368460178375244
Batch 54/64 loss: -0.29346349835395813
Batch 55/64 loss: -0.29053795337677
Batch 56/64 loss: -0.2933570146560669
Batch 57/64 loss: -0.28930702805519104
Batch 58/64 loss: -0.2800402045249939
Batch 59/64 loss: -0.29100024700164795
Batch 60/64 loss: -0.2924662232398987
Batch 61/64 loss: -0.2865893840789795
Batch 62/64 loss: -0.2934783101081848
Batch 63/64 loss: -0.28696560859680176
Batch 64/64 loss: -0.2846924662590027
Epoch 315  Train loss: -0.28537796081281175  Val loss: -0.26155344624699595
Epoch 316
-------------------------------
Batch 1/64 loss: -0.2998880445957184
Batch 2/64 loss: -0.29393184185028076
Batch 3/64 loss: -0.2958419919013977
Batch 4/64 loss: -0.29741477966308594
Batch 5/64 loss: -0.28788790106773376
Batch 6/64 loss: -0.2822904586791992
Batch 7/64 loss: -0.28720349073410034
Batch 8/64 loss: -0.28069838881492615
Batch 9/64 loss: -0.28665396571159363
Batch 10/64 loss: -0.2909078299999237
Batch 11/64 loss: -0.29014307260513306
Batch 12/64 loss: -0.30170130729675293
Batch 13/64 loss: -0.2827291190624237
Batch 14/64 loss: -0.28513720631599426
Batch 15/64 loss: -0.29413074254989624
Batch 16/64 loss: -0.2996535897254944
Batch 17/64 loss: -0.2901751399040222
Batch 18/64 loss: -0.2852590084075928
Batch 19/64 loss: -0.28835421800613403
Batch 20/64 loss: -0.2787121534347534
Batch 21/64 loss: -0.2841435372829437
Batch 22/64 loss: -0.27557116746902466
Batch 23/64 loss: -0.2912542521953583
Batch 24/64 loss: -0.2871769964694977
Batch 25/64 loss: -0.27857887744903564
Batch 26/64 loss: -0.287791907787323
Batch 27/64 loss: -0.2807542085647583
Batch 28/64 loss: -0.28184598684310913
Batch 29/64 loss: -0.2870138883590698
Batch 30/64 loss: -0.28652453422546387
Batch 31/64 loss: -0.2820932865142822
Batch 32/64 loss: -0.2859731912612915
Batch 33/64 loss: -0.29141026735305786
Batch 34/64 loss: -0.2723103165626526
Batch 35/64 loss: -0.28838276863098145
Batch 36/64 loss: -0.2807818353176117
Batch 37/64 loss: -0.2966447174549103
Batch 38/64 loss: -0.29242223501205444
Batch 39/64 loss: -0.2962780296802521
Batch 40/64 loss: -0.2935754358768463
Batch 41/64 loss: -0.2796998620033264
Batch 42/64 loss: -0.28178805112838745
Batch 43/64 loss: -0.25831782817840576
Batch 44/64 loss: -0.28239479660987854
Batch 45/64 loss: -0.29436421394348145
Batch 46/64 loss: -0.2779640555381775
Batch 47/64 loss: -0.29618388414382935
Batch 48/64 loss: -0.2795153856277466
Batch 49/64 loss: -0.2904995083808899
Batch 50/64 loss: -0.27644503116607666
Batch 51/64 loss: -0.28873661160469055
Batch 52/64 loss: -0.282446026802063
Batch 53/64 loss: -0.2646300196647644
Batch 54/64 loss: -0.28547802567481995
Batch 55/64 loss: -0.2778782248497009
Batch 56/64 loss: -0.29307907819747925
Batch 57/64 loss: -0.2808046340942383
Batch 58/64 loss: -0.27737903594970703
Batch 59/64 loss: -0.290207177400589
Batch 60/64 loss: -0.2790275812149048
Batch 61/64 loss: -0.29197677969932556
Batch 62/64 loss: -0.27610161900520325
Batch 63/64 loss: -0.29630404710769653
Batch 64/64 loss: -0.28637072443962097
Epoch 316  Train loss: -0.2860429994031495  Val loss: -0.24640993420610724
Epoch 317
-------------------------------
Batch 1/64 loss: -0.2894223928451538
Batch 2/64 loss: -0.284235417842865
Batch 3/64 loss: -0.2899412512779236
Batch 4/64 loss: -0.2977597415447235
Batch 5/64 loss: -0.288858562707901
Batch 6/64 loss: -0.28403252363204956
Batch 7/64 loss: -0.2767391800880432
Batch 8/64 loss: -0.2805715799331665
Batch 9/64 loss: -0.29010605812072754
Batch 10/64 loss: -0.28681814670562744
Batch 11/64 loss: -0.275387704372406
Batch 12/64 loss: -0.29636824131011963
Batch 13/64 loss: -0.2875415086746216
Batch 14/64 loss: -0.29169541597366333
Batch 15/64 loss: -0.2939356863498688
Batch 16/64 loss: -0.2890658974647522
Batch 17/64 loss: -0.29735851287841797
Batch 18/64 loss: -0.2634963393211365
Batch 19/64 loss: -0.28194132447242737
Batch 20/64 loss: -0.28753846883773804
Batch 21/64 loss: -0.28205257654190063
Batch 22/64 loss: -0.2909625172615051
Batch 23/64 loss: -0.2818654775619507
Batch 24/64 loss: -0.28543299436569214
Batch 25/64 loss: -0.2787148654460907
Batch 26/64 loss: -0.26987338066101074
Batch 27/64 loss: -0.27343714237213135
Batch 28/64 loss: -0.2764609456062317
Batch 29/64 loss: -0.28685471415519714
Batch 30/64 loss: -0.2662554979324341
Batch 31/64 loss: -0.25128960609436035
Batch 32/64 loss: -0.2795562148094177
Batch 33/64 loss: -0.28831979632377625
Batch 34/64 loss: -0.2697705030441284
Batch 35/64 loss: -0.2800026535987854
Batch 36/64 loss: -0.2807558476924896
Batch 37/64 loss: -0.27816328406333923
Batch 38/64 loss: -0.2847863435745239
Batch 39/64 loss: -0.2829919457435608
Batch 40/64 loss: -0.27996039390563965
Batch 41/64 loss: -0.2887793779373169
Batch 42/64 loss: -0.275011271238327
Batch 43/64 loss: -0.2816101908683777
Batch 44/64 loss: -0.27957797050476074
Batch 45/64 loss: -0.2909374535083771
Batch 46/64 loss: -0.2961099147796631
Batch 47/64 loss: -0.2899520993232727
Batch 48/64 loss: -0.29322004318237305
Batch 49/64 loss: -0.28780341148376465
Batch 50/64 loss: -0.290243923664093
Batch 51/64 loss: -0.28336215019226074
Batch 52/64 loss: -0.28871262073516846
Batch 53/64 loss: -0.2780448794364929
Batch 54/64 loss: -0.2782529294490814
Batch 55/64 loss: -0.2771312892436981
Batch 56/64 loss: -0.2992808222770691
Batch 57/64 loss: -0.28945890069007874
Batch 58/64 loss: -0.2945769727230072
Batch 59/64 loss: -0.27979937195777893
Batch 60/64 loss: -0.287788450717926
Batch 61/64 loss: -0.2897607684135437
Batch 62/64 loss: -0.26977652311325073
Batch 63/64 loss: -0.29258543252944946
Batch 64/64 loss: -0.28436386585235596
Epoch 317  Train loss: -0.28384894623475915  Val loss: -0.2513623385085273
Epoch 318
-------------------------------
Batch 1/64 loss: -0.27814778685569763
Batch 2/64 loss: -0.2602359652519226
Batch 3/64 loss: -0.2925565242767334
Batch 4/64 loss: -0.28230124711990356
Batch 5/64 loss: -0.2784648537635803
Batch 6/64 loss: -0.27803337574005127
Batch 7/64 loss: -0.29761362075805664
Batch 8/64 loss: -0.29845890402793884
Batch 9/64 loss: -0.28213220834732056
Batch 10/64 loss: -0.3038649559020996
Batch 11/64 loss: -0.2901087701320648
Batch 12/64 loss: -0.2844197750091553
Batch 13/64 loss: -0.287600040435791
Batch 14/64 loss: -0.2896222174167633
Batch 15/64 loss: -0.2947147488594055
Batch 16/64 loss: -0.26743602752685547
Batch 17/64 loss: -0.27532774209976196
Batch 18/64 loss: -0.2876889109611511
Batch 19/64 loss: -0.2952233850955963
Batch 20/64 loss: -0.2814818322658539
Batch 21/64 loss: -0.27804961800575256
Batch 22/64 loss: -0.2757567763328552
Batch 23/64 loss: -0.2880050539970398
Batch 24/64 loss: -0.27961528301239014
Batch 25/64 loss: -0.28105628490448
Batch 26/64 loss: -0.28405463695526123
Batch 27/64 loss: -0.28808727860450745
Batch 28/64 loss: -0.27391770482063293
Batch 29/64 loss: -0.29504328966140747
Batch 30/64 loss: -0.2788775563240051
Batch 31/64 loss: -0.27228960394859314
Batch 32/64 loss: -0.28141888976097107
Batch 33/64 loss: -0.3002093732357025
Batch 34/64 loss: -0.2769147753715515
Batch 35/64 loss: -0.29705649614334106
Batch 36/64 loss: -0.2978929877281189
Batch 37/64 loss: -0.2830674946308136
Batch 38/64 loss: -0.29205501079559326
Batch 39/64 loss: -0.30045485496520996
Batch 40/64 loss: -0.28503814339637756
Batch 41/64 loss: -0.2778592109680176
Batch 42/64 loss: -0.27512049674987793
Batch 43/64 loss: -0.2820472717285156
Batch 44/64 loss: -0.2797292470932007
Batch 45/64 loss: -0.29734209179878235
Batch 46/64 loss: -0.2834540903568268
Batch 47/64 loss: -0.29911303520202637
Batch 48/64 loss: -0.2804981470108032
Batch 49/64 loss: -0.2743997275829315
Batch 50/64 loss: -0.29742753505706787
Batch 51/64 loss: -0.2880380153656006
Batch 52/64 loss: -0.286817729473114
Batch 53/64 loss: -0.28925469517707825
Batch 54/64 loss: -0.2876240611076355
Batch 55/64 loss: -0.295071542263031
Batch 56/64 loss: -0.2840431034564972
Batch 57/64 loss: -0.2976883053779602
Batch 58/64 loss: -0.28847554326057434
Batch 59/64 loss: -0.2996809482574463
Batch 60/64 loss: -0.2845168709754944
Batch 61/64 loss: -0.29455363750457764
Batch 62/64 loss: -0.2698880434036255
Batch 63/64 loss: -0.2903288006782532
Batch 64/64 loss: -0.2797241806983948
Epoch 318  Train loss: -0.2859146555264791  Val loss: -0.25315981734659254
Epoch 319
-------------------------------
Batch 1/64 loss: -0.2972446084022522
Batch 2/64 loss: -0.2761881351470947
Batch 3/64 loss: -0.29694241285324097
Batch 4/64 loss: -0.29248496890068054
Batch 5/64 loss: -0.28845033049583435
Batch 6/64 loss: -0.2933092415332794
Batch 7/64 loss: -0.2835121154785156
Batch 8/64 loss: -0.29189735651016235
Batch 9/64 loss: -0.28617170453071594
Batch 10/64 loss: -0.2797778248786926
Batch 11/64 loss: -0.29500195384025574
Batch 12/64 loss: -0.2660938501358032
Batch 13/64 loss: -0.2792821526527405
Batch 14/64 loss: -0.29038459062576294
Batch 15/64 loss: -0.27550458908081055
Batch 16/64 loss: -0.24867743253707886
Batch 17/64 loss: -0.2800138592720032
Batch 18/64 loss: -0.27533525228500366
Batch 19/64 loss: -0.28601503372192383
Batch 20/64 loss: -0.28389686346054077
Batch 21/64 loss: -0.28665608167648315
Batch 22/64 loss: -0.27748948335647583
Batch 23/64 loss: -0.29267773032188416
Batch 24/64 loss: -0.2891557216644287
Batch 25/64 loss: -0.29233136773109436
Batch 26/64 loss: -0.2923017740249634
Batch 27/64 loss: -0.2954602837562561
Batch 28/64 loss: -0.2888571619987488
Batch 29/64 loss: -0.2886357307434082
Batch 30/64 loss: -0.290857195854187
Batch 31/64 loss: -0.27480751276016235
Batch 32/64 loss: -0.2843467891216278
Batch 33/64 loss: -0.27621030807495117
Batch 34/64 loss: -0.28335657715797424
Batch 35/64 loss: -0.273770809173584
Batch 36/64 loss: -0.2847243547439575
Batch 37/64 loss: -0.2954556345939636
Batch 38/64 loss: -0.2892672121524811
Batch 39/64 loss: -0.2914768159389496
Batch 40/64 loss: -0.2861716151237488
Batch 41/64 loss: -0.2767614722251892
Batch 42/64 loss: -0.2905893921852112
Batch 43/64 loss: -0.2804638743400574
Batch 44/64 loss: -0.2924171984195709
Batch 45/64 loss: -0.29531633853912354
Batch 46/64 loss: -0.28592008352279663
Batch 47/64 loss: -0.2901347279548645
Batch 48/64 loss: -0.29092761874198914
Batch 49/64 loss: -0.285427451133728
Batch 50/64 loss: -0.278912216424942
Batch 51/64 loss: -0.29347458481788635
Batch 52/64 loss: -0.2835421562194824
Batch 53/64 loss: -0.28623056411743164
Batch 54/64 loss: -0.29448893666267395
Batch 55/64 loss: -0.2918629050254822
Batch 56/64 loss: -0.2962135970592499
Batch 57/64 loss: -0.2917272746562958
Batch 58/64 loss: -0.2906866669654846
Batch 59/64 loss: -0.287847101688385
Batch 60/64 loss: -0.2934759259223938
Batch 61/64 loss: -0.27462711930274963
Batch 62/64 loss: -0.2932748794555664
Batch 63/64 loss: -0.288004994392395
Batch 64/64 loss: -0.27447381615638733
Epoch 319  Train loss: -0.28609218632473665  Val loss: -0.26262175688628886
Epoch 320
-------------------------------
Batch 1/64 loss: -0.29413944482803345
Batch 2/64 loss: -0.2902700901031494
Batch 3/64 loss: -0.2870713472366333
Batch 4/64 loss: -0.289434552192688
Batch 5/64 loss: -0.30074045062065125
Batch 6/64 loss: -0.2878330945968628
Batch 7/64 loss: -0.2995554208755493
Batch 8/64 loss: -0.2964332401752472
Batch 9/64 loss: -0.28313976526260376
Batch 10/64 loss: -0.2797949016094208
Batch 11/64 loss: -0.27780595421791077
Batch 12/64 loss: -0.286513090133667
Batch 13/64 loss: -0.27912068367004395
Batch 14/64 loss: -0.2893735468387604
Batch 15/64 loss: -0.2886456847190857
Batch 16/64 loss: -0.2837165594100952
Batch 17/64 loss: -0.2873247265815735
Batch 18/64 loss: -0.27493974566459656
Batch 19/64 loss: -0.29281049966812134
Batch 20/64 loss: -0.2941502034664154
Batch 21/64 loss: -0.28125905990600586
Batch 22/64 loss: -0.2810385823249817
Batch 23/64 loss: -0.2871987521648407
Batch 24/64 loss: -0.2851492762565613
Batch 25/64 loss: -0.2950358986854553
Batch 26/64 loss: -0.3013278841972351
Batch 27/64 loss: -0.27936437726020813
Batch 28/64 loss: -0.28390228748321533
Batch 29/64 loss: -0.2750472128391266
Batch 30/64 loss: -0.28430652618408203
Batch 31/64 loss: -0.2897545099258423
Batch 32/64 loss: -0.2793903648853302
Batch 33/64 loss: -0.28076836466789246
Batch 34/64 loss: -0.2825635075569153
Batch 35/64 loss: -0.28163355588912964
Batch 36/64 loss: -0.2772543430328369
Batch 37/64 loss: -0.27440735697746277
Batch 38/64 loss: -0.2810332179069519
Batch 39/64 loss: -0.2995801568031311
Batch 40/64 loss: -0.2665826082229614
Batch 41/64 loss: -0.28761693835258484
Batch 42/64 loss: -0.29779183864593506
Batch 43/64 loss: -0.29150503873825073
Batch 44/64 loss: -0.30013900995254517
Batch 45/64 loss: -0.30054664611816406
Batch 46/64 loss: -0.2970004677772522
Batch 47/64 loss: -0.28120124340057373
Batch 48/64 loss: -0.29134345054626465
Batch 49/64 loss: -0.2856856882572174
Batch 50/64 loss: -0.2874518036842346
Batch 51/64 loss: -0.2898784875869751
Batch 52/64 loss: -0.2931166887283325
Batch 53/64 loss: -0.29883700609207153
Batch 54/64 loss: -0.29415035247802734
Batch 55/64 loss: -0.28439006209373474
Batch 56/64 loss: -0.2807983458042145
Batch 57/64 loss: -0.29327625036239624
Batch 58/64 loss: -0.2811640202999115
Batch 59/64 loss: -0.2901965379714966
Batch 60/64 loss: -0.29288583993911743
Batch 61/64 loss: -0.2946246266365051
Batch 62/64 loss: -0.28951525688171387
Batch 63/64 loss: -0.2946189045906067
Batch 64/64 loss: -0.29459646344184875
Epoch 320  Train loss: -0.2878132187852673  Val loss: -0.2562400143580748
Epoch 321
-------------------------------
Batch 1/64 loss: -0.29672718048095703
Batch 2/64 loss: -0.29868125915527344
Batch 3/64 loss: -0.28164801001548767
Batch 4/64 loss: -0.2736205756664276
Batch 5/64 loss: -0.288798063993454
Batch 6/64 loss: -0.287001371383667
Batch 7/64 loss: -0.2934730648994446
Batch 8/64 loss: -0.29251572489738464
Batch 9/64 loss: -0.28697097301483154
Batch 10/64 loss: -0.295602023601532
Batch 11/64 loss: -0.2962041199207306
Batch 12/64 loss: -0.2939903438091278
Batch 13/64 loss: -0.30173927545547485
Batch 14/64 loss: -0.29093390703201294
Batch 15/64 loss: -0.2854154407978058
Batch 16/64 loss: -0.29417985677719116
Batch 17/64 loss: -0.28568097949028015
Batch 18/64 loss: -0.2865830063819885
Batch 19/64 loss: -0.2878066897392273
Batch 20/64 loss: -0.29636287689208984
Batch 21/64 loss: -0.2999061346054077
Batch 22/64 loss: -0.2949635088443756
Batch 23/64 loss: -0.2819361686706543
Batch 24/64 loss: -0.27888399362564087
Batch 25/64 loss: -0.29823821783065796
Batch 26/64 loss: -0.28073233366012573
Batch 27/64 loss: -0.27846652269363403
Batch 28/64 loss: -0.2711971402168274
Batch 29/64 loss: -0.2918902039527893
Batch 30/64 loss: -0.28251123428344727
Batch 31/64 loss: -0.28698664903640747
Batch 32/64 loss: -0.277631938457489
Batch 33/64 loss: -0.27907711267471313
Batch 34/64 loss: -0.2840174436569214
Batch 35/64 loss: -0.289288729429245
Batch 36/64 loss: -0.29616597294807434
Batch 37/64 loss: -0.29045817255973816
Batch 38/64 loss: -0.29550889134407043
Batch 39/64 loss: -0.28702545166015625
Batch 40/64 loss: -0.28662219643592834
Batch 41/64 loss: -0.283742755651474
Batch 42/64 loss: -0.30263376235961914
Batch 43/64 loss: -0.29134154319763184
Batch 44/64 loss: -0.29712003469467163
Batch 45/64 loss: -0.2760746479034424
Batch 46/64 loss: -0.28252026438713074
Batch 47/64 loss: -0.2712234854698181
Batch 48/64 loss: -0.2828693091869354
Batch 49/64 loss: -0.2870669662952423
Batch 50/64 loss: -0.28808683156967163
Batch 51/64 loss: -0.2920609712600708
Batch 52/64 loss: -0.28547418117523193
Batch 53/64 loss: -0.30193647742271423
Batch 54/64 loss: -0.28464722633361816
Batch 55/64 loss: -0.30044206976890564
Batch 56/64 loss: -0.27607059478759766
Batch 57/64 loss: -0.29070842266082764
Batch 58/64 loss: -0.28190547227859497
Batch 59/64 loss: -0.28297197818756104
Batch 60/64 loss: -0.2965778112411499
Batch 61/64 loss: -0.2787267565727234
Batch 62/64 loss: -0.2876949906349182
Batch 63/64 loss: -0.2669358253479004
Batch 64/64 loss: -0.28413909673690796
Epoch 321  Train loss: -0.2878020307596992  Val loss: -0.24845318962208593
Epoch 322
-------------------------------
Batch 1/64 loss: -0.2833057641983032
Batch 2/64 loss: -0.2967916429042816
Batch 3/64 loss: -0.2911985516548157
Batch 4/64 loss: -0.297095388174057
Batch 5/64 loss: -0.29916349053382874
Batch 6/64 loss: -0.2828301787376404
Batch 7/64 loss: -0.29274654388427734
Batch 8/64 loss: -0.2938126027584076
Batch 9/64 loss: -0.2969191074371338
Batch 10/64 loss: -0.28573256731033325
Batch 11/64 loss: -0.2905551493167877
Batch 12/64 loss: -0.29132920503616333
Batch 13/64 loss: -0.29088085889816284
Batch 14/64 loss: -0.27930542826652527
Batch 15/64 loss: -0.29272282123565674
Batch 16/64 loss: -0.28039056062698364
Batch 17/64 loss: -0.29016149044036865
Batch 18/64 loss: -0.2944408357143402
Batch 19/64 loss: -0.2917187213897705
Batch 20/64 loss: -0.2840821146965027
Batch 21/64 loss: -0.28616097569465637
Batch 22/64 loss: -0.2861398458480835
Batch 23/64 loss: -0.2869052290916443
Batch 24/64 loss: -0.2914392352104187
Batch 25/64 loss: -0.2890695631504059
Batch 26/64 loss: -0.2939477264881134
Batch 27/64 loss: -0.29507768154144287
Batch 28/64 loss: -0.2853165864944458
Batch 29/64 loss: -0.2874767482280731
Batch 30/64 loss: -0.2926953434944153
Batch 31/64 loss: -0.2819983959197998
Batch 32/64 loss: -0.2788877487182617
Batch 33/64 loss: -0.2857675850391388
Batch 34/64 loss: -0.28869321942329407
Batch 35/64 loss: -0.29481983184814453
Batch 36/64 loss: -0.2755591869354248
Batch 37/64 loss: -0.28903114795684814
Batch 38/64 loss: -0.28931623697280884
Batch 39/64 loss: -0.28538060188293457
Batch 40/64 loss: -0.2901824414730072
Batch 41/64 loss: -0.28236883878707886
Batch 42/64 loss: -0.2937551736831665
Batch 43/64 loss: -0.28242403268814087
Batch 44/64 loss: -0.3014953136444092
Batch 45/64 loss: -0.29274624586105347
Batch 46/64 loss: -0.2918233275413513
Batch 47/64 loss: -0.3057660162448883
Batch 48/64 loss: -0.2952580749988556
Batch 49/64 loss: -0.2840094566345215
Batch 50/64 loss: -0.2813211679458618
Batch 51/64 loss: -0.28344297409057617
Batch 52/64 loss: -0.2839946150779724
Batch 53/64 loss: -0.30449071526527405
Batch 54/64 loss: -0.2698439359664917
Batch 55/64 loss: -0.3005942702293396
Batch 56/64 loss: -0.28282469511032104
Batch 57/64 loss: -0.2801477015018463
Batch 58/64 loss: -0.2839328646659851
Batch 59/64 loss: -0.28886792063713074
Batch 60/64 loss: -0.27833908796310425
Batch 61/64 loss: -0.2921423316001892
Batch 62/64 loss: -0.2877522110939026
Batch 63/64 loss: -0.2791333794593811
Batch 64/64 loss: -0.2777177393436432
Epoch 322  Train loss: -0.28853036878155724  Val loss: -0.26455874367268223
Epoch 323
-------------------------------
Batch 1/64 loss: -0.27773797512054443
Batch 2/64 loss: -0.30269408226013184
Batch 3/64 loss: -0.28420883417129517
Batch 4/64 loss: -0.28547024726867676
Batch 5/64 loss: -0.2823774218559265
Batch 6/64 loss: -0.29272404313087463
Batch 7/64 loss: -0.2941701114177704
Batch 8/64 loss: -0.2927688956260681
Batch 9/64 loss: -0.290528804063797
Batch 10/64 loss: -0.2938954830169678
Batch 11/64 loss: -0.27366191148757935
Batch 12/64 loss: -0.2902255654335022
Batch 13/64 loss: -0.2939718961715698
Batch 14/64 loss: -0.29096317291259766
Batch 15/64 loss: -0.2902453541755676
Batch 16/64 loss: -0.2929859161376953
Batch 17/64 loss: -0.3025053143501282
Batch 18/64 loss: -0.28558415174484253
Batch 19/64 loss: -0.29670190811157227
Batch 20/64 loss: -0.29317766427993774
Batch 21/64 loss: -0.2992354929447174
Batch 22/64 loss: -0.2951114773750305
Batch 23/64 loss: -0.29256901144981384
Batch 24/64 loss: -0.2855170965194702
Batch 25/64 loss: -0.2982679307460785
Batch 26/64 loss: -0.296646386384964
Batch 27/64 loss: -0.28741133213043213
Batch 28/64 loss: -0.29150593280792236
Batch 29/64 loss: -0.2803257703781128
Batch 30/64 loss: -0.3051074743270874
Batch 31/64 loss: -0.29512277245521545
Batch 32/64 loss: -0.2777864933013916
Batch 33/64 loss: -0.3051714301109314
Batch 34/64 loss: -0.28888702392578125
Batch 35/64 loss: -0.28956907987594604
Batch 36/64 loss: -0.2845161557197571
Batch 37/64 loss: -0.290799617767334
Batch 38/64 loss: -0.28146010637283325
Batch 39/64 loss: -0.2964729070663452
Batch 40/64 loss: -0.29474085569381714
Batch 41/64 loss: -0.28564414381980896
Batch 42/64 loss: -0.2932952642440796
Batch 43/64 loss: -0.27174097299575806
Batch 44/64 loss: -0.2801295220851898
Batch 45/64 loss: -0.26129400730133057
Batch 46/64 loss: -0.2783493399620056
Batch 47/64 loss: -0.28714215755462646
Batch 48/64 loss: -0.28892531991004944
Batch 49/64 loss: -0.28711560368537903
Batch 50/64 loss: -0.28704726696014404
Batch 51/64 loss: -0.2829017639160156
Batch 52/64 loss: -0.280488520860672
Batch 53/64 loss: -0.2811468541622162
Batch 54/64 loss: -0.2932722270488739
Batch 55/64 loss: -0.2764192819595337
Batch 56/64 loss: -0.26822829246520996
Batch 57/64 loss: -0.28929853439331055
Batch 58/64 loss: -0.28537890315055847
Batch 59/64 loss: -0.2718057632446289
Batch 60/64 loss: -0.2865651845932007
Batch 61/64 loss: -0.27980148792266846
Batch 62/64 loss: -0.2857716381549835
Batch 63/64 loss: -0.2767648696899414
Batch 64/64 loss: -0.297412633895874
Epoch 323  Train loss: -0.2877554430681116  Val loss: -0.26064203162373545
Epoch 324
-------------------------------
Batch 1/64 loss: -0.29902857542037964
Batch 2/64 loss: -0.27334606647491455
Batch 3/64 loss: -0.280682235956192
Batch 4/64 loss: -0.28505098819732666
Batch 5/64 loss: -0.2874124348163605
Batch 6/64 loss: -0.28604209423065186
Batch 7/64 loss: -0.28240591287612915
Batch 8/64 loss: -0.2817506194114685
Batch 9/64 loss: -0.28893375396728516
Batch 10/64 loss: -0.2930249571800232
Batch 11/64 loss: -0.281759113073349
Batch 12/64 loss: -0.2987511157989502
Batch 13/64 loss: -0.27817654609680176
Batch 14/64 loss: -0.2772068381309509
Batch 15/64 loss: -0.2834770381450653
Batch 16/64 loss: -0.28311994671821594
Batch 17/64 loss: -0.2803722023963928
Batch 18/64 loss: -0.28676337003707886
Batch 19/64 loss: -0.29035860300064087
Batch 20/64 loss: -0.2835085988044739
Batch 21/64 loss: -0.2832973599433899
Batch 22/64 loss: -0.2813248336315155
Batch 23/64 loss: -0.2712576687335968
Batch 24/64 loss: -0.2813689708709717
Batch 25/64 loss: -0.28188762068748474
Batch 26/64 loss: -0.2780018150806427
Batch 27/64 loss: -0.29178062081336975
Batch 28/64 loss: -0.2902013063430786
Batch 29/64 loss: -0.2796369791030884
Batch 30/64 loss: -0.28822001814842224
Batch 31/64 loss: -0.2804787755012512
Batch 32/64 loss: -0.28924599289894104
Batch 33/64 loss: -0.2850830554962158
Batch 34/64 loss: -0.2788844108581543
Batch 35/64 loss: -0.2889096438884735
Batch 36/64 loss: -0.2842893600463867
Batch 37/64 loss: -0.28777727484703064
Batch 38/64 loss: -0.27803465723991394
Batch 39/64 loss: -0.29506462812423706
Batch 40/64 loss: -0.2807931900024414
Batch 41/64 loss: -0.2841419577598572
Batch 42/64 loss: -0.2718743681907654
Batch 43/64 loss: -0.2809487283229828
Batch 44/64 loss: -0.2921735346317291
Batch 45/64 loss: -0.28769493103027344
Batch 46/64 loss: -0.281318724155426
Batch 47/64 loss: -0.2860783636569977
Batch 48/64 loss: -0.2915417551994324
Batch 49/64 loss: -0.2981531620025635
Batch 50/64 loss: -0.28000563383102417
Batch 51/64 loss: -0.28095096349716187
Batch 52/64 loss: -0.2861080765724182
Batch 53/64 loss: -0.28715693950653076
Batch 54/64 loss: -0.2908816933631897
Batch 55/64 loss: -0.2972185015678406
Batch 56/64 loss: -0.2941504716873169
Batch 57/64 loss: -0.27892839908599854
Batch 58/64 loss: -0.29740723967552185
Batch 59/64 loss: -0.2834795415401459
Batch 60/64 loss: -0.28875666856765747
Batch 61/64 loss: -0.282201886177063
Batch 62/64 loss: -0.2921081781387329
Batch 63/64 loss: -0.2903904318809509
Batch 64/64 loss: -0.28320184350013733
Epoch 324  Train loss: -0.28537695255934026  Val loss: -0.2587029423910318
Epoch 325
-------------------------------
Batch 1/64 loss: -0.28572678565979004
Batch 2/64 loss: -0.2781594395637512
Batch 3/64 loss: -0.29529666900634766
Batch 4/64 loss: -0.28468355536460876
Batch 5/64 loss: -0.28455138206481934
Batch 6/64 loss: -0.2794595956802368
Batch 7/64 loss: -0.28753426671028137
Batch 8/64 loss: -0.2826225757598877
Batch 9/64 loss: -0.2815065383911133
Batch 10/64 loss: -0.2939260005950928
Batch 11/64 loss: -0.2861974239349365
Batch 12/64 loss: -0.28594082593917847
Batch 13/64 loss: -0.28953972458839417
Batch 14/64 loss: -0.2722699046134949
Batch 15/64 loss: -0.29056236147880554
Batch 16/64 loss: -0.2964608669281006
Batch 17/64 loss: -0.3040847182273865
Batch 18/64 loss: -0.2912755608558655
Batch 19/64 loss: -0.2789503335952759
Batch 20/64 loss: -0.2822675108909607
Batch 21/64 loss: -0.2969524562358856
Batch 22/64 loss: -0.28924018144607544
Batch 23/64 loss: -0.29309672117233276
Batch 24/64 loss: -0.28738540410995483
Batch 25/64 loss: -0.27373868227005005
Batch 26/64 loss: -0.2883673906326294
Batch 27/64 loss: -0.2976163625717163
Batch 28/64 loss: -0.2871154844760895
Batch 29/64 loss: -0.27167224884033203
Batch 30/64 loss: -0.28079718351364136
Batch 31/64 loss: -0.2824143171310425
Batch 32/64 loss: -0.2918854355812073
Batch 33/64 loss: -0.2830582559108734
Batch 34/64 loss: -0.2908737063407898
Batch 35/64 loss: -0.2899070978164673
Batch 36/64 loss: -0.2864096462726593
Batch 37/64 loss: -0.27730119228363037
Batch 38/64 loss: -0.2890286445617676
Batch 39/64 loss: -0.28428739309310913
Batch 40/64 loss: -0.2984510660171509
Batch 41/64 loss: -0.294355183839798
Batch 42/64 loss: -0.2813708186149597
Batch 43/64 loss: -0.2853955030441284
Batch 44/64 loss: -0.2910667657852173
Batch 45/64 loss: -0.28266602754592896
Batch 46/64 loss: -0.28222784399986267
Batch 47/64 loss: -0.291736900806427
Batch 48/64 loss: -0.2828441858291626
Batch 49/64 loss: -0.27877557277679443
Batch 50/64 loss: -0.2955089211463928
Batch 51/64 loss: -0.3025778532028198
Batch 52/64 loss: -0.28317320346832275
Batch 53/64 loss: -0.2892856001853943
Batch 54/64 loss: -0.2824854850769043
Batch 55/64 loss: -0.30112844705581665
Batch 56/64 loss: -0.28899049758911133
Batch 57/64 loss: -0.29034948348999023
Batch 58/64 loss: -0.29410016536712646
Batch 59/64 loss: -0.28078874945640564
Batch 60/64 loss: -0.2913311719894409
Batch 61/64 loss: -0.3056403398513794
Batch 62/64 loss: -0.28127771615982056
Batch 63/64 loss: -0.2943459749221802
Batch 64/64 loss: -0.2834762930870056
Epoch 325  Train loss: -0.28760226731206856  Val loss: -0.26340315923658025
Epoch 326
-------------------------------
Batch 1/64 loss: -0.2903965711593628
Batch 2/64 loss: -0.2920597493648529
Batch 3/64 loss: -0.29757606983184814
Batch 4/64 loss: -0.28395161032676697
Batch 5/64 loss: -0.2980324625968933
Batch 6/64 loss: -0.2764844000339508
Batch 7/64 loss: -0.2812238931655884
Batch 8/64 loss: -0.2876719832420349
Batch 9/64 loss: -0.2974381744861603
Batch 10/64 loss: -0.26688170433044434
Batch 11/64 loss: -0.2879359722137451
Batch 12/64 loss: -0.2870390713214874
Batch 13/64 loss: -0.2865659296512604
Batch 14/64 loss: -0.286999374628067
Batch 15/64 loss: -0.2932649850845337
Batch 16/64 loss: -0.28199902176856995
Batch 17/64 loss: -0.29377707839012146
Batch 18/64 loss: -0.2952175736427307
Batch 19/64 loss: -0.2836175560951233
Batch 20/64 loss: -0.29210221767425537
Batch 21/64 loss: -0.28802603483200073
Batch 22/64 loss: -0.29499465227127075
Batch 23/64 loss: -0.29499563574790955
Batch 24/64 loss: -0.28755688667297363
Batch 25/64 loss: -0.2972167730331421
Batch 26/64 loss: -0.2838761806488037
Batch 27/64 loss: -0.28522902727127075
Batch 28/64 loss: -0.28676652908325195
Batch 29/64 loss: -0.2895510792732239
Batch 30/64 loss: -0.2799381613731384
Batch 31/64 loss: -0.30287033319473267
Batch 32/64 loss: -0.2845771610736847
Batch 33/64 loss: -0.289048969745636
Batch 34/64 loss: -0.29433515667915344
Batch 35/64 loss: -0.29067063331604004
Batch 36/64 loss: -0.2817089259624481
Batch 37/64 loss: -0.2904582619667053
Batch 38/64 loss: -0.27767035365104675
Batch 39/64 loss: -0.2706249952316284
Batch 40/64 loss: -0.28276321291923523
Batch 41/64 loss: -0.2927801012992859
Batch 42/64 loss: -0.2908189594745636
Batch 43/64 loss: -0.29054367542266846
Batch 44/64 loss: -0.28568488359451294
Batch 45/64 loss: -0.29115721583366394
Batch 46/64 loss: -0.28714197874069214
Batch 47/64 loss: -0.27963370084762573
Batch 48/64 loss: -0.2898491621017456
Batch 49/64 loss: -0.2900059223175049
Batch 50/64 loss: -0.2839618921279907
Batch 51/64 loss: -0.28593888878822327
Batch 52/64 loss: -0.28464579582214355
Batch 53/64 loss: -0.2941942811012268
Batch 54/64 loss: -0.288880854845047
Batch 55/64 loss: -0.29207175970077515
Batch 56/64 loss: -0.28622621297836304
Batch 57/64 loss: -0.2718130946159363
Batch 58/64 loss: -0.2819077968597412
Batch 59/64 loss: -0.267289936542511
Batch 60/64 loss: -0.2654038667678833
Batch 61/64 loss: -0.27743151783943176
Batch 62/64 loss: -0.29342204332351685
Batch 63/64 loss: -0.3011976480484009
Batch 64/64 loss: -0.2872041165828705
Epoch 326  Train loss: -0.28706695904918744  Val loss: -0.26205544701146916
Epoch 327
-------------------------------
Batch 1/64 loss: -0.286715030670166
Batch 2/64 loss: -0.2954367399215698
Batch 3/64 loss: -0.27979564666748047
Batch 4/64 loss: -0.291412889957428
Batch 5/64 loss: -0.287270188331604
Batch 6/64 loss: -0.28967413306236267
Batch 7/64 loss: -0.27610406279563904
Batch 8/64 loss: -0.26937317848205566
Batch 9/64 loss: -0.28275066614151
Batch 10/64 loss: -0.28340768814086914
Batch 11/64 loss: -0.2788020670413971
Batch 12/64 loss: -0.27192264795303345
Batch 13/64 loss: -0.29664045572280884
Batch 14/64 loss: -0.27270179986953735
Batch 15/64 loss: -0.2895871698856354
Batch 16/64 loss: -0.27963054180145264
Batch 17/64 loss: -0.2844081521034241
Batch 18/64 loss: -0.2826724052429199
Batch 19/64 loss: -0.285388320684433
Batch 20/64 loss: -0.2761176824569702
Batch 21/64 loss: -0.2835004925727844
Batch 22/64 loss: -0.28679922223091125
Batch 23/64 loss: -0.29532286524772644
Batch 24/64 loss: -0.27906832098960876
Batch 25/64 loss: -0.2910814881324768
Batch 26/64 loss: -0.27731531858444214
Batch 27/64 loss: -0.2905145287513733
Batch 28/64 loss: -0.2815902829170227
Batch 29/64 loss: -0.2723960280418396
Batch 30/64 loss: -0.2813699245452881
Batch 31/64 loss: -0.2847033739089966
Batch 32/64 loss: -0.2911365330219269
Batch 33/64 loss: -0.2842216193675995
Batch 34/64 loss: -0.2836369276046753
Batch 35/64 loss: -0.28649136424064636
Batch 36/64 loss: -0.27931028604507446
Batch 37/64 loss: -0.27826571464538574
Batch 38/64 loss: -0.2824837863445282
Batch 39/64 loss: -0.2828720808029175
Batch 40/64 loss: -0.2914928197860718
Batch 41/64 loss: -0.28678402304649353
Batch 42/64 loss: -0.2897889018058777
Batch 43/64 loss: -0.28708186745643616
Batch 44/64 loss: -0.2803787589073181
Batch 45/64 loss: -0.2853476405143738
Batch 46/64 loss: -0.2811221182346344
Batch 47/64 loss: -0.25877106189727783
Batch 48/64 loss: -0.27557116746902466
Batch 49/64 loss: -0.27901721000671387
Batch 50/64 loss: -0.28350013494491577
Batch 51/64 loss: -0.2775122821331024
Batch 52/64 loss: -0.2757489085197449
Batch 53/64 loss: -0.27626341581344604
Batch 54/64 loss: -0.28789380192756653
Batch 55/64 loss: -0.2936782240867615
Batch 56/64 loss: -0.28665000200271606
Batch 57/64 loss: -0.27809059619903564
Batch 58/64 loss: -0.2768439054489136
Batch 59/64 loss: -0.2830653786659241
Batch 60/64 loss: -0.2901122570037842
Batch 61/64 loss: -0.2744438946247101
Batch 62/64 loss: -0.2883063554763794
Batch 63/64 loss: -0.28919148445129395
Batch 64/64 loss: -0.28800588846206665
Epoch 327  Train loss: -0.2830518000266131  Val loss: -0.25020235285316544
Epoch 328
-------------------------------
Batch 1/64 loss: -0.26476937532424927
Batch 2/64 loss: -0.2665526866912842
Batch 3/64 loss: -0.28246009349823
Batch 4/64 loss: -0.2844787836074829
Batch 5/64 loss: -0.2718200087547302
Batch 6/64 loss: -0.2601635456085205
Batch 7/64 loss: -0.27119553089141846
Batch 8/64 loss: -0.2669210433959961
Batch 9/64 loss: -0.28422296047210693
Batch 10/64 loss: -0.27734827995300293
Batch 11/64 loss: -0.25918275117874146
Batch 12/64 loss: -0.27207404375076294
Batch 13/64 loss: -0.2825692296028137
Batch 14/64 loss: -0.2741556167602539
Batch 15/64 loss: -0.2814486026763916
Batch 16/64 loss: -0.2754027247428894
Batch 17/64 loss: -0.27617931365966797
Batch 18/64 loss: -0.28203386068344116
Batch 19/64 loss: -0.284986674785614
Batch 20/64 loss: -0.2756851613521576
Batch 21/64 loss: -0.26695507764816284
Batch 22/64 loss: -0.2800334692001343
Batch 23/64 loss: -0.2596511244773865
Batch 24/64 loss: -0.2789279818534851
Batch 25/64 loss: -0.2824839949607849
Batch 26/64 loss: -0.27129149436950684
Batch 27/64 loss: -0.274444043636322
Batch 28/64 loss: -0.27588075399398804
Batch 29/64 loss: -0.2625861167907715
Batch 30/64 loss: -0.28408533334732056
Batch 31/64 loss: -0.2885773181915283
Batch 32/64 loss: -0.29902154207229614
Batch 33/64 loss: -0.2853161096572876
Batch 34/64 loss: -0.2806780934333801
Batch 35/64 loss: -0.26862192153930664
Batch 36/64 loss: -0.28304341435432434
Batch 37/64 loss: -0.27239537239074707
Batch 38/64 loss: -0.272818386554718
Batch 39/64 loss: -0.28189462423324585
Batch 40/64 loss: -0.27975672483444214
Batch 41/64 loss: -0.2817898392677307
Batch 42/64 loss: -0.28756487369537354
Batch 43/64 loss: -0.2871328294277191
Batch 44/64 loss: -0.27707046270370483
Batch 45/64 loss: -0.28703194856643677
Batch 46/64 loss: -0.2729755640029907
Batch 47/64 loss: -0.28154584765434265
Batch 48/64 loss: -0.27828386425971985
Batch 49/64 loss: -0.29121315479278564
Batch 50/64 loss: -0.2732303738594055
Batch 51/64 loss: -0.2901208698749542
Batch 52/64 loss: -0.29898399114608765
Batch 53/64 loss: -0.27995792031288147
Batch 54/64 loss: -0.28659987449645996
Batch 55/64 loss: -0.2921048700809479
Batch 56/64 loss: -0.3002569377422333
Batch 57/64 loss: -0.27860936522483826
Batch 58/64 loss: -0.29580414295196533
Batch 59/64 loss: -0.29718491435050964
Batch 60/64 loss: -0.2924191951751709
Batch 61/64 loss: -0.2820388078689575
Batch 62/64 loss: -0.29003995656967163
Batch 63/64 loss: -0.283461332321167
Batch 64/64 loss: -0.2941160798072815
Epoch 328  Train loss: -0.2799705283314574  Val loss: -0.2606606979140711
Epoch 329
-------------------------------
Batch 1/64 loss: -0.27707958221435547
Batch 2/64 loss: -0.27379536628723145
Batch 3/64 loss: -0.29924795031547546
Batch 4/64 loss: -0.2910583019256592
Batch 5/64 loss: -0.2642245888710022
Batch 6/64 loss: -0.27159368991851807
Batch 7/64 loss: -0.2840357720851898
Batch 8/64 loss: -0.2949289083480835
Batch 9/64 loss: -0.2897152304649353
Batch 10/64 loss: -0.2906630039215088
Batch 11/64 loss: -0.2819977104663849
Batch 12/64 loss: -0.2845993936061859
Batch 13/64 loss: -0.2999166250228882
Batch 14/64 loss: -0.28905147314071655
Batch 15/64 loss: -0.2818887233734131
Batch 16/64 loss: -0.3021247982978821
Batch 17/64 loss: -0.2891297936439514
Batch 18/64 loss: -0.28260040283203125
Batch 19/64 loss: -0.2794588506221771
Batch 20/64 loss: -0.2765399217605591
Batch 21/64 loss: -0.2934991121292114
Batch 22/64 loss: -0.2991374135017395
Batch 23/64 loss: -0.2909661531448364
Batch 24/64 loss: -0.28253570199012756
Batch 25/64 loss: -0.27557334303855896
Batch 26/64 loss: -0.27550897002220154
Batch 27/64 loss: -0.28471335768699646
Batch 28/64 loss: -0.2750300168991089
Batch 29/64 loss: -0.28016799688339233
Batch 30/64 loss: -0.27912306785583496
Batch 31/64 loss: -0.2746371626853943
Batch 32/64 loss: -0.2673928737640381
Batch 33/64 loss: -0.3054288625717163
Batch 34/64 loss: -0.2818162441253662
Batch 35/64 loss: -0.2986893355846405
Batch 36/64 loss: -0.2994633913040161
Batch 37/64 loss: -0.28992795944213867
Batch 38/64 loss: -0.2813072204589844
Batch 39/64 loss: -0.27152499556541443
Batch 40/64 loss: -0.27893587946891785
Batch 41/64 loss: -0.2761119306087494
Batch 42/64 loss: -0.28764188289642334
Batch 43/64 loss: -0.27389106154441833
Batch 44/64 loss: -0.2690168023109436
Batch 45/64 loss: -0.2908800542354584
Batch 46/64 loss: -0.2973887324333191
Batch 47/64 loss: -0.2889382243156433
Batch 48/64 loss: -0.27236637473106384
Batch 49/64 loss: -0.27534958720207214
Batch 50/64 loss: -0.29280000925064087
Batch 51/64 loss: -0.2847921848297119
Batch 52/64 loss: -0.2865075170993805
Batch 53/64 loss: -0.2864740192890167
Batch 54/64 loss: -0.2889638841152191
Batch 55/64 loss: -0.27149638533592224
Batch 56/64 loss: -0.279257208108902
Batch 57/64 loss: -0.2775573134422302
Batch 58/64 loss: -0.29233673214912415
Batch 59/64 loss: -0.2914808392524719
Batch 60/64 loss: -0.302015483379364
Batch 61/64 loss: -0.2794283330440521
Batch 62/64 loss: -0.29240885376930237
Batch 63/64 loss: -0.29342013597488403
Batch 64/64 loss: -0.2720015048980713
Epoch 329  Train loss: -0.2846047658546298  Val loss: -0.2520374981398435
Epoch 330
-------------------------------
Batch 1/64 loss: -0.27827754616737366
Batch 2/64 loss: -0.28592050075531006
Batch 3/64 loss: -0.274563193321228
Batch 4/64 loss: -0.2854444980621338
Batch 5/64 loss: -0.2863674759864807
Batch 6/64 loss: -0.29529744386672974
Batch 7/64 loss: -0.2914860248565674
Batch 8/64 loss: -0.2799956500530243
Batch 9/64 loss: -0.27473461627960205
Batch 10/64 loss: -0.2887839078903198
Batch 11/64 loss: -0.28190910816192627
Batch 12/64 loss: -0.29097074270248413
Batch 13/64 loss: -0.2766912579536438
Batch 14/64 loss: -0.2699793577194214
Batch 15/64 loss: -0.2897512912750244
Batch 16/64 loss: -0.2737565338611603
Batch 17/64 loss: -0.2851390838623047
Batch 18/64 loss: -0.2810036540031433
Batch 19/64 loss: -0.27205973863601685
Batch 20/64 loss: -0.27702903747558594
Batch 21/64 loss: -0.2690572440624237
Batch 22/64 loss: -0.27372288703918457
Batch 23/64 loss: -0.2694525718688965
Batch 24/64 loss: -0.2800636291503906
Batch 25/64 loss: -0.27312034368515015
Batch 26/64 loss: -0.2879822850227356
Batch 27/64 loss: -0.2834833562374115
Batch 28/64 loss: -0.2767718434333801
Batch 29/64 loss: -0.2864273190498352
Batch 30/64 loss: -0.28859245777130127
Batch 31/64 loss: -0.2771610617637634
Batch 32/64 loss: -0.26536035537719727
Batch 33/64 loss: -0.28714627027511597
Batch 34/64 loss: -0.28728342056274414
Batch 35/64 loss: -0.2967289984226227
Batch 36/64 loss: -0.2874005436897278
Batch 37/64 loss: -0.27531296014785767
Batch 38/64 loss: -0.29424139857292175
Batch 39/64 loss: -0.29073566198349
Batch 40/64 loss: -0.28982439637184143
Batch 41/64 loss: -0.2785723805427551
Batch 42/64 loss: -0.27872568368911743
Batch 43/64 loss: -0.28802433609962463
Batch 44/64 loss: -0.29266098141670227
Batch 45/64 loss: -0.2995920479297638
Batch 46/64 loss: -0.2756832540035248
Batch 47/64 loss: -0.28426820039749146
Batch 48/64 loss: -0.29226118326187134
Batch 49/64 loss: -0.28538569808006287
Batch 50/64 loss: -0.2833471894264221
Batch 51/64 loss: -0.29124462604522705
Batch 52/64 loss: -0.29039716720581055
Batch 53/64 loss: -0.28554394841194153
Batch 54/64 loss: -0.2911927402019501
Batch 55/64 loss: -0.2831871509552002
Batch 56/64 loss: -0.30050724744796753
Batch 57/64 loss: -0.2791329026222229
Batch 58/64 loss: -0.29963216185569763
Batch 59/64 loss: -0.29308730363845825
Batch 60/64 loss: -0.2615712285041809
Batch 61/64 loss: -0.29384857416152954
Batch 62/64 loss: -0.28831663727760315
Batch 63/64 loss: -0.2872633934020996
Batch 64/64 loss: -0.28193795680999756
Epoch 330  Train loss: -0.28382633994607365  Val loss: -0.26396502887260465
Epoch 331
-------------------------------
Batch 1/64 loss: -0.2928432822227478
Batch 2/64 loss: -0.29287970066070557
Batch 3/64 loss: -0.2856137156486511
Batch 4/64 loss: -0.28455573320388794
Batch 5/64 loss: -0.28559789061546326
Batch 6/64 loss: -0.28175878524780273
Batch 7/64 loss: -0.28740984201431274
Batch 8/64 loss: -0.28767332434654236
Batch 9/64 loss: -0.2897125482559204
Batch 10/64 loss: -0.2854456305503845
Batch 11/64 loss: -0.2961123585700989
Batch 12/64 loss: -0.2865660786628723
Batch 13/64 loss: -0.2967489957809448
Batch 14/64 loss: -0.2926042079925537
Batch 15/64 loss: -0.28580766916275024
Batch 16/64 loss: -0.28608354926109314
Batch 17/64 loss: -0.27733051776885986
Batch 18/64 loss: -0.28625601530075073
Batch 19/64 loss: -0.28388720750808716
Batch 20/64 loss: -0.29205945134162903
Batch 21/64 loss: -0.2789529263973236
Batch 22/64 loss: -0.2777788043022156
Batch 23/64 loss: -0.2921730875968933
Batch 24/64 loss: -0.27600109577178955
Batch 25/64 loss: -0.28096187114715576
Batch 26/64 loss: -0.2851598858833313
Batch 27/64 loss: -0.27582621574401855
Batch 28/64 loss: -0.27637118101119995
Batch 29/64 loss: -0.29128628969192505
Batch 30/64 loss: -0.2809262275695801
Batch 31/64 loss: -0.28641074895858765
Batch 32/64 loss: -0.2760409116744995
Batch 33/64 loss: -0.2726605534553528
Batch 34/64 loss: -0.2798566222190857
Batch 35/64 loss: -0.2897309362888336
Batch 36/64 loss: -0.28666597604751587
Batch 37/64 loss: -0.2899765372276306
Batch 38/64 loss: -0.2852339744567871
Batch 39/64 loss: -0.2822255492210388
Batch 40/64 loss: -0.2907181978225708
Batch 41/64 loss: -0.30277493596076965
Batch 42/64 loss: -0.30132198333740234
Batch 43/64 loss: -0.28601181507110596
Batch 44/64 loss: -0.2867462635040283
Batch 45/64 loss: -0.28155145049095154
Batch 46/64 loss: -0.2764759957790375
Batch 47/64 loss: -0.2895597815513611
Batch 48/64 loss: -0.27848026156425476
Batch 49/64 loss: -0.29141634702682495
Batch 50/64 loss: -0.2918950319290161
Batch 51/64 loss: -0.28970271348953247
Batch 52/64 loss: -0.2771393954753876
Batch 53/64 loss: -0.2835032343864441
Batch 54/64 loss: -0.26530706882476807
Batch 55/64 loss: -0.2850158214569092
Batch 56/64 loss: -0.28173205256462097
Batch 57/64 loss: -0.2975936830043793
Batch 58/64 loss: -0.2942352294921875
Batch 59/64 loss: -0.2760602533817291
Batch 60/64 loss: -0.2944541573524475
Batch 61/64 loss: -0.3001673221588135
Batch 62/64 loss: -0.2837122082710266
Batch 63/64 loss: -0.2937639653682709
Batch 64/64 loss: -0.2967050075531006
Epoch 331  Train loss: -0.2861655501758351  Val loss: -0.2702254921300305
Saving best model, epoch: 331
Epoch 332
-------------------------------
Batch 1/64 loss: -0.2924548089504242
Batch 2/64 loss: -0.2993207573890686
Batch 3/64 loss: -0.27720779180526733
Batch 4/64 loss: -0.29036232829093933
Batch 5/64 loss: -0.28350692987442017
Batch 6/64 loss: -0.2852174639701843
Batch 7/64 loss: -0.2734668254852295
Batch 8/64 loss: -0.2897337079048157
Batch 9/64 loss: -0.29012376070022583
Batch 10/64 loss: -0.28011947870254517
Batch 11/64 loss: -0.2742917835712433
Batch 12/64 loss: -0.28336167335510254
Batch 13/64 loss: -0.2911869287490845
Batch 14/64 loss: -0.28687772154808044
Batch 15/64 loss: -0.28529343008995056
Batch 16/64 loss: -0.2942437529563904
Batch 17/64 loss: -0.2892455756664276
Batch 18/64 loss: -0.2948063015937805
Batch 19/64 loss: -0.29337334632873535
Batch 20/64 loss: -0.2975318729877472
Batch 21/64 loss: -0.2904089093208313
Batch 22/64 loss: -0.2918122410774231
Batch 23/64 loss: -0.27818846702575684
Batch 24/64 loss: -0.2861553728580475
Batch 25/64 loss: -0.30027371644973755
Batch 26/64 loss: -0.29704511165618896
Batch 27/64 loss: -0.2947652041912079
Batch 28/64 loss: -0.2898213565349579
Batch 29/64 loss: -0.287108838558197
Batch 30/64 loss: -0.27996957302093506
Batch 31/64 loss: -0.29173600673675537
Batch 32/64 loss: -0.27521321177482605
Batch 33/64 loss: -0.26518189907073975
Batch 34/64 loss: -0.291456013917923
Batch 35/64 loss: -0.29113295674324036
Batch 36/64 loss: -0.29503148794174194
Batch 37/64 loss: -0.2663654088973999
Batch 38/64 loss: -0.2962062954902649
Batch 39/64 loss: -0.3040807843208313
Batch 40/64 loss: -0.27195048332214355
Batch 41/64 loss: -0.2872810959815979
Batch 42/64 loss: -0.28467991948127747
Batch 43/64 loss: -0.30239012837409973
Batch 44/64 loss: -0.28597292304039
Batch 45/64 loss: -0.28997302055358887
Batch 46/64 loss: -0.2942202091217041
Batch 47/64 loss: -0.27706193923950195
Batch 48/64 loss: -0.2919013500213623
Batch 49/64 loss: -0.2924801707267761
Batch 50/64 loss: -0.28211086988449097
Batch 51/64 loss: -0.2976774573326111
Batch 52/64 loss: -0.28715217113494873
Batch 53/64 loss: -0.27402463555336
Batch 54/64 loss: -0.28813499212265015
Batch 55/64 loss: -0.27776408195495605
Batch 56/64 loss: -0.28661665320396423
Batch 57/64 loss: -0.275090754032135
Batch 58/64 loss: -0.2829762101173401
Batch 59/64 loss: -0.29238495230674744
Batch 60/64 loss: -0.2753291428089142
Batch 61/64 loss: -0.2907094955444336
Batch 62/64 loss: -0.2839493751525879
Batch 63/64 loss: -0.3010188937187195
Batch 64/64 loss: -0.2993013858795166
Epoch 332  Train loss: -0.28738833034739775  Val loss: -0.2619498463225938
Epoch 333
-------------------------------
Batch 1/64 loss: -0.2889193594455719
Batch 2/64 loss: -0.28663983941078186
Batch 3/64 loss: -0.2823074162006378
Batch 4/64 loss: -0.28914856910705566
Batch 5/64 loss: -0.28917205333709717
Batch 6/64 loss: -0.3021499216556549
Batch 7/64 loss: -0.29356908798217773
Batch 8/64 loss: -0.2987126111984253
Batch 9/64 loss: -0.29046517610549927
Batch 10/64 loss: -0.2927006483078003
Batch 11/64 loss: -0.2818637490272522
Batch 12/64 loss: -0.2971247732639313
Batch 13/64 loss: -0.30719929933547974
Batch 14/64 loss: -0.29434263706207275
Batch 15/64 loss: -0.28883931040763855
Batch 16/64 loss: -0.29683953523635864
Batch 17/64 loss: -0.2832532525062561
Batch 18/64 loss: -0.30216291546821594
Batch 19/64 loss: -0.28591376543045044
Batch 20/64 loss: -0.28049713373184204
Batch 21/64 loss: -0.29071396589279175
Batch 22/64 loss: -0.2891100347042084
Batch 23/64 loss: -0.2880786061286926
Batch 24/64 loss: -0.28155970573425293
Batch 25/64 loss: -0.2840699553489685
Batch 26/64 loss: -0.3016316294670105
Batch 27/64 loss: -0.2965153455734253
Batch 28/64 loss: -0.2709253430366516
Batch 29/64 loss: -0.2798196077346802
Batch 30/64 loss: -0.2958986759185791
Batch 31/64 loss: -0.2766318619251251
Batch 32/64 loss: -0.2741718590259552
Batch 33/64 loss: -0.2856125235557556
Batch 34/64 loss: -0.28632214665412903
Batch 35/64 loss: -0.27704036235809326
Batch 36/64 loss: -0.292153000831604
Batch 37/64 loss: -0.2755795121192932
Batch 38/64 loss: -0.2896538972854614
Batch 39/64 loss: -0.2776474952697754
Batch 40/64 loss: -0.28460943698883057
Batch 41/64 loss: -0.29856911301612854
Batch 42/64 loss: -0.28545624017715454
Batch 43/64 loss: -0.29740768671035767
Batch 44/64 loss: -0.28198522329330444
Batch 45/64 loss: -0.2857583165168762
Batch 46/64 loss: -0.29841291904449463
Batch 47/64 loss: -0.2899934947490692
Batch 48/64 loss: -0.27381789684295654
Batch 49/64 loss: -0.2866150140762329
Batch 50/64 loss: -0.2807818651199341
Batch 51/64 loss: -0.28430676460266113
Batch 52/64 loss: -0.2887706756591797
Batch 53/64 loss: -0.2664458155632019
Batch 54/64 loss: -0.29571858048439026
Batch 55/64 loss: -0.28670042753219604
Batch 56/64 loss: -0.2943945527076721
Batch 57/64 loss: -0.2956515848636627
Batch 58/64 loss: -0.28920456767082214
Batch 59/64 loss: -0.2856432795524597
Batch 60/64 loss: -0.29153066873550415
Batch 61/64 loss: -0.29477447271347046
Batch 62/64 loss: -0.2992800176143646
Batch 63/64 loss: -0.2886175513267517
Batch 64/64 loss: -0.2790094017982483
Epoch 333  Train loss: -0.28829270274031393  Val loss: -0.2682630384910557
Epoch 334
-------------------------------
Batch 1/64 loss: -0.295322984457016
Batch 2/64 loss: -0.29628050327301025
Batch 3/64 loss: -0.29300370812416077
Batch 4/64 loss: -0.28090548515319824
Batch 5/64 loss: -0.2921779453754425
Batch 6/64 loss: -0.28450295329093933
Batch 7/64 loss: -0.2868058681488037
Batch 8/64 loss: -0.29486626386642456
Batch 9/64 loss: -0.27584943175315857
Batch 10/64 loss: -0.2894652485847473
Batch 11/64 loss: -0.28694161772727966
Batch 12/64 loss: -0.27846938371658325
Batch 13/64 loss: -0.2870026230812073
Batch 14/64 loss: -0.30096176266670227
Batch 15/64 loss: -0.28249937295913696
Batch 16/64 loss: -0.29195496439933777
Batch 17/64 loss: -0.2970391511917114
Batch 18/64 loss: -0.2847368121147156
Batch 19/64 loss: -0.27126094698905945
Batch 20/64 loss: -0.304355263710022
Batch 21/64 loss: -0.2942485511302948
Batch 22/64 loss: -0.29921507835388184
Batch 23/64 loss: -0.28752657771110535
Batch 24/64 loss: -0.28134334087371826
Batch 25/64 loss: -0.303247332572937
Batch 26/64 loss: -0.28718316555023193
Batch 27/64 loss: -0.27723607420921326
Batch 28/64 loss: -0.28528133034706116
Batch 29/64 loss: -0.27593153715133667
Batch 30/64 loss: -0.30181729793548584
Batch 31/64 loss: -0.2819589674472809
Batch 32/64 loss: -0.2904781401157379
Batch 33/64 loss: -0.2888081669807434
Batch 34/64 loss: -0.2994198203086853
Batch 35/64 loss: -0.2911916673183441
Batch 36/64 loss: -0.29673510789871216
Batch 37/64 loss: -0.2828010320663452
Batch 38/64 loss: -0.2853957712650299
Batch 39/64 loss: -0.29681870341300964
Batch 40/64 loss: -0.27708154916763306
Batch 41/64 loss: -0.29625943303108215
Batch 42/64 loss: -0.28419798612594604
Batch 43/64 loss: -0.2881828546524048
Batch 44/64 loss: -0.281294047832489
Batch 45/64 loss: -0.2626984715461731
Batch 46/64 loss: -0.2862042784690857
Batch 47/64 loss: -0.287140429019928
Batch 48/64 loss: -0.2812521457672119
Batch 49/64 loss: -0.2671557068824768
Batch 50/64 loss: -0.28756287693977356
Batch 51/64 loss: -0.2754327654838562
Batch 52/64 loss: -0.2843784689903259
Batch 53/64 loss: -0.27728843688964844
Batch 54/64 loss: -0.2846922278404236
Batch 55/64 loss: -0.3007245659828186
Batch 56/64 loss: -0.2830498516559601
Batch 57/64 loss: -0.29151618480682373
Batch 58/64 loss: -0.2915801405906677
Batch 59/64 loss: -0.30224084854125977
Batch 60/64 loss: -0.27283555269241333
Batch 61/64 loss: -0.2879753112792969
Batch 62/64 loss: -0.2941361665725708
Batch 63/64 loss: -0.28886228799819946
Batch 64/64 loss: -0.3009122908115387
Epoch 334  Train loss: -0.28769359623684604  Val loss: -0.25686739195663083
Epoch 335
-------------------------------
Batch 1/64 loss: -0.2838583290576935
Batch 2/64 loss: -0.27626603841781616
Batch 3/64 loss: -0.27767413854599
Batch 4/64 loss: -0.28599807620048523
Batch 5/64 loss: -0.28387022018432617
Batch 6/64 loss: -0.29210448265075684
Batch 7/64 loss: -0.2853633165359497
Batch 8/64 loss: -0.2889010012149811
Batch 9/64 loss: -0.2784329056739807
Batch 10/64 loss: -0.28377601504325867
Batch 11/64 loss: -0.28525954484939575
Batch 12/64 loss: -0.2948319613933563
Batch 13/64 loss: -0.30793696641921997
Batch 14/64 loss: -0.28500401973724365
Batch 15/64 loss: -0.29056259989738464
Batch 16/64 loss: -0.27641990780830383
Batch 17/64 loss: -0.2650734782218933
Batch 18/64 loss: -0.2751989960670471
Batch 19/64 loss: -0.2991803288459778
Batch 20/64 loss: -0.28063997626304626
Batch 21/64 loss: -0.2921675443649292
Batch 22/64 loss: -0.2846717834472656
Batch 23/64 loss: -0.275634765625
Batch 24/64 loss: -0.27750062942504883
Batch 25/64 loss: -0.2972147464752197
Batch 26/64 loss: -0.2772942781448364
Batch 27/64 loss: -0.28924471139907837
Batch 28/64 loss: -0.2864789366722107
Batch 29/64 loss: -0.2928881049156189
Batch 30/64 loss: -0.28699594736099243
Batch 31/64 loss: -0.28571414947509766
Batch 32/64 loss: -0.2737870514392853
Batch 33/64 loss: -0.2810443341732025
Batch 34/64 loss: -0.2913321852684021
Batch 35/64 loss: -0.2962803244590759
Batch 36/64 loss: -0.2845405340194702
Batch 37/64 loss: -0.28659170866012573
Batch 38/64 loss: -0.29632776975631714
Batch 39/64 loss: -0.29492807388305664
Batch 40/64 loss: -0.29656341671943665
Batch 41/64 loss: -0.2915964424610138
Batch 42/64 loss: -0.2720506191253662
Batch 43/64 loss: -0.2735446095466614
Batch 44/64 loss: -0.2838500440120697
Batch 45/64 loss: -0.27580130100250244
Batch 46/64 loss: -0.2908765971660614
Batch 47/64 loss: -0.25515735149383545
Batch 48/64 loss: -0.28734150528907776
Batch 49/64 loss: -0.27689290046691895
Batch 50/64 loss: -0.28328120708465576
Batch 51/64 loss: -0.29271724820137024
Batch 52/64 loss: -0.28471970558166504
Batch 53/64 loss: -0.2942151427268982
Batch 54/64 loss: -0.27820250391960144
Batch 55/64 loss: -0.27739468216896057
Batch 56/64 loss: -0.274501234292984
Batch 57/64 loss: -0.28297585248947144
Batch 58/64 loss: -0.3049837350845337
Batch 59/64 loss: -0.29175257682800293
Batch 60/64 loss: -0.28772974014282227
Batch 61/64 loss: -0.27970024943351746
Batch 62/64 loss: -0.2772539258003235
Batch 63/64 loss: -0.27480876445770264
Batch 64/64 loss: -0.28689879179000854
Epoch 335  Train loss: -0.28470706402086743  Val loss: -0.25297631963421796
Epoch 336
-------------------------------
Batch 1/64 loss: -0.277343213558197
Batch 2/64 loss: -0.2971837520599365
Batch 3/64 loss: -0.28591522574424744
Batch 4/64 loss: -0.27664947509765625
Batch 5/64 loss: -0.28736838698387146
Batch 6/64 loss: -0.28439390659332275
Batch 7/64 loss: -0.28140175342559814
Batch 8/64 loss: -0.27829548716545105
Batch 9/64 loss: -0.2827078104019165
Batch 10/64 loss: -0.27029919624328613
Batch 11/64 loss: -0.29710233211517334
Batch 12/64 loss: -0.28522807359695435
Batch 13/64 loss: -0.2872923016548157
Batch 14/64 loss: -0.29540014266967773
Batch 15/64 loss: -0.28728187084198
Batch 16/64 loss: -0.29184824228286743
Batch 17/64 loss: -0.2913459539413452
Batch 18/64 loss: -0.2729599177837372
Batch 19/64 loss: -0.2880104184150696
Batch 20/64 loss: -0.28102707862854004
Batch 21/64 loss: -0.28776970505714417
Batch 22/64 loss: -0.27123889327049255
Batch 23/64 loss: -0.28073155879974365
Batch 24/64 loss: -0.28430458903312683
Batch 25/64 loss: -0.282559335231781
Batch 26/64 loss: -0.27556777000427246
Batch 27/64 loss: -0.27740657329559326
Batch 28/64 loss: -0.2877187728881836
Batch 29/64 loss: -0.276378333568573
Batch 30/64 loss: -0.3038495182991028
Batch 31/64 loss: -0.2879236936569214
Batch 32/64 loss: -0.2799912393093109
Batch 33/64 loss: -0.28596246242523193
Batch 34/64 loss: -0.28074318170547485
Batch 35/64 loss: -0.2963184714317322
Batch 36/64 loss: -0.28230607509613037
Batch 37/64 loss: -0.300152450799942
Batch 38/64 loss: -0.28105461597442627
Batch 39/64 loss: -0.28080201148986816
Batch 40/64 loss: -0.2933403253555298
Batch 41/64 loss: -0.2807357907295227
Batch 42/64 loss: -0.29464054107666016
Batch 43/64 loss: -0.2904209494590759
Batch 44/64 loss: -0.2965703010559082
Batch 45/64 loss: -0.2878926694393158
Batch 46/64 loss: -0.2893274426460266
Batch 47/64 loss: -0.2798890471458435
Batch 48/64 loss: -0.27485471963882446
Batch 49/64 loss: -0.28897008299827576
Batch 50/64 loss: -0.2809150815010071
Batch 51/64 loss: -0.27244752645492554
Batch 52/64 loss: -0.28670215606689453
Batch 53/64 loss: -0.2852542996406555
Batch 54/64 loss: -0.2833409905433655
Batch 55/64 loss: -0.28637033700942993
Batch 56/64 loss: -0.29530224204063416
Batch 57/64 loss: -0.2801498472690582
Batch 58/64 loss: -0.26723265647888184
Batch 59/64 loss: -0.28313595056533813
Batch 60/64 loss: -0.2690200209617615
Batch 61/64 loss: -0.26675158739089966
Batch 62/64 loss: -0.24941158294677734
Batch 63/64 loss: -0.28226208686828613
Batch 64/64 loss: -0.2792571783065796
Epoch 336  Train loss: -0.2835484691694671  Val loss: -0.2492989967369132
Epoch 337
-------------------------------
Batch 1/64 loss: -0.2816513180732727
Batch 2/64 loss: -0.26004767417907715
Batch 3/64 loss: -0.2870294451713562
Batch 4/64 loss: -0.27423134446144104
Batch 5/64 loss: -0.30108481645584106
Batch 6/64 loss: -0.26802492141723633
Batch 7/64 loss: -0.2735242545604706
Batch 8/64 loss: -0.2772728204727173
Batch 9/64 loss: -0.28449082374572754
Batch 10/64 loss: -0.2914218604564667
Batch 11/64 loss: -0.2871840000152588
Batch 12/64 loss: -0.28194934129714966
Batch 13/64 loss: -0.2830171585083008
Batch 14/64 loss: -0.2768716812133789
Batch 15/64 loss: -0.28232353925704956
Batch 16/64 loss: -0.28356456756591797
Batch 17/64 loss: -0.27735310792922974
Batch 18/64 loss: -0.27726635336875916
Batch 19/64 loss: -0.2787553071975708
Batch 20/64 loss: -0.2914750277996063
Batch 21/64 loss: -0.2767627537250519
Batch 22/64 loss: -0.2804560363292694
Batch 23/64 loss: -0.270125150680542
Batch 24/64 loss: -0.28140464425086975
Batch 25/64 loss: -0.2896149158477783
Batch 26/64 loss: -0.27610012888908386
Batch 27/64 loss: -0.2975378632545471
Batch 28/64 loss: -0.2799120545387268
Batch 29/64 loss: -0.2899438142776489
Batch 30/64 loss: -0.2992073893547058
Batch 31/64 loss: -0.2951822280883789
Batch 32/64 loss: -0.2686440944671631
Batch 33/64 loss: -0.28623443841934204
Batch 34/64 loss: -0.2539374828338623
Batch 35/64 loss: -0.2896478772163391
Batch 36/64 loss: -0.26455605030059814
Batch 37/64 loss: -0.2787885069847107
Batch 38/64 loss: -0.27184146642684937
Batch 39/64 loss: -0.27315443754196167
Batch 40/64 loss: -0.27157777547836304
Batch 41/64 loss: -0.28290122747421265
Batch 42/64 loss: -0.2862536907196045
Batch 43/64 loss: -0.2614598274230957
Batch 44/64 loss: -0.26850688457489014
Batch 45/64 loss: -0.27127858996391296
Batch 46/64 loss: -0.2802596688270569
Batch 47/64 loss: -0.2918214797973633
Batch 48/64 loss: -0.2974430024623871
Batch 49/64 loss: -0.2824150323867798
Batch 50/64 loss: -0.27072086930274963
Batch 51/64 loss: -0.27545422315597534
Batch 52/64 loss: -0.2930203080177307
Batch 53/64 loss: -0.26434123516082764
Batch 54/64 loss: -0.2842510938644409
Batch 55/64 loss: -0.2590435743331909
Batch 56/64 loss: -0.27473849058151245
Batch 57/64 loss: -0.280894011259079
Batch 58/64 loss: -0.29156258702278137
Batch 59/64 loss: -0.28287985920906067
Batch 60/64 loss: -0.28866004943847656
Batch 61/64 loss: -0.288579523563385
Batch 62/64 loss: -0.27129441499710083
Batch 63/64 loss: -0.29054194688796997
Batch 64/64 loss: -0.27210476994514465
Epoch 337  Train loss: -0.28008735121465195  Val loss: -0.2501819088696614
Epoch 338
-------------------------------
Batch 1/64 loss: -0.2813478112220764
Batch 2/64 loss: -0.2821234166622162
Batch 3/64 loss: -0.29100245237350464
Batch 4/64 loss: -0.28218722343444824
Batch 5/64 loss: -0.28861576318740845
Batch 6/64 loss: -0.29388847947120667
Batch 7/64 loss: -0.2828017473220825
Batch 8/64 loss: -0.27858930826187134
Batch 9/64 loss: -0.2892894446849823
Batch 10/64 loss: -0.27541959285736084
Batch 11/64 loss: -0.2785364091396332
Batch 12/64 loss: -0.27932310104370117
Batch 13/64 loss: -0.2823692858219147
Batch 14/64 loss: -0.2837245464324951
Batch 15/64 loss: -0.29873719811439514
Batch 16/64 loss: -0.2635403871536255
Batch 17/64 loss: -0.282779335975647
Batch 18/64 loss: -0.2821355164051056
Batch 19/64 loss: -0.28142687678337097
Batch 20/64 loss: -0.2810136675834656
Batch 21/64 loss: -0.2901770770549774
Batch 22/64 loss: -0.28613921999931335
Batch 23/64 loss: -0.2714492082595825
Batch 24/64 loss: -0.2849985957145691
Batch 25/64 loss: -0.27974486351013184
Batch 26/64 loss: -0.29030168056488037
Batch 27/64 loss: -0.27966436743736267
Batch 28/64 loss: -0.2813494801521301
Batch 29/64 loss: -0.2848507761955261
Batch 30/64 loss: -0.2822644114494324
Batch 31/64 loss: -0.2784581184387207
Batch 32/64 loss: -0.27129095792770386
Batch 33/64 loss: -0.28253936767578125
Batch 34/64 loss: -0.2862217426300049
Batch 35/64 loss: -0.2725796699523926
Batch 36/64 loss: -0.28441375494003296
Batch 37/64 loss: -0.283297598361969
Batch 38/64 loss: -0.2762564420700073
Batch 39/64 loss: -0.28456979990005493
Batch 40/64 loss: -0.2806115746498108
Batch 41/64 loss: -0.2871147096157074
Batch 42/64 loss: -0.2961782217025757
Batch 43/64 loss: -0.2860477864742279
Batch 44/64 loss: -0.2880115807056427
Batch 45/64 loss: -0.25376278162002563
Batch 46/64 loss: -0.28297463059425354
Batch 47/64 loss: -0.27118292450904846
Batch 48/64 loss: -0.2915104031562805
Batch 49/64 loss: -0.28278255462646484
Batch 50/64 loss: -0.2845766246318817
Batch 51/64 loss: -0.2735051214694977
Batch 52/64 loss: -0.2760810852050781
Batch 53/64 loss: -0.26969432830810547
Batch 54/64 loss: -0.27412617206573486
Batch 55/64 loss: -0.2725989818572998
Batch 56/64 loss: -0.2851403057575226
Batch 57/64 loss: -0.26974862813949585
Batch 58/64 loss: -0.2937166392803192
Batch 59/64 loss: -0.28056415915489197
Batch 60/64 loss: -0.28989115357398987
Batch 61/64 loss: -0.28628653287887573
Batch 62/64 loss: -0.2747876048088074
Batch 63/64 loss: -0.27144789695739746
Batch 64/64 loss: -0.2674773931503296
Epoch 338  Train loss: -0.2812920649846395  Val loss: -0.2544922838915664
Epoch 339
-------------------------------
Batch 1/64 loss: -0.26141631603240967
Batch 2/64 loss: -0.2737109661102295
Batch 3/64 loss: -0.29058071970939636
Batch 4/64 loss: -0.28315454721450806
Batch 5/64 loss: -0.28835979104042053
Batch 6/64 loss: -0.2873229384422302
Batch 7/64 loss: -0.2812334895133972
Batch 8/64 loss: -0.27814751863479614
Batch 9/64 loss: -0.27246999740600586
Batch 10/64 loss: -0.26981836557388306
Batch 11/64 loss: -0.2739161252975464
Batch 12/64 loss: -0.2619137763977051
Batch 13/64 loss: -0.26432204246520996
Batch 14/64 loss: -0.2860255539417267
Batch 15/64 loss: -0.26490575075149536
Batch 16/64 loss: -0.2900565266609192
Batch 17/64 loss: -0.28894123435020447
Batch 18/64 loss: -0.28660979866981506
Batch 19/64 loss: -0.27937501668930054
Batch 20/64 loss: -0.29767709970474243
Batch 21/64 loss: -0.2798174321651459
Batch 22/64 loss: -0.29090458154678345
Batch 23/64 loss: -0.2760568857192993
Batch 24/64 loss: -0.2812480032444
Batch 25/64 loss: -0.24362331628799438
Batch 26/64 loss: -0.28119492530822754
Batch 27/64 loss: -0.2796092927455902
Batch 28/64 loss: -0.27965328097343445
Batch 29/64 loss: -0.2876642048358917
Batch 30/64 loss: -0.2779512405395508
Batch 31/64 loss: -0.27105361223220825
Batch 32/64 loss: -0.27998101711273193
Batch 33/64 loss: -0.2755656838417053
Batch 34/64 loss: -0.27137213945388794
Batch 35/64 loss: -0.2669077515602112
Batch 36/64 loss: -0.2744419574737549
Batch 37/64 loss: -0.26776325702667236
Batch 38/64 loss: -0.27383434772491455
Batch 39/64 loss: -0.28694605827331543
Batch 40/64 loss: -0.28057023882865906
Batch 41/64 loss: -0.26962023973464966
Batch 42/64 loss: -0.2850215435028076
Batch 43/64 loss: -0.284171462059021
Batch 44/64 loss: -0.2750917077064514
Batch 45/64 loss: -0.29480379819869995
Batch 46/64 loss: -0.2723346948623657
Batch 47/64 loss: -0.26404404640197754
Batch 48/64 loss: -0.27509480714797974
Batch 49/64 loss: -0.26569223403930664
Batch 50/64 loss: -0.2729509472846985
Batch 51/64 loss: -0.28755438327789307
Batch 52/64 loss: -0.28932318091392517
Batch 53/64 loss: -0.2830928564071655
Batch 54/64 loss: -0.2805432677268982
Batch 55/64 loss: -0.2833836078643799
Batch 56/64 loss: -0.26991987228393555
Batch 57/64 loss: -0.2804679870605469
Batch 58/64 loss: -0.2757670283317566
Batch 59/64 loss: -0.2792033851146698
Batch 60/64 loss: -0.2848469614982605
Batch 61/64 loss: -0.2920956015586853
Batch 62/64 loss: -0.2928975820541382
Batch 63/64 loss: -0.27953773736953735
Batch 64/64 loss: -0.2797984480857849
Epoch 339  Train loss: -0.27848509129355936  Val loss: -0.2420927574954082
Epoch 340
-------------------------------
Batch 1/64 loss: -0.2833783030509949
Batch 2/64 loss: -0.2902103364467621
Batch 3/64 loss: -0.29954642057418823
Batch 4/64 loss: -0.27091312408447266
Batch 5/64 loss: -0.29219746589660645
Batch 6/64 loss: -0.28830423951148987
Batch 7/64 loss: -0.2954096794128418
Batch 8/64 loss: -0.2782675623893738
Batch 9/64 loss: -0.2766284644603729
Batch 10/64 loss: -0.28735029697418213
Batch 11/64 loss: -0.2797505855560303
Batch 12/64 loss: -0.295911967754364
Batch 13/64 loss: -0.2782425880432129
Batch 14/64 loss: -0.287264347076416
Batch 15/64 loss: -0.27684110403060913
Batch 16/64 loss: -0.2742818295955658
Batch 17/64 loss: -0.286906361579895
Batch 18/64 loss: -0.2914060950279236
Batch 19/64 loss: -0.27261245250701904
Batch 20/64 loss: -0.2716146409511566
Batch 21/64 loss: -0.27865883708000183
Batch 22/64 loss: -0.2687496542930603
Batch 23/64 loss: -0.2806295156478882
Batch 24/64 loss: -0.28644925355911255
Batch 25/64 loss: -0.28835350275039673
Batch 26/64 loss: -0.2828930914402008
Batch 27/64 loss: -0.24799692630767822
Batch 28/64 loss: -0.2762935161590576
Batch 29/64 loss: -0.2744506001472473
Batch 30/64 loss: -0.28040841221809387
Batch 31/64 loss: -0.2710173726081848
Batch 32/64 loss: -0.28031933307647705
Batch 33/64 loss: -0.27855655550956726
Batch 34/64 loss: -0.2718198597431183
Batch 35/64 loss: -0.2585059404373169
Batch 36/64 loss: -0.2775886058807373
Batch 37/64 loss: -0.286743700504303
Batch 38/64 loss: -0.28106269240379333
Batch 39/64 loss: -0.27919286489486694
Batch 40/64 loss: -0.28907132148742676
Batch 41/64 loss: -0.2870159149169922
Batch 42/64 loss: -0.2732294201850891
Batch 43/64 loss: -0.27224504947662354
Batch 44/64 loss: -0.2530519962310791
Batch 45/64 loss: -0.2812999188899994
Batch 46/64 loss: -0.2875438630580902
Batch 47/64 loss: -0.2520519495010376
Batch 48/64 loss: -0.26873505115509033
Batch 49/64 loss: -0.2709994912147522
Batch 50/64 loss: -0.28180646896362305
Batch 51/64 loss: -0.26921987533569336
Batch 52/64 loss: -0.26674884557724
Batch 53/64 loss: -0.26701319217681885
Batch 54/64 loss: -0.28005748987197876
Batch 55/64 loss: -0.2867533564567566
Batch 56/64 loss: -0.2928876280784607
Batch 57/64 loss: -0.2636125087738037
Batch 58/64 loss: -0.2870883345603943
Batch 59/64 loss: -0.26726484298706055
Batch 60/64 loss: -0.29190707206726074
Batch 61/64 loss: -0.28525057435035706
Batch 62/64 loss: -0.2847898602485657
Batch 63/64 loss: -0.2928013205528259
Batch 64/64 loss: -0.2699262499809265
Epoch 340  Train loss: -0.278927343265683  Val loss: -0.24835121979828143
Epoch 341
-------------------------------
Batch 1/64 loss: -0.282181978225708
Batch 2/64 loss: -0.2877371907234192
Batch 3/64 loss: -0.2771749198436737
Batch 4/64 loss: -0.2787844240665436
Batch 5/64 loss: -0.2640685439109802
Batch 6/64 loss: -0.2841205596923828
Batch 7/64 loss: -0.2847500443458557
Batch 8/64 loss: -0.28017738461494446
Batch 9/64 loss: -0.28982722759246826
Batch 10/64 loss: -0.27222082018852234
Batch 11/64 loss: -0.30040305852890015
Batch 12/64 loss: -0.2693279981613159
Batch 13/64 loss: -0.25903308391571045
Batch 14/64 loss: -0.2631821632385254
Batch 15/64 loss: -0.27101004123687744
Batch 16/64 loss: -0.27827391028404236
Batch 17/64 loss: -0.2833236753940582
Batch 18/64 loss: -0.27947425842285156
Batch 19/64 loss: -0.27693110704421997
Batch 20/64 loss: -0.2726615071296692
Batch 21/64 loss: -0.27900224924087524
Batch 22/64 loss: -0.25912100076675415
Batch 23/64 loss: -0.2782144844532013
Batch 24/64 loss: -0.28698450326919556
Batch 25/64 loss: -0.26302069425582886
Batch 26/64 loss: -0.2794058918952942
Batch 27/64 loss: -0.26591795682907104
Batch 28/64 loss: -0.26266926527023315
Batch 29/64 loss: -0.28921228647232056
Batch 30/64 loss: -0.29124295711517334
Batch 31/64 loss: -0.26290833950042725
Batch 32/64 loss: -0.2685166597366333
Batch 33/64 loss: -0.28051453828811646
Batch 34/64 loss: -0.262842059135437
Batch 35/64 loss: -0.27245160937309265
Batch 36/64 loss: -0.28926146030426025
Batch 37/64 loss: -0.26654577255249023
Batch 38/64 loss: -0.27714264392852783
Batch 39/64 loss: -0.27807390689849854
Batch 40/64 loss: -0.2890688180923462
Batch 41/64 loss: -0.27758079767227173
Batch 42/64 loss: -0.2790432274341583
Batch 43/64 loss: -0.281173974275589
Batch 44/64 loss: -0.2632797956466675
Batch 45/64 loss: -0.2886196970939636
Batch 46/64 loss: -0.2709258198738098
Batch 47/64 loss: -0.2762284278869629
Batch 48/64 loss: -0.2718496322631836
Batch 49/64 loss: -0.2696029841899872
Batch 50/64 loss: -0.28060421347618103
Batch 51/64 loss: -0.2671850621700287
Batch 52/64 loss: -0.28388476371765137
Batch 53/64 loss: -0.2685398459434509
Batch 54/64 loss: -0.2722715735435486
Batch 55/64 loss: -0.28328895568847656
Batch 56/64 loss: -0.2820425033569336
Batch 57/64 loss: -0.28115883469581604
Batch 58/64 loss: -0.2814475893974304
Batch 59/64 loss: -0.27933281660079956
Batch 60/64 loss: -0.28187233209609985
Batch 61/64 loss: -0.23659580945968628
Batch 62/64 loss: -0.2880308926105499
Batch 63/64 loss: -0.27405527234077454
Batch 64/64 loss: -0.26967480778694153
Epoch 341  Train loss: -0.2760415987641204  Val loss: -0.2521902142521442
Epoch 342
-------------------------------
Batch 1/64 loss: -0.2663315534591675
Batch 2/64 loss: -0.2671823501586914
Batch 3/64 loss: -0.26707279682159424
Batch 4/64 loss: -0.28652679920196533
Batch 5/64 loss: -0.2654542326927185
Batch 6/64 loss: -0.2836131453514099
Batch 7/64 loss: -0.2724631428718567
Batch 8/64 loss: -0.2510826587677002
Batch 9/64 loss: -0.2646098732948303
Batch 10/64 loss: -0.24066460132598877
Batch 11/64 loss: -0.2599846124649048
Batch 12/64 loss: -0.25064903497695923
Batch 13/64 loss: -0.2712171673774719
Batch 14/64 loss: -0.2734486758708954
Batch 15/64 loss: -0.27163559198379517
Batch 16/64 loss: -0.2709537744522095
Batch 17/64 loss: -0.26997530460357666
Batch 18/64 loss: -0.26179617643356323
Batch 19/64 loss: -0.278870165348053
Batch 20/64 loss: -0.28007733821868896
Batch 21/64 loss: -0.2756628394126892
Batch 22/64 loss: -0.27172619104385376
Batch 23/64 loss: -0.28081583976745605
Batch 24/64 loss: -0.2642579674720764
Batch 25/64 loss: -0.28534743189811707
Batch 26/64 loss: -0.2714959383010864
Batch 27/64 loss: -0.27853360772132874
Batch 28/64 loss: -0.27309027314186096
Batch 29/64 loss: -0.2937716543674469
Batch 30/64 loss: -0.2713993489742279
Batch 31/64 loss: -0.2614232897758484
Batch 32/64 loss: -0.2817718982696533
Batch 33/64 loss: -0.27504515647888184
Batch 34/64 loss: -0.2702048122882843
Batch 35/64 loss: -0.2694697082042694
Batch 36/64 loss: -0.2708362340927124
Batch 37/64 loss: -0.2709830105304718
Batch 38/64 loss: -0.2671187222003937
Batch 39/64 loss: -0.2696058750152588
Batch 40/64 loss: -0.27513420581817627
Batch 41/64 loss: -0.27616897225379944
Batch 42/64 loss: -0.26358646154403687
Batch 43/64 loss: -0.27623891830444336
Batch 44/64 loss: -0.27062904834747314
Batch 45/64 loss: -0.27635303139686584
Batch 46/64 loss: -0.29258906841278076
Batch 47/64 loss: -0.2817148268222809
Batch 48/64 loss: -0.24953299760818481
Batch 49/64 loss: -0.2582477927207947
Batch 50/64 loss: -0.25959473848342896
Batch 51/64 loss: -0.2616817355155945
Batch 52/64 loss: -0.25732702016830444
Batch 53/64 loss: -0.26222991943359375
Batch 54/64 loss: -0.2605665922164917
Batch 55/64 loss: -0.27610403299331665
Batch 56/64 loss: -0.26645177602767944
Batch 57/64 loss: -0.2728155851364136
Batch 58/64 loss: -0.26539337635040283
Batch 59/64 loss: -0.26926398277282715
Batch 60/64 loss: -0.2748332619667053
Batch 61/64 loss: -0.26406705379486084
Batch 62/64 loss: -0.2555575370788574
Batch 63/64 loss: -0.27336472272872925
Batch 64/64 loss: -0.27657240629196167
Epoch 342  Train loss: -0.26985168246661917  Val loss: -0.24208372442173384
Epoch 343
-------------------------------
Batch 1/64 loss: -0.27361905574798584
Batch 2/64 loss: -0.2685122489929199
Batch 3/64 loss: -0.28676649928092957
Batch 4/64 loss: -0.28843551874160767
Batch 5/64 loss: -0.27550262212753296
Batch 6/64 loss: -0.2733689546585083
Batch 7/64 loss: -0.2732813358306885
Batch 8/64 loss: -0.2666550874710083
Batch 9/64 loss: -0.29514896869659424
Batch 10/64 loss: -0.2764555513858795
Batch 11/64 loss: -0.26894527673721313
Batch 12/64 loss: -0.28078240156173706
Batch 13/64 loss: -0.2868143916130066
Batch 14/64 loss: -0.2763978838920593
Batch 15/64 loss: -0.2910037636756897
Batch 16/64 loss: -0.26706475019454956
Batch 17/64 loss: -0.2776351869106293
Batch 18/64 loss: -0.28056448698043823
Batch 19/64 loss: -0.2899225354194641
Batch 20/64 loss: -0.2762775421142578
Batch 21/64 loss: -0.27827441692352295
Batch 22/64 loss: -0.2753557860851288
Batch 23/64 loss: -0.27582645416259766
Batch 24/64 loss: -0.2658195495605469
Batch 25/64 loss: -0.27479130029678345
Batch 26/64 loss: -0.28629374504089355
Batch 27/64 loss: -0.28645041584968567
Batch 28/64 loss: -0.2856057286262512
Batch 29/64 loss: -0.2798087000846863
Batch 30/64 loss: -0.2766795754432678
Batch 31/64 loss: -0.2697902321815491
Batch 32/64 loss: -0.28880575299263
Batch 33/64 loss: -0.28164541721343994
Batch 34/64 loss: -0.286218523979187
Batch 35/64 loss: -0.29786956310272217
Batch 36/64 loss: -0.2933797240257263
Batch 37/64 loss: -0.27847355604171753
Batch 38/64 loss: -0.290719211101532
Batch 39/64 loss: -0.2727981507778168
Batch 40/64 loss: -0.30369430780410767
Batch 41/64 loss: -0.2642580270767212
Batch 42/64 loss: -0.2648732662200928
Batch 43/64 loss: -0.25625187158584595
Batch 44/64 loss: -0.2718268930912018
Batch 45/64 loss: -0.279313862323761
Batch 46/64 loss: -0.2910797894001007
Batch 47/64 loss: -0.26906418800354004
Batch 48/64 loss: -0.27396124601364136
Batch 49/64 loss: -0.2808384597301483
Batch 50/64 loss: -0.28497934341430664
Batch 51/64 loss: -0.2551158666610718
Batch 52/64 loss: -0.2813144326210022
Batch 53/64 loss: -0.2984094023704529
Batch 54/64 loss: -0.2787777781486511
Batch 55/64 loss: -0.2826308012008667
Batch 56/64 loss: -0.27270984649658203
Batch 57/64 loss: -0.3006630539894104
Batch 58/64 loss: -0.2491835355758667
Batch 59/64 loss: -0.27353745698928833
Batch 60/64 loss: -0.26531004905700684
Batch 61/64 loss: -0.2709959149360657
Batch 62/64 loss: -0.2707436978816986
Batch 63/64 loss: -0.2663338780403137
Batch 64/64 loss: -0.2934434413909912
Epoch 343  Train loss: -0.27833269670897837  Val loss: -0.2597369561899978
Epoch 344
-------------------------------
Batch 1/64 loss: -0.2745877802371979
Batch 2/64 loss: -0.27190321683883667
Batch 3/64 loss: -0.2578378915786743
Batch 4/64 loss: -0.28951549530029297
Batch 5/64 loss: -0.27413347363471985
Batch 6/64 loss: -0.2881760001182556
Batch 7/64 loss: -0.2870560884475708
Batch 8/64 loss: -0.2767041325569153
Batch 9/64 loss: -0.2907981276512146
Batch 10/64 loss: -0.2777259945869446
Batch 11/64 loss: -0.27913737297058105
Batch 12/64 loss: -0.2689235806465149
Batch 13/64 loss: -0.2753141224384308
Batch 14/64 loss: -0.271877646446228
Batch 15/64 loss: -0.2751290202140808
Batch 16/64 loss: -0.2749733328819275
Batch 17/64 loss: -0.28163301944732666
Batch 18/64 loss: -0.288287878036499
Batch 19/64 loss: -0.28592169284820557
Batch 20/64 loss: -0.26758822798728943
Batch 21/64 loss: -0.2724056839942932
Batch 22/64 loss: -0.23765504360198975
Batch 23/64 loss: -0.2942836284637451
Batch 24/64 loss: -0.2811858654022217
Batch 25/64 loss: -0.272184818983078
Batch 26/64 loss: -0.2666379511356354
Batch 27/64 loss: -0.2914988100528717
Batch 28/64 loss: -0.266842782497406
Batch 29/64 loss: -0.2828167974948883
Batch 30/64 loss: -0.2785283327102661
Batch 31/64 loss: -0.29128605127334595
Batch 32/64 loss: -0.296852707862854
Batch 33/64 loss: -0.28818389773368835
Batch 34/64 loss: -0.2825012803077698
Batch 35/64 loss: -0.2914808392524719
Batch 36/64 loss: -0.28530624508857727
Batch 37/64 loss: -0.2826490104198456
Batch 38/64 loss: -0.28646135330200195
Batch 39/64 loss: -0.2798423171043396
Batch 40/64 loss: -0.2736244201660156
Batch 41/64 loss: -0.2967625856399536
Batch 42/64 loss: -0.28294438123703003
Batch 43/64 loss: -0.2914653420448303
Batch 44/64 loss: -0.28563982248306274
Batch 45/64 loss: -0.27814781665802
Batch 46/64 loss: -0.28624212741851807
Batch 47/64 loss: -0.2919098734855652
Batch 48/64 loss: -0.27072495222091675
Batch 49/64 loss: -0.2930426001548767
Batch 50/64 loss: -0.281888484954834
Batch 51/64 loss: -0.28828930854797363
Batch 52/64 loss: -0.25740331411361694
Batch 53/64 loss: -0.29266470670700073
Batch 54/64 loss: -0.26351189613342285
Batch 55/64 loss: -0.2710076570510864
Batch 56/64 loss: -0.2797481417655945
Batch 57/64 loss: -0.28153154253959656
Batch 58/64 loss: -0.27842235565185547
Batch 59/64 loss: -0.2819240391254425
Batch 60/64 loss: -0.2785155177116394
Batch 61/64 loss: -0.2681907117366791
Batch 62/64 loss: -0.2881662845611572
Batch 63/64 loss: -0.2916526794433594
Batch 64/64 loss: -0.2801225185394287
Epoch 344  Train loss: -0.2799896464628332  Val loss: -0.247856236405389
Epoch 345
-------------------------------
Batch 1/64 loss: -0.2698283791542053
Batch 2/64 loss: -0.27941906452178955
Batch 3/64 loss: -0.26235902309417725
Batch 4/64 loss: -0.29064327478408813
Batch 5/64 loss: -0.25695139169692993
Batch 6/64 loss: -0.2790321111679077
Batch 7/64 loss: -0.27208518981933594
Batch 8/64 loss: -0.27795112133026123
Batch 9/64 loss: -0.2846883535385132
Batch 10/64 loss: -0.2868066728115082
Batch 11/64 loss: -0.30472785234451294
Batch 12/64 loss: -0.2940925657749176
Batch 13/64 loss: -0.28224700689315796
Batch 14/64 loss: -0.2776673436164856
Batch 15/64 loss: -0.2777983546257019
Batch 16/64 loss: -0.2904471755027771
Batch 17/64 loss: -0.2746051847934723
Batch 18/64 loss: -0.27799302339553833
Batch 19/64 loss: -0.2965816259384155
Batch 20/64 loss: -0.2853119969367981
Batch 21/64 loss: -0.27884024381637573
Batch 22/64 loss: -0.30451294779777527
Batch 23/64 loss: -0.279109925031662
Batch 24/64 loss: -0.2811952829360962
Batch 25/64 loss: -0.30223995447158813
Batch 26/64 loss: -0.2821938395500183
Batch 27/64 loss: -0.27654701471328735
Batch 28/64 loss: -0.29238834977149963
Batch 29/64 loss: -0.2797585129737854
Batch 30/64 loss: -0.29209768772125244
Batch 31/64 loss: -0.28569793701171875
Batch 32/64 loss: -0.28673678636550903
Batch 33/64 loss: -0.26731324195861816
Batch 34/64 loss: -0.2757680416107178
Batch 35/64 loss: -0.2697622776031494
Batch 36/64 loss: -0.27673739194869995
Batch 37/64 loss: -0.285065621137619
Batch 38/64 loss: -0.2940599322319031
Batch 39/64 loss: -0.29167255759239197
Batch 40/64 loss: -0.27588510513305664
Batch 41/64 loss: -0.29303622245788574
Batch 42/64 loss: -0.28951969742774963
Batch 43/64 loss: -0.28033536672592163
Batch 44/64 loss: -0.2737441658973694
Batch 45/64 loss: -0.2728511095046997
Batch 46/64 loss: -0.2944818437099457
Batch 47/64 loss: -0.28212398290634155
Batch 48/64 loss: -0.2931366562843323
Batch 49/64 loss: -0.2696939706802368
Batch 50/64 loss: -0.281306654214859
Batch 51/64 loss: -0.28710299730300903
Batch 52/64 loss: -0.2859037518501282
Batch 53/64 loss: -0.29187729954719543
Batch 54/64 loss: -0.2651045322418213
Batch 55/64 loss: -0.290552020072937
Batch 56/64 loss: -0.28817641735076904
Batch 57/64 loss: -0.2787574827671051
Batch 58/64 loss: -0.2737927734851837
Batch 59/64 loss: -0.27480044960975647
Batch 60/64 loss: -0.28182876110076904
Batch 61/64 loss: -0.2906336486339569
Batch 62/64 loss: -0.2822567820549011
Batch 63/64 loss: -0.2893572747707367
Batch 64/64 loss: -0.2833423614501953
Epoch 345  Train loss: -0.2826933332518035  Val loss: -0.2677066352768862
Epoch 346
-------------------------------
Batch 1/64 loss: -0.29252010583877563
Batch 2/64 loss: -0.2859886884689331
Batch 3/64 loss: -0.29950374364852905
Batch 4/64 loss: -0.28826025128364563
Batch 5/64 loss: -0.28713908791542053
Batch 6/64 loss: -0.28885260224342346
Batch 7/64 loss: -0.2753370404243469
Batch 8/64 loss: -0.2922722101211548
Batch 9/64 loss: -0.31275519728660583
Batch 10/64 loss: -0.27071863412857056
Batch 11/64 loss: -0.288574755191803
Batch 12/64 loss: -0.28430432081222534
Batch 13/64 loss: -0.2696589529514313
Batch 14/64 loss: -0.3022589683532715
Batch 15/64 loss: -0.2828625440597534
Batch 16/64 loss: -0.2915468215942383
Batch 17/64 loss: -0.2831878066062927
Batch 18/64 loss: -0.2908937931060791
Batch 19/64 loss: -0.3045806586742401
Batch 20/64 loss: -0.2833389639854431
Batch 21/64 loss: -0.2554481625556946
Batch 22/64 loss: -0.28993216156959534
Batch 23/64 loss: -0.29487311840057373
Batch 24/64 loss: -0.2855771481990814
Batch 25/64 loss: -0.28939464688301086
Batch 26/64 loss: -0.27707093954086304
Batch 27/64 loss: -0.28605619072914124
Batch 28/64 loss: -0.28708523511886597
Batch 29/64 loss: -0.27938616275787354
Batch 30/64 loss: -0.2925698459148407
Batch 31/64 loss: -0.2942749261856079
Batch 32/64 loss: -0.28669473528862
Batch 33/64 loss: -0.289095401763916
Batch 34/64 loss: -0.2927701473236084
Batch 35/64 loss: -0.2879668176174164
Batch 36/64 loss: -0.29911237955093384
Batch 37/64 loss: -0.28670161962509155
Batch 38/64 loss: -0.2850247025489807
Batch 39/64 loss: -0.2736157476902008
Batch 40/64 loss: -0.2973496913909912
Batch 41/64 loss: -0.26824134588241577
Batch 42/64 loss: -0.28580915927886963
Batch 43/64 loss: -0.27539363503456116
Batch 44/64 loss: -0.29010623693466187
Batch 45/64 loss: -0.2859620749950409
Batch 46/64 loss: -0.28594672679901123
Batch 47/64 loss: -0.29565316438674927
Batch 48/64 loss: -0.26006633043289185
Batch 49/64 loss: -0.28691914677619934
Batch 50/64 loss: -0.24378180503845215
Batch 51/64 loss: -0.2644750475883484
Batch 52/64 loss: -0.28185635805130005
Batch 53/64 loss: -0.2620886564254761
Batch 54/64 loss: -0.29157233238220215
Batch 55/64 loss: -0.2808229923248291
Batch 56/64 loss: -0.27564001083374023
Batch 57/64 loss: -0.2902405858039856
Batch 58/64 loss: -0.2833821773529053
Batch 59/64 loss: -0.259490966796875
Batch 60/64 loss: -0.2858625650405884
Batch 61/64 loss: -0.2857106924057007
Batch 62/64 loss: -0.289226770401001
Batch 63/64 loss: -0.28002405166625977
Batch 64/64 loss: -0.3001757264137268
Epoch 346  Train loss: -0.2845170126241796  Val loss: -0.2560662348655491
Epoch 347
-------------------------------
Batch 1/64 loss: -0.263525128364563
Batch 2/64 loss: -0.28523486852645874
Batch 3/64 loss: -0.28829431533813477
Batch 4/64 loss: -0.29936689138412476
Batch 5/64 loss: -0.2861882448196411
Batch 6/64 loss: -0.2854626774787903
Batch 7/64 loss: -0.28204572200775146
Batch 8/64 loss: -0.2883821725845337
Batch 9/64 loss: -0.2907857894897461
Batch 10/64 loss: -0.27872663736343384
Batch 11/64 loss: -0.2767724394798279
Batch 12/64 loss: -0.28212037682533264
Batch 13/64 loss: -0.2891978323459625
Batch 14/64 loss: -0.284191370010376
Batch 15/64 loss: -0.27458345890045166
Batch 16/64 loss: -0.2810400724411011
Batch 17/64 loss: -0.29578477144241333
Batch 18/64 loss: -0.2763743996620178
Batch 19/64 loss: -0.27870166301727295
Batch 20/64 loss: -0.28065216541290283
Batch 21/64 loss: -0.26350027322769165
Batch 22/64 loss: -0.2950725853443146
Batch 23/64 loss: -0.292841374874115
Batch 24/64 loss: -0.27577561140060425
Batch 25/64 loss: -0.26617172360420227
Batch 26/64 loss: -0.27591127157211304
Batch 27/64 loss: -0.28333136439323425
Batch 28/64 loss: -0.27502310276031494
Batch 29/64 loss: -0.277807354927063
Batch 30/64 loss: -0.2905994653701782
Batch 31/64 loss: -0.2765175998210907
Batch 32/64 loss: -0.2788020968437195
Batch 33/64 loss: -0.29182904958724976
Batch 34/64 loss: -0.25838136672973633
Batch 35/64 loss: -0.28339463472366333
Batch 36/64 loss: -0.2941130995750427
Batch 37/64 loss: -0.28844255208969116
Batch 38/64 loss: -0.28944331407546997
Batch 39/64 loss: -0.2836005985736847
Batch 40/64 loss: -0.2824779748916626
Batch 41/64 loss: -0.2758064270019531
Batch 42/64 loss: -0.2823883295059204
Batch 43/64 loss: -0.28315940499305725
Batch 44/64 loss: -0.29000917077064514
Batch 45/64 loss: -0.2879120707511902
Batch 46/64 loss: -0.30620044469833374
Batch 47/64 loss: -0.28526613116264343
Batch 48/64 loss: -0.28833043575286865
Batch 49/64 loss: -0.2740519642829895
Batch 50/64 loss: -0.2972365617752075
Batch 51/64 loss: -0.28486448526382446
Batch 52/64 loss: -0.2913578748703003
Batch 53/64 loss: -0.28793784976005554
Batch 54/64 loss: -0.2754433751106262
Batch 55/64 loss: -0.29040858149528503
Batch 56/64 loss: -0.29849570989608765
Batch 57/64 loss: -0.27289652824401855
Batch 58/64 loss: -0.2942763566970825
Batch 59/64 loss: -0.2886660695075989
Batch 60/64 loss: -0.2848721742630005
Batch 61/64 loss: -0.27659428119659424
Batch 62/64 loss: -0.29317939281463623
Batch 63/64 loss: -0.2750437557697296
Batch 64/64 loss: -0.29027214646339417
Epoch 347  Train loss: -0.28380500230134703  Val loss: -0.2535492812644985
Epoch 348
-------------------------------
Batch 1/64 loss: -0.2773706912994385
Batch 2/64 loss: -0.29291075468063354
Batch 3/64 loss: -0.302887499332428
Batch 4/64 loss: -0.2936958074569702
Batch 5/64 loss: -0.2899057865142822
Batch 6/64 loss: -0.28496527671813965
Batch 7/64 loss: -0.28184810280799866
Batch 8/64 loss: -0.28827717900276184
Batch 9/64 loss: -0.26832789182662964
Batch 10/64 loss: -0.2773681879043579
Batch 11/64 loss: -0.28916800022125244
Batch 12/64 loss: -0.2909390926361084
Batch 13/64 loss: -0.28305310010910034
Batch 14/64 loss: -0.29483088850975037
Batch 15/64 loss: -0.28260356187820435
Batch 16/64 loss: -0.2812001705169678
Batch 17/64 loss: -0.2776273190975189
Batch 18/64 loss: -0.26861634850502014
Batch 19/64 loss: -0.2921212911605835
Batch 20/64 loss: -0.2892177700996399
Batch 21/64 loss: -0.2865355908870697
Batch 22/64 loss: -0.2889033257961273
Batch 23/64 loss: -0.29196807742118835
Batch 24/64 loss: -0.27958929538726807
Batch 25/64 loss: -0.29356440901756287
Batch 26/64 loss: -0.2908792495727539
Batch 27/64 loss: -0.28217339515686035
Batch 28/64 loss: -0.2853408753871918
Batch 29/64 loss: -0.28780055046081543
Batch 30/64 loss: -0.2655627131462097
Batch 31/64 loss: -0.2756366729736328
Batch 32/64 loss: -0.28343939781188965
Batch 33/64 loss: -0.2878459692001343
Batch 34/64 loss: -0.2841345965862274
Batch 35/64 loss: -0.2902592420578003
Batch 36/64 loss: -0.2828804850578308
Batch 37/64 loss: -0.28213953971862793
Batch 38/64 loss: -0.2821945548057556
Batch 39/64 loss: -0.2874879539012909
Batch 40/64 loss: -0.2732475996017456
Batch 41/64 loss: -0.2699761390686035
Batch 42/64 loss: -0.2849971055984497
Batch 43/64 loss: -0.29642778635025024
Batch 44/64 loss: -0.29144495725631714
Batch 45/64 loss: -0.2709848880767822
Batch 46/64 loss: -0.28450316190719604
Batch 47/64 loss: -0.2928727865219116
Batch 48/64 loss: -0.2742495536804199
Batch 49/64 loss: -0.27384188771247864
Batch 50/64 loss: -0.27487891912460327
Batch 51/64 loss: -0.28015029430389404
Batch 52/64 loss: -0.29633885622024536
Batch 53/64 loss: -0.27272430062294006
Batch 54/64 loss: -0.2869575023651123
Batch 55/64 loss: -0.28911012411117554
Batch 56/64 loss: -0.26574546098709106
Batch 57/64 loss: -0.2885734736919403
Batch 58/64 loss: -0.287797212600708
Batch 59/64 loss: -0.2871047258377075
Batch 60/64 loss: -0.2853689193725586
Batch 61/64 loss: -0.2914004325866699
Batch 62/64 loss: -0.29132774472236633
Batch 63/64 loss: -0.29235392808914185
Batch 64/64 loss: -0.2925184667110443
Epoch 348  Train loss: -0.284502544706943  Val loss: -0.25727400583090243
Epoch 349
-------------------------------
Batch 1/64 loss: -0.26917338371276855
Batch 2/64 loss: -0.26466256380081177
Batch 3/64 loss: -0.26485562324523926
Batch 4/64 loss: -0.29307571053504944
Batch 5/64 loss: -0.30337268114089966
Batch 6/64 loss: -0.30365994572639465
Batch 7/64 loss: -0.287309855222702
Batch 8/64 loss: -0.2867496609687805
Batch 9/64 loss: -0.2697479724884033
Batch 10/64 loss: -0.29396671056747437
Batch 11/64 loss: -0.2980000972747803
Batch 12/64 loss: -0.2816179692745209
Batch 13/64 loss: -0.2860615849494934
Batch 14/64 loss: -0.2897573709487915
Batch 15/64 loss: -0.2856544852256775
Batch 16/64 loss: -0.27705076336860657
Batch 17/64 loss: -0.29015132784843445
Batch 18/64 loss: -0.2847723960876465
Batch 19/64 loss: -0.2893277406692505
Batch 20/64 loss: -0.27498501539230347
Batch 21/64 loss: -0.2885792553424835
Batch 22/64 loss: -0.2803407311439514
Batch 23/64 loss: -0.28963860869407654
Batch 24/64 loss: -0.28478699922561646
Batch 25/64 loss: -0.2927640676498413
Batch 26/64 loss: -0.27712148427963257
Batch 27/64 loss: -0.2913816571235657
Batch 28/64 loss: -0.28721657395362854
Batch 29/64 loss: -0.30319082736968994
Batch 30/64 loss: -0.27921730279922485
Batch 31/64 loss: -0.27446460723876953
Batch 32/64 loss: -0.2834746539592743
Batch 33/64 loss: -0.2857966423034668
Batch 34/64 loss: -0.2863383889198303
Batch 35/64 loss: -0.2806142568588257
Batch 36/64 loss: -0.27872830629348755
Batch 37/64 loss: -0.2870335578918457
Batch 38/64 loss: -0.2916799783706665
Batch 39/64 loss: -0.27699515223503113
Batch 40/64 loss: -0.29857927560806274
Batch 41/64 loss: -0.2892090082168579
Batch 42/64 loss: -0.304020494222641
Batch 43/64 loss: -0.28225958347320557
Batch 44/64 loss: -0.28583329916000366
Batch 45/64 loss: -0.27537524700164795
Batch 46/64 loss: -0.2927517294883728
Batch 47/64 loss: -0.28225022554397583
Batch 48/64 loss: -0.29683077335357666
Batch 49/64 loss: -0.2929490804672241
Batch 50/64 loss: -0.2784191966056824
Batch 51/64 loss: -0.3007924556732178
Batch 52/64 loss: -0.2793336808681488
Batch 53/64 loss: -0.2748730778694153
Batch 54/64 loss: -0.2900160253047943
Batch 55/64 loss: -0.28527140617370605
Batch 56/64 loss: -0.2855817675590515
Batch 57/64 loss: -0.2888243496417999
Batch 58/64 loss: -0.2903103232383728
Batch 59/64 loss: -0.27677053213119507
Batch 60/64 loss: -0.2834700345993042
Batch 61/64 loss: -0.29179680347442627
Batch 62/64 loss: -0.2957710921764374
Batch 63/64 loss: -0.2887727618217468
Batch 64/64 loss: -0.28365662693977356
Epoch 349  Train loss: -0.28605679378790017  Val loss: -0.2620504247568727
Epoch 350
-------------------------------
Batch 1/64 loss: -0.2950914800167084
Batch 2/64 loss: -0.2803974151611328
Batch 3/64 loss: -0.2953464984893799
Batch 4/64 loss: -0.28432339429855347
Batch 5/64 loss: -0.2930866479873657
Batch 6/64 loss: -0.3063381314277649
Batch 7/64 loss: -0.29500991106033325
Batch 8/64 loss: -0.294034868478775
Batch 9/64 loss: -0.29645389318466187
Batch 10/64 loss: -0.2885839343070984
Batch 11/64 loss: -0.28500089049339294
Batch 12/64 loss: -0.2905696630477905
Batch 13/64 loss: -0.28012609481811523
Batch 14/64 loss: -0.288536012172699
Batch 15/64 loss: -0.29765784740448
Batch 16/64 loss: -0.2910028100013733
Batch 17/64 loss: -0.28197890520095825
Batch 18/64 loss: -0.2847036123275757
Batch 19/64 loss: -0.28331053256988525
Batch 20/64 loss: -0.28689733147621155
Batch 21/64 loss: -0.2822883427143097
Batch 22/64 loss: -0.29407984018325806
Batch 23/64 loss: -0.2854717969894409
Batch 24/64 loss: -0.29995059967041016
Batch 25/64 loss: -0.29206621646881104
Batch 26/64 loss: -0.2773027718067169
Batch 27/64 loss: -0.2825872302055359
Batch 28/64 loss: -0.2953232526779175
Batch 29/64 loss: -0.3022196292877197
Batch 30/64 loss: -0.2980833053588867
Batch 31/64 loss: -0.2882811427116394
Batch 32/64 loss: -0.28843921422958374
Batch 33/64 loss: -0.29100602865219116
Batch 34/64 loss: -0.2896040678024292
Batch 35/64 loss: -0.2909983992576599
Batch 36/64 loss: -0.28635212779045105
Batch 37/64 loss: -0.2964063286781311
Batch 38/64 loss: -0.29768210649490356
Batch 39/64 loss: -0.2749478816986084
Batch 40/64 loss: -0.2888551354408264
Batch 41/64 loss: -0.284450888633728
Batch 42/64 loss: -0.29410287737846375
Batch 43/64 loss: -0.28414809703826904
Batch 44/64 loss: -0.27677804231643677
Batch 45/64 loss: -0.28345322608947754
Batch 46/64 loss: -0.2971804141998291
Batch 47/64 loss: -0.28222566843032837
Batch 48/64 loss: -0.28102797269821167
Batch 49/64 loss: -0.28188586235046387
Batch 50/64 loss: -0.2927837073802948
Batch 51/64 loss: -0.28353798389434814
Batch 52/64 loss: -0.2746298909187317
Batch 53/64 loss: -0.3024369478225708
Batch 54/64 loss: -0.2819841504096985
Batch 55/64 loss: -0.2895103096961975
Batch 56/64 loss: -0.27460604906082153
Batch 57/64 loss: -0.30269795656204224
Batch 58/64 loss: -0.28022855520248413
Batch 59/64 loss: -0.28215593099594116
Batch 60/64 loss: -0.280587375164032
Batch 61/64 loss: -0.276849627494812
Batch 62/64 loss: -0.2799200713634491
Batch 63/64 loss: -0.27974992990493774
Batch 64/64 loss: -0.2785990238189697
Epoch 350  Train loss: -0.28794158580256446  Val loss: -0.2613569377214229
Epoch 351
-------------------------------
Batch 1/64 loss: -0.2950555682182312
Batch 2/64 loss: -0.28127115964889526
Batch 3/64 loss: -0.3111608326435089
Batch 4/64 loss: -0.29606837034225464
Batch 5/64 loss: -0.2942084074020386
Batch 6/64 loss: -0.2748373746871948
Batch 7/64 loss: -0.28736454248428345
Batch 8/64 loss: -0.2908276617527008
Batch 9/64 loss: -0.2867784798145294
Batch 10/64 loss: -0.2848757803440094
Batch 11/64 loss: -0.287253201007843
Batch 12/64 loss: -0.28724193572998047
Batch 13/64 loss: -0.2941543161869049
Batch 14/64 loss: -0.29795408248901367
Batch 15/64 loss: -0.2685563862323761
Batch 16/64 loss: -0.2917388081550598
Batch 17/64 loss: -0.28060299158096313
Batch 18/64 loss: -0.2723753750324249
Batch 19/64 loss: -0.29407811164855957
Batch 20/64 loss: -0.2820017337799072
Batch 21/64 loss: -0.28999876976013184
Batch 22/64 loss: -0.2874278426170349
Batch 23/64 loss: -0.2776646018028259
Batch 24/64 loss: -0.2859713137149811
Batch 25/64 loss: -0.28282666206359863
Batch 26/64 loss: -0.2811506390571594
Batch 27/64 loss: -0.2874533236026764
Batch 28/64 loss: -0.2955436110496521
Batch 29/64 loss: -0.2930821180343628
Batch 30/64 loss: -0.28763270378112793
Batch 31/64 loss: -0.27796870470046997
Batch 32/64 loss: -0.27938705682754517
Batch 33/64 loss: -0.280814528465271
Batch 34/64 loss: -0.2914949357509613
Batch 35/64 loss: -0.2763364911079407
Batch 36/64 loss: -0.2947118282318115
Batch 37/64 loss: -0.2890704274177551
Batch 38/64 loss: -0.28496748208999634
Batch 39/64 loss: -0.2889764904975891
Batch 40/64 loss: -0.2802467942237854
Batch 41/64 loss: -0.3001130223274231
Batch 42/64 loss: -0.27947765588760376
Batch 43/64 loss: -0.28734534978866577
Batch 44/64 loss: -0.28740233182907104
Batch 45/64 loss: -0.29031848907470703
Batch 46/64 loss: -0.2920769453048706
Batch 47/64 loss: -0.285987913608551
Batch 48/64 loss: -0.29009810090065
Batch 49/64 loss: -0.2879551351070404
Batch 50/64 loss: -0.29305148124694824
Batch 51/64 loss: -0.2883799076080322
Batch 52/64 loss: -0.26875174045562744
Batch 53/64 loss: -0.2946634888648987
Batch 54/64 loss: -0.2933982014656067
Batch 55/64 loss: -0.29994863271713257
Batch 56/64 loss: -0.2938278615474701
Batch 57/64 loss: -0.29120075702667236
Batch 58/64 loss: -0.2918074429035187
Batch 59/64 loss: -0.27654528617858887
Batch 60/64 loss: -0.29355061054229736
Batch 61/64 loss: -0.28724873065948486
Batch 62/64 loss: -0.30779415369033813
Batch 63/64 loss: -0.28564393520355225
Batch 64/64 loss: -0.2807829976081848
Epoch 351  Train loss: -0.28781656260583915  Val loss: -0.2599882229496933
Epoch 352
-------------------------------
Batch 1/64 loss: -0.28722554445266724
Batch 2/64 loss: -0.2809229791164398
Batch 3/64 loss: -0.2785218060016632
Batch 4/64 loss: -0.29091739654541016
Batch 5/64 loss: -0.3073665201663971
Batch 6/64 loss: -0.29245108366012573
Batch 7/64 loss: -0.2768687605857849
Batch 8/64 loss: -0.29733818769454956
Batch 9/64 loss: -0.28102588653564453
Batch 10/64 loss: -0.2862156927585602
Batch 11/64 loss: -0.283557653427124
Batch 12/64 loss: -0.2846031188964844
Batch 13/64 loss: -0.28202882409095764
Batch 14/64 loss: -0.29359567165374756
Batch 15/64 loss: -0.307705819606781
Batch 16/64 loss: -0.280468225479126
Batch 17/64 loss: -0.2905983328819275
Batch 18/64 loss: -0.2899623215198517
Batch 19/64 loss: -0.29214826226234436
Batch 20/64 loss: -0.2792293429374695
Batch 21/64 loss: -0.29571306705474854
Batch 22/64 loss: -0.2807773947715759
Batch 23/64 loss: -0.27481868863105774
Batch 24/64 loss: -0.2873537540435791
Batch 25/64 loss: -0.3024122714996338
Batch 26/64 loss: -0.2955940067768097
Batch 27/64 loss: -0.2808088958263397
Batch 28/64 loss: -0.276311457157135
Batch 29/64 loss: -0.28906863927841187
Batch 30/64 loss: -0.270774245262146
Batch 31/64 loss: -0.29810985922813416
Batch 32/64 loss: -0.2818560004234314
Batch 33/64 loss: -0.29339486360549927
Batch 34/64 loss: -0.299762487411499
Batch 35/64 loss: -0.2773861885070801
Batch 36/64 loss: -0.26606202125549316
Batch 37/64 loss: -0.28395694494247437
Batch 38/64 loss: -0.2639433741569519
Batch 39/64 loss: -0.28289079666137695
Batch 40/64 loss: -0.2832399010658264
Batch 41/64 loss: -0.28130030632019043
Batch 42/64 loss: -0.27604594826698303
Batch 43/64 loss: -0.2861553430557251
Batch 44/64 loss: -0.2595263719558716
Batch 45/64 loss: -0.27202922105789185
Batch 46/64 loss: -0.2915636897087097
Batch 47/64 loss: -0.27839210629463196
Batch 48/64 loss: -0.2711159586906433
Batch 49/64 loss: -0.2885628938674927
Batch 50/64 loss: -0.2877460718154907
Batch 51/64 loss: -0.28990262746810913
Batch 52/64 loss: -0.2717128396034241
Batch 53/64 loss: -0.2816087305545807
Batch 54/64 loss: -0.27640584111213684
Batch 55/64 loss: -0.27803510427474976
Batch 56/64 loss: -0.2859659492969513
Batch 57/64 loss: -0.2899034917354584
Batch 58/64 loss: -0.2790686786174774
Batch 59/64 loss: -0.2682758569717407
Batch 60/64 loss: -0.2744309604167938
Batch 61/64 loss: -0.2662001848220825
Batch 62/64 loss: -0.2778651714324951
Batch 63/64 loss: -0.2939535081386566
Batch 64/64 loss: -0.28407278656959534
Epoch 352  Train loss: -0.28369894483510183  Val loss: -0.2549882508635111
Epoch 353
-------------------------------
Batch 1/64 loss: -0.29413414001464844
Batch 2/64 loss: -0.2701731026172638
Batch 3/64 loss: -0.28283458948135376
Batch 4/64 loss: -0.2814706563949585
Batch 5/64 loss: -0.2762816846370697
Batch 6/64 loss: -0.293744295835495
Batch 7/64 loss: -0.2759822905063629
Batch 8/64 loss: -0.2753388583660126
Batch 9/64 loss: -0.27373647689819336
Batch 10/64 loss: -0.27739179134368896
Batch 11/64 loss: -0.27921366691589355
Batch 12/64 loss: -0.2758016586303711
Batch 13/64 loss: -0.28202733397483826
Batch 14/64 loss: -0.2612895965576172
Batch 15/64 loss: -0.28794968128204346
Batch 16/64 loss: -0.2879156768321991
Batch 17/64 loss: -0.28232884407043457
Batch 18/64 loss: -0.27602618932724
Batch 19/64 loss: -0.27487242221832275
Batch 20/64 loss: -0.28501468896865845
Batch 21/64 loss: -0.27710652351379395
Batch 22/64 loss: -0.2807040810585022
Batch 23/64 loss: -0.25509536266326904
Batch 24/64 loss: -0.28709205985069275
Batch 25/64 loss: -0.28963690996170044
Batch 26/64 loss: -0.27990809082984924
Batch 27/64 loss: -0.2775517404079437
Batch 28/64 loss: -0.28368303179740906
Batch 29/64 loss: -0.2905648350715637
Batch 30/64 loss: -0.28605175018310547
Batch 31/64 loss: -0.2840437591075897
Batch 32/64 loss: -0.27865517139434814
Batch 33/64 loss: -0.2726251482963562
Batch 34/64 loss: -0.2815879285335541
Batch 35/64 loss: -0.28698158264160156
Batch 36/64 loss: -0.262525737285614
Batch 37/64 loss: -0.29503583908081055
Batch 38/64 loss: -0.28472843766212463
Batch 39/64 loss: -0.2846909761428833
Batch 40/64 loss: -0.2866172790527344
Batch 41/64 loss: -0.28888946771621704
Batch 42/64 loss: -0.2912856936454773
Batch 43/64 loss: -0.2745043635368347
Batch 44/64 loss: -0.2924021780490875
Batch 45/64 loss: -0.29858726263046265
Batch 46/64 loss: -0.2711315155029297
Batch 47/64 loss: -0.2832777798175812
Batch 48/64 loss: -0.27517202496528625
Batch 49/64 loss: -0.2888182997703552
Batch 50/64 loss: -0.2811867296695709
Batch 51/64 loss: -0.29292845726013184
Batch 52/64 loss: -0.2834194302558899
Batch 53/64 loss: -0.26190072298049927
Batch 54/64 loss: -0.2954760789871216
Batch 55/64 loss: -0.2918231785297394
Batch 56/64 loss: -0.2794889211654663
Batch 57/64 loss: -0.28691941499710083
Batch 58/64 loss: -0.28715962171554565
Batch 59/64 loss: -0.28600746393203735
Batch 60/64 loss: -0.28483855724334717
Batch 61/64 loss: -0.2859160900115967
Batch 62/64 loss: -0.2794884443283081
Batch 63/64 loss: -0.28272777795791626
Batch 64/64 loss: -0.29480692744255066
Epoch 353  Train loss: -0.2820842127005259  Val loss: -0.25416388114293414
Epoch 354
-------------------------------
Batch 1/64 loss: -0.2980324327945709
Batch 2/64 loss: -0.2815280556678772
Batch 3/64 loss: -0.2900858521461487
Batch 4/64 loss: -0.2742442190647125
Batch 5/64 loss: -0.2886822819709778
Batch 6/64 loss: -0.27525007724761963
Batch 7/64 loss: -0.2949683666229248
Batch 8/64 loss: -0.2863224744796753
Batch 9/64 loss: -0.2851572334766388
Batch 10/64 loss: -0.2901972234249115
Batch 11/64 loss: -0.29661738872528076
Batch 12/64 loss: -0.2739167809486389
Batch 13/64 loss: -0.2818375527858734
Batch 14/64 loss: -0.29427605867385864
Batch 15/64 loss: -0.271838903427124
Batch 16/64 loss: -0.28606581687927246
Batch 17/64 loss: -0.2942967712879181
Batch 18/64 loss: -0.29673218727111816
Batch 19/64 loss: -0.2844921946525574
Batch 20/64 loss: -0.2721424996852875
Batch 21/64 loss: -0.28719550371170044
Batch 22/64 loss: -0.28021714091300964
Batch 23/64 loss: -0.29163968563079834
Batch 24/64 loss: -0.29882365465164185
Batch 25/64 loss: -0.292179137468338
Batch 26/64 loss: -0.2805749773979187
Batch 27/64 loss: -0.29405704140663147
Batch 28/64 loss: -0.28468793630599976
Batch 29/64 loss: -0.287651926279068
Batch 30/64 loss: -0.29464614391326904
Batch 31/64 loss: -0.2953428626060486
Batch 32/64 loss: -0.29819488525390625
Batch 33/64 loss: -0.28305351734161377
Batch 34/64 loss: -0.2830439507961273
Batch 35/64 loss: -0.2984358072280884
Batch 36/64 loss: -0.29136228561401367
Batch 37/64 loss: -0.28449946641921997
Batch 38/64 loss: -0.2869560122489929
Batch 39/64 loss: -0.28938764333724976
Batch 40/64 loss: -0.28821036219596863
Batch 41/64 loss: -0.28068047761917114
Batch 42/64 loss: -0.29131728410720825
Batch 43/64 loss: -0.29335159063339233
Batch 44/64 loss: -0.2728506922721863
Batch 45/64 loss: -0.29533588886260986
Batch 46/64 loss: -0.28825050592422485
Batch 47/64 loss: -0.28734421730041504
Batch 48/64 loss: -0.28525155782699585
Batch 49/64 loss: -0.27926069498062134
Batch 50/64 loss: -0.3015517294406891
Batch 51/64 loss: -0.2740844488143921
Batch 52/64 loss: -0.2851296663284302
Batch 53/64 loss: -0.2919237017631531
Batch 54/64 loss: -0.2813480496406555
Batch 55/64 loss: -0.2831166982650757
Batch 56/64 loss: -0.28184738755226135
Batch 57/64 loss: -0.2908998131752014
Batch 58/64 loss: -0.2729347050189972
Batch 59/64 loss: -0.2912861406803131
Batch 60/64 loss: -0.2687157988548279
Batch 61/64 loss: -0.2769114673137665
Batch 62/64 loss: -0.29087352752685547
Batch 63/64 loss: -0.3088065981864929
Batch 64/64 loss: -0.2872205972671509
Epoch 354  Train loss: -0.28698560630573944  Val loss: -0.2704297652359271
Saving best model, epoch: 354
Epoch 355
-------------------------------
Batch 1/64 loss: -0.29230162501335144
Batch 2/64 loss: -0.2787542939186096
Batch 3/64 loss: -0.2816751003265381
Batch 4/64 loss: -0.2968335747718811
Batch 5/64 loss: -0.2937190532684326
Batch 6/64 loss: -0.2933043837547302
Batch 7/64 loss: -0.2897005081176758
Batch 8/64 loss: -0.2797112762928009
Batch 9/64 loss: -0.2954813241958618
Batch 10/64 loss: -0.2917608618736267
Batch 11/64 loss: -0.2867223024368286
Batch 12/64 loss: -0.28799283504486084
Batch 13/64 loss: -0.27901771664619446
Batch 14/64 loss: -0.2937871813774109
Batch 15/64 loss: -0.2950551509857178
Batch 16/64 loss: -0.29461348056793213
Batch 17/64 loss: -0.2907477617263794
Batch 18/64 loss: -0.2846547067165375
Batch 19/64 loss: -0.2867235541343689
Batch 20/64 loss: -0.2820764482021332
Batch 21/64 loss: -0.283319890499115
Batch 22/64 loss: -0.29539334774017334
Batch 23/64 loss: -0.2936650216579437
Batch 24/64 loss: -0.2826457619667053
Batch 25/64 loss: -0.2848033607006073
Batch 26/64 loss: -0.28790926933288574
Batch 27/64 loss: -0.2780456244945526
Batch 28/64 loss: -0.2812633514404297
Batch 29/64 loss: -0.2757645845413208
Batch 30/64 loss: -0.2872081398963928
Batch 31/64 loss: -0.2894153296947479
Batch 32/64 loss: -0.29570990800857544
Batch 33/64 loss: -0.2899853587150574
Batch 34/64 loss: -0.2937682867050171
Batch 35/64 loss: -0.2816484868526459
Batch 36/64 loss: -0.29667359590530396
Batch 37/64 loss: -0.28360992670059204
Batch 38/64 loss: -0.28138625621795654
Batch 39/64 loss: -0.2883254289627075
Batch 40/64 loss: -0.294898122549057
Batch 41/64 loss: -0.2852313220500946
Batch 42/64 loss: -0.2686002850532532
Batch 43/64 loss: -0.27963072061538696
Batch 44/64 loss: -0.29769062995910645
Batch 45/64 loss: -0.29333382844924927
Batch 46/64 loss: -0.2930721044540405
Batch 47/64 loss: -0.2839322090148926
Batch 48/64 loss: -0.27869418263435364
Batch 49/64 loss: -0.27782362699508667
Batch 50/64 loss: -0.28286218643188477
Batch 51/64 loss: -0.28273457288742065
Batch 52/64 loss: -0.2706603407859802
Batch 53/64 loss: -0.2854105532169342
Batch 54/64 loss: -0.27609947323799133
Batch 55/64 loss: -0.2763217091560364
Batch 56/64 loss: -0.2842918932437897
Batch 57/64 loss: -0.2842068672180176
Batch 58/64 loss: -0.29554063081741333
Batch 59/64 loss: -0.28805193305015564
Batch 60/64 loss: -0.2850927710533142
Batch 61/64 loss: -0.28506049513816833
Batch 62/64 loss: -0.28826236724853516
Batch 63/64 loss: -0.2862948179244995
Batch 64/64 loss: -0.27955329418182373
Epoch 355  Train loss: -0.28640998718785304  Val loss: -0.2592876778845115
Epoch 356
-------------------------------
Batch 1/64 loss: -0.2969054579734802
Batch 2/64 loss: -0.2736271917819977
Batch 3/64 loss: -0.2833564281463623
Batch 4/64 loss: -0.2928336560726166
Batch 5/64 loss: -0.28557127714157104
Batch 6/64 loss: -0.2876659035682678
Batch 7/64 loss: -0.29059404134750366
Batch 8/64 loss: -0.28638797998428345
Batch 9/64 loss: -0.2928919792175293
Batch 10/64 loss: -0.2832311987876892
Batch 11/64 loss: -0.2817607522010803
Batch 12/64 loss: -0.29638323187828064
Batch 13/64 loss: -0.29040199518203735
Batch 14/64 loss: -0.27643537521362305
Batch 15/64 loss: -0.28101256489753723
Batch 16/64 loss: -0.28787291049957275
Batch 17/64 loss: -0.28849321603775024
Batch 18/64 loss: -0.28766411542892456
Batch 19/64 loss: -0.2949029803276062
Batch 20/64 loss: -0.2940896153450012
Batch 21/64 loss: -0.29336708784103394
Batch 22/64 loss: -0.2879953384399414
Batch 23/64 loss: -0.29286882281303406
Batch 24/64 loss: -0.3018367290496826
Batch 25/64 loss: -0.2983909845352173
Batch 26/64 loss: -0.2918332815170288
Batch 27/64 loss: -0.2723437547683716
Batch 28/64 loss: -0.2996113896369934
Batch 29/64 loss: -0.281221479177475
Batch 30/64 loss: -0.278883159160614
Batch 31/64 loss: -0.2801494002342224
Batch 32/64 loss: -0.2864946126937866
Batch 33/64 loss: -0.28053730726242065
Batch 34/64 loss: -0.28228121995925903
Batch 35/64 loss: -0.28886327147483826
Batch 36/64 loss: -0.2795027494430542
Batch 37/64 loss: -0.2868012487888336
Batch 38/64 loss: -0.2849419116973877
Batch 39/64 loss: -0.29157572984695435
Batch 40/64 loss: -0.28695064783096313
Batch 41/64 loss: -0.3001547157764435
Batch 42/64 loss: -0.29687270522117615
Batch 43/64 loss: -0.2855169177055359
Batch 44/64 loss: -0.2851576805114746
Batch 45/64 loss: -0.28930097818374634
Batch 46/64 loss: -0.2915690839290619
Batch 47/64 loss: -0.27709364891052246
Batch 48/64 loss: -0.2868014872074127
Batch 49/64 loss: -0.2942676246166229
Batch 50/64 loss: -0.28870752453804016
Batch 51/64 loss: -0.28382962942123413
Batch 52/64 loss: -0.28611525893211365
Batch 53/64 loss: -0.29802459478378296
Batch 54/64 loss: -0.28826218843460083
Batch 55/64 loss: -0.284562885761261
Batch 56/64 loss: -0.2856006622314453
Batch 57/64 loss: -0.29895907640457153
Batch 58/64 loss: -0.2956605553627014
Batch 59/64 loss: -0.29374900460243225
Batch 60/64 loss: -0.2804868817329407
Batch 61/64 loss: -0.2926921844482422
Batch 62/64 loss: -0.29125046730041504
Batch 63/64 loss: -0.2738262712955475
Batch 64/64 loss: -0.2877139449119568
Epoch 356  Train loss: -0.28804359973645677  Val loss: -0.26878270168894347
Epoch 357
-------------------------------
Batch 1/64 loss: -0.27720728516578674
Batch 2/64 loss: -0.28135567903518677
Batch 3/64 loss: -0.28647297620773315
Batch 4/64 loss: -0.29586338996887207
Batch 5/64 loss: -0.29078981280326843
Batch 6/64 loss: -0.295282781124115
Batch 7/64 loss: -0.2895321249961853
Batch 8/64 loss: -0.29789674282073975
Batch 9/64 loss: -0.2957884669303894
Batch 10/64 loss: -0.29851287603378296
Batch 11/64 loss: -0.2826589345932007
Batch 12/64 loss: -0.29843538999557495
Batch 13/64 loss: -0.2944904565811157
Batch 14/64 loss: -0.29537835717201233
Batch 15/64 loss: -0.2747649550437927
Batch 16/64 loss: -0.2963188588619232
Batch 17/64 loss: -0.2920665144920349
Batch 18/64 loss: -0.2968859076499939
Batch 19/64 loss: -0.290682315826416
Batch 20/64 loss: -0.28196489810943604
Batch 21/64 loss: -0.30129480361938477
Batch 22/64 loss: -0.2943398058414459
Batch 23/64 loss: -0.2949516177177429
Batch 24/64 loss: -0.29235249757766724
Batch 25/64 loss: -0.2775493860244751
Batch 26/64 loss: -0.2955678105354309
Batch 27/64 loss: -0.2851071357727051
Batch 28/64 loss: -0.2754653990268707
Batch 29/64 loss: -0.27326875925064087
Batch 30/64 loss: -0.27708879113197327
Batch 31/64 loss: -0.29000651836395264
Batch 32/64 loss: -0.2761749029159546
Batch 33/64 loss: -0.25800108909606934
Batch 34/64 loss: -0.2822442352771759
Batch 35/64 loss: -0.2936956286430359
Batch 36/64 loss: -0.28053319454193115
Batch 37/64 loss: -0.277784526348114
Batch 38/64 loss: -0.2876051068305969
Batch 39/64 loss: -0.29193827509880066
Batch 40/64 loss: -0.2833040654659271
Batch 41/64 loss: -0.28727203607559204
Batch 42/64 loss: -0.2791377902030945
Batch 43/64 loss: -0.2822266221046448
Batch 44/64 loss: -0.28421857953071594
Batch 45/64 loss: -0.29517120122909546
Batch 46/64 loss: -0.29026639461517334
Batch 47/64 loss: -0.3025658428668976
Batch 48/64 loss: -0.2873607873916626
Batch 49/64 loss: -0.27694815397262573
Batch 50/64 loss: -0.28140145540237427
Batch 51/64 loss: -0.2866235375404358
Batch 52/64 loss: -0.2755686044692993
Batch 53/64 loss: -0.28701508045196533
Batch 54/64 loss: -0.27763769030570984
Batch 55/64 loss: -0.29362019896507263
Batch 56/64 loss: -0.2934967577457428
Batch 57/64 loss: -0.2920120358467102
Batch 58/64 loss: -0.2786184549331665
Batch 59/64 loss: -0.28516361117362976
Batch 60/64 loss: -0.29960399866104126
Batch 61/64 loss: -0.28058987855911255
Batch 62/64 loss: -0.30446648597717285
Batch 63/64 loss: -0.2812880277633667
Batch 64/64 loss: -0.29933595657348633
Epoch 357  Train loss: -0.28748858769734703  Val loss: -0.2666004828571044
Epoch 358
-------------------------------
Batch 1/64 loss: -0.29190924763679504
Batch 2/64 loss: -0.2962743937969208
Batch 3/64 loss: -0.2846466600894928
Batch 4/64 loss: -0.2835792601108551
Batch 5/64 loss: -0.2980377972126007
Batch 6/64 loss: -0.3049836754798889
Batch 7/64 loss: -0.2920157015323639
Batch 8/64 loss: -0.28611141443252563
Batch 9/64 loss: -0.28916752338409424
Batch 10/64 loss: -0.30615538358688354
Batch 11/64 loss: -0.28782957792282104
Batch 12/64 loss: -0.28716155886650085
Batch 13/64 loss: -0.29417097568511963
Batch 14/64 loss: -0.2869457006454468
Batch 15/64 loss: -0.28904086351394653
Batch 16/64 loss: -0.2981743812561035
Batch 17/64 loss: -0.28396955132484436
Batch 18/64 loss: -0.28865471482276917
Batch 19/64 loss: -0.2826193571090698
Batch 20/64 loss: -0.2870236039161682
Batch 21/64 loss: -0.291978120803833
Batch 22/64 loss: -0.2809755206108093
Batch 23/64 loss: -0.29505741596221924
Batch 24/64 loss: -0.28329533338546753
Batch 25/64 loss: -0.2763175666332245
Batch 26/64 loss: -0.2900809645652771
Batch 27/64 loss: -0.2913379967212677
Batch 28/64 loss: -0.29155176877975464
Batch 29/64 loss: -0.27419939637184143
Batch 30/64 loss: -0.30258578062057495
Batch 31/64 loss: -0.2934984862804413
Batch 32/64 loss: -0.29354026913642883
Batch 33/64 loss: -0.2883821725845337
Batch 34/64 loss: -0.2944207787513733
Batch 35/64 loss: -0.2742578387260437
Batch 36/64 loss: -0.2749282121658325
Batch 37/64 loss: -0.2757934331893921
Batch 38/64 loss: -0.28333115577697754
Batch 39/64 loss: -0.2951793670654297
Batch 40/64 loss: -0.2890072464942932
Batch 41/64 loss: -0.28664806485176086
Batch 42/64 loss: -0.294041246175766
Batch 43/64 loss: -0.2795029282569885
Batch 44/64 loss: -0.2967718839645386
Batch 45/64 loss: -0.29843229055404663
Batch 46/64 loss: -0.2816997170448303
Batch 47/64 loss: -0.27870091795921326
Batch 48/64 loss: -0.29204505681991577
Batch 49/64 loss: -0.2793252766132355
Batch 50/64 loss: -0.293900728225708
Batch 51/64 loss: -0.29548102617263794
Batch 52/64 loss: -0.2877979278564453
Batch 53/64 loss: -0.2911723852157593
Batch 54/64 loss: -0.28311994671821594
Batch 55/64 loss: -0.2891598343849182
Batch 56/64 loss: -0.28408095240592957
Batch 57/64 loss: -0.27973851561546326
Batch 58/64 loss: -0.286454439163208
Batch 59/64 loss: -0.28674644231796265
Batch 60/64 loss: -0.29023247957229614
Batch 61/64 loss: -0.28260624408721924
Batch 62/64 loss: -0.2985485792160034
Batch 63/64 loss: -0.3012375235557556
Batch 64/64 loss: -0.298647403717041
Epoch 358  Train loss: -0.2889352176703659  Val loss: -0.2645398013780207
Epoch 359
-------------------------------
Batch 1/64 loss: -0.30335378646850586
Batch 2/64 loss: -0.284673810005188
Batch 3/64 loss: -0.2946668863296509
Batch 4/64 loss: -0.2862311005592346
Batch 5/64 loss: -0.3064306080341339
Batch 6/64 loss: -0.2946079969406128
Batch 7/64 loss: -0.29143235087394714
Batch 8/64 loss: -0.28677138686180115
Batch 9/64 loss: -0.2883860468864441
Batch 10/64 loss: -0.2909362316131592
Batch 11/64 loss: -0.29032421112060547
Batch 12/64 loss: -0.28979256749153137
Batch 13/64 loss: -0.30936092138290405
Batch 14/64 loss: -0.29875653982162476
Batch 15/64 loss: -0.29895684123039246
Batch 16/64 loss: -0.2894964814186096
Batch 17/64 loss: -0.2983763813972473
Batch 18/64 loss: -0.28023362159729004
Batch 19/64 loss: -0.2898027300834656
Batch 20/64 loss: -0.2983093857765198
Batch 21/64 loss: -0.3060190677642822
Batch 22/64 loss: -0.2986673414707184
Batch 23/64 loss: -0.2936927378177643
Batch 24/64 loss: -0.29154539108276367
Batch 25/64 loss: -0.29177770018577576
Batch 26/64 loss: -0.2873324155807495
Batch 27/64 loss: -0.29619306325912476
Batch 28/64 loss: -0.30360615253448486
Batch 29/64 loss: -0.29179441928863525
Batch 30/64 loss: -0.2914782166481018
Batch 31/64 loss: -0.28378283977508545
Batch 32/64 loss: -0.29211097955703735
Batch 33/64 loss: -0.280758261680603
Batch 34/64 loss: -0.2882305681705475
Batch 35/64 loss: -0.30220091342926025
Batch 36/64 loss: -0.2826814651489258
Batch 37/64 loss: -0.2896236181259155
Batch 38/64 loss: -0.29305124282836914
Batch 39/64 loss: -0.2848878502845764
Batch 40/64 loss: -0.26538634300231934
Batch 41/64 loss: -0.2828938364982605
Batch 42/64 loss: -0.28948909044265747
Batch 43/64 loss: -0.2882004976272583
Batch 44/64 loss: -0.2898317575454712
Batch 45/64 loss: -0.28928619623184204
Batch 46/64 loss: -0.29318928718566895
Batch 47/64 loss: -0.28613728284835815
Batch 48/64 loss: -0.2925053536891937
Batch 49/64 loss: -0.29671144485473633
Batch 50/64 loss: -0.29303663969039917
Batch 51/64 loss: -0.27101606130599976
Batch 52/64 loss: -0.30693215131759644
Batch 53/64 loss: -0.282960444688797
Batch 54/64 loss: -0.2918917238712311
Batch 55/64 loss: -0.2905612885951996
Batch 56/64 loss: -0.2891431450843811
Batch 57/64 loss: -0.2916070818901062
Batch 58/64 loss: -0.29707154631614685
Batch 59/64 loss: -0.2925190329551697
Batch 60/64 loss: -0.2846929430961609
Batch 61/64 loss: -0.294100284576416
Batch 62/64 loss: -0.2877345085144043
Batch 63/64 loss: -0.2868717312812805
Batch 64/64 loss: -0.3022918701171875
Epoch 359  Train loss: -0.29146388558780445  Val loss: -0.2547387267715743
Epoch 360
-------------------------------
Batch 1/64 loss: -0.2931033670902252
Batch 2/64 loss: -0.2888447046279907
Batch 3/64 loss: -0.28689271211624146
Batch 4/64 loss: -0.2914934754371643
Batch 5/64 loss: -0.2856042981147766
Batch 6/64 loss: -0.27224892377853394
Batch 7/64 loss: -0.2886697053909302
Batch 8/64 loss: -0.2859021723270416
Batch 9/64 loss: -0.28850096464157104
Batch 10/64 loss: -0.3060625493526459
Batch 11/64 loss: -0.2827945649623871
Batch 12/64 loss: -0.3066624701023102
Batch 13/64 loss: -0.29669827222824097
Batch 14/64 loss: -0.2928060293197632
Batch 15/64 loss: -0.283886194229126
Batch 16/64 loss: -0.301114559173584
Batch 17/64 loss: -0.2922697961330414
Batch 18/64 loss: -0.2926805019378662
Batch 19/64 loss: -0.2993879020214081
Batch 20/64 loss: -0.3001822233200073
Batch 21/64 loss: -0.2988511919975281
Batch 22/64 loss: -0.2891945242881775
Batch 23/64 loss: -0.27663642168045044
Batch 24/64 loss: -0.29827508330345154
Batch 25/64 loss: -0.2929580509662628
Batch 26/64 loss: -0.2994944453239441
Batch 27/64 loss: -0.29312819242477417
Batch 28/64 loss: -0.2913854122161865
Batch 29/64 loss: -0.29499268531799316
Batch 30/64 loss: -0.28126204013824463
Batch 31/64 loss: -0.2884024977684021
Batch 32/64 loss: -0.29188793897628784
Batch 33/64 loss: -0.28645405173301697
Batch 34/64 loss: -0.2868713140487671
Batch 35/64 loss: -0.2907905578613281
Batch 36/64 loss: -0.3097338378429413
Batch 37/64 loss: -0.29801827669143677
Batch 38/64 loss: -0.2918252646923065
Batch 39/64 loss: -0.2925616204738617
Batch 40/64 loss: -0.2969532907009125
Batch 41/64 loss: -0.2722975015640259
Batch 42/64 loss: -0.2999628186225891
Batch 43/64 loss: -0.2940402626991272
Batch 44/64 loss: -0.2855488657951355
Batch 45/64 loss: -0.2871358394622803
Batch 46/64 loss: -0.29482877254486084
Batch 47/64 loss: -0.291635125875473
Batch 48/64 loss: -0.2967973053455353
Batch 49/64 loss: -0.29647427797317505
Batch 50/64 loss: -0.2874665856361389
Batch 51/64 loss: -0.29156017303466797
Batch 52/64 loss: -0.28317129611968994
Batch 53/64 loss: -0.2845003008842468
Batch 54/64 loss: -0.2878759801387787
Batch 55/64 loss: -0.2869058847427368
Batch 56/64 loss: -0.2967900037765503
Batch 57/64 loss: -0.28859174251556396
Batch 58/64 loss: -0.27264106273651123
Batch 59/64 loss: -0.2808864712715149
Batch 60/64 loss: -0.2838513255119324
Batch 61/64 loss: -0.2843226492404938
Batch 62/64 loss: -0.28453966975212097
Batch 63/64 loss: -0.2906714677810669
Batch 64/64 loss: -0.2864778935909271
Epoch 360  Train loss: -0.2905542884387222  Val loss: -0.2729361727065647
Saving best model, epoch: 360
Epoch 361
-------------------------------
Batch 1/64 loss: -0.2910655736923218
Batch 2/64 loss: -0.296173095703125
Batch 3/64 loss: -0.29662466049194336
Batch 4/64 loss: -0.2909429967403412
Batch 5/64 loss: -0.2873751223087311
Batch 6/64 loss: -0.2982216477394104
Batch 7/64 loss: -0.30458900332450867
Batch 8/64 loss: -0.2956477701663971
Batch 9/64 loss: -0.30244889855384827
Batch 10/64 loss: -0.294373095035553
Batch 11/64 loss: -0.2835298776626587
Batch 12/64 loss: -0.2873329222202301
Batch 13/64 loss: -0.2985228896141052
Batch 14/64 loss: -0.28359463810920715
Batch 15/64 loss: -0.2961152195930481
Batch 16/64 loss: -0.30115407705307007
Batch 17/64 loss: -0.29058951139450073
Batch 18/64 loss: -0.27922523021698
Batch 19/64 loss: -0.29700618982315063
Batch 20/64 loss: -0.2970752716064453
Batch 21/64 loss: -0.28968197107315063
Batch 22/64 loss: -0.28893208503723145
Batch 23/64 loss: -0.28088295459747314
Batch 24/64 loss: -0.2837832272052765
Batch 25/64 loss: -0.29179859161376953
Batch 26/64 loss: -0.2887812554836273
Batch 27/64 loss: -0.29589903354644775
Batch 28/64 loss: -0.2899794280529022
Batch 29/64 loss: -0.2889637351036072
Batch 30/64 loss: -0.2999834716320038
Batch 31/64 loss: -0.297148197889328
Batch 32/64 loss: -0.29998892545700073
Batch 33/64 loss: -0.2948620021343231
Batch 34/64 loss: -0.290433406829834
Batch 35/64 loss: -0.2831517457962036
Batch 36/64 loss: -0.2830953598022461
Batch 37/64 loss: -0.2851821184158325
Batch 38/64 loss: -0.2626008987426758
Batch 39/64 loss: -0.282909631729126
Batch 40/64 loss: -0.30703961849212646
Batch 41/64 loss: -0.29247134923934937
Batch 42/64 loss: -0.28953680396080017
Batch 43/64 loss: -0.2845327854156494
Batch 44/64 loss: -0.2903509736061096
Batch 45/64 loss: -0.28330540657043457
Batch 46/64 loss: -0.28796035051345825
Batch 47/64 loss: -0.2945876717567444
Batch 48/64 loss: -0.3016086220741272
Batch 49/64 loss: -0.28968819975852966
Batch 50/64 loss: -0.2993716597557068
Batch 51/64 loss: -0.2957165241241455
Batch 52/64 loss: -0.29664450883865356
Batch 53/64 loss: -0.2943466901779175
Batch 54/64 loss: -0.28403717279434204
Batch 55/64 loss: -0.27676984667778015
Batch 56/64 loss: -0.2849637269973755
Batch 57/64 loss: -0.29141664505004883
Batch 58/64 loss: -0.2820117473602295
Batch 59/64 loss: -0.2836771607398987
Batch 60/64 loss: -0.28347212076187134
Batch 61/64 loss: -0.29010215401649475
Batch 62/64 loss: -0.29730263352394104
Batch 63/64 loss: -0.2831852436065674
Batch 64/64 loss: -0.2887566089630127
Epoch 361  Train loss: -0.29067188711727365  Val loss: -0.27164403679444615
Epoch 362
-------------------------------
Batch 1/64 loss: -0.30293163657188416
Batch 2/64 loss: -0.2861616015434265
Batch 3/64 loss: -0.2842050790786743
Batch 4/64 loss: -0.293168842792511
Batch 5/64 loss: -0.29121798276901245
Batch 6/64 loss: -0.2814120650291443
Batch 7/64 loss: -0.2916530966758728
Batch 8/64 loss: -0.2999538779258728
Batch 9/64 loss: -0.28931647539138794
Batch 10/64 loss: -0.2832064628601074
Batch 11/64 loss: -0.28571873903274536
Batch 12/64 loss: -0.2970218062400818
Batch 13/64 loss: -0.28107333183288574
Batch 14/64 loss: -0.2930927872657776
Batch 15/64 loss: -0.27387291193008423
Batch 16/64 loss: -0.2786366939544678
Batch 17/64 loss: -0.27007633447647095
Batch 18/64 loss: -0.2970139682292938
Batch 19/64 loss: -0.28122827410697937
Batch 20/64 loss: -0.28982675075531006
Batch 21/64 loss: -0.2852831780910492
Batch 22/64 loss: -0.2994590997695923
Batch 23/64 loss: -0.29402247071266174
Batch 24/64 loss: -0.28906384110450745
Batch 25/64 loss: -0.29740920662879944
Batch 26/64 loss: -0.28672289848327637
Batch 27/64 loss: -0.29701846837997437
Batch 28/64 loss: -0.2942274212837219
Batch 29/64 loss: -0.2962949872016907
Batch 30/64 loss: -0.29756659269332886
Batch 31/64 loss: -0.28607916831970215
Batch 32/64 loss: -0.29008984565734863
Batch 33/64 loss: -0.29663312435150146
Batch 34/64 loss: -0.28649455308914185
Batch 35/64 loss: -0.2965713143348694
Batch 36/64 loss: -0.29241153597831726
Batch 37/64 loss: -0.3050980269908905
Batch 38/64 loss: -0.2855926752090454
Batch 39/64 loss: -0.3020388185977936
Batch 40/64 loss: -0.2740367352962494
Batch 41/64 loss: -0.28694236278533936
Batch 42/64 loss: -0.27945244312286377
Batch 43/64 loss: -0.2922370731830597
Batch 44/64 loss: -0.2923998534679413
Batch 45/64 loss: -0.30234742164611816
Batch 46/64 loss: -0.29615503549575806
Batch 47/64 loss: -0.296525239944458
Batch 48/64 loss: -0.2977861762046814
Batch 49/64 loss: -0.29319965839385986
Batch 50/64 loss: -0.2818509340286255
Batch 51/64 loss: -0.2930251359939575
Batch 52/64 loss: -0.29393187165260315
Batch 53/64 loss: -0.2860926687717438
Batch 54/64 loss: -0.29166367650032043
Batch 55/64 loss: -0.27934151887893677
Batch 56/64 loss: -0.28574448823928833
Batch 57/64 loss: -0.2952987551689148
Batch 58/64 loss: -0.27650636434555054
Batch 59/64 loss: -0.286897212266922
Batch 60/64 loss: -0.29163679480552673
Batch 61/64 loss: -0.28846096992492676
Batch 62/64 loss: -0.2922201156616211
Batch 63/64 loss: -0.29145902395248413
Batch 64/64 loss: -0.27670755982398987
Epoch 362  Train loss: -0.2899075787441403  Val loss: -0.2724489639305167
Epoch 363
-------------------------------
Batch 1/64 loss: -0.2686973214149475
Batch 2/64 loss: -0.28433939814567566
Batch 3/64 loss: -0.2889651656150818
Batch 4/64 loss: -0.2872456908226013
Batch 5/64 loss: -0.3062306046485901
Batch 6/64 loss: -0.29349809885025024
Batch 7/64 loss: -0.28669273853302
Batch 8/64 loss: -0.2900112271308899
Batch 9/64 loss: -0.2839702367782593
Batch 10/64 loss: -0.2804860472679138
Batch 11/64 loss: -0.2956628203392029
Batch 12/64 loss: -0.2867642939090729
Batch 13/64 loss: -0.2902992069721222
Batch 14/64 loss: -0.2851076126098633
Batch 15/64 loss: -0.26973241567611694
Batch 16/64 loss: -0.2981221377849579
Batch 17/64 loss: -0.2929301857948303
Batch 18/64 loss: -0.29762768745422363
Batch 19/64 loss: -0.26329660415649414
Batch 20/64 loss: -0.2949455976486206
Batch 21/64 loss: -0.2891610860824585
Batch 22/64 loss: -0.29838770627975464
Batch 23/64 loss: -0.2976371645927429
Batch 24/64 loss: -0.26962652802467346
Batch 25/64 loss: -0.2678152322769165
Batch 26/64 loss: -0.2812919616699219
Batch 27/64 loss: -0.2814209461212158
Batch 28/64 loss: -0.29255223274230957
Batch 29/64 loss: -0.2877572178840637
Batch 30/64 loss: -0.28499835729599
Batch 31/64 loss: -0.2873784303665161
Batch 32/64 loss: -0.2918539345264435
Batch 33/64 loss: -0.3016166687011719
Batch 34/64 loss: -0.29102736711502075
Batch 35/64 loss: -0.29321399331092834
Batch 36/64 loss: -0.29146188497543335
Batch 37/64 loss: -0.29209503531455994
Batch 38/64 loss: -0.2891473174095154
Batch 39/64 loss: -0.303335964679718
Batch 40/64 loss: -0.27995938062667847
Batch 41/64 loss: -0.2988431751728058
Batch 42/64 loss: -0.2952319085597992
Batch 43/64 loss: -0.28863540291786194
Batch 44/64 loss: -0.28234022855758667
Batch 45/64 loss: -0.2890019118785858
Batch 46/64 loss: -0.2693086862564087
Batch 47/64 loss: -0.2954300045967102
Batch 48/64 loss: -0.29017603397369385
Batch 49/64 loss: -0.28740406036376953
Batch 50/64 loss: -0.2734079360961914
Batch 51/64 loss: -0.28554368019104004
Batch 52/64 loss: -0.2997323274612427
Batch 53/64 loss: -0.2792750597000122
Batch 54/64 loss: -0.2872765362262726
Batch 55/64 loss: -0.2879789471626282
Batch 56/64 loss: -0.2944742739200592
Batch 57/64 loss: -0.2924955487251282
Batch 58/64 loss: -0.2954186499118805
Batch 59/64 loss: -0.2934921383857727
Batch 60/64 loss: -0.2901726961135864
Batch 61/64 loss: -0.29207661747932434
Batch 62/64 loss: -0.2920723855495453
Batch 63/64 loss: -0.29964566230773926
Batch 64/64 loss: -0.28937333822250366
Epoch 363  Train loss: -0.28851493924271826  Val loss: -0.26552724510533704
Epoch 364
-------------------------------
Batch 1/64 loss: -0.3045943081378937
Batch 2/64 loss: -0.2865414619445801
Batch 3/64 loss: -0.2978702485561371
Batch 4/64 loss: -0.296974241733551
Batch 5/64 loss: -0.287103533744812
Batch 6/64 loss: -0.294100821018219
Batch 7/64 loss: -0.27849847078323364
Batch 8/64 loss: -0.2823822498321533
Batch 9/64 loss: -0.29386401176452637
Batch 10/64 loss: -0.2835437059402466
Batch 11/64 loss: -0.2941172122955322
Batch 12/64 loss: -0.2976807951927185
Batch 13/64 loss: -0.294283390045166
Batch 14/64 loss: -0.28292566537857056
Batch 15/64 loss: -0.2855198085308075
Batch 16/64 loss: -0.2852340042591095
Batch 17/64 loss: -0.2896861433982849
Batch 18/64 loss: -0.3003060817718506
Batch 19/64 loss: -0.28632503747940063
Batch 20/64 loss: -0.2950620651245117
Batch 21/64 loss: -0.2956329584121704
Batch 22/64 loss: -0.28529685735702515
Batch 23/64 loss: -0.29189592599868774
Batch 24/64 loss: -0.2988134026527405
Batch 25/64 loss: -0.2893778681755066
Batch 26/64 loss: -0.29517239332199097
Batch 27/64 loss: -0.2900921702384949
Batch 28/64 loss: -0.2949613332748413
Batch 29/64 loss: -0.2891688346862793
Batch 30/64 loss: -0.30015701055526733
Batch 31/64 loss: -0.2931842803955078
Batch 32/64 loss: -0.28411418199539185
Batch 33/64 loss: -0.2865685224533081
Batch 34/64 loss: -0.30809485912323
Batch 35/64 loss: -0.2930721938610077
Batch 36/64 loss: -0.28215664625167847
Batch 37/64 loss: -0.30224743485450745
Batch 38/64 loss: -0.2936023473739624
Batch 39/64 loss: -0.2781542241573334
Batch 40/64 loss: -0.2897982895374298
Batch 41/64 loss: -0.29512345790863037
Batch 42/64 loss: -0.3033646047115326
Batch 43/64 loss: -0.291000097990036
Batch 44/64 loss: -0.2936820387840271
Batch 45/64 loss: -0.2851925492286682
Batch 46/64 loss: -0.3055017590522766
Batch 47/64 loss: -0.29354360699653625
Batch 48/64 loss: -0.2992914319038391
Batch 49/64 loss: -0.2960854172706604
Batch 50/64 loss: -0.2986437976360321
Batch 51/64 loss: -0.2941325306892395
Batch 52/64 loss: -0.2947459816932678
Batch 53/64 loss: -0.28119319677352905
Batch 54/64 loss: -0.29059597849845886
Batch 55/64 loss: -0.29883861541748047
Batch 56/64 loss: -0.29279935359954834
Batch 57/64 loss: -0.2822287976741791
Batch 58/64 loss: -0.2955353260040283
Batch 59/64 loss: -0.2885565757751465
Batch 60/64 loss: -0.2885517179965973
Batch 61/64 loss: -0.2955266237258911
Batch 62/64 loss: -0.29846900701522827
Batch 63/64 loss: -0.296334445476532
Batch 64/64 loss: -0.291057288646698
Epoch 364  Train loss: -0.2923200763908087  Val loss: -0.26681756666026163
Epoch 365
-------------------------------
Batch 1/64 loss: -0.2899901568889618
Batch 2/64 loss: -0.29815512895584106
Batch 3/64 loss: -0.2933187484741211
Batch 4/64 loss: -0.292102575302124
Batch 5/64 loss: -0.28789597749710083
Batch 6/64 loss: -0.29617393016815186
Batch 7/64 loss: -0.297800213098526
Batch 8/64 loss: -0.2919674813747406
Batch 9/64 loss: -0.2998986840248108
Batch 10/64 loss: -0.29118046164512634
Batch 11/64 loss: -0.28196460008621216
Batch 12/64 loss: -0.2960365116596222
Batch 13/64 loss: -0.3029031455516815
Batch 14/64 loss: -0.29628294706344604
Batch 15/64 loss: -0.2863013744354248
Batch 16/64 loss: -0.2969081401824951
Batch 17/64 loss: -0.3001548945903778
Batch 18/64 loss: -0.2889840006828308
Batch 19/64 loss: -0.3068293333053589
Batch 20/64 loss: -0.3016541302204132
Batch 21/64 loss: -0.28895148634910583
Batch 22/64 loss: -0.29058849811553955
Batch 23/64 loss: -0.28626081347465515
Batch 24/64 loss: -0.2808372378349304
Batch 25/64 loss: -0.2930944561958313
Batch 26/64 loss: -0.29049253463745117
Batch 27/64 loss: -0.28444814682006836
Batch 28/64 loss: -0.2881295680999756
Batch 29/64 loss: -0.30362382531166077
Batch 30/64 loss: -0.29495787620544434
Batch 31/64 loss: -0.2987538278102875
Batch 32/64 loss: -0.29621607065200806
Batch 33/64 loss: -0.2845371961593628
Batch 34/64 loss: -0.2976137399673462
Batch 35/64 loss: -0.2885119915008545
Batch 36/64 loss: -0.31109848618507385
Batch 37/64 loss: -0.2999311685562134
Batch 38/64 loss: -0.2911239266395569
Batch 39/64 loss: -0.29974043369293213
Batch 40/64 loss: -0.29722607135772705
Batch 41/64 loss: -0.28858670592308044
Batch 42/64 loss: -0.2847288250923157
Batch 43/64 loss: -0.3006117343902588
Batch 44/64 loss: -0.29911500215530396
Batch 45/64 loss: -0.2876374125480652
Batch 46/64 loss: -0.2999523878097534
Batch 47/64 loss: -0.29307442903518677
Batch 48/64 loss: -0.27496159076690674
Batch 49/64 loss: -0.2952394485473633
Batch 50/64 loss: -0.28721123933792114
Batch 51/64 loss: -0.2797168493270874
Batch 52/64 loss: -0.2970806360244751
Batch 53/64 loss: -0.2939368486404419
Batch 54/64 loss: -0.29657605290412903
Batch 55/64 loss: -0.28213226795196533
Batch 56/64 loss: -0.2751714587211609
Batch 57/64 loss: -0.29384636878967285
Batch 58/64 loss: -0.2864832580089569
Batch 59/64 loss: -0.2878876030445099
Batch 60/64 loss: -0.2985117435455322
Batch 61/64 loss: -0.2856900095939636
Batch 62/64 loss: -0.3012334704399109
Batch 63/64 loss: -0.2684449553489685
Batch 64/64 loss: -0.30977946519851685
Epoch 365  Train loss: -0.2925930147077523  Val loss: -0.25985665583528605
Epoch 366
-------------------------------
Batch 1/64 loss: -0.2812159061431885
Batch 2/64 loss: -0.2987298369407654
Batch 3/64 loss: -0.28882908821105957
Batch 4/64 loss: -0.2917327582836151
Batch 5/64 loss: -0.29329270124435425
Batch 6/64 loss: -0.29176393151283264
Batch 7/64 loss: -0.3028463125228882
Batch 8/64 loss: -0.2812306880950928
Batch 9/64 loss: -0.30729585886001587
Batch 10/64 loss: -0.2860984802246094
Batch 11/64 loss: -0.29462599754333496
Batch 12/64 loss: -0.29939064383506775
Batch 13/64 loss: -0.29521623253822327
Batch 14/64 loss: -0.29108816385269165
Batch 15/64 loss: -0.3019826114177704
Batch 16/64 loss: -0.30195456743240356
Batch 17/64 loss: -0.2948931157588959
Batch 18/64 loss: -0.2983989417552948
Batch 19/64 loss: -0.2890237867832184
Batch 20/64 loss: -0.29208552837371826
Batch 21/64 loss: -0.296244740486145
Batch 22/64 loss: -0.2957497239112854
Batch 23/64 loss: -0.28303346037864685
Batch 24/64 loss: -0.29220908880233765
Batch 25/64 loss: -0.29169219732284546
Batch 26/64 loss: -0.3015541732311249
Batch 27/64 loss: -0.29565054178237915
Batch 28/64 loss: -0.2986907362937927
Batch 29/64 loss: -0.2996859550476074
Batch 30/64 loss: -0.29272741079330444
Batch 31/64 loss: -0.30084627866744995
Batch 32/64 loss: -0.30014222860336304
Batch 33/64 loss: -0.2869411110877991
Batch 34/64 loss: -0.2948615550994873
Batch 35/64 loss: -0.2900235056877136
Batch 36/64 loss: -0.281992107629776
Batch 37/64 loss: -0.2953222393989563
Batch 38/64 loss: -0.29715874791145325
Batch 39/64 loss: -0.28533563017845154
Batch 40/64 loss: -0.2869252860546112
Batch 41/64 loss: -0.29120585322380066
Batch 42/64 loss: -0.2929152250289917
Batch 43/64 loss: -0.28888386487960815
Batch 44/64 loss: -0.2941831648349762
Batch 45/64 loss: -0.28078940510749817
Batch 46/64 loss: -0.285794198513031
Batch 47/64 loss: -0.2910587787628174
Batch 48/64 loss: -0.30395302176475525
Batch 49/64 loss: -0.2897942066192627
Batch 50/64 loss: -0.2772729992866516
Batch 51/64 loss: -0.296190083026886
Batch 52/64 loss: -0.29467493295669556
Batch 53/64 loss: -0.3023621141910553
Batch 54/64 loss: -0.2935396432876587
Batch 55/64 loss: -0.2885017991065979
Batch 56/64 loss: -0.3127351999282837
Batch 57/64 loss: -0.27291032671928406
Batch 58/64 loss: -0.295021653175354
Batch 59/64 loss: -0.2859400510787964
Batch 60/64 loss: -0.2907605767250061
Batch 61/64 loss: -0.29986700415611267
Batch 62/64 loss: -0.2843220829963684
Batch 63/64 loss: -0.2845315635204315
Batch 64/64 loss: -0.29048362374305725
Epoch 366  Train loss: -0.29276160527678097  Val loss: -0.27593680233070533
Saving best model, epoch: 366
Epoch 367
-------------------------------
Batch 1/64 loss: -0.2887479066848755
Batch 2/64 loss: -0.29430365562438965
Batch 3/64 loss: -0.2905883193016052
Batch 4/64 loss: -0.29461470246315
Batch 5/64 loss: -0.301092267036438
Batch 6/64 loss: -0.2857789397239685
Batch 7/64 loss: -0.2998584508895874
Batch 8/64 loss: -0.28349876403808594
Batch 9/64 loss: -0.2927505075931549
Batch 10/64 loss: -0.2731754779815674
Batch 11/64 loss: -0.2836323380470276
Batch 12/64 loss: -0.2870705723762512
Batch 13/64 loss: -0.2869596481323242
Batch 14/64 loss: -0.2972450256347656
Batch 15/64 loss: -0.29358547925949097
Batch 16/64 loss: -0.2681870758533478
Batch 17/64 loss: -0.28851956129074097
Batch 18/64 loss: -0.30057597160339355
Batch 19/64 loss: -0.2877504229545593
Batch 20/64 loss: -0.3034690022468567
Batch 21/64 loss: -0.28864917159080505
Batch 22/64 loss: -0.2939888834953308
Batch 23/64 loss: -0.29875022172927856
Batch 24/64 loss: -0.2959921658039093
Batch 25/64 loss: -0.2941802740097046
Batch 26/64 loss: -0.2884501516819
Batch 27/64 loss: -0.29675549268722534
Batch 28/64 loss: -0.2831076979637146
Batch 29/64 loss: -0.2863585948944092
Batch 30/64 loss: -0.2974977493286133
Batch 31/64 loss: -0.29886680841445923
Batch 32/64 loss: -0.29747676849365234
Batch 33/64 loss: -0.27771270275115967
Batch 34/64 loss: -0.29032567143440247
Batch 35/64 loss: -0.29725319147109985
Batch 36/64 loss: -0.2845791280269623
Batch 37/64 loss: -0.2893475294113159
Batch 38/64 loss: -0.2849382162094116
Batch 39/64 loss: -0.283214271068573
Batch 40/64 loss: -0.3020528554916382
Batch 41/64 loss: -0.2842113971710205
Batch 42/64 loss: -0.2813190221786499
Batch 43/64 loss: -0.28522413969039917
Batch 44/64 loss: -0.28128448128700256
Batch 45/64 loss: -0.2915027141571045
Batch 46/64 loss: -0.2911718189716339
Batch 47/64 loss: -0.2730368971824646
Batch 48/64 loss: -0.29710736870765686
Batch 49/64 loss: -0.28103527426719666
Batch 50/64 loss: -0.27080798149108887
Batch 51/64 loss: -0.28691303730010986
Batch 52/64 loss: -0.2617937922477722
Batch 53/64 loss: -0.2738192677497864
Batch 54/64 loss: -0.2789517641067505
Batch 55/64 loss: -0.2896950840950012
Batch 56/64 loss: -0.26296114921569824
Batch 57/64 loss: -0.2918413579463959
Batch 58/64 loss: -0.2853461503982544
Batch 59/64 loss: -0.2760073244571686
Batch 60/64 loss: -0.27225810289382935
Batch 61/64 loss: -0.2825244963169098
Batch 62/64 loss: -0.29011425375938416
Batch 63/64 loss: -0.2737797498703003
Batch 64/64 loss: -0.27704811096191406
Epoch 367  Train loss: -0.28708069558237115  Val loss: -0.25929754337494315
Epoch 368
-------------------------------
Batch 1/64 loss: -0.292267382144928
Batch 2/64 loss: -0.27070337533950806
Batch 3/64 loss: -0.27547964453697205
Batch 4/64 loss: -0.2928094267845154
Batch 5/64 loss: -0.28360533714294434
Batch 6/64 loss: -0.29960015416145325
Batch 7/64 loss: -0.2911814749240875
Batch 8/64 loss: -0.2793090045452118
Batch 9/64 loss: -0.28525322675704956
Batch 10/64 loss: -0.28147029876708984
Batch 11/64 loss: -0.293670654296875
Batch 12/64 loss: -0.3000486493110657
Batch 13/64 loss: -0.2774391770362854
Batch 14/64 loss: -0.27353352308273315
Batch 15/64 loss: -0.2903911769390106
Batch 16/64 loss: -0.30277106165885925
Batch 17/64 loss: -0.3016406297683716
Batch 18/64 loss: -0.28375154733657837
Batch 19/64 loss: -0.29302167892456055
Batch 20/64 loss: -0.30514445900917053
Batch 21/64 loss: -0.28669625520706177
Batch 22/64 loss: -0.29973104596138
Batch 23/64 loss: -0.28501057624816895
Batch 24/64 loss: -0.2824932336807251
Batch 25/64 loss: -0.2922009527683258
Batch 26/64 loss: -0.28986603021621704
Batch 27/64 loss: -0.2773846983909607
Batch 28/64 loss: -0.29017555713653564
Batch 29/64 loss: -0.2892645597457886
Batch 30/64 loss: -0.2859048843383789
Batch 31/64 loss: -0.3042154312133789
Batch 32/64 loss: -0.2919413447380066
Batch 33/64 loss: -0.28286266326904297
Batch 34/64 loss: -0.3080995976924896
Batch 35/64 loss: -0.30310434103012085
Batch 36/64 loss: -0.30047428607940674
Batch 37/64 loss: -0.271779328584671
Batch 38/64 loss: -0.28312161564826965
Batch 39/64 loss: -0.27684637904167175
Batch 40/64 loss: -0.297742635011673
Batch 41/64 loss: -0.29471808671951294
Batch 42/64 loss: -0.284360408782959
Batch 43/64 loss: -0.2808128595352173
Batch 44/64 loss: -0.2759581208229065
Batch 45/64 loss: -0.2839733958244324
Batch 46/64 loss: -0.2946212887763977
Batch 47/64 loss: -0.29941117763519287
Batch 48/64 loss: -0.2897500693798065
Batch 49/64 loss: -0.2883777320384979
Batch 50/64 loss: -0.2770373821258545
Batch 51/64 loss: -0.29501673579216003
Batch 52/64 loss: -0.27969950437545776
Batch 53/64 loss: -0.29273128509521484
Batch 54/64 loss: -0.2815432548522949
Batch 55/64 loss: -0.278934121131897
Batch 56/64 loss: -0.2749641537666321
Batch 57/64 loss: -0.3032391667366028
Batch 58/64 loss: -0.27983561158180237
Batch 59/64 loss: -0.28566431999206543
Batch 60/64 loss: -0.294941782951355
Batch 61/64 loss: -0.2969810962677002
Batch 62/64 loss: -0.2894648313522339
Batch 63/64 loss: -0.3003782629966736
Batch 64/64 loss: -0.2961520552635193
Epoch 368  Train loss: -0.28888683903451057  Val loss: -0.25259340444381295
Epoch 369
-------------------------------
Batch 1/64 loss: -0.28119662404060364
Batch 2/64 loss: -0.27818769216537476
Batch 3/64 loss: -0.2566182613372803
Batch 4/64 loss: -0.2989634871482849
Batch 5/64 loss: -0.2808351516723633
Batch 6/64 loss: -0.2958970069885254
Batch 7/64 loss: -0.2918316423892975
Batch 8/64 loss: -0.3024073541164398
Batch 9/64 loss: -0.2921064496040344
Batch 10/64 loss: -0.28846392035484314
Batch 11/64 loss: -0.28092220425605774
Batch 12/64 loss: -0.2871003746986389
Batch 13/64 loss: -0.28596529364585876
Batch 14/64 loss: -0.3014606833457947
Batch 15/64 loss: -0.27288973331451416
Batch 16/64 loss: -0.2922317087650299
Batch 17/64 loss: -0.2832930386066437
Batch 18/64 loss: -0.2886526882648468
Batch 19/64 loss: -0.29105767607688904
Batch 20/64 loss: -0.2891909182071686
Batch 21/64 loss: -0.2919025719165802
Batch 22/64 loss: -0.28062576055526733
Batch 23/64 loss: -0.29048699140548706
Batch 24/64 loss: -0.29444319009780884
Batch 25/64 loss: -0.27207744121551514
Batch 26/64 loss: -0.2749732434749603
Batch 27/64 loss: -0.2830859422683716
Batch 28/64 loss: -0.30255261063575745
Batch 29/64 loss: -0.28554433584213257
Batch 30/64 loss: -0.2723342776298523
Batch 31/64 loss: -0.29270055890083313
Batch 32/64 loss: -0.28846418857574463
Batch 33/64 loss: -0.296612948179245
Batch 34/64 loss: -0.2861173450946808
Batch 35/64 loss: -0.30076366662979126
Batch 36/64 loss: -0.28565943241119385
Batch 37/64 loss: -0.27833303809165955
Batch 38/64 loss: -0.2707669138908386
Batch 39/64 loss: -0.29447412490844727
Batch 40/64 loss: -0.29663121700286865
Batch 41/64 loss: -0.273395299911499
Batch 42/64 loss: -0.2969997525215149
Batch 43/64 loss: -0.2941116988658905
Batch 44/64 loss: -0.27869266271591187
Batch 45/64 loss: -0.2917591333389282
Batch 46/64 loss: -0.3002856373786926
Batch 47/64 loss: -0.28796565532684326
Batch 48/64 loss: -0.29021233320236206
Batch 49/64 loss: -0.28304123878479004
Batch 50/64 loss: -0.28695550560951233
Batch 51/64 loss: -0.27899202704429626
Batch 52/64 loss: -0.27225184440612793
Batch 53/64 loss: -0.28392481803894043
Batch 54/64 loss: -0.27736541628837585
Batch 55/64 loss: -0.2794714868068695
Batch 56/64 loss: -0.293243408203125
Batch 57/64 loss: -0.2802787125110626
Batch 58/64 loss: -0.2957085371017456
Batch 59/64 loss: -0.2977687120437622
Batch 60/64 loss: -0.3000122308731079
Batch 61/64 loss: -0.28868556022644043
Batch 62/64 loss: -0.2858176529407501
Batch 63/64 loss: -0.28550368547439575
Batch 64/64 loss: -0.2976360321044922
Epoch 369  Train loss: -0.2871135959438249  Val loss: -0.2674214506067361
Epoch 370
-------------------------------
Batch 1/64 loss: -0.2813212275505066
Batch 2/64 loss: -0.2889491617679596
Batch 3/64 loss: -0.2867647409439087
Batch 4/64 loss: -0.28769993782043457
Batch 5/64 loss: -0.3016546964645386
Batch 6/64 loss: -0.28813886642456055
Batch 7/64 loss: -0.29356178641319275
Batch 8/64 loss: -0.29950082302093506
Batch 9/64 loss: -0.2818797826766968
Batch 10/64 loss: -0.30146485567092896
Batch 11/64 loss: -0.305697500705719
Batch 12/64 loss: -0.27493709325790405
Batch 13/64 loss: -0.28981685638427734
Batch 14/64 loss: -0.2914419174194336
Batch 15/64 loss: -0.26401448249816895
Batch 16/64 loss: -0.28203749656677246
Batch 17/64 loss: -0.2799546718597412
Batch 18/64 loss: -0.27973029017448425
Batch 19/64 loss: -0.2832408547401428
Batch 20/64 loss: -0.29273396730422974
Batch 21/64 loss: -0.2933900058269501
Batch 22/64 loss: -0.28926312923431396
Batch 23/64 loss: -0.28949451446533203
Batch 24/64 loss: -0.283652663230896
Batch 25/64 loss: -0.2838454842567444
Batch 26/64 loss: -0.3006505072116852
Batch 27/64 loss: -0.29555457830429077
Batch 28/64 loss: -0.3011513948440552
Batch 29/64 loss: -0.28478628396987915
Batch 30/64 loss: -0.2695462703704834
Batch 31/64 loss: -0.2818682789802551
Batch 32/64 loss: -0.28748443722724915
Batch 33/64 loss: -0.30388110876083374
Batch 34/64 loss: -0.27612394094467163
Batch 35/64 loss: -0.2783122658729553
Batch 36/64 loss: -0.29294392466545105
Batch 37/64 loss: -0.28602322936058044
Batch 38/64 loss: -0.293620765209198
Batch 39/64 loss: -0.29766541719436646
Batch 40/64 loss: -0.2515125870704651
Batch 41/64 loss: -0.2799980044364929
Batch 42/64 loss: -0.27753356099128723
Batch 43/64 loss: -0.2974918782711029
Batch 44/64 loss: -0.29771846532821655
Batch 45/64 loss: -0.3026866316795349
Batch 46/64 loss: -0.28501543402671814
Batch 47/64 loss: -0.28570711612701416
Batch 48/64 loss: -0.2808242440223694
Batch 49/64 loss: -0.2815500497817993
Batch 50/64 loss: -0.2765742242336273
Batch 51/64 loss: -0.28342440724372864
Batch 52/64 loss: -0.2854421138763428
Batch 53/64 loss: -0.2809949517250061
Batch 54/64 loss: -0.28273823857307434
Batch 55/64 loss: -0.2928937077522278
Batch 56/64 loss: -0.27369239926338196
Batch 57/64 loss: -0.2933812439441681
Batch 58/64 loss: -0.2982301115989685
Batch 59/64 loss: -0.2854524850845337
Batch 60/64 loss: -0.2943853735923767
Batch 61/64 loss: -0.2894554138183594
Batch 62/64 loss: -0.28350287675857544
Batch 63/64 loss: -0.28435218334198
Batch 64/64 loss: -0.28931665420532227
Epoch 370  Train loss: -0.2871426572986678  Val loss: -0.23485019137359567
Epoch 371
-------------------------------
Batch 1/64 loss: -0.2753651738166809
Batch 2/64 loss: -0.29189154505729675
Batch 3/64 loss: -0.2785030007362366
Batch 4/64 loss: -0.2788325250148773
Batch 5/64 loss: -0.27899420261383057
Batch 6/64 loss: -0.2801762819290161
Batch 7/64 loss: -0.2875884175300598
Batch 8/64 loss: -0.276966392993927
Batch 9/64 loss: -0.2928708791732788
Batch 10/64 loss: -0.28179049491882324
Batch 11/64 loss: -0.2761728763580322
Batch 12/64 loss: -0.2758428454399109
Batch 13/64 loss: -0.2876342833042145
Batch 14/64 loss: -0.2892321050167084
Batch 15/64 loss: -0.2688186764717102
Batch 16/64 loss: -0.286344051361084
Batch 17/64 loss: -0.283621609210968
Batch 18/64 loss: -0.30396798253059387
Batch 19/64 loss: -0.29466837644577026
Batch 20/64 loss: -0.2786557078361511
Batch 21/64 loss: -0.2845362424850464
Batch 22/64 loss: -0.2876847982406616
Batch 23/64 loss: -0.2909295856952667
Batch 24/64 loss: -0.2960643172264099
Batch 25/64 loss: -0.279410719871521
Batch 26/64 loss: -0.27105292677879333
Batch 27/64 loss: -0.27544689178466797
Batch 28/64 loss: -0.2841401696205139
Batch 29/64 loss: -0.2843882441520691
Batch 30/64 loss: -0.274191677570343
Batch 31/64 loss: -0.297941118478775
Batch 32/64 loss: -0.28766438364982605
Batch 33/64 loss: -0.2952401340007782
Batch 34/64 loss: -0.2825072407722473
Batch 35/64 loss: -0.2959251403808594
Batch 36/64 loss: -0.2899453043937683
Batch 37/64 loss: -0.28752684593200684
Batch 38/64 loss: -0.28021106123924255
Batch 39/64 loss: -0.2983015477657318
Batch 40/64 loss: -0.28639236092567444
Batch 41/64 loss: -0.28765684366226196
Batch 42/64 loss: -0.28527921438217163
Batch 43/64 loss: -0.2794960141181946
Batch 44/64 loss: -0.2836143374443054
Batch 45/64 loss: -0.2945089340209961
Batch 46/64 loss: -0.2843390107154846
Batch 47/64 loss: -0.2796327471733093
Batch 48/64 loss: -0.2896716296672821
Batch 49/64 loss: -0.29035505652427673
Batch 50/64 loss: -0.2680184245109558
Batch 51/64 loss: -0.2641630172729492
Batch 52/64 loss: -0.2926762104034424
Batch 53/64 loss: -0.27793771028518677
Batch 54/64 loss: -0.27131134271621704
Batch 55/64 loss: -0.2822827100753784
Batch 56/64 loss: -0.2676735520362854
Batch 57/64 loss: -0.2888631224632263
Batch 58/64 loss: -0.2474731206893921
Batch 59/64 loss: -0.2843710780143738
Batch 60/64 loss: -0.28538888692855835
Batch 61/64 loss: -0.2837033271789551
Batch 62/64 loss: -0.2967812418937683
Batch 63/64 loss: -0.2912360429763794
Batch 64/64 loss: -0.2727741301059723
Epoch 371  Train loss: -0.2835835656699012  Val loss: -0.26762396435147706
Epoch 372
-------------------------------
Batch 1/64 loss: -0.29343584179878235
Batch 2/64 loss: -0.27752241492271423
Batch 3/64 loss: -0.2840323746204376
Batch 4/64 loss: -0.280874103307724
Batch 5/64 loss: -0.27831852436065674
Batch 6/64 loss: -0.27248746156692505
Batch 7/64 loss: -0.2923487424850464
Batch 8/64 loss: -0.2783224284648895
Batch 9/64 loss: -0.2989071011543274
Batch 10/64 loss: -0.29551321268081665
Batch 11/64 loss: -0.27385249733924866
Batch 12/64 loss: -0.29442688822746277
Batch 13/64 loss: -0.2871898114681244
Batch 14/64 loss: -0.2901558578014374
Batch 15/64 loss: -0.2898363471031189
Batch 16/64 loss: -0.2941780686378479
Batch 17/64 loss: -0.28392449021339417
Batch 18/64 loss: -0.2881755232810974
Batch 19/64 loss: -0.28236743807792664
Batch 20/64 loss: -0.2895527184009552
Batch 21/64 loss: -0.29108741879463196
Batch 22/64 loss: -0.2835538685321808
Batch 23/64 loss: -0.2923174500465393
Batch 24/64 loss: -0.2793456017971039
Batch 25/64 loss: -0.2873963713645935
Batch 26/64 loss: -0.2886362671852112
Batch 27/64 loss: -0.2868669629096985
Batch 28/64 loss: -0.301338791847229
Batch 29/64 loss: -0.301736444234848
Batch 30/64 loss: -0.2828640341758728
Batch 31/64 loss: -0.287523090839386
Batch 32/64 loss: -0.2889971435070038
Batch 33/64 loss: -0.2789841890335083
Batch 34/64 loss: -0.2947971224784851
Batch 35/64 loss: -0.2914015054702759
Batch 36/64 loss: -0.30493277311325073
Batch 37/64 loss: -0.288959801197052
Batch 38/64 loss: -0.28126803040504456
Batch 39/64 loss: -0.28468528389930725
Batch 40/64 loss: -0.2906706929206848
Batch 41/64 loss: -0.28868162631988525
Batch 42/64 loss: -0.290418416261673
Batch 43/64 loss: -0.29035067558288574
Batch 44/64 loss: -0.30418726801872253
Batch 45/64 loss: -0.29417240619659424
Batch 46/64 loss: -0.26310139894485474
Batch 47/64 loss: -0.29378649592399597
Batch 48/64 loss: -0.26809120178222656
Batch 49/64 loss: -0.2733655273914337
Batch 50/64 loss: -0.28957295417785645
Batch 51/64 loss: -0.28163108229637146
Batch 52/64 loss: -0.2921978831291199
Batch 53/64 loss: -0.2989158630371094
Batch 54/64 loss: -0.29090380668640137
Batch 55/64 loss: -0.28363823890686035
Batch 56/64 loss: -0.28237175941467285
Batch 57/64 loss: -0.28363746404647827
Batch 58/64 loss: -0.29229265451431274
Batch 59/64 loss: -0.2972645163536072
Batch 60/64 loss: -0.2890852689743042
Batch 61/64 loss: -0.28204014897346497
Batch 62/64 loss: -0.2834417223930359
Batch 63/64 loss: -0.25869882106781006
Batch 64/64 loss: -0.2726775109767914
Epoch 372  Train loss: -0.28688847095358605  Val loss: -0.2657842281757761
Epoch 373
-------------------------------
Batch 1/64 loss: -0.280302494764328
Batch 2/64 loss: -0.2914998531341553
Batch 3/64 loss: -0.2928771376609802
Batch 4/64 loss: -0.2844926416873932
Batch 5/64 loss: -0.299869567155838
Batch 6/64 loss: -0.28024744987487793
Batch 7/64 loss: -0.2879420220851898
Batch 8/64 loss: -0.27542242407798767
Batch 9/64 loss: -0.2810510993003845
Batch 10/64 loss: -0.29941803216934204
Batch 11/64 loss: -0.28308361768722534
Batch 12/64 loss: -0.28959551453590393
Batch 13/64 loss: -0.2970588803291321
Batch 14/64 loss: -0.2909039855003357
Batch 15/64 loss: -0.29335618019104004
Batch 16/64 loss: -0.29216867685317993
Batch 17/64 loss: -0.27158746123313904
Batch 18/64 loss: -0.27549558877944946
Batch 19/64 loss: -0.2743087112903595
Batch 20/64 loss: -0.2977214753627777
Batch 21/64 loss: -0.29031848907470703
Batch 22/64 loss: -0.3013420104980469
Batch 23/64 loss: -0.297483891248703
Batch 24/64 loss: -0.2976900339126587
Batch 25/64 loss: -0.28274253010749817
Batch 26/64 loss: -0.2870175242424011
Batch 27/64 loss: -0.2837179899215698
Batch 28/64 loss: -0.2935848832130432
Batch 29/64 loss: -0.2945369482040405
Batch 30/64 loss: -0.29435431957244873
Batch 31/64 loss: -0.2970747947692871
Batch 32/64 loss: -0.2993083596229553
Batch 33/64 loss: -0.29579463601112366
Batch 34/64 loss: -0.3028319776058197
Batch 35/64 loss: -0.288432240486145
Batch 36/64 loss: -0.2891412079334259
Batch 37/64 loss: -0.29243096709251404
Batch 38/64 loss: -0.29600217938423157
Batch 39/64 loss: -0.2855125069618225
Batch 40/64 loss: -0.2923370599746704
Batch 41/64 loss: -0.29897207021713257
Batch 42/64 loss: -0.28364965319633484
Batch 43/64 loss: -0.2753850221633911
Batch 44/64 loss: -0.2890225052833557
Batch 45/64 loss: -0.2851233184337616
Batch 46/64 loss: -0.28841835260391235
Batch 47/64 loss: -0.27929651737213135
Batch 48/64 loss: -0.29317423701286316
Batch 49/64 loss: -0.2749360501766205
Batch 50/64 loss: -0.2918853163719177
Batch 51/64 loss: -0.29719674587249756
Batch 52/64 loss: -0.2901494801044464
Batch 53/64 loss: -0.3010040521621704
Batch 54/64 loss: -0.29228639602661133
Batch 55/64 loss: -0.2954586148262024
Batch 56/64 loss: -0.2933477759361267
Batch 57/64 loss: -0.30598264932632446
Batch 58/64 loss: -0.29767677187919617
Batch 59/64 loss: -0.26933249831199646
Batch 60/64 loss: -0.29473477602005005
Batch 61/64 loss: -0.2827404737472534
Batch 62/64 loss: -0.3002539575099945
Batch 63/64 loss: -0.2770656943321228
Batch 64/64 loss: -0.292242169380188
Epoch 373  Train loss: -0.28982481442245783  Val loss: -0.2720979548401849
Epoch 374
-------------------------------
Batch 1/64 loss: -0.2865690290927887
Batch 2/64 loss: -0.3041313588619232
Batch 3/64 loss: -0.28873902559280396
Batch 4/64 loss: -0.3042559027671814
Batch 5/64 loss: -0.282204806804657
Batch 6/64 loss: -0.2737247049808502
Batch 7/64 loss: -0.2956169843673706
Batch 8/64 loss: -0.3016379475593567
Batch 9/64 loss: -0.2910963296890259
Batch 10/64 loss: -0.29730460047721863
Batch 11/64 loss: -0.28317683935165405
Batch 12/64 loss: -0.295526385307312
Batch 13/64 loss: -0.3050077557563782
Batch 14/64 loss: -0.29997509717941284
Batch 15/64 loss: -0.2906772792339325
Batch 16/64 loss: -0.2768923044204712
Batch 17/64 loss: -0.30062150955200195
Batch 18/64 loss: -0.28623926639556885
Batch 19/64 loss: -0.28429245948791504
Batch 20/64 loss: -0.2777670919895172
Batch 21/64 loss: -0.2995731830596924
Batch 22/64 loss: -0.286284863948822
Batch 23/64 loss: -0.28695136308670044
Batch 24/64 loss: -0.28849542140960693
Batch 25/64 loss: -0.2974015474319458
Batch 26/64 loss: -0.28724491596221924
Batch 27/64 loss: -0.28201183676719666
Batch 28/64 loss: -0.28872573375701904
Batch 29/64 loss: -0.29221367835998535
Batch 30/64 loss: -0.29635828733444214
Batch 31/64 loss: -0.27933788299560547
Batch 32/64 loss: -0.2997376322746277
Batch 33/64 loss: -0.29490548372268677
Batch 34/64 loss: -0.29580843448638916
Batch 35/64 loss: -0.2861921489238739
Batch 36/64 loss: -0.28824952244758606
Batch 37/64 loss: -0.28092318773269653
Batch 38/64 loss: -0.2766677737236023
Batch 39/64 loss: -0.2860865294933319
Batch 40/64 loss: -0.2866077423095703
Batch 41/64 loss: -0.2986203134059906
Batch 42/64 loss: -0.2920581102371216
Batch 43/64 loss: -0.2874318063259125
Batch 44/64 loss: -0.2957942485809326
Batch 45/64 loss: -0.290507972240448
Batch 46/64 loss: -0.29975980520248413
Batch 47/64 loss: -0.2912829518318176
Batch 48/64 loss: -0.28699904680252075
Batch 49/64 loss: -0.272769033908844
Batch 50/64 loss: -0.2940736413002014
Batch 51/64 loss: -0.2783238887786865
Batch 52/64 loss: -0.2974960207939148
Batch 53/64 loss: -0.2935398817062378
Batch 54/64 loss: -0.2850918769836426
Batch 55/64 loss: -0.30261680483818054
Batch 56/64 loss: -0.30116501450538635
Batch 57/64 loss: -0.2883723974227905
Batch 58/64 loss: -0.2712400555610657
Batch 59/64 loss: -0.28518831729888916
Batch 60/64 loss: -0.2916220426559448
Batch 61/64 loss: -0.287417471408844
Batch 62/64 loss: -0.30072277784347534
Batch 63/64 loss: -0.2908777892589569
Batch 64/64 loss: -0.2924412786960602
Epoch 374  Train loss: -0.2903142913883808  Val loss: -0.2614890885926604
Epoch 375
-------------------------------
Batch 1/64 loss: -0.29292821884155273
Batch 2/64 loss: -0.28887301683425903
Batch 3/64 loss: -0.2810748219490051
Batch 4/64 loss: -0.29737263917922974
Batch 5/64 loss: -0.3010721802711487
Batch 6/64 loss: -0.29559487104415894
Batch 7/64 loss: -0.30063676834106445
Batch 8/64 loss: -0.29831796884536743
Batch 9/64 loss: -0.29244816303253174
Batch 10/64 loss: -0.29315632581710815
Batch 11/64 loss: -0.28221237659454346
Batch 12/64 loss: -0.29491499066352844
Batch 13/64 loss: -0.3031933307647705
Batch 14/64 loss: -0.28916770219802856
Batch 15/64 loss: -0.2913709878921509
Batch 16/64 loss: -0.28939929604530334
Batch 17/64 loss: -0.30027905106544495
Batch 18/64 loss: -0.29032498598098755
Batch 19/64 loss: -0.3088863492012024
Batch 20/64 loss: -0.28843003511428833
Batch 21/64 loss: -0.2923956513404846
Batch 22/64 loss: -0.2909189462661743
Batch 23/64 loss: -0.2923620939254761
Batch 24/64 loss: -0.2864058017730713
Batch 25/64 loss: -0.30037447810173035
Batch 26/64 loss: -0.3036910593509674
Batch 27/64 loss: -0.2847049832344055
Batch 28/64 loss: -0.28130751848220825
Batch 29/64 loss: -0.2925429344177246
Batch 30/64 loss: -0.29420235753059387
Batch 31/64 loss: -0.2859935760498047
Batch 32/64 loss: -0.2930672764778137
Batch 33/64 loss: -0.2724458575248718
Batch 34/64 loss: -0.28256064653396606
Batch 35/64 loss: -0.2890990078449249
Batch 36/64 loss: -0.2885957360267639
Batch 37/64 loss: -0.2915436029434204
Batch 38/64 loss: -0.2936519682407379
Batch 39/64 loss: -0.28340986371040344
Batch 40/64 loss: -0.29332655668258667
Batch 41/64 loss: -0.29722321033477783
Batch 42/64 loss: -0.287452757358551
Batch 43/64 loss: -0.2894211709499359
Batch 44/64 loss: -0.28943702578544617
Batch 45/64 loss: -0.29664936661720276
Batch 46/64 loss: -0.272885262966156
Batch 47/64 loss: -0.28581854701042175
Batch 48/64 loss: -0.28152996301651
Batch 49/64 loss: -0.29436957836151123
Batch 50/64 loss: -0.29364800453186035
Batch 51/64 loss: -0.28576961159706116
Batch 52/64 loss: -0.29427534341812134
Batch 53/64 loss: -0.2967507541179657
Batch 54/64 loss: -0.30405402183532715
Batch 55/64 loss: -0.29193198680877686
Batch 56/64 loss: -0.2861470580101013
Batch 57/64 loss: -0.2896297574043274
Batch 58/64 loss: -0.2844795882701874
Batch 59/64 loss: -0.30312588810920715
Batch 60/64 loss: -0.2986706495285034
Batch 61/64 loss: -0.2788843512535095
Batch 62/64 loss: -0.2997100353240967
Batch 63/64 loss: -0.2965872287750244
Batch 64/64 loss: -0.2859750986099243
Epoch 375  Train loss: -0.29153233696432673  Val loss: -0.28241498855381075
Saving best model, epoch: 375
Epoch 376
-------------------------------
Batch 1/64 loss: -0.2891922891139984
Batch 2/64 loss: -0.3022266924381256
Batch 3/64 loss: -0.29357191920280457
Batch 4/64 loss: -0.2904248535633087
Batch 5/64 loss: -0.29071879386901855
Batch 6/64 loss: -0.297664999961853
Batch 7/64 loss: -0.3105090260505676
Batch 8/64 loss: -0.296602725982666
Batch 9/64 loss: -0.30078327655792236
Batch 10/64 loss: -0.28744733333587646
Batch 11/64 loss: -0.30128973722457886
Batch 12/64 loss: -0.29359230399131775
Batch 13/64 loss: -0.2912246286869049
Batch 14/64 loss: -0.2909923195838928
Batch 15/64 loss: -0.28448814153671265
Batch 16/64 loss: -0.29196545481681824
Batch 17/64 loss: -0.29407423734664917
Batch 18/64 loss: -0.2957034111022949
Batch 19/64 loss: -0.2951728105545044
Batch 20/64 loss: -0.2766034007072449
Batch 21/64 loss: -0.28898507356643677
Batch 22/64 loss: -0.2882276177406311
Batch 23/64 loss: -0.2903192639350891
Batch 24/64 loss: -0.2785235345363617
Batch 25/64 loss: -0.2883227467536926
Batch 26/64 loss: -0.28850841522216797
Batch 27/64 loss: -0.3013850748538971
Batch 28/64 loss: -0.2719065546989441
Batch 29/64 loss: -0.2918051481246948
Batch 30/64 loss: -0.3025889992713928
Batch 31/64 loss: -0.29360711574554443
Batch 32/64 loss: -0.2861550450325012
Batch 33/64 loss: -0.29956290125846863
Batch 34/64 loss: -0.2882285416126251
Batch 35/64 loss: -0.27806031703948975
Batch 36/64 loss: -0.2963486313819885
Batch 37/64 loss: -0.293063223361969
Batch 38/64 loss: -0.28969961404800415
Batch 39/64 loss: -0.2815037667751312
Batch 40/64 loss: -0.2970784306526184
Batch 41/64 loss: -0.2823929190635681
Batch 42/64 loss: -0.28539854288101196
Batch 43/64 loss: -0.2857247292995453
Batch 44/64 loss: -0.28689640760421753
Batch 45/64 loss: -0.2810622751712799
Batch 46/64 loss: -0.2781599462032318
Batch 47/64 loss: -0.2837809920310974
Batch 48/64 loss: -0.29453045129776
Batch 49/64 loss: -0.3060898184776306
Batch 50/64 loss: -0.2967745363712311
Batch 51/64 loss: -0.29520243406295776
Batch 52/64 loss: -0.2891145348548889
Batch 53/64 loss: -0.2969789505004883
Batch 54/64 loss: -0.28455132246017456
Batch 55/64 loss: -0.2902076244354248
Batch 56/64 loss: -0.29280325770378113
Batch 57/64 loss: -0.28698694705963135
Batch 58/64 loss: -0.2874651551246643
Batch 59/64 loss: -0.30376943945884705
Batch 60/64 loss: -0.28213924169540405
Batch 61/64 loss: -0.2743554711341858
Batch 62/64 loss: -0.2790696620941162
Batch 63/64 loss: -0.27962183952331543
Batch 64/64 loss: -0.29483699798583984
Epoch 376  Train loss: -0.29038953126645556  Val loss: -0.2705735754720944
Epoch 377
-------------------------------
Batch 1/64 loss: -0.30162835121154785
Batch 2/64 loss: -0.2810971736907959
Batch 3/64 loss: -0.28589338064193726
Batch 4/64 loss: -0.30541688203811646
Batch 5/64 loss: -0.30233362317085266
Batch 6/64 loss: -0.29088425636291504
Batch 7/64 loss: -0.2749815881252289
Batch 8/64 loss: -0.29358506202697754
Batch 9/64 loss: -0.29107439517974854
Batch 10/64 loss: -0.29776906967163086
Batch 11/64 loss: -0.2842194437980652
Batch 12/64 loss: -0.2883976697921753
Batch 13/64 loss: -0.29244011640548706
Batch 14/64 loss: -0.28820520639419556
Batch 15/64 loss: -0.29352277517318726
Batch 16/64 loss: -0.2942262887954712
Batch 17/64 loss: -0.27974605560302734
Batch 18/64 loss: -0.28972816467285156
Batch 19/64 loss: -0.2902553379535675
Batch 20/64 loss: -0.29898637533187866
Batch 21/64 loss: -0.2859342694282532
Batch 22/64 loss: -0.2978525161743164
Batch 23/64 loss: -0.28493237495422363
Batch 24/64 loss: -0.2958891987800598
Batch 25/64 loss: -0.2919835150241852
Batch 26/64 loss: -0.30003249645233154
Batch 27/64 loss: -0.30196860432624817
Batch 28/64 loss: -0.28960466384887695
Batch 29/64 loss: -0.28261807560920715
Batch 30/64 loss: -0.3058878183364868
Batch 31/64 loss: -0.27220284938812256
Batch 32/64 loss: -0.298467755317688
Batch 33/64 loss: -0.29926711320877075
Batch 34/64 loss: -0.28890204429626465
Batch 35/64 loss: -0.2948300242424011
Batch 36/64 loss: -0.2911571264266968
Batch 37/64 loss: -0.27929413318634033
Batch 38/64 loss: -0.2888146638870239
Batch 39/64 loss: -0.2903861701488495
Batch 40/64 loss: -0.2972671687602997
Batch 41/64 loss: -0.28084808588027954
Batch 42/64 loss: -0.27481189370155334
Batch 43/64 loss: -0.2792777419090271
Batch 44/64 loss: -0.28257548809051514
Batch 45/64 loss: -0.27189046144485474
Batch 46/64 loss: -0.26804327964782715
Batch 47/64 loss: -0.28832513093948364
Batch 48/64 loss: -0.2846600115299225
Batch 49/64 loss: -0.2781755328178406
Batch 50/64 loss: -0.28564944863319397
Batch 51/64 loss: -0.29183894395828247
Batch 52/64 loss: -0.28748661279678345
Batch 53/64 loss: -0.28212040662765503
Batch 54/64 loss: -0.2765812277793884
Batch 55/64 loss: -0.2901768982410431
Batch 56/64 loss: -0.28915998339653015
Batch 57/64 loss: -0.28014785051345825
Batch 58/64 loss: -0.29030686616897583
Batch 59/64 loss: -0.27871379256248474
Batch 60/64 loss: -0.28873980045318604
Batch 61/64 loss: -0.2988124489784241
Batch 62/64 loss: -0.2929909825325012
Batch 63/64 loss: -0.2867205739021301
Batch 64/64 loss: -0.2912803888320923
Epoch 377  Train loss: -0.2887560400308347  Val loss: -0.26428681550566685
Epoch 378
-------------------------------
Batch 1/64 loss: -0.2893405854701996
Batch 2/64 loss: -0.30019456148147583
Batch 3/64 loss: -0.2959080934524536
Batch 4/64 loss: -0.29461634159088135
Batch 5/64 loss: -0.29134976863861084
Batch 6/64 loss: -0.29917722940444946
Batch 7/64 loss: -0.2935429513454437
Batch 8/64 loss: -0.2995351254940033
Batch 9/64 loss: -0.2985532879829407
Batch 10/64 loss: -0.29708588123321533
Batch 11/64 loss: -0.27688589692115784
Batch 12/64 loss: -0.29111596941947937
Batch 13/64 loss: -0.31149759888648987
Batch 14/64 loss: -0.2976403534412384
Batch 15/64 loss: -0.2839083671569824
Batch 16/64 loss: -0.2935125231742859
Batch 17/64 loss: -0.28436291217803955
Batch 18/64 loss: -0.29222556948661804
Batch 19/64 loss: -0.2998439371585846
Batch 20/64 loss: -0.2812861502170563
Batch 21/64 loss: -0.2927272915840149
Batch 22/64 loss: -0.3027585744857788
Batch 23/64 loss: -0.293448269367218
Batch 24/64 loss: -0.27429208159446716
Batch 25/64 loss: -0.2936396896839142
Batch 26/64 loss: -0.2902849316596985
Batch 27/64 loss: -0.284677654504776
Batch 28/64 loss: -0.29725760221481323
Batch 29/64 loss: -0.288249671459198
Batch 30/64 loss: -0.2906104624271393
Batch 31/64 loss: -0.2832168936729431
Batch 32/64 loss: -0.28488361835479736
Batch 33/64 loss: -0.28101083636283875
Batch 34/64 loss: -0.2919657230377197
Batch 35/64 loss: -0.3110682964324951
Batch 36/64 loss: -0.285832017660141
Batch 37/64 loss: -0.3004096746444702
Batch 38/64 loss: -0.2936500012874603
Batch 39/64 loss: -0.2876904606819153
Batch 40/64 loss: -0.2938876748085022
Batch 41/64 loss: -0.2923038601875305
Batch 42/64 loss: -0.2867443859577179
Batch 43/64 loss: -0.28744858503341675
Batch 44/64 loss: -0.2913592457771301
Batch 45/64 loss: -0.28815215826034546
Batch 46/64 loss: -0.2909606099128723
Batch 47/64 loss: -0.306771457195282
Batch 48/64 loss: -0.2913625240325928
Batch 49/64 loss: -0.28063488006591797
Batch 50/64 loss: -0.29346662759780884
Batch 51/64 loss: -0.30789291858673096
Batch 52/64 loss: -0.29500019550323486
Batch 53/64 loss: -0.27701443433761597
Batch 54/64 loss: -0.3106709122657776
Batch 55/64 loss: -0.291869193315506
Batch 56/64 loss: -0.29199981689453125
Batch 57/64 loss: -0.276279091835022
Batch 58/64 loss: -0.2732504606246948
Batch 59/64 loss: -0.30015259981155396
Batch 60/64 loss: -0.30364441871643066
Batch 61/64 loss: -0.2842745780944824
Batch 62/64 loss: -0.29608336091041565
Batch 63/64 loss: -0.2827470302581787
Batch 64/64 loss: -0.2878643870353699
Epoch 378  Train loss: -0.29190786141975256  Val loss: -0.2690395762010948
Epoch 379
-------------------------------
Batch 1/64 loss: -0.28254085779190063
Batch 2/64 loss: -0.2929762005805969
Batch 3/64 loss: -0.29890912771224976
Batch 4/64 loss: -0.28815335035324097
Batch 5/64 loss: -0.2966306209564209
Batch 6/64 loss: -0.2866992950439453
Batch 7/64 loss: -0.27689892053604126
Batch 8/64 loss: -0.28190600872039795
Batch 9/64 loss: -0.2896381616592407
Batch 10/64 loss: -0.2738480567932129
Batch 11/64 loss: -0.29741814732551575
Batch 12/64 loss: -0.2955194413661957
Batch 13/64 loss: -0.2909819483757019
Batch 14/64 loss: -0.297473669052124
Batch 15/64 loss: -0.2994210422039032
Batch 16/64 loss: -0.29010283946990967
Batch 17/64 loss: -0.2881530821323395
Batch 18/64 loss: -0.28559523820877075
Batch 19/64 loss: -0.3009132444858551
Batch 20/64 loss: -0.2888193726539612
Batch 21/64 loss: -0.29222139716148376
Batch 22/64 loss: -0.2903136909008026
Batch 23/64 loss: -0.28573256731033325
Batch 24/64 loss: -0.30792340636253357
Batch 25/64 loss: -0.29209303855895996
Batch 26/64 loss: -0.294697642326355
Batch 27/64 loss: -0.29252028465270996
Batch 28/64 loss: -0.28129997849464417
Batch 29/64 loss: -0.29010218381881714
Batch 30/64 loss: -0.30221349000930786
Batch 31/64 loss: -0.3006095588207245
Batch 32/64 loss: -0.2911977171897888
Batch 33/64 loss: -0.2966576814651489
Batch 34/64 loss: -0.2892926335334778
Batch 35/64 loss: -0.29551637172698975
Batch 36/64 loss: -0.2840123176574707
Batch 37/64 loss: -0.2955224812030792
Batch 38/64 loss: -0.29159480333328247
Batch 39/64 loss: -0.2851102948188782
Batch 40/64 loss: -0.30116939544677734
Batch 41/64 loss: -0.29389488697052
Batch 42/64 loss: -0.3074585795402527
Batch 43/64 loss: -0.2914189100265503
Batch 44/64 loss: -0.29177290201187134
Batch 45/64 loss: -0.28647568821907043
Batch 46/64 loss: -0.28356343507766724
Batch 47/64 loss: -0.29580315947532654
Batch 48/64 loss: -0.2926872968673706
Batch 49/64 loss: -0.2981991171836853
Batch 50/64 loss: -0.2773994505405426
Batch 51/64 loss: -0.29331836104393005
Batch 52/64 loss: -0.3014683723449707
Batch 53/64 loss: -0.2925647497177124
Batch 54/64 loss: -0.2899646759033203
Batch 55/64 loss: -0.29858434200286865
Batch 56/64 loss: -0.30234384536743164
Batch 57/64 loss: -0.29362326860427856
Batch 58/64 loss: -0.30021995306015015
Batch 59/64 loss: -0.29786497354507446
Batch 60/64 loss: -0.28296998143196106
Batch 61/64 loss: -0.28323012590408325
Batch 62/64 loss: -0.2881935238838196
Batch 63/64 loss: -0.28641191124916077
Batch 64/64 loss: -0.2898147702217102
Epoch 379  Train loss: -0.2919088964368783  Val loss: -0.263036798365747
Epoch 380
-------------------------------
Batch 1/64 loss: -0.3053033649921417
Batch 2/64 loss: -0.29856133460998535
Batch 3/64 loss: -0.27945563197135925
Batch 4/64 loss: -0.2830463647842407
Batch 5/64 loss: -0.28625065088272095
Batch 6/64 loss: -0.28091490268707275
Batch 7/64 loss: -0.2875499129295349
Batch 8/64 loss: -0.29490694403648376
Batch 9/64 loss: -0.294398695230484
Batch 10/64 loss: -0.2899797856807709
Batch 11/64 loss: -0.3031744360923767
Batch 12/64 loss: -0.2823290228843689
Batch 13/64 loss: -0.2851167321205139
Batch 14/64 loss: -0.28768622875213623
Batch 15/64 loss: -0.2852068245410919
Batch 16/64 loss: -0.29714035987854004
Batch 17/64 loss: -0.2933053970336914
Batch 18/64 loss: -0.2762241065502167
Batch 19/64 loss: -0.282408744096756
Batch 20/64 loss: -0.28178703784942627
Batch 21/64 loss: -0.2913423776626587
Batch 22/64 loss: -0.26726865768432617
Batch 23/64 loss: -0.29510706663131714
Batch 24/64 loss: -0.28326666355133057
Batch 25/64 loss: -0.28509873151779175
Batch 26/64 loss: -0.2829586863517761
Batch 27/64 loss: -0.2844686210155487
Batch 28/64 loss: -0.2902664542198181
Batch 29/64 loss: -0.28834888339042664
Batch 30/64 loss: -0.27863791584968567
Batch 31/64 loss: -0.2854664921760559
Batch 32/64 loss: -0.2842692732810974
Batch 33/64 loss: -0.2920474708080292
Batch 34/64 loss: -0.2840048670768738
Batch 35/64 loss: -0.2908228039741516
Batch 36/64 loss: -0.28210970759391785
Batch 37/64 loss: -0.2835944592952728
Batch 38/64 loss: -0.2875918447971344
Batch 39/64 loss: -0.29564470052719116
Batch 40/64 loss: -0.300502210855484
Batch 41/64 loss: -0.2855665385723114
Batch 42/64 loss: -0.27810588479042053
Batch 43/64 loss: -0.28257322311401367
Batch 44/64 loss: -0.2948312759399414
Batch 45/64 loss: -0.2842507064342499
Batch 46/64 loss: -0.2960090637207031
Batch 47/64 loss: -0.2969086170196533
Batch 48/64 loss: -0.2857225835323334
Batch 49/64 loss: -0.294172078371048
Batch 50/64 loss: -0.2967137396335602
Batch 51/64 loss: -0.29303643107414246
Batch 52/64 loss: -0.29252204298973083
Batch 53/64 loss: -0.28651902079582214
Batch 54/64 loss: -0.29038161039352417
Batch 55/64 loss: -0.2895044684410095
Batch 56/64 loss: -0.30340081453323364
Batch 57/64 loss: -0.2960043251514435
Batch 58/64 loss: -0.27857816219329834
Batch 59/64 loss: -0.2757710814476013
Batch 60/64 loss: -0.2783881425857544
Batch 61/64 loss: -0.2967323064804077
Batch 62/64 loss: -0.2989499568939209
Batch 63/64 loss: -0.30013763904571533
Batch 64/64 loss: -0.3005356192588806
Epoch 380  Train loss: -0.2887489536229302  Val loss: -0.2757737554635379
Epoch 381
-------------------------------
Batch 1/64 loss: -0.29868078231811523
Batch 2/64 loss: -0.30918675661087036
Batch 3/64 loss: -0.29301318526268005
Batch 4/64 loss: -0.2818906307220459
Batch 5/64 loss: -0.27782171964645386
Batch 6/64 loss: -0.2940702438354492
Batch 7/64 loss: -0.2924884855747223
Batch 8/64 loss: -0.2928805947303772
Batch 9/64 loss: -0.28420472145080566
Batch 10/64 loss: -0.28620824217796326
Batch 11/64 loss: -0.2892721891403198
Batch 12/64 loss: -0.3059978485107422
Batch 13/64 loss: -0.30176085233688354
Batch 14/64 loss: -0.292452335357666
Batch 15/64 loss: -0.27992090582847595
Batch 16/64 loss: -0.2967716455459595
Batch 17/64 loss: -0.298808217048645
Batch 18/64 loss: -0.2897358536720276
Batch 19/64 loss: -0.294354110956192
Batch 20/64 loss: -0.2888718545436859
Batch 21/64 loss: -0.3023117184638977
Batch 22/64 loss: -0.2948388457298279
Batch 23/64 loss: -0.2880482077598572
Batch 24/64 loss: -0.3030698299407959
Batch 25/64 loss: -0.2753329277038574
Batch 26/64 loss: -0.2975407838821411
Batch 27/64 loss: -0.2861883342266083
Batch 28/64 loss: -0.2891008257865906
Batch 29/64 loss: -0.3037283718585968
Batch 30/64 loss: -0.2901191711425781
Batch 31/64 loss: -0.30474603176116943
Batch 32/64 loss: -0.2988584041595459
Batch 33/64 loss: -0.29445934295654297
Batch 34/64 loss: -0.2992309331893921
Batch 35/64 loss: -0.2930847406387329
Batch 36/64 loss: -0.28203392028808594
Batch 37/64 loss: -0.3005704879760742
Batch 38/64 loss: -0.31195640563964844
Batch 39/64 loss: -0.28302645683288574
Batch 40/64 loss: -0.30138862133026123
Batch 41/64 loss: -0.29032447934150696
Batch 42/64 loss: -0.29434317350387573
Batch 43/64 loss: -0.2878929078578949
Batch 44/64 loss: -0.2767968773841858
Batch 45/64 loss: -0.29149895906448364
Batch 46/64 loss: -0.2942902445793152
Batch 47/64 loss: -0.2931159734725952
Batch 48/64 loss: -0.30416345596313477
Batch 49/64 loss: -0.28760185837745667
Batch 50/64 loss: -0.28442731499671936
Batch 51/64 loss: -0.2948278486728668
Batch 52/64 loss: -0.2936287820339203
Batch 53/64 loss: -0.26666855812072754
Batch 54/64 loss: -0.28921645879745483
Batch 55/64 loss: -0.2982841730117798
Batch 56/64 loss: -0.2945178151130676
Batch 57/64 loss: -0.29291224479675293
Batch 58/64 loss: -0.29625582695007324
Batch 59/64 loss: -0.29217836260795593
Batch 60/64 loss: -0.29547983407974243
Batch 61/64 loss: -0.28629833459854126
Batch 62/64 loss: -0.2882145643234253
Batch 63/64 loss: -0.2867847979068756
Batch 64/64 loss: -0.27570465207099915
Epoch 381  Train loss: -0.29230630386109446  Val loss: -0.24478708243451988
Epoch 382
-------------------------------
Batch 1/64 loss: -0.2935140132904053
Batch 2/64 loss: -0.29070428013801575
Batch 3/64 loss: -0.29785895347595215
Batch 4/64 loss: -0.3018379807472229
Batch 5/64 loss: -0.30583667755126953
Batch 6/64 loss: -0.2902803421020508
Batch 7/64 loss: -0.29512840509414673
Batch 8/64 loss: -0.2917160391807556
Batch 9/64 loss: -0.29626595973968506
Batch 10/64 loss: -0.30017223954200745
Batch 11/64 loss: -0.27472543716430664
Batch 12/64 loss: -0.29369300603866577
Batch 13/64 loss: -0.2937239408493042
Batch 14/64 loss: -0.2952045500278473
Batch 15/64 loss: -0.28524428606033325
Batch 16/64 loss: -0.3050685524940491
Batch 17/64 loss: -0.28636494278907776
Batch 18/64 loss: -0.29208165407180786
Batch 19/64 loss: -0.2777388095855713
Batch 20/64 loss: -0.29074031114578247
Batch 21/64 loss: -0.2928944230079651
Batch 22/64 loss: -0.28146666288375854
Batch 23/64 loss: -0.2790972590446472
Batch 24/64 loss: -0.2836518883705139
Batch 25/64 loss: -0.2914807200431824
Batch 26/64 loss: -0.2913252115249634
Batch 27/64 loss: -0.3033820390701294
Batch 28/64 loss: -0.28182679414749146
Batch 29/64 loss: -0.29465624690055847
Batch 30/64 loss: -0.2860339879989624
Batch 31/64 loss: -0.2734948694705963
Batch 32/64 loss: -0.28063341975212097
Batch 33/64 loss: -0.2900737524032593
Batch 34/64 loss: -0.2787235677242279
Batch 35/64 loss: -0.2904728055000305
Batch 36/64 loss: -0.28746315836906433
Batch 37/64 loss: -0.2953794598579407
Batch 38/64 loss: -0.293840229511261
Batch 39/64 loss: -0.2843063473701477
Batch 40/64 loss: -0.2744302451610565
Batch 41/64 loss: -0.2946552336215973
Batch 42/64 loss: -0.30032461881637573
Batch 43/64 loss: -0.2913474440574646
Batch 44/64 loss: -0.2930985391139984
Batch 45/64 loss: -0.28968122601509094
Batch 46/64 loss: -0.2909044027328491
Batch 47/64 loss: -0.28614145517349243
Batch 48/64 loss: -0.2928294539451599
Batch 49/64 loss: -0.31070637702941895
Batch 50/64 loss: -0.29840609431266785
Batch 51/64 loss: -0.2949475347995758
Batch 52/64 loss: -0.2955324947834015
Batch 53/64 loss: -0.3022972345352173
Batch 54/64 loss: -0.3011583089828491
Batch 55/64 loss: -0.29069745540618896
Batch 56/64 loss: -0.30304938554763794
Batch 57/64 loss: -0.2960872948169708
Batch 58/64 loss: -0.29058802127838135
Batch 59/64 loss: -0.2837906777858734
Batch 60/64 loss: -0.2900560200214386
Batch 61/64 loss: -0.27105236053466797
Batch 62/64 loss: -0.3007316589355469
Batch 63/64 loss: -0.29417693614959717
Batch 64/64 loss: -0.30132192373275757
Epoch 382  Train loss: -0.2914632963199241  Val loss: -0.27575433684378553
Epoch 383
-------------------------------
Batch 1/64 loss: -0.2966812252998352
Batch 2/64 loss: -0.29950761795043945
Batch 3/64 loss: -0.30661922693252563
Batch 4/64 loss: -0.2987735867500305
Batch 5/64 loss: -0.3004305362701416
Batch 6/64 loss: -0.2968558073043823
Batch 7/64 loss: -0.295173317193985
Batch 8/64 loss: -0.28827959299087524
Batch 9/64 loss: -0.28300410509109497
Batch 10/64 loss: -0.28035274147987366
Batch 11/64 loss: -0.2841791808605194
Batch 12/64 loss: -0.2724507451057434
Batch 13/64 loss: -0.27495038509368896
Batch 14/64 loss: -0.29448843002319336
Batch 15/64 loss: -0.29485952854156494
Batch 16/64 loss: -0.27863723039627075
Batch 17/64 loss: -0.2902623414993286
Batch 18/64 loss: -0.28497180342674255
Batch 19/64 loss: -0.28074362874031067
Batch 20/64 loss: -0.29494041204452515
Batch 21/64 loss: -0.29758983850479126
Batch 22/64 loss: -0.30271196365356445
Batch 23/64 loss: -0.291864812374115
Batch 24/64 loss: -0.29523923993110657
Batch 25/64 loss: -0.2612413167953491
Batch 26/64 loss: -0.2918528914451599
Batch 27/64 loss: -0.286765992641449
Batch 28/64 loss: -0.3086128830909729
Batch 29/64 loss: -0.28654250502586365
Batch 30/64 loss: -0.2911365032196045
Batch 31/64 loss: -0.2728569507598877
Batch 32/64 loss: -0.28355199098587036
Batch 33/64 loss: -0.2969750165939331
Batch 34/64 loss: -0.29206985235214233
Batch 35/64 loss: -0.27104541659355164
Batch 36/64 loss: -0.2876385748386383
Batch 37/64 loss: -0.29435813426971436
Batch 38/64 loss: -0.28612226247787476
Batch 39/64 loss: -0.2908896207809448
Batch 40/64 loss: -0.29767096042633057
Batch 41/64 loss: -0.285334974527359
Batch 42/64 loss: -0.27013882994651794
Batch 43/64 loss: -0.2936437726020813
Batch 44/64 loss: -0.2959848642349243
Batch 45/64 loss: -0.2917802035808563
Batch 46/64 loss: -0.2837064266204834
Batch 47/64 loss: -0.26609694957733154
Batch 48/64 loss: -0.28822746872901917
Batch 49/64 loss: -0.28995221853256226
Batch 50/64 loss: -0.27514979243278503
Batch 51/64 loss: -0.2726755738258362
Batch 52/64 loss: -0.27530571818351746
Batch 53/64 loss: -0.2880409359931946
Batch 54/64 loss: -0.28759562969207764
Batch 55/64 loss: -0.2972344756126404
Batch 56/64 loss: -0.29945826530456543
Batch 57/64 loss: -0.2871221899986267
Batch 58/64 loss: -0.2955348491668701
Batch 59/64 loss: -0.2771609425544739
Batch 60/64 loss: -0.2506980895996094
Batch 61/64 loss: -0.28402119874954224
Batch 62/64 loss: -0.2607181668281555
Batch 63/64 loss: -0.2975856065750122
Batch 64/64 loss: -0.292625367641449
Epoch 383  Train loss: -0.2873024994251775  Val loss: -0.2597510302599353
Epoch 384
-------------------------------
Batch 1/64 loss: -0.2609967589378357
Batch 2/64 loss: -0.2798152565956116
Batch 3/64 loss: -0.2881414294242859
Batch 4/64 loss: -0.2797037959098816
Batch 5/64 loss: -0.27576422691345215
Batch 6/64 loss: -0.2850283980369568
Batch 7/64 loss: -0.2962125539779663
Batch 8/64 loss: -0.306735098361969
Batch 9/64 loss: -0.2812022268772125
Batch 10/64 loss: -0.2873704433441162
Batch 11/64 loss: -0.28873881697654724
Batch 12/64 loss: -0.27871960401535034
Batch 13/64 loss: -0.2676640748977661
Batch 14/64 loss: -0.2943003177642822
Batch 15/64 loss: -0.28274935483932495
Batch 16/64 loss: -0.2734396457672119
Batch 17/64 loss: -0.27826058864593506
Batch 18/64 loss: -0.25023382902145386
Batch 19/64 loss: -0.2921016216278076
Batch 20/64 loss: -0.2841411232948303
Batch 21/64 loss: -0.2821759581565857
Batch 22/64 loss: -0.2811320424079895
Batch 23/64 loss: -0.29316970705986023
Batch 24/64 loss: -0.2788878083229065
Batch 25/64 loss: -0.2951242923736572
Batch 26/64 loss: -0.2878907322883606
Batch 27/64 loss: -0.2870151400566101
Batch 28/64 loss: -0.2784603536128998
Batch 29/64 loss: -0.29212889075279236
Batch 30/64 loss: -0.2873748540878296
Batch 31/64 loss: -0.2961219847202301
Batch 32/64 loss: -0.26372647285461426
Batch 33/64 loss: -0.29909688234329224
Batch 34/64 loss: -0.28951722383499146
Batch 35/64 loss: -0.2868158221244812
Batch 36/64 loss: -0.27883630990982056
Batch 37/64 loss: -0.29494062066078186
Batch 38/64 loss: -0.28283464908599854
Batch 39/64 loss: -0.30046629905700684
Batch 40/64 loss: -0.29077935218811035
Batch 41/64 loss: -0.2870616018772125
Batch 42/64 loss: -0.29013487696647644
Batch 43/64 loss: -0.2918276786804199
Batch 44/64 loss: -0.30014076828956604
Batch 45/64 loss: -0.29234078526496887
Batch 46/64 loss: -0.2815682590007782
Batch 47/64 loss: -0.29356086254119873
Batch 48/64 loss: -0.27470189332962036
Batch 49/64 loss: -0.29805734753608704
Batch 50/64 loss: -0.27627596259117126
Batch 51/64 loss: -0.2803103029727936
Batch 52/64 loss: -0.2750961184501648
Batch 53/64 loss: -0.2937350571155548
Batch 54/64 loss: -0.2926625907421112
Batch 55/64 loss: -0.2967713475227356
Batch 56/64 loss: -0.28991174697875977
Batch 57/64 loss: -0.2760065793991089
Batch 58/64 loss: -0.29639869928359985
Batch 59/64 loss: -0.2757125794887543
Batch 60/64 loss: -0.27891242504119873
Batch 61/64 loss: -0.2783812880516052
Batch 62/64 loss: -0.28775763511657715
Batch 63/64 loss: -0.28665608167648315
Batch 64/64 loss: -0.29954564571380615
Epoch 384  Train loss: -0.2854346083659752  Val loss: -0.2637918753312625
Epoch 385
-------------------------------
Batch 1/64 loss: -0.2789565324783325
Batch 2/64 loss: -0.2732422947883606
Batch 3/64 loss: -0.28376585245132446
Batch 4/64 loss: -0.285810261964798
Batch 5/64 loss: -0.2806793451309204
Batch 6/64 loss: -0.28365927934646606
Batch 7/64 loss: -0.2789859175682068
Batch 8/64 loss: -0.300118625164032
Batch 9/64 loss: -0.2860714793205261
Batch 10/64 loss: -0.29172849655151367
Batch 11/64 loss: -0.2874387502670288
Batch 12/64 loss: -0.27748164534568787
Batch 13/64 loss: -0.2932254374027252
Batch 14/64 loss: -0.2907610535621643
Batch 15/64 loss: -0.2884112596511841
Batch 16/64 loss: -0.2871893048286438
Batch 17/64 loss: -0.24610233306884766
Batch 18/64 loss: -0.2940997779369354
Batch 19/64 loss: -0.2587316036224365
Batch 20/64 loss: -0.2886352837085724
Batch 21/64 loss: -0.2869112491607666
Batch 22/64 loss: -0.2981683313846588
Batch 23/64 loss: -0.29291582107543945
Batch 24/64 loss: -0.28346768021583557
Batch 25/64 loss: -0.2765125632286072
Batch 26/64 loss: -0.296455442905426
Batch 27/64 loss: -0.2721964716911316
Batch 28/64 loss: -0.27362966537475586
Batch 29/64 loss: -0.26491719484329224
Batch 30/64 loss: -0.30084866285324097
Batch 31/64 loss: -0.2775573134422302
Batch 32/64 loss: -0.284981906414032
Batch 33/64 loss: -0.27836698293685913
Batch 34/64 loss: -0.2887629270553589
Batch 35/64 loss: -0.2888517677783966
Batch 36/64 loss: -0.2852945923805237
Batch 37/64 loss: -0.28693363070487976
Batch 38/64 loss: -0.26009678840637207
Batch 39/64 loss: -0.27662551403045654
Batch 40/64 loss: -0.2927466630935669
Batch 41/64 loss: -0.2695557177066803
Batch 42/64 loss: -0.26455914974212646
Batch 43/64 loss: -0.27511709928512573
Batch 44/64 loss: -0.29912275075912476
Batch 45/64 loss: -0.28465262055397034
Batch 46/64 loss: -0.28709110617637634
Batch 47/64 loss: -0.29480329155921936
Batch 48/64 loss: -0.2830858826637268
Batch 49/64 loss: -0.28760120272636414
Batch 50/64 loss: -0.2276185154914856
Batch 51/64 loss: -0.25958430767059326
Batch 52/64 loss: -0.27290529012680054
Batch 53/64 loss: -0.2766788601875305
Batch 54/64 loss: -0.2904640734195709
Batch 55/64 loss: -0.2882951498031616
Batch 56/64 loss: -0.2916814088821411
Batch 57/64 loss: -0.30380120873451233
Batch 58/64 loss: -0.27826476097106934
Batch 59/64 loss: -0.2874835133552551
Batch 60/64 loss: -0.29399487376213074
Batch 61/64 loss: -0.2929336428642273
Batch 62/64 loss: -0.2939954102039337
Batch 63/64 loss: -0.2771557569503784
Batch 64/64 loss: -0.2897947430610657
Epoch 385  Train loss: -0.2826529781023661  Val loss: -0.24818427435720908
Epoch 386
-------------------------------
Batch 1/64 loss: -0.2844044864177704
Batch 2/64 loss: -0.2956348657608032
Batch 3/64 loss: -0.2780202627182007
Batch 4/64 loss: -0.28545480966567993
Batch 5/64 loss: -0.29917609691619873
Batch 6/64 loss: -0.29686617851257324
Batch 7/64 loss: -0.29800331592559814
Batch 8/64 loss: -0.2896440625190735
Batch 9/64 loss: -0.2912623882293701
Batch 10/64 loss: -0.2828253209590912
Batch 11/64 loss: -0.2892976403236389
Batch 12/64 loss: -0.3028087615966797
Batch 13/64 loss: -0.285652756690979
Batch 14/64 loss: -0.29132789373397827
Batch 15/64 loss: -0.2824305295944214
Batch 16/64 loss: -0.2988439202308655
Batch 17/64 loss: -0.2881578803062439
Batch 18/64 loss: -0.29970237612724304
Batch 19/64 loss: -0.30268269777297974
Batch 20/64 loss: -0.30189281702041626
Batch 21/64 loss: -0.296836793422699
Batch 22/64 loss: -0.27816784381866455
Batch 23/64 loss: -0.27463024854660034
Batch 24/64 loss: -0.28695350885391235
Batch 25/64 loss: -0.3007042109966278
Batch 26/64 loss: -0.2828071713447571
Batch 27/64 loss: -0.2810850143432617
Batch 28/64 loss: -0.2818005383014679
Batch 29/64 loss: -0.2774134874343872
Batch 30/64 loss: -0.29447126388549805
Batch 31/64 loss: -0.2957310676574707
Batch 32/64 loss: -0.2673236131668091
Batch 33/64 loss: -0.29268679022789
Batch 34/64 loss: -0.2971115708351135
Batch 35/64 loss: -0.2971319258213043
Batch 36/64 loss: -0.29938989877700806
Batch 37/64 loss: -0.295529842376709
Batch 38/64 loss: -0.2914632558822632
Batch 39/64 loss: -0.28763043880462646
Batch 40/64 loss: -0.2954181134700775
Batch 41/64 loss: -0.2856912612915039
Batch 42/64 loss: -0.29325366020202637
Batch 43/64 loss: -0.27634286880493164
Batch 44/64 loss: -0.29401278495788574
Batch 45/64 loss: -0.29457810521125793
Batch 46/64 loss: -0.2746385633945465
Batch 47/64 loss: -0.2779206931591034
Batch 48/64 loss: -0.3078887462615967
Batch 49/64 loss: -0.2839701175689697
Batch 50/64 loss: -0.29653704166412354
Batch 51/64 loss: -0.28788378834724426
Batch 52/64 loss: -0.2894444763660431
Batch 53/64 loss: -0.2871595621109009
Batch 54/64 loss: -0.29755187034606934
Batch 55/64 loss: -0.2887721061706543
Batch 56/64 loss: -0.2878977954387665
Batch 57/64 loss: -0.28805944323539734
Batch 58/64 loss: -0.29114317893981934
Batch 59/64 loss: -0.29891854524612427
Batch 60/64 loss: -0.2811886668205261
Batch 61/64 loss: -0.29716557264328003
Batch 62/64 loss: -0.30590856075286865
Batch 63/64 loss: -0.29457226395606995
Batch 64/64 loss: -0.2820108234882355
Epoch 386  Train loss: -0.29035895607050727  Val loss: -0.25542056478585573
Epoch 387
-------------------------------
Batch 1/64 loss: -0.2844952940940857
Batch 2/64 loss: -0.2730260491371155
Batch 3/64 loss: -0.28147295117378235
Batch 4/64 loss: -0.2991379499435425
Batch 5/64 loss: -0.29336410760879517
Batch 6/64 loss: -0.282573401927948
Batch 7/64 loss: -0.2863679528236389
Batch 8/64 loss: -0.2904859781265259
Batch 9/64 loss: -0.2872886061668396
Batch 10/64 loss: -0.28687527775764465
Batch 11/64 loss: -0.2964387536048889
Batch 12/64 loss: -0.29515743255615234
Batch 13/64 loss: -0.27143752574920654
Batch 14/64 loss: -0.2958647608757019
Batch 15/64 loss: -0.29700249433517456
Batch 16/64 loss: -0.2942681908607483
Batch 17/64 loss: -0.2830108106136322
Batch 18/64 loss: -0.29010918736457825
Batch 19/64 loss: -0.2927454710006714
Batch 20/64 loss: -0.2889723777770996
Batch 21/64 loss: -0.2834409475326538
Batch 22/64 loss: -0.2969023883342743
Batch 23/64 loss: -0.277322918176651
Batch 24/64 loss: -0.277481347322464
Batch 25/64 loss: -0.29564300179481506
Batch 26/64 loss: -0.29379570484161377
Batch 27/64 loss: -0.2946088910102844
Batch 28/64 loss: -0.28125452995300293
Batch 29/64 loss: -0.30077266693115234
Batch 30/64 loss: -0.2982858419418335
Batch 31/64 loss: -0.30109816789627075
Batch 32/64 loss: -0.29240185022354126
Batch 33/64 loss: -0.289855420589447
Batch 34/64 loss: -0.25539571046829224
Batch 35/64 loss: -0.29548612236976624
Batch 36/64 loss: -0.2803715169429779
Batch 37/64 loss: -0.2864881753921509
Batch 38/64 loss: -0.27846208214759827
Batch 39/64 loss: -0.304492712020874
Batch 40/64 loss: -0.29108595848083496
Batch 41/64 loss: -0.2944546341896057
Batch 42/64 loss: -0.296126127243042
Batch 43/64 loss: -0.2756614685058594
Batch 44/64 loss: -0.2868489623069763
Batch 45/64 loss: -0.2831436097621918
Batch 46/64 loss: -0.30224889516830444
Batch 47/64 loss: -0.30163276195526123
Batch 48/64 loss: -0.29374849796295166
Batch 49/64 loss: -0.28337037563323975
Batch 50/64 loss: -0.292701780796051
Batch 51/64 loss: -0.28871065378189087
Batch 52/64 loss: -0.301252543926239
Batch 53/64 loss: -0.2965623140335083
Batch 54/64 loss: -0.2843610346317291
Batch 55/64 loss: -0.2946842312812805
Batch 56/64 loss: -0.29556065797805786
Batch 57/64 loss: -0.28827518224716187
Batch 58/64 loss: -0.2895580530166626
Batch 59/64 loss: -0.29742419719696045
Batch 60/64 loss: -0.2975298762321472
Batch 61/64 loss: -0.2982598841190338
Batch 62/64 loss: -0.2877424359321594
Batch 63/64 loss: -0.28953954577445984
Batch 64/64 loss: -0.30081290006637573
Epoch 387  Train loss: -0.290035261593613  Val loss: -0.2666671296575225
Epoch 388
-------------------------------
Batch 1/64 loss: -0.2754686176776886
Batch 2/64 loss: -0.29908326268196106
Batch 3/64 loss: -0.3009030818939209
Batch 4/64 loss: -0.30252861976623535
Batch 5/64 loss: -0.28356218338012695
Batch 6/64 loss: -0.29471784830093384
Batch 7/64 loss: -0.2825896739959717
Batch 8/64 loss: -0.2886362075805664
Batch 9/64 loss: -0.29957517981529236
Batch 10/64 loss: -0.310064435005188
Batch 11/64 loss: -0.29628774523735046
Batch 12/64 loss: -0.29641711711883545
Batch 13/64 loss: -0.2996114492416382
Batch 14/64 loss: -0.29078829288482666
Batch 15/64 loss: -0.28692150115966797
Batch 16/64 loss: -0.2670067548751831
Batch 17/64 loss: -0.26817578077316284
Batch 18/64 loss: -0.27628952264785767
Batch 19/64 loss: -0.2929772138595581
Batch 20/64 loss: -0.2846141457557678
Batch 21/64 loss: -0.2851681709289551
Batch 22/64 loss: -0.28955286741256714
Batch 23/64 loss: -0.2762838304042816
Batch 24/64 loss: -0.2988620400428772
Batch 25/64 loss: -0.29063525795936584
Batch 26/64 loss: -0.2935134172439575
Batch 27/64 loss: -0.28059107065200806
Batch 28/64 loss: -0.28866416215896606
Batch 29/64 loss: -0.2938251495361328
Batch 30/64 loss: -0.2910010814666748
Batch 31/64 loss: -0.28948140144348145
Batch 32/64 loss: -0.3024967610836029
Batch 33/64 loss: -0.3083604574203491
Batch 34/64 loss: -0.2971082031726837
Batch 35/64 loss: -0.2930285930633545
Batch 36/64 loss: -0.30408743023872375
Batch 37/64 loss: -0.28558602929115295
Batch 38/64 loss: -0.29299259185791016
Batch 39/64 loss: -0.2848713994026184
Batch 40/64 loss: -0.278399795293808
Batch 41/64 loss: -0.2892465591430664
Batch 42/64 loss: -0.29188260436058044
Batch 43/64 loss: -0.2879628837108612
Batch 44/64 loss: -0.2808246612548828
Batch 45/64 loss: -0.2909512221813202
Batch 46/64 loss: -0.29928964376449585
Batch 47/64 loss: -0.2932717800140381
Batch 48/64 loss: -0.2853256165981293
Batch 49/64 loss: -0.29463934898376465
Batch 50/64 loss: -0.29952889680862427
Batch 51/64 loss: -0.2845842242240906
Batch 52/64 loss: -0.294349730014801
Batch 53/64 loss: -0.29364657402038574
Batch 54/64 loss: -0.2999274730682373
Batch 55/64 loss: -0.29291826486587524
Batch 56/64 loss: -0.28853124380111694
Batch 57/64 loss: -0.30205148458480835
Batch 58/64 loss: -0.28934234380722046
Batch 59/64 loss: -0.2864988148212433
Batch 60/64 loss: -0.2924354672431946
Batch 61/64 loss: -0.2716037333011627
Batch 62/64 loss: -0.2969924211502075
Batch 63/64 loss: -0.2927743196487427
Batch 64/64 loss: -0.29470688104629517
Epoch 388  Train loss: -0.29082882848440433  Val loss: -0.25969292761124285
Epoch 389
-------------------------------
Batch 1/64 loss: -0.27194398641586304
Batch 2/64 loss: -0.2930063009262085
Batch 3/64 loss: -0.2933101952075958
Batch 4/64 loss: -0.3003992438316345
Batch 5/64 loss: -0.28953105211257935
Batch 6/64 loss: -0.2772389352321625
Batch 7/64 loss: -0.28879064321517944
Batch 8/64 loss: -0.29300832748413086
Batch 9/64 loss: -0.29750126600265503
Batch 10/64 loss: -0.2958448529243469
Batch 11/64 loss: -0.27127233147621155
Batch 12/64 loss: -0.28608638048171997
Batch 13/64 loss: -0.28416895866394043
Batch 14/64 loss: -0.27709412574768066
Batch 15/64 loss: -0.2782471477985382
Batch 16/64 loss: -0.2925747036933899
Batch 17/64 loss: -0.2764514982700348
Batch 18/64 loss: -0.28568434715270996
Batch 19/64 loss: -0.2776426672935486
Batch 20/64 loss: -0.3001762628555298
Batch 21/64 loss: -0.26544106006622314
Batch 22/64 loss: -0.2927684187889099
Batch 23/64 loss: -0.2976895570755005
Batch 24/64 loss: -0.2885984778404236
Batch 25/64 loss: -0.29807907342910767
Batch 26/64 loss: -0.2920829653739929
Batch 27/64 loss: -0.27438509464263916
Batch 28/64 loss: -0.283974289894104
Batch 29/64 loss: -0.28952786326408386
Batch 30/64 loss: -0.2894187867641449
Batch 31/64 loss: -0.2888180911540985
Batch 32/64 loss: -0.2845751643180847
Batch 33/64 loss: -0.3143312335014343
Batch 34/64 loss: -0.2963106036186218
Batch 35/64 loss: -0.301300048828125
Batch 36/64 loss: -0.27472686767578125
Batch 37/64 loss: -0.2930261194705963
Batch 38/64 loss: -0.2849292755126953
Batch 39/64 loss: -0.2965945601463318
Batch 40/64 loss: -0.2862702012062073
Batch 41/64 loss: -0.30447572469711304
Batch 42/64 loss: -0.28879666328430176
Batch 43/64 loss: -0.2900748550891876
Batch 44/64 loss: -0.2930605411529541
Batch 45/64 loss: -0.2897936701774597
Batch 46/64 loss: -0.2946944832801819
Batch 47/64 loss: -0.2819918692111969
Batch 48/64 loss: -0.2902621030807495
Batch 49/64 loss: -0.26297682523727417
Batch 50/64 loss: -0.28709715604782104
Batch 51/64 loss: -0.26988351345062256
Batch 52/64 loss: -0.3033652901649475
Batch 53/64 loss: -0.29356634616851807
Batch 54/64 loss: -0.2977263927459717
Batch 55/64 loss: -0.29108914732933044
Batch 56/64 loss: -0.3085036277770996
Batch 57/64 loss: -0.28917941451072693
Batch 58/64 loss: -0.2811722159385681
Batch 59/64 loss: -0.2954389750957489
Batch 60/64 loss: -0.293013334274292
Batch 61/64 loss: -0.2975456416606903
Batch 62/64 loss: -0.29034876823425293
Batch 63/64 loss: -0.28550028800964355
Batch 64/64 loss: -0.2813701629638672
Epoch 389  Train loss: -0.28883773252075795  Val loss: -0.2709839710664913
Epoch 390
-------------------------------
Batch 1/64 loss: -0.2937315106391907
Batch 2/64 loss: -0.2629472017288208
Batch 3/64 loss: -0.2638906240463257
Batch 4/64 loss: -0.3018481731414795
Batch 5/64 loss: -0.2796878218650818
Batch 6/64 loss: -0.29256701469421387
Batch 7/64 loss: -0.29726648330688477
Batch 8/64 loss: -0.2898620367050171
Batch 9/64 loss: -0.2879609763622284
Batch 10/64 loss: -0.2906162142753601
Batch 11/64 loss: -0.27592602372169495
Batch 12/64 loss: -0.30046725273132324
Batch 13/64 loss: -0.2960469722747803
Batch 14/64 loss: -0.29352566599845886
Batch 15/64 loss: -0.30042731761932373
Batch 16/64 loss: -0.29112714529037476
Batch 17/64 loss: -0.2822394371032715
Batch 18/64 loss: -0.29280585050582886
Batch 19/64 loss: -0.2816033661365509
Batch 20/64 loss: -0.2823736369609833
Batch 21/64 loss: -0.2965104877948761
Batch 22/64 loss: -0.28660517930984497
Batch 23/64 loss: -0.29348695278167725
Batch 24/64 loss: -0.30294206738471985
Batch 25/64 loss: -0.30805066227912903
Batch 26/64 loss: -0.3003402352333069
Batch 27/64 loss: -0.2982773184776306
Batch 28/64 loss: -0.3001772165298462
Batch 29/64 loss: -0.2956041693687439
Batch 30/64 loss: -0.29940828680992126
Batch 31/64 loss: -0.3007940649986267
Batch 32/64 loss: -0.29333215951919556
Batch 33/64 loss: -0.2956784963607788
Batch 34/64 loss: -0.3052329421043396
Batch 35/64 loss: -0.28922680020332336
Batch 36/64 loss: -0.29409733414649963
Batch 37/64 loss: -0.2816730737686157
Batch 38/64 loss: -0.2853907346725464
Batch 39/64 loss: -0.2777840197086334
Batch 40/64 loss: -0.28573423624038696
Batch 41/64 loss: -0.28514766693115234
Batch 42/64 loss: -0.2961328625679016
Batch 43/64 loss: -0.28501570224761963
Batch 44/64 loss: -0.2980705201625824
Batch 45/64 loss: -0.2668469548225403
Batch 46/64 loss: -0.2947092652320862
Batch 47/64 loss: -0.29148897528648376
Batch 48/64 loss: -0.29211926460266113
Batch 49/64 loss: -0.28607484698295593
Batch 50/64 loss: -0.29075759649276733
Batch 51/64 loss: -0.2911425828933716
Batch 52/64 loss: -0.2948433756828308
Batch 53/64 loss: -0.27715426683425903
Batch 54/64 loss: -0.2829994559288025
Batch 55/64 loss: -0.29642757773399353
Batch 56/64 loss: -0.28789764642715454
Batch 57/64 loss: -0.28836163878440857
Batch 58/64 loss: -0.299074649810791
Batch 59/64 loss: -0.2905636429786682
Batch 60/64 loss: -0.28876858949661255
Batch 61/64 loss: -0.29672157764434814
Batch 62/64 loss: -0.29117685556411743
Batch 63/64 loss: -0.28556495904922485
Batch 64/64 loss: -0.29654258489608765
Epoch 390  Train loss: -0.2906154443235958  Val loss: -0.2680463608597562
Epoch 391
-------------------------------
Batch 1/64 loss: -0.29782795906066895
Batch 2/64 loss: -0.3050234317779541
Batch 3/64 loss: -0.28656384348869324
Batch 4/64 loss: -0.2855748236179352
Batch 5/64 loss: -0.3027934432029724
Batch 6/64 loss: -0.3046736419200897
Batch 7/64 loss: -0.2908724546432495
Batch 8/64 loss: -0.28902095556259155
Batch 9/64 loss: -0.28634047508239746
Batch 10/64 loss: -0.28704360127449036
Batch 11/64 loss: -0.2892310321331024
Batch 12/64 loss: -0.2823597490787506
Batch 13/64 loss: -0.29677242040634155
Batch 14/64 loss: -0.2955963909626007
Batch 15/64 loss: -0.28255289793014526
Batch 16/64 loss: -0.3048620820045471
Batch 17/64 loss: -0.2681061029434204
Batch 18/64 loss: -0.28347480297088623
Batch 19/64 loss: -0.28898411989212036
Batch 20/64 loss: -0.3004051446914673
Batch 21/64 loss: -0.2908925414085388
Batch 22/64 loss: -0.288178026676178
Batch 23/64 loss: -0.30037736892700195
Batch 24/64 loss: -0.28711238503456116
Batch 25/64 loss: -0.2740236818790436
Batch 26/64 loss: -0.28764262795448303
Batch 27/64 loss: -0.30399465560913086
Batch 28/64 loss: -0.2936294376850128
Batch 29/64 loss: -0.29214537143707275
Batch 30/64 loss: -0.2903607487678528
Batch 31/64 loss: -0.28690534830093384
Batch 32/64 loss: -0.29732075333595276
Batch 33/64 loss: -0.256669282913208
Batch 34/64 loss: -0.2865230441093445
Batch 35/64 loss: -0.26070475578308105
Batch 36/64 loss: -0.284490168094635
Batch 37/64 loss: -0.26390886306762695
Batch 38/64 loss: -0.2862875461578369
Batch 39/64 loss: -0.30106422305107117
Batch 40/64 loss: -0.2946013808250427
Batch 41/64 loss: -0.2834010124206543
Batch 42/64 loss: -0.2862764000892639
Batch 43/64 loss: -0.29592815041542053
Batch 44/64 loss: -0.2635113596916199
Batch 45/64 loss: -0.29527440667152405
Batch 46/64 loss: -0.28548282384872437
Batch 47/64 loss: -0.2982806861400604
Batch 48/64 loss: -0.2882915735244751
Batch 49/64 loss: -0.29330188035964966
Batch 50/64 loss: -0.2991187572479248
Batch 51/64 loss: -0.2795717716217041
Batch 52/64 loss: -0.2778131365776062
Batch 53/64 loss: -0.2894601821899414
Batch 54/64 loss: -0.29777348041534424
Batch 55/64 loss: -0.29479485750198364
Batch 56/64 loss: -0.28759801387786865
Batch 57/64 loss: -0.2928028404712677
Batch 58/64 loss: -0.30731332302093506
Batch 59/64 loss: -0.27921104431152344
Batch 60/64 loss: -0.2812773287296295
Batch 61/64 loss: -0.28324490785598755
Batch 62/64 loss: -0.27649399638175964
Batch 63/64 loss: -0.29700136184692383
Batch 64/64 loss: -0.2696705758571625
Epoch 391  Train loss: -0.28847667154143836  Val loss: -0.2578250384822334
Epoch 392
-------------------------------
Batch 1/64 loss: -0.2981981039047241
Batch 2/64 loss: -0.29522770643234253
Batch 3/64 loss: -0.2862360179424286
Batch 4/64 loss: -0.289539098739624
Batch 5/64 loss: -0.2873220443725586
Batch 6/64 loss: -0.29189532995224
Batch 7/64 loss: -0.28269025683403015
Batch 8/64 loss: -0.2690890431404114
Batch 9/64 loss: -0.2841953635215759
Batch 10/64 loss: -0.27989861369132996
Batch 11/64 loss: -0.27743858098983765
Batch 12/64 loss: -0.28509682416915894
Batch 13/64 loss: -0.28337886929512024
Batch 14/64 loss: -0.2931334972381592
Batch 15/64 loss: -0.26218050718307495
Batch 16/64 loss: -0.27913039922714233
Batch 17/64 loss: -0.29066240787506104
Batch 18/64 loss: -0.2559305429458618
Batch 19/64 loss: -0.27952665090560913
Batch 20/64 loss: -0.2830957770347595
Batch 21/64 loss: -0.2935549020767212
Batch 22/64 loss: -0.281820148229599
Batch 23/64 loss: -0.27978360652923584
Batch 24/64 loss: -0.27834540605545044
Batch 25/64 loss: -0.2940010130405426
Batch 26/64 loss: -0.28491443395614624
Batch 27/64 loss: -0.2819911539554596
Batch 28/64 loss: -0.2697153687477112
Batch 29/64 loss: -0.2905568480491638
Batch 30/64 loss: -0.2908322215080261
Batch 31/64 loss: -0.2971445322036743
Batch 32/64 loss: -0.3004063069820404
Batch 33/64 loss: -0.2939472496509552
Batch 34/64 loss: -0.25404858589172363
Batch 35/64 loss: -0.2529025077819824
Batch 36/64 loss: -0.2873588800430298
Batch 37/64 loss: -0.2675706148147583
Batch 38/64 loss: -0.26131314039230347
Batch 39/64 loss: -0.2712031900882721
Batch 40/64 loss: -0.29067572951316833
Batch 41/64 loss: -0.2935456335544586
Batch 42/64 loss: -0.27354222536087036
Batch 43/64 loss: -0.2787398099899292
Batch 44/64 loss: -0.30262547731399536
Batch 45/64 loss: -0.29369017481803894
Batch 46/64 loss: -0.28473228216171265
Batch 47/64 loss: -0.2952756881713867
Batch 48/64 loss: -0.29968297481536865
Batch 49/64 loss: -0.2821305990219116
Batch 50/64 loss: -0.3000193238258362
Batch 51/64 loss: -0.29049384593963623
Batch 52/64 loss: -0.279082715511322
Batch 53/64 loss: -0.2837130129337311
Batch 54/64 loss: -0.2929117977619171
Batch 55/64 loss: -0.2867145538330078
Batch 56/64 loss: -0.26568955183029175
Batch 57/64 loss: -0.29403427243232727
Batch 58/64 loss: -0.2867680788040161
Batch 59/64 loss: -0.27906912565231323
Batch 60/64 loss: -0.262961208820343
Batch 61/64 loss: -0.2980552613735199
Batch 62/64 loss: -0.28318145871162415
Batch 63/64 loss: -0.26360344886779785
Batch 64/64 loss: -0.2939571738243103
Epoch 392  Train loss: -0.2833989314004487  Val loss: -0.2732636871206801
Epoch 393
-------------------------------
Batch 1/64 loss: -0.29279083013534546
Batch 2/64 loss: -0.2868199944496155
Batch 3/64 loss: -0.28945392370224
Batch 4/64 loss: -0.29718607664108276
Batch 5/64 loss: -0.30551230907440186
Batch 6/64 loss: -0.2760515809059143
Batch 7/64 loss: -0.25772178173065186
Batch 8/64 loss: -0.29984498023986816
Batch 9/64 loss: -0.28035932779312134
Batch 10/64 loss: -0.3007492423057556
Batch 11/64 loss: -0.2718956172466278
Batch 12/64 loss: -0.29482728242874146
Batch 13/64 loss: -0.29579755663871765
Batch 14/64 loss: -0.28467777371406555
Batch 15/64 loss: -0.2895251512527466
Batch 16/64 loss: -0.2889270782470703
Batch 17/64 loss: -0.2760433256626129
Batch 18/64 loss: -0.2679021954536438
Batch 19/64 loss: -0.28097060322761536
Batch 20/64 loss: -0.2900698781013489
Batch 21/64 loss: -0.2880869209766388
Batch 22/64 loss: -0.28907933831214905
Batch 23/64 loss: -0.31245797872543335
Batch 24/64 loss: -0.28785762190818787
Batch 25/64 loss: -0.2889861464500427
Batch 26/64 loss: -0.29133716225624084
Batch 27/64 loss: -0.27533888816833496
Batch 28/64 loss: -0.30544692277908325
Batch 29/64 loss: -0.28165203332901
Batch 30/64 loss: -0.27986156940460205
Batch 31/64 loss: -0.2883918285369873
Batch 32/64 loss: -0.2658562660217285
Batch 33/64 loss: -0.3009936213493347
Batch 34/64 loss: -0.2932499945163727
Batch 35/64 loss: -0.28507059812545776
Batch 36/64 loss: -0.29403162002563477
Batch 37/64 loss: -0.30163872241973877
Batch 38/64 loss: -0.2968728542327881
Batch 39/64 loss: -0.29521864652633667
Batch 40/64 loss: -0.3054962754249573
Batch 41/64 loss: -0.2772983908653259
Batch 42/64 loss: -0.3017336130142212
Batch 43/64 loss: -0.2822433114051819
Batch 44/64 loss: -0.2869751751422882
Batch 45/64 loss: -0.29007065296173096
Batch 46/64 loss: -0.30163121223449707
Batch 47/64 loss: -0.28430891036987305
Batch 48/64 loss: -0.28066739439964294
Batch 49/64 loss: -0.2967314124107361
Batch 50/64 loss: -0.2982743978500366
Batch 51/64 loss: -0.2907165288925171
Batch 52/64 loss: -0.293099969625473
Batch 53/64 loss: -0.301951140165329
Batch 54/64 loss: -0.2977451682090759
Batch 55/64 loss: -0.2969290316104889
Batch 56/64 loss: -0.28693604469299316
Batch 57/64 loss: -0.2939460575580597
Batch 58/64 loss: -0.2840716242790222
Batch 59/64 loss: -0.2796438932418823
Batch 60/64 loss: -0.2949575185775757
Batch 61/64 loss: -0.30046409368515015
Batch 62/64 loss: -0.30165913701057434
Batch 63/64 loss: -0.265238881111145
Batch 64/64 loss: -0.3002805709838867
Epoch 393  Train loss: -0.2896714589175056  Val loss: -0.26192670917183264
Epoch 394
-------------------------------
Batch 1/64 loss: -0.2940340042114258
Batch 2/64 loss: -0.2838563621044159
Batch 3/64 loss: -0.28849637508392334
Batch 4/64 loss: -0.2878871560096741
Batch 5/64 loss: -0.3003513813018799
Batch 6/64 loss: -0.29712459444999695
Batch 7/64 loss: -0.279913991689682
Batch 8/64 loss: -0.28320831060409546
Batch 9/64 loss: -0.2975156307220459
Batch 10/64 loss: -0.2968584895133972
Batch 11/64 loss: -0.2890985906124115
Batch 12/64 loss: -0.2793501317501068
Batch 13/64 loss: -0.29690855741500854
Batch 14/64 loss: -0.2908332347869873
Batch 15/64 loss: -0.2853630483150482
Batch 16/64 loss: -0.2746543288230896
Batch 17/64 loss: -0.28493285179138184
Batch 18/64 loss: -0.28047195076942444
Batch 19/64 loss: -0.29021698236465454
Batch 20/64 loss: -0.26357316970825195
Batch 21/64 loss: -0.30189239978790283
Batch 22/64 loss: -0.2812514305114746
Batch 23/64 loss: -0.28690892457962036
Batch 24/64 loss: -0.28595083951950073
Batch 25/64 loss: -0.2894752621650696
Batch 26/64 loss: -0.279642254114151
Batch 27/64 loss: -0.2908242344856262
Batch 28/64 loss: -0.29300159215927124
Batch 29/64 loss: -0.2662111818790436
Batch 30/64 loss: -0.2933501601219177
Batch 31/64 loss: -0.2889725863933563
Batch 32/64 loss: -0.28288745880126953
Batch 33/64 loss: -0.29126811027526855
Batch 34/64 loss: -0.29651856422424316
Batch 35/64 loss: -0.2829012870788574
Batch 36/64 loss: -0.29542863368988037
Batch 37/64 loss: -0.27742576599121094
Batch 38/64 loss: -0.2830735743045807
Batch 39/64 loss: -0.28678417205810547
Batch 40/64 loss: -0.28500011563301086
Batch 41/64 loss: -0.27111712098121643
Batch 42/64 loss: -0.29628777503967285
Batch 43/64 loss: -0.27775830030441284
Batch 44/64 loss: -0.28644445538520813
Batch 45/64 loss: -0.2859092354774475
Batch 46/64 loss: -0.29800868034362793
Batch 47/64 loss: -0.2800968885421753
Batch 48/64 loss: -0.3023555278778076
Batch 49/64 loss: -0.28626930713653564
Batch 50/64 loss: -0.28375470638275146
Batch 51/64 loss: -0.2961927056312561
Batch 52/64 loss: -0.302697092294693
Batch 53/64 loss: -0.279782772064209
Batch 54/64 loss: -0.2782597839832306
Batch 55/64 loss: -0.27634197473526
Batch 56/64 loss: -0.29726845026016235
Batch 57/64 loss: -0.2786038815975189
Batch 58/64 loss: -0.2857709527015686
Batch 59/64 loss: -0.2788391411304474
Batch 60/64 loss: -0.28401896357536316
Batch 61/64 loss: -0.277543842792511
Batch 62/64 loss: -0.29166048765182495
Batch 63/64 loss: -0.3006282448768616
Batch 64/64 loss: -0.2914729714393616
Epoch 394  Train loss: -0.2870216895552242  Val loss: -0.26872892539525767
Epoch 395
-------------------------------
Batch 1/64 loss: -0.2951854467391968
Batch 2/64 loss: -0.2995416820049286
Batch 3/64 loss: -0.29414063692092896
Batch 4/64 loss: -0.2912670373916626
Batch 5/64 loss: -0.28679877519607544
Batch 6/64 loss: -0.28179606795310974
Batch 7/64 loss: -0.29344576597213745
Batch 8/64 loss: -0.2839910387992859
Batch 9/64 loss: -0.2834092974662781
Batch 10/64 loss: -0.3032231032848358
Batch 11/64 loss: -0.29380473494529724
Batch 12/64 loss: -0.29571375250816345
Batch 13/64 loss: -0.30027925968170166
Batch 14/64 loss: -0.2871282398700714
Batch 15/64 loss: -0.29484957456588745
Batch 16/64 loss: -0.2948220670223236
Batch 17/64 loss: -0.29495543241500854
Batch 18/64 loss: -0.28416478633880615
Batch 19/64 loss: -0.2871708869934082
Batch 20/64 loss: -0.31442809104919434
Batch 21/64 loss: -0.29370325803756714
Batch 22/64 loss: -0.28916895389556885
Batch 23/64 loss: -0.2938383221626282
Batch 24/64 loss: -0.293068528175354
Batch 25/64 loss: -0.29290205240249634
Batch 26/64 loss: -0.28832197189331055
Batch 27/64 loss: -0.30034902691841125
Batch 28/64 loss: -0.28228724002838135
Batch 29/64 loss: -0.2821350693702698
Batch 30/64 loss: -0.28880998492240906
Batch 31/64 loss: -0.2915278673171997
Batch 32/64 loss: -0.2888641655445099
Batch 33/64 loss: -0.2891215682029724
Batch 34/64 loss: -0.286380410194397
Batch 35/64 loss: -0.2996324896812439
Batch 36/64 loss: -0.29944881796836853
Batch 37/64 loss: -0.2712264060974121
Batch 38/64 loss: -0.28292545676231384
Batch 39/64 loss: -0.2988308072090149
Batch 40/64 loss: -0.29306477308273315
Batch 41/64 loss: -0.283255398273468
Batch 42/64 loss: -0.29200947284698486
Batch 43/64 loss: -0.28588584065437317
Batch 44/64 loss: -0.2812409996986389
Batch 45/64 loss: -0.2871038317680359
Batch 46/64 loss: -0.2773815393447876
Batch 47/64 loss: -0.2879970073699951
Batch 48/64 loss: -0.2934945821762085
Batch 49/64 loss: -0.27304399013519287
Batch 50/64 loss: -0.2928866147994995
Batch 51/64 loss: -0.2940562069416046
Batch 52/64 loss: -0.28694379329681396
Batch 53/64 loss: -0.2937943935394287
Batch 54/64 loss: -0.2869270443916321
Batch 55/64 loss: -0.288524866104126
Batch 56/64 loss: -0.28445446491241455
Batch 57/64 loss: -0.28514736890792847
Batch 58/64 loss: -0.2922380566596985
Batch 59/64 loss: -0.2897287905216217
Batch 60/64 loss: -0.28908535838127136
Batch 61/64 loss: -0.2864774465560913
Batch 62/64 loss: -0.29767248034477234
Batch 63/64 loss: -0.2813297510147095
Batch 64/64 loss: -0.2896183133125305
Epoch 395  Train loss: -0.29009593538209505  Val loss: -0.26604267084311783
Epoch 396
-------------------------------
Batch 1/64 loss: -0.29707151651382446
Batch 2/64 loss: -0.28772976994514465
Batch 3/64 loss: -0.2944713830947876
Batch 4/64 loss: -0.2901039719581604
Batch 5/64 loss: -0.2850164771080017
Batch 6/64 loss: -0.29584944248199463
Batch 7/64 loss: -0.28901100158691406
Batch 8/64 loss: -0.2936547100543976
Batch 9/64 loss: -0.2702282667160034
Batch 10/64 loss: -0.2793291509151459
Batch 11/64 loss: -0.28004467487335205
Batch 12/64 loss: -0.2830365002155304
Batch 13/64 loss: -0.28831714391708374
Batch 14/64 loss: -0.28445929288864136
Batch 15/64 loss: -0.27471059560775757
Batch 16/64 loss: -0.2813737392425537
Batch 17/64 loss: -0.2865425944328308
Batch 18/64 loss: -0.29127374291419983
Batch 19/64 loss: -0.2949523329734802
Batch 20/64 loss: -0.28883594274520874
Batch 21/64 loss: -0.30315741896629333
Batch 22/64 loss: -0.28124600648880005
Batch 23/64 loss: -0.3017929792404175
Batch 24/64 loss: -0.27714505791664124
Batch 25/64 loss: -0.29486551880836487
Batch 26/64 loss: -0.2955273985862732
Batch 27/64 loss: -0.29485243558883667
Batch 28/64 loss: -0.27591612935066223
Batch 29/64 loss: -0.27322912216186523
Batch 30/64 loss: -0.29844266176223755
Batch 31/64 loss: -0.2968803346157074
Batch 32/64 loss: -0.29066964983940125
Batch 33/64 loss: -0.28673017024993896
Batch 34/64 loss: -0.2852823734283447
Batch 35/64 loss: -0.2970935106277466
Batch 36/64 loss: -0.2799202799797058
Batch 37/64 loss: -0.290720671415329
Batch 38/64 loss: -0.27517223358154297
Batch 39/64 loss: -0.2847394645214081
Batch 40/64 loss: -0.2922102212905884
Batch 41/64 loss: -0.28960758447647095
Batch 42/64 loss: -0.29127201437950134
Batch 43/64 loss: -0.2889173924922943
Batch 44/64 loss: -0.2979377210140228
Batch 45/64 loss: -0.2821439206600189
Batch 46/64 loss: -0.2785196006298065
Batch 47/64 loss: -0.28483226895332336
Batch 48/64 loss: -0.2819864749908447
Batch 49/64 loss: -0.2966880202293396
Batch 50/64 loss: -0.2803336977958679
Batch 51/64 loss: -0.28896528482437134
Batch 52/64 loss: -0.28242623805999756
Batch 53/64 loss: -0.26293349266052246
Batch 54/64 loss: -0.2866896390914917
Batch 55/64 loss: -0.2856560945510864
Batch 56/64 loss: -0.26756417751312256
Batch 57/64 loss: -0.2851041853427887
Batch 58/64 loss: -0.2845189571380615
Batch 59/64 loss: -0.2915775775909424
Batch 60/64 loss: -0.2914333939552307
Batch 61/64 loss: -0.2832310199737549
Batch 62/64 loss: -0.2735573649406433
Batch 63/64 loss: -0.27712100744247437
Batch 64/64 loss: -0.28631919622421265
Epoch 396  Train loss: -0.28642137120751776  Val loss: -0.2555208933312459
Epoch 397
-------------------------------
Batch 1/64 loss: -0.2838051915168762
Batch 2/64 loss: -0.2825860381126404
Batch 3/64 loss: -0.28698503971099854
Batch 4/64 loss: -0.28844892978668213
Batch 5/64 loss: -0.2915811240673065
Batch 6/64 loss: -0.2905559539794922
Batch 7/64 loss: -0.2931387424468994
Batch 8/64 loss: -0.2795202136039734
Batch 9/64 loss: -0.27423739433288574
Batch 10/64 loss: -0.27057647705078125
Batch 11/64 loss: -0.2738942801952362
Batch 12/64 loss: -0.28169184923171997
Batch 13/64 loss: -0.28834521770477295
Batch 14/64 loss: -0.2802512049674988
Batch 15/64 loss: -0.2850053310394287
Batch 16/64 loss: -0.2874768376350403
Batch 17/64 loss: -0.2814488410949707
Batch 18/64 loss: -0.27600669860839844
Batch 19/64 loss: -0.28045618534088135
Batch 20/64 loss: -0.27330881357192993
Batch 21/64 loss: -0.28064194321632385
Batch 22/64 loss: -0.2832038104534149
Batch 23/64 loss: -0.27770769596099854
Batch 24/64 loss: -0.28059250116348267
Batch 25/64 loss: -0.28641122579574585
Batch 26/64 loss: -0.2830163836479187
Batch 27/64 loss: -0.2957519590854645
Batch 28/64 loss: -0.289893239736557
Batch 29/64 loss: -0.288769394159317
Batch 30/64 loss: -0.2836412191390991
Batch 31/64 loss: -0.292027086019516
Batch 32/64 loss: -0.29003340005874634
Batch 33/64 loss: -0.27126598358154297
Batch 34/64 loss: -0.2812478542327881
Batch 35/64 loss: -0.2850732207298279
Batch 36/64 loss: -0.29622846841812134
Batch 37/64 loss: -0.28804606199264526
Batch 38/64 loss: -0.29381805658340454
Batch 39/64 loss: -0.2859162390232086
Batch 40/64 loss: -0.2738085985183716
Batch 41/64 loss: -0.2591501474380493
Batch 42/64 loss: -0.29194724559783936
Batch 43/64 loss: -0.26233625411987305
Batch 44/64 loss: -0.28406602144241333
Batch 45/64 loss: -0.2786254584789276
Batch 46/64 loss: -0.2821961045265198
Batch 47/64 loss: -0.28069013357162476
Batch 48/64 loss: -0.27821382880210876
Batch 49/64 loss: -0.26160281896591187
Batch 50/64 loss: -0.28016674518585205
Batch 51/64 loss: -0.26361119747161865
Batch 52/64 loss: -0.2720339596271515
Batch 53/64 loss: -0.2692188024520874
Batch 54/64 loss: -0.2767689526081085
Batch 55/64 loss: -0.29301196336746216
Batch 56/64 loss: -0.2542773485183716
Batch 57/64 loss: -0.28001463413238525
Batch 58/64 loss: -0.2766682505607605
Batch 59/64 loss: -0.28310173749923706
Batch 60/64 loss: -0.26772111654281616
Batch 61/64 loss: -0.2634384036064148
Batch 62/64 loss: -0.27097463607788086
Batch 63/64 loss: -0.2762889266014099
Batch 64/64 loss: -0.28059929609298706
Epoch 397  Train loss: -0.28036060964359955  Val loss: -0.2462154321654146
Epoch 398
-------------------------------
Batch 1/64 loss: -0.27103015780448914
Batch 2/64 loss: -0.2796730399131775
Batch 3/64 loss: -0.291648805141449
Batch 4/64 loss: -0.28249385952949524
Batch 5/64 loss: -0.2867177128791809
Batch 6/64 loss: -0.26857322454452515
Batch 7/64 loss: -0.2754543423652649
Batch 8/64 loss: -0.2713766396045685
Batch 9/64 loss: -0.27474796772003174
Batch 10/64 loss: -0.27706098556518555
Batch 11/64 loss: -0.25892674922943115
Batch 12/64 loss: -0.27870622277259827
Batch 13/64 loss: -0.2742130756378174
Batch 14/64 loss: -0.2784227132797241
Batch 15/64 loss: -0.2530478239059448
Batch 16/64 loss: -0.2666904330253601
Batch 17/64 loss: -0.2701356112957001
Batch 18/64 loss: -0.2759767770767212
Batch 19/64 loss: -0.24996989965438843
Batch 20/64 loss: -0.286233127117157
Batch 21/64 loss: -0.2576274275779724
Batch 22/64 loss: -0.264495313167572
Batch 23/64 loss: -0.26892417669296265
Batch 24/64 loss: -0.288107693195343
Batch 25/64 loss: -0.2797987461090088
Batch 26/64 loss: -0.2727878987789154
Batch 27/64 loss: -0.2696126699447632
Batch 28/64 loss: -0.27109938859939575
Batch 29/64 loss: -0.268521249294281
Batch 30/64 loss: -0.2803115248680115
Batch 31/64 loss: -0.2691885232925415
Batch 32/64 loss: -0.24635183811187744
Batch 33/64 loss: -0.27580440044403076
Batch 34/64 loss: -0.28104469180107117
Batch 35/64 loss: -0.2657606601715088
Batch 36/64 loss: -0.2932307720184326
Batch 37/64 loss: -0.27695828676223755
Batch 38/64 loss: -0.27233678102493286
Batch 39/64 loss: -0.27019932866096497
Batch 40/64 loss: -0.27529552578926086
Batch 41/64 loss: -0.2606005072593689
Batch 42/64 loss: -0.26854246854782104
Batch 43/64 loss: -0.28282251954078674
Batch 44/64 loss: -0.27582335472106934
Batch 45/64 loss: -0.2813299894332886
Batch 46/64 loss: -0.28218579292297363
Batch 47/64 loss: -0.2763787508010864
Batch 48/64 loss: -0.28954899311065674
Batch 49/64 loss: -0.2932751774787903
Batch 50/64 loss: -0.2717432975769043
Batch 51/64 loss: -0.2892410159111023
Batch 52/64 loss: -0.28894802927970886
Batch 53/64 loss: -0.2842254638671875
Batch 54/64 loss: -0.2732360064983368
Batch 55/64 loss: -0.2751734256744385
Batch 56/64 loss: -0.2863697111606598
Batch 57/64 loss: -0.2961135804653168
Batch 58/64 loss: -0.272631973028183
Batch 59/64 loss: -0.2777518033981323
Batch 60/64 loss: -0.292805016040802
Batch 61/64 loss: -0.27851051092147827
Batch 62/64 loss: -0.2814542055130005
Batch 63/64 loss: -0.26654332876205444
Batch 64/64 loss: -0.2837069630622864
Epoch 398  Train loss: -0.27571123464434755  Val loss: -0.25794730911549835
Epoch 399
-------------------------------
Batch 1/64 loss: -0.2714555263519287
Batch 2/64 loss: -0.25328493118286133
Batch 3/64 loss: -0.27930235862731934
Batch 4/64 loss: -0.2752076983451843
Batch 5/64 loss: -0.28891271352767944
Batch 6/64 loss: -0.27923476696014404
Batch 7/64 loss: -0.28534746170043945
Batch 8/64 loss: -0.2851155400276184
Batch 9/64 loss: -0.2873101830482483
Batch 10/64 loss: -0.2885217070579529
Batch 11/64 loss: -0.27657392621040344
Batch 12/64 loss: -0.29326754808425903
Batch 13/64 loss: -0.27484679222106934
Batch 14/64 loss: -0.28130021691322327
Batch 15/64 loss: -0.2829345464706421
Batch 16/64 loss: -0.28683578968048096
Batch 17/64 loss: -0.28097397089004517
Batch 18/64 loss: -0.28376948833465576
Batch 19/64 loss: -0.2957591712474823
Batch 20/64 loss: -0.28847479820251465
Batch 21/64 loss: -0.2966252565383911
Batch 22/64 loss: -0.2861270010471344
Batch 23/64 loss: -0.2677839398384094
Batch 24/64 loss: -0.27097320556640625
Batch 25/64 loss: -0.27228498458862305
Batch 26/64 loss: -0.28827154636383057
Batch 27/64 loss: -0.27924948930740356
Batch 28/64 loss: -0.27928468585014343
Batch 29/64 loss: -0.28278306126594543
Batch 30/64 loss: -0.29171568155288696
Batch 31/64 loss: -0.2503235936164856
Batch 32/64 loss: -0.2728770673274994
Batch 33/64 loss: -0.288486123085022
Batch 34/64 loss: -0.28961652517318726
Batch 35/64 loss: -0.28761523962020874
Batch 36/64 loss: -0.2656558156013489
Batch 37/64 loss: -0.28606000542640686
Batch 38/64 loss: -0.2756766080856323
Batch 39/64 loss: -0.27993953227996826
Batch 40/64 loss: -0.2866928279399872
Batch 41/64 loss: -0.28132909536361694
Batch 42/64 loss: -0.2738162577152252
Batch 43/64 loss: -0.2974696159362793
Batch 44/64 loss: -0.3012460470199585
Batch 45/64 loss: -0.2892090380191803
Batch 46/64 loss: -0.27728337049484253
Batch 47/64 loss: -0.2902207374572754
Batch 48/64 loss: -0.28973400592803955
Batch 49/64 loss: -0.2916342616081238
Batch 50/64 loss: -0.2815231680870056
Batch 51/64 loss: -0.24379116296768188
Batch 52/64 loss: -0.2876489460468292
Batch 53/64 loss: -0.2949664890766144
Batch 54/64 loss: -0.273686945438385
Batch 55/64 loss: -0.26894688606262207
Batch 56/64 loss: -0.2899650037288666
Batch 57/64 loss: -0.29764324426651
Batch 58/64 loss: -0.2790011465549469
Batch 59/64 loss: -0.27630814909935
Batch 60/64 loss: -0.28014636039733887
Batch 61/64 loss: -0.2883937358856201
Batch 62/64 loss: -0.2890072762966156
Batch 63/64 loss: -0.28318485617637634
Batch 64/64 loss: -0.2500494718551636
Epoch 399  Train loss: -0.28157120358710197  Val loss: -0.2535610766345283
Epoch 400
-------------------------------
Batch 1/64 loss: -0.296738862991333
Batch 2/64 loss: -0.27413323521614075
Batch 3/64 loss: -0.2710893154144287
Batch 4/64 loss: -0.2709883749485016
Batch 5/64 loss: -0.29727107286453247
Batch 6/64 loss: -0.28795841336250305
Batch 7/64 loss: -0.2902897000312805
Batch 8/64 loss: -0.285760760307312
Batch 9/64 loss: -0.2911452651023865
Batch 10/64 loss: -0.2841908633708954
Batch 11/64 loss: -0.2913667857646942
Batch 12/64 loss: -0.28607380390167236
Batch 13/64 loss: -0.27639830112457275
Batch 14/64 loss: -0.2805634140968323
Batch 15/64 loss: -0.2880467176437378
Batch 16/64 loss: -0.29716765880584717
Batch 17/64 loss: -0.28622519969940186
Batch 18/64 loss: -0.2905767261981964
Batch 19/64 loss: -0.2901516854763031
Batch 20/64 loss: -0.2880709171295166
Batch 21/64 loss: -0.2775741517543793
Batch 22/64 loss: -0.27333304286003113
Batch 23/64 loss: -0.2775190472602844
Batch 24/64 loss: -0.2939443588256836
Batch 25/64 loss: -0.2857951521873474
Batch 26/64 loss: -0.27717334032058716
Batch 27/64 loss: -0.28798285126686096
Batch 28/64 loss: -0.2827607989311218
Batch 29/64 loss: -0.2899796962738037
Batch 30/64 loss: -0.2905425429344177
Batch 31/64 loss: -0.2968583405017853
Batch 32/64 loss: -0.2848256826400757
Batch 33/64 loss: -0.26291149854660034
Batch 34/64 loss: -0.3010847568511963
Batch 35/64 loss: -0.2938667833805084
Batch 36/64 loss: -0.28112712502479553
Batch 37/64 loss: -0.2889232039451599
Batch 38/64 loss: -0.2963598370552063
Batch 39/64 loss: -0.29601186513900757
Batch 40/64 loss: -0.28327617049217224
Batch 41/64 loss: -0.2932581901550293
Batch 42/64 loss: -0.29458096623420715
Batch 43/64 loss: -0.2828906774520874
Batch 44/64 loss: -0.2877632975578308
Batch 45/64 loss: -0.3040574789047241
Batch 46/64 loss: -0.2919653058052063
Batch 47/64 loss: -0.2762502431869507
Batch 48/64 loss: -0.29204660654067993
Batch 49/64 loss: -0.2854827642440796
Batch 50/64 loss: -0.29443302750587463
Batch 51/64 loss: -0.29199138283729553
Batch 52/64 loss: -0.2618563771247864
Batch 53/64 loss: -0.29102233052253723
Batch 54/64 loss: -0.29607707262039185
Batch 55/64 loss: -0.28751033544540405
Batch 56/64 loss: -0.28612345457077026
Batch 57/64 loss: -0.29170411825180054
Batch 58/64 loss: -0.28045156598091125
Batch 59/64 loss: -0.28278887271881104
Batch 60/64 loss: -0.2809104025363922
Batch 61/64 loss: -0.2933867275714874
Batch 62/64 loss: -0.2813674211502075
Batch 63/64 loss: -0.27086499333381653
Batch 64/64 loss: -0.2826205790042877
Epoch 400  Train loss: -0.28638127633169586  Val loss: -0.2681825318696982
Epoch 401
-------------------------------
Batch 1/64 loss: -0.29364413022994995
Batch 2/64 loss: -0.2707661986351013
Batch 3/64 loss: -0.29716920852661133
Batch 4/64 loss: -0.29229122400283813
Batch 5/64 loss: -0.3047599792480469
Batch 6/64 loss: -0.30674833059310913
Batch 7/64 loss: -0.2858718931674957
Batch 8/64 loss: -0.2794250547885895
Batch 9/64 loss: -0.2813403308391571
Batch 10/64 loss: -0.2871379852294922
Batch 11/64 loss: -0.29366534948349
Batch 12/64 loss: -0.2838894724845886
Batch 13/64 loss: -0.27495646476745605
Batch 14/64 loss: -0.28788018226623535
Batch 15/64 loss: -0.2883414030075073
Batch 16/64 loss: -0.2935906648635864
Batch 17/64 loss: -0.29174163937568665
Batch 18/64 loss: -0.29167503118515015
Batch 19/64 loss: -0.30742350220680237
Batch 20/64 loss: -0.2954551577568054
Batch 21/64 loss: -0.28487420082092285
Batch 22/64 loss: -0.28464996814727783
Batch 23/64 loss: -0.29398873448371887
Batch 24/64 loss: -0.2887375056743622
Batch 25/64 loss: -0.2845137119293213
Batch 26/64 loss: -0.28719770908355713
Batch 27/64 loss: -0.300639271736145
Batch 28/64 loss: -0.29463648796081543
Batch 29/64 loss: -0.2955568730831146
Batch 30/64 loss: -0.2910376787185669
Batch 31/64 loss: -0.2996799945831299
Batch 32/64 loss: -0.28909170627593994
Batch 33/64 loss: -0.2778981328010559
Batch 34/64 loss: -0.28894707560539246
Batch 35/64 loss: -0.29208827018737793
Batch 36/64 loss: -0.2954128086566925
Batch 37/64 loss: -0.29682430624961853
Batch 38/64 loss: -0.28577283024787903
Batch 39/64 loss: -0.2759179472923279
Batch 40/64 loss: -0.27994927763938904
Batch 41/64 loss: -0.2970871925354004
Batch 42/64 loss: -0.28282541036605835
Batch 43/64 loss: -0.2854436933994293
Batch 44/64 loss: -0.2975040078163147
Batch 45/64 loss: -0.29624029994010925
Batch 46/64 loss: -0.29669877886772156
Batch 47/64 loss: -0.29085588455200195
Batch 48/64 loss: -0.2888944149017334
Batch 49/64 loss: -0.2955853343009949
Batch 50/64 loss: -0.28812068700790405
Batch 51/64 loss: -0.2930082082748413
Batch 52/64 loss: -0.2869694232940674
Batch 53/64 loss: -0.2908881604671478
Batch 54/64 loss: -0.28261449933052063
Batch 55/64 loss: -0.2918655276298523
Batch 56/64 loss: -0.2912498414516449
Batch 57/64 loss: -0.2789791524410248
Batch 58/64 loss: -0.27661633491516113
Batch 59/64 loss: -0.28409647941589355
Batch 60/64 loss: -0.28616833686828613
Batch 61/64 loss: -0.28010621666908264
Batch 62/64 loss: -0.2903105914592743
Batch 63/64 loss: -0.295158326625824
Batch 64/64 loss: -0.29472318291664124
Epoch 401  Train loss: -0.28962379425179724  Val loss: -0.2605879964287748
Epoch 402
-------------------------------
Batch 1/64 loss: -0.2912048399448395
Batch 2/64 loss: -0.2892162501811981
Batch 3/64 loss: -0.28159669041633606
Batch 4/64 loss: -0.2896392345428467
Batch 5/64 loss: -0.2791060209274292
Batch 6/64 loss: -0.2821122407913208
Batch 7/64 loss: -0.2723226845264435
Batch 8/64 loss: -0.2803853154182434
Batch 9/64 loss: -0.283393532037735
Batch 10/64 loss: -0.2948346734046936
Batch 11/64 loss: -0.28796759247779846
Batch 12/64 loss: -0.2867169678211212
Batch 13/64 loss: -0.2710931897163391
Batch 14/64 loss: -0.2869434654712677
Batch 15/64 loss: -0.28951939940452576
Batch 16/64 loss: -0.281116783618927
Batch 17/64 loss: -0.29000332951545715
Batch 18/64 loss: -0.27977854013442993
Batch 19/64 loss: -0.27635830640792847
Batch 20/64 loss: -0.29192179441452026
Batch 21/64 loss: -0.28502780199050903
Batch 22/64 loss: -0.28444135189056396
Batch 23/64 loss: -0.272268146276474
Batch 24/64 loss: -0.2848086953163147
Batch 25/64 loss: -0.27461305260658264
Batch 26/64 loss: -0.28989577293395996
Batch 27/64 loss: -0.2754161059856415
Batch 28/64 loss: -0.28508150577545166
Batch 29/64 loss: -0.2855607867240906
Batch 30/64 loss: -0.29519620537757874
Batch 31/64 loss: -0.27845144271850586
Batch 32/64 loss: -0.29002225399017334
Batch 33/64 loss: -0.2805476188659668
Batch 34/64 loss: -0.2746477723121643
Batch 35/64 loss: -0.2837691009044647
Batch 36/64 loss: -0.2946588695049286
Batch 37/64 loss: -0.2923184037208557
Batch 38/64 loss: -0.28403037786483765
Batch 39/64 loss: -0.28457874059677124
Batch 40/64 loss: -0.2891230881214142
Batch 41/64 loss: -0.2910608649253845
Batch 42/64 loss: -0.29652559757232666
Batch 43/64 loss: -0.2802202105522156
Batch 44/64 loss: -0.28109613060951233
Batch 45/64 loss: -0.3001879155635834
Batch 46/64 loss: -0.2812258005142212
Batch 47/64 loss: -0.28533610701560974
Batch 48/64 loss: -0.2749815583229065
Batch 49/64 loss: -0.2840695083141327
Batch 50/64 loss: -0.29714447259902954
Batch 51/64 loss: -0.2798052132129669
Batch 52/64 loss: -0.2884591221809387
Batch 53/64 loss: -0.29262295365333557
Batch 54/64 loss: -0.2807905673980713
Batch 55/64 loss: -0.2969149351119995
Batch 56/64 loss: -0.2924802303314209
Batch 57/64 loss: -0.2863314747810364
Batch 58/64 loss: -0.2970477044582367
Batch 59/64 loss: -0.2945418953895569
Batch 60/64 loss: -0.2882099747657776
Batch 61/64 loss: -0.2806922197341919
Batch 62/64 loss: -0.27822673320770264
Batch 63/64 loss: -0.29294320940971375
Batch 64/64 loss: -0.2852690815925598
Epoch 402  Train loss: -0.2855616338112775  Val loss: -0.26325074503921564
Epoch 403
-------------------------------
Batch 1/64 loss: -0.28285935521125793
Batch 2/64 loss: -0.2734944224357605
Batch 3/64 loss: -0.2915459871292114
Batch 4/64 loss: -0.28917622566223145
Batch 5/64 loss: -0.2743092179298401
Batch 6/64 loss: -0.28355666995048523
Batch 7/64 loss: -0.29353970289230347
Batch 8/64 loss: -0.28741133213043213
Batch 9/64 loss: -0.2881779670715332
Batch 10/64 loss: -0.27979034185409546
Batch 11/64 loss: -0.28763696551322937
Batch 12/64 loss: -0.30282488465309143
Batch 13/64 loss: -0.29201364517211914
Batch 14/64 loss: -0.2898194193840027
Batch 15/64 loss: -0.26662755012512207
Batch 16/64 loss: -0.28493577241897583
Batch 17/64 loss: -0.29167985916137695
Batch 18/64 loss: -0.2869442105293274
Batch 19/64 loss: -0.28298354148864746
Batch 20/64 loss: -0.29223310947418213
Batch 21/64 loss: -0.28027474880218506
Batch 22/64 loss: -0.29148322343826294
Batch 23/64 loss: -0.27345556020736694
Batch 24/64 loss: -0.28810563683509827
Batch 25/64 loss: -0.30197247862815857
Batch 26/64 loss: -0.2913650870323181
Batch 27/64 loss: -0.28423842787742615
Batch 28/64 loss: -0.27662673592567444
Batch 29/64 loss: -0.29498955607414246
Batch 30/64 loss: -0.29484668374061584
Batch 31/64 loss: -0.29593905806541443
Batch 32/64 loss: -0.29314136505126953
Batch 33/64 loss: -0.28835535049438477
Batch 34/64 loss: -0.29751431941986084
Batch 35/64 loss: -0.28682073950767517
Batch 36/64 loss: -0.2893131375312805
Batch 37/64 loss: -0.2755500376224518
Batch 38/64 loss: -0.29566872119903564
Batch 39/64 loss: -0.2910451292991638
Batch 40/64 loss: -0.28586190938949585
Batch 41/64 loss: -0.27602413296699524
Batch 42/64 loss: -0.2787376940250397
Batch 43/64 loss: -0.29332271218299866
Batch 44/64 loss: -0.2839423418045044
Batch 45/64 loss: -0.2846699655056
Batch 46/64 loss: -0.275412380695343
Batch 47/64 loss: -0.28258687257766724
Batch 48/64 loss: -0.2972392737865448
Batch 49/64 loss: -0.29871708154678345
Batch 50/64 loss: -0.2847693860530853
Batch 51/64 loss: -0.2903040945529938
Batch 52/64 loss: -0.27822089195251465
Batch 53/64 loss: -0.28992170095443726
Batch 54/64 loss: -0.2847363352775574
Batch 55/64 loss: -0.28040191531181335
Batch 56/64 loss: -0.2906327247619629
Batch 57/64 loss: -0.28467193245887756
Batch 58/64 loss: -0.2945455312728882
Batch 59/64 loss: -0.2835577726364136
Batch 60/64 loss: -0.2778472900390625
Batch 61/64 loss: -0.28240370750427246
Batch 62/64 loss: -0.282429963350296
Batch 63/64 loss: -0.28286588191986084
Batch 64/64 loss: -0.2936425507068634
Epoch 403  Train loss: -0.2866873973724889  Val loss: -0.26257736781208785
Epoch 404
-------------------------------
Batch 1/64 loss: -0.295515775680542
Batch 2/64 loss: -0.280143678188324
Batch 3/64 loss: -0.2889610826969147
Batch 4/64 loss: -0.2930483818054199
Batch 5/64 loss: -0.2919989824295044
Batch 6/64 loss: -0.27777451276779175
Batch 7/64 loss: -0.3004504144191742
Batch 8/64 loss: -0.29285892844200134
Batch 9/64 loss: -0.28168827295303345
Batch 10/64 loss: -0.2900715172290802
Batch 11/64 loss: -0.29267072677612305
Batch 12/64 loss: -0.2780187726020813
Batch 13/64 loss: -0.2901318669319153
Batch 14/64 loss: -0.29350483417510986
Batch 15/64 loss: -0.29478415846824646
Batch 16/64 loss: -0.29488277435302734
Batch 17/64 loss: -0.3049134910106659
Batch 18/64 loss: -0.29265815019607544
Batch 19/64 loss: -0.2919144630432129
Batch 20/64 loss: -0.27711591124534607
Batch 21/64 loss: -0.292341023683548
Batch 22/64 loss: -0.28743666410446167
Batch 23/64 loss: -0.3000839054584503
Batch 24/64 loss: -0.29311105608940125
Batch 25/64 loss: -0.29679247736930847
Batch 26/64 loss: -0.28780514001846313
Batch 27/64 loss: -0.29227414727211
Batch 28/64 loss: -0.2904720902442932
Batch 29/64 loss: -0.2940366864204407
Batch 30/64 loss: -0.2994168996810913
Batch 31/64 loss: -0.2907775640487671
Batch 32/64 loss: -0.3094947934150696
Batch 33/64 loss: -0.29084962606430054
Batch 34/64 loss: -0.2987208962440491
Batch 35/64 loss: -0.29901590943336487
Batch 36/64 loss: -0.27934205532073975
Batch 37/64 loss: -0.2734035849571228
Batch 38/64 loss: -0.30452412366867065
Batch 39/64 loss: -0.2914048433303833
Batch 40/64 loss: -0.2926980257034302
Batch 41/64 loss: -0.2920100688934326
Batch 42/64 loss: -0.2852563261985779
Batch 43/64 loss: -0.2941977381706238
Batch 44/64 loss: -0.2998189330101013
Batch 45/64 loss: -0.30389493703842163
Batch 46/64 loss: -0.28037142753601074
Batch 47/64 loss: -0.2856154441833496
Batch 48/64 loss: -0.29094552993774414
Batch 49/64 loss: -0.2805940508842468
Batch 50/64 loss: -0.2984080910682678
Batch 51/64 loss: -0.2892512083053589
Batch 52/64 loss: -0.2948795258998871
Batch 53/64 loss: -0.296796590089798
Batch 54/64 loss: -0.28874844312667847
Batch 55/64 loss: -0.2999938130378723
Batch 56/64 loss: -0.2804141640663147
Batch 57/64 loss: -0.2937721610069275
Batch 58/64 loss: -0.2828125059604645
Batch 59/64 loss: -0.28079432249069214
Batch 60/64 loss: -0.2990363836288452
Batch 61/64 loss: -0.30187132954597473
Batch 62/64 loss: -0.27831947803497314
Batch 63/64 loss: -0.2613939046859741
Batch 64/64 loss: -0.2874607443809509
Epoch 404  Train loss: -0.2908533355768989  Val loss: -0.26694591409971624
Epoch 405
-------------------------------
Batch 1/64 loss: -0.28405582904815674
Batch 2/64 loss: -0.28937771916389465
Batch 3/64 loss: -0.3029162585735321
Batch 4/64 loss: -0.26751911640167236
Batch 5/64 loss: -0.29042649269104004
Batch 6/64 loss: -0.29545342922210693
Batch 7/64 loss: -0.2994929552078247
Batch 8/64 loss: -0.28455132246017456
Batch 9/64 loss: -0.28595227003097534
Batch 10/64 loss: -0.2933301031589508
Batch 11/64 loss: -0.28668999671936035
Batch 12/64 loss: -0.2916702628135681
Batch 13/64 loss: -0.2996158301830292
Batch 14/64 loss: -0.29688727855682373
Batch 15/64 loss: -0.28265124559402466
Batch 16/64 loss: -0.29716384410858154
Batch 17/64 loss: -0.29148969054222107
Batch 18/64 loss: -0.2952079772949219
Batch 19/64 loss: -0.28653329610824585
Batch 20/64 loss: -0.2929249703884125
Batch 21/64 loss: -0.29111284017562866
Batch 22/64 loss: -0.2703191339969635
Batch 23/64 loss: -0.28045931458473206
Batch 24/64 loss: -0.29750409722328186
Batch 25/64 loss: -0.27206873893737793
Batch 26/64 loss: -0.27499085664749146
Batch 27/64 loss: -0.2785469889640808
Batch 28/64 loss: -0.2915570139884949
Batch 29/64 loss: -0.2841157019138336
Batch 30/64 loss: -0.2974640130996704
Batch 31/64 loss: -0.30084729194641113
Batch 32/64 loss: -0.2845926582813263
Batch 33/64 loss: -0.2833718955516815
Batch 34/64 loss: -0.3010517954826355
Batch 35/64 loss: -0.28975412249565125
Batch 36/64 loss: -0.2858729958534241
Batch 37/64 loss: -0.2974984645843506
Batch 38/64 loss: -0.2905883193016052
Batch 39/64 loss: -0.30459243059158325
Batch 40/64 loss: -0.29834526777267456
Batch 41/64 loss: -0.2948438227176666
Batch 42/64 loss: -0.2953858971595764
Batch 43/64 loss: -0.27781209349632263
Batch 44/64 loss: -0.2860139012336731
Batch 45/64 loss: -0.2901892364025116
Batch 46/64 loss: -0.3003009259700775
Batch 47/64 loss: -0.2952231764793396
Batch 48/64 loss: -0.29605674743652344
Batch 49/64 loss: -0.2733457386493683
Batch 50/64 loss: -0.28680166602134705
Batch 51/64 loss: -0.2821338474750519
Batch 52/64 loss: -0.28858333826065063
Batch 53/64 loss: -0.28342530131340027
Batch 54/64 loss: -0.2834886610507965
Batch 55/64 loss: -0.2784182131290436
Batch 56/64 loss: -0.27618324756622314
Batch 57/64 loss: -0.2834065556526184
Batch 58/64 loss: -0.2880813777446747
Batch 59/64 loss: -0.2940952777862549
Batch 60/64 loss: -0.28969836235046387
Batch 61/64 loss: -0.29001089930534363
Batch 62/64 loss: -0.2899867594242096
Batch 63/64 loss: -0.2801845073699951
Batch 64/64 loss: -0.2948344945907593
Epoch 405  Train loss: -0.28883700791527245  Val loss: -0.2589297212685916
Epoch 406
-------------------------------
Batch 1/64 loss: -0.29652154445648193
Batch 2/64 loss: -0.3073187470436096
Batch 3/64 loss: -0.28251174092292786
Batch 4/64 loss: -0.28635716438293457
Batch 5/64 loss: -0.2965177297592163
Batch 6/64 loss: -0.2837609052658081
Batch 7/64 loss: -0.303153395652771
Batch 8/64 loss: -0.2910902500152588
Batch 9/64 loss: -0.28981876373291016
Batch 10/64 loss: -0.293387770652771
Batch 11/64 loss: -0.2953670620918274
Batch 12/64 loss: -0.277620404958725
Batch 13/64 loss: -0.29315975308418274
Batch 14/64 loss: -0.3020239472389221
Batch 15/64 loss: -0.28413334488868713
Batch 16/64 loss: -0.2794466018676758
Batch 17/64 loss: -0.27257198095321655
Batch 18/64 loss: -0.28789395093917847
Batch 19/64 loss: -0.2807686924934387
Batch 20/64 loss: -0.2804032862186432
Batch 21/64 loss: -0.29165220260620117
Batch 22/64 loss: -0.3026895523071289
Batch 23/64 loss: -0.2854839265346527
Batch 24/64 loss: -0.28466999530792236
Batch 25/64 loss: -0.3009859323501587
Batch 26/64 loss: -0.3021811842918396
Batch 27/64 loss: -0.294888973236084
Batch 28/64 loss: -0.3050350546836853
Batch 29/64 loss: -0.30430981516838074
Batch 30/64 loss: -0.29981306195259094
Batch 31/64 loss: -0.3018133044242859
Batch 32/64 loss: -0.284973680973053
Batch 33/64 loss: -0.28666800260543823
Batch 34/64 loss: -0.27981287240982056
Batch 35/64 loss: -0.29279783368110657
Batch 36/64 loss: -0.2980247735977173
Batch 37/64 loss: -0.2810371518135071
Batch 38/64 loss: -0.2864094078540802
Batch 39/64 loss: -0.26948434114456177
Batch 40/64 loss: -0.28533580899238586
Batch 41/64 loss: -0.281103253364563
Batch 42/64 loss: -0.2858220338821411
Batch 43/64 loss: -0.29103678464889526
Batch 44/64 loss: -0.28844642639160156
Batch 45/64 loss: -0.29254910349845886
Batch 46/64 loss: -0.294455349445343
Batch 47/64 loss: -0.2805434763431549
Batch 48/64 loss: -0.2863919734954834
Batch 49/64 loss: -0.2845110297203064
Batch 50/64 loss: -0.31234055757522583
Batch 51/64 loss: -0.29098454117774963
Batch 52/64 loss: -0.2839485704898834
Batch 53/64 loss: -0.28929734230041504
Batch 54/64 loss: -0.29774728417396545
Batch 55/64 loss: -0.28487399220466614
Batch 56/64 loss: -0.2934526205062866
Batch 57/64 loss: -0.2881339490413666
Batch 58/64 loss: -0.29265475273132324
Batch 59/64 loss: -0.2973446249961853
Batch 60/64 loss: -0.27406376600265503
Batch 61/64 loss: -0.2907772362232208
Batch 62/64 loss: -0.28770312666893005
Batch 63/64 loss: -0.29747653007507324
Batch 64/64 loss: -0.29084187746047974
Epoch 406  Train loss: -0.29028523365656533  Val loss: -0.2669880984165415
Epoch 407
-------------------------------
Batch 1/64 loss: -0.2953156530857086
Batch 2/64 loss: -0.2982000410556793
Batch 3/64 loss: -0.27428138256073
Batch 4/64 loss: -0.2910318374633789
Batch 5/64 loss: -0.2845889925956726
Batch 6/64 loss: -0.2898266911506653
Batch 7/64 loss: -0.27743303775787354
Batch 8/64 loss: -0.28638145327568054
Batch 9/64 loss: -0.2994195818901062
Batch 10/64 loss: -0.29963088035583496
Batch 11/64 loss: -0.29733389616012573
Batch 12/64 loss: -0.2753128111362457
Batch 13/64 loss: -0.2931814193725586
Batch 14/64 loss: -0.29914021492004395
Batch 15/64 loss: -0.2902458608150482
Batch 16/64 loss: -0.2950146794319153
Batch 17/64 loss: -0.2924497127532959
Batch 18/64 loss: -0.2812209725379944
Batch 19/64 loss: -0.29643160104751587
Batch 20/64 loss: -0.27639883756637573
Batch 21/64 loss: -0.28840264678001404
Batch 22/64 loss: -0.2927391529083252
Batch 23/64 loss: -0.28099456429481506
Batch 24/64 loss: -0.2944101095199585
Batch 25/64 loss: -0.2940695583820343
Batch 26/64 loss: -0.2829870581626892
Batch 27/64 loss: -0.27345162630081177
Batch 28/64 loss: -0.2898848354816437
Batch 29/64 loss: -0.29592621326446533
Batch 30/64 loss: -0.2715567350387573
Batch 31/64 loss: -0.29079219698905945
Batch 32/64 loss: -0.28037697076797485
Batch 33/64 loss: -0.2789548635482788
Batch 34/64 loss: -0.2902889549732208
Batch 35/64 loss: -0.30135470628738403
Batch 36/64 loss: -0.2917982339859009
Batch 37/64 loss: -0.2863582670688629
Batch 38/64 loss: -0.2828752100467682
Batch 39/64 loss: -0.28657007217407227
Batch 40/64 loss: -0.2700396180152893
Batch 41/64 loss: -0.29431575536727905
Batch 42/64 loss: -0.2833266854286194
Batch 43/64 loss: -0.2789069414138794
Batch 44/64 loss: -0.28417718410491943
Batch 45/64 loss: -0.2741616368293762
Batch 46/64 loss: -0.29436278343200684
Batch 47/64 loss: -0.2807154357433319
Batch 48/64 loss: -0.2888902723789215
Batch 49/64 loss: -0.2917894721031189
Batch 50/64 loss: -0.2847789227962494
Batch 51/64 loss: -0.28715503215789795
Batch 52/64 loss: -0.2737582325935364
Batch 53/64 loss: -0.28898611664772034
Batch 54/64 loss: -0.284679651260376
Batch 55/64 loss: -0.2853226661682129
Batch 56/64 loss: -0.3005155920982361
Batch 57/64 loss: -0.2854330539703369
Batch 58/64 loss: -0.2866578996181488
Batch 59/64 loss: -0.290846049785614
Batch 60/64 loss: -0.2914254367351532
Batch 61/64 loss: -0.2794654965400696
Batch 62/64 loss: -0.2695891857147217
Batch 63/64 loss: -0.2818310558795929
Batch 64/64 loss: -0.27369359135627747
Epoch 407  Train loss: -0.2867926573052126  Val loss: -0.2592204827623269
Epoch 408
-------------------------------
Batch 1/64 loss: -0.2769832909107208
Batch 2/64 loss: -0.2822280824184418
Batch 3/64 loss: -0.2871914505958557
Batch 4/64 loss: -0.29004794359207153
Batch 5/64 loss: -0.29052865505218506
Batch 6/64 loss: -0.28593695163726807
Batch 7/64 loss: -0.3008228540420532
Batch 8/64 loss: -0.2919556498527527
Batch 9/64 loss: -0.2831081748008728
Batch 10/64 loss: -0.291570782661438
Batch 11/64 loss: -0.29661938548088074
Batch 12/64 loss: -0.2952682375907898
Batch 13/64 loss: -0.29722487926483154
Batch 14/64 loss: -0.2813737988471985
Batch 15/64 loss: -0.2805732786655426
Batch 16/64 loss: -0.2932955026626587
Batch 17/64 loss: -0.2867816090583801
Batch 18/64 loss: -0.2953565716743469
Batch 19/64 loss: -0.28788912296295166
Batch 20/64 loss: -0.280937522649765
Batch 21/64 loss: -0.28676265478134155
Batch 22/64 loss: -0.29621070623397827
Batch 23/64 loss: -0.29198765754699707
Batch 24/64 loss: -0.2851466238498688
Batch 25/64 loss: -0.30059921741485596
Batch 26/64 loss: -0.2873336672782898
Batch 27/64 loss: -0.29759806394577026
Batch 28/64 loss: -0.28780749440193176
Batch 29/64 loss: -0.29114753007888794
Batch 30/64 loss: -0.28639596700668335
Batch 31/64 loss: -0.28505241870880127
Batch 32/64 loss: -0.2845851182937622
Batch 33/64 loss: -0.2784629762172699
Batch 34/64 loss: -0.2689655125141144
Batch 35/64 loss: -0.2999536693096161
Batch 36/64 loss: -0.28325968980789185
Batch 37/64 loss: -0.292117178440094
Batch 38/64 loss: -0.2856377363204956
Batch 39/64 loss: -0.28046905994415283
Batch 40/64 loss: -0.28181910514831543
Batch 41/64 loss: -0.28822416067123413
Batch 42/64 loss: -0.29556575417518616
Batch 43/64 loss: -0.2941746711730957
Batch 44/64 loss: -0.28291645646095276
Batch 45/64 loss: -0.27324292063713074
Batch 46/64 loss: -0.2926905155181885
Batch 47/64 loss: -0.30011874437332153
Batch 48/64 loss: -0.29169726371765137
Batch 49/64 loss: -0.28963160514831543
Batch 50/64 loss: -0.27818578481674194
Batch 51/64 loss: -0.27923113107681274
Batch 52/64 loss: -0.28426551818847656
Batch 53/64 loss: -0.27986279129981995
Batch 54/64 loss: -0.29103517532348633
Batch 55/64 loss: -0.29273760318756104
Batch 56/64 loss: -0.2927703559398651
Batch 57/64 loss: -0.2837895154953003
Batch 58/64 loss: -0.2764759361743927
Batch 59/64 loss: -0.2911834716796875
Batch 60/64 loss: -0.2851829528808594
Batch 61/64 loss: -0.2827540636062622
Batch 62/64 loss: -0.29189133644104004
Batch 63/64 loss: -0.2864340543746948
Batch 64/64 loss: -0.29493990540504456
Epoch 408  Train loss: -0.2878787843620076  Val loss: -0.27163656962286564
Epoch 409
-------------------------------
Batch 1/64 loss: -0.27569395303726196
Batch 2/64 loss: -0.3100513219833374
Batch 3/64 loss: -0.2895370125770569
Batch 4/64 loss: -0.30519258975982666
Batch 5/64 loss: -0.26415741443634033
Batch 6/64 loss: -0.293464332818985
Batch 7/64 loss: -0.28895124793052673
Batch 8/64 loss: -0.3001765012741089
Batch 9/64 loss: -0.28542011976242065
Batch 10/64 loss: -0.2732979655265808
Batch 11/64 loss: -0.28780826926231384
Batch 12/64 loss: -0.2960955500602722
Batch 13/64 loss: -0.2847749888896942
Batch 14/64 loss: -0.28867292404174805
Batch 15/64 loss: -0.2959095537662506
Batch 16/64 loss: -0.28091561794281006
Batch 17/64 loss: -0.2870335578918457
Batch 18/64 loss: -0.2894238233566284
Batch 19/64 loss: -0.27903878688812256
Batch 20/64 loss: -0.27731090784072876
Batch 21/64 loss: -0.3039659261703491
Batch 22/64 loss: -0.28916940093040466
Batch 23/64 loss: -0.28889572620391846
Batch 24/64 loss: -0.29025447368621826
Batch 25/64 loss: -0.28998643159866333
Batch 26/64 loss: -0.27849137783050537
Batch 27/64 loss: -0.27134519815444946
Batch 28/64 loss: -0.2846865653991699
Batch 29/64 loss: -0.29996511340141296
Batch 30/64 loss: -0.29509422183036804
Batch 31/64 loss: -0.27656638622283936
Batch 32/64 loss: -0.28807222843170166
Batch 33/64 loss: -0.2811293601989746
Batch 34/64 loss: -0.2896605134010315
Batch 35/64 loss: -0.2912601828575134
Batch 36/64 loss: -0.2785249352455139
Batch 37/64 loss: -0.2818273901939392
Batch 38/64 loss: -0.2774261236190796
Batch 39/64 loss: -0.2724539041519165
Batch 40/64 loss: -0.2601640224456787
Batch 41/64 loss: -0.2731683850288391
Batch 42/64 loss: -0.2896391749382019
Batch 43/64 loss: -0.28144800662994385
Batch 44/64 loss: -0.2942376732826233
Batch 45/64 loss: -0.2859222888946533
Batch 46/64 loss: -0.2847059965133667
Batch 47/64 loss: -0.2888922691345215
Batch 48/64 loss: -0.29018634557724
Batch 49/64 loss: -0.28714263439178467
Batch 50/64 loss: -0.2874224781990051
Batch 51/64 loss: -0.27778375148773193
Batch 52/64 loss: -0.2834155559539795
Batch 53/64 loss: -0.2868313789367676
Batch 54/64 loss: -0.2705426812171936
Batch 55/64 loss: -0.27974867820739746
Batch 56/64 loss: -0.2657225728034973
Batch 57/64 loss: -0.2865738570690155
Batch 58/64 loss: -0.28878700733184814
Batch 59/64 loss: -0.27842360734939575
Batch 60/64 loss: -0.29314684867858887
Batch 61/64 loss: -0.2796449661254883
Batch 62/64 loss: -0.2942419648170471
Batch 63/64 loss: -0.2732044458389282
Batch 64/64 loss: -0.30092644691467285
Epoch 409  Train loss: -0.2853081305821737  Val loss: -0.24269799944461415
Epoch 410
-------------------------------
Batch 1/64 loss: -0.2795759439468384
Batch 2/64 loss: -0.2830312252044678
Batch 3/64 loss: -0.29227250814437866
Batch 4/64 loss: -0.29019466042518616
Batch 5/64 loss: -0.2929351031780243
Batch 6/64 loss: -0.27458491921424866
Batch 7/64 loss: -0.2975347340106964
Batch 8/64 loss: -0.2804785966873169
Batch 9/64 loss: -0.29369044303894043
Batch 10/64 loss: -0.27823108434677124
Batch 11/64 loss: -0.2913072109222412
Batch 12/64 loss: -0.29144686460494995
Batch 13/64 loss: -0.28059595823287964
Batch 14/64 loss: -0.27423807978630066
Batch 15/64 loss: -0.2828046679496765
Batch 16/64 loss: -0.29777830839157104
Batch 17/64 loss: -0.2914651036262512
Batch 18/64 loss: -0.2884693741798401
Batch 19/64 loss: -0.2992417812347412
Batch 20/64 loss: -0.2794632315635681
Batch 21/64 loss: -0.29633811116218567
Batch 22/64 loss: -0.29724007844924927
Batch 23/64 loss: -0.2911757826805115
Batch 24/64 loss: -0.2783709168434143
Batch 25/64 loss: -0.28972816467285156
Batch 26/64 loss: -0.294317364692688
Batch 27/64 loss: -0.2847031056880951
Batch 28/64 loss: -0.27989476919174194
Batch 29/64 loss: -0.28959885239601135
Batch 30/64 loss: -0.29544949531555176
Batch 31/64 loss: -0.2808281183242798
Batch 32/64 loss: -0.30196696519851685
Batch 33/64 loss: -0.29535722732543945
Batch 34/64 loss: -0.2855237126350403
Batch 35/64 loss: -0.2968575358390808
Batch 36/64 loss: -0.29807427525520325
Batch 37/64 loss: -0.2760051488876343
Batch 38/64 loss: -0.2884823977947235
Batch 39/64 loss: -0.2828843295574188
Batch 40/64 loss: -0.2958701550960541
Batch 41/64 loss: -0.2897915840148926
Batch 42/64 loss: -0.28929296135902405
Batch 43/64 loss: -0.29522213339805603
Batch 44/64 loss: -0.28398579359054565
Batch 45/64 loss: -0.2724194824695587
Batch 46/64 loss: -0.3038560450077057
Batch 47/64 loss: -0.2931175231933594
Batch 48/64 loss: -0.2891466021537781
Batch 49/64 loss: -0.2733002305030823
Batch 50/64 loss: -0.302534282207489
Batch 51/64 loss: -0.29548490047454834
Batch 52/64 loss: -0.2824741005897522
Batch 53/64 loss: -0.28820013999938965
Batch 54/64 loss: -0.2948680520057678
Batch 55/64 loss: -0.27519360184669495
Batch 56/64 loss: -0.2918776273727417
Batch 57/64 loss: -0.30203837156295776
Batch 58/64 loss: -0.2851814031600952
Batch 59/64 loss: -0.27783268690109253
Batch 60/64 loss: -0.2893579602241516
Batch 61/64 loss: -0.2905575931072235
Batch 62/64 loss: -0.2943271994590759
Batch 63/64 loss: -0.3006840944290161
Batch 64/64 loss: -0.29251956939697266
Epoch 410  Train loss: -0.288912005517997  Val loss: -0.27199144973787653
Epoch 411
-------------------------------
Batch 1/64 loss: -0.29913121461868286
Batch 2/64 loss: -0.2936576306819916
Batch 3/64 loss: -0.30028724670410156
Batch 4/64 loss: -0.2969348430633545
Batch 5/64 loss: -0.289986789226532
Batch 6/64 loss: -0.2906012237071991
Batch 7/64 loss: -0.2773189842700958
Batch 8/64 loss: -0.2972128093242645
Batch 9/64 loss: -0.2826843559741974
Batch 10/64 loss: -0.30321216583251953
Batch 11/64 loss: -0.284307599067688
Batch 12/64 loss: -0.31086957454681396
Batch 13/64 loss: -0.28586626052856445
Batch 14/64 loss: -0.2840230464935303
Batch 15/64 loss: -0.29061341285705566
Batch 16/64 loss: -0.29144564270973206
Batch 17/64 loss: -0.287509948015213
Batch 18/64 loss: -0.2923606336116791
Batch 19/64 loss: -0.2863927483558655
Batch 20/64 loss: -0.28622519969940186
Batch 21/64 loss: -0.30080851912498474
Batch 22/64 loss: -0.2951785922050476
Batch 23/64 loss: -0.29692429304122925
Batch 24/64 loss: -0.2951149344444275
Batch 25/64 loss: -0.2935071289539337
Batch 26/64 loss: -0.2865668535232544
Batch 27/64 loss: -0.2943536639213562
Batch 28/64 loss: -0.290357768535614
Batch 29/64 loss: -0.29018333554267883
Batch 30/64 loss: -0.28811073303222656
Batch 31/64 loss: -0.2887422442436218
Batch 32/64 loss: -0.29844820499420166
Batch 33/64 loss: -0.2873334586620331
Batch 34/64 loss: -0.28831207752227783
Batch 35/64 loss: -0.2949584722518921
Batch 36/64 loss: -0.2871205806732178
Batch 37/64 loss: -0.28359201550483704
Batch 38/64 loss: -0.2954781949520111
Batch 39/64 loss: -0.2925039529800415
Batch 40/64 loss: -0.2805330753326416
Batch 41/64 loss: -0.29900938272476196
Batch 42/64 loss: -0.2992442846298218
Batch 43/64 loss: -0.295846164226532
Batch 44/64 loss: -0.2893323302268982
Batch 45/64 loss: -0.3015415668487549
Batch 46/64 loss: -0.2840333878993988
Batch 47/64 loss: -0.29170846939086914
Batch 48/64 loss: -0.27789831161499023
Batch 49/64 loss: -0.28071290254592896
Batch 50/64 loss: -0.30134981870651245
Batch 51/64 loss: -0.28951597213745117
Batch 52/64 loss: -0.2933783531188965
Batch 53/64 loss: -0.2899247109889984
Batch 54/64 loss: -0.2873121201992035
Batch 55/64 loss: -0.2789209187030792
Batch 56/64 loss: -0.2797238230705261
Batch 57/64 loss: -0.2942394018173218
Batch 58/64 loss: -0.29652807116508484
Batch 59/64 loss: -0.28105872869491577
Batch 60/64 loss: -0.2922672927379608
Batch 61/64 loss: -0.2841883897781372
Batch 62/64 loss: -0.28185999393463135
Batch 63/64 loss: -0.2880462110042572
Batch 64/64 loss: -0.2874675393104553
Epoch 411  Train loss: -0.29069820689220055  Val loss: -0.2587890815489071
Epoch 412
-------------------------------
Batch 1/64 loss: -0.2987595796585083
Batch 2/64 loss: -0.29370370507240295
Batch 3/64 loss: -0.29645296931266785
Batch 4/64 loss: -0.2889564633369446
Batch 5/64 loss: -0.28593581914901733
Batch 6/64 loss: -0.2984210252761841
Batch 7/64 loss: -0.2802554965019226
Batch 8/64 loss: -0.27155137062072754
Batch 9/64 loss: -0.2978629171848297
Batch 10/64 loss: -0.2884157598018646
Batch 11/64 loss: -0.2947709858417511
Batch 12/64 loss: -0.2885274887084961
Batch 13/64 loss: -0.2898988127708435
Batch 14/64 loss: -0.2819035053253174
Batch 15/64 loss: -0.2700437903404236
Batch 16/64 loss: -0.29073745012283325
Batch 17/64 loss: -0.29671594500541687
Batch 18/64 loss: -0.2953968048095703
Batch 19/64 loss: -0.3034825325012207
Batch 20/64 loss: -0.2813931405544281
Batch 21/64 loss: -0.28477007150650024
Batch 22/64 loss: -0.2981170415878296
Batch 23/64 loss: -0.2900397777557373
Batch 24/64 loss: -0.29491478204727173
Batch 25/64 loss: -0.2946799099445343
Batch 26/64 loss: -0.2894635498523712
Batch 27/64 loss: -0.2993605136871338
Batch 28/64 loss: -0.28934136033058167
Batch 29/64 loss: -0.3017386198043823
Batch 30/64 loss: -0.3089243769645691
Batch 31/64 loss: -0.295808881521225
Batch 32/64 loss: -0.29698342084884644
Batch 33/64 loss: -0.3010479211807251
Batch 34/64 loss: -0.2884613275527954
Batch 35/64 loss: -0.30046841502189636
Batch 36/64 loss: -0.2836795151233673
Batch 37/64 loss: -0.29567039012908936
Batch 38/64 loss: -0.2974720001220703
Batch 39/64 loss: -0.2990467846393585
Batch 40/64 loss: -0.2860456705093384
Batch 41/64 loss: -0.3061676025390625
Batch 42/64 loss: -0.2928733229637146
Batch 43/64 loss: -0.29241943359375
Batch 44/64 loss: -0.28106534481048584
Batch 45/64 loss: -0.29288482666015625
Batch 46/64 loss: -0.2887656092643738
Batch 47/64 loss: -0.2774399220943451
Batch 48/64 loss: -0.2804272770881653
Batch 49/64 loss: -0.266260027885437
Batch 50/64 loss: -0.2759656012058258
Batch 51/64 loss: -0.2795815169811249
Batch 52/64 loss: -0.2929433584213257
Batch 53/64 loss: -0.2949724793434143
Batch 54/64 loss: -0.286213755607605
Batch 55/64 loss: -0.29232293367385864
Batch 56/64 loss: -0.2953777313232422
Batch 57/64 loss: -0.2872047424316406
Batch 58/64 loss: -0.291389524936676
Batch 59/64 loss: -0.2828209400177002
Batch 60/64 loss: -0.27179551124572754
Batch 61/64 loss: -0.2883848249912262
Batch 62/64 loss: -0.2959589660167694
Batch 63/64 loss: -0.2934708595275879
Batch 64/64 loss: -0.27823641896247864
Epoch 412  Train loss: -0.2902682873548246  Val loss: -0.254185038129079
Epoch 413
-------------------------------
Batch 1/64 loss: -0.27908992767333984
Batch 2/64 loss: -0.27999818325042725
Batch 3/64 loss: -0.28212404251098633
Batch 4/64 loss: -0.28891801834106445
Batch 5/64 loss: -0.2797005772590637
Batch 6/64 loss: -0.29217010736465454
Batch 7/64 loss: -0.28660380840301514
Batch 8/64 loss: -0.29704806208610535
Batch 9/64 loss: -0.28020620346069336
Batch 10/64 loss: -0.2683171033859253
Batch 11/64 loss: -0.2760065793991089
Batch 12/64 loss: -0.2811942994594574
Batch 13/64 loss: -0.27924737334251404
Batch 14/64 loss: -0.28241950273513794
Batch 15/64 loss: -0.2919348478317261
Batch 16/64 loss: -0.2849700450897217
Batch 17/64 loss: -0.28510579466819763
Batch 18/64 loss: -0.2886584997177124
Batch 19/64 loss: -0.2886067032814026
Batch 20/64 loss: -0.294977605342865
Batch 21/64 loss: -0.28098803758621216
Batch 22/64 loss: -0.29875338077545166
Batch 23/64 loss: -0.27589693665504456
Batch 24/64 loss: -0.28179508447647095
Batch 25/64 loss: -0.2845243215560913
Batch 26/64 loss: -0.2890818417072296
Batch 27/64 loss: -0.29309478402137756
Batch 28/64 loss: -0.2889135777950287
Batch 29/64 loss: -0.29708611965179443
Batch 30/64 loss: -0.29011431336402893
Batch 31/64 loss: -0.28538280725479126
Batch 32/64 loss: -0.285925030708313
Batch 33/64 loss: -0.289434015750885
Batch 34/64 loss: -0.2756126821041107
Batch 35/64 loss: -0.27617689967155457
Batch 36/64 loss: -0.29278790950775146
Batch 37/64 loss: -0.3010218143463135
Batch 38/64 loss: -0.2944108843803406
Batch 39/64 loss: -0.2826234996318817
Batch 40/64 loss: -0.2689918279647827
Batch 41/64 loss: -0.2847355306148529
Batch 42/64 loss: -0.29643499851226807
Batch 43/64 loss: -0.27460813522338867
Batch 44/64 loss: -0.28324341773986816
Batch 45/64 loss: -0.27659791707992554
Batch 46/64 loss: -0.30344757437705994
Batch 47/64 loss: -0.2775821089744568
Batch 48/64 loss: -0.28676384687423706
Batch 49/64 loss: -0.2924848794937134
Batch 50/64 loss: -0.283225953578949
Batch 51/64 loss: -0.2858964502811432
Batch 52/64 loss: -0.2937186658382416
Batch 53/64 loss: -0.2982117533683777
Batch 54/64 loss: -0.2917579412460327
Batch 55/64 loss: -0.29834890365600586
Batch 56/64 loss: -0.28883180022239685
Batch 57/64 loss: -0.2975807785987854
Batch 58/64 loss: -0.2874417304992676
Batch 59/64 loss: -0.28079983592033386
Batch 60/64 loss: -0.291866272687912
Batch 61/64 loss: -0.300432413816452
Batch 62/64 loss: -0.2951566278934479
Batch 63/64 loss: -0.30196088552474976
Batch 64/64 loss: -0.29687991738319397
Epoch 413  Train loss: -0.2872737476638719  Val loss: -0.26196186095988216
Epoch 414
-------------------------------
Batch 1/64 loss: -0.2864352762699127
Batch 2/64 loss: -0.2911476492881775
Batch 3/64 loss: -0.28422605991363525
Batch 4/64 loss: -0.28138506412506104
Batch 5/64 loss: -0.292977511882782
Batch 6/64 loss: -0.2902674376964569
Batch 7/64 loss: -0.28966444730758667
Batch 8/64 loss: -0.2905944585800171
Batch 9/64 loss: -0.28521811962127686
Batch 10/64 loss: -0.28167805075645447
Batch 11/64 loss: -0.2842135429382324
Batch 12/64 loss: -0.28474706411361694
Batch 13/64 loss: -0.28764432668685913
Batch 14/64 loss: -0.2826187014579773
Batch 15/64 loss: -0.28506723046302795
Batch 16/64 loss: -0.29726362228393555
Batch 17/64 loss: -0.30048805475234985
Batch 18/64 loss: -0.2862776517868042
Batch 19/64 loss: -0.3025536835193634
Batch 20/64 loss: -0.2963297963142395
Batch 21/64 loss: -0.2881333827972412
Batch 22/64 loss: -0.2800907492637634
Batch 23/64 loss: -0.2783811688423157
Batch 24/64 loss: -0.26144611835479736
Batch 25/64 loss: -0.2836820185184479
Batch 26/64 loss: -0.2897772192955017
Batch 27/64 loss: -0.2751014232635498
Batch 28/64 loss: -0.29106688499450684
Batch 29/64 loss: -0.2708069682121277
Batch 30/64 loss: -0.28619587421417236
Batch 31/64 loss: -0.28822776675224304
Batch 32/64 loss: -0.29964762926101685
Batch 33/64 loss: -0.2862052023410797
Batch 34/64 loss: -0.28325796127319336
Batch 35/64 loss: -0.2666575312614441
Batch 36/64 loss: -0.2975495159626007
Batch 37/64 loss: -0.292227566242218
Batch 38/64 loss: -0.29376310110092163
Batch 39/64 loss: -0.2720613181591034
Batch 40/64 loss: -0.27690398693084717
Batch 41/64 loss: -0.2833530008792877
Batch 42/64 loss: -0.2995060086250305
Batch 43/64 loss: -0.29718178510665894
Batch 44/64 loss: -0.27507999539375305
Batch 45/64 loss: -0.2892788052558899
Batch 46/64 loss: -0.2870614528656006
Batch 47/64 loss: -0.28053179383277893
Batch 48/64 loss: -0.2918814718723297
Batch 49/64 loss: -0.29625940322875977
Batch 50/64 loss: -0.29785820841789246
Batch 51/64 loss: -0.28017503023147583
Batch 52/64 loss: -0.29349833726882935
Batch 53/64 loss: -0.2910497784614563
Batch 54/64 loss: -0.2783487141132355
Batch 55/64 loss: -0.29216426610946655
Batch 56/64 loss: -0.2941342890262604
Batch 57/64 loss: -0.300454318523407
Batch 58/64 loss: -0.29955142736434937
Batch 59/64 loss: -0.27611416578292847
Batch 60/64 loss: -0.28598856925964355
Batch 61/64 loss: -0.2832138240337372
Batch 62/64 loss: -0.2921890616416931
Batch 63/64 loss: -0.2897060811519623
Batch 64/64 loss: -0.3021532893180847
Epoch 414  Train loss: -0.28742236693700157  Val loss: -0.26868670318544524
Epoch 415
-------------------------------
Batch 1/64 loss: -0.29192110896110535
Batch 2/64 loss: -0.2871469259262085
Batch 3/64 loss: -0.2859669029712677
Batch 4/64 loss: -0.28933751583099365
Batch 5/64 loss: -0.29416102170944214
Batch 6/64 loss: -0.27272748947143555
Batch 7/64 loss: -0.28838324546813965
Batch 8/64 loss: -0.29329752922058105
Batch 9/64 loss: -0.27448272705078125
Batch 10/64 loss: -0.28070586919784546
Batch 11/64 loss: -0.2803286612033844
Batch 12/64 loss: -0.2852663993835449
Batch 13/64 loss: -0.27729812264442444
Batch 14/64 loss: -0.2800748348236084
Batch 15/64 loss: -0.2872164845466614
Batch 16/64 loss: -0.28744393587112427
Batch 17/64 loss: -0.29221540689468384
Batch 18/64 loss: -0.29197055101394653
Batch 19/64 loss: -0.2892184853553772
Batch 20/64 loss: -0.28261294960975647
Batch 21/64 loss: -0.2809787392616272
Batch 22/64 loss: -0.2925504446029663
Batch 23/64 loss: -0.2847191095352173
Batch 24/64 loss: -0.2694131135940552
Batch 25/64 loss: -0.2866826057434082
Batch 26/64 loss: -0.2907356023788452
Batch 27/64 loss: -0.29544275999069214
Batch 28/64 loss: -0.28536444902420044
Batch 29/64 loss: -0.28114229440689087
Batch 30/64 loss: -0.29244017601013184
Batch 31/64 loss: -0.3072545528411865
Batch 32/64 loss: -0.2974051237106323
Batch 33/64 loss: -0.29951485991477966
Batch 34/64 loss: -0.2838192582130432
Batch 35/64 loss: -0.29142314195632935
Batch 36/64 loss: -0.2932078242301941
Batch 37/64 loss: -0.2924180328845978
Batch 38/64 loss: -0.2919563055038452
Batch 39/64 loss: -0.29595160484313965
Batch 40/64 loss: -0.28120261430740356
Batch 41/64 loss: -0.28964200615882874
Batch 42/64 loss: -0.2776983082294464
Batch 43/64 loss: -0.2895786166191101
Batch 44/64 loss: -0.2958589792251587
Batch 45/64 loss: -0.2893315553665161
Batch 46/64 loss: -0.29002341628074646
Batch 47/64 loss: -0.2844700217247009
Batch 48/64 loss: -0.2704240083694458
Batch 49/64 loss: -0.29782581329345703
Batch 50/64 loss: -0.29552173614501953
Batch 51/64 loss: -0.2930986285209656
Batch 52/64 loss: -0.29699796438217163
Batch 53/64 loss: -0.2789425253868103
Batch 54/64 loss: -0.29087862372398376
Batch 55/64 loss: -0.2867603302001953
Batch 56/64 loss: -0.29102399945259094
Batch 57/64 loss: -0.2970938980579376
Batch 58/64 loss: -0.28006839752197266
Batch 59/64 loss: -0.25608891248703003
Batch 60/64 loss: -0.2710081934928894
Batch 61/64 loss: -0.2924520969390869
Batch 62/64 loss: -0.2939980626106262
Batch 63/64 loss: -0.28909868001937866
Batch 64/64 loss: -0.28321006894111633
Epoch 415  Train loss: -0.28730495852582594  Val loss: -0.25380739323871654
Epoch 416
-------------------------------
Batch 1/64 loss: -0.2827027440071106
Batch 2/64 loss: -0.29720526933670044
Batch 3/64 loss: -0.27661776542663574
Batch 4/64 loss: -0.2801375389099121
Batch 5/64 loss: -0.29430320858955383
Batch 6/64 loss: -0.275859534740448
Batch 7/64 loss: -0.29160499572753906
Batch 8/64 loss: -0.28864169120788574
Batch 9/64 loss: -0.2904549837112427
Batch 10/64 loss: -0.29619336128234863
Batch 11/64 loss: -0.29520946741104126
Batch 12/64 loss: -0.28338587284088135
Batch 13/64 loss: -0.28412044048309326
Batch 14/64 loss: -0.28260183334350586
Batch 15/64 loss: -0.28674715757369995
Batch 16/64 loss: -0.29398104548454285
Batch 17/64 loss: -0.2799902558326721
Batch 18/64 loss: -0.28285473585128784
Batch 19/64 loss: -0.28513872623443604
Batch 20/64 loss: -0.28290724754333496
Batch 21/64 loss: -0.27901995182037354
Batch 22/64 loss: -0.2821091413497925
Batch 23/64 loss: -0.28265202045440674
Batch 24/64 loss: -0.2767418324947357
Batch 25/64 loss: -0.28160232305526733
Batch 26/64 loss: -0.2746334671974182
Batch 27/64 loss: -0.2726050615310669
Batch 28/64 loss: -0.293526291847229
Batch 29/64 loss: -0.2817091941833496
Batch 30/64 loss: -0.26645439863204956
Batch 31/64 loss: -0.2919766902923584
Batch 32/64 loss: -0.3019089698791504
Batch 33/64 loss: -0.2809261679649353
Batch 34/64 loss: -0.27383679151535034
Batch 35/64 loss: -0.287441611289978
Batch 36/64 loss: -0.29180654883384705
Batch 37/64 loss: -0.27666741609573364
Batch 38/64 loss: -0.28360992670059204
Batch 39/64 loss: -0.2664933204650879
Batch 40/64 loss: -0.26665574312210083
Batch 41/64 loss: -0.29347413778305054
Batch 42/64 loss: -0.26356691122055054
Batch 43/64 loss: -0.2863055169582367
Batch 44/64 loss: -0.27495279908180237
Batch 45/64 loss: -0.29476481676101685
Batch 46/64 loss: -0.2780303955078125
Batch 47/64 loss: -0.2804703116416931
Batch 48/64 loss: -0.2864467203617096
Batch 49/64 loss: -0.2548079490661621
Batch 50/64 loss: -0.2936134338378906
Batch 51/64 loss: -0.2742055356502533
Batch 52/64 loss: -0.288086473941803
Batch 53/64 loss: -0.2888242304325104
Batch 54/64 loss: -0.2741219401359558
Batch 55/64 loss: -0.28668421506881714
Batch 56/64 loss: -0.2845531105995178
Batch 57/64 loss: -0.28863734006881714
Batch 58/64 loss: -0.2778685688972473
Batch 59/64 loss: -0.2820519208908081
Batch 60/64 loss: -0.26775115728378296
Batch 61/64 loss: -0.2954975366592407
Batch 62/64 loss: -0.2917420268058777
Batch 63/64 loss: -0.2770325541496277
Batch 64/64 loss: -0.26949599385261536
Epoch 416  Train loss: -0.2828022955679426  Val loss: -0.2592992770303156
Epoch 417
-------------------------------
Batch 1/64 loss: -0.2797831892967224
Batch 2/64 loss: -0.27746492624282837
Batch 3/64 loss: -0.2989368140697479
Batch 4/64 loss: -0.25307178497314453
Batch 5/64 loss: -0.3012087941169739
Batch 6/64 loss: -0.27723169326782227
Batch 7/64 loss: -0.28468000888824463
Batch 8/64 loss: -0.2842455804347992
Batch 9/64 loss: -0.2933270335197449
Batch 10/64 loss: -0.2765843868255615
Batch 11/64 loss: -0.29155540466308594
Batch 12/64 loss: -0.2830193042755127
Batch 13/64 loss: -0.2918676733970642
Batch 14/64 loss: -0.283322811126709
Batch 15/64 loss: -0.28252190351486206
Batch 16/64 loss: -0.2919054627418518
Batch 17/64 loss: -0.29309460520744324
Batch 18/64 loss: -0.2946701645851135
Batch 19/64 loss: -0.28182142972946167
Batch 20/64 loss: -0.2960607409477234
Batch 21/64 loss: -0.2895505130290985
Batch 22/64 loss: -0.2841387689113617
Batch 23/64 loss: -0.29490020871162415
Batch 24/64 loss: -0.28657808899879456
Batch 25/64 loss: -0.2793981432914734
Batch 26/64 loss: -0.2966499328613281
Batch 27/64 loss: -0.2819874882698059
Batch 28/64 loss: -0.29577749967575073
Batch 29/64 loss: -0.2700175940990448
Batch 30/64 loss: -0.2811734676361084
Batch 31/64 loss: -0.2757754623889923
Batch 32/64 loss: -0.28058093786239624
Batch 33/64 loss: -0.2851496934890747
Batch 34/64 loss: -0.2879290282726288
Batch 35/64 loss: -0.2907300889492035
Batch 36/64 loss: -0.2676469385623932
Batch 37/64 loss: -0.2851727604866028
Batch 38/64 loss: -0.2829000949859619
Batch 39/64 loss: -0.27825191617012024
Batch 40/64 loss: -0.2774218022823334
Batch 41/64 loss: -0.2977195680141449
Batch 42/64 loss: -0.29355311393737793
Batch 43/64 loss: -0.2945420444011688
Batch 44/64 loss: -0.29013755917549133
Batch 45/64 loss: -0.27027779817581177
Batch 46/64 loss: -0.2921091318130493
Batch 47/64 loss: -0.28071141242980957
Batch 48/64 loss: -0.29083505272865295
Batch 49/64 loss: -0.2922113537788391
Batch 50/64 loss: -0.281831830739975
Batch 51/64 loss: -0.2904224693775177
Batch 52/64 loss: -0.27347999811172485
Batch 53/64 loss: -0.2928473651409149
Batch 54/64 loss: -0.27862316370010376
Batch 55/64 loss: -0.30364638566970825
Batch 56/64 loss: -0.29128900170326233
Batch 57/64 loss: -0.27692291140556335
Batch 58/64 loss: -0.2843857705593109
Batch 59/64 loss: -0.2890546917915344
Batch 60/64 loss: -0.28633788228034973
Batch 61/64 loss: -0.2823111414909363
Batch 62/64 loss: -0.30363476276397705
Batch 63/64 loss: -0.2949971854686737
Batch 64/64 loss: -0.2908712923526764
Epoch 417  Train loss: -0.2860884581126419  Val loss: -0.2664681033989818
Epoch 418
-------------------------------
Batch 1/64 loss: -0.28813403844833374
Batch 2/64 loss: -0.29941630363464355
Batch 3/64 loss: -0.2908187210559845
Batch 4/64 loss: -0.2800372838973999
Batch 5/64 loss: -0.2914828062057495
Batch 6/64 loss: -0.2934110462665558
Batch 7/64 loss: -0.2681525945663452
Batch 8/64 loss: -0.2776692509651184
Batch 9/64 loss: -0.28022056818008423
Batch 10/64 loss: -0.2832379937171936
Batch 11/64 loss: -0.2841888666152954
Batch 12/64 loss: -0.28333646059036255
Batch 13/64 loss: -0.2900088131427765
Batch 14/64 loss: -0.2809114158153534
Batch 15/64 loss: -0.30097922682762146
Batch 16/64 loss: -0.27736079692840576
Batch 17/64 loss: -0.2913137674331665
Batch 18/64 loss: -0.2813922166824341
Batch 19/64 loss: -0.2891729176044464
Batch 20/64 loss: -0.3028777837753296
Batch 21/64 loss: -0.3008175194263458
Batch 22/64 loss: -0.28096556663513184
Batch 23/64 loss: -0.2685626149177551
Batch 24/64 loss: -0.28107941150665283
Batch 25/64 loss: -0.29775381088256836
Batch 26/64 loss: -0.2911273241043091
Batch 27/64 loss: -0.2975504994392395
Batch 28/64 loss: -0.27947425842285156
Batch 29/64 loss: -0.2888985276222229
Batch 30/64 loss: -0.28669917583465576
Batch 31/64 loss: -0.29813894629478455
Batch 32/64 loss: -0.29523539543151855
Batch 33/64 loss: -0.29934054613113403
Batch 34/64 loss: -0.2965371608734131
Batch 35/64 loss: -0.27662527561187744
Batch 36/64 loss: -0.2706992030143738
Batch 37/64 loss: -0.28807851672172546
Batch 38/64 loss: -0.29519933462142944
Batch 39/64 loss: -0.29054611921310425
Batch 40/64 loss: -0.3012816309928894
Batch 41/64 loss: -0.2899031639099121
Batch 42/64 loss: -0.29803553223609924
Batch 43/64 loss: -0.2709977924823761
Batch 44/64 loss: -0.3011069893836975
Batch 45/64 loss: -0.2973746061325073
Batch 46/64 loss: -0.2862143814563751
Batch 47/64 loss: -0.2895109951496124
Batch 48/64 loss: -0.28735262155532837
Batch 49/64 loss: -0.29462432861328125
Batch 50/64 loss: -0.30106183886528015
Batch 51/64 loss: -0.2931651473045349
Batch 52/64 loss: -0.2687099575996399
Batch 53/64 loss: -0.2807069420814514
Batch 54/64 loss: -0.29015928506851196
Batch 55/64 loss: -0.28572696447372437
Batch 56/64 loss: -0.2892102301120758
Batch 57/64 loss: -0.2738110423088074
Batch 58/64 loss: -0.2727247476577759
Batch 59/64 loss: -0.29474523663520813
Batch 60/64 loss: -0.2888025641441345
Batch 61/64 loss: -0.27357611060142517
Batch 62/64 loss: -0.2807510197162628
Batch 63/64 loss: -0.28247490525245667
Batch 64/64 loss: -0.2804919481277466
Epoch 418  Train loss: -0.2873701815511666  Val loss: -0.25744015162753076
Epoch 419
-------------------------------
Batch 1/64 loss: -0.2805665135383606
Batch 2/64 loss: -0.2905339300632477
Batch 3/64 loss: -0.280741423368454
Batch 4/64 loss: -0.2946102023124695
Batch 5/64 loss: -0.28061479330062866
Batch 6/64 loss: -0.2503877282142639
Batch 7/64 loss: -0.2678726017475128
Batch 8/64 loss: -0.27964577078819275
Batch 9/64 loss: -0.28341978788375854
Batch 10/64 loss: -0.2555992603302002
Batch 11/64 loss: -0.2825778126716614
Batch 12/64 loss: -0.28187328577041626
Batch 13/64 loss: -0.2854148745536804
Batch 14/64 loss: -0.2807657718658447
Batch 15/64 loss: -0.2810187339782715
Batch 16/64 loss: -0.2878061532974243
Batch 17/64 loss: -0.28175005316734314
Batch 18/64 loss: -0.29650944471359253
Batch 19/64 loss: -0.28305310010910034
Batch 20/64 loss: -0.28900158405303955
Batch 21/64 loss: -0.29292571544647217
Batch 22/64 loss: -0.28206342458724976
Batch 23/64 loss: -0.2853403389453888
Batch 24/64 loss: -0.27418339252471924
Batch 25/64 loss: -0.2901890277862549
Batch 26/64 loss: -0.299238383769989
Batch 27/64 loss: -0.28938981890678406
Batch 28/64 loss: -0.2694193124771118
Batch 29/64 loss: -0.28779137134552
Batch 30/64 loss: -0.2778109908103943
Batch 31/64 loss: -0.28270983695983887
Batch 32/64 loss: -0.291767954826355
Batch 33/64 loss: -0.2827138900756836
Batch 34/64 loss: -0.2818767726421356
Batch 35/64 loss: -0.28476834297180176
Batch 36/64 loss: -0.283679723739624
Batch 37/64 loss: -0.2909456491470337
Batch 38/64 loss: -0.2759786546230316
Batch 39/64 loss: -0.2936033606529236
Batch 40/64 loss: -0.2954462468624115
Batch 41/64 loss: -0.26371026039123535
Batch 42/64 loss: -0.28404223918914795
Batch 43/64 loss: -0.29277002811431885
Batch 44/64 loss: -0.28339526057243347
Batch 45/64 loss: -0.2808692455291748
Batch 46/64 loss: -0.2818165719509125
Batch 47/64 loss: -0.29503971338272095
Batch 48/64 loss: -0.2929145395755768
Batch 49/64 loss: -0.2826116681098938
Batch 50/64 loss: -0.2960156500339508
Batch 51/64 loss: -0.2800482511520386
Batch 52/64 loss: -0.27945849299430847
Batch 53/64 loss: -0.28134459257125854
Batch 54/64 loss: -0.3011634647846222
Batch 55/64 loss: -0.3033098578453064
Batch 56/64 loss: -0.2854517102241516
Batch 57/64 loss: -0.27898141741752625
Batch 58/64 loss: -0.2949909567832947
Batch 59/64 loss: -0.3005366325378418
Batch 60/64 loss: -0.29457852244377136
Batch 61/64 loss: -0.2893979847431183
Batch 62/64 loss: -0.2923661470413208
Batch 63/64 loss: -0.29267483949661255
Batch 64/64 loss: -0.3045027554035187
Epoch 419  Train loss: -0.28519953173749585  Val loss: -0.27046377060749277
Epoch 420
-------------------------------
Batch 1/64 loss: -0.29062920808792114
Batch 2/64 loss: -0.2851921319961548
Batch 3/64 loss: -0.29759708046913147
Batch 4/64 loss: -0.291586697101593
Batch 5/64 loss: -0.2947283685207367
Batch 6/64 loss: -0.2870348393917084
Batch 7/64 loss: -0.2873910665512085
Batch 8/64 loss: -0.29424747824668884
Batch 9/64 loss: -0.2888842821121216
Batch 10/64 loss: -0.28405746817588806
Batch 11/64 loss: -0.30764737725257874
Batch 12/64 loss: -0.29597344994544983
Batch 13/64 loss: -0.2837693989276886
Batch 14/64 loss: -0.2804528772830963
Batch 15/64 loss: -0.2668868899345398
Batch 16/64 loss: -0.2932886481285095
Batch 17/64 loss: -0.30483394861221313
Batch 18/64 loss: -0.28258785605430603
Batch 19/64 loss: -0.29308652877807617
Batch 20/64 loss: -0.2962530851364136
Batch 21/64 loss: -0.2901226282119751
Batch 22/64 loss: -0.29131269454956055
Batch 23/64 loss: -0.28924936056137085
Batch 24/64 loss: -0.29099923372268677
Batch 25/64 loss: -0.2709430456161499
Batch 26/64 loss: -0.30089354515075684
Batch 27/64 loss: -0.28929877281188965
Batch 28/64 loss: -0.28651633858680725
Batch 29/64 loss: -0.28896427154541016
Batch 30/64 loss: -0.3013383150100708
Batch 31/64 loss: -0.2929583489894867
Batch 32/64 loss: -0.2878727614879608
Batch 33/64 loss: -0.2862786054611206
Batch 34/64 loss: -0.2825186252593994
Batch 35/64 loss: -0.2904149889945984
Batch 36/64 loss: -0.28758639097213745
Batch 37/64 loss: -0.28614234924316406
Batch 38/64 loss: -0.27789178490638733
Batch 39/64 loss: -0.2873568534851074
Batch 40/64 loss: -0.2963062524795532
Batch 41/64 loss: -0.2984076738357544
Batch 42/64 loss: -0.2798295021057129
Batch 43/64 loss: -0.27955928444862366
Batch 44/64 loss: -0.2969173192977905
Batch 45/64 loss: -0.2877553701400757
Batch 46/64 loss: -0.2982047200202942
Batch 47/64 loss: -0.2941422462463379
Batch 48/64 loss: -0.28815025091171265
Batch 49/64 loss: -0.2890250086784363
Batch 50/64 loss: -0.29276418685913086
Batch 51/64 loss: -0.29720059037208557
Batch 52/64 loss: -0.2927089333534241
Batch 53/64 loss: -0.2992147207260132
Batch 54/64 loss: -0.28632423281669617
Batch 55/64 loss: -0.28885412216186523
Batch 56/64 loss: -0.28845924139022827
Batch 57/64 loss: -0.27779620885849
Batch 58/64 loss: -0.29607510566711426
Batch 59/64 loss: -0.28294607996940613
Batch 60/64 loss: -0.29030096530914307
Batch 61/64 loss: -0.30092427134513855
Batch 62/64 loss: -0.2822236120700836
Batch 63/64 loss: -0.30128809809684753
Batch 64/64 loss: -0.2972986698150635
Epoch 420  Train loss: -0.2900884642320521  Val loss: -0.2717238928853851
Epoch 421
-------------------------------
Batch 1/64 loss: -0.2944304943084717
Batch 2/64 loss: -0.27589163184165955
Batch 3/64 loss: -0.29241427779197693
Batch 4/64 loss: -0.29339155554771423
Batch 5/64 loss: -0.30394744873046875
Batch 6/64 loss: -0.2789285182952881
Batch 7/64 loss: -0.3005492091178894
Batch 8/64 loss: -0.29183974862098694
Batch 9/64 loss: -0.28187209367752075
Batch 10/64 loss: -0.2870764136314392
Batch 11/64 loss: -0.28036510944366455
Batch 12/64 loss: -0.29573023319244385
Batch 13/64 loss: -0.2877276539802551
Batch 14/64 loss: -0.2982119917869568
Batch 15/64 loss: -0.29675087332725525
Batch 16/64 loss: -0.28329795598983765
Batch 17/64 loss: -0.2935774326324463
Batch 18/64 loss: -0.2716282606124878
Batch 19/64 loss: -0.29220837354660034
Batch 20/64 loss: -0.28058168292045593
Batch 21/64 loss: -0.28908973932266235
Batch 22/64 loss: -0.29228317737579346
Batch 23/64 loss: -0.3028773069381714
Batch 24/64 loss: -0.29630059003829956
Batch 25/64 loss: -0.2741926312446594
Batch 26/64 loss: -0.27402570843696594
Batch 27/64 loss: -0.29886841773986816
Batch 28/64 loss: -0.29463180899620056
Batch 29/64 loss: -0.291252076625824
Batch 30/64 loss: -0.28594106435775757
Batch 31/64 loss: -0.2672693133354187
Batch 32/64 loss: -0.27739059925079346
Batch 33/64 loss: -0.2878912389278412
Batch 34/64 loss: -0.2877100706100464
Batch 35/64 loss: -0.2699810266494751
Batch 36/64 loss: -0.29849866032600403
Batch 37/64 loss: -0.2912544906139374
Batch 38/64 loss: -0.29764413833618164
Batch 39/64 loss: -0.2771310806274414
Batch 40/64 loss: -0.28589963912963867
Batch 41/64 loss: -0.27730339765548706
Batch 42/64 loss: -0.29969269037246704
Batch 43/64 loss: -0.29028868675231934
Batch 44/64 loss: -0.2982979714870453
Batch 45/64 loss: -0.2837923765182495
Batch 46/64 loss: -0.28126680850982666
Batch 47/64 loss: -0.28634947538375854
Batch 48/64 loss: -0.29616665840148926
Batch 49/64 loss: -0.29274922609329224
Batch 50/64 loss: -0.2788126468658447
Batch 51/64 loss: -0.30076828598976135
Batch 52/64 loss: -0.28995954990386963
Batch 53/64 loss: -0.2696983814239502
Batch 54/64 loss: -0.2849556803703308
Batch 55/64 loss: -0.29912176728248596
Batch 56/64 loss: -0.30983614921569824
Batch 57/64 loss: -0.3010624051094055
Batch 58/64 loss: -0.2975289821624756
Batch 59/64 loss: -0.2919955849647522
Batch 60/64 loss: -0.2953067719936371
Batch 61/64 loss: -0.30064111948013306
Batch 62/64 loss: -0.2748044729232788
Batch 63/64 loss: -0.29065990447998047
Batch 64/64 loss: -0.2994016110897064
Epoch 421  Train loss: -0.2891947284633038  Val loss: -0.27452706317721365
Epoch 422
-------------------------------
Batch 1/64 loss: -0.28711146116256714
Batch 2/64 loss: -0.302016019821167
Batch 3/64 loss: -0.294486939907074
Batch 4/64 loss: -0.301068514585495
Batch 5/64 loss: -0.3030869960784912
Batch 6/64 loss: -0.30661386251449585
Batch 7/64 loss: -0.296150267124176
Batch 8/64 loss: -0.3035697042942047
Batch 9/64 loss: -0.2972547113895416
Batch 10/64 loss: -0.30274373292922974
Batch 11/64 loss: -0.29232436418533325
Batch 12/64 loss: -0.29499197006225586
Batch 13/64 loss: -0.27313387393951416
Batch 14/64 loss: -0.29530033469200134
Batch 15/64 loss: -0.2992868423461914
Batch 16/64 loss: -0.28675585985183716
Batch 17/64 loss: -0.2764439582824707
Batch 18/64 loss: -0.2840309739112854
Batch 19/64 loss: -0.29158174991607666
Batch 20/64 loss: -0.2773137390613556
Batch 21/64 loss: -0.2985091805458069
Batch 22/64 loss: -0.27274131774902344
Batch 23/64 loss: -0.2810314893722534
Batch 24/64 loss: -0.2984159588813782
Batch 25/64 loss: -0.3017961084842682
Batch 26/64 loss: -0.2780594527721405
Batch 27/64 loss: -0.30139097571372986
Batch 28/64 loss: -0.291012167930603
Batch 29/64 loss: -0.2777586877346039
Batch 30/64 loss: -0.285671591758728
Batch 31/64 loss: -0.29405730962753296
Batch 32/64 loss: -0.294024258852005
Batch 33/64 loss: -0.2952081561088562
Batch 34/64 loss: -0.2950282096862793
Batch 35/64 loss: -0.288260281085968
Batch 36/64 loss: -0.3027445673942566
Batch 37/64 loss: -0.30111223459243774
Batch 38/64 loss: -0.30092817544937134
Batch 39/64 loss: -0.27917009592056274
Batch 40/64 loss: -0.26986294984817505
Batch 41/64 loss: -0.2785767912864685
Batch 42/64 loss: -0.29072830080986023
Batch 43/64 loss: -0.28885573148727417
Batch 44/64 loss: -0.2996560037136078
Batch 45/64 loss: -0.282947838306427
Batch 46/64 loss: -0.27989161014556885
Batch 47/64 loss: -0.282939076423645
Batch 48/64 loss: -0.2838839292526245
Batch 49/64 loss: -0.27358025312423706
Batch 50/64 loss: -0.2821540832519531
Batch 51/64 loss: -0.2897864878177643
Batch 52/64 loss: -0.2910720109939575
Batch 53/64 loss: -0.2919262945652008
Batch 54/64 loss: -0.2747972309589386
Batch 55/64 loss: -0.27908337116241455
Batch 56/64 loss: -0.2868874669075012
Batch 57/64 loss: -0.2910394072532654
Batch 58/64 loss: -0.2994898557662964
Batch 59/64 loss: -0.28220513463020325
Batch 60/64 loss: -0.2877972722053528
Batch 61/64 loss: -0.29562076926231384
Batch 62/64 loss: -0.29190540313720703
Batch 63/64 loss: -0.30779388546943665
Batch 64/64 loss: -0.28005972504615784
Epoch 422  Train loss: -0.29011313015339424  Val loss: -0.2648372244589108
Epoch 423
-------------------------------
Batch 1/64 loss: -0.285675048828125
Batch 2/64 loss: -0.2873222231864929
Batch 3/64 loss: -0.29890933632850647
Batch 4/64 loss: -0.28413474559783936
Batch 5/64 loss: -0.3002074062824249
Batch 6/64 loss: -0.29818227887153625
Batch 7/64 loss: -0.3013615012168884
Batch 8/64 loss: -0.2937503457069397
Batch 9/64 loss: -0.29816848039627075
Batch 10/64 loss: -0.30849188566207886
Batch 11/64 loss: -0.28797346353530884
Batch 12/64 loss: -0.29035788774490356
Batch 13/64 loss: -0.2809411883354187
Batch 14/64 loss: -0.3021560609340668
Batch 15/64 loss: -0.28973597288131714
Batch 16/64 loss: -0.2852732241153717
Batch 17/64 loss: -0.291823148727417
Batch 18/64 loss: -0.27212339639663696
Batch 19/64 loss: -0.2931423783302307
Batch 20/64 loss: -0.2973394989967346
Batch 21/64 loss: -0.2905932664871216
Batch 22/64 loss: -0.29234081506729126
Batch 23/64 loss: -0.3002614676952362
Batch 24/64 loss: -0.2912478446960449
Batch 25/64 loss: -0.2795025110244751
Batch 26/64 loss: -0.2987039089202881
Batch 27/64 loss: -0.3076038360595703
Batch 28/64 loss: -0.29541853070259094
Batch 29/64 loss: -0.2919330596923828
Batch 30/64 loss: -0.2886703908443451
Batch 31/64 loss: -0.30080777406692505
Batch 32/64 loss: -0.28492599725723267
Batch 33/64 loss: -0.2909437119960785
Batch 34/64 loss: -0.3027702569961548
Batch 35/64 loss: -0.29573649168014526
Batch 36/64 loss: -0.29878032207489014
Batch 37/64 loss: -0.29523441195487976
Batch 38/64 loss: -0.29357099533081055
Batch 39/64 loss: -0.29621708393096924
Batch 40/64 loss: -0.29590168595314026
Batch 41/64 loss: -0.286200612783432
Batch 42/64 loss: -0.28966474533081055
Batch 43/64 loss: -0.29569536447525024
Batch 44/64 loss: -0.2898288667201996
Batch 45/64 loss: -0.30186235904693604
Batch 46/64 loss: -0.29065245389938354
Batch 47/64 loss: -0.29817259311676025
Batch 48/64 loss: -0.29341039061546326
Batch 49/64 loss: -0.3035898506641388
Batch 50/64 loss: -0.2729100286960602
Batch 51/64 loss: -0.29559531807899475
Batch 52/64 loss: -0.2758578360080719
Batch 53/64 loss: -0.30004245042800903
Batch 54/64 loss: -0.28958892822265625
Batch 55/64 loss: -0.2939823269844055
Batch 56/64 loss: -0.2774633765220642
Batch 57/64 loss: -0.3012729287147522
Batch 58/64 loss: -0.29448631405830383
Batch 59/64 loss: -0.28856390714645386
Batch 60/64 loss: -0.30170178413391113
Batch 61/64 loss: -0.30936020612716675
Batch 62/64 loss: -0.303500771522522
Batch 63/64 loss: -0.2884323000907898
Batch 64/64 loss: -0.27877628803253174
Epoch 423  Train loss: -0.29316316492417277  Val loss: -0.2718967041608804
Epoch 424
-------------------------------
Batch 1/64 loss: -0.2870957553386688
Batch 2/64 loss: -0.3009696900844574
Batch 3/64 loss: -0.30498120188713074
Batch 4/64 loss: -0.30238813161849976
Batch 5/64 loss: -0.29364073276519775
Batch 6/64 loss: -0.2917582392692566
Batch 7/64 loss: -0.291149765253067
Batch 8/64 loss: -0.28881311416625977
Batch 9/64 loss: -0.29520854353904724
Batch 10/64 loss: -0.28472691774368286
Batch 11/64 loss: -0.2812551259994507
Batch 12/64 loss: -0.2975507080554962
Batch 13/64 loss: -0.29119300842285156
Batch 14/64 loss: -0.2915566563606262
Batch 15/64 loss: -0.29864710569381714
Batch 16/64 loss: -0.2980000972747803
Batch 17/64 loss: -0.2772270441055298
Batch 18/64 loss: -0.2862594723701477
Batch 19/64 loss: -0.2853527069091797
Batch 20/64 loss: -0.2863653302192688
Batch 21/64 loss: -0.2913016080856323
Batch 22/64 loss: -0.3042159676551819
Batch 23/64 loss: -0.3007025718688965
Batch 24/64 loss: -0.29983797669410706
Batch 25/64 loss: -0.2977356016635895
Batch 26/64 loss: -0.2998068332672119
Batch 27/64 loss: -0.2725958526134491
Batch 28/64 loss: -0.29610520601272583
Batch 29/64 loss: -0.2973972260951996
Batch 30/64 loss: -0.2932364344596863
Batch 31/64 loss: -0.2940155863761902
Batch 32/64 loss: -0.2943376302719116
Batch 33/64 loss: -0.295976847410202
Batch 34/64 loss: -0.2945358157157898
Batch 35/64 loss: -0.29016685485839844
Batch 36/64 loss: -0.3066328763961792
Batch 37/64 loss: -0.29718297719955444
Batch 38/64 loss: -0.28211238980293274
Batch 39/64 loss: -0.29308515787124634
Batch 40/64 loss: -0.2998102307319641
Batch 41/64 loss: -0.3050382435321808
Batch 42/64 loss: -0.2883347272872925
Batch 43/64 loss: -0.29976266622543335
Batch 44/64 loss: -0.29834914207458496
Batch 45/64 loss: -0.275936484336853
Batch 46/64 loss: -0.2912391722202301
Batch 47/64 loss: -0.2933923006057739
Batch 48/64 loss: -0.28961193561553955
Batch 49/64 loss: -0.29358476400375366
Batch 50/64 loss: -0.2844809889793396
Batch 51/64 loss: -0.303516685962677
Batch 52/64 loss: -0.2960873544216156
Batch 53/64 loss: -0.29723453521728516
Batch 54/64 loss: -0.29466986656188965
Batch 55/64 loss: -0.2970193326473236
Batch 56/64 loss: -0.29203981161117554
Batch 57/64 loss: -0.30546608567237854
Batch 58/64 loss: -0.3019455671310425
Batch 59/64 loss: -0.28905540704727173
Batch 60/64 loss: -0.2884927988052368
Batch 61/64 loss: -0.3036125898361206
Batch 62/64 loss: -0.27778515219688416
Batch 63/64 loss: -0.283907413482666
Batch 64/64 loss: -0.2973482608795166
Epoch 424  Train loss: -0.29346674844330434  Val loss: -0.2665233557781403
Epoch 425
-------------------------------
Batch 1/64 loss: -0.2990632653236389
Batch 2/64 loss: -0.28491249680519104
Batch 3/64 loss: -0.2818848490715027
Batch 4/64 loss: -0.2832925319671631
Batch 5/64 loss: -0.29677051305770874
Batch 6/64 loss: -0.29929375648498535
Batch 7/64 loss: -0.29325366020202637
Batch 8/64 loss: -0.28582149744033813
Batch 9/64 loss: -0.2970820665359497
Batch 10/64 loss: -0.3003310561180115
Batch 11/64 loss: -0.2864806652069092
Batch 12/64 loss: -0.2927885949611664
Batch 13/64 loss: -0.2892969846725464
Batch 14/64 loss: -0.2911711037158966
Batch 15/64 loss: -0.2927730977535248
Batch 16/64 loss: -0.28410476446151733
Batch 17/64 loss: -0.2968832552433014
Batch 18/64 loss: -0.3015480935573578
Batch 19/64 loss: -0.3051401972770691
Batch 20/64 loss: -0.29423534870147705
Batch 21/64 loss: -0.3000447750091553
Batch 22/64 loss: -0.28853631019592285
Batch 23/64 loss: -0.28773823380470276
Batch 24/64 loss: -0.27488183975219727
Batch 25/64 loss: -0.30264589190483093
Batch 26/64 loss: -0.2836737632751465
Batch 27/64 loss: -0.30046650767326355
Batch 28/64 loss: -0.294744610786438
Batch 29/64 loss: -0.2924891710281372
Batch 30/64 loss: -0.28403693437576294
Batch 31/64 loss: -0.2894213795661926
Batch 32/64 loss: -0.29224687814712524
Batch 33/64 loss: -0.28074145317077637
Batch 34/64 loss: -0.2895110249519348
Batch 35/64 loss: -0.2860358953475952
Batch 36/64 loss: -0.286124050617218
Batch 37/64 loss: -0.2787703573703766
Batch 38/64 loss: -0.30085551738739014
Batch 39/64 loss: -0.2963840961456299
Batch 40/64 loss: -0.282805860042572
Batch 41/64 loss: -0.31108295917510986
Batch 42/64 loss: -0.3047937750816345
Batch 43/64 loss: -0.2902785539627075
Batch 44/64 loss: -0.304679274559021
Batch 45/64 loss: -0.29478341341018677
Batch 46/64 loss: -0.30462396144866943
Batch 47/64 loss: -0.2896248400211334
Batch 48/64 loss: -0.2889508008956909
Batch 49/64 loss: -0.2928430736064911
Batch 50/64 loss: -0.29977697134017944
Batch 51/64 loss: -0.29686301946640015
Batch 52/64 loss: -0.27310800552368164
Batch 53/64 loss: -0.2871624827384949
Batch 54/64 loss: -0.2742210626602173
Batch 55/64 loss: -0.29151737689971924
Batch 56/64 loss: -0.28641602396965027
Batch 57/64 loss: -0.3045147955417633
Batch 58/64 loss: -0.28857970237731934
Batch 59/64 loss: -0.2880662977695465
Batch 60/64 loss: -0.2982296347618103
Batch 61/64 loss: -0.2908937335014343
Batch 62/64 loss: -0.2960110902786255
Batch 63/64 loss: -0.2733757793903351
Batch 64/64 loss: -0.30170750617980957
Epoch 425  Train loss: -0.29184250364116593  Val loss: -0.27198946926601975
Epoch 426
-------------------------------
Batch 1/64 loss: -0.2816865146160126
Batch 2/64 loss: -0.30363208055496216
Batch 3/64 loss: -0.31092941761016846
Batch 4/64 loss: -0.2974065840244293
Batch 5/64 loss: -0.29868537187576294
Batch 6/64 loss: -0.29993852972984314
Batch 7/64 loss: -0.29754552245140076
Batch 8/64 loss: -0.2893639802932739
Batch 9/64 loss: -0.30019575357437134
Batch 10/64 loss: -0.27876073122024536
Batch 11/64 loss: -0.2994515001773834
Batch 12/64 loss: -0.3057762086391449
Batch 13/64 loss: -0.3001265227794647
Batch 14/64 loss: -0.29734915494918823
Batch 15/64 loss: -0.29330942034721375
Batch 16/64 loss: -0.2961488962173462
Batch 17/64 loss: -0.2986034154891968
Batch 18/64 loss: -0.28972822427749634
Batch 19/64 loss: -0.298520565032959
Batch 20/64 loss: -0.2994574308395386
Batch 21/64 loss: -0.28751951456069946
Batch 22/64 loss: -0.3106701970100403
Batch 23/64 loss: -0.3028719425201416
Batch 24/64 loss: -0.29561519622802734
Batch 25/64 loss: -0.2680209279060364
Batch 26/64 loss: -0.2735269069671631
Batch 27/64 loss: -0.2932640314102173
Batch 28/64 loss: -0.30222228169441223
Batch 29/64 loss: -0.2868077754974365
Batch 30/64 loss: -0.2816649079322815
Batch 31/64 loss: -0.29057759046554565
Batch 32/64 loss: -0.2965933680534363
Batch 33/64 loss: -0.305541455745697
Batch 34/64 loss: -0.2907182574272156
Batch 35/64 loss: -0.26906299591064453
Batch 36/64 loss: -0.2904619574546814
Batch 37/64 loss: -0.2910083532333374
Batch 38/64 loss: -0.29586145281791687
Batch 39/64 loss: -0.29840922355651855
Batch 40/64 loss: -0.2914453446865082
Batch 41/64 loss: -0.2900529205799103
Batch 42/64 loss: -0.2822389602661133
Batch 43/64 loss: -0.2963789105415344
Batch 44/64 loss: -0.2756723165512085
Batch 45/64 loss: -0.2961769104003906
Batch 46/64 loss: -0.3049769401550293
Batch 47/64 loss: -0.26915258169174194
Batch 48/64 loss: -0.3004196286201477
Batch 49/64 loss: -0.29942142963409424
Batch 50/64 loss: -0.292033851146698
Batch 51/64 loss: -0.28504759073257446
Batch 52/64 loss: -0.29988300800323486
Batch 53/64 loss: -0.28868648409843445
Batch 54/64 loss: -0.2965145409107208
Batch 55/64 loss: -0.2737835943698883
Batch 56/64 loss: -0.2889009416103363
Batch 57/64 loss: -0.2923087775707245
Batch 58/64 loss: -0.2990052402019501
Batch 59/64 loss: -0.29161518812179565
Batch 60/64 loss: -0.2887301743030548
Batch 61/64 loss: -0.28667187690734863
Batch 62/64 loss: -0.2968062460422516
Batch 63/64 loss: -0.3023277223110199
Batch 64/64 loss: -0.30043530464172363
Epoch 426  Train loss: -0.2930292050043742  Val loss: -0.27090954780578613
Epoch 427
-------------------------------
Batch 1/64 loss: -0.3013410270214081
Batch 2/64 loss: -0.28948086500167847
Batch 3/64 loss: -0.289890319108963
Batch 4/64 loss: -0.2814416289329529
Batch 5/64 loss: -0.3008940517902374
Batch 6/64 loss: -0.29374459385871887
Batch 7/64 loss: -0.29418808221817017
Batch 8/64 loss: -0.29723459482192993
Batch 9/64 loss: -0.2991127669811249
Batch 10/64 loss: -0.3048597574234009
Batch 11/64 loss: -0.2914261221885681
Batch 12/64 loss: -0.29224342107772827
Batch 13/64 loss: -0.2909509539604187
Batch 14/64 loss: -0.2892414331436157
Batch 15/64 loss: -0.2895854711532593
Batch 16/64 loss: -0.3071218430995941
Batch 17/64 loss: -0.2908664345741272
Batch 18/64 loss: -0.3033825755119324
Batch 19/64 loss: -0.2893887460231781
Batch 20/64 loss: -0.2929746210575104
Batch 21/64 loss: -0.3048555850982666
Batch 22/64 loss: -0.29658639430999756
Batch 23/64 loss: -0.27655571699142456
Batch 24/64 loss: -0.29203832149505615
Batch 25/64 loss: -0.30650028586387634
Batch 26/64 loss: -0.30250346660614014
Batch 27/64 loss: -0.2845076322555542
Batch 28/64 loss: -0.30023014545440674
Batch 29/64 loss: -0.3067718744277954
Batch 30/64 loss: -0.30062568187713623
Batch 31/64 loss: -0.3024207353591919
Batch 32/64 loss: -0.29244592785835266
Batch 33/64 loss: -0.29065099358558655
Batch 34/64 loss: -0.2840048670768738
Batch 35/64 loss: -0.29076164960861206
Batch 36/64 loss: -0.28107839822769165
Batch 37/64 loss: -0.288449227809906
Batch 38/64 loss: -0.2980014681816101
Batch 39/64 loss: -0.29014086723327637
Batch 40/64 loss: -0.2947017252445221
Batch 41/64 loss: -0.2963968515396118
Batch 42/64 loss: -0.28814828395843506
Batch 43/64 loss: -0.28242599964141846
Batch 44/64 loss: -0.2994426488876343
Batch 45/64 loss: -0.296194851398468
Batch 46/64 loss: -0.29551124572753906
Batch 47/64 loss: -0.2991410791873932
Batch 48/64 loss: -0.30050039291381836
Batch 49/64 loss: -0.2940133213996887
Batch 50/64 loss: -0.29349982738494873
Batch 51/64 loss: -0.30107030272483826
Batch 52/64 loss: -0.28360313177108765
Batch 53/64 loss: -0.27498859167099
Batch 54/64 loss: -0.2903158962726593
Batch 55/64 loss: -0.29641270637512207
Batch 56/64 loss: -0.28478825092315674
Batch 57/64 loss: -0.30664169788360596
Batch 58/64 loss: -0.29859474301338196
Batch 59/64 loss: -0.28219497203826904
Batch 60/64 loss: -0.2950519323348999
Batch 61/64 loss: -0.2883342504501343
Batch 62/64 loss: -0.28041979670524597
Batch 63/64 loss: -0.30125850439071655
Batch 64/64 loss: -0.30540651082992554
Epoch 427  Train loss: -0.29382281467026355  Val loss: -0.26659026322086243
Epoch 428
-------------------------------
Batch 1/64 loss: -0.28643500804901123
Batch 2/64 loss: -0.28483331203460693
Batch 3/64 loss: -0.3004240393638611
Batch 4/64 loss: -0.28953757882118225
Batch 5/64 loss: -0.30691400170326233
Batch 6/64 loss: -0.3066869378089905
Batch 7/64 loss: -0.2911549508571625
Batch 8/64 loss: -0.28836849331855774
Batch 9/64 loss: -0.2967124581336975
Batch 10/64 loss: -0.302482545375824
Batch 11/64 loss: -0.2916300892829895
Batch 12/64 loss: -0.28454747796058655
Batch 13/64 loss: -0.2977588176727295
Batch 14/64 loss: -0.286085307598114
Batch 15/64 loss: -0.29099923372268677
Batch 16/64 loss: -0.2970755100250244
Batch 17/64 loss: -0.3013877868652344
Batch 18/64 loss: -0.2954794764518738
Batch 19/64 loss: -0.2908749580383301
Batch 20/64 loss: -0.295509934425354
Batch 21/64 loss: -0.29869717359542847
Batch 22/64 loss: -0.28757399320602417
Batch 23/64 loss: -0.2854999601840973
Batch 24/64 loss: -0.28489816188812256
Batch 25/64 loss: -0.30251267552375793
Batch 26/64 loss: -0.30268436670303345
Batch 27/64 loss: -0.29289501905441284
Batch 28/64 loss: -0.28542906045913696
Batch 29/64 loss: -0.28405797481536865
Batch 30/64 loss: -0.2889351546764374
Batch 31/64 loss: -0.3044026494026184
Batch 32/64 loss: -0.2947958707809448
Batch 33/64 loss: -0.28804120421409607
Batch 34/64 loss: -0.28113698959350586
Batch 35/64 loss: -0.2798789143562317
Batch 36/64 loss: -0.3006826639175415
Batch 37/64 loss: -0.2765161991119385
Batch 38/64 loss: -0.2776416540145874
Batch 39/64 loss: -0.29330283403396606
Batch 40/64 loss: -0.28772759437561035
Batch 41/64 loss: -0.29313337802886963
Batch 42/64 loss: -0.2947435677051544
Batch 43/64 loss: -0.27831268310546875
Batch 44/64 loss: -0.27854442596435547
Batch 45/64 loss: -0.2969655394554138
Batch 46/64 loss: -0.2986815571784973
Batch 47/64 loss: -0.29212501645088196
Batch 48/64 loss: -0.2923193573951721
Batch 49/64 loss: -0.29383212327957153
Batch 50/64 loss: -0.2903781831264496
Batch 51/64 loss: -0.2914537489414215
Batch 52/64 loss: -0.2766646444797516
Batch 53/64 loss: -0.292238712310791
Batch 54/64 loss: -0.29044681787490845
Batch 55/64 loss: -0.2778865694999695
Batch 56/64 loss: -0.29116106033325195
Batch 57/64 loss: -0.29031169414520264
Batch 58/64 loss: -0.2955511808395386
Batch 59/64 loss: -0.28509432077407837
Batch 60/64 loss: -0.29904428124427795
Batch 61/64 loss: -0.28584468364715576
Batch 62/64 loss: -0.2897874712944031
Batch 63/64 loss: -0.29083168506622314
Batch 64/64 loss: -0.27710044384002686
Epoch 428  Train loss: -0.29106487947351795  Val loss: -0.263451747468247
Epoch 429
-------------------------------
Batch 1/64 loss: -0.2953875958919525
Batch 2/64 loss: -0.29683583974838257
Batch 3/64 loss: -0.2967671751976013
Batch 4/64 loss: -0.28891173005104065
Batch 5/64 loss: -0.2949148416519165
Batch 6/64 loss: -0.30095645785331726
Batch 7/64 loss: -0.30128952860832214
Batch 8/64 loss: -0.29645901918411255
Batch 9/64 loss: -0.27637988328933716
Batch 10/64 loss: -0.28709179162979126
Batch 11/64 loss: -0.2937704026699066
Batch 12/64 loss: -0.30279916524887085
Batch 13/64 loss: -0.30697935819625854
Batch 14/64 loss: -0.29891565442085266
Batch 15/64 loss: -0.29912054538726807
Batch 16/64 loss: -0.30060064792633057
Batch 17/64 loss: -0.2902446985244751
Batch 18/64 loss: -0.30567413568496704
Batch 19/64 loss: -0.29762402176856995
Batch 20/64 loss: -0.2959419786930084
Batch 21/64 loss: -0.2958228588104248
Batch 22/64 loss: -0.2803255319595337
Batch 23/64 loss: -0.2823941707611084
Batch 24/64 loss: -0.29698190093040466
Batch 25/64 loss: -0.29424145817756653
Batch 26/64 loss: -0.29159343242645264
Batch 27/64 loss: -0.2949163317680359
Batch 28/64 loss: -0.31376129388809204
Batch 29/64 loss: -0.3049185276031494
Batch 30/64 loss: -0.2989179491996765
Batch 31/64 loss: -0.2960056960582733
Batch 32/64 loss: -0.27896127104759216
Batch 33/64 loss: -0.2856234610080719
Batch 34/64 loss: -0.29674211144447327
Batch 35/64 loss: -0.29560744762420654
Batch 36/64 loss: -0.28719887137413025
Batch 37/64 loss: -0.293784499168396
Batch 38/64 loss: -0.30080872774124146
Batch 39/64 loss: -0.29051071405410767
Batch 40/64 loss: -0.2955666780471802
Batch 41/64 loss: -0.29272305965423584
Batch 42/64 loss: -0.2784278690814972
Batch 43/64 loss: -0.2782474756240845
Batch 44/64 loss: -0.2881924510002136
Batch 45/64 loss: -0.2889865040779114
Batch 46/64 loss: -0.29592177271842957
Batch 47/64 loss: -0.27954500913619995
Batch 48/64 loss: -0.2942808270454407
Batch 49/64 loss: -0.3000909388065338
Batch 50/64 loss: -0.27633076906204224
Batch 51/64 loss: -0.29389312863349915
Batch 52/64 loss: -0.3005112111568451
Batch 53/64 loss: -0.28400522470474243
Batch 54/64 loss: -0.2721540331840515
Batch 55/64 loss: -0.2974391579627991
Batch 56/64 loss: -0.2831726670265198
Batch 57/64 loss: -0.27546927332878113
Batch 58/64 loss: -0.2842189371585846
Batch 59/64 loss: -0.2758340835571289
Batch 60/64 loss: -0.28015756607055664
Batch 61/64 loss: -0.29713451862335205
Batch 62/64 loss: -0.2957002520561218
Batch 63/64 loss: -0.2942377030849457
Batch 64/64 loss: -0.28078538179397583
Epoch 429  Train loss: -0.29205664115793567  Val loss: -0.2613609214009288
Epoch 430
-------------------------------
Batch 1/64 loss: -0.29069286584854126
Batch 2/64 loss: -0.28480619192123413
Batch 3/64 loss: -0.2903876304626465
Batch 4/64 loss: -0.28263577818870544
Batch 5/64 loss: -0.27636104822158813
Batch 6/64 loss: -0.29549920558929443
Batch 7/64 loss: -0.2816838026046753
Batch 8/64 loss: -0.297394335269928
Batch 9/64 loss: -0.27281317114830017
Batch 10/64 loss: -0.2791242301464081
Batch 11/64 loss: -0.2933673858642578
Batch 12/64 loss: -0.26349490880966187
Batch 13/64 loss: -0.27617985010147095
Batch 14/64 loss: -0.2791019380092621
Batch 15/64 loss: -0.2803705036640167
Batch 16/64 loss: -0.27433598041534424
Batch 17/64 loss: -0.2795001268386841
Batch 18/64 loss: -0.2720911204814911
Batch 19/64 loss: -0.2874595522880554
Batch 20/64 loss: -0.2946106791496277
Batch 21/64 loss: -0.2791602313518524
Batch 22/64 loss: -0.2928369641304016
Batch 23/64 loss: -0.28491654992103577
Batch 24/64 loss: -0.28770655393600464
Batch 25/64 loss: -0.28419262170791626
Batch 26/64 loss: -0.283625066280365
Batch 27/64 loss: -0.2920762598514557
Batch 28/64 loss: -0.2848961353302002
Batch 29/64 loss: -0.28296828269958496
Batch 30/64 loss: -0.29718759655952454
Batch 31/64 loss: -0.2887641191482544
Batch 32/64 loss: -0.29398566484451294
Batch 33/64 loss: -0.28366053104400635
Batch 34/64 loss: -0.28279513120651245
Batch 35/64 loss: -0.2918032705783844
Batch 36/64 loss: -0.2702624201774597
Batch 37/64 loss: -0.2871599495410919
Batch 38/64 loss: -0.272085964679718
Batch 39/64 loss: -0.30479782819747925
Batch 40/64 loss: -0.2787332832813263
Batch 41/64 loss: -0.29104211926460266
Batch 42/64 loss: -0.29004883766174316
Batch 43/64 loss: -0.2957925796508789
Batch 44/64 loss: -0.26974064111709595
Batch 45/64 loss: -0.27257147431373596
Batch 46/64 loss: -0.2818574905395508
Batch 47/64 loss: -0.2813182473182678
Batch 48/64 loss: -0.28392869234085083
Batch 49/64 loss: -0.2977411448955536
Batch 50/64 loss: -0.284637451171875
Batch 51/64 loss: -0.3005083203315735
Batch 52/64 loss: -0.288483589887619
Batch 53/64 loss: -0.29007527232170105
Batch 54/64 loss: -0.288099467754364
Batch 55/64 loss: -0.2910363972187042
Batch 56/64 loss: -0.278939813375473
Batch 57/64 loss: -0.2847461998462677
Batch 58/64 loss: -0.29138562083244324
Batch 59/64 loss: -0.2909081280231476
Batch 60/64 loss: -0.2881702780723572
Batch 61/64 loss: -0.30309298634529114
Batch 62/64 loss: -0.2721121311187744
Batch 63/64 loss: -0.28203171491622925
Batch 64/64 loss: -0.286077082157135
Epoch 430  Train loss: -0.2853074688537448  Val loss: -0.27114960489813816
Epoch 431
-------------------------------
Batch 1/64 loss: -0.2933724522590637
Batch 2/64 loss: -0.2947876751422882
Batch 3/64 loss: -0.27056246995925903
Batch 4/64 loss: -0.2935488224029541
Batch 5/64 loss: -0.27671605348587036
Batch 6/64 loss: -0.2839481830596924
Batch 7/64 loss: -0.2989429235458374
Batch 8/64 loss: -0.2832399606704712
Batch 9/64 loss: -0.2955548167228699
Batch 10/64 loss: -0.2904267907142639
Batch 11/64 loss: -0.29541176557540894
Batch 12/64 loss: -0.28086942434310913
Batch 13/64 loss: -0.28374767303466797
Batch 14/64 loss: -0.2858583927154541
Batch 15/64 loss: -0.29046475887298584
Batch 16/64 loss: -0.3024599850177765
Batch 17/64 loss: -0.2872040271759033
Batch 18/64 loss: -0.29120737314224243
Batch 19/64 loss: -0.2860640585422516
Batch 20/64 loss: -0.2853209972381592
Batch 21/64 loss: -0.2918534576892853
Batch 22/64 loss: -0.2934736907482147
Batch 23/64 loss: -0.2945082187652588
Batch 24/64 loss: -0.2791432738304138
Batch 25/64 loss: -0.28549155592918396
Batch 26/64 loss: -0.2983109951019287
Batch 27/64 loss: -0.29957115650177
Batch 28/64 loss: -0.2814464569091797
Batch 29/64 loss: -0.27052557468414307
Batch 30/64 loss: -0.31316494941711426
Batch 31/64 loss: -0.28715625405311584
Batch 32/64 loss: -0.2786755859851837
Batch 33/64 loss: -0.27921637892723083
Batch 34/64 loss: -0.2777726948261261
Batch 35/64 loss: -0.2915107011795044
Batch 36/64 loss: -0.29361557960510254
Batch 37/64 loss: -0.2835204601287842
Batch 38/64 loss: -0.28594857454299927
Batch 39/64 loss: -0.29217812418937683
Batch 40/64 loss: -0.2941854000091553
Batch 41/64 loss: -0.2814056873321533
Batch 42/64 loss: -0.2941947281360626
Batch 43/64 loss: -0.292167603969574
Batch 44/64 loss: -0.29680073261260986
Batch 45/64 loss: -0.28259342908859253
Batch 46/64 loss: -0.295436829328537
Batch 47/64 loss: -0.2946244478225708
Batch 48/64 loss: -0.28726208209991455
Batch 49/64 loss: -0.28639745712280273
Batch 50/64 loss: -0.2935000956058502
Batch 51/64 loss: -0.2916681170463562
Batch 52/64 loss: -0.2819182574748993
Batch 53/64 loss: -0.2983335852622986
Batch 54/64 loss: -0.30190587043762207
Batch 55/64 loss: -0.29878827929496765
Batch 56/64 loss: -0.2882881760597229
Batch 57/64 loss: -0.28371381759643555
Batch 58/64 loss: -0.30138862133026123
Batch 59/64 loss: -0.29441338777542114
Batch 60/64 loss: -0.2942478656768799
Batch 61/64 loss: -0.29184699058532715
Batch 62/64 loss: -0.2855593264102936
Batch 63/64 loss: -0.2827429175376892
Batch 64/64 loss: -0.27335959672927856
Epoch 431  Train loss: -0.28933640297721414  Val loss: -0.25879718102130694
Epoch 432
-------------------------------
Batch 1/64 loss: -0.297538697719574
Batch 2/64 loss: -0.2863128185272217
Batch 3/64 loss: -0.29590341448783875
Batch 4/64 loss: -0.286933958530426
Batch 5/64 loss: -0.2694283127784729
Batch 6/64 loss: -0.30231642723083496
Batch 7/64 loss: -0.2770991325378418
Batch 8/64 loss: -0.290547639131546
Batch 9/64 loss: -0.2997656464576721
Batch 10/64 loss: -0.28432753682136536
Batch 11/64 loss: -0.29822564125061035
Batch 12/64 loss: -0.2921034097671509
Batch 13/64 loss: -0.2951788306236267
Batch 14/64 loss: -0.28900134563446045
Batch 15/64 loss: -0.29798972606658936
Batch 16/64 loss: -0.2947838306427002
Batch 17/64 loss: -0.2923647463321686
Batch 18/64 loss: -0.29553478956222534
Batch 19/64 loss: -0.29297447204589844
Batch 20/64 loss: -0.28377512097358704
Batch 21/64 loss: -0.28057289123535156
Batch 22/64 loss: -0.30370670557022095
Batch 23/64 loss: -0.29299235343933105
Batch 24/64 loss: -0.2842376232147217
Batch 25/64 loss: -0.3056779205799103
Batch 26/64 loss: -0.2958333492279053
Batch 27/64 loss: -0.2859150469303131
Batch 28/64 loss: -0.2855709195137024
Batch 29/64 loss: -0.2998729944229126
Batch 30/64 loss: -0.2987683117389679
Batch 31/64 loss: -0.2866031527519226
Batch 32/64 loss: -0.305895060300827
Batch 33/64 loss: -0.3003506660461426
Batch 34/64 loss: -0.2809571325778961
Batch 35/64 loss: -0.2855885624885559
Batch 36/64 loss: -0.2714659571647644
Batch 37/64 loss: -0.2903904616832733
Batch 38/64 loss: -0.2767055630683899
Batch 39/64 loss: -0.2729911804199219
Batch 40/64 loss: -0.299691379070282
Batch 41/64 loss: -0.2895668148994446
Batch 42/64 loss: -0.28106045722961426
Batch 43/64 loss: -0.28929415345191956
Batch 44/64 loss: -0.2901848554611206
Batch 45/64 loss: -0.2896146774291992
Batch 46/64 loss: -0.2865873873233795
Batch 47/64 loss: -0.29033052921295166
Batch 48/64 loss: -0.2687727212905884
Batch 49/64 loss: -0.2839158773422241
Batch 50/64 loss: -0.30588147044181824
Batch 51/64 loss: -0.29359763860702515
Batch 52/64 loss: -0.29815152287483215
Batch 53/64 loss: -0.2588345408439636
Batch 54/64 loss: -0.30090728402137756
Batch 55/64 loss: -0.2851683497428894
Batch 56/64 loss: -0.29090651869773865
Batch 57/64 loss: -0.28992199897766113
Batch 58/64 loss: -0.284753680229187
Batch 59/64 loss: -0.2676200866699219
Batch 60/64 loss: -0.28977030515670776
Batch 61/64 loss: -0.29927682876586914
Batch 62/64 loss: -0.2987710237503052
Batch 63/64 loss: -0.272363543510437
Batch 64/64 loss: -0.2851932644844055
Epoch 432  Train loss: -0.2893339598880095  Val loss: -0.267653146457836
Epoch 433
-------------------------------
Batch 1/64 loss: -0.2954060733318329
Batch 2/64 loss: -0.2832820415496826
Batch 3/64 loss: -0.2934562563896179
Batch 4/64 loss: -0.2896355986595154
Batch 5/64 loss: -0.2799575924873352
Batch 6/64 loss: -0.2948973476886749
Batch 7/64 loss: -0.29840749502182007
Batch 8/64 loss: -0.2725861072540283
Batch 9/64 loss: -0.2957252264022827
Batch 10/64 loss: -0.2829420566558838
Batch 11/64 loss: -0.2980303168296814
Batch 12/64 loss: -0.28267350792884827
Batch 13/64 loss: -0.28259798884391785
Batch 14/64 loss: -0.2941936254501343
Batch 15/64 loss: -0.3009853959083557
Batch 16/64 loss: -0.28493061661720276
Batch 17/64 loss: -0.293931245803833
Batch 18/64 loss: -0.28394532203674316
Batch 19/64 loss: -0.2892005145549774
Batch 20/64 loss: -0.288324773311615
Batch 21/64 loss: -0.2897140383720398
Batch 22/64 loss: -0.28236478567123413
Batch 23/64 loss: -0.2883921265602112
Batch 24/64 loss: -0.2975037693977356
Batch 25/64 loss: -0.27100104093551636
Batch 26/64 loss: -0.2778264284133911
Batch 27/64 loss: -0.2946740388870239
Batch 28/64 loss: -0.3001517057418823
Batch 29/64 loss: -0.29138144850730896
Batch 30/64 loss: -0.28273940086364746
Batch 31/64 loss: -0.2784828543663025
Batch 32/64 loss: -0.2924381494522095
Batch 33/64 loss: -0.29502665996551514
Batch 34/64 loss: -0.28619539737701416
Batch 35/64 loss: -0.28610891103744507
Batch 36/64 loss: -0.2917172312736511
Batch 37/64 loss: -0.2913839817047119
Batch 38/64 loss: -0.3052077889442444
Batch 39/64 loss: -0.2893674671649933
Batch 40/64 loss: -0.28729110956192017
Batch 41/64 loss: -0.298238068819046
Batch 42/64 loss: -0.3035547733306885
Batch 43/64 loss: -0.28760987520217896
Batch 44/64 loss: -0.28679779171943665
Batch 45/64 loss: -0.2884244918823242
Batch 46/64 loss: -0.2996463477611542
Batch 47/64 loss: -0.2910642921924591
Batch 48/64 loss: -0.29884016513824463
Batch 49/64 loss: -0.29187068343162537
Batch 50/64 loss: -0.30615872144699097
Batch 51/64 loss: -0.29279616475105286
Batch 52/64 loss: -0.27982622385025024
Batch 53/64 loss: -0.2931065261363983
Batch 54/64 loss: -0.28685715794563293
Batch 55/64 loss: -0.2835506796836853
Batch 56/64 loss: -0.3009313941001892
Batch 57/64 loss: -0.2949315905570984
Batch 58/64 loss: -0.29098016023635864
Batch 59/64 loss: -0.2892504930496216
Batch 60/64 loss: -0.3045263886451721
Batch 61/64 loss: -0.2996498942375183
Batch 62/64 loss: -0.2811521291732788
Batch 63/64 loss: -0.2867773771286011
Batch 64/64 loss: -0.29446548223495483
Epoch 433  Train loss: -0.2905328304159875  Val loss: -0.26610622270820067
Epoch 434
-------------------------------
Batch 1/64 loss: -0.29291218519210815
Batch 2/64 loss: -0.2994043529033661
Batch 3/64 loss: -0.2938050627708435
Batch 4/64 loss: -0.2921616733074188
Batch 5/64 loss: -0.29461297392845154
Batch 6/64 loss: -0.2876715064048767
Batch 7/64 loss: -0.29133594036102295
Batch 8/64 loss: -0.2825574278831482
Batch 9/64 loss: -0.2800723910331726
Batch 10/64 loss: -0.29061126708984375
Batch 11/64 loss: -0.29084616899490356
Batch 12/64 loss: -0.2842540144920349
Batch 13/64 loss: -0.2854362726211548
Batch 14/64 loss: -0.29423415660858154
Batch 15/64 loss: -0.2963095009326935
Batch 16/64 loss: -0.2832139730453491
Batch 17/64 loss: -0.3067176938056946
Batch 18/64 loss: -0.298432320356369
Batch 19/64 loss: -0.27608194947242737
Batch 20/64 loss: -0.2978447675704956
Batch 21/64 loss: -0.2812199592590332
Batch 22/64 loss: -0.29534420371055603
Batch 23/64 loss: -0.2884652614593506
Batch 24/64 loss: -0.2979522943496704
Batch 25/64 loss: -0.29288196563720703
Batch 26/64 loss: -0.28537872433662415
Batch 27/64 loss: -0.2860715091228485
Batch 28/64 loss: -0.2998831272125244
Batch 29/64 loss: -0.28902900218963623
Batch 30/64 loss: -0.2991933524608612
Batch 31/64 loss: -0.28553664684295654
Batch 32/64 loss: -0.2931869328022003
Batch 33/64 loss: -0.2968740463256836
Batch 34/64 loss: -0.2922871708869934
Batch 35/64 loss: -0.30064553022384644
Batch 36/64 loss: -0.2918936014175415
Batch 37/64 loss: -0.28901785612106323
Batch 38/64 loss: -0.30901268124580383
Batch 39/64 loss: -0.30574744939804077
Batch 40/64 loss: -0.2984095811843872
Batch 41/64 loss: -0.2919788658618927
Batch 42/64 loss: -0.26710015535354614
Batch 43/64 loss: -0.2982786297798157
Batch 44/64 loss: -0.27912646532058716
Batch 45/64 loss: -0.3032943606376648
Batch 46/64 loss: -0.2940196990966797
Batch 47/64 loss: -0.294374942779541
Batch 48/64 loss: -0.2843567132949829
Batch 49/64 loss: -0.29189786314964294
Batch 50/64 loss: -0.2832564413547516
Batch 51/64 loss: -0.28372931480407715
Batch 52/64 loss: -0.30075979232788086
Batch 53/64 loss: -0.2845684885978699
Batch 54/64 loss: -0.30628442764282227
Batch 55/64 loss: -0.2994289994239807
Batch 56/64 loss: -0.302430123090744
Batch 57/64 loss: -0.3059930205345154
Batch 58/64 loss: -0.29500019550323486
Batch 59/64 loss: -0.3006035089492798
Batch 60/64 loss: -0.3044450581073761
Batch 61/64 loss: -0.30245769023895264
Batch 62/64 loss: -0.29176998138427734
Batch 63/64 loss: -0.2910679578781128
Batch 64/64 loss: -0.2862296402454376
Epoch 434  Train loss: -0.29282261051383673  Val loss: -0.26781265801170845
Epoch 435
-------------------------------
Batch 1/64 loss: -0.29872727394104004
Batch 2/64 loss: -0.2823001444339752
Batch 3/64 loss: -0.294262170791626
Batch 4/64 loss: -0.29377812147140503
Batch 5/64 loss: -0.2999786138534546
Batch 6/64 loss: -0.28242021799087524
Batch 7/64 loss: -0.2914312481880188
Batch 8/64 loss: -0.3047370910644531
Batch 9/64 loss: -0.2960453927516937
Batch 10/64 loss: -0.2951265573501587
Batch 11/64 loss: -0.29625391960144043
Batch 12/64 loss: -0.29990002512931824
Batch 13/64 loss: -0.2737041413784027
Batch 14/64 loss: -0.2946382761001587
Batch 15/64 loss: -0.29681649804115295
Batch 16/64 loss: -0.29855597019195557
Batch 17/64 loss: -0.2923675775527954
Batch 18/64 loss: -0.29813021421432495
Batch 19/64 loss: -0.3061025142669678
Batch 20/64 loss: -0.2918700575828552
Batch 21/64 loss: -0.3015381097793579
Batch 22/64 loss: -0.28256550431251526
Batch 23/64 loss: -0.29689478874206543
Batch 24/64 loss: -0.29730290174484253
Batch 25/64 loss: -0.2999221086502075
Batch 26/64 loss: -0.2976740300655365
Batch 27/64 loss: -0.303555965423584
Batch 28/64 loss: -0.28755050897598267
Batch 29/64 loss: -0.30201178789138794
Batch 30/64 loss: -0.3047662675380707
Batch 31/64 loss: -0.2927102744579315
Batch 32/64 loss: -0.27940842509269714
Batch 33/64 loss: -0.29394981265068054
Batch 34/64 loss: -0.293885201215744
Batch 35/64 loss: -0.28685736656188965
Batch 36/64 loss: -0.296326220035553
Batch 37/64 loss: -0.2972995340824127
Batch 38/64 loss: -0.2945064306259155
Batch 39/64 loss: -0.2923640310764313
Batch 40/64 loss: -0.30068716406822205
Batch 41/64 loss: -0.2862436771392822
Batch 42/64 loss: -0.28342902660369873
Batch 43/64 loss: -0.29034245014190674
Batch 44/64 loss: -0.2921984791755676
Batch 45/64 loss: -0.2869340181350708
Batch 46/64 loss: -0.2958040237426758
Batch 47/64 loss: -0.29685211181640625
Batch 48/64 loss: -0.2971395254135132
Batch 49/64 loss: -0.2940903902053833
Batch 50/64 loss: -0.27727633714675903
Batch 51/64 loss: -0.29091882705688477
Batch 52/64 loss: -0.2912657856941223
Batch 53/64 loss: -0.2893119752407074
Batch 54/64 loss: -0.29355019330978394
Batch 55/64 loss: -0.29041895270347595
Batch 56/64 loss: -0.2833668291568756
Batch 57/64 loss: -0.29148656129837036
Batch 58/64 loss: -0.29008781909942627
Batch 59/64 loss: -0.28945380449295044
Batch 60/64 loss: -0.2867770195007324
Batch 61/64 loss: -0.2934272289276123
Batch 62/64 loss: -0.2827483117580414
Batch 63/64 loss: -0.29544544219970703
Batch 64/64 loss: -0.2961759567260742
Epoch 435  Train loss: -0.29301369751200956  Val loss: -0.26335148569644523
Epoch 436
-------------------------------
Batch 1/64 loss: -0.30008918046951294
Batch 2/64 loss: -0.29625165462493896
Batch 3/64 loss: -0.2950466275215149
Batch 4/64 loss: -0.293191134929657
Batch 5/64 loss: -0.28753477334976196
Batch 6/64 loss: -0.28170955181121826
Batch 7/64 loss: -0.3000805974006653
Batch 8/64 loss: -0.29221096634864807
Batch 9/64 loss: -0.2677990198135376
Batch 10/64 loss: -0.2882016897201538
Batch 11/64 loss: -0.2975667715072632
Batch 12/64 loss: -0.2926139235496521
Batch 13/64 loss: -0.29655542969703674
Batch 14/64 loss: -0.2971542179584503
Batch 15/64 loss: -0.286487340927124
Batch 16/64 loss: -0.2850136160850525
Batch 17/64 loss: -0.2921515703201294
Batch 18/64 loss: -0.29443734884262085
Batch 19/64 loss: -0.3002889156341553
Batch 20/64 loss: -0.2970525324344635
Batch 21/64 loss: -0.29643383622169495
Batch 22/64 loss: -0.2919517159461975
Batch 23/64 loss: -0.30051136016845703
Batch 24/64 loss: -0.2974851429462433
Batch 25/64 loss: -0.3049207925796509
Batch 26/64 loss: -0.2878882884979248
Batch 27/64 loss: -0.3005884289741516
Batch 28/64 loss: -0.28028982877731323
Batch 29/64 loss: -0.29890716075897217
Batch 30/64 loss: -0.30164963006973267
Batch 31/64 loss: -0.30217814445495605
Batch 32/64 loss: -0.2872672975063324
Batch 33/64 loss: -0.27657389640808105
Batch 34/64 loss: -0.28648051619529724
Batch 35/64 loss: -0.29779061675071716
Batch 36/64 loss: -0.2904520630836487
Batch 37/64 loss: -0.29474690556526184
Batch 38/64 loss: -0.28720059990882874
Batch 39/64 loss: -0.2815437912940979
Batch 40/64 loss: -0.2994093596935272
Batch 41/64 loss: -0.2934612035751343
Batch 42/64 loss: -0.2817511558532715
Batch 43/64 loss: -0.2953857183456421
Batch 44/64 loss: -0.2924821078777313
Batch 45/64 loss: -0.28598928451538086
Batch 46/64 loss: -0.2818335294723511
Batch 47/64 loss: -0.3074679970741272
Batch 48/64 loss: -0.283588707447052
Batch 49/64 loss: -0.28579777479171753
Batch 50/64 loss: -0.30115872621536255
Batch 51/64 loss: -0.2969224750995636
Batch 52/64 loss: -0.28896501660346985
Batch 53/64 loss: -0.2886424660682678
Batch 54/64 loss: -0.301505982875824
Batch 55/64 loss: -0.29280412197113037
Batch 56/64 loss: -0.2973178029060364
Batch 57/64 loss: -0.2954941391944885
Batch 58/64 loss: -0.29349571466445923
Batch 59/64 loss: -0.2908781170845032
Batch 60/64 loss: -0.286565899848938
Batch 61/64 loss: -0.2985170781612396
Batch 62/64 loss: -0.28530266880989075
Batch 63/64 loss: -0.2948126792907715
Batch 64/64 loss: -0.302886962890625
Epoch 436  Train loss: -0.2925962639789955  Val loss: -0.26629112010559264
Epoch 437
-------------------------------
Batch 1/64 loss: -0.3090748190879822
Batch 2/64 loss: -0.2855343222618103
Batch 3/64 loss: -0.29705870151519775
Batch 4/64 loss: -0.29427990317344666
Batch 5/64 loss: -0.3050185739994049
Batch 6/64 loss: -0.2767871022224426
Batch 7/64 loss: -0.2966233491897583
Batch 8/64 loss: -0.305114209651947
Batch 9/64 loss: -0.2919652462005615
Batch 10/64 loss: -0.2895803153514862
Batch 11/64 loss: -0.29673242568969727
Batch 12/64 loss: -0.28874826431274414
Batch 13/64 loss: -0.286207377910614
Batch 14/64 loss: -0.28369516134262085
Batch 15/64 loss: -0.2837236523628235
Batch 16/64 loss: -0.29719221591949463
Batch 17/64 loss: -0.2905538082122803
Batch 18/64 loss: -0.28352293372154236
Batch 19/64 loss: -0.2945426404476166
Batch 20/64 loss: -0.2912898361682892
Batch 21/64 loss: -0.28757303953170776
Batch 22/64 loss: -0.29833537340164185
Batch 23/64 loss: -0.2945786118507385
Batch 24/64 loss: -0.2922290861606598
Batch 25/64 loss: -0.2961477041244507
Batch 26/64 loss: -0.29802748560905457
Batch 27/64 loss: -0.30251792073249817
Batch 28/64 loss: -0.2922721207141876
Batch 29/64 loss: -0.29155483841896057
Batch 30/64 loss: -0.2968933582305908
Batch 31/64 loss: -0.2958405017852783
Batch 32/64 loss: -0.29934531450271606
Batch 33/64 loss: -0.3064977526664734
Batch 34/64 loss: -0.28933194279670715
Batch 35/64 loss: -0.2778768539428711
Batch 36/64 loss: -0.279421329498291
Batch 37/64 loss: -0.2938786745071411
Batch 38/64 loss: -0.2989543080329895
Batch 39/64 loss: -0.2885322868824005
Batch 40/64 loss: -0.2829858064651489
Batch 41/64 loss: -0.303821861743927
Batch 42/64 loss: -0.2985571026802063
Batch 43/64 loss: -0.2858257293701172
Batch 44/64 loss: -0.29803338646888733
Batch 45/64 loss: -0.2941807508468628
Batch 46/64 loss: -0.2924060523509979
Batch 47/64 loss: -0.2892382740974426
Batch 48/64 loss: -0.30362600088119507
Batch 49/64 loss: -0.29106467962265015
Batch 50/64 loss: -0.302761048078537
Batch 51/64 loss: -0.29941725730895996
Batch 52/64 loss: -0.2988426983356476
Batch 53/64 loss: -0.2966381311416626
Batch 54/64 loss: -0.2882564067840576
Batch 55/64 loss: -0.2892034947872162
Batch 56/64 loss: -0.3024664521217346
Batch 57/64 loss: -0.2838488817214966
Batch 58/64 loss: -0.29696157574653625
Batch 59/64 loss: -0.30017906427383423
Batch 60/64 loss: -0.2890576422214508
Batch 61/64 loss: -0.3057800531387329
Batch 62/64 loss: -0.3002094626426697
Batch 63/64 loss: -0.30015355348587036
Batch 64/64 loss: -0.2915096879005432
Epoch 437  Train loss: -0.29394825089211557  Val loss: -0.27525312875963975
Epoch 438
-------------------------------
Batch 1/64 loss: -0.29526302218437195
Batch 2/64 loss: -0.31235647201538086
Batch 3/64 loss: -0.30314844846725464
Batch 4/64 loss: -0.2779043912887573
Batch 5/64 loss: -0.29253143072128296
Batch 6/64 loss: -0.2780575752258301
Batch 7/64 loss: -0.29461735486984253
Batch 8/64 loss: -0.2813553512096405
Batch 9/64 loss: -0.28801101446151733
Batch 10/64 loss: -0.2965879440307617
Batch 11/64 loss: -0.3069010376930237
Batch 12/64 loss: -0.3127315938472748
Batch 13/64 loss: -0.2872678339481354
Batch 14/64 loss: -0.29128021001815796
Batch 15/64 loss: -0.29243630170822144
Batch 16/64 loss: -0.2838932275772095
Batch 17/64 loss: -0.29039597511291504
Batch 18/64 loss: -0.2832767963409424
Batch 19/64 loss: -0.29798534512519836
Batch 20/64 loss: -0.28614485263824463
Batch 21/64 loss: -0.29585346579551697
Batch 22/64 loss: -0.2951515316963196
Batch 23/64 loss: -0.2943236827850342
Batch 24/64 loss: -0.2945433557033539
Batch 25/64 loss: -0.2875019311904907
Batch 26/64 loss: -0.2978687882423401
Batch 27/64 loss: -0.2991427779197693
Batch 28/64 loss: -0.2909305691719055
Batch 29/64 loss: -0.2981564998626709
Batch 30/64 loss: -0.27240419387817383
Batch 31/64 loss: -0.2974407374858856
Batch 32/64 loss: -0.2696530818939209
Batch 33/64 loss: -0.2941782474517822
Batch 34/64 loss: -0.291400671005249
Batch 35/64 loss: -0.28829824924468994
Batch 36/64 loss: -0.2813590466976166
Batch 37/64 loss: -0.2984214127063751
Batch 38/64 loss: -0.29473474621772766
Batch 39/64 loss: -0.29579973220825195
Batch 40/64 loss: -0.2959906756877899
Batch 41/64 loss: -0.29756221175193787
Batch 42/64 loss: -0.2926054000854492
Batch 43/64 loss: -0.2886715531349182
Batch 44/64 loss: -0.2753570079803467
Batch 45/64 loss: -0.29330945014953613
Batch 46/64 loss: -0.27395471930503845
Batch 47/64 loss: -0.26827841997146606
Batch 48/64 loss: -0.27111661434173584
Batch 49/64 loss: -0.2833973169326782
Batch 50/64 loss: -0.2958824634552002
Batch 51/64 loss: -0.2827272117137909
Batch 52/64 loss: -0.2798348069190979
Batch 53/64 loss: -0.29213032126426697
Batch 54/64 loss: -0.2735154628753662
Batch 55/64 loss: -0.29334649443626404
Batch 56/64 loss: -0.29580408334732056
Batch 57/64 loss: -0.2998759150505066
Batch 58/64 loss: -0.284956693649292
Batch 59/64 loss: -0.2882451117038727
Batch 60/64 loss: -0.28932464122772217
Batch 61/64 loss: -0.287240207195282
Batch 62/64 loss: -0.2960379123687744
Batch 63/64 loss: -0.28996482491493225
Batch 64/64 loss: -0.27482905983924866
Epoch 438  Train loss: -0.2899538857095382  Val loss: -0.26174787680308026
Epoch 439
-------------------------------
Batch 1/64 loss: -0.289459228515625
Batch 2/64 loss: -0.28514912724494934
Batch 3/64 loss: -0.2887685298919678
Batch 4/64 loss: -0.29130154848098755
Batch 5/64 loss: -0.29009711742401123
Batch 6/64 loss: -0.2799166142940521
Batch 7/64 loss: -0.2897220849990845
Batch 8/64 loss: -0.29387909173965454
Batch 9/64 loss: -0.2785555422306061
Batch 10/64 loss: -0.2760806083679199
Batch 11/64 loss: -0.30049943923950195
Batch 12/64 loss: -0.2893233001232147
Batch 13/64 loss: -0.27565306425094604
Batch 14/64 loss: -0.2947724163532257
Batch 15/64 loss: -0.2941736876964569
Batch 16/64 loss: -0.29872000217437744
Batch 17/64 loss: -0.2833360731601715
Batch 18/64 loss: -0.2913702428340912
Batch 19/64 loss: -0.2949264645576477
Batch 20/64 loss: -0.2811776399612427
Batch 21/64 loss: -0.2955678105354309
Batch 22/64 loss: -0.2893296480178833
Batch 23/64 loss: -0.2888087332248688
Batch 24/64 loss: -0.2963021993637085
Batch 25/64 loss: -0.29967913031578064
Batch 26/64 loss: -0.2775435447692871
Batch 27/64 loss: -0.29890620708465576
Batch 28/64 loss: -0.2905651926994324
Batch 29/64 loss: -0.3006443977355957
Batch 30/64 loss: -0.30286502838134766
Batch 31/64 loss: -0.29543501138687134
Batch 32/64 loss: -0.28990986943244934
Batch 33/64 loss: -0.2967016100883484
Batch 34/64 loss: -0.286445677280426
Batch 35/64 loss: -0.2937129735946655
Batch 36/64 loss: -0.26973873376846313
Batch 37/64 loss: -0.27317526936531067
Batch 38/64 loss: -0.27557504177093506
Batch 39/64 loss: -0.2861471176147461
Batch 40/64 loss: -0.2892717123031616
Batch 41/64 loss: -0.2807033360004425
Batch 42/64 loss: -0.28144213557243347
Batch 43/64 loss: -0.2933972477912903
Batch 44/64 loss: -0.2927611172199249
Batch 45/64 loss: -0.29695987701416016
Batch 46/64 loss: -0.26665985584259033
Batch 47/64 loss: -0.2889699935913086
Batch 48/64 loss: -0.2794983983039856
Batch 49/64 loss: -0.27576515078544617
Batch 50/64 loss: -0.27841001749038696
Batch 51/64 loss: -0.28846773505210876
Batch 52/64 loss: -0.2752636671066284
Batch 53/64 loss: -0.29098162055015564
Batch 54/64 loss: -0.2896789610385895
Batch 55/64 loss: -0.2925436198711395
Batch 56/64 loss: -0.27804213762283325
Batch 57/64 loss: -0.2831401824951172
Batch 58/64 loss: -0.2967662811279297
Batch 59/64 loss: -0.2912865877151489
Batch 60/64 loss: -0.2738305628299713
Batch 61/64 loss: -0.29227209091186523
Batch 62/64 loss: -0.29500675201416016
Batch 63/64 loss: -0.2850170433521271
Batch 64/64 loss: -0.28200221061706543
Epoch 439  Train loss: -0.2877109452789905  Val loss: -0.2741145217131913
Epoch 440
-------------------------------
Batch 1/64 loss: -0.29578036069869995
Batch 2/64 loss: -0.28834664821624756
Batch 3/64 loss: -0.28950273990631104
Batch 4/64 loss: -0.28975385427474976
Batch 5/64 loss: -0.30005931854248047
Batch 6/64 loss: -0.2951588034629822
Batch 7/64 loss: -0.2922142744064331
Batch 8/64 loss: -0.2882395386695862
Batch 9/64 loss: -0.29503899812698364
Batch 10/64 loss: -0.284481018781662
Batch 11/64 loss: -0.2967020273208618
Batch 12/64 loss: -0.2960873246192932
Batch 13/64 loss: -0.29435575008392334
Batch 14/64 loss: -0.2923740744590759
Batch 15/64 loss: -0.27294543385505676
Batch 16/64 loss: -0.2850637435913086
Batch 17/64 loss: -0.2972720265388489
Batch 18/64 loss: -0.288998007774353
Batch 19/64 loss: -0.29126274585723877
Batch 20/64 loss: -0.2989414930343628
Batch 21/64 loss: -0.29652607440948486
Batch 22/64 loss: -0.2956605553627014
Batch 23/64 loss: -0.2848049998283386
Batch 24/64 loss: -0.29679983854293823
Batch 25/64 loss: -0.29344186186790466
Batch 26/64 loss: -0.28341811895370483
Batch 27/64 loss: -0.2877331078052521
Batch 28/64 loss: -0.2939281761646271
Batch 29/64 loss: -0.2979544997215271
Batch 30/64 loss: -0.28628814220428467
Batch 31/64 loss: -0.2980935275554657
Batch 32/64 loss: -0.2850770354270935
Batch 33/64 loss: -0.28366732597351074
Batch 34/64 loss: -0.2906484603881836
Batch 35/64 loss: -0.2860652506351471
Batch 36/64 loss: -0.29053962230682373
Batch 37/64 loss: -0.2882201075553894
Batch 38/64 loss: -0.2980669140815735
Batch 39/64 loss: -0.2950800359249115
Batch 40/64 loss: -0.2761552631855011
Batch 41/64 loss: -0.29541608691215515
Batch 42/64 loss: -0.28193968534469604
Batch 43/64 loss: -0.2764606475830078
Batch 44/64 loss: -0.2866933047771454
Batch 45/64 loss: -0.2900463342666626
Batch 46/64 loss: -0.2901163101196289
Batch 47/64 loss: -0.28837865591049194
Batch 48/64 loss: -0.27100080251693726
Batch 49/64 loss: -0.2910839319229126
Batch 50/64 loss: -0.30277588963508606
Batch 51/64 loss: -0.2832848131656647
Batch 52/64 loss: -0.29880064725875854
Batch 53/64 loss: -0.29063570499420166
Batch 54/64 loss: -0.2969051003456116
Batch 55/64 loss: -0.3021782338619232
Batch 56/64 loss: -0.28373321890830994
Batch 57/64 loss: -0.30148977041244507
Batch 58/64 loss: -0.3014863133430481
Batch 59/64 loss: -0.30228766798973083
Batch 60/64 loss: -0.2851625382900238
Batch 61/64 loss: -0.2790471911430359
Batch 62/64 loss: -0.3042992949485779
Batch 63/64 loss: -0.2962953746318817
Batch 64/64 loss: -0.2862914800643921
Epoch 440  Train loss: -0.29105856044619693  Val loss: -0.2666281146282183
Epoch 441
-------------------------------
Batch 1/64 loss: -0.2847343683242798
Batch 2/64 loss: -0.29196804761886597
Batch 3/64 loss: -0.2859273850917816
Batch 4/64 loss: -0.29239755868911743
Batch 5/64 loss: -0.29468297958374023
Batch 6/64 loss: -0.2884608507156372
Batch 7/64 loss: -0.3070880174636841
Batch 8/64 loss: -0.29511067271232605
Batch 9/64 loss: -0.28769397735595703
Batch 10/64 loss: -0.30323946475982666
Batch 11/64 loss: -0.30472421646118164
Batch 12/64 loss: -0.2899761497974396
Batch 13/64 loss: -0.30119258165359497
Batch 14/64 loss: -0.29991525411605835
Batch 15/64 loss: -0.2843410074710846
Batch 16/64 loss: -0.29442572593688965
Batch 17/64 loss: -0.2833484709262848
Batch 18/64 loss: -0.2908528745174408
Batch 19/64 loss: -0.30954310297966003
Batch 20/64 loss: -0.2956819534301758
Batch 21/64 loss: -0.28292420506477356
Batch 22/64 loss: -0.2952490746974945
Batch 23/64 loss: -0.28376173973083496
Batch 24/64 loss: -0.28411775827407837
Batch 25/64 loss: -0.2928488850593567
Batch 26/64 loss: -0.30106624960899353
Batch 27/64 loss: -0.27384495735168457
Batch 28/64 loss: -0.2877676486968994
Batch 29/64 loss: -0.29050037264823914
Batch 30/64 loss: -0.2960194945335388
Batch 31/64 loss: -0.2784588038921356
Batch 32/64 loss: -0.2998543977737427
Batch 33/64 loss: -0.282426655292511
Batch 34/64 loss: -0.2949070334434509
Batch 35/64 loss: -0.29230546951293945
Batch 36/64 loss: -0.29166024923324585
Batch 37/64 loss: -0.27873873710632324
Batch 38/64 loss: -0.2866052985191345
Batch 39/64 loss: -0.29344919323921204
Batch 40/64 loss: -0.28917038440704346
Batch 41/64 loss: -0.3062017261981964
Batch 42/64 loss: -0.3084442913532257
Batch 43/64 loss: -0.29204732179641724
Batch 44/64 loss: -0.2966300845146179
Batch 45/64 loss: -0.28715282678604126
Batch 46/64 loss: -0.283630907535553
Batch 47/64 loss: -0.27553892135620117
Batch 48/64 loss: -0.2851906418800354
Batch 49/64 loss: -0.28057926893234253
Batch 50/64 loss: -0.28689897060394287
Batch 51/64 loss: -0.29744628071784973
Batch 52/64 loss: -0.28505009412765503
Batch 53/64 loss: -0.3010352551937103
Batch 54/64 loss: -0.28502970933914185
Batch 55/64 loss: -0.29199379682540894
Batch 56/64 loss: -0.2989407777786255
Batch 57/64 loss: -0.29824936389923096
Batch 58/64 loss: -0.291072279214859
Batch 59/64 loss: -0.2909819483757019
Batch 60/64 loss: -0.28197455406188965
Batch 61/64 loss: -0.28568920493125916
Batch 62/64 loss: -0.2925671339035034
Batch 63/64 loss: -0.27938711643218994
Batch 64/64 loss: -0.29586130380630493
Epoch 441  Train loss: -0.29120956425573313  Val loss: -0.27104724006554515
Epoch 442
-------------------------------
Batch 1/64 loss: -0.30554550886154175
Batch 2/64 loss: -0.2942731976509094
Batch 3/64 loss: -0.2842727601528168
Batch 4/64 loss: -0.28041568398475647
Batch 5/64 loss: -0.30737781524658203
Batch 6/64 loss: -0.2938288748264313
Batch 7/64 loss: -0.28473517298698425
Batch 8/64 loss: -0.30414170026779175
Batch 9/64 loss: -0.2956700921058655
Batch 10/64 loss: -0.28185325860977173
Batch 11/64 loss: -0.28186312317848206
Batch 12/64 loss: -0.2791517972946167
Batch 13/64 loss: -0.2830416262149811
Batch 14/64 loss: -0.28300052881240845
Batch 15/64 loss: -0.27028658986091614
Batch 16/64 loss: -0.29325252771377563
Batch 17/64 loss: -0.2904282212257385
Batch 18/64 loss: -0.2997344136238098
Batch 19/64 loss: -0.2868735194206238
Batch 20/64 loss: -0.2764602303504944
Batch 21/64 loss: -0.28901374340057373
Batch 22/64 loss: -0.2949053943157196
Batch 23/64 loss: -0.2960244417190552
Batch 24/64 loss: -0.29820701479911804
Batch 25/64 loss: -0.29462796449661255
Batch 26/64 loss: -0.29707539081573486
Batch 27/64 loss: -0.2956501841545105
Batch 28/64 loss: -0.27806535363197327
Batch 29/64 loss: -0.3017125725746155
Batch 30/64 loss: -0.2935040295124054
Batch 31/64 loss: -0.2866785526275635
Batch 32/64 loss: -0.2828226685523987
Batch 33/64 loss: -0.2930731773376465
Batch 34/64 loss: -0.29964348673820496
Batch 35/64 loss: -0.2801746129989624
Batch 36/64 loss: -0.29048481583595276
Batch 37/64 loss: -0.2863956093788147
Batch 38/64 loss: -0.2920512855052948
Batch 39/64 loss: -0.29371178150177
Batch 40/64 loss: -0.269733190536499
Batch 41/64 loss: -0.2925834655761719
Batch 42/64 loss: -0.29198452830314636
Batch 43/64 loss: -0.28626924753189087
Batch 44/64 loss: -0.29613685607910156
Batch 45/64 loss: -0.27350932359695435
Batch 46/64 loss: -0.29221856594085693
Batch 47/64 loss: -0.28879666328430176
Batch 48/64 loss: -0.28084972500801086
Batch 49/64 loss: -0.2712745666503906
Batch 50/64 loss: -0.29483863711357117
Batch 51/64 loss: -0.30064645409584045
Batch 52/64 loss: -0.289654403924942
Batch 53/64 loss: -0.2879171073436737
Batch 54/64 loss: -0.30175718665122986
Batch 55/64 loss: -0.2970236539840698
Batch 56/64 loss: -0.3103749752044678
Batch 57/64 loss: -0.28951627016067505
Batch 58/64 loss: -0.30332690477371216
Batch 59/64 loss: -0.3040863871574402
Batch 60/64 loss: -0.2940210700035095
Batch 61/64 loss: -0.28630104660987854
Batch 62/64 loss: -0.2779850959777832
Batch 63/64 loss: -0.2907337546348572
Batch 64/64 loss: -0.3009423613548279
Epoch 442  Train loss: -0.2904670521324756  Val loss: -0.2764723959452508
Epoch 443
-------------------------------
Batch 1/64 loss: -0.2879067063331604
Batch 2/64 loss: -0.2944408059120178
Batch 3/64 loss: -0.28565317392349243
Batch 4/64 loss: -0.29935601353645325
Batch 5/64 loss: -0.3037213683128357
Batch 6/64 loss: -0.3072003424167633
Batch 7/64 loss: -0.28536203503608704
Batch 8/64 loss: -0.2962357997894287
Batch 9/64 loss: -0.29145437479019165
Batch 10/64 loss: -0.2971673309803009
Batch 11/64 loss: -0.2954024076461792
Batch 12/64 loss: -0.28116297721862793
Batch 13/64 loss: -0.29871106147766113
Batch 14/64 loss: -0.2836993634700775
Batch 15/64 loss: -0.2961614727973938
Batch 16/64 loss: -0.29533788561820984
Batch 17/64 loss: -0.29039937257766724
Batch 18/64 loss: -0.30891716480255127
Batch 19/64 loss: -0.29105570912361145
Batch 20/64 loss: -0.2966405749320984
Batch 21/64 loss: -0.2943764925003052
Batch 22/64 loss: -0.28348487615585327
Batch 23/64 loss: -0.291708767414093
Batch 24/64 loss: -0.2941831946372986
Batch 25/64 loss: -0.2929368317127228
Batch 26/64 loss: -0.29691463708877563
Batch 27/64 loss: -0.28976261615753174
Batch 28/64 loss: -0.3001282215118408
Batch 29/64 loss: -0.30220478773117065
Batch 30/64 loss: -0.3006635010242462
Batch 31/64 loss: -0.2743135094642639
Batch 32/64 loss: -0.28915897011756897
Batch 33/64 loss: -0.2939608693122864
Batch 34/64 loss: -0.2984861135482788
Batch 35/64 loss: -0.2989996671676636
Batch 36/64 loss: -0.29566097259521484
Batch 37/64 loss: -0.2981891632080078
Batch 38/64 loss: -0.30397897958755493
Batch 39/64 loss: -0.28210538625717163
Batch 40/64 loss: -0.3060753345489502
Batch 41/64 loss: -0.28283461928367615
Batch 42/64 loss: -0.2981642186641693
Batch 43/64 loss: -0.2926552891731262
Batch 44/64 loss: -0.30258670449256897
Batch 45/64 loss: -0.2959325909614563
Batch 46/64 loss: -0.29129618406295776
Batch 47/64 loss: -0.293214350938797
Batch 48/64 loss: -0.3088829517364502
Batch 49/64 loss: -0.297829806804657
Batch 50/64 loss: -0.294483482837677
Batch 51/64 loss: -0.27565228939056396
Batch 52/64 loss: -0.29691851139068604
Batch 53/64 loss: -0.2934432029724121
Batch 54/64 loss: -0.2994094491004944
Batch 55/64 loss: -0.3011428117752075
Batch 56/64 loss: -0.29712092876434326
Batch 57/64 loss: -0.304423063993454
Batch 58/64 loss: -0.29129254817962646
Batch 59/64 loss: -0.2801929712295532
Batch 60/64 loss: -0.28986886143684387
Batch 61/64 loss: -0.299546480178833
Batch 62/64 loss: -0.26107358932495117
Batch 63/64 loss: -0.27841001749038696
Batch 64/64 loss: -0.2899479866027832
Epoch 443  Train loss: -0.2936017999462053  Val loss: -0.261826665950395
Epoch 444
-------------------------------
Batch 1/64 loss: -0.29025980830192566
Batch 2/64 loss: -0.284645140171051
Batch 3/64 loss: -0.2957002520561218
Batch 4/64 loss: -0.2945065498352051
Batch 5/64 loss: -0.29191169142723083
Batch 6/64 loss: -0.2920185923576355
Batch 7/64 loss: -0.2887183725833893
Batch 8/64 loss: -0.290285587310791
Batch 9/64 loss: -0.3062330484390259
Batch 10/64 loss: -0.3002331554889679
Batch 11/64 loss: -0.2810313105583191
Batch 12/64 loss: -0.2831876873970032
Batch 13/64 loss: -0.3059416115283966
Batch 14/64 loss: -0.27883315086364746
Batch 15/64 loss: -0.28944188356399536
Batch 16/64 loss: -0.28505218029022217
Batch 17/64 loss: -0.273202121257782
Batch 18/64 loss: -0.30639147758483887
Batch 19/64 loss: -0.28888723254203796
Batch 20/64 loss: -0.2894343137741089
Batch 21/64 loss: -0.287697970867157
Batch 22/64 loss: -0.293443500995636
Batch 23/64 loss: -0.2954411804676056
Batch 24/64 loss: -0.28782010078430176
Batch 25/64 loss: -0.2817961573600769
Batch 26/64 loss: -0.28235769271850586
Batch 27/64 loss: -0.2975921630859375
Batch 28/64 loss: -0.29749876260757446
Batch 29/64 loss: -0.30574852228164673
Batch 30/64 loss: -0.28538644313812256
Batch 31/64 loss: -0.2752641439437866
Batch 32/64 loss: -0.29354327917099
Batch 33/64 loss: -0.2998288869857788
Batch 34/64 loss: -0.2994310259819031
Batch 35/64 loss: -0.2898690402507782
Batch 36/64 loss: -0.290244996547699
Batch 37/64 loss: -0.27871811389923096
Batch 38/64 loss: -0.2908901572227478
Batch 39/64 loss: -0.2773659825325012
Batch 40/64 loss: -0.2972927987575531
Batch 41/64 loss: -0.29554393887519836
Batch 42/64 loss: -0.2960408627986908
Batch 43/64 loss: -0.2939269244670868
Batch 44/64 loss: -0.2964498698711395
Batch 45/64 loss: -0.27744919061660767
Batch 46/64 loss: -0.29953935742378235
Batch 47/64 loss: -0.2913573384284973
Batch 48/64 loss: -0.2972690761089325
Batch 49/64 loss: -0.286660760641098
Batch 50/64 loss: -0.3008955121040344
Batch 51/64 loss: -0.26935213804244995
Batch 52/64 loss: -0.2923034727573395
Batch 53/64 loss: -0.2690434455871582
Batch 54/64 loss: -0.29441291093826294
Batch 55/64 loss: -0.3026507496833801
Batch 56/64 loss: -0.28375136852264404
Batch 57/64 loss: -0.2728700041770935
Batch 58/64 loss: -0.2931661009788513
Batch 59/64 loss: -0.2929457426071167
Batch 60/64 loss: -0.28122633695602417
Batch 61/64 loss: -0.29399028420448303
Batch 62/64 loss: -0.299052894115448
Batch 63/64 loss: -0.299379825592041
Batch 64/64 loss: -0.2839983105659485
Epoch 444  Train loss: -0.2904380066722047  Val loss: -0.2732320851886395
Epoch 445
-------------------------------
Batch 1/64 loss: -0.2944769263267517
Batch 2/64 loss: -0.2913498282432556
Batch 3/64 loss: -0.29356783628463745
Batch 4/64 loss: -0.2840117812156677
Batch 5/64 loss: -0.298888623714447
Batch 6/64 loss: -0.29450368881225586
Batch 7/64 loss: -0.265256404876709
Batch 8/64 loss: -0.2916053235530853
Batch 9/64 loss: -0.29374516010284424
Batch 10/64 loss: -0.2824154794216156
Batch 11/64 loss: -0.2923995852470398
Batch 12/64 loss: -0.2973097562789917
Batch 13/64 loss: -0.30223411321640015
Batch 14/64 loss: -0.3058570623397827
Batch 15/64 loss: -0.28653424978256226
Batch 16/64 loss: -0.3019019663333893
Batch 17/64 loss: -0.29427576065063477
Batch 18/64 loss: -0.2902488112449646
Batch 19/64 loss: -0.2891068756580353
Batch 20/64 loss: -0.28259575366973877
Batch 21/64 loss: -0.2900017201900482
Batch 22/64 loss: -0.29165297746658325
Batch 23/64 loss: -0.28777259588241577
Batch 24/64 loss: -0.29003962874412537
Batch 25/64 loss: -0.2779344916343689
Batch 26/64 loss: -0.28765368461608887
Batch 27/64 loss: -0.30351197719573975
Batch 28/64 loss: -0.3005602955818176
Batch 29/64 loss: -0.2948571741580963
Batch 30/64 loss: -0.2871164083480835
Batch 31/64 loss: -0.29858124256134033
Batch 32/64 loss: -0.2918175160884857
Batch 33/64 loss: -0.29322385787963867
Batch 34/64 loss: -0.2928583025932312
Batch 35/64 loss: -0.28754711151123047
Batch 36/64 loss: -0.29225295782089233
Batch 37/64 loss: -0.2867639660835266
Batch 38/64 loss: -0.2952520251274109
Batch 39/64 loss: -0.29759716987609863
Batch 40/64 loss: -0.30175304412841797
Batch 41/64 loss: -0.2865568995475769
Batch 42/64 loss: -0.297997385263443
Batch 43/64 loss: -0.2859661877155304
Batch 44/64 loss: -0.28145965933799744
Batch 45/64 loss: -0.2680093050003052
Batch 46/64 loss: -0.3012392222881317
Batch 47/64 loss: -0.29590779542922974
Batch 48/64 loss: -0.2847989499568939
Batch 49/64 loss: -0.2958698868751526
Batch 50/64 loss: -0.29105377197265625
Batch 51/64 loss: -0.2906152307987213
Batch 52/64 loss: -0.2915828824043274
Batch 53/64 loss: -0.29489654302597046
Batch 54/64 loss: -0.2931795120239258
Batch 55/64 loss: -0.2881834805011749
Batch 56/64 loss: -0.29924044013023376
Batch 57/64 loss: -0.3007224202156067
Batch 58/64 loss: -0.3094438314437866
Batch 59/64 loss: -0.28501754999160767
Batch 60/64 loss: -0.28183817863464355
Batch 61/64 loss: -0.2885805368423462
Batch 62/64 loss: -0.2903980612754822
Batch 63/64 loss: -0.299013614654541
Batch 64/64 loss: -0.2789308726787567
Epoch 445  Train loss: -0.2916361197537067  Val loss: -0.275164249324307
Epoch 446
-------------------------------
Batch 1/64 loss: -0.2962275743484497
Batch 2/64 loss: -0.2758653461933136
Batch 3/64 loss: -0.305107444524765
Batch 4/64 loss: -0.29208099842071533
Batch 5/64 loss: -0.30461251735687256
Batch 6/64 loss: -0.2939680814743042
Batch 7/64 loss: -0.28468915820121765
Batch 8/64 loss: -0.30119627714157104
Batch 9/64 loss: -0.3034808039665222
Batch 10/64 loss: -0.2889106869697571
Batch 11/64 loss: -0.29540401697158813
Batch 12/64 loss: -0.29106855392456055
Batch 13/64 loss: -0.2996131181716919
Batch 14/64 loss: -0.296688437461853
Batch 15/64 loss: -0.29806363582611084
Batch 16/64 loss: -0.2967606782913208
Batch 17/64 loss: -0.2899973392486572
Batch 18/64 loss: -0.2978343367576599
Batch 19/64 loss: -0.28689977526664734
Batch 20/64 loss: -0.2845979928970337
Batch 21/64 loss: -0.31226521730422974
Batch 22/64 loss: -0.29563823342323303
Batch 23/64 loss: -0.28238391876220703
Batch 24/64 loss: -0.29644954204559326
Batch 25/64 loss: -0.2904016375541687
Batch 26/64 loss: -0.29298245906829834
Batch 27/64 loss: -0.2773633599281311
Batch 28/64 loss: -0.30339157581329346
Batch 29/64 loss: -0.28305280208587646
Batch 30/64 loss: -0.29616743326187134
Batch 31/64 loss: -0.2817617356777191
Batch 32/64 loss: -0.2863892912864685
Batch 33/64 loss: -0.29974284768104553
Batch 34/64 loss: -0.2914878726005554
Batch 35/64 loss: -0.2927207946777344
Batch 36/64 loss: -0.28563356399536133
Batch 37/64 loss: -0.29121047258377075
Batch 38/64 loss: -0.30748867988586426
Batch 39/64 loss: -0.29002559185028076
Batch 40/64 loss: -0.29743731021881104
Batch 41/64 loss: -0.2939251661300659
Batch 42/64 loss: -0.2859927713871002
Batch 43/64 loss: -0.2940334975719452
Batch 44/64 loss: -0.280986487865448
Batch 45/64 loss: -0.28277063369750977
Batch 46/64 loss: -0.3006570041179657
Batch 47/64 loss: -0.2917196750640869
Batch 48/64 loss: -0.28217393159866333
Batch 49/64 loss: -0.29021593928337097
Batch 50/64 loss: -0.3036794066429138
Batch 51/64 loss: -0.2899603247642517
Batch 52/64 loss: -0.30165156722068787
Batch 53/64 loss: -0.3024902939796448
Batch 54/64 loss: -0.2959417700767517
Batch 55/64 loss: -0.27397915720939636
Batch 56/64 loss: -0.3004072606563568
Batch 57/64 loss: -0.3055609464645386
Batch 58/64 loss: -0.29294246435165405
Batch 59/64 loss: -0.2820393443107605
Batch 60/64 loss: -0.28680312633514404
Batch 61/64 loss: -0.3005104064941406
Batch 62/64 loss: -0.31188449263572693
Batch 63/64 loss: -0.3003152310848236
Batch 64/64 loss: -0.2965942919254303
Epoch 446  Train loss: -0.2934925134275474  Val loss: -0.274487999091853
Epoch 447
-------------------------------
Batch 1/64 loss: -0.30139148235321045
Batch 2/64 loss: -0.29998543858528137
Batch 3/64 loss: -0.2940906882286072
Batch 4/64 loss: -0.2943986654281616
Batch 5/64 loss: -0.3038947582244873
Batch 6/64 loss: -0.2937469482421875
Batch 7/64 loss: -0.291259229183197
Batch 8/64 loss: -0.29819703102111816
Batch 9/64 loss: -0.29148566722869873
Batch 10/64 loss: -0.2849254608154297
Batch 11/64 loss: -0.29053834080696106
Batch 12/64 loss: -0.285478413105011
Batch 13/64 loss: -0.28817111253738403
Batch 14/64 loss: -0.3031873106956482
Batch 15/64 loss: -0.29872483015060425
Batch 16/64 loss: -0.3006722927093506
Batch 17/64 loss: -0.2864495813846588
Batch 18/64 loss: -0.2874463200569153
Batch 19/64 loss: -0.299333393573761
Batch 20/64 loss: -0.29797178506851196
Batch 21/64 loss: -0.2953602373600006
Batch 22/64 loss: -0.30531051754951477
Batch 23/64 loss: -0.30073678493499756
Batch 24/64 loss: -0.29843324422836304
Batch 25/64 loss: -0.2967239022254944
Batch 26/64 loss: -0.2973760664463043
Batch 27/64 loss: -0.2871362566947937
Batch 28/64 loss: -0.2870159447193146
Batch 29/64 loss: -0.2827945947647095
Batch 30/64 loss: -0.29144200682640076
Batch 31/64 loss: -0.2898653447628021
Batch 32/64 loss: -0.29375311732292175
Batch 33/64 loss: -0.3002319037914276
Batch 34/64 loss: -0.29440557956695557
Batch 35/64 loss: -0.28955408930778503
Batch 36/64 loss: -0.2995752692222595
Batch 37/64 loss: -0.3060770630836487
Batch 38/64 loss: -0.29264432191848755
Batch 39/64 loss: -0.29109707474708557
Batch 40/64 loss: -0.2969571352005005
Batch 41/64 loss: -0.2830440402030945
Batch 42/64 loss: -0.29195326566696167
Batch 43/64 loss: -0.2927546799182892
Batch 44/64 loss: -0.2953754663467407
Batch 45/64 loss: -0.30094194412231445
Batch 46/64 loss: -0.29972848296165466
Batch 47/64 loss: -0.2918474078178406
Batch 48/64 loss: -0.30061468482017517
Batch 49/64 loss: -0.2990691065788269
Batch 50/64 loss: -0.29651832580566406
Batch 51/64 loss: -0.2952089011669159
Batch 52/64 loss: -0.29291242361068726
Batch 53/64 loss: -0.3035489618778229
Batch 54/64 loss: -0.29609450697898865
Batch 55/64 loss: -0.2947135865688324
Batch 56/64 loss: -0.2952907085418701
Batch 57/64 loss: -0.2953486740589142
Batch 58/64 loss: -0.29757630825042725
Batch 59/64 loss: -0.29747217893600464
Batch 60/64 loss: -0.29488879442214966
Batch 61/64 loss: -0.29663676023483276
Batch 62/64 loss: -0.2964613139629364
Batch 63/64 loss: -0.29557567834854126
Batch 64/64 loss: -0.296295702457428
Epoch 447  Train loss: -0.29511587736653344  Val loss: -0.27575741498331025
Epoch 448
-------------------------------
Batch 1/64 loss: -0.30898353457450867
Batch 2/64 loss: -0.2978249192237854
Batch 3/64 loss: -0.3081941604614258
Batch 4/64 loss: -0.2960045337677002
Batch 5/64 loss: -0.2946823835372925
Batch 6/64 loss: -0.30373573303222656
Batch 7/64 loss: -0.29105865955352783
Batch 8/64 loss: -0.29185938835144043
Batch 9/64 loss: -0.3074941635131836
Batch 10/64 loss: -0.29636695981025696
Batch 11/64 loss: -0.2833191156387329
Batch 12/64 loss: -0.2953289747238159
Batch 13/64 loss: -0.31212639808654785
Batch 14/64 loss: -0.3007460832595825
Batch 15/64 loss: -0.2777809500694275
Batch 16/64 loss: -0.2977449893951416
Batch 17/64 loss: -0.28760817646980286
Batch 18/64 loss: -0.2843351364135742
Batch 19/64 loss: -0.30039554834365845
Batch 20/64 loss: -0.29706114530563354
Batch 21/64 loss: -0.30380865931510925
Batch 22/64 loss: -0.2944374084472656
Batch 23/64 loss: -0.2986615002155304
Batch 24/64 loss: -0.29633772373199463
Batch 25/64 loss: -0.29186075925827026
Batch 26/64 loss: -0.2987562417984009
Batch 27/64 loss: -0.3013269603252411
Batch 28/64 loss: -0.3155159056186676
Batch 29/64 loss: -0.30119773745536804
Batch 30/64 loss: -0.306551456451416
Batch 31/64 loss: -0.30030056834220886
Batch 32/64 loss: -0.2930499315261841
Batch 33/64 loss: -0.3041313886642456
Batch 34/64 loss: -0.3024534285068512
Batch 35/64 loss: -0.28351378440856934
Batch 36/64 loss: -0.3058907687664032
Batch 37/64 loss: -0.29489248991012573
Batch 38/64 loss: -0.2930193543434143
Batch 39/64 loss: -0.2943464517593384
Batch 40/64 loss: -0.30710703134536743
Batch 41/64 loss: -0.2971980571746826
Batch 42/64 loss: -0.3034372627735138
Batch 43/64 loss: -0.30074620246887207
Batch 44/64 loss: -0.3133752644062042
Batch 45/64 loss: -0.3041173815727234
Batch 46/64 loss: -0.2900034785270691
Batch 47/64 loss: -0.3047562837600708
Batch 48/64 loss: -0.2950489819049835
Batch 49/64 loss: -0.2925575375556946
Batch 50/64 loss: -0.30813854932785034
Batch 51/64 loss: -0.2927817106246948
Batch 52/64 loss: -0.3048393428325653
Batch 53/64 loss: -0.30113792419433594
Batch 54/64 loss: -0.2903875410556793
Batch 55/64 loss: -0.2993859052658081
Batch 56/64 loss: -0.30397093296051025
Batch 57/64 loss: -0.2899967432022095
Batch 58/64 loss: -0.29952916502952576
Batch 59/64 loss: -0.30177319049835205
Batch 60/64 loss: -0.30217084288597107
Batch 61/64 loss: -0.29178494215011597
Batch 62/64 loss: -0.29157117009162903
Batch 63/64 loss: -0.3090149164199829
Batch 64/64 loss: -0.300580769777298
Epoch 448  Train loss: -0.29855638286646674  Val loss: -0.27662053530158864
Epoch 449
-------------------------------
Batch 1/64 loss: -0.309745192527771
Batch 2/64 loss: -0.29746729135513306
Batch 3/64 loss: -0.3033640384674072
Batch 4/64 loss: -0.2822052240371704
Batch 5/64 loss: -0.2936987280845642
Batch 6/64 loss: -0.29443877935409546
Batch 7/64 loss: -0.30378931760787964
Batch 8/64 loss: -0.2952520251274109
Batch 9/64 loss: -0.29919320344924927
Batch 10/64 loss: -0.2867324948310852
Batch 11/64 loss: -0.3004698157310486
Batch 12/64 loss: -0.28169888257980347
Batch 13/64 loss: -0.29441580176353455
Batch 14/64 loss: -0.2910501956939697
Batch 15/64 loss: -0.2966982126235962
Batch 16/64 loss: -0.28594738245010376
Batch 17/64 loss: -0.29521018266677856
Batch 18/64 loss: -0.2938668131828308
Batch 19/64 loss: -0.3022086024284363
Batch 20/64 loss: -0.2748457193374634
Batch 21/64 loss: -0.28066593408584595
Batch 22/64 loss: -0.2960549592971802
Batch 23/64 loss: -0.29524338245391846
Batch 24/64 loss: -0.28854313492774963
Batch 25/64 loss: -0.29286718368530273
Batch 26/64 loss: -0.3005133271217346
Batch 27/64 loss: -0.28293025493621826
Batch 28/64 loss: -0.2866423726081848
Batch 29/64 loss: -0.2992057800292969
Batch 30/64 loss: -0.2802650034427643
Batch 31/64 loss: -0.29856371879577637
Batch 32/64 loss: -0.29500117897987366
Batch 33/64 loss: -0.29608267545700073
Batch 34/64 loss: -0.2946934401988983
Batch 35/64 loss: -0.2803076505661011
Batch 36/64 loss: -0.2994219958782196
Batch 37/64 loss: -0.2914085388183594
Batch 38/64 loss: -0.30289793014526367
Batch 39/64 loss: -0.29717978835105896
Batch 40/64 loss: -0.29951921105384827
Batch 41/64 loss: -0.3070119321346283
Batch 42/64 loss: -0.29338204860687256
Batch 43/64 loss: -0.27983778715133667
Batch 44/64 loss: -0.2838468551635742
Batch 45/64 loss: -0.3000936210155487
Batch 46/64 loss: -0.2952992916107178
Batch 47/64 loss: -0.2931373417377472
Batch 48/64 loss: -0.2751188278198242
Batch 49/64 loss: -0.2910274267196655
Batch 50/64 loss: -0.28273987770080566
Batch 51/64 loss: -0.27781131863594055
Batch 52/64 loss: -0.301502525806427
Batch 53/64 loss: -0.2839946150779724
Batch 54/64 loss: -0.29001182317733765
Batch 55/64 loss: -0.2743172347545624
Batch 56/64 loss: -0.30148637294769287
Batch 57/64 loss: -0.3031446933746338
Batch 58/64 loss: -0.3010260760784149
Batch 59/64 loss: -0.29574617743492126
Batch 60/64 loss: -0.29487669467926025
Batch 61/64 loss: -0.3107199966907501
Batch 62/64 loss: -0.29241472482681274
Batch 63/64 loss: -0.3074604868888855
Batch 64/64 loss: -0.2957724928855896
Epoch 449  Train loss: -0.2933041958247914  Val loss: -0.26604315833127784
Epoch 450
-------------------------------
Batch 1/64 loss: -0.2955247163772583
Batch 2/64 loss: -0.2867991328239441
Batch 3/64 loss: -0.2886180579662323
Batch 4/64 loss: -0.299963116645813
Batch 5/64 loss: -0.3008052706718445
Batch 6/64 loss: -0.2861306071281433
Batch 7/64 loss: -0.2944604158401489
Batch 8/64 loss: -0.29268887639045715
Batch 9/64 loss: -0.28726673126220703
Batch 10/64 loss: -0.2880914509296417
Batch 11/64 loss: -0.29817846417427063
Batch 12/64 loss: -0.299993097782135
Batch 13/64 loss: -0.28417399525642395
Batch 14/64 loss: -0.29859215021133423
Batch 15/64 loss: -0.29869455099105835
Batch 16/64 loss: -0.2676630914211273
Batch 17/64 loss: -0.2975347638130188
Batch 18/64 loss: -0.2937799394130707
Batch 19/64 loss: -0.3000462055206299
Batch 20/64 loss: -0.292747437953949
Batch 21/64 loss: -0.29749253392219543
Batch 22/64 loss: -0.29578450322151184
Batch 23/64 loss: -0.271217405796051
Batch 24/64 loss: -0.29258954524993896
Batch 25/64 loss: -0.30644649267196655
Batch 26/64 loss: -0.28505823016166687
Batch 27/64 loss: -0.29730188846588135
Batch 28/64 loss: -0.3067651391029358
Batch 29/64 loss: -0.2946321368217468
Batch 30/64 loss: -0.28048214316368103
Batch 31/64 loss: -0.29968947172164917
Batch 32/64 loss: -0.283069908618927
Batch 33/64 loss: -0.28273680806159973
Batch 34/64 loss: -0.3019208610057831
Batch 35/64 loss: -0.28898900747299194
Batch 36/64 loss: -0.2957612872123718
Batch 37/64 loss: -0.2957265377044678
Batch 38/64 loss: -0.30426347255706787
Batch 39/64 loss: -0.28452548384666443
Batch 40/64 loss: -0.29795193672180176
Batch 41/64 loss: -0.2935180962085724
Batch 42/64 loss: -0.2938932180404663
Batch 43/64 loss: -0.29465535283088684
Batch 44/64 loss: -0.27488234639167786
Batch 45/64 loss: -0.26358646154403687
Batch 46/64 loss: -0.30220654606819153
Batch 47/64 loss: -0.29407429695129395
Batch 48/64 loss: -0.29598650336265564
Batch 49/64 loss: -0.292221337556839
Batch 50/64 loss: -0.28304600715637207
Batch 51/64 loss: -0.29099002480506897
Batch 52/64 loss: -0.2830122113227844
Batch 53/64 loss: -0.2897561490535736
Batch 54/64 loss: -0.29109787940979004
Batch 55/64 loss: -0.2879674434661865
Batch 56/64 loss: -0.2989250421524048
Batch 57/64 loss: -0.289869487285614
Batch 58/64 loss: -0.2887331247329712
Batch 59/64 loss: -0.29649996757507324
Batch 60/64 loss: -0.29645487666130066
Batch 61/64 loss: -0.28511038422584534
Batch 62/64 loss: -0.28429433703422546
Batch 63/64 loss: -0.29780861735343933
Batch 64/64 loss: -0.2991117835044861
Epoch 450  Train loss: -0.2918757712139803  Val loss: -0.2798634822835627
Epoch 451
-------------------------------
Batch 1/64 loss: -0.2892688512802124
Batch 2/64 loss: -0.3016965389251709
Batch 3/64 loss: -0.29314038157463074
Batch 4/64 loss: -0.29278600215911865
Batch 5/64 loss: -0.3071054220199585
Batch 6/64 loss: -0.2874913215637207
Batch 7/64 loss: -0.2690950930118561
Batch 8/64 loss: -0.2912081480026245
Batch 9/64 loss: -0.2963365912437439
Batch 10/64 loss: -0.290252685546875
Batch 11/64 loss: -0.28639844059944153
Batch 12/64 loss: -0.28983771800994873
Batch 13/64 loss: -0.3006577491760254
Batch 14/64 loss: -0.2894443869590759
Batch 15/64 loss: -0.2934211492538452
Batch 16/64 loss: -0.29704734683036804
Batch 17/64 loss: -0.2793256938457489
Batch 18/64 loss: -0.28451967239379883
Batch 19/64 loss: -0.29041731357574463
Batch 20/64 loss: -0.28555524349212646
Batch 21/64 loss: -0.29747137427330017
Batch 22/64 loss: -0.308074414730072
Batch 23/64 loss: -0.29949820041656494
Batch 24/64 loss: -0.28846025466918945
Batch 25/64 loss: -0.27531468868255615
Batch 26/64 loss: -0.2964150309562683
Batch 27/64 loss: -0.2863467335700989
Batch 28/64 loss: -0.30276405811309814
Batch 29/64 loss: -0.30206823348999023
Batch 30/64 loss: -0.3015452027320862
Batch 31/64 loss: -0.27883297204971313
Batch 32/64 loss: -0.29191717505455017
Batch 33/64 loss: -0.2988494336605072
Batch 34/64 loss: -0.30020660161972046
Batch 35/64 loss: -0.30169904232025146
Batch 36/64 loss: -0.29848790168762207
Batch 37/64 loss: -0.3061944246292114
Batch 38/64 loss: -0.2736433148384094
Batch 39/64 loss: -0.2949244976043701
Batch 40/64 loss: -0.3063831031322479
Batch 41/64 loss: -0.29345446825027466
Batch 42/64 loss: -0.30142486095428467
Batch 43/64 loss: -0.30015137791633606
Batch 44/64 loss: -0.2860516607761383
Batch 45/64 loss: -0.28403180837631226
Batch 46/64 loss: -0.2975691556930542
Batch 47/64 loss: -0.2883097231388092
Batch 48/64 loss: -0.3037620186805725
Batch 49/64 loss: -0.2907116413116455
Batch 50/64 loss: -0.2784048318862915
Batch 51/64 loss: -0.28427666425704956
Batch 52/64 loss: -0.2866157293319702
Batch 53/64 loss: -0.2925679683685303
Batch 54/64 loss: -0.2980690896511078
Batch 55/64 loss: -0.2842055559158325
Batch 56/64 loss: -0.28442391753196716
Batch 57/64 loss: -0.3022635877132416
Batch 58/64 loss: -0.2787627875804901
Batch 59/64 loss: -0.2903911769390106
Batch 60/64 loss: -0.27994900941848755
Batch 61/64 loss: -0.2896113693714142
Batch 62/64 loss: -0.30179986357688904
Batch 63/64 loss: -0.29984959959983826
Batch 64/64 loss: -0.3077566921710968
Epoch 451  Train loss: -0.29257376930292917  Val loss: -0.2663175375600861
Epoch 452
-------------------------------
Batch 1/64 loss: -0.28918689489364624
Batch 2/64 loss: -0.2866138219833374
Batch 3/64 loss: -0.29811862111091614
Batch 4/64 loss: -0.28544771671295166
Batch 5/64 loss: -0.2844231128692627
Batch 6/64 loss: -0.2921172082424164
Batch 7/64 loss: -0.29015424847602844
Batch 8/64 loss: -0.294603556394577
Batch 9/64 loss: -0.28798720240592957
Batch 10/64 loss: -0.28524667024612427
Batch 11/64 loss: -0.29784202575683594
Batch 12/64 loss: -0.2910110056400299
Batch 13/64 loss: -0.29899004101753235
Batch 14/64 loss: -0.2831043601036072
Batch 15/64 loss: -0.30338382720947266
Batch 16/64 loss: -0.2849509119987488
Batch 17/64 loss: -0.29500770568847656
Batch 18/64 loss: -0.2944350838661194
Batch 19/64 loss: -0.2968701124191284
Batch 20/64 loss: -0.28282901644706726
Batch 21/64 loss: -0.28596052527427673
Batch 22/64 loss: -0.2773234248161316
Batch 23/64 loss: -0.29742252826690674
Batch 24/64 loss: -0.2986563444137573
Batch 25/64 loss: -0.2938815951347351
Batch 26/64 loss: -0.3005371689796448
Batch 27/64 loss: -0.2890079617500305
Batch 28/64 loss: -0.29388439655303955
Batch 29/64 loss: -0.2967798411846161
Batch 30/64 loss: -0.29391878843307495
Batch 31/64 loss: -0.2971804141998291
Batch 32/64 loss: -0.2884334325790405
Batch 33/64 loss: -0.283744752407074
Batch 34/64 loss: -0.2946903109550476
Batch 35/64 loss: -0.2896285951137543
Batch 36/64 loss: -0.3032926917076111
Batch 37/64 loss: -0.2996744215488434
Batch 38/64 loss: -0.3051973581314087
Batch 39/64 loss: -0.2913927137851715
Batch 40/64 loss: -0.2931493818759918
Batch 41/64 loss: -0.28927192091941833
Batch 42/64 loss: -0.2972376346588135
Batch 43/64 loss: -0.29406240582466125
Batch 44/64 loss: -0.3014742136001587
Batch 45/64 loss: -0.29209303855895996
Batch 46/64 loss: -0.297042578458786
Batch 47/64 loss: -0.2896058261394501
Batch 48/64 loss: -0.29083704948425293
Batch 49/64 loss: -0.2873292863368988
Batch 50/64 loss: -0.29419493675231934
Batch 51/64 loss: -0.2890200912952423
Batch 52/64 loss: -0.28841501474380493
Batch 53/64 loss: -0.2828296422958374
Batch 54/64 loss: -0.29089564085006714
Batch 55/64 loss: -0.29493021965026855
Batch 56/64 loss: -0.28276312351226807
Batch 57/64 loss: -0.2912113666534424
Batch 58/64 loss: -0.2913661003112793
Batch 59/64 loss: -0.2901346683502197
Batch 60/64 loss: -0.31092745065689087
Batch 61/64 loss: -0.30665862560272217
Batch 62/64 loss: -0.2881165146827698
Batch 63/64 loss: -0.29142510890960693
Batch 64/64 loss: -0.29580289125442505
Epoch 452  Train loss: -0.2925454810553906  Val loss: -0.26574769544437576
Epoch 453
-------------------------------
Batch 1/64 loss: -0.30959105491638184
Batch 2/64 loss: -0.2763412594795227
Batch 3/64 loss: -0.2948150634765625
Batch 4/64 loss: -0.2964550852775574
Batch 5/64 loss: -0.27893418073654175
Batch 6/64 loss: -0.30804234743118286
Batch 7/64 loss: -0.2925707697868347
Batch 8/64 loss: -0.30535030364990234
Batch 9/64 loss: -0.29577121138572693
Batch 10/64 loss: -0.30629831552505493
Batch 11/64 loss: -0.3030892312526703
Batch 12/64 loss: -0.2906426787376404
Batch 13/64 loss: -0.29593905806541443
Batch 14/64 loss: -0.28742319345474243
Batch 15/64 loss: -0.2750847041606903
Batch 16/64 loss: -0.29846417903900146
Batch 17/64 loss: -0.2881632149219513
Batch 18/64 loss: -0.2911648154258728
Batch 19/64 loss: -0.2933253049850464
Batch 20/64 loss: -0.30132418870925903
Batch 21/64 loss: -0.293596476316452
Batch 22/64 loss: -0.2941330671310425
Batch 23/64 loss: -0.2854073643684387
Batch 24/64 loss: -0.280803918838501
Batch 25/64 loss: -0.30194714665412903
Batch 26/64 loss: -0.2697356939315796
Batch 27/64 loss: -0.27844560146331787
Batch 28/64 loss: -0.29370030760765076
Batch 29/64 loss: -0.2990466058254242
Batch 30/64 loss: -0.3046892285346985
Batch 31/64 loss: -0.2933460474014282
Batch 32/64 loss: -0.2999812364578247
Batch 33/64 loss: -0.3011261224746704
Batch 34/64 loss: -0.27503129839897156
Batch 35/64 loss: -0.30028676986694336
Batch 36/64 loss: -0.2811441123485565
Batch 37/64 loss: -0.29623278975486755
Batch 38/64 loss: -0.2947011888027191
Batch 39/64 loss: -0.285114049911499
Batch 40/64 loss: -0.2925758957862854
Batch 41/64 loss: -0.275546133518219
Batch 42/64 loss: -0.28582167625427246
Batch 43/64 loss: -0.286516398191452
Batch 44/64 loss: -0.2886511981487274
Batch 45/64 loss: -0.28549036383628845
Batch 46/64 loss: -0.30285701155662537
Batch 47/64 loss: -0.274938702583313
Batch 48/64 loss: -0.2793029546737671
Batch 49/64 loss: -0.2884921133518219
Batch 50/64 loss: -0.2787420451641083
Batch 51/64 loss: -0.2967853546142578
Batch 52/64 loss: -0.27072861790657043
Batch 53/64 loss: -0.28503546118736267
Batch 54/64 loss: -0.2928429841995239
Batch 55/64 loss: -0.2873762249946594
Batch 56/64 loss: -0.28910285234451294
Batch 57/64 loss: -0.2974064350128174
Batch 58/64 loss: -0.2714777886867523
Batch 59/64 loss: -0.28247392177581787
Batch 60/64 loss: -0.2979770004749298
Batch 61/64 loss: -0.2902441620826721
Batch 62/64 loss: -0.294725239276886
Batch 63/64 loss: -0.298816978931427
Batch 64/64 loss: -0.2962714731693268
Epoch 453  Train loss: -0.29071984793625627  Val loss: -0.2563674636313186
Epoch 454
-------------------------------
Batch 1/64 loss: -0.2884499430656433
Batch 2/64 loss: -0.2965033948421478
Batch 3/64 loss: -0.2823951840400696
Batch 4/64 loss: -0.3029893636703491
Batch 5/64 loss: -0.28798097372055054
Batch 6/64 loss: -0.29246193170547485
Batch 7/64 loss: -0.2898186445236206
Batch 8/64 loss: -0.3030494153499603
Batch 9/64 loss: -0.29304951429367065
Batch 10/64 loss: -0.2787798047065735
Batch 11/64 loss: -0.29168516397476196
Batch 12/64 loss: -0.28234148025512695
Batch 13/64 loss: -0.2917293906211853
Batch 14/64 loss: -0.29522597789764404
Batch 15/64 loss: -0.28418493270874023
Batch 16/64 loss: -0.2985583543777466
Batch 17/64 loss: -0.2996056079864502
Batch 18/64 loss: -0.3047376871109009
Batch 19/64 loss: -0.2956959009170532
Batch 20/64 loss: -0.29252001643180847
Batch 21/64 loss: -0.2887914180755615
Batch 22/64 loss: -0.2762147784233093
Batch 23/64 loss: -0.29746556282043457
Batch 24/64 loss: -0.2607764005661011
Batch 25/64 loss: -0.2790948152542114
Batch 26/64 loss: -0.2886257767677307
Batch 27/64 loss: -0.29215237498283386
Batch 28/64 loss: -0.28582704067230225
Batch 29/64 loss: -0.2793459892272949
Batch 30/64 loss: -0.2744706869125366
Batch 31/64 loss: -0.29247528314590454
Batch 32/64 loss: -0.2883595824241638
Batch 33/64 loss: -0.2909297049045563
Batch 34/64 loss: -0.28579777479171753
Batch 35/64 loss: -0.2883615493774414
Batch 36/64 loss: -0.28489041328430176
Batch 37/64 loss: -0.28870296478271484
Batch 38/64 loss: -0.28668612241744995
Batch 39/64 loss: -0.28969165682792664
Batch 40/64 loss: -0.29196882247924805
Batch 41/64 loss: -0.29436200857162476
Batch 42/64 loss: -0.2880868911743164
Batch 43/64 loss: -0.29632532596588135
Batch 44/64 loss: -0.2960944175720215
Batch 45/64 loss: -0.29929307103157043
Batch 46/64 loss: -0.2807197868824005
Batch 47/64 loss: -0.2791907489299774
Batch 48/64 loss: -0.2910839915275574
Batch 49/64 loss: -0.28039878606796265
Batch 50/64 loss: -0.28193384408950806
Batch 51/64 loss: -0.2800426185131073
Batch 52/64 loss: -0.3029477596282959
Batch 53/64 loss: -0.26204442977905273
Batch 54/64 loss: -0.29308539628982544
Batch 55/64 loss: -0.29307234287261963
Batch 56/64 loss: -0.28320562839508057
Batch 57/64 loss: -0.28395533561706543
Batch 58/64 loss: -0.2884647250175476
Batch 59/64 loss: -0.28844964504241943
Batch 60/64 loss: -0.29331159591674805
Batch 61/64 loss: -0.2940875291824341
Batch 62/64 loss: -0.28611910343170166
Batch 63/64 loss: -0.2750661373138428
Batch 64/64 loss: -0.2977326214313507
Epoch 454  Train loss: -0.28858089388585556  Val loss: -0.2761102972161729
Epoch 455
-------------------------------
Batch 1/64 loss: -0.2862483859062195
Batch 2/64 loss: -0.28891411423683167
Batch 3/64 loss: -0.29295653104782104
Batch 4/64 loss: -0.27714869379997253
Batch 5/64 loss: -0.28382542729377747
Batch 6/64 loss: -0.28982070088386536
Batch 7/64 loss: -0.2878921627998352
Batch 8/64 loss: -0.29011639952659607
Batch 9/64 loss: -0.27279219031333923
Batch 10/64 loss: -0.296641081571579
Batch 11/64 loss: -0.2984277606010437
Batch 12/64 loss: -0.30333417654037476
Batch 13/64 loss: -0.2815946340560913
Batch 14/64 loss: -0.2844919264316559
Batch 15/64 loss: -0.2887713611125946
Batch 16/64 loss: -0.27990010380744934
Batch 17/64 loss: -0.28379392623901367
Batch 18/64 loss: -0.29716435074806213
Batch 19/64 loss: -0.2939962148666382
Batch 20/64 loss: -0.28906846046447754
Batch 21/64 loss: -0.29085808992385864
Batch 22/64 loss: -0.29291123151779175
Batch 23/64 loss: -0.2809889018535614
Batch 24/64 loss: -0.2848868668079376
Batch 25/64 loss: -0.3005675673484802
Batch 26/64 loss: -0.2817309498786926
Batch 27/64 loss: -0.2967248857021332
Batch 28/64 loss: -0.2947953939437866
Batch 29/64 loss: -0.3050082325935364
Batch 30/64 loss: -0.28033357858657837
Batch 31/64 loss: -0.29432907700538635
Batch 32/64 loss: -0.2962091863155365
Batch 33/64 loss: -0.2882358729839325
Batch 34/64 loss: -0.29275959730148315
Batch 35/64 loss: -0.2916123569011688
Batch 36/64 loss: -0.2913406193256378
Batch 37/64 loss: -0.28245025873184204
Batch 38/64 loss: -0.2912006974220276
Batch 39/64 loss: -0.294968843460083
Batch 40/64 loss: -0.2942659258842468
Batch 41/64 loss: -0.28273922204971313
Batch 42/64 loss: -0.30063509941101074
Batch 43/64 loss: -0.29097217321395874
Batch 44/64 loss: -0.31511417031288147
Batch 45/64 loss: -0.28806546330451965
Batch 46/64 loss: -0.30608952045440674
Batch 47/64 loss: -0.29197320342063904
Batch 48/64 loss: -0.292081356048584
Batch 49/64 loss: -0.29248976707458496
Batch 50/64 loss: -0.2898906469345093
Batch 51/64 loss: -0.29383230209350586
Batch 52/64 loss: -0.30059659481048584
Batch 53/64 loss: -0.2641294002532959
Batch 54/64 loss: -0.293559193611145
Batch 55/64 loss: -0.3056604266166687
Batch 56/64 loss: -0.29399099946022034
Batch 57/64 loss: -0.3032926321029663
Batch 58/64 loss: -0.29490095376968384
Batch 59/64 loss: -0.2926228642463684
Batch 60/64 loss: -0.2887117862701416
Batch 61/64 loss: -0.27735209465026855
Batch 62/64 loss: -0.2986406087875366
Batch 63/64 loss: -0.2901068925857544
Batch 64/64 loss: -0.25315916538238525
Epoch 455  Train loss: -0.2906728389216404  Val loss: -0.27040967171134817
Epoch 456
-------------------------------
Batch 1/64 loss: -0.28709155321121216
Batch 2/64 loss: -0.2954142391681671
Batch 3/64 loss: -0.2943766117095947
Batch 4/64 loss: -0.29816263914108276
Batch 5/64 loss: -0.2919854521751404
Batch 6/64 loss: -0.304767906665802
Batch 7/64 loss: -0.27147185802459717
Batch 8/64 loss: -0.27830803394317627
Batch 9/64 loss: -0.29235395789146423
Batch 10/64 loss: -0.2888457775115967
Batch 11/64 loss: -0.28961390256881714
Batch 12/64 loss: -0.3014559745788574
Batch 13/64 loss: -0.28825613856315613
Batch 14/64 loss: -0.2902403771877289
Batch 15/64 loss: -0.2826564908027649
Batch 16/64 loss: -0.28403449058532715
Batch 17/64 loss: -0.2791452407836914
Batch 18/64 loss: -0.27906370162963867
Batch 19/64 loss: -0.2962949275970459
Batch 20/64 loss: -0.27986907958984375
Batch 21/64 loss: -0.291803240776062
Batch 22/64 loss: -0.29474371671676636
Batch 23/64 loss: -0.2869352698326111
Batch 24/64 loss: -0.2891620397567749
Batch 25/64 loss: -0.28613826632499695
Batch 26/64 loss: -0.30409371852874756
Batch 27/64 loss: -0.3000754714012146
Batch 28/64 loss: -0.29361486434936523
Batch 29/64 loss: -0.29289954900741577
Batch 30/64 loss: -0.2931717038154602
Batch 31/64 loss: -0.3020065724849701
Batch 32/64 loss: -0.28583288192749023
Batch 33/64 loss: -0.2841210663318634
Batch 34/64 loss: -0.29864662885665894
Batch 35/64 loss: -0.3110530972480774
Batch 36/64 loss: -0.29690641164779663
Batch 37/64 loss: -0.2974071502685547
Batch 38/64 loss: -0.303006649017334
Batch 39/64 loss: -0.2859560251235962
Batch 40/64 loss: -0.298108845949173
Batch 41/64 loss: -0.27784818410873413
Batch 42/64 loss: -0.2948881983757019
Batch 43/64 loss: -0.2818407714366913
Batch 44/64 loss: -0.2731647491455078
Batch 45/64 loss: -0.28223443031311035
Batch 46/64 loss: -0.277396023273468
Batch 47/64 loss: -0.284918874502182
Batch 48/64 loss: -0.2996487021446228
Batch 49/64 loss: -0.2898748815059662
Batch 50/64 loss: -0.28006017208099365
Batch 51/64 loss: -0.30860966444015503
Batch 52/64 loss: -0.3069773316383362
Batch 53/64 loss: -0.2923199534416199
Batch 54/64 loss: -0.2904663681983948
Batch 55/64 loss: -0.3020501136779785
Batch 56/64 loss: -0.2929876446723938
Batch 57/64 loss: -0.26719367504119873
Batch 58/64 loss: -0.2921364903450012
Batch 59/64 loss: -0.28823649883270264
Batch 60/64 loss: -0.28887638449668884
Batch 61/64 loss: -0.2817971706390381
Batch 62/64 loss: -0.2931176424026489
Batch 63/64 loss: -0.2942621111869812
Batch 64/64 loss: -0.29703661799430847
Epoch 456  Train loss: -0.29071019642493307  Val loss: -0.26391792174467105
Epoch 457
-------------------------------
Batch 1/64 loss: -0.2960307002067566
Batch 2/64 loss: -0.28612691164016724
Batch 3/64 loss: -0.29956313967704773
Batch 4/64 loss: -0.2979292571544647
Batch 5/64 loss: -0.2761780023574829
Batch 6/64 loss: -0.28766530752182007
Batch 7/64 loss: -0.28509408235549927
Batch 8/64 loss: -0.29380524158477783
Batch 9/64 loss: -0.30570393800735474
Batch 10/64 loss: -0.2935263216495514
Batch 11/64 loss: -0.29944220185279846
Batch 12/64 loss: -0.2718645930290222
Batch 13/64 loss: -0.3045189380645752
Batch 14/64 loss: -0.2887037694454193
Batch 15/64 loss: -0.29054391384124756
Batch 16/64 loss: -0.2844788432121277
Batch 17/64 loss: -0.30652838945388794
Batch 18/64 loss: -0.29701724648475647
Batch 19/64 loss: -0.29936715960502625
Batch 20/64 loss: -0.2913351058959961
Batch 21/64 loss: -0.2936246991157532
Batch 22/64 loss: -0.2923794388771057
Batch 23/64 loss: -0.29639726877212524
Batch 24/64 loss: -0.2897704243659973
Batch 25/64 loss: -0.3017418384552002
Batch 26/64 loss: -0.28240758180618286
Batch 27/64 loss: -0.28546738624572754
Batch 28/64 loss: -0.2883453071117401
Batch 29/64 loss: -0.2900482714176178
Batch 30/64 loss: -0.29798173904418945
Batch 31/64 loss: -0.29633235931396484
Batch 32/64 loss: -0.27053403854370117
Batch 33/64 loss: -0.2942066788673401
Batch 34/64 loss: -0.2962077558040619
Batch 35/64 loss: -0.2912985384464264
Batch 36/64 loss: -0.2904723286628723
Batch 37/64 loss: -0.30471163988113403
Batch 38/64 loss: -0.29283982515335083
Batch 39/64 loss: -0.3060944378376007
Batch 40/64 loss: -0.30265170335769653
Batch 41/64 loss: -0.2865464687347412
Batch 42/64 loss: -0.2962906062602997
Batch 43/64 loss: -0.2868425250053406
Batch 44/64 loss: -0.29023677110671997
Batch 45/64 loss: -0.2886829972267151
Batch 46/64 loss: -0.2977778911590576
Batch 47/64 loss: -0.28772103786468506
Batch 48/64 loss: -0.29846829175949097
Batch 49/64 loss: -0.2781406044960022
Batch 50/64 loss: -0.2932998538017273
Batch 51/64 loss: -0.30186742544174194
Batch 52/64 loss: -0.2936270236968994
Batch 53/64 loss: -0.2966042459011078
Batch 54/64 loss: -0.30335479974746704
Batch 55/64 loss: -0.305269718170166
Batch 56/64 loss: -0.29970914125442505
Batch 57/64 loss: -0.2966281771659851
Batch 58/64 loss: -0.3046755790710449
Batch 59/64 loss: -0.2980532944202423
Batch 60/64 loss: -0.2981169819831848
Batch 61/64 loss: -0.27553319931030273
Batch 62/64 loss: -0.2825441360473633
Batch 63/64 loss: -0.29352688789367676
Batch 64/64 loss: -0.2929205298423767
Epoch 457  Train loss: -0.29321010790619195  Val loss: -0.25992237364303616
Epoch 458
-------------------------------
Batch 1/64 loss: -0.3088279664516449
Batch 2/64 loss: -0.28482139110565186
Batch 3/64 loss: -0.304340124130249
Batch 4/64 loss: -0.28988784551620483
Batch 5/64 loss: -0.29046642780303955
Batch 6/64 loss: -0.2880554497241974
Batch 7/64 loss: -0.3002660572528839
Batch 8/64 loss: -0.2895539402961731
Batch 9/64 loss: -0.2928674817085266
Batch 10/64 loss: -0.2930830121040344
Batch 11/64 loss: -0.3021047115325928
Batch 12/64 loss: -0.3069957494735718
Batch 13/64 loss: -0.28356361389160156
Batch 14/64 loss: -0.2886064052581787
Batch 15/64 loss: -0.29459136724472046
Batch 16/64 loss: -0.2978028655052185
Batch 17/64 loss: -0.2998921275138855
Batch 18/64 loss: -0.286298930644989
Batch 19/64 loss: -0.2907427251338959
Batch 20/64 loss: -0.3024609684944153
Batch 21/64 loss: -0.2956686317920685
Batch 22/64 loss: -0.2863301634788513
Batch 23/64 loss: -0.30069464445114136
Batch 24/64 loss: -0.28774315118789673
Batch 25/64 loss: -0.2996867299079895
Batch 26/64 loss: -0.3039349913597107
Batch 27/64 loss: -0.30079972743988037
Batch 28/64 loss: -0.2925559878349304
Batch 29/64 loss: -0.29887938499450684
Batch 30/64 loss: -0.29226160049438477
Batch 31/64 loss: -0.2898387908935547
Batch 32/64 loss: -0.2916067838668823
Batch 33/64 loss: -0.29339396953582764
Batch 34/64 loss: -0.28938382863998413
Batch 35/64 loss: -0.29558396339416504
Batch 36/64 loss: -0.3079602122306824
Batch 37/64 loss: -0.3042052388191223
Batch 38/64 loss: -0.2950609028339386
Batch 39/64 loss: -0.30466923117637634
Batch 40/64 loss: -0.3079957962036133
Batch 41/64 loss: -0.29926782846450806
Batch 42/64 loss: -0.29042211174964905
Batch 43/64 loss: -0.29570209980010986
Batch 44/64 loss: -0.28911519050598145
Batch 45/64 loss: -0.2926139533519745
Batch 46/64 loss: -0.2985096275806427
Batch 47/64 loss: -0.2956407368183136
Batch 48/64 loss: -0.2871095836162567
Batch 49/64 loss: -0.2899373769760132
Batch 50/64 loss: -0.2874145209789276
Batch 51/64 loss: -0.29220545291900635
Batch 52/64 loss: -0.28728458285331726
Batch 53/64 loss: -0.2904840409755707
Batch 54/64 loss: -0.29461920261383057
Batch 55/64 loss: -0.27585235238075256
Batch 56/64 loss: -0.2984982132911682
Batch 57/64 loss: -0.30012691020965576
Batch 58/64 loss: -0.29470157623291016
Batch 59/64 loss: -0.2768697142601013
Batch 60/64 loss: -0.29945337772369385
Batch 61/64 loss: -0.29278564453125
Batch 62/64 loss: -0.2849479913711548
Batch 63/64 loss: -0.28459039330482483
Batch 64/64 loss: -0.30415207147598267
Epoch 458  Train loss: -0.294270579253926  Val loss: -0.27379611810458077
Epoch 459
-------------------------------
Batch 1/64 loss: -0.2948813736438751
Batch 2/64 loss: -0.26529285311698914
Batch 3/64 loss: -0.2917613685131073
Batch 4/64 loss: -0.30936166644096375
Batch 5/64 loss: -0.2852418124675751
Batch 6/64 loss: -0.288955956697464
Batch 7/64 loss: -0.28566157817840576
Batch 8/64 loss: -0.29542452096939087
Batch 9/64 loss: -0.2877507507801056
Batch 10/64 loss: -0.3036963641643524
Batch 11/64 loss: -0.2882484793663025
Batch 12/64 loss: -0.3047766089439392
Batch 13/64 loss: -0.2979603409767151
Batch 14/64 loss: -0.2894444763660431
Batch 15/64 loss: -0.2839195430278778
Batch 16/64 loss: -0.2895630896091461
Batch 17/64 loss: -0.28013116121292114
Batch 18/64 loss: -0.29046303033828735
Batch 19/64 loss: -0.2919076383113861
Batch 20/64 loss: -0.2973713278770447
Batch 21/64 loss: -0.3044651746749878
Batch 22/64 loss: -0.2895173132419586
Batch 23/64 loss: -0.28593122959136963
Batch 24/64 loss: -0.30962929129600525
Batch 25/64 loss: -0.2916335463523865
Batch 26/64 loss: -0.2914406359195709
Batch 27/64 loss: -0.2779865562915802
Batch 28/64 loss: -0.2849041819572449
Batch 29/64 loss: -0.28511855006217957
Batch 30/64 loss: -0.2913774251937866
Batch 31/64 loss: -0.29012975096702576
Batch 32/64 loss: -0.27845102548599243
Batch 33/64 loss: -0.2845272421836853
Batch 34/64 loss: -0.3012259006500244
Batch 35/64 loss: -0.2960507571697235
Batch 36/64 loss: -0.2690821886062622
Batch 37/64 loss: -0.3033374547958374
Batch 38/64 loss: -0.30124419927597046
Batch 39/64 loss: -0.2777256369590759
Batch 40/64 loss: -0.29052671790122986
Batch 41/64 loss: -0.26801252365112305
Batch 42/64 loss: -0.29507824778556824
Batch 43/64 loss: -0.3039455711841583
Batch 44/64 loss: -0.2911779284477234
Batch 45/64 loss: -0.2969023287296295
Batch 46/64 loss: -0.28092262148857117
Batch 47/64 loss: -0.29296770691871643
Batch 48/64 loss: -0.29577842354774475
Batch 49/64 loss: -0.297055721282959
Batch 50/64 loss: -0.29744184017181396
Batch 51/64 loss: -0.28128570318222046
Batch 52/64 loss: -0.27371034026145935
Batch 53/64 loss: -0.2909631133079529
Batch 54/64 loss: -0.30334556102752686
Batch 55/64 loss: -0.29518836736679077
Batch 56/64 loss: -0.29456815123558044
Batch 57/64 loss: -0.30022621154785156
Batch 58/64 loss: -0.2918364405632019
Batch 59/64 loss: -0.29011836647987366
Batch 60/64 loss: -0.2923187017440796
Batch 61/64 loss: -0.2896409034729004
Batch 62/64 loss: -0.2944749891757965
Batch 63/64 loss: -0.30514073371887207
Batch 64/64 loss: -0.30080103874206543
Epoch 459  Train loss: -0.2913540391361012  Val loss: -0.27439517315310713
Epoch 460
-------------------------------
Batch 1/64 loss: -0.28676390647888184
Batch 2/64 loss: -0.29820990562438965
Batch 3/64 loss: -0.29854312539100647
Batch 4/64 loss: -0.2995131015777588
Batch 5/64 loss: -0.3019861876964569
Batch 6/64 loss: -0.2940259575843811
Batch 7/64 loss: -0.29869672656059265
Batch 8/64 loss: -0.2818797826766968
Batch 9/64 loss: -0.2738879323005676
Batch 10/64 loss: -0.2649192810058594
Batch 11/64 loss: -0.28776633739471436
Batch 12/64 loss: -0.2780064344406128
Batch 13/64 loss: -0.29833388328552246
Batch 14/64 loss: -0.2997686564922333
Batch 15/64 loss: -0.3052952289581299
Batch 16/64 loss: -0.2911543846130371
Batch 17/64 loss: -0.29537343978881836
Batch 18/64 loss: -0.2812017500400543
Batch 19/64 loss: -0.293113648891449
Batch 20/64 loss: -0.30359160900115967
Batch 21/64 loss: -0.2948930561542511
Batch 22/64 loss: -0.29379525780677795
Batch 23/64 loss: -0.2985040843486786
Batch 24/64 loss: -0.29241740703582764
Batch 25/64 loss: -0.28788915276527405
Batch 26/64 loss: -0.2977157235145569
Batch 27/64 loss: -0.3029591143131256
Batch 28/64 loss: -0.294063925743103
Batch 29/64 loss: -0.29969239234924316
Batch 30/64 loss: -0.28542202711105347
Batch 31/64 loss: -0.28379225730895996
Batch 32/64 loss: -0.2859542667865753
Batch 33/64 loss: -0.3019966781139374
Batch 34/64 loss: -0.2988068461418152
Batch 35/64 loss: -0.297608882188797
Batch 36/64 loss: -0.29591596126556396
Batch 37/64 loss: -0.2742765247821808
Batch 38/64 loss: -0.29324692487716675
Batch 39/64 loss: -0.30399924516677856
Batch 40/64 loss: -0.29962480068206787
Batch 41/64 loss: -0.2911757528781891
Batch 42/64 loss: -0.29227644205093384
Batch 43/64 loss: -0.30202344059944153
Batch 44/64 loss: -0.29419994354248047
Batch 45/64 loss: -0.28804275393486023
Batch 46/64 loss: -0.2939697504043579
Batch 47/64 loss: -0.2852770984172821
Batch 48/64 loss: -0.28637927770614624
Batch 49/64 loss: -0.2873639166355133
Batch 50/64 loss: -0.2990201413631439
Batch 51/64 loss: -0.28685301542282104
Batch 52/64 loss: -0.29127442836761475
Batch 53/64 loss: -0.2953101396560669
Batch 54/64 loss: -0.2976588010787964
Batch 55/64 loss: -0.2931850850582123
Batch 56/64 loss: -0.2962568700313568
Batch 57/64 loss: -0.28875553607940674
Batch 58/64 loss: -0.3007224202156067
Batch 59/64 loss: -0.2994246184825897
Batch 60/64 loss: -0.29507678747177124
Batch 61/64 loss: -0.2992982566356659
Batch 62/64 loss: -0.29011446237564087
Batch 63/64 loss: -0.2903611660003662
Batch 64/64 loss: -0.2920902967453003
Epoch 460  Train loss: -0.2929834295721615  Val loss: -0.27027035865587057
Epoch 461
-------------------------------
Batch 1/64 loss: -0.2959407567977905
Batch 2/64 loss: -0.29159072041511536
Batch 3/64 loss: -0.30367621779441833
Batch 4/64 loss: -0.2923530340194702
Batch 5/64 loss: -0.28902727365493774
Batch 6/64 loss: -0.300566166639328
Batch 7/64 loss: -0.2954523265361786
Batch 8/64 loss: -0.29421234130859375
Batch 9/64 loss: -0.29061275720596313
Batch 10/64 loss: -0.2954038381576538
Batch 11/64 loss: -0.2984353303909302
Batch 12/64 loss: -0.2967090308666229
Batch 13/64 loss: -0.2971690595149994
Batch 14/64 loss: -0.29387956857681274
Batch 15/64 loss: -0.2954157292842865
Batch 16/64 loss: -0.28292161226272583
Batch 17/64 loss: -0.2724871039390564
Batch 18/64 loss: -0.30091580748558044
Batch 19/64 loss: -0.2860286235809326
Batch 20/64 loss: -0.28800445795059204
Batch 21/64 loss: -0.2862921953201294
Batch 22/64 loss: -0.2913474440574646
Batch 23/64 loss: -0.28528058528900146
Batch 24/64 loss: -0.2971859574317932
Batch 25/64 loss: -0.29389822483062744
Batch 26/64 loss: -0.3001565933227539
Batch 27/64 loss: -0.28118735551834106
Batch 28/64 loss: -0.2922435402870178
Batch 29/64 loss: -0.3004574775695801
Batch 30/64 loss: -0.28163790702819824
Batch 31/64 loss: -0.2915612459182739
Batch 32/64 loss: -0.2819663882255554
Batch 33/64 loss: -0.2986866533756256
Batch 34/64 loss: -0.29307347536087036
Batch 35/64 loss: -0.2962389290332794
Batch 36/64 loss: -0.29394254088401794
Batch 37/64 loss: -0.2969212532043457
Batch 38/64 loss: -0.2847062945365906
Batch 39/64 loss: -0.3031579852104187
Batch 40/64 loss: -0.27507278323173523
Batch 41/64 loss: -0.29259979724884033
Batch 42/64 loss: -0.29653850197792053
Batch 43/64 loss: -0.2983686625957489
Batch 44/64 loss: -0.27998560667037964
Batch 45/64 loss: -0.2766604423522949
Batch 46/64 loss: -0.2935560941696167
Batch 47/64 loss: -0.2852901518344879
Batch 48/64 loss: -0.28352949023246765
Batch 49/64 loss: -0.29166340827941895
Batch 50/64 loss: -0.2893397808074951
Batch 51/64 loss: -0.28045853972435
Batch 52/64 loss: -0.2877207100391388
Batch 53/64 loss: -0.2832704186439514
Batch 54/64 loss: -0.2878669500350952
Batch 55/64 loss: -0.2890838384628296
Batch 56/64 loss: -0.28617608547210693
Batch 57/64 loss: -0.2847875952720642
Batch 58/64 loss: -0.29554319381713867
Batch 59/64 loss: -0.2760148346424103
Batch 60/64 loss: -0.28792423009872437
Batch 61/64 loss: -0.2851763069629669
Batch 62/64 loss: -0.2776181399822235
Batch 63/64 loss: -0.29892686009407043
Batch 64/64 loss: -0.27778467535972595
Epoch 461  Train loss: -0.2902314467757356  Val loss: -0.27458939324949205
Epoch 462
-------------------------------
Batch 1/64 loss: -0.29287517070770264
Batch 2/64 loss: -0.28678104281425476
Batch 3/64 loss: -0.2869429588317871
Batch 4/64 loss: -0.2999451160430908
Batch 5/64 loss: -0.29306909441947937
Batch 6/64 loss: -0.3010339140892029
Batch 7/64 loss: -0.29261690378189087
Batch 8/64 loss: -0.2938676178455353
Batch 9/64 loss: -0.2961263954639435
Batch 10/64 loss: -0.2903652787208557
Batch 11/64 loss: -0.2978500425815582
Batch 12/64 loss: -0.29579296708106995
Batch 13/64 loss: -0.2883293926715851
Batch 14/64 loss: -0.29666703939437866
Batch 15/64 loss: -0.2926802933216095
Batch 16/64 loss: -0.30165910720825195
Batch 17/64 loss: -0.2893581688404083
Batch 18/64 loss: -0.2982366383075714
Batch 19/64 loss: -0.2847173511981964
Batch 20/64 loss: -0.29324930906295776
Batch 21/64 loss: -0.2954629063606262
Batch 22/64 loss: -0.2908092737197876
Batch 23/64 loss: -0.29484254121780396
Batch 24/64 loss: -0.2942909896373749
Batch 25/64 loss: -0.29494670033454895
Batch 26/64 loss: -0.28281959891319275
Batch 27/64 loss: -0.28775322437286377
Batch 28/64 loss: -0.29483503103256226
Batch 29/64 loss: -0.30504685640335083
Batch 30/64 loss: -0.288923442363739
Batch 31/64 loss: -0.2943483591079712
Batch 32/64 loss: -0.28795889019966125
Batch 33/64 loss: -0.28365305066108704
Batch 34/64 loss: -0.29564589262008667
Batch 35/64 loss: -0.2951570451259613
Batch 36/64 loss: -0.29222846031188965
Batch 37/64 loss: -0.2996148467063904
Batch 38/64 loss: -0.29349738359451294
Batch 39/64 loss: -0.29623666405677795
Batch 40/64 loss: -0.2897454500198364
Batch 41/64 loss: -0.2900589108467102
Batch 42/64 loss: -0.2919381856918335
Batch 43/64 loss: -0.2918238639831543
Batch 44/64 loss: -0.29398053884506226
Batch 45/64 loss: -0.29787713289260864
Batch 46/64 loss: -0.30406221747398376
Batch 47/64 loss: -0.28677812218666077
Batch 48/64 loss: -0.2904820144176483
Batch 49/64 loss: -0.30195021629333496
Batch 50/64 loss: -0.29518911242485046
Batch 51/64 loss: -0.2924903929233551
Batch 52/64 loss: -0.28182321786880493
Batch 53/64 loss: -0.2872602939605713
Batch 54/64 loss: -0.29740989208221436
Batch 55/64 loss: -0.3040335178375244
Batch 56/64 loss: -0.2979114055633545
Batch 57/64 loss: -0.2952902913093567
Batch 58/64 loss: -0.2905905544757843
Batch 59/64 loss: -0.282260000705719
Batch 60/64 loss: -0.29940956830978394
Batch 61/64 loss: -0.29436564445495605
Batch 62/64 loss: -0.2848849892616272
Batch 63/64 loss: -0.29489338397979736
Batch 64/64 loss: -0.29944705963134766
Epoch 462  Train loss: -0.2933541830848245  Val loss: -0.27762290599829553
Epoch 463
-------------------------------
Batch 1/64 loss: -0.2946828603744507
Batch 2/64 loss: -0.2899993658065796
Batch 3/64 loss: -0.29258957505226135
Batch 4/64 loss: -0.3054139018058777
Batch 5/64 loss: -0.30118313431739807
Batch 6/64 loss: -0.2969758212566376
Batch 7/64 loss: -0.3066501319408417
Batch 8/64 loss: -0.2902492582798004
Batch 9/64 loss: -0.30641913414001465
Batch 10/64 loss: -0.29328489303588867
Batch 11/64 loss: -0.2946242094039917
Batch 12/64 loss: -0.28988176584243774
Batch 13/64 loss: -0.29690438508987427
Batch 14/64 loss: -0.2895732522010803
Batch 15/64 loss: -0.2919938266277313
Batch 16/64 loss: -0.28437280654907227
Batch 17/64 loss: -0.2786281704902649
Batch 18/64 loss: -0.29319390654563904
Batch 19/64 loss: -0.2939090430736542
Batch 20/64 loss: -0.3038407266139984
Batch 21/64 loss: -0.3084562420845032
Batch 22/64 loss: -0.3026156425476074
Batch 23/64 loss: -0.2966209053993225
Batch 24/64 loss: -0.28791722655296326
Batch 25/64 loss: -0.3001748323440552
Batch 26/64 loss: -0.29888486862182617
Batch 27/64 loss: -0.2986047863960266
Batch 28/64 loss: -0.28515052795410156
Batch 29/64 loss: -0.2890440821647644
Batch 30/64 loss: -0.2995120882987976
Batch 31/64 loss: -0.3041195869445801
Batch 32/64 loss: -0.2858907878398895
Batch 33/64 loss: -0.29226213693618774
Batch 34/64 loss: -0.28241533041000366
Batch 35/64 loss: -0.2949197292327881
Batch 36/64 loss: -0.2945476770401001
Batch 37/64 loss: -0.29462307691574097
Batch 38/64 loss: -0.282503604888916
Batch 39/64 loss: -0.2884419560432434
Batch 40/64 loss: -0.2966383099555969
Batch 41/64 loss: -0.2975896894931793
Batch 42/64 loss: -0.2873345911502838
Batch 43/64 loss: -0.2941824197769165
Batch 44/64 loss: -0.29554247856140137
Batch 45/64 loss: -0.2942727208137512
Batch 46/64 loss: -0.27726054191589355
Batch 47/64 loss: -0.2967037558555603
Batch 48/64 loss: -0.29911836981773376
Batch 49/64 loss: -0.29582372307777405
Batch 50/64 loss: -0.28117263317108154
Batch 51/64 loss: -0.2905294895172119
Batch 52/64 loss: -0.2955179810523987
Batch 53/64 loss: -0.29491642117500305
Batch 54/64 loss: -0.30983904004096985
Batch 55/64 loss: -0.30205321311950684
Batch 56/64 loss: -0.29973703622817993
Batch 57/64 loss: -0.29261529445648193
Batch 58/64 loss: -0.2955945134162903
Batch 59/64 loss: -0.3024289608001709
Batch 60/64 loss: -0.2874491214752197
Batch 61/64 loss: -0.27744734287261963
Batch 62/64 loss: -0.2908746004104614
Batch 63/64 loss: -0.2763858139514923
Batch 64/64 loss: -0.29569724202156067
Epoch 463  Train loss: -0.29386472550092957  Val loss: -0.2714308467517604
Epoch 464
-------------------------------
Batch 1/64 loss: -0.29619020223617554
Batch 2/64 loss: -0.2993955612182617
Batch 3/64 loss: -0.3015431761741638
Batch 4/64 loss: -0.29494810104370117
Batch 5/64 loss: -0.29088544845581055
Batch 6/64 loss: -0.3090234398841858
Batch 7/64 loss: -0.2956114411354065
Batch 8/64 loss: -0.29635587334632874
Batch 9/64 loss: -0.2906074821949005
Batch 10/64 loss: -0.29212698340415955
Batch 11/64 loss: -0.2930777668952942
Batch 12/64 loss: -0.29335731267929077
Batch 13/64 loss: -0.29403454065322876
Batch 14/64 loss: -0.298103928565979
Batch 15/64 loss: -0.2946587800979614
Batch 16/64 loss: -0.29465252161026
Batch 17/64 loss: -0.28717154264450073
Batch 18/64 loss: -0.2957969903945923
Batch 19/64 loss: -0.281339555978775
Batch 20/64 loss: -0.28713202476501465
Batch 21/64 loss: -0.2898241877555847
Batch 22/64 loss: -0.29833194613456726
Batch 23/64 loss: -0.2777433693408966
Batch 24/64 loss: -0.28452879190444946
Batch 25/64 loss: -0.30592525005340576
Batch 26/64 loss: -0.3055877089500427
Batch 27/64 loss: -0.3023378252983093
Batch 28/64 loss: -0.3081827163696289
Batch 29/64 loss: -0.3020695447921753
Batch 30/64 loss: -0.2914388179779053
Batch 31/64 loss: -0.2915500998497009
Batch 32/64 loss: -0.2843691110610962
Batch 33/64 loss: -0.29627564549446106
Batch 34/64 loss: -0.3005028963088989
Batch 35/64 loss: -0.29767829179763794
Batch 36/64 loss: -0.29268890619277954
Batch 37/64 loss: -0.29624584317207336
Batch 38/64 loss: -0.2889699935913086
Batch 39/64 loss: -0.29340702295303345
Batch 40/64 loss: -0.2909742295742035
Batch 41/64 loss: -0.2885056734085083
Batch 42/64 loss: -0.29135552048683167
Batch 43/64 loss: -0.29501521587371826
Batch 44/64 loss: -0.29524409770965576
Batch 45/64 loss: -0.2875652313232422
Batch 46/64 loss: -0.292799711227417
Batch 47/64 loss: -0.30413877964019775
Batch 48/64 loss: -0.28711235523223877
Batch 49/64 loss: -0.29519468545913696
Batch 50/64 loss: -0.28459596633911133
Batch 51/64 loss: -0.2981716990470886
Batch 52/64 loss: -0.30684810876846313
Batch 53/64 loss: -0.29136598110198975
Batch 54/64 loss: -0.2648184299468994
Batch 55/64 loss: -0.2901047468185425
Batch 56/64 loss: -0.30425789952278137
Batch 57/64 loss: -0.3003012239933014
Batch 58/64 loss: -0.29069316387176514
Batch 59/64 loss: -0.3038633167743683
Batch 60/64 loss: -0.2824658155441284
Batch 61/64 loss: -0.29802200198173523
Batch 62/64 loss: -0.30633485317230225
Batch 63/64 loss: -0.30325061082839966
Batch 64/64 loss: -0.2965407371520996
Epoch 464  Train loss: -0.2944168707903694  Val loss: -0.27587165865291846
Epoch 465
-------------------------------
Batch 1/64 loss: -0.291927307844162
Batch 2/64 loss: -0.29319214820861816
Batch 3/64 loss: -0.2968248128890991
Batch 4/64 loss: -0.2928783893585205
Batch 5/64 loss: -0.3006265461444855
Batch 6/64 loss: -0.2967306077480316
Batch 7/64 loss: -0.29903972148895264
Batch 8/64 loss: -0.2920585870742798
Batch 9/64 loss: -0.2921639084815979
Batch 10/64 loss: -0.28538668155670166
Batch 11/64 loss: -0.29741209745407104
Batch 12/64 loss: -0.29108354449272156
Batch 13/64 loss: -0.3035711944103241
Batch 14/64 loss: -0.2968927323818207
Batch 15/64 loss: -0.301556795835495
Batch 16/64 loss: -0.2959006428718567
Batch 17/64 loss: -0.2925897240638733
Batch 18/64 loss: -0.279287725687027
Batch 19/64 loss: -0.2951585650444031
Batch 20/64 loss: -0.29786258935928345
Batch 21/64 loss: -0.2985811233520508
Batch 22/64 loss: -0.29740652441978455
Batch 23/64 loss: -0.2968955636024475
Batch 24/64 loss: -0.3066016435623169
Batch 25/64 loss: -0.2945144474506378
Batch 26/64 loss: -0.2943832278251648
Batch 27/64 loss: -0.297892689704895
Batch 28/64 loss: -0.2990918755531311
Batch 29/64 loss: -0.302976131439209
Batch 30/64 loss: -0.2983004152774811
Batch 31/64 loss: -0.2954871654510498
Batch 32/64 loss: -0.2895483374595642
Batch 33/64 loss: -0.2975955903530121
Batch 34/64 loss: -0.30082404613494873
Batch 35/64 loss: -0.2912699580192566
Batch 36/64 loss: -0.2939794063568115
Batch 37/64 loss: -0.2991189956665039
Batch 38/64 loss: -0.2929813265800476
Batch 39/64 loss: -0.3023713529109955
Batch 40/64 loss: -0.28476011753082275
Batch 41/64 loss: -0.3098326325416565
Batch 42/64 loss: -0.27568531036376953
Batch 43/64 loss: -0.29785001277923584
Batch 44/64 loss: -0.29461610317230225
Batch 45/64 loss: -0.30241382122039795
Batch 46/64 loss: -0.3000241219997406
Batch 47/64 loss: -0.2883673906326294
Batch 48/64 loss: -0.2997669577598572
Batch 49/64 loss: -0.2919235825538635
Batch 50/64 loss: -0.2916472554206848
Batch 51/64 loss: -0.29735803604125977
Batch 52/64 loss: -0.28770554065704346
Batch 53/64 loss: -0.29134422540664673
Batch 54/64 loss: -0.2818862795829773
Batch 55/64 loss: -0.2923475503921509
Batch 56/64 loss: -0.29114899039268494
Batch 57/64 loss: -0.29675379395484924
Batch 58/64 loss: -0.29743146896362305
Batch 59/64 loss: -0.2992793917655945
Batch 60/64 loss: -0.2910577952861786
Batch 61/64 loss: -0.3092455267906189
Batch 62/64 loss: -0.30091384053230286
Batch 63/64 loss: -0.2991017699241638
Batch 64/64 loss: -0.28871285915374756
Epoch 465  Train loss: -0.2953876126046274  Val loss: -0.27767022553178455
Epoch 466
-------------------------------
Batch 1/64 loss: -0.30103951692581177
Batch 2/64 loss: -0.2877112627029419
Batch 3/64 loss: -0.2962282598018646
Batch 4/64 loss: -0.3002411425113678
Batch 5/64 loss: -0.307925820350647
Batch 6/64 loss: -0.2996404767036438
Batch 7/64 loss: -0.2970108985900879
Batch 8/64 loss: -0.29574182629585266
Batch 9/64 loss: -0.29533955454826355
Batch 10/64 loss: -0.29542461037635803
Batch 11/64 loss: -0.29804903268814087
Batch 12/64 loss: -0.3054754137992859
Batch 13/64 loss: -0.2912868857383728
Batch 14/64 loss: -0.30854547023773193
Batch 15/64 loss: -0.29278087615966797
Batch 16/64 loss: -0.31014639139175415
Batch 17/64 loss: -0.3044004440307617
Batch 18/64 loss: -0.300680935382843
Batch 19/64 loss: -0.30877381563186646
Batch 20/64 loss: -0.3070255517959595
Batch 21/64 loss: -0.2964738607406616
Batch 22/64 loss: -0.29079169034957886
Batch 23/64 loss: -0.3060963749885559
Batch 24/64 loss: -0.2757624387741089
Batch 25/64 loss: -0.2912493348121643
Batch 26/64 loss: -0.2910565137863159
Batch 27/64 loss: -0.29994767904281616
Batch 28/64 loss: -0.2992892861366272
Batch 29/64 loss: -0.2919514775276184
Batch 30/64 loss: -0.3059685528278351
Batch 31/64 loss: -0.3070456385612488
Batch 32/64 loss: -0.2938607931137085
Batch 33/64 loss: -0.2967708110809326
Batch 34/64 loss: -0.3001552224159241
Batch 35/64 loss: -0.2985354959964752
Batch 36/64 loss: -0.31302064657211304
Batch 37/64 loss: -0.29799020290374756
Batch 38/64 loss: -0.29216891527175903
Batch 39/64 loss: -0.29754847288131714
Batch 40/64 loss: -0.30568164587020874
Batch 41/64 loss: -0.2741661071777344
Batch 42/64 loss: -0.28410494327545166
Batch 43/64 loss: -0.30323144793510437
Batch 44/64 loss: -0.2983168661594391
Batch 45/64 loss: -0.29919856786727905
Batch 46/64 loss: -0.2989845275878906
Batch 47/64 loss: -0.2927815616130829
Batch 48/64 loss: -0.29980945587158203
Batch 49/64 loss: -0.29437899589538574
Batch 50/64 loss: -0.3009302616119385
Batch 51/64 loss: -0.2964922785758972
Batch 52/64 loss: -0.29344797134399414
Batch 53/64 loss: -0.2936350107192993
Batch 54/64 loss: -0.29817891120910645
Batch 55/64 loss: -0.29700005054473877
Batch 56/64 loss: -0.29949840903282166
Batch 57/64 loss: -0.2922699749469757
Batch 58/64 loss: -0.2900068461894989
Batch 59/64 loss: -0.2941560447216034
Batch 60/64 loss: -0.29072850942611694
Batch 61/64 loss: -0.29320669174194336
Batch 62/64 loss: -0.2978426218032837
Batch 63/64 loss: -0.3027006685733795
Batch 64/64 loss: -0.29965198040008545
Epoch 466  Train loss: -0.2974845325245577  Val loss: -0.2720179697082624
Epoch 467
-------------------------------
Batch 1/64 loss: -0.2877102196216583
Batch 2/64 loss: -0.3152232766151428
Batch 3/64 loss: -0.30373433232307434
Batch 4/64 loss: -0.3095754384994507
Batch 5/64 loss: -0.30273768305778503
Batch 6/64 loss: -0.2999246418476105
Batch 7/64 loss: -0.2926851809024811
Batch 8/64 loss: -0.2909093499183655
Batch 9/64 loss: -0.2958207428455353
Batch 10/64 loss: -0.2963584065437317
Batch 11/64 loss: -0.296456515789032
Batch 12/64 loss: -0.3062599003314972
Batch 13/64 loss: -0.29652464389801025
Batch 14/64 loss: -0.29595574736595154
Batch 15/64 loss: -0.3118774890899658
Batch 16/64 loss: -0.30584022402763367
Batch 17/64 loss: -0.30255749821662903
Batch 18/64 loss: -0.3068104386329651
Batch 19/64 loss: -0.296780526638031
Batch 20/64 loss: -0.3103148639202118
Batch 21/64 loss: -0.30062374472618103
Batch 22/64 loss: -0.3004991412162781
Batch 23/64 loss: -0.28951379656791687
Batch 24/64 loss: -0.3073175251483917
Batch 25/64 loss: -0.2997826635837555
Batch 26/64 loss: -0.30000436305999756
Batch 27/64 loss: -0.2999078035354614
Batch 28/64 loss: -0.3035813868045807
Batch 29/64 loss: -0.30543866753578186
Batch 30/64 loss: -0.3082842230796814
Batch 31/64 loss: -0.2978779077529907
Batch 32/64 loss: -0.3039751648902893
Batch 33/64 loss: -0.30697035789489746
Batch 34/64 loss: -0.3053152561187744
Batch 35/64 loss: -0.3043077290058136
Batch 36/64 loss: -0.2907552123069763
Batch 37/64 loss: -0.30216526985168457
Batch 38/64 loss: -0.2913990020751953
Batch 39/64 loss: -0.30880647897720337
Batch 40/64 loss: -0.30240604281425476
Batch 41/64 loss: -0.30833888053894043
Batch 42/64 loss: -0.29328081011772156
Batch 43/64 loss: -0.2857455611228943
Batch 44/64 loss: -0.3010609447956085
Batch 45/64 loss: -0.29885491728782654
Batch 46/64 loss: -0.28102606534957886
Batch 47/64 loss: -0.2905094027519226
Batch 48/64 loss: -0.3031994104385376
Batch 49/64 loss: -0.30790960788726807
Batch 50/64 loss: -0.293011337518692
Batch 51/64 loss: -0.30066394805908203
Batch 52/64 loss: -0.2826076149940491
Batch 53/64 loss: -0.29690730571746826
Batch 54/64 loss: -0.29317620396614075
Batch 55/64 loss: -0.2869194746017456
Batch 56/64 loss: -0.28612256050109863
Batch 57/64 loss: -0.30640625953674316
Batch 58/64 loss: -0.3046763241291046
Batch 59/64 loss: -0.29065677523612976
Batch 60/64 loss: -0.3018486499786377
Batch 61/64 loss: -0.29489707946777344
Batch 62/64 loss: -0.30991995334625244
Batch 63/64 loss: -0.28798961639404297
Batch 64/64 loss: -0.28847265243530273
Epoch 467  Train loss: -0.2992168164720722  Val loss: -0.2777885661502065
Epoch 468
-------------------------------
Batch 1/64 loss: -0.3081113398075104
Batch 2/64 loss: -0.31606316566467285
Batch 3/64 loss: -0.2875487208366394
Batch 4/64 loss: -0.2828821539878845
Batch 5/64 loss: -0.2913198471069336
Batch 6/64 loss: -0.29496273398399353
Batch 7/64 loss: -0.2978076934814453
Batch 8/64 loss: -0.2987496256828308
Batch 9/64 loss: -0.30708950757980347
Batch 10/64 loss: -0.29522353410720825
Batch 11/64 loss: -0.2864983379840851
Batch 12/64 loss: -0.29365968704223633
Batch 13/64 loss: -0.28818053007125854
Batch 14/64 loss: -0.29348769783973694
Batch 15/64 loss: -0.2980601489543915
Batch 16/64 loss: -0.29550522565841675
Batch 17/64 loss: -0.31267863512039185
Batch 18/64 loss: -0.30037784576416016
Batch 19/64 loss: -0.30375367403030396
Batch 20/64 loss: -0.29708412289619446
Batch 21/64 loss: -0.28857970237731934
Batch 22/64 loss: -0.2946094274520874
Batch 23/64 loss: -0.2939763367176056
Batch 24/64 loss: -0.29920315742492676
Batch 25/64 loss: -0.30882108211517334
Batch 26/64 loss: -0.30851638317108154
Batch 27/64 loss: -0.2810904383659363
Batch 28/64 loss: -0.30057060718536377
Batch 29/64 loss: -0.3004681468009949
Batch 30/64 loss: -0.27895933389663696
Batch 31/64 loss: -0.2844173014163971
Batch 32/64 loss: -0.304939866065979
Batch 33/64 loss: -0.28961583971977234
Batch 34/64 loss: -0.30520692467689514
Batch 35/64 loss: -0.2976644039154053
Batch 36/64 loss: -0.29438695311546326
Batch 37/64 loss: -0.28713950514793396
Batch 38/64 loss: -0.30359914898872375
Batch 39/64 loss: -0.29144662618637085
Batch 40/64 loss: -0.29888463020324707
Batch 41/64 loss: -0.2961341142654419
Batch 42/64 loss: -0.29874488711357117
Batch 43/64 loss: -0.2834617495536804
Batch 44/64 loss: -0.313811719417572
Batch 45/64 loss: -0.30176132917404175
Batch 46/64 loss: -0.29628220200538635
Batch 47/64 loss: -0.28557610511779785
Batch 48/64 loss: -0.2913813591003418
Batch 49/64 loss: -0.2853485345840454
Batch 50/64 loss: -0.29345667362213135
Batch 51/64 loss: -0.3034633994102478
Batch 52/64 loss: -0.30195319652557373
Batch 53/64 loss: -0.30106210708618164
Batch 54/64 loss: -0.29442736506462097
Batch 55/64 loss: -0.3000224232673645
Batch 56/64 loss: -0.28855836391448975
Batch 57/64 loss: -0.2923644185066223
Batch 58/64 loss: -0.2933107912540436
Batch 59/64 loss: -0.3081844747066498
Batch 60/64 loss: -0.30072250962257385
Batch 61/64 loss: -0.29129013419151306
Batch 62/64 loss: -0.2856670916080475
Batch 63/64 loss: -0.2875465154647827
Batch 64/64 loss: -0.304496705532074
Epoch 468  Train loss: -0.29622029860814414  Val loss: -0.2710666892045142
Epoch 469
-------------------------------
Batch 1/64 loss: -0.30011409521102905
Batch 2/64 loss: -0.29656901955604553
Batch 3/64 loss: -0.29709669947624207
Batch 4/64 loss: -0.3050828278064728
Batch 5/64 loss: -0.3013518750667572
Batch 6/64 loss: -0.29638347029685974
Batch 7/64 loss: -0.291703462600708
Batch 8/64 loss: -0.28967785835266113
Batch 9/64 loss: -0.3001117706298828
Batch 10/64 loss: -0.2900160551071167
Batch 11/64 loss: -0.2852179706096649
Batch 12/64 loss: -0.2906697690486908
Batch 13/64 loss: -0.2928377389907837
Batch 14/64 loss: -0.3068315386772156
Batch 15/64 loss: -0.30156102776527405
Batch 16/64 loss: -0.2939896583557129
Batch 17/64 loss: -0.3047448992729187
Batch 18/64 loss: -0.2948678135871887
Batch 19/64 loss: -0.3026406168937683
Batch 20/64 loss: -0.2808394432067871
Batch 21/64 loss: -0.291068971157074
Batch 22/64 loss: -0.287614107131958
Batch 23/64 loss: -0.2893754243850708
Batch 24/64 loss: -0.29641813039779663
Batch 25/64 loss: -0.3011143207550049
Batch 26/64 loss: -0.2888849675655365
Batch 27/64 loss: -0.2982296347618103
Batch 28/64 loss: -0.29692351818084717
Batch 29/64 loss: -0.27809199690818787
Batch 30/64 loss: -0.2889297902584076
Batch 31/64 loss: -0.28831225633621216
Batch 32/64 loss: -0.28915631771087646
Batch 33/64 loss: -0.2871485650539398
Batch 34/64 loss: -0.2981944680213928
Batch 35/64 loss: -0.2885488271713257
Batch 36/64 loss: -0.2934838533401489
Batch 37/64 loss: -0.2966536283493042
Batch 38/64 loss: -0.2825133800506592
Batch 39/64 loss: -0.2948882281780243
Batch 40/64 loss: -0.29831454157829285
Batch 41/64 loss: -0.3047941327095032
Batch 42/64 loss: -0.30194199085235596
Batch 43/64 loss: -0.28608620166778564
Batch 44/64 loss: -0.27559542655944824
Batch 45/64 loss: -0.2937971353530884
Batch 46/64 loss: -0.287275493144989
Batch 47/64 loss: -0.29955875873565674
Batch 48/64 loss: -0.28273820877075195
Batch 49/64 loss: -0.30197280645370483
Batch 50/64 loss: -0.2912980318069458
Batch 51/64 loss: -0.2873568534851074
Batch 52/64 loss: -0.2968672215938568
Batch 53/64 loss: -0.2895861864089966
Batch 54/64 loss: -0.30144935846328735
Batch 55/64 loss: -0.3005326986312866
Batch 56/64 loss: -0.29205697774887085
Batch 57/64 loss: -0.2984330952167511
Batch 58/64 loss: -0.30406010150909424
Batch 59/64 loss: -0.2945963740348816
Batch 60/64 loss: -0.28285518288612366
Batch 61/64 loss: -0.29531925916671753
Batch 62/64 loss: -0.28982794284820557
Batch 63/64 loss: -0.29509204626083374
Batch 64/64 loss: -0.2949593663215637
Epoch 469  Train loss: -0.293811506617303  Val loss: -0.26765849520660345
Epoch 470
-------------------------------
Batch 1/64 loss: -0.2905377149581909
Batch 2/64 loss: -0.29915302991867065
Batch 3/64 loss: -0.29767903685569763
Batch 4/64 loss: -0.2921738624572754
Batch 5/64 loss: -0.29921793937683105
Batch 6/64 loss: -0.28585633635520935
Batch 7/64 loss: -0.2937740087509155
Batch 8/64 loss: -0.29714322090148926
Batch 9/64 loss: -0.2920629382133484
Batch 10/64 loss: -0.27620792388916016
Batch 11/64 loss: -0.2995870113372803
Batch 12/64 loss: -0.2907530665397644
Batch 13/64 loss: -0.29923146963119507
Batch 14/64 loss: -0.298520565032959
Batch 15/64 loss: -0.2818603515625
Batch 16/64 loss: -0.29088443517684937
Batch 17/64 loss: -0.29133743047714233
Batch 18/64 loss: -0.29950618743896484
Batch 19/64 loss: -0.2985133230686188
Batch 20/64 loss: -0.2984302043914795
Batch 21/64 loss: -0.2997075617313385
Batch 22/64 loss: -0.2951200008392334
Batch 23/64 loss: -0.2960786819458008
Batch 24/64 loss: -0.2932065725326538
Batch 25/64 loss: -0.29144495725631714
Batch 26/64 loss: -0.29745447635650635
Batch 27/64 loss: -0.2968185544013977
Batch 28/64 loss: -0.2888711988925934
Batch 29/64 loss: -0.302556574344635
Batch 30/64 loss: -0.29260748624801636
Batch 31/64 loss: -0.29671114683151245
Batch 32/64 loss: -0.28645551204681396
Batch 33/64 loss: -0.3078436851501465
Batch 34/64 loss: -0.29739901423454285
Batch 35/64 loss: -0.300154447555542
Batch 36/64 loss: -0.3104739189147949
Batch 37/64 loss: -0.29244303703308105
Batch 38/64 loss: -0.29711171984672546
Batch 39/64 loss: -0.28945019841194153
Batch 40/64 loss: -0.28333938121795654
Batch 41/64 loss: -0.3044649362564087
Batch 42/64 loss: -0.28942611813545227
Batch 43/64 loss: -0.2855720520019531
Batch 44/64 loss: -0.28022733330726624
Batch 45/64 loss: -0.277778685092926
Batch 46/64 loss: -0.2976410984992981
Batch 47/64 loss: -0.2879974842071533
Batch 48/64 loss: -0.2969006896018982
Batch 49/64 loss: -0.2831372022628784
Batch 50/64 loss: -0.3030136227607727
Batch 51/64 loss: -0.2916293740272522
Batch 52/64 loss: -0.30035078525543213
Batch 53/64 loss: -0.28665685653686523
Batch 54/64 loss: -0.2937560975551605
Batch 55/64 loss: -0.3010179400444031
Batch 56/64 loss: -0.30040913820266724
Batch 57/64 loss: -0.298615425825119
Batch 58/64 loss: -0.28648635745048523
Batch 59/64 loss: -0.3023754358291626
Batch 60/64 loss: -0.28080183267593384
Batch 61/64 loss: -0.2865299880504608
Batch 62/64 loss: -0.28753334283828735
Batch 63/64 loss: -0.3003738522529602
Batch 64/64 loss: -0.28115755319595337
Epoch 470  Train loss: -0.29366654110889806  Val loss: -0.2780658193060623
Epoch 471
-------------------------------
Batch 1/64 loss: -0.29170575737953186
Batch 2/64 loss: -0.2975805103778839
Batch 3/64 loss: -0.292095422744751
Batch 4/64 loss: -0.2975125014781952
Batch 5/64 loss: -0.292414128780365
Batch 6/64 loss: -0.2994978427886963
Batch 7/64 loss: -0.29061052203178406
Batch 8/64 loss: -0.2974914312362671
Batch 9/64 loss: -0.3019912540912628
Batch 10/64 loss: -0.3006460666656494
Batch 11/64 loss: -0.28850239515304565
Batch 12/64 loss: -0.30123770236968994
Batch 13/64 loss: -0.29484590888023376
Batch 14/64 loss: -0.28870224952697754
Batch 15/64 loss: -0.2967851758003235
Batch 16/64 loss: -0.2852102518081665
Batch 17/64 loss: -0.29528018832206726
Batch 18/64 loss: -0.3015391230583191
Batch 19/64 loss: -0.29797568917274475
Batch 20/64 loss: -0.300469309091568
Batch 21/64 loss: -0.2837894856929779
Batch 22/64 loss: -0.2908238470554352
Batch 23/64 loss: -0.30079659819602966
Batch 24/64 loss: -0.2949955463409424
Batch 25/64 loss: -0.29842108488082886
Batch 26/64 loss: -0.3015081584453583
Batch 27/64 loss: -0.2865440249443054
Batch 28/64 loss: -0.2958368957042694
Batch 29/64 loss: -0.28964871168136597
Batch 30/64 loss: -0.2986789345741272
Batch 31/64 loss: -0.2916373014450073
Batch 32/64 loss: -0.2935658097267151
Batch 33/64 loss: -0.28116700053215027
Batch 34/64 loss: -0.2882949411869049
Batch 35/64 loss: -0.2702055871486664
Batch 36/64 loss: -0.2947457730770111
Batch 37/64 loss: -0.2873491048812866
Batch 38/64 loss: -0.2987264394760132
Batch 39/64 loss: -0.3005729913711548
Batch 40/64 loss: -0.2987595796585083
Batch 41/64 loss: -0.28428712487220764
Batch 42/64 loss: -0.2920227646827698
Batch 43/64 loss: -0.2924221158027649
Batch 44/64 loss: -0.2849125564098358
Batch 45/64 loss: -0.28430458903312683
Batch 46/64 loss: -0.2899872362613678
Batch 47/64 loss: -0.27037346363067627
Batch 48/64 loss: -0.2904212474822998
Batch 49/64 loss: -0.29162323474884033
Batch 50/64 loss: -0.30049067735671997
Batch 51/64 loss: -0.2962615191936493
Batch 52/64 loss: -0.28443053364753723
Batch 53/64 loss: -0.2872142195701599
Batch 54/64 loss: -0.2943618893623352
Batch 55/64 loss: -0.2860732078552246
Batch 56/64 loss: -0.3008243441581726
Batch 57/64 loss: -0.3036364018917084
Batch 58/64 loss: -0.28493160009384155
Batch 59/64 loss: -0.30261582136154175
Batch 60/64 loss: -0.30451899766921997
Batch 61/64 loss: -0.2836908996105194
Batch 62/64 loss: -0.2938910126686096
Batch 63/64 loss: -0.29544055461883545
Batch 64/64 loss: -0.29365044832229614
Epoch 471  Train loss: -0.29297466815686696  Val loss: -0.2648488338460627
Epoch 472
-------------------------------
Batch 1/64 loss: -0.2996252775192261
Batch 2/64 loss: -0.28493404388427734
Batch 3/64 loss: -0.3000595271587372
Batch 4/64 loss: -0.296427845954895
Batch 5/64 loss: -0.2852007746696472
Batch 6/64 loss: -0.29723188281059265
Batch 7/64 loss: -0.28435540199279785
Batch 8/64 loss: -0.2943163812160492
Batch 9/64 loss: -0.2873263359069824
Batch 10/64 loss: -0.2891409993171692
Batch 11/64 loss: -0.29975152015686035
Batch 12/64 loss: -0.3002627491950989
Batch 13/64 loss: -0.29582899808883667
Batch 14/64 loss: -0.29626157879829407
Batch 15/64 loss: -0.2935303747653961
Batch 16/64 loss: -0.2753196060657501
Batch 17/64 loss: -0.29459908604621887
Batch 18/64 loss: -0.3035967946052551
Batch 19/64 loss: -0.29681694507598877
Batch 20/64 loss: -0.304629921913147
Batch 21/64 loss: -0.3083609342575073
Batch 22/64 loss: -0.30492687225341797
Batch 23/64 loss: -0.2942330241203308
Batch 24/64 loss: -0.2932141423225403
Batch 25/64 loss: -0.28589046001434326
Batch 26/64 loss: -0.2996944189071655
Batch 27/64 loss: -0.29092609882354736
Batch 28/64 loss: -0.29841095209121704
Batch 29/64 loss: -0.3024011552333832
Batch 30/64 loss: -0.28588202595710754
Batch 31/64 loss: -0.30533838272094727
Batch 32/64 loss: -0.2962709069252014
Batch 33/64 loss: -0.2865673899650574
Batch 34/64 loss: -0.2871553301811218
Batch 35/64 loss: -0.29329442977905273
Batch 36/64 loss: -0.2972744107246399
Batch 37/64 loss: -0.28671276569366455
Batch 38/64 loss: -0.291644811630249
Batch 39/64 loss: -0.308044970035553
Batch 40/64 loss: -0.2959405779838562
Batch 41/64 loss: -0.293770432472229
Batch 42/64 loss: -0.30156561732292175
Batch 43/64 loss: -0.2769402265548706
Batch 44/64 loss: -0.2987036406993866
Batch 45/64 loss: -0.29249221086502075
Batch 46/64 loss: -0.29135841131210327
Batch 47/64 loss: -0.2905045747756958
Batch 48/64 loss: -0.2946341633796692
Batch 49/64 loss: -0.29362863302230835
Batch 50/64 loss: -0.28446710109710693
Batch 51/64 loss: -0.29307764768600464
Batch 52/64 loss: -0.30175304412841797
Batch 53/64 loss: -0.2898273468017578
Batch 54/64 loss: -0.28688549995422363
Batch 55/64 loss: -0.3025054931640625
Batch 56/64 loss: -0.2842698097229004
Batch 57/64 loss: -0.2968817353248596
Batch 58/64 loss: -0.2876742482185364
Batch 59/64 loss: -0.287605345249176
Batch 60/64 loss: -0.3066045045852661
Batch 61/64 loss: -0.29154813289642334
Batch 62/64 loss: -0.2914642095565796
Batch 63/64 loss: -0.28396034240722656
Batch 64/64 loss: -0.28700101375579834
Epoch 472  Train loss: -0.293784678216074  Val loss: -0.27304170826046736
Epoch 473
-------------------------------
Batch 1/64 loss: -0.29998862743377686
Batch 2/64 loss: -0.30299779772758484
Batch 3/64 loss: -0.2868603467941284
Batch 4/64 loss: -0.3084932863712311
Batch 5/64 loss: -0.2861747741699219
Batch 6/64 loss: -0.2910314202308655
Batch 7/64 loss: -0.2924104332923889
Batch 8/64 loss: -0.27665066719055176
Batch 9/64 loss: -0.2901467978954315
Batch 10/64 loss: -0.28787559270858765
Batch 11/64 loss: -0.28775978088378906
Batch 12/64 loss: -0.28325939178466797
Batch 13/64 loss: -0.2890365421772003
Batch 14/64 loss: -0.29402419924736023
Batch 15/64 loss: -0.28414463996887207
Batch 16/64 loss: -0.28654223680496216
Batch 17/64 loss: -0.2857210636138916
Batch 18/64 loss: -0.2942436635494232
Batch 19/64 loss: -0.3093400001525879
Batch 20/64 loss: -0.30823206901550293
Batch 21/64 loss: -0.2892932891845703
Batch 22/64 loss: -0.28420817852020264
Batch 23/64 loss: -0.29317402839660645
Batch 24/64 loss: -0.2914202809333801
Batch 25/64 loss: -0.3005056083202362
Batch 26/64 loss: -0.2864971160888672
Batch 27/64 loss: -0.28300994634628296
Batch 28/64 loss: -0.2984590530395508
Batch 29/64 loss: -0.2870025932788849
Batch 30/64 loss: -0.29708555340766907
Batch 31/64 loss: -0.2947980761528015
Batch 32/64 loss: -0.3003578782081604
Batch 33/64 loss: -0.3022891879081726
Batch 34/64 loss: -0.2853407859802246
Batch 35/64 loss: -0.2948856055736542
Batch 36/64 loss: -0.28599533438682556
Batch 37/64 loss: -0.2890901267528534
Batch 38/64 loss: -0.30378758907318115
Batch 39/64 loss: -0.30042213201522827
Batch 40/64 loss: -0.29729247093200684
Batch 41/64 loss: -0.2857021391391754
Batch 42/64 loss: -0.29143089056015015
Batch 43/64 loss: -0.2899029850959778
Batch 44/64 loss: -0.2931099832057953
Batch 45/64 loss: -0.2901420593261719
Batch 46/64 loss: -0.2843981981277466
Batch 47/64 loss: -0.2848536968231201
Batch 48/64 loss: -0.2792348861694336
Batch 49/64 loss: -0.29647189378738403
Batch 50/64 loss: -0.2987847328186035
Batch 51/64 loss: -0.29286593198776245
Batch 52/64 loss: -0.2895745038986206
Batch 53/64 loss: -0.2887282967567444
Batch 54/64 loss: -0.31192538142204285
Batch 55/64 loss: -0.28808003664016724
Batch 56/64 loss: -0.28626424074172974
Batch 57/64 loss: -0.29804888367652893
Batch 58/64 loss: -0.3009014129638672
Batch 59/64 loss: -0.28479868173599243
Batch 60/64 loss: -0.2964913249015808
Batch 61/64 loss: -0.2899550199508667
Batch 62/64 loss: -0.2849738597869873
Batch 63/64 loss: -0.29994088411331177
Batch 64/64 loss: -0.31093910336494446
Epoch 473  Train loss: -0.2925432535947538  Val loss: -0.265908575959222
Epoch 474
-------------------------------
Batch 1/64 loss: -0.2886153757572174
Batch 2/64 loss: -0.30050355195999146
Batch 3/64 loss: -0.2875088155269623
Batch 4/64 loss: -0.30085787177085876
Batch 5/64 loss: -0.291340172290802
Batch 6/64 loss: -0.2912776470184326
Batch 7/64 loss: -0.3018345832824707
Batch 8/64 loss: -0.3002724349498749
Batch 9/64 loss: -0.3018589913845062
Batch 10/64 loss: -0.30401140451431274
Batch 11/64 loss: -0.3023771643638611
Batch 12/64 loss: -0.30497652292251587
Batch 13/64 loss: -0.29899996519088745
Batch 14/64 loss: -0.28637558221817017
Batch 15/64 loss: -0.3106423616409302
Batch 16/64 loss: -0.30046048760414124
Batch 17/64 loss: -0.291257381439209
Batch 18/64 loss: -0.2911692261695862
Batch 19/64 loss: -0.29316073656082153
Batch 20/64 loss: -0.30107787251472473
Batch 21/64 loss: -0.3039078116416931
Batch 22/64 loss: -0.2888990044593811
Batch 23/64 loss: -0.2919725179672241
Batch 24/64 loss: -0.2916516661643982
Batch 25/64 loss: -0.2982858419418335
Batch 26/64 loss: -0.2976665496826172
Batch 27/64 loss: -0.28166401386260986
Batch 28/64 loss: -0.2916524112224579
Batch 29/64 loss: -0.2846619486808777
Batch 30/64 loss: -0.2957266569137573
Batch 31/64 loss: -0.29349178075790405
Batch 32/64 loss: -0.2941821813583374
Batch 33/64 loss: -0.29228925704956055
Batch 34/64 loss: -0.29257023334503174
Batch 35/64 loss: -0.2968723773956299
Batch 36/64 loss: -0.2844039797782898
Batch 37/64 loss: -0.2938716411590576
Batch 38/64 loss: -0.288193017244339
Batch 39/64 loss: -0.28584039211273193
Batch 40/64 loss: -0.2993561625480652
Batch 41/64 loss: -0.28607964515686035
Batch 42/64 loss: -0.2920994460582733
Batch 43/64 loss: -0.2993091940879822
Batch 44/64 loss: -0.2944122552871704
Batch 45/64 loss: -0.2978816628456116
Batch 46/64 loss: -0.29843777418136597
Batch 47/64 loss: -0.31075000762939453
Batch 48/64 loss: -0.29624414443969727
Batch 49/64 loss: -0.2939709722995758
Batch 50/64 loss: -0.2867952585220337
Batch 51/64 loss: -0.30130505561828613
Batch 52/64 loss: -0.28956663608551025
Batch 53/64 loss: -0.29594504833221436
Batch 54/64 loss: -0.30334722995758057
Batch 55/64 loss: -0.2949982285499573
Batch 56/64 loss: -0.2856394648551941
Batch 57/64 loss: -0.2959263324737549
Batch 58/64 loss: -0.2849224805831909
Batch 59/64 loss: -0.2886618971824646
Batch 60/64 loss: -0.29695090651512146
Batch 61/64 loss: -0.27836722135543823
Batch 62/64 loss: -0.2930997610092163
Batch 63/64 loss: -0.2930167317390442
Batch 64/64 loss: -0.28999054431915283
Epoch 474  Train loss: -0.29444639776267256  Val loss: -0.27268304560602324
Epoch 475
-------------------------------
Batch 1/64 loss: -0.2944871783256531
Batch 2/64 loss: -0.2982732057571411
Batch 3/64 loss: -0.27949249744415283
Batch 4/64 loss: -0.2872885465621948
Batch 5/64 loss: -0.2820467948913574
Batch 6/64 loss: -0.3133718967437744
Batch 7/64 loss: -0.28761225938796997
Batch 8/64 loss: -0.2942966818809509
Batch 9/64 loss: -0.28753527998924255
Batch 10/64 loss: -0.2959684729576111
Batch 11/64 loss: -0.28685247898101807
Batch 12/64 loss: -0.30019405484199524
Batch 13/64 loss: -0.2995399236679077
Batch 14/64 loss: -0.2890464663505554
Batch 15/64 loss: -0.29595035314559937
Batch 16/64 loss: -0.29588818550109863
Batch 17/64 loss: -0.28984272480010986
Batch 18/64 loss: -0.29686975479125977
Batch 19/64 loss: -0.29847025871276855
Batch 20/64 loss: -0.2999933063983917
Batch 21/64 loss: -0.3126407861709595
Batch 22/64 loss: -0.28982263803482056
Batch 23/64 loss: -0.29102665185928345
Batch 24/64 loss: -0.30783531069755554
Batch 25/64 loss: -0.2973012328147888
Batch 26/64 loss: -0.30779409408569336
Batch 27/64 loss: -0.285555899143219
Batch 28/64 loss: -0.3000086545944214
Batch 29/64 loss: -0.3007534146308899
Batch 30/64 loss: -0.2931949496269226
Batch 31/64 loss: -0.30785784125328064
Batch 32/64 loss: -0.2938568592071533
Batch 33/64 loss: -0.283927321434021
Batch 34/64 loss: -0.2889147400856018
Batch 35/64 loss: -0.2986563742160797
Batch 36/64 loss: -0.29608938097953796
Batch 37/64 loss: -0.29585450887680054
Batch 38/64 loss: -0.2977680265903473
Batch 39/64 loss: -0.3048793077468872
Batch 40/64 loss: -0.30401360988616943
Batch 41/64 loss: -0.2847321629524231
Batch 42/64 loss: -0.29000335931777954
Batch 43/64 loss: -0.2894752323627472
Batch 44/64 loss: -0.3026142716407776
Batch 45/64 loss: -0.3036996126174927
Batch 46/64 loss: -0.29335498809814453
Batch 47/64 loss: -0.2969095706939697
Batch 48/64 loss: -0.29283809661865234
Batch 49/64 loss: -0.292542040348053
Batch 50/64 loss: -0.3039904832839966
Batch 51/64 loss: -0.3089582324028015
Batch 52/64 loss: -0.30053603649139404
Batch 53/64 loss: -0.2909746468067169
Batch 54/64 loss: -0.2991805076599121
Batch 55/64 loss: -0.2942858934402466
Batch 56/64 loss: -0.29540592432022095
Batch 57/64 loss: -0.29683107137680054
Batch 58/64 loss: -0.30081313848495483
Batch 59/64 loss: -0.28874772787094116
Batch 60/64 loss: -0.30413663387298584
Batch 61/64 loss: -0.30246561765670776
Batch 62/64 loss: -0.29755699634552
Batch 63/64 loss: -0.290036141872406
Batch 64/64 loss: -0.30002710223197937
Epoch 475  Train loss: -0.2960922452748991  Val loss: -0.2587557623476507
Epoch 476
-------------------------------
Batch 1/64 loss: -0.304460346698761
Batch 2/64 loss: -0.29637062549591064
Batch 3/64 loss: -0.30259132385253906
Batch 4/64 loss: -0.29871866106987
Batch 5/64 loss: -0.29621899127960205
Batch 6/64 loss: -0.28829461336135864
Batch 7/64 loss: -0.29739680886268616
Batch 8/64 loss: -0.3024570047855377
Batch 9/64 loss: -0.28894302248954773
Batch 10/64 loss: -0.3020917475223541
Batch 11/64 loss: -0.29678845405578613
Batch 12/64 loss: -0.29347342252731323
Batch 13/64 loss: -0.2954661548137665
Batch 14/64 loss: -0.30009156465530396
Batch 15/64 loss: -0.3023046553134918
Batch 16/64 loss: -0.30211126804351807
Batch 17/64 loss: -0.28853166103363037
Batch 18/64 loss: -0.29157689213752747
Batch 19/64 loss: -0.29224202036857605
Batch 20/64 loss: -0.2890228033065796
Batch 21/64 loss: -0.3039645552635193
Batch 22/64 loss: -0.3081384301185608
Batch 23/64 loss: -0.30299314856529236
Batch 24/64 loss: -0.2985641062259674
Batch 25/64 loss: -0.3086874485015869
Batch 26/64 loss: -0.2833939492702484
Batch 27/64 loss: -0.313105046749115
Batch 28/64 loss: -0.2959232032299042
Batch 29/64 loss: -0.2921087145805359
Batch 30/64 loss: -0.3097362816333771
Batch 31/64 loss: -0.29706189036369324
Batch 32/64 loss: -0.2924546003341675
Batch 33/64 loss: -0.2933521866798401
Batch 34/64 loss: -0.29525694251060486
Batch 35/64 loss: -0.2813895344734192
Batch 36/64 loss: -0.2873668074607849
Batch 37/64 loss: -0.2770575284957886
Batch 38/64 loss: -0.288427472114563
Batch 39/64 loss: -0.29099005460739136
Batch 40/64 loss: -0.30731356143951416
Batch 41/64 loss: -0.29209035634994507
Batch 42/64 loss: -0.2785956859588623
Batch 43/64 loss: -0.2901606857776642
Batch 44/64 loss: -0.2928532063961029
Batch 45/64 loss: -0.30050069093704224
Batch 46/64 loss: -0.3036333918571472
Batch 47/64 loss: -0.29283860325813293
Batch 48/64 loss: -0.29587557911872864
Batch 49/64 loss: -0.2920868992805481
Batch 50/64 loss: -0.2871595621109009
Batch 51/64 loss: -0.2885206937789917
Batch 52/64 loss: -0.29304039478302
Batch 53/64 loss: -0.2920764684677124
Batch 54/64 loss: -0.29912176728248596
Batch 55/64 loss: -0.29280155897140503
Batch 56/64 loss: -0.2878456115722656
Batch 57/64 loss: -0.3038164973258972
Batch 58/64 loss: -0.28882795572280884
Batch 59/64 loss: -0.2669823169708252
Batch 60/64 loss: -0.281327486038208
Batch 61/64 loss: -0.2861718535423279
Batch 62/64 loss: -0.287086546421051
Batch 63/64 loss: -0.2860105633735657
Batch 64/64 loss: -0.30344918370246887
Epoch 476  Train loss: -0.29432860811551415  Val loss: -0.27525123411027835
Epoch 477
-------------------------------
Batch 1/64 loss: -0.2742927074432373
Batch 2/64 loss: -0.282584547996521
Batch 3/64 loss: -0.28953272104263306
Batch 4/64 loss: -0.2853112816810608
Batch 5/64 loss: -0.2695501744747162
Batch 6/64 loss: -0.29200828075408936
Batch 7/64 loss: -0.29143035411834717
Batch 8/64 loss: -0.2923435568809509
Batch 9/64 loss: -0.2894492745399475
Batch 10/64 loss: -0.2982020378112793
Batch 11/64 loss: -0.28510522842407227
Batch 12/64 loss: -0.2934733033180237
Batch 13/64 loss: -0.2966260015964508
Batch 14/64 loss: -0.2934775650501251
Batch 15/64 loss: -0.291696697473526
Batch 16/64 loss: -0.28827351331710815
Batch 17/64 loss: -0.30948758125305176
Batch 18/64 loss: -0.29708418250083923
Batch 19/64 loss: -0.30111345648765564
Batch 20/64 loss: -0.2883217930793762
Batch 21/64 loss: -0.29779213666915894
Batch 22/64 loss: -0.2812000513076782
Batch 23/64 loss: -0.2868868112564087
Batch 24/64 loss: -0.2993921637535095
Batch 25/64 loss: -0.285275399684906
Batch 26/64 loss: -0.28244441747665405
Batch 27/64 loss: -0.2795144021511078
Batch 28/64 loss: -0.30375075340270996
Batch 29/64 loss: -0.28974485397338867
Batch 30/64 loss: -0.288550466299057
Batch 31/64 loss: -0.2887452244758606
Batch 32/64 loss: -0.3037451207637787
Batch 33/64 loss: -0.2916233539581299
Batch 34/64 loss: -0.29814600944519043
Batch 35/64 loss: -0.3020813465118408
Batch 36/64 loss: -0.29154887795448303
Batch 37/64 loss: -0.29465848207473755
Batch 38/64 loss: -0.29376038908958435
Batch 39/64 loss: -0.26384663581848145
Batch 40/64 loss: -0.3003593981266022
Batch 41/64 loss: -0.29429498314857483
Batch 42/64 loss: -0.30022603273391724
Batch 43/64 loss: -0.29518622159957886
Batch 44/64 loss: -0.29523515701293945
Batch 45/64 loss: -0.2904024124145508
Batch 46/64 loss: -0.3073866367340088
Batch 47/64 loss: -0.29031267762184143
Batch 48/64 loss: -0.2931522727012634
Batch 49/64 loss: -0.29243141412734985
Batch 50/64 loss: -0.3057725429534912
Batch 51/64 loss: -0.28429102897644043
Batch 52/64 loss: -0.29554224014282227
Batch 53/64 loss: -0.3082674741744995
Batch 54/64 loss: -0.3034130334854126
Batch 55/64 loss: -0.30677515268325806
Batch 56/64 loss: -0.3033629059791565
Batch 57/64 loss: -0.2994357943534851
Batch 58/64 loss: -0.30184078216552734
Batch 59/64 loss: -0.29216668009757996
Batch 60/64 loss: -0.29608023166656494
Batch 61/64 loss: -0.2969598174095154
Batch 62/64 loss: -0.28374460339546204
Batch 63/64 loss: -0.2841845452785492
Batch 64/64 loss: -0.29409900307655334
Epoch 477  Train loss: -0.2929171677897958  Val loss: -0.27009013346380384
Epoch 478
-------------------------------
Batch 1/64 loss: -0.3030305504798889
Batch 2/64 loss: -0.3008076250553131
Batch 3/64 loss: -0.3024313449859619
Batch 4/64 loss: -0.29587990045547485
Batch 5/64 loss: -0.29556185007095337
Batch 6/64 loss: -0.2994302809238434
Batch 7/64 loss: -0.2977239489555359
Batch 8/64 loss: -0.30327051877975464
Batch 9/64 loss: -0.29617923498153687
Batch 10/64 loss: -0.28714391589164734
Batch 11/64 loss: -0.29366588592529297
Batch 12/64 loss: -0.29742348194122314
Batch 13/64 loss: -0.2915404140949249
Batch 14/64 loss: -0.3006063997745514
Batch 15/64 loss: -0.29646024107933044
Batch 16/64 loss: -0.2859756052494049
Batch 17/64 loss: -0.286613404750824
Batch 18/64 loss: -0.29822421073913574
Batch 19/64 loss: -0.2805666923522949
Batch 20/64 loss: -0.2942458391189575
Batch 21/64 loss: -0.31495311856269836
Batch 22/64 loss: -0.29064804315567017
Batch 23/64 loss: -0.2986975312232971
Batch 24/64 loss: -0.302076518535614
Batch 25/64 loss: -0.3021855056285858
Batch 26/64 loss: -0.29679572582244873
Batch 27/64 loss: -0.29103684425354004
Batch 28/64 loss: -0.29032132029533386
Batch 29/64 loss: -0.2884882092475891
Batch 30/64 loss: -0.293936550617218
Batch 31/64 loss: -0.30024880170822144
Batch 32/64 loss: -0.2982032597064972
Batch 33/64 loss: -0.30341774225234985
Batch 34/64 loss: -0.2973670959472656
Batch 35/64 loss: -0.2943454384803772
Batch 36/64 loss: -0.29567793011665344
Batch 37/64 loss: -0.29055535793304443
Batch 38/64 loss: -0.3061574697494507
Batch 39/64 loss: -0.29450246691703796
Batch 40/64 loss: -0.29561087489128113
Batch 41/64 loss: -0.2961069345474243
Batch 42/64 loss: -0.27987465262413025
Batch 43/64 loss: -0.2908540666103363
Batch 44/64 loss: -0.2858833074569702
Batch 45/64 loss: -0.29353803396224976
Batch 46/64 loss: -0.2865634858608246
Batch 47/64 loss: -0.29239970445632935
Batch 48/64 loss: -0.285602331161499
Batch 49/64 loss: -0.29701972007751465
Batch 50/64 loss: -0.28833842277526855
Batch 51/64 loss: -0.30390962958335876
Batch 52/64 loss: -0.294344961643219
Batch 53/64 loss: -0.28504449129104614
Batch 54/64 loss: -0.2972959876060486
Batch 55/64 loss: -0.29466766119003296
Batch 56/64 loss: -0.29104480147361755
Batch 57/64 loss: -0.2970040440559387
Batch 58/64 loss: -0.2936280369758606
Batch 59/64 loss: -0.2881697118282318
Batch 60/64 loss: -0.3008809983730316
Batch 61/64 loss: -0.30035775899887085
Batch 62/64 loss: -0.29683399200439453
Batch 63/64 loss: -0.2898930609226227
Batch 64/64 loss: -0.2908920645713806
Epoch 478  Train loss: -0.2948930508950177  Val loss: -0.26854325938470586
Epoch 479
-------------------------------
Batch 1/64 loss: -0.2896536588668823
Batch 2/64 loss: -0.30348092317581177
Batch 3/64 loss: -0.2887544631958008
Batch 4/64 loss: -0.2866024374961853
Batch 5/64 loss: -0.305836021900177
Batch 6/64 loss: -0.28403013944625854
Batch 7/64 loss: -0.298123300075531
Batch 8/64 loss: -0.28436437249183655
Batch 9/64 loss: -0.2932640612125397
Batch 10/64 loss: -0.2907111644744873
Batch 11/64 loss: -0.2909770607948303
Batch 12/64 loss: -0.28784388303756714
Batch 13/64 loss: -0.29050350189208984
Batch 14/64 loss: -0.3030535578727722
Batch 15/64 loss: -0.29184746742248535
Batch 16/64 loss: -0.27630937099456787
Batch 17/64 loss: -0.3058532178401947
Batch 18/64 loss: -0.2960165739059448
Batch 19/64 loss: -0.27881255745887756
Batch 20/64 loss: -0.2901189625263214
Batch 21/64 loss: -0.27040812373161316
Batch 22/64 loss: -0.2936605215072632
Batch 23/64 loss: -0.3116651475429535
Batch 24/64 loss: -0.28577274084091187
Batch 25/64 loss: -0.29838597774505615
Batch 26/64 loss: -0.2976202964782715
Batch 27/64 loss: -0.2982257604598999
Batch 28/64 loss: -0.3084399998188019
Batch 29/64 loss: -0.30147185921669006
Batch 30/64 loss: -0.29503804445266724
Batch 31/64 loss: -0.2850457727909088
Batch 32/64 loss: -0.2919633984565735
Batch 33/64 loss: -0.3038303852081299
Batch 34/64 loss: -0.2892615497112274
Batch 35/64 loss: -0.29796096682548523
Batch 36/64 loss: -0.2843676805496216
Batch 37/64 loss: -0.2865561842918396
Batch 38/64 loss: -0.2864830493927002
Batch 39/64 loss: -0.3037538528442383
Batch 40/64 loss: -0.30308234691619873
Batch 41/64 loss: -0.2959417998790741
Batch 42/64 loss: -0.2867768108844757
Batch 43/64 loss: -0.26933425664901733
Batch 44/64 loss: -0.30117976665496826
Batch 45/64 loss: -0.30068379640579224
Batch 46/64 loss: -0.29940932989120483
Batch 47/64 loss: -0.29383397102355957
Batch 48/64 loss: -0.29312241077423096
Batch 49/64 loss: -0.2947341799736023
Batch 50/64 loss: -0.304954469203949
Batch 51/64 loss: -0.27120158076286316
Batch 52/64 loss: -0.30669981241226196
Batch 53/64 loss: -0.292011559009552
Batch 54/64 loss: -0.2991044223308563
Batch 55/64 loss: -0.30467721819877625
Batch 56/64 loss: -0.29250460863113403
Batch 57/64 loss: -0.29805588722229004
Batch 58/64 loss: -0.2876087427139282
Batch 59/64 loss: -0.2945282459259033
Batch 60/64 loss: -0.30308037996292114
Batch 61/64 loss: -0.2951548099517822
Batch 62/64 loss: -0.29749995470046997
Batch 63/64 loss: -0.29543399810791016
Batch 64/64 loss: -0.2929551601409912
Epoch 479  Train loss: -0.29374743115668206  Val loss: -0.2641639924540962
Epoch 480
-------------------------------
Batch 1/64 loss: -0.2933769226074219
Batch 2/64 loss: -0.2731342017650604
Batch 3/64 loss: -0.2909233868122101
Batch 4/64 loss: -0.27884945273399353
Batch 5/64 loss: -0.29843801259994507
Batch 6/64 loss: -0.29327669739723206
Batch 7/64 loss: -0.2783488929271698
Batch 8/64 loss: -0.297110378742218
Batch 9/64 loss: -0.2867557406425476
Batch 10/64 loss: -0.2966216504573822
Batch 11/64 loss: -0.29012829065322876
Batch 12/64 loss: -0.3030613660812378
Batch 13/64 loss: -0.3035510778427124
Batch 14/64 loss: -0.2968575358390808
Batch 15/64 loss: -0.28674089908599854
Batch 16/64 loss: -0.2891553044319153
Batch 17/64 loss: -0.2999809682369232
Batch 18/64 loss: -0.28238093852996826
Batch 19/64 loss: -0.30102092027664185
Batch 20/64 loss: -0.28483474254608154
Batch 21/64 loss: -0.2995482385158539
Batch 22/64 loss: -0.3211372196674347
Batch 23/64 loss: -0.2928776741027832
Batch 24/64 loss: -0.27435481548309326
Batch 25/64 loss: -0.30111533403396606
Batch 26/64 loss: -0.29228144884109497
Batch 27/64 loss: -0.2919940948486328
Batch 28/64 loss: -0.288669228553772
Batch 29/64 loss: -0.2906076908111572
Batch 30/64 loss: -0.29159197211265564
Batch 31/64 loss: -0.2917977571487427
Batch 32/64 loss: -0.2928315997123718
Batch 33/64 loss: -0.29799678921699524
Batch 34/64 loss: -0.29563602805137634
Batch 35/64 loss: -0.29297539591789246
Batch 36/64 loss: -0.2898566722869873
Batch 37/64 loss: -0.3035106658935547
Batch 38/64 loss: -0.29650700092315674
Batch 39/64 loss: -0.3102918267250061
Batch 40/64 loss: -0.2984668016433716
Batch 41/64 loss: -0.289206326007843
Batch 42/64 loss: -0.2998437285423279
Batch 43/64 loss: -0.2940037250518799
Batch 44/64 loss: -0.2947353720664978
Batch 45/64 loss: -0.2906905710697174
Batch 46/64 loss: -0.27632564306259155
Batch 47/64 loss: -0.3044334650039673
Batch 48/64 loss: -0.29683828353881836
Batch 49/64 loss: -0.29451391100883484
Batch 50/64 loss: -0.3031638264656067
Batch 51/64 loss: -0.293416291475296
Batch 52/64 loss: -0.3025937080383301
Batch 53/64 loss: -0.31015580892562866
Batch 54/64 loss: -0.2912880480289459
Batch 55/64 loss: -0.29788780212402344
Batch 56/64 loss: -0.29470402002334595
Batch 57/64 loss: -0.2888220548629761
Batch 58/64 loss: -0.29334938526153564
Batch 59/64 loss: -0.2824910581111908
Batch 60/64 loss: -0.2917642891407013
Batch 61/64 loss: -0.2850733995437622
Batch 62/64 loss: -0.3036865293979645
Batch 63/64 loss: -0.29128509759902954
Batch 64/64 loss: -0.29259565472602844
Epoch 480  Train loss: -0.29377748580539925  Val loss: -0.2744706154279283
Epoch 481
-------------------------------
Batch 1/64 loss: -0.3080171048641205
Batch 2/64 loss: -0.2955041527748108
Batch 3/64 loss: -0.3017885088920593
Batch 4/64 loss: -0.29424113035202026
Batch 5/64 loss: -0.29072004556655884
Batch 6/64 loss: -0.2936832904815674
Batch 7/64 loss: -0.30184683203697205
Batch 8/64 loss: -0.29505327343940735
Batch 9/64 loss: -0.3065379858016968
Batch 10/64 loss: -0.305660218000412
Batch 11/64 loss: -0.27264711260795593
Batch 12/64 loss: -0.3126353919506073
Batch 13/64 loss: -0.2782387435436249
Batch 14/64 loss: -0.273643434047699
Batch 15/64 loss: -0.3001987338066101
Batch 16/64 loss: -0.2978411018848419
Batch 17/64 loss: -0.3011762499809265
Batch 18/64 loss: -0.29363352060317993
Batch 19/64 loss: -0.29148751497268677
Batch 20/64 loss: -0.3004445433616638
Batch 21/64 loss: -0.2972918152809143
Batch 22/64 loss: -0.2948746681213379
Batch 23/64 loss: -0.28947195410728455
Batch 24/64 loss: -0.283080130815506
Batch 25/64 loss: -0.29796382784843445
Batch 26/64 loss: -0.30517148971557617
Batch 27/64 loss: -0.27407318353652954
Batch 28/64 loss: -0.2757137417793274
Batch 29/64 loss: -0.28637784719467163
Batch 30/64 loss: -0.29705527424812317
Batch 31/64 loss: -0.2987097501754761
Batch 32/64 loss: -0.3005729913711548
Batch 33/64 loss: -0.30000656843185425
Batch 34/64 loss: -0.29605633020401
Batch 35/64 loss: -0.2935265898704529
Batch 36/64 loss: -0.2956439256668091
Batch 37/64 loss: -0.28943750262260437
Batch 38/64 loss: -0.2898901104927063
Batch 39/64 loss: -0.28608250617980957
Batch 40/64 loss: -0.2834405303001404
Batch 41/64 loss: -0.2874718904495239
Batch 42/64 loss: -0.2774388790130615
Batch 43/64 loss: -0.2971848249435425
Batch 44/64 loss: -0.2942890524864197
Batch 45/64 loss: -0.29297393560409546
Batch 46/64 loss: -0.28812718391418457
Batch 47/64 loss: -0.2974472641944885
Batch 48/64 loss: -0.29297614097595215
Batch 49/64 loss: -0.2913080155849457
Batch 50/64 loss: -0.29127073287963867
Batch 51/64 loss: -0.29515108466148376
Batch 52/64 loss: -0.29302874207496643
Batch 53/64 loss: -0.264934778213501
Batch 54/64 loss: -0.2864804267883301
Batch 55/64 loss: -0.27755260467529297
Batch 56/64 loss: -0.2858978509902954
Batch 57/64 loss: -0.28534582257270813
Batch 58/64 loss: -0.2761284410953522
Batch 59/64 loss: -0.29852020740509033
Batch 60/64 loss: -0.2951945662498474
Batch 61/64 loss: -0.28386613726615906
Batch 62/64 loss: -0.2987551689147949
Batch 63/64 loss: -0.28647416830062866
Batch 64/64 loss: -0.29678642749786377
Epoch 481  Train loss: -0.2919191743813309  Val loss: -0.268157349624175
Epoch 482
-------------------------------
Batch 1/64 loss: -0.3027520775794983
Batch 2/64 loss: -0.29547417163848877
Batch 3/64 loss: -0.29452529549598694
Batch 4/64 loss: -0.3044283986091614
Batch 5/64 loss: -0.2891426682472229
Batch 6/64 loss: -0.28551989793777466
Batch 7/64 loss: -0.29380351305007935
Batch 8/64 loss: -0.26472699642181396
Batch 9/64 loss: -0.2793721556663513
Batch 10/64 loss: -0.27721554040908813
Batch 11/64 loss: -0.2993391752243042
Batch 12/64 loss: -0.28828948736190796
Batch 13/64 loss: -0.2817443609237671
Batch 14/64 loss: -0.2933458983898163
Batch 15/64 loss: -0.30466148257255554
Batch 16/64 loss: -0.294162780046463
Batch 17/64 loss: -0.28306296467781067
Batch 18/64 loss: -0.2899092435836792
Batch 19/64 loss: -0.2794136703014374
Batch 20/64 loss: -0.29357999563217163
Batch 21/64 loss: -0.27672767639160156
Batch 22/64 loss: -0.27968329191207886
Batch 23/64 loss: -0.2889403700828552
Batch 24/64 loss: -0.29813364148139954
Batch 25/64 loss: -0.29934161901474
Batch 26/64 loss: -0.29354482889175415
Batch 27/64 loss: -0.30360472202301025
Batch 28/64 loss: -0.2929318845272064
Batch 29/64 loss: -0.291555255651474
Batch 30/64 loss: -0.29505109786987305
Batch 31/64 loss: -0.29985105991363525
Batch 32/64 loss: -0.2920113205909729
Batch 33/64 loss: -0.29324984550476074
Batch 34/64 loss: -0.2775675058364868
Batch 35/64 loss: -0.27218323945999146
Batch 36/64 loss: -0.2712717056274414
Batch 37/64 loss: -0.28517356514930725
Batch 38/64 loss: -0.29634782671928406
Batch 39/64 loss: -0.2608690857887268
Batch 40/64 loss: -0.2918555438518524
Batch 41/64 loss: -0.299617201089859
Batch 42/64 loss: -0.2827450931072235
Batch 43/64 loss: -0.28443238139152527
Batch 44/64 loss: -0.2628324031829834
Batch 45/64 loss: -0.2597981095314026
Batch 46/64 loss: -0.2878941297531128
Batch 47/64 loss: -0.2931217551231384
Batch 48/64 loss: -0.29189279675483704
Batch 49/64 loss: -0.28185415267944336
Batch 50/64 loss: -0.2893095314502716
Batch 51/64 loss: -0.2751157879829407
Batch 52/64 loss: -0.28941166400909424
Batch 53/64 loss: -0.2925379276275635
Batch 54/64 loss: -0.3061867356300354
Batch 55/64 loss: -0.290676474571228
Batch 56/64 loss: -0.28252720832824707
Batch 57/64 loss: -0.29094624519348145
Batch 58/64 loss: -0.2736634612083435
Batch 59/64 loss: -0.27964118123054504
Batch 60/64 loss: -0.2966511845588684
Batch 61/64 loss: -0.28483015298843384
Batch 62/64 loss: -0.28979557752609253
Batch 63/64 loss: -0.2986809015274048
Batch 64/64 loss: -0.29434651136398315
Epoch 482  Train loss: -0.2879888125494415  Val loss: -0.25972651800339164
Epoch 483
-------------------------------
Batch 1/64 loss: -0.28767794370651245
Batch 2/64 loss: -0.2981541156768799
Batch 3/64 loss: -0.29895710945129395
Batch 4/64 loss: -0.28649458289146423
Batch 5/64 loss: -0.2784522771835327
Batch 6/64 loss: -0.28450292348861694
Batch 7/64 loss: -0.2977074682712555
Batch 8/64 loss: -0.2841625213623047
Batch 9/64 loss: -0.3040170669555664
Batch 10/64 loss: -0.28759121894836426
Batch 11/64 loss: -0.2874942421913147
Batch 12/64 loss: -0.3019472360610962
Batch 13/64 loss: -0.2750701904296875
Batch 14/64 loss: -0.3042606711387634
Batch 15/64 loss: -0.3017083406448364
Batch 16/64 loss: -0.27800899744033813
Batch 17/64 loss: -0.3098721206188202
Batch 18/64 loss: -0.3046332001686096
Batch 19/64 loss: -0.2825278639793396
Batch 20/64 loss: -0.29609793424606323
Batch 21/64 loss: -0.2843264639377594
Batch 22/64 loss: -0.2820592522621155
Batch 23/64 loss: -0.3011552393436432
Batch 24/64 loss: -0.2839531898498535
Batch 25/64 loss: -0.2769773602485657
Batch 26/64 loss: -0.29034048318862915
Batch 27/64 loss: -0.278020441532135
Batch 28/64 loss: -0.2821274399757385
Batch 29/64 loss: -0.278122216463089
Batch 30/64 loss: -0.2874237895011902
Batch 31/64 loss: -0.27947819232940674
Batch 32/64 loss: -0.2949531674385071
Batch 33/64 loss: -0.29279613494873047
Batch 34/64 loss: -0.28924816846847534
Batch 35/64 loss: -0.2991177439689636
Batch 36/64 loss: -0.28192564845085144
Batch 37/64 loss: -0.29925501346588135
Batch 38/64 loss: -0.29655885696411133
Batch 39/64 loss: -0.31199946999549866
Batch 40/64 loss: -0.29534435272216797
Batch 41/64 loss: -0.28621381521224976
Batch 42/64 loss: -0.2777632176876068
Batch 43/64 loss: -0.29877999424934387
Batch 44/64 loss: -0.2972814738750458
Batch 45/64 loss: -0.2957197427749634
Batch 46/64 loss: -0.2700638771057129
Batch 47/64 loss: -0.28011348843574524
Batch 48/64 loss: -0.2905983626842499
Batch 49/64 loss: -0.27302834391593933
Batch 50/64 loss: -0.28703704476356506
Batch 51/64 loss: -0.2977696657180786
Batch 52/64 loss: -0.2789609432220459
Batch 53/64 loss: -0.2967027425765991
Batch 54/64 loss: -0.2708597779273987
Batch 55/64 loss: -0.2868554890155792
Batch 56/64 loss: -0.2855471968650818
Batch 57/64 loss: -0.29357409477233887
Batch 58/64 loss: -0.2990599274635315
Batch 59/64 loss: -0.2808048725128174
Batch 60/64 loss: -0.30295702815055847
Batch 61/64 loss: -0.2999384105205536
Batch 62/64 loss: -0.300322562456131
Batch 63/64 loss: -0.2960033714771271
Batch 64/64 loss: -0.302431583404541
Epoch 483  Train loss: -0.2903105847975787  Val loss: -0.26758730493460325
Epoch 484
-------------------------------
Batch 1/64 loss: -0.2815721333026886
Batch 2/64 loss: -0.3085179030895233
Batch 3/64 loss: -0.29032692313194275
Batch 4/64 loss: -0.30794191360473633
Batch 5/64 loss: -0.2995246648788452
Batch 6/64 loss: -0.30541327595710754
Batch 7/64 loss: -0.2739661633968353
Batch 8/64 loss: -0.2979831099510193
Batch 9/64 loss: -0.28578266501426697
Batch 10/64 loss: -0.2917218804359436
Batch 11/64 loss: -0.2990225851535797
Batch 12/64 loss: -0.2992772161960602
Batch 13/64 loss: -0.28086233139038086
Batch 14/64 loss: -0.2862507998943329
Batch 15/64 loss: -0.29332032799720764
Batch 16/64 loss: -0.29923558235168457
Batch 17/64 loss: -0.2899653911590576
Batch 18/64 loss: -0.28886258602142334
Batch 19/64 loss: -0.29259800910949707
Batch 20/64 loss: -0.2961619198322296
Batch 21/64 loss: -0.2845976948738098
Batch 22/64 loss: -0.28376612067222595
Batch 23/64 loss: -0.2886180281639099
Batch 24/64 loss: -0.282198965549469
Batch 25/64 loss: -0.30328449606895447
Batch 26/64 loss: -0.2990947961807251
Batch 27/64 loss: -0.29763132333755493
Batch 28/64 loss: -0.29420167207717896
Batch 29/64 loss: -0.29549068212509155
Batch 30/64 loss: -0.2882503867149353
Batch 31/64 loss: -0.2782245874404907
Batch 32/64 loss: -0.2877883315086365
Batch 33/64 loss: -0.29628071188926697
Batch 34/64 loss: -0.2907734215259552
Batch 35/64 loss: -0.3028133511543274
Batch 36/64 loss: -0.3003009259700775
Batch 37/64 loss: -0.2892146408557892
Batch 38/64 loss: -0.25740599632263184
Batch 39/64 loss: -0.29620611667633057
Batch 40/64 loss: -0.2814325988292694
Batch 41/64 loss: -0.2881985306739807
Batch 42/64 loss: -0.26728910207748413
Batch 43/64 loss: -0.289936900138855
Batch 44/64 loss: -0.29883089661598206
Batch 45/64 loss: -0.3049663007259369
Batch 46/64 loss: -0.2733664810657501
Batch 47/64 loss: -0.2890583872795105
Batch 48/64 loss: -0.2994333505630493
Batch 49/64 loss: -0.2906154692173004
Batch 50/64 loss: -0.2944009006023407
Batch 51/64 loss: -0.2913166880607605
Batch 52/64 loss: -0.28808116912841797
Batch 53/64 loss: -0.2933436930179596
Batch 54/64 loss: -0.29752016067504883
Batch 55/64 loss: -0.2902486324310303
Batch 56/64 loss: -0.29175102710723877
Batch 57/64 loss: -0.28939810395240784
Batch 58/64 loss: -0.30278441309928894
Batch 59/64 loss: -0.29340773820877075
Batch 60/64 loss: -0.29333704710006714
Batch 61/64 loss: -0.3003212511539459
Batch 62/64 loss: -0.2734730541706085
Batch 63/64 loss: -0.2777983248233795
Batch 64/64 loss: -0.2935759425163269
Epoch 484  Train loss: -0.2912147734679428  Val loss: -0.2651808540436001
Epoch 485
-------------------------------
Batch 1/64 loss: -0.26915499567985535
Batch 2/64 loss: -0.2851158082485199
Batch 3/64 loss: -0.27418649196624756
Batch 4/64 loss: -0.28962963819503784
Batch 5/64 loss: -0.2910279333591461
Batch 6/64 loss: -0.29042598605155945
Batch 7/64 loss: -0.29845166206359863
Batch 8/64 loss: -0.30569690465927124
Batch 9/64 loss: -0.2905349135398865
Batch 10/64 loss: -0.27813997864723206
Batch 11/64 loss: -0.28867781162261963
Batch 12/64 loss: -0.28532910346984863
Batch 13/64 loss: -0.2975694537162781
Batch 14/64 loss: -0.29224127531051636
Batch 15/64 loss: -0.29743650555610657
Batch 16/64 loss: -0.30121177434921265
Batch 17/64 loss: -0.2819399833679199
Batch 18/64 loss: -0.2852628827095032
Batch 19/64 loss: -0.2896372079849243
Batch 20/64 loss: -0.29985177516937256
Batch 21/64 loss: -0.28553506731987
Batch 22/64 loss: -0.29735976457595825
Batch 23/64 loss: -0.2694042921066284
Batch 24/64 loss: -0.29427844285964966
Batch 25/64 loss: -0.2792891263961792
Batch 26/64 loss: -0.2903365194797516
Batch 27/64 loss: -0.2693154215812683
Batch 28/64 loss: -0.28651928901672363
Batch 29/64 loss: -0.29352596402168274
Batch 30/64 loss: -0.2909747362136841
Batch 31/64 loss: -0.28845614194869995
Batch 32/64 loss: -0.278785765171051
Batch 33/64 loss: -0.2839542627334595
Batch 34/64 loss: -0.30112868547439575
Batch 35/64 loss: -0.2810085415840149
Batch 36/64 loss: -0.2910303473472595
Batch 37/64 loss: -0.30338144302368164
Batch 38/64 loss: -0.2977794408798218
Batch 39/64 loss: -0.28072524070739746
Batch 40/64 loss: -0.28733253479003906
Batch 41/64 loss: -0.2866981327533722
Batch 42/64 loss: -0.2900674641132355
Batch 43/64 loss: -0.28746867179870605
Batch 44/64 loss: -0.2875898480415344
Batch 45/64 loss: -0.2950880527496338
Batch 46/64 loss: -0.28387412428855896
Batch 47/64 loss: -0.27461856603622437
Batch 48/64 loss: -0.2798501253128052
Batch 49/64 loss: -0.28883278369903564
Batch 50/64 loss: -0.30421334505081177
Batch 51/64 loss: -0.30376118421554565
Batch 52/64 loss: -0.2757464647293091
Batch 53/64 loss: -0.27887722849845886
Batch 54/64 loss: -0.2770283818244934
Batch 55/64 loss: -0.28658783435821533
Batch 56/64 loss: -0.29703187942504883
Batch 57/64 loss: -0.29228153824806213
Batch 58/64 loss: -0.28692615032196045
Batch 59/64 loss: -0.2982812523841858
Batch 60/64 loss: -0.27841389179229736
Batch 61/64 loss: -0.26993754506111145
Batch 62/64 loss: -0.30422133207321167
Batch 63/64 loss: -0.29051846265792847
Batch 64/64 loss: -0.29126471281051636
Epoch 485  Train loss: -0.2882824455990511  Val loss: -0.2717170474660356
Epoch 486
-------------------------------
Batch 1/64 loss: -0.2974494695663452
Batch 2/64 loss: -0.30019474029541016
Batch 3/64 loss: -0.2927699387073517
Batch 4/64 loss: -0.293814092874527
Batch 5/64 loss: -0.2837277948856354
Batch 6/64 loss: -0.2847738265991211
Batch 7/64 loss: -0.2966133952140808
Batch 8/64 loss: -0.2889596223831177
Batch 9/64 loss: -0.2941439747810364
Batch 10/64 loss: -0.30564993619918823
Batch 11/64 loss: -0.29036664962768555
Batch 12/64 loss: -0.2737045884132385
Batch 13/64 loss: -0.2901197075843811
Batch 14/64 loss: -0.29079410433769226
Batch 15/64 loss: -0.2929326295852661
Batch 16/64 loss: -0.2989307940006256
Batch 17/64 loss: -0.2883557081222534
Batch 18/64 loss: -0.2904968559741974
Batch 19/64 loss: -0.29388976097106934
Batch 20/64 loss: -0.3015682101249695
Batch 21/64 loss: -0.297279417514801
Batch 22/64 loss: -0.297379732131958
Batch 23/64 loss: -0.28256505727767944
Batch 24/64 loss: -0.3004215955734253
Batch 25/64 loss: -0.2866055965423584
Batch 26/64 loss: -0.2972790002822876
Batch 27/64 loss: -0.27684348821640015
Batch 28/64 loss: -0.29152578115463257
Batch 29/64 loss: -0.29241520166397095
Batch 30/64 loss: -0.28584396839141846
Batch 31/64 loss: -0.30024439096450806
Batch 32/64 loss: -0.27634748816490173
Batch 33/64 loss: -0.2969456613063812
Batch 34/64 loss: -0.2879140377044678
Batch 35/64 loss: -0.2857001721858978
Batch 36/64 loss: -0.28478550910949707
Batch 37/64 loss: -0.2791793644428253
Batch 38/64 loss: -0.28968292474746704
Batch 39/64 loss: -0.28097593784332275
Batch 40/64 loss: -0.29125750064849854
Batch 41/64 loss: -0.284201443195343
Batch 42/64 loss: -0.27433571219444275
Batch 43/64 loss: -0.28293389081954956
Batch 44/64 loss: -0.29068487882614136
Batch 45/64 loss: -0.3067470192909241
Batch 46/64 loss: -0.2874276638031006
Batch 47/64 loss: -0.28810468316078186
Batch 48/64 loss: -0.2983057200908661
Batch 49/64 loss: -0.2710104286670685
Batch 50/64 loss: -0.2944331467151642
Batch 51/64 loss: -0.29218801856040955
Batch 52/64 loss: -0.28114181756973267
Batch 53/64 loss: -0.28513580560684204
Batch 54/64 loss: -0.2778056561946869
Batch 55/64 loss: -0.2984541952610016
Batch 56/64 loss: -0.2949259877204895
Batch 57/64 loss: -0.28110092878341675
Batch 58/64 loss: -0.2942730188369751
Batch 59/64 loss: -0.2992727756500244
Batch 60/64 loss: -0.2795755863189697
Batch 61/64 loss: -0.29441797733306885
Batch 62/64 loss: -0.2843095362186432
Batch 63/64 loss: -0.3025897443294525
Batch 64/64 loss: -0.29249107837677
Epoch 486  Train loss: -0.2900892795300951  Val loss: -0.2684291660580848
Epoch 487
-------------------------------
Batch 1/64 loss: -0.29940474033355713
Batch 2/64 loss: -0.290831983089447
Batch 3/64 loss: -0.2944459021091461
Batch 4/64 loss: -0.291692316532135
Batch 5/64 loss: -0.29497745633125305
Batch 6/64 loss: -0.2893308103084564
Batch 7/64 loss: -0.2963312268257141
Batch 8/64 loss: -0.28807488083839417
Batch 9/64 loss: -0.29019173979759216
Batch 10/64 loss: -0.294377863407135
Batch 11/64 loss: -0.2806442975997925
Batch 12/64 loss: -0.30080559849739075
Batch 13/64 loss: -0.3017359673976898
Batch 14/64 loss: -0.2995801270008087
Batch 15/64 loss: -0.27631086111068726
Batch 16/64 loss: -0.3106202781200409
Batch 17/64 loss: -0.28579771518707275
Batch 18/64 loss: -0.288658082485199
Batch 19/64 loss: -0.3049912452697754
Batch 20/64 loss: -0.29393115639686584
Batch 21/64 loss: -0.29228517413139343
Batch 22/64 loss: -0.2798489034175873
Batch 23/64 loss: -0.2792416214942932
Batch 24/64 loss: -0.28633183240890503
Batch 25/64 loss: -0.28338849544525146
Batch 26/64 loss: -0.29208922386169434
Batch 27/64 loss: -0.28487855195999146
Batch 28/64 loss: -0.2952728867530823
Batch 29/64 loss: -0.298659086227417
Batch 30/64 loss: -0.2982725501060486
Batch 31/64 loss: -0.2944347560405731
Batch 32/64 loss: -0.30491018295288086
Batch 33/64 loss: -0.29016926884651184
Batch 34/64 loss: -0.29804015159606934
Batch 35/64 loss: -0.289445698261261
Batch 36/64 loss: -0.293758749961853
Batch 37/64 loss: -0.2788488268852234
Batch 38/64 loss: -0.2850008010864258
Batch 39/64 loss: -0.2974739074707031
Batch 40/64 loss: -0.2773229777812958
Batch 41/64 loss: -0.2928428053855896
Batch 42/64 loss: -0.29307347536087036
Batch 43/64 loss: -0.29314255714416504
Batch 44/64 loss: -0.28185710310935974
Batch 45/64 loss: -0.2890266478061676
Batch 46/64 loss: -0.2980468273162842
Batch 47/64 loss: -0.3074491322040558
Batch 48/64 loss: -0.30234047770500183
Batch 49/64 loss: -0.2928442656993866
Batch 50/64 loss: -0.30434033274650574
Batch 51/64 loss: -0.2971677780151367
Batch 52/64 loss: -0.3032262623310089
Batch 53/64 loss: -0.30611300468444824
Batch 54/64 loss: -0.2944445013999939
Batch 55/64 loss: -0.29004615545272827
Batch 56/64 loss: -0.2688092589378357
Batch 57/64 loss: -0.2984144687652588
Batch 58/64 loss: -0.30899858474731445
Batch 59/64 loss: -0.27342915534973145
Batch 60/64 loss: -0.3049946129322052
Batch 61/64 loss: -0.29593294858932495
Batch 62/64 loss: -0.2835797667503357
Batch 63/64 loss: -0.28916072845458984
Batch 64/64 loss: -0.2888023257255554
Epoch 487  Train loss: -0.2926790665177738  Val loss: -0.2710094654682985
Epoch 488
-------------------------------
Batch 1/64 loss: -0.28441157937049866
Batch 2/64 loss: -0.30003005266189575
Batch 3/64 loss: -0.2889079451560974
Batch 4/64 loss: -0.3017227053642273
Batch 5/64 loss: -0.2844974994659424
Batch 6/64 loss: -0.2885449528694153
Batch 7/64 loss: -0.29783737659454346
Batch 8/64 loss: -0.28676265478134155
Batch 9/64 loss: -0.2979082465171814
Batch 10/64 loss: -0.30189380049705505
Batch 11/64 loss: -0.296244740486145
Batch 12/64 loss: -0.30325883626937866
Batch 13/64 loss: -0.29626503586769104
Batch 14/64 loss: -0.29969194531440735
Batch 15/64 loss: -0.30270910263061523
Batch 16/64 loss: -0.3004574179649353
Batch 17/64 loss: -0.2925165295600891
Batch 18/64 loss: -0.26564478874206543
Batch 19/64 loss: -0.29713165760040283
Batch 20/64 loss: -0.29747098684310913
Batch 21/64 loss: -0.30654415488243103
Batch 22/64 loss: -0.29613161087036133
Batch 23/64 loss: -0.2950538694858551
Batch 24/64 loss: -0.2889036238193512
Batch 25/64 loss: -0.2872292995452881
Batch 26/64 loss: -0.28998297452926636
Batch 27/64 loss: -0.29148605465888977
Batch 28/64 loss: -0.29014021158218384
Batch 29/64 loss: -0.3066032826900482
Batch 30/64 loss: -0.3024592995643616
Batch 31/64 loss: -0.29492709040641785
Batch 32/64 loss: -0.2990284264087677
Batch 33/64 loss: -0.29119253158569336
Batch 34/64 loss: -0.3008286654949188
Batch 35/64 loss: -0.28666749596595764
Batch 36/64 loss: -0.29637616872787476
Batch 37/64 loss: -0.29632413387298584
Batch 38/64 loss: -0.2977694869041443
Batch 39/64 loss: -0.29570960998535156
Batch 40/64 loss: -0.2897973656654358
Batch 41/64 loss: -0.29413342475891113
Batch 42/64 loss: -0.30734819173812866
Batch 43/64 loss: -0.2747446596622467
Batch 44/64 loss: -0.28090935945510864
Batch 45/64 loss: -0.29481440782546997
Batch 46/64 loss: -0.2832500636577606
Batch 47/64 loss: -0.2823335826396942
Batch 48/64 loss: -0.2936824560165405
Batch 49/64 loss: -0.2841360569000244
Batch 50/64 loss: -0.2925145626068115
Batch 51/64 loss: -0.30077576637268066
Batch 52/64 loss: -0.28871574997901917
Batch 53/64 loss: -0.2886422574520111
Batch 54/64 loss: -0.28657835721969604
Batch 55/64 loss: -0.3047493100166321
Batch 56/64 loss: -0.28410786390304565
Batch 57/64 loss: -0.2897073030471802
Batch 58/64 loss: -0.2944989800453186
Batch 59/64 loss: -0.283253014087677
Batch 60/64 loss: -0.2915217876434326
Batch 61/64 loss: -0.29706454277038574
Batch 62/64 loss: -0.28662729263305664
Batch 63/64 loss: -0.296612411737442
Batch 64/64 loss: -0.29397761821746826
Epoch 488  Train loss: -0.2931492992475921  Val loss: -0.2662245131440179
Epoch 489
-------------------------------
Batch 1/64 loss: -0.3002984821796417
Batch 2/64 loss: -0.30305981636047363
Batch 3/64 loss: -0.29702532291412354
Batch 4/64 loss: -0.2767478823661804
Batch 5/64 loss: -0.28873902559280396
Batch 6/64 loss: -0.2825942635536194
Batch 7/64 loss: -0.28874003887176514
Batch 8/64 loss: -0.28278520703315735
Batch 9/64 loss: -0.2864686846733093
Batch 10/64 loss: -0.2924601435661316
Batch 11/64 loss: -0.2947289049625397
Batch 12/64 loss: -0.30679941177368164
Batch 13/64 loss: -0.2881145179271698
Batch 14/64 loss: -0.2981955409049988
Batch 15/64 loss: -0.28318995237350464
Batch 16/64 loss: -0.29157745838165283
Batch 17/64 loss: -0.28357431292533875
Batch 18/64 loss: -0.28837794065475464
Batch 19/64 loss: -0.30501097440719604
Batch 20/64 loss: -0.2936495542526245
Batch 21/64 loss: -0.2889209985733032
Batch 22/64 loss: -0.31032973527908325
Batch 23/64 loss: -0.288413941860199
Batch 24/64 loss: -0.3019072413444519
Batch 25/64 loss: -0.2954427897930145
Batch 26/64 loss: -0.29220789670944214
Batch 27/64 loss: -0.3037591576576233
Batch 28/64 loss: -0.3039102554321289
Batch 29/64 loss: -0.2999974191188812
Batch 30/64 loss: -0.287548303604126
Batch 31/64 loss: -0.28801217675209045
Batch 32/64 loss: -0.2907676100730896
Batch 33/64 loss: -0.3016156554222107
Batch 34/64 loss: -0.2839908003807068
Batch 35/64 loss: -0.28165438771247864
Batch 36/64 loss: -0.2885664999485016
Batch 37/64 loss: -0.2987251281738281
Batch 38/64 loss: -0.28986215591430664
Batch 39/64 loss: -0.274736225605011
Batch 40/64 loss: -0.285552978515625
Batch 41/64 loss: -0.29154396057128906
Batch 42/64 loss: -0.28919607400894165
Batch 43/64 loss: -0.30191394686698914
Batch 44/64 loss: -0.30643609166145325
Batch 45/64 loss: -0.29639875888824463
Batch 46/64 loss: -0.29597800970077515
Batch 47/64 loss: -0.2951936423778534
Batch 48/64 loss: -0.29427921772003174
Batch 49/64 loss: -0.29228508472442627
Batch 50/64 loss: -0.283799409866333
Batch 51/64 loss: -0.30783894658088684
Batch 52/64 loss: -0.3010002374649048
Batch 53/64 loss: -0.29560405015945435
Batch 54/64 loss: -0.2910732626914978
Batch 55/64 loss: -0.29536983370780945
Batch 56/64 loss: -0.30384424328804016
Batch 57/64 loss: -0.2900586426258087
Batch 58/64 loss: -0.2936328649520874
Batch 59/64 loss: -0.3098420798778534
Batch 60/64 loss: -0.28876882791519165
Batch 61/64 loss: -0.28421568870544434
Batch 62/64 loss: -0.3007093667984009
Batch 63/64 loss: -0.29711413383483887
Batch 64/64 loss: -0.31299012899398804
Epoch 489  Train loss: -0.29378663161221674  Val loss: -0.2673056262874931
Epoch 490
-------------------------------
Batch 1/64 loss: -0.2971024513244629
Batch 2/64 loss: -0.2976168990135193
Batch 3/64 loss: -0.3057575821876526
Batch 4/64 loss: -0.2895064055919647
Batch 5/64 loss: -0.30061790347099304
Batch 6/64 loss: -0.29329657554626465
Batch 7/64 loss: -0.28886982798576355
Batch 8/64 loss: -0.2819778323173523
Batch 9/64 loss: -0.30479884147644043
Batch 10/64 loss: -0.2986829876899719
Batch 11/64 loss: -0.2978901267051697
Batch 12/64 loss: -0.30153000354766846
Batch 13/64 loss: -0.28582096099853516
Batch 14/64 loss: -0.27969929575920105
Batch 15/64 loss: -0.2843356430530548
Batch 16/64 loss: -0.28952932357788086
Batch 17/64 loss: -0.2985428273677826
Batch 18/64 loss: -0.2933802902698517
Batch 19/64 loss: -0.29445576667785645
Batch 20/64 loss: -0.2812592089176178
Batch 21/64 loss: -0.29660671949386597
Batch 22/64 loss: -0.29843103885650635
Batch 23/64 loss: -0.30277949571609497
Batch 24/64 loss: -0.2905925214290619
Batch 25/64 loss: -0.29592788219451904
Batch 26/64 loss: -0.2668171525001526
Batch 27/64 loss: -0.2940605580806732
Batch 28/64 loss: -0.29123973846435547
Batch 29/64 loss: -0.27890557050704956
Batch 30/64 loss: -0.2842254638671875
Batch 31/64 loss: -0.2878629267215729
Batch 32/64 loss: -0.29932528734207153
Batch 33/64 loss: -0.2956903576850891
Batch 34/64 loss: -0.29899540543556213
Batch 35/64 loss: -0.30791914463043213
Batch 36/64 loss: -0.30401140451431274
Batch 37/64 loss: -0.2833724617958069
Batch 38/64 loss: -0.29466092586517334
Batch 39/64 loss: -0.297136515378952
Batch 40/64 loss: -0.30968132615089417
Batch 41/64 loss: -0.291836678981781
Batch 42/64 loss: -0.30130690336227417
Batch 43/64 loss: -0.2848236560821533
Batch 44/64 loss: -0.27517634630203247
Batch 45/64 loss: -0.27807480096817017
Batch 46/64 loss: -0.29207539558410645
Batch 47/64 loss: -0.30493658781051636
Batch 48/64 loss: -0.3002832531929016
Batch 49/64 loss: -0.2877670228481293
Batch 50/64 loss: -0.290472149848938
Batch 51/64 loss: -0.2961125373840332
Batch 52/64 loss: -0.3052526116371155
Batch 53/64 loss: -0.3026474118232727
Batch 54/64 loss: -0.3060602843761444
Batch 55/64 loss: -0.29358214139938354
Batch 56/64 loss: -0.29669278860092163
Batch 57/64 loss: -0.30384963750839233
Batch 58/64 loss: -0.2676202654838562
Batch 59/64 loss: -0.30507880449295044
Batch 60/64 loss: -0.29989638924598694
Batch 61/64 loss: -0.2858726680278778
Batch 62/64 loss: -0.2936266362667084
Batch 63/64 loss: -0.3006090521812439
Batch 64/64 loss: -0.2965182363986969
Epoch 490  Train loss: -0.2937875349147647  Val loss: -0.26806021043934775
Epoch 491
-------------------------------
Batch 1/64 loss: -0.29143908619880676
Batch 2/64 loss: -0.3013063371181488
Batch 3/64 loss: -0.29250597953796387
Batch 4/64 loss: -0.295246958732605
Batch 5/64 loss: -0.2899578809738159
Batch 6/64 loss: -0.29943498969078064
Batch 7/64 loss: -0.2949371337890625
Batch 8/64 loss: -0.2812812030315399
Batch 9/64 loss: -0.29620617628097534
Batch 10/64 loss: -0.29426196217536926
Batch 11/64 loss: -0.2945523262023926
Batch 12/64 loss: -0.29241862893104553
Batch 13/64 loss: -0.28354954719543457
Batch 14/64 loss: -0.2846277952194214
Batch 15/64 loss: -0.2870303690433502
Batch 16/64 loss: -0.28228455781936646
Batch 17/64 loss: -0.28901100158691406
Batch 18/64 loss: -0.29285728931427
Batch 19/64 loss: -0.29253479838371277
Batch 20/64 loss: -0.28716298937797546
Batch 21/64 loss: -0.2968218922615051
Batch 22/64 loss: -0.2851458191871643
Batch 23/64 loss: -0.2785106897354126
Batch 24/64 loss: -0.29112279415130615
Batch 25/64 loss: -0.2802656888961792
Batch 26/64 loss: -0.2855978012084961
Batch 27/64 loss: -0.29305300116539
Batch 28/64 loss: -0.2931428551673889
Batch 29/64 loss: -0.281427264213562
Batch 30/64 loss: -0.2805935740470886
Batch 31/64 loss: -0.2971227765083313
Batch 32/64 loss: -0.30532777309417725
Batch 33/64 loss: -0.29175281524658203
Batch 34/64 loss: -0.2748875319957733
Batch 35/64 loss: -0.2862841486930847
Batch 36/64 loss: -0.29515039920806885
Batch 37/64 loss: -0.2945047616958618
Batch 38/64 loss: -0.30135130882263184
Batch 39/64 loss: -0.27327099442481995
Batch 40/64 loss: -0.2793331742286682
Batch 41/64 loss: -0.2757939100265503
Batch 42/64 loss: -0.29218390583992004
Batch 43/64 loss: -0.2786197364330292
Batch 44/64 loss: -0.29770374298095703
Batch 45/64 loss: -0.29058611392974854
Batch 46/64 loss: -0.2725602388381958
Batch 47/64 loss: -0.29748600721359253
Batch 48/64 loss: -0.3029184341430664
Batch 49/64 loss: -0.30367547273635864
Batch 50/64 loss: -0.27802231907844543
Batch 51/64 loss: -0.2909386157989502
Batch 52/64 loss: -0.2850884199142456
Batch 53/64 loss: -0.3030208945274353
Batch 54/64 loss: -0.28185877203941345
Batch 55/64 loss: -0.28500860929489136
Batch 56/64 loss: -0.27980971336364746
Batch 57/64 loss: -0.29003584384918213
Batch 58/64 loss: -0.26875853538513184
Batch 59/64 loss: -0.29082241654396057
Batch 60/64 loss: -0.2839125990867615
Batch 61/64 loss: -0.2858525514602661
Batch 62/64 loss: -0.28718966245651245
Batch 63/64 loss: -0.30222320556640625
Batch 64/64 loss: -0.295152485370636
Epoch 491  Train loss: -0.2891405201425739  Val loss: -0.26716985735286963
Epoch 492
-------------------------------
Batch 1/64 loss: -0.2839708924293518
Batch 2/64 loss: -0.282046914100647
Batch 3/64 loss: -0.2841821312904358
Batch 4/64 loss: -0.2953583896160126
Batch 5/64 loss: -0.29594582319259644
Batch 6/64 loss: -0.28072890639305115
Batch 7/64 loss: -0.2906586527824402
Batch 8/64 loss: -0.26861780881881714
Batch 9/64 loss: -0.2835392355918884
Batch 10/64 loss: -0.2924894690513611
Batch 11/64 loss: -0.28936272859573364
Batch 12/64 loss: -0.2955326437950134
Batch 13/64 loss: -0.29836612939834595
Batch 14/64 loss: -0.2909572720527649
Batch 15/64 loss: -0.2865028381347656
Batch 16/64 loss: -0.28238028287887573
Batch 17/64 loss: -0.2865167558193207
Batch 18/64 loss: -0.30049800872802734
Batch 19/64 loss: -0.2895011901855469
Batch 20/64 loss: -0.2991048991680145
Batch 21/64 loss: -0.30060142278671265
Batch 22/64 loss: -0.28687044978141785
Batch 23/64 loss: -0.2950902581214905
Batch 24/64 loss: -0.2991626560688019
Batch 25/64 loss: -0.28287994861602783
Batch 26/64 loss: -0.28868916630744934
Batch 27/64 loss: -0.29288437962532043
Batch 28/64 loss: -0.29632508754730225
Batch 29/64 loss: -0.2941173017024994
Batch 30/64 loss: -0.2958497107028961
Batch 31/64 loss: -0.2973010540008545
Batch 32/64 loss: -0.29601192474365234
Batch 33/64 loss: -0.286857008934021
Batch 34/64 loss: -0.28843432664871216
Batch 35/64 loss: -0.2903473377227783
Batch 36/64 loss: -0.2914085388183594
Batch 37/64 loss: -0.2839573323726654
Batch 38/64 loss: -0.29476064443588257
Batch 39/64 loss: -0.29833680391311646
Batch 40/64 loss: -0.2811998426914215
Batch 41/64 loss: -0.29519057273864746
Batch 42/64 loss: -0.3056863248348236
Batch 43/64 loss: -0.29662713408470154
Batch 44/64 loss: -0.3059377074241638
Batch 45/64 loss: -0.2864189147949219
Batch 46/64 loss: -0.3021555542945862
Batch 47/64 loss: -0.3025612235069275
Batch 48/64 loss: -0.2959907650947571
Batch 49/64 loss: -0.295154333114624
Batch 50/64 loss: -0.2989916205406189
Batch 51/64 loss: -0.2912873327732086
Batch 52/64 loss: -0.28801143169403076
Batch 53/64 loss: -0.2827860713005066
Batch 54/64 loss: -0.29616981744766235
Batch 55/64 loss: -0.29570287466049194
Batch 56/64 loss: -0.28823649883270264
Batch 57/64 loss: -0.29552632570266724
Batch 58/64 loss: -0.291749507188797
Batch 59/64 loss: -0.2871566414833069
Batch 60/64 loss: -0.3020329177379608
Batch 61/64 loss: -0.2596312165260315
Batch 62/64 loss: -0.29609614610671997
Batch 63/64 loss: -0.29207882285118103
Batch 64/64 loss: -0.29807227849960327
Epoch 492  Train loss: -0.2916404726458531  Val loss: -0.2599203504647586
Epoch 493
-------------------------------
Batch 1/64 loss: -0.29616332054138184
Batch 2/64 loss: -0.2973376512527466
Batch 3/64 loss: -0.31013306975364685
Batch 4/64 loss: -0.2924007773399353
Batch 5/64 loss: -0.2924424111843109
Batch 6/64 loss: -0.29824090003967285
Batch 7/64 loss: -0.29225367307662964
Batch 8/64 loss: -0.30437761545181274
Batch 9/64 loss: -0.29292750358581543
Batch 10/64 loss: -0.2986573576927185
Batch 11/64 loss: -0.2922179102897644
Batch 12/64 loss: -0.29313158988952637
Batch 13/64 loss: -0.30287808179855347
Batch 14/64 loss: -0.2882715165615082
Batch 15/64 loss: -0.30264991521835327
Batch 16/64 loss: -0.2953023314476013
Batch 17/64 loss: -0.30308425426483154
Batch 18/64 loss: -0.2999798059463501
Batch 19/64 loss: -0.29116058349609375
Batch 20/64 loss: -0.30427777767181396
Batch 21/64 loss: -0.2975490093231201
Batch 22/64 loss: -0.29808568954467773
Batch 23/64 loss: -0.29715120792388916
Batch 24/64 loss: -0.29897451400756836
Batch 25/64 loss: -0.30505889654159546
Batch 26/64 loss: -0.30209091305732727
Batch 27/64 loss: -0.2871776819229126
Batch 28/64 loss: -0.30674993991851807
Batch 29/64 loss: -0.30178800225257874
Batch 30/64 loss: -0.29158204793930054
Batch 31/64 loss: -0.29411178827285767
Batch 32/64 loss: -0.2980721592903137
Batch 33/64 loss: -0.2907612919807434
Batch 34/64 loss: -0.27889978885650635
Batch 35/64 loss: -0.299325168132782
Batch 36/64 loss: -0.30017706751823425
Batch 37/64 loss: -0.29423996806144714
Batch 38/64 loss: -0.30077481269836426
Batch 39/64 loss: -0.2853109836578369
Batch 40/64 loss: -0.29889339208602905
Batch 41/64 loss: -0.29601922631263733
Batch 42/64 loss: -0.28985852003097534
Batch 43/64 loss: -0.29269784688949585
Batch 44/64 loss: -0.29091310501098633
Batch 45/64 loss: -0.29910314083099365
Batch 46/64 loss: -0.29569336771965027
Batch 47/64 loss: -0.2924642264842987
Batch 48/64 loss: -0.29416659474372864
Batch 49/64 loss: -0.2949063777923584
Batch 50/64 loss: -0.2844443619251251
Batch 51/64 loss: -0.29869455099105835
Batch 52/64 loss: -0.2984377145767212
Batch 53/64 loss: -0.29160070419311523
Batch 54/64 loss: -0.2874988317489624
Batch 55/64 loss: -0.2983342111110687
Batch 56/64 loss: -0.2916673421859741
Batch 57/64 loss: -0.292930543422699
Batch 58/64 loss: -0.3197842836380005
Batch 59/64 loss: -0.3029976189136505
Batch 60/64 loss: -0.28484201431274414
Batch 61/64 loss: -0.29674237966537476
Batch 62/64 loss: -0.29353705048561096
Batch 63/64 loss: -0.29623907804489136
Batch 64/64 loss: -0.28964167833328247
Epoch 493  Train loss: -0.2960857524591334  Val loss: -0.2737981764106816
Epoch 494
-------------------------------
Batch 1/64 loss: -0.302964985370636
Batch 2/64 loss: -0.2965390086174011
Batch 3/64 loss: -0.2938120365142822
Batch 4/64 loss: -0.2998180389404297
Batch 5/64 loss: -0.294668048620224
Batch 6/64 loss: -0.298597127199173
Batch 7/64 loss: -0.2711407542228699
Batch 8/64 loss: -0.29612797498703003
Batch 9/64 loss: -0.29585397243499756
Batch 10/64 loss: -0.2936805784702301
Batch 11/64 loss: -0.29541441798210144
Batch 12/64 loss: -0.29212576150894165
Batch 13/64 loss: -0.30388548970222473
Batch 14/64 loss: -0.2737882435321808
Batch 15/64 loss: -0.294919490814209
Batch 16/64 loss: -0.3073955774307251
Batch 17/64 loss: -0.30058208107948303
Batch 18/64 loss: -0.3007642328739166
Batch 19/64 loss: -0.293342649936676
Batch 20/64 loss: -0.2934200167655945
Batch 21/64 loss: -0.2830163240432739
Batch 22/64 loss: -0.3012479543685913
Batch 23/64 loss: -0.28673845529556274
Batch 24/64 loss: -0.2867281436920166
Batch 25/64 loss: -0.29436159133911133
Batch 26/64 loss: -0.3033241033554077
Batch 27/64 loss: -0.29681578278541565
Batch 28/64 loss: -0.3063492774963379
Batch 29/64 loss: -0.2987001836299896
Batch 30/64 loss: -0.296671986579895
Batch 31/64 loss: -0.2991366386413574
Batch 32/64 loss: -0.28493058681488037
Batch 33/64 loss: -0.28583770990371704
Batch 34/64 loss: -0.28374341130256653
Batch 35/64 loss: -0.30265358090400696
Batch 36/64 loss: -0.2969447374343872
Batch 37/64 loss: -0.28449684381484985
Batch 38/64 loss: -0.28002357482910156
Batch 39/64 loss: -0.30675017833709717
Batch 40/64 loss: -0.2958468198776245
Batch 41/64 loss: -0.28625255823135376
Batch 42/64 loss: -0.29937922954559326
Batch 43/64 loss: -0.2915607690811157
Batch 44/64 loss: -0.2829529345035553
Batch 45/64 loss: -0.292084664106369
Batch 46/64 loss: -0.3074009120464325
Batch 47/64 loss: -0.29729846119880676
Batch 48/64 loss: -0.27770310640335083
Batch 49/64 loss: -0.2953060269355774
Batch 50/64 loss: -0.2986302375793457
Batch 51/64 loss: -0.2952038645744324
Batch 52/64 loss: -0.2961249351501465
Batch 53/64 loss: -0.3018757998943329
Batch 54/64 loss: -0.30069422721862793
Batch 55/64 loss: -0.303133487701416
Batch 56/64 loss: -0.2909117341041565
Batch 57/64 loss: -0.29592323303222656
Batch 58/64 loss: -0.27632275223731995
Batch 59/64 loss: -0.2823159992694855
Batch 60/64 loss: -0.31026923656463623
Batch 61/64 loss: -0.2961066961288452
Batch 62/64 loss: -0.29597654938697815
Batch 63/64 loss: -0.2932004928588867
Batch 64/64 loss: -0.291903018951416
Epoch 494  Train loss: -0.2942543300927854  Val loss: -0.27303055553501826
Epoch 495
-------------------------------
Batch 1/64 loss: -0.26541268825531006
Batch 2/64 loss: -0.2954293489456177
Batch 3/64 loss: -0.28676071763038635
Batch 4/64 loss: -0.30792659521102905
Batch 5/64 loss: -0.2906460165977478
Batch 6/64 loss: -0.2923624515533447
Batch 7/64 loss: -0.29470592737197876
Batch 8/64 loss: -0.2805787920951843
Batch 9/64 loss: -0.29704076051712036
Batch 10/64 loss: -0.27757203578948975
Batch 11/64 loss: -0.28901734948158264
Batch 12/64 loss: -0.2939855456352234
Batch 13/64 loss: -0.29037541151046753
Batch 14/64 loss: -0.283312052488327
Batch 15/64 loss: -0.30005669593811035
Batch 16/64 loss: -0.2880787253379822
Batch 17/64 loss: -0.2815902829170227
Batch 18/64 loss: -0.2650613784790039
Batch 19/64 loss: -0.3001682162284851
Batch 20/64 loss: -0.3006190359592438
Batch 21/64 loss: -0.285888671875
Batch 22/64 loss: -0.3031013011932373
Batch 23/64 loss: -0.2984882891178131
Batch 24/64 loss: -0.29591578245162964
Batch 25/64 loss: -0.2917829751968384
Batch 26/64 loss: -0.28365495800971985
Batch 27/64 loss: -0.2763158082962036
Batch 28/64 loss: -0.2965083122253418
Batch 29/64 loss: -0.2894093990325928
Batch 30/64 loss: -0.27156978845596313
Batch 31/64 loss: -0.2956450581550598
Batch 32/64 loss: -0.2986002564430237
Batch 33/64 loss: -0.2869452238082886
Batch 34/64 loss: -0.2978922724723816
Batch 35/64 loss: -0.29118067026138306
Batch 36/64 loss: -0.3148946762084961
Batch 37/64 loss: -0.28197211027145386
Batch 38/64 loss: -0.30428820848464966
Batch 39/64 loss: -0.3015326261520386
Batch 40/64 loss: -0.28146594762802124
Batch 41/64 loss: -0.28678202629089355
Batch 42/64 loss: -0.29505860805511475
Batch 43/64 loss: -0.30370381474494934
Batch 44/64 loss: -0.2937504053115845
Batch 45/64 loss: -0.3012126386165619
Batch 46/64 loss: -0.2919886112213135
Batch 47/64 loss: -0.30724090337753296
Batch 48/64 loss: -0.29275357723236084
Batch 49/64 loss: -0.28574734926223755
Batch 50/64 loss: -0.2778596580028534
Batch 51/64 loss: -0.2957252264022827
Batch 52/64 loss: -0.29192471504211426
Batch 53/64 loss: -0.29692691564559937
Batch 54/64 loss: -0.30103999376296997
Batch 55/64 loss: -0.300853967666626
Batch 56/64 loss: -0.3004586696624756
Batch 57/64 loss: -0.2866818904876709
Batch 58/64 loss: -0.30207839608192444
Batch 59/64 loss: -0.2906564474105835
Batch 60/64 loss: -0.29786917567253113
Batch 61/64 loss: -0.28960537910461426
Batch 62/64 loss: -0.30279314517974854
Batch 63/64 loss: -0.2764419615268707
Batch 64/64 loss: -0.30267447233200073
Epoch 495  Train loss: -0.29213979127360323  Val loss: -0.2666429389793029
Epoch 496
-------------------------------
Batch 1/64 loss: -0.3011760711669922
Batch 2/64 loss: -0.2939814329147339
Batch 3/64 loss: -0.29378241300582886
Batch 4/64 loss: -0.2708686888217926
Batch 5/64 loss: -0.2914503812789917
Batch 6/64 loss: -0.28741973638534546
Batch 7/64 loss: -0.2879607379436493
Batch 8/64 loss: -0.29172635078430176
Batch 9/64 loss: -0.309175968170166
Batch 10/64 loss: -0.28974467515945435
Batch 11/64 loss: -0.28903597593307495
Batch 12/64 loss: -0.3006955683231354
Batch 13/64 loss: -0.28754791617393494
Batch 14/64 loss: -0.3042656183242798
Batch 15/64 loss: -0.30130070447921753
Batch 16/64 loss: -0.29975050687789917
Batch 17/64 loss: -0.2921670079231262
Batch 18/64 loss: -0.28767889738082886
Batch 19/64 loss: -0.2952194809913635
Batch 20/64 loss: -0.2982978820800781
Batch 21/64 loss: -0.29491060972213745
Batch 22/64 loss: -0.28975415229797363
Batch 23/64 loss: -0.2892575263977051
Batch 24/64 loss: -0.2946622967720032
Batch 25/64 loss: -0.30389225482940674
Batch 26/64 loss: -0.2937586307525635
Batch 27/64 loss: -0.29186010360717773
Batch 28/64 loss: -0.28190407156944275
Batch 29/64 loss: -0.30383652448654175
Batch 30/64 loss: -0.2885540723800659
Batch 31/64 loss: -0.2897530496120453
Batch 32/64 loss: -0.298728346824646
Batch 33/64 loss: -0.30145925283432007
Batch 34/64 loss: -0.2930564880371094
Batch 35/64 loss: -0.27535417675971985
Batch 36/64 loss: -0.29938799142837524
Batch 37/64 loss: -0.29238027334213257
Batch 38/64 loss: -0.2829468846321106
Batch 39/64 loss: -0.2953077256679535
Batch 40/64 loss: -0.30703163146972656
Batch 41/64 loss: -0.29173707962036133
Batch 42/64 loss: -0.29654550552368164
Batch 43/64 loss: -0.29097631573677063
Batch 44/64 loss: -0.29982125759124756
Batch 45/64 loss: -0.29527154564857483
Batch 46/64 loss: -0.2937047779560089
Batch 47/64 loss: -0.29436349868774414
Batch 48/64 loss: -0.2910022735595703
Batch 49/64 loss: -0.2905796766281128
Batch 50/64 loss: -0.29769381880760193
Batch 51/64 loss: -0.2875751256942749
Batch 52/64 loss: -0.2829684019088745
Batch 53/64 loss: -0.29922613501548767
Batch 54/64 loss: -0.2967391610145569
Batch 55/64 loss: -0.298503577709198
Batch 56/64 loss: -0.30291441082954407
Batch 57/64 loss: -0.31808894872665405
Batch 58/64 loss: -0.30075603723526
Batch 59/64 loss: -0.3122428357601166
Batch 60/64 loss: -0.29907917976379395
Batch 61/64 loss: -0.29203423857688904
Batch 62/64 loss: -0.2779444456100464
Batch 63/64 loss: -0.3042290210723877
Batch 64/64 loss: -0.3008859157562256
Epoch 496  Train loss: -0.29459927502800437  Val loss: -0.2730966894487335
Epoch 497
-------------------------------
Batch 1/64 loss: -0.30163928866386414
Batch 2/64 loss: -0.28642958402633667
Batch 3/64 loss: -0.2952292561531067
Batch 4/64 loss: -0.30098628997802734
Batch 5/64 loss: -0.3038695752620697
Batch 6/64 loss: -0.2999212443828583
Batch 7/64 loss: -0.31077712774276733
Batch 8/64 loss: -0.30433332920074463
Batch 9/64 loss: -0.29353171586990356
Batch 10/64 loss: -0.28957754373550415
Batch 11/64 loss: -0.2943536639213562
Batch 12/64 loss: -0.2999906539916992
Batch 13/64 loss: -0.30495643615722656
Batch 14/64 loss: -0.29783713817596436
Batch 15/64 loss: -0.30562058091163635
Batch 16/64 loss: -0.29587218165397644
Batch 17/64 loss: -0.2965657413005829
Batch 18/64 loss: -0.2940707206726074
Batch 19/64 loss: -0.29568272829055786
Batch 20/64 loss: -0.2842739224433899
Batch 21/64 loss: -0.29194530844688416
Batch 22/64 loss: -0.276539146900177
Batch 23/64 loss: -0.29964709281921387
Batch 24/64 loss: -0.29133516550064087
Batch 25/64 loss: -0.2907305061817169
Batch 26/64 loss: -0.285732239484787
Batch 27/64 loss: -0.29349756240844727
Batch 28/64 loss: -0.2955293357372284
Batch 29/64 loss: -0.301219642162323
Batch 30/64 loss: -0.3015052378177643
Batch 31/64 loss: -0.3045753240585327
Batch 32/64 loss: -0.280523419380188
Batch 33/64 loss: -0.2907794713973999
Batch 34/64 loss: -0.2879555821418762
Batch 35/64 loss: -0.29106488823890686
Batch 36/64 loss: -0.28364992141723633
Batch 37/64 loss: -0.27404096722602844
Batch 38/64 loss: -0.28992128372192383
Batch 39/64 loss: -0.2780188322067261
Batch 40/64 loss: -0.2994331121444702
Batch 41/64 loss: -0.285414457321167
Batch 42/64 loss: -0.29636454582214355
Batch 43/64 loss: -0.30439597368240356
Batch 44/64 loss: -0.2779691219329834
Batch 45/64 loss: -0.28821295499801636
Batch 46/64 loss: -0.2955540418624878
Batch 47/64 loss: -0.29959923028945923
Batch 48/64 loss: -0.2960579991340637
Batch 49/64 loss: -0.3024231195449829
Batch 50/64 loss: -0.2975141704082489
Batch 51/64 loss: -0.28579676151275635
Batch 52/64 loss: -0.3103240430355072
Batch 53/64 loss: -0.29751724004745483
Batch 54/64 loss: -0.31203246116638184
Batch 55/64 loss: -0.2890089750289917
Batch 56/64 loss: -0.29227006435394287
Batch 57/64 loss: -0.2810558080673218
Batch 58/64 loss: -0.2885851263999939
Batch 59/64 loss: -0.3024926483631134
Batch 60/64 loss: -0.3071132004261017
Batch 61/64 loss: -0.2970408797264099
Batch 62/64 loss: -0.29557573795318604
Batch 63/64 loss: -0.29703056812286377
Batch 64/64 loss: -0.2877494692802429
Epoch 497  Train loss: -0.2945619449895971  Val loss: -0.27021462175854294
Epoch 498
-------------------------------
Batch 1/64 loss: -0.285005122423172
Batch 2/64 loss: -0.2826758623123169
Batch 3/64 loss: -0.2875068187713623
Batch 4/64 loss: -0.295766681432724
Batch 5/64 loss: -0.2959265410900116
Batch 6/64 loss: -0.29683345556259155
Batch 7/64 loss: -0.3053220510482788
Batch 8/64 loss: -0.28517231345176697
Batch 9/64 loss: -0.291628897190094
Batch 10/64 loss: -0.3028416335582733
Batch 11/64 loss: -0.29069629311561584
Batch 12/64 loss: -0.29807984828948975
Batch 13/64 loss: -0.2961779832839966
Batch 14/64 loss: -0.30070531368255615
Batch 15/64 loss: -0.2954520583152771
Batch 16/64 loss: -0.2885690927505493
Batch 17/64 loss: -0.28305697441101074
Batch 18/64 loss: -0.2839767634868622
Batch 19/64 loss: -0.29256635904312134
Batch 20/64 loss: -0.28487730026245117
Batch 21/64 loss: -0.29886680841445923
Batch 22/64 loss: -0.3155721127986908
Batch 23/64 loss: -0.27126210927963257
Batch 24/64 loss: -0.2939609885215759
Batch 25/64 loss: -0.2893400490283966
Batch 26/64 loss: -0.2919207215309143
Batch 27/64 loss: -0.30056798458099365
Batch 28/64 loss: -0.2971271574497223
Batch 29/64 loss: -0.2890108823776245
Batch 30/64 loss: -0.2921212911605835
Batch 31/64 loss: -0.2953779101371765
Batch 32/64 loss: -0.2886478006839752
Batch 33/64 loss: -0.2905966639518738
Batch 34/64 loss: -0.2958405613899231
Batch 35/64 loss: -0.2925179600715637
Batch 36/64 loss: -0.2866697907447815
Batch 37/64 loss: -0.2863328456878662
Batch 38/64 loss: -0.29372161626815796
Batch 39/64 loss: -0.2850034236907959
Batch 40/64 loss: -0.2933482527732849
Batch 41/64 loss: -0.28847503662109375
Batch 42/64 loss: -0.2919706702232361
Batch 43/64 loss: -0.2759220004081726
Batch 44/64 loss: -0.2961421012878418
Batch 45/64 loss: -0.29549679160118103
Batch 46/64 loss: -0.2921307682991028
Batch 47/64 loss: -0.2933673560619354
Batch 48/64 loss: -0.292194128036499
Batch 49/64 loss: -0.3010087013244629
Batch 50/64 loss: -0.29847121238708496
Batch 51/64 loss: -0.29350876808166504
Batch 52/64 loss: -0.28652051091194153
Batch 53/64 loss: -0.28755438327789307
Batch 54/64 loss: -0.30509108304977417
Batch 55/64 loss: -0.29160332679748535
Batch 56/64 loss: -0.3077375292778015
Batch 57/64 loss: -0.2864082455635071
Batch 58/64 loss: -0.302217960357666
Batch 59/64 loss: -0.30200618505477905
Batch 60/64 loss: -0.2970806956291199
Batch 61/64 loss: -0.29160887002944946
Batch 62/64 loss: -0.295632541179657
Batch 63/64 loss: -0.2920205891132355
Batch 64/64 loss: -0.2815883755683899
Epoch 498  Train loss: -0.29280007890626497  Val loss: -0.2651221385526493
Epoch 499
-------------------------------
Batch 1/64 loss: -0.3024935722351074
Batch 2/64 loss: -0.298094242811203
Batch 3/64 loss: -0.277092844247818
Batch 4/64 loss: -0.3061520457267761
Batch 5/64 loss: -0.2947947382926941
Batch 6/64 loss: -0.298869252204895
Batch 7/64 loss: -0.2825591266155243
Batch 8/64 loss: -0.2993127703666687
Batch 9/64 loss: -0.2967243790626526
Batch 10/64 loss: -0.29901063442230225
Batch 11/64 loss: -0.29362910985946655
Batch 12/64 loss: -0.29012614488601685
Batch 13/64 loss: -0.2977942228317261
Batch 14/64 loss: -0.29690486192703247
Batch 15/64 loss: -0.2965649962425232
Batch 16/64 loss: -0.3029016852378845
Batch 17/64 loss: -0.2827279567718506
Batch 18/64 loss: -0.2807440757751465
Batch 19/64 loss: -0.28616952896118164
Batch 20/64 loss: -0.2928180396556854
Batch 21/64 loss: -0.3100239038467407
Batch 22/64 loss: -0.29375335574150085
Batch 23/64 loss: -0.2801457643508911
Batch 24/64 loss: -0.28500643372535706
Batch 25/64 loss: -0.2969319820404053
Batch 26/64 loss: -0.2878190577030182
Batch 27/64 loss: -0.2900501787662506
Batch 28/64 loss: -0.2968699336051941
Batch 29/64 loss: -0.2933287024497986
Batch 30/64 loss: -0.2724609076976776
Batch 31/64 loss: -0.30994758009910583
Batch 32/64 loss: -0.2945416569709778
Batch 33/64 loss: -0.28985023498535156
Batch 34/64 loss: -0.2864898443222046
Batch 35/64 loss: -0.2784087359905243
Batch 36/64 loss: -0.29337945580482483
Batch 37/64 loss: -0.2989341616630554
Batch 38/64 loss: -0.29034119844436646
Batch 39/64 loss: -0.3005109429359436
Batch 40/64 loss: -0.2865052819252014
Batch 41/64 loss: -0.2978299856185913
Batch 42/64 loss: -0.29979759454727173
Batch 43/64 loss: -0.28309041261672974
Batch 44/64 loss: -0.2732665240764618
Batch 45/64 loss: -0.27985450625419617
Batch 46/64 loss: -0.2935630679130554
Batch 47/64 loss: -0.2974647283554077
Batch 48/64 loss: -0.29673540592193604
Batch 49/64 loss: -0.2837890386581421
Batch 50/64 loss: -0.29061728715896606
Batch 51/64 loss: -0.29409778118133545
Batch 52/64 loss: -0.2970066964626312
Batch 53/64 loss: -0.29217880964279175
Batch 54/64 loss: -0.3061082661151886
Batch 55/64 loss: -0.2828257083892822
Batch 56/64 loss: -0.2833724915981293
Batch 57/64 loss: -0.28907525539398193
Batch 58/64 loss: -0.2937529385089874
Batch 59/64 loss: -0.302425742149353
Batch 60/64 loss: -0.29608458280563354
Batch 61/64 loss: -0.2978668212890625
Batch 62/64 loss: -0.27186334133148193
Batch 63/64 loss: -0.3005759119987488
Batch 64/64 loss: -0.2964176833629608
Epoch 499  Train loss: -0.2923347404190138  Val loss: -0.27411289886920315
Epoch 500
-------------------------------
Batch 1/64 loss: -0.30429863929748535
Batch 2/64 loss: -0.28638726472854614
Batch 3/64 loss: -0.29859161376953125
Batch 4/64 loss: -0.2996765077114105
Batch 5/64 loss: -0.28159627318382263
Batch 6/64 loss: -0.2983095645904541
Batch 7/64 loss: -0.29439008235931396
Batch 8/64 loss: -0.2946241796016693
Batch 9/64 loss: -0.29816508293151855
Batch 10/64 loss: -0.29115843772888184
Batch 11/64 loss: -0.29268360137939453
Batch 12/64 loss: -0.2898188531398773
Batch 13/64 loss: -0.2928317189216614
Batch 14/64 loss: -0.29630598425865173
Batch 15/64 loss: -0.2788802981376648
Batch 16/64 loss: -0.28266066312789917
Batch 17/64 loss: -0.3047662079334259
Batch 18/64 loss: -0.28302329778671265
Batch 19/64 loss: -0.28501853346824646
Batch 20/64 loss: -0.2960491478443146
Batch 21/64 loss: -0.2898241877555847
Batch 22/64 loss: -0.2739816904067993
Batch 23/64 loss: -0.2881285846233368
Batch 24/64 loss: -0.29708802700042725
Batch 25/64 loss: -0.2906208634376526
Batch 26/64 loss: -0.2962643802165985
Batch 27/64 loss: -0.2917513847351074
Batch 28/64 loss: -0.29144418239593506
Batch 29/64 loss: -0.29548394680023193
Batch 30/64 loss: -0.2870059013366699
Batch 31/64 loss: -0.292792409658432
Batch 32/64 loss: -0.2932783365249634
Batch 33/64 loss: -0.2928979694843292
Batch 34/64 loss: -0.30224236845970154
Batch 35/64 loss: -0.31260719895362854
Batch 36/64 loss: -0.2919241189956665
Batch 37/64 loss: -0.28532859683036804
Batch 38/64 loss: -0.27650871872901917
Batch 39/64 loss: -0.29371288418769836
Batch 40/64 loss: -0.29210612177848816
Batch 41/64 loss: -0.2847755253314972
Batch 42/64 loss: -0.2736460268497467
Batch 43/64 loss: -0.3011225461959839
Batch 44/64 loss: -0.30875587463378906
Batch 45/64 loss: -0.2896748185157776
Batch 46/64 loss: -0.2852458953857422
Batch 47/64 loss: -0.2865341603755951
Batch 48/64 loss: -0.29331499338150024
Batch 49/64 loss: -0.28624337911605835
Batch 50/64 loss: -0.2886042296886444
Batch 51/64 loss: -0.292502760887146
Batch 52/64 loss: -0.27534955739974976
Batch 53/64 loss: -0.2858227491378784
Batch 54/64 loss: -0.2805989384651184
Batch 55/64 loss: -0.2972841262817383
Batch 56/64 loss: -0.29328083992004395
Batch 57/64 loss: -0.30047231912612915
Batch 58/64 loss: -0.2874128818511963
Batch 59/64 loss: -0.2919384241104126
Batch 60/64 loss: -0.29272371530532837
Batch 61/64 loss: -0.2966165840625763
Batch 62/64 loss: -0.28367120027542114
Batch 63/64 loss: -0.2748563289642334
Batch 64/64 loss: -0.3019285798072815
Epoch 500  Train loss: -0.2911548570090649  Val loss: -0.26514259974161786
SLIC undersegmentation error: 0.17155876288659785
SLIC inter-cluster variation: 0.2547478839367162
SLIC number of superpixels: 9669
SLIC superpixels per image: 33.22680412371134
Model loaded
Test metrics:
-0.28690490546505065 0.42843711340206186 23.71864175640179 tensor(0.4537, dtype=torch.float64) 0.9662162162162162 6.286196911196911 14504
Inference time: 0.003951737970830649 seconds
Relabeled undersegmentation error: 0.09427766323024056
Relabeled inter-cluster variation: 0.06263559581750318
Relabeled mean superpixels count: 313.30927835051546
Original mean superpixels count: 49.84192439862543
Done!
Job id: 420590
Job id: 422807
Job id: 422861
