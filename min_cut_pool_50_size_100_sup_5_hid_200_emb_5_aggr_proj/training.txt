Started preprocessing dataset
Number of training samples: 2040
Number of validation samples: 582
Number of testing samples: 291
Using cuda device
Epoch 1
-------------------------------
Batch 1/64 loss: 0.4528687596321106
Batch 2/64 loss: 0.36651670932769775
Batch 3/64 loss: 0.34519457817077637
Batch 4/64 loss: 0.3388082981109619
Batch 5/64 loss: 0.3317211866378784
Batch 6/64 loss: 0.3246920108795166
Batch 7/64 loss: 0.32311999797821045
Batch 8/64 loss: 0.32151365280151367
Batch 9/64 loss: 0.32493579387664795
Batch 10/64 loss: 0.32176119089126587
Batch 11/64 loss: 0.3196009397506714
Batch 12/64 loss: 0.31976163387298584
Batch 13/64 loss: 0.314874529838562
Batch 14/64 loss: 0.3169938325881958
Batch 15/64 loss: 0.31380176544189453
Batch 16/64 loss: 0.31244152784347534
Batch 17/64 loss: 0.3106980323791504
Batch 18/64 loss: 0.30826908349990845
Batch 19/64 loss: 0.3093928098678589
Batch 20/64 loss: 0.31017065048217773
Batch 21/64 loss: 0.3079336881637573
Batch 22/64 loss: 0.30411362648010254
Batch 23/64 loss: 0.3053938150405884
Batch 24/64 loss: 0.2987862825393677
Batch 25/64 loss: 0.29423606395721436
Batch 26/64 loss: 0.3011329174041748
Batch 27/64 loss: 0.3011400103569031
Batch 28/64 loss: 0.30079197883605957
Batch 29/64 loss: 0.2953519821166992
Batch 30/64 loss: 0.2965729236602783
Batch 31/64 loss: 0.29128825664520264
Batch 32/64 loss: 0.28992605209350586
Batch 33/64 loss: 0.2909829616546631
Batch 34/64 loss: 0.2971383333206177
Batch 35/64 loss: 0.2837609648704529
Batch 36/64 loss: 0.28700190782546997
Batch 37/64 loss: 0.28002840280532837
Batch 38/64 loss: 0.2816506624221802
Batch 39/64 loss: 0.29004597663879395
Batch 40/64 loss: 0.2815816402435303
Batch 41/64 loss: 0.27839821577072144
Batch 42/64 loss: 0.2881408929824829
Batch 43/64 loss: 0.284984827041626
Batch 44/64 loss: 0.2843524217605591
Batch 45/64 loss: 0.2824975848197937
Batch 46/64 loss: 0.28488731384277344
Batch 47/64 loss: 0.2774878144264221
Batch 48/64 loss: 0.28286832571029663
Batch 49/64 loss: 0.2813147306442261
Batch 50/64 loss: 0.28196895122528076
Batch 51/64 loss: 0.27272820472717285
Batch 52/64 loss: 0.271382212638855
Batch 53/64 loss: 0.2729474902153015
Batch 54/64 loss: 0.2734941244125366
Batch 55/64 loss: 0.27345746755599976
Batch 56/64 loss: 0.2716323733329773
Batch 57/64 loss: 0.2717762589454651
Batch 58/64 loss: 0.27008533477783203
Batch 59/64 loss: 0.2613423466682434
Batch 60/64 loss: 0.2720527648925781
Batch 61/64 loss: 0.26127874851226807
Batch 62/64 loss: 0.2628138065338135
Batch 63/64 loss: 0.27295422554016113
Batch 64/64 loss: 0.2757633924484253
Epoch 1  Train loss: 0.29815944176094206  Val loss: 0.28200402763700977
Saving best model, epoch: 1
Epoch 2
-------------------------------
Batch 1/64 loss: 0.26291191577911377
Batch 2/64 loss: 0.2745591998100281
Batch 3/64 loss: 0.27743881940841675
Batch 4/64 loss: 0.263982892036438
Batch 5/64 loss: 0.2679988741874695
Batch 6/64 loss: 0.26541662216186523
Batch 7/64 loss: 0.2718181610107422
Batch 8/64 loss: 0.2680644989013672
Batch 9/64 loss: 0.26875054836273193
Batch 10/64 loss: 0.2627739906311035
Batch 11/64 loss: 0.2664916515350342
Batch 12/64 loss: 0.26176369190216064
Batch 13/64 loss: 0.2602503299713135
Batch 14/64 loss: 0.268357515335083
Batch 15/64 loss: 0.26548993587493896
Batch 16/64 loss: 0.2588949203491211
Batch 17/64 loss: 0.2575662136077881
Batch 18/64 loss: 0.2652103900909424
Batch 19/64 loss: 0.2543497681617737
Batch 20/64 loss: 0.25069236755371094
Batch 21/64 loss: 0.26512372493743896
Batch 22/64 loss: 0.2557307481765747
Batch 23/64 loss: 0.25960659980773926
Batch 24/64 loss: 0.25497233867645264
Batch 25/64 loss: 0.2564241290092468
Batch 26/64 loss: 0.25575363636016846
Batch 27/64 loss: 0.2508430480957031
Batch 28/64 loss: 0.25064408779144287
Batch 29/64 loss: 0.2601894736289978
Batch 30/64 loss: 0.25995510816574097
Batch 31/64 loss: 0.2508930563926697
Batch 32/64 loss: 0.24339061975479126
Batch 33/64 loss: 0.24963200092315674
Batch 34/64 loss: 0.2464621663093567
Batch 35/64 loss: 0.25686681270599365
Batch 36/64 loss: 0.25516122579574585
Batch 37/64 loss: 0.24942803382873535
Batch 38/64 loss: 0.23954689502716064
Batch 39/64 loss: 0.25497448444366455
Batch 40/64 loss: 0.25217097997665405
Batch 41/64 loss: 0.24217921495437622
Batch 42/64 loss: 0.2521967887878418
Batch 43/64 loss: 0.24267816543579102
Batch 44/64 loss: 0.24943649768829346
Batch 45/64 loss: 0.24521315097808838
Batch 46/64 loss: 0.24871498346328735
Batch 47/64 loss: 0.24810445308685303
Batch 48/64 loss: 0.24743306636810303
Batch 49/64 loss: 0.2398761510848999
Batch 50/64 loss: 0.2502216100692749
Batch 51/64 loss: 0.24720430374145508
Batch 52/64 loss: 0.24424785375595093
Batch 53/64 loss: 0.24553775787353516
Batch 54/64 loss: 0.24620968103408813
Batch 55/64 loss: 0.24848049879074097
Batch 56/64 loss: 0.2305614948272705
Batch 57/64 loss: 0.23777854442596436
Batch 58/64 loss: 0.24265611171722412
Batch 59/64 loss: 0.23685383796691895
Batch 60/64 loss: 0.2314196228981018
Batch 61/64 loss: 0.24674087762832642
Batch 62/64 loss: 0.23717272281646729
Batch 63/64 loss: 0.2320019006729126
Batch 64/64 loss: 0.2362613081932068
Epoch 2  Train loss: 0.25303006663041955  Val loss: 0.2479199908443333
Saving best model, epoch: 2
Epoch 3
-------------------------------
Batch 1/64 loss: 0.2525731921195984
Batch 2/64 loss: 0.24466508626937866
Batch 3/64 loss: 0.23542439937591553
Batch 4/64 loss: 0.24138611555099487
Batch 5/64 loss: 0.2309657335281372
Batch 6/64 loss: 0.24615478515625
Batch 7/64 loss: 0.2244948148727417
Batch 8/64 loss: 0.23771196603775024
Batch 9/64 loss: 0.2494441270828247
Batch 10/64 loss: 0.24972212314605713
Batch 11/64 loss: 0.24544310569763184
Batch 12/64 loss: 0.23202139139175415
Batch 13/64 loss: 0.23480522632598877
Batch 14/64 loss: 0.23504751920700073
Batch 15/64 loss: 0.24013173580169678
Batch 16/64 loss: 0.24213778972625732
Batch 17/64 loss: 0.23938369750976562
Batch 18/64 loss: 0.24085569381713867
Batch 19/64 loss: 0.23182660341262817
Batch 20/64 loss: 0.2475745677947998
Batch 21/64 loss: 0.22937369346618652
Batch 22/64 loss: 0.24011915922164917
Batch 23/64 loss: 0.2352125644683838
Batch 24/64 loss: 0.2254006266593933
Batch 25/64 loss: 0.23882567882537842
Batch 26/64 loss: 0.2352808713912964
Batch 27/64 loss: 0.2165292501449585
Batch 28/64 loss: 0.24088168144226074
Batch 29/64 loss: 0.22827684879302979
Batch 30/64 loss: 0.23535168170928955
Batch 31/64 loss: 0.23805081844329834
Batch 32/64 loss: 0.2411208152770996
Batch 33/64 loss: 0.21414107084274292
Batch 34/64 loss: 0.2309083342552185
Batch 35/64 loss: 0.239501953125
Batch 36/64 loss: 0.22945082187652588
Batch 37/64 loss: 0.23731005191802979
Batch 38/64 loss: 0.22182327508926392
Batch 39/64 loss: 0.2279142141342163
Batch 40/64 loss: 0.22125262022018433
Batch 41/64 loss: 0.2305898666381836
Batch 42/64 loss: 0.2309454083442688
Batch 43/64 loss: 0.22099369764328003
Batch 44/64 loss: 0.23659038543701172
Batch 45/64 loss: 0.22769176959991455
Batch 46/64 loss: 0.22811013460159302
Batch 47/64 loss: 0.22951793670654297
Batch 48/64 loss: 0.23541271686553955
Batch 49/64 loss: 0.2244887351989746
Batch 50/64 loss: 0.23618292808532715
Batch 51/64 loss: 0.23126065731048584
Batch 52/64 loss: 0.22845745086669922
Batch 53/64 loss: 0.22353994846343994
Batch 54/64 loss: 0.228351891040802
Batch 55/64 loss: 0.21960216760635376
Batch 56/64 loss: 0.22747623920440674
Batch 57/64 loss: 0.23192381858825684
Batch 58/64 loss: 0.2206462025642395
Batch 59/64 loss: 0.21843522787094116
Batch 60/64 loss: 0.22698283195495605
Batch 61/64 loss: 0.22299551963806152
Batch 62/64 loss: 0.2254488468170166
Batch 63/64 loss: 0.23320460319519043
Batch 64/64 loss: 0.22178727388381958
Epoch 3  Train loss: 0.23268525717305202  Val loss: 0.2407364884193001
Saving best model, epoch: 3
Epoch 4
-------------------------------
Batch 1/64 loss: 0.2241421937942505
Batch 2/64 loss: 0.23451846837997437
Batch 3/64 loss: 0.2340395450592041
Batch 4/64 loss: 0.21728116273880005
Batch 5/64 loss: 0.21823644638061523
Batch 6/64 loss: 0.22011590003967285
Batch 7/64 loss: 0.223688542842865
Batch 8/64 loss: 0.23162144422531128
Batch 9/64 loss: 0.23399269580841064
Batch 10/64 loss: 0.22987043857574463
Batch 11/64 loss: 0.23738247156143188
Batch 12/64 loss: 0.2230026125907898
Batch 13/64 loss: 0.22368216514587402
Batch 14/64 loss: 0.21755683422088623
Batch 15/64 loss: 0.21829688549041748
Batch 16/64 loss: 0.21364176273345947
Batch 17/64 loss: 0.2248234748840332
Batch 18/64 loss: 0.23541301488876343
Batch 19/64 loss: 0.23167306184768677
Batch 20/64 loss: 0.2213151454925537
Batch 21/64 loss: 0.2386299967765808
Batch 22/64 loss: 0.23254001140594482
Batch 23/64 loss: 0.22594547271728516
Batch 24/64 loss: 0.21803081035614014
Batch 25/64 loss: 0.22057592868804932
Batch 26/64 loss: 0.21595895290374756
Batch 27/64 loss: 0.22178268432617188
Batch 28/64 loss: 0.2299898862838745
Batch 29/64 loss: 0.22880268096923828
Batch 30/64 loss: 0.23041033744812012
Batch 31/64 loss: 0.22852826118469238
Batch 32/64 loss: 0.21988826990127563
Batch 33/64 loss: 0.228035569190979
Batch 34/64 loss: 0.22032946348190308
Batch 35/64 loss: 0.22004270553588867
Batch 36/64 loss: 0.2104235291481018
Batch 37/64 loss: 0.2232062816619873
Batch 38/64 loss: 0.2260681390762329
Batch 39/64 loss: 0.2307577133178711
Batch 40/64 loss: 0.2129637598991394
Batch 41/64 loss: 0.21928685903549194
Batch 42/64 loss: 0.23423326015472412
Batch 43/64 loss: 0.2321406602859497
Batch 44/64 loss: 0.22946035861968994
Batch 45/64 loss: 0.2218329906463623
Batch 46/64 loss: 0.22168421745300293
Batch 47/64 loss: 0.2359302043914795
Batch 48/64 loss: 0.21476149559020996
Batch 49/64 loss: 0.22010469436645508
Batch 50/64 loss: 0.21823573112487793
Batch 51/64 loss: 0.22017204761505127
Batch 52/64 loss: 0.20896869897842407
Batch 53/64 loss: 0.22224724292755127
Batch 54/64 loss: 0.23129057884216309
Batch 55/64 loss: 0.23156225681304932
Batch 56/64 loss: 0.22204500436782837
Batch 57/64 loss: 0.22651469707489014
Batch 58/64 loss: 0.2146207094192505
Batch 59/64 loss: 0.21966814994812012
Batch 60/64 loss: 0.21416044235229492
Batch 61/64 loss: 0.22272276878356934
Batch 62/64 loss: 0.22528022527694702
Batch 63/64 loss: 0.2195720076560974
Batch 64/64 loss: 0.21954119205474854
Epoch 4  Train loss: 0.22413107320374134  Val loss: 0.22958263703637927
Saving best model, epoch: 4
Epoch 5
-------------------------------
Batch 1/64 loss: 0.22526347637176514
Batch 2/64 loss: 0.21845674514770508
Batch 3/64 loss: 0.21461641788482666
Batch 4/64 loss: 0.22073006629943848
Batch 5/64 loss: 0.22142183780670166
Batch 6/64 loss: 0.22428441047668457
Batch 7/64 loss: 0.22328180074691772
Batch 8/64 loss: 0.20923978090286255
Batch 9/64 loss: 0.22254419326782227
Batch 10/64 loss: 0.2298303246498108
Batch 11/64 loss: 0.2161838412284851
Batch 12/64 loss: 0.21637707948684692
Batch 13/64 loss: 0.20140445232391357
Batch 14/64 loss: 0.20378124713897705
Batch 15/64 loss: 0.21159160137176514
Batch 16/64 loss: 0.23415225744247437
Batch 17/64 loss: 0.21440088748931885
Batch 18/64 loss: 0.20618212223052979
Batch 19/64 loss: 0.21822214126586914
Batch 20/64 loss: 0.21282601356506348
Batch 21/64 loss: 0.21314769983291626
Batch 22/64 loss: 0.21617019176483154
Batch 23/64 loss: 0.2134574055671692
Batch 24/64 loss: 0.21984368562698364
Batch 25/64 loss: 0.20981502532958984
Batch 26/64 loss: 0.22020959854125977
Batch 27/64 loss: 0.21916699409484863
Batch 28/64 loss: 0.21379107236862183
Batch 29/64 loss: 0.20810407400131226
Batch 30/64 loss: 0.20605087280273438
Batch 31/64 loss: 0.20481926202774048
Batch 32/64 loss: 0.2116793394088745
Batch 33/64 loss: 0.20732712745666504
Batch 34/64 loss: 0.21480178833007812
Batch 35/64 loss: 0.22355902194976807
Batch 36/64 loss: 0.21490478515625
Batch 37/64 loss: 0.227056622505188
Batch 38/64 loss: 0.2069147825241089
Batch 39/64 loss: 0.2211012840270996
Batch 40/64 loss: 0.21305793523788452
Batch 41/64 loss: 0.21156394481658936
Batch 42/64 loss: 0.2030441164970398
Batch 43/64 loss: 0.2071346640586853
Batch 44/64 loss: 0.2121853232383728
Batch 45/64 loss: 0.21052855253219604
Batch 46/64 loss: 0.20562201738357544
Batch 47/64 loss: 0.20613956451416016
Batch 48/64 loss: 0.21124547719955444
Batch 49/64 loss: 0.21023398637771606
Batch 50/64 loss: 0.2099599838256836
Batch 51/64 loss: 0.21953678131103516
Batch 52/64 loss: 0.21807026863098145
Batch 53/64 loss: 0.21693623065948486
Batch 54/64 loss: 0.22411084175109863
Batch 55/64 loss: 0.21253538131713867
Batch 56/64 loss: 0.21390420198440552
Batch 57/64 loss: 0.20993781089782715
Batch 58/64 loss: 0.2134053111076355
Batch 59/64 loss: 0.22437989711761475
Batch 60/64 loss: 0.22305428981781006
Batch 61/64 loss: 0.21917730569839478
Batch 62/64 loss: 0.22033178806304932
Batch 63/64 loss: 0.20270371437072754
Batch 64/64 loss: 0.21037673950195312
Epoch 5  Train loss: 0.21479675723057168  Val loss: 0.2214309612090645
Saving best model, epoch: 5
Epoch 6
-------------------------------
Batch 1/64 loss: 0.2107682228088379
Batch 2/64 loss: 0.20926576852798462
Batch 3/64 loss: 0.2084600329399109
Batch 4/64 loss: 0.21858233213424683
Batch 5/64 loss: 0.21611642837524414
Batch 6/64 loss: 0.205116868019104
Batch 7/64 loss: 0.19716238975524902
Batch 8/64 loss: 0.21744823455810547
Batch 9/64 loss: 0.21023035049438477
Batch 10/64 loss: 0.22091490030288696
Batch 11/64 loss: 0.20425760746002197
Batch 12/64 loss: 0.21165752410888672
Batch 13/64 loss: 0.2030198574066162
Batch 14/64 loss: 0.20068812370300293
Batch 15/64 loss: 0.2145402431488037
Batch 16/64 loss: 0.20933294296264648
Batch 17/64 loss: 0.1968977451324463
Batch 18/64 loss: 0.2160351276397705
Batch 19/64 loss: 0.21083295345306396
Batch 20/64 loss: 0.20250821113586426
Batch 21/64 loss: 0.20107591152191162
Batch 22/64 loss: 0.2193995714187622
Batch 23/64 loss: 0.2061215043067932
Batch 24/64 loss: 0.20285356044769287
Batch 25/64 loss: 0.21652257442474365
Batch 26/64 loss: 0.21492135524749756
Batch 27/64 loss: 0.20758622884750366
Batch 28/64 loss: 0.20501458644866943
Batch 29/64 loss: 0.2051612138748169
Batch 30/64 loss: 0.21329021453857422
Batch 31/64 loss: 0.2063935399055481
Batch 32/64 loss: 0.21778017282485962
Batch 33/64 loss: 0.2054586410522461
Batch 34/64 loss: 0.22524189949035645
Batch 35/64 loss: 0.21459084749221802
Batch 36/64 loss: 0.21246862411499023
Batch 37/64 loss: 0.19764316082000732
Batch 38/64 loss: 0.1947464942932129
Batch 39/64 loss: 0.2054010033607483
Batch 40/64 loss: 0.20489883422851562
Batch 41/64 loss: 0.1997915506362915
Batch 42/64 loss: 0.1990135908126831
Batch 43/64 loss: 0.20481479167938232
Batch 44/64 loss: 0.20373374223709106
Batch 45/64 loss: 0.19755756855010986
Batch 46/64 loss: 0.20388388633728027
Batch 47/64 loss: 0.20022982358932495
Batch 48/64 loss: 0.20124804973602295
Batch 49/64 loss: 0.21079480648040771
Batch 50/64 loss: 0.19803285598754883
Batch 51/64 loss: 0.19703346490859985
Batch 52/64 loss: 0.21513426303863525
Batch 53/64 loss: 0.19405698776245117
Batch 54/64 loss: 0.2010403871536255
Batch 55/64 loss: 0.20711493492126465
Batch 56/64 loss: 0.1960088610649109
Batch 57/64 loss: 0.2052464485168457
Batch 58/64 loss: 0.20713460445404053
Batch 59/64 loss: 0.1954212188720703
Batch 60/64 loss: 0.19482839107513428
Batch 61/64 loss: 0.20462578535079956
Batch 62/64 loss: 0.19554823637008667
Batch 63/64 loss: 0.19528424739837646
Batch 64/64 loss: 0.19628596305847168
Epoch 6  Train loss: 0.20604233367770325  Val loss: 0.22218399850773238
Epoch 7
-------------------------------
Batch 1/64 loss: 0.18896633386611938
Batch 2/64 loss: 0.192898690700531
Batch 3/64 loss: 0.19533562660217285
Batch 4/64 loss: 0.2039763331413269
Batch 5/64 loss: 0.1943567991256714
Batch 6/64 loss: 0.19794821739196777
Batch 7/64 loss: 0.1980918049812317
Batch 8/64 loss: 0.19198918342590332
Batch 9/64 loss: 0.1823686957359314
Batch 10/64 loss: 0.20404815673828125
Batch 11/64 loss: 0.1985645294189453
Batch 12/64 loss: 0.21148371696472168
Batch 13/64 loss: 0.1954602599143982
Batch 14/64 loss: 0.21494007110595703
Batch 15/64 loss: 0.2120494246482849
Batch 16/64 loss: 0.1908661127090454
Batch 17/64 loss: 0.18969297409057617
Batch 18/64 loss: 0.19819188117980957
Batch 19/64 loss: 0.2168484330177307
Batch 20/64 loss: 0.19556844234466553
Batch 21/64 loss: 0.19669818878173828
Batch 22/64 loss: 0.19158589839935303
Batch 23/64 loss: 0.1876513957977295
Batch 24/64 loss: 0.2054445743560791
Batch 25/64 loss: 0.2085784673690796
Batch 26/64 loss: 0.19826006889343262
Batch 27/64 loss: 0.19987249374389648
Batch 28/64 loss: 0.19173723459243774
Batch 29/64 loss: 0.19044625759124756
Batch 30/64 loss: 0.19897472858428955
Batch 31/64 loss: 0.193475604057312
Batch 32/64 loss: 0.19982481002807617
Batch 33/64 loss: 0.20312237739562988
Batch 34/64 loss: 0.18068015575408936
Batch 35/64 loss: 0.17715036869049072
Batch 36/64 loss: 0.2143552303314209
Batch 37/64 loss: 0.19279468059539795
Batch 38/64 loss: 0.192102313041687
Batch 39/64 loss: 0.19310879707336426
Batch 40/64 loss: 0.18848192691802979
Batch 41/64 loss: 0.20016992092132568
Batch 42/64 loss: 0.20066142082214355
Batch 43/64 loss: 0.20447206497192383
Batch 44/64 loss: 0.1835615038871765
Batch 45/64 loss: 0.19293320178985596
Batch 46/64 loss: 0.20677483081817627
Batch 47/64 loss: 0.18494027853012085
Batch 48/64 loss: 0.1934106945991516
Batch 49/64 loss: 0.19890999794006348
Batch 50/64 loss: 0.18734264373779297
Batch 51/64 loss: 0.19735604524612427
Batch 52/64 loss: 0.20641613006591797
Batch 53/64 loss: 0.18572676181793213
Batch 54/64 loss: 0.19984352588653564
Batch 55/64 loss: 0.19082331657409668
Batch 56/64 loss: 0.1947709321975708
Batch 57/64 loss: 0.17957746982574463
Batch 58/64 loss: 0.19891107082366943
Batch 59/64 loss: 0.194848895072937
Batch 60/64 loss: 0.19316750764846802
Batch 61/64 loss: 0.1971445083618164
Batch 62/64 loss: 0.20374011993408203
Batch 63/64 loss: 0.20719873905181885
Batch 64/64 loss: 0.20131772756576538
Epoch 7  Train loss: 0.1965753903575972  Val loss: 0.2075813476162678
Saving best model, epoch: 7
Epoch 8
-------------------------------
Batch 1/64 loss: 0.18857693672180176
Batch 2/64 loss: 0.17998230457305908
Batch 3/64 loss: 0.18908077478408813
Batch 4/64 loss: 0.18339943885803223
Batch 5/64 loss: 0.19656044244766235
Batch 6/64 loss: 0.19189375638961792
Batch 7/64 loss: 0.17069697380065918
Batch 8/64 loss: 0.19872450828552246
Batch 9/64 loss: 0.18949627876281738
Batch 10/64 loss: 0.19436907768249512
Batch 11/64 loss: 0.19221317768096924
Batch 12/64 loss: 0.20037591457366943
Batch 13/64 loss: 0.1817387342453003
Batch 14/64 loss: 0.18324387073516846
Batch 15/64 loss: 0.1971164345741272
Batch 16/64 loss: 0.190757155418396
Batch 17/64 loss: 0.18467187881469727
Batch 18/64 loss: 0.2162877321243286
Batch 19/64 loss: 0.20127063989639282
Batch 20/64 loss: 0.20610451698303223
Batch 21/64 loss: 0.19454503059387207
Batch 22/64 loss: 0.1917262077331543
Batch 23/64 loss: 0.198086678981781
Batch 24/64 loss: 0.19086730480194092
Batch 25/64 loss: 0.19120705127716064
Batch 26/64 loss: 0.19715964794158936
Batch 27/64 loss: 0.18454253673553467
Batch 28/64 loss: 0.18439459800720215
Batch 29/64 loss: 0.19385278224945068
Batch 30/64 loss: 0.1802215576171875
Batch 31/64 loss: 0.1925656795501709
Batch 32/64 loss: 0.17721933126449585
Batch 33/64 loss: 0.18534481525421143
Batch 34/64 loss: 0.18704533576965332
Batch 35/64 loss: 0.18901526927947998
Batch 36/64 loss: 0.1989147663116455
Batch 37/64 loss: 0.19055050611495972
Batch 38/64 loss: 0.17777800559997559
Batch 39/64 loss: 0.19403457641601562
Batch 40/64 loss: 0.19528526067733765
Batch 41/64 loss: 0.20050311088562012
Batch 42/64 loss: 0.21076220273971558
Batch 43/64 loss: 0.18298494815826416
Batch 44/64 loss: 0.1883333921432495
Batch 45/64 loss: 0.18578922748565674
Batch 46/64 loss: 0.17154860496520996
Batch 47/64 loss: 0.1884068250656128
Batch 48/64 loss: 0.1723528504371643
Batch 49/64 loss: 0.16969531774520874
Batch 50/64 loss: 0.1868349313735962
Batch 51/64 loss: 0.1959955096244812
Batch 52/64 loss: 0.18914508819580078
Batch 53/64 loss: 0.17631232738494873
Batch 54/64 loss: 0.17557108402252197
Batch 55/64 loss: 0.1803768277168274
Batch 56/64 loss: 0.1861633062362671
Batch 57/64 loss: 0.1710498332977295
Batch 58/64 loss: 0.1812676191329956
Batch 59/64 loss: 0.1874845027923584
Batch 60/64 loss: 0.17309391498565674
Batch 61/64 loss: 0.18952006101608276
Batch 62/64 loss: 0.18148040771484375
Batch 63/64 loss: 0.20765697956085205
Batch 64/64 loss: 0.19279897212982178
Epoch 8  Train loss: 0.18867208770677155  Val loss: 0.19485180136264393
Saving best model, epoch: 8
Epoch 9
-------------------------------
Batch 1/64 loss: 0.17402803897857666
Batch 2/64 loss: 0.1861894130706787
Batch 3/64 loss: 0.18750739097595215
Batch 4/64 loss: 0.18232381343841553
Batch 5/64 loss: 0.17946553230285645
Batch 6/64 loss: 0.19095981121063232
Batch 7/64 loss: 0.1899738311767578
Batch 8/64 loss: 0.17594194412231445
Batch 9/64 loss: 0.189711332321167
Batch 10/64 loss: 0.191703200340271
Batch 11/64 loss: 0.16491496562957764
Batch 12/64 loss: 0.1875426173210144
Batch 13/64 loss: 0.1774228811264038
Batch 14/64 loss: 0.1801130771636963
Batch 15/64 loss: 0.1781303882598877
Batch 16/64 loss: 0.17096853256225586
Batch 17/64 loss: 0.19751501083374023
Batch 18/64 loss: 0.1746511459350586
Batch 19/64 loss: 0.1882343292236328
Batch 20/64 loss: 0.16799795627593994
Batch 21/64 loss: 0.1758754849433899
Batch 22/64 loss: 0.1768452525138855
Batch 23/64 loss: 0.1769692301750183
Batch 24/64 loss: 0.180733323097229
Batch 25/64 loss: 0.18860304355621338
Batch 26/64 loss: 0.17101049423217773
Batch 27/64 loss: 0.1701870560646057
Batch 28/64 loss: 0.1886286735534668
Batch 29/64 loss: 0.19429129362106323
Batch 30/64 loss: 0.19635212421417236
Batch 31/64 loss: 0.18653494119644165
Batch 32/64 loss: 0.17980611324310303
Batch 33/64 loss: 0.17820650339126587
Batch 34/64 loss: 0.17586255073547363
Batch 35/64 loss: 0.19239437580108643
Batch 36/64 loss: 0.18512845039367676
Batch 37/64 loss: 0.17507702112197876
Batch 38/64 loss: 0.18616807460784912
Batch 39/64 loss: 0.17730510234832764
Batch 40/64 loss: 0.1822570562362671
Batch 41/64 loss: 0.17506027221679688
Batch 42/64 loss: 0.17208373546600342
Batch 43/64 loss: 0.17270541191101074
Batch 44/64 loss: 0.17929744720458984
Batch 45/64 loss: 0.1798725128173828
Batch 46/64 loss: 0.16944360733032227
Batch 47/64 loss: 0.16513192653656006
Batch 48/64 loss: 0.17608976364135742
Batch 49/64 loss: 0.172021746635437
Batch 50/64 loss: 0.18469679355621338
Batch 51/64 loss: 0.1688109040260315
Batch 52/64 loss: 0.17150557041168213
Batch 53/64 loss: 0.18311703205108643
Batch 54/64 loss: 0.17249715328216553
Batch 55/64 loss: 0.19214189052581787
Batch 56/64 loss: 0.17713499069213867
Batch 57/64 loss: 0.16615080833435059
Batch 58/64 loss: 0.17485696077346802
Batch 59/64 loss: 0.17995792627334595
Batch 60/64 loss: 0.16842341423034668
Batch 61/64 loss: 0.17294436693191528
Batch 62/64 loss: 0.18316739797592163
Batch 63/64 loss: 0.16814351081848145
Batch 64/64 loss: 0.18110239505767822
Epoch 9  Train loss: 0.17936648340786204  Val loss: 0.19398804014081397
Saving best model, epoch: 9
Epoch 10
-------------------------------
Batch 1/64 loss: 0.18764251470565796
Batch 2/64 loss: 0.1685107946395874
Batch 3/64 loss: 0.1692051887512207
Batch 4/64 loss: 0.17776739597320557
Batch 5/64 loss: 0.18468797206878662
Batch 6/64 loss: 0.17532360553741455
Batch 7/64 loss: 0.16749083995819092
Batch 8/64 loss: 0.1673746109008789
Batch 9/64 loss: 0.19039463996887207
Batch 10/64 loss: 0.16884928941726685
Batch 11/64 loss: 0.1669776439666748
Batch 12/64 loss: 0.1692851185798645
Batch 13/64 loss: 0.16429275274276733
Batch 14/64 loss: 0.16291558742523193
Batch 15/64 loss: 0.14658451080322266
Batch 16/64 loss: 0.19022297859191895
Batch 17/64 loss: 0.17613649368286133
Batch 18/64 loss: 0.1655845046043396
Batch 19/64 loss: 0.17165279388427734
Batch 20/64 loss: 0.17874538898468018
Batch 21/64 loss: 0.1622772216796875
Batch 22/64 loss: 0.17742019891738892
Batch 23/64 loss: 0.17461776733398438
Batch 24/64 loss: 0.16566884517669678
Batch 25/64 loss: 0.185632586479187
Batch 26/64 loss: 0.16315865516662598
Batch 27/64 loss: 0.17615079879760742
Batch 28/64 loss: 0.15633976459503174
Batch 29/64 loss: 0.16798031330108643
Batch 30/64 loss: 0.17252123355865479
Batch 31/64 loss: 0.18334519863128662
Batch 32/64 loss: 0.17528343200683594
Batch 33/64 loss: 0.1669061779975891
Batch 34/64 loss: 0.17514503002166748
Batch 35/64 loss: 0.16504502296447754
Batch 36/64 loss: 0.15641558170318604
Batch 37/64 loss: 0.16884279251098633
Batch 38/64 loss: 0.1644660234451294
Batch 39/64 loss: 0.16655397415161133
Batch 40/64 loss: 0.16380935907363892
Batch 41/64 loss: 0.16511142253875732
Batch 42/64 loss: 0.17103934288024902
Batch 43/64 loss: 0.1623239517211914
Batch 44/64 loss: 0.16776251792907715
Batch 45/64 loss: 0.18237853050231934
Batch 46/64 loss: 0.17622703313827515
Batch 47/64 loss: 0.16412222385406494
Batch 48/64 loss: 0.17037087678909302
Batch 49/64 loss: 0.16938650608062744
Batch 50/64 loss: 0.15948891639709473
Batch 51/64 loss: 0.16725945472717285
Batch 52/64 loss: 0.1685405969619751
Batch 53/64 loss: 0.15718460083007812
Batch 54/64 loss: 0.16150420904159546
Batch 55/64 loss: 0.15740644931793213
Batch 56/64 loss: 0.16255664825439453
Batch 57/64 loss: 0.1577901840209961
Batch 58/64 loss: 0.1585843563079834
Batch 59/64 loss: 0.16950064897537231
Batch 60/64 loss: 0.16693741083145142
Batch 61/64 loss: 0.1723429560661316
Batch 62/64 loss: 0.18756616115570068
Batch 63/64 loss: 0.16542011499404907
Batch 64/64 loss: 0.19537299871444702
Epoch 10  Train loss: 0.16979701121648152  Val loss: 0.18718879206483716
Saving best model, epoch: 10
Epoch 11
-------------------------------
Batch 1/64 loss: 0.1617339849472046
Batch 2/64 loss: 0.15469884872436523
Batch 3/64 loss: 0.17287331819534302
Batch 4/64 loss: 0.18210870027542114
Batch 5/64 loss: 0.18548130989074707
Batch 6/64 loss: 0.16376930475234985
Batch 7/64 loss: 0.17324614524841309
Batch 8/64 loss: 0.18447375297546387
Batch 9/64 loss: 0.17097574472427368
Batch 10/64 loss: 0.16558003425598145
Batch 11/64 loss: 0.15102195739746094
Batch 12/64 loss: 0.15773433446884155
Batch 13/64 loss: 0.16210299730300903
Batch 14/64 loss: 0.1603604555130005
Batch 15/64 loss: 0.1735748052597046
Batch 16/64 loss: 0.18186640739440918
Batch 17/64 loss: 0.17352116107940674
Batch 18/64 loss: 0.1539400815963745
Batch 19/64 loss: 0.1801256537437439
Batch 20/64 loss: 0.1653738021850586
Batch 21/64 loss: 0.15352195501327515
Batch 22/64 loss: 0.16203725337982178
Batch 23/64 loss: 0.16191625595092773
Batch 24/64 loss: 0.1691930890083313
Batch 25/64 loss: 0.16099005937576294
Batch 26/64 loss: 0.16223156452178955
Batch 27/64 loss: 0.16460323333740234
Batch 28/64 loss: 0.14036977291107178
Batch 29/64 loss: 0.14918649196624756
Batch 30/64 loss: 0.15215450525283813
Batch 31/64 loss: 0.15023255348205566
Batch 32/64 loss: 0.1494382619857788
Batch 33/64 loss: 0.1666865348815918
Batch 34/64 loss: 0.15986859798431396
Batch 35/64 loss: 0.15387463569641113
Batch 36/64 loss: 0.16966092586517334
Batch 37/64 loss: 0.14036911725997925
Batch 38/64 loss: 0.16179251670837402
Batch 39/64 loss: 0.1656063199043274
Batch 40/64 loss: 0.15467500686645508
Batch 41/64 loss: 0.1562117338180542
Batch 42/64 loss: 0.16238486766815186
Batch 43/64 loss: 0.16842645406723022
Batch 44/64 loss: 0.16375267505645752
Batch 45/64 loss: 0.1644473671913147
Batch 46/64 loss: 0.16948330402374268
Batch 47/64 loss: 0.1473674774169922
Batch 48/64 loss: 0.146173357963562
Batch 49/64 loss: 0.15510153770446777
Batch 50/64 loss: 0.15533924102783203
Batch 51/64 loss: 0.14316314458847046
Batch 52/64 loss: 0.15962165594100952
Batch 53/64 loss: 0.1689983606338501
Batch 54/64 loss: 0.14494121074676514
Batch 55/64 loss: 0.14574873447418213
Batch 56/64 loss: 0.13366687297821045
Batch 57/64 loss: 0.14058345556259155
Batch 58/64 loss: 0.14424103498458862
Batch 59/64 loss: 0.15027689933776855
Batch 60/64 loss: 0.1408337950706482
Batch 61/64 loss: 0.16092348098754883
Batch 62/64 loss: 0.14107352495193481
Batch 63/64 loss: 0.15691709518432617
Batch 64/64 loss: 0.15746957063674927
Epoch 11  Train loss: 0.1593843280100355  Val loss: 0.17411995056978205
Saving best model, epoch: 11
Epoch 12
-------------------------------
Batch 1/64 loss: 0.14251363277435303
Batch 2/64 loss: 0.1600785255432129
Batch 3/64 loss: 0.16546869277954102
Batch 4/64 loss: 0.14677107334136963
Batch 5/64 loss: 0.14651250839233398
Batch 6/64 loss: 0.1434628963470459
Batch 7/64 loss: 0.15506738424301147
Batch 8/64 loss: 0.13283729553222656
Batch 9/64 loss: 0.16632425785064697
Batch 10/64 loss: 0.14505892992019653
Batch 11/64 loss: 0.15915733575820923
Batch 12/64 loss: 0.14011788368225098
Batch 13/64 loss: 0.13411223888397217
Batch 14/64 loss: 0.15960633754730225
Batch 15/64 loss: 0.15638279914855957
Batch 16/64 loss: 0.14567720890045166
Batch 17/64 loss: 0.1637195348739624
Batch 18/64 loss: 0.15630990266799927
Batch 19/64 loss: 0.16376417875289917
Batch 20/64 loss: 0.16465258598327637
Batch 21/64 loss: 0.15521323680877686
Batch 22/64 loss: 0.15541481971740723
Batch 23/64 loss: 0.1384735107421875
Batch 24/64 loss: 0.15511345863342285
Batch 25/64 loss: 0.1437886357307434
Batch 26/64 loss: 0.16059720516204834
Batch 27/64 loss: 0.14697998762130737
Batch 28/64 loss: 0.15334999561309814
Batch 29/64 loss: 0.1466425657272339
Batch 30/64 loss: 0.14384818077087402
Batch 31/64 loss: 0.14419704675674438
Batch 32/64 loss: 0.14443820714950562
Batch 33/64 loss: 0.146651029586792
Batch 34/64 loss: 0.14001882076263428
Batch 35/64 loss: 0.1666736602783203
Batch 36/64 loss: 0.13724803924560547
Batch 37/64 loss: 0.16580599546432495
Batch 38/64 loss: 0.14375758171081543
Batch 39/64 loss: 0.14479714632034302
Batch 40/64 loss: 0.16713297367095947
Batch 41/64 loss: 0.14465445280075073
Batch 42/64 loss: 0.1476868987083435
Batch 43/64 loss: 0.15804141759872437
Batch 44/64 loss: 0.13825547695159912
Batch 45/64 loss: 0.1662989854812622
Batch 46/64 loss: 0.15309345722198486
Batch 47/64 loss: 0.1548830270767212
Batch 48/64 loss: 0.1435530185699463
Batch 49/64 loss: 0.14593303203582764
Batch 50/64 loss: 0.13382601737976074
Batch 51/64 loss: 0.14116287231445312
Batch 52/64 loss: 0.1469915509223938
Batch 53/64 loss: 0.1400638222694397
Batch 54/64 loss: 0.14739936590194702
Batch 55/64 loss: 0.1500064730644226
Batch 56/64 loss: 0.1529330015182495
Batch 57/64 loss: 0.1442214846611023
Batch 58/64 loss: 0.14482569694519043
Batch 59/64 loss: 0.1501595377922058
Batch 60/64 loss: 0.16586685180664062
Batch 61/64 loss: 0.15508878231048584
Batch 62/64 loss: 0.14797240495681763
Batch 63/64 loss: 0.13628584146499634
Batch 64/64 loss: 0.14721715450286865
Epoch 12  Train loss: 0.1500761354670805  Val loss: 0.15647466854541162
Saving best model, epoch: 12
Epoch 13
-------------------------------
Batch 1/64 loss: 0.13267040252685547
Batch 2/64 loss: 0.13820195198059082
Batch 3/64 loss: 0.1292896866798401
Batch 4/64 loss: 0.15082967281341553
Batch 5/64 loss: 0.14114755392074585
Batch 6/64 loss: 0.13815289735794067
Batch 7/64 loss: 0.13805800676345825
Batch 8/64 loss: 0.14657187461853027
Batch 9/64 loss: 0.14465099573135376
Batch 10/64 loss: 0.11894941329956055
Batch 11/64 loss: 0.1446068286895752
Batch 12/64 loss: 0.13422882556915283
Batch 13/64 loss: 0.13169294595718384
Batch 14/64 loss: 0.1514081358909607
Batch 15/64 loss: 0.12314873933792114
Batch 16/64 loss: 0.13742882013320923
Batch 17/64 loss: 0.16701728105545044
Batch 18/64 loss: 0.14478707313537598
Batch 19/64 loss: 0.13904190063476562
Batch 20/64 loss: 0.15450644493103027
Batch 21/64 loss: 0.14250123500823975
Batch 22/64 loss: 0.12191253900527954
Batch 23/64 loss: 0.13892459869384766
Batch 24/64 loss: 0.14417129755020142
Batch 25/64 loss: 0.15142512321472168
Batch 26/64 loss: 0.14270126819610596
Batch 27/64 loss: 0.1470416784286499
Batch 28/64 loss: 0.12807106971740723
Batch 29/64 loss: 0.12005484104156494
Batch 30/64 loss: 0.13568377494812012
Batch 31/64 loss: 0.15044677257537842
Batch 32/64 loss: 0.1462532877922058
Batch 33/64 loss: 0.15131348371505737
Batch 34/64 loss: 0.15328943729400635
Batch 35/64 loss: 0.14128708839416504
Batch 36/64 loss: 0.1353115439414978
Batch 37/64 loss: 0.14542818069458008
Batch 38/64 loss: 0.13366085290908813
Batch 39/64 loss: 0.14333665370941162
Batch 40/64 loss: 0.13595211505889893
Batch 41/64 loss: 0.15052878856658936
Batch 42/64 loss: 0.1522655487060547
Batch 43/64 loss: 0.1352607011795044
Batch 44/64 loss: 0.14993035793304443
Batch 45/64 loss: 0.142372727394104
Batch 46/64 loss: 0.1474994421005249
Batch 47/64 loss: 0.12447869777679443
Batch 48/64 loss: 0.11184722185134888
Batch 49/64 loss: 0.14060282707214355
Batch 50/64 loss: 0.1451401710510254
Batch 51/64 loss: 0.14707374572753906
Batch 52/64 loss: 0.12923628091812134
Batch 53/64 loss: 0.13974744081497192
Batch 54/64 loss: 0.14364200830459595
Batch 55/64 loss: 0.15533632040023804
Batch 56/64 loss: 0.13343292474746704
Batch 57/64 loss: 0.140017569065094
Batch 58/64 loss: 0.12154114246368408
Batch 59/64 loss: 0.1562865972518921
Batch 60/64 loss: 0.13664627075195312
Batch 61/64 loss: 0.12685513496398926
Batch 62/64 loss: 0.12108564376831055
Batch 63/64 loss: 0.14999234676361084
Batch 64/64 loss: 0.1534419059753418
Epoch 13  Train loss: 0.14009502167795218  Val loss: 0.15750849840976938
Epoch 14
-------------------------------
Batch 1/64 loss: 0.14520269632339478
Batch 2/64 loss: 0.11745166778564453
Batch 3/64 loss: 0.1419661045074463
Batch 4/64 loss: 0.11933553218841553
Batch 5/64 loss: 0.13202005624771118
Batch 6/64 loss: 0.14427834749221802
Batch 7/64 loss: 0.1335911750793457
Batch 8/64 loss: 0.12330293655395508
Batch 9/64 loss: 0.12119650840759277
Batch 10/64 loss: 0.15146714448928833
Batch 11/64 loss: 0.11824429035186768
Batch 12/64 loss: 0.13258832693099976
Batch 13/64 loss: 0.14573806524276733
Batch 14/64 loss: 0.1315588355064392
Batch 15/64 loss: 0.13229882717132568
Batch 16/64 loss: 0.12444430589675903
Batch 17/64 loss: 0.1307404637336731
Batch 18/64 loss: 0.12211519479751587
Batch 19/64 loss: 0.1310715675354004
Batch 20/64 loss: 0.12173521518707275
Batch 21/64 loss: 0.11988502740859985
Batch 22/64 loss: 0.1342383623123169
Batch 23/64 loss: 0.1280118227005005
Batch 24/64 loss: 0.14914977550506592
Batch 25/64 loss: 0.14236831665039062
Batch 26/64 loss: 0.14160484075546265
Batch 27/64 loss: 0.13466870784759521
Batch 28/64 loss: 0.1347675323486328
Batch 29/64 loss: 0.13326507806777954
Batch 30/64 loss: 0.12834316492080688
Batch 31/64 loss: 0.10965502262115479
Batch 32/64 loss: 0.1532788872718811
Batch 33/64 loss: 0.14218205213546753
Batch 34/64 loss: 0.13404059410095215
Batch 35/64 loss: 0.12326109409332275
Batch 36/64 loss: 0.13274186849594116
Batch 37/64 loss: 0.13025963306427002
Batch 38/64 loss: 0.14949041604995728
Batch 39/64 loss: 0.11518818140029907
Batch 40/64 loss: 0.12728166580200195
Batch 41/64 loss: 0.1297769546508789
Batch 42/64 loss: 0.1299615502357483
Batch 43/64 loss: 0.11718493700027466
Batch 44/64 loss: 0.11706840991973877
Batch 45/64 loss: 0.11333400011062622
Batch 46/64 loss: 0.12206274271011353
Batch 47/64 loss: 0.1210862398147583
Batch 48/64 loss: 0.10870093107223511
Batch 49/64 loss: 0.12122642993927002
Batch 50/64 loss: 0.11356008052825928
Batch 51/64 loss: 0.12379896640777588
Batch 52/64 loss: 0.1104692816734314
Batch 53/64 loss: 0.11930882930755615
Batch 54/64 loss: 0.117531418800354
Batch 55/64 loss: 0.11310184001922607
Batch 56/64 loss: 0.12009447813034058
Batch 57/64 loss: 0.13608819246292114
Batch 58/64 loss: 0.13434553146362305
Batch 59/64 loss: 0.12489664554595947
Batch 60/64 loss: 0.13781040906906128
Batch 61/64 loss: 0.12506115436553955
Batch 62/64 loss: 0.12699371576309204
Batch 63/64 loss: 0.1328829526901245
Batch 64/64 loss: 0.1103217601776123
Epoch 14  Train loss: 0.12836260889090745  Val loss: 0.1610644816123333
Epoch 15
-------------------------------
Batch 1/64 loss: 0.12380242347717285
Batch 2/64 loss: 0.12184387445449829
Batch 3/64 loss: 0.14148032665252686
Batch 4/64 loss: 0.13086771965026855
Batch 5/64 loss: 0.1484137773513794
Batch 6/64 loss: 0.12954455614089966
Batch 7/64 loss: 0.12651050090789795
Batch 8/64 loss: 0.12163698673248291
Batch 9/64 loss: 0.10946989059448242
Batch 10/64 loss: 0.11174851655960083
Batch 11/64 loss: 0.11968338489532471
Batch 12/64 loss: 0.1009557843208313
Batch 13/64 loss: 0.14614808559417725
Batch 14/64 loss: 0.1417827010154724
Batch 15/64 loss: 0.11296868324279785
Batch 16/64 loss: 0.12825149297714233
Batch 17/64 loss: 0.12751317024230957
Batch 18/64 loss: 0.13485437631607056
Batch 19/64 loss: 0.14438462257385254
Batch 20/64 loss: 0.10571855306625366
Batch 21/64 loss: 0.12691503763198853
Batch 22/64 loss: 0.1319999098777771
Batch 23/64 loss: 0.12246137857437134
Batch 24/64 loss: 0.12846434116363525
Batch 25/64 loss: 0.1278882622718811
Batch 26/64 loss: 0.10938763618469238
Batch 27/64 loss: 0.10872787237167358
Batch 28/64 loss: 0.13523650169372559
Batch 29/64 loss: 0.1264992356300354
Batch 30/64 loss: 0.11080813407897949
Batch 31/64 loss: 0.11530673503875732
Batch 32/64 loss: 0.11883300542831421
Batch 33/64 loss: 0.11153364181518555
Batch 34/64 loss: 0.10494357347488403
Batch 35/64 loss: 0.09290957450866699
Batch 36/64 loss: 0.11539530754089355
Batch 37/64 loss: 0.09919524192810059
Batch 38/64 loss: 0.10989189147949219
Batch 39/64 loss: 0.13016068935394287
Batch 40/64 loss: 0.11278736591339111
Batch 41/64 loss: 0.14396047592163086
Batch 42/64 loss: 0.14422649145126343
Batch 43/64 loss: 0.11649906635284424
Batch 44/64 loss: 0.1028296947479248
Batch 45/64 loss: 0.10379242897033691
Batch 46/64 loss: 0.1397559642791748
Batch 47/64 loss: 0.1074368953704834
Batch 48/64 loss: 0.11689293384552002
Batch 49/64 loss: 0.1166297197341919
Batch 50/64 loss: 0.11328279972076416
Batch 51/64 loss: 0.09627759456634521
Batch 52/64 loss: 0.10692179203033447
Batch 53/64 loss: 0.11873513460159302
Batch 54/64 loss: 0.11093181371688843
Batch 55/64 loss: 0.10484308004379272
Batch 56/64 loss: 0.11200559139251709
Batch 57/64 loss: 0.11057031154632568
Batch 58/64 loss: 0.12068831920623779
Batch 59/64 loss: 0.11791419982910156
Batch 60/64 loss: 0.11491894721984863
Batch 61/64 loss: 0.10327756404876709
Batch 62/64 loss: 0.11085176467895508
Batch 63/64 loss: 0.10143446922302246
Batch 64/64 loss: 0.10829615592956543
Epoch 15  Train loss: 0.11894625775954303  Val loss: 0.1318958391438645
Saving best model, epoch: 15
Epoch 16
-------------------------------
Batch 1/64 loss: 0.096088707447052
Batch 2/64 loss: 0.086009681224823
Batch 3/64 loss: 0.11118650436401367
Batch 4/64 loss: 0.10643821954727173
Batch 5/64 loss: 0.10616642236709595
Batch 6/64 loss: 0.11645543575286865
Batch 7/64 loss: 0.09047091007232666
Batch 8/64 loss: 0.11539298295974731
Batch 9/64 loss: 0.12827563285827637
Batch 10/64 loss: 0.09963130950927734
Batch 11/64 loss: 0.11651211977005005
Batch 12/64 loss: 0.12702977657318115
Batch 13/64 loss: 0.10484474897384644
Batch 14/64 loss: 0.11643052101135254
Batch 15/64 loss: 0.11021184921264648
Batch 16/64 loss: 0.11978304386138916
Batch 17/64 loss: 0.10772353410720825
Batch 18/64 loss: 0.11100828647613525
Batch 19/64 loss: 0.11361825466156006
Batch 20/64 loss: 0.08863669633865356
Batch 21/64 loss: 0.1165848970413208
Batch 22/64 loss: 0.11946839094161987
Batch 23/64 loss: 0.11951643228530884
Batch 24/64 loss: 0.0896722674369812
Batch 25/64 loss: 0.10071349143981934
Batch 26/64 loss: 0.11821693181991577
Batch 27/64 loss: 0.08880645036697388
Batch 28/64 loss: 0.0942152738571167
Batch 29/64 loss: 0.10059410333633423
Batch 30/64 loss: 0.12741219997406006
Batch 31/64 loss: 0.09819388389587402
Batch 32/64 loss: 0.094845712184906
Batch 33/64 loss: 0.0980907678604126
Batch 34/64 loss: 0.09160888195037842
Batch 35/64 loss: 0.14038634300231934
Batch 36/64 loss: 0.08487701416015625
Batch 37/64 loss: 0.10747349262237549
Batch 38/64 loss: 0.09682905673980713
Batch 39/64 loss: 0.10158377885818481
Batch 40/64 loss: 0.09080833196640015
Batch 41/64 loss: 0.1157347559928894
Batch 42/64 loss: 0.09224146604537964
Batch 43/64 loss: 0.08782345056533813
Batch 44/64 loss: 0.11539942026138306
Batch 45/64 loss: 0.09431344270706177
Batch 46/64 loss: 0.11416548490524292
Batch 47/64 loss: 0.11060631275177002
Batch 48/64 loss: 0.12114584445953369
Batch 49/64 loss: 0.12087643146514893
Batch 50/64 loss: 0.08791971206665039
Batch 51/64 loss: 0.10309076309204102
Batch 52/64 loss: 0.10799825191497803
Batch 53/64 loss: 0.11016470193862915
Batch 54/64 loss: 0.1310238242149353
Batch 55/64 loss: 0.10742384195327759
Batch 56/64 loss: 0.11330193281173706
Batch 57/64 loss: 0.1431906819343567
Batch 58/64 loss: 0.1101539134979248
Batch 59/64 loss: 0.11010479927062988
Batch 60/64 loss: 0.08725553750991821
Batch 61/64 loss: 0.10124784708023071
Batch 62/64 loss: 0.11463534832000732
Batch 63/64 loss: 0.10609132051467896
Batch 64/64 loss: 0.1197250485420227
Epoch 16  Train loss: 0.10741200330210667  Val loss: 0.13180966680402198
Saving best model, epoch: 16
Epoch 17
-------------------------------
Batch 1/64 loss: 0.12179374694824219
Batch 2/64 loss: 0.08090275526046753
Batch 3/64 loss: 0.10779678821563721
Batch 4/64 loss: 0.09048408269882202
Batch 5/64 loss: 0.0954599380493164
Batch 6/64 loss: 0.08780992031097412
Batch 7/64 loss: 0.13208794593811035
Batch 8/64 loss: 0.09037083387374878
Batch 9/64 loss: 0.11389446258544922
Batch 10/64 loss: 0.1165618896484375
Batch 11/64 loss: 0.1219639778137207
Batch 12/64 loss: 0.09459441900253296
Batch 13/64 loss: 0.0986449122428894
Batch 14/64 loss: 0.08873724937438965
Batch 15/64 loss: 0.10324835777282715
Batch 16/64 loss: 0.09380000829696655
Batch 17/64 loss: 0.09865415096282959
Batch 18/64 loss: 0.09102296829223633
Batch 19/64 loss: 0.11110180616378784
Batch 20/64 loss: 0.09679555892944336
Batch 21/64 loss: 0.08597719669342041
Batch 22/64 loss: 0.10706889629364014
Batch 23/64 loss: 0.12122470140457153
Batch 24/64 loss: 0.09053409099578857
Batch 25/64 loss: 0.0912749171257019
Batch 26/64 loss: 0.07873457670211792
Batch 27/64 loss: 0.09364104270935059
Batch 28/64 loss: 0.09681576490402222
Batch 29/64 loss: 0.08418035507202148
Batch 30/64 loss: 0.08447182178497314
Batch 31/64 loss: 0.0870409607887268
Batch 32/64 loss: 0.09445434808731079
Batch 33/64 loss: 0.08187901973724365
Batch 34/64 loss: 0.0927470326423645
Batch 35/64 loss: 0.08614093065261841
Batch 36/64 loss: 0.09464752674102783
Batch 37/64 loss: 0.1008826494216919
Batch 38/64 loss: 0.1107829213142395
Batch 39/64 loss: 0.09252649545669556
Batch 40/64 loss: 0.09978294372558594
Batch 41/64 loss: 0.11905544996261597
Batch 42/64 loss: 0.10089778900146484
Batch 43/64 loss: 0.10493254661560059
Batch 44/64 loss: 0.09051704406738281
Batch 45/64 loss: 0.09733742475509644
Batch 46/64 loss: 0.07970130443572998
Batch 47/64 loss: 0.08417177200317383
Batch 48/64 loss: 0.08178865909576416
Batch 49/64 loss: 0.08580672740936279
Batch 50/64 loss: 0.06303656101226807
Batch 51/64 loss: 0.0888635516166687
Batch 52/64 loss: 0.08358430862426758
Batch 53/64 loss: 0.09727340936660767
Batch 54/64 loss: 0.09385895729064941
Batch 55/64 loss: 0.10637694597244263
Batch 56/64 loss: 0.1077347993850708
Batch 57/64 loss: 0.09177196025848389
Batch 58/64 loss: 0.0811118483543396
Batch 59/64 loss: 0.08285236358642578
Batch 60/64 loss: 0.10720837116241455
Batch 61/64 loss: 0.08327323198318481
Batch 62/64 loss: 0.10551363229751587
Batch 63/64 loss: 0.09902447462081909
Batch 64/64 loss: 0.08105975389480591
Epoch 17  Train loss: 0.09579642216364542  Val loss: 0.12440561920506847
Saving best model, epoch: 17
Epoch 18
-------------------------------
Batch 1/64 loss: 0.06962811946868896
Batch 2/64 loss: 0.10363614559173584
Batch 3/64 loss: 0.0741768479347229
Batch 4/64 loss: 0.09763818979263306
Batch 5/64 loss: 0.08301639556884766
Batch 6/64 loss: 0.08708512783050537
Batch 7/64 loss: 0.08115476369857788
Batch 8/64 loss: 0.10377103090286255
Batch 9/64 loss: 0.09037107229232788
Batch 10/64 loss: 0.09809982776641846
Batch 11/64 loss: 0.096432626247406
Batch 12/64 loss: 0.0888364315032959
Batch 13/64 loss: 0.10291397571563721
Batch 14/64 loss: 0.10592228174209595
Batch 15/64 loss: 0.07380855083465576
Batch 16/64 loss: 0.09284675121307373
Batch 17/64 loss: 0.07901078462600708
Batch 18/64 loss: 0.08969074487686157
Batch 19/64 loss: 0.07562696933746338
Batch 20/64 loss: 0.09078359603881836
Batch 21/64 loss: 0.0993499755859375
Batch 22/64 loss: 0.07570254802703857
Batch 23/64 loss: 0.0688444972038269
Batch 24/64 loss: 0.07766598463058472
Batch 25/64 loss: 0.08518218994140625
Batch 26/64 loss: 0.09574359655380249
Batch 27/64 loss: 0.09381192922592163
Batch 28/64 loss: 0.07163619995117188
Batch 29/64 loss: 0.09013766050338745
Batch 30/64 loss: 0.07737976312637329
Batch 31/64 loss: 0.07441103458404541
Batch 32/64 loss: 0.08921325206756592
Batch 33/64 loss: 0.09143245220184326
Batch 34/64 loss: 0.09164905548095703
Batch 35/64 loss: 0.10249775648117065
Batch 36/64 loss: 0.0877423882484436
Batch 37/64 loss: 0.09710425138473511
Batch 38/64 loss: 0.08913630247116089
Batch 39/64 loss: 0.10743677616119385
Batch 40/64 loss: 0.0945124626159668
Batch 41/64 loss: 0.10054713487625122
Batch 42/64 loss: 0.07886946201324463
Batch 43/64 loss: 0.08837538957595825
Batch 44/64 loss: 0.08799505233764648
Batch 45/64 loss: 0.07456624507904053
Batch 46/64 loss: 0.08418995141983032
Batch 47/64 loss: 0.05197477340698242
Batch 48/64 loss: 0.08383005857467651
Batch 49/64 loss: 0.09921830892562866
Batch 50/64 loss: 0.10867500305175781
Batch 51/64 loss: 0.11149585247039795
Batch 52/64 loss: 0.09754747152328491
Batch 53/64 loss: 0.0879700779914856
Batch 54/64 loss: 0.09270352125167847
Batch 55/64 loss: 0.09202992916107178
Batch 56/64 loss: 0.08504283428192139
Batch 57/64 loss: 0.0858801007270813
Batch 58/64 loss: 0.09218049049377441
Batch 59/64 loss: 0.09046721458435059
Batch 60/64 loss: 0.08682423830032349
Batch 61/64 loss: 0.08172363042831421
Batch 62/64 loss: 0.06994807720184326
Batch 63/64 loss: 0.08721840381622314
Batch 64/64 loss: 0.08021605014801025
Epoch 18  Train loss: 0.08822706774169324  Val loss: 0.09987544706187297
Saving best model, epoch: 18
Epoch 19
-------------------------------
Batch 1/64 loss: 0.07265281677246094
Batch 2/64 loss: 0.09044003486633301
Batch 3/64 loss: 0.06744062900543213
Batch 4/64 loss: 0.08847188949584961
Batch 5/64 loss: 0.08668094873428345
Batch 6/64 loss: 0.058394789695739746
Batch 7/64 loss: 0.06136220693588257
Batch 8/64 loss: 0.08043599128723145
Batch 9/64 loss: 0.07467776536941528
Batch 10/64 loss: 0.07802748680114746
Batch 11/64 loss: 0.07414281368255615
Batch 12/64 loss: 0.09252965450286865
Batch 13/64 loss: 0.0853455662727356
Batch 14/64 loss: 0.07773697376251221
Batch 15/64 loss: 0.09519869089126587
Batch 16/64 loss: 0.08758246898651123
Batch 17/64 loss: 0.059377074241638184
Batch 18/64 loss: 0.07692676782608032
Batch 19/64 loss: 0.07209765911102295
Batch 20/64 loss: 0.06498569250106812
Batch 21/64 loss: 0.08680915832519531
Batch 22/64 loss: 0.09074646234512329
Batch 23/64 loss: 0.04678601026535034
Batch 24/64 loss: 0.07877576351165771
Batch 25/64 loss: 0.06224948167800903
Batch 26/64 loss: 0.07014226913452148
Batch 27/64 loss: 0.09726786613464355
Batch 28/64 loss: 0.0719153881072998
Batch 29/64 loss: 0.08305788040161133
Batch 30/64 loss: 0.06464433670043945
Batch 31/64 loss: 0.08987152576446533
Batch 32/64 loss: 0.09797263145446777
Batch 33/64 loss: 0.08278125524520874
Batch 34/64 loss: 0.05371493101119995
Batch 35/64 loss: 0.09128725528717041
Batch 36/64 loss: 0.07945841550827026
Batch 37/64 loss: 0.09155595302581787
Batch 38/64 loss: 0.07782787084579468
Batch 39/64 loss: 0.0834585428237915
Batch 40/64 loss: 0.10731381177902222
Batch 41/64 loss: 0.0757676362991333
Batch 42/64 loss: 0.0983385443687439
Batch 43/64 loss: 0.07662475109100342
Batch 44/64 loss: 0.04989027976989746
Batch 45/64 loss: 0.0632164478302002
Batch 46/64 loss: 0.06435823440551758
Batch 47/64 loss: 0.07475614547729492
Batch 48/64 loss: 0.07750743627548218
Batch 49/64 loss: 0.08234512805938721
Batch 50/64 loss: 0.07295799255371094
Batch 51/64 loss: 0.08924460411071777
Batch 52/64 loss: 0.10317122936248779
Batch 53/64 loss: 0.06764602661132812
Batch 54/64 loss: 0.08293354511260986
Batch 55/64 loss: 0.08379006385803223
Batch 56/64 loss: 0.0772322416305542
Batch 57/64 loss: 0.08596402406692505
Batch 58/64 loss: 0.06450092792510986
Batch 59/64 loss: 0.07084721326828003
Batch 60/64 loss: 0.07809287309646606
Batch 61/64 loss: 0.059967041015625
Batch 62/64 loss: 0.08410561084747314
Batch 63/64 loss: 0.0728445053100586
Batch 64/64 loss: 0.05917090177536011
Epoch 19  Train loss: 0.07768824170617496  Val loss: 0.09595905403091326
Saving best model, epoch: 19
Epoch 20
-------------------------------
Batch 1/64 loss: 0.06696552038192749
Batch 2/64 loss: 0.07169771194458008
Batch 3/64 loss: 0.06446897983551025
Batch 4/64 loss: 0.07071143388748169
Batch 5/64 loss: 0.07275354862213135
Batch 6/64 loss: 0.08227735757827759
Batch 7/64 loss: 0.08247226476669312
Batch 8/64 loss: 0.0772925615310669
Batch 9/64 loss: 0.07800555229187012
Batch 10/64 loss: 0.06753695011138916
Batch 11/64 loss: 0.06349945068359375
Batch 12/64 loss: 0.07143467664718628
Batch 13/64 loss: 0.05543941259384155
Batch 14/64 loss: 0.08007729053497314
Batch 15/64 loss: 0.08526343107223511
Batch 16/64 loss: 0.08235162496566772
Batch 17/64 loss: 0.07809680700302124
Batch 18/64 loss: 0.08086645603179932
Batch 19/64 loss: 0.05717110633850098
Batch 20/64 loss: 0.10236591100692749
Batch 21/64 loss: 0.0931781530380249
Batch 22/64 loss: 0.09419804811477661
Batch 23/64 loss: 0.0711287260055542
Batch 24/64 loss: 0.07838422060012817
Batch 25/64 loss: 0.07669073343276978
Batch 26/64 loss: 0.07818526029586792
Batch 27/64 loss: 0.05913287401199341
Batch 28/64 loss: 0.06921076774597168
Batch 29/64 loss: 0.08580321073532104
Batch 30/64 loss: 0.06528329849243164
Batch 31/64 loss: 0.08482658863067627
Batch 32/64 loss: 0.08513981103897095
Batch 33/64 loss: 0.09614300727844238
Batch 34/64 loss: 0.07758283615112305
Batch 35/64 loss: 0.06847292184829712
Batch 36/64 loss: 0.09522664546966553
Batch 37/64 loss: 0.0693473219871521
Batch 38/64 loss: 0.10239642858505249
Batch 39/64 loss: 0.08242654800415039
Batch 40/64 loss: 0.1002570390701294
Batch 41/64 loss: 0.0818590521812439
Batch 42/64 loss: 0.06849950551986694
Batch 43/64 loss: 0.09851396083831787
Batch 44/64 loss: 0.10098230838775635
Batch 45/64 loss: 0.05845630168914795
Batch 46/64 loss: 0.0689154863357544
Batch 47/64 loss: 0.06960421800613403
Batch 48/64 loss: 0.08197736740112305
Batch 49/64 loss: 0.06813251972198486
Batch 50/64 loss: 0.09494686126708984
Batch 51/64 loss: 0.06675088405609131
Batch 52/64 loss: 0.07578742504119873
Batch 53/64 loss: 0.06351989507675171
Batch 54/64 loss: 0.06074953079223633
Batch 55/64 loss: 0.08492207527160645
Batch 56/64 loss: 0.09466665983200073
Batch 57/64 loss: 0.04353344440460205
Batch 58/64 loss: 0.06549185514450073
Batch 59/64 loss: 0.06563901901245117
Batch 60/64 loss: 0.04745084047317505
Batch 61/64 loss: 0.07726609706878662
Batch 62/64 loss: 0.057531774044036865
Batch 63/64 loss: 0.0938233733177185
Batch 64/64 loss: 0.0863426923751831
Epoch 20  Train loss: 0.07651043078478645  Val loss: 0.12180228618412084
Epoch 21
-------------------------------
Batch 1/64 loss: 0.07629328966140747
Batch 2/64 loss: 0.06915503740310669
Batch 3/64 loss: 0.07170021533966064
Batch 4/64 loss: 0.08942580223083496
Batch 5/64 loss: 0.06731855869293213
Batch 6/64 loss: 0.07267403602600098
Batch 7/64 loss: 0.055274009704589844
Batch 8/64 loss: 0.0703955888748169
Batch 9/64 loss: 0.0667731761932373
Batch 10/64 loss: 0.09167587757110596
Batch 11/64 loss: 0.07881629467010498
Batch 12/64 loss: 0.061298370361328125
Batch 13/64 loss: 0.0594598650932312
Batch 14/64 loss: 0.08873254060745239
Batch 15/64 loss: 0.08040308952331543
Batch 16/64 loss: 0.07540488243103027
Batch 17/64 loss: 0.07783949375152588
Batch 18/64 loss: 0.06371563673019409
Batch 19/64 loss: 0.09248751401901245
Batch 20/64 loss: 0.07897156476974487
Batch 21/64 loss: 0.06882214546203613
Batch 22/64 loss: 0.06689691543579102
Batch 23/64 loss: 0.06659799814224243
Batch 24/64 loss: 0.088229238986969
Batch 25/64 loss: 0.058838486671447754
Batch 26/64 loss: 0.07896208763122559
Batch 27/64 loss: 0.09388220310211182
Batch 28/64 loss: 0.07997232675552368
Batch 29/64 loss: 0.07115185260772705
Batch 30/64 loss: 0.06569874286651611
Batch 31/64 loss: 0.0859689712524414
Batch 32/64 loss: 0.05984997749328613
Batch 33/64 loss: 0.06276726722717285
Batch 34/64 loss: 0.049918949604034424
Batch 35/64 loss: 0.07372832298278809
Batch 36/64 loss: 0.054546058177948
Batch 37/64 loss: 0.06189990043640137
Batch 38/64 loss: 0.07209509611129761
Batch 39/64 loss: 0.06076967716217041
Batch 40/64 loss: 0.06974101066589355
Batch 41/64 loss: 0.06047630310058594
Batch 42/64 loss: 0.06543207168579102
Batch 43/64 loss: 0.0638464093208313
Batch 44/64 loss: 0.07620495557785034
Batch 45/64 loss: 0.05498933792114258
Batch 46/64 loss: 0.04895120859146118
Batch 47/64 loss: 0.06420707702636719
Batch 48/64 loss: 0.07494735717773438
Batch 49/64 loss: 0.06343746185302734
Batch 50/64 loss: 0.07140177488327026
Batch 51/64 loss: 0.07260477542877197
Batch 52/64 loss: 0.0626993179321289
Batch 53/64 loss: 0.04711711406707764
Batch 54/64 loss: 0.06311237812042236
Batch 55/64 loss: 0.06452047824859619
Batch 56/64 loss: 0.0699581503868103
Batch 57/64 loss: 0.05657303333282471
Batch 58/64 loss: 0.06317198276519775
Batch 59/64 loss: 0.0576440691947937
Batch 60/64 loss: 0.03689157962799072
Batch 61/64 loss: 0.0807805061340332
Batch 62/64 loss: 0.06908047199249268
Batch 63/64 loss: 0.06630933284759521
Batch 64/64 loss: 0.038664162158966064
Epoch 21  Train loss: 0.06841580143161849  Val loss: 0.08423997733191527
Saving best model, epoch: 21
Epoch 22
-------------------------------
Batch 1/64 loss: 0.07580834627151489
Batch 2/64 loss: 0.05583345890045166
Batch 3/64 loss: 0.06776535511016846
Batch 4/64 loss: 0.06677228212356567
Batch 5/64 loss: 0.07524341344833374
Batch 6/64 loss: 0.054351091384887695
Batch 7/64 loss: 0.08195453882217407
Batch 8/64 loss: 0.052320122718811035
Batch 9/64 loss: 0.061090946197509766
Batch 10/64 loss: 0.06534683704376221
Batch 11/64 loss: 0.06524717807769775
Batch 12/64 loss: 0.06747859716415405
Batch 13/64 loss: 0.05230063199996948
Batch 14/64 loss: 0.08576208353042603
Batch 15/64 loss: 0.0738821029663086
Batch 16/64 loss: 0.04594296216964722
Batch 17/64 loss: 0.06398296356201172
Batch 18/64 loss: 0.06328016519546509
Batch 19/64 loss: 0.05815964937210083
Batch 20/64 loss: 0.06165921688079834
Batch 21/64 loss: 0.05667769908905029
Batch 22/64 loss: 0.06763815879821777
Batch 23/64 loss: 0.05066937208175659
Batch 24/64 loss: 0.0538751482963562
Batch 25/64 loss: 0.04992485046386719
Batch 26/64 loss: 0.05565237998962402
Batch 27/64 loss: 0.057393431663513184
Batch 28/64 loss: 0.07701516151428223
Batch 29/64 loss: 0.05506110191345215
Batch 30/64 loss: 0.050837576389312744
Batch 31/64 loss: 0.060318052768707275
Batch 32/64 loss: 0.04551666975021362
Batch 33/64 loss: 0.04985404014587402
Batch 34/64 loss: 0.05475127696990967
Batch 35/64 loss: 0.054472923278808594
Batch 36/64 loss: 0.046668052673339844
Batch 37/64 loss: 0.0717577338218689
Batch 38/64 loss: 0.04080688953399658
Batch 39/64 loss: 0.04352080821990967
Batch 40/64 loss: 0.0973891019821167
Batch 41/64 loss: 0.04653674364089966
Batch 42/64 loss: 0.06859040260314941
Batch 43/64 loss: 0.05014866590499878
Batch 44/64 loss: 0.055201947689056396
Batch 45/64 loss: 0.0736953616142273
Batch 46/64 loss: 0.059172868728637695
Batch 47/64 loss: 0.06098496913909912
Batch 48/64 loss: 0.0694509744644165
Batch 49/64 loss: 0.048189640045166016
Batch 50/64 loss: 0.04189485311508179
Batch 51/64 loss: 0.0576898455619812
Batch 52/64 loss: 0.06257355213165283
Batch 53/64 loss: 0.07843756675720215
Batch 54/64 loss: 0.052854299545288086
Batch 55/64 loss: 0.057430922985076904
Batch 56/64 loss: 0.05027490854263306
Batch 57/64 loss: 0.059605956077575684
Batch 58/64 loss: 0.04398113489151001
Batch 59/64 loss: 0.09027087688446045
Batch 60/64 loss: 0.08397239446640015
Batch 61/64 loss: 0.0698704719543457
Batch 62/64 loss: 0.07616943120956421
Batch 63/64 loss: 0.046471238136291504
Batch 64/64 loss: 0.054020702838897705
Epoch 22  Train loss: 0.06083085373336194  Val loss: 0.08461381807360042
Epoch 23
-------------------------------
Batch 1/64 loss: 0.08069431781768799
Batch 2/64 loss: 0.06838583946228027
Batch 3/64 loss: 0.06058019399642944
Batch 4/64 loss: 0.06892204284667969
Batch 5/64 loss: 0.05246317386627197
Batch 6/64 loss: 0.050363898277282715
Batch 7/64 loss: 0.08512592315673828
Batch 8/64 loss: 0.06000316143035889
Batch 9/64 loss: 0.0622904896736145
Batch 10/64 loss: 0.06627833843231201
Batch 11/64 loss: 0.05337291955947876
Batch 12/64 loss: 0.064220130443573
Batch 13/64 loss: 0.06053513288497925
Batch 14/64 loss: 0.04283493757247925
Batch 15/64 loss: 0.054363906383514404
Batch 16/64 loss: 0.06373870372772217
Batch 17/64 loss: 0.055980801582336426
Batch 18/64 loss: 0.052268266677856445
Batch 19/64 loss: 0.05507552623748779
Batch 20/64 loss: 0.06410962343215942
Batch 21/64 loss: 0.05046653747558594
Batch 22/64 loss: 0.04819130897521973
Batch 23/64 loss: 0.059324562549591064
Batch 24/64 loss: 0.05575186014175415
Batch 25/64 loss: 0.08174169063568115
Batch 26/64 loss: 0.06015026569366455
Batch 27/64 loss: 0.06514674425125122
Batch 28/64 loss: 0.04780393838882446
Batch 29/64 loss: 0.05091142654418945
Batch 30/64 loss: 0.05646979808807373
Batch 31/64 loss: 0.055588603019714355
Batch 32/64 loss: 0.0535506010055542
Batch 33/64 loss: 0.04700440168380737
Batch 34/64 loss: 0.043049156665802
Batch 35/64 loss: 0.08354932069778442
Batch 36/64 loss: 0.056428611278533936
Batch 37/64 loss: 0.05444788932800293
Batch 38/64 loss: 0.05045956373214722
Batch 39/64 loss: 0.06656789779663086
Batch 40/64 loss: 0.04654771089553833
Batch 41/64 loss: 0.056127071380615234
Batch 42/64 loss: 0.04486316442489624
Batch 43/64 loss: 0.06711053848266602
Batch 44/64 loss: 0.045584678649902344
Batch 45/64 loss: 0.05022549629211426
Batch 46/64 loss: 0.06660223007202148
Batch 47/64 loss: 0.09365624189376831
Batch 48/64 loss: 0.06948870420455933
Batch 49/64 loss: 0.049480140209198
Batch 50/64 loss: 0.05008876323699951
Batch 51/64 loss: 0.062351107597351074
Batch 52/64 loss: 0.05290716886520386
Batch 53/64 loss: 0.06630855798721313
Batch 54/64 loss: 0.06039273738861084
Batch 55/64 loss: 0.05495786666870117
Batch 56/64 loss: 0.06315219402313232
Batch 57/64 loss: 0.05788904428482056
Batch 58/64 loss: 0.040305912494659424
Batch 59/64 loss: 0.028306245803833008
Batch 60/64 loss: 0.04525095224380493
Batch 61/64 loss: 0.08198201656341553
Batch 62/64 loss: 0.04671519994735718
Batch 63/64 loss: 0.04973912239074707
Batch 64/64 loss: 0.05747705698013306
Epoch 23  Train loss: 0.0580604261043025  Val loss: 0.07996869967975158
Saving best model, epoch: 23
Epoch 24
-------------------------------
Batch 1/64 loss: 0.04685622453689575
Batch 2/64 loss: 0.06051898002624512
Batch 3/64 loss: 0.05131763219833374
Batch 4/64 loss: 0.04738074541091919
Batch 5/64 loss: 0.05593061447143555
Batch 6/64 loss: 0.05400460958480835
Batch 7/64 loss: 0.0657505989074707
Batch 8/64 loss: 0.038717031478881836
Batch 9/64 loss: 0.05632144212722778
Batch 10/64 loss: 0.044621825218200684
Batch 11/64 loss: 0.09169661998748779
Batch 12/64 loss: 0.06227296590805054
Batch 13/64 loss: 0.06023991107940674
Batch 14/64 loss: 0.03790849447250366
Batch 15/64 loss: 0.035874247550964355
Batch 16/64 loss: 0.06215035915374756
Batch 17/64 loss: 0.03819543123245239
Batch 18/64 loss: 0.033019304275512695
Batch 19/64 loss: 0.0623478889465332
Batch 20/64 loss: 0.06495112180709839
Batch 21/64 loss: 0.043526411056518555
Batch 22/64 loss: 0.0353851318359375
Batch 23/64 loss: 0.056443989276885986
Batch 24/64 loss: 0.06886172294616699
Batch 25/64 loss: 0.03823447227478027
Batch 26/64 loss: 0.08895623683929443
Batch 27/64 loss: 0.05737823247909546
Batch 28/64 loss: 0.054222047328948975
Batch 29/64 loss: 0.06014913320541382
Batch 30/64 loss: 0.059693872928619385
Batch 31/64 loss: 0.05446058511734009
Batch 32/64 loss: 0.04187905788421631
Batch 33/64 loss: 0.04616737365722656
Batch 34/64 loss: 0.06436878442764282
Batch 35/64 loss: 0.05250084400177002
Batch 36/64 loss: 0.04907900094985962
Batch 37/64 loss: 0.04444676637649536
Batch 38/64 loss: 0.05315899848937988
Batch 39/64 loss: 0.05324488878250122
Batch 40/64 loss: 0.056256771087646484
Batch 41/64 loss: 0.06416046619415283
Batch 42/64 loss: 0.04671180248260498
Batch 43/64 loss: 0.050164997577667236
Batch 44/64 loss: 0.08264005184173584
Batch 45/64 loss: 0.04212719202041626
Batch 46/64 loss: 0.07162153720855713
Batch 47/64 loss: 0.046385109424591064
Batch 48/64 loss: 0.0421370267868042
Batch 49/64 loss: 0.03665566444396973
Batch 50/64 loss: 0.039547085762023926
Batch 51/64 loss: 0.017973899841308594
Batch 52/64 loss: 0.03737109899520874
Batch 53/64 loss: 0.04343259334564209
Batch 54/64 loss: 0.04510843753814697
Batch 55/64 loss: 0.04426610469818115
Batch 56/64 loss: 0.027762770652770996
Batch 57/64 loss: 0.06942689418792725
Batch 58/64 loss: 0.04383140802383423
Batch 59/64 loss: 0.03915691375732422
Batch 60/64 loss: 0.051912009716033936
Batch 61/64 loss: 0.06030988693237305
Batch 62/64 loss: 0.054526329040527344
Batch 63/64 loss: 0.04306513071060181
Batch 64/64 loss: 0.0953095555305481
Epoch 24  Train loss: 0.052082618778827146  Val loss: 0.07698249468688703
Saving best model, epoch: 24
Epoch 25
-------------------------------
Batch 1/64 loss: 0.03880608081817627
Batch 2/64 loss: 0.03526771068572998
Batch 3/64 loss: 0.07875716686248779
Batch 4/64 loss: 0.04526257514953613
Batch 5/64 loss: 0.06405484676361084
Batch 6/64 loss: 0.06081420183181763
Batch 7/64 loss: 0.051561832427978516
Batch 8/64 loss: 0.038127601146698
Batch 9/64 loss: 0.04814785718917847
Batch 10/64 loss: 0.05561023950576782
Batch 11/64 loss: 0.04455143213272095
Batch 12/64 loss: 0.021542787551879883
Batch 13/64 loss: 0.06081181764602661
Batch 14/64 loss: 0.03628396987915039
Batch 15/64 loss: 0.04973572492599487
Batch 16/64 loss: 0.03873419761657715
Batch 17/64 loss: 0.048362016677856445
Batch 18/64 loss: 0.05727410316467285
Batch 19/64 loss: 0.043367624282836914
Batch 20/64 loss: 0.04642045497894287
Batch 21/64 loss: 0.046968162059783936
Batch 22/64 loss: 0.06584829092025757
Batch 23/64 loss: 0.040879249572753906
Batch 24/64 loss: 0.06269139051437378
Batch 25/64 loss: 0.05452388525009155
Batch 26/64 loss: 0.054068684577941895
Batch 27/64 loss: 0.04083043336868286
Batch 28/64 loss: 0.08186477422714233
Batch 29/64 loss: 0.04909545183181763
Batch 30/64 loss: 0.07923507690429688
Batch 31/64 loss: 0.03904050588607788
Batch 32/64 loss: 0.05849313735961914
Batch 33/64 loss: 0.032618701457977295
Batch 34/64 loss: 0.06020796298980713
Batch 35/64 loss: 0.08345139026641846
Batch 36/64 loss: 0.036611318588256836
Batch 37/64 loss: 0.045002281665802
Batch 38/64 loss: 0.04284697771072388
Batch 39/64 loss: 0.02155405282974243
Batch 40/64 loss: 0.04876810312271118
Batch 41/64 loss: 0.0315016508102417
Batch 42/64 loss: 0.05448979139328003
Batch 43/64 loss: 0.06647199392318726
Batch 44/64 loss: 0.0367317795753479
Batch 45/64 loss: 0.0348740816116333
Batch 46/64 loss: 0.03915536403656006
Batch 47/64 loss: 0.053546786308288574
Batch 48/64 loss: 0.055073082447052
Batch 49/64 loss: 0.03774762153625488
Batch 50/64 loss: 0.049677252769470215
Batch 51/64 loss: 0.054870426654815674
Batch 52/64 loss: 0.040824055671691895
Batch 53/64 loss: 0.03693920373916626
Batch 54/64 loss: 0.06064552068710327
Batch 55/64 loss: 0.04997020959854126
Batch 56/64 loss: 0.03202474117279053
Batch 57/64 loss: 0.044718384742736816
Batch 58/64 loss: 0.051663875579833984
Batch 59/64 loss: 0.07805490493774414
Batch 60/64 loss: 0.06227761507034302
Batch 61/64 loss: 0.06952774524688721
Batch 62/64 loss: 0.05350285768508911
Batch 63/64 loss: 0.04769337177276611
Batch 64/64 loss: 0.03667670488357544
Epoch 25  Train loss: 0.04984445361530079  Val loss: 0.0803291848844679
Epoch 26
-------------------------------
Batch 1/64 loss: 0.041138648986816406
Batch 2/64 loss: 0.027280151844024658
Batch 3/64 loss: 0.058132171630859375
Batch 4/64 loss: 0.038124263286590576
Batch 5/64 loss: 0.0692780613899231
Batch 6/64 loss: 0.06191551685333252
Batch 7/64 loss: 0.050550758838653564
Batch 8/64 loss: 0.06943494081497192
Batch 9/64 loss: 0.04983633756637573
Batch 10/64 loss: 0.05296516418457031
Batch 11/64 loss: 0.04327654838562012
Batch 12/64 loss: 0.05508166551589966
Batch 13/64 loss: 0.0650215744972229
Batch 14/64 loss: 0.04015415906906128
Batch 15/64 loss: 0.04334992170333862
Batch 16/64 loss: 0.04302185773849487
Batch 17/64 loss: 0.02982431650161743
Batch 18/64 loss: 0.05294519662857056
Batch 19/64 loss: 0.051188111305236816
Batch 20/64 loss: 0.06539362668991089
Batch 21/64 loss: 0.038903772830963135
Batch 22/64 loss: 0.04753613471984863
Batch 23/64 loss: 0.027836084365844727
Batch 24/64 loss: 0.036006033420562744
Batch 25/64 loss: 0.04790794849395752
Batch 26/64 loss: 0.05094057321548462
Batch 27/64 loss: 0.03692871332168579
Batch 28/64 loss: 0.05357921123504639
Batch 29/64 loss: 0.05407726764678955
Batch 30/64 loss: 0.018918871879577637
Batch 31/64 loss: 0.06873941421508789
Batch 32/64 loss: 0.043163001537323
Batch 33/64 loss: 0.057145118713378906
Batch 34/64 loss: 0.058728814125061035
Batch 35/64 loss: 0.034171998500823975
Batch 36/64 loss: 0.038006365299224854
Batch 37/64 loss: 0.04221552610397339
Batch 38/64 loss: 0.06081342697143555
Batch 39/64 loss: 0.06033885478973389
Batch 40/64 loss: 0.059159696102142334
Batch 41/64 loss: 0.041567087173461914
Batch 42/64 loss: 0.05355501174926758
Batch 43/64 loss: 0.0612567663192749
Batch 44/64 loss: 0.057112693786621094
Batch 45/64 loss: 0.029889345169067383
Batch 46/64 loss: 0.03918135166168213
Batch 47/64 loss: 0.03086256980895996
Batch 48/64 loss: 0.03635138273239136
Batch 49/64 loss: 0.032315969467163086
Batch 50/64 loss: 0.05889922380447388
Batch 51/64 loss: 0.03189361095428467
Batch 52/64 loss: 0.042225003242492676
Batch 53/64 loss: 0.032194435596466064
Batch 54/64 loss: 0.03167867660522461
Batch 55/64 loss: 0.04866015911102295
Batch 56/64 loss: 0.031667351722717285
Batch 57/64 loss: 0.06138968467712402
Batch 58/64 loss: 0.04583871364593506
Batch 59/64 loss: 0.048269689083099365
Batch 60/64 loss: 0.02381300926208496
Batch 61/64 loss: 0.03958505392074585
Batch 62/64 loss: 0.02973198890686035
Batch 63/64 loss: 0.014793634414672852
Batch 64/64 loss: 0.06523770093917847
Epoch 26  Train loss: 0.04572063544217278  Val loss: 0.08007084995610607
Epoch 27
-------------------------------
Batch 1/64 loss: 0.03920269012451172
Batch 2/64 loss: 0.05381232500076294
Batch 3/64 loss: 0.042152345180511475
Batch 4/64 loss: 0.02082991600036621
Batch 5/64 loss: 0.04072088003158569
Batch 6/64 loss: 0.05614912509918213
Batch 7/64 loss: 0.05370306968688965
Batch 8/64 loss: 0.021873116493225098
Batch 9/64 loss: 0.047093987464904785
Batch 10/64 loss: 0.03804314136505127
Batch 11/64 loss: 0.035824596881866455
Batch 12/64 loss: 0.07605099678039551
Batch 13/64 loss: 0.046642839908599854
Batch 14/64 loss: 0.05534946918487549
Batch 15/64 loss: 0.03925204277038574
Batch 16/64 loss: 0.04489731788635254
Batch 17/64 loss: 0.08155369758605957
Batch 18/64 loss: 0.03330349922180176
Batch 19/64 loss: 0.01937687397003174
Batch 20/64 loss: 0.03476446866989136
Batch 21/64 loss: 0.039136648178100586
Batch 22/64 loss: 0.0348857045173645
Batch 23/64 loss: 0.033477067947387695
Batch 24/64 loss: 0.027571916580200195
Batch 25/64 loss: 0.022962868213653564
Batch 26/64 loss: 0.047257184982299805
Batch 27/64 loss: 0.01983410120010376
Batch 28/64 loss: 0.05937600135803223
Batch 29/64 loss: 0.034893572330474854
Batch 30/64 loss: 0.06196439266204834
Batch 31/64 loss: 0.04591357707977295
Batch 32/64 loss: 0.03639960289001465
Batch 33/64 loss: 0.03200608491897583
Batch 34/64 loss: 0.08060610294342041
Batch 35/64 loss: 0.03161686658859253
Batch 36/64 loss: 0.013926029205322266
Batch 37/64 loss: 0.0433383584022522
Batch 38/64 loss: 0.03545790910720825
Batch 39/64 loss: 0.052705228328704834
Batch 40/64 loss: 0.05563896894454956
Batch 41/64 loss: 0.030157387256622314
Batch 42/64 loss: 0.05331486463546753
Batch 43/64 loss: 0.01740860939025879
Batch 44/64 loss: 0.047170937061309814
Batch 45/64 loss: 0.04083448648452759
Batch 46/64 loss: 0.024172306060791016
Batch 47/64 loss: 0.024885594844818115
Batch 48/64 loss: 0.03613084554672241
Batch 49/64 loss: 0.03367561101913452
Batch 50/64 loss: 0.04845088720321655
Batch 51/64 loss: 0.04535269737243652
Batch 52/64 loss: 0.028277277946472168
Batch 53/64 loss: 0.03279823064804077
Batch 54/64 loss: 0.0392909049987793
Batch 55/64 loss: 0.048491060733795166
Batch 56/64 loss: 0.03259420394897461
Batch 57/64 loss: 0.05295276641845703
Batch 58/64 loss: 0.062451064586639404
Batch 59/64 loss: 0.04621434211730957
Batch 60/64 loss: 0.031511008739471436
Batch 61/64 loss: 0.03784441947937012
Batch 62/64 loss: 0.03465116024017334
Batch 63/64 loss: 0.0386730432510376
Batch 64/64 loss: 0.043552398681640625
Epoch 27  Train loss: 0.040933844622443705  Val loss: 0.07021242031936384
Saving best model, epoch: 27
Epoch 28
-------------------------------
Batch 1/64 loss: 0.05012321472167969
Batch 2/64 loss: 0.04063856601715088
Batch 3/64 loss: 0.032459378242492676
Batch 4/64 loss: 0.030538201332092285
Batch 5/64 loss: 0.03811240196228027
Batch 6/64 loss: 0.03324687480926514
Batch 7/64 loss: 0.05786287784576416
Batch 8/64 loss: 0.020238637924194336
Batch 9/64 loss: 0.03486454486846924
Batch 10/64 loss: 0.02047795057296753
Batch 11/64 loss: 0.06526637077331543
Batch 12/64 loss: 0.04098385572433472
Batch 13/64 loss: 0.026460766792297363
Batch 14/64 loss: 0.03967028856277466
Batch 15/64 loss: 0.05474275350570679
Batch 16/64 loss: 0.03168129920959473
Batch 17/64 loss: 0.047620415687561035
Batch 18/64 loss: 0.03245466947555542
Batch 19/64 loss: 0.03905302286148071
Batch 20/64 loss: 0.0677727460861206
Batch 21/64 loss: 0.048085033893585205
Batch 22/64 loss: 0.036189258098602295
Batch 23/64 loss: 0.019247829914093018
Batch 24/64 loss: 0.03570932149887085
Batch 25/64 loss: 0.043447256088256836
Batch 26/64 loss: 0.02618265151977539
Batch 27/64 loss: 0.03329044580459595
Batch 28/64 loss: 0.05277109146118164
Batch 29/64 loss: 0.019138455390930176
Batch 30/64 loss: 0.04161882400512695
Batch 31/64 loss: 0.034998297691345215
Batch 32/64 loss: 0.024729609489440918
Batch 33/64 loss: 0.07307195663452148
Batch 34/64 loss: 0.0275038480758667
Batch 35/64 loss: 0.03782248497009277
Batch 36/64 loss: 0.0464024543762207
Batch 37/64 loss: 0.03119683265686035
Batch 38/64 loss: 0.024856269359588623
Batch 39/64 loss: 0.02107846736907959
Batch 40/64 loss: 0.040762484073638916
Batch 41/64 loss: 0.03294181823730469
Batch 42/64 loss: 0.041538357734680176
Batch 43/64 loss: 0.05106472969055176
Batch 44/64 loss: 0.04422515630722046
Batch 45/64 loss: 0.0363084077835083
Batch 46/64 loss: 0.037228941917419434
Batch 47/64 loss: 0.04223501682281494
Batch 48/64 loss: 0.015603065490722656
Batch 49/64 loss: 0.01619553565979004
Batch 50/64 loss: 0.036215007305145264
Batch 51/64 loss: 0.03164315223693848
Batch 52/64 loss: 0.043520331382751465
Batch 53/64 loss: 0.04363316297531128
Batch 54/64 loss: 0.01124727725982666
Batch 55/64 loss: 0.046671152114868164
Batch 56/64 loss: 0.04499238729476929
Batch 57/64 loss: 0.09048843383789062
Batch 58/64 loss: 0.06455695629119873
Batch 59/64 loss: 0.04480242729187012
Batch 60/64 loss: 0.04719775915145874
Batch 61/64 loss: 0.049472808837890625
Batch 62/64 loss: 0.05431699752807617
Batch 63/64 loss: 0.049628615379333496
Batch 64/64 loss: 0.039883971214294434
Epoch 28  Train loss: 0.03965509311825621  Val loss: 0.056478783232239926
Saving best model, epoch: 28
Epoch 29
-------------------------------
Batch 1/64 loss: 0.014236927032470703
Batch 2/64 loss: 0.027472257614135742
Batch 3/64 loss: 0.03382164239883423
Batch 4/64 loss: 0.04061812162399292
Batch 5/64 loss: 0.036750614643096924
Batch 6/64 loss: 0.05078840255737305
Batch 7/64 loss: 0.048798561096191406
Batch 8/64 loss: 0.03704625368118286
Batch 9/64 loss: 0.0468021035194397
Batch 10/64 loss: 0.03961437940597534
Batch 11/64 loss: 0.051700055599212646
Batch 12/64 loss: 0.042755722999572754
Batch 13/64 loss: 0.0190122127532959
Batch 14/64 loss: 0.029382050037384033
Batch 15/64 loss: 0.02862715721130371
Batch 16/64 loss: 0.028023958206176758
Batch 17/64 loss: 0.049553632736206055
Batch 18/64 loss: 0.05149281024932861
Batch 19/64 loss: 0.019824087619781494
Batch 20/64 loss: 0.020675063133239746
Batch 21/64 loss: 0.03182995319366455
Batch 22/64 loss: 0.05194133520126343
Batch 23/64 loss: 0.04079240560531616
Batch 24/64 loss: 0.0388759970664978
Batch 25/64 loss: 0.04338103532791138
Batch 26/64 loss: 0.02283644676208496
Batch 27/64 loss: 0.039794921875
Batch 28/64 loss: 0.03348296880722046
Batch 29/64 loss: 0.0393366813659668
Batch 30/64 loss: 0.0572657585144043
Batch 31/64 loss: 0.04778861999511719
Batch 32/64 loss: 0.038292765617370605
Batch 33/64 loss: 0.051040053367614746
Batch 34/64 loss: 0.04654407501220703
Batch 35/64 loss: 0.03785926103591919
Batch 36/64 loss: 0.05201613903045654
Batch 37/64 loss: 0.04448223114013672
Batch 38/64 loss: 0.04865002632141113
Batch 39/64 loss: 0.02950870990753174
Batch 40/64 loss: 0.04758095741271973
Batch 41/64 loss: 0.048186659812927246
Batch 42/64 loss: 0.05352318286895752
Batch 43/64 loss: 0.03167867660522461
Batch 44/64 loss: 0.04251593351364136
Batch 45/64 loss: 0.052649736404418945
Batch 46/64 loss: 0.03551417589187622
Batch 47/64 loss: 0.06968307495117188
Batch 48/64 loss: 0.02780461311340332
Batch 49/64 loss: 0.04098200798034668
Batch 50/64 loss: 0.026603341102600098
Batch 51/64 loss: 0.041783809661865234
Batch 52/64 loss: 0.048797011375427246
Batch 53/64 loss: 0.046757638454437256
Batch 54/64 loss: 0.022766709327697754
Batch 55/64 loss: 0.039489567279815674
Batch 56/64 loss: 0.03597545623779297
Batch 57/64 loss: 0.050344884395599365
Batch 58/64 loss: 0.04693204164505005
Batch 59/64 loss: 0.02623063325881958
Batch 60/64 loss: 0.03324216604232788
Batch 61/64 loss: 0.04490506649017334
Batch 62/64 loss: 0.04074704647064209
Batch 63/64 loss: 0.03348058462142944
Batch 64/64 loss: 0.04818183183670044
Epoch 29  Train loss: 0.039796498008802826  Val loss: 0.06299577115737286
Epoch 30
-------------------------------
Batch 1/64 loss: 0.039060771465301514
Batch 2/64 loss: 0.03817856311798096
Batch 3/64 loss: 0.04011934995651245
Batch 4/64 loss: 0.0422324538230896
Batch 5/64 loss: 0.0321926474571228
Batch 6/64 loss: 0.03128248453140259
Batch 7/64 loss: 0.04124230146408081
Batch 8/64 loss: 0.0383797287940979
Batch 9/64 loss: 0.05528378486633301
Batch 10/64 loss: 0.013508915901184082
Batch 11/64 loss: 0.025224387645721436
Batch 12/64 loss: 0.03859323263168335
Batch 13/64 loss: 0.03618812561035156
Batch 14/64 loss: 0.025799155235290527
Batch 15/64 loss: 0.041693925857543945
Batch 16/64 loss: 0.03293335437774658
Batch 17/64 loss: 0.03631323575973511
Batch 18/64 loss: 0.04486125707626343
Batch 19/64 loss: 0.04319584369659424
Batch 20/64 loss: 0.02543795108795166
Batch 21/64 loss: 0.020923197269439697
Batch 22/64 loss: 0.04633986949920654
Batch 23/64 loss: 0.049853622913360596
Batch 24/64 loss: 0.03275948762893677
Batch 25/64 loss: 0.04228389263153076
Batch 26/64 loss: 0.06549578905105591
Batch 27/64 loss: 0.031987547874450684
Batch 28/64 loss: 0.040325820446014404
Batch 29/64 loss: 0.03172945976257324
Batch 30/64 loss: 0.040403664112091064
Batch 31/64 loss: 0.02034914493560791
Batch 32/64 loss: 0.042145371437072754
Batch 33/64 loss: 0.01633012294769287
Batch 34/64 loss: 0.022854089736938477
Batch 35/64 loss: 0.03166860342025757
Batch 36/64 loss: 0.018128693103790283
Batch 37/64 loss: 0.03614163398742676
Batch 38/64 loss: 0.02233445644378662
Batch 39/64 loss: 0.04266810417175293
Batch 40/64 loss: 0.03192567825317383
Batch 41/64 loss: 0.020000696182250977
Batch 42/64 loss: 0.044043540954589844
Batch 43/64 loss: 0.041090548038482666
Batch 44/64 loss: 0.028608620166778564
Batch 45/64 loss: 0.021866142749786377
Batch 46/64 loss: 0.033531129360198975
Batch 47/64 loss: 0.04470103979110718
Batch 48/64 loss: 0.028751254081726074
Batch 49/64 loss: 0.04241424798965454
Batch 50/64 loss: 0.04321640729904175
Batch 51/64 loss: 0.030926764011383057
Batch 52/64 loss: 0.047204434871673584
Batch 53/64 loss: 0.03247225284576416
Batch 54/64 loss: 0.03399878740310669
Batch 55/64 loss: 0.047376811504364014
Batch 56/64 loss: 0.022489070892333984
Batch 57/64 loss: 0.027092158794403076
Batch 58/64 loss: 0.023134469985961914
Batch 59/64 loss: 0.031334757804870605
Batch 60/64 loss: 0.029918551445007324
Batch 61/64 loss: 0.0754731297492981
Batch 62/64 loss: 0.03980553150177002
Batch 63/64 loss: 0.050068557262420654
Batch 64/64 loss: 0.059676289558410645
Epoch 30  Train loss: 0.035963072028814574  Val loss: 0.08151673430839355
Epoch 31
-------------------------------
Batch 1/64 loss: 0.03878563642501831
Batch 2/64 loss: 0.02779334783554077
Batch 3/64 loss: 0.016266584396362305
Batch 4/64 loss: 0.02312755584716797
Batch 5/64 loss: 0.02400803565979004
Batch 6/64 loss: 0.014888226985931396
Batch 7/64 loss: 0.015230894088745117
Batch 8/64 loss: 0.04068511724472046
Batch 9/64 loss: 0.05677664279937744
Batch 10/64 loss: 0.045641958713531494
Batch 11/64 loss: 0.024824440479278564
Batch 12/64 loss: 0.04319995641708374
Batch 13/64 loss: 0.03727906942367554
Batch 14/64 loss: 0.04210180044174194
Batch 15/64 loss: 0.02895289659500122
Batch 16/64 loss: 0.02991342544555664
Batch 17/64 loss: 0.039604902267456055
Batch 18/64 loss: 0.038924455642700195
Batch 19/64 loss: 0.05557900667190552
Batch 20/64 loss: 0.05965685844421387
Batch 21/64 loss: 0.021687030792236328
Batch 22/64 loss: 0.05216866731643677
Batch 23/64 loss: 0.03121083974838257
Batch 24/64 loss: 0.01882469654083252
Batch 25/64 loss: 0.02331399917602539
Batch 26/64 loss: 0.029296398162841797
Batch 27/64 loss: 0.02488332986831665
Batch 28/64 loss: 0.025329947471618652
Batch 29/64 loss: 0.04592770338058472
Batch 30/64 loss: 0.044733285903930664
Batch 31/64 loss: 0.0043700337409973145
Batch 32/64 loss: 0.07302224636077881
Batch 33/64 loss: 0.02574867010116577
Batch 34/64 loss: 0.045275986194610596
Batch 35/64 loss: 0.0423886775970459
Batch 36/64 loss: 0.03199470043182373
Batch 37/64 loss: 0.030454397201538086
Batch 38/64 loss: 0.05384397506713867
Batch 39/64 loss: 0.03137081861495972
Batch 40/64 loss: 0.04261207580566406
Batch 41/64 loss: 0.03392529487609863
Batch 42/64 loss: 0.042371928691864014
Batch 43/64 loss: 0.02677971124649048
Batch 44/64 loss: 0.04199874401092529
Batch 45/64 loss: 0.009512543678283691
Batch 46/64 loss: 0.03323739767074585
Batch 47/64 loss: 0.023345112800598145
Batch 48/64 loss: 0.037049055099487305
Batch 49/64 loss: 0.05511707067489624
Batch 50/64 loss: 0.029612064361572266
Batch 51/64 loss: 0.034184396266937256
Batch 52/64 loss: 0.03243345022201538
Batch 53/64 loss: 0.022085726261138916
Batch 54/64 loss: 0.045823097229003906
Batch 55/64 loss: 0.03859579563140869
Batch 56/64 loss: 0.041978418827056885
Batch 57/64 loss: 0.03304755687713623
Batch 58/64 loss: 0.034221351146698
Batch 59/64 loss: 0.06785422563552856
Batch 60/64 loss: 0.06439751386642456
Batch 61/64 loss: 0.03361618518829346
Batch 62/64 loss: 0.0303037166595459
Batch 63/64 loss: 0.010591328144073486
Batch 64/64 loss: 0.018701612949371338
Epoch 31  Train loss: 0.035102842134587905  Val loss: 0.05517553033697646
Saving best model, epoch: 31
Epoch 32
-------------------------------
Batch 1/64 loss: 0.0333247184753418
Batch 2/64 loss: 0.012668430805206299
Batch 3/64 loss: 0.04570579528808594
Batch 4/64 loss: 0.022675037384033203
Batch 5/64 loss: 0.039215683937072754
Batch 6/64 loss: 0.03851348161697388
Batch 7/64 loss: 0.0259169340133667
Batch 8/64 loss: 0.036927878856658936
Batch 9/64 loss: 0.023356854915618896
Batch 10/64 loss: 0.015767395496368408
Batch 11/64 loss: 0.03302741050720215
Batch 12/64 loss: 0.012430191040039062
Batch 13/64 loss: 0.031444549560546875
Batch 14/64 loss: 0.03067547082901001
Batch 15/64 loss: 0.052654266357421875
Batch 16/64 loss: 0.023943066596984863
Batch 17/64 loss: 0.02953857183456421
Batch 18/64 loss: 0.04748868942260742
Batch 19/64 loss: 0.028984129428863525
Batch 20/64 loss: 0.030051112174987793
Batch 21/64 loss: 0.012277960777282715
Batch 22/64 loss: 0.034878313541412354
Batch 23/64 loss: 0.038234174251556396
Batch 24/64 loss: 0.03396904468536377
Batch 25/64 loss: 0.02739429473876953
Batch 26/64 loss: 0.025589823722839355
Batch 27/64 loss: 0.025230467319488525
Batch 28/64 loss: 0.045693278312683105
Batch 29/64 loss: 0.03823584318161011
Batch 30/64 loss: 0.04480785131454468
Batch 31/64 loss: 0.017449617385864258
Batch 32/64 loss: 0.025388240814208984
Batch 33/64 loss: 0.038172006607055664
Batch 34/64 loss: 0.049869537353515625
Batch 35/64 loss: 0.03525000810623169
Batch 36/64 loss: 0.03139770030975342
Batch 37/64 loss: 0.025760352611541748
Batch 38/64 loss: 0.044678330421447754
Batch 39/64 loss: 0.03548520803451538
Batch 40/64 loss: 0.024908125400543213
Batch 41/64 loss: 0.02201920747756958
Batch 42/64 loss: 0.02147573232650757
Batch 43/64 loss: 0.00592494010925293
Batch 44/64 loss: 0.027174949645996094
Batch 45/64 loss: 0.026671528816223145
Batch 46/64 loss: 0.017314612865447998
Batch 47/64 loss: 0.031053245067596436
Batch 48/64 loss: 0.051271677017211914
Batch 49/64 loss: 0.07434618473052979
Batch 50/64 loss: 0.027281999588012695
Batch 51/64 loss: 0.04849445819854736
Batch 52/64 loss: 0.02306598424911499
Batch 53/64 loss: 0.03265756368637085
Batch 54/64 loss: 0.05423849821090698
Batch 55/64 loss: 0.028458774089813232
Batch 56/64 loss: 0.037202656269073486
Batch 57/64 loss: 0.005096614360809326
Batch 58/64 loss: 0.01671898365020752
Batch 59/64 loss: 0.00971895456314087
Batch 60/64 loss: 0.025295734405517578
Batch 61/64 loss: 0.008094966411590576
Batch 62/64 loss: 0.03435182571411133
Batch 63/64 loss: 0.010090768337249756
Batch 64/64 loss: 0.0148240327835083
Epoch 32  Train loss: 0.030088121283288097  Val loss: 0.04802974044662161
Saving best model, epoch: 32
Epoch 33
-------------------------------
Batch 1/64 loss: 0.06825888156890869
Batch 2/64 loss: -0.0005540847778320312
Batch 3/64 loss: 0.023902177810668945
Batch 4/64 loss: 0.029575884342193604
Batch 5/64 loss: 0.04135847091674805
Batch 6/64 loss: 0.04273116588592529
Batch 7/64 loss: 0.03589397668838501
Batch 8/64 loss: 0.02817589044570923
Batch 9/64 loss: 0.024006187915802002
Batch 10/64 loss: 0.003961682319641113
Batch 11/64 loss: 0.05221801996231079
Batch 12/64 loss: 0.022536814212799072
Batch 13/64 loss: 0.024272799491882324
Batch 14/64 loss: 0.035904526710510254
Batch 15/64 loss: 0.028073430061340332
Batch 16/64 loss: 0.02958577871322632
Batch 17/64 loss: 0.041787683963775635
Batch 18/64 loss: 0.04635322093963623
Batch 19/64 loss: 0.016227781772613525
Batch 20/64 loss: 0.03812909126281738
Batch 21/64 loss: 0.05199623107910156
Batch 22/64 loss: 0.029459595680236816
Batch 23/64 loss: 0.04915165901184082
Batch 24/64 loss: 0.00910031795501709
Batch 25/64 loss: 0.032436370849609375
Batch 26/64 loss: 0.05496114492416382
Batch 27/64 loss: 0.04105478525161743
Batch 28/64 loss: 0.03766202926635742
Batch 29/64 loss: 0.01339811086654663
Batch 30/64 loss: 0.019682228565216064
Batch 31/64 loss: 0.017492711544036865
Batch 32/64 loss: 0.0031282901763916016
Batch 33/64 loss: 0.01395481824874878
Batch 34/64 loss: 0.021526634693145752
Batch 35/64 loss: 0.033739566802978516
Batch 36/64 loss: 0.01886451244354248
Batch 37/64 loss: 0.016946077346801758
Batch 38/64 loss: 0.013622939586639404
Batch 39/64 loss: 0.016602277755737305
Batch 40/64 loss: 0.015294075012207031
Batch 41/64 loss: 0.033235371112823486
Batch 42/64 loss: 0.01274573802947998
Batch 43/64 loss: 0.034535884857177734
Batch 44/64 loss: 0.029044151306152344
Batch 45/64 loss: 0.03917205333709717
Batch 46/64 loss: 0.008788585662841797
Batch 47/64 loss: 0.030745744705200195
Batch 48/64 loss: 0.03366363048553467
Batch 49/64 loss: 0.028355181217193604
Batch 50/64 loss: 0.026993095874786377
Batch 51/64 loss: 0.02497631311416626
Batch 52/64 loss: 0.01925283670425415
Batch 53/64 loss: 0.028361082077026367
Batch 54/64 loss: 0.03925371170043945
Batch 55/64 loss: 0.0029631853103637695
Batch 56/64 loss: 0.029485225677490234
Batch 57/64 loss: 0.028528571128845215
Batch 58/64 loss: 0.03754448890686035
Batch 59/64 loss: 0.02201789617538452
Batch 60/64 loss: 0.02472400665283203
Batch 61/64 loss: 0.024934351444244385
Batch 62/64 loss: 0.03919851779937744
Batch 63/64 loss: 0.02936720848083496
Batch 64/64 loss: 0.010631740093231201
Epoch 33  Train loss: 0.02789544147603652  Val loss: 0.05374165335062034
Epoch 34
-------------------------------
Batch 1/64 loss: 0.050747692584991455
Batch 2/64 loss: 0.04103744029998779
Batch 3/64 loss: 0.005845248699188232
Batch 4/64 loss: 0.01661890745162964
Batch 5/64 loss: 0.0328407883644104
Batch 6/64 loss: 0.03377485275268555
Batch 7/64 loss: 0.046758294105529785
Batch 8/64 loss: 0.019475102424621582
Batch 9/64 loss: 0.0540812611579895
Batch 10/64 loss: 0.025939881801605225
Batch 11/64 loss: 0.050837695598602295
Batch 12/64 loss: 0.017091095447540283
Batch 13/64 loss: 0.028553366661071777
Batch 14/64 loss: 0.009068071842193604
Batch 15/64 loss: 0.03160315752029419
Batch 16/64 loss: 0.04649341106414795
Batch 17/64 loss: 0.02402317523956299
Batch 18/64 loss: 0.026943206787109375
Batch 19/64 loss: 0.020338475704193115
Batch 20/64 loss: 0.0449446439743042
Batch 21/64 loss: 0.017534255981445312
Batch 22/64 loss: 0.01844179630279541
Batch 23/64 loss: 0.03460603952407837
Batch 24/64 loss: 0.03302121162414551
Batch 25/64 loss: 0.017477869987487793
Batch 26/64 loss: 0.02343761920928955
Batch 27/64 loss: 0.05367898941040039
Batch 28/64 loss: 0.04793757200241089
Batch 29/64 loss: 0.010705769062042236
Batch 30/64 loss: 0.04945164918899536
Batch 31/64 loss: 0.01782745122909546
Batch 32/64 loss: 0.02411520481109619
Batch 33/64 loss: 0.03546637296676636
Batch 34/64 loss: 0.048610031604766846
Batch 35/64 loss: 0.02631819248199463
Batch 36/64 loss: 0.022058069705963135
Batch 37/64 loss: 0.049344658851623535
Batch 38/64 loss: 0.016606450080871582
Batch 39/64 loss: 0.0342327356338501
Batch 40/64 loss: 0.028240561485290527
Batch 41/64 loss: 0.03846925497055054
Batch 42/64 loss: 0.042321741580963135
Batch 43/64 loss: 0.013422012329101562
Batch 44/64 loss: 0.03909260034561157
Batch 45/64 loss: 0.028916478157043457
Batch 46/64 loss: 0.0416945219039917
Batch 47/64 loss: 0.0240592360496521
Batch 48/64 loss: 0.010446131229400635
Batch 49/64 loss: 0.013976216316223145
Batch 50/64 loss: 0.04496896266937256
Batch 51/64 loss: 0.025832653045654297
Batch 52/64 loss: 0.019845545291900635
Batch 53/64 loss: 0.015892326831817627
Batch 54/64 loss: 0.013826727867126465
Batch 55/64 loss: 0.031354665756225586
Batch 56/64 loss: 0.013759791851043701
Batch 57/64 loss: 0.007879436016082764
Batch 58/64 loss: 0.0166318416595459
Batch 59/64 loss: 0.005513548851013184
Batch 60/64 loss: 0.02403336763381958
Batch 61/64 loss: 0.020637929439544678
Batch 62/64 loss: 0.009299397468566895
Batch 63/64 loss: 0.03941977024078369
Batch 64/64 loss: 0.03224635124206543
Epoch 34  Train loss: 0.028260504965688667  Val loss: 0.043012254016915545
Saving best model, epoch: 34
Epoch 35
-------------------------------
Batch 1/64 loss: 0.028488337993621826
Batch 2/64 loss: 0.018722057342529297
Batch 3/64 loss: 0.018094182014465332
Batch 4/64 loss: 0.015371501445770264
Batch 5/64 loss: 0.03716683387756348
Batch 6/64 loss: 0.02488386631011963
Batch 7/64 loss: 0.0008034706115722656
Batch 8/64 loss: 0.029805481433868408
Batch 9/64 loss: 0.022410929203033447
Batch 10/64 loss: 0.010545074939727783
Batch 11/64 loss: 0.03148144483566284
Batch 12/64 loss: 0.04109758138656616
Batch 13/64 loss: 0.009439408779144287
Batch 14/64 loss: 0.021465420722961426
Batch 15/64 loss: 0.03134512901306152
Batch 16/64 loss: 0.037873923778533936
Batch 17/64 loss: 0.06257563829421997
Batch 18/64 loss: 0.014249622821807861
Batch 19/64 loss: 0.023456275463104248
Batch 20/64 loss: 0.022065460681915283
Batch 21/64 loss: 0.007744014263153076
Batch 22/64 loss: 0.021189212799072266
Batch 23/64 loss: 0.0431898832321167
Batch 24/64 loss: 0.021656334400177002
Batch 25/64 loss: 0.02617126703262329
Batch 26/64 loss: 0.03312814235687256
Batch 27/64 loss: 0.024942636489868164
Batch 28/64 loss: 0.0400809645652771
Batch 29/64 loss: 0.014383077621459961
Batch 30/64 loss: 0.01548147201538086
Batch 31/64 loss: -0.0036597251892089844
Batch 32/64 loss: 0.024804651737213135
Batch 33/64 loss: 0.02112191915512085
Batch 34/64 loss: 0.0440526008605957
Batch 35/64 loss: 0.001680135726928711
Batch 36/64 loss: 0.013633489608764648
Batch 37/64 loss: 0.04039037227630615
Batch 38/64 loss: 0.03034001588821411
Batch 39/64 loss: 0.027715563774108887
Batch 40/64 loss: 0.023198604583740234
Batch 41/64 loss: 0.026229143142700195
Batch 42/64 loss: 0.032059669494628906
Batch 43/64 loss: 0.024937093257904053
Batch 44/64 loss: 0.03150683641433716
Batch 45/64 loss: 0.06364113092422485
Batch 46/64 loss: 0.007131516933441162
Batch 47/64 loss: 0.019609570503234863
Batch 48/64 loss: 0.016616523265838623
Batch 49/64 loss: 0.019214987754821777
Batch 50/64 loss: 0.03557676076889038
Batch 51/64 loss: 0.024576663970947266
Batch 52/64 loss: 0.025854945182800293
Batch 53/64 loss: 0.018903017044067383
Batch 54/64 loss: 0.027293086051940918
Batch 55/64 loss: -0.0027382373809814453
Batch 56/64 loss: 0.008165299892425537
Batch 57/64 loss: 0.010350584983825684
Batch 58/64 loss: 0.017133057117462158
Batch 59/64 loss: 0.008168518543243408
Batch 60/64 loss: 0.04011034965515137
Batch 61/64 loss: 0.04523730278015137
Batch 62/64 loss: 0.02775019407272339
Batch 63/64 loss: 0.015257835388183594
Batch 64/64 loss: 0.01329106092453003
Epoch 35  Train loss: 0.02392377128788069  Val loss: 0.04724114027219949
Epoch 36
-------------------------------
Batch 1/64 loss: 0.019087553024291992
Batch 2/64 loss: 0.03281891345977783
Batch 3/64 loss: 0.011741876602172852
Batch 4/64 loss: 0.005754590034484863
Batch 5/64 loss: 0.013527870178222656
Batch 6/64 loss: 0.01607060432434082
Batch 7/64 loss: 0.013128876686096191
Batch 8/64 loss: -0.00034683942794799805
Batch 9/64 loss: 0.033152878284454346
Batch 10/64 loss: 0.015445172786712646
Batch 11/64 loss: 0.02781367301940918
Batch 12/64 loss: 0.01747530698776245
Batch 13/64 loss: 0.00935375690460205
Batch 14/64 loss: 0.02781665325164795
Batch 15/64 loss: -0.0030664801597595215
Batch 16/64 loss: 0.0240858793258667
Batch 17/64 loss: 0.012795805931091309
Batch 18/64 loss: 0.025038599967956543
Batch 19/64 loss: 0.013517439365386963
Batch 20/64 loss: 0.007299661636352539
Batch 21/64 loss: 0.031152069568634033
Batch 22/64 loss: 0.03662216663360596
Batch 23/64 loss: 0.012597799301147461
Batch 24/64 loss: 0.04026216268539429
Batch 25/64 loss: 0.01921379566192627
Batch 26/64 loss: 0.020124495029449463
Batch 27/64 loss: 0.030215084552764893
Batch 28/64 loss: 0.009823143482208252
Batch 29/64 loss: 0.026694774627685547
Batch 30/64 loss: -0.00581812858581543
Batch 31/64 loss: 0.02932906150817871
Batch 32/64 loss: 0.030579984188079834
Batch 33/64 loss: 0.04229080677032471
Batch 34/64 loss: 0.009637713432312012
Batch 35/64 loss: 0.01400834321975708
Batch 36/64 loss: 0.021391749382019043
Batch 37/64 loss: 0.019013166427612305
Batch 38/64 loss: 0.032720208168029785
Batch 39/64 loss: 0.0037040114402770996
Batch 40/64 loss: 0.027300119400024414
Batch 41/64 loss: 0.05204862356185913
Batch 42/64 loss: 0.011657655239105225
Batch 43/64 loss: 0.0009242892265319824
Batch 44/64 loss: 0.006180167198181152
Batch 45/64 loss: 0.049753665924072266
Batch 46/64 loss: 0.026297569274902344
Batch 47/64 loss: 0.029510021209716797
Batch 48/64 loss: 0.00858163833618164
Batch 49/64 loss: 0.018609046936035156
Batch 50/64 loss: 0.021339893341064453
Batch 51/64 loss: 0.015089035034179688
Batch 52/64 loss: 0.031932711601257324
Batch 53/64 loss: -0.0012015104293823242
Batch 54/64 loss: 0.024686753749847412
Batch 55/64 loss: 0.027554214000701904
Batch 56/64 loss: 0.02332174777984619
Batch 57/64 loss: 0.01828181743621826
Batch 58/64 loss: 0.01835876703262329
Batch 59/64 loss: 0.03888183832168579
Batch 60/64 loss: 0.026329517364501953
Batch 61/64 loss: 0.0109100341796875
Batch 62/64 loss: 0.008063733577728271
Batch 63/64 loss: 0.021822035312652588
Batch 64/64 loss: -0.002726733684539795
Epoch 36  Train loss: 0.01976882406309539  Val loss: 0.0556037040920192
Epoch 37
-------------------------------
Batch 1/64 loss: 0.023368477821350098
Batch 2/64 loss: 0.010363161563873291
Batch 3/64 loss: 0.019844353199005127
Batch 4/64 loss: 0.023683369159698486
Batch 5/64 loss: 0.03524059057235718
Batch 6/64 loss: 0.0035625696182250977
Batch 7/64 loss: 0.021533608436584473
Batch 8/64 loss: 0.01614820957183838
Batch 9/64 loss: 0.028266310691833496
Batch 10/64 loss: 0.00828564167022705
Batch 11/64 loss: 0.01803690195083618
Batch 12/64 loss: 0.03386658430099487
Batch 13/64 loss: 0.016300439834594727
Batch 14/64 loss: 0.015953242778778076
Batch 15/64 loss: 0.02510124444961548
Batch 16/64 loss: 0.010241150856018066
Batch 17/64 loss: 0.005334138870239258
Batch 18/64 loss: 0.05012553930282593
Batch 19/64 loss: 0.029847443103790283
Batch 20/64 loss: -0.0008321404457092285
Batch 21/64 loss: -0.002239048480987549
Batch 22/64 loss: 0.05018395185470581
Batch 23/64 loss: 0.05089837312698364
Batch 24/64 loss: 0.008762538433074951
Batch 25/64 loss: 0.010354220867156982
Batch 26/64 loss: 0.0325702428817749
Batch 27/64 loss: 0.027890503406524658
Batch 28/64 loss: 0.017885684967041016
Batch 29/64 loss: 0.056006550788879395
Batch 30/64 loss: 0.010085999965667725
Batch 31/64 loss: 0.041263699531555176
Batch 32/64 loss: 0.01819133758544922
Batch 33/64 loss: 0.01995551586151123
Batch 34/64 loss: 0.018760085105895996
Batch 35/64 loss: 0.008393406867980957
Batch 36/64 loss: 0.006159543991088867
Batch 37/64 loss: 0.03231161832809448
Batch 38/64 loss: 0.04628753662109375
Batch 39/64 loss: 0.01938152313232422
Batch 40/64 loss: 0.02607870101928711
Batch 41/64 loss: 0.022912979125976562
Batch 42/64 loss: 0.03619474172592163
Batch 43/64 loss: -0.0006854534149169922
Batch 44/64 loss: 0.02184450626373291
Batch 45/64 loss: 0.02217012643814087
Batch 46/64 loss: 0.031018972396850586
Batch 47/64 loss: 0.011322736740112305
Batch 48/64 loss: 0.025258898735046387
Batch 49/64 loss: 0.003742218017578125
Batch 50/64 loss: 0.025055229663848877
Batch 51/64 loss: 0.01675856113433838
Batch 52/64 loss: 0.005223989486694336
Batch 53/64 loss: 0.01498878002166748
Batch 54/64 loss: 0.020146429538726807
Batch 55/64 loss: 0.005545377731323242
Batch 56/64 loss: 0.01155400276184082
Batch 57/64 loss: 0.021874725818634033
Batch 58/64 loss: 0.031602680683135986
Batch 59/64 loss: 0.009548544883728027
Batch 60/64 loss: 0.021322965621948242
Batch 61/64 loss: 0.010326504707336426
Batch 62/64 loss: 0.029034972190856934
Batch 63/64 loss: 0.013480067253112793
Batch 64/64 loss: -0.0025188326835632324
Epoch 37  Train loss: 0.020420490292941823  Val loss: 0.046326541818703985
Epoch 38
-------------------------------
Batch 1/64 loss: 0.04398036003112793
Batch 2/64 loss: 0.04015302658081055
Batch 3/64 loss: 0.02328115701675415
Batch 4/64 loss: 0.025741219520568848
Batch 5/64 loss: 0.009276211261749268
Batch 6/64 loss: 0.016224443912506104
Batch 7/64 loss: 0.019839048385620117
Batch 8/64 loss: 0.04477846622467041
Batch 9/64 loss: 0.03710472583770752
Batch 10/64 loss: 0.021709024906158447
Batch 11/64 loss: 0.02726125717163086
Batch 12/64 loss: 0.03006458282470703
Batch 13/64 loss: -7.647275924682617e-05
Batch 14/64 loss: 0.014135181903839111
Batch 15/64 loss: 0.015255510807037354
Batch 16/64 loss: 0.011820614337921143
Batch 17/64 loss: 0.004724264144897461
Batch 18/64 loss: 0.02611863613128662
Batch 19/64 loss: 0.003761589527130127
Batch 20/64 loss: 0.007401704788208008
Batch 21/64 loss: 0.025587618350982666
Batch 22/64 loss: 0.04256922006607056
Batch 23/64 loss: 0.01510077714920044
Batch 24/64 loss: -0.01980304718017578
Batch 25/64 loss: 0.0036951303482055664
Batch 26/64 loss: 0.03468286991119385
Batch 27/64 loss: 0.013352572917938232
Batch 28/64 loss: 0.005495727062225342
Batch 29/64 loss: 0.01865255832672119
Batch 30/64 loss: 0.025167405605316162
Batch 31/64 loss: 0.037331223487854004
Batch 32/64 loss: 0.025847136974334717
Batch 33/64 loss: 0.01800847053527832
Batch 34/64 loss: 0.027944564819335938
Batch 35/64 loss: 0.003563523292541504
Batch 36/64 loss: 0.02569258213043213
Batch 37/64 loss: 0.003265082836151123
Batch 38/64 loss: 0.020088016986846924
Batch 39/64 loss: 0.01467806100845337
Batch 40/64 loss: 0.02452927827835083
Batch 41/64 loss: 0.014631450176239014
Batch 42/64 loss: 0.014382243156433105
Batch 43/64 loss: 0.02624332904815674
Batch 44/64 loss: 0.012194931507110596
Batch 45/64 loss: 0.017903625965118408
Batch 46/64 loss: -0.0002276897430419922
Batch 47/64 loss: 0.0024200081825256348
Batch 48/64 loss: 0.029485702514648438
Batch 49/64 loss: 0.010175526142120361
Batch 50/64 loss: 0.014735102653503418
Batch 51/64 loss: 0.004902005195617676
Batch 52/64 loss: 0.036685824394226074
Batch 53/64 loss: 0.007281899452209473
Batch 54/64 loss: 0.0027826428413391113
Batch 55/64 loss: 0.013584375381469727
Batch 56/64 loss: 0.012805342674255371
Batch 57/64 loss: 0.019706010818481445
Batch 58/64 loss: 0.00882941484451294
Batch 59/64 loss: 0.01888573169708252
Batch 60/64 loss: 0.022882819175720215
Batch 61/64 loss: 0.05017101764678955
Batch 62/64 loss: 0.021268725395202637
Batch 63/64 loss: 0.021409988403320312
Batch 64/64 loss: 0.02703273296356201
Epoch 38  Train loss: 0.018688908277773388  Val loss: 0.044493854455521835
Epoch 39
-------------------------------
Batch 1/64 loss: 0.019965767860412598
Batch 2/64 loss: 0.00823146104812622
Batch 3/64 loss: 0.03251147270202637
Batch 4/64 loss: 0.01641368865966797
Batch 5/64 loss: 0.004332125186920166
Batch 6/64 loss: 0.006227016448974609
Batch 7/64 loss: 0.008142232894897461
Batch 8/64 loss: 0.035285890102386475
Batch 9/64 loss: 0.04254615306854248
Batch 10/64 loss: 0.012663424015045166
Batch 11/64 loss: 0.03861796855926514
Batch 12/64 loss: 0.02027195692062378
Batch 13/64 loss: 0.024780094623565674
Batch 14/64 loss: 0.018382728099822998
Batch 15/64 loss: 0.02170485258102417
Batch 16/64 loss: 0.028266847133636475
Batch 17/64 loss: 0.01749807596206665
Batch 18/64 loss: 0.012503623962402344
Batch 19/64 loss: 0.014125168323516846
Batch 20/64 loss: 0.03482401371002197
Batch 21/64 loss: 0.013097763061523438
Batch 22/64 loss: 0.018920958042144775
Batch 23/64 loss: 0.01958751678466797
Batch 24/64 loss: 0.018326997756958008
Batch 25/64 loss: 0.013136148452758789
Batch 26/64 loss: 0.02691471576690674
Batch 27/64 loss: -0.005901753902435303
Batch 28/64 loss: 0.017736315727233887
Batch 29/64 loss: 0.019664883613586426
Batch 30/64 loss: 0.009459376335144043
Batch 31/64 loss: 0.02623230218887329
Batch 32/64 loss: 0.0070070624351501465
Batch 33/64 loss: 0.0228193998336792
Batch 34/64 loss: 0.04041647911071777
Batch 35/64 loss: -4.649162292480469e-06
Batch 36/64 loss: 0.03671950101852417
Batch 37/64 loss: 0.0012004971504211426
Batch 38/64 loss: 0.03186321258544922
Batch 39/64 loss: 0.011292695999145508
Batch 40/64 loss: 0.04075515270233154
Batch 41/64 loss: 0.02890467643737793
Batch 42/64 loss: 0.01006317138671875
Batch 43/64 loss: 0.01708662509918213
Batch 44/64 loss: 0.0002841353416442871
Batch 45/64 loss: 0.007165670394897461
Batch 46/64 loss: 0.014523446559906006
Batch 47/64 loss: -0.001657724380493164
Batch 48/64 loss: 0.005106985569000244
Batch 49/64 loss: 0.034532904624938965
Batch 50/64 loss: 0.014560699462890625
Batch 51/64 loss: 0.027819275856018066
Batch 52/64 loss: 0.014860808849334717
Batch 53/64 loss: 0.025539100170135498
Batch 54/64 loss: 0.004865527153015137
Batch 55/64 loss: 0.003113090991973877
Batch 56/64 loss: 0.025520920753479004
Batch 57/64 loss: 0.04233109951019287
Batch 58/64 loss: 0.005931973457336426
Batch 59/64 loss: 0.024072587490081787
Batch 60/64 loss: 0.023720502853393555
Batch 61/64 loss: 0.005731105804443359
Batch 62/64 loss: 0.0349273681640625
Batch 63/64 loss: 0.006439566612243652
Batch 64/64 loss: 0.03951746225357056
Epoch 39  Train loss: 0.018692090230829576  Val loss: 0.04075939774103591
Saving best model, epoch: 39
Epoch 40
-------------------------------
Batch 1/64 loss: 0.02915322780609131
Batch 2/64 loss: 0.03458327054977417
Batch 3/64 loss: 0.02724677324295044
Batch 4/64 loss: 0.0028896331787109375
Batch 5/64 loss: 0.024690210819244385
Batch 6/64 loss: 0.0367470383644104
Batch 7/64 loss: -0.00942373275756836
Batch 8/64 loss: 0.002940535545349121
Batch 9/64 loss: -0.0012232661247253418
Batch 10/64 loss: 0.008706808090209961
Batch 11/64 loss: 0.047805845737457275
Batch 12/64 loss: 0.044861674308776855
Batch 13/64 loss: 0.001177370548248291
Batch 14/64 loss: 0.03414654731750488
Batch 15/64 loss: 0.029464006423950195
Batch 16/64 loss: 0.0018928050994873047
Batch 17/64 loss: 0.0034064054489135742
Batch 18/64 loss: 0.0008889436721801758
Batch 19/64 loss: 0.012043476104736328
Batch 20/64 loss: 0.029310107231140137
Batch 21/64 loss: 0.014351606369018555
Batch 22/64 loss: 0.0307997465133667
Batch 23/64 loss: 0.02523481845855713
Batch 24/64 loss: -0.009404897689819336
Batch 25/64 loss: 0.01756000518798828
Batch 26/64 loss: 0.011967241764068604
Batch 27/64 loss: 0.0046939849853515625
Batch 28/64 loss: -0.004697322845458984
Batch 29/64 loss: 0.017845571041107178
Batch 30/64 loss: 0.022809743881225586
Batch 31/64 loss: 0.01434856653213501
Batch 32/64 loss: 0.02135312557220459
Batch 33/64 loss: 0.01293724775314331
Batch 34/64 loss: 0.008359432220458984
Batch 35/64 loss: -0.003744959831237793
Batch 36/64 loss: 0.017356455326080322
Batch 37/64 loss: 0.0030930638313293457
Batch 38/64 loss: 0.0009822249412536621
Batch 39/64 loss: 0.008270025253295898
Batch 40/64 loss: 0.05033451318740845
Batch 41/64 loss: 0.02943652868270874
Batch 42/64 loss: 0.010355174541473389
Batch 43/64 loss: 0.011909842491149902
Batch 44/64 loss: 0.026699483394622803
Batch 45/64 loss: 0.01022559404373169
Batch 46/64 loss: -0.0035875439643859863
Batch 47/64 loss: -0.005048274993896484
Batch 48/64 loss: 0.019080400466918945
Batch 49/64 loss: 0.0027385950088500977
Batch 50/64 loss: 0.025526046752929688
Batch 51/64 loss: 0.006496846675872803
Batch 52/64 loss: 0.007242739200592041
Batch 53/64 loss: 0.05341446399688721
Batch 54/64 loss: 0.025474250316619873
Batch 55/64 loss: 0.003524482250213623
Batch 56/64 loss: 0.03737282752990723
Batch 57/64 loss: 0.015236973762512207
Batch 58/64 loss: 0.021132469177246094
Batch 59/64 loss: 0.0006022453308105469
Batch 60/64 loss: 0.008476555347442627
Batch 61/64 loss: 0.02709507942199707
Batch 62/64 loss: 0.007853031158447266
Batch 63/64 loss: 0.01517266035079956
Batch 64/64 loss: 0.006295561790466309
Epoch 40  Train loss: 0.015480941417170506  Val loss: 0.05027480530984623
Epoch 41
-------------------------------
Batch 1/64 loss: 0.010254263877868652
Batch 2/64 loss: 0.0077939629554748535
Batch 3/64 loss: 0.0316084623336792
Batch 4/64 loss: 0.004568278789520264
Batch 5/64 loss: 0.02224212884902954
Batch 6/64 loss: 0.01452934741973877
Batch 7/64 loss: 0.040360450744628906
Batch 8/64 loss: 0.005749821662902832
Batch 9/64 loss: 0.023707091808319092
Batch 10/64 loss: 0.016459107398986816
Batch 11/64 loss: 0.011586189270019531
Batch 12/64 loss: 0.010199964046478271
Batch 13/64 loss: -0.0024510622024536133
Batch 14/64 loss: 0.021056711673736572
Batch 15/64 loss: 0.0075241923332214355
Batch 16/64 loss: 0.016497671604156494
Batch 17/64 loss: 0.002203822135925293
Batch 18/64 loss: 0.016579270362854004
Batch 19/64 loss: 0.004893779754638672
Batch 20/64 loss: 0.029589533805847168
Batch 21/64 loss: -0.02264326810836792
Batch 22/64 loss: 0.019593358039855957
Batch 23/64 loss: 0.026676595211029053
Batch 24/64 loss: 0.015572071075439453
Batch 25/64 loss: 0.005775868892669678
Batch 26/64 loss: 0.010966122150421143
Batch 27/64 loss: 0.036485910415649414
Batch 28/64 loss: -0.01082068681716919
Batch 29/64 loss: 0.03699606657028198
Batch 30/64 loss: 0.008802711963653564
Batch 31/64 loss: 0.008032381534576416
Batch 32/64 loss: -0.009729087352752686
Batch 33/64 loss: -0.00996541976928711
Batch 34/64 loss: 0.02431035041809082
Batch 35/64 loss: 0.019203484058380127
Batch 36/64 loss: 0.014410912990570068
Batch 37/64 loss: 0.024114489555358887
Batch 38/64 loss: 0.007550954818725586
Batch 39/64 loss: 0.016503751277923584
Batch 40/64 loss: 0.006740212440490723
Batch 41/64 loss: 0.013986289501190186
Batch 42/64 loss: 0.014121770858764648
Batch 43/64 loss: -0.00667726993560791
Batch 44/64 loss: 0.021180331707000732
Batch 45/64 loss: 0.003287971019744873
Batch 46/64 loss: 0.008211493492126465
Batch 47/64 loss: 0.027916014194488525
Batch 48/64 loss: 0.01684647798538208
Batch 49/64 loss: 0.015151500701904297
Batch 50/64 loss: 0.009032130241394043
Batch 51/64 loss: 0.009173095226287842
Batch 52/64 loss: 0.006372928619384766
Batch 53/64 loss: 0.02452617883682251
Batch 54/64 loss: 0.014354586601257324
Batch 55/64 loss: 0.006102323532104492
Batch 56/64 loss: 0.012233138084411621
Batch 57/64 loss: 0.017319560050964355
Batch 58/64 loss: 0.01036304235458374
Batch 59/64 loss: 0.0048909783363342285
Batch 60/64 loss: 0.019626140594482422
Batch 61/64 loss: 0.025987863540649414
Batch 62/64 loss: 0.03520697355270386
Batch 63/64 loss: 0.01814854145050049
Batch 64/64 loss: -0.009060382843017578
Epoch 41  Train loss: 0.013240730061250575  Val loss: 0.0411432702926426
Epoch 42
-------------------------------
Batch 1/64 loss: 0.009998798370361328
Batch 2/64 loss: -0.003563404083251953
Batch 3/64 loss: 0.00759202241897583
Batch 4/64 loss: 0.053191184997558594
Batch 5/64 loss: 0.0011121034622192383
Batch 6/64 loss: 0.0010455846786499023
Batch 7/64 loss: 0.007221102714538574
Batch 8/64 loss: 0.003651261329650879
Batch 9/64 loss: -0.005965173244476318
Batch 10/64 loss: 0.014874458312988281
Batch 11/64 loss: 0.017868220806121826
Batch 12/64 loss: 0.0024233460426330566
Batch 13/64 loss: 0.0076084136962890625
Batch 14/64 loss: 0.0008247494697570801
Batch 15/64 loss: 0.009172976016998291
Batch 16/64 loss: 0.023449838161468506
Batch 17/64 loss: 0.018659770488739014
Batch 18/64 loss: 0.038031578063964844
Batch 19/64 loss: 0.048715949058532715
Batch 20/64 loss: 0.001397550106048584
Batch 21/64 loss: 0.024051010608673096
Batch 22/64 loss: 0.0061397552490234375
Batch 23/64 loss: 0.013362109661102295
Batch 24/64 loss: 0.02501058578491211
Batch 25/64 loss: 0.014342188835144043
Batch 26/64 loss: 0.017623841762542725
Batch 27/64 loss: 0.005004942417144775
Batch 28/64 loss: 0.001619577407836914
Batch 29/64 loss: 0.010518372058868408
Batch 30/64 loss: -0.0036944150924682617
Batch 31/64 loss: 0.008965134620666504
Batch 32/64 loss: 0.03415966033935547
Batch 33/64 loss: 0.012280166149139404
Batch 34/64 loss: 0.009857118129730225
Batch 35/64 loss: 0.013027310371398926
Batch 36/64 loss: 0.02151036262512207
Batch 37/64 loss: -0.009290814399719238
Batch 38/64 loss: 0.0081024169921875
Batch 39/64 loss: 0.001462697982788086
Batch 40/64 loss: 0.009792506694793701
Batch 41/64 loss: 0.014457941055297852
Batch 42/64 loss: 0.022256672382354736
Batch 43/64 loss: 0.021900475025177002
Batch 44/64 loss: -0.0024310946464538574
Batch 45/64 loss: 0.011369884014129639
Batch 46/64 loss: 0.0004203319549560547
Batch 47/64 loss: -0.005397021770477295
Batch 48/64 loss: 0.03076040744781494
Batch 49/64 loss: 0.00738602876663208
Batch 50/64 loss: -0.0012323856353759766
Batch 51/64 loss: -0.005087733268737793
Batch 52/64 loss: 0.022094786167144775
Batch 53/64 loss: -0.007251143455505371
Batch 54/64 loss: 0.03308314085006714
Batch 55/64 loss: 0.008987605571746826
Batch 56/64 loss: 0.01533585786819458
Batch 57/64 loss: 0.006636381149291992
Batch 58/64 loss: 0.002311229705810547
Batch 59/64 loss: 0.04124712944030762
Batch 60/64 loss: 0.007738053798675537
Batch 61/64 loss: 0.017101526260375977
Batch 62/64 loss: 0.009201526641845703
Batch 63/64 loss: 0.016404271125793457
Batch 64/64 loss: 0.02579706907272339
Epoch 42  Train loss: 0.012043867157954795  Val loss: 0.037481674213999325
Saving best model, epoch: 42
Epoch 43
-------------------------------
Batch 1/64 loss: 0.003979146480560303
Batch 2/64 loss: 0.014950752258300781
Batch 3/64 loss: -0.008037090301513672
Batch 4/64 loss: 0.017440855503082275
Batch 5/64 loss: 0.05107307434082031
Batch 6/64 loss: 0.009210467338562012
Batch 7/64 loss: 0.005026102066040039
Batch 8/64 loss: 0.014569580554962158
Batch 9/64 loss: -0.002261519432067871
Batch 10/64 loss: 0.008769214153289795
Batch 11/64 loss: 0.01925182342529297
Batch 12/64 loss: -0.002250492572784424
Batch 13/64 loss: 0.004394173622131348
Batch 14/64 loss: 0.026431262493133545
Batch 15/64 loss: 0.03402739763259888
Batch 16/64 loss: 0.017315685749053955
Batch 17/64 loss: 0.02251654863357544
Batch 18/64 loss: 0.016949176788330078
Batch 19/64 loss: 0.001529991626739502
Batch 20/64 loss: 0.028856515884399414
Batch 21/64 loss: 0.020089566707611084
Batch 22/64 loss: 0.011291980743408203
Batch 23/64 loss: -0.005719542503356934
Batch 24/64 loss: 0.011463940143585205
Batch 25/64 loss: 0.06363475322723389
Batch 26/64 loss: -0.0028507113456726074
Batch 27/64 loss: 0.04206341505050659
Batch 28/64 loss: 0.007379353046417236
Batch 29/64 loss: -0.007513165473937988
Batch 30/64 loss: 0.0219956636428833
Batch 31/64 loss: 0.02968776226043701
Batch 32/64 loss: 0.023766279220581055
Batch 33/64 loss: 0.00878363847732544
Batch 34/64 loss: 0.01772075891494751
Batch 35/64 loss: -0.0009112954139709473
Batch 36/64 loss: -0.00457453727722168
Batch 37/64 loss: 0.010388672351837158
Batch 38/64 loss: 0.006029605865478516
Batch 39/64 loss: 0.032087504863739014
Batch 40/64 loss: 0.012308716773986816
Batch 41/64 loss: 0.018500447273254395
Batch 42/64 loss: 0.01847594976425171
Batch 43/64 loss: 0.0015314817428588867
Batch 44/64 loss: 0.009798824787139893
Batch 45/64 loss: 0.006601870059967041
Batch 46/64 loss: 0.013774275779724121
Batch 47/64 loss: 0.026292026042938232
Batch 48/64 loss: 0.03399455547332764
Batch 49/64 loss: 0.01630789041519165
Batch 50/64 loss: 0.018846750259399414
Batch 51/64 loss: 0.01576751470565796
Batch 52/64 loss: 0.006411314010620117
Batch 53/64 loss: 0.023931682109832764
Batch 54/64 loss: 0.011866986751556396
Batch 55/64 loss: 0.02109605073928833
Batch 56/64 loss: 0.008250892162322998
Batch 57/64 loss: -0.0010476112365722656
Batch 58/64 loss: 0.02536475658416748
Batch 59/64 loss: -0.006492018699645996
Batch 60/64 loss: 0.005864977836608887
Batch 61/64 loss: 0.0014857053756713867
Batch 62/64 loss: 0.012717008590698242
Batch 63/64 loss: 0.012118816375732422
Batch 64/64 loss: -0.003969907760620117
Epoch 43  Train loss: 0.013793690064374139  Val loss: 0.03349831772014448
Saving best model, epoch: 43
Epoch 44
-------------------------------
Batch 1/64 loss: 0.0032134056091308594
Batch 2/64 loss: 0.03438413143157959
Batch 3/64 loss: -0.0020551681518554688
Batch 4/64 loss: -0.010972976684570312
Batch 5/64 loss: 0.007132112979888916
Batch 6/64 loss: 0.00040286779403686523
Batch 7/64 loss: 0.015901386737823486
Batch 8/64 loss: 0.018303632736206055
Batch 9/64 loss: 0.024588346481323242
Batch 10/64 loss: 0.026267290115356445
Batch 11/64 loss: 0.019283175468444824
Batch 12/64 loss: 0.01816195249557495
Batch 13/64 loss: 0.0012013912200927734
Batch 14/64 loss: 0.01611793041229248
Batch 15/64 loss: 0.008685529232025146
Batch 16/64 loss: -0.008678793907165527
Batch 17/64 loss: 0.021492063999176025
Batch 18/64 loss: 0.0007706880569458008
Batch 19/64 loss: 0.020416259765625
Batch 20/64 loss: 0.014902949333190918
Batch 21/64 loss: 0.014456987380981445
Batch 22/64 loss: 0.03268003463745117
Batch 23/64 loss: 0.03229355812072754
Batch 24/64 loss: 0.010503530502319336
Batch 25/64 loss: 0.02128124237060547
Batch 26/64 loss: 0.0004423856735229492
Batch 27/64 loss: -0.0024546384811401367
Batch 28/64 loss: 0.02010101079940796
Batch 29/64 loss: 0.0023800134658813477
Batch 30/64 loss: -0.0027905702590942383
Batch 31/64 loss: 0.030297160148620605
Batch 32/64 loss: 0.013401329517364502
Batch 33/64 loss: 0.005779623985290527
Batch 34/64 loss: 0.004313230514526367
Batch 35/64 loss: 0.0008977651596069336
Batch 36/64 loss: -0.0005900263786315918
Batch 37/64 loss: 0.011301994323730469
Batch 38/64 loss: 0.03697383403778076
Batch 39/64 loss: -0.009971082210540771
Batch 40/64 loss: 0.029810547828674316
Batch 41/64 loss: 0.00828784704208374
Batch 42/64 loss: 0.022606194019317627
Batch 43/64 loss: 0.007731437683105469
Batch 44/64 loss: 0.013517975807189941
Batch 45/64 loss: -0.00973653793334961
Batch 46/64 loss: -0.004711747169494629
Batch 47/64 loss: 0.017976880073547363
Batch 48/64 loss: 0.018362104892730713
Batch 49/64 loss: -0.017206311225891113
Batch 50/64 loss: 0.017236709594726562
Batch 51/64 loss: -0.0023865699768066406
Batch 52/64 loss: 0.010684609413146973
Batch 53/64 loss: -0.014568626880645752
Batch 54/64 loss: 0.00814056396484375
Batch 55/64 loss: 0.04274827241897583
Batch 56/64 loss: -5.137920379638672e-05
Batch 57/64 loss: 0.0037874579429626465
Batch 58/64 loss: 0.016054987907409668
Batch 59/64 loss: 0.015284895896911621
Batch 60/64 loss: -0.009521842002868652
Batch 61/64 loss: 0.01883876323699951
Batch 62/64 loss: 0.004570364952087402
Batch 63/64 loss: -0.0038263797760009766
Batch 64/64 loss: 0.007763385772705078
Epoch 44  Train loss: 0.010200287314022288  Val loss: 0.03334612637451015
Saving best model, epoch: 44
Epoch 45
-------------------------------
Batch 1/64 loss: -0.006624698638916016
Batch 2/64 loss: -0.011636734008789062
Batch 3/64 loss: 0.006605982780456543
Batch 4/64 loss: 0.0128861665725708
Batch 5/64 loss: -0.0018862485885620117
Batch 6/64 loss: 0.03839612007141113
Batch 7/64 loss: -0.0003370046615600586
Batch 8/64 loss: -0.008198261260986328
Batch 9/64 loss: -2.0444393157958984e-05
Batch 10/64 loss: 0.02470463514328003
Batch 11/64 loss: 0.006627082824707031
Batch 12/64 loss: -0.0039536356925964355
Batch 13/64 loss: -0.006267070770263672
Batch 14/64 loss: 0.005485475063323975
Batch 15/64 loss: -0.0035322904586791992
Batch 16/64 loss: -9.965896606445312e-05
Batch 17/64 loss: 0.04909175634384155
Batch 18/64 loss: -0.010623693466186523
Batch 19/64 loss: 0.01262521743774414
Batch 20/64 loss: 0.0011897087097167969
Batch 21/64 loss: 0.004694998264312744
Batch 22/64 loss: 0.014787793159484863
Batch 23/64 loss: 0.003444075584411621
Batch 24/64 loss: -0.005500614643096924
Batch 25/64 loss: 0.004000484943389893
Batch 26/64 loss: -0.009271979331970215
Batch 27/64 loss: 0.008521556854248047
Batch 28/64 loss: 0.002878904342651367
Batch 29/64 loss: -0.003638744354248047
Batch 30/64 loss: 0.0030396580696105957
Batch 31/64 loss: 0.007368206977844238
Batch 32/64 loss: -0.006188809871673584
Batch 33/64 loss: -0.004603087902069092
Batch 34/64 loss: 0.01586461067199707
Batch 35/64 loss: 0.014005780220031738
Batch 36/64 loss: 0.0027676820755004883
Batch 37/64 loss: -0.006431460380554199
Batch 38/64 loss: 0.0032339096069335938
Batch 39/64 loss: -0.002018272876739502
Batch 40/64 loss: 0.030757904052734375
Batch 41/64 loss: 0.024775445461273193
Batch 42/64 loss: 0.011976838111877441
Batch 43/64 loss: 0.012754201889038086
Batch 44/64 loss: 0.01093059778213501
Batch 45/64 loss: 0.007400989532470703
Batch 46/64 loss: -0.0062760114669799805
Batch 47/64 loss: -0.012872755527496338
Batch 48/64 loss: 0.009823143482208252
Batch 49/64 loss: -0.0002608299255371094
Batch 50/64 loss: 0.0030078887939453125
Batch 51/64 loss: -0.0009201169013977051
Batch 52/64 loss: 0.01987588405609131
Batch 53/64 loss: 0.01079702377319336
Batch 54/64 loss: -0.0061421990394592285
Batch 55/64 loss: -0.012697100639343262
Batch 56/64 loss: 0.027297496795654297
Batch 57/64 loss: 0.025098025798797607
Batch 58/64 loss: -0.00731813907623291
Batch 59/64 loss: -0.008904516696929932
Batch 60/64 loss: 0.0317920446395874
Batch 61/64 loss: 0.0023609399795532227
Batch 62/64 loss: 0.004042088985443115
Batch 63/64 loss: 0.02191305160522461
Batch 64/64 loss: 0.024404525756835938
Epoch 45  Train loss: 0.005786704082115024  Val loss: 0.04285618345352383
Epoch 46
-------------------------------
Batch 1/64 loss: 0.004476666450500488
Batch 2/64 loss: 0.02329421043395996
Batch 3/64 loss: 0.001854240894317627
Batch 4/64 loss: -0.0030428171157836914
Batch 5/64 loss: -0.010809481143951416
Batch 6/64 loss: -0.010307610034942627
Batch 7/64 loss: 0.04265797138214111
Batch 8/64 loss: -0.003068089485168457
Batch 9/64 loss: 0.02657461166381836
Batch 10/64 loss: -0.01650172472000122
Batch 11/64 loss: -0.00730663537979126
Batch 12/64 loss: 0.019064486026763916
Batch 13/64 loss: 0.013548612594604492
Batch 14/64 loss: 0.005636334419250488
Batch 15/64 loss: 0.034116923809051514
Batch 16/64 loss: -0.01035916805267334
Batch 17/64 loss: 0.0445781946182251
Batch 18/64 loss: 0.0010883808135986328
Batch 19/64 loss: 0.006920337677001953
Batch 20/64 loss: 0.008328795433044434
Batch 21/64 loss: -0.011356055736541748
Batch 22/64 loss: -0.009990930557250977
Batch 23/64 loss: 0.0067337751388549805
Batch 24/64 loss: 0.004564762115478516
Batch 25/64 loss: -0.0036842823028564453
Batch 26/64 loss: 0.011222362518310547
Batch 27/64 loss: 0.009289145469665527
Batch 28/64 loss: 0.011088371276855469
Batch 29/64 loss: 0.029760777950286865
Batch 30/64 loss: -0.00038951635360717773
Batch 31/64 loss: 0.006806552410125732
Batch 32/64 loss: 0.028703927993774414
Batch 33/64 loss: 0.0121193528175354
Batch 34/64 loss: 0.0021049976348876953
Batch 35/64 loss: 0.021615803241729736
Batch 36/64 loss: 0.03388237953186035
Batch 37/64 loss: 0.0021314024925231934
Batch 38/64 loss: -0.017254769802093506
Batch 39/64 loss: 0.0032991766929626465
Batch 40/64 loss: 0.00623244047164917
Batch 41/64 loss: 0.0005987882614135742
Batch 42/64 loss: 0.018362224102020264
Batch 43/64 loss: -0.0052103400230407715
Batch 44/64 loss: 0.0214841365814209
Batch 45/64 loss: -0.008997619152069092
Batch 46/64 loss: 0.021212339401245117
Batch 47/64 loss: -0.0023341774940490723
Batch 48/64 loss: 0.01656275987625122
Batch 49/64 loss: 0.00847691297531128
Batch 50/64 loss: 0.011553406715393066
Batch 51/64 loss: -0.005574643611907959
Batch 52/64 loss: -0.006390094757080078
Batch 53/64 loss: 0.025958657264709473
Batch 54/64 loss: 0.0007358193397521973
Batch 55/64 loss: 0.01267153024673462
Batch 56/64 loss: 0.040552735328674316
Batch 57/64 loss: 0.020047008991241455
Batch 58/64 loss: 0.0011119842529296875
Batch 59/64 loss: -0.0017783641815185547
Batch 60/64 loss: -0.019205927848815918
Batch 61/64 loss: -0.010116100311279297
Batch 62/64 loss: -0.007036745548248291
Batch 63/64 loss: 0.01412808895111084
Batch 64/64 loss: -0.0026667118072509766
Epoch 46  Train loss: 0.0072539020987118  Val loss: 0.023744442208935714
Saving best model, epoch: 46
Epoch 47
-------------------------------
Batch 1/64 loss: -0.00385129451751709
Batch 2/64 loss: 0.0003859996795654297
Batch 3/64 loss: -0.0010464787483215332
Batch 4/64 loss: -0.008561968803405762
Batch 5/64 loss: 0.007500052452087402
Batch 6/64 loss: 0.008849084377288818
Batch 7/64 loss: 0.0371779203414917
Batch 8/64 loss: 0.01583421230316162
Batch 9/64 loss: -0.014850854873657227
Batch 10/64 loss: 0.00039899349212646484
Batch 11/64 loss: 0.0006365776062011719
Batch 12/64 loss: -0.003353714942932129
Batch 13/64 loss: -0.003116309642791748
Batch 14/64 loss: -0.0008571743965148926
Batch 15/64 loss: -0.0039792656898498535
Batch 16/64 loss: 0.0019030570983886719
Batch 17/64 loss: -0.0016928315162658691
Batch 18/64 loss: 0.011708557605743408
Batch 19/64 loss: 0.014709830284118652
Batch 20/64 loss: 0.014191687107086182
Batch 21/64 loss: 0.00854480266571045
Batch 22/64 loss: 0.012591004371643066
Batch 23/64 loss: 0.01484149694442749
Batch 24/64 loss: 0.002795398235321045
Batch 25/64 loss: 0.0252038836479187
Batch 26/64 loss: -0.012381255626678467
Batch 27/64 loss: 0.006417214870452881
Batch 28/64 loss: -0.0031272172927856445
Batch 29/64 loss: -0.0027967095375061035
Batch 30/64 loss: -0.004125356674194336
Batch 31/64 loss: 0.00038170814514160156
Batch 32/64 loss: 0.004616200923919678
Batch 33/64 loss: -0.01768893003463745
Batch 34/64 loss: 0.005118727684020996
Batch 35/64 loss: 0.01227426528930664
Batch 36/64 loss: 0.01553887128829956
Batch 37/64 loss: 0.021485328674316406
Batch 38/64 loss: 0.0012952089309692383
Batch 39/64 loss: 0.026916146278381348
Batch 40/64 loss: 0.0006746053695678711
Batch 41/64 loss: -0.01777505874633789
Batch 42/64 loss: -0.007555365562438965
Batch 43/64 loss: 0.0033538341522216797
Batch 44/64 loss: 0.019089996814727783
Batch 45/64 loss: -0.0037712454795837402
Batch 46/64 loss: 0.012341797351837158
Batch 47/64 loss: -0.0008654594421386719
Batch 48/64 loss: 0.005888700485229492
Batch 49/64 loss: 0.011890232563018799
Batch 50/64 loss: -0.013802945613861084
Batch 51/64 loss: 0.005473017692565918
Batch 52/64 loss: 0.015291273593902588
Batch 53/64 loss: -0.0012616515159606934
Batch 54/64 loss: 0.006931722164154053
Batch 55/64 loss: 0.011735081672668457
Batch 56/64 loss: 0.008536577224731445
Batch 57/64 loss: 0.005187630653381348
Batch 58/64 loss: 0.00046384334564208984
Batch 59/64 loss: 0.02393639087677002
Batch 60/64 loss: 0.010240316390991211
Batch 61/64 loss: 0.008999884128570557
Batch 62/64 loss: 0.0011668205261230469
Batch 63/64 loss: 0.011464834213256836
Batch 64/64 loss: 0.01219862699508667
Epoch 47  Train loss: 0.0049673830761628995  Val loss: 0.019883585140057856
Saving best model, epoch: 47
Epoch 48
-------------------------------
Batch 1/64 loss: -0.002302825450897217
Batch 2/64 loss: -0.026921212673187256
Batch 3/64 loss: -0.009856164455413818
Batch 4/64 loss: -0.0057086944580078125
Batch 5/64 loss: 0.018142282962799072
Batch 6/64 loss: -0.01451331377029419
Batch 7/64 loss: 0.0003288388252258301
Batch 8/64 loss: -0.009093046188354492
Batch 9/64 loss: -0.004753828048706055
Batch 10/64 loss: 0.013261556625366211
Batch 11/64 loss: 0.00918114185333252
Batch 12/64 loss: 0.01491469144821167
Batch 13/64 loss: -0.013010740280151367
Batch 14/64 loss: 0.0068089962005615234
Batch 15/64 loss: 0.01751124858856201
Batch 16/64 loss: 0.005662202835083008
Batch 17/64 loss: 7.2479248046875e-05
Batch 18/64 loss: 0.009011030197143555
Batch 19/64 loss: -0.010305702686309814
Batch 20/64 loss: -0.010417759418487549
Batch 21/64 loss: 0.006913661956787109
Batch 22/64 loss: -0.004220843315124512
Batch 23/64 loss: -0.012212276458740234
Batch 24/64 loss: -0.013349413871765137
Batch 25/64 loss: 0.023048996925354004
Batch 26/64 loss: 0.010560035705566406
Batch 27/64 loss: 0.008292019367218018
Batch 28/64 loss: -0.0029116272926330566
Batch 29/64 loss: -0.020029783248901367
Batch 30/64 loss: 0.02378547191619873
Batch 31/64 loss: -0.019927263259887695
Batch 32/64 loss: 0.021973609924316406
Batch 33/64 loss: 0.014374256134033203
Batch 34/64 loss: 0.006076455116271973
Batch 35/64 loss: -0.008308947086334229
Batch 36/64 loss: 0.008869767189025879
Batch 37/64 loss: 0.003570854663848877
Batch 38/64 loss: -0.018672466278076172
Batch 39/64 loss: -0.013205289840698242
Batch 40/64 loss: 0.006686151027679443
Batch 41/64 loss: 0.012751579284667969
Batch 42/64 loss: 0.005956172943115234
Batch 43/64 loss: 0.00799781084060669
Batch 44/64 loss: -0.0065819621086120605
Batch 45/64 loss: -0.009259045124053955
Batch 46/64 loss: 0.008050739765167236
Batch 47/64 loss: 0.007211625576019287
Batch 48/64 loss: 0.0027957558631896973
Batch 49/64 loss: -0.0010587573051452637
Batch 50/64 loss: 0.016695618629455566
Batch 51/64 loss: 0.01845383644104004
Batch 52/64 loss: -0.002450108528137207
Batch 53/64 loss: 0.031087636947631836
Batch 54/64 loss: 0.007713675498962402
Batch 55/64 loss: 0.010296940803527832
Batch 56/64 loss: 0.01881086826324463
Batch 57/64 loss: 0.019514918327331543
Batch 58/64 loss: -0.01006406545639038
Batch 59/64 loss: -0.010826170444488525
Batch 60/64 loss: 0.007401704788208008
Batch 61/64 loss: 0.002334415912628174
Batch 62/64 loss: 0.024072349071502686
Batch 63/64 loss: 0.014454007148742676
Batch 64/64 loss: 0.004942417144775391
Epoch 48  Train loss: 0.002955151539222867  Val loss: 0.02613513084621364
Epoch 49
-------------------------------
Batch 1/64 loss: 0.013721227645874023
Batch 2/64 loss: 0.010067164897918701
Batch 3/64 loss: -0.0063103437423706055
Batch 4/64 loss: 0.007225632667541504
Batch 5/64 loss: 0.0014209151268005371
Batch 6/64 loss: -0.013419747352600098
Batch 7/64 loss: 0.006080746650695801
Batch 8/64 loss: 0.008058905601501465
Batch 9/64 loss: 0.003227412700653076
Batch 10/64 loss: -0.020231127738952637
Batch 11/64 loss: 0.029804110527038574
Batch 12/64 loss: -0.0024347901344299316
Batch 13/64 loss: 0.01942431926727295
Batch 14/64 loss: -0.005880594253540039
Batch 15/64 loss: -0.01082378625869751
Batch 16/64 loss: 0.008393645286560059
Batch 17/64 loss: 0.007255256175994873
Batch 18/64 loss: 0.014181137084960938
Batch 19/64 loss: 0.007877469062805176
Batch 20/64 loss: 0.0021096467971801758
Batch 21/64 loss: -0.004585325717926025
Batch 22/64 loss: 0.004426181316375732
Batch 23/64 loss: 0.023199737071990967
Batch 24/64 loss: 0.0035223960876464844
Batch 25/64 loss: -0.003750443458557129
Batch 26/64 loss: -0.000510871410369873
Batch 27/64 loss: 0.002084493637084961
Batch 28/64 loss: -0.009578168392181396
Batch 29/64 loss: 0.0206720232963562
Batch 30/64 loss: 0.0033341646194458008
Batch 31/64 loss: 0.0012484192848205566
Batch 32/64 loss: -0.0014578700065612793
Batch 33/64 loss: -0.012147903442382812
Batch 34/64 loss: 0.0188523530960083
Batch 35/64 loss: 0.00611644983291626
Batch 36/64 loss: 0.0020873546600341797
Batch 37/64 loss: -0.0010698437690734863
Batch 38/64 loss: -0.004254341125488281
Batch 39/64 loss: 0.014751195907592773
Batch 40/64 loss: -0.007284998893737793
Batch 41/64 loss: 0.0012562274932861328
Batch 42/64 loss: -0.006822526454925537
Batch 43/64 loss: 0.012658536434173584
Batch 44/64 loss: 0.008774340152740479
Batch 45/64 loss: -0.004420459270477295
Batch 46/64 loss: -0.004932582378387451
Batch 47/64 loss: 0.016051411628723145
Batch 48/64 loss: -0.00648951530456543
Batch 49/64 loss: -0.00609511137008667
Batch 50/64 loss: 0.010247230529785156
Batch 51/64 loss: -0.0019993185997009277
Batch 52/64 loss: -0.005092322826385498
Batch 53/64 loss: -0.007857441902160645
Batch 54/64 loss: -0.0015805959701538086
Batch 55/64 loss: -0.010459601879119873
Batch 56/64 loss: -0.007922172546386719
Batch 57/64 loss: 0.0065264105796813965
Batch 58/64 loss: 0.021320998668670654
Batch 59/64 loss: -0.018654942512512207
Batch 60/64 loss: 0.002897500991821289
Batch 61/64 loss: 0.0044484734535217285
Batch 62/64 loss: 0.005954146385192871
Batch 63/64 loss: -0.011019885540008545
Batch 64/64 loss: -0.0033271312713623047
Epoch 49  Train loss: 0.002034441630045573  Val loss: 0.02680478943991907
Epoch 50
-------------------------------
Batch 1/64 loss: -0.001423478126525879
Batch 2/64 loss: 0.011861264705657959
Batch 3/64 loss: -0.0005144476890563965
Batch 4/64 loss: 0.0007749199867248535
Batch 5/64 loss: 0.0024878978729248047
Batch 6/64 loss: 0.02267169952392578
Batch 7/64 loss: -0.008205235004425049
Batch 8/64 loss: 0.005965173244476318
Batch 9/64 loss: 0.007467865943908691
Batch 10/64 loss: 0.017393887042999268
Batch 11/64 loss: 0.008522987365722656
Batch 12/64 loss: 0.002560257911682129
Batch 13/64 loss: 0.0037116408348083496
Batch 14/64 loss: 0.00041365623474121094
Batch 15/64 loss: 0.027899861335754395
Batch 16/64 loss: -0.00946033000946045
Batch 17/64 loss: -0.004464864730834961
Batch 18/64 loss: 0.016633808612823486
Batch 19/64 loss: 0.024066388607025146
Batch 20/64 loss: 0.0042972564697265625
Batch 21/64 loss: -0.01658254861831665
Batch 22/64 loss: -0.0004972219467163086
Batch 23/64 loss: -0.00579988956451416
Batch 24/64 loss: 0.018606603145599365
Batch 25/64 loss: -0.007122397422790527
Batch 26/64 loss: 0.01569122076034546
Batch 27/64 loss: 0.004899501800537109
Batch 28/64 loss: 0.013155698776245117
Batch 29/64 loss: -0.007362484931945801
Batch 30/64 loss: -0.003985106945037842
Batch 31/64 loss: -0.0036725401878356934
Batch 32/64 loss: 0.002820611000061035
Batch 33/64 loss: 0.017654776573181152
Batch 34/64 loss: 0.004381537437438965
Batch 35/64 loss: 0.0017391443252563477
Batch 36/64 loss: 0.020703375339508057
Batch 37/64 loss: 0.010327696800231934
Batch 38/64 loss: -0.0013594627380371094
Batch 39/64 loss: 0.012850940227508545
Batch 40/64 loss: 0.0069427490234375
Batch 41/64 loss: -0.009920954704284668
Batch 42/64 loss: 0.032356858253479004
Batch 43/64 loss: -0.02133917808532715
Batch 44/64 loss: 0.004871070384979248
Batch 45/64 loss: 0.012289166450500488
Batch 46/64 loss: -0.010378479957580566
Batch 47/64 loss: 0.007792174816131592
Batch 48/64 loss: -0.002112150192260742
Batch 49/64 loss: 0.012296497821807861
Batch 50/64 loss: 0.006449580192565918
Batch 51/64 loss: -0.002656102180480957
Batch 52/64 loss: -0.012956321239471436
Batch 53/64 loss: -0.016136765480041504
Batch 54/64 loss: -0.021915018558502197
Batch 55/64 loss: -0.02177053689956665
Batch 56/64 loss: -0.02074199914932251
Batch 57/64 loss: -0.0008579492568969727
Batch 58/64 loss: 0.00018656253814697266
Batch 59/64 loss: 0.003742814064025879
Batch 60/64 loss: -0.017980098724365234
Batch 61/64 loss: -0.0009749531745910645
Batch 62/64 loss: 0.007776737213134766
Batch 63/64 loss: -0.0011160969734191895
Batch 64/64 loss: 0.0016753673553466797
Epoch 50  Train loss: 0.0022621771868537453  Val loss: 0.015574897072978855
Saving best model, epoch: 50
Epoch 51
-------------------------------
Batch 1/64 loss: -0.007757782936096191
Batch 2/64 loss: -0.015609025955200195
Batch 3/64 loss: 0.014838218688964844
Batch 4/64 loss: 0.004288911819458008
Batch 5/64 loss: -0.0186769962310791
Batch 6/64 loss: 0.009246468544006348
Batch 7/64 loss: -0.005713403224945068
Batch 8/64 loss: -0.007429242134094238
Batch 9/64 loss: -0.004896402359008789
Batch 10/64 loss: -0.0015747547149658203
Batch 11/64 loss: -0.004372239112854004
Batch 12/64 loss: -0.022893428802490234
Batch 13/64 loss: -0.012854158878326416
Batch 14/64 loss: -0.012788236141204834
Batch 15/64 loss: -0.015429317951202393
Batch 16/64 loss: 0.033065199851989746
Batch 17/64 loss: -0.010823726654052734
Batch 18/64 loss: 0.00522226095199585
Batch 19/64 loss: 0.00046694278717041016
Batch 20/64 loss: 0.0041925907135009766
Batch 21/64 loss: 0.007799506187438965
Batch 22/64 loss: -0.009543836116790771
Batch 23/64 loss: 0.008239448070526123
Batch 24/64 loss: 0.021069049835205078
Batch 25/64 loss: 0.0016402602195739746
Batch 26/64 loss: -0.0139579176902771
Batch 27/64 loss: 0.022341907024383545
Batch 28/64 loss: 0.0021550655364990234
Batch 29/64 loss: -0.005152344703674316
Batch 30/64 loss: -0.011591792106628418
Batch 31/64 loss: 0.02768772840499878
Batch 32/64 loss: 0.012825608253479004
Batch 33/64 loss: 0.01934349536895752
Batch 34/64 loss: 0.002283036708831787
Batch 35/64 loss: -0.003742218017578125
Batch 36/64 loss: -0.01840996742248535
Batch 37/64 loss: -0.023090779781341553
Batch 38/64 loss: 0.0027692317962646484
Batch 39/64 loss: 0.026324033737182617
Batch 40/64 loss: -0.0121690034866333
Batch 41/64 loss: -0.0016870498657226562
Batch 42/64 loss: 0.01399374008178711
Batch 43/64 loss: 0.018025219440460205
Batch 44/64 loss: -0.009090065956115723
Batch 45/64 loss: 0.0002289414405822754
Batch 46/64 loss: 0.02135920524597168
Batch 47/64 loss: -0.022888660430908203
Batch 48/64 loss: -0.001837015151977539
Batch 49/64 loss: 0.0010470151901245117
Batch 50/64 loss: 0.03406810760498047
Batch 51/64 loss: 0.009529590606689453
Batch 52/64 loss: -0.00994873046875
Batch 53/64 loss: -0.023657917976379395
Batch 54/64 loss: 0.0038616061210632324
Batch 55/64 loss: -0.0004234910011291504
Batch 56/64 loss: 0.0009169578552246094
Batch 57/64 loss: -0.006543576717376709
Batch 58/64 loss: 0.008261919021606445
Batch 59/64 loss: 0.01373279094696045
Batch 60/64 loss: -0.00783538818359375
Batch 61/64 loss: -0.0038461685180664062
Batch 62/64 loss: -0.007649362087249756
Batch 63/64 loss: 0.029458820819854736
Batch 64/64 loss: -0.0028944015502929688
Epoch 51  Train loss: 0.0006937737558402267  Val loss: 0.0292217332063262
Epoch 52
-------------------------------
Batch 1/64 loss: 0.003914773464202881
Batch 2/64 loss: 0.005204141139984131
Batch 3/64 loss: 0.024717092514038086
Batch 4/64 loss: -0.013396203517913818
Batch 5/64 loss: -0.006466925144195557
Batch 6/64 loss: -0.008797526359558105
Batch 7/64 loss: -0.001284956932067871
Batch 8/64 loss: -0.0020479559898376465
Batch 9/64 loss: -0.0020672082901000977
Batch 10/64 loss: 0.002635359764099121
Batch 11/64 loss: -0.010142743587493896
Batch 12/64 loss: 0.00814807415008545
Batch 13/64 loss: -0.022061169147491455
Batch 14/64 loss: 0.005092620849609375
Batch 15/64 loss: -0.0051572322845458984
Batch 16/64 loss: -0.0010215044021606445
Batch 17/64 loss: -0.003659367561340332
Batch 18/64 loss: -0.01149815320968628
Batch 19/64 loss: -0.003946065902709961
Batch 20/64 loss: 0.009224534034729004
Batch 21/64 loss: 0.01057887077331543
Batch 22/64 loss: 0.018116116523742676
Batch 23/64 loss: 0.012560486793518066
Batch 24/64 loss: -0.0003066062927246094
Batch 25/64 loss: 0.004164993762969971
Batch 26/64 loss: -0.014499306678771973
Batch 27/64 loss: -0.008209586143493652
Batch 28/64 loss: 0.010149896144866943
Batch 29/64 loss: -0.020566463470458984
Batch 30/64 loss: -0.002819240093231201
Batch 31/64 loss: -0.0058097243309021
Batch 32/64 loss: 0.002048015594482422
Batch 33/64 loss: 0.009013473987579346
Batch 34/64 loss: 0.003949761390686035
Batch 35/64 loss: -0.021592378616333008
Batch 36/64 loss: 0.01014488935470581
Batch 37/64 loss: -0.004437446594238281
Batch 38/64 loss: 0.006613790988922119
Batch 39/64 loss: 0.006783604621887207
Batch 40/64 loss: 0.0025767087936401367
Batch 41/64 loss: -0.01278775930404663
Batch 42/64 loss: -0.02292698621749878
Batch 43/64 loss: 0.0028489232063293457
Batch 44/64 loss: -0.0055664777755737305
Batch 45/64 loss: -0.0077533721923828125
Batch 46/64 loss: -0.003966033458709717
Batch 47/64 loss: -0.012898266315460205
Batch 48/64 loss: 0.009183645248413086
Batch 49/64 loss: 0.011618375778198242
Batch 50/64 loss: 0.004910290241241455
Batch 51/64 loss: 0.01819366216659546
Batch 52/64 loss: -0.003258943557739258
Batch 53/64 loss: 0.00946652889251709
Batch 54/64 loss: 0.004047870635986328
Batch 55/64 loss: -0.0018846392631530762
Batch 56/64 loss: 0.0022716522216796875
Batch 57/64 loss: -0.006179213523864746
Batch 58/64 loss: 0.00506967306137085
Batch 59/64 loss: -0.007665753364562988
Batch 60/64 loss: -0.0029314756393432617
Batch 61/64 loss: 0.028032898902893066
Batch 62/64 loss: 0.004282832145690918
Batch 63/64 loss: 0.006240248680114746
Batch 64/64 loss: -0.01210319995880127
Epoch 52  Train loss: -7.65534008250517e-05  Val loss: 0.03266237855367234
Epoch 53
-------------------------------
Batch 1/64 loss: -0.019890308380126953
Batch 2/64 loss: 0.004658043384552002
Batch 3/64 loss: 0.01110929250717163
Batch 4/64 loss: -0.013783276081085205
Batch 5/64 loss: 0.014322936534881592
Batch 6/64 loss: -0.0050585269927978516
Batch 7/64 loss: 0.006550788879394531
Batch 8/64 loss: 0.009461164474487305
Batch 9/64 loss: 0.00249403715133667
Batch 10/64 loss: -0.011566460132598877
Batch 11/64 loss: 0.012919068336486816
Batch 12/64 loss: -0.004982113838195801
Batch 13/64 loss: -0.02073192596435547
Batch 14/64 loss: -0.0012500286102294922
Batch 15/64 loss: -0.006614387035369873
Batch 16/64 loss: -0.014383256435394287
Batch 17/64 loss: -0.002045273780822754
Batch 18/64 loss: -0.017425358295440674
Batch 19/64 loss: -0.006451845169067383
Batch 20/64 loss: -0.02010166645050049
Batch 21/64 loss: -0.005584239959716797
Batch 22/64 loss: 0.009761631488800049
Batch 23/64 loss: -0.028950631618499756
Batch 24/64 loss: -0.01711440086364746
Batch 25/64 loss: 0.019118666648864746
Batch 26/64 loss: -0.01643967628479004
Batch 27/64 loss: 7.843971252441406e-05
Batch 28/64 loss: 0.014752745628356934
Batch 29/64 loss: 0.009601950645446777
Batch 30/64 loss: 0.004634678363800049
Batch 31/64 loss: 0.009362459182739258
Batch 32/64 loss: -0.022615253925323486
Batch 33/64 loss: -0.0057141780853271484
Batch 34/64 loss: -0.01486581563949585
Batch 35/64 loss: 0.008759856224060059
Batch 36/64 loss: -0.008259057998657227
Batch 37/64 loss: -0.013350248336791992
Batch 38/64 loss: 0.0020516514778137207
Batch 39/64 loss: 0.022716403007507324
Batch 40/64 loss: 0.011083662509918213
Batch 41/64 loss: -0.02081131935119629
Batch 42/64 loss: 0.020385265350341797
Batch 43/64 loss: 0.02678591012954712
Batch 44/64 loss: -0.0019417405128479004
Batch 45/64 loss: -0.002723097801208496
Batch 46/64 loss: -0.020110607147216797
Batch 47/64 loss: 0.0008909702301025391
Batch 48/64 loss: -0.0035470128059387207
Batch 49/64 loss: -0.020995676517486572
Batch 50/64 loss: -0.022222936153411865
Batch 51/64 loss: -0.0009564757347106934
Batch 52/64 loss: -0.014371395111083984
Batch 53/64 loss: 0.004240751266479492
Batch 54/64 loss: -0.001152336597442627
Batch 55/64 loss: 0.0004553794860839844
Batch 56/64 loss: -0.013439536094665527
Batch 57/64 loss: -0.010042548179626465
Batch 58/64 loss: -0.00419539213180542
Batch 59/64 loss: -0.011336684226989746
Batch 60/64 loss: -0.01707291603088379
Batch 61/64 loss: 0.0005946755409240723
Batch 62/64 loss: -0.02040499448776245
Batch 63/64 loss: -0.0008054971694946289
Batch 64/64 loss: 0.013647794723510742
Epoch 53  Train loss: -0.003549518772200042  Val loss: 0.01895967955441819
Epoch 54
-------------------------------
Batch 1/64 loss: -0.003705739974975586
Batch 2/64 loss: 0.0067629218101501465
Batch 3/64 loss: -0.015819013118743896
Batch 4/64 loss: -0.020361661911010742
Batch 5/64 loss: -0.03125578165054321
Batch 6/64 loss: 0.001183629035949707
Batch 7/64 loss: -0.007786273956298828
Batch 8/64 loss: -0.006317496299743652
Batch 9/64 loss: 0.006506383419036865
Batch 10/64 loss: -0.011509895324707031
Batch 11/64 loss: -0.008152663707733154
Batch 12/64 loss: 0.010824739933013916
Batch 13/64 loss: 0.010212540626525879
Batch 14/64 loss: -0.02607792615890503
Batch 15/64 loss: 0.006803274154663086
Batch 16/64 loss: 0.00357663631439209
Batch 17/64 loss: 0.01836615800857544
Batch 18/64 loss: -0.0028520822525024414
Batch 19/64 loss: -0.004120886325836182
Batch 20/64 loss: 0.010416984558105469
Batch 21/64 loss: -0.008975982666015625
Batch 22/64 loss: 0.00011289119720458984
Batch 23/64 loss: 0.008907079696655273
Batch 24/64 loss: 0.004491925239562988
Batch 25/64 loss: -0.0006186962127685547
Batch 26/64 loss: 0.003978729248046875
Batch 27/64 loss: -0.00026732683181762695
Batch 28/64 loss: -0.00310361385345459
Batch 29/64 loss: 0.015043914318084717
Batch 30/64 loss: 0.01342099905014038
Batch 31/64 loss: 0.025112688541412354
Batch 32/64 loss: 0.00031381845474243164
Batch 33/64 loss: -0.0027088522911071777
Batch 34/64 loss: 0.021498024463653564
Batch 35/64 loss: 2.9206275939941406e-06
Batch 36/64 loss: -0.006031513214111328
Batch 37/64 loss: -0.015050828456878662
Batch 38/64 loss: -0.01040661334991455
Batch 39/64 loss: -0.009204626083374023
Batch 40/64 loss: -0.00907289981842041
Batch 41/64 loss: -0.01604539155960083
Batch 42/64 loss: -0.005017518997192383
Batch 43/64 loss: -0.007383823394775391
Batch 44/64 loss: -0.010860919952392578
Batch 45/64 loss: -0.026213467121124268
Batch 46/64 loss: -0.016641557216644287
Batch 47/64 loss: 0.04838573932647705
Batch 48/64 loss: -0.011282920837402344
Batch 49/64 loss: -0.01059812307357788
Batch 50/64 loss: 0.030189216136932373
Batch 51/64 loss: -0.031221330165863037
Batch 52/64 loss: -0.024807512760162354
Batch 53/64 loss: 0.0029535293579101562
Batch 54/64 loss: 0.005190014839172363
Batch 55/64 loss: 0.013867735862731934
Batch 56/64 loss: -0.014389395713806152
Batch 57/64 loss: -0.009022235870361328
Batch 58/64 loss: -0.019766926765441895
Batch 59/64 loss: -0.008779942989349365
Batch 60/64 loss: 0.004785299301147461
Batch 61/64 loss: -0.017046689987182617
Batch 62/64 loss: 0.014969170093536377
Batch 63/64 loss: 0.02811598777770996
Batch 64/64 loss: 0.009488582611083984
Epoch 54  Train loss: -0.0017155881021537033  Val loss: 0.019077192262275933
Epoch 55
-------------------------------
Batch 1/64 loss: -0.021192550659179688
Batch 2/64 loss: -0.003942251205444336
Batch 3/64 loss: -0.00545346736907959
Batch 4/64 loss: 0.024591445922851562
Batch 5/64 loss: -0.018167078495025635
Batch 6/64 loss: 0.0060495734214782715
Batch 7/64 loss: -0.015437483787536621
Batch 8/64 loss: 0.00010150671005249023
Batch 9/64 loss: 0.001391291618347168
Batch 10/64 loss: 0.004173994064331055
Batch 11/64 loss: 0.003750622272491455
Batch 12/64 loss: 0.006120443344116211
Batch 13/64 loss: -0.015531539916992188
Batch 14/64 loss: 0.0029001832008361816
Batch 15/64 loss: -0.007082700729370117
Batch 16/64 loss: -0.027048468589782715
Batch 17/64 loss: 0.01388782262802124
Batch 18/64 loss: 0.005671381950378418
Batch 19/64 loss: -0.008382260799407959
Batch 20/64 loss: -0.016798436641693115
Batch 21/64 loss: -0.0026617050170898438
Batch 22/64 loss: -0.011231660842895508
Batch 23/64 loss: -0.02068185806274414
Batch 24/64 loss: -0.004385232925415039
Batch 25/64 loss: -0.009618937969207764
Batch 26/64 loss: -0.018601417541503906
Batch 27/64 loss: 0.0004321932792663574
Batch 28/64 loss: -0.006585955619812012
Batch 29/64 loss: 0.009568929672241211
Batch 30/64 loss: -0.023740828037261963
Batch 31/64 loss: -0.009665608406066895
Batch 32/64 loss: -0.0028344392776489258
Batch 33/64 loss: 0.003779172897338867
Batch 34/64 loss: -0.01510310173034668
Batch 35/64 loss: 0.0024025440216064453
Batch 36/64 loss: -0.004301011562347412
Batch 37/64 loss: -0.011308073997497559
Batch 38/64 loss: -0.02269911766052246
Batch 39/64 loss: 0.014239728450775146
Batch 40/64 loss: 0.0006153583526611328
Batch 41/64 loss: -0.017495930194854736
Batch 42/64 loss: 0.008301973342895508
Batch 43/64 loss: -0.0036013126373291016
Batch 44/64 loss: -0.027388274669647217
Batch 45/64 loss: 0.0030704736709594727
Batch 46/64 loss: -0.01842111349105835
Batch 47/64 loss: -0.01398533582687378
Batch 48/64 loss: -0.013477802276611328
Batch 49/64 loss: -0.014206230640411377
Batch 50/64 loss: -0.008130073547363281
Batch 51/64 loss: -0.005121171474456787
Batch 52/64 loss: 0.005441725254058838
Batch 53/64 loss: -0.027125239372253418
Batch 54/64 loss: -0.0010166168212890625
Batch 55/64 loss: 0.0068558454513549805
Batch 56/64 loss: -0.0007256269454956055
Batch 57/64 loss: -0.026055514812469482
Batch 58/64 loss: 0.013139843940734863
Batch 59/64 loss: -0.01953786611557007
Batch 60/64 loss: 0.013257145881652832
Batch 61/64 loss: 0.004993915557861328
Batch 62/64 loss: -0.02250051498413086
Batch 63/64 loss: -0.0048798322677612305
Batch 64/64 loss: -0.009344339370727539
Epoch 55  Train loss: -0.00593560443204992  Val loss: 0.014715492930199272
Saving best model, epoch: 55
Epoch 56
-------------------------------
Batch 1/64 loss: -0.016772925853729248
Batch 2/64 loss: -0.018035948276519775
Batch 3/64 loss: -0.006689250469207764
Batch 4/64 loss: -0.00012093782424926758
Batch 5/64 loss: 0.021196842193603516
Batch 6/64 loss: -0.014092326164245605
Batch 7/64 loss: -0.004142165184020996
Batch 8/64 loss: -0.012253105640411377
Batch 9/64 loss: -0.014131426811218262
Batch 10/64 loss: -0.023024439811706543
Batch 11/64 loss: -0.005787253379821777
Batch 12/64 loss: 0.0014050006866455078
Batch 13/64 loss: -0.002290010452270508
Batch 14/64 loss: -0.016134023666381836
Batch 15/64 loss: -0.0010571479797363281
Batch 16/64 loss: -0.008536279201507568
Batch 17/64 loss: -0.008896887302398682
Batch 18/64 loss: -0.006974518299102783
Batch 19/64 loss: 0.0059372782707214355
Batch 20/64 loss: -0.015496969223022461
Batch 21/64 loss: 0.0039084553718566895
Batch 22/64 loss: -0.013279974460601807
Batch 23/64 loss: -0.00486600399017334
Batch 24/64 loss: 0.01877206563949585
Batch 25/64 loss: -0.018978655338287354
Batch 26/64 loss: -0.01387012004852295
Batch 27/64 loss: 0.006458103656768799
Batch 28/64 loss: 0.016188859939575195
Batch 29/64 loss: -0.017886877059936523
Batch 30/64 loss: -0.02168726921081543
Batch 31/64 loss: 0.014999151229858398
Batch 32/64 loss: 0.008585870265960693
Batch 33/64 loss: -0.027899205684661865
Batch 34/64 loss: -0.0021918416023254395
Batch 35/64 loss: -0.01350092887878418
Batch 36/64 loss: -0.0141715407371521
Batch 37/64 loss: -0.004688680171966553
Batch 38/64 loss: -0.002925574779510498
Batch 39/64 loss: -0.0014185309410095215
Batch 40/64 loss: -0.02426522970199585
Batch 41/64 loss: -0.006505787372589111
Batch 42/64 loss: -0.007367372512817383
Batch 43/64 loss: -0.028330206871032715
Batch 44/64 loss: 0.004272580146789551
Batch 45/64 loss: -2.866983413696289e-05
Batch 46/64 loss: 0.010419011116027832
Batch 47/64 loss: -0.016689598560333252
Batch 48/64 loss: -0.029941141605377197
Batch 49/64 loss: -0.01747894287109375
Batch 50/64 loss: -0.0063430070877075195
Batch 51/64 loss: -0.010287344455718994
Batch 52/64 loss: 0.000593721866607666
Batch 53/64 loss: -0.018076717853546143
Batch 54/64 loss: 0.00613856315612793
Batch 55/64 loss: 0.010351181030273438
Batch 56/64 loss: 0.0022339224815368652
Batch 57/64 loss: 0.013992547988891602
Batch 58/64 loss: -0.013088047504425049
Batch 59/64 loss: 0.030837535858154297
Batch 60/64 loss: -0.019697070121765137
Batch 61/64 loss: 0.00026023387908935547
Batch 62/64 loss: 0.023166000843048096
Batch 63/64 loss: -0.0017004609107971191
Batch 64/64 loss: -0.014342963695526123
Epoch 56  Train loss: -0.005374756280113669  Val loss: 0.016891396537269513
Epoch 57
-------------------------------
Batch 1/64 loss: 0.002506077289581299
Batch 2/64 loss: -0.02191370725631714
Batch 3/64 loss: -0.016093194484710693
Batch 4/64 loss: 0.014398276805877686
Batch 5/64 loss: -0.002298414707183838
Batch 6/64 loss: -0.025369763374328613
Batch 7/64 loss: -0.0002554655075073242
Batch 8/64 loss: -0.00839543342590332
Batch 9/64 loss: -0.0006890296936035156
Batch 10/64 loss: 0.0030487775802612305
Batch 11/64 loss: -0.015743732452392578
Batch 12/64 loss: 0.005346357822418213
Batch 13/64 loss: -0.02438443899154663
Batch 14/64 loss: 0.004819989204406738
Batch 15/64 loss: -0.029081404209136963
Batch 16/64 loss: -0.01958012580871582
Batch 17/64 loss: -0.01423501968383789
Batch 18/64 loss: -0.005132198333740234
Batch 19/64 loss: -0.016411006450653076
Batch 20/64 loss: 0.0055062174797058105
Batch 21/64 loss: -0.00883471965789795
Batch 22/64 loss: -0.01116478443145752
Batch 23/64 loss: 0.0008751749992370605
Batch 24/64 loss: -0.0013387799263000488
Batch 25/64 loss: -0.02608245611190796
Batch 26/64 loss: -0.02959352731704712
Batch 27/64 loss: 0.025882363319396973
Batch 28/64 loss: 0.019158601760864258
Batch 29/64 loss: -0.01367253065109253
Batch 30/64 loss: -0.0034672021865844727
Batch 31/64 loss: -0.006941437721252441
Batch 32/64 loss: -0.014558851718902588
Batch 33/64 loss: -0.010531902313232422
Batch 34/64 loss: -0.008640289306640625
Batch 35/64 loss: 0.011169731616973877
Batch 36/64 loss: -0.0014473199844360352
Batch 37/64 loss: 0.0011243820190429688
Batch 38/64 loss: -0.0014265179634094238
Batch 39/64 loss: -0.0017442107200622559
Batch 40/64 loss: -0.006412386894226074
Batch 41/64 loss: 0.002103865146636963
Batch 42/64 loss: -0.017595291137695312
Batch 43/64 loss: -0.02568948268890381
Batch 44/64 loss: 0.003781437873840332
Batch 45/64 loss: 0.020912647247314453
Batch 46/64 loss: -0.01530909538269043
Batch 47/64 loss: 0.012699306011199951
Batch 48/64 loss: -0.01730823516845703
Batch 49/64 loss: -0.014089345932006836
Batch 50/64 loss: 0.005382895469665527
Batch 51/64 loss: -0.00668710470199585
Batch 52/64 loss: 0.014035582542419434
Batch 53/64 loss: -0.011444509029388428
Batch 54/64 loss: -0.013628542423248291
Batch 55/64 loss: -0.009953737258911133
Batch 56/64 loss: -0.01153939962387085
Batch 57/64 loss: -0.006798207759857178
Batch 58/64 loss: -0.0014953017234802246
Batch 59/64 loss: -0.025734663009643555
Batch 60/64 loss: -0.021355032920837402
Batch 61/64 loss: -0.0005990266799926758
Batch 62/64 loss: -0.006675541400909424
Batch 63/64 loss: -0.010885953903198242
Batch 64/64 loss: -0.0018644928932189941
Epoch 57  Train loss: -0.006445098157022514  Val loss: 0.01889375558833486
Epoch 58
-------------------------------
Batch 1/64 loss: 0.011494636535644531
Batch 2/64 loss: -0.026430249214172363
Batch 3/64 loss: 0.008017778396606445
Batch 4/64 loss: -0.0038806796073913574
Batch 5/64 loss: -0.017021000385284424
Batch 6/64 loss: -0.008388936519622803
Batch 7/64 loss: 0.006019234657287598
Batch 8/64 loss: -0.01795172691345215
Batch 9/64 loss: 0.007374405860900879
Batch 10/64 loss: -0.011850714683532715
Batch 11/64 loss: 0.011711180210113525
Batch 12/64 loss: -0.007537782192230225
Batch 13/64 loss: -0.013668239116668701
Batch 14/64 loss: -0.0022644996643066406
Batch 15/64 loss: -0.011317551136016846
Batch 16/64 loss: -0.015016138553619385
Batch 17/64 loss: -0.019282221794128418
Batch 18/64 loss: -0.011312007904052734
Batch 19/64 loss: -0.0020315051078796387
Batch 20/64 loss: 0.010486304759979248
Batch 21/64 loss: 0.005000889301300049
Batch 22/64 loss: -0.013324141502380371
Batch 23/64 loss: 0.00013679265975952148
Batch 24/64 loss: 0.001031041145324707
Batch 25/64 loss: -0.0021973252296447754
Batch 26/64 loss: 0.0008993148803710938
Batch 27/64 loss: -0.019779682159423828
Batch 28/64 loss: -0.0005110502243041992
Batch 29/64 loss: -0.009134054183959961
Batch 30/64 loss: -0.0013889074325561523
Batch 31/64 loss: 0.004849255084991455
Batch 32/64 loss: -0.006331741809844971
Batch 33/64 loss: -0.019044220447540283
Batch 34/64 loss: -0.004877030849456787
Batch 35/64 loss: -0.015739738941192627
Batch 36/64 loss: 0.008940577507019043
Batch 37/64 loss: -0.00014269351959228516
Batch 38/64 loss: -0.007153987884521484
Batch 39/64 loss: 0.009486079216003418
Batch 40/64 loss: -0.0115431547164917
Batch 41/64 loss: 0.02296847105026245
Batch 42/64 loss: 0.0030062198638916016
Batch 43/64 loss: -0.021066367626190186
Batch 44/64 loss: -0.008242249488830566
Batch 45/64 loss: -0.0014678239822387695
Batch 46/64 loss: -0.01686406135559082
Batch 47/64 loss: -0.013410866260528564
Batch 48/64 loss: 0.0036635398864746094
Batch 49/64 loss: 0.012847840785980225
Batch 50/64 loss: -0.005500078201293945
Batch 51/64 loss: -0.01800006628036499
Batch 52/64 loss: 0.002594590187072754
Batch 53/64 loss: -0.0056154727935791016
Batch 54/64 loss: 0.009501874446868896
Batch 55/64 loss: -0.0012233257293701172
Batch 56/64 loss: -0.006762146949768066
Batch 57/64 loss: -0.005436241626739502
Batch 58/64 loss: 0.002799808979034424
Batch 59/64 loss: -0.005106329917907715
Batch 60/64 loss: 0.006085693836212158
Batch 61/64 loss: 0.014264404773712158
Batch 62/64 loss: -0.005382359027862549
Batch 63/64 loss: -0.0044738054275512695
Batch 64/64 loss: -0.025467216968536377
Epoch 58  Train loss: -0.003977924000983145  Val loss: 0.030964107652710064
Epoch 59
-------------------------------
Batch 1/64 loss: 0.00602346658706665
Batch 2/64 loss: -0.019460737705230713
Batch 3/64 loss: 0.014282405376434326
Batch 4/64 loss: -0.016425788402557373
Batch 5/64 loss: 0.029020845890045166
Batch 6/64 loss: -0.01969730854034424
Batch 7/64 loss: 0.0042678117752075195
Batch 8/64 loss: -0.007921278476715088
Batch 9/64 loss: -0.009238898754119873
Batch 10/64 loss: -0.008856654167175293
Batch 11/64 loss: -0.010063707828521729
Batch 12/64 loss: -0.01106405258178711
Batch 13/64 loss: 0.013119816780090332
Batch 14/64 loss: -0.011417567729949951
Batch 15/64 loss: 0.008803129196166992
Batch 16/64 loss: -0.0052530765533447266
Batch 17/64 loss: -0.0028159618377685547
Batch 18/64 loss: 0.0007537603378295898
Batch 19/64 loss: -0.01074838638305664
Batch 20/64 loss: -0.008543074131011963
Batch 21/64 loss: -0.02939784526824951
Batch 22/64 loss: -0.022405028343200684
Batch 23/64 loss: -0.008681535720825195
Batch 24/64 loss: -0.0164911150932312
Batch 25/64 loss: 0.0025154948234558105
Batch 26/64 loss: 0.005031168460845947
Batch 27/64 loss: 0.00014269351959228516
Batch 28/64 loss: 0.009708642959594727
Batch 29/64 loss: -0.018056392669677734
Batch 30/64 loss: -0.010496139526367188
Batch 31/64 loss: 0.008161187171936035
Batch 32/64 loss: -0.029658913612365723
Batch 33/64 loss: -0.0037796497344970703
Batch 34/64 loss: -0.015489637851715088
Batch 35/64 loss: -0.02096724510192871
Batch 36/64 loss: -0.00854039192199707
Batch 37/64 loss: -0.002892017364501953
Batch 38/64 loss: -0.01714634895324707
Batch 39/64 loss: 0.011337876319885254
Batch 40/64 loss: -0.024535536766052246
Batch 41/64 loss: -0.013063907623291016
Batch 42/64 loss: -0.03488355875015259
Batch 43/64 loss: 0.0018402338027954102
Batch 44/64 loss: -0.005777120590209961
Batch 45/64 loss: -0.020603835582733154
Batch 46/64 loss: 0.0416949987411499
Batch 47/64 loss: -0.012898087501525879
Batch 48/64 loss: -0.005517899990081787
Batch 49/64 loss: -0.004041612148284912
Batch 50/64 loss: -0.013078272342681885
Batch 51/64 loss: -0.013420403003692627
Batch 52/64 loss: -0.009112536907196045
Batch 53/64 loss: -0.006888985633850098
Batch 54/64 loss: -0.017306208610534668
Batch 55/64 loss: -0.019774138927459717
Batch 56/64 loss: -0.002523183822631836
Batch 57/64 loss: -0.015046000480651855
Batch 58/64 loss: 0.0013331770896911621
Batch 59/64 loss: -0.018252015113830566
Batch 60/64 loss: -0.002129852771759033
Batch 61/64 loss: 0.008527636528015137
Batch 62/64 loss: -0.016856014728546143
Batch 63/64 loss: -0.007021307945251465
Batch 64/64 loss: -0.011401236057281494
Epoch 59  Train loss: -0.007062365728266099  Val loss: 0.01389388591563169
Saving best model, epoch: 59
Epoch 60
-------------------------------
Batch 1/64 loss: -0.010986328125
Batch 2/64 loss: -0.026456236839294434
Batch 3/64 loss: 0.018277525901794434
Batch 4/64 loss: -0.010865926742553711
Batch 5/64 loss: -0.008663177490234375
Batch 6/64 loss: -0.0023352503776550293
Batch 7/64 loss: -0.022144556045532227
Batch 8/64 loss: 0.0014853477478027344
Batch 9/64 loss: -0.008878231048583984
Batch 10/64 loss: -0.006586551666259766
Batch 11/64 loss: -0.007246732711791992
Batch 12/64 loss: 0.0018164515495300293
Batch 13/64 loss: -0.002660512924194336
Batch 14/64 loss: -0.01587057113647461
Batch 15/64 loss: -0.01243281364440918
Batch 16/64 loss: -0.02154254913330078
Batch 17/64 loss: -0.03338843584060669
Batch 18/64 loss: -0.0006070137023925781
Batch 19/64 loss: -0.009040772914886475
Batch 20/64 loss: 0.0036743879318237305
Batch 21/64 loss: 0.022924184799194336
Batch 22/64 loss: -0.023195862770080566
Batch 23/64 loss: -0.020370781421661377
Batch 24/64 loss: 0.011195540428161621
Batch 25/64 loss: -0.014706850051879883
Batch 26/64 loss: -0.023819327354431152
Batch 27/64 loss: -0.019878149032592773
Batch 28/64 loss: 0.012286484241485596
Batch 29/64 loss: 0.01066434383392334
Batch 30/64 loss: -0.0022638440132141113
Batch 31/64 loss: -0.0051590800285339355
Batch 32/64 loss: -0.009164988994598389
Batch 33/64 loss: -0.013448655605316162
Batch 34/64 loss: -0.01847970485687256
Batch 35/64 loss: -0.020892441272735596
Batch 36/64 loss: 0.009737908840179443
Batch 37/64 loss: -0.008924603462219238
Batch 38/64 loss: -0.016584932804107666
Batch 39/64 loss: -0.01965707540512085
Batch 40/64 loss: -0.01921314001083374
Batch 41/64 loss: -0.02524852752685547
Batch 42/64 loss: -0.00669330358505249
Batch 43/64 loss: -0.007778048515319824
Batch 44/64 loss: -0.030693650245666504
Batch 45/64 loss: -0.015372753143310547
Batch 46/64 loss: 0.015927374362945557
Batch 47/64 loss: -0.021131813526153564
Batch 48/64 loss: -0.004370391368865967
Batch 49/64 loss: 0.022669076919555664
Batch 50/64 loss: -0.01108551025390625
Batch 51/64 loss: 0.0017679333686828613
Batch 52/64 loss: -0.022986888885498047
Batch 53/64 loss: -0.01532447338104248
Batch 54/64 loss: -0.01347494125366211
Batch 55/64 loss: -0.007480740547180176
Batch 56/64 loss: -0.021786093711853027
Batch 57/64 loss: -0.027595877647399902
Batch 58/64 loss: -0.028325915336608887
Batch 59/64 loss: 0.004180192947387695
Batch 60/64 loss: -0.013524830341339111
Batch 61/64 loss: -0.015268683433532715
Batch 62/64 loss: -0.02232050895690918
Batch 63/64 loss: -0.012174487113952637
Batch 64/64 loss: -0.014539122581481934
Epoch 60  Train loss: -0.009920001964943082  Val loss: 0.011896876739882111
Saving best model, epoch: 60
Epoch 61
-------------------------------
Batch 1/64 loss: -0.00937187671661377
Batch 2/64 loss: 0.00036537647247314453
Batch 3/64 loss: -0.02798604965209961
Batch 4/64 loss: 0.00865185260772705
Batch 5/64 loss: -0.009454011917114258
Batch 6/64 loss: -0.005004405975341797
Batch 7/64 loss: -0.00033396482467651367
Batch 8/64 loss: -0.0063433051109313965
Batch 9/64 loss: -0.01837480068206787
Batch 10/64 loss: -0.017767369747161865
Batch 11/64 loss: -0.013779163360595703
Batch 12/64 loss: -0.019653916358947754
Batch 13/64 loss: 0.006748557090759277
Batch 14/64 loss: -0.018696188926696777
Batch 15/64 loss: -0.031445443630218506
Batch 16/64 loss: -0.018359482288360596
Batch 17/64 loss: -0.013941407203674316
Batch 18/64 loss: -0.0018962621688842773
Batch 19/64 loss: -0.006719470024108887
Batch 20/64 loss: -0.014011383056640625
Batch 21/64 loss: 0.0038245320320129395
Batch 22/64 loss: -0.018974363803863525
Batch 23/64 loss: -0.000890195369720459
Batch 24/64 loss: -0.01050037145614624
Batch 25/64 loss: -0.012093961238861084
Batch 26/64 loss: -0.013053417205810547
Batch 27/64 loss: 0.007156252861022949
Batch 28/64 loss: 0.005192279815673828
Batch 29/64 loss: -0.009087085723876953
Batch 30/64 loss: 0.021962404251098633
Batch 31/64 loss: -0.0001341104507446289
Batch 32/64 loss: 0.0013628602027893066
Batch 33/64 loss: -0.008929073810577393
Batch 34/64 loss: -0.01628977060317993
Batch 35/64 loss: -0.011908888816833496
Batch 36/64 loss: 0.0009764432907104492
Batch 37/64 loss: 0.01907169818878174
Batch 38/64 loss: -0.022504687309265137
Batch 39/64 loss: -0.0070105791091918945
Batch 40/64 loss: -0.011174976825714111
Batch 41/64 loss: -0.005257964134216309
Batch 42/64 loss: -0.02965158224105835
Batch 43/64 loss: -0.017806589603424072
Batch 44/64 loss: -0.02389204502105713
Batch 45/64 loss: 0.018062829971313477
Batch 46/64 loss: -0.019444942474365234
Batch 47/64 loss: 0.015102207660675049
Batch 48/64 loss: 0.003507673740386963
Batch 49/64 loss: -0.025888919830322266
Batch 50/64 loss: -0.03364980220794678
Batch 51/64 loss: -0.001033782958984375
Batch 52/64 loss: -0.007303059101104736
Batch 53/64 loss: -0.007699072360992432
Batch 54/64 loss: -0.002568840980529785
Batch 55/64 loss: -0.010703802108764648
Batch 56/64 loss: -0.0026640892028808594
Batch 57/64 loss: 0.0021638870239257812
Batch 58/64 loss: -0.013861238956451416
Batch 59/64 loss: -0.022132158279418945
Batch 60/64 loss: -0.014535784721374512
Batch 61/64 loss: -0.009876012802124023
Batch 62/64 loss: -0.0036647915840148926
Batch 63/64 loss: 0.006791949272155762
Batch 64/64 loss: -0.018674731254577637
Epoch 61  Train loss: -0.008162975778766707  Val loss: 0.014339217615291425
Epoch 62
-------------------------------
Batch 1/64 loss: -0.029558897018432617
Batch 2/64 loss: -0.028479337692260742
Batch 3/64 loss: -0.009183287620544434
Batch 4/64 loss: -0.03192168474197388
Batch 5/64 loss: 0.025379955768585205
Batch 6/64 loss: -0.008916258811950684
Batch 7/64 loss: -0.0055124759674072266
Batch 8/64 loss: -0.026611626148223877
Batch 9/64 loss: -0.03530073165893555
Batch 10/64 loss: -0.022507131099700928
Batch 11/64 loss: -0.005917608737945557
Batch 12/64 loss: -0.007225334644317627
Batch 13/64 loss: -0.02169722318649292
Batch 14/64 loss: -0.010226726531982422
Batch 15/64 loss: -0.00842583179473877
Batch 16/64 loss: -0.013969838619232178
Batch 17/64 loss: -0.010402917861938477
Batch 18/64 loss: -0.035166382789611816
Batch 19/64 loss: 0.0006949901580810547
Batch 20/64 loss: -0.020586788654327393
Batch 21/64 loss: -0.025465309619903564
Batch 22/64 loss: -0.01426088809967041
Batch 23/64 loss: -0.015406489372253418
Batch 24/64 loss: 0.006112337112426758
Batch 25/64 loss: -0.0066149234771728516
Batch 26/64 loss: -0.02063077688217163
Batch 27/64 loss: 0.002461552619934082
Batch 28/64 loss: -0.0020323991775512695
Batch 29/64 loss: -0.018329620361328125
Batch 30/64 loss: 0.00012540817260742188
Batch 31/64 loss: -0.004311323165893555
Batch 32/64 loss: -0.01710379123687744
Batch 33/64 loss: -0.008633911609649658
Batch 34/64 loss: -0.013569414615631104
Batch 35/64 loss: -0.010069966316223145
Batch 36/64 loss: 0.004662871360778809
Batch 37/64 loss: -0.010767042636871338
Batch 38/64 loss: -0.01048976182937622
Batch 39/64 loss: -0.010609745979309082
Batch 40/64 loss: 0.0022058486938476562
Batch 41/64 loss: 0.015056192874908447
Batch 42/64 loss: -0.02535301446914673
Batch 43/64 loss: 0.0038499832153320312
Batch 44/64 loss: -0.020568907260894775
Batch 45/64 loss: -0.01398378610610962
Batch 46/64 loss: -0.01134943962097168
Batch 47/64 loss: 0.0015766620635986328
Batch 48/64 loss: 0.029950737953186035
Batch 49/64 loss: -0.03471577167510986
Batch 50/64 loss: -0.013821005821228027
Batch 51/64 loss: -0.01155233383178711
Batch 52/64 loss: -0.011742353439331055
Batch 53/64 loss: -0.003150463104248047
Batch 54/64 loss: 0.03235960006713867
Batch 55/64 loss: -0.025082647800445557
Batch 56/64 loss: 0.009361743927001953
Batch 57/64 loss: -0.021891534328460693
Batch 58/64 loss: 0.0013042092323303223
Batch 59/64 loss: -0.008540868759155273
Batch 60/64 loss: -0.010368883609771729
Batch 61/64 loss: -0.009253084659576416
Batch 62/64 loss: -0.020504772663116455
Batch 63/64 loss: -0.02179861068725586
Batch 64/64 loss: -0.021113157272338867
Epoch 62  Train loss: -0.010420638439702052  Val loss: 0.02331690177884708
Epoch 63
-------------------------------
Batch 1/64 loss: -0.01275634765625
Batch 2/64 loss: 0.005695164203643799
Batch 3/64 loss: -0.003918766975402832
Batch 4/64 loss: -0.003432750701904297
Batch 5/64 loss: -0.01694965362548828
Batch 6/64 loss: -0.026739835739135742
Batch 7/64 loss: -0.019285202026367188
Batch 8/64 loss: 0.01543569564819336
Batch 9/64 loss: 0.004867494106292725
Batch 10/64 loss: 0.005503177642822266
Batch 11/64 loss: -0.02038860321044922
Batch 12/64 loss: 0.008807480335235596
Batch 13/64 loss: -0.00785917043685913
Batch 14/64 loss: -0.022052764892578125
Batch 15/64 loss: -0.012393474578857422
Batch 16/64 loss: -0.019723474979400635
Batch 17/64 loss: -0.016407132148742676
Batch 18/64 loss: 0.011151611804962158
Batch 19/64 loss: -0.01279228925704956
Batch 20/64 loss: -0.017566919326782227
Batch 21/64 loss: -0.0038671493530273438
Batch 22/64 loss: -0.016960084438323975
Batch 23/64 loss: -0.013187885284423828
Batch 24/64 loss: 0.014112472534179688
Batch 25/64 loss: -0.02673196792602539
Batch 26/64 loss: 0.00011241436004638672
Batch 27/64 loss: -0.016215205192565918
Batch 28/64 loss: 0.002681910991668701
Batch 29/64 loss: -0.0069048404693603516
Batch 30/64 loss: -0.016853153705596924
Batch 31/64 loss: 0.0020290613174438477
Batch 32/64 loss: -0.01229161024093628
Batch 33/64 loss: -0.009982645511627197
Batch 34/64 loss: -0.03529942035675049
Batch 35/64 loss: -0.009207606315612793
Batch 36/64 loss: 0.016857385635375977
Batch 37/64 loss: -0.019789695739746094
Batch 38/64 loss: -0.005781888961791992
Batch 39/64 loss: -0.027840495109558105
Batch 40/64 loss: -0.026674866676330566
Batch 41/64 loss: -0.01746898889541626
Batch 42/64 loss: -0.003995418548583984
Batch 43/64 loss: -0.002115488052368164
Batch 44/64 loss: -0.0020527243614196777
Batch 45/64 loss: -0.009382069110870361
Batch 46/64 loss: -0.011891484260559082
Batch 47/64 loss: -0.01673591136932373
Batch 48/64 loss: -0.01594918966293335
Batch 49/64 loss: -0.008720636367797852
Batch 50/64 loss: -0.00041484832763671875
Batch 51/64 loss: -0.0025679469108581543
Batch 52/64 loss: -0.0009740591049194336
Batch 53/64 loss: -0.0029500722885131836
Batch 54/64 loss: -0.022387385368347168
Batch 55/64 loss: -0.02526038885116577
Batch 56/64 loss: -0.011584103107452393
Batch 57/64 loss: -0.028232812881469727
Batch 58/64 loss: -0.025075793266296387
Batch 59/64 loss: 0.004126787185668945
Batch 60/64 loss: -0.009678900241851807
Batch 61/64 loss: -0.009524822235107422
Batch 62/64 loss: -0.010954022407531738
Batch 63/64 loss: -0.014005422592163086
Batch 64/64 loss: -0.015084028244018555
Epoch 63  Train loss: -0.009909172619090361  Val loss: 0.015037105255520222
Epoch 64
-------------------------------
Batch 1/64 loss: -0.0032515525817871094
Batch 2/64 loss: -0.01523822546005249
Batch 3/64 loss: -0.015750527381896973
Batch 4/64 loss: -0.04503202438354492
Batch 5/64 loss: -0.0021616220474243164
Batch 6/64 loss: -0.015308380126953125
Batch 7/64 loss: -0.021291136741638184
Batch 8/64 loss: -0.016979694366455078
Batch 9/64 loss: -0.01437067985534668
Batch 10/64 loss: -0.020526111125946045
Batch 11/64 loss: -0.018286585807800293
Batch 12/64 loss: -0.0028834939002990723
Batch 13/64 loss: -0.02894383668899536
Batch 14/64 loss: 0.0037738680839538574
Batch 15/64 loss: -0.024074077606201172
Batch 16/64 loss: -0.011049509048461914
Batch 17/64 loss: 0.0036891698837280273
Batch 18/64 loss: -0.03026902675628662
Batch 19/64 loss: -0.007606863975524902
Batch 20/64 loss: -0.019390404224395752
Batch 21/64 loss: -0.005390942096710205
Batch 22/64 loss: -0.017117977142333984
Batch 23/64 loss: -0.013430953025817871
Batch 24/64 loss: -0.010728955268859863
Batch 25/64 loss: -0.02013242244720459
Batch 26/64 loss: 0.00047200918197631836
Batch 27/64 loss: -0.021197915077209473
Batch 28/64 loss: -0.03769046068191528
Batch 29/64 loss: -0.026772379875183105
Batch 30/64 loss: -0.02838677167892456
Batch 31/64 loss: -0.023146331310272217
Batch 32/64 loss: -0.01130145788192749
Batch 33/64 loss: -0.023839473724365234
Batch 34/64 loss: -0.01367729902267456
Batch 35/64 loss: -0.03178238868713379
Batch 36/64 loss: -0.0049547553062438965
Batch 37/64 loss: -0.023048877716064453
Batch 38/64 loss: -0.02192068099975586
Batch 39/64 loss: -0.025726675987243652
Batch 40/64 loss: -0.015953421592712402
Batch 41/64 loss: -0.029707670211791992
Batch 42/64 loss: -0.019921183586120605
Batch 43/64 loss: 0.017130792140960693
Batch 44/64 loss: 0.0010338425636291504
Batch 45/64 loss: -0.009400904178619385
Batch 46/64 loss: -0.0007698535919189453
Batch 47/64 loss: -0.008373618125915527
Batch 48/64 loss: -0.01733100414276123
Batch 49/64 loss: -0.02968013286590576
Batch 50/64 loss: -0.02144014835357666
Batch 51/64 loss: -0.026566922664642334
Batch 52/64 loss: -0.014767467975616455
Batch 53/64 loss: -0.014290869235992432
Batch 54/64 loss: -0.02442300319671631
Batch 55/64 loss: -0.01871359348297119
Batch 56/64 loss: -0.024365484714508057
Batch 57/64 loss: 0.006951451301574707
Batch 58/64 loss: -0.01641899347305298
Batch 59/64 loss: -0.010860860347747803
Batch 60/64 loss: -0.0017050504684448242
Batch 61/64 loss: -0.015140712261199951
Batch 62/64 loss: -0.01916801929473877
Batch 63/64 loss: -0.0045392513275146484
Batch 64/64 loss: 0.008756875991821289
Epoch 64  Train loss: -0.015318899528653014  Val loss: 0.015916632622787634
Epoch 65
-------------------------------
Batch 1/64 loss: -0.016790807247161865
Batch 2/64 loss: -0.018279552459716797
Batch 3/64 loss: -0.012847959995269775
Batch 4/64 loss: 0.0068550705909729
Batch 5/64 loss: -0.01366966962814331
Batch 6/64 loss: -0.01305854320526123
Batch 7/64 loss: -0.02951967716217041
Batch 8/64 loss: -0.02608954906463623
Batch 9/64 loss: -0.016118645668029785
Batch 10/64 loss: -0.018522918224334717
Batch 11/64 loss: -0.015130162239074707
Batch 12/64 loss: -0.023850560188293457
Batch 13/64 loss: -0.02779972553253174
Batch 14/64 loss: -0.024757742881774902
Batch 15/64 loss: -0.005401194095611572
Batch 16/64 loss: -0.01852196455001831
Batch 17/64 loss: -0.012542009353637695
Batch 18/64 loss: -0.01956033706665039
Batch 19/64 loss: -0.031810462474823
Batch 20/64 loss: -0.018836379051208496
Batch 21/64 loss: -0.01874840259552002
Batch 22/64 loss: -0.02760028839111328
Batch 23/64 loss: -0.011345505714416504
Batch 24/64 loss: -0.03394150733947754
Batch 25/64 loss: -0.01576179265975952
Batch 26/64 loss: -0.027339577674865723
Batch 27/64 loss: -0.008657395839691162
Batch 28/64 loss: -0.015463113784790039
Batch 29/64 loss: -0.012103497982025146
Batch 30/64 loss: -0.005283236503601074
Batch 31/64 loss: -0.016987204551696777
Batch 32/64 loss: -0.004829704761505127
Batch 33/64 loss: -0.01771986484527588
Batch 34/64 loss: 0.008478164672851562
Batch 35/64 loss: -0.015012681484222412
Batch 36/64 loss: -0.031078815460205078
Batch 37/64 loss: -0.02010399103164673
Batch 38/64 loss: -0.010476350784301758
Batch 39/64 loss: -0.02724623680114746
Batch 40/64 loss: 0.0023522377014160156
Batch 41/64 loss: 0.019808053970336914
Batch 42/64 loss: -0.021892547607421875
Batch 43/64 loss: -0.0125199556350708
Batch 44/64 loss: -0.0022493600845336914
Batch 45/64 loss: -0.03372037410736084
Batch 46/64 loss: -0.02290785312652588
Batch 47/64 loss: -0.009667396545410156
Batch 48/64 loss: -0.03016817569732666
Batch 49/64 loss: -0.020993530750274658
Batch 50/64 loss: 0.008306503295898438
Batch 51/64 loss: -0.020464062690734863
Batch 52/64 loss: -0.028304338455200195
Batch 53/64 loss: -0.0023630261421203613
Batch 54/64 loss: 0.008605718612670898
Batch 55/64 loss: -0.032975971698760986
Batch 56/64 loss: -0.008599340915679932
Batch 57/64 loss: -0.017283499240875244
Batch 58/64 loss: 0.0025586485862731934
Batch 59/64 loss: -0.0015437006950378418
Batch 60/64 loss: -0.007664799690246582
Batch 61/64 loss: -0.012392759323120117
Batch 62/64 loss: -0.02371281385421753
Batch 63/64 loss: 0.0046443939208984375
Batch 64/64 loss: 0.022001564502716064
Epoch 65  Train loss: -0.014339146193336039  Val loss: 0.012798094462692942
Epoch 66
-------------------------------
Batch 1/64 loss: -0.011606276035308838
Batch 2/64 loss: -0.027160942554473877
Batch 3/64 loss: -0.015790343284606934
Batch 4/64 loss: -0.017774105072021484
Batch 5/64 loss: -0.028299272060394287
Batch 6/64 loss: -0.005217075347900391
Batch 7/64 loss: -0.022983074188232422
Batch 8/64 loss: -0.004013419151306152
Batch 9/64 loss: -0.0036640167236328125
Batch 10/64 loss: -0.007164597511291504
Batch 11/64 loss: -0.019353389739990234
Batch 12/64 loss: -0.023273348808288574
Batch 13/64 loss: -0.012877166271209717
Batch 14/64 loss: -0.005411803722381592
Batch 15/64 loss: -0.006795644760131836
Batch 16/64 loss: 0.008020341396331787
Batch 17/64 loss: -0.014619529247283936
Batch 18/64 loss: -0.03235435485839844
Batch 19/64 loss: -0.010869264602661133
Batch 20/64 loss: 0.0033499598503112793
Batch 21/64 loss: -0.020579218864440918
Batch 22/64 loss: 0.008122086524963379
Batch 23/64 loss: -0.017597615718841553
Batch 24/64 loss: -0.020557522773742676
Batch 25/64 loss: 0.016759634017944336
Batch 26/64 loss: -0.012348711490631104
Batch 27/64 loss: -0.02460479736328125
Batch 28/64 loss: -0.02973729372024536
Batch 29/64 loss: -0.007550597190856934
Batch 30/64 loss: -0.012125611305236816
Batch 31/64 loss: -0.014796137809753418
Batch 32/64 loss: 0.0037169456481933594
Batch 33/64 loss: 0.0008045434951782227
Batch 34/64 loss: -0.01284027099609375
Batch 35/64 loss: -0.012910187244415283
Batch 36/64 loss: 0.006405651569366455
Batch 37/64 loss: -0.019470393657684326
Batch 38/64 loss: -0.011756956577301025
Batch 39/64 loss: -0.011174380779266357
Batch 40/64 loss: -0.0030243396759033203
Batch 41/64 loss: -0.009213805198669434
Batch 42/64 loss: -0.014288544654846191
Batch 43/64 loss: -0.016677916049957275
Batch 44/64 loss: -0.017861247062683105
Batch 45/64 loss: -0.022263944149017334
Batch 46/64 loss: -0.01702594757080078
Batch 47/64 loss: -0.010452508926391602
Batch 48/64 loss: -0.017188608646392822
Batch 49/64 loss: -0.02349907159805298
Batch 50/64 loss: -0.023831546306610107
Batch 51/64 loss: -0.01202237606048584
Batch 52/64 loss: -0.03243643045425415
Batch 53/64 loss: -0.032815396785736084
Batch 54/64 loss: 0.005953550338745117
Batch 55/64 loss: -0.02273350954055786
Batch 56/64 loss: -0.02069568634033203
Batch 57/64 loss: -0.009483575820922852
Batch 58/64 loss: -0.023118674755096436
Batch 59/64 loss: -0.019080758094787598
Batch 60/64 loss: -0.009958744049072266
Batch 61/64 loss: -0.018378734588623047
Batch 62/64 loss: -0.01475071907043457
Batch 63/64 loss: -0.0026190876960754395
Batch 64/64 loss: -0.018603146076202393
Epoch 66  Train loss: -0.013357147282245112  Val loss: 0.014582543848306453
Epoch 67
-------------------------------
Batch 1/64 loss: -0.025712013244628906
Batch 2/64 loss: -0.026887178421020508
Batch 3/64 loss: -0.024782836437225342
Batch 4/64 loss: -0.023661017417907715
Batch 5/64 loss: -0.015241801738739014
Batch 6/64 loss: -0.023224353790283203
Batch 7/64 loss: -0.02351844310760498
Batch 8/64 loss: -0.030738651752471924
Batch 9/64 loss: -0.013735473155975342
Batch 10/64 loss: -0.007579326629638672
Batch 11/64 loss: -0.02740567922592163
Batch 12/64 loss: -0.020616590976715088
Batch 13/64 loss: -0.004745125770568848
Batch 14/64 loss: -0.030593812465667725
Batch 15/64 loss: -0.008101999759674072
Batch 16/64 loss: -0.02342665195465088
Batch 17/64 loss: -0.019913911819458008
Batch 18/64 loss: -0.020726323127746582
Batch 19/64 loss: -0.007239103317260742
Batch 20/64 loss: -0.02010709047317505
Batch 21/64 loss: -0.0002327561378479004
Batch 22/64 loss: -0.015697479248046875
Batch 23/64 loss: -0.019742250442504883
Batch 24/64 loss: -0.0063117146492004395
Batch 25/64 loss: -0.03442692756652832
Batch 26/64 loss: -0.025048434734344482
Batch 27/64 loss: -0.031568944454193115
Batch 28/64 loss: 0.011341094970703125
Batch 29/64 loss: -0.008749961853027344
Batch 30/64 loss: -0.02111262083053589
Batch 31/64 loss: -0.014469742774963379
Batch 32/64 loss: -0.013736605644226074
Batch 33/64 loss: -0.034877896308898926
Batch 34/64 loss: -0.03599214553833008
Batch 35/64 loss: -0.027727186679840088
Batch 36/64 loss: -0.021695494651794434
Batch 37/64 loss: -0.007828176021575928
Batch 38/64 loss: -0.04149395227432251
Batch 39/64 loss: -0.02903813123703003
Batch 40/64 loss: -0.011231064796447754
Batch 41/64 loss: -0.02501070499420166
Batch 42/64 loss: -0.012390613555908203
Batch 43/64 loss: 0.005584239959716797
Batch 44/64 loss: -0.024243593215942383
Batch 45/64 loss: -0.009654045104980469
Batch 46/64 loss: -0.009687840938568115
Batch 47/64 loss: -0.023190736770629883
Batch 48/64 loss: 0.007058322429656982
Batch 49/64 loss: -0.008380115032196045
Batch 50/64 loss: -0.0009150505065917969
Batch 51/64 loss: 0.0035260915756225586
Batch 52/64 loss: 0.0059743523597717285
Batch 53/64 loss: -0.027949213981628418
Batch 54/64 loss: 0.005770683288574219
Batch 55/64 loss: -0.01658928394317627
Batch 56/64 loss: -0.021191835403442383
Batch 57/64 loss: 0.005755066871643066
Batch 58/64 loss: -0.022724032402038574
Batch 59/64 loss: -0.02425593137741089
Batch 60/64 loss: -0.023808419704437256
Batch 61/64 loss: -0.03344792127609253
Batch 62/64 loss: -0.030384063720703125
Batch 63/64 loss: -0.016736984252929688
Batch 64/64 loss: 0.03442811965942383
Epoch 67  Train loss: -0.01660656181036257  Val loss: 0.009197846515891478
Saving best model, epoch: 67
Epoch 68
-------------------------------
Batch 1/64 loss: -0.008114814758300781
Batch 2/64 loss: -0.028484642505645752
Batch 3/64 loss: -0.040732622146606445
Batch 4/64 loss: -0.03669393062591553
Batch 5/64 loss: -0.0134507417678833
Batch 6/64 loss: -0.008659720420837402
Batch 7/64 loss: 0.003911137580871582
Batch 8/64 loss: -0.02107059955596924
Batch 9/64 loss: 0.00090789794921875
Batch 10/64 loss: -0.0010533332824707031
Batch 11/64 loss: -0.013003408908843994
Batch 12/64 loss: -0.022808551788330078
Batch 13/64 loss: -0.022117018699645996
Batch 14/64 loss: -0.018484890460968018
Batch 15/64 loss: 0.004247426986694336
Batch 16/64 loss: -0.014817118644714355
Batch 17/64 loss: -0.03471583127975464
Batch 18/64 loss: -0.02782154083251953
Batch 19/64 loss: -0.024363696575164795
Batch 20/64 loss: -0.019006013870239258
Batch 21/64 loss: -0.0017201900482177734
Batch 22/64 loss: -0.011822998523712158
Batch 23/64 loss: -0.0035905838012695312
Batch 24/64 loss: -0.0176541805267334
Batch 25/64 loss: -0.02933955192565918
Batch 26/64 loss: -0.016382157802581787
Batch 27/64 loss: -0.0074547529220581055
Batch 28/64 loss: -0.015843510627746582
Batch 29/64 loss: -0.007635295391082764
Batch 30/64 loss: -0.02373427152633667
Batch 31/64 loss: -0.04465550184249878
Batch 32/64 loss: 0.007453501224517822
Batch 33/64 loss: -0.017571330070495605
Batch 34/64 loss: -0.01693028211593628
Batch 35/64 loss: -0.028744280338287354
Batch 36/64 loss: -0.00585627555847168
Batch 37/64 loss: -0.005186498165130615
Batch 38/64 loss: -0.020208239555358887
Batch 39/64 loss: -0.02365189790725708
Batch 40/64 loss: -0.01835179328918457
Batch 41/64 loss: -0.015125393867492676
Batch 42/64 loss: -0.025923490524291992
Batch 43/64 loss: -0.021048009395599365
Batch 44/64 loss: -0.04095900058746338
Batch 45/64 loss: -0.0025342702865600586
Batch 46/64 loss: -0.016196131706237793
Batch 47/64 loss: -0.002408266067504883
Batch 48/64 loss: -0.027759909629821777
Batch 49/64 loss: -0.018695294857025146
Batch 50/64 loss: -0.022497057914733887
Batch 51/64 loss: -0.022978365421295166
Batch 52/64 loss: -0.029087543487548828
Batch 53/64 loss: -0.017891764640808105
Batch 54/64 loss: -0.01869022846221924
Batch 55/64 loss: -0.032910048961639404
Batch 56/64 loss: -0.02869284152984619
Batch 57/64 loss: -0.01601952314376831
Batch 58/64 loss: -0.01042330265045166
Batch 59/64 loss: -0.0038764476776123047
Batch 60/64 loss: -0.014337241649627686
Batch 61/64 loss: -0.026714324951171875
Batch 62/64 loss: -0.013040900230407715
Batch 63/64 loss: -0.010951042175292969
Batch 64/64 loss: -0.0017277002334594727
Epoch 68  Train loss: -0.017180678891200646  Val loss: 0.008246023835185468
Saving best model, epoch: 68
Epoch 69
-------------------------------
Batch 1/64 loss: -0.009157776832580566
Batch 2/64 loss: -0.009001970291137695
Batch 3/64 loss: -0.02553349733352661
Batch 4/64 loss: -0.0010170936584472656
Batch 5/64 loss: -0.003651440143585205
Batch 6/64 loss: -0.016180336475372314
Batch 7/64 loss: -0.0031552910804748535
Batch 8/64 loss: -0.03443562984466553
Batch 9/64 loss: -0.020293056964874268
Batch 10/64 loss: -0.015141546726226807
Batch 11/64 loss: -0.032889366149902344
Batch 12/64 loss: -0.006357789039611816
Batch 13/64 loss: -0.016026794910430908
Batch 14/64 loss: -0.0056418776512146
Batch 15/64 loss: -0.03748476505279541
Batch 16/64 loss: -0.026641547679901123
Batch 17/64 loss: 0.01242673397064209
Batch 18/64 loss: -0.04591643810272217
Batch 19/64 loss: -0.008764922618865967
Batch 20/64 loss: -0.03679454326629639
Batch 21/64 loss: -0.030142486095428467
Batch 22/64 loss: -0.023684203624725342
Batch 23/64 loss: -0.021947801113128662
Batch 24/64 loss: -0.0017563104629516602
Batch 25/64 loss: -0.023087739944458008
Batch 26/64 loss: -0.008032798767089844
Batch 27/64 loss: -0.00922691822052002
Batch 28/64 loss: -0.02408897876739502
Batch 29/64 loss: -0.03213244676589966
Batch 30/64 loss: -0.009573936462402344
Batch 31/64 loss: -0.01297760009765625
Batch 32/64 loss: -0.01361459493637085
Batch 33/64 loss: -0.028695762157440186
Batch 34/64 loss: -0.013172507286071777
Batch 35/64 loss: -0.03920036554336548
Batch 36/64 loss: -0.011037945747375488
Batch 37/64 loss: 0.008927643299102783
Batch 38/64 loss: -0.022246599197387695
Batch 39/64 loss: -0.019968092441558838
Batch 40/64 loss: -0.026282668113708496
Batch 41/64 loss: -0.021027863025665283
Batch 42/64 loss: -0.029634535312652588
Batch 43/64 loss: -0.014404058456420898
Batch 44/64 loss: -0.01190108060836792
Batch 45/64 loss: -0.008803784847259521
Batch 46/64 loss: -0.037760138511657715
Batch 47/64 loss: -0.016323328018188477
Batch 48/64 loss: -0.02879863977432251
Batch 49/64 loss: -0.00027501583099365234
Batch 50/64 loss: -0.015737712383270264
Batch 51/64 loss: -0.009462296962738037
Batch 52/64 loss: -0.01687610149383545
Batch 53/64 loss: -0.0063974857330322266
Batch 54/64 loss: -0.008809089660644531
Batch 55/64 loss: -0.018783211708068848
Batch 56/64 loss: -0.026737570762634277
Batch 57/64 loss: 0.003426849842071533
Batch 58/64 loss: 0.0024469494819641113
Batch 59/64 loss: -0.02464085817337036
Batch 60/64 loss: -0.022059321403503418
Batch 61/64 loss: 0.0011875033378601074
Batch 62/64 loss: 0.0034034252166748047
Batch 63/64 loss: -0.0023870468139648438
Batch 64/64 loss: -0.014261424541473389
Epoch 69  Train loss: -0.01607296536950504  Val loss: 0.01232667600166347
Epoch 70
-------------------------------
Batch 1/64 loss: -0.01871204376220703
Batch 2/64 loss: 0.012283027172088623
Batch 3/64 loss: -0.025357961654663086
Batch 4/64 loss: -0.024783670902252197
Batch 5/64 loss: -0.002297639846801758
Batch 6/64 loss: -0.04459106922149658
Batch 7/64 loss: -0.019236445426940918
Batch 8/64 loss: -0.020255327224731445
Batch 9/64 loss: -0.031412601470947266
Batch 10/64 loss: -0.006942152976989746
Batch 11/64 loss: -0.019009292125701904
Batch 12/64 loss: -0.019013047218322754
Batch 13/64 loss: -0.004226982593536377
Batch 14/64 loss: -0.02089005708694458
Batch 15/64 loss: -0.015454232692718506
Batch 16/64 loss: -0.03617823123931885
Batch 17/64 loss: -0.010707259178161621
Batch 18/64 loss: -0.040247321128845215
Batch 19/64 loss: -0.018189191818237305
Batch 20/64 loss: -0.006757378578186035
Batch 21/64 loss: -0.013343214988708496
Batch 22/64 loss: -0.016078174114227295
Batch 23/64 loss: -0.01726984977722168
Batch 24/64 loss: -0.02555137872695923
Batch 25/64 loss: -0.00720667839050293
Batch 26/64 loss: -0.04398477077484131
Batch 27/64 loss: 0.0031508803367614746
Batch 28/64 loss: -0.03307831287384033
Batch 29/64 loss: -0.0333591103553772
Batch 30/64 loss: -0.022821128368377686
Batch 31/64 loss: 0.01401972770690918
Batch 32/64 loss: -0.018742382526397705
Batch 33/64 loss: -0.0032137632369995117
Batch 34/64 loss: -0.02247697114944458
Batch 35/64 loss: -0.031993865966796875
Batch 36/64 loss: -0.02824312448501587
Batch 37/64 loss: -0.02567744255065918
Batch 38/64 loss: -0.014511585235595703
Batch 39/64 loss: -0.017113089561462402
Batch 40/64 loss: -0.02049499750137329
Batch 41/64 loss: -0.020104169845581055
Batch 42/64 loss: -0.007684111595153809
Batch 43/64 loss: -0.025132477283477783
Batch 44/64 loss: -0.03158974647521973
Batch 45/64 loss: -0.03199362754821777
Batch 46/64 loss: -0.006461381912231445
Batch 47/64 loss: -0.04086858034133911
Batch 48/64 loss: -0.04270124435424805
Batch 49/64 loss: 0.00928729772567749
Batch 50/64 loss: -0.02474665641784668
Batch 51/64 loss: -0.021158814430236816
Batch 52/64 loss: -0.009031891822814941
Batch 53/64 loss: -0.027390480041503906
Batch 54/64 loss: -0.010759890079498291
Batch 55/64 loss: -0.011110544204711914
Batch 56/64 loss: -0.01233971118927002
Batch 57/64 loss: 0.0034379959106445312
Batch 58/64 loss: -0.004367232322692871
Batch 59/64 loss: -0.023700714111328125
Batch 60/64 loss: 0.02179265022277832
Batch 61/64 loss: -0.02851724624633789
Batch 62/64 loss: -0.03167450428009033
Batch 63/64 loss: -0.005551576614379883
Batch 64/64 loss: -0.03732961416244507
Epoch 70  Train loss: -0.01820128595127779  Val loss: 0.004940600944138884
Saving best model, epoch: 70
Epoch 71
-------------------------------
Batch 1/64 loss: -0.030432462692260742
Batch 2/64 loss: -0.03530532121658325
Batch 3/64 loss: -0.01889580488204956
Batch 4/64 loss: -0.02571946382522583
Batch 5/64 loss: -0.0016443133354187012
Batch 6/64 loss: -0.03556239604949951
Batch 7/64 loss: -0.030123651027679443
Batch 8/64 loss: -0.018688678741455078
Batch 9/64 loss: -0.03405773639678955
Batch 10/64 loss: 0.003522515296936035
Batch 11/64 loss: -0.022236347198486328
Batch 12/64 loss: -0.011853218078613281
Batch 13/64 loss: -0.0021761059761047363
Batch 14/64 loss: 0.009132087230682373
Batch 15/64 loss: -0.018212199211120605
Batch 16/64 loss: -0.036007583141326904
Batch 17/64 loss: -0.010162949562072754
Batch 18/64 loss: -0.028366565704345703
Batch 19/64 loss: -0.020478010177612305
Batch 20/64 loss: -0.006041049957275391
Batch 21/64 loss: -0.019120514392852783
Batch 22/64 loss: -0.007417500019073486
Batch 23/64 loss: -0.01791536808013916
Batch 24/64 loss: -0.02375560998916626
Batch 25/64 loss: -0.02852606773376465
Batch 26/64 loss: 0.012956738471984863
Batch 27/64 loss: 0.017387986183166504
Batch 28/64 loss: -0.016923606395721436
Batch 29/64 loss: -0.010017991065979004
Batch 30/64 loss: -0.018515169620513916
Batch 31/64 loss: -0.01346886157989502
Batch 32/64 loss: -0.03278553485870361
Batch 33/64 loss: -0.03045707941055298
Batch 34/64 loss: -0.01604592800140381
Batch 35/64 loss: -0.03378647565841675
Batch 36/64 loss: -0.018783152103424072
Batch 37/64 loss: -0.02979940176010132
Batch 38/64 loss: -0.022958219051361084
Batch 39/64 loss: -0.015171825885772705
Batch 40/64 loss: -0.008063912391662598
Batch 41/64 loss: -0.038556694984436035
Batch 42/64 loss: -0.02495431900024414
Batch 43/64 loss: -0.027316510677337646
Batch 44/64 loss: -0.0268973708152771
Batch 45/64 loss: -0.007468163967132568
Batch 46/64 loss: -0.03659355640411377
Batch 47/64 loss: -0.0028396248817443848
Batch 48/64 loss: -0.016065537929534912
Batch 49/64 loss: -0.04286515712738037
Batch 50/64 loss: -0.03239339590072632
Batch 51/64 loss: -0.019042491912841797
Batch 52/64 loss: -0.020269155502319336
Batch 53/64 loss: -0.017859280109405518
Batch 54/64 loss: -0.03020501136779785
Batch 55/64 loss: -0.0004360675811767578
Batch 56/64 loss: -0.02366805076599121
Batch 57/64 loss: -0.01800912618637085
Batch 58/64 loss: -0.024803519248962402
Batch 59/64 loss: -0.006680011749267578
Batch 60/64 loss: -0.01521909236907959
Batch 61/64 loss: -0.021705031394958496
Batch 62/64 loss: -0.018618881702423096
Batch 63/64 loss: -0.021148204803466797
Batch 64/64 loss: -0.022439181804656982
Epoch 71  Train loss: -0.019088947305492328  Val loss: 0.009936477310469061
Epoch 72
-------------------------------
Batch 1/64 loss: -0.016022980213165283
Batch 2/64 loss: 0.0034672021865844727
Batch 3/64 loss: -0.034509897232055664
Batch 4/64 loss: -0.034467339515686035
Batch 5/64 loss: -0.03256654739379883
Batch 6/64 loss: -0.03958195447921753
Batch 7/64 loss: -0.03341662883758545
Batch 8/64 loss: -0.007654130458831787
Batch 9/64 loss: -0.014059066772460938
Batch 10/64 loss: -0.024564802646636963
Batch 11/64 loss: -0.03079366683959961
Batch 12/64 loss: -0.008491456508636475
Batch 13/64 loss: -0.02069377899169922
Batch 14/64 loss: -0.02569448947906494
Batch 15/64 loss: -0.023704230785369873
Batch 16/64 loss: -0.010491549968719482
Batch 17/64 loss: -0.010335564613342285
Batch 18/64 loss: -0.02051544189453125
Batch 19/64 loss: -0.028117775917053223
Batch 20/64 loss: -0.013239264488220215
Batch 21/64 loss: -0.018680214881896973
Batch 22/64 loss: -0.015508174896240234
Batch 23/64 loss: -0.025422215461730957
Batch 24/64 loss: -0.023275017738342285
Batch 25/64 loss: -0.03728902339935303
Batch 26/64 loss: -0.005886077880859375
Batch 27/64 loss: -0.03431379795074463
Batch 28/64 loss: -0.0245969295501709
Batch 29/64 loss: -0.023794472217559814
Batch 30/64 loss: -0.013263463973999023
Batch 31/64 loss: -0.03495281934738159
Batch 32/64 loss: -0.038124680519104004
Batch 33/64 loss: -0.02981436252593994
Batch 34/64 loss: -0.004245460033416748
Batch 35/64 loss: -0.023264646530151367
Batch 36/64 loss: -0.022664308547973633
Batch 37/64 loss: -0.030746638774871826
Batch 38/64 loss: -0.00867164134979248
Batch 39/64 loss: -0.006872057914733887
Batch 40/64 loss: -0.008460044860839844
Batch 41/64 loss: -0.03853428363800049
Batch 42/64 loss: -0.03282445669174194
Batch 43/64 loss: -0.020420968532562256
Batch 44/64 loss: -0.03328120708465576
Batch 45/64 loss: -0.01909559965133667
Batch 46/64 loss: -0.028520703315734863
Batch 47/64 loss: -0.006075739860534668
Batch 48/64 loss: -0.02207636833190918
Batch 49/64 loss: -0.014387547969818115
Batch 50/64 loss: -0.023225247859954834
Batch 51/64 loss: -0.029676318168640137
Batch 52/64 loss: -0.018656134605407715
Batch 53/64 loss: -0.020534932613372803
Batch 54/64 loss: -0.026670396327972412
Batch 55/64 loss: -0.020871520042419434
Batch 56/64 loss: -0.01694566011428833
Batch 57/64 loss: -0.01296144723892212
Batch 58/64 loss: -0.02801734209060669
Batch 59/64 loss: -0.02461111545562744
Batch 60/64 loss: -0.02783966064453125
Batch 61/64 loss: -0.002925395965576172
Batch 62/64 loss: -0.028329789638519287
Batch 63/64 loss: 0.00198972225189209
Batch 64/64 loss: -0.03812289237976074
Epoch 72  Train loss: -0.02162170503653732  Val loss: 0.011529592099468322
Epoch 73
-------------------------------
Batch 1/64 loss: -0.01977682113647461
Batch 2/64 loss: -0.01742410659790039
Batch 3/64 loss: -0.01656109094619751
Batch 4/64 loss: -0.02720874547958374
Batch 5/64 loss: -0.013567328453063965
Batch 6/64 loss: -0.028765499591827393
Batch 7/64 loss: -0.026352345943450928
Batch 8/64 loss: -0.020998775959014893
Batch 9/64 loss: 0.001574873924255371
Batch 10/64 loss: -0.018003344535827637
Batch 11/64 loss: -0.0239143967628479
Batch 12/64 loss: -0.03932911157608032
Batch 13/64 loss: -0.020672917366027832
Batch 14/64 loss: -0.00841754674911499
Batch 15/64 loss: -0.029953479766845703
Batch 16/64 loss: -0.02988952398300171
Batch 17/64 loss: -0.003674149513244629
Batch 18/64 loss: -0.018849849700927734
Batch 19/64 loss: -0.04216873645782471
Batch 20/64 loss: -0.02859872579574585
Batch 21/64 loss: -0.009888410568237305
Batch 22/64 loss: -0.008748054504394531
Batch 23/64 loss: -0.04097992181777954
Batch 24/64 loss: -0.04143291711807251
Batch 25/64 loss: -0.020328044891357422
Batch 26/64 loss: -0.01239246129989624
Batch 27/64 loss: -0.0003758668899536133
Batch 28/64 loss: 0.01054084300994873
Batch 29/64 loss: -0.03818398714065552
Batch 30/64 loss: -0.0362735390663147
Batch 31/64 loss: -0.04759621620178223
Batch 32/64 loss: -0.0031603574752807617
Batch 33/64 loss: -0.01657170057296753
Batch 34/64 loss: -0.013681113719940186
Batch 35/64 loss: -0.023340165615081787
Batch 36/64 loss: -0.01411658525466919
Batch 37/64 loss: -0.002158045768737793
Batch 38/64 loss: -0.029747068881988525
Batch 39/64 loss: -0.03865694999694824
Batch 40/64 loss: -0.04706698656082153
Batch 41/64 loss: 0.0023947954177856445
Batch 42/64 loss: -0.012962698936462402
Batch 43/64 loss: -0.030259430408477783
Batch 44/64 loss: -0.012411713600158691
Batch 45/64 loss: -0.0346144437789917
Batch 46/64 loss: -0.016460299491882324
Batch 47/64 loss: -0.02056097984313965
Batch 48/64 loss: -0.013178467750549316
Batch 49/64 loss: -0.05108082294464111
Batch 50/64 loss: -0.03643864393234253
Batch 51/64 loss: -0.03090280294418335
Batch 52/64 loss: -0.027136266231536865
Batch 53/64 loss: -0.024507761001586914
Batch 54/64 loss: -0.006476104259490967
Batch 55/64 loss: -0.015534579753875732
Batch 56/64 loss: -0.033259034156799316
Batch 57/64 loss: -0.012790679931640625
Batch 58/64 loss: -0.028589367866516113
Batch 59/64 loss: -0.025536298751831055
Batch 60/64 loss: -0.017659664154052734
Batch 61/64 loss: -0.021895945072174072
Batch 62/64 loss: -0.03051924705505371
Batch 63/64 loss: -0.02061671018600464
Batch 64/64 loss: -0.029540061950683594
Epoch 73  Train loss: -0.022115472718781115  Val loss: -0.0005531229104373054
Saving best model, epoch: 73
Epoch 74
-------------------------------
Batch 1/64 loss: -0.0065514445304870605
Batch 2/64 loss: -0.03880375623703003
Batch 3/64 loss: -0.030654311180114746
Batch 4/64 loss: -0.00327455997467041
Batch 5/64 loss: -0.020779073238372803
Batch 6/64 loss: -0.022718966007232666
Batch 7/64 loss: -0.044503748416900635
Batch 8/64 loss: -0.03487193584442139
Batch 9/64 loss: -0.014010488986968994
Batch 10/64 loss: -0.04093801975250244
Batch 11/64 loss: -0.02585446834564209
Batch 12/64 loss: -0.03217458724975586
Batch 13/64 loss: -0.031040430068969727
Batch 14/64 loss: -0.013812422752380371
Batch 15/64 loss: -0.03693670034408569
Batch 16/64 loss: -0.02632153034210205
Batch 17/64 loss: -0.03599965572357178
Batch 18/64 loss: -0.0170212984085083
Batch 19/64 loss: -0.01718658208847046
Batch 20/64 loss: -0.011279940605163574
Batch 21/64 loss: -0.022740662097930908
Batch 22/64 loss: -0.019437551498413086
Batch 23/64 loss: -0.021205782890319824
Batch 24/64 loss: -0.026779651641845703
Batch 25/64 loss: -0.03440701961517334
Batch 26/64 loss: -0.036596477031707764
Batch 27/64 loss: -0.030898869037628174
Batch 28/64 loss: -0.04619956016540527
Batch 29/64 loss: -0.011993885040283203
Batch 30/64 loss: -0.007574021816253662
Batch 31/64 loss: -0.02439570426940918
Batch 32/64 loss: -0.014027535915374756
Batch 33/64 loss: -0.03019094467163086
Batch 34/64 loss: -0.011967957019805908
Batch 35/64 loss: -0.009433507919311523
Batch 36/64 loss: -0.029387295246124268
Batch 37/64 loss: -0.035799264907836914
Batch 38/64 loss: -0.010949850082397461
Batch 39/64 loss: -0.033827364444732666
Batch 40/64 loss: -0.01128607988357544
Batch 41/64 loss: -0.03037893772125244
Batch 42/64 loss: -0.02834630012512207
Batch 43/64 loss: -0.024404048919677734
Batch 44/64 loss: -0.02845609188079834
Batch 45/64 loss: -0.026546359062194824
Batch 46/64 loss: -0.03172200918197632
Batch 47/64 loss: -0.026693761348724365
Batch 48/64 loss: -0.005603432655334473
Batch 49/64 loss: -0.02993154525756836
Batch 50/64 loss: -0.02023601531982422
Batch 51/64 loss: -0.017068207263946533
Batch 52/64 loss: -0.02285933494567871
Batch 53/64 loss: -0.025000572204589844
Batch 54/64 loss: -0.03223764896392822
Batch 55/64 loss: -0.026088297367095947
Batch 56/64 loss: -0.026526808738708496
Batch 57/64 loss: -0.009012341499328613
Batch 58/64 loss: -0.026569366455078125
Batch 59/64 loss: -0.04762190580368042
Batch 60/64 loss: -0.01070547103881836
Batch 61/64 loss: -0.0025886893272399902
Batch 62/64 loss: -0.0406876802444458
Batch 63/64 loss: -0.03317159414291382
Batch 64/64 loss: -0.01733773946762085
Epoch 74  Train loss: -0.024459492225272984  Val loss: -0.00191305613599692
Saving best model, epoch: 74
Epoch 75
-------------------------------
Batch 1/64 loss: -0.02261883020401001
Batch 2/64 loss: -0.04497891664505005
Batch 3/64 loss: -0.020788371562957764
Batch 4/64 loss: -0.027950286865234375
Batch 5/64 loss: -0.029994487762451172
Batch 6/64 loss: -0.024426043033599854
Batch 7/64 loss: -0.02792733907699585
Batch 8/64 loss: -0.012955069541931152
Batch 9/64 loss: -0.005247831344604492
Batch 10/64 loss: -0.038661181926727295
Batch 11/64 loss: -0.04817187786102295
Batch 12/64 loss: -0.0319972038269043
Batch 13/64 loss: -0.006344199180603027
Batch 14/64 loss: 0.007664203643798828
Batch 15/64 loss: -0.035142362117767334
Batch 16/64 loss: -0.0079115629196167
Batch 17/64 loss: -0.017450034618377686
Batch 18/64 loss: -0.025766313076019287
Batch 19/64 loss: -0.036823809146881104
Batch 20/64 loss: -0.02264583110809326
Batch 21/64 loss: -0.016621649265289307
Batch 22/64 loss: -0.011414170265197754
Batch 23/64 loss: -0.04498213529586792
Batch 24/64 loss: -0.018852591514587402
Batch 25/64 loss: -0.027267754077911377
Batch 26/64 loss: -0.0154532790184021
Batch 27/64 loss: -0.027728736400604248
Batch 28/64 loss: -0.022017300128936768
Batch 29/64 loss: -0.017455339431762695
Batch 30/64 loss: -0.025953590869903564
Batch 31/64 loss: -0.002209961414337158
Batch 32/64 loss: -0.025444328784942627
Batch 33/64 loss: -0.03660064935684204
Batch 34/64 loss: -0.03834497928619385
Batch 35/64 loss: -0.050991058349609375
Batch 36/64 loss: -0.0236741304397583
Batch 37/64 loss: -0.010447323322296143
Batch 38/64 loss: -0.01405256986618042
Batch 39/64 loss: -0.017866194248199463
Batch 40/64 loss: -0.031301140785217285
Batch 41/64 loss: -0.03725707530975342
Batch 42/64 loss: -0.03890258073806763
Batch 43/64 loss: 0.00026738643646240234
Batch 44/64 loss: 0.008016645908355713
Batch 45/64 loss: -0.039709389209747314
Batch 46/64 loss: -0.028392016887664795
Batch 47/64 loss: -0.018644213676452637
Batch 48/64 loss: -0.018036365509033203
Batch 49/64 loss: -0.019320428371429443
Batch 50/64 loss: -0.04948091506958008
Batch 51/64 loss: -0.029153645038604736
Batch 52/64 loss: -0.02820199728012085
Batch 53/64 loss: -0.010617077350616455
Batch 54/64 loss: -0.011340200901031494
Batch 55/64 loss: -0.035385072231292725
Batch 56/64 loss: -0.005963504314422607
Batch 57/64 loss: -0.041381001472473145
Batch 58/64 loss: -0.037555992603302
Batch 59/64 loss: -0.0343359112739563
Batch 60/64 loss: -0.02109086513519287
Batch 61/64 loss: -0.02806103229522705
Batch 62/64 loss: -0.025404393672943115
Batch 63/64 loss: -0.030801117420196533
Batch 64/64 loss: -0.031774044036865234
Epoch 75  Train loss: -0.02452388277240828  Val loss: 0.004048215564583585
Epoch 76
-------------------------------
Batch 1/64 loss: -0.008912086486816406
Batch 2/64 loss: -0.03101712465286255
Batch 3/64 loss: -0.020306050777435303
Batch 4/64 loss: -0.02030324935913086
Batch 5/64 loss: 0.006711900234222412
Batch 6/64 loss: -0.01712512969970703
Batch 7/64 loss: -0.02662503719329834
Batch 8/64 loss: -0.04032790660858154
Batch 9/64 loss: -0.012993574142456055
Batch 10/64 loss: -0.022010326385498047
Batch 11/64 loss: -0.028327584266662598
Batch 12/64 loss: -0.003904283046722412
Batch 13/64 loss: -0.02048259973526001
Batch 14/64 loss: -0.02795618772506714
Batch 15/64 loss: -0.022833824157714844
Batch 16/64 loss: -0.017041325569152832
Batch 17/64 loss: -0.030382752418518066
Batch 18/64 loss: -0.026158571243286133
Batch 19/64 loss: -0.023095250129699707
Batch 20/64 loss: -0.027382850646972656
Batch 21/64 loss: -0.01755434274673462
Batch 22/64 loss: -0.021943390369415283
Batch 23/64 loss: -0.03671663999557495
Batch 24/64 loss: -0.0220034122467041
Batch 25/64 loss: -0.011912882328033447
Batch 26/64 loss: -0.044078528881073
Batch 27/64 loss: -0.0247575044631958
Batch 28/64 loss: -0.037911832332611084
Batch 29/64 loss: -0.0371856689453125
Batch 30/64 loss: -0.03371989727020264
Batch 31/64 loss: -0.03853297233581543
Batch 32/64 loss: -0.047148704528808594
Batch 33/64 loss: -0.028926193714141846
Batch 34/64 loss: -0.02403736114501953
Batch 35/64 loss: -0.029642760753631592
Batch 36/64 loss: -0.008801281452178955
Batch 37/64 loss: 0.003616631031036377
Batch 38/64 loss: -0.03950399160385132
Batch 39/64 loss: -0.03657656908035278
Batch 40/64 loss: -0.03828912973403931
Batch 41/64 loss: -0.011636495590209961
Batch 42/64 loss: -0.0298500657081604
Batch 43/64 loss: -0.006202876567840576
Batch 44/64 loss: -0.034609079360961914
Batch 45/64 loss: -0.03389596939086914
Batch 46/64 loss: -0.045614421367645264
Batch 47/64 loss: -0.03087484836578369
Batch 48/64 loss: -0.015689730644226074
Batch 49/64 loss: -0.0036859512329101562
Batch 50/64 loss: -0.03075200319290161
Batch 51/64 loss: -0.024821162223815918
Batch 52/64 loss: -0.045390307903289795
Batch 53/64 loss: -0.0338364839553833
Batch 54/64 loss: -0.015380501747131348
Batch 55/64 loss: -0.024143576622009277
Batch 56/64 loss: -0.02215278148651123
Batch 57/64 loss: -0.02195906639099121
Batch 58/64 loss: -0.03471273183822632
Batch 59/64 loss: -0.035263657569885254
Batch 60/64 loss: -0.04344969987869263
Batch 61/64 loss: -0.039312660694122314
Batch 62/64 loss: -0.03642082214355469
Batch 63/64 loss: -0.031125128269195557
Batch 64/64 loss: -0.05455982685089111
Epoch 76  Train loss: -0.02647529630100026  Val loss: -0.002403216263682572
Saving best model, epoch: 76
Epoch 77
-------------------------------
Batch 1/64 loss: -0.02911365032196045
Batch 2/64 loss: -0.030889451503753662
Batch 3/64 loss: -0.007215321063995361
Batch 4/64 loss: -0.03099292516708374
Batch 5/64 loss: -0.022829055786132812
Batch 6/64 loss: -0.020970404148101807
Batch 7/64 loss: -0.04324525594711304
Batch 8/64 loss: -0.004372000694274902
Batch 9/64 loss: -0.023961782455444336
Batch 10/64 loss: -0.031260907649993896
Batch 11/64 loss: -0.02300846576690674
Batch 12/64 loss: -0.012068986892700195
Batch 13/64 loss: -0.009420394897460938
Batch 14/64 loss: -0.03078937530517578
Batch 15/64 loss: -0.045977115631103516
Batch 16/64 loss: -0.014025509357452393
Batch 17/64 loss: -0.036273181438446045
Batch 18/64 loss: -0.0015701055526733398
Batch 19/64 loss: -0.041028618812561035
Batch 20/64 loss: -0.024423539638519287
Batch 21/64 loss: -0.03419417142868042
Batch 22/64 loss: -0.023090243339538574
Batch 23/64 loss: -0.03235459327697754
Batch 24/64 loss: -0.029323160648345947
Batch 25/64 loss: -0.029080867767333984
Batch 26/64 loss: -0.017707109451293945
Batch 27/64 loss: -0.019094228744506836
Batch 28/64 loss: -0.04715794324874878
Batch 29/64 loss: -0.03380399942398071
Batch 30/64 loss: -0.03893327713012695
Batch 31/64 loss: -0.03329932689666748
Batch 32/64 loss: -0.023611605167388916
Batch 33/64 loss: -0.013096094131469727
Batch 34/64 loss: -0.03732043504714966
Batch 35/64 loss: -0.026449263095855713
Batch 36/64 loss: -0.010159730911254883
Batch 37/64 loss: -0.014554858207702637
Batch 38/64 loss: -0.003186345100402832
Batch 39/64 loss: -0.027625441551208496
Batch 40/64 loss: -0.027268171310424805
Batch 41/64 loss: -0.014900505542755127
Batch 42/64 loss: -0.027875959873199463
Batch 43/64 loss: -0.019141674041748047
Batch 44/64 loss: -0.03363597393035889
Batch 45/64 loss: 0.0026264190673828125
Batch 46/64 loss: -0.026268184185028076
Batch 47/64 loss: -0.03339928388595581
Batch 48/64 loss: -0.031403303146362305
Batch 49/64 loss: -0.023887932300567627
Batch 50/64 loss: -0.015598058700561523
Batch 51/64 loss: -0.032724618911743164
Batch 52/64 loss: -0.026999235153198242
Batch 53/64 loss: -0.031196177005767822
Batch 54/64 loss: -0.016188979148864746
Batch 55/64 loss: -0.03746515512466431
Batch 56/64 loss: -0.014457941055297852
Batch 57/64 loss: -0.030637383460998535
Batch 58/64 loss: -0.018759429454803467
Batch 59/64 loss: -0.01188361644744873
Batch 60/64 loss: -0.032178282737731934
Batch 61/64 loss: -0.03580451011657715
Batch 62/64 loss: -0.0339738130569458
Batch 63/64 loss: -0.012784242630004883
Batch 64/64 loss: -0.03190290927886963
Epoch 77  Train loss: -0.024897442144506118  Val loss: -0.004014204662690048
Saving best model, epoch: 77
Epoch 78
-------------------------------
Batch 1/64 loss: -0.013706803321838379
Batch 2/64 loss: -0.01908731460571289
Batch 3/64 loss: -0.01033872365951538
Batch 4/64 loss: -0.0341411828994751
Batch 5/64 loss: -0.031348466873168945
Batch 6/64 loss: -0.004153072834014893
Batch 7/64 loss: -0.0039351582527160645
Batch 8/64 loss: -0.02369213104248047
Batch 9/64 loss: -0.023232877254486084
Batch 10/64 loss: -0.034725069999694824
Batch 11/64 loss: -0.025323808193206787
Batch 12/64 loss: -0.0215570330619812
Batch 13/64 loss: -0.03163015842437744
Batch 14/64 loss: -0.04570287466049194
Batch 15/64 loss: -0.01925075054168701
Batch 16/64 loss: -0.03751581907272339
Batch 17/64 loss: -0.041666626930236816
Batch 18/64 loss: -0.02666640281677246
Batch 19/64 loss: -0.044460952281951904
Batch 20/64 loss: -0.03144556283950806
Batch 21/64 loss: -0.028714299201965332
Batch 22/64 loss: -0.02154839038848877
Batch 23/64 loss: -0.009147882461547852
Batch 24/64 loss: -0.03570699691772461
Batch 25/64 loss: -0.018910646438598633
Batch 26/64 loss: -0.031923890113830566
Batch 27/64 loss: -0.0294877290725708
Batch 28/64 loss: -0.03986966609954834
Batch 29/64 loss: -0.02273261547088623
Batch 30/64 loss: -0.02611231803894043
Batch 31/64 loss: -0.03751862049102783
Batch 32/64 loss: -0.040788114070892334
Batch 33/64 loss: -0.018066883087158203
Batch 34/64 loss: -0.031352996826171875
Batch 35/64 loss: 0.013737976551055908
Batch 36/64 loss: -0.027343034744262695
Batch 37/64 loss: -0.014688849449157715
Batch 38/64 loss: -0.021130740642547607
Batch 39/64 loss: -0.028302907943725586
Batch 40/64 loss: -0.029765784740447998
Batch 41/64 loss: -0.02985203266143799
Batch 42/64 loss: -0.03261476755142212
Batch 43/64 loss: -0.039366185665130615
Batch 44/64 loss: -0.022242307662963867
Batch 45/64 loss: -0.03203105926513672
Batch 46/64 loss: -0.03122800588607788
Batch 47/64 loss: -0.04011201858520508
Batch 48/64 loss: -0.03280806541442871
Batch 49/64 loss: -0.02960878610610962
Batch 50/64 loss: -0.04793661832809448
Batch 51/64 loss: -0.03618711233139038
Batch 52/64 loss: -0.028088033199310303
Batch 53/64 loss: -0.03094625473022461
Batch 54/64 loss: -0.008551180362701416
Batch 55/64 loss: -0.02758634090423584
Batch 56/64 loss: -0.029690861701965332
Batch 57/64 loss: -0.020421385765075684
Batch 58/64 loss: -0.019083857536315918
Batch 59/64 loss: -0.045624732971191406
Batch 60/64 loss: -0.018750905990600586
Batch 61/64 loss: -0.04248631000518799
Batch 62/64 loss: -0.015666484832763672
Batch 63/64 loss: -0.01498490571975708
Batch 64/64 loss: -0.017902135848999023
Epoch 78  Train loss: -0.02685882343965418  Val loss: 0.013013203733975125
Epoch 79
-------------------------------
Batch 1/64 loss: -0.01960456371307373
Batch 2/64 loss: -0.019921958446502686
Batch 3/64 loss: -0.01702982187271118
Batch 4/64 loss: -0.03733980655670166
Batch 5/64 loss: -0.04197055101394653
Batch 6/64 loss: -0.021774351596832275
Batch 7/64 loss: -0.04602241516113281
Batch 8/64 loss: -0.038841068744659424
Batch 9/64 loss: -0.04380953311920166
Batch 10/64 loss: -0.01844078302383423
Batch 11/64 loss: -0.03302562236785889
Batch 12/64 loss: -0.018591642379760742
Batch 13/64 loss: -0.016517341136932373
Batch 14/64 loss: 0.017902076244354248
Batch 15/64 loss: -0.03485107421875
Batch 16/64 loss: -0.03485018014907837
Batch 17/64 loss: 0.002536594867706299
Batch 18/64 loss: -0.021616339683532715
Batch 19/64 loss: -0.02924180030822754
Batch 20/64 loss: -0.012188851833343506
Batch 21/64 loss: -0.0257645845413208
Batch 22/64 loss: -0.023193717002868652
Batch 23/64 loss: -0.012637615203857422
Batch 24/64 loss: -0.00596773624420166
Batch 25/64 loss: -0.031120479106903076
Batch 26/64 loss: -0.018089652061462402
Batch 27/64 loss: -0.024800777435302734
Batch 28/64 loss: -0.02631556987762451
Batch 29/64 loss: -0.033603668212890625
Batch 30/64 loss: -0.025322377681732178
Batch 31/64 loss: -0.017714500427246094
Batch 32/64 loss: -0.028050243854522705
Batch 33/64 loss: 0.01461857557296753
Batch 34/64 loss: -0.023945331573486328
Batch 35/64 loss: -0.043609619140625
Batch 36/64 loss: -0.015279829502105713
Batch 37/64 loss: -0.032402992248535156
Batch 38/64 loss: -0.016028940677642822
Batch 39/64 loss: -0.0383639931678772
Batch 40/64 loss: -0.03169792890548706
Batch 41/64 loss: -0.016734063625335693
Batch 42/64 loss: -0.0343894362449646
Batch 43/64 loss: -0.033528804779052734
Batch 44/64 loss: -0.01411658525466919
Batch 45/64 loss: -0.03209090232849121
Batch 46/64 loss: -0.014516353607177734
Batch 47/64 loss: -0.012599349021911621
Batch 48/64 loss: -0.036474645137786865
Batch 49/64 loss: -0.024325668811798096
Batch 50/64 loss: -0.018416225910186768
Batch 51/64 loss: -0.01820361614227295
Batch 52/64 loss: -0.026234090328216553
Batch 53/64 loss: -0.02062058448791504
Batch 54/64 loss: -0.024674534797668457
Batch 55/64 loss: -0.018967390060424805
Batch 56/64 loss: -0.03487437963485718
Batch 57/64 loss: -0.03250396251678467
Batch 58/64 loss: -0.027587831020355225
Batch 59/64 loss: -0.017427802085876465
Batch 60/64 loss: -0.02837127447128296
Batch 61/64 loss: -0.015478968620300293
Batch 62/64 loss: -0.043344736099243164
Batch 63/64 loss: -0.04751241207122803
Batch 64/64 loss: -0.03474855422973633
Epoch 79  Train loss: -0.024526196835087794  Val loss: -0.0005119168471634592
Epoch 80
-------------------------------
Batch 1/64 loss: -0.026862502098083496
Batch 2/64 loss: -0.027395248413085938
Batch 3/64 loss: -0.04361063241958618
Batch 4/64 loss: -0.025660276412963867
Batch 5/64 loss: -0.03809911012649536
Batch 6/64 loss: -0.025835275650024414
Batch 7/64 loss: -0.03674924373626709
Batch 8/64 loss: -0.030010223388671875
Batch 9/64 loss: -0.04516041278839111
Batch 10/64 loss: -0.035504937171936035
Batch 11/64 loss: -0.03367722034454346
Batch 12/64 loss: -0.022048532962799072
Batch 13/64 loss: -0.013765394687652588
Batch 14/64 loss: -0.01840764284133911
Batch 15/64 loss: -0.03704303503036499
Batch 16/64 loss: -0.014438509941101074
Batch 17/64 loss: -0.03829085826873779
Batch 18/64 loss: -0.02379387617111206
Batch 19/64 loss: -0.038510262966156006
Batch 20/64 loss: -0.02450251579284668
Batch 21/64 loss: -0.030037522315979004
Batch 22/64 loss: -0.048059940338134766
Batch 23/64 loss: -0.04587376117706299
Batch 24/64 loss: -0.03132551908493042
Batch 25/64 loss: -0.04302835464477539
Batch 26/64 loss: -0.013273477554321289
Batch 27/64 loss: -0.02244347333908081
Batch 28/64 loss: -0.04852873086929321
Batch 29/64 loss: -0.028901338577270508
Batch 30/64 loss: -0.0379490852355957
Batch 31/64 loss: -0.0351371169090271
Batch 32/64 loss: -0.015033543109893799
Batch 33/64 loss: -0.028039515018463135
Batch 34/64 loss: -0.024880170822143555
Batch 35/64 loss: -0.020046114921569824
Batch 36/64 loss: -0.012843430042266846
Batch 37/64 loss: -0.019728779792785645
Batch 38/64 loss: -0.03306281566619873
Batch 39/64 loss: -0.030177652835845947
Batch 40/64 loss: -0.02200108766555786
Batch 41/64 loss: -0.017609894275665283
Batch 42/64 loss: -0.022914648056030273
Batch 43/64 loss: -0.008604109287261963
Batch 44/64 loss: -0.028402090072631836
Batch 45/64 loss: -0.03043133020401001
Batch 46/64 loss: -0.04089546203613281
Batch 47/64 loss: -0.021582841873168945
Batch 48/64 loss: -0.018757224082946777
Batch 49/64 loss: -0.019588947296142578
Batch 50/64 loss: -0.028219401836395264
Batch 51/64 loss: -0.031056582927703857
Batch 52/64 loss: -0.03045135736465454
Batch 53/64 loss: -0.001065373420715332
Batch 54/64 loss: -0.02259194850921631
Batch 55/64 loss: -0.016266584396362305
Batch 56/64 loss: -0.010120809078216553
Batch 57/64 loss: -0.038674354553222656
Batch 58/64 loss: -0.024255692958831787
Batch 59/64 loss: 0.000503838062286377
Batch 60/64 loss: -0.02952253818511963
Batch 61/64 loss: -0.026994645595550537
Batch 62/64 loss: -0.013127386569976807
Batch 63/64 loss: -0.005109965801239014
Batch 64/64 loss: -0.03534352779388428
Epoch 80  Train loss: -0.026697790388967475  Val loss: 0.001087691980538909
Epoch 81
-------------------------------
Batch 1/64 loss: -0.02071326971054077
Batch 2/64 loss: -0.004158973693847656
Batch 3/64 loss: -0.025523006916046143
Batch 4/64 loss: -0.008541882038116455
Batch 5/64 loss: -0.03700751066207886
Batch 6/64 loss: -0.042516887187957764
Batch 7/64 loss: -0.011487245559692383
Batch 8/64 loss: -0.03388553857803345
Batch 9/64 loss: -0.018706560134887695
Batch 10/64 loss: -0.025000333786010742
Batch 11/64 loss: -0.04579770565032959
Batch 12/64 loss: -0.02350938320159912
Batch 13/64 loss: -0.030780017375946045
Batch 14/64 loss: -0.018244385719299316
Batch 15/64 loss: -0.026728332042694092
Batch 16/64 loss: -0.036777377128601074
Batch 17/64 loss: -0.03876841068267822
Batch 18/64 loss: -0.03703552484512329
Batch 19/64 loss: -0.030313611030578613
Batch 20/64 loss: -0.034323692321777344
Batch 21/64 loss: -0.01434791088104248
Batch 22/64 loss: -0.04092705249786377
Batch 23/64 loss: -0.018984317779541016
Batch 24/64 loss: -0.01560509204864502
Batch 25/64 loss: -0.03216654062271118
Batch 26/64 loss: -0.03574603796005249
Batch 27/64 loss: -0.03237020969390869
Batch 28/64 loss: -0.027775943279266357
Batch 29/64 loss: -0.041289687156677246
Batch 30/64 loss: -0.03256988525390625
Batch 31/64 loss: -0.035022854804992676
Batch 32/64 loss: -0.0348820686340332
Batch 33/64 loss: -0.04018533229827881
Batch 34/64 loss: -0.024236321449279785
Batch 35/64 loss: -0.01663815975189209
Batch 36/64 loss: -0.0393642783164978
Batch 37/64 loss: -0.03865605592727661
Batch 38/64 loss: -0.04283273220062256
Batch 39/64 loss: -0.0029821395874023438
Batch 40/64 loss: -0.03713023662567139
Batch 41/64 loss: -0.03195357322692871
Batch 42/64 loss: -0.029128074645996094
Batch 43/64 loss: -0.045115888118743896
Batch 44/64 loss: -0.03951835632324219
Batch 45/64 loss: -0.010244488716125488
Batch 46/64 loss: -0.028124511241912842
Batch 47/64 loss: -0.0070716142654418945
Batch 48/64 loss: -0.025197744369506836
Batch 49/64 loss: -0.04680591821670532
Batch 50/64 loss: -0.03450864553451538
Batch 51/64 loss: -0.015288114547729492
Batch 52/64 loss: -0.031603991985321045
Batch 53/64 loss: -0.012696504592895508
Batch 54/64 loss: -0.031152665615081787
Batch 55/64 loss: -0.03912353515625
Batch 56/64 loss: -0.02775198221206665
Batch 57/64 loss: -0.014244318008422852
Batch 58/64 loss: -0.023635029792785645
Batch 59/64 loss: -0.036608099937438965
Batch 60/64 loss: 0.003920555114746094
Batch 61/64 loss: -0.0013522505760192871
Batch 62/64 loss: -0.03081798553466797
Batch 63/64 loss: -0.03728276491165161
Batch 64/64 loss: -0.03531813621520996
Epoch 81  Train loss: -0.027848260542925666  Val loss: -0.005720274554904793
Saving best model, epoch: 81
Epoch 82
-------------------------------
Batch 1/64 loss: -0.04971802234649658
Batch 2/64 loss: -0.03130137920379639
Batch 3/64 loss: -0.04879021644592285
Batch 4/64 loss: -0.01948559284210205
Batch 5/64 loss: -0.03342479467391968
Batch 6/64 loss: -0.04398077726364136
Batch 7/64 loss: -0.03330427408218384
Batch 8/64 loss: -0.019380688667297363
Batch 9/64 loss: -0.03253358602523804
Batch 10/64 loss: -0.013928592205047607
Batch 11/64 loss: -0.01848137378692627
Batch 12/64 loss: -0.005375027656555176
Batch 13/64 loss: -0.03189331293106079
Batch 14/64 loss: -0.03484451770782471
Batch 15/64 loss: -0.026787996292114258
Batch 16/64 loss: -0.03412830829620361
Batch 17/64 loss: -0.06068176031112671
Batch 18/64 loss: -0.03493237495422363
Batch 19/64 loss: -0.04629439115524292
Batch 20/64 loss: -0.0114518404006958
Batch 21/64 loss: -0.011737465858459473
Batch 22/64 loss: -0.021927833557128906
Batch 23/64 loss: -0.0247461199760437
Batch 24/64 loss: -0.02076929807662964
Batch 25/64 loss: -0.021703124046325684
Batch 26/64 loss: -0.03136354684829712
Batch 27/64 loss: -0.03788137435913086
Batch 28/64 loss: -0.037159860134124756
Batch 29/64 loss: -0.017586827278137207
Batch 30/64 loss: -0.031568050384521484
Batch 31/64 loss: -0.006280481815338135
Batch 32/64 loss: -0.03092062473297119
Batch 33/64 loss: -0.015595853328704834
Batch 34/64 loss: -0.03218346834182739
Batch 35/64 loss: -0.030493617057800293
Batch 36/64 loss: -0.032711029052734375
Batch 37/64 loss: -0.029112696647644043
Batch 38/64 loss: -0.01488959789276123
Batch 39/64 loss: -0.03133183717727661
Batch 40/64 loss: -0.02181839942932129
Batch 41/64 loss: -0.04683518409729004
Batch 42/64 loss: -0.023427605628967285
Batch 43/64 loss: -0.03815317153930664
Batch 44/64 loss: -0.033153414726257324
Batch 45/64 loss: -0.009732961654663086
Batch 46/64 loss: -0.028555750846862793
Batch 47/64 loss: -0.02874809503555298
Batch 48/64 loss: -0.028197109699249268
Batch 49/64 loss: -0.01629692316055298
Batch 50/64 loss: -0.040122389793395996
Batch 51/64 loss: -0.04344820976257324
Batch 52/64 loss: -0.0263674259185791
Batch 53/64 loss: -0.030610084533691406
Batch 54/64 loss: -0.0453876256942749
Batch 55/64 loss: -0.030207157135009766
Batch 56/64 loss: -0.03300750255584717
Batch 57/64 loss: -0.027492105960845947
Batch 58/64 loss: -0.03848671913146973
Batch 59/64 loss: -0.014036297798156738
Batch 60/64 loss: -0.03283756971359253
Batch 61/64 loss: -0.017053604125976562
Batch 62/64 loss: -0.04217839241027832
Batch 63/64 loss: -0.028451204299926758
Batch 64/64 loss: -0.020655393600463867
Epoch 82  Train loss: -0.029031811508477903  Val loss: -0.0012552496493886836
Epoch 83
-------------------------------
Batch 1/64 loss: -0.02628767490386963
Batch 2/64 loss: -0.012605071067810059
Batch 3/64 loss: -0.03878474235534668
Batch 4/64 loss: -0.0262184739112854
Batch 5/64 loss: -0.02735656499862671
Batch 6/64 loss: -0.010738968849182129
Batch 7/64 loss: -0.015198290348052979
Batch 8/64 loss: -0.023437321186065674
Batch 9/64 loss: -0.02710813283920288
Batch 10/64 loss: -0.030662596225738525
Batch 11/64 loss: -0.02142566442489624
Batch 12/64 loss: -0.036917805671691895
Batch 13/64 loss: -0.03256022930145264
Batch 14/64 loss: -0.016223490238189697
Batch 15/64 loss: -0.029452085494995117
Batch 16/64 loss: -0.0223504900932312
Batch 17/64 loss: -0.017599105834960938
Batch 18/64 loss: -0.0223233699798584
Batch 19/64 loss: -0.023097872734069824
Batch 20/64 loss: -0.045787811279296875
Batch 21/64 loss: -0.03965640068054199
Batch 22/64 loss: -0.020337700843811035
Batch 23/64 loss: -0.03345906734466553
Batch 24/64 loss: -0.03158986568450928
Batch 25/64 loss: -0.009163737297058105
Batch 26/64 loss: -0.029614567756652832
Batch 27/64 loss: -0.025236308574676514
Batch 28/64 loss: -0.02385920286178589
Batch 29/64 loss: -0.02598506212234497
Batch 30/64 loss: -0.015744149684906006
Batch 31/64 loss: -0.024230659008026123
Batch 32/64 loss: -0.04663759469985962
Batch 33/64 loss: -0.032260358333587646
Batch 34/64 loss: -0.0221979022026062
Batch 35/64 loss: -0.03174734115600586
Batch 36/64 loss: -0.03990757465362549
Batch 37/64 loss: -0.025238394737243652
Batch 38/64 loss: -0.03257828950881958
Batch 39/64 loss: -0.015323102474212646
Batch 40/64 loss: -0.025639057159423828
Batch 41/64 loss: -0.019251525402069092
Batch 42/64 loss: -0.04225432872772217
Batch 43/64 loss: -0.043199241161346436
Batch 44/64 loss: -0.05299431085586548
Batch 45/64 loss: -0.033294081687927246
Batch 46/64 loss: -0.02362602949142456
Batch 47/64 loss: -0.03553652763366699
Batch 48/64 loss: -0.014484882354736328
Batch 49/64 loss: -0.017870187759399414
Batch 50/64 loss: -0.04583537578582764
Batch 51/64 loss: -0.012761473655700684
Batch 52/64 loss: -0.024970412254333496
Batch 53/64 loss: -0.02796870470046997
Batch 54/64 loss: -0.0474817156791687
Batch 55/64 loss: -0.026555955410003662
Batch 56/64 loss: -0.04140740633010864
Batch 57/64 loss: -0.03328043222427368
Batch 58/64 loss: -0.024779081344604492
Batch 59/64 loss: -0.03121095895767212
Batch 60/64 loss: -0.04440891742706299
Batch 61/64 loss: -0.0067937374114990234
Batch 62/64 loss: -0.04828137159347534
Batch 63/64 loss: -0.03196513652801514
Batch 64/64 loss: -0.04166156053543091
Epoch 83  Train loss: -0.02851764805176679  Val loss: -0.0042561894839571925
Epoch 84
-------------------------------
Batch 1/64 loss: -0.021166563034057617
Batch 2/64 loss: -0.04662597179412842
Batch 3/64 loss: -0.039975643157958984
Batch 4/64 loss: -0.02356821298599243
Batch 5/64 loss: -0.04159790277481079
Batch 6/64 loss: -0.03935658931732178
Batch 7/64 loss: -0.04838138818740845
Batch 8/64 loss: -0.012051045894622803
Batch 9/64 loss: -0.04240316152572632
Batch 10/64 loss: -0.03458493947982788
Batch 11/64 loss: -0.036357760429382324
Batch 12/64 loss: -0.029709279537200928
Batch 13/64 loss: -0.03999662399291992
Batch 14/64 loss: -0.03808557987213135
Batch 15/64 loss: -0.04768168926239014
Batch 16/64 loss: -0.035455942153930664
Batch 17/64 loss: -0.027225494384765625
Batch 18/64 loss: -0.034485042095184326
Batch 19/64 loss: -0.02082616090774536
Batch 20/64 loss: 0.00955730676651001
Batch 21/64 loss: -0.016631126403808594
Batch 22/64 loss: -0.02725350856781006
Batch 23/64 loss: -0.04849112033843994
Batch 24/64 loss: -0.04157966375350952
Batch 25/64 loss: -0.04139763116836548
Batch 26/64 loss: -0.006844162940979004
Batch 27/64 loss: -0.022791683673858643
Batch 28/64 loss: -0.021326303482055664
Batch 29/64 loss: -0.03425240516662598
Batch 30/64 loss: -0.0196341872215271
Batch 31/64 loss: -0.01720339059829712
Batch 32/64 loss: -0.03179311752319336
Batch 33/64 loss: -0.009957969188690186
Batch 34/64 loss: -0.043931663036346436
Batch 35/64 loss: -0.028340518474578857
Batch 36/64 loss: -0.02710777521133423
Batch 37/64 loss: -0.03867918252944946
Batch 38/64 loss: -0.027858734130859375
Batch 39/64 loss: -0.04040348529815674
Batch 40/64 loss: -0.02949380874633789
Batch 41/64 loss: -0.04194915294647217
Batch 42/64 loss: -0.015101969242095947
Batch 43/64 loss: -0.037412285804748535
Batch 44/64 loss: -0.02929222583770752
Batch 45/64 loss: -0.03313177824020386
Batch 46/64 loss: -0.02639925479888916
Batch 47/64 loss: -0.026830673217773438
Batch 48/64 loss: -0.014999568462371826
Batch 49/64 loss: -0.017570793628692627
Batch 50/64 loss: -0.008525192737579346
Batch 51/64 loss: -0.01592862606048584
Batch 52/64 loss: -0.04102438688278198
Batch 53/64 loss: -0.007499814033508301
Batch 54/64 loss: -0.01648569107055664
Batch 55/64 loss: -0.03667187690734863
Batch 56/64 loss: -0.025975823402404785
Batch 57/64 loss: -0.03581291437149048
Batch 58/64 loss: -0.027862250804901123
Batch 59/64 loss: -0.026394546031951904
Batch 60/64 loss: -0.02146202325820923
Batch 61/64 loss: -0.01691281795501709
Batch 62/64 loss: -0.032840073108673096
Batch 63/64 loss: -0.02941715717315674
Batch 64/64 loss: -0.038195133209228516
Epoch 84  Train loss: -0.028848570468379003  Val loss: -0.004333755199851859
Epoch 85
-------------------------------
Batch 1/64 loss: -0.04036056995391846
Batch 2/64 loss: -0.04513418674468994
Batch 3/64 loss: -0.03290748596191406
Batch 4/64 loss: -0.03841942548751831
Batch 5/64 loss: -0.04004436731338501
Batch 6/64 loss: -0.029086291790008545
Batch 7/64 loss: -0.03706657886505127
Batch 8/64 loss: -0.032168030738830566
Batch 9/64 loss: -0.038593590259552
Batch 10/64 loss: -0.021921873092651367
Batch 11/64 loss: -0.011364340782165527
Batch 12/64 loss: -0.034308016300201416
Batch 13/64 loss: -0.03837251663208008
Batch 14/64 loss: -0.03972214460372925
Batch 15/64 loss: -0.028262197971343994
Batch 16/64 loss: -0.03417938947677612
Batch 17/64 loss: -0.029173195362091064
Batch 18/64 loss: -0.030796349048614502
Batch 19/64 loss: -0.026453077793121338
Batch 20/64 loss: -0.005306601524353027
Batch 21/64 loss: -0.025943517684936523
Batch 22/64 loss: -0.032852768898010254
Batch 23/64 loss: -0.023949742317199707
Batch 24/64 loss: -0.03235459327697754
Batch 25/64 loss: -0.045177161693573
Batch 26/64 loss: -0.012328863143920898
Batch 27/64 loss: -0.030756235122680664
Batch 28/64 loss: -0.026430130004882812
Batch 29/64 loss: 0.009032130241394043
Batch 30/64 loss: -0.04603421688079834
Batch 31/64 loss: -0.012731671333312988
Batch 32/64 loss: 0.002042829990386963
Batch 33/64 loss: -0.03140890598297119
Batch 34/64 loss: -0.05033576488494873
Batch 35/64 loss: -0.02510654926300049
Batch 36/64 loss: -0.033270061016082764
Batch 37/64 loss: -0.03018784523010254
Batch 38/64 loss: -0.03140592575073242
Batch 39/64 loss: -0.0239260196685791
Batch 40/64 loss: 0.003446221351623535
Batch 41/64 loss: -0.01078110933303833
Batch 42/64 loss: -0.037159621715545654
Batch 43/64 loss: -0.00783085823059082
Batch 44/64 loss: -0.02360093593597412
Batch 45/64 loss: -0.029668450355529785
Batch 46/64 loss: -0.020285964012145996
Batch 47/64 loss: -0.002828061580657959
Batch 48/64 loss: -0.03233599662780762
Batch 49/64 loss: -0.03665518760681152
Batch 50/64 loss: -0.04622989892959595
Batch 51/64 loss: -0.045024871826171875
Batch 52/64 loss: -0.042092204093933105
Batch 53/64 loss: -0.016444683074951172
Batch 54/64 loss: -0.03631854057312012
Batch 55/64 loss: -0.019729554653167725
Batch 56/64 loss: -0.02288949489593506
Batch 57/64 loss: -0.023700356483459473
Batch 58/64 loss: -0.020171761512756348
Batch 59/64 loss: -0.03119736909866333
Batch 60/64 loss: -0.025039851665496826
Batch 61/64 loss: -0.031851112842559814
Batch 62/64 loss: -0.04397648572921753
Batch 63/64 loss: -0.034154653549194336
Batch 64/64 loss: -0.049286603927612305
Epoch 85  Train loss: -0.028396094079111138  Val loss: -0.0031456125970558612
Epoch 86
-------------------------------
Batch 1/64 loss: -0.023717284202575684
Batch 2/64 loss: -0.03131222724914551
Batch 3/64 loss: -0.009735941886901855
Batch 4/64 loss: -0.0560070276260376
Batch 5/64 loss: -0.03079533576965332
Batch 6/64 loss: -0.04219973087310791
Batch 7/64 loss: -0.028759241104125977
Batch 8/64 loss: -0.03734177350997925
Batch 9/64 loss: -0.03947257995605469
Batch 10/64 loss: -0.04738086462020874
Batch 11/64 loss: -0.0417974591255188
Batch 12/64 loss: -0.019624531269073486
Batch 13/64 loss: -0.038172006607055664
Batch 14/64 loss: -0.03423506021499634
Batch 15/64 loss: -0.030862927436828613
Batch 16/64 loss: -0.03130996227264404
Batch 17/64 loss: -0.012960970401763916
Batch 18/64 loss: -0.055890440940856934
Batch 19/64 loss: -0.04243040084838867
Batch 20/64 loss: -0.05059671401977539
Batch 21/64 loss: -0.03126871585845947
Batch 22/64 loss: -0.028751254081726074
Batch 23/64 loss: -0.03835231065750122
Batch 24/64 loss: -0.04746723175048828
Batch 25/64 loss: -0.03394520282745361
Batch 26/64 loss: -0.019855022430419922
Batch 27/64 loss: -0.039400577545166016
Batch 28/64 loss: -0.03707540035247803
Batch 29/64 loss: -0.017680466175079346
Batch 30/64 loss: -0.02507549524307251
Batch 31/64 loss: -0.012693345546722412
Batch 32/64 loss: -0.05283385515213013
Batch 33/64 loss: -0.013061344623565674
Batch 34/64 loss: -0.02692723274230957
Batch 35/64 loss: -0.03983461856842041
Batch 36/64 loss: 0.006812989711761475
Batch 37/64 loss: -0.0217970609664917
Batch 38/64 loss: -0.03676396608352661
Batch 39/64 loss: -0.023786723613739014
Batch 40/64 loss: -0.026691555976867676
Batch 41/64 loss: -0.005376338958740234
Batch 42/64 loss: -0.018770158290863037
Batch 43/64 loss: -0.040258169174194336
Batch 44/64 loss: -0.022381067276000977
Batch 45/64 loss: -0.028589487075805664
Batch 46/64 loss: -0.019495785236358643
Batch 47/64 loss: -0.03712040185928345
Batch 48/64 loss: -0.04013621807098389
Batch 49/64 loss: -0.008125543594360352
Batch 50/64 loss: -0.013328194618225098
Batch 51/64 loss: -0.021727561950683594
Batch 52/64 loss: -0.02102452516555786
Batch 53/64 loss: -0.03476130962371826
Batch 54/64 loss: -0.031780123710632324
Batch 55/64 loss: -0.027499377727508545
Batch 56/64 loss: -0.008274257183074951
Batch 57/64 loss: -0.05488163232803345
Batch 58/64 loss: -0.03375214338302612
Batch 59/64 loss: -0.032138168811798096
Batch 60/64 loss: -0.03208267688751221
Batch 61/64 loss: -0.003380119800567627
Batch 62/64 loss: -0.021314501762390137
Batch 63/64 loss: -0.053884267807006836
Batch 64/64 loss: -0.03268790245056152
Epoch 86  Train loss: -0.029892530628279144  Val loss: -0.0014735320999040636
Epoch 87
-------------------------------
Batch 1/64 loss: -0.026183605194091797
Batch 2/64 loss: -0.018165290355682373
Batch 3/64 loss: -0.042972445487976074
Batch 4/64 loss: -0.03358262777328491
Batch 5/64 loss: -0.04111480712890625
Batch 6/64 loss: -0.03183114528656006
Batch 7/64 loss: -0.03152930736541748
Batch 8/64 loss: -0.031367361545562744
Batch 9/64 loss: -0.04117459058761597
Batch 10/64 loss: 0.009099900722503662
Batch 11/64 loss: -0.04166752099990845
Batch 12/64 loss: -0.03975790739059448
Batch 13/64 loss: -0.016147255897521973
Batch 14/64 loss: -0.03116917610168457
Batch 15/64 loss: -0.02668839693069458
Batch 16/64 loss: -0.043186962604522705
Batch 17/64 loss: -0.049073100090026855
Batch 18/64 loss: -0.03139662742614746
Batch 19/64 loss: -0.028498589992523193
Batch 20/64 loss: -0.03513801097869873
Batch 21/64 loss: -0.04288870096206665
Batch 22/64 loss: -0.02553170919418335
Batch 23/64 loss: -0.034291744232177734
Batch 24/64 loss: -0.03512692451477051
Batch 25/64 loss: -0.050127267837524414
Batch 26/64 loss: -0.017727971076965332
Batch 27/64 loss: -0.022850632667541504
Batch 28/64 loss: -0.04549551010131836
Batch 29/64 loss: -0.01599341630935669
Batch 30/64 loss: -0.025911450386047363
Batch 31/64 loss: -0.02170640230178833
Batch 32/64 loss: -0.044466257095336914
Batch 33/64 loss: -0.025060713291168213
Batch 34/64 loss: -0.0350419282913208
Batch 35/64 loss: -0.04050642251968384
Batch 36/64 loss: -0.031769514083862305
Batch 37/64 loss: -0.014674663543701172
Batch 38/64 loss: -0.05387824773788452
Batch 39/64 loss: -0.0464777946472168
Batch 40/64 loss: -0.04200279712677002
Batch 41/64 loss: -0.034886062145233154
Batch 42/64 loss: -0.014905273914337158
Batch 43/64 loss: -0.03847616910934448
Batch 44/64 loss: -0.02968519926071167
Batch 45/64 loss: -0.032028913497924805
Batch 46/64 loss: -0.041901469230651855
Batch 47/64 loss: -0.05365622043609619
Batch 48/64 loss: -0.03961181640625
Batch 49/64 loss: -0.0460052490234375
Batch 50/64 loss: -0.04188603162765503
Batch 51/64 loss: -0.03139472007751465
Batch 52/64 loss: -0.02277052402496338
Batch 53/64 loss: -0.04304128885269165
Batch 54/64 loss: -0.025302112102508545
Batch 55/64 loss: -0.03866630792617798
Batch 56/64 loss: -0.025998473167419434
Batch 57/64 loss: -0.01618891954421997
Batch 58/64 loss: -0.005617856979370117
Batch 59/64 loss: -0.03830081224441528
Batch 60/64 loss: -0.04233527183532715
Batch 61/64 loss: -0.054774701595306396
Batch 62/64 loss: -0.05332225561141968
Batch 63/64 loss: -0.03745436668395996
Batch 64/64 loss: -0.010593414306640625
Epoch 87  Train loss: -0.03324282309588264  Val loss: -0.008576183179809465
Saving best model, epoch: 87
Epoch 88
-------------------------------
Batch 1/64 loss: -0.03795325756072998
Batch 2/64 loss: -0.03718554973602295
Batch 3/64 loss: -0.02576422691345215
Batch 4/64 loss: -0.03657585382461548
Batch 5/64 loss: -0.03704845905303955
Batch 6/64 loss: -0.04015833139419556
Batch 7/64 loss: -0.01917731761932373
Batch 8/64 loss: -0.0471804141998291
Batch 9/64 loss: -0.02304023504257202
Batch 10/64 loss: -0.04678553342819214
Batch 11/64 loss: 0.004990875720977783
Batch 12/64 loss: -0.03016120195388794
Batch 13/64 loss: -0.045993924140930176
Batch 14/64 loss: -0.0409969687461853
Batch 15/64 loss: -0.02491438388824463
Batch 16/64 loss: -0.04509025812149048
Batch 17/64 loss: -0.03145182132720947
Batch 18/64 loss: -0.043801307678222656
Batch 19/64 loss: -0.025111913681030273
Batch 20/64 loss: -0.028088152408599854
Batch 21/64 loss: -0.020385384559631348
Batch 22/64 loss: -0.03486442565917969
Batch 23/64 loss: -0.032365620136260986
Batch 24/64 loss: -0.025445103645324707
Batch 25/64 loss: -0.025349974632263184
Batch 26/64 loss: -0.036532461643218994
Batch 27/64 loss: -0.009928584098815918
Batch 28/64 loss: -0.011369884014129639
Batch 29/64 loss: -0.03115260601043701
Batch 30/64 loss: -0.042009592056274414
Batch 31/64 loss: -0.0285719633102417
Batch 32/64 loss: -0.026419520378112793
Batch 33/64 loss: -0.036410510540008545
Batch 34/64 loss: -0.019319236278533936
Batch 35/64 loss: -0.0416339635848999
Batch 36/64 loss: -0.039331674575805664
Batch 37/64 loss: -0.02966374158859253
Batch 38/64 loss: -0.014155805110931396
Batch 39/64 loss: -0.04518991708755493
Batch 40/64 loss: -0.01879894733428955
Batch 41/64 loss: -0.019344568252563477
Batch 42/64 loss: -0.04456043243408203
Batch 43/64 loss: -0.03413665294647217
Batch 44/64 loss: -0.03230959177017212
Batch 45/64 loss: -0.030360937118530273
Batch 46/64 loss: -0.02713167667388916
Batch 47/64 loss: -0.020961344242095947
Batch 48/64 loss: -0.02473759651184082
Batch 49/64 loss: -0.02381265163421631
Batch 50/64 loss: -0.021300792694091797
Batch 51/64 loss: -0.00654149055480957
Batch 52/64 loss: -0.0324363112449646
Batch 53/64 loss: -0.0199163556098938
Batch 54/64 loss: -0.0149766206741333
Batch 55/64 loss: -0.02669692039489746
Batch 56/64 loss: -0.037643253803253174
Batch 57/64 loss: -0.007702231407165527
Batch 58/64 loss: -0.023356199264526367
Batch 59/64 loss: -0.034688353538513184
Batch 60/64 loss: -0.044461607933044434
Batch 61/64 loss: -0.02281421422958374
Batch 62/64 loss: -0.02293473482131958
Batch 63/64 loss: -0.024081766605377197
Batch 64/64 loss: -0.030615627765655518
Epoch 88  Train loss: -0.029023610610587926  Val loss: 0.0004125714711716904
Epoch 89
-------------------------------
Batch 1/64 loss: -0.03666633367538452
Batch 2/64 loss: -0.03167194128036499
Batch 3/64 loss: -0.03696376085281372
Batch 4/64 loss: -0.03497576713562012
Batch 5/64 loss: -0.038585782051086426
Batch 6/64 loss: -0.029142260551452637
Batch 7/64 loss: -0.027819931507110596
Batch 8/64 loss: -0.048482656478881836
Batch 9/64 loss: -0.0512009859085083
Batch 10/64 loss: -0.024579882621765137
Batch 11/64 loss: -0.025800585746765137
Batch 12/64 loss: -0.04608732461929321
Batch 13/64 loss: -0.011013984680175781
Batch 14/64 loss: -0.018738627433776855
Batch 15/64 loss: -0.02917647361755371
Batch 16/64 loss: -0.039252638816833496
Batch 17/64 loss: -0.02738487720489502
Batch 18/64 loss: -0.011124134063720703
Batch 19/64 loss: -0.0338515043258667
Batch 20/64 loss: -0.024652183055877686
Batch 21/64 loss: -0.042501747608184814
Batch 22/64 loss: -0.028839588165283203
Batch 23/64 loss: -0.03984224796295166
Batch 24/64 loss: -0.027710556983947754
Batch 25/64 loss: -0.05569988489151001
Batch 26/64 loss: -0.04629790782928467
Batch 27/64 loss: -0.0196993350982666
Batch 28/64 loss: -0.030431151390075684
Batch 29/64 loss: -0.027423977851867676
Batch 30/64 loss: -0.02563488483428955
Batch 31/64 loss: -0.04004162549972534
Batch 32/64 loss: -0.03938925266265869
Batch 33/64 loss: -0.047856152057647705
Batch 34/64 loss: -0.027416467666625977
Batch 35/64 loss: -0.03212010860443115
Batch 36/64 loss: -0.018370985984802246
Batch 37/64 loss: -0.030803799629211426
Batch 38/64 loss: -0.011878669261932373
Batch 39/64 loss: -0.026650846004486084
Batch 40/64 loss: -0.02928072214126587
Batch 41/64 loss: -0.04968661069869995
Batch 42/64 loss: -0.023286879062652588
Batch 43/64 loss: -0.026892423629760742
Batch 44/64 loss: -0.027010977268218994
Batch 45/64 loss: -0.03582799434661865
Batch 46/64 loss: -0.038593947887420654
Batch 47/64 loss: -0.02172619104385376
Batch 48/64 loss: -0.04740643501281738
Batch 49/64 loss: -0.02064269781112671
Batch 50/64 loss: -0.030402719974517822
Batch 51/64 loss: -0.04228389263153076
Batch 52/64 loss: -0.03286135196685791
Batch 53/64 loss: -0.01994425058364868
Batch 54/64 loss: -0.04428589344024658
Batch 55/64 loss: -0.04495149850845337
Batch 56/64 loss: -0.03056037425994873
Batch 57/64 loss: -0.03505295515060425
Batch 58/64 loss: -0.04890429973602295
Batch 59/64 loss: -0.0397418737411499
Batch 60/64 loss: -0.057628750801086426
Batch 61/64 loss: -0.03466898202896118
Batch 62/64 loss: -0.060830652713775635
Batch 63/64 loss: -0.037000060081481934
Batch 64/64 loss: -0.031069278717041016
Epoch 89  Train loss: -0.03370282696742637  Val loss: -0.010246903216306287
Saving best model, epoch: 89
Epoch 90
-------------------------------
Batch 1/64 loss: -0.03272026777267456
Batch 2/64 loss: -0.03400701284408569
Batch 3/64 loss: -0.04781419038772583
Batch 4/64 loss: -0.05302238464355469
Batch 5/64 loss: -0.048951923847198486
Batch 6/64 loss: -0.0181429386138916
Batch 7/64 loss: -0.04490530490875244
Batch 8/64 loss: -0.030088484287261963
Batch 9/64 loss: -0.0390247106552124
Batch 10/64 loss: -0.04084980487823486
Batch 11/64 loss: -0.04095458984375
Batch 12/64 loss: -0.03741908073425293
Batch 13/64 loss: -0.05261588096618652
Batch 14/64 loss: -0.03405481576919556
Batch 15/64 loss: -0.05046272277832031
Batch 16/64 loss: -0.04277944564819336
Batch 17/64 loss: -0.039369404315948486
Batch 18/64 loss: 0.0012328028678894043
Batch 19/64 loss: -0.03704535961151123
Batch 20/64 loss: -0.0209428071975708
Batch 21/64 loss: -0.021810054779052734
Batch 22/64 loss: -0.03586018085479736
Batch 23/64 loss: -0.052213966846466064
Batch 24/64 loss: -0.055142998695373535
Batch 25/64 loss: -0.04657864570617676
Batch 26/64 loss: -0.053188979625701904
Batch 27/64 loss: -0.058335721492767334
Batch 28/64 loss: -0.037035226821899414
Batch 29/64 loss: -0.053256988525390625
Batch 30/64 loss: -0.021712541580200195
Batch 31/64 loss: -0.03117460012435913
Batch 32/64 loss: -0.043756067752838135
Batch 33/64 loss: -0.03847789764404297
Batch 34/64 loss: -0.04885542392730713
Batch 35/64 loss: -0.04494720697402954
Batch 36/64 loss: -0.02325308322906494
Batch 37/64 loss: -0.04206967353820801
Batch 38/64 loss: -0.0459519624710083
Batch 39/64 loss: -0.04147803783416748
Batch 40/64 loss: -0.04389899969100952
Batch 41/64 loss: -0.016305387020111084
Batch 42/64 loss: -0.04296368360519409
Batch 43/64 loss: -0.04526621103286743
Batch 44/64 loss: -0.04995369911193848
Batch 45/64 loss: -0.03528308868408203
Batch 46/64 loss: -0.04091775417327881
Batch 47/64 loss: -0.03336304426193237
Batch 48/64 loss: -0.04008305072784424
Batch 49/64 loss: -0.01863342523574829
Batch 50/64 loss: -0.036457180976867676
Batch 51/64 loss: 0.003943920135498047
Batch 52/64 loss: -0.022683680057525635
Batch 53/64 loss: -0.03625202178955078
Batch 54/64 loss: -0.023038804531097412
Batch 55/64 loss: -0.038294434547424316
Batch 56/64 loss: -0.029645204544067383
Batch 57/64 loss: 0.0095633864402771
Batch 58/64 loss: -0.03294265270233154
Batch 59/64 loss: -0.041954219341278076
Batch 60/64 loss: -0.023964285850524902
Batch 61/64 loss: -0.02855372428894043
Batch 62/64 loss: -0.03806459903717041
Batch 63/64 loss: -0.030121266841888428
Batch 64/64 loss: -0.04459226131439209
Epoch 90  Train loss: -0.036197880670136094  Val loss: -0.0067437203069732774
Epoch 91
-------------------------------
Batch 1/64 loss: -0.04321169853210449
Batch 2/64 loss: -0.026863574981689453
Batch 3/64 loss: -0.04498833417892456
Batch 4/64 loss: -0.04493057727813721
Batch 5/64 loss: -0.03662818670272827
Batch 6/64 loss: -0.03353780508041382
Batch 7/64 loss: -0.0303533673286438
Batch 8/64 loss: -0.04080992937088013
Batch 9/64 loss: -0.04688310623168945
Batch 10/64 loss: -0.044491469860076904
Batch 11/64 loss: -0.04038912057876587
Batch 12/64 loss: -0.019867122173309326
Batch 13/64 loss: -0.029698550701141357
Batch 14/64 loss: -0.04992532730102539
Batch 15/64 loss: -0.03860396146774292
Batch 16/64 loss: -0.01328742504119873
Batch 17/64 loss: -0.04124939441680908
Batch 18/64 loss: -0.00948035717010498
Batch 19/64 loss: -0.025824010372161865
Batch 20/64 loss: -0.03940004110336304
Batch 21/64 loss: -0.0270577073097229
Batch 22/64 loss: -0.056102454662323
Batch 23/64 loss: -0.05985677242279053
Batch 24/64 loss: -0.03949350118637085
Batch 25/64 loss: -0.024909675121307373
Batch 26/64 loss: -0.0384063720703125
Batch 27/64 loss: -0.056234896183013916
Batch 28/64 loss: -0.03765761852264404
Batch 29/64 loss: -0.04109162092208862
Batch 30/64 loss: -0.03940671682357788
Batch 31/64 loss: -0.007328212261199951
Batch 32/64 loss: -0.03341430425643921
Batch 33/64 loss: -0.02869516611099243
Batch 34/64 loss: -0.03468209505081177
Batch 35/64 loss: -0.019696712493896484
Batch 36/64 loss: -0.0400615930557251
Batch 37/64 loss: -0.03816932439804077
Batch 38/64 loss: -0.045441389083862305
Batch 39/64 loss: -0.025860965251922607
Batch 40/64 loss: -0.031184017658233643
Batch 41/64 loss: -0.03021925687789917
Batch 42/64 loss: -0.03219324350357056
Batch 43/64 loss: -0.0165175199508667
Batch 44/64 loss: -0.033474504947662354
Batch 45/64 loss: -0.03759342432022095
Batch 46/64 loss: -0.043544650077819824
Batch 47/64 loss: -0.018520593643188477
Batch 48/64 loss: -0.018898367881774902
Batch 49/64 loss: -0.039846837520599365
Batch 50/64 loss: -0.008093535900115967
Batch 51/64 loss: -0.037656188011169434
Batch 52/64 loss: -0.04571634531021118
Batch 53/64 loss: -0.048249244689941406
Batch 54/64 loss: -0.016629576683044434
Batch 55/64 loss: -0.038167357444763184
Batch 56/64 loss: -0.02637714147567749
Batch 57/64 loss: -0.03368520736694336
Batch 58/64 loss: -0.01745915412902832
Batch 59/64 loss: -0.04159301519393921
Batch 60/64 loss: -0.03330838680267334
Batch 61/64 loss: -0.03680211305618286
Batch 62/64 loss: -0.0341717004776001
Batch 63/64 loss: -0.024746179580688477
Batch 64/64 loss: -0.0399937629699707
Epoch 91  Train loss: -0.03401777791041954  Val loss: -0.00966726054031005
Epoch 92
-------------------------------
Batch 1/64 loss: -0.03008556365966797
Batch 2/64 loss: -0.03609776496887207
Batch 3/64 loss: -0.02546858787536621
Batch 4/64 loss: -0.032829999923706055
Batch 5/64 loss: -0.03851437568664551
Batch 6/64 loss: -0.0262908935546875
Batch 7/64 loss: -0.030511021614074707
Batch 8/64 loss: -0.01569211483001709
Batch 9/64 loss: -0.04644244909286499
Batch 10/64 loss: -0.03486829996109009
Batch 11/64 loss: -0.03874003887176514
Batch 12/64 loss: -0.04509568214416504
Batch 13/64 loss: -0.0389178991317749
Batch 14/64 loss: -0.05505728721618652
Batch 15/64 loss: -0.03706550598144531
Batch 16/64 loss: -0.028116226196289062
Batch 17/64 loss: -0.050335705280303955
Batch 18/64 loss: -0.050356924533843994
Batch 19/64 loss: -0.04337948560714722
Batch 20/64 loss: -0.00031262636184692383
Batch 21/64 loss: -0.0589178204536438
Batch 22/64 loss: -0.03261750936508179
Batch 23/64 loss: -0.04018282890319824
Batch 24/64 loss: -0.048489391803741455
Batch 25/64 loss: -0.05221247673034668
Batch 26/64 loss: -0.025740087032318115
Batch 27/64 loss: -0.022732436656951904
Batch 28/64 loss: -0.032574594020843506
Batch 29/64 loss: -0.04425668716430664
Batch 30/64 loss: -0.047774553298950195
Batch 31/64 loss: -0.039897143840789795
Batch 32/64 loss: -0.048092782497406006
Batch 33/64 loss: -0.01313692331314087
Batch 34/64 loss: -0.036121904850006104
Batch 35/64 loss: -0.04123729467391968
Batch 36/64 loss: 0.005851149559020996
Batch 37/64 loss: -0.021580100059509277
Batch 38/64 loss: -0.022347629070281982
Batch 39/64 loss: -0.007084310054779053
Batch 40/64 loss: -0.033999502658843994
Batch 41/64 loss: -0.0366634726524353
Batch 42/64 loss: -0.032146334648132324
Batch 43/64 loss: -0.04235994815826416
Batch 44/64 loss: -0.027403533458709717
Batch 45/64 loss: -0.04153472185134888
Batch 46/64 loss: -0.02426975965499878
Batch 47/64 loss: -0.03358781337738037
Batch 48/64 loss: -0.05538302659988403
Batch 49/64 loss: -0.06502419710159302
Batch 50/64 loss: -0.030645668506622314
Batch 51/64 loss: -0.03697216510772705
Batch 52/64 loss: -0.03555190563201904
Batch 53/64 loss: -0.03838527202606201
Batch 54/64 loss: -0.05650818347930908
Batch 55/64 loss: -0.015067696571350098
Batch 56/64 loss: -0.03797602653503418
Batch 57/64 loss: -0.04074728488922119
Batch 58/64 loss: -0.04641073942184448
Batch 59/64 loss: -0.042398154735565186
Batch 60/64 loss: -0.03240323066711426
Batch 61/64 loss: -0.04607558250427246
Batch 62/64 loss: -0.029911518096923828
Batch 63/64 loss: -0.0241890549659729
Batch 64/64 loss: -0.02271115779876709
Epoch 92  Train loss: -0.035387795111712285  Val loss: -0.012356078911483083
Saving best model, epoch: 92
Epoch 93
-------------------------------
Batch 1/64 loss: -0.045173704624176025
Batch 2/64 loss: -0.04722505807876587
Batch 3/64 loss: 0.009528040885925293
Batch 4/64 loss: -0.03116023540496826
Batch 5/64 loss: -0.05465143918991089
Batch 6/64 loss: -0.03224891424179077
Batch 7/64 loss: -0.05980968475341797
Batch 8/64 loss: -0.039921700954437256
Batch 9/64 loss: -0.04992067813873291
Batch 10/64 loss: -0.04660087823867798
Batch 11/64 loss: -0.027102410793304443
Batch 12/64 loss: -0.05416667461395264
Batch 13/64 loss: -0.03305506706237793
Batch 14/64 loss: -0.026671767234802246
Batch 15/64 loss: -0.04217374324798584
Batch 16/64 loss: -0.05477553606033325
Batch 17/64 loss: -0.05005753040313721
Batch 18/64 loss: -0.04964709281921387
Batch 19/64 loss: -0.026824772357940674
Batch 20/64 loss: -0.04294455051422119
Batch 21/64 loss: -0.05822014808654785
Batch 22/64 loss: -0.011529147624969482
Batch 23/64 loss: -0.038648903369903564
Batch 24/64 loss: -0.04804372787475586
Batch 25/64 loss: 0.009807884693145752
Batch 26/64 loss: -0.05537378787994385
Batch 27/64 loss: -0.036150336265563965
Batch 28/64 loss: -0.03405404090881348
Batch 29/64 loss: -0.04311668872833252
Batch 30/64 loss: -0.0377810001373291
Batch 31/64 loss: -0.05262303352355957
Batch 32/64 loss: -0.050824105739593506
Batch 33/64 loss: -0.03634977340698242
Batch 34/64 loss: -0.01721799373626709
Batch 35/64 loss: -0.015076100826263428
Batch 36/64 loss: -0.048446714878082275
Batch 37/64 loss: -0.04923027753829956
Batch 38/64 loss: -0.04859280586242676
Batch 39/64 loss: -0.03673911094665527
Batch 40/64 loss: -0.03375697135925293
Batch 41/64 loss: -0.046989500522613525
Batch 42/64 loss: -0.049741387367248535
Batch 43/64 loss: -0.029534578323364258
Batch 44/64 loss: -0.0316813588142395
Batch 45/64 loss: -0.030022382736206055
Batch 46/64 loss: -0.05880671739578247
Batch 47/64 loss: -0.032745301723480225
Batch 48/64 loss: -0.04881769418716431
Batch 49/64 loss: -0.03635317087173462
Batch 50/64 loss: -0.03650081157684326
Batch 51/64 loss: -0.04622769355773926
Batch 52/64 loss: -0.03298300504684448
Batch 53/64 loss: -0.04631221294403076
Batch 54/64 loss: -0.026859045028686523
Batch 55/64 loss: -0.051984548568725586
Batch 56/64 loss: -0.04733633995056152
Batch 57/64 loss: -0.04162156581878662
Batch 58/64 loss: -0.041810035705566406
Batch 59/64 loss: -0.04437065124511719
Batch 60/64 loss: -0.012454986572265625
Batch 61/64 loss: -0.0005207061767578125
Batch 62/64 loss: -0.04897129535675049
Batch 63/64 loss: -0.0598602294921875
Batch 64/64 loss: -0.056380271911621094
Epoch 93  Train loss: -0.03908016634922402  Val loss: -0.015484602590606794
Saving best model, epoch: 93
Epoch 94
-------------------------------
Batch 1/64 loss: -0.05917328596115112
Batch 2/64 loss: -0.05584096908569336
Batch 3/64 loss: -0.05876576900482178
Batch 4/64 loss: -0.0519864559173584
Batch 5/64 loss: -0.03851437568664551
Batch 6/64 loss: -0.043986618518829346
Batch 7/64 loss: -0.03522461652755737
Batch 8/64 loss: -0.053090453147888184
Batch 9/64 loss: -0.04988676309585571
Batch 10/64 loss: -0.033709049224853516
Batch 11/64 loss: -0.0568806529045105
Batch 12/64 loss: -0.042883992195129395
Batch 13/64 loss: -0.048441171646118164
Batch 14/64 loss: -0.033736228942871094
Batch 15/64 loss: -0.0480080246925354
Batch 16/64 loss: -0.041474997997283936
Batch 17/64 loss: -0.020994484424591064
Batch 18/64 loss: -0.04849648475646973
Batch 19/64 loss: -0.05325597524642944
Batch 20/64 loss: -0.03894233703613281
Batch 21/64 loss: -0.04620492458343506
Batch 22/64 loss: -0.03853452205657959
Batch 23/64 loss: -0.03069591522216797
Batch 24/64 loss: -0.05105012655258179
Batch 25/64 loss: -0.02948451042175293
Batch 26/64 loss: -0.026123762130737305
Batch 27/64 loss: -0.008090317249298096
Batch 28/64 loss: -0.030826210975646973
Batch 29/64 loss: -0.04562193155288696
Batch 30/64 loss: -0.03439265489578247
Batch 31/64 loss: -0.01740807294845581
Batch 32/64 loss: -0.03285104036331177
Batch 33/64 loss: -0.04914355278015137
Batch 34/64 loss: -0.02618032693862915
Batch 35/64 loss: -0.01796591281890869
Batch 36/64 loss: -0.045706212520599365
Batch 37/64 loss: -0.027605712413787842
Batch 38/64 loss: -0.041791677474975586
Batch 39/64 loss: -0.03525775671005249
Batch 40/64 loss: -0.040513694286346436
Batch 41/64 loss: -0.03842651844024658
Batch 42/64 loss: -0.048580706119537354
Batch 43/64 loss: -0.036293745040893555
Batch 44/64 loss: -0.028921067714691162
Batch 45/64 loss: -0.033313632011413574
Batch 46/64 loss: -0.05374932289123535
Batch 47/64 loss: -0.027826547622680664
Batch 48/64 loss: -0.039618849754333496
Batch 49/64 loss: -0.03499668836593628
Batch 50/64 loss: -0.04679262638092041
Batch 51/64 loss: -0.034183502197265625
Batch 52/64 loss: -0.03835254907608032
Batch 53/64 loss: -0.058725833892822266
Batch 54/64 loss: -0.03282982110977173
Batch 55/64 loss: -0.041313111782073975
Batch 56/64 loss: -0.03510761260986328
Batch 57/64 loss: -0.0468440055847168
Batch 58/64 loss: -0.04754585027694702
Batch 59/64 loss: -0.049307286739349365
Batch 60/64 loss: -0.044934093952178955
Batch 61/64 loss: -0.02691054344177246
Batch 62/64 loss: -0.027943849563598633
Batch 63/64 loss: -0.035394132137298584
Batch 64/64 loss: -0.030244827270507812
Epoch 94  Train loss: -0.03951901267556583  Val loss: -0.014233559472454372
Epoch 95
-------------------------------
Batch 1/64 loss: -0.03862202167510986
Batch 2/64 loss: -0.050368428230285645
Batch 3/64 loss: -0.049585938453674316
Batch 4/64 loss: -0.03595525026321411
Batch 5/64 loss: -0.03541499376296997
Batch 6/64 loss: -0.042369961738586426
Batch 7/64 loss: -0.04291635751724243
Batch 8/64 loss: -0.05101221799850464
Batch 9/64 loss: -0.04138380289077759
Batch 10/64 loss: -0.04118567705154419
Batch 11/64 loss: -0.009985923767089844
Batch 12/64 loss: -0.014506042003631592
Batch 13/64 loss: -0.03754478693008423
Batch 14/64 loss: -0.052482545375823975
Batch 15/64 loss: -0.05182516574859619
Batch 16/64 loss: -0.056455790996551514
Batch 17/64 loss: -0.036046385765075684
Batch 18/64 loss: -0.04019582271575928
Batch 19/64 loss: -0.04080718755722046
Batch 20/64 loss: -0.03108459711074829
Batch 21/64 loss: -0.046209752559661865
Batch 22/64 loss: -0.04282480478286743
Batch 23/64 loss: -0.020639240741729736
Batch 24/64 loss: -0.05634075403213501
Batch 25/64 loss: -0.04740595817565918
Batch 26/64 loss: -0.023959875106811523
Batch 27/64 loss: -0.0526200532913208
Batch 28/64 loss: -0.028118491172790527
Batch 29/64 loss: -0.04036635160446167
Batch 30/64 loss: -0.050300002098083496
Batch 31/64 loss: -0.03481656312942505
Batch 32/64 loss: -0.04631161689758301
Batch 33/64 loss: -0.03893125057220459
Batch 34/64 loss: -0.04905533790588379
Batch 35/64 loss: -0.042206525802612305
Batch 36/64 loss: -0.05269169807434082
Batch 37/64 loss: -0.03874391317367554
Batch 38/64 loss: -0.010697722434997559
Batch 39/64 loss: -0.03375744819641113
Batch 40/64 loss: -0.04075956344604492
Batch 41/64 loss: -0.04476743936538696
Batch 42/64 loss: -0.0483705997467041
Batch 43/64 loss: -0.04522895812988281
Batch 44/64 loss: -0.03818875551223755
Batch 45/64 loss: -0.04843825101852417
Batch 46/64 loss: -0.02512061595916748
Batch 47/64 loss: -0.044044435024261475
Batch 48/64 loss: -0.04223746061325073
Batch 49/64 loss: -0.02890223264694214
Batch 50/64 loss: -0.04766559600830078
Batch 51/64 loss: -0.057417213916778564
Batch 52/64 loss: -0.05212348699569702
Batch 53/64 loss: -0.033648908138275146
Batch 54/64 loss: -0.037919461727142334
Batch 55/64 loss: -0.0374721884727478
Batch 56/64 loss: -0.0315670371055603
Batch 57/64 loss: -0.02484261989593506
Batch 58/64 loss: -0.028731584548950195
Batch 59/64 loss: -0.0345383882522583
Batch 60/64 loss: -0.04246401786804199
Batch 61/64 loss: -0.02821803092956543
Batch 62/64 loss: -0.042733073234558105
Batch 63/64 loss: -0.0420992374420166
Batch 64/64 loss: -0.04206979274749756
Epoch 95  Train loss: -0.039761533456690173  Val loss: -0.011103830591509841
Epoch 96
-------------------------------
Batch 1/64 loss: -0.03571963310241699
Batch 2/64 loss: -0.014975130558013916
Batch 3/64 loss: -0.024333178997039795
Batch 4/64 loss: -0.03118455410003662
Batch 5/64 loss: -0.0429917573928833
Batch 6/64 loss: -0.05361771583557129
Batch 7/64 loss: -0.05120939016342163
Batch 8/64 loss: -0.044586241245269775
Batch 9/64 loss: -0.0346340537071228
Batch 10/64 loss: -0.053485214710235596
Batch 11/64 loss: -0.03457266092300415
Batch 12/64 loss: -0.039882540702819824
Batch 13/64 loss: -0.03218817710876465
Batch 14/64 loss: -0.03893327713012695
Batch 15/64 loss: -0.036740243434906006
Batch 16/64 loss: -0.04923081398010254
Batch 17/64 loss: -0.049791574478149414
Batch 18/64 loss: -0.04742419719696045
Batch 19/64 loss: -0.03376460075378418
Batch 20/64 loss: -0.03802669048309326
Batch 21/64 loss: -0.04011422395706177
Batch 22/64 loss: -0.03605908155441284
Batch 23/64 loss: -0.038125455379486084
Batch 24/64 loss: -0.055810391902923584
Batch 25/64 loss: -0.02929145097732544
Batch 26/64 loss: -0.029373109340667725
Batch 27/64 loss: -0.03176218271255493
Batch 28/64 loss: -0.04132091999053955
Batch 29/64 loss: -0.052448272705078125
Batch 30/64 loss: -0.05760699510574341
Batch 31/64 loss: -0.03436356782913208
Batch 32/64 loss: -0.04911196231842041
Batch 33/64 loss: -0.034552574157714844
Batch 34/64 loss: -0.04114812612533569
Batch 35/64 loss: -0.05927157402038574
Batch 36/64 loss: -0.040647804737091064
Batch 37/64 loss: -0.026069045066833496
Batch 38/64 loss: -0.06222051382064819
Batch 39/64 loss: -0.051413118839263916
Batch 40/64 loss: -0.02125263214111328
Batch 41/64 loss: -0.046350300312042236
Batch 42/64 loss: -0.04229426383972168
Batch 43/64 loss: -0.03230714797973633
Batch 44/64 loss: -0.03823280334472656
Batch 45/64 loss: -0.030519485473632812
Batch 46/64 loss: -0.005907773971557617
Batch 47/64 loss: -0.0378948450088501
Batch 48/64 loss: -0.03964221477508545
Batch 49/64 loss: -0.04353266954421997
Batch 50/64 loss: -0.04248923063278198
Batch 51/64 loss: -0.053381264209747314
Batch 52/64 loss: -0.04740935564041138
Batch 53/64 loss: -0.02572190761566162
Batch 54/64 loss: -0.03139454126358032
Batch 55/64 loss: -0.06439709663391113
Batch 56/64 loss: -0.03245508670806885
Batch 57/64 loss: -0.05313980579376221
Batch 58/64 loss: -0.03905379772186279
Batch 59/64 loss: -0.03493237495422363
Batch 60/64 loss: -0.03582876920700073
Batch 61/64 loss: -0.05303269624710083
Batch 62/64 loss: -0.05084890127182007
Batch 63/64 loss: -0.036042213439941406
Batch 64/64 loss: -0.04781001806259155
Epoch 96  Train loss: -0.04034385424034268  Val loss: -0.009192789747952596
Epoch 97
-------------------------------
Batch 1/64 loss: -0.04495441913604736
Batch 2/64 loss: -0.04179292917251587
Batch 3/64 loss: -0.05860501527786255
Batch 4/64 loss: -0.05101507902145386
Batch 5/64 loss: -0.041026294231414795
Batch 6/64 loss: -0.031423985958099365
Batch 7/64 loss: -0.04151725769042969
Batch 8/64 loss: -0.05290663242340088
Batch 9/64 loss: -0.046680450439453125
Batch 10/64 loss: -0.039016127586364746
Batch 11/64 loss: -0.06214916706085205
Batch 12/64 loss: -0.03587061166763306
Batch 13/64 loss: -0.042806148529052734
Batch 14/64 loss: -0.029142320156097412
Batch 15/64 loss: -0.05628389120101929
Batch 16/64 loss: -0.03350710868835449
Batch 17/64 loss: -0.041626155376434326
Batch 18/64 loss: -0.05122929811477661
Batch 19/64 loss: -0.05405426025390625
Batch 20/64 loss: -0.04540586471557617
Batch 21/64 loss: -0.06063640117645264
Batch 22/64 loss: -0.024637222290039062
Batch 23/64 loss: -0.05148005485534668
Batch 24/64 loss: -0.02272409200668335
Batch 25/64 loss: -0.031611740589141846
Batch 26/64 loss: -0.0214993953704834
Batch 27/64 loss: -0.043157100677490234
Batch 28/64 loss: -0.04034900665283203
Batch 29/64 loss: -0.03572726249694824
Batch 30/64 loss: -0.05253869295120239
Batch 31/64 loss: -0.047658443450927734
Batch 32/64 loss: -0.036097586154937744
Batch 33/64 loss: -0.03289222717285156
Batch 34/64 loss: -0.054073989391326904
Batch 35/64 loss: -0.045189857482910156
Batch 36/64 loss: -0.042652666568756104
Batch 37/64 loss: -0.040486156940460205
Batch 38/64 loss: -0.048974454402923584
Batch 39/64 loss: -0.045181095600128174
Batch 40/64 loss: -0.05158662796020508
Batch 41/64 loss: -0.05638605356216431
Batch 42/64 loss: -0.056684017181396484
Batch 43/64 loss: -0.05411553382873535
Batch 44/64 loss: -0.017939329147338867
Batch 45/64 loss: -0.034442901611328125
Batch 46/64 loss: -0.03206181526184082
Batch 47/64 loss: -0.05458158254623413
Batch 48/64 loss: -0.0484657883644104
Batch 49/64 loss: -0.031410932540893555
Batch 50/64 loss: -0.0385628342628479
Batch 51/64 loss: -0.02444601058959961
Batch 52/64 loss: -0.028611481189727783
Batch 53/64 loss: -0.05465364456176758
Batch 54/64 loss: -0.056312739849090576
Batch 55/64 loss: -0.02329576015472412
Batch 56/64 loss: -0.03651714324951172
Batch 57/64 loss: -0.05174088478088379
Batch 58/64 loss: -0.04928421974182129
Batch 59/64 loss: -0.05219006538391113
Batch 60/64 loss: -0.02727818489074707
Batch 61/64 loss: -0.0382537841796875
Batch 62/64 loss: -0.04791557788848877
Batch 63/64 loss: -0.037558674812316895
Batch 64/64 loss: -0.01349717378616333
Epoch 97  Train loss: -0.04224312048332364  Val loss: -0.002550271368518318
Epoch 98
-------------------------------
Batch 1/64 loss: -0.05040478706359863
Batch 2/64 loss: -0.04617387056350708
Batch 3/64 loss: -0.06377542018890381
Batch 4/64 loss: -0.03965073823928833
Batch 5/64 loss: -0.05285453796386719
Batch 6/64 loss: -0.021208524703979492
Batch 7/64 loss: -0.04077041149139404
Batch 8/64 loss: -0.050818443298339844
Batch 9/64 loss: -0.028926968574523926
Batch 10/64 loss: -0.024929046630859375
Batch 11/64 loss: -0.039908766746520996
Batch 12/64 loss: -0.041153907775878906
Batch 13/64 loss: -0.03915983438491821
Batch 14/64 loss: -0.04679960012435913
Batch 15/64 loss: -0.058967530727386475
Batch 16/64 loss: -0.048914432525634766
Batch 17/64 loss: -0.034456491470336914
Batch 18/64 loss: -0.03783881664276123
Batch 19/64 loss: -0.03876680135726929
Batch 20/64 loss: -0.04376119375228882
Batch 21/64 loss: -0.04900026321411133
Batch 22/64 loss: -0.046795010566711426
Batch 23/64 loss: -0.03357630968093872
Batch 24/64 loss: -0.02786332368850708
Batch 25/64 loss: -0.037700116634368896
Batch 26/64 loss: -0.06421637535095215
Batch 27/64 loss: -0.03796041011810303
Batch 28/64 loss: -0.05167579650878906
Batch 29/64 loss: -0.03904670476913452
Batch 30/64 loss: -0.033130109310150146
Batch 31/64 loss: -0.020118534564971924
Batch 32/64 loss: -0.018979549407958984
Batch 33/64 loss: -0.032382190227508545
Batch 34/64 loss: -0.040854692459106445
Batch 35/64 loss: -0.02542853355407715
Batch 36/64 loss: -0.0445597767829895
Batch 37/64 loss: -0.024457991123199463
Batch 38/64 loss: -0.03343653678894043
Batch 39/64 loss: -0.02260565757751465
Batch 40/64 loss: -0.03595626354217529
Batch 41/64 loss: -0.02838951349258423
Batch 42/64 loss: -0.03995215892791748
Batch 43/64 loss: -0.049720823764801025
Batch 44/64 loss: -0.04238098859786987
Batch 45/64 loss: -0.03606337308883667
Batch 46/64 loss: -0.0408174991607666
Batch 47/64 loss: -0.020522892475128174
Batch 48/64 loss: -0.03650057315826416
Batch 49/64 loss: -0.009713053703308105
Batch 50/64 loss: 0.0005054473876953125
Batch 51/64 loss: -0.052330970764160156
Batch 52/64 loss: -0.050496459007263184
Batch 53/64 loss: -0.033104538917541504
Batch 54/64 loss: -0.031552791595458984
Batch 55/64 loss: -0.037637948989868164
Batch 56/64 loss: -0.032893478870391846
Batch 57/64 loss: -0.02199709415435791
Batch 58/64 loss: -0.0426560640335083
Batch 59/64 loss: -0.045588016510009766
Batch 60/64 loss: -0.03305315971374512
Batch 61/64 loss: -0.029451966285705566
Batch 62/64 loss: -0.031043410301208496
Batch 63/64 loss: -0.02921271324157715
Batch 64/64 loss: -0.045333266258239746
Epoch 98  Train loss: -0.03729503154754639  Val loss: -0.008956172621946564
Epoch 99
-------------------------------
Batch 1/64 loss: -0.04782211780548096
Batch 2/64 loss: -0.03397369384765625
Batch 3/64 loss: -0.04209327697753906
Batch 4/64 loss: -0.043559908866882324
Batch 5/64 loss: -0.027256429195404053
Batch 6/64 loss: -0.04653167724609375
Batch 7/64 loss: -0.03349238634109497
Batch 8/64 loss: -0.04283595085144043
Batch 9/64 loss: -0.038818955421447754
Batch 10/64 loss: -0.04878515005111694
Batch 11/64 loss: -0.051507532596588135
Batch 12/64 loss: -0.039224207401275635
Batch 13/64 loss: -0.041399240493774414
Batch 14/64 loss: -0.047965288162231445
Batch 15/64 loss: -0.012949228286743164
Batch 16/64 loss: -0.06491255760192871
Batch 17/64 loss: -0.040232717990875244
Batch 18/64 loss: -0.06032282114028931
Batch 19/64 loss: -0.038417041301727295
Batch 20/64 loss: -0.019663870334625244
Batch 21/64 loss: -0.049507975578308105
Batch 22/64 loss: -0.042091965675354004
Batch 23/64 loss: -0.022538423538208008
Batch 24/64 loss: -0.031486332416534424
Batch 25/64 loss: -0.04589498043060303
Batch 26/64 loss: -0.03227114677429199
Batch 27/64 loss: -0.04472535848617554
Batch 28/64 loss: -0.03264588117599487
Batch 29/64 loss: -0.04313623905181885
Batch 30/64 loss: -0.031630098819732666
Batch 31/64 loss: -0.03921663761138916
Batch 32/64 loss: -0.04364573955535889
Batch 33/64 loss: -0.03386443853378296
Batch 34/64 loss: -0.032034099102020264
Batch 35/64 loss: -0.022927522659301758
Batch 36/64 loss: -0.019544601440429688
Batch 37/64 loss: -0.032158732414245605
Batch 38/64 loss: -0.047548890113830566
Batch 39/64 loss: -0.04099142551422119
Batch 40/64 loss: -0.031198203563690186
Batch 41/64 loss: -0.028193175792694092
Batch 42/64 loss: -0.03484851121902466
Batch 43/64 loss: -0.028165042400360107
Batch 44/64 loss: -0.04265642166137695
Batch 45/64 loss: -0.04332613945007324
Batch 46/64 loss: -0.04199671745300293
Batch 47/64 loss: -0.04697388410568237
Batch 48/64 loss: -0.05040287971496582
Batch 49/64 loss: -0.04170185327529907
Batch 50/64 loss: -0.04218953847885132
Batch 51/64 loss: -0.04645341634750366
Batch 52/64 loss: -0.011942148208618164
Batch 53/64 loss: -0.04601401090621948
Batch 54/64 loss: -0.028076529502868652
Batch 55/64 loss: -0.04379010200500488
Batch 56/64 loss: -0.008449018001556396
Batch 57/64 loss: -0.04472541809082031
Batch 58/64 loss: -0.03378552198410034
Batch 59/64 loss: -0.04100966453552246
Batch 60/64 loss: -0.031294167041778564
Batch 61/64 loss: -0.02711087465286255
Batch 62/64 loss: -0.016532540321350098
Batch 63/64 loss: -0.06281864643096924
Batch 64/64 loss: -0.011913418769836426
Epoch 99  Train loss: -0.037524925961213956  Val loss: -0.007460515318867267
Epoch 100
-------------------------------
Batch 1/64 loss: -0.04781198501586914
Batch 2/64 loss: -0.058222413063049316
Batch 3/64 loss: -0.03596609830856323
Batch 4/64 loss: -0.042124807834625244
Batch 5/64 loss: -0.05458188056945801
Batch 6/64 loss: -0.042011141777038574
Batch 7/64 loss: -0.04555690288543701
Batch 8/64 loss: -0.04348522424697876
Batch 9/64 loss: -0.047841668128967285
Batch 10/64 loss: -0.06642669439315796
Batch 11/64 loss: -0.03438568115234375
Batch 12/64 loss: -0.045950353145599365
Batch 13/64 loss: -0.019867122173309326
Batch 14/64 loss: -0.05677211284637451
Batch 15/64 loss: -0.04435610771179199
Batch 16/64 loss: -0.043783605098724365
Batch 17/64 loss: -0.02549344301223755
Batch 18/64 loss: -0.034929633140563965
Batch 19/64 loss: -0.04807734489440918
Batch 20/64 loss: -0.03708076477050781
Batch 21/64 loss: -0.025726318359375
Batch 22/64 loss: -0.039875924587249756
Batch 23/64 loss: -0.05479252338409424
Batch 24/64 loss: -0.04450029134750366
Batch 25/64 loss: -0.012629032135009766
Batch 26/64 loss: -0.034896790981292725
Batch 27/64 loss: -0.025337815284729004
Batch 28/64 loss: -0.04468280076980591
Batch 29/64 loss: -0.02264946699142456
Batch 30/64 loss: -0.043010056018829346
Batch 31/64 loss: -0.02379465103149414
Batch 32/64 loss: -0.054185688495635986
Batch 33/64 loss: -0.05031782388687134
Batch 34/64 loss: -0.0446016788482666
Batch 35/64 loss: -0.047735631465911865
Batch 36/64 loss: -0.0377352237701416
Batch 37/64 loss: -0.035045504570007324
Batch 38/64 loss: -0.06197667121887207
Batch 39/64 loss: -0.045798420906066895
Batch 40/64 loss: -0.036649465560913086
Batch 41/64 loss: -0.04615950584411621
Batch 42/64 loss: -0.04214131832122803
Batch 43/64 loss: -0.03991258144378662
Batch 44/64 loss: -0.03831678628921509
Batch 45/64 loss: -0.030350089073181152
Batch 46/64 loss: -0.028094351291656494
Batch 47/64 loss: -0.019044220447540283
Batch 48/64 loss: -0.05027979612350464
Batch 49/64 loss: -0.016949832439422607
Batch 50/64 loss: -0.04250752925872803
Batch 51/64 loss: -0.04758107662200928
Batch 52/64 loss: -0.040409743785858154
Batch 53/64 loss: -0.035854458808898926
Batch 54/64 loss: -0.03466641902923584
Batch 55/64 loss: -0.025339126586914062
Batch 56/64 loss: -0.046513140201568604
Batch 57/64 loss: -0.030568718910217285
Batch 58/64 loss: -0.04079711437225342
Batch 59/64 loss: -0.041226208209991455
Batch 60/64 loss: -0.04435157775878906
Batch 61/64 loss: -0.0599096417427063
Batch 62/64 loss: -0.027279198169708252
Batch 63/64 loss: -0.04668194055557251
Batch 64/64 loss: -0.04329788684844971
Epoch 100  Train loss: -0.040283522418901034  Val loss: -0.01796332114340923
Saving best model, epoch: 100
Epoch 101
-------------------------------
Batch 1/64 loss: -0.05899548530578613
Batch 2/64 loss: -0.03689754009246826
Batch 3/64 loss: -0.012705981731414795
Batch 4/64 loss: -0.03681999444961548
Batch 5/64 loss: -0.04049265384674072
Batch 6/64 loss: -0.05760812759399414
Batch 7/64 loss: -0.04445827007293701
Batch 8/64 loss: -0.03311187028884888
Batch 9/64 loss: -0.041938602924346924
Batch 10/64 loss: -0.04621016979217529
Batch 11/64 loss: -0.05866885185241699
Batch 12/64 loss: -0.055287957191467285
Batch 13/64 loss: -0.03063291311264038
Batch 14/64 loss: -0.0632825493812561
Batch 15/64 loss: -0.04943549633026123
Batch 16/64 loss: -0.029636502265930176
Batch 17/64 loss: -0.03704136610031128
Batch 18/64 loss: -0.06017017364501953
Batch 19/64 loss: -0.03656858205795288
Batch 20/64 loss: -0.04712098836898804
Batch 21/64 loss: -0.02311486005783081
Batch 22/64 loss: -0.04524177312850952
Batch 23/64 loss: -0.07286614179611206
Batch 24/64 loss: -0.031084895133972168
Batch 25/64 loss: -0.031171798706054688
Batch 26/64 loss: -0.05910539627075195
Batch 27/64 loss: -0.045585691928863525
Batch 28/64 loss: -0.05281001329421997
Batch 29/64 loss: -0.0454481840133667
Batch 30/64 loss: -0.0429115891456604
Batch 31/64 loss: -0.03795754909515381
Batch 32/64 loss: -0.03916740417480469
Batch 33/64 loss: -0.04774129390716553
Batch 34/64 loss: -0.04294031858444214
Batch 35/64 loss: -0.050400495529174805
Batch 36/64 loss: -0.05473971366882324
Batch 37/64 loss: -0.055742502212524414
Batch 38/64 loss: -0.04080933332443237
Batch 39/64 loss: -0.04036712646484375
Batch 40/64 loss: -0.05184638500213623
Batch 41/64 loss: -0.05545467138290405
Batch 42/64 loss: -0.039029717445373535
Batch 43/64 loss: -0.0598483681678772
Batch 44/64 loss: -0.019241154193878174
Batch 45/64 loss: -0.03076338768005371
Batch 46/64 loss: -0.046686410903930664
Batch 47/64 loss: -0.03926008939743042
Batch 48/64 loss: -0.054097652435302734
Batch 49/64 loss: -0.0349544882774353
Batch 50/64 loss: -0.0442204475402832
Batch 51/64 loss: -0.033040642738342285
Batch 52/64 loss: -0.03870910406112671
Batch 53/64 loss: -0.03997659683227539
Batch 54/64 loss: -0.0507512092590332
Batch 55/64 loss: -0.0559309720993042
Batch 56/64 loss: -0.05059623718261719
Batch 57/64 loss: -0.04883188009262085
Batch 58/64 loss: -0.03257393836975098
Batch 59/64 loss: -0.03375810384750366
Batch 60/64 loss: -0.03084772825241089
Batch 61/64 loss: -0.050405144691467285
Batch 62/64 loss: -0.0388181209564209
Batch 63/64 loss: -0.039944469928741455
Batch 64/64 loss: -0.041914522647857666
Epoch 101  Train loss: -0.04372255638533948  Val loss: -0.012403096325209051
Epoch 102
-------------------------------
Batch 1/64 loss: -0.05854213237762451
Batch 2/64 loss: -0.050717949867248535
Batch 3/64 loss: -0.05914705991744995
Batch 4/64 loss: -0.044256389141082764
Batch 5/64 loss: -0.05107158422470093
Batch 6/64 loss: -0.035343706607818604
Batch 7/64 loss: -0.04305899143218994
Batch 8/64 loss: -0.04600942134857178
Batch 9/64 loss: -0.011912822723388672
Batch 10/64 loss: -0.047042787075042725
Batch 11/64 loss: -0.035534441471099854
Batch 12/64 loss: -0.05428963899612427
Batch 13/64 loss: -0.03413522243499756
Batch 14/64 loss: -0.04644322395324707
Batch 15/64 loss: -0.0626416802406311
Batch 16/64 loss: -0.04389315843582153
Batch 17/64 loss: -0.04153931140899658
Batch 18/64 loss: -0.046127378940582275
Batch 19/64 loss: -0.05246490240097046
Batch 20/64 loss: -0.060786306858062744
Batch 21/64 loss: -0.0540618896484375
Batch 22/64 loss: -0.05608975887298584
Batch 23/64 loss: -0.05260586738586426
Batch 24/64 loss: -0.04412037134170532
Batch 25/64 loss: -0.06516826152801514
Batch 26/64 loss: -0.032544493675231934
Batch 27/64 loss: -0.051797330379486084
Batch 28/64 loss: -0.049768149852752686
Batch 29/64 loss: -0.026962101459503174
Batch 30/64 loss: -0.04593956470489502
Batch 31/64 loss: -0.05421161651611328
Batch 32/64 loss: -0.03291839361190796
Batch 33/64 loss: -0.038503408432006836
Batch 34/64 loss: -0.04671412706375122
Batch 35/64 loss: -0.0535663366317749
Batch 36/64 loss: -0.028348803520202637
Batch 37/64 loss: -0.024289190769195557
Batch 38/64 loss: -0.02260667085647583
Batch 39/64 loss: -0.0332607626914978
Batch 40/64 loss: -0.010641336441040039
Batch 41/64 loss: -0.04731476306915283
Batch 42/64 loss: -0.03241848945617676
Batch 43/64 loss: -0.04690665006637573
Batch 44/64 loss: -0.03898727893829346
Batch 45/64 loss: -0.05098789930343628
Batch 46/64 loss: -0.04353410005569458
Batch 47/64 loss: -0.04268753528594971
Batch 48/64 loss: -0.035640716552734375
Batch 49/64 loss: -0.04832977056503296
Batch 50/64 loss: -0.025598526000976562
Batch 51/64 loss: -0.051650285720825195
Batch 52/64 loss: -0.03926163911819458
Batch 53/64 loss: -0.04559779167175293
Batch 54/64 loss: -0.044940829277038574
Batch 55/64 loss: -0.04477345943450928
Batch 56/64 loss: -0.03183990716934204
Batch 57/64 loss: -0.049077749252319336
Batch 58/64 loss: -0.04879981279373169
Batch 59/64 loss: -0.033153533935546875
Batch 60/64 loss: -0.01878899335861206
Batch 61/64 loss: -0.02832108736038208
Batch 62/64 loss: -0.05162781476974487
Batch 63/64 loss: -0.034273743629455566
Batch 64/64 loss: -0.029166221618652344
Epoch 102  Train loss: -0.04243864452137667  Val loss: 0.00043336017844603235
Epoch 103
-------------------------------
Batch 1/64 loss: -0.0361899733543396
Batch 2/64 loss: -0.041184306144714355
Batch 3/64 loss: -0.03791689872741699
Batch 4/64 loss: -0.026160001754760742
Batch 5/64 loss: -0.04280376434326172
Batch 6/64 loss: -0.04395771026611328
Batch 7/64 loss: -0.04648691415786743
Batch 8/64 loss: -0.02817380428314209
Batch 9/64 loss: -0.05120569467544556
Batch 10/64 loss: -0.0531386137008667
Batch 11/64 loss: -0.030817389488220215
Batch 12/64 loss: -0.04560619592666626
Batch 13/64 loss: -0.04120141267776489
Batch 14/64 loss: -0.06177431344985962
Batch 15/64 loss: -0.02945232391357422
Batch 16/64 loss: -0.04576408863067627
Batch 17/64 loss: -0.03669935464859009
Batch 18/64 loss: -0.03222858905792236
Batch 19/64 loss: -0.04609203338623047
Batch 20/64 loss: -0.03927111625671387
Batch 21/64 loss: -0.03986024856567383
Batch 22/64 loss: -0.059094011783599854
Batch 23/64 loss: -0.028934597969055176
Batch 24/64 loss: -0.02770465612411499
Batch 25/64 loss: -0.0540003776550293
Batch 26/64 loss: -0.029586315155029297
Batch 27/64 loss: -0.023246347904205322
Batch 28/64 loss: -0.05232870578765869
Batch 29/64 loss: -0.037787795066833496
Batch 30/64 loss: -0.04175323247909546
Batch 31/64 loss: -0.02947676181793213
Batch 32/64 loss: -0.040562212467193604
Batch 33/64 loss: -0.056108713150024414
Batch 34/64 loss: -0.04496443271636963
Batch 35/64 loss: -0.056075215339660645
Batch 36/64 loss: -0.05546927452087402
Batch 37/64 loss: -0.05639028549194336
Batch 38/64 loss: -0.05332052707672119
Batch 39/64 loss: -0.05438792705535889
Batch 40/64 loss: -0.04427969455718994
Batch 41/64 loss: -0.052255094051361084
Batch 42/64 loss: -0.04440796375274658
Batch 43/64 loss: -0.02167332172393799
Batch 44/64 loss: -0.04805099964141846
Batch 45/64 loss: -0.009287536144256592
Batch 46/64 loss: -0.03850686550140381
Batch 47/64 loss: -0.03898841142654419
Batch 48/64 loss: -0.028994977474212646
Batch 49/64 loss: -0.035869717597961426
Batch 50/64 loss: -0.03860604763031006
Batch 51/64 loss: -0.044932007789611816
Batch 52/64 loss: -0.0536799430847168
Batch 53/64 loss: -0.028673887252807617
Batch 54/64 loss: -0.04838371276855469
Batch 55/64 loss: -0.037798404693603516
Batch 56/64 loss: -0.043223679065704346
Batch 57/64 loss: -0.024269521236419678
Batch 58/64 loss: -0.033747076988220215
Batch 59/64 loss: -0.026728451251983643
Batch 60/64 loss: -0.013667464256286621
Batch 61/64 loss: -0.030890822410583496
Batch 62/64 loss: -0.037307918071746826
Batch 63/64 loss: -0.04339718818664551
Batch 64/64 loss: -0.04645836353302002
Epoch 103  Train loss: -0.04015122535181981  Val loss: -0.01305591896227545
Epoch 104
-------------------------------
Batch 1/64 loss: -0.04943215847015381
Batch 2/64 loss: -0.029457032680511475
Batch 3/64 loss: -0.04259765148162842
Batch 4/64 loss: -0.036998629570007324
Batch 5/64 loss: -0.030620157718658447
Batch 6/64 loss: -0.028067171573638916
Batch 7/64 loss: -0.02849632501602173
Batch 8/64 loss: -0.03195822238922119
Batch 9/64 loss: -0.0525059700012207
Batch 10/64 loss: -0.03462183475494385
Batch 11/64 loss: -0.028939545154571533
Batch 12/64 loss: -0.04383587837219238
Batch 13/64 loss: -0.03894728422164917
Batch 14/64 loss: -0.03490406274795532
Batch 15/64 loss: -0.020332813262939453
Batch 16/64 loss: -0.06313914060592651
Batch 17/64 loss: -0.05082780122756958
Batch 18/64 loss: -0.052079200744628906
Batch 19/64 loss: -0.03962099552154541
Batch 20/64 loss: -0.04227173328399658
Batch 21/64 loss: -0.04922676086425781
Batch 22/64 loss: -0.0399932861328125
Batch 23/64 loss: -0.05593883991241455
Batch 24/64 loss: -0.0518762469291687
Batch 25/64 loss: -0.04835927486419678
Batch 26/64 loss: -0.04163825511932373
Batch 27/64 loss: -0.04934501647949219
Batch 28/64 loss: -0.041075825691223145
Batch 29/64 loss: -0.03602355718612671
Batch 30/64 loss: -0.04696798324584961
Batch 31/64 loss: -0.05341541767120361
Batch 32/64 loss: -0.0417712926864624
Batch 33/64 loss: -0.03177475929260254
Batch 34/64 loss: -0.04653751850128174
Batch 35/64 loss: -0.050884127616882324
Batch 36/64 loss: -0.042232632637023926
Batch 37/64 loss: -0.051491379737854004
Batch 38/64 loss: -0.04217195510864258
Batch 39/64 loss: -0.024628937244415283
Batch 40/64 loss: -0.04770785570144653
Batch 41/64 loss: -0.04198718070983887
Batch 42/64 loss: -0.04125553369522095
Batch 43/64 loss: -0.037671446800231934
Batch 44/64 loss: -0.049936652183532715
Batch 45/64 loss: -0.054177045822143555
Batch 46/64 loss: -0.043473005294799805
Batch 47/64 loss: -0.025017619132995605
Batch 48/64 loss: -0.05025714635848999
Batch 49/64 loss: -0.018332719802856445
Batch 50/64 loss: -0.0480426549911499
Batch 51/64 loss: -0.07192325592041016
Batch 52/64 loss: -0.0417901873588562
Batch 53/64 loss: -0.018941283226013184
Batch 54/64 loss: -0.012081384658813477
Batch 55/64 loss: -0.03160965442657471
Batch 56/64 loss: -0.031848788261413574
Batch 57/64 loss: -0.05545186996459961
Batch 58/64 loss: -0.04410499334335327
Batch 59/64 loss: -0.04417949914932251
Batch 60/64 loss: -0.05097109079360962
Batch 61/64 loss: -0.05849260091781616
Batch 62/64 loss: -0.048414528369903564
Batch 63/64 loss: -0.039144158363342285
Batch 64/64 loss: -0.06070685386657715
Epoch 104  Train loss: -0.04199763092340208  Val loss: -0.019560458733863438
Saving best model, epoch: 104
Epoch 105
-------------------------------
Batch 1/64 loss: -0.06276047229766846
Batch 2/64 loss: -0.04965519905090332
Batch 3/64 loss: -0.05043405294418335
Batch 4/64 loss: -0.04000401496887207
Batch 5/64 loss: -0.0479925274848938
Batch 6/64 loss: -0.050186753273010254
Batch 7/64 loss: -0.04386460781097412
Batch 8/64 loss: -0.03787839412689209
Batch 9/64 loss: -0.05451953411102295
Batch 10/64 loss: -0.05730390548706055
Batch 11/64 loss: -0.059733569622039795
Batch 12/64 loss: -0.06136661767959595
Batch 13/64 loss: -0.04876554012298584
Batch 14/64 loss: -0.039143383502960205
Batch 15/64 loss: -0.04953658580780029
Batch 16/64 loss: -0.054973065853118896
Batch 17/64 loss: -0.04349130392074585
Batch 18/64 loss: -0.04473161697387695
Batch 19/64 loss: -0.03835725784301758
Batch 20/64 loss: -0.05303674936294556
Batch 21/64 loss: -0.049690961837768555
Batch 22/64 loss: -0.06350487470626831
Batch 23/64 loss: -0.04635167121887207
Batch 24/64 loss: -0.03488171100616455
Batch 25/64 loss: -0.05584019422531128
Batch 26/64 loss: -0.03698241710662842
Batch 27/64 loss: -0.018105506896972656
Batch 28/64 loss: -0.05096989870071411
Batch 29/64 loss: -0.05254322290420532
Batch 30/64 loss: -0.047058165073394775
Batch 31/64 loss: -0.04113727807998657
Batch 32/64 loss: -0.03765058517456055
Batch 33/64 loss: -0.0473933219909668
Batch 34/64 loss: -0.05174458026885986
Batch 35/64 loss: -0.0268096923828125
Batch 36/64 loss: -0.05243241786956787
Batch 37/64 loss: -0.04748833179473877
Batch 38/64 loss: -0.052337646484375
Batch 39/64 loss: -0.04387247562408447
Batch 40/64 loss: -0.02921241521835327
Batch 41/64 loss: -0.06329303979873657
Batch 42/64 loss: -0.045401930809020996
Batch 43/64 loss: -0.05612987279891968
Batch 44/64 loss: -0.049073100090026855
Batch 45/64 loss: -0.03139132261276245
Batch 46/64 loss: -0.042699456214904785
Batch 47/64 loss: -0.060099005699157715
Batch 48/64 loss: -0.05505949258804321
Batch 49/64 loss: -0.04526406526565552
Batch 50/64 loss: -0.03237098455429077
Batch 51/64 loss: -0.041304945945739746
Batch 52/64 loss: -0.04094964265823364
Batch 53/64 loss: -0.051609039306640625
Batch 54/64 loss: -0.060382962226867676
Batch 55/64 loss: -0.03477656841278076
Batch 56/64 loss: -0.04605311155319214
Batch 57/64 loss: -0.043592095375061035
Batch 58/64 loss: -0.027679800987243652
Batch 59/64 loss: -0.039750874042510986
Batch 60/64 loss: -0.01755315065383911
Batch 61/64 loss: -0.028981685638427734
Batch 62/64 loss: -0.03847223520278931
Batch 63/64 loss: -0.03931540250778198
Batch 64/64 loss: -0.04409170150756836
Epoch 105  Train loss: -0.04545906010796042  Val loss: -0.015916891319235574
Epoch 106
-------------------------------
Batch 1/64 loss: -0.05871230363845825
Batch 2/64 loss: -0.027852416038513184
Batch 3/64 loss: -0.04735362529754639
Batch 4/64 loss: -0.03477734327316284
Batch 5/64 loss: -0.054805099964141846
Batch 6/64 loss: -0.020951390266418457
Batch 7/64 loss: -0.04910987615585327
Batch 8/64 loss: -0.04517829418182373
Batch 9/64 loss: -0.05472284555435181
Batch 10/64 loss: -0.05166119337081909
Batch 11/64 loss: -0.053172945976257324
Batch 12/64 loss: -0.05073881149291992
Batch 13/64 loss: -0.05758094787597656
Batch 14/64 loss: -0.04059642553329468
Batch 15/64 loss: -0.05487489700317383
Batch 16/64 loss: -0.04369843006134033
Batch 17/64 loss: -0.05879366397857666
Batch 18/64 loss: -0.04878842830657959
Batch 19/64 loss: -0.0543096661567688
Batch 20/64 loss: -0.04402029514312744
Batch 21/64 loss: -0.0415918231010437
Batch 22/64 loss: -0.04643815755844116
Batch 23/64 loss: -0.042600810527801514
Batch 24/64 loss: -0.0543743371963501
Batch 25/64 loss: -0.05195647478103638
Batch 26/64 loss: -0.030851781368255615
Batch 27/64 loss: -0.04316592216491699
Batch 28/64 loss: -0.04033142328262329
Batch 29/64 loss: -0.03740489482879639
Batch 30/64 loss: -0.03701281547546387
Batch 31/64 loss: -0.04634219408035278
Batch 32/64 loss: -0.050144851207733154
Batch 33/64 loss: -0.04414987564086914
Batch 34/64 loss: -0.043536484241485596
Batch 35/64 loss: -0.04974377155303955
Batch 36/64 loss: -0.03711438179016113
Batch 37/64 loss: -0.06140357255935669
Batch 38/64 loss: -0.05582594871520996
Batch 39/64 loss: -0.04145002365112305
Batch 40/64 loss: -0.06510412693023682
Batch 41/64 loss: -0.06226986646652222
Batch 42/64 loss: -0.05050784349441528
Batch 43/64 loss: -0.03504836559295654
Batch 44/64 loss: -0.0518038272857666
Batch 45/64 loss: -0.028064489364624023
Batch 46/64 loss: -0.06918996572494507
Batch 47/64 loss: -0.05634927749633789
Batch 48/64 loss: -0.0330737829208374
Batch 49/64 loss: -0.029520034790039062
Batch 50/64 loss: -0.06554698944091797
Batch 51/64 loss: -0.039705753326416016
Batch 52/64 loss: -0.050496816635131836
Batch 53/64 loss: -0.05662649869918823
Batch 54/64 loss: -0.055219054222106934
Batch 55/64 loss: -0.04614073038101196
Batch 56/64 loss: -0.06082725524902344
Batch 57/64 loss: -0.017969965934753418
Batch 58/64 loss: -0.03245341777801514
Batch 59/64 loss: -0.05430489778518677
Batch 60/64 loss: -0.05448508262634277
Batch 61/64 loss: -0.03443390130996704
Batch 62/64 loss: -0.05180215835571289
Batch 63/64 loss: -0.04975932836532593
Batch 64/64 loss: -0.0591357946395874
Epoch 106  Train loss: -0.04709323481017468  Val loss: -0.020983219146728516
Saving best model, epoch: 106
Epoch 107
-------------------------------
Batch 1/64 loss: -0.046367764472961426
Batch 2/64 loss: -0.054005324840545654
Batch 3/64 loss: -0.04533761739730835
Batch 4/64 loss: -0.04371142387390137
Batch 5/64 loss: -0.04604816436767578
Batch 6/64 loss: -0.06400299072265625
Batch 7/64 loss: -0.05164980888366699
Batch 8/64 loss: -0.026380300521850586
Batch 9/64 loss: -0.0559617280960083
Batch 10/64 loss: -0.05194532871246338
Batch 11/64 loss: -0.05268895626068115
Batch 12/64 loss: -0.04692208766937256
Batch 13/64 loss: -0.0393180251121521
Batch 14/64 loss: -0.040332913398742676
Batch 15/64 loss: -0.040846049785614014
Batch 16/64 loss: -0.030114948749542236
Batch 17/64 loss: -0.06297874450683594
Batch 18/64 loss: -0.05463850498199463
Batch 19/64 loss: -0.0466882586479187
Batch 20/64 loss: -0.059384167194366455
Batch 21/64 loss: -0.057177960872650146
Batch 22/64 loss: -0.04173684120178223
Batch 23/64 loss: -0.0684543251991272
Batch 24/64 loss: -0.027217566967010498
Batch 25/64 loss: -0.04268902540206909
Batch 26/64 loss: -0.05834078788757324
Batch 27/64 loss: -0.051407456398010254
Batch 28/64 loss: -0.04554373025894165
Batch 29/64 loss: -0.053028762340545654
Batch 30/64 loss: -0.07179397344589233
Batch 31/64 loss: -0.05278325080871582
Batch 32/64 loss: -0.07575356960296631
Batch 33/64 loss: -0.04057985544204712
Batch 34/64 loss: -0.042545974254608154
Batch 35/64 loss: -0.03715848922729492
Batch 36/64 loss: -0.037698566913604736
Batch 37/64 loss: -0.04699438810348511
Batch 38/64 loss: -0.06051057577133179
Batch 39/64 loss: -0.047121286392211914
Batch 40/64 loss: -0.06163305044174194
Batch 41/64 loss: -0.04363781213760376
Batch 42/64 loss: -0.04300051927566528
Batch 43/64 loss: -0.05182236433029175
Batch 44/64 loss: -0.04883468151092529
Batch 45/64 loss: -0.03990769386291504
Batch 46/64 loss: -0.030344069004058838
Batch 47/64 loss: -0.06662428379058838
Batch 48/64 loss: -0.04928535223007202
Batch 49/64 loss: -0.06353753805160522
Batch 50/64 loss: -0.043890416622161865
Batch 51/64 loss: -0.03939855098724365
Batch 52/64 loss: -0.0455174446105957
Batch 53/64 loss: -0.04774737358093262
Batch 54/64 loss: -0.05075204372406006
Batch 55/64 loss: -0.04798305034637451
Batch 56/64 loss: -0.05397087335586548
Batch 57/64 loss: -0.05722016096115112
Batch 58/64 loss: -0.05125075578689575
Batch 59/64 loss: -0.041168034076690674
Batch 60/64 loss: -0.03437626361846924
Batch 61/64 loss: -0.0503501296043396
Batch 62/64 loss: -0.0342637300491333
Batch 63/64 loss: -0.008467495441436768
Batch 64/64 loss: -0.036859333515167236
Epoch 107  Train loss: -0.04785078717213051  Val loss: -0.019900914934492604
Epoch 108
-------------------------------
Batch 1/64 loss: -0.02447056770324707
Batch 2/64 loss: -0.029597759246826172
Batch 3/64 loss: -0.04985928535461426
Batch 4/64 loss: -0.06402504444122314
Batch 5/64 loss: -0.039356112480163574
Batch 6/64 loss: -0.052083730697631836
Batch 7/64 loss: -0.04605841636657715
Batch 8/64 loss: -0.056327223777770996
Batch 9/64 loss: -0.040825605392456055
Batch 10/64 loss: -0.0449671745300293
Batch 11/64 loss: -0.05865544080734253
Batch 12/64 loss: -0.05181610584259033
Batch 13/64 loss: -0.0481603741645813
Batch 14/64 loss: -0.048705220222473145
Batch 15/64 loss: -0.05622553825378418
Batch 16/64 loss: -0.05405163764953613
Batch 17/64 loss: -0.05008196830749512
Batch 18/64 loss: -0.05209827423095703
Batch 19/64 loss: -0.049563705921173096
Batch 20/64 loss: -0.04461169242858887
Batch 21/64 loss: -0.043531060218811035
Batch 22/64 loss: -0.04502081871032715
Batch 23/64 loss: -0.04160672426223755
Batch 24/64 loss: -0.062885582447052
Batch 25/64 loss: -0.016915202140808105
Batch 26/64 loss: -0.05137741565704346
Batch 27/64 loss: -0.05757051706314087
Batch 28/64 loss: -0.05692821741104126
Batch 29/64 loss: -0.0353928804397583
Batch 30/64 loss: -0.05821889638900757
Batch 31/64 loss: -0.03901565074920654
Batch 32/64 loss: -0.05884087085723877
Batch 33/64 loss: -0.03412508964538574
Batch 34/64 loss: -0.03458040952682495
Batch 35/64 loss: -0.0600506067276001
Batch 36/64 loss: -0.06812489032745361
Batch 37/64 loss: -0.026500344276428223
Batch 38/64 loss: -0.030260324478149414
Batch 39/64 loss: -0.04462265968322754
Batch 40/64 loss: -0.026242852210998535
Batch 41/64 loss: -0.043087899684906006
Batch 42/64 loss: -0.031302452087402344
Batch 43/64 loss: -0.02163141965866089
Batch 44/64 loss: -0.0432438850402832
Batch 45/64 loss: -0.05486327409744263
Batch 46/64 loss: -0.04148697853088379
Batch 47/64 loss: -0.04130369424819946
Batch 48/64 loss: -0.04253464937210083
Batch 49/64 loss: -0.0368923544883728
Batch 50/64 loss: -0.060349345207214355
Batch 51/64 loss: -0.056690335273742676
Batch 52/64 loss: -0.054840683937072754
Batch 53/64 loss: -0.0479159951210022
Batch 54/64 loss: -0.03254365921020508
Batch 55/64 loss: -0.05040973424911499
Batch 56/64 loss: -0.03407818078994751
Batch 57/64 loss: -0.0331425666809082
Batch 58/64 loss: -0.0631597638130188
Batch 59/64 loss: -0.05977052450180054
Batch 60/64 loss: -0.05092573165893555
Batch 61/64 loss: -0.041360676288604736
Batch 62/64 loss: -0.024523019790649414
Batch 63/64 loss: -0.03908979892730713
Batch 64/64 loss: -0.0540582537651062
Epoch 108  Train loss: -0.045475171827802474  Val loss: -0.014140537104655787
Epoch 109
-------------------------------
Batch 1/64 loss: -0.05119204521179199
Batch 2/64 loss: -0.053766608238220215
Batch 3/64 loss: -0.04581868648529053
Batch 4/64 loss: -0.0388181209564209
Batch 5/64 loss: -0.04270678758621216
Batch 6/64 loss: -0.06337243318557739
Batch 7/64 loss: -0.05037122964859009
Batch 8/64 loss: -0.0494462251663208
Batch 9/64 loss: -0.06028622388839722
Batch 10/64 loss: -0.04637414216995239
Batch 11/64 loss: -0.04893672466278076
Batch 12/64 loss: -0.03307795524597168
Batch 13/64 loss: -0.048015713691711426
Batch 14/64 loss: -0.039566874504089355
Batch 15/64 loss: -0.03646135330200195
Batch 16/64 loss: -0.05218607187271118
Batch 17/64 loss: -0.008036494255065918
Batch 18/64 loss: -0.044760704040527344
Batch 19/64 loss: -0.04313862323760986
Batch 20/64 loss: -0.040510475635528564
Batch 21/64 loss: -0.04937911033630371
Batch 22/64 loss: -0.030968666076660156
Batch 23/64 loss: -0.03229022026062012
Batch 24/64 loss: -0.04170107841491699
Batch 25/64 loss: -0.013993799686431885
Batch 26/64 loss: -0.034577786922454834
Batch 27/64 loss: -0.05560272932052612
Batch 28/64 loss: -0.04892450571060181
Batch 29/64 loss: -0.04154390096664429
Batch 30/64 loss: -0.025864779949188232
Batch 31/64 loss: -0.0629165768623352
Batch 32/64 loss: -0.02664083242416382
Batch 33/64 loss: -0.047722041606903076
Batch 34/64 loss: -0.061117589473724365
Batch 35/64 loss: -0.056789517402648926
Batch 36/64 loss: -0.02385425567626953
Batch 37/64 loss: -0.04316294193267822
Batch 38/64 loss: -0.03617733716964722
Batch 39/64 loss: -0.03403264284133911
Batch 40/64 loss: -0.0378686785697937
Batch 41/64 loss: -0.04183542728424072
Batch 42/64 loss: -0.012083768844604492
Batch 43/64 loss: -0.029373228549957275
Batch 44/64 loss: -0.03178638219833374
Batch 45/64 loss: -0.04700911045074463
Batch 46/64 loss: -0.04453837871551514
Batch 47/64 loss: -0.03154522180557251
Batch 48/64 loss: -0.051509320735931396
Batch 49/64 loss: -0.0027298927307128906
Batch 50/64 loss: -0.038436710834503174
Batch 51/64 loss: -0.053653836250305176
Batch 52/64 loss: -0.04702097177505493
Batch 53/64 loss: -0.04640829563140869
Batch 54/64 loss: -0.03339153528213501
Batch 55/64 loss: -0.05037504434585571
Batch 56/64 loss: -0.059316039085388184
Batch 57/64 loss: -0.042629897594451904
Batch 58/64 loss: -0.050044894218444824
Batch 59/64 loss: -0.05713546276092529
Batch 60/64 loss: -0.04099631309509277
Batch 61/64 loss: -0.03129345178604126
Batch 62/64 loss: -0.05456280708312988
Batch 63/64 loss: -0.043283700942993164
Batch 64/64 loss: -0.0420689582824707
Epoch 109  Train loss: -0.041952688553754024  Val loss: -0.013138753032356603
Epoch 110
-------------------------------
Batch 1/64 loss: -0.04682326316833496
Batch 2/64 loss: -0.03161263465881348
Batch 3/64 loss: -0.04954063892364502
Batch 4/64 loss: -0.030112385749816895
Batch 5/64 loss: -0.05452185869216919
Batch 6/64 loss: -0.027778148651123047
Batch 7/64 loss: -0.05524849891662598
Batch 8/64 loss: -0.036187589168548584
Batch 9/64 loss: -0.06215035915374756
Batch 10/64 loss: -0.04309189319610596
Batch 11/64 loss: -0.038156867027282715
Batch 12/64 loss: -0.04344594478607178
Batch 13/64 loss: -0.05227971076965332
Batch 14/64 loss: -0.03632992506027222
Batch 15/64 loss: -0.047930240631103516
Batch 16/64 loss: -0.012136399745941162
Batch 17/64 loss: -0.03571128845214844
Batch 18/64 loss: -0.05224108695983887
Batch 19/64 loss: -0.027430832386016846
Batch 20/64 loss: -0.045657217502593994
Batch 21/64 loss: -0.05164152383804321
Batch 22/64 loss: -0.05654323101043701
Batch 23/64 loss: -0.056497275829315186
Batch 24/64 loss: -0.06256246566772461
Batch 25/64 loss: -0.060315489768981934
Batch 26/64 loss: -0.05664825439453125
Batch 27/64 loss: -0.05472981929779053
Batch 28/64 loss: -0.0639878511428833
Batch 29/64 loss: -0.04892367124557495
Batch 30/64 loss: -0.06198537349700928
Batch 31/64 loss: -0.06721973419189453
Batch 32/64 loss: -0.06864672899246216
Batch 33/64 loss: -0.04147934913635254
Batch 34/64 loss: -0.03289979696273804
Batch 35/64 loss: -0.033859431743621826
Batch 36/64 loss: -0.03783136606216431
Batch 37/64 loss: -0.03007984161376953
Batch 38/64 loss: -0.051964521408081055
Batch 39/64 loss: -0.045478105545043945
Batch 40/64 loss: -0.05781972408294678
Batch 41/64 loss: -0.02794712781906128
Batch 42/64 loss: -0.05354201793670654
Batch 43/64 loss: -0.03651118278503418
Batch 44/64 loss: -0.040164828300476074
Batch 45/64 loss: -0.05971449613571167
Batch 46/64 loss: -0.06170231103897095
Batch 47/64 loss: -0.05092579126358032
Batch 48/64 loss: -0.04947531223297119
Batch 49/64 loss: -0.05883443355560303
Batch 50/64 loss: -0.05196964740753174
Batch 51/64 loss: -0.055519282817840576
Batch 52/64 loss: -0.04683399200439453
Batch 53/64 loss: -0.06942003965377808
Batch 54/64 loss: -0.05629390478134155
Batch 55/64 loss: -0.05291718244552612
Batch 56/64 loss: -0.054003238677978516
Batch 57/64 loss: -0.044502437114715576
Batch 58/64 loss: -0.036593616008758545
Batch 59/64 loss: -0.04517453908920288
Batch 60/64 loss: -0.05281996726989746
Batch 61/64 loss: -0.05718493461608887
Batch 62/64 loss: -0.04957634210586548
Batch 63/64 loss: -0.04278677701950073
Batch 64/64 loss: -0.04609304666519165
Epoch 110  Train loss: -0.047976212174284695  Val loss: -0.018029677294373922
Epoch 111
-------------------------------
Batch 1/64 loss: -0.051544249057769775
Batch 2/64 loss: -0.06255525350570679
Batch 3/64 loss: -0.05163776874542236
Batch 4/64 loss: -0.06465280055999756
Batch 5/64 loss: -0.06306064128875732
Batch 6/64 loss: -0.056961894035339355
Batch 7/64 loss: -0.046565234661102295
Batch 8/64 loss: -0.03721529245376587
Batch 9/64 loss: -0.058130085468292236
Batch 10/64 loss: -0.04358035326004028
Batch 11/64 loss: -0.042170822620391846
Batch 12/64 loss: -0.05308878421783447
Batch 13/64 loss: -0.058379292488098145
Batch 14/64 loss: -0.058394432067871094
Batch 15/64 loss: -0.0709417462348938
Batch 16/64 loss: -0.0554925799369812
Batch 17/64 loss: -0.050481975078582764
Batch 18/64 loss: -0.04464375972747803
Batch 19/64 loss: -0.024441063404083252
Batch 20/64 loss: -0.05842423439025879
Batch 21/64 loss: -0.061234116554260254
Batch 22/64 loss: -0.04044121503829956
Batch 23/64 loss: -0.053887248039245605
Batch 24/64 loss: -0.03872978687286377
Batch 25/64 loss: -0.02335512638092041
Batch 26/64 loss: -0.030073046684265137
Batch 27/64 loss: -0.0491369366645813
Batch 28/64 loss: -0.052040815353393555
Batch 29/64 loss: -0.0522955060005188
Batch 30/64 loss: -0.01831156015396118
Batch 31/64 loss: -0.0504070520401001
Batch 32/64 loss: -0.05077922344207764
Batch 33/64 loss: -0.054105520248413086
Batch 34/64 loss: -0.04210031032562256
Batch 35/64 loss: -0.0545387864112854
Batch 36/64 loss: -0.03986746072769165
Batch 37/64 loss: -0.061836421489715576
Batch 38/64 loss: -0.06256532669067383
Batch 39/64 loss: -0.052629828453063965
Batch 40/64 loss: -0.03850388526916504
Batch 41/64 loss: -0.04096752405166626
Batch 42/64 loss: -0.06116509437561035
Batch 43/64 loss: -0.05324000120162964
Batch 44/64 loss: -0.061145246028900146
Batch 45/64 loss: -0.035217463970184326
Batch 46/64 loss: -0.02078765630722046
Batch 47/64 loss: -0.04229378700256348
Batch 48/64 loss: -0.0520971417427063
Batch 49/64 loss: -0.059899210929870605
Batch 50/64 loss: -0.02412271499633789
Batch 51/64 loss: -0.056415557861328125
Batch 52/64 loss: -0.053593456745147705
Batch 53/64 loss: -0.054676353931427
Batch 54/64 loss: -0.058360159397125244
Batch 55/64 loss: -0.06497764587402344
Batch 56/64 loss: -0.06402015686035156
Batch 57/64 loss: -0.05301022529602051
Batch 58/64 loss: -0.06257092952728271
Batch 59/64 loss: -0.05300062894821167
Batch 60/64 loss: -0.03908395767211914
Batch 61/64 loss: -0.043680012226104736
Batch 62/64 loss: -0.05145871639251709
Batch 63/64 loss: -0.05330580472946167
Batch 64/64 loss: -0.0261383056640625
Epoch 111  Train loss: -0.04953560174680224  Val loss: -0.023321539266002958
Saving best model, epoch: 111
Epoch 112
-------------------------------
Batch 1/64 loss: -0.05550253391265869
Batch 2/64 loss: -0.047892868518829346
Batch 3/64 loss: -0.04733693599700928
Batch 4/64 loss: -0.04909580945968628
Batch 5/64 loss: -0.041667938232421875
Batch 6/64 loss: -0.05921578407287598
Batch 7/64 loss: -0.0619807243347168
Batch 8/64 loss: -0.042683303356170654
Batch 9/64 loss: -0.037893593311309814
Batch 10/64 loss: -0.05735868215560913
Batch 11/64 loss: -0.05782073736190796
Batch 12/64 loss: -0.03975039720535278
Batch 13/64 loss: -0.0659174919128418
Batch 14/64 loss: -0.06887960433959961
Batch 15/64 loss: -0.06907373666763306
Batch 16/64 loss: -0.06259697675704956
Batch 17/64 loss: -0.03792613744735718
Batch 18/64 loss: -0.0516657829284668
Batch 19/64 loss: -0.06849217414855957
Batch 20/64 loss: -0.05274313688278198
Batch 21/64 loss: -0.05350315570831299
Batch 22/64 loss: -0.04564422369003296
Batch 23/64 loss: -0.05430293083190918
Batch 24/64 loss: -0.049921274185180664
Batch 25/64 loss: -0.049947917461395264
Batch 26/64 loss: -0.04953956604003906
Batch 27/64 loss: -0.04607337713241577
Batch 28/64 loss: -0.044925034046173096
Batch 29/64 loss: -0.0515667200088501
Batch 30/64 loss: -0.04899036884307861
Batch 31/64 loss: -0.034653544425964355
Batch 32/64 loss: -0.05746722221374512
Batch 33/64 loss: -0.05277794599533081
Batch 34/64 loss: -0.052641212940216064
Batch 35/64 loss: -0.05032145977020264
Batch 36/64 loss: -0.05579638481140137
Batch 37/64 loss: -0.049645841121673584
Batch 38/64 loss: -0.031179726123809814
Batch 39/64 loss: -0.04079139232635498
Batch 40/64 loss: -0.06343686580657959
Batch 41/64 loss: -0.03755831718444824
Batch 42/64 loss: -0.046922504901885986
Batch 43/64 loss: -0.05536407232284546
Batch 44/64 loss: -0.05034005641937256
Batch 45/64 loss: -0.05995994806289673
Batch 46/64 loss: -0.06718778610229492
Batch 47/64 loss: -0.05237984657287598
Batch 48/64 loss: -0.027557432651519775
Batch 49/64 loss: -0.03955584764480591
Batch 50/64 loss: -0.03242778778076172
Batch 51/64 loss: -0.04088401794433594
Batch 52/64 loss: -0.03419309854507446
Batch 53/64 loss: -0.07400190830230713
Batch 54/64 loss: -0.039473772048950195
Batch 55/64 loss: -0.0582999587059021
Batch 56/64 loss: -0.06250107288360596
Batch 57/64 loss: -0.053469836711883545
Batch 58/64 loss: -0.05791693925857544
Batch 59/64 loss: -0.04764151573181152
Batch 60/64 loss: -0.04409587383270264
Batch 61/64 loss: -0.056649744510650635
Batch 62/64 loss: -0.03969907760620117
Batch 63/64 loss: -0.061975717544555664
Batch 64/64 loss: -0.058545053005218506
Epoch 112  Train loss: -0.050864085262896964  Val loss: -0.01799442391215321
Epoch 113
-------------------------------
Batch 1/64 loss: -0.04403340816497803
Batch 2/64 loss: -0.049530029296875
Batch 3/64 loss: -0.058562278747558594
Batch 4/64 loss: -0.07286310195922852
Batch 5/64 loss: -0.05251199007034302
Batch 6/64 loss: -0.04134535789489746
Batch 7/64 loss: -0.05300557613372803
Batch 8/64 loss: -0.07258862257003784
Batch 9/64 loss: -0.06413918733596802
Batch 10/64 loss: -0.05718463659286499
Batch 11/64 loss: -0.04492992162704468
Batch 12/64 loss: -0.02197849750518799
Batch 13/64 loss: -0.06576228141784668
Batch 14/64 loss: -0.05124044418334961
Batch 15/64 loss: -0.05017977952957153
Batch 16/64 loss: -0.0433613657951355
Batch 17/64 loss: -0.06987392902374268
Batch 18/64 loss: -0.06362789869308472
Batch 19/64 loss: -0.059728026390075684
Batch 20/64 loss: -0.020142853260040283
Batch 21/64 loss: -0.06335103511810303
Batch 22/64 loss: -0.0635496973991394
Batch 23/64 loss: -0.06226986646652222
Batch 24/64 loss: -0.05805373191833496
Batch 25/64 loss: -0.04956334829330444
Batch 26/64 loss: -0.055876970291137695
Batch 27/64 loss: -0.03521090745925903
Batch 28/64 loss: -0.046624720096588135
Batch 29/64 loss: -0.04652881622314453
Batch 30/64 loss: -0.05340111255645752
Batch 31/64 loss: -0.05878281593322754
Batch 32/64 loss: -0.04944753646850586
Batch 33/64 loss: -0.05746561288833618
Batch 34/64 loss: -0.0419040322303772
Batch 35/64 loss: -0.03960692882537842
Batch 36/64 loss: -0.04115074872970581
Batch 37/64 loss: -0.06175851821899414
Batch 38/64 loss: -0.044042348861694336
Batch 39/64 loss: -0.05127108097076416
Batch 40/64 loss: -0.06133383512496948
Batch 41/64 loss: -0.03940534591674805
Batch 42/64 loss: -0.0370599627494812
Batch 43/64 loss: -0.047680020332336426
Batch 44/64 loss: -0.06701183319091797
Batch 45/64 loss: -0.05105710029602051
Batch 46/64 loss: -0.05457735061645508
Batch 47/64 loss: -0.06922304630279541
Batch 48/64 loss: -0.06800556182861328
Batch 49/64 loss: -0.03991520404815674
Batch 50/64 loss: -0.041076838970184326
Batch 51/64 loss: -0.02327340841293335
Batch 52/64 loss: -0.061125874519348145
Batch 53/64 loss: -0.05073779821395874
Batch 54/64 loss: -0.06327742338180542
Batch 55/64 loss: -0.05710196495056152
Batch 56/64 loss: -0.056180715560913086
Batch 57/64 loss: -0.040372371673583984
Batch 58/64 loss: -0.03428506851196289
Batch 59/64 loss: -0.0525931715965271
Batch 60/64 loss: -0.06274557113647461
Batch 61/64 loss: -0.04307115077972412
Batch 62/64 loss: -0.0452723503112793
Batch 63/64 loss: -0.06046169996261597
Batch 64/64 loss: -0.03707164525985718
Epoch 113  Train loss: -0.05162505752900067  Val loss: -0.010647478382202358
Epoch 114
-------------------------------
Batch 1/64 loss: -0.05703389644622803
Batch 2/64 loss: -0.05466979742050171
Batch 3/64 loss: -0.048072993755340576
Batch 4/64 loss: -0.06824439764022827
Batch 5/64 loss: -0.06218850612640381
Batch 6/64 loss: -0.051758527755737305
Batch 7/64 loss: -0.06552088260650635
Batch 8/64 loss: -0.04969167709350586
Batch 9/64 loss: -0.05535203218460083
Batch 10/64 loss: -0.05512571334838867
Batch 11/64 loss: -0.046631455421447754
Batch 12/64 loss: -0.050240278244018555
Batch 13/64 loss: -0.049233973026275635
Batch 14/64 loss: -0.047977447509765625
Batch 15/64 loss: -0.03811800479888916
Batch 16/64 loss: -0.05543851852416992
Batch 17/64 loss: -0.051206231117248535
Batch 18/64 loss: -0.05289280414581299
Batch 19/64 loss: -0.04730290174484253
Batch 20/64 loss: -0.0682482123374939
Batch 21/64 loss: -0.0335845947265625
Batch 22/64 loss: -0.05816918611526489
Batch 23/64 loss: -0.0608525276184082
Batch 24/64 loss: -0.05708819627761841
Batch 25/64 loss: -0.05486792325973511
Batch 26/64 loss: -0.038148343563079834
Batch 27/64 loss: -0.054866015911102295
Batch 28/64 loss: -0.05847209692001343
Batch 29/64 loss: -0.055351316928863525
Batch 30/64 loss: -0.04085266590118408
Batch 31/64 loss: -0.050198137760162354
Batch 32/64 loss: -0.05547785758972168
Batch 33/64 loss: -0.06614720821380615
Batch 34/64 loss: -0.05842757225036621
Batch 35/64 loss: -0.05246537923812866
Batch 36/64 loss: -0.049411773681640625
Batch 37/64 loss: -0.059090256690979004
Batch 38/64 loss: -0.03084087371826172
Batch 39/64 loss: -0.04142540693283081
Batch 40/64 loss: -0.05332350730895996
Batch 41/64 loss: -0.05954462289810181
Batch 42/64 loss: -0.02738058567047119
Batch 43/64 loss: -0.03558170795440674
Batch 44/64 loss: -0.04849743843078613
Batch 45/64 loss: -0.05147749185562134
Batch 46/64 loss: -0.04016768932342529
Batch 47/64 loss: -0.04632604122161865
Batch 48/64 loss: -0.050782859325408936
Batch 49/64 loss: -0.0733029842376709
Batch 50/64 loss: -0.06598567962646484
Batch 51/64 loss: -0.06190681457519531
Batch 52/64 loss: -0.057267606258392334
Batch 53/64 loss: -0.05806678533554077
Batch 54/64 loss: -0.04082787036895752
Batch 55/64 loss: -0.05080980062484741
Batch 56/64 loss: -0.06011694669723511
Batch 57/64 loss: -0.05513828992843628
Batch 58/64 loss: -0.059848785400390625
Batch 59/64 loss: -0.05321305990219116
Batch 60/64 loss: -0.0576024055480957
Batch 61/64 loss: -0.05824148654937744
Batch 62/64 loss: -0.06371545791625977
Batch 63/64 loss: -0.04433315992355347
Batch 64/64 loss: -0.04131883382797241
Epoch 114  Train loss: -0.0526295495968239  Val loss: -0.022920041559488093
Epoch 115
-------------------------------
Batch 1/64 loss: -0.06370031833648682
Batch 2/64 loss: -0.0577239990234375
Batch 3/64 loss: -0.052903711795806885
Batch 4/64 loss: -0.05528002977371216
Batch 5/64 loss: -0.04507941007614136
Batch 6/64 loss: -0.05948066711425781
Batch 7/64 loss: -0.06955897808074951
Batch 8/64 loss: -0.0636909008026123
Batch 9/64 loss: -0.0600852370262146
Batch 10/64 loss: -0.023393332958221436
Batch 11/64 loss: -0.07065051794052124
Batch 12/64 loss: -0.04723864793777466
Batch 13/64 loss: -0.06078702211380005
Batch 14/64 loss: -0.045909881591796875
Batch 15/64 loss: -0.040369391441345215
Batch 16/64 loss: -0.059938788414001465
Batch 17/64 loss: -0.051894962787628174
Batch 18/64 loss: -0.024653613567352295
Batch 19/64 loss: -0.05097973346710205
Batch 20/64 loss: -0.032572269439697266
Batch 21/64 loss: -0.04486072063446045
Batch 22/64 loss: -0.04126298427581787
Batch 23/64 loss: -0.05862241983413696
Batch 24/64 loss: -0.06570953130722046
Batch 25/64 loss: -0.04065835475921631
Batch 26/64 loss: -0.05668485164642334
Batch 27/64 loss: -0.052833378314971924
Batch 28/64 loss: -0.056939125061035156
Batch 29/64 loss: -0.049781620502471924
Batch 30/64 loss: -0.04866594076156616
Batch 31/64 loss: -0.04461038112640381
Batch 32/64 loss: -0.06114482879638672
Batch 33/64 loss: -0.033625245094299316
Batch 34/64 loss: -0.04981130361557007
Batch 35/64 loss: -0.06702136993408203
Batch 36/64 loss: -0.06104224920272827
Batch 37/64 loss: -0.06853729486465454
Batch 38/64 loss: -0.061532676219940186
Batch 39/64 loss: -0.042160630226135254
Batch 40/64 loss: -0.05308687686920166
Batch 41/64 loss: -0.046274423599243164
Batch 42/64 loss: -0.05180865526199341
Batch 43/64 loss: -0.05893796682357788
Batch 44/64 loss: -0.06951576471328735
Batch 45/64 loss: -0.06982743740081787
Batch 46/64 loss: -0.05465930700302124
Batch 47/64 loss: -0.05696004629135132
Batch 48/64 loss: -0.03941994905471802
Batch 49/64 loss: -0.07299530506134033
Batch 50/64 loss: -0.0456729531288147
Batch 51/64 loss: -0.03373610973358154
Batch 52/64 loss: -0.04440176486968994
Batch 53/64 loss: -0.07215625047683716
Batch 54/64 loss: -0.05409550666809082
Batch 55/64 loss: -0.06830263137817383
Batch 56/64 loss: -0.05620765686035156
Batch 57/64 loss: -0.04605686664581299
Batch 58/64 loss: -0.0363277792930603
Batch 59/64 loss: -0.050137996673583984
Batch 60/64 loss: -0.043769896030426025
Batch 61/64 loss: -0.05362588167190552
Batch 62/64 loss: -0.03396493196487427
Batch 63/64 loss: -0.06750637292861938
Batch 64/64 loss: -0.057179927825927734
Epoch 115  Train loss: -0.05276438582177256  Val loss: -0.025086179631682196
Saving best model, epoch: 115
Epoch 116
-------------------------------
Batch 1/64 loss: -0.024823427200317383
Batch 2/64 loss: -0.04464852809906006
Batch 3/64 loss: -0.05645066499710083
Batch 4/64 loss: -0.04288250207901001
Batch 5/64 loss: -0.06285977363586426
Batch 6/64 loss: -0.04853332042694092
Batch 7/64 loss: -0.042182207107543945
Batch 8/64 loss: -0.06549632549285889
Batch 9/64 loss: -0.05274820327758789
Batch 10/64 loss: -0.06472408771514893
Batch 11/64 loss: -0.07234197854995728
Batch 12/64 loss: -0.06635785102844238
Batch 13/64 loss: -0.04251939058303833
Batch 14/64 loss: -0.06164008378982544
Batch 15/64 loss: -0.05281919240951538
Batch 16/64 loss: -0.05191457271575928
Batch 17/64 loss: -0.06032097339630127
Batch 18/64 loss: -0.05244547128677368
Batch 19/64 loss: -0.035130083560943604
Batch 20/64 loss: -0.052201151847839355
Batch 21/64 loss: -0.03202652931213379
Batch 22/64 loss: -0.049022376537323
Batch 23/64 loss: -0.07355707883834839
Batch 24/64 loss: -0.06847840547561646
Batch 25/64 loss: -0.05705440044403076
Batch 26/64 loss: -0.05724126100540161
Batch 27/64 loss: -0.03283286094665527
Batch 28/64 loss: -0.05968594551086426
Batch 29/64 loss: -0.053638756275177
Batch 30/64 loss: -0.0599481463432312
Batch 31/64 loss: -0.07200169563293457
Batch 32/64 loss: -0.06482577323913574
Batch 33/64 loss: -0.04047501087188721
Batch 34/64 loss: -0.07054036855697632
Batch 35/64 loss: -0.0507737398147583
Batch 36/64 loss: -0.04376095533370972
Batch 37/64 loss: -0.048790931701660156
Batch 38/64 loss: -0.050701022148132324
Batch 39/64 loss: -0.05245864391326904
Batch 40/64 loss: -0.060208678245544434
Batch 41/64 loss: -0.08003532886505127
Batch 42/64 loss: -0.04211324453353882
Batch 43/64 loss: -0.07206368446350098
Batch 44/64 loss: -0.05323821306228638
Batch 45/64 loss: -0.044208526611328125
Batch 46/64 loss: -0.04310774803161621
Batch 47/64 loss: -0.06260854005813599
Batch 48/64 loss: -0.025795578956604004
Batch 49/64 loss: -0.033344149589538574
Batch 50/64 loss: -0.06454133987426758
Batch 51/64 loss: -0.057835161685943604
Batch 52/64 loss: -0.024761736392974854
Batch 53/64 loss: -0.04824560880661011
Batch 54/64 loss: -0.05450254678726196
Batch 55/64 loss: -0.06353986263275146
Batch 56/64 loss: -0.049188971519470215
Batch 57/64 loss: -0.047571539878845215
Batch 58/64 loss: -0.05635780096054077
Batch 59/64 loss: -0.06599324941635132
Batch 60/64 loss: -0.045061588287353516
Batch 61/64 loss: -0.06174999475479126
Batch 62/64 loss: -0.0630044937133789
Batch 63/64 loss: -0.04161113500595093
Batch 64/64 loss: -0.06142711639404297
Epoch 116  Train loss: -0.05329588628282734  Val loss: -0.014910344815336143
Epoch 117
-------------------------------
Batch 1/64 loss: -0.05212700366973877
Batch 2/64 loss: -0.049904584884643555
Batch 3/64 loss: -0.06448209285736084
Batch 4/64 loss: -0.05288422107696533
Batch 5/64 loss: -0.06646227836608887
Batch 6/64 loss: -0.07174789905548096
Batch 7/64 loss: -0.06263738870620728
Batch 8/64 loss: -0.06146925687789917
Batch 9/64 loss: -0.06626725196838379
Batch 10/64 loss: -0.054076433181762695
Batch 11/64 loss: -0.023171186447143555
Batch 12/64 loss: -0.038423120975494385
Batch 13/64 loss: -0.05225968360900879
Batch 14/64 loss: -0.061380624771118164
Batch 15/64 loss: -0.06776434183120728
Batch 16/64 loss: -0.07192140817642212
Batch 17/64 loss: -0.05608808994293213
Batch 18/64 loss: -0.0560111403465271
Batch 19/64 loss: -0.05825388431549072
Batch 20/64 loss: -0.06632977724075317
Batch 21/64 loss: -0.06407856941223145
Batch 22/64 loss: -0.03539574146270752
Batch 23/64 loss: -0.047705113887786865
Batch 24/64 loss: -0.06942868232727051
Batch 25/64 loss: -0.06747162342071533
Batch 26/64 loss: -0.052288055419921875
Batch 27/64 loss: -0.06090247631072998
Batch 28/64 loss: -0.055940449237823486
Batch 29/64 loss: -0.035771310329437256
Batch 30/64 loss: -0.04836982488632202
Batch 31/64 loss: -0.0755804181098938
Batch 32/64 loss: -0.05278199911117554
Batch 33/64 loss: -0.044256746768951416
Batch 34/64 loss: -0.05942612886428833
Batch 35/64 loss: -0.04525566101074219
Batch 36/64 loss: -0.05093538761138916
Batch 37/64 loss: -0.04548943042755127
Batch 38/64 loss: -0.06075865030288696
Batch 39/64 loss: -0.07344710826873779
Batch 40/64 loss: -0.04620462656021118
Batch 41/64 loss: -0.055694758892059326
Batch 42/64 loss: -0.05572158098220825
Batch 43/64 loss: -0.06975078582763672
Batch 44/64 loss: -0.02025008201599121
Batch 45/64 loss: -0.06523317098617554
Batch 46/64 loss: -0.0749625563621521
Batch 47/64 loss: -0.04333782196044922
Batch 48/64 loss: -0.04299271106719971
Batch 49/64 loss: -0.060263216495513916
Batch 50/64 loss: -0.0601123571395874
Batch 51/64 loss: -0.06128406524658203
Batch 52/64 loss: -0.050727784633636475
Batch 53/64 loss: -0.05214613676071167
Batch 54/64 loss: -0.026885390281677246
Batch 55/64 loss: -0.06964671611785889
Batch 56/64 loss: -0.045288264751434326
Batch 57/64 loss: -0.0667186975479126
Batch 58/64 loss: -0.04016268253326416
Batch 59/64 loss: -0.06581717729568481
Batch 60/64 loss: -0.03947460651397705
Batch 61/64 loss: -0.048604369163513184
Batch 62/64 loss: -0.011528730392456055
Batch 63/64 loss: -0.036640465259552
Batch 64/64 loss: -0.07041949033737183
Epoch 117  Train loss: -0.05429346538057514  Val loss: -0.02790019930023508
Saving best model, epoch: 117
Epoch 118
-------------------------------
Batch 1/64 loss: -0.048519790172576904
Batch 2/64 loss: -0.0284234881401062
Batch 3/64 loss: -0.07491922378540039
Batch 4/64 loss: -0.06250143051147461
Batch 5/64 loss: -0.06621694564819336
Batch 6/64 loss: -0.0679861307144165
Batch 7/64 loss: -0.05087345838546753
Batch 8/64 loss: -0.0596843957901001
Batch 9/64 loss: -0.07410943508148193
Batch 10/64 loss: -0.07260441780090332
Batch 11/64 loss: -0.06937170028686523
Batch 12/64 loss: -0.05212527513504028
Batch 13/64 loss: -0.04523128271102905
Batch 14/64 loss: -0.06578528881072998
Batch 15/64 loss: -0.06095367670059204
Batch 16/64 loss: -0.06654661893844604
Batch 17/64 loss: -0.05964714288711548
Batch 18/64 loss: -0.04772961139678955
Batch 19/64 loss: -0.0649709701538086
Batch 20/64 loss: -0.03806096315383911
Batch 21/64 loss: -0.06490612030029297
Batch 22/64 loss: -0.08295226097106934
Batch 23/64 loss: -0.060455501079559326
Batch 24/64 loss: -0.06793904304504395
Batch 25/64 loss: -0.07434195280075073
Batch 26/64 loss: -0.04753446578979492
Batch 27/64 loss: -0.05692631006240845
Batch 28/64 loss: -0.0493205189704895
Batch 29/64 loss: -0.034869253635406494
Batch 30/64 loss: -0.04790031909942627
Batch 31/64 loss: -0.06985896825790405
Batch 32/64 loss: -0.04543936252593994
Batch 33/64 loss: -0.05334550142288208
Batch 34/64 loss: -0.06135684251785278
Batch 35/64 loss: -0.04836833477020264
Batch 36/64 loss: -0.07474899291992188
Batch 37/64 loss: -0.032077789306640625
Batch 38/64 loss: -0.05353325605392456
Batch 39/64 loss: -0.045395612716674805
Batch 40/64 loss: -0.056872427463531494
Batch 41/64 loss: -0.045530080795288086
Batch 42/64 loss: -0.047917306423187256
Batch 43/64 loss: -0.06102907657623291
Batch 44/64 loss: -0.05826926231384277
Batch 45/64 loss: -0.07286298274993896
Batch 46/64 loss: -0.06690871715545654
Batch 47/64 loss: -0.05564415454864502
Batch 48/64 loss: -0.052835822105407715
Batch 49/64 loss: -0.04546177387237549
Batch 50/64 loss: -0.06035804748535156
Batch 51/64 loss: -0.02643609046936035
Batch 52/64 loss: -0.046498656272888184
Batch 53/64 loss: -0.0719602108001709
Batch 54/64 loss: -0.04348033666610718
Batch 55/64 loss: -0.03448319435119629
Batch 56/64 loss: -0.06416136026382446
Batch 57/64 loss: -0.03819149732589722
Batch 58/64 loss: -0.04464542865753174
Batch 59/64 loss: -0.06110060214996338
Batch 60/64 loss: -0.03704631328582764
Batch 61/64 loss: -0.02839648723602295
Batch 62/64 loss: -0.04192245006561279
Batch 63/64 loss: -0.047450125217437744
Batch 64/64 loss: -0.05703383684158325
Epoch 118  Train loss: -0.05489834406796624  Val loss: -0.020623793307038927
Epoch 119
-------------------------------
Batch 1/64 loss: -0.045908451080322266
Batch 2/64 loss: -0.06718891859054565
Batch 3/64 loss: -0.05349820852279663
Batch 4/64 loss: -0.06183558702468872
Batch 5/64 loss: -0.026939749717712402
Batch 6/64 loss: -0.041126370429992676
Batch 7/64 loss: -0.04100799560546875
Batch 8/64 loss: -0.05295044183731079
Batch 9/64 loss: -0.044718384742736816
Batch 10/64 loss: -0.02953571081161499
Batch 11/64 loss: -0.05773627758026123
Batch 12/64 loss: -0.039109110832214355
Batch 13/64 loss: -0.06497859954833984
Batch 14/64 loss: -0.03350675106048584
Batch 15/64 loss: -0.051383912563323975
Batch 16/64 loss: -0.05838727951049805
Batch 17/64 loss: -0.06949496269226074
Batch 18/64 loss: -0.04781317710876465
Batch 19/64 loss: -0.06329447031021118
Batch 20/64 loss: -0.07189637422561646
Batch 21/64 loss: -0.056368350982666016
Batch 22/64 loss: -0.04632389545440674
Batch 23/64 loss: -0.060505211353302
Batch 24/64 loss: -0.05485188961029053
Batch 25/64 loss: -0.05557358264923096
Batch 26/64 loss: -0.07076072692871094
Batch 27/64 loss: -0.04496431350708008
Batch 28/64 loss: -0.05773484706878662
Batch 29/64 loss: -0.04439377784729004
Batch 30/64 loss: -0.06229603290557861
Batch 31/64 loss: -0.06384360790252686
Batch 32/64 loss: -0.048222124576568604
Batch 33/64 loss: -0.05464959144592285
Batch 34/64 loss: -0.05909264087677002
Batch 35/64 loss: -0.060546934604644775
Batch 36/64 loss: -0.0723717212677002
Batch 37/64 loss: -0.049268484115600586
Batch 38/64 loss: -0.053198158740997314
Batch 39/64 loss: -0.05660742521286011
Batch 40/64 loss: -0.051684558391571045
Batch 41/64 loss: -0.06418108940124512
Batch 42/64 loss: -0.049409329891204834
Batch 43/64 loss: -0.06598567962646484
Batch 44/64 loss: -0.03407973051071167
Batch 45/64 loss: -0.0697367787361145
Batch 46/64 loss: -0.042229652404785156
Batch 47/64 loss: -0.06234288215637207
Batch 48/64 loss: -0.04053562879562378
Batch 49/64 loss: -0.06253045797348022
Batch 50/64 loss: -0.059955477714538574
Batch 51/64 loss: -0.04135262966156006
Batch 52/64 loss: -0.04854536056518555
Batch 53/64 loss: -0.057369887828826904
Batch 54/64 loss: -0.04744148254394531
Batch 55/64 loss: -0.05101698637008667
Batch 56/64 loss: -0.04903203248977661
Batch 57/64 loss: -0.059586405754089355
Batch 58/64 loss: -0.0632205605506897
Batch 59/64 loss: -0.059787094593048096
Batch 60/64 loss: -0.05333864688873291
Batch 61/64 loss: -0.038034021854400635
Batch 62/64 loss: -0.05926543474197388
Batch 63/64 loss: -0.03690695762634277
Batch 64/64 loss: -0.0659945011138916
Epoch 119  Train loss: -0.05350507754905551  Val loss: -0.02346455846045845
Epoch 120
-------------------------------
Batch 1/64 loss: -0.05195075273513794
Batch 2/64 loss: -0.04754137992858887
Batch 3/64 loss: -0.0689893364906311
Batch 4/64 loss: -0.047584474086761475
Batch 5/64 loss: -0.05419796705245972
Batch 6/64 loss: -0.07025092840194702
Batch 7/64 loss: -0.059431493282318115
Batch 8/64 loss: -0.05114024877548218
Batch 9/64 loss: -0.03689765930175781
Batch 10/64 loss: -0.052205681800842285
Batch 11/64 loss: -0.05830579996109009
Batch 12/64 loss: -0.0626559853553772
Batch 13/64 loss: -0.0668954849243164
Batch 14/64 loss: -0.05864006280899048
Batch 15/64 loss: -0.0698578953742981
Batch 16/64 loss: -0.07494223117828369
Batch 17/64 loss: -0.051351726055145264
Batch 18/64 loss: -0.04967987537384033
Batch 19/64 loss: -0.061157405376434326
Batch 20/64 loss: -0.0401531457901001
Batch 21/64 loss: -0.05769282579421997
Batch 22/64 loss: -0.06094628572463989
Batch 23/64 loss: -0.05884599685668945
Batch 24/64 loss: -0.0665135383605957
Batch 25/64 loss: -0.07016825675964355
Batch 26/64 loss: -0.043943583965301514
Batch 27/64 loss: -0.056787848472595215
Batch 28/64 loss: -0.038958847522735596
Batch 29/64 loss: -0.04531639814376831
Batch 30/64 loss: -0.06943279504776001
Batch 31/64 loss: -0.07034063339233398
Batch 32/64 loss: -0.07521271705627441
Batch 33/64 loss: -0.04706978797912598
Batch 34/64 loss: -0.053250014781951904
Batch 35/64 loss: -0.05644845962524414
Batch 36/64 loss: -0.059952735900878906
Batch 37/64 loss: -0.05294066667556763
Batch 38/64 loss: -0.05266368389129639
Batch 39/64 loss: -0.038252055644989014
Batch 40/64 loss: -0.06697893142700195
Batch 41/64 loss: -0.06649035215377808
Batch 42/64 loss: -0.051712751388549805
Batch 43/64 loss: -0.07138264179229736
Batch 44/64 loss: -0.054583847522735596
Batch 45/64 loss: -0.02759242057800293
Batch 46/64 loss: -0.042302489280700684
Batch 47/64 loss: -0.0633431077003479
Batch 48/64 loss: -0.0617142915725708
Batch 49/64 loss: -0.052486538887023926
Batch 50/64 loss: -0.05507171154022217
Batch 51/64 loss: -0.0572015643119812
Batch 52/64 loss: -0.05354762077331543
Batch 53/64 loss: -0.06063401699066162
Batch 54/64 loss: -0.04009401798248291
Batch 55/64 loss: -0.046279847621917725
Batch 56/64 loss: -0.041261136531829834
Batch 57/64 loss: -0.06384313106536865
Batch 58/64 loss: -0.044274866580963135
Batch 59/64 loss: -0.05294269323348999
Batch 60/64 loss: -0.05939650535583496
Batch 61/64 loss: -0.06289887428283691
Batch 62/64 loss: -0.06798529624938965
Batch 63/64 loss: -0.05710732936859131
Batch 64/64 loss: -0.046495676040649414
Epoch 120  Train loss: -0.05591473579406738  Val loss: -0.028382973982296448
Saving best model, epoch: 120
Epoch 121
-------------------------------
Batch 1/64 loss: -0.06106531620025635
Batch 2/64 loss: -0.058493614196777344
Batch 3/64 loss: -0.06918364763259888
Batch 4/64 loss: -0.054206252098083496
Batch 5/64 loss: -0.07213824987411499
Batch 6/64 loss: -0.061802029609680176
Batch 7/64 loss: -0.04952681064605713
Batch 8/64 loss: -0.04883700609207153
Batch 9/64 loss: -0.05138117074966431
Batch 10/64 loss: -0.07589548826217651
Batch 11/64 loss: -0.04331701993942261
Batch 12/64 loss: -0.06530356407165527
Batch 13/64 loss: -0.05272120237350464
Batch 14/64 loss: -0.04231822490692139
Batch 15/64 loss: -0.06965267658233643
Batch 16/64 loss: -0.06228715181350708
Batch 17/64 loss: -0.04497337341308594
Batch 18/64 loss: -0.04993671178817749
Batch 19/64 loss: -0.0651625394821167
Batch 20/64 loss: -0.07137095928192139
Batch 21/64 loss: -0.06990790367126465
Batch 22/64 loss: -0.04564106464385986
Batch 23/64 loss: -0.059741199016571045
Batch 24/64 loss: -0.03786081075668335
Batch 25/64 loss: -0.04277163743972778
Batch 26/64 loss: -0.06054079532623291
Batch 27/64 loss: -0.04540252685546875
Batch 28/64 loss: -0.04145169258117676
Batch 29/64 loss: -0.0504763126373291
Batch 30/64 loss: -0.047375261783599854
Batch 31/64 loss: -0.0565338134765625
Batch 32/64 loss: -0.03244823217391968
Batch 33/64 loss: -0.03895676136016846
Batch 34/64 loss: -0.05758833885192871
Batch 35/64 loss: -0.03934365510940552
Batch 36/64 loss: -0.05614650249481201
Batch 37/64 loss: -0.06262820959091187
Batch 38/64 loss: -0.06730145215988159
Batch 39/64 loss: -0.040357768535614014
Batch 40/64 loss: -0.04219770431518555
Batch 41/64 loss: -0.059892892837524414
Batch 42/64 loss: -0.07358527183532715
Batch 43/64 loss: -0.04995208978652954
Batch 44/64 loss: -0.0490415096282959
Batch 45/64 loss: -0.06525397300720215
Batch 46/64 loss: -0.06962895393371582
Batch 47/64 loss: -0.06397044658660889
Batch 48/64 loss: -0.05853480100631714
Batch 49/64 loss: -0.04297977685928345
Batch 50/64 loss: -0.046908557415008545
Batch 51/64 loss: -0.045921504497528076
Batch 52/64 loss: -0.0609171986579895
Batch 53/64 loss: -0.06745415925979614
Batch 54/64 loss: -0.07006847858428955
Batch 55/64 loss: -0.05473601818084717
Batch 56/64 loss: -0.06322968006134033
Batch 57/64 loss: -0.05302459001541138
Batch 58/64 loss: -0.0646291971206665
Batch 59/64 loss: -0.05572408437728882
Batch 60/64 loss: -0.03327775001525879
Batch 61/64 loss: -0.0577392578125
Batch 62/64 loss: -0.04068434238433838
Batch 63/64 loss: -0.05619943141937256
Batch 64/64 loss: -0.062280356884002686
Epoch 121  Train loss: -0.055126413878272565  Val loss: -0.027668900711020242
Epoch 122
-------------------------------
Batch 1/64 loss: -0.07067430019378662
Batch 2/64 loss: -0.07420814037322998
Batch 3/64 loss: -0.052782654762268066
Batch 4/64 loss: -0.056744515895843506
Batch 5/64 loss: -0.038861989974975586
Batch 6/64 loss: -0.05907958745956421
Batch 7/64 loss: -0.06241947412490845
Batch 8/64 loss: -0.06802940368652344
Batch 9/64 loss: -0.0451432466506958
Batch 10/64 loss: -0.0562899112701416
Batch 11/64 loss: -0.04818779230117798
Batch 12/64 loss: -0.06649988889694214
Batch 13/64 loss: -0.07265639305114746
Batch 14/64 loss: -0.0599401593208313
Batch 15/64 loss: -0.043649911880493164
Batch 16/64 loss: -0.037623584270477295
Batch 17/64 loss: -0.04139596223831177
Batch 18/64 loss: -0.04678148031234741
Batch 19/64 loss: -0.06432414054870605
Batch 20/64 loss: -0.07645672559738159
Batch 21/64 loss: -0.06783002614974976
Batch 22/64 loss: -0.06474089622497559
Batch 23/64 loss: -0.05747663974761963
Batch 24/64 loss: -0.05469870567321777
Batch 25/64 loss: -0.03783309459686279
Batch 26/64 loss: -0.05176270008087158
Batch 27/64 loss: -0.05164998769760132
Batch 28/64 loss: -0.055107176303863525
Batch 29/64 loss: -0.06404316425323486
Batch 30/64 loss: -0.04531204700469971
Batch 31/64 loss: -0.05411261320114136
Batch 32/64 loss: -0.04815936088562012
Batch 33/64 loss: -0.022905170917510986
Batch 34/64 loss: -0.04838317632675171
Batch 35/64 loss: -0.058595478534698486
Batch 36/64 loss: -0.022344768047332764
Batch 37/64 loss: -0.061164259910583496
Batch 38/64 loss: -0.03061598539352417
Batch 39/64 loss: -0.06185591220855713
Batch 40/64 loss: -0.07056993246078491
Batch 41/64 loss: -0.048165202140808105
Batch 42/64 loss: -0.05888396501541138
Batch 43/64 loss: -0.05113685131072998
Batch 44/64 loss: -0.04643279314041138
Batch 45/64 loss: -0.05124610662460327
Batch 46/64 loss: -0.031357765197753906
Batch 47/64 loss: -0.0554957389831543
Batch 48/64 loss: -0.0587010383605957
Batch 49/64 loss: -0.06341207027435303
Batch 50/64 loss: -0.06339883804321289
Batch 51/64 loss: -0.061202943325042725
Batch 52/64 loss: -0.05546015501022339
Batch 53/64 loss: -0.05086475610733032
Batch 54/64 loss: -0.03750443458557129
Batch 55/64 loss: -0.05658721923828125
Batch 56/64 loss: -0.05914783477783203
Batch 57/64 loss: -0.05617630481719971
Batch 58/64 loss: -0.05507326126098633
Batch 59/64 loss: -0.05537915229797363
Batch 60/64 loss: -0.06232208013534546
Batch 61/64 loss: -0.043954312801361084
Batch 62/64 loss: -0.050140380859375
Batch 63/64 loss: -0.06604373455047607
Batch 64/64 loss: -0.05854576826095581
Epoch 122  Train loss: -0.05416324115266987  Val loss: -0.016730714499745582
Epoch 123
-------------------------------
Batch 1/64 loss: -0.04689151048660278
Batch 2/64 loss: -0.049758315086364746
Batch 3/64 loss: -0.03559964895248413
Batch 4/64 loss: -0.055248141288757324
Batch 5/64 loss: -0.041028380393981934
Batch 6/64 loss: -0.05907440185546875
Batch 7/64 loss: -0.05206775665283203
Batch 8/64 loss: -0.05032706260681152
Batch 9/64 loss: -0.0680997371673584
Batch 10/64 loss: -0.05553966760635376
Batch 11/64 loss: -0.04910975694656372
Batch 12/64 loss: -0.05704432725906372
Batch 13/64 loss: -0.05684554576873779
Batch 14/64 loss: -0.05939745903015137
Batch 15/64 loss: -0.037926673889160156
Batch 16/64 loss: -0.06567370891571045
Batch 17/64 loss: -0.05330932140350342
Batch 18/64 loss: -0.06908643245697021
Batch 19/64 loss: -0.05035901069641113
Batch 20/64 loss: -0.07651591300964355
Batch 21/64 loss: -0.05819988250732422
Batch 22/64 loss: -0.06820416450500488
Batch 23/64 loss: -0.06288880109786987
Batch 24/64 loss: -0.06356596946716309
Batch 25/64 loss: -0.05608224868774414
Batch 26/64 loss: -0.054050326347351074
Batch 27/64 loss: -0.06180661916732788
Batch 28/64 loss: -0.05904114246368408
Batch 29/64 loss: -0.06270325183868408
Batch 30/64 loss: -0.06854116916656494
Batch 31/64 loss: -0.05241525173187256
Batch 32/64 loss: -0.042903363704681396
Batch 33/64 loss: -0.06504553556442261
Batch 34/64 loss: -0.06234383583068848
Batch 35/64 loss: -0.06397837400436401
Batch 36/64 loss: -0.05315101146697998
Batch 37/64 loss: -0.07254844903945923
Batch 38/64 loss: -0.056322455406188965
Batch 39/64 loss: -0.0626327395439148
Batch 40/64 loss: -0.041541874408721924
Batch 41/64 loss: -0.056900084018707275
Batch 42/64 loss: -0.05440413951873779
Batch 43/64 loss: -0.056104421615600586
Batch 44/64 loss: -0.06002199649810791
Batch 45/64 loss: -0.04207581281661987
Batch 46/64 loss: -0.06414854526519775
Batch 47/64 loss: -0.050347089767456055
Batch 48/64 loss: -0.04508465528488159
Batch 49/64 loss: -0.045381009578704834
Batch 50/64 loss: -0.057906270027160645
Batch 51/64 loss: -0.05945920944213867
Batch 52/64 loss: -0.07258415222167969
Batch 53/64 loss: -0.05227476358413696
Batch 54/64 loss: -0.06907165050506592
Batch 55/64 loss: -0.04069995880126953
Batch 56/64 loss: -0.04124295711517334
Batch 57/64 loss: -0.04917103052139282
Batch 58/64 loss: -0.05392950773239136
Batch 59/64 loss: -0.0656772255897522
Batch 60/64 loss: -0.056079089641571045
Batch 61/64 loss: -0.04839867353439331
Batch 62/64 loss: -0.051780521869659424
Batch 63/64 loss: -0.06263864040374756
Batch 64/64 loss: -0.03086543083190918
Epoch 123  Train loss: -0.05577097593569288  Val loss: -0.02361898160062705
Epoch 124
-------------------------------
Batch 1/64 loss: -0.05299258232116699
Batch 2/64 loss: -0.055028676986694336
Batch 3/64 loss: -0.06614822149276733
Batch 4/64 loss: -0.07843035459518433
Batch 5/64 loss: -0.068351149559021
Batch 6/64 loss: -0.07897281646728516
Batch 7/64 loss: -0.05828732252120972
Batch 8/64 loss: -0.06083637475967407
Batch 9/64 loss: -0.056522369384765625
Batch 10/64 loss: -0.06497901678085327
Batch 11/64 loss: -0.04514515399932861
Batch 12/64 loss: -0.06569439172744751
Batch 13/64 loss: -0.05115079879760742
Batch 14/64 loss: -0.05692011117935181
Batch 15/64 loss: -0.058373451232910156
Batch 16/64 loss: -0.062424302101135254
Batch 17/64 loss: -0.066608726978302
Batch 18/64 loss: -0.05786478519439697
Batch 19/64 loss: -0.06025451421737671
Batch 20/64 loss: -0.052036285400390625
Batch 21/64 loss: -0.056381940841674805
Batch 22/64 loss: -0.06837999820709229
Batch 23/64 loss: -0.052981674671173096
Batch 24/64 loss: -0.016614079475402832
Batch 25/64 loss: -0.06955242156982422
Batch 26/64 loss: -0.06879448890686035
Batch 27/64 loss: -0.055181682109832764
Batch 28/64 loss: -0.06767338514328003
Batch 29/64 loss: -0.0830228328704834
Batch 30/64 loss: -0.05022597312927246
Batch 31/64 loss: -0.0522613525390625
Batch 32/64 loss: -0.052881836891174316
Batch 33/64 loss: -0.050518035888671875
Batch 34/64 loss: -0.05818808078765869
Batch 35/64 loss: -0.04468041658401489
Batch 36/64 loss: -0.05533379316329956
Batch 37/64 loss: -0.05832207202911377
Batch 38/64 loss: -0.06829488277435303
Batch 39/64 loss: -0.05970203876495361
Batch 40/64 loss: -0.05855363607406616
Batch 41/64 loss: -0.051956117153167725
Batch 42/64 loss: -0.027325093746185303
Batch 43/64 loss: -0.06858015060424805
Batch 44/64 loss: -0.04861825704574585
Batch 45/64 loss: -0.061174869537353516
Batch 46/64 loss: -0.04400956630706787
Batch 47/64 loss: -0.07341557741165161
Batch 48/64 loss: -0.05695843696594238
Batch 49/64 loss: -0.05163693428039551
Batch 50/64 loss: -0.052863121032714844
Batch 51/64 loss: -0.0693368911743164
Batch 52/64 loss: -0.04163622856140137
Batch 53/64 loss: -0.07779812812805176
Batch 54/64 loss: -0.06151258945465088
Batch 55/64 loss: -0.03848522901535034
Batch 56/64 loss: -0.03221392631530762
Batch 57/64 loss: -0.06172114610671997
Batch 58/64 loss: -0.052280426025390625
Batch 59/64 loss: -0.055470407009124756
Batch 60/64 loss: -0.06600970029830933
Batch 61/64 loss: -0.04754161834716797
Batch 62/64 loss: -0.027326583862304688
Batch 63/64 loss: -0.05653834342956543
Batch 64/64 loss: -0.028403043746948242
Epoch 124  Train loss: -0.0566318063174977  Val loss: -0.015130064741442703
Epoch 125
-------------------------------
Batch 1/64 loss: -0.051036179065704346
Batch 2/64 loss: -0.06256347894668579
Batch 3/64 loss: -0.05914419889450073
Batch 4/64 loss: -0.04050397872924805
Batch 5/64 loss: -0.04599863290786743
Batch 6/64 loss: -0.0714261531829834
Batch 7/64 loss: -0.051072001457214355
Batch 8/64 loss: -0.06700700521469116
Batch 9/64 loss: -0.06178438663482666
Batch 10/64 loss: -0.07213836908340454
Batch 11/64 loss: -0.08206403255462646
Batch 12/64 loss: -0.05528831481933594
Batch 13/64 loss: -0.06259644031524658
Batch 14/64 loss: -0.05416285991668701
Batch 15/64 loss: -0.054194867610931396
Batch 16/64 loss: -0.05884438753128052
Batch 17/64 loss: -0.060222327709198
Batch 18/64 loss: -0.058635592460632324
Batch 19/64 loss: -0.05861258506774902
Batch 20/64 loss: -0.06842243671417236
Batch 21/64 loss: -0.059155285358428955
Batch 22/64 loss: -0.051865458488464355
Batch 23/64 loss: -0.05814516544342041
Batch 24/64 loss: -0.04721575975418091
Batch 25/64 loss: -0.05292379856109619
Batch 26/64 loss: -0.07087874412536621
Batch 27/64 loss: -0.033007800579071045
Batch 28/64 loss: -0.06100594997406006
Batch 29/64 loss: -0.06337130069732666
Batch 30/64 loss: -0.05534219741821289
Batch 31/64 loss: -0.060359835624694824
Batch 32/64 loss: -0.04777729511260986
Batch 33/64 loss: -0.05069077014923096
Batch 34/64 loss: -0.04395151138305664
Batch 35/64 loss: -0.04168635606765747
Batch 36/64 loss: -0.06329542398452759
Batch 37/64 loss: -0.0676000714302063
Batch 38/64 loss: -0.06937462091445923
Batch 39/64 loss: -0.05639493465423584
Batch 40/64 loss: -0.06988918781280518
Batch 41/64 loss: -0.06830072402954102
Batch 42/64 loss: -0.04605334997177124
Batch 43/64 loss: -0.05202770233154297
Batch 44/64 loss: -0.07241910696029663
Batch 45/64 loss: -0.05666404962539673
Batch 46/64 loss: -0.05985450744628906
Batch 47/64 loss: -0.04860633611679077
Batch 48/64 loss: -0.054454147815704346
Batch 49/64 loss: -0.07004672288894653
Batch 50/64 loss: -0.041401565074920654
Batch 51/64 loss: -0.06834149360656738
Batch 52/64 loss: -0.04104959964752197
Batch 53/64 loss: -0.06585490703582764
Batch 54/64 loss: -0.05678606033325195
Batch 55/64 loss: -0.06766599416732788
Batch 56/64 loss: -0.0582202672958374
Batch 57/64 loss: -0.06024169921875
Batch 58/64 loss: -0.051215529441833496
Batch 59/64 loss: -0.064170241355896
Batch 60/64 loss: -0.05732029676437378
Batch 61/64 loss: -0.053514838218688965
Batch 62/64 loss: -0.06630128622055054
Batch 63/64 loss: -0.04139232635498047
Batch 64/64 loss: -0.05543243885040283
Epoch 125  Train loss: -0.0577745376848707  Val loss: -0.0265994784758263
Epoch 126
-------------------------------
Batch 1/64 loss: -0.05453813076019287
Batch 2/64 loss: -0.06056898832321167
Batch 3/64 loss: -0.04627102613449097
Batch 4/64 loss: -0.07283449172973633
Batch 5/64 loss: -0.06521952152252197
Batch 6/64 loss: -0.05665862560272217
Batch 7/64 loss: -0.06386983394622803
Batch 8/64 loss: -0.06535202264785767
Batch 9/64 loss: -0.05334264039993286
Batch 10/64 loss: -0.06171691417694092
Batch 11/64 loss: -0.07061499357223511
Batch 12/64 loss: -0.05475914478302002
Batch 13/64 loss: -0.059630632400512695
Batch 14/64 loss: -0.04797804355621338
Batch 15/64 loss: -0.050892651081085205
Batch 16/64 loss: -0.07342058420181274
Batch 17/64 loss: -0.06837522983551025
Batch 18/64 loss: -0.06490534543991089
Batch 19/64 loss: -0.06469148397445679
Batch 20/64 loss: -0.05166888236999512
Batch 21/64 loss: -0.051218271255493164
Batch 22/64 loss: -0.07125592231750488
Batch 23/64 loss: -0.058902859687805176
Batch 24/64 loss: -0.06161624193191528
Batch 25/64 loss: -0.06412601470947266
Batch 26/64 loss: -0.05432426929473877
Batch 27/64 loss: -0.054232895374298096
Batch 28/64 loss: -0.05113041400909424
Batch 29/64 loss: -0.05571085214614868
Batch 30/64 loss: -0.060807108879089355
Batch 31/64 loss: -0.0639115571975708
Batch 32/64 loss: -0.07106399536132812
Batch 33/64 loss: -0.03560280799865723
Batch 34/64 loss: -0.07252979278564453
Batch 35/64 loss: -0.07343584299087524
Batch 36/64 loss: -0.05463892221450806
Batch 37/64 loss: -0.07336032390594482
Batch 38/64 loss: -0.07370263338088989
Batch 39/64 loss: -0.07086491584777832
Batch 40/64 loss: -0.0707048773765564
Batch 41/64 loss: -0.06300008296966553
Batch 42/64 loss: -0.04849916696548462
Batch 43/64 loss: -0.0688711404800415
Batch 44/64 loss: -0.05078941583633423
Batch 45/64 loss: -0.062930166721344
Batch 46/64 loss: -0.0626596212387085
Batch 47/64 loss: -0.039307594299316406
Batch 48/64 loss: -0.06834948062896729
Batch 49/64 loss: -0.06698602437973022
Batch 50/64 loss: -0.06124138832092285
Batch 51/64 loss: -0.059812307357788086
Batch 52/64 loss: -0.05710381269454956
Batch 53/64 loss: -0.06201201677322388
Batch 54/64 loss: -0.054762303829193115
Batch 55/64 loss: -0.05802696943283081
Batch 56/64 loss: -0.04993391036987305
Batch 57/64 loss: -0.050432562828063965
Batch 58/64 loss: -0.04429888725280762
Batch 59/64 loss: -0.05025005340576172
Batch 60/64 loss: -0.04501795768737793
Batch 61/64 loss: -0.05453604459762573
Batch 62/64 loss: -0.059169888496398926
Batch 63/64 loss: -0.0657649040222168
Batch 64/64 loss: -0.044029951095581055
Epoch 126  Train loss: -0.059407495984844134  Val loss: -0.023714108975072905
Epoch 127
-------------------------------
Batch 1/64 loss: -0.05532193183898926
Batch 2/64 loss: -0.06139731407165527
Batch 3/64 loss: -0.0680801272392273
Batch 4/64 loss: -0.036230623722076416
Batch 5/64 loss: -0.0601271390914917
Batch 6/64 loss: -0.059271395206451416
Batch 7/64 loss: -0.039007484912872314
Batch 8/64 loss: -0.042928993701934814
Batch 9/64 loss: -0.06131476163864136
Batch 10/64 loss: -0.05893230438232422
Batch 11/64 loss: -0.06685245037078857
Batch 12/64 loss: -0.05418705940246582
Batch 13/64 loss: -0.049407899379730225
Batch 14/64 loss: -0.06877034902572632
Batch 15/64 loss: -0.0650486946105957
Batch 16/64 loss: -0.05788809061050415
Batch 17/64 loss: -0.07249903678894043
Batch 18/64 loss: -0.06436789035797119
Batch 19/64 loss: -0.07930070161819458
Batch 20/64 loss: -0.06723570823669434
Batch 21/64 loss: -0.06785178184509277
Batch 22/64 loss: -0.07468044757843018
Batch 23/64 loss: -0.04231452941894531
Batch 24/64 loss: -0.06628364324569702
Batch 25/64 loss: -0.05933237075805664
Batch 26/64 loss: -0.06069624423980713
Batch 27/64 loss: -0.06317746639251709
Batch 28/64 loss: -0.04996013641357422
Batch 29/64 loss: -0.05279397964477539
Batch 30/64 loss: -0.058754920959472656
Batch 31/64 loss: -0.06843167543411255
Batch 32/64 loss: -0.048787057399749756
Batch 33/64 loss: -0.045314908027648926
Batch 34/64 loss: -0.06257081031799316
Batch 35/64 loss: -0.07183098793029785
Batch 36/64 loss: -0.058507442474365234
Batch 37/64 loss: -0.058980345726013184
Batch 38/64 loss: -0.054390668869018555
Batch 39/64 loss: -0.06659841537475586
Batch 40/64 loss: -0.0631973147392273
Batch 41/64 loss: -0.07147711515426636
Batch 42/64 loss: -0.06960302591323853
Batch 43/64 loss: -0.05991131067276001
Batch 44/64 loss: -0.05668371915817261
Batch 45/64 loss: -0.05958151817321777
Batch 46/64 loss: -0.05488508939743042
Batch 47/64 loss: -0.044013023376464844
Batch 48/64 loss: -0.05947619676589966
Batch 49/64 loss: -0.059513866901397705
Batch 50/64 loss: -0.046827733516693115
Batch 51/64 loss: -0.011833727359771729
Batch 52/64 loss: -0.062306880950927734
Batch 53/64 loss: -0.07119953632354736
Batch 54/64 loss: -0.03315901756286621
Batch 55/64 loss: -0.04961562156677246
Batch 56/64 loss: -0.053673744201660156
Batch 57/64 loss: -0.05016481876373291
Batch 58/64 loss: -0.051114022731781006
Batch 59/64 loss: -0.05774778127670288
Batch 60/64 loss: -0.05191761255264282
Batch 61/64 loss: -0.05129802227020264
Batch 62/64 loss: -0.059583067893981934
Batch 63/64 loss: -0.06027841567993164
Batch 64/64 loss: -0.07087403535842896
Epoch 127  Train loss: -0.05775130192438761  Val loss: -0.023665848056884976
Epoch 128
-------------------------------
Batch 1/64 loss: -0.06352972984313965
Batch 2/64 loss: -0.0674334168434143
Batch 3/64 loss: -0.06921321153640747
Batch 4/64 loss: -0.050766825675964355
Batch 5/64 loss: -0.058649301528930664
Batch 6/64 loss: -0.07165849208831787
Batch 7/64 loss: -0.04865068197250366
Batch 8/64 loss: -0.051300883293151855
Batch 9/64 loss: -0.055008888244628906
Batch 10/64 loss: -0.06767833232879639
Batch 11/64 loss: -0.05708611011505127
Batch 12/64 loss: -0.07463449239730835
Batch 13/64 loss: -0.05102872848510742
Batch 14/64 loss: -0.0536799430847168
Batch 15/64 loss: -0.053696274757385254
Batch 16/64 loss: -0.05724191665649414
Batch 17/64 loss: -0.05501866340637207
Batch 18/64 loss: -0.06734108924865723
Batch 19/64 loss: -0.04937851428985596
Batch 20/64 loss: -0.07179301977157593
Batch 21/64 loss: -0.0628042221069336
Batch 22/64 loss: -0.06250119209289551
Batch 23/64 loss: -0.058745622634887695
Batch 24/64 loss: -0.04888129234313965
Batch 25/64 loss: -0.06338274478912354
Batch 26/64 loss: -0.05605858564376831
Batch 27/64 loss: -0.07152599096298218
Batch 28/64 loss: -0.060205042362213135
Batch 29/64 loss: -0.04466843605041504
Batch 30/64 loss: -0.05343097448348999
Batch 31/64 loss: -0.0548059344291687
Batch 32/64 loss: -0.06152236461639404
Batch 33/64 loss: -0.052499353885650635
Batch 34/64 loss: -0.053917765617370605
Batch 35/64 loss: -0.058649659156799316
Batch 36/64 loss: -0.03284120559692383
Batch 37/64 loss: -0.0612453818321228
Batch 38/64 loss: -0.04446864128112793
Batch 39/64 loss: -0.061551034450531006
Batch 40/64 loss: -0.061435163021087646
Batch 41/64 loss: -0.06575065851211548
Batch 42/64 loss: -0.038192152976989746
Batch 43/64 loss: -0.06638789176940918
Batch 44/64 loss: -0.05865764617919922
Batch 45/64 loss: -0.07586139440536499
Batch 46/64 loss: -0.07867592573165894
Batch 47/64 loss: -0.06637096405029297
Batch 48/64 loss: -0.058568358421325684
Batch 49/64 loss: -0.05999040603637695
Batch 50/64 loss: -0.050251543521881104
Batch 51/64 loss: -0.04713785648345947
Batch 52/64 loss: -0.05573558807373047
Batch 53/64 loss: -0.0699649453163147
Batch 54/64 loss: -0.05409330129623413
Batch 55/64 loss: -0.05500626564025879
Batch 56/64 loss: -0.08281075954437256
Batch 57/64 loss: -0.06892180442810059
Batch 58/64 loss: -0.07431912422180176
Batch 59/64 loss: -0.058016300201416016
Batch 60/64 loss: -0.053344130516052246
Batch 61/64 loss: -0.0680246353149414
Batch 62/64 loss: -0.05232346057891846
Batch 63/64 loss: -0.07323610782623291
Batch 64/64 loss: -0.051221489906311035
Epoch 128  Train loss: -0.05945076802197625  Val loss: -0.0219990035512603
Epoch 129
-------------------------------
Batch 1/64 loss: -0.053746700286865234
Batch 2/64 loss: -0.06596595048904419
Batch 3/64 loss: -0.05167907476425171
Batch 4/64 loss: -0.043167054653167725
Batch 5/64 loss: -0.05760306119918823
Batch 6/64 loss: -0.0599902868270874
Batch 7/64 loss: -0.07613813877105713
Batch 8/64 loss: -0.05295765399932861
Batch 9/64 loss: -0.07748603820800781
Batch 10/64 loss: -0.03884422779083252
Batch 11/64 loss: -0.06892544031143188
Batch 12/64 loss: -0.04779881238937378
Batch 13/64 loss: -0.04884219169616699
Batch 14/64 loss: -0.07241356372833252
Batch 15/64 loss: -0.0684899091720581
Batch 16/64 loss: -0.06602668762207031
Batch 17/64 loss: -0.043945133686065674
Batch 18/64 loss: -0.03626096248626709
Batch 19/64 loss: -0.051246464252471924
Batch 20/64 loss: -0.0611380934715271
Batch 21/64 loss: -0.04870247840881348
Batch 22/64 loss: -0.0410994291305542
Batch 23/64 loss: -0.05326646566390991
Batch 24/64 loss: -0.055875420570373535
Batch 25/64 loss: -0.052627384662628174
Batch 26/64 loss: -0.05891776084899902
Batch 27/64 loss: -0.06608986854553223
Batch 28/64 loss: -0.05557656288146973
Batch 29/64 loss: -0.060862720012664795
Batch 30/64 loss: -0.06287437677383423
Batch 31/64 loss: -0.06980478763580322
Batch 32/64 loss: -0.04941761493682861
Batch 33/64 loss: -0.06720471382141113
Batch 34/64 loss: -0.06740593910217285
Batch 35/64 loss: -0.07878381013870239
Batch 36/64 loss: -0.07309454679489136
Batch 37/64 loss: -0.07239508628845215
Batch 38/64 loss: -0.06377220153808594
Batch 39/64 loss: -0.07409411668777466
Batch 40/64 loss: -0.07177728414535522
Batch 41/64 loss: -0.044290363788604736
Batch 42/64 loss: -0.06470024585723877
Batch 43/64 loss: -0.054779767990112305
Batch 44/64 loss: -0.06169551610946655
Batch 45/64 loss: -0.07207047939300537
Batch 46/64 loss: -0.06658732891082764
Batch 47/64 loss: -0.05123567581176758
Batch 48/64 loss: -0.05391210317611694
Batch 49/64 loss: -0.05058044195175171
Batch 50/64 loss: -0.07026630640029907
Batch 51/64 loss: -0.07024884223937988
Batch 52/64 loss: -0.06299638748168945
Batch 53/64 loss: -0.04932141304016113
Batch 54/64 loss: -0.0415460467338562
Batch 55/64 loss: -0.06623733043670654
Batch 56/64 loss: -0.06454634666442871
Batch 57/64 loss: -0.05852019786834717
Batch 58/64 loss: -0.07875770330429077
Batch 59/64 loss: -0.05379903316497803
Batch 60/64 loss: -0.07420802116394043
Batch 61/64 loss: -0.06187528371810913
Batch 62/64 loss: -0.07532989978790283
Batch 63/64 loss: -0.059473395347595215
Batch 64/64 loss: -0.04074966907501221
Epoch 129  Train loss: -0.05998193563199511  Val loss: -0.023829673573733196
Epoch 130
-------------------------------
Batch 1/64 loss: -0.06017190217971802
Batch 2/64 loss: -0.044403076171875
Batch 3/64 loss: -0.05845677852630615
Batch 4/64 loss: -0.07274037599563599
Batch 5/64 loss: -0.04575467109680176
Batch 6/64 loss: -0.06462717056274414
Batch 7/64 loss: -0.06211167573928833
Batch 8/64 loss: -0.07073372602462769
Batch 9/64 loss: -0.08076775074005127
Batch 10/64 loss: -0.054113686084747314
Batch 11/64 loss: -0.05222940444946289
Batch 12/64 loss: -0.04897046089172363
Batch 13/64 loss: -0.043157875537872314
Batch 14/64 loss: -0.0744485855102539
Batch 15/64 loss: -0.07769978046417236
Batch 16/64 loss: -0.07659077644348145
Batch 17/64 loss: -0.06115865707397461
Batch 18/64 loss: -0.052633047103881836
Batch 19/64 loss: -0.050984740257263184
Batch 20/64 loss: -0.058676838874816895
Batch 21/64 loss: -0.025539398193359375
Batch 22/64 loss: -0.06293994188308716
Batch 23/64 loss: -0.06480246782302856
Batch 24/64 loss: -0.07479417324066162
Batch 25/64 loss: -0.054100215435028076
Batch 26/64 loss: -0.07591921091079712
Batch 27/64 loss: -0.0757671594619751
Batch 28/64 loss: -0.07121193408966064
Batch 29/64 loss: -0.06134730577468872
Batch 30/64 loss: -0.026774108409881592
Batch 31/64 loss: -0.0572735071182251
Batch 32/64 loss: -0.03889644145965576
Batch 33/64 loss: -0.06368660926818848
Batch 34/64 loss: -0.05115973949432373
Batch 35/64 loss: -0.07088792324066162
Batch 36/64 loss: -0.060737013816833496
Batch 37/64 loss: -0.060332417488098145
Batch 38/64 loss: -0.062034666538238525
Batch 39/64 loss: -0.053779006004333496
Batch 40/64 loss: -0.05686473846435547
Batch 41/64 loss: -0.06397354602813721
Batch 42/64 loss: -0.055425405502319336
Batch 43/64 loss: -0.05648595094680786
Batch 44/64 loss: -0.06454217433929443
Batch 45/64 loss: -0.06710481643676758
Batch 46/64 loss: -0.06621837615966797
Batch 47/64 loss: -0.07615971565246582
Batch 48/64 loss: -0.05305039882659912
Batch 49/64 loss: -0.06568729877471924
Batch 50/64 loss: -0.06098717451095581
Batch 51/64 loss: -0.07511276006698608
Batch 52/64 loss: -0.06257808208465576
Batch 53/64 loss: -0.06427168846130371
Batch 54/64 loss: -0.06587517261505127
Batch 55/64 loss: -0.0646061897277832
Batch 56/64 loss: -0.06884455680847168
Batch 57/64 loss: -0.06816405057907104
Batch 58/64 loss: -0.06310951709747314
Batch 59/64 loss: -0.06524813175201416
Batch 60/64 loss: -0.06379115581512451
Batch 61/64 loss: -0.049663424491882324
Batch 62/64 loss: -0.07249897718429565
Batch 63/64 loss: -0.0655815601348877
Batch 64/64 loss: -0.04481470584869385
Epoch 130  Train loss: -0.0610489429212084  Val loss: -0.0254995511569518
Epoch 131
-------------------------------
Batch 1/64 loss: -0.06950521469116211
Batch 2/64 loss: -0.04898148775100708
Batch 3/64 loss: -0.05968070030212402
Batch 4/64 loss: -0.05456644296646118
Batch 5/64 loss: -0.06645584106445312
Batch 6/64 loss: -0.06560397148132324
Batch 7/64 loss: -0.05623829364776611
Batch 8/64 loss: -0.06918901205062866
Batch 9/64 loss: -0.07304412126541138
Batch 10/64 loss: -0.06742912530899048
Batch 11/64 loss: -0.07584154605865479
Batch 12/64 loss: -0.06431996822357178
Batch 13/64 loss: -0.049445509910583496
Batch 14/64 loss: -0.06530606746673584
Batch 15/64 loss: -0.04178053140640259
Batch 16/64 loss: -0.0757601261138916
Batch 17/64 loss: -0.04786282777786255
Batch 18/64 loss: -0.07099968194961548
Batch 19/64 loss: -0.05006915330886841
Batch 20/64 loss: -0.05772042274475098
Batch 21/64 loss: -0.07895594835281372
Batch 22/64 loss: -0.06721913814544678
Batch 23/64 loss: -0.06097400188446045
Batch 24/64 loss: -0.06361126899719238
Batch 25/64 loss: -0.076560378074646
Batch 26/64 loss: -0.05219310522079468
Batch 27/64 loss: -0.065224289894104
Batch 28/64 loss: -0.05769515037536621
Batch 29/64 loss: -0.05958437919616699
Batch 30/64 loss: -0.05983090400695801
Batch 31/64 loss: -0.05019855499267578
Batch 32/64 loss: -0.06166684627532959
Batch 33/64 loss: -0.07076239585876465
Batch 34/64 loss: -0.06539523601531982
Batch 35/64 loss: -0.052683353424072266
Batch 36/64 loss: -0.057759642601013184
Batch 37/64 loss: -0.07408648729324341
Batch 38/64 loss: -0.05255478620529175
Batch 39/64 loss: -0.040962278842926025
Batch 40/64 loss: -0.0624852180480957
Batch 41/64 loss: -0.06046414375305176
Batch 42/64 loss: -0.05242717266082764
Batch 43/64 loss: -0.040463149547576904
Batch 44/64 loss: -0.06056499481201172
Batch 45/64 loss: -0.013378381729125977
Batch 46/64 loss: -0.07022583484649658
Batch 47/64 loss: -0.0706322193145752
Batch 48/64 loss: -0.06558412313461304
Batch 49/64 loss: -0.04586261510848999
Batch 50/64 loss: -0.06306511163711548
Batch 51/64 loss: -0.08293992280960083
Batch 52/64 loss: -0.07373052835464478
Batch 53/64 loss: -0.06426292657852173
Batch 54/64 loss: -0.05451524257659912
Batch 55/64 loss: -0.06351113319396973
Batch 56/64 loss: -0.042922377586364746
Batch 57/64 loss: -0.07712602615356445
Batch 58/64 loss: -0.0691608190536499
Batch 59/64 loss: -0.06971704959869385
Batch 60/64 loss: -0.06162142753601074
Batch 61/64 loss: -0.07038819789886475
Batch 62/64 loss: -0.061469316482543945
Batch 63/64 loss: -0.04778778553009033
Batch 64/64 loss: -0.06853830814361572
Epoch 131  Train loss: -0.06107337474822998  Val loss: -0.03326340332064023
Saving best model, epoch: 131
Epoch 132
-------------------------------
Batch 1/64 loss: -0.060160934925079346
Batch 2/64 loss: -0.05450403690338135
Batch 3/64 loss: -0.06975150108337402
Batch 4/64 loss: -0.06216710805892944
Batch 5/64 loss: -0.06905180215835571
Batch 6/64 loss: -0.0586315393447876
Batch 7/64 loss: -0.05758523941040039
Batch 8/64 loss: -0.0795324444770813
Batch 9/64 loss: -0.06386935710906982
Batch 10/64 loss: -0.05991768836975098
Batch 11/64 loss: -0.06320464611053467
Batch 12/64 loss: -0.05452573299407959
Batch 13/64 loss: -0.08410841226577759
Batch 14/64 loss: -0.07532370090484619
Batch 15/64 loss: -0.051166415214538574
Batch 16/64 loss: -0.05672985315322876
Batch 17/64 loss: -0.06630021333694458
Batch 18/64 loss: -0.04898017644882202
Batch 19/64 loss: -0.07801061868667603
Batch 20/64 loss: -0.07361888885498047
Batch 21/64 loss: -0.059380948543548584
Batch 22/64 loss: -0.05188179016113281
Batch 23/64 loss: -0.0737268328666687
Batch 24/64 loss: -0.060767292976379395
Batch 25/64 loss: -0.051025986671447754
Batch 26/64 loss: -0.06576311588287354
Batch 27/64 loss: -0.06789004802703857
Batch 28/64 loss: -0.06850188970565796
Batch 29/64 loss: -0.06491565704345703
Batch 30/64 loss: -0.06194096803665161
Batch 31/64 loss: -0.05884969234466553
Batch 32/64 loss: -0.07221662998199463
Batch 33/64 loss: -0.04494476318359375
Batch 34/64 loss: -0.06870460510253906
Batch 35/64 loss: -0.06919056177139282
Batch 36/64 loss: -0.07660967111587524
Batch 37/64 loss: -0.08250927925109863
Batch 38/64 loss: -0.0553818941116333
Batch 39/64 loss: -0.06739354133605957
Batch 40/64 loss: -0.04748409986495972
Batch 41/64 loss: -0.0835006833076477
Batch 42/64 loss: -0.06060558557510376
Batch 43/64 loss: -0.06146430969238281
Batch 44/64 loss: -0.06120091676712036
Batch 45/64 loss: -0.07192355394363403
Batch 46/64 loss: -0.04504585266113281
Batch 47/64 loss: -0.06872212886810303
Batch 48/64 loss: -0.08096003532409668
Batch 49/64 loss: -0.0706365704536438
Batch 50/64 loss: -0.07451707124710083
Batch 51/64 loss: -0.0464290976524353
Batch 52/64 loss: -0.03583419322967529
Batch 53/64 loss: -0.07000815868377686
Batch 54/64 loss: -0.05975455045700073
Batch 55/64 loss: -0.059967100620269775
Batch 56/64 loss: -0.0515097975730896
Batch 57/64 loss: -0.06350713968276978
Batch 58/64 loss: -0.07074481248855591
Batch 59/64 loss: -0.07240539789199829
Batch 60/64 loss: -0.07460391521453857
Batch 61/64 loss: -0.06593489646911621
Batch 62/64 loss: -0.058843016624450684
Batch 63/64 loss: -0.060043394565582275
Batch 64/64 loss: -0.05935108661651611
Epoch 132  Train loss: -0.06382580504697911  Val loss: -0.030477239503893246
Epoch 133
-------------------------------
Batch 1/64 loss: -0.06756186485290527
Batch 2/64 loss: -0.06056421995162964
Batch 3/64 loss: -0.06575894355773926
Batch 4/64 loss: -0.06873470544815063
Batch 5/64 loss: -0.049645841121673584
Batch 6/64 loss: -0.07158517837524414
Batch 7/64 loss: -0.05850011110305786
Batch 8/64 loss: -0.07440370321273804
Batch 9/64 loss: -0.07515287399291992
Batch 10/64 loss: -0.06808090209960938
Batch 11/64 loss: -0.05671656131744385
Batch 12/64 loss: -0.06770634651184082
Batch 13/64 loss: -0.06537270545959473
Batch 14/64 loss: -0.04776906967163086
Batch 15/64 loss: -0.04388695955276489
Batch 16/64 loss: -0.05916237831115723
Batch 17/64 loss: -0.06201601028442383
Batch 18/64 loss: -0.0790439248085022
Batch 19/64 loss: -0.07421374320983887
Batch 20/64 loss: -0.06382870674133301
Batch 21/64 loss: -0.05513566732406616
Batch 22/64 loss: -0.07580065727233887
Batch 23/64 loss: -0.06972628831863403
Batch 24/64 loss: -0.07841718196868896
Batch 25/64 loss: -0.07925409078598022
Batch 26/64 loss: -0.08422207832336426
Batch 27/64 loss: -0.06757652759552002
Batch 28/64 loss: -0.05541634559631348
Batch 29/64 loss: -0.06898796558380127
Batch 30/64 loss: -0.05089771747589111
Batch 31/64 loss: -0.07441383600234985
Batch 32/64 loss: -0.05877566337585449
Batch 33/64 loss: -0.05012869834899902
Batch 34/64 loss: -0.08388543128967285
Batch 35/64 loss: -0.07038325071334839
Batch 36/64 loss: -0.04338258504867554
Batch 37/64 loss: -0.06939643621444702
Batch 38/64 loss: -0.07766848802566528
Batch 39/64 loss: -0.07040220499038696
Batch 40/64 loss: -0.07880395650863647
Batch 41/64 loss: -0.060814619064331055
Batch 42/64 loss: -0.06352019309997559
Batch 43/64 loss: -0.07652395963668823
Batch 44/64 loss: -0.029640018939971924
Batch 45/64 loss: -0.06787163019180298
Batch 46/64 loss: -0.0630577802658081
Batch 47/64 loss: -0.06362181901931763
Batch 48/64 loss: -0.03025531768798828
Batch 49/64 loss: -0.07140189409255981
Batch 50/64 loss: -0.06113696098327637
Batch 51/64 loss: -0.06113964319229126
Batch 52/64 loss: -0.039001643657684326
Batch 53/64 loss: -0.054337382316589355
Batch 54/64 loss: -0.059961915016174316
Batch 55/64 loss: -0.04127800464630127
Batch 56/64 loss: -0.04520547389984131
Batch 57/64 loss: -0.05922889709472656
Batch 58/64 loss: -0.06917369365692139
Batch 59/64 loss: -0.07185083627700806
Batch 60/64 loss: -0.0568506121635437
Batch 61/64 loss: -0.07842165231704712
Batch 62/64 loss: -0.041805386543273926
Batch 63/64 loss: -0.06849777698516846
Batch 64/64 loss: -0.054451942443847656
Epoch 133  Train loss: -0.06302456294789034  Val loss: -0.02709411673529451
Epoch 134
-------------------------------
Batch 1/64 loss: -0.0651671290397644
Batch 2/64 loss: -0.0667499303817749
Batch 3/64 loss: -0.05687522888183594
Batch 4/64 loss: -0.06946533918380737
Batch 5/64 loss: -0.03770637512207031
Batch 6/64 loss: -0.047931015491485596
Batch 7/64 loss: -0.07105934619903564
Batch 8/64 loss: -0.059721291065216064
Batch 9/64 loss: -0.06438279151916504
Batch 10/64 loss: -0.05659842491149902
Batch 11/64 loss: -0.072482168674469
Batch 12/64 loss: -0.08093476295471191
Batch 13/64 loss: -0.06160116195678711
Batch 14/64 loss: -0.07044553756713867
Batch 15/64 loss: -0.07165950536727905
Batch 16/64 loss: -0.06600415706634521
Batch 17/64 loss: -0.06625938415527344
Batch 18/64 loss: -0.06261634826660156
Batch 19/64 loss: -0.0789063572883606
Batch 20/64 loss: -0.0657152533531189
Batch 21/64 loss: -0.08755910396575928
Batch 22/64 loss: -0.06297755241394043
Batch 23/64 loss: -0.06772118806838989
Batch 24/64 loss: -0.0761914849281311
Batch 25/64 loss: -0.06429493427276611
Batch 26/64 loss: -0.06684774160385132
Batch 27/64 loss: -0.05233311653137207
Batch 28/64 loss: -0.05310267210006714
Batch 29/64 loss: -0.07286202907562256
Batch 30/64 loss: -0.04120135307312012
Batch 31/64 loss: -0.05547386407852173
Batch 32/64 loss: -0.06909525394439697
Batch 33/64 loss: -0.038602471351623535
Batch 34/64 loss: -0.06785959005355835
Batch 35/64 loss: -0.05638432502746582
Batch 36/64 loss: -0.04990202188491821
Batch 37/64 loss: -0.06998956203460693
Batch 38/64 loss: -0.0660211443901062
Batch 39/64 loss: -0.05977487564086914
Batch 40/64 loss: -0.06473362445831299
Batch 41/64 loss: -0.048111557960510254
Batch 42/64 loss: -0.05374497175216675
Batch 43/64 loss: -0.045851826667785645
Batch 44/64 loss: -0.06343543529510498
Batch 45/64 loss: -0.06433326005935669
Batch 46/64 loss: -0.06787419319152832
Batch 47/64 loss: -0.06663274765014648
Batch 48/64 loss: -0.06516563892364502
Batch 49/64 loss: -0.0789104700088501
Batch 50/64 loss: -0.056079208850860596
Batch 51/64 loss: -0.07121682167053223
Batch 52/64 loss: -0.05265408754348755
Batch 53/64 loss: -0.07258909940719604
Batch 54/64 loss: -0.04370772838592529
Batch 55/64 loss: -0.03748023509979248
Batch 56/64 loss: -0.05328798294067383
Batch 57/64 loss: -0.07218062877655029
Batch 58/64 loss: -0.04448091983795166
Batch 59/64 loss: -0.06430470943450928
Batch 60/64 loss: -0.07954776287078857
Batch 61/64 loss: -0.0754808783531189
Batch 62/64 loss: -0.07690179347991943
Batch 63/64 loss: -0.06159406900405884
Batch 64/64 loss: -0.06033968925476074
Epoch 134  Train loss: -0.06268282684625363  Val loss: -0.03410967027198818
Saving best model, epoch: 134
Epoch 135
-------------------------------
Batch 1/64 loss: -0.059408724308013916
Batch 2/64 loss: -0.0553128719329834
Batch 3/64 loss: -0.06915664672851562
Batch 4/64 loss: -0.03210043907165527
Batch 5/64 loss: -0.06878411769866943
Batch 6/64 loss: -0.07371121644973755
Batch 7/64 loss: -0.03966104984283447
Batch 8/64 loss: -0.057216763496398926
Batch 9/64 loss: -0.062468767166137695
Batch 10/64 loss: -0.06898993253707886
Batch 11/64 loss: -0.036643147468566895
Batch 12/64 loss: -0.05884349346160889
Batch 13/64 loss: -0.0759546160697937
Batch 14/64 loss: -0.0694846510887146
Batch 15/64 loss: -0.06200319528579712
Batch 16/64 loss: -0.054907023906707764
Batch 17/64 loss: -0.06874388456344604
Batch 18/64 loss: -0.05663776397705078
Batch 19/64 loss: -0.07079261541366577
Batch 20/64 loss: -0.06267130374908447
Batch 21/64 loss: -0.04324871301651001
Batch 22/64 loss: -0.06109654903411865
Batch 23/64 loss: -0.07826757431030273
Batch 24/64 loss: -0.05457139015197754
Batch 25/64 loss: -0.07466602325439453
Batch 26/64 loss: -0.07959461212158203
Batch 27/64 loss: -0.05673009157180786
Batch 28/64 loss: -0.07594192028045654
Batch 29/64 loss: -0.07162284851074219
Batch 30/64 loss: -0.05458015203475952
Batch 31/64 loss: -0.05931180715560913
Batch 32/64 loss: -0.06191736459732056
Batch 33/64 loss: -0.07550859451293945
Batch 34/64 loss: -0.06735193729400635
Batch 35/64 loss: -0.06881392002105713
Batch 36/64 loss: -0.058850884437561035
Batch 37/64 loss: -0.09580707550048828
Batch 38/64 loss: -0.06183689832687378
Batch 39/64 loss: -0.06185197830200195
Batch 40/64 loss: -0.06935608386993408
Batch 41/64 loss: -0.07591485977172852
Batch 42/64 loss: -0.05694502592086792
Batch 43/64 loss: -0.06264400482177734
Batch 44/64 loss: -0.08314579725265503
Batch 45/64 loss: -0.0726553201675415
Batch 46/64 loss: -0.0623704195022583
Batch 47/64 loss: -0.06441229581832886
Batch 48/64 loss: -0.07024800777435303
Batch 49/64 loss: -0.05547773838043213
Batch 50/64 loss: -0.06004774570465088
Batch 51/64 loss: -0.06660765409469604
Batch 52/64 loss: -0.055357933044433594
Batch 53/64 loss: -0.03053903579711914
Batch 54/64 loss: -0.055950701236724854
Batch 55/64 loss: -0.06339466571807861
Batch 56/64 loss: -0.06835222244262695
Batch 57/64 loss: -0.05262911319732666
Batch 58/64 loss: -0.07723230123519897
Batch 59/64 loss: -0.05066502094268799
Batch 60/64 loss: -0.0675438642501831
Batch 61/64 loss: -0.06660020351409912
Batch 62/64 loss: -0.05352205038070679
Batch 63/64 loss: -0.08329993486404419
Batch 64/64 loss: -0.0573582649230957
Epoch 135  Train loss: -0.06326263932620778  Val loss: -0.025528462277245277
Epoch 136
-------------------------------
Batch 1/64 loss: -0.07744938135147095
Batch 2/64 loss: -0.06523555517196655
Batch 3/64 loss: -0.07897746562957764
Batch 4/64 loss: -0.04754459857940674
Batch 5/64 loss: -0.07003122568130493
Batch 6/64 loss: -0.08152097463607788
Batch 7/64 loss: -0.06556493043899536
Batch 8/64 loss: -0.0755237340927124
Batch 9/64 loss: -0.08317935466766357
Batch 10/64 loss: -0.07488316297531128
Batch 11/64 loss: -0.05125129222869873
Batch 12/64 loss: -0.06884998083114624
Batch 13/64 loss: -0.06706464290618896
Batch 14/64 loss: -0.07023495435714722
Batch 15/64 loss: -0.06795930862426758
Batch 16/64 loss: -0.059957683086395264
Batch 17/64 loss: -0.08299273252487183
Batch 18/64 loss: -0.0702977180480957
Batch 19/64 loss: -0.06657612323760986
Batch 20/64 loss: -0.07023054361343384
Batch 21/64 loss: -0.06342107057571411
Batch 22/64 loss: -0.0749555230140686
Batch 23/64 loss: -0.05796760320663452
Batch 24/64 loss: -0.061792850494384766
Batch 25/64 loss: -0.05346667766571045
Batch 26/64 loss: -0.060957491397857666
Batch 27/64 loss: -0.061670541763305664
Batch 28/64 loss: -0.0701029896736145
Batch 29/64 loss: -0.050011396408081055
Batch 30/64 loss: -0.052713632583618164
Batch 31/64 loss: -0.05044257640838623
Batch 32/64 loss: -0.07004153728485107
Batch 33/64 loss: -0.03376656770706177
Batch 34/64 loss: -0.05387240648269653
Batch 35/64 loss: -0.06228828430175781
Batch 36/64 loss: -0.0596163272857666
Batch 37/64 loss: -0.06725066900253296
Batch 38/64 loss: -0.04501032829284668
Batch 39/64 loss: -0.07450777292251587
Batch 40/64 loss: -0.0804603099822998
Batch 41/64 loss: -0.0772971510887146
Batch 42/64 loss: -0.05122339725494385
Batch 43/64 loss: -0.06764918565750122
Batch 44/64 loss: -0.051432013511657715
Batch 45/64 loss: -0.07422423362731934
Batch 46/64 loss: -0.04843264818191528
Batch 47/64 loss: -0.06360632181167603
Batch 48/64 loss: -0.07814085483551025
Batch 49/64 loss: -0.07360059022903442
Batch 50/64 loss: -0.04727065563201904
Batch 51/64 loss: -0.05178636312484741
Batch 52/64 loss: -0.06735473871231079
Batch 53/64 loss: -0.06745976209640503
Batch 54/64 loss: -0.044596195220947266
Batch 55/64 loss: -0.06792455911636353
Batch 56/64 loss: -0.06409823894500732
Batch 57/64 loss: -0.061495959758758545
Batch 58/64 loss: -0.06695091724395752
Batch 59/64 loss: -0.04475200176239014
Batch 60/64 loss: -0.07556307315826416
Batch 61/64 loss: -0.08102869987487793
Batch 62/64 loss: -0.07246148586273193
Batch 63/64 loss: -0.07123738527297974
Batch 64/64 loss: -0.08093923330307007
Epoch 136  Train loss: -0.06478325923283895  Val loss: -0.033283496640392186
Epoch 137
-------------------------------
Batch 1/64 loss: -0.07174575328826904
Batch 2/64 loss: -0.06328052282333374
Batch 3/64 loss: -0.0735272765159607
Batch 4/64 loss: -0.07813811302185059
Batch 5/64 loss: -0.06811940670013428
Batch 6/64 loss: -0.06726288795471191
Batch 7/64 loss: -0.07725107669830322
Batch 8/64 loss: -0.08421707153320312
Batch 9/64 loss: -0.08278012275695801
Batch 10/64 loss: -0.07043629884719849
Batch 11/64 loss: -0.06546914577484131
Batch 12/64 loss: -0.06144756078720093
Batch 13/64 loss: -0.061552584171295166
Batch 14/64 loss: -0.07441306114196777
Batch 15/64 loss: -0.07111525535583496
Batch 16/64 loss: -0.07042813301086426
Batch 17/64 loss: -0.06396651268005371
Batch 18/64 loss: -0.08424478769302368
Batch 19/64 loss: -0.07441127300262451
Batch 20/64 loss: -0.06190764904022217
Batch 21/64 loss: -0.07746624946594238
Batch 22/64 loss: -0.04579895734786987
Batch 23/64 loss: -0.06175035238265991
Batch 24/64 loss: -0.07156658172607422
Batch 25/64 loss: -0.07578408718109131
Batch 26/64 loss: -0.07501477003097534
Batch 27/64 loss: -0.06267595291137695
Batch 28/64 loss: -0.06890624761581421
Batch 29/64 loss: -0.0608675479888916
Batch 30/64 loss: -0.05462312698364258
Batch 31/64 loss: -0.0506129264831543
Batch 32/64 loss: -0.07900875806808472
Batch 33/64 loss: -0.04766649007797241
Batch 34/64 loss: -0.044452786445617676
Batch 35/64 loss: -0.06575566530227661
Batch 36/64 loss: -0.08442193269729614
Batch 37/64 loss: -0.05301356315612793
Batch 38/64 loss: -0.0766671895980835
Batch 39/64 loss: -0.06511932611465454
Batch 40/64 loss: -0.08265888690948486
Batch 41/64 loss: -0.07827454805374146
Batch 42/64 loss: -0.04946011304855347
Batch 43/64 loss: -0.047679603099823
Batch 44/64 loss: -0.07854610681533813
Batch 45/64 loss: -0.06381034851074219
Batch 46/64 loss: -0.06977015733718872
Batch 47/64 loss: -0.06369513273239136
Batch 48/64 loss: -0.051720619201660156
Batch 49/64 loss: -0.07687658071517944
Batch 50/64 loss: -0.0733875036239624
Batch 51/64 loss: -0.08615654706954956
Batch 52/64 loss: -0.06156390905380249
Batch 53/64 loss: -0.0627446174621582
Batch 54/64 loss: -0.07091385126113892
Batch 55/64 loss: -0.06687599420547485
Batch 56/64 loss: -0.06983965635299683
Batch 57/64 loss: -0.04097020626068115
Batch 58/64 loss: -0.06629198789596558
Batch 59/64 loss: -0.07357668876647949
Batch 60/64 loss: -0.0498502254486084
Batch 61/64 loss: -0.05888849496841431
Batch 62/64 loss: -0.04615175724029541
Batch 63/64 loss: -0.07447808980941772
Batch 64/64 loss: -0.08021944761276245
Epoch 137  Train loss: -0.06699973672044043  Val loss: -0.024737397215210694
Epoch 138
-------------------------------
Batch 1/64 loss: -0.061866581439971924
Batch 2/64 loss: -0.059219956398010254
Batch 3/64 loss: -0.06512492895126343
Batch 4/64 loss: -0.06145590543746948
Batch 5/64 loss: -0.058014750480651855
Batch 6/64 loss: -0.06388115882873535
Batch 7/64 loss: -0.0706067681312561
Batch 8/64 loss: -0.07393133640289307
Batch 9/64 loss: -0.07206833362579346
Batch 10/64 loss: -0.07176953554153442
Batch 11/64 loss: -0.07907211780548096
Batch 12/64 loss: -0.08002156019210815
Batch 13/64 loss: -0.07461029291152954
Batch 14/64 loss: -0.07404083013534546
Batch 15/64 loss: -0.07219135761260986
Batch 16/64 loss: -0.06281685829162598
Batch 17/64 loss: -0.08084923028945923
Batch 18/64 loss: -0.08698195219039917
Batch 19/64 loss: -0.047024428844451904
Batch 20/64 loss: -0.058494389057159424
Batch 21/64 loss: -0.04366481304168701
Batch 22/64 loss: -0.06925511360168457
Batch 23/64 loss: -0.04984015226364136
Batch 24/64 loss: -0.07395410537719727
Batch 25/64 loss: -0.0811772346496582
Batch 26/64 loss: -0.054723262786865234
Batch 27/64 loss: -0.08386826515197754
Batch 28/64 loss: -0.06542468070983887
Batch 29/64 loss: -0.05784231424331665
Batch 30/64 loss: -0.06937998533248901
Batch 31/64 loss: -0.07422971725463867
Batch 32/64 loss: -0.06901472806930542
Batch 33/64 loss: -0.09578907489776611
Batch 34/64 loss: -0.06913280487060547
Batch 35/64 loss: -0.04707205295562744
Batch 36/64 loss: -0.06922811269760132
Batch 37/64 loss: -0.07574462890625
Batch 38/64 loss: -0.06923526525497437
Batch 39/64 loss: -0.07731568813323975
Batch 40/64 loss: -0.07229989767074585
Batch 41/64 loss: -0.06393587589263916
Batch 42/64 loss: -0.07611274719238281
Batch 43/64 loss: -0.07643061876296997
Batch 44/64 loss: -0.06400811672210693
Batch 45/64 loss: -0.0682978630065918
Batch 46/64 loss: -0.06045502424240112
Batch 47/64 loss: -0.06671327352523804
Batch 48/64 loss: -0.077980637550354
Batch 49/64 loss: -0.07382404804229736
Batch 50/64 loss: -0.06733047962188721
Batch 51/64 loss: -0.07229053974151611
Batch 52/64 loss: -0.04973864555358887
Batch 53/64 loss: -0.06329149007797241
Batch 54/64 loss: -0.08703827857971191
Batch 55/64 loss: -0.06938987970352173
Batch 56/64 loss: -0.08113175630569458
Batch 57/64 loss: -0.06538385152816772
Batch 58/64 loss: -0.055726706981658936
Batch 59/64 loss: -0.07760018110275269
Batch 60/64 loss: -0.06272417306900024
Batch 61/64 loss: -0.061921119689941406
Batch 62/64 loss: -0.06367194652557373
Batch 63/64 loss: -0.06519395112991333
Batch 64/64 loss: -0.054636478424072266
Epoch 138  Train loss: -0.0682886703341615  Val loss: -0.02479138652893276
Epoch 139
-------------------------------
Batch 1/64 loss: -0.07815206050872803
Batch 2/64 loss: -0.07030922174453735
Batch 3/64 loss: -0.06733667850494385
Batch 4/64 loss: -0.05374646186828613
Batch 5/64 loss: -0.05968928337097168
Batch 6/64 loss: -0.06914740800857544
Batch 7/64 loss: -0.07203394174575806
Batch 8/64 loss: -0.05801057815551758
Batch 9/64 loss: -0.06585574150085449
Batch 10/64 loss: -0.052918076515197754
Batch 11/64 loss: -0.0681537389755249
Batch 12/64 loss: -0.0789719820022583
Batch 13/64 loss: -0.06227511167526245
Batch 14/64 loss: -0.06916487216949463
Batch 15/64 loss: -0.07010108232498169
Batch 16/64 loss: -0.06747663021087646
Batch 17/64 loss: -0.0654822587966919
Batch 18/64 loss: -0.07618504762649536
Batch 19/64 loss: -0.08004587888717651
Batch 20/64 loss: -0.0676468014717102
Batch 21/64 loss: -0.06270289421081543
Batch 22/64 loss: -0.07274305820465088
Batch 23/64 loss: -0.057305753231048584
Batch 24/64 loss: -0.0815308690071106
Batch 25/64 loss: -0.0819578766822815
Batch 26/64 loss: -0.07518422603607178
Batch 27/64 loss: -0.06970852613449097
Batch 28/64 loss: -0.04931187629699707
Batch 29/64 loss: -0.07332587242126465
Batch 30/64 loss: -0.05948185920715332
Batch 31/64 loss: -0.07217836380004883
Batch 32/64 loss: -0.06828451156616211
Batch 33/64 loss: -0.08414125442504883
Batch 34/64 loss: -0.05516695976257324
Batch 35/64 loss: -0.0679519772529602
Batch 36/64 loss: -0.06476861238479614
Batch 37/64 loss: -0.056679606437683105
Batch 38/64 loss: -0.06995058059692383
Batch 39/64 loss: -0.0765644907951355
Batch 40/64 loss: -0.057546138763427734
Batch 41/64 loss: -0.06794893741607666
Batch 42/64 loss: -0.06050592660903931
Batch 43/64 loss: -0.08188366889953613
Batch 44/64 loss: -0.039122045040130615
Batch 45/64 loss: -0.05854862928390503
Batch 46/64 loss: -0.07296919822692871
Batch 47/64 loss: -0.07718765735626221
Batch 48/64 loss: -0.07038354873657227
Batch 49/64 loss: -0.05305379629135132
Batch 50/64 loss: -0.056043386459350586
Batch 51/64 loss: -0.07459068298339844
Batch 52/64 loss: -0.06828731298446655
Batch 53/64 loss: -0.04684871435165405
Batch 54/64 loss: -0.06127685308456421
Batch 55/64 loss: -0.05263864994049072
Batch 56/64 loss: -0.039429306983947754
Batch 57/64 loss: -0.04954385757446289
Batch 58/64 loss: -0.06036818027496338
Batch 59/64 loss: -0.08270740509033203
Batch 60/64 loss: -0.06881511211395264
Batch 61/64 loss: -0.08268791437149048
Batch 62/64 loss: -0.06842267513275146
Batch 63/64 loss: -0.05770576000213623
Batch 64/64 loss: -0.08227193355560303
Epoch 139  Train loss: -0.06622527580635221  Val loss: -0.029445702267676285
Epoch 140
-------------------------------
Batch 1/64 loss: -0.07966715097427368
Batch 2/64 loss: -0.0891115665435791
Batch 3/64 loss: -0.060678720474243164
Batch 4/64 loss: -0.08586466312408447
Batch 5/64 loss: -0.07273733615875244
Batch 6/64 loss: -0.06829982995986938
Batch 7/64 loss: -0.060821592807769775
Batch 8/64 loss: -0.0703544020652771
Batch 9/64 loss: -0.07144057750701904
Batch 10/64 loss: -0.07727622985839844
Batch 11/64 loss: -0.0687330961227417
Batch 12/64 loss: -0.07066923379898071
Batch 13/64 loss: -0.08260416984558105
Batch 14/64 loss: -0.060539186000823975
Batch 15/64 loss: -0.04454129934310913
Batch 16/64 loss: -0.0666121244430542
Batch 17/64 loss: -0.07543766498565674
Batch 18/64 loss: -0.032678186893463135
Batch 19/64 loss: -0.06683129072189331
Batch 20/64 loss: -0.07829821109771729
Batch 21/64 loss: -0.07810771465301514
Batch 22/64 loss: -0.07225865125656128
Batch 23/64 loss: -0.053968966007232666
Batch 24/64 loss: -0.07375818490982056
Batch 25/64 loss: -0.059634625911712646
Batch 26/64 loss: -0.07554709911346436
Batch 27/64 loss: -0.06944215297698975
Batch 28/64 loss: -0.05962848663330078
Batch 29/64 loss: -0.08023995161056519
Batch 30/64 loss: -0.07011890411376953
Batch 31/64 loss: -0.0744180679321289
Batch 32/64 loss: -0.05346798896789551
Batch 33/64 loss: -0.08203905820846558
Batch 34/64 loss: -0.06861531734466553
Batch 35/64 loss: -0.05645644664764404
Batch 36/64 loss: -0.05438202619552612
Batch 37/64 loss: -0.08294063806533813
Batch 38/64 loss: -0.0825318694114685
Batch 39/64 loss: -0.04654192924499512
Batch 40/64 loss: -0.058986008167266846
Batch 41/64 loss: -0.057292938232421875
Batch 42/64 loss: -0.07042741775512695
Batch 43/64 loss: -0.07890552282333374
Batch 44/64 loss: -0.0695723295211792
Batch 45/64 loss: -0.05978834629058838
Batch 46/64 loss: -0.07372128963470459
Batch 47/64 loss: -0.05967819690704346
Batch 48/64 loss: -0.07780998945236206
Batch 49/64 loss: -0.07347702980041504
Batch 50/64 loss: -0.07908964157104492
Batch 51/64 loss: -0.051532864570617676
Batch 52/64 loss: -0.08797746896743774
Batch 53/64 loss: -0.07432842254638672
Batch 54/64 loss: -0.0765761137008667
Batch 55/64 loss: -0.06772136688232422
Batch 56/64 loss: -0.07137787342071533
Batch 57/64 loss: -0.05762159824371338
Batch 58/64 loss: -0.06507289409637451
Batch 59/64 loss: -0.0724329948425293
Batch 60/64 loss: -0.06921178102493286
Batch 61/64 loss: -0.0771784782409668
Batch 62/64 loss: -0.07656288146972656
Batch 63/64 loss: -0.07038646936416626
Batch 64/64 loss: -0.06447231769561768
Epoch 140  Train loss: -0.06905692184672636  Val loss: -0.030018886749687064
Epoch 141
-------------------------------
Batch 1/64 loss: -0.0654187798500061
Batch 2/64 loss: -0.08091789484024048
Batch 3/64 loss: -0.07066142559051514
Batch 4/64 loss: -0.06808984279632568
Batch 5/64 loss: -0.0682099461555481
Batch 6/64 loss: -0.07040667533874512
Batch 7/64 loss: -0.0697031021118164
Batch 8/64 loss: -0.08541882038116455
Batch 9/64 loss: -0.07322072982788086
Batch 10/64 loss: -0.061753809452056885
Batch 11/64 loss: -0.057863473892211914
Batch 12/64 loss: -0.057780563831329346
Batch 13/64 loss: -0.07249534130096436
Batch 14/64 loss: -0.06640398502349854
Batch 15/64 loss: -0.06957215070724487
Batch 16/64 loss: -0.0574113130569458
Batch 17/64 loss: -0.0867316722869873
Batch 18/64 loss: -0.07475972175598145
Batch 19/64 loss: -0.06612652540206909
Batch 20/64 loss: -0.05078935623168945
Batch 21/64 loss: -0.0725778341293335
Batch 22/64 loss: -0.06264287233352661
Batch 23/64 loss: -0.048347413539886475
Batch 24/64 loss: -0.0636478066444397
Batch 25/64 loss: -0.07063478231430054
Batch 26/64 loss: -0.058726370334625244
Batch 27/64 loss: -0.0573042631149292
Batch 28/64 loss: -0.0726436972618103
Batch 29/64 loss: -0.059206247329711914
Batch 30/64 loss: -0.06702989339828491
Batch 31/64 loss: -0.05375528335571289
Batch 32/64 loss: -0.06161177158355713
Batch 33/64 loss: -0.05774199962615967
Batch 34/64 loss: -0.06252503395080566
Batch 35/64 loss: -0.07482409477233887
Batch 36/64 loss: -0.07167994976043701
Batch 37/64 loss: -0.0750383734703064
Batch 38/64 loss: -0.06889092922210693
Batch 39/64 loss: -0.06823396682739258
Batch 40/64 loss: -0.04107499122619629
Batch 41/64 loss: -0.05141782760620117
Batch 42/64 loss: -0.06311607360839844
Batch 43/64 loss: -0.07057547569274902
Batch 44/64 loss: -0.078094482421875
Batch 45/64 loss: -0.0485040545463562
Batch 46/64 loss: -0.06142306327819824
Batch 47/64 loss: -0.06917458772659302
Batch 48/64 loss: -0.06605362892150879
Batch 49/64 loss: -0.06944823265075684
Batch 50/64 loss: -0.0644868016242981
Batch 51/64 loss: -0.06343865394592285
Batch 52/64 loss: -0.06471008062362671
Batch 53/64 loss: -0.05655604600906372
Batch 54/64 loss: -0.06985700130462646
Batch 55/64 loss: -0.05619680881500244
Batch 56/64 loss: -0.0739966630935669
Batch 57/64 loss: -0.07729482650756836
Batch 58/64 loss: -0.053951263427734375
Batch 59/64 loss: -0.04590338468551636
Batch 60/64 loss: -0.07352298498153687
Batch 61/64 loss: -0.07010281085968018
Batch 62/64 loss: -0.0727415680885315
Batch 63/64 loss: -0.08735036849975586
Batch 64/64 loss: -0.06051194667816162
Epoch 141  Train loss: -0.06580664083069446  Val loss: -0.03354360291228671
Epoch 142
-------------------------------
Batch 1/64 loss: -0.08093714714050293
Batch 2/64 loss: -0.060308635234832764
Batch 3/64 loss: -0.0761566162109375
Batch 4/64 loss: -0.06930571794509888
Batch 5/64 loss: -0.07379400730133057
Batch 6/64 loss: -0.05957746505737305
Batch 7/64 loss: -0.08102589845657349
Batch 8/64 loss: -0.07486695051193237
Batch 9/64 loss: -0.0614473819732666
Batch 10/64 loss: -0.05597478151321411
Batch 11/64 loss: -0.07997548580169678
Batch 12/64 loss: -0.0746002197265625
Batch 13/64 loss: -0.07853925228118896
Batch 14/64 loss: -0.0674636960029602
Batch 15/64 loss: -0.07638508081436157
Batch 16/64 loss: -0.06229829788208008
Batch 17/64 loss: -0.0727536678314209
Batch 18/64 loss: -0.07861858606338501
Batch 19/64 loss: -0.07153809070587158
Batch 20/64 loss: -0.04417085647583008
Batch 21/64 loss: -0.06687742471694946
Batch 22/64 loss: -0.06111860275268555
Batch 23/64 loss: -0.07069939374923706
Batch 24/64 loss: -0.061859130859375
Batch 25/64 loss: -0.08334726095199585
Batch 26/64 loss: -0.04659473896026611
Batch 27/64 loss: -0.06534862518310547
Batch 28/64 loss: -0.06738829612731934
Batch 29/64 loss: -0.069638192653656
Batch 30/64 loss: -0.05929088592529297
Batch 31/64 loss: -0.042400479316711426
Batch 32/64 loss: -0.07176238298416138
Batch 33/64 loss: -0.08309125900268555
Batch 34/64 loss: -0.06173264980316162
Batch 35/64 loss: -0.05441772937774658
Batch 36/64 loss: -0.05949658155441284
Batch 37/64 loss: -0.06624674797058105
Batch 38/64 loss: -0.07389271259307861
Batch 39/64 loss: -0.07893967628479004
Batch 40/64 loss: -0.07327497005462646
Batch 41/64 loss: -0.07660835981369019
Batch 42/64 loss: -0.07375317811965942
Batch 43/64 loss: -0.06681227684020996
Batch 44/64 loss: -0.07349979877471924
Batch 45/64 loss: -0.06023061275482178
Batch 46/64 loss: -0.06779813766479492
Batch 47/64 loss: -0.056406378746032715
Batch 48/64 loss: -0.07521283626556396
Batch 49/64 loss: -0.06568789482116699
Batch 50/64 loss: -0.0774964690208435
Batch 51/64 loss: -0.06654584407806396
Batch 52/64 loss: -0.06570637226104736
Batch 53/64 loss: -0.06387370824813843
Batch 54/64 loss: -0.07454538345336914
Batch 55/64 loss: -0.07936060428619385
Batch 56/64 loss: -0.07220613956451416
Batch 57/64 loss: -0.07726180553436279
Batch 58/64 loss: -0.0660696029663086
Batch 59/64 loss: -0.06168651580810547
Batch 60/64 loss: -0.08065366744995117
Batch 61/64 loss: -0.06525170803070068
Batch 62/64 loss: -0.07053858041763306
Batch 63/64 loss: -0.08453285694122314
Batch 64/64 loss: -0.08684659004211426
Epoch 142  Train loss: -0.06908281176697974  Val loss: -0.037554606744104234
Saving best model, epoch: 142
Epoch 143
-------------------------------
Batch 1/64 loss: -0.0759502649307251
Batch 2/64 loss: -0.09116899967193604
Batch 3/64 loss: -0.07429063320159912
Batch 4/64 loss: -0.06740027666091919
Batch 5/64 loss: -0.0793379545211792
Batch 6/64 loss: -0.09182339906692505
Batch 7/64 loss: -0.0794450044631958
Batch 8/64 loss: -0.05742686986923218
Batch 9/64 loss: -0.07338935136795044
Batch 10/64 loss: -0.06760179996490479
Batch 11/64 loss: -0.07167327404022217
Batch 12/64 loss: -0.044872283935546875
Batch 13/64 loss: -0.08328491449356079
Batch 14/64 loss: -0.07333600521087646
Batch 15/64 loss: -0.07450520992279053
Batch 16/64 loss: -0.07584702968597412
Batch 17/64 loss: -0.062153637409210205
Batch 18/64 loss: -0.06675904989242554
Batch 19/64 loss: -0.07048594951629639
Batch 20/64 loss: -0.0809440016746521
Batch 21/64 loss: -0.07381188869476318
Batch 22/64 loss: -0.062484920024871826
Batch 23/64 loss: -0.06433624029159546
Batch 24/64 loss: -0.07231611013412476
Batch 25/64 loss: -0.08380407094955444
Batch 26/64 loss: -0.06357860565185547
Batch 27/64 loss: -0.07367300987243652
Batch 28/64 loss: -0.05453014373779297
Batch 29/64 loss: -0.06125462055206299
Batch 30/64 loss: -0.07205748558044434
Batch 31/64 loss: -0.06728315353393555
Batch 32/64 loss: -0.08755689859390259
Batch 33/64 loss: -0.07008218765258789
Batch 34/64 loss: -0.07242035865783691
Batch 35/64 loss: -0.06704056262969971
Batch 36/64 loss: -0.06721502542495728
Batch 37/64 loss: -0.06195259094238281
Batch 38/64 loss: -0.058029770851135254
Batch 39/64 loss: -0.07544642686843872
Batch 40/64 loss: -0.07340705394744873
Batch 41/64 loss: -0.04781538248062134
Batch 42/64 loss: -0.06664752960205078
Batch 43/64 loss: -0.0685034990310669
Batch 44/64 loss: -0.049083828926086426
Batch 45/64 loss: -0.06650733947753906
Batch 46/64 loss: -0.06970906257629395
Batch 47/64 loss: -0.07373672723770142
Batch 48/64 loss: -0.06057107448577881
Batch 49/64 loss: -0.05027860403060913
Batch 50/64 loss: -0.06605815887451172
Batch 51/64 loss: -0.08473771810531616
Batch 52/64 loss: -0.06740784645080566
Batch 53/64 loss: -0.06208091974258423
Batch 54/64 loss: -0.061014533042907715
Batch 55/64 loss: -0.06613481044769287
Batch 56/64 loss: -0.05202007293701172
Batch 57/64 loss: -0.05284416675567627
Batch 58/64 loss: -0.065868079662323
Batch 59/64 loss: -0.07543104887008667
Batch 60/64 loss: -0.06243562698364258
Batch 61/64 loss: -0.07839906215667725
Batch 62/64 loss: -0.07066106796264648
Batch 63/64 loss: -0.06488692760467529
Batch 64/64 loss: -0.07205373048782349
Epoch 143  Train loss: -0.06871922226513133  Val loss: -0.03147556712127633
Epoch 144
-------------------------------
Batch 1/64 loss: -0.039871275424957275
Batch 2/64 loss: -0.06340163946151733
Batch 3/64 loss: -0.07450246810913086
Batch 4/64 loss: -0.0782538652420044
Batch 5/64 loss: -0.0792422890663147
Batch 6/64 loss: -0.0770685076713562
Batch 7/64 loss: -0.07653182744979858
Batch 8/64 loss: -0.07344281673431396
Batch 9/64 loss: -0.06739473342895508
Batch 10/64 loss: -0.051632821559906006
Batch 11/64 loss: -0.0692717432975769
Batch 12/64 loss: -0.07706481218338013
Batch 13/64 loss: -0.08434683084487915
Batch 14/64 loss: -0.06550329923629761
Batch 15/64 loss: -0.07293224334716797
Batch 16/64 loss: -0.07296931743621826
Batch 17/64 loss: -0.06766033172607422
Batch 18/64 loss: -0.06893068552017212
Batch 19/64 loss: -0.07216763496398926
Batch 20/64 loss: -0.04751557111740112
Batch 21/64 loss: -0.0651140809059143
Batch 22/64 loss: -0.06957221031188965
Batch 23/64 loss: -0.0614507794380188
Batch 24/64 loss: -0.07516860961914062
Batch 25/64 loss: -0.0908089280128479
Batch 26/64 loss: -0.08533579111099243
Batch 27/64 loss: -0.05502057075500488
Batch 28/64 loss: -0.08356291055679321
Batch 29/64 loss: -0.06451994180679321
Batch 30/64 loss: -0.07242953777313232
Batch 31/64 loss: -0.0744524598121643
Batch 32/64 loss: -0.05338066816329956
Batch 33/64 loss: -0.07073318958282471
Batch 34/64 loss: -0.07258403301239014
Batch 35/64 loss: -0.05532163381576538
Batch 36/64 loss: -0.06672579050064087
Batch 37/64 loss: -0.057739973068237305
Batch 38/64 loss: -0.07442289590835571
Batch 39/64 loss: -0.07323700189590454
Batch 40/64 loss: -0.072437584400177
Batch 41/64 loss: -0.05924350023269653
Batch 42/64 loss: -0.07714331150054932
Batch 43/64 loss: -0.07699984312057495
Batch 44/64 loss: -0.06723982095718384
Batch 45/64 loss: -0.08330750465393066
Batch 46/64 loss: -0.06485974788665771
Batch 47/64 loss: -0.07535797357559204
Batch 48/64 loss: -0.06463676691055298
Batch 49/64 loss: -0.07389158010482788
Batch 50/64 loss: -0.042673468589782715
Batch 51/64 loss: -0.07586944103240967
Batch 52/64 loss: -0.02525162696838379
Batch 53/64 loss: -0.07887691259384155
Batch 54/64 loss: -0.06798315048217773
Batch 55/64 loss: -0.07140344381332397
Batch 56/64 loss: -0.05268961191177368
Batch 57/64 loss: -0.06171095371246338
Batch 58/64 loss: -0.05889993906021118
Batch 59/64 loss: -0.0917198657989502
Batch 60/64 loss: -0.08206892013549805
Batch 61/64 loss: -0.07838594913482666
Batch 62/64 loss: -0.0767022967338562
Batch 63/64 loss: -0.06360578536987305
Batch 64/64 loss: -0.07975363731384277
Epoch 144  Train loss: -0.06908329327901204  Val loss: -0.031031731477717765
Epoch 145
-------------------------------
Batch 1/64 loss: -0.08039748668670654
Batch 2/64 loss: -0.07661682367324829
Batch 3/64 loss: -0.07675153017044067
Batch 4/64 loss: -0.08550822734832764
Batch 5/64 loss: -0.06308364868164062
Batch 6/64 loss: -0.07299435138702393
Batch 7/64 loss: -0.05180162191390991
Batch 8/64 loss: -0.07423031330108643
Batch 9/64 loss: -0.08312064409255981
Batch 10/64 loss: -0.05492091178894043
Batch 11/64 loss: -0.06082034111022949
Batch 12/64 loss: -0.05904954671859741
Batch 13/64 loss: -0.07982182502746582
Batch 14/64 loss: -0.08281242847442627
Batch 15/64 loss: -0.0732717514038086
Batch 16/64 loss: -0.09196925163269043
Batch 17/64 loss: -0.09087193012237549
Batch 18/64 loss: -0.05827689170837402
Batch 19/64 loss: -0.07381093502044678
Batch 20/64 loss: -0.06921231746673584
Batch 21/64 loss: -0.05591261386871338
Batch 22/64 loss: -0.07887399196624756
Batch 23/64 loss: -0.07545262575149536
Batch 24/64 loss: -0.07026362419128418
Batch 25/64 loss: -0.08250832557678223
Batch 26/64 loss: -0.0900343656539917
Batch 27/64 loss: -0.07938891649246216
Batch 28/64 loss: -0.05227094888687134
Batch 29/64 loss: -0.07317769527435303
Batch 30/64 loss: -0.07071304321289062
Batch 31/64 loss: -0.07387828826904297
Batch 32/64 loss: -0.07559263706207275
Batch 33/64 loss: -0.05665326118469238
Batch 34/64 loss: -0.06251013278961182
Batch 35/64 loss: -0.06743234395980835
Batch 36/64 loss: -0.07422739267349243
Batch 37/64 loss: -0.07790249586105347
Batch 38/64 loss: -0.04675012826919556
Batch 39/64 loss: -0.06618201732635498
Batch 40/64 loss: -0.05997413396835327
Batch 41/64 loss: -0.07118386030197144
Batch 42/64 loss: -0.0757567286491394
Batch 43/64 loss: -0.08577865362167358
Batch 44/64 loss: -0.08181428909301758
Batch 45/64 loss: -0.0376511812210083
Batch 46/64 loss: -0.07806354761123657
Batch 47/64 loss: -0.05636143684387207
Batch 48/64 loss: -0.06404620409011841
Batch 49/64 loss: -0.07556134462356567
Batch 50/64 loss: -0.07775253057479858
Batch 51/64 loss: -0.06620293855667114
Batch 52/64 loss: -0.08215987682342529
Batch 53/64 loss: -0.07795113325119019
Batch 54/64 loss: -0.07168537378311157
Batch 55/64 loss: -0.058683037757873535
Batch 56/64 loss: -0.0805235505104065
Batch 57/64 loss: -0.05206620693206787
Batch 58/64 loss: -0.07318592071533203
Batch 59/64 loss: -0.07347828149795532
Batch 60/64 loss: -0.07420450448989868
Batch 61/64 loss: -0.06485283374786377
Batch 62/64 loss: -0.06081724166870117
Batch 63/64 loss: -0.07245182991027832
Batch 64/64 loss: -0.04761958122253418
Epoch 145  Train loss: -0.0705410031711354  Val loss: -0.031850843495110054
Epoch 146
-------------------------------
Batch 1/64 loss: -0.07712370157241821
Batch 2/64 loss: -0.06976401805877686
Batch 3/64 loss: -0.08241146802902222
Batch 4/64 loss: -0.05526137351989746
Batch 5/64 loss: -0.07549142837524414
Batch 6/64 loss: -0.06176704168319702
Batch 7/64 loss: -0.08548378944396973
Batch 8/64 loss: -0.09706360101699829
Batch 9/64 loss: -0.08193236589431763
Batch 10/64 loss: -0.07784056663513184
Batch 11/64 loss: -0.07278984785079956
Batch 12/64 loss: -0.07330882549285889
Batch 13/64 loss: -0.07949268817901611
Batch 14/64 loss: -0.07263445854187012
Batch 15/64 loss: -0.07270252704620361
Batch 16/64 loss: -0.07807856798171997
Batch 17/64 loss: -0.07046705484390259
Batch 18/64 loss: -0.05425405502319336
Batch 19/64 loss: -0.07359963655471802
Batch 20/64 loss: -0.0852428674697876
Batch 21/64 loss: -0.07238668203353882
Batch 22/64 loss: -0.07055091857910156
Batch 23/64 loss: -0.08325964212417603
Batch 24/64 loss: -0.06326806545257568
Batch 25/64 loss: -0.07500922679901123
Batch 26/64 loss: -0.08869600296020508
Batch 27/64 loss: -0.07815933227539062
Batch 28/64 loss: -0.08871710300445557
Batch 29/64 loss: -0.08330011367797852
Batch 30/64 loss: -0.061863481998443604
Batch 31/64 loss: -0.08388972282409668
Batch 32/64 loss: -0.08782929182052612
Batch 33/64 loss: -0.07987779378890991
Batch 34/64 loss: -0.06540846824645996
Batch 35/64 loss: -0.08418005704879761
Batch 36/64 loss: -0.08076351881027222
Batch 37/64 loss: -0.08890193700790405
Batch 38/64 loss: -0.06601840257644653
Batch 39/64 loss: -0.0872848629951477
Batch 40/64 loss: -0.06645900011062622
Batch 41/64 loss: -0.06400400400161743
Batch 42/64 loss: -0.07653927803039551
Batch 43/64 loss: -0.07030612230300903
Batch 44/64 loss: -0.06667393445968628
Batch 45/64 loss: -0.06999754905700684
Batch 46/64 loss: -0.08420127630233765
Batch 47/64 loss: -0.06196945905685425
Batch 48/64 loss: -0.0859450101852417
Batch 49/64 loss: -0.06641244888305664
Batch 50/64 loss: -0.08558404445648193
Batch 51/64 loss: -0.074607253074646
Batch 52/64 loss: -0.06864726543426514
Batch 53/64 loss: -0.07758224010467529
Batch 54/64 loss: -0.07597947120666504
Batch 55/64 loss: -0.06483936309814453
Batch 56/64 loss: -0.09373795986175537
Batch 57/64 loss: -0.08233511447906494
Batch 58/64 loss: -0.05729818344116211
Batch 59/64 loss: -0.053802490234375
Batch 60/64 loss: -0.06966239213943481
Batch 61/64 loss: -0.06313157081604004
Batch 62/64 loss: -0.0648006796836853
Batch 63/64 loss: -0.06729137897491455
Batch 64/64 loss: -0.07431012392044067
Epoch 146  Train loss: -0.07456650336583455  Val loss: -0.027423195003234233
Epoch 147
-------------------------------
Batch 1/64 loss: -0.07826519012451172
Batch 2/64 loss: -0.08897250890731812
Batch 3/64 loss: -0.07031905651092529
Batch 4/64 loss: -0.0379793643951416
Batch 5/64 loss: -0.06706845760345459
Batch 6/64 loss: -0.0793766975402832
Batch 7/64 loss: -0.08117806911468506
Batch 8/64 loss: -0.07762736082077026
Batch 9/64 loss: -0.07791531085968018
Batch 10/64 loss: -0.07613086700439453
Batch 11/64 loss: -0.0691569447517395
Batch 12/64 loss: -0.08615219593048096
Batch 13/64 loss: -0.06688499450683594
Batch 14/64 loss: -0.07463550567626953
Batch 15/64 loss: -0.06719982624053955
Batch 16/64 loss: -0.08917421102523804
Batch 17/64 loss: -0.05446457862854004
Batch 18/64 loss: -0.09640109539031982
Batch 19/64 loss: -0.07573235034942627
Batch 20/64 loss: -0.06961959600448608
Batch 21/64 loss: -0.07295989990234375
Batch 22/64 loss: -0.08288419246673584
Batch 23/64 loss: -0.07954055070877075
Batch 24/64 loss: -0.05618858337402344
Batch 25/64 loss: -0.0724339485168457
Batch 26/64 loss: -0.08851891756057739
Batch 27/64 loss: -0.041579604148864746
Batch 28/64 loss: -0.06476962566375732
Batch 29/64 loss: -0.06391799449920654
Batch 30/64 loss: -0.07227468490600586
Batch 31/64 loss: -0.07960057258605957
Batch 32/64 loss: -0.07342100143432617
Batch 33/64 loss: -0.07283759117126465
Batch 34/64 loss: -0.056861281394958496
Batch 35/64 loss: -0.046674370765686035
Batch 36/64 loss: -0.08171826601028442
Batch 37/64 loss: -0.05991089344024658
Batch 38/64 loss: -0.06571680307388306
Batch 39/64 loss: -0.09035694599151611
Batch 40/64 loss: -0.07560944557189941
Batch 41/64 loss: -0.07072007656097412
Batch 42/64 loss: -0.07885360717773438
Batch 43/64 loss: -0.07343846559524536
Batch 44/64 loss: -0.07334470748901367
Batch 45/64 loss: -0.0832284688949585
Batch 46/64 loss: -0.06964236497879028
Batch 47/64 loss: -0.04623591899871826
Batch 48/64 loss: -0.08204829692840576
Batch 49/64 loss: -0.04358565807342529
Batch 50/64 loss: -0.06715750694274902
Batch 51/64 loss: -0.07021063566207886
Batch 52/64 loss: -0.07815927267074585
Batch 53/64 loss: -0.08451575040817261
Batch 54/64 loss: -0.060204386711120605
Batch 55/64 loss: -0.07031279802322388
Batch 56/64 loss: -0.04851663112640381
Batch 57/64 loss: -0.06233716011047363
Batch 58/64 loss: -0.05630761384963989
Batch 59/64 loss: -0.04286801815032959
Batch 60/64 loss: -0.08015519380569458
Batch 61/64 loss: -0.08769690990447998
Batch 62/64 loss: -0.07833307981491089
Batch 63/64 loss: -0.09332889318466187
Batch 64/64 loss: -0.0816265344619751
Epoch 147  Train loss: -0.07128550071342318  Val loss: -0.035513128611640014
Epoch 148
-------------------------------
Batch 1/64 loss: -0.08022117614746094
Batch 2/64 loss: -0.08374351263046265
Batch 3/64 loss: -0.07097691297531128
Batch 4/64 loss: -0.08248966932296753
Batch 5/64 loss: -0.06990712881088257
Batch 6/64 loss: -0.09082084894180298
Batch 7/64 loss: -0.06988632678985596
Batch 8/64 loss: -0.0778505802154541
Batch 9/64 loss: -0.0813036561012268
Batch 10/64 loss: -0.06300526857376099
Batch 11/64 loss: -0.0711408257484436
Batch 12/64 loss: -0.07182931900024414
Batch 13/64 loss: -0.06512612104415894
Batch 14/64 loss: -0.06740164756774902
Batch 15/64 loss: -0.08278560638427734
Batch 16/64 loss: -0.06528770923614502
Batch 17/64 loss: -0.06753015518188477
Batch 18/64 loss: -0.07790976762771606
Batch 19/64 loss: -0.09528225660324097
Batch 20/64 loss: -0.07575678825378418
Batch 21/64 loss: -0.08316034078598022
Batch 22/64 loss: -0.07494121789932251
Batch 23/64 loss: -0.04650616645812988
Batch 24/64 loss: -0.0708281397819519
Batch 25/64 loss: -0.07048481702804565
Batch 26/64 loss: -0.08225482702255249
Batch 27/64 loss: -0.07422834634780884
Batch 28/64 loss: -0.06378680467605591
Batch 29/64 loss: -0.06568145751953125
Batch 30/64 loss: -0.07727175951004028
Batch 31/64 loss: -0.07333159446716309
Batch 32/64 loss: -0.0693395733833313
Batch 33/64 loss: -0.06576085090637207
Batch 34/64 loss: -0.07011574506759644
Batch 35/64 loss: -0.06771910190582275
Batch 36/64 loss: -0.07997125387191772
Batch 37/64 loss: -0.08290702104568481
Batch 38/64 loss: -0.0767754316329956
Batch 39/64 loss: -0.0732622742652893
Batch 40/64 loss: -0.08159977197647095
Batch 41/64 loss: -0.0864640474319458
Batch 42/64 loss: -0.08606678247451782
Batch 43/64 loss: -0.07294273376464844
Batch 44/64 loss: -0.08058023452758789
Batch 45/64 loss: -0.06680077314376831
Batch 46/64 loss: -0.08808743953704834
Batch 47/64 loss: -0.07488381862640381
Batch 48/64 loss: -0.08462584018707275
Batch 49/64 loss: -0.08004927635192871
Batch 50/64 loss: -0.07546287775039673
Batch 51/64 loss: -0.08120357990264893
Batch 52/64 loss: -0.08780741691589355
Batch 53/64 loss: -0.057213008403778076
Batch 54/64 loss: -0.09238564968109131
Batch 55/64 loss: -0.053174614906311035
Batch 56/64 loss: -0.07130199670791626
Batch 57/64 loss: -0.06722640991210938
Batch 58/64 loss: -0.08066511154174805
Batch 59/64 loss: -0.07777214050292969
Batch 60/64 loss: -0.08005326986312866
Batch 61/64 loss: -0.08927029371261597
Batch 62/64 loss: -0.0875471830368042
Batch 63/64 loss: -0.07852005958557129
Batch 64/64 loss: -0.08372640609741211
Epoch 148  Train loss: -0.0756561746784285  Val loss: -0.0350997093616892
Epoch 149
-------------------------------
Batch 1/64 loss: -0.07132941484451294
Batch 2/64 loss: -0.07933950424194336
Batch 3/64 loss: -0.0817188024520874
Batch 4/64 loss: -0.07552742958068848
Batch 5/64 loss: -0.0784919261932373
Batch 6/64 loss: -0.07762289047241211
Batch 7/64 loss: -0.05545675754547119
Batch 8/64 loss: -0.06834256649017334
Batch 9/64 loss: -0.08655714988708496
Batch 10/64 loss: -0.07036960124969482
Batch 11/64 loss: -0.08007991313934326
Batch 12/64 loss: -0.0698620080947876
Batch 13/64 loss: -0.08075380325317383
Batch 14/64 loss: -0.08952391147613525
Batch 15/64 loss: -0.08360284566879272
Batch 16/64 loss: -0.07433372735977173
Batch 17/64 loss: -0.07242131233215332
Batch 18/64 loss: -0.08581161499023438
Batch 19/64 loss: -0.06386768817901611
Batch 20/64 loss: -0.05651402473449707
Batch 21/64 loss: -0.08522474765777588
Batch 22/64 loss: -0.09807455539703369
Batch 23/64 loss: -0.08340442180633545
Batch 24/64 loss: -0.0820305347442627
Batch 25/64 loss: -0.07669079303741455
Batch 26/64 loss: -0.07050710916519165
Batch 27/64 loss: -0.07209855318069458
Batch 28/64 loss: -0.049559593200683594
Batch 29/64 loss: -0.07357490062713623
Batch 30/64 loss: -0.07144701480865479
Batch 31/64 loss: -0.08257222175598145
Batch 32/64 loss: -0.06671321392059326
Batch 33/64 loss: -0.04898059368133545
Batch 34/64 loss: -0.0735430121421814
Batch 35/64 loss: -0.07629001140594482
Batch 36/64 loss: -0.08647763729095459
Batch 37/64 loss: -0.08139508962631226
Batch 38/64 loss: -0.07629364728927612
Batch 39/64 loss: -0.08175456523895264
Batch 40/64 loss: -0.07574689388275146
Batch 41/64 loss: -0.060927748680114746
Batch 42/64 loss: -0.07301342487335205
Batch 43/64 loss: -0.07450848817825317
Batch 44/64 loss: -0.07051968574523926
Batch 45/64 loss: -0.04897141456604004
Batch 46/64 loss: -0.07904934883117676
Batch 47/64 loss: -0.05796360969543457
Batch 48/64 loss: -0.07955944538116455
Batch 49/64 loss: -0.07000219821929932
Batch 50/64 loss: -0.06777292490005493
Batch 51/64 loss: -0.0802115797996521
Batch 52/64 loss: -0.08060365915298462
Batch 53/64 loss: -0.0849946141242981
Batch 54/64 loss: -0.07580399513244629
Batch 55/64 loss: -0.05559176206588745
Batch 56/64 loss: -0.06826895475387573
Batch 57/64 loss: -0.08065944910049438
Batch 58/64 loss: -0.061405718326568604
Batch 59/64 loss: -0.057677507400512695
Batch 60/64 loss: -0.06578671932220459
Batch 61/64 loss: -0.06898146867752075
Batch 62/64 loss: -0.0766403079032898
Batch 63/64 loss: -0.06616836786270142
Batch 64/64 loss: -0.07940053939819336
Epoch 149  Train loss: -0.07338884391036687  Val loss: -0.03400032626804208
Epoch 150
-------------------------------
Batch 1/64 loss: -0.08274465799331665
Batch 2/64 loss: -0.05015343427658081
Batch 3/64 loss: -0.0759592056274414
Batch 4/64 loss: -0.05242842435836792
Batch 5/64 loss: -0.06415897607803345
Batch 6/64 loss: -0.08359789848327637
Batch 7/64 loss: -0.0756688117980957
Batch 8/64 loss: -0.06260067224502563
Batch 9/64 loss: -0.06955689191818237
Batch 10/64 loss: -0.08307671546936035
Batch 11/64 loss: -0.07047301530838013
Batch 12/64 loss: -0.07876420021057129
Batch 13/64 loss: -0.07329356670379639
Batch 14/64 loss: -0.061165571212768555
Batch 15/64 loss: -0.07961380481719971
Batch 16/64 loss: -0.08602690696716309
Batch 17/64 loss: -0.06330305337905884
Batch 18/64 loss: -0.0730370283126831
Batch 19/64 loss: -0.0833393931388855
Batch 20/64 loss: -0.07335680723190308
Batch 21/64 loss: -0.08036530017852783
Batch 22/64 loss: -0.07380622625350952
Batch 23/64 loss: -0.07876169681549072
Batch 24/64 loss: -0.07621848583221436
Batch 25/64 loss: -0.06554806232452393
Batch 26/64 loss: -0.08479022979736328
Batch 27/64 loss: -0.048259079456329346
Batch 28/64 loss: -0.06860500574111938
Batch 29/64 loss: -0.05913710594177246
Batch 30/64 loss: -0.05184018611907959
Batch 31/64 loss: -0.08610451221466064
Batch 32/64 loss: -0.06686460971832275
Batch 33/64 loss: -0.06746387481689453
Batch 34/64 loss: -0.0649482011795044
Batch 35/64 loss: -0.07736748456954956
Batch 36/64 loss: -0.09896689653396606
Batch 37/64 loss: -0.07604879140853882
Batch 38/64 loss: -0.06922882795333862
Batch 39/64 loss: -0.08517098426818848
Batch 40/64 loss: -0.06207752227783203
Batch 41/64 loss: -0.07716977596282959
Batch 42/64 loss: -0.05733209848403931
Batch 43/64 loss: -0.07028108835220337
Batch 44/64 loss: -0.05306118726730347
Batch 45/64 loss: -0.06835907697677612
Batch 46/64 loss: -0.07281160354614258
Batch 47/64 loss: -0.03134584426879883
Batch 48/64 loss: -0.06931853294372559
Batch 49/64 loss: -0.07399755716323853
Batch 50/64 loss: -0.06752407550811768
Batch 51/64 loss: -0.08832740783691406
Batch 52/64 loss: -0.06567502021789551
Batch 53/64 loss: -0.08796095848083496
Batch 54/64 loss: -0.08677756786346436
Batch 55/64 loss: -0.05668991804122925
Batch 56/64 loss: -0.08288323879241943
Batch 57/64 loss: -0.0787779688835144
Batch 58/64 loss: -0.07269346714019775
Batch 59/64 loss: -0.07567530870437622
Batch 60/64 loss: -0.05991727113723755
Batch 61/64 loss: -0.0683438777923584
Batch 62/64 loss: -0.0728183388710022
Batch 63/64 loss: -0.059618473052978516
Batch 64/64 loss: -0.08188033103942871
Epoch 150  Train loss: -0.07125744352153704  Val loss: -0.033497274946101344
Epoch 151
-------------------------------
Batch 1/64 loss: -0.06935763359069824
Batch 2/64 loss: -0.06887376308441162
Batch 3/64 loss: -0.06969517469406128
Batch 4/64 loss: -0.07887202501296997
Batch 5/64 loss: -0.07189321517944336
Batch 6/64 loss: -0.0551605224609375
Batch 7/64 loss: -0.074795663356781
Batch 8/64 loss: -0.07434123754501343
Batch 9/64 loss: -0.08521407842636108
Batch 10/64 loss: -0.08671236038208008
Batch 11/64 loss: -0.07219803333282471
Batch 12/64 loss: -0.07995414733886719
Batch 13/64 loss: -0.07539820671081543
Batch 14/64 loss: -0.08347952365875244
Batch 15/64 loss: -0.08168894052505493
Batch 16/64 loss: -0.09377539157867432
Batch 17/64 loss: -0.08570772409439087
Batch 18/64 loss: -0.06567037105560303
Batch 19/64 loss: -0.08002448081970215
Batch 20/64 loss: -0.06751304864883423
Batch 21/64 loss: -0.06395161151885986
Batch 22/64 loss: -0.08336144685745239
Batch 23/64 loss: -0.062244951725006104
Batch 24/64 loss: -0.08226150274276733
Batch 25/64 loss: -0.07469213008880615
Batch 26/64 loss: -0.05478239059448242
Batch 27/64 loss: -0.06549906730651855
Batch 28/64 loss: -0.07305741310119629
Batch 29/64 loss: -0.06676477193832397
Batch 30/64 loss: -0.047326505184173584
Batch 31/64 loss: -0.06993186473846436
Batch 32/64 loss: -0.0578843355178833
Batch 33/64 loss: -0.0539165735244751
Batch 34/64 loss: -0.0810995101928711
Batch 35/64 loss: -0.04790371656417847
Batch 36/64 loss: -0.07860440015792847
Batch 37/64 loss: -0.0748753547668457
Batch 38/64 loss: -0.0832321047782898
Batch 39/64 loss: -0.08745920658111572
Batch 40/64 loss: -0.09052050113677979
Batch 41/64 loss: -0.07542890310287476
Batch 42/64 loss: -0.08528310060501099
Batch 43/64 loss: -0.08804720640182495
Batch 44/64 loss: -0.05427753925323486
Batch 45/64 loss: -0.06436514854431152
Batch 46/64 loss: -0.080197274684906
Batch 47/64 loss: -0.06255877017974854
Batch 48/64 loss: -0.08393657207489014
Batch 49/64 loss: -0.08673202991485596
Batch 50/64 loss: -0.06829208135604858
Batch 51/64 loss: -0.09384822845458984
Batch 52/64 loss: -0.07696533203125
Batch 53/64 loss: -0.07838475704193115
Batch 54/64 loss: -0.07739824056625366
Batch 55/64 loss: -0.07633322477340698
Batch 56/64 loss: -0.08673971891403198
Batch 57/64 loss: -0.08339309692382812
Batch 58/64 loss: -0.06325197219848633
Batch 59/64 loss: -0.07252627611160278
Batch 60/64 loss: -0.06523787975311279
Batch 61/64 loss: -0.07567739486694336
Batch 62/64 loss: -0.08714234828948975
Batch 63/64 loss: -0.06630557775497437
Batch 64/64 loss: -0.08645594120025635
Epoch 151  Train loss: -0.07430367890526267  Val loss: -0.035292913004295115
Epoch 152
-------------------------------
Batch 1/64 loss: -0.07956916093826294
Batch 2/64 loss: -0.08775907754898071
Batch 3/64 loss: -0.07480359077453613
Batch 4/64 loss: -0.07572758197784424
Batch 5/64 loss: -0.0631822943687439
Batch 6/64 loss: -0.06842911243438721
Batch 7/64 loss: -0.0671796202659607
Batch 8/64 loss: -0.08167940378189087
Batch 9/64 loss: -0.08324301242828369
Batch 10/64 loss: -0.08838033676147461
Batch 11/64 loss: -0.07917481660842896
Batch 12/64 loss: -0.07867288589477539
Batch 13/64 loss: -0.07764935493469238
Batch 14/64 loss: -0.06105273962020874
Batch 15/64 loss: -0.08468866348266602
Batch 16/64 loss: -0.08773064613342285
Batch 17/64 loss: -0.06802588701248169
Batch 18/64 loss: -0.08134359121322632
Batch 19/64 loss: -0.07663142681121826
Batch 20/64 loss: -0.07828027009963989
Batch 21/64 loss: -0.07784628868103027
Batch 22/64 loss: -0.060033321380615234
Batch 23/64 loss: -0.06926286220550537
Batch 24/64 loss: -0.07692986726760864
Batch 25/64 loss: -0.08813202381134033
Batch 26/64 loss: -0.07591968774795532
Batch 27/64 loss: -0.06512802839279175
Batch 28/64 loss: -0.07784169912338257
Batch 29/64 loss: -0.0762224793434143
Batch 30/64 loss: -0.06979405879974365
Batch 31/64 loss: -0.086031973361969
Batch 32/64 loss: -0.06355494260787964
Batch 33/64 loss: -0.07651174068450928
Batch 34/64 loss: -0.07365906238555908
Batch 35/64 loss: -0.07407236099243164
Batch 36/64 loss: -0.08258867263793945
Batch 37/64 loss: -0.07881337404251099
Batch 38/64 loss: -0.07342934608459473
Batch 39/64 loss: -0.07854503393173218
Batch 40/64 loss: -0.0750918984413147
Batch 41/64 loss: -0.06670928001403809
Batch 42/64 loss: -0.07247340679168701
Batch 43/64 loss: -0.0909351110458374
Batch 44/64 loss: -0.10042351484298706
Batch 45/64 loss: -0.07831060886383057
Batch 46/64 loss: -0.08183181285858154
Batch 47/64 loss: -0.06512856483459473
Batch 48/64 loss: -0.07185524702072144
Batch 49/64 loss: -0.07157254219055176
Batch 50/64 loss: -0.08300769329071045
Batch 51/64 loss: -0.06980103254318237
Batch 52/64 loss: -0.07001197338104248
Batch 53/64 loss: -0.0750458836555481
Batch 54/64 loss: -0.06957435607910156
Batch 55/64 loss: -0.06780886650085449
Batch 56/64 loss: -0.10457998514175415
Batch 57/64 loss: -0.09428775310516357
Batch 58/64 loss: -0.0744897723197937
Batch 59/64 loss: -0.08521050214767456
Batch 60/64 loss: -0.08112895488739014
Batch 61/64 loss: -0.06890976428985596
Batch 62/64 loss: -0.08601880073547363
Batch 63/64 loss: -0.09309619665145874
Batch 64/64 loss: -0.07289296388626099
Epoch 152  Train loss: -0.07716852613523895  Val loss: -0.03603382229395339
Epoch 153
-------------------------------
Batch 1/64 loss: -0.08464258909225464
Batch 2/64 loss: -0.07716226577758789
Batch 3/64 loss: -0.08892560005187988
Batch 4/64 loss: -0.07447826862335205
Batch 5/64 loss: -0.06707942485809326
Batch 6/64 loss: -0.08804976940155029
Batch 7/64 loss: -0.07525759935379028
Batch 8/64 loss: -0.06775802373886108
Batch 9/64 loss: -0.07814282178878784
Batch 10/64 loss: -0.07717382907867432
Batch 11/64 loss: -0.07075941562652588
Batch 12/64 loss: -0.08417665958404541
Batch 13/64 loss: -0.0768614411354065
Batch 14/64 loss: -0.07221943140029907
Batch 15/64 loss: -0.06331682205200195
Batch 16/64 loss: -0.055708348751068115
Batch 17/64 loss: -0.08449828624725342
Batch 18/64 loss: -0.07752656936645508
Batch 19/64 loss: -0.06727319955825806
Batch 20/64 loss: -0.07664918899536133
Batch 21/64 loss: -0.0760909914970398
Batch 22/64 loss: -0.07462620735168457
Batch 23/64 loss: -0.08148765563964844
Batch 24/64 loss: -0.07093048095703125
Batch 25/64 loss: -0.06903624534606934
Batch 26/64 loss: -0.06397497653961182
Batch 27/64 loss: -0.0768018364906311
Batch 28/64 loss: -0.07483750581741333
Batch 29/64 loss: -0.08054143190383911
Batch 30/64 loss: -0.09047973155975342
Batch 31/64 loss: -0.07706648111343384
Batch 32/64 loss: -0.07513165473937988
Batch 33/64 loss: -0.05566513538360596
Batch 34/64 loss: -0.05532991886138916
Batch 35/64 loss: -0.07846945524215698
Batch 36/64 loss: -0.09098124504089355
Batch 37/64 loss: -0.07750463485717773
Batch 38/64 loss: -0.08084774017333984
Batch 39/64 loss: -0.08175218105316162
Batch 40/64 loss: -0.08203506469726562
Batch 41/64 loss: -0.08553451299667358
Batch 42/64 loss: -0.06388252973556519
Batch 43/64 loss: -0.07410049438476562
Batch 44/64 loss: -0.08583849668502808
Batch 45/64 loss: -0.09596878290176392
Batch 46/64 loss: -0.07196915149688721
Batch 47/64 loss: -0.08316761255264282
Batch 48/64 loss: -0.07057464122772217
Batch 49/64 loss: -0.08705544471740723
Batch 50/64 loss: -0.05030953884124756
Batch 51/64 loss: -0.08740562200546265
Batch 52/64 loss: -0.05954849720001221
Batch 53/64 loss: -0.08759361505508423
Batch 54/64 loss: -0.07674652338027954
Batch 55/64 loss: -0.07478213310241699
Batch 56/64 loss: -0.0636870265007019
Batch 57/64 loss: -0.08766889572143555
Batch 58/64 loss: -0.07670646905899048
Batch 59/64 loss: -0.08202648162841797
Batch 60/64 loss: -0.09122079610824585
Batch 61/64 loss: -0.07957756519317627
Batch 62/64 loss: -0.07344251871109009
Batch 63/64 loss: -0.07731735706329346
Batch 64/64 loss: -0.0805208683013916
Epoch 153  Train loss: -0.07635710566651588  Val loss: -0.03564308127996438
Epoch 154
-------------------------------
Batch 1/64 loss: -0.099398672580719
Batch 2/64 loss: -0.07501792907714844
Batch 3/64 loss: -0.10786569118499756
Batch 4/64 loss: -0.10219687223434448
Batch 5/64 loss: -0.08501017093658447
Batch 6/64 loss: -0.083854079246521
Batch 7/64 loss: -0.08361715078353882
Batch 8/64 loss: -0.07058274745941162
Batch 9/64 loss: -0.08126217126846313
Batch 10/64 loss: -0.05971789360046387
Batch 11/64 loss: -0.07373762130737305
Batch 12/64 loss: -0.0756683349609375
Batch 13/64 loss: -0.07465529441833496
Batch 14/64 loss: -0.07402670383453369
Batch 15/64 loss: -0.0839543342590332
Batch 16/64 loss: -0.08871853351593018
Batch 17/64 loss: -0.0754205584526062
Batch 18/64 loss: -0.07232058048248291
Batch 19/64 loss: -0.0714259147644043
Batch 20/64 loss: -0.08325129747390747
Batch 21/64 loss: -0.08752709627151489
Batch 22/64 loss: -0.07361483573913574
Batch 23/64 loss: -0.07704341411590576
Batch 24/64 loss: -0.075950026512146
Batch 25/64 loss: -0.0775194764137268
Batch 26/64 loss: -0.08200913667678833
Batch 27/64 loss: -0.07539397478103638
Batch 28/64 loss: -0.07503402233123779
Batch 29/64 loss: -0.0650099515914917
Batch 30/64 loss: -0.08722090721130371
Batch 31/64 loss: -0.08359885215759277
Batch 32/64 loss: -0.07635056972503662
Batch 33/64 loss: -0.06195646524429321
Batch 34/64 loss: -0.0737568736076355
Batch 35/64 loss: -0.07892709970474243
Batch 36/64 loss: -0.10250359773635864
Batch 37/64 loss: -0.08681118488311768
Batch 38/64 loss: -0.07088321447372437
Batch 39/64 loss: -0.076809823513031
Batch 40/64 loss: -0.08278357982635498
Batch 41/64 loss: -0.08334046602249146
Batch 42/64 loss: -0.08114731311798096
Batch 43/64 loss: -0.07034075260162354
Batch 44/64 loss: -0.0642094612121582
Batch 45/64 loss: -0.07785356044769287
Batch 46/64 loss: -0.07484197616577148
Batch 47/64 loss: -0.07876569032669067
Batch 48/64 loss: -0.05502879619598389
Batch 49/64 loss: -0.0764690637588501
Batch 50/64 loss: -0.06988191604614258
Batch 51/64 loss: -0.08176052570343018
Batch 52/64 loss: -0.07329577207565308
Batch 53/64 loss: -0.0682687759399414
Batch 54/64 loss: -0.07105147838592529
Batch 55/64 loss: -0.08927369117736816
Batch 56/64 loss: -0.09473598003387451
Batch 57/64 loss: -0.0928836464881897
Batch 58/64 loss: -0.0820091962814331
Batch 59/64 loss: -0.07198405265808105
Batch 60/64 loss: -0.08202493190765381
Batch 61/64 loss: -0.08793509006500244
Batch 62/64 loss: -0.07779723405838013
Batch 63/64 loss: -0.07089567184448242
Batch 64/64 loss: -0.05154842138290405
Epoch 154  Train loss: -0.07853902765348846  Val loss: -0.033744120515908575
Epoch 155
-------------------------------
Batch 1/64 loss: -0.07080382108688354
Batch 2/64 loss: -0.07936465740203857
Batch 3/64 loss: -0.06759905815124512
Batch 4/64 loss: -0.09076738357543945
Batch 5/64 loss: -0.07061922550201416
Batch 6/64 loss: -0.06811589002609253
Batch 7/64 loss: -0.09189629554748535
Batch 8/64 loss: -0.0926363468170166
Batch 9/64 loss: -0.0716518759727478
Batch 10/64 loss: -0.09559476375579834
Batch 11/64 loss: -0.08214610815048218
Batch 12/64 loss: -0.09856069087982178
Batch 13/64 loss: -0.10054832696914673
Batch 14/64 loss: -0.08698046207427979
Batch 15/64 loss: -0.09028273820877075
Batch 16/64 loss: -0.07420724630355835
Batch 17/64 loss: -0.06259399652481079
Batch 18/64 loss: -0.08851873874664307
Batch 19/64 loss: -0.08476245403289795
Batch 20/64 loss: -0.0788884162902832
Batch 21/64 loss: -0.06905055046081543
Batch 22/64 loss: -0.07920628786087036
Batch 23/64 loss: -0.09009683132171631
Batch 24/64 loss: -0.08438503742218018
Batch 25/64 loss: -0.08407831192016602
Batch 26/64 loss: -0.09342455863952637
Batch 27/64 loss: -0.09668409824371338
Batch 28/64 loss: -0.08667021989822388
Batch 29/64 loss: -0.08026754856109619
Batch 30/64 loss: -0.06431818008422852
Batch 31/64 loss: -0.07847702503204346
Batch 32/64 loss: -0.07253974676132202
Batch 33/64 loss: -0.0883866548538208
Batch 34/64 loss: -0.08058810234069824
Batch 35/64 loss: -0.06648045778274536
Batch 36/64 loss: -0.06268763542175293
Batch 37/64 loss: -0.08511000871658325
Batch 38/64 loss: -0.06293565034866333
Batch 39/64 loss: -0.08102113008499146
Batch 40/64 loss: -0.05678856372833252
Batch 41/64 loss: -0.060432612895965576
Batch 42/64 loss: -0.0828317403793335
Batch 43/64 loss: -0.09201681613922119
Batch 44/64 loss: -0.0955662727355957
Batch 45/64 loss: -0.08352398872375488
Batch 46/64 loss: -0.10087662935256958
Batch 47/64 loss: -0.08637779951095581
Batch 48/64 loss: -0.08331674337387085
Batch 49/64 loss: -0.08387798070907593
Batch 50/64 loss: -0.08611869812011719
Batch 51/64 loss: -0.07462799549102783
Batch 52/64 loss: -0.07453751564025879
Batch 53/64 loss: -0.05675017833709717
Batch 54/64 loss: -0.05738192796707153
Batch 55/64 loss: -0.07713741064071655
Batch 56/64 loss: -0.06974124908447266
Batch 57/64 loss: -0.07005047798156738
Batch 58/64 loss: -0.08731508255004883
Batch 59/64 loss: -0.08575242757797241
Batch 60/64 loss: -0.0718645453453064
Batch 61/64 loss: -0.07584303617477417
Batch 62/64 loss: -0.05903446674346924
Batch 63/64 loss: -0.08837568759918213
Batch 64/64 loss: -0.08195579051971436
Epoch 155  Train loss: -0.07960083484649658  Val loss: -0.0382366901410814
Saving best model, epoch: 155
Epoch 156
-------------------------------
Batch 1/64 loss: -0.06788015365600586
Batch 2/64 loss: -0.0781714916229248
Batch 3/64 loss: -0.08958274126052856
Batch 4/64 loss: -0.07887184619903564
Batch 5/64 loss: -0.09761452674865723
Batch 6/64 loss: -0.0828399658203125
Batch 7/64 loss: -0.08675700426101685
Batch 8/64 loss: -0.08691918849945068
Batch 9/64 loss: -0.0804792046546936
Batch 10/64 loss: -0.07988929748535156
Batch 11/64 loss: -0.07640677690505981
Batch 12/64 loss: -0.08777326345443726
Batch 13/64 loss: -0.08475208282470703
Batch 14/64 loss: -0.0925440788269043
Batch 15/64 loss: -0.10636454820632935
Batch 16/64 loss: -0.07276976108551025
Batch 17/64 loss: -0.0722198486328125
Batch 18/64 loss: -0.06986355781555176
Batch 19/64 loss: -0.07668209075927734
Batch 20/64 loss: -0.0781753659248352
Batch 21/64 loss: -0.07336682081222534
Batch 22/64 loss: -0.08442002534866333
Batch 23/64 loss: -0.07671916484832764
Batch 24/64 loss: -0.07947313785552979
Batch 25/64 loss: -0.08745503425598145
Batch 26/64 loss: -0.05469566583633423
Batch 27/64 loss: -0.08036881685256958
Batch 28/64 loss: -0.0809168815612793
Batch 29/64 loss: -0.08209604024887085
Batch 30/64 loss: -0.07733172178268433
Batch 31/64 loss: -0.06470745801925659
Batch 32/64 loss: -0.06961548328399658
Batch 33/64 loss: -0.07736867666244507
Batch 34/64 loss: -0.07949411869049072
Batch 35/64 loss: -0.08146786689758301
Batch 36/64 loss: -0.0923992395401001
Batch 37/64 loss: -0.07637298107147217
Batch 38/64 loss: -0.07604247331619263
Batch 39/64 loss: -0.09890991449356079
Batch 40/64 loss: -0.05723154544830322
Batch 41/64 loss: -0.10033392906188965
Batch 42/64 loss: -0.05424082279205322
Batch 43/64 loss: -0.07861953973770142
Batch 44/64 loss: -0.08757257461547852
Batch 45/64 loss: -0.07440537214279175
Batch 46/64 loss: -0.09015518426895142
Batch 47/64 loss: -0.08311951160430908
Batch 48/64 loss: -0.06790190935134888
Batch 49/64 loss: -0.07792067527770996
Batch 50/64 loss: -0.05925154685974121
Batch 51/64 loss: -0.06918936967849731
Batch 52/64 loss: -0.08725124597549438
Batch 53/64 loss: -0.07623201608657837
Batch 54/64 loss: -0.08179450035095215
Batch 55/64 loss: -0.08563435077667236
Batch 56/64 loss: -0.07538759708404541
Batch 57/64 loss: -0.07781165838241577
Batch 58/64 loss: -0.08727484941482544
Batch 59/64 loss: -0.07960182428359985
Batch 60/64 loss: -0.07922768592834473
Batch 61/64 loss: -0.05494976043701172
Batch 62/64 loss: -0.07244503498077393
Batch 63/64 loss: -0.07771766185760498
Batch 64/64 loss: -0.08806252479553223
Epoch 156  Train loss: -0.07907600589826995  Val loss: -0.0296697784535254
Epoch 157
-------------------------------
Batch 1/64 loss: -0.08734077215194702
Batch 2/64 loss: -0.08456087112426758
Batch 3/64 loss: -0.06981581449508667
Batch 4/64 loss: -0.06166577339172363
Batch 5/64 loss: -0.09076166152954102
Batch 6/64 loss: -0.08836096525192261
Batch 7/64 loss: -0.08742284774780273
Batch 8/64 loss: -0.08245229721069336
Batch 9/64 loss: -0.06042224168777466
Batch 10/64 loss: -0.08860510587692261
Batch 11/64 loss: -0.05841028690338135
Batch 12/64 loss: -0.0873708724975586
Batch 13/64 loss: -0.0790022611618042
Batch 14/64 loss: -0.09193545579910278
Batch 15/64 loss: -0.07319080829620361
Batch 16/64 loss: -0.08058208227157593
Batch 17/64 loss: -0.07932287454605103
Batch 18/64 loss: -0.10019171237945557
Batch 19/64 loss: -0.08903700113296509
Batch 20/64 loss: -0.10048544406890869
Batch 21/64 loss: -0.08449232578277588
Batch 22/64 loss: -0.06773269176483154
Batch 23/64 loss: -0.07726085186004639
Batch 24/64 loss: -0.07453733682632446
Batch 25/64 loss: -0.08272755146026611
Batch 26/64 loss: -0.08796626329421997
Batch 27/64 loss: -0.09064972400665283
Batch 28/64 loss: -0.07306712865829468
Batch 29/64 loss: -0.07230585813522339
Batch 30/64 loss: -0.09071201086044312
Batch 31/64 loss: -0.07035160064697266
Batch 32/64 loss: -0.07961249351501465
Batch 33/64 loss: -0.09355103969573975
Batch 34/64 loss: -0.09438401460647583
Batch 35/64 loss: -0.08894705772399902
Batch 36/64 loss: -0.07578533887863159
Batch 37/64 loss: -0.08705544471740723
Batch 38/64 loss: -0.08234238624572754
Batch 39/64 loss: -0.09589457511901855
Batch 40/64 loss: -0.060406267642974854
Batch 41/64 loss: -0.06709742546081543
Batch 42/64 loss: -0.08576589822769165
Batch 43/64 loss: -0.07473665475845337
Batch 44/64 loss: -0.05821353197097778
Batch 45/64 loss: -0.07272905111312866
Batch 46/64 loss: -0.07377946376800537
Batch 47/64 loss: -0.07194620370864868
Batch 48/64 loss: -0.07391202449798584
Batch 49/64 loss: -0.08678621053695679
Batch 50/64 loss: -0.07849162817001343
Batch 51/64 loss: -0.086331307888031
Batch 52/64 loss: -0.07064449787139893
Batch 53/64 loss: -0.09615552425384521
Batch 54/64 loss: -0.08080613613128662
Batch 55/64 loss: -0.0992506742477417
Batch 56/64 loss: -0.08753699064254761
Batch 57/64 loss: -0.09179902076721191
Batch 58/64 loss: -0.08070820569992065
Batch 59/64 loss: -0.07072794437408447
Batch 60/64 loss: -0.05953794717788696
Batch 61/64 loss: -0.07806992530822754
Batch 62/64 loss: -0.08008021116256714
Batch 63/64 loss: -0.06295531988143921
Batch 64/64 loss: -0.07525360584259033
Epoch 157  Train loss: -0.08023876255633784  Val loss: -0.03540425878210166
Epoch 158
-------------------------------
Batch 1/64 loss: -0.09706544876098633
Batch 2/64 loss: -0.08228421211242676
Batch 3/64 loss: -0.08669447898864746
Batch 4/64 loss: -0.0735311508178711
Batch 5/64 loss: -0.09580063819885254
Batch 6/64 loss: -0.08456110954284668
Batch 7/64 loss: -0.08898574113845825
Batch 8/64 loss: -0.06945765018463135
Batch 9/64 loss: -0.0693202018737793
Batch 10/64 loss: -0.08652663230895996
Batch 11/64 loss: -0.08837831020355225
Batch 12/64 loss: -0.0785025954246521
Batch 13/64 loss: -0.08843028545379639
Batch 14/64 loss: -0.09399914741516113
Batch 15/64 loss: -0.06722331047058105
Batch 16/64 loss: -0.08796459436416626
Batch 17/64 loss: -0.0834847092628479
Batch 18/64 loss: -0.07273650169372559
Batch 19/64 loss: -0.07317578792572021
Batch 20/64 loss: -0.09082627296447754
Batch 21/64 loss: -0.06423664093017578
Batch 22/64 loss: -0.09256547689437866
Batch 23/64 loss: -0.07859021425247192
Batch 24/64 loss: -0.05344271659851074
Batch 25/64 loss: -0.06254053115844727
Batch 26/64 loss: -0.09442317485809326
Batch 27/64 loss: -0.06477254629135132
Batch 28/64 loss: -0.06869089603424072
Batch 29/64 loss: -0.08706998825073242
Batch 30/64 loss: -0.08652466535568237
Batch 31/64 loss: -0.08850741386413574
Batch 32/64 loss: -0.08688443899154663
Batch 33/64 loss: -0.085696280002594
Batch 34/64 loss: -0.08782607316970825
Batch 35/64 loss: -0.06660628318786621
Batch 36/64 loss: -0.0780748724937439
Batch 37/64 loss: -0.06897139549255371
Batch 38/64 loss: -0.07091450691223145
Batch 39/64 loss: -0.08177709579467773
Batch 40/64 loss: -0.08494162559509277
Batch 41/64 loss: -0.07851821184158325
Batch 42/64 loss: -0.07479631900787354
Batch 43/64 loss: -0.08057612180709839
Batch 44/64 loss: -0.07153439521789551
Batch 45/64 loss: -0.06478631496429443
Batch 46/64 loss: -0.08222818374633789
Batch 47/64 loss: -0.0826076865196228
Batch 48/64 loss: -0.08912408351898193
Batch 49/64 loss: -0.08808135986328125
Batch 50/64 loss: -0.05913984775543213
Batch 51/64 loss: -0.0787118673324585
Batch 52/64 loss: -0.08743125200271606
Batch 53/64 loss: -0.06719708442687988
Batch 54/64 loss: -0.10016477108001709
Batch 55/64 loss: -0.08201384544372559
Batch 56/64 loss: -0.07875251770019531
Batch 57/64 loss: -0.08592844009399414
Batch 58/64 loss: -0.07231748104095459
Batch 59/64 loss: -0.07486087083816528
Batch 60/64 loss: -0.0813988447189331
Batch 61/64 loss: -0.07233542203903198
Batch 62/64 loss: -0.0870671272277832
Batch 63/64 loss: -0.08824384212493896
Batch 64/64 loss: -0.06611979007720947
Epoch 158  Train loss: -0.07983390349967807  Val loss: -0.03365353681787183
Epoch 159
-------------------------------
Batch 1/64 loss: -0.07517212629318237
Batch 2/64 loss: -0.062049031257629395
Batch 3/64 loss: -0.09459942579269409
Batch 4/64 loss: -0.093089759349823
Batch 5/64 loss: -0.09897619485855103
Batch 6/64 loss: -0.08685803413391113
Batch 7/64 loss: -0.09170687198638916
Batch 8/64 loss: -0.07712894678115845
Batch 9/64 loss: -0.0956607460975647
Batch 10/64 loss: -0.07325732707977295
Batch 11/64 loss: -0.08683979511260986
Batch 12/64 loss: -0.08931589126586914
Batch 13/64 loss: -0.09414631128311157
Batch 14/64 loss: -0.07904326915740967
Batch 15/64 loss: -0.0896424651145935
Batch 16/64 loss: -0.09188014268875122
Batch 17/64 loss: -0.08169329166412354
Batch 18/64 loss: -0.0731579065322876
Batch 19/64 loss: -0.08868527412414551
Batch 20/64 loss: -0.07142108678817749
Batch 21/64 loss: -0.08287715911865234
Batch 22/64 loss: -0.08954757452011108
Batch 23/64 loss: -0.09404808282852173
Batch 24/64 loss: -0.05118840932846069
Batch 25/64 loss: -0.06341946125030518
Batch 26/64 loss: -0.06726586818695068
Batch 27/64 loss: -0.061107516288757324
Batch 28/64 loss: -0.08794337511062622
Batch 29/64 loss: -0.06502318382263184
Batch 30/64 loss: -0.08313345909118652
Batch 31/64 loss: -0.09017074108123779
Batch 32/64 loss: -0.07722818851470947
Batch 33/64 loss: -0.08438575267791748
Batch 34/64 loss: -0.08205604553222656
Batch 35/64 loss: -0.09090679883956909
Batch 36/64 loss: -0.05624425411224365
Batch 37/64 loss: -0.08104419708251953
Batch 38/64 loss: -0.04480928182601929
Batch 39/64 loss: -0.07900190353393555
Batch 40/64 loss: -0.08009493350982666
Batch 41/64 loss: -0.08154910802841187
Batch 42/64 loss: -0.06374633312225342
Batch 43/64 loss: -0.08355057239532471
Batch 44/64 loss: -0.07100027799606323
Batch 45/64 loss: -0.07799077033996582
Batch 46/64 loss: -0.08684402704238892
Batch 47/64 loss: -0.08912062644958496
Batch 48/64 loss: -0.09027594327926636
Batch 49/64 loss: -0.07620459794998169
Batch 50/64 loss: -0.09909462928771973
Batch 51/64 loss: -0.10604226589202881
Batch 52/64 loss: -0.08188354969024658
Batch 53/64 loss: -0.06831425428390503
Batch 54/64 loss: -0.044966161251068115
Batch 55/64 loss: -0.07758378982543945
Batch 56/64 loss: -0.07914131879806519
Batch 57/64 loss: -0.09068983793258667
Batch 58/64 loss: -0.06664824485778809
Batch 59/64 loss: -0.06763458251953125
Batch 60/64 loss: -0.09075736999511719
Batch 61/64 loss: -0.07096207141876221
Batch 62/64 loss: -0.08811843395233154
Batch 63/64 loss: -0.06761699914932251
Batch 64/64 loss: -0.08342528343200684
Epoch 159  Train loss: -0.07981372253567565  Val loss: -0.0285523918895787
Epoch 160
-------------------------------
Batch 1/64 loss: -0.08240830898284912
Batch 2/64 loss: -0.09201443195343018
Batch 3/64 loss: -0.07161712646484375
Batch 4/64 loss: -0.083579421043396
Batch 5/64 loss: -0.08713984489440918
Batch 6/64 loss: -0.07745224237442017
Batch 7/64 loss: -0.0764007568359375
Batch 8/64 loss: -0.09187638759613037
Batch 9/64 loss: -0.07682549953460693
Batch 10/64 loss: -0.06814610958099365
Batch 11/64 loss: -0.04705578088760376
Batch 12/64 loss: -0.07872593402862549
Batch 13/64 loss: -0.08420062065124512
Batch 14/64 loss: -0.09203857183456421
Batch 15/64 loss: -0.09556269645690918
Batch 16/64 loss: -0.06717079877853394
Batch 17/64 loss: -0.08081197738647461
Batch 18/64 loss: -0.09460461139678955
Batch 19/64 loss: -0.08126980066299438
Batch 20/64 loss: -0.08172005414962769
Batch 21/64 loss: -0.06876373291015625
Batch 22/64 loss: -0.0754021406173706
Batch 23/64 loss: -0.08588564395904541
Batch 24/64 loss: -0.05128765106201172
Batch 25/64 loss: -0.07859092950820923
Batch 26/64 loss: -0.07148396968841553
Batch 27/64 loss: -0.08593475818634033
Batch 28/64 loss: -0.09515905380249023
Batch 29/64 loss: -0.08674615621566772
Batch 30/64 loss: -0.09317553043365479
Batch 31/64 loss: -0.06628155708312988
Batch 32/64 loss: -0.06563597917556763
Batch 33/64 loss: -0.07357627153396606
Batch 34/64 loss: -0.09185785055160522
Batch 35/64 loss: -0.06120175123214722
Batch 36/64 loss: -0.07252436876296997
Batch 37/64 loss: -0.0842059850692749
Batch 38/64 loss: -0.08325326442718506
Batch 39/64 loss: -0.08129340410232544
Batch 40/64 loss: -0.09322148561477661
Batch 41/64 loss: -0.08756256103515625
Batch 42/64 loss: -0.054668962955474854
Batch 43/64 loss: -0.078826904296875
Batch 44/64 loss: -0.07980412244796753
Batch 45/64 loss: -0.07347053289413452
Batch 46/64 loss: -0.07208847999572754
Batch 47/64 loss: -0.08435428142547607
Batch 48/64 loss: -0.07915836572647095
Batch 49/64 loss: -0.07450157403945923
Batch 50/64 loss: -0.08631718158721924
Batch 51/64 loss: -0.08797639608383179
Batch 52/64 loss: -0.08662748336791992
Batch 53/64 loss: -0.07567065954208374
Batch 54/64 loss: -0.07873785495758057
Batch 55/64 loss: -0.08501338958740234
Batch 56/64 loss: -0.08506208658218384
Batch 57/64 loss: -0.09054040908813477
Batch 58/64 loss: -0.0911099910736084
Batch 59/64 loss: -0.10684698820114136
Batch 60/64 loss: -0.05686670541763306
Batch 61/64 loss: -0.09766381978988647
Batch 62/64 loss: -0.09346753358840942
Batch 63/64 loss: -0.08278131484985352
Batch 64/64 loss: -0.07941597700119019
Epoch 160  Train loss: -0.08038873789357204  Val loss: -0.03178360310616772
Epoch 161
-------------------------------
Batch 1/64 loss: -0.07355058193206787
Batch 2/64 loss: -0.09465765953063965
Batch 3/64 loss: -0.06969422101974487
Batch 4/64 loss: -0.06367599964141846
Batch 5/64 loss: -0.09378564357757568
Batch 6/64 loss: -0.08647382259368896
Batch 7/64 loss: -0.0806279182434082
Batch 8/64 loss: -0.08748513460159302
Batch 9/64 loss: -0.0992196798324585
Batch 10/64 loss: -0.09504806995391846
Batch 11/64 loss: -0.08613723516464233
Batch 12/64 loss: -0.08463609218597412
Batch 13/64 loss: -0.08546954393386841
Batch 14/64 loss: -0.0821046233177185
Batch 15/64 loss: -0.09494662284851074
Batch 16/64 loss: -0.08433234691619873
Batch 17/64 loss: -0.08688360452651978
Batch 18/64 loss: -0.08865630626678467
Batch 19/64 loss: -0.06806498765945435
Batch 20/64 loss: -0.07706254720687866
Batch 21/64 loss: -0.0941627025604248
Batch 22/64 loss: -0.09183645248413086
Batch 23/64 loss: -0.07517993450164795
Batch 24/64 loss: -0.0969533920288086
Batch 25/64 loss: -0.0695996880531311
Batch 26/64 loss: -0.08871716260910034
Batch 27/64 loss: -0.09567832946777344
Batch 28/64 loss: -0.08227050304412842
Batch 29/64 loss: -0.0916796326637268
Batch 30/64 loss: -0.09266656637191772
Batch 31/64 loss: -0.07757949829101562
Batch 32/64 loss: -0.08581608533859253
Batch 33/64 loss: -0.09312528371810913
Batch 34/64 loss: -0.08845371007919312
Batch 35/64 loss: -0.09319716691970825
Batch 36/64 loss: -0.08237826824188232
Batch 37/64 loss: -0.07748031616210938
Batch 38/64 loss: -0.09092360734939575
Batch 39/64 loss: -0.05262458324432373
Batch 40/64 loss: -0.09273719787597656
Batch 41/64 loss: -0.06441450119018555
Batch 42/64 loss: -0.06597381830215454
Batch 43/64 loss: -0.0799018144607544
Batch 44/64 loss: -0.08102715015411377
Batch 45/64 loss: -0.09046739339828491
Batch 46/64 loss: -0.07571274042129517
Batch 47/64 loss: -0.07846343517303467
Batch 48/64 loss: -0.07447278499603271
Batch 49/64 loss: -0.0702630877494812
Batch 50/64 loss: -0.07780194282531738
Batch 51/64 loss: -0.09551435708999634
Batch 52/64 loss: -0.08995604515075684
Batch 53/64 loss: -0.07630807161331177
Batch 54/64 loss: -0.07147216796875
Batch 55/64 loss: -0.07780706882476807
Batch 56/64 loss: -0.0917055606842041
Batch 57/64 loss: -0.07160526514053345
Batch 58/64 loss: -0.07655799388885498
Batch 59/64 loss: -0.07461857795715332
Batch 60/64 loss: -0.08060026168823242
Batch 61/64 loss: -0.08218997716903687
Batch 62/64 loss: -0.06807184219360352
Batch 63/64 loss: -0.07976293563842773
Batch 64/64 loss: -0.07383817434310913
Epoch 161  Train loss: -0.0822842375904906  Val loss: -0.03139876377131931
Epoch 162
-------------------------------
Batch 1/64 loss: -0.09719669818878174
Batch 2/64 loss: -0.0912579894065857
Batch 3/64 loss: -0.08079791069030762
Batch 4/64 loss: -0.09268784523010254
Batch 5/64 loss: -0.07478964328765869
Batch 6/64 loss: -0.06767719984054565
Batch 7/64 loss: -0.052651405334472656
Batch 8/64 loss: -0.08638191223144531
Batch 9/64 loss: -0.07408791780471802
Batch 10/64 loss: -0.07429122924804688
Batch 11/64 loss: -0.08135819435119629
Batch 12/64 loss: -0.08162462711334229
Batch 13/64 loss: -0.0902707576751709
Batch 14/64 loss: -0.056835293769836426
Batch 15/64 loss: -0.07515078783035278
Batch 16/64 loss: -0.08283078670501709
Batch 17/64 loss: -0.06992685794830322
Batch 18/64 loss: -0.10156452655792236
Batch 19/64 loss: -0.07836747169494629
Batch 20/64 loss: -0.08969569206237793
Batch 21/64 loss: -0.07028377056121826
Batch 22/64 loss: -0.052056193351745605
Batch 23/64 loss: -0.08461803197860718
Batch 24/64 loss: -0.08593279123306274
Batch 25/64 loss: -0.08207279443740845
Batch 26/64 loss: -0.07627254724502563
Batch 27/64 loss: -0.07109439373016357
Batch 28/64 loss: -0.08768826723098755
Batch 29/64 loss: -0.07963335514068604
Batch 30/64 loss: -0.07127934694290161
Batch 31/64 loss: -0.09510993957519531
Batch 32/64 loss: -0.07852929830551147
Batch 33/64 loss: -0.08889138698577881
Batch 34/64 loss: -0.0846872329711914
Batch 35/64 loss: -0.08317822217941284
Batch 36/64 loss: -0.09031158685684204
Batch 37/64 loss: -0.08315664529800415
Batch 38/64 loss: -0.07350915670394897
Batch 39/64 loss: -0.08243125677108765
Batch 40/64 loss: -0.07766854763031006
Batch 41/64 loss: -0.08136844635009766
Batch 42/64 loss: -0.06871747970581055
Batch 43/64 loss: -0.07890307903289795
Batch 44/64 loss: -0.07880961894989014
Batch 45/64 loss: -0.0851706862449646
Batch 46/64 loss: -0.10275810956954956
Batch 47/64 loss: -0.08877092599868774
Batch 48/64 loss: -0.09380155801773071
Batch 49/64 loss: -0.07069277763366699
Batch 50/64 loss: -0.076496422290802
Batch 51/64 loss: -0.08019852638244629
Batch 52/64 loss: -0.09056812524795532
Batch 53/64 loss: -0.0757972002029419
Batch 54/64 loss: -0.057755112648010254
Batch 55/64 loss: -0.08282613754272461
Batch 56/64 loss: -0.07368773221969604
Batch 57/64 loss: -0.04418504238128662
Batch 58/64 loss: -0.06162083148956299
Batch 59/64 loss: -0.08709114789962769
Batch 60/64 loss: -0.07423245906829834
Batch 61/64 loss: -0.07537335157394409
Batch 62/64 loss: -0.06625765562057495
Batch 63/64 loss: -0.09467577934265137
Batch 64/64 loss: -0.08408856391906738
Epoch 162  Train loss: -0.07922626102671904  Val loss: -0.03804873252652355
Epoch 163
-------------------------------
Batch 1/64 loss: -0.0978429913520813
Batch 2/64 loss: -0.08643287420272827
Batch 3/64 loss: -0.09473824501037598
Batch 4/64 loss: -0.10229289531707764
Batch 5/64 loss: -0.0893135666847229
Batch 6/64 loss: -0.09218001365661621
Batch 7/64 loss: -0.06669622659683228
Batch 8/64 loss: -0.09201228618621826
Batch 9/64 loss: -0.0945691466331482
Batch 10/64 loss: -0.08168965578079224
Batch 11/64 loss: -0.07623541355133057
Batch 12/64 loss: -0.09238475561141968
Batch 13/64 loss: -0.10049515962600708
Batch 14/64 loss: -0.08241790533065796
Batch 15/64 loss: -0.07544243335723877
Batch 16/64 loss: -0.08573800325393677
Batch 17/64 loss: -0.10714524984359741
Batch 18/64 loss: -0.09348791837692261
Batch 19/64 loss: -0.05488330125808716
Batch 20/64 loss: -0.08067327737808228
Batch 21/64 loss: -0.07459759712219238
Batch 22/64 loss: -0.07176482677459717
Batch 23/64 loss: -0.08575934171676636
Batch 24/64 loss: -0.09398585557937622
Batch 25/64 loss: -0.08683919906616211
Batch 26/64 loss: -0.09836339950561523
Batch 27/64 loss: -0.0736086368560791
Batch 28/64 loss: -0.08936887979507446
Batch 29/64 loss: -0.07474178075790405
Batch 30/64 loss: -0.07999348640441895
Batch 31/64 loss: -0.10005491971969604
Batch 32/64 loss: -0.07255935668945312
Batch 33/64 loss: -0.08016467094421387
Batch 34/64 loss: -0.06431394815444946
Batch 35/64 loss: -0.08177566528320312
Batch 36/64 loss: -0.08556443452835083
Batch 37/64 loss: -0.08210903406143188
Batch 38/64 loss: -0.09464681148529053
Batch 39/64 loss: -0.055006563663482666
Batch 40/64 loss: -0.09205585718154907
Batch 41/64 loss: -0.06813442707061768
Batch 42/64 loss: -0.07529711723327637
Batch 43/64 loss: -0.0823068618774414
Batch 44/64 loss: -0.08785611391067505
Batch 45/64 loss: -0.08716464042663574
Batch 46/64 loss: -0.09025323390960693
Batch 47/64 loss: -0.09206581115722656
Batch 48/64 loss: -0.0835120677947998
Batch 49/64 loss: -0.0625838041305542
Batch 50/64 loss: -0.10000669956207275
Batch 51/64 loss: -0.08653485774993896
Batch 52/64 loss: -0.06604117155075073
Batch 53/64 loss: -0.07677429914474487
Batch 54/64 loss: -0.09213143587112427
Batch 55/64 loss: -0.08241921663284302
Batch 56/64 loss: -0.07529991865158081
Batch 57/64 loss: -0.07411938905715942
Batch 58/64 loss: -0.08708620071411133
Batch 59/64 loss: -0.08149796724319458
Batch 60/64 loss: -0.09040820598602295
Batch 61/64 loss: -0.09775376319885254
Batch 62/64 loss: -0.06792241334915161
Batch 63/64 loss: -0.08639836311340332
Batch 64/64 loss: -0.07804208993911743
Epoch 163  Train loss: -0.08370266872293809  Val loss: -0.03954232169180801
Saving best model, epoch: 163
Epoch 164
-------------------------------
Batch 1/64 loss: -0.08505082130432129
Batch 2/64 loss: -0.08027452230453491
Batch 3/64 loss: -0.07348477840423584
Batch 4/64 loss: -0.10983139276504517
Batch 5/64 loss: -0.09166324138641357
Batch 6/64 loss: -0.09512120485305786
Batch 7/64 loss: -0.0904085636138916
Batch 8/64 loss: -0.09543156623840332
Batch 9/64 loss: -0.10142374038696289
Batch 10/64 loss: -0.09010642766952515
Batch 11/64 loss: -0.08435511589050293
Batch 12/64 loss: -0.07174217700958252
Batch 13/64 loss: -0.09288203716278076
Batch 14/64 loss: -0.08509945869445801
Batch 15/64 loss: -0.08820855617523193
Batch 16/64 loss: -0.07905161380767822
Batch 17/64 loss: -0.08840972185134888
Batch 18/64 loss: -0.07924073934555054
Batch 19/64 loss: -0.08022916316986084
Batch 20/64 loss: -0.07303488254547119
Batch 21/64 loss: -0.09240710735321045
Batch 22/64 loss: -0.08284425735473633
Batch 23/64 loss: -0.07891845703125
Batch 24/64 loss: -0.07717323303222656
Batch 25/64 loss: -0.09039980173110962
Batch 26/64 loss: -0.08862864971160889
Batch 27/64 loss: -0.10291290283203125
Batch 28/64 loss: -0.09204447269439697
Batch 29/64 loss: -0.09312659502029419
Batch 30/64 loss: -0.07509273290634155
Batch 31/64 loss: -0.08690834045410156
Batch 32/64 loss: -0.09558713436126709
Batch 33/64 loss: -0.08028936386108398
Batch 34/64 loss: -0.08047837018966675
Batch 35/64 loss: -0.07959854602813721
Batch 36/64 loss: -0.09132999181747437
Batch 37/64 loss: -0.057480812072753906
Batch 38/64 loss: -0.08984541893005371
Batch 39/64 loss: -0.08807015419006348
Batch 40/64 loss: -0.07659566402435303
Batch 41/64 loss: -0.08702605962753296
Batch 42/64 loss: -0.08378571271896362
Batch 43/64 loss: -0.09073686599731445
Batch 44/64 loss: -0.06708484888076782
Batch 45/64 loss: -0.09859097003936768
Batch 46/64 loss: -0.07233196496963501
Batch 47/64 loss: -0.09293210506439209
Batch 48/64 loss: -0.09442198276519775
Batch 49/64 loss: -0.09431880712509155
Batch 50/64 loss: -0.09008902311325073
Batch 51/64 loss: -0.09612798690795898
Batch 52/64 loss: -0.08137422800064087
Batch 53/64 loss: -0.0637744665145874
Batch 54/64 loss: -0.08336138725280762
Batch 55/64 loss: -0.09742540121078491
Batch 56/64 loss: -0.0912940502166748
Batch 57/64 loss: -0.08637422323226929
Batch 58/64 loss: -0.0663648247718811
Batch 59/64 loss: -0.08264201879501343
Batch 60/64 loss: -0.07366788387298584
Batch 61/64 loss: -0.06841510534286499
Batch 62/64 loss: -0.08726441860198975
Batch 63/64 loss: -0.09432756900787354
Batch 64/64 loss: -0.0936928391456604
Epoch 164  Train loss: -0.08547111001669191  Val loss: -0.03770358537890248
Epoch 165
-------------------------------
Batch 1/64 loss: -0.10099613666534424
Batch 2/64 loss: -0.08735466003417969
Batch 3/64 loss: -0.08523571491241455
Batch 4/64 loss: -0.05416512489318848
Batch 5/64 loss: -0.09043282270431519
Batch 6/64 loss: -0.08244979381561279
Batch 7/64 loss: -0.09389340877532959
Batch 8/64 loss: -0.06751823425292969
Batch 9/64 loss: -0.06328028440475464
Batch 10/64 loss: -0.07052481174468994
Batch 11/64 loss: -0.06026601791381836
Batch 12/64 loss: -0.07753586769104004
Batch 13/64 loss: -0.0671347975730896
Batch 14/64 loss: -0.09578752517700195
Batch 15/64 loss: -0.08785974979400635
Batch 16/64 loss: -0.0890304446220398
Batch 17/64 loss: -0.10290640592575073
Batch 18/64 loss: -0.07825690507888794
Batch 19/64 loss: -0.09023308753967285
Batch 20/64 loss: -0.08189409971237183
Batch 21/64 loss: -0.07021439075469971
Batch 22/64 loss: -0.04894232749938965
Batch 23/64 loss: -0.06970727443695068
Batch 24/64 loss: -0.0779309868812561
Batch 25/64 loss: -0.08665746450424194
Batch 26/64 loss: -0.0690913200378418
Batch 27/64 loss: -0.09164875745773315
Batch 28/64 loss: -0.070628821849823
Batch 29/64 loss: -0.09337872266769409
Batch 30/64 loss: -0.07622849941253662
Batch 31/64 loss: -0.09340131282806396
Batch 32/64 loss: -0.08811861276626587
Batch 33/64 loss: -0.0709846019744873
Batch 34/64 loss: -0.046802401542663574
Batch 35/64 loss: -0.09259569644927979
Batch 36/64 loss: -0.08642160892486572
Batch 37/64 loss: -0.0903693437576294
Batch 38/64 loss: -0.08251065015792847
Batch 39/64 loss: -0.0731162428855896
Batch 40/64 loss: -0.08912163972854614
Batch 41/64 loss: -0.08523952960968018
Batch 42/64 loss: -0.07781463861465454
Batch 43/64 loss: -0.09186923503875732
Batch 44/64 loss: -0.08799475431442261
Batch 45/64 loss: -0.06870359182357788
Batch 46/64 loss: -0.0652502179145813
Batch 47/64 loss: -0.09457361698150635
Batch 48/64 loss: -0.08862775564193726
Batch 49/64 loss: -0.057831645011901855
Batch 50/64 loss: -0.09206974506378174
Batch 51/64 loss: -0.07692396640777588
Batch 52/64 loss: -0.07735443115234375
Batch 53/64 loss: -0.08355987071990967
Batch 54/64 loss: -0.09497731924057007
Batch 55/64 loss: -0.07923412322998047
Batch 56/64 loss: -0.0923117995262146
Batch 57/64 loss: -0.07859736680984497
Batch 58/64 loss: -0.08561891317367554
Batch 59/64 loss: -0.09129393100738525
Batch 60/64 loss: -0.07624948024749756
Batch 61/64 loss: -0.08554798364639282
Batch 62/64 loss: -0.09397071599960327
Batch 63/64 loss: -0.09871083498001099
Batch 64/64 loss: -0.08327513933181763
Epoch 165  Train loss: -0.08127699435925952  Val loss: -0.0375822462986425
Epoch 166
-------------------------------
Batch 1/64 loss: -0.08985310792922974
Batch 2/64 loss: -0.07048499584197998
Batch 3/64 loss: -0.07605987787246704
Batch 4/64 loss: -0.0857362151145935
Batch 5/64 loss: -0.07260185480117798
Batch 6/64 loss: -0.08041977882385254
Batch 7/64 loss: -0.08124697208404541
Batch 8/64 loss: -0.08238112926483154
Batch 9/64 loss: -0.0857129693031311
Batch 10/64 loss: -0.08199912309646606
Batch 11/64 loss: -0.07962948083877563
Batch 12/64 loss: -0.07442009449005127
Batch 13/64 loss: -0.08794808387756348
Batch 14/64 loss: -0.07810431718826294
Batch 15/64 loss: -0.09848219156265259
Batch 16/64 loss: -0.07773178815841675
Batch 17/64 loss: -0.08834755420684814
Batch 18/64 loss: -0.09184277057647705
Batch 19/64 loss: -0.08884704113006592
Batch 20/64 loss: -0.08834707736968994
Batch 21/64 loss: -0.09972655773162842
Batch 22/64 loss: -0.07773792743682861
Batch 23/64 loss: -0.07832139730453491
Batch 24/64 loss: -0.0681607723236084
Batch 25/64 loss: -0.08786594867706299
Batch 26/64 loss: -0.08054876327514648
Batch 27/64 loss: -0.08908551931381226
Batch 28/64 loss: -0.09258568286895752
Batch 29/64 loss: -0.08835875988006592
Batch 30/64 loss: -0.0755261778831482
Batch 31/64 loss: -0.08311593532562256
Batch 32/64 loss: -0.08214372396469116
Batch 33/64 loss: -0.07501983642578125
Batch 34/64 loss: -0.06937623023986816
Batch 35/64 loss: -0.09089958667755127
Batch 36/64 loss: -0.07085037231445312
Batch 37/64 loss: -0.08270233869552612
Batch 38/64 loss: -0.07515859603881836
Batch 39/64 loss: -0.07288378477096558
Batch 40/64 loss: -0.07643675804138184
Batch 41/64 loss: -0.07755178213119507
Batch 42/64 loss: -0.061646223068237305
Batch 43/64 loss: -0.07451659440994263
Batch 44/64 loss: -0.0930975079536438
Batch 45/64 loss: -0.08217728137969971
Batch 46/64 loss: -0.07997304201126099
Batch 47/64 loss: -0.07425320148468018
Batch 48/64 loss: -0.07950496673583984
Batch 49/64 loss: -0.10762470960617065
Batch 50/64 loss: -0.09599161148071289
Batch 51/64 loss: -0.08655869960784912
Batch 52/64 loss: -0.08690088987350464
Batch 53/64 loss: -0.062641441822052
Batch 54/64 loss: -0.06645166873931885
Batch 55/64 loss: -0.07287317514419556
Batch 56/64 loss: -0.09143561124801636
Batch 57/64 loss: -0.07338577508926392
Batch 58/64 loss: -0.08745419979095459
Batch 59/64 loss: -0.06421554088592529
Batch 60/64 loss: -0.07329940795898438
Batch 61/64 loss: -0.08792150020599365
Batch 62/64 loss: -0.0865246057510376
Batch 63/64 loss: -0.06549888849258423
Batch 64/64 loss: -0.08583807945251465
Epoch 166  Train loss: -0.08113847807341931  Val loss: -0.040305957351763226
Saving best model, epoch: 166
Epoch 167
-------------------------------
Batch 1/64 loss: -0.0864342451095581
Batch 2/64 loss: -0.08158004283905029
Batch 3/64 loss: -0.08017802238464355
Batch 4/64 loss: -0.0921562910079956
Batch 5/64 loss: -0.08901810646057129
Batch 6/64 loss: -0.08109539747238159
Batch 7/64 loss: -0.09022301435470581
Batch 8/64 loss: -0.08958721160888672
Batch 9/64 loss: -0.0956621766090393
Batch 10/64 loss: -0.07880938053131104
Batch 11/64 loss: -0.08290880918502808
Batch 12/64 loss: -0.087155282497406
Batch 13/64 loss: -0.0826907753944397
Batch 14/64 loss: -0.08501994609832764
Batch 15/64 loss: -0.09479594230651855
Batch 16/64 loss: -0.06551569700241089
Batch 17/64 loss: -0.08240240812301636
Batch 18/64 loss: -0.10531139373779297
Batch 19/64 loss: -0.0897035002708435
Batch 20/64 loss: -0.08836889266967773
Batch 21/64 loss: -0.08968520164489746
Batch 22/64 loss: -0.08902424573898315
Batch 23/64 loss: -0.09465211629867554
Batch 24/64 loss: -0.07964414358139038
Batch 25/64 loss: -0.10099059343338013
Batch 26/64 loss: -0.06987184286117554
Batch 27/64 loss: -0.09018188714981079
Batch 28/64 loss: -0.09549319744110107
Batch 29/64 loss: -0.08754998445510864
Batch 30/64 loss: -0.09774130582809448
Batch 31/64 loss: -0.07691586017608643
Batch 32/64 loss: -0.08990323543548584
Batch 33/64 loss: -0.07004880905151367
Batch 34/64 loss: -0.07344120740890503
Batch 35/64 loss: -0.07528465986251831
Batch 36/64 loss: -0.10416805744171143
Batch 37/64 loss: -0.08648186922073364
Batch 38/64 loss: -0.0680321455001831
Batch 39/64 loss: -0.08935546875
Batch 40/64 loss: -0.0899844765663147
Batch 41/64 loss: -0.10258883237838745
Batch 42/64 loss: -0.08542269468307495
Batch 43/64 loss: -0.08139443397521973
Batch 44/64 loss: -0.10019099712371826
Batch 45/64 loss: -0.08134669065475464
Batch 46/64 loss: -0.0798804759979248
Batch 47/64 loss: -0.09572464227676392
Batch 48/64 loss: -0.07734382152557373
Batch 49/64 loss: -0.10472041368484497
Batch 50/64 loss: -0.08355027437210083
Batch 51/64 loss: -0.08369463682174683
Batch 52/64 loss: -0.08145976066589355
Batch 53/64 loss: -0.09239161014556885
Batch 54/64 loss: -0.07470458745956421
Batch 55/64 loss: -0.07661336660385132
Batch 56/64 loss: -0.09068691730499268
Batch 57/64 loss: -0.0824316143989563
Batch 58/64 loss: -0.08511388301849365
Batch 59/64 loss: -0.07071852684020996
Batch 60/64 loss: -0.0812692642211914
Batch 61/64 loss: -0.09111303091049194
Batch 62/64 loss: -0.08213597536087036
Batch 63/64 loss: -0.07543134689331055
Batch 64/64 loss: -0.08929252624511719
Epoch 167  Train loss: -0.08586610251781987  Val loss: -0.04013024777481236
Epoch 168
-------------------------------
Batch 1/64 loss: -0.09051483869552612
Batch 2/64 loss: -0.0973019003868103
Batch 3/64 loss: -0.06823211908340454
Batch 4/64 loss: -0.09354698657989502
Batch 5/64 loss: -0.08419919013977051
Batch 6/64 loss: -0.08167946338653564
Batch 7/64 loss: -0.07722842693328857
Batch 8/64 loss: -0.09140229225158691
Batch 9/64 loss: -0.10048490762710571
Batch 10/64 loss: -0.08689570426940918
Batch 11/64 loss: -0.07473266124725342
Batch 12/64 loss: -0.09130609035491943
Batch 13/64 loss: -0.06522226333618164
Batch 14/64 loss: -0.08387041091918945
Batch 15/64 loss: -0.09000027179718018
Batch 16/64 loss: -0.09785652160644531
Batch 17/64 loss: -0.09667384624481201
Batch 18/64 loss: -0.09078633785247803
Batch 19/64 loss: -0.06384819746017456
Batch 20/64 loss: -0.08040398359298706
Batch 21/64 loss: -0.06666076183319092
Batch 22/64 loss: -0.0807684063911438
Batch 23/64 loss: -0.09956836700439453
Batch 24/64 loss: -0.06155800819396973
Batch 25/64 loss: -0.09317886829376221
Batch 26/64 loss: -0.09482598304748535
Batch 27/64 loss: -0.08043849468231201
Batch 28/64 loss: -0.09863322973251343
Batch 29/64 loss: -0.08714926242828369
Batch 30/64 loss: -0.0982394814491272
Batch 31/64 loss: -0.07661229372024536
Batch 32/64 loss: -0.09177732467651367
Batch 33/64 loss: -0.09638011455535889
Batch 34/64 loss: -0.08727729320526123
Batch 35/64 loss: -0.08972764015197754
Batch 36/64 loss: -0.08506834506988525
Batch 37/64 loss: -0.1061638593673706
Batch 38/64 loss: -0.08413249254226685
Batch 39/64 loss: -0.08119642734527588
Batch 40/64 loss: -0.09649121761322021
Batch 41/64 loss: -0.09084266424179077
Batch 42/64 loss: -0.08812510967254639
Batch 43/64 loss: -0.08851790428161621
Batch 44/64 loss: -0.057641029357910156
Batch 45/64 loss: -0.10808378458023071
Batch 46/64 loss: -0.09473526477813721
Batch 47/64 loss: -0.09886139631271362
Batch 48/64 loss: -0.07356703281402588
Batch 49/64 loss: -0.09114807844161987
Batch 50/64 loss: -0.0724368691444397
Batch 51/64 loss: -0.09148770570755005
Batch 52/64 loss: -0.08791089057922363
Batch 53/64 loss: -0.08473414182662964
Batch 54/64 loss: -0.0762636661529541
Batch 55/64 loss: -0.09256106615066528
Batch 56/64 loss: -0.08849984407424927
Batch 57/64 loss: -0.07710927724838257
Batch 58/64 loss: -0.07070279121398926
Batch 59/64 loss: -0.09362250566482544
Batch 60/64 loss: -0.08413678407669067
Batch 61/64 loss: -0.08195912837982178
Batch 62/64 loss: -0.08765542507171631
Batch 63/64 loss: -0.08500993251800537
Batch 64/64 loss: -0.09234648942947388
Epoch 168  Train loss: -0.08622598344204473  Val loss: -0.0402835439570581
Epoch 169
-------------------------------
Batch 1/64 loss: -0.08621728420257568
Batch 2/64 loss: -0.10952961444854736
Batch 3/64 loss: -0.10050642490386963
Batch 4/64 loss: -0.0803450345993042
Batch 5/64 loss: -0.09847670793533325
Batch 6/64 loss: -0.0879167914390564
Batch 7/64 loss: -0.08406561613082886
Batch 8/64 loss: -0.09124046564102173
Batch 9/64 loss: -0.08205699920654297
Batch 10/64 loss: -0.07105016708374023
Batch 11/64 loss: -0.07916945219039917
Batch 12/64 loss: -0.072243332862854
Batch 13/64 loss: -0.0834505558013916
Batch 14/64 loss: -0.09745603799819946
Batch 15/64 loss: -0.08194279670715332
Batch 16/64 loss: -0.09096843004226685
Batch 17/64 loss: -0.07850801944732666
Batch 18/64 loss: -0.09554702043533325
Batch 19/64 loss: -0.06662893295288086
Batch 20/64 loss: -0.09625762701034546
Batch 21/64 loss: -0.08108210563659668
Batch 22/64 loss: -0.08480548858642578
Batch 23/64 loss: -0.08979344367980957
Batch 24/64 loss: -0.08418798446655273
Batch 25/64 loss: -0.07083213329315186
Batch 26/64 loss: -0.08505517244338989
Batch 27/64 loss: -0.07350021600723267
Batch 28/64 loss: -0.07446688413619995
Batch 29/64 loss: -0.09557604789733887
Batch 30/64 loss: -0.08575969934463501
Batch 31/64 loss: -0.0909242033958435
Batch 32/64 loss: -0.09256744384765625
Batch 33/64 loss: -0.08672606945037842
Batch 34/64 loss: -0.06876528263092041
Batch 35/64 loss: -0.083698570728302
Batch 36/64 loss: -0.09212666749954224
Batch 37/64 loss: -0.04599422216415405
Batch 38/64 loss: -0.0997614860534668
Batch 39/64 loss: -0.07265007495880127
Batch 40/64 loss: -0.10303652286529541
Batch 41/64 loss: -0.09969156980514526
Batch 42/64 loss: -0.07793217897415161
Batch 43/64 loss: -0.08435052633285522
Batch 44/64 loss: -0.0946928858757019
Batch 45/64 loss: -0.09383755922317505
Batch 46/64 loss: -0.04772371053695679
Batch 47/64 loss: -0.09067666530609131
Batch 48/64 loss: -0.09683704376220703
Batch 49/64 loss: -0.10202419757843018
Batch 50/64 loss: -0.09936857223510742
Batch 51/64 loss: -0.0903775691986084
Batch 52/64 loss: -0.0791746973991394
Batch 53/64 loss: -0.09315681457519531
Batch 54/64 loss: -0.08285385370254517
Batch 55/64 loss: -0.10538303852081299
Batch 56/64 loss: -0.08504271507263184
Batch 57/64 loss: -0.0879330039024353
Batch 58/64 loss: -0.08337068557739258
Batch 59/64 loss: -0.08264625072479248
Batch 60/64 loss: -0.0759347677230835
Batch 61/64 loss: -0.10519272089004517
Batch 62/64 loss: -0.08350735902786255
Batch 63/64 loss: -0.106364905834198
Batch 64/64 loss: -0.06827181577682495
Epoch 169  Train loss: -0.08618296758801329  Val loss: -0.03795463379306072
Epoch 170
-------------------------------
Batch 1/64 loss: -0.09208393096923828
Batch 2/64 loss: -0.09733271598815918
Batch 3/64 loss: -0.08794784545898438
Batch 4/64 loss: -0.07041347026824951
Batch 5/64 loss: -0.09324586391448975
Batch 6/64 loss: -0.0934293270111084
Batch 7/64 loss: -0.08723819255828857
Batch 8/64 loss: -0.07413315773010254
Batch 9/64 loss: -0.08051621913909912
Batch 10/64 loss: -0.07583820819854736
Batch 11/64 loss: -0.0999443531036377
Batch 12/64 loss: -0.08398270606994629
Batch 13/64 loss: -0.09419363737106323
Batch 14/64 loss: -0.0999457836151123
Batch 15/64 loss: -0.09586602449417114
Batch 16/64 loss: -0.08446747064590454
Batch 17/64 loss: -0.10256302356719971
Batch 18/64 loss: -0.08278870582580566
Batch 19/64 loss: -0.11255538463592529
Batch 20/64 loss: -0.09767156839370728
Batch 21/64 loss: -0.09581100940704346
Batch 22/64 loss: -0.07101154327392578
Batch 23/64 loss: -0.08497899770736694
Batch 24/64 loss: -0.09151685237884521
Batch 25/64 loss: -0.08044016361236572
Batch 26/64 loss: -0.06285935640335083
Batch 27/64 loss: -0.10014694929122925
Batch 28/64 loss: -0.0996847152709961
Batch 29/64 loss: -0.059465646743774414
Batch 30/64 loss: -0.06844168901443481
Batch 31/64 loss: -0.0820392370223999
Batch 32/64 loss: -0.08094316720962524
Batch 33/64 loss: -0.08416980504989624
Batch 34/64 loss: -0.07856285572052002
Batch 35/64 loss: -0.09767228364944458
Batch 36/64 loss: -0.09663951396942139
Batch 37/64 loss: -0.08626210689544678
Batch 38/64 loss: -0.08683443069458008
Batch 39/64 loss: -0.09448903799057007
Batch 40/64 loss: -0.08548390865325928
Batch 41/64 loss: -0.0968700647354126
Batch 42/64 loss: -0.0852891206741333
Batch 43/64 loss: -0.08386749029159546
Batch 44/64 loss: -0.08111625909805298
Batch 45/64 loss: -0.09701919555664062
Batch 46/64 loss: -0.07557225227355957
Batch 47/64 loss: -0.09210419654846191
Batch 48/64 loss: -0.096385657787323
Batch 49/64 loss: -0.09933996200561523
Batch 50/64 loss: -0.070345938205719
Batch 51/64 loss: -0.09814560413360596
Batch 52/64 loss: -0.084164559841156
Batch 53/64 loss: -0.08122938871383667
Batch 54/64 loss: -0.08617627620697021
Batch 55/64 loss: -0.0753440260887146
Batch 56/64 loss: -0.08946305513381958
Batch 57/64 loss: -0.09419536590576172
Batch 58/64 loss: -0.07513308525085449
Batch 59/64 loss: -0.08548372983932495
Batch 60/64 loss: -0.09387475252151489
Batch 61/64 loss: -0.09478002786636353
Batch 62/64 loss: -0.09736984968185425
Batch 63/64 loss: -0.06902891397476196
Batch 64/64 loss: -0.08850222826004028
Epoch 170  Train loss: -0.08725154984231089  Val loss: -0.03567495214980083
Epoch 171
-------------------------------
Batch 1/64 loss: -0.09187197685241699
Batch 2/64 loss: -0.09099334478378296
Batch 3/64 loss: -0.10463905334472656
Batch 4/64 loss: -0.05947023630142212
Batch 5/64 loss: -0.1020851731300354
Batch 6/64 loss: -0.0872650146484375
Batch 7/64 loss: -0.08769547939300537
Batch 8/64 loss: -0.10403501987457275
Batch 9/64 loss: -0.10162413120269775
Batch 10/64 loss: -0.08721953630447388
Batch 11/64 loss: -0.09924513101577759
Batch 12/64 loss: -0.08502012491226196
Batch 13/64 loss: -0.10751360654830933
Batch 14/64 loss: -0.08222270011901855
Batch 15/64 loss: -0.09665882587432861
Batch 16/64 loss: -0.0872986912727356
Batch 17/64 loss: -0.08694511651992798
Batch 18/64 loss: -0.0950058102607727
Batch 19/64 loss: -0.08486324548721313
Batch 20/64 loss: -0.07931536436080933
Batch 21/64 loss: -0.08173108100891113
Batch 22/64 loss: -0.08646190166473389
Batch 23/64 loss: -0.08360052108764648
Batch 24/64 loss: -0.09830260276794434
Batch 25/64 loss: -0.07720887660980225
Batch 26/64 loss: -0.09788084030151367
Batch 27/64 loss: -0.09784543514251709
Batch 28/64 loss: -0.08852177858352661
Batch 29/64 loss: -0.09069901704788208
Batch 30/64 loss: -0.08203351497650146
Batch 31/64 loss: -0.10458707809448242
Batch 32/64 loss: -0.07188171148300171
Batch 33/64 loss: -0.0640418529510498
Batch 34/64 loss: -0.0692937970161438
Batch 35/64 loss: -0.09826064109802246
Batch 36/64 loss: -0.08876049518585205
Batch 37/64 loss: -0.06683570146560669
Batch 38/64 loss: -0.08999943733215332
Batch 39/64 loss: -0.08895057439804077
Batch 40/64 loss: -0.0893697738647461
Batch 41/64 loss: -0.07467871904373169
Batch 42/64 loss: -0.09521764516830444
Batch 43/64 loss: -0.08347070217132568
Batch 44/64 loss: -0.08750396966934204
Batch 45/64 loss: -0.09057897329330444
Batch 46/64 loss: -0.08732408285140991
Batch 47/64 loss: -0.0841829776763916
Batch 48/64 loss: -0.06520092487335205
Batch 49/64 loss: -0.07842427492141724
Batch 50/64 loss: -0.0842355489730835
Batch 51/64 loss: -0.07722997665405273
Batch 52/64 loss: -0.08596330881118774
Batch 53/64 loss: -0.08846533298492432
Batch 54/64 loss: -0.08223015069961548
Batch 55/64 loss: -0.08048278093338013
Batch 56/64 loss: -0.08810508251190186
Batch 57/64 loss: -0.06819528341293335
Batch 58/64 loss: -0.09583532810211182
Batch 59/64 loss: -0.09710407257080078
Batch 60/64 loss: -0.07521653175354004
Batch 61/64 loss: -0.06679946184158325
Batch 62/64 loss: -0.08199483156204224
Batch 63/64 loss: -0.08187437057495117
Batch 64/64 loss: -0.09800750017166138
Epoch 171  Train loss: -0.08644822228188608  Val loss: -0.03914001819604041
Epoch 172
-------------------------------
Batch 1/64 loss: -0.08579802513122559
Batch 2/64 loss: -0.09488242864608765
Batch 3/64 loss: -0.08263397216796875
Batch 4/64 loss: -0.11404663324356079
Batch 5/64 loss: -0.09615367650985718
Batch 6/64 loss: -0.09531384706497192
Batch 7/64 loss: -0.09538620710372925
Batch 8/64 loss: -0.09796404838562012
Batch 9/64 loss: -0.08613640069961548
Batch 10/64 loss: -0.09892678260803223
Batch 11/64 loss: -0.08194869756698608
Batch 12/64 loss: -0.08627504110336304
Batch 13/64 loss: -0.10776722431182861
Batch 14/64 loss: -0.07127988338470459
Batch 15/64 loss: -0.0630761981010437
Batch 16/64 loss: -0.08900988101959229
Batch 17/64 loss: -0.06916487216949463
Batch 18/64 loss: -0.09714484214782715
Batch 19/64 loss: -0.10912936925888062
Batch 20/64 loss: -0.08221465349197388
Batch 21/64 loss: -0.09983569383621216
Batch 22/64 loss: -0.09282457828521729
Batch 23/64 loss: -0.09719425439834595
Batch 24/64 loss: -0.09026843309402466
Batch 25/64 loss: -0.06511116027832031
Batch 26/64 loss: -0.0813748836517334
Batch 27/64 loss: -0.08374857902526855
Batch 28/64 loss: -0.07466256618499756
Batch 29/64 loss: -0.08684080839157104
Batch 30/64 loss: -0.07517701387405396
Batch 31/64 loss: -0.08841454982757568
Batch 32/64 loss: -0.0880783200263977
Batch 33/64 loss: -0.08999913930892944
Batch 34/64 loss: -0.08395498991012573
Batch 35/64 loss: -0.09280920028686523
Batch 36/64 loss: -0.1002843976020813
Batch 37/64 loss: -0.07775300741195679
Batch 38/64 loss: -0.08562475442886353
Batch 39/64 loss: -0.10270613431930542
Batch 40/64 loss: -0.0720415711402893
Batch 41/64 loss: -0.09411847591400146
Batch 42/64 loss: -0.08862602710723877
Batch 43/64 loss: -0.0956832766532898
Batch 44/64 loss: -0.07851558923721313
Batch 45/64 loss: -0.08880680799484253
Batch 46/64 loss: -0.086972177028656
Batch 47/64 loss: -0.09139549732208252
Batch 48/64 loss: -0.07995778322219849
Batch 49/64 loss: -0.089000403881073
Batch 50/64 loss: -0.0985485315322876
Batch 51/64 loss: -0.09182989597320557
Batch 52/64 loss: -0.07843589782714844
Batch 53/64 loss: -0.07391631603240967
Batch 54/64 loss: -0.08645373582839966
Batch 55/64 loss: -0.0985875129699707
Batch 56/64 loss: -0.07586854696273804
Batch 57/64 loss: -0.10728871822357178
Batch 58/64 loss: -0.07800734043121338
Batch 59/64 loss: -0.09279751777648926
Batch 60/64 loss: -0.07783639430999756
Batch 61/64 loss: -0.10278862714767456
Batch 62/64 loss: -0.09761613607406616
Batch 63/64 loss: -0.0907866358757019
Batch 64/64 loss: -0.08433687686920166
Epoch 172  Train loss: -0.08847132897844502  Val loss: -0.038894069973136144
Epoch 173
-------------------------------
Batch 1/64 loss: -0.09280568361282349
Batch 2/64 loss: -0.09638071060180664
Batch 3/64 loss: -0.0799475908279419
Batch 4/64 loss: -0.0704270601272583
Batch 5/64 loss: -0.10615032911300659
Batch 6/64 loss: -0.08951741456985474
Batch 7/64 loss: -0.08551448583602905
Batch 8/64 loss: -0.08401191234588623
Batch 9/64 loss: -0.08510154485702515
Batch 10/64 loss: -0.08791172504425049
Batch 11/64 loss: -0.09379696846008301
Batch 12/64 loss: -0.09722411632537842
Batch 13/64 loss: -0.08544313907623291
Batch 14/64 loss: -0.08053851127624512
Batch 15/64 loss: -0.09985673427581787
Batch 16/64 loss: -0.08828979730606079
Batch 17/64 loss: -0.085193932056427
Batch 18/64 loss: -0.09244048595428467
Batch 19/64 loss: -0.08199846744537354
Batch 20/64 loss: -0.09584671258926392
Batch 21/64 loss: -0.09382259845733643
Batch 22/64 loss: -0.08341747522354126
Batch 23/64 loss: -0.07786250114440918
Batch 24/64 loss: -0.07344508171081543
Batch 25/64 loss: -0.08647125959396362
Batch 26/64 loss: -0.0886964201927185
Batch 27/64 loss: -0.07500028610229492
Batch 28/64 loss: -0.10975110530853271
Batch 29/64 loss: -0.09779584407806396
Batch 30/64 loss: -0.07216167449951172
Batch 31/64 loss: -0.09563344717025757
Batch 32/64 loss: -0.08924609422683716
Batch 33/64 loss: -0.08172434568405151
Batch 34/64 loss: -0.08177226781845093
Batch 35/64 loss: -0.10185414552688599
Batch 36/64 loss: -0.08773064613342285
Batch 37/64 loss: -0.09068763256072998
Batch 38/64 loss: -0.09560465812683105
Batch 39/64 loss: -0.09117907285690308
Batch 40/64 loss: -0.08544361591339111
Batch 41/64 loss: -0.09356081485748291
Batch 42/64 loss: -0.08921253681182861
Batch 43/64 loss: -0.08748465776443481
Batch 44/64 loss: -0.08571702241897583
Batch 45/64 loss: -0.0810011625289917
Batch 46/64 loss: -0.10544705390930176
Batch 47/64 loss: -0.11321359872817993
Batch 48/64 loss: -0.08926534652709961
Batch 49/64 loss: -0.08879232406616211
Batch 50/64 loss: -0.08713710308074951
Batch 51/64 loss: -0.1014663577079773
Batch 52/64 loss: -0.08011460304260254
Batch 53/64 loss: -0.09230875968933105
Batch 54/64 loss: -0.09399664402008057
Batch 55/64 loss: -0.10811853408813477
Batch 56/64 loss: -0.08425736427307129
Batch 57/64 loss: -0.10260176658630371
Batch 58/64 loss: -0.09557497501373291
Batch 59/64 loss: -0.06301093101501465
Batch 60/64 loss: -0.07695132493972778
Batch 61/64 loss: -0.08818656206130981
Batch 62/64 loss: -0.08805900812149048
Batch 63/64 loss: -0.07412248849868774
Batch 64/64 loss: -0.0685386061668396
Epoch 173  Train loss: -0.08876395903381647  Val loss: -0.03509274176305922
Epoch 174
-------------------------------
Batch 1/64 loss: -0.0951874852180481
Batch 2/64 loss: -0.07526540756225586
Batch 3/64 loss: -0.09987097978591919
Batch 4/64 loss: -0.0991060733795166
Batch 5/64 loss: -0.09743142127990723
Batch 6/64 loss: -0.08767205476760864
Batch 7/64 loss: -0.09243017435073853
Batch 8/64 loss: -0.09253805875778198
Batch 9/64 loss: -0.08562088012695312
Batch 10/64 loss: -0.08480346202850342
Batch 11/64 loss: -0.09997320175170898
Batch 12/64 loss: -0.07053470611572266
Batch 13/64 loss: -0.09982401132583618
Batch 14/64 loss: -0.1051105260848999
Batch 15/64 loss: -0.08681070804595947
Batch 16/64 loss: -0.09510797262191772
Batch 17/64 loss: -0.10489219427108765
Batch 18/64 loss: -0.09330695867538452
Batch 19/64 loss: -0.10034960508346558
Batch 20/64 loss: -0.08928483724594116
Batch 21/64 loss: -0.09412878751754761
Batch 22/64 loss: -0.08793902397155762
Batch 23/64 loss: -0.09653633832931519
Batch 24/64 loss: -0.10737055540084839
Batch 25/64 loss: -0.08762288093566895
Batch 26/64 loss: -0.08656001091003418
Batch 27/64 loss: -0.09878605604171753
Batch 28/64 loss: -0.08686590194702148
Batch 29/64 loss: -0.09717267751693726
Batch 30/64 loss: -0.0828670859336853
Batch 31/64 loss: -0.1044074296951294
Batch 32/64 loss: -0.09649050235748291
Batch 33/64 loss: -0.10084503889083862
Batch 34/64 loss: -0.09744775295257568
Batch 35/64 loss: -0.10615289211273193
Batch 36/64 loss: -0.07408827543258667
Batch 37/64 loss: -0.09290671348571777
Batch 38/64 loss: -0.09730416536331177
Batch 39/64 loss: -0.09776961803436279
Batch 40/64 loss: -0.07558727264404297
Batch 41/64 loss: -0.0872340202331543
Batch 42/64 loss: -0.08844780921936035
Batch 43/64 loss: -0.08465641736984253
Batch 44/64 loss: -0.09477967023849487
Batch 45/64 loss: -0.0918203592300415
Batch 46/64 loss: -0.09091269969940186
Batch 47/64 loss: -0.07711923122406006
Batch 48/64 loss: -0.07296741008758545
Batch 49/64 loss: -0.09607911109924316
Batch 50/64 loss: -0.10496217012405396
Batch 51/64 loss: -0.08848512172698975
Batch 52/64 loss: -0.0553969144821167
Batch 53/64 loss: -0.09382742643356323
Batch 54/64 loss: -0.09661155939102173
Batch 55/64 loss: -0.08635604381561279
Batch 56/64 loss: -0.06712150573730469
Batch 57/64 loss: -0.08944046497344971
Batch 58/64 loss: -0.06863158941268921
Batch 59/64 loss: -0.09315407276153564
Batch 60/64 loss: -0.10289061069488525
Batch 61/64 loss: -0.10321366786956787
Batch 62/64 loss: -0.07755827903747559
Batch 63/64 loss: -0.0717042088508606
Batch 64/64 loss: -0.07456886768341064
Epoch 174  Train loss: -0.09040418372434729  Val loss: -0.027323896532615844
Epoch 175
-------------------------------
Batch 1/64 loss: -0.09788721799850464
Batch 2/64 loss: -0.08724689483642578
Batch 3/64 loss: -0.09405350685119629
Batch 4/64 loss: -0.0814286470413208
Batch 5/64 loss: -0.11276054382324219
Batch 6/64 loss: -0.09862351417541504
Batch 7/64 loss: -0.09813946485519409
Batch 8/64 loss: -0.10815262794494629
Batch 9/64 loss: -0.0775296688079834
Batch 10/64 loss: -0.0986330509185791
Batch 11/64 loss: -0.07738864421844482
Batch 12/64 loss: -0.09919947385787964
Batch 13/64 loss: -0.09516829252243042
Batch 14/64 loss: -0.09066474437713623
Batch 15/64 loss: -0.09364187717437744
Batch 16/64 loss: -0.08576732873916626
Batch 17/64 loss: -0.08625400066375732
Batch 18/64 loss: -0.08861911296844482
Batch 19/64 loss: -0.09913069009780884
Batch 20/64 loss: -0.10152512788772583
Batch 21/64 loss: -0.07521992921829224
Batch 22/64 loss: -0.094024658203125
Batch 23/64 loss: -0.09521269798278809
Batch 24/64 loss: -0.05439269542694092
Batch 25/64 loss: -0.09774529933929443
Batch 26/64 loss: -0.09720158576965332
Batch 27/64 loss: -0.10726809501647949
Batch 28/64 loss: -0.09129530191421509
Batch 29/64 loss: -0.08592468500137329
Batch 30/64 loss: -0.08139818906784058
Batch 31/64 loss: -0.09132605791091919
Batch 32/64 loss: -0.06408083438873291
Batch 33/64 loss: -0.09487020969390869
Batch 34/64 loss: -0.09360301494598389
Batch 35/64 loss: -0.09258127212524414
Batch 36/64 loss: -0.0789603590965271
Batch 37/64 loss: -0.0868951678276062
Batch 38/64 loss: -0.09252870082855225
Batch 39/64 loss: -0.07933151721954346
Batch 40/64 loss: -0.06494075059890747
Batch 41/64 loss: -0.08623433113098145
Batch 42/64 loss: -0.08079415559768677
Batch 43/64 loss: -0.08708590269088745
Batch 44/64 loss: -0.09265124797821045
Batch 45/64 loss: -0.07557040452957153
Batch 46/64 loss: -0.09085255861282349
Batch 47/64 loss: -0.08403980731964111
Batch 48/64 loss: -0.09471863508224487
Batch 49/64 loss: -0.10142290592193604
Batch 50/64 loss: -0.0963781476020813
Batch 51/64 loss: -0.075408935546875
Batch 52/64 loss: -0.08922481536865234
Batch 53/64 loss: -0.08792531490325928
Batch 54/64 loss: -0.09242820739746094
Batch 55/64 loss: -0.07340651750564575
Batch 56/64 loss: -0.08806174993515015
Batch 57/64 loss: -0.09009939432144165
Batch 58/64 loss: -0.10298663377761841
Batch 59/64 loss: -0.08146131038665771
Batch 60/64 loss: -0.07681751251220703
Batch 61/64 loss: -0.0979340672492981
Batch 62/64 loss: -0.08436810970306396
Batch 63/64 loss: -0.09634292125701904
Batch 64/64 loss: -0.0699383020401001
Epoch 175  Train loss: -0.08892992571288465  Val loss: -0.03544085951605204
Epoch 176
-------------------------------
Batch 1/64 loss: -0.08687096834182739
Batch 2/64 loss: -0.07988244295120239
Batch 3/64 loss: -0.05584597587585449
Batch 4/64 loss: -0.09061115980148315
Batch 5/64 loss: -0.09773129224777222
Batch 6/64 loss: -0.09783118963241577
Batch 7/64 loss: -0.1090766191482544
Batch 8/64 loss: -0.0893700122833252
Batch 9/64 loss: -0.09666740894317627
Batch 10/64 loss: -0.09570753574371338
Batch 11/64 loss: -0.09342718124389648
Batch 12/64 loss: -0.05470460653305054
Batch 13/64 loss: -0.09965699911117554
Batch 14/64 loss: -0.0961911678314209
Batch 15/64 loss: -0.06729996204376221
Batch 16/64 loss: -0.07616770267486572
Batch 17/64 loss: -0.09416860342025757
Batch 18/64 loss: -0.08308839797973633
Batch 19/64 loss: -0.07396835088729858
Batch 20/64 loss: -0.08618801832199097
Batch 21/64 loss: -0.10515731573104858
Batch 22/64 loss: -0.10314410924911499
Batch 23/64 loss: -0.08322679996490479
Batch 24/64 loss: -0.08359479904174805
Batch 25/64 loss: -0.09515506029129028
Batch 26/64 loss: -0.10917401313781738
Batch 27/64 loss: -0.09456682205200195
Batch 28/64 loss: -0.0885966420173645
Batch 29/64 loss: -0.09611779451370239
Batch 30/64 loss: -0.10328799486160278
Batch 31/64 loss: -0.06675028800964355
Batch 32/64 loss: -0.10251849889755249
Batch 33/64 loss: -0.08526420593261719
Batch 34/64 loss: -0.10791629552841187
Batch 35/64 loss: -0.08282756805419922
Batch 36/64 loss: -0.09754300117492676
Batch 37/64 loss: -0.10221618413925171
Batch 38/64 loss: -0.09327757358551025
Batch 39/64 loss: -0.10327863693237305
Batch 40/64 loss: -0.09431606531143188
Batch 41/64 loss: -0.09024178981781006
Batch 42/64 loss: -0.08077359199523926
Batch 43/64 loss: -0.09249722957611084
Batch 44/64 loss: -0.08669829368591309
Batch 45/64 loss: -0.09324628114700317
Batch 46/64 loss: -0.09230738878250122
Batch 47/64 loss: -0.10335785150527954
Batch 48/64 loss: -0.07733643054962158
Batch 49/64 loss: -0.08757466077804565
Batch 50/64 loss: -0.0950707197189331
Batch 51/64 loss: -0.05769503116607666
Batch 52/64 loss: -0.10750418901443481
Batch 53/64 loss: -0.11330342292785645
Batch 54/64 loss: -0.08755868673324585
Batch 55/64 loss: -0.10786545276641846
Batch 56/64 loss: -0.10588723421096802
Batch 57/64 loss: -0.08070588111877441
Batch 58/64 loss: -0.0932498574256897
Batch 59/64 loss: -0.07988715171813965
Batch 60/64 loss: -0.09642356634140015
Batch 61/64 loss: -0.10212814807891846
Batch 62/64 loss: -0.09143972396850586
Batch 63/64 loss: -0.10333138704299927
Batch 64/64 loss: -0.08433020114898682
Epoch 176  Train loss: -0.09116418642156264  Val loss: -0.04043432855114494
Saving best model, epoch: 176
Epoch 177
-------------------------------
Batch 1/64 loss: -0.10860258340835571
Batch 2/64 loss: -0.09670132398605347
Batch 3/64 loss: -0.102367103099823
Batch 4/64 loss: -0.08594202995300293
Batch 5/64 loss: -0.09510773420333862
Batch 6/64 loss: -0.09624683856964111
Batch 7/64 loss: -0.071921706199646
Batch 8/64 loss: -0.0958743691444397
Batch 9/64 loss: -0.0893743634223938
Batch 10/64 loss: -0.08923321962356567
Batch 11/64 loss: -0.08581942319869995
Batch 12/64 loss: -0.08900606632232666
Batch 13/64 loss: -0.07015478610992432
Batch 14/64 loss: -0.09568232297897339
Batch 15/64 loss: -0.0950387716293335
Batch 16/64 loss: -0.08336704969406128
Batch 17/64 loss: -0.0964895486831665
Batch 18/64 loss: -0.10388213396072388
Batch 19/64 loss: -0.0924386978149414
Batch 20/64 loss: -0.071769118309021
Batch 21/64 loss: -0.09638172388076782
Batch 22/64 loss: -0.09314632415771484
Batch 23/64 loss: -0.08191162347793579
Batch 24/64 loss: -0.08593344688415527
Batch 25/64 loss: -0.08823013305664062
Batch 26/64 loss: -0.10244739055633545
Batch 27/64 loss: -0.07987630367279053
Batch 28/64 loss: -0.09703600406646729
Batch 29/64 loss: -0.0878751277923584
Batch 30/64 loss: -0.08347022533416748
Batch 31/64 loss: -0.09458154439926147
Batch 32/64 loss: -0.07863909006118774
Batch 33/64 loss: -0.1037752628326416
Batch 34/64 loss: -0.07493036985397339
Batch 35/64 loss: -0.10110807418823242
Batch 36/64 loss: -0.09583783149719238
Batch 37/64 loss: -0.09349572658538818
Batch 38/64 loss: -0.1012919545173645
Batch 39/64 loss: -0.09163504838943481
Batch 40/64 loss: -0.08222794532775879
Batch 41/64 loss: -0.11891686916351318
Batch 42/64 loss: -0.07593226432800293
Batch 43/64 loss: -0.08510100841522217
Batch 44/64 loss: -0.09564602375030518
Batch 45/64 loss: -0.09490495920181274
Batch 46/64 loss: -0.11021000146865845
Batch 47/64 loss: -0.08735436201095581
Batch 48/64 loss: -0.08852523565292358
Batch 49/64 loss: -0.1056896448135376
Batch 50/64 loss: -0.0935811996459961
Batch 51/64 loss: -0.08596336841583252
Batch 52/64 loss: -0.08948296308517456
Batch 53/64 loss: -0.08675563335418701
Batch 54/64 loss: -0.10452926158905029
Batch 55/64 loss: -0.10082554817199707
Batch 56/64 loss: -0.0892820954322815
Batch 57/64 loss: -0.08656644821166992
Batch 58/64 loss: -0.10175126791000366
Batch 59/64 loss: -0.09572678804397583
Batch 60/64 loss: -0.09310704469680786
Batch 61/64 loss: -0.0843573808670044
Batch 62/64 loss: -0.09509032964706421
Batch 63/64 loss: -0.08855235576629639
Batch 64/64 loss: -0.10385924577713013
Epoch 177  Train loss: -0.09199367574616975  Val loss: -0.035276006177528615
Epoch 178
-------------------------------
Batch 1/64 loss: -0.09882837533950806
Batch 2/64 loss: -0.10300850868225098
Batch 3/64 loss: -0.08503085374832153
Batch 4/64 loss: -0.11004006862640381
Batch 5/64 loss: -0.09795606136322021
Batch 6/64 loss: -0.09123343229293823
Batch 7/64 loss: -0.07384693622589111
Batch 8/64 loss: -0.10521411895751953
Batch 9/64 loss: -0.09748661518096924
Batch 10/64 loss: -0.10356789827346802
Batch 11/64 loss: -0.10964524745941162
Batch 12/64 loss: -0.10275793075561523
Batch 13/64 loss: -0.06690263748168945
Batch 14/64 loss: -0.09115856885910034
Batch 15/64 loss: -0.10754990577697754
Batch 16/64 loss: -0.08312028646469116
Batch 17/64 loss: -0.1034744381904602
Batch 18/64 loss: -0.09654998779296875
Batch 19/64 loss: -0.09855133295059204
Batch 20/64 loss: -0.08227378129959106
Batch 21/64 loss: -0.08636146783828735
Batch 22/64 loss: -0.09207481145858765
Batch 23/64 loss: -0.08969193696975708
Batch 24/64 loss: -0.08875882625579834
Batch 25/64 loss: -0.11743825674057007
Batch 26/64 loss: -0.07252395153045654
Batch 27/64 loss: -0.08237320184707642
Batch 28/64 loss: -0.08089977502822876
Batch 29/64 loss: -0.10696911811828613
Batch 30/64 loss: -0.092967689037323
Batch 31/64 loss: -0.09905874729156494
Batch 32/64 loss: -0.07897251844406128
Batch 33/64 loss: -0.09511053562164307
Batch 34/64 loss: -0.07403576374053955
Batch 35/64 loss: -0.09752178192138672
Batch 36/64 loss: -0.09545344114303589
Batch 37/64 loss: -0.0666513442993164
Batch 38/64 loss: -0.08124440908432007
Batch 39/64 loss: -0.07928705215454102
Batch 40/64 loss: -0.08700335025787354
Batch 41/64 loss: -0.080943763256073
Batch 42/64 loss: -0.10544788837432861
Batch 43/64 loss: -0.0968734622001648
Batch 44/64 loss: -0.09292912483215332
Batch 45/64 loss: -0.08964580297470093
Batch 46/64 loss: -0.08816593885421753
Batch 47/64 loss: -0.09403747320175171
Batch 48/64 loss: -0.08037835359573364
Batch 49/64 loss: -0.10586303472518921
Batch 50/64 loss: -0.10504406690597534
Batch 51/64 loss: -0.10238277912139893
Batch 52/64 loss: -0.08821040391921997
Batch 53/64 loss: -0.1087384819984436
Batch 54/64 loss: -0.10437917709350586
Batch 55/64 loss: -0.10259926319122314
Batch 56/64 loss: -0.0693749189376831
Batch 57/64 loss: -0.10162174701690674
Batch 58/64 loss: -0.09558475017547607
Batch 59/64 loss: -0.08253645896911621
Batch 60/64 loss: -0.08480703830718994
Batch 61/64 loss: -0.07585132122039795
Batch 62/64 loss: -0.10482990741729736
Batch 63/64 loss: -0.09622961282730103
Batch 64/64 loss: -0.08653777837753296
Epoch 178  Train loss: -0.092454479488672  Val loss: -0.03821906601030802
Epoch 179
-------------------------------
Batch 1/64 loss: -0.1177595853805542
Batch 2/64 loss: -0.09696316719055176
Batch 3/64 loss: -0.11401748657226562
Batch 4/64 loss: -0.09467411041259766
Batch 5/64 loss: -0.09982264041900635
Batch 6/64 loss: -0.0949016809463501
Batch 7/64 loss: -0.1081397533416748
Batch 8/64 loss: -0.10309791564941406
Batch 9/64 loss: -0.07877129316329956
Batch 10/64 loss: -0.10694712400436401
Batch 11/64 loss: -0.10224848985671997
Batch 12/64 loss: -0.09551775455474854
Batch 13/64 loss: -0.08566075563430786
Batch 14/64 loss: -0.0682329535484314
Batch 15/64 loss: -0.0859224796295166
Batch 16/64 loss: -0.0768662691116333
Batch 17/64 loss: -0.09629398584365845
Batch 18/64 loss: -0.08853578567504883
Batch 19/64 loss: -0.0984378457069397
Batch 20/64 loss: -0.10063648223876953
Batch 21/64 loss: -0.11823421716690063
Batch 22/64 loss: -0.1051628589630127
Batch 23/64 loss: -0.08979201316833496
Batch 24/64 loss: -0.10327470302581787
Batch 25/64 loss: -0.10891014337539673
Batch 26/64 loss: -0.11169898509979248
Batch 27/64 loss: -0.06463336944580078
Batch 28/64 loss: -0.09015023708343506
Batch 29/64 loss: -0.09840387105941772
Batch 30/64 loss: -0.11748456954956055
Batch 31/64 loss: -0.0883408784866333
Batch 32/64 loss: -0.06918340921401978
Batch 33/64 loss: -0.09811651706695557
Batch 34/64 loss: -0.08343952894210815
Batch 35/64 loss: -0.08211314678192139
Batch 36/64 loss: -0.08044254779815674
Batch 37/64 loss: -0.0788540244102478
Batch 38/64 loss: -0.08949589729309082
Batch 39/64 loss: -0.09556543827056885
Batch 40/64 loss: -0.08483743667602539
Batch 41/64 loss: -0.0927431583404541
Batch 42/64 loss: -0.09710836410522461
Batch 43/64 loss: -0.09957653284072876
Batch 44/64 loss: -0.11103296279907227
Batch 45/64 loss: -0.08618932962417603
Batch 46/64 loss: -0.06508368253707886
Batch 47/64 loss: -0.0752936601638794
Batch 48/64 loss: -0.09226036071777344
Batch 49/64 loss: -0.0977582335472107
Batch 50/64 loss: -0.08738082647323608
Batch 51/64 loss: -0.08975684642791748
Batch 52/64 loss: -0.09760868549346924
Batch 53/64 loss: -0.09622043371200562
Batch 54/64 loss: -0.10320889949798584
Batch 55/64 loss: -0.09045040607452393
Batch 56/64 loss: -0.08853232860565186
Batch 57/64 loss: -0.10211074352264404
Batch 58/64 loss: -0.08408153057098389
Batch 59/64 loss: -0.08279991149902344
Batch 60/64 loss: -0.10547369718551636
Batch 61/64 loss: -0.08133554458618164
Batch 62/64 loss: -0.09918415546417236
Batch 63/64 loss: -0.08439397811889648
Batch 64/64 loss: -0.10224813222885132
Epoch 179  Train loss: -0.09345649761312148  Val loss: -0.037692426406231126
Epoch 180
-------------------------------
Batch 1/64 loss: -0.090423583984375
Batch 2/64 loss: -0.07760262489318848
Batch 3/64 loss: -0.10104870796203613
Batch 4/64 loss: -0.09334814548492432
Batch 5/64 loss: -0.09347385168075562
Batch 6/64 loss: -0.08338016271591187
Batch 7/64 loss: -0.10380446910858154
Batch 8/64 loss: -0.1095932126045227
Batch 9/64 loss: -0.08928275108337402
Batch 10/64 loss: -0.10927891731262207
Batch 11/64 loss: -0.08948397636413574
Batch 12/64 loss: -0.08270114660263062
Batch 13/64 loss: -0.09453517198562622
Batch 14/64 loss: -0.08863133192062378
Batch 15/64 loss: -0.10755270719528198
Batch 16/64 loss: -0.10088378190994263
Batch 17/64 loss: -0.07011079788208008
Batch 18/64 loss: -0.08028090000152588
Batch 19/64 loss: -0.10461163520812988
Batch 20/64 loss: -0.09622794389724731
Batch 21/64 loss: -0.10428047180175781
Batch 22/64 loss: -0.08524692058563232
Batch 23/64 loss: -0.1012234091758728
Batch 24/64 loss: -0.07637578248977661
Batch 25/64 loss: -0.10761070251464844
Batch 26/64 loss: -0.09385448694229126
Batch 27/64 loss: -0.0904088020324707
Batch 28/64 loss: -0.09075850248336792
Batch 29/64 loss: -0.08085745573043823
Batch 30/64 loss: -0.08829480409622192
Batch 31/64 loss: -0.09367269277572632
Batch 32/64 loss: -0.09932959079742432
Batch 33/64 loss: -0.09081608057022095
Batch 34/64 loss: -0.0932074785232544
Batch 35/64 loss: -0.09822338819503784
Batch 36/64 loss: -0.10099929571151733
Batch 37/64 loss: -0.1015886664390564
Batch 38/64 loss: -0.1063389778137207
Batch 39/64 loss: -0.07905793190002441
Batch 40/64 loss: -0.09152024984359741
Batch 41/64 loss: -0.08657681941986084
Batch 42/64 loss: -0.0860700011253357
Batch 43/64 loss: -0.09597897529602051
Batch 44/64 loss: -0.09425801038742065
Batch 45/64 loss: -0.098685622215271
Batch 46/64 loss: -0.08147835731506348
Batch 47/64 loss: -0.10910153388977051
Batch 48/64 loss: -0.10167956352233887
Batch 49/64 loss: -0.09501153230667114
Batch 50/64 loss: -0.07741796970367432
Batch 51/64 loss: -0.10733377933502197
Batch 52/64 loss: -0.09850531816482544
Batch 53/64 loss: -0.09202766418457031
Batch 54/64 loss: -0.09034454822540283
Batch 55/64 loss: -0.09999072551727295
Batch 56/64 loss: -0.09462368488311768
Batch 57/64 loss: -0.09752625226974487
Batch 58/64 loss: -0.09921759366989136
Batch 59/64 loss: -0.07585418224334717
Batch 60/64 loss: -0.09750878810882568
Batch 61/64 loss: -0.09454858303070068
Batch 62/64 loss: -0.11597603559494019
Batch 63/64 loss: -0.09804761409759521
Batch 64/64 loss: -0.0929267406463623
Epoch 180  Train loss: -0.09407654463076125  Val loss: -0.040149324743198773
Epoch 181
-------------------------------
Batch 1/64 loss: -0.09951639175415039
Batch 2/64 loss: -0.09588325023651123
Batch 3/64 loss: -0.08942842483520508
Batch 4/64 loss: -0.11360365152359009
Batch 5/64 loss: -0.09486091136932373
Batch 6/64 loss: -0.09363692998886108
Batch 7/64 loss: -0.09258782863616943
Batch 8/64 loss: -0.08928149938583374
Batch 9/64 loss: -0.11360198259353638
Batch 10/64 loss: -0.11101210117340088
Batch 11/64 loss: -0.08803677558898926
Batch 12/64 loss: -0.09788858890533447
Batch 13/64 loss: -0.09338051080703735
Batch 14/64 loss: -0.09420841932296753
Batch 15/64 loss: -0.09738403558731079
Batch 16/64 loss: -0.09897810220718384
Batch 17/64 loss: -0.0911787748336792
Batch 18/64 loss: -0.08171170949935913
Batch 19/64 loss: -0.11838948726654053
Batch 20/64 loss: -0.09783756732940674
Batch 21/64 loss: -0.08342629671096802
Batch 22/64 loss: -0.09874355792999268
Batch 23/64 loss: -0.09801304340362549
Batch 24/64 loss: -0.09810882806777954
Batch 25/64 loss: -0.09458935260772705
Batch 26/64 loss: -0.09041404724121094
Batch 27/64 loss: -0.07368111610412598
Batch 28/64 loss: -0.06695175170898438
Batch 29/64 loss: -0.10172826051712036
Batch 30/64 loss: -0.10141831636428833
Batch 31/64 loss: -0.07399189472198486
Batch 32/64 loss: -0.08878076076507568
Batch 33/64 loss: -0.10948026180267334
Batch 34/64 loss: -0.09901160001754761
Batch 35/64 loss: -0.0868523120880127
Batch 36/64 loss: -0.08807265758514404
Batch 37/64 loss: -0.10000699758529663
Batch 38/64 loss: -0.09921032190322876
Batch 39/64 loss: -0.06490671634674072
Batch 40/64 loss: -0.10717076063156128
Batch 41/64 loss: -0.08796334266662598
Batch 42/64 loss: -0.08768779039382935
Batch 43/64 loss: -0.10249531269073486
Batch 44/64 loss: -0.10053396224975586
Batch 45/64 loss: -0.1020321249961853
Batch 46/64 loss: -0.10430961847305298
Batch 47/64 loss: -0.08448386192321777
Batch 48/64 loss: -0.11243373155593872
Batch 49/64 loss: -0.09726119041442871
Batch 50/64 loss: -0.10351783037185669
Batch 51/64 loss: -0.1051168441772461
Batch 52/64 loss: -0.11401617527008057
Batch 53/64 loss: -0.08396482467651367
Batch 54/64 loss: -0.09263992309570312
Batch 55/64 loss: -0.09309166669845581
Batch 56/64 loss: -0.10918581485748291
Batch 57/64 loss: -0.0925283432006836
Batch 58/64 loss: -0.10346841812133789
Batch 59/64 loss: -0.11757057905197144
Batch 60/64 loss: -0.11579298973083496
Batch 61/64 loss: -0.10892152786254883
Batch 62/64 loss: -0.10157877206802368
Batch 63/64 loss: -0.10201835632324219
Batch 64/64 loss: -0.10287332534790039
Epoch 181  Train loss: -0.09688994183259851  Val loss: -0.042931451830257664
Saving best model, epoch: 181
Epoch 182
-------------------------------
Batch 1/64 loss: -0.09909963607788086
Batch 2/64 loss: -0.10989749431610107
Batch 3/64 loss: -0.11277323961257935
Batch 4/64 loss: -0.08829790353775024
Batch 5/64 loss: -0.10787487030029297
Batch 6/64 loss: -0.10687267780303955
Batch 7/64 loss: -0.10585242509841919
Batch 8/64 loss: -0.11042660474777222
Batch 9/64 loss: -0.11539089679718018
Batch 10/64 loss: -0.0886116623878479
Batch 11/64 loss: -0.08680200576782227
Batch 12/64 loss: -0.09398466348648071
Batch 13/64 loss: -0.08950674533843994
Batch 14/64 loss: -0.08762878179550171
Batch 15/64 loss: -0.1142776608467102
Batch 16/64 loss: -0.09019964933395386
Batch 17/64 loss: -0.10578691959381104
Batch 18/64 loss: -0.09042507410049438
Batch 19/64 loss: -0.10701531171798706
Batch 20/64 loss: -0.08606666326522827
Batch 21/64 loss: -0.1148604154586792
Batch 22/64 loss: -0.0899972915649414
Batch 23/64 loss: -0.09714949131011963
Batch 24/64 loss: -0.10502701997756958
Batch 25/64 loss: -0.10171222686767578
Batch 26/64 loss: -0.09981822967529297
Batch 27/64 loss: -0.0941389799118042
Batch 28/64 loss: -0.09311646223068237
Batch 29/64 loss: -0.09341925382614136
Batch 30/64 loss: -0.09375858306884766
Batch 31/64 loss: -0.10572206974029541
Batch 32/64 loss: -0.09620320796966553
Batch 33/64 loss: -0.10109436511993408
Batch 34/64 loss: -0.09613418579101562
Batch 35/64 loss: -0.10010409355163574
Batch 36/64 loss: -0.09630197286605835
Batch 37/64 loss: -0.09130340814590454
Batch 38/64 loss: -0.10448789596557617
Batch 39/64 loss: -0.10417330265045166
Batch 40/64 loss: -0.08525186777114868
Batch 41/64 loss: -0.0968790054321289
Batch 42/64 loss: -0.09931713342666626
Batch 43/64 loss: -0.08707064390182495
Batch 44/64 loss: -0.10739248991012573
Batch 45/64 loss: -0.08124130964279175
Batch 46/64 loss: -0.10365676879882812
Batch 47/64 loss: -0.09938943386077881
Batch 48/64 loss: -0.10361063480377197
Batch 49/64 loss: -0.0950811505317688
Batch 50/64 loss: -0.08350992202758789
Batch 51/64 loss: -0.08543145656585693
Batch 52/64 loss: -0.08323556184768677
Batch 53/64 loss: -0.10280555486679077
Batch 54/64 loss: -0.09065532684326172
Batch 55/64 loss: -0.100771963596344
Batch 56/64 loss: -0.09942913055419922
Batch 57/64 loss: -0.08974051475524902
Batch 58/64 loss: -0.08762586116790771
Batch 59/64 loss: -0.09826803207397461
Batch 60/64 loss: -0.09793174266815186
Batch 61/64 loss: -0.11068207025527954
Batch 62/64 loss: -0.0956500768661499
Batch 63/64 loss: -0.09607148170471191
Batch 64/64 loss: -0.08891129493713379
Epoch 182  Train loss: -0.09761091681087718  Val loss: -0.041583263587296214
Epoch 183
-------------------------------
Batch 1/64 loss: -0.09173285961151123
Batch 2/64 loss: -0.11474037170410156
Batch 3/64 loss: -0.11281543970108032
Batch 4/64 loss: -0.09287089109420776
Batch 5/64 loss: -0.10540711879730225
Batch 6/64 loss: -0.10359269380569458
Batch 7/64 loss: -0.1097114086151123
Batch 8/64 loss: -0.10586029291152954
Batch 9/64 loss: -0.10022121667861938
Batch 10/64 loss: -0.11710548400878906
Batch 11/64 loss: -0.0871957540512085
Batch 12/64 loss: -0.11174362897872925
Batch 13/64 loss: -0.10340225696563721
Batch 14/64 loss: -0.10463100671768188
Batch 15/64 loss: -0.1029476523399353
Batch 16/64 loss: -0.11249798536300659
Batch 17/64 loss: -0.0775718092918396
Batch 18/64 loss: -0.09219413995742798
Batch 19/64 loss: -0.08405375480651855
Batch 20/64 loss: -0.08344447612762451
Batch 21/64 loss: -0.12130051851272583
Batch 22/64 loss: -0.09504687786102295
Batch 23/64 loss: -0.10433101654052734
Batch 24/64 loss: -0.1077432632446289
Batch 25/64 loss: -0.09780323505401611
Batch 26/64 loss: -0.0882488489151001
Batch 27/64 loss: -0.11198371648788452
Batch 28/64 loss: -0.10788428783416748
Batch 29/64 loss: -0.09406876564025879
Batch 30/64 loss: -0.11121124029159546
Batch 31/64 loss: -0.1034732460975647
Batch 32/64 loss: -0.09977161884307861
Batch 33/64 loss: -0.10367929935455322
Batch 34/64 loss: -0.0923992395401001
Batch 35/64 loss: -0.08782482147216797
Batch 36/64 loss: -0.10617899894714355
Batch 37/64 loss: -0.09257751703262329
Batch 38/64 loss: -0.10023576021194458
Batch 39/64 loss: -0.09706419706344604
Batch 40/64 loss: -0.10158693790435791
Batch 41/64 loss: -0.10648053884506226
Batch 42/64 loss: -0.09508079290390015
Batch 43/64 loss: -0.10061955451965332
Batch 44/64 loss: -0.09137016534805298
Batch 45/64 loss: -0.10940617322921753
Batch 46/64 loss: -0.10168129205703735
Batch 47/64 loss: -0.08672738075256348
Batch 48/64 loss: -0.09058219194412231
Batch 49/64 loss: -0.08777183294296265
Batch 50/64 loss: -0.0908849835395813
Batch 51/64 loss: -0.09142017364501953
Batch 52/64 loss: -0.11305451393127441
Batch 53/64 loss: -0.09756696224212646
Batch 54/64 loss: -0.08583486080169678
Batch 55/64 loss: -0.08465582132339478
Batch 56/64 loss: -0.08790278434753418
Batch 57/64 loss: -0.09385108947753906
Batch 58/64 loss: -0.09485852718353271
Batch 59/64 loss: -0.08803492784500122
Batch 60/64 loss: -0.09949541091918945
Batch 61/64 loss: -0.09059280157089233
Batch 62/64 loss: -0.08360636234283447
Batch 63/64 loss: -0.06524980068206787
Batch 64/64 loss: -0.09737014770507812
Epoch 183  Train loss: -0.09800643453411027  Val loss: -0.03935866503371406
Epoch 184
-------------------------------
Batch 1/64 loss: -0.0929681658744812
Batch 2/64 loss: -0.0882684588432312
Batch 3/64 loss: -0.10667496919631958
Batch 4/64 loss: -0.11194956302642822
Batch 5/64 loss: -0.10674989223480225
Batch 6/64 loss: -0.09367555379867554
Batch 7/64 loss: -0.11099565029144287
Batch 8/64 loss: -0.10011923313140869
Batch 9/64 loss: -0.11023801565170288
Batch 10/64 loss: -0.07095199823379517
Batch 11/64 loss: -0.1010701060295105
Batch 12/64 loss: -0.1080254316329956
Batch 13/64 loss: -0.08648771047592163
Batch 14/64 loss: -0.08531421422958374
Batch 15/64 loss: -0.10187768936157227
Batch 16/64 loss: -0.08695828914642334
Batch 17/64 loss: -0.11074274778366089
Batch 18/64 loss: -0.10535615682601929
Batch 19/64 loss: -0.1087375283241272
Batch 20/64 loss: -0.07444453239440918
Batch 21/64 loss: -0.10242503881454468
Batch 22/64 loss: -0.10176897048950195
Batch 23/64 loss: -0.10087299346923828
Batch 24/64 loss: -0.10591113567352295
Batch 25/64 loss: -0.10138672590255737
Batch 26/64 loss: -0.11224567890167236
Batch 27/64 loss: -0.09078407287597656
Batch 28/64 loss: -0.11556887626647949
Batch 29/64 loss: -0.09799134731292725
Batch 30/64 loss: -0.1095430850982666
Batch 31/64 loss: -0.0876474380493164
Batch 32/64 loss: -0.10106879472732544
Batch 33/64 loss: -0.09179443120956421
Batch 34/64 loss: -0.0769432783126831
Batch 35/64 loss: -0.11017030477523804
Batch 36/64 loss: -0.0941704511642456
Batch 37/64 loss: -0.08245992660522461
Batch 38/64 loss: -0.11552119255065918
Batch 39/64 loss: -0.09254753589630127
Batch 40/64 loss: -0.08910000324249268
Batch 41/64 loss: -0.09663504362106323
Batch 42/64 loss: -0.10916978120803833
Batch 43/64 loss: -0.10134518146514893
Batch 44/64 loss: -0.09465301036834717
Batch 45/64 loss: -0.10624384880065918
Batch 46/64 loss: -0.0923013687133789
Batch 47/64 loss: -0.06787550449371338
Batch 48/64 loss: -0.09098172187805176
Batch 49/64 loss: -0.06810629367828369
Batch 50/64 loss: -0.09007668495178223
Batch 51/64 loss: -0.09710681438446045
Batch 52/64 loss: -0.08935403823852539
Batch 53/64 loss: -0.0913439393043518
Batch 54/64 loss: -0.09909874200820923
Batch 55/64 loss: -0.09686297178268433
Batch 56/64 loss: -0.12072151899337769
Batch 57/64 loss: -0.09568870067596436
Batch 58/64 loss: -0.08813250064849854
Batch 59/64 loss: -0.0904768705368042
Batch 60/64 loss: -0.09957253932952881
Batch 61/64 loss: -0.08251446485519409
Batch 62/64 loss: -0.06401562690734863
Batch 63/64 loss: -0.09287971258163452
Batch 64/64 loss: -0.11790657043457031
Epoch 184  Train loss: -0.0965508077658859  Val loss: -0.04571140590811923
Saving best model, epoch: 184
Epoch 185
-------------------------------
Batch 1/64 loss: -0.10988384485244751
Batch 2/64 loss: -0.09757936000823975
Batch 3/64 loss: -0.11136412620544434
Batch 4/64 loss: -0.10820168256759644
Batch 5/64 loss: -0.10074341297149658
Batch 6/64 loss: -0.10199344158172607
Batch 7/64 loss: -0.10129803419113159
Batch 8/64 loss: -0.08305269479751587
Batch 9/64 loss: -0.08221191167831421
Batch 10/64 loss: -0.10541456937789917
Batch 11/64 loss: -0.10697364807128906
Batch 12/64 loss: -0.10487008094787598
Batch 13/64 loss: -0.09481531381607056
Batch 14/64 loss: -0.08667612075805664
Batch 15/64 loss: -0.10131192207336426
Batch 16/64 loss: -0.09070968627929688
Batch 17/64 loss: -0.11420643329620361
Batch 18/64 loss: -0.12110739946365356
Batch 19/64 loss: -0.10524201393127441
Batch 20/64 loss: -0.09856569766998291
Batch 21/64 loss: -0.10070610046386719
Batch 22/64 loss: -0.10909366607666016
Batch 23/64 loss: -0.1085696816444397
Batch 24/64 loss: -0.11919331550598145
Batch 25/64 loss: -0.09819769859313965
Batch 26/64 loss: -0.11210507154464722
Batch 27/64 loss: -0.10329955816268921
Batch 28/64 loss: -0.09683161973953247
Batch 29/64 loss: -0.08407628536224365
Batch 30/64 loss: -0.10070747137069702
Batch 31/64 loss: -0.11034011840820312
Batch 32/64 loss: -0.08804154396057129
Batch 33/64 loss: -0.09972351789474487
Batch 34/64 loss: -0.10410451889038086
Batch 35/64 loss: -0.10175567865371704
Batch 36/64 loss: -0.09151244163513184
Batch 37/64 loss: -0.10149872303009033
Batch 38/64 loss: -0.09171479940414429
Batch 39/64 loss: -0.09670501947402954
Batch 40/64 loss: -0.10272306203842163
Batch 41/64 loss: -0.1203070878982544
Batch 42/64 loss: -0.11311060190200806
Batch 43/64 loss: -0.10924208164215088
Batch 44/64 loss: -0.10939979553222656
Batch 45/64 loss: -0.0850096344947815
Batch 46/64 loss: -0.09953749179840088
Batch 47/64 loss: -0.07843899726867676
Batch 48/64 loss: -0.09334027767181396
Batch 49/64 loss: -0.079490065574646
Batch 50/64 loss: -0.11528950929641724
Batch 51/64 loss: -0.0906115174293518
Batch 52/64 loss: -0.09891867637634277
Batch 53/64 loss: -0.10424602031707764
Batch 54/64 loss: -0.08458054065704346
Batch 55/64 loss: -0.11206960678100586
Batch 56/64 loss: -0.07566219568252563
Batch 57/64 loss: -0.08777344226837158
Batch 58/64 loss: -0.09207767248153687
Batch 59/64 loss: -0.0816047191619873
Batch 60/64 loss: -0.09426403045654297
Batch 61/64 loss: -0.09493756294250488
Batch 62/64 loss: -0.10999566316604614
Batch 63/64 loss: -0.09236103296279907
Batch 64/64 loss: -0.0721248984336853
Epoch 185  Train loss: -0.09919189308203903  Val loss: -0.04270790532692192
Epoch 186
-------------------------------
Batch 1/64 loss: -0.10323739051818848
Batch 2/64 loss: -0.09245836734771729
Batch 3/64 loss: -0.11011815071105957
Batch 4/64 loss: -0.09863203763961792
Batch 5/64 loss: -0.10852718353271484
Batch 6/64 loss: -0.10033237934112549
Batch 7/64 loss: -0.09715151786804199
Batch 8/64 loss: -0.10981786251068115
Batch 9/64 loss: -0.11452621221542358
Batch 10/64 loss: -0.10209852457046509
Batch 11/64 loss: -0.11195719242095947
Batch 12/64 loss: -0.11109238862991333
Batch 13/64 loss: -0.08329558372497559
Batch 14/64 loss: -0.0948343276977539
Batch 15/64 loss: -0.09448599815368652
Batch 16/64 loss: -0.0856783390045166
Batch 17/64 loss: -0.09060025215148926
Batch 18/64 loss: -0.10278332233428955
Batch 19/64 loss: -0.11792594194412231
Batch 20/64 loss: -0.08857762813568115
Batch 21/64 loss: -0.09745144844055176
Batch 22/64 loss: -0.08211499452590942
Batch 23/64 loss: -0.10101461410522461
Batch 24/64 loss: -0.0974283218383789
Batch 25/64 loss: -0.09640246629714966
Batch 26/64 loss: -0.07729655504226685
Batch 27/64 loss: -0.08766680955886841
Batch 28/64 loss: -0.09971165657043457
Batch 29/64 loss: -0.10756385326385498
Batch 30/64 loss: -0.1066097617149353
Batch 31/64 loss: -0.10109645128250122
Batch 32/64 loss: -0.10083305835723877
Batch 33/64 loss: -0.09180217981338501
Batch 34/64 loss: -0.09384232759475708
Batch 35/64 loss: -0.08989489078521729
Batch 36/64 loss: -0.10989594459533691
Batch 37/64 loss: -0.11847078800201416
Batch 38/64 loss: -0.06349438428878784
Batch 39/64 loss: -0.10049200057983398
Batch 40/64 loss: -0.10455811023712158
Batch 41/64 loss: -0.08095628023147583
Batch 42/64 loss: -0.10721093416213989
Batch 43/64 loss: -0.11129641532897949
Batch 44/64 loss: -0.10162973403930664
Batch 45/64 loss: -0.10659527778625488
Batch 46/64 loss: -0.11121368408203125
Batch 47/64 loss: -0.07908308506011963
Batch 48/64 loss: -0.09309667348861694
Batch 49/64 loss: -0.11610400676727295
Batch 50/64 loss: -0.09202760457992554
Batch 51/64 loss: -0.10387414693832397
Batch 52/64 loss: -0.11049723625183105
Batch 53/64 loss: -0.09932267665863037
Batch 54/64 loss: -0.10024797916412354
Batch 55/64 loss: -0.1003885269165039
Batch 56/64 loss: -0.11168444156646729
Batch 57/64 loss: -0.08566468954086304
Batch 58/64 loss: -0.1015821099281311
Batch 59/64 loss: -0.0752916932106018
Batch 60/64 loss: -0.0821027159690857
Batch 61/64 loss: -0.07674801349639893
Batch 62/64 loss: -0.07710593938827515
Batch 63/64 loss: -0.0693972110748291
Batch 64/64 loss: -0.08797717094421387
Epoch 186  Train loss: -0.0973313752342673  Val loss: -0.039592212827754596
Epoch 187
-------------------------------
Batch 1/64 loss: -0.09352374076843262
Batch 2/64 loss: -0.0966460108757019
Batch 3/64 loss: -0.09274423122406006
Batch 4/64 loss: -0.08428514003753662
Batch 5/64 loss: -0.07859891653060913
Batch 6/64 loss: -0.09138083457946777
Batch 7/64 loss: -0.09641122817993164
Batch 8/64 loss: -0.10157394409179688
Batch 9/64 loss: -0.09561920166015625
Batch 10/64 loss: -0.08912003040313721
Batch 11/64 loss: -0.09658926725387573
Batch 12/64 loss: -0.09631550312042236
Batch 13/64 loss: -0.10045146942138672
Batch 14/64 loss: -0.10295355319976807
Batch 15/64 loss: -0.09927958250045776
Batch 16/64 loss: -0.10242462158203125
Batch 17/64 loss: -0.08429861068725586
Batch 18/64 loss: -0.10959583520889282
Batch 19/64 loss: -0.10683202743530273
Batch 20/64 loss: -0.1124160885810852
Batch 21/64 loss: -0.10017377138137817
Batch 22/64 loss: -0.10659939050674438
Batch 23/64 loss: -0.10398250818252563
Batch 24/64 loss: -0.09599459171295166
Batch 25/64 loss: -0.08713054656982422
Batch 26/64 loss: -0.09511494636535645
Batch 27/64 loss: -0.09591615200042725
Batch 28/64 loss: -0.10760855674743652
Batch 29/64 loss: -0.10811948776245117
Batch 30/64 loss: -0.11055666208267212
Batch 31/64 loss: -0.10495072603225708
Batch 32/64 loss: -0.11732900142669678
Batch 33/64 loss: -0.10799521207809448
Batch 34/64 loss: -0.10864686965942383
Batch 35/64 loss: -0.1021614670753479
Batch 36/64 loss: -0.10396194458007812
Batch 37/64 loss: -0.08901447057723999
Batch 38/64 loss: -0.07827991247177124
Batch 39/64 loss: -0.06761324405670166
Batch 40/64 loss: -0.08315420150756836
Batch 41/64 loss: -0.0943191647529602
Batch 42/64 loss: -0.07494616508483887
Batch 43/64 loss: -0.10988056659698486
Batch 44/64 loss: -0.11037802696228027
Batch 45/64 loss: -0.10844230651855469
Batch 46/64 loss: -0.08517086505889893
Batch 47/64 loss: -0.0922234058380127
Batch 48/64 loss: -0.10814177989959717
Batch 49/64 loss: -0.09184384346008301
Batch 50/64 loss: -0.08529436588287354
Batch 51/64 loss: -0.10863304138183594
Batch 52/64 loss: -0.09801054000854492
Batch 53/64 loss: -0.10655021667480469
Batch 54/64 loss: -0.09190529584884644
Batch 55/64 loss: -0.09494119882583618
Batch 56/64 loss: -0.09184098243713379
Batch 57/64 loss: -0.11006605625152588
Batch 58/64 loss: -0.11026614904403687
Batch 59/64 loss: -0.10995960235595703
Batch 60/64 loss: -0.10642647743225098
Batch 61/64 loss: -0.11933934688568115
Batch 62/64 loss: -0.05695462226867676
Batch 63/64 loss: -0.08469575643539429
Batch 64/64 loss: -0.12382608652114868
Epoch 187  Train loss: -0.09801510338689767  Val loss: -0.04516942271661922
Epoch 188
-------------------------------
Batch 1/64 loss: -0.09377288818359375
Batch 2/64 loss: -0.1087833046913147
Batch 3/64 loss: -0.10353624820709229
Batch 4/64 loss: -0.10804462432861328
Batch 5/64 loss: -0.11069643497467041
Batch 6/64 loss: -0.09435075521469116
Batch 7/64 loss: -0.10169970989227295
Batch 8/64 loss: -0.0977211594581604
Batch 9/64 loss: -0.10665911436080933
Batch 10/64 loss: -0.11136531829833984
Batch 11/64 loss: -0.10043048858642578
Batch 12/64 loss: -0.10236877202987671
Batch 13/64 loss: -0.11751067638397217
Batch 14/64 loss: -0.09761571884155273
Batch 15/64 loss: -0.0973859429359436
Batch 16/64 loss: -0.11250478029251099
Batch 17/64 loss: -0.09276854991912842
Batch 18/64 loss: -0.09722232818603516
Batch 19/64 loss: -0.08850032091140747
Batch 20/64 loss: -0.06784623861312866
Batch 21/64 loss: -0.09862077236175537
Batch 22/64 loss: -0.08770668506622314
Batch 23/64 loss: -0.10014897584915161
Batch 24/64 loss: -0.10859459638595581
Batch 25/64 loss: -0.10568398237228394
Batch 26/64 loss: -0.09745991230010986
Batch 27/64 loss: -0.10246777534484863
Batch 28/64 loss: -0.07494831085205078
Batch 29/64 loss: -0.10059750080108643
Batch 30/64 loss: -0.09071838855743408
Batch 31/64 loss: -0.08988779783248901
Batch 32/64 loss: -0.08398115634918213
Batch 33/64 loss: -0.09090030193328857
Batch 34/64 loss: -0.09194058179855347
Batch 35/64 loss: -0.10942023992538452
Batch 36/64 loss: -0.10003834962844849
Batch 37/64 loss: -0.10255634784698486
Batch 38/64 loss: -0.10646486282348633
Batch 39/64 loss: -0.08860945701599121
Batch 40/64 loss: -0.10358315706253052
Batch 41/64 loss: -0.10936182737350464
Batch 42/64 loss: -0.0985175371170044
Batch 43/64 loss: -0.1018824577331543
Batch 44/64 loss: -0.10448139905929565
Batch 45/64 loss: -0.1045178771018982
Batch 46/64 loss: -0.10848641395568848
Batch 47/64 loss: -0.08895695209503174
Batch 48/64 loss: -0.10702234506607056
Batch 49/64 loss: -0.0933694839477539
Batch 50/64 loss: -0.09241223335266113
Batch 51/64 loss: -0.10536569356918335
Batch 52/64 loss: -0.11016321182250977
Batch 53/64 loss: -0.12302619218826294
Batch 54/64 loss: -0.10168516635894775
Batch 55/64 loss: -0.09795737266540527
Batch 56/64 loss: -0.10739755630493164
Batch 57/64 loss: -0.11374884843826294
Batch 58/64 loss: -0.10196417570114136
Batch 59/64 loss: -0.08593577146530151
Batch 60/64 loss: -0.10076022148132324
Batch 61/64 loss: -0.09547382593154907
Batch 62/64 loss: -0.08200186491012573
Batch 63/64 loss: -0.10425126552581787
Batch 64/64 loss: -0.09986311197280884
Epoch 188  Train loss: -0.09974509103625429  Val loss: -0.041561083695323194
Epoch 189
-------------------------------
Batch 1/64 loss: -0.08926272392272949
Batch 2/64 loss: -0.1181255578994751
Batch 3/64 loss: -0.0977553129196167
Batch 4/64 loss: -0.10988801717758179
Batch 5/64 loss: -0.09669047594070435
Batch 6/64 loss: -0.10408967733383179
Batch 7/64 loss: -0.1096879243850708
Batch 8/64 loss: -0.10025680065155029
Batch 9/64 loss: -0.10847479104995728
Batch 10/64 loss: -0.10971367359161377
Batch 11/64 loss: -0.07808339595794678
Batch 12/64 loss: -0.10432416200637817
Batch 13/64 loss: -0.11736297607421875
Batch 14/64 loss: -0.08552294969558716
Batch 15/64 loss: -0.10634297132492065
Batch 16/64 loss: -0.10480797290802002
Batch 17/64 loss: -0.1047711968421936
Batch 18/64 loss: -0.09867268800735474
Batch 19/64 loss: -0.10952603816986084
Batch 20/64 loss: -0.10724008083343506
Batch 21/64 loss: -0.10957252979278564
Batch 22/64 loss: -0.11824953556060791
Batch 23/64 loss: -0.09436577558517456
Batch 24/64 loss: -0.0975337028503418
Batch 25/64 loss: -0.11090999841690063
Batch 26/64 loss: -0.09696924686431885
Batch 27/64 loss: -0.10645836591720581
Batch 28/64 loss: -0.09257113933563232
Batch 29/64 loss: -0.09598422050476074
Batch 30/64 loss: -0.10452687740325928
Batch 31/64 loss: -0.1139107346534729
Batch 32/64 loss: -0.09939217567443848
Batch 33/64 loss: -0.09660691022872925
Batch 34/64 loss: -0.09015494585037231
Batch 35/64 loss: -0.10206598043441772
Batch 36/64 loss: -0.08940058946609497
Batch 37/64 loss: -0.10921180248260498
Batch 38/64 loss: -0.07437556982040405
Batch 39/64 loss: -0.10759657621383667
Batch 40/64 loss: -0.1132650375366211
Batch 41/64 loss: -0.09158098697662354
Batch 42/64 loss: -0.1041642427444458
Batch 43/64 loss: -0.10515731573104858
Batch 44/64 loss: -0.10049688816070557
Batch 45/64 loss: -0.11043858528137207
Batch 46/64 loss: -0.09047871828079224
Batch 47/64 loss: -0.11321389675140381
Batch 48/64 loss: -0.09850525856018066
Batch 49/64 loss: -0.10111099481582642
Batch 50/64 loss: -0.10027116537094116
Batch 51/64 loss: -0.10677647590637207
Batch 52/64 loss: -0.09091758728027344
Batch 53/64 loss: -0.09833604097366333
Batch 54/64 loss: -0.09384548664093018
Batch 55/64 loss: -0.10977393388748169
Batch 56/64 loss: -0.11012822389602661
Batch 57/64 loss: -0.08180844783782959
Batch 58/64 loss: -0.11303633451461792
Batch 59/64 loss: -0.09239184856414795
Batch 60/64 loss: -0.09141397476196289
Batch 61/64 loss: -0.10582661628723145
Batch 62/64 loss: -0.08392482995986938
Batch 63/64 loss: -0.10125398635864258
Batch 64/64 loss: -0.0846821665763855
Epoch 189  Train loss: -0.10105230691386204  Val loss: -0.04386209560833436
Epoch 190
-------------------------------
Batch 1/64 loss: -0.10303223133087158
Batch 2/64 loss: -0.10241430997848511
Batch 3/64 loss: -0.09846711158752441
Batch 4/64 loss: -0.10694664716720581
Batch 5/64 loss: -0.10409033298492432
Batch 6/64 loss: -0.1037071943283081
Batch 7/64 loss: -0.10854339599609375
Batch 8/64 loss: -0.11299550533294678
Batch 9/64 loss: -0.11619424819946289
Batch 10/64 loss: -0.09843659400939941
Batch 11/64 loss: -0.10744154453277588
Batch 12/64 loss: -0.10240292549133301
Batch 13/64 loss: -0.10509622097015381
Batch 14/64 loss: -0.11169934272766113
Batch 15/64 loss: -0.11212199926376343
Batch 16/64 loss: -0.09778296947479248
Batch 17/64 loss: -0.11442667245864868
Batch 18/64 loss: -0.09962183237075806
Batch 19/64 loss: -0.09925830364227295
Batch 20/64 loss: -0.09668868780136108
Batch 21/64 loss: -0.11503469944000244
Batch 22/64 loss: -0.11507725715637207
Batch 23/64 loss: -0.09957993030548096
Batch 24/64 loss: -0.08890700340270996
Batch 25/64 loss: -0.11507183313369751
Batch 26/64 loss: -0.10527670383453369
Batch 27/64 loss: -0.08987736701965332
Batch 28/64 loss: -0.09749656915664673
Batch 29/64 loss: -0.10132920742034912
Batch 30/64 loss: -0.10056424140930176
Batch 31/64 loss: -0.08845973014831543
Batch 32/64 loss: -0.11819851398468018
Batch 33/64 loss: -0.08141624927520752
Batch 34/64 loss: -0.10259455442428589
Batch 35/64 loss: -0.09024953842163086
Batch 36/64 loss: -0.10370421409606934
Batch 37/64 loss: -0.10586810111999512
Batch 38/64 loss: -0.1054723858833313
Batch 39/64 loss: -0.09281468391418457
Batch 40/64 loss: -0.06548476219177246
Batch 41/64 loss: -0.10849052667617798
Batch 42/64 loss: -0.10394614934921265
Batch 43/64 loss: -0.1020154356956482
Batch 44/64 loss: -0.10652989149093628
Batch 45/64 loss: -0.11517417430877686
Batch 46/64 loss: -0.0843459963798523
Batch 47/64 loss: -0.10540556907653809
Batch 48/64 loss: -0.10755395889282227
Batch 49/64 loss: -0.09469282627105713
Batch 50/64 loss: -0.10007774829864502
Batch 51/64 loss: -0.08312380313873291
Batch 52/64 loss: -0.10227644443511963
Batch 53/64 loss: -0.10316860675811768
Batch 54/64 loss: -0.08179682493209839
Batch 55/64 loss: -0.09771817922592163
Batch 56/64 loss: -0.09804600477218628
Batch 57/64 loss: -0.08283126354217529
Batch 58/64 loss: -0.08849996328353882
Batch 59/64 loss: -0.0954427719116211
Batch 60/64 loss: -0.11295008659362793
Batch 61/64 loss: -0.10486483573913574
Batch 62/64 loss: -0.0878603458404541
Batch 63/64 loss: -0.10515350103378296
Batch 64/64 loss: -0.08786612749099731
Epoch 190  Train loss: -0.10073270774355121  Val loss: -0.04026717316244066
Epoch 191
-------------------------------
Batch 1/64 loss: -0.10407757759094238
Batch 2/64 loss: -0.11553370952606201
Batch 3/64 loss: -0.11244869232177734
Batch 4/64 loss: -0.11427128314971924
Batch 5/64 loss: -0.09679675102233887
Batch 6/64 loss: -0.09045463800430298
Batch 7/64 loss: -0.10459387302398682
Batch 8/64 loss: -0.11324864625930786
Batch 9/64 loss: -0.11256808042526245
Batch 10/64 loss: -0.10526371002197266
Batch 11/64 loss: -0.07840800285339355
Batch 12/64 loss: -0.09006243944168091
Batch 13/64 loss: -0.12583625316619873
Batch 14/64 loss: -0.10333645343780518
Batch 15/64 loss: -0.11376523971557617
Batch 16/64 loss: -0.08566069602966309
Batch 17/64 loss: -0.11042672395706177
Batch 18/64 loss: -0.11772686243057251
Batch 19/64 loss: -0.0971725583076477
Batch 20/64 loss: -0.1167747974395752
Batch 21/64 loss: -0.10662388801574707
Batch 22/64 loss: -0.09157782793045044
Batch 23/64 loss: -0.09974473714828491
Batch 24/64 loss: -0.08272194862365723
Batch 25/64 loss: -0.08753114938735962
Batch 26/64 loss: -0.10486364364624023
Batch 27/64 loss: -0.08420908451080322
Batch 28/64 loss: -0.11595690250396729
Batch 29/64 loss: -0.10784167051315308
Batch 30/64 loss: -0.11212509870529175
Batch 31/64 loss: -0.09099751710891724
Batch 32/64 loss: -0.10169059038162231
Batch 33/64 loss: -0.11018550395965576
Batch 34/64 loss: -0.11381274461746216
Batch 35/64 loss: -0.10193920135498047
Batch 36/64 loss: -0.08991813659667969
Batch 37/64 loss: -0.09995728731155396
Batch 38/64 loss: -0.10335850715637207
Batch 39/64 loss: -0.08955907821655273
Batch 40/64 loss: -0.08777886629104614
Batch 41/64 loss: -0.10904812812805176
Batch 42/64 loss: -0.1100127100944519
Batch 43/64 loss: -0.09472924470901489
Batch 44/64 loss: -0.10194694995880127
Batch 45/64 loss: -0.09630829095840454
Batch 46/64 loss: -0.1015729308128357
Batch 47/64 loss: -0.11027467250823975
Batch 48/64 loss: -0.09403610229492188
Batch 49/64 loss: -0.0788353681564331
Batch 50/64 loss: -0.10166686773300171
Batch 51/64 loss: -0.115486741065979
Batch 52/64 loss: -0.11047202348709106
Batch 53/64 loss: -0.11348968744277954
Batch 54/64 loss: -0.11703962087631226
Batch 55/64 loss: -0.09951037168502808
Batch 56/64 loss: -0.08443707227706909
Batch 57/64 loss: -0.10504496097564697
Batch 58/64 loss: -0.1082267165184021
Batch 59/64 loss: -0.07018643617630005
Batch 60/64 loss: -0.09800994396209717
Batch 61/64 loss: -0.09262281656265259
Batch 62/64 loss: -0.09272098541259766
Batch 63/64 loss: -0.11088484525680542
Batch 64/64 loss: -0.0916285514831543
Epoch 191  Train loss: -0.1015859650630577  Val loss: -0.04293310519346257
Epoch 192
-------------------------------
Batch 1/64 loss: -0.1039055585861206
Batch 2/64 loss: -0.09119689464569092
Batch 3/64 loss: -0.08571720123291016
Batch 4/64 loss: -0.10973763465881348
Batch 5/64 loss: -0.09392529726028442
Batch 6/64 loss: -0.1251617670059204
Batch 7/64 loss: -0.09701001644134521
Batch 8/64 loss: -0.12479782104492188
Batch 9/64 loss: -0.10728073120117188
Batch 10/64 loss: -0.07462096214294434
Batch 11/64 loss: -0.12103736400604248
Batch 12/64 loss: -0.105904221534729
Batch 13/64 loss: -0.11501741409301758
Batch 14/64 loss: -0.10860168933868408
Batch 15/64 loss: -0.10473865270614624
Batch 16/64 loss: -0.1123695969581604
Batch 17/64 loss: -0.12477242946624756
Batch 18/64 loss: -0.11195552349090576
Batch 19/64 loss: -0.09614741802215576
Batch 20/64 loss: -0.1170077919960022
Batch 21/64 loss: -0.08657717704772949
Batch 22/64 loss: -0.10002917051315308
Batch 23/64 loss: -0.0917520523071289
Batch 24/64 loss: -0.09638261795043945
Batch 25/64 loss: -0.09846663475036621
Batch 26/64 loss: -0.11733603477478027
Batch 27/64 loss: -0.09170347452163696
Batch 28/64 loss: -0.09232443571090698
Batch 29/64 loss: -0.10471558570861816
Batch 30/64 loss: -0.090354323387146
Batch 31/64 loss: -0.1055067777633667
Batch 32/64 loss: -0.10280609130859375
Batch 33/64 loss: -0.09804606437683105
Batch 34/64 loss: -0.10997903347015381
Batch 35/64 loss: -0.09328222274780273
Batch 36/64 loss: -0.0990632176399231
Batch 37/64 loss: -0.0970989465713501
Batch 38/64 loss: -0.0948643684387207
Batch 39/64 loss: -0.08589363098144531
Batch 40/64 loss: -0.09091901779174805
Batch 41/64 loss: -0.10386037826538086
Batch 42/64 loss: -0.10533541440963745
Batch 43/64 loss: -0.0962103009223938
Batch 44/64 loss: -0.10914313793182373
Batch 45/64 loss: -0.11065727472305298
Batch 46/64 loss: -0.10244417190551758
Batch 47/64 loss: -0.11232495307922363
Batch 48/64 loss: -0.13222450017929077
Batch 49/64 loss: -0.10056138038635254
Batch 50/64 loss: -0.09029597043991089
Batch 51/64 loss: -0.09883886575698853
Batch 52/64 loss: -0.09684944152832031
Batch 53/64 loss: -0.10375732183456421
Batch 54/64 loss: -0.0888625979423523
Batch 55/64 loss: -0.11352211236953735
Batch 56/64 loss: -0.11050939559936523
Batch 57/64 loss: -0.089851975440979
Batch 58/64 loss: -0.1051129698753357
Batch 59/64 loss: -0.1069454550743103
Batch 60/64 loss: -0.10075289011001587
Batch 61/64 loss: -0.10734403133392334
Batch 62/64 loss: -0.1072649359703064
Batch 63/64 loss: -0.08979475498199463
Batch 64/64 loss: -0.07092785835266113
Epoch 192  Train loss: -0.10217516843010398  Val loss: -0.04379258532704357
Epoch 193
-------------------------------
Batch 1/64 loss: -0.10635501146316528
Batch 2/64 loss: -0.10053908824920654
Batch 3/64 loss: -0.09606325626373291
Batch 4/64 loss: -0.09788835048675537
Batch 5/64 loss: -0.11470335721969604
Batch 6/64 loss: -0.09962570667266846
Batch 7/64 loss: -0.11074823141098022
Batch 8/64 loss: -0.08834165334701538
Batch 9/64 loss: -0.11325132846832275
Batch 10/64 loss: -0.08894228935241699
Batch 11/64 loss: -0.09797704219818115
Batch 12/64 loss: -0.10680663585662842
Batch 13/64 loss: -0.10688745975494385
Batch 14/64 loss: -0.08384323120117188
Batch 15/64 loss: -0.08336210250854492
Batch 16/64 loss: -0.08927738666534424
Batch 17/64 loss: -0.12049061059951782
Batch 18/64 loss: -0.10221314430236816
Batch 19/64 loss: -0.07915890216827393
Batch 20/64 loss: -0.11230254173278809
Batch 21/64 loss: -0.10594826936721802
Batch 22/64 loss: -0.1088218092918396
Batch 23/64 loss: -0.07822197675704956
Batch 24/64 loss: -0.09223371744155884
Batch 25/64 loss: -0.08431774377822876
Batch 26/64 loss: -0.10352212190628052
Batch 27/64 loss: -0.11356562376022339
Batch 28/64 loss: -0.11309921741485596
Batch 29/64 loss: -0.09065377712249756
Batch 30/64 loss: -0.10095149278640747
Batch 31/64 loss: -0.10176533460617065
Batch 32/64 loss: -0.11162501573562622
Batch 33/64 loss: -0.10175013542175293
Batch 34/64 loss: -0.11784940958023071
Batch 35/64 loss: -0.10337543487548828
Batch 36/64 loss: -0.10405850410461426
Batch 37/64 loss: -0.09764420986175537
Batch 38/64 loss: -0.10882449150085449
Batch 39/64 loss: -0.1141698956489563
Batch 40/64 loss: -0.10452306270599365
Batch 41/64 loss: -0.1246843934059143
Batch 42/64 loss: -0.10573530197143555
Batch 43/64 loss: -0.10537755489349365
Batch 44/64 loss: -0.10057592391967773
Batch 45/64 loss: -0.10351049900054932
Batch 46/64 loss: -0.11486130952835083
Batch 47/64 loss: -0.08388662338256836
Batch 48/64 loss: -0.12771177291870117
Batch 49/64 loss: -0.11099076271057129
Batch 50/64 loss: -0.10879403352737427
Batch 51/64 loss: -0.1170274019241333
Batch 52/64 loss: -0.10804450511932373
Batch 53/64 loss: -0.11415612697601318
Batch 54/64 loss: -0.09833502769470215
Batch 55/64 loss: -0.10219967365264893
Batch 56/64 loss: -0.11171752214431763
Batch 57/64 loss: -0.09937494993209839
Batch 58/64 loss: -0.07590627670288086
Batch 59/64 loss: -0.0958714485168457
Batch 60/64 loss: -0.10422718524932861
Batch 61/64 loss: -0.09654104709625244
Batch 62/64 loss: -0.10865819454193115
Batch 63/64 loss: -0.11374539136886597
Batch 64/64 loss: -0.10542970895767212
Epoch 193  Train loss: -0.10300711814094993  Val loss: -0.044129793586599866
Epoch 194
-------------------------------
Batch 1/64 loss: -0.10272455215454102
Batch 2/64 loss: -0.12776970863342285
Batch 3/64 loss: -0.11903631687164307
Batch 4/64 loss: -0.10933578014373779
Batch 5/64 loss: -0.11360204219818115
Batch 6/64 loss: -0.10518753528594971
Batch 7/64 loss: -0.1262568235397339
Batch 8/64 loss: -0.10155051946640015
Batch 9/64 loss: -0.10819041728973389
Batch 10/64 loss: -0.08214133977890015
Batch 11/64 loss: -0.09355980157852173
Batch 12/64 loss: -0.10846680402755737
Batch 13/64 loss: -0.11055862903594971
Batch 14/64 loss: -0.09755271673202515
Batch 15/64 loss: -0.09981787204742432
Batch 16/64 loss: -0.11407768726348877
Batch 17/64 loss: -0.07691705226898193
Batch 18/64 loss: -0.10464513301849365
Batch 19/64 loss: -0.0852741003036499
Batch 20/64 loss: -0.11297738552093506
Batch 21/64 loss: -0.07847148180007935
Batch 22/64 loss: -0.10049641132354736
Batch 23/64 loss: -0.09631496667861938
Batch 24/64 loss: -0.09901803731918335
Batch 25/64 loss: -0.1131594181060791
Batch 26/64 loss: -0.0969846248626709
Batch 27/64 loss: -0.10231649875640869
Batch 28/64 loss: -0.10591703653335571
Batch 29/64 loss: -0.10495328903198242
Batch 30/64 loss: -0.11071121692657471
Batch 31/64 loss: -0.09324878454208374
Batch 32/64 loss: -0.10533589124679565
Batch 33/64 loss: -0.10339558124542236
Batch 34/64 loss: -0.07283133268356323
Batch 35/64 loss: -0.10079628229141235
Batch 36/64 loss: -0.11079013347625732
Batch 37/64 loss: -0.1197819709777832
Batch 38/64 loss: -0.1141272783279419
Batch 39/64 loss: -0.10957354307174683
Batch 40/64 loss: -0.09452009201049805
Batch 41/64 loss: -0.09020859003067017
Batch 42/64 loss: -0.09900772571563721
Batch 43/64 loss: -0.10450345277786255
Batch 44/64 loss: -0.09744179248809814
Batch 45/64 loss: -0.0990365743637085
Batch 46/64 loss: -0.10273700952529907
Batch 47/64 loss: -0.09854936599731445
Batch 48/64 loss: -0.08662986755371094
Batch 49/64 loss: -0.10349661111831665
Batch 50/64 loss: -0.11433613300323486
Batch 51/64 loss: -0.0913991928100586
Batch 52/64 loss: -0.09075927734375
Batch 53/64 loss: -0.09841996431350708
Batch 54/64 loss: -0.0975235104560852
Batch 55/64 loss: -0.10613369941711426
Batch 56/64 loss: -0.11768805980682373
Batch 57/64 loss: -0.11751645803451538
Batch 58/64 loss: -0.1077277660369873
Batch 59/64 loss: -0.11163806915283203
Batch 60/64 loss: -0.11142909526824951
Batch 61/64 loss: -0.1091001033782959
Batch 62/64 loss: -0.11878174543380737
Batch 63/64 loss: -0.07416343688964844
Batch 64/64 loss: -0.11128091812133789
Epoch 194  Train loss: -0.10296587102553424  Val loss: -0.04228312739801571
Epoch 195
-------------------------------
Batch 1/64 loss: -0.11492395401000977
Batch 2/64 loss: -0.11094093322753906
Batch 3/64 loss: -0.10699689388275146
Batch 4/64 loss: -0.11373651027679443
Batch 5/64 loss: -0.11203604936599731
Batch 6/64 loss: -0.10483473539352417
Batch 7/64 loss: -0.10568118095397949
Batch 8/64 loss: -0.10233676433563232
Batch 9/64 loss: -0.07996779680252075
Batch 10/64 loss: -0.11406815052032471
Batch 11/64 loss: -0.10537689924240112
Batch 12/64 loss: -0.10476595163345337
Batch 13/64 loss: -0.11516964435577393
Batch 14/64 loss: -0.10912233591079712
Batch 15/64 loss: -0.11800730228424072
Batch 16/64 loss: -0.11506044864654541
Batch 17/64 loss: -0.10692518949508667
Batch 18/64 loss: -0.09753191471099854
Batch 19/64 loss: -0.10959577560424805
Batch 20/64 loss: -0.12497854232788086
Batch 21/64 loss: -0.10672295093536377
Batch 22/64 loss: -0.1019393801689148
Batch 23/64 loss: -0.12365204095840454
Batch 24/64 loss: -0.11698853969573975
Batch 25/64 loss: -0.10163551568984985
Batch 26/64 loss: -0.09937655925750732
Batch 27/64 loss: -0.10548931360244751
Batch 28/64 loss: -0.10897523164749146
Batch 29/64 loss: -0.11624652147293091
Batch 30/64 loss: -0.09129375219345093
Batch 31/64 loss: -0.1073765754699707
Batch 32/64 loss: -0.10605895519256592
Batch 33/64 loss: -0.10001611709594727
Batch 34/64 loss: -0.10971987247467041
Batch 35/64 loss: -0.12213915586471558
Batch 36/64 loss: -0.10055088996887207
Batch 37/64 loss: -0.1082887053489685
Batch 38/64 loss: -0.08938497304916382
Batch 39/64 loss: -0.10045534372329712
Batch 40/64 loss: -0.08963137865066528
Batch 41/64 loss: -0.1090996265411377
Batch 42/64 loss: -0.09752237796783447
Batch 43/64 loss: -0.10940742492675781
Batch 44/64 loss: -0.11428827047348022
Batch 45/64 loss: -0.0910722017288208
Batch 46/64 loss: -0.09328806400299072
Batch 47/64 loss: -0.11743438243865967
Batch 48/64 loss: -0.11572104692459106
Batch 49/64 loss: -0.08885711431503296
Batch 50/64 loss: -0.10810136795043945
Batch 51/64 loss: -0.09490537643432617
Batch 52/64 loss: -0.10040587186813354
Batch 53/64 loss: -0.11294955015182495
Batch 54/64 loss: -0.08794832229614258
Batch 55/64 loss: -0.1011461615562439
Batch 56/64 loss: -0.11525589227676392
Batch 57/64 loss: -0.10009032487869263
Batch 58/64 loss: -0.08461999893188477
Batch 59/64 loss: -0.09441983699798584
Batch 60/64 loss: -0.09177190065383911
Batch 61/64 loss: -0.11817646026611328
Batch 62/64 loss: -0.11203569173812866
Batch 63/64 loss: -0.09927117824554443
Batch 64/64 loss: -0.12819194793701172
Epoch 195  Train loss: -0.10559891999936571  Val loss: -0.041955437037543335
Epoch 196
-------------------------------
Batch 1/64 loss: -0.09819728136062622
Batch 2/64 loss: -0.11153042316436768
Batch 3/64 loss: -0.0994003415107727
Batch 4/64 loss: -0.10736346244812012
Batch 5/64 loss: -0.1074264645576477
Batch 6/64 loss: -0.1198798418045044
Batch 7/64 loss: -0.11383163928985596
Batch 8/64 loss: -0.08983969688415527
Batch 9/64 loss: -0.11286777257919312
Batch 10/64 loss: -0.11456120014190674
Batch 11/64 loss: -0.1124882698059082
Batch 12/64 loss: -0.11561888456344604
Batch 13/64 loss: -0.10386484861373901
Batch 14/64 loss: -0.08839470148086548
Batch 15/64 loss: -0.10895037651062012
Batch 16/64 loss: -0.10839533805847168
Batch 17/64 loss: -0.10939198732376099
Batch 18/64 loss: -0.08234930038452148
Batch 19/64 loss: -0.08248436450958252
Batch 20/64 loss: -0.08763039112091064
Batch 21/64 loss: -0.09971529245376587
Batch 22/64 loss: -0.11701023578643799
Batch 23/64 loss: -0.11677974462509155
Batch 24/64 loss: -0.11852645874023438
Batch 25/64 loss: -0.11033499240875244
Batch 26/64 loss: -0.11328887939453125
Batch 27/64 loss: -0.10690516233444214
Batch 28/64 loss: -0.10378968715667725
Batch 29/64 loss: -0.10483920574188232
Batch 30/64 loss: -0.06998485326766968
Batch 31/64 loss: -0.120716392993927
Batch 32/64 loss: -0.10792523622512817
Batch 33/64 loss: -0.10981351137161255
Batch 34/64 loss: -0.11161381006240845
Batch 35/64 loss: -0.11461204290390015
Batch 36/64 loss: -0.08535808324813843
Batch 37/64 loss: -0.11643439531326294
Batch 38/64 loss: -0.09869062900543213
Batch 39/64 loss: -0.12917572259902954
Batch 40/64 loss: -0.10638219118118286
Batch 41/64 loss: -0.09840893745422363
Batch 42/64 loss: -0.10940927267074585
Batch 43/64 loss: -0.1230156421661377
Batch 44/64 loss: -0.11651575565338135
Batch 45/64 loss: -0.0936623215675354
Batch 46/64 loss: -0.08730405569076538
Batch 47/64 loss: -0.09651386737823486
Batch 48/64 loss: -0.10766667127609253
Batch 49/64 loss: -0.11608421802520752
Batch 50/64 loss: -0.09795081615447998
Batch 51/64 loss: -0.08987939357757568
Batch 52/64 loss: -0.11788028478622437
Batch 53/64 loss: -0.10502356290817261
Batch 54/64 loss: -0.10218983888626099
Batch 55/64 loss: -0.09547042846679688
Batch 56/64 loss: -0.09949195384979248
Batch 57/64 loss: -0.10969072580337524
Batch 58/64 loss: -0.08818668127059937
Batch 59/64 loss: -0.11660093069076538
Batch 60/64 loss: -0.09722840785980225
Batch 61/64 loss: -0.10585451126098633
Batch 62/64 loss: -0.10168612003326416
Batch 63/64 loss: -0.10176801681518555
Batch 64/64 loss: -0.11814999580383301
Epoch 196  Train loss: -0.10513659645529354  Val loss: -0.039626500860522294
Epoch 197
-------------------------------
Batch 1/64 loss: -0.09621208906173706
Batch 2/64 loss: -0.11941033601760864
Batch 3/64 loss: -0.12105715274810791
Batch 4/64 loss: -0.11279678344726562
Batch 5/64 loss: -0.10940319299697876
Batch 6/64 loss: -0.11408543586730957
Batch 7/64 loss: -0.10218918323516846
Batch 8/64 loss: -0.11463433504104614
Batch 9/64 loss: -0.11191153526306152
Batch 10/64 loss: -0.11033815145492554
Batch 11/64 loss: -0.12244772911071777
Batch 12/64 loss: -0.09947824478149414
Batch 13/64 loss: -0.1046590805053711
Batch 14/64 loss: -0.10397952795028687
Batch 15/64 loss: -0.10767579078674316
Batch 16/64 loss: -0.11101579666137695
Batch 17/64 loss: -0.08608448505401611
Batch 18/64 loss: -0.08774739503860474
Batch 19/64 loss: -0.11972236633300781
Batch 20/64 loss: -0.10461771488189697
Batch 21/64 loss: -0.10884177684783936
Batch 22/64 loss: -0.10038858652114868
Batch 23/64 loss: -0.09679305553436279
Batch 24/64 loss: -0.09611111879348755
Batch 25/64 loss: -0.11572271585464478
Batch 26/64 loss: -0.112496018409729
Batch 27/64 loss: -0.0858953595161438
Batch 28/64 loss: -0.08295536041259766
Batch 29/64 loss: -0.10304450988769531
Batch 30/64 loss: -0.1084294319152832
Batch 31/64 loss: -0.10469019412994385
Batch 32/64 loss: -0.11364984512329102
Batch 33/64 loss: -0.09663790464401245
Batch 34/64 loss: -0.10691100358963013
Batch 35/64 loss: -0.11272692680358887
Batch 36/64 loss: -0.09337872266769409
Batch 37/64 loss: -0.11558699607849121
Batch 38/64 loss: -0.09765726327896118
Batch 39/64 loss: -0.10188877582550049
Batch 40/64 loss: -0.10055720806121826
Batch 41/64 loss: -0.12032425403594971
Batch 42/64 loss: -0.10764813423156738
Batch 43/64 loss: -0.08998137712478638
Batch 44/64 loss: -0.09897106885910034
Batch 45/64 loss: -0.10876399278640747
Batch 46/64 loss: -0.09243452548980713
Batch 47/64 loss: -0.11654436588287354
Batch 48/64 loss: -0.10374772548675537
Batch 49/64 loss: -0.12374246120452881
Batch 50/64 loss: -0.10440844297409058
Batch 51/64 loss: -0.10807347297668457
Batch 52/64 loss: -0.10551953315734863
Batch 53/64 loss: -0.10746115446090698
Batch 54/64 loss: -0.09968400001525879
Batch 55/64 loss: -0.0887521505355835
Batch 56/64 loss: -0.09069162607192993
Batch 57/64 loss: -0.08798551559448242
Batch 58/64 loss: -0.11822640895843506
Batch 59/64 loss: -0.09823733568191528
Batch 60/64 loss: -0.10453319549560547
Batch 61/64 loss: -0.09721499681472778
Batch 62/64 loss: -0.11124861240386963
Batch 63/64 loss: -0.09420663118362427
Batch 64/64 loss: -0.12058502435684204
Epoch 197  Train loss: -0.10482617803648406  Val loss: -0.04476340509362237
Epoch 198
-------------------------------
Batch 1/64 loss: -0.08715641498565674
Batch 2/64 loss: -0.13551169633865356
Batch 3/64 loss: -0.09988969564437866
Batch 4/64 loss: -0.11967635154724121
Batch 5/64 loss: -0.09712296724319458
Batch 6/64 loss: -0.0763392448425293
Batch 7/64 loss: -0.10041618347167969
Batch 8/64 loss: -0.1274077296257019
Batch 9/64 loss: -0.08595234155654907
Batch 10/64 loss: -0.10502105951309204
Batch 11/64 loss: -0.10662519931793213
Batch 12/64 loss: -0.10323995351791382
Batch 13/64 loss: -0.10742419958114624
Batch 14/64 loss: -0.12092858552932739
Batch 15/64 loss: -0.09763872623443604
Batch 16/64 loss: -0.11402523517608643
Batch 17/64 loss: -0.107657790184021
Batch 18/64 loss: -0.10358428955078125
Batch 19/64 loss: -0.09869879484176636
Batch 20/64 loss: -0.10523116588592529
Batch 21/64 loss: -0.11275267601013184
Batch 22/64 loss: -0.12730318307876587
Batch 23/64 loss: -0.11964505910873413
Batch 24/64 loss: -0.11245179176330566
Batch 25/64 loss: -0.08835768699645996
Batch 26/64 loss: -0.0986560583114624
Batch 27/64 loss: -0.10138159990310669
Batch 28/64 loss: -0.12780624628067017
Batch 29/64 loss: -0.12055754661560059
Batch 30/64 loss: -0.1146402359008789
Batch 31/64 loss: -0.1030389666557312
Batch 32/64 loss: -0.09201222658157349
Batch 33/64 loss: -0.10271811485290527
Batch 34/64 loss: -0.1212611198425293
Batch 35/64 loss: -0.09958946704864502
Batch 36/64 loss: -0.11514627933502197
Batch 37/64 loss: -0.10617220401763916
Batch 38/64 loss: -0.08709859848022461
Batch 39/64 loss: -0.11762964725494385
Batch 40/64 loss: -0.11998534202575684
Batch 41/64 loss: -0.10801535844802856
Batch 42/64 loss: -0.12230205535888672
Batch 43/64 loss: -0.1103065013885498
Batch 44/64 loss: -0.10843050479888916
Batch 45/64 loss: -0.11211687326431274
Batch 46/64 loss: -0.1126089096069336
Batch 47/64 loss: -0.10343581438064575
Batch 48/64 loss: -0.11235231161117554
Batch 49/64 loss: -0.10338354110717773
Batch 50/64 loss: -0.10381662845611572
Batch 51/64 loss: -0.1108628511428833
Batch 52/64 loss: -0.1037524938583374
Batch 53/64 loss: -0.11557084321975708
Batch 54/64 loss: -0.10926926136016846
Batch 55/64 loss: -0.11249101161956787
Batch 56/64 loss: -0.09563636779785156
Batch 57/64 loss: -0.1028587818145752
Batch 58/64 loss: -0.10615921020507812
Batch 59/64 loss: -0.0983114242553711
Batch 60/64 loss: -0.10572266578674316
Batch 61/64 loss: -0.10365009307861328
Batch 62/64 loss: -0.0996820330619812
Batch 63/64 loss: -0.10581880807876587
Batch 64/64 loss: -0.11865103244781494
Epoch 198  Train loss: -0.10734579282648424  Val loss: -0.04662523748948402
Saving best model, epoch: 198
Epoch 199
-------------------------------
Batch 1/64 loss: -0.09038007259368896
Batch 2/64 loss: -0.1214684247970581
Batch 3/64 loss: -0.09321737289428711
Batch 4/64 loss: -0.0771833062171936
Batch 5/64 loss: -0.09336668252944946
Batch 6/64 loss: -0.1025923490524292
Batch 7/64 loss: -0.10315805673599243
Batch 8/64 loss: -0.11600661277770996
Batch 9/64 loss: -0.09650343656539917
Batch 10/64 loss: -0.11441361904144287
Batch 11/64 loss: -0.11147952079772949
Batch 12/64 loss: -0.11542391777038574
Batch 13/64 loss: -0.10741227865219116
Batch 14/64 loss: -0.11748003959655762
Batch 15/64 loss: -0.1027299165725708
Batch 16/64 loss: -0.09256225824356079
Batch 17/64 loss: -0.09528440237045288
Batch 18/64 loss: -0.11742216348648071
Batch 19/64 loss: -0.10791981220245361
Batch 20/64 loss: -0.09854656457901001
Batch 21/64 loss: -0.09837400913238525
Batch 22/64 loss: -0.07920026779174805
Batch 23/64 loss: -0.10284936428070068
Batch 24/64 loss: -0.11039787530899048
Batch 25/64 loss: -0.0880158543586731
Batch 26/64 loss: -0.09597110748291016
Batch 27/64 loss: -0.11096328496932983
Batch 28/64 loss: -0.11996877193450928
Batch 29/64 loss: -0.10595047473907471
Batch 30/64 loss: -0.10107111930847168
Batch 31/64 loss: -0.10438650846481323
Batch 32/64 loss: -0.10178059339523315
Batch 33/64 loss: -0.1114230751991272
Batch 34/64 loss: -0.11957430839538574
Batch 35/64 loss: -0.10662829875946045
Batch 36/64 loss: -0.09430360794067383
Batch 37/64 loss: -0.11923831701278687
Batch 38/64 loss: -0.12338870763778687
Batch 39/64 loss: -0.10878366231918335
Batch 40/64 loss: -0.10310983657836914
Batch 41/64 loss: -0.09516388177871704
Batch 42/64 loss: -0.1231042742729187
Batch 43/64 loss: -0.1206197738647461
Batch 44/64 loss: -0.11124879121780396
Batch 45/64 loss: -0.10676050186157227
Batch 46/64 loss: -0.10385727882385254
Batch 47/64 loss: -0.11196386814117432
Batch 48/64 loss: -0.10055899620056152
Batch 49/64 loss: -0.10838103294372559
Batch 50/64 loss: -0.10492062568664551
Batch 51/64 loss: -0.10708010196685791
Batch 52/64 loss: -0.10867029428482056
Batch 53/64 loss: -0.1053500771522522
Batch 54/64 loss: -0.11322546005249023
Batch 55/64 loss: -0.11480677127838135
Batch 56/64 loss: -0.10313951969146729
Batch 57/64 loss: -0.10981357097625732
Batch 58/64 loss: -0.10399782657623291
Batch 59/64 loss: -0.1060248613357544
Batch 60/64 loss: -0.07665383815765381
Batch 61/64 loss: -0.11049020290374756
Batch 62/64 loss: -0.11043339967727661
Batch 63/64 loss: -0.11937516927719116
Batch 64/64 loss: -0.11186057329177856
Epoch 199  Train loss: -0.10571710432277007  Val loss: -0.040276195175459294
Epoch 200
-------------------------------
Batch 1/64 loss: -0.09886682033538818
Batch 2/64 loss: -0.10571402311325073
Batch 3/64 loss: -0.09451788663864136
Batch 4/64 loss: -0.10406297445297241
Batch 5/64 loss: -0.09171831607818604
Batch 6/64 loss: -0.11787652969360352
Batch 7/64 loss: -0.12795990705490112
Batch 8/64 loss: -0.11725372076034546
Batch 9/64 loss: -0.11009109020233154
Batch 10/64 loss: -0.12254220247268677
Batch 11/64 loss: -0.10257583856582642
Batch 12/64 loss: -0.11283218860626221
Batch 13/64 loss: -0.11553233861923218
Batch 14/64 loss: -0.09686112403869629
Batch 15/64 loss: -0.08987367153167725
Batch 16/64 loss: -0.10559588670730591
Batch 17/64 loss: -0.12839066982269287
Batch 18/64 loss: -0.11901897192001343
Batch 19/64 loss: -0.09577947854995728
Batch 20/64 loss: -0.11463218927383423
Batch 21/64 loss: -0.11766183376312256
Batch 22/64 loss: -0.11212915182113647
Batch 23/64 loss: -0.1281096339225769
Batch 24/64 loss: -0.1372286081314087
Batch 25/64 loss: -0.10415792465209961
Batch 26/64 loss: -0.1001209020614624
Batch 27/64 loss: -0.11129105091094971
Batch 28/64 loss: -0.09130692481994629
Batch 29/64 loss: -0.11150234937667847
Batch 30/64 loss: -0.0957263708114624
Batch 31/64 loss: -0.12186264991760254
Batch 32/64 loss: -0.12373954057693481
Batch 33/64 loss: -0.09798812866210938
Batch 34/64 loss: -0.11162161827087402
Batch 35/64 loss: -0.10108029842376709
Batch 36/64 loss: -0.10525500774383545
Batch 37/64 loss: -0.1022375226020813
Batch 38/64 loss: -0.09454607963562012
Batch 39/64 loss: -0.10747343301773071
Batch 40/64 loss: -0.10608869791030884
Batch 41/64 loss: -0.10402750968933105
Batch 42/64 loss: -0.10189813375473022
Batch 43/64 loss: -0.10211724042892456
Batch 44/64 loss: -0.09830337762832642
Batch 45/64 loss: -0.11251997947692871
Batch 46/64 loss: -0.10082972049713135
Batch 47/64 loss: -0.10506176948547363
Batch 48/64 loss: -0.08441329002380371
Batch 49/64 loss: -0.11078709363937378
Batch 50/64 loss: -0.10194498300552368
Batch 51/64 loss: -0.09966200590133667
Batch 52/64 loss: -0.11190521717071533
Batch 53/64 loss: -0.1136060357093811
Batch 54/64 loss: -0.08075529336929321
Batch 55/64 loss: -0.10821551084518433
Batch 56/64 loss: -0.1012493371963501
Batch 57/64 loss: -0.11623579263687134
Batch 58/64 loss: -0.1041368842124939
Batch 59/64 loss: -0.11245226860046387
Batch 60/64 loss: -0.10955196619033813
Batch 61/64 loss: -0.12016928195953369
Batch 62/64 loss: -0.11917984485626221
Batch 63/64 loss: -0.09324133396148682
Batch 64/64 loss: -0.11296743154525757
Epoch 200  Train loss: -0.10744804704890532  Val loss: -0.0468581034145814
Saving best model, epoch: 200
Epoch 201
-------------------------------
Batch 1/64 loss: -0.11477446556091309
Batch 2/64 loss: -0.10996365547180176
Batch 3/64 loss: -0.11360138654708862
Batch 4/64 loss: -0.12684059143066406
Batch 5/64 loss: -0.10623544454574585
Batch 6/64 loss: -0.11160701513290405
Batch 7/64 loss: -0.11043864488601685
Batch 8/64 loss: -0.105502188205719
Batch 9/64 loss: -0.11987227201461792
Batch 10/64 loss: -0.10643893480300903
Batch 11/64 loss: -0.1249731183052063
Batch 12/64 loss: -0.11537176370620728
Batch 13/64 loss: -0.11809921264648438
Batch 14/64 loss: -0.09885048866271973
Batch 15/64 loss: -0.12408232688903809
Batch 16/64 loss: -0.12096917629241943
Batch 17/64 loss: -0.12292814254760742
Batch 18/64 loss: -0.10417252779006958
Batch 19/64 loss: -0.09399425983428955
Batch 20/64 loss: -0.10315537452697754
Batch 21/64 loss: -0.11461079120635986
Batch 22/64 loss: -0.11048799753189087
Batch 23/64 loss: -0.10674011707305908
Batch 24/64 loss: -0.11228775978088379
Batch 25/64 loss: -0.09911918640136719
Batch 26/64 loss: -0.11578291654586792
Batch 27/64 loss: -0.10391867160797119
Batch 28/64 loss: -0.10643291473388672
Batch 29/64 loss: -0.11218440532684326
Batch 30/64 loss: -0.10303914546966553
Batch 31/64 loss: -0.10981148481369019
Batch 32/64 loss: -0.09645527601242065
Batch 33/64 loss: -0.11589854955673218
Batch 34/64 loss: -0.09214556217193604
Batch 35/64 loss: -0.08857756853103638
Batch 36/64 loss: -0.11367249488830566
Batch 37/64 loss: -0.1227988600730896
Batch 38/64 loss: -0.12241750955581665
Batch 39/64 loss: -0.1092258095741272
Batch 40/64 loss: -0.11213749647140503
Batch 41/64 loss: -0.10033583641052246
Batch 42/64 loss: -0.1119111180305481
Batch 43/64 loss: -0.10717165470123291
Batch 44/64 loss: -0.10119658708572388
Batch 45/64 loss: -0.08713066577911377
Batch 46/64 loss: -0.10091972351074219
Batch 47/64 loss: -0.11305868625640869
Batch 48/64 loss: -0.11900430917739868
Batch 49/64 loss: -0.110079824924469
Batch 50/64 loss: -0.10710102319717407
Batch 51/64 loss: -0.10696589946746826
Batch 52/64 loss: -0.07865715026855469
Batch 53/64 loss: -0.11319541931152344
Batch 54/64 loss: -0.06834149360656738
Batch 55/64 loss: -0.10235488414764404
Batch 56/64 loss: -0.10040396451950073
Batch 57/64 loss: -0.10658985376358032
Batch 58/64 loss: -0.11298537254333496
Batch 59/64 loss: -0.11841797828674316
Batch 60/64 loss: -0.11847394704818726
Batch 61/64 loss: -0.11254632472991943
Batch 62/64 loss: -0.11361593008041382
Batch 63/64 loss: -0.11544674634933472
Batch 64/64 loss: -0.1267600655555725
Epoch 201  Train loss: -0.10887199920766494  Val loss: -0.04146286794000475
Epoch 202
-------------------------------
Batch 1/64 loss: -0.12103015184402466
Batch 2/64 loss: -0.1280374526977539
Batch 3/64 loss: -0.12254446744918823
Batch 4/64 loss: -0.10382777452468872
Batch 5/64 loss: -0.11465829610824585
Batch 6/64 loss: -0.09295523166656494
Batch 7/64 loss: -0.09854578971862793
Batch 8/64 loss: -0.10976940393447876
Batch 9/64 loss: -0.11847269535064697
Batch 10/64 loss: -0.11119163036346436
Batch 11/64 loss: -0.12041264772415161
Batch 12/64 loss: -0.09274148941040039
Batch 13/64 loss: -0.11011934280395508
Batch 14/64 loss: -0.12070274353027344
Batch 15/64 loss: -0.11146020889282227
Batch 16/64 loss: -0.09441232681274414
Batch 17/64 loss: -0.10871315002441406
Batch 18/64 loss: -0.12350869178771973
Batch 19/64 loss: -0.10201430320739746
Batch 20/64 loss: -0.10431092977523804
Batch 21/64 loss: -0.08094930648803711
Batch 22/64 loss: -0.10652291774749756
Batch 23/64 loss: -0.11585140228271484
Batch 24/64 loss: -0.11202913522720337
Batch 25/64 loss: -0.11366415023803711
Batch 26/64 loss: -0.10536050796508789
Batch 27/64 loss: -0.10343611240386963
Batch 28/64 loss: -0.1049264669418335
Batch 29/64 loss: -0.0988168716430664
Batch 30/64 loss: -0.12022542953491211
Batch 31/64 loss: -0.08684533834457397
Batch 32/64 loss: -0.13019680976867676
Batch 33/64 loss: -0.12853890657424927
Batch 34/64 loss: -0.08247524499893188
Batch 35/64 loss: -0.10953664779663086
Batch 36/64 loss: -0.09604406356811523
Batch 37/64 loss: -0.11008691787719727
Batch 38/64 loss: -0.11302340030670166
Batch 39/64 loss: -0.1045064926147461
Batch 40/64 loss: -0.10368645191192627
Batch 41/64 loss: -0.09994572401046753
Batch 42/64 loss: -0.10322022438049316
Batch 43/64 loss: -0.09909355640411377
Batch 44/64 loss: -0.1097988486289978
Batch 45/64 loss: -0.10292708873748779
Batch 46/64 loss: -0.12478262186050415
Batch 47/64 loss: -0.09397876262664795
Batch 48/64 loss: -0.11076349020004272
Batch 49/64 loss: -0.10075294971466064
Batch 50/64 loss: -0.10657083988189697
Batch 51/64 loss: -0.09447789192199707
Batch 52/64 loss: -0.12670785188674927
Batch 53/64 loss: -0.1107473373413086
Batch 54/64 loss: -0.12937068939208984
Batch 55/64 loss: -0.08967477083206177
Batch 56/64 loss: -0.12267500162124634
Batch 57/64 loss: -0.09629333019256592
Batch 58/64 loss: -0.08320260047912598
Batch 59/64 loss: -0.09632772207260132
Batch 60/64 loss: -0.118702232837677
Batch 61/64 loss: -0.11099529266357422
Batch 62/64 loss: -0.12296527624130249
Batch 63/64 loss: -0.11606711149215698
Batch 64/64 loss: -0.11990928649902344
Epoch 202  Train loss: -0.10817450168086033  Val loss: -0.04188656827428497
Epoch 203
-------------------------------
Batch 1/64 loss: -0.12071305513381958
Batch 2/64 loss: -0.106453537940979
Batch 3/64 loss: -0.11776936054229736
Batch 4/64 loss: -0.1287899613380432
Batch 5/64 loss: -0.0989830493927002
Batch 6/64 loss: -0.11115586757659912
Batch 7/64 loss: -0.13881021738052368
Batch 8/64 loss: -0.1313372254371643
Batch 9/64 loss: -0.08933597803115845
Batch 10/64 loss: -0.12342900037765503
Batch 11/64 loss: -0.09000635147094727
Batch 12/64 loss: -0.1161414384841919
Batch 13/64 loss: -0.11526942253112793
Batch 14/64 loss: -0.0986107587814331
Batch 15/64 loss: -0.09799313545227051
Batch 16/64 loss: -0.10148853063583374
Batch 17/64 loss: -0.11585420370101929
Batch 18/64 loss: -0.1244509220123291
Batch 19/64 loss: -0.09910249710083008
Batch 20/64 loss: -0.08991706371307373
Batch 21/64 loss: -0.12340468168258667
Batch 22/64 loss: -0.09599900245666504
Batch 23/64 loss: -0.11859238147735596
Batch 24/64 loss: -0.10189473628997803
Batch 25/64 loss: -0.1029888391494751
Batch 26/64 loss: -0.12582862377166748
Batch 27/64 loss: -0.11052727699279785
Batch 28/64 loss: -0.09753233194351196
Batch 29/64 loss: -0.10409575700759888
Batch 30/64 loss: -0.11101710796356201
Batch 31/64 loss: -0.12104207277297974
Batch 32/64 loss: -0.11316728591918945
Batch 33/64 loss: -0.08470731973648071
Batch 34/64 loss: -0.121615469455719
Batch 35/64 loss: -0.11615478992462158
Batch 36/64 loss: -0.07572096586227417
Batch 37/64 loss: -0.12557047605514526
Batch 38/64 loss: -0.11023879051208496
Batch 39/64 loss: -0.11091995239257812
Batch 40/64 loss: -0.12343591451644897
Batch 41/64 loss: -0.1009054183959961
Batch 42/64 loss: -0.11939555406570435
Batch 43/64 loss: -0.12636315822601318
Batch 44/64 loss: -0.11029911041259766
Batch 45/64 loss: -0.09257376194000244
Batch 46/64 loss: -0.10941076278686523
Batch 47/64 loss: -0.10779160261154175
Batch 48/64 loss: -0.08534342050552368
Batch 49/64 loss: -0.10209733247756958
Batch 50/64 loss: -0.09375286102294922
Batch 51/64 loss: -0.11163538694381714
Batch 52/64 loss: -0.08577889204025269
Batch 53/64 loss: -0.09655362367630005
Batch 54/64 loss: -0.0802469253540039
Batch 55/64 loss: -0.09957742691040039
Batch 56/64 loss: -0.10337895154953003
Batch 57/64 loss: -0.11256390810012817
Batch 58/64 loss: -0.0951891541481018
Batch 59/64 loss: -0.1258702278137207
Batch 60/64 loss: -0.08643591403961182
Batch 61/64 loss: -0.1070023775100708
Batch 62/64 loss: -0.09482216835021973
Batch 63/64 loss: -0.10019862651824951
Batch 64/64 loss: -0.12864893674850464
Epoch 203  Train loss: -0.10750962542552574  Val loss: -0.04352380545278595
Epoch 204
-------------------------------
Batch 1/64 loss: -0.11657017469406128
Batch 2/64 loss: -0.10508733987808228
Batch 3/64 loss: -0.12196528911590576
Batch 4/64 loss: -0.12253069877624512
Batch 5/64 loss: -0.10866951942443848
Batch 6/64 loss: -0.10924345254898071
Batch 7/64 loss: -0.10363972187042236
Batch 8/64 loss: -0.08094143867492676
Batch 9/64 loss: -0.10122138261795044
Batch 10/64 loss: -0.09480226039886475
Batch 11/64 loss: -0.10892444849014282
Batch 12/64 loss: -0.10607779026031494
Batch 13/64 loss: -0.10970300436019897
Batch 14/64 loss: -0.11284536123275757
Batch 15/64 loss: -0.10852301120758057
Batch 16/64 loss: -0.11549800634384155
Batch 17/64 loss: -0.11402857303619385
Batch 18/64 loss: -0.11145645380020142
Batch 19/64 loss: -0.09830260276794434
Batch 20/64 loss: -0.09600627422332764
Batch 21/64 loss: -0.10426020622253418
Batch 22/64 loss: -0.12099874019622803
Batch 23/64 loss: -0.1193816065788269
Batch 24/64 loss: -0.08391731977462769
Batch 25/64 loss: -0.10843783617019653
Batch 26/64 loss: -0.1344965696334839
Batch 27/64 loss: -0.10211950540542603
Batch 28/64 loss: -0.10765045881271362
Batch 29/64 loss: -0.11015969514846802
Batch 30/64 loss: -0.12142252922058105
Batch 31/64 loss: -0.10874474048614502
Batch 32/64 loss: -0.12520527839660645
Batch 33/64 loss: -0.12475979328155518
Batch 34/64 loss: -0.11350274085998535
Batch 35/64 loss: -0.12788015604019165
Batch 36/64 loss: -0.09888219833374023
Batch 37/64 loss: -0.09921514987945557
Batch 38/64 loss: -0.11563658714294434
Batch 39/64 loss: -0.09782916307449341
Batch 40/64 loss: -0.10667270421981812
Batch 41/64 loss: -0.1044815182685852
Batch 42/64 loss: -0.11010867357254028
Batch 43/64 loss: -0.10943067073822021
Batch 44/64 loss: -0.11060494184494019
Batch 45/64 loss: -0.11610788106918335
Batch 46/64 loss: -0.1255970001220703
Batch 47/64 loss: -0.11232364177703857
Batch 48/64 loss: -0.11418569087982178
Batch 49/64 loss: -0.09135156869888306
Batch 50/64 loss: -0.11430132389068604
Batch 51/64 loss: -0.1163473129272461
Batch 52/64 loss: -0.10957843065261841
Batch 53/64 loss: -0.10300958156585693
Batch 54/64 loss: -0.11320120096206665
Batch 55/64 loss: -0.11347997188568115
Batch 56/64 loss: -0.11063003540039062
Batch 57/64 loss: -0.11595809459686279
Batch 58/64 loss: -0.11592894792556763
Batch 59/64 loss: -0.10945087671279907
Batch 60/64 loss: -0.13324832916259766
Batch 61/64 loss: -0.11570084095001221
Batch 62/64 loss: -0.09157669544219971
Batch 63/64 loss: -0.09747141599655151
Batch 64/64 loss: -0.09986227750778198
Epoch 204  Train loss: -0.10990088056115543  Val loss: -0.04308789558836685
Epoch 205
-------------------------------
Batch 1/64 loss: -0.12341022491455078
Batch 2/64 loss: -0.11982017755508423
Batch 3/64 loss: -0.11493134498596191
Batch 4/64 loss: -0.09903323650360107
Batch 5/64 loss: -0.10618925094604492
Batch 6/64 loss: -0.09465265274047852
Batch 7/64 loss: -0.11258566379547119
Batch 8/64 loss: -0.13304364681243896
Batch 9/64 loss: -0.11053067445755005
Batch 10/64 loss: -0.11873137950897217
Batch 11/64 loss: -0.08491843938827515
Batch 12/64 loss: -0.1336977481842041
Batch 13/64 loss: -0.13319426774978638
Batch 14/64 loss: -0.0902029275894165
Batch 15/64 loss: -0.12529033422470093
Batch 16/64 loss: -0.12713199853897095
Batch 17/64 loss: -0.12519484758377075
Batch 18/64 loss: -0.11990183591842651
Batch 19/64 loss: -0.12646591663360596
Batch 20/64 loss: -0.11052227020263672
Batch 21/64 loss: -0.1203455924987793
Batch 22/64 loss: -0.11954343318939209
Batch 23/64 loss: -0.11466479301452637
Batch 24/64 loss: -0.1167294979095459
Batch 25/64 loss: -0.11367738246917725
Batch 26/64 loss: -0.12382698059082031
Batch 27/64 loss: -0.10866683721542358
Batch 28/64 loss: -0.10735088586807251
Batch 29/64 loss: -0.1169472336769104
Batch 30/64 loss: -0.11432582139968872
Batch 31/64 loss: -0.10971790552139282
Batch 32/64 loss: -0.11065000295639038
Batch 33/64 loss: -0.12068754434585571
Batch 34/64 loss: -0.08706247806549072
Batch 35/64 loss: -0.12327772378921509
Batch 36/64 loss: -0.10669058561325073
Batch 37/64 loss: -0.11519789695739746
Batch 38/64 loss: -0.10781055688858032
Batch 39/64 loss: -0.11503928899765015
Batch 40/64 loss: -0.12617701292037964
Batch 41/64 loss: -0.08261555433273315
Batch 42/64 loss: -0.1140984296798706
Batch 43/64 loss: -0.09909176826477051
Batch 44/64 loss: -0.10062050819396973
Batch 45/64 loss: -0.11763614416122437
Batch 46/64 loss: -0.11934685707092285
Batch 47/64 loss: -0.09771764278411865
Batch 48/64 loss: -0.11639946699142456
Batch 49/64 loss: -0.12170606851577759
Batch 50/64 loss: -0.11388653516769409
Batch 51/64 loss: -0.11933314800262451
Batch 52/64 loss: -0.09387087821960449
Batch 53/64 loss: -0.11464434862136841
Batch 54/64 loss: -0.1237141489982605
Batch 55/64 loss: -0.11428624391555786
Batch 56/64 loss: -0.09050148725509644
Batch 57/64 loss: -0.10115277767181396
Batch 58/64 loss: -0.09994524717330933
Batch 59/64 loss: -0.09779715538024902
Batch 60/64 loss: -0.12342000007629395
Batch 61/64 loss: -0.11139565706253052
Batch 62/64 loss: -0.09199070930480957
Batch 63/64 loss: -0.12190824747085571
Batch 64/64 loss: -0.1110450029373169
Epoch 205  Train loss: -0.11228550695905498  Val loss: -0.04088907819433311
Epoch 206
-------------------------------
Batch 1/64 loss: -0.13453245162963867
Batch 2/64 loss: -0.1311875581741333
Batch 3/64 loss: -0.10747367143630981
Batch 4/64 loss: -0.12846535444259644
Batch 5/64 loss: -0.106548011302948
Batch 6/64 loss: -0.13490402698516846
Batch 7/64 loss: -0.10948735475540161
Batch 8/64 loss: -0.11938846111297607
Batch 9/64 loss: -0.0987289547920227
Batch 10/64 loss: -0.13413292169570923
Batch 11/64 loss: -0.12974286079406738
Batch 12/64 loss: -0.10443979501724243
Batch 13/64 loss: -0.12085199356079102
Batch 14/64 loss: -0.09997165203094482
Batch 15/64 loss: -0.06888824701309204
Batch 16/64 loss: -0.09576255083084106
Batch 17/64 loss: -0.10634076595306396
Batch 18/64 loss: -0.11521601676940918
Batch 19/64 loss: -0.10524940490722656
Batch 20/64 loss: -0.09784972667694092
Batch 21/64 loss: -0.10589665174484253
Batch 22/64 loss: -0.10819226503372192
Batch 23/64 loss: -0.09879779815673828
Batch 24/64 loss: -0.1185721755027771
Batch 25/64 loss: -0.08792954683303833
Batch 26/64 loss: -0.11500334739685059
Batch 27/64 loss: -0.11787146329879761
Batch 28/64 loss: -0.11857551336288452
Batch 29/64 loss: -0.11122751235961914
Batch 30/64 loss: -0.10732382535934448
Batch 31/64 loss: -0.12087893486022949
Batch 32/64 loss: -0.08174407482147217
Batch 33/64 loss: -0.10981523990631104
Batch 34/64 loss: -0.11588644981384277
Batch 35/64 loss: -0.10667669773101807
Batch 36/64 loss: -0.11755603551864624
Batch 37/64 loss: -0.11320352554321289
Batch 38/64 loss: -0.10493898391723633
Batch 39/64 loss: -0.11329960823059082
Batch 40/64 loss: -0.133134126663208
Batch 41/64 loss: -0.09035176038742065
Batch 42/64 loss: -0.12033331394195557
Batch 43/64 loss: -0.12793266773223877
Batch 44/64 loss: -0.09820067882537842
Batch 45/64 loss: -0.09896832704544067
Batch 46/64 loss: -0.12032771110534668
Batch 47/64 loss: -0.10498237609863281
Batch 48/64 loss: -0.11867004632949829
Batch 49/64 loss: -0.11613225936889648
Batch 50/64 loss: -0.11384326219558716
Batch 51/64 loss: -0.12471282482147217
Batch 52/64 loss: -0.11644566059112549
Batch 53/64 loss: -0.08903896808624268
Batch 54/64 loss: -0.11776244640350342
Batch 55/64 loss: -0.12730515003204346
Batch 56/64 loss: -0.10303735733032227
Batch 57/64 loss: -0.12390714883804321
Batch 58/64 loss: -0.11838740110397339
Batch 59/64 loss: -0.12307173013687134
Batch 60/64 loss: -0.10071229934692383
Batch 61/64 loss: -0.10778117179870605
Batch 62/64 loss: -0.10036331415176392
Batch 63/64 loss: -0.10384362936019897
Batch 64/64 loss: -0.09393244981765747
Epoch 206  Train loss: -0.11125092389536839  Val loss: -0.035512595651895316
Epoch 207
-------------------------------
Batch 1/64 loss: -0.12296473979949951
Batch 2/64 loss: -0.1178884506225586
Batch 3/64 loss: -0.08269858360290527
Batch 4/64 loss: -0.11155480146408081
Batch 5/64 loss: -0.10945212841033936
Batch 6/64 loss: -0.11276900768280029
Batch 7/64 loss: -0.1302570104598999
Batch 8/64 loss: -0.09458684921264648
Batch 9/64 loss: -0.11978209018707275
Batch 10/64 loss: -0.12267714738845825
Batch 11/64 loss: -0.11338698863983154
Batch 12/64 loss: -0.12410646677017212
Batch 13/64 loss: -0.1202782392501831
Batch 14/64 loss: -0.11924362182617188
Batch 15/64 loss: -0.11486184597015381
Batch 16/64 loss: -0.11182451248168945
Batch 17/64 loss: -0.12033557891845703
Batch 18/64 loss: -0.1332186460494995
Batch 19/64 loss: -0.1071465015411377
Batch 20/64 loss: -0.12063509225845337
Batch 21/64 loss: -0.10203027725219727
Batch 22/64 loss: -0.12685149908065796
Batch 23/64 loss: -0.10323178768157959
Batch 24/64 loss: -0.12078136205673218
Batch 25/64 loss: -0.09006845951080322
Batch 26/64 loss: -0.12749195098876953
Batch 27/64 loss: -0.134973406791687
Batch 28/64 loss: -0.09720951318740845
Batch 29/64 loss: -0.10964429378509521
Batch 30/64 loss: -0.10933107137680054
Batch 31/64 loss: -0.09537768363952637
Batch 32/64 loss: -0.10955536365509033
Batch 33/64 loss: -0.11054307222366333
Batch 34/64 loss: -0.10815715789794922
Batch 35/64 loss: -0.1279727816581726
Batch 36/64 loss: -0.1210903525352478
Batch 37/64 loss: -0.08765166997909546
Batch 38/64 loss: -0.10164284706115723
Batch 39/64 loss: -0.12827134132385254
Batch 40/64 loss: -0.09726375341415405
Batch 41/64 loss: -0.130548357963562
Batch 42/64 loss: -0.10497981309890747
Batch 43/64 loss: -0.09917914867401123
Batch 44/64 loss: -0.11550867557525635
Batch 45/64 loss: -0.11088645458221436
Batch 46/64 loss: -0.11273539066314697
Batch 47/64 loss: -0.11892962455749512
Batch 48/64 loss: -0.10296440124511719
Batch 49/64 loss: -0.1165846586227417
Batch 50/64 loss: -0.11505693197250366
Batch 51/64 loss: -0.09433561563491821
Batch 52/64 loss: -0.080555260181427
Batch 53/64 loss: -0.10543185472488403
Batch 54/64 loss: -0.12371939420700073
Batch 55/64 loss: -0.1190100908279419
Batch 56/64 loss: -0.1157718300819397
Batch 57/64 loss: -0.11345601081848145
Batch 58/64 loss: -0.11926853656768799
Batch 59/64 loss: -0.09648996591567993
Batch 60/64 loss: -0.11431217193603516
Batch 61/64 loss: -0.11237853765487671
Batch 62/64 loss: -0.0979728102684021
Batch 63/64 loss: -0.10606569051742554
Batch 64/64 loss: -0.12298113107681274
Epoch 207  Train loss: -0.1119558434860379  Val loss: -0.0386685219007669
Epoch 208
-------------------------------
Batch 1/64 loss: -0.11362487077713013
Batch 2/64 loss: -0.12783050537109375
Batch 3/64 loss: -0.12081468105316162
Batch 4/64 loss: -0.11001962423324585
Batch 5/64 loss: -0.12979596853256226
Batch 6/64 loss: -0.12900328636169434
Batch 7/64 loss: -0.12776565551757812
Batch 8/64 loss: -0.10963362455368042
Batch 9/64 loss: -0.10645794868469238
Batch 10/64 loss: -0.12280082702636719
Batch 11/64 loss: -0.11621946096420288
Batch 12/64 loss: -0.12922382354736328
Batch 13/64 loss: -0.0991823673248291
Batch 14/64 loss: -0.11171746253967285
Batch 15/64 loss: -0.11970317363739014
Batch 16/64 loss: -0.12043952941894531
Batch 17/64 loss: -0.12093144655227661
Batch 18/64 loss: -0.09784048795700073
Batch 19/64 loss: -0.12627875804901123
Batch 20/64 loss: -0.1049489974975586
Batch 21/64 loss: -0.10701149702072144
Batch 22/64 loss: -0.12306958436965942
Batch 23/64 loss: -0.11475181579589844
Batch 24/64 loss: -0.09818905591964722
Batch 25/64 loss: -0.1097216010093689
Batch 26/64 loss: -0.13990920782089233
Batch 27/64 loss: -0.09459793567657471
Batch 28/64 loss: -0.11057102680206299
Batch 29/64 loss: -0.1204524040222168
Batch 30/64 loss: -0.1136581301689148
Batch 31/64 loss: -0.10539460182189941
Batch 32/64 loss: -0.10759294033050537
Batch 33/64 loss: -0.11965179443359375
Batch 34/64 loss: -0.11778080463409424
Batch 35/64 loss: -0.11947554349899292
Batch 36/64 loss: -0.11233878135681152
Batch 37/64 loss: -0.12610918283462524
Batch 38/64 loss: -0.1224067211151123
Batch 39/64 loss: -0.11367189884185791
Batch 40/64 loss: -0.12264364957809448
Batch 41/64 loss: -0.11832171678543091
Batch 42/64 loss: -0.11899358034133911
Batch 43/64 loss: -0.10815316438674927
Batch 44/64 loss: -0.12077665328979492
Batch 45/64 loss: -0.11623287200927734
Batch 46/64 loss: -0.102658212184906
Batch 47/64 loss: -0.11438524723052979
Batch 48/64 loss: -0.10563665628433228
Batch 49/64 loss: -0.10745048522949219
Batch 50/64 loss: -0.10838890075683594
Batch 51/64 loss: -0.11860454082489014
Batch 52/64 loss: -0.10565459728240967
Batch 53/64 loss: -0.09814286231994629
Batch 54/64 loss: -0.12440663576126099
Batch 55/64 loss: -0.10043573379516602
Batch 56/64 loss: -0.10098659992218018
Batch 57/64 loss: -0.12270599603652954
Batch 58/64 loss: -0.09630316495895386
Batch 59/64 loss: -0.10818111896514893
Batch 60/64 loss: -0.11680877208709717
Batch 61/64 loss: -0.09971100091934204
Batch 62/64 loss: -0.10871899127960205
Batch 63/64 loss: -0.11173701286315918
Batch 64/64 loss: -0.11193448305130005
Epoch 208  Train loss: -0.11389138908947215  Val loss: -0.04000214858563086
Epoch 209
-------------------------------
Batch 1/64 loss: -0.11089968681335449
Batch 2/64 loss: -0.12747961282730103
Batch 3/64 loss: -0.11499983072280884
Batch 4/64 loss: -0.115928053855896
Batch 5/64 loss: -0.10614657402038574
Batch 6/64 loss: -0.11936599016189575
Batch 7/64 loss: -0.1189536452293396
Batch 8/64 loss: -0.10373055934906006
Batch 9/64 loss: -0.10750418901443481
Batch 10/64 loss: -0.09386450052261353
Batch 11/64 loss: -0.10857516527175903
Batch 12/64 loss: -0.11539185047149658
Batch 13/64 loss: -0.121090829372406
Batch 14/64 loss: -0.10587280988693237
Batch 15/64 loss: -0.088481605052948
Batch 16/64 loss: -0.11637479066848755
Batch 17/64 loss: -0.09092682600021362
Batch 18/64 loss: -0.09667229652404785
Batch 19/64 loss: -0.108242928981781
Batch 20/64 loss: -0.12321209907531738
Batch 21/64 loss: -0.12401872873306274
Batch 22/64 loss: -0.09760695695877075
Batch 23/64 loss: -0.12586814165115356
Batch 24/64 loss: -0.11677610874176025
Batch 25/64 loss: -0.09090781211853027
Batch 26/64 loss: -0.11006468534469604
Batch 27/64 loss: -0.10211098194122314
Batch 28/64 loss: -0.126414954662323
Batch 29/64 loss: -0.10894119739532471
Batch 30/64 loss: -0.10882425308227539
Batch 31/64 loss: -0.12382632493972778
Batch 32/64 loss: -0.12876218557357788
Batch 33/64 loss: -0.1233363151550293
Batch 34/64 loss: -0.10271984338760376
Batch 35/64 loss: -0.10353302955627441
Batch 36/64 loss: -0.09814310073852539
Batch 37/64 loss: -0.12813931703567505
Batch 38/64 loss: -0.12456411123275757
Batch 39/64 loss: -0.12543213367462158
Batch 40/64 loss: -0.11294049024581909
Batch 41/64 loss: -0.1172109842300415
Batch 42/64 loss: -0.11922860145568848
Batch 43/64 loss: -0.12661302089691162
Batch 44/64 loss: -0.09976369142532349
Batch 45/64 loss: -0.12112748622894287
Batch 46/64 loss: -0.11400431394577026
Batch 47/64 loss: -0.13191616535186768
Batch 48/64 loss: -0.1258506178855896
Batch 49/64 loss: -0.08822751045227051
Batch 50/64 loss: -0.11510574817657471
Batch 51/64 loss: -0.1251755952835083
Batch 52/64 loss: -0.1196325421333313
Batch 53/64 loss: -0.11285585165023804
Batch 54/64 loss: -0.11405342817306519
Batch 55/64 loss: -0.12461566925048828
Batch 56/64 loss: -0.10311591625213623
Batch 57/64 loss: -0.11607992649078369
Batch 58/64 loss: -0.10424518585205078
Batch 59/64 loss: -0.12071150541305542
Batch 60/64 loss: -0.11757928133010864
Batch 61/64 loss: -0.11158323287963867
Batch 62/64 loss: -0.11999702453613281
Batch 63/64 loss: -0.10778629779815674
Batch 64/64 loss: -0.12070786952972412
Epoch 209  Train loss: -0.11331223553302242  Val loss: -0.04776426852773555
Saving best model, epoch: 209
Epoch 210
-------------------------------
Batch 1/64 loss: -0.125377357006073
Batch 2/64 loss: -0.10068964958190918
Batch 3/64 loss: -0.11281979084014893
Batch 4/64 loss: -0.10161054134368896
Batch 5/64 loss: -0.13577616214752197
Batch 6/64 loss: -0.1478850245475769
Batch 7/64 loss: -0.12399888038635254
Batch 8/64 loss: -0.1343020796775818
Batch 9/64 loss: -0.13436663150787354
Batch 10/64 loss: -0.12679284811019897
Batch 11/64 loss: -0.10126113891601562
Batch 12/64 loss: -0.09166264533996582
Batch 13/64 loss: -0.13084328174591064
Batch 14/64 loss: -0.10510116815567017
Batch 15/64 loss: -0.09250307083129883
Batch 16/64 loss: -0.0975298285484314
Batch 17/64 loss: -0.10359549522399902
Batch 18/64 loss: -0.1264328956604004
Batch 19/64 loss: -0.10843950510025024
Batch 20/64 loss: -0.1113923192024231
Batch 21/64 loss: -0.08270430564880371
Batch 22/64 loss: -0.09802311658859253
Batch 23/64 loss: -0.10480707883834839
Batch 24/64 loss: -0.1043388843536377
Batch 25/64 loss: -0.1231083869934082
Batch 26/64 loss: -0.11332011222839355
Batch 27/64 loss: -0.11939966678619385
Batch 28/64 loss: -0.10987162590026855
Batch 29/64 loss: -0.09720420837402344
Batch 30/64 loss: -0.11596488952636719
Batch 31/64 loss: -0.11695480346679688
Batch 32/64 loss: -0.13039952516555786
Batch 33/64 loss: -0.11367648839950562
Batch 34/64 loss: -0.11964678764343262
Batch 35/64 loss: -0.12223833799362183
Batch 36/64 loss: -0.11597239971160889
Batch 37/64 loss: -0.11224162578582764
Batch 38/64 loss: -0.11743110418319702
Batch 39/64 loss: -0.10683733224868774
Batch 40/64 loss: -0.12056386470794678
Batch 41/64 loss: -0.11188018321990967
Batch 42/64 loss: -0.12680232524871826
Batch 43/64 loss: -0.12745219469070435
Batch 44/64 loss: -0.10132980346679688
Batch 45/64 loss: -0.09767913818359375
Batch 46/64 loss: -0.12969237565994263
Batch 47/64 loss: -0.12054294347763062
Batch 48/64 loss: -0.10809695720672607
Batch 49/64 loss: -0.11982661485671997
Batch 50/64 loss: -0.11585044860839844
Batch 51/64 loss: -0.09798699617385864
Batch 52/64 loss: -0.10544490814208984
Batch 53/64 loss: -0.12509238719940186
Batch 54/64 loss: -0.1034923791885376
Batch 55/64 loss: -0.11198949813842773
Batch 56/64 loss: -0.10439199209213257
Batch 57/64 loss: -0.08777099847793579
Batch 58/64 loss: -0.0991165041923523
Batch 59/64 loss: -0.11140751838684082
Batch 60/64 loss: -0.11763530969619751
Batch 61/64 loss: -0.1218339204788208
Batch 62/64 loss: -0.13151323795318604
Batch 63/64 loss: -0.11520504951477051
Batch 64/64 loss: -0.10489332675933838
Epoch 210  Train loss: -0.11337707510181502  Val loss: -0.04576799550007299
Epoch 211
-------------------------------
Batch 1/64 loss: -0.09627765417098999
Batch 2/64 loss: -0.11829882860183716
Batch 3/64 loss: -0.11669886112213135
Batch 4/64 loss: -0.10782104730606079
Batch 5/64 loss: -0.12459725141525269
Batch 6/64 loss: -0.12178802490234375
Batch 7/64 loss: -0.11359280347824097
Batch 8/64 loss: -0.11915051937103271
Batch 9/64 loss: -0.10133552551269531
Batch 10/64 loss: -0.12909865379333496
Batch 11/64 loss: -0.07918274402618408
Batch 12/64 loss: -0.11924934387207031
Batch 13/64 loss: -0.13765555620193481
Batch 14/64 loss: -0.11800730228424072
Batch 15/64 loss: -0.11034905910491943
Batch 16/64 loss: -0.08865976333618164
Batch 17/64 loss: -0.09975910186767578
Batch 18/64 loss: -0.11559516191482544
Batch 19/64 loss: -0.12689584493637085
Batch 20/64 loss: -0.09580940008163452
Batch 21/64 loss: -0.12985152006149292
Batch 22/64 loss: -0.12575864791870117
Batch 23/64 loss: -0.11924821138381958
Batch 24/64 loss: -0.118114173412323
Batch 25/64 loss: -0.11111730337142944
Batch 26/64 loss: -0.11093556880950928
Batch 27/64 loss: -0.10846495628356934
Batch 28/64 loss: -0.09329748153686523
Batch 29/64 loss: -0.136041522026062
Batch 30/64 loss: -0.12685495615005493
Batch 31/64 loss: -0.1116095781326294
Batch 32/64 loss: -0.12422627210617065
Batch 33/64 loss: -0.12412476539611816
Batch 34/64 loss: -0.12705987691879272
Batch 35/64 loss: -0.11382675170898438
Batch 36/64 loss: -0.10641294717788696
Batch 37/64 loss: -0.12121075391769409
Batch 38/64 loss: -0.12029093503952026
Batch 39/64 loss: -0.11974978446960449
Batch 40/64 loss: -0.1316370964050293
Batch 41/64 loss: -0.10380542278289795
Batch 42/64 loss: -0.1186830997467041
Batch 43/64 loss: -0.08849447965621948
Batch 44/64 loss: -0.09935736656188965
Batch 45/64 loss: -0.10820692777633667
Batch 46/64 loss: -0.10916298627853394
Batch 47/64 loss: -0.10762405395507812
Batch 48/64 loss: -0.11605393886566162
Batch 49/64 loss: -0.11711406707763672
Batch 50/64 loss: -0.09404581785202026
Batch 51/64 loss: -0.12136805057525635
Batch 52/64 loss: -0.12193650007247925
Batch 53/64 loss: -0.11160117387771606
Batch 54/64 loss: -0.13025766611099243
Batch 55/64 loss: -0.10745370388031006
Batch 56/64 loss: -0.10757315158843994
Batch 57/64 loss: -0.1370786428451538
Batch 58/64 loss: -0.12376904487609863
Batch 59/64 loss: -0.12363934516906738
Batch 60/64 loss: -0.11567497253417969
Batch 61/64 loss: -0.10786688327789307
Batch 62/64 loss: -0.1189236044883728
Batch 63/64 loss: -0.10203427076339722
Batch 64/64 loss: -0.10407042503356934
Epoch 211  Train loss: -0.11434405551237219  Val loss: -0.04795680259101579
Saving best model, epoch: 211
Epoch 212
-------------------------------
Batch 1/64 loss: -0.09503626823425293
Batch 2/64 loss: -0.11386442184448242
Batch 3/64 loss: -0.12812286615371704
Batch 4/64 loss: -0.1346966028213501
Batch 5/64 loss: -0.13025182485580444
Batch 6/64 loss: -0.13136029243469238
Batch 7/64 loss: -0.1127704381942749
Batch 8/64 loss: -0.12385773658752441
Batch 9/64 loss: -0.12071645259857178
Batch 10/64 loss: -0.1057233214378357
Batch 11/64 loss: -0.12861520051956177
Batch 12/64 loss: -0.09914910793304443
Batch 13/64 loss: -0.11709415912628174
Batch 14/64 loss: -0.12727129459381104
Batch 15/64 loss: -0.10691851377487183
Batch 16/64 loss: -0.111095130443573
Batch 17/64 loss: -0.13150280714035034
Batch 18/64 loss: -0.11238563060760498
Batch 19/64 loss: -0.12540626525878906
Batch 20/64 loss: -0.09685158729553223
Batch 21/64 loss: -0.12524735927581787
Batch 22/64 loss: -0.09874707460403442
Batch 23/64 loss: -0.12682878971099854
Batch 24/64 loss: -0.11960834264755249
Batch 25/64 loss: -0.120086669921875
Batch 26/64 loss: -0.13015705347061157
Batch 27/64 loss: -0.10145771503448486
Batch 28/64 loss: -0.11795765161514282
Batch 29/64 loss: -0.12282860279083252
Batch 30/64 loss: -0.1271257996559143
Batch 31/64 loss: -0.12370187044143677
Batch 32/64 loss: -0.09953510761260986
Batch 33/64 loss: -0.1040734052658081
Batch 34/64 loss: -0.11329472064971924
Batch 35/64 loss: -0.08266651630401611
Batch 36/64 loss: -0.08661162853240967
Batch 37/64 loss: -0.12049388885498047
Batch 38/64 loss: -0.11312448978424072
Batch 39/64 loss: -0.11076772212982178
Batch 40/64 loss: -0.09998905658721924
Batch 41/64 loss: -0.11394810676574707
Batch 42/64 loss: -0.1236119270324707
Batch 43/64 loss: -0.09932857751846313
Batch 44/64 loss: -0.11016786098480225
Batch 45/64 loss: -0.11536359786987305
Batch 46/64 loss: -0.12188076972961426
Batch 47/64 loss: -0.11041492223739624
Batch 48/64 loss: -0.11935752630233765
Batch 49/64 loss: -0.11292785406112671
Batch 50/64 loss: -0.12614428997039795
Batch 51/64 loss: -0.1224249005317688
Batch 52/64 loss: -0.11197876930236816
Batch 53/64 loss: -0.11779248714447021
Batch 54/64 loss: -0.12544512748718262
Batch 55/64 loss: -0.10749804973602295
Batch 56/64 loss: -0.12145763635635376
Batch 57/64 loss: -0.12747567892074585
Batch 58/64 loss: -0.12079471349716187
Batch 59/64 loss: -0.1010621190071106
Batch 60/64 loss: -0.10283792018890381
Batch 61/64 loss: -0.12203121185302734
Batch 62/64 loss: -0.1013217568397522
Batch 63/64 loss: -0.1257878541946411
Batch 64/64 loss: -0.10777151584625244
Epoch 212  Train loss: -0.1151196185280295  Val loss: -0.04542281762840822
Epoch 213
-------------------------------
Batch 1/64 loss: -0.11610198020935059
Batch 2/64 loss: -0.1275760531425476
Batch 3/64 loss: -0.12122893333435059
Batch 4/64 loss: -0.08623552322387695
Batch 5/64 loss: -0.12010294198989868
Batch 6/64 loss: -0.11655443906784058
Batch 7/64 loss: -0.10939294099807739
Batch 8/64 loss: -0.13468146324157715
Batch 9/64 loss: -0.11147761344909668
Batch 10/64 loss: -0.14211857318878174
Batch 11/64 loss: -0.12733721733093262
Batch 12/64 loss: -0.13029223680496216
Batch 13/64 loss: -0.11778664588928223
Batch 14/64 loss: -0.10510414838790894
Batch 15/64 loss: -0.12060898542404175
Batch 16/64 loss: -0.12539851665496826
Batch 17/64 loss: -0.12812060117721558
Batch 18/64 loss: -0.12220942974090576
Batch 19/64 loss: -0.12968695163726807
Batch 20/64 loss: -0.11067426204681396
Batch 21/64 loss: -0.12399715185165405
Batch 22/64 loss: -0.09502851963043213
Batch 23/64 loss: -0.10640716552734375
Batch 24/64 loss: -0.1276291012763977
Batch 25/64 loss: -0.12243592739105225
Batch 26/64 loss: -0.10319113731384277
Batch 27/64 loss: -0.12265539169311523
Batch 28/64 loss: -0.10520809888839722
Batch 29/64 loss: -0.10173499584197998
Batch 30/64 loss: -0.09821581840515137
Batch 31/64 loss: -0.12713390588760376
Batch 32/64 loss: -0.12442946434020996
Batch 33/64 loss: -0.1200937032699585
Batch 34/64 loss: -0.10907328128814697
Batch 35/64 loss: -0.1130940318107605
Batch 36/64 loss: -0.12429225444793701
Batch 37/64 loss: -0.11800146102905273
Batch 38/64 loss: -0.11047273874282837
Batch 39/64 loss: -0.10996288061141968
Batch 40/64 loss: -0.12516605854034424
Batch 41/64 loss: -0.09111154079437256
Batch 42/64 loss: -0.12487965822219849
Batch 43/64 loss: -0.12629032135009766
Batch 44/64 loss: -0.0933457612991333
Batch 45/64 loss: -0.1110125184059143
Batch 46/64 loss: -0.12911659479141235
Batch 47/64 loss: -0.1287623643875122
Batch 48/64 loss: -0.11646753549575806
Batch 49/64 loss: -0.12866908311843872
Batch 50/64 loss: -0.10384035110473633
Batch 51/64 loss: -0.101715087890625
Batch 52/64 loss: -0.10131299495697021
Batch 53/64 loss: -0.08619606494903564
Batch 54/64 loss: -0.11312693357467651
Batch 55/64 loss: -0.11866164207458496
Batch 56/64 loss: -0.10512775182723999
Batch 57/64 loss: -0.08511066436767578
Batch 58/64 loss: -0.10425329208374023
Batch 59/64 loss: -0.12752586603164673
Batch 60/64 loss: -0.14059579372406006
Batch 61/64 loss: -0.09669941663742065
Batch 62/64 loss: -0.12798333168029785
Batch 63/64 loss: -0.11356443166732788
Batch 64/64 loss: -0.1282917857170105
Epoch 213  Train loss: -0.11549020210901896  Val loss: -0.04531178175378911
Epoch 214
-------------------------------
Batch 1/64 loss: -0.13716936111450195
Batch 2/64 loss: -0.12246918678283691
Batch 3/64 loss: -0.12474364042282104
Batch 4/64 loss: -0.11814326047897339
Batch 5/64 loss: -0.12637364864349365
Batch 6/64 loss: -0.11134511232376099
Batch 7/64 loss: -0.13348805904388428
Batch 8/64 loss: -0.12123233079910278
Batch 9/64 loss: -0.1250377893447876
Batch 10/64 loss: -0.11641532182693481
Batch 11/64 loss: -0.11269229650497437
Batch 12/64 loss: -0.11195838451385498
Batch 13/64 loss: -0.133103609085083
Batch 14/64 loss: -0.116965651512146
Batch 15/64 loss: -0.10092133283615112
Batch 16/64 loss: -0.10953617095947266
Batch 17/64 loss: -0.13836669921875
Batch 18/64 loss: -0.12375575304031372
Batch 19/64 loss: -0.10606634616851807
Batch 20/64 loss: -0.12034720182418823
Batch 21/64 loss: -0.12583112716674805
Batch 22/64 loss: -0.12121236324310303
Batch 23/64 loss: -0.12419623136520386
Batch 24/64 loss: -0.12515276670455933
Batch 25/64 loss: -0.11506837606430054
Batch 26/64 loss: -0.11532872915267944
Batch 27/64 loss: -0.10711038112640381
Batch 28/64 loss: -0.12027406692504883
Batch 29/64 loss: -0.11583125591278076
Batch 30/64 loss: -0.11290860176086426
Batch 31/64 loss: -0.12411725521087646
Batch 32/64 loss: -0.12519186735153198
Batch 33/64 loss: -0.12266874313354492
Batch 34/64 loss: -0.10069739818572998
Batch 35/64 loss: -0.12520653009414673
Batch 36/64 loss: -0.10752785205841064
Batch 37/64 loss: -0.10025113821029663
Batch 38/64 loss: -0.12086313962936401
Batch 39/64 loss: -0.10137099027633667
Batch 40/64 loss: -0.12352609634399414
Batch 41/64 loss: -0.13026028871536255
Batch 42/64 loss: -0.10838252305984497
Batch 43/64 loss: -0.13364505767822266
Batch 44/64 loss: -0.10182404518127441
Batch 45/64 loss: -0.11754918098449707
Batch 46/64 loss: -0.1124122142791748
Batch 47/64 loss: -0.12612920999526978
Batch 48/64 loss: -0.12532079219818115
Batch 49/64 loss: -0.12307119369506836
Batch 50/64 loss: -0.08952420949935913
Batch 51/64 loss: -0.12475854158401489
Batch 52/64 loss: -0.09411180019378662
Batch 53/64 loss: -0.09977841377258301
Batch 54/64 loss: -0.08761340379714966
Batch 55/64 loss: -0.12752199172973633
Batch 56/64 loss: -0.12161362171173096
Batch 57/64 loss: -0.14027857780456543
Batch 58/64 loss: -0.1068069338798523
Batch 59/64 loss: -0.10896730422973633
Batch 60/64 loss: -0.12780451774597168
Batch 61/64 loss: -0.12127751111984253
Batch 62/64 loss: -0.11285185813903809
Batch 63/64 loss: -0.12118208408355713
Batch 64/64 loss: -0.12785708904266357
Epoch 214  Train loss: -0.11769481032502417  Val loss: -0.047725296307265554
Epoch 215
-------------------------------
Batch 1/64 loss: -0.13839823007583618
Batch 2/64 loss: -0.10332262516021729
Batch 3/64 loss: -0.13244402408599854
Batch 4/64 loss: -0.12292003631591797
Batch 5/64 loss: -0.10467654466629028
Batch 6/64 loss: -0.13896214962005615
Batch 7/64 loss: -0.11082494258880615
Batch 8/64 loss: -0.1251230239868164
Batch 9/64 loss: -0.12196344137191772
Batch 10/64 loss: -0.12600266933441162
Batch 11/64 loss: -0.13078391551971436
Batch 12/64 loss: -0.12399256229400635
Batch 13/64 loss: -0.13168865442276
Batch 14/64 loss: -0.11916160583496094
Batch 15/64 loss: -0.10291993618011475
Batch 16/64 loss: -0.12364834547042847
Batch 17/64 loss: -0.08593732118606567
Batch 18/64 loss: -0.12000477313995361
Batch 19/64 loss: -0.12546956539154053
Batch 20/64 loss: -0.1290082335472107
Batch 21/64 loss: -0.12201553583145142
Batch 22/64 loss: -0.10738891363143921
Batch 23/64 loss: -0.10736727714538574
Batch 24/64 loss: -0.12256264686584473
Batch 25/64 loss: -0.11104440689086914
Batch 26/64 loss: -0.11443620920181274
Batch 27/64 loss: -0.11376434564590454
Batch 28/64 loss: -0.1170048713684082
Batch 29/64 loss: -0.08631294965744019
Batch 30/64 loss: -0.12052738666534424
Batch 31/64 loss: -0.12414002418518066
Batch 32/64 loss: -0.09715139865875244
Batch 33/64 loss: -0.0923086404800415
Batch 34/64 loss: -0.1156611442565918
Batch 35/64 loss: -0.08549177646636963
Batch 36/64 loss: -0.13275140523910522
Batch 37/64 loss: -0.10974657535552979
Batch 38/64 loss: -0.10550975799560547
Batch 39/64 loss: -0.10648304224014282
Batch 40/64 loss: -0.12485504150390625
Batch 41/64 loss: -0.12571215629577637
Batch 42/64 loss: -0.11629462242126465
Batch 43/64 loss: -0.12704724073410034
Batch 44/64 loss: -0.12545067071914673
Batch 45/64 loss: -0.10725784301757812
Batch 46/64 loss: -0.09734499454498291
Batch 47/64 loss: -0.11861246824264526
Batch 48/64 loss: -0.12238752841949463
Batch 49/64 loss: -0.12080544233322144
Batch 50/64 loss: -0.10229629278182983
Batch 51/64 loss: -0.11245250701904297
Batch 52/64 loss: -0.11479127407073975
Batch 53/64 loss: -0.13076555728912354
Batch 54/64 loss: -0.10511553287506104
Batch 55/64 loss: -0.13161152601242065
Batch 56/64 loss: -0.112007737159729
Batch 57/64 loss: -0.11522328853607178
Batch 58/64 loss: -0.1365630030632019
Batch 59/64 loss: -0.12422078847885132
Batch 60/64 loss: -0.10613822937011719
Batch 61/64 loss: -0.12667787075042725
Batch 62/64 loss: -0.11428636312484741
Batch 63/64 loss: -0.12409031391143799
Batch 64/64 loss: -0.06081104278564453
Epoch 215  Train loss: -0.11602411737629012  Val loss: -0.044560398227980046
Epoch 216
-------------------------------
Batch 1/64 loss: -0.10627233982086182
Batch 2/64 loss: -0.13373947143554688
Batch 3/64 loss: -0.12593573331832886
Batch 4/64 loss: -0.11986851692199707
Batch 5/64 loss: -0.11754560470581055
Batch 6/64 loss: -0.11868089437484741
Batch 7/64 loss: -0.1213940978050232
Batch 8/64 loss: -0.10834288597106934
Batch 9/64 loss: -0.10056853294372559
Batch 10/64 loss: -0.09694057703018188
Batch 11/64 loss: -0.1113014817237854
Batch 12/64 loss: -0.12661057710647583
Batch 13/64 loss: -0.1154138445854187
Batch 14/64 loss: -0.11526387929916382
Batch 15/64 loss: -0.13179504871368408
Batch 16/64 loss: -0.13332128524780273
Batch 17/64 loss: -0.12804079055786133
Batch 18/64 loss: -0.11598503589630127
Batch 19/64 loss: -0.11850661039352417
Batch 20/64 loss: -0.13176512718200684
Batch 21/64 loss: -0.1309792399406433
Batch 22/64 loss: -0.13065719604492188
Batch 23/64 loss: -0.09630149602890015
Batch 24/64 loss: -0.1362149715423584
Batch 25/64 loss: -0.12257713079452515
Batch 26/64 loss: -0.1052025556564331
Batch 27/64 loss: -0.1333654522895813
Batch 28/64 loss: -0.12650281190872192
Batch 29/64 loss: -0.12133502960205078
Batch 30/64 loss: -0.125893235206604
Batch 31/64 loss: -0.11653709411621094
Batch 32/64 loss: -0.1390567421913147
Batch 33/64 loss: -0.1395401954650879
Batch 34/64 loss: -0.12845361232757568
Batch 35/64 loss: -0.1382703185081482
Batch 36/64 loss: -0.10674393177032471
Batch 37/64 loss: -0.1464327573776245
Batch 38/64 loss: -0.12869060039520264
Batch 39/64 loss: -0.13767820596694946
Batch 40/64 loss: -0.11374074220657349
Batch 41/64 loss: -0.13002991676330566
Batch 42/64 loss: -0.11039793491363525
Batch 43/64 loss: -0.12938332557678223
Batch 44/64 loss: -0.1284845471382141
Batch 45/64 loss: -0.11764353513717651
Batch 46/64 loss: -0.12086313962936401
Batch 47/64 loss: -0.12653112411499023
Batch 48/64 loss: -0.12004512548446655
Batch 49/64 loss: -0.08604854345321655
Batch 50/64 loss: -0.12467527389526367
Batch 51/64 loss: -0.11321514844894409
Batch 52/64 loss: -0.10951501131057739
Batch 53/64 loss: -0.12613457441329956
Batch 54/64 loss: -0.11816859245300293
Batch 55/64 loss: -0.1285797357559204
Batch 56/64 loss: -0.11678016185760498
Batch 57/64 loss: -0.10976582765579224
Batch 58/64 loss: -0.10055041313171387
Batch 59/64 loss: -0.11246693134307861
Batch 60/64 loss: -0.11862772703170776
Batch 61/64 loss: -0.12886005640029907
Batch 62/64 loss: -0.12479817867279053
Batch 63/64 loss: -0.10268557071685791
Batch 64/64 loss: -0.1065678596496582
Epoch 216  Train loss: -0.1205590893240536  Val loss: -0.04593713689096195
Epoch 217
-------------------------------
Batch 1/64 loss: -0.1180841326713562
Batch 2/64 loss: -0.12805521488189697
Batch 3/64 loss: -0.11947637796401978
Batch 4/64 loss: -0.12037253379821777
Batch 5/64 loss: -0.1375885009765625
Batch 6/64 loss: -0.10963648557662964
Batch 7/64 loss: -0.13726305961608887
Batch 8/64 loss: -0.12568581104278564
Batch 9/64 loss: -0.13153529167175293
Batch 10/64 loss: -0.11629390716552734
Batch 11/64 loss: -0.12211644649505615
Batch 12/64 loss: -0.1047016978263855
Batch 13/64 loss: -0.11378812789916992
Batch 14/64 loss: -0.11314952373504639
Batch 15/64 loss: -0.11552542448043823
Batch 16/64 loss: -0.12095993757247925
Batch 17/64 loss: -0.1175912618637085
Batch 18/64 loss: -0.1344529390335083
Batch 19/64 loss: -0.12302345037460327
Batch 20/64 loss: -0.11333668231964111
Batch 21/64 loss: -0.12416714429855347
Batch 22/64 loss: -0.10954314470291138
Batch 23/64 loss: -0.12226825952529907
Batch 24/64 loss: -0.12498009204864502
Batch 25/64 loss: -0.133403480052948
Batch 26/64 loss: -0.11326849460601807
Batch 27/64 loss: -0.12648612260818481
Batch 28/64 loss: -0.1171572208404541
Batch 29/64 loss: -0.11114269495010376
Batch 30/64 loss: -0.12148678302764893
Batch 31/64 loss: -0.11650139093399048
Batch 32/64 loss: -0.08016097545623779
Batch 33/64 loss: -0.09226465225219727
Batch 34/64 loss: -0.10732787847518921
Batch 35/64 loss: -0.12823379039764404
Batch 36/64 loss: -0.12519842386245728
Batch 37/64 loss: -0.1258808970451355
Batch 38/64 loss: -0.1327124834060669
Batch 39/64 loss: -0.10745257139205933
Batch 40/64 loss: -0.13851386308670044
Batch 41/64 loss: -0.11785674095153809
Batch 42/64 loss: -0.12763798236846924
Batch 43/64 loss: -0.12548762559890747
Batch 44/64 loss: -0.11379742622375488
Batch 45/64 loss: -0.13708531856536865
Batch 46/64 loss: -0.10440659523010254
Batch 47/64 loss: -0.10932821035385132
Batch 48/64 loss: -0.12755000591278076
Batch 49/64 loss: -0.12022984027862549
Batch 50/64 loss: -0.11616933345794678
Batch 51/64 loss: -0.12793612480163574
Batch 52/64 loss: -0.11163586378097534
Batch 53/64 loss: -0.12546026706695557
Batch 54/64 loss: -0.10596734285354614
Batch 55/64 loss: -0.12243032455444336
Batch 56/64 loss: -0.12091106176376343
Batch 57/64 loss: -0.12427288293838501
Batch 58/64 loss: -0.09806424379348755
Batch 59/64 loss: -0.11169922351837158
Batch 60/64 loss: -0.12052673101425171
Batch 61/64 loss: -0.1250845193862915
Batch 62/64 loss: -0.11711716651916504
Batch 63/64 loss: -0.1272796392440796
Batch 64/64 loss: -0.13273382186889648
Epoch 217  Train loss: -0.11950230598449707  Val loss: -0.04785853542413089
Epoch 218
-------------------------------
Batch 1/64 loss: -0.13491439819335938
Batch 2/64 loss: -0.13854098320007324
Batch 3/64 loss: -0.12190234661102295
Batch 4/64 loss: -0.1294516921043396
Batch 5/64 loss: -0.11807221174240112
Batch 6/64 loss: -0.1362115740776062
Batch 7/64 loss: -0.11221045255661011
Batch 8/64 loss: -0.10798871517181396
Batch 9/64 loss: -0.13221198320388794
Batch 10/64 loss: -0.10299861431121826
Batch 11/64 loss: -0.12224388122558594
Batch 12/64 loss: -0.12345004081726074
Batch 13/64 loss: -0.11986935138702393
Batch 14/64 loss: -0.13452696800231934
Batch 15/64 loss: -0.11177587509155273
Batch 16/64 loss: -0.12239521741867065
Batch 17/64 loss: -0.13782745599746704
Batch 18/64 loss: -0.1280706524848938
Batch 19/64 loss: -0.1143503189086914
Batch 20/64 loss: -0.10914754867553711
Batch 21/64 loss: -0.12794411182403564
Batch 22/64 loss: -0.11261355876922607
Batch 23/64 loss: -0.12538748979568481
Batch 24/64 loss: -0.1318807601928711
Batch 25/64 loss: -0.1175161600112915
Batch 26/64 loss: -0.10414552688598633
Batch 27/64 loss: -0.10093963146209717
Batch 28/64 loss: -0.11920249462127686
Batch 29/64 loss: -0.13143670558929443
Batch 30/64 loss: -0.10790461301803589
Batch 31/64 loss: -0.1137513518333435
Batch 32/64 loss: -0.13060086965560913
Batch 33/64 loss: -0.11728465557098389
Batch 34/64 loss: -0.11336630582809448
Batch 35/64 loss: -0.13079917430877686
Batch 36/64 loss: -0.11339777708053589
Batch 37/64 loss: -0.11451447010040283
Batch 38/64 loss: -0.11176872253417969
Batch 39/64 loss: -0.12751519680023193
Batch 40/64 loss: -0.0886467695236206
Batch 41/64 loss: -0.11693298816680908
Batch 42/64 loss: -0.11251950263977051
Batch 43/64 loss: -0.12779688835144043
Batch 44/64 loss: -0.11121761798858643
Batch 45/64 loss: -0.11372095346450806
Batch 46/64 loss: -0.13435840606689453
Batch 47/64 loss: -0.1240893006324768
Batch 48/64 loss: -0.10631358623504639
Batch 49/64 loss: -0.12203997373580933
Batch 50/64 loss: -0.1153101921081543
Batch 51/64 loss: -0.10149765014648438
Batch 52/64 loss: -0.1106492280960083
Batch 53/64 loss: -0.09732788801193237
Batch 54/64 loss: -0.12665629386901855
Batch 55/64 loss: -0.127888023853302
Batch 56/64 loss: -0.12518155574798584
Batch 57/64 loss: -0.09732800722122192
Batch 58/64 loss: -0.11345469951629639
Batch 59/64 loss: -0.13339966535568237
Batch 60/64 loss: -0.12589937448501587
Batch 61/64 loss: -0.13360106945037842
Batch 62/64 loss: -0.12085914611816406
Batch 63/64 loss: -0.11761140823364258
Batch 64/64 loss: -0.12192082405090332
Epoch 218  Train loss: -0.11930777512344659  Val loss: -0.04476630790127102
Epoch 219
-------------------------------
Batch 1/64 loss: -0.11903399229049683
Batch 2/64 loss: -0.1039208173751831
Batch 3/64 loss: -0.10967051982879639
Batch 4/64 loss: -0.1258997917175293
Batch 5/64 loss: -0.12657403945922852
Batch 6/64 loss: -0.11276674270629883
Batch 7/64 loss: -0.11858862638473511
Batch 8/64 loss: -0.10129982233047485
Batch 9/64 loss: -0.11060869693756104
Batch 10/64 loss: -0.1356678009033203
Batch 11/64 loss: -0.12598222494125366
Batch 12/64 loss: -0.1340814232826233
Batch 13/64 loss: -0.12488925457000732
Batch 14/64 loss: -0.1155431866645813
Batch 15/64 loss: -0.12729692459106445
Batch 16/64 loss: -0.11705613136291504
Batch 17/64 loss: -0.1237383484840393
Batch 18/64 loss: -0.15114754438400269
Batch 19/64 loss: -0.11448812484741211
Batch 20/64 loss: -0.13062655925750732
Batch 21/64 loss: -0.12883925437927246
Batch 22/64 loss: -0.13128662109375
Batch 23/64 loss: -0.1254359483718872
Batch 24/64 loss: -0.13334715366363525
Batch 25/64 loss: -0.1390969157218933
Batch 26/64 loss: -0.103019118309021
Batch 27/64 loss: -0.12986677885055542
Batch 28/64 loss: -0.0965394377708435
Batch 29/64 loss: -0.11275780200958252
Batch 30/64 loss: -0.10291695594787598
Batch 31/64 loss: -0.11737841367721558
Batch 32/64 loss: -0.13354718685150146
Batch 33/64 loss: -0.12290060520172119
Batch 34/64 loss: -0.11226266622543335
Batch 35/64 loss: -0.13296526670455933
Batch 36/64 loss: -0.11212384700775146
Batch 37/64 loss: -0.1191146969795227
Batch 38/64 loss: -0.12296593189239502
Batch 39/64 loss: -0.11980539560317993
Batch 40/64 loss: -0.12516844272613525
Batch 41/64 loss: -0.11672073602676392
Batch 42/64 loss: -0.11941063404083252
Batch 43/64 loss: -0.12169253826141357
Batch 44/64 loss: -0.10649234056472778
Batch 45/64 loss: -0.11983680725097656
Batch 46/64 loss: -0.11430096626281738
Batch 47/64 loss: -0.1235126256942749
Batch 48/64 loss: -0.11519300937652588
Batch 49/64 loss: -0.12493979930877686
Batch 50/64 loss: -0.1253066062927246
Batch 51/64 loss: -0.13015532493591309
Batch 52/64 loss: -0.1329028606414795
Batch 53/64 loss: -0.12319052219390869
Batch 54/64 loss: -0.12733906507492065
Batch 55/64 loss: -0.12510454654693604
Batch 56/64 loss: -0.11423599720001221
Batch 57/64 loss: -0.11946624517440796
Batch 58/64 loss: -0.11257493495941162
Batch 59/64 loss: -0.12327736616134644
Batch 60/64 loss: -0.10951614379882812
Batch 61/64 loss: -0.09821981191635132
Batch 62/64 loss: -0.10880440473556519
Batch 63/64 loss: -0.11806529760360718
Batch 64/64 loss: -0.1134408712387085
Epoch 219  Train loss: -0.12024405096091476  Val loss: -0.0450205931958464
Epoch 220
-------------------------------
Batch 1/64 loss: -0.10772347450256348
Batch 2/64 loss: -0.13469862937927246
Batch 3/64 loss: -0.1327686309814453
Batch 4/64 loss: -0.13681501150131226
Batch 5/64 loss: -0.10255122184753418
Batch 6/64 loss: -0.1299532651901245
Batch 7/64 loss: -0.12063133716583252
Batch 8/64 loss: -0.1272284984588623
Batch 9/64 loss: -0.11136096715927124
Batch 10/64 loss: -0.14437514543533325
Batch 11/64 loss: -0.13553154468536377
Batch 12/64 loss: -0.1281372308731079
Batch 13/64 loss: -0.1300402283668518
Batch 14/64 loss: -0.09137922525405884
Batch 15/64 loss: -0.1271982192993164
Batch 16/64 loss: -0.10651141405105591
Batch 17/64 loss: -0.11104995012283325
Batch 18/64 loss: -0.13377153873443604
Batch 19/64 loss: -0.13801050186157227
Batch 20/64 loss: -0.12151855230331421
Batch 21/64 loss: -0.12532544136047363
Batch 22/64 loss: -0.10064494609832764
Batch 23/64 loss: -0.12176299095153809
Batch 24/64 loss: -0.13109558820724487
Batch 25/64 loss: -0.10918092727661133
Batch 26/64 loss: -0.11319124698638916
Batch 27/64 loss: -0.12728852033615112
Batch 28/64 loss: -0.1018599271774292
Batch 29/64 loss: -0.12089955806732178
Batch 30/64 loss: -0.10058140754699707
Batch 31/64 loss: -0.1321645975112915
Batch 32/64 loss: -0.10971999168395996
Batch 33/64 loss: -0.12421447038650513
Batch 34/64 loss: -0.10678303241729736
Batch 35/64 loss: -0.13428246974945068
Batch 36/64 loss: -0.1144152283668518
Batch 37/64 loss: -0.09494251012802124
Batch 38/64 loss: -0.12374001741409302
Batch 39/64 loss: -0.11410218477249146
Batch 40/64 loss: -0.1317998170852661
Batch 41/64 loss: -0.12257713079452515
Batch 42/64 loss: -0.10746335983276367
Batch 43/64 loss: -0.11453115940093994
Batch 44/64 loss: -0.09894263744354248
Batch 45/64 loss: -0.12249863147735596
Batch 46/64 loss: -0.13318634033203125
Batch 47/64 loss: -0.12816286087036133
Batch 48/64 loss: -0.11643058061599731
Batch 49/64 loss: -0.13147872686386108
Batch 50/64 loss: -0.10158514976501465
Batch 51/64 loss: -0.11309939622879028
Batch 52/64 loss: -0.11672919988632202
Batch 53/64 loss: -0.1184343695640564
Batch 54/64 loss: -0.1190459132194519
Batch 55/64 loss: -0.10930496454238892
Batch 56/64 loss: -0.12302958965301514
Batch 57/64 loss: -0.11351281404495239
Batch 58/64 loss: -0.13644498586654663
Batch 59/64 loss: -0.10552978515625
Batch 60/64 loss: -0.11957228183746338
Batch 61/64 loss: -0.137440025806427
Batch 62/64 loss: -0.13992077112197876
Batch 63/64 loss: -0.1316589117050171
Batch 64/64 loss: -0.12099909782409668
Epoch 220  Train loss: -0.12016593521716548  Val loss: -0.046487298208413665
Epoch 221
-------------------------------
Batch 1/64 loss: -0.12920475006103516
Batch 2/64 loss: -0.13250696659088135
Batch 3/64 loss: -0.11296123266220093
Batch 4/64 loss: -0.12260407209396362
Batch 5/64 loss: -0.13695496320724487
Batch 6/64 loss: -0.11289918422698975
Batch 7/64 loss: -0.13131862878799438
Batch 8/64 loss: -0.12342667579650879
Batch 9/64 loss: -0.11433762311935425
Batch 10/64 loss: -0.1416306495666504
Batch 11/64 loss: -0.11847013235092163
Batch 12/64 loss: -0.1387779712677002
Batch 13/64 loss: -0.13084125518798828
Batch 14/64 loss: -0.12035250663757324
Batch 15/64 loss: -0.1306389570236206
Batch 16/64 loss: -0.1348143219947815
Batch 17/64 loss: -0.13990598917007446
Batch 18/64 loss: -0.1456792950630188
Batch 19/64 loss: -0.1132548451423645
Batch 20/64 loss: -0.1039775013923645
Batch 21/64 loss: -0.1159171462059021
Batch 22/64 loss: -0.12329232692718506
Batch 23/64 loss: -0.1097404956817627
Batch 24/64 loss: -0.1395740509033203
Batch 25/64 loss: -0.12722522020339966
Batch 26/64 loss: -0.1417553424835205
Batch 27/64 loss: -0.13253986835479736
Batch 28/64 loss: -0.13206952810287476
Batch 29/64 loss: -0.1275869607925415
Batch 30/64 loss: -0.09662878513336182
Batch 31/64 loss: -0.13098114728927612
Batch 32/64 loss: -0.10063612461090088
Batch 33/64 loss: -0.1149371862411499
Batch 34/64 loss: -0.11855185031890869
Batch 35/64 loss: -0.11193394660949707
Batch 36/64 loss: -0.12336152791976929
Batch 37/64 loss: -0.10835576057434082
Batch 38/64 loss: -0.12119728326797485
Batch 39/64 loss: -0.12294250726699829
Batch 40/64 loss: -0.12722444534301758
Batch 41/64 loss: -0.11420726776123047
Batch 42/64 loss: -0.11172866821289062
Batch 43/64 loss: -0.11379492282867432
Batch 44/64 loss: -0.11940312385559082
Batch 45/64 loss: -0.09537357091903687
Batch 46/64 loss: -0.1256292462348938
Batch 47/64 loss: -0.10830831527709961
Batch 48/64 loss: -0.12498855590820312
Batch 49/64 loss: -0.1359720230102539
Batch 50/64 loss: -0.11021512746810913
Batch 51/64 loss: -0.10304182767868042
Batch 52/64 loss: -0.1115725040435791
Batch 53/64 loss: -0.12335419654846191
Batch 54/64 loss: -0.13196730613708496
Batch 55/64 loss: -0.10623151063919067
Batch 56/64 loss: -0.12197095155715942
Batch 57/64 loss: -0.11974620819091797
Batch 58/64 loss: -0.12611210346221924
Batch 59/64 loss: -0.10604846477508545
Batch 60/64 loss: -0.12332051992416382
Batch 61/64 loss: -0.12440496683120728
Batch 62/64 loss: -0.1265026330947876
Batch 63/64 loss: -0.12610775232315063
Batch 64/64 loss: -0.09664362668991089
Epoch 221  Train loss: -0.12146656489839741  Val loss: -0.044457304518657044
Epoch 222
-------------------------------
Batch 1/64 loss: -0.09751296043395996
Batch 2/64 loss: -0.1349109411239624
Batch 3/64 loss: -0.1358014941215515
Batch 4/64 loss: -0.1384754776954651
Batch 5/64 loss: -0.13127249479293823
Batch 6/64 loss: -0.12473464012145996
Batch 7/64 loss: -0.1270877718925476
Batch 8/64 loss: -0.11497169733047485
Batch 9/64 loss: -0.10105621814727783
Batch 10/64 loss: -0.12147533893585205
Batch 11/64 loss: -0.12378686666488647
Batch 12/64 loss: -0.11859393119812012
Batch 13/64 loss: -0.13574600219726562
Batch 14/64 loss: -0.11672890186309814
Batch 15/64 loss: -0.10450094938278198
Batch 16/64 loss: -0.10691696405410767
Batch 17/64 loss: -0.13569003343582153
Batch 18/64 loss: -0.13465172052383423
Batch 19/64 loss: -0.13394874334335327
Batch 20/64 loss: -0.10749417543411255
Batch 21/64 loss: -0.13475894927978516
Batch 22/64 loss: -0.12096458673477173
Batch 23/64 loss: -0.12214648723602295
Batch 24/64 loss: -0.13376927375793457
Batch 25/64 loss: -0.13816988468170166
Batch 26/64 loss: -0.11567133665084839
Batch 27/64 loss: -0.10477936267852783
Batch 28/64 loss: -0.12723064422607422
Batch 29/64 loss: -0.10422694683074951
Batch 30/64 loss: -0.10452020168304443
Batch 31/64 loss: -0.11888223886489868
Batch 32/64 loss: -0.11728775501251221
Batch 33/64 loss: -0.12741541862487793
Batch 34/64 loss: -0.13898897171020508
Batch 35/64 loss: -0.12932831048965454
Batch 36/64 loss: -0.12522035837173462
Batch 37/64 loss: -0.1349583864212036
Batch 38/64 loss: -0.11693501472473145
Batch 39/64 loss: -0.11516523361206055
Batch 40/64 loss: -0.1300411820411682
Batch 41/64 loss: -0.09740245342254639
Batch 42/64 loss: -0.11144119501113892
Batch 43/64 loss: -0.12632429599761963
Batch 44/64 loss: -0.10844427347183228
Batch 45/64 loss: -0.12509536743164062
Batch 46/64 loss: -0.11218583583831787
Batch 47/64 loss: -0.11639583110809326
Batch 48/64 loss: -0.1202426552772522
Batch 49/64 loss: -0.10279041528701782
Batch 50/64 loss: -0.13043224811553955
Batch 51/64 loss: -0.11726993322372437
Batch 52/64 loss: -0.10654330253601074
Batch 53/64 loss: -0.1061093807220459
Batch 54/64 loss: -0.11577349901199341
Batch 55/64 loss: -0.1053931713104248
Batch 56/64 loss: -0.12452572584152222
Batch 57/64 loss: -0.13418954610824585
Batch 58/64 loss: -0.13029152154922485
Batch 59/64 loss: -0.12148380279541016
Batch 60/64 loss: -0.12304770946502686
Batch 61/64 loss: -0.12801367044448853
Batch 62/64 loss: -0.12883436679840088
Batch 63/64 loss: -0.12414437532424927
Batch 64/64 loss: -0.10080885887145996
Epoch 222  Train loss: -0.1207497895932665  Val loss: -0.04598530952873099
Epoch 223
-------------------------------
Batch 1/64 loss: -0.1289781928062439
Batch 2/64 loss: -0.10231399536132812
Batch 3/64 loss: -0.1282745599746704
Batch 4/64 loss: -0.11612695455551147
Batch 5/64 loss: -0.12343466281890869
Batch 6/64 loss: -0.12305307388305664
Batch 7/64 loss: -0.1254117488861084
Batch 8/64 loss: -0.11403179168701172
Batch 9/64 loss: -0.12093794345855713
Batch 10/64 loss: -0.13194090127944946
Batch 11/64 loss: -0.13907265663146973
Batch 12/64 loss: -0.10801398754119873
Batch 13/64 loss: -0.12236499786376953
Batch 14/64 loss: -0.13530296087265015
Batch 15/64 loss: -0.13882529735565186
Batch 16/64 loss: -0.12949156761169434
Batch 17/64 loss: -0.1283705234527588
Batch 18/64 loss: -0.13486790657043457
Batch 19/64 loss: -0.12522095441818237
Batch 20/64 loss: -0.12946641445159912
Batch 21/64 loss: -0.1300647258758545
Batch 22/64 loss: -0.1473764181137085
Batch 23/64 loss: -0.13808166980743408
Batch 24/64 loss: -0.11934071779251099
Batch 25/64 loss: -0.13223665952682495
Batch 26/64 loss: -0.12777811288833618
Batch 27/64 loss: -0.10543251037597656
Batch 28/64 loss: -0.13535934686660767
Batch 29/64 loss: -0.11648952960968018
Batch 30/64 loss: -0.13576436042785645
Batch 31/64 loss: -0.11177623271942139
Batch 32/64 loss: -0.12953954935073853
Batch 33/64 loss: -0.12338799238204956
Batch 34/64 loss: -0.12719857692718506
Batch 35/64 loss: -0.1175156831741333
Batch 36/64 loss: -0.11022734642028809
Batch 37/64 loss: -0.11460161209106445
Batch 38/64 loss: -0.10822868347167969
Batch 39/64 loss: -0.11786633729934692
Batch 40/64 loss: -0.13275885581970215
Batch 41/64 loss: -0.11135828495025635
Batch 42/64 loss: -0.12481069564819336
Batch 43/64 loss: -0.13223016262054443
Batch 44/64 loss: -0.11516863107681274
Batch 45/64 loss: -0.13158220052719116
Batch 46/64 loss: -0.10599446296691895
Batch 47/64 loss: -0.12311750650405884
Batch 48/64 loss: -0.13291066884994507
Batch 49/64 loss: -0.09839844703674316
Batch 50/64 loss: -0.13416314125061035
Batch 51/64 loss: -0.13490545749664307
Batch 52/64 loss: -0.11431711912155151
Batch 53/64 loss: -0.13518553972244263
Batch 54/64 loss: -0.1333211064338684
Batch 55/64 loss: -0.1401945948600769
Batch 56/64 loss: -0.12360537052154541
Batch 57/64 loss: -0.13299745321273804
Batch 58/64 loss: -0.12958300113677979
Batch 59/64 loss: -0.13066375255584717
Batch 60/64 loss: -0.12871187925338745
Batch 61/64 loss: -0.11614620685577393
Batch 62/64 loss: -0.08027005195617676
Batch 63/64 loss: -0.12113922834396362
Batch 64/64 loss: -0.11774694919586182
Epoch 223  Train loss: -0.12400958725050384  Val loss: -0.04733448176039863
Epoch 224
-------------------------------
Batch 1/64 loss: -0.13080942630767822
Batch 2/64 loss: -0.12033957242965698
Batch 3/64 loss: -0.14215105772018433
Batch 4/64 loss: -0.11025941371917725
Batch 5/64 loss: -0.1301860809326172
Batch 6/64 loss: -0.09765076637268066
Batch 7/64 loss: -0.1389562487602234
Batch 8/64 loss: -0.10860508680343628
Batch 9/64 loss: -0.12785565853118896
Batch 10/64 loss: -0.1251431703567505
Batch 11/64 loss: -0.130179762840271
Batch 12/64 loss: -0.12325650453567505
Batch 13/64 loss: -0.09865713119506836
Batch 14/64 loss: -0.12978559732437134
Batch 15/64 loss: -0.1308211088180542
Batch 16/64 loss: -0.11368459463119507
Batch 17/64 loss: -0.12805265188217163
Batch 18/64 loss: -0.10967350006103516
Batch 19/64 loss: -0.10588353872299194
Batch 20/64 loss: -0.10114878416061401
Batch 21/64 loss: -0.12739747762680054
Batch 22/64 loss: -0.09536480903625488
Batch 23/64 loss: -0.13278913497924805
Batch 24/64 loss: -0.1414724588394165
Batch 25/64 loss: -0.12937796115875244
Batch 26/64 loss: -0.12559765577316284
Batch 27/64 loss: -0.13382971286773682
Batch 28/64 loss: -0.14042514562606812
Batch 29/64 loss: -0.12468641996383667
Batch 30/64 loss: -0.12068170309066772
Batch 31/64 loss: -0.13923215866088867
Batch 32/64 loss: -0.12393862009048462
Batch 33/64 loss: -0.1525680422782898
Batch 34/64 loss: -0.13315337896347046
Batch 35/64 loss: -0.1281915307044983
Batch 36/64 loss: -0.13696199655532837
Batch 37/64 loss: -0.1235206127166748
Batch 38/64 loss: -0.12679630517959595
Batch 39/64 loss: -0.11650288105010986
Batch 40/64 loss: -0.13649523258209229
Batch 41/64 loss: -0.11616742610931396
Batch 42/64 loss: -0.1350032091140747
Batch 43/64 loss: -0.1331443190574646
Batch 44/64 loss: -0.13991832733154297
Batch 45/64 loss: -0.09303003549575806
Batch 46/64 loss: -0.1162714958190918
Batch 47/64 loss: -0.12495434284210205
Batch 48/64 loss: -0.13318103551864624
Batch 49/64 loss: -0.11863988637924194
Batch 50/64 loss: -0.12899154424667358
Batch 51/64 loss: -0.13027596473693848
Batch 52/64 loss: -0.09598648548126221
Batch 53/64 loss: -0.13174307346343994
Batch 54/64 loss: -0.13178515434265137
Batch 55/64 loss: -0.11784696578979492
Batch 56/64 loss: -0.11608397960662842
Batch 57/64 loss: -0.13440537452697754
Batch 58/64 loss: -0.13181763887405396
Batch 59/64 loss: -0.12323230504989624
Batch 60/64 loss: -0.13771188259124756
Batch 61/64 loss: -0.12807810306549072
Batch 62/64 loss: -0.12682509422302246
Batch 63/64 loss: -0.0909339189529419
Batch 64/64 loss: -0.11152118444442749
Epoch 224  Train loss: -0.12410590344784307  Val loss: -0.041462115610588046
Epoch 225
-------------------------------
Batch 1/64 loss: -0.11588001251220703
Batch 2/64 loss: -0.1131696105003357
Batch 3/64 loss: -0.10323363542556763
Batch 4/64 loss: -0.11012321710586548
Batch 5/64 loss: -0.13201546669006348
Batch 6/64 loss: -0.1135910153388977
Batch 7/64 loss: -0.1249610185623169
Batch 8/64 loss: -0.1400611400604248
Batch 9/64 loss: -0.1041649580001831
Batch 10/64 loss: -0.12133932113647461
Batch 11/64 loss: -0.12097668647766113
Batch 12/64 loss: -0.11170673370361328
Batch 13/64 loss: -0.1323016881942749
Batch 14/64 loss: -0.11932533979415894
Batch 15/64 loss: -0.09499174356460571
Batch 16/64 loss: -0.121035635471344
Batch 17/64 loss: -0.12579739093780518
Batch 18/64 loss: -0.1373680830001831
Batch 19/64 loss: -0.11298924684524536
Batch 20/64 loss: -0.12008112668991089
Batch 21/64 loss: -0.12870031595230103
Batch 22/64 loss: -0.09559619426727295
Batch 23/64 loss: -0.13561725616455078
Batch 24/64 loss: -0.1098318099975586
Batch 25/64 loss: -0.1243857741355896
Batch 26/64 loss: -0.12506985664367676
Batch 27/64 loss: -0.13801848888397217
Batch 28/64 loss: -0.1371409296989441
Batch 29/64 loss: -0.132315993309021
Batch 30/64 loss: -0.11739760637283325
Batch 31/64 loss: -0.14259624481201172
Batch 32/64 loss: -0.10073459148406982
Batch 33/64 loss: -0.11020386219024658
Batch 34/64 loss: -0.12820792198181152
Batch 35/64 loss: -0.12345904111862183
Batch 36/64 loss: -0.11668956279754639
Batch 37/64 loss: -0.1278889775276184
Batch 38/64 loss: -0.1175227165222168
Batch 39/64 loss: -0.10922455787658691
Batch 40/64 loss: -0.13535434007644653
Batch 41/64 loss: -0.13101065158843994
Batch 42/64 loss: -0.14038145542144775
Batch 43/64 loss: -0.11325377225875854
Batch 44/64 loss: -0.11933910846710205
Batch 45/64 loss: -0.13439834117889404
Batch 46/64 loss: -0.13993078470230103
Batch 47/64 loss: -0.13655948638916016
Batch 48/64 loss: -0.1198192834854126
Batch 49/64 loss: -0.12524020671844482
Batch 50/64 loss: -0.11264872550964355
Batch 51/64 loss: -0.1310039758682251
Batch 52/64 loss: -0.12227052450180054
Batch 53/64 loss: -0.12516802549362183
Batch 54/64 loss: -0.11463344097137451
Batch 55/64 loss: -0.12476736307144165
Batch 56/64 loss: -0.13068515062332153
Batch 57/64 loss: -0.1174999475479126
Batch 58/64 loss: -0.10829520225524902
Batch 59/64 loss: -0.12515342235565186
Batch 60/64 loss: -0.12618666887283325
Batch 61/64 loss: -0.11784470081329346
Batch 62/64 loss: -0.10929208993911743
Batch 63/64 loss: -0.12589335441589355
Batch 64/64 loss: -0.12773555517196655
Epoch 225  Train loss: -0.12201014055925256  Val loss: -0.04298765470891474
Epoch 226
-------------------------------
Batch 1/64 loss: -0.1265971064567566
Batch 2/64 loss: -0.12382686138153076
Batch 3/64 loss: -0.11946946382522583
Batch 4/64 loss: -0.13547682762145996
Batch 5/64 loss: -0.14087820053100586
Batch 6/64 loss: -0.12361931800842285
Batch 7/64 loss: -0.12432235479354858
Batch 8/64 loss: -0.14442384243011475
Batch 9/64 loss: -0.14739203453063965
Batch 10/64 loss: -0.12313008308410645
Batch 11/64 loss: -0.13157546520233154
Batch 12/64 loss: -0.1210210919380188
Batch 13/64 loss: -0.12528598308563232
Batch 14/64 loss: -0.12125968933105469
Batch 15/64 loss: -0.1327837109565735
Batch 16/64 loss: -0.11666148900985718
Batch 17/64 loss: -0.11666196584701538
Batch 18/64 loss: -0.12900471687316895
Batch 19/64 loss: -0.14613085985183716
Batch 20/64 loss: -0.14669537544250488
Batch 21/64 loss: -0.12476444244384766
Batch 22/64 loss: -0.12551349401474
Batch 23/64 loss: -0.11475962400436401
Batch 24/64 loss: -0.11388272047042847
Batch 25/64 loss: -0.12232381105422974
Batch 26/64 loss: -0.12737995386123657
Batch 27/64 loss: -0.11683380603790283
Batch 28/64 loss: -0.12757790088653564
Batch 29/64 loss: -0.11817008256912231
Batch 30/64 loss: -0.11759799718856812
Batch 31/64 loss: -0.12901395559310913
Batch 32/64 loss: -0.10857176780700684
Batch 33/64 loss: -0.14068681001663208
Batch 34/64 loss: -0.13166660070419312
Batch 35/64 loss: -0.12385541200637817
Batch 36/64 loss: -0.11584138870239258
Batch 37/64 loss: -0.11347156763076782
Batch 38/64 loss: -0.11914581060409546
Batch 39/64 loss: -0.12730932235717773
Batch 40/64 loss: -0.10947316884994507
Batch 41/64 loss: -0.09656190872192383
Batch 42/64 loss: -0.12967157363891602
Batch 43/64 loss: -0.11608314514160156
Batch 44/64 loss: -0.12259811162948608
Batch 45/64 loss: -0.11947882175445557
Batch 46/64 loss: -0.1253347396850586
Batch 47/64 loss: -0.14263248443603516
Batch 48/64 loss: -0.10305070877075195
Batch 49/64 loss: -0.10083901882171631
Batch 50/64 loss: -0.13222169876098633
Batch 51/64 loss: -0.11765551567077637
Batch 52/64 loss: -0.11508035659790039
Batch 53/64 loss: -0.1329612135887146
Batch 54/64 loss: -0.12401914596557617
Batch 55/64 loss: -0.10796105861663818
Batch 56/64 loss: -0.1159239411354065
Batch 57/64 loss: -0.14553731679916382
Batch 58/64 loss: -0.1128392219543457
Batch 59/64 loss: -0.1341748833656311
Batch 60/64 loss: -0.12842392921447754
Batch 61/64 loss: -0.10944974422454834
Batch 62/64 loss: -0.11821115016937256
Batch 63/64 loss: -0.10044997930526733
Batch 64/64 loss: -0.1171112060546875
Epoch 226  Train loss: -0.123341947443345  Val loss: -0.04627693150051681
Epoch 227
-------------------------------
Batch 1/64 loss: -0.12629568576812744
Batch 2/64 loss: -0.1275591254234314
Batch 3/64 loss: -0.13129055500030518
Batch 4/64 loss: -0.13961172103881836
Batch 5/64 loss: -0.1296987533569336
Batch 6/64 loss: -0.09498047828674316
Batch 7/64 loss: -0.13028091192245483
Batch 8/64 loss: -0.1106230616569519
Batch 9/64 loss: -0.14247441291809082
Batch 10/64 loss: -0.10639345645904541
Batch 11/64 loss: -0.13348400592803955
Batch 12/64 loss: -0.14375382661819458
Batch 13/64 loss: -0.07395493984222412
Batch 14/64 loss: -0.12598001956939697
Batch 15/64 loss: -0.1447129249572754
Batch 16/64 loss: -0.10202300548553467
Batch 17/64 loss: -0.12796759605407715
Batch 18/64 loss: -0.11194920539855957
Batch 19/64 loss: -0.13249468803405762
Batch 20/64 loss: -0.12825381755828857
Batch 21/64 loss: -0.11632311344146729
Batch 22/64 loss: -0.13617634773254395
Batch 23/64 loss: -0.11301249265670776
Batch 24/64 loss: -0.1304745078086853
Batch 25/64 loss: -0.12976950407028198
Batch 26/64 loss: -0.13437795639038086
Batch 27/64 loss: -0.13106423616409302
Batch 28/64 loss: -0.13347584009170532
Batch 29/64 loss: -0.11518943309783936
Batch 30/64 loss: -0.09347933530807495
Batch 31/64 loss: -0.11232954263687134
Batch 32/64 loss: -0.11758124828338623
Batch 33/64 loss: -0.12549352645874023
Batch 34/64 loss: -0.12772011756896973
Batch 35/64 loss: -0.12879836559295654
Batch 36/64 loss: -0.12407279014587402
Batch 37/64 loss: -0.14171117544174194
Batch 38/64 loss: -0.12178409099578857
Batch 39/64 loss: -0.10588675737380981
Batch 40/64 loss: -0.1389409899711609
Batch 41/64 loss: -0.1298123598098755
Batch 42/64 loss: -0.13729852437973022
Batch 43/64 loss: -0.11004531383514404
Batch 44/64 loss: -0.1339203119277954
Batch 45/64 loss: -0.13569843769073486
Batch 46/64 loss: -0.14386999607086182
Batch 47/64 loss: -0.10903418064117432
Batch 48/64 loss: -0.11395126581192017
Batch 49/64 loss: -0.13102281093597412
Batch 50/64 loss: -0.1338934302330017
Batch 51/64 loss: -0.0939217209815979
Batch 52/64 loss: -0.12475568056106567
Batch 53/64 loss: -0.10760581493377686
Batch 54/64 loss: -0.13711512088775635
Batch 55/64 loss: -0.10125499963760376
Batch 56/64 loss: -0.12856602668762207
Batch 57/64 loss: -0.10462582111358643
Batch 58/64 loss: -0.1278012990951538
Batch 59/64 loss: -0.13068705797195435
Batch 60/64 loss: -0.12839680910110474
Batch 61/64 loss: -0.1269136667251587
Batch 62/64 loss: -0.13329893350601196
Batch 63/64 loss: -0.14068681001663208
Batch 64/64 loss: -0.12843811511993408
Epoch 227  Train loss: -0.12395213398278929  Val loss: -0.043919617367773944
Epoch 228
-------------------------------
Batch 1/64 loss: -0.13915520906448364
Batch 2/64 loss: -0.09132081270217896
Batch 3/64 loss: -0.1431066393852234
Batch 4/64 loss: -0.13602054119110107
Batch 5/64 loss: -0.12462162971496582
Batch 6/64 loss: -0.12386995553970337
Batch 7/64 loss: -0.12394195795059204
Batch 8/64 loss: -0.09554708003997803
Batch 9/64 loss: -0.11127299070358276
Batch 10/64 loss: -0.12096977233886719
Batch 11/64 loss: -0.14186573028564453
Batch 12/64 loss: -0.13791972398757935
Batch 13/64 loss: -0.11828255653381348
Batch 14/64 loss: -0.12846285104751587
Batch 15/64 loss: -0.12588655948638916
Batch 16/64 loss: -0.10459369421005249
Batch 17/64 loss: -0.14395439624786377
Batch 18/64 loss: -0.12085366249084473
Batch 19/64 loss: -0.12600749731063843
Batch 20/64 loss: -0.11670780181884766
Batch 21/64 loss: -0.14543884992599487
Batch 22/64 loss: -0.1420731544494629
Batch 23/64 loss: -0.13262683153152466
Batch 24/64 loss: -0.1610889434814453
Batch 25/64 loss: -0.13597089052200317
Batch 26/64 loss: -0.1401432752609253
Batch 27/64 loss: -0.13623428344726562
Batch 28/64 loss: -0.10822844505310059
Batch 29/64 loss: -0.14256525039672852
Batch 30/64 loss: -0.12394380569458008
Batch 31/64 loss: -0.10209691524505615
Batch 32/64 loss: -0.10337430238723755
Batch 33/64 loss: -0.11718344688415527
Batch 34/64 loss: -0.11358612775802612
Batch 35/64 loss: -0.12798112630844116
Batch 36/64 loss: -0.12771070003509521
Batch 37/64 loss: -0.14576709270477295
Batch 38/64 loss: -0.15565109252929688
Batch 39/64 loss: -0.10517323017120361
Batch 40/64 loss: -0.1357901692390442
Batch 41/64 loss: -0.1205018162727356
Batch 42/64 loss: -0.12531936168670654
Batch 43/64 loss: -0.14145195484161377
Batch 44/64 loss: -0.13898032903671265
Batch 45/64 loss: -0.1363011598587036
Batch 46/64 loss: -0.09641218185424805
Batch 47/64 loss: -0.10750389099121094
Batch 48/64 loss: -0.12161165475845337
Batch 49/64 loss: -0.142284095287323
Batch 50/64 loss: -0.14932399988174438
Batch 51/64 loss: -0.12469720840454102
Batch 52/64 loss: -0.12101894617080688
Batch 53/64 loss: -0.13515770435333252
Batch 54/64 loss: -0.10852152109146118
Batch 55/64 loss: -0.11270815134048462
Batch 56/64 loss: -0.1281333565711975
Batch 57/64 loss: -0.11881619691848755
Batch 58/64 loss: -0.1347815990447998
Batch 59/64 loss: -0.11356830596923828
Batch 60/64 loss: -0.12672150135040283
Batch 61/64 loss: -0.13052678108215332
Batch 62/64 loss: -0.12222671508789062
Batch 63/64 loss: -0.1273488998413086
Batch 64/64 loss: -0.11746501922607422
Epoch 228  Train loss: -0.12625890339122098  Val loss: -0.04699613056641674
Epoch 229
-------------------------------
Batch 1/64 loss: -0.14038395881652832
Batch 2/64 loss: -0.1296757459640503
Batch 3/64 loss: -0.10647690296173096
Batch 4/64 loss: -0.11003953218460083
Batch 5/64 loss: -0.14137381315231323
Batch 6/64 loss: -0.13118445873260498
Batch 7/64 loss: -0.11854273080825806
Batch 8/64 loss: -0.13474929332733154
Batch 9/64 loss: -0.12046933174133301
Batch 10/64 loss: -0.12481415271759033
Batch 11/64 loss: -0.1375231146812439
Batch 12/64 loss: -0.1298019289970398
Batch 13/64 loss: -0.11081165075302124
Batch 14/64 loss: -0.11559617519378662
Batch 15/64 loss: -0.13328534364700317
Batch 16/64 loss: -0.13188165426254272
Batch 17/64 loss: -0.137798011302948
Batch 18/64 loss: -0.1232677698135376
Batch 19/64 loss: -0.13097542524337769
Batch 20/64 loss: -0.14000284671783447
Batch 21/64 loss: -0.12714773416519165
Batch 22/64 loss: -0.13399285078048706
Batch 23/64 loss: -0.1289680004119873
Batch 24/64 loss: -0.13050192594528198
Batch 25/64 loss: -0.1108657717704773
Batch 26/64 loss: -0.12937867641448975
Batch 27/64 loss: -0.11391347646713257
Batch 28/64 loss: -0.1291845440864563
Batch 29/64 loss: -0.13451218605041504
Batch 30/64 loss: -0.14106255769729614
Batch 31/64 loss: -0.12871253490447998
Batch 32/64 loss: -0.14252561330795288
Batch 33/64 loss: -0.1437026858329773
Batch 34/64 loss: -0.13750457763671875
Batch 35/64 loss: -0.12999868392944336
Batch 36/64 loss: -0.12374907732009888
Batch 37/64 loss: -0.11446678638458252
Batch 38/64 loss: -0.1455814242362976
Batch 39/64 loss: -0.15133172273635864
Batch 40/64 loss: -0.13657784461975098
Batch 41/64 loss: -0.12978589534759521
Batch 42/64 loss: -0.14017218351364136
Batch 43/64 loss: -0.12720704078674316
Batch 44/64 loss: -0.10979712009429932
Batch 45/64 loss: -0.12626731395721436
Batch 46/64 loss: -0.1296367049217224
Batch 47/64 loss: -0.12591421604156494
Batch 48/64 loss: -0.11097049713134766
Batch 49/64 loss: -0.1242823600769043
Batch 50/64 loss: -0.12096297740936279
Batch 51/64 loss: -0.1289876103401184
Batch 52/64 loss: -0.1045181155204773
Batch 53/64 loss: -0.14621120691299438
Batch 54/64 loss: -0.13084137439727783
Batch 55/64 loss: -0.122250497341156
Batch 56/64 loss: -0.1368153691291809
Batch 57/64 loss: -0.12622249126434326
Batch 58/64 loss: -0.11785304546356201
Batch 59/64 loss: -0.1200331449508667
Batch 60/64 loss: -0.1288520097732544
Batch 61/64 loss: -0.13537991046905518
Batch 62/64 loss: -0.1335430145263672
Batch 63/64 loss: -0.10805678367614746
Batch 64/64 loss: -0.12509971857070923
Epoch 229  Train loss: -0.12801126568925147  Val loss: -0.043982019129487654
Epoch 230
-------------------------------
Batch 1/64 loss: -0.15096420049667358
Batch 2/64 loss: -0.1195860505104065
Batch 3/64 loss: -0.131458580493927
Batch 4/64 loss: -0.1375061273574829
Batch 5/64 loss: -0.14830660820007324
Batch 6/64 loss: -0.1161462664604187
Batch 7/64 loss: -0.13482016324996948
Batch 8/64 loss: -0.12616592645645142
Batch 9/64 loss: -0.12531894445419312
Batch 10/64 loss: -0.12592142820358276
Batch 11/64 loss: -0.13718396425247192
Batch 12/64 loss: -0.09790432453155518
Batch 13/64 loss: -0.12037539482116699
Batch 14/64 loss: -0.13933873176574707
Batch 15/64 loss: -0.13622230291366577
Batch 16/64 loss: -0.12191396951675415
Batch 17/64 loss: -0.14363694190979004
Batch 18/64 loss: -0.11426538228988647
Batch 19/64 loss: -0.14610344171524048
Batch 20/64 loss: -0.1275891661643982
Batch 21/64 loss: -0.12627029418945312
Batch 22/64 loss: -0.13766002655029297
Batch 23/64 loss: -0.13150984048843384
Batch 24/64 loss: -0.14496290683746338
Batch 25/64 loss: -0.1354687213897705
Batch 26/64 loss: -0.13549447059631348
Batch 27/64 loss: -0.126697838306427
Batch 28/64 loss: -0.11702203750610352
Batch 29/64 loss: -0.11909258365631104
Batch 30/64 loss: -0.1156536340713501
Batch 31/64 loss: -0.11330533027648926
Batch 32/64 loss: -0.14308524131774902
Batch 33/64 loss: -0.13113027811050415
Batch 34/64 loss: -0.13555598258972168
Batch 35/64 loss: -0.12762033939361572
Batch 36/64 loss: -0.12994563579559326
Batch 37/64 loss: -0.13039779663085938
Batch 38/64 loss: -0.12777924537658691
Batch 39/64 loss: -0.10603809356689453
Batch 40/64 loss: -0.13066822290420532
Batch 41/64 loss: -0.11889791488647461
Batch 42/64 loss: -0.12912309169769287
Batch 43/64 loss: -0.12999612092971802
Batch 44/64 loss: -0.13020116090774536
Batch 45/64 loss: -0.12913864850997925
Batch 46/64 loss: -0.127740740776062
Batch 47/64 loss: -0.1195480227470398
Batch 48/64 loss: -0.11395388841629028
Batch 49/64 loss: -0.1200748085975647
Batch 50/64 loss: -0.1483268141746521
Batch 51/64 loss: -0.13536971807479858
Batch 52/64 loss: -0.11428278684616089
Batch 53/64 loss: -0.13588595390319824
Batch 54/64 loss: -0.10958927869796753
Batch 55/64 loss: -0.10814189910888672
Batch 56/64 loss: -0.11204999685287476
Batch 57/64 loss: -0.1297963261604309
Batch 58/64 loss: -0.12542623281478882
Batch 59/64 loss: -0.10798335075378418
Batch 60/64 loss: -0.12674164772033691
Batch 61/64 loss: -0.13713949918746948
Batch 62/64 loss: -0.13908600807189941
Batch 63/64 loss: -0.14235377311706543
Batch 64/64 loss: -0.13036298751831055
Epoch 230  Train loss: -0.1279169624926997  Val loss: -0.04600531886943018
Epoch 231
-------------------------------
Batch 1/64 loss: -0.13276159763336182
Batch 2/64 loss: -0.13903546333312988
Batch 3/64 loss: -0.12260246276855469
Batch 4/64 loss: -0.11794346570968628
Batch 5/64 loss: -0.133228600025177
Batch 6/64 loss: -0.14095723628997803
Batch 7/64 loss: -0.13325929641723633
Batch 8/64 loss: -0.12541544437408447
Batch 9/64 loss: -0.1346798539161682
Batch 10/64 loss: -0.13043224811553955
Batch 11/64 loss: -0.13400030136108398
Batch 12/64 loss: -0.13524115085601807
Batch 13/64 loss: -0.12522006034851074
Batch 14/64 loss: -0.12566781044006348
Batch 15/64 loss: -0.10904097557067871
Batch 16/64 loss: -0.1295488476753235
Batch 17/64 loss: -0.11823034286499023
Batch 18/64 loss: -0.11476069688796997
Batch 19/64 loss: -0.14566081762313843
Batch 20/64 loss: -0.13762015104293823
Batch 21/64 loss: -0.1399298906326294
Batch 22/64 loss: -0.12864309549331665
Batch 23/64 loss: -0.1279522180557251
Batch 24/64 loss: -0.07892173528671265
Batch 25/64 loss: -0.12670528888702393
Batch 26/64 loss: -0.14769071340560913
Batch 27/64 loss: -0.12649893760681152
Batch 28/64 loss: -0.11620748043060303
Batch 29/64 loss: -0.12197089195251465
Batch 30/64 loss: -0.12297475337982178
Batch 31/64 loss: -0.13801532983779907
Batch 32/64 loss: -0.14220571517944336
Batch 33/64 loss: -0.1402403712272644
Batch 34/64 loss: -0.12740886211395264
Batch 35/64 loss: -0.12701058387756348
Batch 36/64 loss: -0.12047171592712402
Batch 37/64 loss: -0.11759090423583984
Batch 38/64 loss: -0.13593465089797974
Batch 39/64 loss: -0.13810110092163086
Batch 40/64 loss: -0.09962248802185059
Batch 41/64 loss: -0.11969500780105591
Batch 42/64 loss: -0.12862837314605713
Batch 43/64 loss: -0.13164734840393066
Batch 44/64 loss: -0.1036217212677002
Batch 45/64 loss: -0.11504471302032471
Batch 46/64 loss: -0.129919171333313
Batch 47/64 loss: -0.12832629680633545
Batch 48/64 loss: -0.12452226877212524
Batch 49/64 loss: -0.11988484859466553
Batch 50/64 loss: -0.13404613733291626
Batch 51/64 loss: -0.1168891191482544
Batch 52/64 loss: -0.13265633583068848
Batch 53/64 loss: -0.1352863311767578
Batch 54/64 loss: -0.12700313329696655
Batch 55/64 loss: -0.12710368633270264
Batch 56/64 loss: -0.14380574226379395
Batch 57/64 loss: -0.12830984592437744
Batch 58/64 loss: -0.140600323677063
Batch 59/64 loss: -0.12024098634719849
Batch 60/64 loss: -0.13049852848052979
Batch 61/64 loss: -0.1306319236755371
Batch 62/64 loss: -0.1368139386177063
Batch 63/64 loss: -0.1325920820236206
Batch 64/64 loss: -0.11926448345184326
Epoch 231  Train loss: -0.12763325139588  Val loss: -0.04555576609582016
Epoch 232
-------------------------------
Batch 1/64 loss: -0.10751217603683472
Batch 2/64 loss: -0.1103360652923584
Batch 3/64 loss: -0.12318503856658936
Batch 4/64 loss: -0.11565262079238892
Batch 5/64 loss: -0.12246823310852051
Batch 6/64 loss: -0.12737584114074707
Batch 7/64 loss: -0.1482311487197876
Batch 8/64 loss: -0.14156311750411987
Batch 9/64 loss: -0.12594467401504517
Batch 10/64 loss: -0.1443212628364563
Batch 11/64 loss: -0.12256717681884766
Batch 12/64 loss: -0.13791745901107788
Batch 13/64 loss: -0.15480810403823853
Batch 14/64 loss: -0.11699390411376953
Batch 15/64 loss: -0.11093008518218994
Batch 16/64 loss: -0.13753294944763184
Batch 17/64 loss: -0.12621748447418213
Batch 18/64 loss: -0.13427114486694336
Batch 19/64 loss: -0.11907839775085449
Batch 20/64 loss: -0.11729788780212402
Batch 21/64 loss: -0.13304686546325684
Batch 22/64 loss: -0.15771764516830444
Batch 23/64 loss: -0.1416528820991516
Batch 24/64 loss: -0.13716399669647217
Batch 25/64 loss: -0.1205717921257019
Batch 26/64 loss: -0.14399254322052002
Batch 27/64 loss: -0.14112621545791626
Batch 28/64 loss: -0.11762595176696777
Batch 29/64 loss: -0.11005866527557373
Batch 30/64 loss: -0.11587250232696533
Batch 31/64 loss: -0.14387142658233643
Batch 32/64 loss: -0.11969393491744995
Batch 33/64 loss: -0.12596631050109863
Batch 34/64 loss: -0.12143707275390625
Batch 35/64 loss: -0.13664329051971436
Batch 36/64 loss: -0.10744065046310425
Batch 37/64 loss: -0.1092599630355835
Batch 38/64 loss: -0.14883863925933838
Batch 39/64 loss: -0.13030338287353516
Batch 40/64 loss: -0.14000076055526733
Batch 41/64 loss: -0.1326659917831421
Batch 42/64 loss: -0.13241928815841675
Batch 43/64 loss: -0.14016997814178467
Batch 44/64 loss: -0.13426607847213745
Batch 45/64 loss: -0.11084175109863281
Batch 46/64 loss: -0.12684345245361328
Batch 47/64 loss: -0.1139102578163147
Batch 48/64 loss: -0.11141371726989746
Batch 49/64 loss: -0.13183444738388062
Batch 50/64 loss: -0.12865352630615234
Batch 51/64 loss: -0.10927575826644897
Batch 52/64 loss: -0.13834410905838013
Batch 53/64 loss: -0.13358569145202637
Batch 54/64 loss: -0.11846858263015747
Batch 55/64 loss: -0.12296712398529053
Batch 56/64 loss: -0.115811288356781
Batch 57/64 loss: -0.13994669914245605
Batch 58/64 loss: -0.13765496015548706
Batch 59/64 loss: -0.11890971660614014
Batch 60/64 loss: -0.13551312685012817
Batch 61/64 loss: -0.11742764711380005
Batch 62/64 loss: -0.11141294240951538
Batch 63/64 loss: -0.13840746879577637
Batch 64/64 loss: -0.15353161096572876
Epoch 232  Train loss: -0.1280373580315534  Val loss: -0.043541082401865536
Epoch 233
-------------------------------
Batch 1/64 loss: -0.14087557792663574
Batch 2/64 loss: -0.1137385368347168
Batch 3/64 loss: -0.12478166818618774
Batch 4/64 loss: -0.11883139610290527
Batch 5/64 loss: -0.12409836053848267
Batch 6/64 loss: -0.13711071014404297
Batch 7/64 loss: -0.11123228073120117
Batch 8/64 loss: -0.12134075164794922
Batch 9/64 loss: -0.14971888065338135
Batch 10/64 loss: -0.1449630856513977
Batch 11/64 loss: -0.13624918460845947
Batch 12/64 loss: -0.132060706615448
Batch 13/64 loss: -0.13719922304153442
Batch 14/64 loss: -0.13863414525985718
Batch 15/64 loss: -0.1282351016998291
Batch 16/64 loss: -0.13711118698120117
Batch 17/64 loss: -0.14412397146224976
Batch 18/64 loss: -0.13915693759918213
Batch 19/64 loss: -0.10317122936248779
Batch 20/64 loss: -0.1255354881286621
Batch 21/64 loss: -0.11043578386306763
Batch 22/64 loss: -0.14591658115386963
Batch 23/64 loss: -0.14292263984680176
Batch 24/64 loss: -0.11523085832595825
Batch 25/64 loss: -0.14316010475158691
Batch 26/64 loss: -0.14260590076446533
Batch 27/64 loss: -0.14017564058303833
Batch 28/64 loss: -0.14385974407196045
Batch 29/64 loss: -0.14819908142089844
Batch 30/64 loss: -0.1336432695388794
Batch 31/64 loss: -0.10454154014587402
Batch 32/64 loss: -0.14925694465637207
Batch 33/64 loss: -0.13547980785369873
Batch 34/64 loss: -0.13106054067611694
Batch 35/64 loss: -0.12229537963867188
Batch 36/64 loss: -0.11167317628860474
Batch 37/64 loss: -0.13379007577896118
Batch 38/64 loss: -0.11288225650787354
Batch 39/64 loss: -0.09779739379882812
Batch 40/64 loss: -0.1287292242050171
Batch 41/64 loss: -0.10646778345108032
Batch 42/64 loss: -0.12522447109222412
Batch 43/64 loss: -0.14262574911117554
Batch 44/64 loss: -0.12220817804336548
Batch 45/64 loss: -0.12691420316696167
Batch 46/64 loss: -0.10859233140945435
Batch 47/64 loss: -0.11429500579833984
Batch 48/64 loss: -0.14014744758605957
Batch 49/64 loss: -0.12485027313232422
Batch 50/64 loss: -0.1325981616973877
Batch 51/64 loss: -0.12235039472579956
Batch 52/64 loss: -0.12803679704666138
Batch 53/64 loss: -0.13958191871643066
Batch 54/64 loss: -0.10261470079421997
Batch 55/64 loss: -0.10233467817306519
Batch 56/64 loss: -0.11123889684677124
Batch 57/64 loss: -0.11935943365097046
Batch 58/64 loss: -0.12486374378204346
Batch 59/64 loss: -0.12978005409240723
Batch 60/64 loss: -0.12207549810409546
Batch 61/64 loss: -0.1289384961128235
Batch 62/64 loss: -0.13537389039993286
Batch 63/64 loss: -0.12291836738586426
Batch 64/64 loss: -0.13368362188339233
Epoch 233  Train loss: -0.12761533423966054  Val loss: -0.046046791412576366
Epoch 234
-------------------------------
Batch 1/64 loss: -0.12453025579452515
Batch 2/64 loss: -0.13938528299331665
Batch 3/64 loss: -0.13375401496887207
Batch 4/64 loss: -0.13239014148712158
Batch 5/64 loss: -0.13081777095794678
Batch 6/64 loss: -0.14556884765625
Batch 7/64 loss: -0.13858294486999512
Batch 8/64 loss: -0.11618411540985107
Batch 9/64 loss: -0.13170099258422852
Batch 10/64 loss: -0.1303616762161255
Batch 11/64 loss: -0.13376963138580322
Batch 12/64 loss: -0.14138460159301758
Batch 13/64 loss: -0.14790159463882446
Batch 14/64 loss: -0.13085007667541504
Batch 15/64 loss: -0.12777268886566162
Batch 16/64 loss: -0.14664554595947266
Batch 17/64 loss: -0.14082038402557373
Batch 18/64 loss: -0.1392916440963745
Batch 19/64 loss: -0.13255858421325684
Batch 20/64 loss: -0.10266339778900146
Batch 21/64 loss: -0.1367219090461731
Batch 22/64 loss: -0.13144034147262573
Batch 23/64 loss: -0.10524237155914307
Batch 24/64 loss: -0.12643814086914062
Batch 25/64 loss: -0.14359837770462036
Batch 26/64 loss: -0.1326308250427246
Batch 27/64 loss: -0.14069509506225586
Batch 28/64 loss: -0.128692626953125
Batch 29/64 loss: -0.13755875825881958
Batch 30/64 loss: -0.12819242477416992
Batch 31/64 loss: -0.15190833806991577
Batch 32/64 loss: -0.14718598127365112
Batch 33/64 loss: -0.11518186330795288
Batch 34/64 loss: -0.14254969358444214
Batch 35/64 loss: -0.1172170639038086
Batch 36/64 loss: -0.14309853315353394
Batch 37/64 loss: -0.14408719539642334
Batch 38/64 loss: -0.11049127578735352
Batch 39/64 loss: -0.12743771076202393
Batch 40/64 loss: -0.14417356252670288
Batch 41/64 loss: -0.12714874744415283
Batch 42/64 loss: -0.12494683265686035
Batch 43/64 loss: -0.1133999228477478
Batch 44/64 loss: -0.1477491855621338
Batch 45/64 loss: -0.12338852882385254
Batch 46/64 loss: -0.14627337455749512
Batch 47/64 loss: -0.12739670276641846
Batch 48/64 loss: -0.0991816520690918
Batch 49/64 loss: -0.12955045700073242
Batch 50/64 loss: -0.13976716995239258
Batch 51/64 loss: -0.1291453242301941
Batch 52/64 loss: -0.1222047209739685
Batch 53/64 loss: -0.10979199409484863
Batch 54/64 loss: -0.1303645372390747
Batch 55/64 loss: -0.11402392387390137
Batch 56/64 loss: -0.13331568241119385
Batch 57/64 loss: -0.11772966384887695
Batch 58/64 loss: -0.13284897804260254
Batch 59/64 loss: -0.12241047620773315
Batch 60/64 loss: -0.12576699256896973
Batch 61/64 loss: -0.1140131950378418
Batch 62/64 loss: -0.10868710279464722
Batch 63/64 loss: -0.1340203881263733
Batch 64/64 loss: -0.1477583646774292
Epoch 234  Train loss: -0.13028110756593592  Val loss: -0.04582914576907338
Epoch 235
-------------------------------
Batch 1/64 loss: -0.14701557159423828
Batch 2/64 loss: -0.13753926753997803
Batch 3/64 loss: -0.13342654705047607
Batch 4/64 loss: -0.13874000310897827
Batch 5/64 loss: -0.14478260278701782
Batch 6/64 loss: -0.14306390285491943
Batch 7/64 loss: -0.12983733415603638
Batch 8/64 loss: -0.12029391527175903
Batch 9/64 loss: -0.1320444941520691
Batch 10/64 loss: -0.1400507688522339
Batch 11/64 loss: -0.14945369958877563
Batch 12/64 loss: -0.13631194829940796
Batch 13/64 loss: -0.12734895944595337
Batch 14/64 loss: -0.1039423942565918
Batch 15/64 loss: -0.13119125366210938
Batch 16/64 loss: -0.14857232570648193
Batch 17/64 loss: -0.13476026058197021
Batch 18/64 loss: -0.14432227611541748
Batch 19/64 loss: -0.1330767273902893
Batch 20/64 loss: -0.1362575888633728
Batch 21/64 loss: -0.13448697328567505
Batch 22/64 loss: -0.13492584228515625
Batch 23/64 loss: -0.14718925952911377
Batch 24/64 loss: -0.13788360357284546
Batch 25/64 loss: -0.12176686525344849
Batch 26/64 loss: -0.13006842136383057
Batch 27/64 loss: -0.11693239212036133
Batch 28/64 loss: -0.15404140949249268
Batch 29/64 loss: -0.11951035261154175
Batch 30/64 loss: -0.12356233596801758
Batch 31/64 loss: -0.1329439878463745
Batch 32/64 loss: -0.13052594661712646
Batch 33/64 loss: -0.1455710530281067
Batch 34/64 loss: -0.13653165102005005
Batch 35/64 loss: -0.10819470882415771
Batch 36/64 loss: -0.12377971410751343
Batch 37/64 loss: -0.14507555961608887
Batch 38/64 loss: -0.13670992851257324
Batch 39/64 loss: -0.10840070247650146
Batch 40/64 loss: -0.11071246862411499
Batch 41/64 loss: -0.11663436889648438
Batch 42/64 loss: -0.13191580772399902
Batch 43/64 loss: -0.1302374005317688
Batch 44/64 loss: -0.13940644264221191
Batch 45/64 loss: -0.11662161350250244
Batch 46/64 loss: -0.12567263841629028
Batch 47/64 loss: -0.13293051719665527
Batch 48/64 loss: -0.1316514015197754
Batch 49/64 loss: -0.11771702766418457
Batch 50/64 loss: -0.12646883726119995
Batch 51/64 loss: -0.1258244514465332
Batch 52/64 loss: -0.13017290830612183
Batch 53/64 loss: -0.14324796199798584
Batch 54/64 loss: -0.13432633876800537
Batch 55/64 loss: -0.12768298387527466
Batch 56/64 loss: -0.11634606122970581
Batch 57/64 loss: -0.12792503833770752
Batch 58/64 loss: -0.1345807909965515
Batch 59/64 loss: -0.15367251634597778
Batch 60/64 loss: -0.10724705457687378
Batch 61/64 loss: -0.14217281341552734
Batch 62/64 loss: -0.14017677307128906
Batch 63/64 loss: -0.14879417419433594
Batch 64/64 loss: -0.12895774841308594
Epoch 235  Train loss: -0.13190571373584223  Val loss: -0.04573450711174929
Epoch 236
-------------------------------
Batch 1/64 loss: -0.14893978834152222
Batch 2/64 loss: -0.14711451530456543
Batch 3/64 loss: -0.11717021465301514
Batch 4/64 loss: -0.12135154008865356
Batch 5/64 loss: -0.14190596342086792
Batch 6/64 loss: -0.14156806468963623
Batch 7/64 loss: -0.16062170267105103
Batch 8/64 loss: -0.1392219066619873
Batch 9/64 loss: -0.1589474081993103
Batch 10/64 loss: -0.1431233286857605
Batch 11/64 loss: -0.13073211908340454
Batch 12/64 loss: -0.1444440484046936
Batch 13/64 loss: -0.15448546409606934
Batch 14/64 loss: -0.1433972716331482
Batch 15/64 loss: -0.11896085739135742
Batch 16/64 loss: -0.14379239082336426
Batch 17/64 loss: -0.12395548820495605
Batch 18/64 loss: -0.11852413415908813
Batch 19/64 loss: -0.12247902154922485
Batch 20/64 loss: -0.1491377353668213
Batch 21/64 loss: -0.12662148475646973
Batch 22/64 loss: -0.13527244329452515
Batch 23/64 loss: -0.12437236309051514
Batch 24/64 loss: -0.1353832483291626
Batch 25/64 loss: -0.11336135864257812
Batch 26/64 loss: -0.11943411827087402
Batch 27/64 loss: -0.1462000012397766
Batch 28/64 loss: -0.14160031080245972
Batch 29/64 loss: -0.13529616594314575
Batch 30/64 loss: -0.1358339786529541
Batch 31/64 loss: -0.1184842586517334
Batch 32/64 loss: -0.13936352729797363
Batch 33/64 loss: -0.12094271183013916
Batch 34/64 loss: -0.13122916221618652
Batch 35/64 loss: -0.1272677183151245
Batch 36/64 loss: -0.1404515504837036
Batch 37/64 loss: -0.14526498317718506
Batch 38/64 loss: -0.1348780393600464
Batch 39/64 loss: -0.14779984951019287
Batch 40/64 loss: -0.12272870540618896
Batch 41/64 loss: -0.12362301349639893
Batch 42/64 loss: -0.11897087097167969
Batch 43/64 loss: -0.1090700626373291
Batch 44/64 loss: -0.12234610319137573
Batch 45/64 loss: -0.12263178825378418
Batch 46/64 loss: -0.1364990472793579
Batch 47/64 loss: -0.12466806173324585
Batch 48/64 loss: -0.1231808066368103
Batch 49/64 loss: -0.13057595491409302
Batch 50/64 loss: -0.1263691782951355
Batch 51/64 loss: -0.12363338470458984
Batch 52/64 loss: -0.11025834083557129
Batch 53/64 loss: -0.11860954761505127
Batch 54/64 loss: -0.10538285970687866
Batch 55/64 loss: -0.13794898986816406
Batch 56/64 loss: -0.1357513666152954
Batch 57/64 loss: -0.13209038972854614
Batch 58/64 loss: -0.14182502031326294
Batch 59/64 loss: -0.13280928134918213
Batch 60/64 loss: -0.12628453969955444
Batch 61/64 loss: -0.13983911275863647
Batch 62/64 loss: -0.11036741733551025
Batch 63/64 loss: -0.13165026903152466
Batch 64/64 loss: -0.14253085851669312
Epoch 236  Train loss: -0.13181086264404596  Val loss: -0.04641347734379195
Epoch 237
-------------------------------
Batch 1/64 loss: -0.11541306972503662
Batch 2/64 loss: -0.13636666536331177
Batch 3/64 loss: -0.14200985431671143
Batch 4/64 loss: -0.13043689727783203
Batch 5/64 loss: -0.1341773271560669
Batch 6/64 loss: -0.137115478515625
Batch 7/64 loss: -0.1299123764038086
Batch 8/64 loss: -0.13254833221435547
Batch 9/64 loss: -0.13916760683059692
Batch 10/64 loss: -0.11875247955322266
Batch 11/64 loss: -0.13528335094451904
Batch 12/64 loss: -0.13475966453552246
Batch 13/64 loss: -0.1387103796005249
Batch 14/64 loss: -0.13349485397338867
Batch 15/64 loss: -0.13487505912780762
Batch 16/64 loss: -0.15396517515182495
Batch 17/64 loss: -0.13052034378051758
Batch 18/64 loss: -0.15128499269485474
Batch 19/64 loss: -0.1310635805130005
Batch 20/64 loss: -0.1328684687614441
Batch 21/64 loss: -0.1452193260192871
Batch 22/64 loss: -0.1467483639717102
Batch 23/64 loss: -0.13018471002578735
Batch 24/64 loss: -0.1387518048286438
Batch 25/64 loss: -0.12994384765625
Batch 26/64 loss: -0.1201789379119873
Batch 27/64 loss: -0.15179598331451416
Batch 28/64 loss: -0.12826478481292725
Batch 29/64 loss: -0.12951505184173584
Batch 30/64 loss: -0.14395177364349365
Batch 31/64 loss: -0.1095360517501831
Batch 32/64 loss: -0.14939779043197632
Batch 33/64 loss: -0.14155113697052002
Batch 34/64 loss: -0.13738876581192017
Batch 35/64 loss: -0.12895125150680542
Batch 36/64 loss: -0.1473921537399292
Batch 37/64 loss: -0.13214606046676636
Batch 38/64 loss: -0.13811755180358887
Batch 39/64 loss: -0.13368499279022217
Batch 40/64 loss: -0.1393113136291504
Batch 41/64 loss: -0.14398562908172607
Batch 42/64 loss: -0.13042479753494263
Batch 43/64 loss: -0.14520615339279175
Batch 44/64 loss: -0.14990568161010742
Batch 45/64 loss: -0.13233411312103271
Batch 46/64 loss: -0.149674654006958
Batch 47/64 loss: -0.12806487083435059
Batch 48/64 loss: -0.15139710903167725
Batch 49/64 loss: -0.11323946714401245
Batch 50/64 loss: -0.14356684684753418
Batch 51/64 loss: -0.12487167119979858
Batch 52/64 loss: -0.13066250085830688
Batch 53/64 loss: -0.13223189115524292
Batch 54/64 loss: -0.1335851550102234
Batch 55/64 loss: -0.12669914960861206
Batch 56/64 loss: -0.12415480613708496
Batch 57/64 loss: -0.1280907392501831
Batch 58/64 loss: -0.14296162128448486
Batch 59/64 loss: -0.10839903354644775
Batch 60/64 loss: -0.1341003179550171
Batch 61/64 loss: -0.13245052099227905
Batch 62/64 loss: -0.1035919189453125
Batch 63/64 loss: -0.12411844730377197
Batch 64/64 loss: -0.11621779203414917
Epoch 237  Train loss: -0.13389236155678244  Val loss: -0.04117301034763506
Epoch 238
-------------------------------
Batch 1/64 loss: -0.12062430381774902
Batch 2/64 loss: -0.10823017358779907
Batch 3/64 loss: -0.13234806060791016
Batch 4/64 loss: -0.12940150499343872
Batch 5/64 loss: -0.13940685987472534
Batch 6/64 loss: -0.09872627258300781
Batch 7/64 loss: -0.14094406366348267
Batch 8/64 loss: -0.1478593349456787
Batch 9/64 loss: -0.12215334177017212
Batch 10/64 loss: -0.13914108276367188
Batch 11/64 loss: -0.1435902714729309
Batch 12/64 loss: -0.14844053983688354
Batch 13/64 loss: -0.14969635009765625
Batch 14/64 loss: -0.1271662712097168
Batch 15/64 loss: -0.1445668339729309
Batch 16/64 loss: -0.12509262561798096
Batch 17/64 loss: -0.11900591850280762
Batch 18/64 loss: -0.14519906044006348
Batch 19/64 loss: -0.15415579080581665
Batch 20/64 loss: -0.140533447265625
Batch 21/64 loss: -0.13826489448547363
Batch 22/64 loss: -0.13332980871200562
Batch 23/64 loss: -0.12996190786361694
Batch 24/64 loss: -0.12347382307052612
Batch 25/64 loss: -0.13551628589630127
Batch 26/64 loss: -0.10554224252700806
Batch 27/64 loss: -0.13912010192871094
Batch 28/64 loss: -0.12866920232772827
Batch 29/64 loss: -0.11500668525695801
Batch 30/64 loss: -0.14171528816223145
Batch 31/64 loss: -0.13411855697631836
Batch 32/64 loss: -0.1391238570213318
Batch 33/64 loss: -0.1310495138168335
Batch 34/64 loss: -0.11781227588653564
Batch 35/64 loss: -0.13004350662231445
Batch 36/64 loss: -0.12721288204193115
Batch 37/64 loss: -0.13927119970321655
Batch 38/64 loss: -0.12692517042160034
Batch 39/64 loss: -0.15205448865890503
Batch 40/64 loss: -0.12486261129379272
Batch 41/64 loss: -0.13937771320343018
Batch 42/64 loss: -0.14823776483535767
Batch 43/64 loss: -0.13381022214889526
Batch 44/64 loss: -0.1319730281829834
Batch 45/64 loss: -0.14332687854766846
Batch 46/64 loss: -0.13435673713684082
Batch 47/64 loss: -0.1237560510635376
Batch 48/64 loss: -0.11791330575942993
Batch 49/64 loss: -0.1187901496887207
Batch 50/64 loss: -0.14560538530349731
Batch 51/64 loss: -0.13861507177352905
Batch 52/64 loss: -0.14104986190795898
Batch 53/64 loss: -0.1304810643196106
Batch 54/64 loss: -0.11042124032974243
Batch 55/64 loss: -0.12118494510650635
Batch 56/64 loss: -0.13667011260986328
Batch 57/64 loss: -0.12160539627075195
Batch 58/64 loss: -0.1068650484085083
Batch 59/64 loss: -0.13717961311340332
Batch 60/64 loss: -0.12873774766921997
Batch 61/64 loss: -0.12221461534500122
Batch 62/64 loss: -0.13376951217651367
Batch 63/64 loss: -0.1524343490600586
Batch 64/64 loss: -0.09246152639389038
Epoch 238  Train loss: -0.13140515089035035  Val loss: -0.04361135156703569
Epoch 239
-------------------------------
Batch 1/64 loss: -0.14727014303207397
Batch 2/64 loss: -0.14131313562393188
Batch 3/64 loss: -0.119692862033844
Batch 4/64 loss: -0.13181966543197632
Batch 5/64 loss: -0.15470921993255615
Batch 6/64 loss: -0.143757164478302
Batch 7/64 loss: -0.08199340105056763
Batch 8/64 loss: -0.12190485000610352
Batch 9/64 loss: -0.12868618965148926
Batch 10/64 loss: -0.12046903371810913
Batch 11/64 loss: -0.14262843132019043
Batch 12/64 loss: -0.14441466331481934
Batch 13/64 loss: -0.14713668823242188
Batch 14/64 loss: -0.1408941149711609
Batch 15/64 loss: -0.1299312710762024
Batch 16/64 loss: -0.13575923442840576
Batch 17/64 loss: -0.14381325244903564
Batch 18/64 loss: -0.12773895263671875
Batch 19/64 loss: -0.15380114316940308
Batch 20/64 loss: -0.12197786569595337
Batch 21/64 loss: -0.1400209665298462
Batch 22/64 loss: -0.13949090242385864
Batch 23/64 loss: -0.13632804155349731
Batch 24/64 loss: -0.08122491836547852
Batch 25/64 loss: -0.13094669580459595
Batch 26/64 loss: -0.133048415184021
Batch 27/64 loss: -0.08767914772033691
Batch 28/64 loss: -0.13915151357650757
Batch 29/64 loss: -0.1478254199028015
Batch 30/64 loss: -0.1506333351135254
Batch 31/64 loss: -0.1115615963935852
Batch 32/64 loss: -0.13437503576278687
Batch 33/64 loss: -0.15110397338867188
Batch 34/64 loss: -0.13549882173538208
Batch 35/64 loss: -0.13276994228363037
Batch 36/64 loss: -0.1272820234298706
Batch 37/64 loss: -0.11294931173324585
Batch 38/64 loss: -0.1244734525680542
Batch 39/64 loss: -0.12677502632141113
Batch 40/64 loss: -0.13628828525543213
Batch 41/64 loss: -0.11520892381668091
Batch 42/64 loss: -0.1371016502380371
Batch 43/64 loss: -0.13017964363098145
Batch 44/64 loss: -0.09912586212158203
Batch 45/64 loss: -0.12595760822296143
Batch 46/64 loss: -0.11034059524536133
Batch 47/64 loss: -0.13063502311706543
Batch 48/64 loss: -0.12770915031433105
Batch 49/64 loss: -0.12729042768478394
Batch 50/64 loss: -0.1372472047805786
Batch 51/64 loss: -0.11075633764266968
Batch 52/64 loss: -0.13768470287322998
Batch 53/64 loss: -0.13749265670776367
Batch 54/64 loss: -0.12332481145858765
Batch 55/64 loss: -0.13509351015090942
Batch 56/64 loss: -0.12738633155822754
Batch 57/64 loss: -0.13932621479034424
Batch 58/64 loss: -0.1420649290084839
Batch 59/64 loss: -0.1340874433517456
Batch 60/64 loss: -0.14628338813781738
Batch 61/64 loss: -0.14154428243637085
Batch 62/64 loss: -0.15247297286987305
Batch 63/64 loss: -0.12315541505813599
Batch 64/64 loss: -0.1247829794883728
Epoch 239  Train loss: -0.13088932434717815  Val loss: -0.04167728182376455
Epoch 240
-------------------------------
Batch 1/64 loss: -0.12294071912765503
Batch 2/64 loss: -0.15449857711791992
Batch 3/64 loss: -0.1364310383796692
Batch 4/64 loss: -0.147824227809906
Batch 5/64 loss: -0.12353837490081787
Batch 6/64 loss: -0.14626115560531616
Batch 7/64 loss: -0.15023308992385864
Batch 8/64 loss: -0.15706521272659302
Batch 9/64 loss: -0.13306277990341187
Batch 10/64 loss: -0.13285577297210693
Batch 11/64 loss: -0.13528656959533691
Batch 12/64 loss: -0.14511042833328247
Batch 13/64 loss: -0.14249855279922485
Batch 14/64 loss: -0.12713837623596191
Batch 15/64 loss: -0.1364285945892334
Batch 16/64 loss: -0.13260287046432495
Batch 17/64 loss: -0.1280590295791626
Batch 18/64 loss: -0.13549202680587769
Batch 19/64 loss: -0.1521817445755005
Batch 20/64 loss: -0.14375650882720947
Batch 21/64 loss: -0.13265419006347656
Batch 22/64 loss: -0.1454128623008728
Batch 23/64 loss: -0.14960908889770508
Batch 24/64 loss: -0.1486644744873047
Batch 25/64 loss: -0.12214499711990356
Batch 26/64 loss: -0.14022529125213623
Batch 27/64 loss: -0.12732964754104614
Batch 28/64 loss: -0.13328039646148682
Batch 29/64 loss: -0.13397729396820068
Batch 30/64 loss: -0.14872866868972778
Batch 31/64 loss: -0.12319678068161011
Batch 32/64 loss: -0.13444775342941284
Batch 33/64 loss: -0.12180089950561523
Batch 34/64 loss: -0.147141695022583
Batch 35/64 loss: -0.1486547589302063
Batch 36/64 loss: -0.13322550058364868
Batch 37/64 loss: -0.13128405809402466
Batch 38/64 loss: -0.142156720161438
Batch 39/64 loss: -0.14295554161071777
Batch 40/64 loss: -0.12138789892196655
Batch 41/64 loss: -0.1459803581237793
Batch 42/64 loss: -0.15141832828521729
Batch 43/64 loss: -0.1415858268737793
Batch 44/64 loss: -0.14151990413665771
Batch 45/64 loss: -0.09746438264846802
Batch 46/64 loss: -0.13117223978042603
Batch 47/64 loss: -0.13978153467178345
Batch 48/64 loss: -0.11716228723526001
Batch 49/64 loss: -0.1488080620765686
Batch 50/64 loss: -0.1451093554496765
Batch 51/64 loss: -0.1421126127243042
Batch 52/64 loss: -0.10586869716644287
Batch 53/64 loss: -0.12396395206451416
Batch 54/64 loss: -0.12978839874267578
Batch 55/64 loss: -0.12513655424118042
Batch 56/64 loss: -0.13047730922698975
Batch 57/64 loss: -0.12086349725723267
Batch 58/64 loss: -0.12120568752288818
Batch 59/64 loss: -0.12044823169708252
Batch 60/64 loss: -0.1356392502784729
Batch 61/64 loss: -0.14702671766281128
Batch 62/64 loss: -0.13555467128753662
Batch 63/64 loss: -0.14301258325576782
Batch 64/64 loss: -0.11902976036071777
Epoch 240  Train loss: -0.13555948126549813  Val loss: -0.04765946594710203
Epoch 241
-------------------------------
Batch 1/64 loss: -0.12688183784484863
Batch 2/64 loss: -0.1470443606376648
Batch 3/64 loss: -0.14658337831497192
Batch 4/64 loss: -0.1530534029006958
Batch 5/64 loss: -0.14209604263305664
Batch 6/64 loss: -0.10738205909729004
Batch 7/64 loss: -0.13194692134857178
Batch 8/64 loss: -0.13547730445861816
Batch 9/64 loss: -0.1283922791481018
Batch 10/64 loss: -0.14984971284866333
Batch 11/64 loss: -0.1249273419380188
Batch 12/64 loss: -0.12368237972259521
Batch 13/64 loss: -0.1384514570236206
Batch 14/64 loss: -0.14118099212646484
Batch 15/64 loss: -0.1459481120109558
Batch 16/64 loss: -0.14278405904769897
Batch 17/64 loss: -0.13625425100326538
Batch 18/64 loss: -0.12252151966094971
Batch 19/64 loss: -0.12178236246109009
Batch 20/64 loss: -0.1380050778388977
Batch 21/64 loss: -0.1488809585571289
Batch 22/64 loss: -0.1251475214958191
Batch 23/64 loss: -0.15478134155273438
Batch 24/64 loss: -0.13737887144088745
Batch 25/64 loss: -0.14071029424667358
Batch 26/64 loss: -0.13819700479507446
Batch 27/64 loss: -0.12894046306610107
Batch 28/64 loss: -0.1377376914024353
Batch 29/64 loss: -0.1305607557296753
Batch 30/64 loss: -0.13204073905944824
Batch 31/64 loss: -0.13702714443206787
Batch 32/64 loss: -0.14706480503082275
Batch 33/64 loss: -0.12882280349731445
Batch 34/64 loss: -0.14360618591308594
Batch 35/64 loss: -0.13154035806655884
Batch 36/64 loss: -0.13305091857910156
Batch 37/64 loss: -0.1224585771560669
Batch 38/64 loss: -0.12909001111984253
Batch 39/64 loss: -0.1616339087486267
Batch 40/64 loss: -0.14224296808242798
Batch 41/64 loss: -0.1325470209121704
Batch 42/64 loss: -0.13864535093307495
Batch 43/64 loss: -0.1506279706954956
Batch 44/64 loss: -0.12378966808319092
Batch 45/64 loss: -0.15253674983978271
Batch 46/64 loss: -0.13406729698181152
Batch 47/64 loss: -0.1130097508430481
Batch 48/64 loss: -0.14098483324050903
Batch 49/64 loss: -0.1480199098587036
Batch 50/64 loss: -0.12343072891235352
Batch 51/64 loss: -0.12544018030166626
Batch 52/64 loss: -0.13951271772384644
Batch 53/64 loss: -0.13530057668685913
Batch 54/64 loss: -0.12382817268371582
Batch 55/64 loss: -0.1375327706336975
Batch 56/64 loss: -0.10921257734298706
Batch 57/64 loss: -0.10617947578430176
Batch 58/64 loss: -0.11862069368362427
Batch 59/64 loss: -0.12059223651885986
Batch 60/64 loss: -0.13746464252471924
Batch 61/64 loss: -0.15653926134109497
Batch 62/64 loss: -0.14201492071151733
Batch 63/64 loss: -0.1448969841003418
Batch 64/64 loss: -0.1296626329421997
Epoch 241  Train loss: -0.13501489629932478  Val loss: -0.046037028335623724
Epoch 242
-------------------------------
Batch 1/64 loss: -0.14468997716903687
Batch 2/64 loss: -0.1444074511528015
Batch 3/64 loss: -0.13701307773590088
Batch 4/64 loss: -0.14517974853515625
Batch 5/64 loss: -0.12392127513885498
Batch 6/64 loss: -0.14954900741577148
Batch 7/64 loss: -0.14383983612060547
Batch 8/64 loss: -0.15074783563613892
Batch 9/64 loss: -0.13855910301208496
Batch 10/64 loss: -0.15976852178573608
Batch 11/64 loss: -0.1518571972846985
Batch 12/64 loss: -0.14643430709838867
Batch 13/64 loss: -0.14728152751922607
Batch 14/64 loss: -0.16102057695388794
Batch 15/64 loss: -0.12756383419036865
Batch 16/64 loss: -0.1409938931465149
Batch 17/64 loss: -0.13927441835403442
Batch 18/64 loss: -0.15820330381393433
Batch 19/64 loss: -0.13185560703277588
Batch 20/64 loss: -0.15854692459106445
Batch 21/64 loss: -0.1337946057319641
Batch 22/64 loss: -0.1149061918258667
Batch 23/64 loss: -0.1366204023361206
Batch 24/64 loss: -0.11680018901824951
Batch 25/64 loss: -0.14592885971069336
Batch 26/64 loss: -0.12474989891052246
Batch 27/64 loss: -0.1370915174484253
Batch 28/64 loss: -0.13701140880584717
Batch 29/64 loss: -0.13198018074035645
Batch 30/64 loss: -0.13567042350769043
Batch 31/64 loss: -0.14025777578353882
Batch 32/64 loss: -0.1325225830078125
Batch 33/64 loss: -0.13223838806152344
Batch 34/64 loss: -0.13961637020111084
Batch 35/64 loss: -0.1281556487083435
Batch 36/64 loss: -0.1228177547454834
Batch 37/64 loss: -0.16187012195587158
Batch 38/64 loss: -0.12322503328323364
Batch 39/64 loss: -0.14648795127868652
Batch 40/64 loss: -0.12470352649688721
Batch 41/64 loss: -0.14477050304412842
Batch 42/64 loss: -0.1323590874671936
Batch 43/64 loss: -0.14073282480239868
Batch 44/64 loss: -0.12531828880310059
Batch 45/64 loss: -0.12953931093215942
Batch 46/64 loss: -0.1312907338142395
Batch 47/64 loss: -0.13510334491729736
Batch 48/64 loss: -0.12433898448944092
Batch 49/64 loss: -0.13120955228805542
Batch 50/64 loss: -0.13081085681915283
Batch 51/64 loss: -0.14539361000061035
Batch 52/64 loss: -0.14723706245422363
Batch 53/64 loss: -0.13907819986343384
Batch 54/64 loss: -0.13096344470977783
Batch 55/64 loss: -0.12652933597564697
Batch 56/64 loss: -0.13685262203216553
Batch 57/64 loss: -0.11255550384521484
Batch 58/64 loss: -0.11366921663284302
Batch 59/64 loss: -0.1331171989440918
Batch 60/64 loss: -0.14146804809570312
Batch 61/64 loss: -0.10756629705429077
Batch 62/64 loss: -0.11332052946090698
Batch 63/64 loss: -0.13666611909866333
Batch 64/64 loss: -0.13218533992767334
Epoch 242  Train loss: -0.13609703428605024  Val loss: -0.045961049209345656
Epoch 243
-------------------------------
Batch 1/64 loss: -0.11808985471725464
Batch 2/64 loss: -0.13900303840637207
Batch 3/64 loss: -0.15107190608978271
Batch 4/64 loss: -0.1401272416114807
Batch 5/64 loss: -0.13718891143798828
Batch 6/64 loss: -0.14392399787902832
Batch 7/64 loss: -0.12368142604827881
Batch 8/64 loss: -0.11042797565460205
Batch 9/64 loss: -0.11164367198944092
Batch 10/64 loss: -0.13876116275787354
Batch 11/64 loss: -0.13350176811218262
Batch 12/64 loss: -0.14144331216812134
Batch 13/64 loss: -0.11743158102035522
Batch 14/64 loss: -0.14222651720046997
Batch 15/64 loss: -0.13655447959899902
Batch 16/64 loss: -0.15627282857894897
Batch 17/64 loss: -0.14626145362854004
Batch 18/64 loss: -0.14693981409072876
Batch 19/64 loss: -0.13686519861221313
Batch 20/64 loss: -0.1389332413673401
Batch 21/64 loss: -0.13938117027282715
Batch 22/64 loss: -0.12758642435073853
Batch 23/64 loss: -0.1280255913734436
Batch 24/64 loss: -0.1436365246772766
Batch 25/64 loss: -0.13286077976226807
Batch 26/64 loss: -0.11957359313964844
Batch 27/64 loss: -0.14458125829696655
Batch 28/64 loss: -0.12341815233230591
Batch 29/64 loss: -0.14438724517822266
Batch 30/64 loss: -0.13620483875274658
Batch 31/64 loss: -0.1382623314857483
Batch 32/64 loss: -0.13776278495788574
Batch 33/64 loss: -0.13529139757156372
Batch 34/64 loss: -0.14248502254486084
Batch 35/64 loss: -0.1240694522857666
Batch 36/64 loss: -0.147100031375885
Batch 37/64 loss: -0.14779901504516602
Batch 38/64 loss: -0.1240507960319519
Batch 39/64 loss: -0.14641165733337402
Batch 40/64 loss: -0.13927775621414185
Batch 41/64 loss: -0.13949114084243774
Batch 42/64 loss: -0.14483964443206787
Batch 43/64 loss: -0.1443294882774353
Batch 44/64 loss: -0.12585800886154175
Batch 45/64 loss: -0.12949490547180176
Batch 46/64 loss: -0.12602752447128296
Batch 47/64 loss: -0.1430172324180603
Batch 48/64 loss: -0.13023579120635986
Batch 49/64 loss: -0.1365365982055664
Batch 50/64 loss: -0.14006030559539795
Batch 51/64 loss: -0.12712055444717407
Batch 52/64 loss: -0.12576603889465332
Batch 53/64 loss: -0.13140863180160522
Batch 54/64 loss: -0.11605197191238403
Batch 55/64 loss: -0.13871580362319946
Batch 56/64 loss: -0.1538737416267395
Batch 57/64 loss: -0.14440321922302246
Batch 58/64 loss: -0.14071369171142578
Batch 59/64 loss: -0.14713799953460693
Batch 60/64 loss: -0.13947272300720215
Batch 61/64 loss: -0.12948864698410034
Batch 62/64 loss: -0.14233285188674927
Batch 63/64 loss: -0.11911308765411377
Batch 64/64 loss: -0.1270350217819214
Epoch 243  Train loss: -0.13558087956671622  Val loss: -0.0470085328387231
Epoch 244
-------------------------------
Batch 1/64 loss: -0.15067344903945923
Batch 2/64 loss: -0.1192048192024231
Batch 3/64 loss: -0.15434223413467407
Batch 4/64 loss: -0.13521337509155273
Batch 5/64 loss: -0.12340068817138672
Batch 6/64 loss: -0.13773751258850098
Batch 7/64 loss: -0.14044338464736938
Batch 8/64 loss: -0.14310932159423828
Batch 9/64 loss: -0.13216811418533325
Batch 10/64 loss: -0.14233022928237915
Batch 11/64 loss: -0.15168845653533936
Batch 12/64 loss: -0.13701999187469482
Batch 13/64 loss: -0.14418792724609375
Batch 14/64 loss: -0.12226730585098267
Batch 15/64 loss: -0.1477261185646057
Batch 16/64 loss: -0.14516162872314453
Batch 17/64 loss: -0.15008985996246338
Batch 18/64 loss: -0.15274113416671753
Batch 19/64 loss: -0.14669638872146606
Batch 20/64 loss: -0.13449937105178833
Batch 21/64 loss: -0.13951587677001953
Batch 22/64 loss: -0.12978243827819824
Batch 23/64 loss: -0.13999205827713013
Batch 24/64 loss: -0.1483132243156433
Batch 25/64 loss: -0.14395201206207275
Batch 26/64 loss: -0.11711329221725464
Batch 27/64 loss: -0.1237252950668335
Batch 28/64 loss: -0.13566088676452637
Batch 29/64 loss: -0.13378947973251343
Batch 30/64 loss: -0.15537941455841064
Batch 31/64 loss: -0.12949275970458984
Batch 32/64 loss: -0.15610194206237793
Batch 33/64 loss: -0.1431208848953247
Batch 34/64 loss: -0.14182335138320923
Batch 35/64 loss: -0.14456570148468018
Batch 36/64 loss: -0.1422405242919922
Batch 37/64 loss: -0.12648743391036987
Batch 38/64 loss: -0.1337416172027588
Batch 39/64 loss: -0.1368340253829956
Batch 40/64 loss: -0.1317760944366455
Batch 41/64 loss: -0.13388210535049438
Batch 42/64 loss: -0.14046013355255127
Batch 43/64 loss: -0.13714730739593506
Batch 44/64 loss: -0.1347140669822693
Batch 45/64 loss: -0.14591538906097412
Batch 46/64 loss: -0.13749361038208008
Batch 47/64 loss: -0.13957267999649048
Batch 48/64 loss: -0.14536571502685547
Batch 49/64 loss: -0.13657879829406738
Batch 50/64 loss: -0.1298500895500183
Batch 51/64 loss: -0.136924147605896
Batch 52/64 loss: -0.14656531810760498
Batch 53/64 loss: -0.14123457670211792
Batch 54/64 loss: -0.1330176591873169
Batch 55/64 loss: -0.14746999740600586
Batch 56/64 loss: -0.12439513206481934
Batch 57/64 loss: -0.1266157031059265
Batch 58/64 loss: -0.14581304788589478
Batch 59/64 loss: -0.1203344464302063
Batch 60/64 loss: -0.1399468183517456
Batch 61/64 loss: -0.12287020683288574
Batch 62/64 loss: -0.1321226954460144
Batch 63/64 loss: -0.14987719058990479
Batch 64/64 loss: -0.12277275323867798
Epoch 244  Train loss: -0.13810754546932147  Val loss: -0.04391170080584759
Epoch 245
-------------------------------
Batch 1/64 loss: -0.13657242059707642
Batch 2/64 loss: -0.11398512125015259
Batch 3/64 loss: -0.15129685401916504
Batch 4/64 loss: -0.13301944732666016
Batch 5/64 loss: -0.13603973388671875
Batch 6/64 loss: -0.14445865154266357
Batch 7/64 loss: -0.13718873262405396
Batch 8/64 loss: -0.14811468124389648
Batch 9/64 loss: -0.14859557151794434
Batch 10/64 loss: -0.15526551008224487
Batch 11/64 loss: -0.12790656089782715
Batch 12/64 loss: -0.1376025676727295
Batch 13/64 loss: -0.1252734661102295
Batch 14/64 loss: -0.13536858558654785
Batch 15/64 loss: -0.14604341983795166
Batch 16/64 loss: -0.15550708770751953
Batch 17/64 loss: -0.12522149085998535
Batch 18/64 loss: -0.14125317335128784
Batch 19/64 loss: -0.11833196878433228
Batch 20/64 loss: -0.1330013871192932
Batch 21/64 loss: -0.148919939994812
Batch 22/64 loss: -0.12639474868774414
Batch 23/64 loss: -0.11249768733978271
Batch 24/64 loss: -0.129058837890625
Batch 25/64 loss: -0.13483524322509766
Batch 26/64 loss: -0.11715912818908691
Batch 27/64 loss: -0.13623201847076416
Batch 28/64 loss: -0.14973634481430054
Batch 29/64 loss: -0.1561906933784485
Batch 30/64 loss: -0.14119064807891846
Batch 31/64 loss: -0.12792623043060303
Batch 32/64 loss: -0.1527489423751831
Batch 33/64 loss: -0.15703356266021729
Batch 34/64 loss: -0.15393996238708496
Batch 35/64 loss: -0.14936292171478271
Batch 36/64 loss: -0.13689512014389038
Batch 37/64 loss: -0.12656933069229126
Batch 38/64 loss: -0.13912171125411987
Batch 39/64 loss: -0.11447978019714355
Batch 40/64 loss: -0.10368138551712036
Batch 41/64 loss: -0.14475834369659424
Batch 42/64 loss: -0.1228032112121582
Batch 43/64 loss: -0.13181841373443604
Batch 44/64 loss: -0.12136572599411011
Batch 45/64 loss: -0.12395644187927246
Batch 46/64 loss: -0.15696918964385986
Batch 47/64 loss: -0.13635492324829102
Batch 48/64 loss: -0.1399688720703125
Batch 49/64 loss: -0.1334654688835144
Batch 50/64 loss: -0.12651515007019043
Batch 51/64 loss: -0.1336308717727661
Batch 52/64 loss: -0.10473877191543579
Batch 53/64 loss: -0.13207828998565674
Batch 54/64 loss: -0.10656440258026123
Batch 55/64 loss: -0.12027513980865479
Batch 56/64 loss: -0.14437639713287354
Batch 57/64 loss: -0.12217682600021362
Batch 58/64 loss: -0.13379931449890137
Batch 59/64 loss: -0.1417398452758789
Batch 60/64 loss: -0.13893860578536987
Batch 61/64 loss: -0.13806867599487305
Batch 62/64 loss: -0.1377640962600708
Batch 63/64 loss: -0.14576232433319092
Batch 64/64 loss: -0.12204265594482422
Epoch 245  Train loss: -0.13479908961875767  Val loss: -0.04391012507205976
Epoch 246
-------------------------------
Batch 1/64 loss: -0.13753855228424072
Batch 2/64 loss: -0.14041179418563843
Batch 3/64 loss: -0.13143271207809448
Batch 4/64 loss: -0.13615190982818604
Batch 5/64 loss: -0.13838618993759155
Batch 6/64 loss: -0.14124751091003418
Batch 7/64 loss: -0.14325988292694092
Batch 8/64 loss: -0.13133257627487183
Batch 9/64 loss: -0.142103910446167
Batch 10/64 loss: -0.14993077516555786
Batch 11/64 loss: -0.15152937173843384
Batch 12/64 loss: -0.14853894710540771
Batch 13/64 loss: -0.13483357429504395
Batch 14/64 loss: -0.15356045961380005
Batch 15/64 loss: -0.146531879901886
Batch 16/64 loss: -0.13760709762573242
Batch 17/64 loss: -0.15897738933563232
Batch 18/64 loss: -0.13495826721191406
Batch 19/64 loss: -0.12685155868530273
Batch 20/64 loss: -0.10642218589782715
Batch 21/64 loss: -0.13573169708251953
Batch 22/64 loss: -0.13578635454177856
Batch 23/64 loss: -0.15019160509109497
Batch 24/64 loss: -0.13527286052703857
Batch 25/64 loss: -0.13740170001983643
Batch 26/64 loss: -0.12765085697174072
Batch 27/64 loss: -0.11559325456619263
Batch 28/64 loss: -0.1402774453163147
Batch 29/64 loss: -0.14574521780014038
Batch 30/64 loss: -0.1210932731628418
Batch 31/64 loss: -0.14323514699935913
Batch 32/64 loss: -0.13761186599731445
Batch 33/64 loss: -0.1339614987373352
Batch 34/64 loss: -0.135328471660614
Batch 35/64 loss: -0.13214945793151855
Batch 36/64 loss: -0.11897307634353638
Batch 37/64 loss: -0.1389055848121643
Batch 38/64 loss: -0.13602066040039062
Batch 39/64 loss: -0.15953969955444336
Batch 40/64 loss: -0.11503058671951294
Batch 41/64 loss: -0.13908284902572632
Batch 42/64 loss: -0.13883322477340698
Batch 43/64 loss: -0.1314910650253296
Batch 44/64 loss: -0.12968581914901733
Batch 45/64 loss: -0.14982259273529053
Batch 46/64 loss: -0.1480633020401001
Batch 47/64 loss: -0.11902934312820435
Batch 48/64 loss: -0.13580691814422607
Batch 49/64 loss: -0.15359073877334595
Batch 50/64 loss: -0.10294032096862793
Batch 51/64 loss: -0.14899849891662598
Batch 52/64 loss: -0.11611443758010864
Batch 53/64 loss: -0.13672888278961182
Batch 54/64 loss: -0.12770509719848633
Batch 55/64 loss: -0.14321482181549072
Batch 56/64 loss: -0.14246952533721924
Batch 57/64 loss: -0.13983964920043945
Batch 58/64 loss: -0.159589946269989
Batch 59/64 loss: -0.15094399452209473
Batch 60/64 loss: -0.137803316116333
Batch 61/64 loss: -0.15346920490264893
Batch 62/64 loss: -0.15402233600616455
Batch 63/64 loss: -0.14871954917907715
Batch 64/64 loss: -0.14085334539413452
Epoch 246  Train loss: -0.13805038905611225  Val loss: -0.04391630552069018
Epoch 247
-------------------------------
Batch 1/64 loss: -0.1507299542427063
Batch 2/64 loss: -0.1205257773399353
Batch 3/64 loss: -0.13885164260864258
Batch 4/64 loss: -0.1528174877166748
Batch 5/64 loss: -0.14718002080917358
Batch 6/64 loss: -0.14932787418365479
Batch 7/64 loss: -0.12180167436599731
Batch 8/64 loss: -0.15092581510543823
Batch 9/64 loss: -0.12903708219528198
Batch 10/64 loss: -0.10315942764282227
Batch 11/64 loss: -0.13777339458465576
Batch 12/64 loss: -0.15418672561645508
Batch 13/64 loss: -0.14820396900177002
Batch 14/64 loss: -0.1312849521636963
Batch 15/64 loss: -0.1504417061805725
Batch 16/64 loss: -0.13740050792694092
Batch 17/64 loss: -0.15494322776794434
Batch 18/64 loss: -0.15331923961639404
Batch 19/64 loss: -0.1332542896270752
Batch 20/64 loss: -0.14615464210510254
Batch 21/64 loss: -0.14377427101135254
Batch 22/64 loss: -0.1478947401046753
Batch 23/64 loss: -0.13320159912109375
Batch 24/64 loss: -0.10911238193511963
Batch 25/64 loss: -0.15383684635162354
Batch 26/64 loss: -0.14327877759933472
Batch 27/64 loss: -0.1410733461380005
Batch 28/64 loss: -0.14410465955734253
Batch 29/64 loss: -0.1352618932723999
Batch 30/64 loss: -0.13869547843933105
Batch 31/64 loss: -0.13686561584472656
Batch 32/64 loss: -0.1283387541770935
Batch 33/64 loss: -0.12433421611785889
Batch 34/64 loss: -0.14665758609771729
Batch 35/64 loss: -0.1381179690361023
Batch 36/64 loss: -0.13773488998413086
Batch 37/64 loss: -0.12964850664138794
Batch 38/64 loss: -0.12414741516113281
Batch 39/64 loss: -0.14333975315093994
Batch 40/64 loss: -0.15121936798095703
Batch 41/64 loss: -0.13929277658462524
Batch 42/64 loss: -0.14958304166793823
Batch 43/64 loss: -0.1433659791946411
Batch 44/64 loss: -0.13550567626953125
Batch 45/64 loss: -0.11721068620681763
Batch 46/64 loss: -0.1303156614303589
Batch 47/64 loss: -0.13544166088104248
Batch 48/64 loss: -0.1491025686264038
Batch 49/64 loss: -0.13433849811553955
Batch 50/64 loss: -0.13771754503250122
Batch 51/64 loss: -0.133536696434021
Batch 52/64 loss: -0.1366485357284546
Batch 53/64 loss: -0.14467978477478027
Batch 54/64 loss: -0.10826784372329712
Batch 55/64 loss: -0.1354975700378418
Batch 56/64 loss: -0.11429953575134277
Batch 57/64 loss: -0.14281415939331055
Batch 58/64 loss: -0.1287447214126587
Batch 59/64 loss: -0.13435685634613037
Batch 60/64 loss: -0.13193976879119873
Batch 61/64 loss: -0.10918998718261719
Batch 62/64 loss: -0.13535159826278687
Batch 63/64 loss: -0.1406947374343872
Batch 64/64 loss: -0.13654589653015137
Epoch 247  Train loss: -0.13697667121887208  Val loss: -0.0495088327791273
Saving best model, epoch: 247
Epoch 248
-------------------------------
Batch 1/64 loss: -0.13011950254440308
Batch 2/64 loss: -0.13912266492843628
Batch 3/64 loss: -0.1402609944343567
Batch 4/64 loss: -0.15207070112228394
Batch 5/64 loss: -0.1461620330810547
Batch 6/64 loss: -0.14849865436553955
Batch 7/64 loss: -0.1387995481491089
Batch 8/64 loss: -0.1565108299255371
Batch 9/64 loss: -0.143185555934906
Batch 10/64 loss: -0.14323413372039795
Batch 11/64 loss: -0.15150868892669678
Batch 12/64 loss: -0.13687580823898315
Batch 13/64 loss: -0.14281988143920898
Batch 14/64 loss: -0.1622563600540161
Batch 15/64 loss: -0.12951594591140747
Batch 16/64 loss: -0.11857134103775024
Batch 17/64 loss: -0.13627636432647705
Batch 18/64 loss: -0.13079482316970825
Batch 19/64 loss: -0.13478457927703857
Batch 20/64 loss: -0.11668318510055542
Batch 21/64 loss: -0.14186394214630127
Batch 22/64 loss: -0.14001303911209106
Batch 23/64 loss: -0.12880098819732666
Batch 24/64 loss: -0.11138612031936646
Batch 25/64 loss: -0.1449207067489624
Batch 26/64 loss: -0.13425374031066895
Batch 27/64 loss: -0.14215922355651855
Batch 28/64 loss: -0.14856648445129395
Batch 29/64 loss: -0.1338297724723816
Batch 30/64 loss: -0.14257705211639404
Batch 31/64 loss: -0.13802558183670044
Batch 32/64 loss: -0.14418810606002808
Batch 33/64 loss: -0.14944440126419067
Batch 34/64 loss: -0.14141982793807983
Batch 35/64 loss: -0.1533493995666504
Batch 36/64 loss: -0.10071682929992676
Batch 37/64 loss: -0.13398927450180054
Batch 38/64 loss: -0.13494479656219482
Batch 39/64 loss: -0.12441438436508179
Batch 40/64 loss: -0.13167345523834229
Batch 41/64 loss: -0.1396285891532898
Batch 42/64 loss: -0.13608628511428833
Batch 43/64 loss: -0.15213793516159058
Batch 44/64 loss: -0.12996536493301392
Batch 45/64 loss: -0.13575077056884766
Batch 46/64 loss: -0.14430248737335205
Batch 47/64 loss: -0.12518084049224854
Batch 48/64 loss: -0.1522802710533142
Batch 49/64 loss: -0.1295122504234314
Batch 50/64 loss: -0.09397053718566895
Batch 51/64 loss: -0.14945757389068604
Batch 52/64 loss: -0.14942258596420288
Batch 53/64 loss: -0.1372484564781189
Batch 54/64 loss: -0.14208197593688965
Batch 55/64 loss: -0.13368427753448486
Batch 56/64 loss: -0.13559067249298096
Batch 57/64 loss: -0.14265918731689453
Batch 58/64 loss: -0.1373748779296875
Batch 59/64 loss: -0.1484740972518921
Batch 60/64 loss: -0.14437872171401978
Batch 61/64 loss: -0.13920533657073975
Batch 62/64 loss: -0.16071850061416626
Batch 63/64 loss: -0.14130181074142456
Batch 64/64 loss: -0.12948179244995117
Epoch 248  Train loss: -0.1382919760311351  Val loss: -0.04447245638804747
Epoch 249
-------------------------------
Batch 1/64 loss: -0.13323217630386353
Batch 2/64 loss: -0.1201825737953186
Batch 3/64 loss: -0.15203356742858887
Batch 4/64 loss: -0.13986682891845703
Batch 5/64 loss: -0.14711833000183105
Batch 6/64 loss: -0.140472412109375
Batch 7/64 loss: -0.1495119333267212
Batch 8/64 loss: -0.14926081895828247
Batch 9/64 loss: -0.14289748668670654
Batch 10/64 loss: -0.1457383632659912
Batch 11/64 loss: -0.14169460535049438
Batch 12/64 loss: -0.14664578437805176
Batch 13/64 loss: -0.13162976503372192
Batch 14/64 loss: -0.12853610515594482
Batch 15/64 loss: -0.15194392204284668
Batch 16/64 loss: -0.14225280284881592
Batch 17/64 loss: -0.12841737270355225
Batch 18/64 loss: -0.14387518167495728
Batch 19/64 loss: -0.14153718948364258
Batch 20/64 loss: -0.1371895670890808
Batch 21/64 loss: -0.13349312543869019
Batch 22/64 loss: -0.1466771960258484
Batch 23/64 loss: -0.13214457035064697
Batch 24/64 loss: -0.15082907676696777
Batch 25/64 loss: -0.13095295429229736
Batch 26/64 loss: -0.13741683959960938
Batch 27/64 loss: -0.1296129822731018
Batch 28/64 loss: -0.13245606422424316
Batch 29/64 loss: -0.14104878902435303
Batch 30/64 loss: -0.13837391138076782
Batch 31/64 loss: -0.14428794384002686
Batch 32/64 loss: -0.13541769981384277
Batch 33/64 loss: -0.13074368238449097
Batch 34/64 loss: -0.1562902331352234
Batch 35/64 loss: -0.14619386196136475
Batch 36/64 loss: -0.13491541147232056
Batch 37/64 loss: -0.15824520587921143
Batch 38/64 loss: -0.11605989933013916
Batch 39/64 loss: -0.14700794219970703
Batch 40/64 loss: -0.15271466970443726
Batch 41/64 loss: -0.1368401050567627
Batch 42/64 loss: -0.13098162412643433
Batch 43/64 loss: -0.13601303100585938
Batch 44/64 loss: -0.14395862817764282
Batch 45/64 loss: -0.11595642566680908
Batch 46/64 loss: -0.13440614938735962
Batch 47/64 loss: -0.14825600385665894
Batch 48/64 loss: -0.13706767559051514
Batch 49/64 loss: -0.13964378833770752
Batch 50/64 loss: -0.1448177695274353
Batch 51/64 loss: -0.14844787120819092
Batch 52/64 loss: -0.1557139754295349
Batch 53/64 loss: -0.14232254028320312
Batch 54/64 loss: -0.146792471408844
Batch 55/64 loss: -0.15204095840454102
Batch 56/64 loss: -0.14431142807006836
Batch 57/64 loss: -0.12791550159454346
Batch 58/64 loss: -0.13142120838165283
Batch 59/64 loss: -0.13881897926330566
Batch 60/64 loss: -0.14906954765319824
Batch 61/64 loss: -0.14631426334381104
Batch 62/64 loss: -0.12268543243408203
Batch 63/64 loss: -0.14718008041381836
Batch 64/64 loss: -0.13734978437423706
Epoch 249  Train loss: -0.1401240257655873  Val loss: -0.0458439475891926
Epoch 250
-------------------------------
Batch 1/64 loss: -0.12434780597686768
Batch 2/64 loss: -0.14275944232940674
Batch 3/64 loss: -0.14992547035217285
Batch 4/64 loss: -0.14599770307540894
Batch 5/64 loss: -0.1292683482170105
Batch 6/64 loss: -0.1430949568748474
Batch 7/64 loss: -0.15347909927368164
Batch 8/64 loss: -0.14467614889144897
Batch 9/64 loss: -0.1439739465713501
Batch 10/64 loss: -0.11764192581176758
Batch 11/64 loss: -0.1608368158340454
Batch 12/64 loss: -0.15959787368774414
Batch 13/64 loss: -0.134260892868042
Batch 14/64 loss: -0.1381741762161255
Batch 15/64 loss: -0.15255117416381836
Batch 16/64 loss: -0.15135806798934937
Batch 17/64 loss: -0.15641891956329346
Batch 18/64 loss: -0.14765572547912598
Batch 19/64 loss: -0.14487922191619873
Batch 20/64 loss: -0.16236472129821777
Batch 21/64 loss: -0.13042610883712769
Batch 22/64 loss: -0.14415103197097778
Batch 23/64 loss: -0.14653944969177246
Batch 24/64 loss: -0.15255457162857056
Batch 25/64 loss: -0.12130129337310791
Batch 26/64 loss: -0.1640205979347229
Batch 27/64 loss: -0.16088807582855225
Batch 28/64 loss: -0.1696566343307495
Batch 29/64 loss: -0.15807491540908813
Batch 30/64 loss: -0.15184998512268066
Batch 31/64 loss: -0.15022671222686768
Batch 32/64 loss: -0.14574259519577026
Batch 33/64 loss: -0.12991517782211304
Batch 34/64 loss: -0.1581484079360962
Batch 35/64 loss: -0.15228772163391113
Batch 36/64 loss: -0.13809281587600708
Batch 37/64 loss: -0.13584983348846436
Batch 38/64 loss: -0.13946276903152466
Batch 39/64 loss: -0.12300938367843628
Batch 40/64 loss: -0.1173332929611206
Batch 41/64 loss: -0.118297278881073
Batch 42/64 loss: -0.12436646223068237
Batch 43/64 loss: -0.12102794647216797
Batch 44/64 loss: -0.1345510482788086
Batch 45/64 loss: -0.12986576557159424
Batch 46/64 loss: -0.11219340562820435
Batch 47/64 loss: -0.14122360944747925
Batch 48/64 loss: -0.1518537998199463
Batch 49/64 loss: -0.13143622875213623
Batch 50/64 loss: -0.09429007768630981
Batch 51/64 loss: -0.1404944658279419
Batch 52/64 loss: -0.1510877013206482
Batch 53/64 loss: -0.1434640884399414
Batch 54/64 loss: -0.13976478576660156
Batch 55/64 loss: -0.10736727714538574
Batch 56/64 loss: -0.1328284740447998
Batch 57/64 loss: -0.13053476810455322
Batch 58/64 loss: -0.14588195085525513
Batch 59/64 loss: -0.14955425262451172
Batch 60/64 loss: -0.14187496900558472
Batch 61/64 loss: -0.12495112419128418
Batch 62/64 loss: -0.15230679512023926
Batch 63/64 loss: -0.13391590118408203
Batch 64/64 loss: -0.12552779912948608
Epoch 250  Train loss: -0.14023642095864988  Val loss: -0.045274594600258004
Epoch 251
-------------------------------
Batch 1/64 loss: -0.14619386196136475
Batch 2/64 loss: -0.1551000475883484
Batch 3/64 loss: -0.14568942785263062
Batch 4/64 loss: -0.15636253356933594
Batch 5/64 loss: -0.1449231505393982
Batch 6/64 loss: -0.1507500410079956
Batch 7/64 loss: -0.14150571823120117
Batch 8/64 loss: -0.1570168137550354
Batch 9/64 loss: -0.13608694076538086
Batch 10/64 loss: -0.13934922218322754
Batch 11/64 loss: -0.131686270236969
Batch 12/64 loss: -0.13384050130844116
Batch 13/64 loss: -0.13668251037597656
Batch 14/64 loss: -0.13987648487091064
Batch 15/64 loss: -0.14599615335464478
Batch 16/64 loss: -0.16068726778030396
Batch 17/64 loss: -0.13448566198349
Batch 18/64 loss: -0.11374908685684204
Batch 19/64 loss: -0.1610414981842041
Batch 20/64 loss: -0.12235170602798462
Batch 21/64 loss: -0.15247029066085815
Batch 22/64 loss: -0.14353430271148682
Batch 23/64 loss: -0.13037604093551636
Batch 24/64 loss: -0.14944207668304443
Batch 25/64 loss: -0.1457100510597229
Batch 26/64 loss: -0.1305796504020691
Batch 27/64 loss: -0.1487981081008911
Batch 28/64 loss: -0.149258553981781
Batch 29/64 loss: -0.13856852054595947
Batch 30/64 loss: -0.14947009086608887
Batch 31/64 loss: -0.12820309400558472
Batch 32/64 loss: -0.13055050373077393
Batch 33/64 loss: -0.14252758026123047
Batch 34/64 loss: -0.126670241355896
Batch 35/64 loss: -0.14617812633514404
Batch 36/64 loss: -0.14455926418304443
Batch 37/64 loss: -0.120902419090271
Batch 38/64 loss: -0.1127592921257019
Batch 39/64 loss: -0.1597226858139038
Batch 40/64 loss: -0.14859306812286377
Batch 41/64 loss: -0.13340336084365845
Batch 42/64 loss: -0.14902597665786743
Batch 43/64 loss: -0.15319079160690308
Batch 44/64 loss: -0.15187251567840576
Batch 45/64 loss: -0.13333660364151
Batch 46/64 loss: -0.1401146650314331
Batch 47/64 loss: -0.15599381923675537
Batch 48/64 loss: -0.15301942825317383
Batch 49/64 loss: -0.13795357942581177
Batch 50/64 loss: -0.15958040952682495
Batch 51/64 loss: -0.14689278602600098
Batch 52/64 loss: -0.11745190620422363
Batch 53/64 loss: -0.13549017906188965
Batch 54/64 loss: -0.13105928897857666
Batch 55/64 loss: -0.14128780364990234
Batch 56/64 loss: -0.15259861946105957
Batch 57/64 loss: -0.12977075576782227
Batch 58/64 loss: -0.14425408840179443
Batch 59/64 loss: -0.15572607517242432
Batch 60/64 loss: -0.1403295397758484
Batch 61/64 loss: -0.13959795236587524
Batch 62/64 loss: -0.15325325727462769
Batch 63/64 loss: -0.10712689161300659
Batch 64/64 loss: -0.15412485599517822
Epoch 251  Train loss: -0.1416497693342321  Val loss: -0.04956900581871111
Saving best model, epoch: 251
Epoch 252
-------------------------------
Batch 1/64 loss: -0.16429924964904785
Batch 2/64 loss: -0.15747565031051636
Batch 3/64 loss: -0.1562379002571106
Batch 4/64 loss: -0.1430286169052124
Batch 5/64 loss: -0.1166110634803772
Batch 6/64 loss: -0.1352710723876953
Batch 7/64 loss: -0.15563368797302246
Batch 8/64 loss: -0.14805138111114502
Batch 9/64 loss: -0.1415386199951172
Batch 10/64 loss: -0.13899797201156616
Batch 11/64 loss: -0.13170552253723145
Batch 12/64 loss: -0.15401935577392578
Batch 13/64 loss: -0.14812123775482178
Batch 14/64 loss: -0.1333562731742859
Batch 15/64 loss: -0.13812321424484253
Batch 16/64 loss: -0.13598603010177612
Batch 17/64 loss: -0.11261963844299316
Batch 18/64 loss: -0.14035218954086304
Batch 19/64 loss: -0.14525246620178223
Batch 20/64 loss: -0.14884042739868164
Batch 21/64 loss: -0.15208065509796143
Batch 22/64 loss: -0.1365639567375183
Batch 23/64 loss: -0.1522335410118103
Batch 24/64 loss: -0.13446688652038574
Batch 25/64 loss: -0.14409363269805908
Batch 26/64 loss: -0.147544264793396
Batch 27/64 loss: -0.14066684246063232
Batch 28/64 loss: -0.11826354265213013
Batch 29/64 loss: -0.12757742404937744
Batch 30/64 loss: -0.13447844982147217
Batch 31/64 loss: -0.12465673685073853
Batch 32/64 loss: -0.14170271158218384
Batch 33/64 loss: -0.1274697184562683
Batch 34/64 loss: -0.15067249536514282
Batch 35/64 loss: -0.15339624881744385
Batch 36/64 loss: -0.14239037036895752
Batch 37/64 loss: -0.15020990371704102
Batch 38/64 loss: -0.14985042810440063
Batch 39/64 loss: -0.14697253704071045
Batch 40/64 loss: -0.14197909832000732
Batch 41/64 loss: -0.11886227130889893
Batch 42/64 loss: -0.15419304370880127
Batch 43/64 loss: -0.13783204555511475
Batch 44/64 loss: -0.13520628213882446
Batch 45/64 loss: -0.13946962356567383
Batch 46/64 loss: -0.14226508140563965
Batch 47/64 loss: -0.14029943943023682
Batch 48/64 loss: -0.15721791982650757
Batch 49/64 loss: -0.14828169345855713
Batch 50/64 loss: -0.1451714038848877
Batch 51/64 loss: -0.151170015335083
Batch 52/64 loss: -0.12177258729934692
Batch 53/64 loss: -0.12118375301361084
Batch 54/64 loss: -0.14800983667373657
Batch 55/64 loss: -0.15287995338439941
Batch 56/64 loss: -0.13746660947799683
Batch 57/64 loss: -0.13227081298828125
Batch 58/64 loss: -0.14885663986206055
Batch 59/64 loss: -0.153816819190979
Batch 60/64 loss: -0.15165096521377563
Batch 61/64 loss: -0.1352074146270752
Batch 62/64 loss: -0.12627440690994263
Batch 63/64 loss: -0.14635109901428223
Batch 64/64 loss: -0.12240862846374512
Epoch 252  Train loss: -0.1411812894484576  Val loss: -0.04949855783960663
Epoch 253
-------------------------------
Batch 1/64 loss: -0.15856361389160156
Batch 2/64 loss: -0.13943469524383545
Batch 3/64 loss: -0.14352071285247803
Batch 4/64 loss: -0.1567177176475525
Batch 5/64 loss: -0.14546823501586914
Batch 6/64 loss: -0.16102683544158936
Batch 7/64 loss: -0.1372111439704895
Batch 8/64 loss: -0.16063863039016724
Batch 9/64 loss: -0.15282827615737915
Batch 10/64 loss: -0.15377694368362427
Batch 11/64 loss: -0.1695423126220703
Batch 12/64 loss: -0.15123283863067627
Batch 13/64 loss: -0.15750843286514282
Batch 14/64 loss: -0.14430725574493408
Batch 15/64 loss: -0.1358291506767273
Batch 16/64 loss: -0.1583232879638672
Batch 17/64 loss: -0.13612902164459229
Batch 18/64 loss: -0.16152840852737427
Batch 19/64 loss: -0.15088874101638794
Batch 20/64 loss: -0.12372469902038574
Batch 21/64 loss: -0.16161704063415527
Batch 22/64 loss: -0.14043128490447998
Batch 23/64 loss: -0.14343303442001343
Batch 24/64 loss: -0.12261885404586792
Batch 25/64 loss: -0.13815593719482422
Batch 26/64 loss: -0.1097104549407959
Batch 27/64 loss: -0.13389909267425537
Batch 28/64 loss: -0.1356857419013977
Batch 29/64 loss: -0.13606864213943481
Batch 30/64 loss: -0.1548750400543213
Batch 31/64 loss: -0.13698840141296387
Batch 32/64 loss: -0.15012222528457642
Batch 33/64 loss: -0.11010962724685669
Batch 34/64 loss: -0.14468151330947876
Batch 35/64 loss: -0.15281128883361816
Batch 36/64 loss: -0.16339707374572754
Batch 37/64 loss: -0.14540904760360718
Batch 38/64 loss: -0.12982118129730225
Batch 39/64 loss: -0.13383245468139648
Batch 40/64 loss: -0.139370858669281
Batch 41/64 loss: -0.11250782012939453
Batch 42/64 loss: -0.124744713306427
Batch 43/64 loss: -0.1572691798210144
Batch 44/64 loss: -0.13926327228546143
Batch 45/64 loss: -0.13145005702972412
Batch 46/64 loss: -0.1398928165435791
Batch 47/64 loss: -0.13331496715545654
Batch 48/64 loss: -0.1353853940963745
Batch 49/64 loss: -0.1510080099105835
Batch 50/64 loss: -0.15269911289215088
Batch 51/64 loss: -0.11746096611022949
Batch 52/64 loss: -0.13870728015899658
Batch 53/64 loss: -0.11503756046295166
Batch 54/64 loss: -0.1392204761505127
Batch 55/64 loss: -0.15365475416183472
Batch 56/64 loss: -0.12754708528518677
Batch 57/64 loss: -0.14876854419708252
Batch 58/64 loss: -0.1327364444732666
Batch 59/64 loss: -0.1415894627571106
Batch 60/64 loss: -0.14747512340545654
Batch 61/64 loss: -0.12512630224227905
Batch 62/64 loss: -0.12305021286010742
Batch 63/64 loss: -0.14635932445526123
Batch 64/64 loss: -0.1481006145477295
Epoch 253  Train loss: -0.14159347590278176  Val loss: -0.04582620691188013
Epoch 254
-------------------------------
Batch 1/64 loss: -0.15916091203689575
Batch 2/64 loss: -0.13263940811157227
Batch 3/64 loss: -0.15927863121032715
Batch 4/64 loss: -0.14669358730316162
Batch 5/64 loss: -0.15044474601745605
Batch 6/64 loss: -0.13758951425552368
Batch 7/64 loss: -0.16107064485549927
Batch 8/64 loss: -0.15627187490463257
Batch 9/64 loss: -0.15119171142578125
Batch 10/64 loss: -0.1507895588874817
Batch 11/64 loss: -0.14603745937347412
Batch 12/64 loss: -0.14109861850738525
Batch 13/64 loss: -0.14682453870773315
Batch 14/64 loss: -0.15145200490951538
Batch 15/64 loss: -0.1361033320426941
Batch 16/64 loss: -0.13603216409683228
Batch 17/64 loss: -0.14109516143798828
Batch 18/64 loss: -0.12377786636352539
Batch 19/64 loss: -0.13201755285263062
Batch 20/64 loss: -0.13730788230895996
Batch 21/64 loss: -0.1455312967300415
Batch 22/64 loss: -0.14651787281036377
Batch 23/64 loss: -0.15244805812835693
Batch 24/64 loss: -0.1368977427482605
Batch 25/64 loss: -0.1409779191017151
Batch 26/64 loss: -0.13721299171447754
Batch 27/64 loss: -0.14466148614883423
Batch 28/64 loss: -0.1519242525100708
Batch 29/64 loss: -0.15152162313461304
Batch 30/64 loss: -0.10520774126052856
Batch 31/64 loss: -0.14086699485778809
Batch 32/64 loss: -0.1465795636177063
Batch 33/64 loss: -0.1407092809677124
Batch 34/64 loss: -0.15544700622558594
Batch 35/64 loss: -0.14773166179656982
Batch 36/64 loss: -0.130057692527771
Batch 37/64 loss: -0.14249032735824585
Batch 38/64 loss: -0.11566168069839478
Batch 39/64 loss: -0.1569218635559082
Batch 40/64 loss: -0.13768398761749268
Batch 41/64 loss: -0.11839175224304199
Batch 42/64 loss: -0.12590575218200684
Batch 43/64 loss: -0.1479783058166504
Batch 44/64 loss: -0.11851507425308228
Batch 45/64 loss: -0.13426053524017334
Batch 46/64 loss: -0.14852267503738403
Batch 47/64 loss: -0.14080405235290527
Batch 48/64 loss: -0.1506648063659668
Batch 49/64 loss: -0.13377511501312256
Batch 50/64 loss: -0.13686317205429077
Batch 51/64 loss: -0.1644212007522583
Batch 52/64 loss: -0.14605504274368286
Batch 53/64 loss: -0.16187149286270142
Batch 54/64 loss: -0.13415604829788208
Batch 55/64 loss: -0.16280585527420044
Batch 56/64 loss: -0.14280104637145996
Batch 57/64 loss: -0.13765132427215576
Batch 58/64 loss: -0.15026843547821045
Batch 59/64 loss: -0.12800103425979614
Batch 60/64 loss: -0.13285869359970093
Batch 61/64 loss: -0.14803332090377808
Batch 62/64 loss: -0.1281806230545044
Batch 63/64 loss: -0.14277160167694092
Batch 64/64 loss: -0.14400321245193481
Epoch 254  Train loss: -0.1422350993343428  Val loss: -0.04199995146584265
Epoch 255
-------------------------------
Batch 1/64 loss: -0.15868788957595825
Batch 2/64 loss: -0.15244567394256592
Batch 3/64 loss: -0.13674426078796387
Batch 4/64 loss: -0.15115046501159668
Batch 5/64 loss: -0.1514878273010254
Batch 6/64 loss: -0.14150691032409668
Batch 7/64 loss: -0.1511109471321106
Batch 8/64 loss: -0.15960794687271118
Batch 9/64 loss: -0.14356720447540283
Batch 10/64 loss: -0.15061616897583008
Batch 11/64 loss: -0.17466485500335693
Batch 12/64 loss: -0.15247201919555664
Batch 13/64 loss: -0.15068399906158447
Batch 14/64 loss: -0.12394130229949951
Batch 15/64 loss: -0.14664214849472046
Batch 16/64 loss: -0.1330319046974182
Batch 17/64 loss: -0.13815617561340332
Batch 18/64 loss: -0.1547560691833496
Batch 19/64 loss: -0.14875108003616333
Batch 20/64 loss: -0.13104701042175293
Batch 21/64 loss: -0.14399510622024536
Batch 22/64 loss: -0.1151612401008606
Batch 23/64 loss: -0.1555386781692505
Batch 24/64 loss: -0.15278559923171997
Batch 25/64 loss: -0.16253429651260376
Batch 26/64 loss: -0.15777486562728882
Batch 27/64 loss: -0.14020007848739624
Batch 28/64 loss: -0.14614331722259521
Batch 29/64 loss: -0.1379411816596985
Batch 30/64 loss: -0.14519286155700684
Batch 31/64 loss: -0.12347537279129028
Batch 32/64 loss: -0.12855899333953857
Batch 33/64 loss: -0.12256139516830444
Batch 34/64 loss: -0.14686906337738037
Batch 35/64 loss: -0.13779568672180176
Batch 36/64 loss: -0.1310741901397705
Batch 37/64 loss: -0.13444668054580688
Batch 38/64 loss: -0.13802319765090942
Batch 39/64 loss: -0.1254621148109436
Batch 40/64 loss: -0.15160620212554932
Batch 41/64 loss: -0.13982760906219482
Batch 42/64 loss: -0.14160841703414917
Batch 43/64 loss: -0.1395840048789978
Batch 44/64 loss: -0.15117979049682617
Batch 45/64 loss: -0.09124356508255005
Batch 46/64 loss: -0.12681961059570312
Batch 47/64 loss: -0.1270679235458374
Batch 48/64 loss: -0.1417633295059204
Batch 49/64 loss: -0.1441347599029541
Batch 50/64 loss: -0.15232300758361816
Batch 51/64 loss: -0.13548344373703003
Batch 52/64 loss: -0.14285004138946533
Batch 53/64 loss: -0.13045430183410645
Batch 54/64 loss: -0.13332051038742065
Batch 55/64 loss: -0.14371860027313232
Batch 56/64 loss: -0.1402168869972229
Batch 57/64 loss: -0.14922338724136353
Batch 58/64 loss: -0.14736580848693848
Batch 59/64 loss: -0.1399242877960205
Batch 60/64 loss: -0.1483282446861267
Batch 61/64 loss: -0.14562487602233887
Batch 62/64 loss: -0.1590559482574463
Batch 63/64 loss: -0.14687120914459229
Batch 64/64 loss: -0.16289687156677246
Epoch 255  Train loss: -0.14256273250953824  Val loss: -0.04597061688138038
Epoch 256
-------------------------------
Batch 1/64 loss: -0.15514391660690308
Batch 2/64 loss: -0.15273767709732056
Batch 3/64 loss: -0.12930482625961304
Batch 4/64 loss: -0.14353680610656738
Batch 5/64 loss: -0.1378246545791626
Batch 6/64 loss: -0.1507323980331421
Batch 7/64 loss: -0.14886879920959473
Batch 8/64 loss: -0.1542978286743164
Batch 9/64 loss: -0.14923095703125
Batch 10/64 loss: -0.15451419353485107
Batch 11/64 loss: -0.13243305683135986
Batch 12/64 loss: -0.14664286375045776
Batch 13/64 loss: -0.15406817197799683
Batch 14/64 loss: -0.15920710563659668
Batch 15/64 loss: -0.14609360694885254
Batch 16/64 loss: -0.13035202026367188
Batch 17/64 loss: -0.14545464515686035
Batch 18/64 loss: -0.122988760471344
Batch 19/64 loss: -0.14289885759353638
Batch 20/64 loss: -0.16254210472106934
Batch 21/64 loss: -0.123507559299469
Batch 22/64 loss: -0.13872772455215454
Batch 23/64 loss: -0.13646560907363892
Batch 24/64 loss: -0.15253520011901855
Batch 25/64 loss: -0.1467190384864807
Batch 26/64 loss: -0.15353190898895264
Batch 27/64 loss: -0.14587461948394775
Batch 28/64 loss: -0.1393468976020813
Batch 29/64 loss: -0.15090394020080566
Batch 30/64 loss: -0.15175843238830566
Batch 31/64 loss: -0.1614171266555786
Batch 32/64 loss: -0.14173859357833862
Batch 33/64 loss: -0.1436893343925476
Batch 34/64 loss: -0.15597456693649292
Batch 35/64 loss: -0.14180278778076172
Batch 36/64 loss: -0.13648724555969238
Batch 37/64 loss: -0.12212175130844116
Batch 38/64 loss: -0.12324351072311401
Batch 39/64 loss: -0.1075102686882019
Batch 40/64 loss: -0.130448579788208
Batch 41/64 loss: -0.1322041153907776
Batch 42/64 loss: -0.14192044734954834
Batch 43/64 loss: -0.131392240524292
Batch 44/64 loss: -0.13907188177108765
Batch 45/64 loss: -0.14131325483322144
Batch 46/64 loss: -0.161321222782135
Batch 47/64 loss: -0.15395766496658325
Batch 48/64 loss: -0.12644773721694946
Batch 49/64 loss: -0.14032119512557983
Batch 50/64 loss: -0.14429593086242676
Batch 51/64 loss: -0.14435076713562012
Batch 52/64 loss: -0.14191317558288574
Batch 53/64 loss: -0.12876081466674805
Batch 54/64 loss: -0.13635343313217163
Batch 55/64 loss: -0.12908339500427246
Batch 56/64 loss: -0.15313339233398438
Batch 57/64 loss: -0.12791550159454346
Batch 58/64 loss: -0.13638168573379517
Batch 59/64 loss: -0.1576615571975708
Batch 60/64 loss: -0.11668330430984497
Batch 61/64 loss: -0.1389440894126892
Batch 62/64 loss: -0.15021395683288574
Batch 63/64 loss: -0.14143085479736328
Batch 64/64 loss: -0.12503904104232788
Epoch 256  Train loss: -0.14167104071261835  Val loss: -0.04486918879538467
Epoch 257
-------------------------------
Batch 1/64 loss: -0.15221089124679565
Batch 2/64 loss: -0.16129529476165771
Batch 3/64 loss: -0.14026576280593872
Batch 4/64 loss: -0.13821423053741455
Batch 5/64 loss: -0.16777122020721436
Batch 6/64 loss: -0.15439558029174805
Batch 7/64 loss: -0.13646197319030762
Batch 8/64 loss: -0.14010930061340332
Batch 9/64 loss: -0.1452348828315735
Batch 10/64 loss: -0.15347039699554443
Batch 11/64 loss: -0.15419018268585205
Batch 12/64 loss: -0.16733163595199585
Batch 13/64 loss: -0.12068313360214233
Batch 14/64 loss: -0.1377008557319641
Batch 15/64 loss: -0.13609927892684937
Batch 16/64 loss: -0.14528250694274902
Batch 17/64 loss: -0.14653760194778442
Batch 18/64 loss: -0.12960660457611084
Batch 19/64 loss: -0.1333533525466919
Batch 20/64 loss: -0.14437907934188843
Batch 21/64 loss: -0.14465749263763428
Batch 22/64 loss: -0.1421629786491394
Batch 23/64 loss: -0.1507878303527832
Batch 24/64 loss: -0.1372707486152649
Batch 25/64 loss: -0.139137864112854
Batch 26/64 loss: -0.16042089462280273
Batch 27/64 loss: -0.1575092077255249
Batch 28/64 loss: -0.14539450407028198
Batch 29/64 loss: -0.1405552625656128
Batch 30/64 loss: -0.11167067289352417
Batch 31/64 loss: -0.13440507650375366
Batch 32/64 loss: -0.13018298149108887
Batch 33/64 loss: -0.15248888731002808
Batch 34/64 loss: -0.12991172075271606
Batch 35/64 loss: -0.12157762050628662
Batch 36/64 loss: -0.13429152965545654
Batch 37/64 loss: -0.14458847045898438
Batch 38/64 loss: -0.14336538314819336
Batch 39/64 loss: -0.146906316280365
Batch 40/64 loss: -0.13601112365722656
Batch 41/64 loss: -0.13024568557739258
Batch 42/64 loss: -0.15984070301055908
Batch 43/64 loss: -0.13797736167907715
Batch 44/64 loss: -0.15777146816253662
Batch 45/64 loss: -0.14992713928222656
Batch 46/64 loss: -0.1219414472579956
Batch 47/64 loss: -0.1485021710395813
Batch 48/64 loss: -0.1195610761642456
Batch 49/64 loss: -0.14019381999969482
Batch 50/64 loss: -0.13872134685516357
Batch 51/64 loss: -0.14717358350753784
Batch 52/64 loss: -0.13404345512390137
Batch 53/64 loss: -0.14838409423828125
Batch 54/64 loss: -0.11682075262069702
Batch 55/64 loss: -0.14211028814315796
Batch 56/64 loss: -0.13813328742980957
Batch 57/64 loss: -0.11452794075012207
Batch 58/64 loss: -0.15259718894958496
Batch 59/64 loss: -0.14880263805389404
Batch 60/64 loss: -0.1499454379081726
Batch 61/64 loss: -0.1380489468574524
Batch 62/64 loss: -0.14498931169509888
Batch 63/64 loss: -0.15255683660507202
Batch 64/64 loss: -0.1296742558479309
Epoch 257  Train loss: -0.14177195301242904  Val loss: -0.048961817603750325
Epoch 258
-------------------------------
Batch 1/64 loss: -0.13145750761032104
Batch 2/64 loss: -0.17427444458007812
Batch 3/64 loss: -0.1415499448776245
Batch 4/64 loss: -0.14775681495666504
Batch 5/64 loss: -0.15400904417037964
Batch 6/64 loss: -0.17348712682724
Batch 7/64 loss: -0.14913326501846313
Batch 8/64 loss: -0.14287090301513672
Batch 9/64 loss: -0.1343950629234314
Batch 10/64 loss: -0.14183533191680908
Batch 11/64 loss: -0.15779930353164673
Batch 12/64 loss: -0.14556556940078735
Batch 13/64 loss: -0.1299535036087036
Batch 14/64 loss: -0.13158875703811646
Batch 15/64 loss: -0.14926356077194214
Batch 16/64 loss: -0.13968050479888916
Batch 17/64 loss: -0.15718191862106323
Batch 18/64 loss: -0.13521134853363037
Batch 19/64 loss: -0.14178752899169922
Batch 20/64 loss: -0.13044822216033936
Batch 21/64 loss: -0.1285262107849121
Batch 22/64 loss: -0.1647549271583557
Batch 23/64 loss: -0.14928090572357178
Batch 24/64 loss: -0.15515100955963135
Batch 25/64 loss: -0.14817821979522705
Batch 26/64 loss: -0.1527307629585266
Batch 27/64 loss: -0.1582176685333252
Batch 28/64 loss: -0.14000976085662842
Batch 29/64 loss: -0.1487143635749817
Batch 30/64 loss: -0.14280903339385986
Batch 31/64 loss: -0.13577383756637573
Batch 32/64 loss: -0.13352680206298828
Batch 33/64 loss: -0.13541638851165771
Batch 34/64 loss: -0.15451663732528687
Batch 35/64 loss: -0.1248369812965393
Batch 36/64 loss: -0.1529475450515747
Batch 37/64 loss: -0.1429743766784668
Batch 38/64 loss: -0.1484280824661255
Batch 39/64 loss: -0.15458381175994873
Batch 40/64 loss: -0.16676557064056396
Batch 41/64 loss: -0.1340530514717102
Batch 42/64 loss: -0.13236993551254272
Batch 43/64 loss: -0.14493811130523682
Batch 44/64 loss: -0.13619601726531982
Batch 45/64 loss: -0.14537650346755981
Batch 46/64 loss: -0.1513502597808838
Batch 47/64 loss: -0.14320552349090576
Batch 48/64 loss: -0.15295279026031494
Batch 49/64 loss: -0.14229947328567505
Batch 50/64 loss: -0.1491129994392395
Batch 51/64 loss: -0.11351245641708374
Batch 52/64 loss: -0.1441117525100708
Batch 53/64 loss: -0.14902544021606445
Batch 54/64 loss: -0.12402647733688354
Batch 55/64 loss: -0.14248180389404297
Batch 56/64 loss: -0.15970730781555176
Batch 57/64 loss: -0.14640790224075317
Batch 58/64 loss: -0.1426263451576233
Batch 59/64 loss: -0.14738011360168457
Batch 60/64 loss: -0.14360058307647705
Batch 61/64 loss: -0.12711989879608154
Batch 62/64 loss: -0.1421363353729248
Batch 63/64 loss: -0.13088345527648926
Batch 64/64 loss: -0.14414119720458984
Epoch 258  Train loss: -0.1443195763756247  Val loss: -0.047995248201376794
Epoch 259
-------------------------------
Batch 1/64 loss: -0.15039968490600586
Batch 2/64 loss: -0.1479099988937378
Batch 3/64 loss: -0.13510429859161377
Batch 4/64 loss: -0.13608676195144653
Batch 5/64 loss: -0.1530512571334839
Batch 6/64 loss: -0.13142311573028564
Batch 7/64 loss: -0.12874138355255127
Batch 8/64 loss: -0.16457617282867432
Batch 9/64 loss: -0.14156091213226318
Batch 10/64 loss: -0.14384829998016357
Batch 11/64 loss: -0.13732320070266724
Batch 12/64 loss: -0.16572952270507812
Batch 13/64 loss: -0.15534204244613647
Batch 14/64 loss: -0.12064170837402344
Batch 15/64 loss: -0.16232693195343018
Batch 16/64 loss: -0.13768351078033447
Batch 17/64 loss: -0.13306832313537598
Batch 18/64 loss: -0.14726531505584717
Batch 19/64 loss: -0.13955044746398926
Batch 20/64 loss: -0.16690593957901
Batch 21/64 loss: -0.1440526843070984
Batch 22/64 loss: -0.11500996351242065
Batch 23/64 loss: -0.1500910520553589
Batch 24/64 loss: -0.13604766130447388
Batch 25/64 loss: -0.15156733989715576
Batch 26/64 loss: -0.13922780752182007
Batch 27/64 loss: -0.12932151556015015
Batch 28/64 loss: -0.15049612522125244
Batch 29/64 loss: -0.12838637828826904
Batch 30/64 loss: -0.15367227792739868
Batch 31/64 loss: -0.14938533306121826
Batch 32/64 loss: -0.15175622701644897
Batch 33/64 loss: -0.15382635593414307
Batch 34/64 loss: -0.12891703844070435
Batch 35/64 loss: -0.15479350090026855
Batch 36/64 loss: -0.15162980556488037
Batch 37/64 loss: -0.14184057712554932
Batch 38/64 loss: -0.13515704870224
Batch 39/64 loss: -0.1590980887413025
Batch 40/64 loss: -0.12535113096237183
Batch 41/64 loss: -0.1508616805076599
Batch 42/64 loss: -0.15691447257995605
Batch 43/64 loss: -0.16295570135116577
Batch 44/64 loss: -0.15485548973083496
Batch 45/64 loss: -0.13651913404464722
Batch 46/64 loss: -0.14925813674926758
Batch 47/64 loss: -0.14437389373779297
Batch 48/64 loss: -0.14981454610824585
Batch 49/64 loss: -0.14628338813781738
Batch 50/64 loss: -0.15213149785995483
Batch 51/64 loss: -0.1508777141571045
Batch 52/64 loss: -0.15857946872711182
Batch 53/64 loss: -0.1378057599067688
Batch 54/64 loss: -0.15034902095794678
Batch 55/64 loss: -0.15495967864990234
Batch 56/64 loss: -0.15951359272003174
Batch 57/64 loss: -0.13286590576171875
Batch 58/64 loss: -0.14388370513916016
Batch 59/64 loss: -0.1291491985321045
Batch 60/64 loss: -0.1412397027015686
Batch 61/64 loss: -0.1695309281349182
Batch 62/64 loss: -0.14892929792404175
Batch 63/64 loss: -0.16189247369766235
Batch 64/64 loss: -0.12285721302032471
Epoch 259  Train loss: -0.14562908294154148  Val loss: -0.04700356373672223
Epoch 260
-------------------------------
Batch 1/64 loss: -0.15878188610076904
Batch 2/64 loss: -0.1529395580291748
Batch 3/64 loss: -0.16421306133270264
Batch 4/64 loss: -0.09132319688796997
Batch 5/64 loss: -0.143621027469635
Batch 6/64 loss: -0.16000479459762573
Batch 7/64 loss: -0.14816802740097046
Batch 8/64 loss: -0.15404701232910156
Batch 9/64 loss: -0.17230159044265747
Batch 10/64 loss: -0.14871454238891602
Batch 11/64 loss: -0.1399904489517212
Batch 12/64 loss: -0.1362931728363037
Batch 13/64 loss: -0.16282540559768677
Batch 14/64 loss: -0.1423816680908203
Batch 15/64 loss: -0.14235258102416992
Batch 16/64 loss: -0.15843158960342407
Batch 17/64 loss: -0.14809632301330566
Batch 18/64 loss: -0.1442393660545349
Batch 19/64 loss: -0.14683103561401367
Batch 20/64 loss: -0.17258107662200928
Batch 21/64 loss: -0.15464246273040771
Batch 22/64 loss: -0.157586932182312
Batch 23/64 loss: -0.16962528228759766
Batch 24/64 loss: -0.14168375730514526
Batch 25/64 loss: -0.15211975574493408
Batch 26/64 loss: -0.15912866592407227
Batch 27/64 loss: -0.15744900703430176
Batch 28/64 loss: -0.1735132932662964
Batch 29/64 loss: -0.11342489719390869
Batch 30/64 loss: -0.14103400707244873
Batch 31/64 loss: -0.1556248664855957
Batch 32/64 loss: -0.144761323928833
Batch 33/64 loss: -0.14417791366577148
Batch 34/64 loss: -0.13988584280014038
Batch 35/64 loss: -0.14888066053390503
Batch 36/64 loss: -0.15283221006393433
Batch 37/64 loss: -0.1614222526550293
Batch 38/64 loss: -0.15013039112091064
Batch 39/64 loss: -0.1301196813583374
Batch 40/64 loss: -0.15463125705718994
Batch 41/64 loss: -0.13254952430725098
Batch 42/64 loss: -0.15486687421798706
Batch 43/64 loss: -0.1360747218132019
Batch 44/64 loss: -0.15349829196929932
Batch 45/64 loss: -0.1531292200088501
Batch 46/64 loss: -0.1591167449951172
Batch 47/64 loss: -0.13734102249145508
Batch 48/64 loss: -0.1444544792175293
Batch 49/64 loss: -0.14716815948486328
Batch 50/64 loss: -0.14628958702087402
Batch 51/64 loss: -0.1592058539390564
Batch 52/64 loss: -0.1544470191001892
Batch 53/64 loss: -0.14821070432662964
Batch 54/64 loss: -0.13716602325439453
Batch 55/64 loss: -0.1444445252418518
Batch 56/64 loss: -0.12113529443740845
Batch 57/64 loss: -0.132656991481781
Batch 58/64 loss: -0.14084678888320923
Batch 59/64 loss: -0.13246983289718628
Batch 60/64 loss: -0.1513361930847168
Batch 61/64 loss: -0.1356675624847412
Batch 62/64 loss: -0.1380499005317688
Batch 63/64 loss: -0.15126168727874756
Batch 64/64 loss: -0.15249496698379517
Epoch 260  Train loss: -0.14771090278438492  Val loss: -0.04522127916722773
Epoch 261
-------------------------------
Batch 1/64 loss: -0.15702998638153076
Batch 2/64 loss: -0.16330087184906006
Batch 3/64 loss: -0.16009563207626343
Batch 4/64 loss: -0.135903000831604
Batch 5/64 loss: -0.1627635955810547
Batch 6/64 loss: -0.12963449954986572
Batch 7/64 loss: -0.1508963704109192
Batch 8/64 loss: -0.17034673690795898
Batch 9/64 loss: -0.15138757228851318
Batch 10/64 loss: -0.12015193700790405
Batch 11/64 loss: -0.140733003616333
Batch 12/64 loss: -0.15671730041503906
Batch 13/64 loss: -0.16537004709243774
Batch 14/64 loss: -0.15390413999557495
Batch 15/64 loss: -0.14917707443237305
Batch 16/64 loss: -0.13922500610351562
Batch 17/64 loss: -0.166670024394989
Batch 18/64 loss: -0.12317609786987305
Batch 19/64 loss: -0.14298516511917114
Batch 20/64 loss: -0.12516427040100098
Batch 21/64 loss: -0.153938889503479
Batch 22/64 loss: -0.12350529432296753
Batch 23/64 loss: -0.1422593593597412
Batch 24/64 loss: -0.16456884145736694
Batch 25/64 loss: -0.1400463581085205
Batch 26/64 loss: -0.14543896913528442
Batch 27/64 loss: -0.1234174370765686
Batch 28/64 loss: -0.10946393013000488
Batch 29/64 loss: -0.143687903881073
Batch 30/64 loss: -0.15313386917114258
Batch 31/64 loss: -0.15268874168395996
Batch 32/64 loss: -0.14304447174072266
Batch 33/64 loss: -0.1448230743408203
Batch 34/64 loss: -0.15702170133590698
Batch 35/64 loss: -0.13727056980133057
Batch 36/64 loss: -0.13186436891555786
Batch 37/64 loss: -0.13936680555343628
Batch 38/64 loss: -0.1330965757369995
Batch 39/64 loss: -0.15575772523880005
Batch 40/64 loss: -0.14827990531921387
Batch 41/64 loss: -0.15446937084197998
Batch 42/64 loss: -0.13278424739837646
Batch 43/64 loss: -0.14146685600280762
Batch 44/64 loss: -0.15248918533325195
Batch 45/64 loss: -0.13964688777923584
Batch 46/64 loss: -0.1434594988822937
Batch 47/64 loss: -0.15218037366867065
Batch 48/64 loss: -0.11665081977844238
Batch 49/64 loss: -0.11367732286453247
Batch 50/64 loss: -0.14455664157867432
Batch 51/64 loss: -0.13139015436172485
Batch 52/64 loss: -0.14167630672454834
Batch 53/64 loss: -0.12050461769104004
Batch 54/64 loss: -0.16152238845825195
Batch 55/64 loss: -0.15735000371932983
Batch 56/64 loss: -0.14164185523986816
Batch 57/64 loss: -0.1474313735961914
Batch 58/64 loss: -0.13748961687088013
Batch 59/64 loss: -0.12296974658966064
Batch 60/64 loss: -0.1274709701538086
Batch 61/64 loss: -0.1398223638534546
Batch 62/64 loss: -0.14317220449447632
Batch 63/64 loss: -0.14035886526107788
Batch 64/64 loss: -0.16138505935668945
Epoch 261  Train loss: -0.143255334741929  Val loss: -0.046164420871800164
Epoch 262
-------------------------------
Batch 1/64 loss: -0.14846622943878174
Batch 2/64 loss: -0.15397381782531738
Batch 3/64 loss: -0.16348159313201904
Batch 4/64 loss: -0.1500377655029297
Batch 5/64 loss: -0.1438104510307312
Batch 6/64 loss: -0.15409034490585327
Batch 7/64 loss: -0.15087324380874634
Batch 8/64 loss: -0.166598379611969
Batch 9/64 loss: -0.14566737413406372
Batch 10/64 loss: -0.14585763216018677
Batch 11/64 loss: -0.17049086093902588
Batch 12/64 loss: -0.16704225540161133
Batch 13/64 loss: -0.15778475999832153
Batch 14/64 loss: -0.12193882465362549
Batch 15/64 loss: -0.16339975595474243
Batch 16/64 loss: -0.16289108991622925
Batch 17/64 loss: -0.12495309114456177
Batch 18/64 loss: -0.15406739711761475
Batch 19/64 loss: -0.15335237979888916
Batch 20/64 loss: -0.14823788404464722
Batch 21/64 loss: -0.14255017042160034
Batch 22/64 loss: -0.13772809505462646
Batch 23/64 loss: -0.1357572078704834
Batch 24/64 loss: -0.1535763144493103
Batch 25/64 loss: -0.12876832485198975
Batch 26/64 loss: -0.15566271543502808
Batch 27/64 loss: -0.14557278156280518
Batch 28/64 loss: -0.13016057014465332
Batch 29/64 loss: -0.13605332374572754
Batch 30/64 loss: -0.15254968404769897
Batch 31/64 loss: -0.14253783226013184
Batch 32/64 loss: -0.1416490077972412
Batch 33/64 loss: -0.1394152045249939
Batch 34/64 loss: -0.14753663539886475
Batch 35/64 loss: -0.11515569686889648
Batch 36/64 loss: -0.14905166625976562
Batch 37/64 loss: -0.14202064275741577
Batch 38/64 loss: -0.13920199871063232
Batch 39/64 loss: -0.16458672285079956
Batch 40/64 loss: -0.1437242031097412
Batch 41/64 loss: -0.157539963722229
Batch 42/64 loss: -0.13497716188430786
Batch 43/64 loss: -0.13679230213165283
Batch 44/64 loss: -0.15185695886611938
Batch 45/64 loss: -0.1441715955734253
Batch 46/64 loss: -0.13171052932739258
Batch 47/64 loss: -0.12238878011703491
Batch 48/64 loss: -0.14758920669555664
Batch 49/64 loss: -0.15715914964675903
Batch 50/64 loss: -0.14765340089797974
Batch 51/64 loss: -0.14031320810317993
Batch 52/64 loss: -0.1282200813293457
Batch 53/64 loss: -0.16033196449279785
Batch 54/64 loss: -0.1485772728919983
Batch 55/64 loss: -0.13883864879608154
Batch 56/64 loss: -0.1265122890472412
Batch 57/64 loss: -0.14616739749908447
Batch 58/64 loss: -0.15054166316986084
Batch 59/64 loss: -0.145033061504364
Batch 60/64 loss: -0.1419094204902649
Batch 61/64 loss: -0.12597262859344482
Batch 62/64 loss: -0.14781814813613892
Batch 63/64 loss: -0.16465771198272705
Batch 64/64 loss: -0.15905815362930298
Epoch 262  Train loss: -0.14598117786295273  Val loss: -0.045927012908909326
Epoch 263
-------------------------------
Batch 1/64 loss: -0.152818500995636
Batch 2/64 loss: -0.16181188821792603
Batch 3/64 loss: -0.155947744846344
Batch 4/64 loss: -0.15344703197479248
Batch 5/64 loss: -0.1485641598701477
Batch 6/64 loss: -0.13604354858398438
Batch 7/64 loss: -0.16012853384017944
Batch 8/64 loss: -0.1655985713005066
Batch 9/64 loss: -0.1600472331047058
Batch 10/64 loss: -0.14321231842041016
Batch 11/64 loss: -0.15404736995697021
Batch 12/64 loss: -0.1713705062866211
Batch 13/64 loss: -0.1576995849609375
Batch 14/64 loss: -0.15068107843399048
Batch 15/64 loss: -0.15380585193634033
Batch 16/64 loss: -0.16783422231674194
Batch 17/64 loss: -0.11814695596694946
Batch 18/64 loss: -0.15965288877487183
Batch 19/64 loss: -0.15244078636169434
Batch 20/64 loss: -0.1654372215270996
Batch 21/64 loss: -0.1378706693649292
Batch 22/64 loss: -0.16405177116394043
Batch 23/64 loss: -0.12600398063659668
Batch 24/64 loss: -0.14353007078170776
Batch 25/64 loss: -0.15325307846069336
Batch 26/64 loss: -0.15252083539962769
Batch 27/64 loss: -0.1464138627052307
Batch 28/64 loss: -0.1471455693244934
Batch 29/64 loss: -0.15899640321731567
Batch 30/64 loss: -0.15950411558151245
Batch 31/64 loss: -0.13464665412902832
Batch 32/64 loss: -0.15472811460494995
Batch 33/64 loss: -0.15302199125289917
Batch 34/64 loss: -0.14483124017715454
Batch 35/64 loss: -0.1626439094543457
Batch 36/64 loss: -0.1485258936882019
Batch 37/64 loss: -0.1513381004333496
Batch 38/64 loss: -0.13924109935760498
Batch 39/64 loss: -0.13911068439483643
Batch 40/64 loss: -0.16243821382522583
Batch 41/64 loss: -0.12309545278549194
Batch 42/64 loss: -0.14376920461654663
Batch 43/64 loss: -0.14510506391525269
Batch 44/64 loss: -0.14855611324310303
Batch 45/64 loss: -0.1574714183807373
Batch 46/64 loss: -0.13189059495925903
Batch 47/64 loss: -0.11783385276794434
Batch 48/64 loss: -0.13617825508117676
Batch 49/64 loss: -0.1210947036743164
Batch 50/64 loss: -0.10710424184799194
Batch 51/64 loss: -0.16385865211486816
Batch 52/64 loss: -0.1460089087486267
Batch 53/64 loss: -0.14717990159988403
Batch 54/64 loss: -0.1203606128692627
Batch 55/64 loss: -0.1313876509666443
Batch 56/64 loss: -0.14937710762023926
Batch 57/64 loss: -0.14974939823150635
Batch 58/64 loss: -0.15442222356796265
Batch 59/64 loss: -0.16640347242355347
Batch 60/64 loss: -0.16524052619934082
Batch 61/64 loss: -0.13399553298950195
Batch 62/64 loss: -0.1548701524734497
Batch 63/64 loss: -0.14459824562072754
Batch 64/64 loss: -0.1427428126335144
Epoch 263  Train loss: -0.14800252002828262  Val loss: -0.044785317481588253
Epoch 264
-------------------------------
Batch 1/64 loss: -0.15190660953521729
Batch 2/64 loss: -0.13538283109664917
Batch 3/64 loss: -0.16597628593444824
Batch 4/64 loss: -0.14883476495742798
Batch 5/64 loss: -0.14736604690551758
Batch 6/64 loss: -0.16687899827957153
Batch 7/64 loss: -0.16440415382385254
Batch 8/64 loss: -0.17056316137313843
Batch 9/64 loss: -0.1313825249671936
Batch 10/64 loss: -0.16930657625198364
Batch 11/64 loss: -0.1317511796951294
Batch 12/64 loss: -0.14905858039855957
Batch 13/64 loss: -0.15834927558898926
Batch 14/64 loss: -0.1586294174194336
Batch 15/64 loss: -0.15151530504226685
Batch 16/64 loss: -0.15652436017990112
Batch 17/64 loss: -0.1213691234588623
Batch 18/64 loss: -0.16065406799316406
Batch 19/64 loss: -0.1461014747619629
Batch 20/64 loss: -0.13931572437286377
Batch 21/64 loss: -0.13537180423736572
Batch 22/64 loss: -0.14771264791488647
Batch 23/64 loss: -0.1669529676437378
Batch 24/64 loss: -0.14989590644836426
Batch 25/64 loss: -0.15432053804397583
Batch 26/64 loss: -0.13696515560150146
Batch 27/64 loss: -0.14700394868850708
Batch 28/64 loss: -0.1651172637939453
Batch 29/64 loss: -0.16545838117599487
Batch 30/64 loss: -0.15391385555267334
Batch 31/64 loss: -0.1518496870994568
Batch 32/64 loss: -0.15188872814178467
Batch 33/64 loss: -0.15687334537506104
Batch 34/64 loss: -0.13253390789031982
Batch 35/64 loss: -0.137431800365448
Batch 36/64 loss: -0.1409926414489746
Batch 37/64 loss: -0.1474049687385559
Batch 38/64 loss: -0.14288103580474854
Batch 39/64 loss: -0.13925206661224365
Batch 40/64 loss: -0.14592432975769043
Batch 41/64 loss: -0.15062183141708374
Batch 42/64 loss: -0.16705572605133057
Batch 43/64 loss: -0.16136306524276733
Batch 44/64 loss: -0.15784770250320435
Batch 45/64 loss: -0.1649879813194275
Batch 46/64 loss: -0.15318572521209717
Batch 47/64 loss: -0.11970967054367065
Batch 48/64 loss: -0.14761996269226074
Batch 49/64 loss: -0.16567152738571167
Batch 50/64 loss: -0.14300483465194702
Batch 51/64 loss: -0.13017749786376953
Batch 52/64 loss: -0.12267154455184937
Batch 53/64 loss: -0.13359391689300537
Batch 54/64 loss: -0.1381208896636963
Batch 55/64 loss: -0.15279054641723633
Batch 56/64 loss: -0.12455284595489502
Batch 57/64 loss: -0.15592193603515625
Batch 58/64 loss: -0.13765501976013184
Batch 59/64 loss: -0.13910382986068726
Batch 60/64 loss: -0.1445561647415161
Batch 61/64 loss: -0.1530001163482666
Batch 62/64 loss: -0.1432805061340332
Batch 63/64 loss: -0.1267378330230713
Batch 64/64 loss: -0.1547715663909912
Epoch 264  Train loss: -0.14814627123814003  Val loss: -0.04302683319013143
Epoch 265
-------------------------------
Batch 1/64 loss: -0.1374645233154297
Batch 2/64 loss: -0.14482688903808594
Batch 3/64 loss: -0.16282778978347778
Batch 4/64 loss: -0.15272468328475952
Batch 5/64 loss: -0.16198194026947021
Batch 6/64 loss: -0.16504639387130737
Batch 7/64 loss: -0.1595062017440796
Batch 8/64 loss: -0.16116446256637573
Batch 9/64 loss: -0.1495290994644165
Batch 10/64 loss: -0.173772931098938
Batch 11/64 loss: -0.14665532112121582
Batch 12/64 loss: -0.14605772495269775
Batch 13/64 loss: -0.16224133968353271
Batch 14/64 loss: -0.14654123783111572
Batch 15/64 loss: -0.1400701403617859
Batch 16/64 loss: -0.15808910131454468
Batch 17/64 loss: -0.16101986169815063
Batch 18/64 loss: -0.15672820806503296
Batch 19/64 loss: -0.142034113407135
Batch 20/64 loss: -0.13184314966201782
Batch 21/64 loss: -0.15177029371261597
Batch 22/64 loss: -0.16390633583068848
Batch 23/64 loss: -0.13946157693862915
Batch 24/64 loss: -0.1707773208618164
Batch 25/64 loss: -0.15690076351165771
Batch 26/64 loss: -0.13249504566192627
Batch 27/64 loss: -0.12854397296905518
Batch 28/64 loss: -0.1490498185157776
Batch 29/64 loss: -0.14202892780303955
Batch 30/64 loss: -0.12852329015731812
Batch 31/64 loss: -0.14951574802398682
Batch 32/64 loss: -0.14325261116027832
Batch 33/64 loss: -0.14622712135314941
Batch 34/64 loss: -0.13503903150558472
Batch 35/64 loss: -0.13972824811935425
Batch 36/64 loss: -0.16390812397003174
Batch 37/64 loss: -0.1536858081817627
Batch 38/64 loss: -0.16675782203674316
Batch 39/64 loss: -0.1605743169784546
Batch 40/64 loss: -0.1331447958946228
Batch 41/64 loss: -0.1572657823562622
Batch 42/64 loss: -0.1597464680671692
Batch 43/64 loss: -0.14959394931793213
Batch 44/64 loss: -0.1548251509666443
Batch 45/64 loss: -0.16232657432556152
Batch 46/64 loss: -0.14646291732788086
Batch 47/64 loss: -0.12941956520080566
Batch 48/64 loss: -0.13728511333465576
Batch 49/64 loss: -0.1588228940963745
Batch 50/64 loss: -0.15436100959777832
Batch 51/64 loss: -0.14962458610534668
Batch 52/64 loss: -0.14715200662612915
Batch 53/64 loss: -0.14838194847106934
Batch 54/64 loss: -0.14954495429992676
Batch 55/64 loss: -0.14333927631378174
Batch 56/64 loss: -0.14260351657867432
Batch 57/64 loss: -0.13907384872436523
Batch 58/64 loss: -0.13361990451812744
Batch 59/64 loss: -0.1525193452835083
Batch 60/64 loss: -0.13160717487335205
Batch 61/64 loss: -0.147499680519104
Batch 62/64 loss: -0.1516563892364502
Batch 63/64 loss: -0.11175978183746338
Batch 64/64 loss: -0.14449012279510498
Epoch 265  Train loss: -0.1487415767183491  Val loss: -0.048803829245550934
Epoch 266
-------------------------------
Batch 1/64 loss: -0.16071254014968872
Batch 2/64 loss: -0.15966767072677612
Batch 3/64 loss: -0.16122597455978394
Batch 4/64 loss: -0.15930825471878052
Batch 5/64 loss: -0.14402413368225098
Batch 6/64 loss: -0.15196722745895386
Batch 7/64 loss: -0.1667577624320984
Batch 8/64 loss: -0.15697157382965088
Batch 9/64 loss: -0.15351903438568115
Batch 10/64 loss: -0.15049433708190918
Batch 11/64 loss: -0.145729660987854
Batch 12/64 loss: -0.1608903408050537
Batch 13/64 loss: -0.1218714714050293
Batch 14/64 loss: -0.14010852575302124
Batch 15/64 loss: -0.14305293560028076
Batch 16/64 loss: -0.15994226932525635
Batch 17/64 loss: -0.16872137784957886
Batch 18/64 loss: -0.1610601544380188
Batch 19/64 loss: -0.14242655038833618
Batch 20/64 loss: -0.16986793279647827
Batch 21/64 loss: -0.1336638331413269
Batch 22/64 loss: -0.13519233465194702
Batch 23/64 loss: -0.14432156085968018
Batch 24/64 loss: -0.1516270637512207
Batch 25/64 loss: -0.144831120967865
Batch 26/64 loss: -0.14607000350952148
Batch 27/64 loss: -0.14864200353622437
Batch 28/64 loss: -0.16348952054977417
Batch 29/64 loss: -0.1604304313659668
Batch 30/64 loss: -0.16328191757202148
Batch 31/64 loss: -0.15619516372680664
Batch 32/64 loss: -0.12840485572814941
Batch 33/64 loss: -0.15943628549575806
Batch 34/64 loss: -0.1271529197692871
Batch 35/64 loss: -0.12819963693618774
Batch 36/64 loss: -0.14893150329589844
Batch 37/64 loss: -0.12541013956069946
Batch 38/64 loss: -0.17025858163833618
Batch 39/64 loss: -0.1444392204284668
Batch 40/64 loss: -0.1571241021156311
Batch 41/64 loss: -0.14515763521194458
Batch 42/64 loss: -0.1438206434249878
Batch 43/64 loss: -0.1416996717453003
Batch 44/64 loss: -0.12886565923690796
Batch 45/64 loss: -0.15384036302566528
Batch 46/64 loss: -0.16547167301177979
Batch 47/64 loss: -0.14049774408340454
Batch 48/64 loss: -0.1449558138847351
Batch 49/64 loss: -0.1559850573539734
Batch 50/64 loss: -0.15233755111694336
Batch 51/64 loss: -0.16532039642333984
Batch 52/64 loss: -0.16346144676208496
Batch 53/64 loss: -0.13700437545776367
Batch 54/64 loss: -0.1364709734916687
Batch 55/64 loss: -0.16144543886184692
Batch 56/64 loss: -0.1531834602355957
Batch 57/64 loss: -0.13320744037628174
Batch 58/64 loss: -0.14931416511535645
Batch 59/64 loss: -0.15970486402511597
Batch 60/64 loss: -0.162528395652771
Batch 61/64 loss: -0.1272728443145752
Batch 62/64 loss: -0.16131329536437988
Batch 63/64 loss: -0.1388993263244629
Batch 64/64 loss: -0.16127365827560425
Epoch 266  Train loss: -0.14993153576757393  Val loss: -0.047026407677693054
Epoch 267
-------------------------------
Batch 1/64 loss: -0.1591680645942688
Batch 2/64 loss: -0.16678881645202637
Batch 3/64 loss: -0.15909075736999512
Batch 4/64 loss: -0.16925203800201416
Batch 5/64 loss: -0.13984239101409912
Batch 6/64 loss: -0.1597733497619629
Batch 7/64 loss: -0.1577308177947998
Batch 8/64 loss: -0.16471147537231445
Batch 9/64 loss: -0.1723489761352539
Batch 10/64 loss: -0.15220952033996582
Batch 11/64 loss: -0.15272372961044312
Batch 12/64 loss: -0.14547449350357056
Batch 13/64 loss: -0.13332217931747437
Batch 14/64 loss: -0.17113351821899414
Batch 15/64 loss: -0.15701818466186523
Batch 16/64 loss: -0.15725862979888916
Batch 17/64 loss: -0.15855646133422852
Batch 18/64 loss: -0.13188904523849487
Batch 19/64 loss: -0.13657796382904053
Batch 20/64 loss: -0.1592763066291809
Batch 21/64 loss: -0.14371812343597412
Batch 22/64 loss: -0.14498209953308105
Batch 23/64 loss: -0.1625577211380005
Batch 24/64 loss: -0.1275087594985962
Batch 25/64 loss: -0.16320979595184326
Batch 26/64 loss: -0.16077494621276855
Batch 27/64 loss: -0.09790301322937012
Batch 28/64 loss: -0.15978825092315674
Batch 29/64 loss: -0.15078264474868774
Batch 30/64 loss: -0.15715181827545166
Batch 31/64 loss: -0.15220314264297485
Batch 32/64 loss: -0.13299137353897095
Batch 33/64 loss: -0.16217094659805298
Batch 34/64 loss: -0.15927249193191528
Batch 35/64 loss: -0.1668473482131958
Batch 36/64 loss: -0.1576089859008789
Batch 37/64 loss: -0.12629663944244385
Batch 38/64 loss: -0.15038740634918213
Batch 39/64 loss: -0.15814560651779175
Batch 40/64 loss: -0.1527402400970459
Batch 41/64 loss: -0.16821396350860596
Batch 42/64 loss: -0.13550496101379395
Batch 43/64 loss: -0.14988237619400024
Batch 44/64 loss: -0.15216654539108276
Batch 45/64 loss: -0.14840054512023926
Batch 46/64 loss: -0.13159608840942383
Batch 47/64 loss: -0.16684037446975708
Batch 48/64 loss: -0.16010761260986328
Batch 49/64 loss: -0.14094007015228271
Batch 50/64 loss: -0.14301782846450806
Batch 51/64 loss: -0.11430543661117554
Batch 52/64 loss: -0.1549699902534485
Batch 53/64 loss: -0.13469886779785156
Batch 54/64 loss: -0.1620544195175171
Batch 55/64 loss: -0.1128547191619873
Batch 56/64 loss: -0.14366918802261353
Batch 57/64 loss: -0.13330763578414917
Batch 58/64 loss: -0.12540745735168457
Batch 59/64 loss: -0.16462135314941406
Batch 60/64 loss: -0.16321784257888794
Batch 61/64 loss: -0.14796841144561768
Batch 62/64 loss: -0.15725302696228027
Batch 63/64 loss: -0.15030121803283691
Batch 64/64 loss: -0.14823049306869507
Epoch 267  Train loss: -0.15001820980333813  Val loss: -0.04613607650769945
Epoch 268
-------------------------------
Batch 1/64 loss: -0.17048192024230957
Batch 2/64 loss: -0.16461384296417236
Batch 3/64 loss: -0.1796492338180542
Batch 4/64 loss: -0.16948449611663818
Batch 5/64 loss: -0.13392245769500732
Batch 6/64 loss: -0.16036808490753174
Batch 7/64 loss: -0.14478445053100586
Batch 8/64 loss: -0.16301876306533813
Batch 9/64 loss: -0.15202796459197998
Batch 10/64 loss: -0.16260606050491333
Batch 11/64 loss: -0.14565908908843994
Batch 12/64 loss: -0.15472054481506348
Batch 13/64 loss: -0.16387534141540527
Batch 14/64 loss: -0.15698784589767456
Batch 15/64 loss: -0.16190284490585327
Batch 16/64 loss: -0.16603899002075195
Batch 17/64 loss: -0.11248117685317993
Batch 18/64 loss: -0.1516706943511963
Batch 19/64 loss: -0.16224569082260132
Batch 20/64 loss: -0.1401585340499878
Batch 21/64 loss: -0.15875089168548584
Batch 22/64 loss: -0.16752398014068604
Batch 23/64 loss: -0.15770137310028076
Batch 24/64 loss: -0.15100198984146118
Batch 25/64 loss: -0.16022348403930664
Batch 26/64 loss: -0.14954841136932373
Batch 27/64 loss: -0.13241338729858398
Batch 28/64 loss: -0.15048182010650635
Batch 29/64 loss: -0.13456881046295166
Batch 30/64 loss: -0.15639472007751465
Batch 31/64 loss: -0.156286358833313
Batch 32/64 loss: -0.14128369092941284
Batch 33/64 loss: -0.17114830017089844
Batch 34/64 loss: -0.15300029516220093
Batch 35/64 loss: -0.15359389781951904
Batch 36/64 loss: -0.1531217098236084
Batch 37/64 loss: -0.13430362939834595
Batch 38/64 loss: -0.15655052661895752
Batch 39/64 loss: -0.15845346450805664
Batch 40/64 loss: -0.13980591297149658
Batch 41/64 loss: -0.14602404832839966
Batch 42/64 loss: -0.15001684427261353
Batch 43/64 loss: -0.15363258123397827
Batch 44/64 loss: -0.16075831651687622
Batch 45/64 loss: -0.14488989114761353
Batch 46/64 loss: -0.1582697629928589
Batch 47/64 loss: -0.13287603855133057
Batch 48/64 loss: -0.15789449214935303
Batch 49/64 loss: -0.1602371335029602
Batch 50/64 loss: -0.12224501371383667
Batch 51/64 loss: -0.15244519710540771
Batch 52/64 loss: -0.16589194536209106
Batch 53/64 loss: -0.15140080451965332
Batch 54/64 loss: -0.15970712900161743
Batch 55/64 loss: -0.1504470705986023
Batch 56/64 loss: -0.1632102131843567
Batch 57/64 loss: -0.16099482774734497
Batch 58/64 loss: -0.15587592124938965
Batch 59/64 loss: -0.16910040378570557
Batch 60/64 loss: -0.14247900247573853
Batch 61/64 loss: -0.14959752559661865
Batch 62/64 loss: -0.14403140544891357
Batch 63/64 loss: -0.16337895393371582
Batch 64/64 loss: -0.15125298500061035
Epoch 268  Train loss: -0.1534384147793639  Val loss: -0.043605702644361255
Epoch 269
-------------------------------
Batch 1/64 loss: -0.16192662715911865
Batch 2/64 loss: -0.16112923622131348
Batch 3/64 loss: -0.15245121717453003
Batch 4/64 loss: -0.15400320291519165
Batch 5/64 loss: -0.1497134566307068
Batch 6/64 loss: -0.16147875785827637
Batch 7/64 loss: -0.145258367061615
Batch 8/64 loss: -0.14530020952224731
Batch 9/64 loss: -0.16335898637771606
Batch 10/64 loss: -0.1574907898902893
Batch 11/64 loss: -0.1619020700454712
Batch 12/64 loss: -0.1639195680618286
Batch 13/64 loss: -0.16418087482452393
Batch 14/64 loss: -0.16142487525939941
Batch 15/64 loss: -0.1493564248085022
Batch 16/64 loss: -0.1466166377067566
Batch 17/64 loss: -0.16481924057006836
Batch 18/64 loss: -0.15253597497940063
Batch 19/64 loss: -0.1606864333152771
Batch 20/64 loss: -0.15630590915679932
Batch 21/64 loss: -0.1632457971572876
Batch 22/64 loss: -0.1603999137878418
Batch 23/64 loss: -0.15205323696136475
Batch 24/64 loss: -0.16014909744262695
Batch 25/64 loss: -0.13677150011062622
Batch 26/64 loss: -0.15936905145645142
Batch 27/64 loss: -0.12431257963180542
Batch 28/64 loss: -0.14023810625076294
Batch 29/64 loss: -0.15016663074493408
Batch 30/64 loss: -0.14680635929107666
Batch 31/64 loss: -0.16415780782699585
Batch 32/64 loss: -0.14620518684387207
Batch 33/64 loss: -0.16605055332183838
Batch 34/64 loss: -0.15667450428009033
Batch 35/64 loss: -0.15560954809188843
Batch 36/64 loss: -0.12407225370407104
Batch 37/64 loss: -0.12742483615875244
Batch 38/64 loss: -0.149003803730011
Batch 39/64 loss: -0.16117554903030396
Batch 40/64 loss: -0.14536243677139282
Batch 41/64 loss: -0.1515514850616455
Batch 42/64 loss: -0.14754462242126465
Batch 43/64 loss: -0.15480411052703857
Batch 44/64 loss: -0.15144604444503784
Batch 45/64 loss: -0.1519174575805664
Batch 46/64 loss: -0.1655617356300354
Batch 47/64 loss: -0.14321351051330566
Batch 48/64 loss: -0.1464424729347229
Batch 49/64 loss: -0.14065897464752197
Batch 50/64 loss: -0.13003504276275635
Batch 51/64 loss: -0.15187788009643555
Batch 52/64 loss: -0.1461377739906311
Batch 53/64 loss: -0.15439510345458984
Batch 54/64 loss: -0.17224526405334473
Batch 55/64 loss: -0.147047221660614
Batch 56/64 loss: -0.13582170009613037
Batch 57/64 loss: -0.15286332368850708
Batch 58/64 loss: -0.13319259881973267
Batch 59/64 loss: -0.1463027000427246
Batch 60/64 loss: -0.14953011274337769
Batch 61/64 loss: -0.13607889413833618
Batch 62/64 loss: -0.13533324003219604
Batch 63/64 loss: -0.11734926700592041
Batch 64/64 loss: -0.14785748720169067
Epoch 269  Train loss: -0.15051531394322712  Val loss: -0.04312912950810698
Epoch 270
-------------------------------
Batch 1/64 loss: -0.14949721097946167
Batch 2/64 loss: -0.16095423698425293
Batch 3/64 loss: -0.15128105878829956
Batch 4/64 loss: -0.16681581735610962
Batch 5/64 loss: -0.14786267280578613
Batch 6/64 loss: -0.15757977962493896
Batch 7/64 loss: -0.1546492576599121
Batch 8/64 loss: -0.15632915496826172
Batch 9/64 loss: -0.15830248594284058
Batch 10/64 loss: -0.14107012748718262
Batch 11/64 loss: -0.1428866982460022
Batch 12/64 loss: -0.1555027961730957
Batch 13/64 loss: -0.16304075717926025
Batch 14/64 loss: -0.13379621505737305
Batch 15/64 loss: -0.16214138269424438
Batch 16/64 loss: -0.17012691497802734
Batch 17/64 loss: -0.16580307483673096
Batch 18/64 loss: -0.16273266077041626
Batch 19/64 loss: -0.14762967824935913
Batch 20/64 loss: -0.12231653928756714
Batch 21/64 loss: -0.1604245901107788
Batch 22/64 loss: -0.14275866746902466
Batch 23/64 loss: -0.16751235723495483
Batch 24/64 loss: -0.14480793476104736
Batch 25/64 loss: -0.14615774154663086
Batch 26/64 loss: -0.13049721717834473
Batch 27/64 loss: -0.15385973453521729
Batch 28/64 loss: -0.15972477197647095
Batch 29/64 loss: -0.1589573621749878
Batch 30/64 loss: -0.17133855819702148
Batch 31/64 loss: -0.13323968648910522
Batch 32/64 loss: -0.1557936668395996
Batch 33/64 loss: -0.1514187455177307
Batch 34/64 loss: -0.14509940147399902
Batch 35/64 loss: -0.13972175121307373
Batch 36/64 loss: -0.13329064846038818
Batch 37/64 loss: -0.14730304479599
Batch 38/64 loss: -0.14666813611984253
Batch 39/64 loss: -0.14751273393630981
Batch 40/64 loss: -0.16493761539459229
Batch 41/64 loss: -0.1521531343460083
Batch 42/64 loss: -0.16032367944717407
Batch 43/64 loss: -0.13933151960372925
Batch 44/64 loss: -0.1636589765548706
Batch 45/64 loss: -0.16014152765274048
Batch 46/64 loss: -0.13874483108520508
Batch 47/64 loss: -0.16188907623291016
Batch 48/64 loss: -0.15210145711898804
Batch 49/64 loss: -0.13754284381866455
Batch 50/64 loss: -0.16873282194137573
Batch 51/64 loss: -0.15555256605148315
Batch 52/64 loss: -0.15539216995239258
Batch 53/64 loss: -0.16811120510101318
Batch 54/64 loss: -0.12313228845596313
Batch 55/64 loss: -0.15812480449676514
Batch 56/64 loss: -0.16100049018859863
Batch 57/64 loss: -0.16011905670166016
Batch 58/64 loss: -0.1629384160041809
Batch 59/64 loss: -0.15707427263259888
Batch 60/64 loss: -0.16340315341949463
Batch 61/64 loss: -0.147882878780365
Batch 62/64 loss: -0.16035383939743042
Batch 63/64 loss: -0.15611302852630615
Batch 64/64 loss: -0.12731599807739258
Epoch 270  Train loss: -0.15263761445587756  Val loss: -0.04358757095238597
Epoch 271
-------------------------------
Batch 1/64 loss: -0.17112427949905396
Batch 2/64 loss: -0.16254639625549316
Batch 3/64 loss: -0.16616129875183105
Batch 4/64 loss: -0.1836743950843811
Batch 5/64 loss: -0.16546201705932617
Batch 6/64 loss: -0.1442098617553711
Batch 7/64 loss: -0.14286267757415771
Batch 8/64 loss: -0.1349714994430542
Batch 9/64 loss: -0.14584565162658691
Batch 10/64 loss: -0.13911819458007812
Batch 11/64 loss: -0.14960461854934692
Batch 12/64 loss: -0.14425241947174072
Batch 13/64 loss: -0.14425629377365112
Batch 14/64 loss: -0.14963191747665405
Batch 15/64 loss: -0.1552969217300415
Batch 16/64 loss: -0.14954644441604614
Batch 17/64 loss: -0.15252035856246948
Batch 18/64 loss: -0.14433276653289795
Batch 19/64 loss: -0.14560168981552124
Batch 20/64 loss: -0.1292549967765808
Batch 21/64 loss: -0.13434797525405884
Batch 22/64 loss: -0.13925260305404663
Batch 23/64 loss: -0.13221889734268188
Batch 24/64 loss: -0.14244937896728516
Batch 25/64 loss: -0.1421489715576172
Batch 26/64 loss: -0.14228707551956177
Batch 27/64 loss: -0.12122905254364014
Batch 28/64 loss: -0.13671809434890747
Batch 29/64 loss: -0.13687336444854736
Batch 30/64 loss: -0.15915364027023315
Batch 31/64 loss: -0.16167688369750977
Batch 32/64 loss: -0.1638171672821045
Batch 33/64 loss: -0.15449583530426025
Batch 34/64 loss: -0.16918396949768066
Batch 35/64 loss: -0.16803890466690063
Batch 36/64 loss: -0.15427815914154053
Batch 37/64 loss: -0.1655680537223816
Batch 38/64 loss: -0.15831971168518066
Batch 39/64 loss: -0.16855943202972412
Batch 40/64 loss: -0.12234872579574585
Batch 41/64 loss: -0.13402646780014038
Batch 42/64 loss: -0.15875154733657837
Batch 43/64 loss: -0.1499273180961609
Batch 44/64 loss: -0.15766990184783936
Batch 45/64 loss: -0.15439260005950928
Batch 46/64 loss: -0.14788246154785156
Batch 47/64 loss: -0.13974857330322266
Batch 48/64 loss: -0.1651173233985901
Batch 49/64 loss: -0.11982512474060059
Batch 50/64 loss: -0.14866894483566284
Batch 51/64 loss: -0.1426514983177185
Batch 52/64 loss: -0.16762059926986694
Batch 53/64 loss: -0.12737512588500977
Batch 54/64 loss: -0.15730124711990356
Batch 55/64 loss: -0.14752960205078125
Batch 56/64 loss: -0.13362383842468262
Batch 57/64 loss: -0.1530049443244934
Batch 58/64 loss: -0.1292460560798645
Batch 59/64 loss: -0.1540989875793457
Batch 60/64 loss: -0.1505315899848938
Batch 61/64 loss: -0.1578882932662964
Batch 62/64 loss: -0.13843870162963867
Batch 63/64 loss: -0.15688210725784302
Batch 64/64 loss: -0.15556234121322632
Epoch 271  Train loss: -0.14905278752831852  Val loss: -0.04350528704751398
Epoch 272
-------------------------------
Batch 1/64 loss: -0.16148757934570312
Batch 2/64 loss: -0.17107707262039185
Batch 3/64 loss: -0.1280961036682129
Batch 4/64 loss: -0.16507041454315186
Batch 5/64 loss: -0.15130603313446045
Batch 6/64 loss: -0.14572173357009888
Batch 7/64 loss: -0.15676921606063843
Batch 8/64 loss: -0.16750186681747437
Batch 9/64 loss: -0.16491520404815674
Batch 10/64 loss: -0.1563296914100647
Batch 11/64 loss: -0.16575539112091064
Batch 12/64 loss: -0.1630561351776123
Batch 13/64 loss: -0.16057425737380981
Batch 14/64 loss: -0.14873993396759033
Batch 15/64 loss: -0.1704992651939392
Batch 16/64 loss: -0.16069483757019043
Batch 17/64 loss: -0.1401505470275879
Batch 18/64 loss: -0.15748053789138794
Batch 19/64 loss: -0.16144287586212158
Batch 20/64 loss: -0.13571977615356445
Batch 21/64 loss: -0.1588253378868103
Batch 22/64 loss: -0.1625666618347168
Batch 23/64 loss: -0.13831740617752075
Batch 24/64 loss: -0.1392613649368286
Batch 25/64 loss: -0.1344287395477295
Batch 26/64 loss: -0.12146294116973877
Batch 27/64 loss: -0.1559543013572693
Batch 28/64 loss: -0.16331344842910767
Batch 29/64 loss: -0.1252632737159729
Batch 30/64 loss: -0.16573214530944824
Batch 31/64 loss: -0.1252964735031128
Batch 32/64 loss: -0.1561371088027954
Batch 33/64 loss: -0.14176899194717407
Batch 34/64 loss: -0.16066741943359375
Batch 35/64 loss: -0.1661151647567749
Batch 36/64 loss: -0.13013094663619995
Batch 37/64 loss: -0.12516403198242188
Batch 38/64 loss: -0.13757747411727905
Batch 39/64 loss: -0.16979163885116577
Batch 40/64 loss: -0.15070486068725586
Batch 41/64 loss: -0.15797388553619385
Batch 42/64 loss: -0.1421082615852356
Batch 43/64 loss: -0.15985894203186035
Batch 44/64 loss: -0.14566701650619507
Batch 45/64 loss: -0.16252481937408447
Batch 46/64 loss: -0.13232290744781494
Batch 47/64 loss: -0.1330019235610962
Batch 48/64 loss: -0.1518375277519226
Batch 49/64 loss: -0.13556146621704102
Batch 50/64 loss: -0.13193011283874512
Batch 51/64 loss: -0.14976561069488525
Batch 52/64 loss: -0.15652447938919067
Batch 53/64 loss: -0.1745051145553589
Batch 54/64 loss: -0.15859311819076538
Batch 55/64 loss: -0.14644986391067505
Batch 56/64 loss: -0.12249600887298584
Batch 57/64 loss: -0.13680016994476318
Batch 58/64 loss: -0.14524996280670166
Batch 59/64 loss: -0.15360581874847412
Batch 60/64 loss: -0.15721243619918823
Batch 61/64 loss: -0.15877622365951538
Batch 62/64 loss: -0.16400575637817383
Batch 63/64 loss: -0.14685070514678955
Batch 64/64 loss: -0.12732237577438354
Epoch 272  Train loss: -0.15027422928342632  Val loss: -0.04662189389422178
Epoch 273
-------------------------------
Batch 1/64 loss: -0.1635206937789917
Batch 2/64 loss: -0.14635765552520752
Batch 3/64 loss: -0.1481878161430359
Batch 4/64 loss: -0.15526938438415527
Batch 5/64 loss: -0.1374044418334961
Batch 6/64 loss: -0.15275055170059204
Batch 7/64 loss: -0.1445673108100891
Batch 8/64 loss: -0.13931673765182495
Batch 9/64 loss: -0.1318332552909851
Batch 10/64 loss: -0.15122562646865845
Batch 11/64 loss: -0.15827429294586182
Batch 12/64 loss: -0.1743519902229309
Batch 13/64 loss: -0.16599911451339722
Batch 14/64 loss: -0.1575641632080078
Batch 15/64 loss: -0.14482557773590088
Batch 16/64 loss: -0.14389115571975708
Batch 17/64 loss: -0.15345865488052368
Batch 18/64 loss: -0.1390312910079956
Batch 19/64 loss: -0.16983449459075928
Batch 20/64 loss: -0.15449446439743042
Batch 21/64 loss: -0.15963387489318848
Batch 22/64 loss: -0.16950368881225586
Batch 23/64 loss: -0.11716622114181519
Batch 24/64 loss: -0.16875016689300537
Batch 25/64 loss: -0.15597254037857056
Batch 26/64 loss: -0.15598446130752563
Batch 27/64 loss: -0.16836798191070557
Batch 28/64 loss: -0.15976089239120483
Batch 29/64 loss: -0.15318256616592407
Batch 30/64 loss: -0.16531622409820557
Batch 31/64 loss: -0.1277068853378296
Batch 32/64 loss: -0.15642213821411133
Batch 33/64 loss: -0.15152573585510254
Batch 34/64 loss: -0.15609252452850342
Batch 35/64 loss: -0.15724408626556396
Batch 36/64 loss: -0.16359573602676392
Batch 37/64 loss: -0.14946770668029785
Batch 38/64 loss: -0.1482728123664856
Batch 39/64 loss: -0.17099124193191528
Batch 40/64 loss: -0.16636884212493896
Batch 41/64 loss: -0.16660737991333008
Batch 42/64 loss: -0.13788831233978271
Batch 43/64 loss: -0.16484880447387695
Batch 44/64 loss: -0.15434658527374268
Batch 45/64 loss: -0.16431641578674316
Batch 46/64 loss: -0.16617852449417114
Batch 47/64 loss: -0.16882508993148804
Batch 48/64 loss: -0.1478162407875061
Batch 49/64 loss: -0.13618820905685425
Batch 50/64 loss: -0.15349459648132324
Batch 51/64 loss: -0.1616806983947754
Batch 52/64 loss: -0.15237879753112793
Batch 53/64 loss: -0.15098291635513306
Batch 54/64 loss: -0.16266131401062012
Batch 55/64 loss: -0.13867336511611938
Batch 56/64 loss: -0.15076547861099243
Batch 57/64 loss: -0.16624760627746582
Batch 58/64 loss: -0.13364166021347046
Batch 59/64 loss: -0.15373820066452026
Batch 60/64 loss: -0.1498088240623474
Batch 61/64 loss: -0.13854432106018066
Batch 62/64 loss: -0.1573927402496338
Batch 63/64 loss: -0.14402097463607788
Batch 64/64 loss: -0.14520996809005737
Epoch 273  Train loss: -0.1534657181478014  Val loss: -0.046502867105490564
Epoch 274
-------------------------------
Batch 1/64 loss: -0.15449142456054688
Batch 2/64 loss: -0.15067362785339355
Batch 3/64 loss: -0.15794521570205688
Batch 4/64 loss: -0.17317330837249756
Batch 5/64 loss: -0.1598415970802307
Batch 6/64 loss: -0.14929533004760742
Batch 7/64 loss: -0.12203919887542725
Batch 8/64 loss: -0.15602636337280273
Batch 9/64 loss: -0.14119398593902588
Batch 10/64 loss: -0.1239967942237854
Batch 11/64 loss: -0.15559756755828857
Batch 12/64 loss: -0.1360875964164734
Batch 13/64 loss: -0.1615639328956604
Batch 14/64 loss: -0.1645742654800415
Batch 15/64 loss: -0.1319904327392578
Batch 16/64 loss: -0.14948314428329468
Batch 17/64 loss: -0.1598777174949646
Batch 18/64 loss: -0.15707707405090332
Batch 19/64 loss: -0.15202027559280396
Batch 20/64 loss: -0.1339731216430664
Batch 21/64 loss: -0.15552157163619995
Batch 22/64 loss: -0.17432785034179688
Batch 23/64 loss: -0.14133864641189575
Batch 24/64 loss: -0.16731053590774536
Batch 25/64 loss: -0.15558946132659912
Batch 26/64 loss: -0.1559373140335083
Batch 27/64 loss: -0.14088129997253418
Batch 28/64 loss: -0.16307544708251953
Batch 29/64 loss: -0.16109639406204224
Batch 30/64 loss: -0.14323359727859497
Batch 31/64 loss: -0.13830602169036865
Batch 32/64 loss: -0.17541992664337158
Batch 33/64 loss: -0.1293136477470398
Batch 34/64 loss: -0.15253233909606934
Batch 35/64 loss: -0.16654014587402344
Batch 36/64 loss: -0.13899332284927368
Batch 37/64 loss: -0.15784776210784912
Batch 38/64 loss: -0.15974712371826172
Batch 39/64 loss: -0.16548049449920654
Batch 40/64 loss: -0.15405625104904175
Batch 41/64 loss: -0.15913331508636475
Batch 42/64 loss: -0.13933086395263672
Batch 43/64 loss: -0.15632981061935425
Batch 44/64 loss: -0.16603171825408936
Batch 45/64 loss: -0.16984063386917114
Batch 46/64 loss: -0.15894579887390137
Batch 47/64 loss: -0.1377459168434143
Batch 48/64 loss: -0.1522747278213501
Batch 49/64 loss: -0.1585283875465393
Batch 50/64 loss: -0.16721433401107788
Batch 51/64 loss: -0.1472422480583191
Batch 52/64 loss: -0.15638333559036255
Batch 53/64 loss: -0.12669438123703003
Batch 54/64 loss: -0.14902913570404053
Batch 55/64 loss: -0.15324914455413818
Batch 56/64 loss: -0.16254013776779175
Batch 57/64 loss: -0.1661229133605957
Batch 58/64 loss: -0.1546904444694519
Batch 59/64 loss: -0.1634274125099182
Batch 60/64 loss: -0.1599823236465454
Batch 61/64 loss: -0.15720409154891968
Batch 62/64 loss: -0.1617295742034912
Batch 63/64 loss: -0.16106778383255005
Batch 64/64 loss: -0.16255146265029907
Epoch 274  Train loss: -0.15363330443700154  Val loss: -0.04857189794586286
Epoch 275
-------------------------------
Batch 1/64 loss: -0.17534953355789185
Batch 2/64 loss: -0.15861713886260986
Batch 3/64 loss: -0.15677803754806519
Batch 4/64 loss: -0.1543155312538147
Batch 5/64 loss: -0.1648883819580078
Batch 6/64 loss: -0.16047966480255127
Batch 7/64 loss: -0.15079087018966675
Batch 8/64 loss: -0.159984290599823
Batch 9/64 loss: -0.16591191291809082
Batch 10/64 loss: -0.16499674320220947
Batch 11/64 loss: -0.16947048902511597
Batch 12/64 loss: -0.1440514326095581
Batch 13/64 loss: -0.14917701482772827
Batch 14/64 loss: -0.17779135704040527
Batch 15/64 loss: -0.16149425506591797
Batch 16/64 loss: -0.1592411994934082
Batch 17/64 loss: -0.16933029890060425
Batch 18/64 loss: -0.15811687707901
Batch 19/64 loss: -0.17326223850250244
Batch 20/64 loss: -0.16034269332885742
Batch 21/64 loss: -0.15138840675354004
Batch 22/64 loss: -0.15336519479751587
Batch 23/64 loss: -0.14262712001800537
Batch 24/64 loss: -0.12409782409667969
Batch 25/64 loss: -0.14977240562438965
Batch 26/64 loss: -0.14898645877838135
Batch 27/64 loss: -0.16745728254318237
Batch 28/64 loss: -0.1626315712928772
Batch 29/64 loss: -0.17711549997329712
Batch 30/64 loss: -0.134979248046875
Batch 31/64 loss: -0.13918274641036987
Batch 32/64 loss: -0.1300932765007019
Batch 33/64 loss: -0.13782018423080444
Batch 34/64 loss: -0.1706470251083374
Batch 35/64 loss: -0.17824578285217285
Batch 36/64 loss: -0.15275055170059204
Batch 37/64 loss: -0.14370596408843994
Batch 38/64 loss: -0.14624392986297607
Batch 39/64 loss: -0.15716075897216797
Batch 40/64 loss: -0.1406114101409912
Batch 41/64 loss: -0.15833663940429688
Batch 42/64 loss: -0.1574602723121643
Batch 43/64 loss: -0.16890281438827515
Batch 44/64 loss: -0.15698570013046265
Batch 45/64 loss: -0.1661626696586609
Batch 46/64 loss: -0.13293111324310303
Batch 47/64 loss: -0.14915555715560913
Batch 48/64 loss: -0.1553090214729309
Batch 49/64 loss: -0.15112006664276123
Batch 50/64 loss: -0.13165265321731567
Batch 51/64 loss: -0.15595299005508423
Batch 52/64 loss: -0.1554720401763916
Batch 53/64 loss: -0.17443585395812988
Batch 54/64 loss: -0.13200408220291138
Batch 55/64 loss: -0.15356826782226562
Batch 56/64 loss: -0.1528211236000061
Batch 57/64 loss: -0.15600216388702393
Batch 58/64 loss: -0.12430351972579956
Batch 59/64 loss: -0.1574547290802002
Batch 60/64 loss: -0.13503432273864746
Batch 61/64 loss: -0.1513250470161438
Batch 62/64 loss: -0.16158396005630493
Batch 63/64 loss: -0.12062901258468628
Batch 64/64 loss: -0.13066476583480835
Epoch 275  Train loss: -0.15372352623472027  Val loss: -0.042175450685507654
Epoch 276
-------------------------------
Batch 1/64 loss: -0.15304768085479736
Batch 2/64 loss: -0.15193605422973633
Batch 3/64 loss: -0.16381269693374634
Batch 4/64 loss: -0.15853041410446167
Batch 5/64 loss: -0.16305500268936157
Batch 6/64 loss: -0.16586953401565552
Batch 7/64 loss: -0.173639714717865
Batch 8/64 loss: -0.1620844602584839
Batch 9/64 loss: -0.14929765462875366
Batch 10/64 loss: -0.1587812304496765
Batch 11/64 loss: -0.14506196975708008
Batch 12/64 loss: -0.15513163805007935
Batch 13/64 loss: -0.17497050762176514
Batch 14/64 loss: -0.1677621603012085
Batch 15/64 loss: -0.1690577268600464
Batch 16/64 loss: -0.13326656818389893
Batch 17/64 loss: -0.1632024645805359
Batch 18/64 loss: -0.1825084686279297
Batch 19/64 loss: -0.1663799285888672
Batch 20/64 loss: -0.1430269479751587
Batch 21/64 loss: -0.18113994598388672
Batch 22/64 loss: -0.15075671672821045
Batch 23/64 loss: -0.16917002201080322
Batch 24/64 loss: -0.16793465614318848
Batch 25/64 loss: -0.15365999937057495
Batch 26/64 loss: -0.16948401927947998
Batch 27/64 loss: -0.1708216667175293
Batch 28/64 loss: -0.1453288197517395
Batch 29/64 loss: -0.15856826305389404
Batch 30/64 loss: -0.13730734586715698
Batch 31/64 loss: -0.1477452516555786
Batch 32/64 loss: -0.13559383153915405
Batch 33/64 loss: -0.1667594313621521
Batch 34/64 loss: -0.15518313646316528
Batch 35/64 loss: -0.1652960181236267
Batch 36/64 loss: -0.14144301414489746
Batch 37/64 loss: -0.14120912551879883
Batch 38/64 loss: -0.16808587312698364
Batch 39/64 loss: -0.15587228536605835
Batch 40/64 loss: -0.15777242183685303
Batch 41/64 loss: -0.14803200960159302
Batch 42/64 loss: -0.13442057371139526
Batch 43/64 loss: -0.1533752679824829
Batch 44/64 loss: -0.14635956287384033
Batch 45/64 loss: -0.1632363200187683
Batch 46/64 loss: -0.1467323899269104
Batch 47/64 loss: -0.14931023120880127
Batch 48/64 loss: -0.17370712757110596
Batch 49/64 loss: -0.16830575466156006
Batch 50/64 loss: -0.14871692657470703
Batch 51/64 loss: -0.15368926525115967
Batch 52/64 loss: -0.13716888427734375
Batch 53/64 loss: -0.15282189846038818
Batch 54/64 loss: -0.15899956226348877
Batch 55/64 loss: -0.1648273468017578
Batch 56/64 loss: -0.14827120304107666
Batch 57/64 loss: -0.15688025951385498
Batch 58/64 loss: -0.15681153535842896
Batch 59/64 loss: -0.14745044708251953
Batch 60/64 loss: -0.17071163654327393
Batch 61/64 loss: -0.15935158729553223
Batch 62/64 loss: -0.15534204244613647
Batch 63/64 loss: -0.16568970680236816
Batch 64/64 loss: -0.13383424282073975
Epoch 276  Train loss: -0.15686497080559825  Val loss: -0.043995150176110547
Epoch 277
-------------------------------
Batch 1/64 loss: -0.15640020370483398
Batch 2/64 loss: -0.14880281686782837
Batch 3/64 loss: -0.15877926349639893
Batch 4/64 loss: -0.14858800172805786
Batch 5/64 loss: -0.16451048851013184
Batch 6/64 loss: -0.14438718557357788
Batch 7/64 loss: -0.16026753187179565
Batch 8/64 loss: -0.15158188343048096
Batch 9/64 loss: -0.16662997007369995
Batch 10/64 loss: -0.16492140293121338
Batch 11/64 loss: -0.1542772650718689
Batch 12/64 loss: -0.1592768430709839
Batch 13/64 loss: -0.16261625289916992
Batch 14/64 loss: -0.1435742974281311
Batch 15/64 loss: -0.16685396432876587
Batch 16/64 loss: -0.16294854879379272
Batch 17/64 loss: -0.17636597156524658
Batch 18/64 loss: -0.15512949228286743
Batch 19/64 loss: -0.16135108470916748
Batch 20/64 loss: -0.12484019994735718
Batch 21/64 loss: -0.15529751777648926
Batch 22/64 loss: -0.14327311515808105
Batch 23/64 loss: -0.15782994031906128
Batch 24/64 loss: -0.14863187074661255
Batch 25/64 loss: -0.17742377519607544
Batch 26/64 loss: -0.16417908668518066
Batch 27/64 loss: -0.15328067541122437
Batch 28/64 loss: -0.16155123710632324
Batch 29/64 loss: -0.16933244466781616
Batch 30/64 loss: -0.16465318202972412
Batch 31/64 loss: -0.1618935465812683
Batch 32/64 loss: -0.16341549158096313
Batch 33/64 loss: -0.16594171524047852
Batch 34/64 loss: -0.17522841691970825
Batch 35/64 loss: -0.1568596363067627
Batch 36/64 loss: -0.1532854437828064
Batch 37/64 loss: -0.17218565940856934
Batch 38/64 loss: -0.16404980421066284
Batch 39/64 loss: -0.17642134428024292
Batch 40/64 loss: -0.13875579833984375
Batch 41/64 loss: -0.12310254573822021
Batch 42/64 loss: -0.1548703908920288
Batch 43/64 loss: -0.14876961708068848
Batch 44/64 loss: -0.12536650896072388
Batch 45/64 loss: -0.17684441804885864
Batch 46/64 loss: -0.14927279949188232
Batch 47/64 loss: -0.14924192428588867
Batch 48/64 loss: -0.16776371002197266
Batch 49/64 loss: -0.14970988035202026
Batch 50/64 loss: -0.1453711986541748
Batch 51/64 loss: -0.17469167709350586
Batch 52/64 loss: -0.1676623821258545
Batch 53/64 loss: -0.17976891994476318
Batch 54/64 loss: -0.16780370473861694
Batch 55/64 loss: -0.16189825534820557
Batch 56/64 loss: -0.14613497257232666
Batch 57/64 loss: -0.14046907424926758
Batch 58/64 loss: -0.16363525390625
Batch 59/64 loss: -0.17060703039169312
Batch 60/64 loss: -0.15738344192504883
Batch 61/64 loss: -0.16049939393997192
Batch 62/64 loss: -0.1377134919166565
Batch 63/64 loss: -0.15864628553390503
Batch 64/64 loss: -0.13518130779266357
Epoch 277  Train loss: -0.1573992976955339  Val loss: -0.04720822580901208
Epoch 278
-------------------------------
Batch 1/64 loss: -0.14293521642684937
Batch 2/64 loss: -0.1635332703590393
Batch 3/64 loss: -0.13556671142578125
Batch 4/64 loss: -0.14166510105133057
Batch 5/64 loss: -0.15632164478302002
Batch 6/64 loss: -0.1507137417793274
Batch 7/64 loss: -0.16563230752944946
Batch 8/64 loss: -0.13916075229644775
Batch 9/64 loss: -0.1788334846496582
Batch 10/64 loss: -0.15924811363220215
Batch 11/64 loss: -0.17178523540496826
Batch 12/64 loss: -0.1678473949432373
Batch 13/64 loss: -0.1711176633834839
Batch 14/64 loss: -0.15696734189987183
Batch 15/64 loss: -0.16603213548660278
Batch 16/64 loss: -0.17766833305358887
Batch 17/64 loss: -0.12111759185791016
Batch 18/64 loss: -0.14024651050567627
Batch 19/64 loss: -0.17283964157104492
Batch 20/64 loss: -0.1315077543258667
Batch 21/64 loss: -0.16444659233093262
Batch 22/64 loss: -0.17164015769958496
Batch 23/64 loss: -0.16989094018936157
Batch 24/64 loss: -0.1469985842704773
Batch 25/64 loss: -0.16028869152069092
Batch 26/64 loss: -0.149544358253479
Batch 27/64 loss: -0.1589455008506775
Batch 28/64 loss: -0.14927518367767334
Batch 29/64 loss: -0.14645349979400635
Batch 30/64 loss: -0.1589738130569458
Batch 31/64 loss: -0.1650906801223755
Batch 32/64 loss: -0.16664516925811768
Batch 33/64 loss: -0.1744910478591919
Batch 34/64 loss: -0.1641674041748047
Batch 35/64 loss: -0.15758538246154785
Batch 36/64 loss: -0.15866708755493164
Batch 37/64 loss: -0.15289568901062012
Batch 38/64 loss: -0.17487013339996338
Batch 39/64 loss: -0.15888690948486328
Batch 40/64 loss: -0.12628358602523804
Batch 41/64 loss: -0.1483960747718811
Batch 42/64 loss: -0.14862090349197388
Batch 43/64 loss: -0.16358739137649536
Batch 44/64 loss: -0.1499466896057129
Batch 45/64 loss: -0.16300839185714722
Batch 46/64 loss: -0.14846456050872803
Batch 47/64 loss: -0.15968209505081177
Batch 48/64 loss: -0.16794824600219727
Batch 49/64 loss: -0.1351332664489746
Batch 50/64 loss: -0.16320621967315674
Batch 51/64 loss: -0.14309346675872803
Batch 52/64 loss: -0.15061569213867188
Batch 53/64 loss: -0.168817937374115
Batch 54/64 loss: -0.1542009711265564
Batch 55/64 loss: -0.17273646593093872
Batch 56/64 loss: -0.13080567121505737
Batch 57/64 loss: -0.1700977087020874
Batch 58/64 loss: -0.15872663259506226
Batch 59/64 loss: -0.1444305181503296
Batch 60/64 loss: -0.1555578112602234
Batch 61/64 loss: -0.1428908109664917
Batch 62/64 loss: -0.17864376306533813
Batch 63/64 loss: -0.1458066701889038
Batch 64/64 loss: -0.17299681901931763
Epoch 278  Train loss: -0.15656385772368486  Val loss: -0.04293882785384188
Epoch 279
-------------------------------
Batch 1/64 loss: -0.1664828062057495
Batch 2/64 loss: -0.14930957555770874
Batch 3/64 loss: -0.17482972145080566
Batch 4/64 loss: -0.16944026947021484
Batch 5/64 loss: -0.1803126335144043
Batch 6/64 loss: -0.1655900478363037
Batch 7/64 loss: -0.16351568698883057
Batch 8/64 loss: -0.17793238162994385
Batch 9/64 loss: -0.16753768920898438
Batch 10/64 loss: -0.162897527217865
Batch 11/64 loss: -0.16792839765548706
Batch 12/64 loss: -0.1724071502685547
Batch 13/64 loss: -0.17207080125808716
Batch 14/64 loss: -0.17327773571014404
Batch 15/64 loss: -0.15566647052764893
Batch 16/64 loss: -0.16564244031906128
Batch 17/64 loss: -0.16556811332702637
Batch 18/64 loss: -0.1673683524131775
Batch 19/64 loss: -0.17655116319656372
Batch 20/64 loss: -0.15507006645202637
Batch 21/64 loss: -0.16273492574691772
Batch 22/64 loss: -0.15878641605377197
Batch 23/64 loss: -0.1508064866065979
Batch 24/64 loss: -0.1677284836769104
Batch 25/64 loss: -0.16743111610412598
Batch 26/64 loss: -0.15209412574768066
Batch 27/64 loss: -0.1675785779953003
Batch 28/64 loss: -0.1530185341835022
Batch 29/64 loss: -0.15821248292922974
Batch 30/64 loss: -0.1697695255279541
Batch 31/64 loss: -0.14046496152877808
Batch 32/64 loss: -0.16857004165649414
Batch 33/64 loss: -0.16717183589935303
Batch 34/64 loss: -0.15714025497436523
Batch 35/64 loss: -0.1511515974998474
Batch 36/64 loss: -0.16577446460723877
Batch 37/64 loss: -0.17507916688919067
Batch 38/64 loss: -0.1496516466140747
Batch 39/64 loss: -0.13752269744873047
Batch 40/64 loss: -0.14332276582717896
Batch 41/64 loss: -0.1816306710243225
Batch 42/64 loss: -0.12311553955078125
Batch 43/64 loss: -0.1552363634109497
Batch 44/64 loss: -0.1450042724609375
Batch 45/64 loss: -0.15315967798233032
Batch 46/64 loss: -0.16448843479156494
Batch 47/64 loss: -0.17250043153762817
Batch 48/64 loss: -0.14363181591033936
Batch 49/64 loss: -0.1403135061264038
Batch 50/64 loss: -0.17458468675613403
Batch 51/64 loss: -0.16466689109802246
Batch 52/64 loss: -0.15842968225479126
Batch 53/64 loss: -0.1609531044960022
Batch 54/64 loss: -0.1602117419242859
Batch 55/64 loss: -0.16668933629989624
Batch 56/64 loss: -0.13511395454406738
Batch 57/64 loss: -0.16347306966781616
Batch 58/64 loss: -0.16255617141723633
Batch 59/64 loss: -0.15862780809402466
Batch 60/64 loss: -0.14095211029052734
Batch 61/64 loss: -0.1708126664161682
Batch 62/64 loss: -0.1633230447769165
Batch 63/64 loss: -0.14758765697479248
Batch 64/64 loss: -0.1310659646987915
Epoch 279  Train loss: -0.16029445096558215  Val loss: -0.04462643689715985
Epoch 280
-------------------------------
Batch 1/64 loss: -0.14415931701660156
Batch 2/64 loss: -0.16965878009796143
Batch 3/64 loss: -0.1664290428161621
Batch 4/64 loss: -0.14596354961395264
Batch 5/64 loss: -0.14763176441192627
Batch 6/64 loss: -0.1572026014328003
Batch 7/64 loss: -0.15580260753631592
Batch 8/64 loss: -0.17698562145233154
Batch 9/64 loss: -0.178483247756958
Batch 10/64 loss: -0.15452510118484497
Batch 11/64 loss: -0.15831917524337769
Batch 12/64 loss: -0.15593189001083374
Batch 13/64 loss: -0.14977961778640747
Batch 14/64 loss: -0.16728508472442627
Batch 15/64 loss: -0.1810705065727234
Batch 16/64 loss: -0.16778719425201416
Batch 17/64 loss: -0.1470981240272522
Batch 18/64 loss: -0.16332721710205078
Batch 19/64 loss: -0.15447473526000977
Batch 20/64 loss: -0.1792619228363037
Batch 21/64 loss: -0.15584290027618408
Batch 22/64 loss: -0.1319185495376587
Batch 23/64 loss: -0.15722393989562988
Batch 24/64 loss: -0.14807742834091187
Batch 25/64 loss: -0.15140146017074585
Batch 26/64 loss: -0.14987659454345703
Batch 27/64 loss: -0.159399151802063
Batch 28/64 loss: -0.15674465894699097
Batch 29/64 loss: -0.14813822507858276
Batch 30/64 loss: -0.15196532011032104
Batch 31/64 loss: -0.15836364030838013
Batch 32/64 loss: -0.13609671592712402
Batch 33/64 loss: -0.16728031635284424
Batch 34/64 loss: -0.15392494201660156
Batch 35/64 loss: -0.1575474739074707
Batch 36/64 loss: -0.1588829755783081
Batch 37/64 loss: -0.14639317989349365
Batch 38/64 loss: -0.1650967001914978
Batch 39/64 loss: -0.17262911796569824
Batch 40/64 loss: -0.153281569480896
Batch 41/64 loss: -0.17307090759277344
Batch 42/64 loss: -0.1281116008758545
Batch 43/64 loss: -0.17242956161499023
Batch 44/64 loss: -0.15952062606811523
Batch 45/64 loss: -0.16092008352279663
Batch 46/64 loss: -0.1658933162689209
Batch 47/64 loss: -0.17856967449188232
Batch 48/64 loss: -0.16600024700164795
Batch 49/64 loss: -0.15024292469024658
Batch 50/64 loss: -0.15853559970855713
Batch 51/64 loss: -0.11644977331161499
Batch 52/64 loss: -0.13080650568008423
Batch 53/64 loss: -0.15880048274993896
Batch 54/64 loss: -0.15928101539611816
Batch 55/64 loss: -0.16276520490646362
Batch 56/64 loss: -0.1609351634979248
Batch 57/64 loss: -0.16184329986572266
Batch 58/64 loss: -0.17215347290039062
Batch 59/64 loss: -0.1575922966003418
Batch 60/64 loss: -0.15908461809158325
Batch 61/64 loss: -0.17022323608398438
Batch 62/64 loss: -0.16095221042633057
Batch 63/64 loss: -0.15031099319458008
Batch 64/64 loss: -0.1651926040649414
Epoch 280  Train loss: -0.1577984192792107  Val loss: -0.04515209394631926
Epoch 281
-------------------------------
Batch 1/64 loss: -0.17738956212997437
Batch 2/64 loss: -0.1616317629814148
Batch 3/64 loss: -0.14573681354522705
Batch 4/64 loss: -0.17549139261245728
Batch 5/64 loss: -0.15979397296905518
Batch 6/64 loss: -0.17484188079833984
Batch 7/64 loss: -0.17021536827087402
Batch 8/64 loss: -0.17334818840026855
Batch 9/64 loss: -0.16877275705337524
Batch 10/64 loss: -0.1853809356689453
Batch 11/64 loss: -0.17429524660110474
Batch 12/64 loss: -0.1679215431213379
Batch 13/64 loss: -0.15124386548995972
Batch 14/64 loss: -0.16259914636611938
Batch 15/64 loss: -0.16506046056747437
Batch 16/64 loss: -0.18484294414520264
Batch 17/64 loss: -0.14463073015213013
Batch 18/64 loss: -0.15620529651641846
Batch 19/64 loss: -0.15809625387191772
Batch 20/64 loss: -0.14905571937561035
Batch 21/64 loss: -0.153875470161438
Batch 22/64 loss: -0.18575680255889893
Batch 23/64 loss: -0.1697169542312622
Batch 24/64 loss: -0.14445209503173828
Batch 25/64 loss: -0.1639559268951416
Batch 26/64 loss: -0.13598662614822388
Batch 27/64 loss: -0.14640897512435913
Batch 28/64 loss: -0.1419193148612976
Batch 29/64 loss: -0.16514307260513306
Batch 30/64 loss: -0.13870614767074585
Batch 31/64 loss: -0.17374134063720703
Batch 32/64 loss: -0.17364507913589478
Batch 33/64 loss: -0.17786413431167603
Batch 34/64 loss: -0.1591562032699585
Batch 35/64 loss: -0.16697299480438232
Batch 36/64 loss: -0.1668252944946289
Batch 37/64 loss: -0.17290383577346802
Batch 38/64 loss: -0.16799205541610718
Batch 39/64 loss: -0.1658320426940918
Batch 40/64 loss: -0.16607016324996948
Batch 41/64 loss: -0.1357995867729187
Batch 42/64 loss: -0.14864593744277954
Batch 43/64 loss: -0.17046064138412476
Batch 44/64 loss: -0.15821874141693115
Batch 45/64 loss: -0.16819965839385986
Batch 46/64 loss: -0.1575942039489746
Batch 47/64 loss: -0.16061657667160034
Batch 48/64 loss: -0.17882907390594482
Batch 49/64 loss: -0.1464734673500061
Batch 50/64 loss: -0.175173819065094
Batch 51/64 loss: -0.17357385158538818
Batch 52/64 loss: -0.18018198013305664
Batch 53/64 loss: -0.17012453079223633
Batch 54/64 loss: -0.1532372236251831
Batch 55/64 loss: -0.16098618507385254
Batch 56/64 loss: -0.16597694158554077
Batch 57/64 loss: -0.16608864068984985
Batch 58/64 loss: -0.1280345320701599
Batch 59/64 loss: -0.15523725748062134
Batch 60/64 loss: -0.14939647912979126
Batch 61/64 loss: -0.1740369200706482
Batch 62/64 loss: -0.14539146423339844
Batch 63/64 loss: -0.12792891263961792
Batch 64/64 loss: -0.16588950157165527
Epoch 281  Train loss: -0.16185258229573568  Val loss: -0.04464955878831267
Epoch 282
-------------------------------
Batch 1/64 loss: -0.1751970648765564
Batch 2/64 loss: -0.1630120873451233
Batch 3/64 loss: -0.16714692115783691
Batch 4/64 loss: -0.15571528673171997
Batch 5/64 loss: -0.1728106141090393
Batch 6/64 loss: -0.17567837238311768
Batch 7/64 loss: -0.1589057445526123
Batch 8/64 loss: -0.1711828112602234
Batch 9/64 loss: -0.14033466577529907
Batch 10/64 loss: -0.14082491397857666
Batch 11/64 loss: -0.16180813312530518
Batch 12/64 loss: -0.12870270013809204
Batch 13/64 loss: -0.1613767147064209
Batch 14/64 loss: -0.15191292762756348
Batch 15/64 loss: -0.1506527066230774
Batch 16/64 loss: -0.14968037605285645
Batch 17/64 loss: -0.15563762187957764
Batch 18/64 loss: -0.14211207628250122
Batch 19/64 loss: -0.17019492387771606
Batch 20/64 loss: -0.15883320569992065
Batch 21/64 loss: -0.1630493402481079
Batch 22/64 loss: -0.1596510410308838
Batch 23/64 loss: -0.15403544902801514
Batch 24/64 loss: -0.1677398681640625
Batch 25/64 loss: -0.1549537181854248
Batch 26/64 loss: -0.17051845788955688
Batch 27/64 loss: -0.15837544202804565
Batch 28/64 loss: -0.15322357416152954
Batch 29/64 loss: -0.15331768989562988
Batch 30/64 loss: -0.17581510543823242
Batch 31/64 loss: -0.15588855743408203
Batch 32/64 loss: -0.1653696894645691
Batch 33/64 loss: -0.14121150970458984
Batch 34/64 loss: -0.1590023636817932
Batch 35/64 loss: -0.15532219409942627
Batch 36/64 loss: -0.14743149280548096
Batch 37/64 loss: -0.16088485717773438
Batch 38/64 loss: -0.15647757053375244
Batch 39/64 loss: -0.15460294485092163
Batch 40/64 loss: -0.1580480933189392
Batch 41/64 loss: -0.16150355339050293
Batch 42/64 loss: -0.15078258514404297
Batch 43/64 loss: -0.14259326457977295
Batch 44/64 loss: -0.16365140676498413
Batch 45/64 loss: -0.14472877979278564
Batch 46/64 loss: -0.15180730819702148
Batch 47/64 loss: -0.15135490894317627
Batch 48/64 loss: -0.16493010520935059
Batch 49/64 loss: -0.15134108066558838
Batch 50/64 loss: -0.1667773723602295
Batch 51/64 loss: -0.13478994369506836
Batch 52/64 loss: -0.1688753366470337
Batch 53/64 loss: -0.1671091914176941
Batch 54/64 loss: -0.14984679222106934
Batch 55/64 loss: -0.15312182903289795
Batch 56/64 loss: -0.1641046404838562
Batch 57/64 loss: -0.1293402910232544
Batch 58/64 loss: -0.1553490161895752
Batch 59/64 loss: -0.1395038366317749
Batch 60/64 loss: -0.18455028533935547
Batch 61/64 loss: -0.1684412956237793
Batch 62/64 loss: -0.14955204725265503
Batch 63/64 loss: -0.1604379415512085
Batch 64/64 loss: -0.15684717893600464
Epoch 282  Train loss: -0.15700026703815834  Val loss: -0.047149103941376676
Epoch 283
-------------------------------
Batch 1/64 loss: -0.15514636039733887
Batch 2/64 loss: -0.14435923099517822
Batch 3/64 loss: -0.15972161293029785
Batch 4/64 loss: -0.14965099096298218
Batch 5/64 loss: -0.12442541122436523
Batch 6/64 loss: -0.1384761929512024
Batch 7/64 loss: -0.14719444513320923
Batch 8/64 loss: -0.15590345859527588
Batch 9/64 loss: -0.1520567536354065
Batch 10/64 loss: -0.16422778367996216
Batch 11/64 loss: -0.14231765270233154
Batch 12/64 loss: -0.1701543927192688
Batch 13/64 loss: -0.17248886823654175
Batch 14/64 loss: -0.1491917371749878
Batch 15/64 loss: -0.14344996213912964
Batch 16/64 loss: -0.14647507667541504
Batch 17/64 loss: -0.15434104204177856
Batch 18/64 loss: -0.14225733280181885
Batch 19/64 loss: -0.13908684253692627
Batch 20/64 loss: -0.16472017765045166
Batch 21/64 loss: -0.13078546524047852
Batch 22/64 loss: -0.1729297637939453
Batch 23/64 loss: -0.13492143154144287
Batch 24/64 loss: -0.14260631799697876
Batch 25/64 loss: -0.15793412923812866
Batch 26/64 loss: -0.16849321126937866
Batch 27/64 loss: -0.14941298961639404
Batch 28/64 loss: -0.12052464485168457
Batch 29/64 loss: -0.16646820306777954
Batch 30/64 loss: -0.1622941493988037
Batch 31/64 loss: -0.1600348949432373
Batch 32/64 loss: -0.16272491216659546
Batch 33/64 loss: -0.1523040533065796
Batch 34/64 loss: -0.1489439606666565
Batch 35/64 loss: -0.18076711893081665
Batch 36/64 loss: -0.14858198165893555
Batch 37/64 loss: -0.1551980972290039
Batch 38/64 loss: -0.13564389944076538
Batch 39/64 loss: -0.14068734645843506
Batch 40/64 loss: -0.17611676454544067
Batch 41/64 loss: -0.16555702686309814
Batch 42/64 loss: -0.18194937705993652
Batch 43/64 loss: -0.1569463610649109
Batch 44/64 loss: -0.16311174631118774
Batch 45/64 loss: -0.1709219217300415
Batch 46/64 loss: -0.1677420735359192
Batch 47/64 loss: -0.17220807075500488
Batch 48/64 loss: -0.15198028087615967
Batch 49/64 loss: -0.16057133674621582
Batch 50/64 loss: -0.13157618045806885
Batch 51/64 loss: -0.15780824422836304
Batch 52/64 loss: -0.15094947814941406
Batch 53/64 loss: -0.1603403091430664
Batch 54/64 loss: -0.13007307052612305
Batch 55/64 loss: -0.15205222368240356
Batch 56/64 loss: -0.17258834838867188
Batch 57/64 loss: -0.1659020185470581
Batch 58/64 loss: -0.15699678659439087
Batch 59/64 loss: -0.15073275566101074
Batch 60/64 loss: -0.17194974422454834
Batch 61/64 loss: -0.18220239877700806
Batch 62/64 loss: -0.14620482921600342
Batch 63/64 loss: -0.16404718160629272
Batch 64/64 loss: -0.15814387798309326
Epoch 283  Train loss: -0.1550751111086677  Val loss: -0.04513919517346674
Epoch 284
-------------------------------
Batch 1/64 loss: -0.16653937101364136
Batch 2/64 loss: -0.1707911491394043
Batch 3/64 loss: -0.16412746906280518
Batch 4/64 loss: -0.156715989112854
Batch 5/64 loss: -0.176935076713562
Batch 6/64 loss: -0.15586882829666138
Batch 7/64 loss: -0.17340046167373657
Batch 8/64 loss: -0.15540337562561035
Batch 9/64 loss: -0.16470521688461304
Batch 10/64 loss: -0.12753009796142578
Batch 11/64 loss: -0.16975295543670654
Batch 12/64 loss: -0.156419575214386
Batch 13/64 loss: -0.15945732593536377
Batch 14/64 loss: -0.17179733514785767
Batch 15/64 loss: -0.17863649129867554
Batch 16/64 loss: -0.16139763593673706
Batch 17/64 loss: -0.16863775253295898
Batch 18/64 loss: -0.1621120572090149
Batch 19/64 loss: -0.16154628992080688
Batch 20/64 loss: -0.16937971115112305
Batch 21/64 loss: -0.1710922122001648
Batch 22/64 loss: -0.13763034343719482
Batch 23/64 loss: -0.17258399724960327
Batch 24/64 loss: -0.15260040760040283
Batch 25/64 loss: -0.18149369955062866
Batch 26/64 loss: -0.1537453532218933
Batch 27/64 loss: -0.17769896984100342
Batch 28/64 loss: -0.1584916114807129
Batch 29/64 loss: -0.16919296979904175
Batch 30/64 loss: -0.151447594165802
Batch 31/64 loss: -0.15295743942260742
Batch 32/64 loss: -0.15766280889511108
Batch 33/64 loss: -0.16565591096878052
Batch 34/64 loss: -0.18247318267822266
Batch 35/64 loss: -0.16748327016830444
Batch 36/64 loss: -0.1666637659072876
Batch 37/64 loss: -0.13235139846801758
Batch 38/64 loss: -0.15131521224975586
Batch 39/64 loss: -0.15671288967132568
Batch 40/64 loss: -0.1718233823776245
Batch 41/64 loss: -0.16503912210464478
Batch 42/64 loss: -0.16585993766784668
Batch 43/64 loss: -0.16865509748458862
Batch 44/64 loss: -0.17089128494262695
Batch 45/64 loss: -0.15423732995986938
Batch 46/64 loss: -0.1508040428161621
Batch 47/64 loss: -0.1337476372718811
Batch 48/64 loss: -0.1450902819633484
Batch 49/64 loss: -0.1561894416809082
Batch 50/64 loss: -0.16921329498291016
Batch 51/64 loss: -0.16184592247009277
Batch 52/64 loss: -0.1552751064300537
Batch 53/64 loss: -0.15763843059539795
Batch 54/64 loss: -0.13605105876922607
Batch 55/64 loss: -0.1708543300628662
Batch 56/64 loss: -0.15086054801940918
Batch 57/64 loss: -0.15524667501449585
Batch 58/64 loss: -0.1607075333595276
Batch 59/64 loss: -0.17040091753005981
Batch 60/64 loss: -0.14139986038208008
Batch 61/64 loss: -0.16465288400650024
Batch 62/64 loss: -0.16482388973236084
Batch 63/64 loss: -0.15356212854385376
Batch 64/64 loss: -0.12642478942871094
Epoch 284  Train loss: -0.1603152303134694  Val loss: -0.04446371847001957
Epoch 285
-------------------------------
Batch 1/64 loss: -0.1578131914138794
Batch 2/64 loss: -0.17223691940307617
Batch 3/64 loss: -0.16423875093460083
Batch 4/64 loss: -0.1636582612991333
Batch 5/64 loss: -0.16673803329467773
Batch 6/64 loss: -0.16031134128570557
Batch 7/64 loss: -0.17650431394577026
Batch 8/64 loss: -0.16112035512924194
Batch 9/64 loss: -0.14934128522872925
Batch 10/64 loss: -0.1498730182647705
Batch 11/64 loss: -0.16936421394348145
Batch 12/64 loss: -0.18324953317642212
Batch 13/64 loss: -0.17960011959075928
Batch 14/64 loss: -0.15938782691955566
Batch 15/64 loss: -0.1774948239326477
Batch 16/64 loss: -0.17985397577285767
Batch 17/64 loss: -0.1691056489944458
Batch 18/64 loss: -0.16279637813568115
Batch 19/64 loss: -0.14999234676361084
Batch 20/64 loss: -0.1553868055343628
Batch 21/64 loss: -0.15446513891220093
Batch 22/64 loss: -0.18064451217651367
Batch 23/64 loss: -0.15980464220046997
Batch 24/64 loss: -0.17500275373458862
Batch 25/64 loss: -0.1615685224533081
Batch 26/64 loss: -0.14996469020843506
Batch 27/64 loss: -0.13826137781143188
Batch 28/64 loss: -0.1576911211013794
Batch 29/64 loss: -0.1456524133682251
Batch 30/64 loss: -0.15719538927078247
Batch 31/64 loss: -0.1703251600265503
Batch 32/64 loss: -0.16917568445205688
Batch 33/64 loss: -0.17951840162277222
Batch 34/64 loss: -0.1530037522315979
Batch 35/64 loss: -0.1667790412902832
Batch 36/64 loss: -0.15871816873550415
Batch 37/64 loss: -0.1522095799446106
Batch 38/64 loss: -0.15640580654144287
Batch 39/64 loss: -0.16341131925582886
Batch 40/64 loss: -0.16473716497421265
Batch 41/64 loss: -0.1498030424118042
Batch 42/64 loss: -0.1518571972846985
Batch 43/64 loss: -0.17257070541381836
Batch 44/64 loss: -0.1584395170211792
Batch 45/64 loss: -0.1584843397140503
Batch 46/64 loss: -0.14700984954833984
Batch 47/64 loss: -0.15852594375610352
Batch 48/64 loss: -0.16332435607910156
Batch 49/64 loss: -0.15386760234832764
Batch 50/64 loss: -0.1612626314163208
Batch 51/64 loss: -0.15233278274536133
Batch 52/64 loss: -0.16199004650115967
Batch 53/64 loss: -0.17397665977478027
Batch 54/64 loss: -0.17139047384262085
Batch 55/64 loss: -0.16061830520629883
Batch 56/64 loss: -0.13528168201446533
Batch 57/64 loss: -0.1281619668006897
Batch 58/64 loss: -0.15965914726257324
Batch 59/64 loss: -0.13613390922546387
Batch 60/64 loss: -0.16053634881973267
Batch 61/64 loss: -0.1291186809539795
Batch 62/64 loss: -0.14917051792144775
Batch 63/64 loss: -0.15042322874069214
Batch 64/64 loss: -0.16395115852355957
Epoch 285  Train loss: -0.1598353582270005  Val loss: -0.04291375088937504
Epoch 286
-------------------------------
Batch 1/64 loss: -0.15044069290161133
Batch 2/64 loss: -0.16693514585494995
Batch 3/64 loss: -0.18115603923797607
Batch 4/64 loss: -0.16658258438110352
Batch 5/64 loss: -0.1673891544342041
Batch 6/64 loss: -0.15063518285751343
Batch 7/64 loss: -0.18645644187927246
Batch 8/64 loss: -0.16605114936828613
Batch 9/64 loss: -0.15291756391525269
Batch 10/64 loss: -0.16170859336853027
Batch 11/64 loss: -0.16190004348754883
Batch 12/64 loss: -0.1840451955795288
Batch 13/64 loss: -0.15294033288955688
Batch 14/64 loss: -0.1696743369102478
Batch 15/64 loss: -0.16992497444152832
Batch 16/64 loss: -0.17549347877502441
Batch 17/64 loss: -0.16156494617462158
Batch 18/64 loss: -0.15647733211517334
Batch 19/64 loss: -0.1714969277381897
Batch 20/64 loss: -0.16681182384490967
Batch 21/64 loss: -0.15767496824264526
Batch 22/64 loss: -0.1527460217475891
Batch 23/64 loss: -0.17057394981384277
Batch 24/64 loss: -0.1414240002632141
Batch 25/64 loss: -0.1273532509803772
Batch 26/64 loss: -0.17251968383789062
Batch 27/64 loss: -0.16039687395095825
Batch 28/64 loss: -0.15183699131011963
Batch 29/64 loss: -0.1607266664505005
Batch 30/64 loss: -0.15310347080230713
Batch 31/64 loss: -0.16532254219055176
Batch 32/64 loss: -0.14180994033813477
Batch 33/64 loss: -0.1505049467086792
Batch 34/64 loss: -0.1570795178413391
Batch 35/64 loss: -0.1612306833267212
Batch 36/64 loss: -0.1721031665802002
Batch 37/64 loss: -0.1469864845275879
Batch 38/64 loss: -0.18017274141311646
Batch 39/64 loss: -0.1644839644432068
Batch 40/64 loss: -0.1725330352783203
Batch 41/64 loss: -0.16279011964797974
Batch 42/64 loss: -0.16467905044555664
Batch 43/64 loss: -0.1593814492225647
Batch 44/64 loss: -0.1623152494430542
Batch 45/64 loss: -0.15918272733688354
Batch 46/64 loss: -0.17984366416931152
Batch 47/64 loss: -0.13369357585906982
Batch 48/64 loss: -0.159437894821167
Batch 49/64 loss: -0.12776702642440796
Batch 50/64 loss: -0.1734740138053894
Batch 51/64 loss: -0.1476287841796875
Batch 52/64 loss: -0.16209512948989868
Batch 53/64 loss: -0.13645964860916138
Batch 54/64 loss: -0.15985089540481567
Batch 55/64 loss: -0.13630110025405884
Batch 56/64 loss: -0.15881216526031494
Batch 57/64 loss: -0.1709950566291809
Batch 58/64 loss: -0.16679006814956665
Batch 59/64 loss: -0.1365160346031189
Batch 60/64 loss: -0.15403997898101807
Batch 61/64 loss: -0.16982930898666382
Batch 62/64 loss: -0.16064763069152832
Batch 63/64 loss: -0.16285794973373413
Batch 64/64 loss: -0.17042863368988037
Epoch 286  Train loss: -0.160225801374398  Val loss: -0.04746292874575481
Epoch 287
-------------------------------
Batch 1/64 loss: -0.16256237030029297
Batch 2/64 loss: -0.16171181201934814
Batch 3/64 loss: -0.13779520988464355
Batch 4/64 loss: -0.155805766582489
Batch 5/64 loss: -0.16863501071929932
Batch 6/64 loss: -0.16334933042526245
Batch 7/64 loss: -0.16926366090774536
Batch 8/64 loss: -0.15460246801376343
Batch 9/64 loss: -0.17502892017364502
Batch 10/64 loss: -0.16939038038253784
Batch 11/64 loss: -0.17279928922653198
Batch 12/64 loss: -0.16101837158203125
Batch 13/64 loss: -0.1818886399269104
Batch 14/64 loss: -0.15082216262817383
Batch 15/64 loss: -0.15709811449050903
Batch 16/64 loss: -0.16999167203903198
Batch 17/64 loss: -0.13701659440994263
Batch 18/64 loss: -0.16786837577819824
Batch 19/64 loss: -0.14499783515930176
Batch 20/64 loss: -0.1721133589744568
Batch 21/64 loss: -0.16012060642242432
Batch 22/64 loss: -0.15735870599746704
Batch 23/64 loss: -0.1507582664489746
Batch 24/64 loss: -0.16333407163619995
Batch 25/64 loss: -0.17177462577819824
Batch 26/64 loss: -0.17365598678588867
Batch 27/64 loss: -0.1593780517578125
Batch 28/64 loss: -0.1323142647743225
Batch 29/64 loss: -0.15384435653686523
Batch 30/64 loss: -0.16805535554885864
Batch 31/64 loss: -0.1796931028366089
Batch 32/64 loss: -0.15352898836135864
Batch 33/64 loss: -0.15499049425125122
Batch 34/64 loss: -0.13740789890289307
Batch 35/64 loss: -0.18274807929992676
Batch 36/64 loss: -0.15235638618469238
Batch 37/64 loss: -0.15614700317382812
Batch 38/64 loss: -0.15393036603927612
Batch 39/64 loss: -0.16027092933654785
Batch 40/64 loss: -0.14237308502197266
Batch 41/64 loss: -0.14030086994171143
Batch 42/64 loss: -0.14945554733276367
Batch 43/64 loss: -0.15432876348495483
Batch 44/64 loss: -0.16281819343566895
Batch 45/64 loss: -0.17148631811141968
Batch 46/64 loss: -0.1732550859451294
Batch 47/64 loss: -0.1692899465560913
Batch 48/64 loss: -0.163083016872406
Batch 49/64 loss: -0.151347815990448
Batch 50/64 loss: -0.14094114303588867
Batch 51/64 loss: -0.17863398790359497
Batch 52/64 loss: -0.15523087978363037
Batch 53/64 loss: -0.14019817113876343
Batch 54/64 loss: -0.1647402048110962
Batch 55/64 loss: -0.1631772518157959
Batch 56/64 loss: -0.17107939720153809
Batch 57/64 loss: -0.15594971179962158
Batch 58/64 loss: -0.15637588500976562
Batch 59/64 loss: -0.1765841841697693
Batch 60/64 loss: -0.14213800430297852
Batch 61/64 loss: -0.15957659482955933
Batch 62/64 loss: -0.16404306888580322
Batch 63/64 loss: -0.16920024156570435
Batch 64/64 loss: -0.15699398517608643
Epoch 287  Train loss: -0.1598239959455004  Val loss: -0.045861696254756446
Epoch 288
-------------------------------
Batch 1/64 loss: -0.1673595905303955
Batch 2/64 loss: -0.17617809772491455
Batch 3/64 loss: -0.18894559144973755
Batch 4/64 loss: -0.18165314197540283
Batch 5/64 loss: -0.17032039165496826
Batch 6/64 loss: -0.16939294338226318
Batch 7/64 loss: -0.15918421745300293
Batch 8/64 loss: -0.16718459129333496
Batch 9/64 loss: -0.1753774881362915
Batch 10/64 loss: -0.1487116813659668
Batch 11/64 loss: -0.1351318359375
Batch 12/64 loss: -0.1827462911605835
Batch 13/64 loss: -0.17582935094833374
Batch 14/64 loss: -0.16787958145141602
Batch 15/64 loss: -0.16719651222229004
Batch 16/64 loss: -0.15105050802230835
Batch 17/64 loss: -0.15556257963180542
Batch 18/64 loss: -0.14951395988464355
Batch 19/64 loss: -0.16461127996444702
Batch 20/64 loss: -0.17248064279556274
Batch 21/64 loss: -0.15965837240219116
Batch 22/64 loss: -0.16266733407974243
Batch 23/64 loss: -0.15157157182693481
Batch 24/64 loss: -0.1740584373474121
Batch 25/64 loss: -0.15353024005889893
Batch 26/64 loss: -0.17365103960037231
Batch 27/64 loss: -0.1652977466583252
Batch 28/64 loss: -0.1666649580001831
Batch 29/64 loss: -0.17602097988128662
Batch 30/64 loss: -0.1587001085281372
Batch 31/64 loss: -0.17415255308151245
Batch 32/64 loss: -0.14340919256210327
Batch 33/64 loss: -0.14924359321594238
Batch 34/64 loss: -0.1634945273399353
Batch 35/64 loss: -0.1680564284324646
Batch 36/64 loss: -0.16271811723709106
Batch 37/64 loss: -0.17734819650650024
Batch 38/64 loss: -0.16197216510772705
Batch 39/64 loss: -0.160098135471344
Batch 40/64 loss: -0.15905725955963135
Batch 41/64 loss: -0.17635756731033325
Batch 42/64 loss: -0.1761687994003296
Batch 43/64 loss: -0.17188191413879395
Batch 44/64 loss: -0.15487122535705566
Batch 45/64 loss: -0.16321784257888794
Batch 46/64 loss: -0.1678411364555359
Batch 47/64 loss: -0.15369540452957153
Batch 48/64 loss: -0.16952025890350342
Batch 49/64 loss: -0.16420787572860718
Batch 50/64 loss: -0.15298795700073242
Batch 51/64 loss: -0.16331267356872559
Batch 52/64 loss: -0.16519087553024292
Batch 53/64 loss: -0.14813673496246338
Batch 54/64 loss: -0.17553824186325073
Batch 55/64 loss: -0.16829174757003784
Batch 56/64 loss: -0.16670113801956177
Batch 57/64 loss: -0.16200625896453857
Batch 58/64 loss: -0.16600418090820312
Batch 59/64 loss: -0.10427308082580566
Batch 60/64 loss: -0.14524418115615845
Batch 61/64 loss: -0.1372963786125183
Batch 62/64 loss: -0.16998553276062012
Batch 63/64 loss: -0.15878158807754517
Batch 64/64 loss: -0.1671990156173706
Epoch 288  Train loss: -0.16305244062461105  Val loss: -0.04377248504317503
Epoch 289
-------------------------------
Batch 1/64 loss: -0.14901655912399292
Batch 2/64 loss: -0.18663710355758667
Batch 3/64 loss: -0.16335046291351318
Batch 4/64 loss: -0.18291419744491577
Batch 5/64 loss: -0.1668543815612793
Batch 6/64 loss: -0.15460771322250366
Batch 7/64 loss: -0.17640268802642822
Batch 8/64 loss: -0.1679980754852295
Batch 9/64 loss: -0.18679893016815186
Batch 10/64 loss: -0.19410204887390137
Batch 11/64 loss: -0.1809207797050476
Batch 12/64 loss: -0.16541588306427002
Batch 13/64 loss: -0.1704791784286499
Batch 14/64 loss: -0.18782472610473633
Batch 15/64 loss: -0.17819398641586304
Batch 16/64 loss: -0.17404448986053467
Batch 17/64 loss: -0.17268812656402588
Batch 18/64 loss: -0.17138904333114624
Batch 19/64 loss: -0.16527551412582397
Batch 20/64 loss: -0.16673147678375244
Batch 21/64 loss: -0.13060593605041504
Batch 22/64 loss: -0.16931915283203125
Batch 23/64 loss: -0.1320483684539795
Batch 24/64 loss: -0.171728253364563
Batch 25/64 loss: -0.15707343816757202
Batch 26/64 loss: -0.16919690370559692
Batch 27/64 loss: -0.17971926927566528
Batch 28/64 loss: -0.18522268533706665
Batch 29/64 loss: -0.1635289192199707
Batch 30/64 loss: -0.17332148551940918
Batch 31/64 loss: -0.153922438621521
Batch 32/64 loss: -0.1541615128517151
Batch 33/64 loss: -0.1742425560951233
Batch 34/64 loss: -0.16283464431762695
Batch 35/64 loss: -0.18113279342651367
Batch 36/64 loss: -0.1615382432937622
Batch 37/64 loss: -0.15967607498168945
Batch 38/64 loss: -0.15765243768692017
Batch 39/64 loss: -0.159271240234375
Batch 40/64 loss: -0.1517350673675537
Batch 41/64 loss: -0.16381430625915527
Batch 42/64 loss: -0.16575950384140015
Batch 43/64 loss: -0.16977590322494507
Batch 44/64 loss: -0.17744070291519165
Batch 45/64 loss: -0.15491223335266113
Batch 46/64 loss: -0.1441062092781067
Batch 47/64 loss: -0.17813795804977417
Batch 48/64 loss: -0.18501603603363037
Batch 49/64 loss: -0.1442132592201233
Batch 50/64 loss: -0.11946296691894531
Batch 51/64 loss: -0.16244655847549438
Batch 52/64 loss: -0.16436445713043213
Batch 53/64 loss: -0.1611649990081787
Batch 54/64 loss: -0.14756006002426147
Batch 55/64 loss: -0.16172534227371216
Batch 56/64 loss: -0.14462882280349731
Batch 57/64 loss: -0.16413283348083496
Batch 58/64 loss: -0.16892659664154053
Batch 59/64 loss: -0.1798384189605713
Batch 60/64 loss: -0.17928534746170044
Batch 61/64 loss: -0.17631709575653076
Batch 62/64 loss: -0.14422667026519775
Batch 63/64 loss: -0.17233020067214966
Batch 64/64 loss: -0.15794670581817627
Epoch 289  Train loss: -0.16560977720746808  Val loss: -0.043101093818231954
Epoch 290
-------------------------------
Batch 1/64 loss: -0.1765788197517395
Batch 2/64 loss: -0.1675775647163391
Batch 3/64 loss: -0.15023791790008545
Batch 4/64 loss: -0.15824609994888306
Batch 5/64 loss: -0.1638452410697937
Batch 6/64 loss: -0.14735734462738037
Batch 7/64 loss: -0.17362558841705322
Batch 8/64 loss: -0.17021650075912476
Batch 9/64 loss: -0.17641031742095947
Batch 10/64 loss: -0.16161954402923584
Batch 11/64 loss: -0.1714499592781067
Batch 12/64 loss: -0.15175384283065796
Batch 13/64 loss: -0.17934119701385498
Batch 14/64 loss: -0.17453032732009888
Batch 15/64 loss: -0.1738905906677246
Batch 16/64 loss: -0.16380679607391357
Batch 17/64 loss: -0.17781543731689453
Batch 18/64 loss: -0.18313747644424438
Batch 19/64 loss: -0.17409056425094604
Batch 20/64 loss: -0.17834806442260742
Batch 21/64 loss: -0.15677791833877563
Batch 22/64 loss: -0.1738290786743164
Batch 23/64 loss: -0.19048243761062622
Batch 24/64 loss: -0.19152939319610596
Batch 25/64 loss: -0.1674405336380005
Batch 26/64 loss: -0.1563946008682251
Batch 27/64 loss: -0.16445696353912354
Batch 28/64 loss: -0.1579965353012085
Batch 29/64 loss: -0.1831769347190857
Batch 30/64 loss: -0.1521710753440857
Batch 31/64 loss: -0.1645418405532837
Batch 32/64 loss: -0.16375023126602173
Batch 33/64 loss: -0.18518221378326416
Batch 34/64 loss: -0.15612375736236572
Batch 35/64 loss: -0.1538563370704651
Batch 36/64 loss: -0.1520957350730896
Batch 37/64 loss: -0.1623678207397461
Batch 38/64 loss: -0.16023880243301392
Batch 39/64 loss: -0.1788250207901001
Batch 40/64 loss: -0.15614283084869385
Batch 41/64 loss: -0.1707707643508911
Batch 42/64 loss: -0.1370372772216797
Batch 43/64 loss: -0.18289506435394287
Batch 44/64 loss: -0.16347914934158325
Batch 45/64 loss: -0.14392584562301636
Batch 46/64 loss: -0.14233243465423584
Batch 47/64 loss: -0.1656017303466797
Batch 48/64 loss: -0.17588138580322266
Batch 49/64 loss: -0.1612863540649414
Batch 50/64 loss: -0.19486361742019653
Batch 51/64 loss: -0.17757093906402588
Batch 52/64 loss: -0.1736079454421997
Batch 53/64 loss: -0.17253071069717407
Batch 54/64 loss: -0.17882239818572998
Batch 55/64 loss: -0.1643158197402954
Batch 56/64 loss: -0.15193480253219604
Batch 57/64 loss: -0.10911440849304199
Batch 58/64 loss: -0.1526954174041748
Batch 59/64 loss: -0.1632450819015503
Batch 60/64 loss: -0.163271963596344
Batch 61/64 loss: -0.17445755004882812
Batch 62/64 loss: -0.16820472478866577
Batch 63/64 loss: -0.17888414859771729
Batch 64/64 loss: -0.17507785558700562
Epoch 290  Train loss: -0.1662634851885777  Val loss: -0.045027513684276044
Epoch 291
-------------------------------
Batch 1/64 loss: -0.15961956977844238
Batch 2/64 loss: -0.1594979166984558
Batch 3/64 loss: -0.16262274980545044
Batch 4/64 loss: -0.16325247287750244
Batch 5/64 loss: -0.169106125831604
Batch 6/64 loss: -0.18324065208435059
Batch 7/64 loss: -0.17963778972625732
Batch 8/64 loss: -0.1658192276954651
Batch 9/64 loss: -0.16507065296173096
Batch 10/64 loss: -0.17304956912994385
Batch 11/64 loss: -0.1517782211303711
Batch 12/64 loss: -0.15798723697662354
Batch 13/64 loss: -0.1674157977104187
Batch 14/64 loss: -0.16460633277893066
Batch 15/64 loss: -0.1644173264503479
Batch 16/64 loss: -0.16487610340118408
Batch 17/64 loss: -0.1667497158050537
Batch 18/64 loss: -0.179845929145813
Batch 19/64 loss: -0.15852010250091553
Batch 20/64 loss: -0.16229933500289917
Batch 21/64 loss: -0.141792893409729
Batch 22/64 loss: -0.17016685009002686
Batch 23/64 loss: -0.1742963194847107
Batch 24/64 loss: -0.16015195846557617
Batch 25/64 loss: -0.1587963104248047
Batch 26/64 loss: -0.1748337745666504
Batch 27/64 loss: -0.14778780937194824
Batch 28/64 loss: -0.14299112558364868
Batch 29/64 loss: -0.16963160037994385
Batch 30/64 loss: -0.16676700115203857
Batch 31/64 loss: -0.16026568412780762
Batch 32/64 loss: -0.16220688819885254
Batch 33/64 loss: -0.19142907857894897
Batch 34/64 loss: -0.17122960090637207
Batch 35/64 loss: -0.16167962551116943
Batch 36/64 loss: -0.1624966859817505
Batch 37/64 loss: -0.17523670196533203
Batch 38/64 loss: -0.15850085020065308
Batch 39/64 loss: -0.1742154359817505
Batch 40/64 loss: -0.16873681545257568
Batch 41/64 loss: -0.1659458875656128
Batch 42/64 loss: -0.13930082321166992
Batch 43/64 loss: -0.16934335231781006
Batch 44/64 loss: -0.15405762195587158
Batch 45/64 loss: -0.13777220249176025
Batch 46/64 loss: -0.16709595918655396
Batch 47/64 loss: -0.17184334993362427
Batch 48/64 loss: -0.15597951412200928
Batch 49/64 loss: -0.15981727838516235
Batch 50/64 loss: -0.16028869152069092
Batch 51/64 loss: -0.1580348014831543
Batch 52/64 loss: -0.13805431127548218
Batch 53/64 loss: -0.1547958254814148
Batch 54/64 loss: -0.15969181060791016
Batch 55/64 loss: -0.1392371654510498
Batch 56/64 loss: -0.15432798862457275
Batch 57/64 loss: -0.17787086963653564
Batch 58/64 loss: -0.17629849910736084
Batch 59/64 loss: -0.1848163604736328
Batch 60/64 loss: -0.1763594150543213
Batch 61/64 loss: -0.16886276006698608
Batch 62/64 loss: -0.1640993356704712
Batch 63/64 loss: -0.17467302083969116
Batch 64/64 loss: -0.17404627799987793
Epoch 291  Train loss: -0.1639486649457146  Val loss: -0.046214197099823315
Epoch 292
-------------------------------
Batch 1/64 loss: -0.1502300500869751
Batch 2/64 loss: -0.1504535675048828
Batch 3/64 loss: -0.1700735092163086
Batch 4/64 loss: -0.18609297275543213
Batch 5/64 loss: -0.17326325178146362
Batch 6/64 loss: -0.16964936256408691
Batch 7/64 loss: -0.1696993112564087
Batch 8/64 loss: -0.18394625186920166
Batch 9/64 loss: -0.15679287910461426
Batch 10/64 loss: -0.1679544448852539
Batch 11/64 loss: -0.1621241569519043
Batch 12/64 loss: -0.13909590244293213
Batch 13/64 loss: -0.18265318870544434
Batch 14/64 loss: -0.1809353232383728
Batch 15/64 loss: -0.16994363069534302
Batch 16/64 loss: -0.16558009386062622
Batch 17/64 loss: -0.12254774570465088
Batch 18/64 loss: -0.1862965226173401
Batch 19/64 loss: -0.17461276054382324
Batch 20/64 loss: -0.17068099975585938
Batch 21/64 loss: -0.17817777395248413
Batch 22/64 loss: -0.16081762313842773
Batch 23/64 loss: -0.16080749034881592
Batch 24/64 loss: -0.16715121269226074
Batch 25/64 loss: -0.14480024576187134
Batch 26/64 loss: -0.15059077739715576
Batch 27/64 loss: -0.16986513137817383
Batch 28/64 loss: -0.15914618968963623
Batch 29/64 loss: -0.1522747278213501
Batch 30/64 loss: -0.15991872549057007
Batch 31/64 loss: -0.16305840015411377
Batch 32/64 loss: -0.1701069474220276
Batch 33/64 loss: -0.17701447010040283
Batch 34/64 loss: -0.1527732014656067
Batch 35/64 loss: -0.16077381372451782
Batch 36/64 loss: -0.16364151239395142
Batch 37/64 loss: -0.17057693004608154
Batch 38/64 loss: -0.1581820845603943
Batch 39/64 loss: -0.1611003279685974
Batch 40/64 loss: -0.16979271173477173
Batch 41/64 loss: -0.14913147687911987
Batch 42/64 loss: -0.15948486328125
Batch 43/64 loss: -0.17324841022491455
Batch 44/64 loss: -0.17240607738494873
Batch 45/64 loss: -0.16180139780044556
Batch 46/64 loss: -0.16655057668685913
Batch 47/64 loss: -0.14170151948928833
Batch 48/64 loss: -0.17122548818588257
Batch 49/64 loss: -0.139956533908844
Batch 50/64 loss: -0.1634739637374878
Batch 51/64 loss: -0.16471987962722778
Batch 52/64 loss: -0.1786431074142456
Batch 53/64 loss: -0.16596853733062744
Batch 54/64 loss: -0.15823012590408325
Batch 55/64 loss: -0.1656818389892578
Batch 56/64 loss: -0.1588854193687439
Batch 57/64 loss: -0.18383657932281494
Batch 58/64 loss: -0.12468129396438599
Batch 59/64 loss: -0.17377954721450806
Batch 60/64 loss: -0.18319326639175415
Batch 61/64 loss: -0.13022440671920776
Batch 62/64 loss: -0.16075563430786133
Batch 63/64 loss: -0.17171615362167358
Batch 64/64 loss: -0.11895102262496948
Epoch 292  Train loss: -0.16300714647068698  Val loss: -0.043597166480886976
Epoch 293
-------------------------------
Batch 1/64 loss: -0.17392832040786743
Batch 2/64 loss: -0.1640501618385315
Batch 3/64 loss: -0.18684005737304688
Batch 4/64 loss: -0.18103504180908203
Batch 5/64 loss: -0.16420388221740723
Batch 6/64 loss: -0.18209612369537354
Batch 7/64 loss: -0.1670514941215515
Batch 8/64 loss: -0.1496504545211792
Batch 9/64 loss: -0.16356009244918823
Batch 10/64 loss: -0.15562570095062256
Batch 11/64 loss: -0.17445510625839233
Batch 12/64 loss: -0.16996556520462036
Batch 13/64 loss: -0.15698790550231934
Batch 14/64 loss: -0.13864660263061523
Batch 15/64 loss: -0.1597772240638733
Batch 16/64 loss: -0.16332662105560303
Batch 17/64 loss: -0.17065930366516113
Batch 18/64 loss: -0.18494659662246704
Batch 19/64 loss: -0.16969996690750122
Batch 20/64 loss: -0.15656626224517822
Batch 21/64 loss: -0.16783839464187622
Batch 22/64 loss: -0.16793477535247803
Batch 23/64 loss: -0.1700761318206787
Batch 24/64 loss: -0.16344404220581055
Batch 25/64 loss: -0.17527413368225098
Batch 26/64 loss: -0.18594717979431152
Batch 27/64 loss: -0.17043083906173706
Batch 28/64 loss: -0.169538676738739
Batch 29/64 loss: -0.17802965641021729
Batch 30/64 loss: -0.17749881744384766
Batch 31/64 loss: -0.1812545657157898
Batch 32/64 loss: -0.17130649089813232
Batch 33/64 loss: -0.14357447624206543
Batch 34/64 loss: -0.1427597999572754
Batch 35/64 loss: -0.16775721311569214
Batch 36/64 loss: -0.15077507495880127
Batch 37/64 loss: -0.1679985523223877
Batch 38/64 loss: -0.17094957828521729
Batch 39/64 loss: -0.1555708646774292
Batch 40/64 loss: -0.18638986349105835
Batch 41/64 loss: -0.1446893811225891
Batch 42/64 loss: -0.15922695398330688
Batch 43/64 loss: -0.18177413940429688
Batch 44/64 loss: -0.1550978422164917
Batch 45/64 loss: -0.1679198145866394
Batch 46/64 loss: -0.13915789127349854
Batch 47/64 loss: -0.1653362512588501
Batch 48/64 loss: -0.16815614700317383
Batch 49/64 loss: -0.15884095430374146
Batch 50/64 loss: -0.1802312135696411
Batch 51/64 loss: -0.13278931379318237
Batch 52/64 loss: -0.17404907941818237
Batch 53/64 loss: -0.18344849348068237
Batch 54/64 loss: -0.12345212697982788
Batch 55/64 loss: -0.15635180473327637
Batch 56/64 loss: -0.16468244791030884
Batch 57/64 loss: -0.15919291973114014
Batch 58/64 loss: -0.1808817982673645
Batch 59/64 loss: -0.1654071807861328
Batch 60/64 loss: -0.17181497812271118
Batch 61/64 loss: -0.17071908712387085
Batch 62/64 loss: -0.17086738348007202
Batch 63/64 loss: -0.157401442527771
Batch 64/64 loss: -0.18790137767791748
Epoch 293  Train loss: -0.16580135354808734  Val loss: -0.046117880500059355
Epoch 294
-------------------------------
Batch 1/64 loss: -0.1725788116455078
Batch 2/64 loss: -0.16455000638961792
Batch 3/64 loss: -0.13959407806396484
Batch 4/64 loss: -0.1666043996810913
Batch 5/64 loss: -0.1526947021484375
Batch 6/64 loss: -0.17032212018966675
Batch 7/64 loss: -0.15851807594299316
Batch 8/64 loss: -0.1857052445411682
Batch 9/64 loss: -0.1833254098892212
Batch 10/64 loss: -0.15866559743881226
Batch 11/64 loss: -0.1582164764404297
Batch 12/64 loss: -0.16869688034057617
Batch 13/64 loss: -0.17327231168746948
Batch 14/64 loss: -0.16690874099731445
Batch 15/64 loss: -0.16481149196624756
Batch 16/64 loss: -0.18049240112304688
Batch 17/64 loss: -0.18132352828979492
Batch 18/64 loss: -0.14624619483947754
Batch 19/64 loss: -0.17449957132339478
Batch 20/64 loss: -0.1859467625617981
Batch 21/64 loss: -0.16945791244506836
Batch 22/64 loss: -0.16724920272827148
Batch 23/64 loss: -0.16943228244781494
Batch 24/64 loss: -0.16890931129455566
Batch 25/64 loss: -0.17163318395614624
Batch 26/64 loss: -0.16609323024749756
Batch 27/64 loss: -0.16466158628463745
Batch 28/64 loss: -0.18067395687103271
Batch 29/64 loss: -0.14130932092666626
Batch 30/64 loss: -0.17371058464050293
Batch 31/64 loss: -0.1635759472846985
Batch 32/64 loss: -0.16186881065368652
Batch 33/64 loss: -0.16980421543121338
Batch 34/64 loss: -0.16254210472106934
Batch 35/64 loss: -0.16675633192062378
Batch 36/64 loss: -0.1881580948829651
Batch 37/64 loss: -0.17629051208496094
Batch 38/64 loss: -0.18263167142868042
Batch 39/64 loss: -0.17674648761749268
Batch 40/64 loss: -0.17109990119934082
Batch 41/64 loss: -0.17614436149597168
Batch 42/64 loss: -0.15774118900299072
Batch 43/64 loss: -0.166082501411438
Batch 44/64 loss: -0.1573607325553894
Batch 45/64 loss: -0.16477125883102417
Batch 46/64 loss: -0.1647496223449707
Batch 47/64 loss: -0.1671910285949707
Batch 48/64 loss: -0.1592468023300171
Batch 49/64 loss: -0.13541054725646973
Batch 50/64 loss: -0.16557413339614868
Batch 51/64 loss: -0.17257392406463623
Batch 52/64 loss: -0.1903194785118103
Batch 53/64 loss: -0.15269362926483154
Batch 54/64 loss: -0.16890263557434082
Batch 55/64 loss: -0.18042486906051636
Batch 56/64 loss: -0.17437368631362915
Batch 57/64 loss: -0.1708201766014099
Batch 58/64 loss: -0.17356300354003906
Batch 59/64 loss: -0.16389846801757812
Batch 60/64 loss: -0.15197932720184326
Batch 61/64 loss: -0.17450910806655884
Batch 62/64 loss: -0.17589735984802246
Batch 63/64 loss: -0.13533252477645874
Batch 64/64 loss: -0.16918444633483887
Epoch 294  Train loss: -0.16740433188045725  Val loss: -0.04257137095395642
Epoch 295
-------------------------------
Batch 1/64 loss: -0.17387819290161133
Batch 2/64 loss: -0.16763484477996826
Batch 3/64 loss: -0.18553835153579712
Batch 4/64 loss: -0.1823185682296753
Batch 5/64 loss: -0.18625271320343018
Batch 6/64 loss: -0.1699906587600708
Batch 7/64 loss: -0.19146931171417236
Batch 8/64 loss: -0.18119311332702637
Batch 9/64 loss: -0.1867523193359375
Batch 10/64 loss: -0.1846488118171692
Batch 11/64 loss: -0.18387287855148315
Batch 12/64 loss: -0.14487290382385254
Batch 13/64 loss: -0.15638506412506104
Batch 14/64 loss: -0.18041539192199707
Batch 15/64 loss: -0.15955042839050293
Batch 16/64 loss: -0.14923834800720215
Batch 17/64 loss: -0.16484755277633667
Batch 18/64 loss: -0.13492876291275024
Batch 19/64 loss: -0.16288024187088013
Batch 20/64 loss: -0.17484253644943237
Batch 21/64 loss: -0.15377962589263916
Batch 22/64 loss: -0.17631471157073975
Batch 23/64 loss: -0.18120944499969482
Batch 24/64 loss: -0.15608179569244385
Batch 25/64 loss: -0.1477961540222168
Batch 26/64 loss: -0.17014604806900024
Batch 27/64 loss: -0.1796463131904602
Batch 28/64 loss: -0.12633591890335083
Batch 29/64 loss: -0.15549838542938232
Batch 30/64 loss: -0.16543012857437134
Batch 31/64 loss: -0.16633039712905884
Batch 32/64 loss: -0.1733909249305725
Batch 33/64 loss: -0.17636585235595703
Batch 34/64 loss: -0.18067896366119385
Batch 35/64 loss: -0.16024601459503174
Batch 36/64 loss: -0.19935041666030884
Batch 37/64 loss: -0.1430051326751709
Batch 38/64 loss: -0.171830952167511
Batch 39/64 loss: -0.17565959692001343
Batch 40/64 loss: -0.16638916730880737
Batch 41/64 loss: -0.15799152851104736
Batch 42/64 loss: -0.18351662158966064
Batch 43/64 loss: -0.1586437225341797
Batch 44/64 loss: -0.15194100141525269
Batch 45/64 loss: -0.14086556434631348
Batch 46/64 loss: -0.15087759494781494
Batch 47/64 loss: -0.17265033721923828
Batch 48/64 loss: -0.140541672706604
Batch 49/64 loss: -0.1708657145500183
Batch 50/64 loss: -0.149505615234375
Batch 51/64 loss: -0.14884114265441895
Batch 52/64 loss: -0.1412978172302246
Batch 53/64 loss: -0.16574788093566895
Batch 54/64 loss: -0.1727880835533142
Batch 55/64 loss: -0.17322486639022827
Batch 56/64 loss: -0.17743706703186035
Batch 57/64 loss: -0.1602034568786621
Batch 58/64 loss: -0.16409426927566528
Batch 59/64 loss: -0.16190403699874878
Batch 60/64 loss: -0.1340770721435547
Batch 61/64 loss: -0.1648939847946167
Batch 62/64 loss: -0.18150144815444946
Batch 63/64 loss: -0.15962636470794678
Batch 64/64 loss: -0.18314862251281738
Epoch 295  Train loss: -0.1657630630567962  Val loss: -0.04331939818523184
Epoch 296
-------------------------------
Batch 1/64 loss: -0.1458187699317932
Batch 2/64 loss: -0.16788500547409058
Batch 3/64 loss: -0.16938912868499756
Batch 4/64 loss: -0.18694061040878296
Batch 5/64 loss: -0.164958655834198
Batch 6/64 loss: -0.18271636962890625
Batch 7/64 loss: -0.16952472925186157
Batch 8/64 loss: -0.16564130783081055
Batch 9/64 loss: -0.16805583238601685
Batch 10/64 loss: -0.19113856554031372
Batch 11/64 loss: -0.17123043537139893
Batch 12/64 loss: -0.19285517930984497
Batch 13/64 loss: -0.16226410865783691
Batch 14/64 loss: -0.16573095321655273
Batch 15/64 loss: -0.1739615797996521
Batch 16/64 loss: -0.16404306888580322
Batch 17/64 loss: -0.16553330421447754
Batch 18/64 loss: -0.16249805688858032
Batch 19/64 loss: -0.15265917778015137
Batch 20/64 loss: -0.17799806594848633
Batch 21/64 loss: -0.16021031141281128
Batch 22/64 loss: -0.1946866512298584
Batch 23/64 loss: -0.16126424074172974
Batch 24/64 loss: -0.17308640480041504
Batch 25/64 loss: -0.16587084531784058
Batch 26/64 loss: -0.17053931951522827
Batch 27/64 loss: -0.1982201337814331
Batch 28/64 loss: -0.14293217658996582
Batch 29/64 loss: -0.14203906059265137
Batch 30/64 loss: -0.16125935316085815
Batch 31/64 loss: -0.17814767360687256
Batch 32/64 loss: -0.17722350358963013
Batch 33/64 loss: -0.1849231719970703
Batch 34/64 loss: -0.17743778228759766
Batch 35/64 loss: -0.1450875997543335
Batch 36/64 loss: -0.1641978621482849
Batch 37/64 loss: -0.15889877080917358
Batch 38/64 loss: -0.17811626195907593
Batch 39/64 loss: -0.1577223539352417
Batch 40/64 loss: -0.17453885078430176
Batch 41/64 loss: -0.1562860608100891
Batch 42/64 loss: -0.1536913514137268
Batch 43/64 loss: -0.16981583833694458
Batch 44/64 loss: -0.16183841228485107
Batch 45/64 loss: -0.17981374263763428
Batch 46/64 loss: -0.17840856313705444
Batch 47/64 loss: -0.17212331295013428
Batch 48/64 loss: -0.17987358570098877
Batch 49/64 loss: -0.18277597427368164
Batch 50/64 loss: -0.17626571655273438
Batch 51/64 loss: -0.16094934940338135
Batch 52/64 loss: -0.16892671585083008
Batch 53/64 loss: -0.17286527156829834
Batch 54/64 loss: -0.16618245840072632
Batch 55/64 loss: -0.15983796119689941
Batch 56/64 loss: -0.17732644081115723
Batch 57/64 loss: -0.17465001344680786
Batch 58/64 loss: -0.18001842498779297
Batch 59/64 loss: -0.1905311942100525
Batch 60/64 loss: -0.16773223876953125
Batch 61/64 loss: -0.17581939697265625
Batch 62/64 loss: -0.17923009395599365
Batch 63/64 loss: -0.18150949478149414
Batch 64/64 loss: -0.18104755878448486
Epoch 296  Train loss: -0.17050200022903142  Val loss: -0.044933471073399706
Epoch 297
-------------------------------
Batch 1/64 loss: -0.14775413274765015
Batch 2/64 loss: -0.16633498668670654
Batch 3/64 loss: -0.18128043413162231
Batch 4/64 loss: -0.1669192910194397
Batch 5/64 loss: -0.177354097366333
Batch 6/64 loss: -0.16395485401153564
Batch 7/64 loss: -0.17292499542236328
Batch 8/64 loss: -0.1636289358139038
Batch 9/64 loss: -0.16911828517913818
Batch 10/64 loss: -0.18136882781982422
Batch 11/64 loss: -0.19950151443481445
Batch 12/64 loss: -0.19278860092163086
Batch 13/64 loss: -0.18141651153564453
Batch 14/64 loss: -0.1909767985343933
Batch 15/64 loss: -0.1744707226753235
Batch 16/64 loss: -0.17664921283721924
Batch 17/64 loss: -0.19124829769134521
Batch 18/64 loss: -0.16674602031707764
Batch 19/64 loss: -0.19404548406600952
Batch 20/64 loss: -0.15692436695098877
Batch 21/64 loss: -0.1605094075202942
Batch 22/64 loss: -0.15484589338302612
Batch 23/64 loss: -0.17935121059417725
Batch 24/64 loss: -0.17525607347488403
Batch 25/64 loss: -0.17427003383636475
Batch 26/64 loss: -0.166118323802948
Batch 27/64 loss: -0.17674624919891357
Batch 28/64 loss: -0.15750843286514282
Batch 29/64 loss: -0.1582912802696228
Batch 30/64 loss: -0.1844933032989502
Batch 31/64 loss: -0.16475576162338257
Batch 32/64 loss: -0.1751110553741455
Batch 33/64 loss: -0.15618950128555298
Batch 34/64 loss: -0.16447186470031738
Batch 35/64 loss: -0.17294859886169434
Batch 36/64 loss: -0.15639901161193848
Batch 37/64 loss: -0.16410595178604126
Batch 38/64 loss: -0.16309314966201782
Batch 39/64 loss: -0.1680496335029602
Batch 40/64 loss: -0.15541082620620728
Batch 41/64 loss: -0.1410045623779297
Batch 42/64 loss: -0.1538216471672058
Batch 43/64 loss: -0.1783270239830017
Batch 44/64 loss: -0.1680932641029358
Batch 45/64 loss: -0.187872052192688
Batch 46/64 loss: -0.15170300006866455
Batch 47/64 loss: -0.162689208984375
Batch 48/64 loss: -0.17293322086334229
Batch 49/64 loss: -0.1576979160308838
Batch 50/64 loss: -0.1728452444076538
Batch 51/64 loss: -0.14349722862243652
Batch 52/64 loss: -0.17844688892364502
Batch 53/64 loss: -0.17550945281982422
Batch 54/64 loss: -0.16787219047546387
Batch 55/64 loss: -0.16909950971603394
Batch 56/64 loss: -0.17057031393051147
Batch 57/64 loss: -0.1586545705795288
Batch 58/64 loss: -0.14243364334106445
Batch 59/64 loss: -0.17038923501968384
Batch 60/64 loss: -0.18639612197875977
Batch 61/64 loss: -0.13318300247192383
Batch 62/64 loss: -0.14199519157409668
Batch 63/64 loss: -0.16288775205612183
Batch 64/64 loss: -0.15072137117385864
Epoch 297  Train loss: -0.16791051298964257  Val loss: -0.04331405875609093
Epoch 298
-------------------------------
Batch 1/64 loss: -0.15131443738937378
Batch 2/64 loss: -0.14988303184509277
Batch 3/64 loss: -0.16046535968780518
Batch 4/64 loss: -0.15197789669036865
Batch 5/64 loss: -0.16265273094177246
Batch 6/64 loss: -0.16828656196594238
Batch 7/64 loss: -0.16941261291503906
Batch 8/64 loss: -0.15789878368377686
Batch 9/64 loss: -0.15367627143859863
Batch 10/64 loss: -0.15794020891189575
Batch 11/64 loss: -0.15604424476623535
Batch 12/64 loss: -0.16232305765151978
Batch 13/64 loss: -0.14268475770950317
Batch 14/64 loss: -0.17616379261016846
Batch 15/64 loss: -0.15899038314819336
Batch 16/64 loss: -0.18725049495697021
Batch 17/64 loss: -0.1484227180480957
Batch 18/64 loss: -0.15103226900100708
Batch 19/64 loss: -0.16137003898620605
Batch 20/64 loss: -0.17602407932281494
Batch 21/64 loss: -0.17145776748657227
Batch 22/64 loss: -0.17946964502334595
Batch 23/64 loss: -0.18917560577392578
Batch 24/64 loss: -0.16876453161239624
Batch 25/64 loss: -0.19560867547988892
Batch 26/64 loss: -0.17430484294891357
Batch 27/64 loss: -0.16331350803375244
Batch 28/64 loss: -0.17872262001037598
Batch 29/64 loss: -0.1786339282989502
Batch 30/64 loss: -0.16374057531356812
Batch 31/64 loss: -0.17480111122131348
Batch 32/64 loss: -0.17094212770462036
Batch 33/64 loss: -0.1562938690185547
Batch 34/64 loss: -0.17018187046051025
Batch 35/64 loss: -0.15775102376937866
Batch 36/64 loss: -0.17872726917266846
Batch 37/64 loss: -0.159540057182312
Batch 38/64 loss: -0.18190228939056396
Batch 39/64 loss: -0.1676650047302246
Batch 40/64 loss: -0.16973251104354858
Batch 41/64 loss: -0.15911322832107544
Batch 42/64 loss: -0.15707379579544067
Batch 43/64 loss: -0.1535935401916504
Batch 44/64 loss: -0.14369922876358032
Batch 45/64 loss: -0.15758097171783447
Batch 46/64 loss: -0.17806553840637207
Batch 47/64 loss: -0.1613796353340149
Batch 48/64 loss: -0.1718159317970276
Batch 49/64 loss: -0.16656935214996338
Batch 50/64 loss: -0.14493554830551147
Batch 51/64 loss: -0.18924862146377563
Batch 52/64 loss: -0.181532621383667
Batch 53/64 loss: -0.16705870628356934
Batch 54/64 loss: -0.16754603385925293
Batch 55/64 loss: -0.17325162887573242
Batch 56/64 loss: -0.14586961269378662
Batch 57/64 loss: -0.17737138271331787
Batch 58/64 loss: -0.15245211124420166
Batch 59/64 loss: -0.16328328847885132
Batch 60/64 loss: -0.1609032154083252
Batch 61/64 loss: -0.16121608018875122
Batch 62/64 loss: -0.17137694358825684
Batch 63/64 loss: -0.1778845191001892
Batch 64/64 loss: -0.1457464098930359
Epoch 298  Train loss: -0.16543802209928923  Val loss: -0.0473954829153736
Epoch 299
-------------------------------
Batch 1/64 loss: -0.14576119184494019
Batch 2/64 loss: -0.16548264026641846
Batch 3/64 loss: -0.18021178245544434
Batch 4/64 loss: -0.1667225956916809
Batch 5/64 loss: -0.17261743545532227
Batch 6/64 loss: -0.19177991151809692
Batch 7/64 loss: -0.1629772186279297
Batch 8/64 loss: -0.19214671850204468
Batch 9/64 loss: -0.18456017971038818
Batch 10/64 loss: -0.16858506202697754
Batch 11/64 loss: -0.17123615741729736
Batch 12/64 loss: -0.1696271300315857
Batch 13/64 loss: -0.17046856880187988
Batch 14/64 loss: -0.17177611589431763
Batch 15/64 loss: -0.18478363752365112
Batch 16/64 loss: -0.17259007692337036
Batch 17/64 loss: -0.17461293935775757
Batch 18/64 loss: -0.17657917737960815
Batch 19/64 loss: -0.166756272315979
Batch 20/64 loss: -0.17297691106796265
Batch 21/64 loss: -0.17348253726959229
Batch 22/64 loss: -0.1821414828300476
Batch 23/64 loss: -0.15851956605911255
Batch 24/64 loss: -0.17611974477767944
Batch 25/64 loss: -0.179767906665802
Batch 26/64 loss: -0.17450344562530518
Batch 27/64 loss: -0.1664767861366272
Batch 28/64 loss: -0.1711006760597229
Batch 29/64 loss: -0.17187535762786865
Batch 30/64 loss: -0.18429547548294067
Batch 31/64 loss: -0.14454954862594604
Batch 32/64 loss: -0.16846883296966553
Batch 33/64 loss: -0.15757954120635986
Batch 34/64 loss: -0.150002121925354
Batch 35/64 loss: -0.1430627703666687
Batch 36/64 loss: -0.17308157682418823
Batch 37/64 loss: -0.16675233840942383
Batch 38/64 loss: -0.17309707403182983
Batch 39/64 loss: -0.15884363651275635
Batch 40/64 loss: -0.15497547388076782
Batch 41/64 loss: -0.15799593925476074
Batch 42/64 loss: -0.16247308254241943
Batch 43/64 loss: -0.14117896556854248
Batch 44/64 loss: -0.14329344034194946
Batch 45/64 loss: -0.17710822820663452
Batch 46/64 loss: -0.18092000484466553
Batch 47/64 loss: -0.1754215955734253
Batch 48/64 loss: -0.17244410514831543
Batch 49/64 loss: -0.1805688738822937
Batch 50/64 loss: -0.16517746448516846
Batch 51/64 loss: -0.16055762767791748
Batch 52/64 loss: -0.16312432289123535
Batch 53/64 loss: -0.16231364011764526
Batch 54/64 loss: -0.16718357801437378
Batch 55/64 loss: -0.15262764692306519
Batch 56/64 loss: -0.16977638006210327
Batch 57/64 loss: -0.15157771110534668
Batch 58/64 loss: -0.17728817462921143
Batch 59/64 loss: -0.14349806308746338
Batch 60/64 loss: -0.17105603218078613
Batch 61/64 loss: -0.141595721244812
Batch 62/64 loss: -0.1716974973678589
Batch 63/64 loss: -0.17678743600845337
Batch 64/64 loss: -0.17093396186828613
Epoch 299  Train loss: -0.16760491950839174  Val loss: -0.042677206272112134
Epoch 300
-------------------------------
Batch 1/64 loss: -0.17578887939453125
Batch 2/64 loss: -0.17196869850158691
Batch 3/64 loss: -0.16073888540267944
Batch 4/64 loss: -0.19263839721679688
Batch 5/64 loss: -0.1810876727104187
Batch 6/64 loss: -0.18624168634414673
Batch 7/64 loss: -0.12953191995620728
Batch 8/64 loss: -0.17979437112808228
Batch 9/64 loss: -0.18195068836212158
Batch 10/64 loss: -0.1883544921875
Batch 11/64 loss: -0.19321131706237793
Batch 12/64 loss: -0.1841447353363037
Batch 13/64 loss: -0.15103048086166382
Batch 14/64 loss: -0.19370073080062866
Batch 15/64 loss: -0.18260091543197632
Batch 16/64 loss: -0.17549431324005127
Batch 17/64 loss: -0.16246086359024048
Batch 18/64 loss: -0.1792733073234558
Batch 19/64 loss: -0.1719769835472107
Batch 20/64 loss: -0.1760120987892151
Batch 21/64 loss: -0.18353724479675293
Batch 22/64 loss: -0.17150676250457764
Batch 23/64 loss: -0.16099631786346436
Batch 24/64 loss: -0.1600794792175293
Batch 25/64 loss: -0.18700671195983887
Batch 26/64 loss: -0.18239814043045044
Batch 27/64 loss: -0.15075862407684326
Batch 28/64 loss: -0.1670689582824707
Batch 29/64 loss: -0.17978894710540771
Batch 30/64 loss: -0.17219167947769165
Batch 31/64 loss: -0.156039297580719
Batch 32/64 loss: -0.16625630855560303
Batch 33/64 loss: -0.1890602707862854
Batch 34/64 loss: -0.15828299522399902
Batch 35/64 loss: -0.1659555435180664
Batch 36/64 loss: -0.16184324026107788
Batch 37/64 loss: -0.1635054349899292
Batch 38/64 loss: -0.16653650999069214
Batch 39/64 loss: -0.18392199277877808
Batch 40/64 loss: -0.1216362714767456
Batch 41/64 loss: -0.1700459122657776
Batch 42/64 loss: -0.1707841157913208
Batch 43/64 loss: -0.16708004474639893
Batch 44/64 loss: -0.14342689514160156
Batch 45/64 loss: -0.16663676500320435
Batch 46/64 loss: -0.15908920764923096
Batch 47/64 loss: -0.16423773765563965
Batch 48/64 loss: -0.16606009006500244
Batch 49/64 loss: -0.18740993738174438
Batch 50/64 loss: -0.1734951138496399
Batch 51/64 loss: -0.17444837093353271
Batch 52/64 loss: -0.1631941795349121
Batch 53/64 loss: -0.1795361042022705
Batch 54/64 loss: -0.14888083934783936
Batch 55/64 loss: -0.1814274787902832
Batch 56/64 loss: -0.16730785369873047
Batch 57/64 loss: -0.17328870296478271
Batch 58/64 loss: -0.16111987829208374
Batch 59/64 loss: -0.17420309782028198
Batch 60/64 loss: -0.18100547790527344
Batch 61/64 loss: -0.1780293583869934
Batch 62/64 loss: -0.16702622175216675
Batch 63/64 loss: -0.17511409521102905
Batch 64/64 loss: -0.1481037735939026
Epoch 300  Train loss: -0.17051447022194957  Val loss: -0.044045756567794435
Epoch 301
-------------------------------
Batch 1/64 loss: -0.18402695655822754
Batch 2/64 loss: -0.15919935703277588
Batch 3/64 loss: -0.17498540878295898
Batch 4/64 loss: -0.17000162601470947
Batch 5/64 loss: -0.1665528416633606
Batch 6/64 loss: -0.1337960958480835
Batch 7/64 loss: -0.16832709312438965
Batch 8/64 loss: -0.16324424743652344
Batch 9/64 loss: -0.19407296180725098
Batch 10/64 loss: -0.18720555305480957
Batch 11/64 loss: -0.1714654564857483
Batch 12/64 loss: -0.18161720037460327
Batch 13/64 loss: -0.17626041173934937
Batch 14/64 loss: -0.18721044063568115
Batch 15/64 loss: -0.17928344011306763
Batch 16/64 loss: -0.16524457931518555
Batch 17/64 loss: -0.19313287734985352
Batch 18/64 loss: -0.15122437477111816
Batch 19/64 loss: -0.18383818864822388
Batch 20/64 loss: -0.17377960681915283
Batch 21/64 loss: -0.1512315273284912
Batch 22/64 loss: -0.16497313976287842
Batch 23/64 loss: -0.16054826974868774
Batch 24/64 loss: -0.18040204048156738
Batch 25/64 loss: -0.17178863286972046
Batch 26/64 loss: -0.13320136070251465
Batch 27/64 loss: -0.15265750885009766
Batch 28/64 loss: -0.16864824295043945
Batch 29/64 loss: -0.19811862707138062
Batch 30/64 loss: -0.15704721212387085
Batch 31/64 loss: -0.1732271909713745
Batch 32/64 loss: -0.1765318512916565
Batch 33/64 loss: -0.15229594707489014
Batch 34/64 loss: -0.1716785430908203
Batch 35/64 loss: -0.17717885971069336
Batch 36/64 loss: -0.17008614540100098
Batch 37/64 loss: -0.18822860717773438
Batch 38/64 loss: -0.1819794774055481
Batch 39/64 loss: -0.18230456113815308
Batch 40/64 loss: -0.168196439743042
Batch 41/64 loss: -0.18681740760803223
Batch 42/64 loss: -0.17130929231643677
Batch 43/64 loss: -0.15094900131225586
Batch 44/64 loss: -0.17200630903244019
Batch 45/64 loss: -0.15806317329406738
Batch 46/64 loss: -0.18199443817138672
Batch 47/64 loss: -0.17554324865341187
Batch 48/64 loss: -0.1698702573776245
Batch 49/64 loss: -0.16037774085998535
Batch 50/64 loss: -0.14682114124298096
Batch 51/64 loss: -0.17469316720962524
Batch 52/64 loss: -0.17312932014465332
Batch 53/64 loss: -0.14688998460769653
Batch 54/64 loss: -0.18824458122253418
Batch 55/64 loss: -0.15985965728759766
Batch 56/64 loss: -0.17683982849121094
Batch 57/64 loss: -0.16821151971817017
Batch 58/64 loss: -0.16583871841430664
Batch 59/64 loss: -0.1910862922668457
Batch 60/64 loss: -0.16828924417495728
Batch 61/64 loss: -0.16273033618927002
Batch 62/64 loss: -0.15303534269332886
Batch 63/64 loss: -0.1764376163482666
Batch 64/64 loss: -0.16499900817871094
Epoch 301  Train loss: -0.17015811415279614  Val loss: -0.042876357475097236
Epoch 302
-------------------------------
Batch 1/64 loss: -0.1635117530822754
Batch 2/64 loss: -0.19070249795913696
Batch 3/64 loss: -0.17820072174072266
Batch 4/64 loss: -0.18310219049453735
Batch 5/64 loss: -0.17903244495391846
Batch 6/64 loss: -0.1644647717475891
Batch 7/64 loss: -0.1675536036491394
Batch 8/64 loss: -0.15895509719848633
Batch 9/64 loss: -0.17956292629241943
Batch 10/64 loss: -0.17194676399230957
Batch 11/64 loss: -0.18780982494354248
Batch 12/64 loss: -0.17431586980819702
Batch 13/64 loss: -0.17306971549987793
Batch 14/64 loss: -0.1751495599746704
Batch 15/64 loss: -0.17917251586914062
Batch 16/64 loss: -0.17900443077087402
Batch 17/64 loss: -0.19846504926681519
Batch 18/64 loss: -0.15777325630187988
Batch 19/64 loss: -0.16992878913879395
Batch 20/64 loss: -0.16200965642929077
Batch 21/64 loss: -0.1973057985305786
Batch 22/64 loss: -0.17101162672042847
Batch 23/64 loss: -0.18392682075500488
Batch 24/64 loss: -0.16290271282196045
Batch 25/64 loss: -0.16434770822525024
Batch 26/64 loss: -0.16836369037628174
Batch 27/64 loss: -0.17711442708969116
Batch 28/64 loss: -0.16915202140808105
Batch 29/64 loss: -0.1400301456451416
Batch 30/64 loss: -0.14659541845321655
Batch 31/64 loss: -0.14862865209579468
Batch 32/64 loss: -0.18696844577789307
Batch 33/64 loss: -0.15618735551834106
Batch 34/64 loss: -0.15310323238372803
Batch 35/64 loss: -0.15885233879089355
Batch 36/64 loss: -0.1788579225540161
Batch 37/64 loss: -0.17628806829452515
Batch 38/64 loss: -0.140658438205719
Batch 39/64 loss: -0.17354148626327515
Batch 40/64 loss: -0.14692610502243042
Batch 41/64 loss: -0.17835015058517456
Batch 42/64 loss: -0.1742590069770813
Batch 43/64 loss: -0.17577117681503296
Batch 44/64 loss: -0.178572416305542
Batch 45/64 loss: -0.157484233379364
Batch 46/64 loss: -0.15543478727340698
Batch 47/64 loss: -0.17896270751953125
Batch 48/64 loss: -0.17992174625396729
Batch 49/64 loss: -0.17202627658843994
Batch 50/64 loss: -0.163191020488739
Batch 51/64 loss: -0.17173349857330322
Batch 52/64 loss: -0.17255401611328125
Batch 53/64 loss: -0.16713666915893555
Batch 54/64 loss: -0.18240922689437866
Batch 55/64 loss: -0.154754638671875
Batch 56/64 loss: -0.18456214666366577
Batch 57/64 loss: -0.15635311603546143
Batch 58/64 loss: -0.17519432306289673
Batch 59/64 loss: -0.18022072315216064
Batch 60/64 loss: -0.16729998588562012
Batch 61/64 loss: -0.16973203420639038
Batch 62/64 loss: -0.1919649839401245
Batch 63/64 loss: -0.19175904989242554
Batch 64/64 loss: -0.18432366847991943
Epoch 302  Train loss: -0.17117465944851146  Val loss: -0.04784408590637941
Epoch 303
-------------------------------
Batch 1/64 loss: -0.18978506326675415
Batch 2/64 loss: -0.18062424659729004
Batch 3/64 loss: -0.18588244915008545
Batch 4/64 loss: -0.16697263717651367
Batch 5/64 loss: -0.1747044324874878
Batch 6/64 loss: -0.18594789505004883
Batch 7/64 loss: -0.1674373745918274
Batch 8/64 loss: -0.1677226424217224
Batch 9/64 loss: -0.1341627836227417
Batch 10/64 loss: -0.1656845211982727
Batch 11/64 loss: -0.15077102184295654
Batch 12/64 loss: -0.16694605350494385
Batch 13/64 loss: -0.17706286907196045
Batch 14/64 loss: -0.18393635749816895
Batch 15/64 loss: -0.18658846616744995
Batch 16/64 loss: -0.17393642663955688
Batch 17/64 loss: -0.16522586345672607
Batch 18/64 loss: -0.15866047143936157
Batch 19/64 loss: -0.1687203049659729
Batch 20/64 loss: -0.18321871757507324
Batch 21/64 loss: -0.17942488193511963
Batch 22/64 loss: -0.1621362566947937
Batch 23/64 loss: -0.16461384296417236
Batch 24/64 loss: -0.17102980613708496
Batch 25/64 loss: -0.17842084169387817
Batch 26/64 loss: -0.15269017219543457
Batch 27/64 loss: -0.18887758255004883
Batch 28/64 loss: -0.17772752046585083
Batch 29/64 loss: -0.18111169338226318
Batch 30/64 loss: -0.19794023036956787
Batch 31/64 loss: -0.18520665168762207
Batch 32/64 loss: -0.1704416275024414
Batch 33/64 loss: -0.19125771522521973
Batch 34/64 loss: -0.17999398708343506
Batch 35/64 loss: -0.17385578155517578
Batch 36/64 loss: -0.1689472198486328
Batch 37/64 loss: -0.1821799874305725
Batch 38/64 loss: -0.14558011293411255
Batch 39/64 loss: -0.15291941165924072
Batch 40/64 loss: -0.1631121039390564
Batch 41/64 loss: -0.19778084754943848
Batch 42/64 loss: -0.17379486560821533
Batch 43/64 loss: -0.18462800979614258
Batch 44/64 loss: -0.17111736536026
Batch 45/64 loss: -0.18187814950942993
Batch 46/64 loss: -0.17583715915679932
Batch 47/64 loss: -0.1716260313987732
Batch 48/64 loss: -0.17761725187301636
Batch 49/64 loss: -0.17976152896881104
Batch 50/64 loss: -0.16860944032669067
Batch 51/64 loss: -0.16200894117355347
Batch 52/64 loss: -0.17777353525161743
Batch 53/64 loss: -0.17971587181091309
Batch 54/64 loss: -0.16305840015411377
Batch 55/64 loss: -0.16563737392425537
Batch 56/64 loss: -0.17836827039718628
Batch 57/64 loss: -0.18146860599517822
Batch 58/64 loss: -0.18246585130691528
Batch 59/64 loss: -0.13961797952651978
Batch 60/64 loss: -0.1317901611328125
Batch 61/64 loss: -0.16688323020935059
Batch 62/64 loss: -0.16264283657073975
Batch 63/64 loss: -0.16330748796463013
Batch 64/64 loss: -0.17747890949249268
Epoch 303  Train loss: -0.17210912003236659  Val loss: -0.045185726532821394
Epoch 304
-------------------------------
Batch 1/64 loss: -0.15820097923278809
Batch 2/64 loss: -0.15495693683624268
Batch 3/64 loss: -0.15824353694915771
Batch 4/64 loss: -0.1697770357131958
Batch 5/64 loss: -0.17802345752716064
Batch 6/64 loss: -0.15914183855056763
Batch 7/64 loss: -0.16822034120559692
Batch 8/64 loss: -0.1922445297241211
Batch 9/64 loss: -0.16258525848388672
Batch 10/64 loss: -0.17377817630767822
Batch 11/64 loss: -0.17242228984832764
Batch 12/64 loss: -0.17303681373596191
Batch 13/64 loss: -0.15096396207809448
Batch 14/64 loss: -0.1871720552444458
Batch 15/64 loss: -0.1845167875289917
Batch 16/64 loss: -0.17467880249023438
Batch 17/64 loss: -0.16316640377044678
Batch 18/64 loss: -0.1564469337463379
Batch 19/64 loss: -0.154940664768219
Batch 20/64 loss: -0.1611911654472351
Batch 21/64 loss: -0.15462136268615723
Batch 22/64 loss: -0.184181809425354
Batch 23/64 loss: -0.1605433225631714
Batch 24/64 loss: -0.15715593099594116
Batch 25/64 loss: -0.15747356414794922
Batch 26/64 loss: -0.17057913541793823
Batch 27/64 loss: -0.1775762438774109
Batch 28/64 loss: -0.17469549179077148
Batch 29/64 loss: -0.14685118198394775
Batch 30/64 loss: -0.1681414246559143
Batch 31/64 loss: -0.17134588956832886
Batch 32/64 loss: -0.17742276191711426
Batch 33/64 loss: -0.15898412466049194
Batch 34/64 loss: -0.16568529605865479
Batch 35/64 loss: -0.1341937780380249
Batch 36/64 loss: -0.15426653623580933
Batch 37/64 loss: -0.16647326946258545
Batch 38/64 loss: -0.1773543357849121
Batch 39/64 loss: -0.19303244352340698
Batch 40/64 loss: -0.15276122093200684
Batch 41/64 loss: -0.18390071392059326
Batch 42/64 loss: -0.19124823808670044
Batch 43/64 loss: -0.18567180633544922
Batch 44/64 loss: -0.1872878074645996
Batch 45/64 loss: -0.1526106595993042
Batch 46/64 loss: -0.17519891262054443
Batch 47/64 loss: -0.18008899688720703
Batch 48/64 loss: -0.15533798933029175
Batch 49/64 loss: -0.16047698259353638
Batch 50/64 loss: -0.17706680297851562
Batch 51/64 loss: -0.12022817134857178
Batch 52/64 loss: -0.18070393800735474
Batch 53/64 loss: -0.17854905128479004
Batch 54/64 loss: -0.1739283800125122
Batch 55/64 loss: -0.16772276163101196
Batch 56/64 loss: -0.16415905952453613
Batch 57/64 loss: -0.1997225284576416
Batch 58/64 loss: -0.20128250122070312
Batch 59/64 loss: -0.18439602851867676
Batch 60/64 loss: -0.16254276037216187
Batch 61/64 loss: -0.18584662675857544
Batch 62/64 loss: -0.19195502996444702
Batch 63/64 loss: -0.1677767038345337
Batch 64/64 loss: -0.15769749879837036
Epoch 304  Train loss: -0.16945917910220576  Val loss: -0.04425819018452438
Epoch 305
-------------------------------
Batch 1/64 loss: -0.15445303916931152
Batch 2/64 loss: -0.18446695804595947
Batch 3/64 loss: -0.16283130645751953
Batch 4/64 loss: -0.18260186910629272
Batch 5/64 loss: -0.1869828701019287
Batch 6/64 loss: -0.19847148656845093
Batch 7/64 loss: -0.18358933925628662
Batch 8/64 loss: -0.17329692840576172
Batch 9/64 loss: -0.15842247009277344
Batch 10/64 loss: -0.16712921857833862
Batch 11/64 loss: -0.17455601692199707
Batch 12/64 loss: -0.18220311403274536
Batch 13/64 loss: -0.19427883625030518
Batch 14/64 loss: -0.16117370128631592
Batch 15/64 loss: -0.1779721975326538
Batch 16/64 loss: -0.1690666675567627
Batch 17/64 loss: -0.1814577579498291
Batch 18/64 loss: -0.17541074752807617
Batch 19/64 loss: -0.20186352729797363
Batch 20/64 loss: -0.1945154070854187
Batch 21/64 loss: -0.16183853149414062
Batch 22/64 loss: -0.14829915761947632
Batch 23/64 loss: -0.15652483701705933
Batch 24/64 loss: -0.12859636545181274
Batch 25/64 loss: -0.16538894176483154
Batch 26/64 loss: -0.19189059734344482
Batch 27/64 loss: -0.17531776428222656
Batch 28/64 loss: -0.17553460597991943
Batch 29/64 loss: -0.17079854011535645
Batch 30/64 loss: -0.17189252376556396
Batch 31/64 loss: -0.17022490501403809
Batch 32/64 loss: -0.16873300075531006
Batch 33/64 loss: -0.18057364225387573
Batch 34/64 loss: -0.18570542335510254
Batch 35/64 loss: -0.17752093076705933
Batch 36/64 loss: -0.17467409372329712
Batch 37/64 loss: -0.1746828556060791
Batch 38/64 loss: -0.15789151191711426
Batch 39/64 loss: -0.15850317478179932
Batch 40/64 loss: -0.1585226058959961
Batch 41/64 loss: -0.16831892728805542
Batch 42/64 loss: -0.16924351453781128
Batch 43/64 loss: -0.15639227628707886
Batch 44/64 loss: -0.16257143020629883
Batch 45/64 loss: -0.18566203117370605
Batch 46/64 loss: -0.184928297996521
Batch 47/64 loss: -0.17589229345321655
Batch 48/64 loss: -0.15726590156555176
Batch 49/64 loss: -0.17480742931365967
Batch 50/64 loss: -0.15098083019256592
Batch 51/64 loss: -0.173142671585083
Batch 52/64 loss: -0.17722946405410767
Batch 53/64 loss: -0.1766425371170044
Batch 54/64 loss: -0.16291582584381104
Batch 55/64 loss: -0.1760256290435791
Batch 56/64 loss: -0.16064941883087158
Batch 57/64 loss: -0.1454944610595703
Batch 58/64 loss: -0.18104761838912964
Batch 59/64 loss: -0.16160744428634644
Batch 60/64 loss: -0.18136084079742432
Batch 61/64 loss: -0.14085876941680908
Batch 62/64 loss: -0.1650487184524536
Batch 63/64 loss: -0.14521098136901855
Batch 64/64 loss: -0.11601221561431885
Epoch 305  Train loss: -0.17001041477801754  Val loss: -0.04419813266734487
Epoch 306
-------------------------------
Batch 1/64 loss: -0.18602979183197021
Batch 2/64 loss: -0.1750398874282837
Batch 3/64 loss: -0.17588204145431519
Batch 4/64 loss: -0.17398083209991455
Batch 5/64 loss: -0.18634939193725586
Batch 6/64 loss: -0.17741596698760986
Batch 7/64 loss: -0.17862099409103394
Batch 8/64 loss: -0.16315221786499023
Batch 9/64 loss: -0.17766016721725464
Batch 10/64 loss: -0.17904293537139893
Batch 11/64 loss: -0.17319554090499878
Batch 12/64 loss: -0.18185871839523315
Batch 13/64 loss: -0.1585526466369629
Batch 14/64 loss: -0.1419951319694519
Batch 15/64 loss: -0.1807258129119873
Batch 16/64 loss: -0.1748034954071045
Batch 17/64 loss: -0.15308141708374023
Batch 18/64 loss: -0.16424643993377686
Batch 19/64 loss: -0.12438845634460449
Batch 20/64 loss: -0.17283064126968384
Batch 21/64 loss: -0.16818809509277344
Batch 22/64 loss: -0.15365660190582275
Batch 23/64 loss: -0.18587255477905273
Batch 24/64 loss: -0.18428385257720947
Batch 25/64 loss: -0.16795146465301514
Batch 26/64 loss: -0.18493008613586426
Batch 27/64 loss: -0.18924105167388916
Batch 28/64 loss: -0.1555497646331787
Batch 29/64 loss: -0.1688137650489807
Batch 30/64 loss: -0.17736893892288208
Batch 31/64 loss: -0.17460346221923828
Batch 32/64 loss: -0.17107504606246948
Batch 33/64 loss: -0.1677219271659851
Batch 34/64 loss: -0.14584755897521973
Batch 35/64 loss: -0.1709514856338501
Batch 36/64 loss: -0.18194067478179932
Batch 37/64 loss: -0.1592540144920349
Batch 38/64 loss: -0.1795406937599182
Batch 39/64 loss: -0.1749591827392578
Batch 40/64 loss: -0.17417699098587036
Batch 41/64 loss: -0.18864315748214722
Batch 42/64 loss: -0.15591537952423096
Batch 43/64 loss: -0.18581324815750122
Batch 44/64 loss: -0.180658757686615
Batch 45/64 loss: -0.17947810888290405
Batch 46/64 loss: -0.19275379180908203
Batch 47/64 loss: -0.18909227848052979
Batch 48/64 loss: -0.17983084917068481
Batch 49/64 loss: -0.1876227855682373
Batch 50/64 loss: -0.19261634349822998
Batch 51/64 loss: -0.1695171594619751
Batch 52/64 loss: -0.18187344074249268
Batch 53/64 loss: -0.18532991409301758
Batch 54/64 loss: -0.17067021131515503
Batch 55/64 loss: -0.1674397587776184
Batch 56/64 loss: -0.18826419115066528
Batch 57/64 loss: -0.15813219547271729
Batch 58/64 loss: -0.14935630559921265
Batch 59/64 loss: -0.17670774459838867
Batch 60/64 loss: -0.1649637222290039
Batch 61/64 loss: -0.16207361221313477
Batch 62/64 loss: -0.18049561977386475
Batch 63/64 loss: -0.18237030506134033
Batch 64/64 loss: -0.17098915576934814
Epoch 306  Train loss: -0.17306102724636302  Val loss: -0.043762150294182636
Epoch 307
-------------------------------
Batch 1/64 loss: -0.17230206727981567
Batch 2/64 loss: -0.1716928482055664
Batch 3/64 loss: -0.15659284591674805
Batch 4/64 loss: -0.19708043336868286
Batch 5/64 loss: -0.18446260690689087
Batch 6/64 loss: -0.14483779668807983
Batch 7/64 loss: -0.1932891607284546
Batch 8/64 loss: -0.18080437183380127
Batch 9/64 loss: -0.18729299306869507
Batch 10/64 loss: -0.17737436294555664
Batch 11/64 loss: -0.19989913702011108
Batch 12/64 loss: -0.19055414199829102
Batch 13/64 loss: -0.17039638757705688
Batch 14/64 loss: -0.16028934717178345
Batch 15/64 loss: -0.16717040538787842
Batch 16/64 loss: -0.14785784482955933
Batch 17/64 loss: -0.1775537133216858
Batch 18/64 loss: -0.19619125127792358
Batch 19/64 loss: -0.16603630781173706
Batch 20/64 loss: -0.18461042642593384
Batch 21/64 loss: -0.16846877336502075
Batch 22/64 loss: -0.19641989469528198
Batch 23/64 loss: -0.1652722954750061
Batch 24/64 loss: -0.1825406551361084
Batch 25/64 loss: -0.18232756853103638
Batch 26/64 loss: -0.18877041339874268
Batch 27/64 loss: -0.1436622142791748
Batch 28/64 loss: -0.17994701862335205
Batch 29/64 loss: -0.16036772727966309
Batch 30/64 loss: -0.15433472394943237
Batch 31/64 loss: -0.174049973487854
Batch 32/64 loss: -0.17424553632736206
Batch 33/64 loss: -0.18301713466644287
Batch 34/64 loss: -0.1772596836090088
Batch 35/64 loss: -0.1720825433731079
Batch 36/64 loss: -0.17433607578277588
Batch 37/64 loss: -0.1841740608215332
Batch 38/64 loss: -0.16465258598327637
Batch 39/64 loss: -0.18482595682144165
Batch 40/64 loss: -0.1595098376274109
Batch 41/64 loss: -0.16262394189834595
Batch 42/64 loss: -0.16385972499847412
Batch 43/64 loss: -0.16955184936523438
Batch 44/64 loss: -0.16772407293319702
Batch 45/64 loss: -0.17088603973388672
Batch 46/64 loss: -0.17916810512542725
Batch 47/64 loss: -0.17822951078414917
Batch 48/64 loss: -0.17134225368499756
Batch 49/64 loss: -0.19144171476364136
Batch 50/64 loss: -0.1699150800704956
Batch 51/64 loss: -0.16927659511566162
Batch 52/64 loss: -0.16708683967590332
Batch 53/64 loss: -0.1845598816871643
Batch 54/64 loss: -0.17201828956604004
Batch 55/64 loss: -0.17357337474822998
Batch 56/64 loss: -0.186606764793396
Batch 57/64 loss: -0.17976534366607666
Batch 58/64 loss: -0.16907399892807007
Batch 59/64 loss: -0.16006320714950562
Batch 60/64 loss: -0.15755748748779297
Batch 61/64 loss: -0.16060560941696167
Batch 62/64 loss: -0.16255539655685425
Batch 63/64 loss: -0.18009155988693237
Batch 64/64 loss: -0.1661689281463623
Epoch 307  Train loss: -0.17362755233166263  Val loss: -0.04331775219579743
Epoch 308
-------------------------------
Batch 1/64 loss: -0.18479228019714355
Batch 2/64 loss: -0.17929381132125854
Batch 3/64 loss: -0.16094708442687988
Batch 4/64 loss: -0.17176830768585205
Batch 5/64 loss: -0.18680787086486816
Batch 6/64 loss: -0.19125449657440186
Batch 7/64 loss: -0.19817018508911133
Batch 8/64 loss: -0.19384562969207764
Batch 9/64 loss: -0.19066160917282104
Batch 10/64 loss: -0.17778658866882324
Batch 11/64 loss: -0.1789473295211792
Batch 12/64 loss: -0.17330265045166016
Batch 13/64 loss: -0.18767303228378296
Batch 14/64 loss: -0.1701958179473877
Batch 15/64 loss: -0.18355035781860352
Batch 16/64 loss: -0.16486597061157227
Batch 17/64 loss: -0.19713830947875977
Batch 18/64 loss: -0.15564656257629395
Batch 19/64 loss: -0.16089361906051636
Batch 20/64 loss: -0.15482503175735474
Batch 21/64 loss: -0.1869795322418213
Batch 22/64 loss: -0.17721104621887207
Batch 23/64 loss: -0.18543791770935059
Batch 24/64 loss: -0.17268872261047363
Batch 25/64 loss: -0.16653448343276978
Batch 26/64 loss: -0.16145926713943481
Batch 27/64 loss: -0.17491787672042847
Batch 28/64 loss: -0.18416911363601685
Batch 29/64 loss: -0.17429476976394653
Batch 30/64 loss: -0.17190933227539062
Batch 31/64 loss: -0.1762760877609253
Batch 32/64 loss: -0.1884508728981018
Batch 33/64 loss: -0.15464669466018677
Batch 34/64 loss: -0.14943039417266846
Batch 35/64 loss: -0.14924514293670654
Batch 36/64 loss: -0.17683660984039307
Batch 37/64 loss: -0.16847461462020874
Batch 38/64 loss: -0.19613325595855713
Batch 39/64 loss: -0.16211849451065063
Batch 40/64 loss: -0.17353570461273193
Batch 41/64 loss: -0.17412394285202026
Batch 42/64 loss: -0.1621076464653015
Batch 43/64 loss: -0.17478245496749878
Batch 44/64 loss: -0.16297316551208496
Batch 45/64 loss: -0.17931532859802246
Batch 46/64 loss: -0.19029802083969116
Batch 47/64 loss: -0.15682721138000488
Batch 48/64 loss: -0.12336122989654541
Batch 49/64 loss: -0.17965686321258545
Batch 50/64 loss: -0.15883642435073853
Batch 51/64 loss: -0.15262764692306519
Batch 52/64 loss: -0.1637170910835266
Batch 53/64 loss: -0.161310076713562
Batch 54/64 loss: -0.1780269742012024
Batch 55/64 loss: -0.17277240753173828
Batch 56/64 loss: -0.18024885654449463
Batch 57/64 loss: -0.17829567193984985
Batch 58/64 loss: -0.17156529426574707
Batch 59/64 loss: -0.16520118713378906
Batch 60/64 loss: -0.1568867564201355
Batch 61/64 loss: -0.16332590579986572
Batch 62/64 loss: -0.1807030439376831
Batch 63/64 loss: -0.1698271632194519
Batch 64/64 loss: -0.17672526836395264
Epoch 308  Train loss: -0.17258746343500475  Val loss: -0.04024496746227094
Epoch 309
-------------------------------
Batch 1/64 loss: -0.18987059593200684
Batch 2/64 loss: -0.17054635286331177
Batch 3/64 loss: -0.19464737176895142
Batch 4/64 loss: -0.17521077394485474
Batch 5/64 loss: -0.19565218687057495
Batch 6/64 loss: -0.18726295232772827
Batch 7/64 loss: -0.1896570920944214
Batch 8/64 loss: -0.15458691120147705
Batch 9/64 loss: -0.19350725412368774
Batch 10/64 loss: -0.17999953031539917
Batch 11/64 loss: -0.1578352451324463
Batch 12/64 loss: -0.20454251766204834
Batch 13/64 loss: -0.18203842639923096
Batch 14/64 loss: -0.15165817737579346
Batch 15/64 loss: -0.15716350078582764
Batch 16/64 loss: -0.1869962215423584
Batch 17/64 loss: -0.17267853021621704
Batch 18/64 loss: -0.17283296585083008
Batch 19/64 loss: -0.18657827377319336
Batch 20/64 loss: -0.17666363716125488
Batch 21/64 loss: -0.18449437618255615
Batch 22/64 loss: -0.1514473557472229
Batch 23/64 loss: -0.18139445781707764
Batch 24/64 loss: -0.19893133640289307
Batch 25/64 loss: -0.17569702863693237
Batch 26/64 loss: -0.1704643964767456
Batch 27/64 loss: -0.1635648012161255
Batch 28/64 loss: -0.17480909824371338
Batch 29/64 loss: -0.164361834526062
Batch 30/64 loss: -0.16692310571670532
Batch 31/64 loss: -0.17816954851150513
Batch 32/64 loss: -0.17825424671173096
Batch 33/64 loss: -0.18021094799041748
Batch 34/64 loss: -0.1808587908744812
Batch 35/64 loss: -0.17972898483276367
Batch 36/64 loss: -0.18490362167358398
Batch 37/64 loss: -0.18270498514175415
Batch 38/64 loss: -0.16218936443328857
Batch 39/64 loss: -0.1958409547805786
Batch 40/64 loss: -0.19763725996017456
Batch 41/64 loss: -0.17863839864730835
Batch 42/64 loss: -0.17138421535491943
Batch 43/64 loss: -0.17721807956695557
Batch 44/64 loss: -0.18470442295074463
Batch 45/64 loss: -0.1674429178237915
Batch 46/64 loss: -0.18276453018188477
Batch 47/64 loss: -0.15333837270736694
Batch 48/64 loss: -0.18405991792678833
Batch 49/64 loss: -0.18941789865493774
Batch 50/64 loss: -0.20168030261993408
Batch 51/64 loss: -0.18387442827224731
Batch 52/64 loss: -0.17840665578842163
Batch 53/64 loss: -0.14774936437606812
Batch 54/64 loss: -0.16862857341766357
Batch 55/64 loss: -0.16492998600006104
Batch 56/64 loss: -0.1882568597793579
Batch 57/64 loss: -0.1578388810157776
Batch 58/64 loss: -0.15767842531204224
Batch 59/64 loss: -0.1791486144065857
Batch 60/64 loss: -0.17352700233459473
Batch 61/64 loss: -0.1777622103691101
Batch 62/64 loss: -0.1795923113822937
Batch 63/64 loss: -0.17217892408370972
Batch 64/64 loss: -0.17805147171020508
Epoch 309  Train loss: -0.1770396064309513  Val loss: -0.043457830075136164
Epoch 310
-------------------------------
Batch 1/64 loss: -0.1824294924736023
Batch 2/64 loss: -0.18433845043182373
Batch 3/64 loss: -0.1442394256591797
Batch 4/64 loss: -0.1810665726661682
Batch 5/64 loss: -0.17378783226013184
Batch 6/64 loss: -0.19281721115112305
Batch 7/64 loss: -0.1820847988128662
Batch 8/64 loss: -0.18476426601409912
Batch 9/64 loss: -0.196910560131073
Batch 10/64 loss: -0.1817915439605713
Batch 11/64 loss: -0.18706858158111572
Batch 12/64 loss: -0.16108381748199463
Batch 13/64 loss: -0.17178821563720703
Batch 14/64 loss: -0.1976701021194458
Batch 15/64 loss: -0.16462302207946777
Batch 16/64 loss: -0.18859410285949707
Batch 17/64 loss: -0.17187541723251343
Batch 18/64 loss: -0.15993797779083252
Batch 19/64 loss: -0.18405234813690186
Batch 20/64 loss: -0.17904585599899292
Batch 21/64 loss: -0.15891075134277344
Batch 22/64 loss: -0.16734951734542847
Batch 23/64 loss: -0.1872003674507141
Batch 24/64 loss: -0.18061447143554688
Batch 25/64 loss: -0.19828110933303833
Batch 26/64 loss: -0.184750497341156
Batch 27/64 loss: -0.18702739477157593
Batch 28/64 loss: -0.18320411443710327
Batch 29/64 loss: -0.1477717161178589
Batch 30/64 loss: -0.16767144203186035
Batch 31/64 loss: -0.1727544069290161
Batch 32/64 loss: -0.19164597988128662
Batch 33/64 loss: -0.18445616960525513
Batch 34/64 loss: -0.16785037517547607
Batch 35/64 loss: -0.1797858476638794
Batch 36/64 loss: -0.1903807520866394
Batch 37/64 loss: -0.171209454536438
Batch 38/64 loss: -0.1699318289756775
Batch 39/64 loss: -0.1750720739364624
Batch 40/64 loss: -0.18967658281326294
Batch 41/64 loss: -0.1750556230545044
Batch 42/64 loss: -0.18836671113967896
Batch 43/64 loss: -0.14010179042816162
Batch 44/64 loss: -0.1783435344696045
Batch 45/64 loss: -0.19264531135559082
Batch 46/64 loss: -0.15696752071380615
Batch 47/64 loss: -0.17771893739700317
Batch 48/64 loss: -0.16000455617904663
Batch 49/64 loss: -0.16707903146743774
Batch 50/64 loss: -0.17357194423675537
Batch 51/64 loss: -0.18871307373046875
Batch 52/64 loss: -0.15952658653259277
Batch 53/64 loss: -0.14529234170913696
Batch 54/64 loss: -0.1713208556175232
Batch 55/64 loss: -0.16727197170257568
Batch 56/64 loss: -0.12953639030456543
Batch 57/64 loss: -0.17610329389572144
Batch 58/64 loss: -0.18940168619155884
Batch 59/64 loss: -0.1594451665878296
Batch 60/64 loss: -0.17322194576263428
Batch 61/64 loss: -0.19977599382400513
Batch 62/64 loss: -0.1773194670677185
Batch 63/64 loss: -0.18180251121520996
Batch 64/64 loss: -0.18369287252426147
Epoch 310  Train loss: -0.17555875053592757  Val loss: -0.040894598485677926
Epoch 311
-------------------------------
Batch 1/64 loss: -0.17865681648254395
Batch 2/64 loss: -0.1804453730583191
Batch 3/64 loss: -0.15682381391525269
Batch 4/64 loss: -0.18561768531799316
Batch 5/64 loss: -0.16947758197784424
Batch 6/64 loss: -0.17693501710891724
Batch 7/64 loss: -0.17080384492874146
Batch 8/64 loss: -0.14233016967773438
Batch 9/64 loss: -0.14120924472808838
Batch 10/64 loss: -0.18178576231002808
Batch 11/64 loss: -0.166071355342865
Batch 12/64 loss: -0.17153048515319824
Batch 13/64 loss: -0.1837787628173828
Batch 14/64 loss: -0.17887252569198608
Batch 15/64 loss: -0.18155187368392944
Batch 16/64 loss: -0.17725324630737305
Batch 17/64 loss: -0.15674829483032227
Batch 18/64 loss: -0.1634153127670288
Batch 19/64 loss: -0.15820389986038208
Batch 20/64 loss: -0.14024066925048828
Batch 21/64 loss: -0.1808122992515564
Batch 22/64 loss: -0.1686244010925293
Batch 23/64 loss: -0.17503178119659424
Batch 24/64 loss: -0.17492884397506714
Batch 25/64 loss: -0.18356835842132568
Batch 26/64 loss: -0.15763413906097412
Batch 27/64 loss: -0.18377995491027832
Batch 28/64 loss: -0.18317782878875732
Batch 29/64 loss: -0.18240010738372803
Batch 30/64 loss: -0.17076188325881958
Batch 31/64 loss: -0.18895792961120605
Batch 32/64 loss: -0.14413756132125854
Batch 33/64 loss: -0.18745386600494385
Batch 34/64 loss: -0.17969584465026855
Batch 35/64 loss: -0.18543219566345215
Batch 36/64 loss: -0.17054706811904907
Batch 37/64 loss: -0.2074500322341919
Batch 38/64 loss: -0.1731141209602356
Batch 39/64 loss: -0.16963869333267212
Batch 40/64 loss: -0.1816583275794983
Batch 41/64 loss: -0.17598974704742432
Batch 42/64 loss: -0.18740582466125488
Batch 43/64 loss: -0.16885089874267578
Batch 44/64 loss: -0.17887544631958008
Batch 45/64 loss: -0.19139492511749268
Batch 46/64 loss: -0.18659836053848267
Batch 47/64 loss: -0.15275192260742188
Batch 48/64 loss: -0.19439876079559326
Batch 49/64 loss: -0.19667363166809082
Batch 50/64 loss: -0.1555219292640686
Batch 51/64 loss: -0.17567801475524902
Batch 52/64 loss: -0.16936182975769043
Batch 53/64 loss: -0.1654648780822754
Batch 54/64 loss: -0.1960621476173401
Batch 55/64 loss: -0.1502666473388672
Batch 56/64 loss: -0.1722407341003418
Batch 57/64 loss: -0.14654922485351562
Batch 58/64 loss: -0.16370588541030884
Batch 59/64 loss: -0.16442745923995972
Batch 60/64 loss: -0.16178739070892334
Batch 61/64 loss: -0.17596477270126343
Batch 62/64 loss: -0.18324720859527588
Batch 63/64 loss: -0.15601181983947754
Batch 64/64 loss: -0.14194732904434204
Epoch 311  Train loss: -0.17233328515408086  Val loss: -0.044057548660592936
Epoch 312
-------------------------------
Batch 1/64 loss: -0.1833033561706543
Batch 2/64 loss: -0.19552075862884521
Batch 3/64 loss: -0.18421852588653564
Batch 4/64 loss: -0.19854652881622314
Batch 5/64 loss: -0.17092639207839966
Batch 6/64 loss: -0.17377930879592896
Batch 7/64 loss: -0.18890678882598877
Batch 8/64 loss: -0.1927308440208435
Batch 9/64 loss: -0.15921205282211304
Batch 10/64 loss: -0.18133842945098877
Batch 11/64 loss: -0.19463032484054565
Batch 12/64 loss: -0.17167448997497559
Batch 13/64 loss: -0.18813151121139526
Batch 14/64 loss: -0.18946552276611328
Batch 15/64 loss: -0.18598490953445435
Batch 16/64 loss: -0.18317413330078125
Batch 17/64 loss: -0.17057746648788452
Batch 18/64 loss: -0.17506343126296997
Batch 19/64 loss: -0.1791468858718872
Batch 20/64 loss: -0.1951751708984375
Batch 21/64 loss: -0.1828894019126892
Batch 22/64 loss: -0.1863073706626892
Batch 23/64 loss: -0.1703917384147644
Batch 24/64 loss: -0.18327248096466064
Batch 25/64 loss: -0.17853689193725586
Batch 26/64 loss: -0.1868891716003418
Batch 27/64 loss: -0.17456907033920288
Batch 28/64 loss: -0.1435425877571106
Batch 29/64 loss: -0.18914473056793213
Batch 30/64 loss: -0.1619729995727539
Batch 31/64 loss: -0.18976885080337524
Batch 32/64 loss: -0.17972779273986816
Batch 33/64 loss: -0.17964458465576172
Batch 34/64 loss: -0.18459933996200562
Batch 35/64 loss: -0.1983168125152588
Batch 36/64 loss: -0.16410905122756958
Batch 37/64 loss: -0.1710968017578125
Batch 38/64 loss: -0.16495227813720703
Batch 39/64 loss: -0.1555866003036499
Batch 40/64 loss: -0.17135369777679443
Batch 41/64 loss: -0.1729385256767273
Batch 42/64 loss: -0.1688622236251831
Batch 43/64 loss: -0.18531394004821777
Batch 44/64 loss: -0.16853570938110352
Batch 45/64 loss: -0.1840682029724121
Batch 46/64 loss: -0.18059104681015015
Batch 47/64 loss: -0.16533100605010986
Batch 48/64 loss: -0.18024569749832153
Batch 49/64 loss: -0.1768702268600464
Batch 50/64 loss: -0.15481746196746826
Batch 51/64 loss: -0.17159968614578247
Batch 52/64 loss: -0.14757192134857178
Batch 53/64 loss: -0.15950071811676025
Batch 54/64 loss: -0.18612134456634521
Batch 55/64 loss: -0.17212766408920288
Batch 56/64 loss: -0.1892838478088379
Batch 57/64 loss: -0.17988675832748413
Batch 58/64 loss: -0.18118590116500854
Batch 59/64 loss: -0.18837255239486694
Batch 60/64 loss: -0.17639601230621338
Batch 61/64 loss: -0.1841588020324707
Batch 62/64 loss: -0.17839038372039795
Batch 63/64 loss: -0.16160929203033447
Batch 64/64 loss: -0.1720697283744812
Epoch 312  Train loss: -0.17758447539572622  Val loss: -0.041517071912378786
Epoch 313
-------------------------------
Batch 1/64 loss: -0.19694995880126953
Batch 2/64 loss: -0.1987895369529724
Batch 3/64 loss: -0.1739766001701355
Batch 4/64 loss: -0.20085787773132324
Batch 5/64 loss: -0.16950809955596924
Batch 6/64 loss: -0.1502092480659485
Batch 7/64 loss: -0.1739710569381714
Batch 8/64 loss: -0.16408812999725342
Batch 9/64 loss: -0.20123356580734253
Batch 10/64 loss: -0.16612094640731812
Batch 11/64 loss: -0.19312632083892822
Batch 12/64 loss: -0.1822066307067871
Batch 13/64 loss: -0.15620076656341553
Batch 14/64 loss: -0.2073962688446045
Batch 15/64 loss: -0.1848962903022766
Batch 16/64 loss: -0.19023126363754272
Batch 17/64 loss: -0.173281729221344
Batch 18/64 loss: -0.20011985301971436
Batch 19/64 loss: -0.18489837646484375
Batch 20/64 loss: -0.18131637573242188
Batch 21/64 loss: -0.19105809926986694
Batch 22/64 loss: -0.1861872673034668
Batch 23/64 loss: -0.20519351959228516
Batch 24/64 loss: -0.17210936546325684
Batch 25/64 loss: -0.18384504318237305
Batch 26/64 loss: -0.15010082721710205
Batch 27/64 loss: -0.20514947175979614
Batch 28/64 loss: -0.17931485176086426
Batch 29/64 loss: -0.15524792671203613
Batch 30/64 loss: -0.17160528898239136
Batch 31/64 loss: -0.18431156873703003
Batch 32/64 loss: -0.17715561389923096
Batch 33/64 loss: -0.17616695165634155
Batch 34/64 loss: -0.1694384217262268
Batch 35/64 loss: -0.13909363746643066
Batch 36/64 loss: -0.17367571592330933
Batch 37/64 loss: -0.18463438749313354
Batch 38/64 loss: -0.17099416255950928
Batch 39/64 loss: -0.18195003271102905
Batch 40/64 loss: -0.1652093529701233
Batch 41/64 loss: -0.16783618927001953
Batch 42/64 loss: -0.18644392490386963
Batch 43/64 loss: -0.1802058219909668
Batch 44/64 loss: -0.17634320259094238
Batch 45/64 loss: -0.11892282962799072
Batch 46/64 loss: -0.17332977056503296
Batch 47/64 loss: -0.18166828155517578
Batch 48/64 loss: -0.19815552234649658
Batch 49/64 loss: -0.17532962560653687
Batch 50/64 loss: -0.1564466953277588
Batch 51/64 loss: -0.15618455410003662
Batch 52/64 loss: -0.1766817569732666
Batch 53/64 loss: -0.17558670043945312
Batch 54/64 loss: -0.1941186785697937
Batch 55/64 loss: -0.1387428641319275
Batch 56/64 loss: -0.188912034034729
Batch 57/64 loss: -0.17614299058914185
Batch 58/64 loss: -0.17410635948181152
Batch 59/64 loss: -0.16655004024505615
Batch 60/64 loss: -0.1794167160987854
Batch 61/64 loss: -0.1877502202987671
Batch 62/64 loss: -0.15308141708374023
Batch 63/64 loss: -0.18648308515548706
Batch 64/64 loss: -0.16273796558380127
Epoch 313  Train loss: -0.1766637358010984  Val loss: -0.04416352940588882
Epoch 314
-------------------------------
Batch 1/64 loss: -0.1948401927947998
Batch 2/64 loss: -0.20495200157165527
Batch 3/64 loss: -0.1941271424293518
Batch 4/64 loss: -0.18718212842941284
Batch 5/64 loss: -0.16038620471954346
Batch 6/64 loss: -0.1838686466217041
Batch 7/64 loss: -0.18784403800964355
Batch 8/64 loss: -0.17298048734664917
Batch 9/64 loss: -0.19551599025726318
Batch 10/64 loss: -0.187364399433136
Batch 11/64 loss: -0.17987298965454102
Batch 12/64 loss: -0.17922556400299072
Batch 13/64 loss: -0.19817078113555908
Batch 14/64 loss: -0.19564467668533325
Batch 15/64 loss: -0.18061864376068115
Batch 16/64 loss: -0.1561969518661499
Batch 17/64 loss: -0.16748249530792236
Batch 18/64 loss: -0.16301381587982178
Batch 19/64 loss: -0.16876310110092163
Batch 20/64 loss: -0.17939412593841553
Batch 21/64 loss: -0.1802552342414856
Batch 22/64 loss: -0.2017185091972351
Batch 23/64 loss: -0.1717262864112854
Batch 24/64 loss: -0.16913145780563354
Batch 25/64 loss: -0.19420117139816284
Batch 26/64 loss: -0.17699146270751953
Batch 27/64 loss: -0.18333911895751953
Batch 28/64 loss: -0.17777574062347412
Batch 29/64 loss: -0.17782413959503174
Batch 30/64 loss: -0.18788492679595947
Batch 31/64 loss: -0.16449642181396484
Batch 32/64 loss: -0.2017039656639099
Batch 33/64 loss: -0.17897605895996094
Batch 34/64 loss: -0.1772785782814026
Batch 35/64 loss: -0.16035354137420654
Batch 36/64 loss: -0.14508342742919922
Batch 37/64 loss: -0.15895771980285645
Batch 38/64 loss: -0.1967686414718628
Batch 39/64 loss: -0.17553383111953735
Batch 40/64 loss: -0.1839050054550171
Batch 41/64 loss: -0.19285506010055542
Batch 42/64 loss: -0.17960423231124878
Batch 43/64 loss: -0.14938509464263916
Batch 44/64 loss: -0.16927629709243774
Batch 45/64 loss: -0.1290397047996521
Batch 46/64 loss: -0.16199493408203125
Batch 47/64 loss: -0.18251067399978638
Batch 48/64 loss: -0.1751306653022766
Batch 49/64 loss: -0.18515938520431519
Batch 50/64 loss: -0.1801919937133789
Batch 51/64 loss: -0.1759992241859436
Batch 52/64 loss: -0.1883852481842041
Batch 53/64 loss: -0.17539983987808228
Batch 54/64 loss: -0.19472134113311768
Batch 55/64 loss: -0.1746995449066162
Batch 56/64 loss: -0.18141037225723267
Batch 57/64 loss: -0.19103682041168213
Batch 58/64 loss: -0.15604567527770996
Batch 59/64 loss: -0.18304747343063354
Batch 60/64 loss: -0.17809081077575684
Batch 61/64 loss: -0.15827035903930664
Batch 62/64 loss: -0.17976558208465576
Batch 63/64 loss: -0.17563605308532715
Batch 64/64 loss: -0.18108105659484863
Epoch 314  Train loss: -0.17811477324541877  Val loss: -0.044156704981302475
Epoch 315
-------------------------------
Batch 1/64 loss: -0.19471526145935059
Batch 2/64 loss: -0.19405889511108398
Batch 3/64 loss: -0.15968579053878784
Batch 4/64 loss: -0.19254368543624878
Batch 5/64 loss: -0.19617164134979248
Batch 6/64 loss: -0.14690959453582764
Batch 7/64 loss: -0.18973159790039062
Batch 8/64 loss: -0.17795896530151367
Batch 9/64 loss: -0.19547367095947266
Batch 10/64 loss: -0.19268399477005005
Batch 11/64 loss: -0.15369915962219238
Batch 12/64 loss: -0.1806960105895996
Batch 13/64 loss: -0.1879415512084961
Batch 14/64 loss: -0.17166221141815186
Batch 15/64 loss: -0.1945352554321289
Batch 16/64 loss: -0.1918402910232544
Batch 17/64 loss: -0.15482670068740845
Batch 18/64 loss: -0.17160624265670776
Batch 19/64 loss: -0.18188458681106567
Batch 20/64 loss: -0.18677645921707153
Batch 21/64 loss: -0.18324917554855347
Batch 22/64 loss: -0.19400233030319214
Batch 23/64 loss: -0.14932727813720703
Batch 24/64 loss: -0.1826462745666504
Batch 25/64 loss: -0.18304747343063354
Batch 26/64 loss: -0.1573115587234497
Batch 27/64 loss: -0.20668256282806396
Batch 28/64 loss: -0.17736005783081055
Batch 29/64 loss: -0.14512205123901367
Batch 30/64 loss: -0.17069226503372192
Batch 31/64 loss: -0.19151973724365234
Batch 32/64 loss: -0.1722460389137268
Batch 33/64 loss: -0.16447752714157104
Batch 34/64 loss: -0.1695757508277893
Batch 35/64 loss: -0.1886969804763794
Batch 36/64 loss: -0.1840726137161255
Batch 37/64 loss: -0.18345308303833008
Batch 38/64 loss: -0.16650843620300293
Batch 39/64 loss: -0.16818195581436157
Batch 40/64 loss: -0.18228262662887573
Batch 41/64 loss: -0.18960386514663696
Batch 42/64 loss: -0.19209390878677368
Batch 43/64 loss: -0.18412494659423828
Batch 44/64 loss: -0.18100541830062866
Batch 45/64 loss: -0.16751360893249512
Batch 46/64 loss: -0.17479407787322998
Batch 47/64 loss: -0.14998281002044678
Batch 48/64 loss: -0.18525362014770508
Batch 49/64 loss: -0.17615938186645508
Batch 50/64 loss: -0.17194008827209473
Batch 51/64 loss: -0.17235803604125977
Batch 52/64 loss: -0.17299306392669678
Batch 53/64 loss: -0.18919986486434937
Batch 54/64 loss: -0.18872636556625366
Batch 55/64 loss: -0.16124451160430908
Batch 56/64 loss: -0.18513637781143188
Batch 57/64 loss: -0.17174237966537476
Batch 58/64 loss: -0.17691844701766968
Batch 59/64 loss: -0.18924033641815186
Batch 60/64 loss: -0.15424948930740356
Batch 61/64 loss: -0.16827797889709473
Batch 62/64 loss: -0.15442872047424316
Batch 63/64 loss: -0.1599465012550354
Batch 64/64 loss: -0.1776946783065796
Epoch 315  Train loss: -0.17703626903833128  Val loss: -0.0443299119824806
Epoch 316
-------------------------------
Batch 1/64 loss: -0.20631051063537598
Batch 2/64 loss: -0.17724508047103882
Batch 3/64 loss: -0.19257467985153198
Batch 4/64 loss: -0.19419139623641968
Batch 5/64 loss: -0.17428088188171387
Batch 6/64 loss: -0.18188142776489258
Batch 7/64 loss: -0.1731451153755188
Batch 8/64 loss: -0.17038202285766602
Batch 9/64 loss: -0.1887546181678772
Batch 10/64 loss: -0.16285544633865356
Batch 11/64 loss: -0.18312335014343262
Batch 12/64 loss: -0.14225876331329346
Batch 13/64 loss: -0.1912776231765747
Batch 14/64 loss: -0.18973731994628906
Batch 15/64 loss: -0.17665374279022217
Batch 16/64 loss: -0.17865365743637085
Batch 17/64 loss: -0.18404412269592285
Batch 18/64 loss: -0.18478798866271973
Batch 19/64 loss: -0.19608569145202637
Batch 20/64 loss: -0.1843334436416626
Batch 21/64 loss: -0.17970681190490723
Batch 22/64 loss: -0.192268967628479
Batch 23/64 loss: -0.18250948190689087
Batch 24/64 loss: -0.19272571802139282
Batch 25/64 loss: -0.1535031795501709
Batch 26/64 loss: -0.20092052221298218
Batch 27/64 loss: -0.18710100650787354
Batch 28/64 loss: -0.1704881191253662
Batch 29/64 loss: -0.15122747421264648
Batch 30/64 loss: -0.197440505027771
Batch 31/64 loss: -0.17449021339416504
Batch 32/64 loss: -0.16749882698059082
Batch 33/64 loss: -0.17721456289291382
Batch 34/64 loss: -0.14794623851776123
Batch 35/64 loss: -0.19069015979766846
Batch 36/64 loss: -0.19487041234970093
Batch 37/64 loss: -0.18398797512054443
Batch 38/64 loss: -0.19125771522521973
Batch 39/64 loss: -0.1711025834083557
Batch 40/64 loss: -0.17709434032440186
Batch 41/64 loss: -0.18987488746643066
Batch 42/64 loss: -0.16032397747039795
Batch 43/64 loss: -0.16152924299240112
Batch 44/64 loss: -0.18843334913253784
Batch 45/64 loss: -0.18174374103546143
Batch 46/64 loss: -0.1637217402458191
Batch 47/64 loss: -0.1656838059425354
Batch 48/64 loss: -0.18203997611999512
Batch 49/64 loss: -0.18874549865722656
Batch 50/64 loss: -0.19034343957901
Batch 51/64 loss: -0.17361265420913696
Batch 52/64 loss: -0.1960211992263794
Batch 53/64 loss: -0.15324389934539795
Batch 54/64 loss: -0.16064798831939697
Batch 55/64 loss: -0.1704179048538208
Batch 56/64 loss: -0.171425461769104
Batch 57/64 loss: -0.17447388172149658
Batch 58/64 loss: -0.1947268843650818
Batch 59/64 loss: -0.17128664255142212
Batch 60/64 loss: -0.18659383058547974
Batch 61/64 loss: -0.1989668607711792
Batch 62/64 loss: -0.1656419038772583
Batch 63/64 loss: -0.17261254787445068
Batch 64/64 loss: -0.18653875589370728
Epoch 316  Train loss: -0.17911587579577576  Val loss: -0.04326723551832114
Epoch 317
-------------------------------
Batch 1/64 loss: -0.1842837929725647
Batch 2/64 loss: -0.1613548994064331
Batch 3/64 loss: -0.21359014511108398
Batch 4/64 loss: -0.17700982093811035
Batch 5/64 loss: -0.1880474090576172
Batch 6/64 loss: -0.1931813359260559
Batch 7/64 loss: -0.19767731428146362
Batch 8/64 loss: -0.16413891315460205
Batch 9/64 loss: -0.19843322038650513
Batch 10/64 loss: -0.18940496444702148
Batch 11/64 loss: -0.18994539976119995
Batch 12/64 loss: -0.1925450563430786
Batch 13/64 loss: -0.17366480827331543
Batch 14/64 loss: -0.20804530382156372
Batch 15/64 loss: -0.19717860221862793
Batch 16/64 loss: -0.1829451322555542
Batch 17/64 loss: -0.1905236840248108
Batch 18/64 loss: -0.18401485681533813
Batch 19/64 loss: -0.17068594694137573
Batch 20/64 loss: -0.1844692826271057
Batch 21/64 loss: -0.1848199963569641
Batch 22/64 loss: -0.15207219123840332
Batch 23/64 loss: -0.17880475521087646
Batch 24/64 loss: -0.19503915309906006
Batch 25/64 loss: -0.1789705753326416
Batch 26/64 loss: -0.19017791748046875
Batch 27/64 loss: -0.18539726734161377
Batch 28/64 loss: -0.179867684841156
Batch 29/64 loss: -0.15661931037902832
Batch 30/64 loss: -0.16364949941635132
Batch 31/64 loss: -0.15190517902374268
Batch 32/64 loss: -0.14251047372817993
Batch 33/64 loss: -0.1793506145477295
Batch 34/64 loss: -0.17608195543289185
Batch 35/64 loss: -0.17834651470184326
Batch 36/64 loss: -0.19185638427734375
Batch 37/64 loss: -0.1734929084777832
Batch 38/64 loss: -0.1930546760559082
Batch 39/64 loss: -0.1786097288131714
Batch 40/64 loss: -0.17730391025543213
Batch 41/64 loss: -0.17005938291549683
Batch 42/64 loss: -0.1473177671432495
Batch 43/64 loss: -0.150307297706604
Batch 44/64 loss: -0.15900498628616333
Batch 45/64 loss: -0.1735377311706543
Batch 46/64 loss: -0.17639148235321045
Batch 47/64 loss: -0.1915602684020996
Batch 48/64 loss: -0.16115301847457886
Batch 49/64 loss: -0.19112074375152588
Batch 50/64 loss: -0.17465674877166748
Batch 51/64 loss: -0.18281221389770508
Batch 52/64 loss: -0.1916385293006897
Batch 53/64 loss: -0.172319233417511
Batch 54/64 loss: -0.16577601432800293
Batch 55/64 loss: -0.18336910009384155
Batch 56/64 loss: -0.17655646800994873
Batch 57/64 loss: -0.178943932056427
Batch 58/64 loss: -0.15937668085098267
Batch 59/64 loss: -0.15380513668060303
Batch 60/64 loss: -0.17506426572799683
Batch 61/64 loss: -0.1840883493423462
Batch 62/64 loss: -0.13551580905914307
Batch 63/64 loss: -0.19013595581054688
Batch 64/64 loss: -0.1708444356918335
Epoch 317  Train loss: -0.17759552983676685  Val loss: -0.04240955110268085
Epoch 318
-------------------------------
Batch 1/64 loss: -0.17101669311523438
Batch 2/64 loss: -0.17951524257659912
Batch 3/64 loss: -0.17843413352966309
Batch 4/64 loss: -0.18659108877182007
Batch 5/64 loss: -0.16212451457977295
Batch 6/64 loss: -0.17101192474365234
Batch 7/64 loss: -0.1800769567489624
Batch 8/64 loss: -0.1721208095550537
Batch 9/64 loss: -0.16790693998336792
Batch 10/64 loss: -0.17509686946868896
Batch 11/64 loss: -0.20373356342315674
Batch 12/64 loss: -0.1890670657157898
Batch 13/64 loss: -0.18514204025268555
Batch 14/64 loss: -0.186639666557312
Batch 15/64 loss: -0.1832522749900818
Batch 16/64 loss: -0.18275320529937744
Batch 17/64 loss: -0.18902617692947388
Batch 18/64 loss: -0.17417538166046143
Batch 19/64 loss: -0.1659647822380066
Batch 20/64 loss: -0.17306602001190186
Batch 21/64 loss: -0.17910802364349365
Batch 22/64 loss: -0.18476653099060059
Batch 23/64 loss: -0.1512831449508667
Batch 24/64 loss: -0.1753990650177002
Batch 25/64 loss: -0.19487428665161133
Batch 26/64 loss: -0.195068359375
Batch 27/64 loss: -0.18361550569534302
Batch 28/64 loss: -0.18280279636383057
Batch 29/64 loss: -0.17843526601791382
Batch 30/64 loss: -0.18690133094787598
Batch 31/64 loss: -0.19665884971618652
Batch 32/64 loss: -0.17495226860046387
Batch 33/64 loss: -0.17231470346450806
Batch 34/64 loss: -0.18780046701431274
Batch 35/64 loss: -0.17950594425201416
Batch 36/64 loss: -0.1721135377883911
Batch 37/64 loss: -0.18416619300842285
Batch 38/64 loss: -0.18681734800338745
Batch 39/64 loss: -0.16838884353637695
Batch 40/64 loss: -0.15602272748947144
Batch 41/64 loss: -0.169386088848114
Batch 42/64 loss: -0.1690899133682251
Batch 43/64 loss: -0.18983715772628784
Batch 44/64 loss: -0.19178229570388794
Batch 45/64 loss: -0.16498923301696777
Batch 46/64 loss: -0.17866945266723633
Batch 47/64 loss: -0.1688399314880371
Batch 48/64 loss: -0.19081443548202515
Batch 49/64 loss: -0.17095530033111572
Batch 50/64 loss: -0.19719958305358887
Batch 51/64 loss: -0.18804311752319336
Batch 52/64 loss: -0.17867952585220337
Batch 53/64 loss: -0.16827672719955444
Batch 54/64 loss: -0.1759290099143982
Batch 55/64 loss: -0.1648005247116089
Batch 56/64 loss: -0.20525836944580078
Batch 57/64 loss: -0.18887996673583984
Batch 58/64 loss: -0.18464034795761108
Batch 59/64 loss: -0.18717432022094727
Batch 60/64 loss: -0.18857526779174805
Batch 61/64 loss: -0.17846202850341797
Batch 62/64 loss: -0.1732894778251648
Batch 63/64 loss: -0.18145930767059326
Batch 64/64 loss: -0.16882747411727905
Epoch 318  Train loss: -0.17959784362830367  Val loss: -0.041418500782288224
Epoch 319
-------------------------------
Batch 1/64 loss: -0.1889282464981079
Batch 2/64 loss: -0.19195538759231567
Batch 3/64 loss: -0.18322598934173584
Batch 4/64 loss: -0.17686867713928223
Batch 5/64 loss: -0.1968010663986206
Batch 6/64 loss: -0.21153771877288818
Batch 7/64 loss: -0.19599449634552002
Batch 8/64 loss: -0.190240740776062
Batch 9/64 loss: -0.17903882265090942
Batch 10/64 loss: -0.190152108669281
Batch 11/64 loss: -0.1867092251777649
Batch 12/64 loss: -0.1783493161201477
Batch 13/64 loss: -0.19153201580047607
Batch 14/64 loss: -0.16942918300628662
Batch 15/64 loss: -0.19377833604812622
Batch 16/64 loss: -0.1670689582824707
Batch 17/64 loss: -0.185960590839386
Batch 18/64 loss: -0.1917474865913391
Batch 19/64 loss: -0.18725162744522095
Batch 20/64 loss: -0.17559343576431274
Batch 21/64 loss: -0.1864849328994751
Batch 22/64 loss: -0.1569216251373291
Batch 23/64 loss: -0.1742168664932251
Batch 24/64 loss: -0.15992122888565063
Batch 25/64 loss: -0.1813657283782959
Batch 26/64 loss: -0.1915382742881775
Batch 27/64 loss: -0.17880147695541382
Batch 28/64 loss: -0.17638862133026123
Batch 29/64 loss: -0.197296142578125
Batch 30/64 loss: -0.18314528465270996
Batch 31/64 loss: -0.1909051537513733
Batch 32/64 loss: -0.19035977125167847
Batch 33/64 loss: -0.18803274631500244
Batch 34/64 loss: -0.1672561764717102
Batch 35/64 loss: -0.1852143406867981
Batch 36/64 loss: -0.1589481234550476
Batch 37/64 loss: -0.19407039880752563
Batch 38/64 loss: -0.14661037921905518
Batch 39/64 loss: -0.17998826503753662
Batch 40/64 loss: -0.19630944728851318
Batch 41/64 loss: -0.18879187107086182
Batch 42/64 loss: -0.16709297895431519
Batch 43/64 loss: -0.1584606170654297
Batch 44/64 loss: -0.16008847951889038
Batch 45/64 loss: -0.1712324619293213
Batch 46/64 loss: -0.18185865879058838
Batch 47/64 loss: -0.1791098713874817
Batch 48/64 loss: -0.16992944478988647
Batch 49/64 loss: -0.14476799964904785
Batch 50/64 loss: -0.18745440244674683
Batch 51/64 loss: -0.18271535634994507
Batch 52/64 loss: -0.17532223463058472
Batch 53/64 loss: -0.1786724328994751
Batch 54/64 loss: -0.1669260859489441
Batch 55/64 loss: -0.1929658055305481
Batch 56/64 loss: -0.17784321308135986
Batch 57/64 loss: -0.18378549814224243
Batch 58/64 loss: -0.1580430269241333
Batch 59/64 loss: -0.18329095840454102
Batch 60/64 loss: -0.1771833896636963
Batch 61/64 loss: -0.17937177419662476
Batch 62/64 loss: -0.18717420101165771
Batch 63/64 loss: -0.1791580319404602
Batch 64/64 loss: -0.1780773401260376
Epoch 319  Train loss: -0.18008996411865832  Val loss: -0.04664777234657524
Epoch 320
-------------------------------
Batch 1/64 loss: -0.20706599950790405
Batch 2/64 loss: -0.19190144538879395
Batch 3/64 loss: -0.18023300170898438
Batch 4/64 loss: -0.1772092580795288
Batch 5/64 loss: -0.1894819736480713
Batch 6/64 loss: -0.18549013137817383
Batch 7/64 loss: -0.18290096521377563
Batch 8/64 loss: -0.17709589004516602
Batch 9/64 loss: -0.15754181146621704
Batch 10/64 loss: -0.17611587047576904
Batch 11/64 loss: -0.20869046449661255
Batch 12/64 loss: -0.16781508922576904
Batch 13/64 loss: -0.1855211853981018
Batch 14/64 loss: -0.19428765773773193
Batch 15/64 loss: -0.17503643035888672
Batch 16/64 loss: -0.18041133880615234
Batch 17/64 loss: -0.18715530633926392
Batch 18/64 loss: -0.19112777709960938
Batch 19/64 loss: -0.19153660535812378
Batch 20/64 loss: -0.19234782457351685
Batch 21/64 loss: -0.1617986559867859
Batch 22/64 loss: -0.2092376947402954
Batch 23/64 loss: -0.19143062829971313
Batch 24/64 loss: -0.19179391860961914
Batch 25/64 loss: -0.15689152479171753
Batch 26/64 loss: -0.16847717761993408
Batch 27/64 loss: -0.1928638219833374
Batch 28/64 loss: -0.18379747867584229
Batch 29/64 loss: -0.16498374938964844
Batch 30/64 loss: -0.17339414358139038
Batch 31/64 loss: -0.169802725315094
Batch 32/64 loss: -0.1853960156440735
Batch 33/64 loss: -0.17998415231704712
Batch 34/64 loss: -0.17461133003234863
Batch 35/64 loss: -0.18561077117919922
Batch 36/64 loss: -0.1884174346923828
Batch 37/64 loss: -0.17428112030029297
Batch 38/64 loss: -0.18436330556869507
Batch 39/64 loss: -0.14964449405670166
Batch 40/64 loss: -0.17110514640808105
Batch 41/64 loss: -0.16892367601394653
Batch 42/64 loss: -0.18866795301437378
Batch 43/64 loss: -0.16450458765029907
Batch 44/64 loss: -0.17238038778305054
Batch 45/64 loss: -0.18373829126358032
Batch 46/64 loss: -0.18493199348449707
Batch 47/64 loss: -0.18822556734085083
Batch 48/64 loss: -0.1831410527229309
Batch 49/64 loss: -0.1833210587501526
Batch 50/64 loss: -0.1773877739906311
Batch 51/64 loss: -0.17785310745239258
Batch 52/64 loss: -0.18494927883148193
Batch 53/64 loss: -0.20724451541900635
Batch 54/64 loss: -0.19089281558990479
Batch 55/64 loss: -0.17734205722808838
Batch 56/64 loss: -0.1959397792816162
Batch 57/64 loss: -0.15854400396347046
Batch 58/64 loss: -0.17978835105895996
Batch 59/64 loss: -0.18628942966461182
Batch 60/64 loss: -0.1956700086593628
Batch 61/64 loss: -0.1884211301803589
Batch 62/64 loss: -0.17929893732070923
Batch 63/64 loss: -0.18572580814361572
Batch 64/64 loss: -0.16895413398742676
Epoch 320  Train loss: -0.18175291734583238  Val loss: -0.04330200530409403
Epoch 321
-------------------------------
Batch 1/64 loss: -0.17178595066070557
Batch 2/64 loss: -0.17137926816940308
Batch 3/64 loss: -0.19864243268966675
Batch 4/64 loss: -0.18751156330108643
Batch 5/64 loss: -0.2004932165145874
Batch 6/64 loss: -0.18405157327651978
Batch 7/64 loss: -0.18728268146514893
Batch 8/64 loss: -0.1567608118057251
Batch 9/64 loss: -0.19908112287521362
Batch 10/64 loss: -0.16971063613891602
Batch 11/64 loss: -0.17650055885314941
Batch 12/64 loss: -0.17230641841888428
Batch 13/64 loss: -0.18095600605010986
Batch 14/64 loss: -0.17672115564346313
Batch 15/64 loss: -0.1361101269721985
Batch 16/64 loss: -0.19080829620361328
Batch 17/64 loss: -0.19741952419281006
Batch 18/64 loss: -0.17016935348510742
Batch 19/64 loss: -0.1733928918838501
Batch 20/64 loss: -0.18733912706375122
Batch 21/64 loss: -0.13921892642974854
Batch 22/64 loss: -0.20738112926483154
Batch 23/64 loss: -0.20776742696762085
Batch 24/64 loss: -0.15944629907608032
Batch 25/64 loss: -0.19707733392715454
Batch 26/64 loss: -0.1916852593421936
Batch 27/64 loss: -0.20829635858535767
Batch 28/64 loss: -0.18778270483016968
Batch 29/64 loss: -0.19141322374343872
Batch 30/64 loss: -0.19484978914260864
Batch 31/64 loss: -0.1957697868347168
Batch 32/64 loss: -0.18417614698410034
Batch 33/64 loss: -0.18679368495941162
Batch 34/64 loss: -0.17594701051712036
Batch 35/64 loss: -0.1759588122367859
Batch 36/64 loss: -0.1615353226661682
Batch 37/64 loss: -0.1602444052696228
Batch 38/64 loss: -0.18950021266937256
Batch 39/64 loss: -0.1773909330368042
Batch 40/64 loss: -0.19320780038833618
Batch 41/64 loss: -0.1776067018508911
Batch 42/64 loss: -0.19219249486923218
Batch 43/64 loss: -0.17602479457855225
Batch 44/64 loss: -0.18549412488937378
Batch 45/64 loss: -0.16696077585220337
Batch 46/64 loss: -0.18604058027267456
Batch 47/64 loss: -0.20559871196746826
Batch 48/64 loss: -0.18783456087112427
Batch 49/64 loss: -0.17942672967910767
Batch 50/64 loss: -0.17921417951583862
Batch 51/64 loss: -0.19452691078186035
Batch 52/64 loss: -0.1820177435874939
Batch 53/64 loss: -0.19609493017196655
Batch 54/64 loss: -0.18271464109420776
Batch 55/64 loss: -0.18929654359817505
Batch 56/64 loss: -0.19868910312652588
Batch 57/64 loss: -0.16914284229278564
Batch 58/64 loss: -0.173822283744812
Batch 59/64 loss: -0.1959989070892334
Batch 60/64 loss: -0.20020544528961182
Batch 61/64 loss: -0.17482829093933105
Batch 62/64 loss: -0.14789873361587524
Batch 63/64 loss: -0.1769503355026245
Batch 64/64 loss: -0.15613585710525513
Epoch 321  Train loss: -0.18211054965561513  Val loss: -0.042939949691090794
Epoch 322
-------------------------------
Batch 1/64 loss: -0.18378400802612305
Batch 2/64 loss: -0.17402434349060059
Batch 3/64 loss: -0.20648813247680664
Batch 4/64 loss: -0.18185168504714966
Batch 5/64 loss: -0.19280648231506348
Batch 6/64 loss: -0.19657248258590698
Batch 7/64 loss: -0.1771073341369629
Batch 8/64 loss: -0.1747746467590332
Batch 9/64 loss: -0.19940680265426636
Batch 10/64 loss: -0.17707866430282593
Batch 11/64 loss: -0.20164865255355835
Batch 12/64 loss: -0.17261123657226562
Batch 13/64 loss: -0.1839832067489624
Batch 14/64 loss: -0.18496966361999512
Batch 15/64 loss: -0.17005831003189087
Batch 16/64 loss: -0.17800366878509521
Batch 17/64 loss: -0.19238996505737305
Batch 18/64 loss: -0.21205288171768188
Batch 19/64 loss: -0.17961150407791138
Batch 20/64 loss: -0.19271188974380493
Batch 21/64 loss: -0.17035841941833496
Batch 22/64 loss: -0.18470001220703125
Batch 23/64 loss: -0.18418002128601074
Batch 24/64 loss: -0.18316811323165894
Batch 25/64 loss: -0.18449515104293823
Batch 26/64 loss: -0.19464534521102905
Batch 27/64 loss: -0.19395506381988525
Batch 28/64 loss: -0.1866019368171692
Batch 29/64 loss: -0.19221627712249756
Batch 30/64 loss: -0.18667972087860107
Batch 31/64 loss: -0.18911117315292358
Batch 32/64 loss: -0.1670265793800354
Batch 33/64 loss: -0.2119496464729309
Batch 34/64 loss: -0.19154608249664307
Batch 35/64 loss: -0.20598983764648438
Batch 36/64 loss: -0.18398690223693848
Batch 37/64 loss: -0.18080264329910278
Batch 38/64 loss: -0.1696118712425232
Batch 39/64 loss: -0.18302524089813232
Batch 40/64 loss: -0.15821510553359985
Batch 41/64 loss: -0.14745193719863892
Batch 42/64 loss: -0.18551111221313477
Batch 43/64 loss: -0.18253731727600098
Batch 44/64 loss: -0.19657838344573975
Batch 45/64 loss: -0.1788492202758789
Batch 46/64 loss: -0.18181610107421875
Batch 47/64 loss: -0.17481952905654907
Batch 48/64 loss: -0.18437457084655762
Batch 49/64 loss: -0.1995100975036621
Batch 50/64 loss: -0.1616971492767334
Batch 51/64 loss: -0.19754856824874878
Batch 52/64 loss: -0.1643669605255127
Batch 53/64 loss: -0.18994837999343872
Batch 54/64 loss: -0.18206405639648438
Batch 55/64 loss: -0.1873033046722412
Batch 56/64 loss: -0.1953059434890747
Batch 57/64 loss: -0.1908121109008789
Batch 58/64 loss: -0.20313215255737305
Batch 59/64 loss: -0.17131078243255615
Batch 60/64 loss: -0.1477956771850586
Batch 61/64 loss: -0.1726568341255188
Batch 62/64 loss: -0.18374061584472656
Batch 63/64 loss: -0.18332034349441528
Batch 64/64 loss: -0.18583059310913086
Epoch 322  Train loss: -0.18409450661902335  Val loss: -0.043562824783456286
Epoch 323
-------------------------------
Batch 1/64 loss: -0.18835687637329102
Batch 2/64 loss: -0.19420039653778076
Batch 3/64 loss: -0.18057787418365479
Batch 4/64 loss: -0.20751380920410156
Batch 5/64 loss: -0.18191707134246826
Batch 6/64 loss: -0.18822479248046875
Batch 7/64 loss: -0.17481982707977295
Batch 8/64 loss: -0.19135910272598267
Batch 9/64 loss: -0.1722739338874817
Batch 10/64 loss: -0.1987614631652832
Batch 11/64 loss: -0.16078227758407593
Batch 12/64 loss: -0.16785180568695068
Batch 13/64 loss: -0.170030415058136
Batch 14/64 loss: -0.1849743127822876
Batch 15/64 loss: -0.1665802001953125
Batch 16/64 loss: -0.179033100605011
Batch 17/64 loss: -0.1537647843360901
Batch 18/64 loss: -0.16638201475143433
Batch 19/64 loss: -0.17882567644119263
Batch 20/64 loss: -0.20586150884628296
Batch 21/64 loss: -0.18201780319213867
Batch 22/64 loss: -0.1799045205116272
Batch 23/64 loss: -0.17008095979690552
Batch 24/64 loss: -0.1638484001159668
Batch 25/64 loss: -0.19050312042236328
Batch 26/64 loss: -0.17463457584381104
Batch 27/64 loss: -0.18000566959381104
Batch 28/64 loss: -0.18360668420791626
Batch 29/64 loss: -0.18411922454833984
Batch 30/64 loss: -0.1718364953994751
Batch 31/64 loss: -0.15496361255645752
Batch 32/64 loss: -0.16427743434906006
Batch 33/64 loss: -0.18337970972061157
Batch 34/64 loss: -0.20147252082824707
Batch 35/64 loss: -0.1866658329963684
Batch 36/64 loss: -0.18183958530426025
Batch 37/64 loss: -0.18608516454696655
Batch 38/64 loss: -0.15357542037963867
Batch 39/64 loss: -0.17152684926986694
Batch 40/64 loss: -0.1837984323501587
Batch 41/64 loss: -0.15409600734710693
Batch 42/64 loss: -0.17375993728637695
Batch 43/64 loss: -0.20123159885406494
Batch 44/64 loss: -0.1798642873764038
Batch 45/64 loss: -0.1779932975769043
Batch 46/64 loss: -0.17316514253616333
Batch 47/64 loss: -0.19257503747940063
Batch 48/64 loss: -0.1771392822265625
Batch 49/64 loss: -0.20667356252670288
Batch 50/64 loss: -0.1868641972541809
Batch 51/64 loss: -0.1875704526901245
Batch 52/64 loss: -0.1900736689567566
Batch 53/64 loss: -0.15546518564224243
Batch 54/64 loss: -0.20615774393081665
Batch 55/64 loss: -0.1758342981338501
Batch 56/64 loss: -0.18623435497283936
Batch 57/64 loss: -0.19655299186706543
Batch 58/64 loss: -0.18874835968017578
Batch 59/64 loss: -0.20214790105819702
Batch 60/64 loss: -0.19001245498657227
Batch 61/64 loss: -0.175187349319458
Batch 62/64 loss: -0.1928756833076477
Batch 63/64 loss: -0.16657280921936035
Batch 64/64 loss: -0.16579300165176392
Epoch 323  Train loss: -0.1807274292497074  Val loss: -0.042583193565971664
Epoch 324
-------------------------------
Batch 1/64 loss: -0.19974678754806519
Batch 2/64 loss: -0.19067466259002686
Batch 3/64 loss: -0.1734355092048645
Batch 4/64 loss: -0.19441020488739014
Batch 5/64 loss: -0.2119274139404297
Batch 6/64 loss: -0.21087944507598877
Batch 7/64 loss: -0.21154671907424927
Batch 8/64 loss: -0.18822401762008667
Batch 9/64 loss: -0.18314260244369507
Batch 10/64 loss: -0.19420820474624634
Batch 11/64 loss: -0.20942974090576172
Batch 12/64 loss: -0.2069772481918335
Batch 13/64 loss: -0.16872596740722656
Batch 14/64 loss: -0.19614946842193604
Batch 15/64 loss: -0.19930219650268555
Batch 16/64 loss: -0.197648823261261
Batch 17/64 loss: -0.1695532202720642
Batch 18/64 loss: -0.18751949071884155
Batch 19/64 loss: -0.17093980312347412
Batch 20/64 loss: -0.19776487350463867
Batch 21/64 loss: -0.19384825229644775
Batch 22/64 loss: -0.18107032775878906
Batch 23/64 loss: -0.1680542230606079
Batch 24/64 loss: -0.20457100868225098
Batch 25/64 loss: -0.18301796913146973
Batch 26/64 loss: -0.19500017166137695
Batch 27/64 loss: -0.18731790781021118
Batch 28/64 loss: -0.19761812686920166
Batch 29/64 loss: -0.18508422374725342
Batch 30/64 loss: -0.16925078630447388
Batch 31/64 loss: -0.18710952997207642
Batch 32/64 loss: -0.20414316654205322
Batch 33/64 loss: -0.18683987855911255
Batch 34/64 loss: -0.17290127277374268
Batch 35/64 loss: -0.19825834035873413
Batch 36/64 loss: -0.15648871660232544
Batch 37/64 loss: -0.18226635456085205
Batch 38/64 loss: -0.17844265699386597
Batch 39/64 loss: -0.16804730892181396
Batch 40/64 loss: -0.17377257347106934
Batch 41/64 loss: -0.18902146816253662
Batch 42/64 loss: -0.18293607234954834
Batch 43/64 loss: -0.2007027268409729
Batch 44/64 loss: -0.15644699335098267
Batch 45/64 loss: -0.19069528579711914
Batch 46/64 loss: -0.17194271087646484
Batch 47/64 loss: -0.18682724237442017
Batch 48/64 loss: -0.17374122142791748
Batch 49/64 loss: -0.15565335750579834
Batch 50/64 loss: -0.16180795431137085
Batch 51/64 loss: -0.1478387713432312
Batch 52/64 loss: -0.16071897745132446
Batch 53/64 loss: -0.18054401874542236
Batch 54/64 loss: -0.15365266799926758
Batch 55/64 loss: -0.1828594207763672
Batch 56/64 loss: -0.17215687036514282
Batch 57/64 loss: -0.19084006547927856
Batch 58/64 loss: -0.1715361475944519
Batch 59/64 loss: -0.1669328808784485
Batch 60/64 loss: -0.17246931791305542
Batch 61/64 loss: -0.17244386672973633
Batch 62/64 loss: -0.18276965618133545
Batch 63/64 loss: -0.18184220790863037
Batch 64/64 loss: -0.18103712797164917
Epoch 324  Train loss: -0.18317595206054987  Val loss: -0.03952514387897609
Epoch 325
-------------------------------
Batch 1/64 loss: -0.17497330904006958
Batch 2/64 loss: -0.1920756697654724
Batch 3/64 loss: -0.19287818670272827
Batch 4/64 loss: -0.14997553825378418
Batch 5/64 loss: -0.19517427682876587
Batch 6/64 loss: -0.18041956424713135
Batch 7/64 loss: -0.18862658739089966
Batch 8/64 loss: -0.18000996112823486
Batch 9/64 loss: -0.18366140127182007
Batch 10/64 loss: -0.1561291217803955
Batch 11/64 loss: -0.19894170761108398
Batch 12/64 loss: -0.17971187829971313
Batch 13/64 loss: -0.1951277256011963
Batch 14/64 loss: -0.1953602433204651
Batch 15/64 loss: -0.20967614650726318
Batch 16/64 loss: -0.17952269315719604
Batch 17/64 loss: -0.20067906379699707
Batch 18/64 loss: -0.15651261806488037
Batch 19/64 loss: -0.19787949323654175
Batch 20/64 loss: -0.18137413263320923
Batch 21/64 loss: -0.17525488138198853
Batch 22/64 loss: -0.19227027893066406
Batch 23/64 loss: -0.19665127992630005
Batch 24/64 loss: -0.19600141048431396
Batch 25/64 loss: -0.1924067735671997
Batch 26/64 loss: -0.19511842727661133
Batch 27/64 loss: -0.18396925926208496
Batch 28/64 loss: -0.17911851406097412
Batch 29/64 loss: -0.17916035652160645
Batch 30/64 loss: -0.19646400213241577
Batch 31/64 loss: -0.21411603689193726
Batch 32/64 loss: -0.18627196550369263
Batch 33/64 loss: -0.1958320140838623
Batch 34/64 loss: -0.18626075983047485
Batch 35/64 loss: -0.18944114446640015
Batch 36/64 loss: -0.17691409587860107
Batch 37/64 loss: -0.187164306640625
Batch 38/64 loss: -0.18594753742218018
Batch 39/64 loss: -0.18661612272262573
Batch 40/64 loss: -0.17488741874694824
Batch 41/64 loss: -0.1466313600540161
Batch 42/64 loss: -0.17981523275375366
Batch 43/64 loss: -0.17680907249450684
Batch 44/64 loss: -0.18451803922653198
Batch 45/64 loss: -0.18606257438659668
Batch 46/64 loss: -0.19739294052124023
Batch 47/64 loss: -0.19259798526763916
Batch 48/64 loss: -0.1823042631149292
Batch 49/64 loss: -0.1973249912261963
Batch 50/64 loss: -0.16809332370758057
Batch 51/64 loss: -0.17330008745193481
Batch 52/64 loss: -0.2011479139328003
Batch 53/64 loss: -0.1830885410308838
Batch 54/64 loss: -0.18622571229934692
Batch 55/64 loss: -0.18374091386795044
Batch 56/64 loss: -0.1980152130126953
Batch 57/64 loss: -0.19514548778533936
Batch 58/64 loss: -0.18406152725219727
Batch 59/64 loss: -0.1801050901412964
Batch 60/64 loss: -0.1938413381576538
Batch 61/64 loss: -0.19459182024002075
Batch 62/64 loss: -0.18444490432739258
Batch 63/64 loss: -0.18701666593551636
Batch 64/64 loss: -0.1684064269065857
Epoch 325  Train loss: -0.18574361918019314  Val loss: -0.0426072861320784
Epoch 326
-------------------------------
Batch 1/64 loss: -0.20644330978393555
Batch 2/64 loss: -0.19643265008926392
Batch 3/64 loss: -0.14930719137191772
Batch 4/64 loss: -0.18317067623138428
Batch 5/64 loss: -0.20286142826080322
Batch 6/64 loss: -0.1898471713066101
Batch 7/64 loss: -0.18165451288223267
Batch 8/64 loss: -0.16604429483413696
Batch 9/64 loss: -0.19254273176193237
Batch 10/64 loss: -0.1845771074295044
Batch 11/64 loss: -0.18569660186767578
Batch 12/64 loss: -0.1859966516494751
Batch 13/64 loss: -0.18094688653945923
Batch 14/64 loss: -0.17953795194625854
Batch 15/64 loss: -0.20701569318771362
Batch 16/64 loss: -0.19528865814208984
Batch 17/64 loss: -0.1919649839401245
Batch 18/64 loss: -0.17807239294052124
Batch 19/64 loss: -0.20267480611801147
Batch 20/64 loss: -0.20009195804595947
Batch 21/64 loss: -0.17645573616027832
Batch 22/64 loss: -0.1735813021659851
Batch 23/64 loss: -0.17815369367599487
Batch 24/64 loss: -0.17799735069274902
Batch 25/64 loss: -0.20073896646499634
Batch 26/64 loss: -0.1898629069328308
Batch 27/64 loss: -0.17998838424682617
Batch 28/64 loss: -0.1786339282989502
Batch 29/64 loss: -0.1860995888710022
Batch 30/64 loss: -0.17589819431304932
Batch 31/64 loss: -0.18306875228881836
Batch 32/64 loss: -0.16915202140808105
Batch 33/64 loss: -0.19239306449890137
Batch 34/64 loss: -0.17189264297485352
Batch 35/64 loss: -0.1784989833831787
Batch 36/64 loss: -0.19067907333374023
Batch 37/64 loss: -0.17656266689300537
Batch 38/64 loss: -0.19831013679504395
Batch 39/64 loss: -0.19658660888671875
Batch 40/64 loss: -0.18139350414276123
Batch 41/64 loss: -0.19209003448486328
Batch 42/64 loss: -0.1690616011619568
Batch 43/64 loss: -0.18259793519973755
Batch 44/64 loss: -0.1876721978187561
Batch 45/64 loss: -0.19392269849777222
Batch 46/64 loss: -0.17510056495666504
Batch 47/64 loss: -0.1706753373146057
Batch 48/64 loss: -0.18592846393585205
Batch 49/64 loss: -0.19065022468566895
Batch 50/64 loss: -0.18155419826507568
Batch 51/64 loss: -0.19811058044433594
Batch 52/64 loss: -0.18218791484832764
Batch 53/64 loss: -0.18846863508224487
Batch 54/64 loss: -0.182303786277771
Batch 55/64 loss: -0.19517391920089722
Batch 56/64 loss: -0.18205136060714722
Batch 57/64 loss: -0.15137523412704468
Batch 58/64 loss: -0.14833712577819824
Batch 59/64 loss: -0.17839813232421875
Batch 60/64 loss: -0.19527000188827515
Batch 61/64 loss: -0.16218310594558716
Batch 62/64 loss: -0.15381789207458496
Batch 63/64 loss: -0.187760591506958
Batch 64/64 loss: -0.15762114524841309
Epoch 326  Train loss: -0.18304349113913143  Val loss: -0.04175296026406829
Epoch 327
-------------------------------
Batch 1/64 loss: -0.14450687170028687
Batch 2/64 loss: -0.18336182832717896
Batch 3/64 loss: -0.19931119680404663
Batch 4/64 loss: -0.19228696823120117
Batch 5/64 loss: -0.15983223915100098
Batch 6/64 loss: -0.18891417980194092
Batch 7/64 loss: -0.16342300176620483
Batch 8/64 loss: -0.15639829635620117
Batch 9/64 loss: -0.14915961027145386
Batch 10/64 loss: -0.15708768367767334
Batch 11/64 loss: -0.17671430110931396
Batch 12/64 loss: -0.1802833080291748
Batch 13/64 loss: -0.19294267892837524
Batch 14/64 loss: -0.18363219499588013
Batch 15/64 loss: -0.17195916175842285
Batch 16/64 loss: -0.1692957878112793
Batch 17/64 loss: -0.18994957208633423
Batch 18/64 loss: -0.1966310739517212
Batch 19/64 loss: -0.2026209831237793
Batch 20/64 loss: -0.18296468257904053
Batch 21/64 loss: -0.1659572720527649
Batch 22/64 loss: -0.1846693754196167
Batch 23/64 loss: -0.19557422399520874
Batch 24/64 loss: -0.20216220617294312
Batch 25/64 loss: -0.17144405841827393
Batch 26/64 loss: -0.19281655550003052
Batch 27/64 loss: -0.17289334535598755
Batch 28/64 loss: -0.15991449356079102
Batch 29/64 loss: -0.19395148754119873
Batch 30/64 loss: -0.2007858157157898
Batch 31/64 loss: -0.16377007961273193
Batch 32/64 loss: -0.1671595573425293
Batch 33/64 loss: -0.1751294732093811
Batch 34/64 loss: -0.1813635230064392
Batch 35/64 loss: -0.17475581169128418
Batch 36/64 loss: -0.20195770263671875
Batch 37/64 loss: -0.17164725065231323
Batch 38/64 loss: -0.15869462490081787
Batch 39/64 loss: -0.1937999725341797
Batch 40/64 loss: -0.18307018280029297
Batch 41/64 loss: -0.1667041778564453
Batch 42/64 loss: -0.18264025449752808
Batch 43/64 loss: -0.18549394607543945
Batch 44/64 loss: -0.19582593441009521
Batch 45/64 loss: -0.1564984917640686
Batch 46/64 loss: -0.1951797604560852
Batch 47/64 loss: -0.15869557857513428
Batch 48/64 loss: -0.17721056938171387
Batch 49/64 loss: -0.1830851435661316
Batch 50/64 loss: -0.19791477918624878
Batch 51/64 loss: -0.19096064567565918
Batch 52/64 loss: -0.16647577285766602
Batch 53/64 loss: -0.1883082389831543
Batch 54/64 loss: -0.20995551347732544
Batch 55/64 loss: -0.1512535810470581
Batch 56/64 loss: -0.17893755435943604
Batch 57/64 loss: -0.20454317331314087
Batch 58/64 loss: -0.1970272660255432
Batch 59/64 loss: -0.1800994873046875
Batch 60/64 loss: -0.18344146013259888
Batch 61/64 loss: -0.1646742820739746
Batch 62/64 loss: -0.18807190656661987
Batch 63/64 loss: -0.18478286266326904
Batch 64/64 loss: -0.19476115703582764
Epoch 327  Train loss: -0.18018264910754037  Val loss: -0.04324746316241235
Epoch 328
-------------------------------
Batch 1/64 loss: -0.21681761741638184
Batch 2/64 loss: -0.185461163520813
Batch 3/64 loss: -0.19320017099380493
Batch 4/64 loss: -0.18605726957321167
Batch 5/64 loss: -0.1678558588027954
Batch 6/64 loss: -0.15519213676452637
Batch 7/64 loss: -0.17416155338287354
Batch 8/64 loss: -0.18509000539779663
Batch 9/64 loss: -0.2028181552886963
Batch 10/64 loss: -0.19538313150405884
Batch 11/64 loss: -0.21644335985183716
Batch 12/64 loss: -0.19466769695281982
Batch 13/64 loss: -0.18764960765838623
Batch 14/64 loss: -0.18283814191818237
Batch 15/64 loss: -0.16020315885543823
Batch 16/64 loss: -0.1763935685157776
Batch 17/64 loss: -0.19849801063537598
Batch 18/64 loss: -0.20464837551116943
Batch 19/64 loss: -0.18501365184783936
Batch 20/64 loss: -0.18968069553375244
Batch 21/64 loss: -0.19350415468215942
Batch 22/64 loss: -0.1999509334564209
Batch 23/64 loss: -0.1767503023147583
Batch 24/64 loss: -0.2024332880973816
Batch 25/64 loss: -0.16999709606170654
Batch 26/64 loss: -0.1798374056816101
Batch 27/64 loss: -0.1803533434867859
Batch 28/64 loss: -0.18947696685791016
Batch 29/64 loss: -0.18976140022277832
Batch 30/64 loss: -0.2046758532524109
Batch 31/64 loss: -0.17851752042770386
Batch 32/64 loss: -0.19213765859603882
Batch 33/64 loss: -0.1728706955909729
Batch 34/64 loss: -0.1826155185699463
Batch 35/64 loss: -0.1919771432876587
Batch 36/64 loss: -0.2019280195236206
Batch 37/64 loss: -0.19670629501342773
Batch 38/64 loss: -0.1998090147972107
Batch 39/64 loss: -0.19066554307937622
Batch 40/64 loss: -0.15448147058486938
Batch 41/64 loss: -0.20170122385025024
Batch 42/64 loss: -0.1750396490097046
Batch 43/64 loss: -0.17761462926864624
Batch 44/64 loss: -0.18369168043136597
Batch 45/64 loss: -0.19083893299102783
Batch 46/64 loss: -0.1519426703453064
Batch 47/64 loss: -0.17770355939865112
Batch 48/64 loss: -0.18094706535339355
Batch 49/64 loss: -0.18804609775543213
Batch 50/64 loss: -0.193628191947937
Batch 51/64 loss: -0.17662501335144043
Batch 52/64 loss: -0.1882501244544983
Batch 53/64 loss: -0.20206153392791748
Batch 54/64 loss: -0.17352867126464844
Batch 55/64 loss: -0.19123739004135132
Batch 56/64 loss: -0.17745602130889893
Batch 57/64 loss: -0.1829371452331543
Batch 58/64 loss: -0.21536576747894287
Batch 59/64 loss: -0.17800617218017578
Batch 60/64 loss: -0.18566668033599854
Batch 61/64 loss: -0.19304001331329346
Batch 62/64 loss: -0.1971578598022461
Batch 63/64 loss: -0.1653803586959839
Batch 64/64 loss: -0.19049179553985596
Epoch 328  Train loss: -0.1866236588534187  Val loss: -0.044314244973290826
Epoch 329
-------------------------------
Batch 1/64 loss: -0.1748957633972168
Batch 2/64 loss: -0.18992924690246582
Batch 3/64 loss: -0.19241923093795776
Batch 4/64 loss: -0.17107617855072021
Batch 5/64 loss: -0.18902909755706787
Batch 6/64 loss: -0.19407367706298828
Batch 7/64 loss: -0.18698692321777344
Batch 8/64 loss: -0.20478510856628418
Batch 9/64 loss: -0.2031458616256714
Batch 10/64 loss: -0.18876856565475464
Batch 11/64 loss: -0.16817033290863037
Batch 12/64 loss: -0.18261581659317017
Batch 13/64 loss: -0.19271689653396606
Batch 14/64 loss: -0.17559564113616943
Batch 15/64 loss: -0.18449044227600098
Batch 16/64 loss: -0.1645965576171875
Batch 17/64 loss: -0.18418198823928833
Batch 18/64 loss: -0.19141453504562378
Batch 19/64 loss: -0.17809563875198364
Batch 20/64 loss: -0.18946123123168945
Batch 21/64 loss: -0.20013153553009033
Batch 22/64 loss: -0.18675684928894043
Batch 23/64 loss: -0.1742594838142395
Batch 24/64 loss: -0.1976521611213684
Batch 25/64 loss: -0.15292149782180786
Batch 26/64 loss: -0.1930783987045288
Batch 27/64 loss: -0.20019769668579102
Batch 28/64 loss: -0.21059656143188477
Batch 29/64 loss: -0.1884104609489441
Batch 30/64 loss: -0.1651984453201294
Batch 31/64 loss: -0.1729729175567627
Batch 32/64 loss: -0.20211577415466309
Batch 33/64 loss: -0.16140347719192505
Batch 34/64 loss: -0.19340872764587402
Batch 35/64 loss: -0.20194047689437866
Batch 36/64 loss: -0.20709240436553955
Batch 37/64 loss: -0.16124475002288818
Batch 38/64 loss: -0.17462646961212158
Batch 39/64 loss: -0.18438869714736938
Batch 40/64 loss: -0.1632920503616333
Batch 41/64 loss: -0.2030245065689087
Batch 42/64 loss: -0.19359934329986572
Batch 43/64 loss: -0.1756047010421753
Batch 44/64 loss: -0.1801406741142273
Batch 45/64 loss: -0.197329580783844
Batch 46/64 loss: -0.20954293012619019
Batch 47/64 loss: -0.19869786500930786
Batch 48/64 loss: -0.14607471227645874
Batch 49/64 loss: -0.18562668561935425
Batch 50/64 loss: -0.19759416580200195
Batch 51/64 loss: -0.19020956754684448
Batch 52/64 loss: -0.20763427019119263
Batch 53/64 loss: -0.1952418088912964
Batch 54/64 loss: -0.17633450031280518
Batch 55/64 loss: -0.20692455768585205
Batch 56/64 loss: -0.2029288411140442
Batch 57/64 loss: -0.18894410133361816
Batch 58/64 loss: -0.18363183736801147
Batch 59/64 loss: -0.19262391328811646
Batch 60/64 loss: -0.20234692096710205
Batch 61/64 loss: -0.19283312559127808
Batch 62/64 loss: -0.20638656616210938
Batch 63/64 loss: -0.1739758849143982
Batch 64/64 loss: -0.19578874111175537
Epoch 329  Train loss: -0.18748596369051465  Val loss: -0.043000236819290216
Epoch 330
-------------------------------
Batch 1/64 loss: -0.18547606468200684
Batch 2/64 loss: -0.21252548694610596
Batch 3/64 loss: -0.18378591537475586
Batch 4/64 loss: -0.20508623123168945
Batch 5/64 loss: -0.1758163571357727
Batch 6/64 loss: -0.21394497156143188
Batch 7/64 loss: -0.18870508670806885
Batch 8/64 loss: -0.1893472671508789
Batch 9/64 loss: -0.15251904726028442
Batch 10/64 loss: -0.19793665409088135
Batch 11/64 loss: -0.19223302602767944
Batch 12/64 loss: -0.18042659759521484
Batch 13/64 loss: -0.1876315474510193
Batch 14/64 loss: -0.18772244453430176
Batch 15/64 loss: -0.1609407663345337
Batch 16/64 loss: -0.2079097032546997
Batch 17/64 loss: -0.19429099559783936
Batch 18/64 loss: -0.19180625677108765
Batch 19/64 loss: -0.19569003582000732
Batch 20/64 loss: -0.18816083669662476
Batch 21/64 loss: -0.18233001232147217
Batch 22/64 loss: -0.1767352819442749
Batch 23/64 loss: -0.1481197476387024
Batch 24/64 loss: -0.17923617362976074
Batch 25/64 loss: -0.1651543378829956
Batch 26/64 loss: -0.17727410793304443
Batch 27/64 loss: -0.18230551481246948
Batch 28/64 loss: -0.17526978254318237
Batch 29/64 loss: -0.18852335214614868
Batch 30/64 loss: -0.1874324083328247
Batch 31/64 loss: -0.17869359254837036
Batch 32/64 loss: -0.20868563652038574
Batch 33/64 loss: -0.18575811386108398
Batch 34/64 loss: -0.17957782745361328
Batch 35/64 loss: -0.2060146927833557
Batch 36/64 loss: -0.1817479133605957
Batch 37/64 loss: -0.18351984024047852
Batch 38/64 loss: -0.16894638538360596
Batch 39/64 loss: -0.17005586624145508
Batch 40/64 loss: -0.1877126693725586
Batch 41/64 loss: -0.17935574054718018
Batch 42/64 loss: -0.18739140033721924
Batch 43/64 loss: -0.20831209421157837
Batch 44/64 loss: -0.1946670413017273
Batch 45/64 loss: -0.19753926992416382
Batch 46/64 loss: -0.19028782844543457
Batch 47/64 loss: -0.1801416277885437
Batch 48/64 loss: -0.18650680780410767
Batch 49/64 loss: -0.18742257356643677
Batch 50/64 loss: -0.1873524785041809
Batch 51/64 loss: -0.17503637075424194
Batch 52/64 loss: -0.18650424480438232
Batch 53/64 loss: -0.1672992706298828
Batch 54/64 loss: -0.19041472673416138
Batch 55/64 loss: -0.16903531551361084
Batch 56/64 loss: -0.2024986743927002
Batch 57/64 loss: -0.19793754816055298
Batch 58/64 loss: -0.1969597339630127
Batch 59/64 loss: -0.17691797018051147
Batch 60/64 loss: -0.19764214754104614
Batch 61/64 loss: -0.1905057430267334
Batch 62/64 loss: -0.17627662420272827
Batch 63/64 loss: -0.204362690448761
Batch 64/64 loss: -0.18008017539978027
Epoch 330  Train loss: -0.18620355456483131  Val loss: -0.04455170414292116
Epoch 331
-------------------------------
Batch 1/64 loss: -0.21099889278411865
Batch 2/64 loss: -0.23000246286392212
Batch 3/64 loss: -0.19751381874084473
Batch 4/64 loss: -0.19367295503616333
Batch 5/64 loss: -0.16446822881698608
Batch 6/64 loss: -0.19622117280960083
Batch 7/64 loss: -0.2018810510635376
Batch 8/64 loss: -0.18507224321365356
Batch 9/64 loss: -0.1756269931793213
Batch 10/64 loss: -0.19914543628692627
Batch 11/64 loss: -0.17366927862167358
Batch 12/64 loss: -0.18339276313781738
Batch 13/64 loss: -0.2030407190322876
Batch 14/64 loss: -0.18509751558303833
Batch 15/64 loss: -0.18456727266311646
Batch 16/64 loss: -0.17263364791870117
Batch 17/64 loss: -0.2074032425880432
Batch 18/64 loss: -0.20556974411010742
Batch 19/64 loss: -0.1796858310699463
Batch 20/64 loss: -0.18188339471817017
Batch 21/64 loss: -0.1886749267578125
Batch 22/64 loss: -0.17121291160583496
Batch 23/64 loss: -0.17858213186264038
Batch 24/64 loss: -0.17827844619750977
Batch 25/64 loss: -0.19347155094146729
Batch 26/64 loss: -0.18305176496505737
Batch 27/64 loss: -0.1770399808883667
Batch 28/64 loss: -0.1943904161453247
Batch 29/64 loss: -0.1830216646194458
Batch 30/64 loss: -0.19611340761184692
Batch 31/64 loss: -0.1859877109527588
Batch 32/64 loss: -0.18278223276138306
Batch 33/64 loss: -0.19763445854187012
Batch 34/64 loss: -0.18505114316940308
Batch 35/64 loss: -0.1839037537574768
Batch 36/64 loss: -0.1991347074508667
Batch 37/64 loss: -0.1863795518875122
Batch 38/64 loss: -0.18558382987976074
Batch 39/64 loss: -0.19590407609939575
Batch 40/64 loss: -0.1673184633255005
Batch 41/64 loss: -0.19750815629959106
Batch 42/64 loss: -0.1732845902442932
Batch 43/64 loss: -0.1781640648841858
Batch 44/64 loss: -0.20255553722381592
Batch 45/64 loss: -0.20071321725845337
Batch 46/64 loss: -0.18923020362854004
Batch 47/64 loss: -0.20567065477371216
Batch 48/64 loss: -0.1849348545074463
Batch 49/64 loss: -0.18034666776657104
Batch 50/64 loss: -0.1808043122291565
Batch 51/64 loss: -0.1467006802558899
Batch 52/64 loss: -0.19808292388916016
Batch 53/64 loss: -0.18192481994628906
Batch 54/64 loss: -0.1989574432373047
Batch 55/64 loss: -0.20618540048599243
Batch 56/64 loss: -0.16762644052505493
Batch 57/64 loss: -0.16116976737976074
Batch 58/64 loss: -0.152784526348114
Batch 59/64 loss: -0.18610167503356934
Batch 60/64 loss: -0.17313623428344727
Batch 61/64 loss: -0.1658356785774231
Batch 62/64 loss: -0.18433260917663574
Batch 63/64 loss: -0.16969382762908936
Batch 64/64 loss: -0.1749793291091919
Epoch 331  Train loss: -0.18607125609528785  Val loss: -0.04124235492391685
Epoch 332
-------------------------------
Batch 1/64 loss: -0.18340635299682617
Batch 2/64 loss: -0.181779146194458
Batch 3/64 loss: -0.18868261575698853
Batch 4/64 loss: -0.19681060314178467
Batch 5/64 loss: -0.18950200080871582
Batch 6/64 loss: -0.16790789365768433
Batch 7/64 loss: -0.13974666595458984
Batch 8/64 loss: -0.1860182285308838
Batch 9/64 loss: -0.19197940826416016
Batch 10/64 loss: -0.17219024896621704
Batch 11/64 loss: -0.1898459792137146
Batch 12/64 loss: -0.17829179763793945
Batch 13/64 loss: -0.1886877417564392
Batch 14/64 loss: -0.1993427276611328
Batch 15/64 loss: -0.20561879873275757
Batch 16/64 loss: -0.19089794158935547
Batch 17/64 loss: -0.16777461767196655
Batch 18/64 loss: -0.20916879177093506
Batch 19/64 loss: -0.20761841535568237
Batch 20/64 loss: -0.19609016180038452
Batch 21/64 loss: -0.20358479022979736
Batch 22/64 loss: -0.18574798107147217
Batch 23/64 loss: -0.18362516164779663
Batch 24/64 loss: -0.17972540855407715
Batch 25/64 loss: -0.19606763124465942
Batch 26/64 loss: -0.19141513109207153
Batch 27/64 loss: -0.18297749757766724
Batch 28/64 loss: -0.18869447708129883
Batch 29/64 loss: -0.1810048222541809
Batch 30/64 loss: -0.19819355010986328
Batch 31/64 loss: -0.15685617923736572
Batch 32/64 loss: -0.1909010410308838
Batch 33/64 loss: -0.19228780269622803
Batch 34/64 loss: -0.21178221702575684
Batch 35/64 loss: -0.20165681838989258
Batch 36/64 loss: -0.20372945070266724
Batch 37/64 loss: -0.19507044553756714
Batch 38/64 loss: -0.18097513914108276
Batch 39/64 loss: -0.18582266569137573
Batch 40/64 loss: -0.20858341455459595
Batch 41/64 loss: -0.16958856582641602
Batch 42/64 loss: -0.16901803016662598
Batch 43/64 loss: -0.17029094696044922
Batch 44/64 loss: -0.18557381629943848
Batch 45/64 loss: -0.17561793327331543
Batch 46/64 loss: -0.1949637532234192
Batch 47/64 loss: -0.17722564935684204
Batch 48/64 loss: -0.20236730575561523
Batch 49/64 loss: -0.1837989091873169
Batch 50/64 loss: -0.20184224843978882
Batch 51/64 loss: -0.18188464641571045
Batch 52/64 loss: -0.2127143144607544
Batch 53/64 loss: -0.16553860902786255
Batch 54/64 loss: -0.16507011651992798
Batch 55/64 loss: -0.1665489673614502
Batch 56/64 loss: -0.17708534002304077
Batch 57/64 loss: -0.15040737390518188
Batch 58/64 loss: -0.18036866188049316
Batch 59/64 loss: -0.19431674480438232
Batch 60/64 loss: -0.1858087182044983
Batch 61/64 loss: -0.17375940084457397
Batch 62/64 loss: -0.18714404106140137
Batch 63/64 loss: -0.19109445810317993
Batch 64/64 loss: -0.18605095148086548
Epoch 332  Train loss: -0.1859078984634549  Val loss: -0.04482655385925188
Epoch 333
-------------------------------
Batch 1/64 loss: -0.189797043800354
Batch 2/64 loss: -0.2073614001274109
Batch 3/64 loss: -0.17450803518295288
Batch 4/64 loss: -0.19876813888549805
Batch 5/64 loss: -0.18661630153656006
Batch 6/64 loss: -0.19930493831634521
Batch 7/64 loss: -0.18258553743362427
Batch 8/64 loss: -0.1703106164932251
Batch 9/64 loss: -0.1587333083152771
Batch 10/64 loss: -0.1969873309135437
Batch 11/64 loss: -0.20537424087524414
Batch 12/64 loss: -0.2060953974723816
Batch 13/64 loss: -0.1992998719215393
Batch 14/64 loss: -0.16980981826782227
Batch 15/64 loss: -0.21086567640304565
Batch 16/64 loss: -0.20323342084884644
Batch 17/64 loss: -0.19904839992523193
Batch 18/64 loss: -0.1913124918937683
Batch 19/64 loss: -0.21306633949279785
Batch 20/64 loss: -0.17414045333862305
Batch 21/64 loss: -0.1896345019340515
Batch 22/64 loss: -0.19656777381896973
Batch 23/64 loss: -0.1942865252494812
Batch 24/64 loss: -0.19651073217391968
Batch 25/64 loss: -0.19877249002456665
Batch 26/64 loss: -0.18798613548278809
Batch 27/64 loss: -0.17795562744140625
Batch 28/64 loss: -0.1740093231201172
Batch 29/64 loss: -0.191331684589386
Batch 30/64 loss: -0.18743538856506348
Batch 31/64 loss: -0.1944827437400818
Batch 32/64 loss: -0.1856977939605713
Batch 33/64 loss: -0.1742182970046997
Batch 34/64 loss: -0.1841551661491394
Batch 35/64 loss: -0.213178813457489
Batch 36/64 loss: -0.20885753631591797
Batch 37/64 loss: -0.20770257711410522
Batch 38/64 loss: -0.1820082664489746
Batch 39/64 loss: -0.15998423099517822
Batch 40/64 loss: -0.20010441541671753
Batch 41/64 loss: -0.19173622131347656
Batch 42/64 loss: -0.1653950810432434
Batch 43/64 loss: -0.1732437014579773
Batch 44/64 loss: -0.18868350982666016
Batch 45/64 loss: -0.14752954244613647
Batch 46/64 loss: -0.18423324823379517
Batch 47/64 loss: -0.20205360651016235
Batch 48/64 loss: -0.2063581943511963
Batch 49/64 loss: -0.17467814683914185
Batch 50/64 loss: -0.19404596090316772
Batch 51/64 loss: -0.17382001876831055
Batch 52/64 loss: -0.1695617437362671
Batch 53/64 loss: -0.19247537851333618
Batch 54/64 loss: -0.18953877687454224
Batch 55/64 loss: -0.19743406772613525
Batch 56/64 loss: -0.18495196104049683
Batch 57/64 loss: -0.19620460271835327
Batch 58/64 loss: -0.19801104068756104
Batch 59/64 loss: -0.19602465629577637
Batch 60/64 loss: -0.1819387674331665
Batch 61/64 loss: -0.17146068811416626
Batch 62/64 loss: -0.1884084939956665
Batch 63/64 loss: -0.19352304935455322
Batch 64/64 loss: -0.1862626075744629
Epoch 333  Train loss: -0.18891146977742512  Val loss: -0.04244453964364488
Epoch 334
-------------------------------
Batch 1/64 loss: -0.19067293405532837
Batch 2/64 loss: -0.1826794147491455
Batch 3/64 loss: -0.17115402221679688
Batch 4/64 loss: -0.18775945901870728
Batch 5/64 loss: -0.20253348350524902
Batch 6/64 loss: -0.18222403526306152
Batch 7/64 loss: -0.1895838975906372
Batch 8/64 loss: -0.20275264978408813
Batch 9/64 loss: -0.16407227516174316
Batch 10/64 loss: -0.18511873483657837
Batch 11/64 loss: -0.19333869218826294
Batch 12/64 loss: -0.19750583171844482
Batch 13/64 loss: -0.1977406144142151
Batch 14/64 loss: -0.19244718551635742
Batch 15/64 loss: -0.19654643535614014
Batch 16/64 loss: -0.1975582242012024
Batch 17/64 loss: -0.17388761043548584
Batch 18/64 loss: -0.1923990249633789
Batch 19/64 loss: -0.2171492576599121
Batch 20/64 loss: -0.1637173891067505
Batch 21/64 loss: -0.17838776111602783
Batch 22/64 loss: -0.19215726852416992
Batch 23/64 loss: -0.195584237575531
Batch 24/64 loss: -0.20045268535614014
Batch 25/64 loss: -0.20301008224487305
Batch 26/64 loss: -0.21064478158950806
Batch 27/64 loss: -0.1782323122024536
Batch 28/64 loss: -0.174757719039917
Batch 29/64 loss: -0.20816195011138916
Batch 30/64 loss: -0.19450414180755615
Batch 31/64 loss: -0.22067874670028687
Batch 32/64 loss: -0.19344395399093628
Batch 33/64 loss: -0.19955116510391235
Batch 34/64 loss: -0.2023037075996399
Batch 35/64 loss: -0.17706423997879028
Batch 36/64 loss: -0.18545764684677124
Batch 37/64 loss: -0.16360598802566528
Batch 38/64 loss: -0.17682790756225586
Batch 39/64 loss: -0.19067269563674927
Batch 40/64 loss: -0.1912097930908203
Batch 41/64 loss: -0.13392794132232666
Batch 42/64 loss: -0.17089509963989258
Batch 43/64 loss: -0.17664527893066406
Batch 44/64 loss: -0.1832515001296997
Batch 45/64 loss: -0.17488527297973633
Batch 46/64 loss: -0.18171805143356323
Batch 47/64 loss: -0.18383216857910156
Batch 48/64 loss: -0.17054373025894165
Batch 49/64 loss: -0.19784408807754517
Batch 50/64 loss: -0.18627160787582397
Batch 51/64 loss: -0.16589081287384033
Batch 52/64 loss: -0.18033427000045776
Batch 53/64 loss: -0.19925421476364136
Batch 54/64 loss: -0.19078302383422852
Batch 55/64 loss: -0.18921732902526855
Batch 56/64 loss: -0.19570457935333252
Batch 57/64 loss: -0.1828010082244873
Batch 58/64 loss: -0.16549980640411377
Batch 59/64 loss: -0.1812223196029663
Batch 60/64 loss: -0.1734679937362671
Batch 61/64 loss: -0.17250335216522217
Batch 62/64 loss: -0.18309205770492554
Batch 63/64 loss: -0.17382484674453735
Batch 64/64 loss: -0.1677526831626892
Epoch 334  Train loss: -0.18598862465690164  Val loss: -0.04301007895944864
Epoch 335
-------------------------------
Batch 1/64 loss: -0.19926190376281738
Batch 2/64 loss: -0.1973438262939453
Batch 3/64 loss: -0.20020830631256104
Batch 4/64 loss: -0.18652218580245972
Batch 5/64 loss: -0.19755685329437256
Batch 6/64 loss: -0.18860644102096558
Batch 7/64 loss: -0.18158739805221558
Batch 8/64 loss: -0.1875464916229248
Batch 9/64 loss: -0.1980043649673462
Batch 10/64 loss: -0.19520998001098633
Batch 11/64 loss: -0.209564208984375
Batch 12/64 loss: -0.1878710389137268
Batch 13/64 loss: -0.20390665531158447
Batch 14/64 loss: -0.17580914497375488
Batch 15/64 loss: -0.19810611009597778
Batch 16/64 loss: -0.19701719284057617
Batch 17/64 loss: -0.21247756481170654
Batch 18/64 loss: -0.20090115070343018
Batch 19/64 loss: -0.1708540916442871
Batch 20/64 loss: -0.18686330318450928
Batch 21/64 loss: -0.19201231002807617
Batch 22/64 loss: -0.18820065259933472
Batch 23/64 loss: -0.20272290706634521
Batch 24/64 loss: -0.18552470207214355
Batch 25/64 loss: -0.19325518608093262
Batch 26/64 loss: -0.2023627758026123
Batch 27/64 loss: -0.19543564319610596
Batch 28/64 loss: -0.20223265886306763
Batch 29/64 loss: -0.1989162564277649
Batch 30/64 loss: -0.1638379693031311
Batch 31/64 loss: -0.20400482416152954
Batch 32/64 loss: -0.18015289306640625
Batch 33/64 loss: -0.2096443772315979
Batch 34/64 loss: -0.18040794134140015
Batch 35/64 loss: -0.19899433851242065
Batch 36/64 loss: -0.19515150785446167
Batch 37/64 loss: -0.18409156799316406
Batch 38/64 loss: -0.19048380851745605
Batch 39/64 loss: -0.17263877391815186
Batch 40/64 loss: -0.1897411346435547
Batch 41/64 loss: -0.18776220083236694
Batch 42/64 loss: -0.18767249584197998
Batch 43/64 loss: -0.16426444053649902
Batch 44/64 loss: -0.20203977823257446
Batch 45/64 loss: -0.19709992408752441
Batch 46/64 loss: -0.20153892040252686
Batch 47/64 loss: -0.18284940719604492
Batch 48/64 loss: -0.1904982328414917
Batch 49/64 loss: -0.19450616836547852
Batch 50/64 loss: -0.20040792226791382
Batch 51/64 loss: -0.19555777311325073
Batch 52/64 loss: -0.19270074367523193
Batch 53/64 loss: -0.16907018423080444
Batch 54/64 loss: -0.19200199842453003
Batch 55/64 loss: -0.1904277205467224
Batch 56/64 loss: -0.19042432308197021
Batch 57/64 loss: -0.17404377460479736
Batch 58/64 loss: -0.199482262134552
Batch 59/64 loss: -0.17003881931304932
Batch 60/64 loss: -0.16705107688903809
Batch 61/64 loss: -0.188254714012146
Batch 62/64 loss: -0.1709955930709839
Batch 63/64 loss: -0.16387593746185303
Batch 64/64 loss: -0.16817235946655273
Epoch 335  Train loss: -0.18986233916937137  Val loss: -0.039985849890102634
Epoch 336
-------------------------------
Batch 1/64 loss: -0.1986255645751953
Batch 2/64 loss: -0.19554811716079712
Batch 3/64 loss: -0.1606447696685791
Batch 4/64 loss: -0.20274460315704346
Batch 5/64 loss: -0.18679147958755493
Batch 6/64 loss: -0.1683446764945984
Batch 7/64 loss: -0.20011305809020996
Batch 8/64 loss: -0.1848663091659546
Batch 9/64 loss: -0.1743573546409607
Batch 10/64 loss: -0.18495798110961914
Batch 11/64 loss: -0.20748043060302734
Batch 12/64 loss: -0.19481736421585083
Batch 13/64 loss: -0.20802289247512817
Batch 14/64 loss: -0.18652641773223877
Batch 15/64 loss: -0.18617713451385498
Batch 16/64 loss: -0.20169514417648315
Batch 17/64 loss: -0.1972799301147461
Batch 18/64 loss: -0.17889320850372314
Batch 19/64 loss: -0.1728278398513794
Batch 20/64 loss: -0.1878809928894043
Batch 21/64 loss: -0.1964513063430786
Batch 22/64 loss: -0.19461899995803833
Batch 23/64 loss: -0.20024460554122925
Batch 24/64 loss: -0.18663114309310913
Batch 25/64 loss: -0.20787394046783447
Batch 26/64 loss: -0.21355170011520386
Batch 27/64 loss: -0.19713717699050903
Batch 28/64 loss: -0.20062196254730225
Batch 29/64 loss: -0.19250434637069702
Batch 30/64 loss: -0.1967267394065857
Batch 31/64 loss: -0.19768363237380981
Batch 32/64 loss: -0.18904560804367065
Batch 33/64 loss: -0.18293797969818115
Batch 34/64 loss: -0.1604650616645813
Batch 35/64 loss: -0.1939769983291626
Batch 36/64 loss: -0.1870693564414978
Batch 37/64 loss: -0.18293553590774536
Batch 38/64 loss: -0.1909712553024292
Batch 39/64 loss: -0.17341232299804688
Batch 40/64 loss: -0.20861774682998657
Batch 41/64 loss: -0.18316245079040527
Batch 42/64 loss: -0.1864680051803589
Batch 43/64 loss: -0.1989111304283142
Batch 44/64 loss: -0.19022154808044434
Batch 45/64 loss: -0.1965773105621338
Batch 46/64 loss: -0.17677342891693115
Batch 47/64 loss: -0.20983421802520752
Batch 48/64 loss: -0.1990613341331482
Batch 49/64 loss: -0.18675631284713745
Batch 50/64 loss: -0.21012431383132935
Batch 51/64 loss: -0.18791407346725464
Batch 52/64 loss: -0.1836567521095276
Batch 53/64 loss: -0.20972537994384766
Batch 54/64 loss: -0.1967325210571289
Batch 55/64 loss: -0.1699402928352356
Batch 56/64 loss: -0.2164098620414734
Batch 57/64 loss: -0.17090266942977905
Batch 58/64 loss: -0.18715685606002808
Batch 59/64 loss: -0.196044921875
Batch 60/64 loss: -0.18499594926834106
Batch 61/64 loss: -0.18485677242279053
Batch 62/64 loss: -0.18929702043533325
Batch 63/64 loss: -0.19699549674987793
Batch 64/64 loss: -0.17024993896484375
Epoch 336  Train loss: -0.19093770606845034  Val loss: -0.042586801388009715
Epoch 337
-------------------------------
Batch 1/64 loss: -0.2134796380996704
Batch 2/64 loss: -0.18655723333358765
Batch 3/64 loss: -0.20170879364013672
Batch 4/64 loss: -0.17366951704025269
Batch 5/64 loss: -0.17196398973464966
Batch 6/64 loss: -0.19604212045669556
Batch 7/64 loss: -0.18083065748214722
Batch 8/64 loss: -0.1930045485496521
Batch 9/64 loss: -0.1960100531578064
Batch 10/64 loss: -0.19175541400909424
Batch 11/64 loss: -0.19126015901565552
Batch 12/64 loss: -0.17318391799926758
Batch 13/64 loss: -0.19285070896148682
Batch 14/64 loss: -0.20075279474258423
Batch 15/64 loss: -0.19652432203292847
Batch 16/64 loss: -0.18667864799499512
Batch 17/64 loss: -0.19378799200057983
Batch 18/64 loss: -0.19623368978500366
Batch 19/64 loss: -0.19486933946609497
Batch 20/64 loss: -0.1672978401184082
Batch 21/64 loss: -0.19895213842391968
Batch 22/64 loss: -0.2049967646598816
Batch 23/64 loss: -0.18886268138885498
Batch 24/64 loss: -0.20313990116119385
Batch 25/64 loss: -0.19594413042068481
Batch 26/64 loss: -0.21228349208831787
Batch 27/64 loss: -0.1887528896331787
Batch 28/64 loss: -0.21421444416046143
Batch 29/64 loss: -0.18344193696975708
Batch 30/64 loss: -0.21037310361862183
Batch 31/64 loss: -0.20216584205627441
Batch 32/64 loss: -0.19062519073486328
Batch 33/64 loss: -0.20661020278930664
Batch 34/64 loss: -0.18564176559448242
Batch 35/64 loss: -0.21909385919570923
Batch 36/64 loss: -0.18897253274917603
Batch 37/64 loss: -0.16584843397140503
Batch 38/64 loss: -0.16348999738693237
Batch 39/64 loss: -0.18416666984558105
Batch 40/64 loss: -0.16270005702972412
Batch 41/64 loss: -0.20620769262313843
Batch 42/64 loss: -0.18548017740249634
Batch 43/64 loss: -0.2026258111000061
Batch 44/64 loss: -0.16804683208465576
Batch 45/64 loss: -0.18995022773742676
Batch 46/64 loss: -0.16438639163970947
Batch 47/64 loss: -0.2004411816596985
Batch 48/64 loss: -0.182578444480896
Batch 49/64 loss: -0.20059925317764282
Batch 50/64 loss: -0.20159047842025757
Batch 51/64 loss: -0.2008904218673706
Batch 52/64 loss: -0.17319434881210327
Batch 53/64 loss: -0.18294590711593628
Batch 54/64 loss: -0.1930791139602661
Batch 55/64 loss: -0.2088223695755005
Batch 56/64 loss: -0.17011535167694092
Batch 57/64 loss: -0.15072238445281982
Batch 58/64 loss: -0.19615280628204346
Batch 59/64 loss: -0.19647258520126343
Batch 60/64 loss: -0.1975741982460022
Batch 61/64 loss: -0.1930018663406372
Batch 62/64 loss: -0.18224304914474487
Batch 63/64 loss: -0.1698591709136963
Batch 64/64 loss: -0.19057416915893555
Epoch 337  Train loss: -0.190253303565231  Val loss: -0.04430767822101763
Epoch 338
-------------------------------
Batch 1/64 loss: -0.17414218187332153
Batch 2/64 loss: -0.19022953510284424
Batch 3/64 loss: -0.19341695308685303
Batch 4/64 loss: -0.21303468942642212
Batch 5/64 loss: -0.2029607892036438
Batch 6/64 loss: -0.20353245735168457
Batch 7/64 loss: -0.16891080141067505
Batch 8/64 loss: -0.18803244829177856
Batch 9/64 loss: -0.18196415901184082
Batch 10/64 loss: -0.2054959535598755
Batch 11/64 loss: -0.20171892642974854
Batch 12/64 loss: -0.21005594730377197
Batch 13/64 loss: -0.20790362358093262
Batch 14/64 loss: -0.18625110387802124
Batch 15/64 loss: -0.18262732028961182
Batch 16/64 loss: -0.18963652849197388
Batch 17/64 loss: -0.2160434126853943
Batch 18/64 loss: -0.17415177822113037
Batch 19/64 loss: -0.20104336738586426
Batch 20/64 loss: -0.15597200393676758
Batch 21/64 loss: -0.1812722086906433
Batch 22/64 loss: -0.21365946531295776
Batch 23/64 loss: -0.19250082969665527
Batch 24/64 loss: -0.19620883464813232
Batch 25/64 loss: -0.1534045934677124
Batch 26/64 loss: -0.1875033974647522
Batch 27/64 loss: -0.1933971643447876
Batch 28/64 loss: -0.19268858432769775
Batch 29/64 loss: -0.17281806468963623
Batch 30/64 loss: -0.17665588855743408
Batch 31/64 loss: -0.19347596168518066
Batch 32/64 loss: -0.197462797164917
Batch 33/64 loss: -0.1818695068359375
Batch 34/64 loss: -0.16155850887298584
Batch 35/64 loss: -0.18830209970474243
Batch 36/64 loss: -0.21208876371383667
Batch 37/64 loss: -0.16378819942474365
Batch 38/64 loss: -0.1771460771560669
Batch 39/64 loss: -0.20574790239334106
Batch 40/64 loss: -0.18066030740737915
Batch 41/64 loss: -0.20276319980621338
Batch 42/64 loss: -0.17984867095947266
Batch 43/64 loss: -0.1886916160583496
Batch 44/64 loss: -0.20059269666671753
Batch 45/64 loss: -0.19378662109375
Batch 46/64 loss: -0.18186980485916138
Batch 47/64 loss: -0.2072455883026123
Batch 48/64 loss: -0.20837897062301636
Batch 49/64 loss: -0.19727110862731934
Batch 50/64 loss: -0.20120322704315186
Batch 51/64 loss: -0.20836007595062256
Batch 52/64 loss: -0.19699400663375854
Batch 53/64 loss: -0.19773364067077637
Batch 54/64 loss: -0.18079811334609985
Batch 55/64 loss: -0.1697901487350464
Batch 56/64 loss: -0.19051021337509155
Batch 57/64 loss: -0.17796319723129272
Batch 58/64 loss: -0.17007875442504883
Batch 59/64 loss: -0.1893177628517151
Batch 60/64 loss: -0.1871742606163025
Batch 61/64 loss: -0.19659632444381714
Batch 62/64 loss: -0.20481228828430176
Batch 63/64 loss: -0.18010002374649048
Batch 64/64 loss: -0.19973218441009521
Epoch 338  Train loss: -0.19029039354885327  Val loss: -0.038911378465567255
Epoch 339
-------------------------------
Batch 1/64 loss: -0.20754486322402954
Batch 2/64 loss: -0.1923719048500061
Batch 3/64 loss: -0.19092786312103271
Batch 4/64 loss: -0.154870867729187
Batch 5/64 loss: -0.2009015679359436
Batch 6/64 loss: -0.17587363719940186
Batch 7/64 loss: -0.1907132863998413
Batch 8/64 loss: -0.1832585334777832
Batch 9/64 loss: -0.17844194173812866
Batch 10/64 loss: -0.19320034980773926
Batch 11/64 loss: -0.2079707384109497
Batch 12/64 loss: -0.18376725912094116
Batch 13/64 loss: -0.16954636573791504
Batch 14/64 loss: -0.18622195720672607
Batch 15/64 loss: -0.20593589544296265
Batch 16/64 loss: -0.18106180429458618
Batch 17/64 loss: -0.16328448057174683
Batch 18/64 loss: -0.179482102394104
Batch 19/64 loss: -0.17195147275924683
Batch 20/64 loss: -0.178547203540802
Batch 21/64 loss: -0.20974385738372803
Batch 22/64 loss: -0.20112287998199463
Batch 23/64 loss: -0.18456023931503296
Batch 24/64 loss: -0.2000037431716919
Batch 25/64 loss: -0.204750657081604
Batch 26/64 loss: -0.2057121992111206
Batch 27/64 loss: -0.20129549503326416
Batch 28/64 loss: -0.16268640756607056
Batch 29/64 loss: -0.19423741102218628
Batch 30/64 loss: -0.18140864372253418
Batch 31/64 loss: -0.2225669026374817
Batch 32/64 loss: -0.19233852624893188
Batch 33/64 loss: -0.20597326755523682
Batch 34/64 loss: -0.20570629835128784
Batch 35/64 loss: -0.1656663417816162
Batch 36/64 loss: -0.1663268804550171
Batch 37/64 loss: -0.1704484224319458
Batch 38/64 loss: -0.17119050025939941
Batch 39/64 loss: -0.19182896614074707
Batch 40/64 loss: -0.21214628219604492
Batch 41/64 loss: -0.17948544025421143
Batch 42/64 loss: -0.18843936920166016
Batch 43/64 loss: -0.18768310546875
Batch 44/64 loss: -0.19814014434814453
Batch 45/64 loss: -0.1681116223335266
Batch 46/64 loss: -0.19671869277954102
Batch 47/64 loss: -0.19584423303604126
Batch 48/64 loss: -0.21443301439285278
Batch 49/64 loss: -0.1967402696609497
Batch 50/64 loss: -0.185238778591156
Batch 51/64 loss: -0.19345980882644653
Batch 52/64 loss: -0.18996495008468628
Batch 53/64 loss: -0.19528496265411377
Batch 54/64 loss: -0.19293755292892456
Batch 55/64 loss: -0.19104617834091187
Batch 56/64 loss: -0.1981378197669983
Batch 57/64 loss: -0.18359321355819702
Batch 58/64 loss: -0.19920611381530762
Batch 59/64 loss: -0.20582348108291626
Batch 60/64 loss: -0.20372533798217773
Batch 61/64 loss: -0.18488061428070068
Batch 62/64 loss: -0.1861613392829895
Batch 63/64 loss: -0.19345349073410034
Batch 64/64 loss: -0.18137681484222412
Epoch 339  Train loss: -0.18996282605563894  Val loss: -0.04232719407458486
Epoch 340
-------------------------------
Batch 1/64 loss: -0.2203172743320465
Batch 2/64 loss: -0.18832576274871826
Batch 3/64 loss: -0.20704740285873413
Batch 4/64 loss: -0.19657671451568604
Batch 5/64 loss: -0.17918014526367188
Batch 6/64 loss: -0.20076870918273926
Batch 7/64 loss: -0.2123408317565918
Batch 8/64 loss: -0.21580290794372559
Batch 9/64 loss: -0.19921767711639404
Batch 10/64 loss: -0.18971019983291626
Batch 11/64 loss: -0.21285855770111084
Batch 12/64 loss: -0.17456746101379395
Batch 13/64 loss: -0.20147264003753662
Batch 14/64 loss: -0.20118260383605957
Batch 15/64 loss: -0.19411081075668335
Batch 16/64 loss: -0.19931578636169434
Batch 17/64 loss: -0.20503151416778564
Batch 18/64 loss: -0.19003760814666748
Batch 19/64 loss: -0.19062691926956177
Batch 20/64 loss: -0.1975345015525818
Batch 21/64 loss: -0.2076980471611023
Batch 22/64 loss: -0.16607320308685303
Batch 23/64 loss: -0.2083776593208313
Batch 24/64 loss: -0.1997072696685791
Batch 25/64 loss: -0.175215482711792
Batch 26/64 loss: -0.19496679306030273
Batch 27/64 loss: -0.1939467191696167
Batch 28/64 loss: -0.18790370225906372
Batch 29/64 loss: -0.1919783353805542
Batch 30/64 loss: -0.2096768021583557
Batch 31/64 loss: -0.1916964054107666
Batch 32/64 loss: -0.17217737436294556
Batch 33/64 loss: -0.18616271018981934
Batch 34/64 loss: -0.17116916179656982
Batch 35/64 loss: -0.2068389654159546
Batch 36/64 loss: -0.19665497541427612
Batch 37/64 loss: -0.1870863437652588
Batch 38/64 loss: -0.19651448726654053
Batch 39/64 loss: -0.1761874556541443
Batch 40/64 loss: -0.19112569093704224
Batch 41/64 loss: -0.19773125648498535
Batch 42/64 loss: -0.1682702898979187
Batch 43/64 loss: -0.18811661005020142
Batch 44/64 loss: -0.20480167865753174
Batch 45/64 loss: -0.19725501537322998
Batch 46/64 loss: -0.2024674415588379
Batch 47/64 loss: -0.18114376068115234
Batch 48/64 loss: -0.19154399633407593
Batch 49/64 loss: -0.1962563395500183
Batch 50/64 loss: -0.18849235773086548
Batch 51/64 loss: -0.21207165718078613
Batch 52/64 loss: -0.1697467565536499
Batch 53/64 loss: -0.18279337882995605
Batch 54/64 loss: -0.19254881143569946
Batch 55/64 loss: -0.1883474588394165
Batch 56/64 loss: -0.1844150424003601
Batch 57/64 loss: -0.20260965824127197
Batch 58/64 loss: -0.18487012386322021
Batch 59/64 loss: -0.19354432821273804
Batch 60/64 loss: -0.18847119808197021
Batch 61/64 loss: -0.1981034278869629
Batch 62/64 loss: -0.18240219354629517
Batch 63/64 loss: -0.1758219599723816
Batch 64/64 loss: -0.166634202003479
Epoch 340  Train loss: -0.19268994518354826  Val loss: -0.042030929699796174
Epoch 341
-------------------------------
Batch 1/64 loss: -0.16035759449005127
Batch 2/64 loss: -0.18141967058181763
Batch 3/64 loss: -0.20559340715408325
Batch 4/64 loss: -0.18721991777420044
Batch 5/64 loss: -0.2027209997177124
Batch 6/64 loss: -0.16658622026443481
Batch 7/64 loss: -0.18658584356307983
Batch 8/64 loss: -0.22471609711647034
Batch 9/64 loss: -0.18843042850494385
Batch 10/64 loss: -0.1711641550064087
Batch 11/64 loss: -0.19667428731918335
Batch 12/64 loss: -0.1898786425590515
Batch 13/64 loss: -0.1799665093421936
Batch 14/64 loss: -0.21385204792022705
Batch 15/64 loss: -0.21192997694015503
Batch 16/64 loss: -0.21170485019683838
Batch 17/64 loss: -0.1790582537651062
Batch 18/64 loss: -0.17548108100891113
Batch 19/64 loss: -0.21731901168823242
Batch 20/64 loss: -0.212752103805542
Batch 21/64 loss: -0.1909436583518982
Batch 22/64 loss: -0.20220845937728882
Batch 23/64 loss: -0.21306979656219482
Batch 24/64 loss: -0.19573020935058594
Batch 25/64 loss: -0.2054201364517212
Batch 26/64 loss: -0.18454086780548096
Batch 27/64 loss: -0.20801961421966553
Batch 28/64 loss: -0.21427565813064575
Batch 29/64 loss: -0.18654167652130127
Batch 30/64 loss: -0.18777680397033691
Batch 31/64 loss: -0.2107411026954651
Batch 32/64 loss: -0.17197608947753906
Batch 33/64 loss: -0.17657971382141113
Batch 34/64 loss: -0.1892380714416504
Batch 35/64 loss: -0.2043684720993042
Batch 36/64 loss: -0.20043134689331055
Batch 37/64 loss: -0.17995691299438477
Batch 38/64 loss: -0.19598644971847534
Batch 39/64 loss: -0.1975572109222412
Batch 40/64 loss: -0.20459997653961182
Batch 41/64 loss: -0.2026187777519226
Batch 42/64 loss: -0.19732356071472168
Batch 43/64 loss: -0.19794130325317383
Batch 44/64 loss: -0.20658373832702637
Batch 45/64 loss: -0.21201997995376587
Batch 46/64 loss: -0.19871610403060913
Batch 47/64 loss: -0.17192471027374268
Batch 48/64 loss: -0.18621307611465454
Batch 49/64 loss: -0.19508576393127441
Batch 50/64 loss: -0.16659420728683472
Batch 51/64 loss: -0.19721651077270508
Batch 52/64 loss: -0.18937861919403076
Batch 53/64 loss: -0.19046980142593384
Batch 54/64 loss: -0.19239026308059692
Batch 55/64 loss: -0.2007840871810913
Batch 56/64 loss: -0.19411516189575195
Batch 57/64 loss: -0.1917964220046997
Batch 58/64 loss: -0.19583183526992798
Batch 59/64 loss: -0.2001744508743286
Batch 60/64 loss: -0.20855331420898438
Batch 61/64 loss: -0.20470720529556274
Batch 62/64 loss: -0.17663657665252686
Batch 63/64 loss: -0.19733035564422607
Batch 64/64 loss: -0.1726498007774353
Epoch 341  Train loss: -0.19431006277308743  Val loss: -0.043291855104190785
Epoch 342
-------------------------------
Batch 1/64 loss: -0.1924474835395813
Batch 2/64 loss: -0.20790576934814453
Batch 3/64 loss: -0.21620792150497437
Batch 4/64 loss: -0.1783539056777954
Batch 5/64 loss: -0.2053120732307434
Batch 6/64 loss: -0.20342254638671875
Batch 7/64 loss: -0.1984020471572876
Batch 8/64 loss: -0.1942826509475708
Batch 9/64 loss: -0.13231372833251953
Batch 10/64 loss: -0.20675253868103027
Batch 11/64 loss: -0.19829297065734863
Batch 12/64 loss: -0.17780965566635132
Batch 13/64 loss: -0.20590496063232422
Batch 14/64 loss: -0.20870202779769897
Batch 15/64 loss: -0.1975836157798767
Batch 16/64 loss: -0.18512094020843506
Batch 17/64 loss: -0.19866943359375
Batch 18/64 loss: -0.17958569526672363
Batch 19/64 loss: -0.20902663469314575
Batch 20/64 loss: -0.20394998788833618
Batch 21/64 loss: -0.18549156188964844
Batch 22/64 loss: -0.18182986974716187
Batch 23/64 loss: -0.2014223337173462
Batch 24/64 loss: -0.17375797033309937
Batch 25/64 loss: -0.18098604679107666
Batch 26/64 loss: -0.1900467872619629
Batch 27/64 loss: -0.19938766956329346
Batch 28/64 loss: -0.18068194389343262
Batch 29/64 loss: -0.21538960933685303
Batch 30/64 loss: -0.19369518756866455
Batch 31/64 loss: -0.21601784229278564
Batch 32/64 loss: -0.19457340240478516
Batch 33/64 loss: -0.2095920443534851
Batch 34/64 loss: -0.18854331970214844
Batch 35/64 loss: -0.19171154499053955
Batch 36/64 loss: -0.1912781000137329
Batch 37/64 loss: -0.187325119972229
Batch 38/64 loss: -0.19479447603225708
Batch 39/64 loss: -0.1840682029724121
Batch 40/64 loss: -0.17913663387298584
Batch 41/64 loss: -0.21451377868652344
Batch 42/64 loss: -0.19946706295013428
Batch 43/64 loss: -0.2103971242904663
Batch 44/64 loss: -0.20791500806808472
Batch 45/64 loss: -0.1898488998413086
Batch 46/64 loss: -0.2004883885383606
Batch 47/64 loss: -0.19409078359603882
Batch 48/64 loss: -0.19758272171020508
Batch 49/64 loss: -0.19697028398513794
Batch 50/64 loss: -0.1976374387741089
Batch 51/64 loss: -0.16963207721710205
Batch 52/64 loss: -0.19776123762130737
Batch 53/64 loss: -0.21292972564697266
Batch 54/64 loss: -0.17103493213653564
Batch 55/64 loss: -0.16267579793930054
Batch 56/64 loss: -0.17676901817321777
Batch 57/64 loss: -0.17871052026748657
Batch 58/64 loss: -0.18124079704284668
Batch 59/64 loss: -0.19518721103668213
Batch 60/64 loss: -0.19701850414276123
Batch 61/64 loss: -0.14788097143173218
Batch 62/64 loss: -0.17133325338363647
Batch 63/64 loss: -0.19603610038757324
Batch 64/64 loss: -0.16864436864852905
Epoch 342  Train loss: -0.19189617516947727  Val loss: -0.04445068061966257
Epoch 343
-------------------------------
Batch 1/64 loss: -0.19923824071884155
Batch 2/64 loss: -0.18245118856430054
Batch 3/64 loss: -0.17121785879135132
Batch 4/64 loss: -0.19458508491516113
Batch 5/64 loss: -0.1807226538658142
Batch 6/64 loss: -0.19185680150985718
Batch 7/64 loss: -0.19803458452224731
Batch 8/64 loss: -0.18277734518051147
Batch 9/64 loss: -0.18484538793563843
Batch 10/64 loss: -0.19896697998046875
Batch 11/64 loss: -0.17244631052017212
Batch 12/64 loss: -0.2043132185935974
Batch 13/64 loss: -0.1920718550682068
Batch 14/64 loss: -0.1792985200881958
Batch 15/64 loss: -0.20003461837768555
Batch 16/64 loss: -0.2056451439857483
Batch 17/64 loss: -0.15639936923980713
Batch 18/64 loss: -0.1761622428894043
Batch 19/64 loss: -0.17491209506988525
Batch 20/64 loss: -0.19346952438354492
Batch 21/64 loss: -0.188262939453125
Batch 22/64 loss: -0.20718616247177124
Batch 23/64 loss: -0.17744427919387817
Batch 24/64 loss: -0.2120988965034485
Batch 25/64 loss: -0.2213343381881714
Batch 26/64 loss: -0.19811302423477173
Batch 27/64 loss: -0.20084989070892334
Batch 28/64 loss: -0.20178401470184326
Batch 29/64 loss: -0.18593966960906982
Batch 30/64 loss: -0.2067280411720276
Batch 31/64 loss: -0.1998235583305359
Batch 32/64 loss: -0.2102791666984558
Batch 33/64 loss: -0.19451510906219482
Batch 34/64 loss: -0.20166015625
Batch 35/64 loss: -0.216985821723938
Batch 36/64 loss: -0.19752776622772217
Batch 37/64 loss: -0.1930527687072754
Batch 38/64 loss: -0.1969204545021057
Batch 39/64 loss: -0.17157208919525146
Batch 40/64 loss: -0.18946272134780884
Batch 41/64 loss: -0.20923805236816406
Batch 42/64 loss: -0.1883378028869629
Batch 43/64 loss: -0.22014403343200684
Batch 44/64 loss: -0.20291513204574585
Batch 45/64 loss: -0.21301919221878052
Batch 46/64 loss: -0.17245548963546753
Batch 47/64 loss: -0.20480895042419434
Batch 48/64 loss: -0.20859295129776
Batch 49/64 loss: -0.20472240447998047
Batch 50/64 loss: -0.197851300239563
Batch 51/64 loss: -0.1767006516456604
Batch 52/64 loss: -0.20584583282470703
Batch 53/64 loss: -0.19988512992858887
Batch 54/64 loss: -0.1760813593864441
Batch 55/64 loss: -0.19252055883407593
Batch 56/64 loss: -0.19381314516067505
Batch 57/64 loss: -0.18385547399520874
Batch 58/64 loss: -0.19827616214752197
Batch 59/64 loss: -0.20465344190597534
Batch 60/64 loss: -0.1973227858543396
Batch 61/64 loss: -0.175223708152771
Batch 62/64 loss: -0.18822699785232544
Batch 63/64 loss: -0.1746288537979126
Batch 64/64 loss: -0.1934491991996765
Epoch 343  Train loss: -0.19365013650819368  Val loss: -0.041627778425249444
Epoch 344
-------------------------------
Batch 1/64 loss: -0.1551445722579956
Batch 2/64 loss: -0.21624255180358887
Batch 3/64 loss: -0.19950848817825317
Batch 4/64 loss: -0.20488214492797852
Batch 5/64 loss: -0.2059590220451355
Batch 6/64 loss: -0.18558835983276367
Batch 7/64 loss: -0.20731782913208008
Batch 8/64 loss: -0.20754098892211914
Batch 9/64 loss: -0.1894720196723938
Batch 10/64 loss: -0.19848644733428955
Batch 11/64 loss: -0.21206098794937134
Batch 12/64 loss: -0.1705625057220459
Batch 13/64 loss: -0.19813764095306396
Batch 14/64 loss: -0.21402031183242798
Batch 15/64 loss: -0.18825560808181763
Batch 16/64 loss: -0.18197298049926758
Batch 17/64 loss: -0.21616703271865845
Batch 18/64 loss: -0.19509565830230713
Batch 19/64 loss: -0.1991976499557495
Batch 20/64 loss: -0.19588696956634521
Batch 21/64 loss: -0.19490879774093628
Batch 22/64 loss: -0.21242916584014893
Batch 23/64 loss: -0.153764545917511
Batch 24/64 loss: -0.20212280750274658
Batch 25/64 loss: -0.18758130073547363
Batch 26/64 loss: -0.21907514333724976
Batch 27/64 loss: -0.18037879467010498
Batch 28/64 loss: -0.18512630462646484
Batch 29/64 loss: -0.20326626300811768
Batch 30/64 loss: -0.17081111669540405
Batch 31/64 loss: -0.1768462061882019
Batch 32/64 loss: -0.19108730554580688
Batch 33/64 loss: -0.1899089813232422
Batch 34/64 loss: -0.20364069938659668
Batch 35/64 loss: -0.19964635372161865
Batch 36/64 loss: -0.20229679346084595
Batch 37/64 loss: -0.20933759212493896
Batch 38/64 loss: -0.19678157567977905
Batch 39/64 loss: -0.19970178604125977
Batch 40/64 loss: -0.2009519338607788
Batch 41/64 loss: -0.19391357898712158
Batch 42/64 loss: -0.19148647785186768
Batch 43/64 loss: -0.1918216347694397
Batch 44/64 loss: -0.19946074485778809
Batch 45/64 loss: -0.19937515258789062
Batch 46/64 loss: -0.18224167823791504
Batch 47/64 loss: -0.20336002111434937
Batch 48/64 loss: -0.17799317836761475
Batch 49/64 loss: -0.21511566638946533
Batch 50/64 loss: -0.189322829246521
Batch 51/64 loss: -0.19508105516433716
Batch 52/64 loss: -0.20728182792663574
Batch 53/64 loss: -0.19949674606323242
Batch 54/64 loss: -0.20341479778289795
Batch 55/64 loss: -0.198827862739563
Batch 56/64 loss: -0.21611320972442627
Batch 57/64 loss: -0.19058644771575928
Batch 58/64 loss: -0.18133878707885742
Batch 59/64 loss: -0.2167743444442749
Batch 60/64 loss: -0.18038588762283325
Batch 61/64 loss: -0.2000596523284912
Batch 62/64 loss: -0.19580626487731934
Batch 63/64 loss: -0.2085942029953003
Batch 64/64 loss: -0.19780617952346802
Epoch 344  Train loss: -0.1961940379703746  Val loss: -0.044303849800345824
Epoch 345
-------------------------------
Batch 1/64 loss: -0.19036298990249634
Batch 2/64 loss: -0.20855414867401123
Batch 3/64 loss: -0.19622695446014404
Batch 4/64 loss: -0.20594924688339233
Batch 5/64 loss: -0.19499683380126953
Batch 6/64 loss: -0.1997024416923523
Batch 7/64 loss: -0.21975255012512207
Batch 8/64 loss: -0.19081956148147583
Batch 9/64 loss: -0.16760051250457764
Batch 10/64 loss: -0.1947426199913025
Batch 11/64 loss: -0.1968691349029541
Batch 12/64 loss: -0.20661592483520508
Batch 13/64 loss: -0.1964084506034851
Batch 14/64 loss: -0.18900161981582642
Batch 15/64 loss: -0.18795418739318848
Batch 16/64 loss: -0.17818790674209595
Batch 17/64 loss: -0.21080869436264038
Batch 18/64 loss: -0.1922835111618042
Batch 19/64 loss: -0.22010400891304016
Batch 20/64 loss: -0.2160428762435913
Batch 21/64 loss: -0.18046128749847412
Batch 22/64 loss: -0.16505271196365356
Batch 23/64 loss: -0.18623554706573486
Batch 24/64 loss: -0.17987006902694702
Batch 25/64 loss: -0.18996959924697876
Batch 26/64 loss: -0.1948004961013794
Batch 27/64 loss: -0.19207841157913208
Batch 28/64 loss: -0.18823355436325073
Batch 29/64 loss: -0.19644266366958618
Batch 30/64 loss: -0.16383075714111328
Batch 31/64 loss: -0.18710631132125854
Batch 32/64 loss: -0.18036484718322754
Batch 33/64 loss: -0.19811701774597168
Batch 34/64 loss: -0.17812985181808472
Batch 35/64 loss: -0.1829824447631836
Batch 36/64 loss: -0.17129331827163696
Batch 37/64 loss: -0.19368863105773926
Batch 38/64 loss: -0.19819122552871704
Batch 39/64 loss: -0.18315136432647705
Batch 40/64 loss: -0.20240098237991333
Batch 41/64 loss: -0.19354379177093506
Batch 42/64 loss: -0.20716339349746704
Batch 43/64 loss: -0.201626718044281
Batch 44/64 loss: -0.20387381315231323
Batch 45/64 loss: -0.18968510627746582
Batch 46/64 loss: -0.1938568353652954
Batch 47/64 loss: -0.19208228588104248
Batch 48/64 loss: -0.2187747359275818
Batch 49/64 loss: -0.20183664560317993
Batch 50/64 loss: -0.21100127696990967
Batch 51/64 loss: -0.2113550305366516
Batch 52/64 loss: -0.20605766773223877
Batch 53/64 loss: -0.21041011810302734
Batch 54/64 loss: -0.21191054582595825
Batch 55/64 loss: -0.1965448260307312
Batch 56/64 loss: -0.17753934860229492
Batch 57/64 loss: -0.1819114089012146
Batch 58/64 loss: -0.17300766706466675
Batch 59/64 loss: -0.2059687376022339
Batch 60/64 loss: -0.20320987701416016
Batch 61/64 loss: -0.228378027677536
Batch 62/64 loss: -0.21463441848754883
Batch 63/64 loss: -0.20075899362564087
Batch 64/64 loss: -0.1931300163269043
Epoch 345  Train loss: -0.1953782595840155  Val loss: -0.0429302839069432
Epoch 346
-------------------------------
Batch 1/64 loss: -0.18272632360458374
Batch 2/64 loss: -0.19509780406951904
Batch 3/64 loss: -0.21063905954360962
Batch 4/64 loss: -0.20331275463104248
Batch 5/64 loss: -0.19309914112091064
Batch 6/64 loss: -0.19300377368927002
Batch 7/64 loss: -0.19468122720718384
Batch 8/64 loss: -0.22286704182624817
Batch 9/64 loss: -0.16706234216690063
Batch 10/64 loss: -0.18420255184173584
Batch 11/64 loss: -0.20044314861297607
Batch 12/64 loss: -0.20832514762878418
Batch 13/64 loss: -0.19005101919174194
Batch 14/64 loss: -0.1782880425453186
Batch 15/64 loss: -0.21123182773590088
Batch 16/64 loss: -0.1683526635169983
Batch 17/64 loss: -0.20942074060440063
Batch 18/64 loss: -0.19254648685455322
Batch 19/64 loss: -0.186454176902771
Batch 20/64 loss: -0.20372027158737183
Batch 21/64 loss: -0.19703859090805054
Batch 22/64 loss: -0.199377179145813
Batch 23/64 loss: -0.2050483226776123
Batch 24/64 loss: -0.2071741819381714
Batch 25/64 loss: -0.20028257369995117
Batch 26/64 loss: -0.20239675045013428
Batch 27/64 loss: -0.19620752334594727
Batch 28/64 loss: -0.2226947844028473
Batch 29/64 loss: -0.21966630220413208
Batch 30/64 loss: -0.19341248273849487
Batch 31/64 loss: -0.18596339225769043
Batch 32/64 loss: -0.19149339199066162
Batch 33/64 loss: -0.16862696409225464
Batch 34/64 loss: -0.21102005243301392
Batch 35/64 loss: -0.19058233499526978
Batch 36/64 loss: -0.21486103534698486
Batch 37/64 loss: -0.19826382398605347
Batch 38/64 loss: -0.19219231605529785
Batch 39/64 loss: -0.15643316507339478
Batch 40/64 loss: -0.1835116744041443
Batch 41/64 loss: -0.20564836263656616
Batch 42/64 loss: -0.1992005705833435
Batch 43/64 loss: -0.21273428201675415
Batch 44/64 loss: -0.1932263970375061
Batch 45/64 loss: -0.1826959252357483
Batch 46/64 loss: -0.20190024375915527
Batch 47/64 loss: -0.21965408325195312
Batch 48/64 loss: -0.16226333379745483
Batch 49/64 loss: -0.17598652839660645
Batch 50/64 loss: -0.19728422164916992
Batch 51/64 loss: -0.20064657926559448
Batch 52/64 loss: -0.20129799842834473
Batch 53/64 loss: -0.20181798934936523
Batch 54/64 loss: -0.19266539812088013
Batch 55/64 loss: -0.19694942235946655
Batch 56/64 loss: -0.2086484432220459
Batch 57/64 loss: -0.19937574863433838
Batch 58/64 loss: -0.19806629419326782
Batch 59/64 loss: -0.19018274545669556
Batch 60/64 loss: -0.20606791973114014
Batch 61/64 loss: -0.2058125138282776
Batch 62/64 loss: -0.1803818941116333
Batch 63/64 loss: -0.20596659183502197
Batch 64/64 loss: -0.16353100538253784
Epoch 346  Train loss: -0.19596696671317607  Val loss: -0.04084687687687038
Epoch 347
-------------------------------
Batch 1/64 loss: -0.20277202129364014
Batch 2/64 loss: -0.2130260467529297
Batch 3/64 loss: -0.197490394115448
Batch 4/64 loss: -0.19807881116867065
Batch 5/64 loss: -0.21343863010406494
Batch 6/64 loss: -0.2070329785346985
Batch 7/64 loss: -0.19145214557647705
Batch 8/64 loss: -0.20732617378234863
Batch 9/64 loss: -0.18540990352630615
Batch 10/64 loss: -0.2205473780632019
Batch 11/64 loss: -0.18519997596740723
Batch 12/64 loss: -0.18678003549575806
Batch 13/64 loss: -0.14560580253601074
Batch 14/64 loss: -0.19717586040496826
Batch 15/64 loss: -0.19766122102737427
Batch 16/64 loss: -0.20986831188201904
Batch 17/64 loss: -0.19040155410766602
Batch 18/64 loss: -0.19342947006225586
Batch 19/64 loss: -0.19493067264556885
Batch 20/64 loss: -0.19830095767974854
Batch 21/64 loss: -0.2001044750213623
Batch 22/64 loss: -0.19127118587493896
Batch 23/64 loss: -0.19482052326202393
Batch 24/64 loss: -0.1863011121749878
Batch 25/64 loss: -0.2079271674156189
Batch 26/64 loss: -0.23099350929260254
Batch 27/64 loss: -0.20305776596069336
Batch 28/64 loss: -0.19182229042053223
Batch 29/64 loss: -0.2018865942955017
Batch 30/64 loss: -0.19968807697296143
Batch 31/64 loss: -0.2102389931678772
Batch 32/64 loss: -0.1849914789199829
Batch 33/64 loss: -0.1991068720817566
Batch 34/64 loss: -0.2020512819290161
Batch 35/64 loss: -0.18898999691009521
Batch 36/64 loss: -0.18493741750717163
Batch 37/64 loss: -0.2083359956741333
Batch 38/64 loss: -0.21068084239959717
Batch 39/64 loss: -0.20532143115997314
Batch 40/64 loss: -0.16934853792190552
Batch 41/64 loss: -0.21763145923614502
Batch 42/64 loss: -0.18727481365203857
Batch 43/64 loss: -0.20139390230178833
Batch 44/64 loss: -0.21203607320785522
Batch 45/64 loss: -0.20100247859954834
Batch 46/64 loss: -0.21429288387298584
Batch 47/64 loss: -0.21133089065551758
Batch 48/64 loss: -0.18838393688201904
Batch 49/64 loss: -0.20810765027999878
Batch 50/64 loss: -0.19974452257156372
Batch 51/64 loss: -0.17525839805603027
Batch 52/64 loss: -0.19171887636184692
Batch 53/64 loss: -0.19690197706222534
Batch 54/64 loss: -0.20329421758651733
Batch 55/64 loss: -0.19153159856796265
Batch 56/64 loss: -0.18013477325439453
Batch 57/64 loss: -0.17881494760513306
Batch 58/64 loss: -0.14420485496520996
Batch 59/64 loss: -0.1756575107574463
Batch 60/64 loss: -0.19274461269378662
Batch 61/64 loss: -0.1782192587852478
Batch 62/64 loss: -0.17441409826278687
Batch 63/64 loss: -0.19223546981811523
Batch 64/64 loss: -0.19900751113891602
Epoch 347  Train loss: -0.1956609997094846  Val loss: -0.03786747500658855
Epoch 348
-------------------------------
Batch 1/64 loss: -0.19325041770935059
Batch 2/64 loss: -0.2137138843536377
Batch 3/64 loss: -0.21153753995895386
Batch 4/64 loss: -0.19074249267578125
Batch 5/64 loss: -0.21461135149002075
Batch 6/64 loss: -0.201377272605896
Batch 7/64 loss: -0.20597583055496216
Batch 8/64 loss: -0.20244085788726807
Batch 9/64 loss: -0.16016411781311035
Batch 10/64 loss: -0.1986827850341797
Batch 11/64 loss: -0.17577707767486572
Batch 12/64 loss: -0.19586700201034546
Batch 13/64 loss: -0.1975451111793518
Batch 14/64 loss: -0.19980281591415405
Batch 15/64 loss: -0.15451747179031372
Batch 16/64 loss: -0.22123205661773682
Batch 17/64 loss: -0.21004724502563477
Batch 18/64 loss: -0.19306540489196777
Batch 19/64 loss: -0.20510894060134888
Batch 20/64 loss: -0.20068514347076416
Batch 21/64 loss: -0.17291724681854248
Batch 22/64 loss: -0.1666104793548584
Batch 23/64 loss: -0.2047063708305359
Batch 24/64 loss: -0.13796722888946533
Batch 25/64 loss: -0.18992286920547485
Batch 26/64 loss: -0.21011829376220703
Batch 27/64 loss: -0.17431384325027466
Batch 28/64 loss: -0.199041485786438
Batch 29/64 loss: -0.2028881311416626
Batch 30/64 loss: -0.19133394956588745
Batch 31/64 loss: -0.19245022535324097
Batch 32/64 loss: -0.20632630586624146
Batch 33/64 loss: -0.18618625402450562
Batch 34/64 loss: -0.21400386095046997
Batch 35/64 loss: -0.20049017667770386
Batch 36/64 loss: -0.20745885372161865
Batch 37/64 loss: -0.179642915725708
Batch 38/64 loss: -0.22170811891555786
Batch 39/64 loss: -0.20664644241333008
Batch 40/64 loss: -0.1904723048210144
Batch 41/64 loss: -0.20311039686203003
Batch 42/64 loss: -0.21670177578926086
Batch 43/64 loss: -0.19179695844650269
Batch 44/64 loss: -0.21654397249221802
Batch 45/64 loss: -0.1528668999671936
Batch 46/64 loss: -0.2037484049797058
Batch 47/64 loss: -0.2040436863899231
Batch 48/64 loss: -0.20284873247146606
Batch 49/64 loss: -0.16027402877807617
Batch 50/64 loss: -0.1851714849472046
Batch 51/64 loss: -0.19789975881576538
Batch 52/64 loss: -0.18232834339141846
Batch 53/64 loss: -0.21423625946044922
Batch 54/64 loss: -0.19189053773880005
Batch 55/64 loss: -0.2117597460746765
Batch 56/64 loss: -0.19831907749176025
Batch 57/64 loss: -0.20186316967010498
Batch 58/64 loss: -0.19911295175552368
Batch 59/64 loss: -0.21352970600128174
Batch 60/64 loss: -0.21941697597503662
Batch 61/64 loss: -0.19213122129440308
Batch 62/64 loss: -0.19984734058380127
Batch 63/64 loss: -0.20561927556991577
Batch 64/64 loss: -0.20205408334732056
Epoch 348  Train loss: -0.19636002264770808  Val loss: -0.038472259577197306
Epoch 349
-------------------------------
Batch 1/64 loss: -0.20307207107543945
Batch 2/64 loss: -0.21027237176895142
Batch 3/64 loss: -0.17867153882980347
Batch 4/64 loss: -0.18985867500305176
Batch 5/64 loss: -0.2015356421470642
Batch 6/64 loss: -0.19422376155853271
Batch 7/64 loss: -0.20958375930786133
Batch 8/64 loss: -0.19797730445861816
Batch 9/64 loss: -0.20047003030776978
Batch 10/64 loss: -0.19199037551879883
Batch 11/64 loss: -0.20161527395248413
Batch 12/64 loss: -0.18878567218780518
Batch 13/64 loss: -0.16910028457641602
Batch 14/64 loss: -0.20385682582855225
Batch 15/64 loss: -0.21700149774551392
Batch 16/64 loss: -0.20022732019424438
Batch 17/64 loss: -0.20152229070663452
Batch 18/64 loss: -0.21285778284072876
Batch 19/64 loss: -0.195787250995636
Batch 20/64 loss: -0.1927039623260498
Batch 21/64 loss: -0.1927056908607483
Batch 22/64 loss: -0.19798064231872559
Batch 23/64 loss: -0.19344830513000488
Batch 24/64 loss: -0.17131543159484863
Batch 25/64 loss: -0.18723714351654053
Batch 26/64 loss: -0.16301292181015015
Batch 27/64 loss: -0.20629799365997314
Batch 28/64 loss: -0.2049008011817932
Batch 29/64 loss: -0.1914309859275818
Batch 30/64 loss: -0.1546001434326172
Batch 31/64 loss: -0.19664782285690308
Batch 32/64 loss: -0.19277441501617432
Batch 33/64 loss: -0.1561495065689087
Batch 34/64 loss: -0.2186759114265442
Batch 35/64 loss: -0.21391761302947998
Batch 36/64 loss: -0.19254183769226074
Batch 37/64 loss: -0.1975879669189453
Batch 38/64 loss: -0.19311529397964478
Batch 39/64 loss: -0.1969388723373413
Batch 40/64 loss: -0.18235737085342407
Batch 41/64 loss: -0.20125311613082886
Batch 42/64 loss: -0.1917300820350647
Batch 43/64 loss: -0.17638295888900757
Batch 44/64 loss: -0.19017624855041504
Batch 45/64 loss: -0.18885189294815063
Batch 46/64 loss: -0.16367197036743164
Batch 47/64 loss: -0.2049243450164795
Batch 48/64 loss: -0.2065299153327942
Batch 49/64 loss: -0.20014351606369019
Batch 50/64 loss: -0.20865654945373535
Batch 51/64 loss: -0.20199453830718994
Batch 52/64 loss: -0.21689105033874512
Batch 53/64 loss: -0.17524075508117676
Batch 54/64 loss: -0.18851172924041748
Batch 55/64 loss: -0.20901060104370117
Batch 56/64 loss: -0.2002689242362976
Batch 57/64 loss: -0.20352685451507568
Batch 58/64 loss: -0.2145189642906189
Batch 59/64 loss: -0.1932259202003479
Batch 60/64 loss: -0.21217453479766846
Batch 61/64 loss: -0.20031976699829102
Batch 62/64 loss: -0.1843278408050537
Batch 63/64 loss: -0.20173251628875732
Batch 64/64 loss: -0.18323922157287598
Epoch 349  Train loss: -0.19507834303612803  Val loss: -0.04069961223405661
Epoch 350
-------------------------------
Batch 1/64 loss: -0.21219056844711304
Batch 2/64 loss: -0.1643241047859192
Batch 3/64 loss: -0.19102466106414795
Batch 4/64 loss: -0.2000058889389038
Batch 5/64 loss: -0.17910540103912354
Batch 6/64 loss: -0.20314842462539673
Batch 7/64 loss: -0.20828378200531006
Batch 8/64 loss: -0.20288455486297607
Batch 9/64 loss: -0.17364877462387085
Batch 10/64 loss: -0.19728463888168335
Batch 11/64 loss: -0.18250340223312378
Batch 12/64 loss: -0.23310428857803345
Batch 13/64 loss: -0.20475685596466064
Batch 14/64 loss: -0.20704245567321777
Batch 15/64 loss: -0.194075345993042
Batch 16/64 loss: -0.20415222644805908
Batch 17/64 loss: -0.19261568784713745
Batch 18/64 loss: -0.21593636274337769
Batch 19/64 loss: -0.19359159469604492
Batch 20/64 loss: -0.1904059648513794
Batch 21/64 loss: -0.19112300872802734
Batch 22/64 loss: -0.21684885025024414
Batch 23/64 loss: -0.20987099409103394
Batch 24/64 loss: -0.18608099222183228
Batch 25/64 loss: -0.21486437320709229
Batch 26/64 loss: -0.1948530077934265
Batch 27/64 loss: -0.21592003107070923
Batch 28/64 loss: -0.18602633476257324
Batch 29/64 loss: -0.1420222520828247
Batch 30/64 loss: -0.19001460075378418
Batch 31/64 loss: -0.20191353559494019
Batch 32/64 loss: -0.19477736949920654
Batch 33/64 loss: -0.209048330783844
Batch 34/64 loss: -0.20036965608596802
Batch 35/64 loss: -0.19863617420196533
Batch 36/64 loss: -0.19837051630020142
Batch 37/64 loss: -0.18987703323364258
Batch 38/64 loss: -0.20978736877441406
Batch 39/64 loss: -0.22530528903007507
Batch 40/64 loss: -0.17366468906402588
Batch 41/64 loss: -0.18565350770950317
Batch 42/64 loss: -0.18965643644332886
Batch 43/64 loss: -0.2051389217376709
Batch 44/64 loss: -0.21481138467788696
Batch 45/64 loss: -0.19550466537475586
Batch 46/64 loss: -0.19661349058151245
Batch 47/64 loss: -0.19069421291351318
Batch 48/64 loss: -0.17170292139053345
Batch 49/64 loss: -0.1920262575149536
Batch 50/64 loss: -0.1883344054222107
Batch 51/64 loss: -0.1902804970741272
Batch 52/64 loss: -0.2033172845840454
Batch 53/64 loss: -0.20032882690429688
Batch 54/64 loss: -0.15991586446762085
Batch 55/64 loss: -0.20256584882736206
Batch 56/64 loss: -0.17285627126693726
Batch 57/64 loss: -0.2033092975616455
Batch 58/64 loss: -0.19263768196105957
Batch 59/64 loss: -0.18040943145751953
Batch 60/64 loss: -0.2005230188369751
Batch 61/64 loss: -0.21595299243927002
Batch 62/64 loss: -0.17558175325393677
Batch 63/64 loss: -0.19908034801483154
Batch 64/64 loss: -0.1933574676513672
Epoch 350  Train loss: -0.19562937745860978  Val loss: -0.041679399120029305
Epoch 351
-------------------------------
Batch 1/64 loss: -0.19531649351119995
Batch 2/64 loss: -0.21297335624694824
Batch 3/64 loss: -0.2100275158882141
Batch 4/64 loss: -0.17914706468582153
Batch 5/64 loss: -0.18213725090026855
Batch 6/64 loss: -0.1941867470741272
Batch 7/64 loss: -0.1887110471725464
Batch 8/64 loss: -0.18369704484939575
Batch 9/64 loss: -0.20918452739715576
Batch 10/64 loss: -0.19471138715744019
Batch 11/64 loss: -0.2217734456062317
Batch 12/64 loss: -0.19579458236694336
Batch 13/64 loss: -0.2037319540977478
Batch 14/64 loss: -0.21323353052139282
Batch 15/64 loss: -0.2076789140701294
Batch 16/64 loss: -0.2135569453239441
Batch 17/64 loss: -0.1799560785293579
Batch 18/64 loss: -0.1965685486793518
Batch 19/64 loss: -0.20237970352172852
Batch 20/64 loss: -0.19244956970214844
Batch 21/64 loss: -0.1879069209098816
Batch 22/64 loss: -0.19699054956436157
Batch 23/64 loss: -0.1988263726234436
Batch 24/64 loss: -0.19933509826660156
Batch 25/64 loss: -0.18123114109039307
Batch 26/64 loss: -0.204298198223114
Batch 27/64 loss: -0.1797810196876526
Batch 28/64 loss: -0.18162620067596436
Batch 29/64 loss: -0.18532758951187134
Batch 30/64 loss: -0.16640335321426392
Batch 31/64 loss: -0.20895934104919434
Batch 32/64 loss: -0.18285512924194336
Batch 33/64 loss: -0.18389499187469482
Batch 34/64 loss: -0.1989656686782837
Batch 35/64 loss: -0.17128080129623413
Batch 36/64 loss: -0.1924867033958435
Batch 37/64 loss: -0.19352221488952637
Batch 38/64 loss: -0.21758395433425903
Batch 39/64 loss: -0.2022167444229126
Batch 40/64 loss: -0.19238895177841187
Batch 41/64 loss: -0.21409457921981812
Batch 42/64 loss: -0.18715065717697144
Batch 43/64 loss: -0.1761329174041748
Batch 44/64 loss: -0.19742727279663086
Batch 45/64 loss: -0.21267414093017578
Batch 46/64 loss: -0.15573227405548096
Batch 47/64 loss: -0.1997067928314209
Batch 48/64 loss: -0.20624381303787231
Batch 49/64 loss: -0.19791394472122192
Batch 50/64 loss: -0.16638147830963135
Batch 51/64 loss: -0.18577510118484497
Batch 52/64 loss: -0.2045886516571045
Batch 53/64 loss: -0.19195395708084106
Batch 54/64 loss: -0.22249752283096313
Batch 55/64 loss: -0.18425893783569336
Batch 56/64 loss: -0.19534194469451904
Batch 57/64 loss: -0.20356464385986328
Batch 58/64 loss: -0.19384682178497314
Batch 59/64 loss: -0.19490736722946167
Batch 60/64 loss: -0.2017037272453308
Batch 61/64 loss: -0.18871498107910156
Batch 62/64 loss: -0.20352071523666382
Batch 63/64 loss: -0.19311749935150146
Batch 64/64 loss: -0.17606717348098755
Epoch 351  Train loss: -0.19470426311679914  Val loss: -0.042402037025726945
Epoch 352
-------------------------------
Batch 1/64 loss: -0.17521482706069946
Batch 2/64 loss: -0.18178975582122803
Batch 3/64 loss: -0.1779876947402954
Batch 4/64 loss: -0.1855878233909607
Batch 5/64 loss: -0.19338202476501465
Batch 6/64 loss: -0.20724523067474365
Batch 7/64 loss: -0.20491397380828857
Batch 8/64 loss: -0.20009195804595947
Batch 9/64 loss: -0.21449851989746094
Batch 10/64 loss: -0.2136749029159546
Batch 11/64 loss: -0.20309233665466309
Batch 12/64 loss: -0.20766258239746094
Batch 13/64 loss: -0.21100914478302002
Batch 14/64 loss: -0.1850757598876953
Batch 15/64 loss: -0.21430885791778564
Batch 16/64 loss: -0.21259737014770508
Batch 17/64 loss: -0.21085703372955322
Batch 18/64 loss: -0.17834866046905518
Batch 19/64 loss: -0.2213461697101593
Batch 20/64 loss: -0.20537781715393066
Batch 21/64 loss: -0.17862051725387573
Batch 22/64 loss: -0.18770980834960938
Batch 23/64 loss: -0.21616721153259277
Batch 24/64 loss: -0.19343018531799316
Batch 25/64 loss: -0.22695305943489075
Batch 26/64 loss: -0.21883362531661987
Batch 27/64 loss: -0.2017154097557068
Batch 28/64 loss: -0.20058673620224
Batch 29/64 loss: -0.2083057165145874
Batch 30/64 loss: -0.20887839794158936
Batch 31/64 loss: -0.19021499156951904
Batch 32/64 loss: -0.2079758644104004
Batch 33/64 loss: -0.19328254461288452
Batch 34/64 loss: -0.20978647470474243
Batch 35/64 loss: -0.19746309518814087
Batch 36/64 loss: -0.20481574535369873
Batch 37/64 loss: -0.1958160400390625
Batch 38/64 loss: -0.18889886140823364
Batch 39/64 loss: -0.20762181282043457
Batch 40/64 loss: -0.2102799415588379
Batch 41/64 loss: -0.21271342039108276
Batch 42/64 loss: -0.18833237886428833
Batch 43/64 loss: -0.16115492582321167
Batch 44/64 loss: -0.21465331315994263
Batch 45/64 loss: -0.17293179035186768
Batch 46/64 loss: -0.19501745700836182
Batch 47/64 loss: -0.19378453493118286
Batch 48/64 loss: -0.2080402970314026
Batch 49/64 loss: -0.19507485628128052
Batch 50/64 loss: -0.22058483958244324
Batch 51/64 loss: -0.214089035987854
Batch 52/64 loss: -0.2124156355857849
Batch 53/64 loss: -0.18701666593551636
Batch 54/64 loss: -0.1809406280517578
Batch 55/64 loss: -0.1986910104751587
Batch 56/64 loss: -0.20300662517547607
Batch 57/64 loss: -0.1652509570121765
Batch 58/64 loss: -0.18473809957504272
Batch 59/64 loss: -0.21047240495681763
Batch 60/64 loss: -0.18126940727233887
Batch 61/64 loss: -0.17802530527114868
Batch 62/64 loss: -0.2195761799812317
Batch 63/64 loss: -0.18698805570602417
Batch 64/64 loss: -0.18760216236114502
Epoch 352  Train loss: -0.19885314399120854  Val loss: -0.042804409753006346
Epoch 353
-------------------------------
Batch 1/64 loss: -0.203460693359375
Batch 2/64 loss: -0.20794004201889038
Batch 3/64 loss: -0.1838393211364746
Batch 4/64 loss: -0.21173733472824097
Batch 5/64 loss: -0.19478970766067505
Batch 6/64 loss: -0.2110862135887146
Batch 7/64 loss: -0.1980654001235962
Batch 8/64 loss: -0.2066962718963623
Batch 9/64 loss: -0.18647217750549316
Batch 10/64 loss: -0.1986960768699646
Batch 11/64 loss: -0.22041919827461243
Batch 12/64 loss: -0.21727347373962402
Batch 13/64 loss: -0.20295041799545288
Batch 14/64 loss: -0.19515317678451538
Batch 15/64 loss: -0.19288790225982666
Batch 16/64 loss: -0.20928674936294556
Batch 17/64 loss: -0.19176959991455078
Batch 18/64 loss: -0.2185402512550354
Batch 19/64 loss: -0.20748549699783325
Batch 20/64 loss: -0.20878124237060547
Batch 21/64 loss: -0.21652674674987793
Batch 22/64 loss: -0.21106892824172974
Batch 23/64 loss: -0.18948376178741455
Batch 24/64 loss: -0.18640029430389404
Batch 25/64 loss: -0.1940886378288269
Batch 26/64 loss: -0.18313777446746826
Batch 27/64 loss: -0.2050684690475464
Batch 28/64 loss: -0.1843317747116089
Batch 29/64 loss: -0.2017858624458313
Batch 30/64 loss: -0.20825976133346558
Batch 31/64 loss: -0.19256508350372314
Batch 32/64 loss: -0.2089899778366089
Batch 33/64 loss: -0.2016497254371643
Batch 34/64 loss: -0.20752185583114624
Batch 35/64 loss: -0.1942596435546875
Batch 36/64 loss: -0.18800592422485352
Batch 37/64 loss: -0.1974961757659912
Batch 38/64 loss: -0.21925956010818481
Batch 39/64 loss: -0.20480424165725708
Batch 40/64 loss: -0.20241791009902954
Batch 41/64 loss: -0.2241487205028534
Batch 42/64 loss: -0.1908397078514099
Batch 43/64 loss: -0.2183207869529724
Batch 44/64 loss: -0.2013845443725586
Batch 45/64 loss: -0.19662851095199585
Batch 46/64 loss: -0.1990194320678711
Batch 47/64 loss: -0.17873704433441162
Batch 48/64 loss: -0.21801155805587769
Batch 49/64 loss: -0.1886724829673767
Batch 50/64 loss: -0.2071487307548523
Batch 51/64 loss: -0.22015830874443054
Batch 52/64 loss: -0.20435816049575806
Batch 53/64 loss: -0.19007360935211182
Batch 54/64 loss: -0.2043246030807495
Batch 55/64 loss: -0.19012516736984253
Batch 56/64 loss: -0.20804262161254883
Batch 57/64 loss: -0.203985333442688
Batch 58/64 loss: -0.1740371584892273
Batch 59/64 loss: -0.18927055597305298
Batch 60/64 loss: -0.17856943607330322
Batch 61/64 loss: -0.21222203969955444
Batch 62/64 loss: -0.2035772204399109
Batch 63/64 loss: -0.1846897006034851
Batch 64/64 loss: -0.20200377702713013
Epoch 353  Train loss: -0.20082040978413002  Val loss: -0.04371969089475284
Epoch 354
-------------------------------
Batch 1/64 loss: -0.20681291818618774
Batch 2/64 loss: -0.21169281005859375
Batch 3/64 loss: -0.19556927680969238
Batch 4/64 loss: -0.20676445960998535
Batch 5/64 loss: -0.1906411051750183
Batch 6/64 loss: -0.20144718885421753
Batch 7/64 loss: -0.2249954342842102
Batch 8/64 loss: -0.1983034610748291
Batch 9/64 loss: -0.21048831939697266
Batch 10/64 loss: -0.14592450857162476
Batch 11/64 loss: -0.22260597348213196
Batch 12/64 loss: -0.19023120403289795
Batch 13/64 loss: -0.22291481494903564
Batch 14/64 loss: -0.2170148491859436
Batch 15/64 loss: -0.2068302035331726
Batch 16/64 loss: -0.18548434972763062
Batch 17/64 loss: -0.22051924467086792
Batch 18/64 loss: -0.16642004251480103
Batch 19/64 loss: -0.2145746946334839
Batch 20/64 loss: -0.22966456413269043
Batch 21/64 loss: -0.1970353126525879
Batch 22/64 loss: -0.19511866569519043
Batch 23/64 loss: -0.21149593591690063
Batch 24/64 loss: -0.2080531120300293
Batch 25/64 loss: -0.2171728014945984
Batch 26/64 loss: -0.16234803199768066
Batch 27/64 loss: -0.16805225610733032
Batch 28/64 loss: -0.20639705657958984
Batch 29/64 loss: -0.2194594144821167
Batch 30/64 loss: -0.21373969316482544
Batch 31/64 loss: -0.20072978734970093
Batch 32/64 loss: -0.21414893865585327
Batch 33/64 loss: -0.20281481742858887
Batch 34/64 loss: -0.16816258430480957
Batch 35/64 loss: -0.2074955701828003
Batch 36/64 loss: -0.22033992409706116
Batch 37/64 loss: -0.207281231880188
Batch 38/64 loss: -0.21344530582427979
Batch 39/64 loss: -0.23029226064682007
Batch 40/64 loss: -0.21463042497634888
Batch 41/64 loss: -0.20939356088638306
Batch 42/64 loss: -0.1973620057106018
Batch 43/64 loss: -0.2176891565322876
Batch 44/64 loss: -0.19607102870941162
Batch 45/64 loss: -0.1935558319091797
Batch 46/64 loss: -0.14403367042541504
Batch 47/64 loss: -0.21367371082305908
Batch 48/64 loss: -0.2121071219444275
Batch 49/64 loss: -0.17973804473876953
Batch 50/64 loss: -0.18594402074813843
Batch 51/64 loss: -0.21128016710281372
Batch 52/64 loss: -0.1831066608428955
Batch 53/64 loss: -0.19407588243484497
Batch 54/64 loss: -0.20705276727676392
Batch 55/64 loss: -0.2163143754005432
Batch 56/64 loss: -0.19547802209854126
Batch 57/64 loss: -0.19843095541000366
Batch 58/64 loss: -0.19732356071472168
Batch 59/64 loss: -0.19948476552963257
Batch 60/64 loss: -0.212161123752594
Batch 61/64 loss: -0.1806955337524414
Batch 62/64 loss: -0.1538475751876831
Batch 63/64 loss: -0.18999981880187988
Batch 64/64 loss: -0.19616222381591797
Epoch 354  Train loss: -0.20051848748150994  Val loss: -0.03820993257142424
Epoch 355
-------------------------------
Batch 1/64 loss: -0.19761765003204346
Batch 2/64 loss: -0.18476951122283936
Batch 3/64 loss: -0.21227794885635376
Batch 4/64 loss: -0.19995111227035522
Batch 5/64 loss: -0.15760338306427002
Batch 6/64 loss: -0.18315190076828003
Batch 7/64 loss: -0.22430896759033203
Batch 8/64 loss: -0.22605186700820923
Batch 9/64 loss: -0.22167253494262695
Batch 10/64 loss: -0.1816818118095398
Batch 11/64 loss: -0.19648122787475586
Batch 12/64 loss: -0.20986169576644897
Batch 13/64 loss: -0.21524637937545776
Batch 14/64 loss: -0.21566885709762573
Batch 15/64 loss: -0.1627572774887085
Batch 16/64 loss: -0.2158096432685852
Batch 17/64 loss: -0.21714740991592407
Batch 18/64 loss: -0.20776957273483276
Batch 19/64 loss: -0.21875661611557007
Batch 20/64 loss: -0.2047620415687561
Batch 21/64 loss: -0.18095529079437256
Batch 22/64 loss: -0.21476483345031738
Batch 23/64 loss: -0.2252885103225708
Batch 24/64 loss: -0.16574722528457642
Batch 25/64 loss: -0.22132554650306702
Batch 26/64 loss: -0.20521944761276245
Batch 27/64 loss: -0.19955313205718994
Batch 28/64 loss: -0.19354528188705444
Batch 29/64 loss: -0.2054382562637329
Batch 30/64 loss: -0.197307288646698
Batch 31/64 loss: -0.19862425327301025
Batch 32/64 loss: -0.19446486234664917
Batch 33/64 loss: -0.19518178701400757
Batch 34/64 loss: -0.2120821475982666
Batch 35/64 loss: -0.19507980346679688
Batch 36/64 loss: -0.17813193798065186
Batch 37/64 loss: -0.18080949783325195
Batch 38/64 loss: -0.21085047721862793
Batch 39/64 loss: -0.19305598735809326
Batch 40/64 loss: -0.1939869523048401
Batch 41/64 loss: -0.2131742238998413
Batch 42/64 loss: -0.21415579319000244
Batch 43/64 loss: -0.18400514125823975
Batch 44/64 loss: -0.2087162733078003
Batch 45/64 loss: -0.20404517650604248
Batch 46/64 loss: -0.21670037508010864
Batch 47/64 loss: -0.19617414474487305
Batch 48/64 loss: -0.20020663738250732
Batch 49/64 loss: -0.1965346336364746
Batch 50/64 loss: -0.23005256056785583
Batch 51/64 loss: -0.2084386944770813
Batch 52/64 loss: -0.20108342170715332
Batch 53/64 loss: -0.21811747550964355
Batch 54/64 loss: -0.16112107038497925
Batch 55/64 loss: -0.1650691032409668
Batch 56/64 loss: -0.1960204839706421
Batch 57/64 loss: -0.2025255560874939
Batch 58/64 loss: -0.204382061958313
Batch 59/64 loss: -0.19592952728271484
Batch 60/64 loss: -0.21013247966766357
Batch 61/64 loss: -0.19880831241607666
Batch 62/64 loss: -0.19475126266479492
Batch 63/64 loss: -0.21716690063476562
Batch 64/64 loss: -0.1846124529838562
Epoch 355  Train loss: -0.20063577376159966  Val loss: -0.040556649367014565
Epoch 356
-------------------------------
Batch 1/64 loss: -0.22220900654792786
Batch 2/64 loss: -0.20255225896835327
Batch 3/64 loss: -0.2099076509475708
Batch 4/64 loss: -0.18330955505371094
Batch 5/64 loss: -0.2001793384552002
Batch 6/64 loss: -0.19730287790298462
Batch 7/64 loss: -0.1821277141571045
Batch 8/64 loss: -0.19522994756698608
Batch 9/64 loss: -0.21271979808807373
Batch 10/64 loss: -0.1944359540939331
Batch 11/64 loss: -0.19569295644760132
Batch 12/64 loss: -0.2137020230293274
Batch 13/64 loss: -0.21725761890411377
Batch 14/64 loss: -0.1937536597251892
Batch 15/64 loss: -0.20180881023406982
Batch 16/64 loss: -0.20363426208496094
Batch 17/64 loss: -0.2077832818031311
Batch 18/64 loss: -0.18685311079025269
Batch 19/64 loss: -0.21238523721694946
Batch 20/64 loss: -0.20725715160369873
Batch 21/64 loss: -0.19393247365951538
Batch 22/64 loss: -0.1908860206604004
Batch 23/64 loss: -0.21812427043914795
Batch 24/64 loss: -0.1976456642150879
Batch 25/64 loss: -0.213725745677948
Batch 26/64 loss: -0.20026272535324097
Batch 27/64 loss: -0.20383542776107788
Batch 28/64 loss: -0.17277944087982178
Batch 29/64 loss: -0.23669353127479553
Batch 30/64 loss: -0.2045731544494629
Batch 31/64 loss: -0.20617210865020752
Batch 32/64 loss: -0.20527982711791992
Batch 33/64 loss: -0.2082509994506836
Batch 34/64 loss: -0.20611101388931274
Batch 35/64 loss: -0.19273972511291504
Batch 36/64 loss: -0.20002400875091553
Batch 37/64 loss: -0.1935899257659912
Batch 38/64 loss: -0.193007230758667
Batch 39/64 loss: -0.17291933298110962
Batch 40/64 loss: -0.19001305103302002
Batch 41/64 loss: -0.20595091581344604
Batch 42/64 loss: -0.19150537252426147
Batch 43/64 loss: -0.2137289047241211
Batch 44/64 loss: -0.18553143739700317
Batch 45/64 loss: -0.20819693803787231
Batch 46/64 loss: -0.19270294904708862
Batch 47/64 loss: -0.2100030779838562
Batch 48/64 loss: -0.1759619116783142
Batch 49/64 loss: -0.21556943655014038
Batch 50/64 loss: -0.19493156671524048
Batch 51/64 loss: -0.2179250717163086
Batch 52/64 loss: -0.17070376873016357
Batch 53/64 loss: -0.18751674890518188
Batch 54/64 loss: -0.21332305669784546
Batch 55/64 loss: -0.19497382640838623
Batch 56/64 loss: -0.2191423773765564
Batch 57/64 loss: -0.2054610252380371
Batch 58/64 loss: -0.2061065435409546
Batch 59/64 loss: -0.17121881246566772
Batch 60/64 loss: -0.2092851996421814
Batch 61/64 loss: -0.2224501073360443
Batch 62/64 loss: -0.1844770312309265
Batch 63/64 loss: -0.21004974842071533
Batch 64/64 loss: -0.19930189847946167
Epoch 356  Train loss: -0.20073506107517317  Val loss: -0.041927018526083826
Epoch 357
-------------------------------
Batch 1/64 loss: -0.19179272651672363
Batch 2/64 loss: -0.19716209173202515
Batch 3/64 loss: -0.2157692313194275
Batch 4/64 loss: -0.2067199945449829
Batch 5/64 loss: -0.20675933361053467
Batch 6/64 loss: -0.1933918595314026
Batch 7/64 loss: -0.2059338092803955
Batch 8/64 loss: -0.19881325960159302
Batch 9/64 loss: -0.1932154893875122
Batch 10/64 loss: -0.2289348542690277
Batch 11/64 loss: -0.19174623489379883
Batch 12/64 loss: -0.22129476070404053
Batch 13/64 loss: -0.19213533401489258
Batch 14/64 loss: -0.21827274560928345
Batch 15/64 loss: -0.19026947021484375
Batch 16/64 loss: -0.21298354864120483
Batch 17/64 loss: -0.20716655254364014
Batch 18/64 loss: -0.2247980237007141
Batch 19/64 loss: -0.2037423849105835
Batch 20/64 loss: -0.20470905303955078
Batch 21/64 loss: -0.20559972524642944
Batch 22/64 loss: -0.16379404067993164
Batch 23/64 loss: -0.20774197578430176
Batch 24/64 loss: -0.1874678134918213
Batch 25/64 loss: -0.19362449645996094
Batch 26/64 loss: -0.21903765201568604
Batch 27/64 loss: -0.2157052755355835
Batch 28/64 loss: -0.18967598676681519
Batch 29/64 loss: -0.22789287567138672
Batch 30/64 loss: -0.17397308349609375
Batch 31/64 loss: -0.216528058052063
Batch 32/64 loss: -0.21605771780014038
Batch 33/64 loss: -0.22108054161071777
Batch 34/64 loss: -0.1788480281829834
Batch 35/64 loss: -0.16788214445114136
Batch 36/64 loss: -0.21997302770614624
Batch 37/64 loss: -0.2164309024810791
Batch 38/64 loss: -0.18265444040298462
Batch 39/64 loss: -0.16018462181091309
Batch 40/64 loss: -0.2124927043914795
Batch 41/64 loss: -0.2132144570350647
Batch 42/64 loss: -0.21953162550926208
Batch 43/64 loss: -0.21258211135864258
Batch 44/64 loss: -0.2147277593612671
Batch 45/64 loss: -0.1981513500213623
Batch 46/64 loss: -0.20073312520980835
Batch 47/64 loss: -0.2171412706375122
Batch 48/64 loss: -0.213758647441864
Batch 49/64 loss: -0.19029700756072998
Batch 50/64 loss: -0.21187901496887207
Batch 51/64 loss: -0.21906596422195435
Batch 52/64 loss: -0.204026460647583
Batch 53/64 loss: -0.21927329897880554
Batch 54/64 loss: -0.20215606689453125
Batch 55/64 loss: -0.21455633640289307
Batch 56/64 loss: -0.22173714637756348
Batch 57/64 loss: -0.18843770027160645
Batch 58/64 loss: -0.223344087600708
Batch 59/64 loss: -0.1887834072113037
Batch 60/64 loss: -0.20160186290740967
Batch 61/64 loss: -0.2284928262233734
Batch 62/64 loss: -0.201646625995636
Batch 63/64 loss: -0.20314228534698486
Batch 64/64 loss: -0.20683610439300537
Epoch 357  Train loss: -0.20463785704444437  Val loss: -0.037857342421803684
Epoch 358
-------------------------------
Batch 1/64 loss: -0.1958087682723999
Batch 2/64 loss: -0.21056139469146729
Batch 3/64 loss: -0.20164775848388672
Batch 4/64 loss: -0.21151185035705566
Batch 5/64 loss: -0.17359298467636108
Batch 6/64 loss: -0.2211104929447174
Batch 7/64 loss: -0.20924878120422363
Batch 8/64 loss: -0.17444932460784912
Batch 9/64 loss: -0.21895673871040344
Batch 10/64 loss: -0.20229566097259521
Batch 11/64 loss: -0.19710755348205566
Batch 12/64 loss: -0.19556885957717896
Batch 13/64 loss: -0.20117241144180298
Batch 14/64 loss: -0.19681769609451294
Batch 15/64 loss: -0.15137577056884766
Batch 16/64 loss: -0.20339810848236084
Batch 17/64 loss: -0.20973193645477295
Batch 18/64 loss: -0.1986786127090454
Batch 19/64 loss: -0.21886003017425537
Batch 20/64 loss: -0.19451981782913208
Batch 21/64 loss: -0.19136524200439453
Batch 22/64 loss: -0.2088414430618286
Batch 23/64 loss: -0.20257210731506348
Batch 24/64 loss: -0.19775331020355225
Batch 25/64 loss: -0.22249871492385864
Batch 26/64 loss: -0.22402560710906982
Batch 27/64 loss: -0.2267482876777649
Batch 28/64 loss: -0.2022535800933838
Batch 29/64 loss: -0.2137424349784851
Batch 30/64 loss: -0.20939093828201294
Batch 31/64 loss: -0.2044738531112671
Batch 32/64 loss: -0.22698867321014404
Batch 33/64 loss: -0.19628757238388062
Batch 34/64 loss: -0.20310425758361816
Batch 35/64 loss: -0.21377748250961304
Batch 36/64 loss: -0.2088039517402649
Batch 37/64 loss: -0.2094278335571289
Batch 38/64 loss: -0.20148348808288574
Batch 39/64 loss: -0.20512622594833374
Batch 40/64 loss: -0.2204245924949646
Batch 41/64 loss: -0.2114788293838501
Batch 42/64 loss: -0.1683582067489624
Batch 43/64 loss: -0.19275504350662231
Batch 44/64 loss: -0.20663785934448242
Batch 45/64 loss: -0.19227027893066406
Batch 46/64 loss: -0.18329912424087524
Batch 47/64 loss: -0.21541893482208252
Batch 48/64 loss: -0.18881845474243164
Batch 49/64 loss: -0.16096705198287964
Batch 50/64 loss: -0.15416604280471802
Batch 51/64 loss: -0.21478992700576782
Batch 52/64 loss: -0.20244455337524414
Batch 53/64 loss: -0.1850244402885437
Batch 54/64 loss: -0.20910680294036865
Batch 55/64 loss: -0.19312429428100586
Batch 56/64 loss: -0.21434468030929565
Batch 57/64 loss: -0.22112569212913513
Batch 58/64 loss: -0.17088210582733154
Batch 59/64 loss: -0.21473461389541626
Batch 60/64 loss: -0.19523727893829346
Batch 61/64 loss: -0.20689362287521362
Batch 62/64 loss: -0.203890860080719
Batch 63/64 loss: -0.21079981327056885
Batch 64/64 loss: -0.13998013734817505
Epoch 358  Train loss: -0.2007381609841889  Val loss: -0.039378755076234694
Epoch 359
-------------------------------
Batch 1/64 loss: -0.19884777069091797
Batch 2/64 loss: -0.19276773929595947
Batch 3/64 loss: -0.1839708685874939
Batch 4/64 loss: -0.19026261568069458
Batch 5/64 loss: -0.19624978303909302
Batch 6/64 loss: -0.19945895671844482
Batch 7/64 loss: -0.17890000343322754
Batch 8/64 loss: -0.2197638750076294
Batch 9/64 loss: -0.1939498782157898
Batch 10/64 loss: -0.20190465450286865
Batch 11/64 loss: -0.2215997576713562
Batch 12/64 loss: -0.18389421701431274
Batch 13/64 loss: -0.2247770130634308
Batch 14/64 loss: -0.17426836490631104
Batch 15/64 loss: -0.17878925800323486
Batch 16/64 loss: -0.21082627773284912
Batch 17/64 loss: -0.18500494956970215
Batch 18/64 loss: -0.20593613386154175
Batch 19/64 loss: -0.18493306636810303
Batch 20/64 loss: -0.21770769357681274
Batch 21/64 loss: -0.22581729292869568
Batch 22/64 loss: -0.20632809400558472
Batch 23/64 loss: -0.21912038326263428
Batch 24/64 loss: -0.19353264570236206
Batch 25/64 loss: -0.20024478435516357
Batch 26/64 loss: -0.21118181943893433
Batch 27/64 loss: -0.20699501037597656
Batch 28/64 loss: -0.2138158679008484
Batch 29/64 loss: -0.21081501245498657
Batch 30/64 loss: -0.2162068486213684
Batch 31/64 loss: -0.22483503818511963
Batch 32/64 loss: -0.1879006028175354
Batch 33/64 loss: -0.19157207012176514
Batch 34/64 loss: -0.22051486372947693
Batch 35/64 loss: -0.19717496633529663
Batch 36/64 loss: -0.2029780149459839
Batch 37/64 loss: -0.19989705085754395
Batch 38/64 loss: -0.22398042678833008
Batch 39/64 loss: -0.20900940895080566
Batch 40/64 loss: -0.21184921264648438
Batch 41/64 loss: -0.178475022315979
Batch 42/64 loss: -0.19278061389923096
Batch 43/64 loss: -0.19832968711853027
Batch 44/64 loss: -0.20893216133117676
Batch 45/64 loss: -0.21406376361846924
Batch 46/64 loss: -0.17638355493545532
Batch 47/64 loss: -0.17746472358703613
Batch 48/64 loss: -0.1833091378211975
Batch 49/64 loss: -0.2072088122367859
Batch 50/64 loss: -0.1970624327659607
Batch 51/64 loss: -0.21033531427383423
Batch 52/64 loss: -0.18784821033477783
Batch 53/64 loss: -0.20733678340911865
Batch 54/64 loss: -0.18733304738998413
Batch 55/64 loss: -0.21551334857940674
Batch 56/64 loss: -0.2156991958618164
Batch 57/64 loss: -0.2151031494140625
Batch 58/64 loss: -0.2136218547821045
Batch 59/64 loss: -0.19536930322647095
Batch 60/64 loss: -0.20750713348388672
Batch 61/64 loss: -0.21149426698684692
Batch 62/64 loss: -0.19671493768692017
Batch 63/64 loss: -0.22236773371696472
Batch 64/64 loss: -0.17233115434646606
Epoch 359  Train loss: -0.2018366250337339  Val loss: -0.04019107380273826
Epoch 360
-------------------------------
Batch 1/64 loss: -0.21911054849624634
Batch 2/64 loss: -0.16272789239883423
Batch 3/64 loss: -0.21116101741790771
Batch 4/64 loss: -0.20499169826507568
Batch 5/64 loss: -0.2084289789199829
Batch 6/64 loss: -0.20314085483551025
Batch 7/64 loss: -0.21452546119689941
Batch 8/64 loss: -0.18206405639648438
Batch 9/64 loss: -0.1727498173713684
Batch 10/64 loss: -0.2167952060699463
Batch 11/64 loss: -0.21288079023361206
Batch 12/64 loss: -0.22620174288749695
Batch 13/64 loss: -0.22337335348129272
Batch 14/64 loss: -0.21084308624267578
Batch 15/64 loss: -0.1777726411819458
Batch 16/64 loss: -0.23097935318946838
Batch 17/64 loss: -0.20807266235351562
Batch 18/64 loss: -0.2206958830356598
Batch 19/64 loss: -0.19646483659744263
Batch 20/64 loss: -0.1948758363723755
Batch 21/64 loss: -0.19062167406082153
Batch 22/64 loss: -0.21411895751953125
Batch 23/64 loss: -0.20603150129318237
Batch 24/64 loss: -0.21051180362701416
Batch 25/64 loss: -0.20972293615341187
Batch 26/64 loss: -0.22415056824684143
Batch 27/64 loss: -0.21022790670394897
Batch 28/64 loss: -0.1841004490852356
Batch 29/64 loss: -0.18115592002868652
Batch 30/64 loss: -0.174197256565094
Batch 31/64 loss: -0.20995086431503296
Batch 32/64 loss: -0.1853366494178772
Batch 33/64 loss: -0.23008644580841064
Batch 34/64 loss: -0.18917477130889893
Batch 35/64 loss: -0.20497095584869385
Batch 36/64 loss: -0.22676929831504822
Batch 37/64 loss: -0.22653669118881226
Batch 38/64 loss: -0.21660125255584717
Batch 39/64 loss: -0.22287502884864807
Batch 40/64 loss: -0.17046940326690674
Batch 41/64 loss: -0.22251221537590027
Batch 42/64 loss: -0.22869932651519775
Batch 43/64 loss: -0.19906491041183472
Batch 44/64 loss: -0.1953049898147583
Batch 45/64 loss: -0.20185810327529907
Batch 46/64 loss: -0.2086825966835022
Batch 47/64 loss: -0.20204651355743408
Batch 48/64 loss: -0.21482080221176147
Batch 49/64 loss: -0.21236532926559448
Batch 50/64 loss: -0.20597249269485474
Batch 51/64 loss: -0.21017158031463623
Batch 52/64 loss: -0.2107332944869995
Batch 53/64 loss: -0.21169358491897583
Batch 54/64 loss: -0.1982412338256836
Batch 55/64 loss: -0.1998336911201477
Batch 56/64 loss: -0.18485593795776367
Batch 57/64 loss: -0.19551098346710205
Batch 58/64 loss: -0.21000885963439941
Batch 59/64 loss: -0.20312440395355225
Batch 60/64 loss: -0.2217777967453003
Batch 61/64 loss: -0.21309101581573486
Batch 62/64 loss: -0.2029482126235962
Batch 63/64 loss: -0.20570695400238037
Batch 64/64 loss: -0.20566695928573608
Epoch 360  Train loss: -0.20547044861550426  Val loss: -0.041479426151288745
Epoch 361
-------------------------------
Batch 1/64 loss: -0.18972772359848022
Batch 2/64 loss: -0.19383752346038818
Batch 3/64 loss: -0.2215029001235962
Batch 4/64 loss: -0.20009636878967285
Batch 5/64 loss: -0.18957722187042236
Batch 6/64 loss: -0.21732419729232788
Batch 7/64 loss: -0.1841953992843628
Batch 8/64 loss: -0.2237817943096161
Batch 9/64 loss: -0.2109915018081665
Batch 10/64 loss: -0.1992173194885254
Batch 11/64 loss: -0.21591335535049438
Batch 12/64 loss: -0.21772903203964233
Batch 13/64 loss: -0.21314895153045654
Batch 14/64 loss: -0.22999238967895508
Batch 15/64 loss: -0.19424289464950562
Batch 16/64 loss: -0.17455703020095825
Batch 17/64 loss: -0.20334303379058838
Batch 18/64 loss: -0.21295952796936035
Batch 19/64 loss: -0.19128447771072388
Batch 20/64 loss: -0.17852574586868286
Batch 21/64 loss: -0.20574021339416504
Batch 22/64 loss: -0.21011584997177124
Batch 23/64 loss: -0.2178056836128235
Batch 24/64 loss: -0.1981264352798462
Batch 25/64 loss: -0.18991971015930176
Batch 26/64 loss: -0.20941424369812012
Batch 27/64 loss: -0.1689344048500061
Batch 28/64 loss: -0.2092881202697754
Batch 29/64 loss: -0.2053622007369995
Batch 30/64 loss: -0.22270649671554565
Batch 31/64 loss: -0.20877206325531006
Batch 32/64 loss: -0.20363986492156982
Batch 33/64 loss: -0.20313680171966553
Batch 34/64 loss: -0.1995631456375122
Batch 35/64 loss: -0.20127594470977783
Batch 36/64 loss: -0.20197683572769165
Batch 37/64 loss: -0.16874760389328003
Batch 38/64 loss: -0.18218636512756348
Batch 39/64 loss: -0.2077162265777588
Batch 40/64 loss: -0.2085409164428711
Batch 41/64 loss: -0.20155799388885498
Batch 42/64 loss: -0.21243298053741455
Batch 43/64 loss: -0.20694178342819214
Batch 44/64 loss: -0.2158374786376953
Batch 45/64 loss: -0.21266686916351318
Batch 46/64 loss: -0.1944054365158081
Batch 47/64 loss: -0.18213284015655518
Batch 48/64 loss: -0.2063523530960083
Batch 49/64 loss: -0.1989126205444336
Batch 50/64 loss: -0.20499825477600098
Batch 51/64 loss: -0.21238350868225098
Batch 52/64 loss: -0.17101460695266724
Batch 53/64 loss: -0.1761012077331543
Batch 54/64 loss: -0.1539270281791687
Batch 55/64 loss: -0.20700979232788086
Batch 56/64 loss: -0.2100210189819336
Batch 57/64 loss: -0.17663335800170898
Batch 58/64 loss: -0.21531647443771362
Batch 59/64 loss: -0.21651005744934082
Batch 60/64 loss: -0.1971433162689209
Batch 61/64 loss: -0.20495206117630005
Batch 62/64 loss: -0.2195912003517151
Batch 63/64 loss: -0.20757484436035156
Batch 64/64 loss: -0.21462970972061157
Epoch 361  Train loss: -0.20157344130908741  Val loss: -0.03967458112133328
Epoch 362
-------------------------------
Batch 1/64 loss: -0.2035619020462036
Batch 2/64 loss: -0.20699268579483032
Batch 3/64 loss: -0.204037606716156
Batch 4/64 loss: -0.19376397132873535
Batch 5/64 loss: -0.238128662109375
Batch 6/64 loss: -0.19441086053848267
Batch 7/64 loss: -0.22053605318069458
Batch 8/64 loss: -0.21264517307281494
Batch 9/64 loss: -0.2059011459350586
Batch 10/64 loss: -0.2143658995628357
Batch 11/64 loss: -0.17056804895401
Batch 12/64 loss: -0.1901302933692932
Batch 13/64 loss: -0.21968919038772583
Batch 14/64 loss: -0.20499849319458008
Batch 15/64 loss: -0.17971140146255493
Batch 16/64 loss: -0.21427661180496216
Batch 17/64 loss: -0.20783907175064087
Batch 18/64 loss: -0.223975270986557
Batch 19/64 loss: -0.21500808000564575
Batch 20/64 loss: -0.20813238620758057
Batch 21/64 loss: -0.20795345306396484
Batch 22/64 loss: -0.2123124599456787
Batch 23/64 loss: -0.18330776691436768
Batch 24/64 loss: -0.16993749141693115
Batch 25/64 loss: -0.21128666400909424
Batch 26/64 loss: -0.1926712989807129
Batch 27/64 loss: -0.21424448490142822
Batch 28/64 loss: -0.1925545334815979
Batch 29/64 loss: -0.19520634412765503
Batch 30/64 loss: -0.21091681718826294
Batch 31/64 loss: -0.1919388771057129
Batch 32/64 loss: -0.18983811140060425
Batch 33/64 loss: -0.2139066457748413
Batch 34/64 loss: -0.22646698355674744
Batch 35/64 loss: -0.171004056930542
Batch 36/64 loss: -0.2109861969947815
Batch 37/64 loss: -0.19544219970703125
Batch 38/64 loss: -0.19970011711120605
Batch 39/64 loss: -0.2192264199256897
Batch 40/64 loss: -0.21386045217514038
Batch 41/64 loss: -0.20439815521240234
Batch 42/64 loss: -0.18934184312820435
Batch 43/64 loss: -0.1391696333885193
Batch 44/64 loss: -0.2217860221862793
Batch 45/64 loss: -0.19801276922225952
Batch 46/64 loss: -0.21570324897766113
Batch 47/64 loss: -0.211236834526062
Batch 48/64 loss: -0.2165181040763855
Batch 49/64 loss: -0.19743841886520386
Batch 50/64 loss: -0.20976924896240234
Batch 51/64 loss: -0.20486122369766235
Batch 52/64 loss: -0.2175481915473938
Batch 53/64 loss: -0.19778406620025635
Batch 54/64 loss: -0.2130749225616455
Batch 55/64 loss: -0.214455246925354
Batch 56/64 loss: -0.19818717241287231
Batch 57/64 loss: -0.20410704612731934
Batch 58/64 loss: -0.20299410820007324
Batch 59/64 loss: -0.19749009609222412
Batch 60/64 loss: -0.18233132362365723
Batch 61/64 loss: -0.19803667068481445
Batch 62/64 loss: -0.2154061198234558
Batch 63/64 loss: -0.21677720546722412
Batch 64/64 loss: -0.1979251503944397
Epoch 362  Train loss: -0.2033930308678571  Val loss: -0.04146215841942227
Epoch 363
-------------------------------
Batch 1/64 loss: -0.20458459854125977
Batch 2/64 loss: -0.2261277139186859
Batch 3/64 loss: -0.20713144540786743
Batch 4/64 loss: -0.22270354628562927
Batch 5/64 loss: -0.1869702935218811
Batch 6/64 loss: -0.21977084875106812
Batch 7/64 loss: -0.19392770528793335
Batch 8/64 loss: -0.21575522422790527
Batch 9/64 loss: -0.20982319116592407
Batch 10/64 loss: -0.18255221843719482
Batch 11/64 loss: -0.21160799264907837
Batch 12/64 loss: -0.22843718528747559
Batch 13/64 loss: -0.210129976272583
Batch 14/64 loss: -0.1900424361228943
Batch 15/64 loss: -0.1985417604446411
Batch 16/64 loss: -0.2134431004524231
Batch 17/64 loss: -0.19857531785964966
Batch 18/64 loss: -0.21236175298690796
Batch 19/64 loss: -0.198633074760437
Batch 20/64 loss: -0.2074984312057495
Batch 21/64 loss: -0.22712576389312744
Batch 22/64 loss: -0.22922277450561523
Batch 23/64 loss: -0.18949496746063232
Batch 24/64 loss: -0.21173399686813354
Batch 25/64 loss: -0.1872185468673706
Batch 26/64 loss: -0.2154545783996582
Batch 27/64 loss: -0.2127249836921692
Batch 28/64 loss: -0.19644373655319214
Batch 29/64 loss: -0.20144277811050415
Batch 30/64 loss: -0.20988893508911133
Batch 31/64 loss: -0.20131874084472656
Batch 32/64 loss: -0.2231295108795166
Batch 33/64 loss: -0.2012731432914734
Batch 34/64 loss: -0.21500080823898315
Batch 35/64 loss: -0.18580776453018188
Batch 36/64 loss: -0.1820855736732483
Batch 37/64 loss: -0.21324622631072998
Batch 38/64 loss: -0.1977291703224182
Batch 39/64 loss: -0.20267236232757568
Batch 40/64 loss: -0.21360623836517334
Batch 41/64 loss: -0.21099478006362915
Batch 42/64 loss: -0.23026880621910095
Batch 43/64 loss: -0.20224696397781372
Batch 44/64 loss: -0.20668011903762817
Batch 45/64 loss: -0.21912556886672974
Batch 46/64 loss: -0.19869095087051392
Batch 47/64 loss: -0.18848568201065063
Batch 48/64 loss: -0.22308430075645447
Batch 49/64 loss: -0.20161545276641846
Batch 50/64 loss: -0.19684815406799316
Batch 51/64 loss: -0.22678512334823608
Batch 52/64 loss: -0.21099567413330078
Batch 53/64 loss: -0.19300991296768188
Batch 54/64 loss: -0.21213901042938232
Batch 55/64 loss: -0.21224528551101685
Batch 56/64 loss: -0.21023094654083252
Batch 57/64 loss: -0.1813298463821411
Batch 58/64 loss: -0.20159488916397095
Batch 59/64 loss: -0.19656717777252197
Batch 60/64 loss: -0.19772976636886597
Batch 61/64 loss: -0.1754271388053894
Batch 62/64 loss: -0.2182091474533081
Batch 63/64 loss: -0.1823437213897705
Batch 64/64 loss: -0.17535126209259033
Epoch 363  Train loss: -0.20522980830248663  Val loss: -0.04055208193067832
Epoch 364
-------------------------------
Batch 1/64 loss: -0.19581210613250732
Batch 2/64 loss: -0.15494763851165771
Batch 3/64 loss: -0.1957910656929016
Batch 4/64 loss: -0.19618213176727295
Batch 5/64 loss: -0.2090335488319397
Batch 6/64 loss: -0.20476031303405762
Batch 7/64 loss: -0.21215081214904785
Batch 8/64 loss: -0.2050066590309143
Batch 9/64 loss: -0.1992276906967163
Batch 10/64 loss: -0.2060646414756775
Batch 11/64 loss: -0.1779165267944336
Batch 12/64 loss: -0.2245641052722931
Batch 13/64 loss: -0.20707136392593384
Batch 14/64 loss: -0.22775763273239136
Batch 15/64 loss: -0.20808571577072144
Batch 16/64 loss: -0.20947396755218506
Batch 17/64 loss: -0.20577049255371094
Batch 18/64 loss: -0.17838561534881592
Batch 19/64 loss: -0.20016145706176758
Batch 20/64 loss: -0.20408934354782104
Batch 21/64 loss: -0.21843484044075012
Batch 22/64 loss: -0.22349709272384644
Batch 23/64 loss: -0.19937825202941895
Batch 24/64 loss: -0.1809442639350891
Batch 25/64 loss: -0.19992077350616455
Batch 26/64 loss: -0.21468472480773926
Batch 27/64 loss: -0.2029135823249817
Batch 28/64 loss: -0.19195449352264404
Batch 29/64 loss: -0.180436372756958
Batch 30/64 loss: -0.2222144603729248
Batch 31/64 loss: -0.20542383193969727
Batch 32/64 loss: -0.21891504526138306
Batch 33/64 loss: -0.1706518530845642
Batch 34/64 loss: -0.19772392511367798
Batch 35/64 loss: -0.1796523928642273
Batch 36/64 loss: -0.2183576226234436
Batch 37/64 loss: -0.19022440910339355
Batch 38/64 loss: -0.19907522201538086
Batch 39/64 loss: -0.21247631311416626
Batch 40/64 loss: -0.21238619089126587
Batch 41/64 loss: -0.18425416946411133
Batch 42/64 loss: -0.21618521213531494
Batch 43/64 loss: -0.20389539003372192
Batch 44/64 loss: -0.22457051277160645
Batch 45/64 loss: -0.22485136985778809
Batch 46/64 loss: -0.2258981466293335
Batch 47/64 loss: -0.1872844696044922
Batch 48/64 loss: -0.21573078632354736
Batch 49/64 loss: -0.20639383792877197
Batch 50/64 loss: -0.2133769989013672
Batch 51/64 loss: -0.21481889486312866
Batch 52/64 loss: -0.20616871118545532
Batch 53/64 loss: -0.19993841648101807
Batch 54/64 loss: -0.1585465669631958
Batch 55/64 loss: -0.20415616035461426
Batch 56/64 loss: -0.16154563426971436
Batch 57/64 loss: -0.2134089469909668
Batch 58/64 loss: -0.198918879032135
Batch 59/64 loss: -0.2136225700378418
Batch 60/64 loss: -0.2266247570514679
Batch 61/64 loss: -0.16111350059509277
Batch 62/64 loss: -0.22305327653884888
Batch 63/64 loss: -0.20090961456298828
Batch 64/64 loss: -0.19292330741882324
Epoch 364  Train loss: -0.2022192595051784  Val loss: -0.04107663393839938
Epoch 365
-------------------------------
Batch 1/64 loss: -0.22196948528289795
Batch 2/64 loss: -0.21189647912979126
Batch 3/64 loss: -0.21312326192855835
Batch 4/64 loss: -0.21928435564041138
Batch 5/64 loss: -0.21531355381011963
Batch 6/64 loss: -0.21620428562164307
Batch 7/64 loss: -0.21060526371002197
Batch 8/64 loss: -0.2180446982383728
Batch 9/64 loss: -0.205150306224823
Batch 10/64 loss: -0.18980836868286133
Batch 11/64 loss: -0.21819108724594116
Batch 12/64 loss: -0.21675771474838257
Batch 13/64 loss: -0.21574103832244873
Batch 14/64 loss: -0.20333027839660645
Batch 15/64 loss: -0.20311295986175537
Batch 16/64 loss: -0.20231682062149048
Batch 17/64 loss: -0.2082471251487732
Batch 18/64 loss: -0.19779425859451294
Batch 19/64 loss: -0.217892587184906
Batch 20/64 loss: -0.21352648735046387
Batch 21/64 loss: -0.2119951844215393
Batch 22/64 loss: -0.22409087419509888
Batch 23/64 loss: -0.16625475883483887
Batch 24/64 loss: -0.19661206007003784
Batch 25/64 loss: -0.2149636149406433
Batch 26/64 loss: -0.2030673623085022
Batch 27/64 loss: -0.21462374925613403
Batch 28/64 loss: -0.1940293312072754
Batch 29/64 loss: -0.211769700050354
Batch 30/64 loss: -0.19086313247680664
Batch 31/64 loss: -0.21672511100769043
Batch 32/64 loss: -0.2164773941040039
Batch 33/64 loss: -0.21858099102973938
Batch 34/64 loss: -0.2174031138420105
Batch 35/64 loss: -0.2056826949119568
Batch 36/64 loss: -0.1823657751083374
Batch 37/64 loss: -0.16989481449127197
Batch 38/64 loss: -0.22088655829429626
Batch 39/64 loss: -0.21880537271499634
Batch 40/64 loss: -0.20874369144439697
Batch 41/64 loss: -0.20381921529769897
Batch 42/64 loss: -0.18190622329711914
Batch 43/64 loss: -0.21289783716201782
Batch 44/64 loss: -0.1689712405204773
Batch 45/64 loss: -0.19476795196533203
Batch 46/64 loss: -0.21907156705856323
Batch 47/64 loss: -0.21310675144195557
Batch 48/64 loss: -0.19651806354522705
Batch 49/64 loss: -0.20516014099121094
Batch 50/64 loss: -0.1891723871231079
Batch 51/64 loss: -0.1910971999168396
Batch 52/64 loss: -0.20344537496566772
Batch 53/64 loss: -0.18381750583648682
Batch 54/64 loss: -0.21278995275497437
Batch 55/64 loss: -0.20850098133087158
Batch 56/64 loss: -0.22158056497573853
Batch 57/64 loss: -0.20605510473251343
Batch 58/64 loss: -0.19395679235458374
Batch 59/64 loss: -0.20999127626419067
Batch 60/64 loss: -0.2091660499572754
Batch 61/64 loss: -0.20548295974731445
Batch 62/64 loss: -0.20238780975341797
Batch 63/64 loss: -0.18930113315582275
Batch 64/64 loss: -0.19854587316513062
Epoch 365  Train loss: -0.20539637944277594  Val loss: -0.040971282421518436
Epoch 366
-------------------------------
Batch 1/64 loss: -0.18410354852676392
Batch 2/64 loss: -0.21237826347351074
Batch 3/64 loss: -0.22989565134048462
Batch 4/64 loss: -0.2183154821395874
Batch 5/64 loss: -0.20546233654022217
Batch 6/64 loss: -0.22789722681045532
Batch 7/64 loss: -0.19584685564041138
Batch 8/64 loss: -0.2208886742591858
Batch 9/64 loss: -0.18777799606323242
Batch 10/64 loss: -0.19175124168395996
Batch 11/64 loss: -0.15221655368804932
Batch 12/64 loss: -0.20116674900054932
Batch 13/64 loss: -0.18111932277679443
Batch 14/64 loss: -0.224204421043396
Batch 15/64 loss: -0.16515368223190308
Batch 16/64 loss: -0.2082754373550415
Batch 17/64 loss: -0.18105947971343994
Batch 18/64 loss: -0.21319079399108887
Batch 19/64 loss: -0.2090085744857788
Batch 20/64 loss: -0.2150149941444397
Batch 21/64 loss: -0.21593904495239258
Batch 22/64 loss: -0.21864622831344604
Batch 23/64 loss: -0.22272342443466187
Batch 24/64 loss: -0.22825032472610474
Batch 25/64 loss: -0.20938646793365479
Batch 26/64 loss: -0.21364498138427734
Batch 27/64 loss: -0.18234467506408691
Batch 28/64 loss: -0.2227122187614441
Batch 29/64 loss: -0.19979584217071533
Batch 30/64 loss: -0.2175682783126831
Batch 31/64 loss: -0.20990538597106934
Batch 32/64 loss: -0.19972234964370728
Batch 33/64 loss: -0.23001456260681152
Batch 34/64 loss: -0.20957845449447632
Batch 35/64 loss: -0.21779242157936096
Batch 36/64 loss: -0.19996821880340576
Batch 37/64 loss: -0.20665383338928223
Batch 38/64 loss: -0.21223008632659912
Batch 39/64 loss: -0.20261609554290771
Batch 40/64 loss: -0.2156052589416504
Batch 41/64 loss: -0.19906830787658691
Batch 42/64 loss: -0.2157084345817566
Batch 43/64 loss: -0.20360130071640015
Batch 44/64 loss: -0.21822234988212585
Batch 45/64 loss: -0.21450865268707275
Batch 46/64 loss: -0.2086925506591797
Batch 47/64 loss: -0.22568851709365845
Batch 48/64 loss: -0.1862107515335083
Batch 49/64 loss: -0.19613665342330933
Batch 50/64 loss: -0.17555904388427734
Batch 51/64 loss: -0.19722437858581543
Batch 52/64 loss: -0.20296257734298706
Batch 53/64 loss: -0.20005953311920166
Batch 54/64 loss: -0.19935500621795654
Batch 55/64 loss: -0.1929733157157898
Batch 56/64 loss: -0.218921959400177
Batch 57/64 loss: -0.21642297506332397
Batch 58/64 loss: -0.2249903678894043
Batch 59/64 loss: -0.20405995845794678
Batch 60/64 loss: -0.21864911913871765
Batch 61/64 loss: -0.21947157382965088
Batch 62/64 loss: -0.2159045934677124
Batch 63/64 loss: -0.2067509889602661
Batch 64/64 loss: -0.2055395245552063
Epoch 366  Train loss: -0.206668595940459  Val loss: -0.040378107852542526
Epoch 367
-------------------------------
Batch 1/64 loss: -0.22346439957618713
Batch 2/64 loss: -0.21423524618148804
Batch 3/64 loss: -0.24448174238204956
Batch 4/64 loss: -0.2002326250076294
Batch 5/64 loss: -0.22010532021522522
Batch 6/64 loss: -0.20703458786010742
Batch 7/64 loss: -0.18546098470687866
Batch 8/64 loss: -0.21575534343719482
Batch 9/64 loss: -0.2194579541683197
Batch 10/64 loss: -0.22427412867546082
Batch 11/64 loss: -0.18846344947814941
Batch 12/64 loss: -0.2067091464996338
Batch 13/64 loss: -0.19055181741714478
Batch 14/64 loss: -0.1901235580444336
Batch 15/64 loss: -0.22197246551513672
Batch 16/64 loss: -0.21612823009490967
Batch 17/64 loss: -0.19141972064971924
Batch 18/64 loss: -0.20396006107330322
Batch 19/64 loss: -0.21375662088394165
Batch 20/64 loss: -0.21488934755325317
Batch 21/64 loss: -0.17285490036010742
Batch 22/64 loss: -0.18959873914718628
Batch 23/64 loss: -0.21518653631210327
Batch 24/64 loss: -0.1965862512588501
Batch 25/64 loss: -0.21165311336517334
Batch 26/64 loss: -0.19036906957626343
Batch 27/64 loss: -0.21993663907051086
Batch 28/64 loss: -0.20045709609985352
Batch 29/64 loss: -0.206915020942688
Batch 30/64 loss: -0.21464645862579346
Batch 31/64 loss: -0.21660423278808594
Batch 32/64 loss: -0.2073972225189209
Batch 33/64 loss: -0.21844613552093506
Batch 34/64 loss: -0.1969442367553711
Batch 35/64 loss: -0.22544723749160767
Batch 36/64 loss: -0.23731958866119385
Batch 37/64 loss: -0.18919026851654053
Batch 38/64 loss: -0.17993396520614624
Batch 39/64 loss: -0.20430141687393188
Batch 40/64 loss: -0.21201646327972412
Batch 41/64 loss: -0.19455170631408691
Batch 42/64 loss: -0.2185373306274414
Batch 43/64 loss: -0.21827054023742676
Batch 44/64 loss: -0.20016425848007202
Batch 45/64 loss: -0.21766898036003113
Batch 46/64 loss: -0.20074129104614258
Batch 47/64 loss: -0.19901466369628906
Batch 48/64 loss: -0.2357720136642456
Batch 49/64 loss: -0.1868981122970581
Batch 50/64 loss: -0.19511985778808594
Batch 51/64 loss: -0.2190735936164856
Batch 52/64 loss: -0.22574716806411743
Batch 53/64 loss: -0.22066453099250793
Batch 54/64 loss: -0.19451338052749634
Batch 55/64 loss: -0.20073312520980835
Batch 56/64 loss: -0.21975445747375488
Batch 57/64 loss: -0.22709158062934875
Batch 58/64 loss: -0.2122012972831726
Batch 59/64 loss: -0.1734224557876587
Batch 60/64 loss: -0.2195296287536621
Batch 61/64 loss: -0.18618309497833252
Batch 62/64 loss: -0.20350384712219238
Batch 63/64 loss: -0.18210935592651367
Batch 64/64 loss: -0.21284276247024536
Epoch 367  Train loss: -0.20720281858070225  Val loss: -0.037981652107435405
Epoch 368
-------------------------------
Batch 1/64 loss: -0.22411134839057922
Batch 2/64 loss: -0.20088672637939453
Batch 3/64 loss: -0.2218450903892517
Batch 4/64 loss: -0.19415372610092163
Batch 5/64 loss: -0.21611779928207397
Batch 6/64 loss: -0.21195948123931885
Batch 7/64 loss: -0.20402806997299194
Batch 8/64 loss: -0.2366611361503601
Batch 9/64 loss: -0.2169792652130127
Batch 10/64 loss: -0.18785423040390015
Batch 11/64 loss: -0.21416479349136353
Batch 12/64 loss: -0.1957380175590515
Batch 13/64 loss: -0.21762901544570923
Batch 14/64 loss: -0.20880800485610962
Batch 15/64 loss: -0.21123629808425903
Batch 16/64 loss: -0.19858205318450928
Batch 17/64 loss: -0.1985822319984436
Batch 18/64 loss: -0.17453384399414062
Batch 19/64 loss: -0.21175307035446167
Batch 20/64 loss: -0.21300506591796875
Batch 21/64 loss: -0.22269898653030396
Batch 22/64 loss: -0.21529114246368408
Batch 23/64 loss: -0.21633809804916382
Batch 24/64 loss: -0.22259289026260376
Batch 25/64 loss: -0.16638755798339844
Batch 26/64 loss: -0.23041951656341553
Batch 27/64 loss: -0.2024746537208557
Batch 28/64 loss: -0.20485526323318481
Batch 29/64 loss: -0.22813034057617188
Batch 30/64 loss: -0.20096057653427124
Batch 31/64 loss: -0.2301557958126068
Batch 32/64 loss: -0.21116578578948975
Batch 33/64 loss: -0.21935024857521057
Batch 34/64 loss: -0.21141952276229858
Batch 35/64 loss: -0.1900208592414856
Batch 36/64 loss: -0.21464985609054565
Batch 37/64 loss: -0.18311089277267456
Batch 38/64 loss: -0.18730151653289795
Batch 39/64 loss: -0.19257378578186035
Batch 40/64 loss: -0.2126408815383911
Batch 41/64 loss: -0.22525566816329956
Batch 42/64 loss: -0.19768327474594116
Batch 43/64 loss: -0.17888641357421875
Batch 44/64 loss: -0.19732427597045898
Batch 45/64 loss: -0.20431190729141235
Batch 46/64 loss: -0.1924254298210144
Batch 47/64 loss: -0.20588338375091553
Batch 48/64 loss: -0.2136305570602417
Batch 49/64 loss: -0.21739867329597473
Batch 50/64 loss: -0.21491014957427979
Batch 51/64 loss: -0.20044022798538208
Batch 52/64 loss: -0.20847874879837036
Batch 53/64 loss: -0.19808733463287354
Batch 54/64 loss: -0.18882811069488525
Batch 55/64 loss: -0.19812512397766113
Batch 56/64 loss: -0.19931483268737793
Batch 57/64 loss: -0.22395896911621094
Batch 58/64 loss: -0.22070473432540894
Batch 59/64 loss: -0.17946600914001465
Batch 60/64 loss: -0.19981050491333008
Batch 61/64 loss: -0.20177078247070312
Batch 62/64 loss: -0.19263464212417603
Batch 63/64 loss: -0.1947198510169983
Batch 64/64 loss: -0.2119399905204773
Epoch 368  Train loss: -0.20602622803519755  Val loss: -0.037324361579934344
Epoch 369
-------------------------------
Batch 1/64 loss: -0.2235720455646515
Batch 2/64 loss: -0.19891345500946045
Batch 3/64 loss: -0.1861788034439087
Batch 4/64 loss: -0.20695966482162476
Batch 5/64 loss: -0.18628937005996704
Batch 6/64 loss: -0.19814372062683105
Batch 7/64 loss: -0.2182391881942749
Batch 8/64 loss: -0.22297430038452148
Batch 9/64 loss: -0.21944516897201538
Batch 10/64 loss: -0.2284514307975769
Batch 11/64 loss: -0.17761236429214478
Batch 12/64 loss: -0.19937098026275635
Batch 13/64 loss: -0.19269484281539917
Batch 14/64 loss: -0.20202261209487915
Batch 15/64 loss: -0.2176876664161682
Batch 16/64 loss: -0.1851334571838379
Batch 17/64 loss: -0.18755143880844116
Batch 18/64 loss: -0.20261740684509277
Batch 19/64 loss: -0.22320008277893066
Batch 20/64 loss: -0.20788627862930298
Batch 21/64 loss: -0.20539391040802002
Batch 22/64 loss: -0.21857023239135742
Batch 23/64 loss: -0.21876448392868042
Batch 24/64 loss: -0.190765380859375
Batch 25/64 loss: -0.17570853233337402
Batch 26/64 loss: -0.18975281715393066
Batch 27/64 loss: -0.1928432583808899
Batch 28/64 loss: -0.21460986137390137
Batch 29/64 loss: -0.21730148792266846
Batch 30/64 loss: -0.20942986011505127
Batch 31/64 loss: -0.18260550498962402
Batch 32/64 loss: -0.21203410625457764
Batch 33/64 loss: -0.2057591676712036
Batch 34/64 loss: -0.19002145528793335
Batch 35/64 loss: -0.1826527714729309
Batch 36/64 loss: -0.20527833700180054
Batch 37/64 loss: -0.20157617330551147
Batch 38/64 loss: -0.17993587255477905
Batch 39/64 loss: -0.20289522409439087
Batch 40/64 loss: -0.2351599633693695
Batch 41/64 loss: -0.20242011547088623
Batch 42/64 loss: -0.21494489908218384
Batch 43/64 loss: -0.19713205099105835
Batch 44/64 loss: -0.21995079517364502
Batch 45/64 loss: -0.20870250463485718
Batch 46/64 loss: -0.20877277851104736
Batch 47/64 loss: -0.215584397315979
Batch 48/64 loss: -0.19454175233840942
Batch 49/64 loss: -0.16483479738235474
Batch 50/64 loss: -0.18554502725601196
Batch 51/64 loss: -0.22350358963012695
Batch 52/64 loss: -0.22552180290222168
Batch 53/64 loss: -0.2061631679534912
Batch 54/64 loss: -0.2119067907333374
Batch 55/64 loss: -0.20342183113098145
Batch 56/64 loss: -0.20910018682479858
Batch 57/64 loss: -0.2098575234413147
Batch 58/64 loss: -0.18020200729370117
Batch 59/64 loss: -0.2053772211074829
Batch 60/64 loss: -0.206792950630188
Batch 61/64 loss: -0.22950389981269836
Batch 62/64 loss: -0.20375502109527588
Batch 63/64 loss: -0.21124356985092163
Batch 64/64 loss: -0.20842838287353516
Epoch 369  Train loss: -0.20409572778963575  Val loss: -0.03967742432433715
Epoch 370
-------------------------------
Batch 1/64 loss: -0.2219265103340149
Batch 2/64 loss: -0.22796660661697388
Batch 3/64 loss: -0.1861979365348816
Batch 4/64 loss: -0.22664105892181396
Batch 5/64 loss: -0.19924390316009521
Batch 6/64 loss: -0.2327999472618103
Batch 7/64 loss: -0.23065224289894104
Batch 8/64 loss: -0.22298887372016907
Batch 9/64 loss: -0.2254827320575714
Batch 10/64 loss: -0.21628978848457336
Batch 11/64 loss: -0.20269685983657837
Batch 12/64 loss: -0.20150011777877808
Batch 13/64 loss: -0.1981012225151062
Batch 14/64 loss: -0.20400470495224
Batch 15/64 loss: -0.20022058486938477
Batch 16/64 loss: -0.202775776386261
Batch 17/64 loss: -0.20611608028411865
Batch 18/64 loss: -0.2050466537475586
Batch 19/64 loss: -0.1814192533493042
Batch 20/64 loss: -0.21783041954040527
Batch 21/64 loss: -0.20272421836853027
Batch 22/64 loss: -0.21659862995147705
Batch 23/64 loss: -0.1649450659751892
Batch 24/64 loss: -0.22709286212921143
Batch 25/64 loss: -0.19806766510009766
Batch 26/64 loss: -0.21564388275146484
Batch 27/64 loss: -0.1900222897529602
Batch 28/64 loss: -0.17108654975891113
Batch 29/64 loss: -0.2066238522529602
Batch 30/64 loss: -0.19770407676696777
Batch 31/64 loss: -0.22839069366455078
Batch 32/64 loss: -0.2147698998451233
Batch 33/64 loss: -0.21373379230499268
Batch 34/64 loss: -0.21921712160110474
Batch 35/64 loss: -0.20290327072143555
Batch 36/64 loss: -0.18578946590423584
Batch 37/64 loss: -0.18720579147338867
Batch 38/64 loss: -0.18787860870361328
Batch 39/64 loss: -0.18208467960357666
Batch 40/64 loss: -0.20183926820755005
Batch 41/64 loss: -0.1720539927482605
Batch 42/64 loss: -0.2089117169380188
Batch 43/64 loss: -0.230665385723114
Batch 44/64 loss: -0.20269012451171875
Batch 45/64 loss: -0.20545035600662231
Batch 46/64 loss: -0.2214566469192505
Batch 47/64 loss: -0.19121474027633667
Batch 48/64 loss: -0.1885187029838562
Batch 49/64 loss: -0.19598233699798584
Batch 50/64 loss: -0.21666359901428223
Batch 51/64 loss: -0.20695900917053223
Batch 52/64 loss: -0.21807608008384705
Batch 53/64 loss: -0.21556735038757324
Batch 54/64 loss: -0.20934289693832397
Batch 55/64 loss: -0.2117220163345337
Batch 56/64 loss: -0.18592387437820435
Batch 57/64 loss: -0.21878668665885925
Batch 58/64 loss: -0.20957547426223755
Batch 59/64 loss: -0.2097855806350708
Batch 60/64 loss: -0.21455514430999756
Batch 61/64 loss: -0.2108616828918457
Batch 62/64 loss: -0.16882586479187012
Batch 63/64 loss: -0.21060162782669067
Batch 64/64 loss: -0.1657109260559082
Epoch 370  Train loss: -0.20506191440657073  Val loss: -0.04138394237793598
Epoch 371
-------------------------------
Batch 1/64 loss: -0.22686925530433655
Batch 2/64 loss: -0.21580612659454346
Batch 3/64 loss: -0.2113887071609497
Batch 4/64 loss: -0.1805989146232605
Batch 5/64 loss: -0.22203177213668823
Batch 6/64 loss: -0.20922523736953735
Batch 7/64 loss: -0.1972752809524536
Batch 8/64 loss: -0.20986014604568481
Batch 9/64 loss: -0.20899063348770142
Batch 10/64 loss: -0.2136683464050293
Batch 11/64 loss: -0.1942010521888733
Batch 12/64 loss: -0.22703883051872253
Batch 13/64 loss: -0.21708190441131592
Batch 14/64 loss: -0.19463622570037842
Batch 15/64 loss: -0.2151116132736206
Batch 16/64 loss: -0.22134608030319214
Batch 17/64 loss: -0.21482086181640625
Batch 18/64 loss: -0.20824956893920898
Batch 19/64 loss: -0.22328516840934753
Batch 20/64 loss: -0.20224076509475708
Batch 21/64 loss: -0.19367718696594238
Batch 22/64 loss: -0.20648646354675293
Batch 23/64 loss: -0.19971466064453125
Batch 24/64 loss: -0.19526588916778564
Batch 25/64 loss: -0.22779130935668945
Batch 26/64 loss: -0.21789008378982544
Batch 27/64 loss: -0.22135457396507263
Batch 28/64 loss: -0.20244872570037842
Batch 29/64 loss: -0.21912169456481934
Batch 30/64 loss: -0.21801108121871948
Batch 31/64 loss: -0.21818149089813232
Batch 32/64 loss: -0.19124287366867065
Batch 33/64 loss: -0.22807985544204712
Batch 34/64 loss: -0.22795915603637695
Batch 35/64 loss: -0.20170950889587402
Batch 36/64 loss: -0.1971374750137329
Batch 37/64 loss: -0.22812002897262573
Batch 38/64 loss: -0.22996315360069275
Batch 39/64 loss: -0.2239033579826355
Batch 40/64 loss: -0.14346599578857422
Batch 41/64 loss: -0.2409524917602539
Batch 42/64 loss: -0.20118343830108643
Batch 43/64 loss: -0.20700788497924805
Batch 44/64 loss: -0.19815593957901
Batch 45/64 loss: -0.2226734757423401
Batch 46/64 loss: -0.22604328393936157
Batch 47/64 loss: -0.19743531942367554
Batch 48/64 loss: -0.21589094400405884
Batch 49/64 loss: -0.21274060010910034
Batch 50/64 loss: -0.22954776883125305
Batch 51/64 loss: -0.20978569984436035
Batch 52/64 loss: -0.21157068014144897
Batch 53/64 loss: -0.20758235454559326
Batch 54/64 loss: -0.19884902238845825
Batch 55/64 loss: -0.19308382272720337
Batch 56/64 loss: -0.2067130208015442
Batch 57/64 loss: -0.20301896333694458
Batch 58/64 loss: -0.16846036911010742
Batch 59/64 loss: -0.20414167642593384
Batch 60/64 loss: -0.20542913675308228
Batch 61/64 loss: -0.21296954154968262
Batch 62/64 loss: -0.22187930345535278
Batch 63/64 loss: -0.14954030513763428
Batch 64/64 loss: -0.2248135507106781
Epoch 371  Train loss: -0.2089179021470687  Val loss: -0.0395565536833301
Epoch 372
-------------------------------
Batch 1/64 loss: -0.1910204291343689
Batch 2/64 loss: -0.2097368836402893
Batch 3/64 loss: -0.18017518520355225
Batch 4/64 loss: -0.219485342502594
Batch 5/64 loss: -0.20879453420639038
Batch 6/64 loss: -0.21053576469421387
Batch 7/64 loss: -0.21594780683517456
Batch 8/64 loss: -0.21080708503723145
Batch 9/64 loss: -0.2081860899925232
Batch 10/64 loss: -0.233678936958313
Batch 11/64 loss: -0.17179203033447266
Batch 12/64 loss: -0.21031206846237183
Batch 13/64 loss: -0.20037472248077393
Batch 14/64 loss: -0.20720618963241577
Batch 15/64 loss: -0.2112073302268982
Batch 16/64 loss: -0.21009200811386108
Batch 17/64 loss: -0.2106621265411377
Batch 18/64 loss: -0.2115539312362671
Batch 19/64 loss: -0.22172945737838745
Batch 20/64 loss: -0.2008022665977478
Batch 21/64 loss: -0.22092369198799133
Batch 22/64 loss: -0.2066870927810669
Batch 23/64 loss: -0.17814487218856812
Batch 24/64 loss: -0.17915856838226318
Batch 25/64 loss: -0.2147080898284912
Batch 26/64 loss: -0.22691750526428223
Batch 27/64 loss: -0.1737452745437622
Batch 28/64 loss: -0.19496548175811768
Batch 29/64 loss: -0.2130035161972046
Batch 30/64 loss: -0.19000482559204102
Batch 31/64 loss: -0.18020999431610107
Batch 32/64 loss: -0.20199847221374512
Batch 33/64 loss: -0.18114060163497925
Batch 34/64 loss: -0.19982492923736572
Batch 35/64 loss: -0.21986204385757446
Batch 36/64 loss: -0.21090692281723022
Batch 37/64 loss: -0.21077823638916016
Batch 38/64 loss: -0.22181686758995056
Batch 39/64 loss: -0.20254838466644287
Batch 40/64 loss: -0.19183605909347534
Batch 41/64 loss: -0.20420962572097778
Batch 42/64 loss: -0.2192169427871704
Batch 43/64 loss: -0.20343124866485596
Batch 44/64 loss: -0.19546276330947876
Batch 45/64 loss: -0.23066186904907227
Batch 46/64 loss: -0.23175838589668274
Batch 47/64 loss: -0.2158951759338379
Batch 48/64 loss: -0.20555394887924194
Batch 49/64 loss: -0.185150146484375
Batch 50/64 loss: -0.2192876935005188
Batch 51/64 loss: -0.19785046577453613
Batch 52/64 loss: -0.2220718264579773
Batch 53/64 loss: -0.22444018721580505
Batch 54/64 loss: -0.2374475598335266
Batch 55/64 loss: -0.18551939725875854
Batch 56/64 loss: -0.21658164262771606
Batch 57/64 loss: -0.20910626649856567
Batch 58/64 loss: -0.20125937461853027
Batch 59/64 loss: -0.2081548571586609
Batch 60/64 loss: -0.2305336594581604
Batch 61/64 loss: -0.22527655959129333
Batch 62/64 loss: -0.214586079120636
Batch 63/64 loss: -0.15258216857910156
Batch 64/64 loss: -0.1960376501083374
Epoch 372  Train loss: -0.20621721884783575  Val loss: -0.03812524338358456
Epoch 373
-------------------------------
Batch 1/64 loss: -0.21209830045700073
Batch 2/64 loss: -0.1938154101371765
Batch 3/64 loss: -0.21283197402954102
Batch 4/64 loss: -0.1787777543067932
Batch 5/64 loss: -0.19086170196533203
Batch 6/64 loss: -0.18045079708099365
Batch 7/64 loss: -0.21610510349273682
Batch 8/64 loss: -0.2002582550048828
Batch 9/64 loss: -0.20657974481582642
Batch 10/64 loss: -0.21886304020881653
Batch 11/64 loss: -0.19846433401107788
Batch 12/64 loss: -0.23910576105117798
Batch 13/64 loss: -0.2121163010597229
Batch 14/64 loss: -0.2191951870918274
Batch 15/64 loss: -0.22932010889053345
Batch 16/64 loss: -0.2079908847808838
Batch 17/64 loss: -0.2093302607536316
Batch 18/64 loss: -0.20537304878234863
Batch 19/64 loss: -0.20874863862991333
Batch 20/64 loss: -0.17847955226898193
Batch 21/64 loss: -0.19453173875808716
Batch 22/64 loss: -0.2156885266304016
Batch 23/64 loss: -0.22654873132705688
Batch 24/64 loss: -0.18125462532043457
Batch 25/64 loss: -0.20627671480178833
Batch 26/64 loss: -0.19701385498046875
Batch 27/64 loss: -0.19757264852523804
Batch 28/64 loss: -0.19829338788986206
Batch 29/64 loss: -0.21095490455627441
Batch 30/64 loss: -0.219031423330307
Batch 31/64 loss: -0.21115851402282715
Batch 32/64 loss: -0.21445691585540771
Batch 33/64 loss: -0.2041354775428772
Batch 34/64 loss: -0.19333064556121826
Batch 35/64 loss: -0.19512492418289185
Batch 36/64 loss: -0.21177434921264648
Batch 37/64 loss: -0.20137923955917358
Batch 38/64 loss: -0.19177281856536865
Batch 39/64 loss: -0.20578372478485107
Batch 40/64 loss: -0.1977849006652832
Batch 41/64 loss: -0.22217750549316406
Batch 42/64 loss: -0.22106248140335083
Batch 43/64 loss: -0.2027938961982727
Batch 44/64 loss: -0.22100985050201416
Batch 45/64 loss: -0.21720963716506958
Batch 46/64 loss: -0.22404593229293823
Batch 47/64 loss: -0.2169528603553772
Batch 48/64 loss: -0.1933882236480713
Batch 49/64 loss: -0.2104005217552185
Batch 50/64 loss: -0.22064226865768433
Batch 51/64 loss: -0.23211514949798584
Batch 52/64 loss: -0.18125778436660767
Batch 53/64 loss: -0.21341383457183838
Batch 54/64 loss: -0.22300469875335693
Batch 55/64 loss: -0.2352885603904724
Batch 56/64 loss: -0.21071982383728027
Batch 57/64 loss: -0.1955464482307434
Batch 58/64 loss: -0.19819307327270508
Batch 59/64 loss: -0.21538046002388
Batch 60/64 loss: -0.21218246221542358
Batch 61/64 loss: -0.2018941044807434
Batch 62/64 loss: -0.2013552188873291
Batch 63/64 loss: -0.2056601643562317
Batch 64/64 loss: -0.21968907117843628
Epoch 373  Train loss: -0.20757797629225488  Val loss: -0.038375921265775804
Epoch 374
-------------------------------
Batch 1/64 loss: -0.21142399311065674
Batch 2/64 loss: -0.1921912431716919
Batch 3/64 loss: -0.18524742126464844
Batch 4/64 loss: -0.21697402000427246
Batch 5/64 loss: -0.2025158405303955
Batch 6/64 loss: -0.2071744203567505
Batch 7/64 loss: -0.23525559902191162
Batch 8/64 loss: -0.22688478231430054
Batch 9/64 loss: -0.2262398600578308
Batch 10/64 loss: -0.21322929859161377
Batch 11/64 loss: -0.20800834894180298
Batch 12/64 loss: -0.19857728481292725
Batch 13/64 loss: -0.22088056802749634
Batch 14/64 loss: -0.2314348816871643
Batch 15/64 loss: -0.21695980429649353
Batch 16/64 loss: -0.21188801527023315
Batch 17/64 loss: -0.21282219886779785
Batch 18/64 loss: -0.2206510305404663
Batch 19/64 loss: -0.17673248052597046
Batch 20/64 loss: -0.2131403088569641
Batch 21/64 loss: -0.220991313457489
Batch 22/64 loss: -0.1826392412185669
Batch 23/64 loss: -0.18945246934890747
Batch 24/64 loss: -0.19011569023132324
Batch 25/64 loss: -0.2124156951904297
Batch 26/64 loss: -0.19828307628631592
Batch 27/64 loss: -0.1903514862060547
Batch 28/64 loss: -0.15878236293792725
Batch 29/64 loss: -0.21838125586509705
Batch 30/64 loss: -0.20382535457611084
Batch 31/64 loss: -0.1936485767364502
Batch 32/64 loss: -0.18574655055999756
Batch 33/64 loss: -0.22331011295318604
Batch 34/64 loss: -0.1896134614944458
Batch 35/64 loss: -0.19559842348098755
Batch 36/64 loss: -0.218013733625412
Batch 37/64 loss: -0.19428688287734985
Batch 38/64 loss: -0.19201290607452393
Batch 39/64 loss: -0.20879769325256348
Batch 40/64 loss: -0.19129890203475952
Batch 41/64 loss: -0.22660398483276367
Batch 42/64 loss: -0.22294199466705322
Batch 43/64 loss: -0.20459634065628052
Batch 44/64 loss: -0.2244042456150055
Batch 45/64 loss: -0.20519888401031494
Batch 46/64 loss: -0.2156783938407898
Batch 47/64 loss: -0.22649982571601868
Batch 48/64 loss: -0.21121931076049805
Batch 49/64 loss: -0.23480939865112305
Batch 50/64 loss: -0.21639001369476318
Batch 51/64 loss: -0.1453145146369934
Batch 52/64 loss: -0.18535327911376953
Batch 53/64 loss: -0.20964884757995605
Batch 54/64 loss: -0.1971982717514038
Batch 55/64 loss: -0.2131444215774536
Batch 56/64 loss: -0.20636773109436035
Batch 57/64 loss: -0.22123980522155762
Batch 58/64 loss: -0.2101956009864807
Batch 59/64 loss: -0.2119307518005371
Batch 60/64 loss: -0.2125086784362793
Batch 61/64 loss: -0.1820123791694641
Batch 62/64 loss: -0.17422759532928467
Batch 63/64 loss: -0.20755165815353394
Batch 64/64 loss: -0.16984593868255615
Epoch 374  Train loss: -0.20514850148967667  Val loss: -0.03891478760545606
Epoch 375
-------------------------------
Batch 1/64 loss: -0.2280568778514862
Batch 2/64 loss: -0.22367584705352783
Batch 3/64 loss: -0.21424752473831177
Batch 4/64 loss: -0.20522576570510864
Batch 5/64 loss: -0.19737303256988525
Batch 6/64 loss: -0.2082688808441162
Batch 7/64 loss: -0.21264123916625977
Batch 8/64 loss: -0.22460797429084778
Batch 9/64 loss: -0.1882076859474182
Batch 10/64 loss: -0.2052299976348877
Batch 11/64 loss: -0.23979714512825012
Batch 12/64 loss: -0.19765311479568481
Batch 13/64 loss: -0.1998525857925415
Batch 14/64 loss: -0.2036014199256897
Batch 15/64 loss: -0.17436474561691284
Batch 16/64 loss: -0.19923794269561768
Batch 17/64 loss: -0.21084409952163696
Batch 18/64 loss: -0.20432811975479126
Batch 19/64 loss: -0.22015538811683655
Batch 20/64 loss: -0.22496145963668823
Batch 21/64 loss: -0.21675199270248413
Batch 22/64 loss: -0.20417869091033936
Batch 23/64 loss: -0.18583130836486816
Batch 24/64 loss: -0.2072066068649292
Batch 25/64 loss: -0.21345114707946777
Batch 26/64 loss: -0.2188923954963684
Batch 27/64 loss: -0.22047966718673706
Batch 28/64 loss: -0.23423564434051514
Batch 29/64 loss: -0.2089310884475708
Batch 30/64 loss: -0.21179187297821045
Batch 31/64 loss: -0.22372710704803467
Batch 32/64 loss: -0.19495868682861328
Batch 33/64 loss: -0.21181535720825195
Batch 34/64 loss: -0.2034183144569397
Batch 35/64 loss: -0.2229035496711731
Batch 36/64 loss: -0.2157495617866516
Batch 37/64 loss: -0.22295665740966797
Batch 38/64 loss: -0.20888197422027588
Batch 39/64 loss: -0.214074969291687
Batch 40/64 loss: -0.21060210466384888
Batch 41/64 loss: -0.20624995231628418
Batch 42/64 loss: -0.21761202812194824
Batch 43/64 loss: -0.21461009979248047
Batch 44/64 loss: -0.2331382930278778
Batch 45/64 loss: -0.20185112953186035
Batch 46/64 loss: -0.16635513305664062
Batch 47/64 loss: -0.22423326969146729
Batch 48/64 loss: -0.19444650411605835
Batch 49/64 loss: -0.21732795238494873
Batch 50/64 loss: -0.20315563678741455
Batch 51/64 loss: -0.18356895446777344
Batch 52/64 loss: -0.20389223098754883
Batch 53/64 loss: -0.21542876958847046
Batch 54/64 loss: -0.2040708065032959
Batch 55/64 loss: -0.1910327672958374
Batch 56/64 loss: -0.17903655767440796
Batch 57/64 loss: -0.21165138483047485
Batch 58/64 loss: -0.203014075756073
Batch 59/64 loss: -0.21379411220550537
Batch 60/64 loss: -0.20997023582458496
Batch 61/64 loss: -0.19392269849777222
Batch 62/64 loss: -0.20223218202590942
Batch 63/64 loss: -0.20529383420944214
Batch 64/64 loss: -0.21702182292938232
Epoch 375  Train loss: -0.20849920815112544  Val loss: -0.03978581936498688
Epoch 376
-------------------------------
Batch 1/64 loss: -0.19859176874160767
Batch 2/64 loss: -0.20850825309753418
Batch 3/64 loss: -0.21128082275390625
Batch 4/64 loss: -0.22342252731323242
Batch 5/64 loss: -0.20706260204315186
Batch 6/64 loss: -0.19028133153915405
Batch 7/64 loss: -0.17132848501205444
Batch 8/64 loss: -0.1874990463256836
Batch 9/64 loss: -0.20797467231750488
Batch 10/64 loss: -0.22346296906471252
Batch 11/64 loss: -0.20105886459350586
Batch 12/64 loss: -0.21709027886390686
Batch 13/64 loss: -0.2272738218307495
Batch 14/64 loss: -0.18575239181518555
Batch 15/64 loss: -0.22547656297683716
Batch 16/64 loss: -0.21550673246383667
Batch 17/64 loss: -0.21329402923583984
Batch 18/64 loss: -0.23092776536941528
Batch 19/64 loss: -0.2345685362815857
Batch 20/64 loss: -0.20467454195022583
Batch 21/64 loss: -0.2232741117477417
Batch 22/64 loss: -0.22438031435012817
Batch 23/64 loss: -0.20428264141082764
Batch 24/64 loss: -0.21780043840408325
Batch 25/64 loss: -0.2109876275062561
Batch 26/64 loss: -0.21243435144424438
Batch 27/64 loss: -0.22208115458488464
Batch 28/64 loss: -0.223099946975708
Batch 29/64 loss: -0.1956215500831604
Batch 30/64 loss: -0.22478222846984863
Batch 31/64 loss: -0.20105016231536865
Batch 32/64 loss: -0.2354182004928589
Batch 33/64 loss: -0.1995266079902649
Batch 34/64 loss: -0.22481930255889893
Batch 35/64 loss: -0.20805221796035767
Batch 36/64 loss: -0.213606059551239
Batch 37/64 loss: -0.22000333666801453
Batch 38/64 loss: -0.19795489311218262
Batch 39/64 loss: -0.22582662105560303
Batch 40/64 loss: -0.20561975240707397
Batch 41/64 loss: -0.19356530904769897
Batch 42/64 loss: -0.1885967254638672
Batch 43/64 loss: -0.19932198524475098
Batch 44/64 loss: -0.20351636409759521
Batch 45/64 loss: -0.20750296115875244
Batch 46/64 loss: -0.19931185245513916
Batch 47/64 loss: -0.19850897789001465
Batch 48/64 loss: -0.18022257089614868
Batch 49/64 loss: -0.2199658751487732
Batch 50/64 loss: -0.21749424934387207
Batch 51/64 loss: -0.23142677545547485
Batch 52/64 loss: -0.19827967882156372
Batch 53/64 loss: -0.20088452100753784
Batch 54/64 loss: -0.16325825452804565
Batch 55/64 loss: -0.18800604343414307
Batch 56/64 loss: -0.2151789665222168
Batch 57/64 loss: -0.2204572558403015
Batch 58/64 loss: -0.19598805904388428
Batch 59/64 loss: -0.21969762444496155
Batch 60/64 loss: -0.22587710618972778
Batch 61/64 loss: -0.2049131989479065
Batch 62/64 loss: -0.18240410089492798
Batch 63/64 loss: -0.1827073097229004
Batch 64/64 loss: -0.23523575067520142
Epoch 376  Train loss: -0.20845757021623498  Val loss: -0.038690477097567004
Epoch 377
-------------------------------
Batch 1/64 loss: -0.20886355638504028
Batch 2/64 loss: -0.20196747779846191
Batch 3/64 loss: -0.2117500901222229
Batch 4/64 loss: -0.20125561952590942
Batch 5/64 loss: -0.204867422580719
Batch 6/64 loss: -0.22033464908599854
Batch 7/64 loss: -0.21052062511444092
Batch 8/64 loss: -0.22373312711715698
Batch 9/64 loss: -0.219468355178833
Batch 10/64 loss: -0.23444676399230957
Batch 11/64 loss: -0.23315808176994324
Batch 12/64 loss: -0.18586117029190063
Batch 13/64 loss: -0.1916433572769165
Batch 14/64 loss: -0.20208996534347534
Batch 15/64 loss: -0.20429086685180664
Batch 16/64 loss: -0.22745659947395325
Batch 17/64 loss: -0.21873903274536133
Batch 18/64 loss: -0.21008169651031494
Batch 19/64 loss: -0.22825664281845093
Batch 20/64 loss: -0.22410792112350464
Batch 21/64 loss: -0.22267407178878784
Batch 22/64 loss: -0.21434438228607178
Batch 23/64 loss: -0.2327767014503479
Batch 24/64 loss: -0.1975647211074829
Batch 25/64 loss: -0.23182281851768494
Batch 26/64 loss: -0.22718912363052368
Batch 27/64 loss: -0.1979461908340454
Batch 28/64 loss: -0.21731364727020264
Batch 29/64 loss: -0.19882184267044067
Batch 30/64 loss: -0.2401447892189026
Batch 31/64 loss: -0.2207837700843811
Batch 32/64 loss: -0.21211528778076172
Batch 33/64 loss: -0.20328903198242188
Batch 34/64 loss: -0.20586556196212769
Batch 35/64 loss: -0.2123897671699524
Batch 36/64 loss: -0.22501176595687866
Batch 37/64 loss: -0.23889654874801636
Batch 38/64 loss: -0.21103769540786743
Batch 39/64 loss: -0.19462096691131592
Batch 40/64 loss: -0.22249603271484375
Batch 41/64 loss: -0.213950514793396
Batch 42/64 loss: -0.2198823094367981
Batch 43/64 loss: -0.23093965649604797
Batch 44/64 loss: -0.2150261402130127
Batch 45/64 loss: -0.22391992807388306
Batch 46/64 loss: -0.21921846270561218
Batch 47/64 loss: -0.2027619481086731
Batch 48/64 loss: -0.21302026510238647
Batch 49/64 loss: -0.243547260761261
Batch 50/64 loss: -0.20983660221099854
Batch 51/64 loss: -0.2158794403076172
Batch 52/64 loss: -0.22720780968666077
Batch 53/64 loss: -0.23047029972076416
Batch 54/64 loss: -0.21001851558685303
Batch 55/64 loss: -0.19615763425827026
Batch 56/64 loss: -0.23476415872573853
Batch 57/64 loss: -0.19939815998077393
Batch 58/64 loss: -0.22868171334266663
Batch 59/64 loss: -0.2209203541278839
Batch 60/64 loss: -0.178783118724823
Batch 61/64 loss: -0.16876095533370972
Batch 62/64 loss: -0.21008503437042236
Batch 63/64 loss: -0.20253264904022217
Batch 64/64 loss: -0.18375754356384277
Epoch 377  Train loss: -0.21401692278244916  Val loss: -0.039510815618783744
Epoch 378
-------------------------------
Batch 1/64 loss: -0.19060933589935303
Batch 2/64 loss: -0.21913906931877136
Batch 3/64 loss: -0.23215851187705994
Batch 4/64 loss: -0.22044366598129272
Batch 5/64 loss: -0.2267446517944336
Batch 6/64 loss: -0.2253093719482422
Batch 7/64 loss: -0.2124735713005066
Batch 8/64 loss: -0.20421189069747925
Batch 9/64 loss: -0.2283984124660492
Batch 10/64 loss: -0.2340584397315979
Batch 11/64 loss: -0.2211683988571167
Batch 12/64 loss: -0.19413506984710693
Batch 13/64 loss: -0.19169706106185913
Batch 14/64 loss: -0.20191216468811035
Batch 15/64 loss: -0.186587393283844
Batch 16/64 loss: -0.2056659460067749
Batch 17/64 loss: -0.2040313482284546
Batch 18/64 loss: -0.18735718727111816
Batch 19/64 loss: -0.18515264987945557
Batch 20/64 loss: -0.2132980227470398
Batch 21/64 loss: -0.21428555250167847
Batch 22/64 loss: -0.20956403017044067
Batch 23/64 loss: -0.2182416319847107
Batch 24/64 loss: -0.2183350920677185
Batch 25/64 loss: -0.20803076028823853
Batch 26/64 loss: -0.20826566219329834
Batch 27/64 loss: -0.21504056453704834
Batch 28/64 loss: -0.19683825969696045
Batch 29/64 loss: -0.2224651277065277
Batch 30/64 loss: -0.20982003211975098
Batch 31/64 loss: -0.22327905893325806
Batch 32/64 loss: -0.2284461259841919
Batch 33/64 loss: -0.23255756497383118
Batch 34/64 loss: -0.20899784564971924
Batch 35/64 loss: -0.2153509259223938
Batch 36/64 loss: -0.20841312408447266
Batch 37/64 loss: -0.23153814673423767
Batch 38/64 loss: -0.19771277904510498
Batch 39/64 loss: -0.18137377500534058
Batch 40/64 loss: -0.22515881061553955
Batch 41/64 loss: -0.21977126598358154
Batch 42/64 loss: -0.19537395238876343
Batch 43/64 loss: -0.1807234287261963
Batch 44/64 loss: -0.23733589053153992
Batch 45/64 loss: -0.2191603183746338
Batch 46/64 loss: -0.2108367681503296
Batch 47/64 loss: -0.22841262817382812
Batch 48/64 loss: -0.2143877148628235
Batch 49/64 loss: -0.22385025024414062
Batch 50/64 loss: -0.212119460105896
Batch 51/64 loss: -0.22657674551010132
Batch 52/64 loss: -0.21015238761901855
Batch 53/64 loss: -0.22481238842010498
Batch 54/64 loss: -0.21951749920845032
Batch 55/64 loss: -0.22371888160705566
Batch 56/64 loss: -0.19665664434432983
Batch 57/64 loss: -0.1699589490890503
Batch 58/64 loss: -0.21652504801750183
Batch 59/64 loss: -0.17465734481811523
Batch 60/64 loss: -0.1657017469406128
Batch 61/64 loss: -0.19303882122039795
Batch 62/64 loss: -0.1960986852645874
Batch 63/64 loss: -0.21682941913604736
Batch 64/64 loss: -0.20366084575653076
Epoch 378  Train loss: -0.20999574754752365  Val loss: -0.039487424789835086
Epoch 379
-------------------------------
Batch 1/64 loss: -0.23194736242294312
Batch 2/64 loss: -0.17612981796264648
Batch 3/64 loss: -0.2051640748977661
Batch 4/64 loss: -0.21748369932174683
Batch 5/64 loss: -0.22484350204467773
Batch 6/64 loss: -0.20258289575576782
Batch 7/64 loss: -0.20416605472564697
Batch 8/64 loss: -0.19443202018737793
Batch 9/64 loss: -0.2050168514251709
Batch 10/64 loss: -0.18698656558990479
Batch 11/64 loss: -0.20582079887390137
Batch 12/64 loss: -0.2119959592819214
Batch 13/64 loss: -0.23518991470336914
Batch 14/64 loss: -0.2046266794204712
Batch 15/64 loss: -0.1894938349723816
Batch 16/64 loss: -0.21147692203521729
Batch 17/64 loss: -0.21208912134170532
Batch 18/64 loss: -0.2310141921043396
Batch 19/64 loss: -0.21176952123641968
Batch 20/64 loss: -0.20459026098251343
Batch 21/64 loss: -0.2259712815284729
Batch 22/64 loss: -0.20933270454406738
Batch 23/64 loss: -0.21947288513183594
Batch 24/64 loss: -0.2426053285598755
Batch 25/64 loss: -0.22476232051849365
Batch 26/64 loss: -0.20742326974868774
Batch 27/64 loss: -0.22485241293907166
Batch 28/64 loss: -0.2138432264328003
Batch 29/64 loss: -0.22272160649299622
Batch 30/64 loss: -0.2096218466758728
Batch 31/64 loss: -0.22383949160575867
Batch 32/64 loss: -0.21525448560714722
Batch 33/64 loss: -0.17507541179656982
Batch 34/64 loss: -0.23349574208259583
Batch 35/64 loss: -0.20226210355758667
Batch 36/64 loss: -0.19876086711883545
Batch 37/64 loss: -0.22689470648765564
Batch 38/64 loss: -0.2257346510887146
Batch 39/64 loss: -0.21410810947418213
Batch 40/64 loss: -0.2261205017566681
Batch 41/64 loss: -0.2359955906867981
Batch 42/64 loss: -0.22956177592277527
Batch 43/64 loss: -0.21449482440948486
Batch 44/64 loss: -0.20804846286773682
Batch 45/64 loss: -0.20796436071395874
Batch 46/64 loss: -0.20810353755950928
Batch 47/64 loss: -0.19012945890426636
Batch 48/64 loss: -0.21910569071769714
Batch 49/64 loss: -0.22468045353889465
Batch 50/64 loss: -0.22676581144332886
Batch 51/64 loss: -0.22505521774291992
Batch 52/64 loss: -0.17780226469039917
Batch 53/64 loss: -0.21288615465164185
Batch 54/64 loss: -0.21605968475341797
Batch 55/64 loss: -0.219743549823761
Batch 56/64 loss: -0.19237196445465088
Batch 57/64 loss: -0.2089442014694214
Batch 58/64 loss: -0.22585836052894592
Batch 59/64 loss: -0.2041468620300293
Batch 60/64 loss: -0.2070525884628296
Batch 61/64 loss: -0.22951072454452515
Batch 62/64 loss: -0.20681113004684448
Batch 63/64 loss: -0.21688345074653625
Batch 64/64 loss: -0.2087695598602295
Epoch 379  Train loss: -0.21291805949865603  Val loss: -0.038431413394888654
Epoch 380
-------------------------------
Batch 1/64 loss: -0.1988639235496521
Batch 2/64 loss: -0.22787106037139893
Batch 3/64 loss: -0.22812232375144958
Batch 4/64 loss: -0.20862215757369995
Batch 5/64 loss: -0.20614051818847656
Batch 6/64 loss: -0.22524607181549072
Batch 7/64 loss: -0.17816650867462158
Batch 8/64 loss: -0.1959153413772583
Batch 9/64 loss: -0.21811476349830627
Batch 10/64 loss: -0.2263737916946411
Batch 11/64 loss: -0.2234882116317749
Batch 12/64 loss: -0.2301005721092224
Batch 13/64 loss: -0.2337893545627594
Batch 14/64 loss: -0.2303381860256195
Batch 15/64 loss: -0.22823649644851685
Batch 16/64 loss: -0.2205033004283905
Batch 17/64 loss: -0.21805965900421143
Batch 18/64 loss: -0.19200986623764038
Batch 19/64 loss: -0.2091994285583496
Batch 20/64 loss: -0.21491599082946777
Batch 21/64 loss: -0.22290220856666565
Batch 22/64 loss: -0.21748846769332886
Batch 23/64 loss: -0.21065294742584229
Batch 24/64 loss: -0.21220946311950684
Batch 25/64 loss: -0.22146350145339966
Batch 26/64 loss: -0.22685620188713074
Batch 27/64 loss: -0.21024787425994873
Batch 28/64 loss: -0.19676631689071655
Batch 29/64 loss: -0.17217493057250977
Batch 30/64 loss: -0.2230127453804016
Batch 31/64 loss: -0.21401339769363403
Batch 32/64 loss: -0.21301573514938354
Batch 33/64 loss: -0.21925482153892517
Batch 34/64 loss: -0.22661066055297852
Batch 35/64 loss: -0.21035057306289673
Batch 36/64 loss: -0.2233787178993225
Batch 37/64 loss: -0.20439088344573975
Batch 38/64 loss: -0.21628695726394653
Batch 39/64 loss: -0.18214738368988037
Batch 40/64 loss: -0.2131108045578003
Batch 41/64 loss: -0.21876883506774902
Batch 42/64 loss: -0.21193701028823853
Batch 43/64 loss: -0.2169475555419922
Batch 44/64 loss: -0.21446853876113892
Batch 45/64 loss: -0.21232855319976807
Batch 46/64 loss: -0.23324799537658691
Batch 47/64 loss: -0.23052895069122314
Batch 48/64 loss: -0.22429054975509644
Batch 49/64 loss: -0.20910799503326416
Batch 50/64 loss: -0.22508016228675842
Batch 51/64 loss: -0.23411527276039124
Batch 52/64 loss: -0.2294982671737671
Batch 53/64 loss: -0.21591484546661377
Batch 54/64 loss: -0.201127290725708
Batch 55/64 loss: -0.20848095417022705
Batch 56/64 loss: -0.20904147624969482
Batch 57/64 loss: -0.1947706937789917
Batch 58/64 loss: -0.2230355441570282
Batch 59/64 loss: -0.2102360725402832
Batch 60/64 loss: -0.21141231060028076
Batch 61/64 loss: -0.21691906452178955
Batch 62/64 loss: -0.19275695085525513
Batch 63/64 loss: -0.2206113338470459
Batch 64/64 loss: -0.22415459156036377
Epoch 380  Train loss: -0.21463771427378936  Val loss: -0.03928062944477776
Epoch 381
-------------------------------
Batch 1/64 loss: -0.20896637439727783
Batch 2/64 loss: -0.22694790363311768
Batch 3/64 loss: -0.20927798748016357
Batch 4/64 loss: -0.21561336517333984
Batch 5/64 loss: -0.20127618312835693
Batch 6/64 loss: -0.18217235803604126
Batch 7/64 loss: -0.21674305200576782
Batch 8/64 loss: -0.22166115045547485
Batch 9/64 loss: -0.2383442223072052
Batch 10/64 loss: -0.2198939323425293
Batch 11/64 loss: -0.20192480087280273
Batch 12/64 loss: -0.22489145398139954
Batch 13/64 loss: -0.20971202850341797
Batch 14/64 loss: -0.21473681926727295
Batch 15/64 loss: -0.22468072175979614
Batch 16/64 loss: -0.22432631254196167
Batch 17/64 loss: -0.22780629992485046
Batch 18/64 loss: -0.22142580151557922
Batch 19/64 loss: -0.20948928594589233
Batch 20/64 loss: -0.23749685287475586
Batch 21/64 loss: -0.24264240264892578
Batch 22/64 loss: -0.22191226482391357
Batch 23/64 loss: -0.2265627086162567
Batch 24/64 loss: -0.21011334657669067
Batch 25/64 loss: -0.20225292444229126
Batch 26/64 loss: -0.22490623593330383
Batch 27/64 loss: -0.2060949206352234
Batch 28/64 loss: -0.21496939659118652
Batch 29/64 loss: -0.18500268459320068
Batch 30/64 loss: -0.20953065156936646
Batch 31/64 loss: -0.21064597368240356
Batch 32/64 loss: -0.20356732606887817
Batch 33/64 loss: -0.19886791706085205
Batch 34/64 loss: -0.17998427152633667
Batch 35/64 loss: -0.22480088472366333
Batch 36/64 loss: -0.21658897399902344
Batch 37/64 loss: -0.20662808418273926
Batch 38/64 loss: -0.21140730381011963
Batch 39/64 loss: -0.21914178133010864
Batch 40/64 loss: -0.2026456594467163
Batch 41/64 loss: -0.21242058277130127
Batch 42/64 loss: -0.18768692016601562
Batch 43/64 loss: -0.21058857440948486
Batch 44/64 loss: -0.1944187879562378
Batch 45/64 loss: -0.22012853622436523
Batch 46/64 loss: -0.23093241453170776
Batch 47/64 loss: -0.22339534759521484
Batch 48/64 loss: -0.19584238529205322
Batch 49/64 loss: -0.2168818712234497
Batch 50/64 loss: -0.22214838862419128
Batch 51/64 loss: -0.22744101285934448
Batch 52/64 loss: -0.2206628918647766
Batch 53/64 loss: -0.2228243350982666
Batch 54/64 loss: -0.21712380647659302
Batch 55/64 loss: -0.22002381086349487
Batch 56/64 loss: -0.2230185568332672
Batch 57/64 loss: -0.16943359375
Batch 58/64 loss: -0.22709238529205322
Batch 59/64 loss: -0.21756938099861145
Batch 60/64 loss: -0.19198524951934814
Batch 61/64 loss: -0.2082846760749817
Batch 62/64 loss: -0.21896588802337646
Batch 63/64 loss: -0.2181236743927002
Batch 64/64 loss: -0.20164471864700317
Epoch 381  Train loss: -0.21339418397230261  Val loss: -0.038187326639378606
Epoch 382
-------------------------------
Batch 1/64 loss: -0.19433844089508057
Batch 2/64 loss: -0.20998597145080566
Batch 3/64 loss: -0.2197939157485962
Batch 4/64 loss: -0.21645578742027283
Batch 5/64 loss: -0.19948196411132812
Batch 6/64 loss: -0.22073611617088318
Batch 7/64 loss: -0.21531635522842407
Batch 8/64 loss: -0.21877193450927734
Batch 9/64 loss: -0.2185463011264801
Batch 10/64 loss: -0.21804970502853394
Batch 11/64 loss: -0.16796505451202393
Batch 12/64 loss: -0.15899217128753662
Batch 13/64 loss: -0.22578606009483337
Batch 14/64 loss: -0.20914983749389648
Batch 15/64 loss: -0.2197745442390442
Batch 16/64 loss: -0.19610309600830078
Batch 17/64 loss: -0.19131910800933838
Batch 18/64 loss: -0.2345225214958191
Batch 19/64 loss: -0.22332549095153809
Batch 20/64 loss: -0.20363950729370117
Batch 21/64 loss: -0.1886165738105774
Batch 22/64 loss: -0.18384158611297607
Batch 23/64 loss: -0.21237999200820923
Batch 24/64 loss: -0.22114580869674683
Batch 25/64 loss: -0.21699172258377075
Batch 26/64 loss: -0.21324920654296875
Batch 27/64 loss: -0.23956602811813354
Batch 28/64 loss: -0.23418554663658142
Batch 29/64 loss: -0.21698179841041565
Batch 30/64 loss: -0.20938217639923096
Batch 31/64 loss: -0.24107712507247925
Batch 32/64 loss: -0.1820719838142395
Batch 33/64 loss: -0.22190961241722107
Batch 34/64 loss: -0.20787125825881958
Batch 35/64 loss: -0.22167018055915833
Batch 36/64 loss: -0.22704049944877625
Batch 37/64 loss: -0.23211535811424255
Batch 38/64 loss: -0.2100285291671753
Batch 39/64 loss: -0.23436230421066284
Batch 40/64 loss: -0.21464747190475464
Batch 41/64 loss: -0.20132899284362793
Batch 42/64 loss: -0.22204455733299255
Batch 43/64 loss: -0.18414455652236938
Batch 44/64 loss: -0.21569573879241943
Batch 45/64 loss: -0.23301708698272705
Batch 46/64 loss: -0.23822200298309326
Batch 47/64 loss: -0.21793535351753235
Batch 48/64 loss: -0.19930344820022583
Batch 49/64 loss: -0.19536179304122925
Batch 50/64 loss: -0.18470853567123413
Batch 51/64 loss: -0.19107192754745483
Batch 52/64 loss: -0.2382826805114746
Batch 53/64 loss: -0.21825379133224487
Batch 54/64 loss: -0.22469377517700195
Batch 55/64 loss: -0.2324339747428894
Batch 56/64 loss: -0.234678715467453
Batch 57/64 loss: -0.19376063346862793
Batch 58/64 loss: -0.20638394355773926
Batch 59/64 loss: -0.2381574511528015
Batch 60/64 loss: -0.18009096384048462
Batch 61/64 loss: -0.2005404233932495
Batch 62/64 loss: -0.2179904282093048
Batch 63/64 loss: -0.20087701082229614
Batch 64/64 loss: -0.23530995845794678
Epoch 382  Train loss: -0.21233959057751825  Val loss: -0.038842794002126584
Epoch 383
-------------------------------
Batch 1/64 loss: -0.2384139895439148
Batch 2/64 loss: -0.2224932312965393
Batch 3/64 loss: -0.2260197401046753
Batch 4/64 loss: -0.23294082283973694
Batch 5/64 loss: -0.16733330488204956
Batch 6/64 loss: -0.22227105498313904
Batch 7/64 loss: -0.2170964479446411
Batch 8/64 loss: -0.1665061116218567
Batch 9/64 loss: -0.21729886531829834
Batch 10/64 loss: -0.21081018447875977
Batch 11/64 loss: -0.21127569675445557
Batch 12/64 loss: -0.23214879631996155
Batch 13/64 loss: -0.22029682993888855
Batch 14/64 loss: -0.21837973594665527
Batch 15/64 loss: -0.23083609342575073
Batch 16/64 loss: -0.19777792692184448
Batch 17/64 loss: -0.21633446216583252
Batch 18/64 loss: -0.19114059209823608
Batch 19/64 loss: -0.21504724025726318
Batch 20/64 loss: -0.1959977149963379
Batch 21/64 loss: -0.21152979135513306
Batch 22/64 loss: -0.20949268341064453
Batch 23/64 loss: -0.22192364931106567
Batch 24/64 loss: -0.2166677713394165
Batch 25/64 loss: -0.22208988666534424
Batch 26/64 loss: -0.20307576656341553
Batch 27/64 loss: -0.20703768730163574
Batch 28/64 loss: -0.2240639328956604
Batch 29/64 loss: -0.21465039253234863
Batch 30/64 loss: -0.19307231903076172
Batch 31/64 loss: -0.2189120650291443
Batch 32/64 loss: -0.21430110931396484
Batch 33/64 loss: -0.20378124713897705
Batch 34/64 loss: -0.209538996219635
Batch 35/64 loss: -0.21690547466278076
Batch 36/64 loss: -0.213994562625885
Batch 37/64 loss: -0.17519736289978027
Batch 38/64 loss: -0.20622843503952026
Batch 39/64 loss: -0.22008872032165527
Batch 40/64 loss: -0.22189784049987793
Batch 41/64 loss: -0.2225019931793213
Batch 42/64 loss: -0.21708258986473083
Batch 43/64 loss: -0.23010706901550293
Batch 44/64 loss: -0.2097528576850891
Batch 45/64 loss: -0.1904834508895874
Batch 46/64 loss: -0.20707958936691284
Batch 47/64 loss: -0.21712452173233032
Batch 48/64 loss: -0.21334081888198853
Batch 49/64 loss: -0.2110074758529663
Batch 50/64 loss: -0.20974516868591309
Batch 51/64 loss: -0.21713843941688538
Batch 52/64 loss: -0.21578937768936157
Batch 53/64 loss: -0.19513201713562012
Batch 54/64 loss: -0.2009899616241455
Batch 55/64 loss: -0.2189907729625702
Batch 56/64 loss: -0.22635090351104736
Batch 57/64 loss: -0.19602596759796143
Batch 58/64 loss: -0.21741235256195068
Batch 59/64 loss: -0.20444542169570923
Batch 60/64 loss: -0.20187079906463623
Batch 61/64 loss: -0.21885746717453003
Batch 62/64 loss: -0.20705491304397583
Batch 63/64 loss: -0.2031409740447998
Batch 64/64 loss: -0.18152207136154175
Epoch 383  Train loss: -0.21117545085794787  Val loss: -0.033655526097287836
Epoch 384
-------------------------------
Batch 1/64 loss: -0.20352816581726074
Batch 2/64 loss: -0.1841781735420227
Batch 3/64 loss: -0.2010120153427124
Batch 4/64 loss: -0.19509822130203247
Batch 5/64 loss: -0.20959162712097168
Batch 6/64 loss: -0.2174937129020691
Batch 7/64 loss: -0.2156403660774231
Batch 8/64 loss: -0.20954668521881104
Batch 9/64 loss: -0.2190912365913391
Batch 10/64 loss: -0.21927756071090698
Batch 11/64 loss: -0.2193387746810913
Batch 12/64 loss: -0.16260123252868652
Batch 13/64 loss: -0.20829451084136963
Batch 14/64 loss: -0.1912192702293396
Batch 15/64 loss: -0.22622957825660706
Batch 16/64 loss: -0.212024986743927
Batch 17/64 loss: -0.2189772129058838
Batch 18/64 loss: -0.21769553422927856
Batch 19/64 loss: -0.22755461931228638
Batch 20/64 loss: -0.2108156681060791
Batch 21/64 loss: -0.2176658809185028
Batch 22/64 loss: -0.2151801586151123
Batch 23/64 loss: -0.2089245319366455
Batch 24/64 loss: -0.2241748571395874
Batch 25/64 loss: -0.21223783493041992
Batch 26/64 loss: -0.22114339470863342
Batch 27/64 loss: -0.2225225567817688
Batch 28/64 loss: -0.2118028998374939
Batch 29/64 loss: -0.1785091757774353
Batch 30/64 loss: -0.2334492802619934
Batch 31/64 loss: -0.2119089961051941
Batch 32/64 loss: -0.22117775678634644
Batch 33/64 loss: -0.22592580318450928
Batch 34/64 loss: -0.2267051339149475
Batch 35/64 loss: -0.1872616410255432
Batch 36/64 loss: -0.21409070491790771
Batch 37/64 loss: -0.2285909652709961
Batch 38/64 loss: -0.19088053703308105
Batch 39/64 loss: -0.20040297508239746
Batch 40/64 loss: -0.19207799434661865
Batch 41/64 loss: -0.2176489233970642
Batch 42/64 loss: -0.2053775191307068
Batch 43/64 loss: -0.2071487307548523
Batch 44/64 loss: -0.2278864085674286
Batch 45/64 loss: -0.22300183773040771
Batch 46/64 loss: -0.22860482335090637
Batch 47/64 loss: -0.20981228351593018
Batch 48/64 loss: -0.18727147579193115
Batch 49/64 loss: -0.22102737426757812
Batch 50/64 loss: -0.224065363407135
Batch 51/64 loss: -0.20929211378097534
Batch 52/64 loss: -0.2020505666732788
Batch 53/64 loss: -0.23035499453544617
Batch 54/64 loss: -0.2183399200439453
Batch 55/64 loss: -0.21930670738220215
Batch 56/64 loss: -0.21645408868789673
Batch 57/64 loss: -0.2064475417137146
Batch 58/64 loss: -0.22406333684921265
Batch 59/64 loss: -0.21155577898025513
Batch 60/64 loss: -0.21672070026397705
Batch 61/64 loss: -0.20159059762954712
Batch 62/64 loss: -0.2362058460712433
Batch 63/64 loss: -0.20491337776184082
Batch 64/64 loss: -0.21941858530044556
Epoch 384  Train loss: -0.21219680753408693  Val loss: -0.03915205956324679
Epoch 385
-------------------------------
Batch 1/64 loss: -0.20619213581085205
Batch 2/64 loss: -0.2319141924381256
Batch 3/64 loss: -0.23585247993469238
Batch 4/64 loss: -0.222145676612854
Batch 5/64 loss: -0.20921969413757324
Batch 6/64 loss: -0.21506989002227783
Batch 7/64 loss: -0.2409077286720276
Batch 8/64 loss: -0.23459070920944214
Batch 9/64 loss: -0.2245866060256958
Batch 10/64 loss: -0.2408134937286377
Batch 11/64 loss: -0.21555495262145996
Batch 12/64 loss: -0.22926563024520874
Batch 13/64 loss: -0.2326507568359375
Batch 14/64 loss: -0.21097028255462646
Batch 15/64 loss: -0.2419249415397644
Batch 16/64 loss: -0.2112187147140503
Batch 17/64 loss: -0.20076978206634521
Batch 18/64 loss: -0.2282334268093109
Batch 19/64 loss: -0.20094084739685059
Batch 20/64 loss: -0.22707173228263855
Batch 21/64 loss: -0.21675431728363037
Batch 22/64 loss: -0.18864178657531738
Batch 23/64 loss: -0.23989203572273254
Batch 24/64 loss: -0.22732478380203247
Batch 25/64 loss: -0.2289838194847107
Batch 26/64 loss: -0.2239648997783661
Batch 27/64 loss: -0.20751893520355225
Batch 28/64 loss: -0.23338359594345093
Batch 29/64 loss: -0.15625572204589844
Batch 30/64 loss: -0.2370704710483551
Batch 31/64 loss: -0.21786347031593323
Batch 32/64 loss: -0.22101670503616333
Batch 33/64 loss: -0.1991192102432251
Batch 34/64 loss: -0.20946788787841797
Batch 35/64 loss: -0.23021864891052246
Batch 36/64 loss: -0.2342154085636139
Batch 37/64 loss: -0.19374781847000122
Batch 38/64 loss: -0.23561915755271912
Batch 39/64 loss: -0.2161145806312561
Batch 40/64 loss: -0.21222823858261108
Batch 41/64 loss: -0.22783952951431274
Batch 42/64 loss: -0.23380351066589355
Batch 43/64 loss: -0.21361911296844482
Batch 44/64 loss: -0.2195952832698822
Batch 45/64 loss: -0.22699880599975586
Batch 46/64 loss: -0.2021687626838684
Batch 47/64 loss: -0.22431272268295288
Batch 48/64 loss: -0.21400564908981323
Batch 49/64 loss: -0.22765958309173584
Batch 50/64 loss: -0.22376888990402222
Batch 51/64 loss: -0.1893414855003357
Batch 52/64 loss: -0.23495060205459595
Batch 53/64 loss: -0.19725477695465088
Batch 54/64 loss: -0.2096874713897705
Batch 55/64 loss: -0.21227705478668213
Batch 56/64 loss: -0.20754307508468628
Batch 57/64 loss: -0.21008557081222534
Batch 58/64 loss: -0.21348458528518677
Batch 59/64 loss: -0.21948742866516113
Batch 60/64 loss: -0.21834024786949158
Batch 61/64 loss: -0.22168898582458496
Batch 62/64 loss: -0.20830851793289185
Batch 63/64 loss: -0.22150978446006775
Batch 64/64 loss: -0.19055461883544922
Epoch 385  Train loss: -0.218195177527035  Val loss: -0.03960105725580065
Epoch 386
-------------------------------
Batch 1/64 loss: -0.2187000811100006
Batch 2/64 loss: -0.21341931819915771
Batch 3/64 loss: -0.2228497862815857
Batch 4/64 loss: -0.21752342581748962
Batch 5/64 loss: -0.22317489981651306
Batch 6/64 loss: -0.22359991073608398
Batch 7/64 loss: -0.19753259420394897
Batch 8/64 loss: -0.21144789457321167
Batch 9/64 loss: -0.23325571417808533
Batch 10/64 loss: -0.23071786761283875
Batch 11/64 loss: -0.22454184293746948
Batch 12/64 loss: -0.21739763021469116
Batch 13/64 loss: -0.22267070412635803
Batch 14/64 loss: -0.21983689069747925
Batch 15/64 loss: -0.22168996930122375
Batch 16/64 loss: -0.20540505647659302
Batch 17/64 loss: -0.21222954988479614
Batch 18/64 loss: -0.21526551246643066
Batch 19/64 loss: -0.2247207760810852
Batch 20/64 loss: -0.22474214434623718
Batch 21/64 loss: -0.22122961282730103
Batch 22/64 loss: -0.17020678520202637
Batch 23/64 loss: -0.2086113691329956
Batch 24/64 loss: -0.2212902307510376
Batch 25/64 loss: -0.18311059474945068
Batch 26/64 loss: -0.20319384336471558
Batch 27/64 loss: -0.20465773344039917
Batch 28/64 loss: -0.2073395848274231
Batch 29/64 loss: -0.23855498433113098
Batch 30/64 loss: -0.2243366241455078
Batch 31/64 loss: -0.21747979521751404
Batch 32/64 loss: -0.22789889574050903
Batch 33/64 loss: -0.23199689388275146
Batch 34/64 loss: -0.23599064350128174
Batch 35/64 loss: -0.22562986612319946
Batch 36/64 loss: -0.1938716173171997
Batch 37/64 loss: -0.21785005927085876
Batch 38/64 loss: -0.22496119141578674
Batch 39/64 loss: -0.2098524570465088
Batch 40/64 loss: -0.23648905754089355
Batch 41/64 loss: -0.20751512050628662
Batch 42/64 loss: -0.22449886798858643
Batch 43/64 loss: -0.20637726783752441
Batch 44/64 loss: -0.2214103639125824
Batch 45/64 loss: -0.18364083766937256
Batch 46/64 loss: -0.2090301513671875
Batch 47/64 loss: -0.21407759189605713
Batch 48/64 loss: -0.21757879853248596
Batch 49/64 loss: -0.23878490924835205
Batch 50/64 loss: -0.19034290313720703
Batch 51/64 loss: -0.1978587508201599
Batch 52/64 loss: -0.23538392782211304
Batch 53/64 loss: -0.236735999584198
Batch 54/64 loss: -0.2019575834274292
Batch 55/64 loss: -0.22169899940490723
Batch 56/64 loss: -0.22021779417991638
Batch 57/64 loss: -0.21058154106140137
Batch 58/64 loss: -0.1925818920135498
Batch 59/64 loss: -0.2126113772392273
Batch 60/64 loss: -0.23617562651634216
Batch 61/64 loss: -0.22079402208328247
Batch 62/64 loss: -0.1842331886291504
Batch 63/64 loss: -0.21580660343170166
Batch 64/64 loss: -0.2164309024810791
Epoch 386  Train loss: -0.21564691300485647  Val loss: -0.040019087775056715
Epoch 387
-------------------------------
Batch 1/64 loss: -0.2218887209892273
Batch 2/64 loss: -0.22341454029083252
Batch 3/64 loss: -0.21657085418701172
Batch 4/64 loss: -0.2265319526195526
Batch 5/64 loss: -0.23220866918563843
Batch 6/64 loss: -0.22016596794128418
Batch 7/64 loss: -0.23217111825942993
Batch 8/64 loss: -0.23155000805854797
Batch 9/64 loss: -0.22863537073135376
Batch 10/64 loss: -0.23760387301445007
Batch 11/64 loss: -0.2388157844543457
Batch 12/64 loss: -0.21908721327781677
Batch 13/64 loss: -0.20819568634033203
Batch 14/64 loss: -0.23162567615509033
Batch 15/64 loss: -0.21955382823944092
Batch 16/64 loss: -0.22739216685295105
Batch 17/64 loss: -0.2063997983932495
Batch 18/64 loss: -0.19727766513824463
Batch 19/64 loss: -0.19015860557556152
Batch 20/64 loss: -0.21695730090141296
Batch 21/64 loss: -0.2279348373413086
Batch 22/64 loss: -0.2152881622314453
Batch 23/64 loss: -0.23688703775405884
Batch 24/64 loss: -0.20668816566467285
Batch 25/64 loss: -0.22961783409118652
Batch 26/64 loss: -0.21454128623008728
Batch 27/64 loss: -0.1952102780342102
Batch 28/64 loss: -0.21061640977859497
Batch 29/64 loss: -0.21226930618286133
Batch 30/64 loss: -0.19125688076019287
Batch 31/64 loss: -0.2020425796508789
Batch 32/64 loss: -0.20685964822769165
Batch 33/64 loss: -0.17048227787017822
Batch 34/64 loss: -0.19380950927734375
Batch 35/64 loss: -0.2024356722831726
Batch 36/64 loss: -0.22682511806488037
Batch 37/64 loss: -0.21080726385116577
Batch 38/64 loss: -0.218272864818573
Batch 39/64 loss: -0.21076762676239014
Batch 40/64 loss: -0.2081717848777771
Batch 41/64 loss: -0.16318607330322266
Batch 42/64 loss: -0.22179710865020752
Batch 43/64 loss: -0.1890411376953125
Batch 44/64 loss: -0.18180698156356812
Batch 45/64 loss: -0.21434438228607178
Batch 46/64 loss: -0.2128448486328125
Batch 47/64 loss: -0.1695328950881958
Batch 48/64 loss: -0.21713173389434814
Batch 49/64 loss: -0.21936029195785522
Batch 50/64 loss: -0.21398723125457764
Batch 51/64 loss: -0.21401631832122803
Batch 52/64 loss: -0.21481680870056152
Batch 53/64 loss: -0.18818241357803345
Batch 54/64 loss: -0.19879519939422607
Batch 55/64 loss: -0.2088260054588318
Batch 56/64 loss: -0.21963238716125488
Batch 57/64 loss: -0.22789126634597778
Batch 58/64 loss: -0.17348098754882812
Batch 59/64 loss: -0.19734221696853638
Batch 60/64 loss: -0.20584064722061157
Batch 61/64 loss: -0.2178376317024231
Batch 62/64 loss: -0.22583132982254028
Batch 63/64 loss: -0.18930810689926147
Batch 64/64 loss: -0.19808530807495117
Epoch 387  Train loss: -0.2109864679037356  Val loss: -0.03801171271661712
Epoch 388
-------------------------------
Batch 1/64 loss: -0.18774449825286865
Batch 2/64 loss: -0.214957594871521
Batch 3/64 loss: -0.21929121017456055
Batch 4/64 loss: -0.20758509635925293
Batch 5/64 loss: -0.2217334508895874
Batch 6/64 loss: -0.20678842067718506
Batch 7/64 loss: -0.1924504041671753
Batch 8/64 loss: -0.2207408845424652
Batch 9/64 loss: -0.20090079307556152
Batch 10/64 loss: -0.19959378242492676
Batch 11/64 loss: -0.20542436838150024
Batch 12/64 loss: -0.23462852835655212
Batch 13/64 loss: -0.20655375719070435
Batch 14/64 loss: -0.2100246548652649
Batch 15/64 loss: -0.2318401038646698
Batch 16/64 loss: -0.20960789918899536
Batch 17/64 loss: -0.19985425472259521
Batch 18/64 loss: -0.22114068269729614
Batch 19/64 loss: -0.2089904546737671
Batch 20/64 loss: -0.20231401920318604
Batch 21/64 loss: -0.18645381927490234
Batch 22/64 loss: -0.1924583911895752
Batch 23/64 loss: -0.22943729162216187
Batch 24/64 loss: -0.17644065618515015
Batch 25/64 loss: -0.2281387448310852
Batch 26/64 loss: -0.21727412939071655
Batch 27/64 loss: -0.20994406938552856
Batch 28/64 loss: -0.21259057521820068
Batch 29/64 loss: -0.17071419954299927
Batch 30/64 loss: -0.21757099032402039
Batch 31/64 loss: -0.2215697169303894
Batch 32/64 loss: -0.2003568410873413
Batch 33/64 loss: -0.216880202293396
Batch 34/64 loss: -0.21797853708267212
Batch 35/64 loss: -0.2083970308303833
Batch 36/64 loss: -0.19243693351745605
Batch 37/64 loss: -0.21699514985084534
Batch 38/64 loss: -0.2032908797264099
Batch 39/64 loss: -0.21481072902679443
Batch 40/64 loss: -0.23732712864875793
Batch 41/64 loss: -0.19270402193069458
Batch 42/64 loss: -0.20311695337295532
Batch 43/64 loss: -0.20250177383422852
Batch 44/64 loss: -0.20968270301818848
Batch 45/64 loss: -0.1972675323486328
Batch 46/64 loss: -0.20692992210388184
Batch 47/64 loss: -0.23278596997261047
Batch 48/64 loss: -0.22557181119918823
Batch 49/64 loss: -0.22693324089050293
Batch 50/64 loss: -0.23780575394630432
Batch 51/64 loss: -0.23394852876663208
Batch 52/64 loss: -0.2267288863658905
Batch 53/64 loss: -0.22591441869735718
Batch 54/64 loss: -0.2172422707080841
Batch 55/64 loss: -0.19807952642440796
Batch 56/64 loss: -0.21663516759872437
Batch 57/64 loss: -0.2395201027393341
Batch 58/64 loss: -0.2332315742969513
Batch 59/64 loss: -0.19688314199447632
Batch 60/64 loss: -0.19310057163238525
Batch 61/64 loss: -0.20407181978225708
Batch 62/64 loss: -0.2091497778892517
Batch 63/64 loss: -0.19848990440368652
Batch 64/64 loss: -0.22525179386138916
Epoch 388  Train loss: -0.21130141323687984  Val loss: -0.03513578328070362
Epoch 389
-------------------------------
Batch 1/64 loss: -0.1699061393737793
Batch 2/64 loss: -0.19171220064163208
Batch 3/64 loss: -0.21506929397583008
Batch 4/64 loss: -0.20714294910430908
Batch 5/64 loss: -0.23356947302818298
Batch 6/64 loss: -0.204770028591156
Batch 7/64 loss: -0.22592943906784058
Batch 8/64 loss: -0.2253081202507019
Batch 9/64 loss: -0.20657968521118164
Batch 10/64 loss: -0.19839775562286377
Batch 11/64 loss: -0.22663283348083496
Batch 12/64 loss: -0.21558785438537598
Batch 13/64 loss: -0.2262604534626007
Batch 14/64 loss: -0.21920377016067505
Batch 15/64 loss: -0.20847547054290771
Batch 16/64 loss: -0.2084341049194336
Batch 17/64 loss: -0.21595239639282227
Batch 18/64 loss: -0.222988098859787
Batch 19/64 loss: -0.2314421832561493
Batch 20/64 loss: -0.22062236070632935
Batch 21/64 loss: -0.22341248393058777
Batch 22/64 loss: -0.21437138319015503
Batch 23/64 loss: -0.24390852451324463
Batch 24/64 loss: -0.15544533729553223
Batch 25/64 loss: -0.19953441619873047
Batch 26/64 loss: -0.19920390844345093
Batch 27/64 loss: -0.22593289613723755
Batch 28/64 loss: -0.2221117615699768
Batch 29/64 loss: -0.20826131105422974
Batch 30/64 loss: -0.21683281660079956
Batch 31/64 loss: -0.23677459359169006
Batch 32/64 loss: -0.24475252628326416
Batch 33/64 loss: -0.24615606665611267
Batch 34/64 loss: -0.2077072262763977
Batch 35/64 loss: -0.1665480136871338
Batch 36/64 loss: -0.24282652139663696
Batch 37/64 loss: -0.20506370067596436
Batch 38/64 loss: -0.228485107421875
Batch 39/64 loss: -0.21996811032295227
Batch 40/64 loss: -0.22252511978149414
Batch 41/64 loss: -0.20248639583587646
Batch 42/64 loss: -0.24061697721481323
Batch 43/64 loss: -0.22606399655342102
Batch 44/64 loss: -0.23773455619812012
Batch 45/64 loss: -0.21674469113349915
Batch 46/64 loss: -0.2275877594947815
Batch 47/64 loss: -0.21369004249572754
Batch 48/64 loss: -0.19844883680343628
Batch 49/64 loss: -0.19747644662857056
Batch 50/64 loss: -0.22980529069900513
Batch 51/64 loss: -0.2074928879737854
Batch 52/64 loss: -0.22861236333847046
Batch 53/64 loss: -0.240198016166687
Batch 54/64 loss: -0.22321933507919312
Batch 55/64 loss: -0.23697948455810547
Batch 56/64 loss: -0.2143458127975464
Batch 57/64 loss: -0.22366541624069214
Batch 58/64 loss: -0.22723066806793213
Batch 59/64 loss: -0.21546602249145508
Batch 60/64 loss: -0.20574718713760376
Batch 61/64 loss: -0.2176477313041687
Batch 62/64 loss: -0.22573590278625488
Batch 63/64 loss: -0.21858945488929749
Batch 64/64 loss: -0.22741344571113586
Epoch 389  Train loss: -0.21725414582327301  Val loss: -0.03627229965839189
Epoch 390
-------------------------------
Batch 1/64 loss: -0.22148889303207397
Batch 2/64 loss: -0.21791675686836243
Batch 3/64 loss: -0.20838916301727295
Batch 4/64 loss: -0.20129716396331787
Batch 5/64 loss: -0.2279108762741089
Batch 6/64 loss: -0.20617210865020752
Batch 7/64 loss: -0.23228293657302856
Batch 8/64 loss: -0.2275109887123108
Batch 9/64 loss: -0.22008371353149414
Batch 10/64 loss: -0.20991665124893188
Batch 11/64 loss: -0.22209429740905762
Batch 12/64 loss: -0.2071111798286438
Batch 13/64 loss: -0.23128756880760193
Batch 14/64 loss: -0.22600224614143372
Batch 15/64 loss: -0.239776611328125
Batch 16/64 loss: -0.22545212507247925
Batch 17/64 loss: -0.2255476415157318
Batch 18/64 loss: -0.24371880292892456
Batch 19/64 loss: -0.20526748895645142
Batch 20/64 loss: -0.20632755756378174
Batch 21/64 loss: -0.22208291292190552
Batch 22/64 loss: -0.202367901802063
Batch 23/64 loss: -0.2304101586341858
Batch 24/64 loss: -0.22247502207756042
Batch 25/64 loss: -0.22454935312271118
Batch 26/64 loss: -0.20447921752929688
Batch 27/64 loss: -0.20696675777435303
Batch 28/64 loss: -0.20927149057388306
Batch 29/64 loss: -0.1944788694381714
Batch 30/64 loss: -0.1897796392440796
Batch 31/64 loss: -0.20738136768341064
Batch 32/64 loss: -0.22166261076927185
Batch 33/64 loss: -0.22397655248641968
Batch 34/64 loss: -0.20943117141723633
Batch 35/64 loss: -0.2046918272972107
Batch 36/64 loss: -0.22855091094970703
Batch 37/64 loss: -0.20959508419036865
Batch 38/64 loss: -0.2344939112663269
Batch 39/64 loss: -0.18281978368759155
Batch 40/64 loss: -0.19522905349731445
Batch 41/64 loss: -0.19314920902252197
Batch 42/64 loss: -0.22274553775787354
Batch 43/64 loss: -0.19832855463027954
Batch 44/64 loss: -0.21159720420837402
Batch 45/64 loss: -0.18805408477783203
Batch 46/64 loss: -0.219188392162323
Batch 47/64 loss: -0.20886075496673584
Batch 48/64 loss: -0.22629374265670776
Batch 49/64 loss: -0.2118312120437622
Batch 50/64 loss: -0.22255468368530273
Batch 51/64 loss: -0.22431617975234985
Batch 52/64 loss: -0.20145082473754883
Batch 53/64 loss: -0.2207215130329132
Batch 54/64 loss: -0.2113250494003296
Batch 55/64 loss: -0.21190565824508667
Batch 56/64 loss: -0.22589606046676636
Batch 57/64 loss: -0.219690203666687
Batch 58/64 loss: -0.2373722493648529
Batch 59/64 loss: -0.21210205554962158
Batch 60/64 loss: -0.2119741439819336
Batch 61/64 loss: -0.19756633043289185
Batch 62/64 loss: -0.21959245204925537
Batch 63/64 loss: -0.21583908796310425
Batch 64/64 loss: -0.1950177550315857
Epoch 390  Train loss: -0.21472732342925727  Val loss: -0.0394247815371379
Epoch 391
-------------------------------
Batch 1/64 loss: -0.23198962211608887
Batch 2/64 loss: -0.22689080238342285
Batch 3/64 loss: -0.2145242691040039
Batch 4/64 loss: -0.23986083269119263
Batch 5/64 loss: -0.22916638851165771
Batch 6/64 loss: -0.20473939180374146
Batch 7/64 loss: -0.22790318727493286
Batch 8/64 loss: -0.23919230699539185
Batch 9/64 loss: -0.21421661972999573
Batch 10/64 loss: -0.19523859024047852
Batch 11/64 loss: -0.14956527948379517
Batch 12/64 loss: -0.2399517297744751
Batch 13/64 loss: -0.22440886497497559
Batch 14/64 loss: -0.22217202186584473
Batch 15/64 loss: -0.21855154633522034
Batch 16/64 loss: -0.2260981798171997
Batch 17/64 loss: -0.21056675910949707
Batch 18/64 loss: -0.2107832431793213
Batch 19/64 loss: -0.2282383143901825
Batch 20/64 loss: -0.22608476877212524
Batch 21/64 loss: -0.20509099960327148
Batch 22/64 loss: -0.21064776182174683
Batch 23/64 loss: -0.236042320728302
Batch 24/64 loss: -0.21809977293014526
Batch 25/64 loss: -0.2240440845489502
Batch 26/64 loss: -0.2184467315673828
Batch 27/64 loss: -0.2078971266746521
Batch 28/64 loss: -0.2019742727279663
Batch 29/64 loss: -0.22240155935287476
Batch 30/64 loss: -0.22771888971328735
Batch 31/64 loss: -0.198286771774292
Batch 32/64 loss: -0.23802489042282104
Batch 33/64 loss: -0.2286217212677002
Batch 34/64 loss: -0.20167672634124756
Batch 35/64 loss: -0.21773940324783325
Batch 36/64 loss: -0.22380006313323975
Batch 37/64 loss: -0.20777934789657593
Batch 38/64 loss: -0.2017168402671814
Batch 39/64 loss: -0.18067485094070435
Batch 40/64 loss: -0.23741844296455383
Batch 41/64 loss: -0.2247193455696106
Batch 42/64 loss: -0.22285962104797363
Batch 43/64 loss: -0.20019066333770752
Batch 44/64 loss: -0.220997154712677
Batch 45/64 loss: -0.1993931531906128
Batch 46/64 loss: -0.20305263996124268
Batch 47/64 loss: -0.22293606400489807
Batch 48/64 loss: -0.23250210285186768
Batch 49/64 loss: -0.22348785400390625
Batch 50/64 loss: -0.21788573265075684
Batch 51/64 loss: -0.22702556848526
Batch 52/64 loss: -0.22053062915802002
Batch 53/64 loss: -0.22422358393669128
Batch 54/64 loss: -0.2296350598335266
Batch 55/64 loss: -0.20601904392242432
Batch 56/64 loss: -0.20308297872543335
Batch 57/64 loss: -0.20607155561447144
Batch 58/64 loss: -0.2332659363746643
Batch 59/64 loss: -0.2237580120563507
Batch 60/64 loss: -0.2418864369392395
Batch 61/64 loss: -0.20638465881347656
Batch 62/64 loss: -0.21956932544708252
Batch 63/64 loss: -0.21570903062820435
Batch 64/64 loss: -0.2511080801486969
Epoch 391  Train loss: -0.21806686286832772  Val loss: -0.037172035048507746
Epoch 392
-------------------------------
Batch 1/64 loss: -0.24367013573646545
Batch 2/64 loss: -0.22820058465003967
Batch 3/64 loss: -0.22478145360946655
Batch 4/64 loss: -0.20604175329208374
Batch 5/64 loss: -0.2171497941017151
Batch 6/64 loss: -0.1987825632095337
Batch 7/64 loss: -0.23173683881759644
Batch 8/64 loss: -0.21854650974273682
Batch 9/64 loss: -0.21955668926239014
Batch 10/64 loss: -0.18091970682144165
Batch 11/64 loss: -0.22701221704483032
Batch 12/64 loss: -0.22770148515701294
Batch 13/64 loss: -0.23504990339279175
Batch 14/64 loss: -0.19932085275650024
Batch 15/64 loss: -0.21932345628738403
Batch 16/64 loss: -0.21497702598571777
Batch 17/64 loss: -0.21147102117538452
Batch 18/64 loss: -0.19079208374023438
Batch 19/64 loss: -0.22437435388565063
Batch 20/64 loss: -0.21109670400619507
Batch 21/64 loss: -0.22479206323623657
Batch 22/64 loss: -0.2033403515815735
Batch 23/64 loss: -0.20500236749649048
Batch 24/64 loss: -0.18896335363388062
Batch 25/64 loss: -0.18623507022857666
Batch 26/64 loss: -0.19858884811401367
Batch 27/64 loss: -0.17419123649597168
Batch 28/64 loss: -0.21293902397155762
Batch 29/64 loss: -0.21666336059570312
Batch 30/64 loss: -0.22021549940109253
Batch 31/64 loss: -0.2071293592453003
Batch 32/64 loss: -0.21855109930038452
Batch 33/64 loss: -0.2064780592918396
Batch 34/64 loss: -0.232883483171463
Batch 35/64 loss: -0.2076188325881958
Batch 36/64 loss: -0.22127443552017212
Batch 37/64 loss: -0.20011210441589355
Batch 38/64 loss: -0.21609646081924438
Batch 39/64 loss: -0.218918114900589
Batch 40/64 loss: -0.22136300802230835
Batch 41/64 loss: -0.20926785469055176
Batch 42/64 loss: -0.22204476594924927
Batch 43/64 loss: -0.2282850742340088
Batch 44/64 loss: -0.22143793106079102
Batch 45/64 loss: -0.217855304479599
Batch 46/64 loss: -0.2346099615097046
Batch 47/64 loss: -0.18936151266098022
Batch 48/64 loss: -0.2206781506538391
Batch 49/64 loss: -0.20784902572631836
Batch 50/64 loss: -0.2216947376728058
Batch 51/64 loss: -0.23995441198349
Batch 52/64 loss: -0.21413838863372803
Batch 53/64 loss: -0.21579790115356445
Batch 54/64 loss: -0.18886810541152954
Batch 55/64 loss: -0.23094835877418518
Batch 56/64 loss: -0.19656389951705933
Batch 57/64 loss: -0.18290436267852783
Batch 58/64 loss: -0.2143285870552063
Batch 59/64 loss: -0.17515742778778076
Batch 60/64 loss: -0.24029546976089478
Batch 61/64 loss: -0.20037585496902466
Batch 62/64 loss: -0.20986568927764893
Batch 63/64 loss: -0.2218177318572998
Batch 64/64 loss: -0.20420628786087036
Epoch 392  Train loss: -0.21284888608782898  Val loss: -0.03896379184067454
Epoch 393
-------------------------------
Batch 1/64 loss: -0.2133256196975708
Batch 2/64 loss: -0.2302316129207611
Batch 3/64 loss: -0.2029086947441101
Batch 4/64 loss: -0.23927807807922363
Batch 5/64 loss: -0.21816563606262207
Batch 6/64 loss: -0.22410434484481812
Batch 7/64 loss: -0.18959105014801025
Batch 8/64 loss: -0.22136709094047546
Batch 9/64 loss: -0.21932578086853027
Batch 10/64 loss: -0.2299376130104065
Batch 11/64 loss: -0.23211896419525146
Batch 12/64 loss: -0.20867860317230225
Batch 13/64 loss: -0.21187323331832886
Batch 14/64 loss: -0.2085740566253662
Batch 15/64 loss: -0.21978482604026794
Batch 16/64 loss: -0.21794909238815308
Batch 17/64 loss: -0.23079562187194824
Batch 18/64 loss: -0.22265556454658508
Batch 19/64 loss: -0.21696165204048157
Batch 20/64 loss: -0.22619599103927612
Batch 21/64 loss: -0.22348374128341675
Batch 22/64 loss: -0.21830135583877563
Batch 23/64 loss: -0.21613773703575134
Batch 24/64 loss: -0.21817275881767273
Batch 25/64 loss: -0.24842676520347595
Batch 26/64 loss: -0.24725675582885742
Batch 27/64 loss: -0.24369722604751587
Batch 28/64 loss: -0.2283417284488678
Batch 29/64 loss: -0.24238914251327515
Batch 30/64 loss: -0.2104972004890442
Batch 31/64 loss: -0.2167051136493683
Batch 32/64 loss: -0.230243980884552
Batch 33/64 loss: -0.23362469673156738
Batch 34/64 loss: -0.2408376932144165
Batch 35/64 loss: -0.20541387796401978
Batch 36/64 loss: -0.1905684471130371
Batch 37/64 loss: -0.21726197004318237
Batch 38/64 loss: -0.23797625303268433
Batch 39/64 loss: -0.22463172674179077
Batch 40/64 loss: -0.2385644018650055
Batch 41/64 loss: -0.21321392059326172
Batch 42/64 loss: -0.21441411972045898
Batch 43/64 loss: -0.23182988166809082
Batch 44/64 loss: -0.2389959692955017
Batch 45/64 loss: -0.21263998746871948
Batch 46/64 loss: -0.2236875295639038
Batch 47/64 loss: -0.23304015398025513
Batch 48/64 loss: -0.1914994716644287
Batch 49/64 loss: -0.2219381034374237
Batch 50/64 loss: -0.2233709692955017
Batch 51/64 loss: -0.22111710906028748
Batch 52/64 loss: -0.22223523259162903
Batch 53/64 loss: -0.21272122859954834
Batch 54/64 loss: -0.21284985542297363
Batch 55/64 loss: -0.19874542951583862
Batch 56/64 loss: -0.22018808126449585
Batch 57/64 loss: -0.19601380825042725
Batch 58/64 loss: -0.2129313349723816
Batch 59/64 loss: -0.22416287660598755
Batch 60/64 loss: -0.19483435153961182
Batch 61/64 loss: -0.21065545082092285
Batch 62/64 loss: -0.21231651306152344
Batch 63/64 loss: -0.2073381543159485
Batch 64/64 loss: -0.21479851007461548
Epoch 393  Train loss: -0.2200501037578957  Val loss: -0.03774974108561618
Epoch 394
-------------------------------
Batch 1/64 loss: -0.22583472728729248
Batch 2/64 loss: -0.23422771692276
Batch 3/64 loss: -0.21385717391967773
Batch 4/64 loss: -0.19357943534851074
Batch 5/64 loss: -0.2022864818572998
Batch 6/64 loss: -0.24245697259902954
Batch 7/64 loss: -0.2282356321811676
Batch 8/64 loss: -0.22367948293685913
Batch 9/64 loss: -0.21986523270606995
Batch 10/64 loss: -0.22537696361541748
Batch 11/64 loss: -0.21378391981124878
Batch 12/64 loss: -0.1983073353767395
Batch 13/64 loss: -0.18718546628952026
Batch 14/64 loss: -0.22568970918655396
Batch 15/64 loss: -0.22550496459007263
Batch 16/64 loss: -0.19249451160430908
Batch 17/64 loss: -0.22291642427444458
Batch 18/64 loss: -0.2027360200881958
Batch 19/64 loss: -0.2156938910484314
Batch 20/64 loss: -0.2307915985584259
Batch 21/64 loss: -0.2236870527267456
Batch 22/64 loss: -0.2057034969329834
Batch 23/64 loss: -0.23517277836799622
Batch 24/64 loss: -0.2178196907043457
Batch 25/64 loss: -0.2367159128189087
Batch 26/64 loss: -0.2005627155303955
Batch 27/64 loss: -0.22423964738845825
Batch 28/64 loss: -0.22828766703605652
Batch 29/64 loss: -0.2158830165863037
Batch 30/64 loss: -0.23788374662399292
Batch 31/64 loss: -0.236649751663208
Batch 32/64 loss: -0.21651166677474976
Batch 33/64 loss: -0.21810102462768555
Batch 34/64 loss: -0.23046660423278809
Batch 35/64 loss: -0.24060773849487305
Batch 36/64 loss: -0.20394372940063477
Batch 37/64 loss: -0.20315688848495483
Batch 38/64 loss: -0.21702730655670166
Batch 39/64 loss: -0.22293460369110107
Batch 40/64 loss: -0.22776257991790771
Batch 41/64 loss: -0.22925662994384766
Batch 42/64 loss: -0.2226804494857788
Batch 43/64 loss: -0.22165009379386902
Batch 44/64 loss: -0.220096617937088
Batch 45/64 loss: -0.19987159967422485
Batch 46/64 loss: -0.23724746704101562
Batch 47/64 loss: -0.24988898634910583
Batch 48/64 loss: -0.2502567172050476
Batch 49/64 loss: -0.19751280546188354
Batch 50/64 loss: -0.2362709939479828
Batch 51/64 loss: -0.2156926393508911
Batch 52/64 loss: -0.21554496884346008
Batch 53/64 loss: -0.23274850845336914
Batch 54/64 loss: -0.2176322340965271
Batch 55/64 loss: -0.2047308087348938
Batch 56/64 loss: -0.22011393308639526
Batch 57/64 loss: -0.2316543459892273
Batch 58/64 loss: -0.21148735284805298
Batch 59/64 loss: -0.2331809401512146
Batch 60/64 loss: -0.24178779125213623
Batch 61/64 loss: -0.21506226062774658
Batch 62/64 loss: -0.22202539443969727
Batch 63/64 loss: -0.22591787576675415
Batch 64/64 loss: -0.17604833841323853
Epoch 394  Train loss: -0.22042304230671303  Val loss: -0.03710368634089572
Epoch 395
-------------------------------
Batch 1/64 loss: -0.21770676970481873
Batch 2/64 loss: -0.23213180899620056
Batch 3/64 loss: -0.2062067985534668
Batch 4/64 loss: -0.24369466304779053
Batch 5/64 loss: -0.22240853309631348
Batch 6/64 loss: -0.2254682183265686
Batch 7/64 loss: -0.23426562547683716
Batch 8/64 loss: -0.23976442217826843
Batch 9/64 loss: -0.20605456829071045
Batch 10/64 loss: -0.21104228496551514
Batch 11/64 loss: -0.22085541486740112
Batch 12/64 loss: -0.23258525133132935
Batch 13/64 loss: -0.2130412459373474
Batch 14/64 loss: -0.22275623679161072
Batch 15/64 loss: -0.23163330554962158
Batch 16/64 loss: -0.24294644594192505
Batch 17/64 loss: -0.23478251695632935
Batch 18/64 loss: -0.23030182719230652
Batch 19/64 loss: -0.20959120988845825
Batch 20/64 loss: -0.24683022499084473
Batch 21/64 loss: -0.2030717134475708
Batch 22/64 loss: -0.23120728135108948
Batch 23/64 loss: -0.176738440990448
Batch 24/64 loss: -0.2112773060798645
Batch 25/64 loss: -0.24469837546348572
Batch 26/64 loss: -0.2210572361946106
Batch 27/64 loss: -0.22003725171089172
Batch 28/64 loss: -0.1942152976989746
Batch 29/64 loss: -0.2005159854888916
Batch 30/64 loss: -0.2059006690979004
Batch 31/64 loss: -0.23061251640319824
Batch 32/64 loss: -0.2376645803451538
Batch 33/64 loss: -0.20083677768707275
Batch 34/64 loss: -0.23465606570243835
Batch 35/64 loss: -0.2382510006427765
Batch 36/64 loss: -0.1893702745437622
Batch 37/64 loss: -0.2120499610900879
Batch 38/64 loss: -0.2247154712677002
Batch 39/64 loss: -0.2306625247001648
Batch 40/64 loss: -0.22870764136314392
Batch 41/64 loss: -0.19776129722595215
Batch 42/64 loss: -0.2208516001701355
Batch 43/64 loss: -0.23417726159095764
Batch 44/64 loss: -0.21730828285217285
Batch 45/64 loss: -0.22099319100379944
Batch 46/64 loss: -0.24339726567268372
Batch 47/64 loss: -0.2046828269958496
Batch 48/64 loss: -0.18326067924499512
Batch 49/64 loss: -0.22100001573562622
Batch 50/64 loss: -0.1895160675048828
Batch 51/64 loss: -0.1965641975402832
Batch 52/64 loss: -0.2293885350227356
Batch 53/64 loss: -0.21776258945465088
Batch 54/64 loss: -0.21975046396255493
Batch 55/64 loss: -0.22124511003494263
Batch 56/64 loss: -0.21331405639648438
Batch 57/64 loss: -0.21913227438926697
Batch 58/64 loss: -0.2055114507675171
Batch 59/64 loss: -0.20386844873428345
Batch 60/64 loss: -0.20695459842681885
Batch 61/64 loss: -0.2251570224761963
Batch 62/64 loss: -0.22870194911956787
Batch 63/64 loss: -0.20541411638259888
Batch 64/64 loss: -0.20855212211608887
Epoch 395  Train loss: -0.21870495899050843  Val loss: -0.03368514919608729
Epoch 396
-------------------------------
Batch 1/64 loss: -0.21822771430015564
Batch 2/64 loss: -0.24565938115119934
Batch 3/64 loss: -0.21800869703292847
Batch 4/64 loss: -0.23553472757339478
Batch 5/64 loss: -0.22610118985176086
Batch 6/64 loss: -0.21816998720169067
Batch 7/64 loss: -0.2213989794254303
Batch 8/64 loss: -0.20546174049377441
Batch 9/64 loss: -0.22129753232002258
Batch 10/64 loss: -0.2021869421005249
Batch 11/64 loss: -0.23534893989562988
Batch 12/64 loss: -0.22685769200325012
Batch 13/64 loss: -0.20870685577392578
Batch 14/64 loss: -0.22491300106048584
Batch 15/64 loss: -0.2330390214920044
Batch 16/64 loss: -0.21353375911712646
Batch 17/64 loss: -0.21418488025665283
Batch 18/64 loss: -0.20944881439208984
Batch 19/64 loss: -0.21046870946884155
Batch 20/64 loss: -0.22757303714752197
Batch 21/64 loss: -0.21210283041000366
Batch 22/64 loss: -0.23180031776428223
Batch 23/64 loss: -0.22880864143371582
Batch 24/64 loss: -0.19924533367156982
Batch 25/64 loss: -0.1966564655303955
Batch 26/64 loss: -0.23254260420799255
Batch 27/64 loss: -0.21968260407447815
Batch 28/64 loss: -0.22196325659751892
Batch 29/64 loss: -0.21760284900665283
Batch 30/64 loss: -0.2083193063735962
Batch 31/64 loss: -0.2020907998085022
Batch 32/64 loss: -0.22168293595314026
Batch 33/64 loss: -0.2362237572669983
Batch 34/64 loss: -0.2088850736618042
Batch 35/64 loss: -0.21305227279663086
Batch 36/64 loss: -0.2181454300880432
Batch 37/64 loss: -0.23166176676750183
Batch 38/64 loss: -0.2180899679660797
Batch 39/64 loss: -0.22247886657714844
Batch 40/64 loss: -0.19826549291610718
Batch 41/64 loss: -0.20264041423797607
Batch 42/64 loss: -0.20733124017715454
Batch 43/64 loss: -0.216325581073761
Batch 44/64 loss: -0.21170645952224731
Batch 45/64 loss: -0.20749813318252563
Batch 46/64 loss: -0.2269536256790161
Batch 47/64 loss: -0.22782453894615173
Batch 48/64 loss: -0.1992625594139099
Batch 49/64 loss: -0.20501840114593506
Batch 50/64 loss: -0.19573765993118286
Batch 51/64 loss: -0.21595245599746704
Batch 52/64 loss: -0.2214900553226471
Batch 53/64 loss: -0.20572024583816528
Batch 54/64 loss: -0.22039592266082764
Batch 55/64 loss: -0.21873056888580322
Batch 56/64 loss: -0.2120915651321411
Batch 57/64 loss: -0.20404678583145142
Batch 58/64 loss: -0.2436733841896057
Batch 59/64 loss: -0.20672351121902466
Batch 60/64 loss: -0.20676618814468384
Batch 61/64 loss: -0.20343327522277832
Batch 62/64 loss: -0.2154289186000824
Batch 63/64 loss: -0.22607016563415527
Batch 64/64 loss: -0.1939285397529602
Epoch 396  Train loss: -0.2164971017370037  Val loss: -0.03418814910646157
Epoch 397
-------------------------------
Batch 1/64 loss: -0.19723737239837646
Batch 2/64 loss: -0.23107707500457764
Batch 3/64 loss: -0.2138945460319519
Batch 4/64 loss: -0.21660619974136353
Batch 5/64 loss: -0.23834294080734253
Batch 6/64 loss: -0.20591306686401367
Batch 7/64 loss: -0.22026261687278748
Batch 8/64 loss: -0.22576886415481567
Batch 9/64 loss: -0.2409725785255432
Batch 10/64 loss: -0.21486473083496094
Batch 11/64 loss: -0.2059396505355835
Batch 12/64 loss: -0.19449388980865479
Batch 13/64 loss: -0.21549934148788452
Batch 14/64 loss: -0.2205844521522522
Batch 15/64 loss: -0.20580971240997314
Batch 16/64 loss: -0.21555817127227783
Batch 17/64 loss: -0.2354927957057953
Batch 18/64 loss: -0.2261177897453308
Batch 19/64 loss: -0.24837344884872437
Batch 20/64 loss: -0.22750288248062134
Batch 21/64 loss: -0.23097926378250122
Batch 22/64 loss: -0.20677632093429565
Batch 23/64 loss: -0.20393306016921997
Batch 24/64 loss: -0.21234357357025146
Batch 25/64 loss: -0.2114795446395874
Batch 26/64 loss: -0.2265164852142334
Batch 27/64 loss: -0.22895905375480652
Batch 28/64 loss: -0.23944184184074402
Batch 29/64 loss: -0.23858952522277832
Batch 30/64 loss: -0.21190571784973145
Batch 31/64 loss: -0.23247084021568298
Batch 32/64 loss: -0.24513673782348633
Batch 33/64 loss: -0.22661179304122925
Batch 34/64 loss: -0.22519519925117493
Batch 35/64 loss: -0.21208351850509644
Batch 36/64 loss: -0.20061606168746948
Batch 37/64 loss: -0.20348960161209106
Batch 38/64 loss: -0.21325057744979858
Batch 39/64 loss: -0.2089071273803711
Batch 40/64 loss: -0.24069565534591675
Batch 41/64 loss: -0.21120542287826538
Batch 42/64 loss: -0.24830710887908936
Batch 43/64 loss: -0.22011446952819824
Batch 44/64 loss: -0.2139378786087036
Batch 45/64 loss: -0.23513495922088623
Batch 46/64 loss: -0.21050304174423218
Batch 47/64 loss: -0.2137451171875
Batch 48/64 loss: -0.2163802981376648
Batch 49/64 loss: -0.2283836007118225
Batch 50/64 loss: -0.22514259815216064
Batch 51/64 loss: -0.22536733746528625
Batch 52/64 loss: -0.22512125968933105
Batch 53/64 loss: -0.19529622793197632
Batch 54/64 loss: -0.19033688306808472
Batch 55/64 loss: -0.20167261362075806
Batch 56/64 loss: -0.1650347113609314
Batch 57/64 loss: -0.19936788082122803
Batch 58/64 loss: -0.23828989267349243
Batch 59/64 loss: -0.22538471221923828
Batch 60/64 loss: -0.2160496711730957
Batch 61/64 loss: -0.2245824635028839
Batch 62/64 loss: -0.22872871160507202
Batch 63/64 loss: -0.22958159446716309
Batch 64/64 loss: -0.20276588201522827
Epoch 397  Train loss: -0.21897155280206718  Val loss: -0.04000842325466195
Epoch 398
-------------------------------
Batch 1/64 loss: -0.19416874647140503
Batch 2/64 loss: -0.18927878141403198
Batch 3/64 loss: -0.25156348943710327
Batch 4/64 loss: -0.21194970607757568
Batch 5/64 loss: -0.22537446022033691
Batch 6/64 loss: -0.22791096568107605
Batch 7/64 loss: -0.2312358021736145
Batch 8/64 loss: -0.21006786823272705
Batch 9/64 loss: -0.21846893429756165
Batch 10/64 loss: -0.23489364981651306
Batch 11/64 loss: -0.21452701091766357
Batch 12/64 loss: -0.21389901638031006
Batch 13/64 loss: -0.23555999994277954
Batch 14/64 loss: -0.23129838705062866
Batch 15/64 loss: -0.23566237092018127
Batch 16/64 loss: -0.24326908588409424
Batch 17/64 loss: -0.23527875542640686
Batch 18/64 loss: -0.23881837725639343
Batch 19/64 loss: -0.2257707715034485
Batch 20/64 loss: -0.22679024934768677
Batch 21/64 loss: -0.20386266708374023
Batch 22/64 loss: -0.2216840386390686
Batch 23/64 loss: -0.194410502910614
Batch 24/64 loss: -0.24164438247680664
Batch 25/64 loss: -0.22941374778747559
Batch 26/64 loss: -0.20867019891738892
Batch 27/64 loss: -0.19656115770339966
Batch 28/64 loss: -0.23359140753746033
Batch 29/64 loss: -0.20957940816879272
Batch 30/64 loss: -0.2289755940437317
Batch 31/64 loss: -0.18029570579528809
Batch 32/64 loss: -0.21195077896118164
Batch 33/64 loss: -0.20674002170562744
Batch 34/64 loss: -0.21490156650543213
Batch 35/64 loss: -0.23556971549987793
Batch 36/64 loss: -0.23605558276176453
Batch 37/64 loss: -0.22328725457191467
Batch 38/64 loss: -0.20554864406585693
Batch 39/64 loss: -0.21677684783935547
Batch 40/64 loss: -0.23082047700881958
Batch 41/64 loss: -0.1780308485031128
Batch 42/64 loss: -0.23634344339370728
Batch 43/64 loss: -0.22689521312713623
Batch 44/64 loss: -0.19697606563568115
Batch 45/64 loss: -0.21014267206192017
Batch 46/64 loss: -0.21038979291915894
Batch 47/64 loss: -0.2201242744922638
Batch 48/64 loss: -0.2165539264678955
Batch 49/64 loss: -0.21661800146102905
Batch 50/64 loss: -0.23040908575057983
Batch 51/64 loss: -0.19293248653411865
Batch 52/64 loss: -0.19431042671203613
Batch 53/64 loss: -0.20904755592346191
Batch 54/64 loss: -0.21047872304916382
Batch 55/64 loss: -0.2049109935760498
Batch 56/64 loss: -0.21309518814086914
Batch 57/64 loss: -0.2216862440109253
Batch 58/64 loss: -0.22797641158103943
Batch 59/64 loss: -0.19757306575775146
Batch 60/64 loss: -0.19164234399795532
Batch 61/64 loss: -0.20723611116409302
Batch 62/64 loss: -0.19775229692459106
Batch 63/64 loss: -0.2111438512802124
Batch 64/64 loss: -0.22963476181030273
Epoch 398  Train loss: -0.21679405838835472  Val loss: -0.03797970809477711
Epoch 399
-------------------------------
Batch 1/64 loss: -0.22231367230415344
Batch 2/64 loss: -0.22629940509796143
Batch 3/64 loss: -0.24215292930603027
Batch 4/64 loss: -0.20859390497207642
Batch 5/64 loss: -0.18973404169082642
Batch 6/64 loss: -0.20756292343139648
Batch 7/64 loss: -0.23738664388656616
Batch 8/64 loss: -0.20496565103530884
Batch 9/64 loss: -0.21698161959648132
Batch 10/64 loss: -0.22180312871932983
Batch 11/64 loss: -0.24486291408538818
Batch 12/64 loss: -0.2353043258190155
Batch 13/64 loss: -0.2144092321395874
Batch 14/64 loss: -0.2268698811531067
Batch 15/64 loss: -0.1835760474205017
Batch 16/64 loss: -0.21469157934188843
Batch 17/64 loss: -0.21177935600280762
Batch 18/64 loss: -0.20920097827911377
Batch 19/64 loss: -0.20857810974121094
Batch 20/64 loss: -0.22925665974617004
Batch 21/64 loss: -0.2240481972694397
Batch 22/64 loss: -0.22487589716911316
Batch 23/64 loss: -0.23277467489242554
Batch 24/64 loss: -0.22582221031188965
Batch 25/64 loss: -0.23707211017608643
Batch 26/64 loss: -0.22635528445243835
Batch 27/64 loss: -0.22268980741500854
Batch 28/64 loss: -0.2062740921974182
Batch 29/64 loss: -0.21088284254074097
Batch 30/64 loss: -0.22306370735168457
Batch 31/64 loss: -0.20070147514343262
Batch 32/64 loss: -0.21612107753753662
Batch 33/64 loss: -0.23820358514785767
Batch 34/64 loss: -0.24512255191802979
Batch 35/64 loss: -0.19632554054260254
Batch 36/64 loss: -0.19905942678451538
Batch 37/64 loss: -0.22855937480926514
Batch 38/64 loss: -0.2202916145324707
Batch 39/64 loss: -0.2414233684539795
Batch 40/64 loss: -0.20569294691085815
Batch 41/64 loss: -0.22580671310424805
Batch 42/64 loss: -0.19601362943649292
Batch 43/64 loss: -0.22709250450134277
Batch 44/64 loss: -0.20604854822158813
Batch 45/64 loss: -0.2232714593410492
Batch 46/64 loss: -0.22737812995910645
Batch 47/64 loss: -0.21793889999389648
Batch 48/64 loss: -0.23546266555786133
Batch 49/64 loss: -0.20764201879501343
Batch 50/64 loss: -0.22743037343025208
Batch 51/64 loss: -0.2210381031036377
Batch 52/64 loss: -0.20860004425048828
Batch 53/64 loss: -0.22860682010650635
Batch 54/64 loss: -0.21046340465545654
Batch 55/64 loss: -0.20095854997634888
Batch 56/64 loss: -0.21772080659866333
Batch 57/64 loss: -0.22582286596298218
Batch 58/64 loss: -0.2102251648902893
Batch 59/64 loss: -0.214491069316864
Batch 60/64 loss: -0.2439766228199005
Batch 61/64 loss: -0.22653251886367798
Batch 62/64 loss: -0.194949209690094
Batch 63/64 loss: -0.22547346353530884
Batch 64/64 loss: -0.22475147247314453
Epoch 399  Train loss: -0.2191872938006532  Val loss: -0.03835204131005146
Epoch 400
-------------------------------
Batch 1/64 loss: -0.2024264931678772
Batch 2/64 loss: -0.218124657869339
Batch 3/64 loss: -0.22838348150253296
Batch 4/64 loss: -0.21872949600219727
Batch 5/64 loss: -0.234624445438385
Batch 6/64 loss: -0.23700881004333496
Batch 7/64 loss: -0.22699061036109924
Batch 8/64 loss: -0.22093424201011658
Batch 9/64 loss: -0.22138285636901855
Batch 10/64 loss: -0.2292901575565338
Batch 11/64 loss: -0.23248884081840515
Batch 12/64 loss: -0.21588215231895447
Batch 13/64 loss: -0.21969282627105713
Batch 14/64 loss: -0.21146619319915771
Batch 15/64 loss: -0.23799476027488708
Batch 16/64 loss: -0.1916695237159729
Batch 17/64 loss: -0.22871935367584229
Batch 18/64 loss: -0.2046259641647339
Batch 19/64 loss: -0.2387094497680664
Batch 20/64 loss: -0.1879480481147766
Batch 21/64 loss: -0.22479289770126343
Batch 22/64 loss: -0.2085365653038025
Batch 23/64 loss: -0.20145809650421143
Batch 24/64 loss: -0.2389155626296997
Batch 25/64 loss: -0.21603691577911377
Batch 26/64 loss: -0.22538721561431885
Batch 27/64 loss: -0.21044057607650757
Batch 28/64 loss: -0.22888797521591187
Batch 29/64 loss: -0.21654805541038513
Batch 30/64 loss: -0.22705918550491333
Batch 31/64 loss: -0.2469126582145691
Batch 32/64 loss: -0.23244211077690125
Batch 33/64 loss: -0.22981560230255127
Batch 34/64 loss: -0.2158781886100769
Batch 35/64 loss: -0.23049384355545044
Batch 36/64 loss: -0.19667404890060425
Batch 37/64 loss: -0.2421054244041443
Batch 38/64 loss: -0.2033778429031372
Batch 39/64 loss: -0.24151551723480225
Batch 40/64 loss: -0.19956398010253906
Batch 41/64 loss: -0.23199158906936646
Batch 42/64 loss: -0.22090357542037964
Batch 43/64 loss: -0.2151503562927246
Batch 44/64 loss: -0.2322821319103241
Batch 45/64 loss: -0.21444201469421387
Batch 46/64 loss: -0.20316874980926514
Batch 47/64 loss: -0.19929802417755127
Batch 48/64 loss: -0.21605199575424194
Batch 49/64 loss: -0.20719397068023682
Batch 50/64 loss: -0.22582510113716125
Batch 51/64 loss: -0.22839674353599548
Batch 52/64 loss: -0.22769713401794434
Batch 53/64 loss: -0.2189362645149231
Batch 54/64 loss: -0.23205888271331787
Batch 55/64 loss: -0.1952143907546997
Batch 56/64 loss: -0.21539419889450073
Batch 57/64 loss: -0.20831507444381714
Batch 58/64 loss: -0.20693600177764893
Batch 59/64 loss: -0.218951016664505
Batch 60/64 loss: -0.22280123829841614
Batch 61/64 loss: -0.18622112274169922
Batch 62/64 loss: -0.231828510761261
Batch 63/64 loss: -0.2193548083305359
Batch 64/64 loss: -0.24000659584999084
Epoch 400  Train loss: -0.21964474460657904  Val loss: -0.035931350029620925
Epoch 401
-------------------------------
Batch 1/64 loss: -0.2362138330936432
Batch 2/64 loss: -0.22604140639305115
Batch 3/64 loss: -0.2442537546157837
Batch 4/64 loss: -0.24235650897026062
Batch 5/64 loss: -0.21316593885421753
Batch 6/64 loss: -0.2161080539226532
Batch 7/64 loss: -0.19412755966186523
Batch 8/64 loss: -0.22565379738807678
Batch 9/64 loss: -0.2226632833480835
Batch 10/64 loss: -0.22965776920318604
Batch 11/64 loss: -0.2327670454978943
Batch 12/64 loss: -0.23017460107803345
Batch 13/64 loss: -0.24470111727714539
Batch 14/64 loss: -0.24133500456809998
Batch 15/64 loss: -0.18125873804092407
Batch 16/64 loss: -0.21699586510658264
Batch 17/64 loss: -0.2388494610786438
Batch 18/64 loss: -0.2402476966381073
Batch 19/64 loss: -0.20981788635253906
Batch 20/64 loss: -0.2280433177947998
Batch 21/64 loss: -0.22290188074111938
Batch 22/64 loss: -0.2318306863307953
Batch 23/64 loss: -0.20297032594680786
Batch 24/64 loss: -0.19623637199401855
Batch 25/64 loss: -0.23077255487442017
Batch 26/64 loss: -0.22821635007858276
Batch 27/64 loss: -0.21803629398345947
Batch 28/64 loss: -0.1966259479522705
Batch 29/64 loss: -0.23327931761741638
Batch 30/64 loss: -0.21733039617538452
Batch 31/64 loss: -0.22173723578453064
Batch 32/64 loss: -0.224209725856781
Batch 33/64 loss: -0.23604339361190796
Batch 34/64 loss: -0.23389160633087158
Batch 35/64 loss: -0.22272449731826782
Batch 36/64 loss: -0.23732787370681763
Batch 37/64 loss: -0.22702628374099731
Batch 38/64 loss: -0.2401190996170044
Batch 39/64 loss: -0.2241499125957489
Batch 40/64 loss: -0.23301368951797485
Batch 41/64 loss: -0.229964017868042
Batch 42/64 loss: -0.24091368913650513
Batch 43/64 loss: -0.21107035875320435
Batch 44/64 loss: -0.23105132579803467
Batch 45/64 loss: -0.2308882474899292
Batch 46/64 loss: -0.21682578325271606
Batch 47/64 loss: -0.22636836767196655
Batch 48/64 loss: -0.23736190795898438
Batch 49/64 loss: -0.221378356218338
Batch 50/64 loss: -0.22362428903579712
Batch 51/64 loss: -0.23596858978271484
Batch 52/64 loss: -0.21545574069023132
Batch 53/64 loss: -0.2049965262413025
Batch 54/64 loss: -0.24048486351966858
Batch 55/64 loss: -0.249414324760437
Batch 56/64 loss: -0.22880136966705322
Batch 57/64 loss: -0.23627430200576782
Batch 58/64 loss: -0.2513791620731354
Batch 59/64 loss: -0.20112407207489014
Batch 60/64 loss: -0.22185319662094116
Batch 61/64 loss: -0.2086278200149536
Batch 62/64 loss: -0.2230445146560669
Batch 63/64 loss: -0.2245556116104126
Batch 64/64 loss: -0.20054668188095093
Epoch 401  Train loss: -0.22517196127012665  Val loss: -0.03414812141267704
Epoch 402
-------------------------------
Batch 1/64 loss: -0.22378376126289368
Batch 2/64 loss: -0.25504037737846375
Batch 3/64 loss: -0.24424049258232117
Batch 4/64 loss: -0.2360137701034546
Batch 5/64 loss: -0.22037208080291748
Batch 6/64 loss: -0.24494576454162598
Batch 7/64 loss: -0.24558350443840027
Batch 8/64 loss: -0.2416980266571045
Batch 9/64 loss: -0.2253234088420868
Batch 10/64 loss: -0.22809907793998718
Batch 11/64 loss: -0.21471136808395386
Batch 12/64 loss: -0.19897955656051636
Batch 13/64 loss: -0.20185130834579468
Batch 14/64 loss: -0.23561671376228333
Batch 15/64 loss: -0.18560391664505005
Batch 16/64 loss: -0.24479830265045166
Batch 17/64 loss: -0.23419004678726196
Batch 18/64 loss: -0.2407715916633606
Batch 19/64 loss: -0.20730674266815186
Batch 20/64 loss: -0.21403825283050537
Batch 21/64 loss: -0.22062748670578003
Batch 22/64 loss: -0.23713690042495728
Batch 23/64 loss: -0.23220425844192505
Batch 24/64 loss: -0.23102819919586182
Batch 25/64 loss: -0.23882418870925903
Batch 26/64 loss: -0.22913026809692383
Batch 27/64 loss: -0.18728601932525635
Batch 28/64 loss: -0.2226097285747528
Batch 29/64 loss: -0.22568660974502563
Batch 30/64 loss: -0.23130404949188232
Batch 31/64 loss: -0.218747079372406
Batch 32/64 loss: -0.22208622097969055
Batch 33/64 loss: -0.2393416464328766
Batch 34/64 loss: -0.22812560200691223
Batch 35/64 loss: -0.21153461933135986
Batch 36/64 loss: -0.2388865351676941
Batch 37/64 loss: -0.225080668926239
Batch 38/64 loss: -0.23193401098251343
Batch 39/64 loss: -0.23887979984283447
Batch 40/64 loss: -0.23296105861663818
Batch 41/64 loss: -0.22336235642433167
Batch 42/64 loss: -0.161318838596344
Batch 43/64 loss: -0.24122226238250732
Batch 44/64 loss: -0.21515491604804993
Batch 45/64 loss: -0.20120465755462646
Batch 46/64 loss: -0.2389909327030182
Batch 47/64 loss: -0.22511014342308044
Batch 48/64 loss: -0.1977519989013672
Batch 49/64 loss: -0.2287079095840454
Batch 50/64 loss: -0.20928478240966797
Batch 51/64 loss: -0.21976137161254883
Batch 52/64 loss: -0.22357526421546936
Batch 53/64 loss: -0.21596673130989075
Batch 54/64 loss: -0.22263506054878235
Batch 55/64 loss: -0.23444023728370667
Batch 56/64 loss: -0.22551795840263367
Batch 57/64 loss: -0.215472012758255
Batch 58/64 loss: -0.20806264877319336
Batch 59/64 loss: -0.22145241498947144
Batch 60/64 loss: -0.22724300622940063
Batch 61/64 loss: -0.21153491735458374
Batch 62/64 loss: -0.23301365971565247
Batch 63/64 loss: -0.19353491067886353
Batch 64/64 loss: -0.19018834829330444
Epoch 402  Train loss: -0.22311126694959751  Val loss: -0.04092796745988512
Epoch 403
-------------------------------
Batch 1/64 loss: -0.22537505626678467
Batch 2/64 loss: -0.2356027066707611
Batch 3/64 loss: -0.24441802501678467
Batch 4/64 loss: -0.23102915287017822
Batch 5/64 loss: -0.22885075211524963
Batch 6/64 loss: -0.23090001940727234
Batch 7/64 loss: -0.2384037971496582
Batch 8/64 loss: -0.2085070013999939
Batch 9/64 loss: -0.2474149465560913
Batch 10/64 loss: -0.24759677052497864
Batch 11/64 loss: -0.2361830472946167
Batch 12/64 loss: -0.20871686935424805
Batch 13/64 loss: -0.21804070472717285
Batch 14/64 loss: -0.23911252617835999
Batch 15/64 loss: -0.23781991004943848
Batch 16/64 loss: -0.2288915514945984
Batch 17/64 loss: -0.21443024277687073
Batch 18/64 loss: -0.24614813923835754
Batch 19/64 loss: -0.23395156860351562
Batch 20/64 loss: -0.22723937034606934
Batch 21/64 loss: -0.23313894867897034
Batch 22/64 loss: -0.22081279754638672
Batch 23/64 loss: -0.19846457242965698
Batch 24/64 loss: -0.22156968712806702
Batch 25/64 loss: -0.20439088344573975
Batch 26/64 loss: -0.25282198190689087
Batch 27/64 loss: -0.21044987440109253
Batch 28/64 loss: -0.23137551546096802
Batch 29/64 loss: -0.2285900115966797
Batch 30/64 loss: -0.18939679861068726
Batch 31/64 loss: -0.22931909561157227
Batch 32/64 loss: -0.22141748666763306
Batch 33/64 loss: -0.19947540760040283
Batch 34/64 loss: -0.23241448402404785
Batch 35/64 loss: -0.20772439241409302
Batch 36/64 loss: -0.22458937764167786
Batch 37/64 loss: -0.2220292091369629
Batch 38/64 loss: -0.2364812195301056
Batch 39/64 loss: -0.2035422921180725
Batch 40/64 loss: -0.1991279125213623
Batch 41/64 loss: -0.185671865940094
Batch 42/64 loss: -0.21692326664924622
Batch 43/64 loss: -0.22859814763069153
Batch 44/64 loss: -0.2504884600639343
Batch 45/64 loss: -0.236314594745636
Batch 46/64 loss: -0.21281147003173828
Batch 47/64 loss: -0.229089617729187
Batch 48/64 loss: -0.20616227388381958
Batch 49/64 loss: -0.18305158615112305
Batch 50/64 loss: -0.1958456039428711
Batch 51/64 loss: -0.22936701774597168
Batch 52/64 loss: -0.2225612998008728
Batch 53/64 loss: -0.23238155245780945
Batch 54/64 loss: -0.1921827793121338
Batch 55/64 loss: -0.21919089555740356
Batch 56/64 loss: -0.2274235486984253
Batch 57/64 loss: -0.22555994987487793
Batch 58/64 loss: -0.2181369662284851
Batch 59/64 loss: -0.22132468223571777
Batch 60/64 loss: -0.21436673402786255
Batch 61/64 loss: -0.2423543930053711
Batch 62/64 loss: -0.20668673515319824
Batch 63/64 loss: -0.20324546098709106
Batch 64/64 loss: -0.21577852964401245
Epoch 403  Train loss: -0.22207587302899828  Val loss: -0.0349459732111377
Epoch 404
-------------------------------
Batch 1/64 loss: -0.24923834204673767
Batch 2/64 loss: -0.21601653099060059
Batch 3/64 loss: -0.2441844344139099
Batch 4/64 loss: -0.20067238807678223
Batch 5/64 loss: -0.23010027408599854
Batch 6/64 loss: -0.22611260414123535
Batch 7/64 loss: -0.2296508252620697
Batch 8/64 loss: -0.22148257493972778
Batch 9/64 loss: -0.22600960731506348
Batch 10/64 loss: -0.2059239149093628
Batch 11/64 loss: -0.2376686930656433
Batch 12/64 loss: -0.1873016357421875
Batch 13/64 loss: -0.25303512811660767
Batch 14/64 loss: -0.253149151802063
Batch 15/64 loss: -0.21486034989356995
Batch 16/64 loss: -0.24991083145141602
Batch 17/64 loss: -0.22274062037467957
Batch 18/64 loss: -0.2492830455303192
Batch 19/64 loss: -0.23812061548233032
Batch 20/64 loss: -0.21566644310951233
Batch 21/64 loss: -0.22575527429580688
Batch 22/64 loss: -0.20350128412246704
Batch 23/64 loss: -0.21165311336517334
Batch 24/64 loss: -0.21750354766845703
Batch 25/64 loss: -0.18215346336364746
Batch 26/64 loss: -0.22730666399002075
Batch 27/64 loss: -0.22062557935714722
Batch 28/64 loss: -0.22603699564933777
Batch 29/64 loss: -0.21628636121749878
Batch 30/64 loss: -0.22889524698257446
Batch 31/64 loss: -0.2297966480255127
Batch 32/64 loss: -0.21083950996398926
Batch 33/64 loss: -0.23211205005645752
Batch 34/64 loss: -0.20512771606445312
Batch 35/64 loss: -0.21242940425872803
Batch 36/64 loss: -0.24138593673706055
Batch 37/64 loss: -0.2322463095188141
Batch 38/64 loss: -0.17737990617752075
Batch 39/64 loss: -0.2266942858695984
Batch 40/64 loss: -0.1710752248764038
Batch 41/64 loss: -0.23753690719604492
Batch 42/64 loss: -0.2121589183807373
Batch 43/64 loss: -0.20297271013259888
Batch 44/64 loss: -0.2100314497947693
Batch 45/64 loss: -0.21035069227218628
Batch 46/64 loss: -0.22414171695709229
Batch 47/64 loss: -0.2270461916923523
Batch 48/64 loss: -0.1958865523338318
Batch 49/64 loss: -0.24031123518943787
Batch 50/64 loss: -0.22556105256080627
Batch 51/64 loss: -0.22277694940567017
Batch 52/64 loss: -0.22177797555923462
Batch 53/64 loss: -0.20450466871261597
Batch 54/64 loss: -0.2335738241672516
Batch 55/64 loss: -0.23958444595336914
Batch 56/64 loss: -0.21931341290473938
Batch 57/64 loss: -0.24116379022598267
Batch 58/64 loss: -0.2265639305114746
Batch 59/64 loss: -0.2239859402179718
Batch 60/64 loss: -0.23088225722312927
Batch 61/64 loss: -0.23451745510101318
Batch 62/64 loss: -0.24267730116844177
Batch 63/64 loss: -0.226849764585495
Batch 64/64 loss: -0.21642833948135376
Epoch 404  Train loss: -0.22253212437910191  Val loss: -0.036472406993616896
Epoch 405
-------------------------------
Batch 1/64 loss: -0.24580484628677368
Batch 2/64 loss: -0.25670337677001953
Batch 3/64 loss: -0.229638934135437
Batch 4/64 loss: -0.2463580071926117
Batch 5/64 loss: -0.25971558690071106
Batch 6/64 loss: -0.2598455250263214
Batch 7/64 loss: -0.23420816659927368
Batch 8/64 loss: -0.23290878534317017
Batch 9/64 loss: -0.2246546447277069
Batch 10/64 loss: -0.21334677934646606
Batch 11/64 loss: -0.20573395490646362
Batch 12/64 loss: -0.23185187578201294
Batch 13/64 loss: -0.22855114936828613
Batch 14/64 loss: -0.21480786800384521
Batch 15/64 loss: -0.23199516534805298
Batch 16/64 loss: -0.1868281364440918
Batch 17/64 loss: -0.23669666051864624
Batch 18/64 loss: -0.20147407054901123
Batch 19/64 loss: -0.2109420895576477
Batch 20/64 loss: -0.2428184151649475
Batch 21/64 loss: -0.23755890130996704
Batch 22/64 loss: -0.24385300278663635
Batch 23/64 loss: -0.20676851272583008
Batch 24/64 loss: -0.23315215110778809
Batch 25/64 loss: -0.2226458489894867
Batch 26/64 loss: -0.2245045304298401
Batch 27/64 loss: -0.2235649824142456
Batch 28/64 loss: -0.23099535703659058
Batch 29/64 loss: -0.21983659267425537
Batch 30/64 loss: -0.23213475942611694
Batch 31/64 loss: -0.20168781280517578
Batch 32/64 loss: -0.2254367470741272
Batch 33/64 loss: -0.22652772068977356
Batch 34/64 loss: -0.23393899202346802
Batch 35/64 loss: -0.23321834206581116
Batch 36/64 loss: -0.22512072324752808
Batch 37/64 loss: -0.2516156733036041
Batch 38/64 loss: -0.2397749423980713
Batch 39/64 loss: -0.22778883576393127
Batch 40/64 loss: -0.23551952838897705
Batch 41/64 loss: -0.22511562705039978
Batch 42/64 loss: -0.22909244894981384
Batch 43/64 loss: -0.22788023948669434
Batch 44/64 loss: -0.233778715133667
Batch 45/64 loss: -0.19005751609802246
Batch 46/64 loss: -0.23827320337295532
Batch 47/64 loss: -0.17914444208145142
Batch 48/64 loss: -0.20124435424804688
Batch 49/64 loss: -0.2131805419921875
Batch 50/64 loss: -0.2440459132194519
Batch 51/64 loss: -0.23967388272285461
Batch 52/64 loss: -0.22610998153686523
Batch 53/64 loss: -0.21545496582984924
Batch 54/64 loss: -0.22739750146865845
Batch 55/64 loss: -0.21334481239318848
Batch 56/64 loss: -0.2228764295578003
Batch 57/64 loss: -0.20537453889846802
Batch 58/64 loss: -0.22340962290763855
Batch 59/64 loss: -0.2103826403617859
Batch 60/64 loss: -0.22238650918006897
Batch 61/64 loss: -0.23318040370941162
Batch 62/64 loss: -0.22271966934204102
Batch 63/64 loss: -0.2045331597328186
Batch 64/64 loss: -0.24513059854507446
Epoch 405  Train loss: -0.22592993039710849  Val loss: -0.03740412058289518
Epoch 406
-------------------------------
Batch 1/64 loss: -0.2330990433692932
Batch 2/64 loss: -0.23573875427246094
Batch 3/64 loss: -0.2358497679233551
Batch 4/64 loss: -0.22286158800125122
Batch 5/64 loss: -0.22876733541488647
Batch 6/64 loss: -0.20985078811645508
Batch 7/64 loss: -0.23330426216125488
Batch 8/64 loss: -0.23308604955673218
Batch 9/64 loss: -0.22089546918869019
Batch 10/64 loss: -0.23801559209823608
Batch 11/64 loss: -0.22258999943733215
Batch 12/64 loss: -0.2408934235572815
Batch 13/64 loss: -0.24147745966911316
Batch 14/64 loss: -0.2442007064819336
Batch 15/64 loss: -0.2491357922554016
Batch 16/64 loss: -0.24117985367774963
Batch 17/64 loss: -0.22417119145393372
Batch 18/64 loss: -0.23272868990898132
Batch 19/64 loss: -0.21539801359176636
Batch 20/64 loss: -0.23745989799499512
Batch 21/64 loss: -0.22969627380371094
Batch 22/64 loss: -0.2300574779510498
Batch 23/64 loss: -0.208407461643219
Batch 24/64 loss: -0.21831589937210083
Batch 25/64 loss: -0.23463916778564453
Batch 26/64 loss: -0.23991242051124573
Batch 27/64 loss: -0.26315468549728394
Batch 28/64 loss: -0.25262054800987244
Batch 29/64 loss: -0.23526281118392944
Batch 30/64 loss: -0.21306145191192627
Batch 31/64 loss: -0.21675395965576172
Batch 32/64 loss: -0.2099379301071167
Batch 33/64 loss: -0.23725873231887817
Batch 34/64 loss: -0.23841893672943115
Batch 35/64 loss: -0.23886972665786743
Batch 36/64 loss: -0.22783339023590088
Batch 37/64 loss: -0.25169241428375244
Batch 38/64 loss: -0.23728707432746887
Batch 39/64 loss: -0.21742740273475647
Batch 40/64 loss: -0.22404581308364868
Batch 41/64 loss: -0.23561066389083862
Batch 42/64 loss: -0.2151506543159485
Batch 43/64 loss: -0.2255711555480957
Batch 44/64 loss: -0.2297399938106537
Batch 45/64 loss: -0.23661929368972778
Batch 46/64 loss: -0.22440105676651
Batch 47/64 loss: -0.22440046072006226
Batch 48/64 loss: -0.20182842016220093
Batch 49/64 loss: -0.22822651267051697
Batch 50/64 loss: -0.22494840621948242
Batch 51/64 loss: -0.2104601263999939
Batch 52/64 loss: -0.2025742530822754
Batch 53/64 loss: -0.18232345581054688
Batch 54/64 loss: -0.219482421875
Batch 55/64 loss: -0.23020458221435547
Batch 56/64 loss: -0.20788532495498657
Batch 57/64 loss: -0.1986750364303589
Batch 58/64 loss: -0.23246264457702637
Batch 59/64 loss: -0.23499250411987305
Batch 60/64 loss: -0.21853286027908325
Batch 61/64 loss: -0.21035653352737427
Batch 62/64 loss: -0.2212456464767456
Batch 63/64 loss: -0.19934910535812378
Batch 64/64 loss: -0.22363561391830444
Epoch 406  Train loss: -0.2266372561454773  Val loss: -0.037995320005515185
Epoch 407
-------------------------------
Batch 1/64 loss: -0.24517273902893066
Batch 2/64 loss: -0.2314460277557373
Batch 3/64 loss: -0.24550312757492065
Batch 4/64 loss: -0.2340097427368164
Batch 5/64 loss: -0.2238815724849701
Batch 6/64 loss: -0.24476709961891174
Batch 7/64 loss: -0.23923489451408386
Batch 8/64 loss: -0.20838773250579834
Batch 9/64 loss: -0.2427312135696411
Batch 10/64 loss: -0.1992570161819458
Batch 11/64 loss: -0.20414257049560547
Batch 12/64 loss: -0.21863830089569092
Batch 13/64 loss: -0.2068876028060913
Batch 14/64 loss: -0.2103704810142517
Batch 15/64 loss: -0.20286548137664795
Batch 16/64 loss: -0.21550172567367554
Batch 17/64 loss: -0.20052069425582886
Batch 18/64 loss: -0.2238650619983673
Batch 19/64 loss: -0.23415794968605042
Batch 20/64 loss: -0.23570120334625244
Batch 21/64 loss: -0.23161917924880981
Batch 22/64 loss: -0.22070825099945068
Batch 23/64 loss: -0.21353977918624878
Batch 24/64 loss: -0.23730266094207764
Batch 25/64 loss: -0.21782582998275757
Batch 26/64 loss: -0.22334367036819458
Batch 27/64 loss: -0.17764919996261597
Batch 28/64 loss: -0.20687949657440186
Batch 29/64 loss: -0.22261783480644226
Batch 30/64 loss: -0.21266525983810425
Batch 31/64 loss: -0.21903741359710693
Batch 32/64 loss: -0.2037665843963623
Batch 33/64 loss: -0.20980465412139893
Batch 34/64 loss: -0.23342502117156982
Batch 35/64 loss: -0.2060452699661255
Batch 36/64 loss: -0.2062915563583374
Batch 37/64 loss: -0.22362130880355835
Batch 38/64 loss: -0.2407684028148651
Batch 39/64 loss: -0.18573719263076782
Batch 40/64 loss: -0.18290096521377563
Batch 41/64 loss: -0.22913923859596252
Batch 42/64 loss: -0.22246119379997253
Batch 43/64 loss: -0.23389309644699097
Batch 44/64 loss: -0.21423858404159546
Batch 45/64 loss: -0.21217375993728638
Batch 46/64 loss: -0.2261086106300354
Batch 47/64 loss: -0.23097538948059082
Batch 48/64 loss: -0.24427786469459534
Batch 49/64 loss: -0.2375280261039734
Batch 50/64 loss: -0.22197911143302917
Batch 51/64 loss: -0.23677492141723633
Batch 52/64 loss: -0.25038406252861023
Batch 53/64 loss: -0.2319328784942627
Batch 54/64 loss: -0.23433300852775574
Batch 55/64 loss: -0.23417913913726807
Batch 56/64 loss: -0.20000267028808594
Batch 57/64 loss: -0.23359635472297668
Batch 58/64 loss: -0.2272028625011444
Batch 59/64 loss: -0.22203993797302246
Batch 60/64 loss: -0.21207952499389648
Batch 61/64 loss: -0.21499380469322205
Batch 62/64 loss: -0.21230852603912354
Batch 63/64 loss: -0.2420666217803955
Batch 64/64 loss: -0.23498019576072693
Epoch 407  Train loss: -0.2218587624091728  Val loss: -0.03830379206700014
Epoch 408
-------------------------------
Batch 1/64 loss: -0.23224413394927979
Batch 2/64 loss: -0.2468109428882599
Batch 3/64 loss: -0.23424720764160156
Batch 4/64 loss: -0.24351173639297485
Batch 5/64 loss: -0.19986605644226074
Batch 6/64 loss: -0.22155827283859253
Batch 7/64 loss: -0.25266310572624207
Batch 8/64 loss: -0.22300735116004944
Batch 9/64 loss: -0.2382071614265442
Batch 10/64 loss: -0.2447616457939148
Batch 11/64 loss: -0.24484360218048096
Batch 12/64 loss: -0.2513294219970703
Batch 13/64 loss: -0.24445974826812744
Batch 14/64 loss: -0.23716551065444946
Batch 15/64 loss: -0.2401496171951294
Batch 16/64 loss: -0.2083594799041748
Batch 17/64 loss: -0.22949334979057312
Batch 18/64 loss: -0.2170802354812622
Batch 19/64 loss: -0.18392229080200195
Batch 20/64 loss: -0.24748563766479492
Batch 21/64 loss: -0.22712469100952148
Batch 22/64 loss: -0.24489766359329224
Batch 23/64 loss: -0.21937328577041626
Batch 24/64 loss: -0.24489134550094604
Batch 25/64 loss: -0.21810680627822876
Batch 26/64 loss: -0.22233888506889343
Batch 27/64 loss: -0.23003286123275757
Batch 28/64 loss: -0.21304768323898315
Batch 29/64 loss: -0.2241286337375641
Batch 30/64 loss: -0.2160865068435669
Batch 31/64 loss: -0.2357596755027771
Batch 32/64 loss: -0.22530776262283325
Batch 33/64 loss: -0.19003015756607056
Batch 34/64 loss: -0.2323148250579834
Batch 35/64 loss: -0.2094862461090088
Batch 36/64 loss: -0.24035018682479858
Batch 37/64 loss: -0.20373880863189697
Batch 38/64 loss: -0.21846964955329895
Batch 39/64 loss: -0.24468699097633362
Batch 40/64 loss: -0.23982664942741394
Batch 41/64 loss: -0.22358128428459167
Batch 42/64 loss: -0.21730372309684753
Batch 43/64 loss: -0.21815261244773865
Batch 44/64 loss: -0.22058212757110596
Batch 45/64 loss: -0.22113743424415588
Batch 46/64 loss: -0.22794216871261597
Batch 47/64 loss: -0.23918214440345764
Batch 48/64 loss: -0.22676336765289307
Batch 49/64 loss: -0.24097353219985962
Batch 50/64 loss: -0.20352870225906372
Batch 51/64 loss: -0.2070530652999878
Batch 52/64 loss: -0.21561884880065918
Batch 53/64 loss: -0.20936161279678345
Batch 54/64 loss: -0.2191808819770813
Batch 55/64 loss: -0.20290780067443848
Batch 56/64 loss: -0.21205580234527588
Batch 57/64 loss: -0.20417386293411255
Batch 58/64 loss: -0.20927375555038452
Batch 59/64 loss: -0.2374778389930725
Batch 60/64 loss: -0.22553130984306335
Batch 61/64 loss: -0.2432464063167572
Batch 62/64 loss: -0.2191384732723236
Batch 63/64 loss: -0.2104474902153015
Batch 64/64 loss: -0.24100333452224731
Epoch 408  Train loss: -0.22551423638474707  Val loss: -0.035953529716766984
Epoch 409
-------------------------------
Batch 1/64 loss: -0.23460602760314941
Batch 2/64 loss: -0.23388490080833435
Batch 3/64 loss: -0.21215343475341797
Batch 4/64 loss: -0.24679148197174072
Batch 5/64 loss: -0.19814938306808472
Batch 6/64 loss: -0.17668652534484863
Batch 7/64 loss: -0.24370187520980835
Batch 8/64 loss: -0.22062066197395325
Batch 9/64 loss: -0.22998106479644775
Batch 10/64 loss: -0.23706784844398499
Batch 11/64 loss: -0.2262418270111084
Batch 12/64 loss: -0.17090481519699097
Batch 13/64 loss: -0.2270755171775818
Batch 14/64 loss: -0.21497005224227905
Batch 15/64 loss: -0.22735166549682617
Batch 16/64 loss: -0.21510359644889832
Batch 17/64 loss: -0.19964754581451416
Batch 18/64 loss: -0.20821350812911987
Batch 19/64 loss: -0.22507140040397644
Batch 20/64 loss: -0.233914315700531
Batch 21/64 loss: -0.23291581869125366
Batch 22/64 loss: -0.240566223859787
Batch 23/64 loss: -0.23893409967422485
Batch 24/64 loss: -0.24064922332763672
Batch 25/64 loss: -0.21497398614883423
Batch 26/64 loss: -0.25096720457077026
Batch 27/64 loss: -0.22927340865135193
Batch 28/64 loss: -0.20431947708129883
Batch 29/64 loss: -0.22261005640029907
Batch 30/64 loss: -0.246172696352005
Batch 31/64 loss: -0.23158475756645203
Batch 32/64 loss: -0.2356124222278595
Batch 33/64 loss: -0.23453140258789062
Batch 34/64 loss: -0.21229326725006104
Batch 35/64 loss: -0.24662357568740845
Batch 36/64 loss: -0.20292514562606812
Batch 37/64 loss: -0.20105749368667603
Batch 38/64 loss: -0.23503106832504272
Batch 39/64 loss: -0.24656984210014343
Batch 40/64 loss: -0.2260885238647461
Batch 41/64 loss: -0.23202669620513916
Batch 42/64 loss: -0.2309630811214447
Batch 43/64 loss: -0.2455531358718872
Batch 44/64 loss: -0.2442602515220642
Batch 45/64 loss: -0.18678951263427734
Batch 46/64 loss: -0.24505874514579773
Batch 47/64 loss: -0.2195967435836792
Batch 48/64 loss: -0.24419447779655457
Batch 49/64 loss: -0.18151849508285522
Batch 50/64 loss: -0.225597083568573
Batch 51/64 loss: -0.23584070801734924
Batch 52/64 loss: -0.24640130996704102
Batch 53/64 loss: -0.1657620668411255
Batch 54/64 loss: -0.22854843735694885
Batch 55/64 loss: -0.2400977611541748
Batch 56/64 loss: -0.22987747192382812
Batch 57/64 loss: -0.24282127618789673
Batch 58/64 loss: -0.2252047061920166
Batch 59/64 loss: -0.24474835395812988
Batch 60/64 loss: -0.23156636953353882
Batch 61/64 loss: -0.2261456847190857
Batch 62/64 loss: -0.23507100343704224
Batch 63/64 loss: -0.22098183631896973
Batch 64/64 loss: -0.21399936079978943
Epoch 409  Train loss: -0.22533273521591635  Val loss: -0.034459667107493604
Epoch 410
-------------------------------
Batch 1/64 loss: -0.22820648550987244
Batch 2/64 loss: -0.22679850459098816
Batch 3/64 loss: -0.21540534496307373
Batch 4/64 loss: -0.2372388243675232
Batch 5/64 loss: -0.22627893090248108
Batch 6/64 loss: -0.2174283266067505
Batch 7/64 loss: -0.21661391854286194
Batch 8/64 loss: -0.22409242391586304
Batch 9/64 loss: -0.20446234941482544
Batch 10/64 loss: -0.22036778926849365
Batch 11/64 loss: -0.21670961380004883
Batch 12/64 loss: -0.21151387691497803
Batch 13/64 loss: -0.22333306074142456
Batch 14/64 loss: -0.19667643308639526
Batch 15/64 loss: -0.22312551736831665
Batch 16/64 loss: -0.22862708568572998
Batch 17/64 loss: -0.22667518258094788
Batch 18/64 loss: -0.23806655406951904
Batch 19/64 loss: -0.21964284777641296
Batch 20/64 loss: -0.23296207189559937
Batch 21/64 loss: -0.23480892181396484
Batch 22/64 loss: -0.20876067876815796
Batch 23/64 loss: -0.24153858423233032
Batch 24/64 loss: -0.201138436794281
Batch 25/64 loss: -0.21443486213684082
Batch 26/64 loss: -0.19150370359420776
Batch 27/64 loss: -0.24665987491607666
Batch 28/64 loss: -0.24049916863441467
Batch 29/64 loss: -0.21971583366394043
Batch 30/64 loss: -0.22859597206115723
Batch 31/64 loss: -0.21266818046569824
Batch 32/64 loss: -0.23940959572792053
Batch 33/64 loss: -0.23466119170188904
Batch 34/64 loss: -0.2448866367340088
Batch 35/64 loss: -0.22091329097747803
Batch 36/64 loss: -0.21210306882858276
Batch 37/64 loss: -0.2589440643787384
Batch 38/64 loss: -0.19804227352142334
Batch 39/64 loss: -0.22431612014770508
Batch 40/64 loss: -0.21956932544708252
Batch 41/64 loss: -0.23843204975128174
Batch 42/64 loss: -0.21393856406211853
Batch 43/64 loss: -0.22900256514549255
Batch 44/64 loss: -0.21685686707496643
Batch 45/64 loss: -0.199587881565094
Batch 46/64 loss: -0.21920549869537354
Batch 47/64 loss: -0.22525939345359802
Batch 48/64 loss: -0.22954031825065613
Batch 49/64 loss: -0.22099369764328003
Batch 50/64 loss: -0.21465370059013367
Batch 51/64 loss: -0.24492555856704712
Batch 52/64 loss: -0.2167208194732666
Batch 53/64 loss: -0.2243438959121704
Batch 54/64 loss: -0.20538437366485596
Batch 55/64 loss: -0.2260226309299469
Batch 56/64 loss: -0.22187581658363342
Batch 57/64 loss: -0.24097132682800293
Batch 58/64 loss: -0.19710713624954224
Batch 59/64 loss: -0.25216400623321533
Batch 60/64 loss: -0.23924535512924194
Batch 61/64 loss: -0.24463346600532532
Batch 62/64 loss: -0.2186291515827179
Batch 63/64 loss: -0.22739309072494507
Batch 64/64 loss: -0.2239707112312317
Epoch 410  Train loss: -0.22372172743666405  Val loss: -0.03829478807875381
Epoch 411
-------------------------------
Batch 1/64 loss: -0.2267114520072937
Batch 2/64 loss: -0.2626469135284424
Batch 3/64 loss: -0.20555955171585083
Batch 4/64 loss: -0.22908419370651245
Batch 5/64 loss: -0.24856775999069214
Batch 6/64 loss: -0.23953711986541748
Batch 7/64 loss: -0.21673208475112915
Batch 8/64 loss: -0.21242636442184448
Batch 9/64 loss: -0.19768935441970825
Batch 10/64 loss: -0.23417207598686218
Batch 11/64 loss: -0.22649142146110535
Batch 12/64 loss: -0.2322169542312622
Batch 13/64 loss: -0.24388739466667175
Batch 14/64 loss: -0.23353791236877441
Batch 15/64 loss: -0.217867910861969
Batch 16/64 loss: -0.22592967748641968
Batch 17/64 loss: -0.2317304015159607
Batch 18/64 loss: -0.22365736961364746
Batch 19/64 loss: -0.23873192071914673
Batch 20/64 loss: -0.2579318881034851
Batch 21/64 loss: -0.23203492164611816
Batch 22/64 loss: -0.22730591893196106
Batch 23/64 loss: -0.21974676847457886
Batch 24/64 loss: -0.23159146308898926
Batch 25/64 loss: -0.24285203218460083
Batch 26/64 loss: -0.1919877529144287
Batch 27/64 loss: -0.210796058177948
Batch 28/64 loss: -0.2316427230834961
Batch 29/64 loss: -0.2452356219291687
Batch 30/64 loss: -0.23447132110595703
Batch 31/64 loss: -0.2123379111289978
Batch 32/64 loss: -0.2382141649723053
Batch 33/64 loss: -0.2104933261871338
Batch 34/64 loss: -0.24786978960037231
Batch 35/64 loss: -0.2384597659111023
Batch 36/64 loss: -0.2070426344871521
Batch 37/64 loss: -0.20991629362106323
Batch 38/64 loss: -0.22815197706222534
Batch 39/64 loss: -0.2265228033065796
Batch 40/64 loss: -0.24814927577972412
Batch 41/64 loss: -0.24073967337608337
Batch 42/64 loss: -0.22503936290740967
Batch 43/64 loss: -0.22415244579315186
Batch 44/64 loss: -0.23771220445632935
Batch 45/64 loss: -0.2215253710746765
Batch 46/64 loss: -0.22274038195610046
Batch 47/64 loss: -0.22491168975830078
Batch 48/64 loss: -0.21590614318847656
Batch 49/64 loss: -0.23707997798919678
Batch 50/64 loss: -0.2074032425880432
Batch 51/64 loss: -0.21926993131637573
Batch 52/64 loss: -0.21502521634101868
Batch 53/64 loss: -0.21056878566741943
Batch 54/64 loss: -0.23197555541992188
Batch 55/64 loss: -0.24908968806266785
Batch 56/64 loss: -0.23477593064308167
Batch 57/64 loss: -0.22408515214920044
Batch 58/64 loss: -0.23814162611961365
Batch 59/64 loss: -0.23935261368751526
Batch 60/64 loss: -0.2345733940601349
Batch 61/64 loss: -0.2099742889404297
Batch 62/64 loss: -0.2194095253944397
Batch 63/64 loss: -0.22122347354888916
Batch 64/64 loss: -0.21207767724990845
Epoch 411  Train loss: -0.2275084890571295  Val loss: -0.03423012675288616
Epoch 412
-------------------------------
Batch 1/64 loss: -0.2328331470489502
Batch 2/64 loss: -0.22514626383781433
Batch 3/64 loss: -0.20425957441329956
Batch 4/64 loss: -0.22163453698158264
Batch 5/64 loss: -0.21422719955444336
Batch 6/64 loss: -0.2111731767654419
Batch 7/64 loss: -0.2442418932914734
Batch 8/64 loss: -0.20699572563171387
Batch 9/64 loss: -0.19773638248443604
Batch 10/64 loss: -0.23233112692832947
Batch 11/64 loss: -0.25389307737350464
Batch 12/64 loss: -0.2268160879611969
Batch 13/64 loss: -0.22770139575004578
Batch 14/64 loss: -0.25899988412857056
Batch 15/64 loss: -0.20915430784225464
Batch 16/64 loss: -0.23994803428649902
Batch 17/64 loss: -0.2342008352279663
Batch 18/64 loss: -0.23844444751739502
Batch 19/64 loss: -0.22172096371650696
Batch 20/64 loss: -0.2389289140701294
Batch 21/64 loss: -0.23720967769622803
Batch 22/64 loss: -0.22513744235038757
Batch 23/64 loss: -0.24002695083618164
Batch 24/64 loss: -0.2124595046043396
Batch 25/64 loss: -0.22416287660598755
Batch 26/64 loss: -0.22872161865234375
Batch 27/64 loss: -0.2556561231613159
Batch 28/64 loss: -0.21567678451538086
Batch 29/64 loss: -0.22504907846450806
Batch 30/64 loss: -0.23866915702819824
Batch 31/64 loss: -0.2236832082271576
Batch 32/64 loss: -0.23040378093719482
Batch 33/64 loss: -0.25150221586227417
Batch 34/64 loss: -0.22577303647994995
Batch 35/64 loss: -0.2302771806716919
Batch 36/64 loss: -0.21983307600021362
Batch 37/64 loss: -0.22015750408172607
Batch 38/64 loss: -0.2198985517024994
Batch 39/64 loss: -0.20019876956939697
Batch 40/64 loss: -0.2333555817604065
Batch 41/64 loss: -0.23005294799804688
Batch 42/64 loss: -0.2306738793849945
Batch 43/64 loss: -0.22837957739830017
Batch 44/64 loss: -0.2227143943309784
Batch 45/64 loss: -0.20927834510803223
Batch 46/64 loss: -0.2321549654006958
Batch 47/64 loss: -0.22286608815193176
Batch 48/64 loss: -0.2246803641319275
Batch 49/64 loss: -0.2129460871219635
Batch 50/64 loss: -0.2151412069797516
Batch 51/64 loss: -0.23758256435394287
Batch 52/64 loss: -0.24096333980560303
Batch 53/64 loss: -0.22503018379211426
Batch 54/64 loss: -0.2602089047431946
Batch 55/64 loss: -0.22214478254318237
Batch 56/64 loss: -0.226798415184021
Batch 57/64 loss: -0.19518917798995972
Batch 58/64 loss: -0.23967912793159485
Batch 59/64 loss: -0.251611590385437
Batch 60/64 loss: -0.2519548237323761
Batch 61/64 loss: -0.2344050407409668
Batch 62/64 loss: -0.23095303773880005
Batch 63/64 loss: -0.21987411379814148
Batch 64/64 loss: -0.18692058324813843
Epoch 412  Train loss: -0.22750921553256465  Val loss: -0.037064084687183814
Epoch 413
-------------------------------
Batch 1/64 loss: -0.23987343907356262
Batch 2/64 loss: -0.2304288148880005
Batch 3/64 loss: -0.2402918040752411
Batch 4/64 loss: -0.21967780590057373
Batch 5/64 loss: -0.24574631452560425
Batch 6/64 loss: -0.26871395111083984
Batch 7/64 loss: -0.2557833194732666
Batch 8/64 loss: -0.23532849550247192
Batch 9/64 loss: -0.2400711178779602
Batch 10/64 loss: -0.2315416932106018
Batch 11/64 loss: -0.23672401905059814
Batch 12/64 loss: -0.24806919693946838
Batch 13/64 loss: -0.23905795812606812
Batch 14/64 loss: -0.23468056321144104
Batch 15/64 loss: -0.20827853679656982
Batch 16/64 loss: -0.22981250286102295
Batch 17/64 loss: -0.2337942123413086
Batch 18/64 loss: -0.20332741737365723
Batch 19/64 loss: -0.22885662317276
Batch 20/64 loss: -0.22530171275138855
Batch 21/64 loss: -0.23972511291503906
Batch 22/64 loss: -0.24754735827445984
Batch 23/64 loss: -0.22032195329666138
Batch 24/64 loss: -0.23267683386802673
Batch 25/64 loss: -0.23788636922836304
Batch 26/64 loss: -0.23665982484817505
Batch 27/64 loss: -0.2164868414402008
Batch 28/64 loss: -0.2318904995918274
Batch 29/64 loss: -0.21704387664794922
Batch 30/64 loss: -0.23133569955825806
Batch 31/64 loss: -0.22907418012619019
Batch 32/64 loss: -0.2213890552520752
Batch 33/64 loss: -0.24481046199798584
Batch 34/64 loss: -0.22439390420913696
Batch 35/64 loss: -0.2274037003517151
Batch 36/64 loss: -0.24135321378707886
Batch 37/64 loss: -0.16066813468933105
Batch 38/64 loss: -0.20741784572601318
Batch 39/64 loss: -0.20001214742660522
Batch 40/64 loss: -0.21864652633666992
Batch 41/64 loss: -0.24229520559310913
Batch 42/64 loss: -0.23135387897491455
Batch 43/64 loss: -0.2233278751373291
Batch 44/64 loss: -0.2290658950805664
Batch 45/64 loss: -0.21339750289916992
Batch 46/64 loss: -0.21731555461883545
Batch 47/64 loss: -0.2174895703792572
Batch 48/64 loss: -0.24475079774856567
Batch 49/64 loss: -0.2270958423614502
Batch 50/64 loss: -0.21725445985794067
Batch 51/64 loss: -0.19054603576660156
Batch 52/64 loss: -0.24428322911262512
Batch 53/64 loss: -0.22896966338157654
Batch 54/64 loss: -0.2393902838230133
Batch 55/64 loss: -0.2165951430797577
Batch 56/64 loss: -0.20890194177627563
Batch 57/64 loss: -0.23783725500106812
Batch 58/64 loss: -0.24575871229171753
Batch 59/64 loss: -0.2285228669643402
Batch 60/64 loss: -0.2108249068260193
Batch 61/64 loss: -0.2370356321334839
Batch 62/64 loss: -0.24001860618591309
Batch 63/64 loss: -0.23322784900665283
Batch 64/64 loss: -0.2271835207939148
Epoch 413  Train loss: -0.2286705786106633  Val loss: -0.03730342293932676
Epoch 414
-------------------------------
Batch 1/64 loss: -0.25322169065475464
Batch 2/64 loss: -0.23849371075630188
Batch 3/64 loss: -0.20114785432815552
Batch 4/64 loss: -0.204287588596344
Batch 5/64 loss: -0.23332399129867554
Batch 6/64 loss: -0.2349557876586914
Batch 7/64 loss: -0.2496776580810547
Batch 8/64 loss: -0.20766288042068481
Batch 9/64 loss: -0.2537158727645874
Batch 10/64 loss: -0.2461749017238617
Batch 11/64 loss: -0.21930891275405884
Batch 12/64 loss: -0.21819472312927246
Batch 13/64 loss: -0.19886893033981323
Batch 14/64 loss: -0.23590761423110962
Batch 15/64 loss: -0.19229185581207275
Batch 16/64 loss: -0.2368229627609253
Batch 17/64 loss: -0.23872387409210205
Batch 18/64 loss: -0.2313988208770752
Batch 19/64 loss: -0.21555745601654053
Batch 20/64 loss: -0.23579418659210205
Batch 21/64 loss: -0.22241467237472534
Batch 22/64 loss: -0.22827738523483276
Batch 23/64 loss: -0.27061551809310913
Batch 24/64 loss: -0.24741768836975098
Batch 25/64 loss: -0.23206403851509094
Batch 26/64 loss: -0.21458297967910767
Batch 27/64 loss: -0.20746934413909912
Batch 28/64 loss: -0.22732394933700562
Batch 29/64 loss: -0.23891079425811768
Batch 30/64 loss: -0.23953300714492798
Batch 31/64 loss: -0.2209329903125763
Batch 32/64 loss: -0.2068554162979126
Batch 33/64 loss: -0.23902621865272522
Batch 34/64 loss: -0.20088642835617065
Batch 35/64 loss: -0.228876531124115
Batch 36/64 loss: -0.2221091389656067
Batch 37/64 loss: -0.22093939781188965
Batch 38/64 loss: -0.2349810004234314
Batch 39/64 loss: -0.2181781828403473
Batch 40/64 loss: -0.2305716872215271
Batch 41/64 loss: -0.24218958616256714
Batch 42/64 loss: -0.2476431131362915
Batch 43/64 loss: -0.2300589680671692
Batch 44/64 loss: -0.22329717874526978
Batch 45/64 loss: -0.24482673406600952
Batch 46/64 loss: -0.22018983960151672
Batch 47/64 loss: -0.2326851487159729
Batch 48/64 loss: -0.22013753652572632
Batch 49/64 loss: -0.19721931219100952
Batch 50/64 loss: -0.1869543194770813
Batch 51/64 loss: -0.20860230922698975
Batch 52/64 loss: -0.2419721484184265
Batch 53/64 loss: -0.2122575342655182
Batch 54/64 loss: -0.21794894337654114
Batch 55/64 loss: -0.21984392404556274
Batch 56/64 loss: -0.2266446352005005
Batch 57/64 loss: -0.22097283601760864
Batch 58/64 loss: -0.2190537452697754
Batch 59/64 loss: -0.23029109835624695
Batch 60/64 loss: -0.22672486305236816
Batch 61/64 loss: -0.24845585227012634
Batch 62/64 loss: -0.23576277494430542
Batch 63/64 loss: -0.23209336400032043
Batch 64/64 loss: -0.2079613208770752
Epoch 414  Train loss: -0.2264987356522504  Val loss: -0.03778605936319148
Epoch 415
-------------------------------
Batch 1/64 loss: -0.23600554466247559
Batch 2/64 loss: -0.2438046932220459
Batch 3/64 loss: -0.23908835649490356
Batch 4/64 loss: -0.23611950874328613
Batch 5/64 loss: -0.2544911205768585
Batch 6/64 loss: -0.2440773844718933
Batch 7/64 loss: -0.23509731888771057
Batch 8/64 loss: -0.22263193130493164
Batch 9/64 loss: -0.23127150535583496
Batch 10/64 loss: -0.2275073528289795
Batch 11/64 loss: -0.2568890452384949
Batch 12/64 loss: -0.21077871322631836
Batch 13/64 loss: -0.23287618160247803
Batch 14/64 loss: -0.23425054550170898
Batch 15/64 loss: -0.1593860387802124
Batch 16/64 loss: -0.2331017553806305
Batch 17/64 loss: -0.24056336283683777
Batch 18/64 loss: -0.2157309651374817
Batch 19/64 loss: -0.231947124004364
Batch 20/64 loss: -0.20116865634918213
Batch 21/64 loss: -0.20721280574798584
Batch 22/64 loss: -0.23945438861846924
Batch 23/64 loss: -0.21757829189300537
Batch 24/64 loss: -0.20966893434524536
Batch 25/64 loss: -0.21578705310821533
Batch 26/64 loss: -0.2369484305381775
Batch 27/64 loss: -0.21395862102508545
Batch 28/64 loss: -0.2319432497024536
Batch 29/64 loss: -0.2291724979877472
Batch 30/64 loss: -0.21788042783737183
Batch 31/64 loss: -0.21537140011787415
Batch 32/64 loss: -0.22364568710327148
Batch 33/64 loss: -0.22649884223937988
Batch 34/64 loss: -0.19981950521469116
Batch 35/64 loss: -0.25286808609962463
Batch 36/64 loss: -0.23955631256103516
Batch 37/64 loss: -0.21928465366363525
Batch 38/64 loss: -0.18815135955810547
Batch 39/64 loss: -0.210421621799469
Batch 40/64 loss: -0.23152852058410645
Batch 41/64 loss: -0.23955601453781128
Batch 42/64 loss: -0.2324252724647522
Batch 43/64 loss: -0.2345954179763794
Batch 44/64 loss: -0.18393784761428833
Batch 45/64 loss: -0.2122688889503479
Batch 46/64 loss: -0.22372260689735413
Batch 47/64 loss: -0.21031814813613892
Batch 48/64 loss: -0.21121227741241455
Batch 49/64 loss: -0.22559842467308044
Batch 50/64 loss: -0.21414810419082642
Batch 51/64 loss: -0.24012136459350586
Batch 52/64 loss: -0.23032861948013306
Batch 53/64 loss: -0.250325083732605
Batch 54/64 loss: -0.22471582889556885
Batch 55/64 loss: -0.20986229181289673
Batch 56/64 loss: -0.19259858131408691
Batch 57/64 loss: -0.22247660160064697
Batch 58/64 loss: -0.24407190084457397
Batch 59/64 loss: -0.25230270624160767
Batch 60/64 loss: -0.2393595576286316
Batch 61/64 loss: -0.2179114818572998
Batch 62/64 loss: -0.249720960855484
Batch 63/64 loss: -0.2184939682483673
Batch 64/64 loss: -0.2098497748374939
Epoch 415  Train loss: -0.22511367961472156  Val loss: -0.03477671629784443
Epoch 416
-------------------------------
Batch 1/64 loss: -0.23450499773025513
Batch 2/64 loss: -0.23607990145683289
Batch 3/64 loss: -0.22585523128509521
Batch 4/64 loss: -0.2341308891773224
Batch 5/64 loss: -0.22420614957809448
Batch 6/64 loss: -0.23237141966819763
Batch 7/64 loss: -0.2266979217529297
Batch 8/64 loss: -0.20435333251953125
Batch 9/64 loss: -0.2341947853565216
Batch 10/64 loss: -0.2327960729598999
Batch 11/64 loss: -0.22791701555252075
Batch 12/64 loss: -0.23162347078323364
Batch 13/64 loss: -0.22050970792770386
Batch 14/64 loss: -0.22460776567459106
Batch 15/64 loss: -0.25575003027915955
Batch 16/64 loss: -0.2223576307296753
Batch 17/64 loss: -0.23565465211868286
Batch 18/64 loss: -0.22128263115882874
Batch 19/64 loss: -0.2305179238319397
Batch 20/64 loss: -0.2263466715812683
Batch 21/64 loss: -0.23015880584716797
Batch 22/64 loss: -0.22938090562820435
Batch 23/64 loss: -0.2329726219177246
Batch 24/64 loss: -0.22995442152023315
Batch 25/64 loss: -0.2512463927268982
Batch 26/64 loss: -0.2350185215473175
Batch 27/64 loss: -0.21446287631988525
Batch 28/64 loss: -0.22540730237960815
Batch 29/64 loss: -0.20173323154449463
Batch 30/64 loss: -0.22745758295059204
Batch 31/64 loss: -0.2121051549911499
Batch 32/64 loss: -0.23560869693756104
Batch 33/64 loss: -0.24237602949142456
Batch 34/64 loss: -0.23689565062522888
Batch 35/64 loss: -0.237200528383255
Batch 36/64 loss: -0.24441462755203247
Batch 37/64 loss: -0.25399667024612427
Batch 38/64 loss: -0.20730185508728027
Batch 39/64 loss: -0.2276771068572998
Batch 40/64 loss: -0.21678638458251953
Batch 41/64 loss: -0.25655871629714966
Batch 42/64 loss: -0.24692228436470032
Batch 43/64 loss: -0.22716906666755676
Batch 44/64 loss: -0.17458653450012207
Batch 45/64 loss: -0.24344086647033691
Batch 46/64 loss: -0.24881377816200256
Batch 47/64 loss: -0.24302077293395996
Batch 48/64 loss: -0.18216776847839355
Batch 49/64 loss: -0.2234039008617401
Batch 50/64 loss: -0.22056472301483154
Batch 51/64 loss: -0.22783887386322021
Batch 52/64 loss: -0.2382487952709198
Batch 53/64 loss: -0.21874475479125977
Batch 54/64 loss: -0.22290760278701782
Batch 55/64 loss: -0.23600199818611145
Batch 56/64 loss: -0.22758829593658447
Batch 57/64 loss: -0.2374275028705597
Batch 58/64 loss: -0.23332390189170837
Batch 59/64 loss: -0.20644640922546387
Batch 60/64 loss: -0.2294575273990631
Batch 61/64 loss: -0.22463679313659668
Batch 62/64 loss: -0.21701741218566895
Batch 63/64 loss: -0.23622897267341614
Batch 64/64 loss: -0.23364296555519104
Epoch 416  Train loss: -0.22857510657871472  Val loss: -0.03626643443844982
Epoch 417
-------------------------------
Batch 1/64 loss: -0.24537140130996704
Batch 2/64 loss: -0.23075520992279053
Batch 3/64 loss: -0.2278512716293335
Batch 4/64 loss: -0.21526408195495605
Batch 5/64 loss: -0.2173393964767456
Batch 6/64 loss: -0.2215445637702942
Batch 7/64 loss: -0.2441565990447998
Batch 8/64 loss: -0.21473398804664612
Batch 9/64 loss: -0.24425309896469116
Batch 10/64 loss: -0.23175156116485596
Batch 11/64 loss: -0.23918557167053223
Batch 12/64 loss: -0.24729347229003906
Batch 13/64 loss: -0.23095667362213135
Batch 14/64 loss: -0.23627161979675293
Batch 15/64 loss: -0.23726603388786316
Batch 16/64 loss: -0.21641549468040466
Batch 17/64 loss: -0.233679860830307
Batch 18/64 loss: -0.2271503508090973
Batch 19/64 loss: -0.21487197279930115
Batch 20/64 loss: -0.22917890548706055
Batch 21/64 loss: -0.20663732290267944
Batch 22/64 loss: -0.23849308490753174
Batch 23/64 loss: -0.2237970530986786
Batch 24/64 loss: -0.2283719778060913
Batch 25/64 loss: -0.21186351776123047
Batch 26/64 loss: -0.2374219298362732
Batch 27/64 loss: -0.19756031036376953
Batch 28/64 loss: -0.23482227325439453
Batch 29/64 loss: -0.24239134788513184
Batch 30/64 loss: -0.20871227979660034
Batch 31/64 loss: -0.23185214400291443
Batch 32/64 loss: -0.2310105264186859
Batch 33/64 loss: -0.24184072017669678
Batch 34/64 loss: -0.2428678274154663
Batch 35/64 loss: -0.22472092509269714
Batch 36/64 loss: -0.22937622666358948
Batch 37/64 loss: -0.23736846446990967
Batch 38/64 loss: -0.21560153365135193
Batch 39/64 loss: -0.22009074687957764
Batch 40/64 loss: -0.1920243501663208
Batch 41/64 loss: -0.2587020993232727
Batch 42/64 loss: -0.21427911520004272
Batch 43/64 loss: -0.235988050699234
Batch 44/64 loss: -0.2448253631591797
Batch 45/64 loss: -0.2197473645210266
Batch 46/64 loss: -0.2533413767814636
Batch 47/64 loss: -0.23983019590377808
Batch 48/64 loss: -0.2061328887939453
Batch 49/64 loss: -0.23031827807426453
Batch 50/64 loss: -0.21420246362686157
Batch 51/64 loss: -0.23935341835021973
Batch 52/64 loss: -0.25180667638778687
Batch 53/64 loss: -0.20151638984680176
Batch 54/64 loss: -0.21067216992378235
Batch 55/64 loss: -0.23780590295791626
Batch 56/64 loss: -0.23636379837989807
Batch 57/64 loss: -0.22963601350784302
Batch 58/64 loss: -0.24454781413078308
Batch 59/64 loss: -0.23411113023757935
Batch 60/64 loss: -0.23427826166152954
Batch 61/64 loss: -0.23854988813400269
Batch 62/64 loss: -0.22878941893577576
Batch 63/64 loss: -0.24766147136688232
Batch 64/64 loss: -0.20914793014526367
Epoch 417  Train loss: -0.2291989990309173  Val loss: -0.03314355025996048
Epoch 418
-------------------------------
Batch 1/64 loss: -0.24713179469108582
Batch 2/64 loss: -0.23894745111465454
Batch 3/64 loss: -0.2381419539451599
Batch 4/64 loss: -0.2212359607219696
Batch 5/64 loss: -0.2190638780593872
Batch 6/64 loss: -0.20945942401885986
Batch 7/64 loss: -0.23031580448150635
Batch 8/64 loss: -0.23644256591796875
Batch 9/64 loss: -0.23274824023246765
Batch 10/64 loss: -0.23246240615844727
Batch 11/64 loss: -0.20314347743988037
Batch 12/64 loss: -0.23805564641952515
Batch 13/64 loss: -0.24715018272399902
Batch 14/64 loss: -0.2582814693450928
Batch 15/64 loss: -0.23408880829811096
Batch 16/64 loss: -0.22092756628990173
Batch 17/64 loss: -0.2224588394165039
Batch 18/64 loss: -0.1879415512084961
Batch 19/64 loss: -0.2259291410446167
Batch 20/64 loss: -0.20346373319625854
Batch 21/64 loss: -0.2400994896888733
Batch 22/64 loss: -0.23574167490005493
Batch 23/64 loss: -0.2250080108642578
Batch 24/64 loss: -0.22705316543579102
Batch 25/64 loss: -0.2248293161392212
Batch 26/64 loss: -0.25284838676452637
Batch 27/64 loss: -0.211944580078125
Batch 28/64 loss: -0.24879097938537598
Batch 29/64 loss: -0.24815985560417175
Batch 30/64 loss: -0.21162539720535278
Batch 31/64 loss: -0.2085343599319458
Batch 32/64 loss: -0.23412847518920898
Batch 33/64 loss: -0.2513054311275482
Batch 34/64 loss: -0.22426682710647583
Batch 35/64 loss: -0.22118759155273438
Batch 36/64 loss: -0.21842718124389648
Batch 37/64 loss: -0.22796058654785156
Batch 38/64 loss: -0.2346189022064209
Batch 39/64 loss: -0.19877588748931885
Batch 40/64 loss: -0.22843492031097412
Batch 41/64 loss: -0.2088267207145691
Batch 42/64 loss: -0.2215057611465454
Batch 43/64 loss: -0.2029011845588684
Batch 44/64 loss: -0.24273043870925903
Batch 45/64 loss: -0.2236483097076416
Batch 46/64 loss: -0.22365760803222656
Batch 47/64 loss: -0.20851868391036987
Batch 48/64 loss: -0.21284490823745728
Batch 49/64 loss: -0.19533944129943848
Batch 50/64 loss: -0.23419952392578125
Batch 51/64 loss: -0.23575854301452637
Batch 52/64 loss: -0.2373260259628296
Batch 53/64 loss: -0.2290581464767456
Batch 54/64 loss: -0.20400285720825195
Batch 55/64 loss: -0.23081648349761963
Batch 56/64 loss: -0.2153438925743103
Batch 57/64 loss: -0.2509821951389313
Batch 58/64 loss: -0.2535536289215088
Batch 59/64 loss: -0.25393491983413696
Batch 60/64 loss: -0.22126486897468567
Batch 61/64 loss: -0.22489559650421143
Batch 62/64 loss: -0.21582245826721191
Batch 63/64 loss: -0.24621668457984924
Batch 64/64 loss: -0.23273834586143494
Epoch 418  Train loss: -0.22727582045629913  Val loss: -0.03646765091165235
Epoch 419
-------------------------------
Batch 1/64 loss: -0.2009068727493286
Batch 2/64 loss: -0.2323226034641266
Batch 3/64 loss: -0.21554937958717346
Batch 4/64 loss: -0.23356688022613525
Batch 5/64 loss: -0.18523907661437988
Batch 6/64 loss: -0.24774959683418274
Batch 7/64 loss: -0.21617788076400757
Batch 8/64 loss: -0.2521322965621948
Batch 9/64 loss: -0.21839728951454163
Batch 10/64 loss: -0.22304758429527283
Batch 11/64 loss: -0.2551613450050354
Batch 12/64 loss: -0.23045361042022705
Batch 13/64 loss: -0.20364707708358765
Batch 14/64 loss: -0.23282918334007263
Batch 15/64 loss: -0.23775142431259155
Batch 16/64 loss: -0.2336215078830719
Batch 17/64 loss: -0.21175166964530945
Batch 18/64 loss: -0.25489407777786255
Batch 19/64 loss: -0.24904906749725342
Batch 20/64 loss: -0.22227954864501953
Batch 21/64 loss: -0.21503573656082153
Batch 22/64 loss: -0.23751300573349
Batch 23/64 loss: -0.21888500452041626
Batch 24/64 loss: -0.26033803820610046
Batch 25/64 loss: -0.2536882758140564
Batch 26/64 loss: -0.21307867765426636
Batch 27/64 loss: -0.24294477701187134
Batch 28/64 loss: -0.22625845670700073
Batch 29/64 loss: -0.23563548922538757
Batch 30/64 loss: -0.22452783584594727
Batch 31/64 loss: -0.2051733136177063
Batch 32/64 loss: -0.2296012043952942
Batch 33/64 loss: -0.23040109872817993
Batch 34/64 loss: -0.2339625358581543
Batch 35/64 loss: -0.23766982555389404
Batch 36/64 loss: -0.23419740796089172
Batch 37/64 loss: -0.234119713306427
Batch 38/64 loss: -0.22619760036468506
Batch 39/64 loss: -0.2338409423828125
Batch 40/64 loss: -0.23582756519317627
Batch 41/64 loss: -0.2526179850101471
Batch 42/64 loss: -0.22428467869758606
Batch 43/64 loss: -0.21121907234191895
Batch 44/64 loss: -0.15259623527526855
Batch 45/64 loss: -0.22958791255950928
Batch 46/64 loss: -0.2121463418006897
Batch 47/64 loss: -0.22250908613204956
Batch 48/64 loss: -0.22284001111984253
Batch 49/64 loss: -0.19746094942092896
Batch 50/64 loss: -0.2278095781803131
Batch 51/64 loss: -0.23124414682388306
Batch 52/64 loss: -0.23356270790100098
Batch 53/64 loss: -0.2558673620223999
Batch 54/64 loss: -0.23804274201393127
Batch 55/64 loss: -0.20568299293518066
Batch 56/64 loss: -0.2314121425151825
Batch 57/64 loss: -0.22223597764968872
Batch 58/64 loss: -0.22604775428771973
Batch 59/64 loss: -0.22680509090423584
Batch 60/64 loss: -0.23443830013275146
Batch 61/64 loss: -0.23433047533035278
Batch 62/64 loss: -0.23516011238098145
Batch 63/64 loss: -0.22638732194900513
Batch 64/64 loss: -0.22872376441955566
Epoch 419  Train loss: -0.2276275494519402  Val loss: -0.03467571755864776
Epoch 420
-------------------------------
Batch 1/64 loss: -0.25204771757125854
Batch 2/64 loss: -0.24893856048583984
Batch 3/64 loss: -0.22603467106819153
Batch 4/64 loss: -0.23635908961296082
Batch 5/64 loss: -0.2519978880882263
Batch 6/64 loss: -0.22438082098960876
Batch 7/64 loss: -0.24034589529037476
Batch 8/64 loss: -0.24374258518218994
Batch 9/64 loss: -0.2115563452243805
Batch 10/64 loss: -0.253851056098938
Batch 11/64 loss: -0.22715625166893005
Batch 12/64 loss: -0.23396405577659607
Batch 13/64 loss: -0.2389003038406372
Batch 14/64 loss: -0.229384183883667
Batch 15/64 loss: -0.2292611002922058
Batch 16/64 loss: -0.2591986656188965
Batch 17/64 loss: -0.2238646149635315
Batch 18/64 loss: -0.23169565200805664
Batch 19/64 loss: -0.2373337745666504
Batch 20/64 loss: -0.23114794492721558
Batch 21/64 loss: -0.18525445461273193
Batch 22/64 loss: -0.22806745767593384
Batch 23/64 loss: -0.2238500416278839
Batch 24/64 loss: -0.22197726368904114
Batch 25/64 loss: -0.21903914213180542
Batch 26/64 loss: -0.22793793678283691
Batch 27/64 loss: -0.23320603370666504
Batch 28/64 loss: -0.23050794005393982
Batch 29/64 loss: -0.2208762764930725
Batch 30/64 loss: -0.22252410650253296
Batch 31/64 loss: -0.2455136775970459
Batch 32/64 loss: -0.2486068308353424
Batch 33/64 loss: -0.2329137921333313
Batch 34/64 loss: -0.22832801938056946
Batch 35/64 loss: -0.2110287845134735
Batch 36/64 loss: -0.2053765058517456
Batch 37/64 loss: -0.23506319522857666
Batch 38/64 loss: -0.23363211750984192
Batch 39/64 loss: -0.19029438495635986
Batch 40/64 loss: -0.23715442419052124
Batch 41/64 loss: -0.2378835678100586
Batch 42/64 loss: -0.2289472222328186
Batch 43/64 loss: -0.24295026063919067
Batch 44/64 loss: -0.18585443496704102
Batch 45/64 loss: -0.2378706932067871
Batch 46/64 loss: -0.22394150495529175
Batch 47/64 loss: -0.20522582530975342
Batch 48/64 loss: -0.2508896291255951
Batch 49/64 loss: -0.25482791662216187
Batch 50/64 loss: -0.22426897287368774
Batch 51/64 loss: -0.2199697494506836
Batch 52/64 loss: -0.23611930012702942
Batch 53/64 loss: -0.23583999276161194
Batch 54/64 loss: -0.23588690161705017
Batch 55/64 loss: -0.2525117099285126
Batch 56/64 loss: -0.2557712197303772
Batch 57/64 loss: -0.23096740245819092
Batch 58/64 loss: -0.22792094945907593
Batch 59/64 loss: -0.24039655923843384
Batch 60/64 loss: -0.25095880031585693
Batch 61/64 loss: -0.21832486987113953
Batch 62/64 loss: -0.22217705845832825
Batch 63/64 loss: -0.1935848593711853
Batch 64/64 loss: -0.23267343640327454
Epoch 420  Train loss: -0.23058679279159097  Val loss: -0.03778921185490192
Epoch 421
-------------------------------
Batch 1/64 loss: -0.23490315675735474
Batch 2/64 loss: -0.254116028547287
Batch 3/64 loss: -0.26392751932144165
Batch 4/64 loss: -0.2400270700454712
Batch 5/64 loss: -0.2311469316482544
Batch 6/64 loss: -0.2467389702796936
Batch 7/64 loss: -0.21656015515327454
Batch 8/64 loss: -0.236165851354599
Batch 9/64 loss: -0.22800618410110474
Batch 10/64 loss: -0.24998027086257935
Batch 11/64 loss: -0.216170072555542
Batch 12/64 loss: -0.25891125202178955
Batch 13/64 loss: -0.22929680347442627
Batch 14/64 loss: -0.18693053722381592
Batch 15/64 loss: -0.24002718925476074
Batch 16/64 loss: -0.23087012767791748
Batch 17/64 loss: -0.23915094137191772
Batch 18/64 loss: -0.22739166021347046
Batch 19/64 loss: -0.23229843378067017
Batch 20/64 loss: -0.2156560719013214
Batch 21/64 loss: -0.2534138560295105
Batch 22/64 loss: -0.2013533115386963
Batch 23/64 loss: -0.23143881559371948
Batch 24/64 loss: -0.24084430932998657
Batch 25/64 loss: -0.230973482131958
Batch 26/64 loss: -0.2532331943511963
Batch 27/64 loss: -0.23878294229507446
Batch 28/64 loss: -0.22123229503631592
Batch 29/64 loss: -0.24314874410629272
Batch 30/64 loss: -0.20846378803253174
Batch 31/64 loss: -0.22462299466133118
Batch 32/64 loss: -0.2262762188911438
Batch 33/64 loss: -0.20931661128997803
Batch 34/64 loss: -0.22112950682640076
Batch 35/64 loss: -0.21590068936347961
Batch 36/64 loss: -0.23916226625442505
Batch 37/64 loss: -0.24044418334960938
Batch 38/64 loss: -0.22662872076034546
Batch 39/64 loss: -0.21834367513656616
Batch 40/64 loss: -0.21991991996765137
Batch 41/64 loss: -0.25388944149017334
Batch 42/64 loss: -0.2207149863243103
Batch 43/64 loss: -0.22817569971084595
Batch 44/64 loss: -0.2294432520866394
Batch 45/64 loss: -0.23895913362503052
Batch 46/64 loss: -0.22956156730651855
Batch 47/64 loss: -0.24741187691688538
Batch 48/64 loss: -0.22302448749542236
Batch 49/64 loss: -0.2180613875389099
Batch 50/64 loss: -0.2192901372909546
Batch 51/64 loss: -0.2348165512084961
Batch 52/64 loss: -0.23861688375473022
Batch 53/64 loss: -0.23345664143562317
Batch 54/64 loss: -0.2394886016845703
Batch 55/64 loss: -0.2294614613056183
Batch 56/64 loss: -0.20858561992645264
Batch 57/64 loss: -0.23763683438301086
Batch 58/64 loss: -0.2085326910018921
Batch 59/64 loss: -0.2512807250022888
Batch 60/64 loss: -0.24455100297927856
Batch 61/64 loss: -0.24153056740760803
Batch 62/64 loss: -0.22640317678451538
Batch 63/64 loss: -0.22966331243515015
Batch 64/64 loss: -0.2036534547805786
Epoch 421  Train loss: -0.23103060208114923  Val loss: -0.0358547836644543
Epoch 422
-------------------------------
Batch 1/64 loss: -0.24767696857452393
Batch 2/64 loss: -0.2265172004699707
Batch 3/64 loss: -0.24522048234939575
Batch 4/64 loss: -0.21698728203773499
Batch 5/64 loss: -0.24357250332832336
Batch 6/64 loss: -0.26291194558143616
Batch 7/64 loss: -0.25189268589019775
Batch 8/64 loss: -0.23285722732543945
Batch 9/64 loss: -0.22474199533462524
Batch 10/64 loss: -0.2532997727394104
Batch 11/64 loss: -0.24555116891860962
Batch 12/64 loss: -0.23882481455802917
Batch 13/64 loss: -0.20775675773620605
Batch 14/64 loss: -0.22745466232299805
Batch 15/64 loss: -0.2361029088497162
Batch 16/64 loss: -0.23837074637413025
Batch 17/64 loss: -0.2242373526096344
Batch 18/64 loss: -0.22692963480949402
Batch 19/64 loss: -0.2594050168991089
Batch 20/64 loss: -0.21052443981170654
Batch 21/64 loss: -0.22844812273979187
Batch 22/64 loss: -0.24766594171524048
Batch 23/64 loss: -0.2154378592967987
Batch 24/64 loss: -0.23244395852088928
Batch 25/64 loss: -0.23369848728179932
Batch 26/64 loss: -0.22297093272209167
Batch 27/64 loss: -0.2410009503364563
Batch 28/64 loss: -0.2432953119277954
Batch 29/64 loss: -0.2021549940109253
Batch 30/64 loss: -0.23577117919921875
Batch 31/64 loss: -0.23713552951812744
Batch 32/64 loss: -0.23220068216323853
Batch 33/64 loss: -0.23699086904525757
Batch 34/64 loss: -0.26534518599510193
Batch 35/64 loss: -0.23694366216659546
Batch 36/64 loss: -0.243094801902771
Batch 37/64 loss: -0.24534106254577637
Batch 38/64 loss: -0.2206484079360962
Batch 39/64 loss: -0.2561784088611603
Batch 40/64 loss: -0.24312764406204224
Batch 41/64 loss: -0.24086269736289978
Batch 42/64 loss: -0.25154006481170654
Batch 43/64 loss: -0.2317999005317688
Batch 44/64 loss: -0.23668918013572693
Batch 45/64 loss: -0.2072840929031372
Batch 46/64 loss: -0.24447175860404968
Batch 47/64 loss: -0.20863854885101318
Batch 48/64 loss: -0.21634402871131897
Batch 49/64 loss: -0.21339067816734314
Batch 50/64 loss: -0.20366454124450684
Batch 51/64 loss: -0.2328527569770813
Batch 52/64 loss: -0.22601929306983948
Batch 53/64 loss: -0.21622273325920105
Batch 54/64 loss: -0.2514054775238037
Batch 55/64 loss: -0.23452913761138916
Batch 56/64 loss: -0.23517918586730957
Batch 57/64 loss: -0.24348992109298706
Batch 58/64 loss: -0.20201468467712402
Batch 59/64 loss: -0.22082793712615967
Batch 60/64 loss: -0.22598731517791748
Batch 61/64 loss: -0.2244066596031189
Batch 62/64 loss: -0.23063817620277405
Batch 63/64 loss: -0.25529080629348755
Batch 64/64 loss: -0.20491623878479004
Epoch 422  Train loss: -0.23290927550371957  Val loss: -0.03225293950116921
Epoch 423
-------------------------------
Batch 1/64 loss: -0.25065821409225464
Batch 2/64 loss: -0.20744383335113525
Batch 3/64 loss: -0.2407706379890442
Batch 4/64 loss: -0.22690734267234802
Batch 5/64 loss: -0.23001819849014282
Batch 6/64 loss: -0.24203574657440186
Batch 7/64 loss: -0.2630035877227783
Batch 8/64 loss: -0.1936919093132019
Batch 9/64 loss: -0.2556811571121216
Batch 10/64 loss: -0.24636149406433105
Batch 11/64 loss: -0.22990703582763672
Batch 12/64 loss: -0.2278885841369629
Batch 13/64 loss: -0.21028071641921997
Batch 14/64 loss: -0.23106634616851807
Batch 15/64 loss: -0.2516860067844391
Batch 16/64 loss: -0.24529826641082764
Batch 17/64 loss: -0.24197077751159668
Batch 18/64 loss: -0.22458148002624512
Batch 19/64 loss: -0.22404813766479492
Batch 20/64 loss: -0.22695213556289673
Batch 21/64 loss: -0.24937587976455688
Batch 22/64 loss: -0.24025875329971313
Batch 23/64 loss: -0.2634209990501404
Batch 24/64 loss: -0.22696813941001892
Batch 25/64 loss: -0.23628628253936768
Batch 26/64 loss: -0.23874664306640625
Batch 27/64 loss: -0.22344639897346497
Batch 28/64 loss: -0.22275888919830322
Batch 29/64 loss: -0.24706876277923584
Batch 30/64 loss: -0.22535711526870728
Batch 31/64 loss: -0.19655221700668335
Batch 32/64 loss: -0.2096920609474182
Batch 33/64 loss: -0.24006152153015137
Batch 34/64 loss: -0.21233877539634705
Batch 35/64 loss: -0.2444811463356018
Batch 36/64 loss: -0.24359804391860962
Batch 37/64 loss: -0.23767125606536865
Batch 38/64 loss: -0.22683703899383545
Batch 39/64 loss: -0.2490571141242981
Batch 40/64 loss: -0.22472697496414185
Batch 41/64 loss: -0.24021804332733154
Batch 42/64 loss: -0.2280421257019043
Batch 43/64 loss: -0.23116272687911987
Batch 44/64 loss: -0.215182363986969
Batch 45/64 loss: -0.23984485864639282
Batch 46/64 loss: -0.22467440366744995
Batch 47/64 loss: -0.20934772491455078
Batch 48/64 loss: -0.2303006649017334
Batch 49/64 loss: -0.21352112293243408
Batch 50/64 loss: -0.24097943305969238
Batch 51/64 loss: -0.24323320388793945
Batch 52/64 loss: -0.2400253415107727
Batch 53/64 loss: -0.20051831007003784
Batch 54/64 loss: -0.24254971742630005
Batch 55/64 loss: -0.23284929990768433
Batch 56/64 loss: -0.2253897786140442
Batch 57/64 loss: -0.2221185863018036
Batch 58/64 loss: -0.2552781403064728
Batch 59/64 loss: -0.24065399169921875
Batch 60/64 loss: -0.2076238989830017
Batch 61/64 loss: -0.25383734703063965
Batch 62/64 loss: -0.25323787331581116
Batch 63/64 loss: -0.22928965091705322
Batch 64/64 loss: -0.2609856426715851
Epoch 423  Train loss: -0.2328560542826559  Val loss: -0.03470918853668003
Epoch 424
-------------------------------
Batch 1/64 loss: -0.26051628589630127
Batch 2/64 loss: -0.26558926701545715
Batch 3/64 loss: -0.23640277981758118
Batch 4/64 loss: -0.25340384244918823
Batch 5/64 loss: -0.21935191750526428
Batch 6/64 loss: -0.21507906913757324
Batch 7/64 loss: -0.22882550954818726
Batch 8/64 loss: -0.21591627597808838
Batch 9/64 loss: -0.25841498374938965
Batch 10/64 loss: -0.23177891969680786
Batch 11/64 loss: -0.2324509620666504
Batch 12/64 loss: -0.23665136098861694
Batch 13/64 loss: -0.24640381336212158
Batch 14/64 loss: -0.240319162607193
Batch 15/64 loss: -0.2186647653579712
Batch 16/64 loss: -0.2508868873119354
Batch 17/64 loss: -0.2277175784111023
Batch 18/64 loss: -0.17644423246383667
Batch 19/64 loss: -0.2407921552658081
Batch 20/64 loss: -0.23692587018013
Batch 21/64 loss: -0.24007365107536316
Batch 22/64 loss: -0.21945422887802124
Batch 23/64 loss: -0.257831871509552
Batch 24/64 loss: -0.24306505918502808
Batch 25/64 loss: -0.24873635172843933
Batch 26/64 loss: -0.23354649543762207
Batch 27/64 loss: -0.2043997049331665
Batch 28/64 loss: -0.2248690128326416
Batch 29/64 loss: -0.2146013081073761
Batch 30/64 loss: -0.24032020568847656
Batch 31/64 loss: -0.23129254579544067
Batch 32/64 loss: -0.22624808549880981
Batch 33/64 loss: -0.2346448302268982
Batch 34/64 loss: -0.22530627250671387
Batch 35/64 loss: -0.22420910000801086
Batch 36/64 loss: -0.21227961778640747
Batch 37/64 loss: -0.234796404838562
Batch 38/64 loss: -0.24648070335388184
Batch 39/64 loss: -0.24113547801971436
Batch 40/64 loss: -0.19341003894805908
Batch 41/64 loss: -0.24153834581375122
Batch 42/64 loss: -0.251221239566803
Batch 43/64 loss: -0.23531630635261536
Batch 44/64 loss: -0.2321326732635498
Batch 45/64 loss: -0.24044299125671387
Batch 46/64 loss: -0.2337714433670044
Batch 47/64 loss: -0.2293580174446106
Batch 48/64 loss: -0.21522116661071777
Batch 49/64 loss: -0.23332637548446655
Batch 50/64 loss: -0.22920280694961548
Batch 51/64 loss: -0.23101544380187988
Batch 52/64 loss: -0.24032583832740784
Batch 53/64 loss: -0.2265075147151947
Batch 54/64 loss: -0.22160691022872925
Batch 55/64 loss: -0.22091755270957947
Batch 56/64 loss: -0.24324184656143188
Batch 57/64 loss: -0.23860913515090942
Batch 58/64 loss: -0.23061031103134155
Batch 59/64 loss: -0.21799525618553162
Batch 60/64 loss: -0.24965885281562805
Batch 61/64 loss: -0.23517495393753052
Batch 62/64 loss: -0.23906546831130981
Batch 63/64 loss: -0.24144020676612854
Batch 64/64 loss: -0.22612783312797546
Epoch 424  Train loss: -0.23272993155554228  Val loss: -0.0357062187801112
Epoch 425
-------------------------------
Batch 1/64 loss: -0.22362112998962402
Batch 2/64 loss: -0.2387925386428833
Batch 3/64 loss: -0.2520565986633301
Batch 4/64 loss: -0.24234271049499512
Batch 5/64 loss: -0.2442653477191925
Batch 6/64 loss: -0.25401338934898376
Batch 7/64 loss: -0.24725642800331116
Batch 8/64 loss: -0.236142098903656
Batch 9/64 loss: -0.24169260263442993
Batch 10/64 loss: -0.2421918511390686
Batch 11/64 loss: -0.2473698854446411
Batch 12/64 loss: -0.2539704144001007
Batch 13/64 loss: -0.24502065777778625
Batch 14/64 loss: -0.24655365943908691
Batch 15/64 loss: -0.2219197154045105
Batch 16/64 loss: -0.2294970154762268
Batch 17/64 loss: -0.23219263553619385
Batch 18/64 loss: -0.24365520477294922
Batch 19/64 loss: -0.2224544882774353
Batch 20/64 loss: -0.2363385260105133
Batch 21/64 loss: -0.2508341073989868
Batch 22/64 loss: -0.22584635019302368
Batch 23/64 loss: -0.161271870136261
Batch 24/64 loss: -0.24444782733917236
Batch 25/64 loss: -0.22245246171951294
Batch 26/64 loss: -0.23570215702056885
Batch 27/64 loss: -0.25898581743240356
Batch 28/64 loss: -0.24007827043533325
Batch 29/64 loss: -0.24150598049163818
Batch 30/64 loss: -0.22938865423202515
Batch 31/64 loss: -0.26164644956588745
Batch 32/64 loss: -0.21620601415634155
Batch 33/64 loss: -0.19632315635681152
Batch 34/64 loss: -0.23488742113113403
Batch 35/64 loss: -0.2193029522895813
Batch 36/64 loss: -0.23013761639595032
Batch 37/64 loss: -0.22261875867843628
Batch 38/64 loss: -0.23201918601989746
Batch 39/64 loss: -0.23642867803573608
Batch 40/64 loss: -0.21943047642707825
Batch 41/64 loss: -0.21678775548934937
Batch 42/64 loss: -0.22999227046966553
Batch 43/64 loss: -0.2338392436504364
Batch 44/64 loss: -0.23819443583488464
Batch 45/64 loss: -0.2241252064704895
Batch 46/64 loss: -0.23162370920181274
Batch 47/64 loss: -0.23728764057159424
Batch 48/64 loss: -0.21968421339988708
Batch 49/64 loss: -0.2242189645767212
Batch 50/64 loss: -0.23349297046661377
Batch 51/64 loss: -0.23288142681121826
Batch 52/64 loss: -0.21032214164733887
Batch 53/64 loss: -0.22905349731445312
Batch 54/64 loss: -0.22287461161613464
Batch 55/64 loss: -0.23188304901123047
Batch 56/64 loss: -0.20725560188293457
Batch 57/64 loss: -0.22279706597328186
Batch 58/64 loss: -0.19453132152557373
Batch 59/64 loss: -0.22518163919448853
Batch 60/64 loss: -0.23878800868988037
Batch 61/64 loss: -0.2259942591190338
Batch 62/64 loss: -0.2328414022922516
Batch 63/64 loss: -0.2095252275466919
Batch 64/64 loss: -0.1882811188697815
Epoch 425  Train loss: -0.23051368792851765  Val loss: -0.03489152406089494
Epoch 426
-------------------------------
Batch 1/64 loss: -0.22991257905960083
Batch 2/64 loss: -0.17411470413208008
Batch 3/64 loss: -0.24145525693893433
Batch 4/64 loss: -0.2178848683834076
Batch 5/64 loss: -0.2308064103126526
Batch 6/64 loss: -0.22266721725463867
Batch 7/64 loss: -0.22943395376205444
Batch 8/64 loss: -0.24226176738739014
Batch 9/64 loss: -0.22471529245376587
Batch 10/64 loss: -0.20420533418655396
Batch 11/64 loss: -0.2380034327507019
Batch 12/64 loss: -0.247714102268219
Batch 13/64 loss: -0.2301560640335083
Batch 14/64 loss: -0.2271175980567932
Batch 15/64 loss: -0.26122668385505676
Batch 16/64 loss: -0.24354618787765503
Batch 17/64 loss: -0.22955575585365295
Batch 18/64 loss: -0.24208301305770874
Batch 19/64 loss: -0.23118382692337036
Batch 20/64 loss: -0.23627948760986328
Batch 21/64 loss: -0.23151850700378418
Batch 22/64 loss: -0.25458407402038574
Batch 23/64 loss: -0.25885000824928284
Batch 24/64 loss: -0.23088622093200684
Batch 25/64 loss: -0.21658387780189514
Batch 26/64 loss: -0.23416730761528015
Batch 27/64 loss: -0.2724832594394684
Batch 28/64 loss: -0.2352910041809082
Batch 29/64 loss: -0.20267462730407715
Batch 30/64 loss: -0.2468811273574829
Batch 31/64 loss: -0.24392777681350708
Batch 32/64 loss: -0.22459131479263306
Batch 33/64 loss: -0.2423110008239746
Batch 34/64 loss: -0.22211751341819763
Batch 35/64 loss: -0.16073405742645264
Batch 36/64 loss: -0.23433798551559448
Batch 37/64 loss: -0.22147485613822937
Batch 38/64 loss: -0.18022805452346802
Batch 39/64 loss: -0.20030075311660767
Batch 40/64 loss: -0.2302054762840271
Batch 41/64 loss: -0.23272740840911865
Batch 42/64 loss: -0.24071094393730164
Batch 43/64 loss: -0.2442462146282196
Batch 44/64 loss: -0.21216538548469543
Batch 45/64 loss: -0.25954902172088623
Batch 46/64 loss: -0.24177026748657227
Batch 47/64 loss: -0.2327684760093689
Batch 48/64 loss: -0.24811357259750366
Batch 49/64 loss: -0.2233467698097229
Batch 50/64 loss: -0.2272915244102478
Batch 51/64 loss: -0.2414761483669281
Batch 52/64 loss: -0.23905980587005615
Batch 53/64 loss: -0.2375989556312561
Batch 54/64 loss: -0.22117558121681213
Batch 55/64 loss: -0.25026294589042664
Batch 56/64 loss: -0.23646211624145508
Batch 57/64 loss: -0.25898420810699463
Batch 58/64 loss: -0.22978979349136353
Batch 59/64 loss: -0.24598848819732666
Batch 60/64 loss: -0.20963793992996216
Batch 61/64 loss: -0.21984601020812988
Batch 62/64 loss: -0.2099173665046692
Batch 63/64 loss: -0.23036110401153564
Batch 64/64 loss: -0.22562256455421448
Epoch 426  Train loss: -0.2307598323214288  Val loss: -0.034418437489119595
Epoch 427
-------------------------------
Batch 1/64 loss: -0.24559122323989868
Batch 2/64 loss: -0.251670241355896
Batch 3/64 loss: -0.2382735311985016
Batch 4/64 loss: -0.24460402131080627
Batch 5/64 loss: -0.24505120515823364
Batch 6/64 loss: -0.23534029722213745
Batch 7/64 loss: -0.2454894781112671
Batch 8/64 loss: -0.23549306392669678
Batch 9/64 loss: -0.22139272093772888
Batch 10/64 loss: -0.26086315512657166
Batch 11/64 loss: -0.2520734369754791
Batch 12/64 loss: -0.22292524576187134
Batch 13/64 loss: -0.23926693201065063
Batch 14/64 loss: -0.2446596920490265
Batch 15/64 loss: -0.23114651441574097
Batch 16/64 loss: -0.2374275028705597
Batch 17/64 loss: -0.25629645586013794
Batch 18/64 loss: -0.24255076050758362
Batch 19/64 loss: -0.237637460231781
Batch 20/64 loss: -0.20974713563919067
Batch 21/64 loss: -0.2537645101547241
Batch 22/64 loss: -0.23516494035720825
Batch 23/64 loss: -0.22915160655975342
Batch 24/64 loss: -0.23033049702644348
Batch 25/64 loss: -0.2484292984008789
Batch 26/64 loss: -0.26360630989074707
Batch 27/64 loss: -0.2094401717185974
Batch 28/64 loss: -0.23165267705917358
Batch 29/64 loss: -0.24650505185127258
Batch 30/64 loss: -0.24584820866584778
Batch 31/64 loss: -0.2686808705329895
Batch 32/64 loss: -0.2325107455253601
Batch 33/64 loss: -0.23468273878097534
Batch 34/64 loss: -0.2492588758468628
Batch 35/64 loss: -0.2326372265815735
Batch 36/64 loss: -0.22023546695709229
Batch 37/64 loss: -0.21585434675216675
Batch 38/64 loss: -0.20396661758422852
Batch 39/64 loss: -0.23944830894470215
Batch 40/64 loss: -0.25068771839141846
Batch 41/64 loss: -0.21324360370635986
Batch 42/64 loss: -0.23865261673927307
Batch 43/64 loss: -0.2663341462612152
Batch 44/64 loss: -0.23313891887664795
Batch 45/64 loss: -0.2503959536552429
Batch 46/64 loss: -0.23348546028137207
Batch 47/64 loss: -0.23029017448425293
Batch 48/64 loss: -0.2139883041381836
Batch 49/64 loss: -0.23011165857315063
Batch 50/64 loss: -0.21187859773635864
Batch 51/64 loss: -0.21585607528686523
Batch 52/64 loss: -0.22651952505111694
Batch 53/64 loss: -0.2092965543270111
Batch 54/64 loss: -0.23813140392303467
Batch 55/64 loss: -0.21696817874908447
Batch 56/64 loss: -0.21902361512184143
Batch 57/64 loss: -0.20753991603851318
Batch 58/64 loss: -0.24209022521972656
Batch 59/64 loss: -0.2302316427230835
Batch 60/64 loss: -0.23450681567192078
Batch 61/64 loss: -0.23198968172073364
Batch 62/64 loss: -0.24217286705970764
Batch 63/64 loss: -0.23255231976509094
Batch 64/64 loss: -0.2046973705291748
Epoch 427  Train loss: -0.23468670658036775  Val loss: -0.031010184910698856
Epoch 428
-------------------------------
Batch 1/64 loss: -0.2136533260345459
Batch 2/64 loss: -0.23827296495437622
Batch 3/64 loss: -0.2332969307899475
Batch 4/64 loss: -0.22942832112312317
Batch 5/64 loss: -0.2417464554309845
Batch 6/64 loss: -0.24736696481704712
Batch 7/64 loss: -0.24056285619735718
Batch 8/64 loss: -0.21462905406951904
Batch 9/64 loss: -0.23603612184524536
Batch 10/64 loss: -0.201593279838562
Batch 11/64 loss: -0.23018071055412292
Batch 12/64 loss: -0.20143240690231323
Batch 13/64 loss: -0.22709858417510986
Batch 14/64 loss: -0.23631209135055542
Batch 15/64 loss: -0.25789692997932434
Batch 16/64 loss: -0.2393152117729187
Batch 17/64 loss: -0.25118017196655273
Batch 18/64 loss: -0.2252158224582672
Batch 19/64 loss: -0.26563477516174316
Batch 20/64 loss: -0.2329826056957245
Batch 21/64 loss: -0.22204792499542236
Batch 22/64 loss: -0.23785758018493652
Batch 23/64 loss: -0.22687798738479614
Batch 24/64 loss: -0.23871204257011414
Batch 25/64 loss: -0.21178698539733887
Batch 26/64 loss: -0.247076153755188
Batch 27/64 loss: -0.21996718645095825
Batch 28/64 loss: -0.24252581596374512
Batch 29/64 loss: -0.24632015824317932
Batch 30/64 loss: -0.2580128312110901
Batch 31/64 loss: -0.227626234292984
Batch 32/64 loss: -0.239711195230484
Batch 33/64 loss: -0.2197219729423523
Batch 34/64 loss: -0.19035565853118896
Batch 35/64 loss: -0.24562114477157593
Batch 36/64 loss: -0.22582915425300598
Batch 37/64 loss: -0.23670578002929688
Batch 38/64 loss: -0.2543882131576538
Batch 39/64 loss: -0.2320803701877594
Batch 40/64 loss: -0.2415100634098053
Batch 41/64 loss: -0.20584499835968018
Batch 42/64 loss: -0.24391692876815796
Batch 43/64 loss: -0.2316514253616333
Batch 44/64 loss: -0.21930834650993347
Batch 45/64 loss: -0.25208398699760437
Batch 46/64 loss: -0.24367356300354004
Batch 47/64 loss: -0.24107709527015686
Batch 48/64 loss: -0.24777325987815857
Batch 49/64 loss: -0.21235859394073486
Batch 50/64 loss: -0.2408396601676941
Batch 51/64 loss: -0.22968286275863647
Batch 52/64 loss: -0.23247680068016052
Batch 53/64 loss: -0.21015480160713196
Batch 54/64 loss: -0.2515612840652466
Batch 55/64 loss: -0.23076844215393066
Batch 56/64 loss: -0.20727640390396118
Batch 57/64 loss: -0.2501078248023987
Batch 58/64 loss: -0.2514287233352661
Batch 59/64 loss: -0.24793845415115356
Batch 60/64 loss: -0.2605459988117218
Batch 61/64 loss: -0.2444726824760437
Batch 62/64 loss: -0.25040531158447266
Batch 63/64 loss: -0.2352236807346344
Batch 64/64 loss: -0.22879567742347717
Epoch 428  Train loss: -0.23436451647795883  Val loss: -0.03402830849808106
Epoch 429
-------------------------------
Batch 1/64 loss: -0.23528623580932617
Batch 2/64 loss: -0.2078290581703186
Batch 3/64 loss: -0.23058462142944336
Batch 4/64 loss: -0.24306952953338623
Batch 5/64 loss: -0.24421197175979614
Batch 6/64 loss: -0.23379993438720703
Batch 7/64 loss: -0.21196109056472778
Batch 8/64 loss: -0.2388685643672943
Batch 9/64 loss: -0.23474133014678955
Batch 10/64 loss: -0.23167327046394348
Batch 11/64 loss: -0.24045667052268982
Batch 12/64 loss: -0.22533082962036133
Batch 13/64 loss: -0.1813010573387146
Batch 14/64 loss: -0.20889317989349365
Batch 15/64 loss: -0.24263453483581543
Batch 16/64 loss: -0.24880105257034302
Batch 17/64 loss: -0.23211067914962769
Batch 18/64 loss: -0.23842370510101318
Batch 19/64 loss: -0.22342783212661743
Batch 20/64 loss: -0.2383628487586975
Batch 21/64 loss: -0.23301035165786743
Batch 22/64 loss: -0.25335240364074707
Batch 23/64 loss: -0.24442216753959656
Batch 24/64 loss: -0.21059274673461914
Batch 25/64 loss: -0.23425275087356567
Batch 26/64 loss: -0.2503718435764313
Batch 27/64 loss: -0.24597999453544617
Batch 28/64 loss: -0.23494896292686462
Batch 29/64 loss: -0.23197460174560547
Batch 30/64 loss: -0.25885432958602905
Batch 31/64 loss: -0.21005868911743164
Batch 32/64 loss: -0.22978520393371582
Batch 33/64 loss: -0.24579691886901855
Batch 34/64 loss: -0.22900688648223877
Batch 35/64 loss: -0.24078354239463806
Batch 36/64 loss: -0.20708376169204712
Batch 37/64 loss: -0.20624130964279175
Batch 38/64 loss: -0.24291056394577026
Batch 39/64 loss: -0.19932568073272705
Batch 40/64 loss: -0.23652559518814087
Batch 41/64 loss: -0.2427855134010315
Batch 42/64 loss: -0.23689591884613037
Batch 43/64 loss: -0.23692864179611206
Batch 44/64 loss: -0.24557393789291382
Batch 45/64 loss: -0.2004469633102417
Batch 46/64 loss: -0.20809346437454224
Batch 47/64 loss: -0.22762590646743774
Batch 48/64 loss: -0.2460973858833313
Batch 49/64 loss: -0.2370958924293518
Batch 50/64 loss: -0.23927825689315796
Batch 51/64 loss: -0.21982896327972412
Batch 52/64 loss: -0.19600564241409302
Batch 53/64 loss: -0.24429836869239807
Batch 54/64 loss: -0.24050003290176392
Batch 55/64 loss: -0.2564736604690552
Batch 56/64 loss: -0.24342495203018188
Batch 57/64 loss: -0.25582605600357056
Batch 58/64 loss: -0.26118969917297363
Batch 59/64 loss: -0.22569793462753296
Batch 60/64 loss: -0.2249423861503601
Batch 61/64 loss: -0.25551456212997437
Batch 62/64 loss: -0.24736127257347107
Batch 63/64 loss: -0.2450183629989624
Batch 64/64 loss: -0.24964088201522827
Epoch 429  Train loss: -0.23311656059003344  Val loss: -0.0344255814437604
Epoch 430
-------------------------------
Batch 1/64 loss: -0.2341720461845398
Batch 2/64 loss: -0.23588207364082336
Batch 3/64 loss: -0.24080196022987366
Batch 4/64 loss: -0.23539358377456665
Batch 5/64 loss: -0.21124517917633057
Batch 6/64 loss: -0.2533087432384491
Batch 7/64 loss: -0.2531105875968933
Batch 8/64 loss: -0.25913310050964355
Batch 9/64 loss: -0.24252191185951233
Batch 10/64 loss: -0.2139691710472107
Batch 11/64 loss: -0.2373085916042328
Batch 12/64 loss: -0.23746037483215332
Batch 13/64 loss: -0.24472206830978394
Batch 14/64 loss: -0.2004193663597107
Batch 15/64 loss: -0.20946508646011353
Batch 16/64 loss: -0.22251060605049133
Batch 17/64 loss: -0.25710973143577576
Batch 18/64 loss: -0.24070417881011963
Batch 19/64 loss: -0.24211567640304565
Batch 20/64 loss: -0.20198172330856323
Batch 21/64 loss: -0.2179519236087799
Batch 22/64 loss: -0.21203812956809998
Batch 23/64 loss: -0.23243576288223267
Batch 24/64 loss: -0.22474616765975952
Batch 25/64 loss: -0.25153112411499023
Batch 26/64 loss: -0.2387365698814392
Batch 27/64 loss: -0.21910536289215088
Batch 28/64 loss: -0.2504257559776306
Batch 29/64 loss: -0.2564629316329956
Batch 30/64 loss: -0.21674326062202454
Batch 31/64 loss: -0.23160004615783691
Batch 32/64 loss: -0.2506277859210968
Batch 33/64 loss: -0.24478569626808167
Batch 34/64 loss: -0.19857656955718994
Batch 35/64 loss: -0.2426556944847107
Batch 36/64 loss: -0.24240022897720337
Batch 37/64 loss: -0.25906652212142944
Batch 38/64 loss: -0.22883298993110657
Batch 39/64 loss: -0.24844586849212646
Batch 40/64 loss: -0.24107182025909424
Batch 41/64 loss: -0.22465142607688904
Batch 42/64 loss: -0.2457098662853241
Batch 43/64 loss: -0.24630284309387207
Batch 44/64 loss: -0.25828710198402405
Batch 45/64 loss: -0.2558058500289917
Batch 46/64 loss: -0.2402215600013733
Batch 47/64 loss: -0.2447318434715271
Batch 48/64 loss: -0.2420114278793335
Batch 49/64 loss: -0.24825367331504822
Batch 50/64 loss: -0.2225421667098999
Batch 51/64 loss: -0.256194531917572
Batch 52/64 loss: -0.21834850311279297
Batch 53/64 loss: -0.20325791835784912
Batch 54/64 loss: -0.22180557250976562
Batch 55/64 loss: -0.2286820411682129
Batch 56/64 loss: -0.23896124958992004
Batch 57/64 loss: -0.23383575677871704
Batch 58/64 loss: -0.2511764168739319
Batch 59/64 loss: -0.24478334188461304
Batch 60/64 loss: -0.22908970713615417
Batch 61/64 loss: -0.23496907949447632
Batch 62/64 loss: -0.2460133135318756
Batch 63/64 loss: -0.24926280975341797
Batch 64/64 loss: -0.2523719072341919
Epoch 430  Train loss: -0.2361686729917339  Val loss: -0.034121228247573694
Epoch 431
-------------------------------
Batch 1/64 loss: -0.2378314733505249
Batch 2/64 loss: -0.24300426244735718
Batch 3/64 loss: -0.1988523006439209
Batch 4/64 loss: -0.20616447925567627
Batch 5/64 loss: -0.22447505593299866
Batch 6/64 loss: -0.2566964626312256
Batch 7/64 loss: -0.2560897171497345
Batch 8/64 loss: -0.2529531419277191
Batch 9/64 loss: -0.23613333702087402
Batch 10/64 loss: -0.22395089268684387
Batch 11/64 loss: -0.24146103858947754
Batch 12/64 loss: -0.24827873706817627
Batch 13/64 loss: -0.2563345432281494
Batch 14/64 loss: -0.23920369148254395
Batch 15/64 loss: -0.21328073740005493
Batch 16/64 loss: -0.19582360982894897
Batch 17/64 loss: -0.2495667040348053
Batch 18/64 loss: -0.22691553831100464
Batch 19/64 loss: -0.253989040851593
Batch 20/64 loss: -0.25112679600715637
Batch 21/64 loss: -0.21617579460144043
Batch 22/64 loss: -0.23540806770324707
Batch 23/64 loss: -0.2480538785457611
Batch 24/64 loss: -0.2295495569705963
Batch 25/64 loss: -0.2339976727962494
Batch 26/64 loss: -0.2327195405960083
Batch 27/64 loss: -0.26105302572250366
Batch 28/64 loss: -0.22952193021774292
Batch 29/64 loss: -0.21365275979042053
Batch 30/64 loss: -0.25810807943344116
Batch 31/64 loss: -0.24589991569519043
Batch 32/64 loss: -0.2197212278842926
Batch 33/64 loss: -0.2536226809024811
Batch 34/64 loss: -0.25336822867393494
Batch 35/64 loss: -0.22343125939369202
Batch 36/64 loss: -0.24905240535736084
Batch 37/64 loss: -0.24142241477966309
Batch 38/64 loss: -0.22065263986587524
Batch 39/64 loss: -0.25161850452423096
Batch 40/64 loss: -0.21893596649169922
Batch 41/64 loss: -0.26024627685546875
Batch 42/64 loss: -0.25637781620025635
Batch 43/64 loss: -0.23788827657699585
Batch 44/64 loss: -0.23082438111305237
Batch 45/64 loss: -0.21663719415664673
Batch 46/64 loss: -0.2552468776702881
Batch 47/64 loss: -0.22105401754379272
Batch 48/64 loss: -0.22984018921852112
Batch 49/64 loss: -0.22191080451011658
Batch 50/64 loss: -0.1987696886062622
Batch 51/64 loss: -0.24826174974441528
Batch 52/64 loss: -0.24371081590652466
Batch 53/64 loss: -0.25440651178359985
Batch 54/64 loss: -0.2479967474937439
Batch 55/64 loss: -0.21189367771148682
Batch 56/64 loss: -0.235419362783432
Batch 57/64 loss: -0.21892312169075012
Batch 58/64 loss: -0.2389257848262787
Batch 59/64 loss: -0.23549050092697144
Batch 60/64 loss: -0.2314145565032959
Batch 61/64 loss: -0.24054980278015137
Batch 62/64 loss: -0.24076679348945618
Batch 63/64 loss: -0.24404549598693848
Batch 64/64 loss: -0.2245885133743286
Epoch 431  Train loss: -0.2358766892377068  Val loss: -0.03521629010688808
Epoch 432
-------------------------------
Batch 1/64 loss: -0.22114896774291992
Batch 2/64 loss: -0.23196011781692505
Batch 3/64 loss: -0.25693273544311523
Batch 4/64 loss: -0.24822360277175903
Batch 5/64 loss: -0.2596052587032318
Batch 6/64 loss: -0.2590198516845703
Batch 7/64 loss: -0.19012677669525146
Batch 8/64 loss: -0.24308109283447266
Batch 9/64 loss: -0.2629527747631073
Batch 10/64 loss: -0.23405838012695312
Batch 11/64 loss: -0.21960866451263428
Batch 12/64 loss: -0.24162280559539795
Batch 13/64 loss: -0.23741203546524048
Batch 14/64 loss: -0.20690637826919556
Batch 15/64 loss: -0.2535138428211212
Batch 16/64 loss: -0.2420821189880371
Batch 17/64 loss: -0.22254329919815063
Batch 18/64 loss: -0.23524975776672363
Batch 19/64 loss: -0.14977556467056274
Batch 20/64 loss: -0.2252579927444458
Batch 21/64 loss: -0.21416610479354858
Batch 22/64 loss: -0.21458318829536438
Batch 23/64 loss: -0.21995127201080322
Batch 24/64 loss: -0.24276673793792725
Batch 25/64 loss: -0.22296643257141113
Batch 26/64 loss: -0.21296125650405884
Batch 27/64 loss: -0.23769116401672363
Batch 28/64 loss: -0.24391505122184753
Batch 29/64 loss: -0.24548178911209106
Batch 30/64 loss: -0.24463123083114624
Batch 31/64 loss: -0.22263306379318237
Batch 32/64 loss: -0.2335735559463501
Batch 33/64 loss: -0.24976730346679688
Batch 34/64 loss: -0.2539050579071045
Batch 35/64 loss: -0.24597275257110596
Batch 36/64 loss: -0.2365710437297821
Batch 37/64 loss: -0.1895592212677002
Batch 38/64 loss: -0.22417819499969482
Batch 39/64 loss: -0.2566125988960266
Batch 40/64 loss: -0.25163501501083374
Batch 41/64 loss: -0.22393596172332764
Batch 42/64 loss: -0.22881543636322021
Batch 43/64 loss: -0.23092269897460938
Batch 44/64 loss: -0.2339862585067749
Batch 45/64 loss: -0.2220483124256134
Batch 46/64 loss: -0.23189318180084229
Batch 47/64 loss: -0.24971458315849304
Batch 48/64 loss: -0.24266791343688965
Batch 49/64 loss: -0.2438012957572937
Batch 50/64 loss: -0.19048327207565308
Batch 51/64 loss: -0.22948890924453735
Batch 52/64 loss: -0.22577041387557983
Batch 53/64 loss: -0.2547927498817444
Batch 54/64 loss: -0.24775034189224243
Batch 55/64 loss: -0.23517060279846191
Batch 56/64 loss: -0.2567247748374939
Batch 57/64 loss: -0.23704952001571655
Batch 58/64 loss: -0.2335067093372345
Batch 59/64 loss: -0.25292080640792847
Batch 60/64 loss: -0.2137097716331482
Batch 61/64 loss: -0.21361035108566284
Batch 62/64 loss: -0.24971503019332886
Batch 63/64 loss: -0.23760360479354858
Batch 64/64 loss: -0.23651397228240967
Epoch 432  Train loss: -0.23322465934005437  Val loss: -0.03493116422207495
Epoch 433
-------------------------------
Batch 1/64 loss: -0.21665629744529724
Batch 2/64 loss: -0.23868125677108765
Batch 3/64 loss: -0.24334314465522766
Batch 4/64 loss: -0.23161473870277405
Batch 5/64 loss: -0.265705943107605
Batch 6/64 loss: -0.2501288056373596
Batch 7/64 loss: -0.26704883575439453
Batch 8/64 loss: -0.26194140315055847
Batch 9/64 loss: -0.2569119334220886
Batch 10/64 loss: -0.23224449157714844
Batch 11/64 loss: -0.2484791874885559
Batch 12/64 loss: -0.2543178200721741
Batch 13/64 loss: -0.2669540047645569
Batch 14/64 loss: -0.2365962266921997
Batch 15/64 loss: -0.25194400548934937
Batch 16/64 loss: -0.23193293809890747
Batch 17/64 loss: -0.25908389687538147
Batch 18/64 loss: -0.24747750163078308
Batch 19/64 loss: -0.2557222843170166
Batch 20/64 loss: -0.24655020236968994
Batch 21/64 loss: -0.2095136046409607
Batch 22/64 loss: -0.19789886474609375
Batch 23/64 loss: -0.21946090459823608
Batch 24/64 loss: -0.2226850688457489
Batch 25/64 loss: -0.21651384234428406
Batch 26/64 loss: -0.226506769657135
Batch 27/64 loss: -0.23080208897590637
Batch 28/64 loss: -0.24899303913116455
Batch 29/64 loss: -0.24861353635787964
Batch 30/64 loss: -0.23522347211837769
Batch 31/64 loss: -0.23496735095977783
Batch 32/64 loss: -0.2106035351753235
Batch 33/64 loss: -0.2195562720298767
Batch 34/64 loss: -0.191095769405365
Batch 35/64 loss: -0.23591530323028564
Batch 36/64 loss: -0.20791840553283691
Batch 37/64 loss: -0.2144741415977478
Batch 38/64 loss: -0.24282434582710266
Batch 39/64 loss: -0.21688920259475708
Batch 40/64 loss: -0.25303715467453003
Batch 41/64 loss: -0.23122763633728027
Batch 42/64 loss: -0.21960178017616272
Batch 43/64 loss: -0.2418646216392517
Batch 44/64 loss: -0.23722094297409058
Batch 45/64 loss: -0.2537737488746643
Batch 46/64 loss: -0.2535150945186615
Batch 47/64 loss: -0.2223147749900818
Batch 48/64 loss: -0.24344462156295776
Batch 49/64 loss: -0.21596771478652954
Batch 50/64 loss: -0.2204209566116333
Batch 51/64 loss: -0.23523500561714172
Batch 52/64 loss: -0.27024006843566895
Batch 53/64 loss: -0.2352685034275055
Batch 54/64 loss: -0.24497875571250916
Batch 55/64 loss: -0.21957507729530334
Batch 56/64 loss: -0.23278141021728516
Batch 57/64 loss: -0.24559730291366577
Batch 58/64 loss: -0.23595431447029114
Batch 59/64 loss: -0.23054257035255432
Batch 60/64 loss: -0.24638020992279053
Batch 61/64 loss: -0.20013809204101562
Batch 62/64 loss: -0.24348488450050354
Batch 63/64 loss: -0.22489309310913086
Batch 64/64 loss: -0.24432975053787231
Epoch 433  Train loss: -0.23583523268793144  Val loss: -0.032474065769169336
Epoch 434
-------------------------------
Batch 1/64 loss: -0.25608041882514954
Batch 2/64 loss: -0.23798620700836182
Batch 3/64 loss: -0.23791572451591492
Batch 4/64 loss: -0.2370021939277649
Batch 5/64 loss: -0.23406445980072021
Batch 6/64 loss: -0.23054856061935425
Batch 7/64 loss: -0.23532068729400635
Batch 8/64 loss: -0.2606560289859772
Batch 9/64 loss: -0.26332899928092957
Batch 10/64 loss: -0.26329246163368225
Batch 11/64 loss: -0.20515316724777222
Batch 12/64 loss: -0.2283908724784851
Batch 13/64 loss: -0.231500506401062
Batch 14/64 loss: -0.25057557225227356
Batch 15/64 loss: -0.24081286787986755
Batch 16/64 loss: -0.24498212337493896
Batch 17/64 loss: -0.20391112565994263
Batch 18/64 loss: -0.2475683093070984
Batch 19/64 loss: -0.23335325717926025
Batch 20/64 loss: -0.2131023406982422
Batch 21/64 loss: -0.2021409273147583
Batch 22/64 loss: -0.21857798099517822
Batch 23/64 loss: -0.2303178310394287
Batch 24/64 loss: -0.24116051197052002
Batch 25/64 loss: -0.23112040758132935
Batch 26/64 loss: -0.24033239483833313
Batch 27/64 loss: -0.25314778089523315
Batch 28/64 loss: -0.20382559299468994
Batch 29/64 loss: -0.24348637461662292
Batch 30/64 loss: -0.245309978723526
Batch 31/64 loss: -0.23610129952430725
Batch 32/64 loss: -0.22922250628471375
Batch 33/64 loss: -0.22965073585510254
Batch 34/64 loss: -0.24614375829696655
Batch 35/64 loss: -0.21426105499267578
Batch 36/64 loss: -0.22349056601524353
Batch 37/64 loss: -0.21560096740722656
Batch 38/64 loss: -0.2416958510875702
Batch 39/64 loss: -0.23223358392715454
Batch 40/64 loss: -0.24164754152297974
Batch 41/64 loss: -0.25907155871391296
Batch 42/64 loss: -0.23271432518959045
Batch 43/64 loss: -0.2524144649505615
Batch 44/64 loss: -0.24674195051193237
Batch 45/64 loss: -0.24850285053253174
Batch 46/64 loss: -0.24464750289916992
Batch 47/64 loss: -0.24031943082809448
Batch 48/64 loss: -0.23768293857574463
Batch 49/64 loss: -0.1885545253753662
Batch 50/64 loss: -0.2483568787574768
Batch 51/64 loss: -0.23389065265655518
Batch 52/64 loss: -0.25530415773391724
Batch 53/64 loss: -0.22289317846298218
Batch 54/64 loss: -0.20468246936798096
Batch 55/64 loss: -0.2062651515007019
Batch 56/64 loss: -0.22384881973266602
Batch 57/64 loss: -0.24328017234802246
Batch 58/64 loss: -0.2444114089012146
Batch 59/64 loss: -0.24102765321731567
Batch 60/64 loss: -0.25276100635528564
Batch 61/64 loss: -0.2422853708267212
Batch 62/64 loss: -0.20024418830871582
Batch 63/64 loss: -0.24427926540374756
Batch 64/64 loss: -0.22840532660484314
Epoch 434  Train loss: -0.23467446974679534  Val loss: -0.03678352972076521
Epoch 435
-------------------------------
Batch 1/64 loss: -0.21619033813476562
Batch 2/64 loss: -0.23948371410369873
Batch 3/64 loss: -0.24978241324424744
Batch 4/64 loss: -0.2416812777519226
Batch 5/64 loss: -0.24478712677955627
Batch 6/64 loss: -0.2335827648639679
Batch 7/64 loss: -0.2468869686126709
Batch 8/64 loss: -0.2574329376220703
Batch 9/64 loss: -0.2224748730659485
Batch 10/64 loss: -0.23665866255760193
Batch 11/64 loss: -0.2610664963722229
Batch 12/64 loss: -0.2530997693538666
Batch 13/64 loss: -0.23804470896720886
Batch 14/64 loss: -0.2160009741783142
Batch 15/64 loss: -0.24770909547805786
Batch 16/64 loss: -0.2557159662246704
Batch 17/64 loss: -0.2540974020957947
Batch 18/64 loss: -0.22804325819015503
Batch 19/64 loss: -0.2466111183166504
Batch 20/64 loss: -0.24463403224945068
Batch 21/64 loss: -0.21268314123153687
Batch 22/64 loss: -0.24011385440826416
Batch 23/64 loss: -0.20889651775360107
Batch 24/64 loss: -0.24341565370559692
Batch 25/64 loss: -0.24175351858139038
Batch 26/64 loss: -0.23528820276260376
Batch 27/64 loss: -0.24280983209609985
Batch 28/64 loss: -0.24607235193252563
Batch 29/64 loss: -0.22256571054458618
Batch 30/64 loss: -0.22604268789291382
Batch 31/64 loss: -0.2597420811653137
Batch 32/64 loss: -0.24008461833000183
Batch 33/64 loss: -0.2384759485721588
Batch 34/64 loss: -0.2440577745437622
Batch 35/64 loss: -0.23238226771354675
Batch 36/64 loss: -0.23358309268951416
Batch 37/64 loss: -0.24048244953155518
Batch 38/64 loss: -0.22868025302886963
Batch 39/64 loss: -0.22099554538726807
Batch 40/64 loss: -0.25209084153175354
Batch 41/64 loss: -0.2301916480064392
Batch 42/64 loss: -0.21449080109596252
Batch 43/64 loss: -0.24536019563674927
Batch 44/64 loss: -0.24972206354141235
Batch 45/64 loss: -0.24769937992095947
Batch 46/64 loss: -0.22397994995117188
Batch 47/64 loss: -0.23023474216461182
Batch 48/64 loss: -0.22902607917785645
Batch 49/64 loss: -0.22812092304229736
Batch 50/64 loss: -0.23363640904426575
Batch 51/64 loss: -0.22324982285499573
Batch 52/64 loss: -0.24152123928070068
Batch 53/64 loss: -0.21952837705612183
Batch 54/64 loss: -0.2543434798717499
Batch 55/64 loss: -0.2261064052581787
Batch 56/64 loss: -0.23722293972969055
Batch 57/64 loss: -0.2237674593925476
Batch 58/64 loss: -0.24349427223205566
Batch 59/64 loss: -0.2085382342338562
Batch 60/64 loss: -0.24669477343559265
Batch 61/64 loss: -0.23378324508666992
Batch 62/64 loss: -0.2388514280319214
Batch 63/64 loss: -0.2297087013721466
Batch 64/64 loss: -0.2143757939338684
Epoch 435  Train loss: -0.23630203382641662  Val loss: -0.034828768563024776
Epoch 436
-------------------------------
Batch 1/64 loss: -0.21764230728149414
Batch 2/64 loss: -0.2630208134651184
Batch 3/64 loss: -0.20088142156600952
Batch 4/64 loss: -0.24376654624938965
Batch 5/64 loss: -0.2324204444885254
Batch 6/64 loss: -0.2674725651741028
Batch 7/64 loss: -0.24336597323417664
Batch 8/64 loss: -0.2604019045829773
Batch 9/64 loss: -0.22747239470481873
Batch 10/64 loss: -0.21360844373703003
Batch 11/64 loss: -0.23675918579101562
Batch 12/64 loss: -0.261441707611084
Batch 13/64 loss: -0.23012906312942505
Batch 14/64 loss: -0.21146902441978455
Batch 15/64 loss: -0.24903303384780884
Batch 16/64 loss: -0.2262895107269287
Batch 17/64 loss: -0.23892080783843994
Batch 18/64 loss: -0.2607603073120117
Batch 19/64 loss: -0.210191011428833
Batch 20/64 loss: -0.23480725288391113
Batch 21/64 loss: -0.22999173402786255
Batch 22/64 loss: -0.24305325746536255
Batch 23/64 loss: -0.24549046158790588
Batch 24/64 loss: -0.2470046877861023
Batch 25/64 loss: -0.24871939420700073
Batch 26/64 loss: -0.24499067664146423
Batch 27/64 loss: -0.24502694606781006
Batch 28/64 loss: -0.25239408016204834
Batch 29/64 loss: -0.22493132948875427
Batch 30/64 loss: -0.23594576120376587
Batch 31/64 loss: -0.2308099865913391
Batch 32/64 loss: -0.24471604824066162
Batch 33/64 loss: -0.24273788928985596
Batch 34/64 loss: -0.2481968104839325
Batch 35/64 loss: -0.23410269618034363
Batch 36/64 loss: -0.268657386302948
Batch 37/64 loss: -0.25558024644851685
Batch 38/64 loss: -0.21877974271774292
Batch 39/64 loss: -0.22513383626937866
Batch 40/64 loss: -0.2301771640777588
Batch 41/64 loss: -0.2550494372844696
Batch 42/64 loss: -0.25247305631637573
Batch 43/64 loss: -0.2302914261817932
Batch 44/64 loss: -0.2794579863548279
Batch 45/64 loss: -0.2402077317237854
Batch 46/64 loss: -0.25862693786621094
Batch 47/64 loss: -0.24241358041763306
Batch 48/64 loss: -0.21651014685630798
Batch 49/64 loss: -0.2228803038597107
Batch 50/64 loss: -0.19845092296600342
Batch 51/64 loss: -0.2335260808467865
Batch 52/64 loss: -0.2577189803123474
Batch 53/64 loss: -0.24560484290122986
Batch 54/64 loss: -0.23882171511650085
Batch 55/64 loss: -0.25571513175964355
Batch 56/64 loss: -0.2433086633682251
Batch 57/64 loss: -0.24367475509643555
Batch 58/64 loss: -0.22964990139007568
Batch 59/64 loss: -0.2228330373764038
Batch 60/64 loss: -0.24102115631103516
Batch 61/64 loss: -0.2398662567138672
Batch 62/64 loss: -0.24638831615447998
Batch 63/64 loss: -0.23166054487228394
Batch 64/64 loss: -0.22844615578651428
Epoch 436  Train loss: -0.23911810797803543  Val loss: -0.034159342447916664
Epoch 437
-------------------------------
Batch 1/64 loss: -0.2573299705982208
Batch 2/64 loss: -0.24629420042037964
Batch 3/64 loss: -0.2614591717720032
Batch 4/64 loss: -0.22544652223587036
Batch 5/64 loss: -0.2445026934146881
Batch 6/64 loss: -0.25071534514427185
Batch 7/64 loss: -0.20416349172592163
Batch 8/64 loss: -0.24716556072235107
Batch 9/64 loss: -0.22985076904296875
Batch 10/64 loss: -0.2481086254119873
Batch 11/64 loss: -0.24023285508155823
Batch 12/64 loss: -0.2756918966770172
Batch 13/64 loss: -0.22885605692863464
Batch 14/64 loss: -0.20384174585342407
Batch 15/64 loss: -0.247038334608078
Batch 16/64 loss: -0.2343597412109375
Batch 17/64 loss: -0.24689003825187683
Batch 18/64 loss: -0.23545265197753906
Batch 19/64 loss: -0.2405497431755066
Batch 20/64 loss: -0.23553985357284546
Batch 21/64 loss: -0.23871192336082458
Batch 22/64 loss: -0.2535066604614258
Batch 23/64 loss: -0.23275259137153625
Batch 24/64 loss: -0.2368442416191101
Batch 25/64 loss: -0.2324863076210022
Batch 26/64 loss: -0.21457713842391968
Batch 27/64 loss: -0.25299787521362305
Batch 28/64 loss: -0.21849730610847473
Batch 29/64 loss: -0.24990731477737427
Batch 30/64 loss: -0.23820337653160095
Batch 31/64 loss: -0.25153857469558716
Batch 32/64 loss: -0.2306300699710846
Batch 33/64 loss: -0.21582096815109253
Batch 34/64 loss: -0.2530946433544159
Batch 35/64 loss: -0.25997394323349
Batch 36/64 loss: -0.23862645030021667
Batch 37/64 loss: -0.2337402105331421
Batch 38/64 loss: -0.24079129099845886
Batch 39/64 loss: -0.2584301829338074
Batch 40/64 loss: -0.2550565004348755
Batch 41/64 loss: -0.23157471418380737
Batch 42/64 loss: -0.2582418918609619
Batch 43/64 loss: -0.24641144275665283
Batch 44/64 loss: -0.23806408047676086
Batch 45/64 loss: -0.2233067750930786
Batch 46/64 loss: -0.2461458444595337
Batch 47/64 loss: -0.23078584671020508
Batch 48/64 loss: -0.25356388092041016
Batch 49/64 loss: -0.24770814180374146
Batch 50/64 loss: -0.24430415034294128
Batch 51/64 loss: -0.23359113931655884
Batch 52/64 loss: -0.24204403162002563
Batch 53/64 loss: -0.25869667530059814
Batch 54/64 loss: -0.25321412086486816
Batch 55/64 loss: -0.23876625299453735
Batch 56/64 loss: -0.24360710382461548
Batch 57/64 loss: -0.2442011535167694
Batch 58/64 loss: -0.2468177080154419
Batch 59/64 loss: -0.2450513243675232
Batch 60/64 loss: -0.20298391580581665
Batch 61/64 loss: -0.25974714756011963
Batch 62/64 loss: -0.24656829237937927
Batch 63/64 loss: -0.22748243808746338
Batch 64/64 loss: -0.18529045581817627
Epoch 437  Train loss: -0.24018074905171113  Val loss: -0.03398128452989244
Epoch 438
-------------------------------
Batch 1/64 loss: -0.24409565329551697
Batch 2/64 loss: -0.2328534722328186
Batch 3/64 loss: -0.2511618733406067
Batch 4/64 loss: -0.250568687915802
Batch 5/64 loss: -0.24314755201339722
Batch 6/64 loss: -0.24274194240570068
Batch 7/64 loss: -0.2542378306388855
Batch 8/64 loss: -0.24893945455551147
Batch 9/64 loss: -0.24364829063415527
Batch 10/64 loss: -0.2700386643409729
Batch 11/64 loss: -0.2595279812812805
Batch 12/64 loss: -0.25073540210723877
Batch 13/64 loss: -0.2624514102935791
Batch 14/64 loss: -0.2480904757976532
Batch 15/64 loss: -0.2605747580528259
Batch 16/64 loss: -0.243147611618042
Batch 17/64 loss: -0.23777079582214355
Batch 18/64 loss: -0.24906542897224426
Batch 19/64 loss: -0.24128255248069763
Batch 20/64 loss: -0.2532479166984558
Batch 21/64 loss: -0.23826798796653748
Batch 22/64 loss: -0.2438359260559082
Batch 23/64 loss: -0.22583234310150146
Batch 24/64 loss: -0.2639209032058716
Batch 25/64 loss: -0.22692593932151794
Batch 26/64 loss: -0.23846763372421265
Batch 27/64 loss: -0.21496564149856567
Batch 28/64 loss: -0.2630380094051361
Batch 29/64 loss: -0.25308093428611755
Batch 30/64 loss: -0.25057917833328247
Batch 31/64 loss: -0.2682740092277527
Batch 32/64 loss: -0.26713991165161133
Batch 33/64 loss: -0.2562099099159241
Batch 34/64 loss: -0.25008511543273926
Batch 35/64 loss: -0.25564783811569214
Batch 36/64 loss: -0.2146231234073639
Batch 37/64 loss: -0.19207286834716797
Batch 38/64 loss: -0.24998098611831665
Batch 39/64 loss: -0.23707568645477295
Batch 40/64 loss: -0.22284340858459473
Batch 41/64 loss: -0.23364898562431335
Batch 42/64 loss: -0.25163763761520386
Batch 43/64 loss: -0.23935925960540771
Batch 44/64 loss: -0.23141011595726013
Batch 45/64 loss: -0.2249884009361267
Batch 46/64 loss: -0.24084120988845825
Batch 47/64 loss: -0.18556338548660278
Batch 48/64 loss: -0.24345722794532776
Batch 49/64 loss: -0.2106650471687317
Batch 50/64 loss: -0.2124137580394745
Batch 51/64 loss: -0.23504382371902466
Batch 52/64 loss: -0.23357972502708435
Batch 53/64 loss: -0.23255598545074463
Batch 54/64 loss: -0.24635809659957886
Batch 55/64 loss: -0.25361892580986023
Batch 56/64 loss: -0.2414952516555786
Batch 57/64 loss: -0.2375696897506714
Batch 58/64 loss: -0.24131327867507935
Batch 59/64 loss: -0.23091262578964233
Batch 60/64 loss: -0.20814445614814758
Batch 61/64 loss: -0.23580747842788696
Batch 62/64 loss: -0.1965373158454895
Batch 63/64 loss: -0.24901741743087769
Batch 64/64 loss: -0.23195111751556396
Epoch 438  Train loss: -0.24015839286878998  Val loss: -0.03457948432345571
Epoch 439
-------------------------------
Batch 1/64 loss: -0.2620614767074585
Batch 2/64 loss: -0.23849177360534668
Batch 3/64 loss: -0.2573389410972595
Batch 4/64 loss: -0.26140016317367554
Batch 5/64 loss: -0.2340450882911682
Batch 6/64 loss: -0.24029165506362915
Batch 7/64 loss: -0.2595813274383545
Batch 8/64 loss: -0.2489222288131714
Batch 9/64 loss: -0.24938777089118958
Batch 10/64 loss: -0.26222866773605347
Batch 11/64 loss: -0.23177853226661682
Batch 12/64 loss: -0.22230708599090576
Batch 13/64 loss: -0.25678277015686035
Batch 14/64 loss: -0.2497735321521759
Batch 15/64 loss: -0.2360961139202118
Batch 16/64 loss: -0.23692363500595093
Batch 17/64 loss: -0.25044286251068115
Batch 18/64 loss: -0.24695661664009094
Batch 19/64 loss: -0.2526238262653351
Batch 20/64 loss: -0.2206408977508545
Batch 21/64 loss: -0.20780372619628906
Batch 22/64 loss: -0.21865341067314148
Batch 23/64 loss: -0.2400152087211609
Batch 24/64 loss: -0.23311930894851685
Batch 25/64 loss: -0.28025156259536743
Batch 26/64 loss: -0.22802186012268066
Batch 27/64 loss: -0.22178387641906738
Batch 28/64 loss: -0.23452973365783691
Batch 29/64 loss: -0.21166038513183594
Batch 30/64 loss: -0.20615196228027344
Batch 31/64 loss: -0.24300017952919006
Batch 32/64 loss: -0.2131345272064209
Batch 33/64 loss: -0.23011514544487
Batch 34/64 loss: -0.2293679118156433
Batch 35/64 loss: -0.23330679535865784
Batch 36/64 loss: -0.23351958394050598
Batch 37/64 loss: -0.24061477184295654
Batch 38/64 loss: -0.2161877453327179
Batch 39/64 loss: -0.1952838897705078
Batch 40/64 loss: -0.22649657726287842
Batch 41/64 loss: -0.2253226637840271
Batch 42/64 loss: -0.2486448884010315
Batch 43/64 loss: -0.2577730119228363
Batch 44/64 loss: -0.2287203073501587
Batch 45/64 loss: -0.23452973365783691
Batch 46/64 loss: -0.24074915051460266
Batch 47/64 loss: -0.24295076727867126
Batch 48/64 loss: -0.23882368206977844
Batch 49/64 loss: -0.25065815448760986
Batch 50/64 loss: -0.22168517112731934
Batch 51/64 loss: -0.24259018898010254
Batch 52/64 loss: -0.21730729937553406
Batch 53/64 loss: -0.23348084092140198
Batch 54/64 loss: -0.21515589952468872
Batch 55/64 loss: -0.22590166330337524
Batch 56/64 loss: -0.20357823371887207
Batch 57/64 loss: -0.22592869400978088
Batch 58/64 loss: -0.20460689067840576
Batch 59/64 loss: -0.22600072622299194
Batch 60/64 loss: -0.22112086415290833
Batch 61/64 loss: -0.2305201292037964
Batch 62/64 loss: -0.23620525002479553
Batch 63/64 loss: -0.16199612617492676
Batch 64/64 loss: -0.2370486557483673
Epoch 439  Train loss: -0.23330399931645862  Val loss: -0.032678187917597926
Epoch 440
-------------------------------
Batch 1/64 loss: -0.23069432377815247
Batch 2/64 loss: -0.21875593066215515
Batch 3/64 loss: -0.25577831268310547
Batch 4/64 loss: -0.2555965781211853
Batch 5/64 loss: -0.23494556546211243
Batch 6/64 loss: -0.24342533946037292
Batch 7/64 loss: -0.2107628583908081
Batch 8/64 loss: -0.20797723531723022
Batch 9/64 loss: -0.24654877185821533
Batch 10/64 loss: -0.23911088705062866
Batch 11/64 loss: -0.23738300800323486
Batch 12/64 loss: -0.22617080807685852
Batch 13/64 loss: -0.24134308099746704
Batch 14/64 loss: -0.2361057996749878
Batch 15/64 loss: -0.26207515597343445
Batch 16/64 loss: -0.23978570103645325
Batch 17/64 loss: -0.23121881484985352
Batch 18/64 loss: -0.2112218737602234
Batch 19/64 loss: -0.22175094485282898
Batch 20/64 loss: -0.2402578890323639
Batch 21/64 loss: -0.2363576889038086
Batch 22/64 loss: -0.24467548727989197
Batch 23/64 loss: -0.25115060806274414
Batch 24/64 loss: -0.23162490129470825
Batch 25/64 loss: -0.25488221645355225
Batch 26/64 loss: -0.23139643669128418
Batch 27/64 loss: -0.23655694723129272
Batch 28/64 loss: -0.24268794059753418
Batch 29/64 loss: -0.27165305614471436
Batch 30/64 loss: -0.2335294485092163
Batch 31/64 loss: -0.2360396385192871
Batch 32/64 loss: -0.24454179406166077
Batch 33/64 loss: -0.21940654516220093
Batch 34/64 loss: -0.2180795669555664
Batch 35/64 loss: -0.22877514362335205
Batch 36/64 loss: -0.23389625549316406
Batch 37/64 loss: -0.22680169343948364
Batch 38/64 loss: -0.2654477655887604
Batch 39/64 loss: -0.23899948596954346
Batch 40/64 loss: -0.24573686718940735
Batch 41/64 loss: -0.2520090639591217
Batch 42/64 loss: -0.22611528635025024
Batch 43/64 loss: -0.2366265058517456
Batch 44/64 loss: -0.23672807216644287
Batch 45/64 loss: -0.23105260729789734
Batch 46/64 loss: -0.24204111099243164
Batch 47/64 loss: -0.23116755485534668
Batch 48/64 loss: -0.23891830444335938
Batch 49/64 loss: -0.2239847183227539
Batch 50/64 loss: -0.25754278898239136
Batch 51/64 loss: -0.2187815010547638
Batch 52/64 loss: -0.23046383261680603
Batch 53/64 loss: -0.2294982671737671
Batch 54/64 loss: -0.21998709440231323
Batch 55/64 loss: -0.2241673469543457
Batch 56/64 loss: -0.2617643177509308
Batch 57/64 loss: -0.2661206126213074
Batch 58/64 loss: -0.23871630430221558
Batch 59/64 loss: -0.23447296023368835
Batch 60/64 loss: -0.2445710301399231
Batch 61/64 loss: -0.2276322841644287
Batch 62/64 loss: -0.21632003784179688
Batch 63/64 loss: -0.23260539770126343
Batch 64/64 loss: -0.23528221249580383
Epoch 440  Train loss: -0.23656309060021943  Val loss: -0.03493184527171027
Epoch 441
-------------------------------
Batch 1/64 loss: -0.22759565711021423
Batch 2/64 loss: -0.24904149770736694
Batch 3/64 loss: -0.26785892248153687
Batch 4/64 loss: -0.255037784576416
Batch 5/64 loss: -0.23412221670150757
Batch 6/64 loss: -0.2566061317920685
Batch 7/64 loss: -0.2555086612701416
Batch 8/64 loss: -0.23961955308914185
Batch 9/64 loss: -0.22999781370162964
Batch 10/64 loss: -0.24063265323638916
Batch 11/64 loss: -0.24111336469650269
Batch 12/64 loss: -0.247519850730896
Batch 13/64 loss: -0.23189818859100342
Batch 14/64 loss: -0.24016141891479492
Batch 15/64 loss: -0.2031763195991516
Batch 16/64 loss: -0.19330787658691406
Batch 17/64 loss: -0.21719586849212646
Batch 18/64 loss: -0.24405282735824585
Batch 19/64 loss: -0.23092669248580933
Batch 20/64 loss: -0.22503212094306946
Batch 21/64 loss: -0.23427128791809082
Batch 22/64 loss: -0.2510893642902374
Batch 23/64 loss: -0.2542330324649811
Batch 24/64 loss: -0.2387813925743103
Batch 25/64 loss: -0.26192334294319153
Batch 26/64 loss: -0.23717868328094482
Batch 27/64 loss: -0.21890351176261902
Batch 28/64 loss: -0.23712468147277832
Batch 29/64 loss: -0.25326430797576904
Batch 30/64 loss: -0.24367603659629822
Batch 31/64 loss: -0.20779746770858765
Batch 32/64 loss: -0.233018159866333
Batch 33/64 loss: -0.2322363257408142
Batch 34/64 loss: -0.2498762011528015
Batch 35/64 loss: -0.24901175498962402
Batch 36/64 loss: -0.23793673515319824
Batch 37/64 loss: -0.24338847398757935
Batch 38/64 loss: -0.21207672357559204
Batch 39/64 loss: -0.23669451475143433
Batch 40/64 loss: -0.23270076513290405
Batch 41/64 loss: -0.2331870198249817
Batch 42/64 loss: -0.2241421341896057
Batch 43/64 loss: -0.24537315964698792
Batch 44/64 loss: -0.22610026597976685
Batch 45/64 loss: -0.2552183270454407
Batch 46/64 loss: -0.23901590704917908
Batch 47/64 loss: -0.2512194514274597
Batch 48/64 loss: -0.23101770877838135
Batch 49/64 loss: -0.262725830078125
Batch 50/64 loss: -0.25365936756134033
Batch 51/64 loss: -0.23767343163490295
Batch 52/64 loss: -0.24865257740020752
Batch 53/64 loss: -0.24663344025611877
Batch 54/64 loss: -0.24233919382095337
Batch 55/64 loss: -0.23010557889938354
Batch 56/64 loss: -0.2632409334182739
Batch 57/64 loss: -0.2212420105934143
Batch 58/64 loss: -0.24661314487457275
Batch 59/64 loss: -0.23571741580963135
Batch 60/64 loss: -0.24289286136627197
Batch 61/64 loss: -0.22583195567131042
Batch 62/64 loss: -0.22284194827079773
Batch 63/64 loss: -0.23938283324241638
Batch 64/64 loss: -0.23536056280136108
Epoch 441  Train loss: -0.23838329572303624  Val loss: -0.03260297566345058
Epoch 442
-------------------------------
Batch 1/64 loss: -0.25880196690559387
Batch 2/64 loss: -0.24584633111953735
Batch 3/64 loss: -0.2429429590702057
Batch 4/64 loss: -0.23917031288146973
Batch 5/64 loss: -0.2539452314376831
Batch 6/64 loss: -0.24365001916885376
Batch 7/64 loss: -0.23279660940170288
Batch 8/64 loss: -0.21357136964797974
Batch 9/64 loss: -0.24822667241096497
Batch 10/64 loss: -0.24505767226219177
Batch 11/64 loss: -0.1901308298110962
Batch 12/64 loss: -0.24517259001731873
Batch 13/64 loss: -0.24820780754089355
Batch 14/64 loss: -0.23104223608970642
Batch 15/64 loss: -0.23390376567840576
Batch 16/64 loss: -0.234913170337677
Batch 17/64 loss: -0.2440483570098877
Batch 18/64 loss: -0.23225238919258118
Batch 19/64 loss: -0.259680837392807
Batch 20/64 loss: -0.24397674202919006
Batch 21/64 loss: -0.2268354892730713
Batch 22/64 loss: -0.24960023164749146
Batch 23/64 loss: -0.2024499773979187
Batch 24/64 loss: -0.23796385526657104
Batch 25/64 loss: -0.21504032611846924
Batch 26/64 loss: -0.2266535460948944
Batch 27/64 loss: -0.23509269952774048
Batch 28/64 loss: -0.2421228289604187
Batch 29/64 loss: -0.24184447526931763
Batch 30/64 loss: -0.2076880931854248
Batch 31/64 loss: -0.22735649347305298
Batch 32/64 loss: -0.24965357780456543
Batch 33/64 loss: -0.24561193585395813
Batch 34/64 loss: -0.22107923030853271
Batch 35/64 loss: -0.23758465051651
Batch 36/64 loss: -0.23670896887779236
Batch 37/64 loss: -0.26108571887016296
Batch 38/64 loss: -0.23313462734222412
Batch 39/64 loss: -0.24114710092544556
Batch 40/64 loss: -0.262819766998291
Batch 41/64 loss: -0.24895882606506348
Batch 42/64 loss: -0.23112237453460693
Batch 43/64 loss: -0.24277186393737793
Batch 44/64 loss: -0.25505805015563965
Batch 45/64 loss: -0.24201998114585876
Batch 46/64 loss: -0.24090105295181274
Batch 47/64 loss: -0.23605823516845703
Batch 48/64 loss: -0.2501235902309418
Batch 49/64 loss: -0.24542436003684998
Batch 50/64 loss: -0.23143130540847778
Batch 51/64 loss: -0.22255706787109375
Batch 52/64 loss: -0.2069377899169922
Batch 53/64 loss: -0.24283644556999207
Batch 54/64 loss: -0.2546716034412384
Batch 55/64 loss: -0.23387229442596436
Batch 56/64 loss: -0.23422011733055115
Batch 57/64 loss: -0.24427169561386108
Batch 58/64 loss: -0.2412935197353363
Batch 59/64 loss: -0.24480628967285156
Batch 60/64 loss: -0.24713432788848877
Batch 61/64 loss: -0.2600248157978058
Batch 62/64 loss: -0.19445961713790894
Batch 63/64 loss: -0.2515103220939636
Batch 64/64 loss: -0.23241132497787476
Epoch 442  Train loss: -0.2378915373016806  Val loss: -0.033012905481345056
Epoch 443
-------------------------------
Batch 1/64 loss: -0.24063420295715332
Batch 2/64 loss: -0.23746398091316223
Batch 3/64 loss: -0.24315989017486572
Batch 4/64 loss: -0.250302791595459
Batch 5/64 loss: -0.2212715744972229
Batch 6/64 loss: -0.2173323631286621
Batch 7/64 loss: -0.2259933352470398
Batch 8/64 loss: -0.25171154737472534
Batch 9/64 loss: -0.24760735034942627
Batch 10/64 loss: -0.24596837162971497
Batch 11/64 loss: -0.25907236337661743
Batch 12/64 loss: -0.22467124462127686
Batch 13/64 loss: -0.2311534285545349
Batch 14/64 loss: -0.24385875463485718
Batch 15/64 loss: -0.25826495885849
Batch 16/64 loss: -0.23939597606658936
Batch 17/64 loss: -0.25743329524993896
Batch 18/64 loss: -0.24751070141792297
Batch 19/64 loss: -0.22535887360572815
Batch 20/64 loss: -0.2579518258571625
Batch 21/64 loss: -0.2307714819908142
Batch 22/64 loss: -0.23723304271697998
Batch 23/64 loss: -0.24749046564102173
Batch 24/64 loss: -0.25731921195983887
Batch 25/64 loss: -0.2480788230895996
Batch 26/64 loss: -0.2565830945968628
Batch 27/64 loss: -0.2130787968635559
Batch 28/64 loss: -0.25854045152664185
Batch 29/64 loss: -0.21178552508354187
Batch 30/64 loss: -0.2379266321659088
Batch 31/64 loss: -0.25501686334609985
Batch 32/64 loss: -0.22293227910995483
Batch 33/64 loss: -0.2419549822807312
Batch 34/64 loss: -0.2508591413497925
Batch 35/64 loss: -0.2541534900665283
Batch 36/64 loss: -0.21756833791732788
Batch 37/64 loss: -0.25699958205223083
Batch 38/64 loss: -0.25616633892059326
Batch 39/64 loss: -0.23154202103614807
Batch 40/64 loss: -0.22999447584152222
Batch 41/64 loss: -0.23268935084342957
Batch 42/64 loss: -0.25013360381126404
Batch 43/64 loss: -0.23131206631660461
Batch 44/64 loss: -0.24159684777259827
Batch 45/64 loss: -0.24634486436843872
Batch 46/64 loss: -0.2169642448425293
Batch 47/64 loss: -0.2498171627521515
Batch 48/64 loss: -0.24155369400978088
Batch 49/64 loss: -0.21753394603729248
Batch 50/64 loss: -0.2051127552986145
Batch 51/64 loss: -0.24085578322410583
Batch 52/64 loss: -0.23739075660705566
Batch 53/64 loss: -0.261023610830307
Batch 54/64 loss: -0.21332493424415588
Batch 55/64 loss: -0.24275776743888855
Batch 56/64 loss: -0.2558242380619049
Batch 57/64 loss: -0.24638405442237854
Batch 58/64 loss: -0.2333584725856781
Batch 59/64 loss: -0.23688778281211853
Batch 60/64 loss: -0.261141300201416
Batch 61/64 loss: -0.2561272382736206
Batch 62/64 loss: -0.26112183928489685
Batch 63/64 loss: -0.2651614248752594
Batch 64/64 loss: -0.2534586191177368
Epoch 443  Train loss: -0.24120193836735745  Val loss: -0.03611603573835183
Epoch 444
-------------------------------
Batch 1/64 loss: -0.22776567935943604
Batch 2/64 loss: -0.2645990550518036
Batch 3/64 loss: -0.2744191586971283
Batch 4/64 loss: -0.25689566135406494
Batch 5/64 loss: -0.21593594551086426
Batch 6/64 loss: -0.2521102726459503
Batch 7/64 loss: -0.2267856001853943
Batch 8/64 loss: -0.2145603895187378
Batch 9/64 loss: -0.23341721296310425
Batch 10/64 loss: -0.26433321833610535
Batch 11/64 loss: -0.268399715423584
Batch 12/64 loss: -0.25685828924179077
Batch 13/64 loss: -0.26035693287849426
Batch 14/64 loss: -0.22698736190795898
Batch 15/64 loss: -0.24124258756637573
Batch 16/64 loss: -0.2471846044063568
Batch 17/64 loss: -0.25896507501602173
Batch 18/64 loss: -0.24106189608573914
Batch 19/64 loss: -0.24134227633476257
Batch 20/64 loss: -0.20728355646133423
Batch 21/64 loss: -0.23957574367523193
Batch 22/64 loss: -0.2642425298690796
Batch 23/64 loss: -0.24308598041534424
Batch 24/64 loss: -0.23604756593704224
Batch 25/64 loss: -0.22823858261108398
Batch 26/64 loss: -0.25663116574287415
Batch 27/64 loss: -0.2514539957046509
Batch 28/64 loss: -0.20621323585510254
Batch 29/64 loss: -0.21429675817489624
Batch 30/64 loss: -0.25294503569602966
Batch 31/64 loss: -0.2336832880973816
Batch 32/64 loss: -0.24364668130874634
Batch 33/64 loss: -0.2163143754005432
Batch 34/64 loss: -0.2379186451435089
Batch 35/64 loss: -0.24007323384284973
Batch 36/64 loss: -0.2427433729171753
Batch 37/64 loss: -0.22508075833320618
Batch 38/64 loss: -0.2560741603374481
Batch 39/64 loss: -0.2296167016029358
Batch 40/64 loss: -0.2405261993408203
Batch 41/64 loss: -0.24532076716423035
Batch 42/64 loss: -0.2699233591556549
Batch 43/64 loss: -0.25764980912208557
Batch 44/64 loss: -0.26713424921035767
Batch 45/64 loss: -0.2505180537700653
Batch 46/64 loss: -0.260168194770813
Batch 47/64 loss: -0.2600071132183075
Batch 48/64 loss: -0.22852739691734314
Batch 49/64 loss: -0.2485189437866211
Batch 50/64 loss: -0.2471766471862793
Batch 51/64 loss: -0.24507027864456177
Batch 52/64 loss: -0.2404395341873169
Batch 53/64 loss: -0.24492305517196655
Batch 54/64 loss: -0.23237395286560059
Batch 55/64 loss: -0.19978654384613037
Batch 56/64 loss: -0.24215048551559448
Batch 57/64 loss: -0.23874711990356445
Batch 58/64 loss: -0.2509542405605316
Batch 59/64 loss: -0.238023579120636
Batch 60/64 loss: -0.22303426265716553
Batch 61/64 loss: -0.22607213258743286
Batch 62/64 loss: -0.24776104092597961
Batch 63/64 loss: -0.2385694682598114
Batch 64/64 loss: -0.20245373249053955
Epoch 444  Train loss: -0.24146828277438295  Val loss: -0.03559820758518075
Epoch 445
-------------------------------
Batch 1/64 loss: -0.248601496219635
Batch 2/64 loss: -0.24810069799423218
Batch 3/64 loss: -0.2380886673927307
Batch 4/64 loss: -0.22771233320236206
Batch 5/64 loss: -0.23670357465744019
Batch 6/64 loss: -0.20899856090545654
Batch 7/64 loss: -0.22717517614364624
Batch 8/64 loss: -0.23235255479812622
Batch 9/64 loss: -0.22650539875030518
Batch 10/64 loss: -0.255903959274292
Batch 11/64 loss: -0.24184858798980713
Batch 12/64 loss: -0.2545009255409241
Batch 13/64 loss: -0.254122793674469
Batch 14/64 loss: -0.24255633354187012
Batch 15/64 loss: -0.2470521628856659
Batch 16/64 loss: -0.21696138381958008
Batch 17/64 loss: -0.23996302485466003
Batch 18/64 loss: -0.2456836998462677
Batch 19/64 loss: -0.24405431747436523
Batch 20/64 loss: -0.21277010440826416
Batch 21/64 loss: -0.24570155143737793
Batch 22/64 loss: -0.20917165279388428
Batch 23/64 loss: -0.2534829378128052
Batch 24/64 loss: -0.2427515983581543
Batch 25/64 loss: -0.20243215560913086
Batch 26/64 loss: -0.20749413967132568
Batch 27/64 loss: -0.2452298104763031
Batch 28/64 loss: -0.26360273361206055
Batch 29/64 loss: -0.2187604308128357
Batch 30/64 loss: -0.23073649406433105
Batch 31/64 loss: -0.25055673718452454
Batch 32/64 loss: -0.24774041771888733
Batch 33/64 loss: -0.20906203985214233
Batch 34/64 loss: -0.2462921142578125
Batch 35/64 loss: -0.2463700771331787
Batch 36/64 loss: -0.23434826731681824
Batch 37/64 loss: -0.2537449300289154
Batch 38/64 loss: -0.23642826080322266
Batch 39/64 loss: -0.2615185081958771
Batch 40/64 loss: -0.24705153703689575
Batch 41/64 loss: -0.24008703231811523
Batch 42/64 loss: -0.24094054102897644
Batch 43/64 loss: -0.24232324957847595
Batch 44/64 loss: -0.22360080480575562
Batch 45/64 loss: -0.22831588983535767
Batch 46/64 loss: -0.2361707091331482
Batch 47/64 loss: -0.24669045209884644
Batch 48/64 loss: -0.2571756839752197
Batch 49/64 loss: -0.26103174686431885
Batch 50/64 loss: -0.24437302350997925
Batch 51/64 loss: -0.24242451786994934
Batch 52/64 loss: -0.2308935523033142
Batch 53/64 loss: -0.24045449495315552
Batch 54/64 loss: -0.23973217606544495
Batch 55/64 loss: -0.2576596140861511
Batch 56/64 loss: -0.24016445875167847
Batch 57/64 loss: -0.22495952248573303
Batch 58/64 loss: -0.21428704261779785
Batch 59/64 loss: -0.23551633954048157
Batch 60/64 loss: -0.22564947605133057
Batch 61/64 loss: -0.2565833330154419
Batch 62/64 loss: -0.2282313108444214
Batch 63/64 loss: -0.22355490922927856
Batch 64/64 loss: -0.23275214433670044
Epoch 445  Train loss: -0.23776495900808597  Val loss: -0.034959361315592866
Epoch 446
-------------------------------
Batch 1/64 loss: -0.210629403591156
Batch 2/64 loss: -0.25701677799224854
Batch 3/64 loss: -0.2591976821422577
Batch 4/64 loss: -0.24207919836044312
Batch 5/64 loss: -0.2744094431400299
Batch 6/64 loss: -0.24733984470367432
Batch 7/64 loss: -0.24581441283226013
Batch 8/64 loss: -0.2591712176799774
Batch 9/64 loss: -0.2178434431552887
Batch 10/64 loss: -0.2554594874382019
Batch 11/64 loss: -0.23965772986412048
Batch 12/64 loss: -0.2544102668762207
Batch 13/64 loss: -0.2616526484489441
Batch 14/64 loss: -0.25803086161613464
Batch 15/64 loss: -0.2144521176815033
Batch 16/64 loss: -0.2224811613559723
Batch 17/64 loss: -0.2506853938102722
Batch 18/64 loss: -0.25421157479286194
Batch 19/64 loss: -0.25264737010002136
Batch 20/64 loss: -0.25273221731185913
Batch 21/64 loss: -0.23047372698783875
Batch 22/64 loss: -0.2397696077823639
Batch 23/64 loss: -0.2580433785915375
Batch 24/64 loss: -0.19784599542617798
Batch 25/64 loss: -0.2304002046585083
Batch 26/64 loss: -0.23898237943649292
Batch 27/64 loss: -0.21173095703125
Batch 28/64 loss: -0.23770076036453247
Batch 29/64 loss: -0.24138736724853516
Batch 30/64 loss: -0.21265381574630737
Batch 31/64 loss: -0.2373397946357727
Batch 32/64 loss: -0.28113651275634766
Batch 33/64 loss: -0.2043147087097168
Batch 34/64 loss: -0.22410836815834045
Batch 35/64 loss: -0.22828161716461182
Batch 36/64 loss: -0.24087777733802795
Batch 37/64 loss: -0.25293654203414917
Batch 38/64 loss: -0.22472327947616577
Batch 39/64 loss: -0.23426896333694458
Batch 40/64 loss: -0.24740689992904663
Batch 41/64 loss: -0.23611637949943542
Batch 42/64 loss: -0.2501171827316284
Batch 43/64 loss: -0.24373364448547363
Batch 44/64 loss: -0.2613263428211212
Batch 45/64 loss: -0.2541772723197937
Batch 46/64 loss: -0.24472475051879883
Batch 47/64 loss: -0.22767895460128784
Batch 48/64 loss: -0.21274477243423462
Batch 49/64 loss: -0.22859084606170654
Batch 50/64 loss: -0.22156831622123718
Batch 51/64 loss: -0.21165424585342407
Batch 52/64 loss: -0.23803505301475525
Batch 53/64 loss: -0.2545942962169647
Batch 54/64 loss: -0.22222024202346802
Batch 55/64 loss: -0.2205774188041687
Batch 56/64 loss: -0.17522287368774414
Batch 57/64 loss: -0.2604781687259674
Batch 58/64 loss: -0.254056841135025
Batch 59/64 loss: -0.254125714302063
Batch 60/64 loss: -0.26009586453437805
Batch 61/64 loss: -0.2429736852645874
Batch 62/64 loss: -0.22755134105682373
Batch 63/64 loss: -0.20582515001296997
Batch 64/64 loss: -0.21603524684906006
Epoch 446  Train loss: -0.2380316972732544  Val loss: -0.03256108936984924
Epoch 447
-------------------------------
Batch 1/64 loss: -0.23579704761505127
Batch 2/64 loss: -0.243615984916687
Batch 3/64 loss: -0.2723042368888855
Batch 4/64 loss: -0.26505085825920105
Batch 5/64 loss: -0.2615405321121216
Batch 6/64 loss: -0.26306167244911194
Batch 7/64 loss: -0.23265281319618225
Batch 8/64 loss: -0.23869428038597107
Batch 9/64 loss: -0.2121252715587616
Batch 10/64 loss: -0.25003355741500854
Batch 11/64 loss: -0.22949320077896118
Batch 12/64 loss: -0.2537316679954529
Batch 13/64 loss: -0.25547128915786743
Batch 14/64 loss: -0.23762443661689758
Batch 15/64 loss: -0.21482276916503906
Batch 16/64 loss: -0.25544530153274536
Batch 17/64 loss: -0.262295126914978
Batch 18/64 loss: -0.2290230393409729
Batch 19/64 loss: -0.23820066452026367
Batch 20/64 loss: -0.23936843872070312
Batch 21/64 loss: -0.25724634528160095
Batch 22/64 loss: -0.24796950817108154
Batch 23/64 loss: -0.2494766116142273
Batch 24/64 loss: -0.2588104009628296
Batch 25/64 loss: -0.2549116909503937
Batch 26/64 loss: -0.2652295231819153
Batch 27/64 loss: -0.24879759550094604
Batch 28/64 loss: -0.22998014092445374
Batch 29/64 loss: -0.23690924048423767
Batch 30/64 loss: -0.23346641659736633
Batch 31/64 loss: -0.25784820318222046
Batch 32/64 loss: -0.25648438930511475
Batch 33/64 loss: -0.24765479564666748
Batch 34/64 loss: -0.2619604468345642
Batch 35/64 loss: -0.2612845003604889
Batch 36/64 loss: -0.21084916591644287
Batch 37/64 loss: -0.23790481686592102
Batch 38/64 loss: -0.26205843687057495
Batch 39/64 loss: -0.24268653988838196
Batch 40/64 loss: -0.2630123198032379
Batch 41/64 loss: -0.2678089439868927
Batch 42/64 loss: -0.2449890375137329
Batch 43/64 loss: -0.23725414276123047
Batch 44/64 loss: -0.23579341173171997
Batch 45/64 loss: -0.24279284477233887
Batch 46/64 loss: -0.20210480690002441
Batch 47/64 loss: -0.24691197276115417
Batch 48/64 loss: -0.21603178977966309
Batch 49/64 loss: -0.2366487681865692
Batch 50/64 loss: -0.23750022053718567
Batch 51/64 loss: -0.25012069940567017
Batch 52/64 loss: -0.22358500957489014
Batch 53/64 loss: -0.24023988842964172
Batch 54/64 loss: -0.21321669220924377
Batch 55/64 loss: -0.24991834163665771
Batch 56/64 loss: -0.23443284630775452
Batch 57/64 loss: -0.252577543258667
Batch 58/64 loss: -0.2300897240638733
Batch 59/64 loss: -0.24218326807022095
Batch 60/64 loss: -0.24229371547698975
Batch 61/64 loss: -0.25285375118255615
Batch 62/64 loss: -0.24808010458946228
Batch 63/64 loss: -0.26109546422958374
Batch 64/64 loss: -0.2413969337940216
Epoch 447  Train loss: -0.24414845436227087  Val loss: -0.03426963582481306
Epoch 448
-------------------------------
Batch 1/64 loss: -0.2286255657672882
Batch 2/64 loss: -0.26176801323890686
Batch 3/64 loss: -0.2604210376739502
Batch 4/64 loss: -0.2580316364765167
Batch 5/64 loss: -0.24813926219940186
Batch 6/64 loss: -0.2616158127784729
Batch 7/64 loss: -0.2343493103981018
Batch 8/64 loss: -0.25614166259765625
Batch 9/64 loss: -0.24859973788261414
Batch 10/64 loss: -0.2416423261165619
Batch 11/64 loss: -0.2295115888118744
Batch 12/64 loss: -0.23269933462142944
Batch 13/64 loss: -0.2347933053970337
Batch 14/64 loss: -0.23108622431755066
Batch 15/64 loss: -0.22475048899650574
Batch 16/64 loss: -0.22430852055549622
Batch 17/64 loss: -0.23608696460723877
Batch 18/64 loss: -0.23680299520492554
Batch 19/64 loss: -0.2232135534286499
Batch 20/64 loss: -0.24459028244018555
Batch 21/64 loss: -0.23206555843353271
Batch 22/64 loss: -0.23144805431365967
Batch 23/64 loss: -0.24844008684158325
Batch 24/64 loss: -0.22187387943267822
Batch 25/64 loss: -0.2591908574104309
Batch 26/64 loss: -0.22224316000938416
Batch 27/64 loss: -0.23592185974121094
Batch 28/64 loss: -0.2571503520011902
Batch 29/64 loss: -0.2321840524673462
Batch 30/64 loss: -0.24086400866508484
Batch 31/64 loss: -0.22470268607139587
Batch 32/64 loss: -0.23179304599761963
Batch 33/64 loss: -0.2318444848060608
Batch 34/64 loss: -0.2635572850704193
Batch 35/64 loss: -0.2382487654685974
Batch 36/64 loss: -0.25508785247802734
Batch 37/64 loss: -0.27162832021713257
Batch 38/64 loss: -0.20897632837295532
Batch 39/64 loss: -0.2586221992969513
Batch 40/64 loss: -0.24491292238235474
Batch 41/64 loss: -0.25399142503738403
Batch 42/64 loss: -0.22941040992736816
Batch 43/64 loss: -0.2481555938720703
Batch 44/64 loss: -0.26074278354644775
Batch 45/64 loss: -0.2339736819267273
Batch 46/64 loss: -0.25940781831741333
Batch 47/64 loss: -0.2534697949886322
Batch 48/64 loss: -0.23447611927986145
Batch 49/64 loss: -0.24506008625030518
Batch 50/64 loss: -0.21144497394561768
Batch 51/64 loss: -0.25575506687164307
Batch 52/64 loss: -0.2600727379322052
Batch 53/64 loss: -0.1871674656867981
Batch 54/64 loss: -0.2256419062614441
Batch 55/64 loss: -0.2396332025527954
Batch 56/64 loss: -0.2508932948112488
Batch 57/64 loss: -0.2524425983428955
Batch 58/64 loss: -0.23713791370391846
Batch 59/64 loss: -0.2386152744293213
Batch 60/64 loss: -0.2513311803340912
Batch 61/64 loss: -0.23072800040245056
Batch 62/64 loss: -0.2476092278957367
Batch 63/64 loss: -0.2326907515525818
Batch 64/64 loss: -0.22698861360549927
Epoch 448  Train loss: -0.24059648864409502  Val loss: -0.03443342734038625
Epoch 449
-------------------------------
Batch 1/64 loss: -0.24991744756698608
Batch 2/64 loss: -0.25854435563087463
Batch 3/64 loss: -0.2394910454750061
Batch 4/64 loss: -0.2545628547668457
Batch 5/64 loss: -0.26349136233329773
Batch 6/64 loss: -0.216054767370224
Batch 7/64 loss: -0.25946348905563354
Batch 8/64 loss: -0.23418551683425903
Batch 9/64 loss: -0.24876073002815247
Batch 10/64 loss: -0.2415187954902649
Batch 11/64 loss: -0.24078863859176636
Batch 12/64 loss: -0.24095404148101807
Batch 13/64 loss: -0.23501530289649963
Batch 14/64 loss: -0.24661248922348022
Batch 15/64 loss: -0.24603423476219177
Batch 16/64 loss: -0.2373572587966919
Batch 17/64 loss: -0.25301194190979004
Batch 18/64 loss: -0.24042260646820068
Batch 19/64 loss: -0.21735703945159912
Batch 20/64 loss: -0.21725866198539734
Batch 21/64 loss: -0.2615067660808563
Batch 22/64 loss: -0.2454300820827484
Batch 23/64 loss: -0.22273868322372437
Batch 24/64 loss: -0.24467352032661438
Batch 25/64 loss: -0.23223334550857544
Batch 26/64 loss: -0.23481059074401855
Batch 27/64 loss: -0.25192081928253174
Batch 28/64 loss: -0.24270981550216675
Batch 29/64 loss: -0.25569504499435425
Batch 30/64 loss: -0.247625470161438
Batch 31/64 loss: -0.2453555464744568
Batch 32/64 loss: -0.24993345141410828
Batch 33/64 loss: -0.22557061910629272
Batch 34/64 loss: -0.2576293349266052
Batch 35/64 loss: -0.2525211274623871
Batch 36/64 loss: -0.2774132490158081
Batch 37/64 loss: -0.2146798074245453
Batch 38/64 loss: -0.27792924642562866
Batch 39/64 loss: -0.23758867383003235
Batch 40/64 loss: -0.23616164922714233
Batch 41/64 loss: -0.23121869564056396
Batch 42/64 loss: -0.27008605003356934
Batch 43/64 loss: -0.23796570301055908
Batch 44/64 loss: -0.254966139793396
Batch 45/64 loss: -0.25846782326698303
Batch 46/64 loss: -0.26304981112480164
Batch 47/64 loss: -0.18787497282028198
Batch 48/64 loss: -0.253282755613327
Batch 49/64 loss: -0.20302248001098633
Batch 50/64 loss: -0.2486664056777954
Batch 51/64 loss: -0.215896338224411
Batch 52/64 loss: -0.20662742853164673
Batch 53/64 loss: -0.2432299256324768
Batch 54/64 loss: -0.23398065567016602
Batch 55/64 loss: -0.23116564750671387
Batch 56/64 loss: -0.2529945373535156
Batch 57/64 loss: -0.2342764139175415
Batch 58/64 loss: -0.25198692083358765
Batch 59/64 loss: -0.2495993673801422
Batch 60/64 loss: -0.25374579429626465
Batch 61/64 loss: -0.23198455572128296
Batch 62/64 loss: -0.23077240586280823
Batch 63/64 loss: -0.27357515692710876
Batch 64/64 loss: -0.2679651975631714
Epoch 449  Train loss: -0.24273473421732586  Val loss: -0.03326423286982009
Epoch 450
-------------------------------
Batch 1/64 loss: -0.24676889181137085
Batch 2/64 loss: -0.20771503448486328
Batch 3/64 loss: -0.20875537395477295
Batch 4/64 loss: -0.2545652389526367
Batch 5/64 loss: -0.2624255418777466
Batch 6/64 loss: -0.2434721291065216
Batch 7/64 loss: -0.2500084638595581
Batch 8/64 loss: -0.26193004846572876
Batch 9/64 loss: -0.25805073976516724
Batch 10/64 loss: -0.24886217713356018
Batch 11/64 loss: -0.24803119897842407
Batch 12/64 loss: -0.2402324676513672
Batch 13/64 loss: -0.2653506398200989
Batch 14/64 loss: -0.20521557331085205
Batch 15/64 loss: -0.22597220540046692
Batch 16/64 loss: -0.22462892532348633
Batch 17/64 loss: -0.22579509019851685
Batch 18/64 loss: -0.22386837005615234
Batch 19/64 loss: -0.2554188370704651
Batch 20/64 loss: -0.2540930509567261
Batch 21/64 loss: -0.24137884378433228
Batch 22/64 loss: -0.2483951449394226
Batch 23/64 loss: -0.236752450466156
Batch 24/64 loss: -0.251625657081604
Batch 25/64 loss: -0.23117786645889282
Batch 26/64 loss: -0.2562001347541809
Batch 27/64 loss: -0.19681453704833984
Batch 28/64 loss: -0.2261354923248291
Batch 29/64 loss: -0.2378975749015808
Batch 30/64 loss: -0.2308935523033142
Batch 31/64 loss: -0.23399505019187927
Batch 32/64 loss: -0.24656367301940918
Batch 33/64 loss: -0.21523573994636536
Batch 34/64 loss: -0.26067978143692017
Batch 35/64 loss: -0.2457643449306488
Batch 36/64 loss: -0.2407476007938385
Batch 37/64 loss: -0.2330647110939026
Batch 38/64 loss: -0.251984179019928
Batch 39/64 loss: -0.2429060935974121
Batch 40/64 loss: -0.26467612385749817
Batch 41/64 loss: -0.2548412084579468
Batch 42/64 loss: -0.2370256781578064
Batch 43/64 loss: -0.2684687674045563
Batch 44/64 loss: -0.2678014934062958
Batch 45/64 loss: -0.23591583967208862
Batch 46/64 loss: -0.26263293623924255
Batch 47/64 loss: -0.25115180015563965
Batch 48/64 loss: -0.24968823790550232
Batch 49/64 loss: -0.27704668045043945
Batch 50/64 loss: -0.25052374601364136
Batch 51/64 loss: -0.2447623610496521
Batch 52/64 loss: -0.2543735206127167
Batch 53/64 loss: -0.19962358474731445
Batch 54/64 loss: -0.25073814392089844
Batch 55/64 loss: -0.26011067628860474
Batch 56/64 loss: -0.2560540437698364
Batch 57/64 loss: -0.2322104573249817
Batch 58/64 loss: -0.22631564736366272
Batch 59/64 loss: -0.228995680809021
Batch 60/64 loss: -0.26395362615585327
Batch 61/64 loss: -0.25080204010009766
Batch 62/64 loss: -0.2008836269378662
Batch 63/64 loss: -0.23141050338745117
Batch 64/64 loss: -0.2503621578216553
Epoch 450  Train loss: -0.2423082426482556  Val loss: -0.03224843515153603
Epoch 451
-------------------------------
Batch 1/64 loss: -0.26472461223602295
Batch 2/64 loss: -0.25028812885284424
Batch 3/64 loss: -0.21999245882034302
Batch 4/64 loss: -0.2376294732093811
Batch 5/64 loss: -0.22436106204986572
Batch 6/64 loss: -0.24403339624404907
Batch 7/64 loss: -0.2710111141204834
Batch 8/64 loss: -0.2528083920478821
Batch 9/64 loss: -0.2528133690357208
Batch 10/64 loss: -0.24890202283859253
Batch 11/64 loss: -0.2537367343902588
Batch 12/64 loss: -0.23079928755760193
Batch 13/64 loss: -0.23715215921401978
Batch 14/64 loss: -0.23275846242904663
Batch 15/64 loss: -0.25038790702819824
Batch 16/64 loss: -0.246841698884964
Batch 17/64 loss: -0.23872041702270508
Batch 18/64 loss: -0.22297221422195435
Batch 19/64 loss: -0.24626535177230835
Batch 20/64 loss: -0.2516234815120697
Batch 21/64 loss: -0.2682698369026184
Batch 22/64 loss: -0.263180136680603
Batch 23/64 loss: -0.25238242745399475
Batch 24/64 loss: -0.26148587465286255
Batch 25/64 loss: -0.25664111971855164
Batch 26/64 loss: -0.2448289692401886
Batch 27/64 loss: -0.27146968245506287
Batch 28/64 loss: -0.2582077085971832
Batch 29/64 loss: -0.25575900077819824
Batch 30/64 loss: -0.2541978061199188
Batch 31/64 loss: -0.25414085388183594
Batch 32/64 loss: -0.26057830452919006
Batch 33/64 loss: -0.21166712045669556
Batch 34/64 loss: -0.2187199592590332
Batch 35/64 loss: -0.22640389204025269
Batch 36/64 loss: -0.24704670906066895
Batch 37/64 loss: -0.2635670602321625
Batch 38/64 loss: -0.21684110164642334
Batch 39/64 loss: -0.24152874946594238
Batch 40/64 loss: -0.24557167291641235
Batch 41/64 loss: -0.22815167903900146
Batch 42/64 loss: -0.23908063769340515
Batch 43/64 loss: -0.2379840910434723
Batch 44/64 loss: -0.24034923315048218
Batch 45/64 loss: -0.244942307472229
Batch 46/64 loss: -0.22490733861923218
Batch 47/64 loss: -0.2502731680870056
Batch 48/64 loss: -0.22961902618408203
Batch 49/64 loss: -0.23400112986564636
Batch 50/64 loss: -0.23580330610275269
Batch 51/64 loss: -0.2467857003211975
Batch 52/64 loss: -0.24829986691474915
Batch 53/64 loss: -0.24328351020812988
Batch 54/64 loss: -0.25042733550071716
Batch 55/64 loss: -0.2441437840461731
Batch 56/64 loss: -0.25194379687309265
Batch 57/64 loss: -0.2498704493045807
Batch 58/64 loss: -0.25171908736228943
Batch 59/64 loss: -0.2351331114768982
Batch 60/64 loss: -0.2325814664363861
Batch 61/64 loss: -0.24780526757240295
Batch 62/64 loss: -0.2441171407699585
Batch 63/64 loss: -0.24657511711120605
Batch 64/64 loss: -0.23347410559654236
Epoch 451  Train loss: -0.2444425546655468  Val loss: -0.03293778335105922
Epoch 452
-------------------------------
Batch 1/64 loss: -0.24809986352920532
Batch 2/64 loss: -0.23194831609725952
Batch 3/64 loss: -0.26917052268981934
Batch 4/64 loss: -0.2510233521461487
Batch 5/64 loss: -0.24283021688461304
Batch 6/64 loss: -0.23597988486289978
Batch 7/64 loss: -0.2328682839870453
Batch 8/64 loss: -0.25288087129592896
Batch 9/64 loss: -0.22897839546203613
Batch 10/64 loss: -0.2543291449546814
Batch 11/64 loss: -0.255013108253479
Batch 12/64 loss: -0.2641424834728241
Batch 13/64 loss: -0.26210343837738037
Batch 14/64 loss: -0.2001001238822937
Batch 15/64 loss: -0.25918129086494446
Batch 16/64 loss: -0.2524923086166382
Batch 17/64 loss: -0.24017149209976196
Batch 18/64 loss: -0.25379687547683716
Batch 19/64 loss: -0.2480374276638031
Batch 20/64 loss: -0.22891896963119507
Batch 21/64 loss: -0.2592848241329193
Batch 22/64 loss: -0.2574835419654846
Batch 23/64 loss: -0.2402200698852539
Batch 24/64 loss: -0.24067050218582153
Batch 25/64 loss: -0.25529569387435913
Batch 26/64 loss: -0.257052481174469
Batch 27/64 loss: -0.2552163898944855
Batch 28/64 loss: -0.22104349732398987
Batch 29/64 loss: -0.2558005750179291
Batch 30/64 loss: -0.24064475297927856
Batch 31/64 loss: -0.23822736740112305
Batch 32/64 loss: -0.252887487411499
Batch 33/64 loss: -0.2433631420135498
Batch 34/64 loss: -0.2529652714729309
Batch 35/64 loss: -0.23879516124725342
Batch 36/64 loss: -0.2739691734313965
Batch 37/64 loss: -0.22223696112632751
Batch 38/64 loss: -0.237787127494812
Batch 39/64 loss: -0.25919610261917114
Batch 40/64 loss: -0.23447567224502563
Batch 41/64 loss: -0.2570388615131378
Batch 42/64 loss: -0.2448773980140686
Batch 43/64 loss: -0.25159454345703125
Batch 44/64 loss: -0.2323949933052063
Batch 45/64 loss: -0.23123538494110107
Batch 46/64 loss: -0.24114540219306946
Batch 47/64 loss: -0.238918274641037
Batch 48/64 loss: -0.2314961850643158
Batch 49/64 loss: -0.21902352571487427
Batch 50/64 loss: -0.25494325160980225
Batch 51/64 loss: -0.2526347041130066
Batch 52/64 loss: -0.252116322517395
Batch 53/64 loss: -0.23652970790863037
Batch 54/64 loss: -0.23624855279922485
Batch 55/64 loss: -0.252862811088562
Batch 56/64 loss: -0.25113922357559204
Batch 57/64 loss: -0.24558019638061523
Batch 58/64 loss: -0.2710902690887451
Batch 59/64 loss: -0.2254561483860016
Batch 60/64 loss: -0.2561262249946594
Batch 61/64 loss: -0.24688705801963806
Batch 62/64 loss: -0.25119030475616455
Batch 63/64 loss: -0.21506643295288086
Batch 64/64 loss: -0.25872641801834106
Epoch 452  Train loss: -0.24530702361873552  Val loss: -0.03301391548307491
Epoch 453
-------------------------------
Batch 1/64 loss: -0.2352270483970642
Batch 2/64 loss: -0.2525448501110077
Batch 3/64 loss: -0.22849464416503906
Batch 4/64 loss: -0.24985605478286743
Batch 5/64 loss: -0.25616127252578735
Batch 6/64 loss: -0.2545127272605896
Batch 7/64 loss: -0.26636284589767456
Batch 8/64 loss: -0.24234527349472046
Batch 9/64 loss: -0.25338637828826904
Batch 10/64 loss: -0.25013697147369385
Batch 11/64 loss: -0.25493836402893066
Batch 12/64 loss: -0.2562530040740967
Batch 13/64 loss: -0.27021852135658264
Batch 14/64 loss: -0.250786155462265
Batch 15/64 loss: -0.237654447555542
Batch 16/64 loss: -0.2259611189365387
Batch 17/64 loss: -0.2529495358467102
Batch 18/64 loss: -0.2585222125053406
Batch 19/64 loss: -0.27484196424484253
Batch 20/64 loss: -0.2530176639556885
Batch 21/64 loss: -0.21530598402023315
Batch 22/64 loss: -0.25108158588409424
Batch 23/64 loss: -0.23639994859695435
Batch 24/64 loss: -0.22093439102172852
Batch 25/64 loss: -0.24153155088424683
Batch 26/64 loss: -0.2450757622718811
Batch 27/64 loss: -0.23699086904525757
Batch 28/64 loss: -0.2600446343421936
Batch 29/64 loss: -0.2700059413909912
Batch 30/64 loss: -0.22327595949172974
Batch 31/64 loss: -0.2378760278224945
Batch 32/64 loss: -0.23671451210975647
Batch 33/64 loss: -0.2625282406806946
Batch 34/64 loss: -0.2435673475265503
Batch 35/64 loss: -0.23882731795310974
Batch 36/64 loss: -0.25422003865242004
Batch 37/64 loss: -0.25058233737945557
Batch 38/64 loss: -0.250688761472702
Batch 39/64 loss: -0.2550179958343506
Batch 40/64 loss: -0.24249893426895142
Batch 41/64 loss: -0.23080217838287354
Batch 42/64 loss: -0.23938262462615967
Batch 43/64 loss: -0.26869428157806396
Batch 44/64 loss: -0.200334370136261
Batch 45/64 loss: -0.23288851976394653
Batch 46/64 loss: -0.24662768840789795
Batch 47/64 loss: -0.25393950939178467
Batch 48/64 loss: -0.20283961296081543
Batch 49/64 loss: -0.2521555423736572
Batch 50/64 loss: -0.25230392813682556
Batch 51/64 loss: -0.2536505460739136
Batch 52/64 loss: -0.24477049708366394
Batch 53/64 loss: -0.25303059816360474
Batch 54/64 loss: -0.24802640080451965
Batch 55/64 loss: -0.2436085045337677
Batch 56/64 loss: -0.26278603076934814
Batch 57/64 loss: -0.28520503640174866
Batch 58/64 loss: -0.22474974393844604
Batch 59/64 loss: -0.24145817756652832
Batch 60/64 loss: -0.2452259361743927
Batch 61/64 loss: -0.24864137172698975
Batch 62/64 loss: -0.236905038356781
Batch 63/64 loss: -0.243991881608963
Batch 64/64 loss: -0.23023995757102966
Epoch 453  Train loss: -0.2459927401121925  Val loss: -0.03382796397323871
Epoch 454
-------------------------------
Batch 1/64 loss: -0.23000195622444153
Batch 2/64 loss: -0.23962581157684326
Batch 3/64 loss: -0.2527239918708801
Batch 4/64 loss: -0.23642143607139587
Batch 5/64 loss: -0.2599499225616455
Batch 6/64 loss: -0.24909937381744385
Batch 7/64 loss: -0.21520525217056274
Batch 8/64 loss: -0.260292112827301
Batch 9/64 loss: -0.2592872977256775
Batch 10/64 loss: -0.26813313364982605
Batch 11/64 loss: -0.26056164503097534
Batch 12/64 loss: -0.2648562788963318
Batch 13/64 loss: -0.26354947686195374
Batch 14/64 loss: -0.23046493530273438
Batch 15/64 loss: -0.26613813638687134
Batch 16/64 loss: -0.2530958652496338
Batch 17/64 loss: -0.24772533774375916
Batch 18/64 loss: -0.22181183099746704
Batch 19/64 loss: -0.2465457022190094
Batch 20/64 loss: -0.2648733854293823
Batch 21/64 loss: -0.26367753744125366
Batch 22/64 loss: -0.26401621103286743
Batch 23/64 loss: -0.23777174949645996
Batch 24/64 loss: -0.2504006028175354
Batch 25/64 loss: -0.22613400220870972
Batch 26/64 loss: -0.2517347037792206
Batch 27/64 loss: -0.22844630479812622
Batch 28/64 loss: -0.2547321319580078
Batch 29/64 loss: -0.22897648811340332
Batch 30/64 loss: -0.23660999536514282
Batch 31/64 loss: -0.2561291456222534
Batch 32/64 loss: -0.259799987077713
Batch 33/64 loss: -0.2539327144622803
Batch 34/64 loss: -0.24231380224227905
Batch 35/64 loss: -0.2681472599506378
Batch 36/64 loss: -0.24924913048744202
Batch 37/64 loss: -0.20644539594650269
Batch 38/64 loss: -0.22866922616958618
Batch 39/64 loss: -0.23512518405914307
Batch 40/64 loss: -0.24997353553771973
Batch 41/64 loss: -0.2527365982532501
Batch 42/64 loss: -0.26065146923065186
Batch 43/64 loss: -0.2502955198287964
Batch 44/64 loss: -0.24360159039497375
Batch 45/64 loss: -0.2418515980243683
Batch 46/64 loss: -0.23399844765663147
Batch 47/64 loss: -0.24768108129501343
Batch 48/64 loss: -0.23217809200286865
Batch 49/64 loss: -0.22878342866897583
Batch 50/64 loss: -0.2555103302001953
Batch 51/64 loss: -0.2684391736984253
Batch 52/64 loss: -0.2605942487716675
Batch 53/64 loss: -0.24273040890693665
Batch 54/64 loss: -0.24012351036071777
Batch 55/64 loss: -0.24403932690620422
Batch 56/64 loss: -0.2576877474784851
Batch 57/64 loss: -0.27223309874534607
Batch 58/64 loss: -0.24367615580558777
Batch 59/64 loss: -0.23464027047157288
Batch 60/64 loss: -0.20458132028579712
Batch 61/64 loss: -0.23092031478881836
Batch 62/64 loss: -0.2518252730369568
Batch 63/64 loss: -0.2353534698486328
Batch 64/64 loss: -0.22634649276733398
Epoch 454  Train loss: -0.2460633778104595  Val loss: -0.03510414714256103
Epoch 455
-------------------------------
Batch 1/64 loss: -0.2379189431667328
Batch 2/64 loss: -0.2519930601119995
Batch 3/64 loss: -0.24320638179779053
Batch 4/64 loss: -0.24767965078353882
Batch 5/64 loss: -0.2490878701210022
Batch 6/64 loss: -0.264961838722229
Batch 7/64 loss: -0.25960519909858704
Batch 8/64 loss: -0.24212872982025146
Batch 9/64 loss: -0.25675633549690247
Batch 10/64 loss: -0.2278977632522583
Batch 11/64 loss: -0.22306865453720093
Batch 12/64 loss: -0.2461344599723816
Batch 13/64 loss: -0.24379605054855347
Batch 14/64 loss: -0.2616857886314392
Batch 15/64 loss: -0.23448675870895386
Batch 16/64 loss: -0.24072086811065674
Batch 17/64 loss: -0.26075613498687744
Batch 18/64 loss: -0.26444780826568604
Batch 19/64 loss: -0.2452722191810608
Batch 20/64 loss: -0.2399599850177765
Batch 21/64 loss: -0.26104995608329773
Batch 22/64 loss: -0.24951797723770142
Batch 23/64 loss: -0.2529657781124115
Batch 24/64 loss: -0.2557675838470459
Batch 25/64 loss: -0.2441694140434265
Batch 26/64 loss: -0.2468135952949524
Batch 27/64 loss: -0.27095484733581543
Batch 28/64 loss: -0.2309197187423706
Batch 29/64 loss: -0.23594409227371216
Batch 30/64 loss: -0.24868911504745483
Batch 31/64 loss: -0.2574845552444458
Batch 32/64 loss: -0.23860400915145874
Batch 33/64 loss: -0.2295660674571991
Batch 34/64 loss: -0.26957058906555176
Batch 35/64 loss: -0.25307899713516235
Batch 36/64 loss: -0.24485769867897034
Batch 37/64 loss: -0.2508937120437622
Batch 38/64 loss: -0.243986576795578
Batch 39/64 loss: -0.2662101089954376
Batch 40/64 loss: -0.2371152639389038
Batch 41/64 loss: -0.26227647066116333
Batch 42/64 loss: -0.26862549781799316
Batch 43/64 loss: -0.2448887825012207
Batch 44/64 loss: -0.2619771361351013
Batch 45/64 loss: -0.2519332468509674
Batch 46/64 loss: -0.25489670038223267
Batch 47/64 loss: -0.2500273585319519
Batch 48/64 loss: -0.2570831775665283
Batch 49/64 loss: -0.25238484144210815
Batch 50/64 loss: -0.2213028371334076
Batch 51/64 loss: -0.2395046055316925
Batch 52/64 loss: -0.25779759883880615
Batch 53/64 loss: -0.22149208188056946
Batch 54/64 loss: -0.258095383644104
Batch 55/64 loss: -0.2191961705684662
Batch 56/64 loss: -0.26486167311668396
Batch 57/64 loss: -0.2428167462348938
Batch 58/64 loss: -0.24760907888412476
Batch 59/64 loss: -0.2537645697593689
Batch 60/64 loss: -0.2536948621273041
Batch 61/64 loss: -0.24320286512374878
Batch 62/64 loss: -0.24527543783187866
Batch 63/64 loss: -0.1892629861831665
Batch 64/64 loss: -0.2454235851764679
Epoch 455  Train loss: -0.24746293261939403  Val loss: -0.031700219485358276
Epoch 456
-------------------------------
Batch 1/64 loss: -0.2520827651023865
Batch 2/64 loss: -0.25970160961151123
Batch 3/64 loss: -0.27380865812301636
Batch 4/64 loss: -0.20533406734466553
Batch 5/64 loss: -0.2379455864429474
Batch 6/64 loss: -0.2549106180667877
Batch 7/64 loss: -0.22755461931228638
Batch 8/64 loss: -0.22462531924247742
Batch 9/64 loss: -0.25902265310287476
Batch 10/64 loss: -0.22101080417633057
Batch 11/64 loss: -0.183726966381073
Batch 12/64 loss: -0.2175338864326477
Batch 13/64 loss: -0.2532930374145508
Batch 14/64 loss: -0.2492489218711853
Batch 15/64 loss: -0.2585570216178894
Batch 16/64 loss: -0.24238747358322144
Batch 17/64 loss: -0.19478094577789307
Batch 18/64 loss: -0.22714310884475708
Batch 19/64 loss: -0.23509669303894043
Batch 20/64 loss: -0.26806512475013733
Batch 21/64 loss: -0.2510833144187927
Batch 22/64 loss: -0.24457687139511108
Batch 23/64 loss: -0.25924018025398254
Batch 24/64 loss: -0.2622203826904297
Batch 25/64 loss: -0.2358272671699524
Batch 26/64 loss: -0.22338193655014038
Batch 27/64 loss: -0.26004907488822937
Batch 28/64 loss: -0.24544930458068848
Batch 29/64 loss: -0.24471551179885864
Batch 30/64 loss: -0.2601965665817261
Batch 31/64 loss: -0.2490474283695221
Batch 32/64 loss: -0.24868905544281006
Batch 33/64 loss: -0.23885780572891235
Batch 34/64 loss: -0.2544664740562439
Batch 35/64 loss: -0.2432112991809845
Batch 36/64 loss: -0.2273695468902588
Batch 37/64 loss: -0.2510973811149597
Batch 38/64 loss: -0.2575104832649231
Batch 39/64 loss: -0.2523300051689148
Batch 40/64 loss: -0.25063496828079224
Batch 41/64 loss: -0.24601712822914124
Batch 42/64 loss: -0.24092847108840942
Batch 43/64 loss: -0.2305368185043335
Batch 44/64 loss: -0.25347307324409485
Batch 45/64 loss: -0.2540033459663391
Batch 46/64 loss: -0.22021043300628662
Batch 47/64 loss: -0.24814268946647644
Batch 48/64 loss: -0.25342458486557007
Batch 49/64 loss: -0.24762874841690063
Batch 50/64 loss: -0.2642377018928528
Batch 51/64 loss: -0.23971840739250183
Batch 52/64 loss: -0.2528039813041687
Batch 53/64 loss: -0.263757586479187
Batch 54/64 loss: -0.25009995698928833
Batch 55/64 loss: -0.24810588359832764
Batch 56/64 loss: -0.2577386498451233
Batch 57/64 loss: -0.2516587972640991
Batch 58/64 loss: -0.25298064947128296
Batch 59/64 loss: -0.2040003538131714
Batch 60/64 loss: -0.23662805557250977
Batch 61/64 loss: -0.262901246547699
Batch 62/64 loss: -0.25048065185546875
Batch 63/64 loss: -0.2529718577861786
Batch 64/64 loss: -0.24034547805786133
Epoch 456  Train loss: -0.24421165363461364  Val loss: -0.0306260903266697
Epoch 457
-------------------------------
Batch 1/64 loss: -0.2015252709388733
Batch 2/64 loss: -0.20954450964927673
Batch 3/64 loss: -0.24588948488235474
Batch 4/64 loss: -0.25909313559532166
Batch 5/64 loss: -0.2551654577255249
Batch 6/64 loss: -0.21734628081321716
Batch 7/64 loss: -0.26054638624191284
Batch 8/64 loss: -0.25382721424102783
Batch 9/64 loss: -0.25278258323669434
Batch 10/64 loss: -0.23150137066841125
Batch 11/64 loss: -0.2640085220336914
Batch 12/64 loss: -0.22414404153823853
Batch 13/64 loss: -0.2624974250793457
Batch 14/64 loss: -0.23703700304031372
Batch 15/64 loss: -0.23652607202529907
Batch 16/64 loss: -0.23184490203857422
Batch 17/64 loss: -0.24857082962989807
Batch 18/64 loss: -0.2386748492717743
Batch 19/64 loss: -0.2680334150791168
Batch 20/64 loss: -0.24482572078704834
Batch 21/64 loss: -0.19145655632019043
Batch 22/64 loss: -0.24523651599884033
Batch 23/64 loss: -0.25567710399627686
Batch 24/64 loss: -0.24140727519989014
Batch 25/64 loss: -0.23689913749694824
Batch 26/64 loss: -0.2435188889503479
Batch 27/64 loss: -0.23451176285743713
Batch 28/64 loss: -0.24021786451339722
Batch 29/64 loss: -0.24117672443389893
Batch 30/64 loss: -0.24653089046478271
Batch 31/64 loss: -0.22462037205696106
Batch 32/64 loss: -0.25194627046585083
Batch 33/64 loss: -0.2472062110900879
Batch 34/64 loss: -0.2552493214607239
Batch 35/64 loss: -0.24335205554962158
Batch 36/64 loss: -0.2450466752052307
Batch 37/64 loss: -0.25513774156570435
Batch 38/64 loss: -0.2568228244781494
Batch 39/64 loss: -0.2307114601135254
Batch 40/64 loss: -0.2541203796863556
Batch 41/64 loss: -0.23074999451637268
Batch 42/64 loss: -0.24929356575012207
Batch 43/64 loss: -0.2531289756298065
Batch 44/64 loss: -0.22621285915374756
Batch 45/64 loss: -0.25952595472335815
Batch 46/64 loss: -0.2674860656261444
Batch 47/64 loss: -0.25366508960723877
Batch 48/64 loss: -0.24699869751930237
Batch 49/64 loss: -0.19267040491104126
Batch 50/64 loss: -0.22875425219535828
Batch 51/64 loss: -0.21670973300933838
Batch 52/64 loss: -0.23320621252059937
Batch 53/64 loss: -0.24843600392341614
Batch 54/64 loss: -0.23934417963027954
Batch 55/64 loss: -0.24340218305587769
Batch 56/64 loss: -0.23968994617462158
Batch 57/64 loss: -0.23576626181602478
Batch 58/64 loss: -0.22381925582885742
Batch 59/64 loss: -0.22056031227111816
Batch 60/64 loss: -0.2570143938064575
Batch 61/64 loss: -0.2654479742050171
Batch 62/64 loss: -0.238154798746109
Batch 63/64 loss: -0.24770775437355042
Batch 64/64 loss: -0.24670732021331787
Epoch 457  Train loss: -0.24136479789135504  Val loss: -0.03300212851095036
Epoch 458
-------------------------------
Batch 1/64 loss: -0.24838215112686157
Batch 2/64 loss: -0.25028571486473083
Batch 3/64 loss: -0.26148444414138794
Batch 4/64 loss: -0.2489159107208252
Batch 5/64 loss: -0.2639269232749939
Batch 6/64 loss: -0.2445935308933258
Batch 7/64 loss: -0.2636091411113739
Batch 8/64 loss: -0.24107211828231812
Batch 9/64 loss: -0.21865582466125488
Batch 10/64 loss: -0.2714841067790985
Batch 11/64 loss: -0.23888123035430908
Batch 12/64 loss: -0.24366521835327148
Batch 13/64 loss: -0.25580844283103943
Batch 14/64 loss: -0.23012542724609375
Batch 15/64 loss: -0.2537562847137451
Batch 16/64 loss: -0.26754695177078247
Batch 17/64 loss: -0.2569517195224762
Batch 18/64 loss: -0.2545556128025055
Batch 19/64 loss: -0.2558683753013611
Batch 20/64 loss: -0.2658792734146118
Batch 21/64 loss: -0.23692816495895386
Batch 22/64 loss: -0.2698265314102173
Batch 23/64 loss: -0.26377689838409424
Batch 24/64 loss: -0.2593270242214203
Batch 25/64 loss: -0.2699674665927887
Batch 26/64 loss: -0.2762700021266937
Batch 27/64 loss: -0.2502858638763428
Batch 28/64 loss: -0.27003803849220276
Batch 29/64 loss: -0.2596605718135834
Batch 30/64 loss: -0.22287559509277344
Batch 31/64 loss: -0.27162301540374756
Batch 32/64 loss: -0.24214094877243042
Batch 33/64 loss: -0.20633965730667114
Batch 34/64 loss: -0.2397424280643463
Batch 35/64 loss: -0.2500390112400055
Batch 36/64 loss: -0.24063053727149963
Batch 37/64 loss: -0.23575088381767273
Batch 38/64 loss: -0.19619160890579224
Batch 39/64 loss: -0.2744506001472473
Batch 40/64 loss: -0.25921887159347534
Batch 41/64 loss: -0.2284422516822815
Batch 42/64 loss: -0.25386226177215576
Batch 43/64 loss: -0.20689129829406738
Batch 44/64 loss: -0.23625925183296204
Batch 45/64 loss: -0.2307329773902893
Batch 46/64 loss: -0.23102372884750366
Batch 47/64 loss: -0.20916271209716797
Batch 48/64 loss: -0.2305513620376587
Batch 49/64 loss: -0.24696820974349976
Batch 50/64 loss: -0.25933167338371277
Batch 51/64 loss: -0.25625079870224
Batch 52/64 loss: -0.23358380794525146
Batch 53/64 loss: -0.23482346534729004
Batch 54/64 loss: -0.25902390480041504
Batch 55/64 loss: -0.2636626958847046
Batch 56/64 loss: -0.21450138092041016
Batch 57/64 loss: -0.2601558566093445
Batch 58/64 loss: -0.2382703423500061
Batch 59/64 loss: -0.27348682284355164
Batch 60/64 loss: -0.2541161775588989
Batch 61/64 loss: -0.24725809693336487
Batch 62/64 loss: -0.22828519344329834
Batch 63/64 loss: -0.25744199752807617
Batch 64/64 loss: -0.25288885831832886
Epoch 458  Train loss: -0.24743976522894467  Val loss: -0.03419880891583629
Epoch 459
-------------------------------
Batch 1/64 loss: -0.2636965811252594
Batch 2/64 loss: -0.2473640739917755
Batch 3/64 loss: -0.2614385187625885
Batch 4/64 loss: -0.25802725553512573
Batch 5/64 loss: -0.25434964895248413
Batch 6/64 loss: -0.2672328054904938
Batch 7/64 loss: -0.2389853298664093
Batch 8/64 loss: -0.23895013332366943
Batch 9/64 loss: -0.27170896530151367
Batch 10/64 loss: -0.21924540400505066
Batch 11/64 loss: -0.26231956481933594
Batch 12/64 loss: -0.24755191802978516
Batch 13/64 loss: -0.2516043186187744
Batch 14/64 loss: -0.24267315864562988
Batch 15/64 loss: -0.25181466341018677
Batch 16/64 loss: -0.267772376537323
Batch 17/64 loss: -0.2526804804801941
Batch 18/64 loss: -0.25455451011657715
Batch 19/64 loss: -0.23195505142211914
Batch 20/64 loss: -0.23240286111831665
Batch 21/64 loss: -0.2379544973373413
Batch 22/64 loss: -0.25127720832824707
Batch 23/64 loss: -0.22314083576202393
Batch 24/64 loss: -0.22876128554344177
Batch 25/64 loss: -0.24341827630996704
Batch 26/64 loss: -0.24806582927703857
Batch 27/64 loss: -0.24492835998535156
Batch 28/64 loss: -0.2604832649230957
Batch 29/64 loss: -0.2503395676612854
Batch 30/64 loss: -0.23292607069015503
Batch 31/64 loss: -0.24595731496810913
Batch 32/64 loss: -0.2431328296661377
Batch 33/64 loss: -0.2692601680755615
Batch 34/64 loss: -0.24293795228004456
Batch 35/64 loss: -0.24253058433532715
Batch 36/64 loss: -0.26572340726852417
Batch 37/64 loss: -0.26470476388931274
Batch 38/64 loss: -0.2557324469089508
Batch 39/64 loss: -0.25708404183387756
Batch 40/64 loss: -0.23203831911087036
Batch 41/64 loss: -0.23593765497207642
Batch 42/64 loss: -0.2555188536643982
Batch 43/64 loss: -0.24268734455108643
Batch 44/64 loss: -0.23895901441574097
Batch 45/64 loss: -0.2492000162601471
Batch 46/64 loss: -0.2299003303050995
Batch 47/64 loss: -0.2614125609397888
Batch 48/64 loss: -0.2523338794708252
Batch 49/64 loss: -0.21998411417007446
Batch 50/64 loss: -0.24191614985466003
Batch 51/64 loss: -0.2565074563026428
Batch 52/64 loss: -0.24964746832847595
Batch 53/64 loss: -0.22124701738357544
Batch 54/64 loss: -0.24190044403076172
Batch 55/64 loss: -0.24026906490325928
Batch 56/64 loss: -0.2511603534221649
Batch 57/64 loss: -0.26301926374435425
Batch 58/64 loss: -0.24191328883171082
Batch 59/64 loss: -0.25469011068344116
Batch 60/64 loss: -0.25877368450164795
Batch 61/64 loss: -0.2441953718662262
Batch 62/64 loss: -0.2450554370880127
Batch 63/64 loss: -0.2367555797100067
Batch 64/64 loss: -0.2316131591796875
Epoch 459  Train loss: -0.2472693172155642  Val loss: -0.031115439544428664
Epoch 460
-------------------------------
Batch 1/64 loss: -0.23705029487609863
Batch 2/64 loss: -0.21511447429656982
Batch 3/64 loss: -0.25097450613975525
Batch 4/64 loss: -0.26796746253967285
Batch 5/64 loss: -0.261386513710022
Batch 6/64 loss: -0.24677050113677979
Batch 7/64 loss: -0.24830859899520874
Batch 8/64 loss: -0.2743040919303894
Batch 9/64 loss: -0.23807024955749512
Batch 10/64 loss: -0.23635485768318176
Batch 11/64 loss: -0.2467852532863617
Batch 12/64 loss: -0.24936223030090332
Batch 13/64 loss: -0.2030327320098877
Batch 14/64 loss: -0.22190919518470764
Batch 15/64 loss: -0.24246233701705933
Batch 16/64 loss: -0.2531280517578125
Batch 17/64 loss: -0.25164973735809326
Batch 18/64 loss: -0.2740047574043274
Batch 19/64 loss: -0.2358606457710266
Batch 20/64 loss: -0.2694641947746277
Batch 21/64 loss: -0.255420982837677
Batch 22/64 loss: -0.22851350903511047
Batch 23/64 loss: -0.23673021793365479
Batch 24/64 loss: -0.23950481414794922
Batch 25/64 loss: -0.21714937686920166
Batch 26/64 loss: -0.2228701412677765
Batch 27/64 loss: -0.2513620853424072
Batch 28/64 loss: -0.22068700194358826
Batch 29/64 loss: -0.2521735429763794
Batch 30/64 loss: -0.24126604199409485
Batch 31/64 loss: -0.24054855108261108
Batch 32/64 loss: -0.2441467046737671
Batch 33/64 loss: -0.25708234310150146
Batch 34/64 loss: -0.2155924141407013
Batch 35/64 loss: -0.2308533787727356
Batch 36/64 loss: -0.2350931167602539
Batch 37/64 loss: -0.24699974060058594
Batch 38/64 loss: -0.241474449634552
Batch 39/64 loss: -0.2393956482410431
Batch 40/64 loss: -0.25288188457489014
Batch 41/64 loss: -0.2297840714454651
Batch 42/64 loss: -0.21821588277816772
Batch 43/64 loss: -0.2509883642196655
Batch 44/64 loss: -0.2377004623413086
Batch 45/64 loss: -0.2528337240219116
Batch 46/64 loss: -0.24705857038497925
Batch 47/64 loss: -0.25278937816619873
Batch 48/64 loss: -0.24385195970535278
Batch 49/64 loss: -0.24248027801513672
Batch 50/64 loss: -0.24507710337638855
Batch 51/64 loss: -0.251578152179718
Batch 52/64 loss: -0.2504552900791168
Batch 53/64 loss: -0.22713184356689453
Batch 54/64 loss: -0.25389522314071655
Batch 55/64 loss: -0.23623982071876526
Batch 56/64 loss: -0.26293790340423584
Batch 57/64 loss: -0.24337202310562134
Batch 58/64 loss: -0.25930553674697876
Batch 59/64 loss: -0.256573885679245
Batch 60/64 loss: -0.2564932107925415
Batch 61/64 loss: -0.23029300570487976
Batch 62/64 loss: -0.2580903172492981
Batch 63/64 loss: -0.2529633641242981
Batch 64/64 loss: -0.216019868850708
Epoch 460  Train loss: -0.24338558281169217  Val loss: -0.02989462171633219
Epoch 461
-------------------------------
Batch 1/64 loss: -0.2597648501396179
Batch 2/64 loss: -0.24164384603500366
Batch 3/64 loss: -0.27579066157341003
Batch 4/64 loss: -0.2629983425140381
Batch 5/64 loss: -0.26460325717926025
Batch 6/64 loss: -0.24795734882354736
Batch 7/64 loss: -0.2553043067455292
Batch 8/64 loss: -0.23005086183547974
Batch 9/64 loss: -0.254894495010376
Batch 10/64 loss: -0.22388190031051636
Batch 11/64 loss: -0.2515544891357422
Batch 12/64 loss: -0.25628724694252014
Batch 13/64 loss: -0.2514655590057373
Batch 14/64 loss: -0.26119595766067505
Batch 15/64 loss: -0.26212382316589355
Batch 16/64 loss: -0.251056432723999
Batch 17/64 loss: -0.2602483034133911
Batch 18/64 loss: -0.24425524473190308
Batch 19/64 loss: -0.2815625071525574
Batch 20/64 loss: -0.259018212556839
Batch 21/64 loss: -0.20417970418930054
Batch 22/64 loss: -0.2594001293182373
Batch 23/64 loss: -0.22958803176879883
Batch 24/64 loss: -0.2693416476249695
Batch 25/64 loss: -0.2755334973335266
Batch 26/64 loss: -0.2570793628692627
Batch 27/64 loss: -0.23459404706954956
Batch 28/64 loss: -0.23269754648208618
Batch 29/64 loss: -0.21736371517181396
Batch 30/64 loss: -0.24001574516296387
Batch 31/64 loss: -0.25052767992019653
Batch 32/64 loss: -0.23023918271064758
Batch 33/64 loss: -0.25391092896461487
Batch 34/64 loss: -0.2501714825630188
Batch 35/64 loss: -0.25444358587265015
Batch 36/64 loss: -0.25456976890563965
Batch 37/64 loss: -0.2606397867202759
Batch 38/64 loss: -0.24294692277908325
Batch 39/64 loss: -0.2260642647743225
Batch 40/64 loss: -0.23036053776741028
Batch 41/64 loss: -0.22624754905700684
Batch 42/64 loss: -0.22473832964897156
Batch 43/64 loss: -0.24200889468193054
Batch 44/64 loss: -0.26022157073020935
Batch 45/64 loss: -0.2066282033920288
Batch 46/64 loss: -0.21167591214179993
Batch 47/64 loss: -0.24523061513900757
Batch 48/64 loss: -0.2464854121208191
Batch 49/64 loss: -0.2664927542209625
Batch 50/64 loss: -0.22325792908668518
Batch 51/64 loss: -0.23491498827934265
Batch 52/64 loss: -0.24183887243270874
Batch 53/64 loss: -0.19893133640289307
Batch 54/64 loss: -0.24872726202011108
Batch 55/64 loss: -0.23942232131958008
Batch 56/64 loss: -0.24570003151893616
Batch 57/64 loss: -0.25725945830345154
Batch 58/64 loss: -0.24747419357299805
Batch 59/64 loss: -0.23291051387786865
Batch 60/64 loss: -0.23975488543510437
Batch 61/64 loss: -0.25452476739883423
Batch 62/64 loss: -0.26633143424987793
Batch 63/64 loss: -0.24516808986663818
Batch 64/64 loss: -0.22737336158752441
Epoch 461  Train loss: -0.2454243225209853  Val loss: -0.03467556455291014
Epoch 462
-------------------------------
Batch 1/64 loss: -0.26606908440589905
Batch 2/64 loss: -0.25260499119758606
Batch 3/64 loss: -0.25564175844192505
Batch 4/64 loss: -0.2811182737350464
Batch 5/64 loss: -0.25334763526916504
Batch 6/64 loss: -0.2341923713684082
Batch 7/64 loss: -0.2351359724998474
Batch 8/64 loss: -0.2457227110862732
Batch 9/64 loss: -0.24203604459762573
Batch 10/64 loss: -0.24021577835083008
Batch 11/64 loss: -0.2333245575428009
Batch 12/64 loss: -0.22063183784484863
Batch 13/64 loss: -0.250396728515625
Batch 14/64 loss: -0.2354406714439392
Batch 15/64 loss: -0.24902477860450745
Batch 16/64 loss: -0.2608928978443146
Batch 17/64 loss: -0.21495968103408813
Batch 18/64 loss: -0.24488484859466553
Batch 19/64 loss: -0.2556982934474945
Batch 20/64 loss: -0.2357393503189087
Batch 21/64 loss: -0.216741144657135
Batch 22/64 loss: -0.2375703752040863
Batch 23/64 loss: -0.22198963165283203
Batch 24/64 loss: -0.25965195894241333
Batch 25/64 loss: -0.2430059313774109
Batch 26/64 loss: -0.22162353992462158
Batch 27/64 loss: -0.2502428889274597
Batch 28/64 loss: -0.25906234979629517
Batch 29/64 loss: -0.25367578864097595
Batch 30/64 loss: -0.26707395911216736
Batch 31/64 loss: -0.25037190318107605
Batch 32/64 loss: -0.23381221294403076
Batch 33/64 loss: -0.24247953295707703
Batch 34/64 loss: -0.2307133674621582
Batch 35/64 loss: -0.24251526594161987
Batch 36/64 loss: -0.24596846103668213
Batch 37/64 loss: -0.24148651957511902
Batch 38/64 loss: -0.2546154856681824
Batch 39/64 loss: -0.2532236576080322
Batch 40/64 loss: -0.22773900628089905
Batch 41/64 loss: -0.22260260581970215
Batch 42/64 loss: -0.2623586058616638
Batch 43/64 loss: -0.219038724899292
Batch 44/64 loss: -0.26988962292671204
Batch 45/64 loss: -0.24724668264389038
Batch 46/64 loss: -0.24700576066970825
Batch 47/64 loss: -0.24899324774742126
Batch 48/64 loss: -0.25665283203125
Batch 49/64 loss: -0.25086402893066406
Batch 50/64 loss: -0.24853461980819702
Batch 51/64 loss: -0.25452515482902527
Batch 52/64 loss: -0.2593976557254791
Batch 53/64 loss: -0.23240134119987488
Batch 54/64 loss: -0.23637458682060242
Batch 55/64 loss: -0.2282017469406128
Batch 56/64 loss: -0.24146544933319092
Batch 57/64 loss: -0.2663167119026184
Batch 58/64 loss: -0.25813645124435425
Batch 59/64 loss: -0.2497050166130066
Batch 60/64 loss: -0.24519574642181396
Batch 61/64 loss: -0.25535115599632263
Batch 62/64 loss: -0.2596321403980255
Batch 63/64 loss: -0.24992352724075317
Batch 64/64 loss: -0.2541748285293579
Epoch 462  Train loss: -0.24566409073623957  Val loss: -0.03616999903905023
Epoch 463
-------------------------------
Batch 1/64 loss: -0.26677143573760986
Batch 2/64 loss: -0.2593885064125061
Batch 3/64 loss: -0.2658439874649048
Batch 4/64 loss: -0.24883896112442017
Batch 5/64 loss: -0.2629370391368866
Batch 6/64 loss: -0.2562049627304077
Batch 7/64 loss: -0.24200376868247986
Batch 8/64 loss: -0.2885420322418213
Batch 9/64 loss: -0.2529047131538391
Batch 10/64 loss: -0.27338558435440063
Batch 11/64 loss: -0.25450384616851807
Batch 12/64 loss: -0.2386952042579651
Batch 13/64 loss: -0.27213311195373535
Batch 14/64 loss: -0.26309287548065186
Batch 15/64 loss: -0.2567952871322632
Batch 16/64 loss: -0.273647278547287
Batch 17/64 loss: -0.20968973636627197
Batch 18/64 loss: -0.2495952546596527
Batch 19/64 loss: -0.23566031455993652
Batch 20/64 loss: -0.23313528299331665
Batch 21/64 loss: -0.28365185856819153
Batch 22/64 loss: -0.2639786899089813
Batch 23/64 loss: -0.24983245134353638
Batch 24/64 loss: -0.20933592319488525
Batch 25/64 loss: -0.24947205185890198
Batch 26/64 loss: -0.2199588418006897
Batch 27/64 loss: -0.2418929934501648
Batch 28/64 loss: -0.23092937469482422
Batch 29/64 loss: -0.2560845911502838
Batch 30/64 loss: -0.2411106824874878
Batch 31/64 loss: -0.25976160168647766
Batch 32/64 loss: -0.19452208280563354
Batch 33/64 loss: -0.24978500604629517
Batch 34/64 loss: -0.2396799623966217
Batch 35/64 loss: -0.2576085925102234
Batch 36/64 loss: -0.26010698080062866
Batch 37/64 loss: -0.23320084810256958
Batch 38/64 loss: -0.23715481162071228
Batch 39/64 loss: -0.2586146593093872
Batch 40/64 loss: -0.2353394627571106
Batch 41/64 loss: -0.24672284722328186
Batch 42/64 loss: -0.2471073865890503
Batch 43/64 loss: -0.22393277287483215
Batch 44/64 loss: -0.21185964345932007
Batch 45/64 loss: -0.2312099039554596
Batch 46/64 loss: -0.25226014852523804
Batch 47/64 loss: -0.25677770376205444
Batch 48/64 loss: -0.2740206718444824
Batch 49/64 loss: -0.26960158348083496
Batch 50/64 loss: -0.262809693813324
Batch 51/64 loss: -0.21763300895690918
Batch 52/64 loss: -0.21058747172355652
Batch 53/64 loss: -0.2317591905593872
Batch 54/64 loss: -0.2367466390132904
Batch 55/64 loss: -0.28014281392097473
Batch 56/64 loss: -0.2427358627319336
Batch 57/64 loss: -0.23224255442619324
Batch 58/64 loss: -0.2505667805671692
Batch 59/64 loss: -0.27395570278167725
Batch 60/64 loss: -0.2713581919670105
Batch 61/64 loss: -0.2400689423084259
Batch 62/64 loss: -0.2465987205505371
Batch 63/64 loss: -0.2380027174949646
Batch 64/64 loss: -0.23953354358673096
Epoch 463  Train loss: -0.2479081060372147  Val loss: -0.03325700677956912
Epoch 464
-------------------------------
Batch 1/64 loss: -0.2589167356491089
Batch 2/64 loss: -0.24459314346313477
Batch 3/64 loss: -0.2851221561431885
Batch 4/64 loss: -0.26677459478378296
Batch 5/64 loss: -0.23585838079452515
Batch 6/64 loss: -0.23928552865982056
Batch 7/64 loss: -0.26734501123428345
Batch 8/64 loss: -0.260358989238739
Batch 9/64 loss: -0.2587955594062805
Batch 10/64 loss: -0.2534843385219574
Batch 11/64 loss: -0.2613387703895569
Batch 12/64 loss: -0.24394899606704712
Batch 13/64 loss: -0.2453095018863678
Batch 14/64 loss: -0.26734521985054016
Batch 15/64 loss: -0.2502251863479614
Batch 16/64 loss: -0.25956612825393677
Batch 17/64 loss: -0.23771488666534424
Batch 18/64 loss: -0.2604144215583801
Batch 19/64 loss: -0.22444576025009155
Batch 20/64 loss: -0.2443305253982544
Batch 21/64 loss: -0.26750296354293823
Batch 22/64 loss: -0.26669323444366455
Batch 23/64 loss: -0.2539774179458618
Batch 24/64 loss: -0.22431856393814087
Batch 25/64 loss: -0.2685006856918335
Batch 26/64 loss: -0.24770471453666687
Batch 27/64 loss: -0.246273934841156
Batch 28/64 loss: -0.25786104798316956
Batch 29/64 loss: -0.23729193210601807
Batch 30/64 loss: -0.2489900290966034
Batch 31/64 loss: -0.2527815103530884
Batch 32/64 loss: -0.2654860019683838
Batch 33/64 loss: -0.2702264189720154
Batch 34/64 loss: -0.2423287332057953
Batch 35/64 loss: -0.2438453733921051
Batch 36/64 loss: -0.26192474365234375
Batch 37/64 loss: -0.24741387367248535
Batch 38/64 loss: -0.24619051814079285
Batch 39/64 loss: -0.2573731541633606
Batch 40/64 loss: -0.23982954025268555
Batch 41/64 loss: -0.2529456317424774
Batch 42/64 loss: -0.24863946437835693
Batch 43/64 loss: -0.2538902759552002
Batch 44/64 loss: -0.24633535742759705
Batch 45/64 loss: -0.23560088872909546
Batch 46/64 loss: -0.26041510701179504
Batch 47/64 loss: -0.26529237627983093
Batch 48/64 loss: -0.2515856921672821
Batch 49/64 loss: -0.25600069761276245
Batch 50/64 loss: -0.2591453790664673
Batch 51/64 loss: -0.23206865787506104
Batch 52/64 loss: -0.2366052269935608
Batch 53/64 loss: -0.2747417092323303
Batch 54/64 loss: -0.2710639238357544
Batch 55/64 loss: -0.27788788080215454
Batch 56/64 loss: -0.25372186303138733
Batch 57/64 loss: -0.23703411221504211
Batch 58/64 loss: -0.2514922022819519
Batch 59/64 loss: -0.25293537974357605
Batch 60/64 loss: -0.24011239409446716
Batch 61/64 loss: -0.2397136092185974
Batch 62/64 loss: -0.23198223114013672
Batch 63/64 loss: -0.2178589105606079
Batch 64/64 loss: -0.25089943408966064
Epoch 464  Train loss: -0.25171657683802584  Val loss: -0.032065931054734695
Epoch 465
-------------------------------
Batch 1/64 loss: -0.25899893045425415
Batch 2/64 loss: -0.2672499418258667
Batch 3/64 loss: -0.26835793256759644
Batch 4/64 loss: -0.2616588771343231
Batch 5/64 loss: -0.22867047786712646
Batch 6/64 loss: -0.24414366483688354
Batch 7/64 loss: -0.24654972553253174
Batch 8/64 loss: -0.24103614687919617
Batch 9/64 loss: -0.26927220821380615
Batch 10/64 loss: -0.2662712037563324
Batch 11/64 loss: -0.24423247575759888
Batch 12/64 loss: -0.23244941234588623
Batch 13/64 loss: -0.26766103506088257
Batch 14/64 loss: -0.25302302837371826
Batch 15/64 loss: -0.24360889196395874
Batch 16/64 loss: -0.264416366815567
Batch 17/64 loss: -0.25578057765960693
Batch 18/64 loss: -0.2554031014442444
Batch 19/64 loss: -0.21992075443267822
Batch 20/64 loss: -0.2664983868598938
Batch 21/64 loss: -0.24144500494003296
Batch 22/64 loss: -0.24009978771209717
Batch 23/64 loss: -0.24359643459320068
Batch 24/64 loss: -0.21220585703849792
Batch 25/64 loss: -0.2695782780647278
Batch 26/64 loss: -0.2741173505783081
Batch 27/64 loss: -0.2651774287223816
Batch 28/64 loss: -0.2762225270271301
Batch 29/64 loss: -0.22091558575630188
Batch 30/64 loss: -0.2570316791534424
Batch 31/64 loss: -0.2010844349861145
Batch 32/64 loss: -0.24836689233779907
Batch 33/64 loss: -0.25932765007019043
Batch 34/64 loss: -0.26297101378440857
Batch 35/64 loss: -0.2575184106826782
Batch 36/64 loss: -0.24705171585083008
Batch 37/64 loss: -0.25311699509620667
Batch 38/64 loss: -0.2688259482383728
Batch 39/64 loss: -0.24867397546768188
Batch 40/64 loss: -0.2214035987854004
Batch 41/64 loss: -0.22873437404632568
Batch 42/64 loss: -0.2536943554878235
Batch 43/64 loss: -0.2665855288505554
Batch 44/64 loss: -0.23865723609924316
Batch 45/64 loss: -0.2552978992462158
Batch 46/64 loss: -0.24311673641204834
Batch 47/64 loss: -0.2597988247871399
Batch 48/64 loss: -0.24422228336334229
Batch 49/64 loss: -0.24557289481163025
Batch 50/64 loss: -0.24926233291625977
Batch 51/64 loss: -0.2420785129070282
Batch 52/64 loss: -0.2500211000442505
Batch 53/64 loss: -0.23286360502243042
Batch 54/64 loss: -0.2629905045032501
Batch 55/64 loss: -0.2683340311050415
Batch 56/64 loss: -0.20331722497940063
Batch 57/64 loss: -0.26819390058517456
Batch 58/64 loss: -0.2422611117362976
Batch 59/64 loss: -0.27155840396881104
Batch 60/64 loss: -0.25855472683906555
Batch 61/64 loss: -0.23945459723472595
Batch 62/64 loss: -0.24986344575881958
Batch 63/64 loss: -0.2703079581260681
Batch 64/64 loss: -0.21867889165878296
Epoch 465  Train loss: -0.24992446221557318  Val loss: -0.03462972550867349
Epoch 466
-------------------------------
Batch 1/64 loss: -0.2476367950439453
Batch 2/64 loss: -0.2518351078033447
Batch 3/64 loss: -0.2687234580516815
Batch 4/64 loss: -0.2481311559677124
Batch 5/64 loss: -0.2650739550590515
Batch 6/64 loss: -0.258789986371994
Batch 7/64 loss: -0.26183319091796875
Batch 8/64 loss: -0.20749163627624512
Batch 9/64 loss: -0.26622286438941956
Batch 10/64 loss: -0.25835883617401123
Batch 11/64 loss: -0.23744428157806396
Batch 12/64 loss: -0.2458450198173523
Batch 13/64 loss: -0.2504352033138275
Batch 14/64 loss: -0.269580602645874
Batch 15/64 loss: -0.24612730741500854
Batch 16/64 loss: -0.2513416111469269
Batch 17/64 loss: -0.2500518560409546
Batch 18/64 loss: -0.24427950382232666
Batch 19/64 loss: -0.24054056406021118
Batch 20/64 loss: -0.21953323483467102
Batch 21/64 loss: -0.23456668853759766
Batch 22/64 loss: -0.2592865228652954
Batch 23/64 loss: -0.2559654712677002
Batch 24/64 loss: -0.27075815200805664
Batch 25/64 loss: -0.22876355051994324
Batch 26/64 loss: -0.2607208490371704
Batch 27/64 loss: -0.22795647382736206
Batch 28/64 loss: -0.2570388913154602
Batch 29/64 loss: -0.23913466930389404
Batch 30/64 loss: -0.2624608874320984
Batch 31/64 loss: -0.2585139870643616
Batch 32/64 loss: -0.27174365520477295
Batch 33/64 loss: -0.2682589888572693
Batch 34/64 loss: -0.2675705850124359
Batch 35/64 loss: -0.24591940641403198
Batch 36/64 loss: -0.24586716294288635
Batch 37/64 loss: -0.2494363784790039
Batch 38/64 loss: -0.24346953630447388
Batch 39/64 loss: -0.2529637813568115
Batch 40/64 loss: -0.2709861993789673
Batch 41/64 loss: -0.2478734254837036
Batch 42/64 loss: -0.241542786359787
Batch 43/64 loss: -0.2702937722206116
Batch 44/64 loss: -0.2501843273639679
Batch 45/64 loss: -0.23775649070739746
Batch 46/64 loss: -0.2580353021621704
Batch 47/64 loss: -0.2316136360168457
Batch 48/64 loss: -0.26617637276649475
Batch 49/64 loss: -0.26209843158721924
Batch 50/64 loss: -0.2505647540092468
Batch 51/64 loss: -0.21462887525558472
Batch 52/64 loss: -0.2674540877342224
Batch 53/64 loss: -0.2545454502105713
Batch 54/64 loss: -0.24222055077552795
Batch 55/64 loss: -0.26263201236724854
Batch 56/64 loss: -0.25796055793762207
Batch 57/64 loss: -0.26881957054138184
Batch 58/64 loss: -0.2578595280647278
Batch 59/64 loss: -0.23804453015327454
Batch 60/64 loss: -0.24901533126831055
Batch 61/64 loss: -0.27980130910873413
Batch 62/64 loss: -0.2341752052307129
Batch 63/64 loss: -0.25165969133377075
Batch 64/64 loss: -0.24472570419311523
Epoch 466  Train loss: -0.2515946397594377  Val loss: -0.03364607087525306
Epoch 467
-------------------------------
Batch 1/64 loss: -0.24769139289855957
Batch 2/64 loss: -0.2569272518157959
Batch 3/64 loss: -0.24991995096206665
Batch 4/64 loss: -0.2705322504043579
Batch 5/64 loss: -0.27092647552490234
Batch 6/64 loss: -0.2744028568267822
Batch 7/64 loss: -0.24823755025863647
Batch 8/64 loss: -0.2606601417064667
Batch 9/64 loss: -0.2670517563819885
Batch 10/64 loss: -0.24925851821899414
Batch 11/64 loss: -0.2546936273574829
Batch 12/64 loss: -0.2446696162223816
Batch 13/64 loss: -0.2682948708534241
Batch 14/64 loss: -0.26506540179252625
Batch 15/64 loss: -0.2158844769001007
Batch 16/64 loss: -0.24671372771263123
Batch 17/64 loss: -0.2530580759048462
Batch 18/64 loss: -0.25179171562194824
Batch 19/64 loss: -0.2372288703918457
Batch 20/64 loss: -0.21199142932891846
Batch 21/64 loss: -0.27169081568717957
Batch 22/64 loss: -0.25007304549217224
Batch 23/64 loss: -0.26890984177589417
Batch 24/64 loss: -0.23165905475616455
Batch 25/64 loss: -0.21934890747070312
Batch 26/64 loss: -0.2606443464756012
Batch 27/64 loss: -0.26050978899002075
Batch 28/64 loss: -0.2498367428779602
Batch 29/64 loss: -0.2508198022842407
Batch 30/64 loss: -0.23536312580108643
Batch 31/64 loss: -0.22617363929748535
Batch 32/64 loss: -0.2247624695301056
Batch 33/64 loss: -0.26447296142578125
Batch 34/64 loss: -0.24798214435577393
Batch 35/64 loss: -0.23964953422546387
Batch 36/64 loss: -0.23516970872879028
Batch 37/64 loss: -0.24418997764587402
Batch 38/64 loss: -0.24487531185150146
Batch 39/64 loss: -0.23730432987213135
Batch 40/64 loss: -0.23937365412712097
Batch 41/64 loss: -0.25602954626083374
Batch 42/64 loss: -0.24739551544189453
Batch 43/64 loss: -0.24313297867774963
Batch 44/64 loss: -0.25381651520729065
Batch 45/64 loss: -0.24398449063301086
Batch 46/64 loss: -0.21379774808883667
Batch 47/64 loss: -0.23616540431976318
Batch 48/64 loss: -0.24781858921051025
Batch 49/64 loss: -0.2777577042579651
Batch 50/64 loss: -0.2733639180660248
Batch 51/64 loss: -0.25798866152763367
Batch 52/64 loss: -0.2438797950744629
Batch 53/64 loss: -0.2513476610183716
Batch 54/64 loss: -0.2692399322986603
Batch 55/64 loss: -0.2631346881389618
Batch 56/64 loss: -0.24759480357170105
Batch 57/64 loss: -0.25755196809768677
Batch 58/64 loss: -0.24429434537887573
Batch 59/64 loss: -0.24070006608963013
Batch 60/64 loss: -0.23938289284706116
Batch 61/64 loss: -0.225763738155365
Batch 62/64 loss: -0.2502427101135254
Batch 63/64 loss: -0.24271172285079956
Batch 64/64 loss: -0.25887492299079895
Epoch 467  Train loss: -0.2489264430953007  Val loss: -0.03220252749026846
Epoch 468
-------------------------------
Batch 1/64 loss: -0.274862140417099
Batch 2/64 loss: -0.27203500270843506
Batch 3/64 loss: -0.26566004753112793
Batch 4/64 loss: -0.2021484375
Batch 5/64 loss: -0.27861282229423523
Batch 6/64 loss: -0.248719722032547
Batch 7/64 loss: -0.2643199563026428
Batch 8/64 loss: -0.25712350010871887
Batch 9/64 loss: -0.27647721767425537
Batch 10/64 loss: -0.2681577801704407
Batch 11/64 loss: -0.2592311501502991
Batch 12/64 loss: -0.26586973667144775
Batch 13/64 loss: -0.2535509467124939
Batch 14/64 loss: -0.2620563507080078
Batch 15/64 loss: -0.24612745642662048
Batch 16/64 loss: -0.22602641582489014
Batch 17/64 loss: -0.25552067160606384
Batch 18/64 loss: -0.2528773248195648
Batch 19/64 loss: -0.24667656421661377
Batch 20/64 loss: -0.2631065249443054
Batch 21/64 loss: -0.2609086334705353
Batch 22/64 loss: -0.24113714694976807
Batch 23/64 loss: -0.25949519872665405
Batch 24/64 loss: -0.276504784822464
Batch 25/64 loss: -0.25681254267692566
Batch 26/64 loss: -0.24446243047714233
Batch 27/64 loss: -0.2662755250930786
Batch 28/64 loss: -0.2696956992149353
Batch 29/64 loss: -0.2622869610786438
Batch 30/64 loss: -0.25987064838409424
Batch 31/64 loss: -0.2807106375694275
Batch 32/64 loss: -0.26212477684020996
Batch 33/64 loss: -0.25872090458869934
Batch 34/64 loss: -0.2385258674621582
Batch 35/64 loss: -0.2511652410030365
Batch 36/64 loss: -0.24930298328399658
Batch 37/64 loss: -0.2520766258239746
Batch 38/64 loss: -0.2527463436126709
Batch 39/64 loss: -0.2453288435935974
Batch 40/64 loss: -0.2543628215789795
Batch 41/64 loss: -0.2718692719936371
Batch 42/64 loss: -0.262484073638916
Batch 43/64 loss: -0.23795947432518005
Batch 44/64 loss: -0.2625390887260437
Batch 45/64 loss: -0.2705073654651642
Batch 46/64 loss: -0.24430948495864868
Batch 47/64 loss: -0.250746488571167
Batch 48/64 loss: -0.27028191089630127
Batch 49/64 loss: -0.20865771174430847
Batch 50/64 loss: -0.2589331269264221
Batch 51/64 loss: -0.24961519241333008
Batch 52/64 loss: -0.24051815271377563
Batch 53/64 loss: -0.27113351225852966
Batch 54/64 loss: -0.25136715173721313
Batch 55/64 loss: -0.21249204874038696
Batch 56/64 loss: -0.2551124095916748
Batch 57/64 loss: -0.2401823103427887
Batch 58/64 loss: -0.25163519382476807
Batch 59/64 loss: -0.23064732551574707
Batch 60/64 loss: -0.23643112182617188
Batch 61/64 loss: -0.22855019569396973
Batch 62/64 loss: -0.24231156706809998
Batch 63/64 loss: -0.2603493928909302
Batch 64/64 loss: -0.2165166139602661
Epoch 468  Train loss: -0.2533756143906537  Val loss: -0.03323406083477322
Epoch 469
-------------------------------
Batch 1/64 loss: -0.26725804805755615
Batch 2/64 loss: -0.2578606903553009
Batch 3/64 loss: -0.2344214916229248
Batch 4/64 loss: -0.23739051818847656
Batch 5/64 loss: -0.261216938495636
Batch 6/64 loss: -0.24569061398506165
Batch 7/64 loss: -0.28777387738227844
Batch 8/64 loss: -0.27478963136672974
Batch 9/64 loss: -0.2714036703109741
Batch 10/64 loss: -0.2327370047569275
Batch 11/64 loss: -0.22629126906394958
Batch 12/64 loss: -0.27357202768325806
Batch 13/64 loss: -0.25162434577941895
Batch 14/64 loss: -0.25088533759117126
Batch 15/64 loss: -0.26354506611824036
Batch 16/64 loss: -0.25730156898498535
Batch 17/64 loss: -0.26269084215164185
Batch 18/64 loss: -0.25320300459861755
Batch 19/64 loss: -0.23021021485328674
Batch 20/64 loss: -0.23839601874351501
Batch 21/64 loss: -0.2485082745552063
Batch 22/64 loss: -0.23773974180221558
Batch 23/64 loss: -0.26237204670906067
Batch 24/64 loss: -0.2561899721622467
Batch 25/64 loss: -0.250855028629303
Batch 26/64 loss: -0.24039381742477417
Batch 27/64 loss: -0.2804614305496216
Batch 28/64 loss: -0.24946829676628113
Batch 29/64 loss: -0.20179450511932373
Batch 30/64 loss: -0.25007885694503784
Batch 31/64 loss: -0.2589593529701233
Batch 32/64 loss: -0.2627957761287689
Batch 33/64 loss: -0.24046272039413452
Batch 34/64 loss: -0.263583779335022
Batch 35/64 loss: -0.2527141273021698
Batch 36/64 loss: -0.23076191544532776
Batch 37/64 loss: -0.2592017650604248
Batch 38/64 loss: -0.2443377673625946
Batch 39/64 loss: -0.24729019403457642
Batch 40/64 loss: -0.2589770555496216
Batch 41/64 loss: -0.26511624455451965
Batch 42/64 loss: -0.27311283349990845
Batch 43/64 loss: -0.211579829454422
Batch 44/64 loss: -0.26841896772384644
Batch 45/64 loss: -0.2443612813949585
Batch 46/64 loss: -0.2590256929397583
Batch 47/64 loss: -0.2636815011501312
Batch 48/64 loss: -0.27058297395706177
Batch 49/64 loss: -0.2676500380039215
Batch 50/64 loss: -0.2724533975124359
Batch 51/64 loss: -0.2668062448501587
Batch 52/64 loss: -0.23540782928466797
Batch 53/64 loss: -0.24831408262252808
Batch 54/64 loss: -0.26447463035583496
Batch 55/64 loss: -0.2404766082763672
Batch 56/64 loss: -0.20758259296417236
Batch 57/64 loss: -0.21855497360229492
Batch 58/64 loss: -0.2607596218585968
Batch 59/64 loss: -0.2585212290287018
Batch 60/64 loss: -0.26727086305618286
Batch 61/64 loss: -0.24168053269386292
Batch 62/64 loss: -0.2622361183166504
Batch 63/64 loss: -0.24996083974838257
Batch 64/64 loss: -0.2455180585384369
Epoch 469  Train loss: -0.25219413451119965  Val loss: -0.03638811373628702
Epoch 470
-------------------------------
Batch 1/64 loss: -0.24967455863952637
Batch 2/64 loss: -0.26547229290008545
Batch 3/64 loss: -0.23999351263046265
Batch 4/64 loss: -0.26753926277160645
Batch 5/64 loss: -0.26560378074645996
Batch 6/64 loss: -0.23603558540344238
Batch 7/64 loss: -0.25390008091926575
Batch 8/64 loss: -0.22974348068237305
Batch 9/64 loss: -0.23120060563087463
Batch 10/64 loss: -0.2431843876838684
Batch 11/64 loss: -0.2513221502304077
Batch 12/64 loss: -0.24860763549804688
Batch 13/64 loss: -0.25445765256881714
Batch 14/64 loss: -0.2583363652229309
Batch 15/64 loss: -0.2523472309112549
Batch 16/64 loss: -0.2617945671081543
Batch 17/64 loss: -0.25861656665802
Batch 18/64 loss: -0.25086718797683716
Batch 19/64 loss: -0.2431773841381073
Batch 20/64 loss: -0.2651451826095581
Batch 21/64 loss: -0.22823920845985413
Batch 22/64 loss: -0.24185848236083984
Batch 23/64 loss: -0.24985551834106445
Batch 24/64 loss: -0.2228773832321167
Batch 25/64 loss: -0.22418659925460815
Batch 26/64 loss: -0.2465224266052246
Batch 27/64 loss: -0.25648438930511475
Batch 28/64 loss: -0.2160143256187439
Batch 29/64 loss: -0.2766459584236145
Batch 30/64 loss: -0.23859363794326782
Batch 31/64 loss: -0.2577441334724426
Batch 32/64 loss: -0.2200634479522705
Batch 33/64 loss: -0.26362937688827515
Batch 34/64 loss: -0.2538982033729553
Batch 35/64 loss: -0.22736182808876038
Batch 36/64 loss: -0.2375895380973816
Batch 37/64 loss: -0.24070456624031067
Batch 38/64 loss: -0.24171680212020874
Batch 39/64 loss: -0.2340531051158905
Batch 40/64 loss: -0.25195983052253723
Batch 41/64 loss: -0.26693224906921387
Batch 42/64 loss: -0.2535214126110077
Batch 43/64 loss: -0.23692327737808228
Batch 44/64 loss: -0.26180341839790344
Batch 45/64 loss: -0.2330995500087738
Batch 46/64 loss: -0.24815815687179565
Batch 47/64 loss: -0.26678213477134705
Batch 48/64 loss: -0.2614080309867859
Batch 49/64 loss: -0.26334136724472046
Batch 50/64 loss: -0.23818975687026978
Batch 51/64 loss: -0.21798521280288696
Batch 52/64 loss: -0.25090014934539795
Batch 53/64 loss: -0.215354323387146
Batch 54/64 loss: -0.24898585677146912
Batch 55/64 loss: -0.2635236084461212
Batch 56/64 loss: -0.2557893693447113
Batch 57/64 loss: -0.2593860626220703
Batch 58/64 loss: -0.22196891903877258
Batch 59/64 loss: -0.23805862665176392
Batch 60/64 loss: -0.24541369080543518
Batch 61/64 loss: -0.26467278599739075
Batch 62/64 loss: -0.25677117705345154
Batch 63/64 loss: -0.23981520533561707
Batch 64/64 loss: -0.22047019004821777
Epoch 470  Train loss: -0.24676321908539417  Val loss: -0.03284837064874131
Epoch 471
-------------------------------
Batch 1/64 loss: -0.2684140205383301
Batch 2/64 loss: -0.24734988808631897
Batch 3/64 loss: -0.2637678384780884
Batch 4/64 loss: -0.2663470208644867
Batch 5/64 loss: -0.22837108373641968
Batch 6/64 loss: -0.24425682425498962
Batch 7/64 loss: -0.25879499316215515
Batch 8/64 loss: -0.26096540689468384
Batch 9/64 loss: -0.2282964587211609
Batch 10/64 loss: -0.24097448587417603
Batch 11/64 loss: -0.2406494915485382
Batch 12/64 loss: -0.2581610381603241
Batch 13/64 loss: -0.28069955110549927
Batch 14/64 loss: -0.2640751004219055
Batch 15/64 loss: -0.2486293613910675
Batch 16/64 loss: -0.24600839614868164
Batch 17/64 loss: -0.27254539728164673
Batch 18/64 loss: -0.2678431272506714
Batch 19/64 loss: -0.2529245615005493
Batch 20/64 loss: -0.20853400230407715
Batch 21/64 loss: -0.23474159836769104
Batch 22/64 loss: -0.2611699104309082
Batch 23/64 loss: -0.2406875193119049
Batch 24/64 loss: -0.24130529165267944
Batch 25/64 loss: -0.264462411403656
Batch 26/64 loss: -0.2665380537509918
Batch 27/64 loss: -0.26626548171043396
Batch 28/64 loss: -0.28643128275871277
Batch 29/64 loss: -0.2540512681007385
Batch 30/64 loss: -0.24511954188346863
Batch 31/64 loss: -0.26362407207489014
Batch 32/64 loss: -0.23649758100509644
Batch 33/64 loss: -0.26716530323028564
Batch 34/64 loss: -0.27113160490989685
Batch 35/64 loss: -0.2674473524093628
Batch 36/64 loss: -0.264570415019989
Batch 37/64 loss: -0.2740013897418976
Batch 38/64 loss: -0.2527259588241577
Batch 39/64 loss: -0.22990721464157104
Batch 40/64 loss: -0.2536989450454712
Batch 41/64 loss: -0.227454274892807
Batch 42/64 loss: -0.24184834957122803
Batch 43/64 loss: -0.1985127329826355
Batch 44/64 loss: -0.250541090965271
Batch 45/64 loss: -0.24996614456176758
Batch 46/64 loss: -0.250240296125412
Batch 47/64 loss: -0.2392287254333496
Batch 48/64 loss: -0.27149224281311035
Batch 49/64 loss: -0.2626485228538513
Batch 50/64 loss: -0.26090601086616516
Batch 51/64 loss: -0.24608319997787476
Batch 52/64 loss: -0.23204731941223145
Batch 53/64 loss: -0.22205790877342224
Batch 54/64 loss: -0.2380240559577942
Batch 55/64 loss: -0.24814856052398682
Batch 56/64 loss: -0.25748229026794434
Batch 57/64 loss: -0.2558021545410156
Batch 58/64 loss: -0.2534984052181244
Batch 59/64 loss: -0.23686271905899048
Batch 60/64 loss: -0.24876999855041504
Batch 61/64 loss: -0.2579759359359741
Batch 62/64 loss: -0.21624791622161865
Batch 63/64 loss: -0.25571659207344055
Batch 64/64 loss: -0.2313990592956543
Epoch 471  Train loss: -0.250733411545847  Val loss: -0.031942209017645454
Epoch 472
-------------------------------
Batch 1/64 loss: -0.22014880180358887
Batch 2/64 loss: -0.24487733840942383
Batch 3/64 loss: -0.25002217292785645
Batch 4/64 loss: -0.23186498880386353
Batch 5/64 loss: -0.2615366578102112
Batch 6/64 loss: -0.2660894989967346
Batch 7/64 loss: -0.277113676071167
Batch 8/64 loss: -0.24517548084259033
Batch 9/64 loss: -0.2797204256057739
Batch 10/64 loss: -0.27380597591400146
Batch 11/64 loss: -0.24702000617980957
Batch 12/64 loss: -0.24531817436218262
Batch 13/64 loss: -0.2610369324684143
Batch 14/64 loss: -0.2327098548412323
Batch 15/64 loss: -0.2636576294898987
Batch 16/64 loss: -0.260917067527771
Batch 17/64 loss: -0.2696179151535034
Batch 18/64 loss: -0.26682981848716736
Batch 19/64 loss: -0.24458694458007812
Batch 20/64 loss: -0.2791336178779602
Batch 21/64 loss: -0.2676008641719818
Batch 22/64 loss: -0.25337710976600647
Batch 23/64 loss: -0.25748902559280396
Batch 24/64 loss: -0.27973851561546326
Batch 25/64 loss: -0.24276891350746155
Batch 26/64 loss: -0.2417718768119812
Batch 27/64 loss: -0.26337140798568726
Batch 28/64 loss: -0.27086693048477173
Batch 29/64 loss: -0.23868808150291443
Batch 30/64 loss: -0.25526848435401917
Batch 31/64 loss: -0.2304350733757019
Batch 32/64 loss: -0.2571832537651062
Batch 33/64 loss: -0.257850706577301
Batch 34/64 loss: -0.2610659897327423
Batch 35/64 loss: -0.26761603355407715
Batch 36/64 loss: -0.24983233213424683
Batch 37/64 loss: -0.2710433602333069
Batch 38/64 loss: -0.23412591218948364
Batch 39/64 loss: -0.25140127539634705
Batch 40/64 loss: -0.26282238960266113
Batch 41/64 loss: -0.2573511004447937
Batch 42/64 loss: -0.27993667125701904
Batch 43/64 loss: -0.22975528240203857
Batch 44/64 loss: -0.25589028000831604
Batch 45/64 loss: -0.25794658064842224
Batch 46/64 loss: -0.27304211258888245
Batch 47/64 loss: -0.24925684928894043
Batch 48/64 loss: -0.26120227575302124
Batch 49/64 loss: -0.24101486802101135
Batch 50/64 loss: -0.22440141439437866
Batch 51/64 loss: -0.2598549425601959
Batch 52/64 loss: -0.27192947268486023
Batch 53/64 loss: -0.2679925560951233
Batch 54/64 loss: -0.2436193823814392
Batch 55/64 loss: -0.26038363575935364
Batch 56/64 loss: -0.25670695304870605
Batch 57/64 loss: -0.2688324749469757
Batch 58/64 loss: -0.25065189599990845
Batch 59/64 loss: -0.27478137612342834
Batch 60/64 loss: -0.2463262379169464
Batch 61/64 loss: -0.26267242431640625
Batch 62/64 loss: -0.24575981497764587
Batch 63/64 loss: -0.23404961824417114
Batch 64/64 loss: -0.2487136721611023
Epoch 472  Train loss: -0.2556140233488644  Val loss: -0.02866919917339312
Epoch 473
-------------------------------
Batch 1/64 loss: -0.251991868019104
Batch 2/64 loss: -0.2751643657684326
Batch 3/64 loss: -0.25719550251960754
Batch 4/64 loss: -0.2729792296886444
Batch 5/64 loss: -0.2685346305370331
Batch 6/64 loss: -0.26150208711624146
Batch 7/64 loss: -0.22484654188156128
Batch 8/64 loss: -0.2752390205860138
Batch 9/64 loss: -0.271422415971756
Batch 10/64 loss: -0.240940660238266
Batch 11/64 loss: -0.25500720739364624
Batch 12/64 loss: -0.28419119119644165
Batch 13/64 loss: -0.23445618152618408
Batch 14/64 loss: -0.23983606696128845
Batch 15/64 loss: -0.2720721364021301
Batch 16/64 loss: -0.24152231216430664
Batch 17/64 loss: -0.2685335576534271
Batch 18/64 loss: -0.23215174674987793
Batch 19/64 loss: -0.24898260831832886
Batch 20/64 loss: -0.24159836769104004
Batch 21/64 loss: -0.24903357028961182
Batch 22/64 loss: -0.26050108671188354
Batch 23/64 loss: -0.20101279020309448
Batch 24/64 loss: -0.2699930667877197
Batch 25/64 loss: -0.26145732402801514
Batch 26/64 loss: -0.23847708106040955
Batch 27/64 loss: -0.2626911699771881
Batch 28/64 loss: -0.24904370307922363
Batch 29/64 loss: -0.23442158102989197
Batch 30/64 loss: -0.25258225202560425
Batch 31/64 loss: -0.24317556619644165
Batch 32/64 loss: -0.2690669000148773
Batch 33/64 loss: -0.2594098448753357
Batch 34/64 loss: -0.2554785907268524
Batch 35/64 loss: -0.26890820264816284
Batch 36/64 loss: -0.2512468993663788
Batch 37/64 loss: -0.24670496582984924
Batch 38/64 loss: -0.26383209228515625
Batch 39/64 loss: -0.22854167222976685
Batch 40/64 loss: -0.20501011610031128
Batch 41/64 loss: -0.2710261046886444
Batch 42/64 loss: -0.23502445220947266
Batch 43/64 loss: -0.2418956756591797
Batch 44/64 loss: -0.2414916753768921
Batch 45/64 loss: -0.2570880651473999
Batch 46/64 loss: -0.22616204619407654
Batch 47/64 loss: -0.25033608078956604
Batch 48/64 loss: -0.25444769859313965
Batch 49/64 loss: -0.25439023971557617
Batch 50/64 loss: -0.2667517364025116
Batch 51/64 loss: -0.2576889395713806
Batch 52/64 loss: -0.2436516284942627
Batch 53/64 loss: -0.24538540840148926
Batch 54/64 loss: -0.2685640752315521
Batch 55/64 loss: -0.2753617465496063
Batch 56/64 loss: -0.2607327997684479
Batch 57/64 loss: -0.22644490003585815
Batch 58/64 loss: -0.25979459285736084
Batch 59/64 loss: -0.27423107624053955
Batch 60/64 loss: -0.2542255222797394
Batch 61/64 loss: -0.2468726634979248
Batch 62/64 loss: -0.23962104320526123
Batch 63/64 loss: -0.24541068077087402
Batch 64/64 loss: -0.2514820694923401
Epoch 473  Train loss: -0.2521406521984175  Val loss: -0.030521907757238013
Epoch 474
-------------------------------
Batch 1/64 loss: -0.2763749361038208
Batch 2/64 loss: -0.2608906626701355
Batch 3/64 loss: -0.24006986618041992
Batch 4/64 loss: -0.26252394914627075
Batch 5/64 loss: -0.2659296989440918
Batch 6/64 loss: -0.26128101348876953
Batch 7/64 loss: -0.26369473338127136
Batch 8/64 loss: -0.27169719338417053
Batch 9/64 loss: -0.26251649856567383
Batch 10/64 loss: -0.25414228439331055
Batch 11/64 loss: -0.2677685022354126
Batch 12/64 loss: -0.2602153420448303
Batch 13/64 loss: -0.2267097532749176
Batch 14/64 loss: -0.25229620933532715
Batch 15/64 loss: -0.2627285420894623
Batch 16/64 loss: -0.26334258913993835
Batch 17/64 loss: -0.2601817846298218
Batch 18/64 loss: -0.27210575342178345
Batch 19/64 loss: -0.2741469144821167
Batch 20/64 loss: -0.24744397401809692
Batch 21/64 loss: -0.26647600531578064
Batch 22/64 loss: -0.272064745426178
Batch 23/64 loss: -0.2753946781158447
Batch 24/64 loss: -0.2786336839199066
Batch 25/64 loss: -0.2519901394844055
Batch 26/64 loss: -0.2524617910385132
Batch 27/64 loss: -0.2510608732700348
Batch 28/64 loss: -0.2658923864364624
Batch 29/64 loss: -0.27524787187576294
Batch 30/64 loss: -0.27226465940475464
Batch 31/64 loss: -0.264662504196167
Batch 32/64 loss: -0.2673800587654114
Batch 33/64 loss: -0.26484817266464233
Batch 34/64 loss: -0.2598028779029846
Batch 35/64 loss: -0.2654380202293396
Batch 36/64 loss: -0.27251696586608887
Batch 37/64 loss: -0.23449891805648804
Batch 38/64 loss: -0.2562353014945984
Batch 39/64 loss: -0.2580430507659912
Batch 40/64 loss: -0.2581411600112915
Batch 41/64 loss: -0.23126903176307678
Batch 42/64 loss: -0.24117839336395264
Batch 43/64 loss: -0.24944502115249634
Batch 44/64 loss: -0.23743343353271484
Batch 45/64 loss: -0.2517068386077881
Batch 46/64 loss: -0.2598242163658142
Batch 47/64 loss: -0.2435888648033142
Batch 48/64 loss: -0.2644616663455963
Batch 49/64 loss: -0.24779963493347168
Batch 50/64 loss: -0.24143177270889282
Batch 51/64 loss: -0.2538055181503296
Batch 52/64 loss: -0.2617867588996887
Batch 53/64 loss: -0.2344914972782135
Batch 54/64 loss: -0.2678786814212799
Batch 55/64 loss: -0.25791841745376587
Batch 56/64 loss: -0.23428940773010254
Batch 57/64 loss: -0.25720542669296265
Batch 58/64 loss: -0.22480595111846924
Batch 59/64 loss: -0.255515992641449
Batch 60/64 loss: -0.26190927624702454
Batch 61/64 loss: -0.25509124994277954
Batch 62/64 loss: -0.24902623891830444
Batch 63/64 loss: -0.2401927411556244
Batch 64/64 loss: -0.2552284598350525
Epoch 474  Train loss: -0.25698182652978335  Val loss: -0.029266304986173754
Epoch 475
-------------------------------
Batch 1/64 loss: -0.2747534513473511
Batch 2/64 loss: -0.2596961259841919
Batch 3/64 loss: -0.26525798439979553
Batch 4/64 loss: -0.23205214738845825
Batch 5/64 loss: -0.2599339783191681
Batch 6/64 loss: -0.2523038983345032
Batch 7/64 loss: -0.27134278416633606
Batch 8/64 loss: -0.24814638495445251
Batch 9/64 loss: -0.24491536617279053
Batch 10/64 loss: -0.264249324798584
Batch 11/64 loss: -0.25222048163414
Batch 12/64 loss: -0.23614037036895752
Batch 13/64 loss: -0.24526914954185486
Batch 14/64 loss: -0.26734015345573425
Batch 15/64 loss: -0.2571656107902527
Batch 16/64 loss: -0.23819440603256226
Batch 17/64 loss: -0.2566568851470947
Batch 18/64 loss: -0.248224139213562
Batch 19/64 loss: -0.23242589831352234
Batch 20/64 loss: -0.2440427541732788
Batch 21/64 loss: -0.24224448204040527
Batch 22/64 loss: -0.24977701902389526
Batch 23/64 loss: -0.24842125177383423
Batch 24/64 loss: -0.2537775933742523
Batch 25/64 loss: -0.25146642327308655
Batch 26/64 loss: -0.2653006315231323
Batch 27/64 loss: -0.22549492120742798
Batch 28/64 loss: -0.23500072956085205
Batch 29/64 loss: -0.23997092247009277
Batch 30/64 loss: -0.22371214628219604
Batch 31/64 loss: -0.23880335688591003
Batch 32/64 loss: -0.24075406789779663
Batch 33/64 loss: -0.23056399822235107
Batch 34/64 loss: -0.2720813751220703
Batch 35/64 loss: -0.2574240565299988
Batch 36/64 loss: -0.23899751901626587
Batch 37/64 loss: -0.2764873504638672
Batch 38/64 loss: -0.26600974798202515
Batch 39/64 loss: -0.24510455131530762
Batch 40/64 loss: -0.22954583168029785
Batch 41/64 loss: -0.2547295391559601
Batch 42/64 loss: -0.27813035249710083
Batch 43/64 loss: -0.2325478196144104
Batch 44/64 loss: -0.25587448477745056
Batch 45/64 loss: -0.2591313123703003
Batch 46/64 loss: -0.2581855058670044
Batch 47/64 loss: -0.2845214903354645
Batch 48/64 loss: -0.26898205280303955
Batch 49/64 loss: -0.2220710813999176
Batch 50/64 loss: -0.2360888123512268
Batch 51/64 loss: -0.24993395805358887
Batch 52/64 loss: -0.27801886200904846
Batch 53/64 loss: -0.2420804500579834
Batch 54/64 loss: -0.250779390335083
Batch 55/64 loss: -0.253555566072464
Batch 56/64 loss: -0.25853127241134644
Batch 57/64 loss: -0.25510770082473755
Batch 58/64 loss: -0.2574164867401123
Batch 59/64 loss: -0.25128746032714844
Batch 60/64 loss: -0.2652739882469177
Batch 61/64 loss: -0.23881381750106812
Batch 62/64 loss: -0.24888429045677185
Batch 63/64 loss: -0.23241335153579712
Batch 64/64 loss: -0.2639104723930359
Epoch 475  Train loss: -0.2511617438465941  Val loss: -0.03037593721114483
Epoch 476
-------------------------------
Batch 1/64 loss: -0.2717387080192566
Batch 2/64 loss: -0.25878071784973145
Batch 3/64 loss: -0.253558486700058
Batch 4/64 loss: -0.2712399959564209
Batch 5/64 loss: -0.21882081031799316
Batch 6/64 loss: -0.23624449968338013
Batch 7/64 loss: -0.2297341227531433
Batch 8/64 loss: -0.2532547414302826
Batch 9/64 loss: -0.25293201208114624
Batch 10/64 loss: -0.25246644020080566
Batch 11/64 loss: -0.2477281093597412
Batch 12/64 loss: -0.23831674456596375
Batch 13/64 loss: -0.25367116928100586
Batch 14/64 loss: -0.24512940645217896
Batch 15/64 loss: -0.2616506814956665
Batch 16/64 loss: -0.224238783121109
Batch 17/64 loss: -0.24817517399787903
Batch 18/64 loss: -0.2035265564918518
Batch 19/64 loss: -0.27371731400489807
Batch 20/64 loss: -0.2543272078037262
Batch 21/64 loss: -0.20375275611877441
Batch 22/64 loss: -0.26891404390335083
Batch 23/64 loss: -0.2544912099838257
Batch 24/64 loss: -0.2685892879962921
Batch 25/64 loss: -0.2598375976085663
Batch 26/64 loss: -0.2638150453567505
Batch 27/64 loss: -0.2838289439678192
Batch 28/64 loss: -0.2557179629802704
Batch 29/64 loss: -0.2574867904186249
Batch 30/64 loss: -0.2650253176689148
Batch 31/64 loss: -0.26015689969062805
Batch 32/64 loss: -0.27078986167907715
Batch 33/64 loss: -0.26220133900642395
Batch 34/64 loss: -0.24522268772125244
Batch 35/64 loss: -0.2687385380268097
Batch 36/64 loss: -0.2481113076210022
Batch 37/64 loss: -0.2488963007926941
Batch 38/64 loss: -0.23537832498550415
Batch 39/64 loss: -0.2664182186126709
Batch 40/64 loss: -0.2804945111274719
Batch 41/64 loss: -0.26452845335006714
Batch 42/64 loss: -0.24885302782058716
Batch 43/64 loss: -0.24067577719688416
Batch 44/64 loss: -0.24337202310562134
Batch 45/64 loss: -0.26008373498916626
Batch 46/64 loss: -0.2684939205646515
Batch 47/64 loss: -0.2595817446708679
Batch 48/64 loss: -0.2574481964111328
Batch 49/64 loss: -0.24482521414756775
Batch 50/64 loss: -0.23178070783615112
Batch 51/64 loss: -0.2585306167602539
Batch 52/64 loss: -0.22442761063575745
Batch 53/64 loss: -0.2536192536354065
Batch 54/64 loss: -0.2761844992637634
Batch 55/64 loss: -0.2651376724243164
Batch 56/64 loss: -0.2471027374267578
Batch 57/64 loss: -0.25951912999153137
Batch 58/64 loss: -0.24092090129852295
Batch 59/64 loss: -0.24826329946517944
Batch 60/64 loss: -0.25835853815078735
Batch 61/64 loss: -0.21999850869178772
Batch 62/64 loss: -0.23792269825935364
Batch 63/64 loss: -0.26591163873672485
Batch 64/64 loss: -0.2753172814846039
Epoch 476  Train loss: -0.2525356312592824  Val loss: -0.03373994388940818
Epoch 477
-------------------------------
Batch 1/64 loss: -0.27140316367149353
Batch 2/64 loss: -0.25686973333358765
Batch 3/64 loss: -0.27459752559661865
Batch 4/64 loss: -0.26496559381484985
Batch 5/64 loss: -0.26697999238967896
Batch 6/64 loss: -0.24560952186584473
Batch 7/64 loss: -0.27890485525131226
Batch 8/64 loss: -0.2725422978401184
Batch 9/64 loss: -0.280337929725647
Batch 10/64 loss: -0.1902550458908081
Batch 11/64 loss: -0.2513214945793152
Batch 12/64 loss: -0.201460063457489
Batch 13/64 loss: -0.2343144714832306
Batch 14/64 loss: -0.25230348110198975
Batch 15/64 loss: -0.23434141278266907
Batch 16/64 loss: -0.2542620897293091
Batch 17/64 loss: -0.2736535668373108
Batch 18/64 loss: -0.264684796333313
Batch 19/64 loss: -0.25942927598953247
Batch 20/64 loss: -0.2620040476322174
Batch 21/64 loss: -0.26512593030929565
Batch 22/64 loss: -0.268927663564682
Batch 23/64 loss: -0.2503064274787903
Batch 24/64 loss: -0.2591356337070465
Batch 25/64 loss: -0.2477537989616394
Batch 26/64 loss: -0.25570571422576904
Batch 27/64 loss: -0.2566404938697815
Batch 28/64 loss: -0.2526029944419861
Batch 29/64 loss: -0.273276150226593
Batch 30/64 loss: -0.26000988483428955
Batch 31/64 loss: -0.23805931210517883
Batch 32/64 loss: -0.23327654600143433
Batch 33/64 loss: -0.2504088282585144
Batch 34/64 loss: -0.24838262796401978
Batch 35/64 loss: -0.2576732635498047
Batch 36/64 loss: -0.2732101082801819
Batch 37/64 loss: -0.2598569393157959
Batch 38/64 loss: -0.19800275564193726
Batch 39/64 loss: -0.19545036554336548
Batch 40/64 loss: -0.2548478841781616
Batch 41/64 loss: -0.2807410955429077
Batch 42/64 loss: -0.26928889751434326
Batch 43/64 loss: -0.23493283987045288
Batch 44/64 loss: -0.2612498700618744
Batch 45/64 loss: -0.24900496006011963
Batch 46/64 loss: -0.2441430389881134
Batch 47/64 loss: -0.23864543437957764
Batch 48/64 loss: -0.25799381732940674
Batch 49/64 loss: -0.22623807191848755
Batch 50/64 loss: -0.25177714228630066
Batch 51/64 loss: -0.2668445110321045
Batch 52/64 loss: -0.2715054154396057
Batch 53/64 loss: -0.2612842917442322
Batch 54/64 loss: -0.25081300735473633
Batch 55/64 loss: -0.2767137885093689
Batch 56/64 loss: -0.2669982612133026
Batch 57/64 loss: -0.2524990439414978
Batch 58/64 loss: -0.24999547004699707
Batch 59/64 loss: -0.24317321181297302
Batch 60/64 loss: -0.2091791033744812
Batch 61/64 loss: -0.26180702447891235
Batch 62/64 loss: -0.2652169466018677
Batch 63/64 loss: -0.24854236841201782
Batch 64/64 loss: -0.22085511684417725
Epoch 477  Train loss: -0.25244113931468887  Val loss: -0.03136318573837018
Epoch 478
-------------------------------
Batch 1/64 loss: -0.24428987503051758
Batch 2/64 loss: -0.2514556646347046
Batch 3/64 loss: -0.24856314063072205
Batch 4/64 loss: -0.2779424786567688
Batch 5/64 loss: -0.2744123935699463
Batch 6/64 loss: -0.2302929162979126
Batch 7/64 loss: -0.27016741037368774
Batch 8/64 loss: -0.26838165521621704
Batch 9/64 loss: -0.286149263381958
Batch 10/64 loss: -0.26371216773986816
Batch 11/64 loss: -0.275197833776474
Batch 12/64 loss: -0.24359434843063354
Batch 13/64 loss: -0.2742795944213867
Batch 14/64 loss: -0.25292229652404785
Batch 15/64 loss: -0.26537904143333435
Batch 16/64 loss: -0.2151750922203064
Batch 17/64 loss: -0.2498210072517395
Batch 18/64 loss: -0.2537240982055664
Batch 19/64 loss: -0.2529173195362091
Batch 20/64 loss: -0.2720215916633606
Batch 21/64 loss: -0.2565675973892212
Batch 22/64 loss: -0.2510830760002136
Batch 23/64 loss: -0.2303633689880371
Batch 24/64 loss: -0.270671546459198
Batch 25/64 loss: -0.25820764899253845
Batch 26/64 loss: -0.2722202241420746
Batch 27/64 loss: -0.2570253014564514
Batch 28/64 loss: -0.26093876361846924
Batch 29/64 loss: -0.24108803272247314
Batch 30/64 loss: -0.22873878479003906
Batch 31/64 loss: -0.24068838357925415
Batch 32/64 loss: -0.28486108779907227
Batch 33/64 loss: -0.23922109603881836
Batch 34/64 loss: -0.2522588074207306
Batch 35/64 loss: -0.2463482916355133
Batch 36/64 loss: -0.2503163814544678
Batch 37/64 loss: -0.2613694667816162
Batch 38/64 loss: -0.24425256252288818
Batch 39/64 loss: -0.25123149156570435
Batch 40/64 loss: -0.25611889362335205
Batch 41/64 loss: -0.25628045201301575
Batch 42/64 loss: -0.2885546088218689
Batch 43/64 loss: -0.2570411264896393
Batch 44/64 loss: -0.26136893033981323
Batch 45/64 loss: -0.22631213068962097
Batch 46/64 loss: -0.2654111981391907
Batch 47/64 loss: -0.23717522621154785
Batch 48/64 loss: -0.24622052907943726
Batch 49/64 loss: -0.2644842267036438
Batch 50/64 loss: -0.25644028186798096
Batch 51/64 loss: -0.24137172102928162
Batch 52/64 loss: -0.23701968789100647
Batch 53/64 loss: -0.2571827173233032
Batch 54/64 loss: -0.24552598595619202
Batch 55/64 loss: -0.22905761003494263
Batch 56/64 loss: -0.27102771401405334
Batch 57/64 loss: -0.2591845393180847
Batch 58/64 loss: -0.2546393573284149
Batch 59/64 loss: -0.2521769404411316
Batch 60/64 loss: -0.25849783420562744
Batch 61/64 loss: -0.2416287660598755
Batch 62/64 loss: -0.2589039206504822
Batch 63/64 loss: -0.23347526788711548
Batch 64/64 loss: -0.16677147150039673
Epoch 478  Train loss: -0.25330242152307547  Val loss: -0.03172941203789203
Epoch 479
-------------------------------
Batch 1/64 loss: -0.2651117742061615
Batch 2/64 loss: -0.2603250741958618
Batch 3/64 loss: -0.26876914501190186
Batch 4/64 loss: -0.2477748990058899
Batch 5/64 loss: -0.2508113384246826
Batch 6/64 loss: -0.2463533580303192
Batch 7/64 loss: -0.2560237646102905
Batch 8/64 loss: -0.2530056834220886
Batch 9/64 loss: -0.25592663884162903
Batch 10/64 loss: -0.24574360251426697
Batch 11/64 loss: -0.2614012658596039
Batch 12/64 loss: -0.2575340270996094
Batch 13/64 loss: -0.24679625034332275
Batch 14/64 loss: -0.20706528425216675
Batch 15/64 loss: -0.2857951521873474
Batch 16/64 loss: -0.25729817152023315
Batch 17/64 loss: -0.25558578968048096
Batch 18/64 loss: -0.2557366192340851
Batch 19/64 loss: -0.24096709489822388
Batch 20/64 loss: -0.26137053966522217
Batch 21/64 loss: -0.26216307282447815
Batch 22/64 loss: -0.20111554861068726
Batch 23/64 loss: -0.2710093855857849
Batch 24/64 loss: -0.246393084526062
Batch 25/64 loss: -0.25830936431884766
Batch 26/64 loss: -0.2619307041168213
Batch 27/64 loss: -0.27172353863716125
Batch 28/64 loss: -0.2556609511375427
Batch 29/64 loss: -0.2528083026409149
Batch 30/64 loss: -0.26202523708343506
Batch 31/64 loss: -0.21924808621406555
Batch 32/64 loss: -0.21741032600402832
Batch 33/64 loss: -0.22163408994674683
Batch 34/64 loss: -0.2357563078403473
Batch 35/64 loss: -0.25761640071868896
Batch 36/64 loss: -0.23963096737861633
Batch 37/64 loss: -0.24548906087875366
Batch 38/64 loss: -0.25648945569992065
Batch 39/64 loss: -0.2609231770038605
Batch 40/64 loss: -0.24625810980796814
Batch 41/64 loss: -0.2540585398674011
Batch 42/64 loss: -0.25599902868270874
Batch 43/64 loss: -0.23202982544898987
Batch 44/64 loss: -0.24730762839317322
Batch 45/64 loss: -0.26461416482925415
Batch 46/64 loss: -0.2612767815589905
Batch 47/64 loss: -0.2595546543598175
Batch 48/64 loss: -0.2618803381919861
Batch 49/64 loss: -0.24031823873519897
Batch 50/64 loss: -0.27081918716430664
Batch 51/64 loss: -0.274338036775589
Batch 52/64 loss: -0.23302629590034485
Batch 53/64 loss: -0.2376437783241272
Batch 54/64 loss: -0.2735827565193176
Batch 55/64 loss: -0.2413783073425293
Batch 56/64 loss: -0.23064884543418884
Batch 57/64 loss: -0.23577088117599487
Batch 58/64 loss: -0.2679157257080078
Batch 59/64 loss: -0.26820528507232666
Batch 60/64 loss: -0.28100794553756714
Batch 61/64 loss: -0.2765464186668396
Batch 62/64 loss: -0.2339760661125183
Batch 63/64 loss: -0.2355826199054718
Batch 64/64 loss: -0.2516992688179016
Epoch 479  Train loss: -0.25175288541644225  Val loss: -0.03223341682932221
Epoch 480
-------------------------------
Batch 1/64 loss: -0.2690761685371399
Batch 2/64 loss: -0.26923584938049316
Batch 3/64 loss: -0.24777555465698242
Batch 4/64 loss: -0.2819582521915436
Batch 5/64 loss: -0.25152385234832764
Batch 6/64 loss: -0.27992552518844604
Batch 7/64 loss: -0.23823148012161255
Batch 8/64 loss: -0.26498186588287354
Batch 9/64 loss: -0.2714313864707947
Batch 10/64 loss: -0.24923932552337646
Batch 11/64 loss: -0.2520846128463745
Batch 12/64 loss: -0.26697421073913574
Batch 13/64 loss: -0.2800774574279785
Batch 14/64 loss: -0.24979066848754883
Batch 15/64 loss: -0.277411550283432
Batch 16/64 loss: -0.2755904197692871
Batch 17/64 loss: -0.27795374393463135
Batch 18/64 loss: -0.24705880880355835
Batch 19/64 loss: -0.2237415909767151
Batch 20/64 loss: -0.25294142961502075
Batch 21/64 loss: -0.22463536262512207
Batch 22/64 loss: -0.2374337911605835
Batch 23/64 loss: -0.23987507820129395
Batch 24/64 loss: -0.2535549998283386
Batch 25/64 loss: -0.2588595151901245
Batch 26/64 loss: -0.2556062340736389
Batch 27/64 loss: -0.248205304145813
Batch 28/64 loss: -0.2702144980430603
Batch 29/64 loss: -0.24578958749771118
Batch 30/64 loss: -0.2553613781929016
Batch 31/64 loss: -0.252538800239563
Batch 32/64 loss: -0.29528945684432983
Batch 33/64 loss: -0.238506019115448
Batch 34/64 loss: -0.24595069885253906
Batch 35/64 loss: -0.24409306049346924
Batch 36/64 loss: -0.2529773712158203
Batch 37/64 loss: -0.2552345395088196
Batch 38/64 loss: -0.21912750601768494
Batch 39/64 loss: -0.26116591691970825
Batch 40/64 loss: -0.2533198595046997
Batch 41/64 loss: -0.23890864849090576
Batch 42/64 loss: -0.25496041774749756
Batch 43/64 loss: -0.24403154850006104
Batch 44/64 loss: -0.2374725341796875
Batch 45/64 loss: -0.2546481192111969
Batch 46/64 loss: -0.24643027782440186
Batch 47/64 loss: -0.26225781440734863
Batch 48/64 loss: -0.24986004829406738
Batch 49/64 loss: -0.2639209032058716
Batch 50/64 loss: -0.2672770023345947
Batch 51/64 loss: -0.2492721974849701
Batch 52/64 loss: -0.25921911001205444
Batch 53/64 loss: -0.2682473957538605
Batch 54/64 loss: -0.24412855505943298
Batch 55/64 loss: -0.26196861267089844
Batch 56/64 loss: -0.2387356460094452
Batch 57/64 loss: -0.23545506596565247
Batch 58/64 loss: -0.22085526585578918
Batch 59/64 loss: -0.25670576095581055
Batch 60/64 loss: -0.23520222306251526
Batch 61/64 loss: -0.2605306804180145
Batch 62/64 loss: -0.2661469280719757
Batch 63/64 loss: -0.24095085263252258
Batch 64/64 loss: -0.23133695125579834
Epoch 480  Train loss: -0.2535753890579822  Val loss: -0.0344325762843758
Epoch 481
-------------------------------
Batch 1/64 loss: -0.2174941897392273
Batch 2/64 loss: -0.22728654742240906
Batch 3/64 loss: -0.2728460431098938
Batch 4/64 loss: -0.2639409005641937
Batch 5/64 loss: -0.25121501088142395
Batch 6/64 loss: -0.24703824520111084
Batch 7/64 loss: -0.24728253483772278
Batch 8/64 loss: -0.2658166289329529
Batch 9/64 loss: -0.2377985119819641
Batch 10/64 loss: -0.28137606382369995
Batch 11/64 loss: -0.2460499405860901
Batch 12/64 loss: -0.24176520109176636
Batch 13/64 loss: -0.24126243591308594
Batch 14/64 loss: -0.26084232330322266
Batch 15/64 loss: -0.2685185372829437
Batch 16/64 loss: -0.23822790384292603
Batch 17/64 loss: -0.2608354985713959
Batch 18/64 loss: -0.265730619430542
Batch 19/64 loss: -0.2561122179031372
Batch 20/64 loss: -0.23927605152130127
Batch 21/64 loss: -0.2604278326034546
Batch 22/64 loss: -0.25994420051574707
Batch 23/64 loss: -0.2662782669067383
Batch 24/64 loss: -0.25642985105514526
Batch 25/64 loss: -0.26402732729911804
Batch 26/64 loss: -0.2464739978313446
Batch 27/64 loss: -0.2857908606529236
Batch 28/64 loss: -0.2672092020511627
Batch 29/64 loss: -0.26399171352386475
Batch 30/64 loss: -0.26259884238243103
Batch 31/64 loss: -0.25469374656677246
Batch 32/64 loss: -0.2689203917980194
Batch 33/64 loss: -0.249493807554245
Batch 34/64 loss: -0.27204304933547974
Batch 35/64 loss: -0.25574350357055664
Batch 36/64 loss: -0.2764137387275696
Batch 37/64 loss: -0.23118892312049866
Batch 38/64 loss: -0.2174711525440216
Batch 39/64 loss: -0.24640429019927979
Batch 40/64 loss: -0.25256770849227905
Batch 41/64 loss: -0.24932587146759033
Batch 42/64 loss: -0.2655109167098999
Batch 43/64 loss: -0.2749408483505249
Batch 44/64 loss: -0.25387659668922424
Batch 45/64 loss: -0.2540222108364105
Batch 46/64 loss: -0.27028536796569824
Batch 47/64 loss: -0.1850716471672058
Batch 48/64 loss: -0.27644258737564087
Batch 49/64 loss: -0.26412931084632874
Batch 50/64 loss: -0.2546246647834778
Batch 51/64 loss: -0.27183759212493896
Batch 52/64 loss: -0.28495609760284424
Batch 53/64 loss: -0.26324787735939026
Batch 54/64 loss: -0.21460357308387756
Batch 55/64 loss: -0.2481825351715088
Batch 56/64 loss: -0.27417826652526855
Batch 57/64 loss: -0.2655629515647888
Batch 58/64 loss: -0.281354159116745
Batch 59/64 loss: -0.26055222749710083
Batch 60/64 loss: -0.24741119146347046
Batch 61/64 loss: -0.25637632608413696
Batch 62/64 loss: -0.2622613310813904
Batch 63/64 loss: -0.2505231201648712
Batch 64/64 loss: -0.2764163017272949
Epoch 481  Train loss: -0.25608497741175634  Val loss: -0.03261105137592329
Epoch 482
-------------------------------
Batch 1/64 loss: -0.26385772228240967
Batch 2/64 loss: -0.23028409481048584
Batch 3/64 loss: -0.25779610872268677
Batch 4/64 loss: -0.26479119062423706
Batch 5/64 loss: -0.25900381803512573
Batch 6/64 loss: -0.26767343282699585
Batch 7/64 loss: -0.2560431957244873
Batch 8/64 loss: -0.24255192279815674
Batch 9/64 loss: -0.25468114018440247
Batch 10/64 loss: -0.25957316160202026
Batch 11/64 loss: -0.26137620210647583
Batch 12/64 loss: -0.25073254108428955
Batch 13/64 loss: -0.24480271339416504
Batch 14/64 loss: -0.25787603855133057
Batch 15/64 loss: -0.25632503628730774
Batch 16/64 loss: -0.2693050801753998
Batch 17/64 loss: -0.2776166796684265
Batch 18/64 loss: -0.278181254863739
Batch 19/64 loss: -0.2606257200241089
Batch 20/64 loss: -0.260056734085083
Batch 21/64 loss: -0.2762468457221985
Batch 22/64 loss: -0.24472641944885254
Batch 23/64 loss: -0.27499204874038696
Batch 24/64 loss: -0.25557082891464233
Batch 25/64 loss: -0.26129013299942017
Batch 26/64 loss: -0.2602677643299103
Batch 27/64 loss: -0.24110722541809082
Batch 28/64 loss: -0.23529052734375
Batch 29/64 loss: -0.27640601992607117
Batch 30/64 loss: -0.260010689496994
Batch 31/64 loss: -0.24398189783096313
Batch 32/64 loss: -0.247127503156662
Batch 33/64 loss: -0.26250818371772766
Batch 34/64 loss: -0.24173825979232788
Batch 35/64 loss: -0.27147483825683594
Batch 36/64 loss: -0.2746189832687378
Batch 37/64 loss: -0.24938321113586426
Batch 38/64 loss: -0.2690851390361786
Batch 39/64 loss: -0.24826166033744812
Batch 40/64 loss: -0.2588322162628174
Batch 41/64 loss: -0.2413341999053955
Batch 42/64 loss: -0.26044976711273193
Batch 43/64 loss: -0.238974928855896
Batch 44/64 loss: -0.2511870265007019
Batch 45/64 loss: -0.25029778480529785
Batch 46/64 loss: -0.26645153760910034
Batch 47/64 loss: -0.2673793137073517
Batch 48/64 loss: -0.21181029081344604
Batch 49/64 loss: -0.26205915212631226
Batch 50/64 loss: -0.24570992588996887
Batch 51/64 loss: -0.23943227529525757
Batch 52/64 loss: -0.2459617555141449
Batch 53/64 loss: -0.2409481406211853
Batch 54/64 loss: -0.242345929145813
Batch 55/64 loss: -0.2523297369480133
Batch 56/64 loss: -0.27023953199386597
Batch 57/64 loss: -0.27280595898628235
Batch 58/64 loss: -0.1849043369293213
Batch 59/64 loss: -0.2360275387763977
Batch 60/64 loss: -0.20503336191177368
Batch 61/64 loss: -0.27563831210136414
Batch 62/64 loss: -0.2738555073738098
Batch 63/64 loss: -0.24846956133842468
Batch 64/64 loss: -0.25212791562080383
Epoch 482  Train loss: -0.25409907441513213  Val loss: -0.031249116581330186
Epoch 483
-------------------------------
Batch 1/64 loss: -0.2233678698539734
Batch 2/64 loss: -0.2443179488182068
Batch 3/64 loss: -0.27494749426841736
Batch 4/64 loss: -0.2491709589958191
Batch 5/64 loss: -0.2713243067264557
Batch 6/64 loss: -0.2879040539264679
Batch 7/64 loss: -0.24256151914596558
Batch 8/64 loss: -0.27391359210014343
Batch 9/64 loss: -0.25710421800613403
Batch 10/64 loss: -0.25657618045806885
Batch 11/64 loss: -0.243854820728302
Batch 12/64 loss: -0.2762296199798584
Batch 13/64 loss: -0.25905758142471313
Batch 14/64 loss: -0.2650197148323059
Batch 15/64 loss: -0.26887938380241394
Batch 16/64 loss: -0.27266961336135864
Batch 17/64 loss: -0.2652589678764343
Batch 18/64 loss: -0.28417110443115234
Batch 19/64 loss: -0.26000547409057617
Batch 20/64 loss: -0.2619148790836334
Batch 21/64 loss: -0.2649931013584137
Batch 22/64 loss: -0.2585395574569702
Batch 23/64 loss: -0.26643043756484985
Batch 24/64 loss: -0.2458311915397644
Batch 25/64 loss: -0.264201819896698
Batch 26/64 loss: -0.2602705657482147
Batch 27/64 loss: -0.283700168132782
Batch 28/64 loss: -0.26823773980140686
Batch 29/64 loss: -0.255834698677063
Batch 30/64 loss: -0.2547377347946167
Batch 31/64 loss: -0.24215373396873474
Batch 32/64 loss: -0.25779855251312256
Batch 33/64 loss: -0.2827148735523224
Batch 34/64 loss: -0.2640676498413086
Batch 35/64 loss: -0.24887338280677795
Batch 36/64 loss: -0.24861013889312744
Batch 37/64 loss: -0.23330357670783997
Batch 38/64 loss: -0.2565746009349823
Batch 39/64 loss: -0.2542020380496979
Batch 40/64 loss: -0.2528754770755768
Batch 41/64 loss: -0.25396132469177246
Batch 42/64 loss: -0.2613752484321594
Batch 43/64 loss: -0.2825346291065216
Batch 44/64 loss: -0.24318727850914001
Batch 45/64 loss: -0.28199878334999084
Batch 46/64 loss: -0.24919310212135315
Batch 47/64 loss: -0.2664377987384796
Batch 48/64 loss: -0.2576725482940674
Batch 49/64 loss: -0.2618803381919861
Batch 50/64 loss: -0.2498164176940918
Batch 51/64 loss: -0.24830543994903564
Batch 52/64 loss: -0.2431429922580719
Batch 53/64 loss: -0.22340407967567444
Batch 54/64 loss: -0.2688528299331665
Batch 55/64 loss: -0.2665148973464966
Batch 56/64 loss: -0.24923449754714966
Batch 57/64 loss: -0.2602386474609375
Batch 58/64 loss: -0.27836549282073975
Batch 59/64 loss: -0.2740173041820526
Batch 60/64 loss: -0.2609180212020874
Batch 61/64 loss: -0.25994834303855896
Batch 62/64 loss: -0.2268368899822235
Batch 63/64 loss: -0.2635631561279297
Batch 64/64 loss: -0.26062047481536865
Epoch 483  Train loss: -0.2592794628704295  Val loss: -0.03246645706216084
Epoch 484
-------------------------------
Batch 1/64 loss: -0.23536226153373718
Batch 2/64 loss: -0.26459386944770813
Batch 3/64 loss: -0.2512108087539673
Batch 4/64 loss: -0.2759470045566559
Batch 5/64 loss: -0.2577236294746399
Batch 6/64 loss: -0.27783381938934326
Batch 7/64 loss: -0.2746025621891022
Batch 8/64 loss: -0.2707539200782776
Batch 9/64 loss: -0.2545509338378906
Batch 10/64 loss: -0.2740604877471924
Batch 11/64 loss: -0.2591025233268738
Batch 12/64 loss: -0.28200143575668335
Batch 13/64 loss: -0.2678644061088562
Batch 14/64 loss: -0.269014447927475
Batch 15/64 loss: -0.2494073510169983
Batch 16/64 loss: -0.2812483310699463
Batch 17/64 loss: -0.23810160160064697
Batch 18/64 loss: -0.25793302059173584
Batch 19/64 loss: -0.2698752284049988
Batch 20/64 loss: -0.2391965389251709
Batch 21/64 loss: -0.2633814811706543
Batch 22/64 loss: -0.25893673300743103
Batch 23/64 loss: -0.29713261127471924
Batch 24/64 loss: -0.27209049463272095
Batch 25/64 loss: -0.2620726227760315
Batch 26/64 loss: -0.2188768982887268
Batch 27/64 loss: -0.28447526693344116
Batch 28/64 loss: -0.2831360697746277
Batch 29/64 loss: -0.22007179260253906
Batch 30/64 loss: -0.2681316137313843
Batch 31/64 loss: -0.2549438178539276
Batch 32/64 loss: -0.2533233165740967
Batch 33/64 loss: -0.2505388855934143
Batch 34/64 loss: -0.24933233857154846
Batch 35/64 loss: -0.25294724106788635
Batch 36/64 loss: -0.2590711712837219
Batch 37/64 loss: -0.24939009547233582
Batch 38/64 loss: -0.26880979537963867
Batch 39/64 loss: -0.2515981197357178
Batch 40/64 loss: -0.27206897735595703
Batch 41/64 loss: -0.23567652702331543
Batch 42/64 loss: -0.25666505098342896
Batch 43/64 loss: -0.21367958188056946
Batch 44/64 loss: -0.26598095893859863
Batch 45/64 loss: -0.22438350319862366
Batch 46/64 loss: -0.2554936110973358
Batch 47/64 loss: -0.254317045211792
Batch 48/64 loss: -0.2621144652366638
Batch 49/64 loss: -0.2508147060871124
Batch 50/64 loss: -0.23467063903808594
Batch 51/64 loss: -0.25818830728530884
Batch 52/64 loss: -0.24718058109283447
Batch 53/64 loss: -0.2752710282802582
Batch 54/64 loss: -0.22292721271514893
Batch 55/64 loss: -0.24549788236618042
Batch 56/64 loss: -0.23475825786590576
Batch 57/64 loss: -0.20823293924331665
Batch 58/64 loss: -0.255334734916687
Batch 59/64 loss: -0.25992387533187866
Batch 60/64 loss: -0.24246743321418762
Batch 61/64 loss: -0.2632591724395752
Batch 62/64 loss: -0.2616509199142456
Batch 63/64 loss: -0.23530161380767822
Batch 64/64 loss: -0.2587328851222992
Epoch 484  Train loss: -0.25566358016986473  Val loss: -0.030836544086023706
Epoch 485
-------------------------------
Batch 1/64 loss: -0.24129855632781982
Batch 2/64 loss: -0.25600236654281616
Batch 3/64 loss: -0.26486510038375854
Batch 4/64 loss: -0.2592766284942627
Batch 5/64 loss: -0.247280091047287
Batch 6/64 loss: -0.25142210721969604
Batch 7/64 loss: -0.25769877433776855
Batch 8/64 loss: -0.2362615466117859
Batch 9/64 loss: -0.25381672382354736
Batch 10/64 loss: -0.24287736415863037
Batch 11/64 loss: -0.2605151832103729
Batch 12/64 loss: -0.21860402822494507
Batch 13/64 loss: -0.2733284831047058
Batch 14/64 loss: -0.2638241648674011
Batch 15/64 loss: -0.2677687406539917
Batch 16/64 loss: -0.27122795581817627
Batch 17/64 loss: -0.26640141010284424
Batch 18/64 loss: -0.2599307596683502
Batch 19/64 loss: -0.257682204246521
Batch 20/64 loss: -0.2322847843170166
Batch 21/64 loss: -0.24868035316467285
Batch 22/64 loss: -0.25170227885246277
Batch 23/64 loss: -0.24803924560546875
Batch 24/64 loss: -0.25274866819381714
Batch 25/64 loss: -0.2786761522293091
Batch 26/64 loss: -0.26743626594543457
Batch 27/64 loss: -0.210135817527771
Batch 28/64 loss: -0.2795940041542053
Batch 29/64 loss: -0.25703346729278564
Batch 30/64 loss: -0.2537912428379059
Batch 31/64 loss: -0.26842013001441956
Batch 32/64 loss: -0.2556925415992737
Batch 33/64 loss: -0.26492542028427124
Batch 34/64 loss: -0.2499847412109375
Batch 35/64 loss: -0.2541213631629944
Batch 36/64 loss: -0.2604965269565582
Batch 37/64 loss: -0.2590389847755432
Batch 38/64 loss: -0.2648368179798126
Batch 39/64 loss: -0.25929898023605347
Batch 40/64 loss: -0.273567259311676
Batch 41/64 loss: -0.24179226160049438
Batch 42/64 loss: -0.25688499212265015
Batch 43/64 loss: -0.25214114785194397
Batch 44/64 loss: -0.2629701495170593
Batch 45/64 loss: -0.25863099098205566
Batch 46/64 loss: -0.27187252044677734
Batch 47/64 loss: -0.20019745826721191
Batch 48/64 loss: -0.25777775049209595
Batch 49/64 loss: -0.25286561250686646
Batch 50/64 loss: -0.2721748948097229
Batch 51/64 loss: -0.25623613595962524
Batch 52/64 loss: -0.2618277072906494
Batch 53/64 loss: -0.26904481649398804
Batch 54/64 loss: -0.2537592351436615
Batch 55/64 loss: -0.2787821888923645
Batch 56/64 loss: -0.25424903631210327
Batch 57/64 loss: -0.2503087520599365
Batch 58/64 loss: -0.2262120544910431
Batch 59/64 loss: -0.2587791979312897
Batch 60/64 loss: -0.25573664903640747
Batch 61/64 loss: -0.2729538083076477
Batch 62/64 loss: -0.2866995930671692
Batch 63/64 loss: -0.26479434967041016
Batch 64/64 loss: -0.21362125873565674
Epoch 485  Train loss: -0.2559606977537566  Val loss: -0.031147650631842334
Epoch 486
-------------------------------
Batch 1/64 loss: -0.26887160539627075
Batch 2/64 loss: -0.25589239597320557
Batch 3/64 loss: -0.2768489718437195
Batch 4/64 loss: -0.24773573875427246
Batch 5/64 loss: -0.2860585153102875
Batch 6/64 loss: -0.2621127963066101
Batch 7/64 loss: -0.2528930604457855
Batch 8/64 loss: -0.2761334180831909
Batch 9/64 loss: -0.2551721930503845
Batch 10/64 loss: -0.26210615038871765
Batch 11/64 loss: -0.2863345742225647
Batch 12/64 loss: -0.2801498770713806
Batch 13/64 loss: -0.24894386529922485
Batch 14/64 loss: -0.2741607427597046
Batch 15/64 loss: -0.2479822039604187
Batch 16/64 loss: -0.2898544669151306
Batch 17/64 loss: -0.25856468081474304
Batch 18/64 loss: -0.21685048937797546
Batch 19/64 loss: -0.25419172644615173
Batch 20/64 loss: -0.24387633800506592
Batch 21/64 loss: -0.27628767490386963
Batch 22/64 loss: -0.2682255506515503
Batch 23/64 loss: -0.23296895623207092
Batch 24/64 loss: -0.2542813718318939
Batch 25/64 loss: -0.27254438400268555
Batch 26/64 loss: -0.2739431858062744
Batch 27/64 loss: -0.25689420104026794
Batch 28/64 loss: -0.25940465927124023
Batch 29/64 loss: -0.25252318382263184
Batch 30/64 loss: -0.2824355959892273
Batch 31/64 loss: -0.28538262844085693
Batch 32/64 loss: -0.2466803789138794
Batch 33/64 loss: -0.20181429386138916
Batch 34/64 loss: -0.23244228959083557
Batch 35/64 loss: -0.2797362208366394
Batch 36/64 loss: -0.26846760511398315
Batch 37/64 loss: -0.26142656803131104
Batch 38/64 loss: -0.24991944432258606
Batch 39/64 loss: -0.28475695848464966
Batch 40/64 loss: -0.23359951376914978
Batch 41/64 loss: -0.24679923057556152
Batch 42/64 loss: -0.2389218807220459
Batch 43/64 loss: -0.24678605794906616
Batch 44/64 loss: -0.2568686008453369
Batch 45/64 loss: -0.2556666135787964
Batch 46/64 loss: -0.23894274234771729
Batch 47/64 loss: -0.2501096725463867
Batch 48/64 loss: -0.23828259110450745
Batch 49/64 loss: -0.2525620758533478
Batch 50/64 loss: -0.26078706979751587
Batch 51/64 loss: -0.2548257112503052
Batch 52/64 loss: -0.2679934501647949
Batch 53/64 loss: -0.2554125487804413
Batch 54/64 loss: -0.27292025089263916
Batch 55/64 loss: -0.26612189412117004
Batch 56/64 loss: -0.2538996636867523
Batch 57/64 loss: -0.2791403532028198
Batch 58/64 loss: -0.27057144045829773
Batch 59/64 loss: -0.24730318784713745
Batch 60/64 loss: -0.22852569818496704
Batch 61/64 loss: -0.24111858010292053
Batch 62/64 loss: -0.24019962549209595
Batch 63/64 loss: -0.25006842613220215
Batch 64/64 loss: -0.2598206400871277
Epoch 486  Train loss: -0.2576809618987289  Val loss: -0.030664411402240244
Epoch 487
-------------------------------
Batch 1/64 loss: -0.2505088150501251
Batch 2/64 loss: -0.23685193061828613
Batch 3/64 loss: -0.23704195022583008
Batch 4/64 loss: -0.25848594307899475
Batch 5/64 loss: -0.26348239183425903
Batch 6/64 loss: -0.2697375416755676
Batch 7/64 loss: -0.28011730313301086
Batch 8/64 loss: -0.24491962790489197
Batch 9/64 loss: -0.26009953022003174
Batch 10/64 loss: -0.2613162398338318
Batch 11/64 loss: -0.2663823962211609
Batch 12/64 loss: -0.2798289656639099
Batch 13/64 loss: -0.27512478828430176
Batch 14/64 loss: -0.25957298278808594
Batch 15/64 loss: -0.24748563766479492
Batch 16/64 loss: -0.26240086555480957
Batch 17/64 loss: -0.239434152841568
Batch 18/64 loss: -0.2775646448135376
Batch 19/64 loss: -0.28959840536117554
Batch 20/64 loss: -0.2598057985305786
Batch 21/64 loss: -0.28385043144226074
Batch 22/64 loss: -0.25870588421821594
Batch 23/64 loss: -0.24181360006332397
Batch 24/64 loss: -0.26934343576431274
Batch 25/64 loss: -0.286880224943161
Batch 26/64 loss: -0.24639159440994263
Batch 27/64 loss: -0.26319605112075806
Batch 28/64 loss: -0.25008419156074524
Batch 29/64 loss: -0.2391040325164795
Batch 30/64 loss: -0.27147114276885986
Batch 31/64 loss: -0.26445043087005615
Batch 32/64 loss: -0.21768099069595337
Batch 33/64 loss: -0.25657275319099426
Batch 34/64 loss: -0.23760554194450378
Batch 35/64 loss: -0.24324575066566467
Batch 36/64 loss: -0.2568780779838562
Batch 37/64 loss: -0.26151061058044434
Batch 38/64 loss: -0.26874130964279175
Batch 39/64 loss: -0.2668268084526062
Batch 40/64 loss: -0.255070298910141
Batch 41/64 loss: -0.2520226836204529
Batch 42/64 loss: -0.23188519477844238
Batch 43/64 loss: -0.23009634017944336
Batch 44/64 loss: -0.256567120552063
Batch 45/64 loss: -0.27207446098327637
Batch 46/64 loss: -0.2821451425552368
Batch 47/64 loss: -0.25533097982406616
Batch 48/64 loss: -0.22602838277816772
Batch 49/64 loss: -0.2524062991142273
Batch 50/64 loss: -0.2602282762527466
Batch 51/64 loss: -0.2494831681251526
Batch 52/64 loss: -0.2667681574821472
Batch 53/64 loss: -0.25782525539398193
Batch 54/64 loss: -0.2713411748409271
Batch 55/64 loss: -0.24424445629119873
Batch 56/64 loss: -0.2632203698158264
Batch 57/64 loss: -0.2724642753601074
Batch 58/64 loss: -0.23477834463119507
Batch 59/64 loss: -0.26812413334846497
Batch 60/64 loss: -0.2773721218109131
Batch 61/64 loss: -0.26699888706207275
Batch 62/64 loss: -0.2742810845375061
Batch 63/64 loss: -0.2753102779388428
Batch 64/64 loss: -0.24829041957855225
Epoch 487  Train loss: -0.25861015647065405  Val loss: -0.02855373555442312
Epoch 488
-------------------------------
Batch 1/64 loss: -0.2833801805973053
Batch 2/64 loss: -0.25235748291015625
Batch 3/64 loss: -0.2567947506904602
Batch 4/64 loss: -0.24111565947532654
Batch 5/64 loss: -0.26870661973953247
Batch 6/64 loss: -0.27055972814559937
Batch 7/64 loss: -0.242978036403656
Batch 8/64 loss: -0.25272661447525024
Batch 9/64 loss: -0.25242435932159424
Batch 10/64 loss: -0.2416297197341919
Batch 11/64 loss: -0.261563241481781
Batch 12/64 loss: -0.2548401355743408
Batch 13/64 loss: -0.24864128232002258
Batch 14/64 loss: -0.23223978281021118
Batch 15/64 loss: -0.23251336812973022
Batch 16/64 loss: -0.2739896774291992
Batch 17/64 loss: -0.22421583533287048
Batch 18/64 loss: -0.2502746284008026
Batch 19/64 loss: -0.2498096227645874
Batch 20/64 loss: -0.24264538288116455
Batch 21/64 loss: -0.24767565727233887
Batch 22/64 loss: -0.25411832332611084
Batch 23/64 loss: -0.2866336703300476
Batch 24/64 loss: -0.2638271450996399
Batch 25/64 loss: -0.25671470165252686
Batch 26/64 loss: -0.2723110020160675
Batch 27/64 loss: -0.23350068926811218
Batch 28/64 loss: -0.23656508326530457
Batch 29/64 loss: -0.2613605260848999
Batch 30/64 loss: -0.22329601645469666
Batch 31/64 loss: -0.27410727739334106
Batch 32/64 loss: -0.23059165477752686
Batch 33/64 loss: -0.2575221657752991
Batch 34/64 loss: -0.2680795192718506
Batch 35/64 loss: -0.254768967628479
Batch 36/64 loss: -0.26120346784591675
Batch 37/64 loss: -0.24717366695404053
Batch 38/64 loss: -0.26275449991226196
Batch 39/64 loss: -0.26404744386672974
Batch 40/64 loss: -0.20682156085968018
Batch 41/64 loss: -0.25624293088912964
Batch 42/64 loss: -0.27315157651901245
Batch 43/64 loss: -0.24035227298736572
Batch 44/64 loss: -0.2835354208946228
Batch 45/64 loss: -0.27224600315093994
Batch 46/64 loss: -0.2749630808830261
Batch 47/64 loss: -0.2321794033050537
Batch 48/64 loss: -0.23449736833572388
Batch 49/64 loss: -0.27018851041793823
Batch 50/64 loss: -0.28695669770240784
Batch 51/64 loss: -0.2895154058933258
Batch 52/64 loss: -0.28616559505462646
Batch 53/64 loss: -0.24621480703353882
Batch 54/64 loss: -0.22961384057998657
Batch 55/64 loss: -0.2636270225048065
Batch 56/64 loss: -0.24802762269973755
Batch 57/64 loss: -0.28941959142684937
Batch 58/64 loss: -0.2629848122596741
Batch 59/64 loss: -0.25437289476394653
Batch 60/64 loss: -0.26620203256607056
Batch 61/64 loss: -0.24633938074111938
Batch 62/64 loss: -0.24411332607269287
Batch 63/64 loss: -0.24863499402999878
Batch 64/64 loss: -0.2632853090763092
Epoch 488  Train loss: -0.2555841211010428  Val loss: -0.03049437573685269
Epoch 489
-------------------------------
Batch 1/64 loss: -0.25258076190948486
Batch 2/64 loss: -0.25041040778160095
Batch 3/64 loss: -0.2741824984550476
Batch 4/64 loss: -0.2694481909275055
Batch 5/64 loss: -0.26541370153427124
Batch 6/64 loss: -0.26897984743118286
Batch 7/64 loss: -0.21817591786384583
Batch 8/64 loss: -0.2454909086227417
Batch 9/64 loss: -0.2771340608596802
Batch 10/64 loss: -0.2783198058605194
Batch 11/64 loss: -0.2578572630882263
Batch 12/64 loss: -0.2928888201713562
Batch 13/64 loss: -0.2329520881175995
Batch 14/64 loss: -0.2666747570037842
Batch 15/64 loss: -0.2659950256347656
Batch 16/64 loss: -0.23458921909332275
Batch 17/64 loss: -0.2553917169570923
Batch 18/64 loss: -0.2498036026954651
Batch 19/64 loss: -0.23922598361968994
Batch 20/64 loss: -0.26568400859832764
Batch 21/64 loss: -0.23897570371627808
Batch 22/64 loss: -0.2483871579170227
Batch 23/64 loss: -0.2783553898334503
Batch 24/64 loss: -0.25135868787765503
Batch 25/64 loss: -0.2582237124443054
Batch 26/64 loss: -0.2527225911617279
Batch 27/64 loss: -0.22473418712615967
Batch 28/64 loss: -0.255155086517334
Batch 29/64 loss: -0.23306870460510254
Batch 30/64 loss: -0.24001449346542358
Batch 31/64 loss: -0.25573790073394775
Batch 32/64 loss: -0.2707463502883911
Batch 33/64 loss: -0.2724672555923462
Batch 34/64 loss: -0.2516533136367798
Batch 35/64 loss: -0.27843278646469116
Batch 36/64 loss: -0.2530824542045593
Batch 37/64 loss: -0.23854678869247437
Batch 38/64 loss: -0.2543811500072479
Batch 39/64 loss: -0.23377513885498047
Batch 40/64 loss: -0.2572597861289978
Batch 41/64 loss: -0.2726146876811981
Batch 42/64 loss: -0.2517754137516022
Batch 43/64 loss: -0.2827364206314087
Batch 44/64 loss: -0.24028360843658447
Batch 45/64 loss: -0.23752057552337646
Batch 46/64 loss: -0.25944191217422485
Batch 47/64 loss: -0.23550266027450562
Batch 48/64 loss: -0.27558261156082153
Batch 49/64 loss: -0.2594854533672333
Batch 50/64 loss: -0.2770040035247803
Batch 51/64 loss: -0.2558826804161072
Batch 52/64 loss: -0.26451680064201355
Batch 53/64 loss: -0.25774937868118286
Batch 54/64 loss: -0.27494680881500244
Batch 55/64 loss: -0.25652629137039185
Batch 56/64 loss: -0.27635622024536133
Batch 57/64 loss: -0.2596150040626526
Batch 58/64 loss: -0.269547700881958
Batch 59/64 loss: -0.27794188261032104
Batch 60/64 loss: -0.25976258516311646
Batch 61/64 loss: -0.27967995405197144
Batch 62/64 loss: -0.24387139081954956
Batch 63/64 loss: -0.27638769149780273
Batch 64/64 loss: -0.2272489070892334
Epoch 489  Train loss: -0.25762267673716827  Val loss: -0.02866785202649041
Epoch 490
-------------------------------
Batch 1/64 loss: -0.2599807381629944
Batch 2/64 loss: -0.2848071753978729
Batch 3/64 loss: -0.2809428572654724
Batch 4/64 loss: -0.29098743200302124
Batch 5/64 loss: -0.2852299213409424
Batch 6/64 loss: -0.2787739634513855
Batch 7/64 loss: -0.25778913497924805
Batch 8/64 loss: -0.2236148715019226
Batch 9/64 loss: -0.28908899426460266
Batch 10/64 loss: -0.25290346145629883
Batch 11/64 loss: -0.2885357141494751
Batch 12/64 loss: -0.2419978380203247
Batch 13/64 loss: -0.2603498101234436
Batch 14/64 loss: -0.24479705095291138
Batch 15/64 loss: -0.26250094175338745
Batch 16/64 loss: -0.2408551573753357
Batch 17/64 loss: -0.25737684965133667
Batch 18/64 loss: -0.2797797620296478
Batch 19/64 loss: -0.2780366837978363
Batch 20/64 loss: -0.2633436918258667
Batch 21/64 loss: -0.25589507818222046
Batch 22/64 loss: -0.2576366364955902
Batch 23/64 loss: -0.2691863775253296
Batch 24/64 loss: -0.2542627453804016
Batch 25/64 loss: -0.26709824800491333
Batch 26/64 loss: -0.2415541410446167
Batch 27/64 loss: -0.2604227662086487
Batch 28/64 loss: -0.2570161819458008
Batch 29/64 loss: -0.25797319412231445
Batch 30/64 loss: -0.27001237869262695
Batch 31/64 loss: -0.2885889410972595
Batch 32/64 loss: -0.25710001587867737
Batch 33/64 loss: -0.22856736183166504
Batch 34/64 loss: -0.2709587812423706
Batch 35/64 loss: -0.2364872395992279
Batch 36/64 loss: -0.24547100067138672
Batch 37/64 loss: -0.24948084354400635
Batch 38/64 loss: -0.27402299642562866
Batch 39/64 loss: -0.24100565910339355
Batch 40/64 loss: -0.25346875190734863
Batch 41/64 loss: -0.25868651270866394
Batch 42/64 loss: -0.2710331678390503
Batch 43/64 loss: -0.2486412525177002
Batch 44/64 loss: -0.26413142681121826
Batch 45/64 loss: -0.28881651163101196
Batch 46/64 loss: -0.2472267746925354
Batch 47/64 loss: -0.2645975351333618
Batch 48/64 loss: -0.22900152206420898
Batch 49/64 loss: -0.25821036100387573
Batch 50/64 loss: -0.2449234426021576
Batch 51/64 loss: -0.26039350032806396
Batch 52/64 loss: -0.25444865226745605
Batch 53/64 loss: -0.22116604447364807
Batch 54/64 loss: -0.22481921315193176
Batch 55/64 loss: -0.26591628789901733
Batch 56/64 loss: -0.27233177423477173
Batch 57/64 loss: -0.2678140699863434
Batch 58/64 loss: -0.25350087881088257
Batch 59/64 loss: -0.27181482315063477
Batch 60/64 loss: -0.25628191232681274
Batch 61/64 loss: -0.22752410173416138
Batch 62/64 loss: -0.22194284200668335
Batch 63/64 loss: -0.2504662871360779
Batch 64/64 loss: -0.25152626633644104
Epoch 490  Train loss: -0.2583566271791271  Val loss: -0.029646745662099308
Epoch 491
-------------------------------
Batch 1/64 loss: -0.2483302354812622
Batch 2/64 loss: -0.26546311378479004
Batch 3/64 loss: -0.26582762598991394
Batch 4/64 loss: -0.24850314855575562
Batch 5/64 loss: -0.2578525245189667
Batch 6/64 loss: -0.2734573185443878
Batch 7/64 loss: -0.2273416519165039
Batch 8/64 loss: -0.2225465178489685
Batch 9/64 loss: -0.2508394718170166
Batch 10/64 loss: -0.2612808346748352
Batch 11/64 loss: -0.25341683626174927
Batch 12/64 loss: -0.26374441385269165
Batch 13/64 loss: -0.2276773452758789
Batch 14/64 loss: -0.24586868286132812
Batch 15/64 loss: -0.25892382860183716
Batch 16/64 loss: -0.2561572790145874
Batch 17/64 loss: -0.2540374994277954
Batch 18/64 loss: -0.27079886198043823
Batch 19/64 loss: -0.2568933963775635
Batch 20/64 loss: -0.25564754009246826
Batch 21/64 loss: -0.26497799158096313
Batch 22/64 loss: -0.2591075897216797
Batch 23/64 loss: -0.2486012578010559
Batch 24/64 loss: -0.2742862105369568
Batch 25/64 loss: -0.25481992959976196
Batch 26/64 loss: -0.2756951153278351
Batch 27/64 loss: -0.2197810411453247
Batch 28/64 loss: -0.25409287214279175
Batch 29/64 loss: -0.2667979598045349
Batch 30/64 loss: -0.25925523042678833
Batch 31/64 loss: -0.2558973729610443
Batch 32/64 loss: -0.237982839345932
Batch 33/64 loss: -0.25315070152282715
Batch 34/64 loss: -0.23997288942337036
Batch 35/64 loss: -0.27175769209861755
Batch 36/64 loss: -0.2322949767112732
Batch 37/64 loss: -0.28308355808258057
Batch 38/64 loss: -0.2535897493362427
Batch 39/64 loss: -0.2653137445449829
Batch 40/64 loss: -0.25559133291244507
Batch 41/64 loss: -0.2465624213218689
Batch 42/64 loss: -0.27712225914001465
Batch 43/64 loss: -0.22729414701461792
Batch 44/64 loss: -0.26100802421569824
Batch 45/64 loss: -0.21774497628211975
Batch 46/64 loss: -0.265793114900589
Batch 47/64 loss: -0.26236802339553833
Batch 48/64 loss: -0.2663501799106598
Batch 49/64 loss: -0.2666400074958801
Batch 50/64 loss: -0.27748793363571167
Batch 51/64 loss: -0.2520347237586975
Batch 52/64 loss: -0.27511364221572876
Batch 53/64 loss: -0.2552250623703003
Batch 54/64 loss: -0.25832152366638184
Batch 55/64 loss: -0.27520138025283813
Batch 56/64 loss: -0.26334553956985474
Batch 57/64 loss: -0.2691890597343445
Batch 58/64 loss: -0.2548696994781494
Batch 59/64 loss: -0.27283650636672974
Batch 60/64 loss: -0.21484103798866272
Batch 61/64 loss: -0.259735107421875
Batch 62/64 loss: -0.2405388355255127
Batch 63/64 loss: -0.2468101978302002
Batch 64/64 loss: -0.2143089473247528
Epoch 491  Train loss: -0.25502467915123583  Val loss: -0.03023367157506779
Epoch 492
-------------------------------
Batch 1/64 loss: -0.2644924819469452
Batch 2/64 loss: -0.2607842683792114
Batch 3/64 loss: -0.2892826199531555
Batch 4/64 loss: -0.23608046770095825
Batch 5/64 loss: -0.26436692476272583
Batch 6/64 loss: -0.25868499279022217
Batch 7/64 loss: -0.26344388723373413
Batch 8/64 loss: -0.2753191888332367
Batch 9/64 loss: -0.2505909502506256
Batch 10/64 loss: -0.28717929124832153
Batch 11/64 loss: -0.2400560975074768
Batch 12/64 loss: -0.2572290897369385
Batch 13/64 loss: -0.275053471326828
Batch 14/64 loss: -0.245649516582489
Batch 15/64 loss: -0.2659396529197693
Batch 16/64 loss: -0.2549518644809723
Batch 17/64 loss: -0.22729837894439697
Batch 18/64 loss: -0.2550565004348755
Batch 19/64 loss: -0.2719290256500244
Batch 20/64 loss: -0.2767256498336792
Batch 21/64 loss: -0.2493119239807129
Batch 22/64 loss: -0.2822158932685852
Batch 23/64 loss: -0.2716341018676758
Batch 24/64 loss: -0.2557966411113739
Batch 25/64 loss: -0.26840832829475403
Batch 26/64 loss: -0.24030077457427979
Batch 27/64 loss: -0.26949554681777954
Batch 28/64 loss: -0.27772754430770874
Batch 29/64 loss: -0.2627031207084656
Batch 30/64 loss: -0.2660408020019531
Batch 31/64 loss: -0.2493743896484375
Batch 32/64 loss: -0.2501458525657654
Batch 33/64 loss: -0.2665655016899109
Batch 34/64 loss: -0.2893539071083069
Batch 35/64 loss: -0.25214052200317383
Batch 36/64 loss: -0.2688831686973572
Batch 37/64 loss: -0.27944087982177734
Batch 38/64 loss: -0.23697039484977722
Batch 39/64 loss: -0.2732377052307129
Batch 40/64 loss: -0.2199426293373108
Batch 41/64 loss: -0.2614297866821289
Batch 42/64 loss: -0.26525020599365234
Batch 43/64 loss: -0.25678330659866333
Batch 44/64 loss: -0.23516112565994263
Batch 45/64 loss: -0.28388702869415283
Batch 46/64 loss: -0.25816336274147034
Batch 47/64 loss: -0.24756234884262085
Batch 48/64 loss: -0.2802926301956177
Batch 49/64 loss: -0.26242756843566895
Batch 50/64 loss: -0.2612764835357666
Batch 51/64 loss: -0.25594380497932434
Batch 52/64 loss: -0.25606322288513184
Batch 53/64 loss: -0.25141072273254395
Batch 54/64 loss: -0.27261579036712646
Batch 55/64 loss: -0.23953750729560852
Batch 56/64 loss: -0.27749449014663696
Batch 57/64 loss: -0.2626836597919464
Batch 58/64 loss: -0.26537662744522095
Batch 59/64 loss: -0.2783364951610565
Batch 60/64 loss: -0.2654283046722412
Batch 61/64 loss: -0.26804119348526
Batch 62/64 loss: -0.23718661069869995
Batch 63/64 loss: -0.25398126244544983
Batch 64/64 loss: -0.2742573022842407
Epoch 492  Train loss: -0.26120518375845514  Val loss: -0.030565905202295362
Epoch 493
-------------------------------
Batch 1/64 loss: -0.27960315346717834
Batch 2/64 loss: -0.2572028338909149
Batch 3/64 loss: -0.25036126375198364
Batch 4/64 loss: -0.23524922132492065
Batch 5/64 loss: -0.2824767231941223
Batch 6/64 loss: -0.26741278171539307
Batch 7/64 loss: -0.23592013120651245
Batch 8/64 loss: -0.27886950969696045
Batch 9/64 loss: -0.2727341651916504
Batch 10/64 loss: -0.2268221080303192
Batch 11/64 loss: -0.2734019458293915
Batch 12/64 loss: -0.2511092722415924
Batch 13/64 loss: -0.27322152256965637
Batch 14/64 loss: -0.2821788191795349
Batch 15/64 loss: -0.25644558668136597
Batch 16/64 loss: -0.2986917495727539
Batch 17/64 loss: -0.27383753657341003
Batch 18/64 loss: -0.25210803747177124
Batch 19/64 loss: -0.25152263045310974
Batch 20/64 loss: -0.2499748170375824
Batch 21/64 loss: -0.27009081840515137
Batch 22/64 loss: -0.26764386892318726
Batch 23/64 loss: -0.27651143074035645
Batch 24/64 loss: -0.25627458095550537
Batch 25/64 loss: -0.2631837725639343
Batch 26/64 loss: -0.2923799455165863
Batch 27/64 loss: -0.25485503673553467
Batch 28/64 loss: -0.25680726766586304
Batch 29/64 loss: -0.23990267515182495
Batch 30/64 loss: -0.2661687135696411
Batch 31/64 loss: -0.24882075190544128
Batch 32/64 loss: -0.2529619038105011
Batch 33/64 loss: -0.22016578912734985
Batch 34/64 loss: -0.2483830451965332
Batch 35/64 loss: -0.26310956478118896
Batch 36/64 loss: -0.27571815252304077
Batch 37/64 loss: -0.2573931813240051
Batch 38/64 loss: -0.2769473195075989
Batch 39/64 loss: -0.2225639820098877
Batch 40/64 loss: -0.2643203139305115
Batch 41/64 loss: -0.2604016363620758
Batch 42/64 loss: -0.2689642608165741
Batch 43/64 loss: -0.25498777627944946
Batch 44/64 loss: -0.24434000253677368
Batch 45/64 loss: -0.23245728015899658
Batch 46/64 loss: -0.23346197605133057
Batch 47/64 loss: -0.24903345108032227
Batch 48/64 loss: -0.2582511007785797
Batch 49/64 loss: -0.25945964455604553
Batch 50/64 loss: -0.21262213587760925
Batch 51/64 loss: -0.2372664213180542
Batch 52/64 loss: -0.27002331614494324
Batch 53/64 loss: -0.2580835223197937
Batch 54/64 loss: -0.2541985809803009
Batch 55/64 loss: -0.24402129650115967
Batch 56/64 loss: -0.24105459451675415
Batch 57/64 loss: -0.2751213610172272
Batch 58/64 loss: -0.2548764944076538
Batch 59/64 loss: -0.24200403690338135
Batch 60/64 loss: -0.2642165720462799
Batch 61/64 loss: -0.2586822211742401
Batch 62/64 loss: -0.24494796991348267
Batch 63/64 loss: -0.25911277532577515
Batch 64/64 loss: -0.21665430068969727
Epoch 493  Train loss: -0.25668117766286813  Val loss: -0.03373166109688094
Epoch 494
-------------------------------
Batch 1/64 loss: -0.2595231235027313
Batch 2/64 loss: -0.2809680998325348
Batch 3/64 loss: -0.2579948902130127
Batch 4/64 loss: -0.24936801195144653
Batch 5/64 loss: -0.25205713510513306
Batch 6/64 loss: -0.27106785774230957
Batch 7/64 loss: -0.24954229593276978
Batch 8/64 loss: -0.19607049226760864
Batch 9/64 loss: -0.2573395371437073
Batch 10/64 loss: -0.26604175567626953
Batch 11/64 loss: -0.2581877112388611
Batch 12/64 loss: -0.21704202890396118
Batch 13/64 loss: -0.268623411655426
Batch 14/64 loss: -0.24099591374397278
Batch 15/64 loss: -0.24830332398414612
Batch 16/64 loss: -0.2723045349121094
Batch 17/64 loss: -0.23598957061767578
Batch 18/64 loss: -0.23753580451011658
Batch 19/64 loss: -0.2509288787841797
Batch 20/64 loss: -0.2167797088623047
Batch 21/64 loss: -0.2719298303127289
Batch 22/64 loss: -0.25356292724609375
Batch 23/64 loss: -0.2636568248271942
Batch 24/64 loss: -0.24602261185646057
Batch 25/64 loss: -0.26958292722702026
Batch 26/64 loss: -0.28374814987182617
Batch 27/64 loss: -0.21579721570014954
Batch 28/64 loss: -0.24442827701568604
Batch 29/64 loss: -0.25499123334884644
Batch 30/64 loss: -0.25389695167541504
Batch 31/64 loss: -0.2507154941558838
Batch 32/64 loss: -0.2118825614452362
Batch 33/64 loss: -0.26286569237709045
Batch 34/64 loss: -0.253738671541214
Batch 35/64 loss: -0.24466067552566528
Batch 36/64 loss: -0.2540624141693115
Batch 37/64 loss: -0.25975313782691956
Batch 38/64 loss: -0.2530069053173065
Batch 39/64 loss: -0.2580610513687134
Batch 40/64 loss: -0.2570497393608093
Batch 41/64 loss: -0.2742232084274292
Batch 42/64 loss: -0.25462567806243896
Batch 43/64 loss: -0.2886206805706024
Batch 44/64 loss: -0.25731879472732544
Batch 45/64 loss: -0.2472197413444519
Batch 46/64 loss: -0.2432386875152588
Batch 47/64 loss: -0.28273576498031616
Batch 48/64 loss: -0.2538546323776245
Batch 49/64 loss: -0.2761968970298767
Batch 50/64 loss: -0.25528013706207275
Batch 51/64 loss: -0.24224838614463806
Batch 52/64 loss: -0.2608969509601593
Batch 53/64 loss: -0.2357545793056488
Batch 54/64 loss: -0.2577381432056427
Batch 55/64 loss: -0.21175172924995422
Batch 56/64 loss: -0.2528620958328247
Batch 57/64 loss: -0.22106695175170898
Batch 58/64 loss: -0.24819186329841614
Batch 59/64 loss: -0.26490992307662964
Batch 60/64 loss: -0.2489093542098999
Batch 61/64 loss: -0.23616361618041992
Batch 62/64 loss: -0.2426249384880066
Batch 63/64 loss: -0.22609439492225647
Batch 64/64 loss: -0.25880593061447144
Epoch 494  Train loss: -0.25139935647740086  Val loss: -0.02823750649121209
Epoch 495
-------------------------------
Batch 1/64 loss: -0.2756604552268982
Batch 2/64 loss: -0.2477489411830902
Batch 3/64 loss: -0.26888707280158997
Batch 4/64 loss: -0.25016388297080994
Batch 5/64 loss: -0.2721245288848877
Batch 6/64 loss: -0.28612738847732544
Batch 7/64 loss: -0.2692500948905945
Batch 8/64 loss: -0.2915576696395874
Batch 9/64 loss: -0.24052822589874268
Batch 10/64 loss: -0.25154563784599304
Batch 11/64 loss: -0.25650110840797424
Batch 12/64 loss: -0.27613356709480286
Batch 13/64 loss: -0.2662985920906067
Batch 14/64 loss: -0.25501999258995056
Batch 15/64 loss: -0.2413543462753296
Batch 16/64 loss: -0.242177814245224
Batch 17/64 loss: -0.2507050931453705
Batch 18/64 loss: -0.24622738361358643
Batch 19/64 loss: -0.272305428981781
Batch 20/64 loss: -0.2629976272583008
Batch 21/64 loss: -0.259634792804718
Batch 22/64 loss: -0.268009752035141
Batch 23/64 loss: -0.27345067262649536
Batch 24/64 loss: -0.2609301805496216
Batch 25/64 loss: -0.2506583034992218
Batch 26/64 loss: -0.26375633478164673
Batch 27/64 loss: -0.2916224002838135
Batch 28/64 loss: -0.23140054941177368
Batch 29/64 loss: -0.2755361795425415
Batch 30/64 loss: -0.2670576572418213
Batch 31/64 loss: -0.25358712673187256
Batch 32/64 loss: -0.2471875548362732
Batch 33/64 loss: -0.26764237880706787
Batch 34/64 loss: -0.25268006324768066
Batch 35/64 loss: -0.24128007888793945
Batch 36/64 loss: -0.269586980342865
Batch 37/64 loss: -0.2377302050590515
Batch 38/64 loss: -0.2645307779312134
Batch 39/64 loss: -0.26232215762138367
Batch 40/64 loss: -0.21114319562911987
Batch 41/64 loss: -0.23993948101997375
Batch 42/64 loss: -0.25527533888816833
Batch 43/64 loss: -0.2673244774341583
Batch 44/64 loss: -0.2607068419456482
Batch 45/64 loss: -0.24808549880981445
Batch 46/64 loss: -0.2381436824798584
Batch 47/64 loss: -0.2720283269882202
Batch 48/64 loss: -0.2687237858772278
Batch 49/64 loss: -0.25826704502105713
Batch 50/64 loss: -0.2759590148925781
Batch 51/64 loss: -0.2548266351222992
Batch 52/64 loss: -0.2490537166595459
Batch 53/64 loss: -0.27325937151908875
Batch 54/64 loss: -0.2690449357032776
Batch 55/64 loss: -0.2751028835773468
Batch 56/64 loss: -0.2657804787158966
Batch 57/64 loss: -0.22631433606147766
Batch 58/64 loss: -0.25637131929397583
Batch 59/64 loss: -0.27892792224884033
Batch 60/64 loss: -0.28565341234207153
Batch 61/64 loss: -0.23060566186904907
Batch 62/64 loss: -0.2563261091709137
Batch 63/64 loss: -0.2633545398712158
Batch 64/64 loss: -0.26203274726867676
Epoch 495  Train loss: -0.25942998540167717  Val loss: -0.027830380754372507
Epoch 496
-------------------------------
Batch 1/64 loss: -0.25015223026275635
Batch 2/64 loss: -0.2658501863479614
Batch 3/64 loss: -0.24812012910842896
Batch 4/64 loss: -0.28820931911468506
Batch 5/64 loss: -0.2778609097003937
Batch 6/64 loss: -0.27005359530448914
Batch 7/64 loss: -0.26214906573295593
Batch 8/64 loss: -0.25122302770614624
Batch 9/64 loss: -0.25371450185775757
Batch 10/64 loss: -0.2938333749771118
Batch 11/64 loss: -0.2568906545639038
Batch 12/64 loss: -0.27000072598457336
Batch 13/64 loss: -0.2735709249973297
Batch 14/64 loss: -0.2809566557407379
Batch 15/64 loss: -0.28874900937080383
Batch 16/64 loss: -0.2833247184753418
Batch 17/64 loss: -0.25241535902023315
Batch 18/64 loss: -0.23586583137512207
Batch 19/64 loss: -0.2818243205547333
Batch 20/64 loss: -0.2696743905544281
Batch 21/64 loss: -0.24503231048583984
Batch 22/64 loss: -0.2588932514190674
Batch 23/64 loss: -0.2510015368461609
Batch 24/64 loss: -0.26382312178611755
Batch 25/64 loss: -0.25157642364501953
Batch 26/64 loss: -0.2416849434375763
Batch 27/64 loss: -0.26190048456192017
Batch 28/64 loss: -0.26466241478919983
Batch 29/64 loss: -0.2720757722854614
Batch 30/64 loss: -0.2669747471809387
Batch 31/64 loss: -0.26599499583244324
Batch 32/64 loss: -0.2541278004646301
Batch 33/64 loss: -0.2802414894104004
Batch 34/64 loss: -0.2589600086212158
Batch 35/64 loss: -0.23221391439437866
Batch 36/64 loss: -0.20926296710968018
Batch 37/64 loss: -0.24131032824516296
Batch 38/64 loss: -0.25113144516944885
Batch 39/64 loss: -0.24457627534866333
Batch 40/64 loss: -0.25037097930908203
Batch 41/64 loss: -0.2721327543258667
Batch 42/64 loss: -0.22583657503128052
Batch 43/64 loss: -0.2628159821033478
Batch 44/64 loss: -0.26947858929634094
Batch 45/64 loss: -0.2607288062572479
Batch 46/64 loss: -0.2745319902896881
Batch 47/64 loss: -0.26995187997817993
Batch 48/64 loss: -0.24067682027816772
Batch 49/64 loss: -0.19981110095977783
Batch 50/64 loss: -0.26850634813308716
Batch 51/64 loss: -0.2590137720108032
Batch 52/64 loss: -0.24002444744110107
Batch 53/64 loss: -0.2671385407447815
Batch 54/64 loss: -0.2450951337814331
Batch 55/64 loss: -0.2697727382183075
Batch 56/64 loss: -0.25975868105888367
Batch 57/64 loss: -0.2850257158279419
Batch 58/64 loss: -0.2801116108894348
Batch 59/64 loss: -0.2542343735694885
Batch 60/64 loss: -0.24007278680801392
Batch 61/64 loss: -0.2384527325630188
Batch 62/64 loss: -0.27991434931755066
Batch 63/64 loss: -0.2656998038291931
Batch 64/64 loss: -0.25763946771621704
Epoch 496  Train loss: -0.25948657685635135  Val loss: -0.03117651771433984
Epoch 497
-------------------------------
Batch 1/64 loss: -0.2653244137763977
Batch 2/64 loss: -0.255829393863678
Batch 3/64 loss: -0.26657819747924805
Batch 4/64 loss: -0.2905401885509491
Batch 5/64 loss: -0.26668882369995117
Batch 6/64 loss: -0.2494259774684906
Batch 7/64 loss: -0.26115334033966064
Batch 8/64 loss: -0.2719987630844116
Batch 9/64 loss: -0.2666124999523163
Batch 10/64 loss: -0.23810070753097534
Batch 11/64 loss: -0.2517823576927185
Batch 12/64 loss: -0.2666160762310028
Batch 13/64 loss: -0.24038603901863098
Batch 14/64 loss: -0.22239148616790771
Batch 15/64 loss: -0.2262929081916809
Batch 16/64 loss: -0.2703580856323242
Batch 17/64 loss: -0.262742817401886
Batch 18/64 loss: -0.23189496994018555
Batch 19/64 loss: -0.272206574678421
Batch 20/64 loss: -0.26298996806144714
Batch 21/64 loss: -0.2915869951248169
Batch 22/64 loss: -0.28577420115470886
Batch 23/64 loss: -0.26706790924072266
Batch 24/64 loss: -0.2605575621128082
Batch 25/64 loss: -0.23877674341201782
Batch 26/64 loss: -0.2739446759223938
Batch 27/64 loss: -0.2505796551704407
Batch 28/64 loss: -0.21769648790359497
Batch 29/64 loss: -0.27325788140296936
Batch 30/64 loss: -0.24367469549179077
Batch 31/64 loss: -0.2599441707134247
Batch 32/64 loss: -0.2830438017845154
Batch 33/64 loss: -0.2443128228187561
Batch 34/64 loss: -0.26308444142341614
Batch 35/64 loss: -0.27603769302368164
Batch 36/64 loss: -0.23459166288375854
Batch 37/64 loss: -0.2714685797691345
Batch 38/64 loss: -0.26945123076438904
Batch 39/64 loss: -0.27704063057899475
Batch 40/64 loss: -0.27420520782470703
Batch 41/64 loss: -0.27206578850746155
Batch 42/64 loss: -0.2625805139541626
Batch 43/64 loss: -0.24887201189994812
Batch 44/64 loss: -0.25089651346206665
Batch 45/64 loss: -0.2791314125061035
Batch 46/64 loss: -0.2789391875267029
Batch 47/64 loss: -0.27448052167892456
Batch 48/64 loss: -0.2742343842983246
Batch 49/64 loss: -0.2541537880897522
Batch 50/64 loss: -0.26061809062957764
Batch 51/64 loss: -0.2682704031467438
Batch 52/64 loss: -0.2848215103149414
Batch 53/64 loss: -0.22402924299240112
Batch 54/64 loss: -0.25484877824783325
Batch 55/64 loss: -0.26824575662612915
Batch 56/64 loss: -0.2783036231994629
Batch 57/64 loss: -0.29340803623199463
Batch 58/64 loss: -0.26034456491470337
Batch 59/64 loss: -0.28241997957229614
Batch 60/64 loss: -0.2934187054634094
Batch 61/64 loss: -0.25283747911453247
Batch 62/64 loss: -0.28595027327537537
Batch 63/64 loss: -0.25918859243392944
Batch 64/64 loss: -0.2767685055732727
Epoch 497  Train loss: -0.26299052822823615  Val loss: -0.029337824005441566
Epoch 498
-------------------------------
Batch 1/64 loss: -0.28558778762817383
Batch 2/64 loss: -0.2602997422218323
Batch 3/64 loss: -0.2689964175224304
Batch 4/64 loss: -0.28285449743270874
Batch 5/64 loss: -0.27809157967567444
Batch 6/64 loss: -0.265438050031662
Batch 7/64 loss: -0.2663725018501282
Batch 8/64 loss: -0.2747948169708252
Batch 9/64 loss: -0.2921372056007385
Batch 10/64 loss: -0.2541922926902771
Batch 11/64 loss: -0.2583591639995575
Batch 12/64 loss: -0.2825948894023895
Batch 13/64 loss: -0.2718743681907654
Batch 14/64 loss: -0.2527042627334595
Batch 15/64 loss: -0.26414740085601807
Batch 16/64 loss: -0.2667825520038605
Batch 17/64 loss: -0.27093929052352905
Batch 18/64 loss: -0.2676597833633423
Batch 19/64 loss: -0.2634168863296509
Batch 20/64 loss: -0.2788674235343933
Batch 21/64 loss: -0.2908956706523895
Batch 22/64 loss: -0.27124059200286865
Batch 23/64 loss: -0.2914949655532837
Batch 24/64 loss: -0.2847836911678314
Batch 25/64 loss: -0.2886582016944885
Batch 26/64 loss: -0.2611949145793915
Batch 27/64 loss: -0.25953811407089233
Batch 28/64 loss: -0.2771259844303131
Batch 29/64 loss: -0.2666943073272705
Batch 30/64 loss: -0.2610655426979065
Batch 31/64 loss: -0.26962393522262573
Batch 32/64 loss: -0.26941731572151184
Batch 33/64 loss: -0.2133750319480896
Batch 34/64 loss: -0.2449297308921814
Batch 35/64 loss: -0.2745584547519684
Batch 36/64 loss: -0.253662645816803
Batch 37/64 loss: -0.23392048478126526
Batch 38/64 loss: -0.2686707377433777
Batch 39/64 loss: -0.2352551817893982
Batch 40/64 loss: -0.27008020877838135
Batch 41/64 loss: -0.2861730456352234
Batch 42/64 loss: -0.2775360345840454
Batch 43/64 loss: -0.20466113090515137
Batch 44/64 loss: -0.24788126349449158
Batch 45/64 loss: -0.2673209607601166
Batch 46/64 loss: -0.25901633501052856
Batch 47/64 loss: -0.24519622325897217
Batch 48/64 loss: -0.22732210159301758
Batch 49/64 loss: -0.270102322101593
Batch 50/64 loss: -0.2315964698791504
Batch 51/64 loss: -0.24431806802749634
Batch 52/64 loss: -0.2534758150577545
Batch 53/64 loss: -0.25824713706970215
Batch 54/64 loss: -0.2383890151977539
Batch 55/64 loss: -0.25404465198516846
Batch 56/64 loss: -0.25934189558029175
Batch 57/64 loss: -0.2629079222679138
Batch 58/64 loss: -0.25988101959228516
Batch 59/64 loss: -0.2609896659851074
Batch 60/64 loss: -0.2612573504447937
Batch 61/64 loss: -0.23937386274337769
Batch 62/64 loss: -0.2767535448074341
Batch 63/64 loss: -0.26971235871315
Batch 64/64 loss: -0.21527275443077087
Epoch 498  Train loss: -0.2621058727011961  Val loss: -0.03097255205370716
Epoch 499
-------------------------------
Batch 1/64 loss: -0.26376819610595703
Batch 2/64 loss: -0.2515816390514374
Batch 3/64 loss: -0.2621335983276367
Batch 4/64 loss: -0.2583322525024414
Batch 5/64 loss: -0.2485952079296112
Batch 6/64 loss: -0.24019339680671692
Batch 7/64 loss: -0.27553194761276245
Batch 8/64 loss: -0.26239249110221863
Batch 9/64 loss: -0.2744031846523285
Batch 10/64 loss: -0.2670122981071472
Batch 11/64 loss: -0.23519277572631836
Batch 12/64 loss: -0.24974548816680908
Batch 13/64 loss: -0.2707563638687134
Batch 14/64 loss: -0.2805159389972687
Batch 15/64 loss: -0.27732348442077637
Batch 16/64 loss: -0.2506781816482544
Batch 17/64 loss: -0.25995123386383057
Batch 18/64 loss: -0.25839585065841675
Batch 19/64 loss: -0.26730576157569885
Batch 20/64 loss: -0.2526191473007202
Batch 21/64 loss: -0.2973599433898926
Batch 22/64 loss: -0.2690209150314331
Batch 23/64 loss: -0.25051891803741455
Batch 24/64 loss: -0.2776991128921509
Batch 25/64 loss: -0.2747114598751068
Batch 26/64 loss: -0.25384578108787537
Batch 27/64 loss: -0.2735849916934967
Batch 28/64 loss: -0.25527602434158325
Batch 29/64 loss: -0.27679798007011414
Batch 30/64 loss: -0.27123552560806274
Batch 31/64 loss: -0.24013370275497437
Batch 32/64 loss: -0.2724108099937439
Batch 33/64 loss: -0.23676007986068726
Batch 34/64 loss: -0.23485064506530762
Batch 35/64 loss: -0.2455628514289856
Batch 36/64 loss: -0.26327288150787354
Batch 37/64 loss: -0.27498987317085266
Batch 38/64 loss: -0.271179735660553
Batch 39/64 loss: -0.23340463638305664
Batch 40/64 loss: -0.26167958974838257
Batch 41/64 loss: -0.23901036381721497
Batch 42/64 loss: -0.28447800874710083
Batch 43/64 loss: -0.27576592564582825
Batch 44/64 loss: -0.2496107816696167
Batch 45/64 loss: -0.2529922425746918
Batch 46/64 loss: -0.27220210433006287
Batch 47/64 loss: -0.2594830393791199
Batch 48/64 loss: -0.2641153335571289
Batch 49/64 loss: -0.21199798583984375
Batch 50/64 loss: -0.22586649656295776
Batch 51/64 loss: -0.26443201303482056
Batch 52/64 loss: -0.2668049931526184
Batch 53/64 loss: -0.2164548635482788
Batch 54/64 loss: -0.2563750147819519
Batch 55/64 loss: -0.24603217840194702
Batch 56/64 loss: -0.23565632104873657
Batch 57/64 loss: -0.2663964629173279
Batch 58/64 loss: -0.28770536184310913
Batch 59/64 loss: -0.2717249393463135
Batch 60/64 loss: -0.261944055557251
Batch 61/64 loss: -0.2473347783088684
Batch 62/64 loss: -0.26188594102859497
Batch 63/64 loss: -0.24390187859535217
Batch 64/64 loss: -0.21125346422195435
Epoch 499  Train loss: -0.25821702082951864  Val loss: -0.02939796980303997
Epoch 500
-------------------------------
Batch 1/64 loss: -0.2139233946800232
Batch 2/64 loss: -0.2544478178024292
Batch 3/64 loss: -0.27788209915161133
Batch 4/64 loss: -0.2674870789051056
Batch 5/64 loss: -0.2578250467777252
Batch 6/64 loss: -0.27719730138778687
Batch 7/64 loss: -0.2760045528411865
Batch 8/64 loss: -0.23331218957901
Batch 9/64 loss: -0.2649257779121399
Batch 10/64 loss: -0.2900594174861908
Batch 11/64 loss: -0.2517763376235962
Batch 12/64 loss: -0.2663060426712036
Batch 13/64 loss: -0.24652141332626343
Batch 14/64 loss: -0.25329703092575073
Batch 15/64 loss: -0.26126205921173096
Batch 16/64 loss: -0.26584118604660034
Batch 17/64 loss: -0.2603551149368286
Batch 18/64 loss: -0.23680633306503296
Batch 19/64 loss: -0.2925480008125305
Batch 20/64 loss: -0.28174349665641785
Batch 21/64 loss: -0.2700818181037903
Batch 22/64 loss: -0.2505895495414734
Batch 23/64 loss: -0.28156155347824097
Batch 24/64 loss: -0.27435943484306335
Batch 25/64 loss: -0.22257304191589355
Batch 26/64 loss: -0.2775459289550781
Batch 27/64 loss: -0.25004082918167114
Batch 28/64 loss: -0.2896634638309479
Batch 29/64 loss: -0.27865445613861084
Batch 30/64 loss: -0.24927756190299988
Batch 31/64 loss: -0.2253904938697815
Batch 32/64 loss: -0.2700778543949127
Batch 33/64 loss: -0.2511509656906128
Batch 34/64 loss: -0.24812215566635132
Batch 35/64 loss: -0.28049421310424805
Batch 36/64 loss: -0.2474622130393982
Batch 37/64 loss: -0.28528928756713867
Batch 38/64 loss: -0.26026058197021484
Batch 39/64 loss: -0.2653486728668213
Batch 40/64 loss: -0.2488185465335846
Batch 41/64 loss: -0.2876909673213959
Batch 42/64 loss: -0.25533270835876465
Batch 43/64 loss: -0.2625541090965271
Batch 44/64 loss: -0.2624967098236084
Batch 45/64 loss: -0.2859860062599182
Batch 46/64 loss: -0.2610296607017517
Batch 47/64 loss: -0.2686801552772522
Batch 48/64 loss: -0.2790449261665344
Batch 49/64 loss: -0.2770918309688568
Batch 50/64 loss: -0.2924569249153137
Batch 51/64 loss: -0.2629532217979431
Batch 52/64 loss: -0.27646780014038086
Batch 53/64 loss: -0.2565423548221588
Batch 54/64 loss: -0.2723049521446228
Batch 55/64 loss: -0.2618562579154968
Batch 56/64 loss: -0.2614270746707916
Batch 57/64 loss: -0.27628543972969055
Batch 58/64 loss: -0.28006309270858765
Batch 59/64 loss: -0.24901634454727173
Batch 60/64 loss: -0.26147425174713135
Batch 61/64 loss: -0.2524767518043518
Batch 62/64 loss: -0.27634063363075256
Batch 63/64 loss: -0.23157000541687012
Batch 64/64 loss: -0.23247981071472168
Epoch 500  Train loss: -0.2632437074885649  Val loss: -0.02854671982145801
SLIC undersegmentation error: 0.12412920962199316
SLIC inter-cluster variation: 0.13904419774313004
SLIC number of superpixels: 21483
SLIC superpixels per image: 73.82474226804123
Model loaded
Test metrics:
-0.05697923345664113 0.35318762886597943 18.118505712447117 tensor(0.2741, dtype=torch.float64) 0.8886830762654508 3.9848965197876964 28073
Inference time: 0.004000720289564624 seconds
Relabeled undersegmentation error: 0.08912852233676963
Relabeled inter-cluster variation: 0.04604945786957989
Relabeled mean superpixels count: 384.4192439862543
Original mean superpixels count: 96.47079037800687
Done!
Job id: 486220
Job id: 487237
