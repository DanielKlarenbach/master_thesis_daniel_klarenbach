Started preprocessing dataset
Number of training samples: 2040
Number of validation samples: 582
Number of testing samples: 291
Using cuda device
Epoch 1
-------------------------------
Batch 1/64 loss: 0.4232172966003418
Batch 2/64 loss: 0.3620975613594055
Batch 3/64 loss: 0.3442288637161255
Batch 4/64 loss: 0.3463943600654602
Batch 5/64 loss: 0.337887167930603
Batch 6/64 loss: 0.3355727195739746
Batch 7/64 loss: 0.3361166715621948
Batch 8/64 loss: 0.3296334743499756
Batch 9/64 loss: 0.3336666226387024
Batch 10/64 loss: 0.3312009572982788
Batch 11/64 loss: 0.33100104331970215
Batch 12/64 loss: 0.328305721282959
Batch 13/64 loss: 0.3285056948661804
Batch 14/64 loss: 0.3281126022338867
Batch 15/64 loss: 0.3262265920639038
Batch 16/64 loss: 0.3247607350349426
Batch 17/64 loss: 0.3268599510192871
Batch 18/64 loss: 0.3247886896133423
Batch 19/64 loss: 0.3240375518798828
Batch 20/64 loss: 0.3229649066925049
Batch 21/64 loss: 0.32619190216064453
Batch 22/64 loss: 0.31864452362060547
Batch 23/64 loss: 0.3214808702468872
Batch 24/64 loss: 0.3168950080871582
Batch 25/64 loss: 0.3224028944969177
Batch 26/64 loss: 0.32515615224838257
Batch 27/64 loss: 0.31289392709732056
Batch 28/64 loss: 0.31677114963531494
Batch 29/64 loss: 0.31858932971954346
Batch 30/64 loss: 0.32199376821517944
Batch 31/64 loss: 0.3151666522026062
Batch 32/64 loss: 0.3167680501937866
Batch 33/64 loss: 0.30834972858428955
Batch 34/64 loss: 0.3127478361129761
Batch 35/64 loss: 0.314090371131897
Batch 36/64 loss: 0.306093692779541
Batch 37/64 loss: 0.3092385530471802
Batch 38/64 loss: 0.30506205558776855
Batch 39/64 loss: 0.3093331456184387
Batch 40/64 loss: 0.31475579738616943
Batch 41/64 loss: 0.3103691339492798
Batch 42/64 loss: 0.30946409702301025
Batch 43/64 loss: 0.3082824945449829
Batch 44/64 loss: 0.30547666549682617
Batch 45/64 loss: 0.305078387260437
Batch 46/64 loss: 0.30688536167144775
Batch 47/64 loss: 0.3065093159675598
Batch 48/64 loss: 0.30766528844833374
Batch 49/64 loss: 0.3090323209762573
Batch 50/64 loss: 0.3083850145339966
Batch 51/64 loss: 0.3019883632659912
Batch 52/64 loss: 0.30108410120010376
Batch 53/64 loss: 0.314217209815979
Batch 54/64 loss: 0.30527687072753906
Batch 55/64 loss: 0.3055753707885742
Batch 56/64 loss: 0.30674856901168823
Batch 57/64 loss: 0.3054856061935425
Batch 58/64 loss: 0.30313074588775635
Batch 59/64 loss: 0.30158448219299316
Batch 60/64 loss: 0.29625409841537476
Batch 61/64 loss: 0.3046787977218628
Batch 62/64 loss: 0.29867780208587646
Batch 63/64 loss: 0.3045176863670349
Batch 64/64 loss: 0.3047176003456116
Epoch 1  Train loss: 0.3186370286287046  Val loss: 0.3094233893037252
Saving best model, epoch: 1
Epoch 2
-------------------------------
Batch 1/64 loss: 0.30139005184173584
Batch 2/64 loss: 0.30788832902908325
Batch 3/64 loss: 0.3077716827392578
Batch 4/64 loss: 0.30233192443847656
Batch 5/64 loss: 0.30003225803375244
Batch 6/64 loss: 0.2992602586746216
Batch 7/64 loss: 0.3056983947753906
Batch 8/64 loss: 0.29693442583084106
Batch 9/64 loss: 0.297260046005249
Batch 10/64 loss: 0.3091268539428711
Batch 11/64 loss: 0.2931244969367981
Batch 12/64 loss: 0.3006080389022827
Batch 13/64 loss: 0.29485297203063965
Batch 14/64 loss: 0.2932252883911133
Batch 15/64 loss: 0.302998423576355
Batch 16/64 loss: 0.30119407176971436
Batch 17/64 loss: 0.29699671268463135
Batch 18/64 loss: 0.3009189963340759
Batch 19/64 loss: 0.29306507110595703
Batch 20/64 loss: 0.30274736881256104
Batch 21/64 loss: 0.297233521938324
Batch 22/64 loss: 0.29838478565216064
Batch 23/64 loss: 0.3033679723739624
Batch 24/64 loss: 0.29942846298217773
Batch 25/64 loss: 0.29702138900756836
Batch 26/64 loss: 0.2981213331222534
Batch 27/64 loss: 0.29473042488098145
Batch 28/64 loss: 0.30154353380203247
Batch 29/64 loss: 0.2955435514450073
Batch 30/64 loss: 0.29604852199554443
Batch 31/64 loss: 0.30308997631073
Batch 32/64 loss: 0.29922258853912354
Batch 33/64 loss: 0.2952765226364136
Batch 34/64 loss: 0.2986714839935303
Batch 35/64 loss: 0.29608774185180664
Batch 36/64 loss: 0.2964738607406616
Batch 37/64 loss: 0.29878783226013184
Batch 38/64 loss: 0.29131627082824707
Batch 39/64 loss: 0.29530733823776245
Batch 40/64 loss: 0.29776883125305176
Batch 41/64 loss: 0.2938523292541504
Batch 42/64 loss: 0.2878187298774719
Batch 43/64 loss: 0.29953980445861816
Batch 44/64 loss: 0.30213379859924316
Batch 45/64 loss: 0.2898139953613281
Batch 46/64 loss: 0.29998666048049927
Batch 47/64 loss: 0.29690420627593994
Batch 48/64 loss: 0.3004702925682068
Batch 49/64 loss: 0.29709434509277344
Batch 50/64 loss: 0.2835003137588501
Batch 51/64 loss: 0.29074031114578247
Batch 52/64 loss: 0.28965818881988525
Batch 53/64 loss: 0.2932091951370239
Batch 54/64 loss: 0.29120397567749023
Batch 55/64 loss: 0.29187941551208496
Batch 56/64 loss: 0.2923828363418579
Batch 57/64 loss: 0.2908231019973755
Batch 58/64 loss: 0.2926875352859497
Batch 59/64 loss: 0.2901885509490967
Batch 60/64 loss: 0.28416556119918823
Batch 61/64 loss: 0.28804540634155273
Batch 62/64 loss: 0.2898330092430115
Batch 63/64 loss: 0.29533350467681885
Batch 64/64 loss: 0.2886536121368408
Epoch 2  Train loss: 0.29660559822531307  Val loss: 0.29511248491883685
Saving best model, epoch: 2
Epoch 3
-------------------------------
Batch 1/64 loss: 0.2931709885597229
Batch 2/64 loss: 0.2858673334121704
Batch 3/64 loss: 0.29021674394607544
Batch 4/64 loss: 0.29270052909851074
Batch 5/64 loss: 0.2880576252937317
Batch 6/64 loss: 0.28505611419677734
Batch 7/64 loss: 0.29680997133255005
Batch 8/64 loss: 0.2921254634857178
Batch 9/64 loss: 0.2953146696090698
Batch 10/64 loss: 0.2968239188194275
Batch 11/64 loss: 0.292309045791626
Batch 12/64 loss: 0.28748780488967896
Batch 13/64 loss: 0.28810131549835205
Batch 14/64 loss: 0.2952723503112793
Batch 15/64 loss: 0.2869049310684204
Batch 16/64 loss: 0.3030116558074951
Batch 17/64 loss: 0.28968822956085205
Batch 18/64 loss: 0.30081355571746826
Batch 19/64 loss: 0.28931868076324463
Batch 20/64 loss: 0.2925429344177246
Batch 21/64 loss: 0.28474295139312744
Batch 22/64 loss: 0.2902625799179077
Batch 23/64 loss: 0.29756617546081543
Batch 24/64 loss: 0.29619425535202026
Batch 25/64 loss: 0.29719078540802
Batch 26/64 loss: 0.2877371311187744
Batch 27/64 loss: 0.2873566150665283
Batch 28/64 loss: 0.29903125762939453
Batch 29/64 loss: 0.28995978832244873
Batch 30/64 loss: 0.28190523386001587
Batch 31/64 loss: 0.2844393253326416
Batch 32/64 loss: 0.2902604341506958
Batch 33/64 loss: 0.28835785388946533
Batch 34/64 loss: 0.2802249789237976
Batch 35/64 loss: 0.27876245975494385
Batch 36/64 loss: 0.29890626668930054
Batch 37/64 loss: 0.2860943078994751
Batch 38/64 loss: 0.28647279739379883
Batch 39/64 loss: 0.29762935638427734
Batch 40/64 loss: 0.2871028184890747
Batch 41/64 loss: 0.2936595678329468
Batch 42/64 loss: 0.2861127257347107
Batch 43/64 loss: 0.2948451042175293
Batch 44/64 loss: 0.29284167289733887
Batch 45/64 loss: 0.28964680433273315
Batch 46/64 loss: 0.28440582752227783
Batch 47/64 loss: 0.2881588339805603
Batch 48/64 loss: 0.2815513014793396
Batch 49/64 loss: 0.28402090072631836
Batch 50/64 loss: 0.2886124849319458
Batch 51/64 loss: 0.2877398729324341
Batch 52/64 loss: 0.2962688207626343
Batch 53/64 loss: 0.28658556938171387
Batch 54/64 loss: 0.289483904838562
Batch 55/64 loss: 0.2887824773788452
Batch 56/64 loss: 0.2886698246002197
Batch 57/64 loss: 0.29384952783584595
Batch 58/64 loss: 0.2887521982192993
Batch 59/64 loss: 0.28330111503601074
Batch 60/64 loss: 0.28802478313446045
Batch 61/64 loss: 0.2877861261367798
Batch 62/64 loss: 0.29257774353027344
Batch 63/64 loss: 0.2880324721336365
Batch 64/64 loss: 0.2855721712112427
Epoch 3  Train loss: 0.2900341964235493  Val loss: 0.28909795226919693
Saving best model, epoch: 3
Epoch 4
-------------------------------
Batch 1/64 loss: 0.28340667486190796
Batch 2/64 loss: 0.2902466058731079
Batch 3/64 loss: 0.2905248999595642
Batch 4/64 loss: 0.2854595184326172
Batch 5/64 loss: 0.29019594192504883
Batch 6/64 loss: 0.2860109210014343
Batch 7/64 loss: 0.2892155647277832
Batch 8/64 loss: 0.2855970859527588
Batch 9/64 loss: 0.28337907791137695
Batch 10/64 loss: 0.2807197570800781
Batch 11/64 loss: 0.27181732654571533
Batch 12/64 loss: 0.2887856960296631
Batch 13/64 loss: 0.2878061532974243
Batch 14/64 loss: 0.28599703311920166
Batch 15/64 loss: 0.2878240942955017
Batch 16/64 loss: 0.27936190366744995
Batch 17/64 loss: 0.2828502058982849
Batch 18/64 loss: 0.2872876524925232
Batch 19/64 loss: 0.2781717777252197
Batch 20/64 loss: 0.28327786922454834
Batch 21/64 loss: 0.29025256633758545
Batch 22/64 loss: 0.28704309463500977
Batch 23/64 loss: 0.2803630232810974
Batch 24/64 loss: 0.28103840351104736
Batch 25/64 loss: 0.2784807085990906
Batch 26/64 loss: 0.2835122346878052
Batch 27/64 loss: 0.2952108383178711
Batch 28/64 loss: 0.2841782569885254
Batch 29/64 loss: 0.28645336627960205
Batch 30/64 loss: 0.29474151134490967
Batch 31/64 loss: 0.2879107594490051
Batch 32/64 loss: 0.2835419178009033
Batch 33/64 loss: 0.27934062480926514
Batch 34/64 loss: 0.2868351936340332
Batch 35/64 loss: 0.28317534923553467
Batch 36/64 loss: 0.2752641439437866
Batch 37/64 loss: 0.2817416191101074
Batch 38/64 loss: 0.2951357960700989
Batch 39/64 loss: 0.2795090079307556
Batch 40/64 loss: 0.280759334564209
Batch 41/64 loss: 0.29872071743011475
Batch 42/64 loss: 0.28320926427841187
Batch 43/64 loss: 0.2874032258987427
Batch 44/64 loss: 0.2816019058227539
Batch 45/64 loss: 0.2784003019332886
Batch 46/64 loss: 0.28543591499328613
Batch 47/64 loss: 0.2768309712409973
Batch 48/64 loss: 0.2876896858215332
Batch 49/64 loss: 0.2830735445022583
Batch 50/64 loss: 0.2911282777786255
Batch 51/64 loss: 0.2789064049720764
Batch 52/64 loss: 0.279509961605072
Batch 53/64 loss: 0.2843344211578369
Batch 54/64 loss: 0.2789584994316101
Batch 55/64 loss: 0.2902461290359497
Batch 56/64 loss: 0.2780565023422241
Batch 57/64 loss: 0.28489863872528076
Batch 58/64 loss: 0.2883734703063965
Batch 59/64 loss: 0.27742260694503784
Batch 60/64 loss: 0.2799926996231079
Batch 61/64 loss: 0.29073774814605713
Batch 62/64 loss: 0.2840486764907837
Batch 63/64 loss: 0.28653210401535034
Batch 64/64 loss: 0.2826259136199951
Epoch 4  Train loss: 0.2845475234237372  Val loss: 0.2831460909335474
Saving best model, epoch: 4
Epoch 5
-------------------------------
Batch 1/64 loss: 0.2847250699996948
Batch 2/64 loss: 0.2841978073120117
Batch 3/64 loss: 0.2759550213813782
Batch 4/64 loss: 0.2803346514701843
Batch 5/64 loss: 0.27595531940460205
Batch 6/64 loss: 0.28707361221313477
Batch 7/64 loss: 0.28305792808532715
Batch 8/64 loss: 0.2838780879974365
Batch 9/64 loss: 0.28827738761901855
Batch 10/64 loss: 0.28290724754333496
Batch 11/64 loss: 0.29044032096862793
Batch 12/64 loss: 0.28152692317962646
Batch 13/64 loss: 0.28075242042541504
Batch 14/64 loss: 0.2858562469482422
Batch 15/64 loss: 0.28319042921066284
Batch 16/64 loss: 0.2929999828338623
Batch 17/64 loss: 0.28195881843566895
Batch 18/64 loss: 0.28815793991088867
Batch 19/64 loss: 0.2937465310096741
Batch 20/64 loss: 0.2929372191429138
Batch 21/64 loss: 0.2785743474960327
Batch 22/64 loss: 0.2901037335395813
Batch 23/64 loss: 0.2799258232116699
Batch 24/64 loss: 0.2813977599143982
Batch 25/64 loss: 0.28845441341400146
Batch 26/64 loss: 0.2797093391418457
Batch 27/64 loss: 0.2841334939002991
Batch 28/64 loss: 0.2733573913574219
Batch 29/64 loss: 0.2782686948776245
Batch 30/64 loss: 0.2767561674118042
Batch 31/64 loss: 0.28906917572021484
Batch 32/64 loss: 0.2798691987991333
Batch 33/64 loss: 0.28029704093933105
Batch 34/64 loss: 0.28648096323013306
Batch 35/64 loss: 0.28638190031051636
Batch 36/64 loss: 0.27946174144744873
Batch 37/64 loss: 0.2800236940383911
Batch 38/64 loss: 0.2844048738479614
Batch 39/64 loss: 0.2733442783355713
Batch 40/64 loss: 0.28600043058395386
Batch 41/64 loss: 0.27425622940063477
Batch 42/64 loss: 0.2787278890609741
Batch 43/64 loss: 0.2734096646308899
Batch 44/64 loss: 0.28288567066192627
Batch 45/64 loss: 0.28741782903671265
Batch 46/64 loss: 0.2863065004348755
Batch 47/64 loss: 0.28368252515792847
Batch 48/64 loss: 0.2714831233024597
Batch 49/64 loss: 0.27810049057006836
Batch 50/64 loss: 0.2756725549697876
Batch 51/64 loss: 0.2852013111114502
Batch 52/64 loss: 0.2729145288467407
Batch 53/64 loss: 0.2809635400772095
Batch 54/64 loss: 0.2776196599006653
Batch 55/64 loss: 0.28295326232910156
Batch 56/64 loss: 0.27447253465652466
Batch 57/64 loss: 0.28857386112213135
Batch 58/64 loss: 0.280903697013855
Batch 59/64 loss: 0.2796381115913391
Batch 60/64 loss: 0.27002567052841187
Batch 61/64 loss: 0.2859358787536621
Batch 62/64 loss: 0.2853519320487976
Batch 63/64 loss: 0.28060418367385864
Batch 64/64 loss: 0.272996723651886
Epoch 5  Train loss: 0.2819732018545562  Val loss: 0.2825687339215754
Saving best model, epoch: 5
Epoch 6
-------------------------------
Batch 1/64 loss: 0.28175973892211914
Batch 2/64 loss: 0.2766951322555542
Batch 3/64 loss: 0.2836773991584778
Batch 4/64 loss: 0.2769066095352173
Batch 5/64 loss: 0.26431047916412354
Batch 6/64 loss: 0.2835838794708252
Batch 7/64 loss: 0.27893126010894775
Batch 8/64 loss: 0.27308833599090576
Batch 9/64 loss: 0.2757844924926758
Batch 10/64 loss: 0.27887654304504395
Batch 11/64 loss: 0.2768709659576416
Batch 12/64 loss: 0.2833356261253357
Batch 13/64 loss: 0.2831374406814575
Batch 14/64 loss: 0.2845815420150757
Batch 15/64 loss: 0.2799721956253052
Batch 16/64 loss: 0.2774680256843567
Batch 17/64 loss: 0.2883256673812866
Batch 18/64 loss: 0.2863677740097046
Batch 19/64 loss: 0.2712706923484802
Batch 20/64 loss: 0.2818481922149658
Batch 21/64 loss: 0.27121448516845703
Batch 22/64 loss: 0.28813230991363525
Batch 23/64 loss: 0.2722511291503906
Batch 24/64 loss: 0.2799603343009949
Batch 25/64 loss: 0.27539920806884766
Batch 26/64 loss: 0.29192638397216797
Batch 27/64 loss: 0.27656090259552
Batch 28/64 loss: 0.28686094284057617
Batch 29/64 loss: 0.27301323413848877
Batch 30/64 loss: 0.2784903049468994
Batch 31/64 loss: 0.2803052067756653
Batch 32/64 loss: 0.27494776248931885
Batch 33/64 loss: 0.28290629386901855
Batch 34/64 loss: 0.26949912309646606
Batch 35/64 loss: 0.2755441665649414
Batch 36/64 loss: 0.28094345331192017
Batch 37/64 loss: 0.2809180021286011
Batch 38/64 loss: 0.2790607810020447
Batch 39/64 loss: 0.2705603837966919
Batch 40/64 loss: 0.28173911571502686
Batch 41/64 loss: 0.2734032869338989
Batch 42/64 loss: 0.28106147050857544
Batch 43/64 loss: 0.29076898097991943
Batch 44/64 loss: 0.2756032347679138
Batch 45/64 loss: 0.2745303511619568
Batch 46/64 loss: 0.2829717993736267
Batch 47/64 loss: 0.28631991147994995
Batch 48/64 loss: 0.28109145164489746
Batch 49/64 loss: 0.2719210386276245
Batch 50/64 loss: 0.28423285484313965
Batch 51/64 loss: 0.2749019265174866
Batch 52/64 loss: 0.2790408730506897
Batch 53/64 loss: 0.2747471332550049
Batch 54/64 loss: 0.26778995990753174
Batch 55/64 loss: 0.2799806594848633
Batch 56/64 loss: 0.2836498022079468
Batch 57/64 loss: 0.2797102928161621
Batch 58/64 loss: 0.2873882055282593
Batch 59/64 loss: 0.2783242464065552
Batch 60/64 loss: 0.28427040576934814
Batch 61/64 loss: 0.2880735993385315
Batch 62/64 loss: 0.28337550163269043
Batch 63/64 loss: 0.2818089723587036
Batch 64/64 loss: 0.26514893770217896
Epoch 6  Train loss: 0.2792290694573346  Val loss: 0.2800032138005155
Saving best model, epoch: 6
Epoch 7
-------------------------------
Batch 1/64 loss: 0.2868316173553467
Batch 2/64 loss: 0.2831522226333618
Batch 3/64 loss: 0.2739959955215454
Batch 4/64 loss: 0.2822725772857666
Batch 5/64 loss: 0.27744603157043457
Batch 6/64 loss: 0.2810800075531006
Batch 7/64 loss: 0.2746542692184448
Batch 8/64 loss: 0.26784229278564453
Batch 9/64 loss: 0.27397358417510986
Batch 10/64 loss: 0.2659482955932617
Batch 11/64 loss: 0.28363728523254395
Batch 12/64 loss: 0.28624528646469116
Batch 13/64 loss: 0.27607738971710205
Batch 14/64 loss: 0.2808385491371155
Batch 15/64 loss: 0.28143584728240967
Batch 16/64 loss: 0.27617037296295166
Batch 17/64 loss: 0.27159345149993896
Batch 18/64 loss: 0.2717820405960083
Batch 19/64 loss: 0.27769970893859863
Batch 20/64 loss: 0.2644433379173279
Batch 21/64 loss: 0.2773935794830322
Batch 22/64 loss: 0.2688709497451782
Batch 23/64 loss: 0.28189200162887573
Batch 24/64 loss: 0.2784389853477478
Batch 25/64 loss: 0.26791608333587646
Batch 26/64 loss: 0.2749725580215454
Batch 27/64 loss: 0.2800482511520386
Batch 28/64 loss: 0.2794545292854309
Batch 29/64 loss: 0.2725822925567627
Batch 30/64 loss: 0.2710419297218323
Batch 31/64 loss: 0.27828991413116455
Batch 32/64 loss: 0.27731573581695557
Batch 33/64 loss: 0.26991111040115356
Batch 34/64 loss: 0.27805769443511963
Batch 35/64 loss: 0.2723730802536011
Batch 36/64 loss: 0.2681029438972473
Batch 37/64 loss: 0.2875455617904663
Batch 38/64 loss: 0.2699397802352905
Batch 39/64 loss: 0.27383631467819214
Batch 40/64 loss: 0.2786518335342407
Batch 41/64 loss: 0.27365702390670776
Batch 42/64 loss: 0.27088677883148193
Batch 43/64 loss: 0.2811816930770874
Batch 44/64 loss: 0.27435505390167236
Batch 45/64 loss: 0.27572011947631836
Batch 46/64 loss: 0.2769799828529358
Batch 47/64 loss: 0.2783162593841553
Batch 48/64 loss: 0.2788017988204956
Batch 49/64 loss: 0.28199338912963867
Batch 50/64 loss: 0.27219295501708984
Batch 51/64 loss: 0.2797146439552307
Batch 52/64 loss: 0.2663424015045166
Batch 53/64 loss: 0.2753814458847046
Batch 54/64 loss: 0.26837241649627686
Batch 55/64 loss: 0.27846866846084595
Batch 56/64 loss: 0.2680542469024658
Batch 57/64 loss: 0.27593517303466797
Batch 58/64 loss: 0.27303820848464966
Batch 59/64 loss: 0.27683037519454956
Batch 60/64 loss: 0.27487385272979736
Batch 61/64 loss: 0.27230292558670044
Batch 62/64 loss: 0.27905017137527466
Batch 63/64 loss: 0.26982200145721436
Batch 64/64 loss: 0.27101612091064453
Epoch 7  Train loss: 0.27559668971043005  Val loss: 0.2761560713302639
Saving best model, epoch: 7
Epoch 8
-------------------------------
Batch 1/64 loss: 0.2716336250305176
Batch 2/64 loss: 0.2732169032096863
Batch 3/64 loss: 0.27384644746780396
Batch 4/64 loss: 0.2736619710922241
Batch 5/64 loss: 0.2753216028213501
Batch 6/64 loss: 0.2708454132080078
Batch 7/64 loss: 0.2624855041503906
Batch 8/64 loss: 0.2726135849952698
Batch 9/64 loss: 0.2785685062408447
Batch 10/64 loss: 0.27547818422317505
Batch 11/64 loss: 0.27481865882873535
Batch 12/64 loss: 0.2689868211746216
Batch 13/64 loss: 0.26356977224349976
Batch 14/64 loss: 0.272591233253479
Batch 15/64 loss: 0.27756839990615845
Batch 16/64 loss: 0.2747446298599243
Batch 17/64 loss: 0.25983208417892456
Batch 18/64 loss: 0.2730783224105835
Batch 19/64 loss: 0.27055823802948
Batch 20/64 loss: 0.262603759765625
Batch 21/64 loss: 0.2741602659225464
Batch 22/64 loss: 0.2761409282684326
Batch 23/64 loss: 0.26053333282470703
Batch 24/64 loss: 0.2742658853530884
Batch 25/64 loss: 0.2801506519317627
Batch 26/64 loss: 0.26499032974243164
Batch 27/64 loss: 0.27374380826950073
Batch 28/64 loss: 0.27133333683013916
Batch 29/64 loss: 0.2783060073852539
Batch 30/64 loss: 0.27171552181243896
Batch 31/64 loss: 0.28220444917678833
Batch 32/64 loss: 0.265855610370636
Batch 33/64 loss: 0.27795541286468506
Batch 34/64 loss: 0.2690613269805908
Batch 35/64 loss: 0.2654333710670471
Batch 36/64 loss: 0.2770106792449951
Batch 37/64 loss: 0.27065110206604004
Batch 38/64 loss: 0.2725536823272705
Batch 39/64 loss: 0.2806553840637207
Batch 40/64 loss: 0.27078670263290405
Batch 41/64 loss: 0.27615511417388916
Batch 42/64 loss: 0.2740459442138672
Batch 43/64 loss: 0.2772437334060669
Batch 44/64 loss: 0.26630377769470215
Batch 45/64 loss: 0.2748366594314575
Batch 46/64 loss: 0.2796662449836731
Batch 47/64 loss: 0.2725588083267212
Batch 48/64 loss: 0.27956414222717285
Batch 49/64 loss: 0.27182531356811523
Batch 50/64 loss: 0.2764958143234253
Batch 51/64 loss: 0.2767259478569031
Batch 52/64 loss: 0.272210955619812
Batch 53/64 loss: 0.2738645076751709
Batch 54/64 loss: 0.26740145683288574
Batch 55/64 loss: 0.26328134536743164
Batch 56/64 loss: 0.2817143201828003
Batch 57/64 loss: 0.26335930824279785
Batch 58/64 loss: 0.27279984951019287
Batch 59/64 loss: 0.2760576009750366
Batch 60/64 loss: 0.27265048027038574
Batch 61/64 loss: 0.27671951055526733
Batch 62/64 loss: 0.29415005445480347
Batch 63/64 loss: 0.27001404762268066
Batch 64/64 loss: 0.26351767778396606
Epoch 8  Train loss: 0.2727970925031924  Val loss: 0.2737907126187459
Saving best model, epoch: 8
Epoch 9
-------------------------------
Batch 1/64 loss: 0.27691805362701416
Batch 2/64 loss: 0.2764991521835327
Batch 3/64 loss: 0.27128922939300537
Batch 4/64 loss: 0.27768898010253906
Batch 5/64 loss: 0.2676659822463989
Batch 6/64 loss: 0.26701271533966064
Batch 7/64 loss: 0.26438379287719727
Batch 8/64 loss: 0.26448655128479004
Batch 9/64 loss: 0.2695753574371338
Batch 10/64 loss: 0.273144006729126
Batch 11/64 loss: 0.275983989238739
Batch 12/64 loss: 0.26888418197631836
Batch 13/64 loss: 0.27461159229278564
Batch 14/64 loss: 0.2708171010017395
Batch 15/64 loss: 0.2779216766357422
Batch 16/64 loss: 0.27490782737731934
Batch 17/64 loss: 0.27179259061813354
Batch 18/64 loss: 0.26547789573669434
Batch 19/64 loss: 0.27347439527511597
Batch 20/64 loss: 0.27013295888900757
Batch 21/64 loss: 0.2695279121398926
Batch 22/64 loss: 0.26938146352767944
Batch 23/64 loss: 0.2666895389556885
Batch 24/64 loss: 0.2818688750267029
Batch 25/64 loss: 0.2840651869773865
Batch 26/64 loss: 0.27228033542633057
Batch 27/64 loss: 0.27488744258880615
Batch 28/64 loss: 0.27057701349258423
Batch 29/64 loss: 0.2729600667953491
Batch 30/64 loss: 0.26904308795928955
Batch 31/64 loss: 0.27342987060546875
Batch 32/64 loss: 0.25509005784988403
Batch 33/64 loss: 0.26846492290496826
Batch 34/64 loss: 0.26740360260009766
Batch 35/64 loss: 0.26924431324005127
Batch 36/64 loss: 0.2732465863227844
Batch 37/64 loss: 0.27475613355636597
Batch 38/64 loss: 0.2640293836593628
Batch 39/64 loss: 0.2792184352874756
Batch 40/64 loss: 0.2693377137184143
Batch 41/64 loss: 0.269791841506958
Batch 42/64 loss: 0.26727157831192017
Batch 43/64 loss: 0.28185850381851196
Batch 44/64 loss: 0.2598137855529785
Batch 45/64 loss: 0.28102564811706543
Batch 46/64 loss: 0.26478391885757446
Batch 47/64 loss: 0.2724100947380066
Batch 48/64 loss: 0.2650567293167114
Batch 49/64 loss: 0.27029651403427124
Batch 50/64 loss: 0.274300217628479
Batch 51/64 loss: 0.2698237895965576
Batch 52/64 loss: 0.24841302633285522
Batch 53/64 loss: 0.25892776250839233
Batch 54/64 loss: 0.2602241039276123
Batch 55/64 loss: 0.2737715244293213
Batch 56/64 loss: 0.2693758010864258
Batch 57/64 loss: 0.2677832841873169
Batch 58/64 loss: 0.26711714267730713
Batch 59/64 loss: 0.26452338695526123
Batch 60/64 loss: 0.28138089179992676
Batch 61/64 loss: 0.26980698108673096
Batch 62/64 loss: 0.2640154957771301
Batch 63/64 loss: 0.2606445550918579
Batch 64/64 loss: 0.2551969289779663
Epoch 9  Train loss: 0.26999190975638  Val loss: 0.2717328878612453
Saving best model, epoch: 9
Epoch 10
-------------------------------
Batch 1/64 loss: 0.2763681411743164
Batch 2/64 loss: 0.2596139907836914
Batch 3/64 loss: 0.2686576843261719
Batch 4/64 loss: 0.2618504762649536
Batch 5/64 loss: 0.2582707405090332
Batch 6/64 loss: 0.26987189054489136
Batch 7/64 loss: 0.2823960781097412
Batch 8/64 loss: 0.2629815340042114
Batch 9/64 loss: 0.2663106322288513
Batch 10/64 loss: 0.26386427879333496
Batch 11/64 loss: 0.2612350583076477
Batch 12/64 loss: 0.2660520076751709
Batch 13/64 loss: 0.26483166217803955
Batch 14/64 loss: 0.271703839302063
Batch 15/64 loss: 0.2599818706512451
Batch 16/64 loss: 0.26607024669647217
Batch 17/64 loss: 0.26657330989837646
Batch 18/64 loss: 0.2758234739303589
Batch 19/64 loss: 0.26781731843948364
Batch 20/64 loss: 0.26165294647216797
Batch 21/64 loss: 0.26236850023269653
Batch 22/64 loss: 0.264856219291687
Batch 23/64 loss: 0.26662778854370117
Batch 24/64 loss: 0.26850444078445435
Batch 25/64 loss: 0.2644795775413513
Batch 26/64 loss: 0.26514190435409546
Batch 27/64 loss: 0.2747523784637451
Batch 28/64 loss: 0.27309083938598633
Batch 29/64 loss: 0.2662229537963867
Batch 30/64 loss: 0.26402783393859863
Batch 31/64 loss: 0.2735506296157837
Batch 32/64 loss: 0.2688143253326416
Batch 33/64 loss: 0.2673634886741638
Batch 34/64 loss: 0.2695481777191162
Batch 35/64 loss: 0.2631911039352417
Batch 36/64 loss: 0.2561768889427185
Batch 37/64 loss: 0.2715058922767639
Batch 38/64 loss: 0.2739270329475403
Batch 39/64 loss: 0.2646961212158203
Batch 40/64 loss: 0.2590656876564026
Batch 41/64 loss: 0.26534318923950195
Batch 42/64 loss: 0.25768542289733887
Batch 43/64 loss: 0.2670137882232666
Batch 44/64 loss: 0.2713012099266052
Batch 45/64 loss: 0.27228474617004395
Batch 46/64 loss: 0.26539385318756104
Batch 47/64 loss: 0.25478458404541016
Batch 48/64 loss: 0.26363831758499146
Batch 49/64 loss: 0.26279062032699585
Batch 50/64 loss: 0.26242196559906006
Batch 51/64 loss: 0.2573821544647217
Batch 52/64 loss: 0.27289289236068726
Batch 53/64 loss: 0.2681717276573181
Batch 54/64 loss: 0.2625773549079895
Batch 55/64 loss: 0.2707945704460144
Batch 56/64 loss: 0.27230894565582275
Batch 57/64 loss: 0.26541668176651
Batch 58/64 loss: 0.25959086418151855
Batch 59/64 loss: 0.26665186882019043
Batch 60/64 loss: 0.27070170640945435
Batch 61/64 loss: 0.27057111263275146
Batch 62/64 loss: 0.2650752067565918
Batch 63/64 loss: 0.2667626738548279
Batch 64/64 loss: 0.2552363872528076
Epoch 10  Train loss: 0.26624034058813956  Val loss: 0.2671026646886085
Saving best model, epoch: 10
Epoch 11
-------------------------------
Batch 1/64 loss: 0.25521159172058105
Batch 2/64 loss: 0.26069188117980957
Batch 3/64 loss: 0.2624913454055786
Batch 4/64 loss: 0.26414185762405396
Batch 5/64 loss: 0.26023411750793457
Batch 6/64 loss: 0.26747262477874756
Batch 7/64 loss: 0.2711871266365051
Batch 8/64 loss: 0.2623818516731262
Batch 9/64 loss: 0.2676023244857788
Batch 10/64 loss: 0.2713077664375305
Batch 11/64 loss: 0.2621604800224304
Batch 12/64 loss: 0.2544541358947754
Batch 13/64 loss: 0.2671709656715393
Batch 14/64 loss: 0.25817573070526123
Batch 15/64 loss: 0.2598038911819458
Batch 16/64 loss: 0.25878095626831055
Batch 17/64 loss: 0.2564225196838379
Batch 18/64 loss: 0.25583410263061523
Batch 19/64 loss: 0.26874059438705444
Batch 20/64 loss: 0.26308488845825195
Batch 21/64 loss: 0.25856947898864746
Batch 22/64 loss: 0.2619110345840454
Batch 23/64 loss: 0.26891595125198364
Batch 24/64 loss: 0.26201891899108887
Batch 25/64 loss: 0.27583909034729004
Batch 26/64 loss: 0.268878698348999
Batch 27/64 loss: 0.2696123719215393
Batch 28/64 loss: 0.2570250630378723
Batch 29/64 loss: 0.24877524375915527
Batch 30/64 loss: 0.26512277126312256
Batch 31/64 loss: 0.2604331374168396
Batch 32/64 loss: 0.2703906297683716
Batch 33/64 loss: 0.2658687233924866
Batch 34/64 loss: 0.2709510922431946
Batch 35/64 loss: 0.26371216773986816
Batch 36/64 loss: 0.2666119933128357
Batch 37/64 loss: 0.2631509304046631
Batch 38/64 loss: 0.26575934886932373
Batch 39/64 loss: 0.2701113224029541
Batch 40/64 loss: 0.2606382966041565
Batch 41/64 loss: 0.26133638620376587
Batch 42/64 loss: 0.24895453453063965
Batch 43/64 loss: 0.257480263710022
Batch 44/64 loss: 0.2653731107711792
Batch 45/64 loss: 0.26507824659347534
Batch 46/64 loss: 0.2546358108520508
Batch 47/64 loss: 0.26121604442596436
Batch 48/64 loss: 0.26956707239151
Batch 49/64 loss: 0.25491833686828613
Batch 50/64 loss: 0.27276933193206787
Batch 51/64 loss: 0.2642788290977478
Batch 52/64 loss: 0.27582013607025146
Batch 53/64 loss: 0.27517443895339966
Batch 54/64 loss: 0.2571958303451538
Batch 55/64 loss: 0.2639818787574768
Batch 56/64 loss: 0.26246190071105957
Batch 57/64 loss: 0.25608646869659424
Batch 58/64 loss: 0.24744153022766113
Batch 59/64 loss: 0.25865060091018677
Batch 60/64 loss: 0.27174264192581177
Batch 61/64 loss: 0.26701533794403076
Batch 62/64 loss: 0.2774254083633423
Batch 63/64 loss: 0.2540280818939209
Batch 64/64 loss: 0.2596532106399536
Epoch 11  Train loss: 0.2632003003475713  Val loss: 0.26760849858477354
Epoch 12
-------------------------------
Batch 1/64 loss: 0.258639395236969
Batch 2/64 loss: 0.265083372592926
Batch 3/64 loss: 0.25244927406311035
Batch 4/64 loss: 0.2676950693130493
Batch 5/64 loss: 0.2499915361404419
Batch 6/64 loss: 0.2728042006492615
Batch 7/64 loss: 0.25617629289627075
Batch 8/64 loss: 0.2690262794494629
Batch 9/64 loss: 0.2601959705352783
Batch 10/64 loss: 0.25418639183044434
Batch 11/64 loss: 0.24991905689239502
Batch 12/64 loss: 0.25272905826568604
Batch 13/64 loss: 0.24904978275299072
Batch 14/64 loss: 0.25403547286987305
Batch 15/64 loss: 0.25388866662979126
Batch 16/64 loss: 0.26143938302993774
Batch 17/64 loss: 0.2520974278450012
Batch 18/64 loss: 0.2649679183959961
Batch 19/64 loss: 0.2597753405570984
Batch 20/64 loss: 0.2638593912124634
Batch 21/64 loss: 0.25540971755981445
Batch 22/64 loss: 0.26639509201049805
Batch 23/64 loss: 0.25649601221084595
Batch 24/64 loss: 0.2632460594177246
Batch 25/64 loss: 0.2658730745315552
Batch 26/64 loss: 0.26119697093963623
Batch 27/64 loss: 0.2735142707824707
Batch 28/64 loss: 0.25891023874282837
Batch 29/64 loss: 0.2553471326828003
Batch 30/64 loss: 0.2639284133911133
Batch 31/64 loss: 0.25558745861053467
Batch 32/64 loss: 0.2594490051269531
Batch 33/64 loss: 0.25985461473464966
Batch 34/64 loss: 0.26744312047958374
Batch 35/64 loss: 0.2716609239578247
Batch 36/64 loss: 0.2551615238189697
Batch 37/64 loss: 0.2571218013763428
Batch 38/64 loss: 0.24624836444854736
Batch 39/64 loss: 0.2642049193382263
Batch 40/64 loss: 0.26198112964630127
Batch 41/64 loss: 0.2484116554260254
Batch 42/64 loss: 0.2591090202331543
Batch 43/64 loss: 0.25070202350616455
Batch 44/64 loss: 0.2655189037322998
Batch 45/64 loss: 0.26136887073516846
Batch 46/64 loss: 0.25970399379730225
Batch 47/64 loss: 0.2608563303947449
Batch 48/64 loss: 0.25682103633880615
Batch 49/64 loss: 0.2653360962867737
Batch 50/64 loss: 0.2521657943725586
Batch 51/64 loss: 0.26480555534362793
Batch 52/64 loss: 0.26233917474746704
Batch 53/64 loss: 0.24713951349258423
Batch 54/64 loss: 0.25590378046035767
Batch 55/64 loss: 0.2620123624801636
Batch 56/64 loss: 0.2649184465408325
Batch 57/64 loss: 0.2587602734565735
Batch 58/64 loss: 0.2603815793991089
Batch 59/64 loss: 0.25914061069488525
Batch 60/64 loss: 0.24193662405014038
Batch 61/64 loss: 0.26344382762908936
Batch 62/64 loss: 0.2613499164581299
Batch 63/64 loss: 0.2581438422203064
Batch 64/64 loss: 0.2505927085876465
Epoch 12  Train loss: 0.2590627904031791  Val loss: 0.2663579167369305
Saving best model, epoch: 12
Epoch 13
-------------------------------
Batch 1/64 loss: 0.25475043058395386
Batch 2/64 loss: 0.25197482109069824
Batch 3/64 loss: 0.24703222513198853
Batch 4/64 loss: 0.25905728340148926
Batch 5/64 loss: 0.25857001543045044
Batch 6/64 loss: 0.2533947825431824
Batch 7/64 loss: 0.25382983684539795
Batch 8/64 loss: 0.26751911640167236
Batch 9/64 loss: 0.251803457736969
Batch 10/64 loss: 0.2538478374481201
Batch 11/64 loss: 0.263691782951355
Batch 12/64 loss: 0.2551044225692749
Batch 13/64 loss: 0.2588269114494324
Batch 14/64 loss: 0.25398677587509155
Batch 15/64 loss: 0.2530670166015625
Batch 16/64 loss: 0.24946188926696777
Batch 17/64 loss: 0.259446382522583
Batch 18/64 loss: 0.2461479902267456
Batch 19/64 loss: 0.24337232112884521
Batch 20/64 loss: 0.2604893445968628
Batch 21/64 loss: 0.2507718801498413
Batch 22/64 loss: 0.25112491846084595
Batch 23/64 loss: 0.2589406967163086
Batch 24/64 loss: 0.2625630497932434
Batch 25/64 loss: 0.24937236309051514
Batch 26/64 loss: 0.25379836559295654
Batch 27/64 loss: 0.26118361949920654
Batch 28/64 loss: 0.25293684005737305
Batch 29/64 loss: 0.25090062618255615
Batch 30/64 loss: 0.2552106976509094
Batch 31/64 loss: 0.2607126235961914
Batch 32/64 loss: 0.24783796072006226
Batch 33/64 loss: 0.2692967653274536
Batch 34/64 loss: 0.24869930744171143
Batch 35/64 loss: 0.2756943702697754
Batch 36/64 loss: 0.2432454228401184
Batch 37/64 loss: 0.26090025901794434
Batch 38/64 loss: 0.23783504962921143
Batch 39/64 loss: 0.25607526302337646
Batch 40/64 loss: 0.2572188377380371
Batch 41/64 loss: 0.25020384788513184
Batch 42/64 loss: 0.24360579252243042
Batch 43/64 loss: 0.26905834674835205
Batch 44/64 loss: 0.25289303064346313
Batch 45/64 loss: 0.25308895111083984
Batch 46/64 loss: 0.25063371658325195
Batch 47/64 loss: 0.25594472885131836
Batch 48/64 loss: 0.26245224475860596
Batch 49/64 loss: 0.2562979459762573
Batch 50/64 loss: 0.251132071018219
Batch 51/64 loss: 0.25750666856765747
Batch 52/64 loss: 0.2524591088294983
Batch 53/64 loss: 0.25635379552841187
Batch 54/64 loss: 0.2687060832977295
Batch 55/64 loss: 0.25619423389434814
Batch 56/64 loss: 0.24700701236724854
Batch 57/64 loss: 0.24884366989135742
Batch 58/64 loss: 0.25521814823150635
Batch 59/64 loss: 0.24510252475738525
Batch 60/64 loss: 0.2479550838470459
Batch 61/64 loss: 0.26421117782592773
Batch 62/64 loss: 0.2536259889602661
Batch 63/64 loss: 0.25210797786712646
Batch 64/64 loss: 0.2561667561531067
Epoch 13  Train loss: 0.25478307102240766  Val loss: 0.2595793496292481
Saving best model, epoch: 13
Epoch 14
-------------------------------
Batch 1/64 loss: 0.262509822845459
Batch 2/64 loss: 0.2655200958251953
Batch 3/64 loss: 0.24970710277557373
Batch 4/64 loss: 0.24184226989746094
Batch 5/64 loss: 0.2455316185951233
Batch 6/64 loss: 0.24513322114944458
Batch 7/64 loss: 0.2543099522590637
Batch 8/64 loss: 0.24269729852676392
Batch 9/64 loss: 0.2474471926689148
Batch 10/64 loss: 0.2558560371398926
Batch 11/64 loss: 0.24651122093200684
Batch 12/64 loss: 0.25829702615737915
Batch 13/64 loss: 0.2533082365989685
Batch 14/64 loss: 0.25957345962524414
Batch 15/64 loss: 0.253690242767334
Batch 16/64 loss: 0.23647284507751465
Batch 17/64 loss: 0.24534231424331665
Batch 18/64 loss: 0.24427497386932373
Batch 19/64 loss: 0.25077056884765625
Batch 20/64 loss: 0.25629669427871704
Batch 21/64 loss: 0.2544935941696167
Batch 22/64 loss: 0.2548869252204895
Batch 23/64 loss: 0.25550782680511475
Batch 24/64 loss: 0.2586096525192261
Batch 25/64 loss: 0.2570427656173706
Batch 26/64 loss: 0.2422161102294922
Batch 27/64 loss: 0.2568349838256836
Batch 28/64 loss: 0.26033079624176025
Batch 29/64 loss: 0.22868973016738892
Batch 30/64 loss: 0.25539493560791016
Batch 31/64 loss: 0.2504274845123291
Batch 32/64 loss: 0.2496551275253296
Batch 33/64 loss: 0.2561608552932739
Batch 34/64 loss: 0.23958075046539307
Batch 35/64 loss: 0.25217658281326294
Batch 36/64 loss: 0.25343000888824463
Batch 37/64 loss: 0.2454068660736084
Batch 38/64 loss: 0.2565053105354309
Batch 39/64 loss: 0.24930846691131592
Batch 40/64 loss: 0.25372862815856934
Batch 41/64 loss: 0.2534504532814026
Batch 42/64 loss: 0.24697303771972656
Batch 43/64 loss: 0.2547045946121216
Batch 44/64 loss: 0.26048213243484497
Batch 45/64 loss: 0.25558769702911377
Batch 46/64 loss: 0.24414420127868652
Batch 47/64 loss: 0.25059258937835693
Batch 48/64 loss: 0.2537533640861511
Batch 49/64 loss: 0.25859081745147705
Batch 50/64 loss: 0.2379729151725769
Batch 51/64 loss: 0.2430863380432129
Batch 52/64 loss: 0.24942445755004883
Batch 53/64 loss: 0.23851048946380615
Batch 54/64 loss: 0.24138784408569336
Batch 55/64 loss: 0.25402843952178955
Batch 56/64 loss: 0.2550314664840698
Batch 57/64 loss: 0.2573885917663574
Batch 58/64 loss: 0.2533324956893921
Batch 59/64 loss: 0.24332088232040405
Batch 60/64 loss: 0.24644184112548828
Batch 61/64 loss: 0.24868708848953247
Batch 62/64 loss: 0.26118898391723633
Batch 63/64 loss: 0.25041842460632324
Batch 64/64 loss: 0.2556811571121216
Epoch 14  Train loss: 0.25091356249416574  Val loss: 0.25379589681363185
Saving best model, epoch: 14
Epoch 15
-------------------------------
Batch 1/64 loss: 0.23979228734970093
Batch 2/64 loss: 0.24322199821472168
Batch 3/64 loss: 0.2585664987564087
Batch 4/64 loss: 0.24000942707061768
Batch 5/64 loss: 0.2547333240509033
Batch 6/64 loss: 0.24182051420211792
Batch 7/64 loss: 0.23871827125549316
Batch 8/64 loss: 0.2430681586265564
Batch 9/64 loss: 0.25170767307281494
Batch 10/64 loss: 0.2507931590080261
Batch 11/64 loss: 0.22852659225463867
Batch 12/64 loss: 0.2492060661315918
Batch 13/64 loss: 0.25885605812072754
Batch 14/64 loss: 0.2530595064163208
Batch 15/64 loss: 0.2623347043991089
Batch 16/64 loss: 0.25009775161743164
Batch 17/64 loss: 0.2528359889984131
Batch 18/64 loss: 0.23476159572601318
Batch 19/64 loss: 0.24650967121124268
Batch 20/64 loss: 0.25904375314712524
Batch 21/64 loss: 0.24183380603790283
Batch 22/64 loss: 0.2553166151046753
Batch 23/64 loss: 0.2486974000930786
Batch 24/64 loss: 0.24746263027191162
Batch 25/64 loss: 0.2452864646911621
Batch 26/64 loss: 0.25179123878479004
Batch 27/64 loss: 0.24945729970932007
Batch 28/64 loss: 0.25614964962005615
Batch 29/64 loss: 0.25148439407348633
Batch 30/64 loss: 0.23697620630264282
Batch 31/64 loss: 0.2690849304199219
Batch 32/64 loss: 0.24754101037979126
Batch 33/64 loss: 0.25784778594970703
Batch 34/64 loss: 0.2520289421081543
Batch 35/64 loss: 0.2432328462600708
Batch 36/64 loss: 0.24244558811187744
Batch 37/64 loss: 0.24377655982971191
Batch 38/64 loss: 0.24880963563919067
Batch 39/64 loss: 0.24713504314422607
Batch 40/64 loss: 0.24992060661315918
Batch 41/64 loss: 0.24654066562652588
Batch 42/64 loss: 0.25140851736068726
Batch 43/64 loss: 0.24400973320007324
Batch 44/64 loss: 0.24430930614471436
Batch 45/64 loss: 0.24851489067077637
Batch 46/64 loss: 0.2474879026412964
Batch 47/64 loss: 0.24120581150054932
Batch 48/64 loss: 0.24707823991775513
Batch 49/64 loss: 0.2468167543411255
Batch 50/64 loss: 0.2330835461616516
Batch 51/64 loss: 0.253781795501709
Batch 52/64 loss: 0.249428391456604
Batch 53/64 loss: 0.2545742988586426
Batch 54/64 loss: 0.2411826252937317
Batch 55/64 loss: 0.2397979497909546
Batch 56/64 loss: 0.24189376831054688
Batch 57/64 loss: 0.24968808889389038
Batch 58/64 loss: 0.24268609285354614
Batch 59/64 loss: 0.25642406940460205
Batch 60/64 loss: 0.2580212354660034
Batch 61/64 loss: 0.260242223739624
Batch 62/64 loss: 0.24969464540481567
Batch 63/64 loss: 0.2381654977798462
Batch 64/64 loss: 0.23244178295135498
Epoch 15  Train loss: 0.24791073004404704  Val loss: 0.25473309177713294
Epoch 16
-------------------------------
Batch 1/64 loss: 0.23923039436340332
Batch 2/64 loss: 0.24498909711837769
Batch 3/64 loss: 0.24936693906784058
Batch 4/64 loss: 0.23241961002349854
Batch 5/64 loss: 0.24521887302398682
Batch 6/64 loss: 0.2559651732444763
Batch 7/64 loss: 0.24305224418640137
Batch 8/64 loss: 0.24814832210540771
Batch 9/64 loss: 0.24805480241775513
Batch 10/64 loss: 0.25562554597854614
Batch 11/64 loss: 0.2535276412963867
Batch 12/64 loss: 0.23844635486602783
Batch 13/64 loss: 0.24029302597045898
Batch 14/64 loss: 0.24635183811187744
Batch 15/64 loss: 0.2544962167739868
Batch 16/64 loss: 0.25739216804504395
Batch 17/64 loss: 0.2438754439353943
Batch 18/64 loss: 0.24678564071655273
Batch 19/64 loss: 0.243688702583313
Batch 20/64 loss: 0.2403354048728943
Batch 21/64 loss: 0.24703645706176758
Batch 22/64 loss: 0.24839580059051514
Batch 23/64 loss: 0.24901586771011353
Batch 24/64 loss: 0.245169997215271
Batch 25/64 loss: 0.24995791912078857
Batch 26/64 loss: 0.2386447787284851
Batch 27/64 loss: 0.23934102058410645
Batch 28/64 loss: 0.2463693618774414
Batch 29/64 loss: 0.23068928718566895
Batch 30/64 loss: 0.24027657508850098
Batch 31/64 loss: 0.24166154861450195
Batch 32/64 loss: 0.24551689624786377
Batch 33/64 loss: 0.2427445650100708
Batch 34/64 loss: 0.2588004469871521
Batch 35/64 loss: 0.24696314334869385
Batch 36/64 loss: 0.24479639530181885
Batch 37/64 loss: 0.24268895387649536
Batch 38/64 loss: 0.2368025779724121
Batch 39/64 loss: 0.2413114309310913
Batch 40/64 loss: 0.2524145245552063
Batch 41/64 loss: 0.2464810013771057
Batch 42/64 loss: 0.24941670894622803
Batch 43/64 loss: 0.25514793395996094
Batch 44/64 loss: 0.24081426858901978
Batch 45/64 loss: 0.247128427028656
Batch 46/64 loss: 0.2501571774482727
Batch 47/64 loss: 0.2642829418182373
Batch 48/64 loss: 0.24387317895889282
Batch 49/64 loss: 0.23125958442687988
Batch 50/64 loss: 0.24880224466323853
Batch 51/64 loss: 0.2427424192428589
Batch 52/64 loss: 0.23632168769836426
Batch 53/64 loss: 0.2511122226715088
Batch 54/64 loss: 0.261325478553772
Batch 55/64 loss: 0.2569580078125
Batch 56/64 loss: 0.24400776624679565
Batch 57/64 loss: 0.24202555418014526
Batch 58/64 loss: 0.23597919940948486
Batch 59/64 loss: 0.2599374055862427
Batch 60/64 loss: 0.24912643432617188
Batch 61/64 loss: 0.2571415901184082
Batch 62/64 loss: 0.23814886808395386
Batch 63/64 loss: 0.22887051105499268
Batch 64/64 loss: 0.24661338329315186
Epoch 16  Train loss: 0.24599030017852783  Val loss: 0.24932402556704492
Saving best model, epoch: 16
Epoch 17
-------------------------------
Batch 1/64 loss: 0.2505289316177368
Batch 2/64 loss: 0.23788058757781982
Batch 3/64 loss: 0.24985873699188232
Batch 4/64 loss: 0.2555350661277771
Batch 5/64 loss: 0.23116320371627808
Batch 6/64 loss: 0.23460698127746582
Batch 7/64 loss: 0.24088633060455322
Batch 8/64 loss: 0.24645239114761353
Batch 9/64 loss: 0.234555184841156
Batch 10/64 loss: 0.2376384735107422
Batch 11/64 loss: 0.243008553981781
Batch 12/64 loss: 0.24594926834106445
Batch 13/64 loss: 0.24239695072174072
Batch 14/64 loss: 0.2385229468345642
Batch 15/64 loss: 0.22738462686538696
Batch 16/64 loss: 0.2462620735168457
Batch 17/64 loss: 0.24394148588180542
Batch 18/64 loss: 0.24024736881256104
Batch 19/64 loss: 0.23482370376586914
Batch 20/64 loss: 0.245408833026886
Batch 21/64 loss: 0.23449546098709106
Batch 22/64 loss: 0.2423083782196045
Batch 23/64 loss: 0.24697256088256836
Batch 24/64 loss: 0.2418190836906433
Batch 25/64 loss: 0.2346174120903015
Batch 26/64 loss: 0.24097448587417603
Batch 27/64 loss: 0.23724889755249023
Batch 28/64 loss: 0.2349386215209961
Batch 29/64 loss: 0.23465418815612793
Batch 30/64 loss: 0.24025213718414307
Batch 31/64 loss: 0.23263370990753174
Batch 32/64 loss: 0.24123656749725342
Batch 33/64 loss: 0.25503313541412354
Batch 34/64 loss: 0.245672345161438
Batch 35/64 loss: 0.2535752058029175
Batch 36/64 loss: 0.2386341094970703
Batch 37/64 loss: 0.23877495527267456
Batch 38/64 loss: 0.2520166039466858
Batch 39/64 loss: 0.2488940954208374
Batch 40/64 loss: 0.24625617265701294
Batch 41/64 loss: 0.24193447828292847
Batch 42/64 loss: 0.2301539182662964
Batch 43/64 loss: 0.24995160102844238
Batch 44/64 loss: 0.2486438751220703
Batch 45/64 loss: 0.23995530605316162
Batch 46/64 loss: 0.24530959129333496
Batch 47/64 loss: 0.2444446086883545
Batch 48/64 loss: 0.23363518714904785
Batch 49/64 loss: 0.23732489347457886
Batch 50/64 loss: 0.23325765132904053
Batch 51/64 loss: 0.23443400859832764
Batch 52/64 loss: 0.24217402935028076
Batch 53/64 loss: 0.23773270845413208
Batch 54/64 loss: 0.24272608757019043
Batch 55/64 loss: 0.24228137731552124
Batch 56/64 loss: 0.24007272720336914
Batch 57/64 loss: 0.24153059720993042
Batch 58/64 loss: 0.23997259140014648
Batch 59/64 loss: 0.24425750970840454
Batch 60/64 loss: 0.24278348684310913
Batch 61/64 loss: 0.2342895269393921
Batch 62/64 loss: 0.24239087104797363
Batch 63/64 loss: 0.23520123958587646
Batch 64/64 loss: 0.24630844593048096
Epoch 17  Train loss: 0.24121174952563118  Val loss: 0.24357829593710884
Saving best model, epoch: 17
Epoch 18
-------------------------------
Batch 1/64 loss: 0.2240384817123413
Batch 2/64 loss: 0.2300935983657837
Batch 3/64 loss: 0.24295032024383545
Batch 4/64 loss: 0.23246175050735474
Batch 5/64 loss: 0.2514333724975586
Batch 6/64 loss: 0.2338705062866211
Batch 7/64 loss: 0.2392733097076416
Batch 8/64 loss: 0.2238699197769165
Batch 9/64 loss: 0.23559099435806274
Batch 10/64 loss: 0.23976773023605347
Batch 11/64 loss: 0.23877942562103271
Batch 12/64 loss: 0.21778661012649536
Batch 13/64 loss: 0.22561109066009521
Batch 14/64 loss: 0.2378532886505127
Batch 15/64 loss: 0.23599934577941895
Batch 16/64 loss: 0.23842668533325195
Batch 17/64 loss: 0.24255013465881348
Batch 18/64 loss: 0.2489336133003235
Batch 19/64 loss: 0.24869835376739502
Batch 20/64 loss: 0.24574971199035645
Batch 21/64 loss: 0.23953205347061157
Batch 22/64 loss: 0.23385202884674072
Batch 23/64 loss: 0.2329249382019043
Batch 24/64 loss: 0.24497854709625244
Batch 25/64 loss: 0.24673724174499512
Batch 26/64 loss: 0.2624495029449463
Batch 27/64 loss: 0.22983193397521973
Batch 28/64 loss: 0.23121261596679688
Batch 29/64 loss: 0.24221527576446533
Batch 30/64 loss: 0.23372197151184082
Batch 31/64 loss: 0.24400508403778076
Batch 32/64 loss: 0.2443150281906128
Batch 33/64 loss: 0.2421286702156067
Batch 34/64 loss: 0.23830187320709229
Batch 35/64 loss: 0.24001216888427734
Batch 36/64 loss: 0.23429632186889648
Batch 37/64 loss: 0.23771804571151733
Batch 38/64 loss: 0.24130034446716309
Batch 39/64 loss: 0.25020724534988403
Batch 40/64 loss: 0.2352313995361328
Batch 41/64 loss: 0.23618459701538086
Batch 42/64 loss: 0.24069273471832275
Batch 43/64 loss: 0.22521507740020752
Batch 44/64 loss: 0.24514436721801758
Batch 45/64 loss: 0.23988717794418335
Batch 46/64 loss: 0.24045491218566895
Batch 47/64 loss: 0.2527272701263428
Batch 48/64 loss: 0.23037505149841309
Batch 49/64 loss: 0.24884694814682007
Batch 50/64 loss: 0.2335636019706726
Batch 51/64 loss: 0.24438470602035522
Batch 52/64 loss: 0.23889899253845215
Batch 53/64 loss: 0.24200081825256348
Batch 54/64 loss: 0.24718129634857178
Batch 55/64 loss: 0.2186967134475708
Batch 56/64 loss: 0.23341697454452515
Batch 57/64 loss: 0.23524737358093262
Batch 58/64 loss: 0.24211502075195312
Batch 59/64 loss: 0.2356201410293579
Batch 60/64 loss: 0.23719477653503418
Batch 61/64 loss: 0.24486708641052246
Batch 62/64 loss: 0.24129056930541992
Batch 63/64 loss: 0.2328813076019287
Batch 64/64 loss: 0.2380380630493164
Epoch 18  Train loss: 0.23849610347373812  Val loss: 0.24047243677054075
Saving best model, epoch: 18
Epoch 19
-------------------------------
Batch 1/64 loss: 0.25081872940063477
Batch 2/64 loss: 0.22218602895736694
Batch 3/64 loss: 0.23961222171783447
Batch 4/64 loss: 0.2304551601409912
Batch 5/64 loss: 0.23867344856262207
Batch 6/64 loss: 0.235101580619812
Batch 7/64 loss: 0.24342000484466553
Batch 8/64 loss: 0.24335503578186035
Batch 9/64 loss: 0.23119163513183594
Batch 10/64 loss: 0.23305928707122803
Batch 11/64 loss: 0.23322612047195435
Batch 12/64 loss: 0.21725845336914062
Batch 13/64 loss: 0.24533379077911377
Batch 14/64 loss: 0.2409287691116333
Batch 15/64 loss: 0.24502015113830566
Batch 16/64 loss: 0.23042339086532593
Batch 17/64 loss: 0.24090510606765747
Batch 18/64 loss: 0.2197251319885254
Batch 19/64 loss: 0.23193883895874023
Batch 20/64 loss: 0.2413882613182068
Batch 21/64 loss: 0.2322501540184021
Batch 22/64 loss: 0.25731003284454346
Batch 23/64 loss: 0.22537744045257568
Batch 24/64 loss: 0.23207640647888184
Batch 25/64 loss: 0.2393810749053955
Batch 26/64 loss: 0.23356056213378906
Batch 27/64 loss: 0.23166918754577637
Batch 28/64 loss: 0.2302457094192505
Batch 29/64 loss: 0.2286309003829956
Batch 30/64 loss: 0.23048675060272217
Batch 31/64 loss: 0.2221199870109558
Batch 32/64 loss: 0.23040664196014404
Batch 33/64 loss: 0.23993271589279175
Batch 34/64 loss: 0.22876596450805664
Batch 35/64 loss: 0.2341136932373047
Batch 36/64 loss: 0.2273033857345581
Batch 37/64 loss: 0.22718417644500732
Batch 38/64 loss: 0.22287678718566895
Batch 39/64 loss: 0.23278212547302246
Batch 40/64 loss: 0.251102089881897
Batch 41/64 loss: 0.22263503074645996
Batch 42/64 loss: 0.24914604425430298
Batch 43/64 loss: 0.24197596311569214
Batch 44/64 loss: 0.22534948587417603
Batch 45/64 loss: 0.22283124923706055
Batch 46/64 loss: 0.23075389862060547
Batch 47/64 loss: 0.2204456329345703
Batch 48/64 loss: 0.2427830696105957
Batch 49/64 loss: 0.24223095178604126
Batch 50/64 loss: 0.22474288940429688
Batch 51/64 loss: 0.22133135795593262
Batch 52/64 loss: 0.2323635220527649
Batch 53/64 loss: 0.22866010665893555
Batch 54/64 loss: 0.24386560916900635
Batch 55/64 loss: 0.24838632345199585
Batch 56/64 loss: 0.22241449356079102
Batch 57/64 loss: 0.2574504613876343
Batch 58/64 loss: 0.23473286628723145
Batch 59/64 loss: 0.23632419109344482
Batch 60/64 loss: 0.24119335412979126
Batch 61/64 loss: 0.248998761177063
Batch 62/64 loss: 0.23015224933624268
Batch 63/64 loss: 0.240095853805542
Batch 64/64 loss: 0.24553966522216797
Epoch 19  Train loss: 0.23473905956043917  Val loss: 0.24087607901530578
Epoch 20
-------------------------------
Batch 1/64 loss: 0.2491319179534912
Batch 2/64 loss: 0.23415470123291016
Batch 3/64 loss: 0.23319393396377563
Batch 4/64 loss: 0.22277045249938965
Batch 5/64 loss: 0.22649133205413818
Batch 6/64 loss: 0.23549485206604004
Batch 7/64 loss: 0.23270010948181152
Batch 8/64 loss: 0.24425137042999268
Batch 9/64 loss: 0.22645246982574463
Batch 10/64 loss: 0.2310476303100586
Batch 11/64 loss: 0.2322351336479187
Batch 12/64 loss: 0.2352999448776245
Batch 13/64 loss: 0.2230156660079956
Batch 14/64 loss: 0.23969650268554688
Batch 15/64 loss: 0.21838176250457764
Batch 16/64 loss: 0.23424410820007324
Batch 17/64 loss: 0.23188579082489014
Batch 18/64 loss: 0.2296953797340393
Batch 19/64 loss: 0.22872579097747803
Batch 20/64 loss: 0.22818076610565186
Batch 21/64 loss: 0.21699142456054688
Batch 22/64 loss: 0.23735815286636353
Batch 23/64 loss: 0.23854684829711914
Batch 24/64 loss: 0.2367824912071228
Batch 25/64 loss: 0.22605550289154053
Batch 26/64 loss: 0.2237619161605835
Batch 27/64 loss: 0.236303448677063
Batch 28/64 loss: 0.22537624835968018
Batch 29/64 loss: 0.24243462085723877
Batch 30/64 loss: 0.22927117347717285
Batch 31/64 loss: 0.25073152780532837
Batch 32/64 loss: 0.22561156749725342
Batch 33/64 loss: 0.21379482746124268
Batch 34/64 loss: 0.22819554805755615
Batch 35/64 loss: 0.23878657817840576
Batch 36/64 loss: 0.2342158555984497
Batch 37/64 loss: 0.23081648349761963
Batch 38/64 loss: 0.2455350160598755
Batch 39/64 loss: 0.22073888778686523
Batch 40/64 loss: 0.24150681495666504
Batch 41/64 loss: 0.23317992687225342
Batch 42/64 loss: 0.238847553730011
Batch 43/64 loss: 0.25883054733276367
Batch 44/64 loss: 0.2394765019416809
Batch 45/64 loss: 0.23675864934921265
Batch 46/64 loss: 0.22819483280181885
Batch 47/64 loss: 0.22910219430923462
Batch 48/64 loss: 0.2450958490371704
Batch 49/64 loss: 0.22993922233581543
Batch 50/64 loss: 0.2307969331741333
Batch 51/64 loss: 0.22327005863189697
Batch 52/64 loss: 0.2311798334121704
Batch 53/64 loss: 0.21805953979492188
Batch 54/64 loss: 0.23100554943084717
Batch 55/64 loss: 0.23327195644378662
Batch 56/64 loss: 0.24591505527496338
Batch 57/64 loss: 0.23742520809173584
Batch 58/64 loss: 0.24084675312042236
Batch 59/64 loss: 0.22740530967712402
Batch 60/64 loss: 0.23534536361694336
Batch 61/64 loss: 0.22793102264404297
Batch 62/64 loss: 0.23354655504226685
Batch 63/64 loss: 0.22705835103988647
Batch 64/64 loss: 0.2560497522354126
Epoch 20  Train loss: 0.23300995499480004  Val loss: 0.23961237533805296
Saving best model, epoch: 20
Epoch 21
-------------------------------
Batch 1/64 loss: 0.23630356788635254
Batch 2/64 loss: 0.234350323677063
Batch 3/64 loss: 0.21935933828353882
Batch 4/64 loss: 0.2306099534034729
Batch 5/64 loss: 0.22794699668884277
Batch 6/64 loss: 0.23400449752807617
Batch 7/64 loss: 0.2356572151184082
Batch 8/64 loss: 0.22861433029174805
Batch 9/64 loss: 0.23275327682495117
Batch 10/64 loss: 0.23768079280853271
Batch 11/64 loss: 0.23712903261184692
Batch 12/64 loss: 0.24001336097717285
Batch 13/64 loss: 0.22482526302337646
Batch 14/64 loss: 0.23005056381225586
Batch 15/64 loss: 0.22220325469970703
Batch 16/64 loss: 0.21509242057800293
Batch 17/64 loss: 0.22149968147277832
Batch 18/64 loss: 0.23038768768310547
Batch 19/64 loss: 0.23292213678359985
Batch 20/64 loss: 0.23071104288101196
Batch 21/64 loss: 0.22274601459503174
Batch 22/64 loss: 0.2267552614212036
Batch 23/64 loss: 0.21986401081085205
Batch 24/64 loss: 0.2305142879486084
Batch 25/64 loss: 0.2320953607559204
Batch 26/64 loss: 0.22661489248275757
Batch 27/64 loss: 0.2229013442993164
Batch 28/64 loss: 0.23588192462921143
Batch 29/64 loss: 0.22698044776916504
Batch 30/64 loss: 0.2423919439315796
Batch 31/64 loss: 0.22875386476516724
Batch 32/64 loss: 0.21832019090652466
Batch 33/64 loss: 0.24621814489364624
Batch 34/64 loss: 0.2295675277709961
Batch 35/64 loss: 0.23426365852355957
Batch 36/64 loss: 0.23619592189788818
Batch 37/64 loss: 0.23039507865905762
Batch 38/64 loss: 0.23249036073684692
Batch 39/64 loss: 0.24336785078048706
Batch 40/64 loss: 0.24379616975784302
Batch 41/64 loss: 0.2299429178237915
Batch 42/64 loss: 0.21431183815002441
Batch 43/64 loss: 0.23102867603302002
Batch 44/64 loss: 0.23039865493774414
Batch 45/64 loss: 0.2384766936302185
Batch 46/64 loss: 0.22085249423980713
Batch 47/64 loss: 0.22742122411727905
Batch 48/64 loss: 0.23243236541748047
Batch 49/64 loss: 0.24173372983932495
Batch 50/64 loss: 0.24332070350646973
Batch 51/64 loss: 0.24024152755737305
Batch 52/64 loss: 0.21914494037628174
Batch 53/64 loss: 0.23499709367752075
Batch 54/64 loss: 0.2320709228515625
Batch 55/64 loss: 0.23331356048583984
Batch 56/64 loss: 0.20985090732574463
Batch 57/64 loss: 0.21910274028778076
Batch 58/64 loss: 0.22678494453430176
Batch 59/64 loss: 0.23282063007354736
Batch 60/64 loss: 0.22654730081558228
Batch 61/64 loss: 0.2259429693222046
Batch 62/64 loss: 0.22948133945465088
Batch 63/64 loss: 0.24079954624176025
Batch 64/64 loss: 0.23675847053527832
Epoch 21  Train loss: 0.23044416577208277  Val loss: 0.23839229734492876
Saving best model, epoch: 21
Epoch 22
-------------------------------
Batch 1/64 loss: 0.21307158470153809
Batch 2/64 loss: 0.2213311791419983
Batch 3/64 loss: 0.22533661127090454
Batch 4/64 loss: 0.21893244981765747
Batch 5/64 loss: 0.22199928760528564
Batch 6/64 loss: 0.22884750366210938
Batch 7/64 loss: 0.2193201780319214
Batch 8/64 loss: 0.2184518575668335
Batch 9/64 loss: 0.22922354936599731
Batch 10/64 loss: 0.22935670614242554
Batch 11/64 loss: 0.2259814739227295
Batch 12/64 loss: 0.21531468629837036
Batch 13/64 loss: 0.22374552488327026
Batch 14/64 loss: 0.23533231019973755
Batch 15/64 loss: 0.22891908884048462
Batch 16/64 loss: 0.23108285665512085
Batch 17/64 loss: 0.23428332805633545
Batch 18/64 loss: 0.23594462871551514
Batch 19/64 loss: 0.23199462890625
Batch 20/64 loss: 0.22172027826309204
Batch 21/64 loss: 0.23644018173217773
Batch 22/64 loss: 0.23013466596603394
Batch 23/64 loss: 0.22998857498168945
Batch 24/64 loss: 0.23006081581115723
Batch 25/64 loss: 0.22955310344696045
Batch 26/64 loss: 0.2173614501953125
Batch 27/64 loss: 0.22456836700439453
Batch 28/64 loss: 0.23269563913345337
Batch 29/64 loss: 0.23297011852264404
Batch 30/64 loss: 0.23721885681152344
Batch 31/64 loss: 0.2291698455810547
Batch 32/64 loss: 0.21688109636306763
Batch 33/64 loss: 0.22967499494552612
Batch 34/64 loss: 0.23203372955322266
Batch 35/64 loss: 0.21356487274169922
Batch 36/64 loss: 0.21959418058395386
Batch 37/64 loss: 0.24305176734924316
Batch 38/64 loss: 0.23330116271972656
Batch 39/64 loss: 0.2228565216064453
Batch 40/64 loss: 0.2331169843673706
Batch 41/64 loss: 0.21576547622680664
Batch 42/64 loss: 0.21747487783432007
Batch 43/64 loss: 0.21513450145721436
Batch 44/64 loss: 0.24171096086502075
Batch 45/64 loss: 0.24517107009887695
Batch 46/64 loss: 0.21705865859985352
Batch 47/64 loss: 0.221840500831604
Batch 48/64 loss: 0.2146269679069519
Batch 49/64 loss: 0.20465683937072754
Batch 50/64 loss: 0.22749805450439453
Batch 51/64 loss: 0.22769290208816528
Batch 52/64 loss: 0.2309090495109558
Batch 53/64 loss: 0.22074562311172485
Batch 54/64 loss: 0.23078227043151855
Batch 55/64 loss: 0.2366238832473755
Batch 56/64 loss: 0.2253652811050415
Batch 57/64 loss: 0.23283666372299194
Batch 58/64 loss: 0.22622156143188477
Batch 59/64 loss: 0.2226172685623169
Batch 60/64 loss: 0.23712694644927979
Batch 61/64 loss: 0.22829699516296387
Batch 62/64 loss: 0.2266979217529297
Batch 63/64 loss: 0.22409796714782715
Batch 64/64 loss: 0.25197744369506836
Epoch 22  Train loss: 0.22689195240245147  Val loss: 0.233514232733815
Saving best model, epoch: 22
Epoch 23
-------------------------------
Batch 1/64 loss: 0.226371169090271
Batch 2/64 loss: 0.21883225440979004
Batch 3/64 loss: 0.2414117455482483
Batch 4/64 loss: 0.2289719581604004
Batch 5/64 loss: 0.23215186595916748
Batch 6/64 loss: 0.2310875654220581
Batch 7/64 loss: 0.23407208919525146
Batch 8/64 loss: 0.23555779457092285
Batch 9/64 loss: 0.2264251708984375
Batch 10/64 loss: 0.21172499656677246
Batch 11/64 loss: 0.21292197704315186
Batch 12/64 loss: 0.21956169605255127
Batch 13/64 loss: 0.23133188486099243
Batch 14/64 loss: 0.2336520552635193
Batch 15/64 loss: 0.2093597650527954
Batch 16/64 loss: 0.2326599359512329
Batch 17/64 loss: 0.21155011653900146
Batch 18/64 loss: 0.23023712635040283
Batch 19/64 loss: 0.21585285663604736
Batch 20/64 loss: 0.2227412462234497
Batch 21/64 loss: 0.2309933304786682
Batch 22/64 loss: 0.22570669651031494
Batch 23/64 loss: 0.2181781530380249
Batch 24/64 loss: 0.23784089088439941
Batch 25/64 loss: 0.2202489972114563
Batch 26/64 loss: 0.21159541606903076
Batch 27/64 loss: 0.22685158252716064
Batch 28/64 loss: 0.21684575080871582
Batch 29/64 loss: 0.23226308822631836
Batch 30/64 loss: 0.23096847534179688
Batch 31/64 loss: 0.21619713306427002
Batch 32/64 loss: 0.22731482982635498
Batch 33/64 loss: 0.22632068395614624
Batch 34/64 loss: 0.22068381309509277
Batch 35/64 loss: 0.20728015899658203
Batch 36/64 loss: 0.2278469204902649
Batch 37/64 loss: 0.23362290859222412
Batch 38/64 loss: 0.23026931285858154
Batch 39/64 loss: 0.2256178855895996
Batch 40/64 loss: 0.21435117721557617
Batch 41/64 loss: 0.22222566604614258
Batch 42/64 loss: 0.222123384475708
Batch 43/64 loss: 0.22896862030029297
Batch 44/64 loss: 0.20172762870788574
Batch 45/64 loss: 0.22327232360839844
Batch 46/64 loss: 0.21609795093536377
Batch 47/64 loss: 0.22283470630645752
Batch 48/64 loss: 0.2239837646484375
Batch 49/64 loss: 0.23484694957733154
Batch 50/64 loss: 0.2154219150543213
Batch 51/64 loss: 0.2269423007965088
Batch 52/64 loss: 0.22745680809020996
Batch 53/64 loss: 0.2307215929031372
Batch 54/64 loss: 0.21821928024291992
Batch 55/64 loss: 0.23196858167648315
Batch 56/64 loss: 0.21748632192611694
Batch 57/64 loss: 0.21891838312149048
Batch 58/64 loss: 0.22294652462005615
Batch 59/64 loss: 0.2284795641899109
Batch 60/64 loss: 0.2094389796257019
Batch 61/64 loss: 0.22992396354675293
Batch 62/64 loss: 0.23471814393997192
Batch 63/64 loss: 0.23002183437347412
Batch 64/64 loss: 0.22025275230407715
Epoch 23  Train loss: 0.2241789369022145  Val loss: 0.23175478841840608
Saving best model, epoch: 23
Epoch 24
-------------------------------
Batch 1/64 loss: 0.23557615280151367
Batch 2/64 loss: 0.2090538740158081
Batch 3/64 loss: 0.21837103366851807
Batch 4/64 loss: 0.22099268436431885
Batch 5/64 loss: 0.2137349247932434
Batch 6/64 loss: 0.22983479499816895
Batch 7/64 loss: 0.21701985597610474
Batch 8/64 loss: 0.23296386003494263
Batch 9/64 loss: 0.22516018152236938
Batch 10/64 loss: 0.2021661400794983
Batch 11/64 loss: 0.22354727983474731
Batch 12/64 loss: 0.2191472053527832
Batch 13/64 loss: 0.2066894769668579
Batch 14/64 loss: 0.22104668617248535
Batch 15/64 loss: 0.2138044834136963
Batch 16/64 loss: 0.22441071271896362
Batch 17/64 loss: 0.22121155261993408
Batch 18/64 loss: 0.21243023872375488
Batch 19/64 loss: 0.2368653416633606
Batch 20/64 loss: 0.21675032377243042
Batch 21/64 loss: 0.22056782245635986
Batch 22/64 loss: 0.2145857810974121
Batch 23/64 loss: 0.21894490718841553
Batch 24/64 loss: 0.22349870204925537
Batch 25/64 loss: 0.22060048580169678
Batch 26/64 loss: 0.22471654415130615
Batch 27/64 loss: 0.22710144519805908
Batch 28/64 loss: 0.22207945585250854
Batch 29/64 loss: 0.2214127779006958
Batch 30/64 loss: 0.22092211246490479
Batch 31/64 loss: 0.22280526161193848
Batch 32/64 loss: 0.2116602063179016
Batch 33/64 loss: 0.22453492879867554
Batch 34/64 loss: 0.2306046485900879
Batch 35/64 loss: 0.22569209337234497
Batch 36/64 loss: 0.22929036617279053
Batch 37/64 loss: 0.21181261539459229
Batch 38/64 loss: 0.21512490510940552
Batch 39/64 loss: 0.21741938591003418
Batch 40/64 loss: 0.21892154216766357
Batch 41/64 loss: 0.212546706199646
Batch 42/64 loss: 0.22509360313415527
Batch 43/64 loss: 0.2190868854522705
Batch 44/64 loss: 0.2290283441543579
Batch 45/64 loss: 0.21523135900497437
Batch 46/64 loss: 0.2254427671432495
Batch 47/64 loss: 0.2184525728225708
Batch 48/64 loss: 0.2150784134864807
Batch 49/64 loss: 0.21889019012451172
Batch 50/64 loss: 0.2131251096725464
Batch 51/64 loss: 0.20790702104568481
Batch 52/64 loss: 0.20782220363616943
Batch 53/64 loss: 0.21293389797210693
Batch 54/64 loss: 0.22644436359405518
Batch 55/64 loss: 0.23019284009933472
Batch 56/64 loss: 0.22337567806243896
Batch 57/64 loss: 0.21622121334075928
Batch 58/64 loss: 0.22581255435943604
Batch 59/64 loss: 0.2180216908454895
Batch 60/64 loss: 0.21936070919036865
Batch 61/64 loss: 0.2266373634338379
Batch 62/64 loss: 0.2051759958267212
Batch 63/64 loss: 0.22162997722625732
Batch 64/64 loss: 0.21351712942123413
Epoch 24  Train loss: 0.21987015055675133  Val loss: 0.22663968535223367
Saving best model, epoch: 24
Epoch 25
-------------------------------
Batch 1/64 loss: 0.19969403743743896
Batch 2/64 loss: 0.21291816234588623
Batch 3/64 loss: 0.2098844051361084
Batch 4/64 loss: 0.2236541509628296
Batch 5/64 loss: 0.22196221351623535
Batch 6/64 loss: 0.22086453437805176
Batch 7/64 loss: 0.2246209979057312
Batch 8/64 loss: 0.2248314619064331
Batch 9/64 loss: 0.21830159425735474
Batch 10/64 loss: 0.20264720916748047
Batch 11/64 loss: 0.21921038627624512
Batch 12/64 loss: 0.23571527004241943
Batch 13/64 loss: 0.22538864612579346
Batch 14/64 loss: 0.21001148223876953
Batch 15/64 loss: 0.22190582752227783
Batch 16/64 loss: 0.22629815340042114
Batch 17/64 loss: 0.20235157012939453
Batch 18/64 loss: 0.1926369071006775
Batch 19/64 loss: 0.2241905927658081
Batch 20/64 loss: 0.21273279190063477
Batch 21/64 loss: 0.23157024383544922
Batch 22/64 loss: 0.22134315967559814
Batch 23/64 loss: 0.20300590991973877
Batch 24/64 loss: 0.20038914680480957
Batch 25/64 loss: 0.21035897731781006
Batch 26/64 loss: 0.2143799066543579
Batch 27/64 loss: 0.22072547674179077
Batch 28/64 loss: 0.23216134309768677
Batch 29/64 loss: 0.21149909496307373
Batch 30/64 loss: 0.22368621826171875
Batch 31/64 loss: 0.21876168251037598
Batch 32/64 loss: 0.19793570041656494
Batch 33/64 loss: 0.2332324981689453
Batch 34/64 loss: 0.20181941986083984
Batch 35/64 loss: 0.20856648683547974
Batch 36/64 loss: 0.23095273971557617
Batch 37/64 loss: 0.22457075119018555
Batch 38/64 loss: 0.2081526517868042
Batch 39/64 loss: 0.22470474243164062
Batch 40/64 loss: 0.2191145420074463
Batch 41/64 loss: 0.21628844738006592
Batch 42/64 loss: 0.21574676036834717
Batch 43/64 loss: 0.23747223615646362
Batch 44/64 loss: 0.21814143657684326
Batch 45/64 loss: 0.2147068977355957
Batch 46/64 loss: 0.21569275856018066
Batch 47/64 loss: 0.22887331247329712
Batch 48/64 loss: 0.21396028995513916
Batch 49/64 loss: 0.198309063911438
Batch 50/64 loss: 0.22068309783935547
Batch 51/64 loss: 0.23816728591918945
Batch 52/64 loss: 0.20818722248077393
Batch 53/64 loss: 0.20922797918319702
Batch 54/64 loss: 0.2204185128211975
Batch 55/64 loss: 0.22117537260055542
Batch 56/64 loss: 0.22051537036895752
Batch 57/64 loss: 0.23247134685516357
Batch 58/64 loss: 0.22202378511428833
Batch 59/64 loss: 0.21131616830825806
Batch 60/64 loss: 0.22659361362457275
Batch 61/64 loss: 0.23947668075561523
Batch 62/64 loss: 0.22776520252227783
Batch 63/64 loss: 0.22420638799667358
Batch 64/64 loss: 0.204811692237854
Epoch 25  Train loss: 0.21806712290819955  Val loss: 0.22725547015462136
Epoch 26
-------------------------------
Batch 1/64 loss: 0.21220189332962036
Batch 2/64 loss: 0.20825397968292236
Batch 3/64 loss: 0.21462523937225342
Batch 4/64 loss: 0.20932739973068237
Batch 5/64 loss: 0.21699368953704834
Batch 6/64 loss: 0.19971495866775513
Batch 7/64 loss: 0.21832001209259033
Batch 8/64 loss: 0.21037781238555908
Batch 9/64 loss: 0.2265118956565857
Batch 10/64 loss: 0.20907914638519287
Batch 11/64 loss: 0.21048998832702637
Batch 12/64 loss: 0.2098931074142456
Batch 13/64 loss: 0.22945129871368408
Batch 14/64 loss: 0.22496116161346436
Batch 15/64 loss: 0.22427982091903687
Batch 16/64 loss: 0.21307867765426636
Batch 17/64 loss: 0.2000439167022705
Batch 18/64 loss: 0.20596742630004883
Batch 19/64 loss: 0.2101224660873413
Batch 20/64 loss: 0.2198050618171692
Batch 21/64 loss: 0.21965277194976807
Batch 22/64 loss: 0.21918606758117676
Batch 23/64 loss: 0.21011751890182495
Batch 24/64 loss: 0.22489070892333984
Batch 25/64 loss: 0.22540712356567383
Batch 26/64 loss: 0.21150600910186768
Batch 27/64 loss: 0.23399204015731812
Batch 28/64 loss: 0.24286824464797974
Batch 29/64 loss: 0.21679478883743286
Batch 30/64 loss: 0.22971820831298828
Batch 31/64 loss: 0.21053653955459595
Batch 32/64 loss: 0.2247544527053833
Batch 33/64 loss: 0.20641696453094482
Batch 34/64 loss: 0.20093464851379395
Batch 35/64 loss: 0.20508575439453125
Batch 36/64 loss: 0.226726233959198
Batch 37/64 loss: 0.21239876747131348
Batch 38/64 loss: 0.19970905780792236
Batch 39/64 loss: 0.2240978479385376
Batch 40/64 loss: 0.23064541816711426
Batch 41/64 loss: 0.21185439825057983
Batch 42/64 loss: 0.1959007978439331
Batch 43/64 loss: 0.23858106136322021
Batch 44/64 loss: 0.22026222944259644
Batch 45/64 loss: 0.21501415967941284
Batch 46/64 loss: 0.21164745092391968
Batch 47/64 loss: 0.2144392728805542
Batch 48/64 loss: 0.21830451488494873
Batch 49/64 loss: 0.22086584568023682
Batch 50/64 loss: 0.21800148487091064
Batch 51/64 loss: 0.2057957649230957
Batch 52/64 loss: 0.22148633003234863
Batch 53/64 loss: 0.2039228081703186
Batch 54/64 loss: 0.21092462539672852
Batch 55/64 loss: 0.21057558059692383
Batch 56/64 loss: 0.2239670753479004
Batch 57/64 loss: 0.21340835094451904
Batch 58/64 loss: 0.21407437324523926
Batch 59/64 loss: 0.21476083993911743
Batch 60/64 loss: 0.21243488788604736
Batch 61/64 loss: 0.2157907485961914
Batch 62/64 loss: 0.20079857110977173
Batch 63/64 loss: 0.22344845533370972
Batch 64/64 loss: 0.20562684535980225
Epoch 26  Train loss: 0.21552028048272226  Val loss: 0.22433261936882518
Saving best model, epoch: 26
Epoch 27
-------------------------------
Batch 1/64 loss: 0.19935429096221924
Batch 2/64 loss: 0.19874286651611328
Batch 3/64 loss: 0.22016417980194092
Batch 4/64 loss: 0.2175079584121704
Batch 5/64 loss: 0.2069394588470459
Batch 6/64 loss: 0.22116881608963013
Batch 7/64 loss: 0.2214655876159668
Batch 8/64 loss: 0.21033036708831787
Batch 9/64 loss: 0.22009992599487305
Batch 10/64 loss: 0.19064182043075562
Batch 11/64 loss: 0.19593244791030884
Batch 12/64 loss: 0.23078787326812744
Batch 13/64 loss: 0.20925116539001465
Batch 14/64 loss: 0.20762509107589722
Batch 15/64 loss: 0.20793229341506958
Batch 16/64 loss: 0.2114049196243286
Batch 17/64 loss: 0.214718759059906
Batch 18/64 loss: 0.2078847885131836
Batch 19/64 loss: 0.20494616031646729
Batch 20/64 loss: 0.19980597496032715
Batch 21/64 loss: 0.20940542221069336
Batch 22/64 loss: 0.21127557754516602
Batch 23/64 loss: 0.20303583145141602
Batch 24/64 loss: 0.209544837474823
Batch 25/64 loss: 0.2130565643310547
Batch 26/64 loss: 0.2306152582168579
Batch 27/64 loss: 0.21008294820785522
Batch 28/64 loss: 0.2052883505821228
Batch 29/64 loss: 0.2023959755897522
Batch 30/64 loss: 0.21595638990402222
Batch 31/64 loss: 0.235221266746521
Batch 32/64 loss: 0.20947229862213135
Batch 33/64 loss: 0.20679885149002075
Batch 34/64 loss: 0.20838308334350586
Batch 35/64 loss: 0.2263551950454712
Batch 36/64 loss: 0.2203143835067749
Batch 37/64 loss: 0.21318435668945312
Batch 38/64 loss: 0.23110496997833252
Batch 39/64 loss: 0.21061444282531738
Batch 40/64 loss: 0.2023773193359375
Batch 41/64 loss: 0.1943952441215515
Batch 42/64 loss: 0.21523785591125488
Batch 43/64 loss: 0.22160136699676514
Batch 44/64 loss: 0.21429157257080078
Batch 45/64 loss: 0.2004222869873047
Batch 46/64 loss: 0.17477631568908691
Batch 47/64 loss: 0.21922039985656738
Batch 48/64 loss: 0.20167195796966553
Batch 49/64 loss: 0.199013352394104
Batch 50/64 loss: 0.23048579692840576
Batch 51/64 loss: 0.21106815338134766
Batch 52/64 loss: 0.21743667125701904
Batch 53/64 loss: 0.22033679485321045
Batch 54/64 loss: 0.22091716527938843
Batch 55/64 loss: 0.1981598138809204
Batch 56/64 loss: 0.2001703381538391
Batch 57/64 loss: 0.20548194646835327
Batch 58/64 loss: 0.20356488227844238
Batch 59/64 loss: 0.20613300800323486
Batch 60/64 loss: 0.22318804264068604
Batch 61/64 loss: 0.22037220001220703
Batch 62/64 loss: 0.2070457935333252
Batch 63/64 loss: 0.2105655074119568
Batch 64/64 loss: 0.2062535285949707
Epoch 27  Train loss: 0.21084603421828327  Val loss: 0.21771616235221783
Saving best model, epoch: 27
Epoch 28
-------------------------------
Batch 1/64 loss: 0.20196247100830078
Batch 2/64 loss: 0.22098326683044434
Batch 3/64 loss: 0.21545833349227905
Batch 4/64 loss: 0.19723206758499146
Batch 5/64 loss: 0.19314032793045044
Batch 6/64 loss: 0.1912846565246582
Batch 7/64 loss: 0.22469168901443481
Batch 8/64 loss: 0.2064957618713379
Batch 9/64 loss: 0.22701239585876465
Batch 10/64 loss: 0.2146892547607422
Batch 11/64 loss: 0.20800304412841797
Batch 12/64 loss: 0.21091222763061523
Batch 13/64 loss: 0.20175707340240479
Batch 14/64 loss: 0.20349937677383423
Batch 15/64 loss: 0.21393543481826782
Batch 16/64 loss: 0.21953517198562622
Batch 17/64 loss: 0.206196129322052
Batch 18/64 loss: 0.20165753364562988
Batch 19/64 loss: 0.20520806312561035
Batch 20/64 loss: 0.21762287616729736
Batch 21/64 loss: 0.19323813915252686
Batch 22/64 loss: 0.1988011598587036
Batch 23/64 loss: 0.21497535705566406
Batch 24/64 loss: 0.20047903060913086
Batch 25/64 loss: 0.21119177341461182
Batch 26/64 loss: 0.21648550033569336
Batch 27/64 loss: 0.20312750339508057
Batch 28/64 loss: 0.21386587619781494
Batch 29/64 loss: 0.1925523281097412
Batch 30/64 loss: 0.21637988090515137
Batch 31/64 loss: 0.22917962074279785
Batch 32/64 loss: 0.21957731246948242
Batch 33/64 loss: 0.2129068374633789
Batch 34/64 loss: 0.2171432375907898
Batch 35/64 loss: 0.21538913249969482
Batch 36/64 loss: 0.19505226612091064
Batch 37/64 loss: 0.21725863218307495
Batch 38/64 loss: 0.2212064266204834
Batch 39/64 loss: 0.20680487155914307
Batch 40/64 loss: 0.22212821245193481
Batch 41/64 loss: 0.19683128595352173
Batch 42/64 loss: 0.20612794160842896
Batch 43/64 loss: 0.21697282791137695
Batch 44/64 loss: 0.2294696569442749
Batch 45/64 loss: 0.2079254388809204
Batch 46/64 loss: 0.19961726665496826
Batch 47/64 loss: 0.21213191747665405
Batch 48/64 loss: 0.20622360706329346
Batch 49/64 loss: 0.22071218490600586
Batch 50/64 loss: 0.20611560344696045
Batch 51/64 loss: 0.2008345127105713
Batch 52/64 loss: 0.20657438039779663
Batch 53/64 loss: 0.22546792030334473
Batch 54/64 loss: 0.20032429695129395
Batch 55/64 loss: 0.20925331115722656
Batch 56/64 loss: 0.1942177414894104
Batch 57/64 loss: 0.20757585763931274
Batch 58/64 loss: 0.22314924001693726
Batch 59/64 loss: 0.19898396730422974
Batch 60/64 loss: 0.21104609966278076
Batch 61/64 loss: 0.22088563442230225
Batch 62/64 loss: 0.21558964252471924
Batch 63/64 loss: 0.2052600383758545
Batch 64/64 loss: 0.19610553979873657
Epoch 28  Train loss: 0.209684545619815  Val loss: 0.21755189707189082
Saving best model, epoch: 28
Epoch 29
-------------------------------
Batch 1/64 loss: 0.19509822130203247
Batch 2/64 loss: 0.2085062861442566
Batch 3/64 loss: 0.2115774154663086
Batch 4/64 loss: 0.20914983749389648
Batch 5/64 loss: 0.19317150115966797
Batch 6/64 loss: 0.20344120264053345
Batch 7/64 loss: 0.2033894658088684
Batch 8/64 loss: 0.20341253280639648
Batch 9/64 loss: 0.21374082565307617
Batch 10/64 loss: 0.2092457413673401
Batch 11/64 loss: 0.21391546726226807
Batch 12/64 loss: 0.20268523693084717
Batch 13/64 loss: 0.22282791137695312
Batch 14/64 loss: 0.20931673049926758
Batch 15/64 loss: 0.21537446975708008
Batch 16/64 loss: 0.20331913232803345
Batch 17/64 loss: 0.20542669296264648
Batch 18/64 loss: 0.19590657949447632
Batch 19/64 loss: 0.19380295276641846
Batch 20/64 loss: 0.19340598583221436
Batch 21/64 loss: 0.19389337301254272
Batch 22/64 loss: 0.20516341924667358
Batch 23/64 loss: 0.21336960792541504
Batch 24/64 loss: 0.19872426986694336
Batch 25/64 loss: 0.21236461400985718
Batch 26/64 loss: 0.2165049910545349
Batch 27/64 loss: 0.20936542749404907
Batch 28/64 loss: 0.2025984525680542
Batch 29/64 loss: 0.2041037678718567
Batch 30/64 loss: 0.21854650974273682
Batch 31/64 loss: 0.21520709991455078
Batch 32/64 loss: 0.21152788400650024
Batch 33/64 loss: 0.22853350639343262
Batch 34/64 loss: 0.20958828926086426
Batch 35/64 loss: 0.2004563808441162
Batch 36/64 loss: 0.21412020921707153
Batch 37/64 loss: 0.20391225814819336
Batch 38/64 loss: 0.19254422187805176
Batch 39/64 loss: 0.21054136753082275
Batch 40/64 loss: 0.21158385276794434
Batch 41/64 loss: 0.21289277076721191
Batch 42/64 loss: 0.18996042013168335
Batch 43/64 loss: 0.20198595523834229
Batch 44/64 loss: 0.1958460807800293
Batch 45/64 loss: 0.19981658458709717
Batch 46/64 loss: 0.19556701183319092
Batch 47/64 loss: 0.22355866432189941
Batch 48/64 loss: 0.22384929656982422
Batch 49/64 loss: 0.19293975830078125
Batch 50/64 loss: 0.1923694610595703
Batch 51/64 loss: 0.20660054683685303
Batch 52/64 loss: 0.19331562519073486
Batch 53/64 loss: 0.20595592260360718
Batch 54/64 loss: 0.21400487422943115
Batch 55/64 loss: 0.21165341138839722
Batch 56/64 loss: 0.21217596530914307
Batch 57/64 loss: 0.19831109046936035
Batch 58/64 loss: 0.20501708984375
Batch 59/64 loss: 0.204095721244812
Batch 60/64 loss: 0.20292139053344727
Batch 61/64 loss: 0.18841934204101562
Batch 62/64 loss: 0.1930776834487915
Batch 63/64 loss: 0.19369828701019287
Batch 64/64 loss: 0.20717090368270874
Epoch 29  Train loss: 0.20537685997345867  Val loss: 0.21736724474995406
Saving best model, epoch: 29
Epoch 30
-------------------------------
Batch 1/64 loss: 0.2141554355621338
Batch 2/64 loss: 0.18907982110977173
Batch 3/64 loss: 0.1957644820213318
Batch 4/64 loss: 0.20913255214691162
Batch 5/64 loss: 0.19004946947097778
Batch 6/64 loss: 0.19127798080444336
Batch 7/64 loss: 0.18430960178375244
Batch 8/64 loss: 0.20374202728271484
Batch 9/64 loss: 0.17831909656524658
Batch 10/64 loss: 0.21269816160202026
Batch 11/64 loss: 0.1923767328262329
Batch 12/64 loss: 0.19147294759750366
Batch 13/64 loss: 0.22217559814453125
Batch 14/64 loss: 0.1975805163383484
Batch 15/64 loss: 0.20603448152542114
Batch 16/64 loss: 0.214350163936615
Batch 17/64 loss: 0.194801926612854
Batch 18/64 loss: 0.20281696319580078
Batch 19/64 loss: 0.21159374713897705
Batch 20/64 loss: 0.20917773246765137
Batch 21/64 loss: 0.22381895780563354
Batch 22/64 loss: 0.19413501024246216
Batch 23/64 loss: 0.20190727710723877
Batch 24/64 loss: 0.21713018417358398
Batch 25/64 loss: 0.2019408941268921
Batch 26/64 loss: 0.19446474313735962
Batch 27/64 loss: 0.20531344413757324
Batch 28/64 loss: 0.1930321455001831
Batch 29/64 loss: 0.20857608318328857
Batch 30/64 loss: 0.20737051963806152
Batch 31/64 loss: 0.19864213466644287
Batch 32/64 loss: 0.19073522090911865
Batch 33/64 loss: 0.19833886623382568
Batch 34/64 loss: 0.2016308307647705
Batch 35/64 loss: 0.22211313247680664
Batch 36/64 loss: 0.19157922267913818
Batch 37/64 loss: 0.2275761365890503
Batch 38/64 loss: 0.21009117364883423
Batch 39/64 loss: 0.204787015914917
Batch 40/64 loss: 0.2189456820487976
Batch 41/64 loss: 0.21409040689468384
Batch 42/64 loss: 0.1956421136856079
Batch 43/64 loss: 0.19037485122680664
Batch 44/64 loss: 0.1995837688446045
Batch 45/64 loss: 0.19425225257873535
Batch 46/64 loss: 0.1918080449104309
Batch 47/64 loss: 0.21071243286132812
Batch 48/64 loss: 0.201163649559021
Batch 49/64 loss: 0.19027495384216309
Batch 50/64 loss: 0.21450352668762207
Batch 51/64 loss: 0.2009572982788086
Batch 52/64 loss: 0.21179187297821045
Batch 53/64 loss: 0.18040084838867188
Batch 54/64 loss: 0.21547818183898926
Batch 55/64 loss: 0.18358975648880005
Batch 56/64 loss: 0.22046387195587158
Batch 57/64 loss: 0.1993253231048584
Batch 58/64 loss: 0.21199536323547363
Batch 59/64 loss: 0.21530216932296753
Batch 60/64 loss: 0.22553396224975586
Batch 61/64 loss: 0.20479083061218262
Batch 62/64 loss: 0.20168030261993408
Batch 63/64 loss: 0.20031100511550903
Batch 64/64 loss: 0.18587815761566162
Epoch 30  Train loss: 0.2029250748017255  Val loss: 0.2127858193469621
Saving best model, epoch: 30
Epoch 31
-------------------------------
Batch 1/64 loss: 0.2023017406463623
Batch 2/64 loss: 0.18045955896377563
Batch 3/64 loss: 0.19575703144073486
Batch 4/64 loss: 0.2062361240386963
Batch 5/64 loss: 0.1925961971282959
Batch 6/64 loss: 0.19121718406677246
Batch 7/64 loss: 0.21356046199798584
Batch 8/64 loss: 0.22376412153244019
Batch 9/64 loss: 0.18710899353027344
Batch 10/64 loss: 0.2094048261642456
Batch 11/64 loss: 0.197792649269104
Batch 12/64 loss: 0.18254303932189941
Batch 13/64 loss: 0.19905877113342285
Batch 14/64 loss: 0.204170823097229
Batch 15/64 loss: 0.2006569504737854
Batch 16/64 loss: 0.2070608139038086
Batch 17/64 loss: 0.18799901008605957
Batch 18/64 loss: 0.2097061276435852
Batch 19/64 loss: 0.19576895236968994
Batch 20/64 loss: 0.20130157470703125
Batch 21/64 loss: 0.20553231239318848
Batch 22/64 loss: 0.20086395740509033
Batch 23/64 loss: 0.1847296953201294
Batch 24/64 loss: 0.19744974374771118
Batch 25/64 loss: 0.19539523124694824
Batch 26/64 loss: 0.19826745986938477
Batch 27/64 loss: 0.18735671043395996
Batch 28/64 loss: 0.20292115211486816
Batch 29/64 loss: 0.22009003162384033
Batch 30/64 loss: 0.20628106594085693
Batch 31/64 loss: 0.18507057428359985
Batch 32/64 loss: 0.1913090944290161
Batch 33/64 loss: 0.20326942205429077
Batch 34/64 loss: 0.18899184465408325
Batch 35/64 loss: 0.19699108600616455
Batch 36/64 loss: 0.21243727207183838
Batch 37/64 loss: 0.1741272211074829
Batch 38/64 loss: 0.18218868970870972
Batch 39/64 loss: 0.19839847087860107
Batch 40/64 loss: 0.22188127040863037
Batch 41/64 loss: 0.20844686031341553
Batch 42/64 loss: 0.20217275619506836
Batch 43/64 loss: 0.19258397817611694
Batch 44/64 loss: 0.20450371503829956
Batch 45/64 loss: 0.18834495544433594
Batch 46/64 loss: 0.18775081634521484
Batch 47/64 loss: 0.21987789869308472
Batch 48/64 loss: 0.2184067964553833
Batch 49/64 loss: 0.19098913669586182
Batch 50/64 loss: 0.189314603805542
Batch 51/64 loss: 0.19243478775024414
Batch 52/64 loss: 0.19605064392089844
Batch 53/64 loss: 0.20911532640457153
Batch 54/64 loss: 0.20870137214660645
Batch 55/64 loss: 0.22475266456604004
Batch 56/64 loss: 0.2017439603805542
Batch 57/64 loss: 0.20468950271606445
Batch 58/64 loss: 0.22035598754882812
Batch 59/64 loss: 0.20029568672180176
Batch 60/64 loss: 0.2145586609840393
Batch 61/64 loss: 0.19749772548675537
Batch 62/64 loss: 0.1796499490737915
Batch 63/64 loss: 0.2098400592803955
Batch 64/64 loss: 0.20234805345535278
Epoch 31  Train loss: 0.2000918610423219  Val loss: 0.21219443230284857
Saving best model, epoch: 31
Epoch 32
-------------------------------
Batch 1/64 loss: 0.17846918106079102
Batch 2/64 loss: 0.19491183757781982
Batch 3/64 loss: 0.19389033317565918
Batch 4/64 loss: 0.19480562210083008
Batch 5/64 loss: 0.20392203330993652
Batch 6/64 loss: 0.20741236209869385
Batch 7/64 loss: 0.18384623527526855
Batch 8/64 loss: 0.19146323204040527
Batch 9/64 loss: 0.20990508794784546
Batch 10/64 loss: 0.1906447410583496
Batch 11/64 loss: 0.19615697860717773
Batch 12/64 loss: 0.2052011489868164
Batch 13/64 loss: 0.19562208652496338
Batch 14/64 loss: 0.1936800479888916
Batch 15/64 loss: 0.19610720872879028
Batch 16/64 loss: 0.18330293893814087
Batch 17/64 loss: 0.20500200986862183
Batch 18/64 loss: 0.207938551902771
Batch 19/64 loss: 0.2042461633682251
Batch 20/64 loss: 0.2166156768798828
Batch 21/64 loss: 0.20021295547485352
Batch 22/64 loss: 0.20932257175445557
Batch 23/64 loss: 0.1928691864013672
Batch 24/64 loss: 0.21327996253967285
Batch 25/64 loss: 0.18698298931121826
Batch 26/64 loss: 0.20340943336486816
Batch 27/64 loss: 0.19869107007980347
Batch 28/64 loss: 0.19462209939956665
Batch 29/64 loss: 0.19102954864501953
Batch 30/64 loss: 0.20109736919403076
Batch 31/64 loss: 0.20926618576049805
Batch 32/64 loss: 0.19205904006958008
Batch 33/64 loss: 0.22052216529846191
Batch 34/64 loss: 0.20219767093658447
Batch 35/64 loss: 0.20590424537658691
Batch 36/64 loss: 0.2043272852897644
Batch 37/64 loss: 0.2117341160774231
Batch 38/64 loss: 0.20366621017456055
Batch 39/64 loss: 0.19411814212799072
Batch 40/64 loss: 0.19901740550994873
Batch 41/64 loss: 0.1907256841659546
Batch 42/64 loss: 0.18761610984802246
Batch 43/64 loss: 0.18655872344970703
Batch 44/64 loss: 0.19223642349243164
Batch 45/64 loss: 0.19248300790786743
Batch 46/64 loss: 0.2245246171951294
Batch 47/64 loss: 0.19644570350646973
Batch 48/64 loss: 0.17653822898864746
Batch 49/64 loss: 0.20850014686584473
Batch 50/64 loss: 0.18647897243499756
Batch 51/64 loss: 0.20435982942581177
Batch 52/64 loss: 0.18567490577697754
Batch 53/64 loss: 0.18764543533325195
Batch 54/64 loss: 0.20160174369812012
Batch 55/64 loss: 0.20903289318084717
Batch 56/64 loss: 0.18930715322494507
Batch 57/64 loss: 0.20174145698547363
Batch 58/64 loss: 0.18765783309936523
Batch 59/64 loss: 0.17949718236923218
Batch 60/64 loss: 0.19636905193328857
Batch 61/64 loss: 0.20234793424606323
Batch 62/64 loss: 0.1909182071685791
Batch 63/64 loss: 0.1792987585067749
Batch 64/64 loss: 0.1962605118751526
Epoch 32  Train loss: 0.19752515320684397  Val loss: 0.21063198359151886
Saving best model, epoch: 32
Epoch 33
-------------------------------
Batch 1/64 loss: 0.1759394407272339
Batch 2/64 loss: 0.17443251609802246
Batch 3/64 loss: 0.20002204179763794
Batch 4/64 loss: 0.19347530603408813
Batch 5/64 loss: 0.19336748123168945
Batch 6/64 loss: 0.21341973543167114
Batch 7/64 loss: 0.19952988624572754
Batch 8/64 loss: 0.20283377170562744
Batch 9/64 loss: 0.1983402967453003
Batch 10/64 loss: 0.2024322748184204
Batch 11/64 loss: 0.18911445140838623
Batch 12/64 loss: 0.17081129550933838
Batch 13/64 loss: 0.18384408950805664
Batch 14/64 loss: 0.2119082808494568
Batch 15/64 loss: 0.1970452070236206
Batch 16/64 loss: 0.18353068828582764
Batch 17/64 loss: 0.17729973793029785
Batch 18/64 loss: 0.2068333625793457
Batch 19/64 loss: 0.1933525800704956
Batch 20/64 loss: 0.2002251148223877
Batch 21/64 loss: 0.19151532649993896
Batch 22/64 loss: 0.20115548372268677
Batch 23/64 loss: 0.2021743655204773
Batch 24/64 loss: 0.21031802892684937
Batch 25/64 loss: 0.19669461250305176
Batch 26/64 loss: 0.2067655324935913
Batch 27/64 loss: 0.20485079288482666
Batch 28/64 loss: 0.18098878860473633
Batch 29/64 loss: 0.2026376724243164
Batch 30/64 loss: 0.2054882049560547
Batch 31/64 loss: 0.17824941873550415
Batch 32/64 loss: 0.19083058834075928
Batch 33/64 loss: 0.20552009344100952
Batch 34/64 loss: 0.20264363288879395
Batch 35/64 loss: 0.2048892378807068
Batch 36/64 loss: 0.20008301734924316
Batch 37/64 loss: 0.19540250301361084
Batch 38/64 loss: 0.20412445068359375
Batch 39/64 loss: 0.21112608909606934
Batch 40/64 loss: 0.21790361404418945
Batch 41/64 loss: 0.21533888578414917
Batch 42/64 loss: 0.19146454334259033
Batch 43/64 loss: 0.1835649013519287
Batch 44/64 loss: 0.19385266304016113
Batch 45/64 loss: 0.21547549962997437
Batch 46/64 loss: 0.17860156297683716
Batch 47/64 loss: 0.21928972005844116
Batch 48/64 loss: 0.19495654106140137
Batch 49/64 loss: 0.1860964298248291
Batch 50/64 loss: 0.18811523914337158
Batch 51/64 loss: 0.18447738885879517
Batch 52/64 loss: 0.2026624083518982
Batch 53/64 loss: 0.1978929042816162
Batch 54/64 loss: 0.19337159395217896
Batch 55/64 loss: 0.1865251660346985
Batch 56/64 loss: 0.1894921064376831
Batch 57/64 loss: 0.19291257858276367
Batch 58/64 loss: 0.19522053003311157
Batch 59/64 loss: 0.19642198085784912
Batch 60/64 loss: 0.20132660865783691
Batch 61/64 loss: 0.20958656072616577
Batch 62/64 loss: 0.1785118579864502
Batch 63/64 loss: 0.20312249660491943
Batch 64/64 loss: 0.20485663414001465
Epoch 33  Train loss: 0.19659673185909496  Val loss: 0.20885133579424567
Saving best model, epoch: 33
Epoch 34
-------------------------------
Batch 1/64 loss: 0.20130455493927002
Batch 2/64 loss: 0.19980746507644653
Batch 3/64 loss: 0.1966397762298584
Batch 4/64 loss: 0.19719910621643066
Batch 5/64 loss: 0.18456876277923584
Batch 6/64 loss: 0.19979918003082275
Batch 7/64 loss: 0.1913999319076538
Batch 8/64 loss: 0.18593692779541016
Batch 9/64 loss: 0.17982280254364014
Batch 10/64 loss: 0.19921696186065674
Batch 11/64 loss: 0.19085121154785156
Batch 12/64 loss: 0.18541443347930908
Batch 13/64 loss: 0.19470906257629395
Batch 14/64 loss: 0.21062719821929932
Batch 15/64 loss: 0.1995147466659546
Batch 16/64 loss: 0.19779646396636963
Batch 17/64 loss: 0.17917490005493164
Batch 18/64 loss: 0.19505620002746582
Batch 19/64 loss: 0.19009935855865479
Batch 20/64 loss: 0.19953852891921997
Batch 21/64 loss: 0.20715326070785522
Batch 22/64 loss: 0.17824530601501465
Batch 23/64 loss: 0.1852043867111206
Batch 24/64 loss: 0.2081519365310669
Batch 25/64 loss: 0.18330276012420654
Batch 26/64 loss: 0.19073975086212158
Batch 27/64 loss: 0.19179439544677734
Batch 28/64 loss: 0.19281041622161865
Batch 29/64 loss: 0.20343375205993652
Batch 30/64 loss: 0.21064507961273193
Batch 31/64 loss: 0.18117964267730713
Batch 32/64 loss: 0.18163180351257324
Batch 33/64 loss: 0.19345623254776
Batch 34/64 loss: 0.18287312984466553
Batch 35/64 loss: 0.18644863367080688
Batch 36/64 loss: 0.18720018863677979
Batch 37/64 loss: 0.20395582914352417
Batch 38/64 loss: 0.2053963541984558
Batch 39/64 loss: 0.19334858655929565
Batch 40/64 loss: 0.1861998438835144
Batch 41/64 loss: 0.17034119367599487
Batch 42/64 loss: 0.18972992897033691
Batch 43/64 loss: 0.19180679321289062
Batch 44/64 loss: 0.1762523055076599
Batch 45/64 loss: 0.19170546531677246
Batch 46/64 loss: 0.1910088062286377
Batch 47/64 loss: 0.20461523532867432
Batch 48/64 loss: 0.1840287446975708
Batch 49/64 loss: 0.18028074502944946
Batch 50/64 loss: 0.1899329423904419
Batch 51/64 loss: 0.1960276961326599
Batch 52/64 loss: 0.19794243574142456
Batch 53/64 loss: 0.1980077028274536
Batch 54/64 loss: 0.2144019603729248
Batch 55/64 loss: 0.17868387699127197
Batch 56/64 loss: 0.18656277656555176
Batch 57/64 loss: 0.1828080415725708
Batch 58/64 loss: 0.19387412071228027
Batch 59/64 loss: 0.20518124103546143
Batch 60/64 loss: 0.17257046699523926
Batch 61/64 loss: 0.20235276222229004
Batch 62/64 loss: 0.18661892414093018
Batch 63/64 loss: 0.2013758420944214
Batch 64/64 loss: 0.1819247603416443
Epoch 34  Train loss: 0.19222278337852627  Val loss: 0.20472403121568084
Saving best model, epoch: 34
Epoch 35
-------------------------------
Batch 1/64 loss: 0.19565409421920776
Batch 2/64 loss: 0.19905853271484375
Batch 3/64 loss: 0.18054258823394775
Batch 4/64 loss: 0.20208019018173218
Batch 5/64 loss: 0.19896602630615234
Batch 6/64 loss: 0.18472421169281006
Batch 7/64 loss: 0.19248241186141968
Batch 8/64 loss: 0.20872652530670166
Batch 9/64 loss: 0.191575825214386
Batch 10/64 loss: 0.19510328769683838
Batch 11/64 loss: 0.18293452262878418
Batch 12/64 loss: 0.19439911842346191
Batch 13/64 loss: 0.20016968250274658
Batch 14/64 loss: 0.20823723077774048
Batch 15/64 loss: 0.18624472618103027
Batch 16/64 loss: 0.20967978239059448
Batch 17/64 loss: 0.16831237077713013
Batch 18/64 loss: 0.18293416500091553
Batch 19/64 loss: 0.1837443709373474
Batch 20/64 loss: 0.17329853773117065
Batch 21/64 loss: 0.19262349605560303
Batch 22/64 loss: 0.18470436334609985
Batch 23/64 loss: 0.20029211044311523
Batch 24/64 loss: 0.16223084926605225
Batch 25/64 loss: 0.20314502716064453
Batch 26/64 loss: 0.18390226364135742
Batch 27/64 loss: 0.17838382720947266
Batch 28/64 loss: 0.1995624303817749
Batch 29/64 loss: 0.1938086748123169
Batch 30/64 loss: 0.19223737716674805
Batch 31/64 loss: 0.18621724843978882
Batch 32/64 loss: 0.17832547426223755
Batch 33/64 loss: 0.19243454933166504
Batch 34/64 loss: 0.1754828691482544
Batch 35/64 loss: 0.19552719593048096
Batch 36/64 loss: 0.18823975324630737
Batch 37/64 loss: 0.19305318593978882
Batch 38/64 loss: 0.1922590732574463
Batch 39/64 loss: 0.17211616039276123
Batch 40/64 loss: 0.1936352252960205
Batch 41/64 loss: 0.19720911979675293
Batch 42/64 loss: 0.19277048110961914
Batch 43/64 loss: 0.18051373958587646
Batch 44/64 loss: 0.20313459634780884
Batch 45/64 loss: 0.186240553855896
Batch 46/64 loss: 0.20685851573944092
Batch 47/64 loss: 0.19470572471618652
Batch 48/64 loss: 0.1757100224494934
Batch 49/64 loss: 0.18971765041351318
Batch 50/64 loss: 0.1778826117515564
Batch 51/64 loss: 0.19057178497314453
Batch 52/64 loss: 0.20074999332427979
Batch 53/64 loss: 0.20877909660339355
Batch 54/64 loss: 0.20280563831329346
Batch 55/64 loss: 0.18615645170211792
Batch 56/64 loss: 0.17648005485534668
Batch 57/64 loss: 0.1859310269355774
Batch 58/64 loss: 0.16275155544281006
Batch 59/64 loss: 0.1937541961669922
Batch 60/64 loss: 0.20666015148162842
Batch 61/64 loss: 0.17971980571746826
Batch 62/64 loss: 0.21172189712524414
Batch 63/64 loss: 0.19701939821243286
Batch 64/64 loss: 0.17937570810317993
Epoch 35  Train loss: 0.19042235612869263  Val loss: 0.20233592012084226
Saving best model, epoch: 35
Epoch 36
-------------------------------
Batch 1/64 loss: 0.18040668964385986
Batch 2/64 loss: 0.19484210014343262
Batch 3/64 loss: 0.19330108165740967
Batch 4/64 loss: 0.17095541954040527
Batch 5/64 loss: 0.20561766624450684
Batch 6/64 loss: 0.1848658323287964
Batch 7/64 loss: 0.1918095350265503
Batch 8/64 loss: 0.1923741102218628
Batch 9/64 loss: 0.18725967407226562
Batch 10/64 loss: 0.20741117000579834
Batch 11/64 loss: 0.18347078561782837
Batch 12/64 loss: 0.19520413875579834
Batch 13/64 loss: 0.19699913263320923
Batch 14/64 loss: 0.1936168670654297
Batch 15/64 loss: 0.18284201622009277
Batch 16/64 loss: 0.1751035451889038
Batch 17/64 loss: 0.174608051776886
Batch 18/64 loss: 0.2072693109512329
Batch 19/64 loss: 0.17691820859909058
Batch 20/64 loss: 0.21190094947814941
Batch 21/64 loss: 0.17825722694396973
Batch 22/64 loss: 0.19116145372390747
Batch 23/64 loss: 0.1985849142074585
Batch 24/64 loss: 0.19388902187347412
Batch 25/64 loss: 0.1754699945449829
Batch 26/64 loss: 0.19875788688659668
Batch 27/64 loss: 0.17370843887329102
Batch 28/64 loss: 0.19552946090698242
Batch 29/64 loss: 0.20332717895507812
Batch 30/64 loss: 0.1853163242340088
Batch 31/64 loss: 0.2041705846786499
Batch 32/64 loss: 0.1900089979171753
Batch 33/64 loss: 0.19778406620025635
Batch 34/64 loss: 0.18789231777191162
Batch 35/64 loss: 0.18797576427459717
Batch 36/64 loss: 0.18424761295318604
Batch 37/64 loss: 0.18909376859664917
Batch 38/64 loss: 0.19849205017089844
Batch 39/64 loss: 0.1938457489013672
Batch 40/64 loss: 0.2055400013923645
Batch 41/64 loss: 0.194993257522583
Batch 42/64 loss: 0.1744200587272644
Batch 43/64 loss: 0.19355309009552002
Batch 44/64 loss: 0.1756764054298401
Batch 45/64 loss: 0.17977052927017212
Batch 46/64 loss: 0.1972421407699585
Batch 47/64 loss: 0.20800459384918213
Batch 48/64 loss: 0.1942138671875
Batch 49/64 loss: 0.18307310342788696
Batch 50/64 loss: 0.18661749362945557
Batch 51/64 loss: 0.21550589799880981
Batch 52/64 loss: 0.18441200256347656
Batch 53/64 loss: 0.19043517112731934
Batch 54/64 loss: 0.20603632926940918
Batch 55/64 loss: 0.2111802101135254
Batch 56/64 loss: 0.18166100978851318
Batch 57/64 loss: 0.1779940128326416
Batch 58/64 loss: 0.1809607744216919
Batch 59/64 loss: 0.16423296928405762
Batch 60/64 loss: 0.17692983150482178
Batch 61/64 loss: 0.1823362112045288
Batch 62/64 loss: 0.18170416355133057
Batch 63/64 loss: 0.1816960573196411
Batch 64/64 loss: 0.17490863800048828
Epoch 36  Train loss: 0.18970446680106368  Val loss: 0.20517724363254927
Epoch 37
-------------------------------
Batch 1/64 loss: 0.17700070142745972
Batch 2/64 loss: 0.19272339344024658
Batch 3/64 loss: 0.19116878509521484
Batch 4/64 loss: 0.19389605522155762
Batch 5/64 loss: 0.1712130308151245
Batch 6/64 loss: 0.20297354459762573
Batch 7/64 loss: 0.17123955488204956
Batch 8/64 loss: 0.19495880603790283
Batch 9/64 loss: 0.19106245040893555
Batch 10/64 loss: 0.18528276681900024
Batch 11/64 loss: 0.2004847526550293
Batch 12/64 loss: 0.19155097007751465
Batch 13/64 loss: 0.2033136487007141
Batch 14/64 loss: 0.19276976585388184
Batch 15/64 loss: 0.1970130205154419
Batch 16/64 loss: 0.18643254041671753
Batch 17/64 loss: 0.18982923030853271
Batch 18/64 loss: 0.18968403339385986
Batch 19/64 loss: 0.20183181762695312
Batch 20/64 loss: 0.18669664859771729
Batch 21/64 loss: 0.16287356615066528
Batch 22/64 loss: 0.1858961582183838
Batch 23/64 loss: 0.16397321224212646
Batch 24/64 loss: 0.17103374004364014
Batch 25/64 loss: 0.17645442485809326
Batch 26/64 loss: 0.19396823644638062
Batch 27/64 loss: 0.18578636646270752
Batch 28/64 loss: 0.1897573471069336
Batch 29/64 loss: 0.1980830430984497
Batch 30/64 loss: 0.17540818452835083
Batch 31/64 loss: 0.18687564134597778
Batch 32/64 loss: 0.18665313720703125
Batch 33/64 loss: 0.163036048412323
Batch 34/64 loss: 0.18909883499145508
Batch 35/64 loss: 0.17566239833831787
Batch 36/64 loss: 0.17883574962615967
Batch 37/64 loss: 0.17326855659484863
Batch 38/64 loss: 0.16765958070755005
Batch 39/64 loss: 0.18771743774414062
Batch 40/64 loss: 0.18629127740859985
Batch 41/64 loss: 0.17567312717437744
Batch 42/64 loss: 0.18378007411956787
Batch 43/64 loss: 0.18918442726135254
Batch 44/64 loss: 0.2042560577392578
Batch 45/64 loss: 0.1806546449661255
Batch 46/64 loss: 0.20659947395324707
Batch 47/64 loss: 0.17195385694503784
Batch 48/64 loss: 0.19389605522155762
Batch 49/64 loss: 0.18714332580566406
Batch 50/64 loss: 0.1817241907119751
Batch 51/64 loss: 0.1809319257736206
Batch 52/64 loss: 0.19306927919387817
Batch 53/64 loss: 0.1935408115386963
Batch 54/64 loss: 0.19182240962982178
Batch 55/64 loss: 0.18572843074798584
Batch 56/64 loss: 0.2010430097579956
Batch 57/64 loss: 0.17933166027069092
Batch 58/64 loss: 0.1795576810836792
Batch 59/64 loss: 0.22234368324279785
Batch 60/64 loss: 0.20442557334899902
Batch 61/64 loss: 0.1718055009841919
Batch 62/64 loss: 0.1981879472732544
Batch 63/64 loss: 0.19015443325042725
Batch 64/64 loss: 0.1926898956298828
Epoch 37  Train loss: 0.18699268172768985  Val loss: 0.20105927240397922
Saving best model, epoch: 37
Epoch 38
-------------------------------
Batch 1/64 loss: 0.20284759998321533
Batch 2/64 loss: 0.1739230751991272
Batch 3/64 loss: 0.19257372617721558
Batch 4/64 loss: 0.1868201494216919
Batch 5/64 loss: 0.18284374475479126
Batch 6/64 loss: 0.18627679347991943
Batch 7/64 loss: 0.1905078887939453
Batch 8/64 loss: 0.20429790019989014
Batch 9/64 loss: 0.18594837188720703
Batch 10/64 loss: 0.1798228621482849
Batch 11/64 loss: 0.19358158111572266
Batch 12/64 loss: 0.1852182149887085
Batch 13/64 loss: 0.18251734972000122
Batch 14/64 loss: 0.166287362575531
Batch 15/64 loss: 0.17669200897216797
Batch 16/64 loss: 0.16982078552246094
Batch 17/64 loss: 0.18518656492233276
Batch 18/64 loss: 0.212732195854187
Batch 19/64 loss: 0.17801523208618164
Batch 20/64 loss: 0.18617844581604004
Batch 21/64 loss: 0.18990790843963623
Batch 22/64 loss: 0.1901034116744995
Batch 23/64 loss: 0.18454867601394653
Batch 24/64 loss: 0.1568692922592163
Batch 25/64 loss: 0.19900262355804443
Batch 26/64 loss: 0.17309945821762085
Batch 27/64 loss: 0.1802259087562561
Batch 28/64 loss: 0.1818523406982422
Batch 29/64 loss: 0.18915629386901855
Batch 30/64 loss: 0.1890876293182373
Batch 31/64 loss: 0.20259618759155273
Batch 32/64 loss: 0.1810014247894287
Batch 33/64 loss: 0.18972760438919067
Batch 34/64 loss: 0.16360074281692505
Batch 35/64 loss: 0.16908758878707886
Batch 36/64 loss: 0.1830253005027771
Batch 37/64 loss: 0.2053983211517334
Batch 38/64 loss: 0.17536985874176025
Batch 39/64 loss: 0.15924781560897827
Batch 40/64 loss: 0.19337010383605957
Batch 41/64 loss: 0.18388879299163818
Batch 42/64 loss: 0.19788378477096558
Batch 43/64 loss: 0.18401002883911133
Batch 44/64 loss: 0.19324815273284912
Batch 45/64 loss: 0.17683547735214233
Batch 46/64 loss: 0.15265023708343506
Batch 47/64 loss: 0.1965928077697754
Batch 48/64 loss: 0.1806817650794983
Batch 49/64 loss: 0.17279285192489624
Batch 50/64 loss: 0.19608449935913086
Batch 51/64 loss: 0.179274320602417
Batch 52/64 loss: 0.15086489915847778
Batch 53/64 loss: 0.1740778684616089
Batch 54/64 loss: 0.1812293529510498
Batch 55/64 loss: 0.19807565212249756
Batch 56/64 loss: 0.18504196405410767
Batch 57/64 loss: 0.18019890785217285
Batch 58/64 loss: 0.19255542755126953
Batch 59/64 loss: 0.17295396327972412
Batch 60/64 loss: 0.1898515820503235
Batch 61/64 loss: 0.18653172254562378
Batch 62/64 loss: 0.16941678524017334
Batch 63/64 loss: 0.19537353515625
Batch 64/64 loss: 0.16726040840148926
Epoch 38  Train loss: 0.18343419654696597  Val loss: 0.20079797895503618
Saving best model, epoch: 38
Epoch 39
-------------------------------
Batch 1/64 loss: 0.19718056917190552
Batch 2/64 loss: 0.2014174461364746
Batch 3/64 loss: 0.18581604957580566
Batch 4/64 loss: 0.17041677236557007
Batch 5/64 loss: 0.18068265914916992
Batch 6/64 loss: 0.1865234375
Batch 7/64 loss: 0.1658799648284912
Batch 8/64 loss: 0.17991244792938232
Batch 9/64 loss: 0.1621493101119995
Batch 10/64 loss: 0.17714643478393555
Batch 11/64 loss: 0.16909974813461304
Batch 12/64 loss: 0.18671905994415283
Batch 13/64 loss: 0.17575079202651978
Batch 14/64 loss: 0.19965749979019165
Batch 15/64 loss: 0.19352936744689941
Batch 16/64 loss: 0.16040921211242676
Batch 17/64 loss: 0.18318158388137817
Batch 18/64 loss: 0.18543016910552979
Batch 19/64 loss: 0.2008296251296997
Batch 20/64 loss: 0.1791331171989441
Batch 21/64 loss: 0.18987417221069336
Batch 22/64 loss: 0.20796310901641846
Batch 23/64 loss: 0.16357046365737915
Batch 24/64 loss: 0.15741276741027832
Batch 25/64 loss: 0.17552876472473145
Batch 26/64 loss: 0.17545992136001587
Batch 27/64 loss: 0.16647768020629883
Batch 28/64 loss: 0.1709209680557251
Batch 29/64 loss: 0.1744070053100586
Batch 30/64 loss: 0.18810635805130005
Batch 31/64 loss: 0.19840627908706665
Batch 32/64 loss: 0.17133843898773193
Batch 33/64 loss: 0.17623567581176758
Batch 34/64 loss: 0.1771754026412964
Batch 35/64 loss: 0.19194990396499634
Batch 36/64 loss: 0.17872774600982666
Batch 37/64 loss: 0.18232107162475586
Batch 38/64 loss: 0.16576313972473145
Batch 39/64 loss: 0.18268823623657227
Batch 40/64 loss: 0.18655997514724731
Batch 41/64 loss: 0.196999192237854
Batch 42/64 loss: 0.18594133853912354
Batch 43/64 loss: 0.19220572710037231
Batch 44/64 loss: 0.17961037158966064
Batch 45/64 loss: 0.19069159030914307
Batch 46/64 loss: 0.19422292709350586
Batch 47/64 loss: 0.1763872504234314
Batch 48/64 loss: 0.1926485300064087
Batch 49/64 loss: 0.2010517120361328
Batch 50/64 loss: 0.20411920547485352
Batch 51/64 loss: 0.170127272605896
Batch 52/64 loss: 0.1816028356552124
Batch 53/64 loss: 0.17957478761672974
Batch 54/64 loss: 0.18355929851531982
Batch 55/64 loss: 0.18972504138946533
Batch 56/64 loss: 0.17462652921676636
Batch 57/64 loss: 0.1932746171951294
Batch 58/64 loss: 0.1791439652442932
Batch 59/64 loss: 0.19435298442840576
Batch 60/64 loss: 0.17275094985961914
Batch 61/64 loss: 0.17593753337860107
Batch 62/64 loss: 0.19037353992462158
Batch 63/64 loss: 0.18214333057403564
Batch 64/64 loss: 0.148970365524292
Epoch 39  Train loss: 0.18218902980580048  Val loss: 0.1952418435070523
Saving best model, epoch: 39
Epoch 40
-------------------------------
Batch 1/64 loss: 0.18927043676376343
Batch 2/64 loss: 0.16957682371139526
Batch 3/64 loss: 0.19338458776474
Batch 4/64 loss: 0.1741337776184082
Batch 5/64 loss: 0.19407415390014648
Batch 6/64 loss: 0.1619657278060913
Batch 7/64 loss: 0.18053275346755981
Batch 8/64 loss: 0.1920243501663208
Batch 9/64 loss: 0.18746960163116455
Batch 10/64 loss: 0.17680609226226807
Batch 11/64 loss: 0.16810619831085205
Batch 12/64 loss: 0.15991050004959106
Batch 13/64 loss: 0.18981319665908813
Batch 14/64 loss: 0.16023975610733032
Batch 15/64 loss: 0.18763023614883423
Batch 16/64 loss: 0.18690598011016846
Batch 17/64 loss: 0.17991501092910767
Batch 18/64 loss: 0.17560988664627075
Batch 19/64 loss: 0.1585981845855713
Batch 20/64 loss: 0.1655433177947998
Batch 21/64 loss: 0.19662493467330933
Batch 22/64 loss: 0.1762474775314331
Batch 23/64 loss: 0.19589084386825562
Batch 24/64 loss: 0.18041235208511353
Batch 25/64 loss: 0.16666370630264282
Batch 26/64 loss: 0.1897280216217041
Batch 27/64 loss: 0.17681407928466797
Batch 28/64 loss: 0.18065351247787476
Batch 29/64 loss: 0.1858959197998047
Batch 30/64 loss: 0.1809244155883789
Batch 31/64 loss: 0.17145681381225586
Batch 32/64 loss: 0.18174993991851807
Batch 33/64 loss: 0.2097530961036682
Batch 34/64 loss: 0.17222899198532104
Batch 35/64 loss: 0.19007456302642822
Batch 36/64 loss: 0.14510101079940796
Batch 37/64 loss: 0.19841808080673218
Batch 38/64 loss: 0.18152159452438354
Batch 39/64 loss: 0.18794310092926025
Batch 40/64 loss: 0.20441734790802002
Batch 41/64 loss: 0.15903598070144653
Batch 42/64 loss: 0.17630457878112793
Batch 43/64 loss: 0.18768024444580078
Batch 44/64 loss: 0.1713361144065857
Batch 45/64 loss: 0.16916978359222412
Batch 46/64 loss: 0.19502902030944824
Batch 47/64 loss: 0.2008916735649109
Batch 48/64 loss: 0.1856088638305664
Batch 49/64 loss: 0.1764124631881714
Batch 50/64 loss: 0.1793392300605774
Batch 51/64 loss: 0.1948612928390503
Batch 52/64 loss: 0.18531358242034912
Batch 53/64 loss: 0.18210840225219727
Batch 54/64 loss: 0.17473185062408447
Batch 55/64 loss: 0.18293404579162598
Batch 56/64 loss: 0.17650443315505981
Batch 57/64 loss: 0.18103957176208496
Batch 58/64 loss: 0.19205701351165771
Batch 59/64 loss: 0.1733441948890686
Batch 60/64 loss: 0.16837596893310547
Batch 61/64 loss: 0.16900646686553955
Batch 62/64 loss: 0.17714911699295044
Batch 63/64 loss: 0.16584837436676025
Batch 64/64 loss: 0.16678929328918457
Epoch 40  Train loss: 0.1799718370624617  Val loss: 0.19313065968837934
Saving best model, epoch: 40
Epoch 41
-------------------------------
Batch 1/64 loss: 0.17487549781799316
Batch 2/64 loss: 0.1856558918952942
Batch 3/64 loss: 0.18430262804031372
Batch 4/64 loss: 0.19734448194503784
Batch 5/64 loss: 0.19117242097854614
Batch 6/64 loss: 0.1715756058692932
Batch 7/64 loss: 0.1737586259841919
Batch 8/64 loss: 0.18609076738357544
Batch 9/64 loss: 0.18724948167800903
Batch 10/64 loss: 0.19310128688812256
Batch 11/64 loss: 0.17804163694381714
Batch 12/64 loss: 0.1734430193901062
Batch 13/64 loss: 0.19587409496307373
Batch 14/64 loss: 0.16649824380874634
Batch 15/64 loss: 0.16309356689453125
Batch 16/64 loss: 0.17971795797348022
Batch 17/64 loss: 0.19690155982971191
Batch 18/64 loss: 0.1829909086227417
Batch 19/64 loss: 0.1926288604736328
Batch 20/64 loss: 0.1800593137741089
Batch 21/64 loss: 0.1947600245475769
Batch 22/64 loss: 0.17525148391723633
Batch 23/64 loss: 0.1802743673324585
Batch 24/64 loss: 0.18052703142166138
Batch 25/64 loss: 0.18515610694885254
Batch 26/64 loss: 0.1670810580253601
Batch 27/64 loss: 0.1602882742881775
Batch 28/64 loss: 0.16703861951828003
Batch 29/64 loss: 0.15815114974975586
Batch 30/64 loss: 0.18415051698684692
Batch 31/64 loss: 0.15178924798965454
Batch 32/64 loss: 0.1883898377418518
Batch 33/64 loss: 0.1860191822052002
Batch 34/64 loss: 0.16409409046173096
Batch 35/64 loss: 0.18226516246795654
Batch 36/64 loss: 0.18357634544372559
Batch 37/64 loss: 0.17771220207214355
Batch 38/64 loss: 0.18970024585723877
Batch 39/64 loss: 0.1865445375442505
Batch 40/64 loss: 0.18707937002182007
Batch 41/64 loss: 0.17011815309524536
Batch 42/64 loss: 0.15608423948287964
Batch 43/64 loss: 0.1716465950012207
Batch 44/64 loss: 0.15425527095794678
Batch 45/64 loss: 0.16991674900054932
Batch 46/64 loss: 0.16014182567596436
Batch 47/64 loss: 0.16484272480010986
Batch 48/64 loss: 0.16244393587112427
Batch 49/64 loss: 0.1750788688659668
Batch 50/64 loss: 0.18515336513519287
Batch 51/64 loss: 0.16994470357894897
Batch 52/64 loss: 0.18061566352844238
Batch 53/64 loss: 0.15795308351516724
Batch 54/64 loss: 0.18250656127929688
Batch 55/64 loss: 0.18479502201080322
Batch 56/64 loss: 0.1666867733001709
Batch 57/64 loss: 0.16092145442962646
Batch 58/64 loss: 0.18421608209609985
Batch 59/64 loss: 0.18615806102752686
Batch 60/64 loss: 0.17175698280334473
Batch 61/64 loss: 0.19447994232177734
Batch 62/64 loss: 0.18037641048431396
Batch 63/64 loss: 0.16527915000915527
Batch 64/64 loss: 0.1678364872932434
Epoch 41  Train loss: 0.17702703826567706  Val loss: 0.19298418690658517
Saving best model, epoch: 41
Epoch 42
-------------------------------
Batch 1/64 loss: 0.16500693559646606
Batch 2/64 loss: 0.1699560284614563
Batch 3/64 loss: 0.17446261644363403
Batch 4/64 loss: 0.19545751810073853
Batch 5/64 loss: 0.1835535764694214
Batch 6/64 loss: 0.17888379096984863
Batch 7/64 loss: 0.1594882607460022
Batch 8/64 loss: 0.17141413688659668
Batch 9/64 loss: 0.160272479057312
Batch 10/64 loss: 0.1644078493118286
Batch 11/64 loss: 0.16199612617492676
Batch 12/64 loss: 0.19156277179718018
Batch 13/64 loss: 0.20089340209960938
Batch 14/64 loss: 0.15160733461380005
Batch 15/64 loss: 0.18542826175689697
Batch 16/64 loss: 0.16728883981704712
Batch 17/64 loss: 0.17802923917770386
Batch 18/64 loss: 0.18530237674713135
Batch 19/64 loss: 0.14661943912506104
Batch 20/64 loss: 0.16460859775543213
Batch 21/64 loss: 0.18081998825073242
Batch 22/64 loss: 0.18295782804489136
Batch 23/64 loss: 0.1632528305053711
Batch 24/64 loss: 0.1825503706932068
Batch 25/64 loss: 0.1977536678314209
Batch 26/64 loss: 0.17349642515182495
Batch 27/64 loss: 0.17401504516601562
Batch 28/64 loss: 0.17249178886413574
Batch 29/64 loss: 0.17493855953216553
Batch 30/64 loss: 0.2047255039215088
Batch 31/64 loss: 0.18386566638946533
Batch 32/64 loss: 0.17837315797805786
Batch 33/64 loss: 0.1606767773628235
Batch 34/64 loss: 0.17758119106292725
Batch 35/64 loss: 0.18600541353225708
Batch 36/64 loss: 0.15018081665039062
Batch 37/64 loss: 0.1644272804260254
Batch 38/64 loss: 0.1991490125656128
Batch 39/64 loss: 0.1834370493888855
Batch 40/64 loss: 0.17463672161102295
Batch 41/64 loss: 0.16746890544891357
Batch 42/64 loss: 0.17714571952819824
Batch 43/64 loss: 0.16869157552719116
Batch 44/64 loss: 0.16626965999603271
Batch 45/64 loss: 0.17544758319854736
Batch 46/64 loss: 0.1868596076965332
Batch 47/64 loss: 0.18931972980499268
Batch 48/64 loss: 0.18155980110168457
Batch 49/64 loss: 0.17129671573638916
Batch 50/64 loss: 0.18111497163772583
Batch 51/64 loss: 0.1828773021697998
Batch 52/64 loss: 0.18042528629302979
Batch 53/64 loss: 0.16538006067276
Batch 54/64 loss: 0.19526565074920654
Batch 55/64 loss: 0.18032163381576538
Batch 56/64 loss: 0.17708420753479004
Batch 57/64 loss: 0.2009744644165039
Batch 58/64 loss: 0.1926203966140747
Batch 59/64 loss: 0.2068706750869751
Batch 60/64 loss: 0.16568076610565186
Batch 61/64 loss: 0.15967297554016113
Batch 62/64 loss: 0.17980414628982544
Batch 63/64 loss: 0.16047585010528564
Batch 64/64 loss: 0.17131167650222778
Epoch 42  Train loss: 0.17666958617229087  Val loss: 0.19404997161983215
Epoch 43
-------------------------------
Batch 1/64 loss: 0.18788444995880127
Batch 2/64 loss: 0.1644134521484375
Batch 3/64 loss: 0.17190545797348022
Batch 4/64 loss: 0.15215075016021729
Batch 5/64 loss: 0.17244327068328857
Batch 6/64 loss: 0.18981820344924927
Batch 7/64 loss: 0.16857975721359253
Batch 8/64 loss: 0.18482083082199097
Batch 9/64 loss: 0.17695367336273193
Batch 10/64 loss: 0.162322998046875
Batch 11/64 loss: 0.186021625995636
Batch 12/64 loss: 0.20267409086227417
Batch 13/64 loss: 0.1687566041946411
Batch 14/64 loss: 0.18176543712615967
Batch 15/64 loss: 0.16576623916625977
Batch 16/64 loss: 0.1579902172088623
Batch 17/64 loss: 0.16782081127166748
Batch 18/64 loss: 0.17675459384918213
Batch 19/64 loss: 0.15838879346847534
Batch 20/64 loss: 0.16780436038970947
Batch 21/64 loss: 0.17685389518737793
Batch 22/64 loss: 0.15908128023147583
Batch 23/64 loss: 0.1789373755455017
Batch 24/64 loss: 0.1989029049873352
Batch 25/64 loss: 0.1826431155204773
Batch 26/64 loss: 0.1671525239944458
Batch 27/64 loss: 0.1780797839164734
Batch 28/64 loss: 0.15709823369979858
Batch 29/64 loss: 0.1864238977432251
Batch 30/64 loss: 0.16880488395690918
Batch 31/64 loss: 0.16855347156524658
Batch 32/64 loss: 0.1647438406944275
Batch 33/64 loss: 0.1729423999786377
Batch 34/64 loss: 0.17938512563705444
Batch 35/64 loss: 0.20433181524276733
Batch 36/64 loss: 0.19247806072235107
Batch 37/64 loss: 0.1778944730758667
Batch 38/64 loss: 0.1662813425064087
Batch 39/64 loss: 0.16964340209960938
Batch 40/64 loss: 0.16145473718643188
Batch 41/64 loss: 0.16202634572982788
Batch 42/64 loss: 0.18664026260375977
Batch 43/64 loss: 0.17887544631958008
Batch 44/64 loss: 0.1556929349899292
Batch 45/64 loss: 0.18864405155181885
Batch 46/64 loss: 0.17633962631225586
Batch 47/64 loss: 0.15083879232406616
Batch 48/64 loss: 0.17981863021850586
Batch 49/64 loss: 0.18675029277801514
Batch 50/64 loss: 0.19367074966430664
Batch 51/64 loss: 0.1975100040435791
Batch 52/64 loss: 0.16290628910064697
Batch 53/64 loss: 0.1628812551498413
Batch 54/64 loss: 0.17220854759216309
Batch 55/64 loss: 0.16307348012924194
Batch 56/64 loss: 0.18686604499816895
Batch 57/64 loss: 0.16593492031097412
Batch 58/64 loss: 0.17652267217636108
Batch 59/64 loss: 0.188002347946167
Batch 60/64 loss: 0.17388415336608887
Batch 61/64 loss: 0.1791013479232788
Batch 62/64 loss: 0.1513965129852295
Batch 63/64 loss: 0.18794894218444824
Batch 64/64 loss: 0.16681355237960815
Epoch 43  Train loss: 0.17457829012590295  Val loss: 0.1902231338507531
Saving best model, epoch: 43
Epoch 44
-------------------------------
Batch 1/64 loss: 0.14932358264923096
Batch 2/64 loss: 0.1743212342262268
Batch 3/64 loss: 0.19800984859466553
Batch 4/64 loss: 0.1892096996307373
Batch 5/64 loss: 0.16140711307525635
Batch 6/64 loss: 0.15596163272857666
Batch 7/64 loss: 0.15482556819915771
Batch 8/64 loss: 0.1664542555809021
Batch 9/64 loss: 0.17420244216918945
Batch 10/64 loss: 0.19610238075256348
Batch 11/64 loss: 0.16437596082687378
Batch 12/64 loss: 0.16227072477340698
Batch 13/64 loss: 0.1674203872680664
Batch 14/64 loss: 0.15904128551483154
Batch 15/64 loss: 0.16783905029296875
Batch 16/64 loss: 0.16462421417236328
Batch 17/64 loss: 0.16114425659179688
Batch 18/64 loss: 0.17705518007278442
Batch 19/64 loss: 0.17597490549087524
Batch 20/64 loss: 0.15892648696899414
Batch 21/64 loss: 0.18451130390167236
Batch 22/64 loss: 0.16057783365249634
Batch 23/64 loss: 0.18915367126464844
Batch 24/64 loss: 0.19345760345458984
Batch 25/64 loss: 0.1631181240081787
Batch 26/64 loss: 0.17637181282043457
Batch 27/64 loss: 0.19316726922988892
Batch 28/64 loss: 0.159751296043396
Batch 29/64 loss: 0.19331330060958862
Batch 30/64 loss: 0.1946645975112915
Batch 31/64 loss: 0.15872514247894287
Batch 32/64 loss: 0.18077123165130615
Batch 33/64 loss: 0.18141847848892212
Batch 34/64 loss: 0.16714084148406982
Batch 35/64 loss: 0.1713343858718872
Batch 36/64 loss: 0.16968274116516113
Batch 37/64 loss: 0.17221695184707642
Batch 38/64 loss: 0.17827492952346802
Batch 39/64 loss: 0.17317253351211548
Batch 40/64 loss: 0.16486310958862305
Batch 41/64 loss: 0.17338401079177856
Batch 42/64 loss: 0.1493489146232605
Batch 43/64 loss: 0.18130099773406982
Batch 44/64 loss: 0.16617029905319214
Batch 45/64 loss: 0.17083144187927246
Batch 46/64 loss: 0.16799265146255493
Batch 47/64 loss: 0.17806845903396606
Batch 48/64 loss: 0.17980217933654785
Batch 49/64 loss: 0.17060720920562744
Batch 50/64 loss: 0.19360005855560303
Batch 51/64 loss: 0.1623244285583496
Batch 52/64 loss: 0.17148780822753906
Batch 53/64 loss: 0.16155266761779785
Batch 54/64 loss: 0.17834389209747314
Batch 55/64 loss: 0.16795563697814941
Batch 56/64 loss: 0.17687535285949707
Batch 57/64 loss: 0.16966134309768677
Batch 58/64 loss: 0.18062466382980347
Batch 59/64 loss: 0.1817762851715088
Batch 60/64 loss: 0.16020238399505615
Batch 61/64 loss: 0.1655280590057373
Batch 62/64 loss: 0.16687113046646118
Batch 63/64 loss: 0.17192816734313965
Batch 64/64 loss: 0.17058563232421875
Epoch 44  Train loss: 0.17220948443693274  Val loss: 0.18783500165873787
Saving best model, epoch: 44
Epoch 45
-------------------------------
Batch 1/64 loss: 0.1823669672012329
Batch 2/64 loss: 0.18039202690124512
Batch 3/64 loss: 0.1623726487159729
Batch 4/64 loss: 0.158635675907135
Batch 5/64 loss: 0.17405694723129272
Batch 6/64 loss: 0.18608605861663818
Batch 7/64 loss: 0.177199125289917
Batch 8/64 loss: 0.18887066841125488
Batch 9/64 loss: 0.1568828821182251
Batch 10/64 loss: 0.1549401879310608
Batch 11/64 loss: 0.15617072582244873
Batch 12/64 loss: 0.16253089904785156
Batch 13/64 loss: 0.16734212636947632
Batch 14/64 loss: 0.19040024280548096
Batch 15/64 loss: 0.16899126768112183
Batch 16/64 loss: 0.16246294975280762
Batch 17/64 loss: 0.18031293153762817
Batch 18/64 loss: 0.1673557162284851
Batch 19/64 loss: 0.17440146207809448
Batch 20/64 loss: 0.18454831838607788
Batch 21/64 loss: 0.16693472862243652
Batch 22/64 loss: 0.1888667345046997
Batch 23/64 loss: 0.15749847888946533
Batch 24/64 loss: 0.15402692556381226
Batch 25/64 loss: 0.1612035036087036
Batch 26/64 loss: 0.15794038772583008
Batch 27/64 loss: 0.1731642484664917
Batch 28/64 loss: 0.1757318377494812
Batch 29/64 loss: 0.1581796407699585
Batch 30/64 loss: 0.16197723150253296
Batch 31/64 loss: 0.15884053707122803
Batch 32/64 loss: 0.1874178647994995
Batch 33/64 loss: 0.16897845268249512
Batch 34/64 loss: 0.15353912115097046
Batch 35/64 loss: 0.17858964204788208
Batch 36/64 loss: 0.17429375648498535
Batch 37/64 loss: 0.1804911494255066
Batch 38/64 loss: 0.174976646900177
Batch 39/64 loss: 0.16486334800720215
Batch 40/64 loss: 0.1736241579055786
Batch 41/64 loss: 0.15472537279129028
Batch 42/64 loss: 0.17152082920074463
Batch 43/64 loss: 0.17987775802612305
Batch 44/64 loss: 0.16112017631530762
Batch 45/64 loss: 0.1652175784111023
Batch 46/64 loss: 0.1631566286087036
Batch 47/64 loss: 0.1638358235359192
Batch 48/64 loss: 0.1890338659286499
Batch 49/64 loss: 0.17609244585037231
Batch 50/64 loss: 0.15518182516098022
Batch 51/64 loss: 0.1725512146949768
Batch 52/64 loss: 0.20149552822113037
Batch 53/64 loss: 0.1644313931465149
Batch 54/64 loss: 0.1501292586326599
Batch 55/64 loss: 0.1713399887084961
Batch 56/64 loss: 0.147530198097229
Batch 57/64 loss: 0.21445727348327637
Batch 58/64 loss: 0.16821914911270142
Batch 59/64 loss: 0.19034677743911743
Batch 60/64 loss: 0.15759843587875366
Batch 61/64 loss: 0.15079450607299805
Batch 62/64 loss: 0.15729838609695435
Batch 63/64 loss: 0.16636115312576294
Batch 64/64 loss: 0.19204187393188477
Epoch 45  Train loss: 0.1700989050023696  Val loss: 0.187044640177304
Saving best model, epoch: 45
Epoch 46
-------------------------------
Batch 1/64 loss: 0.20766681432724
Batch 2/64 loss: 0.18239903450012207
Batch 3/64 loss: 0.1657537817955017
Batch 4/64 loss: 0.16332566738128662
Batch 5/64 loss: 0.1548042893409729
Batch 6/64 loss: 0.17003017663955688
Batch 7/64 loss: 0.17048609256744385
Batch 8/64 loss: 0.17632567882537842
Batch 9/64 loss: 0.1457422971725464
Batch 10/64 loss: 0.17615914344787598
Batch 11/64 loss: 0.17084860801696777
Batch 12/64 loss: 0.18195241689682007
Batch 13/64 loss: 0.1677330732345581
Batch 14/64 loss: 0.12866711616516113
Batch 15/64 loss: 0.16097640991210938
Batch 16/64 loss: 0.17887794971466064
Batch 17/64 loss: 0.17243236303329468
Batch 18/64 loss: 0.1488499641418457
Batch 19/64 loss: 0.16016864776611328
Batch 20/64 loss: 0.16215789318084717
Batch 21/64 loss: 0.16830503940582275
Batch 22/64 loss: 0.15297335386276245
Batch 23/64 loss: 0.17763054370880127
Batch 24/64 loss: 0.1406172513961792
Batch 25/64 loss: 0.1589294672012329
Batch 26/64 loss: 0.1766800880432129
Batch 27/64 loss: 0.1875985860824585
Batch 28/64 loss: 0.1745377779006958
Batch 29/64 loss: 0.1746256947517395
Batch 30/64 loss: 0.1811211109161377
Batch 31/64 loss: 0.16295278072357178
Batch 32/64 loss: 0.17112618684768677
Batch 33/64 loss: 0.17908906936645508
Batch 34/64 loss: 0.1812916398048401
Batch 35/64 loss: 0.14400041103363037
Batch 36/64 loss: 0.1814805269241333
Batch 37/64 loss: 0.17836928367614746
Batch 38/64 loss: 0.16722732782363892
Batch 39/64 loss: 0.17480093240737915
Batch 40/64 loss: 0.19597244262695312
Batch 41/64 loss: 0.17741644382476807
Batch 42/64 loss: 0.18397235870361328
Batch 43/64 loss: 0.15778958797454834
Batch 44/64 loss: 0.18176376819610596
Batch 45/64 loss: 0.15306586027145386
Batch 46/64 loss: 0.15796297788619995
Batch 47/64 loss: 0.18639540672302246
Batch 48/64 loss: 0.1587890386581421
Batch 49/64 loss: 0.1778644323348999
Batch 50/64 loss: 0.15218675136566162
Batch 51/64 loss: 0.16094160079956055
Batch 52/64 loss: 0.17737704515457153
Batch 53/64 loss: 0.14941424131393433
Batch 54/64 loss: 0.17786908149719238
Batch 55/64 loss: 0.13733339309692383
Batch 56/64 loss: 0.17223680019378662
Batch 57/64 loss: 0.1599375605583191
Batch 58/64 loss: 0.15566086769104004
Batch 59/64 loss: 0.18212348222732544
Batch 60/64 loss: 0.16171014308929443
Batch 61/64 loss: 0.1473078727722168
Batch 62/64 loss: 0.17887598276138306
Batch 63/64 loss: 0.17680758237838745
Batch 64/64 loss: 0.16788673400878906
Epoch 46  Train loss: 0.16824163549086626  Val loss: 0.18613840132644496
Saving best model, epoch: 46
Epoch 47
-------------------------------
Batch 1/64 loss: 0.16035562753677368
Batch 2/64 loss: 0.14230841398239136
Batch 3/64 loss: 0.1699739694595337
Batch 4/64 loss: 0.1725010871887207
Batch 5/64 loss: 0.1640230417251587
Batch 6/64 loss: 0.16080820560455322
Batch 7/64 loss: 0.19744694232940674
Batch 8/64 loss: 0.17274576425552368
Batch 9/64 loss: 0.1714215874671936
Batch 10/64 loss: 0.16518449783325195
Batch 11/64 loss: 0.16124117374420166
Batch 12/64 loss: 0.1723005175590515
Batch 13/64 loss: 0.15308421850204468
Batch 14/64 loss: 0.15384691953659058
Batch 15/64 loss: 0.17935502529144287
Batch 16/64 loss: 0.16914421319961548
Batch 17/64 loss: 0.19268584251403809
Batch 18/64 loss: 0.17030930519104004
Batch 19/64 loss: 0.15950798988342285
Batch 20/64 loss: 0.1493096947669983
Batch 21/64 loss: 0.1389479637145996
Batch 22/64 loss: 0.1521376371383667
Batch 23/64 loss: 0.17416423559188843
Batch 24/64 loss: 0.1534668207168579
Batch 25/64 loss: 0.15034157037734985
Batch 26/64 loss: 0.15820610523223877
Batch 27/64 loss: 0.1566551923751831
Batch 28/64 loss: 0.15350472927093506
Batch 29/64 loss: 0.16660654544830322
Batch 30/64 loss: 0.19979643821716309
Batch 31/64 loss: 0.17010843753814697
Batch 32/64 loss: 0.18648779392242432
Batch 33/64 loss: 0.16836100816726685
Batch 34/64 loss: 0.16193127632141113
Batch 35/64 loss: 0.1615317463874817
Batch 36/64 loss: 0.16149133443832397
Batch 37/64 loss: 0.16961443424224854
Batch 38/64 loss: 0.1798996925354004
Batch 39/64 loss: 0.16551107168197632
Batch 40/64 loss: 0.15622985363006592
Batch 41/64 loss: 0.1646307110786438
Batch 42/64 loss: 0.16889673471450806
Batch 43/64 loss: 0.1543722152709961
Batch 44/64 loss: 0.17148244380950928
Batch 45/64 loss: 0.1757584810256958
Batch 46/64 loss: 0.16094303131103516
Batch 47/64 loss: 0.18114954233169556
Batch 48/64 loss: 0.16649776697158813
Batch 49/64 loss: 0.16696393489837646
Batch 50/64 loss: 0.18592095375061035
Batch 51/64 loss: 0.15952610969543457
Batch 52/64 loss: 0.19022005796432495
Batch 53/64 loss: 0.1776130199432373
Batch 54/64 loss: 0.17483198642730713
Batch 55/64 loss: 0.16030198335647583
Batch 56/64 loss: 0.16848623752593994
Batch 57/64 loss: 0.18621718883514404
Batch 58/64 loss: 0.18337678909301758
Batch 59/64 loss: 0.17120331525802612
Batch 60/64 loss: 0.158352792263031
Batch 61/64 loss: 0.17029714584350586
Batch 62/64 loss: 0.1770501732826233
Batch 63/64 loss: 0.16954553127288818
Batch 64/64 loss: 0.14925998449325562
Epoch 47  Train loss: 0.16750009504019045  Val loss: 0.18430093698894856
Saving best model, epoch: 47
Epoch 48
-------------------------------
Batch 1/64 loss: 0.1701735258102417
Batch 2/64 loss: 0.14356642961502075
Batch 3/64 loss: 0.1610410213470459
Batch 4/64 loss: 0.17818289995193481
Batch 5/64 loss: 0.16782927513122559
Batch 6/64 loss: 0.1598411202430725
Batch 7/64 loss: 0.16288608312606812
Batch 8/64 loss: 0.18320196866989136
Batch 9/64 loss: 0.1730104684829712
Batch 10/64 loss: 0.15544062852859497
Batch 11/64 loss: 0.1431838870048523
Batch 12/64 loss: 0.1561295986175537
Batch 13/64 loss: 0.15340644121170044
Batch 14/64 loss: 0.16513091325759888
Batch 15/64 loss: 0.16813844442367554
Batch 16/64 loss: 0.16841626167297363
Batch 17/64 loss: 0.16453295946121216
Batch 18/64 loss: 0.15499132871627808
Batch 19/64 loss: 0.16368991136550903
Batch 20/64 loss: 0.16085898876190186
Batch 21/64 loss: 0.14172613620758057
Batch 22/64 loss: 0.15830188989639282
Batch 23/64 loss: 0.16786032915115356
Batch 24/64 loss: 0.16046583652496338
Batch 25/64 loss: 0.17421597242355347
Batch 26/64 loss: 0.1801384687423706
Batch 27/64 loss: 0.17743217945098877
Batch 28/64 loss: 0.17364954948425293
Batch 29/64 loss: 0.16911369562149048
Batch 30/64 loss: 0.18527638912200928
Batch 31/64 loss: 0.1533299684524536
Batch 32/64 loss: 0.1938924789428711
Batch 33/64 loss: 0.1679297685623169
Batch 34/64 loss: 0.19036531448364258
Batch 35/64 loss: 0.18349003791809082
Batch 36/64 loss: 0.1803654432296753
Batch 37/64 loss: 0.14223700761795044
Batch 38/64 loss: 0.16157376766204834
Batch 39/64 loss: 0.17468804121017456
Batch 40/64 loss: 0.1687057614326477
Batch 41/64 loss: 0.19357365369796753
Batch 42/64 loss: 0.1526273488998413
Batch 43/64 loss: 0.14743751287460327
Batch 44/64 loss: 0.15544593334197998
Batch 45/64 loss: 0.16131895780563354
Batch 46/64 loss: 0.1752532720565796
Batch 47/64 loss: 0.19190102815628052
Batch 48/64 loss: 0.16823315620422363
Batch 49/64 loss: 0.16996771097183228
Batch 50/64 loss: 0.17688733339309692
Batch 51/64 loss: 0.15844237804412842
Batch 52/64 loss: 0.16430115699768066
Batch 53/64 loss: 0.16958636045455933
Batch 54/64 loss: 0.16463392972946167
Batch 55/64 loss: 0.14532673358917236
Batch 56/64 loss: 0.19937503337860107
Batch 57/64 loss: 0.17121505737304688
Batch 58/64 loss: 0.1508539915084839
Batch 59/64 loss: 0.15116453170776367
Batch 60/64 loss: 0.17737078666687012
Batch 61/64 loss: 0.16398179531097412
Batch 62/64 loss: 0.13982534408569336
Batch 63/64 loss: 0.1451033353805542
Batch 64/64 loss: 0.15291285514831543
Epoch 48  Train loss: 0.16575562626707788  Val loss: 0.18546596794193962
Epoch 49
-------------------------------
Batch 1/64 loss: 0.16969412565231323
Batch 2/64 loss: 0.14612245559692383
Batch 3/64 loss: 0.16969692707061768
Batch 4/64 loss: 0.18396419286727905
Batch 5/64 loss: 0.15184879302978516
Batch 6/64 loss: 0.15382927656173706
Batch 7/64 loss: 0.16215866804122925
Batch 8/64 loss: 0.1638258695602417
Batch 9/64 loss: 0.15942376852035522
Batch 10/64 loss: 0.15669429302215576
Batch 11/64 loss: 0.15278887748718262
Batch 12/64 loss: 0.1744990348815918
Batch 13/64 loss: 0.14921969175338745
Batch 14/64 loss: 0.16816705465316772
Batch 15/64 loss: 0.16933727264404297
Batch 16/64 loss: 0.18709343671798706
Batch 17/64 loss: 0.17359739542007446
Batch 18/64 loss: 0.1596764326095581
Batch 19/64 loss: 0.1559467315673828
Batch 20/64 loss: 0.19870853424072266
Batch 21/64 loss: 0.17985814809799194
Batch 22/64 loss: 0.1485079526901245
Batch 23/64 loss: 0.174552321434021
Batch 24/64 loss: 0.1924755573272705
Batch 25/64 loss: 0.1465945839881897
Batch 26/64 loss: 0.18257111310958862
Batch 27/64 loss: 0.14583969116210938
Batch 28/64 loss: 0.1594153642654419
Batch 29/64 loss: 0.1472088098526001
Batch 30/64 loss: 0.16642171144485474
Batch 31/64 loss: 0.15949493646621704
Batch 32/64 loss: 0.16719567775726318
Batch 33/64 loss: 0.15884333848953247
Batch 34/64 loss: 0.1724744439125061
Batch 35/64 loss: 0.16548240184783936
Batch 36/64 loss: 0.14454883337020874
Batch 37/64 loss: 0.15883678197860718
Batch 38/64 loss: 0.15437155961990356
Batch 39/64 loss: 0.15494394302368164
Batch 40/64 loss: 0.16821223497390747
Batch 41/64 loss: 0.17793112993240356
Batch 42/64 loss: 0.15437233448028564
Batch 43/64 loss: 0.15713214874267578
Batch 44/64 loss: 0.17711973190307617
Batch 45/64 loss: 0.16398954391479492
Batch 46/64 loss: 0.16880303621292114
Batch 47/64 loss: 0.17987662553787231
Batch 48/64 loss: 0.1662154197692871
Batch 49/64 loss: 0.17369180917739868
Batch 50/64 loss: 0.18162572383880615
Batch 51/64 loss: 0.17261528968811035
Batch 52/64 loss: 0.1464402675628662
Batch 53/64 loss: 0.17115992307662964
Batch 54/64 loss: 0.16152793169021606
Batch 55/64 loss: 0.17047953605651855
Batch 56/64 loss: 0.15299582481384277
Batch 57/64 loss: 0.1666385531425476
Batch 58/64 loss: 0.17575019598007202
Batch 59/64 loss: 0.15315896272659302
Batch 60/64 loss: 0.14859825372695923
Batch 61/64 loss: 0.17447036504745483
Batch 62/64 loss: 0.1705426573753357
Batch 63/64 loss: 0.15528160333633423
Batch 64/64 loss: 0.14823651313781738
Epoch 49  Train loss: 0.16448214100856406  Val loss: 0.17888080008660806
Saving best model, epoch: 49
Epoch 50
-------------------------------
Batch 1/64 loss: 0.17993927001953125
Batch 2/64 loss: 0.15756916999816895
Batch 3/64 loss: 0.17473512887954712
Batch 4/64 loss: 0.15664047002792358
Batch 5/64 loss: 0.1556239128112793
Batch 6/64 loss: 0.1793544888496399
Batch 7/64 loss: 0.16815149784088135
Batch 8/64 loss: 0.1448468565940857
Batch 9/64 loss: 0.17250442504882812
Batch 10/64 loss: 0.17649704217910767
Batch 11/64 loss: 0.13518452644348145
Batch 12/64 loss: 0.15923166275024414
Batch 13/64 loss: 0.16352957487106323
Batch 14/64 loss: 0.1621485948562622
Batch 15/64 loss: 0.17352324724197388
Batch 16/64 loss: 0.1736304759979248
Batch 17/64 loss: 0.16196095943450928
Batch 18/64 loss: 0.15563911199569702
Batch 19/64 loss: 0.15538620948791504
Batch 20/64 loss: 0.17422109842300415
Batch 21/64 loss: 0.17569828033447266
Batch 22/64 loss: 0.17062914371490479
Batch 23/64 loss: 0.14193803071975708
Batch 24/64 loss: 0.14864718914031982
Batch 25/64 loss: 0.1439945101737976
Batch 26/64 loss: 0.1534428596496582
Batch 27/64 loss: 0.12121665477752686
Batch 28/64 loss: 0.16954362392425537
Batch 29/64 loss: 0.16848313808441162
Batch 30/64 loss: 0.1740778684616089
Batch 31/64 loss: 0.16830718517303467
Batch 32/64 loss: 0.16344648599624634
Batch 33/64 loss: 0.1564120650291443
Batch 34/64 loss: 0.1605844497680664
Batch 35/64 loss: 0.146101176738739
Batch 36/64 loss: 0.14983725547790527
Batch 37/64 loss: 0.1536712646484375
Batch 38/64 loss: 0.16093164682388306
Batch 39/64 loss: 0.1497083306312561
Batch 40/64 loss: 0.16194772720336914
Batch 41/64 loss: 0.15157485008239746
Batch 42/64 loss: 0.14134562015533447
Batch 43/64 loss: 0.13471579551696777
Batch 44/64 loss: 0.18454629182815552
Batch 45/64 loss: 0.16150718927383423
Batch 46/64 loss: 0.15429246425628662
Batch 47/64 loss: 0.16200673580169678
Batch 48/64 loss: 0.16707253456115723
Batch 49/64 loss: 0.17242228984832764
Batch 50/64 loss: 0.1676310896873474
Batch 51/64 loss: 0.16169238090515137
Batch 52/64 loss: 0.18063664436340332
Batch 53/64 loss: 0.16688603162765503
Batch 54/64 loss: 0.15972810983657837
Batch 55/64 loss: 0.14932286739349365
Batch 56/64 loss: 0.16741043329238892
Batch 57/64 loss: 0.16414415836334229
Batch 58/64 loss: 0.16245490312576294
Batch 59/64 loss: 0.1511141061782837
Batch 60/64 loss: 0.18465399742126465
Batch 61/64 loss: 0.17002350091934204
Batch 62/64 loss: 0.15984225273132324
Batch 63/64 loss: 0.15050959587097168
Batch 64/64 loss: 0.15724104642868042
Epoch 50  Train loss: 0.16097884295033474  Val loss: 0.1825374381239062
Epoch 51
-------------------------------
Batch 1/64 loss: 0.1674795150756836
Batch 2/64 loss: 0.1472720503807068
Batch 3/64 loss: 0.14262783527374268
Batch 4/64 loss: 0.14622563123703003
Batch 5/64 loss: 0.16470026969909668
Batch 6/64 loss: 0.16978293657302856
Batch 7/64 loss: 0.16190189123153687
Batch 8/64 loss: 0.14326107501983643
Batch 9/64 loss: 0.17953252792358398
Batch 10/64 loss: 0.18308734893798828
Batch 11/64 loss: 0.1569163203239441
Batch 12/64 loss: 0.1561746597290039
Batch 13/64 loss: 0.1496490240097046
Batch 14/64 loss: 0.15648674964904785
Batch 15/64 loss: 0.17545515298843384
Batch 16/64 loss: 0.15662139654159546
Batch 17/64 loss: 0.1502211093902588
Batch 18/64 loss: 0.15954911708831787
Batch 19/64 loss: 0.15310192108154297
Batch 20/64 loss: 0.1808854341506958
Batch 21/64 loss: 0.16262280941009521
Batch 22/64 loss: 0.1570919156074524
Batch 23/64 loss: 0.15431207418441772
Batch 24/64 loss: 0.15573132038116455
Batch 25/64 loss: 0.15120530128479004
Batch 26/64 loss: 0.1493116021156311
Batch 27/64 loss: 0.17426437139511108
Batch 28/64 loss: 0.19311130046844482
Batch 29/64 loss: 0.16659337282180786
Batch 30/64 loss: 0.15603643655776978
Batch 31/64 loss: 0.15859746932983398
Batch 32/64 loss: 0.15326613187789917
Batch 33/64 loss: 0.1607128381729126
Batch 34/64 loss: 0.14477074146270752
Batch 35/64 loss: 0.17054009437561035
Batch 36/64 loss: 0.1558070182800293
Batch 37/64 loss: 0.18837010860443115
Batch 38/64 loss: 0.14606279134750366
Batch 39/64 loss: 0.16090208292007446
Batch 40/64 loss: 0.16467875242233276
Batch 41/64 loss: 0.16692900657653809
Batch 42/64 loss: 0.16020739078521729
Batch 43/64 loss: 0.16520047187805176
Batch 44/64 loss: 0.1473073959350586
Batch 45/64 loss: 0.18661803007125854
Batch 46/64 loss: 0.1309334635734558
Batch 47/64 loss: 0.16809332370758057
Batch 48/64 loss: 0.12506073713302612
Batch 49/64 loss: 0.15518677234649658
Batch 50/64 loss: 0.17036795616149902
Batch 51/64 loss: 0.16406339406967163
Batch 52/64 loss: 0.16640377044677734
Batch 53/64 loss: 0.14346814155578613
Batch 54/64 loss: 0.15553230047225952
Batch 55/64 loss: 0.19236958026885986
Batch 56/64 loss: 0.1531619429588318
Batch 57/64 loss: 0.1477280855178833
Batch 58/64 loss: 0.17250728607177734
Batch 59/64 loss: 0.1602623462677002
Batch 60/64 loss: 0.14970040321350098
Batch 61/64 loss: 0.1580457091331482
Batch 62/64 loss: 0.14808130264282227
Batch 63/64 loss: 0.15404844284057617
Batch 64/64 loss: 0.16628742218017578
Epoch 51  Train loss: 0.15985746383666993  Val loss: 0.17917820145584054
Epoch 52
-------------------------------
Batch 1/64 loss: 0.18649828433990479
Batch 2/64 loss: 0.15929710865020752
Batch 3/64 loss: 0.15626490116119385
Batch 4/64 loss: 0.15417617559432983
Batch 5/64 loss: 0.17594993114471436
Batch 6/64 loss: 0.1859542727470398
Batch 7/64 loss: 0.15496456623077393
Batch 8/64 loss: 0.16674530506134033
Batch 9/64 loss: 0.16537052392959595
Batch 10/64 loss: 0.16160869598388672
Batch 11/64 loss: 0.12740927934646606
Batch 12/64 loss: 0.13321512937545776
Batch 13/64 loss: 0.15882712602615356
Batch 14/64 loss: 0.1548839807510376
Batch 15/64 loss: 0.14566510915756226
Batch 16/64 loss: 0.15983372926712036
Batch 17/64 loss: 0.1695261001586914
Batch 18/64 loss: 0.1417851448059082
Batch 19/64 loss: 0.1590537428855896
Batch 20/64 loss: 0.1422984004020691
Batch 21/64 loss: 0.17161327600479126
Batch 22/64 loss: 0.15189307928085327
Batch 23/64 loss: 0.17322438955307007
Batch 24/64 loss: 0.1603030562400818
Batch 25/64 loss: 0.1804930567741394
Batch 26/64 loss: 0.16104000806808472
Batch 27/64 loss: 0.1690797209739685
Batch 28/64 loss: 0.15110403299331665
Batch 29/64 loss: 0.1618584394454956
Batch 30/64 loss: 0.15301108360290527
Batch 31/64 loss: 0.1677614450454712
Batch 32/64 loss: 0.1433258056640625
Batch 33/64 loss: 0.16619247198104858
Batch 34/64 loss: 0.15082043409347534
Batch 35/64 loss: 0.17431408166885376
Batch 36/64 loss: 0.15751910209655762
Batch 37/64 loss: 0.1771758794784546
Batch 38/64 loss: 0.1616116762161255
Batch 39/64 loss: 0.1484045386314392
Batch 40/64 loss: 0.1619788408279419
Batch 41/64 loss: 0.16055595874786377
Batch 42/64 loss: 0.14802801609039307
Batch 43/64 loss: 0.16075384616851807
Batch 44/64 loss: 0.1436072587966919
Batch 45/64 loss: 0.15595459938049316
Batch 46/64 loss: 0.15896117687225342
Batch 47/64 loss: 0.16259068250656128
Batch 48/64 loss: 0.1680006980895996
Batch 49/64 loss: 0.14956969022750854
Batch 50/64 loss: 0.1506519317626953
Batch 51/64 loss: 0.14334511756896973
Batch 52/64 loss: 0.1562056541442871
Batch 53/64 loss: 0.16933703422546387
Batch 54/64 loss: 0.13332825899124146
Batch 55/64 loss: 0.16677820682525635
Batch 56/64 loss: 0.1558445692062378
Batch 57/64 loss: 0.1695290207862854
Batch 58/64 loss: 0.137212872505188
Batch 59/64 loss: 0.15714532136917114
Batch 60/64 loss: 0.16594398021697998
Batch 61/64 loss: 0.15985679626464844
Batch 62/64 loss: 0.17984414100646973
Batch 63/64 loss: 0.14034569263458252
Batch 64/64 loss: 0.1738682985305786
Epoch 52  Train loss: 0.15883676351285447  Val loss: 0.17827435703212044
Saving best model, epoch: 52
Epoch 53
-------------------------------
Batch 1/64 loss: 0.15312045812606812
Batch 2/64 loss: 0.16057324409484863
Batch 3/64 loss: 0.16066443920135498
Batch 4/64 loss: 0.15863889455795288
Batch 5/64 loss: 0.1481703519821167
Batch 6/64 loss: 0.13918113708496094
Batch 7/64 loss: 0.16123086214065552
Batch 8/64 loss: 0.16043531894683838
Batch 9/64 loss: 0.15507978200912476
Batch 10/64 loss: 0.1703660488128662
Batch 11/64 loss: 0.14269250631332397
Batch 12/64 loss: 0.1820811629295349
Batch 13/64 loss: 0.16379553079605103
Batch 14/64 loss: 0.1319001317024231
Batch 15/64 loss: 0.17301291227340698
Batch 16/64 loss: 0.14968162775039673
Batch 17/64 loss: 0.17887639999389648
Batch 18/64 loss: 0.15060824155807495
Batch 19/64 loss: 0.1600213646888733
Batch 20/64 loss: 0.17603325843811035
Batch 21/64 loss: 0.1476314663887024
Batch 22/64 loss: 0.17124205827713013
Batch 23/64 loss: 0.17568182945251465
Batch 24/64 loss: 0.1639336347579956
Batch 25/64 loss: 0.1694173812866211
Batch 26/64 loss: 0.12314903736114502
Batch 27/64 loss: 0.1629583239555359
Batch 28/64 loss: 0.14518684148788452
Batch 29/64 loss: 0.18090689182281494
Batch 30/64 loss: 0.1653980016708374
Batch 31/64 loss: 0.16186165809631348
Batch 32/64 loss: 0.15527260303497314
Batch 33/64 loss: 0.1480168104171753
Batch 34/64 loss: 0.1488051414489746
Batch 35/64 loss: 0.1664089560508728
Batch 36/64 loss: 0.13327592611312866
Batch 37/64 loss: 0.15443497896194458
Batch 38/64 loss: 0.1426323652267456
Batch 39/64 loss: 0.14195948839187622
Batch 40/64 loss: 0.1647350788116455
Batch 41/64 loss: 0.1658339500427246
Batch 42/64 loss: 0.13340020179748535
Batch 43/64 loss: 0.17833960056304932
Batch 44/64 loss: 0.14661884307861328
Batch 45/64 loss: 0.15719133615493774
Batch 46/64 loss: 0.15053534507751465
Batch 47/64 loss: 0.15677791833877563
Batch 48/64 loss: 0.1759324073791504
Batch 49/64 loss: 0.17014700174331665
Batch 50/64 loss: 0.17309844493865967
Batch 51/64 loss: 0.16695767641067505
Batch 52/64 loss: 0.14610296487808228
Batch 53/64 loss: 0.15618085861206055
Batch 54/64 loss: 0.15788841247558594
Batch 55/64 loss: 0.15653806924819946
Batch 56/64 loss: 0.14322441816329956
Batch 57/64 loss: 0.1604689359664917
Batch 58/64 loss: 0.17558902502059937
Batch 59/64 loss: 0.14093613624572754
Batch 60/64 loss: 0.1386668086051941
Batch 61/64 loss: 0.13537853956222534
Batch 62/64 loss: 0.16295111179351807
Batch 63/64 loss: 0.15964919328689575
Batch 64/64 loss: 0.16055834293365479
Epoch 53  Train loss: 0.15730036239998013  Val loss: 0.17601172600415155
Saving best model, epoch: 53
Epoch 54
-------------------------------
Batch 1/64 loss: 0.13582217693328857
Batch 2/64 loss: 0.15062373876571655
Batch 3/64 loss: 0.1700044870376587
Batch 4/64 loss: 0.15821349620819092
Batch 5/64 loss: 0.13896417617797852
Batch 6/64 loss: 0.1884821057319641
Batch 7/64 loss: 0.15176522731781006
Batch 8/64 loss: 0.15913063287734985
Batch 9/64 loss: 0.15998560190200806
Batch 10/64 loss: 0.13247334957122803
Batch 11/64 loss: 0.16708040237426758
Batch 12/64 loss: 0.16017615795135498
Batch 13/64 loss: 0.16220396757125854
Batch 14/64 loss: 0.15003138780593872
Batch 15/64 loss: 0.15062052011489868
Batch 16/64 loss: 0.14094871282577515
Batch 17/64 loss: 0.15753334760665894
Batch 18/64 loss: 0.1345604658126831
Batch 19/64 loss: 0.16029703617095947
Batch 20/64 loss: 0.13692903518676758
Batch 21/64 loss: 0.12821805477142334
Batch 22/64 loss: 0.14616090059280396
Batch 23/64 loss: 0.16932207345962524
Batch 24/64 loss: 0.16872555017471313
Batch 25/64 loss: 0.17497491836547852
Batch 26/64 loss: 0.12519603967666626
Batch 27/64 loss: 0.16718244552612305
Batch 28/64 loss: 0.16027414798736572
Batch 29/64 loss: 0.15303313732147217
Batch 30/64 loss: 0.13544940948486328
Batch 31/64 loss: 0.15149009227752686
Batch 32/64 loss: 0.1281341314315796
Batch 33/64 loss: 0.17653626203536987
Batch 34/64 loss: 0.16492903232574463
Batch 35/64 loss: 0.14683085680007935
Batch 36/64 loss: 0.1841335892677307
Batch 37/64 loss: 0.13826411962509155
Batch 38/64 loss: 0.18667298555374146
Batch 39/64 loss: 0.15571027994155884
Batch 40/64 loss: 0.13813048601150513
Batch 41/64 loss: 0.1552823781967163
Batch 42/64 loss: 0.13944268226623535
Batch 43/64 loss: 0.14353495836257935
Batch 44/64 loss: 0.15804272890090942
Batch 45/64 loss: 0.1527133584022522
Batch 46/64 loss: 0.1594821810722351
Batch 47/64 loss: 0.15759414434432983
Batch 48/64 loss: 0.19839942455291748
Batch 49/64 loss: 0.15365147590637207
Batch 50/64 loss: 0.15109944343566895
Batch 51/64 loss: 0.17565929889678955
Batch 52/64 loss: 0.19767677783966064
Batch 53/64 loss: 0.16416585445404053
Batch 54/64 loss: 0.15578573942184448
Batch 55/64 loss: 0.13259434700012207
Batch 56/64 loss: 0.1728684902191162
Batch 57/64 loss: 0.15019023418426514
Batch 58/64 loss: 0.13147932291030884
Batch 59/64 loss: 0.1555371880531311
Batch 60/64 loss: 0.18075048923492432
Batch 61/64 loss: 0.1593746542930603
Batch 62/64 loss: 0.15460216999053955
Batch 63/64 loss: 0.16889691352844238
Batch 64/64 loss: 0.1494562029838562
Epoch 54  Train loss: 0.15601774033378152  Val loss: 0.17940531973166973
Epoch 55
-------------------------------
Batch 1/64 loss: 0.16865772008895874
Batch 2/64 loss: 0.16316896677017212
Batch 3/64 loss: 0.1603606939315796
Batch 4/64 loss: 0.16992616653442383
Batch 5/64 loss: 0.1532282829284668
Batch 6/64 loss: 0.13444936275482178
Batch 7/64 loss: 0.1582697629928589
Batch 8/64 loss: 0.16937601566314697
Batch 9/64 loss: 0.17776525020599365
Batch 10/64 loss: 0.1539849042892456
Batch 11/64 loss: 0.15777534246444702
Batch 12/64 loss: 0.14124011993408203
Batch 13/64 loss: 0.11240702867507935
Batch 14/64 loss: 0.1820349097251892
Batch 15/64 loss: 0.14941519498825073
Batch 16/64 loss: 0.16596496105194092
Batch 17/64 loss: 0.1612473726272583
Batch 18/64 loss: 0.16518718004226685
Batch 19/64 loss: 0.14803588390350342
Batch 20/64 loss: 0.14358383417129517
Batch 21/64 loss: 0.14039182662963867
Batch 22/64 loss: 0.1734302043914795
Batch 23/64 loss: 0.15203356742858887
Batch 24/64 loss: 0.13571149110794067
Batch 25/64 loss: 0.15938544273376465
Batch 26/64 loss: 0.17591798305511475
Batch 27/64 loss: 0.14229077100753784
Batch 28/64 loss: 0.1644604206085205
Batch 29/64 loss: 0.14621001482009888
Batch 30/64 loss: 0.13679200410842896
Batch 31/64 loss: 0.13588333129882812
Batch 32/64 loss: 0.15134310722351074
Batch 33/64 loss: 0.164046049118042
Batch 34/64 loss: 0.1460951566696167
Batch 35/64 loss: 0.14677280187606812
Batch 36/64 loss: 0.13026970624923706
Batch 37/64 loss: 0.1529378890991211
Batch 38/64 loss: 0.16764378547668457
Batch 39/64 loss: 0.14831846952438354
Batch 40/64 loss: 0.12487083673477173
Batch 41/64 loss: 0.14087092876434326
Batch 42/64 loss: 0.14818525314331055
Batch 43/64 loss: 0.16403788328170776
Batch 44/64 loss: 0.17376530170440674
Batch 45/64 loss: 0.15558689832687378
Batch 46/64 loss: 0.19515156745910645
Batch 47/64 loss: 0.16492009162902832
Batch 48/64 loss: 0.159526526927948
Batch 49/64 loss: 0.14094209671020508
Batch 50/64 loss: 0.13487905263900757
Batch 51/64 loss: 0.11572080850601196
Batch 52/64 loss: 0.17424803972244263
Batch 53/64 loss: 0.16505944728851318
Batch 54/64 loss: 0.1428402066230774
Batch 55/64 loss: 0.16781169176101685
Batch 56/64 loss: 0.16616976261138916
Batch 57/64 loss: 0.14543628692626953
Batch 58/64 loss: 0.1558799147605896
Batch 59/64 loss: 0.15012049674987793
Batch 60/64 loss: 0.14095860719680786
Batch 61/64 loss: 0.1519402265548706
Batch 62/64 loss: 0.17348802089691162
Batch 63/64 loss: 0.16745924949645996
Batch 64/64 loss: 0.14742881059646606
Epoch 55  Train loss: 0.1542973142044217  Val loss: 0.17988529545335016
Epoch 56
-------------------------------
Batch 1/64 loss: 0.17327487468719482
Batch 2/64 loss: 0.139717698097229
Batch 3/64 loss: 0.14745742082595825
Batch 4/64 loss: 0.18223583698272705
Batch 5/64 loss: 0.14419716596603394
Batch 6/64 loss: 0.15564674139022827
Batch 7/64 loss: 0.174535870552063
Batch 8/64 loss: 0.13751763105392456
Batch 9/64 loss: 0.16288340091705322
Batch 10/64 loss: 0.14048272371292114
Batch 11/64 loss: 0.14739841222763062
Batch 12/64 loss: 0.15588170289993286
Batch 13/64 loss: 0.1583682894706726
Batch 14/64 loss: 0.14258694648742676
Batch 15/64 loss: 0.15823781490325928
Batch 16/64 loss: 0.15684449672698975
Batch 17/64 loss: 0.1598084568977356
Batch 18/64 loss: 0.16781765222549438
Batch 19/64 loss: 0.15577435493469238
Batch 20/64 loss: 0.16913700103759766
Batch 21/64 loss: 0.1328679323196411
Batch 22/64 loss: 0.17295539379119873
Batch 23/64 loss: 0.15568476915359497
Batch 24/64 loss: 0.15708667039871216
Batch 25/64 loss: 0.1637137532234192
Batch 26/64 loss: 0.16334348917007446
Batch 27/64 loss: 0.14706432819366455
Batch 28/64 loss: 0.14874422550201416
Batch 29/64 loss: 0.13498085737228394
Batch 30/64 loss: 0.1370639204978943
Batch 31/64 loss: 0.15666288137435913
Batch 32/64 loss: 0.1330457329750061
Batch 33/64 loss: 0.1567060351371765
Batch 34/64 loss: 0.13557744026184082
Batch 35/64 loss: 0.15384292602539062
Batch 36/64 loss: 0.16283351182937622
Batch 37/64 loss: 0.15385591983795166
Batch 38/64 loss: 0.14960169792175293
Batch 39/64 loss: 0.17708927392959595
Batch 40/64 loss: 0.14523828029632568
Batch 41/64 loss: 0.1469430923461914
Batch 42/64 loss: 0.1292564868927002
Batch 43/64 loss: 0.15619784593582153
Batch 44/64 loss: 0.1487516164779663
Batch 45/64 loss: 0.13937199115753174
Batch 46/64 loss: 0.16921991109848022
Batch 47/64 loss: 0.1566370129585266
Batch 48/64 loss: 0.15988194942474365
Batch 49/64 loss: 0.15680915117263794
Batch 50/64 loss: 0.14426231384277344
Batch 51/64 loss: 0.12970006465911865
Batch 52/64 loss: 0.1580127477645874
Batch 53/64 loss: 0.15065276622772217
Batch 54/64 loss: 0.1449074149131775
Batch 55/64 loss: 0.16420799493789673
Batch 56/64 loss: 0.14754009246826172
Batch 57/64 loss: 0.1381087303161621
Batch 58/64 loss: 0.15847325325012207
Batch 59/64 loss: 0.15595418214797974
Batch 60/64 loss: 0.16300863027572632
Batch 61/64 loss: 0.1519104242324829
Batch 62/64 loss: 0.1796278953552246
Batch 63/64 loss: 0.1605319380760193
Batch 64/64 loss: 0.16270089149475098
Epoch 56  Train loss: 0.15372167381585813  Val loss: 0.1754206769245187
Saving best model, epoch: 56
Epoch 57
-------------------------------
Batch 1/64 loss: 0.15118169784545898
Batch 2/64 loss: 0.1471630334854126
Batch 3/64 loss: 0.13808774948120117
Batch 4/64 loss: 0.1353110671043396
Batch 5/64 loss: 0.16575205326080322
Batch 6/64 loss: 0.1539144515991211
Batch 7/64 loss: 0.14263254404067993
Batch 8/64 loss: 0.16310787200927734
Batch 9/64 loss: 0.16708582639694214
Batch 10/64 loss: 0.15634047985076904
Batch 11/64 loss: 0.15400171279907227
Batch 12/64 loss: 0.13714444637298584
Batch 13/64 loss: 0.13081622123718262
Batch 14/64 loss: 0.1625983715057373
Batch 15/64 loss: 0.12846946716308594
Batch 16/64 loss: 0.1546715497970581
Batch 17/64 loss: 0.16105157136917114
Batch 18/64 loss: 0.16617125272750854
Batch 19/64 loss: 0.14848721027374268
Batch 20/64 loss: 0.14742207527160645
Batch 21/64 loss: 0.16274875402450562
Batch 22/64 loss: 0.16577231884002686
Batch 23/64 loss: 0.15128284692764282
Batch 24/64 loss: 0.13085335493087769
Batch 25/64 loss: 0.1815892457962036
Batch 26/64 loss: 0.1579309105873108
Batch 27/64 loss: 0.1418290138244629
Batch 28/64 loss: 0.17722004652023315
Batch 29/64 loss: 0.1907588243484497
Batch 30/64 loss: 0.14879989624023438
Batch 31/64 loss: 0.13786113262176514
Batch 32/64 loss: 0.14727342128753662
Batch 33/64 loss: 0.14783036708831787
Batch 34/64 loss: 0.16805070638656616
Batch 35/64 loss: 0.1505720615386963
Batch 36/64 loss: 0.13328468799591064
Batch 37/64 loss: 0.1426575779914856
Batch 38/64 loss: 0.17267626523971558
Batch 39/64 loss: 0.15939128398895264
Batch 40/64 loss: 0.1561092734336853
Batch 41/64 loss: 0.14321845769882202
Batch 42/64 loss: 0.15722501277923584
Batch 43/64 loss: 0.15058594942092896
Batch 44/64 loss: 0.15501683950424194
Batch 45/64 loss: 0.1539197564125061
Batch 46/64 loss: 0.16698312759399414
Batch 47/64 loss: 0.1570490002632141
Batch 48/64 loss: 0.16585296392440796
Batch 49/64 loss: 0.14832472801208496
Batch 50/64 loss: 0.13696938753128052
Batch 51/64 loss: 0.15835505723953247
Batch 52/64 loss: 0.14164185523986816
Batch 53/64 loss: 0.1786348819732666
Batch 54/64 loss: 0.15010076761245728
Batch 55/64 loss: 0.13946431875228882
Batch 56/64 loss: 0.15135222673416138
Batch 57/64 loss: 0.14173036813735962
Batch 58/64 loss: 0.18313872814178467
Batch 59/64 loss: 0.14380359649658203
Batch 60/64 loss: 0.15466928482055664
Batch 61/64 loss: 0.15333455801010132
Batch 62/64 loss: 0.1548801064491272
Batch 63/64 loss: 0.16150939464569092
Batch 64/64 loss: 0.1264183521270752
Epoch 57  Train loss: 0.1533878709755692  Val loss: 0.17160508804714558
Saving best model, epoch: 57
Epoch 58
-------------------------------
Batch 1/64 loss: 0.15846604108810425
Batch 2/64 loss: 0.16483116149902344
Batch 3/64 loss: 0.14045923948287964
Batch 4/64 loss: 0.15514922142028809
Batch 5/64 loss: 0.17041444778442383
Batch 6/64 loss: 0.15088850259780884
Batch 7/64 loss: 0.16532588005065918
Batch 8/64 loss: 0.143690288066864
Batch 9/64 loss: 0.130048930644989
Batch 10/64 loss: 0.17388039827346802
Batch 11/64 loss: 0.13089394569396973
Batch 12/64 loss: 0.167680561542511
Batch 13/64 loss: 0.15308308601379395
Batch 14/64 loss: 0.1410529613494873
Batch 15/64 loss: 0.14133751392364502
Batch 16/64 loss: 0.15900564193725586
Batch 17/64 loss: 0.15087997913360596
Batch 18/64 loss: 0.16785353422164917
Batch 19/64 loss: 0.14604425430297852
Batch 20/64 loss: 0.16312628984451294
Batch 21/64 loss: 0.15849459171295166
Batch 22/64 loss: 0.1408560872077942
Batch 23/64 loss: 0.1277943253517151
Batch 24/64 loss: 0.16546183824539185
Batch 25/64 loss: 0.16632187366485596
Batch 26/64 loss: 0.15205973386764526
Batch 27/64 loss: 0.16010749340057373
Batch 28/64 loss: 0.14308810234069824
Batch 29/64 loss: 0.16401058435440063
Batch 30/64 loss: 0.14319050312042236
Batch 31/64 loss: 0.16319358348846436
Batch 32/64 loss: 0.13045406341552734
Batch 33/64 loss: 0.16086947917938232
Batch 34/64 loss: 0.15072715282440186
Batch 35/64 loss: 0.16846829652786255
Batch 36/64 loss: 0.1368003487586975
Batch 37/64 loss: 0.1482018232345581
Batch 38/64 loss: 0.17491358518600464
Batch 39/64 loss: 0.13986128568649292
Batch 40/64 loss: 0.15228807926177979
Batch 41/64 loss: 0.15452146530151367
Batch 42/64 loss: 0.14742547273635864
Batch 43/64 loss: 0.12623828649520874
Batch 44/64 loss: 0.14155220985412598
Batch 45/64 loss: 0.1515527367591858
Batch 46/64 loss: 0.1642395257949829
Batch 47/64 loss: 0.14481991529464722
Batch 48/64 loss: 0.16131120920181274
Batch 49/64 loss: 0.13988816738128662
Batch 50/64 loss: 0.16307049989700317
Batch 51/64 loss: 0.14379841089248657
Batch 52/64 loss: 0.16387224197387695
Batch 53/64 loss: 0.14778900146484375
Batch 54/64 loss: 0.16414451599121094
Batch 55/64 loss: 0.16808176040649414
Batch 56/64 loss: 0.15775829553604126
Batch 57/64 loss: 0.1739557385444641
Batch 58/64 loss: 0.15642887353897095
Batch 59/64 loss: 0.15173178911209106
Batch 60/64 loss: 0.1382002830505371
Batch 61/64 loss: 0.15854299068450928
Batch 62/64 loss: 0.14398539066314697
Batch 63/64 loss: 0.13744378089904785
Batch 64/64 loss: 0.1339644193649292
Epoch 58  Train loss: 0.15250353859920127  Val loss: 0.1730766329159032
Epoch 59
-------------------------------
Batch 1/64 loss: 0.15986454486846924
Batch 2/64 loss: 0.14655756950378418
Batch 3/64 loss: 0.15982210636138916
Batch 4/64 loss: 0.16387838125228882
Batch 5/64 loss: 0.1313309669494629
Batch 6/64 loss: 0.15862876176834106
Batch 7/64 loss: 0.16718101501464844
Batch 8/64 loss: 0.1628814935684204
Batch 9/64 loss: 0.14101994037628174
Batch 10/64 loss: 0.13470971584320068
Batch 11/64 loss: 0.14644575119018555
Batch 12/64 loss: 0.14025640487670898
Batch 13/64 loss: 0.1631271243095398
Batch 14/64 loss: 0.14888840913772583
Batch 15/64 loss: 0.154396653175354
Batch 16/64 loss: 0.1596338152885437
Batch 17/64 loss: 0.1449471116065979
Batch 18/64 loss: 0.14797699451446533
Batch 19/64 loss: 0.130651593208313
Batch 20/64 loss: 0.16150468587875366
Batch 21/64 loss: 0.14846837520599365
Batch 22/64 loss: 0.14839482307434082
Batch 23/64 loss: 0.13214999437332153
Batch 24/64 loss: 0.12949156761169434
Batch 25/64 loss: 0.17008304595947266
Batch 26/64 loss: 0.15298974514007568
Batch 27/64 loss: 0.16321426630020142
Batch 28/64 loss: 0.151689350605011
Batch 29/64 loss: 0.15941554307937622
Batch 30/64 loss: 0.13812482357025146
Batch 31/64 loss: 0.16160821914672852
Batch 32/64 loss: 0.13504540920257568
Batch 33/64 loss: 0.18049192428588867
Batch 34/64 loss: 0.15066611766815186
Batch 35/64 loss: 0.17442476749420166
Batch 36/64 loss: 0.14867860078811646
Batch 37/64 loss: 0.15365946292877197
Batch 38/64 loss: 0.14743340015411377
Batch 39/64 loss: 0.14185017347335815
Batch 40/64 loss: 0.15845823287963867
Batch 41/64 loss: 0.1526850461959839
Batch 42/64 loss: 0.15853261947631836
Batch 43/64 loss: 0.15656036138534546
Batch 44/64 loss: 0.13603919744491577
Batch 45/64 loss: 0.1669759750366211
Batch 46/64 loss: 0.14784657955169678
Batch 47/64 loss: 0.15503615140914917
Batch 48/64 loss: 0.15797430276870728
Batch 49/64 loss: 0.14113163948059082
Batch 50/64 loss: 0.13956773281097412
Batch 51/64 loss: 0.135442852973938
Batch 52/64 loss: 0.14540159702301025
Batch 53/64 loss: 0.14775335788726807
Batch 54/64 loss: 0.15767884254455566
Batch 55/64 loss: 0.14879369735717773
Batch 56/64 loss: 0.14539456367492676
Batch 57/64 loss: 0.1501610279083252
Batch 58/64 loss: 0.16012156009674072
Batch 59/64 loss: 0.13976073265075684
Batch 60/64 loss: 0.15015852451324463
Batch 61/64 loss: 0.13154828548431396
Batch 62/64 loss: 0.18885177373886108
Batch 63/64 loss: 0.15402019023895264
Batch 64/64 loss: 0.15952861309051514
Epoch 59  Train loss: 0.15148429730359245  Val loss: 0.1747806518757876
Epoch 60
-------------------------------
Batch 1/64 loss: 0.13971751928329468
Batch 2/64 loss: 0.14265722036361694
Batch 3/64 loss: 0.1482502818107605
Batch 4/64 loss: 0.14733093976974487
Batch 5/64 loss: 0.14828139543533325
Batch 6/64 loss: 0.16799914836883545
Batch 7/64 loss: 0.1455872654914856
Batch 8/64 loss: 0.13753777742385864
Batch 9/64 loss: 0.15793800354003906
Batch 10/64 loss: 0.12555384635925293
Batch 11/64 loss: 0.11434781551361084
Batch 12/64 loss: 0.14790767431259155
Batch 13/64 loss: 0.13218796253204346
Batch 14/64 loss: 0.15795278549194336
Batch 15/64 loss: 0.16260814666748047
Batch 16/64 loss: 0.1534590721130371
Batch 17/64 loss: 0.17149513959884644
Batch 18/64 loss: 0.12028175592422485
Batch 19/64 loss: 0.1602042317390442
Batch 20/64 loss: 0.1737278699874878
Batch 21/64 loss: 0.12695688009262085
Batch 22/64 loss: 0.1665571928024292
Batch 23/64 loss: 0.12820321321487427
Batch 24/64 loss: 0.15068179368972778
Batch 25/64 loss: 0.15356040000915527
Batch 26/64 loss: 0.14494770765304565
Batch 27/64 loss: 0.16478514671325684
Batch 28/64 loss: 0.1559721827507019
Batch 29/64 loss: 0.1625472903251648
Batch 30/64 loss: 0.1419321894645691
Batch 31/64 loss: 0.12552839517593384
Batch 32/64 loss: 0.15398240089416504
Batch 33/64 loss: 0.14412081241607666
Batch 34/64 loss: 0.14240175485610962
Batch 35/64 loss: 0.17679977416992188
Batch 36/64 loss: 0.1797882318496704
Batch 37/64 loss: 0.13818365335464478
Batch 38/64 loss: 0.12236851453781128
Batch 39/64 loss: 0.14097249507904053
Batch 40/64 loss: 0.14314931631088257
Batch 41/64 loss: 0.14414173364639282
Batch 42/64 loss: 0.1633908748626709
Batch 43/64 loss: 0.14008265733718872
Batch 44/64 loss: 0.15977883338928223
Batch 45/64 loss: 0.1637204885482788
Batch 46/64 loss: 0.17160946130752563
Batch 47/64 loss: 0.1405758261680603
Batch 48/64 loss: 0.1633920669555664
Batch 49/64 loss: 0.13848745822906494
Batch 50/64 loss: 0.14921975135803223
Batch 51/64 loss: 0.13212227821350098
Batch 52/64 loss: 0.15193098783493042
Batch 53/64 loss: 0.1465919017791748
Batch 54/64 loss: 0.14582175016403198
Batch 55/64 loss: 0.14467447996139526
Batch 56/64 loss: 0.12488991022109985
Batch 57/64 loss: 0.15334129333496094
Batch 58/64 loss: 0.1514607071876526
Batch 59/64 loss: 0.16241484880447388
Batch 60/64 loss: 0.13637477159500122
Batch 61/64 loss: 0.1621021032333374
Batch 62/64 loss: 0.14654392004013062
Batch 63/64 loss: 0.16092383861541748
Batch 64/64 loss: 0.17060667276382446
Epoch 60  Train loss: 0.14901979834425683  Val loss: 0.1686223825228583
Saving best model, epoch: 60
Epoch 61
-------------------------------
Batch 1/64 loss: 0.13310354948043823
Batch 2/64 loss: 0.14702612161636353
Batch 3/64 loss: 0.15851205587387085
Batch 4/64 loss: 0.14114254713058472
Batch 5/64 loss: 0.13001632690429688
Batch 6/64 loss: 0.12258964776992798
Batch 7/64 loss: 0.12817257642745972
Batch 8/64 loss: 0.1413518190383911
Batch 9/64 loss: 0.15404772758483887
Batch 10/64 loss: 0.1574224829673767
Batch 11/64 loss: 0.13909214735031128
Batch 12/64 loss: 0.15410786867141724
Batch 13/64 loss: 0.1512465476989746
Batch 14/64 loss: 0.1522628664970398
Batch 15/64 loss: 0.1411672830581665
Batch 16/64 loss: 0.17385423183441162
Batch 17/64 loss: 0.12900930643081665
Batch 18/64 loss: 0.13065588474273682
Batch 19/64 loss: 0.12976199388504028
Batch 20/64 loss: 0.1644173264503479
Batch 21/64 loss: 0.1476336121559143
Batch 22/64 loss: 0.16205573081970215
Batch 23/64 loss: 0.15094292163848877
Batch 24/64 loss: 0.14287340641021729
Batch 25/64 loss: 0.15688729286193848
Batch 26/64 loss: 0.15362262725830078
Batch 27/64 loss: 0.11470222473144531
Batch 28/64 loss: 0.1499335765838623
Batch 29/64 loss: 0.14426767826080322
Batch 30/64 loss: 0.13892638683319092
Batch 31/64 loss: 0.13661009073257446
Batch 32/64 loss: 0.15939223766326904
Batch 33/64 loss: 0.15455204248428345
Batch 34/64 loss: 0.1214834451675415
Batch 35/64 loss: 0.16057121753692627
Batch 36/64 loss: 0.14883172512054443
Batch 37/64 loss: 0.16894543170928955
Batch 38/64 loss: 0.16104340553283691
Batch 39/64 loss: 0.14673888683319092
Batch 40/64 loss: 0.16055500507354736
Batch 41/64 loss: 0.14650440216064453
Batch 42/64 loss: 0.14338958263397217
Batch 43/64 loss: 0.14941108226776123
Batch 44/64 loss: 0.13830077648162842
Batch 45/64 loss: 0.1349658966064453
Batch 46/64 loss: 0.16020846366882324
Batch 47/64 loss: 0.17418330907821655
Batch 48/64 loss: 0.14021068811416626
Batch 49/64 loss: 0.14778095483779907
Batch 50/64 loss: 0.18253672122955322
Batch 51/64 loss: 0.12540924549102783
Batch 52/64 loss: 0.14862662553787231
Batch 53/64 loss: 0.1414012908935547
Batch 54/64 loss: 0.1404476761817932
Batch 55/64 loss: 0.16671711206436157
Batch 56/64 loss: 0.14922434091567993
Batch 57/64 loss: 0.1548466682434082
Batch 58/64 loss: 0.17262107133865356
Batch 59/64 loss: 0.14611124992370605
Batch 60/64 loss: 0.14168071746826172
Batch 61/64 loss: 0.15298622846603394
Batch 62/64 loss: 0.15664148330688477
Batch 63/64 loss: 0.147455632686615
Batch 64/64 loss: 0.15171480178833008
Epoch 61  Train loss: 0.14799963259229473  Val loss: 0.1720611815600051
Epoch 62
-------------------------------
Batch 1/64 loss: 0.15342170000076294
Batch 2/64 loss: 0.1671103835105896
Batch 3/64 loss: 0.1601959466934204
Batch 4/64 loss: 0.1540590524673462
Batch 5/64 loss: 0.18045347929000854
Batch 6/64 loss: 0.13572561740875244
Batch 7/64 loss: 0.15391284227371216
Batch 8/64 loss: 0.13611257076263428
Batch 9/64 loss: 0.15400344133377075
Batch 10/64 loss: 0.14848029613494873
Batch 11/64 loss: 0.1609775424003601
Batch 12/64 loss: 0.15561926364898682
Batch 13/64 loss: 0.14115434885025024
Batch 14/64 loss: 0.14790493249893188
Batch 15/64 loss: 0.17362236976623535
Batch 16/64 loss: 0.1812206506729126
Batch 17/64 loss: 0.12933552265167236
Batch 18/64 loss: 0.144004225730896
Batch 19/64 loss: 0.14963793754577637
Batch 20/64 loss: 0.14304286241531372
Batch 21/64 loss: 0.15154212713241577
Batch 22/64 loss: 0.14592838287353516
Batch 23/64 loss: 0.13777035474777222
Batch 24/64 loss: 0.14402931928634644
Batch 25/64 loss: 0.1577642560005188
Batch 26/64 loss: 0.13911515474319458
Batch 27/64 loss: 0.14152389764785767
Batch 28/64 loss: 0.1263696551322937
Batch 29/64 loss: 0.14107400178909302
Batch 30/64 loss: 0.13738656044006348
Batch 31/64 loss: 0.15701496601104736
Batch 32/64 loss: 0.13495123386383057
Batch 33/64 loss: 0.13913637399673462
Batch 34/64 loss: 0.13962560892105103
Batch 35/64 loss: 0.14180117845535278
Batch 36/64 loss: 0.13379180431365967
Batch 37/64 loss: 0.14769864082336426
Batch 38/64 loss: 0.1294894814491272
Batch 39/64 loss: 0.15541678667068481
Batch 40/64 loss: 0.15680086612701416
Batch 41/64 loss: 0.1682073473930359
Batch 42/64 loss: 0.15954411029815674
Batch 43/64 loss: 0.16430842876434326
Batch 44/64 loss: 0.15796345472335815
Batch 45/64 loss: 0.16612368822097778
Batch 46/64 loss: 0.15465009212493896
Batch 47/64 loss: 0.13320863246917725
Batch 48/64 loss: 0.1557070016860962
Batch 49/64 loss: 0.1413707137107849
Batch 50/64 loss: 0.1564388871192932
Batch 51/64 loss: 0.16633886098861694
Batch 52/64 loss: 0.15788602828979492
Batch 53/64 loss: 0.15083962678909302
Batch 54/64 loss: 0.1382926106452942
Batch 55/64 loss: 0.13336873054504395
Batch 56/64 loss: 0.14386141300201416
Batch 57/64 loss: 0.12801378965377808
Batch 58/64 loss: 0.12012189626693726
Batch 59/64 loss: 0.12485003471374512
Batch 60/64 loss: 0.14569330215454102
Batch 61/64 loss: 0.1486051082611084
Batch 62/64 loss: 0.1292574405670166
Batch 63/64 loss: 0.14487427473068237
Batch 64/64 loss: 0.14182573556900024
Epoch 62  Train loss: 0.1478293397847344  Val loss: 0.17271191680554263
Epoch 63
-------------------------------
Batch 1/64 loss: 0.1587059497833252
Batch 2/64 loss: 0.16125619411468506
Batch 3/64 loss: 0.1500951051712036
Batch 4/64 loss: 0.10723817348480225
Batch 5/64 loss: 0.14403635263442993
Batch 6/64 loss: 0.15729063749313354
Batch 7/64 loss: 0.1280009150505066
Batch 8/64 loss: 0.17001736164093018
Batch 9/64 loss: 0.14060986042022705
Batch 10/64 loss: 0.14286577701568604
Batch 11/64 loss: 0.13941329717636108
Batch 12/64 loss: 0.1550983190536499
Batch 13/64 loss: 0.14002257585525513
Batch 14/64 loss: 0.13568592071533203
Batch 15/64 loss: 0.14337271451950073
Batch 16/64 loss: 0.15537023544311523
Batch 17/64 loss: 0.150221586227417
Batch 18/64 loss: 0.1443660855293274
Batch 19/64 loss: 0.15835821628570557
Batch 20/64 loss: 0.13346272706985474
Batch 21/64 loss: 0.15674132108688354
Batch 22/64 loss: 0.12985223531723022
Batch 23/64 loss: 0.12384802103042603
Batch 24/64 loss: 0.14680659770965576
Batch 25/64 loss: 0.13228708505630493
Batch 26/64 loss: 0.1485973596572876
Batch 27/64 loss: 0.14299893379211426
Batch 28/64 loss: 0.1405203938484192
Batch 29/64 loss: 0.11430460214614868
Batch 30/64 loss: 0.13260889053344727
Batch 31/64 loss: 0.1556873321533203
Batch 32/64 loss: 0.12885379791259766
Batch 33/64 loss: 0.1529138684272766
Batch 34/64 loss: 0.14786845445632935
Batch 35/64 loss: 0.12526416778564453
Batch 36/64 loss: 0.13312780857086182
Batch 37/64 loss: 0.15654867887496948
Batch 38/64 loss: 0.16631168127059937
Batch 39/64 loss: 0.15765589475631714
Batch 40/64 loss: 0.16162538528442383
Batch 41/64 loss: 0.13571155071258545
Batch 42/64 loss: 0.14289826154708862
Batch 43/64 loss: 0.1584683060646057
Batch 44/64 loss: 0.1287165880203247
Batch 45/64 loss: 0.15226948261260986
Batch 46/64 loss: 0.17467403411865234
Batch 47/64 loss: 0.15431249141693115
Batch 48/64 loss: 0.15534371137619019
Batch 49/64 loss: 0.15563738346099854
Batch 50/64 loss: 0.1483018398284912
Batch 51/64 loss: 0.16539692878723145
Batch 52/64 loss: 0.15137767791748047
Batch 53/64 loss: 0.15761524438858032
Batch 54/64 loss: 0.15896165370941162
Batch 55/64 loss: 0.1258765459060669
Batch 56/64 loss: 0.15014511346817017
Batch 57/64 loss: 0.14837777614593506
Batch 58/64 loss: 0.1368570327758789
Batch 59/64 loss: 0.17044305801391602
Batch 60/64 loss: 0.13494402170181274
Batch 61/64 loss: 0.15337491035461426
Batch 62/64 loss: 0.13317054510116577
Batch 63/64 loss: 0.14874732494354248
Batch 64/64 loss: 0.13002610206604004
Epoch 63  Train loss: 0.14602433840433757  Val loss: 0.16579613775731772
Saving best model, epoch: 63
Epoch 64
-------------------------------
Batch 1/64 loss: 0.11538445949554443
Batch 2/64 loss: 0.15895378589630127
Batch 3/64 loss: 0.1311601996421814
Batch 4/64 loss: 0.14512336254119873
Batch 5/64 loss: 0.13529938459396362
Batch 6/64 loss: 0.15751582384109497
Batch 7/64 loss: 0.13413536548614502
Batch 8/64 loss: 0.16221153736114502
Batch 9/64 loss: 0.11848366260528564
Batch 10/64 loss: 0.162054181098938
Batch 11/64 loss: 0.15323662757873535
Batch 12/64 loss: 0.15472561120986938
Batch 13/64 loss: 0.14001423120498657
Batch 14/64 loss: 0.149527907371521
Batch 15/64 loss: 0.11350762844085693
Batch 16/64 loss: 0.14875507354736328
Batch 17/64 loss: 0.15020817518234253
Batch 18/64 loss: 0.14372241497039795
Batch 19/64 loss: 0.15562677383422852
Batch 20/64 loss: 0.15124690532684326
Batch 21/64 loss: 0.14777898788452148
Batch 22/64 loss: 0.1559773087501526
Batch 23/64 loss: 0.16759860515594482
Batch 24/64 loss: 0.1508769392967224
Batch 25/64 loss: 0.16846323013305664
Batch 26/64 loss: 0.14925158023834229
Batch 27/64 loss: 0.14998888969421387
Batch 28/64 loss: 0.1621474027633667
Batch 29/64 loss: 0.1489887237548828
Batch 30/64 loss: 0.16939449310302734
Batch 31/64 loss: 0.17202448844909668
Batch 32/64 loss: 0.1480787992477417
Batch 33/64 loss: 0.1559801697731018
Batch 34/64 loss: 0.1380399465560913
Batch 35/64 loss: 0.16402184963226318
Batch 36/64 loss: 0.14317727088928223
Batch 37/64 loss: 0.15064513683319092
Batch 38/64 loss: 0.12545156478881836
Batch 39/64 loss: 0.14259475469589233
Batch 40/64 loss: 0.14411133527755737
Batch 41/64 loss: 0.15187227725982666
Batch 42/64 loss: 0.1453419327735901
Batch 43/64 loss: 0.12517297267913818
Batch 44/64 loss: 0.15303194522857666
Batch 45/64 loss: 0.137365460395813
Batch 46/64 loss: 0.1399219036102295
Batch 47/64 loss: 0.14962142705917358
Batch 48/64 loss: 0.12956339120864868
Batch 49/64 loss: 0.13861221075057983
Batch 50/64 loss: 0.14797329902648926
Batch 51/64 loss: 0.13454389572143555
Batch 52/64 loss: 0.11171424388885498
Batch 53/64 loss: 0.1543668508529663
Batch 54/64 loss: 0.13853472471237183
Batch 55/64 loss: 0.13715624809265137
Batch 56/64 loss: 0.12970280647277832
Batch 57/64 loss: 0.1477096676826477
Batch 58/64 loss: 0.13019919395446777
Batch 59/64 loss: 0.14585500955581665
Batch 60/64 loss: 0.1348475217819214
Batch 61/64 loss: 0.15041404962539673
Batch 62/64 loss: 0.14476364850997925
Batch 63/64 loss: 0.14091193675994873
Batch 64/64 loss: 0.13430339097976685
Epoch 64  Train loss: 0.1451828823370092  Val loss: 0.16681890799007876
Epoch 65
-------------------------------
Batch 1/64 loss: 0.17983901500701904
Batch 2/64 loss: 0.15292304754257202
Batch 3/64 loss: 0.14681142568588257
Batch 4/64 loss: 0.14124232530593872
Batch 5/64 loss: 0.1506117582321167
Batch 6/64 loss: 0.165205180644989
Batch 7/64 loss: 0.15493369102478027
Batch 8/64 loss: 0.1488543152809143
Batch 9/64 loss: 0.13689076900482178
Batch 10/64 loss: 0.14942067861557007
Batch 11/64 loss: 0.15377277135849
Batch 12/64 loss: 0.14615130424499512
Batch 13/64 loss: 0.1241302490234375
Batch 14/64 loss: 0.13080346584320068
Batch 15/64 loss: 0.12069636583328247
Batch 16/64 loss: 0.14749598503112793
Batch 17/64 loss: 0.1743064522743225
Batch 18/64 loss: 0.1415879726409912
Batch 19/64 loss: 0.138302743434906
Batch 20/64 loss: 0.16666865348815918
Batch 21/64 loss: 0.1336073875427246
Batch 22/64 loss: 0.15869975090026855
Batch 23/64 loss: 0.12895500659942627
Batch 24/64 loss: 0.1413264274597168
Batch 25/64 loss: 0.1500837802886963
Batch 26/64 loss: 0.14713621139526367
Batch 27/64 loss: 0.14821678400039673
Batch 28/64 loss: 0.1460651159286499
Batch 29/64 loss: 0.1518700122833252
Batch 30/64 loss: 0.15307140350341797
Batch 31/64 loss: 0.12237060070037842
Batch 32/64 loss: 0.13756918907165527
Batch 33/64 loss: 0.14523011445999146
Batch 34/64 loss: 0.131214439868927
Batch 35/64 loss: 0.11464506387710571
Batch 36/64 loss: 0.09558665752410889
Batch 37/64 loss: 0.13600969314575195
Batch 38/64 loss: 0.13022786378860474
Batch 39/64 loss: 0.14437764883041382
Batch 40/64 loss: 0.121884286403656
Batch 41/64 loss: 0.12551814317703247
Batch 42/64 loss: 0.11746931076049805
Batch 43/64 loss: 0.15489286184310913
Batch 44/64 loss: 0.12493056058883667
Batch 45/64 loss: 0.15847563743591309
Batch 46/64 loss: 0.1496090292930603
Batch 47/64 loss: 0.16873139142990112
Batch 48/64 loss: 0.15311479568481445
Batch 49/64 loss: 0.13902157545089722
Batch 50/64 loss: 0.1453472375869751
Batch 51/64 loss: 0.13629364967346191
Batch 52/64 loss: 0.16107434034347534
Batch 53/64 loss: 0.13129746913909912
Batch 54/64 loss: 0.1382167935371399
Batch 55/64 loss: 0.14831674098968506
Batch 56/64 loss: 0.1519562005996704
Batch 57/64 loss: 0.15427732467651367
Batch 58/64 loss: 0.14048612117767334
Batch 59/64 loss: 0.18193954229354858
Batch 60/64 loss: 0.13840442895889282
Batch 61/64 loss: 0.1468098759651184
Batch 62/64 loss: 0.16713058948516846
Batch 63/64 loss: 0.12857931852340698
Batch 64/64 loss: 0.16206282377243042
Epoch 65  Train loss: 0.14419193197699154  Val loss: 0.16641802824649615
Epoch 66
-------------------------------
Batch 1/64 loss: 0.16022354364395142
Batch 2/64 loss: 0.1310657262802124
Batch 3/64 loss: 0.1456465721130371
Batch 4/64 loss: 0.13268691301345825
Batch 5/64 loss: 0.12294644117355347
Batch 6/64 loss: 0.1500905156135559
Batch 7/64 loss: 0.1287001371383667
Batch 8/64 loss: 0.11426043510437012
Batch 9/64 loss: 0.1518823504447937
Batch 10/64 loss: 0.13803094625473022
Batch 11/64 loss: 0.13728654384613037
Batch 12/64 loss: 0.14168566465377808
Batch 13/64 loss: 0.18288367986679077
Batch 14/64 loss: 0.15768665075302124
Batch 15/64 loss: 0.16115975379943848
Batch 16/64 loss: 0.13415145874023438
Batch 17/64 loss: 0.14427250623703003
Batch 18/64 loss: 0.10957813262939453
Batch 19/64 loss: 0.16294580698013306
Batch 20/64 loss: 0.14342546463012695
Batch 21/64 loss: 0.14788836240768433
Batch 22/64 loss: 0.14501559734344482
Batch 23/64 loss: 0.15733402967453003
Batch 24/64 loss: 0.13579416275024414
Batch 25/64 loss: 0.15221917629241943
Batch 26/64 loss: 0.17202329635620117
Batch 27/64 loss: 0.14383715391159058
Batch 28/64 loss: 0.146906316280365
Batch 29/64 loss: 0.1253872513771057
Batch 30/64 loss: 0.1675626039505005
Batch 31/64 loss: 0.1431836485862732
Batch 32/64 loss: 0.1322873830795288
Batch 33/64 loss: 0.1455904245376587
Batch 34/64 loss: 0.14414167404174805
Batch 35/64 loss: 0.16401875019073486
Batch 36/64 loss: 0.1421893835067749
Batch 37/64 loss: 0.12815898656845093
Batch 38/64 loss: 0.13589352369308472
Batch 39/64 loss: 0.15606945753097534
Batch 40/64 loss: 0.14186179637908936
Batch 41/64 loss: 0.13624942302703857
Batch 42/64 loss: 0.12914782762527466
Batch 43/64 loss: 0.12131643295288086
Batch 44/64 loss: 0.1380072832107544
Batch 45/64 loss: 0.13278627395629883
Batch 46/64 loss: 0.15077412128448486
Batch 47/64 loss: 0.15664523839950562
Batch 48/64 loss: 0.1357555389404297
Batch 49/64 loss: 0.13977265357971191
Batch 50/64 loss: 0.1232338547706604
Batch 51/64 loss: 0.12784498929977417
Batch 52/64 loss: 0.12223255634307861
Batch 53/64 loss: 0.13568353652954102
Batch 54/64 loss: 0.16420280933380127
Batch 55/64 loss: 0.16120004653930664
Batch 56/64 loss: 0.1599128246307373
Batch 57/64 loss: 0.14925003051757812
Batch 58/64 loss: 0.1142510175704956
Batch 59/64 loss: 0.12797772884368896
Batch 60/64 loss: 0.13501155376434326
Batch 61/64 loss: 0.14023488759994507
Batch 62/64 loss: 0.1424713134765625
Batch 63/64 loss: 0.1252250075340271
Batch 64/64 loss: 0.1460508108139038
Epoch 66  Train loss: 0.14206583967395858  Val loss: 0.16655587904232064
Epoch 67
-------------------------------
Batch 1/64 loss: 0.166936457157135
Batch 2/64 loss: 0.17318081855773926
Batch 3/64 loss: 0.13738203048706055
Batch 4/64 loss: 0.16781580448150635
Batch 5/64 loss: 0.12391209602355957
Batch 6/64 loss: 0.15479278564453125
Batch 7/64 loss: 0.13295704126358032
Batch 8/64 loss: 0.17395281791687012
Batch 9/64 loss: 0.1310441493988037
Batch 10/64 loss: 0.1352841854095459
Batch 11/64 loss: 0.15941846370697021
Batch 12/64 loss: 0.17198574542999268
Batch 13/64 loss: 0.16453814506530762
Batch 14/64 loss: 0.14078038930892944
Batch 15/64 loss: 0.13219058513641357
Batch 16/64 loss: 0.1271398663520813
Batch 17/64 loss: 0.13452047109603882
Batch 18/64 loss: 0.15317130088806152
Batch 19/64 loss: 0.1569662094116211
Batch 20/64 loss: 0.14002680778503418
Batch 21/64 loss: 0.14317578077316284
Batch 22/64 loss: 0.16027647256851196
Batch 23/64 loss: 0.13630664348602295
Batch 24/64 loss: 0.14866262674331665
Batch 25/64 loss: 0.13475525379180908
Batch 26/64 loss: 0.14691191911697388
Batch 27/64 loss: 0.15117579698562622
Batch 28/64 loss: 0.12558460235595703
Batch 29/64 loss: 0.1254214644432068
Batch 30/64 loss: 0.12673664093017578
Batch 31/64 loss: 0.1393696665763855
Batch 32/64 loss: 0.13864600658416748
Batch 33/64 loss: 0.13114452362060547
Batch 34/64 loss: 0.1381661295890808
Batch 35/64 loss: 0.12843680381774902
Batch 36/64 loss: 0.1400023102760315
Batch 37/64 loss: 0.153048574924469
Batch 38/64 loss: 0.1440933346748352
Batch 39/64 loss: 0.13555890321731567
Batch 40/64 loss: 0.12858128547668457
Batch 41/64 loss: 0.13482171297073364
Batch 42/64 loss: 0.13750040531158447
Batch 43/64 loss: 0.14162898063659668
Batch 44/64 loss: 0.12375712394714355
Batch 45/64 loss: 0.14959102869033813
Batch 46/64 loss: 0.14828109741210938
Batch 47/64 loss: 0.13059145212173462
Batch 48/64 loss: 0.13908648490905762
Batch 49/64 loss: 0.15659737586975098
Batch 50/64 loss: 0.1359797716140747
Batch 51/64 loss: 0.11356258392333984
Batch 52/64 loss: 0.15278029441833496
Batch 53/64 loss: 0.13726484775543213
Batch 54/64 loss: 0.14246076345443726
Batch 55/64 loss: 0.1434105634689331
Batch 56/64 loss: 0.13814282417297363
Batch 57/64 loss: 0.16385096311569214
Batch 58/64 loss: 0.13364624977111816
Batch 59/64 loss: 0.12862563133239746
Batch 60/64 loss: 0.15996181964874268
Batch 61/64 loss: 0.1403214931488037
Batch 62/64 loss: 0.1561182737350464
Batch 63/64 loss: 0.14626586437225342
Batch 64/64 loss: 0.14724373817443848
Epoch 67  Train loss: 0.1430389226651659  Val loss: 0.16611742194985196
Epoch 68
-------------------------------
Batch 1/64 loss: 0.13259583711624146
Batch 2/64 loss: 0.14509624242782593
Batch 3/64 loss: 0.1409519910812378
Batch 4/64 loss: 0.1375589370727539
Batch 5/64 loss: 0.1344650387763977
Batch 6/64 loss: 0.13883626461029053
Batch 7/64 loss: 0.1344754695892334
Batch 8/64 loss: 0.147938072681427
Batch 9/64 loss: 0.13626009225845337
Batch 10/64 loss: 0.13799846172332764
Batch 11/64 loss: 0.14352494478225708
Batch 12/64 loss: 0.16949748992919922
Batch 13/64 loss: 0.14126169681549072
Batch 14/64 loss: 0.1274348497390747
Batch 15/64 loss: 0.15466070175170898
Batch 16/64 loss: 0.15102505683898926
Batch 17/64 loss: 0.12888729572296143
Batch 18/64 loss: 0.12357532978057861
Batch 19/64 loss: 0.14709573984146118
Batch 20/64 loss: 0.14780020713806152
Batch 21/64 loss: 0.13801336288452148
Batch 22/64 loss: 0.14404094219207764
Batch 23/64 loss: 0.13651353120803833
Batch 24/64 loss: 0.1327991485595703
Batch 25/64 loss: 0.13837957382202148
Batch 26/64 loss: 0.14284998178482056
Batch 27/64 loss: 0.14366787672042847
Batch 28/64 loss: 0.15554684400558472
Batch 29/64 loss: 0.13222920894622803
Batch 30/64 loss: 0.16012650728225708
Batch 31/64 loss: 0.12883174419403076
Batch 32/64 loss: 0.1451786756515503
Batch 33/64 loss: 0.133594810962677
Batch 34/64 loss: 0.1383550763130188
Batch 35/64 loss: 0.1435520052909851
Batch 36/64 loss: 0.15355592966079712
Batch 37/64 loss: 0.14714080095291138
Batch 38/64 loss: 0.14872264862060547
Batch 39/64 loss: 0.182891845703125
Batch 40/64 loss: 0.12328660488128662
Batch 41/64 loss: 0.18255549669265747
Batch 42/64 loss: 0.1361370086669922
Batch 43/64 loss: 0.15459918975830078
Batch 44/64 loss: 0.14446121454238892
Batch 45/64 loss: 0.15638679265975952
Batch 46/64 loss: 0.1197507381439209
Batch 47/64 loss: 0.13794785737991333
Batch 48/64 loss: 0.14254289865493774
Batch 49/64 loss: 0.15196311473846436
Batch 50/64 loss: 0.15345382690429688
Batch 51/64 loss: 0.12382334470748901
Batch 52/64 loss: 0.16489511728286743
Batch 53/64 loss: 0.14561933279037476
Batch 54/64 loss: 0.13450121879577637
Batch 55/64 loss: 0.15493714809417725
Batch 56/64 loss: 0.13966608047485352
Batch 57/64 loss: 0.13530617952346802
Batch 58/64 loss: 0.1297682523727417
Batch 59/64 loss: 0.13057690858840942
Batch 60/64 loss: 0.14094316959381104
Batch 61/64 loss: 0.12094783782958984
Batch 62/64 loss: 0.12112116813659668
Batch 63/64 loss: 0.12453639507293701
Batch 64/64 loss: 0.16743355989456177
Epoch 68  Train loss: 0.14215266353943767  Val loss: 0.16501908122059405
Saving best model, epoch: 68
Epoch 69
-------------------------------
Batch 1/64 loss: 0.12350916862487793
Batch 2/64 loss: 0.12338602542877197
Batch 3/64 loss: 0.15402162075042725
Batch 4/64 loss: 0.16115456819534302
Batch 5/64 loss: 0.12730145454406738
Batch 6/64 loss: 0.1480417251586914
Batch 7/64 loss: 0.1647803783416748
Batch 8/64 loss: 0.1289103627204895
Batch 9/64 loss: 0.13961797952651978
Batch 10/64 loss: 0.19208908081054688
Batch 11/64 loss: 0.1303502917289734
Batch 12/64 loss: 0.12494540214538574
Batch 13/64 loss: 0.1225743293762207
Batch 14/64 loss: 0.1322784423828125
Batch 15/64 loss: 0.11446493864059448
Batch 16/64 loss: 0.15062004327774048
Batch 17/64 loss: 0.13958871364593506
Batch 18/64 loss: 0.15156733989715576
Batch 19/64 loss: 0.1510828733444214
Batch 20/64 loss: 0.13675224781036377
Batch 21/64 loss: 0.13861185312271118
Batch 22/64 loss: 0.14958816766738892
Batch 23/64 loss: 0.17579448223114014
Batch 24/64 loss: 0.1366945505142212
Batch 25/64 loss: 0.16189515590667725
Batch 26/64 loss: 0.10652375221252441
Batch 27/64 loss: 0.15552949905395508
Batch 28/64 loss: 0.14298272132873535
Batch 29/64 loss: 0.12233030796051025
Batch 30/64 loss: 0.15460699796676636
Batch 31/64 loss: 0.13360339403152466
Batch 32/64 loss: 0.14183253049850464
Batch 33/64 loss: 0.14622271060943604
Batch 34/64 loss: 0.15580511093139648
Batch 35/64 loss: 0.12843108177185059
Batch 36/64 loss: 0.1272563934326172
Batch 37/64 loss: 0.16009026765823364
Batch 38/64 loss: 0.1447235345840454
Batch 39/64 loss: 0.14159876108169556
Batch 40/64 loss: 0.14872682094573975
Batch 41/64 loss: 0.11754804849624634
Batch 42/64 loss: 0.1211632490158081
Batch 43/64 loss: 0.15029144287109375
Batch 44/64 loss: 0.1121588945388794
Batch 45/64 loss: 0.12081605195999146
Batch 46/64 loss: 0.1267988085746765
Batch 47/64 loss: 0.14647692441940308
Batch 48/64 loss: 0.13979476690292358
Batch 49/64 loss: 0.14791756868362427
Batch 50/64 loss: 0.15462470054626465
Batch 51/64 loss: 0.12280666828155518
Batch 52/64 loss: 0.1338692307472229
Batch 53/64 loss: 0.14857172966003418
Batch 54/64 loss: 0.1461670994758606
Batch 55/64 loss: 0.14870405197143555
Batch 56/64 loss: 0.1447194218635559
Batch 57/64 loss: 0.12003839015960693
Batch 58/64 loss: 0.14979302883148193
Batch 59/64 loss: 0.1468290090560913
Batch 60/64 loss: 0.1542094349861145
Batch 61/64 loss: 0.12108516693115234
Batch 62/64 loss: 0.15089714527130127
Batch 63/64 loss: 0.16625070571899414
Batch 64/64 loss: 0.1481054425239563
Epoch 69  Train loss: 0.141058756089678  Val loss: 0.17175473052611465
Epoch 70
-------------------------------
Batch 1/64 loss: 0.16828036308288574
Batch 2/64 loss: 0.15309405326843262
Batch 3/64 loss: 0.13293522596359253
Batch 4/64 loss: 0.1359456181526184
Batch 5/64 loss: 0.14822763204574585
Batch 6/64 loss: 0.15761339664459229
Batch 7/64 loss: 0.1288076639175415
Batch 8/64 loss: 0.14067506790161133
Batch 9/64 loss: 0.14405429363250732
Batch 10/64 loss: 0.14397943019866943
Batch 11/64 loss: 0.15575534105300903
Batch 12/64 loss: 0.11624157428741455
Batch 13/64 loss: 0.1326196789741516
Batch 14/64 loss: 0.11831295490264893
Batch 15/64 loss: 0.11475461721420288
Batch 16/64 loss: 0.1291060447692871
Batch 17/64 loss: 0.1277921199798584
Batch 18/64 loss: 0.14794254302978516
Batch 19/64 loss: 0.16559159755706787
Batch 20/64 loss: 0.1333240270614624
Batch 21/64 loss: 0.13553452491760254
Batch 22/64 loss: 0.142489492893219
Batch 23/64 loss: 0.1414499282836914
Batch 24/64 loss: 0.13318073749542236
Batch 25/64 loss: 0.13545489311218262
Batch 26/64 loss: 0.14324963092803955
Batch 27/64 loss: 0.12072694301605225
Batch 28/64 loss: 0.12831395864486694
Batch 29/64 loss: 0.12266188859939575
Batch 30/64 loss: 0.1555793285369873
Batch 31/64 loss: 0.12427002191543579
Batch 32/64 loss: 0.13476061820983887
Batch 33/64 loss: 0.12802916765213013
Batch 34/64 loss: 0.13941723108291626
Batch 35/64 loss: 0.1261274218559265
Batch 36/64 loss: 0.12895286083221436
Batch 37/64 loss: 0.1388864517211914
Batch 38/64 loss: 0.12855267524719238
Batch 39/64 loss: 0.161310613155365
Batch 40/64 loss: 0.14505648612976074
Batch 41/64 loss: 0.1349465250968933
Batch 42/64 loss: 0.13830137252807617
Batch 43/64 loss: 0.13145750761032104
Batch 44/64 loss: 0.12071633338928223
Batch 45/64 loss: 0.16208213567733765
Batch 46/64 loss: 0.1364954113960266
Batch 47/64 loss: 0.13620525598526
Batch 48/64 loss: 0.1332411766052246
Batch 49/64 loss: 0.1261724829673767
Batch 50/64 loss: 0.16127562522888184
Batch 51/64 loss: 0.10736554861068726
Batch 52/64 loss: 0.1341266632080078
Batch 53/64 loss: 0.17274552583694458
Batch 54/64 loss: 0.1630144715309143
Batch 55/64 loss: 0.14072245359420776
Batch 56/64 loss: 0.14222711324691772
Batch 57/64 loss: 0.14441686868667603
Batch 58/64 loss: 0.12935298681259155
Batch 59/64 loss: 0.1462918519973755
Batch 60/64 loss: 0.15336161851882935
Batch 61/64 loss: 0.1470012664794922
Batch 62/64 loss: 0.1579122543334961
Batch 63/64 loss: 0.13447022438049316
Batch 64/64 loss: 0.14081978797912598
Epoch 70  Train loss: 0.13911491188348507  Val loss: 0.1634036919095672
Saving best model, epoch: 70
Epoch 71
-------------------------------
Batch 1/64 loss: 0.1462603211402893
Batch 2/64 loss: 0.12885302305221558
Batch 3/64 loss: 0.14186549186706543
Batch 4/64 loss: 0.13120436668395996
Batch 5/64 loss: 0.15209519863128662
Batch 6/64 loss: 0.15477973222732544
Batch 7/64 loss: 0.12874412536621094
Batch 8/64 loss: 0.12202012538909912
Batch 9/64 loss: 0.14482295513153076
Batch 10/64 loss: 0.1521027684211731
Batch 11/64 loss: 0.1575891375541687
Batch 12/64 loss: 0.13024777173995972
Batch 13/64 loss: 0.15425050258636475
Batch 14/64 loss: 0.16122859716415405
Batch 15/64 loss: 0.12521713972091675
Batch 16/64 loss: 0.10904425382614136
Batch 17/64 loss: 0.1473163366317749
Batch 18/64 loss: 0.11816686391830444
Batch 19/64 loss: 0.12779337167739868
Batch 20/64 loss: 0.12338930368423462
Batch 21/64 loss: 0.1489248275756836
Batch 22/64 loss: 0.1254625916481018
Batch 23/64 loss: 0.13314449787139893
Batch 24/64 loss: 0.12412035465240479
Batch 25/64 loss: 0.12782269716262817
Batch 26/64 loss: 0.17691361904144287
Batch 27/64 loss: 0.12117141485214233
Batch 28/64 loss: 0.12524151802062988
Batch 29/64 loss: 0.1385928988456726
Batch 30/64 loss: 0.13698798418045044
Batch 31/64 loss: 0.15398645401000977
Batch 32/64 loss: 0.12899649143218994
Batch 33/64 loss: 0.16690504550933838
Batch 34/64 loss: 0.14820736646652222
Batch 35/64 loss: 0.1322801113128662
Batch 36/64 loss: 0.1338498592376709
Batch 37/64 loss: 0.1409088373184204
Batch 38/64 loss: 0.1242799162864685
Batch 39/64 loss: 0.165449321269989
Batch 40/64 loss: 0.12234526872634888
Batch 41/64 loss: 0.12684619426727295
Batch 42/64 loss: 0.1281723976135254
Batch 43/64 loss: 0.16258645057678223
Batch 44/64 loss: 0.12995010614395142
Batch 45/64 loss: 0.1671099066734314
Batch 46/64 loss: 0.13383209705352783
Batch 47/64 loss: 0.120685875415802
Batch 48/64 loss: 0.1492089033126831
Batch 49/64 loss: 0.14987188577651978
Batch 50/64 loss: 0.14639675617218018
Batch 51/64 loss: 0.13321393728256226
Batch 52/64 loss: 0.11686050891876221
Batch 53/64 loss: 0.16097193956375122
Batch 54/64 loss: 0.12730759382247925
Batch 55/64 loss: 0.13769537210464478
Batch 56/64 loss: 0.11946642398834229
Batch 57/64 loss: 0.13465577363967896
Batch 58/64 loss: 0.13923758268356323
Batch 59/64 loss: 0.14115440845489502
Batch 60/64 loss: 0.14557409286499023
Batch 61/64 loss: 0.1581677794456482
Batch 62/64 loss: 0.12102073431015015
Batch 63/64 loss: 0.15152579545974731
Batch 64/64 loss: 0.15300917625427246
Epoch 71  Train loss: 0.13880551936579685  Val loss: 0.165923847980106
Epoch 72
-------------------------------
Batch 1/64 loss: 0.14579415321350098
Batch 2/64 loss: 0.15111517906188965
Batch 3/64 loss: 0.14953315258026123
Batch 4/64 loss: 0.13536524772644043
Batch 5/64 loss: 0.13371902704238892
Batch 6/64 loss: 0.11042970418930054
Batch 7/64 loss: 0.1482762098312378
Batch 8/64 loss: 0.13761907815933228
Batch 9/64 loss: 0.13453394174575806
Batch 10/64 loss: 0.12352228164672852
Batch 11/64 loss: 0.14982908964157104
Batch 12/64 loss: 0.14863324165344238
Batch 13/64 loss: 0.13484323024749756
Batch 14/64 loss: 0.14733409881591797
Batch 15/64 loss: 0.14054608345031738
Batch 16/64 loss: 0.15437817573547363
Batch 17/64 loss: 0.13590842485427856
Batch 18/64 loss: 0.1405143141746521
Batch 19/64 loss: 0.1351069211959839
Batch 20/64 loss: 0.12895315885543823
Batch 21/64 loss: 0.1455208659172058
Batch 22/64 loss: 0.12924349308013916
Batch 23/64 loss: 0.1553705930709839
Batch 24/64 loss: 0.14049559831619263
Batch 25/64 loss: 0.15528994798660278
Batch 26/64 loss: 0.1358579397201538
Batch 27/64 loss: 0.13403159379959106
Batch 28/64 loss: 0.12850064039230347
Batch 29/64 loss: 0.13058578968048096
Batch 30/64 loss: 0.15990471839904785
Batch 31/64 loss: 0.14317047595977783
Batch 32/64 loss: 0.16605603694915771
Batch 33/64 loss: 0.13899338245391846
Batch 34/64 loss: 0.12594175338745117
Batch 35/64 loss: 0.13819974660873413
Batch 36/64 loss: 0.1184854507446289
Batch 37/64 loss: 0.14639896154403687
Batch 38/64 loss: 0.12670212984085083
Batch 39/64 loss: 0.15159684419631958
Batch 40/64 loss: 0.14602625370025635
Batch 41/64 loss: 0.130423903465271
Batch 42/64 loss: 0.12162834405899048
Batch 43/64 loss: 0.13217264413833618
Batch 44/64 loss: 0.1542167067527771
Batch 45/64 loss: 0.12935501337051392
Batch 46/64 loss: 0.13219237327575684
Batch 47/64 loss: 0.1450759768486023
Batch 48/64 loss: 0.13270550966262817
Batch 49/64 loss: 0.14063900709152222
Batch 50/64 loss: 0.13813650608062744
Batch 51/64 loss: 0.13806843757629395
Batch 52/64 loss: 0.10749953985214233
Batch 53/64 loss: 0.13814735412597656
Batch 54/64 loss: 0.12906253337860107
Batch 55/64 loss: 0.13803881406784058
Batch 56/64 loss: 0.16088759899139404
Batch 57/64 loss: 0.13068372011184692
Batch 58/64 loss: 0.15525269508361816
Batch 59/64 loss: 0.1342950463294983
Batch 60/64 loss: 0.1274474859237671
Batch 61/64 loss: 0.13641518354415894
Batch 62/64 loss: 0.13669759035110474
Batch 63/64 loss: 0.14025843143463135
Batch 64/64 loss: 0.15244722366333008
Epoch 72  Train loss: 0.13876020020129634  Val loss: 0.16630435092342677
Epoch 73
-------------------------------
Batch 1/64 loss: 0.11990916728973389
Batch 2/64 loss: 0.12154889106750488
Batch 3/64 loss: 0.11412274837493896
Batch 4/64 loss: 0.15146124362945557
Batch 5/64 loss: 0.13589948415756226
Batch 6/64 loss: 0.14315563440322876
Batch 7/64 loss: 0.15475976467132568
Batch 8/64 loss: 0.11220568418502808
Batch 9/64 loss: 0.1494143009185791
Batch 10/64 loss: 0.14419931173324585
Batch 11/64 loss: 0.16045218706130981
Batch 12/64 loss: 0.12042009830474854
Batch 13/64 loss: 0.12427037954330444
Batch 14/64 loss: 0.1427893042564392
Batch 15/64 loss: 0.14173930883407593
Batch 16/64 loss: 0.1618381142616272
Batch 17/64 loss: 0.1379626989364624
Batch 18/64 loss: 0.1383047103881836
Batch 19/64 loss: 0.11035341024398804
Batch 20/64 loss: 0.1475352644920349
Batch 21/64 loss: 0.12600821256637573
Batch 22/64 loss: 0.1395794153213501
Batch 23/64 loss: 0.1359381079673767
Batch 24/64 loss: 0.11466658115386963
Batch 25/64 loss: 0.13294470310211182
Batch 26/64 loss: 0.13483721017837524
Batch 27/64 loss: 0.13428783416748047
Batch 28/64 loss: 0.12722373008728027
Batch 29/64 loss: 0.12280279397964478
Batch 30/64 loss: 0.13436901569366455
Batch 31/64 loss: 0.11357754468917847
Batch 32/64 loss: 0.13205409049987793
Batch 33/64 loss: 0.13768696784973145
Batch 34/64 loss: 0.1502959132194519
Batch 35/64 loss: 0.13168752193450928
Batch 36/64 loss: 0.11379337310791016
Batch 37/64 loss: 0.17663919925689697
Batch 38/64 loss: 0.12135112285614014
Batch 39/64 loss: 0.14203917980194092
Batch 40/64 loss: 0.1510879397392273
Batch 41/64 loss: 0.12469679117202759
Batch 42/64 loss: 0.14466756582260132
Batch 43/64 loss: 0.1488257646560669
Batch 44/64 loss: 0.1360052227973938
Batch 45/64 loss: 0.14655625820159912
Batch 46/64 loss: 0.16656672954559326
Batch 47/64 loss: 0.15031349658966064
Batch 48/64 loss: 0.13748377561569214
Batch 49/64 loss: 0.127064049243927
Batch 50/64 loss: 0.15974760055541992
Batch 51/64 loss: 0.14829367399215698
Batch 52/64 loss: 0.147269606590271
Batch 53/64 loss: 0.13168668746948242
Batch 54/64 loss: 0.14000141620635986
Batch 55/64 loss: 0.14100855588912964
Batch 56/64 loss: 0.12310469150543213
Batch 57/64 loss: 0.1438770890235901
Batch 58/64 loss: 0.13721758127212524
Batch 59/64 loss: 0.12871599197387695
Batch 60/64 loss: 0.11864447593688965
Batch 61/64 loss: 0.1279115080833435
Batch 62/64 loss: 0.1539958119392395
Batch 63/64 loss: 0.14190292358398438
Batch 64/64 loss: 0.13618922233581543
Epoch 73  Train loss: 0.13698684467988856  Val loss: 0.16291813137605018
Saving best model, epoch: 73
Epoch 74
-------------------------------
Batch 1/64 loss: 0.11517643928527832
Batch 2/64 loss: 0.1360740065574646
Batch 3/64 loss: 0.12164300680160522
Batch 4/64 loss: 0.11183083057403564
Batch 5/64 loss: 0.13899898529052734
Batch 6/64 loss: 0.10723888874053955
Batch 7/64 loss: 0.13474351167678833
Batch 8/64 loss: 0.13519614934921265
Batch 9/64 loss: 0.1428130865097046
Batch 10/64 loss: 0.12603533267974854
Batch 11/64 loss: 0.15064847469329834
Batch 12/64 loss: 0.14391624927520752
Batch 13/64 loss: 0.16119897365570068
Batch 14/64 loss: 0.14024001359939575
Batch 15/64 loss: 0.1396588683128357
Batch 16/64 loss: 0.155728280544281
Batch 17/64 loss: 0.13275235891342163
Batch 18/64 loss: 0.11884814500808716
Batch 19/64 loss: 0.13225972652435303
Batch 20/64 loss: 0.13093847036361694
Batch 21/64 loss: 0.1063394546508789
Batch 22/64 loss: 0.1496613621711731
Batch 23/64 loss: 0.14032649993896484
Batch 24/64 loss: 0.16914242506027222
Batch 25/64 loss: 0.13121020793914795
Batch 26/64 loss: 0.13360977172851562
Batch 27/64 loss: 0.1518605351448059
Batch 28/64 loss: 0.13768064975738525
Batch 29/64 loss: 0.13814973831176758
Batch 30/64 loss: 0.15494322776794434
Batch 31/64 loss: 0.13132083415985107
Batch 32/64 loss: 0.14067411422729492
Batch 33/64 loss: 0.12499582767486572
Batch 34/64 loss: 0.13366073369979858
Batch 35/64 loss: 0.15288329124450684
Batch 36/64 loss: 0.13740581274032593
Batch 37/64 loss: 0.1345006823539734
Batch 38/64 loss: 0.1544833779335022
Batch 39/64 loss: 0.1274031400680542
Batch 40/64 loss: 0.18895769119262695
Batch 41/64 loss: 0.13791662454605103
Batch 42/64 loss: 0.12481474876403809
Batch 43/64 loss: 0.14179909229278564
Batch 44/64 loss: 0.1352217197418213
Batch 45/64 loss: 0.13352811336517334
Batch 46/64 loss: 0.13565748929977417
Batch 47/64 loss: 0.13714241981506348
Batch 48/64 loss: 0.14082098007202148
Batch 49/64 loss: 0.13926303386688232
Batch 50/64 loss: 0.14053553342819214
Batch 51/64 loss: 0.11320894956588745
Batch 52/64 loss: 0.11468100547790527
Batch 53/64 loss: 0.1325250267982483
Batch 54/64 loss: 0.1427859663963318
Batch 55/64 loss: 0.1303774118423462
Batch 56/64 loss: 0.15312683582305908
Batch 57/64 loss: 0.14240050315856934
Batch 58/64 loss: 0.12302792072296143
Batch 59/64 loss: 0.10957711935043335
Batch 60/64 loss: 0.14250677824020386
Batch 61/64 loss: 0.13734924793243408
Batch 62/64 loss: 0.15136098861694336
Batch 63/64 loss: 0.16492635011672974
Batch 64/64 loss: 0.14625269174575806
Epoch 74  Train loss: 0.13724537339864992  Val loss: 0.16253031590550215
Saving best model, epoch: 74
Epoch 75
-------------------------------
Batch 1/64 loss: 0.13089603185653687
Batch 2/64 loss: 0.1415444016456604
Batch 3/64 loss: 0.1398864984512329
Batch 4/64 loss: 0.1573472023010254
Batch 5/64 loss: 0.1431390643119812
Batch 6/64 loss: 0.16463476419448853
Batch 7/64 loss: 0.10719430446624756
Batch 8/64 loss: 0.15357095003128052
Batch 9/64 loss: 0.1255970597267151
Batch 10/64 loss: 0.1319860816001892
Batch 11/64 loss: 0.1111181378364563
Batch 12/64 loss: 0.13987624645233154
Batch 13/64 loss: 0.10482430458068848
Batch 14/64 loss: 0.11944258213043213
Batch 15/64 loss: 0.1531238555908203
Batch 16/64 loss: 0.14226561784744263
Batch 17/64 loss: 0.15836644172668457
Batch 18/64 loss: 0.11421918869018555
Batch 19/64 loss: 0.12803810834884644
Batch 20/64 loss: 0.12826788425445557
Batch 21/64 loss: 0.12574195861816406
Batch 22/64 loss: 0.156171977519989
Batch 23/64 loss: 0.148759126663208
Batch 24/64 loss: 0.13322943449020386
Batch 25/64 loss: 0.1385636329650879
Batch 26/64 loss: 0.1312723159790039
Batch 27/64 loss: 0.14637362957000732
Batch 28/64 loss: 0.1233476996421814
Batch 29/64 loss: 0.11754059791564941
Batch 30/64 loss: 0.14469188451766968
Batch 31/64 loss: 0.1346527338027954
Batch 32/64 loss: 0.1438617706298828
Batch 33/64 loss: 0.12809360027313232
Batch 34/64 loss: 0.12793302536010742
Batch 35/64 loss: 0.1251543164253235
Batch 36/64 loss: 0.16028320789337158
Batch 37/64 loss: 0.13976317644119263
Batch 38/64 loss: 0.12482339143753052
Batch 39/64 loss: 0.1496347188949585
Batch 40/64 loss: 0.1425357460975647
Batch 41/64 loss: 0.09884989261627197
Batch 42/64 loss: 0.1498962640762329
Batch 43/64 loss: 0.1275036334991455
Batch 44/64 loss: 0.13080954551696777
Batch 45/64 loss: 0.12753665447235107
Batch 46/64 loss: 0.1410665512084961
Batch 47/64 loss: 0.12731397151947021
Batch 48/64 loss: 0.15227824449539185
Batch 49/64 loss: 0.1337449550628662
Batch 50/64 loss: 0.13960981369018555
Batch 51/64 loss: 0.12986129522323608
Batch 52/64 loss: 0.14892244338989258
Batch 53/64 loss: 0.11793243885040283
Batch 54/64 loss: 0.11399489641189575
Batch 55/64 loss: 0.1428733468055725
Batch 56/64 loss: 0.1220102310180664
Batch 57/64 loss: 0.1588345766067505
Batch 58/64 loss: 0.163408100605011
Batch 59/64 loss: 0.12665534019470215
Batch 60/64 loss: 0.1378331184387207
Batch 61/64 loss: 0.14694976806640625
Batch 62/64 loss: 0.14430952072143555
Batch 63/64 loss: 0.1286565065383911
Batch 64/64 loss: 0.15612322092056274
Epoch 75  Train loss: 0.1359327089552786  Val loss: 0.16484457336340574
Epoch 76
-------------------------------
Batch 1/64 loss: 0.11691862344741821
Batch 2/64 loss: 0.13578712940216064
Batch 3/64 loss: 0.14760076999664307
Batch 4/64 loss: 0.13106191158294678
Batch 5/64 loss: 0.11000293493270874
Batch 6/64 loss: 0.12914198637008667
Batch 7/64 loss: 0.13462913036346436
Batch 8/64 loss: 0.11911839246749878
Batch 9/64 loss: 0.13174700736999512
Batch 10/64 loss: 0.13548201322555542
Batch 11/64 loss: 0.1377716064453125
Batch 12/64 loss: 0.1474897265434265
Batch 13/64 loss: 0.1503855586051941
Batch 14/64 loss: 0.12516850233078003
Batch 15/64 loss: 0.11868166923522949
Batch 16/64 loss: 0.15924710035324097
Batch 17/64 loss: 0.1380603313446045
Batch 18/64 loss: 0.14679735898971558
Batch 19/64 loss: 0.11139416694641113
Batch 20/64 loss: 0.14829742908477783
Batch 21/64 loss: 0.1504150629043579
Batch 22/64 loss: 0.12804663181304932
Batch 23/64 loss: 0.12438350915908813
Batch 24/64 loss: 0.13347220420837402
Batch 25/64 loss: 0.13304293155670166
Batch 26/64 loss: 0.15625137090682983
Batch 27/64 loss: 0.13372480869293213
Batch 28/64 loss: 0.13841313123703003
Batch 29/64 loss: 0.1330786943435669
Batch 30/64 loss: 0.15182697772979736
Batch 31/64 loss: 0.1423899531364441
Batch 32/64 loss: 0.13415312767028809
Batch 33/64 loss: 0.13803160190582275
Batch 34/64 loss: 0.14116549491882324
Batch 35/64 loss: 0.13164949417114258
Batch 36/64 loss: 0.14976906776428223
Batch 37/64 loss: 0.13029831647872925
Batch 38/64 loss: 0.1531350016593933
Batch 39/64 loss: 0.12033677101135254
Batch 40/64 loss: 0.13172590732574463
Batch 41/64 loss: 0.13777202367782593
Batch 42/64 loss: 0.15990257263183594
Batch 43/64 loss: 0.12512356042861938
Batch 44/64 loss: 0.12857586145401
Batch 45/64 loss: 0.1304084062576294
Batch 46/64 loss: 0.136477530002594
Batch 47/64 loss: 0.14142262935638428
Batch 48/64 loss: 0.1448538899421692
Batch 49/64 loss: 0.1403140425682068
Batch 50/64 loss: 0.1366945505142212
Batch 51/64 loss: 0.13795387744903564
Batch 52/64 loss: 0.13946068286895752
Batch 53/64 loss: 0.15690523386001587
Batch 54/64 loss: 0.13094472885131836
Batch 55/64 loss: 0.1454761028289795
Batch 56/64 loss: 0.12006598711013794
Batch 57/64 loss: 0.1420457363128662
Batch 58/64 loss: 0.12111115455627441
Batch 59/64 loss: 0.13842087984085083
Batch 60/64 loss: 0.11991685628890991
Batch 61/64 loss: 0.14966702461242676
Batch 62/64 loss: 0.11537498235702515
Batch 63/64 loss: 0.14540040493011475
Batch 64/64 loss: 0.13405269384384155
Epoch 76  Train loss: 0.13607720230139939  Val loss: 0.15838427703405164
Saving best model, epoch: 76
Epoch 77
-------------------------------
Batch 1/64 loss: 0.11845928430557251
Batch 2/64 loss: 0.11655116081237793
Batch 3/64 loss: 0.14624786376953125
Batch 4/64 loss: 0.14539027214050293
Batch 5/64 loss: 0.15233731269836426
Batch 6/64 loss: 0.1730979084968567
Batch 7/64 loss: 0.15041792392730713
Batch 8/64 loss: 0.14129531383514404
Batch 9/64 loss: 0.12703663110733032
Batch 10/64 loss: 0.10753041505813599
Batch 11/64 loss: 0.12862610816955566
Batch 12/64 loss: 0.13531172275543213
Batch 13/64 loss: 0.14369654655456543
Batch 14/64 loss: 0.1393844485282898
Batch 15/64 loss: 0.11842602491378784
Batch 16/64 loss: 0.14012670516967773
Batch 17/64 loss: 0.12438058853149414
Batch 18/64 loss: 0.12661904096603394
Batch 19/64 loss: 0.13453781604766846
Batch 20/64 loss: 0.1389169692993164
Batch 21/64 loss: 0.14969086647033691
Batch 22/64 loss: 0.1398647427558899
Batch 23/64 loss: 0.12224489450454712
Batch 24/64 loss: 0.1254425048828125
Batch 25/64 loss: 0.12453967332839966
Batch 26/64 loss: 0.11186575889587402
Batch 27/64 loss: 0.13397008180618286
Batch 28/64 loss: 0.13172632455825806
Batch 29/64 loss: 0.12161886692047119
Batch 30/64 loss: 0.13425302505493164
Batch 31/64 loss: 0.13560479879379272
Batch 32/64 loss: 0.12589430809020996
Batch 33/64 loss: 0.1529308557510376
Batch 34/64 loss: 0.12888473272323608
Batch 35/64 loss: 0.1369483470916748
Batch 36/64 loss: 0.1474984884262085
Batch 37/64 loss: 0.13455450534820557
Batch 38/64 loss: 0.13684958219528198
Batch 39/64 loss: 0.12541913986206055
Batch 40/64 loss: 0.1246914267539978
Batch 41/64 loss: 0.13417917490005493
Batch 42/64 loss: 0.16022789478302002
Batch 43/64 loss: 0.1366214156150818
Batch 44/64 loss: 0.13961482048034668
Batch 45/64 loss: 0.12912756204605103
Batch 46/64 loss: 0.1265220046043396
Batch 47/64 loss: 0.13597458600997925
Batch 48/64 loss: 0.1391107439994812
Batch 49/64 loss: 0.14097100496292114
Batch 50/64 loss: 0.13984918594360352
Batch 51/64 loss: 0.13010460138320923
Batch 52/64 loss: 0.14994734525680542
Batch 53/64 loss: 0.1221076250076294
Batch 54/64 loss: 0.11804133653640747
Batch 55/64 loss: 0.13950681686401367
Batch 56/64 loss: 0.1276235580444336
Batch 57/64 loss: 0.12546563148498535
Batch 58/64 loss: 0.14907801151275635
Batch 59/64 loss: 0.14685124158859253
Batch 60/64 loss: 0.11184132099151611
Batch 61/64 loss: 0.1345595121383667
Batch 62/64 loss: 0.13938915729522705
Batch 63/64 loss: 0.12073493003845215
Batch 64/64 loss: 0.16368532180786133
Epoch 77  Train loss: 0.13447994437872196  Val loss: 0.157317369254594
Saving best model, epoch: 77
Epoch 78
-------------------------------
Batch 1/64 loss: 0.13354772329330444
Batch 2/64 loss: 0.12336516380310059
Batch 3/64 loss: 0.11939001083374023
Batch 4/64 loss: 0.1511690616607666
Batch 5/64 loss: 0.11043232679367065
Batch 6/64 loss: 0.12619584798812866
Batch 7/64 loss: 0.11398273706436157
Batch 8/64 loss: 0.12491291761398315
Batch 9/64 loss: 0.13361382484436035
Batch 10/64 loss: 0.13635289669036865
Batch 11/64 loss: 0.13672351837158203
Batch 12/64 loss: 0.13778215646743774
Batch 13/64 loss: 0.13217943906784058
Batch 14/64 loss: 0.1438390612602234
Batch 15/64 loss: 0.12046575546264648
Batch 16/64 loss: 0.12285423278808594
Batch 17/64 loss: 0.11767113208770752
Batch 18/64 loss: 0.12643074989318848
Batch 19/64 loss: 0.11646407842636108
Batch 20/64 loss: 0.10792803764343262
Batch 21/64 loss: 0.14671456813812256
Batch 22/64 loss: 0.14628386497497559
Batch 23/64 loss: 0.12311816215515137
Batch 24/64 loss: 0.14248037338256836
Batch 25/64 loss: 0.17416614294052124
Batch 26/64 loss: 0.14300471544265747
Batch 27/64 loss: 0.14327669143676758
Batch 28/64 loss: 0.1234135627746582
Batch 29/64 loss: 0.16178560256958008
Batch 30/64 loss: 0.14482486248016357
Batch 31/64 loss: 0.13642120361328125
Batch 32/64 loss: 0.11775660514831543
Batch 33/64 loss: 0.14198261499404907
Batch 34/64 loss: 0.1519472599029541
Batch 35/64 loss: 0.11614251136779785
Batch 36/64 loss: 0.14273756742477417
Batch 37/64 loss: 0.12649333477020264
Batch 38/64 loss: 0.14644086360931396
Batch 39/64 loss: 0.12096047401428223
Batch 40/64 loss: 0.14418578147888184
Batch 41/64 loss: 0.1192355751991272
Batch 42/64 loss: 0.13475561141967773
Batch 43/64 loss: 0.15073704719543457
Batch 44/64 loss: 0.13317549228668213
Batch 45/64 loss: 0.13337016105651855
Batch 46/64 loss: 0.1319752335548401
Batch 47/64 loss: 0.12365424633026123
Batch 48/64 loss: 0.1195523738861084
Batch 49/64 loss: 0.14066749811172485
Batch 50/64 loss: 0.11614537239074707
Batch 51/64 loss: 0.13716816902160645
Batch 52/64 loss: 0.14295810461044312
Batch 53/64 loss: 0.16050785779953003
Batch 54/64 loss: 0.15555500984191895
Batch 55/64 loss: 0.11318141222000122
Batch 56/64 loss: 0.10378754138946533
Batch 57/64 loss: 0.1145787239074707
Batch 58/64 loss: 0.14377182722091675
Batch 59/64 loss: 0.131769061088562
Batch 60/64 loss: 0.12866514921188354
Batch 61/64 loss: 0.15186339616775513
Batch 62/64 loss: 0.15005481243133545
Batch 63/64 loss: 0.14293056726455688
Batch 64/64 loss: 0.1323103904724121
Epoch 78  Train loss: 0.1334702819001441  Val loss: 0.16000432144735277
Epoch 79
-------------------------------
Batch 1/64 loss: 0.1249232292175293
Batch 2/64 loss: 0.11485302448272705
Batch 3/64 loss: 0.13255012035369873
Batch 4/64 loss: 0.1627286672592163
Batch 5/64 loss: 0.1187865138053894
Batch 6/64 loss: 0.13362252712249756
Batch 7/64 loss: 0.14427554607391357
Batch 8/64 loss: 0.13739430904388428
Batch 9/64 loss: 0.14436089992523193
Batch 10/64 loss: 0.1344844102859497
Batch 11/64 loss: 0.13086402416229248
Batch 12/64 loss: 0.10969901084899902
Batch 13/64 loss: 0.13143563270568848
Batch 14/64 loss: 0.11072278022766113
Batch 15/64 loss: 0.12240374088287354
Batch 16/64 loss: 0.16331279277801514
Batch 17/64 loss: 0.12797445058822632
Batch 18/64 loss: 0.11373436450958252
Batch 19/64 loss: 0.11694788932800293
Batch 20/64 loss: 0.13080507516860962
Batch 21/64 loss: 0.10845571756362915
Batch 22/64 loss: 0.12183070182800293
Batch 23/64 loss: 0.1092606782913208
Batch 24/64 loss: 0.12309485673904419
Batch 25/64 loss: 0.178450345993042
Batch 26/64 loss: 0.12685024738311768
Batch 27/64 loss: 0.14496058225631714
Batch 28/64 loss: 0.1076650619506836
Batch 29/64 loss: 0.12405461072921753
Batch 30/64 loss: 0.11912256479263306
Batch 31/64 loss: 0.1416991949081421
Batch 32/64 loss: 0.14485841989517212
Batch 33/64 loss: 0.13926661014556885
Batch 34/64 loss: 0.1433572769165039
Batch 35/64 loss: 0.12499469518661499
Batch 36/64 loss: 0.1380908489227295
Batch 37/64 loss: 0.1337578296661377
Batch 38/64 loss: 0.12214988470077515
Batch 39/64 loss: 0.14058935642242432
Batch 40/64 loss: 0.13615500926971436
Batch 41/64 loss: 0.13935232162475586
Batch 42/64 loss: 0.12416696548461914
Batch 43/64 loss: 0.14963442087173462
Batch 44/64 loss: 0.11977118253707886
Batch 45/64 loss: 0.13577359914779663
Batch 46/64 loss: 0.1184229850769043
Batch 47/64 loss: 0.14552748203277588
Batch 48/64 loss: 0.1440463662147522
Batch 49/64 loss: 0.12812012434005737
Batch 50/64 loss: 0.15115821361541748
Batch 51/64 loss: 0.11861574649810791
Batch 52/64 loss: 0.12980347871780396
Batch 53/64 loss: 0.13526296615600586
Batch 54/64 loss: 0.12319803237915039
Batch 55/64 loss: 0.12232321500778198
Batch 56/64 loss: 0.13892388343811035
Batch 57/64 loss: 0.13643604516983032
Batch 58/64 loss: 0.11412805318832397
Batch 59/64 loss: 0.1360345482826233
Batch 60/64 loss: 0.12613749504089355
Batch 61/64 loss: 0.12324750423431396
Batch 62/64 loss: 0.14133590459823608
Batch 63/64 loss: 0.13784116506576538
Batch 64/64 loss: 0.1908586025238037
Epoch 79  Train loss: 0.13203057494817996  Val loss: 0.1582043838664838
Epoch 80
-------------------------------
Batch 1/64 loss: 0.12366777658462524
Batch 2/64 loss: 0.10977798700332642
Batch 3/64 loss: 0.13208359479904175
Batch 4/64 loss: 0.1303495168685913
Batch 5/64 loss: 0.12103313207626343
Batch 6/64 loss: 0.12986475229263306
Batch 7/64 loss: 0.1456996202468872
Batch 8/64 loss: 0.12580060958862305
Batch 9/64 loss: 0.12591052055358887
Batch 10/64 loss: 0.12918102741241455
Batch 11/64 loss: 0.13708913326263428
Batch 12/64 loss: 0.14362561702728271
Batch 13/64 loss: 0.13001054525375366
Batch 14/64 loss: 0.11742544174194336
Batch 15/64 loss: 0.13193070888519287
Batch 16/64 loss: 0.11185353994369507
Batch 17/64 loss: 0.12714838981628418
Batch 18/64 loss: 0.13505810499191284
Batch 19/64 loss: 0.12961602210998535
Batch 20/64 loss: 0.14527040719985962
Batch 21/64 loss: 0.1575060486793518
Batch 22/64 loss: 0.16488301753997803
Batch 23/64 loss: 0.14094799757003784
Batch 24/64 loss: 0.11788523197174072
Batch 25/64 loss: 0.14529907703399658
Batch 26/64 loss: 0.1324743628501892
Batch 27/64 loss: 0.14230287075042725
Batch 28/64 loss: 0.1305861473083496
Batch 29/64 loss: 0.1197240948677063
Batch 30/64 loss: 0.12311244010925293
Batch 31/64 loss: 0.129624605178833
Batch 32/64 loss: 0.13453435897827148
Batch 33/64 loss: 0.1387310028076172
Batch 34/64 loss: 0.13433974981307983
Batch 35/64 loss: 0.12693452835083008
Batch 36/64 loss: 0.15565186738967896
Batch 37/64 loss: 0.12397372722625732
Batch 38/64 loss: 0.1485893726348877
Batch 39/64 loss: 0.14541012048721313
Batch 40/64 loss: 0.15693581104278564
Batch 41/64 loss: 0.1259879469871521
Batch 42/64 loss: 0.13001185655593872
Batch 43/64 loss: 0.13893187046051025
Batch 44/64 loss: 0.1185295581817627
Batch 45/64 loss: 0.13192600011825562
Batch 46/64 loss: 0.1114913821220398
Batch 47/64 loss: 0.13225090503692627
Batch 48/64 loss: 0.10631722211837769
Batch 49/64 loss: 0.14281678199768066
Batch 50/64 loss: 0.13078433275222778
Batch 51/64 loss: 0.1284579038619995
Batch 52/64 loss: 0.11381101608276367
Batch 53/64 loss: 0.1404339075088501
Batch 54/64 loss: 0.11996251344680786
Batch 55/64 loss: 0.1250050663948059
Batch 56/64 loss: 0.11078417301177979
Batch 57/64 loss: 0.12949001789093018
Batch 58/64 loss: 0.11424911022186279
Batch 59/64 loss: 0.11880326271057129
Batch 60/64 loss: 0.16070473194122314
Batch 61/64 loss: 0.13940811157226562
Batch 62/64 loss: 0.14536184072494507
Batch 63/64 loss: 0.1216168999671936
Batch 64/64 loss: 0.13670533895492554
Epoch 80  Train loss: 0.13163150268442492  Val loss: 0.1596182937064941
Epoch 81
-------------------------------
Batch 1/64 loss: 0.12874817848205566
Batch 2/64 loss: 0.1484730839729309
Batch 3/64 loss: 0.12428587675094604
Batch 4/64 loss: 0.11876869201660156
Batch 5/64 loss: 0.1420961618423462
Batch 6/64 loss: 0.14268338680267334
Batch 7/64 loss: 0.12296366691589355
Batch 8/64 loss: 0.12947583198547363
Batch 9/64 loss: 0.13000303506851196
Batch 10/64 loss: 0.11965739727020264
Batch 11/64 loss: 0.14233040809631348
Batch 12/64 loss: 0.1255212426185608
Batch 13/64 loss: 0.10974812507629395
Batch 14/64 loss: 0.12724846601486206
Batch 15/64 loss: 0.1590084433555603
Batch 16/64 loss: 0.11569356918334961
Batch 17/64 loss: 0.12186068296432495
Batch 18/64 loss: 0.10655343532562256
Batch 19/64 loss: 0.13811564445495605
Batch 20/64 loss: 0.1323375105857849
Batch 21/64 loss: 0.13774311542510986
Batch 22/64 loss: 0.1350756287574768
Batch 23/64 loss: 0.1513892412185669
Batch 24/64 loss: 0.12617599964141846
Batch 25/64 loss: 0.14514440298080444
Batch 26/64 loss: 0.1340632438659668
Batch 27/64 loss: 0.10584616661071777
Batch 28/64 loss: 0.13086968660354614
Batch 29/64 loss: 0.12485671043395996
Batch 30/64 loss: 0.12748366594314575
Batch 31/64 loss: 0.1227269172668457
Batch 32/64 loss: 0.14565378427505493
Batch 33/64 loss: 0.13862138986587524
Batch 34/64 loss: 0.13202917575836182
Batch 35/64 loss: 0.12013256549835205
Batch 36/64 loss: 0.11612582206726074
Batch 37/64 loss: 0.12460792064666748
Batch 38/64 loss: 0.13803023099899292
Batch 39/64 loss: 0.11429530382156372
Batch 40/64 loss: 0.14278310537338257
Batch 41/64 loss: 0.15105342864990234
Batch 42/64 loss: 0.12342476844787598
Batch 43/64 loss: 0.1370532512664795
Batch 44/64 loss: 0.14152288436889648
Batch 45/64 loss: 0.12865233421325684
Batch 46/64 loss: 0.14517343044281006
Batch 47/64 loss: 0.11279541254043579
Batch 48/64 loss: 0.14737939834594727
Batch 49/64 loss: 0.16146337985992432
Batch 50/64 loss: 0.13181841373443604
Batch 51/64 loss: 0.12349450588226318
Batch 52/64 loss: 0.1451370120048523
Batch 53/64 loss: 0.1443638801574707
Batch 54/64 loss: 0.1311204433441162
Batch 55/64 loss: 0.12699580192565918
Batch 56/64 loss: 0.12318217754364014
Batch 57/64 loss: 0.11386233568191528
Batch 58/64 loss: 0.12179732322692871
Batch 59/64 loss: 0.12089461088180542
Batch 60/64 loss: 0.13051247596740723
Batch 61/64 loss: 0.09762859344482422
Batch 62/64 loss: 0.11854016780853271
Batch 63/64 loss: 0.12543267011642456
Batch 64/64 loss: 0.09396958351135254
Epoch 81  Train loss: 0.12977259299334357  Val loss: 0.15858576264987698
Epoch 82
-------------------------------
Batch 1/64 loss: 0.17567938566207886
Batch 2/64 loss: 0.11844736337661743
Batch 3/64 loss: 0.11659371852874756
Batch 4/64 loss: 0.1237366795539856
Batch 5/64 loss: 0.1294313669204712
Batch 6/64 loss: 0.14189976453781128
Batch 7/64 loss: 0.12041586637496948
Batch 8/64 loss: 0.1349659562110901
Batch 9/64 loss: 0.12425994873046875
Batch 10/64 loss: 0.1205253005027771
Batch 11/64 loss: 0.09639251232147217
Batch 12/64 loss: 0.11571550369262695
Batch 13/64 loss: 0.14664208889007568
Batch 14/64 loss: 0.12444782257080078
Batch 15/64 loss: 0.12253177165985107
Batch 16/64 loss: 0.14777225255966187
Batch 17/64 loss: 0.11299657821655273
Batch 18/64 loss: 0.12275749444961548
Batch 19/64 loss: 0.10537219047546387
Batch 20/64 loss: 0.11906814575195312
Batch 21/64 loss: 0.1265454888343811
Batch 22/64 loss: 0.11486399173736572
Batch 23/64 loss: 0.14148616790771484
Batch 24/64 loss: 0.11973029375076294
Batch 25/64 loss: 0.12321168184280396
Batch 26/64 loss: 0.13671988248825073
Batch 27/64 loss: 0.12760233879089355
Batch 28/64 loss: 0.15450453758239746
Batch 29/64 loss: 0.10010784864425659
Batch 30/64 loss: 0.14172768592834473
Batch 31/64 loss: 0.16680723428726196
Batch 32/64 loss: 0.12509560585021973
Batch 33/64 loss: 0.12309575080871582
Batch 34/64 loss: 0.12576454877853394
Batch 35/64 loss: 0.10887497663497925
Batch 36/64 loss: 0.12680339813232422
Batch 37/64 loss: 0.14169448614120483
Batch 38/64 loss: 0.12487763166427612
Batch 39/64 loss: 0.13974988460540771
Batch 40/64 loss: 0.1305180788040161
Batch 41/64 loss: 0.12902909517288208
Batch 42/64 loss: 0.12792104482650757
Batch 43/64 loss: 0.15217208862304688
Batch 44/64 loss: 0.15384453535079956
Batch 45/64 loss: 0.15014666318893433
Batch 46/64 loss: 0.11517453193664551
Batch 47/64 loss: 0.12054985761642456
Batch 48/64 loss: 0.13605964183807373
Batch 49/64 loss: 0.12622475624084473
Batch 50/64 loss: 0.1292785406112671
Batch 51/64 loss: 0.12993746995925903
Batch 52/64 loss: 0.1325179934501648
Batch 53/64 loss: 0.12618017196655273
Batch 54/64 loss: 0.11548024415969849
Batch 55/64 loss: 0.11963695287704468
Batch 56/64 loss: 0.13881897926330566
Batch 57/64 loss: 0.10620957612991333
Batch 58/64 loss: 0.15102684497833252
Batch 59/64 loss: 0.09605276584625244
Batch 60/64 loss: 0.13485264778137207
Batch 61/64 loss: 0.14839452505111694
Batch 62/64 loss: 0.14166098833084106
Batch 63/64 loss: 0.13093578815460205
Batch 64/64 loss: 0.14900213479995728
Epoch 82  Train loss: 0.12930651809654983  Val loss: 0.1553361782503292
Saving best model, epoch: 82
Epoch 83
-------------------------------
Batch 1/64 loss: 0.11151361465454102
Batch 2/64 loss: 0.12404036521911621
Batch 3/64 loss: 0.1301007866859436
Batch 4/64 loss: 0.1294533610343933
Batch 5/64 loss: 0.161920964717865
Batch 6/64 loss: 0.11739712953567505
Batch 7/64 loss: 0.13991069793701172
Batch 8/64 loss: 0.1288844347000122
Batch 9/64 loss: 0.12234228849411011
Batch 10/64 loss: 0.12011110782623291
Batch 11/64 loss: 0.12321102619171143
Batch 12/64 loss: 0.1452488899230957
Batch 13/64 loss: 0.11318612098693848
Batch 14/64 loss: 0.1384824514389038
Batch 15/64 loss: 0.11315727233886719
Batch 16/64 loss: 0.10769844055175781
Batch 17/64 loss: 0.11619001626968384
Batch 18/64 loss: 0.1268543004989624
Batch 19/64 loss: 0.1329018473625183
Batch 20/64 loss: 0.1380605697631836
Batch 21/64 loss: 0.14381062984466553
Batch 22/64 loss: 0.11712563037872314
Batch 23/64 loss: 0.10577791929244995
Batch 24/64 loss: 0.13766658306121826
Batch 25/64 loss: 0.12783384323120117
Batch 26/64 loss: 0.11496865749359131
Batch 27/64 loss: 0.14934903383255005
Batch 28/64 loss: 0.13045954704284668
Batch 29/64 loss: 0.11504858732223511
Batch 30/64 loss: 0.14380812644958496
Batch 31/64 loss: 0.13200616836547852
Batch 32/64 loss: 0.13449370861053467
Batch 33/64 loss: 0.15055447816848755
Batch 34/64 loss: 0.10678070783615112
Batch 35/64 loss: 0.10130316019058228
Batch 36/64 loss: 0.13620078563690186
Batch 37/64 loss: 0.14114797115325928
Batch 38/64 loss: 0.1227337121963501
Batch 39/64 loss: 0.12688255310058594
Batch 40/64 loss: 0.1422959566116333
Batch 41/64 loss: 0.07266473770141602
Batch 42/64 loss: 0.1273518204689026
Batch 43/64 loss: 0.12500262260437012
Batch 44/64 loss: 0.10634386539459229
Batch 45/64 loss: 0.13211095333099365
Batch 46/64 loss: 0.1259021759033203
Batch 47/64 loss: 0.12268054485321045
Batch 48/64 loss: 0.11753720045089722
Batch 49/64 loss: 0.14599394798278809
Batch 50/64 loss: 0.12502682209014893
Batch 51/64 loss: 0.17234283685684204
Batch 52/64 loss: 0.12875372171401978
Batch 53/64 loss: 0.1270783543586731
Batch 54/64 loss: 0.13198494911193848
Batch 55/64 loss: 0.10928672552108765
Batch 56/64 loss: 0.1383650302886963
Batch 57/64 loss: 0.1497507095336914
Batch 58/64 loss: 0.15618044137954712
Batch 59/64 loss: 0.1379697322845459
Batch 60/64 loss: 0.11757588386535645
Batch 61/64 loss: 0.133134663105011
Batch 62/64 loss: 0.16398698091506958
Batch 63/64 loss: 0.1363919973373413
Batch 64/64 loss: 0.15841341018676758
Epoch 83  Train loss: 0.1293041603237975  Val loss: 0.15859650869140102
Epoch 84
-------------------------------
Batch 1/64 loss: 0.15093159675598145
Batch 2/64 loss: 0.14028698205947876
Batch 3/64 loss: 0.14233815670013428
Batch 4/64 loss: 0.13262540102005005
Batch 5/64 loss: 0.12863552570343018
Batch 6/64 loss: 0.1277260184288025
Batch 7/64 loss: 0.1437443494796753
Batch 8/64 loss: 0.1254456639289856
Batch 9/64 loss: 0.11843687295913696
Batch 10/64 loss: 0.12224197387695312
Batch 11/64 loss: 0.13062477111816406
Batch 12/64 loss: 0.13391470909118652
Batch 13/64 loss: 0.10527557134628296
Batch 14/64 loss: 0.14451199769973755
Batch 15/64 loss: 0.13731443881988525
Batch 16/64 loss: 0.14268970489501953
Batch 17/64 loss: 0.12747061252593994
Batch 18/64 loss: 0.1261773705482483
Batch 19/64 loss: 0.15675216913223267
Batch 20/64 loss: 0.11792439222335815
Batch 21/64 loss: 0.1323949098587036
Batch 22/64 loss: 0.13888567686080933
Batch 23/64 loss: 0.12683236598968506
Batch 24/64 loss: 0.14483225345611572
Batch 25/64 loss: 0.13452810049057007
Batch 26/64 loss: 0.10276353359222412
Batch 27/64 loss: 0.1344587206840515
Batch 28/64 loss: 0.1210741400718689
Batch 29/64 loss: 0.10164046287536621
Batch 30/64 loss: 0.15924721956253052
Batch 31/64 loss: 0.1209871768951416
Batch 32/64 loss: 0.1391286849975586
Batch 33/64 loss: 0.10880136489868164
Batch 34/64 loss: 0.13054579496383667
Batch 35/64 loss: 0.12219280004501343
Batch 36/64 loss: 0.11113226413726807
Batch 37/64 loss: 0.15332382917404175
Batch 38/64 loss: 0.13044095039367676
Batch 39/64 loss: 0.11797761917114258
Batch 40/64 loss: 0.1380840539932251
Batch 41/64 loss: 0.15147161483764648
Batch 42/64 loss: 0.12372434139251709
Batch 43/64 loss: 0.1556720733642578
Batch 44/64 loss: 0.13281506299972534
Batch 45/64 loss: 0.132524311542511
Batch 46/64 loss: 0.14221739768981934
Batch 47/64 loss: 0.1129920482635498
Batch 48/64 loss: 0.12003374099731445
Batch 49/64 loss: 0.08810639381408691
Batch 50/64 loss: 0.12101197242736816
Batch 51/64 loss: 0.1293054223060608
Batch 52/64 loss: 0.12083101272583008
Batch 53/64 loss: 0.15326827764511108
Batch 54/64 loss: 0.13798630237579346
Batch 55/64 loss: 0.10997378826141357
Batch 56/64 loss: 0.14257365465164185
Batch 57/64 loss: 0.12491893768310547
Batch 58/64 loss: 0.1215786337852478
Batch 59/64 loss: 0.11579000949859619
Batch 60/64 loss: 0.12208414077758789
Batch 61/64 loss: 0.10801678895950317
Batch 62/64 loss: 0.12682509422302246
Batch 63/64 loss: 0.11864715814590454
Batch 64/64 loss: 0.13350248336791992
Epoch 84  Train loss: 0.12920525868733723  Val loss: 0.1560983272762233
Epoch 85
-------------------------------
Batch 1/64 loss: 0.11968797445297241
Batch 2/64 loss: 0.14257699251174927
Batch 3/64 loss: 0.11162775754928589
Batch 4/64 loss: 0.16372883319854736
Batch 5/64 loss: 0.1381465196609497
Batch 6/64 loss: 0.125313401222229
Batch 7/64 loss: 0.14993900060653687
Batch 8/64 loss: 0.12348556518554688
Batch 9/64 loss: 0.11622929573059082
Batch 10/64 loss: 0.1317150592803955
Batch 11/64 loss: 0.1226891279220581
Batch 12/64 loss: 0.10010737180709839
Batch 13/64 loss: 0.11811208724975586
Batch 14/64 loss: 0.122813880443573
Batch 15/64 loss: 0.14089322090148926
Batch 16/64 loss: 0.1498280167579651
Batch 17/64 loss: 0.15171581506729126
Batch 18/64 loss: 0.116382896900177
Batch 19/64 loss: 0.13040560483932495
Batch 20/64 loss: 0.13219904899597168
Batch 21/64 loss: 0.12548309564590454
Batch 22/64 loss: 0.13894766569137573
Batch 23/64 loss: 0.11587601900100708
Batch 24/64 loss: 0.12345963716506958
Batch 25/64 loss: 0.12399637699127197
Batch 26/64 loss: 0.12849605083465576
Batch 27/64 loss: 0.13192778825759888
Batch 28/64 loss: 0.1090162992477417
Batch 29/64 loss: 0.12866151332855225
Batch 30/64 loss: 0.11570823192596436
Batch 31/64 loss: 0.10170018672943115
Batch 32/64 loss: 0.13096123933792114
Batch 33/64 loss: 0.10874795913696289
Batch 34/64 loss: 0.14020448923110962
Batch 35/64 loss: 0.14133834838867188
Batch 36/64 loss: 0.10667413473129272
Batch 37/64 loss: 0.1424710750579834
Batch 38/64 loss: 0.12986087799072266
Batch 39/64 loss: 0.14090615510940552
Batch 40/64 loss: 0.13305461406707764
Batch 41/64 loss: 0.12481272220611572
Batch 42/64 loss: 0.12406295537948608
Batch 43/64 loss: 0.13523459434509277
Batch 44/64 loss: 0.13663053512573242
Batch 45/64 loss: 0.13414716720581055
Batch 46/64 loss: 0.12649744749069214
Batch 47/64 loss: 0.1259533166885376
Batch 48/64 loss: 0.09878331422805786
Batch 49/64 loss: 0.1099168062210083
Batch 50/64 loss: 0.11932343244552612
Batch 51/64 loss: 0.13628411293029785
Batch 52/64 loss: 0.1346989870071411
Batch 53/64 loss: 0.12483906745910645
Batch 54/64 loss: 0.13391810655593872
Batch 55/64 loss: 0.13107240200042725
Batch 56/64 loss: 0.1320672631263733
Batch 57/64 loss: 0.11875581741333008
Batch 58/64 loss: 0.15085142850875854
Batch 59/64 loss: 0.12389153242111206
Batch 60/64 loss: 0.1255376935005188
Batch 61/64 loss: 0.10496759414672852
Batch 62/64 loss: 0.1387261152267456
Batch 63/64 loss: 0.13132530450820923
Batch 64/64 loss: 0.09994816780090332
Epoch 85  Train loss: 0.12740938149246514  Val loss: 0.1528878216071637
Saving best model, epoch: 85
Epoch 86
-------------------------------
Batch 1/64 loss: 0.11064237356185913
Batch 2/64 loss: 0.14453142881393433
Batch 3/64 loss: 0.14219391345977783
Batch 4/64 loss: 0.1302424669265747
Batch 5/64 loss: 0.11386412382125854
Batch 6/64 loss: 0.14653778076171875
Batch 7/64 loss: 0.1248788833618164
Batch 8/64 loss: 0.12187153100967407
Batch 9/64 loss: 0.12969505786895752
Batch 10/64 loss: 0.12163996696472168
Batch 11/64 loss: 0.11294656991958618
Batch 12/64 loss: 0.1031448245048523
Batch 13/64 loss: 0.12067782878875732
Batch 14/64 loss: 0.12567484378814697
Batch 15/64 loss: 0.12261819839477539
Batch 16/64 loss: 0.15393739938735962
Batch 17/64 loss: 0.12408429384231567
Batch 18/64 loss: 0.11903393268585205
Batch 19/64 loss: 0.12382495403289795
Batch 20/64 loss: 0.11397165060043335
Batch 21/64 loss: 0.12163615226745605
Batch 22/64 loss: 0.13888949155807495
Batch 23/64 loss: 0.11139237880706787
Batch 24/64 loss: 0.11440038681030273
Batch 25/64 loss: 0.13244998455047607
Batch 26/64 loss: 0.15512335300445557
Batch 27/64 loss: 0.1545230746269226
Batch 28/64 loss: 0.11334359645843506
Batch 29/64 loss: 0.1233065128326416
Batch 30/64 loss: 0.13647955656051636
Batch 31/64 loss: 0.13698160648345947
Batch 32/64 loss: 0.10749799013137817
Batch 33/64 loss: 0.10478591918945312
Batch 34/64 loss: 0.12644588947296143
Batch 35/64 loss: 0.13171780109405518
Batch 36/64 loss: 0.12373995780944824
Batch 37/64 loss: 0.12717598676681519
Batch 38/64 loss: 0.1149340271949768
Batch 39/64 loss: 0.10773622989654541
Batch 40/64 loss: 0.14133328199386597
Batch 41/64 loss: 0.1307607889175415
Batch 42/64 loss: 0.12190389633178711
Batch 43/64 loss: 0.11947137117385864
Batch 44/64 loss: 0.13282495737075806
Batch 45/64 loss: 0.14576482772827148
Batch 46/64 loss: 0.11397093534469604
Batch 47/64 loss: 0.1292022466659546
Batch 48/64 loss: 0.12640631198883057
Batch 49/64 loss: 0.11821615695953369
Batch 50/64 loss: 0.11780202388763428
Batch 51/64 loss: 0.134687602519989
Batch 52/64 loss: 0.1357465386390686
Batch 53/64 loss: 0.12902361154556274
Batch 54/64 loss: 0.12595319747924805
Batch 55/64 loss: 0.1403740644454956
Batch 56/64 loss: 0.10947132110595703
Batch 57/64 loss: 0.13003003597259521
Batch 58/64 loss: 0.11864703893661499
Batch 59/64 loss: 0.1593078374862671
Batch 60/64 loss: 0.12703830003738403
Batch 61/64 loss: 0.16240042448043823
Batch 62/64 loss: 0.11712747812271118
Batch 63/64 loss: 0.1274614930152893
Batch 64/64 loss: 0.11228710412979126
Epoch 86  Train loss: 0.12686608609031227  Val loss: 0.15470988316224613
Epoch 87
-------------------------------
Batch 1/64 loss: 0.1530175805091858
Batch 2/64 loss: 0.13778412342071533
Batch 3/64 loss: 0.14905661344528198
Batch 4/64 loss: 0.11144739389419556
Batch 5/64 loss: 0.1179196834564209
Batch 6/64 loss: 0.11927914619445801
Batch 7/64 loss: 0.15013837814331055
Batch 8/64 loss: 0.11313307285308838
Batch 9/64 loss: 0.09982407093048096
Batch 10/64 loss: 0.12250769138336182
Batch 11/64 loss: 0.13063204288482666
Batch 12/64 loss: 0.1372082233428955
Batch 13/64 loss: 0.12106478214263916
Batch 14/64 loss: 0.14746803045272827
Batch 15/64 loss: 0.10403567552566528
Batch 16/64 loss: 0.13168585300445557
Batch 17/64 loss: 0.13498592376708984
Batch 18/64 loss: 0.13243579864501953
Batch 19/64 loss: 0.1284307837486267
Batch 20/64 loss: 0.11692720651626587
Batch 21/64 loss: 0.14414989948272705
Batch 22/64 loss: 0.1508675217628479
Batch 23/64 loss: 0.1100536584854126
Batch 24/64 loss: 0.11638802289962769
Batch 25/64 loss: 0.12007367610931396
Batch 26/64 loss: 0.11448276042938232
Batch 27/64 loss: 0.11353570222854614
Batch 28/64 loss: 0.12481647729873657
Batch 29/64 loss: 0.1360701322555542
Batch 30/64 loss: 0.13337081670761108
Batch 31/64 loss: 0.14605069160461426
Batch 32/64 loss: 0.1287328004837036
Batch 33/64 loss: 0.10179674625396729
Batch 34/64 loss: 0.12093883752822876
Batch 35/64 loss: 0.10626596212387085
Batch 36/64 loss: 0.12123697996139526
Batch 37/64 loss: 0.13504862785339355
Batch 38/64 loss: 0.12646567821502686
Batch 39/64 loss: 0.1362391710281372
Batch 40/64 loss: 0.130085289478302
Batch 41/64 loss: 0.13198238611221313
Batch 42/64 loss: 0.11758118867874146
Batch 43/64 loss: 0.125912606716156
Batch 44/64 loss: 0.10464203357696533
Batch 45/64 loss: 0.13243889808654785
Batch 46/64 loss: 0.14588099718093872
Batch 47/64 loss: 0.10938668251037598
Batch 48/64 loss: 0.11540591716766357
Batch 49/64 loss: 0.13252323865890503
Batch 50/64 loss: 0.10495424270629883
Batch 51/64 loss: 0.13241606950759888
Batch 52/64 loss: 0.13502240180969238
Batch 53/64 loss: 0.13142001628875732
Batch 54/64 loss: 0.1400269865989685
Batch 55/64 loss: 0.13865453004837036
Batch 56/64 loss: 0.14022958278656006
Batch 57/64 loss: 0.11861836910247803
Batch 58/64 loss: 0.13144785165786743
Batch 59/64 loss: 0.12692958116531372
Batch 60/64 loss: 0.13014447689056396
Batch 61/64 loss: 0.13093918561935425
Batch 62/64 loss: 0.12107181549072266
Batch 63/64 loss: 0.12348318099975586
Batch 64/64 loss: 0.11070173978805542
Epoch 87  Train loss: 0.12674180502985039  Val loss: 0.15823476646364348
Epoch 88
-------------------------------
Batch 1/64 loss: 0.14230328798294067
Batch 2/64 loss: 0.16325950622558594
Batch 3/64 loss: 0.14528906345367432
Batch 4/64 loss: 0.10377949476242065
Batch 5/64 loss: 0.11887425184249878
Batch 6/64 loss: 0.12085962295532227
Batch 7/64 loss: 0.11089205741882324
Batch 8/64 loss: 0.13410407304763794
Batch 9/64 loss: 0.11231595277786255
Batch 10/64 loss: 0.12594246864318848
Batch 11/64 loss: 0.12671834230422974
Batch 12/64 loss: 0.13857203722000122
Batch 13/64 loss: 0.11450350284576416
Batch 14/64 loss: 0.11019349098205566
Batch 15/64 loss: 0.11118823289871216
Batch 16/64 loss: 0.13123250007629395
Batch 17/64 loss: 0.10360026359558105
Batch 18/64 loss: 0.140447735786438
Batch 19/64 loss: 0.12718921899795532
Batch 20/64 loss: 0.13522380590438843
Batch 21/64 loss: 0.13640892505645752
Batch 22/64 loss: 0.12444382905960083
Batch 23/64 loss: 0.13175225257873535
Batch 24/64 loss: 0.13201159238815308
Batch 25/64 loss: 0.1015504002571106
Batch 26/64 loss: 0.13214129209518433
Batch 27/64 loss: 0.0968865156173706
Batch 28/64 loss: 0.13338631391525269
Batch 29/64 loss: 0.13006865978240967
Batch 30/64 loss: 0.10847258567810059
Batch 31/64 loss: 0.09769928455352783
Batch 32/64 loss: 0.1608870029449463
Batch 33/64 loss: 0.12179917097091675
Batch 34/64 loss: 0.13192272186279297
Batch 35/64 loss: 0.12262982130050659
Batch 36/64 loss: 0.1366816759109497
Batch 37/64 loss: 0.12939727306365967
Batch 38/64 loss: 0.10307431221008301
Batch 39/64 loss: 0.10366702079772949
Batch 40/64 loss: 0.11043250560760498
Batch 41/64 loss: 0.13314753770828247
Batch 42/64 loss: 0.13399046659469604
Batch 43/64 loss: 0.10745030641555786
Batch 44/64 loss: 0.1194072961807251
Batch 45/64 loss: 0.15109962224960327
Batch 46/64 loss: 0.11966723203659058
Batch 47/64 loss: 0.1086077094078064
Batch 48/64 loss: 0.14383089542388916
Batch 49/64 loss: 0.1372886300086975
Batch 50/64 loss: 0.1214408278465271
Batch 51/64 loss: 0.12326222658157349
Batch 52/64 loss: 0.11656224727630615
Batch 53/64 loss: 0.1096464991569519
Batch 54/64 loss: 0.12672072649002075
Batch 55/64 loss: 0.12633353471755981
Batch 56/64 loss: 0.12924307584762573
Batch 57/64 loss: 0.09545111656188965
Batch 58/64 loss: 0.1341874599456787
Batch 59/64 loss: 0.12916022539138794
Batch 60/64 loss: 0.14563745260238647
Batch 61/64 loss: 0.12669289112091064
Batch 62/64 loss: 0.1124415397644043
Batch 63/64 loss: 0.14629989862442017
Batch 64/64 loss: 0.12210118770599365
Epoch 88  Train loss: 0.1247207431232228  Val loss: 0.1545724282969314
Epoch 89
-------------------------------
Batch 1/64 loss: 0.12476575374603271
Batch 2/64 loss: 0.12999945878982544
Batch 3/64 loss: 0.14452141523361206
Batch 4/64 loss: 0.1196100115776062
Batch 5/64 loss: 0.12651199102401733
Batch 6/64 loss: 0.11224532127380371
Batch 7/64 loss: 0.15205103158950806
Batch 8/64 loss: 0.123623788356781
Batch 9/64 loss: 0.1287912130355835
Batch 10/64 loss: 0.15159475803375244
Batch 11/64 loss: 0.1462751030921936
Batch 12/64 loss: 0.14629685878753662
Batch 13/64 loss: 0.12356746196746826
Batch 14/64 loss: 0.12396466732025146
Batch 15/64 loss: 0.13532191514968872
Batch 16/64 loss: 0.136380136013031
Batch 17/64 loss: 0.12184041738510132
Batch 18/64 loss: 0.1536296010017395
Batch 19/64 loss: 0.10181570053100586
Batch 20/64 loss: 0.10339117050170898
Batch 21/64 loss: 0.10470056533813477
Batch 22/64 loss: 0.13180941343307495
Batch 23/64 loss: 0.13553935289382935
Batch 24/64 loss: 0.09676408767700195
Batch 25/64 loss: 0.12319052219390869
Batch 26/64 loss: 0.11721444129943848
Batch 27/64 loss: 0.12916022539138794
Batch 28/64 loss: 0.1163637638092041
Batch 29/64 loss: 0.11943626403808594
Batch 30/64 loss: 0.11976206302642822
Batch 31/64 loss: 0.12984824180603027
Batch 32/64 loss: 0.11145782470703125
Batch 33/64 loss: 0.11412358283996582
Batch 34/64 loss: 0.1461222767829895
Batch 35/64 loss: 0.11122584342956543
Batch 36/64 loss: 0.14017856121063232
Batch 37/64 loss: 0.11982107162475586
Batch 38/64 loss: 0.14264118671417236
Batch 39/64 loss: 0.11198580265045166
Batch 40/64 loss: 0.11463415622711182
Batch 41/64 loss: 0.1406954526901245
Batch 42/64 loss: 0.11749184131622314
Batch 43/64 loss: 0.16641229391098022
Batch 44/64 loss: 0.13593047857284546
Batch 45/64 loss: 0.12188804149627686
Batch 46/64 loss: 0.10840010643005371
Batch 47/64 loss: 0.1220930814743042
Batch 48/64 loss: 0.1431061029434204
Batch 49/64 loss: 0.12374615669250488
Batch 50/64 loss: 0.13549423217773438
Batch 51/64 loss: 0.13855504989624023
Batch 52/64 loss: 0.11517399549484253
Batch 53/64 loss: 0.1309654712677002
Batch 54/64 loss: 0.09935319423675537
Batch 55/64 loss: 0.12889337539672852
Batch 56/64 loss: 0.10464990139007568
Batch 57/64 loss: 0.1254715919494629
Batch 58/64 loss: 0.0942581295967102
Batch 59/64 loss: 0.11434471607208252
Batch 60/64 loss: 0.13440430164337158
Batch 61/64 loss: 0.11012351512908936
Batch 62/64 loss: 0.11796683073043823
Batch 63/64 loss: 0.10877573490142822
Batch 64/64 loss: 0.14062100648880005
Epoch 89  Train loss: 0.1252680215181089  Val loss: 0.15603717618791507
Epoch 90
-------------------------------
Batch 1/64 loss: 0.13144183158874512
Batch 2/64 loss: 0.12045669555664062
Batch 3/64 loss: 0.11763572692871094
Batch 4/64 loss: 0.13465094566345215
Batch 5/64 loss: 0.12719082832336426
Batch 6/64 loss: 0.10770726203918457
Batch 7/64 loss: 0.12335842847824097
Batch 8/64 loss: 0.1498144268989563
Batch 9/64 loss: 0.10733187198638916
Batch 10/64 loss: 0.15181785821914673
Batch 11/64 loss: 0.09673243761062622
Batch 12/64 loss: 0.10721319913864136
Batch 13/64 loss: 0.14859437942504883
Batch 14/64 loss: 0.10394704341888428
Batch 15/64 loss: 0.11423051357269287
Batch 16/64 loss: 0.1205521821975708
Batch 17/64 loss: 0.11959218978881836
Batch 18/64 loss: 0.13256394863128662
Batch 19/64 loss: 0.1336234211921692
Batch 20/64 loss: 0.14238077402114868
Batch 21/64 loss: 0.12329041957855225
Batch 22/64 loss: 0.12163138389587402
Batch 23/64 loss: 0.10838603973388672
Batch 24/64 loss: 0.12228631973266602
Batch 25/64 loss: 0.13363105058670044
Batch 26/64 loss: 0.11940526962280273
Batch 27/64 loss: 0.11410081386566162
Batch 28/64 loss: 0.12787771224975586
Batch 29/64 loss: 0.11704283952713013
Batch 30/64 loss: 0.13686507940292358
Batch 31/64 loss: 0.11835634708404541
Batch 32/64 loss: 0.12778544425964355
Batch 33/64 loss: 0.13097476959228516
Batch 34/64 loss: 0.12956738471984863
Batch 35/64 loss: 0.13967770338058472
Batch 36/64 loss: 0.13068318367004395
Batch 37/64 loss: 0.14126908779144287
Batch 38/64 loss: 0.0999288558959961
Batch 39/64 loss: 0.09768372774124146
Batch 40/64 loss: 0.15155452489852905
Batch 41/64 loss: 0.128451406955719
Batch 42/64 loss: 0.1073717474937439
Batch 43/64 loss: 0.12401747703552246
Batch 44/64 loss: 0.12140464782714844
Batch 45/64 loss: 0.1071893572807312
Batch 46/64 loss: 0.12174773216247559
Batch 47/64 loss: 0.15319478511810303
Batch 48/64 loss: 0.12276917695999146
Batch 49/64 loss: 0.1306852102279663
Batch 50/64 loss: 0.12112212181091309
Batch 51/64 loss: 0.13477933406829834
Batch 52/64 loss: 0.11065983772277832
Batch 53/64 loss: 0.11382663249969482
Batch 54/64 loss: 0.1357511281967163
Batch 55/64 loss: 0.13471192121505737
Batch 56/64 loss: 0.11315447092056274
Batch 57/64 loss: 0.1294918656349182
Batch 58/64 loss: 0.1466277837753296
Batch 59/64 loss: 0.10649019479751587
Batch 60/64 loss: 0.1052863597869873
Batch 61/64 loss: 0.09924125671386719
Batch 62/64 loss: 0.1359792947769165
Batch 63/64 loss: 0.12769895792007446
Batch 64/64 loss: 0.10992956161499023
Epoch 90  Train loss: 0.1238734712787703  Val loss: 0.1525194120570966
Saving best model, epoch: 90
Epoch 91
-------------------------------
Batch 1/64 loss: 0.11854946613311768
Batch 2/64 loss: 0.12390655279159546
Batch 3/64 loss: 0.13048720359802246
Batch 4/64 loss: 0.12289297580718994
Batch 5/64 loss: 0.11800903081893921
Batch 6/64 loss: 0.13289570808410645
Batch 7/64 loss: 0.11278319358825684
Batch 8/64 loss: 0.13420414924621582
Batch 9/64 loss: 0.08632582426071167
Batch 10/64 loss: 0.1321008801460266
Batch 11/64 loss: 0.10234701633453369
Batch 12/64 loss: 0.1315651535987854
Batch 13/64 loss: 0.12301063537597656
Batch 14/64 loss: 0.12887239456176758
Batch 15/64 loss: 0.14026975631713867
Batch 16/64 loss: 0.11011368036270142
Batch 17/64 loss: 0.12590527534484863
Batch 18/64 loss: 0.13124442100524902
Batch 19/64 loss: 0.11235243082046509
Batch 20/64 loss: 0.11100518703460693
Batch 21/64 loss: 0.1256866455078125
Batch 22/64 loss: 0.11621928215026855
Batch 23/64 loss: 0.14162832498550415
Batch 24/64 loss: 0.10198080539703369
Batch 25/64 loss: 0.13591766357421875
Batch 26/64 loss: 0.15035408735275269
Batch 27/64 loss: 0.11842525005340576
Batch 28/64 loss: 0.11879009008407593
Batch 29/64 loss: 0.13091278076171875
Batch 30/64 loss: 0.1426161527633667
Batch 31/64 loss: 0.128695547580719
Batch 32/64 loss: 0.1242990493774414
Batch 33/64 loss: 0.1176137924194336
Batch 34/64 loss: 0.11203610897064209
Batch 35/64 loss: 0.11996442079544067
Batch 36/64 loss: 0.1284722089767456
Batch 37/64 loss: 0.12622499465942383
Batch 38/64 loss: 0.11312985420227051
Batch 39/64 loss: 0.1259174942970276
Batch 40/64 loss: 0.13119977712631226
Batch 41/64 loss: 0.13240110874176025
Batch 42/64 loss: 0.12032431364059448
Batch 43/64 loss: 0.14848458766937256
Batch 44/64 loss: 0.10310107469558716
Batch 45/64 loss: 0.1115875244140625
Batch 46/64 loss: 0.14371341466903687
Batch 47/64 loss: 0.11634725332260132
Batch 48/64 loss: 0.13621610403060913
Batch 49/64 loss: 0.11155784130096436
Batch 50/64 loss: 0.11686581373214722
Batch 51/64 loss: 0.10003632307052612
Batch 52/64 loss: 0.12278228998184204
Batch 53/64 loss: 0.10493063926696777
Batch 54/64 loss: 0.1311131715774536
Batch 55/64 loss: 0.1413252353668213
Batch 56/64 loss: 0.14558923244476318
Batch 57/64 loss: 0.1184801459312439
Batch 58/64 loss: 0.12916284799575806
Batch 59/64 loss: 0.11734539270401001
Batch 60/64 loss: 0.12187647819519043
Batch 61/64 loss: 0.10803812742233276
Batch 62/64 loss: 0.10802984237670898
Batch 63/64 loss: 0.1289067268371582
Batch 64/64 loss: 0.13516902923583984
Epoch 91  Train loss: 0.12327086317772959  Val loss: 0.15347370372195424
Epoch 92
-------------------------------
Batch 1/64 loss: 0.13060706853866577
Batch 2/64 loss: 0.11646205186843872
Batch 3/64 loss: 0.13085711002349854
Batch 4/64 loss: 0.1324116587638855
Batch 5/64 loss: 0.10573279857635498
Batch 6/64 loss: 0.10002964735031128
Batch 7/64 loss: 0.1250055432319641
Batch 8/64 loss: 0.15827423334121704
Batch 9/64 loss: 0.14005059003829956
Batch 10/64 loss: 0.10351771116256714
Batch 11/64 loss: 0.1179705262184143
Batch 12/64 loss: 0.16341018676757812
Batch 13/64 loss: 0.1376885175704956
Batch 14/64 loss: 0.12021458148956299
Batch 15/64 loss: 0.1355907917022705
Batch 16/64 loss: 0.12407219409942627
Batch 17/64 loss: 0.11307024955749512
Batch 18/64 loss: 0.11635607481002808
Batch 19/64 loss: 0.11488854885101318
Batch 20/64 loss: 0.12929373979568481
Batch 21/64 loss: 0.13081514835357666
Batch 22/64 loss: 0.12305718660354614
Batch 23/64 loss: 0.11868208646774292
Batch 24/64 loss: 0.12377208471298218
Batch 25/64 loss: 0.10259407758712769
Batch 26/64 loss: 0.1253068447113037
Batch 27/64 loss: 0.09791862964630127
Batch 28/64 loss: 0.1477786898612976
Batch 29/64 loss: 0.09955096244812012
Batch 30/64 loss: 0.11448895931243896
Batch 31/64 loss: 0.10195285081863403
Batch 32/64 loss: 0.12138330936431885
Batch 33/64 loss: 0.12438726425170898
Batch 34/64 loss: 0.11941790580749512
Batch 35/64 loss: 0.12262105941772461
Batch 36/64 loss: 0.12881934642791748
Batch 37/64 loss: 0.11336612701416016
Batch 38/64 loss: 0.1202896237373352
Batch 39/64 loss: 0.10067838430404663
Batch 40/64 loss: 0.12462776899337769
Batch 41/64 loss: 0.11390018463134766
Batch 42/64 loss: 0.13936710357666016
Batch 43/64 loss: 0.11276882886886597
Batch 44/64 loss: 0.12385928630828857
Batch 45/64 loss: 0.13923609256744385
Batch 46/64 loss: 0.1330001950263977
Batch 47/64 loss: 0.12505382299423218
Batch 48/64 loss: 0.13448989391326904
Batch 49/64 loss: 0.11285436153411865
Batch 50/64 loss: 0.11601227521896362
Batch 51/64 loss: 0.10369408130645752
Batch 52/64 loss: 0.15256023406982422
Batch 53/64 loss: 0.1234731674194336
Batch 54/64 loss: 0.14204740524291992
Batch 55/64 loss: 0.12683433294296265
Batch 56/64 loss: 0.10911470651626587
Batch 57/64 loss: 0.11565923690795898
Batch 58/64 loss: 0.12425673007965088
Batch 59/64 loss: 0.13046377897262573
Batch 60/64 loss: 0.13628560304641724
Batch 61/64 loss: 0.10380643606185913
Batch 62/64 loss: 0.12931525707244873
Batch 63/64 loss: 0.12050944566726685
Batch 64/64 loss: 0.11727017164230347
Epoch 92  Train loss: 0.12287885838863896  Val loss: 0.150747698606904
Saving best model, epoch: 92
Epoch 93
-------------------------------
Batch 1/64 loss: 0.13013136386871338
Batch 2/64 loss: 0.119098961353302
Batch 3/64 loss: 0.10365378856658936
Batch 4/64 loss: 0.12026894092559814
Batch 5/64 loss: 0.10895788669586182
Batch 6/64 loss: 0.10182005167007446
Batch 7/64 loss: 0.13027828931808472
Batch 8/64 loss: 0.16061270236968994
Batch 9/64 loss: 0.11950236558914185
Batch 10/64 loss: 0.1244429349899292
Batch 11/64 loss: 0.13700222969055176
Batch 12/64 loss: 0.1374329924583435
Batch 13/64 loss: 0.10952341556549072
Batch 14/64 loss: 0.1254720687866211
Batch 15/64 loss: 0.10591459274291992
Batch 16/64 loss: 0.1416826844215393
Batch 17/64 loss: 0.12112200260162354
Batch 18/64 loss: 0.11828505992889404
Batch 19/64 loss: 0.12019652128219604
Batch 20/64 loss: 0.11198157072067261
Batch 21/64 loss: 0.12954843044281006
Batch 22/64 loss: 0.10589790344238281
Batch 23/64 loss: 0.1060495376586914
Batch 24/64 loss: 0.13304448127746582
Batch 25/64 loss: 0.12156081199645996
Batch 26/64 loss: 0.1192702054977417
Batch 27/64 loss: 0.16054362058639526
Batch 28/64 loss: 0.13389766216278076
Batch 29/64 loss: 0.10844218730926514
Batch 30/64 loss: 0.11333292722702026
Batch 31/64 loss: 0.09275990724563599
Batch 32/64 loss: 0.121326744556427
Batch 33/64 loss: 0.14063143730163574
Batch 34/64 loss: 0.12553787231445312
Batch 35/64 loss: 0.1269562840461731
Batch 36/64 loss: 0.13157576322555542
Batch 37/64 loss: 0.11982667446136475
Batch 38/64 loss: 0.1430467963218689
Batch 39/64 loss: 0.10378670692443848
Batch 40/64 loss: 0.14542829990386963
Batch 41/64 loss: 0.12184226512908936
Batch 42/64 loss: 0.13071990013122559
Batch 43/64 loss: 0.12654495239257812
Batch 44/64 loss: 0.11129158735275269
Batch 45/64 loss: 0.11929738521575928
Batch 46/64 loss: 0.11717796325683594
Batch 47/64 loss: 0.10802567005157471
Batch 48/64 loss: 0.09340274333953857
Batch 49/64 loss: 0.1274760365486145
Batch 50/64 loss: 0.13252246379852295
Batch 51/64 loss: 0.13900452852249146
Batch 52/64 loss: 0.12033599615097046
Batch 53/64 loss: 0.09259593486785889
Batch 54/64 loss: 0.11493319272994995
Batch 55/64 loss: 0.10726940631866455
Batch 56/64 loss: 0.13269925117492676
Batch 57/64 loss: 0.11685770750045776
Batch 58/64 loss: 0.1228933334350586
Batch 59/64 loss: 0.13338184356689453
Batch 60/64 loss: 0.09880030155181885
Batch 61/64 loss: 0.13078808784484863
Batch 62/64 loss: 0.13157141208648682
Batch 63/64 loss: 0.1291840672492981
Batch 64/64 loss: 0.10980510711669922
Epoch 93  Train loss: 0.12189513000787473  Val loss: 0.1516817511152156
Epoch 94
-------------------------------
Batch 1/64 loss: 0.1517276167869568
Batch 2/64 loss: 0.11735576391220093
Batch 3/64 loss: 0.12965232133865356
Batch 4/64 loss: 0.10595625638961792
Batch 5/64 loss: 0.08324885368347168
Batch 6/64 loss: 0.10656249523162842
Batch 7/64 loss: 0.1214478611946106
Batch 8/64 loss: 0.14516973495483398
Batch 9/64 loss: 0.14209866523742676
Batch 10/64 loss: 0.1491466760635376
Batch 11/64 loss: 0.11424148082733154
Batch 12/64 loss: 0.09847593307495117
Batch 13/64 loss: 0.11630403995513916
Batch 14/64 loss: 0.12973380088806152
Batch 15/64 loss: 0.11857980489730835
Batch 16/64 loss: 0.13294947147369385
Batch 17/64 loss: 0.10010033845901489
Batch 18/64 loss: 0.12104994058609009
Batch 19/64 loss: 0.143760085105896
Batch 20/64 loss: 0.10838544368743896
Batch 21/64 loss: 0.14431524276733398
Batch 22/64 loss: 0.09574586153030396
Batch 23/64 loss: 0.1095227599143982
Batch 24/64 loss: 0.11528545618057251
Batch 25/64 loss: 0.12238168716430664
Batch 26/64 loss: 0.12321370840072632
Batch 27/64 loss: 0.12660366296768188
Batch 28/64 loss: 0.11424344778060913
Batch 29/64 loss: 0.10681354999542236
Batch 30/64 loss: 0.09902215003967285
Batch 31/64 loss: 0.1347421407699585
Batch 32/64 loss: 0.09768009185791016
Batch 33/64 loss: 0.14947646856307983
Batch 34/64 loss: 0.12707382440567017
Batch 35/64 loss: 0.11685609817504883
Batch 36/64 loss: 0.13553732633590698
Batch 37/64 loss: 0.11531752347946167
Batch 38/64 loss: 0.11820179224014282
Batch 39/64 loss: 0.1353079080581665
Batch 40/64 loss: 0.15774476528167725
Batch 41/64 loss: 0.12807071208953857
Batch 42/64 loss: 0.08962815999984741
Batch 43/64 loss: 0.1312660574913025
Batch 44/64 loss: 0.10819506645202637
Batch 45/64 loss: 0.1085699200630188
Batch 46/64 loss: 0.11877071857452393
Batch 47/64 loss: 0.11480247974395752
Batch 48/64 loss: 0.11018264293670654
Batch 49/64 loss: 0.14411425590515137
Batch 50/64 loss: 0.12942945957183838
Batch 51/64 loss: 0.1322566270828247
Batch 52/64 loss: 0.137326180934906
Batch 53/64 loss: 0.10975617170333862
Batch 54/64 loss: 0.1018630862236023
Batch 55/64 loss: 0.12683922052383423
Batch 56/64 loss: 0.10065817832946777
Batch 57/64 loss: 0.14232087135314941
Batch 58/64 loss: 0.11594092845916748
Batch 59/64 loss: 0.13653600215911865
Batch 60/64 loss: 0.10172581672668457
Batch 61/64 loss: 0.16206562519073486
Batch 62/64 loss: 0.13836783170700073
Batch 63/64 loss: 0.09842604398727417
Batch 64/64 loss: 0.1290726661682129
Epoch 94  Train loss: 0.12180311539593865  Val loss: 0.15067131740530743
Saving best model, epoch: 94
Epoch 95
-------------------------------
Batch 1/64 loss: 0.10833019018173218
Batch 2/64 loss: 0.12424033880233765
Batch 3/64 loss: 0.10554474592208862
Batch 4/64 loss: 0.1139068603515625
Batch 5/64 loss: 0.12734240293502808
Batch 6/64 loss: 0.11896252632141113
Batch 7/64 loss: 0.14274859428405762
Batch 8/64 loss: 0.11715996265411377
Batch 9/64 loss: 0.14762330055236816
Batch 10/64 loss: 0.1131402850151062
Batch 11/64 loss: 0.12108170986175537
Batch 12/64 loss: 0.1306876540184021
Batch 13/64 loss: 0.13089150190353394
Batch 14/64 loss: 0.13271552324295044
Batch 15/64 loss: 0.1289834976196289
Batch 16/64 loss: 0.1496632695198059
Batch 17/64 loss: 0.15675288438796997
Batch 18/64 loss: 0.11975473165512085
Batch 19/64 loss: 0.11449098587036133
Batch 20/64 loss: 0.10004997253417969
Batch 21/64 loss: 0.12101918458938599
Batch 22/64 loss: 0.1427484154701233
Batch 23/64 loss: 0.11993598937988281
Batch 24/64 loss: 0.1151970624923706
Batch 25/64 loss: 0.11674433946609497
Batch 26/64 loss: 0.10605710744857788
Batch 27/64 loss: 0.1170724630355835
Batch 28/64 loss: 0.12533414363861084
Batch 29/64 loss: 0.13076543807983398
Batch 30/64 loss: 0.13826823234558105
Batch 31/64 loss: 0.12649774551391602
Batch 32/64 loss: 0.12909889221191406
Batch 33/64 loss: 0.13506025075912476
Batch 34/64 loss: 0.13113510608673096
Batch 35/64 loss: 0.1311468482017517
Batch 36/64 loss: 0.10611772537231445
Batch 37/64 loss: 0.13286566734313965
Batch 38/64 loss: 0.11474114656448364
Batch 39/64 loss: 0.14348721504211426
Batch 40/64 loss: 0.12887156009674072
Batch 41/64 loss: 0.10177326202392578
Batch 42/64 loss: 0.1110374927520752
Batch 43/64 loss: 0.12939244508743286
Batch 44/64 loss: 0.11785280704498291
Batch 45/64 loss: 0.11987638473510742
Batch 46/64 loss: 0.1095273494720459
Batch 47/64 loss: 0.13122355937957764
Batch 48/64 loss: 0.12775200605392456
Batch 49/64 loss: 0.1177491545677185
Batch 50/64 loss: 0.11276596784591675
Batch 51/64 loss: 0.11091333627700806
Batch 52/64 loss: 0.13080334663391113
Batch 53/64 loss: 0.09743654727935791
Batch 54/64 loss: 0.1330999732017517
Batch 55/64 loss: 0.1257222294807434
Batch 56/64 loss: 0.08906638622283936
Batch 57/64 loss: 0.14682555198669434
Batch 58/64 loss: 0.09987103939056396
Batch 59/64 loss: 0.13228857517242432
Batch 60/64 loss: 0.13127732276916504
Batch 61/64 loss: 0.11771726608276367
Batch 62/64 loss: 0.12522822618484497
Batch 63/64 loss: 0.1097421646118164
Batch 64/64 loss: 0.13602453470230103
Epoch 95  Train loss: 0.12309327476164873  Val loss: 0.1492973389904114
Saving best model, epoch: 95
Epoch 96
-------------------------------
Batch 1/64 loss: 0.1403651237487793
Batch 2/64 loss: 0.11141550540924072
Batch 3/64 loss: 0.11446565389633179
Batch 4/64 loss: 0.11809873580932617
Batch 5/64 loss: 0.0942121148109436
Batch 6/64 loss: 0.1281556487083435
Batch 7/64 loss: 0.09963995218276978
Batch 8/64 loss: 0.16662901639938354
Batch 9/64 loss: 0.13828390836715698
Batch 10/64 loss: 0.11294025182723999
Batch 11/64 loss: 0.10344648361206055
Batch 12/64 loss: 0.10984230041503906
Batch 13/64 loss: 0.11354035139083862
Batch 14/64 loss: 0.14400750398635864
Batch 15/64 loss: 0.09685027599334717
Batch 16/64 loss: 0.12302029132843018
Batch 17/64 loss: 0.13055485486984253
Batch 18/64 loss: 0.15714967250823975
Batch 19/64 loss: 0.11547845602035522
Batch 20/64 loss: 0.14209628105163574
Batch 21/64 loss: 0.11550158262252808
Batch 22/64 loss: 0.11907601356506348
Batch 23/64 loss: 0.11859112977981567
Batch 24/64 loss: 0.14573049545288086
Batch 25/64 loss: 0.1239851713180542
Batch 26/64 loss: 0.11452323198318481
Batch 27/64 loss: 0.09934508800506592
Batch 28/64 loss: 0.1385955810546875
Batch 29/64 loss: 0.10584133863449097
Batch 30/64 loss: 0.11465668678283691
Batch 31/64 loss: 0.11222809553146362
Batch 32/64 loss: 0.1250191330909729
Batch 33/64 loss: 0.09916931390762329
Batch 34/64 loss: 0.1379273533821106
Batch 35/64 loss: 0.1251140832901001
Batch 36/64 loss: 0.13052523136138916
Batch 37/64 loss: 0.10934537649154663
Batch 38/64 loss: 0.10667312145233154
Batch 39/64 loss: 0.13837838172912598
Batch 40/64 loss: 0.12430727481842041
Batch 41/64 loss: 0.0992203950881958
Batch 42/64 loss: 0.12113559246063232
Batch 43/64 loss: 0.1324911117553711
Batch 44/64 loss: 0.1360796093940735
Batch 45/64 loss: 0.10497850179672241
Batch 46/64 loss: 0.10507476329803467
Batch 47/64 loss: 0.12432712316513062
Batch 48/64 loss: 0.13495439291000366
Batch 49/64 loss: 0.11420851945877075
Batch 50/64 loss: 0.10759395360946655
Batch 51/64 loss: 0.13293367624282837
Batch 52/64 loss: 0.113838791847229
Batch 53/64 loss: 0.1451244354248047
Batch 54/64 loss: 0.15147829055786133
Batch 55/64 loss: 0.11133056879043579
Batch 56/64 loss: 0.12203633785247803
Batch 57/64 loss: 0.11851441860198975
Batch 58/64 loss: 0.14008831977844238
Batch 59/64 loss: 0.10202407836914062
Batch 60/64 loss: 0.12607836723327637
Batch 61/64 loss: 0.10723692178726196
Batch 62/64 loss: 0.1030493974685669
Batch 63/64 loss: 0.1316511631011963
Batch 64/64 loss: 0.11787664890289307
Epoch 96  Train loss: 0.12138952601189706  Val loss: 0.14978759559159427
Epoch 97
-------------------------------
Batch 1/64 loss: 0.1077263355255127
Batch 2/64 loss: 0.124825119972229
Batch 3/64 loss: 0.09855705499649048
Batch 4/64 loss: 0.0997004508972168
Batch 5/64 loss: 0.11884737014770508
Batch 6/64 loss: 0.12106913328170776
Batch 7/64 loss: 0.12149691581726074
Batch 8/64 loss: 0.11339932680130005
Batch 9/64 loss: 0.1250801682472229
Batch 10/64 loss: 0.13320684432983398
Batch 11/64 loss: 0.08626407384872437
Batch 12/64 loss: 0.1295364499092102
Batch 13/64 loss: 0.11383616924285889
Batch 14/64 loss: 0.12351220846176147
Batch 15/64 loss: 0.11496007442474365
Batch 16/64 loss: 0.11836135387420654
Batch 17/64 loss: 0.1036919355392456
Batch 18/64 loss: 0.1118975281715393
Batch 19/64 loss: 0.12174481153488159
Batch 20/64 loss: 0.12822920083999634
Batch 21/64 loss: 0.08162331581115723
Batch 22/64 loss: 0.12808603048324585
Batch 23/64 loss: 0.11626970767974854
Batch 24/64 loss: 0.11763644218444824
Batch 25/64 loss: 0.12443113327026367
Batch 26/64 loss: 0.1119202971458435
Batch 27/64 loss: 0.11115175485610962
Batch 28/64 loss: 0.13339167833328247
Batch 29/64 loss: 0.11271870136260986
Batch 30/64 loss: 0.1280096173286438
Batch 31/64 loss: 0.11621749401092529
Batch 32/64 loss: 0.11795741319656372
Batch 33/64 loss: 0.11454880237579346
Batch 34/64 loss: 0.11702191829681396
Batch 35/64 loss: 0.12766611576080322
Batch 36/64 loss: 0.1188814640045166
Batch 37/64 loss: 0.13048487901687622
Batch 38/64 loss: 0.11180329322814941
Batch 39/64 loss: 0.10960245132446289
Batch 40/64 loss: 0.10219210386276245
Batch 41/64 loss: 0.12354445457458496
Batch 42/64 loss: 0.1469426155090332
Batch 43/64 loss: 0.12396776676177979
Batch 44/64 loss: 0.12343889474868774
Batch 45/64 loss: 0.12609505653381348
Batch 46/64 loss: 0.11175894737243652
Batch 47/64 loss: 0.13251268863677979
Batch 48/64 loss: 0.12634432315826416
Batch 49/64 loss: 0.14717280864715576
Batch 50/64 loss: 0.10522341728210449
Batch 51/64 loss: 0.140064537525177
Batch 52/64 loss: 0.12919825315475464
Batch 53/64 loss: 0.10797369480133057
Batch 54/64 loss: 0.10553497076034546
Batch 55/64 loss: 0.12471604347229004
Batch 56/64 loss: 0.11589193344116211
Batch 57/64 loss: 0.1350613236427307
Batch 58/64 loss: 0.12071049213409424
Batch 59/64 loss: 0.1171339750289917
Batch 60/64 loss: 0.11595046520233154
Batch 61/64 loss: 0.1464627981185913
Batch 62/64 loss: 0.11815214157104492
Batch 63/64 loss: 0.12526321411132812
Batch 64/64 loss: 0.09290331602096558
Epoch 97  Train loss: 0.11900155988394046  Val loss: 0.15003528791604584
Epoch 98
-------------------------------
Batch 1/64 loss: 0.1018747091293335
Batch 2/64 loss: 0.1240234375
Batch 3/64 loss: 0.12061852216720581
Batch 4/64 loss: 0.14050525426864624
Batch 5/64 loss: 0.1421772837638855
Batch 6/64 loss: 0.10191357135772705
Batch 7/64 loss: 0.08508384227752686
Batch 8/64 loss: 0.115902841091156
Batch 9/64 loss: 0.11000561714172363
Batch 10/64 loss: 0.1234012246131897
Batch 11/64 loss: 0.11285793781280518
Batch 12/64 loss: 0.11164611577987671
Batch 13/64 loss: 0.12647438049316406
Batch 14/64 loss: 0.10864031314849854
Batch 15/64 loss: 0.10637205839157104
Batch 16/64 loss: 0.10933834314346313
Batch 17/64 loss: 0.10573607683181763
Batch 18/64 loss: 0.10939085483551025
Batch 19/64 loss: 0.12227869033813477
Batch 20/64 loss: 0.15328431129455566
Batch 21/64 loss: 0.14183390140533447
Batch 22/64 loss: 0.11068546772003174
Batch 23/64 loss: 0.12705260515213013
Batch 24/64 loss: 0.13441252708435059
Batch 25/64 loss: 0.13602840900421143
Batch 26/64 loss: 0.10767251253128052
Batch 27/64 loss: 0.11263793706893921
Batch 28/64 loss: 0.130642831325531
Batch 29/64 loss: 0.1177830696105957
Batch 30/64 loss: 0.1353725790977478
Batch 31/64 loss: 0.09053754806518555
Batch 32/64 loss: 0.09204721450805664
Batch 33/64 loss: 0.12382209300994873
Batch 34/64 loss: 0.10701656341552734
Batch 35/64 loss: 0.11976134777069092
Batch 36/64 loss: 0.13915222883224487
Batch 37/64 loss: 0.11393296718597412
Batch 38/64 loss: 0.13128626346588135
Batch 39/64 loss: 0.12171435356140137
Batch 40/64 loss: 0.12965929508209229
Batch 41/64 loss: 0.12008261680603027
Batch 42/64 loss: 0.10156756639480591
Batch 43/64 loss: 0.10397976636886597
Batch 44/64 loss: 0.14524078369140625
Batch 45/64 loss: 0.13226449489593506
Batch 46/64 loss: 0.1242976188659668
Batch 47/64 loss: 0.1286950707435608
Batch 48/64 loss: 0.12722665071487427
Batch 49/64 loss: 0.12465643882751465
Batch 50/64 loss: 0.11095595359802246
Batch 51/64 loss: 0.12364685535430908
Batch 52/64 loss: 0.1272040605545044
Batch 53/64 loss: 0.13710778951644897
Batch 54/64 loss: 0.12759339809417725
Batch 55/64 loss: 0.1566178798675537
Batch 56/64 loss: 0.12340843677520752
Batch 57/64 loss: 0.09511667490005493
Batch 58/64 loss: 0.13039684295654297
Batch 59/64 loss: 0.12750065326690674
Batch 60/64 loss: 0.09907656908035278
Batch 61/64 loss: 0.12144076824188232
Batch 62/64 loss: 0.1376643180847168
Batch 63/64 loss: 0.10333383083343506
Batch 64/64 loss: 0.13284873962402344
Epoch 98  Train loss: 0.12052217558318494  Val loss: 0.151421806042137
Epoch 99
-------------------------------
Batch 1/64 loss: 0.13684332370758057
Batch 2/64 loss: 0.10194414854049683
Batch 3/64 loss: 0.11773598194122314
Batch 4/64 loss: 0.13118070363998413
Batch 5/64 loss: 0.12702643871307373
Batch 6/64 loss: 0.1325443983078003
Batch 7/64 loss: 0.1339637041091919
Batch 8/64 loss: 0.11369895935058594
Batch 9/64 loss: 0.11903560161590576
Batch 10/64 loss: 0.13233160972595215
Batch 11/64 loss: 0.13688814640045166
Batch 12/64 loss: 0.13302850723266602
Batch 13/64 loss: 0.09170442819595337
Batch 14/64 loss: 0.1277661919593811
Batch 15/64 loss: 0.1446835994720459
Batch 16/64 loss: 0.12588614225387573
Batch 17/64 loss: 0.10188496112823486
Batch 18/64 loss: 0.09156191349029541
Batch 19/64 loss: 0.10768455266952515
Batch 20/64 loss: 0.10851675271987915
Batch 21/64 loss: 0.12933117151260376
Batch 22/64 loss: 0.11306172609329224
Batch 23/64 loss: 0.11738109588623047
Batch 24/64 loss: 0.12650489807128906
Batch 25/64 loss: 0.1294049620628357
Batch 26/64 loss: 0.11454766988754272
Batch 27/64 loss: 0.09978759288787842
Batch 28/64 loss: 0.10410511493682861
Batch 29/64 loss: 0.11142849922180176
Batch 30/64 loss: 0.09844416379928589
Batch 31/64 loss: 0.09149551391601562
Batch 32/64 loss: 0.12727034091949463
Batch 33/64 loss: 0.1234481930732727
Batch 34/64 loss: 0.11424463987350464
Batch 35/64 loss: 0.08575308322906494
Batch 36/64 loss: 0.12731772661209106
Batch 37/64 loss: 0.11611580848693848
Batch 38/64 loss: 0.11904400587081909
Batch 39/64 loss: 0.12417805194854736
Batch 40/64 loss: 0.11478644609451294
Batch 41/64 loss: 0.10798287391662598
Batch 42/64 loss: 0.11955350637435913
Batch 43/64 loss: 0.15129441022872925
Batch 44/64 loss: 0.12678736448287964
Batch 45/64 loss: 0.13910996913909912
Batch 46/64 loss: 0.11949151754379272
Batch 47/64 loss: 0.12204313278198242
Batch 48/64 loss: 0.12884056568145752
Batch 49/64 loss: 0.10796493291854858
Batch 50/64 loss: 0.10934388637542725
Batch 51/64 loss: 0.133466899394989
Batch 52/64 loss: 0.12261795997619629
Batch 53/64 loss: 0.12821650505065918
Batch 54/64 loss: 0.11492198705673218
Batch 55/64 loss: 0.1420261263847351
Batch 56/64 loss: 0.10362261533737183
Batch 57/64 loss: 0.12693995237350464
Batch 58/64 loss: 0.12907671928405762
Batch 59/64 loss: 0.10472261905670166
Batch 60/64 loss: 0.11715424060821533
Batch 61/64 loss: 0.1228029727935791
Batch 62/64 loss: 0.12085753679275513
Batch 63/64 loss: 0.13443279266357422
Batch 64/64 loss: 0.11872458457946777
Epoch 99  Train loss: 0.11962157324248669  Val loss: 0.15049690151542322
Epoch 100
-------------------------------
Batch 1/64 loss: 0.11543911695480347
Batch 2/64 loss: 0.12786012887954712
Batch 3/64 loss: 0.1069909930229187
Batch 4/64 loss: 0.10598170757293701
Batch 5/64 loss: 0.1072760820388794
Batch 6/64 loss: 0.11489313840866089
Batch 7/64 loss: 0.12753045558929443
Batch 8/64 loss: 0.1171579360961914
Batch 9/64 loss: 0.12218457460403442
Batch 10/64 loss: 0.14829838275909424
Batch 11/64 loss: 0.13524973392486572
Batch 12/64 loss: 0.11357790231704712
Batch 13/64 loss: 0.11344993114471436
Batch 14/64 loss: 0.09281677007675171
Batch 15/64 loss: 0.12116414308547974
Batch 16/64 loss: 0.12886905670166016
Batch 17/64 loss: 0.10947757959365845
Batch 18/64 loss: 0.1320403814315796
Batch 19/64 loss: 0.09195882081985474
Batch 20/64 loss: 0.10469597578048706
Batch 21/64 loss: 0.11735868453979492
Batch 22/64 loss: 0.12221479415893555
Batch 23/64 loss: 0.11136341094970703
Batch 24/64 loss: 0.1466560959815979
Batch 25/64 loss: 0.11820149421691895
Batch 26/64 loss: 0.1288960576057434
Batch 27/64 loss: 0.14337337017059326
Batch 28/64 loss: 0.11863231658935547
Batch 29/64 loss: 0.14137786626815796
Batch 30/64 loss: 0.13841676712036133
Batch 31/64 loss: 0.11767137050628662
Batch 32/64 loss: 0.10693091154098511
Batch 33/64 loss: 0.09291791915893555
Batch 34/64 loss: 0.10863298177719116
Batch 35/64 loss: 0.14166951179504395
Batch 36/64 loss: 0.10912060737609863
Batch 37/64 loss: 0.11917179822921753
Batch 38/64 loss: 0.12123942375183105
Batch 39/64 loss: 0.10923415422439575
Batch 40/64 loss: 0.1112443208694458
Batch 41/64 loss: 0.13379216194152832
Batch 42/64 loss: 0.1016739010810852
Batch 43/64 loss: 0.11185789108276367
Batch 44/64 loss: 0.0927121639251709
Batch 45/64 loss: 0.10270839929580688
Batch 46/64 loss: 0.11168479919433594
Batch 47/64 loss: 0.11438357830047607
Batch 48/64 loss: 0.12540000677108765
Batch 49/64 loss: 0.09467566013336182
Batch 50/64 loss: 0.13686823844909668
Batch 51/64 loss: 0.13351356983184814
Batch 52/64 loss: 0.11510241031646729
Batch 53/64 loss: 0.15996074676513672
Batch 54/64 loss: 0.11738139390945435
Batch 55/64 loss: 0.11306345462799072
Batch 56/64 loss: 0.11625796556472778
Batch 57/64 loss: 0.11940604448318481
Batch 58/64 loss: 0.11982369422912598
Batch 59/64 loss: 0.10847711563110352
Batch 60/64 loss: 0.11079949140548706
Batch 61/64 loss: 0.14717984199523926
Batch 62/64 loss: 0.11225831508636475
Batch 63/64 loss: 0.12192344665527344
Batch 64/64 loss: 0.12337911128997803
Epoch 100  Train loss: 0.11881843594943776  Val loss: 0.1476252453843343
Saving best model, epoch: 100
Epoch 101
-------------------------------
Batch 1/64 loss: 0.12026101350784302
Batch 2/64 loss: 0.09289973974227905
Batch 3/64 loss: 0.1160280704498291
Batch 4/64 loss: 0.11571288108825684
Batch 5/64 loss: 0.10769134759902954
Batch 6/64 loss: 0.09893310070037842
Batch 7/64 loss: 0.11486375331878662
Batch 8/64 loss: 0.11426115036010742
Batch 9/64 loss: 0.09160023927688599
Batch 10/64 loss: 0.11446046829223633
Batch 11/64 loss: 0.09026259183883667
Batch 12/64 loss: 0.11762058734893799
Batch 13/64 loss: 0.10592639446258545
Batch 14/64 loss: 0.13832706212997437
Batch 15/64 loss: 0.1307765245437622
Batch 16/64 loss: 0.11902725696563721
Batch 17/64 loss: 0.12897199392318726
Batch 18/64 loss: 0.10328835248947144
Batch 19/64 loss: 0.117462158203125
Batch 20/64 loss: 0.11275672912597656
Batch 21/64 loss: 0.12336134910583496
Batch 22/64 loss: 0.09468138217926025
Batch 23/64 loss: 0.11343473196029663
Batch 24/64 loss: 0.12247049808502197
Batch 25/64 loss: 0.11879098415374756
Batch 26/64 loss: 0.11844098567962646
Batch 27/64 loss: 0.14255040884017944
Batch 28/64 loss: 0.09572350978851318
Batch 29/64 loss: 0.1435459852218628
Batch 30/64 loss: 0.10240441560745239
Batch 31/64 loss: 0.12982070446014404
Batch 32/64 loss: 0.1176598072052002
Batch 33/64 loss: 0.10636526346206665
Batch 34/64 loss: 0.11492377519607544
Batch 35/64 loss: 0.12121719121932983
Batch 36/64 loss: 0.12470954656600952
Batch 37/64 loss: 0.12033361196517944
Batch 38/64 loss: 0.14660340547561646
Batch 39/64 loss: 0.10566812753677368
Batch 40/64 loss: 0.12707990407943726
Batch 41/64 loss: 0.12961715459823608
Batch 42/64 loss: 0.13762551546096802
Batch 43/64 loss: 0.1464042067527771
Batch 44/64 loss: 0.10683351755142212
Batch 45/64 loss: 0.11414206027984619
Batch 46/64 loss: 0.10557663440704346
Batch 47/64 loss: 0.1271466612815857
Batch 48/64 loss: 0.15347886085510254
Batch 49/64 loss: 0.09048295021057129
Batch 50/64 loss: 0.11681801080703735
Batch 51/64 loss: 0.13990527391433716
Batch 52/64 loss: 0.1161576509475708
Batch 53/64 loss: 0.10708290338516235
Batch 54/64 loss: 0.1025649905204773
Batch 55/64 loss: 0.11803430318832397
Batch 56/64 loss: 0.13414496183395386
Batch 57/64 loss: 0.10383325815200806
Batch 58/64 loss: 0.0871114730834961
Batch 59/64 loss: 0.1412513256072998
Batch 60/64 loss: 0.12864184379577637
Batch 61/64 loss: 0.10292243957519531
Batch 62/64 loss: 0.13260114192962646
Batch 63/64 loss: 0.12073230743408203
Batch 64/64 loss: 0.1057356595993042
Epoch 101  Train loss: 0.11738554028903737  Val loss: 0.15126517015634125
Epoch 102
-------------------------------
Batch 1/64 loss: 0.10685932636260986
Batch 2/64 loss: 0.10595685243606567
Batch 3/64 loss: 0.09554457664489746
Batch 4/64 loss: 0.13062191009521484
Batch 5/64 loss: 0.10686111450195312
Batch 6/64 loss: 0.11418652534484863
Batch 7/64 loss: 0.09352445602416992
Batch 8/64 loss: 0.11537009477615356
Batch 9/64 loss: 0.12208050489425659
Batch 10/64 loss: 0.10398435592651367
Batch 11/64 loss: 0.10392647981643677
Batch 12/64 loss: 0.1255890130996704
Batch 13/64 loss: 0.11104679107666016
Batch 14/64 loss: 0.10506337881088257
Batch 15/64 loss: 0.11665338277816772
Batch 16/64 loss: 0.11351275444030762
Batch 17/64 loss: 0.13406699895858765
Batch 18/64 loss: 0.13358628749847412
Batch 19/64 loss: 0.09365677833557129
Batch 20/64 loss: 0.13536220788955688
Batch 21/64 loss: 0.10302668809890747
Batch 22/64 loss: 0.1090397834777832
Batch 23/64 loss: 0.08617109060287476
Batch 24/64 loss: 0.13476812839508057
Batch 25/64 loss: 0.1446002721786499
Batch 26/64 loss: 0.106783926486969
Batch 27/64 loss: 0.13892149925231934
Batch 28/64 loss: 0.10906630754470825
Batch 29/64 loss: 0.09876769781112671
Batch 30/64 loss: 0.14042168855667114
Batch 31/64 loss: 0.10579556226730347
Batch 32/64 loss: 0.12665462493896484
Batch 33/64 loss: 0.13234519958496094
Batch 34/64 loss: 0.13567328453063965
Batch 35/64 loss: 0.08551579713821411
Batch 36/64 loss: 0.10476398468017578
Batch 37/64 loss: 0.11449241638183594
Batch 38/64 loss: 0.10507971048355103
Batch 39/64 loss: 0.11622351408004761
Batch 40/64 loss: 0.13150525093078613
Batch 41/64 loss: 0.11695337295532227
Batch 42/64 loss: 0.11708122491836548
Batch 43/64 loss: 0.12236934900283813
Batch 44/64 loss: 0.11275070905685425
Batch 45/64 loss: 0.0834040641784668
Batch 46/64 loss: 0.1283177137374878
Batch 47/64 loss: 0.13475489616394043
Batch 48/64 loss: 0.10793560743331909
Batch 49/64 loss: 0.11019551753997803
Batch 50/64 loss: 0.128994882106781
Batch 51/64 loss: 0.10952723026275635
Batch 52/64 loss: 0.13550961017608643
Batch 53/64 loss: 0.12179023027420044
Batch 54/64 loss: 0.11700373888015747
Batch 55/64 loss: 0.1087537407875061
Batch 56/64 loss: 0.10000330209732056
Batch 57/64 loss: 0.11203497648239136
Batch 58/64 loss: 0.11674189567565918
Batch 59/64 loss: 0.12036925554275513
Batch 60/64 loss: 0.11576449871063232
Batch 61/64 loss: 0.10530465841293335
Batch 62/64 loss: 0.12217754125595093
Batch 63/64 loss: 0.126373291015625
Batch 64/64 loss: 0.1389053463935852
Epoch 102  Train loss: 0.11569192993874643  Val loss: 0.14986479159482977
Epoch 103
-------------------------------
Batch 1/64 loss: 0.1099170446395874
Batch 2/64 loss: 0.11807984113693237
Batch 3/64 loss: 0.11028414964675903
Batch 4/64 loss: 0.12449485063552856
Batch 5/64 loss: 0.12041282653808594
Batch 6/64 loss: 0.09421509504318237
Batch 7/64 loss: 0.09707236289978027
Batch 8/64 loss: 0.11726802587509155
Batch 9/64 loss: 0.10716855525970459
Batch 10/64 loss: 0.1177743673324585
Batch 11/64 loss: 0.10409927368164062
Batch 12/64 loss: 0.11233067512512207
Batch 13/64 loss: 0.13482201099395752
Batch 14/64 loss: 0.12545502185821533
Batch 15/64 loss: 0.10806488990783691
Batch 16/64 loss: 0.11042815446853638
Batch 17/64 loss: 0.10891067981719971
Batch 18/64 loss: 0.10414570569992065
Batch 19/64 loss: 0.1206897497177124
Batch 20/64 loss: 0.10371917486190796
Batch 21/64 loss: 0.10301578044891357
Batch 22/64 loss: 0.11163163185119629
Batch 23/64 loss: 0.14832913875579834
Batch 24/64 loss: 0.09387898445129395
Batch 25/64 loss: 0.11019635200500488
Batch 26/64 loss: 0.12378960847854614
Batch 27/64 loss: 0.10261601209640503
Batch 28/64 loss: 0.13607549667358398
Batch 29/64 loss: 0.10529285669326782
Batch 30/64 loss: 0.13544237613677979
Batch 31/64 loss: 0.14512169361114502
Batch 32/64 loss: 0.11135667562484741
Batch 33/64 loss: 0.13085389137268066
Batch 34/64 loss: 0.0986936092376709
Batch 35/64 loss: 0.12394386529922485
Batch 36/64 loss: 0.12395483255386353
Batch 37/64 loss: 0.12558478116989136
Batch 38/64 loss: 0.11403751373291016
Batch 39/64 loss: 0.1440313458442688
Batch 40/64 loss: 0.15225082635879517
Batch 41/64 loss: 0.09942150115966797
Batch 42/64 loss: 0.1316295862197876
Batch 43/64 loss: 0.13527119159698486
Batch 44/64 loss: 0.12162196636199951
Batch 45/64 loss: 0.13381683826446533
Batch 46/64 loss: 0.13789379596710205
Batch 47/64 loss: 0.10562330484390259
Batch 48/64 loss: 0.1395559310913086
Batch 49/64 loss: 0.12083929777145386
Batch 50/64 loss: 0.1144101619720459
Batch 51/64 loss: 0.0776640772819519
Batch 52/64 loss: 0.1152181625366211
Batch 53/64 loss: 0.13695675134658813
Batch 54/64 loss: 0.0981399416923523
Batch 55/64 loss: 0.12293803691864014
Batch 56/64 loss: 0.09935444593429565
Batch 57/64 loss: 0.128831148147583
Batch 58/64 loss: 0.09489697217941284
Batch 59/64 loss: 0.09917885065078735
Batch 60/64 loss: 0.11310333013534546
Batch 61/64 loss: 0.1307719349861145
Batch 62/64 loss: 0.1227608323097229
Batch 63/64 loss: 0.11271345615386963
Batch 64/64 loss: 0.12887030839920044
Epoch 103  Train loss: 0.11731316038206512  Val loss: 0.14726657515129274
Saving best model, epoch: 103
Epoch 104
-------------------------------
Batch 1/64 loss: 0.1213374137878418
Batch 2/64 loss: 0.11365664005279541
Batch 3/64 loss: 0.1135406494140625
Batch 4/64 loss: 0.1049838662147522
Batch 5/64 loss: 0.09742563962936401
Batch 6/64 loss: 0.12726515531539917
Batch 7/64 loss: 0.09243786334991455
Batch 8/64 loss: 0.1152535080909729
Batch 9/64 loss: 0.11452341079711914
Batch 10/64 loss: 0.10287171602249146
Batch 11/64 loss: 0.12165993452072144
Batch 12/64 loss: 0.11260861158370972
Batch 13/64 loss: 0.11849957704544067
Batch 14/64 loss: 0.11092078685760498
Batch 15/64 loss: 0.11465948820114136
Batch 16/64 loss: 0.13238096237182617
Batch 17/64 loss: 0.10040098428726196
Batch 18/64 loss: 0.122089684009552
Batch 19/64 loss: 0.10890853404998779
Batch 20/64 loss: 0.11459177732467651
Batch 21/64 loss: 0.12501555681228638
Batch 22/64 loss: 0.13389569520950317
Batch 23/64 loss: 0.12293475866317749
Batch 24/64 loss: 0.11435461044311523
Batch 25/64 loss: 0.11526918411254883
Batch 26/64 loss: 0.11473941802978516
Batch 27/64 loss: 0.106991708278656
Batch 28/64 loss: 0.11955630779266357
Batch 29/64 loss: 0.11931198835372925
Batch 30/64 loss: 0.11487209796905518
Batch 31/64 loss: 0.14638996124267578
Batch 32/64 loss: 0.11753463745117188
Batch 33/64 loss: 0.1282394528388977
Batch 34/64 loss: 0.10973423719406128
Batch 35/64 loss: 0.1327376365661621
Batch 36/64 loss: 0.11978918313980103
Batch 37/64 loss: 0.08435946702957153
Batch 38/64 loss: 0.1178237795829773
Batch 39/64 loss: 0.1231837272644043
Batch 40/64 loss: 0.1070713996887207
Batch 41/64 loss: 0.1031295657157898
Batch 42/64 loss: 0.1001960039138794
Batch 43/64 loss: 0.13586074113845825
Batch 44/64 loss: 0.10750150680541992
Batch 45/64 loss: 0.1165086030960083
Batch 46/64 loss: 0.10156184434890747
Batch 47/64 loss: 0.09901964664459229
Batch 48/64 loss: 0.10444915294647217
Batch 49/64 loss: 0.09649020433425903
Batch 50/64 loss: 0.09766888618469238
Batch 51/64 loss: 0.11154723167419434
Batch 52/64 loss: 0.1340457797050476
Batch 53/64 loss: 0.1167873740196228
Batch 54/64 loss: 0.10784083604812622
Batch 55/64 loss: 0.11843669414520264
Batch 56/64 loss: 0.13729584217071533
Batch 57/64 loss: 0.13182133436203003
Batch 58/64 loss: 0.13600462675094604
Batch 59/64 loss: 0.11529427766799927
Batch 60/64 loss: 0.12393122911453247
Batch 61/64 loss: 0.1321449875831604
Batch 62/64 loss: 0.13057374954223633
Batch 63/64 loss: 0.1271294355392456
Batch 64/64 loss: 0.1269078254699707
Epoch 104  Train loss: 0.1163018264022528  Val loss: 0.14817921238666548
Epoch 105
-------------------------------
Batch 1/64 loss: 0.10817933082580566
Batch 2/64 loss: 0.10679328441619873
Batch 3/64 loss: 0.11491847038269043
Batch 4/64 loss: 0.10005038976669312
Batch 5/64 loss: 0.1263132095336914
Batch 6/64 loss: 0.10106813907623291
Batch 7/64 loss: 0.09792137145996094
Batch 8/64 loss: 0.12876278162002563
Batch 9/64 loss: 0.09071433544158936
Batch 10/64 loss: 0.12032026052474976
Batch 11/64 loss: 0.09738224744796753
Batch 12/64 loss: 0.13751041889190674
Batch 13/64 loss: 0.11020231246948242
Batch 14/64 loss: 0.1245948076248169
Batch 15/64 loss: 0.12732285261154175
Batch 16/64 loss: 0.09278833866119385
Batch 17/64 loss: 0.11018800735473633
Batch 18/64 loss: 0.12425696849822998
Batch 19/64 loss: 0.15922331809997559
Batch 20/64 loss: 0.12421441078186035
Batch 21/64 loss: 0.0995478630065918
Batch 22/64 loss: 0.1254766583442688
Batch 23/64 loss: 0.10153806209564209
Batch 24/64 loss: 0.12768900394439697
Batch 25/64 loss: 0.13502222299575806
Batch 26/64 loss: 0.08588564395904541
Batch 27/64 loss: 0.11153388023376465
Batch 28/64 loss: 0.12612879276275635
Batch 29/64 loss: 0.12373971939086914
Batch 30/64 loss: 0.10602670907974243
Batch 31/64 loss: 0.13039028644561768
Batch 32/64 loss: 0.11648118495941162
Batch 33/64 loss: 0.10607665777206421
Batch 34/64 loss: 0.12188273668289185
Batch 35/64 loss: 0.11107617616653442
Batch 36/64 loss: 0.1531151533126831
Batch 37/64 loss: 0.10661888122558594
Batch 38/64 loss: 0.10387277603149414
Batch 39/64 loss: 0.10445165634155273
Batch 40/64 loss: 0.08812296390533447
Batch 41/64 loss: 0.11050426959991455
Batch 42/64 loss: 0.11277079582214355
Batch 43/64 loss: 0.09666121006011963
Batch 44/64 loss: 0.10717463493347168
Batch 45/64 loss: 0.11910533905029297
Batch 46/64 loss: 0.09938257932662964
Batch 47/64 loss: 0.10555422306060791
Batch 48/64 loss: 0.13047575950622559
Batch 49/64 loss: 0.1412990689277649
Batch 50/64 loss: 0.10997861623764038
Batch 51/64 loss: 0.11430621147155762
Batch 52/64 loss: 0.13591021299362183
Batch 53/64 loss: 0.10776841640472412
Batch 54/64 loss: 0.11470562219619751
Batch 55/64 loss: 0.11432135105133057
Batch 56/64 loss: 0.11957347393035889
Batch 57/64 loss: 0.08425313234329224
Batch 58/64 loss: 0.1212802529335022
Batch 59/64 loss: 0.11000621318817139
Batch 60/64 loss: 0.1245298981666565
Batch 61/64 loss: 0.11095589399337769
Batch 62/64 loss: 0.1183881163597107
Batch 63/64 loss: 0.11897307634353638
Batch 64/64 loss: 0.1295068860054016
Epoch 105  Train loss: 0.11470448339686674  Val loss: 0.14810310974973173
Epoch 106
-------------------------------
Batch 1/64 loss: 0.10407376289367676
Batch 2/64 loss: 0.13007229566574097
Batch 3/64 loss: 0.11162066459655762
Batch 4/64 loss: 0.09185934066772461
Batch 5/64 loss: 0.13918018341064453
Batch 6/64 loss: 0.1168982982635498
Batch 7/64 loss: 0.12002885341644287
Batch 8/64 loss: 0.11908340454101562
Batch 9/64 loss: 0.09724879264831543
Batch 10/64 loss: 0.10993504524230957
Batch 11/64 loss: 0.1051519513130188
Batch 12/64 loss: 0.09584778547286987
Batch 13/64 loss: 0.10711026191711426
Batch 14/64 loss: 0.10987824201583862
Batch 15/64 loss: 0.1125686764717102
Batch 16/64 loss: 0.1265520453453064
Batch 17/64 loss: 0.08904522657394409
Batch 18/64 loss: 0.10858243703842163
Batch 19/64 loss: 0.09823936223983765
Batch 20/64 loss: 0.1123926043510437
Batch 21/64 loss: 0.10412043333053589
Batch 22/64 loss: 0.08779823780059814
Batch 23/64 loss: 0.10836315155029297
Batch 24/64 loss: 0.09220153093338013
Batch 25/64 loss: 0.08675253391265869
Batch 26/64 loss: 0.13044887781143188
Batch 27/64 loss: 0.11804068088531494
Batch 28/64 loss: 0.09976756572723389
Batch 29/64 loss: 0.1101999282836914
Batch 30/64 loss: 0.11106288433074951
Batch 31/64 loss: 0.1182146668434143
Batch 32/64 loss: 0.09225726127624512
Batch 33/64 loss: 0.11067855358123779
Batch 34/64 loss: 0.12087416648864746
Batch 35/64 loss: 0.13545668125152588
Batch 36/64 loss: 0.11596882343292236
Batch 37/64 loss: 0.10652554035186768
Batch 38/64 loss: 0.10006821155548096
Batch 39/64 loss: 0.12199252843856812
Batch 40/64 loss: 0.12399375438690186
Batch 41/64 loss: 0.11114943027496338
Batch 42/64 loss: 0.11906707286834717
Batch 43/64 loss: 0.12818241119384766
Batch 44/64 loss: 0.11842817068099976
Batch 45/64 loss: 0.14029932022094727
Batch 46/64 loss: 0.13520365953445435
Batch 47/64 loss: 0.10628807544708252
Batch 48/64 loss: 0.11432778835296631
Batch 49/64 loss: 0.12686395645141602
Batch 50/64 loss: 0.12657099962234497
Batch 51/64 loss: 0.10042572021484375
Batch 52/64 loss: 0.11259925365447998
Batch 53/64 loss: 0.12975597381591797
Batch 54/64 loss: 0.11503762006759644
Batch 55/64 loss: 0.11883819103240967
Batch 56/64 loss: 0.15905344486236572
Batch 57/64 loss: 0.1349390149116516
Batch 58/64 loss: 0.12450402975082397
Batch 59/64 loss: 0.11628139019012451
Batch 60/64 loss: 0.10369497537612915
Batch 61/64 loss: 0.12682557106018066
Batch 62/64 loss: 0.1024520993232727
Batch 63/64 loss: 0.13142812252044678
Batch 64/64 loss: 0.1409924030303955
Epoch 106  Train loss: 0.11463711121503045  Val loss: 0.14714740150163264
Saving best model, epoch: 106
Epoch 107
-------------------------------
Batch 1/64 loss: 0.13444417715072632
Batch 2/64 loss: 0.09255951642990112
Batch 3/64 loss: 0.10395950078964233
Batch 4/64 loss: 0.13404852151870728
Batch 5/64 loss: 0.10806596279144287
Batch 6/64 loss: 0.10789626836776733
Batch 7/64 loss: 0.11784839630126953
Batch 8/64 loss: 0.13024872541427612
Batch 9/64 loss: 0.08946311473846436
Batch 10/64 loss: 0.0760115385055542
Batch 11/64 loss: 0.12297111749649048
Batch 12/64 loss: 0.0904078483581543
Batch 13/64 loss: 0.10009187459945679
Batch 14/64 loss: 0.1362590193748474
Batch 15/64 loss: 0.09686493873596191
Batch 16/64 loss: 0.11161649227142334
Batch 17/64 loss: 0.09095251560211182
Batch 18/64 loss: 0.1272180676460266
Batch 19/64 loss: 0.11239826679229736
Batch 20/64 loss: 0.11933493614196777
Batch 21/64 loss: 0.10087502002716064
Batch 22/64 loss: 0.122211754322052
Batch 23/64 loss: 0.10157239437103271
Batch 24/64 loss: 0.13608139753341675
Batch 25/64 loss: 0.11962658166885376
Batch 26/64 loss: 0.10107654333114624
Batch 27/64 loss: 0.12510603666305542
Batch 28/64 loss: 0.12489771842956543
Batch 29/64 loss: 0.10588973760604858
Batch 30/64 loss: 0.11053228378295898
Batch 31/64 loss: 0.10766333341598511
Batch 32/64 loss: 0.11876851320266724
Batch 33/64 loss: 0.10860884189605713
Batch 34/64 loss: 0.12927889823913574
Batch 35/64 loss: 0.1019173264503479
Batch 36/64 loss: 0.11815732717514038
Batch 37/64 loss: 0.11464565992355347
Batch 38/64 loss: 0.10572898387908936
Batch 39/64 loss: 0.10702306032180786
Batch 40/64 loss: 0.12404060363769531
Batch 41/64 loss: 0.11731594800949097
Batch 42/64 loss: 0.13150078058242798
Batch 43/64 loss: 0.1315181851387024
Batch 44/64 loss: 0.10978740453720093
Batch 45/64 loss: 0.1142122745513916
Batch 46/64 loss: 0.11830794811248779
Batch 47/64 loss: 0.1342254877090454
Batch 48/64 loss: 0.13410955667495728
Batch 49/64 loss: 0.15586817264556885
Batch 50/64 loss: 0.11446201801300049
Batch 51/64 loss: 0.10074573755264282
Batch 52/64 loss: 0.09581834077835083
Batch 53/64 loss: 0.10462003946304321
Batch 54/64 loss: 0.11512643098831177
Batch 55/64 loss: 0.12018513679504395
Batch 56/64 loss: 0.12505394220352173
Batch 57/64 loss: 0.09549826383590698
Batch 58/64 loss: 0.0957689881324768
Batch 59/64 loss: 0.11883950233459473
Batch 60/64 loss: 0.1144399642944336
Batch 61/64 loss: 0.09852159023284912
Batch 62/64 loss: 0.12611454725265503
Batch 63/64 loss: 0.13180458545684814
Batch 64/64 loss: 0.11894732713699341
Epoch 107  Train loss: 0.11418695145962285  Val loss: 0.14709389598918535
Saving best model, epoch: 107
Epoch 108
-------------------------------
Batch 1/64 loss: 0.11544466018676758
Batch 2/64 loss: 0.1310138702392578
Batch 3/64 loss: 0.12198227643966675
Batch 4/64 loss: 0.08053076267242432
Batch 5/64 loss: 0.13205444812774658
Batch 6/64 loss: 0.1448482871055603
Batch 7/64 loss: 0.1419835090637207
Batch 8/64 loss: 0.1161457896232605
Batch 9/64 loss: 0.11650848388671875
Batch 10/64 loss: 0.12604546546936035
Batch 11/64 loss: 0.0983433723449707
Batch 12/64 loss: 0.11061352491378784
Batch 13/64 loss: 0.11857497692108154
Batch 14/64 loss: 0.07984870672225952
Batch 15/64 loss: 0.11836063861846924
Batch 16/64 loss: 0.12401723861694336
Batch 17/64 loss: 0.07755309343338013
Batch 18/64 loss: 0.10882389545440674
Batch 19/64 loss: 0.10991567373275757
Batch 20/64 loss: 0.10693061351776123
Batch 21/64 loss: 0.10286515951156616
Batch 22/64 loss: 0.12484049797058105
Batch 23/64 loss: 0.13395875692367554
Batch 24/64 loss: 0.10190939903259277
Batch 25/64 loss: 0.10914510488510132
Batch 26/64 loss: 0.1032710075378418
Batch 27/64 loss: 0.1120079755783081
Batch 28/64 loss: 0.09353023767471313
Batch 29/64 loss: 0.13674849271774292
Batch 30/64 loss: 0.11477351188659668
Batch 31/64 loss: 0.09750008583068848
Batch 32/64 loss: 0.08727514743804932
Batch 33/64 loss: 0.12290221452713013
Batch 34/64 loss: 0.09377038478851318
Batch 35/64 loss: 0.11436593532562256
Batch 36/64 loss: 0.12707602977752686
Batch 37/64 loss: 0.09702140092849731
Batch 38/64 loss: 0.09838974475860596
Batch 39/64 loss: 0.1396390199661255
Batch 40/64 loss: 0.11490678787231445
Batch 41/64 loss: 0.11869585514068604
Batch 42/64 loss: 0.1267409324645996
Batch 43/64 loss: 0.11144441366195679
Batch 44/64 loss: 0.09927493333816528
Batch 45/64 loss: 0.1186833381652832
Batch 46/64 loss: 0.08957278728485107
Batch 47/64 loss: 0.10453695058822632
Batch 48/64 loss: 0.1078529953956604
Batch 49/64 loss: 0.12115049362182617
Batch 50/64 loss: 0.11236840486526489
Batch 51/64 loss: 0.11286526918411255
Batch 52/64 loss: 0.12934505939483643
Batch 53/64 loss: 0.12081331014633179
Batch 54/64 loss: 0.10689949989318848
Batch 55/64 loss: 0.11184221506118774
Batch 56/64 loss: 0.11688095331192017
Batch 57/64 loss: 0.12188422679901123
Batch 58/64 loss: 0.10459738969802856
Batch 59/64 loss: 0.11136353015899658
Batch 60/64 loss: 0.11881530284881592
Batch 61/64 loss: 0.11203491687774658
Batch 62/64 loss: 0.11250913143157959
Batch 63/64 loss: 0.11240321397781372
Batch 64/64 loss: 0.1073651909828186
Epoch 108  Train loss: 0.11276133644814584  Val loss: 0.14526103759549328
Saving best model, epoch: 108
Epoch 109
-------------------------------
Batch 1/64 loss: 0.12452572584152222
Batch 2/64 loss: 0.10928821563720703
Batch 3/64 loss: 0.07876646518707275
Batch 4/64 loss: 0.08594745397567749
Batch 5/64 loss: 0.13399171829223633
Batch 6/64 loss: 0.10917907953262329
Batch 7/64 loss: 0.12485527992248535
Batch 8/64 loss: 0.09630095958709717
Batch 9/64 loss: 0.09056717157363892
Batch 10/64 loss: 0.09688258171081543
Batch 11/64 loss: 0.12762248516082764
Batch 12/64 loss: 0.10157948732376099
Batch 13/64 loss: 0.09929478168487549
Batch 14/64 loss: 0.10820406675338745
Batch 15/64 loss: 0.12253332138061523
Batch 16/64 loss: 0.10504007339477539
Batch 17/64 loss: 0.09929823875427246
Batch 18/64 loss: 0.12202441692352295
Batch 19/64 loss: 0.11761742830276489
Batch 20/64 loss: 0.09591996669769287
Batch 21/64 loss: 0.10824227333068848
Batch 22/64 loss: 0.1273806095123291
Batch 23/64 loss: 0.12341189384460449
Batch 24/64 loss: 0.10887634754180908
Batch 25/64 loss: 0.12967824935913086
Batch 26/64 loss: 0.09337103366851807
Batch 27/64 loss: 0.08358889818191528
Batch 28/64 loss: 0.1312735676765442
Batch 29/64 loss: 0.10653072595596313
Batch 30/64 loss: 0.10304069519042969
Batch 31/64 loss: 0.10848939418792725
Batch 32/64 loss: 0.12216341495513916
Batch 33/64 loss: 0.13000112771987915
Batch 34/64 loss: 0.10194790363311768
Batch 35/64 loss: 0.14086633920669556
Batch 36/64 loss: 0.10837960243225098
Batch 37/64 loss: 0.1240033507347107
Batch 38/64 loss: 0.10269153118133545
Batch 39/64 loss: 0.08786308765411377
Batch 40/64 loss: 0.09850800037384033
Batch 41/64 loss: 0.12313872575759888
Batch 42/64 loss: 0.11847579479217529
Batch 43/64 loss: 0.1261020302772522
Batch 44/64 loss: 0.1043093204498291
Batch 45/64 loss: 0.11390846967697144
Batch 46/64 loss: 0.0956268310546875
Batch 47/64 loss: 0.133247971534729
Batch 48/64 loss: 0.1315506100654602
Batch 49/64 loss: 0.11002236604690552
Batch 50/64 loss: 0.11883831024169922
Batch 51/64 loss: 0.08098822832107544
Batch 52/64 loss: 0.11141592264175415
Batch 53/64 loss: 0.11684060096740723
Batch 54/64 loss: 0.13869422674179077
Batch 55/64 loss: 0.11324864625930786
Batch 56/64 loss: 0.14378470182418823
Batch 57/64 loss: 0.1103130578994751
Batch 58/64 loss: 0.11985492706298828
Batch 59/64 loss: 0.10695624351501465
Batch 60/64 loss: 0.12433624267578125
Batch 61/64 loss: 0.12564611434936523
Batch 62/64 loss: 0.10747992992401123
Batch 63/64 loss: 0.1155816912651062
Batch 64/64 loss: 0.12688148021697998
Epoch 109  Train loss: 0.1125532398036882  Val loss: 0.14613038690639116
Epoch 110
-------------------------------
Batch 1/64 loss: 0.0997355580329895
Batch 2/64 loss: 0.09503179788589478
Batch 3/64 loss: 0.08556652069091797
Batch 4/64 loss: 0.1027672290802002
Batch 5/64 loss: 0.11232900619506836
Batch 6/64 loss: 0.10073363780975342
Batch 7/64 loss: 0.12883001565933228
Batch 8/64 loss: 0.1182788610458374
Batch 9/64 loss: 0.11886411905288696
Batch 10/64 loss: 0.11054915189743042
Batch 11/64 loss: 0.09390205144882202
Batch 12/64 loss: 0.09220755100250244
Batch 13/64 loss: 0.1152692437171936
Batch 14/64 loss: 0.12096118927001953
Batch 15/64 loss: 0.10419058799743652
Batch 16/64 loss: 0.1382567286491394
Batch 17/64 loss: 0.11365377902984619
Batch 18/64 loss: 0.10573184490203857
Batch 19/64 loss: 0.09393489360809326
Batch 20/64 loss: 0.10503220558166504
Batch 21/64 loss: 0.12230342626571655
Batch 22/64 loss: 0.10395747423171997
Batch 23/64 loss: 0.10696607828140259
Batch 24/64 loss: 0.10076713562011719
Batch 25/64 loss: 0.13456422090530396
Batch 26/64 loss: 0.0978844165802002
Batch 27/64 loss: 0.12028002738952637
Batch 28/64 loss: 0.10043990612030029
Batch 29/64 loss: 0.1332351565361023
Batch 30/64 loss: 0.1107977032661438
Batch 31/64 loss: 0.12293720245361328
Batch 32/64 loss: 0.10226970911026001
Batch 33/64 loss: 0.10065114498138428
Batch 34/64 loss: 0.09740859270095825
Batch 35/64 loss: 0.1215943694114685
Batch 36/64 loss: 0.13095760345458984
Batch 37/64 loss: 0.12133973836898804
Batch 38/64 loss: 0.11233294010162354
Batch 39/64 loss: 0.1199195384979248
Batch 40/64 loss: 0.11326605081558228
Batch 41/64 loss: 0.12137073278427124
Batch 42/64 loss: 0.10748308897018433
Batch 43/64 loss: 0.12449955940246582
Batch 44/64 loss: 0.12653225660324097
Batch 45/64 loss: 0.10541117191314697
Batch 46/64 loss: 0.12824159860610962
Batch 47/64 loss: 0.10652792453765869
Batch 48/64 loss: 0.14652353525161743
Batch 49/64 loss: 0.12596404552459717
Batch 50/64 loss: 0.09408855438232422
Batch 51/64 loss: 0.11092406511306763
Batch 52/64 loss: 0.1299879550933838
Batch 53/64 loss: 0.1509566307067871
Batch 54/64 loss: 0.09745419025421143
Batch 55/64 loss: 0.10501748323440552
Batch 56/64 loss: 0.10128021240234375
Batch 57/64 loss: 0.10956132411956787
Batch 58/64 loss: 0.11648011207580566
Batch 59/64 loss: 0.10548549890518188
Batch 60/64 loss: 0.10302257537841797
Batch 61/64 loss: 0.10763794183731079
Batch 62/64 loss: 0.11215686798095703
Batch 63/64 loss: 0.13814419507980347
Batch 64/64 loss: 0.1271226406097412
Epoch 110  Train loss: 0.11293791228649663  Val loss: 0.14638569469714083
Epoch 111
-------------------------------
Batch 1/64 loss: 0.11584311723709106
Batch 2/64 loss: 0.10905396938323975
Batch 3/64 loss: 0.10854774713516235
Batch 4/64 loss: 0.15655601024627686
Batch 5/64 loss: 0.1137012243270874
Batch 6/64 loss: 0.1424398422241211
Batch 7/64 loss: 0.14397156238555908
Batch 8/64 loss: 0.10629981756210327
Batch 9/64 loss: 0.10870742797851562
Batch 10/64 loss: 0.10469025373458862
Batch 11/64 loss: 0.087124764919281
Batch 12/64 loss: 0.10542953014373779
Batch 13/64 loss: 0.10697543621063232
Batch 14/64 loss: 0.11075961589813232
Batch 15/64 loss: 0.11467915773391724
Batch 16/64 loss: 0.10306286811828613
Batch 17/64 loss: 0.11258566379547119
Batch 18/64 loss: 0.10410439968109131
Batch 19/64 loss: 0.1324368119239807
Batch 20/64 loss: 0.13737988471984863
Batch 21/64 loss: 0.09193843603134155
Batch 22/64 loss: 0.09355467557907104
Batch 23/64 loss: 0.10933727025985718
Batch 24/64 loss: 0.10821855068206787
Batch 25/64 loss: 0.08813995122909546
Batch 26/64 loss: 0.10875004529953003
Batch 27/64 loss: 0.1145583987236023
Batch 28/64 loss: 0.10410100221633911
Batch 29/64 loss: 0.11149191856384277
Batch 30/64 loss: 0.11029171943664551
Batch 31/64 loss: 0.1420801877975464
Batch 32/64 loss: 0.11581462621688843
Batch 33/64 loss: 0.10406064987182617
Batch 34/64 loss: 0.10746830701828003
Batch 35/64 loss: 0.09882056713104248
Batch 36/64 loss: 0.1086115837097168
Batch 37/64 loss: 0.12851262092590332
Batch 38/64 loss: 0.11492413282394409
Batch 39/64 loss: 0.129966139793396
Batch 40/64 loss: 0.11423325538635254
Batch 41/64 loss: 0.12447655200958252
Batch 42/64 loss: 0.11289048194885254
Batch 43/64 loss: 0.1027253270149231
Batch 44/64 loss: 0.09490573406219482
Batch 45/64 loss: 0.0955880880355835
Batch 46/64 loss: 0.11112892627716064
Batch 47/64 loss: 0.08500730991363525
Batch 48/64 loss: 0.10320162773132324
Batch 49/64 loss: 0.10940337181091309
Batch 50/64 loss: 0.1120787262916565
Batch 51/64 loss: 0.10130429267883301
Batch 52/64 loss: 0.10559183359146118
Batch 53/64 loss: 0.10783219337463379
Batch 54/64 loss: 0.09892916679382324
Batch 55/64 loss: 0.13372600078582764
Batch 56/64 loss: 0.14288365840911865
Batch 57/64 loss: 0.11591494083404541
Batch 58/64 loss: 0.11798256635665894
Batch 59/64 loss: 0.1318562626838684
Batch 60/64 loss: 0.14887094497680664
Batch 61/64 loss: 0.08180254697799683
Batch 62/64 loss: 0.10817807912826538
Batch 63/64 loss: 0.12263208627700806
Batch 64/64 loss: 0.10322701930999756
Epoch 111  Train loss: 0.11255771178825229  Val loss: 0.1463750758531577
Epoch 112
-------------------------------
Batch 1/64 loss: 0.09836500883102417
Batch 2/64 loss: 0.06663644313812256
Batch 3/64 loss: 0.13083720207214355
Batch 4/64 loss: 0.10558593273162842
Batch 5/64 loss: 0.12149739265441895
Batch 6/64 loss: 0.10181671380996704
Batch 7/64 loss: 0.10216748714447021
Batch 8/64 loss: 0.10947072505950928
Batch 9/64 loss: 0.14257359504699707
Batch 10/64 loss: 0.12362098693847656
Batch 11/64 loss: 0.12696319818496704
Batch 12/64 loss: 0.10004550218582153
Batch 13/64 loss: 0.08025312423706055
Batch 14/64 loss: 0.09927833080291748
Batch 15/64 loss: 0.1275721788406372
Batch 16/64 loss: 0.07252317667007446
Batch 17/64 loss: 0.12257111072540283
Batch 18/64 loss: 0.08526957035064697
Batch 19/64 loss: 0.1463489532470703
Batch 20/64 loss: 0.11063575744628906
Batch 21/64 loss: 0.09994924068450928
Batch 22/64 loss: 0.12621474266052246
Batch 23/64 loss: 0.11840420961380005
Batch 24/64 loss: 0.10541081428527832
Batch 25/64 loss: 0.07109713554382324
Batch 26/64 loss: 0.11525696516036987
Batch 27/64 loss: 0.10131961107254028
Batch 28/64 loss: 0.10592347383499146
Batch 29/64 loss: 0.13086652755737305
Batch 30/64 loss: 0.1315414309501648
Batch 31/64 loss: 0.09434616565704346
Batch 32/64 loss: 0.10277783870697021
Batch 33/64 loss: 0.12489718198776245
Batch 34/64 loss: 0.1178632378578186
Batch 35/64 loss: 0.10236746072769165
Batch 36/64 loss: 0.11184906959533691
Batch 37/64 loss: 0.11777818202972412
Batch 38/64 loss: 0.12745338678359985
Batch 39/64 loss: 0.12574118375778198
Batch 40/64 loss: 0.15899765491485596
Batch 41/64 loss: 0.1119922399520874
Batch 42/64 loss: 0.14549678564071655
Batch 43/64 loss: 0.11686629056930542
Batch 44/64 loss: 0.09589773416519165
Batch 45/64 loss: 0.08643734455108643
Batch 46/64 loss: 0.09604501724243164
Batch 47/64 loss: 0.09613335132598877
Batch 48/64 loss: 0.11732417345046997
Batch 49/64 loss: 0.08643054962158203
Batch 50/64 loss: 0.10656154155731201
Batch 51/64 loss: 0.12773650884628296
Batch 52/64 loss: 0.11583852767944336
Batch 53/64 loss: 0.12930268049240112
Batch 54/64 loss: 0.1140434741973877
Batch 55/64 loss: 0.12894999980926514
Batch 56/64 loss: 0.10003030300140381
Batch 57/64 loss: 0.08641213178634644
Batch 58/64 loss: 0.12395918369293213
Batch 59/64 loss: 0.12797462940216064
Batch 60/64 loss: 0.11376100778579712
Batch 61/64 loss: 0.10464060306549072
Batch 62/64 loss: 0.09991705417633057
Batch 63/64 loss: 0.1053084135055542
Batch 64/64 loss: 0.13555335998535156
Epoch 112  Train loss: 0.11141666618047975  Val loss: 0.14940776960136964
Epoch 113
-------------------------------
Batch 1/64 loss: 0.09946203231811523
Batch 2/64 loss: 0.13111060857772827
Batch 3/64 loss: 0.1098172664642334
Batch 4/64 loss: 0.11230838298797607
Batch 5/64 loss: 0.1055915355682373
Batch 6/64 loss: 0.10506951808929443
Batch 7/64 loss: 0.11681860685348511
Batch 8/64 loss: 0.0866667628288269
Batch 9/64 loss: 0.12028366327285767
Batch 10/64 loss: 0.103279709815979
Batch 11/64 loss: 0.1514676809310913
Batch 12/64 loss: 0.09852617979049683
Batch 13/64 loss: 0.1421128511428833
Batch 14/64 loss: 0.09418672323226929
Batch 15/64 loss: 0.11162936687469482
Batch 16/64 loss: 0.11604690551757812
Batch 17/64 loss: 0.10385191440582275
Batch 18/64 loss: 0.11149787902832031
Batch 19/64 loss: 0.10182523727416992
Batch 20/64 loss: 0.12102848291397095
Batch 21/64 loss: 0.11081463098526001
Batch 22/64 loss: 0.12958651781082153
Batch 23/64 loss: 0.11620205640792847
Batch 24/64 loss: 0.11899161338806152
Batch 25/64 loss: 0.11182022094726562
Batch 26/64 loss: 0.14406263828277588
Batch 27/64 loss: 0.11482483148574829
Batch 28/64 loss: 0.12286317348480225
Batch 29/64 loss: 0.12368148565292358
Batch 30/64 loss: 0.11516642570495605
Batch 31/64 loss: 0.1115570068359375
Batch 32/64 loss: 0.11453473567962646
Batch 33/64 loss: 0.11550623178482056
Batch 34/64 loss: 0.1143498420715332
Batch 35/64 loss: 0.09079766273498535
Batch 36/64 loss: 0.11672168970108032
Batch 37/64 loss: 0.1131812334060669
Batch 38/64 loss: 0.11221057176589966
Batch 39/64 loss: 0.0991281270980835
Batch 40/64 loss: 0.11415541172027588
Batch 41/64 loss: 0.10060453414916992
Batch 42/64 loss: 0.10864460468292236
Batch 43/64 loss: 0.11632400751113892
Batch 44/64 loss: 0.1083294153213501
Batch 45/64 loss: 0.12241172790527344
Batch 46/64 loss: 0.1095927357673645
Batch 47/64 loss: 0.09099352359771729
Batch 48/64 loss: 0.1167648434638977
Batch 49/64 loss: 0.11359888315200806
Batch 50/64 loss: 0.09229093790054321
Batch 51/64 loss: 0.1291317343711853
Batch 52/64 loss: 0.09615951776504517
Batch 53/64 loss: 0.09588801860809326
Batch 54/64 loss: 0.10074687004089355
Batch 55/64 loss: 0.11221116781234741
Batch 56/64 loss: 0.09302002191543579
Batch 57/64 loss: 0.08376157283782959
Batch 58/64 loss: 0.10087162256240845
Batch 59/64 loss: 0.10407090187072754
Batch 60/64 loss: 0.0787818431854248
Batch 61/64 loss: 0.11784511804580688
Batch 62/64 loss: 0.10001122951507568
Batch 63/64 loss: 0.09281963109970093
Batch 64/64 loss: 0.1320357322692871
Epoch 113  Train loss: 0.11037864591561111  Val loss: 0.1498121942851142
Epoch 114
-------------------------------
Batch 1/64 loss: 0.1159970760345459
Batch 2/64 loss: 0.10775536298751831
Batch 3/64 loss: 0.12333744764328003
Batch 4/64 loss: 0.1198844313621521
Batch 5/64 loss: 0.1190984845161438
Batch 6/64 loss: 0.10978281497955322
Batch 7/64 loss: 0.11251640319824219
Batch 8/64 loss: 0.11470657587051392
Batch 9/64 loss: 0.12165498733520508
Batch 10/64 loss: 0.12722736597061157
Batch 11/64 loss: 0.09588360786437988
Batch 12/64 loss: 0.10514408349990845
Batch 13/64 loss: 0.10003471374511719
Batch 14/64 loss: 0.11852484941482544
Batch 15/64 loss: 0.1006397008895874
Batch 16/64 loss: 0.10846436023712158
Batch 17/64 loss: 0.10647308826446533
Batch 18/64 loss: 0.09346163272857666
Batch 19/64 loss: 0.11641794443130493
Batch 20/64 loss: 0.1100165843963623
Batch 21/64 loss: 0.09239697456359863
Batch 22/64 loss: 0.09576815366744995
Batch 23/64 loss: 0.07691925764083862
Batch 24/64 loss: 0.08795404434204102
Batch 25/64 loss: 0.12422341108322144
Batch 26/64 loss: 0.11174464225769043
Batch 27/64 loss: 0.12790685892105103
Batch 28/64 loss: 0.13070547580718994
Batch 29/64 loss: 0.1269342303276062
Batch 30/64 loss: 0.10370320081710815
Batch 31/64 loss: 0.11495101451873779
Batch 32/64 loss: 0.11046874523162842
Batch 33/64 loss: 0.09958696365356445
Batch 34/64 loss: 0.0762033462524414
Batch 35/64 loss: 0.09396696090698242
Batch 36/64 loss: 0.10063093900680542
Batch 37/64 loss: 0.08040714263916016
Batch 38/64 loss: 0.0978040099143982
Batch 39/64 loss: 0.0992283821105957
Batch 40/64 loss: 0.09014308452606201
Batch 41/64 loss: 0.14873147010803223
Batch 42/64 loss: 0.133858323097229
Batch 43/64 loss: 0.11964809894561768
Batch 44/64 loss: 0.10704243183135986
Batch 45/64 loss: 0.11822128295898438
Batch 46/64 loss: 0.10064435005187988
Batch 47/64 loss: 0.09860533475875854
Batch 48/64 loss: 0.10801839828491211
Batch 49/64 loss: 0.10535234212875366
Batch 50/64 loss: 0.1359807252883911
Batch 51/64 loss: 0.11281782388687134
Batch 52/64 loss: 0.11136656999588013
Batch 53/64 loss: 0.12079030275344849
Batch 54/64 loss: 0.12097209692001343
Batch 55/64 loss: 0.11826801300048828
Batch 56/64 loss: 0.12486481666564941
Batch 57/64 loss: 0.09512108564376831
Batch 58/64 loss: 0.10583484172821045
Batch 59/64 loss: 0.11164528131484985
Batch 60/64 loss: 0.11763119697570801
Batch 61/64 loss: 0.1295621395111084
Batch 62/64 loss: 0.14190495014190674
Batch 63/64 loss: 0.10434293746948242
Batch 64/64 loss: 0.12822312116622925
Epoch 114  Train loss: 0.1106833020846049  Val loss: 0.1446158224364736
Saving best model, epoch: 114
Epoch 115
-------------------------------
Batch 1/64 loss: 0.09537696838378906
Batch 2/64 loss: 0.08253490924835205
Batch 3/64 loss: 0.1316632628440857
Batch 4/64 loss: 0.11455309391021729
Batch 5/64 loss: 0.09495735168457031
Batch 6/64 loss: 0.1148216724395752
Batch 7/64 loss: 0.11024641990661621
Batch 8/64 loss: 0.09627395868301392
Batch 9/64 loss: 0.12098085880279541
Batch 10/64 loss: 0.12057375907897949
Batch 11/64 loss: 0.1088724136352539
Batch 12/64 loss: 0.09744298458099365
Batch 13/64 loss: 0.13117384910583496
Batch 14/64 loss: 0.11091864109039307
Batch 15/64 loss: 0.12037533521652222
Batch 16/64 loss: 0.10852324962615967
Batch 17/64 loss: 0.11520576477050781
Batch 18/64 loss: 0.12480020523071289
Batch 19/64 loss: 0.08966684341430664
Batch 20/64 loss: 0.084686279296875
Batch 21/64 loss: 0.09101617336273193
Batch 22/64 loss: 0.10569512844085693
Batch 23/64 loss: 0.11535787582397461
Batch 24/64 loss: 0.1163591742515564
Batch 25/64 loss: 0.0973246693611145
Batch 26/64 loss: 0.1107790470123291
Batch 27/64 loss: 0.1199040412902832
Batch 28/64 loss: 0.12081831693649292
Batch 29/64 loss: 0.12661921977996826
Batch 30/64 loss: 0.11457693576812744
Batch 31/64 loss: 0.09742408990859985
Batch 32/64 loss: 0.11320412158966064
Batch 33/64 loss: 0.09069758653640747
Batch 34/64 loss: 0.0974157452583313
Batch 35/64 loss: 0.10949164628982544
Batch 36/64 loss: 0.10897475481033325
Batch 37/64 loss: 0.10882174968719482
Batch 38/64 loss: 0.11499661207199097
Batch 39/64 loss: 0.11315858364105225
Batch 40/64 loss: 0.11921417713165283
Batch 41/64 loss: 0.11797511577606201
Batch 42/64 loss: 0.10296905040740967
Batch 43/64 loss: 0.11736196279525757
Batch 44/64 loss: 0.09752786159515381
Batch 45/64 loss: 0.09649461507797241
Batch 46/64 loss: 0.11672616004943848
Batch 47/64 loss: 0.08909296989440918
Batch 48/64 loss: 0.08842694759368896
Batch 49/64 loss: 0.09622520208358765
Batch 50/64 loss: 0.1030881404876709
Batch 51/64 loss: 0.09762018918991089
Batch 52/64 loss: 0.1279609203338623
Batch 53/64 loss: 0.13131928443908691
Batch 54/64 loss: 0.10878890752792358
Batch 55/64 loss: 0.10736328363418579
Batch 56/64 loss: 0.11885827779769897
Batch 57/64 loss: 0.14130926132202148
Batch 58/64 loss: 0.10508817434310913
Batch 59/64 loss: 0.10079801082611084
Batch 60/64 loss: 0.10480928421020508
Batch 61/64 loss: 0.10683166980743408
Batch 62/64 loss: 0.10666924715042114
Batch 63/64 loss: 0.1279636025428772
Batch 64/64 loss: 0.11208969354629517
Epoch 115  Train loss: 0.10918953535603541  Val loss: 0.14388878320910267
Saving best model, epoch: 115
Epoch 116
-------------------------------
Batch 1/64 loss: 0.12370145320892334
Batch 2/64 loss: 0.11358171701431274
Batch 3/64 loss: 0.12075871229171753
Batch 4/64 loss: 0.1047964096069336
Batch 5/64 loss: 0.10919111967086792
Batch 6/64 loss: 0.1397007703781128
Batch 7/64 loss: 0.08025449514389038
Batch 8/64 loss: 0.08827537298202515
Batch 9/64 loss: 0.10305160284042358
Batch 10/64 loss: 0.12212848663330078
Batch 11/64 loss: 0.11069357395172119
Batch 12/64 loss: 0.11011749505996704
Batch 13/64 loss: 0.09804534912109375
Batch 14/64 loss: 0.11488693952560425
Batch 15/64 loss: 0.11889827251434326
Batch 16/64 loss: 0.11700558662414551
Batch 17/64 loss: 0.11393356323242188
Batch 18/64 loss: 0.0979304313659668
Batch 19/64 loss: 0.09749370813369751
Batch 20/64 loss: 0.15457820892333984
Batch 21/64 loss: 0.10946327447891235
Batch 22/64 loss: 0.10218280553817749
Batch 23/64 loss: 0.0985448956489563
Batch 24/64 loss: 0.11423951387405396
Batch 25/64 loss: 0.09231138229370117
Batch 26/64 loss: 0.12052690982818604
Batch 27/64 loss: 0.1252814531326294
Batch 28/64 loss: 0.08634167909622192
Batch 29/64 loss: 0.12280899286270142
Batch 30/64 loss: 0.10485392808914185
Batch 31/64 loss: 0.11969435214996338
Batch 32/64 loss: 0.1274091601371765
Batch 33/64 loss: 0.08529108762741089
Batch 34/64 loss: 0.11472845077514648
Batch 35/64 loss: 0.11566078662872314
Batch 36/64 loss: 0.10005563497543335
Batch 37/64 loss: 0.12802910804748535
Batch 38/64 loss: 0.09504258632659912
Batch 39/64 loss: 0.09006565809249878
Batch 40/64 loss: 0.09963041543960571
Batch 41/64 loss: 0.07679390907287598
Batch 42/64 loss: 0.0908956527709961
Batch 43/64 loss: 0.11007994413375854
Batch 44/64 loss: 0.11077165603637695
Batch 45/64 loss: 0.10810405015945435
Batch 46/64 loss: 0.09566301107406616
Batch 47/64 loss: 0.12750333547592163
Batch 48/64 loss: 0.11236244440078735
Batch 49/64 loss: 0.1407364010810852
Batch 50/64 loss: 0.11634230613708496
Batch 51/64 loss: 0.10984951257705688
Batch 52/64 loss: 0.10778367519378662
Batch 53/64 loss: 0.112296462059021
Batch 54/64 loss: 0.1209561824798584
Batch 55/64 loss: 0.08779251575469971
Batch 56/64 loss: 0.11445033550262451
Batch 57/64 loss: 0.09833145141601562
Batch 58/64 loss: 0.1053849458694458
Batch 59/64 loss: 0.11466705799102783
Batch 60/64 loss: 0.08207041025161743
Batch 61/64 loss: 0.11164569854736328
Batch 62/64 loss: 0.09055876731872559
Batch 63/64 loss: 0.12646353244781494
Batch 64/64 loss: 0.10132604837417603
Epoch 116  Train loss: 0.10884208842819812  Val loss: 0.14504222443832973
Epoch 117
-------------------------------
Batch 1/64 loss: 0.07862603664398193
Batch 2/64 loss: 0.09912019968032837
Batch 3/64 loss: 0.1027994155883789
Batch 4/64 loss: 0.10482341051101685
Batch 5/64 loss: 0.10161375999450684
Batch 6/64 loss: 0.12359964847564697
Batch 7/64 loss: 0.13659584522247314
Batch 8/64 loss: 0.11199933290481567
Batch 9/64 loss: 0.10049504041671753
Batch 10/64 loss: 0.06786882877349854
Batch 11/64 loss: 0.11536312103271484
Batch 12/64 loss: 0.10600727796554565
Batch 13/64 loss: 0.0995253324508667
Batch 14/64 loss: 0.11041092872619629
Batch 15/64 loss: 0.10966777801513672
Batch 16/64 loss: 0.08471250534057617
Batch 17/64 loss: 0.10240805149078369
Batch 18/64 loss: 0.11011499166488647
Batch 19/64 loss: 0.12229716777801514
Batch 20/64 loss: 0.10851210355758667
Batch 21/64 loss: 0.10785079002380371
Batch 22/64 loss: 0.08506619930267334
Batch 23/64 loss: 0.0935964584350586
Batch 24/64 loss: 0.12236040830612183
Batch 25/64 loss: 0.10395878553390503
Batch 26/64 loss: 0.11918461322784424
Batch 27/64 loss: 0.09915649890899658
Batch 28/64 loss: 0.09193843603134155
Batch 29/64 loss: 0.11887431144714355
Batch 30/64 loss: 0.10692811012268066
Batch 31/64 loss: 0.09335798025131226
Batch 32/64 loss: 0.09514886140823364
Batch 33/64 loss: 0.08600819110870361
Batch 34/64 loss: 0.1231527328491211
Batch 35/64 loss: 0.0946304202079773
Batch 36/64 loss: 0.10298818349838257
Batch 37/64 loss: 0.09550213813781738
Batch 38/64 loss: 0.12013864517211914
Batch 39/64 loss: 0.12434321641921997
Batch 40/64 loss: 0.11575698852539062
Batch 41/64 loss: 0.10949301719665527
Batch 42/64 loss: 0.12080401182174683
Batch 43/64 loss: 0.09218108654022217
Batch 44/64 loss: 0.14069920778274536
Batch 45/64 loss: 0.0905492901802063
Batch 46/64 loss: 0.09672003984451294
Batch 47/64 loss: 0.10792583227157593
Batch 48/64 loss: 0.12883901596069336
Batch 49/64 loss: 0.1070215106010437
Batch 50/64 loss: 0.1220698356628418
Batch 51/64 loss: 0.10224306583404541
Batch 52/64 loss: 0.12415003776550293
Batch 53/64 loss: 0.11413931846618652
Batch 54/64 loss: 0.1367226243019104
Batch 55/64 loss: 0.094510018825531
Batch 56/64 loss: 0.13577169179916382
Batch 57/64 loss: 0.07750499248504639
Batch 58/64 loss: 0.08915424346923828
Batch 59/64 loss: 0.10845232009887695
Batch 60/64 loss: 0.11695206165313721
Batch 61/64 loss: 0.11949402093887329
Batch 62/64 loss: 0.11864537000656128
Batch 63/64 loss: 0.13324880599975586
Batch 64/64 loss: 0.07566159963607788
Epoch 117  Train loss: 0.10730259395113179  Val loss: 0.14486941418696925
Epoch 118
-------------------------------
Batch 1/64 loss: 0.09703820943832397
Batch 2/64 loss: 0.103324294090271
Batch 3/64 loss: 0.08944594860076904
Batch 4/64 loss: 0.10423660278320312
Batch 5/64 loss: 0.09461432695388794
Batch 6/64 loss: 0.12646424770355225
Batch 7/64 loss: 0.08400040864944458
Batch 8/64 loss: 0.08335977792739868
Batch 9/64 loss: 0.10489702224731445
Batch 10/64 loss: 0.11150103807449341
Batch 11/64 loss: 0.14686572551727295
Batch 12/64 loss: 0.11626589298248291
Batch 13/64 loss: 0.1026487946510315
Batch 14/64 loss: 0.1311914324760437
Batch 15/64 loss: 0.09298354387283325
Batch 16/64 loss: 0.11354267597198486
Batch 17/64 loss: 0.08832210302352905
Batch 18/64 loss: 0.11638838052749634
Batch 19/64 loss: 0.0918392539024353
Batch 20/64 loss: 0.08228558301925659
Batch 21/64 loss: 0.08484768867492676
Batch 22/64 loss: 0.10482412576675415
Batch 23/64 loss: 0.10134851932525635
Batch 24/64 loss: 0.09853124618530273
Batch 25/64 loss: 0.1088707447052002
Batch 26/64 loss: 0.11854052543640137
Batch 27/64 loss: 0.11163061857223511
Batch 28/64 loss: 0.11085402965545654
Batch 29/64 loss: 0.09636270999908447
Batch 30/64 loss: 0.11849212646484375
Batch 31/64 loss: 0.10561048984527588
Batch 32/64 loss: 0.1186293363571167
Batch 33/64 loss: 0.08991396427154541
Batch 34/64 loss: 0.10644173622131348
Batch 35/64 loss: 0.11346441507339478
Batch 36/64 loss: 0.1257149577140808
Batch 37/64 loss: 0.12173616886138916
Batch 38/64 loss: 0.10881626605987549
Batch 39/64 loss: 0.11967039108276367
Batch 40/64 loss: 0.08014684915542603
Batch 41/64 loss: 0.1359652876853943
Batch 42/64 loss: 0.12300866842269897
Batch 43/64 loss: 0.1326005458831787
Batch 44/64 loss: 0.11084192991256714
Batch 45/64 loss: 0.10630655288696289
Batch 46/64 loss: 0.10359573364257812
Batch 47/64 loss: 0.11359846591949463
Batch 48/64 loss: 0.10940748453140259
Batch 49/64 loss: 0.1259310245513916
Batch 50/64 loss: 0.123779296875
Batch 51/64 loss: 0.1034056544303894
Batch 52/64 loss: 0.10532045364379883
Batch 53/64 loss: 0.09367787837982178
Batch 54/64 loss: 0.11970847845077515
Batch 55/64 loss: 0.09274131059646606
Batch 56/64 loss: 0.1023111343383789
Batch 57/64 loss: 0.08925163745880127
Batch 58/64 loss: 0.11329805850982666
Batch 59/64 loss: 0.11128640174865723
Batch 60/64 loss: 0.12196135520935059
Batch 61/64 loss: 0.1054924726486206
Batch 62/64 loss: 0.10801565647125244
Batch 63/64 loss: 0.09471309185028076
Batch 64/64 loss: 0.11296266317367554
Epoch 118  Train loss: 0.10755455283557668  Val loss: 0.14091960937296813
Saving best model, epoch: 118
Epoch 119
-------------------------------
Batch 1/64 loss: 0.095736563205719
Batch 2/64 loss: 0.11161535978317261
Batch 3/64 loss: 0.11122256517410278
Batch 4/64 loss: 0.10908979177474976
Batch 5/64 loss: 0.10856252908706665
Batch 6/64 loss: 0.1033589243888855
Batch 7/64 loss: 0.12210416793823242
Batch 8/64 loss: 0.0968969464302063
Batch 9/64 loss: 0.10106778144836426
Batch 10/64 loss: 0.12078869342803955
Batch 11/64 loss: 0.11268830299377441
Batch 12/64 loss: 0.09381401538848877
Batch 13/64 loss: 0.11881202459335327
Batch 14/64 loss: 0.09566313028335571
Batch 15/64 loss: 0.12144041061401367
Batch 16/64 loss: 0.09681594371795654
Batch 17/64 loss: 0.12594103813171387
Batch 18/64 loss: 0.09633272886276245
Batch 19/64 loss: 0.1259007453918457
Batch 20/64 loss: 0.11683285236358643
Batch 21/64 loss: 0.09010553359985352
Batch 22/64 loss: 0.13808077573776245
Batch 23/64 loss: 0.08696228265762329
Batch 24/64 loss: 0.11526185274124146
Batch 25/64 loss: 0.09838926792144775
Batch 26/64 loss: 0.09936726093292236
Batch 27/64 loss: 0.10471463203430176
Batch 28/64 loss: 0.11170172691345215
Batch 29/64 loss: 0.09186828136444092
Batch 30/64 loss: 0.12485325336456299
Batch 31/64 loss: 0.1154935359954834
Batch 32/64 loss: 0.09889400005340576
Batch 33/64 loss: 0.10965275764465332
Batch 34/64 loss: 0.11239957809448242
Batch 35/64 loss: 0.11217278242111206
Batch 36/64 loss: 0.1061638593673706
Batch 37/64 loss: 0.0991942286491394
Batch 38/64 loss: 0.11005914211273193
Batch 39/64 loss: 0.12741518020629883
Batch 40/64 loss: 0.10134363174438477
Batch 41/64 loss: 0.09645217657089233
Batch 42/64 loss: 0.10550010204315186
Batch 43/64 loss: 0.12672102451324463
Batch 44/64 loss: 0.1134989857673645
Batch 45/64 loss: 0.11422973871231079
Batch 46/64 loss: 0.0982208251953125
Batch 47/64 loss: 0.1118132472038269
Batch 48/64 loss: 0.12549054622650146
Batch 49/64 loss: 0.13891053199768066
Batch 50/64 loss: 0.095142662525177
Batch 51/64 loss: 0.0826117992401123
Batch 52/64 loss: 0.1064181923866272
Batch 53/64 loss: 0.10693812370300293
Batch 54/64 loss: 0.11362868547439575
Batch 55/64 loss: 0.1264725923538208
Batch 56/64 loss: 0.10455936193466187
Batch 57/64 loss: 0.09073400497436523
Batch 58/64 loss: 0.09504985809326172
Batch 59/64 loss: 0.1243065595626831
Batch 60/64 loss: 0.09944868087768555
Batch 61/64 loss: 0.0698862075805664
Batch 62/64 loss: 0.1123734712600708
Batch 63/64 loss: 0.12022137641906738
Batch 64/64 loss: 0.09287679195404053
Epoch 119  Train loss: 0.10803238317078236  Val loss: 0.14698354505591377
Epoch 120
-------------------------------
Batch 1/64 loss: 0.0969916582107544
Batch 2/64 loss: 0.09777289628982544
Batch 3/64 loss: 0.14702165126800537
Batch 4/64 loss: 0.08651787042617798
Batch 5/64 loss: 0.11237180233001709
Batch 6/64 loss: 0.12834453582763672
Batch 7/64 loss: 0.08520954847335815
Batch 8/64 loss: 0.10532712936401367
Batch 9/64 loss: 0.09330481290817261
Batch 10/64 loss: 0.09767472743988037
Batch 11/64 loss: 0.08335041999816895
Batch 12/64 loss: 0.11806297302246094
Batch 13/64 loss: 0.10408204793930054
Batch 14/64 loss: 0.11117345094680786
Batch 15/64 loss: 0.10225516557693481
Batch 16/64 loss: 0.11385416984558105
Batch 17/64 loss: 0.12166464328765869
Batch 18/64 loss: 0.10777521133422852
Batch 19/64 loss: 0.09832960367202759
Batch 20/64 loss: 0.10537147521972656
Batch 21/64 loss: 0.11867296695709229
Batch 22/64 loss: 0.13241136074066162
Batch 23/64 loss: 0.10493439435958862
Batch 24/64 loss: 0.09978532791137695
Batch 25/64 loss: 0.11638760566711426
Batch 26/64 loss: 0.08057421445846558
Batch 27/64 loss: 0.08719152212142944
Batch 28/64 loss: 0.10115116834640503
Batch 29/64 loss: 0.100624680519104
Batch 30/64 loss: 0.09163773059844971
Batch 31/64 loss: 0.09789711236953735
Batch 32/64 loss: 0.08754587173461914
Batch 33/64 loss: 0.10815799236297607
Batch 34/64 loss: 0.12767153978347778
Batch 35/64 loss: 0.11633211374282837
Batch 36/64 loss: 0.10093474388122559
Batch 37/64 loss: 0.0996236801147461
Batch 38/64 loss: 0.12301051616668701
Batch 39/64 loss: 0.13507568836212158
Batch 40/64 loss: 0.11255812644958496
Batch 41/64 loss: 0.11724966764450073
Batch 42/64 loss: 0.13717317581176758
Batch 43/64 loss: 0.11075025796890259
Batch 44/64 loss: 0.1321725845336914
Batch 45/64 loss: 0.12214529514312744
Batch 46/64 loss: 0.10004597902297974
Batch 47/64 loss: 0.08855068683624268
Batch 48/64 loss: 0.10810530185699463
Batch 49/64 loss: 0.11083126068115234
Batch 50/64 loss: 0.10304296016693115
Batch 51/64 loss: 0.07083272933959961
Batch 52/64 loss: 0.08375221490859985
Batch 53/64 loss: 0.1165459156036377
Batch 54/64 loss: 0.11639970541000366
Batch 55/64 loss: 0.10118496417999268
Batch 56/64 loss: 0.11625063419342041
Batch 57/64 loss: 0.1087140440940857
Batch 58/64 loss: 0.10548663139343262
Batch 59/64 loss: 0.09147155284881592
Batch 60/64 loss: 0.08188802003860474
Batch 61/64 loss: 0.09976577758789062
Batch 62/64 loss: 0.0960766077041626
Batch 63/64 loss: 0.14271152019500732
Batch 64/64 loss: 0.08825713396072388
Epoch 120  Train loss: 0.10644665862999711  Val loss: 0.14325757124989302
Epoch 121
-------------------------------
Batch 1/64 loss: 0.10896581411361694
Batch 2/64 loss: 0.10910153388977051
Batch 3/64 loss: 0.10168135166168213
Batch 4/64 loss: 0.11593693494796753
Batch 5/64 loss: 0.10471653938293457
Batch 6/64 loss: 0.08313560485839844
Batch 7/64 loss: 0.12512290477752686
Batch 8/64 loss: 0.09700560569763184
Batch 9/64 loss: 0.11206889152526855
Batch 10/64 loss: 0.1003350019454956
Batch 11/64 loss: 0.10242009162902832
Batch 12/64 loss: 0.09354090690612793
Batch 13/64 loss: 0.13475382328033447
Batch 14/64 loss: 0.11501789093017578
Batch 15/64 loss: 0.09212899208068848
Batch 16/64 loss: 0.11472070217132568
Batch 17/64 loss: 0.1241375207901001
Batch 18/64 loss: 0.10330867767333984
Batch 19/64 loss: 0.10042518377304077
Batch 20/64 loss: 0.10972714424133301
Batch 21/64 loss: 0.12419569492340088
Batch 22/64 loss: 0.10036051273345947
Batch 23/64 loss: 0.10862958431243896
Batch 24/64 loss: 0.13806217908859253
Batch 25/64 loss: 0.107322096824646
Batch 26/64 loss: 0.10307860374450684
Batch 27/64 loss: 0.11675280332565308
Batch 28/64 loss: 0.12508022785186768
Batch 29/64 loss: 0.09090173244476318
Batch 30/64 loss: 0.09934026002883911
Batch 31/64 loss: 0.09486138820648193
Batch 32/64 loss: 0.10893356800079346
Batch 33/64 loss: 0.10796433687210083
Batch 34/64 loss: 0.091480553150177
Batch 35/64 loss: 0.09787070751190186
Batch 36/64 loss: 0.11346960067749023
Batch 37/64 loss: 0.08724731206893921
Batch 38/64 loss: 0.13658207654953003
Batch 39/64 loss: 0.09889161586761475
Batch 40/64 loss: 0.08851224184036255
Batch 41/64 loss: 0.09291177988052368
Batch 42/64 loss: 0.12193858623504639
Batch 43/64 loss: 0.11943960189819336
Batch 44/64 loss: 0.11545974016189575
Batch 45/64 loss: 0.08602052927017212
Batch 46/64 loss: 0.08406782150268555
Batch 47/64 loss: 0.12049943208694458
Batch 48/64 loss: 0.10082358121871948
Batch 49/64 loss: 0.1084783673286438
Batch 50/64 loss: 0.07464861869812012
Batch 51/64 loss: 0.09940093755722046
Batch 52/64 loss: 0.08338356018066406
Batch 53/64 loss: 0.10012239217758179
Batch 54/64 loss: 0.10446208715438843
Batch 55/64 loss: 0.08922183513641357
Batch 56/64 loss: 0.10501432418823242
Batch 57/64 loss: 0.10503089427947998
Batch 58/64 loss: 0.11965346336364746
Batch 59/64 loss: 0.09231764078140259
Batch 60/64 loss: 0.1080896258354187
Batch 61/64 loss: 0.11285257339477539
Batch 62/64 loss: 0.10489088296890259
Batch 63/64 loss: 0.10557210445404053
Batch 64/64 loss: 0.10901713371276855
Epoch 121  Train loss: 0.10547217948763979  Val loss: 0.14187941907607404
Epoch 122
-------------------------------
Batch 1/64 loss: 0.0854526162147522
Batch 2/64 loss: 0.08260256052017212
Batch 3/64 loss: 0.11162298917770386
Batch 4/64 loss: 0.10149842500686646
Batch 5/64 loss: 0.11659014225006104
Batch 6/64 loss: 0.09043514728546143
Batch 7/64 loss: 0.11141490936279297
Batch 8/64 loss: 0.08303368091583252
Batch 9/64 loss: 0.10693490505218506
Batch 10/64 loss: 0.1192394495010376
Batch 11/64 loss: 0.09969407320022583
Batch 12/64 loss: 0.14086341857910156
Batch 13/64 loss: 0.11930334568023682
Batch 14/64 loss: 0.09943580627441406
Batch 15/64 loss: 0.11049127578735352
Batch 16/64 loss: 0.11525493860244751
Batch 17/64 loss: 0.11483824253082275
Batch 18/64 loss: 0.11576712131500244
Batch 19/64 loss: 0.09361886978149414
Batch 20/64 loss: 0.09831690788269043
Batch 21/64 loss: 0.09845626354217529
Batch 22/64 loss: 0.11632668972015381
Batch 23/64 loss: 0.105682373046875
Batch 24/64 loss: 0.0971301794052124
Batch 25/64 loss: 0.08570390939712524
Batch 26/64 loss: 0.08603113889694214
Batch 27/64 loss: 0.09375530481338501
Batch 28/64 loss: 0.09091484546661377
Batch 29/64 loss: 0.1718798279762268
Batch 30/64 loss: 0.12626999616622925
Batch 31/64 loss: 0.09741485118865967
Batch 32/64 loss: 0.09422051906585693
Batch 33/64 loss: 0.0959215760231018
Batch 34/64 loss: 0.10450154542922974
Batch 35/64 loss: 0.11624622344970703
Batch 36/64 loss: 0.10275322198867798
Batch 37/64 loss: 0.13277727365493774
Batch 38/64 loss: 0.08997160196304321
Batch 39/64 loss: 0.13125669956207275
Batch 40/64 loss: 0.13716423511505127
Batch 41/64 loss: 0.1075206995010376
Batch 42/64 loss: 0.11422276496887207
Batch 43/64 loss: 0.13401150703430176
Batch 44/64 loss: 0.0908849835395813
Batch 45/64 loss: 0.11190438270568848
Batch 46/64 loss: 0.10051929950714111
Batch 47/64 loss: 0.10632157325744629
Batch 48/64 loss: 0.09887945652008057
Batch 49/64 loss: 0.09762227535247803
Batch 50/64 loss: 0.08627146482467651
Batch 51/64 loss: 0.11611080169677734
Batch 52/64 loss: 0.13008928298950195
Batch 53/64 loss: 0.10082495212554932
Batch 54/64 loss: 0.08954441547393799
Batch 55/64 loss: 0.08662599325180054
Batch 56/64 loss: 0.10899233818054199
Batch 57/64 loss: 0.11703342199325562
Batch 58/64 loss: 0.11047089099884033
Batch 59/64 loss: 0.09109175205230713
Batch 60/64 loss: 0.12228453159332275
Batch 61/64 loss: 0.10070109367370605
Batch 62/64 loss: 0.11424911022186279
Batch 63/64 loss: 0.10383003950119019
Batch 64/64 loss: 0.09693384170532227
Epoch 122  Train loss: 0.10672148255740894  Val loss: 0.14006227828383036
Saving best model, epoch: 122
Epoch 123
-------------------------------
Batch 1/64 loss: 0.11602199077606201
Batch 2/64 loss: 0.11781883239746094
Batch 3/64 loss: 0.10243558883666992
Batch 4/64 loss: 0.08944624662399292
Batch 5/64 loss: 0.11060523986816406
Batch 6/64 loss: 0.10724705457687378
Batch 7/64 loss: 0.10033488273620605
Batch 8/64 loss: 0.09376519918441772
Batch 9/64 loss: 0.11733353137969971
Batch 10/64 loss: 0.10289430618286133
Batch 11/64 loss: 0.10366088151931763
Batch 12/64 loss: 0.10859018564224243
Batch 13/64 loss: 0.0949023962020874
Batch 14/64 loss: 0.09612172842025757
Batch 15/64 loss: 0.08407175540924072
Batch 16/64 loss: 0.09647947549819946
Batch 17/64 loss: 0.08938056230545044
Batch 18/64 loss: 0.11231499910354614
Batch 19/64 loss: 0.09874820709228516
Batch 20/64 loss: 0.09593790769577026
Batch 21/64 loss: 0.08196002244949341
Batch 22/64 loss: 0.11470794677734375
Batch 23/64 loss: 0.1112527847290039
Batch 24/64 loss: 0.11162954568862915
Batch 25/64 loss: 0.11610323190689087
Batch 26/64 loss: 0.11567103862762451
Batch 27/64 loss: 0.08910501003265381
Batch 28/64 loss: 0.10296833515167236
Batch 29/64 loss: 0.12949693202972412
Batch 30/64 loss: 0.11903905868530273
Batch 31/64 loss: 0.13208889961242676
Batch 32/64 loss: 0.11151033639907837
Batch 33/64 loss: 0.07908356189727783
Batch 34/64 loss: 0.09976112842559814
Batch 35/64 loss: 0.09572499990463257
Batch 36/64 loss: 0.12329494953155518
Batch 37/64 loss: 0.08438175916671753
Batch 38/64 loss: 0.11363250017166138
Batch 39/64 loss: 0.10345566272735596
Batch 40/64 loss: 0.10083454847335815
Batch 41/64 loss: 0.11353814601898193
Batch 42/64 loss: 0.1128089427947998
Batch 43/64 loss: 0.1193002462387085
Batch 44/64 loss: 0.09487807750701904
Batch 45/64 loss: 0.14124757051467896
Batch 46/64 loss: 0.0913858413696289
Batch 47/64 loss: 0.06801712512969971
Batch 48/64 loss: 0.12145006656646729
Batch 49/64 loss: 0.11202389001846313
Batch 50/64 loss: 0.10658669471740723
Batch 51/64 loss: 0.08306711912155151
Batch 52/64 loss: 0.1212301254272461
Batch 53/64 loss: 0.09972065687179565
Batch 54/64 loss: 0.11330080032348633
Batch 55/64 loss: 0.14133590459823608
Batch 56/64 loss: 0.09620541334152222
Batch 57/64 loss: 0.10804563760757446
Batch 58/64 loss: 0.11857688426971436
Batch 59/64 loss: 0.08890223503112793
Batch 60/64 loss: 0.09638506174087524
Batch 61/64 loss: 0.10093289613723755
Batch 62/64 loss: 0.13397860527038574
Batch 63/64 loss: 0.12961208820343018
Batch 64/64 loss: 0.09635847806930542
Epoch 123  Train loss: 0.1060174448817384  Val loss: 0.14130095067302795
Epoch 124
-------------------------------
Batch 1/64 loss: 0.1080370545387268
Batch 2/64 loss: 0.10097944736480713
Batch 3/64 loss: 0.11150914430618286
Batch 4/64 loss: 0.09934771060943604
Batch 5/64 loss: 0.10176318883895874
Batch 6/64 loss: 0.10407251119613647
Batch 7/64 loss: 0.10996294021606445
Batch 8/64 loss: 0.11057502031326294
Batch 9/64 loss: 0.0982896089553833
Batch 10/64 loss: 0.10393273830413818
Batch 11/64 loss: 0.09446144104003906
Batch 12/64 loss: 0.08573514223098755
Batch 13/64 loss: 0.09089922904968262
Batch 14/64 loss: 0.11806702613830566
Batch 15/64 loss: 0.11149853467941284
Batch 16/64 loss: 0.09695106744766235
Batch 17/64 loss: 0.09626656770706177
Batch 18/64 loss: 0.11582982540130615
Batch 19/64 loss: 0.09871232509613037
Batch 20/64 loss: 0.10707664489746094
Batch 21/64 loss: 0.0731319785118103
Batch 22/64 loss: 0.13589894771575928
Batch 23/64 loss: 0.1191825270652771
Batch 24/64 loss: 0.10367399454116821
Batch 25/64 loss: 0.08456504344940186
Batch 26/64 loss: 0.10574090480804443
Batch 27/64 loss: 0.10861700773239136
Batch 28/64 loss: 0.14332592487335205
Batch 29/64 loss: 0.10222196578979492
Batch 30/64 loss: 0.09470415115356445
Batch 31/64 loss: 0.10997289419174194
Batch 32/64 loss: 0.06758016347885132
Batch 33/64 loss: 0.08643770217895508
Batch 34/64 loss: 0.08431941270828247
Batch 35/64 loss: 0.08880245685577393
Batch 36/64 loss: 0.10496646165847778
Batch 37/64 loss: 0.10489720106124878
Batch 38/64 loss: 0.10126608610153198
Batch 39/64 loss: 0.11749762296676636
Batch 40/64 loss: 0.1153573989868164
Batch 41/64 loss: 0.1099587082862854
Batch 42/64 loss: 0.11433154344558716
Batch 43/64 loss: 0.10684776306152344
Batch 44/64 loss: 0.0686827301979065
Batch 45/64 loss: 0.08969449996948242
Batch 46/64 loss: 0.09568583965301514
Batch 47/64 loss: 0.1481894850730896
Batch 48/64 loss: 0.09977144002914429
Batch 49/64 loss: 0.10056906938552856
Batch 50/64 loss: 0.12209445238113403
Batch 51/64 loss: 0.11938208341598511
Batch 52/64 loss: 0.12263590097427368
Batch 53/64 loss: 0.1180606484413147
Batch 54/64 loss: 0.11710703372955322
Batch 55/64 loss: 0.07850170135498047
Batch 56/64 loss: 0.09515106678009033
Batch 57/64 loss: 0.10767841339111328
Batch 58/64 loss: 0.11195510625839233
Batch 59/64 loss: 0.1289966106414795
Batch 60/64 loss: 0.10194176435470581
Batch 61/64 loss: 0.10778820514678955
Batch 62/64 loss: 0.10447150468826294
Batch 63/64 loss: 0.07720202207565308
Batch 64/64 loss: 0.08893758058547974
Epoch 124  Train loss: 0.10399259282093422  Val loss: 0.1436520599827324
Epoch 125
-------------------------------
Batch 1/64 loss: 0.09426355361938477
Batch 2/64 loss: 0.10514605045318604
Batch 3/64 loss: 0.11513304710388184
Batch 4/64 loss: 0.13112390041351318
Batch 5/64 loss: 0.09740149974822998
Batch 6/64 loss: 0.09026646614074707
Batch 7/64 loss: 0.09414350986480713
Batch 8/64 loss: 0.10173332691192627
Batch 9/64 loss: 0.10851413011550903
Batch 10/64 loss: 0.11062991619110107
Batch 11/64 loss: 0.07795023918151855
Batch 12/64 loss: 0.08377182483673096
Batch 13/64 loss: 0.10172039270401001
Batch 14/64 loss: 0.09112334251403809
Batch 15/64 loss: 0.09987092018127441
Batch 16/64 loss: 0.09303724765777588
Batch 17/64 loss: 0.11293148994445801
Batch 18/64 loss: 0.12204116582870483
Batch 19/64 loss: 0.12650460004806519
Batch 20/64 loss: 0.08939051628112793
Batch 21/64 loss: 0.10635799169540405
Batch 22/64 loss: 0.10179746150970459
Batch 23/64 loss: 0.08102357387542725
Batch 24/64 loss: 0.09223401546478271
Batch 25/64 loss: 0.14457178115844727
Batch 26/64 loss: 0.11874926090240479
Batch 27/64 loss: 0.08295780420303345
Batch 28/64 loss: 0.09848016500473022
Batch 29/64 loss: 0.10262918472290039
Batch 30/64 loss: 0.1225023865699768
Batch 31/64 loss: 0.11524200439453125
Batch 32/64 loss: 0.11047399044036865
Batch 33/64 loss: 0.12560129165649414
Batch 34/64 loss: 0.14324623346328735
Batch 35/64 loss: 0.08272355794906616
Batch 36/64 loss: 0.1113467812538147
Batch 37/64 loss: 0.1059037446975708
Batch 38/64 loss: 0.09377801418304443
Batch 39/64 loss: 0.12560880184173584
Batch 40/64 loss: 0.09825414419174194
Batch 41/64 loss: 0.09912508726119995
Batch 42/64 loss: 0.11149293184280396
Batch 43/64 loss: 0.10685950517654419
Batch 44/64 loss: 0.10282117128372192
Batch 45/64 loss: 0.11840248107910156
Batch 46/64 loss: 0.09823775291442871
Batch 47/64 loss: 0.08911168575286865
Batch 48/64 loss: 0.09066015481948853
Batch 49/64 loss: 0.11015784740447998
Batch 50/64 loss: 0.09873449802398682
Batch 51/64 loss: 0.10662341117858887
Batch 52/64 loss: 0.101520836353302
Batch 53/64 loss: 0.11535269021987915
Batch 54/64 loss: 0.08058178424835205
Batch 55/64 loss: 0.09585440158843994
Batch 56/64 loss: 0.09483140707015991
Batch 57/64 loss: 0.1134445071220398
Batch 58/64 loss: 0.10680180788040161
Batch 59/64 loss: 0.11412906646728516
Batch 60/64 loss: 0.08745789527893066
Batch 61/64 loss: 0.11445295810699463
Batch 62/64 loss: 0.08977526426315308
Batch 63/64 loss: 0.08644253015518188
Batch 64/64 loss: 0.07662487030029297
Epoch 125  Train loss: 0.10353756324917662  Val loss: 0.13900637073615163
Saving best model, epoch: 125
Epoch 126
-------------------------------
Batch 1/64 loss: 0.08777749538421631
Batch 2/64 loss: 0.10953080654144287
Batch 3/64 loss: 0.07969748973846436
Batch 4/64 loss: 0.0883525013923645
Batch 5/64 loss: 0.09036338329315186
Batch 6/64 loss: 0.10256445407867432
Batch 7/64 loss: 0.10700607299804688
Batch 8/64 loss: 0.09475827217102051
Batch 9/64 loss: 0.07566773891448975
Batch 10/64 loss: 0.139945387840271
Batch 11/64 loss: 0.09275960922241211
Batch 12/64 loss: 0.10369420051574707
Batch 13/64 loss: 0.09216809272766113
Batch 14/64 loss: 0.119789719581604
Batch 15/64 loss: 0.12209272384643555
Batch 16/64 loss: 0.10252887010574341
Batch 17/64 loss: 0.14167052507400513
Batch 18/64 loss: 0.09541058540344238
Batch 19/64 loss: 0.08066868782043457
Batch 20/64 loss: 0.12105351686477661
Batch 21/64 loss: 0.0910344123840332
Batch 22/64 loss: 0.11846518516540527
Batch 23/64 loss: 0.10790717601776123
Batch 24/64 loss: 0.11664032936096191
Batch 25/64 loss: 0.12076497077941895
Batch 26/64 loss: 0.10461139678955078
Batch 27/64 loss: 0.08098798990249634
Batch 28/64 loss: 0.11863577365875244
Batch 29/64 loss: 0.0814138650894165
Batch 30/64 loss: 0.1079869270324707
Batch 31/64 loss: 0.10973703861236572
Batch 32/64 loss: 0.10823369026184082
Batch 33/64 loss: 0.11508619785308838
Batch 34/64 loss: 0.10656934976577759
Batch 35/64 loss: 0.10702317953109741
Batch 36/64 loss: 0.10742437839508057
Batch 37/64 loss: 0.0928809642791748
Batch 38/64 loss: 0.08266627788543701
Batch 39/64 loss: 0.08740133047103882
Batch 40/64 loss: 0.11361271142959595
Batch 41/64 loss: 0.13295865058898926
Batch 42/64 loss: 0.08689022064208984
Batch 43/64 loss: 0.1221466064453125
Batch 44/64 loss: 0.09851855039596558
Batch 45/64 loss: 0.10245126485824585
Batch 46/64 loss: 0.10372579097747803
Batch 47/64 loss: 0.10416167974472046
Batch 48/64 loss: 0.09602785110473633
Batch 49/64 loss: 0.09132826328277588
Batch 50/64 loss: 0.11215782165527344
Batch 51/64 loss: 0.09934026002883911
Batch 52/64 loss: 0.14082813262939453
Batch 53/64 loss: 0.08728528022766113
Batch 54/64 loss: 0.08243608474731445
Batch 55/64 loss: 0.09477639198303223
Batch 56/64 loss: 0.11132127046585083
Batch 57/64 loss: 0.12137776613235474
Batch 58/64 loss: 0.11780107021331787
Batch 59/64 loss: 0.11851078271865845
Batch 60/64 loss: 0.10986959934234619
Batch 61/64 loss: 0.07816535234451294
Batch 62/64 loss: 0.13224023580551147
Batch 63/64 loss: 0.08121764659881592
Batch 64/64 loss: 0.10812556743621826
Epoch 126  Train loss: 0.10405041610493379  Val loss: 0.1391788856270387
Epoch 127
-------------------------------
Batch 1/64 loss: 0.11162447929382324
Batch 2/64 loss: 0.07495605945587158
Batch 3/64 loss: 0.08969610929489136
Batch 4/64 loss: 0.09721648693084717
Batch 5/64 loss: 0.10601365566253662
Batch 6/64 loss: 0.09213924407958984
Batch 7/64 loss: 0.10319405794143677
Batch 8/64 loss: 0.10428893566131592
Batch 9/64 loss: 0.11529731750488281
Batch 10/64 loss: 0.09786826372146606
Batch 11/64 loss: 0.09794539213180542
Batch 12/64 loss: 0.0820818543434143
Batch 13/64 loss: 0.09538257122039795
Batch 14/64 loss: 0.10515862703323364
Batch 15/64 loss: 0.1204192042350769
Batch 16/64 loss: 0.09211194515228271
Batch 17/64 loss: 0.13708269596099854
Batch 18/64 loss: 0.10818231105804443
Batch 19/64 loss: 0.09555405378341675
Batch 20/64 loss: 0.1237974762916565
Batch 21/64 loss: 0.09082233905792236
Batch 22/64 loss: 0.1241270899772644
Batch 23/64 loss: 0.09509503841400146
Batch 24/64 loss: 0.10548621416091919
Batch 25/64 loss: 0.10339641571044922
Batch 26/64 loss: 0.10462707281112671
Batch 27/64 loss: 0.13108181953430176
Batch 28/64 loss: 0.13150638341903687
Batch 29/64 loss: 0.10472583770751953
Batch 30/64 loss: 0.09522342681884766
Batch 31/64 loss: 0.09720522165298462
Batch 32/64 loss: 0.11170691251754761
Batch 33/64 loss: 0.11673712730407715
Batch 34/64 loss: 0.0890122652053833
Batch 35/64 loss: 0.12018251419067383
Batch 36/64 loss: 0.09316599369049072
Batch 37/64 loss: 0.10589873790740967
Batch 38/64 loss: 0.08697861433029175
Batch 39/64 loss: 0.09958750009536743
Batch 40/64 loss: 0.13642728328704834
Batch 41/64 loss: 0.08582150936126709
Batch 42/64 loss: 0.09208059310913086
Batch 43/64 loss: 0.08408647775650024
Batch 44/64 loss: 0.10190188884735107
Batch 45/64 loss: 0.09778040647506714
Batch 46/64 loss: 0.10456538200378418
Batch 47/64 loss: 0.1078803539276123
Batch 48/64 loss: 0.11007875204086304
Batch 49/64 loss: 0.09416460990905762
Batch 50/64 loss: 0.11421394348144531
Batch 51/64 loss: 0.09750902652740479
Batch 52/64 loss: 0.14311683177947998
Batch 53/64 loss: 0.11690056324005127
Batch 54/64 loss: 0.099537193775177
Batch 55/64 loss: 0.08725029230117798
Batch 56/64 loss: 0.08914005756378174
Batch 57/64 loss: 0.11519193649291992
Batch 58/64 loss: 0.09948307275772095
Batch 59/64 loss: 0.10137051343917847
Batch 60/64 loss: 0.07274997234344482
Batch 61/64 loss: 0.11220866441726685
Batch 62/64 loss: 0.11265534162521362
Batch 63/64 loss: 0.09962761402130127
Batch 64/64 loss: 0.08143746852874756
Epoch 127  Train loss: 0.10342584544537115  Val loss: 0.1413754042481229
Epoch 128
-------------------------------
Batch 1/64 loss: 0.12843191623687744
Batch 2/64 loss: 0.10049527883529663
Batch 3/64 loss: 0.11411070823669434
Batch 4/64 loss: 0.09281474351882935
Batch 5/64 loss: 0.11454766988754272
Batch 6/64 loss: 0.11633867025375366
Batch 7/64 loss: 0.11447715759277344
Batch 8/64 loss: 0.09430104494094849
Batch 9/64 loss: 0.09246742725372314
Batch 10/64 loss: 0.10491597652435303
Batch 11/64 loss: 0.11523240804672241
Batch 12/64 loss: 0.1151738166809082
Batch 13/64 loss: 0.10118746757507324
Batch 14/64 loss: 0.08440160751342773
Batch 15/64 loss: 0.114288330078125
Batch 16/64 loss: 0.09135586023330688
Batch 17/64 loss: 0.10538935661315918
Batch 18/64 loss: 0.10359185934066772
Batch 19/64 loss: 0.11360275745391846
Batch 20/64 loss: 0.08625787496566772
Batch 21/64 loss: 0.10593116283416748
Batch 22/64 loss: 0.10228317975997925
Batch 23/64 loss: 0.1096009612083435
Batch 24/64 loss: 0.10144031047821045
Batch 25/64 loss: 0.08567357063293457
Batch 26/64 loss: 0.11741477251052856
Batch 27/64 loss: 0.10877251625061035
Batch 28/64 loss: 0.11935245990753174
Batch 29/64 loss: 0.10779321193695068
Batch 30/64 loss: 0.10755634307861328
Batch 31/64 loss: 0.11636805534362793
Batch 32/64 loss: 0.09922701120376587
Batch 33/64 loss: 0.10683548450469971
Batch 34/64 loss: 0.10862123966217041
Batch 35/64 loss: 0.11610394716262817
Batch 36/64 loss: 0.0861736536026001
Batch 37/64 loss: 0.09900873899459839
Batch 38/64 loss: 0.07412159442901611
Batch 39/64 loss: 0.09815061092376709
Batch 40/64 loss: 0.09627199172973633
Batch 41/64 loss: 0.1381458044052124
Batch 42/64 loss: 0.12127965688705444
Batch 43/64 loss: 0.11285221576690674
Batch 44/64 loss: 0.12533777952194214
Batch 45/64 loss: 0.11928999423980713
Batch 46/64 loss: 0.08168715238571167
Batch 47/64 loss: 0.09472262859344482
Batch 48/64 loss: 0.0681389570236206
Batch 49/64 loss: 0.07986587285995483
Batch 50/64 loss: 0.08629995584487915
Batch 51/64 loss: 0.11030548810958862
Batch 52/64 loss: 0.1294577717781067
Batch 53/64 loss: 0.10693180561065674
Batch 54/64 loss: 0.08433860540390015
Batch 55/64 loss: 0.0894811749458313
Batch 56/64 loss: 0.10561245679855347
Batch 57/64 loss: 0.08843755722045898
Batch 58/64 loss: 0.10049253702163696
Batch 59/64 loss: 0.10323244333267212
Batch 60/64 loss: 0.1243361234664917
Batch 61/64 loss: 0.11119824647903442
Batch 62/64 loss: 0.0959547758102417
Batch 63/64 loss: 0.07015550136566162
Batch 64/64 loss: 0.10344356298446655
Epoch 128  Train loss: 0.10345443019679948  Val loss: 0.13853426241792763
Saving best model, epoch: 128
Epoch 129
-------------------------------
Batch 1/64 loss: 0.07817298173904419
Batch 2/64 loss: 0.11171168088912964
Batch 3/64 loss: 0.12679409980773926
Batch 4/64 loss: 0.09289401769638062
Batch 5/64 loss: 0.09789621829986572
Batch 6/64 loss: 0.11684197187423706
Batch 7/64 loss: 0.0842399001121521
Batch 8/64 loss: 0.11095070838928223
Batch 9/64 loss: 0.09328067302703857
Batch 10/64 loss: 0.09963792562484741
Batch 11/64 loss: 0.12128937244415283
Batch 12/64 loss: 0.12772715091705322
Batch 13/64 loss: 0.09629631042480469
Batch 14/64 loss: 0.10353463888168335
Batch 15/64 loss: 0.07057654857635498
Batch 16/64 loss: 0.08390688896179199
Batch 17/64 loss: 0.0822288990020752
Batch 18/64 loss: 0.09727942943572998
Batch 19/64 loss: 0.09255897998809814
Batch 20/64 loss: 0.12332749366760254
Batch 21/64 loss: 0.09332478046417236
Batch 22/64 loss: 0.1046297550201416
Batch 23/64 loss: 0.10140573978424072
Batch 24/64 loss: 0.08770644664764404
Batch 25/64 loss: 0.10340356826782227
Batch 26/64 loss: 0.12217003107070923
Batch 27/64 loss: 0.09866023063659668
Batch 28/64 loss: 0.10805433988571167
Batch 29/64 loss: 0.09350985288619995
Batch 30/64 loss: 0.0862581729888916
Batch 31/64 loss: 0.14069640636444092
Batch 32/64 loss: 0.10228884220123291
Batch 33/64 loss: 0.11474382877349854
Batch 34/64 loss: 0.11454612016677856
Batch 35/64 loss: 0.07813572883605957
Batch 36/64 loss: 0.10324490070343018
Batch 37/64 loss: 0.07619285583496094
Batch 38/64 loss: 0.12031900882720947
Batch 39/64 loss: 0.08952808380126953
Batch 40/64 loss: 0.10910987854003906
Batch 41/64 loss: 0.11039936542510986
Batch 42/64 loss: 0.12154281139373779
Batch 43/64 loss: 0.11437457799911499
Batch 44/64 loss: 0.1157805323600769
Batch 45/64 loss: 0.07576191425323486
Batch 46/64 loss: 0.09630942344665527
Batch 47/64 loss: 0.09951013326644897
Batch 48/64 loss: 0.08766400814056396
Batch 49/64 loss: 0.09737503528594971
Batch 50/64 loss: 0.11635082960128784
Batch 51/64 loss: 0.11505329608917236
Batch 52/64 loss: 0.10948395729064941
Batch 53/64 loss: 0.1052405834197998
Batch 54/64 loss: 0.12425786256790161
Batch 55/64 loss: 0.0998038649559021
Batch 56/64 loss: 0.09288227558135986
Batch 57/64 loss: 0.11302077770233154
Batch 58/64 loss: 0.10790836811065674
Batch 59/64 loss: 0.08080786466598511
Batch 60/64 loss: 0.08345043659210205
Batch 61/64 loss: 0.10403597354888916
Batch 62/64 loss: 0.11995404958724976
Batch 63/64 loss: 0.09924876689910889
Batch 64/64 loss: 0.08094507455825806
Epoch 129  Train loss: 0.10211764667548386  Val loss: 0.13738462195773304
Saving best model, epoch: 129
Epoch 130
-------------------------------
Batch 1/64 loss: 0.07588118314743042
Batch 2/64 loss: 0.11236602067947388
Batch 3/64 loss: 0.09739845991134644
Batch 4/64 loss: 0.0954352617263794
Batch 5/64 loss: 0.08330440521240234
Batch 6/64 loss: 0.08673501014709473
Batch 7/64 loss: 0.09817147254943848
Batch 8/64 loss: 0.07989954948425293
Batch 9/64 loss: 0.11690855026245117
Batch 10/64 loss: 0.09847933053970337
Batch 11/64 loss: 0.08703947067260742
Batch 12/64 loss: 0.10647702217102051
Batch 13/64 loss: 0.08385002613067627
Batch 14/64 loss: 0.09594184160232544
Batch 15/64 loss: 0.1209571361541748
Batch 16/64 loss: 0.07131302356719971
Batch 17/64 loss: 0.09231317043304443
Batch 18/64 loss: 0.10432744026184082
Batch 19/64 loss: 0.09013080596923828
Batch 20/64 loss: 0.09550237655639648
Batch 21/64 loss: 0.0861741304397583
Batch 22/64 loss: 0.0941152572631836
Batch 23/64 loss: 0.10731828212738037
Batch 24/64 loss: 0.10108846426010132
Batch 25/64 loss: 0.059037208557128906
Batch 26/64 loss: 0.10771560668945312
Batch 27/64 loss: 0.11271482706069946
Batch 28/64 loss: 0.10860317945480347
Batch 29/64 loss: 0.10042965412139893
Batch 30/64 loss: 0.08317321538925171
Batch 31/64 loss: 0.11292314529418945
Batch 32/64 loss: 0.09592097997665405
Batch 33/64 loss: 0.11429733037948608
Batch 34/64 loss: 0.1185300350189209
Batch 35/64 loss: 0.08935505151748657
Batch 36/64 loss: 0.09827244281768799
Batch 37/64 loss: 0.08441519737243652
Batch 38/64 loss: 0.12601017951965332
Batch 39/64 loss: 0.08438467979431152
Batch 40/64 loss: 0.07695907354354858
Batch 41/64 loss: 0.08650535345077515
Batch 42/64 loss: 0.09614992141723633
Batch 43/64 loss: 0.09715801477432251
Batch 44/64 loss: 0.11495453119277954
Batch 45/64 loss: 0.10190439224243164
Batch 46/64 loss: 0.08084142208099365
Batch 47/64 loss: 0.0935208797454834
Batch 48/64 loss: 0.11501955986022949
Batch 49/64 loss: 0.09572595357894897
Batch 50/64 loss: 0.09205621480941772
Batch 51/64 loss: 0.11261099576950073
Batch 52/64 loss: 0.13662219047546387
Batch 53/64 loss: 0.08718150854110718
Batch 54/64 loss: 0.10847407579421997
Batch 55/64 loss: 0.12176156044006348
Batch 56/64 loss: 0.12742340564727783
Batch 57/64 loss: 0.11969900131225586
Batch 58/64 loss: 0.0910906195640564
Batch 59/64 loss: 0.10943818092346191
Batch 60/64 loss: 0.12089306116104126
Batch 61/64 loss: 0.1080322265625
Batch 62/64 loss: 0.11259633302688599
Batch 63/64 loss: 0.08960855007171631
Batch 64/64 loss: 0.10397195816040039
Epoch 130  Train loss: 0.09962535558962354  Val loss: 0.1379988762930906
Epoch 131
-------------------------------
Batch 1/64 loss: 0.08509302139282227
Batch 2/64 loss: 0.08196556568145752
Batch 3/64 loss: 0.08583712577819824
Batch 4/64 loss: 0.09510499238967896
Batch 5/64 loss: 0.09586721658706665
Batch 6/64 loss: 0.09173703193664551
Batch 7/64 loss: 0.12139666080474854
Batch 8/64 loss: 0.08849972486495972
Batch 9/64 loss: 0.09138405323028564
Batch 10/64 loss: 0.11681538820266724
Batch 11/64 loss: 0.0755993127822876
Batch 12/64 loss: 0.10285639762878418
Batch 13/64 loss: 0.07608795166015625
Batch 14/64 loss: 0.1047627329826355
Batch 15/64 loss: 0.08619606494903564
Batch 16/64 loss: 0.10964095592498779
Batch 17/64 loss: 0.1092296838760376
Batch 18/64 loss: 0.10402143001556396
Batch 19/64 loss: 0.11662352085113525
Batch 20/64 loss: 0.09220433235168457
Batch 21/64 loss: 0.11071807146072388
Batch 22/64 loss: 0.08709990978240967
Batch 23/64 loss: 0.09439074993133545
Batch 24/64 loss: 0.1086421012878418
Batch 25/64 loss: 0.08152717351913452
Batch 26/64 loss: 0.09872835874557495
Batch 27/64 loss: 0.08995062112808228
Batch 28/64 loss: 0.08957940340042114
Batch 29/64 loss: 0.09094595909118652
Batch 30/64 loss: 0.12115448713302612
Batch 31/64 loss: 0.10220301151275635
Batch 32/64 loss: 0.10704350471496582
Batch 33/64 loss: 0.09380471706390381
Batch 34/64 loss: 0.14806324243545532
Batch 35/64 loss: 0.0833311676979065
Batch 36/64 loss: 0.10042834281921387
Batch 37/64 loss: 0.07955098152160645
Batch 38/64 loss: 0.11508816480636597
Batch 39/64 loss: 0.0827757716178894
Batch 40/64 loss: 0.09963423013687134
Batch 41/64 loss: 0.08752381801605225
Batch 42/64 loss: 0.07227039337158203
Batch 43/64 loss: 0.08770287036895752
Batch 44/64 loss: 0.1007000207901001
Batch 45/64 loss: 0.11584615707397461
Batch 46/64 loss: 0.11703044176101685
Batch 47/64 loss: 0.09990036487579346
Batch 48/64 loss: 0.1174430251121521
Batch 49/64 loss: 0.10564517974853516
Batch 50/64 loss: 0.13878804445266724
Batch 51/64 loss: 0.09728974103927612
Batch 52/64 loss: 0.11409205198287964
Batch 53/64 loss: 0.09556353092193604
Batch 54/64 loss: 0.08722209930419922
Batch 55/64 loss: 0.12072229385375977
Batch 56/64 loss: 0.10554426908493042
Batch 57/64 loss: 0.0834660530090332
Batch 58/64 loss: 0.09604775905609131
Batch 59/64 loss: 0.1137995719909668
Batch 60/64 loss: 0.13029241561889648
Batch 61/64 loss: 0.08468896150588989
Batch 62/64 loss: 0.12459492683410645
Batch 63/64 loss: 0.09641838073730469
Batch 64/64 loss: 0.11980819702148438
Epoch 131  Train loss: 0.10036128081527411  Val loss: 0.13582195633465483
Saving best model, epoch: 131
Epoch 132
-------------------------------
Batch 1/64 loss: 0.10967713594436646
Batch 2/64 loss: 0.08960235118865967
Batch 3/64 loss: 0.09029543399810791
Batch 4/64 loss: 0.08282208442687988
Batch 5/64 loss: 0.11730355024337769
Batch 6/64 loss: 0.11861568689346313
Batch 7/64 loss: 0.10402226448059082
Batch 8/64 loss: 0.08012187480926514
Batch 9/64 loss: 0.11267197132110596
Batch 10/64 loss: 0.1018834114074707
Batch 11/64 loss: 0.09921658039093018
Batch 12/64 loss: 0.08873170614242554
Batch 13/64 loss: 0.11004751920700073
Batch 14/64 loss: 0.10561233758926392
Batch 15/64 loss: 0.10958194732666016
Batch 16/64 loss: 0.09051281213760376
Batch 17/64 loss: 0.0991925597190857
Batch 18/64 loss: 0.07694244384765625
Batch 19/64 loss: 0.0920448899269104
Batch 20/64 loss: 0.08494514226913452
Batch 21/64 loss: 0.13003301620483398
Batch 22/64 loss: 0.08905363082885742
Batch 23/64 loss: 0.11930984258651733
Batch 24/64 loss: 0.10447847843170166
Batch 25/64 loss: 0.11633241176605225
Batch 26/64 loss: 0.08163827657699585
Batch 27/64 loss: 0.09585082530975342
Batch 28/64 loss: 0.09771525859832764
Batch 29/64 loss: 0.06640183925628662
Batch 30/64 loss: 0.10966956615447998
Batch 31/64 loss: 0.10135471820831299
Batch 32/64 loss: 0.08752751350402832
Batch 33/64 loss: 0.098125159740448
Batch 34/64 loss: 0.08904409408569336
Batch 35/64 loss: 0.13706755638122559
Batch 36/64 loss: 0.09648215770721436
Batch 37/64 loss: 0.08205986022949219
Batch 38/64 loss: 0.13648849725723267
Batch 39/64 loss: 0.09684890508651733
Batch 40/64 loss: 0.10561418533325195
Batch 41/64 loss: 0.07900869846343994
Batch 42/64 loss: 0.12475085258483887
Batch 43/64 loss: 0.08380657434463501
Batch 44/64 loss: 0.11530303955078125
Batch 45/64 loss: 0.12787264585494995
Batch 46/64 loss: 0.08093476295471191
Batch 47/64 loss: 0.11847180128097534
Batch 48/64 loss: 0.09358912706375122
Batch 49/64 loss: 0.0889657735824585
Batch 50/64 loss: 0.09701788425445557
Batch 51/64 loss: 0.07982146739959717
Batch 52/64 loss: 0.11981379985809326
Batch 53/64 loss: 0.10423415899276733
Batch 54/64 loss: 0.10794270038604736
Batch 55/64 loss: 0.07961869239807129
Batch 56/64 loss: 0.10116708278656006
Batch 57/64 loss: 0.0909266471862793
Batch 58/64 loss: 0.11430007219314575
Batch 59/64 loss: 0.09011375904083252
Batch 60/64 loss: 0.10333287715911865
Batch 61/64 loss: 0.10709917545318604
Batch 62/64 loss: 0.08710998296737671
Batch 63/64 loss: 0.11747193336486816
Batch 64/64 loss: 0.14272278547286987
Epoch 132  Train loss: 0.10077890344694548  Val loss: 0.13880844603699097
Epoch 133
-------------------------------
Batch 1/64 loss: 0.06744915246963501
Batch 2/64 loss: 0.11231422424316406
Batch 3/64 loss: 0.11223649978637695
Batch 4/64 loss: 0.09292060136795044
Batch 5/64 loss: 0.105246901512146
Batch 6/64 loss: 0.09390151500701904
Batch 7/64 loss: 0.08576476573944092
Batch 8/64 loss: 0.0963631272315979
Batch 9/64 loss: 0.08027160167694092
Batch 10/64 loss: 0.10742896795272827
Batch 11/64 loss: 0.09139031171798706
Batch 12/64 loss: 0.08690309524536133
Batch 13/64 loss: 0.12044423818588257
Batch 14/64 loss: 0.11583137512207031
Batch 15/64 loss: 0.11674189567565918
Batch 16/64 loss: 0.11002284288406372
Batch 17/64 loss: 0.10350912809371948
Batch 18/64 loss: 0.1061406135559082
Batch 19/64 loss: 0.11047244071960449
Batch 20/64 loss: 0.10354560613632202
Batch 21/64 loss: 0.09423351287841797
Batch 22/64 loss: 0.09838664531707764
Batch 23/64 loss: 0.09733664989471436
Batch 24/64 loss: 0.10008776187896729
Batch 25/64 loss: 0.09149885177612305
Batch 26/64 loss: 0.14053940773010254
Batch 27/64 loss: 0.09296154975891113
Batch 28/64 loss: 0.10784077644348145
Batch 29/64 loss: 0.09377086162567139
Batch 30/64 loss: 0.0972132682800293
Batch 31/64 loss: 0.12274199724197388
Batch 32/64 loss: 0.0927966833114624
Batch 33/64 loss: 0.08606445789337158
Batch 34/64 loss: 0.12010300159454346
Batch 35/64 loss: 0.09126728773117065
Batch 36/64 loss: 0.10844886302947998
Batch 37/64 loss: 0.09503579139709473
Batch 38/64 loss: 0.11112380027770996
Batch 39/64 loss: 0.08597540855407715
Batch 40/64 loss: 0.061898231506347656
Batch 41/64 loss: 0.12083977460861206
Batch 42/64 loss: 0.10896122455596924
Batch 43/64 loss: 0.10445308685302734
Batch 44/64 loss: 0.09839141368865967
Batch 45/64 loss: 0.09492802619934082
Batch 46/64 loss: 0.11937850713729858
Batch 47/64 loss: 0.1136312484741211
Batch 48/64 loss: 0.08540070056915283
Batch 49/64 loss: 0.08736127614974976
Batch 50/64 loss: 0.12034797668457031
Batch 51/64 loss: 0.1086154580116272
Batch 52/64 loss: 0.08514630794525146
Batch 53/64 loss: 0.09896659851074219
Batch 54/64 loss: 0.07403641939163208
Batch 55/64 loss: 0.08988761901855469
Batch 56/64 loss: 0.10142230987548828
Batch 57/64 loss: 0.09241032600402832
Batch 58/64 loss: 0.0853700041770935
Batch 59/64 loss: 0.08901524543762207
Batch 60/64 loss: 0.09487712383270264
Batch 61/64 loss: 0.08357840776443481
Batch 62/64 loss: 0.12119871377944946
Batch 63/64 loss: 0.11631739139556885
Batch 64/64 loss: 0.08999419212341309
Epoch 133  Train loss: 0.0999255610447304  Val loss: 0.13762544356670575
Epoch 134
-------------------------------
Batch 1/64 loss: 0.09732544422149658
Batch 2/64 loss: 0.09845149517059326
Batch 3/64 loss: 0.08943432569503784
Batch 4/64 loss: 0.1034536361694336
Batch 5/64 loss: 0.1295437216758728
Batch 6/64 loss: 0.07791781425476074
Batch 7/64 loss: 0.08550882339477539
Batch 8/64 loss: 0.12064933776855469
Batch 9/64 loss: 0.11644697189331055
Batch 10/64 loss: 0.08878922462463379
Batch 11/64 loss: 0.10277706384658813
Batch 12/64 loss: 0.10085207223892212
Batch 13/64 loss: 0.11555308103561401
Batch 14/64 loss: 0.12789922952651978
Batch 15/64 loss: 0.12281548976898193
Batch 16/64 loss: 0.10072362422943115
Batch 17/64 loss: 0.11193442344665527
Batch 18/64 loss: 0.09600555896759033
Batch 19/64 loss: 0.11217892169952393
Batch 20/64 loss: 0.07528388500213623
Batch 21/64 loss: 0.09610360860824585
Batch 22/64 loss: 0.10382115840911865
Batch 23/64 loss: 0.08765506744384766
Batch 24/64 loss: 0.10240352153778076
Batch 25/64 loss: 0.08162975311279297
Batch 26/64 loss: 0.1203012466430664
Batch 27/64 loss: 0.07602083683013916
Batch 28/64 loss: 0.10793870687484741
Batch 29/64 loss: 0.12908446788787842
Batch 30/64 loss: 0.1273864507675171
Batch 31/64 loss: 0.08286476135253906
Batch 32/64 loss: 0.09410750865936279
Batch 33/64 loss: 0.09432792663574219
Batch 34/64 loss: 0.11336296796798706
Batch 35/64 loss: 0.07704293727874756
Batch 36/64 loss: 0.09549826383590698
Batch 37/64 loss: 0.08942675590515137
Batch 38/64 loss: 0.0947992205619812
Batch 39/64 loss: 0.07985836267471313
Batch 40/64 loss: 0.08919697999954224
Batch 41/64 loss: 0.09105145931243896
Batch 42/64 loss: 0.12165302038192749
Batch 43/64 loss: 0.09951174259185791
Batch 44/64 loss: 0.06975901126861572
Batch 45/64 loss: 0.09194117784500122
Batch 46/64 loss: 0.10773563385009766
Batch 47/64 loss: 0.08399087190628052
Batch 48/64 loss: 0.11384433507919312
Batch 49/64 loss: 0.11097842454910278
Batch 50/64 loss: 0.08494406938552856
Batch 51/64 loss: 0.11587792634963989
Batch 52/64 loss: 0.12035119533538818
Batch 53/64 loss: 0.10588723421096802
Batch 54/64 loss: 0.07438623905181885
Batch 55/64 loss: 0.07545983791351318
Batch 56/64 loss: 0.10493612289428711
Batch 57/64 loss: 0.09119170904159546
Batch 58/64 loss: 0.11083221435546875
Batch 59/64 loss: 0.09738105535507202
Batch 60/64 loss: 0.10343313217163086
Batch 61/64 loss: 0.1040111780166626
Batch 62/64 loss: 0.08095687627792358
Batch 63/64 loss: 0.09308773279190063
Batch 64/64 loss: 0.09770011901855469
Epoch 134  Train loss: 0.0994957166559556  Val loss: 0.13640992338305077
Epoch 135
-------------------------------
Batch 1/64 loss: 0.07981431484222412
Batch 2/64 loss: 0.10250657796859741
Batch 3/64 loss: 0.09669697284698486
Batch 4/64 loss: 0.10682213306427002
Batch 5/64 loss: 0.08810043334960938
Batch 6/64 loss: 0.11128443479537964
Batch 7/64 loss: 0.09828329086303711
Batch 8/64 loss: 0.13624495267868042
Batch 9/64 loss: 0.08002305030822754
Batch 10/64 loss: 0.0922970175743103
Batch 11/64 loss: 0.07566934823989868
Batch 12/64 loss: 0.08256393671035767
Batch 13/64 loss: 0.09200477600097656
Batch 14/64 loss: 0.08950620889663696
Batch 15/64 loss: 0.0948060154914856
Batch 16/64 loss: 0.09632909297943115
Batch 17/64 loss: 0.0829358696937561
Batch 18/64 loss: 0.09692525863647461
Batch 19/64 loss: 0.09383571147918701
Batch 20/64 loss: 0.1139708161354065
Batch 21/64 loss: 0.08749198913574219
Batch 22/64 loss: 0.12359386682510376
Batch 23/64 loss: 0.0979352593421936
Batch 24/64 loss: 0.10296684503555298
Batch 25/64 loss: 0.09090590476989746
Batch 26/64 loss: 0.09772336483001709
Batch 27/64 loss: 0.09333223104476929
Batch 28/64 loss: 0.10377848148345947
Batch 29/64 loss: 0.08392786979675293
Batch 30/64 loss: 0.1077263355255127
Batch 31/64 loss: 0.10869216918945312
Batch 32/64 loss: 0.08346128463745117
Batch 33/64 loss: 0.11334812641143799
Batch 34/64 loss: 0.06310927867889404
Batch 35/64 loss: 0.12438416481018066
Batch 36/64 loss: 0.08329802751541138
Batch 37/64 loss: 0.10194909572601318
Batch 38/64 loss: 0.08402341604232788
Batch 39/64 loss: 0.087929368019104
Batch 40/64 loss: 0.10822194814682007
Batch 41/64 loss: 0.08627647161483765
Batch 42/64 loss: 0.08944576978683472
Batch 43/64 loss: 0.08001387119293213
Batch 44/64 loss: 0.10595810413360596
Batch 45/64 loss: 0.11847454309463501
Batch 46/64 loss: 0.07841527462005615
Batch 47/64 loss: 0.12245714664459229
Batch 48/64 loss: 0.09659832715988159
Batch 49/64 loss: 0.10554695129394531
Batch 50/64 loss: 0.09585601091384888
Batch 51/64 loss: 0.10605770349502563
Batch 52/64 loss: 0.10078948736190796
Batch 53/64 loss: 0.07891392707824707
Batch 54/64 loss: 0.13881778717041016
Batch 55/64 loss: 0.11700665950775146
Batch 56/64 loss: 0.09085869789123535
Batch 57/64 loss: 0.10726845264434814
Batch 58/64 loss: 0.08164054155349731
Batch 59/64 loss: 0.11788821220397949
Batch 60/64 loss: 0.08838289976119995
Batch 61/64 loss: 0.11110377311706543
Batch 62/64 loss: 0.11688125133514404
Batch 63/64 loss: 0.1280246376991272
Batch 64/64 loss: 0.1110687255859375
Epoch 135  Train loss: 0.0988925064311308  Val loss: 0.14259788178906
Epoch 136
-------------------------------
Batch 1/64 loss: 0.08727371692657471
Batch 2/64 loss: 0.09804832935333252
Batch 3/64 loss: 0.08435547351837158
Batch 4/64 loss: 0.10606646537780762
Batch 5/64 loss: 0.09839534759521484
Batch 6/64 loss: 0.10289895534515381
Batch 7/64 loss: 0.1158750057220459
Batch 8/64 loss: 0.1194038987159729
Batch 9/64 loss: 0.0723428726196289
Batch 10/64 loss: 0.09251099824905396
Batch 11/64 loss: 0.08965671062469482
Batch 12/64 loss: 0.08918428421020508
Batch 13/64 loss: 0.09340757131576538
Batch 14/64 loss: 0.07643944025039673
Batch 15/64 loss: 0.11187195777893066
Batch 16/64 loss: 0.09422332048416138
Batch 17/64 loss: 0.10101044178009033
Batch 18/64 loss: 0.1115196943283081
Batch 19/64 loss: 0.10268080234527588
Batch 20/64 loss: 0.09110045433044434
Batch 21/64 loss: 0.09541481733322144
Batch 22/64 loss: 0.09342116117477417
Batch 23/64 loss: 0.08389556407928467
Batch 24/64 loss: 0.09746646881103516
Batch 25/64 loss: 0.08307373523712158
Batch 26/64 loss: 0.10405910015106201
Batch 27/64 loss: 0.08407986164093018
Batch 28/64 loss: 0.10366886854171753
Batch 29/64 loss: 0.11815208196640015
Batch 30/64 loss: 0.08571141958236694
Batch 31/64 loss: 0.12073266506195068
Batch 32/64 loss: 0.07186281681060791
Batch 33/64 loss: 0.10186624526977539
Batch 34/64 loss: 0.11412298679351807
Batch 35/64 loss: 0.06919270753860474
Batch 36/64 loss: 0.07829862833023071
Batch 37/64 loss: 0.1024019718170166
Batch 38/64 loss: 0.10570180416107178
Batch 39/64 loss: 0.08747047185897827
Batch 40/64 loss: 0.09905022382736206
Batch 41/64 loss: 0.09714078903198242
Batch 42/64 loss: 0.09509384632110596
Batch 43/64 loss: 0.08389270305633545
Batch 44/64 loss: 0.10966193675994873
Batch 45/64 loss: 0.09440702199935913
Batch 46/64 loss: 0.10218888521194458
Batch 47/64 loss: 0.08944016695022583
Batch 48/64 loss: 0.10486805438995361
Batch 49/64 loss: 0.0946316123008728
Batch 50/64 loss: 0.10544615983963013
Batch 51/64 loss: 0.09785431623458862
Batch 52/64 loss: 0.08809083700180054
Batch 53/64 loss: 0.0664287805557251
Batch 54/64 loss: 0.13057327270507812
Batch 55/64 loss: 0.10124450922012329
Batch 56/64 loss: 0.09068459272384644
Batch 57/64 loss: 0.08775776624679565
Batch 58/64 loss: 0.09398406744003296
Batch 59/64 loss: 0.12830960750579834
Batch 60/64 loss: 0.11067283153533936
Batch 61/64 loss: 0.13442587852478027
Batch 62/64 loss: 0.10497075319290161
Batch 63/64 loss: 0.10764020681381226
Batch 64/64 loss: 0.11546683311462402
Epoch 136  Train loss: 0.09794381272559073  Val loss: 0.13765186501532486
Epoch 137
-------------------------------
Batch 1/64 loss: 0.077545166015625
Batch 2/64 loss: 0.12963920831680298
Batch 3/64 loss: 0.10592925548553467
Batch 4/64 loss: 0.09193331003189087
Batch 5/64 loss: 0.10883063077926636
Batch 6/64 loss: 0.08643901348114014
Batch 7/64 loss: 0.08610659837722778
Batch 8/64 loss: 0.08544647693634033
Batch 9/64 loss: 0.11186540126800537
Batch 10/64 loss: 0.10157501697540283
Batch 11/64 loss: 0.08576154708862305
Batch 12/64 loss: 0.08843076229095459
Batch 13/64 loss: 0.08522319793701172
Batch 14/64 loss: 0.09766727685928345
Batch 15/64 loss: 0.11148273944854736
Batch 16/64 loss: 0.11319053173065186
Batch 17/64 loss: 0.09006202220916748
Batch 18/64 loss: 0.07900732755661011
Batch 19/64 loss: 0.10531473159790039
Batch 20/64 loss: 0.0829848051071167
Batch 21/64 loss: 0.09404808282852173
Batch 22/64 loss: 0.11380839347839355
Batch 23/64 loss: 0.08690750598907471
Batch 24/64 loss: 0.1010279655456543
Batch 25/64 loss: 0.11486262083053589
Batch 26/64 loss: 0.08373379707336426
Batch 27/64 loss: 0.08987116813659668
Batch 28/64 loss: 0.1132967472076416
Batch 29/64 loss: 0.08900690078735352
Batch 30/64 loss: 0.12868553400039673
Batch 31/64 loss: 0.09091150760650635
Batch 32/64 loss: 0.10185515880584717
Batch 33/64 loss: 0.07880085706710815
Batch 34/64 loss: 0.10588693618774414
Batch 35/64 loss: 0.08734023571014404
Batch 36/64 loss: 0.09349191188812256
Batch 37/64 loss: 0.11068743467330933
Batch 38/64 loss: 0.12630677223205566
Batch 39/64 loss: 0.11216127872467041
Batch 40/64 loss: 0.10865992307662964
Batch 41/64 loss: 0.11946505308151245
Batch 42/64 loss: 0.07708388566970825
Batch 43/64 loss: 0.07644098997116089
Batch 44/64 loss: 0.09959262609481812
Batch 45/64 loss: 0.09224927425384521
Batch 46/64 loss: 0.09960103034973145
Batch 47/64 loss: 0.08710670471191406
Batch 48/64 loss: 0.08733510971069336
Batch 49/64 loss: 0.08815234899520874
Batch 50/64 loss: 0.10139304399490356
Batch 51/64 loss: 0.11256200075149536
Batch 52/64 loss: 0.09162896871566772
Batch 53/64 loss: 0.06830769777297974
Batch 54/64 loss: 0.08730995655059814
Batch 55/64 loss: 0.09157878160476685
Batch 56/64 loss: 0.09999954700469971
Batch 57/64 loss: 0.11039775609970093
Batch 58/64 loss: 0.09059250354766846
Batch 59/64 loss: 0.0943993330001831
Batch 60/64 loss: 0.08570098876953125
Batch 61/64 loss: 0.12557005882263184
Batch 62/64 loss: 0.09945642948150635
Batch 63/64 loss: 0.09408426284790039
Batch 64/64 loss: 0.09952950477600098
Epoch 137  Train loss: 0.09741868598788392  Val loss: 0.13829047552908408
Epoch 138
-------------------------------
Batch 1/64 loss: 0.08985930681228638
Batch 2/64 loss: 0.09495341777801514
Batch 3/64 loss: 0.0953945517539978
Batch 4/64 loss: 0.09996700286865234
Batch 5/64 loss: 0.07325494289398193
Batch 6/64 loss: 0.09003502130508423
Batch 7/64 loss: 0.09943336248397827
Batch 8/64 loss: 0.060724854469299316
Batch 9/64 loss: 0.10279333591461182
Batch 10/64 loss: 0.09438061714172363
Batch 11/64 loss: 0.0882035493850708
Batch 12/64 loss: 0.10049444437026978
Batch 13/64 loss: 0.09473282098770142
Batch 14/64 loss: 0.10210424661636353
Batch 15/64 loss: 0.12505453824996948
Batch 16/64 loss: 0.09905433654785156
Batch 17/64 loss: 0.09331637620925903
Batch 18/64 loss: 0.09305429458618164
Batch 19/64 loss: 0.10517638921737671
Batch 20/64 loss: 0.07750821113586426
Batch 21/64 loss: 0.11402320861816406
Batch 22/64 loss: 0.11928004026412964
Batch 23/64 loss: 0.11760616302490234
Batch 24/64 loss: 0.0994451642036438
Batch 25/64 loss: 0.13012337684631348
Batch 26/64 loss: 0.09461283683776855
Batch 27/64 loss: 0.11292743682861328
Batch 28/64 loss: 0.07837581634521484
Batch 29/64 loss: 0.11479395627975464
Batch 30/64 loss: 0.10935920476913452
Batch 31/64 loss: 0.10570454597473145
Batch 32/64 loss: 0.09450405836105347
Batch 33/64 loss: 0.10177379846572876
Batch 34/64 loss: 0.10252785682678223
Batch 35/64 loss: 0.10069304704666138
Batch 36/64 loss: 0.08247840404510498
Batch 37/64 loss: 0.12312382459640503
Batch 38/64 loss: 0.10202491283416748
Batch 39/64 loss: 0.09671294689178467
Batch 40/64 loss: 0.10132831335067749
Batch 41/64 loss: 0.06356346607208252
Batch 42/64 loss: 0.07045447826385498
Batch 43/64 loss: 0.11779147386550903
Batch 44/64 loss: 0.11526304483413696
Batch 45/64 loss: 0.09113222360610962
Batch 46/64 loss: 0.13431274890899658
Batch 47/64 loss: 0.09871852397918701
Batch 48/64 loss: 0.07897251844406128
Batch 49/64 loss: 0.0761260986328125
Batch 50/64 loss: 0.10080075263977051
Batch 51/64 loss: 0.08588707447052002
Batch 52/64 loss: 0.06533181667327881
Batch 53/64 loss: 0.08263152837753296
Batch 54/64 loss: 0.1070898175239563
Batch 55/64 loss: 0.10209071636199951
Batch 56/64 loss: 0.10142439603805542
Batch 57/64 loss: 0.08125507831573486
Batch 58/64 loss: 0.08948791027069092
Batch 59/64 loss: 0.09734493494033813
Batch 60/64 loss: 0.10854840278625488
Batch 61/64 loss: 0.10476469993591309
Batch 62/64 loss: 0.08784592151641846
Batch 63/64 loss: 0.10617226362228394
Batch 64/64 loss: 0.06139153242111206
Epoch 138  Train loss: 0.09716028349072325  Val loss: 0.13333453366027256
Saving best model, epoch: 138
Epoch 139
-------------------------------
Batch 1/64 loss: 0.07747966051101685
Batch 2/64 loss: 0.11227142810821533
Batch 3/64 loss: 0.1021161675453186
Batch 4/64 loss: 0.10249125957489014
Batch 5/64 loss: 0.08595645427703857
Batch 6/64 loss: 0.10865658521652222
Batch 7/64 loss: 0.10687237977981567
Batch 8/64 loss: 0.07290208339691162
Batch 9/64 loss: 0.08097279071807861
Batch 10/64 loss: 0.12635594606399536
Batch 11/64 loss: 0.09506940841674805
Batch 12/64 loss: 0.0740891695022583
Batch 13/64 loss: 0.10064983367919922
Batch 14/64 loss: 0.10093551874160767
Batch 15/64 loss: 0.06199204921722412
Batch 16/64 loss: 0.10734248161315918
Batch 17/64 loss: 0.09783101081848145
Batch 18/64 loss: 0.09075307846069336
Batch 19/64 loss: 0.09461331367492676
Batch 20/64 loss: 0.09592705965042114
Batch 21/64 loss: 0.07223528623580933
Batch 22/64 loss: 0.11294090747833252
Batch 23/64 loss: 0.11810654401779175
Batch 24/64 loss: 0.11872518062591553
Batch 25/64 loss: 0.09661334753036499
Batch 26/64 loss: 0.09034371376037598
Batch 27/64 loss: 0.08010375499725342
Batch 28/64 loss: 0.08619087934494019
Batch 29/64 loss: 0.10648107528686523
Batch 30/64 loss: 0.0962759256362915
Batch 31/64 loss: 0.1159660816192627
Batch 32/64 loss: 0.08699548244476318
Batch 33/64 loss: 0.1113278865814209
Batch 34/64 loss: 0.10025107860565186
Batch 35/64 loss: 0.1053919792175293
Batch 36/64 loss: 0.13427937030792236
Batch 37/64 loss: 0.09515106678009033
Batch 38/64 loss: 0.08709204196929932
Batch 39/64 loss: 0.08089691400527954
Batch 40/64 loss: 0.08066421747207642
Batch 41/64 loss: 0.07262176275253296
Batch 42/64 loss: 0.1074061393737793
Batch 43/64 loss: 0.114737868309021
Batch 44/64 loss: 0.09980636835098267
Batch 45/64 loss: 0.07744073867797852
Batch 46/64 loss: 0.12206166982650757
Batch 47/64 loss: 0.09020102024078369
Batch 48/64 loss: 0.10984635353088379
Batch 49/64 loss: 0.12460947036743164
Batch 50/64 loss: 0.07909470796585083
Batch 51/64 loss: 0.09514498710632324
Batch 52/64 loss: 0.10604679584503174
Batch 53/64 loss: 0.08387303352355957
Batch 54/64 loss: 0.07728606462478638
Batch 55/64 loss: 0.05925077199935913
Batch 56/64 loss: 0.06318908929824829
Batch 57/64 loss: 0.09365922212600708
Batch 58/64 loss: 0.08595526218414307
Batch 59/64 loss: 0.11945360898971558
Batch 60/64 loss: 0.1102026104927063
Batch 61/64 loss: 0.11586147546768188
Batch 62/64 loss: 0.11563152074813843
Batch 63/64 loss: 0.10220098495483398
Batch 64/64 loss: 0.09825021028518677
Epoch 139  Train loss: 0.09679340544868918  Val loss: 0.14008745213144833
Epoch 140
-------------------------------
Batch 1/64 loss: 0.08641678094863892
Batch 2/64 loss: 0.11048996448516846
Batch 3/64 loss: 0.10380113124847412
Batch 4/64 loss: 0.09418588876724243
Batch 5/64 loss: 0.06359964609146118
Batch 6/64 loss: 0.10074615478515625
Batch 7/64 loss: 0.10725593566894531
Batch 8/64 loss: 0.08223116397857666
Batch 9/64 loss: 0.06539636850357056
Batch 10/64 loss: 0.09346818923950195
Batch 11/64 loss: 0.08621549606323242
Batch 12/64 loss: 0.11522138118743896
Batch 13/64 loss: 0.12229371070861816
Batch 14/64 loss: 0.09837490320205688
Batch 15/64 loss: 0.10481786727905273
Batch 16/64 loss: 0.10989052057266235
Batch 17/64 loss: 0.10094678401947021
Batch 18/64 loss: 0.07538938522338867
Batch 19/64 loss: 0.10602521896362305
Batch 20/64 loss: 0.09252363443374634
Batch 21/64 loss: 0.09959238767623901
Batch 22/64 loss: 0.07834750413894653
Batch 23/64 loss: 0.1025475263595581
Batch 24/64 loss: 0.08792358636856079
Batch 25/64 loss: 0.09473717212677002
Batch 26/64 loss: 0.10547757148742676
Batch 27/64 loss: 0.09391272068023682
Batch 28/64 loss: 0.10800313949584961
Batch 29/64 loss: 0.10137039422988892
Batch 30/64 loss: 0.08858340978622437
Batch 31/64 loss: 0.0919676423072815
Batch 32/64 loss: 0.09791690111160278
Batch 33/64 loss: 0.09826052188873291
Batch 34/64 loss: 0.08945643901824951
Batch 35/64 loss: 0.09884172677993774
Batch 36/64 loss: 0.09885096549987793
Batch 37/64 loss: 0.08355343341827393
Batch 38/64 loss: 0.09140533208847046
Batch 39/64 loss: 0.08551853895187378
Batch 40/64 loss: 0.10555833578109741
Batch 41/64 loss: 0.07670366764068604
Batch 42/64 loss: 0.11063039302825928
Batch 43/64 loss: 0.114837646484375
Batch 44/64 loss: 0.09043765068054199
Batch 45/64 loss: 0.11362934112548828
Batch 46/64 loss: 0.11393129825592041
Batch 47/64 loss: 0.1151551604270935
Batch 48/64 loss: 0.08142733573913574
Batch 49/64 loss: 0.09565776586532593
Batch 50/64 loss: 0.09679710865020752
Batch 51/64 loss: 0.0954136848449707
Batch 52/64 loss: 0.1012914776802063
Batch 53/64 loss: 0.10113519430160522
Batch 54/64 loss: 0.11179232597351074
Batch 55/64 loss: 0.08080399036407471
Batch 56/64 loss: 0.09728950262069702
Batch 57/64 loss: 0.08627241849899292
Batch 58/64 loss: 0.10275518894195557
Batch 59/64 loss: 0.0969017744064331
Batch 60/64 loss: 0.0926174521446228
Batch 61/64 loss: 0.0897565484046936
Batch 62/64 loss: 0.1344834566116333
Batch 63/64 loss: 0.09742021560668945
Batch 64/64 loss: 0.08853721618652344
Epoch 140  Train loss: 0.09701425701964135  Val loss: 0.1382362942925024
Epoch 141
-------------------------------
Batch 1/64 loss: 0.10135120153427124
Batch 2/64 loss: 0.09628432989120483
Batch 3/64 loss: 0.07545888423919678
Batch 4/64 loss: 0.09795111417770386
Batch 5/64 loss: 0.0921279788017273
Batch 6/64 loss: 0.07110828161239624
Batch 7/64 loss: 0.1003372073173523
Batch 8/64 loss: 0.10227447748184204
Batch 9/64 loss: 0.09026342630386353
Batch 10/64 loss: 0.09447312355041504
Batch 11/64 loss: 0.08911871910095215
Batch 12/64 loss: 0.07122576236724854
Batch 13/64 loss: 0.08946108818054199
Batch 14/64 loss: 0.11344176530838013
Batch 15/64 loss: 0.08070099353790283
Batch 16/64 loss: 0.10081696510314941
Batch 17/64 loss: 0.08489519357681274
Batch 18/64 loss: 0.09660518169403076
Batch 19/64 loss: 0.07453757524490356
Batch 20/64 loss: 0.08439266681671143
Batch 21/64 loss: 0.09594285488128662
Batch 22/64 loss: 0.09969282150268555
Batch 23/64 loss: 0.11647117137908936
Batch 24/64 loss: 0.10508936643600464
Batch 25/64 loss: 0.09672445058822632
Batch 26/64 loss: 0.0982964038848877
Batch 27/64 loss: 0.10238724946975708
Batch 28/64 loss: 0.08712995052337646
Batch 29/64 loss: 0.070445716381073
Batch 30/64 loss: 0.0738288164138794
Batch 31/64 loss: 0.08580851554870605
Batch 32/64 loss: 0.11861991882324219
Batch 33/64 loss: 0.07875931262969971
Batch 34/64 loss: 0.11170250177383423
Batch 35/64 loss: 0.07945585250854492
Batch 36/64 loss: 0.13495200872421265
Batch 37/64 loss: 0.09289360046386719
Batch 38/64 loss: 0.10663217306137085
Batch 39/64 loss: 0.12050062417984009
Batch 40/64 loss: 0.12018024921417236
Batch 41/64 loss: 0.07573544979095459
Batch 42/64 loss: 0.115120530128479
Batch 43/64 loss: 0.08583402633666992
Batch 44/64 loss: 0.08657199144363403
Batch 45/64 loss: 0.11051034927368164
Batch 46/64 loss: 0.10942953824996948
Batch 47/64 loss: 0.0949392318725586
Batch 48/64 loss: 0.10776597261428833
Batch 49/64 loss: 0.12378686666488647
Batch 50/64 loss: 0.10796833038330078
Batch 51/64 loss: 0.08373016119003296
Batch 52/64 loss: 0.07719951868057251
Batch 53/64 loss: 0.09868359565734863
Batch 54/64 loss: 0.07923316955566406
Batch 55/64 loss: 0.0814552903175354
Batch 56/64 loss: 0.10591286420822144
Batch 57/64 loss: 0.10213947296142578
Batch 58/64 loss: 0.10119223594665527
Batch 59/64 loss: 0.0889432430267334
Batch 60/64 loss: 0.09128624200820923
Batch 61/64 loss: 0.10574221611022949
Batch 62/64 loss: 0.11603653430938721
Batch 63/64 loss: 0.09196412563323975
Batch 64/64 loss: 0.07747501134872437
Epoch 141  Train loss: 0.09571179151535034  Val loss: 0.1350302399228938
Epoch 142
-------------------------------
Batch 1/64 loss: 0.07648062705993652
Batch 2/64 loss: 0.08240711688995361
Batch 3/64 loss: 0.10859137773513794
Batch 4/64 loss: 0.0996045470237732
Batch 5/64 loss: 0.07512891292572021
Batch 6/64 loss: 0.12076348066329956
Batch 7/64 loss: 0.08616149425506592
Batch 8/64 loss: 0.0779106616973877
Batch 9/64 loss: 0.09996688365936279
Batch 10/64 loss: 0.10107898712158203
Batch 11/64 loss: 0.1010056734085083
Batch 12/64 loss: 0.06683593988418579
Batch 13/64 loss: 0.10207515954971313
Batch 14/64 loss: 0.08923214673995972
Batch 15/64 loss: 0.08023256063461304
Batch 16/64 loss: 0.1071731448173523
Batch 17/64 loss: 0.10398894548416138
Batch 18/64 loss: 0.09841442108154297
Batch 19/64 loss: 0.07726532220840454
Batch 20/64 loss: 0.06994324922561646
Batch 21/64 loss: 0.10936325788497925
Batch 22/64 loss: 0.07727611064910889
Batch 23/64 loss: 0.07250744104385376
Batch 24/64 loss: 0.0950930118560791
Batch 25/64 loss: 0.08501178026199341
Batch 26/64 loss: 0.0793866515159607
Batch 27/64 loss: 0.10037946701049805
Batch 28/64 loss: 0.1026073694229126
Batch 29/64 loss: 0.08603692054748535
Batch 30/64 loss: 0.09359979629516602
Batch 31/64 loss: 0.1170695424079895
Batch 32/64 loss: 0.07739853858947754
Batch 33/64 loss: 0.10274916887283325
Batch 34/64 loss: 0.07417845726013184
Batch 35/64 loss: 0.09504097700119019
Batch 36/64 loss: 0.10913842916488647
Batch 37/64 loss: 0.10400223731994629
Batch 38/64 loss: 0.1256875991821289
Batch 39/64 loss: 0.09179270267486572
Batch 40/64 loss: 0.08865231275558472
Batch 41/64 loss: 0.15616387128829956
Batch 42/64 loss: 0.09639674425125122
Batch 43/64 loss: 0.09611868858337402
Batch 44/64 loss: 0.14102643728256226
Batch 45/64 loss: 0.11023736000061035
Batch 46/64 loss: 0.09316062927246094
Batch 47/64 loss: 0.07534843683242798
Batch 48/64 loss: 0.10001277923583984
Batch 49/64 loss: 0.09765756130218506
Batch 50/64 loss: 0.07688373327255249
Batch 51/64 loss: 0.06773209571838379
Batch 52/64 loss: 0.07628202438354492
Batch 53/64 loss: 0.08916163444519043
Batch 54/64 loss: 0.1048283576965332
Batch 55/64 loss: 0.09143006801605225
Batch 56/64 loss: 0.10339224338531494
Batch 57/64 loss: 0.09180420637130737
Batch 58/64 loss: 0.12054777145385742
Batch 59/64 loss: 0.08550769090652466
Batch 60/64 loss: 0.07966971397399902
Batch 61/64 loss: 0.10993647575378418
Batch 62/64 loss: 0.08699542284011841
Batch 63/64 loss: 0.11093336343765259
Batch 64/64 loss: 0.06892967224121094
Epoch 142  Train loss: 0.09449658019869936  Val loss: 0.13524473819536031
Epoch 143
-------------------------------
Batch 1/64 loss: 0.07882732152938843
Batch 2/64 loss: 0.09010845422744751
Batch 3/64 loss: 0.11295139789581299
Batch 4/64 loss: 0.08982473611831665
Batch 5/64 loss: 0.08727133274078369
Batch 6/64 loss: 0.08852064609527588
Batch 7/64 loss: 0.08852744102478027
Batch 8/64 loss: 0.07516181468963623
Batch 9/64 loss: 0.08918893337249756
Batch 10/64 loss: 0.08275973796844482
Batch 11/64 loss: 0.08799886703491211
Batch 12/64 loss: 0.09387189149856567
Batch 13/64 loss: 0.08809953927993774
Batch 14/64 loss: 0.09319823980331421
Batch 15/64 loss: 0.10941147804260254
Batch 16/64 loss: 0.08026677370071411
Batch 17/64 loss: 0.062298357486724854
Batch 18/64 loss: 0.09241485595703125
Batch 19/64 loss: 0.07657730579376221
Batch 20/64 loss: 0.10964351892471313
Batch 21/64 loss: 0.09230923652648926
Batch 22/64 loss: 0.08875828981399536
Batch 23/64 loss: 0.07803463935852051
Batch 24/64 loss: 0.06666648387908936
Batch 25/64 loss: 0.12041676044464111
Batch 26/64 loss: 0.07133954763412476
Batch 27/64 loss: 0.10273522138595581
Batch 28/64 loss: 0.12657076120376587
Batch 29/64 loss: 0.08909857273101807
Batch 30/64 loss: 0.09968006610870361
Batch 31/64 loss: 0.08352798223495483
Batch 32/64 loss: 0.10542541742324829
Batch 33/64 loss: 0.10722929239273071
Batch 34/64 loss: 0.11468100547790527
Batch 35/64 loss: 0.09450531005859375
Batch 36/64 loss: 0.1032106876373291
Batch 37/64 loss: 0.07484793663024902
Batch 38/64 loss: 0.07000041007995605
Batch 39/64 loss: 0.10202527046203613
Batch 40/64 loss: 0.11346256732940674
Batch 41/64 loss: 0.1109808087348938
Batch 42/64 loss: 0.09382593631744385
Batch 43/64 loss: 0.07533091306686401
Batch 44/64 loss: 0.09261113405227661
Batch 45/64 loss: 0.11898761987686157
Batch 46/64 loss: 0.09592270851135254
Batch 47/64 loss: 0.09967350959777832
Batch 48/64 loss: 0.09891307353973389
Batch 49/64 loss: 0.07893574237823486
Batch 50/64 loss: 0.10082739591598511
Batch 51/64 loss: 0.08097183704376221
Batch 52/64 loss: 0.09952664375305176
Batch 53/64 loss: 0.09802877902984619
Batch 54/64 loss: 0.1092786192893982
Batch 55/64 loss: 0.11793112754821777
Batch 56/64 loss: 0.0976569652557373
Batch 57/64 loss: 0.10814619064331055
Batch 58/64 loss: 0.12981343269348145
Batch 59/64 loss: 0.09170854091644287
Batch 60/64 loss: 0.07713586091995239
Batch 61/64 loss: 0.08565014600753784
Batch 62/64 loss: 0.1098976731300354
Batch 63/64 loss: 0.09607315063476562
Batch 64/64 loss: 0.06988281011581421
Epoch 143  Train loss: 0.0941441257794698  Val loss: 0.13561552926846795
Epoch 144
-------------------------------
Batch 1/64 loss: 0.12446093559265137
Batch 2/64 loss: 0.10713136196136475
Batch 3/64 loss: 0.09012925624847412
Batch 4/64 loss: 0.06208616495132446
Batch 5/64 loss: 0.101956307888031
Batch 6/64 loss: 0.08695751428604126
Batch 7/64 loss: 0.06445109844207764
Batch 8/64 loss: 0.10003155469894409
Batch 9/64 loss: 0.09208714962005615
Batch 10/64 loss: 0.09851819276809692
Batch 11/64 loss: 0.09515833854675293
Batch 12/64 loss: 0.10877412557601929
Batch 13/64 loss: 0.0811845064163208
Batch 14/64 loss: 0.10358297824859619
Batch 15/64 loss: 0.09568953514099121
Batch 16/64 loss: 0.09425508975982666
Batch 17/64 loss: 0.09237098693847656
Batch 18/64 loss: 0.07637941837310791
Batch 19/64 loss: 0.08068329095840454
Batch 20/64 loss: 0.08208870887756348
Batch 21/64 loss: 0.11004006862640381
Batch 22/64 loss: 0.10512340068817139
Batch 23/64 loss: 0.10781210660934448
Batch 24/64 loss: 0.08348298072814941
Batch 25/64 loss: 0.11066234111785889
Batch 26/64 loss: 0.08495497703552246
Batch 27/64 loss: 0.12954580783843994
Batch 28/64 loss: 0.09871864318847656
Batch 29/64 loss: 0.08785438537597656
Batch 30/64 loss: 0.0869288444519043
Batch 31/64 loss: 0.0961846113204956
Batch 32/64 loss: 0.10212820768356323
Batch 33/64 loss: 0.09661567211151123
Batch 34/64 loss: 0.09574276208877563
Batch 35/64 loss: 0.07399749755859375
Batch 36/64 loss: 0.1061810851097107
Batch 37/64 loss: 0.1045992374420166
Batch 38/64 loss: 0.0796765685081482
Batch 39/64 loss: 0.12460207939147949
Batch 40/64 loss: 0.08741486072540283
Batch 41/64 loss: 0.09675639867782593
Batch 42/64 loss: 0.08169806003570557
Batch 43/64 loss: 0.08990520238876343
Batch 44/64 loss: 0.0760645866394043
Batch 45/64 loss: 0.11221915483474731
Batch 46/64 loss: 0.09356164932250977
Batch 47/64 loss: 0.05510479211807251
Batch 48/64 loss: 0.08788496255874634
Batch 49/64 loss: 0.12179368734359741
Batch 50/64 loss: 0.10376298427581787
Batch 51/64 loss: 0.09242546558380127
Batch 52/64 loss: 0.12629348039627075
Batch 53/64 loss: 0.1026921272277832
Batch 54/64 loss: 0.10284310579299927
Batch 55/64 loss: 0.10247999429702759
Batch 56/64 loss: 0.1062391996383667
Batch 57/64 loss: 0.08007854223251343
Batch 58/64 loss: 0.08003103733062744
Batch 59/64 loss: 0.07991695404052734
Batch 60/64 loss: 0.08281594514846802
Batch 61/64 loss: 0.11153954267501831
Batch 62/64 loss: 0.08446455001831055
Batch 63/64 loss: 0.09084504842758179
Batch 64/64 loss: 0.08261626958847046
Epoch 144  Train loss: 0.09464504040923773  Val loss: 0.1349366215086475
Epoch 145
-------------------------------
Batch 1/64 loss: 0.11542212963104248
Batch 2/64 loss: 0.05799388885498047
Batch 3/64 loss: 0.10004037618637085
Batch 4/64 loss: 0.10120099782943726
Batch 5/64 loss: 0.09718716144561768
Batch 6/64 loss: 0.08968210220336914
Batch 7/64 loss: 0.10134381055831909
Batch 8/64 loss: 0.07658761739730835
Batch 9/64 loss: 0.10325765609741211
Batch 10/64 loss: 0.12363314628601074
Batch 11/64 loss: 0.08827459812164307
Batch 12/64 loss: 0.08477944135665894
Batch 13/64 loss: 0.09070366621017456
Batch 14/64 loss: 0.087615966796875
Batch 15/64 loss: 0.07561284303665161
Batch 16/64 loss: 0.09935951232910156
Batch 17/64 loss: 0.09525829553604126
Batch 18/64 loss: 0.0736624002456665
Batch 19/64 loss: 0.09192848205566406
Batch 20/64 loss: 0.12119185924530029
Batch 21/64 loss: 0.08224308490753174
Batch 22/64 loss: 0.09357631206512451
Batch 23/64 loss: 0.09041190147399902
Batch 24/64 loss: 0.09870922565460205
Batch 25/64 loss: 0.10714197158813477
Batch 26/64 loss: 0.09598273038864136
Batch 27/64 loss: 0.0937037467956543
Batch 28/64 loss: 0.09993726015090942
Batch 29/64 loss: 0.11544734239578247
Batch 30/64 loss: 0.10189133882522583
Batch 31/64 loss: 0.0905829668045044
Batch 32/64 loss: 0.06359076499938965
Batch 33/64 loss: 0.0861254334449768
Batch 34/64 loss: 0.07738029956817627
Batch 35/64 loss: 0.07611417770385742
Batch 36/64 loss: 0.07930552959442139
Batch 37/64 loss: 0.09693527221679688
Batch 38/64 loss: 0.08593922853469849
Batch 39/64 loss: 0.09496229887008667
Batch 40/64 loss: 0.07903754711151123
Batch 41/64 loss: 0.08709067106246948
Batch 42/64 loss: 0.09307819604873657
Batch 43/64 loss: 0.07298368215560913
Batch 44/64 loss: 0.11939072608947754
Batch 45/64 loss: 0.09350937604904175
Batch 46/64 loss: 0.09141355752944946
Batch 47/64 loss: 0.1279117465019226
Batch 48/64 loss: 0.10376167297363281
Batch 49/64 loss: 0.08909708261489868
Batch 50/64 loss: 0.0931348204612732
Batch 51/64 loss: 0.10700333118438721
Batch 52/64 loss: 0.08464372158050537
Batch 53/64 loss: 0.09937703609466553
Batch 54/64 loss: 0.1187293529510498
Batch 55/64 loss: 0.08378732204437256
Batch 56/64 loss: 0.10069483518600464
Batch 57/64 loss: 0.07655024528503418
Batch 58/64 loss: 0.12903928756713867
Batch 59/64 loss: 0.08589375019073486
Batch 60/64 loss: 0.10397493839263916
Batch 61/64 loss: 0.09898883104324341
Batch 62/64 loss: 0.11384910345077515
Batch 63/64 loss: 0.09296023845672607
Batch 64/64 loss: 0.10260248184204102
Epoch 145  Train loss: 0.09455011405196845  Val loss: 0.13590255430883558
Epoch 146
-------------------------------
Batch 1/64 loss: 0.10243898630142212
Batch 2/64 loss: 0.09064239263534546
Batch 3/64 loss: 0.10200822353363037
Batch 4/64 loss: 0.06878244876861572
Batch 5/64 loss: 0.0925215482711792
Batch 6/64 loss: 0.08589744567871094
Batch 7/64 loss: 0.06427472829818726
Batch 8/64 loss: 0.10205930471420288
Batch 9/64 loss: 0.09615087509155273
Batch 10/64 loss: 0.10048532485961914
Batch 11/64 loss: 0.10714799165725708
Batch 12/64 loss: 0.08547735214233398
Batch 13/64 loss: 0.09397858381271362
Batch 14/64 loss: 0.1148729920387268
Batch 15/64 loss: 0.08975964784622192
Batch 16/64 loss: 0.12868618965148926
Batch 17/64 loss: 0.0852174162864685
Batch 18/64 loss: 0.10924869775772095
Batch 19/64 loss: 0.12185311317443848
Batch 20/64 loss: 0.10098183155059814
Batch 21/64 loss: 0.09625226259231567
Batch 22/64 loss: 0.10039412975311279
Batch 23/64 loss: 0.06006729602813721
Batch 24/64 loss: 0.08300906419754028
Batch 25/64 loss: 0.11495411396026611
Batch 26/64 loss: 0.0956881046295166
Batch 27/64 loss: 0.10228943824768066
Batch 28/64 loss: 0.0990561842918396
Batch 29/64 loss: 0.09087997674942017
Batch 30/64 loss: 0.08707213401794434
Batch 31/64 loss: 0.11383968591690063
Batch 32/64 loss: 0.09110456705093384
Batch 33/64 loss: 0.10766524076461792
Batch 34/64 loss: 0.08591198921203613
Batch 35/64 loss: 0.10834378004074097
Batch 36/64 loss: 0.06283235549926758
Batch 37/64 loss: 0.0821487307548523
Batch 38/64 loss: 0.08381646871566772
Batch 39/64 loss: 0.09310257434844971
Batch 40/64 loss: 0.09925442934036255
Batch 41/64 loss: 0.09625113010406494
Batch 42/64 loss: 0.07256567478179932
Batch 43/64 loss: 0.08107161521911621
Batch 44/64 loss: 0.07165467739105225
Batch 45/64 loss: 0.06818956136703491
Batch 46/64 loss: 0.12285280227661133
Batch 47/64 loss: 0.059970080852508545
Batch 48/64 loss: 0.09113442897796631
Batch 49/64 loss: 0.13401174545288086
Batch 50/64 loss: 0.09984570741653442
Batch 51/64 loss: 0.06788766384124756
Batch 52/64 loss: 0.07474958896636963
Batch 53/64 loss: 0.11100482940673828
Batch 54/64 loss: 0.08405715227127075
Batch 55/64 loss: 0.09425294399261475
Batch 56/64 loss: 0.08949649333953857
Batch 57/64 loss: 0.09378516674041748
Batch 58/64 loss: 0.07851618528366089
Batch 59/64 loss: 0.0986713171005249
Batch 60/64 loss: 0.10369861125946045
Batch 61/64 loss: 0.11090743541717529
Batch 62/64 loss: 0.09677022695541382
Batch 63/64 loss: 0.13925963640213013
Batch 64/64 loss: 0.10058242082595825
Epoch 146  Train loss: 0.09437190761753157  Val loss: 0.1398703552193658
Epoch 147
-------------------------------
Batch 1/64 loss: 0.10862952470779419
Batch 2/64 loss: 0.08690708875656128
Batch 3/64 loss: 0.09960228204727173
Batch 4/64 loss: 0.1098405122756958
Batch 5/64 loss: 0.09244763851165771
Batch 6/64 loss: 0.08601760864257812
Batch 7/64 loss: 0.10695946216583252
Batch 8/64 loss: 0.10792332887649536
Batch 9/64 loss: 0.09078288078308105
Batch 10/64 loss: 0.11789560317993164
Batch 11/64 loss: 0.09676593542098999
Batch 12/64 loss: 0.07789051532745361
Batch 13/64 loss: 0.07031846046447754
Batch 14/64 loss: 0.0817221999168396
Batch 15/64 loss: 0.09327840805053711
Batch 16/64 loss: 0.08495515584945679
Batch 17/64 loss: 0.11922383308410645
Batch 18/64 loss: 0.0737917423248291
Batch 19/64 loss: 0.08488774299621582
Batch 20/64 loss: 0.10273998975753784
Batch 21/64 loss: 0.077475905418396
Batch 22/64 loss: 0.0626644492149353
Batch 23/64 loss: 0.09599936008453369
Batch 24/64 loss: 0.09363383054733276
Batch 25/64 loss: 0.11252933740615845
Batch 26/64 loss: 0.06647759675979614
Batch 27/64 loss: 0.10587954521179199
Batch 28/64 loss: 0.08740067481994629
Batch 29/64 loss: 0.10802006721496582
Batch 30/64 loss: 0.07269835472106934
Batch 31/64 loss: 0.08747345209121704
Batch 32/64 loss: 0.11724162101745605
Batch 33/64 loss: 0.09968936443328857
Batch 34/64 loss: 0.13420075178146362
Batch 35/64 loss: 0.06391400098800659
Batch 36/64 loss: 0.12110781669616699
Batch 37/64 loss: 0.09520024061203003
Batch 38/64 loss: 0.07530444860458374
Batch 39/64 loss: 0.09227144718170166
Batch 40/64 loss: 0.07106578350067139
Batch 41/64 loss: 0.09131366014480591
Batch 42/64 loss: 0.07194119691848755
Batch 43/64 loss: 0.08335167169570923
Batch 44/64 loss: 0.0860021710395813
Batch 45/64 loss: 0.10266768932342529
Batch 46/64 loss: 0.0733182430267334
Batch 47/64 loss: 0.09816950559616089
Batch 48/64 loss: 0.09406661987304688
Batch 49/64 loss: 0.10741162300109863
Batch 50/64 loss: 0.10611897706985474
Batch 51/64 loss: 0.10012561082839966
Batch 52/64 loss: 0.10957801342010498
Batch 53/64 loss: 0.10244196653366089
Batch 54/64 loss: 0.10480368137359619
Batch 55/64 loss: 0.09220337867736816
Batch 56/64 loss: 0.09601086378097534
Batch 57/64 loss: 0.10843026638031006
Batch 58/64 loss: 0.10713297128677368
Batch 59/64 loss: 0.09021997451782227
Batch 60/64 loss: 0.10602408647537231
Batch 61/64 loss: 0.08027613162994385
Batch 62/64 loss: 0.09461897611618042
Batch 63/64 loss: 0.10035043954849243
Batch 64/64 loss: 0.09279191493988037
Epoch 147  Train loss: 0.09425881750443403  Val loss: 0.13368654394477503
Epoch 148
-------------------------------
Batch 1/64 loss: 0.11126542091369629
Batch 2/64 loss: 0.1045946478843689
Batch 3/64 loss: 0.10633611679077148
Batch 4/64 loss: 0.08798927068710327
Batch 5/64 loss: 0.09244656562805176
Batch 6/64 loss: 0.12904596328735352
Batch 7/64 loss: 0.10763120651245117
Batch 8/64 loss: 0.08657842874526978
Batch 9/64 loss: 0.08595085144042969
Batch 10/64 loss: 0.07510244846343994
Batch 11/64 loss: 0.09780174493789673
Batch 12/64 loss: 0.053167641162872314
Batch 13/64 loss: 0.08588647842407227
Batch 14/64 loss: 0.08864778280258179
Batch 15/64 loss: 0.08326482772827148
Batch 16/64 loss: 0.09444183111190796
Batch 17/64 loss: 0.08932781219482422
Batch 18/64 loss: 0.09308314323425293
Batch 19/64 loss: 0.11024653911590576
Batch 20/64 loss: 0.07660949230194092
Batch 21/64 loss: 0.0964931845664978
Batch 22/64 loss: 0.08510661125183105
Batch 23/64 loss: 0.10169875621795654
Batch 24/64 loss: 0.07612800598144531
Batch 25/64 loss: 0.09171926975250244
Batch 26/64 loss: 0.09935176372528076
Batch 27/64 loss: 0.08700603246688843
Batch 28/64 loss: 0.11003124713897705
Batch 29/64 loss: 0.07971340417861938
Batch 30/64 loss: 0.11496347188949585
Batch 31/64 loss: 0.0889359712600708
Batch 32/64 loss: 0.06411081552505493
Batch 33/64 loss: 0.11049085855484009
Batch 34/64 loss: 0.09413909912109375
Batch 35/64 loss: 0.09529411792755127
Batch 36/64 loss: 0.10972565412521362
Batch 37/64 loss: 0.10871762037277222
Batch 38/64 loss: 0.07910013198852539
Batch 39/64 loss: 0.08184826374053955
Batch 40/64 loss: 0.07146996259689331
Batch 41/64 loss: 0.08880293369293213
Batch 42/64 loss: 0.11773693561553955
Batch 43/64 loss: 0.08983516693115234
Batch 44/64 loss: 0.08855074644088745
Batch 45/64 loss: 0.07224011421203613
Batch 46/64 loss: 0.09634912014007568
Batch 47/64 loss: 0.0720973014831543
Batch 48/64 loss: 0.08588474988937378
Batch 49/64 loss: 0.08793169260025024
Batch 50/64 loss: 0.08719396591186523
Batch 51/64 loss: 0.09860920906066895
Batch 52/64 loss: 0.0858680009841919
Batch 53/64 loss: 0.10207986831665039
Batch 54/64 loss: 0.1291455626487732
Batch 55/64 loss: 0.10331583023071289
Batch 56/64 loss: 0.12490987777709961
Batch 57/64 loss: 0.12694156169891357
Batch 58/64 loss: 0.10464555025100708
Batch 59/64 loss: 0.09908008575439453
Batch 60/64 loss: 0.09284943342208862
Batch 61/64 loss: 0.08130800724029541
Batch 62/64 loss: 0.06951165199279785
Batch 63/64 loss: 0.05973571538925171
Batch 64/64 loss: 0.08526831865310669
Epoch 148  Train loss: 0.09308292982625026  Val loss: 0.1325923843482106
Saving best model, epoch: 148
Epoch 149
-------------------------------
Batch 1/64 loss: 0.1000145673751831
Batch 2/64 loss: 0.08778071403503418
Batch 3/64 loss: 0.08081668615341187
Batch 4/64 loss: 0.07308167219161987
Batch 5/64 loss: 0.061937928199768066
Batch 6/64 loss: 0.08737236261367798
Batch 7/64 loss: 0.09391629695892334
Batch 8/64 loss: 0.09089219570159912
Batch 9/64 loss: 0.10311704874038696
Batch 10/64 loss: 0.10548031330108643
Batch 11/64 loss: 0.09433841705322266
Batch 12/64 loss: 0.0722808837890625
Batch 13/64 loss: 0.10736644268035889
Batch 14/64 loss: 0.09699767827987671
Batch 15/64 loss: 0.0944209098815918
Batch 16/64 loss: 0.07025730609893799
Batch 17/64 loss: 0.12364858388900757
Batch 18/64 loss: 0.08788037300109863
Batch 19/64 loss: 0.09318238496780396
Batch 20/64 loss: 0.08816802501678467
Batch 21/64 loss: 0.08146798610687256
Batch 22/64 loss: 0.08665430545806885
Batch 23/64 loss: 0.09169930219650269
Batch 24/64 loss: 0.10165548324584961
Batch 25/64 loss: 0.101199209690094
Batch 26/64 loss: 0.10080546140670776
Batch 27/64 loss: 0.08509129285812378
Batch 28/64 loss: 0.08288472890853882
Batch 29/64 loss: 0.09646773338317871
Batch 30/64 loss: 0.09087413549423218
Batch 31/64 loss: 0.09785079956054688
Batch 32/64 loss: 0.08274698257446289
Batch 33/64 loss: 0.08550411462783813
Batch 34/64 loss: 0.09467238187789917
Batch 35/64 loss: 0.09371447563171387
Batch 36/64 loss: 0.11021476984024048
Batch 37/64 loss: 0.11117923259735107
Batch 38/64 loss: 0.10034608840942383
Batch 39/64 loss: 0.07969826459884644
Batch 40/64 loss: 0.07792806625366211
Batch 41/64 loss: 0.11726468801498413
Batch 42/64 loss: 0.09527456760406494
Batch 43/64 loss: 0.10152900218963623
Batch 44/64 loss: 0.08550238609313965
Batch 45/64 loss: 0.08745479583740234
Batch 46/64 loss: 0.07697486877441406
Batch 47/64 loss: 0.13154476881027222
Batch 48/64 loss: 0.09485727548599243
Batch 49/64 loss: 0.09481585025787354
Batch 50/64 loss: 0.1143883466720581
Batch 51/64 loss: 0.09223401546478271
Batch 52/64 loss: 0.07273751497268677
Batch 53/64 loss: 0.09771764278411865
Batch 54/64 loss: 0.11019563674926758
Batch 55/64 loss: 0.08303821086883545
Batch 56/64 loss: 0.07756716012954712
Batch 57/64 loss: 0.11310875415802002
Batch 58/64 loss: 0.08327817916870117
Batch 59/64 loss: 0.10343188047409058
Batch 60/64 loss: 0.08514547348022461
Batch 61/64 loss: 0.08911460638046265
Batch 62/64 loss: 0.0967336893081665
Batch 63/64 loss: 0.08464854955673218
Batch 64/64 loss: 0.1112326979637146
Epoch 149  Train loss: 0.09313863515853882  Val loss: 0.13237902388949574
Saving best model, epoch: 149
Epoch 150
-------------------------------
Batch 1/64 loss: 0.0981673002243042
Batch 2/64 loss: 0.0833282470703125
Batch 3/64 loss: 0.1077733039855957
Batch 4/64 loss: 0.09027087688446045
Batch 5/64 loss: 0.07442456483840942
Batch 6/64 loss: 0.09329503774642944
Batch 7/64 loss: 0.09576505422592163
Batch 8/64 loss: 0.09185278415679932
Batch 9/64 loss: 0.07915610074996948
Batch 10/64 loss: 0.10753864049911499
Batch 11/64 loss: 0.10096848011016846
Batch 12/64 loss: 0.07523089647293091
Batch 13/64 loss: 0.11537820100784302
Batch 14/64 loss: 0.09212571382522583
Batch 15/64 loss: 0.0885845422744751
Batch 16/64 loss: 0.10877078771591187
Batch 17/64 loss: 0.07724142074584961
Batch 18/64 loss: 0.09834945201873779
Batch 19/64 loss: 0.09332185983657837
Batch 20/64 loss: 0.07910895347595215
Batch 21/64 loss: 0.07037043571472168
Batch 22/64 loss: 0.11373639106750488
Batch 23/64 loss: 0.07183206081390381
Batch 24/64 loss: 0.11558008193969727
Batch 25/64 loss: 0.0891491174697876
Batch 26/64 loss: 0.06987935304641724
Batch 27/64 loss: 0.09885621070861816
Batch 28/64 loss: 0.08918321132659912
Batch 29/64 loss: 0.0967797040939331
Batch 30/64 loss: 0.10547792911529541
Batch 31/64 loss: 0.08477294445037842
Batch 32/64 loss: 0.10571932792663574
Batch 33/64 loss: 0.0994994044303894
Batch 34/64 loss: 0.09372466802597046
Batch 35/64 loss: 0.12427711486816406
Batch 36/64 loss: 0.08311349153518677
Batch 37/64 loss: 0.10115152597427368
Batch 38/64 loss: 0.09332543611526489
Batch 39/64 loss: 0.06728291511535645
Batch 40/64 loss: 0.09588009119033813
Batch 41/64 loss: 0.07512712478637695
Batch 42/64 loss: 0.07880455255508423
Batch 43/64 loss: 0.08349937200546265
Batch 44/64 loss: 0.08392071723937988
Batch 45/64 loss: 0.07615399360656738
Batch 46/64 loss: 0.11653101444244385
Batch 47/64 loss: 0.07714438438415527
Batch 48/64 loss: 0.09252715110778809
Batch 49/64 loss: 0.08287817239761353
Batch 50/64 loss: 0.09776020050048828
Batch 51/64 loss: 0.08923506736755371
Batch 52/64 loss: 0.10692781209945679
Batch 53/64 loss: 0.08530318737030029
Batch 54/64 loss: 0.07859057188034058
Batch 55/64 loss: 0.102572500705719
Batch 56/64 loss: 0.07023352384567261
Batch 57/64 loss: 0.09083718061447144
Batch 58/64 loss: 0.07422274351119995
Batch 59/64 loss: 0.10505104064941406
Batch 60/64 loss: 0.07745671272277832
Batch 61/64 loss: 0.09647232294082642
Batch 62/64 loss: 0.09856921434402466
Batch 63/64 loss: 0.10741430521011353
Batch 64/64 loss: 0.07665443420410156
Epoch 150  Train loss: 0.09137203646641151  Val loss: 0.13321617931844443
Epoch 151
-------------------------------
Batch 1/64 loss: 0.0859023928642273
Batch 2/64 loss: 0.11379170417785645
Batch 3/64 loss: 0.09266698360443115
Batch 4/64 loss: 0.09579449892044067
Batch 5/64 loss: 0.09278738498687744
Batch 6/64 loss: 0.09761935472488403
Batch 7/64 loss: 0.0780022144317627
Batch 8/64 loss: 0.13649022579193115
Batch 9/64 loss: 0.05629056692123413
Batch 10/64 loss: 0.08535301685333252
Batch 11/64 loss: 0.09207284450531006
Batch 12/64 loss: 0.07961487770080566
Batch 13/64 loss: 0.0690927505493164
Batch 14/64 loss: 0.08038640022277832
Batch 15/64 loss: 0.081326425075531
Batch 16/64 loss: 0.10607665777206421
Batch 17/64 loss: 0.0778512954711914
Batch 18/64 loss: 0.09391462802886963
Batch 19/64 loss: 0.0970715880393982
Batch 20/64 loss: 0.08863919973373413
Batch 21/64 loss: 0.10269200801849365
Batch 22/64 loss: 0.12568384408950806
Batch 23/64 loss: 0.08589297533035278
Batch 24/64 loss: 0.08046537637710571
Batch 25/64 loss: 0.09309804439544678
Batch 26/64 loss: 0.1165500283241272
Batch 27/64 loss: 0.1075754165649414
Batch 28/64 loss: 0.08121597766876221
Batch 29/64 loss: 0.10055547952651978
Batch 30/64 loss: 0.07543236017227173
Batch 31/64 loss: 0.09886223077774048
Batch 32/64 loss: 0.07438808679580688
Batch 33/64 loss: 0.07838541269302368
Batch 34/64 loss: 0.08359181880950928
Batch 35/64 loss: 0.09944498538970947
Batch 36/64 loss: 0.08183473348617554
Batch 37/64 loss: 0.10459905862808228
Batch 38/64 loss: 0.0966607928276062
Batch 39/64 loss: 0.09196054935455322
Batch 40/64 loss: 0.08173167705535889
Batch 41/64 loss: 0.08595925569534302
Batch 42/64 loss: 0.11141222715377808
Batch 43/64 loss: 0.08443588018417358
Batch 44/64 loss: 0.07951849699020386
Batch 45/64 loss: 0.07056218385696411
Batch 46/64 loss: 0.12643766403198242
Batch 47/64 loss: 0.116061270236969
Batch 48/64 loss: 0.08873450756072998
Batch 49/64 loss: 0.08109092712402344
Batch 50/64 loss: 0.06369554996490479
Batch 51/64 loss: 0.09218180179595947
Batch 52/64 loss: 0.07971346378326416
Batch 53/64 loss: 0.09144449234008789
Batch 54/64 loss: 0.0796160101890564
Batch 55/64 loss: 0.06990015506744385
Batch 56/64 loss: 0.0971953272819519
Batch 57/64 loss: 0.11212575435638428
Batch 58/64 loss: 0.05241405963897705
Batch 59/64 loss: 0.09457951784133911
Batch 60/64 loss: 0.06864750385284424
Batch 61/64 loss: 0.11447793245315552
Batch 62/64 loss: 0.10224103927612305
Batch 63/64 loss: 0.09488701820373535
Batch 64/64 loss: 0.09859377145767212
Epoch 151  Train loss: 0.09086499190797993  Val loss: 0.13114660261422909
Saving best model, epoch: 151
Epoch 152
-------------------------------
Batch 1/64 loss: 0.07567501068115234
Batch 2/64 loss: 0.06862354278564453
Batch 3/64 loss: 0.09535729885101318
Batch 4/64 loss: 0.07823067903518677
Batch 5/64 loss: 0.06350094079971313
Batch 6/64 loss: 0.08552420139312744
Batch 7/64 loss: 0.06839483976364136
Batch 8/64 loss: 0.08662456274032593
Batch 9/64 loss: 0.08653980493545532
Batch 10/64 loss: 0.10119122266769409
Batch 11/64 loss: 0.11567157506942749
Batch 12/64 loss: 0.10702073574066162
Batch 13/64 loss: 0.0843619704246521
Batch 14/64 loss: 0.10424548387527466
Batch 15/64 loss: 0.08084028959274292
Batch 16/64 loss: 0.0990564227104187
Batch 17/64 loss: 0.09938371181488037
Batch 18/64 loss: 0.1336510181427002
Batch 19/64 loss: 0.108997642993927
Batch 20/64 loss: 0.08820199966430664
Batch 21/64 loss: 0.08544576168060303
Batch 22/64 loss: 0.09322518110275269
Batch 23/64 loss: 0.09623658657073975
Batch 24/64 loss: 0.09422844648361206
Batch 25/64 loss: 0.09794825315475464
Batch 26/64 loss: 0.08021277189254761
Batch 27/64 loss: 0.08075511455535889
Batch 28/64 loss: 0.10636651515960693
Batch 29/64 loss: 0.09614026546478271
Batch 30/64 loss: 0.11278235912322998
Batch 31/64 loss: 0.0718277096748352
Batch 32/64 loss: 0.06164616346359253
Batch 33/64 loss: 0.07712799310684204
Batch 34/64 loss: 0.0913928747177124
Batch 35/64 loss: 0.0865335464477539
Batch 36/64 loss: 0.10097813606262207
Batch 37/64 loss: 0.09702861309051514
Batch 38/64 loss: 0.08588850498199463
Batch 39/64 loss: 0.11637872457504272
Batch 40/64 loss: 0.10746657848358154
Batch 41/64 loss: 0.06768321990966797
Batch 42/64 loss: 0.0756421685218811
Batch 43/64 loss: 0.09217000007629395
Batch 44/64 loss: 0.09173178672790527
Batch 45/64 loss: 0.09991335868835449
Batch 46/64 loss: 0.0832815170288086
Batch 47/64 loss: 0.08889192342758179
Batch 48/64 loss: 0.0856735110282898
Batch 49/64 loss: 0.07981765270233154
Batch 50/64 loss: 0.09790259599685669
Batch 51/64 loss: 0.09664130210876465
Batch 52/64 loss: 0.09053879976272583
Batch 53/64 loss: 0.07487571239471436
Batch 54/64 loss: 0.06696414947509766
Batch 55/64 loss: 0.09825408458709717
Batch 56/64 loss: 0.10282719135284424
Batch 57/64 loss: 0.10225021839141846
Batch 58/64 loss: 0.10674059391021729
Batch 59/64 loss: 0.06882774829864502
Batch 60/64 loss: 0.11021089553833008
Batch 61/64 loss: 0.0915457010269165
Batch 62/64 loss: 0.07718878984451294
Batch 63/64 loss: 0.08008092641830444
Batch 64/64 loss: 0.101695716381073
Epoch 152  Train loss: 0.09061378334082809  Val loss: 0.13216412661411509
Epoch 153
-------------------------------
Batch 1/64 loss: 0.06554388999938965
Batch 2/64 loss: 0.06761115789413452
Batch 3/64 loss: 0.06815838813781738
Batch 4/64 loss: 0.07245755195617676
Batch 5/64 loss: 0.11010950803756714
Batch 6/64 loss: 0.07666587829589844
Batch 7/64 loss: 0.06983011960983276
Batch 8/64 loss: 0.10654455423355103
Batch 9/64 loss: 0.11124324798583984
Batch 10/64 loss: 0.07621937990188599
Batch 11/64 loss: 0.07584565877914429
Batch 12/64 loss: 0.07738518714904785
Batch 13/64 loss: 0.07145035266876221
Batch 14/64 loss: 0.09544360637664795
Batch 15/64 loss: 0.11377257108688354
Batch 16/64 loss: 0.08768707513809204
Batch 17/64 loss: 0.09692233800888062
Batch 18/64 loss: 0.11166024208068848
Batch 19/64 loss: 0.08224451541900635
Batch 20/64 loss: 0.08729547262191772
Batch 21/64 loss: 0.09416699409484863
Batch 22/64 loss: 0.08965426683425903
Batch 23/64 loss: 0.07664430141448975
Batch 24/64 loss: 0.09552747011184692
Batch 25/64 loss: 0.12691223621368408
Batch 26/64 loss: 0.08861249685287476
Batch 27/64 loss: 0.08249717950820923
Batch 28/64 loss: 0.0801553726196289
Batch 29/64 loss: 0.0627942681312561
Batch 30/64 loss: 0.0687941312789917
Batch 31/64 loss: 0.10443222522735596
Batch 32/64 loss: 0.07499289512634277
Batch 33/64 loss: 0.06781023740768433
Batch 34/64 loss: 0.06817042827606201
Batch 35/64 loss: 0.09915828704833984
Batch 36/64 loss: 0.08162641525268555
Batch 37/64 loss: 0.10123085975646973
Batch 38/64 loss: 0.09878277778625488
Batch 39/64 loss: 0.09238606691360474
Batch 40/64 loss: 0.10541188716888428
Batch 41/64 loss: 0.08351898193359375
Batch 42/64 loss: 0.12623393535614014
Batch 43/64 loss: 0.06979036331176758
Batch 44/64 loss: 0.11286944150924683
Batch 45/64 loss: 0.09445309638977051
Batch 46/64 loss: 0.09352695941925049
Batch 47/64 loss: 0.09834951162338257
Batch 48/64 loss: 0.10135698318481445
Batch 49/64 loss: 0.11695241928100586
Batch 50/64 loss: 0.09248262643814087
Batch 51/64 loss: 0.09670239686965942
Batch 52/64 loss: 0.10309791564941406
Batch 53/64 loss: 0.08018720149993896
Batch 54/64 loss: 0.07842642068862915
Batch 55/64 loss: 0.10209918022155762
Batch 56/64 loss: 0.09768593311309814
Batch 57/64 loss: 0.07435482740402222
Batch 58/64 loss: 0.06842511892318726
Batch 59/64 loss: 0.08602821826934814
Batch 60/64 loss: 0.10043597221374512
Batch 61/64 loss: 0.0910559892654419
Batch 62/64 loss: 0.09985238313674927
Batch 63/64 loss: 0.12433463335037231
Batch 64/64 loss: 0.09208106994628906
Epoch 153  Train loss: 0.09011973025752049  Val loss: 0.13655351937021995
Epoch 154
-------------------------------
Batch 1/64 loss: 0.08250415325164795
Batch 2/64 loss: 0.09045660495758057
Batch 3/64 loss: 0.08551079034805298
Batch 4/64 loss: 0.09136742353439331
Batch 5/64 loss: 0.09722709655761719
Batch 6/64 loss: 0.08759951591491699
Batch 7/64 loss: 0.09319102764129639
Batch 8/64 loss: 0.09195894002914429
Batch 9/64 loss: 0.08473151922225952
Batch 10/64 loss: 0.09224426746368408
Batch 11/64 loss: 0.07420456409454346
Batch 12/64 loss: 0.10960173606872559
Batch 13/64 loss: 0.08616936206817627
Batch 14/64 loss: 0.09018588066101074
Batch 15/64 loss: 0.08717435598373413
Batch 16/64 loss: 0.07749104499816895
Batch 17/64 loss: 0.08735519647598267
Batch 18/64 loss: 0.08610248565673828
Batch 19/64 loss: 0.09923422336578369
Batch 20/64 loss: 0.09976845979690552
Batch 21/64 loss: 0.09503334760665894
Batch 22/64 loss: 0.10374641418457031
Batch 23/64 loss: 0.07905948162078857
Batch 24/64 loss: 0.08229506015777588
Batch 25/64 loss: 0.09227734804153442
Batch 26/64 loss: 0.0851483941078186
Batch 27/64 loss: 0.07529467344284058
Batch 28/64 loss: 0.1068955659866333
Batch 29/64 loss: 0.08841550350189209
Batch 30/64 loss: 0.09585171937942505
Batch 31/64 loss: 0.078704833984375
Batch 32/64 loss: 0.1007920503616333
Batch 33/64 loss: 0.06303757429122925
Batch 34/64 loss: 0.11437070369720459
Batch 35/64 loss: 0.11418157815933228
Batch 36/64 loss: 0.10103893280029297
Batch 37/64 loss: 0.080372154712677
Batch 38/64 loss: 0.08371955156326294
Batch 39/64 loss: 0.10298961400985718
Batch 40/64 loss: 0.09998965263366699
Batch 41/64 loss: 0.06325268745422363
Batch 42/64 loss: 0.0909888744354248
Batch 43/64 loss: 0.06891202926635742
Batch 44/64 loss: 0.07867598533630371
Batch 45/64 loss: 0.08765602111816406
Batch 46/64 loss: 0.07903814315795898
Batch 47/64 loss: 0.07535523176193237
Batch 48/64 loss: 0.12534940242767334
Batch 49/64 loss: 0.09813398122787476
Batch 50/64 loss: 0.13098150491714478
Batch 51/64 loss: 0.12594008445739746
Batch 52/64 loss: 0.09623157978057861
Batch 53/64 loss: 0.07872539758682251
Batch 54/64 loss: 0.11990737915039062
Batch 55/64 loss: 0.07644057273864746
Batch 56/64 loss: 0.09984326362609863
Batch 57/64 loss: 0.0762525200843811
Batch 58/64 loss: 0.08880108594894409
Batch 59/64 loss: 0.10500329732894897
Batch 60/64 loss: 0.10826420783996582
Batch 61/64 loss: 0.07620728015899658
Batch 62/64 loss: 0.06782412528991699
Batch 63/64 loss: 0.07683002948760986
Batch 64/64 loss: 0.0973864197731018
Epoch 154  Train loss: 0.09105799689012416  Val loss: 0.13742814952974877
Epoch 155
-------------------------------
Batch 1/64 loss: 0.09230101108551025
Batch 2/64 loss: 0.0674009919166565
Batch 3/64 loss: 0.09450072050094604
Batch 4/64 loss: 0.07093876600265503
Batch 5/64 loss: 0.08715718984603882
Batch 6/64 loss: 0.08110558986663818
Batch 7/64 loss: 0.08108252286911011
Batch 8/64 loss: 0.07740366458892822
Batch 9/64 loss: 0.0872083306312561
Batch 10/64 loss: 0.08938843011856079
Batch 11/64 loss: 0.08542633056640625
Batch 12/64 loss: 0.09326171875
Batch 13/64 loss: 0.11235088109970093
Batch 14/64 loss: 0.0832023024559021
Batch 15/64 loss: 0.08202880620956421
Batch 16/64 loss: 0.10079008340835571
Batch 17/64 loss: 0.09624552726745605
Batch 18/64 loss: 0.09212601184844971
Batch 19/64 loss: 0.07719546556472778
Batch 20/64 loss: 0.07661551237106323
Batch 21/64 loss: 0.09213292598724365
Batch 22/64 loss: 0.07225733995437622
Batch 23/64 loss: 0.1147872805595398
Batch 24/64 loss: 0.08906185626983643
Batch 25/64 loss: 0.07183808088302612
Batch 26/64 loss: 0.06260174512863159
Batch 27/64 loss: 0.10610544681549072
Batch 28/64 loss: 0.09420585632324219
Batch 29/64 loss: 0.08926594257354736
Batch 30/64 loss: 0.07581835985183716
Batch 31/64 loss: 0.09199404716491699
Batch 32/64 loss: 0.12158775329589844
Batch 33/64 loss: 0.0977325439453125
Batch 34/64 loss: 0.09411287307739258
Batch 35/64 loss: 0.07664269208908081
Batch 36/64 loss: 0.07740449905395508
Batch 37/64 loss: 0.08657526969909668
Batch 38/64 loss: 0.08663463592529297
Batch 39/64 loss: 0.08594828844070435
Batch 40/64 loss: 0.11096042394638062
Batch 41/64 loss: 0.08065545558929443
Batch 42/64 loss: 0.11833620071411133
Batch 43/64 loss: 0.0990668535232544
Batch 44/64 loss: 0.12898552417755127
Batch 45/64 loss: 0.09917616844177246
Batch 46/64 loss: 0.0852365493774414
Batch 47/64 loss: 0.07864159345626831
Batch 48/64 loss: 0.07723581790924072
Batch 49/64 loss: 0.12264186143875122
Batch 50/64 loss: 0.0854460597038269
Batch 51/64 loss: 0.08800190687179565
Batch 52/64 loss: 0.07500886917114258
Batch 53/64 loss: 0.10154002904891968
Batch 54/64 loss: 0.08193331956863403
Batch 55/64 loss: 0.06806212663650513
Batch 56/64 loss: 0.06629037857055664
Batch 57/64 loss: 0.058104634284973145
Batch 58/64 loss: 0.06657677888870239
Batch 59/64 loss: 0.11480194330215454
Batch 60/64 loss: 0.08839905261993408
Batch 61/64 loss: 0.09699565172195435
Batch 62/64 loss: 0.10541439056396484
Batch 63/64 loss: 0.09403848648071289
Batch 64/64 loss: 0.12169694900512695
Epoch 155  Train loss: 0.08936878559636134  Val loss: 0.13638932414071256
Epoch 156
-------------------------------
Batch 1/64 loss: 0.074562668800354
Batch 2/64 loss: 0.11674225330352783
Batch 3/64 loss: 0.09851384162902832
Batch 4/64 loss: 0.07541561126708984
Batch 5/64 loss: 0.07545512914657593
Batch 6/64 loss: 0.0737459659576416
Batch 7/64 loss: 0.07912492752075195
Batch 8/64 loss: 0.0831296443939209
Batch 9/64 loss: 0.07812368869781494
Batch 10/64 loss: 0.06321990489959717
Batch 11/64 loss: 0.08596640825271606
Batch 12/64 loss: 0.0943230390548706
Batch 13/64 loss: 0.09514212608337402
Batch 14/64 loss: 0.053373873233795166
Batch 15/64 loss: 0.07709217071533203
Batch 16/64 loss: 0.09146726131439209
Batch 17/64 loss: 0.07136458158493042
Batch 18/64 loss: 0.09494102001190186
Batch 19/64 loss: 0.11667412519454956
Batch 20/64 loss: 0.08014261722564697
Batch 21/64 loss: 0.07790225744247437
Batch 22/64 loss: 0.08385908603668213
Batch 23/64 loss: 0.07803231477737427
Batch 24/64 loss: 0.08055680990219116
Batch 25/64 loss: 0.0999070405960083
Batch 26/64 loss: 0.11161178350448608
Batch 27/64 loss: 0.0865321159362793
Batch 28/64 loss: 0.10964334011077881
Batch 29/64 loss: 0.08620667457580566
Batch 30/64 loss: 0.10738593339920044
Batch 31/64 loss: 0.10012716054916382
Batch 32/64 loss: 0.12106812000274658
Batch 33/64 loss: 0.11311805248260498
Batch 34/64 loss: 0.1049843430519104
Batch 35/64 loss: 0.09231233596801758
Batch 36/64 loss: 0.08369308710098267
Batch 37/64 loss: 0.09887707233428955
Batch 38/64 loss: 0.08284097909927368
Batch 39/64 loss: 0.10300225019454956
Batch 40/64 loss: 0.09442377090454102
Batch 41/64 loss: 0.1043701171875
Batch 42/64 loss: 0.08428478240966797
Batch 43/64 loss: 0.06876242160797119
Batch 44/64 loss: 0.10682225227355957
Batch 45/64 loss: 0.11877751350402832
Batch 46/64 loss: 0.09852290153503418
Batch 47/64 loss: 0.09203118085861206
Batch 48/64 loss: 0.06051838397979736
Batch 49/64 loss: 0.08580148220062256
Batch 50/64 loss: 0.09163552522659302
Batch 51/64 loss: 0.07848101854324341
Batch 52/64 loss: 0.07660144567489624
Batch 53/64 loss: 0.07610827684402466
Batch 54/64 loss: 0.09512770175933838
Batch 55/64 loss: 0.1051797866821289
Batch 56/64 loss: 0.10298234224319458
Batch 57/64 loss: 0.0731954574584961
Batch 58/64 loss: 0.11242729425430298
Batch 59/64 loss: 0.08868134021759033
Batch 60/64 loss: 0.06771111488342285
Batch 61/64 loss: 0.0897817611694336
Batch 62/64 loss: 0.09494197368621826
Batch 63/64 loss: 0.09407228231430054
Batch 64/64 loss: 0.09045541286468506
Epoch 156  Train loss: 0.0898708282732496  Val loss: 0.13287589128074778
Epoch 157
-------------------------------
Batch 1/64 loss: 0.06638085842132568
Batch 2/64 loss: 0.0998687744140625
Batch 3/64 loss: 0.0726822018623352
Batch 4/64 loss: 0.09716087579727173
Batch 5/64 loss: 0.09943801164627075
Batch 6/64 loss: 0.0899057388305664
Batch 7/64 loss: 0.049949824810028076
Batch 8/64 loss: 0.10041481256484985
Batch 9/64 loss: 0.09338515996932983
Batch 10/64 loss: 0.07272446155548096
Batch 11/64 loss: 0.07249659299850464
Batch 12/64 loss: 0.07437217235565186
Batch 13/64 loss: 0.08631610870361328
Batch 14/64 loss: 0.09026962518692017
Batch 15/64 loss: 0.08188706636428833
Batch 16/64 loss: 0.08648097515106201
Batch 17/64 loss: 0.08832001686096191
Batch 18/64 loss: 0.09124338626861572
Batch 19/64 loss: 0.11406779289245605
Batch 20/64 loss: 0.07695591449737549
Batch 21/64 loss: 0.12118583917617798
Batch 22/64 loss: 0.08683043718338013
Batch 23/64 loss: 0.11953943967819214
Batch 24/64 loss: 0.10148966312408447
Batch 25/64 loss: 0.08378815650939941
Batch 26/64 loss: 0.08740478754043579
Batch 27/64 loss: 0.06982994079589844
Batch 28/64 loss: 0.08385390043258667
Batch 29/64 loss: 0.07401478290557861
Batch 30/64 loss: 0.09825265407562256
Batch 31/64 loss: 0.07901674509048462
Batch 32/64 loss: 0.055875301361083984
Batch 33/64 loss: 0.09106177091598511
Batch 34/64 loss: 0.12266373634338379
Batch 35/64 loss: 0.11163288354873657
Batch 36/64 loss: 0.11632430553436279
Batch 37/64 loss: 0.08826339244842529
Batch 38/64 loss: 0.10803723335266113
Batch 39/64 loss: 0.09516310691833496
Batch 40/64 loss: 0.10875654220581055
Batch 41/64 loss: 0.08336865901947021
Batch 42/64 loss: 0.08132785558700562
Batch 43/64 loss: 0.09294873476028442
Batch 44/64 loss: 0.08850371837615967
Batch 45/64 loss: 0.10232985019683838
Batch 46/64 loss: 0.09566819667816162
Batch 47/64 loss: 0.09732788801193237
Batch 48/64 loss: 0.08016479015350342
Batch 49/64 loss: 0.08143573999404907
Batch 50/64 loss: 0.07893037796020508
Batch 51/64 loss: 0.11045390367507935
Batch 52/64 loss: 0.08465045690536499
Batch 53/64 loss: 0.06583648920059204
Batch 54/64 loss: 0.09102582931518555
Batch 55/64 loss: 0.07085204124450684
Batch 56/64 loss: 0.12047892808914185
Batch 57/64 loss: 0.07217562198638916
Batch 58/64 loss: 0.09659314155578613
Batch 59/64 loss: 0.09756624698638916
Batch 60/64 loss: 0.07361388206481934
Batch 61/64 loss: 0.08385014533996582
Batch 62/64 loss: 0.10073697566986084
Batch 63/64 loss: 0.09715384244918823
Batch 64/64 loss: 0.06857383251190186
Epoch 157  Train loss: 0.0895016263513004  Val loss: 0.13424263909919976
Epoch 158
-------------------------------
Batch 1/64 loss: 0.09584265947341919
Batch 2/64 loss: 0.06900429725646973
Batch 3/64 loss: 0.08740746974945068
Batch 4/64 loss: 0.11634987592697144
Batch 5/64 loss: 0.10858595371246338
Batch 6/64 loss: 0.08088791370391846
Batch 7/64 loss: 0.1004895567893982
Batch 8/64 loss: 0.060069143772125244
Batch 9/64 loss: 0.11503344774246216
Batch 10/64 loss: 0.07038283348083496
Batch 11/64 loss: 0.08089756965637207
Batch 12/64 loss: 0.06374871730804443
Batch 13/64 loss: 0.07526940107345581
Batch 14/64 loss: 0.06505250930786133
Batch 15/64 loss: 0.07585513591766357
Batch 16/64 loss: 0.09422355890274048
Batch 17/64 loss: 0.11920809745788574
Batch 18/64 loss: 0.07544809579849243
Batch 19/64 loss: 0.09841382503509521
Batch 20/64 loss: 0.08058440685272217
Batch 21/64 loss: 0.07761132717132568
Batch 22/64 loss: 0.09631145000457764
Batch 23/64 loss: 0.09247881174087524
Batch 24/64 loss: 0.06866800785064697
Batch 25/64 loss: 0.07772433757781982
Batch 26/64 loss: 0.10823547840118408
Batch 27/64 loss: 0.1002470850944519
Batch 28/64 loss: 0.07369500398635864
Batch 29/64 loss: 0.08187615871429443
Batch 30/64 loss: 0.08734667301177979
Batch 31/64 loss: 0.07727199792861938
Batch 32/64 loss: 0.10622560977935791
Batch 33/64 loss: 0.10465902090072632
Batch 34/64 loss: 0.07153177261352539
Batch 35/64 loss: 0.10878932476043701
Batch 36/64 loss: 0.09082353115081787
Batch 37/64 loss: 0.08788275718688965
Batch 38/64 loss: 0.09605365991592407
Batch 39/64 loss: 0.07826864719390869
Batch 40/64 loss: 0.10610270500183105
Batch 41/64 loss: 0.06406837701797485
Batch 42/64 loss: 0.06354635953903198
Batch 43/64 loss: 0.09188294410705566
Batch 44/64 loss: 0.09205150604248047
Batch 45/64 loss: 0.1150815486907959
Batch 46/64 loss: 0.09027010202407837
Batch 47/64 loss: 0.09809768199920654
Batch 48/64 loss: 0.08913975954055786
Batch 49/64 loss: 0.06503617763519287
Batch 50/64 loss: 0.10911250114440918
Batch 51/64 loss: 0.09093165397644043
Batch 52/64 loss: 0.08655118942260742
Batch 53/64 loss: 0.09325438737869263
Batch 54/64 loss: 0.07229679822921753
Batch 55/64 loss: 0.06944578886032104
Batch 56/64 loss: 0.10508012771606445
Batch 57/64 loss: 0.1101885437965393
Batch 58/64 loss: 0.06851768493652344
Batch 59/64 loss: 0.0842592716217041
Batch 60/64 loss: 0.08326232433319092
Batch 61/64 loss: 0.09122014045715332
Batch 62/64 loss: 0.11178243160247803
Batch 63/64 loss: 0.09726828336715698
Batch 64/64 loss: 0.11712104082107544
Epoch 158  Train loss: 0.0887018696934569  Val loss: 0.13263566018789494
Epoch 159
-------------------------------
Batch 1/64 loss: 0.07262265682220459
Batch 2/64 loss: 0.09255129098892212
Batch 3/64 loss: 0.10008203983306885
Batch 4/64 loss: 0.10965484380722046
Batch 5/64 loss: 0.08826833963394165
Batch 6/64 loss: 0.10101819038391113
Batch 7/64 loss: 0.08282202482223511
Batch 8/64 loss: 0.06678402423858643
Batch 9/64 loss: 0.0928422212600708
Batch 10/64 loss: 0.08326882123947144
Batch 11/64 loss: 0.13997268676757812
Batch 12/64 loss: 0.09946417808532715
Batch 13/64 loss: 0.09667104482650757
Batch 14/64 loss: 0.06407397985458374
Batch 15/64 loss: 0.12723249197006226
Batch 16/64 loss: 0.1018674373626709
Batch 17/64 loss: 0.09278655052185059
Batch 18/64 loss: 0.10733550786972046
Batch 19/64 loss: 0.0812537670135498
Batch 20/64 loss: 0.1037333607673645
Batch 21/64 loss: 0.08259177207946777
Batch 22/64 loss: 0.07706236839294434
Batch 23/64 loss: 0.08104467391967773
Batch 24/64 loss: 0.08644652366638184
Batch 25/64 loss: 0.08661091327667236
Batch 26/64 loss: 0.08689892292022705
Batch 27/64 loss: 0.11117255687713623
Batch 28/64 loss: 0.0701867938041687
Batch 29/64 loss: 0.08929264545440674
Batch 30/64 loss: 0.09561121463775635
Batch 31/64 loss: 0.0658072829246521
Batch 32/64 loss: 0.09763211011886597
Batch 33/64 loss: 0.10051202774047852
Batch 34/64 loss: 0.08218032121658325
Batch 35/64 loss: 0.12112081050872803
Batch 36/64 loss: 0.0886874794960022
Batch 37/64 loss: 0.0929344892501831
Batch 38/64 loss: 0.09213447570800781
Batch 39/64 loss: 0.07057422399520874
Batch 40/64 loss: 0.0855829119682312
Batch 41/64 loss: 0.0884208083152771
Batch 42/64 loss: 0.083282470703125
Batch 43/64 loss: 0.09382009506225586
Batch 44/64 loss: 0.06787675619125366
Batch 45/64 loss: 0.08138871192932129
Batch 46/64 loss: 0.07044154405593872
Batch 47/64 loss: 0.045486629009246826
Batch 48/64 loss: 0.08008813858032227
Batch 49/64 loss: 0.07150834798812866
Batch 50/64 loss: 0.09489727020263672
Batch 51/64 loss: 0.09264111518859863
Batch 52/64 loss: 0.11757630109786987
Batch 53/64 loss: 0.10470670461654663
Batch 54/64 loss: 0.06373965740203857
Batch 55/64 loss: 0.09165936708450317
Batch 56/64 loss: 0.07210439443588257
Batch 57/64 loss: 0.07891732454299927
Batch 58/64 loss: 0.09131360054016113
Batch 59/64 loss: 0.08666527271270752
Batch 60/64 loss: 0.06006741523742676
Batch 61/64 loss: 0.10039520263671875
Batch 62/64 loss: 0.08547848463058472
Batch 63/64 loss: 0.09031999111175537
Batch 64/64 loss: 0.0737837553024292
Epoch 159  Train loss: 0.08841605326708625  Val loss: 0.13429677752694724
Epoch 160
-------------------------------
Batch 1/64 loss: 0.07903909683227539
Batch 2/64 loss: 0.08169674873352051
Batch 3/64 loss: 0.08659815788269043
Batch 4/64 loss: 0.07918286323547363
Batch 5/64 loss: 0.1064291000366211
Batch 6/64 loss: 0.08840316534042358
Batch 7/64 loss: 0.07836037874221802
Batch 8/64 loss: 0.09498918056488037
Batch 9/64 loss: 0.10787957906723022
Batch 10/64 loss: 0.07778018712997437
Batch 11/64 loss: 0.06890332698822021
Batch 12/64 loss: 0.07766920328140259
Batch 13/64 loss: 0.055038273334503174
Batch 14/64 loss: 0.09153234958648682
Batch 15/64 loss: 0.09609264135360718
Batch 16/64 loss: 0.09305071830749512
Batch 17/64 loss: 0.0765262246131897
Batch 18/64 loss: 0.08534282445907593
Batch 19/64 loss: 0.08413046598434448
Batch 20/64 loss: 0.11121350526809692
Batch 21/64 loss: 0.09195631742477417
Batch 22/64 loss: 0.08433336019515991
Batch 23/64 loss: 0.052155137062072754
Batch 24/64 loss: 0.10164040327072144
Batch 25/64 loss: 0.12099742889404297
Batch 26/64 loss: 0.08565545082092285
Batch 27/64 loss: 0.08908790349960327
Batch 28/64 loss: 0.10032707452774048
Batch 29/64 loss: 0.07228875160217285
Batch 30/64 loss: 0.10104197263717651
Batch 31/64 loss: 0.10695546865463257
Batch 32/64 loss: 0.08287960290908813
Batch 33/64 loss: 0.08195608854293823
Batch 34/64 loss: 0.08864849805831909
Batch 35/64 loss: 0.08354079723358154
Batch 36/64 loss: 0.09105604887008667
Batch 37/64 loss: 0.08110672235488892
Batch 38/64 loss: 0.10346490144729614
Batch 39/64 loss: 0.07941633462905884
Batch 40/64 loss: 0.0967627763748169
Batch 41/64 loss: 0.09983307123184204
Batch 42/64 loss: 0.0712013840675354
Batch 43/64 loss: 0.10645937919616699
Batch 44/64 loss: 0.06578445434570312
Batch 45/64 loss: 0.08441615104675293
Batch 46/64 loss: 0.09084022045135498
Batch 47/64 loss: 0.08385640382766724
Batch 48/64 loss: 0.07002919912338257
Batch 49/64 loss: 0.07942074537277222
Batch 50/64 loss: 0.09770995378494263
Batch 51/64 loss: 0.10419690608978271
Batch 52/64 loss: 0.07215923070907593
Batch 53/64 loss: 0.0833941102027893
Batch 54/64 loss: 0.08237743377685547
Batch 55/64 loss: 0.0762937068939209
Batch 56/64 loss: 0.09832650423049927
Batch 57/64 loss: 0.08865684270858765
Batch 58/64 loss: 0.0886572003364563
Batch 59/64 loss: 0.08816349506378174
Batch 60/64 loss: 0.09309107065200806
Batch 61/64 loss: 0.08126407861709595
Batch 62/64 loss: 0.0758315920829773
Batch 63/64 loss: 0.10393983125686646
Batch 64/64 loss: 0.07813918590545654
Epoch 160  Train loss: 0.08720998249801935  Val loss: 0.1336872647308402
Epoch 161
-------------------------------
Batch 1/64 loss: 0.08284854888916016
Batch 2/64 loss: 0.08611631393432617
Batch 3/64 loss: 0.07973527908325195
Batch 4/64 loss: 0.08670258522033691
Batch 5/64 loss: 0.09694021940231323
Batch 6/64 loss: 0.06790786981582642
Batch 7/64 loss: 0.06879740953445435
Batch 8/64 loss: 0.045198917388916016
Batch 9/64 loss: 0.06957286596298218
Batch 10/64 loss: 0.11918628215789795
Batch 11/64 loss: 0.07773548364639282
Batch 12/64 loss: 0.093017578125
Batch 13/64 loss: 0.0911974310874939
Batch 14/64 loss: 0.08191055059432983
Batch 15/64 loss: 0.08668595552444458
Batch 16/64 loss: 0.11373800039291382
Batch 17/64 loss: 0.09534716606140137
Batch 18/64 loss: 0.05914735794067383
Batch 19/64 loss: 0.09791207313537598
Batch 20/64 loss: 0.09667176008224487
Batch 21/64 loss: 0.07792425155639648
Batch 22/64 loss: 0.09798228740692139
Batch 23/64 loss: 0.08886271715164185
Batch 24/64 loss: 0.10118436813354492
Batch 25/64 loss: 0.09149223566055298
Batch 26/64 loss: 0.0931164026260376
Batch 27/64 loss: 0.08565527200698853
Batch 28/64 loss: 0.06557565927505493
Batch 29/64 loss: 0.09987926483154297
Batch 30/64 loss: 0.0910407304763794
Batch 31/64 loss: 0.08176946640014648
Batch 32/64 loss: 0.1234428882598877
Batch 33/64 loss: 0.07102394104003906
Batch 34/64 loss: 0.12198257446289062
Batch 35/64 loss: 0.08259451389312744
Batch 36/64 loss: 0.09194278717041016
Batch 37/64 loss: 0.09512603282928467
Batch 38/64 loss: 0.09901267290115356
Batch 39/64 loss: 0.07274740934371948
Batch 40/64 loss: 0.08179628849029541
Batch 41/64 loss: 0.11966490745544434
Batch 42/64 loss: 0.07997238636016846
Batch 43/64 loss: 0.05967092514038086
Batch 44/64 loss: 0.05949556827545166
Batch 45/64 loss: 0.07322275638580322
Batch 46/64 loss: 0.0679788589477539
Batch 47/64 loss: 0.10760092735290527
Batch 48/64 loss: 0.07446801662445068
Batch 49/64 loss: 0.08441239595413208
Batch 50/64 loss: 0.07343584299087524
Batch 51/64 loss: 0.09823471307754517
Batch 52/64 loss: 0.09226977825164795
Batch 53/64 loss: 0.12754392623901367
Batch 54/64 loss: 0.08014309406280518
Batch 55/64 loss: 0.07860136032104492
Batch 56/64 loss: 0.09170645475387573
Batch 57/64 loss: 0.07162737846374512
Batch 58/64 loss: 0.06557929515838623
Batch 59/64 loss: 0.06702470779418945
Batch 60/64 loss: 0.10709798336029053
Batch 61/64 loss: 0.09426629543304443
Batch 62/64 loss: 0.11859649419784546
Batch 63/64 loss: 0.11506080627441406
Batch 64/64 loss: 0.06932568550109863
Epoch 161  Train loss: 0.08737553802191042  Val loss: 0.1345798436718708
Epoch 162
-------------------------------
Batch 1/64 loss: 0.09908419847488403
Batch 2/64 loss: 0.0720754861831665
Batch 3/64 loss: 0.07378792762756348
Batch 4/64 loss: 0.06382304430007935
Batch 5/64 loss: 0.07618862390518188
Batch 6/64 loss: 0.09219980239868164
Batch 7/64 loss: 0.07544595003128052
Batch 8/64 loss: 0.0921708345413208
Batch 9/64 loss: 0.08300334215164185
Batch 10/64 loss: 0.11486625671386719
Batch 11/64 loss: 0.07657265663146973
Batch 12/64 loss: 0.05695521831512451
Batch 13/64 loss: 0.09504550695419312
Batch 14/64 loss: 0.09633702039718628
Batch 15/64 loss: 0.08704888820648193
Batch 16/64 loss: 0.08857619762420654
Batch 17/64 loss: 0.0914352536201477
Batch 18/64 loss: 0.07652103900909424
Batch 19/64 loss: 0.10847961902618408
Batch 20/64 loss: 0.07444947957992554
Batch 21/64 loss: 0.08864367008209229
Batch 22/64 loss: 0.09171950817108154
Batch 23/64 loss: 0.08635860681533813
Batch 24/64 loss: 0.07918477058410645
Batch 25/64 loss: 0.10486793518066406
Batch 26/64 loss: 0.09004229307174683
Batch 27/64 loss: 0.08670979738235474
Batch 28/64 loss: 0.10534346103668213
Batch 29/64 loss: 0.0987095832824707
Batch 30/64 loss: 0.07476657629013062
Batch 31/64 loss: 0.07450830936431885
Batch 32/64 loss: 0.1118132472038269
Batch 33/64 loss: 0.1048385500907898
Batch 34/64 loss: 0.08999693393707275
Batch 35/64 loss: 0.08468782901763916
Batch 36/64 loss: 0.08741152286529541
Batch 37/64 loss: 0.07779937982559204
Batch 38/64 loss: 0.09121435880661011
Batch 39/64 loss: 0.10480576753616333
Batch 40/64 loss: 0.06362611055374146
Batch 41/64 loss: 0.09237545728683472
Batch 42/64 loss: 0.09682250022888184
Batch 43/64 loss: 0.08367025852203369
Batch 44/64 loss: 0.07359009981155396
Batch 45/64 loss: 0.06808006763458252
Batch 46/64 loss: 0.0744096040725708
Batch 47/64 loss: 0.0825616717338562
Batch 48/64 loss: 0.09098225831985474
Batch 49/64 loss: 0.09256333112716675
Batch 50/64 loss: 0.11535006761550903
Batch 51/64 loss: 0.09419316053390503
Batch 52/64 loss: 0.08820241689682007
Batch 53/64 loss: 0.07075977325439453
Batch 54/64 loss: 0.0962902307510376
Batch 55/64 loss: 0.08746516704559326
Batch 56/64 loss: 0.0772971510887146
Batch 57/64 loss: 0.073952317237854
Batch 58/64 loss: 0.08622723817825317
Batch 59/64 loss: 0.10158169269561768
Batch 60/64 loss: 0.10452783107757568
Batch 61/64 loss: 0.07554954290390015
Batch 62/64 loss: 0.10270535945892334
Batch 63/64 loss: 0.08623510599136353
Batch 64/64 loss: 0.071941077709198
Epoch 162  Train loss: 0.08722294382020539  Val loss: 0.13151632100855773
Epoch 163
-------------------------------
Batch 1/64 loss: 0.08304965496063232
Batch 2/64 loss: 0.07266426086425781
Batch 3/64 loss: 0.1013975739479065
Batch 4/64 loss: 0.08675611019134521
Batch 5/64 loss: 0.10609197616577148
Batch 6/64 loss: 0.09053826332092285
Batch 7/64 loss: 0.09755557775497437
Batch 8/64 loss: 0.10587942600250244
Batch 9/64 loss: 0.07381516695022583
Batch 10/64 loss: 0.0765877366065979
Batch 11/64 loss: 0.08689498901367188
Batch 12/64 loss: 0.1169130802154541
Batch 13/64 loss: 0.1042441725730896
Batch 14/64 loss: 0.06384932994842529
Batch 15/64 loss: 0.08983957767486572
Batch 16/64 loss: 0.11162185668945312
Batch 17/64 loss: 0.07848799228668213
Batch 18/64 loss: 0.07670038938522339
Batch 19/64 loss: 0.07841354608535767
Batch 20/64 loss: 0.09910762310028076
Batch 21/64 loss: 0.08491075038909912
Batch 22/64 loss: 0.0988045334815979
Batch 23/64 loss: 0.07999491691589355
Batch 24/64 loss: 0.08060431480407715
Batch 25/64 loss: 0.08358925580978394
Batch 26/64 loss: 0.08750832080841064
Batch 27/64 loss: 0.0749276876449585
Batch 28/64 loss: 0.09027212858200073
Batch 29/64 loss: 0.12286108732223511
Batch 30/64 loss: 0.08072316646575928
Batch 31/64 loss: 0.09951961040496826
Batch 32/64 loss: 0.11237269639968872
Batch 33/64 loss: 0.07308745384216309
Batch 34/64 loss: 0.0910189151763916
Batch 35/64 loss: 0.06604504585266113
Batch 36/64 loss: 0.08584648370742798
Batch 37/64 loss: 0.0910102128982544
Batch 38/64 loss: 0.09713608026504517
Batch 39/64 loss: 0.081154465675354
Batch 40/64 loss: 0.06956756114959717
Batch 41/64 loss: 0.09274959564208984
Batch 42/64 loss: 0.08945977687835693
Batch 43/64 loss: 0.06476497650146484
Batch 44/64 loss: 0.09054386615753174
Batch 45/64 loss: 0.0978386402130127
Batch 46/64 loss: 0.07177233695983887
Batch 47/64 loss: 0.09039008617401123
Batch 48/64 loss: 0.06582999229431152
Batch 49/64 loss: 0.06873536109924316
Batch 50/64 loss: 0.09061622619628906
Batch 51/64 loss: 0.05897003412246704
Batch 52/64 loss: 0.08918052911758423
Batch 53/64 loss: 0.10565602779388428
Batch 54/64 loss: 0.10719335079193115
Batch 55/64 loss: 0.09279870986938477
Batch 56/64 loss: 0.08223265409469604
Batch 57/64 loss: 0.07991951704025269
Batch 58/64 loss: 0.06481558084487915
Batch 59/64 loss: 0.08244383335113525
Batch 60/64 loss: 0.09715116024017334
Batch 61/64 loss: 0.06551474332809448
Batch 62/64 loss: 0.07803076505661011
Batch 63/64 loss: 0.0924379825592041
Batch 64/64 loss: 0.08963853120803833
Epoch 163  Train loss: 0.08686490409514483  Val loss: 0.12851829340367793
Saving best model, epoch: 163
Epoch 164
-------------------------------
Batch 1/64 loss: 0.09468626976013184
Batch 2/64 loss: 0.06491488218307495
Batch 3/64 loss: 0.09467166662216187
Batch 4/64 loss: 0.09519731998443604
Batch 5/64 loss: 0.0779426097869873
Batch 6/64 loss: 0.08104836940765381
Batch 7/64 loss: 0.058609724044799805
Batch 8/64 loss: 0.07099610567092896
Batch 9/64 loss: 0.10589152574539185
Batch 10/64 loss: 0.07040172815322876
Batch 11/64 loss: 0.08139538764953613
Batch 12/64 loss: 0.08614790439605713
Batch 13/64 loss: 0.07181161642074585
Batch 14/64 loss: 0.1016014814376831
Batch 15/64 loss: 0.070262610912323
Batch 16/64 loss: 0.0874205231666565
Batch 17/64 loss: 0.08858299255371094
Batch 18/64 loss: 0.10096955299377441
Batch 19/64 loss: 0.10041546821594238
Batch 20/64 loss: 0.09644001722335815
Batch 21/64 loss: 0.09707373380661011
Batch 22/64 loss: 0.06752318143844604
Batch 23/64 loss: 0.08441710472106934
Batch 24/64 loss: 0.05423778295516968
Batch 25/64 loss: 0.0897359848022461
Batch 26/64 loss: 0.07108724117279053
Batch 27/64 loss: 0.09490722417831421
Batch 28/64 loss: 0.08881199359893799
Batch 29/64 loss: 0.085116446018219
Batch 30/64 loss: 0.13685619831085205
Batch 31/64 loss: 0.0790356993675232
Batch 32/64 loss: 0.08691442012786865
Batch 33/64 loss: 0.07607114315032959
Batch 34/64 loss: 0.07161319255828857
Batch 35/64 loss: 0.0866471529006958
Batch 36/64 loss: 0.09266197681427002
Batch 37/64 loss: 0.10196942090988159
Batch 38/64 loss: 0.11040568351745605
Batch 39/64 loss: 0.06889593601226807
Batch 40/64 loss: 0.0777851939201355
Batch 41/64 loss: 0.08997732400894165
Batch 42/64 loss: 0.1188386082649231
Batch 43/64 loss: 0.11153745651245117
Batch 44/64 loss: 0.07077288627624512
Batch 45/64 loss: 0.07506370544433594
Batch 46/64 loss: 0.10663807392120361
Batch 47/64 loss: 0.08478713035583496
Batch 48/64 loss: 0.0984652042388916
Batch 49/64 loss: 0.08662796020507812
Batch 50/64 loss: 0.08158749341964722
Batch 51/64 loss: 0.08614927530288696
Batch 52/64 loss: 0.09833037853240967
Batch 53/64 loss: 0.07512021064758301
Batch 54/64 loss: 0.076346755027771
Batch 55/64 loss: 0.11033767461776733
Batch 56/64 loss: 0.050292789936065674
Batch 57/64 loss: 0.07979214191436768
Batch 58/64 loss: 0.1031029224395752
Batch 59/64 loss: 0.07896631956100464
Batch 60/64 loss: 0.07990789413452148
Batch 61/64 loss: 0.10571390390396118
Batch 62/64 loss: 0.08758962154388428
Batch 63/64 loss: 0.09700959920883179
Batch 64/64 loss: 0.09862411022186279
Epoch 164  Train loss: 0.08702898633246328  Val loss: 0.13545563417611664
Epoch 165
-------------------------------
Batch 1/64 loss: 0.08984524011611938
Batch 2/64 loss: 0.07409518957138062
Batch 3/64 loss: 0.06389808654785156
Batch 4/64 loss: 0.0883101224899292
Batch 5/64 loss: 0.07914066314697266
Batch 6/64 loss: 0.09468263387680054
Batch 7/64 loss: 0.07690685987472534
Batch 8/64 loss: 0.08840703964233398
Batch 9/64 loss: 0.08606970310211182
Batch 10/64 loss: 0.08232206106185913
Batch 11/64 loss: 0.08853805065155029
Batch 12/64 loss: 0.08792519569396973
Batch 13/64 loss: 0.06942284107208252
Batch 14/64 loss: 0.07762753963470459
Batch 15/64 loss: 0.07981067895889282
Batch 16/64 loss: 0.09365814924240112
Batch 17/64 loss: 0.08527827262878418
Batch 18/64 loss: 0.07567459344863892
Batch 19/64 loss: 0.0960078239440918
Batch 20/64 loss: 0.09223920106887817
Batch 21/64 loss: 0.07633280754089355
Batch 22/64 loss: 0.07864499092102051
Batch 23/64 loss: 0.08467411994934082
Batch 24/64 loss: 0.08858835697174072
Batch 25/64 loss: 0.09855276346206665
Batch 26/64 loss: 0.07655054330825806
Batch 27/64 loss: 0.08897316455841064
Batch 28/64 loss: 0.08336073160171509
Batch 29/64 loss: 0.07070314884185791
Batch 30/64 loss: 0.06803154945373535
Batch 31/64 loss: 0.07929033041000366
Batch 32/64 loss: 0.10784262418746948
Batch 33/64 loss: 0.11023867130279541
Batch 34/64 loss: 0.09975552558898926
Batch 35/64 loss: 0.09286361932754517
Batch 36/64 loss: 0.08320403099060059
Batch 37/64 loss: 0.08617490530014038
Batch 38/64 loss: 0.10770714282989502
Batch 39/64 loss: 0.12578797340393066
Batch 40/64 loss: 0.08468180894851685
Batch 41/64 loss: 0.09178757667541504
Batch 42/64 loss: 0.09135746955871582
Batch 43/64 loss: 0.08048582077026367
Batch 44/64 loss: 0.0854421854019165
Batch 45/64 loss: 0.09057283401489258
Batch 46/64 loss: 0.07641106843948364
Batch 47/64 loss: 0.10525590181350708
Batch 48/64 loss: 0.06936502456665039
Batch 49/64 loss: 0.07006239891052246
Batch 50/64 loss: 0.09749436378479004
Batch 51/64 loss: 0.07693648338317871
Batch 52/64 loss: 0.0842975378036499
Batch 53/64 loss: 0.0958603024482727
Batch 54/64 loss: 0.10176557302474976
Batch 55/64 loss: 0.09758579730987549
Batch 56/64 loss: 0.06572943925857544
Batch 57/64 loss: 0.08546912670135498
Batch 58/64 loss: 0.0744583010673523
Batch 59/64 loss: 0.09927976131439209
Batch 60/64 loss: 0.09847664833068848
Batch 61/64 loss: 0.08650821447372437
Batch 62/64 loss: 0.07366126775741577
Batch 63/64 loss: 0.08601987361907959
Batch 64/64 loss: 0.07670998573303223
Epoch 165  Train loss: 0.08633151708864699  Val loss: 0.13011597931589866
Epoch 166
-------------------------------
Batch 1/64 loss: 0.09453320503234863
Batch 2/64 loss: 0.07650971412658691
Batch 3/64 loss: 0.10865992307662964
Batch 4/64 loss: 0.07761180400848389
Batch 5/64 loss: 0.08420199155807495
Batch 6/64 loss: 0.061984121799468994
Batch 7/64 loss: 0.061302006244659424
Batch 8/64 loss: 0.08553731441497803
Batch 9/64 loss: 0.08351212739944458
Batch 10/64 loss: 0.07377737760543823
Batch 11/64 loss: 0.06862062215805054
Batch 12/64 loss: 0.07259237766265869
Batch 13/64 loss: 0.07247787714004517
Batch 14/64 loss: 0.08058840036392212
Batch 15/64 loss: 0.10009586811065674
Batch 16/64 loss: 0.06782925128936768
Batch 17/64 loss: 0.05546385049819946
Batch 18/64 loss: 0.07860887050628662
Batch 19/64 loss: 0.09793901443481445
Batch 20/64 loss: 0.08476775884628296
Batch 21/64 loss: 0.0697861909866333
Batch 22/64 loss: 0.05391085147857666
Batch 23/64 loss: 0.07639586925506592
Batch 24/64 loss: 0.08300995826721191
Batch 25/64 loss: 0.10546427965164185
Batch 26/64 loss: 0.11006903648376465
Batch 27/64 loss: 0.07623231410980225
Batch 28/64 loss: 0.08292651176452637
Batch 29/64 loss: 0.09425181150436401
Batch 30/64 loss: 0.08165597915649414
Batch 31/64 loss: 0.10999202728271484
Batch 32/64 loss: 0.09327185153961182
Batch 33/64 loss: 0.09445643424987793
Batch 34/64 loss: 0.08803021907806396
Batch 35/64 loss: 0.08974754810333252
Batch 36/64 loss: 0.09663653373718262
Batch 37/64 loss: 0.07260078191757202
Batch 38/64 loss: 0.07678329944610596
Batch 39/64 loss: 0.09731578826904297
Batch 40/64 loss: 0.11348855495452881
Batch 41/64 loss: 0.10209965705871582
Batch 42/64 loss: 0.096782386302948
Batch 43/64 loss: 0.07362943887710571
Batch 44/64 loss: 0.08995938301086426
Batch 45/64 loss: 0.09590709209442139
Batch 46/64 loss: 0.04538971185684204
Batch 47/64 loss: 0.09614533185958862
Batch 48/64 loss: 0.070797860622406
Batch 49/64 loss: 0.10662508010864258
Batch 50/64 loss: 0.10715174674987793
Batch 51/64 loss: 0.054139137268066406
Batch 52/64 loss: 0.0932464599609375
Batch 53/64 loss: 0.07027655839920044
Batch 54/64 loss: 0.1321394443511963
Batch 55/64 loss: 0.08418196439743042
Batch 56/64 loss: 0.09197759628295898
Batch 57/64 loss: 0.0985708236694336
Batch 58/64 loss: 0.09360337257385254
Batch 59/64 loss: 0.08256018161773682
Batch 60/64 loss: 0.08400094509124756
Batch 61/64 loss: 0.0697450041770935
Batch 62/64 loss: 0.09267735481262207
Batch 63/64 loss: 0.08644717931747437
Batch 64/64 loss: 0.1034659743309021
Epoch 166  Train loss: 0.0854634119015114  Val loss: 0.13143004092973531
Epoch 167
-------------------------------
Batch 1/64 loss: 0.08490073680877686
Batch 2/64 loss: 0.06657874584197998
Batch 3/64 loss: 0.0766294002532959
Batch 4/64 loss: 0.03660541772842407
Batch 5/64 loss: 0.0829818844795227
Batch 6/64 loss: 0.08649694919586182
Batch 7/64 loss: 0.07015639543533325
Batch 8/64 loss: 0.07955402135848999
Batch 9/64 loss: 0.09643042087554932
Batch 10/64 loss: 0.07629013061523438
Batch 11/64 loss: 0.10082495212554932
Batch 12/64 loss: 0.09431928396224976
Batch 13/64 loss: 0.07514435052871704
Batch 14/64 loss: 0.08402520418167114
Batch 15/64 loss: 0.0818939208984375
Batch 16/64 loss: 0.09382563829421997
Batch 17/64 loss: 0.06573808193206787
Batch 18/64 loss: 0.0929403305053711
Batch 19/64 loss: 0.08733773231506348
Batch 20/64 loss: 0.089663565158844
Batch 21/64 loss: 0.062290310859680176
Batch 22/64 loss: 0.07840019464492798
Batch 23/64 loss: 0.05793887376785278
Batch 24/64 loss: 0.098319411277771
Batch 25/64 loss: 0.1019371747970581
Batch 26/64 loss: 0.08714401721954346
Batch 27/64 loss: 0.07830178737640381
Batch 28/64 loss: 0.07537657022476196
Batch 29/64 loss: 0.07768440246582031
Batch 30/64 loss: 0.08677953481674194
Batch 31/64 loss: 0.09841328859329224
Batch 32/64 loss: 0.09583455324172974
Batch 33/64 loss: 0.08463966846466064
Batch 34/64 loss: 0.04798191785812378
Batch 35/64 loss: 0.10079199075698853
Batch 36/64 loss: 0.056666016578674316
Batch 37/64 loss: 0.10869204998016357
Batch 38/64 loss: 0.07992774248123169
Batch 39/64 loss: 0.09075939655303955
Batch 40/64 loss: 0.10637354850769043
Batch 41/64 loss: 0.1045372486114502
Batch 42/64 loss: 0.08726930618286133
Batch 43/64 loss: 0.10649067163467407
Batch 44/64 loss: 0.07738864421844482
Batch 45/64 loss: 0.09892278909683228
Batch 46/64 loss: 0.08211088180541992
Batch 47/64 loss: 0.10010892152786255
Batch 48/64 loss: 0.09426343441009521
Batch 49/64 loss: 0.08491623401641846
Batch 50/64 loss: 0.05476832389831543
Batch 51/64 loss: 0.07913947105407715
Batch 52/64 loss: 0.101959228515625
Batch 53/64 loss: 0.09521842002868652
Batch 54/64 loss: 0.10534662008285522
Batch 55/64 loss: 0.09214335680007935
Batch 56/64 loss: 0.10138022899627686
Batch 57/64 loss: 0.10850781202316284
Batch 58/64 loss: 0.0783962607383728
Batch 59/64 loss: 0.07002699375152588
Batch 60/64 loss: 0.07322472333908081
Batch 61/64 loss: 0.0742347240447998
Batch 62/64 loss: 0.1067819595336914
Batch 63/64 loss: 0.07913780212402344
Batch 64/64 loss: 0.08575505018234253
Epoch 167  Train loss: 0.08497537187501496  Val loss: 0.13016934022051363
Epoch 168
-------------------------------
Batch 1/64 loss: 0.06846904754638672
Batch 2/64 loss: 0.10123687982559204
Batch 3/64 loss: 0.09243327379226685
Batch 4/64 loss: 0.07165360450744629
Batch 5/64 loss: 0.08643227815628052
Batch 6/64 loss: 0.09223270416259766
Batch 7/64 loss: 0.09949713945388794
Batch 8/64 loss: 0.0958564281463623
Batch 9/64 loss: 0.09178566932678223
Batch 10/64 loss: 0.06182992458343506
Batch 11/64 loss: 0.06380915641784668
Batch 12/64 loss: 0.08180934190750122
Batch 13/64 loss: 0.06968438625335693
Batch 14/64 loss: 0.05131340026855469
Batch 15/64 loss: 0.08815830945968628
Batch 16/64 loss: 0.09015238285064697
Batch 17/64 loss: 0.08746129274368286
Batch 18/64 loss: 0.09117090702056885
Batch 19/64 loss: 0.09106612205505371
Batch 20/64 loss: 0.07693475484848022
Batch 21/64 loss: 0.07152748107910156
Batch 22/64 loss: 0.04855990409851074
Batch 23/64 loss: 0.08679920434951782
Batch 24/64 loss: 0.06542474031448364
Batch 25/64 loss: 0.10343557596206665
Batch 26/64 loss: 0.1145511269569397
Batch 27/64 loss: 0.08815586566925049
Batch 28/64 loss: 0.0998908281326294
Batch 29/64 loss: 0.09828215837478638
Batch 30/64 loss: 0.0777367353439331
Batch 31/64 loss: 0.09707003831863403
Batch 32/64 loss: 0.08904767036437988
Batch 33/64 loss: 0.08423656225204468
Batch 34/64 loss: 0.0639614462852478
Batch 35/64 loss: 0.07985556125640869
Batch 36/64 loss: 0.1038748025894165
Batch 37/64 loss: 0.07494920492172241
Batch 38/64 loss: 0.08666670322418213
Batch 39/64 loss: 0.08980125188827515
Batch 40/64 loss: 0.08026516437530518
Batch 41/64 loss: 0.07230740785598755
Batch 42/64 loss: 0.10036295652389526
Batch 43/64 loss: 0.09026825428009033
Batch 44/64 loss: 0.0810922384262085
Batch 45/64 loss: 0.08744692802429199
Batch 46/64 loss: 0.09868085384368896
Batch 47/64 loss: 0.08121287822723389
Batch 48/64 loss: 0.07811468839645386
Batch 49/64 loss: 0.09790784120559692
Batch 50/64 loss: 0.07387447357177734
Batch 51/64 loss: 0.08182066679000854
Batch 52/64 loss: 0.08219832181930542
Batch 53/64 loss: 0.08886885643005371
Batch 54/64 loss: 0.0608752965927124
Batch 55/64 loss: 0.09697514772415161
Batch 56/64 loss: 0.07075381278991699
Batch 57/64 loss: 0.0991021990776062
Batch 58/64 loss: 0.07328188419342041
Batch 59/64 loss: 0.08555293083190918
Batch 60/64 loss: 0.09479635953903198
Batch 61/64 loss: 0.0816044807434082
Batch 62/64 loss: 0.06893545389175415
Batch 63/64 loss: 0.08442670106887817
Batch 64/64 loss: 0.08345931768417358
Epoch 168  Train loss: 0.08392367292852963  Val loss: 0.1314152065421298
Epoch 169
-------------------------------
Batch 1/64 loss: 0.0791701078414917
Batch 2/64 loss: 0.0861169695854187
Batch 3/64 loss: 0.06817471981048584
Batch 4/64 loss: 0.09726548194885254
Batch 5/64 loss: 0.06975424289703369
Batch 6/64 loss: 0.09024649858474731
Batch 7/64 loss: 0.09031671285629272
Batch 8/64 loss: 0.07428020238876343
Batch 9/64 loss: 0.07150876522064209
Batch 10/64 loss: 0.10001623630523682
Batch 11/64 loss: 0.09315752983093262
Batch 12/64 loss: 0.05915874242782593
Batch 13/64 loss: 0.08706080913543701
Batch 14/64 loss: 0.061756908893585205
Batch 15/64 loss: 0.07102441787719727
Batch 16/64 loss: 0.0693463683128357
Batch 17/64 loss: 0.0756692886352539
Batch 18/64 loss: 0.08355778455734253
Batch 19/64 loss: 0.07050430774688721
Batch 20/64 loss: 0.0646134614944458
Batch 21/64 loss: 0.09131515026092529
Batch 22/64 loss: 0.0736580491065979
Batch 23/64 loss: 0.09196031093597412
Batch 24/64 loss: 0.11179548501968384
Batch 25/64 loss: 0.07847946882247925
Batch 26/64 loss: 0.08695995807647705
Batch 27/64 loss: 0.059594154357910156
Batch 28/64 loss: 0.06252443790435791
Batch 29/64 loss: 0.09477925300598145
Batch 30/64 loss: 0.08456087112426758
Batch 31/64 loss: 0.08767104148864746
Batch 32/64 loss: 0.08094078302383423
Batch 33/64 loss: 0.10197621583938599
Batch 34/64 loss: 0.08395278453826904
Batch 35/64 loss: 0.08689677715301514
Batch 36/64 loss: 0.0776941180229187
Batch 37/64 loss: 0.0707848072052002
Batch 38/64 loss: 0.1144673228263855
Batch 39/64 loss: 0.07841408252716064
Batch 40/64 loss: 0.06537723541259766
Batch 41/64 loss: 0.09117388725280762
Batch 42/64 loss: 0.09125649929046631
Batch 43/64 loss: 0.09764420986175537
Batch 44/64 loss: 0.09711563587188721
Batch 45/64 loss: 0.06985855102539062
Batch 46/64 loss: 0.07568323612213135
Batch 47/64 loss: 0.09211373329162598
Batch 48/64 loss: 0.10322427749633789
Batch 49/64 loss: 0.07651829719543457
Batch 50/64 loss: 0.07866048812866211
Batch 51/64 loss: 0.08263891935348511
Batch 52/64 loss: 0.0922321081161499
Batch 53/64 loss: 0.0890313982963562
Batch 54/64 loss: 0.11935049295425415
Batch 55/64 loss: 0.07438927888870239
Batch 56/64 loss: 0.08972400426864624
Batch 57/64 loss: 0.06997096538543701
Batch 58/64 loss: 0.06636232137680054
Batch 59/64 loss: 0.11152482032775879
Batch 60/64 loss: 0.08872658014297485
Batch 61/64 loss: 0.06335663795471191
Batch 62/64 loss: 0.1178441047668457
Batch 63/64 loss: 0.10198843479156494
Batch 64/64 loss: 0.07771879434585571
Epoch 169  Train loss: 0.08384595828897813  Val loss: 0.12826918326702313
Saving best model, epoch: 169
Epoch 170
-------------------------------
Batch 1/64 loss: 0.08816397190093994
Batch 2/64 loss: 0.09092426300048828
Batch 3/64 loss: 0.08775192499160767
Batch 4/64 loss: 0.07958817481994629
Batch 5/64 loss: 0.08758604526519775
Batch 6/64 loss: 0.08166474103927612
Batch 7/64 loss: 0.0931175947189331
Batch 8/64 loss: 0.07095229625701904
Batch 9/64 loss: 0.1006893515586853
Batch 10/64 loss: 0.07010769844055176
Batch 11/64 loss: 0.07506859302520752
Batch 12/64 loss: 0.05947768688201904
Batch 13/64 loss: 0.08263659477233887
Batch 14/64 loss: 0.0785793662071228
Batch 15/64 loss: 0.08383423089981079
Batch 16/64 loss: 0.0923999547958374
Batch 17/64 loss: 0.06844854354858398
Batch 18/64 loss: 0.07380223274230957
Batch 19/64 loss: 0.07162374258041382
Batch 20/64 loss: 0.10315918922424316
Batch 21/64 loss: 0.09131097793579102
Batch 22/64 loss: 0.08554130792617798
Batch 23/64 loss: 0.07707762718200684
Batch 24/64 loss: 0.10771656036376953
Batch 25/64 loss: 0.07392376661300659
Batch 26/64 loss: 0.11366778612136841
Batch 27/64 loss: 0.06744933128356934
Batch 28/64 loss: 0.052030205726623535
Batch 29/64 loss: 0.08330100774765015
Batch 30/64 loss: 0.0728951096534729
Batch 31/64 loss: 0.09474074840545654
Batch 32/64 loss: 0.08228707313537598
Batch 33/64 loss: 0.07700371742248535
Batch 34/64 loss: 0.08411777019500732
Batch 35/64 loss: 0.11017715930938721
Batch 36/64 loss: 0.09488755464553833
Batch 37/64 loss: 0.08925485610961914
Batch 38/64 loss: 0.09608453512191772
Batch 39/64 loss: 0.09936153888702393
Batch 40/64 loss: 0.07232171297073364
Batch 41/64 loss: 0.06858086585998535
Batch 42/64 loss: 0.09610265493392944
Batch 43/64 loss: 0.09885376691818237
Batch 44/64 loss: 0.07023042440414429
Batch 45/64 loss: 0.0722576379776001
Batch 46/64 loss: 0.0728449821472168
Batch 47/64 loss: 0.07543063163757324
Batch 48/64 loss: 0.06888985633850098
Batch 49/64 loss: 0.0846184492111206
Batch 50/64 loss: 0.0810312032699585
Batch 51/64 loss: 0.08796775341033936
Batch 52/64 loss: 0.10263830423355103
Batch 53/64 loss: 0.10088604688644409
Batch 54/64 loss: 0.1007201075553894
Batch 55/64 loss: 0.09782606363296509
Batch 56/64 loss: 0.09032011032104492
Batch 57/64 loss: 0.07886046171188354
Batch 58/64 loss: 0.09779369831085205
Batch 59/64 loss: 0.0659913420677185
Batch 60/64 loss: 0.10996407270431519
Batch 61/64 loss: 0.08032751083374023
Batch 62/64 loss: 0.04131805896759033
Batch 63/64 loss: 0.10947108268737793
Batch 64/64 loss: 0.08222019672393799
Epoch 170  Train loss: 0.0843814396390728  Val loss: 0.13124159480288267
Epoch 171
-------------------------------
Batch 1/64 loss: 0.08011269569396973
Batch 2/64 loss: 0.09407258033752441
Batch 3/64 loss: 0.0681222677230835
Batch 4/64 loss: 0.08037734031677246
Batch 5/64 loss: 0.0678786039352417
Batch 6/64 loss: 0.09453976154327393
Batch 7/64 loss: 0.0738227367401123
Batch 8/64 loss: 0.06861287355422974
Batch 9/64 loss: 0.08828282356262207
Batch 10/64 loss: 0.08242243528366089
Batch 11/64 loss: 0.06981068849563599
Batch 12/64 loss: 0.0876111388206482
Batch 13/64 loss: 0.07271754741668701
Batch 14/64 loss: 0.08187198638916016
Batch 15/64 loss: 0.09131628274917603
Batch 16/64 loss: 0.059056103229522705
Batch 17/64 loss: 0.08147215843200684
Batch 18/64 loss: 0.06442362070083618
Batch 19/64 loss: 0.0825691819190979
Batch 20/64 loss: 0.09336274862289429
Batch 21/64 loss: 0.07576525211334229
Batch 22/64 loss: 0.08636772632598877
Batch 23/64 loss: 0.07834798097610474
Batch 24/64 loss: 0.07988733053207397
Batch 25/64 loss: 0.12531203031539917
Batch 26/64 loss: 0.07044440507888794
Batch 27/64 loss: 0.07104557752609253
Batch 28/64 loss: 0.05308026075363159
Batch 29/64 loss: 0.07545661926269531
Batch 30/64 loss: 0.09065604209899902
Batch 31/64 loss: 0.06886816024780273
Batch 32/64 loss: 0.09575152397155762
Batch 33/64 loss: 0.07811659574508667
Batch 34/64 loss: 0.04681885242462158
Batch 35/64 loss: 0.09147828817367554
Batch 36/64 loss: 0.07300668954849243
Batch 37/64 loss: 0.0657418966293335
Batch 38/64 loss: 0.08125418424606323
Batch 39/64 loss: 0.10093259811401367
Batch 40/64 loss: 0.08038902282714844
Batch 41/64 loss: 0.07924538850784302
Batch 42/64 loss: 0.060125112533569336
Batch 43/64 loss: 0.05344206094741821
Batch 44/64 loss: 0.10122883319854736
Batch 45/64 loss: 0.08849525451660156
Batch 46/64 loss: 0.11193102598190308
Batch 47/64 loss: 0.11182510852813721
Batch 48/64 loss: 0.08536553382873535
Batch 49/64 loss: 0.10999363660812378
Batch 50/64 loss: 0.08106344938278198
Batch 51/64 loss: 0.07654750347137451
Batch 52/64 loss: 0.08848017454147339
Batch 53/64 loss: 0.10496848821640015
Batch 54/64 loss: 0.06276822090148926
Batch 55/64 loss: 0.0795556902885437
Batch 56/64 loss: 0.08450537919998169
Batch 57/64 loss: 0.1102989912033081
Batch 58/64 loss: 0.08227384090423584
Batch 59/64 loss: 0.08416110277175903
Batch 60/64 loss: 0.10248160362243652
Batch 61/64 loss: 0.07373535633087158
Batch 62/64 loss: 0.08409351110458374
Batch 63/64 loss: 0.07998406887054443
Batch 64/64 loss: 0.14353078603744507
Epoch 171  Train loss: 0.08275127901750452  Val loss: 0.12873911345537586
Epoch 172
-------------------------------
Batch 1/64 loss: 0.06877195835113525
Batch 2/64 loss: 0.0773584246635437
Batch 3/64 loss: 0.07710975408554077
Batch 4/64 loss: 0.05336958169937134
Batch 5/64 loss: 0.0757366418838501
Batch 6/64 loss: 0.10605978965759277
Batch 7/64 loss: 0.07192045450210571
Batch 8/64 loss: 0.06875389814376831
Batch 9/64 loss: 0.09118396043777466
Batch 10/64 loss: 0.08938825130462646
Batch 11/64 loss: 0.09373319149017334
Batch 12/64 loss: 0.07007771730422974
Batch 13/64 loss: 0.06703245639801025
Batch 14/64 loss: 0.07907086610794067
Batch 15/64 loss: 0.0712929368019104
Batch 16/64 loss: 0.05221223831176758
Batch 17/64 loss: 0.05740165710449219
Batch 18/64 loss: 0.077644944190979
Batch 19/64 loss: 0.04495948553085327
Batch 20/64 loss: 0.07677102088928223
Batch 21/64 loss: 0.07571417093276978
Batch 22/64 loss: 0.10256379842758179
Batch 23/64 loss: 0.05304229259490967
Batch 24/64 loss: 0.08350968360900879
Batch 25/64 loss: 0.08521157503128052
Batch 26/64 loss: 0.09067773818969727
Batch 27/64 loss: 0.06837457418441772
Batch 28/64 loss: 0.11572974920272827
Batch 29/64 loss: 0.09071433544158936
Batch 30/64 loss: 0.0933716893196106
Batch 31/64 loss: 0.08462643623352051
Batch 32/64 loss: 0.08890938758850098
Batch 33/64 loss: 0.08982312679290771
Batch 34/64 loss: 0.11289292573928833
Batch 35/64 loss: 0.09820365905761719
Batch 36/64 loss: 0.08448314666748047
Batch 37/64 loss: 0.07772696018218994
Batch 38/64 loss: 0.09173405170440674
Batch 39/64 loss: 0.11007171869277954
Batch 40/64 loss: 0.08590030670166016
Batch 41/64 loss: 0.09556454420089722
Batch 42/64 loss: 0.07880830764770508
Batch 43/64 loss: 0.09327757358551025
Batch 44/64 loss: 0.08983027935028076
Batch 45/64 loss: 0.07285565137863159
Batch 46/64 loss: 0.06982386112213135
Batch 47/64 loss: 0.09365099668502808
Batch 48/64 loss: 0.10751456022262573
Batch 49/64 loss: 0.0773550271987915
Batch 50/64 loss: 0.10414975881576538
Batch 51/64 loss: 0.10861963033676147
Batch 52/64 loss: 0.09236413240432739
Batch 53/64 loss: 0.0866079330444336
Batch 54/64 loss: 0.09452825784683228
Batch 55/64 loss: 0.08920848369598389
Batch 56/64 loss: 0.06142449378967285
Batch 57/64 loss: 0.08342188596725464
Batch 58/64 loss: 0.07955062389373779
Batch 59/64 loss: 0.06797116994857788
Batch 60/64 loss: 0.08429384231567383
Batch 61/64 loss: 0.08342576026916504
Batch 62/64 loss: 0.06655287742614746
Batch 63/64 loss: 0.07675588130950928
Batch 64/64 loss: 0.092315673828125
Epoch 172  Train loss: 0.0828223191055597  Val loss: 0.12751373085369358
Saving best model, epoch: 172
Epoch 173
-------------------------------
Batch 1/64 loss: 0.09319949150085449
Batch 2/64 loss: 0.06293612718582153
Batch 3/64 loss: 0.07564753293991089
Batch 4/64 loss: 0.06973564624786377
Batch 5/64 loss: 0.06826543807983398
Batch 6/64 loss: 0.08877813816070557
Batch 7/64 loss: 0.08212161064147949
Batch 8/64 loss: 0.08102685213088989
Batch 9/64 loss: 0.07595270872116089
Batch 10/64 loss: 0.08171403408050537
Batch 11/64 loss: 0.09495782852172852
Batch 12/64 loss: 0.100094735622406
Batch 13/64 loss: 0.12605983018875122
Batch 14/64 loss: 0.04747509956359863
Batch 15/64 loss: 0.104888916015625
Batch 16/64 loss: 0.10631668567657471
Batch 17/64 loss: 0.06630206108093262
Batch 18/64 loss: 0.11468803882598877
Batch 19/64 loss: 0.08031129837036133
Batch 20/64 loss: 0.09667384624481201
Batch 21/64 loss: 0.08610683679580688
Batch 22/64 loss: 0.11527144908905029
Batch 23/64 loss: 0.0941004753112793
Batch 24/64 loss: 0.06418436765670776
Batch 25/64 loss: 0.06817269325256348
Batch 26/64 loss: 0.07978761196136475
Batch 27/64 loss: 0.07743608951568604
Batch 28/64 loss: 0.07688701152801514
Batch 29/64 loss: 0.07475543022155762
Batch 30/64 loss: 0.08439391851425171
Batch 31/64 loss: 0.0934874415397644
Batch 32/64 loss: 0.07095801830291748
Batch 33/64 loss: 0.10491752624511719
Batch 34/64 loss: 0.07085901498794556
Batch 35/64 loss: 0.07355016469955444
Batch 36/64 loss: 0.0722762942314148
Batch 37/64 loss: 0.07709908485412598
Batch 38/64 loss: 0.07925647497177124
Batch 39/64 loss: 0.08359110355377197
Batch 40/64 loss: 0.07435697317123413
Batch 41/64 loss: 0.07201170921325684
Batch 42/64 loss: 0.06432837247848511
Batch 43/64 loss: 0.06632733345031738
Batch 44/64 loss: 0.06434917449951172
Batch 45/64 loss: 0.05705761909484863
Batch 46/64 loss: 0.07485175132751465
Batch 47/64 loss: 0.09924077987670898
Batch 48/64 loss: 0.09193718433380127
Batch 49/64 loss: 0.05536365509033203
Batch 50/64 loss: 0.07845145463943481
Batch 51/64 loss: 0.07730746269226074
Batch 52/64 loss: 0.07842141389846802
Batch 53/64 loss: 0.07813215255737305
Batch 54/64 loss: 0.10033231973648071
Batch 55/64 loss: 0.07661008834838867
Batch 56/64 loss: 0.05918383598327637
Batch 57/64 loss: 0.08204424381256104
Batch 58/64 loss: 0.0713198184967041
Batch 59/64 loss: 0.08410465717315674
Batch 60/64 loss: 0.08521896600723267
Batch 61/64 loss: 0.09599655866622925
Batch 62/64 loss: 0.06745761632919312
Batch 63/64 loss: 0.0962071418762207
Batch 64/64 loss: 0.08863568305969238
Epoch 173  Train loss: 0.08127570152282715  Val loss: 0.13387328365824067
Epoch 174
-------------------------------
Batch 1/64 loss: 0.0977904200553894
Batch 2/64 loss: 0.08921968936920166
Batch 3/64 loss: 0.10045802593231201
Batch 4/64 loss: 0.06594306230545044
Batch 5/64 loss: 0.10083943605422974
Batch 6/64 loss: 0.09709370136260986
Batch 7/64 loss: 0.06603574752807617
Batch 8/64 loss: 0.12259721755981445
Batch 9/64 loss: 0.09741145372390747
Batch 10/64 loss: 0.05770862102508545
Batch 11/64 loss: 0.06961715221405029
Batch 12/64 loss: 0.07505661249160767
Batch 13/64 loss: 0.09993046522140503
Batch 14/64 loss: 0.0828019380569458
Batch 15/64 loss: 0.09956550598144531
Batch 16/64 loss: 0.0891798734664917
Batch 17/64 loss: 0.08125686645507812
Batch 18/64 loss: 0.07249248027801514
Batch 19/64 loss: 0.06126672029495239
Batch 20/64 loss: 0.07001107931137085
Batch 21/64 loss: 0.08324825763702393
Batch 22/64 loss: 0.11208385229110718
Batch 23/64 loss: 0.08390700817108154
Batch 24/64 loss: 0.11900830268859863
Batch 25/64 loss: 0.07895684242248535
Batch 26/64 loss: 0.08672469854354858
Batch 27/64 loss: 0.05607402324676514
Batch 28/64 loss: 0.07372653484344482
Batch 29/64 loss: 0.045160114765167236
Batch 30/64 loss: 0.08687335252761841
Batch 31/64 loss: 0.09440195560455322
Batch 32/64 loss: 0.06580924987792969
Batch 33/64 loss: 0.06482964754104614
Batch 34/64 loss: 0.0628732442855835
Batch 35/64 loss: 0.08573830127716064
Batch 36/64 loss: 0.0770607590675354
Batch 37/64 loss: 0.09366846084594727
Batch 38/64 loss: 0.09034079313278198
Batch 39/64 loss: 0.08176529407501221
Batch 40/64 loss: 0.08317142724990845
Batch 41/64 loss: 0.06912505626678467
Batch 42/64 loss: 0.0689350962638855
Batch 43/64 loss: 0.050263166427612305
Batch 44/64 loss: 0.09258180856704712
Batch 45/64 loss: 0.09047544002532959
Batch 46/64 loss: 0.07712644338607788
Batch 47/64 loss: 0.06353074312210083
Batch 48/64 loss: 0.07970213890075684
Batch 49/64 loss: 0.08040893077850342
Batch 50/64 loss: 0.08929443359375
Batch 51/64 loss: 0.06534504890441895
Batch 52/64 loss: 0.0891539454460144
Batch 53/64 loss: 0.10112452507019043
Batch 54/64 loss: 0.06624555587768555
Batch 55/64 loss: 0.09421157836914062
Batch 56/64 loss: 0.0892593264579773
Batch 57/64 loss: 0.09045600891113281
Batch 58/64 loss: 0.1019485592842102
Batch 59/64 loss: 0.06322216987609863
Batch 60/64 loss: 0.08314388990402222
Batch 61/64 loss: 0.08266961574554443
Batch 62/64 loss: 0.0910799503326416
Batch 63/64 loss: 0.06761419773101807
Batch 64/64 loss: 0.08284544944763184
Epoch 174  Train loss: 0.08205097890367695  Val loss: 0.12822055714236913
Epoch 175
-------------------------------
Batch 1/64 loss: 0.0783379077911377
Batch 2/64 loss: 0.09533089399337769
Batch 3/64 loss: 0.08035087585449219
Batch 4/64 loss: 0.064461350440979
Batch 5/64 loss: 0.08944112062454224
Batch 6/64 loss: 0.05620372295379639
Batch 7/64 loss: 0.0861886739730835
Batch 8/64 loss: 0.0824439525604248
Batch 9/64 loss: 0.08574646711349487
Batch 10/64 loss: 0.07625830173492432
Batch 11/64 loss: 0.09205812215805054
Batch 12/64 loss: 0.08287042379379272
Batch 13/64 loss: 0.08471274375915527
Batch 14/64 loss: 0.11326152086257935
Batch 15/64 loss: 0.0951155424118042
Batch 16/64 loss: 0.08912187814712524
Batch 17/64 loss: 0.06225842237472534
Batch 18/64 loss: 0.06752127408981323
Batch 19/64 loss: 0.07370525598526001
Batch 20/64 loss: 0.06647598743438721
Batch 21/64 loss: 0.07400161027908325
Batch 22/64 loss: 0.09700667858123779
Batch 23/64 loss: 0.087990403175354
Batch 24/64 loss: 0.08138835430145264
Batch 25/64 loss: 0.07970470190048218
Batch 26/64 loss: 0.07922482490539551
Batch 27/64 loss: 0.06356978416442871
Batch 28/64 loss: 0.07884407043457031
Batch 29/64 loss: 0.07625395059585571
Batch 30/64 loss: 0.06990593671798706
Batch 31/64 loss: 0.1124996542930603
Batch 32/64 loss: 0.0815240740776062
Batch 33/64 loss: 0.0829237699508667
Batch 34/64 loss: 0.0763101577758789
Batch 35/64 loss: 0.06559562683105469
Batch 36/64 loss: 0.054946303367614746
Batch 37/64 loss: 0.07462459802627563
Batch 38/64 loss: 0.0842522382736206
Batch 39/64 loss: 0.09150266647338867
Batch 40/64 loss: 0.06664121150970459
Batch 41/64 loss: 0.07292443513870239
Batch 42/64 loss: 0.10065782070159912
Batch 43/64 loss: 0.12063217163085938
Batch 44/64 loss: 0.07911235094070435
Batch 45/64 loss: 0.08806896209716797
Batch 46/64 loss: 0.08388680219650269
Batch 47/64 loss: 0.08715391159057617
Batch 48/64 loss: 0.05310344696044922
Batch 49/64 loss: 0.08347046375274658
Batch 50/64 loss: 0.06519365310668945
Batch 51/64 loss: 0.07455408573150635
Batch 52/64 loss: 0.06491118669509888
Batch 53/64 loss: 0.05861771106719971
Batch 54/64 loss: 0.086434006690979
Batch 55/64 loss: 0.07447773218154907
Batch 56/64 loss: 0.09383535385131836
Batch 57/64 loss: 0.07251417636871338
Batch 58/64 loss: 0.10094982385635376
Batch 59/64 loss: 0.11672019958496094
Batch 60/64 loss: 0.09509062767028809
Batch 61/64 loss: 0.06955534219741821
Batch 62/64 loss: 0.08631312847137451
Batch 63/64 loss: 0.09584838151931763
Batch 64/64 loss: 0.046193480491638184
Epoch 175  Train loss: 0.08092934804804185  Val loss: 0.12839622001877354
Epoch 176
-------------------------------
Batch 1/64 loss: 0.05245727300643921
Batch 2/64 loss: 0.07379239797592163
Batch 3/64 loss: 0.09827089309692383
Batch 4/64 loss: 0.0676584243774414
Batch 5/64 loss: 0.05400878190994263
Batch 6/64 loss: 0.06952822208404541
Batch 7/64 loss: 0.07690918445587158
Batch 8/64 loss: 0.1015235185623169
Batch 9/64 loss: 0.06176424026489258
Batch 10/64 loss: 0.08250057697296143
Batch 11/64 loss: 0.0863579511642456
Batch 12/64 loss: 0.1114010214805603
Batch 13/64 loss: 0.08958834409713745
Batch 14/64 loss: 0.09298050403594971
Batch 15/64 loss: 0.08025956153869629
Batch 16/64 loss: 0.08189725875854492
Batch 17/64 loss: 0.07357949018478394
Batch 18/64 loss: 0.11438858509063721
Batch 19/64 loss: 0.06292253732681274
Batch 20/64 loss: 0.08828574419021606
Batch 21/64 loss: 0.07937276363372803
Batch 22/64 loss: 0.05789506435394287
Batch 23/64 loss: 0.05609554052352905
Batch 24/64 loss: 0.05963170528411865
Batch 25/64 loss: 0.1075749397277832
Batch 26/64 loss: 0.09592163562774658
Batch 27/64 loss: 0.07726925611495972
Batch 28/64 loss: 0.10444843769073486
Batch 29/64 loss: 0.08228540420532227
Batch 30/64 loss: 0.09647291898727417
Batch 31/64 loss: 0.06187838315963745
Batch 32/64 loss: 0.07335227727890015
Batch 33/64 loss: 0.09405213594436646
Batch 34/64 loss: 0.10227811336517334
Batch 35/64 loss: 0.07955491542816162
Batch 36/64 loss: 0.08842289447784424
Batch 37/64 loss: 0.06505721807479858
Batch 38/64 loss: 0.09837406873703003
Batch 39/64 loss: 0.0689842700958252
Batch 40/64 loss: 0.08430439233779907
Batch 41/64 loss: 0.06082433462142944
Batch 42/64 loss: 0.07669723033905029
Batch 43/64 loss: 0.0705675482749939
Batch 44/64 loss: 0.05460989475250244
Batch 45/64 loss: 0.10402441024780273
Batch 46/64 loss: 0.07681173086166382
Batch 47/64 loss: 0.10981214046478271
Batch 48/64 loss: 0.06291550397872925
Batch 49/64 loss: 0.07334238290786743
Batch 50/64 loss: 0.07367229461669922
Batch 51/64 loss: 0.09283715486526489
Batch 52/64 loss: 0.061975836753845215
Batch 53/64 loss: 0.04917633533477783
Batch 54/64 loss: 0.07295846939086914
Batch 55/64 loss: 0.09145539999008179
Batch 56/64 loss: 0.09173011779785156
Batch 57/64 loss: 0.09709876775741577
Batch 58/64 loss: 0.11549645662307739
Batch 59/64 loss: 0.08255726099014282
Batch 60/64 loss: 0.07442563772201538
Batch 61/64 loss: 0.09626257419586182
Batch 62/64 loss: 0.08790558576583862
Batch 63/64 loss: 0.08202755451202393
Batch 64/64 loss: 0.06672364473342896
Epoch 176  Train loss: 0.08098083454019883  Val loss: 0.1307374212340391
Epoch 177
-------------------------------
Batch 1/64 loss: 0.09249317646026611
Batch 2/64 loss: 0.056235671043395996
Batch 3/64 loss: 0.059207141399383545
Batch 4/64 loss: 0.07823383808135986
Batch 5/64 loss: 0.07481241226196289
Batch 6/64 loss: 0.08957517147064209
Batch 7/64 loss: 0.07685708999633789
Batch 8/64 loss: 0.06711339950561523
Batch 9/64 loss: 0.07817256450653076
Batch 10/64 loss: 0.08079218864440918
Batch 11/64 loss: 0.05910813808441162
Batch 12/64 loss: 0.06230086088180542
Batch 13/64 loss: 0.10281497240066528
Batch 14/64 loss: 0.07262438535690308
Batch 15/64 loss: 0.08460068702697754
Batch 16/64 loss: 0.08040034770965576
Batch 17/64 loss: 0.09647446870803833
Batch 18/64 loss: 0.07241964340209961
Batch 19/64 loss: 0.0635749101638794
Batch 20/64 loss: 0.1087222695350647
Batch 21/64 loss: 0.06446373462677002
Batch 22/64 loss: 0.05318182706832886
Batch 23/64 loss: 0.09295356273651123
Batch 24/64 loss: 0.09927195310592651
Batch 25/64 loss: 0.09861737489700317
Batch 26/64 loss: 0.10326987504959106
Batch 27/64 loss: 0.1164393424987793
Batch 28/64 loss: 0.07726538181304932
Batch 29/64 loss: 0.08317315578460693
Batch 30/64 loss: 0.0777844786643982
Batch 31/64 loss: 0.08447873592376709
Batch 32/64 loss: 0.09570080041885376
Batch 33/64 loss: 0.08108758926391602
Batch 34/64 loss: 0.08346909284591675
Batch 35/64 loss: 0.07695329189300537
Batch 36/64 loss: 0.09652990102767944
Batch 37/64 loss: 0.06571167707443237
Batch 38/64 loss: 0.0962904691696167
Batch 39/64 loss: 0.06150710582733154
Batch 40/64 loss: 0.09901392459869385
Batch 41/64 loss: 0.06155961751937866
Batch 42/64 loss: 0.08534067869186401
Batch 43/64 loss: 0.08450436592102051
Batch 44/64 loss: 0.0631866455078125
Batch 45/64 loss: 0.06743544340133667
Batch 46/64 loss: 0.07884550094604492
Batch 47/64 loss: 0.08918410539627075
Batch 48/64 loss: 0.07798463106155396
Batch 49/64 loss: 0.1155996322631836
Batch 50/64 loss: 0.07778871059417725
Batch 51/64 loss: 0.08812791109085083
Batch 52/64 loss: 0.08876001834869385
Batch 53/64 loss: 0.06887280941009521
Batch 54/64 loss: 0.08015286922454834
Batch 55/64 loss: 0.094806969165802
Batch 56/64 loss: 0.06842029094696045
Batch 57/64 loss: 0.08418166637420654
Batch 58/64 loss: 0.09164232015609741
Batch 59/64 loss: 0.06843757629394531
Batch 60/64 loss: 0.08876734972000122
Batch 61/64 loss: 0.0667726993560791
Batch 62/64 loss: 0.06744545698165894
Batch 63/64 loss: 0.10178160667419434
Batch 64/64 loss: 0.07078367471694946
Epoch 177  Train loss: 0.08119816850213443  Val loss: 0.13026658120433898
Epoch 178
-------------------------------
Batch 1/64 loss: 0.06936264038085938
Batch 2/64 loss: 0.09862911701202393
Batch 3/64 loss: 0.07140463590621948
Batch 4/64 loss: 0.0967450737953186
Batch 5/64 loss: 0.07036107778549194
Batch 6/64 loss: 0.08558845520019531
Batch 7/64 loss: 0.10459285974502563
Batch 8/64 loss: 0.08903777599334717
Batch 9/64 loss: 0.07708197832107544
Batch 10/64 loss: 0.08260542154312134
Batch 11/64 loss: 0.07676845788955688
Batch 12/64 loss: 0.07786500453948975
Batch 13/64 loss: 0.08791202306747437
Batch 14/64 loss: 0.09422099590301514
Batch 15/64 loss: 0.06321609020233154
Batch 16/64 loss: 0.08744090795516968
Batch 17/64 loss: 0.08164161443710327
Batch 18/64 loss: 0.0729418396949768
Batch 19/64 loss: 0.07049250602722168
Batch 20/64 loss: 0.09323155879974365
Batch 21/64 loss: 0.09210062026977539
Batch 22/64 loss: 0.08372223377227783
Batch 23/64 loss: 0.07076919078826904
Batch 24/64 loss: 0.09592223167419434
Batch 25/64 loss: 0.07980430126190186
Batch 26/64 loss: 0.11317664384841919
Batch 27/64 loss: 0.07860803604125977
Batch 28/64 loss: 0.0962827205657959
Batch 29/64 loss: 0.07005637884140015
Batch 30/64 loss: 0.08529633283615112
Batch 31/64 loss: 0.11026489734649658
Batch 32/64 loss: 0.06027781963348389
Batch 33/64 loss: 0.07822400331497192
Batch 34/64 loss: 0.0739678144454956
Batch 35/64 loss: 0.06561070680618286
Batch 36/64 loss: 0.10373282432556152
Batch 37/64 loss: 0.08400940895080566
Batch 38/64 loss: 0.08341372013092041
Batch 39/64 loss: 0.06058049201965332
Batch 40/64 loss: 0.07048225402832031
Batch 41/64 loss: 0.0734938383102417
Batch 42/64 loss: 0.06447052955627441
Batch 43/64 loss: 0.07919853925704956
Batch 44/64 loss: 0.08121776580810547
Batch 45/64 loss: 0.06322455406188965
Batch 46/64 loss: 0.07500749826431274
Batch 47/64 loss: 0.0785754919052124
Batch 48/64 loss: 0.09449851512908936
Batch 49/64 loss: 0.11934900283813477
Batch 50/64 loss: 0.07358592748641968
Batch 51/64 loss: 0.0762706995010376
Batch 52/64 loss: 0.09558761119842529
Batch 53/64 loss: 0.06683588027954102
Batch 54/64 loss: 0.07775169610977173
Batch 55/64 loss: 0.08538269996643066
Batch 56/64 loss: 0.08356499671936035
Batch 57/64 loss: 0.0677080750465393
Batch 58/64 loss: 0.0775112509727478
Batch 59/64 loss: 0.10760301351547241
Batch 60/64 loss: 0.0848994255065918
Batch 61/64 loss: 0.06541663408279419
Batch 62/64 loss: 0.06644034385681152
Batch 63/64 loss: 0.07457363605499268
Batch 64/64 loss: 0.0797119140625
Epoch 178  Train loss: 0.0815591251148897  Val loss: 0.1288505090061332
Epoch 179
-------------------------------
Batch 1/64 loss: 0.07574158906936646
Batch 2/64 loss: 0.06919717788696289
Batch 3/64 loss: 0.09705889225006104
Batch 4/64 loss: 0.06456238031387329
Batch 5/64 loss: 0.11437225341796875
Batch 6/64 loss: 0.06627017259597778
Batch 7/64 loss: 0.08722782135009766
Batch 8/64 loss: 0.07316064834594727
Batch 9/64 loss: 0.07276809215545654
Batch 10/64 loss: 0.07306444644927979
Batch 11/64 loss: 0.06774842739105225
Batch 12/64 loss: 0.09103161096572876
Batch 13/64 loss: 0.1018255352973938
Batch 14/64 loss: 0.06470108032226562
Batch 15/64 loss: 0.08212006092071533
Batch 16/64 loss: 0.09280812740325928
Batch 17/64 loss: 0.07769578695297241
Batch 18/64 loss: 0.09499305486679077
Batch 19/64 loss: 0.07456022500991821
Batch 20/64 loss: 0.057806551456451416
Batch 21/64 loss: 0.09600293636322021
Batch 22/64 loss: 0.07609963417053223
Batch 23/64 loss: 0.11746370792388916
Batch 24/64 loss: 0.08138340711593628
Batch 25/64 loss: 0.08532756567001343
Batch 26/64 loss: 0.09355860948562622
Batch 27/64 loss: 0.0808788537979126
Batch 28/64 loss: 0.06329762935638428
Batch 29/64 loss: 0.0932391881942749
Batch 30/64 loss: 0.09377503395080566
Batch 31/64 loss: 0.06546419858932495
Batch 32/64 loss: 0.08294039964675903
Batch 33/64 loss: 0.08987671136856079
Batch 34/64 loss: 0.06756210327148438
Batch 35/64 loss: 0.08118116855621338
Batch 36/64 loss: 0.054476261138916016
Batch 37/64 loss: 0.07100838422775269
Batch 38/64 loss: 0.05968970060348511
Batch 39/64 loss: 0.06535208225250244
Batch 40/64 loss: 0.09437292814254761
Batch 41/64 loss: 0.07014870643615723
Batch 42/64 loss: 0.07913285493850708
Batch 43/64 loss: 0.09186160564422607
Batch 44/64 loss: 0.11383914947509766
Batch 45/64 loss: 0.10316967964172363
Batch 46/64 loss: 0.06281888484954834
Batch 47/64 loss: 0.11503267288208008
Batch 48/64 loss: 0.05689561367034912
Batch 49/64 loss: 0.06216013431549072
Batch 50/64 loss: 0.08627629280090332
Batch 51/64 loss: 0.05321633815765381
Batch 52/64 loss: 0.1068187952041626
Batch 53/64 loss: 0.08625894784927368
Batch 54/64 loss: 0.09756976366043091
Batch 55/64 loss: 0.09262728691101074
Batch 56/64 loss: 0.08146607875823975
Batch 57/64 loss: 0.06107485294342041
Batch 58/64 loss: 0.0971078872680664
Batch 59/64 loss: 0.06026637554168701
Batch 60/64 loss: 0.0976792573928833
Batch 61/64 loss: 0.05868935585021973
Batch 62/64 loss: 0.06774604320526123
Batch 63/64 loss: 0.08431315422058105
Batch 64/64 loss: 0.09133797883987427
Epoch 179  Train loss: 0.08104059065089506  Val loss: 0.12753832319757782
Epoch 180
-------------------------------
Batch 1/64 loss: 0.1013604998588562
Batch 2/64 loss: 0.08752083778381348
Batch 3/64 loss: 0.07710611820220947
Batch 4/64 loss: 0.03951001167297363
Batch 5/64 loss: 0.0914297103881836
Batch 6/64 loss: 0.0872393250465393
Batch 7/64 loss: 0.0751301646232605
Batch 8/64 loss: 0.07789391279220581
Batch 9/64 loss: 0.044319748878479004
Batch 10/64 loss: 0.09223204851150513
Batch 11/64 loss: 0.08869129419326782
Batch 12/64 loss: 0.08401691913604736
Batch 13/64 loss: 0.0998043417930603
Batch 14/64 loss: 0.059665799140930176
Batch 15/64 loss: 0.08808779716491699
Batch 16/64 loss: 0.09687620401382446
Batch 17/64 loss: 0.07998120784759521
Batch 18/64 loss: 0.11675858497619629
Batch 19/64 loss: 0.0716962218284607
Batch 20/64 loss: 0.0863192081451416
Batch 21/64 loss: 0.09639245271682739
Batch 22/64 loss: 0.07607024908065796
Batch 23/64 loss: 0.09362435340881348
Batch 24/64 loss: 0.0704798698425293
Batch 25/64 loss: 0.08783143758773804
Batch 26/64 loss: 0.06283724308013916
Batch 27/64 loss: 0.11878097057342529
Batch 28/64 loss: 0.0684804916381836
Batch 29/64 loss: 0.07976984977722168
Batch 30/64 loss: 0.07940256595611572
Batch 31/64 loss: 0.10026168823242188
Batch 32/64 loss: 0.06711161136627197
Batch 33/64 loss: 0.06778883934020996
Batch 34/64 loss: 0.07938456535339355
Batch 35/64 loss: 0.08421355485916138
Batch 36/64 loss: 0.08460676670074463
Batch 37/64 loss: 0.07275426387786865
Batch 38/64 loss: 0.12117058038711548
Batch 39/64 loss: 0.036898255348205566
Batch 40/64 loss: 0.07114064693450928
Batch 41/64 loss: 0.08429920673370361
Batch 42/64 loss: 0.09738713502883911
Batch 43/64 loss: 0.09073007106781006
Batch 44/64 loss: 0.059873104095458984
Batch 45/64 loss: 0.07746994495391846
Batch 46/64 loss: 0.07642149925231934
Batch 47/64 loss: 0.08209919929504395
Batch 48/64 loss: 0.07427108287811279
Batch 49/64 loss: 0.08252739906311035
Batch 50/64 loss: 0.0545421838760376
Batch 51/64 loss: 0.11388802528381348
Batch 52/64 loss: 0.05982208251953125
Batch 53/64 loss: 0.0744180679321289
Batch 54/64 loss: 0.08855390548706055
Batch 55/64 loss: 0.06848251819610596
Batch 56/64 loss: 0.09236675500869751
Batch 57/64 loss: 0.07386845350265503
Batch 58/64 loss: 0.091888427734375
Batch 59/64 loss: 0.06544286012649536
Batch 60/64 loss: 0.04489409923553467
Batch 61/64 loss: 0.09772872924804688
Batch 62/64 loss: 0.08556270599365234
Batch 63/64 loss: 0.07775384187698364
Batch 64/64 loss: 0.07510817050933838
Epoch 180  Train loss: 0.08055313942479152  Val loss: 0.13006358458004458
Epoch 181
-------------------------------
Batch 1/64 loss: 0.07089567184448242
Batch 2/64 loss: 0.10955643653869629
Batch 3/64 loss: 0.06562858819961548
Batch 4/64 loss: 0.07426095008850098
Batch 5/64 loss: 0.0782284140586853
Batch 6/64 loss: 0.058898329734802246
Batch 7/64 loss: 0.08396732807159424
Batch 8/64 loss: 0.07607179880142212
Batch 9/64 loss: 0.10013526678085327
Batch 10/64 loss: 0.0842214822769165
Batch 11/64 loss: 0.07274794578552246
Batch 12/64 loss: 0.09585851430892944
Batch 13/64 loss: 0.08276474475860596
Batch 14/64 loss: 0.08311158418655396
Batch 15/64 loss: 0.10186994075775146
Batch 16/64 loss: 0.0749158263206482
Batch 17/64 loss: 0.05613982677459717
Batch 18/64 loss: 0.07113128900527954
Batch 19/64 loss: 0.07819169759750366
Batch 20/64 loss: 0.06375020742416382
Batch 21/64 loss: 0.07329630851745605
Batch 22/64 loss: 0.06800848245620728
Batch 23/64 loss: 0.09507197141647339
Batch 24/64 loss: 0.07499879598617554
Batch 25/64 loss: 0.08475428819656372
Batch 26/64 loss: 0.0790015459060669
Batch 27/64 loss: 0.0825798511505127
Batch 28/64 loss: 0.07390052080154419
Batch 29/64 loss: 0.07704275846481323
Batch 30/64 loss: 0.0810282826423645
Batch 31/64 loss: 0.1006155014038086
Batch 32/64 loss: 0.06532078981399536
Batch 33/64 loss: 0.06129896640777588
Batch 34/64 loss: 0.059114038944244385
Batch 35/64 loss: 0.1023334264755249
Batch 36/64 loss: 0.07474243640899658
Batch 37/64 loss: 0.0781370997428894
Batch 38/64 loss: 0.08106958866119385
Batch 39/64 loss: 0.11084073781967163
Batch 40/64 loss: 0.07656598091125488
Batch 41/64 loss: 0.08183920383453369
Batch 42/64 loss: 0.07673996686935425
Batch 43/64 loss: 0.0777508020401001
Batch 44/64 loss: 0.06729912757873535
Batch 45/64 loss: 0.07890647649765015
Batch 46/64 loss: 0.07167333364486694
Batch 47/64 loss: 0.09512239694595337
Batch 48/64 loss: 0.09625953435897827
Batch 49/64 loss: 0.06623220443725586
Batch 50/64 loss: 0.07522439956665039
Batch 51/64 loss: 0.10360729694366455
Batch 52/64 loss: 0.07541489601135254
Batch 53/64 loss: 0.09389567375183105
Batch 54/64 loss: 0.08068031072616577
Batch 55/64 loss: 0.047621190547943115
Batch 56/64 loss: 0.07631450891494751
Batch 57/64 loss: 0.07280713319778442
Batch 58/64 loss: 0.07737088203430176
Batch 59/64 loss: 0.09899383783340454
Batch 60/64 loss: 0.07568299770355225
Batch 61/64 loss: 0.1120377779006958
Batch 62/64 loss: 0.07291096448898315
Batch 63/64 loss: 0.08340543508529663
Batch 64/64 loss: 0.06307286024093628
Epoch 181  Train loss: 0.07979862292607626  Val loss: 0.13072029196519622
Epoch 182
-------------------------------
Batch 1/64 loss: 0.09205693006515503
Batch 2/64 loss: 0.055528223514556885
Batch 3/64 loss: 0.07367360591888428
Batch 4/64 loss: 0.07105666399002075
Batch 5/64 loss: 0.060851097106933594
Batch 6/64 loss: 0.07102692127227783
Batch 7/64 loss: 0.08607351779937744
Batch 8/64 loss: 0.06960982084274292
Batch 9/64 loss: 0.0746566653251648
Batch 10/64 loss: 0.07486152648925781
Batch 11/64 loss: 0.07846879959106445
Batch 12/64 loss: 0.06425964832305908
Batch 13/64 loss: 0.07109129428863525
Batch 14/64 loss: 0.08094191551208496
Batch 15/64 loss: 0.10156267881393433
Batch 16/64 loss: 0.06486833095550537
Batch 17/64 loss: 0.09205257892608643
Batch 18/64 loss: 0.052127182483673096
Batch 19/64 loss: 0.061325907707214355
Batch 20/64 loss: 0.06766766309738159
Batch 21/64 loss: 0.10919153690338135
Batch 22/64 loss: 0.08433926105499268
Batch 23/64 loss: 0.06641936302185059
Batch 24/64 loss: 0.09283512830734253
Batch 25/64 loss: 0.07625472545623779
Batch 26/64 loss: 0.08940654993057251
Batch 27/64 loss: 0.09027308225631714
Batch 28/64 loss: 0.07269287109375
Batch 29/64 loss: 0.06998699903488159
Batch 30/64 loss: 0.07448321580886841
Batch 31/64 loss: 0.09733206033706665
Batch 32/64 loss: 0.0820533037185669
Batch 33/64 loss: 0.0902024507522583
Batch 34/64 loss: 0.08070111274719238
Batch 35/64 loss: 0.10790431499481201
Batch 36/64 loss: 0.08244949579238892
Batch 37/64 loss: 0.08572113513946533
Batch 38/64 loss: 0.07180982828140259
Batch 39/64 loss: 0.0910719633102417
Batch 40/64 loss: 0.0953931212425232
Batch 41/64 loss: 0.09476494789123535
Batch 42/64 loss: 0.0656137466430664
Batch 43/64 loss: 0.08700478076934814
Batch 44/64 loss: 0.09249508380889893
Batch 45/64 loss: 0.06339514255523682
Batch 46/64 loss: 0.07756716012954712
Batch 47/64 loss: 0.1105273962020874
Batch 48/64 loss: 0.07078218460083008
Batch 49/64 loss: 0.07890284061431885
Batch 50/64 loss: 0.08236932754516602
Batch 51/64 loss: 0.07046180963516235
Batch 52/64 loss: 0.08007156848907471
Batch 53/64 loss: 0.11100447177886963
Batch 54/64 loss: 0.07942461967468262
Batch 55/64 loss: 0.0657339096069336
Batch 56/64 loss: 0.07454705238342285
Batch 57/64 loss: 0.08065754175186157
Batch 58/64 loss: 0.07381236553192139
Batch 59/64 loss: 0.0748867392539978
Batch 60/64 loss: 0.08447819948196411
Batch 61/64 loss: 0.07062262296676636
Batch 62/64 loss: 0.08155137300491333
Batch 63/64 loss: 0.06650364398956299
Batch 64/64 loss: 0.09351509809494019
Epoch 182  Train loss: 0.07971133087195602  Val loss: 0.1314190007566996
Epoch 183
-------------------------------
Batch 1/64 loss: 0.0754958987236023
Batch 2/64 loss: 0.11716121435165405
Batch 3/64 loss: 0.09071093797683716
Batch 4/64 loss: 0.050193846225738525
Batch 5/64 loss: 0.07413613796234131
Batch 6/64 loss: 0.06089317798614502
Batch 7/64 loss: 0.08414936065673828
Batch 8/64 loss: 0.07681524753570557
Batch 9/64 loss: 0.06124699115753174
Batch 10/64 loss: 0.09994500875473022
Batch 11/64 loss: 0.0870290994644165
Batch 12/64 loss: 0.08609902858734131
Batch 13/64 loss: 0.08140921592712402
Batch 14/64 loss: 0.08292412757873535
Batch 15/64 loss: 0.0797339677810669
Batch 16/64 loss: 0.0552142858505249
Batch 17/64 loss: 0.06496274471282959
Batch 18/64 loss: 0.10061275959014893
Batch 19/64 loss: 0.09932637214660645
Batch 20/64 loss: 0.08574223518371582
Batch 21/64 loss: 0.07529371976852417
Batch 22/64 loss: 0.10243058204650879
Batch 23/64 loss: 0.10565686225891113
Batch 24/64 loss: 0.07497888803482056
Batch 25/64 loss: 0.07576721906661987
Batch 26/64 loss: 0.07327073812484741
Batch 27/64 loss: 0.059754133224487305
Batch 28/64 loss: 0.06836694478988647
Batch 29/64 loss: 0.06680059432983398
Batch 30/64 loss: 0.061114370822906494
Batch 31/64 loss: 0.07824385166168213
Batch 32/64 loss: 0.08205235004425049
Batch 33/64 loss: 0.08201944828033447
Batch 34/64 loss: 0.07559370994567871
Batch 35/64 loss: 0.07478523254394531
Batch 36/64 loss: 0.06709790229797363
Batch 37/64 loss: 0.07271593809127808
Batch 38/64 loss: 0.0938461422920227
Batch 39/64 loss: 0.07494443655014038
Batch 40/64 loss: 0.10513430833816528
Batch 41/64 loss: 0.06053364276885986
Batch 42/64 loss: 0.07764685153961182
Batch 43/64 loss: 0.07289153337478638
Batch 44/64 loss: 0.06331479549407959
Batch 45/64 loss: 0.050855278968811035
Batch 46/64 loss: 0.08147978782653809
Batch 47/64 loss: 0.07641434669494629
Batch 48/64 loss: 0.06789344549179077
Batch 49/64 loss: 0.09780693054199219
Batch 50/64 loss: 0.0629425048828125
Batch 51/64 loss: 0.11023861169815063
Batch 52/64 loss: 0.0732845664024353
Batch 53/64 loss: 0.08955532312393188
Batch 54/64 loss: 0.06403088569641113
Batch 55/64 loss: 0.05741506814956665
Batch 56/64 loss: 0.06952559947967529
Batch 57/64 loss: 0.10259288549423218
Batch 58/64 loss: 0.06328856945037842
Batch 59/64 loss: 0.09239059686660767
Batch 60/64 loss: 0.0879291296005249
Batch 61/64 loss: 0.07380539178848267
Batch 62/64 loss: 0.0666770339012146
Batch 63/64 loss: 0.06397372484207153
Batch 64/64 loss: 0.05802077054977417
Epoch 183  Train loss: 0.07770464490441716  Val loss: 0.13066226366868952
Epoch 184
-------------------------------
Batch 1/64 loss: 0.0678483247756958
Batch 2/64 loss: 0.08187174797058105
Batch 3/64 loss: 0.07054340839385986
Batch 4/64 loss: 0.07776367664337158
Batch 5/64 loss: 0.06769311428070068
Batch 6/64 loss: 0.07660388946533203
Batch 7/64 loss: 0.07703697681427002
Batch 8/64 loss: 0.053696393966674805
Batch 9/64 loss: 0.06495815515518188
Batch 10/64 loss: 0.07439380884170532
Batch 11/64 loss: 0.08253192901611328
Batch 12/64 loss: 0.07397115230560303
Batch 13/64 loss: 0.06278485059738159
Batch 14/64 loss: 0.0657573938369751
Batch 15/64 loss: 0.08658707141876221
Batch 16/64 loss: 0.07297885417938232
Batch 17/64 loss: 0.05248606204986572
Batch 18/64 loss: 0.07991528511047363
Batch 19/64 loss: 0.08056354522705078
Batch 20/64 loss: 0.08829009532928467
Batch 21/64 loss: 0.0781855583190918
Batch 22/64 loss: 0.06828939914703369
Batch 23/64 loss: 0.07399415969848633
Batch 24/64 loss: 0.09182709455490112
Batch 25/64 loss: 0.08291405439376831
Batch 26/64 loss: 0.053859591484069824
Batch 27/64 loss: 0.09461033344268799
Batch 28/64 loss: 0.08039343357086182
Batch 29/64 loss: 0.07851159572601318
Batch 30/64 loss: 0.07265651226043701
Batch 31/64 loss: 0.08171731233596802
Batch 32/64 loss: 0.07412886619567871
Batch 33/64 loss: 0.10674124956130981
Batch 34/64 loss: 0.0645020604133606
Batch 35/64 loss: 0.06759113073348999
Batch 36/64 loss: 0.08918499946594238
Batch 37/64 loss: 0.07738196849822998
Batch 38/64 loss: 0.08038705587387085
Batch 39/64 loss: 0.053984761238098145
Batch 40/64 loss: 0.09850412607192993
Batch 41/64 loss: 0.0618019700050354
Batch 42/64 loss: 0.07150590419769287
Batch 43/64 loss: 0.10815680027008057
Batch 44/64 loss: 0.08011859655380249
Batch 45/64 loss: 0.08629715442657471
Batch 46/64 loss: 0.10391658544540405
Batch 47/64 loss: 0.07499980926513672
Batch 48/64 loss: 0.07175683975219727
Batch 49/64 loss: 0.0855451226234436
Batch 50/64 loss: 0.08180356025695801
Batch 51/64 loss: 0.0901411771774292
Batch 52/64 loss: 0.10631132125854492
Batch 53/64 loss: 0.09692263603210449
Batch 54/64 loss: 0.11286699771881104
Batch 55/64 loss: 0.07095825672149658
Batch 56/64 loss: 0.0715491771697998
Batch 57/64 loss: 0.08472877740859985
Batch 58/64 loss: 0.08628225326538086
Batch 59/64 loss: 0.08494096994400024
Batch 60/64 loss: 0.06825721263885498
Batch 61/64 loss: 0.0902714729309082
Batch 62/64 loss: 0.07846391201019287
Batch 63/64 loss: 0.06455743312835693
Batch 64/64 loss: 0.08385848999023438
Epoch 184  Train loss: 0.07878727538912904  Val loss: 0.12865899743902723
Epoch 185
-------------------------------
Batch 1/64 loss: 0.07181984186172485
Batch 2/64 loss: 0.07086127996444702
Batch 3/64 loss: 0.07097089290618896
Batch 4/64 loss: 0.07691645622253418
Batch 5/64 loss: 0.055183351039886475
Batch 6/64 loss: 0.06983518600463867
Batch 7/64 loss: 0.06556493043899536
Batch 8/64 loss: 0.06440556049346924
Batch 9/64 loss: 0.07429683208465576
Batch 10/64 loss: 0.10246741771697998
Batch 11/64 loss: 0.06572210788726807
Batch 12/64 loss: 0.06922018527984619
Batch 13/64 loss: 0.11478310823440552
Batch 14/64 loss: 0.07056236267089844
Batch 15/64 loss: 0.10336863994598389
Batch 16/64 loss: 0.0884125828742981
Batch 17/64 loss: 0.0715516209602356
Batch 18/64 loss: 0.05752265453338623
Batch 19/64 loss: 0.07806438207626343
Batch 20/64 loss: 0.08904528617858887
Batch 21/64 loss: 0.08267402648925781
Batch 22/64 loss: 0.07183313369750977
Batch 23/64 loss: 0.10290735960006714
Batch 24/64 loss: 0.09697836637496948
Batch 25/64 loss: 0.07942944765090942
Batch 26/64 loss: 0.04494607448577881
Batch 27/64 loss: 0.09442371129989624
Batch 28/64 loss: 0.06618237495422363
Batch 29/64 loss: 0.09831798076629639
Batch 30/64 loss: 0.08779263496398926
Batch 31/64 loss: 0.08166855573654175
Batch 32/64 loss: 0.09058934450149536
Batch 33/64 loss: 0.09891027212142944
Batch 34/64 loss: 0.1057007908821106
Batch 35/64 loss: 0.07756483554840088
Batch 36/64 loss: 0.08726197481155396
Batch 37/64 loss: 0.08850878477096558
Batch 38/64 loss: 0.07965308427810669
Batch 39/64 loss: 0.08224910497665405
Batch 40/64 loss: 0.07909142971038818
Batch 41/64 loss: 0.08941030502319336
Batch 42/64 loss: 0.07566094398498535
Batch 43/64 loss: 0.06853222846984863
Batch 44/64 loss: 0.08326351642608643
Batch 45/64 loss: 0.08035516738891602
Batch 46/64 loss: 0.09397238492965698
Batch 47/64 loss: 0.09879732131958008
Batch 48/64 loss: 0.0705987811088562
Batch 49/64 loss: 0.08677399158477783
Batch 50/64 loss: 0.054166197776794434
Batch 51/64 loss: 0.07462072372436523
Batch 52/64 loss: 0.08019393682479858
Batch 53/64 loss: 0.07548278570175171
Batch 54/64 loss: 0.0825115442276001
Batch 55/64 loss: 0.08112823963165283
Batch 56/64 loss: 0.045792460441589355
Batch 57/64 loss: 0.05954623222351074
Batch 58/64 loss: 0.07439970970153809
Batch 59/64 loss: 0.07174664735794067
Batch 60/64 loss: 0.05198150873184204
Batch 61/64 loss: 0.06967216730117798
Batch 62/64 loss: 0.07716357707977295
Batch 63/64 loss: 0.08115983009338379
Batch 64/64 loss: 0.07326948642730713
Epoch 185  Train loss: 0.07857474953520532  Val loss: 0.12949159423919887
Epoch 186
-------------------------------
Batch 1/64 loss: 0.0874829888343811
Batch 2/64 loss: 0.048108696937561035
Batch 3/64 loss: 0.08302193880081177
Batch 4/64 loss: 0.10205405950546265
Batch 5/64 loss: 0.07174694538116455
Batch 6/64 loss: 0.08701759576797485
Batch 7/64 loss: 0.0535162091255188
Batch 8/64 loss: 0.04590713977813721
Batch 9/64 loss: 0.07377469539642334
Batch 10/64 loss: 0.08169138431549072
Batch 11/64 loss: 0.07670724391937256
Batch 12/64 loss: 0.051095426082611084
Batch 13/64 loss: 0.04605764150619507
Batch 14/64 loss: 0.0782555341720581
Batch 15/64 loss: 0.118865966796875
Batch 16/64 loss: 0.0769960880279541
Batch 17/64 loss: 0.06799721717834473
Batch 18/64 loss: 0.0699964165687561
Batch 19/64 loss: 0.09764641523361206
Batch 20/64 loss: 0.1058686375617981
Batch 21/64 loss: 0.08192789554595947
Batch 22/64 loss: 0.09134316444396973
Batch 23/64 loss: 0.06902307271957397
Batch 24/64 loss: 0.05979436635971069
Batch 25/64 loss: 0.07430434226989746
Batch 26/64 loss: 0.06432312726974487
Batch 27/64 loss: 0.06492650508880615
Batch 28/64 loss: 0.05046802759170532
Batch 29/64 loss: 0.09929037094116211
Batch 30/64 loss: 0.09940731525421143
Batch 31/64 loss: 0.09189033508300781
Batch 32/64 loss: 0.05742537975311279
Batch 33/64 loss: 0.09730648994445801
Batch 34/64 loss: 0.07780277729034424
Batch 35/64 loss: 0.06798011064529419
Batch 36/64 loss: 0.07448863983154297
Batch 37/64 loss: 0.06807059049606323
Batch 38/64 loss: 0.08124125003814697
Batch 39/64 loss: 0.07123106718063354
Batch 40/64 loss: 0.09801745414733887
Batch 41/64 loss: 0.09302210807800293
Batch 42/64 loss: 0.059542179107666016
Batch 43/64 loss: 0.08266723155975342
Batch 44/64 loss: 0.0835217833518982
Batch 45/64 loss: 0.09604108333587646
Batch 46/64 loss: 0.06725949048995972
Batch 47/64 loss: 0.08076214790344238
Batch 48/64 loss: 0.0980297327041626
Batch 49/64 loss: 0.07976198196411133
Batch 50/64 loss: 0.09155857563018799
Batch 51/64 loss: 0.05958092212677002
Batch 52/64 loss: 0.0895954966545105
Batch 53/64 loss: 0.058508217334747314
Batch 54/64 loss: 0.08779841661453247
Batch 55/64 loss: 0.09520256519317627
Batch 56/64 loss: 0.09983325004577637
Batch 57/64 loss: 0.06750679016113281
Batch 58/64 loss: 0.08030116558074951
Batch 59/64 loss: 0.08986186981201172
Batch 60/64 loss: 0.06332039833068848
Batch 61/64 loss: 0.07049083709716797
Batch 62/64 loss: 0.08509504795074463
Batch 63/64 loss: 0.09233033657073975
Batch 64/64 loss: 0.06502705812454224
Epoch 186  Train loss: 0.0781871755917867  Val loss: 0.1271263379411599
Saving best model, epoch: 186
Epoch 187
-------------------------------
Batch 1/64 loss: 0.09145247936248779
Batch 2/64 loss: 0.07013797760009766
Batch 3/64 loss: 0.06853985786437988
Batch 4/64 loss: 0.08986687660217285
Batch 5/64 loss: 0.07836520671844482
Batch 6/64 loss: 0.05601394176483154
Batch 7/64 loss: 0.08772218227386475
Batch 8/64 loss: 0.06117904186248779
Batch 9/64 loss: 0.07405608892440796
Batch 10/64 loss: 0.04592132568359375
Batch 11/64 loss: 0.07652312517166138
Batch 12/64 loss: 0.07233905792236328
Batch 13/64 loss: 0.08435583114624023
Batch 14/64 loss: 0.07213646173477173
Batch 15/64 loss: 0.08935695886611938
Batch 16/64 loss: 0.07178086042404175
Batch 17/64 loss: 0.09824371337890625
Batch 18/64 loss: 0.059227585792541504
Batch 19/64 loss: 0.059475481510162354
Batch 20/64 loss: 0.07955914735794067
Batch 21/64 loss: 0.11110758781433105
Batch 22/64 loss: 0.06540602445602417
Batch 23/64 loss: 0.10035812854766846
Batch 24/64 loss: 0.06999629735946655
Batch 25/64 loss: 0.05632340908050537
Batch 26/64 loss: 0.07808995246887207
Batch 27/64 loss: 0.07221239805221558
Batch 28/64 loss: 0.07564014196395874
Batch 29/64 loss: 0.07625305652618408
Batch 30/64 loss: 0.09754502773284912
Batch 31/64 loss: 0.04157215356826782
Batch 32/64 loss: 0.10001003742218018
Batch 33/64 loss: 0.04794687032699585
Batch 34/64 loss: 0.0948668122291565
Batch 35/64 loss: 0.0823412537574768
Batch 36/64 loss: 0.07142877578735352
Batch 37/64 loss: 0.08566009998321533
Batch 38/64 loss: 0.06779086589813232
Batch 39/64 loss: 0.08208304643630981
Batch 40/64 loss: 0.07291597127914429
Batch 41/64 loss: 0.08481103181838989
Batch 42/64 loss: 0.08862948417663574
Batch 43/64 loss: 0.03593164682388306
Batch 44/64 loss: 0.03978031873703003
Batch 45/64 loss: 0.08226114511489868
Batch 46/64 loss: 0.08276695013046265
Batch 47/64 loss: 0.07782971858978271
Batch 48/64 loss: 0.05762600898742676
Batch 49/64 loss: 0.09350597858428955
Batch 50/64 loss: 0.0860518217086792
Batch 51/64 loss: 0.09512573480606079
Batch 52/64 loss: 0.08550912141799927
Batch 53/64 loss: 0.055308401584625244
Batch 54/64 loss: 0.07467252016067505
Batch 55/64 loss: 0.0986945629119873
Batch 56/64 loss: 0.08029389381408691
Batch 57/64 loss: 0.06617748737335205
Batch 58/64 loss: 0.10706579685211182
Batch 59/64 loss: 0.09437412023544312
Batch 60/64 loss: 0.0836341381072998
Batch 61/64 loss: 0.09613233804702759
Batch 62/64 loss: 0.07887232303619385
Batch 63/64 loss: 0.07292729616165161
Batch 64/64 loss: 0.08313429355621338
Epoch 187  Train loss: 0.07711582230586632  Val loss: 0.12822659380247503
Epoch 188
-------------------------------
Batch 1/64 loss: 0.03874450922012329
Batch 2/64 loss: 0.06461179256439209
Batch 3/64 loss: 0.07906287908554077
Batch 4/64 loss: 0.08360791206359863
Batch 5/64 loss: 0.10289204120635986
Batch 6/64 loss: 0.07679176330566406
Batch 7/64 loss: 0.07503390312194824
Batch 8/64 loss: 0.07182979583740234
Batch 9/64 loss: 0.08579021692276001
Batch 10/64 loss: 0.061569809913635254
Batch 11/64 loss: 0.07453393936157227
Batch 12/64 loss: 0.04970252513885498
Batch 13/64 loss: 0.07284653186798096
Batch 14/64 loss: 0.0809674859046936
Batch 15/64 loss: 0.07015228271484375
Batch 16/64 loss: 0.08872747421264648
Batch 17/64 loss: 0.09491872787475586
Batch 18/64 loss: 0.08066046237945557
Batch 19/64 loss: 0.0890125036239624
Batch 20/64 loss: 0.05996668338775635
Batch 21/64 loss: 0.07888686656951904
Batch 22/64 loss: 0.07192277908325195
Batch 23/64 loss: 0.08439230918884277
Batch 24/64 loss: 0.07291579246520996
Batch 25/64 loss: 0.07534706592559814
Batch 26/64 loss: 0.0810694694519043
Batch 27/64 loss: 0.09139949083328247
Batch 28/64 loss: 0.05896800756454468
Batch 29/64 loss: 0.07592213153839111
Batch 30/64 loss: 0.1140018105506897
Batch 31/64 loss: 0.08959043025970459
Batch 32/64 loss: 0.07413244247436523
Batch 33/64 loss: 0.07769972085952759
Batch 34/64 loss: 0.07071530818939209
Batch 35/64 loss: 0.059752821922302246
Batch 36/64 loss: 0.0728573203086853
Batch 37/64 loss: 0.05893969535827637
Batch 38/64 loss: 0.07302284240722656
Batch 39/64 loss: 0.0497206449508667
Batch 40/64 loss: 0.05911046266555786
Batch 41/64 loss: 0.06408035755157471
Batch 42/64 loss: 0.07657051086425781
Batch 43/64 loss: 0.08350628614425659
Batch 44/64 loss: 0.07770693302154541
Batch 45/64 loss: 0.0569072961807251
Batch 46/64 loss: 0.07724255323410034
Batch 47/64 loss: 0.08987975120544434
Batch 48/64 loss: 0.06548541784286499
Batch 49/64 loss: 0.0720321536064148
Batch 50/64 loss: 0.06374907493591309
Batch 51/64 loss: 0.0692911148071289
Batch 52/64 loss: 0.05593216419219971
Batch 53/64 loss: 0.0841706395149231
Batch 54/64 loss: 0.10081213712692261
Batch 55/64 loss: 0.09001564979553223
Batch 56/64 loss: 0.07172739505767822
Batch 57/64 loss: 0.05975407361984253
Batch 58/64 loss: 0.07667207717895508
Batch 59/64 loss: 0.08173692226409912
Batch 60/64 loss: 0.09149467945098877
Batch 61/64 loss: 0.09297192096710205
Batch 62/64 loss: 0.05072903633117676
Batch 63/64 loss: 0.09647536277770996
Batch 64/64 loss: 0.09957295656204224
Epoch 188  Train loss: 0.0755359039587133  Val loss: 0.12730792297940074
Epoch 189
-------------------------------
Batch 1/64 loss: 0.07490485906600952
Batch 2/64 loss: 0.06491100788116455
Batch 3/64 loss: 0.04746204614639282
Batch 4/64 loss: 0.06981790065765381
Batch 5/64 loss: 0.06197172403335571
Batch 6/64 loss: 0.08887410163879395
Batch 7/64 loss: 0.06735026836395264
Batch 8/64 loss: 0.05313694477081299
Batch 9/64 loss: 0.07419145107269287
Batch 10/64 loss: 0.07614541053771973
Batch 11/64 loss: 0.060477614402770996
Batch 12/64 loss: 0.07625973224639893
Batch 13/64 loss: 0.07791054248809814
Batch 14/64 loss: 0.04154837131500244
Batch 15/64 loss: 0.08293837308883667
Batch 16/64 loss: 0.07392704486846924
Batch 17/64 loss: 0.06582874059677124
Batch 18/64 loss: 0.0654560923576355
Batch 19/64 loss: 0.0580979585647583
Batch 20/64 loss: 0.08048707246780396
Batch 21/64 loss: 0.0835874080657959
Batch 22/64 loss: 0.06757521629333496
Batch 23/64 loss: 0.07280147075653076
Batch 24/64 loss: 0.07037800550460815
Batch 25/64 loss: 0.07749837636947632
Batch 26/64 loss: 0.07189375162124634
Batch 27/64 loss: 0.09671992063522339
Batch 28/64 loss: 0.08653473854064941
Batch 29/64 loss: 0.09757065773010254
Batch 30/64 loss: 0.08311396837234497
Batch 31/64 loss: 0.07814675569534302
Batch 32/64 loss: 0.06089121103286743
Batch 33/64 loss: 0.10226577520370483
Batch 34/64 loss: 0.06481361389160156
Batch 35/64 loss: 0.10449439287185669
Batch 36/64 loss: 0.07808464765548706
Batch 37/64 loss: 0.058103859424591064
Batch 38/64 loss: 0.08641338348388672
Batch 39/64 loss: 0.06348633766174316
Batch 40/64 loss: 0.09383344650268555
Batch 41/64 loss: 0.09254330396652222
Batch 42/64 loss: 0.09171563386917114
Batch 43/64 loss: 0.05830496549606323
Batch 44/64 loss: 0.0876665711402893
Batch 45/64 loss: 0.07358717918395996
Batch 46/64 loss: 0.05925565958023071
Batch 47/64 loss: 0.07808023691177368
Batch 48/64 loss: 0.09379088878631592
Batch 49/64 loss: 0.09175485372543335
Batch 50/64 loss: 0.10224735736846924
Batch 51/64 loss: 0.0878865122795105
Batch 52/64 loss: 0.08661890029907227
Batch 53/64 loss: 0.07125341892242432
Batch 54/64 loss: 0.07775455713272095
Batch 55/64 loss: 0.09224951267242432
Batch 56/64 loss: 0.09233027696609497
Batch 57/64 loss: 0.06984460353851318
Batch 58/64 loss: 0.08001381158828735
Batch 59/64 loss: 0.08727151155471802
Batch 60/64 loss: 0.0769241452217102
Batch 61/64 loss: 0.08705407381057739
Batch 62/64 loss: 0.07308733463287354
Batch 63/64 loss: 0.06411433219909668
Batch 64/64 loss: 0.09531581401824951
Epoch 189  Train loss: 0.07696848149393119  Val loss: 0.13133701467022454
Epoch 190
-------------------------------
Batch 1/64 loss: 0.07805776596069336
Batch 2/64 loss: 0.07831579446792603
Batch 3/64 loss: 0.08962351083755493
Batch 4/64 loss: 0.05791163444519043
Batch 5/64 loss: 0.07006561756134033
Batch 6/64 loss: 0.07759928703308105
Batch 7/64 loss: 0.09063100814819336
Batch 8/64 loss: 0.07290613651275635
Batch 9/64 loss: 0.09029293060302734
Batch 10/64 loss: 0.0664827823638916
Batch 11/64 loss: 0.09242206811904907
Batch 12/64 loss: 0.09276264905929565
Batch 13/64 loss: 0.06717729568481445
Batch 14/64 loss: 0.09122425317764282
Batch 15/64 loss: 0.08356446027755737
Batch 16/64 loss: 0.08784914016723633
Batch 17/64 loss: 0.07909369468688965
Batch 18/64 loss: 0.10782855749130249
Batch 19/64 loss: 0.08615672588348389
Batch 20/64 loss: 0.06174612045288086
Batch 21/64 loss: 0.05665355920791626
Batch 22/64 loss: 0.08041340112686157
Batch 23/64 loss: 0.08652597665786743
Batch 24/64 loss: 0.04396486282348633
Batch 25/64 loss: 0.07383942604064941
Batch 26/64 loss: 0.07978135347366333
Batch 27/64 loss: 0.10190969705581665
Batch 28/64 loss: 0.04678785800933838
Batch 29/64 loss: 0.04108935594558716
Batch 30/64 loss: 0.06515610218048096
Batch 31/64 loss: 0.06934922933578491
Batch 32/64 loss: 0.06573307514190674
Batch 33/64 loss: 0.04483234882354736
Batch 34/64 loss: 0.09190034866333008
Batch 35/64 loss: 0.0858011245727539
Batch 36/64 loss: 0.08069443702697754
Batch 37/64 loss: 0.055352866649627686
Batch 38/64 loss: 0.07229852676391602
Batch 39/64 loss: 0.08540433645248413
Batch 40/64 loss: 0.07147407531738281
Batch 41/64 loss: 0.09620410203933716
Batch 42/64 loss: 0.10208326578140259
Batch 43/64 loss: 0.07594150304794312
Batch 44/64 loss: 0.08515667915344238
Batch 45/64 loss: 0.08035969734191895
Batch 46/64 loss: 0.07454919815063477
Batch 47/64 loss: 0.07197999954223633
Batch 48/64 loss: 0.08554583787918091
Batch 49/64 loss: 0.07624995708465576
Batch 50/64 loss: 0.10140299797058105
Batch 51/64 loss: 0.077309250831604
Batch 52/64 loss: 0.07721143960952759
Batch 53/64 loss: 0.08835649490356445
Batch 54/64 loss: 0.08030891418457031
Batch 55/64 loss: 0.08170360326766968
Batch 56/64 loss: 0.08202177286148071
Batch 57/64 loss: 0.055695295333862305
Batch 58/64 loss: 0.06759297847747803
Batch 59/64 loss: 0.061338186264038086
Batch 60/64 loss: 0.06247437000274658
Batch 61/64 loss: 0.07631772756576538
Batch 62/64 loss: 0.05552583932876587
Batch 63/64 loss: 0.073372483253479
Batch 64/64 loss: 0.06135982275009155
Epoch 190  Train loss: 0.0761630565512414  Val loss: 0.12613858369617528
Saving best model, epoch: 190
Epoch 191
-------------------------------
Batch 1/64 loss: 0.08739817142486572
Batch 2/64 loss: 0.07972168922424316
Batch 3/64 loss: 0.06536209583282471
Batch 4/64 loss: 0.057902634143829346
Batch 5/64 loss: 0.09487557411193848
Batch 6/64 loss: 0.046002745628356934
Batch 7/64 loss: 0.04139143228530884
Batch 8/64 loss: 0.10269403457641602
Batch 9/64 loss: 0.06954127550125122
Batch 10/64 loss: 0.09081381559371948
Batch 11/64 loss: 0.06715148687362671
Batch 12/64 loss: 0.08208268880844116
Batch 13/64 loss: 0.06702160835266113
Batch 14/64 loss: 0.07827538251876831
Batch 15/64 loss: 0.07632225751876831
Batch 16/64 loss: 0.0744829773902893
Batch 17/64 loss: 0.06420844793319702
Batch 18/64 loss: 0.06773340702056885
Batch 19/64 loss: 0.1004112958908081
Batch 20/64 loss: 0.10128629207611084
Batch 21/64 loss: 0.09173190593719482
Batch 22/64 loss: 0.0669320821762085
Batch 23/64 loss: 0.09441936016082764
Batch 24/64 loss: 0.0950433611869812
Batch 25/64 loss: 0.061727046966552734
Batch 26/64 loss: 0.07126277685165405
Batch 27/64 loss: 0.09389698505401611
Batch 28/64 loss: 0.09084188938140869
Batch 29/64 loss: 0.10133260488510132
Batch 30/64 loss: 0.055904388427734375
Batch 31/64 loss: 0.0870928168296814
Batch 32/64 loss: 0.08454561233520508
Batch 33/64 loss: 0.0714184045791626
Batch 34/64 loss: 0.10690909624099731
Batch 35/64 loss: 0.08497214317321777
Batch 36/64 loss: 0.059870123863220215
Batch 37/64 loss: 0.0774182677268982
Batch 38/64 loss: 0.06723874807357788
Batch 39/64 loss: 0.07620882987976074
Batch 40/64 loss: 0.07339531183242798
Batch 41/64 loss: 0.08807778358459473
Batch 42/64 loss: 0.0667981505393982
Batch 43/64 loss: 0.06717884540557861
Batch 44/64 loss: 0.07116854190826416
Batch 45/64 loss: 0.05112862586975098
Batch 46/64 loss: 0.09081995487213135
Batch 47/64 loss: 0.08036965131759644
Batch 48/64 loss: 0.08242714405059814
Batch 49/64 loss: 0.05771440267562866
Batch 50/64 loss: 0.08362036943435669
Batch 51/64 loss: 0.06775736808776855
Batch 52/64 loss: 0.05976533889770508
Batch 53/64 loss: 0.06710433959960938
Batch 54/64 loss: 0.04180663824081421
Batch 55/64 loss: 0.07062315940856934
Batch 56/64 loss: 0.07400619983673096
Batch 57/64 loss: 0.059855103492736816
Batch 58/64 loss: 0.12541669607162476
Batch 59/64 loss: 0.07597160339355469
Batch 60/64 loss: 0.07392764091491699
Batch 61/64 loss: 0.06549602746963501
Batch 62/64 loss: 0.08176672458648682
Batch 63/64 loss: 0.08396947383880615
Batch 64/64 loss: 0.06493353843688965
Epoch 191  Train loss: 0.07627154518576229  Val loss: 0.13140983147309818
Epoch 192
-------------------------------
Batch 1/64 loss: 0.07077914476394653
Batch 2/64 loss: 0.043445706367492676
Batch 3/64 loss: 0.07642602920532227
Batch 4/64 loss: 0.0660596489906311
Batch 5/64 loss: 0.05671513080596924
Batch 6/64 loss: 0.09073227643966675
Batch 7/64 loss: 0.0692206621170044
Batch 8/64 loss: 0.06949746608734131
Batch 9/64 loss: 0.08037036657333374
Batch 10/64 loss: 0.09574377536773682
Batch 11/64 loss: 0.07455748319625854
Batch 12/64 loss: 0.07670891284942627
Batch 13/64 loss: 0.08041185140609741
Batch 14/64 loss: 0.056277334690093994
Batch 15/64 loss: 0.061484694480895996
Batch 16/64 loss: 0.08294981718063354
Batch 17/64 loss: 0.08407348394393921
Batch 18/64 loss: 0.08602404594421387
Batch 19/64 loss: 0.05500483512878418
Batch 20/64 loss: 0.08076256513595581
Batch 21/64 loss: 0.09338372945785522
Batch 22/64 loss: 0.041285037994384766
Batch 23/64 loss: 0.09020364284515381
Batch 24/64 loss: 0.04888808727264404
Batch 25/64 loss: 0.08814626932144165
Batch 26/64 loss: 0.08367311954498291
Batch 27/64 loss: 0.06997466087341309
Batch 28/64 loss: 0.0666341781616211
Batch 29/64 loss: 0.08329451084136963
Batch 30/64 loss: 0.073763906955719
Batch 31/64 loss: 0.07011288404464722
Batch 32/64 loss: 0.11653006076812744
Batch 33/64 loss: 0.07294225692749023
Batch 34/64 loss: 0.06850022077560425
Batch 35/64 loss: 0.08446919918060303
Batch 36/64 loss: 0.059934377670288086
Batch 37/64 loss: 0.06992697715759277
Batch 38/64 loss: 0.06931734085083008
Batch 39/64 loss: 0.07901793718338013
Batch 40/64 loss: 0.07542908191680908
Batch 41/64 loss: 0.07174009084701538
Batch 42/64 loss: 0.10563480854034424
Batch 43/64 loss: 0.0529097318649292
Batch 44/64 loss: 0.0832831859588623
Batch 45/64 loss: 0.08307045698165894
Batch 46/64 loss: 0.08389008045196533
Batch 47/64 loss: 0.06935811042785645
Batch 48/64 loss: 0.08222156763076782
Batch 49/64 loss: 0.07566368579864502
Batch 50/64 loss: 0.06002175807952881
Batch 51/64 loss: 0.09859776496887207
Batch 52/64 loss: 0.06624853610992432
Batch 53/64 loss: 0.07079482078552246
Batch 54/64 loss: 0.08010441064834595
Batch 55/64 loss: 0.06424278020858765
Batch 56/64 loss: 0.07654005289077759
Batch 57/64 loss: 0.08776670694351196
Batch 58/64 loss: 0.07682687044143677
Batch 59/64 loss: 0.05866348743438721
Batch 60/64 loss: 0.1072620153427124
Batch 61/64 loss: 0.0665743350982666
Batch 62/64 loss: 0.07551372051239014
Batch 63/64 loss: 0.1029784083366394
Batch 64/64 loss: 0.08668851852416992
Epoch 192  Train loss: 0.07572700369591806  Val loss: 0.12835448611642897
Epoch 193
-------------------------------
Batch 1/64 loss: 0.0421140193939209
Batch 2/64 loss: 0.06744575500488281
Batch 3/64 loss: 0.10450458526611328
Batch 4/64 loss: 0.07953518629074097
Batch 5/64 loss: 0.05973261594772339
Batch 6/64 loss: 0.05695384740829468
Batch 7/64 loss: 0.10333383083343506
Batch 8/64 loss: 0.060313522815704346
Batch 9/64 loss: 0.053269028663635254
Batch 10/64 loss: 0.08142644166946411
Batch 11/64 loss: 0.05698293447494507
Batch 12/64 loss: 0.09280472993850708
Batch 13/64 loss: 0.05357837677001953
Batch 14/64 loss: 0.07267868518829346
Batch 15/64 loss: 0.09099525213241577
Batch 16/64 loss: 0.09213709831237793
Batch 17/64 loss: 0.08704507350921631
Batch 18/64 loss: 0.07060641050338745
Batch 19/64 loss: 0.07160258293151855
Batch 20/64 loss: 0.06839501857757568
Batch 21/64 loss: 0.06938761472702026
Batch 22/64 loss: 0.07502835988998413
Batch 23/64 loss: 0.08988356590270996
Batch 24/64 loss: 0.06341034173965454
Batch 25/64 loss: 0.07560867071151733
Batch 26/64 loss: 0.09238237142562866
Batch 27/64 loss: 0.05846434831619263
Batch 28/64 loss: 0.10495918989181519
Batch 29/64 loss: 0.07579153776168823
Batch 30/64 loss: 0.06909418106079102
Batch 31/64 loss: 0.07150322198867798
Batch 32/64 loss: 0.07302707433700562
Batch 33/64 loss: 0.0846300721168518
Batch 34/64 loss: 0.08014678955078125
Batch 35/64 loss: 0.08288311958312988
Batch 36/64 loss: 0.08817917108535767
Batch 37/64 loss: 0.06775349378585815
Batch 38/64 loss: 0.07422327995300293
Batch 39/64 loss: 0.10810971260070801
Batch 40/64 loss: 0.08215755224227905
Batch 41/64 loss: 0.07769882678985596
Batch 42/64 loss: 0.05524367094039917
Batch 43/64 loss: 0.04360556602478027
Batch 44/64 loss: 0.04081970453262329
Batch 45/64 loss: 0.08374351263046265
Batch 46/64 loss: 0.06172168254852295
Batch 47/64 loss: 0.06742876768112183
Batch 48/64 loss: 0.05869382619857788
Batch 49/64 loss: 0.062120020389556885
Batch 50/64 loss: 0.06536394357681274
Batch 51/64 loss: 0.08246362209320068
Batch 52/64 loss: 0.08177614212036133
Batch 53/64 loss: 0.10513162612915039
Batch 54/64 loss: 0.07737624645233154
Batch 55/64 loss: 0.0917930006980896
Batch 56/64 loss: 0.07618355751037598
Batch 57/64 loss: 0.060158610343933105
Batch 58/64 loss: 0.10264992713928223
Batch 59/64 loss: 0.05694764852523804
Batch 60/64 loss: 0.08336448669433594
Batch 61/64 loss: 0.05249720811843872
Batch 62/64 loss: 0.09859466552734375
Batch 63/64 loss: 0.1081233024597168
Batch 64/64 loss: 0.06853508949279785
Epoch 193  Train loss: 0.07524673798504998  Val loss: 0.1296189449906759
Epoch 194
-------------------------------
Batch 1/64 loss: 0.0746617317199707
Batch 2/64 loss: 0.07508057355880737
Batch 3/64 loss: 0.08123540878295898
Batch 4/64 loss: 0.08445274829864502
Batch 5/64 loss: 0.042685627937316895
Batch 6/64 loss: 0.08655363321304321
Batch 7/64 loss: 0.07726287841796875
Batch 8/64 loss: 0.0630374550819397
Batch 9/64 loss: 0.07059091329574585
Batch 10/64 loss: 0.06271547079086304
Batch 11/64 loss: 0.05673563480377197
Batch 12/64 loss: 0.06156754493713379
Batch 13/64 loss: 0.07319265604019165
Batch 14/64 loss: 0.07009339332580566
Batch 15/64 loss: 0.07654917240142822
Batch 16/64 loss: 0.06367635726928711
Batch 17/64 loss: 0.08827149868011475
Batch 18/64 loss: 0.06679022312164307
Batch 19/64 loss: 0.06992286443710327
Batch 20/64 loss: 0.08189922571182251
Batch 21/64 loss: 0.06382346153259277
Batch 22/64 loss: 0.08583050966262817
Batch 23/64 loss: 0.08226883411407471
Batch 24/64 loss: 0.07552909851074219
Batch 25/64 loss: 0.08405208587646484
Batch 26/64 loss: 0.09469842910766602
Batch 27/64 loss: 0.07116830348968506
Batch 28/64 loss: 0.0771896243095398
Batch 29/64 loss: 0.09417945146560669
Batch 30/64 loss: 0.07711005210876465
Batch 31/64 loss: 0.0907973051071167
Batch 32/64 loss: 0.0709030032157898
Batch 33/64 loss: 0.08584356307983398
Batch 34/64 loss: 0.08306825160980225
Batch 35/64 loss: 0.06977581977844238
Batch 36/64 loss: 0.07192027568817139
Batch 37/64 loss: 0.06361889839172363
Batch 38/64 loss: 0.0849452018737793
Batch 39/64 loss: 0.09414207935333252
Batch 40/64 loss: 0.0850517749786377
Batch 41/64 loss: 0.05869126319885254
Batch 42/64 loss: 0.09238314628601074
Batch 43/64 loss: 0.05878400802612305
Batch 44/64 loss: 0.06782007217407227
Batch 45/64 loss: 0.07804745435714722
Batch 46/64 loss: 0.07502502202987671
Batch 47/64 loss: 0.06493186950683594
Batch 48/64 loss: 0.05498439073562622
Batch 49/64 loss: 0.08121275901794434
Batch 50/64 loss: 0.06149333715438843
Batch 51/64 loss: 0.06025552749633789
Batch 52/64 loss: 0.04298049211502075
Batch 53/64 loss: 0.08067941665649414
Batch 54/64 loss: 0.062069058418273926
Batch 55/64 loss: 0.09787505865097046
Batch 56/64 loss: 0.08157449960708618
Batch 57/64 loss: 0.06690067052841187
Batch 58/64 loss: 0.059094786643981934
Batch 59/64 loss: 0.08362030982971191
Batch 60/64 loss: 0.10409462451934814
Batch 61/64 loss: 0.06345361471176147
Batch 62/64 loss: 0.06998801231384277
Batch 63/64 loss: 0.07786715030670166
Batch 64/64 loss: 0.05639803409576416
Epoch 194  Train loss: 0.07402378203822117  Val loss: 0.1280025245807425
Epoch 195
-------------------------------
Batch 1/64 loss: 0.06668788194656372
Batch 2/64 loss: 0.08120095729827881
Batch 3/64 loss: 0.08382797241210938
Batch 4/64 loss: 0.06741243600845337
Batch 5/64 loss: 0.06270372867584229
Batch 6/64 loss: 0.045327067375183105
Batch 7/64 loss: 0.05586522817611694
Batch 8/64 loss: 0.09069234132766724
Batch 9/64 loss: 0.09019869565963745
Batch 10/64 loss: 0.060080766677856445
Batch 11/64 loss: 0.07591617107391357
Batch 12/64 loss: 0.0912163257598877
Batch 13/64 loss: 0.06790131330490112
Batch 14/64 loss: 0.07751286029815674
Batch 15/64 loss: 0.07254993915557861
Batch 16/64 loss: 0.08021712303161621
Batch 17/64 loss: 0.06506621837615967
Batch 18/64 loss: 0.06327509880065918
Batch 19/64 loss: 0.07766234874725342
Batch 20/64 loss: 0.09423184394836426
Batch 21/64 loss: 0.08182346820831299
Batch 22/64 loss: 0.07831817865371704
Batch 23/64 loss: 0.08768671751022339
Batch 24/64 loss: 0.09049147367477417
Batch 25/64 loss: 0.08629655838012695
Batch 26/64 loss: 0.047239720821380615
Batch 27/64 loss: 0.06608736515045166
Batch 28/64 loss: 0.07544320821762085
Batch 29/64 loss: 0.06379461288452148
Batch 30/64 loss: 0.08027416467666626
Batch 31/64 loss: 0.10728299617767334
Batch 32/64 loss: 0.0634455680847168
Batch 33/64 loss: 0.07068610191345215
Batch 34/64 loss: 0.06735897064208984
Batch 35/64 loss: 0.0580328106880188
Batch 36/64 loss: 0.06111353635787964
Batch 37/64 loss: 0.07775431871414185
Batch 38/64 loss: 0.06561130285263062
Batch 39/64 loss: 0.09285145998001099
Batch 40/64 loss: 0.09523123502731323
Batch 41/64 loss: 0.08804470300674438
Batch 42/64 loss: 0.07237797975540161
Batch 43/64 loss: 0.08878028392791748
Batch 44/64 loss: 0.06540554761886597
Batch 45/64 loss: 0.07038182020187378
Batch 46/64 loss: 0.06500256061553955
Batch 47/64 loss: 0.06393933296203613
Batch 48/64 loss: 0.06884855031967163
Batch 49/64 loss: 0.059237122535705566
Batch 50/64 loss: 0.06012928485870361
Batch 51/64 loss: 0.0947008728981018
Batch 52/64 loss: 0.06702053546905518
Batch 53/64 loss: 0.07311147451400757
Batch 54/64 loss: 0.11152064800262451
Batch 55/64 loss: 0.07295000553131104
Batch 56/64 loss: 0.0917167067527771
Batch 57/64 loss: 0.06145429611206055
Batch 58/64 loss: 0.08616840839385986
Batch 59/64 loss: 0.06010329723358154
Batch 60/64 loss: 0.07222473621368408
Batch 61/64 loss: 0.101371169090271
Batch 62/64 loss: 0.093994140625
Batch 63/64 loss: 0.05357646942138672
Batch 64/64 loss: 0.08560311794281006
Epoch 195  Train loss: 0.07517854699901506  Val loss: 0.12592437894073957
Saving best model, epoch: 195
Epoch 196
-------------------------------
Batch 1/64 loss: 0.07251852750778198
Batch 2/64 loss: 0.066933274269104
Batch 3/64 loss: 0.09905087947845459
Batch 4/64 loss: 0.06814944744110107
Batch 5/64 loss: 0.07874679565429688
Batch 6/64 loss: 0.07995200157165527
Batch 7/64 loss: 0.10072880983352661
Batch 8/64 loss: 0.08707666397094727
Batch 9/64 loss: 0.08000767230987549
Batch 10/64 loss: 0.06205141544342041
Batch 11/64 loss: 0.06329959630966187
Batch 12/64 loss: 0.05400872230529785
Batch 13/64 loss: 0.08280766010284424
Batch 14/64 loss: 0.09003734588623047
Batch 15/64 loss: 0.08636653423309326
Batch 16/64 loss: 0.07193106412887573
Batch 17/64 loss: 0.08350145816802979
Batch 18/64 loss: 0.07723593711853027
Batch 19/64 loss: 0.09466606378555298
Batch 20/64 loss: 0.07438886165618896
Batch 21/64 loss: 0.07485902309417725
Batch 22/64 loss: 0.06268417835235596
Batch 23/64 loss: 0.051899850368499756
Batch 24/64 loss: 0.06261402368545532
Batch 25/64 loss: 0.0625423789024353
Batch 26/64 loss: 0.07446730136871338
Batch 27/64 loss: 0.07832419872283936
Batch 28/64 loss: 0.07908475399017334
Batch 29/64 loss: 0.05974334478378296
Batch 30/64 loss: 0.07883751392364502
Batch 31/64 loss: 0.04821741580963135
Batch 32/64 loss: 0.0713549256324768
Batch 33/64 loss: 0.0729406476020813
Batch 34/64 loss: 0.0842810869216919
Batch 35/64 loss: 0.07891100645065308
Batch 36/64 loss: 0.0784153938293457
Batch 37/64 loss: 0.06556761264801025
Batch 38/64 loss: 0.04770827293395996
Batch 39/64 loss: 0.05249077081680298
Batch 40/64 loss: 0.06883496046066284
Batch 41/64 loss: 0.05941128730773926
Batch 42/64 loss: 0.0690353512763977
Batch 43/64 loss: 0.08163321018218994
Batch 44/64 loss: 0.08935517072677612
Batch 45/64 loss: 0.07961368560791016
Batch 46/64 loss: 0.07292884588241577
Batch 47/64 loss: 0.06192803382873535
Batch 48/64 loss: 0.08390462398529053
Batch 49/64 loss: 0.09250080585479736
Batch 50/64 loss: 0.06781268119812012
Batch 51/64 loss: 0.07504904270172119
Batch 52/64 loss: 0.08108651638031006
Batch 53/64 loss: 0.08634209632873535
Batch 54/64 loss: 0.060455262660980225
Batch 55/64 loss: 0.07919365167617798
Batch 56/64 loss: 0.08932876586914062
Batch 57/64 loss: 0.05796051025390625
Batch 58/64 loss: 0.1062626838684082
Batch 59/64 loss: 0.09294962882995605
Batch 60/64 loss: 0.07074630260467529
Batch 61/64 loss: 0.09802818298339844
Batch 62/64 loss: 0.0765981674194336
Batch 63/64 loss: 0.06492996215820312
Batch 64/64 loss: 0.08966565132141113
Epoch 196  Train loss: 0.07516142901252298  Val loss: 0.12657758644766004
Epoch 197
-------------------------------
Batch 1/64 loss: 0.09330403804779053
Batch 2/64 loss: 0.09808635711669922
Batch 3/64 loss: 0.06529444456100464
Batch 4/64 loss: 0.07979750633239746
Batch 5/64 loss: 0.08154439926147461
Batch 6/64 loss: 0.0653030276298523
Batch 7/64 loss: 0.0844799280166626
Batch 8/64 loss: 0.06533390283584595
Batch 9/64 loss: 0.05668532848358154
Batch 10/64 loss: 0.07772254943847656
Batch 11/64 loss: 0.06010329723358154
Batch 12/64 loss: 0.0593334436416626
Batch 13/64 loss: 0.08057236671447754
Batch 14/64 loss: 0.07701742649078369
Batch 15/64 loss: 0.06110912561416626
Batch 16/64 loss: 0.09037363529205322
Batch 17/64 loss: 0.06103241443634033
Batch 18/64 loss: 0.09461146593093872
Batch 19/64 loss: 0.0810842514038086
Batch 20/64 loss: 0.07591366767883301
Batch 21/64 loss: 0.09755021333694458
Batch 22/64 loss: 0.06359678506851196
Batch 23/64 loss: 0.05281054973602295
Batch 24/64 loss: 0.08338803052902222
Batch 25/64 loss: 0.10065191984176636
Batch 26/64 loss: 0.0668153166770935
Batch 27/64 loss: 0.0513269305229187
Batch 28/64 loss: 0.07311743497848511
Batch 29/64 loss: 0.08105528354644775
Batch 30/64 loss: 0.0600665807723999
Batch 31/64 loss: 0.06464201211929321
Batch 32/64 loss: 0.08063936233520508
Batch 33/64 loss: 0.06464153528213501
Batch 34/64 loss: 0.06608039140701294
Batch 35/64 loss: 0.08263653516769409
Batch 36/64 loss: 0.09715485572814941
Batch 37/64 loss: 0.11103874444961548
Batch 38/64 loss: 0.10301381349563599
Batch 39/64 loss: 0.07341891527175903
Batch 40/64 loss: 0.0695188045501709
Batch 41/64 loss: 0.07688850164413452
Batch 42/64 loss: 0.08819538354873657
Batch 43/64 loss: 0.04444640874862671
Batch 44/64 loss: 0.09321683645248413
Batch 45/64 loss: 0.05210226774215698
Batch 46/64 loss: 0.10609579086303711
Batch 47/64 loss: 0.06043529510498047
Batch 48/64 loss: 0.08651900291442871
Batch 49/64 loss: 0.11203980445861816
Batch 50/64 loss: 0.06319183111190796
Batch 51/64 loss: 0.10344833135604858
Batch 52/64 loss: 0.056013405323028564
Batch 53/64 loss: 0.08386772871017456
Batch 54/64 loss: 0.06610596179962158
Batch 55/64 loss: 0.07553088665008545
Batch 56/64 loss: 0.07168459892272949
Batch 57/64 loss: 0.06906169652938843
Batch 58/64 loss: 0.0780450701713562
Batch 59/64 loss: 0.051727116107940674
Batch 60/64 loss: 0.04727029800415039
Batch 61/64 loss: 0.07493942975997925
Batch 62/64 loss: 0.053401827812194824
Batch 63/64 loss: 0.07338368892669678
Batch 64/64 loss: 0.07428532838821411
Epoch 197  Train loss: 0.075218694116555  Val loss: 0.13177780842863
Epoch 198
-------------------------------
Batch 1/64 loss: 0.053223252296447754
Batch 2/64 loss: 0.09174495935440063
Batch 3/64 loss: 0.07679253816604614
Batch 4/64 loss: 0.07245421409606934
Batch 5/64 loss: 0.10692185163497925
Batch 6/64 loss: 0.07267779111862183
Batch 7/64 loss: 0.08578497171401978
Batch 8/64 loss: 0.060630738735198975
Batch 9/64 loss: 0.07701325416564941
Batch 10/64 loss: 0.10162550210952759
Batch 11/64 loss: 0.06509166955947876
Batch 12/64 loss: 0.07418417930603027
Batch 13/64 loss: 0.03741341829299927
Batch 14/64 loss: 0.0845491886138916
Batch 15/64 loss: 0.0726933479309082
Batch 16/64 loss: 0.09188640117645264
Batch 17/64 loss: 0.06865251064300537
Batch 18/64 loss: 0.05738413333892822
Batch 19/64 loss: 0.08972823619842529
Batch 20/64 loss: 0.08203274011611938
Batch 21/64 loss: 0.04776310920715332
Batch 22/64 loss: 0.051286399364471436
Batch 23/64 loss: 0.08443748950958252
Batch 24/64 loss: 0.04765212535858154
Batch 25/64 loss: 0.08239561319351196
Batch 26/64 loss: 0.1048898696899414
Batch 27/64 loss: 0.07057982683181763
Batch 28/64 loss: 0.0746806263923645
Batch 29/64 loss: 0.07259941101074219
Batch 30/64 loss: 0.05759298801422119
Batch 31/64 loss: 0.05195331573486328
Batch 32/64 loss: 0.09502965211868286
Batch 33/64 loss: 0.06759339570999146
Batch 34/64 loss: 0.060963988304138184
Batch 35/64 loss: 0.08234995603561401
Batch 36/64 loss: 0.07978904247283936
Batch 37/64 loss: 0.08850318193435669
Batch 38/64 loss: 0.07860732078552246
Batch 39/64 loss: 0.09544634819030762
Batch 40/64 loss: 0.08427190780639648
Batch 41/64 loss: 0.0566251277923584
Batch 42/64 loss: 0.09291058778762817
Batch 43/64 loss: 0.060012102127075195
Batch 44/64 loss: 0.07117867469787598
Batch 45/64 loss: 0.062207579612731934
Batch 46/64 loss: 0.06527984142303467
Batch 47/64 loss: 0.06149953603744507
Batch 48/64 loss: 0.09929227828979492
Batch 49/64 loss: 0.09214365482330322
Batch 50/64 loss: 0.07864642143249512
Batch 51/64 loss: 0.06856721639633179
Batch 52/64 loss: 0.0788758397102356
Batch 53/64 loss: 0.08535444736480713
Batch 54/64 loss: 0.07407987117767334
Batch 55/64 loss: 0.10931354761123657
Batch 56/64 loss: 0.0911949872970581
Batch 57/64 loss: 0.041146695613861084
Batch 58/64 loss: 0.06629258394241333
Batch 59/64 loss: 0.08725345134735107
Batch 60/64 loss: 0.0832439661026001
Batch 61/64 loss: 0.07551217079162598
Batch 62/64 loss: 0.04159122705459595
Batch 63/64 loss: 0.08359986543655396
Batch 64/64 loss: 0.08984321355819702
Epoch 198  Train loss: 0.07520116941601622  Val loss: 0.12910307969424323
Epoch 199
-------------------------------
Batch 1/64 loss: 0.06929165124893188
Batch 2/64 loss: 0.06678593158721924
Batch 3/64 loss: 0.0495377779006958
Batch 4/64 loss: 0.07720702886581421
Batch 5/64 loss: 0.08274674415588379
Batch 6/64 loss: 0.06951886415481567
Batch 7/64 loss: 0.05689162015914917
Batch 8/64 loss: 0.06263953447341919
Batch 9/64 loss: 0.059689104557037354
Batch 10/64 loss: 0.06495755910873413
Batch 11/64 loss: 0.04314565658569336
Batch 12/64 loss: 0.08288097381591797
Batch 13/64 loss: 0.05506610870361328
Batch 14/64 loss: 0.06533890962600708
Batch 15/64 loss: 0.05676734447479248
Batch 16/64 loss: 0.06792265176773071
Batch 17/64 loss: 0.06321513652801514
Batch 18/64 loss: 0.0962870717048645
Batch 19/64 loss: 0.06486588716506958
Batch 20/64 loss: 0.062327444553375244
Batch 21/64 loss: 0.08168065547943115
Batch 22/64 loss: 0.06767261028289795
Batch 23/64 loss: 0.11002612113952637
Batch 24/64 loss: 0.06210637092590332
Batch 25/64 loss: 0.06989175081253052
Batch 26/64 loss: 0.06515026092529297
Batch 27/64 loss: 0.055718064308166504
Batch 28/64 loss: 0.06886887550354004
Batch 29/64 loss: 0.0831594467163086
Batch 30/64 loss: 0.09770017862319946
Batch 31/64 loss: 0.0877542495727539
Batch 32/64 loss: 0.0902625322341919
Batch 33/64 loss: 0.11318707466125488
Batch 34/64 loss: 0.0777469277381897
Batch 35/64 loss: 0.06320327520370483
Batch 36/64 loss: 0.06744331121444702
Batch 37/64 loss: 0.08085393905639648
Batch 38/64 loss: 0.052038609981536865
Batch 39/64 loss: 0.06226801872253418
Batch 40/64 loss: 0.10036998987197876
Batch 41/64 loss: 0.08805221319198608
Batch 42/64 loss: 0.06033587455749512
Batch 43/64 loss: 0.07282274961471558
Batch 44/64 loss: 0.06915777921676636
Batch 45/64 loss: 0.07408034801483154
Batch 46/64 loss: 0.06214570999145508
Batch 47/64 loss: 0.0847163200378418
Batch 48/64 loss: 0.08236759901046753
Batch 49/64 loss: 0.07074964046478271
Batch 50/64 loss: 0.08808326721191406
Batch 51/64 loss: 0.07910710573196411
Batch 52/64 loss: 0.09923046827316284
Batch 53/64 loss: 0.0615462064743042
Batch 54/64 loss: 0.07968193292617798
Batch 55/64 loss: 0.08685600757598877
Batch 56/64 loss: 0.09631752967834473
Batch 57/64 loss: 0.05798286199569702
Batch 58/64 loss: 0.10077416896820068
Batch 59/64 loss: 0.05080437660217285
Batch 60/64 loss: 0.07587569952011108
Batch 61/64 loss: 0.09038800001144409
Batch 62/64 loss: 0.07373851537704468
Batch 63/64 loss: 0.10799586772918701
Batch 64/64 loss: 0.07958900928497314
Epoch 199  Train loss: 0.07445783942353491  Val loss: 0.12678838943697743
Epoch 200
-------------------------------
Batch 1/64 loss: 0.0754166841506958
Batch 2/64 loss: 0.04597753286361694
Batch 3/64 loss: 0.0828484296798706
Batch 4/64 loss: 0.0657963752746582
Batch 5/64 loss: 0.0474400520324707
Batch 6/64 loss: 0.0758904218673706
Batch 7/64 loss: 0.06868588924407959
Batch 8/64 loss: 0.07423526048660278
Batch 9/64 loss: 0.08440631628036499
Batch 10/64 loss: 0.05439174175262451
Batch 11/64 loss: 0.05835080146789551
Batch 12/64 loss: 0.07557690143585205
Batch 13/64 loss: 0.06333225965499878
Batch 14/64 loss: 0.09068965911865234
Batch 15/64 loss: 0.09219014644622803
Batch 16/64 loss: 0.09461754560470581
Batch 17/64 loss: 0.06762105226516724
Batch 18/64 loss: 0.06723374128341675
Batch 19/64 loss: 0.07709068059921265
Batch 20/64 loss: 0.0582200288772583
Batch 21/64 loss: 0.06600344181060791
Batch 22/64 loss: 0.06133627891540527
Batch 23/64 loss: 0.08186954259872437
Batch 24/64 loss: 0.0983055830001831
Batch 25/64 loss: 0.07521140575408936
Batch 26/64 loss: 0.07676738500595093
Batch 27/64 loss: 0.07126766443252563
Batch 28/64 loss: 0.08828693628311157
Batch 29/64 loss: 0.09883713722229004
Batch 30/64 loss: 0.08716702461242676
Batch 31/64 loss: 0.08303874731063843
Batch 32/64 loss: 0.05198180675506592
Batch 33/64 loss: 0.06615537405014038
Batch 34/64 loss: 0.06830763816833496
Batch 35/64 loss: 0.09785670042037964
Batch 36/64 loss: 0.06834185123443604
Batch 37/64 loss: 0.07616043090820312
Batch 38/64 loss: 0.08653855323791504
Batch 39/64 loss: 0.06818926334381104
Batch 40/64 loss: 0.09741979837417603
Batch 41/64 loss: 0.09102886915206909
Batch 42/64 loss: 0.057566821575164795
Batch 43/64 loss: 0.07743805646896362
Batch 44/64 loss: 0.05748027563095093
Batch 45/64 loss: 0.05409020185470581
Batch 46/64 loss: 0.08443468809127808
Batch 47/64 loss: 0.07998484373092651
Batch 48/64 loss: 0.06908977031707764
Batch 49/64 loss: 0.0594935417175293
Batch 50/64 loss: 0.062464356422424316
Batch 51/64 loss: 0.07524639368057251
Batch 52/64 loss: 0.1272066831588745
Batch 53/64 loss: 0.10401815176010132
Batch 54/64 loss: 0.06075155735015869
Batch 55/64 loss: 0.07309997081756592
Batch 56/64 loss: 0.08331555128097534
Batch 57/64 loss: 0.08100032806396484
Batch 58/64 loss: 0.06975555419921875
Batch 59/64 loss: 0.056785404682159424
Batch 60/64 loss: 0.07367062568664551
Batch 61/64 loss: 0.061362266540527344
Batch 62/64 loss: 0.07665842771530151
Batch 63/64 loss: 0.04651361703872681
Batch 64/64 loss: 0.06161177158355713
Epoch 200  Train loss: 0.07400343324623856  Val loss: 0.12479382691924105
Saving best model, epoch: 200
Epoch 201
-------------------------------
Batch 1/64 loss: 0.058330655097961426
Batch 2/64 loss: 0.06645762920379639
Batch 3/64 loss: 0.05238407850265503
Batch 4/64 loss: 0.04006224870681763
Batch 5/64 loss: 0.08640027046203613
Batch 6/64 loss: 0.07246935367584229
Batch 7/64 loss: 0.07560455799102783
Batch 8/64 loss: 0.06247663497924805
Batch 9/64 loss: 0.043339312076568604
Batch 10/64 loss: 0.05470949411392212
Batch 11/64 loss: 0.081817626953125
Batch 12/64 loss: 0.046478450298309326
Batch 13/64 loss: 0.07465958595275879
Batch 14/64 loss: 0.06554877758026123
Batch 15/64 loss: 0.06682020425796509
Batch 16/64 loss: 0.0808943510055542
Batch 17/64 loss: 0.0782623291015625
Batch 18/64 loss: 0.097098708152771
Batch 19/64 loss: 0.10260021686553955
Batch 20/64 loss: 0.04708367586135864
Batch 21/64 loss: 0.060029685497283936
Batch 22/64 loss: 0.09852969646453857
Batch 23/64 loss: 0.0735403299331665
Batch 24/64 loss: 0.056629300117492676
Batch 25/64 loss: 0.06094765663146973
Batch 26/64 loss: 0.08862870931625366
Batch 27/64 loss: 0.04331552982330322
Batch 28/64 loss: 0.07361042499542236
Batch 29/64 loss: 0.0872654914855957
Batch 30/64 loss: 0.06427782773971558
Batch 31/64 loss: 0.0596504807472229
Batch 32/64 loss: 0.06442791223526001
Batch 33/64 loss: 0.05524325370788574
Batch 34/64 loss: 0.0897449254989624
Batch 35/64 loss: 0.09091651439666748
Batch 36/64 loss: 0.08332771062850952
Batch 37/64 loss: 0.07859277725219727
Batch 38/64 loss: 0.08509331941604614
Batch 39/64 loss: 0.06272053718566895
Batch 40/64 loss: 0.05778062343597412
Batch 41/64 loss: 0.06343871355056763
Batch 42/64 loss: 0.08176648616790771
Batch 43/64 loss: 0.08082365989685059
Batch 44/64 loss: 0.10198318958282471
Batch 45/64 loss: 0.06350868940353394
Batch 46/64 loss: 0.0757373571395874
Batch 47/64 loss: 0.08321255445480347
Batch 48/64 loss: 0.08339107036590576
Batch 49/64 loss: 0.08221232891082764
Batch 50/64 loss: 0.06452041864395142
Batch 51/64 loss: 0.07887071371078491
Batch 52/64 loss: 0.05865585803985596
Batch 53/64 loss: 0.08205997943878174
Batch 54/64 loss: 0.07650774717330933
Batch 55/64 loss: 0.06347984075546265
Batch 56/64 loss: 0.05951017141342163
Batch 57/64 loss: 0.06090503931045532
Batch 58/64 loss: 0.09889602661132812
Batch 59/64 loss: 0.07503581047058105
Batch 60/64 loss: 0.08182555437088013
Batch 61/64 loss: 0.09045666456222534
Batch 62/64 loss: 0.07678389549255371
Batch 63/64 loss: 0.049601078033447266
Batch 64/64 loss: 0.09173440933227539
Epoch 201  Train loss: 0.0719961493623023  Val loss: 0.12844201113350204
Epoch 202
-------------------------------
Batch 1/64 loss: 0.0701025128364563
Batch 2/64 loss: 0.07893651723861694
Batch 3/64 loss: 0.08933693170547485
Batch 4/64 loss: 0.0892411470413208
Batch 5/64 loss: 0.07776898145675659
Batch 6/64 loss: 0.07277935743331909
Batch 7/64 loss: 0.07578212022781372
Batch 8/64 loss: 0.0632830262184143
Batch 9/64 loss: 0.06129300594329834
Batch 10/64 loss: 0.06734824180603027
Batch 11/64 loss: 0.06643015146255493
Batch 12/64 loss: 0.06981658935546875
Batch 13/64 loss: 0.057736873626708984
Batch 14/64 loss: 0.045855939388275146
Batch 15/64 loss: 0.07617843151092529
Batch 16/64 loss: 0.06728726625442505
Batch 17/64 loss: 0.08212655782699585
Batch 18/64 loss: 0.08435297012329102
Batch 19/64 loss: 0.06965595483779907
Batch 20/64 loss: 0.0695791244506836
Batch 21/64 loss: 0.08234065771102905
Batch 22/64 loss: 0.06967765092849731
Batch 23/64 loss: 0.056570470333099365
Batch 24/64 loss: 0.061514317989349365
Batch 25/64 loss: 0.06614500284194946
Batch 26/64 loss: 0.07231450080871582
Batch 27/64 loss: 0.08385062217712402
Batch 28/64 loss: 0.07077997922897339
Batch 29/64 loss: 0.04472625255584717
Batch 30/64 loss: 0.05499148368835449
Batch 31/64 loss: 0.07880574464797974
Batch 32/64 loss: 0.09453803300857544
Batch 33/64 loss: 0.08036744594573975
Batch 34/64 loss: 0.08566021919250488
Batch 35/64 loss: 0.10216295719146729
Batch 36/64 loss: 0.06569737195968628
Batch 37/64 loss: 0.0785144567489624
Batch 38/64 loss: 0.08097171783447266
Batch 39/64 loss: 0.06377953290939331
Batch 40/64 loss: 0.06199920177459717
Batch 41/64 loss: 0.07203483581542969
Batch 42/64 loss: 0.054811716079711914
Batch 43/64 loss: 0.06103450059890747
Batch 44/64 loss: 0.0653335452079773
Batch 45/64 loss: 0.06494027376174927
Batch 46/64 loss: 0.0875813364982605
Batch 47/64 loss: 0.08274966478347778
Batch 48/64 loss: 0.08342224359512329
Batch 49/64 loss: 0.09824246168136597
Batch 50/64 loss: 0.06314265727996826
Batch 51/64 loss: 0.058872222900390625
Batch 52/64 loss: 0.06404435634613037
Batch 53/64 loss: 0.12171471118927002
Batch 54/64 loss: 0.058719515800476074
Batch 55/64 loss: 0.06948667764663696
Batch 56/64 loss: 0.0649714469909668
Batch 57/64 loss: 0.049578309059143066
Batch 58/64 loss: 0.07026898860931396
Batch 59/64 loss: 0.08588969707489014
Batch 60/64 loss: 0.06544792652130127
Batch 61/64 loss: 0.04140877723693848
Batch 62/64 loss: 0.08308130502700806
Batch 63/64 loss: 0.04562711715698242
Batch 64/64 loss: 0.10095518827438354
Epoch 202  Train loss: 0.071818352914324  Val loss: 0.12517160074817357
Epoch 203
-------------------------------
Batch 1/64 loss: 0.06811767816543579
Batch 2/64 loss: 0.07887029647827148
Batch 3/64 loss: 0.07073581218719482
Batch 4/64 loss: 0.10924750566482544
Batch 5/64 loss: 0.06641209125518799
Batch 6/64 loss: 0.07695621252059937
Batch 7/64 loss: 0.05240809917449951
Batch 8/64 loss: 0.05835145711898804
Batch 9/64 loss: 0.06349807977676392
Batch 10/64 loss: 0.07870453596115112
Batch 11/64 loss: 0.07015657424926758
Batch 12/64 loss: 0.0714145302772522
Batch 13/64 loss: 0.06291484832763672
Batch 14/64 loss: 0.062068939208984375
Batch 15/64 loss: 0.06905961036682129
Batch 16/64 loss: 0.06806176900863647
Batch 17/64 loss: 0.06026822328567505
Batch 18/64 loss: 0.06341379880905151
Batch 19/64 loss: 0.07007431983947754
Batch 20/64 loss: 0.1046673059463501
Batch 21/64 loss: 0.07098507881164551
Batch 22/64 loss: 0.0691908597946167
Batch 23/64 loss: 0.05261337757110596
Batch 24/64 loss: 0.05909097194671631
Batch 25/64 loss: 0.07604861259460449
Batch 26/64 loss: 0.06894183158874512
Batch 27/64 loss: 0.05886036157608032
Batch 28/64 loss: 0.08169800043106079
Batch 29/64 loss: 0.08449864387512207
Batch 30/64 loss: 0.07729530334472656
Batch 31/64 loss: 0.07303011417388916
Batch 32/64 loss: 0.08475643396377563
Batch 33/64 loss: 0.0727350115776062
Batch 34/64 loss: 0.1013636589050293
Batch 35/64 loss: 0.07000839710235596
Batch 36/64 loss: 0.09663677215576172
Batch 37/64 loss: 0.04613053798675537
Batch 38/64 loss: 0.05855482816696167
Batch 39/64 loss: 0.06602108478546143
Batch 40/64 loss: 0.0720205307006836
Batch 41/64 loss: 0.07031792402267456
Batch 42/64 loss: 0.052411437034606934
Batch 43/64 loss: 0.055296123027801514
Batch 44/64 loss: 0.08060204982757568
Batch 45/64 loss: 0.0918886661529541
Batch 46/64 loss: 0.0717315673828125
Batch 47/64 loss: 0.08869755268096924
Batch 48/64 loss: 0.08601784706115723
Batch 49/64 loss: 0.07088488340377808
Batch 50/64 loss: 0.07628470659255981
Batch 51/64 loss: 0.04029107093811035
Batch 52/64 loss: 0.07205736637115479
Batch 53/64 loss: 0.0799640417098999
Batch 54/64 loss: 0.061263322830200195
Batch 55/64 loss: 0.11070698499679565
Batch 56/64 loss: 0.06654006242752075
Batch 57/64 loss: 0.08337867259979248
Batch 58/64 loss: 0.07329350709915161
Batch 59/64 loss: 0.08206218481063843
Batch 60/64 loss: 0.09312355518341064
Batch 61/64 loss: 0.0715174674987793
Batch 62/64 loss: 0.05360376834869385
Batch 63/64 loss: 0.0637168288230896
Batch 64/64 loss: 0.08452177047729492
Epoch 203  Train loss: 0.07254784340951957  Val loss: 0.1281571767174501
Epoch 204
-------------------------------
Batch 1/64 loss: 0.06355291604995728
Batch 2/64 loss: 0.049476444721221924
Batch 3/64 loss: 0.09756410121917725
Batch 4/64 loss: 0.0737375020980835
Batch 5/64 loss: 0.061972975730895996
Batch 6/64 loss: 0.06765633821487427
Batch 7/64 loss: 0.0579952597618103
Batch 8/64 loss: 0.05039018392562866
Batch 9/64 loss: 0.08527147769927979
Batch 10/64 loss: 0.08062422275543213
Batch 11/64 loss: 0.07440674304962158
Batch 12/64 loss: 0.056826233863830566
Batch 13/64 loss: 0.05912792682647705
Batch 14/64 loss: 0.08743447065353394
Batch 15/64 loss: 0.07568401098251343
Batch 16/64 loss: 0.05854010581970215
Batch 17/64 loss: 0.05666220188140869
Batch 18/64 loss: 0.07488822937011719
Batch 19/64 loss: 0.062449753284454346
Batch 20/64 loss: 0.08128374814987183
Batch 21/64 loss: 0.05061304569244385
Batch 22/64 loss: 0.07175195217132568
Batch 23/64 loss: 0.10019791126251221
Batch 24/64 loss: 0.05991852283477783
Batch 25/64 loss: 0.06967180967330933
Batch 26/64 loss: 0.05942213535308838
Batch 27/64 loss: 0.07535171508789062
Batch 28/64 loss: 0.06656932830810547
Batch 29/64 loss: 0.06491416692733765
Batch 30/64 loss: 0.08119994401931763
Batch 31/64 loss: 0.10198509693145752
Batch 32/64 loss: 0.06237882375717163
Batch 33/64 loss: 0.07531154155731201
Batch 34/64 loss: 0.08605939149856567
Batch 35/64 loss: 0.07791471481323242
Batch 36/64 loss: 0.09611928462982178
Batch 37/64 loss: 0.08892101049423218
Batch 38/64 loss: 0.06490963697433472
Batch 39/64 loss: 0.042080581188201904
Batch 40/64 loss: 0.07545900344848633
Batch 41/64 loss: 0.05529344081878662
Batch 42/64 loss: 0.07412278652191162
Batch 43/64 loss: 0.09065508842468262
Batch 44/64 loss: 0.0652618408203125
Batch 45/64 loss: 0.06096065044403076
Batch 46/64 loss: 0.050328731536865234
Batch 47/64 loss: 0.07020962238311768
Batch 48/64 loss: 0.06314808130264282
Batch 49/64 loss: 0.09981697797775269
Batch 50/64 loss: 0.06763410568237305
Batch 51/64 loss: 0.07404941320419312
Batch 52/64 loss: 0.06404644250869751
Batch 53/64 loss: 0.07801365852355957
Batch 54/64 loss: 0.0456271767616272
Batch 55/64 loss: 0.06404846906661987
Batch 56/64 loss: 0.059900522232055664
Batch 57/64 loss: 0.0870828628540039
Batch 58/64 loss: 0.0718163251876831
Batch 59/64 loss: 0.07098144292831421
Batch 60/64 loss: 0.06772100925445557
Batch 61/64 loss: 0.06619173288345337
Batch 62/64 loss: 0.06529760360717773
Batch 63/64 loss: 0.08224940299987793
Batch 64/64 loss: 0.049473345279693604
Epoch 204  Train loss: 0.07024089191474167  Val loss: 0.12693856262259468
Epoch 205
-------------------------------
Batch 1/64 loss: 0.06947684288024902
Batch 2/64 loss: 0.05819004774093628
Batch 3/64 loss: 0.07902759313583374
Batch 4/64 loss: 0.042577147483825684
Batch 5/64 loss: 0.08860456943511963
Batch 6/64 loss: 0.05790609121322632
Batch 7/64 loss: 0.06977468729019165
Batch 8/64 loss: 0.05709552764892578
Batch 9/64 loss: 0.05080139636993408
Batch 10/64 loss: 0.0628042221069336
Batch 11/64 loss: 0.0833824872970581
Batch 12/64 loss: 0.07349151372909546
Batch 13/64 loss: 0.06102192401885986
Batch 14/64 loss: 0.0713614821434021
Batch 15/64 loss: 0.07243180274963379
Batch 16/64 loss: 0.05664992332458496
Batch 17/64 loss: 0.07051527500152588
Batch 18/64 loss: 0.0843842625617981
Batch 19/64 loss: 0.05589759349822998
Batch 20/64 loss: 0.12169218063354492
Batch 21/64 loss: 0.0697677731513977
Batch 22/64 loss: 0.052347004413604736
Batch 23/64 loss: 0.06935697793960571
Batch 24/64 loss: 0.07580745220184326
Batch 25/64 loss: 0.06305086612701416
Batch 26/64 loss: 0.1011742353439331
Batch 27/64 loss: 0.06777215003967285
Batch 28/64 loss: 0.09075868129730225
Batch 29/64 loss: 0.06661897897720337
Batch 30/64 loss: 0.10287684202194214
Batch 31/64 loss: 0.06816750764846802
Batch 32/64 loss: 0.09562110900878906
Batch 33/64 loss: 0.06005007028579712
Batch 34/64 loss: 0.05424642562866211
Batch 35/64 loss: 0.07277870178222656
Batch 36/64 loss: 0.10281062126159668
Batch 37/64 loss: 0.07516604661941528
Batch 38/64 loss: 0.061937153339385986
Batch 39/64 loss: 0.07927966117858887
Batch 40/64 loss: 0.08805292844772339
Batch 41/64 loss: 0.05631333589553833
Batch 42/64 loss: 0.08745849132537842
Batch 43/64 loss: 0.07991808652877808
Batch 44/64 loss: 0.08438050746917725
Batch 45/64 loss: 0.07177203893661499
Batch 46/64 loss: 0.0909920334815979
Batch 47/64 loss: 0.061202287673950195
Batch 48/64 loss: 0.04571044445037842
Batch 49/64 loss: 0.06252527236938477
Batch 50/64 loss: 0.07094419002532959
Batch 51/64 loss: 0.06312841176986694
Batch 52/64 loss: 0.07651197910308838
Batch 53/64 loss: 0.08442318439483643
Batch 54/64 loss: 0.05741018056869507
Batch 55/64 loss: 0.0775299072265625
Batch 56/64 loss: 0.05142617225646973
Batch 57/64 loss: 0.06019335985183716
Batch 58/64 loss: 0.0638776421546936
Batch 59/64 loss: 0.08984124660491943
Batch 60/64 loss: 0.11092233657836914
Batch 61/64 loss: 0.07720327377319336
Batch 62/64 loss: 0.07220619916915894
Batch 63/64 loss: 0.08271211385726929
Batch 64/64 loss: 0.06575727462768555
Epoch 205  Train loss: 0.0726689946417715  Val loss: 0.1267302179664271
Epoch 206
-------------------------------
Batch 1/64 loss: 0.05612921714782715
Batch 2/64 loss: 0.05702769756317139
Batch 3/64 loss: 0.05428600311279297
Batch 4/64 loss: 0.05641305446624756
Batch 5/64 loss: 0.05076611042022705
Batch 6/64 loss: 0.07730615139007568
Batch 7/64 loss: 0.08840978145599365
Batch 8/64 loss: 0.07871538400650024
Batch 9/64 loss: 0.08588457107543945
Batch 10/64 loss: 0.059445977210998535
Batch 11/64 loss: 0.07411473989486694
Batch 12/64 loss: 0.06042248010635376
Batch 13/64 loss: 0.07926231622695923
Batch 14/64 loss: 0.0680699348449707
Batch 15/64 loss: 0.06655240058898926
Batch 16/64 loss: 0.059711754322052
Batch 17/64 loss: 0.07488322257995605
Batch 18/64 loss: 0.07630288600921631
Batch 19/64 loss: 0.08264678716659546
Batch 20/64 loss: 0.0884784460067749
Batch 21/64 loss: 0.0690646767616272
Batch 22/64 loss: 0.05829262733459473
Batch 23/64 loss: 0.07883292436599731
Batch 24/64 loss: 0.05103415250778198
Batch 25/64 loss: 0.07196146249771118
Batch 26/64 loss: 0.05269813537597656
Batch 27/64 loss: 0.06508368253707886
Batch 28/64 loss: 0.10731428861618042
Batch 29/64 loss: 0.09214037656784058
Batch 30/64 loss: 0.08479535579681396
Batch 31/64 loss: 0.07393717765808105
Batch 32/64 loss: 0.056917548179626465
Batch 33/64 loss: 0.05323612689971924
Batch 34/64 loss: 0.08025908470153809
Batch 35/64 loss: 0.058324337005615234
Batch 36/64 loss: 0.04536783695220947
Batch 37/64 loss: 0.09633463621139526
Batch 38/64 loss: 0.07382261753082275
Batch 39/64 loss: 0.059929609298706055
Batch 40/64 loss: 0.07603436708450317
Batch 41/64 loss: 0.02563655376434326
Batch 42/64 loss: 0.0816221833229065
Batch 43/64 loss: 0.08089107275009155
Batch 44/64 loss: 0.09204745292663574
Batch 45/64 loss: 0.08034253120422363
Batch 46/64 loss: 0.07825887203216553
Batch 47/64 loss: 0.08660072088241577
Batch 48/64 loss: 0.06447356939315796
Batch 49/64 loss: 0.056342244148254395
Batch 50/64 loss: 0.07608550786972046
Batch 51/64 loss: 0.07852935791015625
Batch 52/64 loss: 0.11424601078033447
Batch 53/64 loss: 0.07111316919326782
Batch 54/64 loss: 0.06351065635681152
Batch 55/64 loss: 0.05699223279953003
Batch 56/64 loss: 0.05945020914077759
Batch 57/64 loss: 0.05105149745941162
Batch 58/64 loss: 0.09812980890274048
Batch 59/64 loss: 0.0694703459739685
Batch 60/64 loss: 0.05065697431564331
Batch 61/64 loss: 0.07325100898742676
Batch 62/64 loss: 0.05454069375991821
Batch 63/64 loss: 0.08527791500091553
Batch 64/64 loss: 0.07082045078277588
Epoch 206  Train loss: 0.07061719006183101  Val loss: 0.12377948593028222
Saving best model, epoch: 206
Epoch 207
-------------------------------
Batch 1/64 loss: 0.07433432340621948
Batch 2/64 loss: 0.08259117603302002
Batch 3/64 loss: 0.08439004421234131
Batch 4/64 loss: 0.07867562770843506
Batch 5/64 loss: 0.05565929412841797
Batch 6/64 loss: 0.08676618337631226
Batch 7/64 loss: 0.05967068672180176
Batch 8/64 loss: 0.06798237562179565
Batch 9/64 loss: 0.03929543495178223
Batch 10/64 loss: 0.07661670446395874
Batch 11/64 loss: 0.07145941257476807
Batch 12/64 loss: 0.05878746509552002
Batch 13/64 loss: 0.08315682411193848
Batch 14/64 loss: 0.06416308879852295
Batch 15/64 loss: 0.07436376810073853
Batch 16/64 loss: 0.03832066059112549
Batch 17/64 loss: 0.06591254472732544
Batch 18/64 loss: 0.04915904998779297
Batch 19/64 loss: 0.042406558990478516
Batch 20/64 loss: 0.08984607458114624
Batch 21/64 loss: 0.06842750310897827
Batch 22/64 loss: 0.08151447772979736
Batch 23/64 loss: 0.09984415769577026
Batch 24/64 loss: 0.07305479049682617
Batch 25/64 loss: 0.09397286176681519
Batch 26/64 loss: 0.08360350131988525
Batch 27/64 loss: 0.059875667095184326
Batch 28/64 loss: 0.07144659757614136
Batch 29/64 loss: 0.06684762239456177
Batch 30/64 loss: 0.02963775396347046
Batch 31/64 loss: 0.06940329074859619
Batch 32/64 loss: 0.07899534702301025
Batch 33/64 loss: 0.0734068751335144
Batch 34/64 loss: 0.05834168195724487
Batch 35/64 loss: 0.0776945948600769
Batch 36/64 loss: 0.11383247375488281
Batch 37/64 loss: 0.08453565835952759
Batch 38/64 loss: 0.10606789588928223
Batch 39/64 loss: 0.08778142929077148
Batch 40/64 loss: 0.05617481470108032
Batch 41/64 loss: 0.07022792100906372
Batch 42/64 loss: 0.04316520690917969
Batch 43/64 loss: 0.09394699335098267
Batch 44/64 loss: 0.07658469676971436
Batch 45/64 loss: 0.0769655704498291
Batch 46/64 loss: 0.07125121355056763
Batch 47/64 loss: 0.05935591459274292
Batch 48/64 loss: 0.07443839311599731
Batch 49/64 loss: 0.07694488763809204
Batch 50/64 loss: 0.07951825857162476
Batch 51/64 loss: 0.057660818099975586
Batch 52/64 loss: 0.07040178775787354
Batch 53/64 loss: 0.055747270584106445
Batch 54/64 loss: 0.05278587341308594
Batch 55/64 loss: 0.05958080291748047
Batch 56/64 loss: 0.07790422439575195
Batch 57/64 loss: 0.06139814853668213
Batch 58/64 loss: 0.07472658157348633
Batch 59/64 loss: 0.058931946754455566
Batch 60/64 loss: 0.061103105545043945
Batch 61/64 loss: 0.06380259990692139
Batch 62/64 loss: 0.0777999758720398
Batch 63/64 loss: 0.05582141876220703
Batch 64/64 loss: 0.059018850326538086
Epoch 207  Train loss: 0.070154416327383  Val loss: 0.12471548265607905
Epoch 208
-------------------------------
Batch 1/64 loss: 0.0780491828918457
Batch 2/64 loss: 0.05879175662994385
Batch 3/64 loss: 0.06570512056350708
Batch 4/64 loss: 0.09085094928741455
Batch 5/64 loss: 0.05422705411911011
Batch 6/64 loss: 0.06629633903503418
Batch 7/64 loss: 0.07460629940032959
Batch 8/64 loss: 0.08139681816101074
Batch 9/64 loss: 0.07718437910079956
Batch 10/64 loss: 0.07373684644699097
Batch 11/64 loss: 0.06430524587631226
Batch 12/64 loss: 0.06788945198059082
Batch 13/64 loss: 0.10668802261352539
Batch 14/64 loss: 0.05042374134063721
Batch 15/64 loss: 0.09793704748153687
Batch 16/64 loss: 0.07764840126037598
Batch 17/64 loss: 0.0430908203125
Batch 18/64 loss: 0.07029283046722412
Batch 19/64 loss: 0.060995519161224365
Batch 20/64 loss: 0.07937496900558472
Batch 21/64 loss: 0.07478988170623779
Batch 22/64 loss: 0.05461084842681885
Batch 23/64 loss: 0.08642691373825073
Batch 24/64 loss: 0.06056714057922363
Batch 25/64 loss: 0.07432425022125244
Batch 26/64 loss: 0.07766485214233398
Batch 27/64 loss: 0.06387001276016235
Batch 28/64 loss: 0.04433506727218628
Batch 29/64 loss: 0.046426236629486084
Batch 30/64 loss: 0.06914782524108887
Batch 31/64 loss: 0.10741686820983887
Batch 32/64 loss: 0.07642930746078491
Batch 33/64 loss: 0.06569290161132812
Batch 34/64 loss: 0.06817984580993652
Batch 35/64 loss: 0.06495118141174316
Batch 36/64 loss: 0.06846630573272705
Batch 37/64 loss: 0.04880416393280029
Batch 38/64 loss: 0.045257508754730225
Batch 39/64 loss: 0.09060239791870117
Batch 40/64 loss: 0.08648926019668579
Batch 41/64 loss: 0.08729034662246704
Batch 42/64 loss: 0.07807052135467529
Batch 43/64 loss: 0.061904847621917725
Batch 44/64 loss: 0.0907135009765625
Batch 45/64 loss: 0.0614781379699707
Batch 46/64 loss: 0.07144927978515625
Batch 47/64 loss: 0.0572129487991333
Batch 48/64 loss: 0.04573500156402588
Batch 49/64 loss: 0.10214006900787354
Batch 50/64 loss: 0.06587028503417969
Batch 51/64 loss: 0.09470599889755249
Batch 52/64 loss: 0.07460451126098633
Batch 53/64 loss: 0.08176523447036743
Batch 54/64 loss: 0.06358599662780762
Batch 55/64 loss: 0.06794977188110352
Batch 56/64 loss: 0.08355522155761719
Batch 57/64 loss: 0.06773233413696289
Batch 58/64 loss: 0.05537128448486328
Batch 59/64 loss: 0.06982553005218506
Batch 60/64 loss: 0.0599210262298584
Batch 61/64 loss: 0.07848304510116577
Batch 62/64 loss: 0.08860975503921509
Batch 63/64 loss: 0.05441009998321533
Batch 64/64 loss: 0.08443909883499146
Epoch 208  Train loss: 0.07121031588199092  Val loss: 0.12363090650322511
Saving best model, epoch: 208
Epoch 209
-------------------------------
Batch 1/64 loss: 0.046289801597595215
Batch 2/64 loss: 0.06234133243560791
Batch 3/64 loss: 0.08954501152038574
Batch 4/64 loss: 0.06912785768508911
Batch 5/64 loss: 0.06736099720001221
Batch 6/64 loss: 0.0635342001914978
Batch 7/64 loss: 0.07020264863967896
Batch 8/64 loss: 0.07491785287857056
Batch 9/64 loss: 0.08489221334457397
Batch 10/64 loss: 0.0495876669883728
Batch 11/64 loss: 0.07769852876663208
Batch 12/64 loss: 0.05242973566055298
Batch 13/64 loss: 0.0641700029373169
Batch 14/64 loss: 0.049239516258239746
Batch 15/64 loss: 0.07927364110946655
Batch 16/64 loss: 0.05427658557891846
Batch 17/64 loss: 0.06254136562347412
Batch 18/64 loss: 0.1061977744102478
Batch 19/64 loss: 0.07686889171600342
Batch 20/64 loss: 0.05953460931777954
Batch 21/64 loss: 0.06130331754684448
Batch 22/64 loss: 0.08172374963760376
Batch 23/64 loss: 0.04352051019668579
Batch 24/64 loss: 0.07858657836914062
Batch 25/64 loss: 0.0848156213760376
Batch 26/64 loss: 0.06722736358642578
Batch 27/64 loss: 0.0834469199180603
Batch 28/64 loss: 0.05839037895202637
Batch 29/64 loss: 0.03065282106399536
Batch 30/64 loss: 0.07234233617782593
Batch 31/64 loss: 0.0596388578414917
Batch 32/64 loss: 0.09609949588775635
Batch 33/64 loss: 0.04092150926589966
Batch 34/64 loss: 0.08736991882324219
Batch 35/64 loss: 0.0645628571510315
Batch 36/64 loss: 0.06125301122665405
Batch 37/64 loss: 0.057739436626434326
Batch 38/64 loss: 0.07248193025588989
Batch 39/64 loss: 0.07813405990600586
Batch 40/64 loss: 0.07342827320098877
Batch 41/64 loss: 0.04722625017166138
Batch 42/64 loss: 0.07035946846008301
Batch 43/64 loss: 0.11771661043167114
Batch 44/64 loss: 0.045445024967193604
Batch 45/64 loss: 0.05013948678970337
Batch 46/64 loss: 0.096976637840271
Batch 47/64 loss: 0.09333032369613647
Batch 48/64 loss: 0.063443124294281
Batch 49/64 loss: 0.06875669956207275
Batch 50/64 loss: 0.09208822250366211
Batch 51/64 loss: 0.08587193489074707
Batch 52/64 loss: 0.07167035341262817
Batch 53/64 loss: 0.08510565757751465
Batch 54/64 loss: 0.05981320142745972
Batch 55/64 loss: 0.09948211908340454
Batch 56/64 loss: 0.06787550449371338
Batch 57/64 loss: 0.07249492406845093
Batch 58/64 loss: 0.06527483463287354
Batch 59/64 loss: 0.08005046844482422
Batch 60/64 loss: 0.08043849468231201
Batch 61/64 loss: 0.03660118579864502
Batch 62/64 loss: 0.05491870641708374
Batch 63/64 loss: 0.05739718675613403
Batch 64/64 loss: 0.0873861312866211
Epoch 209  Train loss: 0.06967349332921645  Val loss: 0.12439864177474451
Epoch 210
-------------------------------
Batch 1/64 loss: 0.05894172191619873
Batch 2/64 loss: 0.05736798048019409
Batch 3/64 loss: 0.050121307373046875
Batch 4/64 loss: 0.06900686025619507
Batch 5/64 loss: 0.06679302453994751
Batch 6/64 loss: 0.07336419820785522
Batch 7/64 loss: 0.04905271530151367
Batch 8/64 loss: 0.04091674089431763
Batch 9/64 loss: 0.08384734392166138
Batch 10/64 loss: 0.07684940099716187
Batch 11/64 loss: 0.0480884313583374
Batch 12/64 loss: 0.07554864883422852
Batch 13/64 loss: 0.06041514873504639
Batch 14/64 loss: 0.08317553997039795
Batch 15/64 loss: 0.058985233306884766
Batch 16/64 loss: 0.08249670267105103
Batch 17/64 loss: 0.039608776569366455
Batch 18/64 loss: 0.07952022552490234
Batch 19/64 loss: 0.03586477041244507
Batch 20/64 loss: 0.06462639570236206
Batch 21/64 loss: 0.050019919872283936
Batch 22/64 loss: 0.0735974907875061
Batch 23/64 loss: 0.0816885232925415
Batch 24/64 loss: 0.0834839940071106
Batch 25/64 loss: 0.062202513217926025
Batch 26/64 loss: 0.04868042469024658
Batch 27/64 loss: 0.05775249004364014
Batch 28/64 loss: 0.08335649967193604
Batch 29/64 loss: 0.05247765779495239
Batch 30/64 loss: 0.05544304847717285
Batch 31/64 loss: 0.07044327259063721
Batch 32/64 loss: 0.08684396743774414
Batch 33/64 loss: 0.08245766162872314
Batch 34/64 loss: 0.06187605857849121
Batch 35/64 loss: 0.03470659255981445
Batch 36/64 loss: 0.06286156177520752
Batch 37/64 loss: 0.06428706645965576
Batch 38/64 loss: 0.09698337316513062
Batch 39/64 loss: 0.05449414253234863
Batch 40/64 loss: 0.041704416275024414
Batch 41/64 loss: 0.0526997447013855
Batch 42/64 loss: 0.07196342945098877
Batch 43/64 loss: 0.07935917377471924
Batch 44/64 loss: 0.10656148195266724
Batch 45/64 loss: 0.07715404033660889
Batch 46/64 loss: 0.05532330274581909
Batch 47/64 loss: 0.09083163738250732
Batch 48/64 loss: 0.06944572925567627
Batch 49/64 loss: 0.06814104318618774
Batch 50/64 loss: 0.06065404415130615
Batch 51/64 loss: 0.07541531324386597
Batch 52/64 loss: 0.10506224632263184
Batch 53/64 loss: 0.07277339696884155
Batch 54/64 loss: 0.08670580387115479
Batch 55/64 loss: 0.07966023683547974
Batch 56/64 loss: 0.09598731994628906
Batch 57/64 loss: 0.09871035814285278
Batch 58/64 loss: 0.08863359689712524
Batch 59/64 loss: 0.06595432758331299
Batch 60/64 loss: 0.06629496812820435
Batch 61/64 loss: 0.09428668022155762
Batch 62/64 loss: 0.06917828321456909
Batch 63/64 loss: 0.0945136547088623
Batch 64/64 loss: 0.06690418720245361
Epoch 210  Train loss: 0.0695755262000888  Val loss: 0.1295947741806712
Epoch 211
-------------------------------
Batch 1/64 loss: 0.07803046703338623
Batch 2/64 loss: 0.1076737642288208
Batch 3/64 loss: 0.05657845735549927
Batch 4/64 loss: 0.06739181280136108
Batch 5/64 loss: 0.058833181858062744
Batch 6/64 loss: 0.06292498111724854
Batch 7/64 loss: 0.07337242364883423
Batch 8/64 loss: 0.06859517097473145
Batch 9/64 loss: 0.053178250789642334
Batch 10/64 loss: 0.0672115683555603
Batch 11/64 loss: 0.07290458679199219
Batch 12/64 loss: 0.0607830286026001
Batch 13/64 loss: 0.061157941818237305
Batch 14/64 loss: 0.0752342939376831
Batch 15/64 loss: 0.05059361457824707
Batch 16/64 loss: 0.05635225772857666
Batch 17/64 loss: 0.06320834159851074
Batch 18/64 loss: 0.08038127422332764
Batch 19/64 loss: 0.11210501194000244
Batch 20/64 loss: 0.05100142955780029
Batch 21/64 loss: 0.05255335569381714
Batch 22/64 loss: 0.09296071529388428
Batch 23/64 loss: 0.04774600267410278
Batch 24/64 loss: 0.05008208751678467
Batch 25/64 loss: 0.08240115642547607
Batch 26/64 loss: 0.0684652328491211
Batch 27/64 loss: 0.04089784622192383
Batch 28/64 loss: 0.06684744358062744
Batch 29/64 loss: 0.07947403192520142
Batch 30/64 loss: 0.07758074998855591
Batch 31/64 loss: 0.10229992866516113
Batch 32/64 loss: 0.0630640983581543
Batch 33/64 loss: 0.060602545738220215
Batch 34/64 loss: 0.07474303245544434
Batch 35/64 loss: 0.05925488471984863
Batch 36/64 loss: 0.07404130697250366
Batch 37/64 loss: 0.04776203632354736
Batch 38/64 loss: 0.0777178406715393
Batch 39/64 loss: 0.04937314987182617
Batch 40/64 loss: 0.07512903213500977
Batch 41/64 loss: 0.08138889074325562
Batch 42/64 loss: 0.06478303670883179
Batch 43/64 loss: 0.052996277809143066
Batch 44/64 loss: 0.06970739364624023
Batch 45/64 loss: 0.08395332098007202
Batch 46/64 loss: 0.08135515451431274
Batch 47/64 loss: 0.06008392572402954
Batch 48/64 loss: 0.07742953300476074
Batch 49/64 loss: 0.053832173347473145
Batch 50/64 loss: 0.09447306394577026
Batch 51/64 loss: 0.07824265956878662
Batch 52/64 loss: 0.07598137855529785
Batch 53/64 loss: 0.07412958145141602
Batch 54/64 loss: 0.08033525943756104
Batch 55/64 loss: 0.051666855812072754
Batch 56/64 loss: 0.08249574899673462
Batch 57/64 loss: 0.06548833847045898
Batch 58/64 loss: 0.06463068723678589
Batch 59/64 loss: 0.064145028591156
Batch 60/64 loss: 0.07498490810394287
Batch 61/64 loss: 0.0753101110458374
Batch 62/64 loss: 0.06864368915557861
Batch 63/64 loss: 0.07081669569015503
Batch 64/64 loss: 0.07049685716629028
Epoch 211  Train loss: 0.06933732850878846  Val loss: 0.1264129805401019
Epoch 212
-------------------------------
Batch 1/64 loss: 0.050874173641204834
Batch 2/64 loss: 0.06888020038604736
Batch 3/64 loss: 0.06713706254959106
Batch 4/64 loss: 0.05937141180038452
Batch 5/64 loss: 0.09238040447235107
Batch 6/64 loss: 0.08845102787017822
Batch 7/64 loss: 0.05732572078704834
Batch 8/64 loss: 0.05983614921569824
Batch 9/64 loss: 0.08352237939834595
Batch 10/64 loss: 0.07456469535827637
Batch 11/64 loss: 0.08264005184173584
Batch 12/64 loss: 0.07220947742462158
Batch 13/64 loss: 0.09107136726379395
Batch 14/64 loss: 0.06432855129241943
Batch 15/64 loss: 0.056041598320007324
Batch 16/64 loss: 0.04332280158996582
Batch 17/64 loss: 0.0614435076713562
Batch 18/64 loss: 0.06384170055389404
Batch 19/64 loss: 0.10301196575164795
Batch 20/64 loss: 0.05952310562133789
Batch 21/64 loss: 0.053865253925323486
Batch 22/64 loss: 0.07223916053771973
Batch 23/64 loss: 0.07647228240966797
Batch 24/64 loss: 0.08407258987426758
Batch 25/64 loss: 0.08005249500274658
Batch 26/64 loss: 0.09252214431762695
Batch 27/64 loss: 0.05385863780975342
Batch 28/64 loss: 0.07542914152145386
Batch 29/64 loss: 0.08020871877670288
Batch 30/64 loss: 0.049477219581604004
Batch 31/64 loss: 0.050960004329681396
Batch 32/64 loss: 0.053530097007751465
Batch 33/64 loss: 0.07013165950775146
Batch 34/64 loss: 0.044608116149902344
Batch 35/64 loss: 0.06201374530792236
Batch 36/64 loss: 0.05764436721801758
Batch 37/64 loss: 0.0709235668182373
Batch 38/64 loss: 0.09304052591323853
Batch 39/64 loss: 0.05974078178405762
Batch 40/64 loss: 0.0738477110862732
Batch 41/64 loss: 0.08473026752471924
Batch 42/64 loss: 0.06884300708770752
Batch 43/64 loss: 0.0854719877243042
Batch 44/64 loss: 0.06781631708145142
Batch 45/64 loss: 0.051811277866363525
Batch 46/64 loss: 0.050588250160217285
Batch 47/64 loss: 0.06467151641845703
Batch 48/64 loss: 0.04460340738296509
Batch 49/64 loss: 0.05173468589782715
Batch 50/64 loss: 0.05470997095108032
Batch 51/64 loss: 0.06868886947631836
Batch 52/64 loss: 0.05393272638320923
Batch 53/64 loss: 0.07209956645965576
Batch 54/64 loss: 0.08514487743377686
Batch 55/64 loss: 0.05968737602233887
Batch 56/64 loss: 0.06259673833847046
Batch 57/64 loss: 0.06729775667190552
Batch 58/64 loss: 0.03925120830535889
Batch 59/64 loss: 0.09444952011108398
Batch 60/64 loss: 0.06321710348129272
Batch 61/64 loss: 0.07268130779266357
Batch 62/64 loss: 0.07083654403686523
Batch 63/64 loss: 0.0926174521446228
Batch 64/64 loss: 0.10710293054580688
Epoch 212  Train loss: 0.06842705104865279  Val loss: 0.12884161951615639
Epoch 213
-------------------------------
Batch 1/64 loss: 0.07699424028396606
Batch 2/64 loss: 0.07432359457015991
Batch 3/64 loss: 0.06965237855911255
Batch 4/64 loss: 0.053780555725097656
Batch 5/64 loss: 0.0812341570854187
Batch 6/64 loss: 0.07412111759185791
Batch 7/64 loss: 0.050733208656311035
Batch 8/64 loss: 0.03953278064727783
Batch 9/64 loss: 0.08319032192230225
Batch 10/64 loss: 0.062361717224121094
Batch 11/64 loss: 0.06986725330352783
Batch 12/64 loss: 0.06099534034729004
Batch 13/64 loss: 0.07054704427719116
Batch 14/64 loss: 0.05748945474624634
Batch 15/64 loss: 0.08784866333007812
Batch 16/64 loss: 0.04475271701812744
Batch 17/64 loss: 0.0627332329750061
Batch 18/64 loss: 0.08422333002090454
Batch 19/64 loss: 0.0500415563583374
Batch 20/64 loss: 0.07302868366241455
Batch 21/64 loss: 0.0451541543006897
Batch 22/64 loss: 0.04433250427246094
Batch 23/64 loss: 0.08180153369903564
Batch 24/64 loss: 0.06839245557785034
Batch 25/64 loss: 0.040700554847717285
Batch 26/64 loss: 0.05901753902435303
Batch 27/64 loss: 0.04963463544845581
Batch 28/64 loss: 0.051379501819610596
Batch 29/64 loss: 0.05502676963806152
Batch 30/64 loss: 0.06210935115814209
Batch 31/64 loss: 0.07095062732696533
Batch 32/64 loss: 0.08250564336776733
Batch 33/64 loss: 0.07508665323257446
Batch 34/64 loss: 0.09003984928131104
Batch 35/64 loss: 0.05387520790100098
Batch 36/64 loss: 0.07877075672149658
Batch 37/64 loss: 0.10200124979019165
Batch 38/64 loss: 0.07887792587280273
Batch 39/64 loss: 0.040904223918914795
Batch 40/64 loss: 0.07958197593688965
Batch 41/64 loss: 0.09102356433868408
Batch 42/64 loss: 0.10244572162628174
Batch 43/64 loss: 0.05977785587310791
Batch 44/64 loss: 0.07074844837188721
Batch 45/64 loss: 0.06788474321365356
Batch 46/64 loss: 0.0700598955154419
Batch 47/64 loss: 0.06573575735092163
Batch 48/64 loss: 0.07130551338195801
Batch 49/64 loss: 0.07475441694259644
Batch 50/64 loss: 0.06340491771697998
Batch 51/64 loss: 0.08978492021560669
Batch 52/64 loss: 0.08008462190628052
Batch 53/64 loss: 0.07986938953399658
Batch 54/64 loss: 0.07790136337280273
Batch 55/64 loss: 0.05477142333984375
Batch 56/64 loss: 0.08176231384277344
Batch 57/64 loss: 0.06899648904800415
Batch 58/64 loss: 0.06809955835342407
Batch 59/64 loss: 0.04424923658370972
Batch 60/64 loss: 0.056844234466552734
Batch 61/64 loss: 0.10038530826568604
Batch 62/64 loss: 0.06666171550750732
Batch 63/64 loss: 0.08078157901763916
Batch 64/64 loss: 0.08243221044540405
Epoch 213  Train loss: 0.06881178991467345  Val loss: 0.12504893643749537
Epoch 214
-------------------------------
Batch 1/64 loss: 0.0726855993270874
Batch 2/64 loss: 0.061562180519104004
Batch 3/64 loss: 0.08532214164733887
Batch 4/64 loss: 0.061180949211120605
Batch 5/64 loss: 0.05156075954437256
Batch 6/64 loss: 0.08581370115280151
Batch 7/64 loss: 0.10163873434066772
Batch 8/64 loss: 0.05479013919830322
Batch 9/64 loss: 0.03565096855163574
Batch 10/64 loss: 0.07510888576507568
Batch 11/64 loss: 0.08854067325592041
Batch 12/64 loss: 0.07503795623779297
Batch 13/64 loss: 0.054999351501464844
Batch 14/64 loss: 0.07571130990982056
Batch 15/64 loss: 0.06066703796386719
Batch 16/64 loss: 0.0567164421081543
Batch 17/64 loss: 0.06310856342315674
Batch 18/64 loss: 0.05911862850189209
Batch 19/64 loss: 0.07874715328216553
Batch 20/64 loss: 0.04705333709716797
Batch 21/64 loss: 0.08790755271911621
Batch 22/64 loss: 0.08803457021713257
Batch 23/64 loss: 0.06496500968933105
Batch 24/64 loss: 0.0956769585609436
Batch 25/64 loss: 0.04934203624725342
Batch 26/64 loss: 0.05733489990234375
Batch 27/64 loss: 0.07822906970977783
Batch 28/64 loss: 0.051615357398986816
Batch 29/64 loss: 0.05949723720550537
Batch 30/64 loss: 0.043398141860961914
Batch 31/64 loss: 0.05214858055114746
Batch 32/64 loss: 0.06438374519348145
Batch 33/64 loss: 0.06305873394012451
Batch 34/64 loss: 0.030351638793945312
Batch 35/64 loss: 0.057444870471954346
Batch 36/64 loss: 0.07822829484939575
Batch 37/64 loss: 0.06473702192306519
Batch 38/64 loss: 0.07535266876220703
Batch 39/64 loss: 0.09379160404205322
Batch 40/64 loss: 0.06618273258209229
Batch 41/64 loss: 0.07763415575027466
Batch 42/64 loss: 0.08866387605667114
Batch 43/64 loss: 0.0714145302772522
Batch 44/64 loss: 0.08075553178787231
Batch 45/64 loss: 0.06514215469360352
Batch 46/64 loss: 0.06649971008300781
Batch 47/64 loss: 0.04766172170639038
Batch 48/64 loss: 0.05642920732498169
Batch 49/64 loss: 0.06758660078048706
Batch 50/64 loss: 0.07596045732498169
Batch 51/64 loss: 0.056913554668426514
Batch 52/64 loss: 0.0678030252456665
Batch 53/64 loss: 0.0974740982055664
Batch 54/64 loss: 0.06438875198364258
Batch 55/64 loss: 0.06085622310638428
Batch 56/64 loss: 0.0643969178199768
Batch 57/64 loss: 0.05815213918685913
Batch 58/64 loss: 0.09072673320770264
Batch 59/64 loss: 0.06498336791992188
Batch 60/64 loss: 0.07532352209091187
Batch 61/64 loss: 0.07493036985397339
Batch 62/64 loss: 0.08007180690765381
Batch 63/64 loss: 0.06423044204711914
Batch 64/64 loss: 0.06464767456054688
Epoch 214  Train loss: 0.06797144927230536  Val loss: 0.12304823156894278
Saving best model, epoch: 214
Epoch 215
-------------------------------
Batch 1/64 loss: 0.06417346000671387
Batch 2/64 loss: 0.05971729755401611
Batch 3/64 loss: 0.040207505226135254
Batch 4/64 loss: 0.05150198936462402
Batch 5/64 loss: 0.0740097165107727
Batch 6/64 loss: 0.07314872741699219
Batch 7/64 loss: 0.07481694221496582
Batch 8/64 loss: 0.0835275650024414
Batch 9/64 loss: 0.08757328987121582
Batch 10/64 loss: 0.059169888496398926
Batch 11/64 loss: 0.06461977958679199
Batch 12/64 loss: 0.10963201522827148
Batch 13/64 loss: 0.06126970052719116
Batch 14/64 loss: 0.04990053176879883
Batch 15/64 loss: 0.07419532537460327
Batch 16/64 loss: 0.09455835819244385
Batch 17/64 loss: 0.07913470268249512
Batch 18/64 loss: 0.04612845182418823
Batch 19/64 loss: 0.05978518724441528
Batch 20/64 loss: 0.08309531211853027
Batch 21/64 loss: 0.0476040244102478
Batch 22/64 loss: 0.0915331244468689
Batch 23/64 loss: 0.055810511112213135
Batch 24/64 loss: 0.12226134538650513
Batch 25/64 loss: 0.06738913059234619
Batch 26/64 loss: 0.05575001239776611
Batch 27/64 loss: 0.09668415784835815
Batch 28/64 loss: 0.08224666118621826
Batch 29/64 loss: 0.05275392532348633
Batch 30/64 loss: 0.05272597074508667
Batch 31/64 loss: 0.04557216167449951
Batch 32/64 loss: 0.055696964263916016
Batch 33/64 loss: 0.04809850454330444
Batch 34/64 loss: 0.0909426212310791
Batch 35/64 loss: 0.04563748836517334
Batch 36/64 loss: 0.07483214139938354
Batch 37/64 loss: 0.05620741844177246
Batch 38/64 loss: 0.06054067611694336
Batch 39/64 loss: 0.07983016967773438
Batch 40/64 loss: 0.09079188108444214
Batch 41/64 loss: 0.04367208480834961
Batch 42/64 loss: 0.04443025588989258
Batch 43/64 loss: 0.06586909294128418
Batch 44/64 loss: 0.060733139514923096
Batch 45/64 loss: 0.067935049533844
Batch 46/64 loss: 0.06579232215881348
Batch 47/64 loss: 0.07401168346405029
Batch 48/64 loss: 0.05537372827529907
Batch 49/64 loss: 0.06177550554275513
Batch 50/64 loss: 0.052162110805511475
Batch 51/64 loss: 0.05445915460586548
Batch 52/64 loss: 0.052102625370025635
Batch 53/64 loss: 0.08748054504394531
Batch 54/64 loss: 0.07397806644439697
Batch 55/64 loss: 0.048450350761413574
Batch 56/64 loss: 0.07774204015731812
Batch 57/64 loss: 0.06667488813400269
Batch 58/64 loss: 0.06314653158187866
Batch 59/64 loss: 0.06848883628845215
Batch 60/64 loss: 0.05867290496826172
Batch 61/64 loss: 0.0446699857711792
Batch 62/64 loss: 0.0631105899810791
Batch 63/64 loss: 0.08378636837005615
Batch 64/64 loss: 0.0832672119140625
Epoch 215  Train loss: 0.06682420244403914  Val loss: 0.12568159488468236
Epoch 216
-------------------------------
Batch 1/64 loss: 0.06165194511413574
Batch 2/64 loss: 0.07606738805770874
Batch 3/64 loss: 0.05461162328720093
Batch 4/64 loss: 0.09470105171203613
Batch 5/64 loss: 0.07563674449920654
Batch 6/64 loss: 0.052874624729156494
Batch 7/64 loss: 0.04104769229888916
Batch 8/64 loss: 0.06113821268081665
Batch 9/64 loss: 0.061635375022888184
Batch 10/64 loss: 0.07976847887039185
Batch 11/64 loss: 0.05394947528839111
Batch 12/64 loss: 0.09037673473358154
Batch 13/64 loss: 0.06780797243118286
Batch 14/64 loss: 0.047183334827423096
Batch 15/64 loss: 0.0809096097946167
Batch 16/64 loss: 0.07589131593704224
Batch 17/64 loss: 0.11043018102645874
Batch 18/64 loss: 0.060793280601501465
Batch 19/64 loss: 0.07553243637084961
Batch 20/64 loss: 0.04171794652938843
Batch 21/64 loss: 0.07990115880966187
Batch 22/64 loss: 0.06810539960861206
Batch 23/64 loss: 0.06559538841247559
Batch 24/64 loss: 0.06848615407943726
Batch 25/64 loss: 0.06873089075088501
Batch 26/64 loss: 0.09201133251190186
Batch 27/64 loss: 0.07321316003799438
Batch 28/64 loss: 0.06072711944580078
Batch 29/64 loss: 0.06664764881134033
Batch 30/64 loss: 0.07028698921203613
Batch 31/64 loss: 0.061165571212768555
Batch 32/64 loss: 0.07416975498199463
Batch 33/64 loss: 0.10550183057785034
Batch 34/64 loss: 0.07017070055007935
Batch 35/64 loss: 0.06151634454727173
Batch 36/64 loss: 0.040638625621795654
Batch 37/64 loss: 0.062358558177948
Batch 38/64 loss: 0.03742384910583496
Batch 39/64 loss: 0.06971538066864014
Batch 40/64 loss: 0.06050515174865723
Batch 41/64 loss: 0.07723212242126465
Batch 42/64 loss: 0.06935131549835205
Batch 43/64 loss: 0.09842550754547119
Batch 44/64 loss: 0.07072865962982178
Batch 45/64 loss: 0.08395493030548096
Batch 46/64 loss: 0.058091580867767334
Batch 47/64 loss: 0.04510009288787842
Batch 48/64 loss: 0.04848158359527588
Batch 49/64 loss: 0.0481376051902771
Batch 50/64 loss: 0.07489722967147827
Batch 51/64 loss: 0.06757080554962158
Batch 52/64 loss: 0.05249488353729248
Batch 53/64 loss: 0.08241629600524902
Batch 54/64 loss: 0.03660553693771362
Batch 55/64 loss: 0.09396332502365112
Batch 56/64 loss: 0.0712704062461853
Batch 57/64 loss: 0.05331689119338989
Batch 58/64 loss: 0.06930285692214966
Batch 59/64 loss: 0.07349944114685059
Batch 60/64 loss: 0.0932379961013794
Batch 61/64 loss: 0.07119083404541016
Batch 62/64 loss: 0.0853620171546936
Batch 63/64 loss: 0.06931984424591064
Batch 64/64 loss: 0.08604496717453003
Epoch 216  Train loss: 0.06869148109473434  Val loss: 0.12651891601864004
Epoch 217
-------------------------------
Batch 1/64 loss: 0.06888598203659058
Batch 2/64 loss: 0.11052536964416504
Batch 3/64 loss: 0.0447392463684082
Batch 4/64 loss: 0.07297110557556152
Batch 5/64 loss: 0.04536396265029907
Batch 6/64 loss: 0.053409576416015625
Batch 7/64 loss: 0.07870835065841675
Batch 8/64 loss: 0.08074712753295898
Batch 9/64 loss: 0.0824728012084961
Batch 10/64 loss: 0.051163315773010254
Batch 11/64 loss: 0.03258669376373291
Batch 12/64 loss: 0.06285542249679565
Batch 13/64 loss: 0.040178894996643066
Batch 14/64 loss: 0.046417832374572754
Batch 15/64 loss: 0.07063686847686768
Batch 16/64 loss: 0.08243018388748169
Batch 17/64 loss: 0.036473095417022705
Batch 18/64 loss: 0.07458186149597168
Batch 19/64 loss: 0.056258976459503174
Batch 20/64 loss: 0.08339804410934448
Batch 21/64 loss: 0.06558263301849365
Batch 22/64 loss: 0.05573141574859619
Batch 23/64 loss: 0.07342272996902466
Batch 24/64 loss: 0.0849076509475708
Batch 25/64 loss: 0.02305614948272705
Batch 26/64 loss: 0.101986825466156
Batch 27/64 loss: 0.08361983299255371
Batch 28/64 loss: 0.07608121633529663
Batch 29/64 loss: 0.07682067155838013
Batch 30/64 loss: 0.08351534605026245
Batch 31/64 loss: 0.045535266399383545
Batch 32/64 loss: 0.05604410171508789
Batch 33/64 loss: 0.0538477897644043
Batch 34/64 loss: 0.05852651596069336
Batch 35/64 loss: 0.09130120277404785
Batch 36/64 loss: 0.06691038608551025
Batch 37/64 loss: 0.051119327545166016
Batch 38/64 loss: 0.08589684963226318
Batch 39/64 loss: 0.07906579971313477
Batch 40/64 loss: 0.0987589955329895
Batch 41/64 loss: 0.051043689250946045
Batch 42/64 loss: 0.07949906587600708
Batch 43/64 loss: 0.13767099380493164
Batch 44/64 loss: 0.08769845962524414
Batch 45/64 loss: 0.07036656141281128
Batch 46/64 loss: 0.07666492462158203
Batch 47/64 loss: 0.03450655937194824
Batch 48/64 loss: 0.04807251691818237
Batch 49/64 loss: 0.07714307308197021
Batch 50/64 loss: 0.06502002477645874
Batch 51/64 loss: 0.06365817785263062
Batch 52/64 loss: 0.0730893611907959
Batch 53/64 loss: 0.0805288553237915
Batch 54/64 loss: 0.08482247591018677
Batch 55/64 loss: 0.08859646320343018
Batch 56/64 loss: 0.09190422296524048
Batch 57/64 loss: 0.056719958782196045
Batch 58/64 loss: 0.05508315563201904
Batch 59/64 loss: 0.05322098731994629
Batch 60/64 loss: 0.05806344747543335
Batch 61/64 loss: 0.08586221933364868
Batch 62/64 loss: 0.06020164489746094
Batch 63/64 loss: 0.06860172748565674
Batch 64/64 loss: 0.039717793464660645
Epoch 217  Train loss: 0.06846058555677825  Val loss: 0.12294916864932608
Saving best model, epoch: 217
Epoch 218
-------------------------------
Batch 1/64 loss: 0.063576340675354
Batch 2/64 loss: 0.053675174713134766
Batch 3/64 loss: 0.06206178665161133
Batch 4/64 loss: 0.07284015417098999
Batch 5/64 loss: 0.07196712493896484
Batch 6/64 loss: 0.04192090034484863
Batch 7/64 loss: 0.0633435845375061
Batch 8/64 loss: 0.04466885328292847
Batch 9/64 loss: 0.07624733448028564
Batch 10/64 loss: 0.07488393783569336
Batch 11/64 loss: 0.049667179584503174
Batch 12/64 loss: 0.061132729053497314
Batch 13/64 loss: 0.07029038667678833
Batch 14/64 loss: 0.06076860427856445
Batch 15/64 loss: 0.04127019643783569
Batch 16/64 loss: 0.08773505687713623
Batch 17/64 loss: 0.04308652877807617
Batch 18/64 loss: 0.05879098176956177
Batch 19/64 loss: 0.05250418186187744
Batch 20/64 loss: 0.04289048910140991
Batch 21/64 loss: 0.09283995628356934
Batch 22/64 loss: 0.04889822006225586
Batch 23/64 loss: 0.04642438888549805
Batch 24/64 loss: 0.031125187873840332
Batch 25/64 loss: 0.06413441896438599
Batch 26/64 loss: 0.08584326505661011
Batch 27/64 loss: 0.0626949667930603
Batch 28/64 loss: 0.0686492919921875
Batch 29/64 loss: 0.06463366746902466
Batch 30/64 loss: 0.10076427459716797
Batch 31/64 loss: 0.08862960338592529
Batch 32/64 loss: 0.06126296520233154
Batch 33/64 loss: 0.07537376880645752
Batch 34/64 loss: 0.07959842681884766
Batch 35/64 loss: 0.06445884704589844
Batch 36/64 loss: 0.07412201166152954
Batch 37/64 loss: 0.06544512510299683
Batch 38/64 loss: 0.06872797012329102
Batch 39/64 loss: 0.060134828090667725
Batch 40/64 loss: 0.10182315111160278
Batch 41/64 loss: 0.07169896364212036
Batch 42/64 loss: 0.07474386692047119
Batch 43/64 loss: 0.07148808240890503
Batch 44/64 loss: 0.06476414203643799
Batch 45/64 loss: 0.07909572124481201
Batch 46/64 loss: 0.05420410633087158
Batch 47/64 loss: 0.053972721099853516
Batch 48/64 loss: 0.06990259885787964
Batch 49/64 loss: 0.07367193698883057
Batch 50/64 loss: 0.08822774887084961
Batch 51/64 loss: 0.09303915500640869
Batch 52/64 loss: 0.07707339525222778
Batch 53/64 loss: 0.05634915828704834
Batch 54/64 loss: 0.057359278202056885
Batch 55/64 loss: 0.057007431983947754
Batch 56/64 loss: 0.07228708267211914
Batch 57/64 loss: 0.09847903251647949
Batch 58/64 loss: 0.04530608654022217
Batch 59/64 loss: 0.057843029499053955
Batch 60/64 loss: 0.0775344967842102
Batch 61/64 loss: 0.05237460136413574
Batch 62/64 loss: 0.06560921669006348
Batch 63/64 loss: 0.06967997550964355
Batch 64/64 loss: 0.05773639678955078
Epoch 218  Train loss: 0.06625756843417299  Val loss: 0.1258619048751097
Epoch 219
-------------------------------
Batch 1/64 loss: 0.06003779172897339
Batch 2/64 loss: 0.06377971172332764
Batch 3/64 loss: 0.03365492820739746
Batch 4/64 loss: 0.06392735242843628
Batch 5/64 loss: 0.07099485397338867
Batch 6/64 loss: 0.07381170988082886
Batch 7/64 loss: 0.06744468212127686
Batch 8/64 loss: 0.08407813310623169
Batch 9/64 loss: 0.057678163051605225
Batch 10/64 loss: 0.06561005115509033
Batch 11/64 loss: 0.08800262212753296
Batch 12/64 loss: 0.05753803253173828
Batch 13/64 loss: 0.09431016445159912
Batch 14/64 loss: 0.04486966133117676
Batch 15/64 loss: 0.07006955146789551
Batch 16/64 loss: 0.06807214021682739
Batch 17/64 loss: 0.057817697525024414
Batch 18/64 loss: 0.0658646821975708
Batch 19/64 loss: 0.08857089281082153
Batch 20/64 loss: 0.05017590522766113
Batch 21/64 loss: 0.09293454885482788
Batch 22/64 loss: 0.05617022514343262
Batch 23/64 loss: 0.08653593063354492
Batch 24/64 loss: 0.06628620624542236
Batch 25/64 loss: 0.08323252201080322
Batch 26/64 loss: 0.08638638257980347
Batch 27/64 loss: 0.06448078155517578
Batch 28/64 loss: 0.0657617449760437
Batch 29/64 loss: 0.05175679922103882
Batch 30/64 loss: 0.0575566291809082
Batch 31/64 loss: 0.05853074789047241
Batch 32/64 loss: 0.06069445610046387
Batch 33/64 loss: 0.07009172439575195
Batch 34/64 loss: 0.07125312089920044
Batch 35/64 loss: 0.07446545362472534
Batch 36/64 loss: 0.05531656742095947
Batch 37/64 loss: 0.08241331577301025
Batch 38/64 loss: 0.06541144847869873
Batch 39/64 loss: 0.06969118118286133
Batch 40/64 loss: 0.06859570741653442
Batch 41/64 loss: 0.07005631923675537
Batch 42/64 loss: 0.050903260707855225
Batch 43/64 loss: 0.06128901243209839
Batch 44/64 loss: 0.08719247579574585
Batch 45/64 loss: 0.06714284420013428
Batch 46/64 loss: 0.0656130313873291
Batch 47/64 loss: 0.047813475131988525
Batch 48/64 loss: 0.04412341117858887
Batch 49/64 loss: 0.06729990243911743
Batch 50/64 loss: 0.059531569480895996
Batch 51/64 loss: 0.05949044227600098
Batch 52/64 loss: 0.03149425983428955
Batch 53/64 loss: 0.054845452308654785
Batch 54/64 loss: 0.06099659204483032
Batch 55/64 loss: 0.04988551139831543
Batch 56/64 loss: 0.0812828540802002
Batch 57/64 loss: 0.11134755611419678
Batch 58/64 loss: 0.0888373851776123
Batch 59/64 loss: 0.05668133497238159
Batch 60/64 loss: 0.06089508533477783
Batch 61/64 loss: 0.0723336935043335
Batch 62/64 loss: 0.0542752742767334
Batch 63/64 loss: 0.06901878118515015
Batch 64/64 loss: 0.05852055549621582
Epoch 219  Train loss: 0.06635473195244285  Val loss: 0.12061525661101456
Saving best model, epoch: 219
Epoch 220
-------------------------------
Batch 1/64 loss: 0.05385810136795044
Batch 2/64 loss: 0.06302756071090698
Batch 3/64 loss: 0.09123295545578003
Batch 4/64 loss: 0.034654855728149414
Batch 5/64 loss: 0.036605775356292725
Batch 6/64 loss: 0.02864319086074829
Batch 7/64 loss: 0.04800081253051758
Batch 8/64 loss: 0.03369569778442383
Batch 9/64 loss: 0.04186058044433594
Batch 10/64 loss: 0.06393176317214966
Batch 11/64 loss: 0.06998193264007568
Batch 12/64 loss: 0.059803128242492676
Batch 13/64 loss: 0.0760796070098877
Batch 14/64 loss: 0.08088111877441406
Batch 15/64 loss: 0.06819194555282593
Batch 16/64 loss: 0.09432250261306763
Batch 17/64 loss: 0.06554675102233887
Batch 18/64 loss: 0.07442808151245117
Batch 19/64 loss: 0.09920215606689453
Batch 20/64 loss: 0.055358707904815674
Batch 21/64 loss: 0.04315507411956787
Batch 22/64 loss: 0.07190275192260742
Batch 23/64 loss: 0.06632304191589355
Batch 24/64 loss: 0.06067556142807007
Batch 25/64 loss: 0.07603931427001953
Batch 26/64 loss: 0.05541330575942993
Batch 27/64 loss: 0.07164871692657471
Batch 28/64 loss: 0.07238119840621948
Batch 29/64 loss: 0.045043885707855225
Batch 30/64 loss: 0.07048171758651733
Batch 31/64 loss: 0.06921756267547607
Batch 32/64 loss: 0.05808520317077637
Batch 33/64 loss: 0.06239861249923706
Batch 34/64 loss: 0.06216418743133545
Batch 35/64 loss: 0.061967432498931885
Batch 36/64 loss: 0.09184837341308594
Batch 37/64 loss: 0.07414251565933228
Batch 38/64 loss: 0.06412428617477417
Batch 39/64 loss: 0.09218424558639526
Batch 40/64 loss: 0.05288875102996826
Batch 41/64 loss: 0.06158888339996338
Batch 42/64 loss: 0.09533226490020752
Batch 43/64 loss: 0.09292370080947876
Batch 44/64 loss: 0.06769347190856934
Batch 45/64 loss: 0.07963782548904419
Batch 46/64 loss: 0.048323214054107666
Batch 47/64 loss: 0.08133172988891602
Batch 48/64 loss: 0.07089412212371826
Batch 49/64 loss: 0.08131372928619385
Batch 50/64 loss: 0.070492684841156
Batch 51/64 loss: 0.07904958724975586
Batch 52/64 loss: 0.06562578678131104
Batch 53/64 loss: 0.05816662311553955
Batch 54/64 loss: 0.06513261795043945
Batch 55/64 loss: 0.05563312768936157
Batch 56/64 loss: 0.07172322273254395
Batch 57/64 loss: 0.07517075538635254
Batch 58/64 loss: 0.08504241704940796
Batch 59/64 loss: 0.0577845573425293
Batch 60/64 loss: 0.065765380859375
Batch 61/64 loss: 0.08746063709259033
Batch 62/64 loss: 0.09375345706939697
Batch 63/64 loss: 0.0843273401260376
Batch 64/64 loss: 0.05640381574630737
Epoch 220  Train loss: 0.06741745822569903  Val loss: 0.12382500872169573
Epoch 221
-------------------------------
Batch 1/64 loss: 0.050058722496032715
Batch 2/64 loss: 0.06778663396835327
Batch 3/64 loss: 0.051389098167419434
Batch 4/64 loss: 0.10770225524902344
Batch 5/64 loss: 0.0764627456665039
Batch 6/64 loss: 0.05571019649505615
Batch 7/64 loss: 0.04623359441757202
Batch 8/64 loss: 0.06617116928100586
Batch 9/64 loss: 0.11111325025558472
Batch 10/64 loss: 0.07366973161697388
Batch 11/64 loss: 0.07802164554595947
Batch 12/64 loss: 0.08680832386016846
Batch 13/64 loss: 0.05577445030212402
Batch 14/64 loss: 0.049996137619018555
Batch 15/64 loss: 0.06387275457382202
Batch 16/64 loss: 0.07042557001113892
Batch 17/64 loss: 0.06472933292388916
Batch 18/64 loss: 0.05628955364227295
Batch 19/64 loss: 0.06815165281295776
Batch 20/64 loss: 0.058548927307128906
Batch 21/64 loss: 0.09349191188812256
Batch 22/64 loss: 0.0699036717414856
Batch 23/64 loss: 0.05600416660308838
Batch 24/64 loss: 0.05340045690536499
Batch 25/64 loss: 0.042403578758239746
Batch 26/64 loss: 0.06858038902282715
Batch 27/64 loss: 0.07188659906387329
Batch 28/64 loss: 0.06926709413528442
Batch 29/64 loss: 0.05368274450302124
Batch 30/64 loss: 0.07640361785888672
Batch 31/64 loss: 0.04175424575805664
Batch 32/64 loss: 0.0793614387512207
Batch 33/64 loss: 0.0406535267829895
Batch 34/64 loss: 0.08867013454437256
Batch 35/64 loss: 0.06357157230377197
Batch 36/64 loss: 0.04629415273666382
Batch 37/64 loss: 0.054075539112091064
Batch 38/64 loss: 0.055438220500946045
Batch 39/64 loss: 0.05863618850708008
Batch 40/64 loss: 0.09248089790344238
Batch 41/64 loss: 0.060651302337646484
Batch 42/64 loss: 0.07121682167053223
Batch 43/64 loss: 0.060593366622924805
Batch 44/64 loss: 0.07362431287765503
Batch 45/64 loss: 0.07001900672912598
Batch 46/64 loss: 0.09120607376098633
Batch 47/64 loss: 0.07743102312088013
Batch 48/64 loss: 0.08440721035003662
Batch 49/64 loss: 0.06533461809158325
Batch 50/64 loss: 0.05540186166763306
Batch 51/64 loss: 0.08486592769622803
Batch 52/64 loss: 0.06341063976287842
Batch 53/64 loss: 0.06271219253540039
Batch 54/64 loss: 0.05684256553649902
Batch 55/64 loss: 0.04901760816574097
Batch 56/64 loss: 0.05619382858276367
Batch 57/64 loss: 0.05074775218963623
Batch 58/64 loss: 0.08055853843688965
Batch 59/64 loss: 0.07030308246612549
Batch 60/64 loss: 0.055950164794921875
Batch 61/64 loss: 0.06068229675292969
Batch 62/64 loss: 0.05307924747467041
Batch 63/64 loss: 0.04842984676361084
Batch 64/64 loss: 0.05906391143798828
Epoch 221  Train loss: 0.06559769593033137  Val loss: 0.1233035098646105
Epoch 222
-------------------------------
Batch 1/64 loss: 0.06158125400543213
Batch 2/64 loss: 0.057257652282714844
Batch 3/64 loss: 0.06731432676315308
Batch 4/64 loss: 0.0489957332611084
Batch 5/64 loss: 0.05615401268005371
Batch 6/64 loss: 0.07039237022399902
Batch 7/64 loss: 0.08399039506912231
Batch 8/64 loss: 0.02847898006439209
Batch 9/64 loss: 0.06662589311599731
Batch 10/64 loss: 0.07416939735412598
Batch 11/64 loss: 0.054699063301086426
Batch 12/64 loss: 0.06489622592926025
Batch 13/64 loss: 0.057816505432128906
Batch 14/64 loss: 0.1074601411819458
Batch 15/64 loss: 0.04783064126968384
Batch 16/64 loss: 0.07778018712997437
Batch 17/64 loss: 0.03280925750732422
Batch 18/64 loss: 0.05929011106491089
Batch 19/64 loss: 0.05420941114425659
Batch 20/64 loss: 0.06863415241241455
Batch 21/64 loss: 0.05586898326873779
Batch 22/64 loss: 0.07098495960235596
Batch 23/64 loss: 0.06327235698699951
Batch 24/64 loss: 0.05252569913864136
Batch 25/64 loss: 0.08837306499481201
Batch 26/64 loss: 0.07900309562683105
Batch 27/64 loss: 0.0929599404335022
Batch 28/64 loss: 0.08059442043304443
Batch 29/64 loss: 0.05450397729873657
Batch 30/64 loss: 0.06625550985336304
Batch 31/64 loss: 0.05684506893157959
Batch 32/64 loss: 0.06082344055175781
Batch 33/64 loss: 0.08539259433746338
Batch 34/64 loss: 0.05791515111923218
Batch 35/64 loss: 0.08052653074264526
Batch 36/64 loss: 0.06731152534484863
Batch 37/64 loss: 0.06966328620910645
Batch 38/64 loss: 0.06507772207260132
Batch 39/64 loss: 0.08219921588897705
Batch 40/64 loss: 0.0649462342262268
Batch 41/64 loss: 0.04228752851486206
Batch 42/64 loss: 0.060877084732055664
Batch 43/64 loss: 0.04838597774505615
Batch 44/64 loss: 0.06705713272094727
Batch 45/64 loss: 0.09165853261947632
Batch 46/64 loss: 0.06940954923629761
Batch 47/64 loss: 0.04126262664794922
Batch 48/64 loss: 0.07267320156097412
Batch 49/64 loss: 0.046443819999694824
Batch 50/64 loss: 0.04831111431121826
Batch 51/64 loss: 0.04549461603164673
Batch 52/64 loss: 0.05766540765762329
Batch 53/64 loss: 0.0613936185836792
Batch 54/64 loss: 0.053105175495147705
Batch 55/64 loss: 0.05662739276885986
Batch 56/64 loss: 0.04828125238418579
Batch 57/64 loss: 0.062035560607910156
Batch 58/64 loss: 0.09056729078292847
Batch 59/64 loss: 0.06567633152008057
Batch 60/64 loss: 0.053088605403900146
Batch 61/64 loss: 0.09113997220993042
Batch 62/64 loss: 0.09264302253723145
Batch 63/64 loss: 0.08807313442230225
Batch 64/64 loss: 0.1036754846572876
Epoch 222  Train loss: 0.0653700870626113  Val loss: 0.12134436745823864
Epoch 223
-------------------------------
Batch 1/64 loss: 0.07450789213180542
Batch 2/64 loss: 0.07230138778686523
Batch 3/64 loss: 0.05274277925491333
Batch 4/64 loss: 0.0640873908996582
Batch 5/64 loss: 0.05031538009643555
Batch 6/64 loss: 0.04495030641555786
Batch 7/64 loss: 0.05790150165557861
Batch 8/64 loss: 0.06962001323699951
Batch 9/64 loss: 0.05666619539260864
Batch 10/64 loss: 0.06013953685760498
Batch 11/64 loss: 0.05515414476394653
Batch 12/64 loss: 0.02925938367843628
Batch 13/64 loss: 0.07925766706466675
Batch 14/64 loss: 0.08456689119338989
Batch 15/64 loss: 0.03839409351348877
Batch 16/64 loss: 0.05274838209152222
Batch 17/64 loss: 0.07493406534194946
Batch 18/64 loss: 0.041274428367614746
Batch 19/64 loss: 0.05879026651382446
Batch 20/64 loss: 0.05948418378829956
Batch 21/64 loss: 0.056130290031433105
Batch 22/64 loss: 0.09370601177215576
Batch 23/64 loss: 0.08553236722946167
Batch 24/64 loss: 0.09424090385437012
Batch 25/64 loss: 0.03626471757888794
Batch 26/64 loss: 0.06059926748275757
Batch 27/64 loss: 0.06568413972854614
Batch 28/64 loss: 0.0624542236328125
Batch 29/64 loss: 0.05965077877044678
Batch 30/64 loss: 0.0692063570022583
Batch 31/64 loss: 0.06357455253601074
Batch 32/64 loss: 0.08640497922897339
Batch 33/64 loss: 0.07574522495269775
Batch 34/64 loss: 0.06743967533111572
Batch 35/64 loss: 0.08625435829162598
Batch 36/64 loss: 0.06637740135192871
Batch 37/64 loss: 0.07444202899932861
Batch 38/64 loss: 0.06062650680541992
Batch 39/64 loss: 0.0616154670715332
Batch 40/64 loss: 0.07734537124633789
Batch 41/64 loss: 0.05858016014099121
Batch 42/64 loss: 0.08070403337478638
Batch 43/64 loss: 0.08177614212036133
Batch 44/64 loss: 0.05686593055725098
Batch 45/64 loss: 0.05221104621887207
Batch 46/64 loss: 0.06233912706375122
Batch 47/64 loss: 0.0702519416809082
Batch 48/64 loss: 0.05399292707443237
Batch 49/64 loss: 0.03436213731765747
Batch 50/64 loss: 0.0579066276550293
Batch 51/64 loss: 0.0867043137550354
Batch 52/64 loss: 0.07232928276062012
Batch 53/64 loss: 0.07723963260650635
Batch 54/64 loss: 0.052375853061676025
Batch 55/64 loss: 0.047821760177612305
Batch 56/64 loss: 0.05343782901763916
Batch 57/64 loss: 0.04286473989486694
Batch 58/64 loss: 0.08422631025314331
Batch 59/64 loss: 0.11228078603744507
Batch 60/64 loss: 0.06980746984481812
Batch 61/64 loss: 0.08430218696594238
Batch 62/64 loss: 0.07265722751617432
Batch 63/64 loss: 0.08096683025360107
Batch 64/64 loss: 0.05904221534729004
Epoch 223  Train loss: 0.06542237599690755  Val loss: 0.1244197145770096
Epoch 224
-------------------------------
Batch 1/64 loss: 0.07937777042388916
Batch 2/64 loss: 0.07130801677703857
Batch 3/64 loss: 0.0383000373840332
Batch 4/64 loss: 0.06541627645492554
Batch 5/64 loss: 0.062203824520111084
Batch 6/64 loss: 0.08052563667297363
Batch 7/64 loss: 0.08049541711807251
Batch 8/64 loss: 0.0760430097579956
Batch 9/64 loss: 0.09130281209945679
Batch 10/64 loss: 0.07693415880203247
Batch 11/64 loss: 0.08005088567733765
Batch 12/64 loss: 0.05351740121841431
Batch 13/64 loss: 0.07801079750061035
Batch 14/64 loss: 0.06675386428833008
Batch 15/64 loss: 0.08731913566589355
Batch 16/64 loss: 0.07296198606491089
Batch 17/64 loss: 0.04390895366668701
Batch 18/64 loss: 0.06580370664596558
Batch 19/64 loss: 0.09352284669876099
Batch 20/64 loss: 0.0557975172996521
Batch 21/64 loss: 0.05411773920059204
Batch 22/64 loss: 0.0636216402053833
Batch 23/64 loss: 0.08080875873565674
Batch 24/64 loss: 0.05111807584762573
Batch 25/64 loss: 0.07061219215393066
Batch 26/64 loss: 0.03588205575942993
Batch 27/64 loss: 0.08045732975006104
Batch 28/64 loss: 0.04939216375350952
Batch 29/64 loss: 0.03709334135055542
Batch 30/64 loss: 0.06345558166503906
Batch 31/64 loss: 0.04690253734588623
Batch 32/64 loss: 0.07863879203796387
Batch 33/64 loss: 0.0861709713935852
Batch 34/64 loss: 0.10113030672073364
Batch 35/64 loss: 0.061191022396087646
Batch 36/64 loss: 0.0727461576461792
Batch 37/64 loss: 0.04553872346878052
Batch 38/64 loss: 0.06847989559173584
Batch 39/64 loss: 0.048345983028411865
Batch 40/64 loss: 0.09090220928192139
Batch 41/64 loss: 0.04802501201629639
Batch 42/64 loss: 0.06312906742095947
Batch 43/64 loss: 0.07682979106903076
Batch 44/64 loss: 0.07419735193252563
Batch 45/64 loss: 0.07297450304031372
Batch 46/64 loss: 0.0792241096496582
Batch 47/64 loss: 0.07140576839447021
Batch 48/64 loss: 0.05960357189178467
Batch 49/64 loss: 0.04445052146911621
Batch 50/64 loss: 0.05052053928375244
Batch 51/64 loss: 0.08254927396774292
Batch 52/64 loss: 0.046135783195495605
Batch 53/64 loss: 0.052716732025146484
Batch 54/64 loss: 0.05081051588058472
Batch 55/64 loss: 0.08810442686080933
Batch 56/64 loss: 0.06849050521850586
Batch 57/64 loss: 0.05735349655151367
Batch 58/64 loss: 0.07590413093566895
Batch 59/64 loss: 0.061259329319000244
Batch 60/64 loss: 0.06024211645126343
Batch 61/64 loss: 0.0705113410949707
Batch 62/64 loss: 0.06536263227462769
Batch 63/64 loss: 0.037088871002197266
Batch 64/64 loss: 0.07706445455551147
Epoch 224  Train loss: 0.06620936884599574  Val loss: 0.12938395398588934
Epoch 225
-------------------------------
Batch 1/64 loss: 0.09005695581436157
Batch 2/64 loss: 0.05011892318725586
Batch 3/64 loss: 0.06007319688796997
Batch 4/64 loss: 0.07426142692565918
Batch 5/64 loss: 0.09729844331741333
Batch 6/64 loss: 0.06174856424331665
Batch 7/64 loss: 0.06084120273590088
Batch 8/64 loss: 0.0411035418510437
Batch 9/64 loss: 0.04174262285232544
Batch 10/64 loss: 0.06256651878356934
Batch 11/64 loss: 0.07667326927185059
Batch 12/64 loss: 0.06343358755111694
Batch 13/64 loss: 0.04887378215789795
Batch 14/64 loss: 0.038801610469818115
Batch 15/64 loss: 0.08110588788986206
Batch 16/64 loss: 0.08383136987686157
Batch 17/64 loss: 0.05356931686401367
Batch 18/64 loss: 0.0727611780166626
Batch 19/64 loss: 0.07109940052032471
Batch 20/64 loss: 0.05507349967956543
Batch 21/64 loss: 0.06788063049316406
Batch 22/64 loss: 0.06271153688430786
Batch 23/64 loss: 0.028133690357208252
Batch 24/64 loss: 0.049432218074798584
Batch 25/64 loss: 0.07483422756195068
Batch 26/64 loss: 0.046909868717193604
Batch 27/64 loss: 0.0768548846244812
Batch 28/64 loss: 0.08686196804046631
Batch 29/64 loss: 0.08485811948776245
Batch 30/64 loss: 0.04724538326263428
Batch 31/64 loss: 0.06443113088607788
Batch 32/64 loss: 0.09428983926773071
Batch 33/64 loss: 0.06591993570327759
Batch 34/64 loss: 0.08539044857025146
Batch 35/64 loss: 0.041860878467559814
Batch 36/64 loss: 0.060724854469299316
Batch 37/64 loss: 0.05908346176147461
Batch 38/64 loss: 0.07650536298751831
Batch 39/64 loss: 0.08438801765441895
Batch 40/64 loss: 0.0650222897529602
Batch 41/64 loss: 0.09228438138961792
Batch 42/64 loss: 0.05325186252593994
Batch 43/64 loss: 0.07531708478927612
Batch 44/64 loss: 0.06482720375061035
Batch 45/64 loss: 0.0918387770652771
Batch 46/64 loss: 0.030745983123779297
Batch 47/64 loss: 0.0509411096572876
Batch 48/64 loss: 0.08161723613739014
Batch 49/64 loss: 0.05889761447906494
Batch 50/64 loss: 0.08515655994415283
Batch 51/64 loss: 0.06385225057601929
Batch 52/64 loss: 0.11058539152145386
Batch 53/64 loss: 0.05923283100128174
Batch 54/64 loss: 0.05209064483642578
Batch 55/64 loss: 0.09346157312393188
Batch 56/64 loss: 0.0590517520904541
Batch 57/64 loss: 0.0384753942489624
Batch 58/64 loss: 0.02950650453567505
Batch 59/64 loss: 0.0911819338798523
Batch 60/64 loss: 0.034542202949523926
Batch 61/64 loss: 0.06521540880203247
Batch 62/64 loss: 0.054368793964385986
Batch 63/64 loss: 0.06585335731506348
Batch 64/64 loss: 0.07199108600616455
Epoch 225  Train loss: 0.06532803470013189  Val loss: 0.12913594274586418
Epoch 226
-------------------------------
Batch 1/64 loss: 0.04986083507537842
Batch 2/64 loss: 0.08170080184936523
Batch 3/64 loss: 0.0776294469833374
Batch 4/64 loss: 0.08592480421066284
Batch 5/64 loss: 0.04057025909423828
Batch 6/64 loss: 0.05004245042800903
Batch 7/64 loss: 0.05725502967834473
Batch 8/64 loss: 0.06074833869934082
Batch 9/64 loss: 0.06490492820739746
Batch 10/64 loss: 0.07976704835891724
Batch 11/64 loss: 0.09604471921920776
Batch 12/64 loss: 0.0601615309715271
Batch 13/64 loss: 0.05407750606536865
Batch 14/64 loss: 0.04875153303146362
Batch 15/64 loss: 0.05412435531616211
Batch 16/64 loss: 0.05682092905044556
Batch 17/64 loss: 0.04363977909088135
Batch 18/64 loss: 0.07992655038833618
Batch 19/64 loss: 0.04018622636795044
Batch 20/64 loss: 0.09172683954238892
Batch 21/64 loss: 0.06130927801132202
Batch 22/64 loss: 0.051752448081970215
Batch 23/64 loss: 0.07659143209457397
Batch 24/64 loss: 0.06391948461532593
Batch 25/64 loss: 0.08462518453598022
Batch 26/64 loss: 0.03809863328933716
Batch 27/64 loss: 0.06374907493591309
Batch 28/64 loss: 0.07261157035827637
Batch 29/64 loss: 0.10245990753173828
Batch 30/64 loss: 0.0364498496055603
Batch 31/64 loss: 0.043390750885009766
Batch 32/64 loss: 0.04222548007965088
Batch 33/64 loss: 0.0464288592338562
Batch 34/64 loss: 0.058037638664245605
Batch 35/64 loss: 0.09101420640945435
Batch 36/64 loss: 0.062216103076934814
Batch 37/64 loss: 0.07233631610870361
Batch 38/64 loss: 0.06880128383636475
Batch 39/64 loss: 0.08059769868850708
Batch 40/64 loss: 0.0874602198600769
Batch 41/64 loss: 0.06546390056610107
Batch 42/64 loss: 0.04467397928237915
Batch 43/64 loss: 0.05516839027404785
Batch 44/64 loss: 0.06431090831756592
Batch 45/64 loss: 0.05640608072280884
Batch 46/64 loss: 0.07055968046188354
Batch 47/64 loss: 0.07795929908752441
Batch 48/64 loss: 0.03075951337814331
Batch 49/64 loss: 0.07182025909423828
Batch 50/64 loss: 0.0548856258392334
Batch 51/64 loss: 0.0600205659866333
Batch 52/64 loss: 0.0471036434173584
Batch 53/64 loss: 0.073403000831604
Batch 54/64 loss: 0.05286872386932373
Batch 55/64 loss: 0.07621067762374878
Batch 56/64 loss: 0.03639054298400879
Batch 57/64 loss: 0.08696317672729492
Batch 58/64 loss: 0.08352190256118774
Batch 59/64 loss: 0.07915002107620239
Batch 60/64 loss: 0.09162086248397827
Batch 61/64 loss: 0.05289655923843384
Batch 62/64 loss: 0.0434303879737854
Batch 63/64 loss: 0.07590848207473755
Batch 64/64 loss: 0.09486496448516846
Epoch 226  Train loss: 0.0643228900198843  Val loss: 0.12276110247648049
Epoch 227
-------------------------------
Batch 1/64 loss: 0.03910034894943237
Batch 2/64 loss: 0.06472146511077881
Batch 3/64 loss: 0.09439927339553833
Batch 4/64 loss: 0.07859539985656738
Batch 5/64 loss: 0.07305628061294556
Batch 6/64 loss: 0.06035560369491577
Batch 7/64 loss: 0.06906718015670776
Batch 8/64 loss: 0.03974425792694092
Batch 9/64 loss: 0.06376856565475464
Batch 10/64 loss: 0.07330811023712158
Batch 11/64 loss: 0.04576826095581055
Batch 12/64 loss: 0.0682639479637146
Batch 13/64 loss: 0.04445070028305054
Batch 14/64 loss: 0.07039082050323486
Batch 15/64 loss: 0.05839496850967407
Batch 16/64 loss: 0.09071534872055054
Batch 17/64 loss: 0.08660924434661865
Batch 18/64 loss: 0.08643704652786255
Batch 19/64 loss: 0.058242619037628174
Batch 20/64 loss: 0.08506453037261963
Batch 21/64 loss: 0.06081032752990723
Batch 22/64 loss: 0.06624370813369751
Batch 23/64 loss: 0.05502486228942871
Batch 24/64 loss: 0.045639634132385254
Batch 25/64 loss: 0.053575098514556885
Batch 26/64 loss: 0.033555805683135986
Batch 27/64 loss: 0.059316039085388184
Batch 28/64 loss: 0.07571762800216675
Batch 29/64 loss: 0.10553926229476929
Batch 30/64 loss: 0.07249903678894043
Batch 31/64 loss: 0.05655491352081299
Batch 32/64 loss: 0.05195033550262451
Batch 33/64 loss: 0.02581942081451416
Batch 34/64 loss: 0.06960940361022949
Batch 35/64 loss: 0.05724179744720459
Batch 36/64 loss: 0.0838279128074646
Batch 37/64 loss: 0.07554805278778076
Batch 38/64 loss: 0.07716178894042969
Batch 39/64 loss: 0.05598771572113037
Batch 40/64 loss: 0.0668225884437561
Batch 41/64 loss: 0.056726932525634766
Batch 42/64 loss: 0.04101717472076416
Batch 43/64 loss: 0.07052791118621826
Batch 44/64 loss: 0.059590935707092285
Batch 45/64 loss: 0.06867039203643799
Batch 46/64 loss: 0.08046156167984009
Batch 47/64 loss: 0.05678457021713257
Batch 48/64 loss: 0.022290527820587158
Batch 49/64 loss: 0.03657883405685425
Batch 50/64 loss: 0.056411683559417725
Batch 51/64 loss: 0.06052464246749878
Batch 52/64 loss: 0.06572729349136353
Batch 53/64 loss: 0.08236730098724365
Batch 54/64 loss: 0.07635712623596191
Batch 55/64 loss: 0.0693669319152832
Batch 56/64 loss: 0.09051483869552612
Batch 57/64 loss: 0.05744123458862305
Batch 58/64 loss: 0.05245506763458252
Batch 59/64 loss: 0.05654942989349365
Batch 60/64 loss: 0.05566227436065674
Batch 61/64 loss: 0.06793355941772461
Batch 62/64 loss: 0.09171414375305176
Batch 63/64 loss: 0.07988440990447998
Batch 64/64 loss: 0.08351713418960571
Epoch 227  Train loss: 0.06458189697826609  Val loss: 0.1299315957269308
Epoch 228
-------------------------------
Batch 1/64 loss: 0.0657767653465271
Batch 2/64 loss: 0.07077693939208984
Batch 3/64 loss: 0.06801462173461914
Batch 4/64 loss: 0.08370411396026611
Batch 5/64 loss: 0.036797940731048584
Batch 6/64 loss: 0.08341747522354126
Batch 7/64 loss: 0.06559360027313232
Batch 8/64 loss: 0.11017942428588867
Batch 9/64 loss: 0.04425179958343506
Batch 10/64 loss: 0.043437182903289795
Batch 11/64 loss: 0.06107449531555176
Batch 12/64 loss: 0.08869588375091553
Batch 13/64 loss: 0.07281571626663208
Batch 14/64 loss: 0.07397383451461792
Batch 15/64 loss: 0.055796146392822266
Batch 16/64 loss: 0.08465683460235596
Batch 17/64 loss: 0.06722736358642578
Batch 18/64 loss: 0.06205856800079346
Batch 19/64 loss: 0.04411429166793823
Batch 20/64 loss: 0.03944438695907593
Batch 21/64 loss: 0.04537612199783325
Batch 22/64 loss: 0.09058135747909546
Batch 23/64 loss: 0.09390246868133545
Batch 24/64 loss: 0.05404460430145264
Batch 25/64 loss: 0.06492066383361816
Batch 26/64 loss: 0.06949818134307861
Batch 27/64 loss: 0.049817442893981934
Batch 28/64 loss: 0.0637049674987793
Batch 29/64 loss: 0.08372682332992554
Batch 30/64 loss: 0.05248713493347168
Batch 31/64 loss: 0.047646164894104004
Batch 32/64 loss: 0.06429505348205566
Batch 33/64 loss: 0.03009587526321411
Batch 34/64 loss: 0.0493088960647583
Batch 35/64 loss: 0.06640446186065674
Batch 36/64 loss: 0.0704575777053833
Batch 37/64 loss: 0.05418694019317627
Batch 38/64 loss: 0.06091630458831787
Batch 39/64 loss: 0.05630624294281006
Batch 40/64 loss: 0.058712661266326904
Batch 41/64 loss: 0.05207693576812744
Batch 42/64 loss: 0.0451200008392334
Batch 43/64 loss: 0.04859590530395508
Batch 44/64 loss: 0.0760190486907959
Batch 45/64 loss: 0.06546109914779663
Batch 46/64 loss: 0.06648552417755127
Batch 47/64 loss: 0.059360504150390625
Batch 48/64 loss: 0.08813321590423584
Batch 49/64 loss: 0.06747347116470337
Batch 50/64 loss: 0.11435407400131226
Batch 51/64 loss: 0.06336241960525513
Batch 52/64 loss: 0.06749540567398071
Batch 53/64 loss: 0.0598529577255249
Batch 54/64 loss: 0.02726113796234131
Batch 55/64 loss: 0.07094520330429077
Batch 56/64 loss: 0.0783616304397583
Batch 57/64 loss: 0.06015586853027344
Batch 58/64 loss: 0.06222832202911377
Batch 59/64 loss: 0.055764973163604736
Batch 60/64 loss: 0.052816689014434814
Batch 61/64 loss: 0.05910766124725342
Batch 62/64 loss: 0.05190885066986084
Batch 63/64 loss: 0.06095701456069946
Batch 64/64 loss: 0.07728207111358643
Epoch 228  Train loss: 0.06367775365418078  Val loss: 0.12208638772931706
Epoch 229
-------------------------------
Batch 1/64 loss: 0.0821191668510437
Batch 2/64 loss: 0.05459296703338623
Batch 3/64 loss: 0.06400108337402344
Batch 4/64 loss: 0.04542100429534912
Batch 5/64 loss: 0.05746668577194214
Batch 6/64 loss: 0.05299109220504761
Batch 7/64 loss: 0.04302561283111572
Batch 8/64 loss: 0.06109267473220825
Batch 9/64 loss: 0.07768440246582031
Batch 10/64 loss: 0.06874507665634155
Batch 11/64 loss: 0.05124378204345703
Batch 12/64 loss: 0.056328535079956055
Batch 13/64 loss: 0.06799238920211792
Batch 14/64 loss: 0.07398563623428345
Batch 15/64 loss: 0.07888209819793701
Batch 16/64 loss: 0.06150329113006592
Batch 17/64 loss: 0.06296783685684204
Batch 18/64 loss: 0.07584595680236816
Batch 19/64 loss: 0.053313374519348145
Batch 20/64 loss: 0.08519172668457031
Batch 21/64 loss: 0.06136816740036011
Batch 22/64 loss: 0.05916619300842285
Batch 23/64 loss: 0.03806513547897339
Batch 24/64 loss: 0.06064927577972412
Batch 25/64 loss: 0.04387784004211426
Batch 26/64 loss: 0.05129384994506836
Batch 27/64 loss: 0.05880153179168701
Batch 28/64 loss: 0.04833155870437622
Batch 29/64 loss: 0.07523810863494873
Batch 30/64 loss: 0.06478369235992432
Batch 31/64 loss: 0.05253785848617554
Batch 32/64 loss: 0.06829214096069336
Batch 33/64 loss: 0.04951441287994385
Batch 34/64 loss: 0.06157153844833374
Batch 35/64 loss: 0.08348989486694336
Batch 36/64 loss: 0.08638805150985718
Batch 37/64 loss: 0.08268362283706665
Batch 38/64 loss: 0.055757761001586914
Batch 39/64 loss: 0.06652969121932983
Batch 40/64 loss: 0.08618927001953125
Batch 41/64 loss: 0.05554032325744629
Batch 42/64 loss: 0.04816913604736328
Batch 43/64 loss: 0.055223286151885986
Batch 44/64 loss: 0.061539530754089355
Batch 45/64 loss: 0.07006722688674927
Batch 46/64 loss: 0.0770559310913086
Batch 47/64 loss: 0.10114479064941406
Batch 48/64 loss: 0.06004536151885986
Batch 49/64 loss: 0.06616091728210449
Batch 50/64 loss: 0.09693849086761475
Batch 51/64 loss: 0.04650294780731201
Batch 52/64 loss: 0.034129440784454346
Batch 53/64 loss: 0.08720004558563232
Batch 54/64 loss: 0.065959632396698
Batch 55/64 loss: 0.05072057247161865
Batch 56/64 loss: 0.050303876399993896
Batch 57/64 loss: 0.07142311334609985
Batch 58/64 loss: 0.061202943325042725
Batch 59/64 loss: 0.06779211759567261
Batch 60/64 loss: 0.04482710361480713
Batch 61/64 loss: 0.052529335021972656
Batch 62/64 loss: 0.06641983985900879
Batch 63/64 loss: 0.04988819360733032
Batch 64/64 loss: 0.07684445381164551
Epoch 229  Train loss: 0.06317398407879998  Val loss: 0.1249178015079695
Epoch 230
-------------------------------
Batch 1/64 loss: 0.0774272084236145
Batch 2/64 loss: 0.04936927556991577
Batch 3/64 loss: 0.057969748973846436
Batch 4/64 loss: 0.05690723657608032
Batch 5/64 loss: 0.025135397911071777
Batch 6/64 loss: 0.06590849161148071
Batch 7/64 loss: 0.07278358936309814
Batch 8/64 loss: 0.040243446826934814
Batch 9/64 loss: 0.07107549905776978
Batch 10/64 loss: 0.05180084705352783
Batch 11/64 loss: 0.0824083685874939
Batch 12/64 loss: 0.040937840938568115
Batch 13/64 loss: 0.06535571813583374
Batch 14/64 loss: 0.0644192099571228
Batch 15/64 loss: 0.08709937334060669
Batch 16/64 loss: 0.048502326011657715
Batch 17/64 loss: 0.02908635139465332
Batch 18/64 loss: 0.059799134731292725
Batch 19/64 loss: 0.055014967918395996
Batch 20/64 loss: 0.06679463386535645
Batch 21/64 loss: 0.06722009181976318
Batch 22/64 loss: 0.10744863748550415
Batch 23/64 loss: 0.041184425354003906
Batch 24/64 loss: 0.07346200942993164
Batch 25/64 loss: 0.04434525966644287
Batch 26/64 loss: 0.052735209465026855
Batch 27/64 loss: 0.05654209852218628
Batch 28/64 loss: 0.07202649116516113
Batch 29/64 loss: 0.08485037088394165
Batch 30/64 loss: 0.0606842041015625
Batch 31/64 loss: 0.06863325834274292
Batch 32/64 loss: 0.06630778312683105
Batch 33/64 loss: 0.07985317707061768
Batch 34/64 loss: 0.04759550094604492
Batch 35/64 loss: 0.07109057903289795
Batch 36/64 loss: 0.042778849601745605
Batch 37/64 loss: 0.08579838275909424
Batch 38/64 loss: 0.06372380256652832
Batch 39/64 loss: 0.0841023325920105
Batch 40/64 loss: 0.02700573205947876
Batch 41/64 loss: 0.04926806688308716
Batch 42/64 loss: 0.07848435640335083
Batch 43/64 loss: 0.08006429672241211
Batch 44/64 loss: 0.0686153769493103
Batch 45/64 loss: 0.0667831301689148
Batch 46/64 loss: 0.06256306171417236
Batch 47/64 loss: 0.047135114669799805
Batch 48/64 loss: 0.048606157302856445
Batch 49/64 loss: 0.07903236150741577
Batch 50/64 loss: 0.05941039323806763
Batch 51/64 loss: 0.05826777219772339
Batch 52/64 loss: 0.10841917991638184
Batch 53/64 loss: 0.03312593698501587
Batch 54/64 loss: 0.07035291194915771
Batch 55/64 loss: 0.061728715896606445
Batch 56/64 loss: 0.047753214836120605
Batch 57/64 loss: 0.05462723970413208
Batch 58/64 loss: 0.06611794233322144
Batch 59/64 loss: 0.06434381008148193
Batch 60/64 loss: 0.049279749393463135
Batch 61/64 loss: 0.09903311729431152
Batch 62/64 loss: 0.04980200529098511
Batch 63/64 loss: 0.07275623083114624
Batch 64/64 loss: 0.059313297271728516
Epoch 230  Train loss: 0.06251736435235715  Val loss: 0.12236330845102002
Epoch 231
-------------------------------
Batch 1/64 loss: 0.0662682056427002
Batch 2/64 loss: 0.061766743659973145
Batch 3/64 loss: 0.05630451440811157
Batch 4/64 loss: 0.05264937877655029
Batch 5/64 loss: 0.0634416937828064
Batch 6/64 loss: 0.07918399572372437
Batch 7/64 loss: 0.04038304090499878
Batch 8/64 loss: 0.048206329345703125
Batch 9/64 loss: 0.02929621934890747
Batch 10/64 loss: 0.06562072038650513
Batch 11/64 loss: 0.061627089977264404
Batch 12/64 loss: 0.05451864004135132
Batch 13/64 loss: 0.0839986801147461
Batch 14/64 loss: 0.07300031185150146
Batch 15/64 loss: 0.06361126899719238
Batch 16/64 loss: 0.07026034593582153
Batch 17/64 loss: 0.07969868183135986
Batch 18/64 loss: 0.06816869974136353
Batch 19/64 loss: 0.0767587423324585
Batch 20/64 loss: 0.04979950189590454
Batch 21/64 loss: 0.06878417730331421
Batch 22/64 loss: 0.039381980895996094
Batch 23/64 loss: 0.09330976009368896
Batch 24/64 loss: 0.03876817226409912
Batch 25/64 loss: 0.05224531888961792
Batch 26/64 loss: 0.05019146203994751
Batch 27/64 loss: 0.06354677677154541
Batch 28/64 loss: 0.07931727170944214
Batch 29/64 loss: 0.0719483494758606
Batch 30/64 loss: 0.056351542472839355
Batch 31/64 loss: 0.08913564682006836
Batch 32/64 loss: 0.050670325756073
Batch 33/64 loss: 0.06117373704910278
Batch 34/64 loss: 0.07360607385635376
Batch 35/64 loss: 0.05875968933105469
Batch 36/64 loss: 0.054007530212402344
Batch 37/64 loss: 0.06491762399673462
Batch 38/64 loss: 0.06447809934616089
Batch 39/64 loss: 0.04880768060684204
Batch 40/64 loss: 0.05638837814331055
Batch 41/64 loss: 0.06683486700057983
Batch 42/64 loss: 0.055982232093811035
Batch 43/64 loss: 0.0441741943359375
Batch 44/64 loss: 0.07784408330917358
Batch 45/64 loss: 0.05487263202667236
Batch 46/64 loss: 0.053990840911865234
Batch 47/64 loss: 0.03183931112289429
Batch 48/64 loss: 0.058111369609832764
Batch 49/64 loss: 0.11372381448745728
Batch 50/64 loss: 0.07379001379013062
Batch 51/64 loss: 0.09033781290054321
Batch 52/64 loss: 0.04356151819229126
Batch 53/64 loss: 0.0684366226196289
Batch 54/64 loss: 0.06729936599731445
Batch 55/64 loss: 0.053852975368499756
Batch 56/64 loss: 0.06992602348327637
Batch 57/64 loss: 0.06861954927444458
Batch 58/64 loss: 0.05376666784286499
Batch 59/64 loss: 0.06359332799911499
Batch 60/64 loss: 0.06196784973144531
Batch 61/64 loss: 0.053264737129211426
Batch 62/64 loss: 0.05165785551071167
Batch 63/64 loss: 0.06306016445159912
Batch 64/64 loss: 0.05714648962020874
Epoch 231  Train loss: 0.06217600153941734  Val loss: 0.12572252238329334
Epoch 232
-------------------------------
Batch 1/64 loss: 0.04336273670196533
Batch 2/64 loss: 0.05515629053115845
Batch 3/64 loss: 0.06293374300003052
Batch 4/64 loss: 0.07058548927307129
Batch 5/64 loss: 0.05095696449279785
Batch 6/64 loss: 0.051619112491607666
Batch 7/64 loss: 0.06367576122283936
Batch 8/64 loss: 0.03499901294708252
Batch 9/64 loss: 0.06177103519439697
Batch 10/64 loss: 0.0456240177154541
Batch 11/64 loss: 0.05031728744506836
Batch 12/64 loss: 0.060434162616729736
Batch 13/64 loss: 0.06040245294570923
Batch 14/64 loss: 0.08415108919143677
Batch 15/64 loss: 0.07988572120666504
Batch 16/64 loss: 0.043734192848205566
Batch 17/64 loss: 0.08088386058807373
Batch 18/64 loss: 0.06790542602539062
Batch 19/64 loss: 0.06270378828048706
Batch 20/64 loss: 0.06426483392715454
Batch 21/64 loss: 0.049867451190948486
Batch 22/64 loss: 0.05715763568878174
Batch 23/64 loss: 0.05238252878189087
Batch 24/64 loss: 0.06296712160110474
Batch 25/64 loss: 0.07034319639205933
Batch 26/64 loss: 0.052678704261779785
Batch 27/64 loss: 0.05918526649475098
Batch 28/64 loss: 0.08476525545120239
Batch 29/64 loss: 0.04338032007217407
Batch 30/64 loss: 0.07308173179626465
Batch 31/64 loss: 0.09385865926742554
Batch 32/64 loss: 0.0626533031463623
Batch 33/64 loss: 0.06241720914840698
Batch 34/64 loss: 0.0638047456741333
Batch 35/64 loss: 0.06432926654815674
Batch 36/64 loss: 0.04998326301574707
Batch 37/64 loss: 0.04061073064804077
Batch 38/64 loss: 0.06699883937835693
Batch 39/64 loss: 0.06863212585449219
Batch 40/64 loss: 0.0613713264465332
Batch 41/64 loss: 0.05430912971496582
Batch 42/64 loss: 0.09564381837844849
Batch 43/64 loss: 0.10969424247741699
Batch 44/64 loss: 0.06570005416870117
Batch 45/64 loss: 0.0410768985748291
Batch 46/64 loss: 0.03599381446838379
Batch 47/64 loss: 0.05226081609725952
Batch 48/64 loss: 0.057811737060546875
Batch 49/64 loss: 0.047349750995635986
Batch 50/64 loss: 0.06438684463500977
Batch 51/64 loss: 0.05283832550048828
Batch 52/64 loss: 0.07919025421142578
Batch 53/64 loss: 0.0665508508682251
Batch 54/64 loss: 0.06554156541824341
Batch 55/64 loss: 0.07389038801193237
Batch 56/64 loss: 0.05918121337890625
Batch 57/64 loss: 0.05808448791503906
Batch 58/64 loss: 0.07289677858352661
Batch 59/64 loss: 0.06182032823562622
Batch 60/64 loss: 0.07449984550476074
Batch 61/64 loss: 0.07509249448776245
Batch 62/64 loss: 0.058283329010009766
Batch 63/64 loss: 0.036475539207458496
Batch 64/64 loss: 0.0727546215057373
Epoch 232  Train loss: 0.0619446922751034  Val loss: 0.12281584063756097
Epoch 233
-------------------------------
Batch 1/64 loss: 0.049619317054748535
Batch 2/64 loss: 0.08563965559005737
Batch 3/64 loss: 0.052652060985565186
Batch 4/64 loss: 0.039879560470581055
Batch 5/64 loss: 0.07561403512954712
Batch 6/64 loss: 0.0800623893737793
Batch 7/64 loss: 0.0503198504447937
Batch 8/64 loss: 0.03448820114135742
Batch 9/64 loss: 0.07529276609420776
Batch 10/64 loss: 0.03241133689880371
Batch 11/64 loss: 0.046096086502075195
Batch 12/64 loss: 0.05521702766418457
Batch 13/64 loss: 0.05619841814041138
Batch 14/64 loss: 0.06421518325805664
Batch 15/64 loss: 0.033999860286712646
Batch 16/64 loss: 0.043714702129364014
Batch 17/64 loss: 0.06965756416320801
Batch 18/64 loss: 0.04246413707733154
Batch 19/64 loss: 0.05146634578704834
Batch 20/64 loss: 0.05105060338973999
Batch 21/64 loss: 0.06565052270889282
Batch 22/64 loss: 0.048714399337768555
Batch 23/64 loss: 0.04370206594467163
Batch 24/64 loss: 0.0586017370223999
Batch 25/64 loss: 0.08437514305114746
Batch 26/64 loss: 0.08200305700302124
Batch 27/64 loss: 0.06704413890838623
Batch 28/64 loss: 0.057137370109558105
Batch 29/64 loss: 0.05618166923522949
Batch 30/64 loss: 0.06849133968353271
Batch 31/64 loss: 0.0645904541015625
Batch 32/64 loss: 0.05618894100189209
Batch 33/64 loss: 0.07683157920837402
Batch 34/64 loss: 0.0661923885345459
Batch 35/64 loss: 0.06867074966430664
Batch 36/64 loss: 0.05537748336791992
Batch 37/64 loss: 0.08123838901519775
Batch 38/64 loss: 0.059996724128723145
Batch 39/64 loss: 0.06098979711532593
Batch 40/64 loss: 0.06453359127044678
Batch 41/64 loss: 0.09654277563095093
Batch 42/64 loss: 0.09342217445373535
Batch 43/64 loss: 0.04984694719314575
Batch 44/64 loss: 0.03542298078536987
Batch 45/64 loss: 0.06973028182983398
Batch 46/64 loss: 0.06606841087341309
Batch 47/64 loss: 0.03418684005737305
Batch 48/64 loss: 0.08065277338027954
Batch 49/64 loss: 0.07645595073699951
Batch 50/64 loss: 0.07081848382949829
Batch 51/64 loss: 0.08093172311782837
Batch 52/64 loss: 0.051849305629730225
Batch 53/64 loss: 0.07742005586624146
Batch 54/64 loss: 0.06968122720718384
Batch 55/64 loss: 0.056066036224365234
Batch 56/64 loss: 0.07028663158416748
Batch 57/64 loss: 0.039720892906188965
Batch 58/64 loss: 0.051662564277648926
Batch 59/64 loss: 0.07801824808120728
Batch 60/64 loss: 0.02884995937347412
Batch 61/64 loss: 0.048962175846099854
Batch 62/64 loss: 0.06899386644363403
Batch 63/64 loss: 0.07376158237457275
Batch 64/64 loss: 0.05748361349105835
Epoch 233  Train loss: 0.06100447435005038  Val loss: 0.12515657607632405
Epoch 234
-------------------------------
Batch 1/64 loss: 0.06712979078292847
Batch 2/64 loss: 0.05694085359573364
Batch 3/64 loss: 0.05317378044128418
Batch 4/64 loss: 0.03417384624481201
Batch 5/64 loss: 0.07428383827209473
Batch 6/64 loss: 0.06000375747680664
Batch 7/64 loss: 0.07852929830551147
Batch 8/64 loss: 0.08088487386703491
Batch 9/64 loss: 0.06864213943481445
Batch 10/64 loss: 0.08142828941345215
Batch 11/64 loss: 0.039133548736572266
Batch 12/64 loss: 0.04894447326660156
Batch 13/64 loss: 0.060401082038879395
Batch 14/64 loss: 0.04704159498214722
Batch 15/64 loss: 0.05316495895385742
Batch 16/64 loss: 0.07867372035980225
Batch 17/64 loss: 0.060982704162597656
Batch 18/64 loss: 0.039275944232940674
Batch 19/64 loss: 0.0661698579788208
Batch 20/64 loss: 0.0739908218383789
Batch 21/64 loss: 0.05905580520629883
Batch 22/64 loss: 0.06173127889633179
Batch 23/64 loss: 0.06819719076156616
Batch 24/64 loss: 0.06352907419204712
Batch 25/64 loss: 0.04394787549972534
Batch 26/64 loss: 0.07561922073364258
Batch 27/64 loss: 0.03855264186859131
Batch 28/64 loss: 0.08099830150604248
Batch 29/64 loss: 0.07565182447433472
Batch 30/64 loss: 0.07911372184753418
Batch 31/64 loss: 0.07652342319488525
Batch 32/64 loss: 0.030421078205108643
Batch 33/64 loss: 0.07736694812774658
Batch 34/64 loss: 0.04886281490325928
Batch 35/64 loss: 0.05092000961303711
Batch 36/64 loss: 0.05295497179031372
Batch 37/64 loss: 0.024893224239349365
Batch 38/64 loss: 0.039783477783203125
Batch 39/64 loss: 0.03758615255355835
Batch 40/64 loss: 0.04470622539520264
Batch 41/64 loss: 0.0564802885055542
Batch 42/64 loss: 0.0605807900428772
Batch 43/64 loss: 0.04419875144958496
Batch 44/64 loss: 0.06887638568878174
Batch 45/64 loss: 0.08090317249298096
Batch 46/64 loss: 0.05448329448699951
Batch 47/64 loss: 0.056603074073791504
Batch 48/64 loss: 0.08098733425140381
Batch 49/64 loss: 0.038293540477752686
Batch 50/64 loss: 0.0802655816078186
Batch 51/64 loss: 0.08736389875411987
Batch 52/64 loss: 0.09024405479431152
Batch 53/64 loss: 0.08035504817962646
Batch 54/64 loss: 0.05425912141799927
Batch 55/64 loss: 0.09887373447418213
Batch 56/64 loss: 0.045972585678100586
Batch 57/64 loss: 0.07777845859527588
Batch 58/64 loss: 0.07087796926498413
Batch 59/64 loss: 0.07439851760864258
Batch 60/64 loss: 0.07919645309448242
Batch 61/64 loss: 0.07164359092712402
Batch 62/64 loss: 0.05146431922912598
Batch 63/64 loss: 0.07328414916992188
Batch 64/64 loss: 0.0825689435005188
Epoch 234  Train loss: 0.06263051383635578  Val loss: 0.12256376972722843
Epoch 235
-------------------------------
Batch 1/64 loss: 0.06587666273117065
Batch 2/64 loss: 0.09377610683441162
Batch 3/64 loss: 0.057634830474853516
Batch 4/64 loss: 0.05286049842834473
Batch 5/64 loss: 0.06217217445373535
Batch 6/64 loss: 0.06794697046279907
Batch 7/64 loss: 0.07415223121643066
Batch 8/64 loss: 0.05502605438232422
Batch 9/64 loss: 0.05245494842529297
Batch 10/64 loss: 0.05141305923461914
Batch 11/64 loss: 0.046486735343933105
Batch 12/64 loss: 0.08376681804656982
Batch 13/64 loss: 0.05948936939239502
Batch 14/64 loss: 0.05571657419204712
Batch 15/64 loss: 0.05359768867492676
Batch 16/64 loss: 0.08103770017623901
Batch 17/64 loss: 0.06500029563903809
Batch 18/64 loss: 0.07334887981414795
Batch 19/64 loss: 0.06180083751678467
Batch 20/64 loss: 0.04244589805603027
Batch 21/64 loss: 0.0792546272277832
Batch 22/64 loss: 0.06152629852294922
Batch 23/64 loss: 0.047001540660858154
Batch 24/64 loss: 0.06152904033660889
Batch 25/64 loss: 0.05345892906188965
Batch 26/64 loss: 0.06557726860046387
Batch 27/64 loss: 0.045210301876068115
Batch 28/64 loss: 0.07116109132766724
Batch 29/64 loss: 0.0774419903755188
Batch 30/64 loss: 0.0666424036026001
Batch 31/64 loss: 0.06179612874984741
Batch 32/64 loss: 0.046720266342163086
Batch 33/64 loss: 0.057849884033203125
Batch 34/64 loss: 0.059727489948272705
Batch 35/64 loss: 0.05963391065597534
Batch 36/64 loss: 0.0676349401473999
Batch 37/64 loss: 0.06580537557601929
Batch 38/64 loss: 0.03310370445251465
Batch 39/64 loss: 0.08323723077774048
Batch 40/64 loss: 0.06820732355117798
Batch 41/64 loss: 0.053913414478302
Batch 42/64 loss: 0.04584997892379761
Batch 43/64 loss: 0.03649258613586426
Batch 44/64 loss: 0.07170021533966064
Batch 45/64 loss: 0.041340529918670654
Batch 46/64 loss: 0.04157060384750366
Batch 47/64 loss: 0.05592060089111328
Batch 48/64 loss: 0.05830484628677368
Batch 49/64 loss: 0.0588756799697876
Batch 50/64 loss: 0.09978282451629639
Batch 51/64 loss: 0.07292628288269043
Batch 52/64 loss: 0.04701274633407593
Batch 53/64 loss: 0.04439663887023926
Batch 54/64 loss: 0.08109676837921143
Batch 55/64 loss: 0.06891614198684692
Batch 56/64 loss: 0.08006197214126587
Batch 57/64 loss: 0.06445962190628052
Batch 58/64 loss: 0.0681542158126831
Batch 59/64 loss: 0.05652809143066406
Batch 60/64 loss: 0.04477411508560181
Batch 61/64 loss: 0.056792616844177246
Batch 62/64 loss: 0.05908924341201782
Batch 63/64 loss: 0.045027852058410645
Batch 64/64 loss: 0.08202004432678223
Epoch 235  Train loss: 0.06122394823560528  Val loss: 0.11925308134957277
Saving best model, epoch: 235
Epoch 236
-------------------------------
Batch 1/64 loss: 0.059715092182159424
Batch 2/64 loss: 0.05095219612121582
Batch 3/64 loss: 0.0759209394454956
Batch 4/64 loss: 0.05090749263763428
Batch 5/64 loss: 0.06751978397369385
Batch 6/64 loss: 0.07406800985336304
Batch 7/64 loss: 0.05624508857727051
Batch 8/64 loss: 0.02254652976989746
Batch 9/64 loss: 0.044011712074279785
Batch 10/64 loss: 0.07264411449432373
Batch 11/64 loss: 0.07576614618301392
Batch 12/64 loss: 0.048973917961120605
Batch 13/64 loss: 0.08050310611724854
Batch 14/64 loss: 0.043905019760131836
Batch 15/64 loss: 0.041204094886779785
Batch 16/64 loss: 0.08357173204421997
Batch 17/64 loss: 0.04796189069747925
Batch 18/64 loss: 0.04287451505661011
Batch 19/64 loss: 0.06479805707931519
Batch 20/64 loss: 0.06603479385375977
Batch 21/64 loss: 0.022995471954345703
Batch 22/64 loss: 0.08300459384918213
Batch 23/64 loss: 0.09819316864013672
Batch 24/64 loss: 0.027541518211364746
Batch 25/64 loss: 0.04619413614273071
Batch 26/64 loss: 0.06691920757293701
Batch 27/64 loss: 0.057458460330963135
Batch 28/64 loss: 0.07723128795623779
Batch 29/64 loss: 0.05517846345901489
Batch 30/64 loss: 0.04969596862792969
Batch 31/64 loss: 0.061071813106536865
Batch 32/64 loss: 0.052655041217803955
Batch 33/64 loss: 0.08015000820159912
Batch 34/64 loss: 0.07223618030548096
Batch 35/64 loss: 0.05020427703857422
Batch 36/64 loss: 0.0664224624633789
Batch 37/64 loss: 0.044327378273010254
Batch 38/64 loss: 0.04603201150894165
Batch 39/64 loss: 0.05368149280548096
Batch 40/64 loss: 0.05406397581100464
Batch 41/64 loss: 0.07415884733200073
Batch 42/64 loss: 0.06588989496231079
Batch 43/64 loss: 0.0689859390258789
Batch 44/64 loss: 0.07307803630828857
Batch 45/64 loss: 0.058579981327056885
Batch 46/64 loss: 0.08977913856506348
Batch 47/64 loss: 0.062298715114593506
Batch 48/64 loss: 0.08451539278030396
Batch 49/64 loss: 0.06695592403411865
Batch 50/64 loss: 0.10086220502853394
Batch 51/64 loss: 0.04212665557861328
Batch 52/64 loss: 0.05163538455963135
Batch 53/64 loss: 0.05101513862609863
Batch 54/64 loss: 0.043317317962646484
Batch 55/64 loss: 0.03882330656051636
Batch 56/64 loss: 0.048040568828582764
Batch 57/64 loss: 0.07480639219284058
Batch 58/64 loss: 0.08138811588287354
Batch 59/64 loss: 0.06817018985748291
Batch 60/64 loss: 0.07527339458465576
Batch 61/64 loss: 0.07623499631881714
Batch 62/64 loss: 0.05015420913696289
Batch 63/64 loss: 0.0868229866027832
Batch 64/64 loss: 0.05933493375778198
Epoch 236  Train loss: 0.061377177752700504  Val loss: 0.1241392497344525
Epoch 237
-------------------------------
Batch 1/64 loss: 0.08082413673400879
Batch 2/64 loss: 0.06899988651275635
Batch 3/64 loss: 0.09886527061462402
Batch 4/64 loss: 0.047094762325286865
Batch 5/64 loss: 0.0803256630897522
Batch 6/64 loss: 0.06349682807922363
Batch 7/64 loss: 0.05560201406478882
Batch 8/64 loss: 0.06230968236923218
Batch 9/64 loss: 0.04332667589187622
Batch 10/64 loss: 0.05791980028152466
Batch 11/64 loss: 0.05952262878417969
Batch 12/64 loss: 0.06942760944366455
Batch 13/64 loss: 0.07442474365234375
Batch 14/64 loss: 0.03232085704803467
Batch 15/64 loss: 0.039623260498046875
Batch 16/64 loss: 0.06481313705444336
Batch 17/64 loss: 0.066983163356781
Batch 18/64 loss: 0.06423616409301758
Batch 19/64 loss: 0.05436605215072632
Batch 20/64 loss: 0.03876698017120361
Batch 21/64 loss: 0.05612868070602417
Batch 22/64 loss: 0.06079995632171631
Batch 23/64 loss: 0.052330732345581055
Batch 24/64 loss: 0.03679490089416504
Batch 25/64 loss: 0.06555193662643433
Batch 26/64 loss: 0.07451134920120239
Batch 27/64 loss: 0.07031941413879395
Batch 28/64 loss: 0.038331031799316406
Batch 29/64 loss: 0.039837539196014404
Batch 30/64 loss: 0.05662953853607178
Batch 31/64 loss: 0.053712546825408936
Batch 32/64 loss: 0.04663491249084473
Batch 33/64 loss: 0.0751076340675354
Batch 34/64 loss: 0.07223337888717651
Batch 35/64 loss: 0.07931458950042725
Batch 36/64 loss: 0.07153934240341187
Batch 37/64 loss: 0.046257972717285156
Batch 38/64 loss: 0.07361602783203125
Batch 39/64 loss: 0.06294924020767212
Batch 40/64 loss: 0.04193753004074097
Batch 41/64 loss: 0.05765843391418457
Batch 42/64 loss: 0.07364612817764282
Batch 43/64 loss: 0.04643434286117554
Batch 44/64 loss: 0.03418451547622681
Batch 45/64 loss: 0.08681857585906982
Batch 46/64 loss: 0.06436097621917725
Batch 47/64 loss: 0.06581926345825195
Batch 48/64 loss: 0.06535059213638306
Batch 49/64 loss: 0.047016799449920654
Batch 50/64 loss: 0.08815664052963257
Batch 51/64 loss: 0.06078106164932251
Batch 52/64 loss: 0.044557809829711914
Batch 53/64 loss: 0.060367047786712646
Batch 54/64 loss: 0.05475574731826782
Batch 55/64 loss: 0.06790685653686523
Batch 56/64 loss: 0.04385340213775635
Batch 57/64 loss: 0.07486456632614136
Batch 58/64 loss: 0.07494890689849854
Batch 59/64 loss: 0.07069408893585205
Batch 60/64 loss: 0.053621530532836914
Batch 61/64 loss: 0.07806885242462158
Batch 62/64 loss: 0.04274404048919678
Batch 63/64 loss: 0.05921339988708496
Batch 64/64 loss: 0.05510067939758301
Epoch 237  Train loss: 0.060469594656252394  Val loss: 0.12204647637724467
Epoch 238
-------------------------------
Batch 1/64 loss: 0.07736551761627197
Batch 2/64 loss: 0.07470548152923584
Batch 3/64 loss: 0.056510329246520996
Batch 4/64 loss: 0.03870284557342529
Batch 5/64 loss: 0.038708269596099854
Batch 6/64 loss: 0.03847116231918335
Batch 7/64 loss: 0.06571364402770996
Batch 8/64 loss: 0.0650092363357544
Batch 9/64 loss: 0.046733200550079346
Batch 10/64 loss: 0.06031090021133423
Batch 11/64 loss: 0.0711250901222229
Batch 12/64 loss: 0.06428229808807373
Batch 13/64 loss: 0.07253396511077881
Batch 14/64 loss: 0.0848623514175415
Batch 15/64 loss: 0.06914311647415161
Batch 16/64 loss: 0.06668508052825928
Batch 17/64 loss: 0.045789361000061035
Batch 18/64 loss: 0.04223531484603882
Batch 19/64 loss: 0.05032843351364136
Batch 20/64 loss: 0.07628905773162842
Batch 21/64 loss: 0.06758135557174683
Batch 22/64 loss: 0.06253921985626221
Batch 23/64 loss: 0.08137917518615723
Batch 24/64 loss: 0.04444873332977295
Batch 25/64 loss: 0.057966649532318115
Batch 26/64 loss: 0.04548346996307373
Batch 27/64 loss: 0.061050593852996826
Batch 28/64 loss: 0.037787795066833496
Batch 29/64 loss: 0.05266600847244263
Batch 30/64 loss: 0.04809540510177612
Batch 31/64 loss: 0.0902252197265625
Batch 32/64 loss: 0.058661818504333496
Batch 33/64 loss: 0.05673462152481079
Batch 34/64 loss: 0.05008894205093384
Batch 35/64 loss: 0.059548020362854004
Batch 36/64 loss: 0.05147451162338257
Batch 37/64 loss: 0.09809601306915283
Batch 38/64 loss: 0.05859863758087158
Batch 39/64 loss: 0.0346064567565918
Batch 40/64 loss: 0.06042802333831787
Batch 41/64 loss: 0.04031860828399658
Batch 42/64 loss: 0.045616984367370605
Batch 43/64 loss: 0.07144439220428467
Batch 44/64 loss: 0.06162148714065552
Batch 45/64 loss: 0.04545313119888306
Batch 46/64 loss: 0.04137998819351196
Batch 47/64 loss: 0.06333273649215698
Batch 48/64 loss: 0.07393336296081543
Batch 49/64 loss: 0.07445251941680908
Batch 50/64 loss: 0.03397560119628906
Batch 51/64 loss: 0.05246251821517944
Batch 52/64 loss: 0.04706299304962158
Batch 53/64 loss: 0.04803192615509033
Batch 54/64 loss: 0.06836915016174316
Batch 55/64 loss: 0.06792986392974854
Batch 56/64 loss: 0.06318992376327515
Batch 57/64 loss: 0.053673386573791504
Batch 58/64 loss: 0.058601200580596924
Batch 59/64 loss: 0.09407031536102295
Batch 60/64 loss: 0.07261288166046143
Batch 61/64 loss: 0.039122939109802246
Batch 62/64 loss: 0.07075530290603638
Batch 63/64 loss: 0.10770946741104126
Batch 64/64 loss: 0.05158650875091553
Epoch 238  Train loss: 0.05987099432477764  Val loss: 0.1231276726804648
Epoch 239
-------------------------------
Batch 1/64 loss: 0.05744260549545288
Batch 2/64 loss: 0.07764488458633423
Batch 3/64 loss: 0.05470097064971924
Batch 4/64 loss: 0.034270524978637695
Batch 5/64 loss: 0.051904499530792236
Batch 6/64 loss: 0.055973172187805176
Batch 7/64 loss: 0.06652075052261353
Batch 8/64 loss: 0.0576515793800354
Batch 9/64 loss: 0.050923287868499756
Batch 10/64 loss: 0.09700381755828857
Batch 11/64 loss: 0.05056482553482056
Batch 12/64 loss: 0.05933332443237305
Batch 13/64 loss: 0.05585348606109619
Batch 14/64 loss: 0.054619550704956055
Batch 15/64 loss: 0.037689924240112305
Batch 16/64 loss: 0.046435534954071045
Batch 17/64 loss: 0.06789469718933105
Batch 18/64 loss: 0.0566481351852417
Batch 19/64 loss: 0.06660854816436768
Batch 20/64 loss: 0.0628775954246521
Batch 21/64 loss: 0.06895500421524048
Batch 22/64 loss: 0.05812567472457886
Batch 23/64 loss: 0.046896517276763916
Batch 24/64 loss: 0.03879344463348389
Batch 25/64 loss: 0.06654977798461914
Batch 26/64 loss: 0.06714397668838501
Batch 27/64 loss: 0.07711136341094971
Batch 28/64 loss: 0.08058345317840576
Batch 29/64 loss: 0.0549435019493103
Batch 30/64 loss: 0.035938918590545654
Batch 31/64 loss: 0.09789109230041504
Batch 32/64 loss: 0.03263753652572632
Batch 33/64 loss: 0.05343508720397949
Batch 34/64 loss: 0.06845599412918091
Batch 35/64 loss: 0.05021768808364868
Batch 36/64 loss: 0.05229896306991577
Batch 37/64 loss: 0.09540927410125732
Batch 38/64 loss: 0.058266282081604004
Batch 39/64 loss: 0.08107340335845947
Batch 40/64 loss: 0.041179776191711426
Batch 41/64 loss: 0.04837554693222046
Batch 42/64 loss: 0.0822533369064331
Batch 43/64 loss: 0.04985100030899048
Batch 44/64 loss: 0.041465938091278076
Batch 45/64 loss: 0.06040751934051514
Batch 46/64 loss: 0.07154524326324463
Batch 47/64 loss: 0.05546367168426514
Batch 48/64 loss: 0.0637350082397461
Batch 49/64 loss: 0.09219694137573242
Batch 50/64 loss: 0.06588584184646606
Batch 51/64 loss: 0.062455058097839355
Batch 52/64 loss: 0.041880130767822266
Batch 53/64 loss: 0.048905789852142334
Batch 54/64 loss: 0.04376596212387085
Batch 55/64 loss: 0.06787818670272827
Batch 56/64 loss: 0.0724867582321167
Batch 57/64 loss: 0.03683340549468994
Batch 58/64 loss: 0.06734055280685425
Batch 59/64 loss: 0.0508652925491333
Batch 60/64 loss: 0.04131990671157837
Batch 61/64 loss: 0.07014816999435425
Batch 62/64 loss: 0.062938392162323
Batch 63/64 loss: 0.05370759963989258
Batch 64/64 loss: 0.05292510986328125
Epoch 239  Train loss: 0.05929203968422086  Val loss: 0.11939040020978738
Epoch 240
-------------------------------
Batch 1/64 loss: 0.04975241422653198
Batch 2/64 loss: 0.0608864426612854
Batch 3/64 loss: 0.04096287488937378
Batch 4/64 loss: 0.060228586196899414
Batch 5/64 loss: 0.0713697075843811
Batch 6/64 loss: 0.09251350164413452
Batch 7/64 loss: 0.03989309072494507
Batch 8/64 loss: 0.04617244005203247
Batch 9/64 loss: 0.05583763122558594
Batch 10/64 loss: 0.03929346799850464
Batch 11/64 loss: 0.04328674077987671
Batch 12/64 loss: 0.06522786617279053
Batch 13/64 loss: 0.07658886909484863
Batch 14/64 loss: 0.05488848686218262
Batch 15/64 loss: 0.03871023654937744
Batch 16/64 loss: 0.05345720052719116
Batch 17/64 loss: 0.04775267839431763
Batch 18/64 loss: 0.06874263286590576
Batch 19/64 loss: 0.062479615211486816
Batch 20/64 loss: 0.0648769736289978
Batch 21/64 loss: 0.050852060317993164
Batch 22/64 loss: 0.06177586317062378
Batch 23/64 loss: 0.06950819492340088
Batch 24/64 loss: 0.08181816339492798
Batch 25/64 loss: 0.03361499309539795
Batch 26/64 loss: 0.0650320053100586
Batch 27/64 loss: 0.040514588356018066
Batch 28/64 loss: 0.05006808042526245
Batch 29/64 loss: 0.07016193866729736
Batch 30/64 loss: 0.055511653423309326
Batch 31/64 loss: 0.052895307540893555
Batch 32/64 loss: 0.06972867250442505
Batch 33/64 loss: 0.05922037363052368
Batch 34/64 loss: 0.037307918071746826
Batch 35/64 loss: 0.07158631086349487
Batch 36/64 loss: 0.04753684997558594
Batch 37/64 loss: 0.057910799980163574
Batch 38/64 loss: 0.07426899671554565
Batch 39/64 loss: 0.042411863803863525
Batch 40/64 loss: 0.06081831455230713
Batch 41/64 loss: 0.07507336139678955
Batch 42/64 loss: 0.07385373115539551
Batch 43/64 loss: 0.05795609951019287
Batch 44/64 loss: 0.061528563499450684
Batch 45/64 loss: 0.06582778692245483
Batch 46/64 loss: 0.06466609239578247
Batch 47/64 loss: 0.08784502744674683
Batch 48/64 loss: 0.045360684394836426
Batch 49/64 loss: 0.08188319206237793
Batch 50/64 loss: 0.05808788537979126
Batch 51/64 loss: 0.058262407779693604
Batch 52/64 loss: 0.07024502754211426
Batch 53/64 loss: 0.08424985408782959
Batch 54/64 loss: 0.08141732215881348
Batch 55/64 loss: 0.04191601276397705
Batch 56/64 loss: 0.04447054862976074
Batch 57/64 loss: 0.04315829277038574
Batch 58/64 loss: 0.0790066123008728
Batch 59/64 loss: 0.040420711040496826
Batch 60/64 loss: 0.08061057329177856
Batch 61/64 loss: 0.07208263874053955
Batch 62/64 loss: 0.06759333610534668
Batch 63/64 loss: 0.0392458438873291
Batch 64/64 loss: 0.06273698806762695
Epoch 240  Train loss: 0.05972201964434455  Val loss: 0.12118537380932942
Epoch 241
-------------------------------
Batch 1/64 loss: 0.05808144807815552
Batch 2/64 loss: 0.051303744316101074
Batch 3/64 loss: 0.06977987289428711
Batch 4/64 loss: 0.038225531578063965
Batch 5/64 loss: 0.04655611515045166
Batch 6/64 loss: 0.052551984786987305
Batch 7/64 loss: 0.08063989877700806
Batch 8/64 loss: 0.05913043022155762
Batch 9/64 loss: 0.05199640989303589
Batch 10/64 loss: 0.0688103437423706
Batch 11/64 loss: 0.03835016489028931
Batch 12/64 loss: 0.0719839334487915
Batch 13/64 loss: 0.05300706624984741
Batch 14/64 loss: 0.06733673810958862
Batch 15/64 loss: 0.057859063148498535
Batch 16/64 loss: 0.05876833200454712
Batch 17/64 loss: 0.05183899402618408
Batch 18/64 loss: 0.05809450149536133
Batch 19/64 loss: 0.05195164680480957
Batch 20/64 loss: 0.055191099643707275
Batch 21/64 loss: 0.040488362312316895
Batch 22/64 loss: 0.05279684066772461
Batch 23/64 loss: 0.07079154253005981
Batch 24/64 loss: 0.08704334497451782
Batch 25/64 loss: 0.0746612548828125
Batch 26/64 loss: 0.0648927092552185
Batch 27/64 loss: 0.06129312515258789
Batch 28/64 loss: 0.04538363218307495
Batch 29/64 loss: 0.0672616958618164
Batch 30/64 loss: 0.032476723194122314
Batch 31/64 loss: 0.029787778854370117
Batch 32/64 loss: 0.053986966609954834
Batch 33/64 loss: 0.04974937438964844
Batch 34/64 loss: 0.03345799446105957
Batch 35/64 loss: 0.04564476013183594
Batch 36/64 loss: 0.058902859687805176
Batch 37/64 loss: 0.06012439727783203
Batch 38/64 loss: 0.05263471603393555
Batch 39/64 loss: 0.06800830364227295
Batch 40/64 loss: 0.05649971961975098
Batch 41/64 loss: 0.08102399110794067
Batch 42/64 loss: 0.06573522090911865
Batch 43/64 loss: 0.06379693746566772
Batch 44/64 loss: 0.06920433044433594
Batch 45/64 loss: 0.05976217985153198
Batch 46/64 loss: 0.051817357540130615
Batch 47/64 loss: 0.051285386085510254
Batch 48/64 loss: 0.0867161750793457
Batch 49/64 loss: 0.03691154718399048
Batch 50/64 loss: 0.0486375093460083
Batch 51/64 loss: 0.06407171487808228
Batch 52/64 loss: 0.06459397077560425
Batch 53/64 loss: 0.05096578598022461
Batch 54/64 loss: 0.0821833610534668
Batch 55/64 loss: 0.08202183246612549
Batch 56/64 loss: 0.04602372646331787
Batch 57/64 loss: 0.08943921327590942
Batch 58/64 loss: 0.04215824604034424
Batch 59/64 loss: 0.06695902347564697
Batch 60/64 loss: 0.05976992845535278
Batch 61/64 loss: 0.07910799980163574
Batch 62/64 loss: 0.03019583225250244
Batch 63/64 loss: 0.07253891229629517
Batch 64/64 loss: 0.057546794414520264
Epoch 241  Train loss: 0.05859488155327591  Val loss: 0.12066641337273457
Epoch 242
-------------------------------
Batch 1/64 loss: 0.04743295907974243
Batch 2/64 loss: 0.06752747297286987
Batch 3/64 loss: 0.022567689418792725
Batch 4/64 loss: 0.0502966046333313
Batch 5/64 loss: 0.04344749450683594
Batch 6/64 loss: 0.05662894248962402
Batch 7/64 loss: 0.051976680755615234
Batch 8/64 loss: 0.053022921085357666
Batch 9/64 loss: 0.045758605003356934
Batch 10/64 loss: 0.05787837505340576
Batch 11/64 loss: 0.10123008489608765
Batch 12/64 loss: 0.07279890775680542
Batch 13/64 loss: 0.053156256675720215
Batch 14/64 loss: 0.06622320413589478
Batch 15/64 loss: 0.04919916391372681
Batch 16/64 loss: 0.045034825801849365
Batch 17/64 loss: 0.0533941388130188
Batch 18/64 loss: 0.059998273849487305
Batch 19/64 loss: 0.08399724960327148
Batch 20/64 loss: 0.08638262748718262
Batch 21/64 loss: 0.059934020042419434
Batch 22/64 loss: 0.0509456992149353
Batch 23/64 loss: 0.08930015563964844
Batch 24/64 loss: 0.05526375770568848
Batch 25/64 loss: 0.04072844982147217
Batch 26/64 loss: 0.07231301069259644
Batch 27/64 loss: 0.05915027856826782
Batch 28/64 loss: 0.0725826621055603
Batch 29/64 loss: 0.044737935066223145
Batch 30/64 loss: 0.03742498159408569
Batch 31/64 loss: 0.054031550884246826
Batch 32/64 loss: 0.06129497289657593
Batch 33/64 loss: 0.09024703502655029
Batch 34/64 loss: 0.03277474641799927
Batch 35/64 loss: 0.07461345195770264
Batch 36/64 loss: 0.06504029035568237
Batch 37/64 loss: 0.07669174671173096
Batch 38/64 loss: 0.0791999101638794
Batch 39/64 loss: 0.07174283266067505
Batch 40/64 loss: 0.0572742223739624
Batch 41/64 loss: 0.07451063394546509
Batch 42/64 loss: 0.050939738750457764
Batch 43/64 loss: 0.08797788619995117
Batch 44/64 loss: 0.04645180702209473
Batch 45/64 loss: 0.03930312395095825
Batch 46/64 loss: 0.06261920928955078
Batch 47/64 loss: 0.07405364513397217
Batch 48/64 loss: 0.05119633674621582
Batch 49/64 loss: 0.0651618242263794
Batch 50/64 loss: 0.05322664976119995
Batch 51/64 loss: 0.03792083263397217
Batch 52/64 loss: 0.03839641809463501
Batch 53/64 loss: 0.042703449726104736
Batch 54/64 loss: 0.09640222787857056
Batch 55/64 loss: 0.05213743448257446
Batch 56/64 loss: 0.04923522472381592
Batch 57/64 loss: 0.029758691787719727
Batch 58/64 loss: 0.049100399017333984
Batch 59/64 loss: 0.03548634052276611
Batch 60/64 loss: 0.07002747058868408
Batch 61/64 loss: 0.073036789894104
Batch 62/64 loss: 0.03753256797790527
Batch 63/64 loss: 0.05299055576324463
Batch 64/64 loss: 0.06346869468688965
Epoch 242  Train loss: 0.05852572497199563  Val loss: 0.1234110765850421
Epoch 243
-------------------------------
Batch 1/64 loss: 0.05100107192993164
Batch 2/64 loss: 0.043614864349365234
Batch 3/64 loss: 0.04302835464477539
Batch 4/64 loss: 0.05576425790786743
Batch 5/64 loss: 0.09448033571243286
Batch 6/64 loss: 0.06014221906661987
Batch 7/64 loss: 0.06350189447402954
Batch 8/64 loss: 0.06579798460006714
Batch 9/64 loss: 0.07627373933792114
Batch 10/64 loss: 0.05113208293914795
Batch 11/64 loss: 0.07091468572616577
Batch 12/64 loss: 0.07758533954620361
Batch 13/64 loss: 0.044799208641052246
Batch 14/64 loss: 0.01584458351135254
Batch 15/64 loss: 0.047463417053222656
Batch 16/64 loss: 0.04749143123626709
Batch 17/64 loss: 0.033996760845184326
Batch 18/64 loss: 0.08075982332229614
Batch 19/64 loss: 0.05408972501754761
Batch 20/64 loss: 0.05243051052093506
Batch 21/64 loss: 0.062255144119262695
Batch 22/64 loss: 0.07700717449188232
Batch 23/64 loss: 0.04731178283691406
Batch 24/64 loss: 0.06838005781173706
Batch 25/64 loss: 0.05934959650039673
Batch 26/64 loss: 0.06494665145874023
Batch 27/64 loss: 0.07600343227386475
Batch 28/64 loss: 0.07844275236129761
Batch 29/64 loss: 0.07468754053115845
Batch 30/64 loss: 0.059413909912109375
Batch 31/64 loss: 0.04449421167373657
Batch 32/64 loss: 0.051450252532958984
Batch 33/64 loss: 0.036886513233184814
Batch 34/64 loss: 0.03989982604980469
Batch 35/64 loss: 0.05452674627304077
Batch 36/64 loss: 0.08128023147583008
Batch 37/64 loss: 0.045726776123046875
Batch 38/64 loss: 0.06269949674606323
Batch 39/64 loss: 0.04671508073806763
Batch 40/64 loss: 0.06703311204910278
Batch 41/64 loss: 0.06367915868759155
Batch 42/64 loss: 0.04782211780548096
Batch 43/64 loss: 0.07497113943099976
Batch 44/64 loss: 0.041834235191345215
Batch 45/64 loss: 0.11386555433273315
Batch 46/64 loss: 0.04295051097869873
Batch 47/64 loss: 0.04186660051345825
Batch 48/64 loss: 0.05139428377151489
Batch 49/64 loss: 0.0769425630569458
Batch 50/64 loss: 0.0706627368927002
Batch 51/64 loss: 0.0627967119216919
Batch 52/64 loss: 0.05759727954864502
Batch 53/64 loss: 0.09073054790496826
Batch 54/64 loss: 0.0819360613822937
Batch 55/64 loss: 0.05147284269332886
Batch 56/64 loss: 0.0824359655380249
Batch 57/64 loss: 0.05447876453399658
Batch 58/64 loss: 0.06813567876815796
Batch 59/64 loss: 0.05679255723953247
Batch 60/64 loss: 0.04466623067855835
Batch 61/64 loss: 0.07240509986877441
Batch 62/64 loss: 0.05536985397338867
Batch 63/64 loss: 0.053376078605651855
Batch 64/64 loss: 0.02927297353744507
Epoch 243  Train loss: 0.05974525306739059  Val loss: 0.12106360909865074
Epoch 244
-------------------------------
Batch 1/64 loss: 0.06194603443145752
Batch 2/64 loss: 0.06228494644165039
Batch 3/64 loss: 0.03881525993347168
Batch 4/64 loss: 0.07152104377746582
Batch 5/64 loss: 0.07400554418563843
Batch 6/64 loss: 0.05098164081573486
Batch 7/64 loss: 0.04877889156341553
Batch 8/64 loss: 0.06999748945236206
Batch 9/64 loss: 0.0733141303062439
Batch 10/64 loss: 0.045006632804870605
Batch 11/64 loss: 0.0636252760887146
Batch 12/64 loss: 0.08098405599594116
Batch 13/64 loss: 0.04046928882598877
Batch 14/64 loss: 0.04179966449737549
Batch 15/64 loss: 0.02450627088546753
Batch 16/64 loss: 0.04114997386932373
Batch 17/64 loss: 0.0704229474067688
Batch 18/64 loss: 0.06781214475631714
Batch 19/64 loss: 0.04235494136810303
Batch 20/64 loss: 0.04936951398849487
Batch 21/64 loss: 0.052721381187438965
Batch 22/64 loss: 0.05494987964630127
Batch 23/64 loss: 0.052002906799316406
Batch 24/64 loss: 0.02989506721496582
Batch 25/64 loss: 0.05336862802505493
Batch 26/64 loss: 0.05731242895126343
Batch 27/64 loss: 0.06719851493835449
Batch 28/64 loss: 0.052447617053985596
Batch 29/64 loss: 0.0373111367225647
Batch 30/64 loss: 0.030461430549621582
Batch 31/64 loss: 0.05858504772186279
Batch 32/64 loss: 0.04828917980194092
Batch 33/64 loss: 0.05652827024459839
Batch 34/64 loss: 0.05940002202987671
Batch 35/64 loss: 0.06333452463150024
Batch 36/64 loss: 0.042091965675354004
Batch 37/64 loss: 0.06103748083114624
Batch 38/64 loss: 0.06882047653198242
Batch 39/64 loss: 0.07938295602798462
Batch 40/64 loss: 0.09624618291854858
Batch 41/64 loss: 0.05471158027648926
Batch 42/64 loss: 0.06286782026290894
Batch 43/64 loss: 0.08444684743881226
Batch 44/64 loss: 0.05167889595031738
Batch 45/64 loss: 0.032395362854003906
Batch 46/64 loss: 0.0427517294883728
Batch 47/64 loss: 0.0725204348564148
Batch 48/64 loss: 0.07217264175415039
Batch 49/64 loss: 0.0845494270324707
Batch 50/64 loss: 0.09805983304977417
Batch 51/64 loss: 0.03137165307998657
Batch 52/64 loss: 0.03653275966644287
Batch 53/64 loss: 0.07353818416595459
Batch 54/64 loss: 0.03079831600189209
Batch 55/64 loss: 0.04801112413406372
Batch 56/64 loss: 0.08043426275253296
Batch 57/64 loss: 0.0901632308959961
Batch 58/64 loss: 0.04620093107223511
Batch 59/64 loss: 0.0718732476234436
Batch 60/64 loss: 0.0746990442276001
Batch 61/64 loss: 0.06159090995788574
Batch 62/64 loss: 0.0790090560913086
Batch 63/64 loss: 0.04899609088897705
Batch 64/64 loss: 0.04602015018463135
Epoch 244  Train loss: 0.05810853780484667  Val loss: 0.11986632527354657
Epoch 245
-------------------------------
Batch 1/64 loss: 0.0369417667388916
Batch 2/64 loss: 0.05328017473220825
Batch 3/64 loss: 0.06343984603881836
Batch 4/64 loss: 0.028490006923675537
Batch 5/64 loss: 0.06154310703277588
Batch 6/64 loss: 0.056255877017974854
Batch 7/64 loss: 0.05591005086898804
Batch 8/64 loss: 0.03830385208129883
Batch 9/64 loss: 0.0366973876953125
Batch 10/64 loss: 0.020792245864868164
Batch 11/64 loss: 0.05591326951980591
Batch 12/64 loss: 0.09315460920333862
Batch 13/64 loss: 0.03111279010772705
Batch 14/64 loss: 0.04064410924911499
Batch 15/64 loss: 0.059682250022888184
Batch 16/64 loss: 0.055613934993743896
Batch 17/64 loss: 0.03896242380142212
Batch 18/64 loss: 0.06054586172103882
Batch 19/64 loss: 0.07308387756347656
Batch 20/64 loss: 0.05790895223617554
Batch 21/64 loss: 0.03579986095428467
Batch 22/64 loss: 0.06448662281036377
Batch 23/64 loss: 0.0798715353012085
Batch 24/64 loss: 0.05504608154296875
Batch 25/64 loss: 0.052111804485321045
Batch 26/64 loss: 0.06393486261367798
Batch 27/64 loss: 0.07411974668502808
Batch 28/64 loss: 0.05640006065368652
Batch 29/64 loss: 0.06510263681411743
Batch 30/64 loss: 0.07396245002746582
Batch 31/64 loss: 0.047597289085388184
Batch 32/64 loss: 0.040588438510894775
Batch 33/64 loss: 0.03597909212112427
Batch 34/64 loss: 0.06394380331039429
Batch 35/64 loss: 0.05905097723007202
Batch 36/64 loss: 0.05870276689529419
Batch 37/64 loss: 0.025906026363372803
Batch 38/64 loss: 0.06902718544006348
Batch 39/64 loss: 0.049008309841156006
Batch 40/64 loss: 0.054878950119018555
Batch 41/64 loss: 0.06546461582183838
Batch 42/64 loss: 0.07267510890960693
Batch 43/64 loss: 0.03300786018371582
Batch 44/64 loss: 0.01862281560897827
Batch 45/64 loss: 0.0449180006980896
Batch 46/64 loss: 0.0638343095779419
Batch 47/64 loss: 0.042991816997528076
Batch 48/64 loss: 0.055444538593292236
Batch 49/64 loss: 0.058362364768981934
Batch 50/64 loss: 0.05166870355606079
Batch 51/64 loss: 0.06765943765640259
Batch 52/64 loss: 0.0679469108581543
Batch 53/64 loss: 0.09826385974884033
Batch 54/64 loss: 0.049683988094329834
Batch 55/64 loss: 0.08117353916168213
Batch 56/64 loss: 0.07704555988311768
Batch 57/64 loss: 0.08995550870895386
Batch 58/64 loss: 0.0807715654373169
Batch 59/64 loss: 0.04962146282196045
Batch 60/64 loss: 0.08749830722808838
Batch 61/64 loss: 0.08477258682250977
Batch 62/64 loss: 0.07628554105758667
Batch 63/64 loss: 0.06789451837539673
Batch 64/64 loss: 0.05710446834564209
Epoch 245  Train loss: 0.057602920251734115  Val loss: 0.11915112955054057
Saving best model, epoch: 245
Epoch 246
-------------------------------
Batch 1/64 loss: 0.06268781423568726
Batch 2/64 loss: 0.05055350065231323
Batch 3/64 loss: 0.04877877235412598
Batch 4/64 loss: 0.05448073148727417
Batch 5/64 loss: 0.05910229682922363
Batch 6/64 loss: 0.06448996067047119
Batch 7/64 loss: 0.05284476280212402
Batch 8/64 loss: 0.04674339294433594
Batch 9/64 loss: 0.0526006817817688
Batch 10/64 loss: 0.07657688856124878
Batch 11/64 loss: 0.07912075519561768
Batch 12/64 loss: 0.03556954860687256
Batch 13/64 loss: 0.06190848350524902
Batch 14/64 loss: 0.039144158363342285
Batch 15/64 loss: 0.03983449935913086
Batch 16/64 loss: 0.04723191261291504
Batch 17/64 loss: 0.07014292478561401
Batch 18/64 loss: 0.05789130926132202
Batch 19/64 loss: 0.057431578636169434
Batch 20/64 loss: 0.0476875901222229
Batch 21/64 loss: 0.03635811805725098
Batch 22/64 loss: 0.08057296276092529
Batch 23/64 loss: 0.07669883966445923
Batch 24/64 loss: 0.06652295589447021
Batch 25/64 loss: 0.03988659381866455
Batch 26/64 loss: 0.034308791160583496
Batch 27/64 loss: 0.06001448631286621
Batch 28/64 loss: 0.06497824192047119
Batch 29/64 loss: 0.05596262216567993
Batch 30/64 loss: 0.04542338848114014
Batch 31/64 loss: 0.033550143241882324
Batch 32/64 loss: 0.06367278099060059
Batch 33/64 loss: 0.05356413125991821
Batch 34/64 loss: 0.07461047172546387
Batch 35/64 loss: 0.06921148300170898
Batch 36/64 loss: 0.048469722270965576
Batch 37/64 loss: 0.03827166557312012
Batch 38/64 loss: 0.07882100343704224
Batch 39/64 loss: 0.025467514991760254
Batch 40/64 loss: 0.05796241760253906
Batch 41/64 loss: 0.06570130586624146
Batch 42/64 loss: 0.040182530879974365
Batch 43/64 loss: 0.05253630876541138
Batch 44/64 loss: 0.05807316303253174
Batch 45/64 loss: 0.07542937994003296
Batch 46/64 loss: 0.04710465669631958
Batch 47/64 loss: 0.039201974868774414
Batch 48/64 loss: 0.08553433418273926
Batch 49/64 loss: 0.06149625778198242
Batch 50/64 loss: 0.0578157901763916
Batch 51/64 loss: 0.0481867790222168
Batch 52/64 loss: 0.05606502294540405
Batch 53/64 loss: 0.08940374851226807
Batch 54/64 loss: 0.05265605449676514
Batch 55/64 loss: 0.07520937919616699
Batch 56/64 loss: 0.0471113920211792
Batch 57/64 loss: 0.04793781042098999
Batch 58/64 loss: 0.07039737701416016
Batch 59/64 loss: 0.09463024139404297
Batch 60/64 loss: 0.054430484771728516
Batch 61/64 loss: 0.06265521049499512
Batch 62/64 loss: 0.07656610012054443
Batch 63/64 loss: 0.055849432945251465
Batch 64/64 loss: 0.03827476501464844
Epoch 246  Train loss: 0.057286756178912  Val loss: 0.12114756521080777
Epoch 247
-------------------------------
Batch 1/64 loss: 0.10318684577941895
Batch 2/64 loss: 0.04881221055984497
Batch 3/64 loss: 0.05039769411087036
Batch 4/64 loss: 0.04625749588012695
Batch 5/64 loss: 0.08013588190078735
Batch 6/64 loss: 0.04086083173751831
Batch 7/64 loss: 0.04691910743713379
Batch 8/64 loss: 0.04436016082763672
Batch 9/64 loss: 0.08083802461624146
Batch 10/64 loss: 0.04218214750289917
Batch 11/64 loss: 0.04454994201660156
Batch 12/64 loss: 0.07909530401229858
Batch 13/64 loss: 0.0422939658164978
Batch 14/64 loss: 0.06963616609573364
Batch 15/64 loss: 0.05652058124542236
Batch 16/64 loss: 0.04713505506515503
Batch 17/64 loss: 0.037069618701934814
Batch 18/64 loss: 0.054489076137542725
Batch 19/64 loss: 0.08266115188598633
Batch 20/64 loss: 0.06628406047821045
Batch 21/64 loss: 0.02054905891418457
Batch 22/64 loss: 0.08793550729751587
Batch 23/64 loss: 0.08249890804290771
Batch 24/64 loss: 0.0382002592086792
Batch 25/64 loss: 0.05816447734832764
Batch 26/64 loss: 0.06557029485702515
Batch 27/64 loss: 0.03780019283294678
Batch 28/64 loss: 0.06661510467529297
Batch 29/64 loss: 0.06941431760787964
Batch 30/64 loss: 0.037598371505737305
Batch 31/64 loss: 0.07384222745895386
Batch 32/64 loss: 0.04118776321411133
Batch 33/64 loss: 0.04553806781768799
Batch 34/64 loss: 0.046765029430389404
Batch 35/64 loss: 0.07362860441207886
Batch 36/64 loss: 0.03513312339782715
Batch 37/64 loss: 0.06259256601333618
Batch 38/64 loss: 0.07436668872833252
Batch 39/64 loss: 0.035925865173339844
Batch 40/64 loss: 0.03861624002456665
Batch 41/64 loss: 0.04713207483291626
Batch 42/64 loss: 0.04133439064025879
Batch 43/64 loss: 0.053336381912231445
Batch 44/64 loss: 0.05499529838562012
Batch 45/64 loss: 0.07182556390762329
Batch 46/64 loss: 0.05639559030532837
Batch 47/64 loss: 0.08459454774856567
Batch 48/64 loss: 0.05495387315750122
Batch 49/64 loss: 0.06302249431610107
Batch 50/64 loss: 0.08272808790206909
Batch 51/64 loss: 0.06503438949584961
Batch 52/64 loss: 0.04187804460525513
Batch 53/64 loss: 0.058885037899017334
Batch 54/64 loss: 0.023976147174835205
Batch 55/64 loss: 0.03788173198699951
Batch 56/64 loss: 0.05643630027770996
Batch 57/64 loss: 0.035162389278411865
Batch 58/64 loss: 0.05573272705078125
Batch 59/64 loss: 0.0765647292137146
Batch 60/64 loss: 0.07872092723846436
Batch 61/64 loss: 0.0922006368637085
Batch 62/64 loss: 0.03762984275817871
Batch 63/64 loss: 0.05555850267410278
Batch 64/64 loss: 0.07459801435470581
Epoch 247  Train loss: 0.05705970525741577  Val loss: 0.1205527434234357
Epoch 248
-------------------------------
Batch 1/64 loss: 0.06564128398895264
Batch 2/64 loss: 0.046405136585235596
Batch 3/64 loss: 0.06710636615753174
Batch 4/64 loss: 0.03769570589065552
Batch 5/64 loss: 0.0330391526222229
Batch 6/64 loss: 0.03539705276489258
Batch 7/64 loss: 0.06427031755447388
Batch 8/64 loss: 0.06028473377227783
Batch 9/64 loss: 0.04302603006362915
Batch 10/64 loss: 0.04965025186538696
Batch 11/64 loss: 0.04841488599777222
Batch 12/64 loss: 0.060344815254211426
Batch 13/64 loss: 0.04606735706329346
Batch 14/64 loss: 0.06637454032897949
Batch 15/64 loss: 0.06122124195098877
Batch 16/64 loss: 0.05865275859832764
Batch 17/64 loss: 0.07897520065307617
Batch 18/64 loss: 0.05400729179382324
Batch 19/64 loss: 0.06920063495635986
Batch 20/64 loss: 0.045269668102264404
Batch 21/64 loss: 0.035490989685058594
Batch 22/64 loss: 0.05582249164581299
Batch 23/64 loss: 0.06178337335586548
Batch 24/64 loss: 0.06852161884307861
Batch 25/64 loss: 0.056134045124053955
Batch 26/64 loss: 0.03843986988067627
Batch 27/64 loss: 0.05417823791503906
Batch 28/64 loss: 0.07722276449203491
Batch 29/64 loss: 0.057611823081970215
Batch 30/64 loss: 0.053758203983306885
Batch 31/64 loss: 0.025721192359924316
Batch 32/64 loss: 0.06352615356445312
Batch 33/64 loss: 0.08311349153518677
Batch 34/64 loss: 0.058164775371551514
Batch 35/64 loss: 0.06051415205001831
Batch 36/64 loss: 0.050031065940856934
Batch 37/64 loss: 0.0666094422340393
Batch 38/64 loss: 0.06827312707901001
Batch 39/64 loss: 0.05580180883407593
Batch 40/64 loss: 0.04303652048110962
Batch 41/64 loss: 0.04478168487548828
Batch 42/64 loss: 0.06548517942428589
Batch 43/64 loss: 0.0558546781539917
Batch 44/64 loss: 0.06349921226501465
Batch 45/64 loss: 0.06116741895675659
Batch 46/64 loss: 0.06499111652374268
Batch 47/64 loss: 0.07566803693771362
Batch 48/64 loss: 0.08905178308486938
Batch 49/64 loss: 0.05254238843917847
Batch 50/64 loss: 0.07593673467636108
Batch 51/64 loss: 0.08628368377685547
Batch 52/64 loss: 0.04813694953918457
Batch 53/64 loss: 0.05752521753311157
Batch 54/64 loss: 0.04550635814666748
Batch 55/64 loss: 0.04722476005554199
Batch 56/64 loss: 0.06208348274230957
Batch 57/64 loss: 0.06618356704711914
Batch 58/64 loss: 0.06293755769729614
Batch 59/64 loss: 0.025676310062408447
Batch 60/64 loss: 0.04489123821258545
Batch 61/64 loss: 0.04624605178833008
Batch 62/64 loss: 0.04426860809326172
Batch 63/64 loss: 0.04131603240966797
Batch 64/64 loss: 0.10579252243041992
Epoch 248  Train loss: 0.056963168873506435  Val loss: 0.12131355206171672
Epoch 249
-------------------------------
Batch 1/64 loss: 0.043355703353881836
Batch 2/64 loss: 0.06890100240707397
Batch 3/64 loss: 0.05016058683395386
Batch 4/64 loss: 0.05526232719421387
Batch 5/64 loss: 0.06915652751922607
Batch 6/64 loss: 0.04581427574157715
Batch 7/64 loss: 0.03564774990081787
Batch 8/64 loss: 0.058944642543792725
Batch 9/64 loss: 0.06002765893936157
Batch 10/64 loss: 0.04069721698760986
Batch 11/64 loss: 0.08603906631469727
Batch 12/64 loss: 0.07736420631408691
Batch 13/64 loss: 0.06931126117706299
Batch 14/64 loss: 0.0626288652420044
Batch 15/64 loss: 0.05820870399475098
Batch 16/64 loss: 0.060019493103027344
Batch 17/64 loss: 0.03924161195755005
Batch 18/64 loss: 0.07564836740493774
Batch 19/64 loss: 0.06782364845275879
Batch 20/64 loss: 0.042602717876434326
Batch 21/64 loss: 0.07721579074859619
Batch 22/64 loss: 0.055896878242492676
Batch 23/64 loss: 0.034335434436798096
Batch 24/64 loss: 0.08361345529556274
Batch 25/64 loss: 0.05907469987869263
Batch 26/64 loss: 0.04564279317855835
Batch 27/64 loss: 0.04467111825942993
Batch 28/64 loss: 0.05733519792556763
Batch 29/64 loss: 0.04834473133087158
Batch 30/64 loss: 0.09263801574707031
Batch 31/64 loss: 0.07408630847930908
Batch 32/64 loss: 0.06389963626861572
Batch 33/64 loss: 0.037294745445251465
Batch 34/64 loss: 0.05879068374633789
Batch 35/64 loss: 0.04985862970352173
Batch 36/64 loss: 0.07493865489959717
Batch 37/64 loss: 0.054631054401397705
Batch 38/64 loss: 0.05952954292297363
Batch 39/64 loss: 0.0623701810836792
Batch 40/64 loss: 0.0733788013458252
Batch 41/64 loss: 0.08610910177230835
Batch 42/64 loss: 0.04820185899734497
Batch 43/64 loss: 0.046982765197753906
Batch 44/64 loss: 0.07590538263320923
Batch 45/64 loss: 0.02762436866760254
Batch 46/64 loss: 0.06109195947647095
Batch 47/64 loss: 0.02784419059753418
Batch 48/64 loss: 0.066680908203125
Batch 49/64 loss: 0.0382227897644043
Batch 50/64 loss: 0.07270437479019165
Batch 51/64 loss: 0.05088287591934204
Batch 52/64 loss: 0.05735433101654053
Batch 53/64 loss: 0.058312833309173584
Batch 54/64 loss: 0.04258441925048828
Batch 55/64 loss: 0.061974942684173584
Batch 56/64 loss: 0.06202191114425659
Batch 57/64 loss: 0.05779445171356201
Batch 58/64 loss: 0.05974334478378296
Batch 59/64 loss: 0.055926740169525146
Batch 60/64 loss: 0.04987996816635132
Batch 61/64 loss: 0.025600194931030273
Batch 62/64 loss: 0.04166489839553833
Batch 63/64 loss: 0.06679809093475342
Batch 64/64 loss: 0.04221057891845703
Epoch 249  Train loss: 0.057223005855784694  Val loss: 0.12041458968854032
Epoch 250
-------------------------------
Batch 1/64 loss: 0.040321528911590576
Batch 2/64 loss: 0.04861462116241455
Batch 3/64 loss: 0.059885501861572266
Batch 4/64 loss: 0.053057968616485596
Batch 5/64 loss: 0.07057082653045654
Batch 6/64 loss: 0.06348901987075806
Batch 7/64 loss: 0.058153629302978516
Batch 8/64 loss: 0.02650207281112671
Batch 9/64 loss: 0.015180587768554688
Batch 10/64 loss: 0.06515908241271973
Batch 11/64 loss: 0.038656771183013916
Batch 12/64 loss: 0.07936751842498779
Batch 13/64 loss: 0.05638056993484497
Batch 14/64 loss: 0.055883049964904785
Batch 15/64 loss: 0.04269927740097046
Batch 16/64 loss: 0.0255739688873291
Batch 17/64 loss: 0.06496906280517578
Batch 18/64 loss: 0.07415860891342163
Batch 19/64 loss: 0.07068586349487305
Batch 20/64 loss: 0.0279807448387146
Batch 21/64 loss: 0.08128458261489868
Batch 22/64 loss: 0.06613844633102417
Batch 23/64 loss: 0.05730390548706055
Batch 24/64 loss: 0.06455332040786743
Batch 25/64 loss: 0.05190014839172363
Batch 26/64 loss: 0.04038131237030029
Batch 27/64 loss: 0.06721639633178711
Batch 28/64 loss: 0.06306225061416626
Batch 29/64 loss: 0.06118100881576538
Batch 30/64 loss: 0.07800590991973877
Batch 31/64 loss: 0.06118118762969971
Batch 32/64 loss: 0.04379987716674805
Batch 33/64 loss: 0.06517672538757324
Batch 34/64 loss: 0.058229684829711914
Batch 35/64 loss: 0.05724906921386719
Batch 36/64 loss: 0.057941317558288574
Batch 37/64 loss: 0.08566313982009888
Batch 38/64 loss: 0.054935336112976074
Batch 39/64 loss: 0.048658132553100586
Batch 40/64 loss: 0.04004549980163574
Batch 41/64 loss: 0.07382386922836304
Batch 42/64 loss: 0.02809983491897583
Batch 43/64 loss: 0.07680624723434448
Batch 44/64 loss: 0.056194305419921875
Batch 45/64 loss: 0.039224445819854736
Batch 46/64 loss: 0.07317644357681274
Batch 47/64 loss: 0.06830614805221558
Batch 48/64 loss: 0.07019466161727905
Batch 49/64 loss: 0.049788057804107666
Batch 50/64 loss: 0.07127505540847778
Batch 51/64 loss: 0.04653960466384888
Batch 52/64 loss: 0.06966948509216309
Batch 53/64 loss: 0.028260350227355957
Batch 54/64 loss: 0.03282034397125244
Batch 55/64 loss: 0.06168532371520996
Batch 56/64 loss: 0.06871563196182251
Batch 57/64 loss: 0.032003939151763916
Batch 58/64 loss: 0.09405738115310669
Batch 59/64 loss: 0.05895644426345825
Batch 60/64 loss: 0.06899482011795044
Batch 61/64 loss: 0.08326399326324463
Batch 62/64 loss: 0.04615378379821777
Batch 63/64 loss: 0.06444603204727173
Batch 64/64 loss: 0.08509355783462524
Epoch 250  Train loss: 0.05752900231118296  Val loss: 0.12503503626564524
Epoch 251
-------------------------------
Batch 1/64 loss: 0.06884074211120605
Batch 2/64 loss: 0.0644868016242981
Batch 3/64 loss: 0.04668569564819336
Batch 4/64 loss: 0.06436800956726074
Batch 5/64 loss: 0.05543816089630127
Batch 6/64 loss: 0.07115107774734497
Batch 7/64 loss: 0.05964803695678711
Batch 8/64 loss: 0.06592965126037598
Batch 9/64 loss: 0.03705787658691406
Batch 10/64 loss: 0.058398306369781494
Batch 11/64 loss: 0.039206862449645996
Batch 12/64 loss: 0.042511165142059326
Batch 13/64 loss: 0.03777879476547241
Batch 14/64 loss: 0.06532961130142212
Batch 15/64 loss: 0.04618185758590698
Batch 16/64 loss: 0.0797349214553833
Batch 17/64 loss: 0.05344736576080322
Batch 18/64 loss: 0.06220453977584839
Batch 19/64 loss: 0.07246911525726318
Batch 20/64 loss: 0.050678551197052
Batch 21/64 loss: 0.07246112823486328
Batch 22/64 loss: 0.04538851976394653
Batch 23/64 loss: 0.05844616889953613
Batch 24/64 loss: 0.06539291143417358
Batch 25/64 loss: 0.06046628952026367
Batch 26/64 loss: 0.04122567176818848
Batch 27/64 loss: 0.07272613048553467
Batch 28/64 loss: 0.03698033094406128
Batch 29/64 loss: 0.02825164794921875
Batch 30/64 loss: 0.07313036918640137
Batch 31/64 loss: 0.04171997308731079
Batch 32/64 loss: 0.021438300609588623
Batch 33/64 loss: 0.095192551612854
Batch 34/64 loss: 0.07271122932434082
Batch 35/64 loss: 0.05307036638259888
Batch 36/64 loss: 0.08551758527755737
Batch 37/64 loss: 0.03320729732513428
Batch 38/64 loss: 0.07942938804626465
Batch 39/64 loss: 0.06517696380615234
Batch 40/64 loss: 0.032427430152893066
Batch 41/64 loss: 0.07260257005691528
Batch 42/64 loss: 0.06969612836837769
Batch 43/64 loss: 0.03735309839248657
Batch 44/64 loss: 0.0733942985534668
Batch 45/64 loss: 0.05339944362640381
Batch 46/64 loss: 0.05788034200668335
Batch 47/64 loss: 0.04399806261062622
Batch 48/64 loss: 0.08711868524551392
Batch 49/64 loss: 0.04160153865814209
Batch 50/64 loss: 0.07889670133590698
Batch 51/64 loss: 0.03985327482223511
Batch 52/64 loss: 0.05244475603103638
Batch 53/64 loss: 0.046653568744659424
Batch 54/64 loss: 0.04490381479263306
Batch 55/64 loss: 0.051392197608947754
Batch 56/64 loss: 0.06552731990814209
Batch 57/64 loss: 0.08567184209823608
Batch 58/64 loss: 0.036204397678375244
Batch 59/64 loss: 0.04824930429458618
Batch 60/64 loss: 0.041972577571868896
Batch 61/64 loss: 0.05507099628448486
Batch 62/64 loss: 0.051756858825683594
Batch 63/64 loss: 0.056200385093688965
Batch 64/64 loss: 0.02655792236328125
Epoch 251  Train loss: 0.05627714512394924  Val loss: 0.11925828129155529
Epoch 252
-------------------------------
Batch 1/64 loss: 0.030718445777893066
Batch 2/64 loss: 0.058750271797180176
Batch 3/64 loss: 0.08733820915222168
Batch 4/64 loss: 0.04890090227127075
Batch 5/64 loss: 0.04594665765762329
Batch 6/64 loss: 0.04733097553253174
Batch 7/64 loss: 0.03999495506286621
Batch 8/64 loss: 0.06902426481246948
Batch 9/64 loss: 0.052654147148132324
Batch 10/64 loss: 0.09003323316574097
Batch 11/64 loss: 0.04208111763000488
Batch 12/64 loss: 0.058013081550598145
Batch 13/64 loss: 0.0438845157623291
Batch 14/64 loss: 0.03490179777145386
Batch 15/64 loss: 0.048834264278411865
Batch 16/64 loss: 0.050493061542510986
Batch 17/64 loss: 0.07814276218414307
Batch 18/64 loss: 0.07636803388595581
Batch 19/64 loss: 0.08793962001800537
Batch 20/64 loss: 0.07493233680725098
Batch 21/64 loss: 0.03367018699645996
Batch 22/64 loss: 0.031161785125732422
Batch 23/64 loss: 0.03644329309463501
Batch 24/64 loss: 0.051516830921173096
Batch 25/64 loss: 0.07353031635284424
Batch 26/64 loss: 0.04097801446914673
Batch 27/64 loss: 0.03477895259857178
Batch 28/64 loss: 0.04238766431808472
Batch 29/64 loss: 0.02861964702606201
Batch 30/64 loss: 0.06678849458694458
Batch 31/64 loss: 0.04682290554046631
Batch 32/64 loss: 0.06276321411132812
Batch 33/64 loss: 0.03401142358779907
Batch 34/64 loss: 0.0692605972290039
Batch 35/64 loss: 0.0530971884727478
Batch 36/64 loss: 0.042035818099975586
Batch 37/64 loss: 0.05735194683074951
Batch 38/64 loss: 0.043250858783721924
Batch 39/64 loss: 0.06256043910980225
Batch 40/64 loss: 0.08009326457977295
Batch 41/64 loss: 0.05699390172958374
Batch 42/64 loss: 0.04549366235733032
Batch 43/64 loss: 0.04582417011260986
Batch 44/64 loss: 0.039361536502838135
Batch 45/64 loss: 0.0555911660194397
Batch 46/64 loss: 0.05386388301849365
Batch 47/64 loss: 0.05403244495391846
Batch 48/64 loss: 0.0722203254699707
Batch 49/64 loss: 0.07122516632080078
Batch 50/64 loss: 0.05840599536895752
Batch 51/64 loss: 0.05627197027206421
Batch 52/64 loss: 0.0594630241394043
Batch 53/64 loss: 0.058756470680236816
Batch 54/64 loss: 0.029647231101989746
Batch 55/64 loss: 0.06469452381134033
Batch 56/64 loss: 0.047961294651031494
Batch 57/64 loss: 0.042302072048187256
Batch 58/64 loss: 0.061596035957336426
Batch 59/64 loss: 0.053612470626831055
Batch 60/64 loss: 0.0975186824798584
Batch 61/64 loss: 0.04680091142654419
Batch 62/64 loss: 0.07065874338150024
Batch 63/64 loss: 0.0397343635559082
Batch 64/64 loss: 0.03504049777984619
Epoch 252  Train loss: 0.05436417121513217  Val loss: 0.1219779537715453
Epoch 253
-------------------------------
Batch 1/64 loss: 0.058789849281311035
Batch 2/64 loss: 0.08459877967834473
Batch 3/64 loss: 0.05541527271270752
Batch 4/64 loss: 0.0294111967086792
Batch 5/64 loss: 0.03521031141281128
Batch 6/64 loss: 0.031965553760528564
Batch 7/64 loss: 0.04897725582122803
Batch 8/64 loss: 0.0638687014579773
Batch 9/64 loss: 0.06225883960723877
Batch 10/64 loss: 0.07090151309967041
Batch 11/64 loss: 0.05045038461685181
Batch 12/64 loss: 0.09157073497772217
Batch 13/64 loss: 0.07124900817871094
Batch 14/64 loss: 0.0604172945022583
Batch 15/64 loss: 0.037806808948516846
Batch 16/64 loss: 0.06983065605163574
Batch 17/64 loss: 0.07544660568237305
Batch 18/64 loss: 0.03329521417617798
Batch 19/64 loss: 0.07522785663604736
Batch 20/64 loss: 0.08798319101333618
Batch 21/64 loss: 0.05297684669494629
Batch 22/64 loss: 0.046081364154815674
Batch 23/64 loss: 0.04726511240005493
Batch 24/64 loss: 0.04864001274108887
Batch 25/64 loss: 0.0581589937210083
Batch 26/64 loss: 0.05466943979263306
Batch 27/64 loss: 0.03061819076538086
Batch 28/64 loss: 0.05727642774581909
Batch 29/64 loss: 0.034002065658569336
Batch 30/64 loss: 0.049702346324920654
Batch 31/64 loss: 0.039259254932403564
Batch 32/64 loss: 0.055846333503723145
Batch 33/64 loss: 0.06341427564620972
Batch 34/64 loss: 0.08603501319885254
Batch 35/64 loss: 0.06388789415359497
Batch 36/64 loss: 0.05110734701156616
Batch 37/64 loss: 0.038073837757110596
Batch 38/64 loss: 0.022034168243408203
Batch 39/64 loss: 0.04841911792755127
Batch 40/64 loss: 0.05099225044250488
Batch 41/64 loss: 0.07902580499649048
Batch 42/64 loss: 0.0847243070602417
Batch 43/64 loss: 0.0573844313621521
Batch 44/64 loss: 0.06368166208267212
Batch 45/64 loss: 0.053377509117126465
Batch 46/64 loss: 0.05265676975250244
Batch 47/64 loss: 0.02802187204360962
Batch 48/64 loss: 0.06872200965881348
Batch 49/64 loss: 0.05498325824737549
Batch 50/64 loss: 0.06356549263000488
Batch 51/64 loss: 0.0709262490272522
Batch 52/64 loss: 0.05704456567764282
Batch 53/64 loss: 0.04344797134399414
Batch 54/64 loss: 0.06257456541061401
Batch 55/64 loss: 0.03734898567199707
Batch 56/64 loss: 0.06701797246932983
Batch 57/64 loss: 0.040478527545928955
Batch 58/64 loss: 0.04439955949783325
Batch 59/64 loss: 0.052318692207336426
Batch 60/64 loss: 0.06729614734649658
Batch 61/64 loss: 0.07203847169876099
Batch 62/64 loss: 0.06729382276535034
Batch 63/64 loss: 0.03676283359527588
Batch 64/64 loss: 0.0476609468460083
Epoch 253  Train loss: 0.05574858843111524  Val loss: 0.11949987243540917
Epoch 254
-------------------------------
Batch 1/64 loss: 0.05058908462524414
Batch 2/64 loss: 0.03793269395828247
Batch 3/64 loss: 0.05191683769226074
Batch 4/64 loss: 0.0618206262588501
Batch 5/64 loss: 0.08011829853057861
Batch 6/64 loss: 0.04166233539581299
Batch 7/64 loss: 0.05282825231552124
Batch 8/64 loss: 0.0716630220413208
Batch 9/64 loss: 0.04869353771209717
Batch 10/64 loss: 0.05989480018615723
Batch 11/64 loss: 0.053366899490356445
Batch 12/64 loss: 0.06059461832046509
Batch 13/64 loss: 0.07177603244781494
Batch 14/64 loss: 0.06758928298950195
Batch 15/64 loss: 0.0874527096748352
Batch 16/64 loss: 0.05877751111984253
Batch 17/64 loss: 0.054560840129852295
Batch 18/64 loss: 0.04544627666473389
Batch 19/64 loss: 0.04171556234359741
Batch 20/64 loss: 0.052380144596099854
Batch 21/64 loss: 0.04238295555114746
Batch 22/64 loss: 0.043849170207977295
Batch 23/64 loss: 0.04405921697616577
Batch 24/64 loss: 0.05682778358459473
Batch 25/64 loss: 0.05538231134414673
Batch 26/64 loss: 0.055883586406707764
Batch 27/64 loss: 0.05574578046798706
Batch 28/64 loss: 0.055442094802856445
Batch 29/64 loss: 0.07570838928222656
Batch 30/64 loss: 0.03402543067932129
Batch 31/64 loss: 0.08321291208267212
Batch 32/64 loss: 0.05325371026992798
Batch 33/64 loss: 0.05279034376144409
Batch 34/64 loss: 0.04980337619781494
Batch 35/64 loss: 0.09157299995422363
Batch 36/64 loss: 0.030508041381835938
Batch 37/64 loss: 0.06393152475357056
Batch 38/64 loss: 0.06699705123901367
Batch 39/64 loss: 0.05728083848953247
Batch 40/64 loss: 0.08741497993469238
Batch 41/64 loss: 0.026433467864990234
Batch 42/64 loss: 0.0571366548538208
Batch 43/64 loss: 0.07048392295837402
Batch 44/64 loss: 0.04959791898727417
Batch 45/64 loss: 0.05056488513946533
Batch 46/64 loss: 0.055248796939849854
Batch 47/64 loss: 0.08717107772827148
Batch 48/64 loss: 0.03813886642456055
Batch 49/64 loss: 0.046099305152893066
Batch 50/64 loss: 0.04317706823348999
Batch 51/64 loss: 0.11330616474151611
Batch 52/64 loss: 0.08003181219100952
Batch 53/64 loss: 0.07757407426834106
Batch 54/64 loss: 0.0595247745513916
Batch 55/64 loss: 0.06849175691604614
Batch 56/64 loss: 0.04219353199005127
Batch 57/64 loss: 0.056176185607910156
Batch 58/64 loss: 0.03547626733779907
Batch 59/64 loss: 0.053270697593688965
Batch 60/64 loss: 0.06251543760299683
Batch 61/64 loss: 0.04993438720703125
Batch 62/64 loss: 0.04707801342010498
Batch 63/64 loss: 0.022362887859344482
Batch 64/64 loss: 0.054651618003845215
Epoch 254  Train loss: 0.05709534953622257  Val loss: 0.12030408513505024
Epoch 255
-------------------------------
Batch 1/64 loss: 0.040120482444763184
Batch 2/64 loss: 0.04690498113632202
Batch 3/64 loss: 0.0695650577545166
Batch 4/64 loss: 0.020752251148223877
Batch 5/64 loss: 0.039035141468048096
Batch 6/64 loss: 0.05188804864883423
Batch 7/64 loss: 0.04073381423950195
Batch 8/64 loss: 0.046450138092041016
Batch 9/64 loss: 0.06854718923568726
Batch 10/64 loss: 0.046023786067962646
Batch 11/64 loss: 0.06993603706359863
Batch 12/64 loss: 0.03657561540603638
Batch 13/64 loss: 0.037989020347595215
Batch 14/64 loss: 0.028379380702972412
Batch 15/64 loss: 0.0544438362121582
Batch 16/64 loss: 0.055244386196136475
Batch 17/64 loss: 0.052944719791412354
Batch 18/64 loss: 0.04906737804412842
Batch 19/64 loss: 0.0788799524307251
Batch 20/64 loss: 0.06106841564178467
Batch 21/64 loss: 0.030161023139953613
Batch 22/64 loss: 0.06359827518463135
Batch 23/64 loss: 0.09028637409210205
Batch 24/64 loss: 0.072346031665802
Batch 25/64 loss: 0.04987061023712158
Batch 26/64 loss: 0.048781633377075195
Batch 27/64 loss: 0.06683260202407837
Batch 28/64 loss: 0.07853484153747559
Batch 29/64 loss: 0.04665684700012207
Batch 30/64 loss: 0.0801357626914978
Batch 31/64 loss: 0.0795699954032898
Batch 32/64 loss: 0.04178506135940552
Batch 33/64 loss: 0.056764304637908936
Batch 34/64 loss: 0.03978109359741211
Batch 35/64 loss: 0.05202764272689819
Batch 36/64 loss: 0.05777096748352051
Batch 37/64 loss: 0.04956763982772827
Batch 38/64 loss: 0.0503121018409729
Batch 39/64 loss: 0.05355280637741089
Batch 40/64 loss: 0.054035484790802
Batch 41/64 loss: 0.06969887018203735
Batch 42/64 loss: 0.05750542879104614
Batch 43/64 loss: 0.05921238660812378
Batch 44/64 loss: 0.053022146224975586
Batch 45/64 loss: 0.02703613042831421
Batch 46/64 loss: 0.054418742656707764
Batch 47/64 loss: 0.03628385066986084
Batch 48/64 loss: 0.07356178760528564
Batch 49/64 loss: 0.0438082218170166
Batch 50/64 loss: 0.07004672288894653
Batch 51/64 loss: 0.052842557430267334
Batch 52/64 loss: 0.05724060535430908
Batch 53/64 loss: 0.05830967426300049
Batch 54/64 loss: 0.03822469711303711
Batch 55/64 loss: 0.0845457911491394
Batch 56/64 loss: 0.06820696592330933
Batch 57/64 loss: 0.035201311111450195
Batch 58/64 loss: 0.04868990182876587
Batch 59/64 loss: 0.09573209285736084
Batch 60/64 loss: 0.05576789379119873
Batch 61/64 loss: 0.027784764766693115
Batch 62/64 loss: 0.06882506608963013
Batch 63/64 loss: 0.050494372844696045
Batch 64/64 loss: 0.046530961990356445
Epoch 255  Train loss: 0.05456123819538191  Val loss: 0.11971858051634326
Epoch 256
-------------------------------
Batch 1/64 loss: 0.057175636291503906
Batch 2/64 loss: 0.04122316837310791
Batch 3/64 loss: 0.046785712242126465
Batch 4/64 loss: 0.04430842399597168
Batch 5/64 loss: 0.03207242488861084
Batch 6/64 loss: 0.07721704244613647
Batch 7/64 loss: 0.03767538070678711
Batch 8/64 loss: 0.0575752854347229
Batch 9/64 loss: 0.03664052486419678
Batch 10/64 loss: 0.04249989986419678
Batch 11/64 loss: 0.031626760959625244
Batch 12/64 loss: 0.06693053245544434
Batch 13/64 loss: 0.0880279541015625
Batch 14/64 loss: 0.04482126235961914
Batch 15/64 loss: 0.0602145791053772
Batch 16/64 loss: 0.04873472452163696
Batch 17/64 loss: 0.06018996238708496
Batch 18/64 loss: 0.04776185750961304
Batch 19/64 loss: 0.0292203426361084
Batch 20/64 loss: 0.04382967948913574
Batch 21/64 loss: 0.05039304494857788
Batch 22/64 loss: 0.07327425479888916
Batch 23/64 loss: 0.07086402177810669
Batch 24/64 loss: 0.05368012189865112
Batch 25/64 loss: 0.06096303462982178
Batch 26/64 loss: 0.06735581159591675
Batch 27/64 loss: 0.048532962799072266
Batch 28/64 loss: 0.06648695468902588
Batch 29/64 loss: 0.0413130521774292
Batch 30/64 loss: 0.05823671817779541
Batch 31/64 loss: 0.041281700134277344
Batch 32/64 loss: 0.0715366005897522
Batch 33/64 loss: 0.06499111652374268
Batch 34/64 loss: 0.05987340211868286
Batch 35/64 loss: 0.04768484830856323
Batch 36/64 loss: 0.052884697914123535
Batch 37/64 loss: 0.05514949560165405
Batch 38/64 loss: 0.02646815776824951
Batch 39/64 loss: 0.045097410678863525
Batch 40/64 loss: 0.041972815990448
Batch 41/64 loss: 0.07642877101898193
Batch 42/64 loss: 0.05225485563278198
Batch 43/64 loss: 0.05532371997833252
Batch 44/64 loss: 0.058586835861206055
Batch 45/64 loss: 0.0426599383354187
Batch 46/64 loss: 0.04452395439147949
Batch 47/64 loss: 0.06020718812942505
Batch 48/64 loss: 0.05160802602767944
Batch 49/64 loss: 0.05386775732040405
Batch 50/64 loss: 0.05262458324432373
Batch 51/64 loss: 0.048720598220825195
Batch 52/64 loss: 0.06734687089920044
Batch 53/64 loss: 0.07082521915435791
Batch 54/64 loss: 0.05790352821350098
Batch 55/64 loss: 0.0455852746963501
Batch 56/64 loss: 0.05465257167816162
Batch 57/64 loss: 0.09264320135116577
Batch 58/64 loss: 0.0662541389465332
Batch 59/64 loss: 0.04913914203643799
Batch 60/64 loss: 0.06327259540557861
Batch 61/64 loss: 0.05824941396713257
Batch 62/64 loss: 0.06621330976486206
Batch 63/64 loss: 0.06200242042541504
Batch 64/64 loss: 0.05690866708755493
Epoch 256  Train loss: 0.05468465557285384  Val loss: 0.12404315004643705
Epoch 257
-------------------------------
Batch 1/64 loss: 0.036068081855773926
Batch 2/64 loss: 0.07755434513092041
Batch 3/64 loss: 0.0476459264755249
Batch 4/64 loss: 0.06628209352493286
Batch 5/64 loss: 0.04036921262741089
Batch 6/64 loss: 0.049846768379211426
Batch 7/64 loss: 0.03478330373764038
Batch 8/64 loss: 0.04603093862533569
Batch 9/64 loss: 0.028243303298950195
Batch 10/64 loss: 0.048600971698760986
Batch 11/64 loss: 0.06375688314437866
Batch 12/64 loss: 0.036594390869140625
Batch 13/64 loss: 0.03357493877410889
Batch 14/64 loss: 0.06424957513809204
Batch 15/64 loss: 0.03884327411651611
Batch 16/64 loss: 0.04404205083847046
Batch 17/64 loss: 0.06419330835342407
Batch 18/64 loss: 0.044830918312072754
Batch 19/64 loss: 0.05497753620147705
Batch 20/64 loss: 0.04409992694854736
Batch 21/64 loss: 0.07587409019470215
Batch 22/64 loss: 0.06657218933105469
Batch 23/64 loss: 0.08265179395675659
Batch 24/64 loss: 0.03758484125137329
Batch 25/64 loss: 0.07444202899932861
Batch 26/64 loss: 0.06184035539627075
Batch 27/64 loss: 0.04736649990081787
Batch 28/64 loss: 0.05698668956756592
Batch 29/64 loss: 0.0747075080871582
Batch 30/64 loss: 0.05291503667831421
Batch 31/64 loss: 0.056175827980041504
Batch 32/64 loss: 0.08116304874420166
Batch 33/64 loss: 0.05219602584838867
Batch 34/64 loss: 0.05466294288635254
Batch 35/64 loss: 0.05666494369506836
Batch 36/64 loss: 0.04529857635498047
Batch 37/64 loss: 0.04740196466445923
Batch 38/64 loss: 0.09174847602844238
Batch 39/64 loss: 0.0488283634185791
Batch 40/64 loss: 0.06352531909942627
Batch 41/64 loss: 0.04181349277496338
Batch 42/64 loss: 0.06535983085632324
Batch 43/64 loss: 0.06305885314941406
Batch 44/64 loss: 0.050020694732666016
Batch 45/64 loss: 0.038987040519714355
Batch 46/64 loss: 0.048915982246398926
Batch 47/64 loss: 0.057438790798187256
Batch 48/64 loss: 0.03936129808425903
Batch 49/64 loss: 0.05034160614013672
Batch 50/64 loss: 0.05965852737426758
Batch 51/64 loss: 0.08881843090057373
Batch 52/64 loss: 0.08023625612258911
Batch 53/64 loss: 0.06582069396972656
Batch 54/64 loss: 0.061123013496398926
Batch 55/64 loss: 0.05921822786331177
Batch 56/64 loss: 0.049402713775634766
Batch 57/64 loss: 0.04246562719345093
Batch 58/64 loss: 0.04660886526107788
Batch 59/64 loss: 0.05375468730926514
Batch 60/64 loss: 0.0656503438949585
Batch 61/64 loss: 0.05685800313949585
Batch 62/64 loss: 0.05028855800628662
Batch 63/64 loss: 0.06225788593292236
Batch 64/64 loss: 0.05569565296173096
Epoch 257  Train loss: 0.05541059503368303  Val loss: 0.12342284041172041
Epoch 258
-------------------------------
Batch 1/64 loss: 0.047955453395843506
Batch 2/64 loss: 0.058889925479888916
Batch 3/64 loss: 0.04430800676345825
Batch 4/64 loss: 0.09245836734771729
Batch 5/64 loss: 0.06218606233596802
Batch 6/64 loss: 0.022266507148742676
Batch 7/64 loss: 0.02158045768737793
Batch 8/64 loss: 0.07679229974746704
Batch 9/64 loss: 0.04503023624420166
Batch 10/64 loss: 0.05329465866088867
Batch 11/64 loss: 0.04383671283721924
Batch 12/64 loss: 0.0370602011680603
Batch 13/64 loss: 0.07569020986557007
Batch 14/64 loss: 0.06877303123474121
Batch 15/64 loss: 0.054195404052734375
Batch 16/64 loss: 0.07529151439666748
Batch 17/64 loss: 0.040181100368499756
Batch 18/64 loss: 0.07294505834579468
Batch 19/64 loss: 0.06253838539123535
Batch 20/64 loss: 0.06494861841201782
Batch 21/64 loss: 0.045756399631500244
Batch 22/64 loss: 0.04679465293884277
Batch 23/64 loss: 0.06855100393295288
Batch 24/64 loss: 0.053082168102264404
Batch 25/64 loss: 0.0691571831703186
Batch 26/64 loss: 0.047647714614868164
Batch 27/64 loss: 0.06771308183670044
Batch 28/64 loss: 0.045893192291259766
Batch 29/64 loss: 0.04654639959335327
Batch 30/64 loss: 0.05492258071899414
Batch 31/64 loss: 0.05415302515029907
Batch 32/64 loss: 0.053702473640441895
Batch 33/64 loss: 0.06037282943725586
Batch 34/64 loss: 0.052569448947906494
Batch 35/64 loss: 0.08923476934432983
Batch 36/64 loss: 0.049878835678100586
Batch 37/64 loss: 0.014956831932067871
Batch 38/64 loss: 0.014806091785430908
Batch 39/64 loss: 0.07556569576263428
Batch 40/64 loss: 0.06891965866088867
Batch 41/64 loss: 0.0735774040222168
Batch 42/64 loss: 0.054215848445892334
Batch 43/64 loss: 0.03748524188995361
Batch 44/64 loss: 0.07080888748168945
Batch 45/64 loss: 0.02710425853729248
Batch 46/64 loss: 0.048337697982788086
Batch 47/64 loss: 0.04917347431182861
Batch 48/64 loss: 0.06347531080245972
Batch 49/64 loss: 0.07208150625228882
Batch 50/64 loss: 0.040002644062042236
Batch 51/64 loss: 0.034984469413757324
Batch 52/64 loss: 0.06371504068374634
Batch 53/64 loss: 0.038712382316589355
Batch 54/64 loss: 0.06268554925918579
Batch 55/64 loss: 0.014040052890777588
Batch 56/64 loss: 0.07526618242263794
Batch 57/64 loss: 0.0736778974533081
Batch 58/64 loss: 0.022532403469085693
Batch 59/64 loss: 0.046582162380218506
Batch 60/64 loss: 0.08415311574935913
Batch 61/64 loss: 0.04229158163070679
Batch 62/64 loss: 0.06390047073364258
Batch 63/64 loss: 0.06095927953720093
Batch 64/64 loss: 0.0812978744506836
Epoch 258  Train loss: 0.054606784558763694  Val loss: 0.12364071402762764
Epoch 259
-------------------------------
Batch 1/64 loss: 0.040438830852508545
Batch 2/64 loss: 0.05060821771621704
Batch 3/64 loss: 0.06247222423553467
Batch 4/64 loss: 0.047344088554382324
Batch 5/64 loss: 0.046858787536621094
Batch 6/64 loss: 0.04687190055847168
Batch 7/64 loss: 0.0337369441986084
Batch 8/64 loss: 0.04298526048660278
Batch 9/64 loss: 0.051211535930633545
Batch 10/64 loss: 0.03765660524368286
Batch 11/64 loss: 0.05798041820526123
Batch 12/64 loss: 0.025233447551727295
Batch 13/64 loss: 0.05430340766906738
Batch 14/64 loss: 0.06862729787826538
Batch 15/64 loss: 0.05179572105407715
Batch 16/64 loss: 0.07142949104309082
Batch 17/64 loss: 0.052968740463256836
Batch 18/64 loss: 0.07278180122375488
Batch 19/64 loss: 0.05445241928100586
Batch 20/64 loss: 0.06206256151199341
Batch 21/64 loss: 0.025871634483337402
Batch 22/64 loss: 0.07000732421875
Batch 23/64 loss: 0.03654026985168457
Batch 24/64 loss: 0.057984113693237305
Batch 25/64 loss: 0.04517662525177002
Batch 26/64 loss: 0.057970523834228516
Batch 27/64 loss: 0.03314691781997681
Batch 28/64 loss: 0.04921954870223999
Batch 29/64 loss: 0.04960799217224121
Batch 30/64 loss: 0.07771539688110352
Batch 31/64 loss: 0.07814103364944458
Batch 32/64 loss: 0.057268738746643066
Batch 33/64 loss: 0.05421632528305054
Batch 34/64 loss: 0.03762853145599365
Batch 35/64 loss: 0.0696706771850586
Batch 36/64 loss: 0.06846994161605835
Batch 37/64 loss: 0.054982006549835205
Batch 38/64 loss: 0.05942791700363159
Batch 39/64 loss: 0.07184940576553345
Batch 40/64 loss: 0.0519832968711853
Batch 41/64 loss: 0.06593477725982666
Batch 42/64 loss: 0.06465351581573486
Batch 43/64 loss: 0.03514885902404785
Batch 44/64 loss: 0.064247727394104
Batch 45/64 loss: 0.037693798542022705
Batch 46/64 loss: 0.04306626319885254
Batch 47/64 loss: 0.04570192098617554
Batch 48/64 loss: 0.060460686683654785
Batch 49/64 loss: 0.03999507427215576
Batch 50/64 loss: 0.04156816005706787
Batch 51/64 loss: 0.08936131000518799
Batch 52/64 loss: 0.07273674011230469
Batch 53/64 loss: 0.052413105964660645
Batch 54/64 loss: 0.06809967756271362
Batch 55/64 loss: 0.06335848569869995
Batch 56/64 loss: 0.03735071420669556
Batch 57/64 loss: 0.06628572940826416
Batch 58/64 loss: 0.08315855264663696
Batch 59/64 loss: 0.031446099281311035
Batch 60/64 loss: 0.056136906147003174
Batch 61/64 loss: 0.03303414583206177
Batch 62/64 loss: 0.050468385219573975
Batch 63/64 loss: 0.08635234832763672
Batch 64/64 loss: 0.06337016820907593
Epoch 259  Train loss: 0.05450821226718379  Val loss: 0.11875696587808353
Saving best model, epoch: 259
Epoch 260
-------------------------------
Batch 1/64 loss: 0.06106013059616089
Batch 2/64 loss: 0.023274898529052734
Batch 3/64 loss: 0.08014094829559326
Batch 4/64 loss: 0.07410937547683716
Batch 5/64 loss: 0.04170936346054077
Batch 6/64 loss: 0.06784176826477051
Batch 7/64 loss: 0.03730273246765137
Batch 8/64 loss: 0.07293343544006348
Batch 9/64 loss: 0.047630488872528076
Batch 10/64 loss: 0.061334431171417236
Batch 11/64 loss: 0.04222393035888672
Batch 12/64 loss: 0.11203622817993164
Batch 13/64 loss: 0.036428213119506836
Batch 14/64 loss: 0.06229567527770996
Batch 15/64 loss: 0.040424883365631104
Batch 16/64 loss: 0.023312509059906006
Batch 17/64 loss: 0.04830271005630493
Batch 18/64 loss: 0.08077752590179443
Batch 19/64 loss: 0.031981706619262695
Batch 20/64 loss: 0.0735856294631958
Batch 21/64 loss: 0.050100862979888916
Batch 22/64 loss: 0.05331587791442871
Batch 23/64 loss: 0.05567526817321777
Batch 24/64 loss: 0.04061877727508545
Batch 25/64 loss: 0.056278884410858154
Batch 26/64 loss: 0.0549544095993042
Batch 27/64 loss: 0.061611831188201904
Batch 28/64 loss: 0.07220536470413208
Batch 29/64 loss: 0.06269276142120361
Batch 30/64 loss: 0.07220536470413208
Batch 31/64 loss: 0.0608137845993042
Batch 32/64 loss: 0.04251730442047119
Batch 33/64 loss: 0.051358938217163086
Batch 34/64 loss: 0.05639159679412842
Batch 35/64 loss: 0.08389192819595337
Batch 36/64 loss: 0.04630434513092041
Batch 37/64 loss: 0.030247747898101807
Batch 38/64 loss: 0.06389623880386353
Batch 39/64 loss: 0.06609135866165161
Batch 40/64 loss: 0.07482123374938965
Batch 41/64 loss: 0.0415571928024292
Batch 42/64 loss: 0.052044451236724854
Batch 43/64 loss: 0.026584148406982422
Batch 44/64 loss: 0.06129664182662964
Batch 45/64 loss: 0.056857943534851074
Batch 46/64 loss: 0.058358967304229736
Batch 47/64 loss: 0.0536198616027832
Batch 48/64 loss: 0.05701446533203125
Batch 49/64 loss: 0.04974573850631714
Batch 50/64 loss: 0.07141411304473877
Batch 51/64 loss: 0.04721832275390625
Batch 52/64 loss: 0.013978660106658936
Batch 53/64 loss: 0.0660741925239563
Batch 54/64 loss: 0.03164023160934448
Batch 55/64 loss: 0.0602869987487793
Batch 56/64 loss: 0.03543764352798462
Batch 57/64 loss: 0.029024779796600342
Batch 58/64 loss: 0.05639052391052246
Batch 59/64 loss: 0.07113540172576904
Batch 60/64 loss: 0.04985916614532471
Batch 61/64 loss: 0.0484466552734375
Batch 62/64 loss: 0.05196094512939453
Batch 63/64 loss: 0.04602724313735962
Batch 64/64 loss: 0.0649612545967102
Epoch 260  Train loss: 0.054265030458861704  Val loss: 0.11978283719098855
Epoch 261
-------------------------------
Batch 1/64 loss: 0.040587425231933594
Batch 2/64 loss: 0.039223432540893555
Batch 3/64 loss: 0.03378504514694214
Batch 4/64 loss: 0.041223347187042236
Batch 5/64 loss: 0.03813433647155762
Batch 6/64 loss: 0.04651826620101929
Batch 7/64 loss: 0.045262694358825684
Batch 8/64 loss: 0.07798731327056885
Batch 9/64 loss: 0.06364995241165161
Batch 10/64 loss: 0.03988957405090332
Batch 11/64 loss: 0.031261444091796875
Batch 12/64 loss: 0.029460251331329346
Batch 13/64 loss: 0.040651917457580566
Batch 14/64 loss: 0.0748978853225708
Batch 15/64 loss: 0.035776734352111816
Batch 16/64 loss: 0.028867244720458984
Batch 17/64 loss: 0.028893351554870605
Batch 18/64 loss: 0.06055682897567749
Batch 19/64 loss: 0.06693029403686523
Batch 20/64 loss: 0.05661976337432861
Batch 21/64 loss: 0.07357347011566162
Batch 22/64 loss: 0.04920220375061035
Batch 23/64 loss: 0.05977916717529297
Batch 24/64 loss: 0.04884076118469238
Batch 25/64 loss: 0.030717194080352783
Batch 26/64 loss: 0.06758701801300049
Batch 27/64 loss: 0.038807570934295654
Batch 28/64 loss: 0.057997703552246094
Batch 29/64 loss: 0.05139279365539551
Batch 30/64 loss: 0.06712967157363892
Batch 31/64 loss: 0.07132375240325928
Batch 32/64 loss: 0.06352680921554565
Batch 33/64 loss: 0.04866272211074829
Batch 34/64 loss: 0.0363498330116272
Batch 35/64 loss: 0.04589581489562988
Batch 36/64 loss: 0.05986952781677246
Batch 37/64 loss: 0.05453145503997803
Batch 38/64 loss: 0.03839695453643799
Batch 39/64 loss: 0.055924177169799805
Batch 40/64 loss: 0.09210139513015747
Batch 41/64 loss: 0.062361180782318115
Batch 42/64 loss: 0.090171217918396
Batch 43/64 loss: 0.04206204414367676
Batch 44/64 loss: 0.06907916069030762
Batch 45/64 loss: 0.04226422309875488
Batch 46/64 loss: 0.09934699535369873
Batch 47/64 loss: 0.07127898931503296
Batch 48/64 loss: 0.05187845230102539
Batch 49/64 loss: 0.03670477867126465
Batch 50/64 loss: 0.035223305225372314
Batch 51/64 loss: 0.06119483709335327
Batch 52/64 loss: 0.052046358585357666
Batch 53/64 loss: 0.06864464282989502
Batch 54/64 loss: 0.05174267292022705
Batch 55/64 loss: 0.04000592231750488
Batch 56/64 loss: 0.08159667253494263
Batch 57/64 loss: 0.06714332103729248
Batch 58/64 loss: 0.06983721256256104
Batch 59/64 loss: 0.04184025526046753
Batch 60/64 loss: 0.039689064025878906
Batch 61/64 loss: 0.08640354871749878
Batch 62/64 loss: 0.04751121997833252
Batch 63/64 loss: 0.04976755380630493
Batch 64/64 loss: 0.04732179641723633
Epoch 261  Train loss: 0.05372662076763078  Val loss: 0.12371394744853384
Epoch 262
-------------------------------
Batch 1/64 loss: 0.04937034845352173
Batch 2/64 loss: 0.08084404468536377
Batch 3/64 loss: 0.0005801916122436523
Batch 4/64 loss: 0.022201597690582275
Batch 5/64 loss: 0.06649911403656006
Batch 6/64 loss: 0.04156303405761719
Batch 7/64 loss: 0.03751903772354126
Batch 8/64 loss: 0.056208252906799316
Batch 9/64 loss: 0.06270289421081543
Batch 10/64 loss: 0.04123830795288086
Batch 11/64 loss: 0.054559528827667236
Batch 12/64 loss: 0.05254852771759033
Batch 13/64 loss: 0.05958378314971924
Batch 14/64 loss: 0.061141908168792725
Batch 15/64 loss: 0.04841160774230957
Batch 16/64 loss: 0.05882388353347778
Batch 17/64 loss: 0.052086055278778076
Batch 18/64 loss: 0.03646647930145264
Batch 19/64 loss: 0.03844618797302246
Batch 20/64 loss: 0.04456603527069092
Batch 21/64 loss: 0.07374674081802368
Batch 22/64 loss: 0.03029000759124756
Batch 23/64 loss: 0.05108523368835449
Batch 24/64 loss: 0.055800437927246094
Batch 25/64 loss: 0.05168592929840088
Batch 26/64 loss: 0.07232964038848877
Batch 27/64 loss: 0.0549623966217041
Batch 28/64 loss: 0.05873215198516846
Batch 29/64 loss: 0.04323846101760864
Batch 30/64 loss: 0.041896939277648926
Batch 31/64 loss: 0.02733743190765381
Batch 32/64 loss: 0.04058051109313965
Batch 33/64 loss: 0.0755261778831482
Batch 34/64 loss: 0.04566490650177002
Batch 35/64 loss: 0.06815463304519653
Batch 36/64 loss: 0.06548506021499634
Batch 37/64 loss: 0.04502058029174805
Batch 38/64 loss: 0.07074594497680664
Batch 39/64 loss: 0.05859935283660889
Batch 40/64 loss: 0.06051141023635864
Batch 41/64 loss: 0.05994540452957153
Batch 42/64 loss: 0.04063284397125244
Batch 43/64 loss: 0.056217312812805176
Batch 44/64 loss: 0.05073624849319458
Batch 45/64 loss: 0.06295979022979736
Batch 46/64 loss: 0.056805551052093506
Batch 47/64 loss: 0.06424123048782349
Batch 48/64 loss: 0.060151755809783936
Batch 49/64 loss: 0.07334005832672119
Batch 50/64 loss: 0.027886152267456055
Batch 51/64 loss: 0.04006081819534302
Batch 52/64 loss: 0.052490413188934326
Batch 53/64 loss: 0.04211312532424927
Batch 54/64 loss: 0.037108659744262695
Batch 55/64 loss: 0.05892181396484375
Batch 56/64 loss: 0.03205615282058716
Batch 57/64 loss: 0.05947083234786987
Batch 58/64 loss: 0.06428122520446777
Batch 59/64 loss: 0.03530615568161011
Batch 60/64 loss: 0.08198291063308716
Batch 61/64 loss: 0.07182544469833374
Batch 62/64 loss: 0.09303140640258789
Batch 63/64 loss: 0.05900299549102783
Batch 64/64 loss: 0.0512734055519104
Epoch 262  Train loss: 0.05295338233311971  Val loss: 0.12334167486203905
Epoch 263
-------------------------------
Batch 1/64 loss: 0.037128746509552
Batch 2/64 loss: 0.0664098858833313
Batch 3/64 loss: 0.06995272636413574
Batch 4/64 loss: 0.03950858116149902
Batch 5/64 loss: 0.052306532859802246
Batch 6/64 loss: 0.03588515520095825
Batch 7/64 loss: 0.0523526668548584
Batch 8/64 loss: 0.04606986045837402
Batch 9/64 loss: 0.04043048620223999
Batch 10/64 loss: 0.04344964027404785
Batch 11/64 loss: 0.057405948638916016
Batch 12/64 loss: 0.04814690351486206
Batch 13/64 loss: 0.05048424005508423
Batch 14/64 loss: 0.0386846661567688
Batch 15/64 loss: 0.037062644958496094
Batch 16/64 loss: 0.060187339782714844
Batch 17/64 loss: 0.06646913290023804
Batch 18/64 loss: 0.06410062313079834
Batch 19/64 loss: 0.060405194759368896
Batch 20/64 loss: 0.039875924587249756
Batch 21/64 loss: 0.06907403469085693
Batch 22/64 loss: 0.048674821853637695
Batch 23/64 loss: 0.04632246494293213
Batch 24/64 loss: 0.04519367218017578
Batch 25/64 loss: 0.05358147621154785
Batch 26/64 loss: 0.046247124671936035
Batch 27/64 loss: 0.05386471748352051
Batch 28/64 loss: 0.03323179483413696
Batch 29/64 loss: 0.037966012954711914
Batch 30/64 loss: 0.033020853996276855
Batch 31/64 loss: 0.04614835977554321
Batch 32/64 loss: 0.03833436965942383
Batch 33/64 loss: 0.05873972177505493
Batch 34/64 loss: 0.032205402851104736
Batch 35/64 loss: 0.03857910633087158
Batch 36/64 loss: 0.10342329740524292
Batch 37/64 loss: 0.046477675437927246
Batch 38/64 loss: 0.09525728225708008
Batch 39/64 loss: 0.05597543716430664
Batch 40/64 loss: 0.05294758081436157
Batch 41/64 loss: 0.05050992965698242
Batch 42/64 loss: 0.07969856262207031
Batch 43/64 loss: 0.07379919290542603
Batch 44/64 loss: 0.08053725957870483
Batch 45/64 loss: 0.034024834632873535
Batch 46/64 loss: 0.057650208473205566
Batch 47/64 loss: 0.06883752346038818
Batch 48/64 loss: 0.05182957649230957
Batch 49/64 loss: 0.054173290729522705
Batch 50/64 loss: 0.04340583086013794
Batch 51/64 loss: 0.05543839931488037
Batch 52/64 loss: 0.05971044301986694
Batch 53/64 loss: 0.06901323795318604
Batch 54/64 loss: 0.0530971884727478
Batch 55/64 loss: 0.03360658884048462
Batch 56/64 loss: 0.05614197254180908
Batch 57/64 loss: 0.03318428993225098
Batch 58/64 loss: 0.05456888675689697
Batch 59/64 loss: 0.07202434539794922
Batch 60/64 loss: 0.06249135732650757
Batch 61/64 loss: 0.06230539083480835
Batch 62/64 loss: 0.037907540798187256
Batch 63/64 loss: 0.06325310468673706
Batch 64/64 loss: 0.07571238279342651
Epoch 263  Train loss: 0.05342078980277566  Val loss: 0.12423061587146877
Epoch 264
-------------------------------
Batch 1/64 loss: 0.05150806903839111
Batch 2/64 loss: 0.05562949180603027
Batch 3/64 loss: 0.05786478519439697
Batch 4/64 loss: 0.06863969564437866
Batch 5/64 loss: 0.04283350706100464
Batch 6/64 loss: 0.029568135738372803
Batch 7/64 loss: 0.054572463035583496
Batch 8/64 loss: 0.05035984516143799
Batch 9/64 loss: 0.06108701229095459
Batch 10/64 loss: 0.057265400886535645
Batch 11/64 loss: 0.03506779670715332
Batch 12/64 loss: 0.03919917345046997
Batch 13/64 loss: 0.029787123203277588
Batch 14/64 loss: 0.04761463403701782
Batch 15/64 loss: 0.06112074851989746
Batch 16/64 loss: 0.0557631254196167
Batch 17/64 loss: 0.08108323812484741
Batch 18/64 loss: 0.041834235191345215
Batch 19/64 loss: 0.08707153797149658
Batch 20/64 loss: 0.040341079235076904
Batch 21/64 loss: 0.03988391160964966
Batch 22/64 loss: 0.05801820755004883
Batch 23/64 loss: 0.0536268949508667
Batch 24/64 loss: 0.059619367122650146
Batch 25/64 loss: 0.030442237854003906
Batch 26/64 loss: 0.06506133079528809
Batch 27/64 loss: 0.06753754615783691
Batch 28/64 loss: 0.07756716012954712
Batch 29/64 loss: 0.08036726713180542
Batch 30/64 loss: 0.04260367155075073
Batch 31/64 loss: 0.0581396222114563
Batch 32/64 loss: 0.08804625272750854
Batch 33/64 loss: 0.054251909255981445
Batch 34/64 loss: 0.0436323881149292
Batch 35/64 loss: 0.0851934552192688
Batch 36/64 loss: 0.050250351428985596
Batch 37/64 loss: 0.05418956279754639
Batch 38/64 loss: 0.04919075965881348
Batch 39/64 loss: 0.04063910245895386
Batch 40/64 loss: 0.03577965497970581
Batch 41/64 loss: 0.08664971590042114
Batch 42/64 loss: 0.04857790470123291
Batch 43/64 loss: 0.04269814491271973
Batch 44/64 loss: 0.05482327938079834
Batch 45/64 loss: 0.026319265365600586
Batch 46/64 loss: 0.02763158082962036
Batch 47/64 loss: 0.06886625289916992
Batch 48/64 loss: 0.03920948505401611
Batch 49/64 loss: 0.07880222797393799
Batch 50/64 loss: 0.05131196975708008
Batch 51/64 loss: 0.02243119478225708
Batch 52/64 loss: 0.04460352659225464
Batch 53/64 loss: 0.06077921390533447
Batch 54/64 loss: 0.052527666091918945
Batch 55/64 loss: 0.0739089846611023
Batch 56/64 loss: 0.07919609546661377
Batch 57/64 loss: 0.038608431816101074
Batch 58/64 loss: 0.06780868768692017
Batch 59/64 loss: 0.02375274896621704
Batch 60/64 loss: 0.03987342119216919
Batch 61/64 loss: 0.04535508155822754
Batch 62/64 loss: 0.05665385723114014
Batch 63/64 loss: 0.04503035545349121
Batch 64/64 loss: 0.03628742694854736
Epoch 264  Train loss: 0.05309625747157078  Val loss: 0.11994147259754823
Epoch 265
-------------------------------
Batch 1/64 loss: 0.061461567878723145
Batch 2/64 loss: 0.04831796884536743
Batch 3/64 loss: 0.03420841693878174
Batch 4/64 loss: 0.043852269649505615
Batch 5/64 loss: 0.03852277994155884
Batch 6/64 loss: 0.07240855693817139
Batch 7/64 loss: 0.039622485637664795
Batch 8/64 loss: 0.06091117858886719
Batch 9/64 loss: 0.03859323263168335
Batch 10/64 loss: 0.05237859487533569
Batch 11/64 loss: 0.024784386157989502
Batch 12/64 loss: 0.0409700870513916
Batch 13/64 loss: 0.059482455253601074
Batch 14/64 loss: 0.03591042757034302
Batch 15/64 loss: 0.037326157093048096
Batch 16/64 loss: 0.05565965175628662
Batch 17/64 loss: 0.0714946985244751
Batch 18/64 loss: 0.019727110862731934
Batch 19/64 loss: 0.058982014656066895
Batch 20/64 loss: 0.07738494873046875
Batch 21/64 loss: 0.055352091789245605
Batch 22/64 loss: 0.06675034761428833
Batch 23/64 loss: 0.04162472486495972
Batch 24/64 loss: 0.05738705396652222
Batch 25/64 loss: 0.03408503532409668
Batch 26/64 loss: 0.06267052888870239
Batch 27/64 loss: 0.04005074501037598
Batch 28/64 loss: 0.05254828929901123
Batch 29/64 loss: 0.059469521045684814
Batch 30/64 loss: 0.04403543472290039
Batch 31/64 loss: 0.05963778495788574
Batch 32/64 loss: 0.031190156936645508
Batch 33/64 loss: 0.051552414894104004
Batch 34/64 loss: 0.05661892890930176
Batch 35/64 loss: 0.03215599060058594
Batch 36/64 loss: 0.08301341533660889
Batch 37/64 loss: 0.06066042184829712
Batch 38/64 loss: 0.019771873950958252
Batch 39/64 loss: 0.04205644130706787
Batch 40/64 loss: 0.03841584920883179
Batch 41/64 loss: 0.07549947500228882
Batch 42/64 loss: 0.08384174108505249
Batch 43/64 loss: 0.06820416450500488
Batch 44/64 loss: 0.03770780563354492
Batch 45/64 loss: 0.04733860492706299
Batch 46/64 loss: 0.06395095586776733
Batch 47/64 loss: 0.04240739345550537
Batch 48/64 loss: 0.05166018009185791
Batch 49/64 loss: 0.04715150594711304
Batch 50/64 loss: 0.060101449489593506
Batch 51/64 loss: 0.06204020977020264
Batch 52/64 loss: 0.04118645191192627
Batch 53/64 loss: 0.06821954250335693
Batch 54/64 loss: 0.061051905155181885
Batch 55/64 loss: 0.08632111549377441
Batch 56/64 loss: 0.0631873607635498
Batch 57/64 loss: 0.061556875705718994
Batch 58/64 loss: 0.057965993881225586
Batch 59/64 loss: 0.05261021852493286
Batch 60/64 loss: 0.04931938648223877
Batch 61/64 loss: 0.0871659517288208
Batch 62/64 loss: 0.06189703941345215
Batch 63/64 loss: 0.04124462604522705
Batch 64/64 loss: 0.03527039289474487
Epoch 265  Train loss: 0.052692216284134806  Val loss: 0.12317368562278878
Epoch 266
-------------------------------
Batch 1/64 loss: 0.0375676155090332
Batch 2/64 loss: 0.09279364347457886
Batch 3/64 loss: 0.06580221652984619
Batch 4/64 loss: 0.0543782114982605
Batch 5/64 loss: 0.046890974044799805
Batch 6/64 loss: 0.028121531009674072
Batch 7/64 loss: 0.07012331485748291
Batch 8/64 loss: 0.04639136791229248
Batch 9/64 loss: 0.06359601020812988
Batch 10/64 loss: 0.0311279296875
Batch 11/64 loss: 0.05611902475357056
Batch 12/64 loss: 0.02448248863220215
Batch 13/64 loss: 0.07099413871765137
Batch 14/64 loss: 0.06009411811828613
Batch 15/64 loss: 0.019767820835113525
Batch 16/64 loss: 0.05366170406341553
Batch 17/64 loss: 0.05320429801940918
Batch 18/64 loss: 0.05150645971298218
Batch 19/64 loss: 0.011641263961791992
Batch 20/64 loss: 0.06176567077636719
Batch 21/64 loss: 0.022711634635925293
Batch 22/64 loss: 0.05155247449874878
Batch 23/64 loss: 0.04071462154388428
Batch 24/64 loss: 0.04210120439529419
Batch 25/64 loss: 0.07597535848617554
Batch 26/64 loss: 0.06591415405273438
Batch 27/64 loss: 0.0684652328491211
Batch 28/64 loss: 0.04622769355773926
Batch 29/64 loss: 0.055535078048706055
Batch 30/64 loss: 0.06426262855529785
Batch 31/64 loss: 0.03348183631896973
Batch 32/64 loss: 0.053285181522369385
Batch 33/64 loss: 0.05966353416442871
Batch 34/64 loss: 0.053767502307891846
Batch 35/64 loss: 0.062329232692718506
Batch 36/64 loss: 0.044174373149871826
Batch 37/64 loss: 0.03463202714920044
Batch 38/64 loss: 0.03612363338470459
Batch 39/64 loss: 0.04216974973678589
Batch 40/64 loss: 0.04437154531478882
Batch 41/64 loss: 0.07537841796875
Batch 42/64 loss: 0.058935344219207764
Batch 43/64 loss: 0.06338083744049072
Batch 44/64 loss: 0.05490219593048096
Batch 45/64 loss: 0.05232030153274536
Batch 46/64 loss: 0.04117131233215332
Batch 47/64 loss: 0.05408191680908203
Batch 48/64 loss: 0.0457417368888855
Batch 49/64 loss: 0.06100189685821533
Batch 50/64 loss: 0.07995235919952393
Batch 51/64 loss: 0.09026777744293213
Batch 52/64 loss: 0.04884958267211914
Batch 53/64 loss: 0.06362152099609375
Batch 54/64 loss: 0.06929999589920044
Batch 55/64 loss: 0.0447574257850647
Batch 56/64 loss: 0.05222684144973755
Batch 57/64 loss: 0.0798807144165039
Batch 58/64 loss: 0.05484962463378906
Batch 59/64 loss: 0.05374032258987427
Batch 60/64 loss: 0.041748881340026855
Batch 61/64 loss: 0.05073064565658569
Batch 62/64 loss: 0.04136669635772705
Batch 63/64 loss: 0.04686683416366577
Batch 64/64 loss: 0.03294181823730469
Epoch 266  Train loss: 0.05250616541095809  Val loss: 0.11927023888453585
Epoch 267
-------------------------------
Batch 1/64 loss: 0.03832590579986572
Batch 2/64 loss: 0.053597092628479004
Batch 3/64 loss: 0.03267312049865723
Batch 4/64 loss: 0.049358487129211426
Batch 5/64 loss: 0.0811966061592102
Batch 6/64 loss: 0.06270444393157959
Batch 7/64 loss: 0.029502689838409424
Batch 8/64 loss: 0.06004321575164795
Batch 9/64 loss: 0.07977187633514404
Batch 10/64 loss: 0.0313144326210022
Batch 11/64 loss: 0.04102218151092529
Batch 12/64 loss: 0.01860201358795166
Batch 13/64 loss: 0.06901860237121582
Batch 14/64 loss: 0.018379807472229004
Batch 15/64 loss: 0.09123837947845459
Batch 16/64 loss: 0.033353984355926514
Batch 17/64 loss: 0.057523369789123535
Batch 18/64 loss: 0.042576730251312256
Batch 19/64 loss: 0.04572486877441406
Batch 20/64 loss: 0.06715011596679688
Batch 21/64 loss: 0.028193116188049316
Batch 22/64 loss: 0.07160687446594238
Batch 23/64 loss: 0.06305211782455444
Batch 24/64 loss: 0.07201254367828369
Batch 25/64 loss: 0.03352010250091553
Batch 26/64 loss: 0.05612528324127197
Batch 27/64 loss: 0.05416041612625122
Batch 28/64 loss: 0.05267918109893799
Batch 29/64 loss: 0.06038475036621094
Batch 30/64 loss: 0.057157039642333984
Batch 31/64 loss: 0.03901439905166626
Batch 32/64 loss: 0.037024080753326416
Batch 33/64 loss: 0.04934209585189819
Batch 34/64 loss: 0.06687569618225098
Batch 35/64 loss: 0.06357234716415405
Batch 36/64 loss: 0.058469533920288086
Batch 37/64 loss: 0.05056256055831909
Batch 38/64 loss: 0.07049548625946045
Batch 39/64 loss: 0.03316092491149902
Batch 40/64 loss: 0.03548085689544678
Batch 41/64 loss: 0.06777369976043701
Batch 42/64 loss: 0.06324255466461182
Batch 43/64 loss: 0.05047422647476196
Batch 44/64 loss: 0.02459484338760376
Batch 45/64 loss: 0.047676146030426025
Batch 46/64 loss: 0.043230414390563965
Batch 47/64 loss: 0.005564332008361816
Batch 48/64 loss: 0.057128727436065674
Batch 49/64 loss: 0.07525241374969482
Batch 50/64 loss: 0.037159740924835205
Batch 51/64 loss: 0.04117804765701294
Batch 52/64 loss: 0.05880075693130493
Batch 53/64 loss: 0.06233888864517212
Batch 54/64 loss: 0.03977560997009277
Batch 55/64 loss: 0.03057861328125
Batch 56/64 loss: 0.05198323726654053
Batch 57/64 loss: 0.06090688705444336
Batch 58/64 loss: 0.08000028133392334
Batch 59/64 loss: 0.05190908908843994
Batch 60/64 loss: 0.03099876642227173
Batch 61/64 loss: 0.03565967082977295
Batch 62/64 loss: 0.059424757957458496
Batch 63/64 loss: 0.0925285816192627
Batch 64/64 loss: 0.036921799182891846
Epoch 267  Train loss: 0.05100923870124069  Val loss: 0.11942229135749266
Epoch 268
-------------------------------
Batch 1/64 loss: 0.023675262928009033
Batch 2/64 loss: 0.04979205131530762
Batch 3/64 loss: 0.06331032514572144
Batch 4/64 loss: 0.06825196743011475
Batch 5/64 loss: 0.027523338794708252
Batch 6/64 loss: 0.03300464153289795
Batch 7/64 loss: 0.04972803592681885
Batch 8/64 loss: 0.0530543327331543
Batch 9/64 loss: 0.0759584903717041
Batch 10/64 loss: 0.04569488763809204
Batch 11/64 loss: 0.07506513595581055
Batch 12/64 loss: 0.06574386358261108
Batch 13/64 loss: 0.036871254444122314
Batch 14/64 loss: 0.028048396110534668
Batch 15/64 loss: 0.051267027854919434
Batch 16/64 loss: 0.027934908866882324
Batch 17/64 loss: 0.03080052137374878
Batch 18/64 loss: 0.06886458396911621
Batch 19/64 loss: 0.07061433792114258
Batch 20/64 loss: 0.04510390758514404
Batch 21/64 loss: 0.06251275539398193
Batch 22/64 loss: 0.06163889169692993
Batch 23/64 loss: 0.04109758138656616
Batch 24/64 loss: 0.05128586292266846
Batch 25/64 loss: 0.04961675405502319
Batch 26/64 loss: 0.025084257125854492
Batch 27/64 loss: 0.0675460696220398
Batch 28/64 loss: 0.050459086894989014
Batch 29/64 loss: 0.04222351312637329
Batch 30/64 loss: 0.01536184549331665
Batch 31/64 loss: 0.05109083652496338
Batch 32/64 loss: 0.05713075399398804
Batch 33/64 loss: 0.02751755714416504
Batch 34/64 loss: 0.05878794193267822
Batch 35/64 loss: 0.036234498023986816
Batch 36/64 loss: 0.04094541072845459
Batch 37/64 loss: 0.06650501489639282
Batch 38/64 loss: 0.04946058988571167
Batch 39/64 loss: 0.0728638768196106
Batch 40/64 loss: 0.037741899490356445
Batch 41/64 loss: 0.0446506142616272
Batch 42/64 loss: 0.06166452169418335
Batch 43/64 loss: 0.063029944896698
Batch 44/64 loss: 0.04294180870056152
Batch 45/64 loss: 0.05183380842208862
Batch 46/64 loss: 0.05569863319396973
Batch 47/64 loss: 0.05623573064804077
Batch 48/64 loss: 0.04544103145599365
Batch 49/64 loss: 0.049163103103637695
Batch 50/64 loss: 0.05654442310333252
Batch 51/64 loss: 0.04692751169204712
Batch 52/64 loss: 0.07801806926727295
Batch 53/64 loss: 0.05454146862030029
Batch 54/64 loss: 0.054546892642974854
Batch 55/64 loss: 0.06608015298843384
Batch 56/64 loss: 0.04000800848007202
Batch 57/64 loss: 0.06424272060394287
Batch 58/64 loss: 0.060836195945739746
Batch 59/64 loss: 0.05650222301483154
Batch 60/64 loss: 0.0862380862236023
Batch 61/64 loss: 0.08590590953826904
Batch 62/64 loss: 0.044631123542785645
Batch 63/64 loss: 0.05253547430038452
Batch 64/64 loss: 0.031528472900390625
Epoch 268  Train loss: 0.05172197772007363  Val loss: 0.11930487164107385
Epoch 269
-------------------------------
Batch 1/64 loss: 0.03942000865936279
Batch 2/64 loss: 0.03854495286941528
Batch 3/64 loss: 0.046707868576049805
Batch 4/64 loss: 0.06433665752410889
Batch 5/64 loss: 0.057787179946899414
Batch 6/64 loss: 0.03405892848968506
Batch 7/64 loss: 0.07030749320983887
Batch 8/64 loss: 0.0551794171333313
Batch 9/64 loss: 0.05355966091156006
Batch 10/64 loss: 0.046116769313812256
Batch 11/64 loss: 0.044926226139068604
Batch 12/64 loss: 0.05088835954666138
Batch 13/64 loss: 0.05307173728942871
Batch 14/64 loss: 0.06618702411651611
Batch 15/64 loss: 0.042487144470214844
Batch 16/64 loss: 0.04720371961593628
Batch 17/64 loss: 0.051489830017089844
Batch 18/64 loss: 0.024007081985473633
Batch 19/64 loss: 0.04656630754470825
Batch 20/64 loss: 0.07065892219543457
Batch 21/64 loss: 0.05303829908370972
Batch 22/64 loss: 0.05862259864807129
Batch 23/64 loss: 0.03468525409698486
Batch 24/64 loss: 0.09231525659561157
Batch 25/64 loss: 0.056617677211761475
Batch 26/64 loss: 0.02700251340866089
Batch 27/64 loss: 0.050088346004486084
Batch 28/64 loss: 0.05295431613922119
Batch 29/64 loss: 0.049999773502349854
Batch 30/64 loss: 0.06363481283187866
Batch 31/64 loss: 0.07496976852416992
Batch 32/64 loss: 0.027948081493377686
Batch 33/64 loss: 0.042708396911621094
Batch 34/64 loss: 0.0639258623123169
Batch 35/64 loss: 0.06209254264831543
Batch 36/64 loss: 0.041631877422332764
Batch 37/64 loss: 0.03375786542892456
Batch 38/64 loss: 0.04086405038833618
Batch 39/64 loss: 0.045921921730041504
Batch 40/64 loss: 0.035469651222229004
Batch 41/64 loss: 0.052712440490722656
Batch 42/64 loss: 0.07117056846618652
Batch 43/64 loss: 0.03811901807785034
Batch 44/64 loss: 0.049615442752838135
Batch 45/64 loss: 0.033370137214660645
Batch 46/64 loss: 0.06769794225692749
Batch 47/64 loss: 0.06069982051849365
Batch 48/64 loss: 0.06936681270599365
Batch 49/64 loss: 0.10316431522369385
Batch 50/64 loss: 0.06317806243896484
Batch 51/64 loss: 0.056609392166137695
Batch 52/64 loss: 0.06025785207748413
Batch 53/64 loss: 0.06132858991622925
Batch 54/64 loss: 0.0461309552192688
Batch 55/64 loss: 0.03844326734542847
Batch 56/64 loss: 0.04171478748321533
Batch 57/64 loss: 0.035439133644104004
Batch 58/64 loss: 0.047001004219055176
Batch 59/64 loss: 0.05452853441238403
Batch 60/64 loss: 0.0469973087310791
Batch 61/64 loss: 0.02948176860809326
Batch 62/64 loss: 0.03455793857574463
Batch 63/64 loss: 0.056094348430633545
Batch 64/64 loss: 0.04376035928726196
Epoch 269  Train loss: 0.05117260965646482  Val loss: 0.1168220092340843
Saving best model, epoch: 269
Epoch 270
-------------------------------
Batch 1/64 loss: 0.05114471912384033
Batch 2/64 loss: 0.030926942825317383
Batch 3/64 loss: 0.03111821413040161
Batch 4/64 loss: 0.01844733953475952
Batch 5/64 loss: 0.019797563552856445
Batch 6/64 loss: 0.07576286792755127
Batch 7/64 loss: 0.05192768573760986
Batch 8/64 loss: 0.07327032089233398
Batch 9/64 loss: 0.039830684661865234
Batch 10/64 loss: 0.07094407081604004
Batch 11/64 loss: 0.07872408628463745
Batch 12/64 loss: 0.04269886016845703
Batch 13/64 loss: 0.07303929328918457
Batch 14/64 loss: 0.055568039417266846
Batch 15/64 loss: 0.03351861238479614
Batch 16/64 loss: 0.06712639331817627
Batch 17/64 loss: 0.029956340789794922
Batch 18/64 loss: 0.04201459884643555
Batch 19/64 loss: 0.0431675910949707
Batch 20/64 loss: 0.028776824474334717
Batch 21/64 loss: 0.05197465419769287
Batch 22/64 loss: 0.045301735401153564
Batch 23/64 loss: 0.03573954105377197
Batch 24/64 loss: 0.05295610427856445
Batch 25/64 loss: 0.058739662170410156
Batch 26/64 loss: 0.060968637466430664
Batch 27/64 loss: 0.032300353050231934
Batch 28/64 loss: 0.03500932455062866
Batch 29/64 loss: 0.04645448923110962
Batch 30/64 loss: 0.03617703914642334
Batch 31/64 loss: 0.053086042404174805
Batch 32/64 loss: 0.08527153730392456
Batch 33/64 loss: 0.0645071268081665
Batch 34/64 loss: 0.0657883882522583
Batch 35/64 loss: 0.06432688236236572
Batch 36/64 loss: 0.05468171834945679
Batch 37/64 loss: 0.04203540086746216
Batch 38/64 loss: 0.022118568420410156
Batch 39/64 loss: 0.07326209545135498
Batch 40/64 loss: 0.0538097620010376
Batch 41/64 loss: 0.03896892070770264
Batch 42/64 loss: 0.053314805030822754
Batch 43/64 loss: 0.06536096334457397
Batch 44/64 loss: 0.06734609603881836
Batch 45/64 loss: 0.06416881084442139
Batch 46/64 loss: 0.04723834991455078
Batch 47/64 loss: 0.06000673770904541
Batch 48/64 loss: 0.07139939069747925
Batch 49/64 loss: 0.033988893032073975
Batch 50/64 loss: 0.04083561897277832
Batch 51/64 loss: 0.038752734661102295
Batch 52/64 loss: 0.055909574031829834
Batch 53/64 loss: 0.036194443702697754
Batch 54/64 loss: 0.05259591341018677
Batch 55/64 loss: 0.048015475273132324
Batch 56/64 loss: 0.04458022117614746
Batch 57/64 loss: 0.030086398124694824
Batch 58/64 loss: 0.05661565065383911
Batch 59/64 loss: 0.08733910322189331
Batch 60/64 loss: 0.06817054748535156
Batch 61/64 loss: 0.05463618040084839
Batch 62/64 loss: 0.06685900688171387
Batch 63/64 loss: 0.03863680362701416
Batch 64/64 loss: 0.04907655715942383
Epoch 270  Train loss: 0.0509819320603913  Val loss: 0.11974451980230325
Epoch 271
-------------------------------
Batch 1/64 loss: 0.021018266677856445
Batch 2/64 loss: 0.039726078510284424
Batch 3/64 loss: 0.0350152850151062
Batch 4/64 loss: 0.044059574604034424
Batch 5/64 loss: 0.054497480392456055
Batch 6/64 loss: 0.04431557655334473
Batch 7/64 loss: 0.062175869941711426
Batch 8/64 loss: 0.05931609869003296
Batch 9/64 loss: 0.039805710315704346
Batch 10/64 loss: 0.05021888017654419
Batch 11/64 loss: 0.08113402128219604
Batch 12/64 loss: 0.05240911245346069
Batch 13/64 loss: 0.04852259159088135
Batch 14/64 loss: 0.019946694374084473
Batch 15/64 loss: 0.02886730432510376
Batch 16/64 loss: 0.046126604080200195
Batch 17/64 loss: 0.07355409860610962
Batch 18/64 loss: 0.025151550769805908
Batch 19/64 loss: 0.043577373027801514
Batch 20/64 loss: 0.04051363468170166
Batch 21/64 loss: 0.04789304733276367
Batch 22/64 loss: 0.025896787643432617
Batch 23/64 loss: 0.03384554386138916
Batch 24/64 loss: 0.03187209367752075
Batch 25/64 loss: 0.040169477462768555
Batch 26/64 loss: 0.04568350315093994
Batch 27/64 loss: 0.0631641149520874
Batch 28/64 loss: 0.04089760780334473
Batch 29/64 loss: 0.03059995174407959
Batch 30/64 loss: 0.050021231174468994
Batch 31/64 loss: 0.08226841688156128
Batch 32/64 loss: 0.07041507959365845
Batch 33/64 loss: 0.07854002714157104
Batch 34/64 loss: 0.05669814348220825
Batch 35/64 loss: 0.043761610984802246
Batch 36/64 loss: 0.038738906383514404
Batch 37/64 loss: 0.048549532890319824
Batch 38/64 loss: 0.05994027853012085
Batch 39/64 loss: 0.04874759912490845
Batch 40/64 loss: 0.032532572746276855
Batch 41/64 loss: 0.05284106731414795
Batch 42/64 loss: 0.05900835990905762
Batch 43/64 loss: 0.06203734874725342
Batch 44/64 loss: 0.09249591827392578
Batch 45/64 loss: 0.0654292106628418
Batch 46/64 loss: 0.07191812992095947
Batch 47/64 loss: 0.08244568109512329
Batch 48/64 loss: 0.060617148876190186
Batch 49/64 loss: 0.059566497802734375
Batch 50/64 loss: 0.050608694553375244
Batch 51/64 loss: 0.066170334815979
Batch 52/64 loss: 0.05460017919540405
Batch 53/64 loss: 0.024441838264465332
Batch 54/64 loss: 0.04051285982131958
Batch 55/64 loss: 0.04744362831115723
Batch 56/64 loss: 0.051407575607299805
Batch 57/64 loss: 0.04022824764251709
Batch 58/64 loss: 0.05267399549484253
Batch 59/64 loss: 0.05001628398895264
Batch 60/64 loss: 0.06270700693130493
Batch 61/64 loss: 0.053131043910980225
Batch 62/64 loss: 0.0562666654586792
Batch 63/64 loss: 0.039484381675720215
Batch 64/64 loss: 0.04482966661453247
Epoch 271  Train loss: 0.05035077146455354  Val loss: 0.12027286192805496
Epoch 272
-------------------------------
Batch 1/64 loss: 0.03443706035614014
Batch 2/64 loss: 0.04486536979675293
Batch 3/64 loss: 0.03592801094055176
Batch 4/64 loss: 0.06706076860427856
Batch 5/64 loss: 0.06326329708099365
Batch 6/64 loss: 0.05405038595199585
Batch 7/64 loss: 0.027104198932647705
Batch 8/64 loss: 0.05457615852355957
Batch 9/64 loss: 0.04240703582763672
Batch 10/64 loss: 0.037421584129333496
Batch 11/64 loss: 0.061380624771118164
Batch 12/64 loss: 0.04181855916976929
Batch 13/64 loss: 0.038845181465148926
Batch 14/64 loss: 0.02126181125640869
Batch 15/64 loss: 0.057428717613220215
Batch 16/64 loss: 0.03835785388946533
Batch 17/64 loss: 0.04881483316421509
Batch 18/64 loss: 0.04824042320251465
Batch 19/64 loss: 0.038609087467193604
Batch 20/64 loss: 0.04369908571243286
Batch 21/64 loss: 0.03490811586380005
Batch 22/64 loss: 0.021252334117889404
Batch 23/64 loss: 0.05337637662887573
Batch 24/64 loss: 0.04160642623901367
Batch 25/64 loss: 0.04207223653793335
Batch 26/64 loss: 0.060288429260253906
Batch 27/64 loss: 0.06737256050109863
Batch 28/64 loss: 0.05752497911453247
Batch 29/64 loss: 0.03354382514953613
Batch 30/64 loss: 0.07345271110534668
Batch 31/64 loss: 0.03633534908294678
Batch 32/64 loss: 0.04702836275100708
Batch 33/64 loss: 0.03163158893585205
Batch 34/64 loss: 0.0655369758605957
Batch 35/64 loss: 0.037548840045928955
Batch 36/64 loss: 0.06615608930587769
Batch 37/64 loss: 0.05012029409408569
Batch 38/64 loss: 0.08566337823867798
Batch 39/64 loss: 0.04472988843917847
Batch 40/64 loss: 0.04068446159362793
Batch 41/64 loss: 0.021418452262878418
Batch 42/64 loss: 0.06538265943527222
Batch 43/64 loss: 0.06332910060882568
Batch 44/64 loss: 0.049173593521118164
Batch 45/64 loss: 0.05948364734649658
Batch 46/64 loss: 0.04052668809890747
Batch 47/64 loss: 0.0691976547241211
Batch 48/64 loss: 0.052134156227111816
Batch 49/64 loss: 0.06140172481536865
Batch 50/64 loss: 0.06839889287948608
Batch 51/64 loss: 0.0533832311630249
Batch 52/64 loss: 0.03090614080429077
Batch 53/64 loss: 0.05198925733566284
Batch 54/64 loss: 0.062457501888275146
Batch 55/64 loss: 0.071189284324646
Batch 56/64 loss: 0.06472784280776978
Batch 57/64 loss: 0.08123838901519775
Batch 58/64 loss: 0.04706913232803345
Batch 59/64 loss: 0.058293044567108154
Batch 60/64 loss: 0.046297311782836914
Batch 61/64 loss: 0.020750820636749268
Batch 62/64 loss: 0.060351431369781494
Batch 63/64 loss: 0.04127061367034912
Batch 64/64 loss: 0.029660701751708984
Epoch 272  Train loss: 0.049459127351349476  Val loss: 0.11990563013299634
Epoch 273
-------------------------------
Batch 1/64 loss: 0.014134526252746582
Batch 2/64 loss: 0.04931938648223877
Batch 3/64 loss: 0.058382272720336914
Batch 4/64 loss: 0.05840909481048584
Batch 5/64 loss: 0.05172783136367798
Batch 6/64 loss: 0.05999958515167236
Batch 7/64 loss: 0.06426125764846802
Batch 8/64 loss: 0.0742998719215393
Batch 9/64 loss: 0.0568547248840332
Batch 10/64 loss: 0.056883037090301514
Batch 11/64 loss: 0.04573976993560791
Batch 12/64 loss: 0.04915809631347656
Batch 13/64 loss: 0.061498403549194336
Batch 14/64 loss: 0.05274200439453125
Batch 15/64 loss: 0.0461353063583374
Batch 16/64 loss: 0.03493976593017578
Batch 17/64 loss: 0.03878289461135864
Batch 18/64 loss: 0.06058371067047119
Batch 19/64 loss: 0.05249834060668945
Batch 20/64 loss: 0.057394564151763916
Batch 21/64 loss: 0.035665810108184814
Batch 22/64 loss: 0.043643414974212646
Batch 23/64 loss: 0.04423165321350098
Batch 24/64 loss: 0.06541794538497925
Batch 25/64 loss: 0.0539400577545166
Batch 26/64 loss: 0.04737001657485962
Batch 27/64 loss: 0.04567497968673706
Batch 28/64 loss: 0.08453196287155151
Batch 29/64 loss: 0.030151724815368652
Batch 30/64 loss: 0.04767662286758423
Batch 31/64 loss: 0.04141885042190552
Batch 32/64 loss: 0.05457645654678345
Batch 33/64 loss: 0.037180542945861816
Batch 34/64 loss: 0.06444865465164185
Batch 35/64 loss: 0.07168078422546387
Batch 36/64 loss: 0.04691267013549805
Batch 37/64 loss: 0.0375480055809021
Batch 38/64 loss: 0.03260338306427002
Batch 39/64 loss: 0.053526341915130615
Batch 40/64 loss: 0.03217953443527222
Batch 41/64 loss: 0.07303035259246826
Batch 42/64 loss: 0.037360191345214844
Batch 43/64 loss: 0.06693494319915771
Batch 44/64 loss: 0.05206722021102905
Batch 45/64 loss: 0.0764087438583374
Batch 46/64 loss: 0.052541494369506836
Batch 47/64 loss: 0.059697866439819336
Batch 48/64 loss: 0.06825900077819824
Batch 49/64 loss: 0.055327773094177246
Batch 50/64 loss: 0.03735530376434326
Batch 51/64 loss: 0.04952722787857056
Batch 52/64 loss: 0.03993332386016846
Batch 53/64 loss: 0.05259519815444946
Batch 54/64 loss: 0.04883289337158203
Batch 55/64 loss: 0.037131667137145996
Batch 56/64 loss: 0.05826675891876221
Batch 57/64 loss: 0.04588061571121216
Batch 58/64 loss: 0.05066847801208496
Batch 59/64 loss: 0.057375550270080566
Batch 60/64 loss: 0.07689148187637329
Batch 61/64 loss: 0.04156780242919922
Batch 62/64 loss: 0.0833662748336792
Batch 63/64 loss: 0.0738821029663086
Batch 64/64 loss: 0.05294644832611084
Epoch 273  Train loss: 0.052529191503337784  Val loss: 0.12377116872682604
Epoch 274
-------------------------------
Batch 1/64 loss: 0.0250241756439209
Batch 2/64 loss: 0.0559658408164978
Batch 3/64 loss: 0.0511012077331543
Batch 4/64 loss: 0.06441605091094971
Batch 5/64 loss: 0.04005694389343262
Batch 6/64 loss: 0.052590250968933105
Batch 7/64 loss: 0.04457676410675049
Batch 8/64 loss: 0.07544839382171631
Batch 9/64 loss: 0.029473304748535156
Batch 10/64 loss: 0.07217967510223389
Batch 11/64 loss: 0.08535933494567871
Batch 12/64 loss: 0.06251895427703857
Batch 13/64 loss: 0.05484318733215332
Batch 14/64 loss: 0.026702284812927246
Batch 15/64 loss: 0.051201701164245605
Batch 16/64 loss: 0.053430378437042236
Batch 17/64 loss: 0.040911197662353516
Batch 18/64 loss: 0.049502670764923096
Batch 19/64 loss: 0.062488675117492676
Batch 20/64 loss: 0.046999454498291016
Batch 21/64 loss: 0.05370277166366577
Batch 22/64 loss: 0.0388219952583313
Batch 23/64 loss: 0.03137749433517456
Batch 24/64 loss: 0.020525455474853516
Batch 25/64 loss: 0.04315447807312012
Batch 26/64 loss: 0.04146701097488403
Batch 27/64 loss: 0.037063419818878174
Batch 28/64 loss: 0.06921225786209106
Batch 29/64 loss: 0.06566321849822998
Batch 30/64 loss: 0.02795875072479248
Batch 31/64 loss: 0.055980384349823
Batch 32/64 loss: 0.059652507305145264
Batch 33/64 loss: 0.04954725503921509
Batch 34/64 loss: 0.04048168659210205
Batch 35/64 loss: 0.058348119258880615
Batch 36/64 loss: 0.06528735160827637
Batch 37/64 loss: 0.05435192584991455
Batch 38/64 loss: 0.06272763013839722
Batch 39/64 loss: 0.04850393533706665
Batch 40/64 loss: 0.054801225662231445
Batch 41/64 loss: 0.04958069324493408
Batch 42/64 loss: 0.051733553409576416
Batch 43/64 loss: 0.036421239376068115
Batch 44/64 loss: 0.03446692228317261
Batch 45/64 loss: 0.05111825466156006
Batch 46/64 loss: 0.05360454320907593
Batch 47/64 loss: 0.0543743371963501
Batch 48/64 loss: 0.056536197662353516
Batch 49/64 loss: 0.04439806938171387
Batch 50/64 loss: 0.06485515832901001
Batch 51/64 loss: 0.045909881591796875
Batch 52/64 loss: 0.0684162974357605
Batch 53/64 loss: 0.04445153474807739
Batch 54/64 loss: 0.05733615159988403
Batch 55/64 loss: 0.07997685670852661
Batch 56/64 loss: 0.04798203706741333
Batch 57/64 loss: 0.04337954521179199
Batch 58/64 loss: 0.05527752637863159
Batch 59/64 loss: 0.04881715774536133
Batch 60/64 loss: 0.05966562032699585
Batch 61/64 loss: 0.05033916234970093
Batch 62/64 loss: 0.058692097663879395
Batch 63/64 loss: 0.03696805238723755
Batch 64/64 loss: 0.025752544403076172
Epoch 274  Train loss: 0.05077701362909055  Val loss: 0.12026027633562121
Epoch 275
-------------------------------
Batch 1/64 loss: 0.04328054189682007
Batch 2/64 loss: 0.03373098373413086
Batch 3/64 loss: 0.027598023414611816
Batch 4/64 loss: 0.03548711538314819
Batch 5/64 loss: 0.06306767463684082
Batch 6/64 loss: 0.06994301080703735
Batch 7/64 loss: 0.04837191104888916
Batch 8/64 loss: 0.06296879053115845
Batch 9/64 loss: 0.07258343696594238
Batch 10/64 loss: 0.048742711544036865
Batch 11/64 loss: 0.0581284761428833
Batch 12/64 loss: 0.023575961589813232
Batch 13/64 loss: 0.04718393087387085
Batch 14/64 loss: 0.045828819274902344
Batch 15/64 loss: 0.03677719831466675
Batch 16/64 loss: 0.050715088844299316
Batch 17/64 loss: 0.04054057598114014
Batch 18/64 loss: 0.02589094638824463
Batch 19/64 loss: 0.05273723602294922
Batch 20/64 loss: 0.07519465684890747
Batch 21/64 loss: 0.0474509596824646
Batch 22/64 loss: 0.04911661148071289
Batch 23/64 loss: 0.0561826229095459
Batch 24/64 loss: 0.04816281795501709
Batch 25/64 loss: 0.03942173719406128
Batch 26/64 loss: 0.06125122308731079
Batch 27/64 loss: 0.05750548839569092
Batch 28/64 loss: 0.05373603105545044
Batch 29/64 loss: 0.055044472217559814
Batch 30/64 loss: 0.037071943283081055
Batch 31/64 loss: 0.054746150970458984
Batch 32/64 loss: 0.06404107809066772
Batch 33/64 loss: 0.03813821077346802
Batch 34/64 loss: 0.04478156566619873
Batch 35/64 loss: 0.04055774211883545
Batch 36/64 loss: 0.03347337245941162
Batch 37/64 loss: 0.07074284553527832
Batch 38/64 loss: 0.034293174743652344
Batch 39/64 loss: 0.051817238330841064
Batch 40/64 loss: 0.02968907356262207
Batch 41/64 loss: 0.04203200340270996
Batch 42/64 loss: 0.04672425985336304
Batch 43/64 loss: 0.048670053482055664
Batch 44/64 loss: 0.05681532621383667
Batch 45/64 loss: 0.04888182878494263
Batch 46/64 loss: 0.04065907001495361
Batch 47/64 loss: 0.0306243896484375
Batch 48/64 loss: 0.024819016456604004
Batch 49/64 loss: 0.01816803216934204
Batch 50/64 loss: 0.019261837005615234
Batch 51/64 loss: 0.04740715026855469
Batch 52/64 loss: 0.05925983190536499
Batch 53/64 loss: 0.06141054630279541
Batch 54/64 loss: 0.0840768814086914
Batch 55/64 loss: 0.06146198511123657
Batch 56/64 loss: 0.07968240976333618
Batch 57/64 loss: 0.05516195297241211
Batch 58/64 loss: 0.044869303703308105
Batch 59/64 loss: 0.0650947093963623
Batch 60/64 loss: 0.04021656513214111
Batch 61/64 loss: 0.07833486795425415
Batch 62/64 loss: 0.062307894229888916
Batch 63/64 loss: 0.047894179821014404
Batch 64/64 loss: 0.03327178955078125
Epoch 275  Train loss: 0.04891544136346555  Val loss: 0.11952978495470028
Epoch 276
-------------------------------
Batch 1/64 loss: 0.0498386025428772
Batch 2/64 loss: 0.05039012432098389
Batch 3/64 loss: 0.06304258108139038
Batch 4/64 loss: 0.038872718811035156
Batch 5/64 loss: 0.034258365631103516
Batch 6/64 loss: 0.036551058292388916
Batch 7/64 loss: 0.043701171875
Batch 8/64 loss: 0.05290818214416504
Batch 9/64 loss: 0.009727001190185547
Batch 10/64 loss: 0.0221976637840271
Batch 11/64 loss: 0.06838607788085938
Batch 12/64 loss: 0.04728645086288452
Batch 13/64 loss: 0.0352250337600708
Batch 14/64 loss: 0.04388391971588135
Batch 15/64 loss: 0.06460487842559814
Batch 16/64 loss: 0.06170070171356201
Batch 17/64 loss: 0.07796549797058105
Batch 18/64 loss: 0.05581015348434448
Batch 19/64 loss: 0.06169748306274414
Batch 20/64 loss: 0.04097098112106323
Batch 21/64 loss: 0.04970860481262207
Batch 22/64 loss: 0.03080826997756958
Batch 23/64 loss: 0.05273163318634033
Batch 24/64 loss: 0.03444564342498779
Batch 25/64 loss: 0.05377465486526489
Batch 26/64 loss: 0.030358552932739258
Batch 27/64 loss: 0.04400026798248291
Batch 28/64 loss: 0.047349512577056885
Batch 29/64 loss: 0.049326956272125244
Batch 30/64 loss: 0.06190955638885498
Batch 31/64 loss: 0.06391394138336182
Batch 32/64 loss: 0.015143811702728271
Batch 33/64 loss: 0.06443798542022705
Batch 34/64 loss: 0.05860888957977295
Batch 35/64 loss: 0.056381940841674805
Batch 36/64 loss: 0.04902470111846924
Batch 37/64 loss: 0.06104964017868042
Batch 38/64 loss: 0.04649794101715088
Batch 39/64 loss: 0.05400782823562622
Batch 40/64 loss: 0.05541181564331055
Batch 41/64 loss: 0.061877429485321045
Batch 42/64 loss: 0.04463082551956177
Batch 43/64 loss: 0.046037137508392334
Batch 44/64 loss: 0.050626277923583984
Batch 45/64 loss: 0.014686346054077148
Batch 46/64 loss: 0.04132354259490967
Batch 47/64 loss: 0.03214442729949951
Batch 48/64 loss: 0.05255192518234253
Batch 49/64 loss: 0.03806692361831665
Batch 50/64 loss: 0.06238865852355957
Batch 51/64 loss: 0.046376943588256836
Batch 52/64 loss: 0.05494487285614014
Batch 53/64 loss: 0.03807467222213745
Batch 54/64 loss: 0.0707712173461914
Batch 55/64 loss: 0.04025512933731079
Batch 56/64 loss: 0.031422436237335205
Batch 57/64 loss: 0.07379037141799927
Batch 58/64 loss: 0.04722392559051514
Batch 59/64 loss: 0.07615113258361816
Batch 60/64 loss: 0.06868737936019897
Batch 61/64 loss: 0.03812354803085327
Batch 62/64 loss: 0.06413590908050537
Batch 63/64 loss: 0.06094050407409668
Batch 64/64 loss: 0.05775034427642822
Epoch 276  Train loss: 0.04919929551143272  Val loss: 0.11954696686407135
Epoch 277
-------------------------------
Batch 1/64 loss: 0.05580759048461914
Batch 2/64 loss: 0.08240807056427002
Batch 3/64 loss: 0.03207278251647949
Batch 4/64 loss: 0.0519258975982666
Batch 5/64 loss: 0.03663092851638794
Batch 6/64 loss: 0.04716038703918457
Batch 7/64 loss: 0.06167727708816528
Batch 8/64 loss: 0.03093510866165161
Batch 9/64 loss: 0.059656739234924316
Batch 10/64 loss: 0.06619185209274292
Batch 11/64 loss: 0.0456734299659729
Batch 12/64 loss: 0.03135037422180176
Batch 13/64 loss: 0.04015272855758667
Batch 14/64 loss: 0.03873080015182495
Batch 15/64 loss: 0.0431978702545166
Batch 16/64 loss: 0.05417388677597046
Batch 17/64 loss: 0.04888826608657837
Batch 18/64 loss: 0.04672461748123169
Batch 19/64 loss: 0.04638171195983887
Batch 20/64 loss: 0.05304217338562012
Batch 21/64 loss: 0.027891695499420166
Batch 22/64 loss: 0.04230976104736328
Batch 23/64 loss: 0.03950923681259155
Batch 24/64 loss: 0.03798329830169678
Batch 25/64 loss: 0.04311203956604004
Batch 26/64 loss: 0.06032395362854004
Batch 27/64 loss: 0.024897873401641846
Batch 28/64 loss: 0.06382662057876587
Batch 29/64 loss: 0.04665273427963257
Batch 30/64 loss: 0.041827499866485596
Batch 31/64 loss: 0.05436033010482788
Batch 32/64 loss: 0.04075199365615845
Batch 33/64 loss: 0.03473764657974243
Batch 34/64 loss: 0.07524949312210083
Batch 35/64 loss: 0.08787113428115845
Batch 36/64 loss: 0.04258543252944946
Batch 37/64 loss: 0.028787970542907715
Batch 38/64 loss: 0.04187721014022827
Batch 39/64 loss: 0.04332268238067627
Batch 40/64 loss: 0.05549478530883789
Batch 41/64 loss: 0.07094210386276245
Batch 42/64 loss: 0.04485499858856201
Batch 43/64 loss: 0.044470787048339844
Batch 44/64 loss: 0.0625043511390686
Batch 45/64 loss: 0.028048336505889893
Batch 46/64 loss: 0.04099118709564209
Batch 47/64 loss: 0.07201611995697021
Batch 48/64 loss: 0.0242081880569458
Batch 49/64 loss: 0.05058276653289795
Batch 50/64 loss: 0.04791903495788574
Batch 51/64 loss: 0.041618943214416504
Batch 52/64 loss: 0.04912203550338745
Batch 53/64 loss: 0.0398787260055542
Batch 54/64 loss: 0.08376836776733398
Batch 55/64 loss: 0.055225491523742676
Batch 56/64 loss: 0.039586007595062256
Batch 57/64 loss: 0.03398597240447998
Batch 58/64 loss: 0.06348073482513428
Batch 59/64 loss: 0.04543900489807129
Batch 60/64 loss: 0.060688018798828125
Batch 61/64 loss: 0.05971479415893555
Batch 62/64 loss: 0.05744361877441406
Batch 63/64 loss: 0.05645394325256348
Batch 64/64 loss: 0.05260187387466431
Epoch 277  Train loss: 0.04891844426884371  Val loss: 0.12083781666772063
Epoch 278
-------------------------------
Batch 1/64 loss: 0.04848802089691162
Batch 2/64 loss: 0.057537972927093506
Batch 3/64 loss: 0.09063100814819336
Batch 4/64 loss: 0.03669542074203491
Batch 5/64 loss: 0.030623674392700195
Batch 6/64 loss: 0.03771018981933594
Batch 7/64 loss: 0.050602853298187256
Batch 8/64 loss: 0.04378092288970947
Batch 9/64 loss: 0.09863734245300293
Batch 10/64 loss: 0.07966327667236328
Batch 11/64 loss: 0.05053305625915527
Batch 12/64 loss: 0.0271071195602417
Batch 13/64 loss: 0.06855344772338867
Batch 14/64 loss: 0.04950106143951416
Batch 15/64 loss: 0.05456113815307617
Batch 16/64 loss: 0.030692994594573975
Batch 17/64 loss: 0.0531919002532959
Batch 18/64 loss: 0.043876051902770996
Batch 19/64 loss: 0.060201942920684814
Batch 20/64 loss: 0.03817272186279297
Batch 21/64 loss: 0.031114935874938965
Batch 22/64 loss: 0.050747573375701904
Batch 23/64 loss: 0.04088860750198364
Batch 24/64 loss: 0.030156314373016357
Batch 25/64 loss: 0.050434768199920654
Batch 26/64 loss: 0.048924148082733154
Batch 27/64 loss: 0.09509789943695068
Batch 28/64 loss: 0.04975646734237671
Batch 29/64 loss: 0.033551037311553955
Batch 30/64 loss: 0.05668222904205322
Batch 31/64 loss: 0.03898662328720093
Batch 32/64 loss: 0.040863990783691406
Batch 33/64 loss: 0.046565115451812744
Batch 34/64 loss: 0.046174824237823486
Batch 35/64 loss: 0.04257327318191528
Batch 36/64 loss: 0.021773934364318848
Batch 37/64 loss: 0.04690450429916382
Batch 38/64 loss: 0.024090588092803955
Batch 39/64 loss: 0.0416523814201355
Batch 40/64 loss: 0.055773019790649414
Batch 41/64 loss: 0.05583411455154419
Batch 42/64 loss: 0.033927202224731445
Batch 43/64 loss: 0.035596251487731934
Batch 44/64 loss: 0.08399105072021484
Batch 45/64 loss: 0.056761980056762695
Batch 46/64 loss: 0.05031585693359375
Batch 47/64 loss: 0.027462482452392578
Batch 48/64 loss: 0.06369805335998535
Batch 49/64 loss: 0.061707139015197754
Batch 50/64 loss: 0.07955783605575562
Batch 51/64 loss: 0.044185101985931396
Batch 52/64 loss: 0.023318707942962646
Batch 53/64 loss: 0.03662395477294922
Batch 54/64 loss: 0.040044546127319336
Batch 55/64 loss: 0.05020618438720703
Batch 56/64 loss: 0.05877983570098877
Batch 57/64 loss: 0.05027580261230469
Batch 58/64 loss: 0.037651777267456055
Batch 59/64 loss: 0.07140952348709106
Batch 60/64 loss: 0.04414975643157959
Batch 61/64 loss: 0.05082523822784424
Batch 62/64 loss: 0.03995382785797119
Batch 63/64 loss: 0.0492362380027771
Batch 64/64 loss: 0.08464527130126953
Epoch 278  Train loss: 0.04945052184310614  Val loss: 0.12225925164534054
Epoch 279
-------------------------------
Batch 1/64 loss: 0.047560811042785645
Batch 2/64 loss: 0.0143623948097229
Batch 3/64 loss: 0.016242027282714844
Batch 4/64 loss: 0.05928772687911987
Batch 5/64 loss: 0.01917111873626709
Batch 6/64 loss: 0.04858386516571045
Batch 7/64 loss: 0.053463518619537354
Batch 8/64 loss: 0.04944896697998047
Batch 9/64 loss: 0.0573500394821167
Batch 10/64 loss: 0.052163004875183105
Batch 11/64 loss: 0.03583645820617676
Batch 12/64 loss: 0.014842092990875244
Batch 13/64 loss: 0.03734230995178223
Batch 14/64 loss: 0.03619718551635742
Batch 15/64 loss: 0.06021714210510254
Batch 16/64 loss: 0.0461696982383728
Batch 17/64 loss: 0.05718553066253662
Batch 18/64 loss: 0.03925168514251709
Batch 19/64 loss: 0.05726218223571777
Batch 20/64 loss: 0.06098693609237671
Batch 21/64 loss: 0.02573603391647339
Batch 22/64 loss: 0.06379640102386475
Batch 23/64 loss: 0.03299713134765625
Batch 24/64 loss: 0.04914504289627075
Batch 25/64 loss: 0.03799879550933838
Batch 26/64 loss: 0.036978840827941895
Batch 27/64 loss: 0.028906047344207764
Batch 28/64 loss: 0.05459016561508179
Batch 29/64 loss: 0.08047473430633545
Batch 30/64 loss: 0.04279184341430664
Batch 31/64 loss: 0.06771570444107056
Batch 32/64 loss: 0.048999667167663574
Batch 33/64 loss: 0.043998122215270996
Batch 34/64 loss: 0.060759782791137695
Batch 35/64 loss: 0.06867945194244385
Batch 36/64 loss: 0.04893404245376587
Batch 37/64 loss: 0.06299042701721191
Batch 38/64 loss: 0.06221586465835571
Batch 39/64 loss: 0.054274797439575195
Batch 40/64 loss: 0.06372267007827759
Batch 41/64 loss: 0.043931007385253906
Batch 42/64 loss: 0.055303215980529785
Batch 43/64 loss: 0.04131007194519043
Batch 44/64 loss: 0.038514018058776855
Batch 45/64 loss: 0.03858250379562378
Batch 46/64 loss: 0.05004066228866577
Batch 47/64 loss: 0.0796424150466919
Batch 48/64 loss: 0.061149001121520996
Batch 49/64 loss: 0.07062959671020508
Batch 50/64 loss: 0.052814602851867676
Batch 51/64 loss: 0.04682117700576782
Batch 52/64 loss: 0.06074202060699463
Batch 53/64 loss: 0.05285072326660156
Batch 54/64 loss: 0.04094123840332031
Batch 55/64 loss: 0.04543757438659668
Batch 56/64 loss: 0.03608548641204834
Batch 57/64 loss: 0.05581015348434448
Batch 58/64 loss: 0.05314362049102783
Batch 59/64 loss: 0.047295212745666504
Batch 60/64 loss: 0.05322265625
Batch 61/64 loss: 0.0438232421875
Batch 62/64 loss: 0.038184404373168945
Batch 63/64 loss: 0.025097966194152832
Batch 64/64 loss: 0.06892478466033936
Epoch 279  Train loss: 0.04834033601424274  Val loss: 0.12345042138574869
Epoch 280
-------------------------------
Batch 1/64 loss: 0.022752702236175537
Batch 2/64 loss: 0.032779812812805176
Batch 3/64 loss: 0.04969894886016846
Batch 4/64 loss: 0.05200076103210449
Batch 5/64 loss: 0.06786787509918213
Batch 6/64 loss: 0.05828672647476196
Batch 7/64 loss: 0.030002057552337646
Batch 8/64 loss: 0.023938775062561035
Batch 9/64 loss: 0.04513561725616455
Batch 10/64 loss: 0.055231690406799316
Batch 11/64 loss: 0.02559840679168701
Batch 12/64 loss: 0.0746038556098938
Batch 13/64 loss: 0.06298643350601196
Batch 14/64 loss: 0.02810513973236084
Batch 15/64 loss: 0.04561173915863037
Batch 16/64 loss: 0.05359739065170288
Batch 17/64 loss: 0.044611334800720215
Batch 18/64 loss: 0.06704431772232056
Batch 19/64 loss: 0.050109267234802246
Batch 20/64 loss: 0.02835869789123535
Batch 21/64 loss: 0.07918363809585571
Batch 22/64 loss: 0.024795114994049072
Batch 23/64 loss: 0.06023836135864258
Batch 24/64 loss: 0.06246829032897949
Batch 25/64 loss: 0.040438175201416016
Batch 26/64 loss: 0.020210087299346924
Batch 27/64 loss: 0.017424285411834717
Batch 28/64 loss: 0.06119680404663086
Batch 29/64 loss: 0.03346163034439087
Batch 30/64 loss: 0.06265825033187866
Batch 31/64 loss: 0.04327678680419922
Batch 32/64 loss: 0.05722618103027344
Batch 33/64 loss: 0.04658782482147217
Batch 34/64 loss: 0.0414697527885437
Batch 35/64 loss: 0.038732051849365234
Batch 36/64 loss: 0.03265583515167236
Batch 37/64 loss: 0.04764360189437866
Batch 38/64 loss: 0.04715836048126221
Batch 39/64 loss: 0.05441027879714966
Batch 40/64 loss: 0.035442233085632324
Batch 41/64 loss: 0.046959519386291504
Batch 42/64 loss: 0.040104568004608154
Batch 43/64 loss: 0.06140679121017456
Batch 44/64 loss: 0.03916674852371216
Batch 45/64 loss: 0.06842303276062012
Batch 46/64 loss: 0.05547749996185303
Batch 47/64 loss: 0.05974048376083374
Batch 48/64 loss: 0.044185519218444824
Batch 49/64 loss: 0.033895015716552734
Batch 50/64 loss: 0.05866038799285889
Batch 51/64 loss: 0.0455327033996582
Batch 52/64 loss: 0.06140553951263428
Batch 53/64 loss: 0.027610182762145996
Batch 54/64 loss: 0.035730957984924316
Batch 55/64 loss: 0.044679880142211914
Batch 56/64 loss: 0.043181657791137695
Batch 57/64 loss: 0.049710214138031006
Batch 58/64 loss: 0.03807169198989868
Batch 59/64 loss: 0.066020667552948
Batch 60/64 loss: 0.04351693391799927
Batch 61/64 loss: 0.07135850191116333
Batch 62/64 loss: 0.04816293716430664
Batch 63/64 loss: 0.0860300064086914
Batch 64/64 loss: 0.04349565505981445
Epoch 280  Train loss: 0.04747689845515232  Val loss: 0.12228327723303202
Epoch 281
-------------------------------
Batch 1/64 loss: 0.039175987243652344
Batch 2/64 loss: 0.043891072273254395
Batch 3/64 loss: 0.0503886342048645
Batch 4/64 loss: 0.03864610195159912
Batch 5/64 loss: 0.049772679805755615
Batch 6/64 loss: 0.03534996509552002
Batch 7/64 loss: 0.04497736692428589
Batch 8/64 loss: 0.07047241926193237
Batch 9/64 loss: 0.017533421516418457
Batch 10/64 loss: 0.06547582149505615
Batch 11/64 loss: 0.0591890811920166
Batch 12/64 loss: 0.07142519950866699
Batch 13/64 loss: 0.05894285440444946
Batch 14/64 loss: 0.05329960584640503
Batch 15/64 loss: 0.02847236394882202
Batch 16/64 loss: 0.05301666259765625
Batch 17/64 loss: 0.03666430711746216
Batch 18/64 loss: 0.0381242036819458
Batch 19/64 loss: 0.04965740442276001
Batch 20/64 loss: 0.05988883972167969
Batch 21/64 loss: 0.03586477041244507
Batch 22/64 loss: 0.06812131404876709
Batch 23/64 loss: 0.03363609313964844
Batch 24/64 loss: 0.07649892568588257
Batch 25/64 loss: 0.03059697151184082
Batch 26/64 loss: 0.028251349925994873
Batch 27/64 loss: 0.05848121643066406
Batch 28/64 loss: 0.04160165786743164
Batch 29/64 loss: 0.03496795892715454
Batch 30/64 loss: 0.03823584318161011
Batch 31/64 loss: 0.061545610427856445
Batch 32/64 loss: 0.06912726163864136
Batch 33/64 loss: 0.04071521759033203
Batch 34/64 loss: 0.07303363084793091
Batch 35/64 loss: 0.026439309120178223
Batch 36/64 loss: 0.05125737190246582
Batch 37/64 loss: 0.03352910280227661
Batch 38/64 loss: 0.06377774477005005
Batch 39/64 loss: 0.025008559226989746
Batch 40/64 loss: 0.04694521427154541
Batch 41/64 loss: 0.07022392749786377
Batch 42/64 loss: 0.030566155910491943
Batch 43/64 loss: 0.05452680587768555
Batch 44/64 loss: 0.03684496879577637
Batch 45/64 loss: 0.03402131795883179
Batch 46/64 loss: 0.044333040714263916
Batch 47/64 loss: 0.04566550254821777
Batch 48/64 loss: 0.045055508613586426
Batch 49/64 loss: 0.0410730242729187
Batch 50/64 loss: 0.03975707292556763
Batch 51/64 loss: 0.06395435333251953
Batch 52/64 loss: 0.03458523750305176
Batch 53/64 loss: 0.02770209312438965
Batch 54/64 loss: 0.06029731035232544
Batch 55/64 loss: 0.04978835582733154
Batch 56/64 loss: 0.08066076040267944
Batch 57/64 loss: 0.030898094177246094
Batch 58/64 loss: 0.07913988828659058
Batch 59/64 loss: 0.06284898519515991
Batch 60/64 loss: 0.06202608346939087
Batch 61/64 loss: 0.037050068378448486
Batch 62/64 loss: 0.0685531497001648
Batch 63/64 loss: 0.04893773794174194
Batch 64/64 loss: 0.02806907892227173
Epoch 281  Train loss: 0.04818133910497029  Val loss: 0.12021672008783137
Epoch 282
-------------------------------
Batch 1/64 loss: 0.04962170124053955
Batch 2/64 loss: 0.03328907489776611
Batch 3/64 loss: 0.04141366481781006
Batch 4/64 loss: 0.05928194522857666
Batch 5/64 loss: 0.041815996170043945
Batch 6/64 loss: 0.058121323585510254
Batch 7/64 loss: 0.04779493808746338
Batch 8/64 loss: 0.04511821269989014
Batch 9/64 loss: 0.05650186538696289
Batch 10/64 loss: 0.05365210771560669
Batch 11/64 loss: 0.08287817239761353
Batch 12/64 loss: 0.03509187698364258
Batch 13/64 loss: 0.04431796073913574
Batch 14/64 loss: 0.07114207744598389
Batch 15/64 loss: 0.03289949893951416
Batch 16/64 loss: 0.05641067028045654
Batch 17/64 loss: 0.03816032409667969
Batch 18/64 loss: 0.040957510471343994
Batch 19/64 loss: 0.022072553634643555
Batch 20/64 loss: 0.013320744037628174
Batch 21/64 loss: 0.06318157911300659
Batch 22/64 loss: 0.03697001934051514
Batch 23/64 loss: 0.034552574157714844
Batch 24/64 loss: 0.018216490745544434
Batch 25/64 loss: 0.04135441780090332
Batch 26/64 loss: 0.0583001971244812
Batch 27/64 loss: 0.041264116764068604
Batch 28/64 loss: 0.05222576856613159
Batch 29/64 loss: 0.03297889232635498
Batch 30/64 loss: 0.02476412057876587
Batch 31/64 loss: 0.045269668102264404
Batch 32/64 loss: 0.03446286916732788
Batch 33/64 loss: 0.05577564239501953
Batch 34/64 loss: 0.05068027973175049
Batch 35/64 loss: 0.05929380655288696
Batch 36/64 loss: 0.0613827109336853
Batch 37/64 loss: 0.06317245960235596
Batch 38/64 loss: 0.06767517328262329
Batch 39/64 loss: 0.06427645683288574
Batch 40/64 loss: 0.06502056121826172
Batch 41/64 loss: 0.0296175479888916
Batch 42/64 loss: 0.05039578676223755
Batch 43/64 loss: 0.030069828033447266
Batch 44/64 loss: 0.06581026315689087
Batch 45/64 loss: 0.06127899885177612
Batch 46/64 loss: 0.05107849836349487
Batch 47/64 loss: 0.014392852783203125
Batch 48/64 loss: 0.05987507104873657
Batch 49/64 loss: 0.08310073614120483
Batch 50/64 loss: 0.07465517520904541
Batch 51/64 loss: 0.05427086353302002
Batch 52/64 loss: 0.04537153244018555
Batch 53/64 loss: 0.04876351356506348
Batch 54/64 loss: 0.0763360857963562
Batch 55/64 loss: 0.03543496131896973
Batch 56/64 loss: 0.0710945725440979
Batch 57/64 loss: 0.04368478059768677
Batch 58/64 loss: 0.06456077098846436
Batch 59/64 loss: 0.050647199153900146
Batch 60/64 loss: 0.06647026538848877
Batch 61/64 loss: 0.04919975996017456
Batch 62/64 loss: 0.06045454740524292
Batch 63/64 loss: 0.05640828609466553
Batch 64/64 loss: 0.030864238739013672
Epoch 282  Train loss: 0.04958117802937825  Val loss: 0.11920135926544871
Epoch 283
-------------------------------
Batch 1/64 loss: 0.0394061803817749
Batch 2/64 loss: 0.038807034492492676
Batch 3/64 loss: 0.038223862648010254
Batch 4/64 loss: 0.06921613216400146
Batch 5/64 loss: 0.038500308990478516
Batch 6/64 loss: 0.0741165280342102
Batch 7/64 loss: 0.022500038146972656
Batch 8/64 loss: 0.03361302614212036
Batch 9/64 loss: 0.047216594219207764
Batch 10/64 loss: 0.04495507478713989
Batch 11/64 loss: 0.04550981521606445
Batch 12/64 loss: 0.05322748422622681
Batch 13/64 loss: 0.04769468307495117
Batch 14/64 loss: 0.06334185600280762
Batch 15/64 loss: 0.029238343238830566
Batch 16/64 loss: 0.04502129554748535
Batch 17/64 loss: 0.02944767475128174
Batch 18/64 loss: 0.062463998794555664
Batch 19/64 loss: 0.02873849868774414
Batch 20/64 loss: 0.031908392906188965
Batch 21/64 loss: 0.05247032642364502
Batch 22/64 loss: 0.04817622900009155
Batch 23/64 loss: 0.05767691135406494
Batch 24/64 loss: 0.04068779945373535
Batch 25/64 loss: 0.04387962818145752
Batch 26/64 loss: 0.024013996124267578
Batch 27/64 loss: 0.07067662477493286
Batch 28/64 loss: 0.04528534412384033
Batch 29/64 loss: 0.05050081014633179
Batch 30/64 loss: 0.08595395088195801
Batch 31/64 loss: 0.058047473430633545
Batch 32/64 loss: 0.04249560832977295
Batch 33/64 loss: 0.033179402351379395
Batch 34/64 loss: 0.035217463970184326
Batch 35/64 loss: 0.07286196947097778
Batch 36/64 loss: 0.06419181823730469
Batch 37/64 loss: 0.07183241844177246
Batch 38/64 loss: 0.04906773567199707
Batch 39/64 loss: 0.02153325080871582
Batch 40/64 loss: 0.0386887788772583
Batch 41/64 loss: 0.03758114576339722
Batch 42/64 loss: 0.050023019313812256
Batch 43/64 loss: 0.05215132236480713
Batch 44/64 loss: 0.04669111967086792
Batch 45/64 loss: 0.04549992084503174
Batch 46/64 loss: 0.08415877819061279
Batch 47/64 loss: 0.04419666528701782
Batch 48/64 loss: 0.0286252498626709
Batch 49/64 loss: 0.031562745571136475
Batch 50/64 loss: 0.0664481520652771
Batch 51/64 loss: 0.07444608211517334
Batch 52/64 loss: 0.03549802303314209
Batch 53/64 loss: 0.06714451313018799
Batch 54/64 loss: 0.05338096618652344
Batch 55/64 loss: 0.03837460279464722
Batch 56/64 loss: 0.04197615385055542
Batch 57/64 loss: 0.03669703006744385
Batch 58/64 loss: 0.03001534938812256
Batch 59/64 loss: 0.06695926189422607
Batch 60/64 loss: 0.05557215213775635
Batch 61/64 loss: 0.025967717170715332
Batch 62/64 loss: 0.03757286071777344
Batch 63/64 loss: 0.050106942653656006
Batch 64/64 loss: 0.048429250717163086
Epoch 283  Train loss: 0.04747538940579284  Val loss: 0.12094009946711694
Epoch 284
-------------------------------
Batch 1/64 loss: 0.032073378562927246
Batch 2/64 loss: 0.05961352586746216
Batch 3/64 loss: 0.06330525875091553
Batch 4/64 loss: 0.039381563663482666
Batch 5/64 loss: 0.05180680751800537
Batch 6/64 loss: 0.041227102279663086
Batch 7/64 loss: 0.014748096466064453
Batch 8/64 loss: 0.06450361013412476
Batch 9/64 loss: 0.06643503904342651
Batch 10/64 loss: 0.024644851684570312
Batch 11/64 loss: 0.04358112812042236
Batch 12/64 loss: 0.044031620025634766
Batch 13/64 loss: 0.03784048557281494
Batch 14/64 loss: 0.04014849662780762
Batch 15/64 loss: 0.054454684257507324
Batch 16/64 loss: 0.015782833099365234
Batch 17/64 loss: 0.04853463172912598
Batch 18/64 loss: 0.07038319110870361
Batch 19/64 loss: 0.03397560119628906
Batch 20/64 loss: 0.02407938241958618
Batch 21/64 loss: 0.012818634510040283
Batch 22/64 loss: 0.05122172832489014
Batch 23/64 loss: 0.06675589084625244
Batch 24/64 loss: 0.03611195087432861
Batch 25/64 loss: 0.0513230562210083
Batch 26/64 loss: 0.05225342512130737
Batch 27/64 loss: 0.044624507427215576
Batch 28/64 loss: 0.03744709491729736
Batch 29/64 loss: 0.050009310245513916
Batch 30/64 loss: 0.03732496500015259
Batch 31/64 loss: 0.05950099229812622
Batch 32/64 loss: 0.057661354541778564
Batch 33/64 loss: 0.026308178901672363
Batch 34/64 loss: 0.05046284198760986
Batch 35/64 loss: 0.0624198317527771
Batch 36/64 loss: 0.03138232231140137
Batch 37/64 loss: 0.04125493764877319
Batch 38/64 loss: 0.043080151081085205
Batch 39/64 loss: 0.06114107370376587
Batch 40/64 loss: 0.05687892436981201
Batch 41/64 loss: 0.04497337341308594
Batch 42/64 loss: 0.0710822343826294
Batch 43/64 loss: 0.047464966773986816
Batch 44/64 loss: 0.044646620750427246
Batch 45/64 loss: 0.04665583372116089
Batch 46/64 loss: 0.02161097526550293
Batch 47/64 loss: 0.0466499924659729
Batch 48/64 loss: 0.04188966751098633
Batch 49/64 loss: 0.04831397533416748
Batch 50/64 loss: 0.04771512746810913
Batch 51/64 loss: 0.061747610569000244
Batch 52/64 loss: 0.024227499961853027
Batch 53/64 loss: 0.04159647226333618
Batch 54/64 loss: 0.03435724973678589
Batch 55/64 loss: 0.052767276763916016
Batch 56/64 loss: 0.0634772777557373
Batch 57/64 loss: 0.08455789089202881
Batch 58/64 loss: 0.06612372398376465
Batch 59/64 loss: 0.07865440845489502
Batch 60/64 loss: 0.04862391948699951
Batch 61/64 loss: 0.06351196765899658
Batch 62/64 loss: 0.05257463455200195
Batch 63/64 loss: 0.030314922332763672
Batch 64/64 loss: 0.040883541107177734
Epoch 284  Train loss: 0.046976199804567825  Val loss: 0.12238215949527177
Epoch 285
-------------------------------
Batch 1/64 loss: 0.05809521675109863
Batch 2/64 loss: 0.044954657554626465
Batch 3/64 loss: 0.046703040599823
Batch 4/64 loss: 0.04763507843017578
Batch 5/64 loss: 0.045426905155181885
Batch 6/64 loss: 0.07923233509063721
Batch 7/64 loss: 0.04073631763458252
Batch 8/64 loss: 0.03048616647720337
Batch 9/64 loss: 0.037970662117004395
Batch 10/64 loss: 0.030901789665222168
Batch 11/64 loss: 0.04136836528778076
Batch 12/64 loss: 0.05249190330505371
Batch 13/64 loss: 0.031931519508361816
Batch 14/64 loss: 0.022465765476226807
Batch 15/64 loss: 0.05978286266326904
Batch 16/64 loss: 0.06478250026702881
Batch 17/64 loss: 0.05511653423309326
Batch 18/64 loss: 0.04777026176452637
Batch 19/64 loss: 0.05432063341140747
Batch 20/64 loss: 0.021616637706756592
Batch 21/64 loss: 0.041064321994781494
Batch 22/64 loss: 0.03444582223892212
Batch 23/64 loss: 0.05602991580963135
Batch 24/64 loss: 0.0694018006324768
Batch 25/64 loss: 0.0376778244972229
Batch 26/64 loss: 0.036439478397369385
Batch 27/64 loss: 0.01805013418197632
Batch 28/64 loss: 0.02794802188873291
Batch 29/64 loss: 0.010609030723571777
Batch 30/64 loss: 0.0497090220451355
Batch 31/64 loss: 0.005805373191833496
Batch 32/64 loss: 0.07085263729095459
Batch 33/64 loss: 0.03819054365158081
Batch 34/64 loss: 0.035280466079711914
Batch 35/64 loss: 0.06159842014312744
Batch 36/64 loss: 0.041823744773864746
Batch 37/64 loss: 0.0594019889831543
Batch 38/64 loss: 0.03815162181854248
Batch 39/64 loss: 0.06537151336669922
Batch 40/64 loss: 0.08301359415054321
Batch 41/64 loss: 0.06499052047729492
Batch 42/64 loss: 0.05536919832229614
Batch 43/64 loss: 0.050695061683654785
Batch 44/64 loss: 0.03115677833557129
Batch 45/64 loss: 0.054266512393951416
Batch 46/64 loss: 0.03599083423614502
Batch 47/64 loss: 0.08470302820205688
Batch 48/64 loss: 0.06958132982254028
Batch 49/64 loss: 0.019679129123687744
Batch 50/64 loss: 0.04971134662628174
Batch 51/64 loss: 0.06407934427261353
Batch 52/64 loss: 0.04083198308944702
Batch 53/64 loss: 0.08061587810516357
Batch 54/64 loss: 0.0736994743347168
Batch 55/64 loss: 0.04325300455093384
Batch 56/64 loss: 0.04768627882003784
Batch 57/64 loss: 0.02043241262435913
Batch 58/64 loss: 0.04643362760543823
Batch 59/64 loss: 0.04700440168380737
Batch 60/64 loss: 0.0505450963973999
Batch 61/64 loss: 0.044958531856536865
Batch 62/64 loss: 0.0911780595779419
Batch 63/64 loss: 0.038252055644989014
Batch 64/64 loss: 0.05122113227844238
Epoch 285  Train loss: 0.04765779083850337  Val loss: 0.11820871395753421
Epoch 286
-------------------------------
Batch 1/64 loss: 0.0434228777885437
Batch 2/64 loss: 0.05465543270111084
Batch 3/64 loss: 0.03076237440109253
Batch 4/64 loss: 0.04968780279159546
Batch 5/64 loss: 0.0512581467628479
Batch 6/64 loss: 0.05486363172531128
Batch 7/64 loss: 0.07018494606018066
Batch 8/64 loss: 0.04978513717651367
Batch 9/64 loss: 0.06601232290267944
Batch 10/64 loss: 0.044251441955566406
Batch 11/64 loss: 0.05988287925720215
Batch 12/64 loss: 0.039452552795410156
Batch 13/64 loss: 0.06615555286407471
Batch 14/64 loss: 0.05236399173736572
Batch 15/64 loss: 0.053760826587677
Batch 16/64 loss: 0.07943767309188843
Batch 17/64 loss: 0.07906931638717651
Batch 18/64 loss: 0.053152263164520264
Batch 19/64 loss: 0.040648818016052246
Batch 20/64 loss: 0.05638819932937622
Batch 21/64 loss: 0.03682661056518555
Batch 22/64 loss: 0.04484677314758301
Batch 23/64 loss: 0.04780423641204834
Batch 24/64 loss: 0.02146780490875244
Batch 25/64 loss: 0.02324455976486206
Batch 26/64 loss: 0.04913151264190674
Batch 27/64 loss: 0.029970228672027588
Batch 28/64 loss: 0.04287022352218628
Batch 29/64 loss: 0.03346896171569824
Batch 30/64 loss: 0.05411100387573242
Batch 31/64 loss: 0.04274183511734009
Batch 32/64 loss: 0.05594944953918457
Batch 33/64 loss: 0.06641519069671631
Batch 34/64 loss: 0.027911067008972168
Batch 35/64 loss: 0.06550407409667969
Batch 36/64 loss: 0.043766796588897705
Batch 37/64 loss: 0.04081064462661743
Batch 38/64 loss: 0.033607304096221924
Batch 39/64 loss: 0.049421727657318115
Batch 40/64 loss: 0.04303252696990967
Batch 41/64 loss: 0.07560408115386963
Batch 42/64 loss: 0.039126038551330566
Batch 43/64 loss: 0.0702216625213623
Batch 44/64 loss: 0.03429192304611206
Batch 45/64 loss: 0.05679291486740112
Batch 46/64 loss: 0.03016066551208496
Batch 47/64 loss: 0.05949908494949341
Batch 48/64 loss: 0.05582326650619507
Batch 49/64 loss: 0.058083534240722656
Batch 50/64 loss: 0.0038268566131591797
Batch 51/64 loss: 0.0471266508102417
Batch 52/64 loss: 0.02132338285446167
Batch 53/64 loss: 0.05355268716812134
Batch 54/64 loss: 0.024458765983581543
Batch 55/64 loss: 0.0314326286315918
Batch 56/64 loss: 0.053446173667907715
Batch 57/64 loss: 0.026329874992370605
Batch 58/64 loss: 0.03453993797302246
Batch 59/64 loss: 0.05644690990447998
Batch 60/64 loss: 0.06992846727371216
Batch 61/64 loss: 0.04442596435546875
Batch 62/64 loss: 0.04626178741455078
Batch 63/64 loss: 0.028449833393096924
Batch 64/64 loss: 0.02872300148010254
Epoch 286  Train loss: 0.04691441666846182  Val loss: 0.11617560034355347
Saving best model, epoch: 286
Epoch 287
-------------------------------
Batch 1/64 loss: 0.03516143560409546
Batch 2/64 loss: 0.029218196868896484
Batch 3/64 loss: 0.02161186933517456
Batch 4/64 loss: 0.06948679685592651
Batch 5/64 loss: 0.09160971641540527
Batch 6/64 loss: 0.017310500144958496
Batch 7/64 loss: 0.0592915415763855
Batch 8/64 loss: 0.049980998039245605
Batch 9/64 loss: 0.03611809015274048
Batch 10/64 loss: 0.07789874076843262
Batch 11/64 loss: 0.03485614061355591
Batch 12/64 loss: 0.038240671157836914
Batch 13/64 loss: 0.050596535205841064
Batch 14/64 loss: 0.04009604454040527
Batch 15/64 loss: 0.028065919876098633
Batch 16/64 loss: 0.027582168579101562
Batch 17/64 loss: 0.04308849573135376
Batch 18/64 loss: 0.07022029161453247
Batch 19/64 loss: 0.050152361392974854
Batch 20/64 loss: 0.04614853858947754
Batch 21/64 loss: 0.028670012950897217
Batch 22/64 loss: 0.018490254878997803
Batch 23/64 loss: 0.03989142179489136
Batch 24/64 loss: 0.055583298206329346
Batch 25/64 loss: 0.03930675983428955
Batch 26/64 loss: 0.013427674770355225
Batch 27/64 loss: 0.03334009647369385
Batch 28/64 loss: 0.04653668403625488
Batch 29/64 loss: 0.04128420352935791
Batch 30/64 loss: 0.03185379505157471
Batch 31/64 loss: 0.07438540458679199
Batch 32/64 loss: 0.04712998867034912
Batch 33/64 loss: 0.03859269618988037
Batch 34/64 loss: 0.03817635774612427
Batch 35/64 loss: 0.05958837270736694
Batch 36/64 loss: 0.04635220766067505
Batch 37/64 loss: 0.03441309928894043
Batch 38/64 loss: 0.03419071435928345
Batch 39/64 loss: 0.05149853229522705
Batch 40/64 loss: 0.05513268709182739
Batch 41/64 loss: 0.03082871437072754
Batch 42/64 loss: 0.05898362398147583
Batch 43/64 loss: 0.09570372104644775
Batch 44/64 loss: 0.0793389081954956
Batch 45/64 loss: 0.004104733467102051
Batch 46/64 loss: 0.05606269836425781
Batch 47/64 loss: 0.06045377254486084
Batch 48/64 loss: 0.05023980140686035
Batch 49/64 loss: 0.01873248815536499
Batch 50/64 loss: 0.03260535001754761
Batch 51/64 loss: 0.02584671974182129
Batch 52/64 loss: 0.05910152196884155
Batch 53/64 loss: 0.04482448101043701
Batch 54/64 loss: 0.04672253131866455
Batch 55/64 loss: 0.03398638963699341
Batch 56/64 loss: 0.040941596031188965
Batch 57/64 loss: 0.0590246319770813
Batch 58/64 loss: 0.03699636459350586
Batch 59/64 loss: 0.05153733491897583
Batch 60/64 loss: 0.055675208568573
Batch 61/64 loss: 0.0635528564453125
Batch 62/64 loss: 0.05750131607055664
Batch 63/64 loss: 0.05570054054260254
Batch 64/64 loss: 0.058039963245391846
Epoch 287  Train loss: 0.045593326933243694  Val loss: 0.11972841077653813
Epoch 288
-------------------------------
Batch 1/64 loss: 0.03521448373794556
Batch 2/64 loss: 0.03222978115081787
Batch 3/64 loss: 0.03727436065673828
Batch 4/64 loss: 0.06639772653579712
Batch 5/64 loss: 0.03505825996398926
Batch 6/64 loss: 0.02479863166809082
Batch 7/64 loss: 0.03460407257080078
Batch 8/64 loss: 0.03969985246658325
Batch 9/64 loss: 0.045288920402526855
Batch 10/64 loss: 0.04502373933792114
Batch 11/64 loss: 0.03152942657470703
Batch 12/64 loss: 0.05694246292114258
Batch 13/64 loss: 0.0575069785118103
Batch 14/64 loss: 0.061264753341674805
Batch 15/64 loss: 0.04667562246322632
Batch 16/64 loss: 0.024932265281677246
Batch 17/64 loss: 0.046353936195373535
Batch 18/64 loss: 0.034584641456604004
Batch 19/64 loss: 0.04493993520736694
Batch 20/64 loss: 0.036881864070892334
Batch 21/64 loss: 0.03037738800048828
Batch 22/64 loss: 0.041301429271698
Batch 23/64 loss: 0.023133814334869385
Batch 24/64 loss: 0.036477863788604736
Batch 25/64 loss: 0.04391711950302124
Batch 26/64 loss: 0.05141419172286987
Batch 27/64 loss: 0.05013686418533325
Batch 28/64 loss: 0.06829380989074707
Batch 29/64 loss: 0.035978078842163086
Batch 30/64 loss: 0.07341450452804565
Batch 31/64 loss: 0.05890309810638428
Batch 32/64 loss: 0.05145692825317383
Batch 33/64 loss: 0.05727744102478027
Batch 34/64 loss: 0.04929673671722412
Batch 35/64 loss: 0.04282844066619873
Batch 36/64 loss: 0.04631364345550537
Batch 37/64 loss: 0.05061995983123779
Batch 38/64 loss: 0.027383625507354736
Batch 39/64 loss: 0.0298917293548584
Batch 40/64 loss: 0.05346804857254028
Batch 41/64 loss: 0.07420384883880615
Batch 42/64 loss: 0.04606068134307861
Batch 43/64 loss: 0.048756182193756104
Batch 44/64 loss: 0.04649239778518677
Batch 45/64 loss: 0.0623285174369812
Batch 46/64 loss: 0.026569008827209473
Batch 47/64 loss: 0.055547237396240234
Batch 48/64 loss: 0.06528699398040771
Batch 49/64 loss: 0.04050713777542114
Batch 50/64 loss: 0.05902576446533203
Batch 51/64 loss: 0.08694320917129517
Batch 52/64 loss: 0.05235576629638672
Batch 53/64 loss: 0.04437971115112305
Batch 54/64 loss: 0.03865319490432739
Batch 55/64 loss: 0.04450869560241699
Batch 56/64 loss: 0.031149208545684814
Batch 57/64 loss: 0.040531694889068604
Batch 58/64 loss: 0.04233360290527344
Batch 59/64 loss: 0.04928946495056152
Batch 60/64 loss: 0.02528226375579834
Batch 61/64 loss: 0.06560873985290527
Batch 62/64 loss: 0.042709290981292725
Batch 63/64 loss: 0.021802186965942383
Batch 64/64 loss: 0.0615391731262207
Epoch 288  Train loss: 0.04573436250873641  Val loss: 0.12094755557804174
Epoch 289
-------------------------------
Batch 1/64 loss: 0.05862545967102051
Batch 2/64 loss: 0.04176276922225952
Batch 3/64 loss: 0.014161527156829834
Batch 4/64 loss: 0.052032470703125
Batch 5/64 loss: 0.03289198875427246
Batch 6/64 loss: 0.019904792308807373
Batch 7/64 loss: 0.06120800971984863
Batch 8/64 loss: 0.00777590274810791
Batch 9/64 loss: 0.04096865653991699
Batch 10/64 loss: 0.0753893256187439
Batch 11/64 loss: 0.031864166259765625
Batch 12/64 loss: 0.059275269508361816
Batch 13/64 loss: 0.033300042152404785
Batch 14/64 loss: 0.03999096155166626
Batch 15/64 loss: 0.05771327018737793
Batch 16/64 loss: 0.07161104679107666
Batch 17/64 loss: 0.05146777629852295
Batch 18/64 loss: 0.02236407995223999
Batch 19/64 loss: 0.04656487703323364
Batch 20/64 loss: 0.062183499336242676
Batch 21/64 loss: 0.051046013832092285
Batch 22/64 loss: 0.028334200382232666
Batch 23/64 loss: 0.025934934616088867
Batch 24/64 loss: 0.06060689687728882
Batch 25/64 loss: 0.0725775957107544
Batch 26/64 loss: 0.058806657791137695
Batch 27/64 loss: 0.045269131660461426
Batch 28/64 loss: 0.0575261116027832
Batch 29/64 loss: 0.03531968593597412
Batch 30/64 loss: 0.09314107894897461
Batch 31/64 loss: 0.04730415344238281
Batch 32/64 loss: 0.05277222394943237
Batch 33/64 loss: 0.036253273487091064
Batch 34/64 loss: 0.04677474498748779
Batch 35/64 loss: 0.04631173610687256
Batch 36/64 loss: 0.05404984951019287
Batch 37/64 loss: 0.01834326982498169
Batch 38/64 loss: 0.049762189388275146
Batch 39/64 loss: 0.05447298288345337
Batch 40/64 loss: 0.04824274778366089
Batch 41/64 loss: 0.05099940299987793
Batch 42/64 loss: 0.035585880279541016
Batch 43/64 loss: 0.044393837451934814
Batch 44/64 loss: 0.05950319766998291
Batch 45/64 loss: 0.046519696712493896
Batch 46/64 loss: 0.021682143211364746
Batch 47/64 loss: 0.05245471000671387
Batch 48/64 loss: 0.018110454082489014
Batch 49/64 loss: 0.04110771417617798
Batch 50/64 loss: 0.0457034707069397
Batch 51/64 loss: 0.03373676538467407
Batch 52/64 loss: 0.03463947772979736
Batch 53/64 loss: 0.051348865032196045
Batch 54/64 loss: 0.035254478454589844
Batch 55/64 loss: 0.02033257484436035
Batch 56/64 loss: 0.027592718601226807
Batch 57/64 loss: 0.023399293422698975
Batch 58/64 loss: 0.02311605215072632
Batch 59/64 loss: 0.07877242565155029
Batch 60/64 loss: 0.03201889991760254
Batch 61/64 loss: 0.060059964656829834
Batch 62/64 loss: 0.03258413076400757
Batch 63/64 loss: 0.07897144556045532
Batch 64/64 loss: 0.02603280544281006
Epoch 289  Train loss: 0.04441282468683579  Val loss: 0.11873623595614613
Epoch 290
-------------------------------
Batch 1/64 loss: 0.04645419120788574
Batch 2/64 loss: 0.053118348121643066
Batch 3/64 loss: 0.0586625337600708
Batch 4/64 loss: 0.05641806125640869
Batch 5/64 loss: 0.028473854064941406
Batch 6/64 loss: 0.03009265661239624
Batch 7/64 loss: 0.020778536796569824
Batch 8/64 loss: 0.06551241874694824
Batch 9/64 loss: 0.04054027795791626
Batch 10/64 loss: 0.03360486030578613
Batch 11/64 loss: 0.048548221588134766
Batch 12/64 loss: 0.0474468469619751
Batch 13/64 loss: 0.03285634517669678
Batch 14/64 loss: 0.020832419395446777
Batch 15/64 loss: 0.039206087589263916
Batch 16/64 loss: 0.046647489070892334
Batch 17/64 loss: 0.046591877937316895
Batch 18/64 loss: 0.03774160146713257
Batch 19/64 loss: 0.04434549808502197
Batch 20/64 loss: 0.06338399648666382
Batch 21/64 loss: 0.04282641410827637
Batch 22/64 loss: 0.04384422302246094
Batch 23/64 loss: 0.03695344924926758
Batch 24/64 loss: 0.042350947856903076
Batch 25/64 loss: 0.058780133724212646
Batch 26/64 loss: 0.046884357929229736
Batch 27/64 loss: 0.04242515563964844
Batch 28/64 loss: 0.03777515888214111
Batch 29/64 loss: 0.031245827674865723
Batch 30/64 loss: 0.050861358642578125
Batch 31/64 loss: 0.04552847146987915
Batch 32/64 loss: 0.04835265874862671
Batch 33/64 loss: 0.04926300048828125
Batch 34/64 loss: 0.030196547508239746
Batch 35/64 loss: 0.08032786846160889
Batch 36/64 loss: 0.043654561042785645
Batch 37/64 loss: 0.06141263246536255
Batch 38/64 loss: 0.0361400842666626
Batch 39/64 loss: 0.05467897653579712
Batch 40/64 loss: 0.0587848424911499
Batch 41/64 loss: 0.028567492961883545
Batch 42/64 loss: 0.05423849821090698
Batch 43/64 loss: 0.048564136028289795
Batch 44/64 loss: 0.025023937225341797
Batch 45/64 loss: 0.03221559524536133
Batch 46/64 loss: 0.03917819261550903
Batch 47/64 loss: 0.04760384559631348
Batch 48/64 loss: 0.042441487312316895
Batch 49/64 loss: 0.04235219955444336
Batch 50/64 loss: 0.08444392681121826
Batch 51/64 loss: 0.06387001276016235
Batch 52/64 loss: 0.05618786811828613
Batch 53/64 loss: 0.07003307342529297
Batch 54/64 loss: 0.03096950054168701
Batch 55/64 loss: 0.06186378002166748
Batch 56/64 loss: 0.026858508586883545
Batch 57/64 loss: 0.0552215576171875
Batch 58/64 loss: 0.05649930238723755
Batch 59/64 loss: 0.031588852405548096
Batch 60/64 loss: 0.05075347423553467
Batch 61/64 loss: 0.04687732458114624
Batch 62/64 loss: 0.032467782497406006
Batch 63/64 loss: 0.03264939785003662
Batch 64/64 loss: 0.03817826509475708
Epoch 290  Train loss: 0.04537484295227948  Val loss: 0.11846337101303835
Epoch 291
-------------------------------
Batch 1/64 loss: 0.07712101936340332
Batch 2/64 loss: 0.058901429176330566
Batch 3/64 loss: 0.038292884826660156
Batch 4/64 loss: 0.06680178642272949
Batch 5/64 loss: 0.05346834659576416
Batch 6/64 loss: 0.04882413148880005
Batch 7/64 loss: 0.0411297082901001
Batch 8/64 loss: 0.04006004333496094
Batch 9/64 loss: 0.03020411729812622
Batch 10/64 loss: 0.049861133098602295
Batch 11/64 loss: 0.0975637435913086
Batch 12/64 loss: 0.04016709327697754
Batch 13/64 loss: 0.04750514030456543
Batch 14/64 loss: 0.04147696495056152
Batch 15/64 loss: 0.04483485221862793
Batch 16/64 loss: 0.0663987398147583
Batch 17/64 loss: 0.043885111808776855
Batch 18/64 loss: 0.024058997631072998
Batch 19/64 loss: 0.050085604190826416
Batch 20/64 loss: 0.03960704803466797
Batch 21/64 loss: 0.05402219295501709
Batch 22/64 loss: 0.045385777950286865
Batch 23/64 loss: 0.06515568494796753
Batch 24/64 loss: 0.04905647039413452
Batch 25/64 loss: 0.048958003520965576
Batch 26/64 loss: 0.04714924097061157
Batch 27/64 loss: 0.04071080684661865
Batch 28/64 loss: 0.02710092067718506
Batch 29/64 loss: 0.03882265090942383
Batch 30/64 loss: 0.024137556552886963
Batch 31/64 loss: 0.04360479116439819
Batch 32/64 loss: 0.07498925924301147
Batch 33/64 loss: 0.042981088161468506
Batch 34/64 loss: 0.03216671943664551
Batch 35/64 loss: 0.05424755811691284
Batch 36/64 loss: 0.04744219779968262
Batch 37/64 loss: 0.04820096492767334
Batch 38/64 loss: 0.045219361782073975
Batch 39/64 loss: 0.034679293632507324
Batch 40/64 loss: 0.027138710021972656
Batch 41/64 loss: 0.019622206687927246
Batch 42/64 loss: 0.0350380539894104
Batch 43/64 loss: 0.036453962326049805
Batch 44/64 loss: 0.034963786602020264
Batch 45/64 loss: 0.02064889669418335
Batch 46/64 loss: 0.05745428800582886
Batch 47/64 loss: 0.05709254741668701
Batch 48/64 loss: 0.03906738758087158
Batch 49/64 loss: 0.04240769147872925
Batch 50/64 loss: 0.04027527570724487
Batch 51/64 loss: 0.03740572929382324
Batch 52/64 loss: 0.06455719470977783
Batch 53/64 loss: 0.040000081062316895
Batch 54/64 loss: 0.052247583866119385
Batch 55/64 loss: 0.06356316804885864
Batch 56/64 loss: 0.0505639910697937
Batch 57/64 loss: 0.05522167682647705
Batch 58/64 loss: 0.05149585008621216
Batch 59/64 loss: 0.03833961486816406
Batch 60/64 loss: 0.053628504276275635
Batch 61/64 loss: 0.040239930152893066
Batch 62/64 loss: 0.049729883670806885
Batch 63/64 loss: 0.016921520233154297
Batch 64/64 loss: 0.03783226013183594
Epoch 291  Train loss: 0.04575263004676969  Val loss: 0.11750506043843797
Epoch 292
-------------------------------
Batch 1/64 loss: 0.052673161029815674
Batch 2/64 loss: 0.04216420650482178
Batch 3/64 loss: 0.056685447692871094
Batch 4/64 loss: 0.08071112632751465
Batch 5/64 loss: 0.047381699085235596
Batch 6/64 loss: 0.039262473583221436
Batch 7/64 loss: 0.08096504211425781
Batch 8/64 loss: 0.03344905376434326
Batch 9/64 loss: 0.03877145051956177
Batch 10/64 loss: 0.06891900300979614
Batch 11/64 loss: 0.041259765625
Batch 12/64 loss: 0.01782238483428955
Batch 13/64 loss: 0.051348090171813965
Batch 14/64 loss: 0.03740423917770386
Batch 15/64 loss: 0.05571097135543823
Batch 16/64 loss: 0.06990718841552734
Batch 17/64 loss: 0.02741551399230957
Batch 18/64 loss: 0.05382394790649414
Batch 19/64 loss: 0.07711672782897949
Batch 20/64 loss: 0.03465926647186279
Batch 21/64 loss: 0.07141530513763428
Batch 22/64 loss: 0.03686940670013428
Batch 23/64 loss: 0.049544572830200195
Batch 24/64 loss: 0.04859358072280884
Batch 25/64 loss: 0.03691279888153076
Batch 26/64 loss: 0.026825666427612305
Batch 27/64 loss: 0.041346967220306396
Batch 28/64 loss: 0.04719620943069458
Batch 29/64 loss: 0.05478847026824951
Batch 30/64 loss: 0.0408669114112854
Batch 31/64 loss: 0.051111698150634766
Batch 32/64 loss: 0.05312955379486084
Batch 33/64 loss: 0.04536294937133789
Batch 34/64 loss: 0.0682634711265564
Batch 35/64 loss: 0.028845608234405518
Batch 36/64 loss: 0.0688057541847229
Batch 37/64 loss: 0.02571195363998413
Batch 38/64 loss: 0.015927910804748535
Batch 39/64 loss: 0.049919724464416504
Batch 40/64 loss: 0.026073753833770752
Batch 41/64 loss: 0.038377463817596436
Batch 42/64 loss: 0.05204284191131592
Batch 43/64 loss: 0.05488157272338867
Batch 44/64 loss: 0.014269649982452393
Batch 45/64 loss: 0.040596961975097656
Batch 46/64 loss: 0.038077354431152344
Batch 47/64 loss: 0.0593947172164917
Batch 48/64 loss: 0.04533421993255615
Batch 49/64 loss: 0.05110687017440796
Batch 50/64 loss: 0.04128009080886841
Batch 51/64 loss: 0.05297952890396118
Batch 52/64 loss: 0.05466395616531372
Batch 53/64 loss: 0.04144287109375
Batch 54/64 loss: 0.015926003456115723
Batch 55/64 loss: 0.0378534197807312
Batch 56/64 loss: 0.031962811946868896
Batch 57/64 loss: 0.031985342502593994
Batch 58/64 loss: 0.031502366065979004
Batch 59/64 loss: 0.04774487018585205
Batch 60/64 loss: 0.06594514846801758
Batch 61/64 loss: 0.06372678279876709
Batch 62/64 loss: 0.02733480930328369
Batch 63/64 loss: 0.030032694339752197
Batch 64/64 loss: 0.03677332401275635
Epoch 292  Train loss: 0.04534910379671583  Val loss: 0.119818594037872
Epoch 293
-------------------------------
Batch 1/64 loss: 0.030077755451202393
Batch 2/64 loss: 0.03586322069168091
Batch 3/64 loss: 0.050749361515045166
Batch 4/64 loss: 0.049082040786743164
Batch 5/64 loss: 0.04872691631317139
Batch 6/64 loss: 0.0013269782066345215
Batch 7/64 loss: 0.03597664833068848
Batch 8/64 loss: 0.03429007530212402
Batch 9/64 loss: 0.04812806844711304
Batch 10/64 loss: 0.04200786352157593
Batch 11/64 loss: 0.048580825328826904
Batch 12/64 loss: 0.049562692642211914
Batch 13/64 loss: 0.02809596061706543
Batch 14/64 loss: 0.007058322429656982
Batch 15/64 loss: 0.02427738904953003
Batch 16/64 loss: 0.06089729070663452
Batch 17/64 loss: 0.04570579528808594
Batch 18/64 loss: 0.01831376552581787
Batch 19/64 loss: 0.04888230562210083
Batch 20/64 loss: 0.053165555000305176
Batch 21/64 loss: 0.05845379829406738
Batch 22/64 loss: 0.04500114917755127
Batch 23/64 loss: 0.05907034873962402
Batch 24/64 loss: 0.0406336784362793
Batch 25/64 loss: 0.05049341917037964
Batch 26/64 loss: 0.043676793575286865
Batch 27/64 loss: 0.04181981086730957
Batch 28/64 loss: 0.03458172082901001
Batch 29/64 loss: 0.020369648933410645
Batch 30/64 loss: 0.04585772752761841
Batch 31/64 loss: 0.03022998571395874
Batch 32/64 loss: 0.0529102087020874
Batch 33/64 loss: 0.05112957954406738
Batch 34/64 loss: 0.05560886859893799
Batch 35/64 loss: 0.08431196212768555
Batch 36/64 loss: 0.025267302989959717
Batch 37/64 loss: 0.04684227705001831
Batch 38/64 loss: 0.06309586763381958
Batch 39/64 loss: 0.03581869602203369
Batch 40/64 loss: 0.03536653518676758
Batch 41/64 loss: 0.0528033971786499
Batch 42/64 loss: 0.031835854053497314
Batch 43/64 loss: 0.05224335193634033
Batch 44/64 loss: 0.049576520919799805
Batch 45/64 loss: 0.0673753023147583
Batch 46/64 loss: 0.05052626132965088
Batch 47/64 loss: 0.05718642473220825
Batch 48/64 loss: 0.033122897148132324
Batch 49/64 loss: 0.039121925830841064
Batch 50/64 loss: 0.04748821258544922
Batch 51/64 loss: 0.0426558256149292
Batch 52/64 loss: 0.04478764533996582
Batch 53/64 loss: 0.038843393325805664
Batch 54/64 loss: 0.04365569353103638
Batch 55/64 loss: 0.025321483612060547
Batch 56/64 loss: 0.07457691431045532
Batch 57/64 loss: 0.03939831256866455
Batch 58/64 loss: 0.04097163677215576
Batch 59/64 loss: 0.02970355749130249
Batch 60/64 loss: 0.047639667987823486
Batch 61/64 loss: 0.04724329710006714
Batch 62/64 loss: 0.060356736183166504
Batch 63/64 loss: 0.0700036883354187
Batch 64/64 loss: 0.028323054313659668
Epoch 293  Train loss: 0.04374883922876096  Val loss: 0.11653997521220204
Epoch 294
-------------------------------
Batch 1/64 loss: 0.0614314079284668
Batch 2/64 loss: 0.05246472358703613
Batch 3/64 loss: 0.043034493923187256
Batch 4/64 loss: 0.039602696895599365
Batch 5/64 loss: 0.026061415672302246
Batch 6/64 loss: 0.05141711235046387
Batch 7/64 loss: 0.04570209980010986
Batch 8/64 loss: 0.06387412548065186
Batch 9/64 loss: 0.03370696306228638
Batch 10/64 loss: 0.023709774017333984
Batch 11/64 loss: 0.06152331829071045
Batch 12/64 loss: 0.037503063678741455
Batch 13/64 loss: 0.02776104211807251
Batch 14/64 loss: 0.04023796319961548
Batch 15/64 loss: 0.04789233207702637
Batch 16/64 loss: 0.06016033887863159
Batch 17/64 loss: 0.03256118297576904
Batch 18/64 loss: 0.043471455574035645
Batch 19/64 loss: 0.021366357803344727
Batch 20/64 loss: 0.04676896333694458
Batch 21/64 loss: 0.055043816566467285
Batch 22/64 loss: 0.03310459852218628
Batch 23/64 loss: 0.037357449531555176
Batch 24/64 loss: 0.03385472297668457
Batch 25/64 loss: 0.019246816635131836
Batch 26/64 loss: 0.01399165391921997
Batch 27/64 loss: 0.03201490640640259
Batch 28/64 loss: 0.042507171630859375
Batch 29/64 loss: 0.03494679927825928
Batch 30/64 loss: 0.044534504413604736
Batch 31/64 loss: 0.0482211709022522
Batch 32/64 loss: 0.03496253490447998
Batch 33/64 loss: 0.060342490673065186
Batch 34/64 loss: 0.06814837455749512
Batch 35/64 loss: 0.03047943115234375
Batch 36/64 loss: 0.03866767883300781
Batch 37/64 loss: 0.06350862979888916
Batch 38/64 loss: 0.02908724546432495
Batch 39/64 loss: 0.03921133279800415
Batch 40/64 loss: 0.07681506872177124
Batch 41/64 loss: 0.022089660167694092
Batch 42/64 loss: 0.017602741718292236
Batch 43/64 loss: 0.03318756818771362
Batch 44/64 loss: 0.07009238004684448
Batch 45/64 loss: 0.03368937969207764
Batch 46/64 loss: 0.06112253665924072
Batch 47/64 loss: 0.0519866943359375
Batch 48/64 loss: 0.0443495512008667
Batch 49/64 loss: 0.032958030700683594
Batch 50/64 loss: 0.033743858337402344
Batch 51/64 loss: 0.07981884479522705
Batch 52/64 loss: 0.05183917284011841
Batch 53/64 loss: 0.042791664600372314
Batch 54/64 loss: 0.025905370712280273
Batch 55/64 loss: 0.049443602561950684
Batch 56/64 loss: 0.04223215579986572
Batch 57/64 loss: 0.046853721141815186
Batch 58/64 loss: 0.03385883569717407
Batch 59/64 loss: 0.08563417196273804
Batch 60/64 loss: 0.037103474140167236
Batch 61/64 loss: 0.051465511322021484
Batch 62/64 loss: 0.02950650453567505
Batch 63/64 loss: 0.04346746206283569
Batch 64/64 loss: 0.07394146919250488
Epoch 294  Train loss: 0.04349013683842678  Val loss: 0.11671671056255851
Epoch 295
-------------------------------
Batch 1/64 loss: 0.018846511840820312
Batch 2/64 loss: 0.06866335868835449
Batch 3/64 loss: 0.054406702518463135
Batch 4/64 loss: 0.01827329397201538
Batch 5/64 loss: 0.022882819175720215
Batch 6/64 loss: 0.02483534812927246
Batch 7/64 loss: 0.07629847526550293
Batch 8/64 loss: 0.04803144931793213
Batch 9/64 loss: 0.04661524295806885
Batch 10/64 loss: 0.027243494987487793
Batch 11/64 loss: 0.04388028383255005
Batch 12/64 loss: 0.02888435125350952
Batch 13/64 loss: 0.060004353523254395
Batch 14/64 loss: 0.03463113307952881
Batch 15/64 loss: 0.06328052282333374
Batch 16/64 loss: 0.0740576982498169
Batch 17/64 loss: 0.0344812273979187
Batch 18/64 loss: 0.026867210865020752
Batch 19/64 loss: 0.03376966714859009
Batch 20/64 loss: 0.026234745979309082
Batch 21/64 loss: 0.0754934549331665
Batch 22/64 loss: 0.04044926166534424
Batch 23/64 loss: 0.01798844337463379
Batch 24/64 loss: 0.05006682872772217
Batch 25/64 loss: 0.04894733428955078
Batch 26/64 loss: 0.05770862102508545
Batch 27/64 loss: 0.04064309597015381
Batch 28/64 loss: 0.03221982717514038
Batch 29/64 loss: 0.03265100717544556
Batch 30/64 loss: 0.042627811431884766
Batch 31/64 loss: 0.024549007415771484
Batch 32/64 loss: 0.027088582515716553
Batch 33/64 loss: 0.051707446575164795
Batch 34/64 loss: 0.05062377452850342
Batch 35/64 loss: 0.046293020248413086
Batch 36/64 loss: 0.043271422386169434
Batch 37/64 loss: 0.0598607063293457
Batch 38/64 loss: 0.07845735549926758
Batch 39/64 loss: 0.04323345422744751
Batch 40/64 loss: 0.038277626037597656
Batch 41/64 loss: 0.04736834764480591
Batch 42/64 loss: 0.03188729286193848
Batch 43/64 loss: 0.04422038793563843
Batch 44/64 loss: 0.07339257001876831
Batch 45/64 loss: 0.04047048091888428
Batch 46/64 loss: 0.06435436010360718
Batch 47/64 loss: 0.031063437461853027
Batch 48/64 loss: 0.03542512655258179
Batch 49/64 loss: 0.03351682424545288
Batch 50/64 loss: 0.034365057945251465
Batch 51/64 loss: 0.03423333168029785
Batch 52/64 loss: 0.05447089672088623
Batch 53/64 loss: 0.03157991170883179
Batch 54/64 loss: 0.052479326725006104
Batch 55/64 loss: 0.05328857898712158
Batch 56/64 loss: 0.07493102550506592
Batch 57/64 loss: 0.052843570709228516
Batch 58/64 loss: 0.024982571601867676
Batch 59/64 loss: 0.037883460521698
Batch 60/64 loss: 0.056014180183410645
Batch 61/64 loss: 0.04483133554458618
Batch 62/64 loss: 0.049509406089782715
Batch 63/64 loss: 0.03403198719024658
Batch 64/64 loss: 0.017069995403289795
Epoch 295  Train loss: 0.04367516765407487  Val loss: 0.11757450025925521
Epoch 296
-------------------------------
Batch 1/64 loss: 0.0472637414932251
Batch 2/64 loss: 0.023265600204467773
Batch 3/64 loss: 0.022054076194763184
Batch 4/64 loss: 0.025774121284484863
Batch 5/64 loss: 0.0317082405090332
Batch 6/64 loss: 0.051449358463287354
Batch 7/64 loss: 0.009479224681854248
Batch 8/64 loss: 0.0259019136428833
Batch 9/64 loss: 0.03963768482208252
Batch 10/64 loss: 0.035566747188568115
Batch 11/64 loss: 0.03255528211593628
Batch 12/64 loss: 0.048799753189086914
Batch 13/64 loss: 0.07906138896942139
Batch 14/64 loss: 0.06877022981643677
Batch 15/64 loss: 0.02428138256072998
Batch 16/64 loss: 0.07534587383270264
Batch 17/64 loss: 0.049726247787475586
Batch 18/64 loss: 0.03601282835006714
Batch 19/64 loss: 0.007520139217376709
Batch 20/64 loss: 0.04720550775527954
Batch 21/64 loss: 0.041753172874450684
Batch 22/64 loss: 0.036561429500579834
Batch 23/64 loss: 0.07498723268508911
Batch 24/64 loss: 0.04940885305404663
Batch 25/64 loss: 0.04501032829284668
Batch 26/64 loss: 0.05028641223907471
Batch 27/64 loss: 0.04612839221954346
Batch 28/64 loss: 0.0501706600189209
Batch 29/64 loss: 0.062977135181427
Batch 30/64 loss: 0.023639321327209473
Batch 31/64 loss: 0.08064061403274536
Batch 32/64 loss: 0.02722644805908203
Batch 33/64 loss: 0.050574421882629395
Batch 34/64 loss: 0.04706329107284546
Batch 35/64 loss: 0.03226524591445923
Batch 36/64 loss: 0.06756448745727539
Batch 37/64 loss: 0.04827934503555298
Batch 38/64 loss: 0.09221106767654419
Batch 39/64 loss: 0.04162365198135376
Batch 40/64 loss: 0.04806935787200928
Batch 41/64 loss: 0.04390352964401245
Batch 42/64 loss: 0.04898989200592041
Batch 43/64 loss: 0.026528656482696533
Batch 44/64 loss: 0.05526626110076904
Batch 45/64 loss: 0.04788625240325928
Batch 46/64 loss: 0.040308237075805664
Batch 47/64 loss: 0.03077518939971924
Batch 48/64 loss: 0.049817800521850586
Batch 49/64 loss: 0.030179083347320557
Batch 50/64 loss: 0.024218380451202393
Batch 51/64 loss: 0.034863054752349854
Batch 52/64 loss: 0.03059399127960205
Batch 53/64 loss: 0.04627025127410889
Batch 54/64 loss: 0.06597447395324707
Batch 55/64 loss: 0.04223138093948364
Batch 56/64 loss: 0.038871705532073975
Batch 57/64 loss: 0.039752960205078125
Batch 58/64 loss: 0.04069375991821289
Batch 59/64 loss: 0.037635207176208496
Batch 60/64 loss: 0.040346384048461914
Batch 61/64 loss: 0.04235649108886719
Batch 62/64 loss: 0.05041027069091797
Batch 63/64 loss: 0.03883945941925049
Batch 64/64 loss: -0.008886396884918213
Epoch 296  Train loss: 0.042946950360840445  Val loss: 0.11866444369771637
Epoch 297
-------------------------------
Batch 1/64 loss: 0.05313754081726074
Batch 2/64 loss: 0.0013455748558044434
Batch 3/64 loss: 0.05358165502548218
Batch 4/64 loss: 0.04082852602005005
Batch 5/64 loss: 0.0623781681060791
Batch 6/64 loss: 0.05638694763183594
Batch 7/64 loss: 0.012087702751159668
Batch 8/64 loss: 0.015174269676208496
Batch 9/64 loss: 0.04254710674285889
Batch 10/64 loss: 0.043056368827819824
Batch 11/64 loss: 0.028258860111236572
Batch 12/64 loss: 0.06507033109664917
Batch 13/64 loss: 0.03865170478820801
Batch 14/64 loss: 0.06325936317443848
Batch 15/64 loss: 0.04265862703323364
Batch 16/64 loss: 0.03732645511627197
Batch 17/64 loss: 0.04515206813812256
Batch 18/64 loss: 0.028033316135406494
Batch 19/64 loss: 0.034362077713012695
Batch 20/64 loss: 0.05563235282897949
Batch 21/64 loss: 0.05344498157501221
Batch 22/64 loss: 0.016539275646209717
Batch 23/64 loss: 0.018311917781829834
Batch 24/64 loss: 0.015942037105560303
Batch 25/64 loss: 0.04530143737792969
Batch 26/64 loss: 0.0468066930770874
Batch 27/64 loss: 0.036032021045684814
Batch 28/64 loss: 0.0649404525756836
Batch 29/64 loss: 0.04368305206298828
Batch 30/64 loss: 0.029265880584716797
Batch 31/64 loss: 0.019632160663604736
Batch 32/64 loss: 0.06722629070281982
Batch 33/64 loss: 0.07211887836456299
Batch 34/64 loss: 0.05797851085662842
Batch 35/64 loss: 0.024090290069580078
Batch 36/64 loss: 0.0524975061416626
Batch 37/64 loss: 0.03621727228164673
Batch 38/64 loss: 0.0640866756439209
Batch 39/64 loss: 0.045511603355407715
Batch 40/64 loss: 0.02844250202178955
Batch 41/64 loss: 0.06287616491317749
Batch 42/64 loss: 0.07066899538040161
Batch 43/64 loss: 0.06158411502838135
Batch 44/64 loss: 0.043646275997161865
Batch 45/64 loss: 0.08397823572158813
Batch 46/64 loss: 0.052110135555267334
Batch 47/64 loss: 0.027793049812316895
Batch 48/64 loss: 0.03539329767227173
Batch 49/64 loss: 0.05966925621032715
Batch 50/64 loss: 0.03735220432281494
Batch 51/64 loss: 0.047567665576934814
Batch 52/64 loss: 0.0352327823638916
Batch 53/64 loss: 0.03600752353668213
Batch 54/64 loss: 0.044600605964660645
Batch 55/64 loss: 0.059759676456451416
Batch 56/64 loss: 0.010615766048431396
Batch 57/64 loss: 0.03250694274902344
Batch 58/64 loss: 0.030249714851379395
Batch 59/64 loss: 0.042251527309417725
Batch 60/64 loss: 0.05337262153625488
Batch 61/64 loss: 0.03617149591445923
Batch 62/64 loss: 0.021196186542510986
Batch 63/64 loss: 0.05455625057220459
Batch 64/64 loss: 0.048634350299835205
Epoch 297  Train loss: 0.042864858169181674  Val loss: 0.11960775827624134
Epoch 298
-------------------------------
Batch 1/64 loss: 0.04731106758117676
Batch 2/64 loss: 0.03676122426986694
Batch 3/64 loss: 0.04111766815185547
Batch 4/64 loss: 0.047054827213287354
Batch 5/64 loss: 0.0413205623626709
Batch 6/64 loss: 0.058129191398620605
Batch 7/64 loss: 0.03097987174987793
Batch 8/64 loss: 0.029318928718566895
Batch 9/64 loss: 0.03688156604766846
Batch 10/64 loss: 0.024588167667388916
Batch 11/64 loss: 0.05106395483016968
Batch 12/64 loss: 0.04805469512939453
Batch 13/64 loss: 0.03265649080276489
Batch 14/64 loss: 0.0625259280204773
Batch 15/64 loss: 0.03514420986175537
Batch 16/64 loss: 0.04403364658355713
Batch 17/64 loss: 0.03617650270462036
Batch 18/64 loss: 0.03301876783370972
Batch 19/64 loss: 0.06657814979553223
Batch 20/64 loss: 0.06253182888031006
Batch 21/64 loss: 0.034436702728271484
Batch 22/64 loss: 0.030247509479522705
Batch 23/64 loss: 0.04476898908615112
Batch 24/64 loss: 0.021855294704437256
Batch 25/64 loss: 0.0727226734161377
Batch 26/64 loss: 0.026964426040649414
Batch 27/64 loss: 0.04293358325958252
Batch 28/64 loss: 0.02007150650024414
Batch 29/64 loss: 0.027828633785247803
Batch 30/64 loss: 0.037467360496520996
Batch 31/64 loss: 0.019761085510253906
Batch 32/64 loss: 0.05129355192184448
Batch 33/64 loss: 0.03314948081970215
Batch 34/64 loss: 0.043744683265686035
Batch 35/64 loss: 0.04986894130706787
Batch 36/64 loss: 0.06368881464004517
Batch 37/64 loss: 0.034857988357543945
Batch 38/64 loss: 0.04036116600036621
Batch 39/64 loss: 0.02584552764892578
Batch 40/64 loss: 0.00273740291595459
Batch 41/64 loss: 0.07499510049819946
Batch 42/64 loss: 0.040776968002319336
Batch 43/64 loss: 0.04910457134246826
Batch 44/64 loss: 0.017500221729278564
Batch 45/64 loss: 0.06462699174880981
Batch 46/64 loss: 0.03062736988067627
Batch 47/64 loss: 0.0508272647857666
Batch 48/64 loss: 0.07481276988983154
Batch 49/64 loss: 0.027986645698547363
Batch 50/64 loss: 0.04410678148269653
Batch 51/64 loss: 0.04253411293029785
Batch 52/64 loss: 0.03737431764602661
Batch 53/64 loss: 0.026778697967529297
Batch 54/64 loss: 0.03480297327041626
Batch 55/64 loss: 0.0564417839050293
Batch 56/64 loss: 0.07298725843429565
Batch 57/64 loss: 0.04492521286010742
Batch 58/64 loss: 0.07782495021820068
Batch 59/64 loss: 0.07010990381240845
Batch 60/64 loss: 0.018682420253753662
Batch 61/64 loss: 0.03217756748199463
Batch 62/64 loss: 0.06948697566986084
Batch 63/64 loss: 0.03455913066864014
Batch 64/64 loss: 0.04711049795150757
Epoch 298  Train loss: 0.042654673492207244  Val loss: 0.11709681573192689
Epoch 299
-------------------------------
Batch 1/64 loss: 0.030482888221740723
Batch 2/64 loss: 0.030214905738830566
Batch 3/64 loss: 0.04673278331756592
Batch 4/64 loss: 0.05241680145263672
Batch 5/64 loss: 0.047377169132232666
Batch 6/64 loss: 0.012471973896026611
Batch 7/64 loss: 0.027254581451416016
Batch 8/64 loss: 0.03638702630996704
Batch 9/64 loss: 0.06784188747406006
Batch 10/64 loss: 0.053357601165771484
Batch 11/64 loss: 0.059811532497406006
Batch 12/64 loss: 0.03464818000793457
Batch 13/64 loss: 0.02502286434173584
Batch 14/64 loss: 0.048918843269348145
Batch 15/64 loss: 0.06949520111083984
Batch 16/64 loss: 0.027988433837890625
Batch 17/64 loss: 0.04869335889816284
Batch 18/64 loss: 0.03991037607192993
Batch 19/64 loss: 0.04379993677139282
Batch 20/64 loss: 0.04912376403808594
Batch 21/64 loss: 0.025079786777496338
Batch 22/64 loss: 0.00857996940612793
Batch 23/64 loss: 0.049896299839019775
Batch 24/64 loss: 0.021050691604614258
Batch 25/64 loss: 0.04158657789230347
Batch 26/64 loss: 0.032496869564056396
Batch 27/64 loss: 0.05659651756286621
Batch 28/64 loss: 0.06881111860275269
Batch 29/64 loss: 0.034282028675079346
Batch 30/64 loss: 0.03325408697128296
Batch 31/64 loss: 0.04159730672836304
Batch 32/64 loss: 0.05644094944000244
Batch 33/64 loss: 0.04212146997451782
Batch 34/64 loss: 0.048644304275512695
Batch 35/64 loss: 0.05040609836578369
Batch 36/64 loss: 0.047655701637268066
Batch 37/64 loss: 0.05944383144378662
Batch 38/64 loss: 0.029433608055114746
Batch 39/64 loss: 0.04531937837600708
Batch 40/64 loss: 0.02221667766571045
Batch 41/64 loss: 0.06805908679962158
Batch 42/64 loss: 0.07353818416595459
Batch 43/64 loss: 0.02839183807373047
Batch 44/64 loss: 0.018842697143554688
Batch 45/64 loss: 0.03791242837905884
Batch 46/64 loss: 0.035307347774505615
Batch 47/64 loss: 0.043655455112457275
Batch 48/64 loss: 0.012415707111358643
Batch 49/64 loss: 0.05798518657684326
Batch 50/64 loss: 0.04668682813644409
Batch 51/64 loss: 0.018208086490631104
Batch 52/64 loss: 0.042338550090789795
Batch 53/64 loss: 0.053858935832977295
Batch 54/64 loss: 0.04567301273345947
Batch 55/64 loss: 0.04338473081588745
Batch 56/64 loss: 0.048239707946777344
Batch 57/64 loss: 0.05225259065628052
Batch 58/64 loss: 0.04993939399719238
Batch 59/64 loss: 0.06438946723937988
Batch 60/64 loss: 0.05490928888320923
Batch 61/64 loss: 0.03804296255111694
Batch 62/64 loss: 0.0491446852684021
Batch 63/64 loss: 0.05543971061706543
Batch 64/64 loss: 0.045143842697143555
Epoch 299  Train loss: 0.04296999445148543  Val loss: 0.12228141883804217
Epoch 300
-------------------------------
Batch 1/64 loss: 0.055664896965026855
Batch 2/64 loss: 0.033484816551208496
Batch 3/64 loss: 0.04426807165145874
Batch 4/64 loss: 0.038668811321258545
Batch 5/64 loss: 0.03630620241165161
Batch 6/64 loss: 0.05932551622390747
Batch 7/64 loss: 0.035169899463653564
Batch 8/64 loss: 0.029793977737426758
Batch 9/64 loss: 0.031849443912506104
Batch 10/64 loss: 0.046392738819122314
Batch 11/64 loss: 0.05427461862564087
Batch 12/64 loss: 0.05218386650085449
Batch 13/64 loss: 0.02420663833618164
Batch 14/64 loss: 0.05746549367904663
Batch 15/64 loss: 0.052798569202423096
Batch 16/64 loss: 0.03651082515716553
Batch 17/64 loss: 0.026372075080871582
Batch 18/64 loss: 0.064292311668396
Batch 19/64 loss: 0.058542609214782715
Batch 20/64 loss: 0.027160942554473877
Batch 21/64 loss: 0.022136569023132324
Batch 22/64 loss: 0.022268056869506836
Batch 23/64 loss: 0.03344881534576416
Batch 24/64 loss: 0.07266503572463989
Batch 25/64 loss: 0.056347548961639404
Batch 26/64 loss: 0.08437144756317139
Batch 27/64 loss: 0.038951992988586426
Batch 28/64 loss: 0.04916030168533325
Batch 29/64 loss: 0.026827096939086914
Batch 30/64 loss: 0.05512261390686035
Batch 31/64 loss: 0.06455695629119873
Batch 32/64 loss: 0.05728328227996826
Batch 33/64 loss: 0.04350811243057251
Batch 34/64 loss: 0.04151815176010132
Batch 35/64 loss: 0.04262268543243408
Batch 36/64 loss: 0.0419461727142334
Batch 37/64 loss: 0.032394349575042725
Batch 38/64 loss: 0.02912306785583496
Batch 39/64 loss: 0.008462071418762207
Batch 40/64 loss: 0.05969595909118652
Batch 41/64 loss: 0.04391211271286011
Batch 42/64 loss: 0.040901124477386475
Batch 43/64 loss: 0.04758650064468384
Batch 44/64 loss: 0.0600200891494751
Batch 45/64 loss: 0.010789990425109863
Batch 46/64 loss: 0.02679544687271118
Batch 47/64 loss: 0.03970998525619507
Batch 48/64 loss: 0.051454246044158936
Batch 49/64 loss: 0.06108790636062622
Batch 50/64 loss: 0.07838624715805054
Batch 51/64 loss: 0.0211029052734375
Batch 52/64 loss: 0.02766132354736328
Batch 53/64 loss: 0.07084167003631592
Batch 54/64 loss: 0.029630422592163086
Batch 55/64 loss: 0.026554882526397705
Batch 56/64 loss: 0.05434781312942505
Batch 57/64 loss: 0.03313338756561279
Batch 58/64 loss: 0.02961277961730957
Batch 59/64 loss: 0.026256442070007324
Batch 60/64 loss: 0.03678208589553833
Batch 61/64 loss: 0.03591752052307129
Batch 62/64 loss: 0.05928385257720947
Batch 63/64 loss: 0.023665428161621094
Batch 64/64 loss: 0.01556706428527832
Epoch 300  Train loss: 0.04226277762768315  Val loss: 0.11639896618951227
Epoch 301
-------------------------------
Batch 1/64 loss: 0.03697627782821655
Batch 2/64 loss: 0.011181533336639404
Batch 3/64 loss: 0.04236328601837158
Batch 4/64 loss: 0.020363271236419678
Batch 5/64 loss: 0.03518521785736084
Batch 6/64 loss: 0.05132699012756348
Batch 7/64 loss: 0.0469319224357605
Batch 8/64 loss: 0.04930901527404785
Batch 9/64 loss: 0.06614917516708374
Batch 10/64 loss: 0.05412256717681885
Batch 11/64 loss: 0.06353777647018433
Batch 12/64 loss: 0.03254997730255127
Batch 13/64 loss: 0.06472325325012207
Batch 14/64 loss: 0.023732483386993408
Batch 15/64 loss: 0.03525388240814209
Batch 16/64 loss: 0.031410157680511475
Batch 17/64 loss: 0.020302236080169678
Batch 18/64 loss: 0.03959786891937256
Batch 19/64 loss: 0.046460747718811035
Batch 20/64 loss: 0.051895081996917725
Batch 21/64 loss: 0.046663880348205566
Batch 22/64 loss: 0.027524828910827637
Batch 23/64 loss: 0.04186832904815674
Batch 24/64 loss: 0.05355703830718994
Batch 25/64 loss: 0.03460294008255005
Batch 26/64 loss: 0.0271567702293396
Batch 27/64 loss: 0.06226927042007446
Batch 28/64 loss: 0.02496659755706787
Batch 29/64 loss: 0.0338057279586792
Batch 30/64 loss: 0.01042717695236206
Batch 31/64 loss: 0.02242177724838257
Batch 32/64 loss: 0.03759932518005371
Batch 33/64 loss: 0.029873132705688477
Batch 34/64 loss: 0.05589795112609863
Batch 35/64 loss: 0.034728169441223145
Batch 36/64 loss: 0.03209066390991211
Batch 37/64 loss: 0.03388381004333496
Batch 38/64 loss: 0.03162336349487305
Batch 39/64 loss: 0.03195750713348389
Batch 40/64 loss: 0.04121166467666626
Batch 41/64 loss: 0.028860926628112793
Batch 42/64 loss: 0.029730558395385742
Batch 43/64 loss: 0.05336332321166992
Batch 44/64 loss: 0.039023518562316895
Batch 45/64 loss: 0.0648077130317688
Batch 46/64 loss: 0.06628251075744629
Batch 47/64 loss: 0.07370108366012573
Batch 48/64 loss: 0.05553716421127319
Batch 49/64 loss: 0.0031474828720092773
Batch 50/64 loss: 0.046546876430511475
Batch 51/64 loss: 0.03509330749511719
Batch 52/64 loss: 0.05223208665847778
Batch 53/64 loss: 0.03719121217727661
Batch 54/64 loss: 0.04398280382156372
Batch 55/64 loss: 0.04092627763748169
Batch 56/64 loss: 0.04956376552581787
Batch 57/64 loss: 0.04345482587814331
Batch 58/64 loss: 0.03319263458251953
Batch 59/64 loss: 0.06502377986907959
Batch 60/64 loss: 0.0545770525932312
Batch 61/64 loss: 0.030134975910186768
Batch 62/64 loss: 0.06234288215637207
Batch 63/64 loss: 0.056743085384368896
Batch 64/64 loss: 0.09177124500274658
Epoch 301  Train loss: 0.04191044592389873  Val loss: 0.11850211239352669
Epoch 302
-------------------------------
Batch 1/64 loss: 0.04978668689727783
Batch 2/64 loss: 0.05543255805969238
Batch 3/64 loss: 0.03384679555892944
Batch 4/64 loss: 0.02426832914352417
Batch 5/64 loss: 0.03876388072967529
Batch 6/64 loss: 0.03209930658340454
Batch 7/64 loss: 0.02442699670791626
Batch 8/64 loss: 0.024747848510742188
Batch 9/64 loss: 0.06037646532058716
Batch 10/64 loss: 0.050514042377471924
Batch 11/64 loss: 0.018272876739501953
Batch 12/64 loss: 0.023896872997283936
Batch 13/64 loss: 0.03798896074295044
Batch 14/64 loss: 0.07023090124130249
Batch 15/64 loss: 0.04078179597854614
Batch 16/64 loss: 0.05120289325714111
Batch 17/64 loss: 0.02326381206512451
Batch 18/64 loss: 0.0486907958984375
Batch 19/64 loss: 0.03350090980529785
Batch 20/64 loss: 0.02594923973083496
Batch 21/64 loss: 0.022652506828308105
Batch 22/64 loss: 0.034956514835357666
Batch 23/64 loss: 0.041866183280944824
Batch 24/64 loss: 0.08365476131439209
Batch 25/64 loss: 0.030615031719207764
Batch 26/64 loss: 0.036009907722473145
Batch 27/64 loss: 0.00995779037475586
Batch 28/64 loss: 0.02609044313430786
Batch 29/64 loss: 0.05959421396255493
Batch 30/64 loss: 0.05175274610519409
Batch 31/64 loss: 0.017789602279663086
Batch 32/64 loss: 0.030635297298431396
Batch 33/64 loss: 0.01976799964904785
Batch 34/64 loss: 0.02725774049758911
Batch 35/64 loss: 0.06531578302383423
Batch 36/64 loss: 0.04647707939147949
Batch 37/64 loss: 0.038445472717285156
Batch 38/64 loss: 0.07109051942825317
Batch 39/64 loss: 0.04042130708694458
Batch 40/64 loss: 0.04378533363342285
Batch 41/64 loss: 0.03666955232620239
Batch 42/64 loss: 0.026431262493133545
Batch 43/64 loss: 0.018822848796844482
Batch 44/64 loss: 0.07166212797164917
Batch 45/64 loss: 0.034747421741485596
Batch 46/64 loss: 0.030784428119659424
Batch 47/64 loss: 0.04886341094970703
Batch 48/64 loss: 0.019384324550628662
Batch 49/64 loss: 0.028477609157562256
Batch 50/64 loss: 0.05232584476470947
Batch 51/64 loss: 0.05108004808425903
Batch 52/64 loss: 0.04416239261627197
Batch 53/64 loss: 0.08597540855407715
Batch 54/64 loss: 0.0639265775680542
Batch 55/64 loss: 0.08350372314453125
Batch 56/64 loss: 0.04858589172363281
Batch 57/64 loss: 0.034778475761413574
Batch 58/64 loss: 0.035101115703582764
Batch 59/64 loss: 0.05513465404510498
Batch 60/64 loss: 0.06984305381774902
Batch 61/64 loss: 0.04451030492782593
Batch 62/64 loss: 0.03556329011917114
Batch 63/64 loss: 0.040406107902526855
Batch 64/64 loss: 0.04162108898162842
Epoch 302  Train loss: 0.04169621794831519  Val loss: 0.12107302765666005
Epoch 303
-------------------------------
Batch 1/64 loss: 0.03869485855102539
Batch 2/64 loss: 0.06439143419265747
Batch 3/64 loss: 0.012966156005859375
Batch 4/64 loss: 0.011754274368286133
Batch 5/64 loss: 0.015203475952148438
Batch 6/64 loss: 0.05189234018325806
Batch 7/64 loss: 0.03046417236328125
Batch 8/64 loss: 0.037999868392944336
Batch 9/64 loss: 0.03902822732925415
Batch 10/64 loss: 0.011725962162017822
Batch 11/64 loss: 0.05229461193084717
Batch 12/64 loss: 0.05058664083480835
Batch 13/64 loss: 0.026801347732543945
Batch 14/64 loss: 0.06254088878631592
Batch 15/64 loss: 0.026857614517211914
Batch 16/64 loss: 0.03893214464187622
Batch 17/64 loss: 0.07009637355804443
Batch 18/64 loss: 0.03220456838607788
Batch 19/64 loss: 0.05914139747619629
Batch 20/64 loss: 0.04078054428100586
Batch 21/64 loss: 0.05385744571685791
Batch 22/64 loss: 0.034046828746795654
Batch 23/64 loss: 0.05474656820297241
Batch 24/64 loss: 0.011183440685272217
Batch 25/64 loss: 0.03847312927246094
Batch 26/64 loss: 0.032963693141937256
Batch 27/64 loss: 0.07135885953903198
Batch 28/64 loss: 0.034296631813049316
Batch 29/64 loss: 0.028428733348846436
Batch 30/64 loss: 0.04864996671676636
Batch 31/64 loss: 0.028595149517059326
Batch 32/64 loss: 0.04437458515167236
Batch 33/64 loss: 0.0526847243309021
Batch 34/64 loss: 0.06794619560241699
Batch 35/64 loss: 0.02014213800430298
Batch 36/64 loss: 0.031437039375305176
Batch 37/64 loss: 0.045057475566864014
Batch 38/64 loss: 0.05859792232513428
Batch 39/64 loss: 0.04254668951034546
Batch 40/64 loss: 0.08313411474227905
Batch 41/64 loss: 0.057105958461761475
Batch 42/64 loss: 0.03418111801147461
Batch 43/64 loss: 0.0539279580116272
Batch 44/64 loss: 0.04249775409698486
Batch 45/64 loss: 0.06359773874282837
Batch 46/64 loss: 0.04804408550262451
Batch 47/64 loss: 0.05576169490814209
Batch 48/64 loss: 0.04216158390045166
Batch 49/64 loss: 0.022192537784576416
Batch 50/64 loss: 0.031492650508880615
Batch 51/64 loss: 0.05081295967102051
Batch 52/64 loss: 0.04353153705596924
Batch 53/64 loss: 0.03510993719100952
Batch 54/64 loss: 0.0283966064453125
Batch 55/64 loss: 0.05303049087524414
Batch 56/64 loss: 0.03478085994720459
Batch 57/64 loss: 0.05746334791183472
Batch 58/64 loss: 0.04808229207992554
Batch 59/64 loss: 0.024670124053955078
Batch 60/64 loss: 0.03169727325439453
Batch 61/64 loss: 0.05616295337677002
Batch 62/64 loss: 0.012194275856018066
Batch 63/64 loss: 0.03977692127227783
Batch 64/64 loss: 0.060822367668151855
Epoch 303  Train loss: 0.04186929674709544  Val loss: 0.11828495608162634
Epoch 304
-------------------------------
Batch 1/64 loss: 0.031077921390533447
Batch 2/64 loss: 0.03850889205932617
Batch 3/64 loss: 0.023857593536376953
Batch 4/64 loss: 0.06134134531021118
Batch 5/64 loss: 0.03507298231124878
Batch 6/64 loss: 0.006804227828979492
Batch 7/64 loss: 0.04089498519897461
Batch 8/64 loss: 0.05868673324584961
Batch 9/64 loss: 0.008630037307739258
Batch 10/64 loss: 0.025671064853668213
Batch 11/64 loss: 0.024654388427734375
Batch 12/64 loss: 0.05448007583618164
Batch 13/64 loss: 0.02810072898864746
Batch 14/64 loss: 0.03435540199279785
Batch 15/64 loss: 0.0537380576133728
Batch 16/64 loss: 0.056182920932769775
Batch 17/64 loss: 0.027811527252197266
Batch 18/64 loss: 0.02279442548751831
Batch 19/64 loss: 0.06889593601226807
Batch 20/64 loss: 0.023884475231170654
Batch 21/64 loss: 0.06063687801361084
Batch 22/64 loss: 0.04963719844818115
Batch 23/64 loss: 0.04246330261230469
Batch 24/64 loss: 0.04528146982192993
Batch 25/64 loss: 0.05306178331375122
Batch 26/64 loss: 0.038279831409454346
Batch 27/64 loss: 0.034371018409729004
Batch 28/64 loss: 0.034151315689086914
Batch 29/64 loss: 0.05113786458969116
Batch 30/64 loss: 0.034361958503723145
Batch 31/64 loss: 0.042934298515319824
Batch 32/64 loss: 0.024664759635925293
Batch 33/64 loss: 0.03980046510696411
Batch 34/64 loss: 0.08479917049407959
Batch 35/64 loss: 0.05632418394088745
Batch 36/64 loss: 0.06585037708282471
Batch 37/64 loss: 0.026135385036468506
Batch 38/64 loss: 0.01742309331893921
Batch 39/64 loss: 0.042104482650756836
Batch 40/64 loss: 0.022420287132263184
Batch 41/64 loss: 0.03979581594467163
Batch 42/64 loss: 0.05587071180343628
Batch 43/64 loss: 0.07368630170822144
Batch 44/64 loss: 0.022018790245056152
Batch 45/64 loss: 0.00921022891998291
Batch 46/64 loss: 0.048755526542663574
Batch 47/64 loss: 0.042543888092041016
Batch 48/64 loss: 0.025084733963012695
Batch 49/64 loss: 0.02571737766265869
Batch 50/64 loss: 0.06510430574417114
Batch 51/64 loss: 0.06164592504501343
Batch 52/64 loss: 0.038349449634552
Batch 53/64 loss: 0.04539179801940918
Batch 54/64 loss: 0.04832941293716431
Batch 55/64 loss: 0.024266302585601807
Batch 56/64 loss: 0.05568826198577881
Batch 57/64 loss: 0.025656402111053467
Batch 58/64 loss: 0.04397660493850708
Batch 59/64 loss: 0.011748671531677246
Batch 60/64 loss: 0.04282093048095703
Batch 61/64 loss: 0.03214043378829956
Batch 62/64 loss: 0.05218863487243652
Batch 63/64 loss: 0.03719973564147949
Batch 64/64 loss: 0.046614646911621094
Epoch 304  Train loss: 0.0400538678262748  Val loss: 0.11893162039137378
Epoch 305
-------------------------------
Batch 1/64 loss: 0.008175790309906006
Batch 2/64 loss: 0.02703559398651123
Batch 3/64 loss: 0.03693890571594238
Batch 4/64 loss: 0.02810758352279663
Batch 5/64 loss: 0.029840469360351562
Batch 6/64 loss: 0.022589385509490967
Batch 7/64 loss: 0.03045475482940674
Batch 8/64 loss: 0.05757904052734375
Batch 9/64 loss: 0.0029325485229492188
Batch 10/64 loss: 0.03972792625427246
Batch 11/64 loss: 0.042984724044799805
Batch 12/64 loss: 0.05052506923675537
Batch 13/64 loss: 0.05455750226974487
Batch 14/64 loss: 0.0145035982131958
Batch 15/64 loss: 0.01862889528274536
Batch 16/64 loss: 0.022843360900878906
Batch 17/64 loss: 0.05914062261581421
Batch 18/64 loss: 0.013857483863830566
Batch 19/64 loss: 0.0262107253074646
Batch 20/64 loss: 0.04980158805847168
Batch 21/64 loss: 0.06091344356536865
Batch 22/64 loss: 0.07358592748641968
Batch 23/64 loss: 0.04388707876205444
Batch 24/64 loss: 0.03182196617126465
Batch 25/64 loss: 0.05576741695404053
Batch 26/64 loss: 0.043538808822631836
Batch 27/64 loss: 0.028598368167877197
Batch 28/64 loss: 0.03151106834411621
Batch 29/64 loss: 0.03255552053451538
Batch 30/64 loss: 0.037576913833618164
Batch 31/64 loss: 0.04436171054840088
Batch 32/64 loss: 0.043228864669799805
Batch 33/64 loss: 0.07327514886856079
Batch 34/64 loss: 0.021992266178131104
Batch 35/64 loss: 0.03439134359359741
Batch 36/64 loss: 0.015326440334320068
Batch 37/64 loss: 0.03336012363433838
Batch 38/64 loss: 0.06766021251678467
Batch 39/64 loss: 0.01912611722946167
Batch 40/64 loss: 0.058606624603271484
Batch 41/64 loss: 0.049121856689453125
Batch 42/64 loss: 0.04451894760131836
Batch 43/64 loss: 0.046407341957092285
Batch 44/64 loss: 0.04645448923110962
Batch 45/64 loss: 0.03998613357543945
Batch 46/64 loss: 0.05395704507827759
Batch 47/64 loss: 0.047648489475250244
Batch 48/64 loss: 0.01839524507522583
Batch 49/64 loss: 0.02348041534423828
Batch 50/64 loss: 0.040725767612457275
Batch 51/64 loss: 0.025645732879638672
Batch 52/64 loss: 0.04139906167984009
Batch 53/64 loss: 0.024979233741760254
Batch 54/64 loss: 0.05938112735748291
Batch 55/64 loss: 0.031726837158203125
Batch 56/64 loss: 0.03952932357788086
Batch 57/64 loss: 0.07823121547698975
Batch 58/64 loss: 0.031645774841308594
Batch 59/64 loss: 0.06211334466934204
Batch 60/64 loss: 0.08483541011810303
Batch 61/64 loss: 0.047819674015045166
Batch 62/64 loss: 0.04934239387512207
Batch 63/64 loss: 0.06741851568222046
Batch 64/64 loss: 0.05013805627822876
Epoch 305  Train loss: 0.040468829052121034  Val loss: 0.11947717719880986
Epoch 306
-------------------------------
Batch 1/64 loss: 0.04983627796173096
Batch 2/64 loss: 0.022875666618347168
Batch 3/64 loss: 0.04250150918960571
Batch 4/64 loss: 0.046286165714263916
Batch 5/64 loss: 0.05350750684738159
Batch 6/64 loss: 0.06179255247116089
Batch 7/64 loss: 0.02254730463027954
Batch 8/64 loss: 0.06075471639633179
Batch 9/64 loss: 0.04655939340591431
Batch 10/64 loss: 0.048842012882232666
Batch 11/64 loss: 0.04787492752075195
Batch 12/64 loss: 0.04026985168457031
Batch 13/64 loss: 0.04221940040588379
Batch 14/64 loss: 0.05922245979309082
Batch 15/64 loss: 0.03305560350418091
Batch 16/64 loss: 0.059737443923950195
Batch 17/64 loss: 0.018070638179779053
Batch 18/64 loss: 0.06279361248016357
Batch 19/64 loss: 0.009986817836761475
Batch 20/64 loss: 0.03233879804611206
Batch 21/64 loss: 0.04699057340621948
Batch 22/64 loss: 0.039891064167022705
Batch 23/64 loss: 0.05592703819274902
Batch 24/64 loss: 0.06552302837371826
Batch 25/64 loss: 0.029063940048217773
Batch 26/64 loss: 0.03097468614578247
Batch 27/64 loss: 0.05552518367767334
Batch 28/64 loss: 0.06254374980926514
Batch 29/64 loss: 0.04820704460144043
Batch 30/64 loss: 0.05805760622024536
Batch 31/64 loss: 0.03768885135650635
Batch 32/64 loss: 0.024084806442260742
Batch 33/64 loss: 0.02435624599456787
Batch 34/64 loss: 0.03210484981536865
Batch 35/64 loss: 0.041687846183776855
Batch 36/64 loss: 0.08083230257034302
Batch 37/64 loss: 0.05190831422805786
Batch 38/64 loss: 0.03273975849151611
Batch 39/64 loss: 0.0508342981338501
Batch 40/64 loss: 0.056276679039001465
Batch 41/64 loss: 0.027954280376434326
Batch 42/64 loss: 0.0333099365234375
Batch 43/64 loss: 0.03631633520126343
Batch 44/64 loss: 0.029585719108581543
Batch 45/64 loss: 0.061486661434173584
Batch 46/64 loss: 0.03826183080673218
Batch 47/64 loss: 0.050866007804870605
Batch 48/64 loss: 0.02870810031890869
Batch 49/64 loss: 0.038320064544677734
Batch 50/64 loss: 0.05755007266998291
Batch 51/64 loss: 0.028293728828430176
Batch 52/64 loss: 0.03859227895736694
Batch 53/64 loss: 0.03374886512756348
Batch 54/64 loss: 0.037947893142700195
Batch 55/64 loss: 0.04693949222564697
Batch 56/64 loss: 0.01852947473526001
Batch 57/64 loss: 0.032453298568725586
Batch 58/64 loss: 0.03246283531188965
Batch 59/64 loss: 0.007367253303527832
Batch 60/64 loss: 0.06411916017532349
Batch 61/64 loss: 0.04539179801940918
Batch 62/64 loss: 0.021040678024291992
Batch 63/64 loss: 0.036893248558044434
Batch 64/64 loss: 0.05098426342010498
Epoch 306  Train loss: 0.04189282725839054  Val loss: 0.11878153671513718
Epoch 307
-------------------------------
Batch 1/64 loss: 0.03281140327453613
Batch 2/64 loss: 0.02108132839202881
Batch 3/64 loss: 0.04789245128631592
Batch 4/64 loss: 0.06325846910476685
Batch 5/64 loss: 0.03677642345428467
Batch 6/64 loss: 0.05459791421890259
Batch 7/64 loss: 0.03456556797027588
Batch 8/64 loss: 0.030578970909118652
Batch 9/64 loss: 0.04150956869125366
Batch 10/64 loss: 0.01561129093170166
Batch 11/64 loss: 0.04789721965789795
Batch 12/64 loss: 0.042816758155822754
Batch 13/64 loss: 0.04271841049194336
Batch 14/64 loss: 0.03669166564941406
Batch 15/64 loss: 0.013333737850189209
Batch 16/64 loss: 0.07458192110061646
Batch 17/64 loss: 0.03875249624252319
Batch 18/64 loss: 0.022982299327850342
Batch 19/64 loss: 0.033180177211761475
Batch 20/64 loss: 0.031035184860229492
Batch 21/64 loss: 0.051127612590789795
Batch 22/64 loss: 0.031008601188659668
Batch 23/64 loss: 0.053489089012145996
Batch 24/64 loss: 0.037714242935180664
Batch 25/64 loss: 0.027391672134399414
Batch 26/64 loss: 0.04756665229797363
Batch 27/64 loss: 0.01542741060256958
Batch 28/64 loss: 0.03116631507873535
Batch 29/64 loss: 0.05013161897659302
Batch 30/64 loss: 0.07738608121871948
Batch 31/64 loss: 0.031219840049743652
Batch 32/64 loss: 0.05004018545150757
Batch 33/64 loss: 0.054184019565582275
Batch 34/64 loss: 0.017715811729431152
Batch 35/64 loss: 0.0613822340965271
Batch 36/64 loss: 0.046148598194122314
Batch 37/64 loss: 0.02475285530090332
Batch 38/64 loss: 0.05211460590362549
Batch 39/64 loss: 0.0671578049659729
Batch 40/64 loss: 0.030850768089294434
Batch 41/64 loss: 0.010609209537506104
Batch 42/64 loss: 0.026372075080871582
Batch 43/64 loss: 0.010013580322265625
Batch 44/64 loss: 0.05901515483856201
Batch 45/64 loss: 0.0429307222366333
Batch 46/64 loss: 0.04236197471618652
Batch 47/64 loss: 0.05032026767730713
Batch 48/64 loss: 0.04070627689361572
Batch 49/64 loss: 0.031239807605743408
Batch 50/64 loss: 0.045555055141448975
Batch 51/64 loss: 0.029606938362121582
Batch 52/64 loss: 0.02865588665008545
Batch 53/64 loss: 0.08449220657348633
Batch 54/64 loss: 0.030750274658203125
Batch 55/64 loss: 0.040561139583587646
Batch 56/64 loss: 0.05119144916534424
Batch 57/64 loss: 0.05220508575439453
Batch 58/64 loss: 0.03903067111968994
Batch 59/64 loss: 0.03432220220565796
Batch 60/64 loss: 0.03555333614349365
Batch 61/64 loss: 0.047929465770721436
Batch 62/64 loss: 0.029420018196105957
Batch 63/64 loss: 0.015950024127960205
Batch 64/64 loss: 0.06476414203643799
Epoch 307  Train loss: 0.03996886599297617  Val loss: 0.11790919385824826
Epoch 308
-------------------------------
Batch 1/64 loss: 0.0255124568939209
Batch 2/64 loss: 0.03406631946563721
Batch 3/64 loss: 0.04317808151245117
Batch 4/64 loss: 0.04336535930633545
Batch 5/64 loss: 0.030485570430755615
Batch 6/64 loss: 0.049500107765197754
Batch 7/64 loss: 0.025739729404449463
Batch 8/64 loss: 0.05253136157989502
Batch 9/64 loss: 0.053092002868652344
Batch 10/64 loss: 0.027957379817962646
Batch 11/64 loss: 0.02288740873336792
Batch 12/64 loss: 0.03378021717071533
Batch 13/64 loss: 0.046541571617126465
Batch 14/64 loss: 0.025818943977355957
Batch 15/64 loss: 0.033257365226745605
Batch 16/64 loss: 0.028532862663269043
Batch 17/64 loss: 0.03750026226043701
Batch 18/64 loss: 0.026798367500305176
Batch 19/64 loss: 0.05594664812088013
Batch 20/64 loss: 0.02712339162826538
Batch 21/64 loss: 0.026397287845611572
Batch 22/64 loss: 0.03279221057891846
Batch 23/64 loss: 0.015025615692138672
Batch 24/64 loss: 0.027216553688049316
Batch 25/64 loss: 0.05342072248458862
Batch 26/64 loss: 0.020594775676727295
Batch 27/64 loss: 0.035133957862854004
Batch 28/64 loss: 0.039077937602996826
Batch 29/64 loss: 0.06675899028778076
Batch 30/64 loss: 0.057220637798309326
Batch 31/64 loss: 0.05301123857498169
Batch 32/64 loss: 0.05237281322479248
Batch 33/64 loss: 0.0208742618560791
Batch 34/64 loss: 0.043437302112579346
Batch 35/64 loss: 0.03657418489456177
Batch 36/64 loss: 0.048615336418151855
Batch 37/64 loss: 0.041846275329589844
Batch 38/64 loss: 0.03000056743621826
Batch 39/64 loss: 0.05145525932312012
Batch 40/64 loss: 0.034723639488220215
Batch 41/64 loss: 0.04356139898300171
Batch 42/64 loss: 0.03467535972595215
Batch 43/64 loss: 0.05595576763153076
Batch 44/64 loss: 0.019481956958770752
Batch 45/64 loss: 0.024290382862091064
Batch 46/64 loss: 0.04960370063781738
Batch 47/64 loss: 0.03753495216369629
Batch 48/64 loss: 0.05459350347518921
Batch 49/64 loss: 0.056907474994659424
Batch 50/64 loss: 0.05923658609390259
Batch 51/64 loss: 0.06885790824890137
Batch 52/64 loss: 0.0689774751663208
Batch 53/64 loss: 0.04347527027130127
Batch 54/64 loss: 0.03851115703582764
Batch 55/64 loss: 0.07274097204208374
Batch 56/64 loss: 0.046537816524505615
Batch 57/64 loss: 0.05323076248168945
Batch 58/64 loss: 0.049893856048583984
Batch 59/64 loss: 0.02571415901184082
Batch 60/64 loss: 0.06066858768463135
Batch 61/64 loss: 0.030398666858673096
Batch 62/64 loss: 0.029040217399597168
Batch 63/64 loss: 0.036828041076660156
Batch 64/64 loss: 0.07188385725021362
Epoch 308  Train loss: 0.0411732367440766  Val loss: 0.118854352698703
Epoch 309
-------------------------------
Batch 1/64 loss: 0.04135704040527344
Batch 2/64 loss: 0.022902727127075195
Batch 3/64 loss: 0.040711402893066406
Batch 4/64 loss: 0.06535887718200684
Batch 5/64 loss: 0.07526493072509766
Batch 6/64 loss: 0.04408860206604004
Batch 7/64 loss: 0.0882146954536438
Batch 8/64 loss: 0.04976385831832886
Batch 9/64 loss: 0.022530436515808105
Batch 10/64 loss: 0.05542337894439697
Batch 11/64 loss: 0.06460195779800415
Batch 12/64 loss: 0.04175257682800293
Batch 13/64 loss: 0.03848212957382202
Batch 14/64 loss: 0.04464024305343628
Batch 15/64 loss: 0.05377620458602905
Batch 16/64 loss: 0.03079843521118164
Batch 17/64 loss: 0.01841980218887329
Batch 18/64 loss: 0.026304244995117188
Batch 19/64 loss: 0.017782866954803467
Batch 20/64 loss: 0.04288429021835327
Batch 21/64 loss: 0.03710055351257324
Batch 22/64 loss: 0.05604052543640137
Batch 23/64 loss: 0.0262182354927063
Batch 24/64 loss: 0.04934579133987427
Batch 25/64 loss: 0.05324244499206543
Batch 26/64 loss: 0.048064231872558594
Batch 27/64 loss: 0.02487999200820923
Batch 28/64 loss: 0.05055844783782959
Batch 29/64 loss: 0.03259235620498657
Batch 30/64 loss: 0.048667192459106445
Batch 31/64 loss: 0.05088287591934204
Batch 32/64 loss: 0.04009556770324707
Batch 33/64 loss: 0.058747172355651855
Batch 34/64 loss: 0.03829532861709595
Batch 35/64 loss: 0.02393251657485962
Batch 36/64 loss: 0.06185799837112427
Batch 37/64 loss: 0.03908812999725342
Batch 38/64 loss: 0.0534626841545105
Batch 39/64 loss: 0.029043912887573242
Batch 40/64 loss: 0.054921865463256836
Batch 41/64 loss: 0.05244636535644531
Batch 42/64 loss: 0.04682779312133789
Batch 43/64 loss: 0.03174436092376709
Batch 44/64 loss: 0.0240364670753479
Batch 45/64 loss: 0.04853564500808716
Batch 46/64 loss: 0.015404045581817627
Batch 47/64 loss: 0.02198314666748047
Batch 48/64 loss: 0.037406742572784424
Batch 49/64 loss: 0.03859943151473999
Batch 50/64 loss: 0.02990955114364624
Batch 51/64 loss: 0.034651219844818115
Batch 52/64 loss: 0.020983397960662842
Batch 53/64 loss: 0.03007107973098755
Batch 54/64 loss: 0.06066232919692993
Batch 55/64 loss: 0.027261435985565186
Batch 56/64 loss: 0.04267477989196777
Batch 57/64 loss: 0.05197173357009888
Batch 58/64 loss: 0.04391813278198242
Batch 59/64 loss: 0.04792863130569458
Batch 60/64 loss: 0.05028420686721802
Batch 61/64 loss: 0.03422027826309204
Batch 62/64 loss: 0.04505264759063721
Batch 63/64 loss: 0.03286004066467285
Batch 64/64 loss: 0.024335384368896484
Epoch 309  Train loss: 0.041565200394275144  Val loss: 0.11789465769869355
Epoch 310
-------------------------------
Batch 1/64 loss: 0.021753549575805664
Batch 2/64 loss: 0.04313230514526367
Batch 3/64 loss: 0.020739376544952393
Batch 4/64 loss: 0.07914578914642334
Batch 5/64 loss: 0.003046393394470215
Batch 6/64 loss: 0.0669863224029541
Batch 7/64 loss: 0.04546356201171875
Batch 8/64 loss: 0.0669369101524353
Batch 9/64 loss: 0.03420448303222656
Batch 10/64 loss: 0.023049116134643555
Batch 11/64 loss: 0.056224048137664795
Batch 12/64 loss: 0.032213807106018066
Batch 13/64 loss: 0.06367385387420654
Batch 14/64 loss: 0.06780582666397095
Batch 15/64 loss: 0.032694339752197266
Batch 16/64 loss: 0.01893603801727295
Batch 17/64 loss: 0.04022955894470215
Batch 18/64 loss: 0.040022194385528564
Batch 19/64 loss: 0.04032421112060547
Batch 20/64 loss: 0.06993842124938965
Batch 21/64 loss: 0.04269510507583618
Batch 22/64 loss: 0.02926933765411377
Batch 23/64 loss: 0.02441108226776123
Batch 24/64 loss: 0.04335367679595947
Batch 25/64 loss: 0.033104538917541504
Batch 26/64 loss: 0.04268693923950195
Batch 27/64 loss: 0.035846590995788574
Batch 28/64 loss: 0.019289731979370117
Batch 29/64 loss: 0.048166751861572266
Batch 30/64 loss: 0.03141200542449951
Batch 31/64 loss: 0.03402048349380493
Batch 32/64 loss: 0.03396785259246826
Batch 33/64 loss: 0.04856514930725098
Batch 34/64 loss: 0.019749581813812256
Batch 35/64 loss: 0.020985543727874756
Batch 36/64 loss: 0.0466008186340332
Batch 37/64 loss: 0.0494389533996582
Batch 38/64 loss: 0.02045994997024536
Batch 39/64 loss: 0.02769458293914795
Batch 40/64 loss: 0.057483792304992676
Batch 41/64 loss: 0.06276679039001465
Batch 42/64 loss: 0.06676965951919556
Batch 43/64 loss: 0.033926963806152344
Batch 44/64 loss: 0.005708575248718262
Batch 45/64 loss: 0.034176647663116455
Batch 46/64 loss: 0.03375208377838135
Batch 47/64 loss: 0.0221441388130188
Batch 48/64 loss: 0.040510356426239014
Batch 49/64 loss: 0.04497480392456055
Batch 50/64 loss: 0.028955399990081787
Batch 51/64 loss: 0.016096949577331543
Batch 52/64 loss: 0.06623673439025879
Batch 53/64 loss: 0.062280893325805664
Batch 54/64 loss: 0.04548978805541992
Batch 55/64 loss: 0.04257655143737793
Batch 56/64 loss: 0.0782918930053711
Batch 57/64 loss: 0.039016544818878174
Batch 58/64 loss: 0.06792324781417847
Batch 59/64 loss: 0.02771627902984619
Batch 60/64 loss: 0.02590322494506836
Batch 61/64 loss: 0.03879880905151367
Batch 62/64 loss: 0.03251463174819946
Batch 63/64 loss: 0.048772215843200684
Batch 64/64 loss: 0.044579505920410156
Epoch 310  Train loss: 0.04038369234870462  Val loss: 0.12431787358936165
Epoch 311
-------------------------------
Batch 1/64 loss: 0.030520319938659668
Batch 2/64 loss: 0.03558582067489624
Batch 3/64 loss: 0.06044352054595947
Batch 4/64 loss: 0.018491268157958984
Batch 5/64 loss: 0.03313261270523071
Batch 6/64 loss: 0.02625131607055664
Batch 7/64 loss: 0.020982563495635986
Batch 8/64 loss: 0.019086122512817383
Batch 9/64 loss: 0.04954653978347778
Batch 10/64 loss: 0.028733789920806885
Batch 11/64 loss: 0.0539395809173584
Batch 12/64 loss: 0.0216483473777771
Batch 13/64 loss: 0.04063093662261963
Batch 14/64 loss: 0.0331341028213501
Batch 15/64 loss: 0.03612923622131348
Batch 16/64 loss: 0.027396738529205322
Batch 17/64 loss: 0.053779661655426025
Batch 18/64 loss: 0.03351670503616333
Batch 19/64 loss: 0.036434173583984375
Batch 20/64 loss: 0.00904393196105957
Batch 21/64 loss: 0.043787479400634766
Batch 22/64 loss: 0.029342293739318848
Batch 23/64 loss: 0.04506492614746094
Batch 24/64 loss: 0.0605694055557251
Batch 25/64 loss: 0.03601187467575073
Batch 26/64 loss: 0.056904494762420654
Batch 27/64 loss: 0.03128528594970703
Batch 28/64 loss: 0.04970031976699829
Batch 29/64 loss: 0.05301135778427124
Batch 30/64 loss: 0.05117756128311157
Batch 31/64 loss: 0.03165179491043091
Batch 32/64 loss: 0.0632333755493164
Batch 33/64 loss: 0.04528886079788208
Batch 34/64 loss: 0.051542043685913086
Batch 35/64 loss: 0.03577309846878052
Batch 36/64 loss: 0.0468449592590332
Batch 37/64 loss: 0.06978726387023926
Batch 38/64 loss: 0.039399802684783936
Batch 39/64 loss: 0.04389244318008423
Batch 40/64 loss: 0.06733846664428711
Batch 41/64 loss: 0.035902202129364014
Batch 42/64 loss: 0.01455622911453247
Batch 43/64 loss: 0.07621723413467407
Batch 44/64 loss: 0.050242602825164795
Batch 45/64 loss: 0.07088667154312134
Batch 46/64 loss: 0.019502341747283936
Batch 47/64 loss: 0.025606751441955566
Batch 48/64 loss: 0.0321316123008728
Batch 49/64 loss: 0.025136470794677734
Batch 50/64 loss: 0.02990776300430298
Batch 51/64 loss: 0.04736816883087158
Batch 52/64 loss: 0.05483436584472656
Batch 53/64 loss: 0.031038641929626465
Batch 54/64 loss: 0.04736614227294922
Batch 55/64 loss: 0.04443269968032837
Batch 56/64 loss: 0.04120451211929321
Batch 57/64 loss: 0.026561737060546875
Batch 58/64 loss: 0.06067538261413574
Batch 59/64 loss: 0.05613565444946289
Batch 60/64 loss: 0.045235276222229004
Batch 61/64 loss: 0.057296574115753174
Batch 62/64 loss: 0.042014479637145996
Batch 63/64 loss: 0.04838550090789795
Batch 64/64 loss: 0.04249042272567749
Epoch 311  Train loss: 0.04132613691629148  Val loss: 0.11969632545287666
Epoch 312
-------------------------------
Batch 1/64 loss: 0.03304338455200195
Batch 2/64 loss: 0.013914942741394043
Batch 3/64 loss: 0.0302199125289917
Batch 4/64 loss: 0.0239027738571167
Batch 5/64 loss: 0.022757768630981445
Batch 6/64 loss: 0.06840723752975464
Batch 7/64 loss: 0.04152703285217285
Batch 8/64 loss: 0.03812289237976074
Batch 9/64 loss: 0.06024205684661865
Batch 10/64 loss: 0.02014082670211792
Batch 11/64 loss: 0.028577804565429688
Batch 12/64 loss: 0.04656344652175903
Batch 13/64 loss: 0.03866314888000488
Batch 14/64 loss: 0.040485262870788574
Batch 15/64 loss: 0.04347282648086548
Batch 16/64 loss: 0.020907700061798096
Batch 17/64 loss: 0.06363457441329956
Batch 18/64 loss: 0.03409940004348755
Batch 19/64 loss: 0.037394583225250244
Batch 20/64 loss: 0.041391074657440186
Batch 21/64 loss: 0.04127335548400879
Batch 22/64 loss: 0.03320187330245972
Batch 23/64 loss: 0.030827045440673828
Batch 24/64 loss: 0.03227031230926514
Batch 25/64 loss: 0.03347259759902954
Batch 26/64 loss: 0.058805108070373535
Batch 27/64 loss: 0.02364063262939453
Batch 28/64 loss: 0.06238788366317749
Batch 29/64 loss: 0.01261824369430542
Batch 30/64 loss: 0.017467021942138672
Batch 31/64 loss: 0.05021929740905762
Batch 32/64 loss: 0.05046588182449341
Batch 33/64 loss: 0.023166537284851074
Batch 34/64 loss: 0.029897570610046387
Batch 35/64 loss: 0.00887519121170044
Batch 36/64 loss: 0.05812358856201172
Batch 37/64 loss: 0.04365032911300659
Batch 38/64 loss: 0.02474159002304077
Batch 39/64 loss: 0.0627831220626831
Batch 40/64 loss: 0.05649477243423462
Batch 41/64 loss: 0.0381852388381958
Batch 42/64 loss: 0.043124258518218994
Batch 43/64 loss: 0.03134441375732422
Batch 44/64 loss: 0.058016300201416016
Batch 45/64 loss: 0.049943387508392334
Batch 46/64 loss: 0.02582859992980957
Batch 47/64 loss: 0.037171900272369385
Batch 48/64 loss: 0.02680671215057373
Batch 49/64 loss: 0.02498859167098999
Batch 50/64 loss: 0.03853917121887207
Batch 51/64 loss: 0.029617726802825928
Batch 52/64 loss: 0.06355106830596924
Batch 53/64 loss: 0.06873226165771484
Batch 54/64 loss: 0.039777159690856934
Batch 55/64 loss: 0.07115370035171509
Batch 56/64 loss: 0.07188266515731812
Batch 57/64 loss: 0.03735768795013428
Batch 58/64 loss: 0.05505478382110596
Batch 59/64 loss: 0.043826520442962646
Batch 60/64 loss: 0.02044093608856201
Batch 61/64 loss: 0.04931825399398804
Batch 62/64 loss: 0.07356846332550049
Batch 63/64 loss: 0.040230631828308105
Batch 64/64 loss: 0.041338562965393066
Epoch 312  Train loss: 0.04033435232499066  Val loss: 0.11925974457534318
Epoch 313
-------------------------------
Batch 1/64 loss: 0.04498744010925293
Batch 2/64 loss: 0.032495856285095215
Batch 3/64 loss: 0.04229581356048584
Batch 4/64 loss: 0.04330068826675415
Batch 5/64 loss: 0.060036420822143555
Batch 6/64 loss: 0.03887397050857544
Batch 7/64 loss: 0.05588984489440918
Batch 8/64 loss: 0.03193008899688721
Batch 9/64 loss: 0.05293917655944824
Batch 10/64 loss: 0.02980881929397583
Batch 11/64 loss: 0.04807543754577637
Batch 12/64 loss: 0.033512115478515625
Batch 13/64 loss: 0.04269719123840332
Batch 14/64 loss: 0.03235161304473877
Batch 15/64 loss: 0.006133317947387695
Batch 16/64 loss: 0.0564650297164917
Batch 17/64 loss: 0.04330313205718994
Batch 18/64 loss: 0.026744842529296875
Batch 19/64 loss: 0.04539752006530762
Batch 20/64 loss: 0.0370408296585083
Batch 21/64 loss: 0.06350666284561157
Batch 22/64 loss: 0.014315485954284668
Batch 23/64 loss: 0.014545798301696777
Batch 24/64 loss: 0.04837852716445923
Batch 25/64 loss: 0.018656611442565918
Batch 26/64 loss: 0.024400770664215088
Batch 27/64 loss: 0.04339319467544556
Batch 28/64 loss: 0.06819552183151245
Batch 29/64 loss: 0.02538120746612549
Batch 30/64 loss: 0.007885217666625977
Batch 31/64 loss: 0.03583306074142456
Batch 32/64 loss: 0.04262024164199829
Batch 33/64 loss: 0.030131101608276367
Batch 34/64 loss: 0.03067225217819214
Batch 35/64 loss: 0.01868760585784912
Batch 36/64 loss: 0.03390538692474365
Batch 37/64 loss: 0.05232733488082886
Batch 38/64 loss: 0.023295164108276367
Batch 39/64 loss: 0.03920555114746094
Batch 40/64 loss: 0.033462584018707275
Batch 41/64 loss: 0.048470377922058105
Batch 42/64 loss: 0.035420238971710205
Batch 43/64 loss: 0.02644360065460205
Batch 44/64 loss: 0.033598482608795166
Batch 45/64 loss: 0.012767314910888672
Batch 46/64 loss: 0.057179808616638184
Batch 47/64 loss: 0.02105891704559326
Batch 48/64 loss: 0.03030109405517578
Batch 49/64 loss: 0.009618580341339111
Batch 50/64 loss: 0.04945790767669678
Batch 51/64 loss: 0.02042442560195923
Batch 52/64 loss: 0.07026880979537964
Batch 53/64 loss: 0.028248488903045654
Batch 54/64 loss: 0.05479741096496582
Batch 55/64 loss: 0.0761256217956543
Batch 56/64 loss: 0.05177575349807739
Batch 57/64 loss: 0.05758863687515259
Batch 58/64 loss: 0.04097682237625122
Batch 59/64 loss: 0.05342268943786621
Batch 60/64 loss: 0.046470582485198975
Batch 61/64 loss: 0.05015671253204346
Batch 62/64 loss: 0.018314123153686523
Batch 63/64 loss: 0.04905223846435547
Batch 64/64 loss: 0.07852709293365479
Epoch 313  Train loss: 0.03880646883272657  Val loss: 0.12082637380488549
Epoch 314
-------------------------------
Batch 1/64 loss: 0.021752357482910156
Batch 2/64 loss: 0.0383034348487854
Batch 3/64 loss: 0.006060242652893066
Batch 4/64 loss: 0.02290123701095581
Batch 5/64 loss: 0.0430905818939209
Batch 6/64 loss: 0.053707659244537354
Batch 7/64 loss: 0.03360283374786377
Batch 8/64 loss: 0.006508350372314453
Batch 9/64 loss: 0.020613372325897217
Batch 10/64 loss: 0.04999375343322754
Batch 11/64 loss: 0.06675434112548828
Batch 12/64 loss: 0.035167038440704346
Batch 13/64 loss: 0.056523680686950684
Batch 14/64 loss: 0.030583322048187256
Batch 15/64 loss: 0.05727320909500122
Batch 16/64 loss: 0.040122270584106445
Batch 17/64 loss: 0.062713623046875
Batch 18/64 loss: 0.03487682342529297
Batch 19/64 loss: 0.01263570785522461
Batch 20/64 loss: 0.039085447788238525
Batch 21/64 loss: 0.06060117483139038
Batch 22/64 loss: 0.049201786518096924
Batch 23/64 loss: 0.03845328092575073
Batch 24/64 loss: 0.0077686309814453125
Batch 25/64 loss: 0.021744489669799805
Batch 26/64 loss: 0.03861522674560547
Batch 27/64 loss: 0.04736810922622681
Batch 28/64 loss: 0.04285091161727905
Batch 29/64 loss: 0.05208849906921387
Batch 30/64 loss: 0.03486424684524536
Batch 31/64 loss: 0.040650248527526855
Batch 32/64 loss: 0.03665506839752197
Batch 33/64 loss: 0.049294352531433105
Batch 34/64 loss: 0.0494767427444458
Batch 35/64 loss: 0.04463505744934082
Batch 36/64 loss: 0.03049081563949585
Batch 37/64 loss: 0.058141112327575684
Batch 38/64 loss: 0.03770649433135986
Batch 39/64 loss: 0.06299489736557007
Batch 40/64 loss: 0.05913585424423218
Batch 41/64 loss: 0.029929041862487793
Batch 42/64 loss: 0.024196088314056396
Batch 43/64 loss: 0.042925119400024414
Batch 44/64 loss: 0.014137685298919678
Batch 45/64 loss: 0.027697384357452393
Batch 46/64 loss: 0.04550999402999878
Batch 47/64 loss: 0.01630532741546631
Batch 48/64 loss: 0.04607653617858887
Batch 49/64 loss: 0.018974125385284424
Batch 50/64 loss: 0.049782514572143555
Batch 51/64 loss: 0.014155983924865723
Batch 52/64 loss: 0.008858561515808105
Batch 53/64 loss: 0.038339197635650635
Batch 54/64 loss: 0.07641512155532837
Batch 55/64 loss: 0.0361289381980896
Batch 56/64 loss: 0.024670720100402832
Batch 57/64 loss: 0.0230291485786438
Batch 58/64 loss: 0.03081268072128296
Batch 59/64 loss: 0.012735366821289062
Batch 60/64 loss: 0.021894216537475586
Batch 61/64 loss: 0.043576955795288086
Batch 62/64 loss: 0.06666481494903564
Batch 63/64 loss: 0.04220861196517944
Batch 64/64 loss: 0.024349987506866455
Epoch 314  Train loss: 0.0371496927504446  Val loss: 0.11775717559139344
Epoch 315
-------------------------------
Batch 1/64 loss: 0.02795886993408203
Batch 2/64 loss: 0.05053436756134033
Batch 3/64 loss: 0.02279818058013916
Batch 4/64 loss: 0.047701478004455566
Batch 5/64 loss: 0.049260497093200684
Batch 6/64 loss: 0.02587658166885376
Batch 7/64 loss: 0.04482060670852661
Batch 8/64 loss: 0.04070448875427246
Batch 9/64 loss: 0.03103715181350708
Batch 10/64 loss: 0.06973636150360107
Batch 11/64 loss: 0.03779804706573486
Batch 12/64 loss: 0.031955838203430176
Batch 13/64 loss: 0.028765201568603516
Batch 14/64 loss: 0.035514771938323975
Batch 15/64 loss: 0.053564369678497314
Batch 16/64 loss: 0.03850466012954712
Batch 17/64 loss: 0.01619088649749756
Batch 18/64 loss: 0.04472070932388306
Batch 19/64 loss: 0.03394252061843872
Batch 20/64 loss: 0.025290489196777344
Batch 21/64 loss: 0.0430869460105896
Batch 22/64 loss: 0.015603601932525635
Batch 23/64 loss: 0.027316391468048096
Batch 24/64 loss: 0.028991103172302246
Batch 25/64 loss: 0.05684983730316162
Batch 26/64 loss: 0.022330939769744873
Batch 27/64 loss: 0.01624232530593872
Batch 28/64 loss: 0.04026728868484497
Batch 29/64 loss: 0.036779165267944336
Batch 30/64 loss: 0.013938188552856445
Batch 31/64 loss: 0.03281068801879883
Batch 32/64 loss: 0.07012581825256348
Batch 33/64 loss: 0.02395009994506836
Batch 34/64 loss: 0.03759223222732544
Batch 35/64 loss: 0.042616963386535645
Batch 36/64 loss: 0.03217655420303345
Batch 37/64 loss: 0.05052900314331055
Batch 38/64 loss: 0.04277503490447998
Batch 39/64 loss: 0.03882479667663574
Batch 40/64 loss: 0.05284303426742554
Batch 41/64 loss: 0.051203131675720215
Batch 42/64 loss: 0.04982179403305054
Batch 43/64 loss: 0.05680859088897705
Batch 44/64 loss: 0.02769291400909424
Batch 45/64 loss: 0.04679393768310547
Batch 46/64 loss: 0.05218273401260376
Batch 47/64 loss: 0.038853228092193604
Batch 48/64 loss: 0.02921593189239502
Batch 49/64 loss: 0.07217615842819214
Batch 50/64 loss: 0.04972118139266968
Batch 51/64 loss: 0.01564878225326538
Batch 52/64 loss: 0.023824334144592285
Batch 53/64 loss: 0.033270955085754395
Batch 54/64 loss: 0.03464794158935547
Batch 55/64 loss: 0.03004932403564453
Batch 56/64 loss: 0.011364102363586426
Batch 57/64 loss: 0.030732691287994385
Batch 58/64 loss: 0.05196797847747803
Batch 59/64 loss: 0.030264973640441895
Batch 60/64 loss: 0.053867876529693604
Batch 61/64 loss: 0.05015367269515991
Batch 62/64 loss: 0.04536151885986328
Batch 63/64 loss: 0.024530410766601562
Batch 64/64 loss: 0.048595130443573
Epoch 315  Train loss: 0.03810080944323072  Val loss: 0.12298413547863256
Epoch 316
-------------------------------
Batch 1/64 loss: 0.054631948471069336
Batch 2/64 loss: 0.04943430423736572
Batch 3/64 loss: 0.026239633560180664
Batch 4/64 loss: 0.00447392463684082
Batch 5/64 loss: 0.04158580303192139
Batch 6/64 loss: 0.056176185607910156
Batch 7/64 loss: 0.06051647663116455
Batch 8/64 loss: 0.027518928050994873
Batch 9/64 loss: 0.04998236894607544
Batch 10/64 loss: 0.01690232753753662
Batch 11/64 loss: 0.04039907455444336
Batch 12/64 loss: 0.03702127933502197
Batch 13/64 loss: 0.01710653305053711
Batch 14/64 loss: 0.005006968975067139
Batch 15/64 loss: 0.09321528673171997
Batch 16/64 loss: 0.0408969521522522
Batch 17/64 loss: 0.04482775926589966
Batch 18/64 loss: 0.02679729461669922
Batch 19/64 loss: 0.06343269348144531
Batch 20/64 loss: 0.035503089427948
Batch 21/64 loss: 0.037429749965667725
Batch 22/64 loss: 0.034026920795440674
Batch 23/64 loss: 0.04278278350830078
Batch 24/64 loss: 0.03229105472564697
Batch 25/64 loss: 0.025761842727661133
Batch 26/64 loss: 0.04312562942504883
Batch 27/64 loss: 0.023681640625
Batch 28/64 loss: 0.03236645460128784
Batch 29/64 loss: 0.053453803062438965
Batch 30/64 loss: 0.03441101312637329
Batch 31/64 loss: 0.0413966178894043
Batch 32/64 loss: 0.08084243535995483
Batch 33/64 loss: 0.028267323970794678
Batch 34/64 loss: 0.08675909042358398
Batch 35/64 loss: 0.027215957641601562
Batch 36/64 loss: 0.05326896905899048
Batch 37/64 loss: 0.042074501514434814
Batch 38/64 loss: 0.018491268157958984
Batch 39/64 loss: 0.023564398288726807
Batch 40/64 loss: 0.029345810413360596
Batch 41/64 loss: 0.051103055477142334
Batch 42/64 loss: 0.07388651371002197
Batch 43/64 loss: 0.007713913917541504
Batch 44/64 loss: 0.055484652519226074
Batch 45/64 loss: 0.048593997955322266
Batch 46/64 loss: 0.03206157684326172
Batch 47/64 loss: 0.03784745931625366
Batch 48/64 loss: 0.04525214433670044
Batch 49/64 loss: 0.01730978488922119
Batch 50/64 loss: 0.033641695976257324
Batch 51/64 loss: 0.046416401863098145
Batch 52/64 loss: 0.030886292457580566
Batch 53/64 loss: 0.018017053604125977
Batch 54/64 loss: 0.03797018527984619
Batch 55/64 loss: 0.039003849029541016
Batch 56/64 loss: 0.017240285873413086
Batch 57/64 loss: 0.02738058567047119
Batch 58/64 loss: 0.03210872411727905
Batch 59/64 loss: 0.05550742149353027
Batch 60/64 loss: 0.033124566078186035
Batch 61/64 loss: 0.025799572467803955
Batch 62/64 loss: 0.038513123989105225
Batch 63/64 loss: 0.03472203016281128
Batch 64/64 loss: 0.03695321083068848
Epoch 316  Train loss: 0.03842393557230631  Val loss: 0.12318832644891903
Epoch 317
-------------------------------
Batch 1/64 loss: 0.038957178592681885
Batch 2/64 loss: 0.05620169639587402
Batch 3/64 loss: 0.0442507266998291
Batch 4/64 loss: 0.033009231090545654
Batch 5/64 loss: 0.0545307993888855
Batch 6/64 loss: 0.03324156999588013
Batch 7/64 loss: 0.056267380714416504
Batch 8/64 loss: 0.02760899066925049
Batch 9/64 loss: 0.04212665557861328
Batch 10/64 loss: 0.0369531512260437
Batch 11/64 loss: 0.028094172477722168
Batch 12/64 loss: 0.029117584228515625
Batch 13/64 loss: 0.0515211820602417
Batch 14/64 loss: 0.02377903461456299
Batch 15/64 loss: 0.042526304721832275
Batch 16/64 loss: 0.06368708610534668
Batch 17/64 loss: 0.05953770875930786
Batch 18/64 loss: 0.030383706092834473
Batch 19/64 loss: 0.02440786361694336
Batch 20/64 loss: 0.03358197212219238
Batch 21/64 loss: 0.030385851860046387
Batch 22/64 loss: 0.04677838087081909
Batch 23/64 loss: 0.010937809944152832
Batch 24/64 loss: 0.043898582458496094
Batch 25/64 loss: 0.031225979328155518
Batch 26/64 loss: 0.012961328029632568
Batch 27/64 loss: 0.02551591396331787
Batch 28/64 loss: 0.02742534875869751
Batch 29/64 loss: 0.03196591138839722
Batch 30/64 loss: 0.040418922901153564
Batch 31/64 loss: 0.03803408145904541
Batch 32/64 loss: 0.08960574865341187
Batch 33/64 loss: 0.014550507068634033
Batch 34/64 loss: 0.01597881317138672
Batch 35/64 loss: 0.020656347274780273
Batch 36/64 loss: 0.047890424728393555
Batch 37/64 loss: 0.05700373649597168
Batch 38/64 loss: 0.05834090709686279
Batch 39/64 loss: 0.049484431743621826
Batch 40/64 loss: 0.051287829875946045
Batch 41/64 loss: 0.03703731298446655
Batch 42/64 loss: 0.0408516526222229
Batch 43/64 loss: 0.06378710269927979
Batch 44/64 loss: 0.060812175273895264
Batch 45/64 loss: 0.016856074333190918
Batch 46/64 loss: 0.015400469303131104
Batch 47/64 loss: 0.055053114891052246
Batch 48/64 loss: 0.02137470245361328
Batch 49/64 loss: 0.05030697584152222
Batch 50/64 loss: 0.03131073713302612
Batch 51/64 loss: 0.03884744644165039
Batch 52/64 loss: 0.06123650074005127
Batch 53/64 loss: 0.06833690404891968
Batch 54/64 loss: 0.03537559509277344
Batch 55/64 loss: 0.018352270126342773
Batch 56/64 loss: 0.04405689239501953
Batch 57/64 loss: 0.030228257179260254
Batch 58/64 loss: 0.04513108730316162
Batch 59/64 loss: 0.02486276626586914
Batch 60/64 loss: 0.04579579830169678
Batch 61/64 loss: 0.04292267560958862
Batch 62/64 loss: 0.026874542236328125
Batch 63/64 loss: 0.030538439750671387
Batch 64/64 loss: 0.04603421688079834
Epoch 317  Train loss: 0.039121662869172937  Val loss: 0.11774227791225787
Epoch 318
-------------------------------
Batch 1/64 loss: 0.03473705053329468
Batch 2/64 loss: 0.03961557149887085
Batch 3/64 loss: 0.02658909559249878
Batch 4/64 loss: 0.04372763633728027
Batch 5/64 loss: 0.035550832748413086
Batch 6/64 loss: 0.016348838806152344
Batch 7/64 loss: 0.05168789625167847
Batch 8/64 loss: 0.011860430240631104
Batch 9/64 loss: 0.03541988134384155
Batch 10/64 loss: 0.03493303060531616
Batch 11/64 loss: 0.05258744955062866
Batch 12/64 loss: 0.012196660041809082
Batch 13/64 loss: 0.01946192979812622
Batch 14/64 loss: 0.04188692569732666
Batch 15/64 loss: 0.05964690446853638
Batch 16/64 loss: 0.048364877700805664
Batch 17/64 loss: 0.007158398628234863
Batch 18/64 loss: 0.07518237829208374
Batch 19/64 loss: 0.027963876724243164
Batch 20/64 loss: 0.03693187236785889
Batch 21/64 loss: 0.033139586448669434
Batch 22/64 loss: 0.04353535175323486
Batch 23/64 loss: 0.04409646987915039
Batch 24/64 loss: 0.05090212821960449
Batch 25/64 loss: 0.04260212182998657
Batch 26/64 loss: 0.03923875093460083
Batch 27/64 loss: 0.04834192991256714
Batch 28/64 loss: 0.020899951457977295
Batch 29/64 loss: 0.02343618869781494
Batch 30/64 loss: 0.053105711936950684
Batch 31/64 loss: 0.035667240619659424
Batch 32/64 loss: 0.04712176322937012
Batch 33/64 loss: 0.029141485691070557
Batch 34/64 loss: 0.04944264888763428
Batch 35/64 loss: 0.039695918560028076
Batch 36/64 loss: 0.030421853065490723
Batch 37/64 loss: 0.04502248764038086
Batch 38/64 loss: 0.041979432106018066
Batch 39/64 loss: 0.029588520526885986
Batch 40/64 loss: 0.04754769802093506
Batch 41/64 loss: 0.0320124626159668
Batch 42/64 loss: 0.022226929664611816
Batch 43/64 loss: 0.01626431941986084
Batch 44/64 loss: 0.063759446144104
Batch 45/64 loss: 0.027422726154327393
Batch 46/64 loss: 0.04170095920562744
Batch 47/64 loss: 0.07322728633880615
Batch 48/64 loss: 0.02751702070236206
Batch 49/64 loss: 0.025398075580596924
Batch 50/64 loss: 0.041101694107055664
Batch 51/64 loss: 0.02551853656768799
Batch 52/64 loss: 0.030162930488586426
Batch 53/64 loss: 0.028517961502075195
Batch 54/64 loss: 0.03920108079910278
Batch 55/64 loss: 0.03344237804412842
Batch 56/64 loss: 0.01714867353439331
Batch 57/64 loss: 0.06310874223709106
Batch 58/64 loss: 0.027155518531799316
Batch 59/64 loss: 0.05679994821548462
Batch 60/64 loss: 0.049886465072631836
Batch 61/64 loss: 0.04871922731399536
Batch 62/64 loss: 0.038938939571380615
Batch 63/64 loss: 0.036438822746276855
Batch 64/64 loss: 0.04136556386947632
Epoch 318  Train loss: 0.03770153873107013  Val loss: 0.1179643271305307
Epoch 319
-------------------------------
Batch 1/64 loss: 0.028811514377593994
Batch 2/64 loss: 0.05617713928222656
Batch 3/64 loss: 0.020495176315307617
Batch 4/64 loss: 0.039051175117492676
Batch 5/64 loss: 0.04013568162918091
Batch 6/64 loss: 0.046478867530822754
Batch 7/64 loss: 0.03174793720245361
Batch 8/64 loss: 0.07130807638168335
Batch 9/64 loss: 0.05075359344482422
Batch 10/64 loss: 0.04384791851043701
Batch 11/64 loss: 0.042252540588378906
Batch 12/64 loss: 0.04332202672958374
Batch 13/64 loss: 0.03320014476776123
Batch 14/64 loss: 0.022758126258850098
Batch 15/64 loss: 0.04404592514038086
Batch 16/64 loss: 0.03804326057434082
Batch 17/64 loss: 0.020667672157287598
Batch 18/64 loss: 0.050374627113342285
Batch 19/64 loss: 0.03252768516540527
Batch 20/64 loss: 0.01815718412399292
Batch 21/64 loss: 0.05542987585067749
Batch 22/64 loss: 0.019993901252746582
Batch 23/64 loss: 0.06496155261993408
Batch 24/64 loss: 0.02877587080001831
Batch 25/64 loss: 0.05509603023529053
Batch 26/64 loss: 0.03163498640060425
Batch 27/64 loss: 0.04999798536300659
Batch 28/64 loss: 0.027262985706329346
Batch 29/64 loss: 0.046805381774902344
Batch 30/64 loss: 0.08089816570281982
Batch 31/64 loss: 0.030609846115112305
Batch 32/64 loss: 0.05355679988861084
Batch 33/64 loss: 0.041513144969940186
Batch 34/64 loss: 0.0651751160621643
Batch 35/64 loss: 0.043549299240112305
Batch 36/64 loss: 0.036018311977386475
Batch 37/64 loss: 0.03696364164352417
Batch 38/64 loss: 0.02104705572128296
Batch 39/64 loss: 0.016433000564575195
Batch 40/64 loss: 0.051830291748046875
Batch 41/64 loss: 0.029338598251342773
Batch 42/64 loss: 0.038748323917388916
Batch 43/64 loss: 0.02892613410949707
Batch 44/64 loss: 0.0511319637298584
Batch 45/64 loss: 0.03612041473388672
Batch 46/64 loss: 0.04938727617263794
Batch 47/64 loss: 0.013468742370605469
Batch 48/64 loss: 0.03164017200469971
Batch 49/64 loss: 0.00893491506576538
Batch 50/64 loss: 0.018676459789276123
Batch 51/64 loss: 0.06104445457458496
Batch 52/64 loss: 0.020982861518859863
Batch 53/64 loss: 0.037534475326538086
Batch 54/64 loss: 0.022929668426513672
Batch 55/64 loss: 0.06187629699707031
Batch 56/64 loss: 0.04106402397155762
Batch 57/64 loss: 0.031139791011810303
Batch 58/64 loss: 0.029319703578948975
Batch 59/64 loss: 0.008441448211669922
Batch 60/64 loss: 0.030174314975738525
Batch 61/64 loss: 0.037177205085754395
Batch 62/64 loss: 0.011897683143615723
Batch 63/64 loss: 0.03328728675842285
Batch 64/64 loss: 0.02827608585357666
Epoch 319  Train loss: 0.03742994186924953  Val loss: 0.11759374477609326
Epoch 320
-------------------------------
Batch 1/64 loss: 0.04319894313812256
Batch 2/64 loss: 0.04157364368438721
Batch 3/64 loss: 0.03640115261077881
Batch 4/64 loss: 0.048868775367736816
Batch 5/64 loss: 0.04097872972488403
Batch 6/64 loss: 0.0445634126663208
Batch 7/64 loss: 0.06111335754394531
Batch 8/64 loss: 0.055181920528411865
Batch 9/64 loss: -0.0007724165916442871
Batch 10/64 loss: 0.03919482231140137
Batch 11/64 loss: 0.035388827323913574
Batch 12/64 loss: 0.06082606315612793
Batch 13/64 loss: 0.015034615993499756
Batch 14/64 loss: 0.03072535991668701
Batch 15/64 loss: 0.03802710771560669
Batch 16/64 loss: 0.067474365234375
Batch 17/64 loss: 0.035950541496276855
Batch 18/64 loss: 0.05792957544326782
Batch 19/64 loss: 0.019408762454986572
Batch 20/64 loss: 0.021026253700256348
Batch 21/64 loss: 0.02253490686416626
Batch 22/64 loss: 0.029296875
Batch 23/64 loss: 0.04992109537124634
Batch 24/64 loss: 0.05771315097808838
Batch 25/64 loss: 0.034462809562683105
Batch 26/64 loss: 0.030029773712158203
Batch 27/64 loss: 0.021563410758972168
Batch 28/64 loss: 0.032698869705200195
Batch 29/64 loss: 0.026651620864868164
Batch 30/64 loss: 0.06154441833496094
Batch 31/64 loss: 0.028507351875305176
Batch 32/64 loss: 0.02671748399734497
Batch 33/64 loss: 0.02277064323425293
Batch 34/64 loss: 0.05740541219711304
Batch 35/64 loss: 0.020824015140533447
Batch 36/64 loss: 0.022694766521453857
Batch 37/64 loss: 0.019297003746032715
Batch 38/64 loss: 0.029255688190460205
Batch 39/64 loss: 0.03214466571807861
Batch 40/64 loss: 0.03846806287765503
Batch 41/64 loss: 0.030046045780181885
Batch 42/64 loss: 0.036776959896087646
Batch 43/64 loss: 0.02666640281677246
Batch 44/64 loss: 0.011099755764007568
Batch 45/64 loss: 0.0285646915435791
Batch 46/64 loss: 0.024156630039215088
Batch 47/64 loss: 0.05356705188751221
Batch 48/64 loss: 0.053466200828552246
Batch 49/64 loss: 0.05904436111450195
Batch 50/64 loss: 0.06023740768432617
Batch 51/64 loss: 0.056455016136169434
Batch 52/64 loss: 0.046322643756866455
Batch 53/64 loss: 0.036711037158966064
Batch 54/64 loss: 0.03721773624420166
Batch 55/64 loss: 0.010866820812225342
Batch 56/64 loss: 0.06890690326690674
Batch 57/64 loss: 0.037562429904937744
Batch 58/64 loss: 0.03489792346954346
Batch 59/64 loss: 0.03199809789657593
Batch 60/64 loss: 0.02324885129928589
Batch 61/64 loss: 0.02535533905029297
Batch 62/64 loss: 0.028271019458770752
Batch 63/64 loss: 0.038163959980010986
Batch 64/64 loss: 0.04025304317474365
Epoch 320  Train loss: 0.036806100490046484  Val loss: 0.11544113265689705
Saving best model, epoch: 320
Epoch 321
-------------------------------
Batch 1/64 loss: 0.06377637386322021
Batch 2/64 loss: 0.039339542388916016
Batch 3/64 loss: 0.03488802909851074
Batch 4/64 loss: 0.04407024383544922
Batch 5/64 loss: 0.027407526969909668
Batch 6/64 loss: 0.02502506971359253
Batch 7/64 loss: 0.031568944454193115
Batch 8/64 loss: 0.018646955490112305
Batch 9/64 loss: 0.007332205772399902
Batch 10/64 loss: 0.037665605545043945
Batch 11/64 loss: 0.049832701683044434
Batch 12/64 loss: 0.04467976093292236
Batch 13/64 loss: 0.024488627910614014
Batch 14/64 loss: 0.028136491775512695
Batch 15/64 loss: 0.045142292976379395
Batch 16/64 loss: 0.05538386106491089
Batch 17/64 loss: 0.03187304735183716
Batch 18/64 loss: 0.04746842384338379
Batch 19/64 loss: 0.07240277528762817
Batch 20/64 loss: 0.082278311252594
Batch 21/64 loss: 0.038259148597717285
Batch 22/64 loss: 0.029124021530151367
Batch 23/64 loss: 0.03415590524673462
Batch 24/64 loss: 0.019145548343658447
Batch 25/64 loss: 0.01721501350402832
Batch 26/64 loss: 0.03919780254364014
Batch 27/64 loss: 0.01826411485671997
Batch 28/64 loss: 0.01687866449356079
Batch 29/64 loss: 0.0076073408126831055
Batch 30/64 loss: 0.04445087909698486
Batch 31/64 loss: 0.015932321548461914
Batch 32/64 loss: 0.0430985689163208
Batch 33/64 loss: 0.009573161602020264
Batch 34/64 loss: 0.021892666816711426
Batch 35/64 loss: 0.04584383964538574
Batch 36/64 loss: 0.04334777593612671
Batch 37/64 loss: 0.07952243089675903
Batch 38/64 loss: 0.046781957149505615
Batch 39/64 loss: 0.040760934352874756
Batch 40/64 loss: 0.06200891733169556
Batch 41/64 loss: 0.030914247035980225
Batch 42/64 loss: 0.03895449638366699
Batch 43/64 loss: 0.03011798858642578
Batch 44/64 loss: 0.047504961490631104
Batch 45/64 loss: 0.02536088228225708
Batch 46/64 loss: 0.05145680904388428
Batch 47/64 loss: 0.03212928771972656
Batch 48/64 loss: 0.023494839668273926
Batch 49/64 loss: 0.0770491361618042
Batch 50/64 loss: 0.018623650074005127
Batch 51/64 loss: 0.058564722537994385
Batch 52/64 loss: 0.0428156852722168
Batch 53/64 loss: 0.03763371706008911
Batch 54/64 loss: 0.03362375497817993
Batch 55/64 loss: 0.01579946279525757
Batch 56/64 loss: 0.044152677059173584
Batch 57/64 loss: 0.03380692005157471
Batch 58/64 loss: 0.043870508670806885
Batch 59/64 loss: 0.05928415060043335
Batch 60/64 loss: 0.03374892473220825
Batch 61/64 loss: 0.03620821237564087
Batch 62/64 loss: 0.042111337184906006
Batch 63/64 loss: 0.05788701772689819
Batch 64/64 loss: 0.07291454076766968
Epoch 321  Train loss: 0.03849830744313259  Val loss: 0.12084936849849741
Epoch 322
-------------------------------
Batch 1/64 loss: 0.020224809646606445
Batch 2/64 loss: 0.049309492111206055
Batch 3/64 loss: 0.03414642810821533
Batch 4/64 loss: 0.02252960205078125
Batch 5/64 loss: 0.024154305458068848
Batch 6/64 loss: 0.03710585832595825
Batch 7/64 loss: 0.025666415691375732
Batch 8/64 loss: 0.05403167009353638
Batch 9/64 loss: 0.047402024269104004
Batch 10/64 loss: 0.023437917232513428
Batch 11/64 loss: 0.017466247081756592
Batch 12/64 loss: 0.0037589073181152344
Batch 13/64 loss: 0.03851664066314697
Batch 14/64 loss: 0.061573028564453125
Batch 15/64 loss: 0.03376805782318115
Batch 16/64 loss: 0.03697800636291504
Batch 17/64 loss: 0.010232508182525635
Batch 18/64 loss: 0.04989057779312134
Batch 19/64 loss: 0.049995362758636475
Batch 20/64 loss: 0.022418081760406494
Batch 21/64 loss: 0.05298405885696411
Batch 22/64 loss: 0.016026735305786133
Batch 23/64 loss: 0.0585637092590332
Batch 24/64 loss: 0.046592652797698975
Batch 25/64 loss: 0.04333913326263428
Batch 26/64 loss: 0.026308894157409668
Batch 27/64 loss: 0.0456695556640625
Batch 28/64 loss: 0.03658062219619751
Batch 29/64 loss: 0.041064560413360596
Batch 30/64 loss: 0.026960909366607666
Batch 31/64 loss: 0.04247474670410156
Batch 32/64 loss: 0.044347405433654785
Batch 33/64 loss: 0.06837856769561768
Batch 34/64 loss: 0.05235570669174194
Batch 35/64 loss: 0.015265583992004395
Batch 36/64 loss: 0.04546386003494263
Batch 37/64 loss: 0.01906108856201172
Batch 38/64 loss: 0.0305289626121521
Batch 39/64 loss: 0.06563794612884521
Batch 40/64 loss: 0.024403870105743408
Batch 41/64 loss: 0.0480731725692749
Batch 42/64 loss: 0.06654465198516846
Batch 43/64 loss: 0.03812927007675171
Batch 44/64 loss: 0.049855709075927734
Batch 45/64 loss: 0.0374642014503479
Batch 46/64 loss: 0.05327415466308594
Batch 47/64 loss: 0.05444669723510742
Batch 48/64 loss: 0.034109652042388916
Batch 49/64 loss: 0.00851738452911377
Batch 50/64 loss: 0.029857277870178223
Batch 51/64 loss: 0.03751140832901001
Batch 52/64 loss: 0.010093092918395996
Batch 53/64 loss: 0.03856390714645386
Batch 54/64 loss: 0.03059595823287964
Batch 55/64 loss: 0.01772773265838623
Batch 56/64 loss: 0.012005388736724854
Batch 57/64 loss: 0.022843539714813232
Batch 58/64 loss: 0.051504433155059814
Batch 59/64 loss: 0.05854475498199463
Batch 60/64 loss: 0.052581191062927246
Batch 61/64 loss: 0.052466630935668945
Batch 62/64 loss: 0.0292091965675354
Batch 63/64 loss: 0.05581384897232056
Batch 64/64 loss: 0.02520507574081421
Epoch 322  Train loss: 0.03722747564315796  Val loss: 0.11898266900445997
Epoch 323
-------------------------------
Batch 1/64 loss: 0.016856014728546143
Batch 2/64 loss: 0.0508233904838562
Batch 3/64 loss: 0.06052684783935547
Batch 4/64 loss: 0.0259091854095459
Batch 5/64 loss: 0.014410257339477539
Batch 6/64 loss: 0.026925981044769287
Batch 7/64 loss: 0.036805570125579834
Batch 8/64 loss: 0.021061837673187256
Batch 9/64 loss: 0.01640266180038452
Batch 10/64 loss: 0.026179254055023193
Batch 11/64 loss: 0.03929257392883301
Batch 12/64 loss: 0.0406460165977478
Batch 13/64 loss: 0.023770153522491455
Batch 14/64 loss: 0.04559791088104248
Batch 15/64 loss: 0.037321627140045166
Batch 16/64 loss: 0.0426936149597168
Batch 17/64 loss: 0.040695130825042725
Batch 18/64 loss: 0.03288239240646362
Batch 19/64 loss: 0.02112036943435669
Batch 20/64 loss: 0.03174394369125366
Batch 21/64 loss: 0.0580977201461792
Batch 22/64 loss: 0.025614023208618164
Batch 23/64 loss: 0.030175447463989258
Batch 24/64 loss: 0.03887289762496948
Batch 25/64 loss: 0.012203335762023926
Batch 26/64 loss: 0.03673774003982544
Batch 27/64 loss: 0.044751524925231934
Batch 28/64 loss: 0.03883671760559082
Batch 29/64 loss: 0.03553783893585205
Batch 30/64 loss: 0.03643912076950073
Batch 31/64 loss: 0.016076207160949707
Batch 32/64 loss: 0.029037117958068848
Batch 33/64 loss: 0.02879655361175537
Batch 34/64 loss: 0.025326251983642578
Batch 35/64 loss: 0.04398214817047119
Batch 36/64 loss: 0.04533267021179199
Batch 37/64 loss: 0.05091816186904907
Batch 38/64 loss: 0.04382491111755371
Batch 39/64 loss: 0.04307025671005249
Batch 40/64 loss: 0.043308377265930176
Batch 41/64 loss: 0.025834321975708008
Batch 42/64 loss: 0.03420048952102661
Batch 43/64 loss: 0.016279637813568115
Batch 44/64 loss: 0.059697747230529785
Batch 45/64 loss: 0.07343524694442749
Batch 46/64 loss: 0.04245668649673462
Batch 47/64 loss: 0.043812572956085205
Batch 48/64 loss: 0.03193247318267822
Batch 49/64 loss: 0.02773648500442505
Batch 50/64 loss: 0.06503838300704956
Batch 51/64 loss: 0.03359031677246094
Batch 52/64 loss: 0.043568968772888184
Batch 53/64 loss: 0.03815662860870361
Batch 54/64 loss: 0.014505386352539062
Batch 55/64 loss: 0.0685352087020874
Batch 56/64 loss: 0.040055036544799805
Batch 57/64 loss: 0.054613351821899414
Batch 58/64 loss: 0.04330641031265259
Batch 59/64 loss: 0.05706179141998291
Batch 60/64 loss: 0.032802462577819824
Batch 61/64 loss: 0.0609128475189209
Batch 62/64 loss: 0.016360163688659668
Batch 63/64 loss: 0.031447768211364746
Batch 64/64 loss: 0.04665094614028931
Epoch 323  Train loss: 0.03715972316031362  Val loss: 0.11931858439625743
Epoch 324
-------------------------------
Batch 1/64 loss: 0.037446022033691406
Batch 2/64 loss: 0.013873457908630371
Batch 3/64 loss: 0.0170743465423584
Batch 4/64 loss: 0.011433720588684082
Batch 5/64 loss: 0.029967784881591797
Batch 6/64 loss: 0.05351221561431885
Batch 7/64 loss: 0.00301361083984375
Batch 8/64 loss: 0.05017447471618652
Batch 9/64 loss: 0.04249972105026245
Batch 10/64 loss: 0.021781861782073975
Batch 11/64 loss: 0.03376692533493042
Batch 12/64 loss: 0.057016968727111816
Batch 13/64 loss: 0.03202188014984131
Batch 14/64 loss: 0.0443386435508728
Batch 15/64 loss: 0.052295565605163574
Batch 16/64 loss: 0.0516471266746521
Batch 17/64 loss: 0.034295082092285156
Batch 18/64 loss: 0.018084347248077393
Batch 19/64 loss: 0.010079145431518555
Batch 20/64 loss: 0.03438675403594971
Batch 21/64 loss: 0.05286365747451782
Batch 22/64 loss: 0.030654311180114746
Batch 23/64 loss: 0.03467804193496704
Batch 24/64 loss: 0.03882712125778198
Batch 25/64 loss: 0.023050665855407715
Batch 26/64 loss: 0.034444212913513184
Batch 27/64 loss: 0.03130149841308594
Batch 28/64 loss: 0.06314074993133545
Batch 29/64 loss: 0.02032536268234253
Batch 30/64 loss: 0.06633591651916504
Batch 31/64 loss: 0.0432056188583374
Batch 32/64 loss: 0.02601701021194458
Batch 33/64 loss: 0.03961247205734253
Batch 34/64 loss: 0.03838467597961426
Batch 35/64 loss: 0.02428293228149414
Batch 36/64 loss: 0.02192896604537964
Batch 37/64 loss: 0.029774904251098633
Batch 38/64 loss: 0.05320185422897339
Batch 39/64 loss: 0.010700047016143799
Batch 40/64 loss: 0.032692551612854004
Batch 41/64 loss: 0.0420374870300293
Batch 42/64 loss: 0.02707594633102417
Batch 43/64 loss: 0.02695333957672119
Batch 44/64 loss: 0.058042943477630615
Batch 45/64 loss: 0.037713587284088135
Batch 46/64 loss: 0.019336342811584473
Batch 47/64 loss: 0.03359520435333252
Batch 48/64 loss: 0.05167192220687866
Batch 49/64 loss: 0.05086970329284668
Batch 50/64 loss: 0.02106398344039917
Batch 51/64 loss: 0.07523369789123535
Batch 52/64 loss: 0.024200081825256348
Batch 53/64 loss: 0.04980957508087158
Batch 54/64 loss: 0.054960012435913086
Batch 55/64 loss: 0.05715590715408325
Batch 56/64 loss: 0.043166935443878174
Batch 57/64 loss: 0.027544736862182617
Batch 58/64 loss: 0.0646817684173584
Batch 59/64 loss: 0.03555309772491455
Batch 60/64 loss: 0.04402536153793335
Batch 61/64 loss: 0.016947388648986816
Batch 62/64 loss: 0.05133056640625
Batch 63/64 loss: 0.0762975811958313
Batch 64/64 loss: 0.03252387046813965
Epoch 324  Train loss: 0.03729870272617714  Val loss: 0.11779791321541436
Epoch 325
-------------------------------
Batch 1/64 loss: 0.023402690887451172
Batch 2/64 loss: 0.025135278701782227
Batch 3/64 loss: 0.026334047317504883
Batch 4/64 loss: 0.030458450317382812
Batch 5/64 loss: 0.042330026626586914
Batch 6/64 loss: 0.040636658668518066
Batch 7/64 loss: 0.03529989719390869
Batch 8/64 loss: 0.05105161666870117
Batch 9/64 loss: 0.03796219825744629
Batch 10/64 loss: 0.04493141174316406
Batch 11/64 loss: 0.05310946702957153
Batch 12/64 loss: 0.034994542598724365
Batch 13/64 loss: 0.004519939422607422
Batch 14/64 loss: 0.03868168592453003
Batch 15/64 loss: 0.022428154945373535
Batch 16/64 loss: 0.06145423650741577
Batch 17/64 loss: 0.034673869609832764
Batch 18/64 loss: -0.003507554531097412
Batch 19/64 loss: 0.04540520906448364
Batch 20/64 loss: 0.032245516777038574
Batch 21/64 loss: 0.027774333953857422
Batch 22/64 loss: 0.015290319919586182
Batch 23/64 loss: 0.048838138580322266
Batch 24/64 loss: 0.038654983043670654
Batch 25/64 loss: 0.0365217924118042
Batch 26/64 loss: 0.022871315479278564
Batch 27/64 loss: 0.019123077392578125
Batch 28/64 loss: 0.016807854175567627
Batch 29/64 loss: 0.04312402009963989
Batch 30/64 loss: 0.03925853967666626
Batch 31/64 loss: 0.056645214557647705
Batch 32/64 loss: 0.05614227056503296
Batch 33/64 loss: 0.05489301681518555
Batch 34/64 loss: 0.0007706284523010254
Batch 35/64 loss: 0.04664653539657593
Batch 36/64 loss: 0.04503631591796875
Batch 37/64 loss: 0.047965049743652344
Batch 38/64 loss: 0.04456436634063721
Batch 39/64 loss: 0.044437944889068604
Batch 40/64 loss: 0.06443536281585693
Batch 41/64 loss: 0.039736270904541016
Batch 42/64 loss: 0.017548799514770508
Batch 43/64 loss: 0.005292415618896484
Batch 44/64 loss: 0.02863609790802002
Batch 45/64 loss: 0.01824897527694702
Batch 46/64 loss: 0.03137218952178955
Batch 47/64 loss: 0.04267680644989014
Batch 48/64 loss: 0.019472599029541016
Batch 49/64 loss: 0.030251741409301758
Batch 50/64 loss: 0.047554075717926025
Batch 51/64 loss: 0.036851584911346436
Batch 52/64 loss: 0.05831336975097656
Batch 53/64 loss: 0.028592348098754883
Batch 54/64 loss: 0.05510544776916504
Batch 55/64 loss: 0.053362250328063965
Batch 56/64 loss: 0.046677470207214355
Batch 57/64 loss: 0.015543878078460693
Batch 58/64 loss: 0.028165459632873535
Batch 59/64 loss: 0.028832614421844482
Batch 60/64 loss: 0.03981161117553711
Batch 61/64 loss: 0.025078415870666504
Batch 62/64 loss: 0.02955043315887451
Batch 63/64 loss: 0.051035284996032715
Batch 64/64 loss: 0.07163876295089722
Epoch 325  Train loss: 0.0358084335046656  Val loss: 0.11810564093573396
Epoch 326
-------------------------------
Batch 1/64 loss: 0.05012941360473633
Batch 2/64 loss: 0.005127012729644775
Batch 3/64 loss: 0.02322995662689209
Batch 4/64 loss: 0.03129744529724121
Batch 5/64 loss: 0.0242462158203125
Batch 6/64 loss: 0.05119448900222778
Batch 7/64 loss: 0.033275604248046875
Batch 8/64 loss: 0.0377887487411499
Batch 9/64 loss: 0.034056663513183594
Batch 10/64 loss: 0.02835071086883545
Batch 11/64 loss: 0.06457650661468506
Batch 12/64 loss: 0.027259230613708496
Batch 13/64 loss: 0.01141422986984253
Batch 14/64 loss: 0.018720149993896484
Batch 15/64 loss: 0.04288703203201294
Batch 16/64 loss: 0.03704249858856201
Batch 17/64 loss: 0.03159439563751221
Batch 18/64 loss: 0.022445499897003174
Batch 19/64 loss: 0.02149951457977295
Batch 20/64 loss: 0.024164319038391113
Batch 21/64 loss: 0.0271298885345459
Batch 22/64 loss: 0.036343932151794434
Batch 23/64 loss: 0.06858158111572266
Batch 24/64 loss: 0.03178143501281738
Batch 25/64 loss: 0.03747439384460449
Batch 26/64 loss: 0.015107333660125732
Batch 27/64 loss: 0.03466254472732544
Batch 28/64 loss: 0.03877061605453491
Batch 29/64 loss: 0.059783101081848145
Batch 30/64 loss: 0.045841872692108154
Batch 31/64 loss: 0.050562381744384766
Batch 32/64 loss: 0.05331599712371826
Batch 33/64 loss: 0.038409411907196045
Batch 34/64 loss: 0.04494798183441162
Batch 35/64 loss: 0.05425727367401123
Batch 36/64 loss: 0.022689402103424072
Batch 37/64 loss: 0.06288027763366699
Batch 38/64 loss: 0.018725037574768066
Batch 39/64 loss: 0.027274250984191895
Batch 40/64 loss: 0.03437930345535278
Batch 41/64 loss: 0.014181852340698242
Batch 42/64 loss: 0.05949699878692627
Batch 43/64 loss: 0.020958423614501953
Batch 44/64 loss: 0.05718362331390381
Batch 45/64 loss: 0.038197219371795654
Batch 46/64 loss: 0.051637887954711914
Batch 47/64 loss: 0.044438958168029785
Batch 48/64 loss: 0.04123342037200928
Batch 49/64 loss: 0.04199618101119995
Batch 50/64 loss: 0.01465839147567749
Batch 51/64 loss: 0.03355652093887329
Batch 52/64 loss: 0.07497107982635498
Batch 53/64 loss: 0.02873009443283081
Batch 54/64 loss: 0.05147576332092285
Batch 55/64 loss: 0.03667241334915161
Batch 56/64 loss: 0.06459081172943115
Batch 57/64 loss: 0.015374183654785156
Batch 58/64 loss: 0.028900444507598877
Batch 59/64 loss: 0.004661381244659424
Batch 60/64 loss: 0.06395465135574341
Batch 61/64 loss: 0.047749876976013184
Batch 62/64 loss: 0.02523672580718994
Batch 63/64 loss: 0.04115605354309082
Batch 64/64 loss: 0.019804298877716064
Epoch 326  Train loss: 0.03669151114482506  Val loss: 0.11743328046962567
Epoch 327
-------------------------------
Batch 1/64 loss: 0.04369950294494629
Batch 2/64 loss: 0.04535174369812012
Batch 3/64 loss: 0.020602881908416748
Batch 4/64 loss: 0.0626559853553772
Batch 5/64 loss: 0.029624223709106445
Batch 6/64 loss: 0.06740784645080566
Batch 7/64 loss: 0.041415274143218994
Batch 8/64 loss: 0.004409611225128174
Batch 9/64 loss: 0.02183002233505249
Batch 10/64 loss: 0.04665112495422363
Batch 11/64 loss: 0.007594108581542969
Batch 12/64 loss: 0.04913938045501709
Batch 13/64 loss: 0.016724586486816406
Batch 14/64 loss: 0.029674410820007324
Batch 15/64 loss: 0.038132309913635254
Batch 16/64 loss: 0.02102804183959961
Batch 17/64 loss: 0.035659193992614746
Batch 18/64 loss: -0.004236698150634766
Batch 19/64 loss: 0.06185507774353027
Batch 20/64 loss: 0.028591036796569824
Batch 21/64 loss: 0.03357958793640137
Batch 22/64 loss: 0.01529783010482788
Batch 23/64 loss: 0.03337395191192627
Batch 24/64 loss: 0.03542417287826538
Batch 25/64 loss: 0.030420362949371338
Batch 26/64 loss: 0.028712093830108643
Batch 27/64 loss: 0.03282660245895386
Batch 28/64 loss: 0.014218568801879883
Batch 29/64 loss: 0.06041598320007324
Batch 30/64 loss: 0.05588996410369873
Batch 31/64 loss: 0.014538168907165527
Batch 32/64 loss: 0.024444401264190674
Batch 33/64 loss: 0.019592761993408203
Batch 34/64 loss: 0.052296459674835205
Batch 35/64 loss: 0.0628395676612854
Batch 36/64 loss: 0.02873528003692627
Batch 37/64 loss: 0.05463641881942749
Batch 38/64 loss: 0.028082847595214844
Batch 39/64 loss: 0.039687275886535645
Batch 40/64 loss: 0.04150962829589844
Batch 41/64 loss: 0.011199593544006348
Batch 42/64 loss: 0.024084627628326416
Batch 43/64 loss: 0.04808449745178223
Batch 44/64 loss: 0.039878904819488525
Batch 45/64 loss: 0.017220616340637207
Batch 46/64 loss: 0.03989124298095703
Batch 47/64 loss: 0.05503404140472412
Batch 48/64 loss: 0.006169915199279785
Batch 49/64 loss: 0.030859768390655518
Batch 50/64 loss: 0.024039387702941895
Batch 51/64 loss: 0.009380161762237549
Batch 52/64 loss: 0.04206395149230957
Batch 53/64 loss: 0.04897814989089966
Batch 54/64 loss: 0.03215247392654419
Batch 55/64 loss: 0.021679222583770752
Batch 56/64 loss: 0.06075388193130493
Batch 57/64 loss: 0.029299378395080566
Batch 58/64 loss: 0.01945674419403076
Batch 59/64 loss: 0.06812340021133423
Batch 60/64 loss: 0.04140204191207886
Batch 61/64 loss: 0.04464179277420044
Batch 62/64 loss: 0.06277680397033691
Batch 63/64 loss: 0.04365718364715576
Batch 64/64 loss: 0.06757330894470215
Epoch 327  Train loss: 0.035228852664723115  Val loss: 0.11636775404317272
Epoch 328
-------------------------------
Batch 1/64 loss: 0.02957737445831299
Batch 2/64 loss: 0.027964532375335693
Batch 3/64 loss: 0.024122416973114014
Batch 4/64 loss: 0.04761052131652832
Batch 5/64 loss: 0.0133514404296875
Batch 6/64 loss: 0.0263671875
Batch 7/64 loss: 0.02861475944519043
Batch 8/64 loss: 0.024014711380004883
Batch 9/64 loss: 0.044497787952423096
Batch 10/64 loss: 0.02450859546661377
Batch 11/64 loss: 0.04845130443572998
Batch 12/64 loss: 0.0279807448387146
Batch 13/64 loss: 0.040527403354644775
Batch 14/64 loss: 0.05671769380569458
Batch 15/64 loss: 0.0268632173538208
Batch 16/64 loss: 0.04863870143890381
Batch 17/64 loss: 0.038064777851104736
Batch 18/64 loss: 0.05221402645111084
Batch 19/64 loss: 0.03334099054336548
Batch 20/64 loss: 0.05008256435394287
Batch 21/64 loss: 0.025596141815185547
Batch 22/64 loss: 0.004703879356384277
Batch 23/64 loss: 0.02919185161590576
Batch 24/64 loss: 0.040230631828308105
Batch 25/64 loss: 0.04667085409164429
Batch 26/64 loss: -0.0006315708160400391
Batch 27/64 loss: 0.054944753646850586
Batch 28/64 loss: 0.04298323392868042
Batch 29/64 loss: 0.041776061058044434
Batch 30/64 loss: 0.057068824768066406
Batch 31/64 loss: 0.01299220323562622
Batch 32/64 loss: 0.025048375129699707
Batch 33/64 loss: 0.038654983043670654
Batch 34/64 loss: 0.05281335115432739
Batch 35/64 loss: 0.03252679109573364
Batch 36/64 loss: 0.05705493688583374
Batch 37/64 loss: 0.02920055389404297
Batch 38/64 loss: 0.06765413284301758
Batch 39/64 loss: 0.04399847984313965
Batch 40/64 loss: 0.025229156017303467
Batch 41/64 loss: 0.03835248947143555
Batch 42/64 loss: 0.027619481086730957
Batch 43/64 loss: 0.03346860408782959
Batch 44/64 loss: 0.06517863273620605
Batch 45/64 loss: 0.006552994251251221
Batch 46/64 loss: 0.025692522525787354
Batch 47/64 loss: 0.03482663631439209
Batch 48/64 loss: 0.056156158447265625
Batch 49/64 loss: -0.0013507604598999023
Batch 50/64 loss: 0.0482945442199707
Batch 51/64 loss: 0.05514723062515259
Batch 52/64 loss: 0.04147154092788696
Batch 53/64 loss: 0.03838068246841431
Batch 54/64 loss: 0.010429799556732178
Batch 55/64 loss: 0.04721546173095703
Batch 56/64 loss: 0.06082797050476074
Batch 57/64 loss: 0.040975332260131836
Batch 58/64 loss: 0.02798449993133545
Batch 59/64 loss: 0.022045671939849854
Batch 60/64 loss: 0.027950644493103027
Batch 61/64 loss: 0.016589701175689697
Batch 62/64 loss: 0.020046591758728027
Batch 63/64 loss: 0.060146868228912354
Batch 64/64 loss: 0.054864704608917236
Epoch 328  Train loss: 0.035864630633709475  Val loss: 0.12155673409655332
Epoch 329
-------------------------------
Batch 1/64 loss: 0.03923344612121582
Batch 2/64 loss: 0.0531926155090332
Batch 3/64 loss: 0.029181957244873047
Batch 4/64 loss: 0.05536985397338867
Batch 5/64 loss: 0.028443872928619385
Batch 6/64 loss: 0.03093898296356201
Batch 7/64 loss: 0.0037142038345336914
Batch 8/64 loss: 0.036607325077056885
Batch 9/64 loss: 0.004963040351867676
Batch 10/64 loss: 0.026877522468566895
Batch 11/64 loss: 0.015894412994384766
Batch 12/64 loss: 0.03385120630264282
Batch 13/64 loss: 0.024924635887145996
Batch 14/64 loss: 0.020691394805908203
Batch 15/64 loss: 0.01451641321182251
Batch 16/64 loss: 0.03409397602081299
Batch 17/64 loss: 0.03720104694366455
Batch 18/64 loss: 0.037586092948913574
Batch 19/64 loss: 0.05197453498840332
Batch 20/64 loss: 0.0413740873336792
Batch 21/64 loss: 0.0402681827545166
Batch 22/64 loss: 0.049466073513031006
Batch 23/64 loss: 0.010155916213989258
Batch 24/64 loss: 0.024632513523101807
Batch 25/64 loss: 0.05898481607437134
Batch 26/64 loss: 0.032082319259643555
Batch 27/64 loss: 0.021033287048339844
Batch 28/64 loss: 0.05149972438812256
Batch 29/64 loss: 0.040996551513671875
Batch 30/64 loss: 0.04404574632644653
Batch 31/64 loss: 0.05856764316558838
Batch 32/64 loss: 0.05218315124511719
Batch 33/64 loss: 0.04785984754562378
Batch 34/64 loss: 0.04333186149597168
Batch 35/64 loss: 0.03772765398025513
Batch 36/64 loss: 0.06270503997802734
Batch 37/64 loss: 0.0433734655380249
Batch 38/64 loss: 0.058324337005615234
Batch 39/64 loss: 0.028326749801635742
Batch 40/64 loss: 0.0026540756225585938
Batch 41/64 loss: 0.048649489879608154
Batch 42/64 loss: 0.028422772884368896
Batch 43/64 loss: 0.02613699436187744
Batch 44/64 loss: 0.04613196849822998
Batch 45/64 loss: 0.027499139308929443
Batch 46/64 loss: 0.03509742021560669
Batch 47/64 loss: 0.029011905193328857
Batch 48/64 loss: 0.07274043560028076
Batch 49/64 loss: 0.022952616214752197
Batch 50/64 loss: 0.01601707935333252
Batch 51/64 loss: 0.034113407135009766
Batch 52/64 loss: 0.019109606742858887
Batch 53/64 loss: 0.013083338737487793
Batch 54/64 loss: 0.030646085739135742
Batch 55/64 loss: 0.03193563222885132
Batch 56/64 loss: 0.031775712966918945
Batch 57/64 loss: 0.037592411041259766
Batch 58/64 loss: 0.037599802017211914
Batch 59/64 loss: 0.027223527431488037
Batch 60/64 loss: 0.019985854625701904
Batch 61/64 loss: 0.01743340492248535
Batch 62/64 loss: 0.01595383882522583
Batch 63/64 loss: 0.03540027141571045
Batch 64/64 loss: 0.04189974069595337
Epoch 329  Train loss: 0.03395744468651566  Val loss: 0.11979763409526077
Epoch 330
-------------------------------
Batch 1/64 loss: 0.05011683702468872
Batch 2/64 loss: 0.04462409019470215
Batch 3/64 loss: -0.0011669397354125977
Batch 4/64 loss: 0.06886869668960571
Batch 5/64 loss: 0.03118187189102173
Batch 6/64 loss: 0.05045205354690552
Batch 7/64 loss: 0.024190187454223633
Batch 8/64 loss: 0.043438076972961426
Batch 9/64 loss: 0.04043567180633545
Batch 10/64 loss: 0.01996147632598877
Batch 11/64 loss: 0.03793531656265259
Batch 12/64 loss: 0.028347015380859375
Batch 13/64 loss: 0.026181340217590332
Batch 14/64 loss: 0.011513173580169678
Batch 15/64 loss: 0.018874645233154297
Batch 16/64 loss: 0.03018176555633545
Batch 17/64 loss: 0.019680261611938477
Batch 18/64 loss: 0.03381723165512085
Batch 19/64 loss: 0.029429197311401367
Batch 20/64 loss: 0.05040806531906128
Batch 21/64 loss: 0.034259796142578125
Batch 22/64 loss: 0.031815528869628906
Batch 23/64 loss: 0.048329710960388184
Batch 24/64 loss: 0.054313600063323975
Batch 25/64 loss: 0.020616114139556885
Batch 26/64 loss: 0.022807776927947998
Batch 27/64 loss: 0.03559607267379761
Batch 28/64 loss: 0.022453904151916504
Batch 29/64 loss: 0.035376906394958496
Batch 30/64 loss: 0.02487492561340332
Batch 31/64 loss: 0.037962377071380615
Batch 32/64 loss: 0.035288453102111816
Batch 33/64 loss: 0.0635024905204773
Batch 34/64 loss: 0.0309603214263916
Batch 35/64 loss: 0.07983356714248657
Batch 36/64 loss: 0.0599215030670166
Batch 37/64 loss: 0.01693880558013916
Batch 38/64 loss: 0.037250638008117676
Batch 39/64 loss: 0.029508590698242188
Batch 40/64 loss: 0.02243494987487793
Batch 41/64 loss: 0.01765209436416626
Batch 42/64 loss: 0.022087931632995605
Batch 43/64 loss: 0.053914666175842285
Batch 44/64 loss: 0.025152981281280518
Batch 45/64 loss: 0.04754948616027832
Batch 46/64 loss: 0.04445993900299072
Batch 47/64 loss: 0.019432127475738525
Batch 48/64 loss: 0.02209794521331787
Batch 49/64 loss: 0.029834091663360596
Batch 50/64 loss: 0.03381067514419556
Batch 51/64 loss: 0.046690404415130615
Batch 52/64 loss: 0.012119114398956299
Batch 53/64 loss: 0.03497272729873657
Batch 54/64 loss: 0.032289206981658936
Batch 55/64 loss: 0.039157748222351074
Batch 56/64 loss: 0.032170236110687256
Batch 57/64 loss: 0.04962170124053955
Batch 58/64 loss: 0.03441077470779419
Batch 59/64 loss: 0.043803632259368896
Batch 60/64 loss: 0.0447993278503418
Batch 61/64 loss: 0.005130887031555176
Batch 62/64 loss: 0.02453458309173584
Batch 63/64 loss: 0.012986958026885986
Batch 64/64 loss: 0.031432926654815674
Epoch 330  Train loss: 0.033800313753240246  Val loss: 0.1176361478481096
Epoch 331
-------------------------------
Batch 1/64 loss: 0.048658132553100586
Batch 2/64 loss: 0.01538175344467163
Batch 3/64 loss: 0.031484782695770264
Batch 4/64 loss: 0.04407763481140137
Batch 5/64 loss: 0.057074129581451416
Batch 6/64 loss: 0.046797096729278564
Batch 7/64 loss: 0.020434856414794922
Batch 8/64 loss: 0.0031540989875793457
Batch 9/64 loss: 0.0510064959526062
Batch 10/64 loss: 0.04247939586639404
Batch 11/64 loss: 0.05029940605163574
Batch 12/64 loss: 0.013441085815429688
Batch 13/64 loss: 0.03477120399475098
Batch 14/64 loss: 0.028177618980407715
Batch 15/64 loss: 0.04022216796875
Batch 16/64 loss: 0.03963661193847656
Batch 17/64 loss: 0.05008453130722046
Batch 18/64 loss: 0.03137713670730591
Batch 19/64 loss: 0.030640244483947754
Batch 20/64 loss: 0.03258782625198364
Batch 21/64 loss: 0.012152612209320068
Batch 22/64 loss: 0.0421907901763916
Batch 23/64 loss: 0.05694323778152466
Batch 24/64 loss: 0.025557756423950195
Batch 25/64 loss: 0.002026379108428955
Batch 26/64 loss: 0.03353917598724365
Batch 27/64 loss: 0.03031015396118164
Batch 28/64 loss: 0.011998116970062256
Batch 29/64 loss: 0.006492197513580322
Batch 30/64 loss: 0.05584782361984253
Batch 31/64 loss: 0.018172740936279297
Batch 32/64 loss: 0.036418616771698
Batch 33/64 loss: 0.010908901691436768
Batch 34/64 loss: 0.04283320903778076
Batch 35/64 loss: 0.060025572776794434
Batch 36/64 loss: 0.025122642517089844
Batch 37/64 loss: 0.027006030082702637
Batch 38/64 loss: 0.03830999135971069
Batch 39/64 loss: 0.008724451065063477
Batch 40/64 loss: 0.032643139362335205
Batch 41/64 loss: 0.0033302903175354004
Batch 42/64 loss: 0.05943894386291504
Batch 43/64 loss: 0.03047466278076172
Batch 44/64 loss: 0.03559064865112305
Batch 45/64 loss: 0.055426716804504395
Batch 46/64 loss: 0.051691532135009766
Batch 47/64 loss: 0.03203541040420532
Batch 48/64 loss: 0.03180569410324097
Batch 49/64 loss: 0.05231189727783203
Batch 50/64 loss: 0.03635889291763306
Batch 51/64 loss: 0.018884897232055664
Batch 52/64 loss: 0.04998981952667236
Batch 53/64 loss: 0.03975015878677368
Batch 54/64 loss: 0.04285621643066406
Batch 55/64 loss: 0.021102070808410645
Batch 56/64 loss: 0.040293753147125244
Batch 57/64 loss: 0.034465670585632324
Batch 58/64 loss: 0.013645410537719727
Batch 59/64 loss: 0.039168596267700195
Batch 60/64 loss: 0.024236738681793213
Batch 61/64 loss: 0.04932183027267456
Batch 62/64 loss: 0.06271791458129883
Batch 63/64 loss: 0.03130924701690674
Batch 64/64 loss: 0.02011275291442871
Epoch 331  Train loss: 0.033887079650280524  Val loss: 0.11935435844860535
Epoch 332
-------------------------------
Batch 1/64 loss: 0.04928696155548096
Batch 2/64 loss: 0.040276169776916504
Batch 3/64 loss: 0.04477304220199585
Batch 4/64 loss: 0.038443922996520996
Batch 5/64 loss: 0.02113521099090576
Batch 6/64 loss: 0.03766077756881714
Batch 7/64 loss: 0.02385580539703369
Batch 8/64 loss: 0.07297420501708984
Batch 9/64 loss: 0.06144297122955322
Batch 10/64 loss: 0.03426837921142578
Batch 11/64 loss: 0.018418550491333008
Batch 12/64 loss: 0.014095604419708252
Batch 13/64 loss: 0.044860005378723145
Batch 14/64 loss: 0.027895629405975342
Batch 15/64 loss: 0.03179425001144409
Batch 16/64 loss: 0.04845726490020752
Batch 17/64 loss: 0.04079514741897583
Batch 18/64 loss: 0.02016228437423706
Batch 19/64 loss: 0.04261535406112671
Batch 20/64 loss: 0.009054601192474365
Batch 21/64 loss: 0.04947638511657715
Batch 22/64 loss: 0.026363253593444824
Batch 23/64 loss: 0.0669739842414856
Batch 24/64 loss: 0.022910594940185547
Batch 25/64 loss: 0.0017544031143188477
Batch 26/64 loss: 0.008496224880218506
Batch 27/64 loss: 0.01479637622833252
Batch 28/64 loss: 0.01699376106262207
Batch 29/64 loss: 0.03408926725387573
Batch 30/64 loss: 0.045329391956329346
Batch 31/64 loss: 0.025160551071166992
Batch 32/64 loss: 0.019408881664276123
Batch 33/64 loss: 0.037339746952056885
Batch 34/64 loss: 0.022077560424804688
Batch 35/64 loss: 0.0328405499458313
Batch 36/64 loss: 0.03997337818145752
Batch 37/64 loss: 0.03086906671524048
Batch 38/64 loss: 0.052895963191986084
Batch 39/64 loss: 0.02576392889022827
Batch 40/64 loss: 0.05152273178100586
Batch 41/64 loss: 0.026583611965179443
Batch 42/64 loss: 0.039110004901885986
Batch 43/64 loss: 0.049399614334106445
Batch 44/64 loss: 0.04761773347854614
Batch 45/64 loss: 0.04521298408508301
Batch 46/64 loss: 0.029688358306884766
Batch 47/64 loss: 0.04980814456939697
Batch 48/64 loss: 0.0593494176864624
Batch 49/64 loss: 0.01776212453842163
Batch 50/64 loss: 0.013942480087280273
Batch 51/64 loss: 0.027607381343841553
Batch 52/64 loss: 0.0250704288482666
Batch 53/64 loss: 0.044132232666015625
Batch 54/64 loss: 0.020670950412750244
Batch 55/64 loss: 0.004520833492279053
Batch 56/64 loss: 0.05084121227264404
Batch 57/64 loss: 0.03156149387359619
Batch 58/64 loss: 0.04314529895782471
Batch 59/64 loss: 0.051335155963897705
Batch 60/64 loss: 0.01063162088394165
Batch 61/64 loss: 0.030507326126098633
Batch 62/64 loss: 0.051500558853149414
Batch 63/64 loss: 0.026084423065185547
Batch 64/64 loss: -0.006107151508331299
Epoch 332  Train loss: 0.033549884955088295  Val loss: 0.11688250590026174
Epoch 333
-------------------------------
Batch 1/64 loss: 0.021406888961791992
Batch 2/64 loss: 0.031349778175354004
Batch 3/64 loss: 0.010218322277069092
Batch 4/64 loss: -0.0010856986045837402
Batch 5/64 loss: 0.042600274085998535
Batch 6/64 loss: 0.05358928442001343
Batch 7/64 loss: 0.028901398181915283
Batch 8/64 loss: 0.022265195846557617
Batch 9/64 loss: 0.03596073389053345
Batch 10/64 loss: 0.03694462776184082
Batch 11/64 loss: 0.04905909299850464
Batch 12/64 loss: 0.040913403034210205
Batch 13/64 loss: -0.0005897283554077148
Batch 14/64 loss: 0.020962834358215332
Batch 15/64 loss: 0.034310877323150635
Batch 16/64 loss: 0.026275336742401123
Batch 17/64 loss: 0.01852285861968994
Batch 18/64 loss: 0.021426498889923096
Batch 19/64 loss: 0.02474379539489746
Batch 20/64 loss: 0.0480840802192688
Batch 21/64 loss: 0.06415969133377075
Batch 22/64 loss: 0.04767751693725586
Batch 23/64 loss: 0.01945704221725464
Batch 24/64 loss: 0.03557252883911133
Batch 25/64 loss: 0.04639315605163574
Batch 26/64 loss: 0.018976151943206787
Batch 27/64 loss: 0.0319744348526001
Batch 28/64 loss: 0.05368095636367798
Batch 29/64 loss: 0.05567049980163574
Batch 30/64 loss: 0.031207680702209473
Batch 31/64 loss: 0.04616856575012207
Batch 32/64 loss: 0.018917441368103027
Batch 33/64 loss: 0.023358643054962158
Batch 34/64 loss: 0.04348564147949219
Batch 35/64 loss: 0.02426159381866455
Batch 36/64 loss: 0.0328444242477417
Batch 37/64 loss: 0.006379365921020508
Batch 38/64 loss: 0.017249226570129395
Batch 39/64 loss: 0.003237426280975342
Batch 40/64 loss: 0.03859430551528931
Batch 41/64 loss: 0.03278559446334839
Batch 42/64 loss: 0.037460148334503174
Batch 43/64 loss: 0.043275296688079834
Batch 44/64 loss: 0.0214921236038208
Batch 45/64 loss: 0.023185372352600098
Batch 46/64 loss: 0.0325663685798645
Batch 47/64 loss: 0.023620903491973877
Batch 48/64 loss: 0.07807362079620361
Batch 49/64 loss: 0.03211867809295654
Batch 50/64 loss: 0.052742183208465576
Batch 51/64 loss: 0.03796517848968506
Batch 52/64 loss: 0.04290825128555298
Batch 53/64 loss: 0.054265737533569336
Batch 54/64 loss: 0.03978246450424194
Batch 55/64 loss: 0.03424084186553955
Batch 56/64 loss: 0.03370636701583862
Batch 57/64 loss: 0.02141547203063965
Batch 58/64 loss: 0.05282723903656006
Batch 59/64 loss: 0.06405794620513916
Batch 60/64 loss: 0.030901312828063965
Batch 61/64 loss: 0.025308072566986084
Batch 62/64 loss: 0.033857882022857666
Batch 63/64 loss: 0.01761537790298462
Batch 64/64 loss: 0.035157859325408936
Epoch 333  Train loss: 0.03321830548492132  Val loss: 0.118404312232106
Epoch 334
-------------------------------
Batch 1/64 loss: 0.03937816619873047
Batch 2/64 loss: 0.01043921709060669
Batch 3/64 loss: 0.009639918804168701
Batch 4/64 loss: 0.009360790252685547
Batch 5/64 loss: 0.04330003261566162
Batch 6/64 loss: 0.012058377265930176
Batch 7/64 loss: 0.0556032657623291
Batch 8/64 loss: -0.007011532783508301
Batch 9/64 loss: 0.023610472679138184
Batch 10/64 loss: 0.014547348022460938
Batch 11/64 loss: 0.04168903827667236
Batch 12/64 loss: 0.011853337287902832
Batch 13/64 loss: 0.00602567195892334
Batch 14/64 loss: 0.01766151189804077
Batch 15/64 loss: 0.005904495716094971
Batch 16/64 loss: 0.03528773784637451
Batch 17/64 loss: 0.0190964937210083
Batch 18/64 loss: 0.04316133260726929
Batch 19/64 loss: 0.039569079875946045
Batch 20/64 loss: 0.033122897148132324
Batch 21/64 loss: 0.028491616249084473
Batch 22/64 loss: 0.03983408212661743
Batch 23/64 loss: 0.04698371887207031
Batch 24/64 loss: 0.01393038034439087
Batch 25/64 loss: 0.05010020732879639
Batch 26/64 loss: 0.0374143123626709
Batch 27/64 loss: 0.02462005615234375
Batch 28/64 loss: 0.042481422424316406
Batch 29/64 loss: 0.026453018188476562
Batch 30/64 loss: 0.02595454454421997
Batch 31/64 loss: 0.024381279945373535
Batch 32/64 loss: 0.0015527606010437012
Batch 33/64 loss: 0.010661721229553223
Batch 34/64 loss: 0.03274267911911011
Batch 35/64 loss: 0.02528160810470581
Batch 36/64 loss: 0.02745354175567627
Batch 37/64 loss: 0.020866215229034424
Batch 38/64 loss: 0.020750880241394043
Batch 39/64 loss: 0.03360944986343384
Batch 40/64 loss: 0.03612309694290161
Batch 41/64 loss: 0.022091984748840332
Batch 42/64 loss: 0.030725181102752686
Batch 43/64 loss: 0.04949396848678589
Batch 44/64 loss: 0.045644164085388184
Batch 45/64 loss: 0.030298173427581787
Batch 46/64 loss: 0.052929699420928955
Batch 47/64 loss: 0.026686549186706543
Batch 48/64 loss: 0.02260112762451172
Batch 49/64 loss: 0.03809559345245361
Batch 50/64 loss: 0.09037762880325317
Batch 51/64 loss: 0.06368076801300049
Batch 52/64 loss: 0.05326348543167114
Batch 53/64 loss: 0.02644401788711548
Batch 54/64 loss: 0.027986109256744385
Batch 55/64 loss: 0.03675639629364014
Batch 56/64 loss: 0.0478249192237854
Batch 57/64 loss: 0.04157364368438721
Batch 58/64 loss: 0.057057738304138184
Batch 59/64 loss: 0.036621689796447754
Batch 60/64 loss: 0.060302019119262695
Batch 61/64 loss: 0.04101288318634033
Batch 62/64 loss: 0.06431859731674194
Batch 63/64 loss: 0.06532430648803711
Batch 64/64 loss: 0.058457374572753906
Epoch 334  Train loss: 0.03308137912376254  Val loss: 0.12099328524468281
Epoch 335
-------------------------------
Batch 1/64 loss: 0.058975934982299805
Batch 2/64 loss: 0.01150137186050415
Batch 3/64 loss: 0.03534287214279175
Batch 4/64 loss: 0.03066498041152954
Batch 5/64 loss: 0.04352915287017822
Batch 6/64 loss: 0.031751036643981934
Batch 7/64 loss: 0.04522162675857544
Batch 8/64 loss: 0.03927832841873169
Batch 9/64 loss: 0.04172635078430176
Batch 10/64 loss: 0.012716889381408691
Batch 11/64 loss: 0.05254238843917847
Batch 12/64 loss: 0.004915177822113037
Batch 13/64 loss: 0.038372159004211426
Batch 14/64 loss: 0.040638864040374756
Batch 15/64 loss: 0.03423190116882324
Batch 16/64 loss: 0.040263235569000244
Batch 17/64 loss: 0.005864500999450684
Batch 18/64 loss: 0.012581050395965576
Batch 19/64 loss: 0.0014011859893798828
Batch 20/64 loss: 0.050035834312438965
Batch 21/64 loss: 0.03751540184020996
Batch 22/64 loss: 0.022603929042816162
Batch 23/64 loss: 0.018863677978515625
Batch 24/64 loss: 0.02563798427581787
Batch 25/64 loss: 0.044178545475006104
Batch 26/64 loss: 0.02051156759262085
Batch 27/64 loss: 0.0473630428314209
Batch 28/64 loss: 0.02221512794494629
Batch 29/64 loss: 0.03080165386199951
Batch 30/64 loss: 0.013321280479431152
Batch 31/64 loss: 0.0015197992324829102
Batch 32/64 loss: 0.039188265800476074
Batch 33/64 loss: 0.04181349277496338
Batch 34/64 loss: 0.0417017936706543
Batch 35/64 loss: 0.027509033679962158
Batch 36/64 loss: 0.05335962772369385
Batch 37/64 loss: 0.024065375328063965
Batch 38/64 loss: 0.029915928840637207
Batch 39/64 loss: 0.016788899898529053
Batch 40/64 loss: 0.04522073268890381
Batch 41/64 loss: 0.02363365888595581
Batch 42/64 loss: 0.027908027172088623
Batch 43/64 loss: 0.022988975048065186
Batch 44/64 loss: 0.02457284927368164
Batch 45/64 loss: 0.033271610736846924
Batch 46/64 loss: 0.026736557483673096
Batch 47/64 loss: 0.04431557655334473
Batch 48/64 loss: 0.0502549409866333
Batch 49/64 loss: 0.03930562734603882
Batch 50/64 loss: 0.032083749771118164
Batch 51/64 loss: 0.0607563853263855
Batch 52/64 loss: 0.026641666889190674
Batch 53/64 loss: 0.01783299446105957
Batch 54/64 loss: 0.012739479541778564
Batch 55/64 loss: 0.025826096534729004
Batch 56/64 loss: 0.06422698497772217
Batch 57/64 loss: 0.03846240043640137
Batch 58/64 loss: 0.05922776460647583
Batch 59/64 loss: 0.03861594200134277
Batch 60/64 loss: 0.026888906955718994
Batch 61/64 loss: 0.049305737018585205
Batch 62/64 loss: 0.052751004695892334
Batch 63/64 loss: 0.022695958614349365
Batch 64/64 loss: 0.050942063331604004
Epoch 335  Train loss: 0.03286116777681837  Val loss: 0.1167674218256449
Epoch 336
-------------------------------
Batch 1/64 loss: 0.01860356330871582
Batch 2/64 loss: 0.046732425689697266
Batch 3/64 loss: 0.04276394844055176
Batch 4/64 loss: 0.03027886152267456
Batch 5/64 loss: 0.035756707191467285
Batch 6/64 loss: 0.005086123943328857
Batch 7/64 loss: 0.0046808719635009766
Batch 8/64 loss: 0.023424625396728516
Batch 9/64 loss: 0.00907975435256958
Batch 10/64 loss: 0.01806551218032837
Batch 11/64 loss: 0.0198822021484375
Batch 12/64 loss: 0.0497930645942688
Batch 13/64 loss: 0.01600325107574463
Batch 14/64 loss: 0.02728581428527832
Batch 15/64 loss: 0.03466010093688965
Batch 16/64 loss: 0.02624720335006714
Batch 17/64 loss: 0.06434261798858643
Batch 18/64 loss: 0.036307334899902344
Batch 19/64 loss: 0.026639044284820557
Batch 20/64 loss: 0.059615492820739746
Batch 21/64 loss: 0.025810718536376953
Batch 22/64 loss: 0.03579282760620117
Batch 23/64 loss: 0.02249622344970703
Batch 24/64 loss: 0.026468873023986816
Batch 25/64 loss: 0.03093796968460083
Batch 26/64 loss: 0.03999662399291992
Batch 27/64 loss: 0.043001413345336914
Batch 28/64 loss: 0.024987757205963135
Batch 29/64 loss: 0.030851781368255615
Batch 30/64 loss: 0.008755743503570557
Batch 31/64 loss: 0.0036615729331970215
Batch 32/64 loss: 0.03992873430252075
Batch 33/64 loss: 0.028029561042785645
Batch 34/64 loss: 0.03754782676696777
Batch 35/64 loss: 0.060685575008392334
Batch 36/64 loss: 0.05076122283935547
Batch 37/64 loss: 0.018325984477996826
Batch 38/64 loss: 0.0456845760345459
Batch 39/64 loss: 0.033697307109832764
Batch 40/64 loss: 0.0555458664894104
Batch 41/64 loss: 0.0678977370262146
Batch 42/64 loss: 0.01929020881652832
Batch 43/64 loss: 0.021135926246643066
Batch 44/64 loss: 0.022671759128570557
Batch 45/64 loss: 0.056157827377319336
Batch 46/64 loss: 0.028651654720306396
Batch 47/64 loss: 0.0330049991607666
Batch 48/64 loss: 0.023090600967407227
Batch 49/64 loss: 0.03216862678527832
Batch 50/64 loss: 0.023049116134643555
Batch 51/64 loss: 0.0765179991722107
Batch 52/64 loss: 0.02471083402633667
Batch 53/64 loss: 0.026490867137908936
Batch 54/64 loss: 0.047889888286590576
Batch 55/64 loss: 0.040477633476257324
Batch 56/64 loss: 0.04211539030075073
Batch 57/64 loss: 0.007392168045043945
Batch 58/64 loss: 0.029920637607574463
Batch 59/64 loss: 0.04566526412963867
Batch 60/64 loss: 0.022363603115081787
Batch 61/64 loss: 0.06567740440368652
Batch 62/64 loss: 0.031049787998199463
Batch 63/64 loss: 0.03148031234741211
Batch 64/64 loss: 0.03497946262359619
Epoch 336  Train loss: 0.03299327878391042  Val loss: 0.11648590376286982
Epoch 337
-------------------------------
Batch 1/64 loss: 0.011375725269317627
Batch 2/64 loss: 0.026764392852783203
Batch 3/64 loss: 0.04079705476760864
Batch 4/64 loss: 0.028905749320983887
Batch 5/64 loss: 0.03361332416534424
Batch 6/64 loss: 0.019678950309753418
Batch 7/64 loss: 0.02947753667831421
Batch 8/64 loss: 0.007913410663604736
Batch 9/64 loss: 0.019872546195983887
Batch 10/64 loss: 0.048753976821899414
Batch 11/64 loss: 0.045916974544525146
Batch 12/64 loss: 0.03022867441177368
Batch 13/64 loss: 0.07318353652954102
Batch 14/64 loss: 0.014904677867889404
Batch 15/64 loss: 0.020292222499847412
Batch 16/64 loss: 0.04191809892654419
Batch 17/64 loss: 0.009305059909820557
Batch 18/64 loss: 0.024806976318359375
Batch 19/64 loss: 0.04716759920120239
Batch 20/64 loss: 0.033844590187072754
Batch 21/64 loss: 0.024279415607452393
Batch 22/64 loss: 0.01512610912322998
Batch 23/64 loss: 0.05462968349456787
Batch 24/64 loss: 0.011667609214782715
Batch 25/64 loss: 0.025818824768066406
Batch 26/64 loss: 0.0764608383178711
Batch 27/64 loss: 0.02182900905609131
Batch 28/64 loss: 0.02422177791595459
Batch 29/64 loss: 0.03778219223022461
Batch 30/64 loss: 0.04151719808578491
Batch 31/64 loss: 0.035605013370513916
Batch 32/64 loss: 0.042094647884368896
Batch 33/64 loss: 0.024609267711639404
Batch 34/64 loss: 0.045911431312561035
Batch 35/64 loss: 0.02492344379425049
Batch 36/64 loss: 0.03583502769470215
Batch 37/64 loss: 0.025391697883605957
Batch 38/64 loss: 0.05269104242324829
Batch 39/64 loss: 0.04059576988220215
Batch 40/64 loss: 0.004240989685058594
Batch 41/64 loss: 0.02517557144165039
Batch 42/64 loss: 0.05687999725341797
Batch 43/64 loss: 0.025249242782592773
Batch 44/64 loss: 0.02419048547744751
Batch 45/64 loss: 0.05843639373779297
Batch 46/64 loss: 0.015384256839752197
Batch 47/64 loss: 0.04044461250305176
Batch 48/64 loss: 0.038091838359832764
Batch 49/64 loss: 0.018047094345092773
Batch 50/64 loss: 0.022238075733184814
Batch 51/64 loss: 0.04925334453582764
Batch 52/64 loss: 0.030683815479278564
Batch 53/64 loss: 0.021281957626342773
Batch 54/64 loss: 0.02909761667251587
Batch 55/64 loss: 0.03823798894882202
Batch 56/64 loss: 0.02021312713623047
Batch 57/64 loss: 0.028683841228485107
Batch 58/64 loss: 0.04052501916885376
Batch 59/64 loss: 0.0689353346824646
Batch 60/64 loss: 0.053215980529785156
Batch 61/64 loss: 0.03808915615081787
Batch 62/64 loss: 0.034081220626831055
Batch 63/64 loss: 0.041191935539245605
Batch 64/64 loss: 0.024454474449157715
Epoch 337  Train loss: 0.03309679732603185  Val loss: 0.11844932910093327
Epoch 338
-------------------------------
Batch 1/64 loss: 0.04653090238571167
Batch 2/64 loss: 0.015683650970458984
Batch 3/64 loss: 0.013192892074584961
Batch 4/64 loss: 0.05922365188598633
Batch 5/64 loss: 0.026484787464141846
Batch 6/64 loss: 0.02890676259994507
Batch 7/64 loss: 0.028826236724853516
Batch 8/64 loss: 0.013213038444519043
Batch 9/64 loss: 0.016394615173339844
Batch 10/64 loss: 0.03889697790145874
Batch 11/64 loss: 0.004511773586273193
Batch 12/64 loss: 0.023943424224853516
Batch 13/64 loss: 0.019154250621795654
Batch 14/64 loss: 0.03269010782241821
Batch 15/64 loss: 0.026546239852905273
Batch 16/64 loss: 0.04801642894744873
Batch 17/64 loss: 0.0003867149353027344
Batch 18/64 loss: 0.04534351825714111
Batch 19/64 loss: 0.039298415184020996
Batch 20/64 loss: 0.06258159875869751
Batch 21/64 loss: 0.009229779243469238
Batch 22/64 loss: 0.03863316774368286
Batch 23/64 loss: 0.02105998992919922
Batch 24/64 loss: 0.047925472259521484
Batch 25/64 loss: 0.03126150369644165
Batch 26/64 loss: 0.028500497341156006
Batch 27/64 loss: 0.05804342031478882
Batch 28/64 loss: 0.03831082582473755
Batch 29/64 loss: 0.046409010887145996
Batch 30/64 loss: 0.016957461833953857
Batch 31/64 loss: 0.027544021606445312
Batch 32/64 loss: 0.03439176082611084
Batch 33/64 loss: 0.021889150142669678
Batch 34/64 loss: 0.04504597187042236
Batch 35/64 loss: 0.04120689630508423
Batch 36/64 loss: 0.03752028942108154
Batch 37/64 loss: 0.028609752655029297
Batch 38/64 loss: 0.04929661750793457
Batch 39/64 loss: 0.022907793521881104
Batch 40/64 loss: 0.004918575286865234
Batch 41/64 loss: 0.025529325008392334
Batch 42/64 loss: 0.035657405853271484
Batch 43/64 loss: 0.044884324073791504
Batch 44/64 loss: 0.007738471031188965
Batch 45/64 loss: 0.038157880306243896
Batch 46/64 loss: 0.034293413162231445
Batch 47/64 loss: 0.028398513793945312
Batch 48/64 loss: 0.042841553688049316
Batch 49/64 loss: 0.05573248863220215
Batch 50/64 loss: 0.023936033248901367
Batch 51/64 loss: 0.04734319448471069
Batch 52/64 loss: 0.0684821605682373
Batch 53/64 loss: 0.014048457145690918
Batch 54/64 loss: 0.06984013319015503
Batch 55/64 loss: 0.06242406368255615
Batch 56/64 loss: 0.03116828203201294
Batch 57/64 loss: 0.06966036558151245
Batch 58/64 loss: 0.056776225566864014
Batch 59/64 loss: 0.04544687271118164
Batch 60/64 loss: 0.016454041004180908
Batch 61/64 loss: 0.014813065528869629
Batch 62/64 loss: 0.0006332993507385254
Batch 63/64 loss: 0.01426631212234497
Batch 64/64 loss: 0.02813631296157837
Epoch 338  Train loss: 0.03308417352975584  Val loss: 0.12096338030398916
Epoch 339
-------------------------------
Batch 1/64 loss: 0.031310081481933594
Batch 2/64 loss: 0.05962258577346802
Batch 3/64 loss: 0.05741763114929199
Batch 4/64 loss: 0.022814571857452393
Batch 5/64 loss: 0.022536635398864746
Batch 6/64 loss: 0.05457758903503418
Batch 7/64 loss: 0.009657144546508789
Batch 8/64 loss: 0.01070624589920044
Batch 9/64 loss: 0.025051653385162354
Batch 10/64 loss: 0.02059835195541382
Batch 11/64 loss: 0.031152725219726562
Batch 12/64 loss: 0.0441204309463501
Batch 13/64 loss: 0.020777761936187744
Batch 14/64 loss: 0.046980082988739014
Batch 15/64 loss: 0.018385887145996094
Batch 16/64 loss: 0.015456795692443848
Batch 17/64 loss: 0.030027687549591064
Batch 18/64 loss: 0.034261882305145264
Batch 19/64 loss: 0.018032968044281006
Batch 20/64 loss: 0.06349587440490723
Batch 21/64 loss: 0.031221449375152588
Batch 22/64 loss: -0.003092348575592041
Batch 23/64 loss: 0.0351027250289917
Batch 24/64 loss: 0.05413854122161865
Batch 25/64 loss: 0.019600212574005127
Batch 26/64 loss: 0.03100752830505371
Batch 27/64 loss: 0.04177415370941162
Batch 28/64 loss: 0.03311187028884888
Batch 29/64 loss: 0.0295751690864563
Batch 30/64 loss: 0.039603471755981445
Batch 31/64 loss: 0.04480701684951782
Batch 32/64 loss: 0.03714728355407715
Batch 33/64 loss: 0.06638991832733154
Batch 34/64 loss: 0.04860711097717285
Batch 35/64 loss: 0.04849421977996826
Batch 36/64 loss: 0.026660561561584473
Batch 37/64 loss: 0.021193623542785645
Batch 38/64 loss: 0.019819438457489014
Batch 39/64 loss: 0.008527934551239014
Batch 40/64 loss: 0.03850966691970825
Batch 41/64 loss: 0.0071985721588134766
Batch 42/64 loss: 0.029679954051971436
Batch 43/64 loss: 0.04235482215881348
Batch 44/64 loss: 0.04232758283615112
Batch 45/64 loss: 0.04070115089416504
Batch 46/64 loss: 0.06461936235427856
Batch 47/64 loss: 0.01918160915374756
Batch 48/64 loss: 0.03427278995513916
Batch 49/64 loss: 0.043120741844177246
Batch 50/64 loss: 0.04214709997177124
Batch 51/64 loss: 0.06005871295928955
Batch 52/64 loss: 0.02232503890991211
Batch 53/64 loss: 0.04774951934814453
Batch 54/64 loss: 0.03363192081451416
Batch 55/64 loss: 0.02805352210998535
Batch 56/64 loss: 0.03253203630447388
Batch 57/64 loss: 0.030643999576568604
Batch 58/64 loss: 0.05156993865966797
Batch 59/64 loss: 0.04184466600418091
Batch 60/64 loss: 0.030436575412750244
Batch 61/64 loss: 0.013569831848144531
Batch 62/64 loss: 0.04113292694091797
Batch 63/64 loss: 0.009594142436981201
Batch 64/64 loss: 0.019347190856933594
Epoch 339  Train loss: 0.0334500555898629  Val loss: 0.11889951499467044
Epoch 340
-------------------------------
Batch 1/64 loss: 0.06414097547531128
Batch 2/64 loss: 0.026565611362457275
Batch 3/64 loss: 0.032396793365478516
Batch 4/64 loss: 0.028821885585784912
Batch 5/64 loss: 0.022449970245361328
Batch 6/64 loss: 0.04212111234664917
Batch 7/64 loss: 0.017351269721984863
Batch 8/64 loss: 0.030537843704223633
Batch 9/64 loss: 0.03478407859802246
Batch 10/64 loss: 0.03187131881713867
Batch 11/64 loss: 0.032589495182037354
Batch 12/64 loss: 0.026043295860290527
Batch 13/64 loss: 0.01298433542251587
Batch 14/64 loss: 0.05303138494491577
Batch 15/64 loss: 0.029958009719848633
Batch 16/64 loss: 0.009275257587432861
Batch 17/64 loss: 0.03127336502075195
Batch 18/64 loss: 0.04117941856384277
Batch 19/64 loss: 0.06083071231842041
Batch 20/64 loss: 0.03601479530334473
Batch 21/64 loss: 0.045404911041259766
Batch 22/64 loss: 0.039984822273254395
Batch 23/64 loss: 0.041645944118499756
Batch 24/64 loss: 0.026882529258728027
Batch 25/64 loss: 0.024700522422790527
Batch 26/64 loss: 0.009544074535369873
Batch 27/64 loss: 0.015583217144012451
Batch 28/64 loss: 0.031400203704833984
Batch 29/64 loss: 0.02147132158279419
Batch 30/64 loss: 0.02156352996826172
Batch 31/64 loss: 0.007848620414733887
Batch 32/64 loss: 0.03896743059158325
Batch 33/64 loss: 0.03212153911590576
Batch 34/64 loss: 0.014180004596710205
Batch 35/64 loss: 0.04195761680603027
Batch 36/64 loss: 0.0551905632019043
Batch 37/64 loss: 0.03920614719390869
Batch 38/64 loss: 0.027175426483154297
Batch 39/64 loss: 0.02012193202972412
Batch 40/64 loss: 0.019640088081359863
Batch 41/64 loss: 0.07067424058914185
Batch 42/64 loss: 0.02231842279434204
Batch 43/64 loss: 0.010168015956878662
Batch 44/64 loss: 0.0514485239982605
Batch 45/64 loss: 0.03467196226119995
Batch 46/64 loss: 0.038848280906677246
Batch 47/64 loss: 0.03204101324081421
Batch 48/64 loss: 0.045378267765045166
Batch 49/64 loss: 0.007271409034729004
Batch 50/64 loss: 0.026468932628631592
Batch 51/64 loss: 0.03793531656265259
Batch 52/64 loss: 0.021331191062927246
Batch 53/64 loss: 0.03645217418670654
Batch 54/64 loss: 0.031356096267700195
Batch 55/64 loss: 0.03606140613555908
Batch 56/64 loss: 0.049525439739227295
Batch 57/64 loss: 0.03120279312133789
Batch 58/64 loss: 0.020314574241638184
Batch 59/64 loss: 0.01467210054397583
Batch 60/64 loss: 0.025563299655914307
Batch 61/64 loss: 0.01345527172088623
Batch 62/64 loss: 0.051055967807769775
Batch 63/64 loss: 0.03865253925323486
Batch 64/64 loss: 0.04759162664413452
Epoch 340  Train loss: 0.0317078013046115  Val loss: 0.12433548624982539
Epoch 341
-------------------------------
Batch 1/64 loss: -0.0028707385063171387
Batch 2/64 loss: 0.022143959999084473
Batch 3/64 loss: 0.026466429233551025
Batch 4/64 loss: 0.047250986099243164
Batch 5/64 loss: 0.04636865854263306
Batch 6/64 loss: 0.013250768184661865
Batch 7/64 loss: 0.039181411266326904
Batch 8/64 loss: 0.03391087055206299
Batch 9/64 loss: 0.03382885456085205
Batch 10/64 loss: 0.02816838026046753
Batch 11/64 loss: 0.04838353395462036
Batch 12/64 loss: 0.02404022216796875
Batch 13/64 loss: 0.0196264386177063
Batch 14/64 loss: 0.019951462745666504
Batch 15/64 loss: 0.03718763589859009
Batch 16/64 loss: 0.0451810359954834
Batch 17/64 loss: 0.01005631685256958
Batch 18/64 loss: 0.02090895175933838
Batch 19/64 loss: 0.01642787456512451
Batch 20/64 loss: 0.008528292179107666
Batch 21/64 loss: 0.06436645984649658
Batch 22/64 loss: 0.028907179832458496
Batch 23/64 loss: 0.04816931486129761
Batch 24/64 loss: 0.022874057292938232
Batch 25/64 loss: 0.059507548809051514
Batch 26/64 loss: 0.03727853298187256
Batch 27/64 loss: 0.04464226961135864
Batch 28/64 loss: 0.044352829456329346
Batch 29/64 loss: 0.043185293674468994
Batch 30/64 loss: 0.024939775466918945
Batch 31/64 loss: 0.011414408683776855
Batch 32/64 loss: 0.012659311294555664
Batch 33/64 loss: 0.042957425117492676
Batch 34/64 loss: 0.03435647487640381
Batch 35/64 loss: 0.03786659240722656
Batch 36/64 loss: 0.05411487817764282
Batch 37/64 loss: 0.11033326387405396
Batch 38/64 loss: 0.04068779945373535
Batch 39/64 loss: 0.012768208980560303
Batch 40/64 loss: 0.024247050285339355
Batch 41/64 loss: 0.04954475164413452
Batch 42/64 loss: 0.027813076972961426
Batch 43/64 loss: 0.038544416427612305
Batch 44/64 loss: 0.044266700744628906
Batch 45/64 loss: 0.015850961208343506
Batch 46/64 loss: 0.04077804088592529
Batch 47/64 loss: 0.03424745798110962
Batch 48/64 loss: 0.06131386756896973
Batch 49/64 loss: 0.051003873348236084
Batch 50/64 loss: 0.05005311965942383
Batch 51/64 loss: 0.002904951572418213
Batch 52/64 loss: -0.00271683931350708
Batch 53/64 loss: 0.03880661725997925
Batch 54/64 loss: 0.04075866937637329
Batch 55/64 loss: 0.02057945728302002
Batch 56/64 loss: 0.0362544059753418
Batch 57/64 loss: 0.04752206802368164
Batch 58/64 loss: 0.032226622104644775
Batch 59/64 loss: 0.039631187915802
Batch 60/64 loss: 0.02484530210494995
Batch 61/64 loss: 0.008683979511260986
Batch 62/64 loss: 0.006705224514007568
Batch 63/64 loss: 0.029993832111358643
Batch 64/64 loss: 0.058677613735198975
Epoch 341  Train loss: 0.03327435255050659  Val loss: 0.12082772996417436
Epoch 342
-------------------------------
Batch 1/64 loss: 0.02433091402053833
Batch 2/64 loss: 0.0452306866645813
Batch 3/64 loss: 0.05628693103790283
Batch 4/64 loss: 0.04631680250167847
Batch 5/64 loss: 0.010145187377929688
Batch 6/64 loss: 0.013728499412536621
Batch 7/64 loss: 0.02776569128036499
Batch 8/64 loss: 0.035839617252349854
Batch 9/64 loss: 0.01814138889312744
Batch 10/64 loss: 0.038586437702178955
Batch 11/64 loss: 0.046441495418548584
Batch 12/64 loss: 0.006757020950317383
Batch 13/64 loss: 0.022902250289916992
Batch 14/64 loss: 0.04835665225982666
Batch 15/64 loss: 0.03051936626434326
Batch 16/64 loss: 0.03192579746246338
Batch 17/64 loss: 0.022217214107513428
Batch 18/64 loss: 0.02605271339416504
Batch 19/64 loss: 0.04201972484588623
Batch 20/64 loss: 0.03249228000640869
Batch 21/64 loss: 0.021494507789611816
Batch 22/64 loss: 0.04422140121459961
Batch 23/64 loss: 0.06223028898239136
Batch 24/64 loss: 0.05870300531387329
Batch 25/64 loss: 0.034406185150146484
Batch 26/64 loss: 0.010400593280792236
Batch 27/64 loss: 0.02140498161315918
Batch 28/64 loss: 0.028282642364501953
Batch 29/64 loss: 0.01192331314086914
Batch 30/64 loss: 0.046198487281799316
Batch 31/64 loss: 0.06005096435546875
Batch 32/64 loss: 0.0752529501914978
Batch 33/64 loss: 0.027462363243103027
Batch 34/64 loss: 0.034350454807281494
Batch 35/64 loss: 0.04250633716583252
Batch 36/64 loss: 0.013730764389038086
Batch 37/64 loss: 0.015682339668273926
Batch 38/64 loss: 0.023693323135375977
Batch 39/64 loss: 0.001116335391998291
Batch 40/64 loss: 0.055276691913604736
Batch 41/64 loss: 0.04321014881134033
Batch 42/64 loss: 0.020184755325317383
Batch 43/64 loss: 0.018848717212677002
Batch 44/64 loss: 0.04390960931777954
Batch 45/64 loss: 0.026818513870239258
Batch 46/64 loss: 0.05998772382736206
Batch 47/64 loss: 0.05867111682891846
Batch 48/64 loss: 0.043692708015441895
Batch 49/64 loss: 0.01606076955795288
Batch 50/64 loss: 0.04140287637710571
Batch 51/64 loss: 0.02826404571533203
Batch 52/64 loss: 0.026797890663146973
Batch 53/64 loss: 0.03647661209106445
Batch 54/64 loss: 0.03046572208404541
Batch 55/64 loss: 0.028547286987304688
Batch 56/64 loss: 0.014833986759185791
Batch 57/64 loss: 0.04005444049835205
Batch 58/64 loss: 0.042766273021698
Batch 59/64 loss: 0.02665644884109497
Batch 60/64 loss: 0.04032927751541138
Batch 61/64 loss: 0.018009960651397705
Batch 62/64 loss: 0.0762891173362732
Batch 63/64 loss: 0.010351181030273438
Batch 64/64 loss: 0.019140243530273438
Epoch 342  Train loss: 0.03327731712191712  Val loss: 0.11753994779488475
Epoch 343
-------------------------------
Batch 1/64 loss: 0.023186564445495605
Batch 2/64 loss: 0.047701120376586914
Batch 3/64 loss: 0.03185272216796875
Batch 4/64 loss: 0.02561187744140625
Batch 5/64 loss: 0.021886765956878662
Batch 6/64 loss: 0.02222365140914917
Batch 7/64 loss: 0.024863600730895996
Batch 8/64 loss: 0.006103694438934326
Batch 9/64 loss: 0.022777080535888672
Batch 10/64 loss: 0.02424168586730957
Batch 11/64 loss: 0.05171877145767212
Batch 12/64 loss: 0.01680278778076172
Batch 13/64 loss: 4.172325134277344e-07
Batch 14/64 loss: 0.04253852367401123
Batch 15/64 loss: 0.013220012187957764
Batch 16/64 loss: 0.07347321510314941
Batch 17/64 loss: 0.039647459983825684
Batch 18/64 loss: 0.036244988441467285
Batch 19/64 loss: 0.0331001877784729
Batch 20/64 loss: 0.057248055934906006
Batch 21/64 loss: 0.03028726577758789
Batch 22/64 loss: 0.03467404842376709
Batch 23/64 loss: 0.027859985828399658
Batch 24/64 loss: 0.029130399227142334
Batch 25/64 loss: 0.07029390335083008
Batch 26/64 loss: 0.05953502655029297
Batch 27/64 loss: 0.02957141399383545
Batch 28/64 loss: 0.03027743101119995
Batch 29/64 loss: -0.003201007843017578
Batch 30/64 loss: 0.010228335857391357
Batch 31/64 loss: 0.04051244258880615
Batch 32/64 loss: 0.05789768695831299
Batch 33/64 loss: 0.019553542137145996
Batch 34/64 loss: 0.002030670642852783
Batch 35/64 loss: 0.02106255292892456
Batch 36/64 loss: 0.05214768648147583
Batch 37/64 loss: 0.008579611778259277
Batch 38/64 loss: 0.02640300989151001
Batch 39/64 loss: -0.0017033219337463379
Batch 40/64 loss: 0.015015244483947754
Batch 41/64 loss: 0.03019338846206665
Batch 42/64 loss: 0.027642488479614258
Batch 43/64 loss: 0.08338087797164917
Batch 44/64 loss: 0.04520106315612793
Batch 45/64 loss: 0.011334598064422607
Batch 46/64 loss: 0.027342796325683594
Batch 47/64 loss: 0.03453326225280762
Batch 48/64 loss: 0.05179530382156372
Batch 49/64 loss: 0.016811668872833252
Batch 50/64 loss: 0.03061842918395996
Batch 51/64 loss: 0.04427659511566162
Batch 52/64 loss: 0.03479743003845215
Batch 53/64 loss: 0.02364981174468994
Batch 54/64 loss: 0.02825862169265747
Batch 55/64 loss: 0.057720184326171875
Batch 56/64 loss: 0.026215553283691406
Batch 57/64 loss: 0.03400897979736328
Batch 58/64 loss: 0.036610424518585205
Batch 59/64 loss: 0.015375733375549316
Batch 60/64 loss: 0.043936848640441895
Batch 61/64 loss: 0.02471071481704712
Batch 62/64 loss: 0.03457045555114746
Batch 63/64 loss: 0.03335130214691162
Batch 64/64 loss: 0.04636979103088379
Epoch 343  Train loss: 0.03146216448615579  Val loss: 0.1201913526787381
Epoch 344
-------------------------------
Batch 1/64 loss: 0.015486061573028564
Batch 2/64 loss: 0.028457999229431152
Batch 3/64 loss: 0.0013357996940612793
Batch 4/64 loss: 0.02036750316619873
Batch 5/64 loss: 0.024296283721923828
Batch 6/64 loss: 0.034659624099731445
Batch 7/64 loss: 0.03714168071746826
Batch 8/64 loss: 0.02260589599609375
Batch 9/64 loss: 0.02276599407196045
Batch 10/64 loss: 0.0043451786041259766
Batch 11/64 loss: 0.028891146183013916
Batch 12/64 loss: 0.029503047466278076
Batch 13/64 loss: 0.028900146484375
Batch 14/64 loss: 0.052104175090789795
Batch 15/64 loss: 0.02383941411972046
Batch 16/64 loss: 0.016697406768798828
Batch 17/64 loss: 0.024230539798736572
Batch 18/64 loss: 0.03405886888504028
Batch 19/64 loss: 0.05143392086029053
Batch 20/64 loss: 0.01563549041748047
Batch 21/64 loss: 0.015440821647644043
Batch 22/64 loss: 0.02717149257659912
Batch 23/64 loss: 0.037624895572662354
Batch 24/64 loss: 0.014803588390350342
Batch 25/64 loss: 0.010288715362548828
Batch 26/64 loss: 0.053111135959625244
Batch 27/64 loss: 0.0497477650642395
Batch 28/64 loss: 0.031184673309326172
Batch 29/64 loss: 0.022629380226135254
Batch 30/64 loss: 0.03805804252624512
Batch 31/64 loss: 0.020658552646636963
Batch 32/64 loss: 0.0366438627243042
Batch 33/64 loss: 0.01646554470062256
Batch 34/64 loss: 0.024662137031555176
Batch 35/64 loss: 0.07454204559326172
Batch 36/64 loss: 0.045407891273498535
Batch 37/64 loss: 0.04941451549530029
Batch 38/64 loss: 0.02950000762939453
Batch 39/64 loss: 0.06897598505020142
Batch 40/64 loss: 0.026404976844787598
Batch 41/64 loss: 0.029167652130126953
Batch 42/64 loss: 0.05198317766189575
Batch 43/64 loss: 0.05338728427886963
Batch 44/64 loss: 0.011943042278289795
Batch 45/64 loss: 0.028561890125274658
Batch 46/64 loss: 0.02073568105697632
Batch 47/64 loss: 0.029439032077789307
Batch 48/64 loss: 0.035822153091430664
Batch 49/64 loss: 0.04270482063293457
Batch 50/64 loss: 0.018957972526550293
Batch 51/64 loss: 0.019243955612182617
Batch 52/64 loss: 0.0063593387603759766
Batch 53/64 loss: 0.04184228181838989
Batch 54/64 loss: 0.02771127223968506
Batch 55/64 loss: 0.029702842235565186
Batch 56/64 loss: 0.05081534385681152
Batch 57/64 loss: 0.036918818950653076
Batch 58/64 loss: 0.016967952251434326
Batch 59/64 loss: 0.05232274532318115
Batch 60/64 loss: 0.04594123363494873
Batch 61/64 loss: 0.036270856857299805
Batch 62/64 loss: 0.04051011800765991
Batch 63/64 loss: 0.03581172227859497
Batch 64/64 loss: 0.037844061851501465
Epoch 344  Train loss: 0.0313881794611613  Val loss: 0.11884037387330097
Epoch 345
-------------------------------
Batch 1/64 loss: 0.038118720054626465
Batch 2/64 loss: 0.036579787731170654
Batch 3/64 loss: 0.02556532621383667
Batch 4/64 loss: 0.02985483407974243
Batch 5/64 loss: 0.029177963733673096
Batch 6/64 loss: 0.06133580207824707
Batch 7/64 loss: 0.019563674926757812
Batch 8/64 loss: -0.004211902618408203
Batch 9/64 loss: 0.022256791591644287
Batch 10/64 loss: 0.05926871299743652
Batch 11/64 loss: 0.016366243362426758
Batch 12/64 loss: 0.00984346866607666
Batch 13/64 loss: 0.03154259920120239
Batch 14/64 loss: 0.0319516658782959
Batch 15/64 loss: 0.023914575576782227
Batch 16/64 loss: 0.031359076499938965
Batch 17/64 loss: 0.01113879680633545
Batch 18/64 loss: 0.04846060276031494
Batch 19/64 loss: 0.05778020620346069
Batch 20/64 loss: 0.03853201866149902
Batch 21/64 loss: 0.04119628667831421
Batch 22/64 loss: 0.019593894481658936
Batch 23/64 loss: 0.03377234935760498
Batch 24/64 loss: 0.04165726900100708
Batch 25/64 loss: 0.0428500771522522
Batch 26/64 loss: 0.03276956081390381
Batch 27/64 loss: 0.048144757747650146
Batch 28/64 loss: -0.013009846210479736
Batch 29/64 loss: 0.009656071662902832
Batch 30/64 loss: 0.023891746997833252
Batch 31/64 loss: 0.0299491286277771
Batch 32/64 loss: 0.0396801233291626
Batch 33/64 loss: 0.034722864627838135
Batch 34/64 loss: 0.02206951379776001
Batch 35/64 loss: 0.019115686416625977
Batch 36/64 loss: 0.040009260177612305
Batch 37/64 loss: -0.006524264812469482
Batch 38/64 loss: 0.03713572025299072
Batch 39/64 loss: 0.022171497344970703
Batch 40/64 loss: 0.022179484367370605
Batch 41/64 loss: 0.03915965557098389
Batch 42/64 loss: 0.010357022285461426
Batch 43/64 loss: 0.028069674968719482
Batch 44/64 loss: 0.03921663761138916
Batch 45/64 loss: 0.04535090923309326
Batch 46/64 loss: 0.0491296648979187
Batch 47/64 loss: 0.04413491487503052
Batch 48/64 loss: 0.020937442779541016
Batch 49/64 loss: 0.05836772918701172
Batch 50/64 loss: 0.033257126808166504
Batch 51/64 loss: 0.06628602743148804
Batch 52/64 loss: 0.03217434883117676
Batch 53/64 loss: 0.04949730634689331
Batch 54/64 loss: 0.026807725429534912
Batch 55/64 loss: 0.021272659301757812
Batch 56/64 loss: 0.045426249504089355
Batch 57/64 loss: 0.06142878532409668
Batch 58/64 loss: 0.03429985046386719
Batch 59/64 loss: 0.0015755891799926758
Batch 60/64 loss: 0.03095954656600952
Batch 61/64 loss: 0.04577016830444336
Batch 62/64 loss: 0.04287838935852051
Batch 63/64 loss: 0.021620512008666992
Batch 64/64 loss: 0.02235233783721924
Epoch 345  Train loss: 0.03175172291549982  Val loss: 0.11892145605841044
Epoch 346
-------------------------------
Batch 1/64 loss: 0.028750598430633545
Batch 2/64 loss: 0.029531240463256836
Batch 3/64 loss: 0.03581118583679199
Batch 4/64 loss: 0.0047416090965271
Batch 5/64 loss: 0.032630741596221924
Batch 6/64 loss: 0.04046618938446045
Batch 7/64 loss: 0.013673543930053711
Batch 8/64 loss: 0.012038946151733398
Batch 9/64 loss: 0.04107201099395752
Batch 10/64 loss: 0.024649620056152344
Batch 11/64 loss: 0.0252150297164917
Batch 12/64 loss: 0.019566774368286133
Batch 13/64 loss: 0.029156863689422607
Batch 14/64 loss: 0.04343944787979126
Batch 15/64 loss: 0.031208515167236328
Batch 16/64 loss: 0.0545995831489563
Batch 17/64 loss: 0.04356193542480469
Batch 18/64 loss: 0.06675577163696289
Batch 19/64 loss: 0.047248005867004395
Batch 20/64 loss: 0.03969883918762207
Batch 21/64 loss: 0.011019587516784668
Batch 22/64 loss: 0.019257962703704834
Batch 23/64 loss: 0.0353546142578125
Batch 24/64 loss: 0.02513277530670166
Batch 25/64 loss: 0.018697261810302734
Batch 26/64 loss: 0.027289867401123047
Batch 27/64 loss: 0.036594390869140625
Batch 28/64 loss: 0.00936877727508545
Batch 29/64 loss: 0.03437197208404541
Batch 30/64 loss: 0.01810169219970703
Batch 31/64 loss: 0.04658693075180054
Batch 32/64 loss: 0.04052096605300903
Batch 33/64 loss: 0.03551417589187622
Batch 34/64 loss: 0.015860676765441895
Batch 35/64 loss: 0.03070056438446045
Batch 36/64 loss: 0.027954816818237305
Batch 37/64 loss: 0.030949056148529053
Batch 38/64 loss: 0.05268961191177368
Batch 39/64 loss: 0.018868982791900635
Batch 40/64 loss: 0.025182902812957764
Batch 41/64 loss: 0.061527132987976074
Batch 42/64 loss: 0.02405393123626709
Batch 43/64 loss: 0.032588303089141846
Batch 44/64 loss: 0.04173564910888672
Batch 45/64 loss: 0.014737188816070557
Batch 46/64 loss: 0.002119898796081543
Batch 47/64 loss: 0.018503189086914062
Batch 48/64 loss: 0.008374273777008057
Batch 49/64 loss: 0.018806278705596924
Batch 50/64 loss: 0.018444299697875977
Batch 51/64 loss: 0.039467692375183105
Batch 52/64 loss: 0.05455845594406128
Batch 53/64 loss: 0.010217547416687012
Batch 54/64 loss: 0.053205013275146484
Batch 55/64 loss: 0.021945416927337646
Batch 56/64 loss: 0.019703209400177002
Batch 57/64 loss: 0.02710545063018799
Batch 58/64 loss: 0.024851858615875244
Batch 59/64 loss: 0.033103764057159424
Batch 60/64 loss: 0.024518132209777832
Batch 61/64 loss: 0.05396527051925659
Batch 62/64 loss: 0.025144636631011963
Batch 63/64 loss: 0.06205993890762329
Batch 64/64 loss: 0.02064305543899536
Epoch 346  Train loss: 0.03027533900504019  Val loss: 0.11698016206833095
Epoch 347
-------------------------------
Batch 1/64 loss: 0.023929357528686523
Batch 2/64 loss: 0.02750396728515625
Batch 3/64 loss: 0.04019731283187866
Batch 4/64 loss: 0.03801858425140381
Batch 5/64 loss: 0.014293432235717773
Batch 6/64 loss: 0.02827519178390503
Batch 7/64 loss: 0.01702338457107544
Batch 8/64 loss: 0.034353673458099365
Batch 9/64 loss: -0.00046128034591674805
Batch 10/64 loss: 0.026403844356536865
Batch 11/64 loss: 0.02893984317779541
Batch 12/64 loss: 0.03307431936264038
Batch 13/64 loss: 0.022686362266540527
Batch 14/64 loss: 0.041469037532806396
Batch 15/64 loss: 0.04465341567993164
Batch 16/64 loss: 0.02913796901702881
Batch 17/64 loss: 0.03768467903137207
Batch 18/64 loss: 0.017598986625671387
Batch 19/64 loss: 0.041479647159576416
Batch 20/64 loss: 0.027485668659210205
Batch 21/64 loss: 0.019166767597198486
Batch 22/64 loss: 0.03324538469314575
Batch 23/64 loss: 0.05551326274871826
Batch 24/64 loss: 0.07421278953552246
Batch 25/64 loss: 0.017292678356170654
Batch 26/64 loss: 0.031350135803222656
Batch 27/64 loss: 0.036984801292419434
Batch 28/64 loss: 0.02481180429458618
Batch 29/64 loss: 0.008344113826751709
Batch 30/64 loss: 0.00652313232421875
Batch 31/64 loss: 0.06841921806335449
Batch 32/64 loss: 0.003948628902435303
Batch 33/64 loss: 0.04197895526885986
Batch 34/64 loss: 0.04491788148880005
Batch 35/64 loss: 0.034921884536743164
Batch 36/64 loss: 0.06028437614440918
Batch 37/64 loss: 0.034866392612457275
Batch 38/64 loss: 0.04774981737136841
Batch 39/64 loss: 0.02623283863067627
Batch 40/64 loss: 0.01976466178894043
Batch 41/64 loss: 0.02415996789932251
Batch 42/64 loss: 0.018874287605285645
Batch 43/64 loss: 0.031338632106781006
Batch 44/64 loss: 0.06585502624511719
Batch 45/64 loss: 0.0326579213142395
Batch 46/64 loss: 0.025830388069152832
Batch 47/64 loss: 0.009341418743133545
Batch 48/64 loss: 0.05983245372772217
Batch 49/64 loss: 0.027425169944763184
Batch 50/64 loss: 0.042766034603118896
Batch 51/64 loss: 0.019928932189941406
Batch 52/64 loss: 0.025941789150238037
Batch 53/64 loss: 0.04750555753707886
Batch 54/64 loss: 0.022117793560028076
Batch 55/64 loss: 0.005541503429412842
Batch 56/64 loss: 0.017522871494293213
Batch 57/64 loss: -0.0017775297164916992
Batch 58/64 loss: 0.035413265228271484
Batch 59/64 loss: 0.011226058006286621
Batch 60/64 loss: 0.04442477226257324
Batch 61/64 loss: 0.04131990671157837
Batch 62/64 loss: 0.02997756004333496
Batch 63/64 loss: 0.028980672359466553
Batch 64/64 loss: 0.03567153215408325
Epoch 347  Train loss: 0.030701725856930602  Val loss: 0.11749900083771277
Epoch 348
-------------------------------
Batch 1/64 loss: 0.03694427013397217
Batch 2/64 loss: 0.023385941982269287
Batch 3/64 loss: 0.027930796146392822
Batch 4/64 loss: 0.03304034471511841
Batch 5/64 loss: 0.035695016384124756
Batch 6/64 loss: 0.0203971266746521
Batch 7/64 loss: 0.0013560056686401367
Batch 8/64 loss: 0.05979090929031372
Batch 9/64 loss: 0.04367661476135254
Batch 10/64 loss: 0.017390966415405273
Batch 11/64 loss: 0.017669618129730225
Batch 12/64 loss: 0.04745274782180786
Batch 13/64 loss: 0.03632694482803345
Batch 14/64 loss: 0.02948099374771118
Batch 15/64 loss: 0.04608297348022461
Batch 16/64 loss: 0.022275090217590332
Batch 17/64 loss: 0.02916240692138672
Batch 18/64 loss: 0.05622369050979614
Batch 19/64 loss: 0.013935565948486328
Batch 20/64 loss: 0.02499610185623169
Batch 21/64 loss: 0.05602860450744629
Batch 22/64 loss: 0.008983850479125977
Batch 23/64 loss: 0.026426494121551514
Batch 24/64 loss: 0.04357022047042847
Batch 25/64 loss: 0.03359806537628174
Batch 26/64 loss: 0.030065417289733887
Batch 27/64 loss: 0.05632436275482178
Batch 28/64 loss: 0.05206191539764404
Batch 29/64 loss: 0.027157187461853027
Batch 30/64 loss: 0.010837376117706299
Batch 31/64 loss: 0.019569993019104004
Batch 32/64 loss: 0.057272911071777344
Batch 33/64 loss: 0.004915833473205566
Batch 34/64 loss: 0.05753225088119507
Batch 35/64 loss: 0.007681131362915039
Batch 36/64 loss: 0.030194997787475586
Batch 37/64 loss: 0.04305607080459595
Batch 38/64 loss: 0.03103506565093994
Batch 39/64 loss: 0.03726857900619507
Batch 40/64 loss: 0.028766155242919922
Batch 41/64 loss: -0.01793837547302246
Batch 42/64 loss: 0.02396845817565918
Batch 43/64 loss: 0.03456753492355347
Batch 44/64 loss: 0.047627925872802734
Batch 45/64 loss: 0.01768869161605835
Batch 46/64 loss: 0.0037661194801330566
Batch 47/64 loss: 0.03542816638946533
Batch 48/64 loss: 0.0254594087600708
Batch 49/64 loss: 0.02475982904434204
Batch 50/64 loss: 0.026714324951171875
Batch 51/64 loss: 0.02491617202758789
Batch 52/64 loss: 0.0122298002243042
Batch 53/64 loss: 0.0545540452003479
Batch 54/64 loss: 0.038030385971069336
Batch 55/64 loss: 0.023836970329284668
Batch 56/64 loss: 0.024827182292938232
Batch 57/64 loss: 0.03163713216781616
Batch 58/64 loss: 0.031880080699920654
Batch 59/64 loss: 0.021423816680908203
Batch 60/64 loss: 0.030840158462524414
Batch 61/64 loss: 0.016701340675354004
Batch 62/64 loss: 0.03921341896057129
Batch 63/64 loss: 0.03971379995346069
Batch 64/64 loss: 0.02008068561553955
Epoch 348  Train loss: 0.029999490345225616  Val loss: 0.11985491886990997
Epoch 349
-------------------------------
Batch 1/64 loss: 0.03044891357421875
Batch 2/64 loss: 0.06057476997375488
Batch 3/64 loss: 0.015109777450561523
Batch 4/64 loss: 0.021172165870666504
Batch 5/64 loss: 0.036119043827056885
Batch 6/64 loss: 0.026221036911010742
Batch 7/64 loss: 0.028922319412231445
Batch 8/64 loss: 0.04162395000457764
Batch 9/64 loss: -0.000678718090057373
Batch 10/64 loss: 0.009863317012786865
Batch 11/64 loss: 0.047665953636169434
Batch 12/64 loss: 0.00656503438949585
Batch 13/64 loss: 0.025043725967407227
Batch 14/64 loss: 0.04250073432922363
Batch 15/64 loss: 0.01011580228805542
Batch 16/64 loss: 0.03090345859527588
Batch 17/64 loss: 0.004717826843261719
Batch 18/64 loss: 0.018588662147521973
Batch 19/64 loss: 0.016385555267333984
Batch 20/64 loss: 0.007536053657531738
Batch 21/64 loss: 0.029813647270202637
Batch 22/64 loss: 0.014595627784729004
Batch 23/64 loss: 0.02448272705078125
Batch 24/64 loss: 0.0392686128616333
Batch 25/64 loss: 0.040973126888275146
Batch 26/64 loss: 0.04545581340789795
Batch 27/64 loss: 0.019774138927459717
Batch 28/64 loss: 0.03826797008514404
Batch 29/64 loss: 0.016711831092834473
Batch 30/64 loss: 0.03988289833068848
Batch 31/64 loss: 0.01980513334274292
Batch 32/64 loss: 0.047580718994140625
Batch 33/64 loss: 0.06897848844528198
Batch 34/64 loss: 0.05204606056213379
Batch 35/64 loss: 0.0034110546112060547
Batch 36/64 loss: 0.028411149978637695
Batch 37/64 loss: 0.024370551109313965
Batch 38/64 loss: 0.02114778757095337
Batch 39/64 loss: 0.03084784746170044
Batch 40/64 loss: 0.0315554141998291
Batch 41/64 loss: 0.02418375015258789
Batch 42/64 loss: 0.03290337324142456
Batch 43/64 loss: 0.040221214294433594
Batch 44/64 loss: 0.04454314708709717
Batch 45/64 loss: 0.0686531662940979
Batch 46/64 loss: 0.036699533462524414
Batch 47/64 loss: 0.039540886878967285
Batch 48/64 loss: 0.01279360055923462
Batch 49/64 loss: 0.03609555959701538
Batch 50/64 loss: 0.011620819568634033
Batch 51/64 loss: 0.06727230548858643
Batch 52/64 loss: 0.02302759885787964
Batch 53/64 loss: 0.055690646171569824
Batch 54/64 loss: 0.04021191596984863
Batch 55/64 loss: 0.06447482109069824
Batch 56/64 loss: 0.03878521919250488
Batch 57/64 loss: 0.035230398178100586
Batch 58/64 loss: 0.014217376708984375
Batch 59/64 loss: 0.056973278522491455
Batch 60/64 loss: 0.048374176025390625
Batch 61/64 loss: 0.034370243549346924
Batch 62/64 loss: 0.016384601593017578
Batch 63/64 loss: -0.0018939971923828125
Batch 64/64 loss: 0.018558502197265625
Epoch 349  Train loss: 0.030918784235038008  Val loss: 0.11598333890495431
Epoch 350
-------------------------------
Batch 1/64 loss: 0.033143818378448486
Batch 2/64 loss: 0.015597283840179443
Batch 3/64 loss: 0.049176931381225586
Batch 4/64 loss: 0.043541789054870605
Batch 5/64 loss: 0.042062997817993164
Batch 6/64 loss: 0.029471755027770996
Batch 7/64 loss: 0.025900185108184814
Batch 8/64 loss: 0.021629750728607178
Batch 9/64 loss: 0.026166796684265137
Batch 10/64 loss: 0.029375851154327393
Batch 11/64 loss: 0.04419839382171631
Batch 12/64 loss: 0.009425640106201172
Batch 13/64 loss: 0.04605227708816528
Batch 14/64 loss: 0.023836731910705566
Batch 15/64 loss: 0.030757606029510498
Batch 16/64 loss: 0.024145901203155518
Batch 17/64 loss: 0.021114647388458252
Batch 18/64 loss: 0.029148638248443604
Batch 19/64 loss: 0.0425679087638855
Batch 20/64 loss: 0.025562644004821777
Batch 21/64 loss: 0.020711958408355713
Batch 22/64 loss: 0.02977168560028076
Batch 23/64 loss: 0.025099992752075195
Batch 24/64 loss: 0.03555727005004883
Batch 25/64 loss: 0.029178261756896973
Batch 26/64 loss: 0.018522679805755615
Batch 27/64 loss: 0.03251993656158447
Batch 28/64 loss: 0.021035492420196533
Batch 29/64 loss: 0.015700697898864746
Batch 30/64 loss: 0.05281180143356323
Batch 31/64 loss: 0.09855282306671143
Batch 32/64 loss: 0.05622529983520508
Batch 33/64 loss: 0.02012413740158081
Batch 34/64 loss: 0.01192253828048706
Batch 35/64 loss: 0.04328155517578125
Batch 36/64 loss: 0.044579923152923584
Batch 37/64 loss: 0.05975449085235596
Batch 38/64 loss: 0.049199700355529785
Batch 39/64 loss: 0.02016925811767578
Batch 40/64 loss: 0.020869135856628418
Batch 41/64 loss: 0.02266162633895874
Batch 42/64 loss: 0.03992527723312378
Batch 43/64 loss: 0.026440978050231934
Batch 44/64 loss: 0.005845785140991211
Batch 45/64 loss: 0.005681455135345459
Batch 46/64 loss: 0.01509714126586914
Batch 47/64 loss: 0.02046412229537964
Batch 48/64 loss: -0.008264660835266113
Batch 49/64 loss: 0.05457526445388794
Batch 50/64 loss: 0.03392857313156128
Batch 51/64 loss: 0.0044248104095458984
Batch 52/64 loss: 0.010150015354156494
Batch 53/64 loss: 0.0356178879737854
Batch 54/64 loss: 0.02992689609527588
Batch 55/64 loss: -0.015023112297058105
Batch 56/64 loss: 0.04663366079330444
Batch 57/64 loss: 0.04184222221374512
Batch 58/64 loss: 0.058136582374572754
Batch 59/64 loss: 0.022542953491210938
Batch 60/64 loss: 0.015833258628845215
Batch 61/64 loss: -0.00445634126663208
Batch 62/64 loss: 0.027629733085632324
Batch 63/64 loss: 0.040522634983062744
Batch 64/64 loss: 0.042849838733673096
Epoch 350  Train loss: 0.02950180909212898  Val loss: 0.11898585631675326
Epoch 351
-------------------------------
Batch 1/64 loss: 0.03917360305786133
Batch 2/64 loss: 0.029201209545135498
Batch 3/64 loss: 0.006560206413269043
Batch 4/64 loss: 0.03928697109222412
Batch 5/64 loss: 0.04575967788696289
Batch 6/64 loss: 0.021509885787963867
Batch 7/64 loss: 0.015751183032989502
Batch 8/64 loss: 0.027497410774230957
Batch 9/64 loss: 0.02425438165664673
Batch 10/64 loss: 0.022137820720672607
Batch 11/64 loss: -0.004067420959472656
Batch 12/64 loss: 0.055870771408081055
Batch 13/64 loss: 0.011631548404693604
Batch 14/64 loss: 0.034403443336486816
Batch 15/64 loss: 0.058333396911621094
Batch 16/64 loss: -0.00865095853805542
Batch 17/64 loss: 0.053570449352264404
Batch 18/64 loss: 0.008973181247711182
Batch 19/64 loss: 0.029710352420806885
Batch 20/64 loss: 0.044832587242126465
Batch 21/64 loss: 0.018061459064483643
Batch 22/64 loss: 0.0067980289459228516
Batch 23/64 loss: 0.04522460699081421
Batch 24/64 loss: 0.041736900806427
Batch 25/64 loss: 0.044791579246520996
Batch 26/64 loss: 0.004510104656219482
Batch 27/64 loss: 0.03297543525695801
Batch 28/64 loss: 0.027138710021972656
Batch 29/64 loss: 0.017280399799346924
Batch 30/64 loss: 0.025890231132507324
Batch 31/64 loss: 0.001840353012084961
Batch 32/64 loss: 0.037259697914123535
Batch 33/64 loss: 0.003612518310546875
Batch 34/64 loss: 0.032604336738586426
Batch 35/64 loss: 0.021545112133026123
Batch 36/64 loss: 0.03721517324447632
Batch 37/64 loss: 0.012152671813964844
Batch 38/64 loss: 0.020303726196289062
Batch 39/64 loss: 0.04417288303375244
Batch 40/64 loss: 0.041627705097198486
Batch 41/64 loss: 0.056727051734924316
Batch 42/64 loss: 0.0499953031539917
Batch 43/64 loss: 0.023482203483581543
Batch 44/64 loss: 0.05353736877441406
Batch 45/64 loss: 0.03711855411529541
Batch 46/64 loss: 0.013588309288024902
Batch 47/64 loss: 0.03168207406997681
Batch 48/64 loss: 0.028568506240844727
Batch 49/64 loss: 0.03801846504211426
Batch 50/64 loss: 0.06030970811843872
Batch 51/64 loss: 0.02648448944091797
Batch 52/64 loss: 0.045931220054626465
Batch 53/64 loss: 0.043670654296875
Batch 54/64 loss: 0.03405410051345825
Batch 55/64 loss: 0.04250192642211914
Batch 56/64 loss: 0.030670344829559326
Batch 57/64 loss: 0.0020362138748168945
Batch 58/64 loss: 0.020600199699401855
Batch 59/64 loss: 0.030403077602386475
Batch 60/64 loss: 0.017710447311401367
Batch 61/64 loss: 0.04104381799697876
Batch 62/64 loss: 0.02932143211364746
Batch 63/64 loss: 0.0410425066947937
Batch 64/64 loss: 0.029625415802001953
Epoch 351  Train loss: 0.029697229347976983  Val loss: 0.11675130890816757
Epoch 352
-------------------------------
Batch 1/64 loss: 0.01751410961151123
Batch 2/64 loss: 0.018747448921203613
Batch 3/64 loss: 0.02417677640914917
Batch 4/64 loss: 0.058609187602996826
Batch 5/64 loss: 0.04901766777038574
Batch 6/64 loss: 0.004816412925720215
Batch 7/64 loss: 0.060559749603271484
Batch 8/64 loss: 0.013474225997924805
Batch 9/64 loss: 0.011853039264678955
Batch 10/64 loss: 0.022588491439819336
Batch 11/64 loss: 0.03454357385635376
Batch 12/64 loss: 0.03596782684326172
Batch 13/64 loss: 0.026258766651153564
Batch 14/64 loss: -0.0011789202690124512
Batch 15/64 loss: 0.03320103883743286
Batch 16/64 loss: 0.028223931789398193
Batch 17/64 loss: 0.03698384761810303
Batch 18/64 loss: 0.016996383666992188
Batch 19/64 loss: 0.03415489196777344
Batch 20/64 loss: 0.04494220018386841
Batch 21/64 loss: 0.014420628547668457
Batch 22/64 loss: 0.03736734390258789
Batch 23/64 loss: 0.010442197322845459
Batch 24/64 loss: 0.0036742091178894043
Batch 25/64 loss: 0.04692482948303223
Batch 26/64 loss: 0.03669339418411255
Batch 27/64 loss: 0.04482865333557129
Batch 28/64 loss: 0.037299275398254395
Batch 29/64 loss: 0.04459834098815918
Batch 30/64 loss: 0.06480860710144043
Batch 31/64 loss: 0.041678547859191895
Batch 32/64 loss: 0.04511618614196777
Batch 33/64 loss: 0.027447819709777832
Batch 34/64 loss: 0.03930628299713135
Batch 35/64 loss: 0.024256110191345215
Batch 36/64 loss: 0.045782506465911865
Batch 37/64 loss: 0.0160791277885437
Batch 38/64 loss: 0.029996395111083984
Batch 39/64 loss: 0.03505849838256836
Batch 40/64 loss: 0.06418013572692871
Batch 41/64 loss: 0.02759462594985962
Batch 42/64 loss: 0.012666463851928711
Batch 43/64 loss: 0.022420406341552734
Batch 44/64 loss: 0.007528901100158691
Batch 45/64 loss: 0.02191448211669922
Batch 46/64 loss: 0.04708969593048096
Batch 47/64 loss: 0.02417778968811035
Batch 48/64 loss: 0.024349629878997803
Batch 49/64 loss: 0.02703714370727539
Batch 50/64 loss: 0.05372762680053711
Batch 51/64 loss: 0.04234600067138672
Batch 52/64 loss: 0.031096577644348145
Batch 53/64 loss: 0.02330869436264038
Batch 54/64 loss: 0.005185067653656006
Batch 55/64 loss: 0.020130157470703125
Batch 56/64 loss: 0.02921980619430542
Batch 57/64 loss: 0.04189133644104004
Batch 58/64 loss: 0.015425264835357666
Batch 59/64 loss: 0.028937220573425293
Batch 60/64 loss: -0.002007603645324707
Batch 61/64 loss: 0.024538159370422363
Batch 62/64 loss: 0.027275443077087402
Batch 63/64 loss: 0.013628184795379639
Batch 64/64 loss: 0.03826028108596802
Epoch 352  Train loss: 0.02948370236976474  Val loss: 0.11756776944058868
Epoch 353
-------------------------------
Batch 1/64 loss: 0.05476897954940796
Batch 2/64 loss: 0.040831804275512695
Batch 3/64 loss: 0.009666025638580322
Batch 4/64 loss: 0.01893901824951172
Batch 5/64 loss: 0.05257076025009155
Batch 6/64 loss: 0.005744814872741699
Batch 7/64 loss: 0.031859397888183594
Batch 8/64 loss: 0.028012335300445557
Batch 9/64 loss: 0.04724842309951782
Batch 10/64 loss: 0.022371351718902588
Batch 11/64 loss: 0.010834872722625732
Batch 12/64 loss: 0.004149436950683594
Batch 13/64 loss: 0.03205376863479614
Batch 14/64 loss: 0.023588597774505615
Batch 15/64 loss: 0.021108627319335938
Batch 16/64 loss: -0.0008293390274047852
Batch 17/64 loss: 0.02764672040939331
Batch 18/64 loss: 0.004558086395263672
Batch 19/64 loss: -0.0015887618064880371
Batch 20/64 loss: 0.025775671005249023
Batch 21/64 loss: 0.03779959678649902
Batch 22/64 loss: 0.0317116379737854
Batch 23/64 loss: 0.012215137481689453
Batch 24/64 loss: 0.025883793830871582
Batch 25/64 loss: 0.04004061222076416
Batch 26/64 loss: 0.027513742446899414
Batch 27/64 loss: 0.05151951313018799
Batch 28/64 loss: 0.006790101528167725
Batch 29/64 loss: 0.023853719234466553
Batch 30/64 loss: 0.026275992393493652
Batch 31/64 loss: 0.02257084846496582
Batch 32/64 loss: 0.027203857898712158
Batch 33/64 loss: 0.026433825492858887
Batch 34/64 loss: 0.03338801860809326
Batch 35/64 loss: 0.013176798820495605
Batch 36/64 loss: 0.029986858367919922
Batch 37/64 loss: 0.06934142112731934
Batch 38/64 loss: 0.02156078815460205
Batch 39/64 loss: 0.00917881727218628
Batch 40/64 loss: 0.053473830223083496
Batch 41/64 loss: 0.0332106351852417
Batch 42/64 loss: 0.060910582542419434
Batch 43/64 loss: 0.014023184776306152
Batch 44/64 loss: 0.03983652591705322
Batch 45/64 loss: 0.03666269779205322
Batch 46/64 loss: 0.0633019208908081
Batch 47/64 loss: 0.023105978965759277
Batch 48/64 loss: 0.03899693489074707
Batch 49/64 loss: 0.019153058528900146
Batch 50/64 loss: 0.03230011463165283
Batch 51/64 loss: 0.028773605823516846
Batch 52/64 loss: 0.04277372360229492
Batch 53/64 loss: 0.05203455686569214
Batch 54/64 loss: 0.05396848917007446
Batch 55/64 loss: 0.04056060314178467
Batch 56/64 loss: 0.07455801963806152
Batch 57/64 loss: 0.02070140838623047
Batch 58/64 loss: 0.023480534553527832
Batch 59/64 loss: 0.022217392921447754
Batch 60/64 loss: 0.03941166400909424
Batch 61/64 loss: 0.011961042881011963
Batch 62/64 loss: -0.002407252788543701
Batch 63/64 loss: 0.05875295400619507
Batch 64/64 loss: 0.03901177644729614
Epoch 353  Train loss: 0.029910222923054414  Val loss: 0.1199848346693819
Epoch 354
-------------------------------
Batch 1/64 loss: 0.02869647741317749
Batch 2/64 loss: 0.013486146926879883
Batch 3/64 loss: 0.012319862842559814
Batch 4/64 loss: -0.00717848539352417
Batch 5/64 loss: 0.03951054811477661
Batch 6/64 loss: 0.0257377028465271
Batch 7/64 loss: 0.002439558506011963
Batch 8/64 loss: 0.027287960052490234
Batch 9/64 loss: 0.013771295547485352
Batch 10/64 loss: 0.02471446990966797
Batch 11/64 loss: 0.040552616119384766
Batch 12/64 loss: 0.011543989181518555
Batch 13/64 loss: 0.055700600147247314
Batch 14/64 loss: 0.027869045734405518
Batch 15/64 loss: 0.03741109371185303
Batch 16/64 loss: 0.03931999206542969
Batch 17/64 loss: 0.01551508903503418
Batch 18/64 loss: 0.029871761798858643
Batch 19/64 loss: 0.029675424098968506
Batch 20/64 loss: 0.00905376672744751
Batch 21/64 loss: 0.04056572914123535
Batch 22/64 loss: 0.00877237319946289
Batch 23/64 loss: 0.03264707326889038
Batch 24/64 loss: 0.04079842567443848
Batch 25/64 loss: 0.008269786834716797
Batch 26/64 loss: 0.04643279314041138
Batch 27/64 loss: -0.0052474141120910645
Batch 28/64 loss: 0.01653754711151123
Batch 29/64 loss: 0.018133163452148438
Batch 30/64 loss: 0.0574725866317749
Batch 31/64 loss: 0.025322437286376953
Batch 32/64 loss: 0.03871041536331177
Batch 33/64 loss: 0.008321702480316162
Batch 34/64 loss: 0.0488896369934082
Batch 35/64 loss: 0.02946186065673828
Batch 36/64 loss: 0.04759716987609863
Batch 37/64 loss: 0.0025229454040527344
Batch 38/64 loss: 0.043489158153533936
Batch 39/64 loss: 0.040748000144958496
Batch 40/64 loss: 0.04654926061630249
Batch 41/64 loss: 0.011418581008911133
Batch 42/64 loss: 0.042128026485443115
Batch 43/64 loss: 0.05582159757614136
Batch 44/64 loss: 0.026994407176971436
Batch 45/64 loss: 0.05588442087173462
Batch 46/64 loss: 0.03973942995071411
Batch 47/64 loss: 0.021728873252868652
Batch 48/64 loss: 0.02456146478652954
Batch 49/64 loss: 0.038198113441467285
Batch 50/64 loss: 0.06951916217803955
Batch 51/64 loss: 0.03108513355255127
Batch 52/64 loss: 0.0013383030891418457
Batch 53/64 loss: 0.05746948719024658
Batch 54/64 loss: 0.030038774013519287
Batch 55/64 loss: 0.02385002374649048
Batch 56/64 loss: 0.046576499938964844
Batch 57/64 loss: 0.04189777374267578
Batch 58/64 loss: 0.02304697036743164
Batch 59/64 loss: 0.01527637243270874
Batch 60/64 loss: 0.028217732906341553
Batch 61/64 loss: 0.027695000171661377
Batch 62/64 loss: 0.023247122764587402
Batch 63/64 loss: 0.04296344518661499
Batch 64/64 loss: 0.04642462730407715
Epoch 354  Train loss: 0.029597000047272327  Val loss: 0.11907205344065767
Epoch 355
-------------------------------
Batch 1/64 loss: 0.026128172874450684
Batch 2/64 loss: 0.043378233909606934
Batch 3/64 loss: 0.016801416873931885
Batch 4/64 loss: 0.05313485860824585
Batch 5/64 loss: 0.002367258071899414
Batch 6/64 loss: 0.007721424102783203
Batch 7/64 loss: 0.04644209146499634
Batch 8/64 loss: 0.02096688747406006
Batch 9/64 loss: 0.028748035430908203
Batch 10/64 loss: 0.017339587211608887
Batch 11/64 loss: 0.049869537353515625
Batch 12/64 loss: 0.012806951999664307
Batch 13/64 loss: 0.012482523918151855
Batch 14/64 loss: 0.01754218339920044
Batch 15/64 loss: 0.00550764799118042
Batch 16/64 loss: 0.060531795024871826
Batch 17/64 loss: 0.04751342535018921
Batch 18/64 loss: 0.05277365446090698
Batch 19/64 loss: 0.03638291358947754
Batch 20/64 loss: 0.04835176467895508
Batch 21/64 loss: 0.004073798656463623
Batch 22/64 loss: 0.027826011180877686
Batch 23/64 loss: 0.031049787998199463
Batch 24/64 loss: 0.03665119409561157
Batch 25/64 loss: 0.029621124267578125
Batch 26/64 loss: 0.026366114616394043
Batch 27/64 loss: 0.02651500701904297
Batch 28/64 loss: 0.06126135587692261
Batch 29/64 loss: 0.005226314067840576
Batch 30/64 loss: 0.02215355634689331
Batch 31/64 loss: 0.039537787437438965
Batch 32/64 loss: 0.038690805435180664
Batch 33/64 loss: 0.012515902519226074
Batch 34/64 loss: 0.04009824991226196
Batch 35/64 loss: 0.02147209644317627
Batch 36/64 loss: 0.016202688217163086
Batch 37/64 loss: 0.019554972648620605
Batch 38/64 loss: 0.0350261926651001
Batch 39/64 loss: 0.0439530611038208
Batch 40/64 loss: 0.02208101749420166
Batch 41/64 loss: 0.05072510242462158
Batch 42/64 loss: 0.01930767297744751
Batch 43/64 loss: -0.004942476749420166
Batch 44/64 loss: 0.02772843837738037
Batch 45/64 loss: 0.00407034158706665
Batch 46/64 loss: 0.010836660861968994
Batch 47/64 loss: 0.03340601921081543
Batch 48/64 loss: 0.023815691471099854
Batch 49/64 loss: 0.032891809940338135
Batch 50/64 loss: 0.01740199327468872
Batch 51/64 loss: 0.07878077030181885
Batch 52/64 loss: 0.05041038990020752
Batch 53/64 loss: 0.03556936979293823
Batch 54/64 loss: 0.014684855937957764
Batch 55/64 loss: 0.013697624206542969
Batch 56/64 loss: 0.013050079345703125
Batch 57/64 loss: 0.0305941104888916
Batch 58/64 loss: 0.0006782412528991699
Batch 59/64 loss: 0.025906264781951904
Batch 60/64 loss: 0.018933773040771484
Batch 61/64 loss: 0.04142343997955322
Batch 62/64 loss: 0.030613362789154053
Batch 63/64 loss: 0.03443348407745361
Batch 64/64 loss: 0.0321192741394043
Epoch 355  Train loss: 0.028153315712423885  Val loss: 0.12122102947169562
Epoch 356
-------------------------------
Batch 1/64 loss: 0.028733909130096436
Batch 2/64 loss: 0.004821956157684326
Batch 3/64 loss: 0.0038385391235351562
Batch 4/64 loss: 0.026543498039245605
Batch 5/64 loss: 0.05325806140899658
Batch 6/64 loss: 0.0023616552352905273
Batch 7/64 loss: 0.03401702642440796
Batch 8/64 loss: 0.03051859140396118
Batch 9/64 loss: 0.05393010377883911
Batch 10/64 loss: 0.047931551933288574
Batch 11/64 loss: 0.03230464458465576
Batch 12/64 loss: 0.023939549922943115
Batch 13/64 loss: 0.04806053638458252
Batch 14/64 loss: 0.03737354278564453
Batch 15/64 loss: 0.015116274356842041
Batch 16/64 loss: 0.0117073655128479
Batch 17/64 loss: 0.004313111305236816
Batch 18/64 loss: 0.04905807971954346
Batch 19/64 loss: 0.03233736753463745
Batch 20/64 loss: 0.03845393657684326
Batch 21/64 loss: 0.006919741630554199
Batch 22/64 loss: 0.05054283142089844
Batch 23/64 loss: 0.03659212589263916
Batch 24/64 loss: 0.006549358367919922
Batch 25/64 loss: 0.034990131855010986
Batch 26/64 loss: 0.017197906970977783
Batch 27/64 loss: 0.03409600257873535
Batch 28/64 loss: 0.011584758758544922
Batch 29/64 loss: 0.010456502437591553
Batch 30/64 loss: 0.0316430926322937
Batch 31/64 loss: 0.011615455150604248
Batch 32/64 loss: 0.01115119457244873
Batch 33/64 loss: 0.035662174224853516
Batch 34/64 loss: 0.02344954013824463
Batch 35/64 loss: 0.003031909465789795
Batch 36/64 loss: 0.0381467342376709
Batch 37/64 loss: 0.023511171340942383
Batch 38/64 loss: 0.03335690498352051
Batch 39/64 loss: 0.009992599487304688
Batch 40/64 loss: 0.035875797271728516
Batch 41/64 loss: 0.007829368114471436
Batch 42/64 loss: 0.01677703857421875
Batch 43/64 loss: 0.05864083766937256
Batch 44/64 loss: 0.02203613519668579
Batch 45/64 loss: 0.056668758392333984
Batch 46/64 loss: 0.032164573669433594
Batch 47/64 loss: 0.020110726356506348
Batch 48/64 loss: 0.007138371467590332
Batch 49/64 loss: 0.03227943181991577
Batch 50/64 loss: 0.03672546148300171
Batch 51/64 loss: 0.017308712005615234
Batch 52/64 loss: 0.04371309280395508
Batch 53/64 loss: 0.03579837083816528
Batch 54/64 loss: 0.05900466442108154
Batch 55/64 loss: 0.0491141676902771
Batch 56/64 loss: 0.026931047439575195
Batch 57/64 loss: 0.02579808235168457
Batch 58/64 loss: 0.04234158992767334
Batch 59/64 loss: 0.016767799854278564
Batch 60/64 loss: 0.05155754089355469
Batch 61/64 loss: 0.036392927169799805
Batch 62/64 loss: 0.051086485385894775
Batch 63/64 loss: 0.03169888257980347
Batch 64/64 loss: 0.012510061264038086
Epoch 356  Train loss: 0.02874120543984806  Val loss: 0.1158441190457426
Epoch 357
-------------------------------
Batch 1/64 loss: 0.002996385097503662
Batch 2/64 loss: 0.06579262018203735
Batch 3/64 loss: 0.022687673568725586
Batch 4/64 loss: 0.0753975510597229
Batch 5/64 loss: 0.001922607421875
Batch 6/64 loss: 0.0024628043174743652
Batch 7/64 loss: 0.05880093574523926
Batch 8/64 loss: 0.013678014278411865
Batch 9/64 loss: 0.011174440383911133
Batch 10/64 loss: 0.03682178258895874
Batch 11/64 loss: 0.013368546962738037
Batch 12/64 loss: 0.02912086248397827
Batch 13/64 loss: 0.00909733772277832
Batch 14/64 loss: 0.0256197452545166
Batch 15/64 loss: 0.009092390537261963
Batch 16/64 loss: 0.01347208023071289
Batch 17/64 loss: 0.008299946784973145
Batch 18/64 loss: 0.035238802433013916
Batch 19/64 loss: 0.05635195970535278
Batch 20/64 loss: 0.037092626094818115
Batch 21/64 loss: 0.024950802326202393
Batch 22/64 loss: 0.04514843225479126
Batch 23/64 loss: 0.025443732738494873
Batch 24/64 loss: 0.03338271379470825
Batch 25/64 loss: 0.031009554862976074
Batch 26/64 loss: -0.005651354789733887
Batch 27/64 loss: 0.030689537525177002
Batch 28/64 loss: 0.022533774375915527
Batch 29/64 loss: 0.025060594081878662
Batch 30/64 loss: 0.013219773769378662
Batch 31/64 loss: 0.02424335479736328
Batch 32/64 loss: 0.020827889442443848
Batch 33/64 loss: 0.024210751056671143
Batch 34/64 loss: 0.01335287094116211
Batch 35/64 loss: 0.037627220153808594
Batch 36/64 loss: 0.004010021686553955
Batch 37/64 loss: 0.01488041877746582
Batch 38/64 loss: 0.01039212942123413
Batch 39/64 loss: 0.05892068147659302
Batch 40/64 loss: 0.06057250499725342
Batch 41/64 loss: 0.0573805570602417
Batch 42/64 loss: 0.030040860176086426
Batch 43/64 loss: 0.007786393165588379
Batch 44/64 loss: 0.01742100715637207
Batch 45/64 loss: 0.0415225625038147
Batch 46/64 loss: 0.011397838592529297
Batch 47/64 loss: 0.020716488361358643
Batch 48/64 loss: 0.012335360050201416
Batch 49/64 loss: 0.006021618843078613
Batch 50/64 loss: 0.031002521514892578
Batch 51/64 loss: 0.04313087463378906
Batch 52/64 loss: 0.052527666091918945
Batch 53/64 loss: 0.04169553518295288
Batch 54/64 loss: 0.016385793685913086
Batch 55/64 loss: 0.02669602632522583
Batch 56/64 loss: 0.033002376556396484
Batch 57/64 loss: 0.02982884645462036
Batch 58/64 loss: 0.05870318412780762
Batch 59/64 loss: 0.02257668972015381
Batch 60/64 loss: 0.05036771297454834
Batch 61/64 loss: 0.014036059379577637
Batch 62/64 loss: 0.039543747901916504
Batch 63/64 loss: 0.0293886661529541
Batch 64/64 loss: 0.008399248123168945
Epoch 357  Train loss: 0.027343095517625995  Val loss: 0.11673251644442581
Epoch 358
-------------------------------
Batch 1/64 loss: 0.0227774977684021
Batch 2/64 loss: 0.015665411949157715
Batch 3/64 loss: 0.03895366191864014
Batch 4/64 loss: 0.017597734928131104
Batch 5/64 loss: 0.04156982898712158
Batch 6/64 loss: 0.0005753040313720703
Batch 7/64 loss: 0.0047454833984375
Batch 8/64 loss: 0.02109616994857788
Batch 9/64 loss: 0.02633059024810791
Batch 10/64 loss: 0.030655622482299805
Batch 11/64 loss: 0.04971194267272949
Batch 12/64 loss: -0.00019675493240356445
Batch 13/64 loss: 0.02579367160797119
Batch 14/64 loss: 0.0100938081741333
Batch 15/64 loss: 0.053510844707489014
Batch 16/64 loss: 0.023814678192138672
Batch 17/64 loss: 0.011953175067901611
Batch 18/64 loss: 0.04237645864486694
Batch 19/64 loss: 0.027944743633270264
Batch 20/64 loss: 0.02754110097885132
Batch 21/64 loss: 0.040705084800720215
Batch 22/64 loss: 0.03079819679260254
Batch 23/64 loss: 0.00788724422454834
Batch 24/64 loss: 0.05070364475250244
Batch 25/64 loss: 0.010247886180877686
Batch 26/64 loss: 0.06186026334762573
Batch 27/64 loss: 0.014670252799987793
Batch 28/64 loss: 0.020592808723449707
Batch 29/64 loss: 0.002742469310760498
Batch 30/64 loss: 0.026845335960388184
Batch 31/64 loss: 0.00819385051727295
Batch 32/64 loss: 0.030394315719604492
Batch 33/64 loss: 0.024482011795043945
Batch 34/64 loss: 0.01792365312576294
Batch 35/64 loss: 0.01572561264038086
Batch 36/64 loss: 0.03752392530441284
Batch 37/64 loss: 0.035247743129730225
Batch 38/64 loss: 0.04252094030380249
Batch 39/64 loss: 0.03102397918701172
Batch 40/64 loss: 0.047587692737579346
Batch 41/64 loss: 0.0261726975440979
Batch 42/64 loss: 0.03541910648345947
Batch 43/64 loss: 0.018417000770568848
Batch 44/64 loss: 0.007701873779296875
Batch 45/64 loss: 0.024877071380615234
Batch 46/64 loss: 0.020742177963256836
Batch 47/64 loss: 0.08074551820755005
Batch 48/64 loss: 0.008189558982849121
Batch 49/64 loss: 0.05412214994430542
Batch 50/64 loss: 0.025078892707824707
Batch 51/64 loss: 0.01462256908416748
Batch 52/64 loss: 0.042478322982788086
Batch 53/64 loss: 0.04135620594024658
Batch 54/64 loss: 0.023494720458984375
Batch 55/64 loss: 0.052117347717285156
Batch 56/64 loss: 0.039924025535583496
Batch 57/64 loss: 0.010388970375061035
Batch 58/64 loss: 0.028858602046966553
Batch 59/64 loss: 0.030050814151763916
Batch 60/64 loss: 0.04877889156341553
Batch 61/64 loss: 0.04146981239318848
Batch 62/64 loss: 0.027550578117370605
Batch 63/64 loss: 0.027576982975006104
Batch 64/64 loss: 0.03801757097244263
Epoch 358  Train loss: 0.028373881648568546  Val loss: 0.11970862899858926
Epoch 359
-------------------------------
Batch 1/64 loss: 0.004066944122314453
Batch 2/64 loss: 0.012525498867034912
Batch 3/64 loss: 0.027855873107910156
Batch 4/64 loss: 0.05306363105773926
Batch 5/64 loss: 0.025007426738739014
Batch 6/64 loss: 0.01692509651184082
Batch 7/64 loss: 0.016972899436950684
Batch 8/64 loss: 0.025742769241333008
Batch 9/64 loss: 0.030143916606903076
Batch 10/64 loss: 0.048242926597595215
Batch 11/64 loss: 0.029147207736968994
Batch 12/64 loss: 0.03832674026489258
Batch 13/64 loss: 0.03767895698547363
Batch 14/64 loss: 0.054377079010009766
Batch 15/64 loss: 0.03875315189361572
Batch 16/64 loss: 0.022476017475128174
Batch 17/64 loss: 0.018212497234344482
Batch 18/64 loss: 0.016322016716003418
Batch 19/64 loss: 0.05345559120178223
Batch 20/64 loss: 0.030630052089691162
Batch 21/64 loss: 0.004777133464813232
Batch 22/64 loss: 0.03920316696166992
Batch 23/64 loss: 0.0590285062789917
Batch 24/64 loss: 0.009486496448516846
Batch 25/64 loss: 0.042615652084350586
Batch 26/64 loss: -0.0025173425674438477
Batch 27/64 loss: 0.03740513324737549
Batch 28/64 loss: 0.03806746006011963
Batch 29/64 loss: 0.03603583574295044
Batch 30/64 loss: 0.02720087766647339
Batch 31/64 loss: 0.01695847511291504
Batch 32/64 loss: 0.0419766902923584
Batch 33/64 loss: 0.012757420539855957
Batch 34/64 loss: 0.030042409896850586
Batch 35/64 loss: 0.03822535276412964
Batch 36/64 loss: 0.025998055934906006
Batch 37/64 loss: 0.01644301414489746
Batch 38/64 loss: 0.016854286193847656
Batch 39/64 loss: 0.05780315399169922
Batch 40/64 loss: 0.021215438842773438
Batch 41/64 loss: 0.0422743558883667
Batch 42/64 loss: 0.010119438171386719
Batch 43/64 loss: -0.002842843532562256
Batch 44/64 loss: 0.02142864465713501
Batch 45/64 loss: 0.018948137760162354
Batch 46/64 loss: 0.03822678327560425
Batch 47/64 loss: 0.018047571182250977
Batch 48/64 loss: 0.02974843978881836
Batch 49/64 loss: 0.08402776718139648
Batch 50/64 loss: 0.022079169750213623
Batch 51/64 loss: 0.03505301475524902
Batch 52/64 loss: -0.004745066165924072
Batch 53/64 loss: 0.022919654846191406
Batch 54/64 loss: 0.023182332515716553
Batch 55/64 loss: 0.01671314239501953
Batch 56/64 loss: 0.0452423095703125
Batch 57/64 loss: 0.05756783485412598
Batch 58/64 loss: 0.05457329750061035
Batch 59/64 loss: 0.02322852611541748
Batch 60/64 loss: -0.0011150836944580078
Batch 61/64 loss: 0.030574917793273926
Batch 62/64 loss: 0.045294880867004395
Batch 63/64 loss: 0.022669970989227295
Batch 64/64 loss: 0.021908044815063477
Epoch 359  Train loss: 0.028849439059986787  Val loss: 0.11754098893031222
Epoch 360
-------------------------------
Batch 1/64 loss: 0.024913430213928223
Batch 2/64 loss: 0.03274655342102051
Batch 3/64 loss: 0.01562511920928955
Batch 4/64 loss: 0.03726637363433838
Batch 5/64 loss: 0.0025550127029418945
Batch 6/64 loss: 0.024631619453430176
Batch 7/64 loss: 0.04221910238265991
Batch 8/64 loss: 0.01885044574737549
Batch 9/64 loss: 0.020130515098571777
Batch 10/64 loss: 0.010767996311187744
Batch 11/64 loss: 0.02902323007583618
Batch 12/64 loss: -0.0008946657180786133
Batch 13/64 loss: 0.05114394426345825
Batch 14/64 loss: 0.0004189014434814453
Batch 15/64 loss: 0.021663188934326172
Batch 16/64 loss: 0.030827105045318604
Batch 17/64 loss: -0.018088817596435547
Batch 18/64 loss: 0.045752644538879395
Batch 19/64 loss: -0.008633136749267578
Batch 20/64 loss: 0.014593362808227539
Batch 21/64 loss: 0.02329355478286743
Batch 22/64 loss: 0.07776755094528198
Batch 23/64 loss: 0.005717456340789795
Batch 24/64 loss: 0.03215467929840088
Batch 25/64 loss: 0.015122771263122559
Batch 26/64 loss: 0.040341973304748535
Batch 27/64 loss: 0.03943204879760742
Batch 28/64 loss: 0.020310640335083008
Batch 29/64 loss: 0.025549590587615967
Batch 30/64 loss: 0.040911078453063965
Batch 31/64 loss: 0.01819908618927002
Batch 32/64 loss: 0.04531830549240112
Batch 33/64 loss: 0.050834834575653076
Batch 34/64 loss: 0.03938037157058716
Batch 35/64 loss: 0.013713717460632324
Batch 36/64 loss: 0.03582412004470825
Batch 37/64 loss: 0.028096139430999756
Batch 38/64 loss: 0.038847506046295166
Batch 39/64 loss: -0.0051991939544677734
Batch 40/64 loss: 0.043540894985198975
Batch 41/64 loss: 0.030347824096679688
Batch 42/64 loss: 0.012616157531738281
Batch 43/64 loss: 0.019916296005249023
Batch 44/64 loss: 0.03010082244873047
Batch 45/64 loss: 0.0342484712600708
Batch 46/64 loss: 0.015159666538238525
Batch 47/64 loss: 0.0574992299079895
Batch 48/64 loss: -0.005135893821716309
Batch 49/64 loss: 0.03728628158569336
Batch 50/64 loss: 0.01735532283782959
Batch 51/64 loss: 0.034028708934783936
Batch 52/64 loss: 0.033848583698272705
Batch 53/64 loss: 0.02849048376083374
Batch 54/64 loss: 0.02798748016357422
Batch 55/64 loss: 0.04513049125671387
Batch 56/64 loss: 0.0498809814453125
Batch 57/64 loss: 0.0255013108253479
Batch 58/64 loss: 0.03499484062194824
Batch 59/64 loss: 0.019099652767181396
Batch 60/64 loss: 0.029587149620056152
Batch 61/64 loss: 0.01909494400024414
Batch 62/64 loss: 0.031936705112457275
Batch 63/64 loss: 0.032949626445770264
Batch 64/64 loss: 0.06040477752685547
Epoch 360  Train loss: 0.0271670238644469  Val loss: 0.12031088989624862
Epoch 361
-------------------------------
Batch 1/64 loss: 0.030432522296905518
Batch 2/64 loss: 0.044207215309143066
Batch 3/64 loss: 0.047973453998565674
Batch 4/64 loss: 0.027893245220184326
Batch 5/64 loss: 0.03756844997406006
Batch 6/64 loss: 0.010504364967346191
Batch 7/64 loss: 0.03146970272064209
Batch 8/64 loss: 0.024777710437774658
Batch 9/64 loss: 0.03735661506652832
Batch 10/64 loss: 0.034111738204956055
Batch 11/64 loss: 0.012158334255218506
Batch 12/64 loss: 0.00992816686630249
Batch 13/64 loss: 0.02697432041168213
Batch 14/64 loss: 0.0009396076202392578
Batch 15/64 loss: 0.05857968330383301
Batch 16/64 loss: 0.059441447257995605
Batch 17/64 loss: 0.02097189426422119
Batch 18/64 loss: 0.017078399658203125
Batch 19/64 loss: 0.029243946075439453
Batch 20/64 loss: 0.033431053161621094
Batch 21/64 loss: 0.04223299026489258
Batch 22/64 loss: 0.028695523738861084
Batch 23/64 loss: 0.04679453372955322
Batch 24/64 loss: 0.01814931631088257
Batch 25/64 loss: 0.038713037967681885
Batch 26/64 loss: 0.05563634634017944
Batch 27/64 loss: 0.0385013222694397
Batch 28/64 loss: 0.0017011165618896484
Batch 29/64 loss: 0.024995028972625732
Batch 30/64 loss: 0.01134800910949707
Batch 31/64 loss: 0.022327661514282227
Batch 32/64 loss: 0.03259950876235962
Batch 33/64 loss: 0.0002187490463256836
Batch 34/64 loss: 0.01863729953765869
Batch 35/64 loss: 0.003922522068023682
Batch 36/64 loss: 0.021681129932403564
Batch 37/64 loss: 0.024169564247131348
Batch 38/64 loss: 0.015030860900878906
Batch 39/64 loss: 0.03403240442276001
Batch 40/64 loss: 0.01537179946899414
Batch 41/64 loss: 0.04106396436691284
Batch 42/64 loss: 0.030464351177215576
Batch 43/64 loss: 0.02835899591445923
Batch 44/64 loss: 0.007821977138519287
Batch 45/64 loss: 0.027180194854736328
Batch 46/64 loss: 0.04342973232269287
Batch 47/64 loss: 0.006619155406951904
Batch 48/64 loss: 0.03766131401062012
Batch 49/64 loss: 0.019035756587982178
Batch 50/64 loss: 0.021243751049041748
Batch 51/64 loss: 0.0483396053314209
Batch 52/64 loss: 0.018979549407958984
Batch 53/64 loss: 0.014904141426086426
Batch 54/64 loss: 0.005898833274841309
Batch 55/64 loss: 0.024859607219696045
Batch 56/64 loss: 0.03089320659637451
Batch 57/64 loss: 0.04693692922592163
Batch 58/64 loss: 0.0407102108001709
Batch 59/64 loss: 0.015352725982666016
Batch 60/64 loss: 0.013783395290374756
Batch 61/64 loss: 0.044176340103149414
Batch 62/64 loss: 0.03936755657196045
Batch 63/64 loss: 0.02998298406600952
Batch 64/64 loss: 0.021024703979492188
Epoch 361  Train loss: 0.027335425919177484  Val loss: 0.11540369971101637
Saving best model, epoch: 361
Epoch 362
-------------------------------
Batch 1/64 loss: 0.03179657459259033
Batch 2/64 loss: 0.0465160608291626
Batch 3/64 loss: -0.010275185108184814
Batch 4/64 loss: 0.013664960861206055
Batch 5/64 loss: 0.01641988754272461
Batch 6/64 loss: 0.011853516101837158
Batch 7/64 loss: 0.05057007074356079
Batch 8/64 loss: 0.00723874568939209
Batch 9/64 loss: 0.007454574108123779
Batch 10/64 loss: 0.009973883628845215
Batch 11/64 loss: 0.006877422332763672
Batch 12/64 loss: 0.02263730764389038
Batch 13/64 loss: 0.012521862983703613
Batch 14/64 loss: 0.03950798511505127
Batch 15/64 loss: 0.025651097297668457
Batch 16/64 loss: 0.03808790445327759
Batch 17/64 loss: 0.012457728385925293
Batch 18/64 loss: -0.003541707992553711
Batch 19/64 loss: -0.0008592605590820312
Batch 20/64 loss: 0.009359419345855713
Batch 21/64 loss: 0.027612626552581787
Batch 22/64 loss: 0.023353874683380127
Batch 23/64 loss: 0.04344230890274048
Batch 24/64 loss: 0.002566218376159668
Batch 25/64 loss: 0.0280684232711792
Batch 26/64 loss: 0.014940142631530762
Batch 27/64 loss: 0.014310836791992188
Batch 28/64 loss: 0.04749631881713867
Batch 29/64 loss: 0.041051626205444336
Batch 30/64 loss: 0.030960023403167725
Batch 31/64 loss: 0.022705554962158203
Batch 32/64 loss: 0.03078591823577881
Batch 33/64 loss: 0.016245722770690918
Batch 34/64 loss: 0.03360503911972046
Batch 35/64 loss: 0.026591122150421143
Batch 36/64 loss: -0.0014529228210449219
Batch 37/64 loss: 0.03659212589263916
Batch 38/64 loss: 0.010959267616271973
Batch 39/64 loss: 0.004410445690155029
Batch 40/64 loss: 0.05954986810684204
Batch 41/64 loss: 0.05618453025817871
Batch 42/64 loss: 0.03111433982849121
Batch 43/64 loss: 0.03374922275543213
Batch 44/64 loss: 0.02928692102432251
Batch 45/64 loss: 0.02995908260345459
Batch 46/64 loss: 0.04192930459976196
Batch 47/64 loss: 0.029268741607666016
Batch 48/64 loss: 0.05441915988922119
Batch 49/64 loss: 0.02065134048461914
Batch 50/64 loss: 0.02297729253768921
Batch 51/64 loss: 0.03467684984207153
Batch 52/64 loss: 0.061279475688934326
Batch 53/64 loss: 0.012145161628723145
Batch 54/64 loss: 0.05011492967605591
Batch 55/64 loss: 0.02849668264389038
Batch 56/64 loss: 0.06285041570663452
Batch 57/64 loss: 0.030909359455108643
Batch 58/64 loss: 0.04845303297042847
Batch 59/64 loss: 0.028772950172424316
Batch 60/64 loss: -0.008508741855621338
Batch 61/64 loss: 0.025903940200805664
Batch 62/64 loss: 0.037001192569732666
Batch 63/64 loss: 0.04716914892196655
Batch 64/64 loss: 0.05864232778549194
Epoch 362  Train loss: 0.0268940151906481  Val loss: 0.1191559091876053
Epoch 363
-------------------------------
Batch 1/64 loss: 0.04577666521072388
Batch 2/64 loss: 0.041183650493621826
Batch 3/64 loss: 0.046787798404693604
Batch 4/64 loss: 0.006144881248474121
Batch 5/64 loss: 0.030634164810180664
Batch 6/64 loss: 0.009550333023071289
Batch 7/64 loss: 0.044319212436676025
Batch 8/64 loss: 0.023733675479888916
Batch 9/64 loss: 0.03231692314147949
Batch 10/64 loss: 0.035608112812042236
Batch 11/64 loss: 0.023773670196533203
Batch 12/64 loss: 0.018213212490081787
Batch 13/64 loss: 5.5849552154541016e-05
Batch 14/64 loss: 0.01542431116104126
Batch 15/64 loss: 0.008631706237792969
Batch 16/64 loss: 0.013685643672943115
Batch 17/64 loss: 0.0524287223815918
Batch 18/64 loss: 0.03182131052017212
Batch 19/64 loss: 0.011699438095092773
Batch 20/64 loss: 0.016033411026000977
Batch 21/64 loss: -0.004129648208618164
Batch 22/64 loss: 0.02176845073699951
Batch 23/64 loss: 0.004389345645904541
Batch 24/64 loss: 0.045039236545562744
Batch 25/64 loss: 0.040152549743652344
Batch 26/64 loss: 0.022356152534484863
Batch 27/64 loss: 0.012721061706542969
Batch 28/64 loss: 0.0324288010597229
Batch 29/64 loss: 0.02407240867614746
Batch 30/64 loss: 0.05206269025802612
Batch 31/64 loss: 0.03567659854888916
Batch 32/64 loss: 0.013791561126708984
Batch 33/64 loss: 0.06906473636627197
Batch 34/64 loss: -0.00218963623046875
Batch 35/64 loss: 0.03204768896102905
Batch 36/64 loss: 0.03248721361160278
Batch 37/64 loss: 0.02690601348876953
Batch 38/64 loss: 0.039078593254089355
Batch 39/64 loss: 0.01569157838821411
Batch 40/64 loss: 0.005950510501861572
Batch 41/64 loss: 0.01743018627166748
Batch 42/64 loss: 0.03570878505706787
Batch 43/64 loss: 0.05551934242248535
Batch 44/64 loss: 0.016619205474853516
Batch 45/64 loss: 0.04569500684738159
Batch 46/64 loss: 0.03941774368286133
Batch 47/64 loss: 0.03391766548156738
Batch 48/64 loss: 0.018428027629852295
Batch 49/64 loss: 0.05167734622955322
Batch 50/64 loss: 0.04612475633621216
Batch 51/64 loss: 0.011654019355773926
Batch 52/64 loss: 0.002009093761444092
Batch 53/64 loss: 0.012210965156555176
Batch 54/64 loss: 0.04699450731277466
Batch 55/64 loss: 0.013834118843078613
Batch 56/64 loss: 0.03475546836853027
Batch 57/64 loss: 0.028106689453125
Batch 58/64 loss: 0.024404704570770264
Batch 59/64 loss: 0.027658939361572266
Batch 60/64 loss: 0.04584646224975586
Batch 61/64 loss: 0.023162662982940674
Batch 62/64 loss: -0.007921099662780762
Batch 63/64 loss: 0.029137849807739258
Batch 64/64 loss: 0.014097988605499268
Epoch 363  Train loss: 0.026512228040134207  Val loss: 0.11768113030600794
Epoch 364
-------------------------------
Batch 1/64 loss: 0.02255392074584961
Batch 2/64 loss: -0.00477975606918335
Batch 3/64 loss: 0.012228608131408691
Batch 4/64 loss: 0.009702444076538086
Batch 5/64 loss: 0.016610383987426758
Batch 6/64 loss: 0.03534352779388428
Batch 7/64 loss: 0.04622143507003784
Batch 8/64 loss: 0.005599379539489746
Batch 9/64 loss: 0.03420555591583252
Batch 10/64 loss: 0.016818523406982422
Batch 11/64 loss: 0.014118313789367676
Batch 12/64 loss: 0.029812991619110107
Batch 13/64 loss: 0.019467651844024658
Batch 14/64 loss: 0.033394694328308105
Batch 15/64 loss: 0.030158400535583496
Batch 16/64 loss: 0.04850560426712036
Batch 17/64 loss: 0.03530400991439819
Batch 18/64 loss: 0.033818721771240234
Batch 19/64 loss: 0.02820289134979248
Batch 20/64 loss: 0.028813540935516357
Batch 21/64 loss: 0.008615851402282715
Batch 22/64 loss: 0.0073212385177612305
Batch 23/64 loss: 0.03416043519973755
Batch 24/64 loss: 0.029833436012268066
Batch 25/64 loss: 0.01969313621520996
Batch 26/64 loss: -0.00375974178314209
Batch 27/64 loss: 0.014710664749145508
Batch 28/64 loss: 0.015309393405914307
Batch 29/64 loss: 0.005713999271392822
Batch 30/64 loss: 0.027229487895965576
Batch 31/64 loss: 0.020833849906921387
Batch 32/64 loss: 0.03720736503601074
Batch 33/64 loss: 0.04593658447265625
Batch 34/64 loss: 0.03250586986541748
Batch 35/64 loss: 0.05567920207977295
Batch 36/64 loss: 0.018227696418762207
Batch 37/64 loss: 0.04815894365310669
Batch 38/64 loss: 0.023525655269622803
Batch 39/64 loss: 0.009639561176300049
Batch 40/64 loss: 0.036069631576538086
Batch 41/64 loss: 0.046546220779418945
Batch 42/64 loss: 0.05200272798538208
Batch 43/64 loss: 0.04035729169845581
Batch 44/64 loss: 0.030964434146881104
Batch 45/64 loss: 0.033650338649749756
Batch 46/64 loss: 0.05230534076690674
Batch 47/64 loss: 0.028773188591003418
Batch 48/64 loss: 0.00991666316986084
Batch 49/64 loss: 0.037794530391693115
Batch 50/64 loss: 0.03439509868621826
Batch 51/64 loss: 0.01807105541229248
Batch 52/64 loss: 0.020231306552886963
Batch 53/64 loss: 0.026283979415893555
Batch 54/64 loss: 0.06713688373565674
Batch 55/64 loss: 0.023014307022094727
Batch 56/64 loss: 0.02562326192855835
Batch 57/64 loss: 0.05214172601699829
Batch 58/64 loss: 0.023802518844604492
Batch 59/64 loss: 0.04437071084976196
Batch 60/64 loss: 0.01922023296356201
Batch 61/64 loss: 0.037923991680145264
Batch 62/64 loss: 0.03199738264083862
Batch 63/64 loss: 0.032694101333618164
Batch 64/64 loss: 0.010966360569000244
Epoch 364  Train loss: 0.027924002619350657  Val loss: 0.1197375364319975
Epoch 365
-------------------------------
Batch 1/64 loss: 0.015755653381347656
Batch 2/64 loss: 0.011454522609710693
Batch 3/64 loss: 0.01541060209274292
Batch 4/64 loss: 0.04817366600036621
Batch 5/64 loss: 0.031366944313049316
Batch 6/64 loss: 0.013777196407318115
Batch 7/64 loss: 0.0371936559677124
Batch 8/64 loss: 0.03998285531997681
Batch 9/64 loss: 0.02379143238067627
Batch 10/64 loss: 0.026395797729492188
Batch 11/64 loss: 0.04779398441314697
Batch 12/64 loss: 0.008429169654846191
Batch 13/64 loss: 0.05119115114212036
Batch 14/64 loss: 0.031796395778656006
Batch 15/64 loss: 0.016833066940307617
Batch 16/64 loss: 0.019341111183166504
Batch 17/64 loss: 0.030609726905822754
Batch 18/64 loss: 0.0373765230178833
Batch 19/64 loss: 0.004511237144470215
Batch 20/64 loss: 0.04804182052612305
Batch 21/64 loss: 0.034257590770721436
Batch 22/64 loss: 0.00924062728881836
Batch 23/64 loss: 0.0289728045463562
Batch 24/64 loss: 0.026362299919128418
Batch 25/64 loss: 0.02361762523651123
Batch 26/64 loss: 0.028657197952270508
Batch 27/64 loss: -0.0065462589263916016
Batch 28/64 loss: 0.03193187713623047
Batch 29/64 loss: 0.040480196475982666
Batch 30/64 loss: 0.0033288002014160156
Batch 31/64 loss: 0.029873967170715332
Batch 32/64 loss: 0.06257426738739014
Batch 33/64 loss: 0.05143529176712036
Batch 34/64 loss: 0.05372214317321777
Batch 35/64 loss: 0.047003746032714844
Batch 36/64 loss: 0.013013899326324463
Batch 37/64 loss: 0.042780518531799316
Batch 38/64 loss: 0.02929288148880005
Batch 39/64 loss: 0.046898603439331055
Batch 40/64 loss: 0.020769774913787842
Batch 41/64 loss: 0.02073734998703003
Batch 42/64 loss: 0.014278709888458252
Batch 43/64 loss: 0.028173744678497314
Batch 44/64 loss: 0.007047116756439209
Batch 45/64 loss: 0.026205360889434814
Batch 46/64 loss: 0.01618105173110962
Batch 47/64 loss: 0.010201096534729004
Batch 48/64 loss: 0.01612645387649536
Batch 49/64 loss: 0.04319792985916138
Batch 50/64 loss: 0.016283392906188965
Batch 51/64 loss: 0.03309148550033569
Batch 52/64 loss: 0.029697954654693604
Batch 53/64 loss: -0.003435075283050537
Batch 54/64 loss: -0.01343381404876709
Batch 55/64 loss: 0.011566519737243652
Batch 56/64 loss: 0.018592119216918945
Batch 57/64 loss: 0.03061121702194214
Batch 58/64 loss: 0.02911430597305298
Batch 59/64 loss: 0.02454221248626709
Batch 60/64 loss: 0.02899378538131714
Batch 61/64 loss: 0.021085739135742188
Batch 62/64 loss: 0.017768144607543945
Batch 63/64 loss: 0.002742767333984375
Batch 64/64 loss: 0.038428425788879395
Epoch 365  Train loss: 0.02564839615541346  Val loss: 0.11868778946473427
Epoch 366
-------------------------------
Batch 1/64 loss: 0.04251134395599365
Batch 2/64 loss: 0.005831480026245117
Batch 3/64 loss: 0.04161417484283447
Batch 4/64 loss: 0.03860664367675781
Batch 5/64 loss: 0.035793423652648926
Batch 6/64 loss: 0.027930498123168945
Batch 7/64 loss: 0.03296703100204468
Batch 8/64 loss: 0.02609461545944214
Batch 9/64 loss: 0.006027519702911377
Batch 10/64 loss: 0.017722487449645996
Batch 11/64 loss: 0.026415526866912842
Batch 12/64 loss: 0.03041207790374756
Batch 13/64 loss: 0.00913149118423462
Batch 14/64 loss: 0.034983038902282715
Batch 15/64 loss: 0.028937220573425293
Batch 16/64 loss: 0.004440724849700928
Batch 17/64 loss: 0.014925122261047363
Batch 18/64 loss: 0.005072474479675293
Batch 19/64 loss: 0.003856182098388672
Batch 20/64 loss: 0.006750762462615967
Batch 21/64 loss: 0.004027307033538818
Batch 22/64 loss: 0.05265921354293823
Batch 23/64 loss: 0.018915534019470215
Batch 24/64 loss: 0.0005092620849609375
Batch 25/64 loss: 0.01457887887954712
Batch 26/64 loss: 0.042127370834350586
Batch 27/64 loss: 0.008903324604034424
Batch 28/64 loss: 0.03693419694900513
Batch 29/64 loss: 0.015328943729400635
Batch 30/64 loss: 0.04955923557281494
Batch 31/64 loss: 0.00915539264678955
Batch 32/64 loss: 0.0398637056350708
Batch 33/64 loss: 0.009610354900360107
Batch 34/64 loss: 0.015513896942138672
Batch 35/64 loss: 0.03130602836608887
Batch 36/64 loss: 0.031409263610839844
Batch 37/64 loss: 0.038379013538360596
Batch 38/64 loss: 0.03608369827270508
Batch 39/64 loss: 0.034662604331970215
Batch 40/64 loss: 0.02527487277984619
Batch 41/64 loss: 0.028821825981140137
Batch 42/64 loss: 0.025852322578430176
Batch 43/64 loss: 0.011304140090942383
Batch 44/64 loss: 0.02893078327178955
Batch 45/64 loss: 0.005840003490447998
Batch 46/64 loss: 0.03621840476989746
Batch 47/64 loss: 0.06779944896697998
Batch 48/64 loss: 0.008826255798339844
Batch 49/64 loss: 0.029574692249298096
Batch 50/64 loss: 0.008612751960754395
Batch 51/64 loss: 0.007973790168762207
Batch 52/64 loss: 0.059362828731536865
Batch 53/64 loss: 0.05374270677566528
Batch 54/64 loss: 0.04250979423522949
Batch 55/64 loss: 0.03386014699935913
Batch 56/64 loss: 0.042928338050842285
Batch 57/64 loss: 0.007561922073364258
Batch 58/64 loss: 0.026740968227386475
Batch 59/64 loss: 0.04169946908950806
Batch 60/64 loss: 0.039443135261535645
Batch 61/64 loss: 0.01272350549697876
Batch 62/64 loss: 0.021950960159301758
Batch 63/64 loss: 0.023859858512878418
Batch 64/64 loss: 0.037439703941345215
Epoch 366  Train loss: 0.025867353233636595  Val loss: 0.11850174862084929
Epoch 367
-------------------------------
Batch 1/64 loss: 0.028894484043121338
Batch 2/64 loss: 0.03244692087173462
Batch 3/64 loss: 0.024700403213500977
Batch 4/64 loss: 0.017183303833007812
Batch 5/64 loss: 0.03111201524734497
Batch 6/64 loss: 0.006045162677764893
Batch 7/64 loss: 0.03553164005279541
Batch 8/64 loss: 0.007162630558013916
Batch 9/64 loss: 0.024346649646759033
Batch 10/64 loss: 0.039011359214782715
Batch 11/64 loss: 0.04210406541824341
Batch 12/64 loss: 0.002411365509033203
Batch 13/64 loss: 0.020383477210998535
Batch 14/64 loss: 0.00013369321823120117
Batch 15/64 loss: 0.010072767734527588
Batch 16/64 loss: 0.015290558338165283
Batch 17/64 loss: 0.06584811210632324
Batch 18/64 loss: 0.015648603439331055
Batch 19/64 loss: 0.02465522289276123
Batch 20/64 loss: 0.03826785087585449
Batch 21/64 loss: 0.03848695755004883
Batch 22/64 loss: 0.05030691623687744
Batch 23/64 loss: 0.015143394470214844
Batch 24/64 loss: -4.1365623474121094e-05
Batch 25/64 loss: 0.01752328872680664
Batch 26/64 loss: 0.021213829517364502
Batch 27/64 loss: 0.002724945545196533
Batch 28/64 loss: 0.03794503211975098
Batch 29/64 loss: 0.011121273040771484
Batch 30/64 loss: 0.03876781463623047
Batch 31/64 loss: 0.010225296020507812
Batch 32/64 loss: 0.09424334764480591
Batch 33/64 loss: 0.000656425952911377
Batch 34/64 loss: 0.038403332233428955
Batch 35/64 loss: 0.015526115894317627
Batch 36/64 loss: 0.03572893142700195
Batch 37/64 loss: 0.04123842716217041
Batch 38/64 loss: 0.01919245719909668
Batch 39/64 loss: 0.039139389991760254
Batch 40/64 loss: 0.008478701114654541
Batch 41/64 loss: 0.039524972438812256
Batch 42/64 loss: 0.03526496887207031
Batch 43/64 loss: 0.009804964065551758
Batch 44/64 loss: 0.04092341661453247
Batch 45/64 loss: 0.0009499192237854004
Batch 46/64 loss: 0.04295647144317627
Batch 47/64 loss: 0.006551802158355713
Batch 48/64 loss: 0.03902798891067505
Batch 49/64 loss: 0.02474534511566162
Batch 50/64 loss: 0.03723561763763428
Batch 51/64 loss: 0.01878732442855835
Batch 52/64 loss: 0.035971105098724365
Batch 53/64 loss: 0.04844844341278076
Batch 54/64 loss: -0.0018914341926574707
Batch 55/64 loss: 0.020931363105773926
Batch 56/64 loss: 0.0022646188735961914
Batch 57/64 loss: 0.012724161148071289
Batch 58/64 loss: 0.041726529598236084
Batch 59/64 loss: 0.027283310890197754
Batch 60/64 loss: 0.007817089557647705
Batch 61/64 loss: 0.034355759620666504
Batch 62/64 loss: 0.04132080078125
Batch 63/64 loss: 0.04279124736785889
Batch 64/64 loss: 0.07720202207565308
Epoch 367  Train loss: 0.026426542740242153  Val loss: 0.11658521698102918
Epoch 368
-------------------------------
Batch 1/64 loss: 0.023716092109680176
Batch 2/64 loss: 0.02912825345993042
Batch 3/64 loss: -0.004520416259765625
Batch 4/64 loss: 0.012844204902648926
Batch 5/64 loss: 0.04387921094894409
Batch 6/64 loss: 0.03742551803588867
Batch 7/64 loss: 0.013527333736419678
Batch 8/64 loss: -0.01765531301498413
Batch 9/64 loss: 0.015078306198120117
Batch 10/64 loss: 0.028373360633850098
Batch 11/64 loss: -0.004516303539276123
Batch 12/64 loss: 0.03681153059005737
Batch 13/64 loss: 0.020263969898223877
Batch 14/64 loss: 0.04414176940917969
Batch 15/64 loss: -7.653236389160156e-05
Batch 16/64 loss: 0.028431594371795654
Batch 17/64 loss: 0.011269152164459229
Batch 18/64 loss: 0.03417551517486572
Batch 19/64 loss: 0.002061605453491211
Batch 20/64 loss: 0.021022439002990723
Batch 21/64 loss: 0.012865245342254639
Batch 22/64 loss: 0.022027134895324707
Batch 23/64 loss: 0.027487516403198242
Batch 24/64 loss: 0.017790913581848145
Batch 25/64 loss: 0.025738239288330078
Batch 26/64 loss: 0.028455376625061035
Batch 27/64 loss: 0.04145669937133789
Batch 28/64 loss: 0.004361629486083984
Batch 29/64 loss: 0.011659026145935059
Batch 30/64 loss: 0.029623031616210938
Batch 31/64 loss: 0.016018688678741455
Batch 32/64 loss: 0.01754230260848999
Batch 33/64 loss: 0.013951420783996582
Batch 34/64 loss: 0.033432066440582275
Batch 35/64 loss: 0.04295921325683594
Batch 36/64 loss: 0.04791301488876343
Batch 37/64 loss: 0.025043129920959473
Batch 38/64 loss: 0.026635050773620605
Batch 39/64 loss: 0.0386011004447937
Batch 40/64 loss: 0.06947535276412964
Batch 41/64 loss: 0.05232107639312744
Batch 42/64 loss: 0.0286942720413208
Batch 43/64 loss: 0.0418735146522522
Batch 44/64 loss: 0.03457432985305786
Batch 45/64 loss: 0.06332957744598389
Batch 46/64 loss: 0.0038312673568725586
Batch 47/64 loss: 0.028633534908294678
Batch 48/64 loss: 0.0362401008605957
Batch 49/64 loss: 0.04207587242126465
Batch 50/64 loss: 0.0028528571128845215
Batch 51/64 loss: 0.03331249952316284
Batch 52/64 loss: 0.030824244022369385
Batch 53/64 loss: 0.026226162910461426
Batch 54/64 loss: 0.012069940567016602
Batch 55/64 loss: 0.012195169925689697
Batch 56/64 loss: 0.02669668197631836
Batch 57/64 loss: 0.013641536235809326
Batch 58/64 loss: 0.05218225717544556
Batch 59/64 loss: 0.024218261241912842
Batch 60/64 loss: 0.016276836395263672
Batch 61/64 loss: 0.005550980567932129
Batch 62/64 loss: 0.003308713436126709
Batch 63/64 loss: 0.014176785945892334
Batch 64/64 loss: 0.018072783946990967
Epoch 368  Train loss: 0.024267898120132147  Val loss: 0.12058045937842929
Epoch 369
-------------------------------
Batch 1/64 loss: 0.046545982360839844
Batch 2/64 loss: -0.00570988655090332
Batch 3/64 loss: 0.03526252508163452
Batch 4/64 loss: 0.024879634380340576
Batch 5/64 loss: 0.048642098903656006
Batch 6/64 loss: -0.001815199851989746
Batch 7/64 loss: 0.03638380765914917
Batch 8/64 loss: 0.017760396003723145
Batch 9/64 loss: 0.025616586208343506
Batch 10/64 loss: 0.012022018432617188
Batch 11/64 loss: 0.026839017868041992
Batch 12/64 loss: 0.016430377960205078
Batch 13/64 loss: 0.0496479868888855
Batch 14/64 loss: 0.015603363513946533
Batch 15/64 loss: 0.04342985153198242
Batch 16/64 loss: 0.00364077091217041
Batch 17/64 loss: 0.008537709712982178
Batch 18/64 loss: 0.010139286518096924
Batch 19/64 loss: 0.03080880641937256
Batch 20/64 loss: 0.05905431509017944
Batch 21/64 loss: 0.026669085025787354
Batch 22/64 loss: 0.043083012104034424
Batch 23/64 loss: 0.02385026216506958
Batch 24/64 loss: 0.03187340497970581
Batch 25/64 loss: 0.02939295768737793
Batch 26/64 loss: 0.01254957914352417
Batch 27/64 loss: 0.027831196784973145
Batch 28/64 loss: 0.011613190174102783
Batch 29/64 loss: 0.06629067659378052
Batch 30/64 loss: 0.0174066424369812
Batch 31/64 loss: 0.023372530937194824
Batch 32/64 loss: 0.004374027252197266
Batch 33/64 loss: 0.02702951431274414
Batch 34/64 loss: 0.006531834602355957
Batch 35/64 loss: 0.0027127861976623535
Batch 36/64 loss: 0.02363407611846924
Batch 37/64 loss: 0.056142449378967285
Batch 38/64 loss: 0.023581206798553467
Batch 39/64 loss: 0.058978915214538574
Batch 40/64 loss: 0.03965634107589722
Batch 41/64 loss: 0.037652671337127686
Batch 42/64 loss: 0.01847207546234131
Batch 43/64 loss: 0.024032294750213623
Batch 44/64 loss: 0.04650503396987915
Batch 45/64 loss: 0.01244497299194336
Batch 46/64 loss: 0.03396576642990112
Batch 47/64 loss: 0.005092799663543701
Batch 48/64 loss: 0.022703588008880615
Batch 49/64 loss: -0.004304170608520508
Batch 50/64 loss: 0.03743475675582886
Batch 51/64 loss: 0.01654684543609619
Batch 52/64 loss: 0.036280810832977295
Batch 53/64 loss: 0.012458324432373047
Batch 54/64 loss: 0.03728020191192627
Batch 55/64 loss: 0.01896756887435913
Batch 56/64 loss: 0.04440188407897949
Batch 57/64 loss: 0.04217439889907837
Batch 58/64 loss: 0.03857356309890747
Batch 59/64 loss: 0.0438656210899353
Batch 60/64 loss: 0.041627585887908936
Batch 61/64 loss: 0.0316011905670166
Batch 62/64 loss: 0.018396973609924316
Batch 63/64 loss: 0.05959725379943848
Batch 64/64 loss: 0.006602048873901367
Epoch 369  Train loss: 0.027310065662159638  Val loss: 0.12053053301224594
Epoch 370
-------------------------------
Batch 1/64 loss: 0.04461872577667236
Batch 2/64 loss: 0.04685795307159424
Batch 3/64 loss: 0.0411761999130249
Batch 4/64 loss: 0.0172346830368042
Batch 5/64 loss: 0.013278484344482422
Batch 6/64 loss: 0.013049423694610596
Batch 7/64 loss: 0.04260450601577759
Batch 8/64 loss: 0.037126779556274414
Batch 9/64 loss: 0.017412006855010986
Batch 10/64 loss: 0.010716795921325684
Batch 11/64 loss: 0.04406476020812988
Batch 12/64 loss: 0.01912635564804077
Batch 13/64 loss: 0.028995096683502197
Batch 14/64 loss: 0.03524726629257202
Batch 15/64 loss: 0.02949315309524536
Batch 16/64 loss: 0.024652302265167236
Batch 17/64 loss: 0.02637791633605957
Batch 18/64 loss: 0.01463770866394043
Batch 19/64 loss: 0.05474650859832764
Batch 20/64 loss: 0.039781153202056885
Batch 21/64 loss: 0.011343777179718018
Batch 22/64 loss: 0.024953365325927734
Batch 23/64 loss: 0.021793127059936523
Batch 24/64 loss: 0.03162902593612671
Batch 25/64 loss: 0.025713801383972168
Batch 26/64 loss: 0.03638947010040283
Batch 27/64 loss: -0.006474018096923828
Batch 28/64 loss: 0.01907801628112793
Batch 29/64 loss: 0.029971003532409668
Batch 30/64 loss: -0.009069979190826416
Batch 31/64 loss: 0.02279287576675415
Batch 32/64 loss: 0.0380207896232605
Batch 33/64 loss: 0.033825039863586426
Batch 34/64 loss: 0.027426600456237793
Batch 35/64 loss: 0.0075296759605407715
Batch 36/64 loss: 0.027953028678894043
Batch 37/64 loss: 0.04903590679168701
Batch 38/64 loss: 0.0374721884727478
Batch 39/64 loss: 0.005665123462677002
Batch 40/64 loss: 0.03246861696243286
Batch 41/64 loss: 0.01737898588180542
Batch 42/64 loss: 0.006509602069854736
Batch 43/64 loss: 0.02019554376602173
Batch 44/64 loss: 0.014482259750366211
Batch 45/64 loss: 0.03997272253036499
Batch 46/64 loss: 0.02604496479034424
Batch 47/64 loss: 0.03184390068054199
Batch 48/64 loss: 0.02677464485168457
Batch 49/64 loss: 0.01973700523376465
Batch 50/64 loss: 0.012633919715881348
Batch 51/64 loss: 0.021974027156829834
Batch 52/64 loss: 0.02644026279449463
Batch 53/64 loss: 0.017071008682250977
Batch 54/64 loss: -0.015430331230163574
Batch 55/64 loss: 0.016697227954864502
Batch 56/64 loss: 0.044459521770477295
Batch 57/64 loss: -0.013191580772399902
Batch 58/64 loss: 0.053084373474121094
Batch 59/64 loss: 0.054134368896484375
Batch 60/64 loss: 0.04353731870651245
Batch 61/64 loss: 0.051096975803375244
Batch 62/64 loss: 0.04615426063537598
Batch 63/64 loss: 0.03689473867416382
Batch 64/64 loss: 0.0036402344703674316
Epoch 370  Train loss: 0.026195154704299627  Val loss: 0.11811232362006538
Epoch 371
-------------------------------
Batch 1/64 loss: 0.028953373432159424
Batch 2/64 loss: -0.0012750029563903809
Batch 3/64 loss: 0.02271956205368042
Batch 4/64 loss: 0.022991478443145752
Batch 5/64 loss: 0.011538028717041016
Batch 6/64 loss: 0.03310227394104004
Batch 7/64 loss: 0.005587577819824219
Batch 8/64 loss: 0.018621385097503662
Batch 9/64 loss: 0.012946605682373047
Batch 10/64 loss: 0.027973651885986328
Batch 11/64 loss: 0.03985553979873657
Batch 12/64 loss: 0.03481251001358032
Batch 13/64 loss: 0.05581289529800415
Batch 14/64 loss: 0.02818983793258667
Batch 15/64 loss: 0.016871213912963867
Batch 16/64 loss: 0.01738286018371582
Batch 17/64 loss: 0.030132830142974854
Batch 18/64 loss: 0.031277358531951904
Batch 19/64 loss: 0.01211237907409668
Batch 20/64 loss: 0.03193455934524536
Batch 21/64 loss: 0.03471094369888306
Batch 22/64 loss: -0.015401124954223633
Batch 23/64 loss: 0.015530645847320557
Batch 24/64 loss: 0.034719109535217285
Batch 25/64 loss: 0.013826489448547363
Batch 26/64 loss: 0.03235268592834473
Batch 27/64 loss: 0.015073895454406738
Batch 28/64 loss: 0.011197984218597412
Batch 29/64 loss: 0.01863914728164673
Batch 30/64 loss: 0.01957571506500244
Batch 31/64 loss: 0.04550600051879883
Batch 32/64 loss: 0.03445446491241455
Batch 33/64 loss: 0.026167213916778564
Batch 34/64 loss: 0.03555166721343994
Batch 35/64 loss: 0.03746521472930908
Batch 36/64 loss: 0.013104796409606934
Batch 37/64 loss: 0.032897114753723145
Batch 38/64 loss: 0.0478290319442749
Batch 39/64 loss: 0.0071007609367370605
Batch 40/64 loss: 0.03637242317199707
Batch 41/64 loss: 0.01660066843032837
Batch 42/64 loss: 0.018271327018737793
Batch 43/64 loss: 0.02721107006072998
Batch 44/64 loss: 0.03482705354690552
Batch 45/64 loss: 0.05295836925506592
Batch 46/64 loss: 0.011080503463745117
Batch 47/64 loss: 0.0306510329246521
Batch 48/64 loss: 0.02402174472808838
Batch 49/64 loss: 0.01824653148651123
Batch 50/64 loss: 0.049593985080718994
Batch 51/64 loss: 0.012495338916778564
Batch 52/64 loss: 0.015987753868103027
Batch 53/64 loss: 0.014281809329986572
Batch 54/64 loss: -0.002376377582550049
Batch 55/64 loss: 0.018325209617614746
Batch 56/64 loss: 0.028334498405456543
Batch 57/64 loss: 0.0036626458168029785
Batch 58/64 loss: 0.009458065032958984
Batch 59/64 loss: 0.028795897960662842
Batch 60/64 loss: 0.004508554935455322
Batch 61/64 loss: 0.021739482879638672
Batch 62/64 loss: 0.026491105556488037
Batch 63/64 loss: 0.02103602886199951
Batch 64/64 loss: 0.02430015802383423
Epoch 371  Train loss: 0.023257137513628193  Val loss: 0.11704785184761912
Epoch 372
-------------------------------
Batch 1/64 loss: 0.014519274234771729
Batch 2/64 loss: 0.01593172550201416
Batch 3/64 loss: 0.047444283962249756
Batch 4/64 loss: 0.036234498023986816
Batch 5/64 loss: 0.016202569007873535
Batch 6/64 loss: 0.0438389778137207
Batch 7/64 loss: 0.030867815017700195
Batch 8/64 loss: 0.01896202564239502
Batch 9/64 loss: -0.006173849105834961
Batch 10/64 loss: 0.019482433795928955
Batch 11/64 loss: 0.02234870195388794
Batch 12/64 loss: 0.017337322235107422
Batch 13/64 loss: 0.04787379503250122
Batch 14/64 loss: 0.007666945457458496
Batch 15/64 loss: 0.03407353162765503
Batch 16/64 loss: 0.005951225757598877
Batch 17/64 loss: 0.03908354043960571
Batch 18/64 loss: 0.01740872859954834
Batch 19/64 loss: 0.03239178657531738
Batch 20/64 loss: 0.027936220169067383
Batch 21/64 loss: 0.015847504138946533
Batch 22/64 loss: 0.05402851104736328
Batch 23/64 loss: 0.04482865333557129
Batch 24/64 loss: 0.047035932540893555
Batch 25/64 loss: 0.0004367232322692871
Batch 26/64 loss: 0.018642783164978027
Batch 27/64 loss: 0.0225980281829834
Batch 28/64 loss: 0.01871466636657715
Batch 29/64 loss: -0.004411101341247559
Batch 30/64 loss: 0.03837871551513672
Batch 31/64 loss: 0.0012793540954589844
Batch 32/64 loss: 0.0395771861076355
Batch 33/64 loss: 0.030103206634521484
Batch 34/64 loss: 0.014366388320922852
Batch 35/64 loss: 0.08437299728393555
Batch 36/64 loss: 0.029851198196411133
Batch 37/64 loss: 0.026145756244659424
Batch 38/64 loss: 0.05065077543258667
Batch 39/64 loss: 0.027343571186065674
Batch 40/64 loss: 0.02411872148513794
Batch 41/64 loss: 0.05041229724884033
Batch 42/64 loss: 0.03198981285095215
Batch 43/64 loss: 0.03966641426086426
Batch 44/64 loss: 0.032808125019073486
Batch 45/64 loss: 0.017640531063079834
Batch 46/64 loss: 0.013804316520690918
Batch 47/64 loss: 0.011118829250335693
Batch 48/64 loss: 0.045985400676727295
Batch 49/64 loss: 0.022526443004608154
Batch 50/64 loss: 0.06209874153137207
Batch 51/64 loss: 0.00330275297164917
Batch 52/64 loss: -0.0017070770263671875
Batch 53/64 loss: 0.03351259231567383
Batch 54/64 loss: 0.030947387218475342
Batch 55/64 loss: 0.03766900300979614
Batch 56/64 loss: 0.023552536964416504
Batch 57/64 loss: 0.02244102954864502
Batch 58/64 loss: 0.02471613883972168
Batch 59/64 loss: 0.024260282516479492
Batch 60/64 loss: 0.03983652591705322
Batch 61/64 loss: 0.0003343820571899414
Batch 62/64 loss: 0.022550761699676514
Batch 63/64 loss: 0.03349483013153076
Batch 64/64 loss: -0.0020833611488342285
Epoch 372  Train loss: 0.026583382896348543  Val loss: 0.11774802085050602
Epoch 373
-------------------------------
Batch 1/64 loss: 0.02461642026901245
Batch 2/64 loss: 0.01648402214050293
Batch 3/64 loss: 0.022721469402313232
Batch 4/64 loss: -0.007492363452911377
Batch 5/64 loss: 0.004411458969116211
Batch 6/64 loss: 0.017053306102752686
Batch 7/64 loss: 0.02756720781326294
Batch 8/64 loss: 0.03194558620452881
Batch 9/64 loss: 0.013802051544189453
Batch 10/64 loss: -0.0042237043380737305
Batch 11/64 loss: 0.021701335906982422
Batch 12/64 loss: 0.03772842884063721
Batch 13/64 loss: 0.026192665100097656
Batch 14/64 loss: 0.0721624493598938
Batch 15/64 loss: -0.0048596858978271484
Batch 16/64 loss: 0.01840001344680786
Batch 17/64 loss: 0.019192218780517578
Batch 18/64 loss: -0.0010334253311157227
Batch 19/64 loss: 0.04465442895889282
Batch 20/64 loss: 0.024916112422943115
Batch 21/64 loss: 0.011983513832092285
Batch 22/64 loss: 0.0400162935256958
Batch 23/64 loss: 0.02437734603881836
Batch 24/64 loss: 0.03402233123779297
Batch 25/64 loss: -0.00572437047958374
Batch 26/64 loss: 0.024843692779541016
Batch 27/64 loss: 0.017468750476837158
Batch 28/64 loss: 0.027539730072021484
Batch 29/64 loss: 0.04841911792755127
Batch 30/64 loss: 0.018497049808502197
Batch 31/64 loss: 0.002237081527709961
Batch 32/64 loss: 0.00884944200515747
Batch 33/64 loss: 0.01817840337753296
Batch 34/64 loss: 0.013270735740661621
Batch 35/64 loss: 0.04858756065368652
Batch 36/64 loss: 0.033967792987823486
Batch 37/64 loss: 0.03470134735107422
Batch 38/64 loss: 0.013594865798950195
Batch 39/64 loss: 0.02740424871444702
Batch 40/64 loss: 0.035896360874176025
Batch 41/64 loss: 0.0331188440322876
Batch 42/64 loss: 0.03552299737930298
Batch 43/64 loss: 0.026320159435272217
Batch 44/64 loss: 0.03652083873748779
Batch 45/64 loss: 0.033606767654418945
Batch 46/64 loss: 0.020329833030700684
Batch 47/64 loss: 0.041787683963775635
Batch 48/64 loss: 0.021404802799224854
Batch 49/64 loss: 0.024769306182861328
Batch 50/64 loss: 0.04172903299331665
Batch 51/64 loss: 0.0029918551445007324
Batch 52/64 loss: 0.003321349620819092
Batch 53/64 loss: 0.02291238307952881
Batch 54/64 loss: 0.058532655239105225
Batch 55/64 loss: 0.03562331199645996
Batch 56/64 loss: 0.02090376615524292
Batch 57/64 loss: 0.02398926019668579
Batch 58/64 loss: 0.01973968744277954
Batch 59/64 loss: 0.023906230926513672
Batch 60/64 loss: 0.02113187313079834
Batch 61/64 loss: 0.049029767513275146
Batch 62/64 loss: 0.029655873775482178
Batch 63/64 loss: 0.03567224740982056
Batch 64/64 loss: 0.011683404445648193
Epoch 373  Train loss: 0.024397715633990718  Val loss: 0.1174007797568934
Epoch 374
-------------------------------
Batch 1/64 loss: 0.0045125484466552734
Batch 2/64 loss: 0.011570453643798828
Batch 3/64 loss: -0.018021881580352783
Batch 4/64 loss: 0.01779484748840332
Batch 5/64 loss: 0.02800285816192627
Batch 6/64 loss: 0.029704153537750244
Batch 7/64 loss: 0.02588832378387451
Batch 8/64 loss: 0.0023242831230163574
Batch 9/64 loss: 0.009675145149230957
Batch 10/64 loss: -0.005656898021697998
Batch 11/64 loss: 0.01588815450668335
Batch 12/64 loss: 0.013070821762084961
Batch 13/64 loss: 0.00654447078704834
Batch 14/64 loss: 0.022253990173339844
Batch 15/64 loss: 0.029500842094421387
Batch 16/64 loss: 0.008533775806427002
Batch 17/64 loss: 0.03965914249420166
Batch 18/64 loss: 0.0250357985496521
Batch 19/64 loss: -0.004152238368988037
Batch 20/64 loss: 0.012239694595336914
Batch 21/64 loss: 0.03477966785430908
Batch 22/64 loss: 0.005530297756195068
Batch 23/64 loss: 0.026091933250427246
Batch 24/64 loss: 0.06414854526519775
Batch 25/64 loss: 0.029677212238311768
Batch 26/64 loss: 0.01863265037536621
Batch 27/64 loss: 0.020216405391693115
Batch 28/64 loss: 0.044019103050231934
Batch 29/64 loss: 0.013067483901977539
Batch 30/64 loss: 0.03345966339111328
Batch 31/64 loss: 0.02064681053161621
Batch 32/64 loss: 0.04507267475128174
Batch 33/64 loss: 0.06281429529190063
Batch 34/64 loss: 0.021960079669952393
Batch 35/64 loss: 0.027936458587646484
Batch 36/64 loss: 0.0027124881744384766
Batch 37/64 loss: 0.03642308712005615
Batch 38/64 loss: 0.025977611541748047
Batch 39/64 loss: 0.03967243432998657
Batch 40/64 loss: 0.028764188289642334
Batch 41/64 loss: 0.041400372982025146
Batch 42/64 loss: 0.04524850845336914
Batch 43/64 loss: 0.022940635681152344
Batch 44/64 loss: 0.01984173059463501
Batch 45/64 loss: 0.04100024700164795
Batch 46/64 loss: 0.0024237632751464844
Batch 47/64 loss: 0.03491455316543579
Batch 48/64 loss: 0.01480412483215332
Batch 49/64 loss: 0.01587444543838501
Batch 50/64 loss: 0.005006194114685059
Batch 51/64 loss: 0.027857422828674316
Batch 52/64 loss: 0.04192262887954712
Batch 53/64 loss: 0.062258362770080566
Batch 54/64 loss: 0.0344386100769043
Batch 55/64 loss: 0.020922422409057617
Batch 56/64 loss: 0.02178436517715454
Batch 57/64 loss: 0.061493754386901855
Batch 58/64 loss: 0.026056647300720215
Batch 59/64 loss: 0.034451961517333984
Batch 60/64 loss: 0.02143913507461548
Batch 61/64 loss: 0.019074440002441406
Batch 62/64 loss: 0.03267478942871094
Batch 63/64 loss: 0.024110853672027588
Batch 64/64 loss: 0.017619669437408447
Epoch 374  Train loss: 0.024488252518223782  Val loss: 0.11791353024977588
Epoch 375
-------------------------------
Batch 1/64 loss: 0.0012575984001159668
Batch 2/64 loss: -0.0004546046257019043
Batch 3/64 loss: 0.031145215034484863
Batch 4/64 loss: 0.007211148738861084
Batch 5/64 loss: 0.02090013027191162
Batch 6/64 loss: 0.005908608436584473
Batch 7/64 loss: 0.003085017204284668
Batch 8/64 loss: 0.03532540798187256
Batch 9/64 loss: 0.0023693442344665527
Batch 10/64 loss: 0.0017345547676086426
Batch 11/64 loss: 0.019435644149780273
Batch 12/64 loss: 0.016488969326019287
Batch 13/64 loss: 0.04030025005340576
Batch 14/64 loss: 0.00926828384399414
Batch 15/64 loss: 0.031667470932006836
Batch 16/64 loss: 0.008571803569793701
Batch 17/64 loss: 0.024315595626831055
Batch 18/64 loss: 0.027113795280456543
Batch 19/64 loss: -0.013435840606689453
Batch 20/64 loss: 0.005951941013336182
Batch 21/64 loss: 0.03444093465805054
Batch 22/64 loss: 0.04102623462677002
Batch 23/64 loss: 0.0572664737701416
Batch 24/64 loss: 0.013301253318786621
Batch 25/64 loss: 0.03452455997467041
Batch 26/64 loss: 0.0016434192657470703
Batch 27/64 loss: -0.0019987821578979492
Batch 28/64 loss: 0.025477588176727295
Batch 29/64 loss: 0.03993713855743408
Batch 30/64 loss: 0.03978407382965088
Batch 31/64 loss: -0.004402279853820801
Batch 32/64 loss: 0.01956087350845337
Batch 33/64 loss: -0.0014564990997314453
Batch 34/64 loss: 0.028894662857055664
Batch 35/64 loss: 0.017610490322113037
Batch 36/64 loss: -0.004343926906585693
Batch 37/64 loss: 0.06483709812164307
Batch 38/64 loss: 0.047465384006500244
Batch 39/64 loss: 0.03980070352554321
Batch 40/64 loss: 0.017482995986938477
Batch 41/64 loss: 0.04406702518463135
Batch 42/64 loss: 0.012431919574737549
Batch 43/64 loss: 0.022721171379089355
Batch 44/64 loss: 0.01376485824584961
Batch 45/64 loss: 0.018581390380859375
Batch 46/64 loss: 0.004545331001281738
Batch 47/64 loss: 0.03820371627807617
Batch 48/64 loss: 0.033100783824920654
Batch 49/64 loss: 0.0342252254486084
Batch 50/64 loss: 0.061694979667663574
Batch 51/64 loss: 0.039461731910705566
Batch 52/64 loss: 0.056170105934143066
Batch 53/64 loss: 0.030821144580841064
Batch 54/64 loss: 0.02250349521636963
Batch 55/64 loss: 0.0367090106010437
Batch 56/64 loss: 0.04294306039810181
Batch 57/64 loss: 0.03218573331832886
Batch 58/64 loss: 0.02930837869644165
Batch 59/64 loss: 0.011719286441802979
Batch 60/64 loss: 0.02217966318130493
Batch 61/64 loss: 0.03251153230667114
Batch 62/64 loss: 0.014292240142822266
Batch 63/64 loss: -0.00559765100479126
Batch 64/64 loss: 0.03003525733947754
Epoch 375  Train loss: 0.022903267542521158  Val loss: 0.12080176405070983
Epoch 376
-------------------------------
Batch 1/64 loss: 0.03471779823303223
Batch 2/64 loss: -0.007105410099029541
Batch 3/64 loss: 0.03126794099807739
Batch 4/64 loss: 0.029656708240509033
Batch 5/64 loss: 0.01303941011428833
Batch 6/64 loss: 0.03490746021270752
Batch 7/64 loss: 0.02771627902984619
Batch 8/64 loss: 0.028381943702697754
Batch 9/64 loss: 0.017581582069396973
Batch 10/64 loss: 0.020399868488311768
Batch 11/64 loss: 0.02885258197784424
Batch 12/64 loss: 0.006096243858337402
Batch 13/64 loss: 0.018195748329162598
Batch 14/64 loss: 0.015591084957122803
Batch 15/64 loss: 0.027660787105560303
Batch 16/64 loss: 0.01713693141937256
Batch 17/64 loss: 0.005645155906677246
Batch 18/64 loss: 0.04379528760910034
Batch 19/64 loss: 0.0362204909324646
Batch 20/64 loss: 0.03682076930999756
Batch 21/64 loss: 0.03762704133987427
Batch 22/64 loss: 0.04169493913650513
Batch 23/64 loss: 0.024121999740600586
Batch 24/64 loss: 0.034721434116363525
Batch 25/64 loss: 0.030690491199493408
Batch 26/64 loss: 0.02422654628753662
Batch 27/64 loss: 0.018939614295959473
Batch 28/64 loss: 0.036948204040527344
Batch 29/64 loss: 0.037926554679870605
Batch 30/64 loss: 0.007588982582092285
Batch 31/64 loss: 0.024719715118408203
Batch 32/64 loss: 0.021527647972106934
Batch 33/64 loss: 0.028281569480895996
Batch 34/64 loss: 0.010385632514953613
Batch 35/64 loss: 0.002154052257537842
Batch 36/64 loss: -0.0006740093231201172
Batch 37/64 loss: 0.017891526222229004
Batch 38/64 loss: 0.027210891246795654
Batch 39/64 loss: 0.04914158582687378
Batch 40/64 loss: 0.03271925449371338
Batch 41/64 loss: 0.021392226219177246
Batch 42/64 loss: 0.007985293865203857
Batch 43/64 loss: 0.039292335510253906
Batch 44/64 loss: 0.018106698989868164
Batch 45/64 loss: 0.022997140884399414
Batch 46/64 loss: 0.041246891021728516
Batch 47/64 loss: 0.025263309478759766
Batch 48/64 loss: 0.0142173171043396
Batch 49/64 loss: 0.028891563415527344
Batch 50/64 loss: 0.008552014827728271
Batch 51/64 loss: 0.02068406343460083
Batch 52/64 loss: 0.04902291297912598
Batch 53/64 loss: 0.023633062839508057
Batch 54/64 loss: 0.0035740137100219727
Batch 55/64 loss: -0.0031969547271728516
Batch 56/64 loss: 0.04874575138092041
Batch 57/64 loss: 0.012243986129760742
Batch 58/64 loss: 0.052217841148376465
Batch 59/64 loss: 0.04081684350967407
Batch 60/64 loss: -0.005753874778747559
Batch 61/64 loss: 0.012607872486114502
Batch 62/64 loss: 0.009942054748535156
Batch 63/64 loss: 0.007967114448547363
Batch 64/64 loss: 0.019402027130126953
Epoch 376  Train loss: 0.023363503287820254  Val loss: 0.11838660719468422
Epoch 377
-------------------------------
Batch 1/64 loss: 0.014530003070831299
Batch 2/64 loss: 0.04016077518463135
Batch 3/64 loss: 0.008177399635314941
Batch 4/64 loss: -0.0037643909454345703
Batch 5/64 loss: 0.00642162561416626
Batch 6/64 loss: 0.03136235475540161
Batch 7/64 loss: 0.041470885276794434
Batch 8/64 loss: -0.01005852222442627
Batch 9/64 loss: 0.014350950717926025
Batch 10/64 loss: 0.003115415573120117
Batch 11/64 loss: 0.024471700191497803
Batch 12/64 loss: 0.0028533339500427246
Batch 13/64 loss: 0.016652345657348633
Batch 14/64 loss: 0.0035715103149414062
Batch 15/64 loss: 0.03662848472595215
Batch 16/64 loss: 0.016872107982635498
Batch 17/64 loss: 0.034384310245513916
Batch 18/64 loss: -0.004969477653503418
Batch 19/64 loss: 0.04531705379486084
Batch 20/64 loss: -0.0039539337158203125
Batch 21/64 loss: 0.007782697677612305
Batch 22/64 loss: 0.0031903982162475586
Batch 23/64 loss: 0.03261256217956543
Batch 24/64 loss: 0.03558725118637085
Batch 25/64 loss: 0.022142231464385986
Batch 26/64 loss: 0.019510388374328613
Batch 27/64 loss: 0.009266853332519531
Batch 28/64 loss: 0.019126057624816895
Batch 29/64 loss: 0.011246681213378906
Batch 30/64 loss: 0.020646393299102783
Batch 31/64 loss: 0.03917485475540161
Batch 32/64 loss: 0.001735985279083252
Batch 33/64 loss: 0.015254974365234375
Batch 34/64 loss: 0.03753781318664551
Batch 35/64 loss: 0.05076909065246582
Batch 36/64 loss: 0.01892000436782837
Batch 37/64 loss: 0.00979602336883545
Batch 38/64 loss: 0.03904300928115845
Batch 39/64 loss: 0.011907517910003662
Batch 40/64 loss: 0.072390615940094
Batch 41/64 loss: 0.00830066204071045
Batch 42/64 loss: 0.043854713439941406
Batch 43/64 loss: 0.036872029304504395
Batch 44/64 loss: 0.018225252628326416
Batch 45/64 loss: 0.029278218746185303
Batch 46/64 loss: 0.01307898759841919
Batch 47/64 loss: 0.00335615873336792
Batch 48/64 loss: 0.00218963623046875
Batch 49/64 loss: 0.01612699031829834
Batch 50/64 loss: 0.026358604431152344
Batch 51/64 loss: 0.02989429235458374
Batch 52/64 loss: 0.04578399658203125
Batch 53/64 loss: 0.034683823585510254
Batch 54/64 loss: 0.043599486351013184
Batch 55/64 loss: 0.040046751499176025
Batch 56/64 loss: 0.029428839683532715
Batch 57/64 loss: 0.041469693183898926
Batch 58/64 loss: 0.044281959533691406
Batch 59/64 loss: 0.016723275184631348
Batch 60/64 loss: 0.02724003791809082
Batch 61/64 loss: 0.051681458950042725
Batch 62/64 loss: 0.029075324535369873
Batch 63/64 loss: -0.016004204750061035
Batch 64/64 loss: 0.027353525161743164
Epoch 377  Train loss: 0.02245170929852654  Val loss: 0.11688891841783557
Epoch 378
-------------------------------
Batch 1/64 loss: 0.024334371089935303
Batch 2/64 loss: 0.023617148399353027
Batch 3/64 loss: 0.012626707553863525
Batch 4/64 loss: -0.0033226609230041504
Batch 5/64 loss: 0.04412561655044556
Batch 6/64 loss: 0.0039708614349365234
Batch 7/64 loss: 0.011160433292388916
Batch 8/64 loss: 0.01310819387435913
Batch 9/64 loss: 0.03585255146026611
Batch 10/64 loss: 0.006351590156555176
Batch 11/64 loss: 0.027332961559295654
Batch 12/64 loss: 0.04105675220489502
Batch 13/64 loss: 0.0006361603736877441
Batch 14/64 loss: 0.014927566051483154
Batch 15/64 loss: 0.03866541385650635
Batch 16/64 loss: 0.024462103843688965
Batch 17/64 loss: 0.054248929023742676
Batch 18/64 loss: 0.03337818384170532
Batch 19/64 loss: 0.054010629653930664
Batch 20/64 loss: 0.0011965632438659668
Batch 21/64 loss: 0.007803916931152344
Batch 22/64 loss: 0.07838451862335205
Batch 23/64 loss: 0.030797243118286133
Batch 24/64 loss: 0.022412657737731934
Batch 25/64 loss: 0.024943828582763672
Batch 26/64 loss: 0.012648046016693115
Batch 27/64 loss: 0.011404275894165039
Batch 28/64 loss: 0.06693011522293091
Batch 29/64 loss: 0.026687324047088623
Batch 30/64 loss: 0.014796853065490723
Batch 31/64 loss: 0.014548778533935547
Batch 32/64 loss: 0.023562967777252197
Batch 33/64 loss: 0.05298429727554321
Batch 34/64 loss: 0.015525341033935547
Batch 35/64 loss: 0.012910068035125732
Batch 36/64 loss: 0.026032686233520508
Batch 37/64 loss: 0.025598883628845215
Batch 38/64 loss: 0.010121703147888184
Batch 39/64 loss: 0.03547847270965576
Batch 40/64 loss: 0.026731252670288086
Batch 41/64 loss: 0.011012732982635498
Batch 42/64 loss: 0.026508092880249023
Batch 43/64 loss: 0.011865317821502686
Batch 44/64 loss: 0.04455530643463135
Batch 45/64 loss: -0.004190206527709961
Batch 46/64 loss: 0.021236658096313477
Batch 47/64 loss: 0.033083975315093994
Batch 48/64 loss: 0.02546548843383789
Batch 49/64 loss: 0.020069599151611328
Batch 50/64 loss: 0.004399418830871582
Batch 51/64 loss: 0.01586294174194336
Batch 52/64 loss: 0.01828819513320923
Batch 53/64 loss: 0.044116973876953125
Batch 54/64 loss: 0.014461040496826172
Batch 55/64 loss: 0.008330464363098145
Batch 56/64 loss: 0.02712005376815796
Batch 57/64 loss: 0.06396329402923584
Batch 58/64 loss: 0.049751102924346924
Batch 59/64 loss: 0.0020164847373962402
Batch 60/64 loss: 0.011021554470062256
Batch 61/64 loss: 0.02695286273956299
Batch 62/64 loss: 0.005404531955718994
Batch 63/64 loss: 0.006338834762573242
Batch 64/64 loss: 0.017740607261657715
Epoch 378  Train loss: 0.02357621146183388  Val loss: 0.11577763078139
Epoch 379
-------------------------------
Batch 1/64 loss: 0.020908355712890625
Batch 2/64 loss: 0.04771322011947632
Batch 3/64 loss: 0.03279006481170654
Batch 4/64 loss: 0.03736460208892822
Batch 5/64 loss: 0.028281688690185547
Batch 6/64 loss: -0.0020418763160705566
Batch 7/64 loss: 0.011387050151824951
Batch 8/64 loss: 0.005172133445739746
Batch 9/64 loss: 0.030547261238098145
Batch 10/64 loss: 0.041685402393341064
Batch 11/64 loss: 0.017129838466644287
Batch 12/64 loss: 0.05656522512435913
Batch 13/64 loss: 0.00033032894134521484
Batch 14/64 loss: 0.030075550079345703
Batch 15/64 loss: 0.01783663034439087
Batch 16/64 loss: 0.029541492462158203
Batch 17/64 loss: -0.0010872483253479004
Batch 18/64 loss: 0.03335988521575928
Batch 19/64 loss: 0.046623945236206055
Batch 20/64 loss: 0.03039950132369995
Batch 21/64 loss: 0.028962314128875732
Batch 22/64 loss: 0.03518432378768921
Batch 23/64 loss: 0.04469197988510132
Batch 24/64 loss: 0.03464353084564209
Batch 25/64 loss: 0.003180861473083496
Batch 26/64 loss: 0.026684284210205078
Batch 27/64 loss: 0.01897740364074707
Batch 28/64 loss: 0.0564579963684082
Batch 29/64 loss: 0.04226773977279663
Batch 30/64 loss: 0.018944501876831055
Batch 31/64 loss: 0.060359835624694824
Batch 32/64 loss: 0.005437016487121582
Batch 33/64 loss: -0.0013769268989562988
Batch 34/64 loss: 0.016550779342651367
Batch 35/64 loss: 0.00917816162109375
Batch 36/64 loss: 0.007777512073516846
Batch 37/64 loss: -0.00392073392868042
Batch 38/64 loss: -7.349252700805664e-05
Batch 39/64 loss: -0.007726550102233887
Batch 40/64 loss: 0.07973003387451172
Batch 41/64 loss: 0.01496589183807373
Batch 42/64 loss: 0.007729589939117432
Batch 43/64 loss: 0.03031688928604126
Batch 44/64 loss: -0.008164703845977783
Batch 45/64 loss: 0.029552102088928223
Batch 46/64 loss: 0.01599860191345215
Batch 47/64 loss: 0.032575130462646484
Batch 48/64 loss: 0.04779893159866333
Batch 49/64 loss: 0.006686091423034668
Batch 50/64 loss: 0.014664649963378906
Batch 51/64 loss: 0.01682990789413452
Batch 52/64 loss: 0.027868330478668213
Batch 53/64 loss: 0.023498713970184326
Batch 54/64 loss: 0.031160175800323486
Batch 55/64 loss: 0.029383718967437744
Batch 56/64 loss: 0.00859522819519043
Batch 57/64 loss: -0.011146306991577148
Batch 58/64 loss: 0.05645585060119629
Batch 59/64 loss: -0.008025705814361572
Batch 60/64 loss: 0.013799309730529785
Batch 61/64 loss: 0.018658459186553955
Batch 62/64 loss: 0.033389389514923096
Batch 63/64 loss: 0.05459952354431152
Batch 64/64 loss: -0.0028658509254455566
Epoch 379  Train loss: 0.023145945165671554  Val loss: 0.11879865758607477
Epoch 380
-------------------------------
Batch 1/64 loss: 0.045492708683013916
Batch 2/64 loss: 0.013798415660858154
Batch 3/64 loss: 0.011350870132446289
Batch 4/64 loss: 0.013940155506134033
Batch 5/64 loss: 0.0037497282028198242
Batch 6/64 loss: 0.0074675679206848145
Batch 7/64 loss: 0.011323750019073486
Batch 8/64 loss: 0.02955949306488037
Batch 9/64 loss: 0.05191147327423096
Batch 10/64 loss: 0.00302731990814209
Batch 11/64 loss: 0.021966278553009033
Batch 12/64 loss: 0.010324478149414062
Batch 13/64 loss: 0.021094918251037598
Batch 14/64 loss: 0.021472573280334473
Batch 15/64 loss: -0.017742156982421875
Batch 16/64 loss: 0.04208040237426758
Batch 17/64 loss: 0.03462904691696167
Batch 18/64 loss: -0.0020917654037475586
Batch 19/64 loss: 0.020677030086517334
Batch 20/64 loss: 0.0102081298828125
Batch 21/64 loss: -0.014296770095825195
Batch 22/64 loss: -0.009535789489746094
Batch 23/64 loss: 0.009006261825561523
Batch 24/64 loss: 0.029823243618011475
Batch 25/64 loss: -0.023155272006988525
Batch 26/64 loss: 0.0075691938400268555
Batch 27/64 loss: 0.04654824733734131
Batch 28/64 loss: 0.024333596229553223
Batch 29/64 loss: 0.039030492305755615
Batch 30/64 loss: 0.016753196716308594
Batch 31/64 loss: 0.009480953216552734
Batch 32/64 loss: 0.021394193172454834
Batch 33/64 loss: 0.021875321865081787
Batch 34/64 loss: 0.03204399347305298
Batch 35/64 loss: 0.020977437496185303
Batch 36/64 loss: 0.0191725492477417
Batch 37/64 loss: 0.016612470149993896
Batch 38/64 loss: 0.02360713481903076
Batch 39/64 loss: 0.03518402576446533
Batch 40/64 loss: 0.03460538387298584
Batch 41/64 loss: 0.015620648860931396
Batch 42/64 loss: 0.04651516675949097
Batch 43/64 loss: 0.012272894382476807
Batch 44/64 loss: 0.01641947031021118
Batch 45/64 loss: 0.013070762157440186
Batch 46/64 loss: 0.017857789993286133
Batch 47/64 loss: 0.057163357734680176
Batch 48/64 loss: 0.0295562744140625
Batch 49/64 loss: 0.021768152713775635
Batch 50/64 loss: 0.03715384006500244
Batch 51/64 loss: 0.04145336151123047
Batch 52/64 loss: -0.017892956733703613
Batch 53/64 loss: 0.03132641315460205
Batch 54/64 loss: 0.01244509220123291
Batch 55/64 loss: 0.028272807598114014
Batch 56/64 loss: 0.03495150804519653
Batch 57/64 loss: 0.027077317237854004
Batch 58/64 loss: 0.026864945888519287
Batch 59/64 loss: 0.020711302757263184
Batch 60/64 loss: 0.03610217571258545
Batch 61/64 loss: 0.03187870979309082
Batch 62/64 loss: 0.008450925350189209
Batch 63/64 loss: 0.06215798854827881
Batch 64/64 loss: 0.004087865352630615
Epoch 380  Train loss: 0.020855437306796804  Val loss: 0.12319246968862527
Epoch 381
-------------------------------
Batch 1/64 loss: 0.006605982780456543
Batch 2/64 loss: 0.03286963701248169
Batch 3/64 loss: 0.04314863681793213
Batch 4/64 loss: 0.012391388416290283
Batch 5/64 loss: 0.01683628559112549
Batch 6/64 loss: 0.0028322935104370117
Batch 7/64 loss: 0.023183465003967285
Batch 8/64 loss: 0.016146183013916016
Batch 9/64 loss: 0.018916726112365723
Batch 10/64 loss: 0.025061607360839844
Batch 11/64 loss: 0.02030855417251587
Batch 12/64 loss: -0.0001399517059326172
Batch 13/64 loss: 0.029322266578674316
Batch 14/64 loss: 0.007316529750823975
Batch 15/64 loss: 0.03483325242996216
Batch 16/64 loss: 0.029725372791290283
Batch 17/64 loss: 0.05065596103668213
Batch 18/64 loss: 0.00793081521987915
Batch 19/64 loss: -0.0018252134323120117
Batch 20/64 loss: 0.02989029884338379
Batch 21/64 loss: 0.012844443321228027
Batch 22/64 loss: 0.018261969089508057
Batch 23/64 loss: 0.039196550846099854
Batch 24/64 loss: 0.02840244770050049
Batch 25/64 loss: 0.026546716690063477
Batch 26/64 loss: 0.01834893226623535
Batch 27/64 loss: 0.04110687971115112
Batch 28/64 loss: 0.017941951751708984
Batch 29/64 loss: 0.022580206394195557
Batch 30/64 loss: 0.02326720952987671
Batch 31/64 loss: 0.018518447875976562
Batch 32/64 loss: 0.01348876953125
Batch 33/64 loss: 0.028386354446411133
Batch 34/64 loss: 0.023288965225219727
Batch 35/64 loss: 0.01694697141647339
Batch 36/64 loss: 0.02323251962661743
Batch 37/64 loss: 0.039320945739746094
Batch 38/64 loss: 0.06118977069854736
Batch 39/64 loss: 0.009324967861175537
Batch 40/64 loss: 8.732080459594727e-05
Batch 41/64 loss: 0.029020190238952637
Batch 42/64 loss: 0.03949838876724243
Batch 43/64 loss: 0.02230280637741089
Batch 44/64 loss: 0.03363293409347534
Batch 45/64 loss: 0.02495741844177246
Batch 46/64 loss: 0.031951308250427246
Batch 47/64 loss: 0.01364433765411377
Batch 48/64 loss: 0.00473475456237793
Batch 49/64 loss: 0.012457847595214844
Batch 50/64 loss: 0.033970415592193604
Batch 51/64 loss: 0.04925614595413208
Batch 52/64 loss: 0.030406057834625244
Batch 53/64 loss: 0.014612078666687012
Batch 54/64 loss: 0.010187864303588867
Batch 55/64 loss: 0.04630887508392334
Batch 56/64 loss: 0.01488041877746582
Batch 57/64 loss: 0.02689671516418457
Batch 58/64 loss: 0.013106584548950195
Batch 59/64 loss: 0.03957563638687134
Batch 60/64 loss: 0.045290350914001465
Batch 61/64 loss: 0.016622424125671387
Batch 62/64 loss: 0.028639912605285645
Batch 63/64 loss: 0.024181365966796875
Batch 64/64 loss: 0.028813183307647705
Epoch 381  Train loss: 0.023780974921058205  Val loss: 0.11803610996692042
Epoch 382
-------------------------------
Batch 1/64 loss: 0.012436389923095703
Batch 2/64 loss: 0.03188621997833252
Batch 3/64 loss: 0.008394360542297363
Batch 4/64 loss: 0.032654523849487305
Batch 5/64 loss: 0.024842500686645508
Batch 6/64 loss: 0.005325794219970703
Batch 7/64 loss: 0.021604180335998535
Batch 8/64 loss: 0.03059631586074829
Batch 9/64 loss: 0.0013938546180725098
Batch 10/64 loss: -0.001201033592224121
Batch 11/64 loss: 0.00694119930267334
Batch 12/64 loss: 0.018939852714538574
Batch 13/64 loss: 0.05772852897644043
Batch 14/64 loss: 0.00432354211807251
Batch 15/64 loss: -0.012967705726623535
Batch 16/64 loss: 0.03860175609588623
Batch 17/64 loss: 0.03739994764328003
Batch 18/64 loss: 0.030598700046539307
Batch 19/64 loss: 0.03199237585067749
Batch 20/64 loss: 0.021461188793182373
Batch 21/64 loss: 0.02513730525970459
Batch 22/64 loss: 0.02417975664138794
Batch 23/64 loss: 0.05208379030227661
Batch 24/64 loss: 0.036072373390197754
Batch 25/64 loss: 0.001278698444366455
Batch 26/64 loss: 0.007704019546508789
Batch 27/64 loss: -0.005070090293884277
Batch 28/64 loss: 0.035289227962493896
Batch 29/64 loss: 0.010587871074676514
Batch 30/64 loss: 0.008855342864990234
Batch 31/64 loss: 0.007024943828582764
Batch 32/64 loss: 0.01944655179977417
Batch 33/64 loss: 0.0373913049697876
Batch 34/64 loss: 0.013972759246826172
Batch 35/64 loss: 0.020697414875030518
Batch 36/64 loss: 0.041180431842803955
Batch 37/64 loss: 0.009666919708251953
Batch 38/64 loss: 0.014734148979187012
Batch 39/64 loss: 0.025589704513549805
Batch 40/64 loss: 0.036608755588531494
Batch 41/64 loss: 0.0019178390502929688
Batch 42/64 loss: 0.008808135986328125
Batch 43/64 loss: 0.04272669553756714
Batch 44/64 loss: 0.027027428150177002
Batch 45/64 loss: 0.02628713846206665
Batch 46/64 loss: 0.015299975872039795
Batch 47/64 loss: 0.02173715829849243
Batch 48/64 loss: 0.03658252954483032
Batch 49/64 loss: -0.007390320301055908
Batch 50/64 loss: 0.05835616588592529
Batch 51/64 loss: 0.03705167770385742
Batch 52/64 loss: 0.029107332229614258
Batch 53/64 loss: 0.0034970641136169434
Batch 54/64 loss: 0.018662214279174805
Batch 55/64 loss: 0.04480695724487305
Batch 56/64 loss: 0.033070504665374756
Batch 57/64 loss: 0.0005830526351928711
Batch 58/64 loss: 0.03430783748626709
Batch 59/64 loss: 0.010280251502990723
Batch 60/64 loss: 0.052206575870513916
Batch 61/64 loss: 0.016218364238739014
Batch 62/64 loss: 0.03499811887741089
Batch 63/64 loss: 0.021815359592437744
Batch 64/64 loss: 0.023180246353149414
Epoch 382  Train loss: 0.022129050423117243  Val loss: 0.11940492223628198
Epoch 383
-------------------------------
Batch 1/64 loss: 0.023604154586791992
Batch 2/64 loss: 0.00993359088897705
Batch 3/64 loss: 0.03461426496505737
Batch 4/64 loss: 0.07726675271987915
Batch 5/64 loss: -0.002971649169921875
Batch 6/64 loss: 0.02780616283416748
Batch 7/64 loss: 0.01836717128753662
Batch 8/64 loss: 0.01446765661239624
Batch 9/64 loss: 0.043448448181152344
Batch 10/64 loss: 0.03892529010772705
Batch 11/64 loss: 0.008229732513427734
Batch 12/64 loss: 0.011564016342163086
Batch 13/64 loss: 0.04488652944564819
Batch 14/64 loss: 0.005950987339019775
Batch 15/64 loss: -0.011888623237609863
Batch 16/64 loss: 0.009540021419525146
Batch 17/64 loss: 0.015429496765136719
Batch 18/64 loss: 0.01877009868621826
Batch 19/64 loss: 0.018056154251098633
Batch 20/64 loss: 0.06356978416442871
Batch 21/64 loss: 0.033129096031188965
Batch 22/64 loss: 0.006232023239135742
Batch 23/64 loss: 0.031141579151153564
Batch 24/64 loss: 0.024542510509490967
Batch 25/64 loss: 0.05228149890899658
Batch 26/64 loss: 0.026052653789520264
Batch 27/64 loss: 0.045426249504089355
Batch 28/64 loss: 0.015545666217803955
Batch 29/64 loss: 0.01776975393295288
Batch 30/64 loss: 0.014668464660644531
Batch 31/64 loss: 0.02223670482635498
Batch 32/64 loss: -0.02261584997177124
Batch 33/64 loss: 0.001406550407409668
Batch 34/64 loss: 0.0089377760887146
Batch 35/64 loss: 0.021403074264526367
Batch 36/64 loss: 0.012162625789642334
Batch 37/64 loss: 0.02194535732269287
Batch 38/64 loss: 0.016115427017211914
Batch 39/64 loss: 0.02235567569732666
Batch 40/64 loss: 0.038279831409454346
Batch 41/64 loss: 0.0055792927742004395
Batch 42/64 loss: 0.016781091690063477
Batch 43/64 loss: -0.0008199214935302734
Batch 44/64 loss: 0.01420736312866211
Batch 45/64 loss: 0.017730891704559326
Batch 46/64 loss: 0.025568068027496338
Batch 47/64 loss: 0.038285255432128906
Batch 48/64 loss: 0.0156862735748291
Batch 49/64 loss: 0.01520925760269165
Batch 50/64 loss: 0.004154860973358154
Batch 51/64 loss: 0.024035871028900146
Batch 52/64 loss: 0.01709115505218506
Batch 53/64 loss: 0.0011335015296936035
Batch 54/64 loss: 0.024133145809173584
Batch 55/64 loss: 0.024146199226379395
Batch 56/64 loss: 0.039449214935302734
Batch 57/64 loss: 0.014267802238464355
Batch 58/64 loss: 0.023800015449523926
Batch 59/64 loss: 0.007835865020751953
Batch 60/64 loss: 0.030306696891784668
Batch 61/64 loss: 0.03632640838623047
Batch 62/64 loss: 0.0371776819229126
Batch 63/64 loss: 0.037284672260284424
Batch 64/64 loss: 0.05951577425003052
Epoch 383  Train loss: 0.021844614954555735  Val loss: 0.11928547862469126
Epoch 384
-------------------------------
Batch 1/64 loss: 0.038696348667144775
Batch 2/64 loss: 0.006307244300842285
Batch 3/64 loss: 0.03919410705566406
Batch 4/64 loss: 0.008086919784545898
Batch 5/64 loss: -0.017359375953674316
Batch 6/64 loss: 0.024985134601593018
Batch 7/64 loss: 0.02042907476425171
Batch 8/64 loss: 0.014558792114257812
Batch 9/64 loss: 0.009178340435028076
Batch 10/64 loss: 0.003354370594024658
Batch 11/64 loss: 0.04662233591079712
Batch 12/64 loss: 0.013527274131774902
Batch 13/64 loss: 0.035733938217163086
Batch 14/64 loss: 0.028959989547729492
Batch 15/64 loss: 0.043117523193359375
Batch 16/64 loss: 0.00648951530456543
Batch 17/64 loss: 0.02276986837387085
Batch 18/64 loss: 0.028068602085113525
Batch 19/64 loss: 0.015922725200653076
Batch 20/64 loss: 0.003972291946411133
Batch 21/64 loss: 0.05028343200683594
Batch 22/64 loss: -0.0006647109985351562
Batch 23/64 loss: -0.006253421306610107
Batch 24/64 loss: 0.024202466011047363
Batch 25/64 loss: 0.004349052906036377
Batch 26/64 loss: 0.021618545055389404
Batch 27/64 loss: 0.03247499465942383
Batch 28/64 loss: 0.033506035804748535
Batch 29/64 loss: 0.021988630294799805
Batch 30/64 loss: 0.046443283557891846
Batch 31/64 loss: 0.017788052558898926
Batch 32/64 loss: 0.02970021963119507
Batch 33/64 loss: 0.023157775402069092
Batch 34/64 loss: 0.017609238624572754
Batch 35/64 loss: 0.03182166814804077
Batch 36/64 loss: 0.040279388427734375
Batch 37/64 loss: 0.007489919662475586
Batch 38/64 loss: 0.03789854049682617
Batch 39/64 loss: 0.03150582313537598
Batch 40/64 loss: 0.021593213081359863
Batch 41/64 loss: 0.02415645122528076
Batch 42/64 loss: 0.039697349071502686
Batch 43/64 loss: 0.0334247350692749
Batch 44/64 loss: 0.03402918577194214
Batch 45/64 loss: 0.02039259672164917
Batch 46/64 loss: 0.014937520027160645
Batch 47/64 loss: 0.013355851173400879
Batch 48/64 loss: 0.027076244354248047
Batch 49/64 loss: -0.0006707906723022461
Batch 50/64 loss: 0.02297508716583252
Batch 51/64 loss: 0.04100608825683594
Batch 52/64 loss: 0.05464047193527222
Batch 53/64 loss: 0.0003210902214050293
Batch 54/64 loss: 0.0323868989944458
Batch 55/64 loss: -0.013969361782073975
Batch 56/64 loss: 0.03252100944519043
Batch 57/64 loss: 0.03018951416015625
Batch 58/64 loss: 0.005640506744384766
Batch 59/64 loss: 0.019379854202270508
Batch 60/64 loss: -0.020544350147247314
Batch 61/64 loss: 0.01784569025039673
Batch 62/64 loss: 0.05917644500732422
Batch 63/64 loss: 0.03856390714645386
Batch 64/64 loss: 0.0203363299369812
Epoch 384  Train loss: 0.022293198576160505  Val loss: 0.1171501846247932
Epoch 385
-------------------------------
Batch 1/64 loss: 0.012662529945373535
Batch 2/64 loss: 0.029963314533233643
Batch 3/64 loss: 0.014019191265106201
Batch 4/64 loss: 0.05370962619781494
Batch 5/64 loss: 0.02677905559539795
Batch 6/64 loss: -0.00045543909072875977
Batch 7/64 loss: 0.013928532600402832
Batch 8/64 loss: 0.024919569492340088
Batch 9/64 loss: 0.035648226737976074
Batch 10/64 loss: -0.012955665588378906
Batch 11/64 loss: 0.006889939308166504
Batch 12/64 loss: 0.017209768295288086
Batch 13/64 loss: 0.012548208236694336
Batch 14/64 loss: 0.018948853015899658
Batch 15/64 loss: 0.021475791931152344
Batch 16/64 loss: 0.020189404487609863
Batch 17/64 loss: 0.0052506327629089355
Batch 18/64 loss: -0.01575756072998047
Batch 19/64 loss: 0.010178089141845703
Batch 20/64 loss: 0.01002812385559082
Batch 21/64 loss: -0.005397915840148926
Batch 22/64 loss: 0.027046382427215576
Batch 23/64 loss: 0.024377882480621338
Batch 24/64 loss: 0.04484152793884277
Batch 25/64 loss: 0.005849778652191162
Batch 26/64 loss: 0.006339371204376221
Batch 27/64 loss: 0.013918101787567139
Batch 28/64 loss: -0.0031846165657043457
Batch 29/64 loss: 0.00728076696395874
Batch 30/64 loss: 0.046253085136413574
Batch 31/64 loss: 0.016101598739624023
Batch 32/64 loss: 0.016378402709960938
Batch 33/64 loss: 0.005990147590637207
Batch 34/64 loss: 0.01773226261138916
Batch 35/64 loss: 0.003819704055786133
Batch 36/64 loss: 0.06206321716308594
Batch 37/64 loss: 0.01098710298538208
Batch 38/64 loss: 0.028546035289764404
Batch 39/64 loss: 0.015841305255889893
Batch 40/64 loss: 0.0437769889831543
Batch 41/64 loss: -0.005636334419250488
Batch 42/64 loss: 0.025332629680633545
Batch 43/64 loss: 0.026940345764160156
Batch 44/64 loss: 0.0073964595794677734
Batch 45/64 loss: 0.04736912250518799
Batch 46/64 loss: 0.05240046977996826
Batch 47/64 loss: 0.0421711802482605
Batch 48/64 loss: 0.022913634777069092
Batch 49/64 loss: 0.028791487216949463
Batch 50/64 loss: -0.0008051395416259766
Batch 51/64 loss: 0.024888813495635986
Batch 52/64 loss: 0.04166567325592041
Batch 53/64 loss: 0.03938436508178711
Batch 54/64 loss: -0.004272580146789551
Batch 55/64 loss: 0.01073545217514038
Batch 56/64 loss: 0.017629623413085938
Batch 57/64 loss: 0.01801246404647827
Batch 58/64 loss: 0.0367431640625
Batch 59/64 loss: 0.02393805980682373
Batch 60/64 loss: 0.03573250770568848
Batch 61/64 loss: 0.0017865896224975586
Batch 62/64 loss: 0.04218482971191406
Batch 63/64 loss: 0.018176555633544922
Batch 64/64 loss: 0.06345474720001221
Epoch 385  Train loss: 0.02031077263402004  Val loss: 0.11721294155645207
Epoch 386
-------------------------------
Batch 1/64 loss: 0.02294909954071045
Batch 2/64 loss: 0.026258468627929688
Batch 3/64 loss: 0.030114293098449707
Batch 4/64 loss: 0.010095953941345215
Batch 5/64 loss: 0.01814359426498413
Batch 6/64 loss: 8.13603401184082e-05
Batch 7/64 loss: 0.05933099985122681
Batch 8/64 loss: 0.022612690925598145
Batch 9/64 loss: -0.003191649913787842
Batch 10/64 loss: -0.005427539348602295
Batch 11/64 loss: 0.025019288063049316
Batch 12/64 loss: -0.0006036758422851562
Batch 13/64 loss: 0.024926304817199707
Batch 14/64 loss: 0.01816427707672119
Batch 15/64 loss: 0.021448850631713867
Batch 16/64 loss: 0.02284395694732666
Batch 17/64 loss: 0.038291215896606445
Batch 18/64 loss: -0.000447690486907959
Batch 19/64 loss: 0.004232466220855713
Batch 20/64 loss: 0.011101067066192627
Batch 21/64 loss: 0.024829089641571045
Batch 22/64 loss: 0.0120924711227417
Batch 23/64 loss: 0.02563035488128662
Batch 24/64 loss: 0.0327109694480896
Batch 25/64 loss: 0.034259140491485596
Batch 26/64 loss: -0.003235459327697754
Batch 27/64 loss: -0.0008101463317871094
Batch 28/64 loss: 0.01124650239944458
Batch 29/64 loss: 0.02683579921722412
Batch 30/64 loss: 0.005278348922729492
Batch 31/64 loss: 0.02298879623413086
Batch 32/64 loss: 0.0035879015922546387
Batch 33/64 loss: 0.03052520751953125
Batch 34/64 loss: 0.04671144485473633
Batch 35/64 loss: 0.012856364250183105
Batch 36/64 loss: 0.024830996990203857
Batch 37/64 loss: 0.017861604690551758
Batch 38/64 loss: 0.014342546463012695
Batch 39/64 loss: -3.641843795776367e-05
Batch 40/64 loss: 0.031132936477661133
Batch 41/64 loss: 0.029262423515319824
Batch 42/64 loss: 0.0385587215423584
Batch 43/64 loss: 0.011961162090301514
Batch 44/64 loss: 0.0384182333946228
Batch 45/64 loss: 0.02426820993423462
Batch 46/64 loss: 0.026776909828186035
Batch 47/64 loss: 0.03184014558792114
Batch 48/64 loss: 0.03414076566696167
Batch 49/64 loss: 0.043084681034088135
Batch 50/64 loss: 0.026026904582977295
Batch 51/64 loss: -0.005166590213775635
Batch 52/64 loss: 0.03553074598312378
Batch 53/64 loss: -0.010483086109161377
Batch 54/64 loss: 0.025385797023773193
Batch 55/64 loss: 0.018634557723999023
Batch 56/64 loss: 0.023795783519744873
Batch 57/64 loss: 0.03290140628814697
Batch 58/64 loss: 0.04174536466598511
Batch 59/64 loss: 0.02003347873687744
Batch 60/64 loss: 0.025365114212036133
Batch 61/64 loss: 0.020711660385131836
Batch 62/64 loss: 0.0211714506149292
Batch 63/64 loss: 0.040564000606536865
Batch 64/64 loss: 0.034148454666137695
Epoch 386  Train loss: 0.021015230814615884  Val loss: 0.1178992212433176
Epoch 387
-------------------------------
Batch 1/64 loss: 0.03193044662475586
Batch 2/64 loss: 0.012323439121246338
Batch 3/64 loss: 0.031477272510528564
Batch 4/64 loss: 0.04583626985549927
Batch 5/64 loss: 0.024733006954193115
Batch 6/64 loss: 0.02503681182861328
Batch 7/64 loss: 0.02025759220123291
Batch 8/64 loss: 0.0016285181045532227
Batch 9/64 loss: 0.011215031147003174
Batch 10/64 loss: 0.025101304054260254
Batch 11/64 loss: -0.016562223434448242
Batch 12/64 loss: 0.016480624675750732
Batch 13/64 loss: 0.01643526554107666
Batch 14/64 loss: 0.009579360485076904
Batch 15/64 loss: 0.04288572072982788
Batch 16/64 loss: 0.0234946608543396
Batch 17/64 loss: -0.009248137474060059
Batch 18/64 loss: 0.03988879919052124
Batch 19/64 loss: 0.004336118698120117
Batch 20/64 loss: -0.010516524314880371
Batch 21/64 loss: 0.0362623929977417
Batch 22/64 loss: 0.02447301149368286
Batch 23/64 loss: 0.030569493770599365
Batch 24/64 loss: 0.005364537239074707
Batch 25/64 loss: -0.010693907737731934
Batch 26/64 loss: 0.04025214910507202
Batch 27/64 loss: 0.0020810365676879883
Batch 28/64 loss: -0.004036307334899902
Batch 29/64 loss: 0.024521946907043457
Batch 30/64 loss: -0.014919579029083252
Batch 31/64 loss: 0.02477031946182251
Batch 32/64 loss: 0.022782087326049805
Batch 33/64 loss: 0.030308067798614502
Batch 34/64 loss: 0.008652210235595703
Batch 35/64 loss: 0.024689912796020508
Batch 36/64 loss: 0.02441006898880005
Batch 37/64 loss: 0.04891347885131836
Batch 38/64 loss: -0.005203366279602051
Batch 39/64 loss: 0.04673820734024048
Batch 40/64 loss: 0.03480339050292969
Batch 41/64 loss: 0.02471768856048584
Batch 42/64 loss: 0.025070786476135254
Batch 43/64 loss: 0.012827575206756592
Batch 44/64 loss: 0.04396897554397583
Batch 45/64 loss: 0.03796875476837158
Batch 46/64 loss: 0.009281158447265625
Batch 47/64 loss: 0.017018914222717285
Batch 48/64 loss: 0.018872320652008057
Batch 49/64 loss: 0.026719748973846436
Batch 50/64 loss: 0.037310779094696045
Batch 51/64 loss: 0.018703937530517578
Batch 52/64 loss: 0.018450021743774414
Batch 53/64 loss: 0.01910257339477539
Batch 54/64 loss: 0.03738892078399658
Batch 55/64 loss: 0.055573105812072754
Batch 56/64 loss: 0.04171609878540039
Batch 57/64 loss: 0.013054132461547852
Batch 58/64 loss: 0.04020249843597412
Batch 59/64 loss: 0.007711589336395264
Batch 60/64 loss: 0.021161913871765137
Batch 61/64 loss: 0.02249777317047119
Batch 62/64 loss: 0.0049784183502197266
Batch 63/64 loss: 0.052472054958343506
Batch 64/64 loss: 0.062166035175323486
Epoch 387  Train loss: 0.021873674906936345  Val loss: 0.11951765469259412
Epoch 388
-------------------------------
Batch 1/64 loss: 0.017702162265777588
Batch 2/64 loss: 0.03870987892150879
Batch 3/64 loss: 0.0078696608543396
Batch 4/64 loss: 0.008263587951660156
Batch 5/64 loss: 0.03099656105041504
Batch 6/64 loss: 0.046919047832489014
Batch 7/64 loss: 0.024086475372314453
Batch 8/64 loss: 0.00027740001678466797
Batch 9/64 loss: 0.006112337112426758
Batch 10/64 loss: 0.01656728982925415
Batch 11/64 loss: 0.014110684394836426
Batch 12/64 loss: 0.025823771953582764
Batch 13/64 loss: 0.034398794174194336
Batch 14/64 loss: 0.011873364448547363
Batch 15/64 loss: 0.053209781646728516
Batch 16/64 loss: 0.020084083080291748
Batch 17/64 loss: 0.06715154647827148
Batch 18/64 loss: 0.035965025424957275
Batch 19/64 loss: 0.017420530319213867
Batch 20/64 loss: 0.00804603099822998
Batch 21/64 loss: 0.0339890718460083
Batch 22/64 loss: -0.0018259882926940918
Batch 23/64 loss: -0.017369389533996582
Batch 24/64 loss: 0.002772212028503418
Batch 25/64 loss: 0.022229671478271484
Batch 26/64 loss: 0.014612078666687012
Batch 27/64 loss: 0.007255613803863525
Batch 28/64 loss: 0.036650896072387695
Batch 29/64 loss: 0.006203949451446533
Batch 30/64 loss: 0.015417873859405518
Batch 31/64 loss: -0.012763381004333496
Batch 32/64 loss: 0.02254319190979004
Batch 33/64 loss: -0.005378007888793945
Batch 34/64 loss: 0.011160612106323242
Batch 35/64 loss: 0.011248767375946045
Batch 36/64 loss: 0.015159547328948975
Batch 37/64 loss: 0.015569031238555908
Batch 38/64 loss: 0.009493649005889893
Batch 39/64 loss: 0.03433489799499512
Batch 40/64 loss: 0.03161776065826416
Batch 41/64 loss: 0.04577082395553589
Batch 42/64 loss: 0.031298696994781494
Batch 43/64 loss: 0.037054598331451416
Batch 44/64 loss: 0.007306814193725586
Batch 45/64 loss: 0.028377175331115723
Batch 46/64 loss: -0.010496377944946289
Batch 47/64 loss: 0.019596219062805176
Batch 48/64 loss: 0.05594080686569214
Batch 49/64 loss: 0.03168243169784546
Batch 50/64 loss: 0.04838293790817261
Batch 51/64 loss: -0.002674877643585205
Batch 52/64 loss: 0.03277820348739624
Batch 53/64 loss: 0.019550561904907227
Batch 54/64 loss: -0.012812316417694092
Batch 55/64 loss: 0.013307631015777588
Batch 56/64 loss: 0.048069000244140625
Batch 57/64 loss: 0.02344411611557007
Batch 58/64 loss: 0.013238608837127686
Batch 59/64 loss: 0.039626896381378174
Batch 60/64 loss: -0.000991225242614746
Batch 61/64 loss: 0.017767369747161865
Batch 62/64 loss: 0.014349520206451416
Batch 63/64 loss: 0.05331510305404663
Batch 64/64 loss: 0.06645047664642334
Epoch 388  Train loss: 0.021054598396899653  Val loss: 0.11851090848241065
Epoch 389
-------------------------------
Batch 1/64 loss: 0.010248184204101562
Batch 2/64 loss: 0.030958592891693115
Batch 3/64 loss: 0.003362298011779785
Batch 4/64 loss: 0.004499375820159912
Batch 5/64 loss: 0.014502763748168945
Batch 6/64 loss: 0.014605879783630371
Batch 7/64 loss: 0.06306880712509155
Batch 8/64 loss: 0.006408572196960449
Batch 9/64 loss: 0.0070381760597229
Batch 10/64 loss: 0.004076361656188965
Batch 11/64 loss: 0.030786097049713135
Batch 12/64 loss: 0.012841105461120605
Batch 13/64 loss: 0.045455098152160645
Batch 14/64 loss: 0.03239583969116211
Batch 15/64 loss: 0.030705809593200684
Batch 16/64 loss: 0.041765570640563965
Batch 17/64 loss: -0.003046274185180664
Batch 18/64 loss: 0.04004126787185669
Batch 19/64 loss: 0.006913304328918457
Batch 20/64 loss: 0.020961463451385498
Batch 21/64 loss: 0.018846750259399414
Batch 22/64 loss: 0.046113669872283936
Batch 23/64 loss: 0.0182725191116333
Batch 24/64 loss: 0.031384170055389404
Batch 25/64 loss: 0.021573185920715332
Batch 26/64 loss: 0.02269977331161499
Batch 27/64 loss: -0.0007691383361816406
Batch 28/64 loss: 0.0327029824256897
Batch 29/64 loss: 0.008167743682861328
Batch 30/64 loss: 0.013626694679260254
Batch 31/64 loss: 0.037757158279418945
Batch 32/64 loss: 0.03508412837982178
Batch 33/64 loss: 0.019034981727600098
Batch 34/64 loss: -0.0030137300491333008
Batch 35/64 loss: -0.006786465644836426
Batch 36/64 loss: -5.066394805908203e-05
Batch 37/64 loss: 0.02519124746322632
Batch 38/64 loss: 0.039989590644836426
Batch 39/64 loss: 0.0034712553024291992
Batch 40/64 loss: 0.05733305215835571
Batch 41/64 loss: 0.03088432550430298
Batch 42/64 loss: 0.01430511474609375
Batch 43/64 loss: 0.0188826322555542
Batch 44/64 loss: 0.0369873046875
Batch 45/64 loss: 0.007812321186065674
Batch 46/64 loss: 0.034693002700805664
Batch 47/64 loss: 0.021937549114227295
Batch 48/64 loss: 0.011910557746887207
Batch 49/64 loss: 0.020431816577911377
Batch 50/64 loss: 0.020412147045135498
Batch 51/64 loss: 0.016547858715057373
Batch 52/64 loss: 0.03634744882583618
Batch 53/64 loss: 0.0021224617958068848
Batch 54/64 loss: 0.024103105068206787
Batch 55/64 loss: 0.0003476142883300781
Batch 56/64 loss: 0.03492021560668945
Batch 57/64 loss: 0.02628779411315918
Batch 58/64 loss: 0.02719748020172119
Batch 59/64 loss: 0.022309720516204834
Batch 60/64 loss: 0.041809022426605225
Batch 61/64 loss: 0.032241106033325195
Batch 62/64 loss: 0.011732757091522217
Batch 63/64 loss: 0.04314357042312622
Batch 64/64 loss: 0.035674333572387695
Epoch 389  Train loss: 0.02199748824624454  Val loss: 0.11951032373094067
Epoch 390
-------------------------------
Batch 1/64 loss: 0.016650855541229248
Batch 2/64 loss: 0.01906263828277588
Batch 3/64 loss: 0.02881002426147461
Batch 4/64 loss: 0.025829553604125977
Batch 5/64 loss: 0.010672271251678467
Batch 6/64 loss: 0.03431063890457153
Batch 7/64 loss: 0.029763221740722656
Batch 8/64 loss: 0.028786659240722656
Batch 9/64 loss: 0.03243941068649292
Batch 10/64 loss: 0.007494509220123291
Batch 11/64 loss: 0.006300091743469238
Batch 12/64 loss: 0.04418390989303589
Batch 13/64 loss: 0.02150857448577881
Batch 14/64 loss: 0.016317784786224365
Batch 15/64 loss: 0.04357939958572388
Batch 16/64 loss: 0.058981359004974365
Batch 17/64 loss: 0.019456028938293457
Batch 18/64 loss: 0.014417827129364014
Batch 19/64 loss: 0.016649365425109863
Batch 20/64 loss: 0.0013502836227416992
Batch 21/64 loss: -0.008548200130462646
Batch 22/64 loss: 0.0052435994148254395
Batch 23/64 loss: 0.013509273529052734
Batch 24/64 loss: -0.002702176570892334
Batch 25/64 loss: -0.011755645275115967
Batch 26/64 loss: 0.03380405902862549
Batch 27/64 loss: 0.007655918598175049
Batch 28/64 loss: 0.005022764205932617
Batch 29/64 loss: 0.0030750036239624023
Batch 30/64 loss: 0.02323007583618164
Batch 31/64 loss: 0.012160718441009521
Batch 32/64 loss: 0.03615778684616089
Batch 33/64 loss: 0.020408809185028076
Batch 34/64 loss: 0.032296597957611084
Batch 35/64 loss: 0.02885812520980835
Batch 36/64 loss: -0.00450742244720459
Batch 37/64 loss: 0.014647483825683594
Batch 38/64 loss: 0.002830326557159424
Batch 39/64 loss: 0.019900977611541748
Batch 40/64 loss: 0.0053757429122924805
Batch 41/64 loss: -0.013897597789764404
Batch 42/64 loss: 0.04553055763244629
Batch 43/64 loss: 0.010982513427734375
Batch 44/64 loss: 0.07352739572525024
Batch 45/64 loss: 0.020451068878173828
Batch 46/64 loss: 0.01821810007095337
Batch 47/64 loss: 0.028720557689666748
Batch 48/64 loss: 0.03041088581085205
Batch 49/64 loss: 0.029288530349731445
Batch 50/64 loss: 0.03244435787200928
Batch 51/64 loss: 0.028744995594024658
Batch 52/64 loss: 0.03826934099197388
Batch 53/64 loss: 0.0257871150970459
Batch 54/64 loss: 0.012044191360473633
Batch 55/64 loss: 0.017386972904205322
Batch 56/64 loss: 0.01828664541244507
Batch 57/64 loss: -0.002488553524017334
Batch 58/64 loss: 0.019887268543243408
Batch 59/64 loss: 0.0314599871635437
Batch 60/64 loss: 0.0063495635986328125
Batch 61/64 loss: 0.032813191413879395
Batch 62/64 loss: 0.031098365783691406
Batch 63/64 loss: 0.01976710557937622
Batch 64/64 loss: 0.021351635456085205
Epoch 390  Train loss: 0.020146266853108125  Val loss: 0.11983317587383834
Epoch 391
-------------------------------
Batch 1/64 loss: 0.027963638305664062
Batch 2/64 loss: 0.019202828407287598
Batch 3/64 loss: 0.022195756435394287
Batch 4/64 loss: 0.015349864959716797
Batch 5/64 loss: 0.01813560724258423
Batch 6/64 loss: -0.01499021053314209
Batch 7/64 loss: 0.018416225910186768
Batch 8/64 loss: 0.014155805110931396
Batch 9/64 loss: 0.04262810945510864
Batch 10/64 loss: 0.005624532699584961
Batch 11/64 loss: 0.01824718713760376
Batch 12/64 loss: 0.004877686500549316
Batch 13/64 loss: 0.022323131561279297
Batch 14/64 loss: 0.00526738166809082
Batch 15/64 loss: 0.006745100021362305
Batch 16/64 loss: 0.034535229206085205
Batch 17/64 loss: -0.0034996867179870605
Batch 18/64 loss: -0.0001087188720703125
Batch 19/64 loss: -0.008655846118927002
Batch 20/64 loss: 0.021587729454040527
Batch 21/64 loss: 0.03880101442337036
Batch 22/64 loss: 0.010942518711090088
Batch 23/64 loss: 0.0022827982902526855
Batch 24/64 loss: 0.025913536548614502
Batch 25/64 loss: 0.037921905517578125
Batch 26/64 loss: 0.018426239490509033
Batch 27/64 loss: 0.04504966735839844
Batch 28/64 loss: 0.033791542053222656
Batch 29/64 loss: 0.022424638271331787
Batch 30/64 loss: 0.03329026699066162
Batch 31/64 loss: 0.028119146823883057
Batch 32/64 loss: 0.003391265869140625
Batch 33/64 loss: 0.05737733840942383
Batch 34/64 loss: 0.03204864263534546
Batch 35/64 loss: 0.023275136947631836
Batch 36/64 loss: 0.02376312017440796
Batch 37/64 loss: 0.03434532880783081
Batch 38/64 loss: 0.03863400220870972
Batch 39/64 loss: 0.04061734676361084
Batch 40/64 loss: 0.018229365348815918
Batch 41/64 loss: 0.03586471080780029
Batch 42/64 loss: 0.011525213718414307
Batch 43/64 loss: -0.00064849853515625
Batch 44/64 loss: 0.022312641143798828
Batch 45/64 loss: 0.05781906843185425
Batch 46/64 loss: 0.005936324596405029
Batch 47/64 loss: 0.011362910270690918
Batch 48/64 loss: 0.016279876232147217
Batch 49/64 loss: 0.03638911247253418
Batch 50/64 loss: 0.023610949516296387
Batch 51/64 loss: 0.00042134523391723633
Batch 52/64 loss: 0.058405518531799316
Batch 53/64 loss: 0.0036512017250061035
Batch 54/64 loss: 0.04240483045578003
Batch 55/64 loss: 0.04350697994232178
Batch 56/64 loss: 0.018312454223632812
Batch 57/64 loss: 0.021706342697143555
Batch 58/64 loss: 0.01925945281982422
Batch 59/64 loss: 0.0016207098960876465
Batch 60/64 loss: 0.03853607177734375
Batch 61/64 loss: 0.016790926456451416
Batch 62/64 loss: 0.02407968044281006
Batch 63/64 loss: 0.00234222412109375
Batch 64/64 loss: 0.007047593593597412
Epoch 391  Train loss: 0.021261520245495963  Val loss: 0.1187559138868273
Epoch 392
-------------------------------
Batch 1/64 loss: -0.012722253799438477
Batch 2/64 loss: 0.00040030479431152344
Batch 3/64 loss: 0.03558170795440674
Batch 4/64 loss: 0.013756036758422852
Batch 5/64 loss: 0.016013503074645996
Batch 6/64 loss: 0.015327155590057373
Batch 7/64 loss: 0.0005003809928894043
Batch 8/64 loss: 0.0008091926574707031
Batch 9/64 loss: 0.030389130115509033
Batch 10/64 loss: 0.01651787757873535
Batch 11/64 loss: 0.03944283723831177
Batch 12/64 loss: 0.04244893789291382
Batch 13/64 loss: 0.025821387767791748
Batch 14/64 loss: 0.03361636400222778
Batch 15/64 loss: 0.016013741493225098
Batch 16/64 loss: 0.008854508399963379
Batch 17/64 loss: 0.01910686492919922
Batch 18/64 loss: 0.02419734001159668
Batch 19/64 loss: -0.013814806938171387
Batch 20/64 loss: 0.03537493944168091
Batch 21/64 loss: 0.005782783031463623
Batch 22/64 loss: 0.03634929656982422
Batch 23/64 loss: 0.0021321773529052734
Batch 24/64 loss: 0.0038639307022094727
Batch 25/64 loss: 0.0087355375289917
Batch 26/64 loss: 0.021188855171203613
Batch 27/64 loss: 0.021732747554779053
Batch 28/64 loss: 0.007166862487792969
Batch 29/64 loss: -0.0004750490188598633
Batch 30/64 loss: 0.024656295776367188
Batch 31/64 loss: 0.004094719886779785
Batch 32/64 loss: 0.026545166969299316
Batch 33/64 loss: 0.019505560398101807
Batch 34/64 loss: 0.0010551810264587402
Batch 35/64 loss: 0.028277993202209473
Batch 36/64 loss: 0.02643948793411255
Batch 37/64 loss: 0.03331935405731201
Batch 38/64 loss: 0.036038875579833984
Batch 39/64 loss: 0.05207419395446777
Batch 40/64 loss: 0.01640009880065918
Batch 41/64 loss: 0.034901440143585205
Batch 42/64 loss: 0.024702072143554688
Batch 43/64 loss: 0.041566312313079834
Batch 44/64 loss: 0.02146434783935547
Batch 45/64 loss: 0.029040873050689697
Batch 46/64 loss: 0.00036704540252685547
Batch 47/64 loss: 0.030205368995666504
Batch 48/64 loss: 0.04193449020385742
Batch 49/64 loss: 0.01737743616104126
Batch 50/64 loss: 0.032647132873535156
Batch 51/64 loss: 0.013712286949157715
Batch 52/64 loss: 0.015991926193237305
Batch 53/64 loss: 0.021280527114868164
Batch 54/64 loss: 0.04859060049057007
Batch 55/64 loss: 0.01902860403060913
Batch 56/64 loss: 0.005746364593505859
Batch 57/64 loss: 0.02432429790496826
Batch 58/64 loss: 0.021680116653442383
Batch 59/64 loss: 0.038143277168273926
Batch 60/64 loss: 0.03435671329498291
Batch 61/64 loss: 0.04703998565673828
Batch 62/64 loss: 0.006267070770263672
Batch 63/64 loss: 0.026719510555267334
Batch 64/64 loss: 0.008797049522399902
Epoch 392  Train loss: 0.020803212652019426  Val loss: 0.11997687038277433
Epoch 393
-------------------------------
Batch 1/64 loss: 0.01406782865524292
Batch 2/64 loss: 0.0012391209602355957
Batch 3/64 loss: 0.020635485649108887
Batch 4/64 loss: 0.03653925657272339
Batch 5/64 loss: 0.019465506076812744
Batch 6/64 loss: 0.002148866653442383
Batch 7/64 loss: 0.02842026948928833
Batch 8/64 loss: 0.027111291885375977
Batch 9/64 loss: 0.019643068313598633
Batch 10/64 loss: 0.03970050811767578
Batch 11/64 loss: 0.00016438961029052734
Batch 12/64 loss: 0.023917317390441895
Batch 13/64 loss: 0.03408116102218628
Batch 14/64 loss: -0.0023239850997924805
Batch 15/64 loss: 0.009682655334472656
Batch 16/64 loss: 0.03291815519332886
Batch 17/64 loss: 0.03792834281921387
Batch 18/64 loss: 0.023431241512298584
Batch 19/64 loss: 0.01779782772064209
Batch 20/64 loss: -0.0179215669631958
Batch 21/64 loss: 0.023347556591033936
Batch 22/64 loss: 0.008857965469360352
Batch 23/64 loss: 0.009488582611083984
Batch 24/64 loss: 0.0019122958183288574
Batch 25/64 loss: 0.014304578304290771
Batch 26/64 loss: 0.007880449295043945
Batch 27/64 loss: 0.029883980751037598
Batch 28/64 loss: 0.05018424987792969
Batch 29/64 loss: 0.05344212055206299
Batch 30/64 loss: -0.002678096294403076
Batch 31/64 loss: 0.0051618218421936035
Batch 32/64 loss: 0.04351240396499634
Batch 33/64 loss: 0.030518770217895508
Batch 34/64 loss: 0.04306447505950928
Batch 35/64 loss: 0.028811335563659668
Batch 36/64 loss: 0.016011416912078857
Batch 37/64 loss: 0.013158321380615234
Batch 38/64 loss: 0.019764482975006104
Batch 39/64 loss: 0.048171043395996094
Batch 40/64 loss: 0.02158534526824951
Batch 41/64 loss: 0.03688079118728638
Batch 42/64 loss: 0.027046799659729004
Batch 43/64 loss: 0.020483434200286865
Batch 44/64 loss: 0.015362441539764404
Batch 45/64 loss: 0.03541296720504761
Batch 46/64 loss: 0.005607485771179199
Batch 47/64 loss: 0.01532834768295288
Batch 48/64 loss: 0.011375665664672852
Batch 49/64 loss: 0.04133868217468262
Batch 50/64 loss: 0.008717179298400879
Batch 51/64 loss: 0.007342159748077393
Batch 52/64 loss: 0.06092315912246704
Batch 53/64 loss: 0.03221261501312256
Batch 54/64 loss: 0.01586735248565674
Batch 55/64 loss: 0.008271634578704834
Batch 56/64 loss: 0.017683327198028564
Batch 57/64 loss: 0.011060059070587158
Batch 58/64 loss: -0.0008488297462463379
Batch 59/64 loss: 0.024131298065185547
Batch 60/64 loss: 0.023487567901611328
Batch 61/64 loss: 0.021016240119934082
Batch 62/64 loss: 0.021483182907104492
Batch 63/64 loss: 0.021694839000701904
Batch 64/64 loss: -0.0026531219482421875
Epoch 393  Train loss: 0.020626170962464576  Val loss: 0.11947499497239943
Epoch 394
-------------------------------
Batch 1/64 loss: 0.008307158946990967
Batch 2/64 loss: 0.017850279808044434
Batch 3/64 loss: -0.01971191167831421
Batch 4/64 loss: 0.008291959762573242
Batch 5/64 loss: 0.024924516677856445
Batch 6/64 loss: 0.014093399047851562
Batch 7/64 loss: 0.014568805694580078
Batch 8/64 loss: -0.011178672313690186
Batch 9/64 loss: 0.037971675395965576
Batch 10/64 loss: -0.01527547836303711
Batch 11/64 loss: 0.026677727699279785
Batch 12/64 loss: 0.0385589599609375
Batch 13/64 loss: 0.07154065370559692
Batch 14/64 loss: 0.0357738733291626
Batch 15/64 loss: 0.00882941484451294
Batch 16/64 loss: 0.03360939025878906
Batch 17/64 loss: 0.007729887962341309
Batch 18/64 loss: 0.04076164960861206
Batch 19/64 loss: 0.020043790340423584
Batch 20/64 loss: 0.005717754364013672
Batch 21/64 loss: 0.010339140892028809
Batch 22/64 loss: 0.02996671199798584
Batch 23/64 loss: 0.0017955303192138672
Batch 24/64 loss: -0.004370748996734619
Batch 25/64 loss: 0.024357855319976807
Batch 26/64 loss: -0.0016227364540100098
Batch 27/64 loss: -0.00043320655822753906
Batch 28/64 loss: -0.014754831790924072
Batch 29/64 loss: 0.0004932284355163574
Batch 30/64 loss: 0.03354579210281372
Batch 31/64 loss: 0.023282647132873535
Batch 32/64 loss: 0.012257933616638184
Batch 33/64 loss: 0.0239107608795166
Batch 34/64 loss: 0.018016338348388672
Batch 35/64 loss: 0.027272939682006836
Batch 36/64 loss: 0.018406152725219727
Batch 37/64 loss: 0.025037825107574463
Batch 38/64 loss: 0.03604644536972046
Batch 39/64 loss: 0.01803821325302124
Batch 40/64 loss: 0.054071247577667236
Batch 41/64 loss: 0.031147003173828125
Batch 42/64 loss: 0.025614142417907715
Batch 43/64 loss: 0.02191758155822754
Batch 44/64 loss: 0.021495938301086426
Batch 45/64 loss: 0.015663564205169678
Batch 46/64 loss: 0.021091878414154053
Batch 47/64 loss: 0.0327184796333313
Batch 48/64 loss: 0.00028514862060546875
Batch 49/64 loss: 0.010138213634490967
Batch 50/64 loss: 0.00420534610748291
Batch 51/64 loss: 0.0345187783241272
Batch 52/64 loss: 0.016142308712005615
Batch 53/64 loss: 0.023832321166992188
Batch 54/64 loss: 0.009196758270263672
Batch 55/64 loss: 0.03900021314620972
Batch 56/64 loss: 0.022559165954589844
Batch 57/64 loss: 0.01152944564819336
Batch 58/64 loss: 0.02649831771850586
Batch 59/64 loss: 0.03943979740142822
Batch 60/64 loss: 0.03535270690917969
Batch 61/64 loss: 0.02277165651321411
Batch 62/64 loss: -0.004059553146362305
Batch 63/64 loss: 0.02005791664123535
Batch 64/64 loss: 0.024637043476104736
Epoch 394  Train loss: 0.018891560563854142  Val loss: 0.12091981258588969
Epoch 395
-------------------------------
Batch 1/64 loss: 0.026750683784484863
Batch 2/64 loss: -0.01603555679321289
Batch 3/64 loss: 0.05603104829788208
Batch 4/64 loss: 0.04878014326095581
Batch 5/64 loss: 0.022776424884796143
Batch 6/64 loss: 0.030452609062194824
Batch 7/64 loss: -0.0046465396881103516
Batch 8/64 loss: 0.03393232822418213
Batch 9/64 loss: 0.026612043380737305
Batch 10/64 loss: -0.007571220397949219
Batch 11/64 loss: 0.03923344612121582
Batch 12/64 loss: 0.011451661586761475
Batch 13/64 loss: -0.0059740543365478516
Batch 14/64 loss: 0.015340805053710938
Batch 15/64 loss: 0.003161013126373291
Batch 16/64 loss: -0.003596186637878418
Batch 17/64 loss: 0.03552812337875366
Batch 18/64 loss: 0.041639626026153564
Batch 19/64 loss: -0.00033724308013916016
Batch 20/64 loss: 0.009736835956573486
Batch 21/64 loss: 0.03427469730377197
Batch 22/64 loss: 0.0303458571434021
Batch 23/64 loss: 0.02455615997314453
Batch 24/64 loss: 0.04625415802001953
Batch 25/64 loss: 0.012216746807098389
Batch 26/64 loss: 0.010829329490661621
Batch 27/64 loss: -0.006224513053894043
Batch 28/64 loss: -0.00025844573974609375
Batch 29/64 loss: 0.04902523756027222
Batch 30/64 loss: 0.01633620262145996
Batch 31/64 loss: 0.036909520626068115
Batch 32/64 loss: 0.017264187335968018
Batch 33/64 loss: 0.010447323322296143
Batch 34/64 loss: 0.04728591442108154
Batch 35/64 loss: 0.02800232172012329
Batch 36/64 loss: -0.0044953227043151855
Batch 37/64 loss: 0.02345353364944458
Batch 38/64 loss: 0.008710622787475586
Batch 39/64 loss: 0.016026973724365234
Batch 40/64 loss: 0.0246351957321167
Batch 41/64 loss: 0.02666604518890381
Batch 42/64 loss: 0.03335368633270264
Batch 43/64 loss: 0.02137678861618042
Batch 44/64 loss: 0.011606216430664062
Batch 45/64 loss: 0.023885250091552734
Batch 46/64 loss: -0.007643580436706543
Batch 47/64 loss: 0.03348177671432495
Batch 48/64 loss: -0.001873314380645752
Batch 49/64 loss: 0.015042603015899658
Batch 50/64 loss: -0.004921674728393555
Batch 51/64 loss: -0.002895355224609375
Batch 52/64 loss: 0.019828319549560547
Batch 53/64 loss: -0.0026550889015197754
Batch 54/64 loss: 0.03729921579360962
Batch 55/64 loss: 0.03746217489242554
Batch 56/64 loss: 0.010602116584777832
Batch 57/64 loss: 0.0153961181640625
Batch 58/64 loss: 0.012138068675994873
Batch 59/64 loss: 0.016005516052246094
Batch 60/64 loss: 0.017821788787841797
Batch 61/64 loss: 0.038954973220825195
Batch 62/64 loss: 0.012600719928741455
Batch 63/64 loss: 0.0032868385314941406
Batch 64/64 loss: -0.003568410873413086
Epoch 395  Train loss: 0.01808634645798627  Val loss: 0.12095450433259158
Epoch 396
-------------------------------
Batch 1/64 loss: 0.010804295539855957
Batch 2/64 loss: 0.02145850658416748
Batch 3/64 loss: -0.00658106803894043
Batch 4/64 loss: -0.004236161708831787
Batch 5/64 loss: 0.017816483974456787
Batch 6/64 loss: 0.015331804752349854
Batch 7/64 loss: 0.0061013102531433105
Batch 8/64 loss: 0.0011119842529296875
Batch 9/64 loss: 0.022974371910095215
Batch 10/64 loss: 0.0056302547454833984
Batch 11/64 loss: 0.02367955446243286
Batch 12/64 loss: 0.00813204050064087
Batch 13/64 loss: 0.03842663764953613
Batch 14/64 loss: -0.012585341930389404
Batch 15/64 loss: 0.023604512214660645
Batch 16/64 loss: -0.009070277214050293
Batch 17/64 loss: 0.040099143981933594
Batch 18/64 loss: 0.037923336029052734
Batch 19/64 loss: 0.03498870134353638
Batch 20/64 loss: 0.03129857778549194
Batch 21/64 loss: 0.010905027389526367
Batch 22/64 loss: 0.01169133186340332
Batch 23/64 loss: 0.05695843696594238
Batch 24/64 loss: 0.03375244140625
Batch 25/64 loss: 0.021572768688201904
Batch 26/64 loss: 0.007290244102478027
Batch 27/64 loss: 0.01697462797164917
Batch 28/64 loss: 0.02648872137069702
Batch 29/64 loss: 0.027784407138824463
Batch 30/64 loss: 0.004447817802429199
Batch 31/64 loss: 0.018225789070129395
Batch 32/64 loss: 0.006973981857299805
Batch 33/64 loss: 0.04511439800262451
Batch 34/64 loss: 0.023791611194610596
Batch 35/64 loss: 0.03244781494140625
Batch 36/64 loss: 0.031773388385772705
Batch 37/64 loss: 0.03165996074676514
Batch 38/64 loss: 0.031506896018981934
Batch 39/64 loss: 0.02198636531829834
Batch 40/64 loss: 0.026471495628356934
Batch 41/64 loss: 0.015444517135620117
Batch 42/64 loss: 0.02057325839996338
Batch 43/64 loss: 0.021079421043395996
Batch 44/64 loss: 0.028926551342010498
Batch 45/64 loss: 0.014773130416870117
Batch 46/64 loss: 0.01728665828704834
Batch 47/64 loss: 0.01991868019104004
Batch 48/64 loss: -0.002388477325439453
Batch 49/64 loss: 0.015488386154174805
Batch 50/64 loss: 0.030734598636627197
Batch 51/64 loss: 0.03916412591934204
Batch 52/64 loss: 0.062494516372680664
Batch 53/64 loss: 0.04724544286727905
Batch 54/64 loss: 0.046732544898986816
Batch 55/64 loss: 0.018194198608398438
Batch 56/64 loss: 0.009007692337036133
Batch 57/64 loss: 0.002295196056365967
Batch 58/64 loss: -0.006308257579803467
Batch 59/64 loss: 0.003145158290863037
Batch 60/64 loss: 0.04257071018218994
Batch 61/64 loss: -0.0021944046020507812
Batch 62/64 loss: 0.02458035945892334
Batch 63/64 loss: 0.027742743492126465
Batch 64/64 loss: -3.153085708618164e-05
Epoch 396  Train loss: 0.020254263457129985  Val loss: 0.12063089043823715
Epoch 397
-------------------------------
Batch 1/64 loss: 0.02371358871459961
Batch 2/64 loss: 0.020752906799316406
Batch 3/64 loss: 0.003264009952545166
Batch 4/64 loss: 0.015168488025665283
Batch 5/64 loss: 0.0240592360496521
Batch 6/64 loss: 0.03685605525970459
Batch 7/64 loss: 0.022160768508911133
Batch 8/64 loss: 0.021201729774475098
Batch 9/64 loss: 0.052850544452667236
Batch 10/64 loss: 0.009673476219177246
Batch 11/64 loss: 0.025431573390960693
Batch 12/64 loss: 0.062391459941864014
Batch 13/64 loss: -0.007855772972106934
Batch 14/64 loss: 0.001482546329498291
Batch 15/64 loss: 0.028036773204803467
Batch 16/64 loss: 0.0417560338973999
Batch 17/64 loss: 0.029420435428619385
Batch 18/64 loss: 0.04061037302017212
Batch 19/64 loss: 0.011569797992706299
Batch 20/64 loss: 0.015491127967834473
Batch 21/64 loss: -0.00904381275177002
Batch 22/64 loss: 0.035597801208496094
Batch 23/64 loss: -0.004901707172393799
Batch 24/64 loss: 0.02035367488861084
Batch 25/64 loss: 0.042704105377197266
Batch 26/64 loss: -0.00605320930480957
Batch 27/64 loss: 0.004034876823425293
Batch 28/64 loss: 0.011922299861907959
Batch 29/64 loss: -0.001058816909790039
Batch 30/64 loss: 0.04385054111480713
Batch 31/64 loss: 0.02372187376022339
Batch 32/64 loss: 0.006265997886657715
Batch 33/64 loss: -0.0060256123542785645
Batch 34/64 loss: 0.022177815437316895
Batch 35/64 loss: 0.019136786460876465
Batch 36/64 loss: 0.0026770830154418945
Batch 37/64 loss: 0.039537668228149414
Batch 38/64 loss: 0.0070108771324157715
Batch 39/64 loss: 0.016037821769714355
Batch 40/64 loss: 0.016954243183135986
Batch 41/64 loss: -0.003718852996826172
Batch 42/64 loss: 0.054010093212127686
Batch 43/64 loss: 0.01926499605178833
Batch 44/64 loss: 0.015613138675689697
Batch 45/64 loss: 0.017778992652893066
Batch 46/64 loss: 0.014119088649749756
Batch 47/64 loss: -0.0019008517265319824
Batch 48/64 loss: 0.03752774000167847
Batch 49/64 loss: 0.026476562023162842
Batch 50/64 loss: -0.007419943809509277
Batch 51/64 loss: 0.04431796073913574
Batch 52/64 loss: 0.004318118095397949
Batch 53/64 loss: 0.023650765419006348
Batch 54/64 loss: 0.016902267932891846
Batch 55/64 loss: 0.03505420684814453
Batch 56/64 loss: 0.022015869617462158
Batch 57/64 loss: 0.01654529571533203
Batch 58/64 loss: 0.007613658905029297
Batch 59/64 loss: 0.04025799036026001
Batch 60/64 loss: -0.004872024059295654
Batch 61/64 loss: 0.009977340698242188
Batch 62/64 loss: 0.01762336492538452
Batch 63/64 loss: 0.029831230640411377
Batch 64/64 loss: 0.03650462627410889
Epoch 397  Train loss: 0.019220406868878534  Val loss: 0.11717488347869559
Epoch 398
-------------------------------
Batch 1/64 loss: 0.014840662479400635
Batch 2/64 loss: 0.038123905658721924
Batch 3/64 loss: -0.0010477304458618164
Batch 4/64 loss: 0.04150599241256714
Batch 5/64 loss: 0.03597384691238403
Batch 6/64 loss: 0.011594772338867188
Batch 7/64 loss: 0.022064268589019775
Batch 8/64 loss: 0.023988664150238037
Batch 9/64 loss: -0.013001024723052979
Batch 10/64 loss: -0.00150376558303833
Batch 11/64 loss: 0.014565587043762207
Batch 12/64 loss: 0.019168615341186523
Batch 13/64 loss: -0.005770087242126465
Batch 14/64 loss: 0.026707589626312256
Batch 15/64 loss: 0.015190958976745605
Batch 16/64 loss: 0.03292369842529297
Batch 17/64 loss: 0.0431361198425293
Batch 18/64 loss: 0.022130966186523438
Batch 19/64 loss: 0.025146901607513428
Batch 20/64 loss: -0.01044166088104248
Batch 21/64 loss: 0.015796542167663574
Batch 22/64 loss: 0.03753846883773804
Batch 23/64 loss: 0.006079435348510742
Batch 24/64 loss: 0.03513169288635254
Batch 25/64 loss: 0.015720784664154053
Batch 26/64 loss: 0.016253113746643066
Batch 27/64 loss: 0.02082526683807373
Batch 28/64 loss: 0.0164300799369812
Batch 29/64 loss: 0.013548433780670166
Batch 30/64 loss: 0.04280710220336914
Batch 31/64 loss: 0.015290260314941406
Batch 32/64 loss: 0.006969630718231201
Batch 33/64 loss: -0.01730501651763916
Batch 34/64 loss: 0.05105018615722656
Batch 35/64 loss: 0.03188014030456543
Batch 36/64 loss: 0.016971468925476074
Batch 37/64 loss: 0.03763502836227417
Batch 38/64 loss: 0.0023383498191833496
Batch 39/64 loss: 0.017868638038635254
Batch 40/64 loss: 0.026083767414093018
Batch 41/64 loss: 0.02948892116546631
Batch 42/64 loss: 0.009409785270690918
Batch 43/64 loss: 0.008749663829803467
Batch 44/64 loss: 0.016204357147216797
Batch 45/64 loss: 0.04678875207901001
Batch 46/64 loss: 0.02449941635131836
Batch 47/64 loss: 0.05982065200805664
Batch 48/64 loss: 0.0017249584197998047
Batch 49/64 loss: -0.007426798343658447
Batch 50/64 loss: 0.03492379188537598
Batch 51/64 loss: 0.03615391254425049
Batch 52/64 loss: 0.01269221305847168
Batch 53/64 loss: 0.04180121421813965
Batch 54/64 loss: -0.01054304838180542
Batch 55/64 loss: 0.023994863033294678
Batch 56/64 loss: -0.007333278656005859
Batch 57/64 loss: 0.016244053840637207
Batch 58/64 loss: 0.01006096601486206
Batch 59/64 loss: 0.02318936586380005
Batch 60/64 loss: 0.006712079048156738
Batch 61/64 loss: 0.02599877119064331
Batch 62/64 loss: 0.02560102939605713
Batch 63/64 loss: 0.02721083164215088
Batch 64/64 loss: 0.03127032518386841
Epoch 398  Train loss: 0.01950793523414462  Val loss: 0.1210839920437213
Epoch 399
-------------------------------
Batch 1/64 loss: 0.015669286251068115
Batch 2/64 loss: 0.016287267208099365
Batch 3/64 loss: 0.02687138319015503
Batch 4/64 loss: 0.013404250144958496
Batch 5/64 loss: 0.02498793601989746
Batch 6/64 loss: 0.010119795799255371
Batch 7/64 loss: 0.021139800548553467
Batch 8/64 loss: 0.014091432094573975
Batch 9/64 loss: 0.009797453880310059
Batch 10/64 loss: 0.0069113969802856445
Batch 11/64 loss: 0.013974487781524658
Batch 12/64 loss: 0.043733417987823486
Batch 13/64 loss: 0.0306626558303833
Batch 14/64 loss: 0.03383362293243408
Batch 15/64 loss: 0.030269205570220947
Batch 16/64 loss: -0.004605889320373535
Batch 17/64 loss: 0.025270164012908936
Batch 18/64 loss: 0.03259170055389404
Batch 19/64 loss: -0.011471033096313477
Batch 20/64 loss: 0.029516756534576416
Batch 21/64 loss: 0.010558247566223145
Batch 22/64 loss: 0.02483499050140381
Batch 23/64 loss: 0.01535874605178833
Batch 24/64 loss: 0.01245206594467163
Batch 25/64 loss: 0.021847188472747803
Batch 26/64 loss: 0.028007328510284424
Batch 27/64 loss: 0.028733015060424805
Batch 28/64 loss: 0.004179120063781738
Batch 29/64 loss: 0.005915224552154541
Batch 30/64 loss: 0.007017731666564941
Batch 31/64 loss: 0.013952493667602539
Batch 32/64 loss: 0.01798081398010254
Batch 33/64 loss: 0.02387315034866333
Batch 34/64 loss: 0.03082674741744995
Batch 35/64 loss: 0.018430352210998535
Batch 36/64 loss: 0.06130027770996094
Batch 37/64 loss: 0.01355433464050293
Batch 38/64 loss: 0.024924159049987793
Batch 39/64 loss: 0.01993238925933838
Batch 40/64 loss: 0.038726210594177246
Batch 41/64 loss: 0.02563410997390747
Batch 42/64 loss: 0.007057309150695801
Batch 43/64 loss: 0.013893663883209229
Batch 44/64 loss: 0.009720444679260254
Batch 45/64 loss: 0.007047057151794434
Batch 46/64 loss: 0.024829983711242676
Batch 47/64 loss: 0.01904428005218506
Batch 48/64 loss: 0.03286266326904297
Batch 49/64 loss: -0.010138928890228271
Batch 50/64 loss: -0.0012946724891662598
Batch 51/64 loss: -0.0038352608680725098
Batch 52/64 loss: 0.03209817409515381
Batch 53/64 loss: 0.013634800910949707
Batch 54/64 loss: 0.007842421531677246
Batch 55/64 loss: 0.03105902671813965
Batch 56/64 loss: 0.023060619831085205
Batch 57/64 loss: 0.03006768226623535
Batch 58/64 loss: 0.015267133712768555
Batch 59/64 loss: 0.024638891220092773
Batch 60/64 loss: 0.0033262968063354492
Batch 61/64 loss: 0.027213096618652344
Batch 62/64 loss: 0.007166147232055664
Batch 63/64 loss: 0.00675809383392334
Batch 64/64 loss: 0.08440321683883667
Epoch 399  Train loss: 0.019070041179656983  Val loss: 0.11933526284096577
Epoch 400
-------------------------------
Batch 1/64 loss: 0.029891371726989746
Batch 2/64 loss: 0.0251045823097229
Batch 3/64 loss: 0.021427392959594727
Batch 4/64 loss: 0.032886624336242676
Batch 5/64 loss: 0.04098165035247803
Batch 6/64 loss: 0.013017117977142334
Batch 7/64 loss: 0.042835533618927
Batch 8/64 loss: 0.03925603628158569
Batch 9/64 loss: 0.0020099878311157227
Batch 10/64 loss: -0.00117415189743042
Batch 11/64 loss: -0.02046644687652588
Batch 12/64 loss: 0.02027231454849243
Batch 13/64 loss: 0.022249460220336914
Batch 14/64 loss: 0.005829930305480957
Batch 15/64 loss: 0.039289116859436035
Batch 16/64 loss: 0.04536473751068115
Batch 17/64 loss: 0.014577686786651611
Batch 18/64 loss: 0.04509556293487549
Batch 19/64 loss: -0.0019278526306152344
Batch 20/64 loss: 0.05029064416885376
Batch 21/64 loss: 0.005432188510894775
Batch 22/64 loss: 0.014405488967895508
Batch 23/64 loss: 0.012818753719329834
Batch 24/64 loss: 0.012718379497528076
Batch 25/64 loss: 0.03010845184326172
Batch 26/64 loss: 0.025181949138641357
Batch 27/64 loss: 0.02994626760482788
Batch 28/64 loss: 0.014380216598510742
Batch 29/64 loss: -0.02979421615600586
Batch 30/64 loss: -0.0022879838943481445
Batch 31/64 loss: 0.011692047119140625
Batch 32/64 loss: 0.0186578631401062
Batch 33/64 loss: 0.037769436836242676
Batch 34/64 loss: 0.017379581928253174
Batch 35/64 loss: 0.0009526610374450684
Batch 36/64 loss: 0.014822840690612793
Batch 37/64 loss: 0.006960391998291016
Batch 38/64 loss: 0.005398094654083252
Batch 39/64 loss: -0.0003905296325683594
Batch 40/64 loss: 0.04490786790847778
Batch 41/64 loss: 0.01449120044708252
Batch 42/64 loss: 0.003376781940460205
Batch 43/64 loss: 0.03938561677932739
Batch 44/64 loss: 0.011537015438079834
Batch 45/64 loss: -0.011523008346557617
Batch 46/64 loss: 0.010474562644958496
Batch 47/64 loss: 0.043076515197753906
Batch 48/64 loss: 0.0030228495597839355
Batch 49/64 loss: 0.0030211806297302246
Batch 50/64 loss: 0.013817310333251953
Batch 51/64 loss: 0.029745042324066162
Batch 52/64 loss: 0.03370070457458496
Batch 53/64 loss: 0.01947242021560669
Batch 54/64 loss: 0.005548596382141113
Batch 55/64 loss: 0.033678650856018066
Batch 56/64 loss: 0.0010175704956054688
Batch 57/64 loss: 0.01809108257293701
Batch 58/64 loss: 0.022684991359710693
Batch 59/64 loss: 0.011668682098388672
Batch 60/64 loss: -0.00042569637298583984
Batch 61/64 loss: 0.015216708183288574
Batch 62/64 loss: 0.02071845531463623
Batch 63/64 loss: 0.03906702995300293
Batch 64/64 loss: 0.011186718940734863
Epoch 400  Train loss: 0.01768039768817378  Val loss: 0.11551213694601944
Epoch 401
-------------------------------
Batch 1/64 loss: 0.03771495819091797
Batch 2/64 loss: 0.002047896385192871
Batch 3/64 loss: 0.011788725852966309
Batch 4/64 loss: 0.0008141994476318359
Batch 5/64 loss: 0.029130935668945312
Batch 6/64 loss: -0.0018312335014343262
Batch 7/64 loss: 0.040728628635406494
Batch 8/64 loss: 0.022138476371765137
Batch 9/64 loss: 0.018182873725891113
Batch 10/64 loss: -0.0004786849021911621
Batch 11/64 loss: 0.02422541379928589
Batch 12/64 loss: 0.0207405686378479
Batch 13/64 loss: 0.008410751819610596
Batch 14/64 loss: 0.006257534027099609
Batch 15/64 loss: 0.015155971050262451
Batch 16/64 loss: 0.013121306896209717
Batch 17/64 loss: 0.007815659046173096
Batch 18/64 loss: 0.018091320991516113
Batch 19/64 loss: 0.017147541046142578
Batch 20/64 loss: -0.0058525800704956055
Batch 21/64 loss: 0.0349498987197876
Batch 22/64 loss: 0.02920585870742798
Batch 23/64 loss: 0.03594815731048584
Batch 24/64 loss: 0.02479630708694458
Batch 25/64 loss: 0.023613452911376953
Batch 26/64 loss: 0.027953028678894043
Batch 27/64 loss: 0.012466490268707275
Batch 28/64 loss: 0.028992414474487305
Batch 29/64 loss: 0.009634733200073242
Batch 30/64 loss: -0.0106583833694458
Batch 31/64 loss: -0.009617030620574951
Batch 32/64 loss: 0.024158835411071777
Batch 33/64 loss: 0.055207669734954834
Batch 34/64 loss: -0.013067364692687988
Batch 35/64 loss: 0.02795100212097168
Batch 36/64 loss: 0.046367526054382324
Batch 37/64 loss: 0.04801833629608154
Batch 38/64 loss: 0.007732689380645752
Batch 39/64 loss: 0.00993567705154419
Batch 40/64 loss: 0.004082024097442627
Batch 41/64 loss: 0.023058772087097168
Batch 42/64 loss: 0.02239936590194702
Batch 43/64 loss: 0.037149906158447266
Batch 44/64 loss: 0.04464191198348999
Batch 45/64 loss: 0.021823644638061523
Batch 46/64 loss: 0.009129941463470459
Batch 47/64 loss: 0.019768178462982178
Batch 48/64 loss: -0.0006413459777832031
Batch 49/64 loss: -0.002423703670501709
Batch 50/64 loss: 0.012720108032226562
Batch 51/64 loss: 0.009828805923461914
Batch 52/64 loss: 0.05718773603439331
Batch 53/64 loss: 0.022372126579284668
Batch 54/64 loss: 0.010228514671325684
Batch 55/64 loss: -0.015866518020629883
Batch 56/64 loss: 0.011766672134399414
Batch 57/64 loss: 0.014185845851898193
Batch 58/64 loss: 0.02138364315032959
Batch 59/64 loss: 0.007496833801269531
Batch 60/64 loss: 0.008170008659362793
Batch 61/64 loss: 0.017985999584197998
Batch 62/64 loss: 0.02606487274169922
Batch 63/64 loss: 0.034116506576538086
Batch 64/64 loss: 0.03919500112533569
Epoch 401  Train loss: 0.01796024570278093  Val loss: 0.11939886632244202
Epoch 402
-------------------------------
Batch 1/64 loss: 0.0002620220184326172
Batch 2/64 loss: 0.019610285758972168
Batch 3/64 loss: 0.031362831592559814
Batch 4/64 loss: -0.021825075149536133
Batch 5/64 loss: 0.04136669635772705
Batch 6/64 loss: 0.03694629669189453
Batch 7/64 loss: 0.005702376365661621
Batch 8/64 loss: -0.005585670471191406
Batch 9/64 loss: 0.017252087593078613
Batch 10/64 loss: -0.004222214221954346
Batch 11/64 loss: 0.019608020782470703
Batch 12/64 loss: 0.010139703750610352
Batch 13/64 loss: 0.03769415616989136
Batch 14/64 loss: 0.04887568950653076
Batch 15/64 loss: -0.005165457725524902
Batch 16/64 loss: 0.004465937614440918
Batch 17/64 loss: 0.05096542835235596
Batch 18/64 loss: 0.002803981304168701
Batch 19/64 loss: 0.026991724967956543
Batch 20/64 loss: 0.03869205713272095
Batch 21/64 loss: 0.01505887508392334
Batch 22/64 loss: 0.02472996711730957
Batch 23/64 loss: -0.0033960342407226562
Batch 24/64 loss: 0.014904379844665527
Batch 25/64 loss: 0.004346907138824463
Batch 26/64 loss: 0.02051830291748047
Batch 27/64 loss: 0.02494490146636963
Batch 28/64 loss: 0.024710476398468018
Batch 29/64 loss: 0.02310866117477417
Batch 30/64 loss: 0.02929610013961792
Batch 31/64 loss: 0.022810280323028564
Batch 32/64 loss: 0.021770000457763672
Batch 33/64 loss: 0.022890567779541016
Batch 34/64 loss: 0.047013282775878906
Batch 35/64 loss: 0.04597139358520508
Batch 36/64 loss: 0.008247852325439453
Batch 37/64 loss: 0.01486295461654663
Batch 38/64 loss: 0.01098400354385376
Batch 39/64 loss: 0.03196156024932861
Batch 40/64 loss: 0.02615940570831299
Batch 41/64 loss: 0.017970681190490723
Batch 42/64 loss: -0.008093297481536865
Batch 43/64 loss: 0.01607370376586914
Batch 44/64 loss: -0.0004113912582397461
Batch 45/64 loss: 0.012981414794921875
Batch 46/64 loss: -0.005178987979888916
Batch 47/64 loss: 0.004621565341949463
Batch 48/64 loss: 0.03672081232070923
Batch 49/64 loss: 0.019306540489196777
Batch 50/64 loss: 0.007895946502685547
Batch 51/64 loss: 0.014960646629333496
Batch 52/64 loss: 0.026570022106170654
Batch 53/64 loss: 0.06040751934051514
Batch 54/64 loss: 0.01637899875640869
Batch 55/64 loss: 0.03159451484680176
Batch 56/64 loss: 0.020025908946990967
Batch 57/64 loss: 0.02596205472946167
Batch 58/64 loss: -0.00241851806640625
Batch 59/64 loss: 0.0057846903800964355
Batch 60/64 loss: -5.412101745605469e-05
Batch 61/64 loss: 0.03119605779647827
Batch 62/64 loss: -0.0009949207305908203
Batch 63/64 loss: -0.008640110492706299
Batch 64/64 loss: -0.008450865745544434
Epoch 402  Train loss: 0.017304412523905435  Val loss: 0.11893136517698412
Epoch 403
-------------------------------
Batch 1/64 loss: 0.019826948642730713
Batch 2/64 loss: 0.048872292041778564
Batch 3/64 loss: 0.037926673889160156
Batch 4/64 loss: 0.0021128058433532715
Batch 5/64 loss: 0.008832335472106934
Batch 6/64 loss: 0.018043816089630127
Batch 7/64 loss: 0.008391201496124268
Batch 8/64 loss: 0.008405089378356934
Batch 9/64 loss: -0.0108642578125
Batch 10/64 loss: 0.0102156400680542
Batch 11/64 loss: 0.030177295207977295
Batch 12/64 loss: 0.009248733520507812
Batch 13/64 loss: 0.010748207569122314
Batch 14/64 loss: 0.008031368255615234
Batch 15/64 loss: 0.02037125825881958
Batch 16/64 loss: -0.00901252031326294
Batch 17/64 loss: 0.06964385509490967
Batch 18/64 loss: 0.020139336585998535
Batch 19/64 loss: -0.010022938251495361
Batch 20/64 loss: 0.025239765644073486
Batch 21/64 loss: 0.01768094301223755
Batch 22/64 loss: -0.011438131332397461
Batch 23/64 loss: -0.011597514152526855
Batch 24/64 loss: 0.005084812641143799
Batch 25/64 loss: -0.010746121406555176
Batch 26/64 loss: 0.012967705726623535
Batch 27/64 loss: 0.0058286190032958984
Batch 28/64 loss: 0.020505130290985107
Batch 29/64 loss: 0.014940440654754639
Batch 30/64 loss: 0.017209231853485107
Batch 31/64 loss: -0.010742723941802979
Batch 32/64 loss: -0.0015458464622497559
Batch 33/64 loss: 0.010828018188476562
Batch 34/64 loss: 0.0343625545501709
Batch 35/64 loss: 0.0009475350379943848
Batch 36/64 loss: 0.0025353431701660156
Batch 37/64 loss: 0.0067865848541259766
Batch 38/64 loss: 0.03569155931472778
Batch 39/64 loss: -0.015962839126586914
Batch 40/64 loss: 0.026995062828063965
Batch 41/64 loss: -0.00555652379989624
Batch 42/64 loss: 0.0096513032913208
Batch 43/64 loss: 0.04458588361740112
Batch 44/64 loss: 0.054619789123535156
Batch 45/64 loss: 0.02259647846221924
Batch 46/64 loss: 0.045081377029418945
Batch 47/64 loss: 0.006732761859893799
Batch 48/64 loss: 0.01668316125869751
Batch 49/64 loss: 0.026142776012420654
Batch 50/64 loss: 0.049171268939971924
Batch 51/64 loss: 0.048204123973846436
Batch 52/64 loss: 0.007382452487945557
Batch 53/64 loss: 0.017346084117889404
Batch 54/64 loss: 0.057050466537475586
Batch 55/64 loss: 0.03492051362991333
Batch 56/64 loss: 0.03218543529510498
Batch 57/64 loss: 0.017958521842956543
Batch 58/64 loss: 0.022624075412750244
Batch 59/64 loss: 0.04355597496032715
Batch 60/64 loss: 0.016254007816314697
Batch 61/64 loss: -0.00582808256149292
Batch 62/64 loss: 0.02148270606994629
Batch 63/64 loss: 0.0052536725997924805
Batch 64/64 loss: 0.051667869091033936
Epoch 403  Train loss: 0.017309904332254447  Val loss: 0.12202565694592662
Epoch 404
-------------------------------
Batch 1/64 loss: 0.0003972649574279785
Batch 2/64 loss: 0.036683082580566406
Batch 3/64 loss: 0.02128523588180542
Batch 4/64 loss: 0.02472221851348877
Batch 5/64 loss: 0.0015169382095336914
Batch 6/64 loss: 0.041601479053497314
Batch 7/64 loss: 0.013829708099365234
Batch 8/64 loss: 0.029103755950927734
Batch 9/64 loss: 0.045214056968688965
Batch 10/64 loss: -0.004708349704742432
Batch 11/64 loss: 0.02001643180847168
Batch 12/64 loss: 0.04238617420196533
Batch 13/64 loss: 0.04409152269363403
Batch 14/64 loss: 0.013539135456085205
Batch 15/64 loss: 0.02867060899734497
Batch 16/64 loss: 0.037241458892822266
Batch 17/64 loss: 0.0027327537536621094
Batch 18/64 loss: 0.04924213886260986
Batch 19/64 loss: -0.019239723682403564
Batch 20/64 loss: 0.03426563739776611
Batch 21/64 loss: 0.00619739294052124
Batch 22/64 loss: -0.007451057434082031
Batch 23/64 loss: 0.021451056003570557
Batch 24/64 loss: 0.04923802614212036
Batch 25/64 loss: 0.007734179496765137
Batch 26/64 loss: 0.03930860757827759
Batch 27/64 loss: 0.049427568912506104
Batch 28/64 loss: 0.00820690393447876
Batch 29/64 loss: 0.008958756923675537
Batch 30/64 loss: 0.0385667085647583
Batch 31/64 loss: 0.014404535293579102
Batch 32/64 loss: 0.01675856113433838
Batch 33/64 loss: 0.017302989959716797
Batch 34/64 loss: 0.007429540157318115
Batch 35/64 loss: 0.028877675533294678
Batch 36/64 loss: 0.0024344325065612793
Batch 37/64 loss: 0.006496071815490723
Batch 38/64 loss: 0.013573527336120605
Batch 39/64 loss: 0.0052092671394348145
Batch 40/64 loss: 0.03787493705749512
Batch 41/64 loss: 0.024576783180236816
Batch 42/64 loss: 0.0026113390922546387
Batch 43/64 loss: 0.0015159249305725098
Batch 44/64 loss: 0.011065185070037842
Batch 45/64 loss: 0.005870699882507324
Batch 46/64 loss: -0.010873258113861084
Batch 47/64 loss: 0.010921359062194824
Batch 48/64 loss: 0.015971481800079346
Batch 49/64 loss: 0.028896808624267578
Batch 50/64 loss: 0.027212321758270264
Batch 51/64 loss: -0.024980366230010986
Batch 52/64 loss: 0.008862435817718506
Batch 53/64 loss: 0.0132332444190979
Batch 54/64 loss: 0.02273249626159668
Batch 55/64 loss: -0.0012280941009521484
Batch 56/64 loss: 0.02990490198135376
Batch 57/64 loss: -0.0007008910179138184
Batch 58/64 loss: 0.06288802623748779
Batch 59/64 loss: 0.035413146018981934
Batch 60/64 loss: 0.007069110870361328
Batch 61/64 loss: 0.03363001346588135
Batch 62/64 loss: 0.03823673725128174
Batch 63/64 loss: 0.02223491668701172
Batch 64/64 loss: 0.010691523551940918
Epoch 404  Train loss: 0.01894390863530776  Val loss: 0.11884430962329878
Epoch 405
-------------------------------
Batch 1/64 loss: 0.008318483829498291
Batch 2/64 loss: 0.048691511154174805
Batch 3/64 loss: -2.2530555725097656e-05
Batch 4/64 loss: 0.0019220709800720215
Batch 5/64 loss: 0.037354469299316406
Batch 6/64 loss: 0.028525352478027344
Batch 7/64 loss: -0.00021588802337646484
Batch 8/64 loss: 0.011782407760620117
Batch 9/64 loss: -0.006330668926239014
Batch 10/64 loss: 0.02354443073272705
Batch 11/64 loss: 0.023946821689605713
Batch 12/64 loss: 0.010391592979431152
Batch 13/64 loss: 0.021317124366760254
Batch 14/64 loss: 0.020936250686645508
Batch 15/64 loss: 0.0035659074783325195
Batch 16/64 loss: 0.021914124488830566
Batch 17/64 loss: 0.008065938949584961
Batch 18/64 loss: 0.004247546195983887
Batch 19/64 loss: 0.0028856992721557617
Batch 20/64 loss: 0.02286696434020996
Batch 21/64 loss: 0.01589524745941162
Batch 22/64 loss: 0.007086455821990967
Batch 23/64 loss: 0.02683579921722412
Batch 24/64 loss: 0.01022869348526001
Batch 25/64 loss: 0.008503079414367676
Batch 26/64 loss: 0.010686099529266357
Batch 27/64 loss: 0.02294105291366577
Batch 28/64 loss: 0.003938019275665283
Batch 29/64 loss: 0.04730135202407837
Batch 30/64 loss: 0.020119965076446533
Batch 31/64 loss: 0.05298566818237305
Batch 32/64 loss: 0.031113922595977783
Batch 33/64 loss: 0.00012606382369995117
Batch 34/64 loss: 0.008682787418365479
Batch 35/64 loss: 0.01235896348953247
Batch 36/64 loss: 0.0499417781829834
Batch 37/64 loss: 0.006098151206970215
Batch 38/64 loss: 0.010270237922668457
Batch 39/64 loss: 0.001616358757019043
Batch 40/64 loss: 0.020356357097625732
Batch 41/64 loss: 0.023483097553253174
Batch 42/64 loss: 0.0081251859664917
Batch 43/64 loss: 0.02252870798110962
Batch 44/64 loss: 0.018852174282073975
Batch 45/64 loss: 0.028000235557556152
Batch 46/64 loss: 0.009479165077209473
Batch 47/64 loss: 0.040823400020599365
Batch 48/64 loss: 0.02076047658920288
Batch 49/64 loss: 0.012855052947998047
Batch 50/64 loss: 0.0036370158195495605
Batch 51/64 loss: -0.005572259426116943
Batch 52/64 loss: -0.005781292915344238
Batch 53/64 loss: 0.025435328483581543
Batch 54/64 loss: 0.029927551746368408
Batch 55/64 loss: 0.0028731822967529297
Batch 56/64 loss: 0.001755058765411377
Batch 57/64 loss: 0.02656233310699463
Batch 58/64 loss: 0.03482860326766968
Batch 59/64 loss: 0.005870699882507324
Batch 60/64 loss: 0.021111130714416504
Batch 61/64 loss: 0.03234034776687622
Batch 62/64 loss: 0.02912682294845581
Batch 63/64 loss: 0.018423914909362793
Batch 64/64 loss: 0.009361505508422852
Epoch 405  Train loss: 0.016835462345796474  Val loss: 0.11932377143414159
Epoch 406
-------------------------------
Batch 1/64 loss: -0.00018858909606933594
Batch 2/64 loss: -0.010507464408874512
Batch 3/64 loss: 0.02638864517211914
Batch 4/64 loss: 0.033068716526031494
Batch 5/64 loss: 0.0057103633880615234
Batch 6/64 loss: 0.04241025447845459
Batch 7/64 loss: -0.003122687339782715
Batch 8/64 loss: 0.003018796443939209
Batch 9/64 loss: 0.009689092636108398
Batch 10/64 loss: 0.00780487060546875
Batch 11/64 loss: 0.00037664175033569336
Batch 12/64 loss: 0.012998104095458984
Batch 13/64 loss: 0.013353705406188965
Batch 14/64 loss: 0.01441103219985962
Batch 15/64 loss: 0.016269683837890625
Batch 16/64 loss: 0.03202742338180542
Batch 17/64 loss: 0.0012369751930236816
Batch 18/64 loss: 0.02050119638442993
Batch 19/64 loss: 0.027438640594482422
Batch 20/64 loss: 0.025803864002227783
Batch 21/64 loss: 0.03407639265060425
Batch 22/64 loss: 0.010632932186126709
Batch 23/64 loss: 0.0050563812255859375
Batch 24/64 loss: 0.014953255653381348
Batch 25/64 loss: 0.028746485710144043
Batch 26/64 loss: -0.007914245128631592
Batch 27/64 loss: 0.021776914596557617
Batch 28/64 loss: 0.010782003402709961
Batch 29/64 loss: 0.008364200592041016
Batch 30/64 loss: 0.00912243127822876
Batch 31/64 loss: 0.01796025037765503
Batch 32/64 loss: 0.0164717435836792
Batch 33/64 loss: 0.02062249183654785
Batch 34/64 loss: 0.016901910305023193
Batch 35/64 loss: 0.013329088687896729
Batch 36/64 loss: 0.04350864887237549
Batch 37/64 loss: -0.0022982358932495117
Batch 38/64 loss: 0.045873284339904785
Batch 39/64 loss: 0.018297255039215088
Batch 40/64 loss: 0.03227519989013672
Batch 41/64 loss: 0.042733848094940186
Batch 42/64 loss: 0.017512142658233643
Batch 43/64 loss: 0.009322404861450195
Batch 44/64 loss: 0.009879589080810547
Batch 45/64 loss: -0.0036320090293884277
Batch 46/64 loss: 0.05504131317138672
Batch 47/64 loss: -0.001789867877960205
Batch 48/64 loss: 0.010976850986480713
Batch 49/64 loss: 0.007822990417480469
Batch 50/64 loss: 0.013974428176879883
Batch 51/64 loss: 0.005433976650238037
Batch 52/64 loss: 0.02769327163696289
Batch 53/64 loss: 0.01819777488708496
Batch 54/64 loss: 0.02221304178237915
Batch 55/64 loss: -0.004867970943450928
Batch 56/64 loss: 0.008011102676391602
Batch 57/64 loss: 0.05203181505203247
Batch 58/64 loss: 0.04270440340042114
Batch 59/64 loss: 0.03718256950378418
Batch 60/64 loss: -0.006761312484741211
Batch 61/64 loss: 0.006667137145996094
Batch 62/64 loss: 0.0071482062339782715
Batch 63/64 loss: -0.001085519790649414
Batch 64/64 loss: 0.024013817310333252
Epoch 406  Train loss: 0.01618271692126405  Val loss: 0.11781310318261896
Epoch 407
-------------------------------
Batch 1/64 loss: -0.01184457540512085
Batch 2/64 loss: 0.010946512222290039
Batch 3/64 loss: 0.010056376457214355
Batch 4/64 loss: 0.048387885093688965
Batch 5/64 loss: -0.019871532917022705
Batch 6/64 loss: 0.015425562858581543
Batch 7/64 loss: 0.006450235843658447
Batch 8/64 loss: 0.011129200458526611
Batch 9/64 loss: 0.022579431533813477
Batch 10/64 loss: 0.011017322540283203
Batch 11/64 loss: -0.0021986961364746094
Batch 12/64 loss: -6.73532485961914e-05
Batch 13/64 loss: 0.01902562379837036
Batch 14/64 loss: 0.03375130891799927
Batch 15/64 loss: 0.006817460060119629
Batch 16/64 loss: -0.0011580586433410645
Batch 17/64 loss: 0.05024373531341553
Batch 18/64 loss: 0.027570366859436035
Batch 19/64 loss: 0.05938148498535156
Batch 20/64 loss: 0.040595173835754395
Batch 21/64 loss: 0.006810009479522705
Batch 22/64 loss: 0.007520020008087158
Batch 23/64 loss: -0.004406094551086426
Batch 24/64 loss: 0.01891690492630005
Batch 25/64 loss: 0.050574541091918945
Batch 26/64 loss: 0.012437880039215088
Batch 27/64 loss: -0.010243654251098633
Batch 28/64 loss: 0.011332929134368896
Batch 29/64 loss: 0.022100985050201416
Batch 30/64 loss: 0.02008455991744995
Batch 31/64 loss: 0.0321803092956543
Batch 32/64 loss: 0.03197050094604492
Batch 33/64 loss: -0.0033228397369384766
Batch 34/64 loss: 0.029620766639709473
Batch 35/64 loss: 0.029651165008544922
Batch 36/64 loss: 0.04385507106781006
Batch 37/64 loss: 0.0383877158164978
Batch 38/64 loss: 0.012518882751464844
Batch 39/64 loss: 0.010996460914611816
Batch 40/64 loss: 0.02753680944442749
Batch 41/64 loss: 0.021905899047851562
Batch 42/64 loss: 0.01754629611968994
Batch 43/64 loss: 0.000895082950592041
Batch 44/64 loss: -0.007615864276885986
Batch 45/64 loss: 0.0022988319396972656
Batch 46/64 loss: 0.005142509937286377
Batch 47/64 loss: 0.019351422786712646
Batch 48/64 loss: -0.005729556083679199
Batch 49/64 loss: 0.004332005977630615
Batch 50/64 loss: -0.008916139602661133
Batch 51/64 loss: 0.005084693431854248
Batch 52/64 loss: 0.057031989097595215
Batch 53/64 loss: 0.036146581172943115
Batch 54/64 loss: 0.03834724426269531
Batch 55/64 loss: -0.0034837722778320312
Batch 56/64 loss: 0.009748458862304688
Batch 57/64 loss: 0.02542489767074585
Batch 58/64 loss: 0.028432011604309082
Batch 59/64 loss: 0.009788811206817627
Batch 60/64 loss: 0.004425168037414551
Batch 61/64 loss: -0.02309626340866089
Batch 62/64 loss: 0.017322838306427002
Batch 63/64 loss: 0.012008309364318848
Batch 64/64 loss: 0.04532831907272339
Epoch 407  Train loss: 0.016112126789841  Val loss: 0.11915102775154245
Epoch 408
-------------------------------
Batch 1/64 loss: -0.0017807483673095703
Batch 2/64 loss: 0.008342444896697998
Batch 3/64 loss: 0.02653789520263672
Batch 4/64 loss: 0.01370096206665039
Batch 5/64 loss: 0.006608724594116211
Batch 6/64 loss: 0.027751922607421875
Batch 7/64 loss: 0.026644766330718994
Batch 8/64 loss: 0.03436565399169922
Batch 9/64 loss: 0.0601961612701416
Batch 10/64 loss: 0.018880009651184082
Batch 11/64 loss: 0.015144884586334229
Batch 12/64 loss: -0.00804215669631958
Batch 13/64 loss: 0.011226892471313477
Batch 14/64 loss: 0.014955997467041016
Batch 15/64 loss: 0.00646597146987915
Batch 16/64 loss: -0.014972269535064697
Batch 17/64 loss: 0.00703662633895874
Batch 18/64 loss: -0.0069893598556518555
Batch 19/64 loss: 0.026895642280578613
Batch 20/64 loss: 0.01524209976196289
Batch 21/64 loss: 0.0405997633934021
Batch 22/64 loss: -0.004130125045776367
Batch 23/64 loss: 0.0015764236450195312
Batch 24/64 loss: 0.0047203898429870605
Batch 25/64 loss: 0.02754455804824829
Batch 26/64 loss: 0.010448694229125977
Batch 27/64 loss: 0.0049399733543396
Batch 28/64 loss: -0.0022791028022766113
Batch 29/64 loss: -0.0033462047576904297
Batch 30/64 loss: 0.0673290491104126
Batch 31/64 loss: 0.030196011066436768
Batch 32/64 loss: 0.03842443227767944
Batch 33/64 loss: 0.0139007568359375
Batch 34/64 loss: 0.007133424282073975
Batch 35/64 loss: 0.004117131233215332
Batch 36/64 loss: 0.05440843105316162
Batch 37/64 loss: 0.007158637046813965
Batch 38/64 loss: 0.017894983291625977
Batch 39/64 loss: 0.011550068855285645
Batch 40/64 loss: 0.016988098621368408
Batch 41/64 loss: 0.008495688438415527
Batch 42/64 loss: 0.022816896438598633
Batch 43/64 loss: 0.006017029285430908
Batch 44/64 loss: 0.022857487201690674
Batch 45/64 loss: 0.04942464828491211
Batch 46/64 loss: 0.028940200805664062
Batch 47/64 loss: 0.013221025466918945
Batch 48/64 loss: 0.040020763874053955
Batch 49/64 loss: 0.041614830493927
Batch 50/64 loss: 0.009094715118408203
Batch 51/64 loss: 0.03701174259185791
Batch 52/64 loss: 0.020733356475830078
Batch 53/64 loss: 0.02862793207168579
Batch 54/64 loss: 0.03280526399612427
Batch 55/64 loss: 0.03836846351623535
Batch 56/64 loss: 0.030014753341674805
Batch 57/64 loss: -0.0025614500045776367
Batch 58/64 loss: 0.028934001922607422
Batch 59/64 loss: 0.005148887634277344
Batch 60/64 loss: 0.026707172393798828
Batch 61/64 loss: 0.024796247482299805
Batch 62/64 loss: 0.0467413067817688
Batch 63/64 loss: 0.031205296516418457
Batch 64/64 loss: 0.007046818733215332
Epoch 408  Train loss: 0.01935229628693824  Val loss: 0.12189292313716665
Epoch 409
-------------------------------
Batch 1/64 loss: 0.07642042636871338
Batch 2/64 loss: 0.031101465225219727
Batch 3/64 loss: -0.011366128921508789
Batch 4/64 loss: 0.012883305549621582
Batch 5/64 loss: 0.043276190757751465
Batch 6/64 loss: 0.016665995121002197
Batch 7/64 loss: 0.017051517963409424
Batch 8/64 loss: 0.01426476240158081
Batch 9/64 loss: 0.021506845951080322
Batch 10/64 loss: 0.00897127389907837
Batch 11/64 loss: 0.009356915950775146
Batch 12/64 loss: 0.032314300537109375
Batch 13/64 loss: 0.0020213723182678223
Batch 14/64 loss: 0.0026621222496032715
Batch 15/64 loss: -0.004321694374084473
Batch 16/64 loss: 0.0059697628021240234
Batch 17/64 loss: 0.0017242431640625
Batch 18/64 loss: -0.012243688106536865
Batch 19/64 loss: 0.049055278301239014
Batch 20/64 loss: 0.01710575819015503
Batch 21/64 loss: 0.02618229389190674
Batch 22/64 loss: 0.03595864772796631
Batch 23/64 loss: 0.03809291124343872
Batch 24/64 loss: -0.00851505994796753
Batch 25/64 loss: 0.013643085956573486
Batch 26/64 loss: 0.019701361656188965
Batch 27/64 loss: -0.0018391609191894531
Batch 28/64 loss: 0.02865135669708252
Batch 29/64 loss: 0.005796492099761963
Batch 30/64 loss: 0.003020644187927246
Batch 31/64 loss: 0.014263510704040527
Batch 32/64 loss: 0.042303264141082764
Batch 33/64 loss: 0.004224956035614014
Batch 34/64 loss: -0.01179194450378418
Batch 35/64 loss: 0.0015114545822143555
Batch 36/64 loss: 6.270408630371094e-05
Batch 37/64 loss: 0.032036542892456055
Batch 38/64 loss: 0.0009701251983642578
Batch 39/64 loss: 0.020891427993774414
Batch 40/64 loss: 0.01684349775314331
Batch 41/64 loss: 0.040644824504852295
Batch 42/64 loss: 0.01805412769317627
Batch 43/64 loss: 0.03387033939361572
Batch 44/64 loss: 0.0011235475540161133
Batch 45/64 loss: 0.0110931396484375
Batch 46/64 loss: 0.02244400978088379
Batch 47/64 loss: 0.026088416576385498
Batch 48/64 loss: 0.009978294372558594
Batch 49/64 loss: 0.03330105543136597
Batch 50/64 loss: 0.01902562379837036
Batch 51/64 loss: -0.014104723930358887
Batch 52/64 loss: 0.004220306873321533
Batch 53/64 loss: 0.003192722797393799
Batch 54/64 loss: 0.05190253257751465
Batch 55/64 loss: 0.005648612976074219
Batch 56/64 loss: 0.04171788692474365
Batch 57/64 loss: 0.005361974239349365
Batch 58/64 loss: 0.0017279386520385742
Batch 59/64 loss: 0.024522006511688232
Batch 60/64 loss: 0.01896834373474121
Batch 61/64 loss: -0.0033379793167114258
Batch 62/64 loss: 0.024324536323547363
Batch 63/64 loss: 0.007390320301055908
Batch 64/64 loss: 0.035741448402404785
Epoch 409  Train loss: 0.016163013028163534  Val loss: 0.12115208225971236
Epoch 410
-------------------------------
Batch 1/64 loss: 0.018976032733917236
Batch 2/64 loss: 0.0003484487533569336
Batch 3/64 loss: 0.006513476371765137
Batch 4/64 loss: 0.03903597593307495
Batch 5/64 loss: 0.022698700428009033
Batch 6/64 loss: 0.018392324447631836
Batch 7/64 loss: -0.019281864166259766
Batch 8/64 loss: 0.01194387674331665
Batch 9/64 loss: -0.006006598472595215
Batch 10/64 loss: -0.004022121429443359
Batch 11/64 loss: 0.02724611759185791
Batch 12/64 loss: 0.005764305591583252
Batch 13/64 loss: -0.002668321132659912
Batch 14/64 loss: 0.0008124709129333496
Batch 15/64 loss: 0.012947797775268555
Batch 16/64 loss: 0.006583571434020996
Batch 17/64 loss: 0.03304457664489746
Batch 18/64 loss: 0.02999114990234375
Batch 19/64 loss: 0.017080485820770264
Batch 20/64 loss: -0.009556174278259277
Batch 21/64 loss: 0.018323123455047607
Batch 22/64 loss: -0.007093369960784912
Batch 23/64 loss: 0.03184109926223755
Batch 24/64 loss: 0.02740699052810669
Batch 25/64 loss: -0.021325767040252686
Batch 26/64 loss: 0.01987534761428833
Batch 27/64 loss: -0.00778043270111084
Batch 28/64 loss: 0.026801764965057373
Batch 29/64 loss: 0.004905223846435547
Batch 30/64 loss: 0.0017261505126953125
Batch 31/64 loss: -0.0032164454460144043
Batch 32/64 loss: 0.006779670715332031
Batch 33/64 loss: 0.02507174015045166
Batch 34/64 loss: 0.06408411264419556
Batch 35/64 loss: 0.029795408248901367
Batch 36/64 loss: 0.023996591567993164
Batch 37/64 loss: 0.052844226360321045
Batch 38/64 loss: 0.010765671730041504
Batch 39/64 loss: -0.014436781406402588
Batch 40/64 loss: 0.020910918712615967
Batch 41/64 loss: 0.024635910987854004
Batch 42/64 loss: 0.01314312219619751
Batch 43/64 loss: 0.02385103702545166
Batch 44/64 loss: 0.029809951782226562
Batch 45/64 loss: 0.026724576950073242
Batch 46/64 loss: -0.003224015235900879
Batch 47/64 loss: 0.029699981212615967
Batch 48/64 loss: 0.0012774467468261719
Batch 49/64 loss: 0.0006073713302612305
Batch 50/64 loss: 0.04019802808761597
Batch 51/64 loss: 0.018086910247802734
Batch 52/64 loss: 0.037013471126556396
Batch 53/64 loss: 0.03153103590011597
Batch 54/64 loss: 0.009982168674468994
Batch 55/64 loss: -0.00021606683731079102
Batch 56/64 loss: 0.04109048843383789
Batch 57/64 loss: 0.0019078850746154785
Batch 58/64 loss: 0.035470783710479736
Batch 59/64 loss: 0.01783013343811035
Batch 60/64 loss: 0.02194511890411377
Batch 61/64 loss: 0.01344311237335205
Batch 62/64 loss: -0.021675288677215576
Batch 63/64 loss: 0.005673468112945557
Batch 64/64 loss: 0.050180137157440186
Epoch 410  Train loss: 0.015020567996829165  Val loss: 0.11954277262245257
Epoch 411
-------------------------------
Batch 1/64 loss: -0.0002587437629699707
Batch 2/64 loss: 0.01912468671798706
Batch 3/64 loss: 0.01620340347290039
Batch 4/64 loss: 0.008053243160247803
Batch 5/64 loss: 0.008662760257720947
Batch 6/64 loss: 0.008933305740356445
Batch 7/64 loss: -0.016544640064239502
Batch 8/64 loss: -0.002972543239593506
Batch 9/64 loss: -0.005974829196929932
Batch 10/64 loss: 0.044215619564056396
Batch 11/64 loss: 0.005217492580413818
Batch 12/64 loss: -0.0017244219779968262
Batch 13/64 loss: 0.009235262870788574
Batch 14/64 loss: 0.014563560485839844
Batch 15/64 loss: -0.0035228729248046875
Batch 16/64 loss: 0.010328531265258789
Batch 17/64 loss: 0.006192684173583984
Batch 18/64 loss: 0.032335758209228516
Batch 19/64 loss: 0.01004338264465332
Batch 20/64 loss: 0.021925032138824463
Batch 21/64 loss: 0.019121110439300537
Batch 22/64 loss: 0.015411436557769775
Batch 23/64 loss: 0.020270824432373047
Batch 24/64 loss: 0.010599732398986816
Batch 25/64 loss: 0.03358113765716553
Batch 26/64 loss: 0.005606591701507568
Batch 27/64 loss: 0.004227817058563232
Batch 28/64 loss: -0.002262413501739502
Batch 29/64 loss: -0.0048334598541259766
Batch 30/64 loss: 0.04859626293182373
Batch 31/64 loss: -0.0010522007942199707
Batch 32/64 loss: 0.024131417274475098
Batch 33/64 loss: -0.015081465244293213
Batch 34/64 loss: 0.0005130171775817871
Batch 35/64 loss: 0.02716165781021118
Batch 36/64 loss: 0.02668154239654541
Batch 37/64 loss: 0.042328596115112305
Batch 38/64 loss: 0.012499392032623291
Batch 39/64 loss: 0.006791949272155762
Batch 40/64 loss: 0.041239142417907715
Batch 41/64 loss: 0.017364859580993652
Batch 42/64 loss: 0.02021270990371704
Batch 43/64 loss: 0.03235292434692383
Batch 44/64 loss: 0.026921093463897705
Batch 45/64 loss: -0.008871078491210938
Batch 46/64 loss: 0.009055793285369873
Batch 47/64 loss: 0.04474002122879028
Batch 48/64 loss: 0.0007631778717041016
Batch 49/64 loss: 0.003985106945037842
Batch 50/64 loss: 0.010575652122497559
Batch 51/64 loss: 0.04606974124908447
Batch 52/64 loss: 0.01158815622329712
Batch 53/64 loss: 0.01092618703842163
Batch 54/64 loss: -0.0008736848831176758
Batch 55/64 loss: 0.053022146224975586
Batch 56/64 loss: -0.0016729235649108887
Batch 57/64 loss: 0.011086642742156982
Batch 58/64 loss: 0.0348362922668457
Batch 59/64 loss: 0.02604013681411743
Batch 60/64 loss: 0.04852169752120972
Batch 61/64 loss: 0.0008335113525390625
Batch 62/64 loss: 0.04442697763442993
Batch 63/64 loss: 0.015052437782287598
Batch 64/64 loss: 0.033815741539001465
Epoch 411  Train loss: 0.01540216698366053  Val loss: 0.11931951226237714
Epoch 412
-------------------------------
Batch 1/64 loss: 0.0017050504684448242
Batch 2/64 loss: 0.01886451244354248
Batch 3/64 loss: 0.01462256908416748
Batch 4/64 loss: -0.01803135871887207
Batch 5/64 loss: -0.01328742504119873
Batch 6/64 loss: 0.006856381893157959
Batch 7/64 loss: -0.007359683513641357
Batch 8/64 loss: 0.004543900489807129
Batch 9/64 loss: 0.011971354484558105
Batch 10/64 loss: 0.012902319431304932
Batch 11/64 loss: -0.006617903709411621
Batch 12/64 loss: 0.0508158802986145
Batch 13/64 loss: 0.04759162664413452
Batch 14/64 loss: -0.0346602201461792
Batch 15/64 loss: 0.0406307578086853
Batch 16/64 loss: 0.016473054885864258
Batch 17/64 loss: -0.011854588985443115
Batch 18/64 loss: 0.0003610849380493164
Batch 19/64 loss: 0.027561485767364502
Batch 20/64 loss: 0.0022832155227661133
Batch 21/64 loss: 0.029403328895568848
Batch 22/64 loss: 0.02509617805480957
Batch 23/64 loss: -0.003651261329650879
Batch 24/64 loss: 0.020431458950042725
Batch 25/64 loss: 0.029552459716796875
Batch 26/64 loss: 0.018904685974121094
Batch 27/64 loss: 0.008554577827453613
Batch 28/64 loss: 0.015236377716064453
Batch 29/64 loss: 0.01797109842300415
Batch 30/64 loss: 0.00551682710647583
Batch 31/64 loss: -0.002222716808319092
Batch 32/64 loss: 0.03893214464187622
Batch 33/64 loss: 0.014303386211395264
Batch 34/64 loss: 0.02502310276031494
Batch 35/64 loss: 0.01448523998260498
Batch 36/64 loss: 0.00405806303024292
Batch 37/64 loss: 0.018598735332489014
Batch 38/64 loss: 0.00036787986755371094
Batch 39/64 loss: 0.024437785148620605
Batch 40/64 loss: 0.02748805284500122
Batch 41/64 loss: 0.045000433921813965
Batch 42/64 loss: 0.0003050565719604492
Batch 43/64 loss: 0.02603679895401001
Batch 44/64 loss: 0.002802252769470215
Batch 45/64 loss: 0.02935171127319336
Batch 46/64 loss: 0.03603994846343994
Batch 47/64 loss: 0.029277265071868896
Batch 48/64 loss: 0.03594827651977539
Batch 49/64 loss: 0.04173755645751953
Batch 50/64 loss: 0.01283574104309082
Batch 51/64 loss: 0.003554105758666992
Batch 52/64 loss: 0.033109426498413086
Batch 53/64 loss: 0.0014033913612365723
Batch 54/64 loss: -0.005893051624298096
Batch 55/64 loss: -0.02694869041442871
Batch 56/64 loss: 0.011894822120666504
Batch 57/64 loss: 0.04690831899642944
Batch 58/64 loss: 0.000613093376159668
Batch 59/64 loss: 0.02916198968887329
Batch 60/64 loss: 0.01988142728805542
Batch 61/64 loss: 0.010494828224182129
Batch 62/64 loss: 0.04743993282318115
Batch 63/64 loss: 0.008184552192687988
Batch 64/64 loss: 0.004130065441131592
Epoch 412  Train loss: 0.014746606349945068  Val loss: 0.11962919710428034
Epoch 413
-------------------------------
Batch 1/64 loss: -0.012053072452545166
Batch 2/64 loss: 0.0007607936859130859
Batch 3/64 loss: 0.021415650844573975
Batch 4/64 loss: 0.0061678290367126465
Batch 5/64 loss: 0.030878424644470215
Batch 6/64 loss: 0.023670554161071777
Batch 7/64 loss: 0.007297158241271973
Batch 8/64 loss: 0.006919264793395996
Batch 9/64 loss: 0.0169675350189209
Batch 10/64 loss: 0.037670791149139404
Batch 11/64 loss: 0.003436446189880371
Batch 12/64 loss: 0.0034123659133911133
Batch 13/64 loss: 0.025434255599975586
Batch 14/64 loss: 0.010372936725616455
Batch 15/64 loss: 0.05584639310836792
Batch 16/64 loss: 0.012025654315948486
Batch 17/64 loss: 0.013227105140686035
Batch 18/64 loss: -0.006354212760925293
Batch 19/64 loss: -0.0036405324935913086
Batch 20/64 loss: 0.008085310459136963
Batch 21/64 loss: 0.01388782262802124
Batch 22/64 loss: 0.014763236045837402
Batch 23/64 loss: 0.019036054611206055
Batch 24/64 loss: -0.0037113428115844727
Batch 25/64 loss: 0.0622294545173645
Batch 26/64 loss: -0.005956172943115234
Batch 27/64 loss: 0.018013715744018555
Batch 28/64 loss: 0.021543920040130615
Batch 29/64 loss: 0.011879265308380127
Batch 30/64 loss: 0.03226125240325928
Batch 31/64 loss: -0.01098787784576416
Batch 32/64 loss: 0.013553500175476074
Batch 33/64 loss: -0.0074492692947387695
Batch 34/64 loss: 0.03085988759994507
Batch 35/64 loss: 0.011494755744934082
Batch 36/64 loss: 0.038593590259552
Batch 37/64 loss: 0.027580320835113525
Batch 38/64 loss: 0.018045544624328613
Batch 39/64 loss: 0.009022176265716553
Batch 40/64 loss: 0.00665438175201416
Batch 41/64 loss: 0.008194267749786377
Batch 42/64 loss: -0.003183722496032715
Batch 43/64 loss: 0.02064567804336548
Batch 44/64 loss: 0.013478517532348633
Batch 45/64 loss: 0.023175477981567383
Batch 46/64 loss: 0.014005064964294434
Batch 47/64 loss: 0.030237317085266113
Batch 48/64 loss: 0.014519214630126953
Batch 49/64 loss: 0.010815739631652832
Batch 50/64 loss: 0.012381434440612793
Batch 51/64 loss: 0.021043121814727783
Batch 52/64 loss: -0.0014767050743103027
Batch 53/64 loss: 0.04208660125732422
Batch 54/64 loss: 0.031214475631713867
Batch 55/64 loss: -0.003249049186706543
Batch 56/64 loss: 0.003811359405517578
Batch 57/64 loss: 6.377696990966797e-06
Batch 58/64 loss: 0.037304043769836426
Batch 59/64 loss: -2.485513687133789e-05
Batch 60/64 loss: 0.026075422763824463
Batch 61/64 loss: 0.04268074035644531
Batch 62/64 loss: 0.017615556716918945
Batch 63/64 loss: 0.02593064308166504
Batch 64/64 loss: 0.05303913354873657
Epoch 413  Train loss: 0.015841897094950957  Val loss: 0.11772307005944531
Epoch 414
-------------------------------
Batch 1/64 loss: 0.01923733949661255
Batch 2/64 loss: 0.028446078300476074
Batch 3/64 loss: 0.004201292991638184
Batch 4/64 loss: 0.01438891887664795
Batch 5/64 loss: 0.02672421932220459
Batch 6/64 loss: 0.008809804916381836
Batch 7/64 loss: -0.0008350610733032227
Batch 8/64 loss: -0.016326069831848145
Batch 9/64 loss: 0.01664835214614868
Batch 10/64 loss: 0.014078974723815918
Batch 11/64 loss: 0.003911077976226807
Batch 12/64 loss: 0.01643151044845581
Batch 13/64 loss: -0.007888495922088623
Batch 14/64 loss: 0.002497851848602295
Batch 15/64 loss: 0.003254413604736328
Batch 16/64 loss: 0.03734612464904785
Batch 17/64 loss: 0.02470386028289795
Batch 18/64 loss: -0.01582205295562744
Batch 19/64 loss: 0.01295691728591919
Batch 20/64 loss: -0.0017105340957641602
Batch 21/64 loss: 0.02052205801010132
Batch 22/64 loss: 0.03014218807220459
Batch 23/64 loss: 0.02991008758544922
Batch 24/64 loss: 0.019591450691223145
Batch 25/64 loss: -0.01438218355178833
Batch 26/64 loss: 0.014641761779785156
Batch 27/64 loss: 0.033048152923583984
Batch 28/64 loss: 0.007549285888671875
Batch 29/64 loss: 0.03634750843048096
Batch 30/64 loss: 0.03501862287521362
Batch 31/64 loss: 0.014789462089538574
Batch 32/64 loss: 0.004862546920776367
Batch 33/64 loss: 0.020854711532592773
Batch 34/64 loss: -0.020052075386047363
Batch 35/64 loss: 0.02342432737350464
Batch 36/64 loss: 0.020920634269714355
Batch 37/64 loss: 0.018062233924865723
Batch 38/64 loss: 0.009249091148376465
Batch 39/64 loss: -0.0005857944488525391
Batch 40/64 loss: 0.014741122722625732
Batch 41/64 loss: 0.02840745449066162
Batch 42/64 loss: 0.0036519765853881836
Batch 43/64 loss: 0.01812344789505005
Batch 44/64 loss: 0.019054889678955078
Batch 45/64 loss: 0.024511098861694336
Batch 46/64 loss: 0.009241282939910889
Batch 47/64 loss: 0.00916522741317749
Batch 48/64 loss: 0.031922996044158936
Batch 49/64 loss: -0.0010083913803100586
Batch 50/64 loss: 0.03588777780532837
Batch 51/64 loss: 0.008449733257293701
Batch 52/64 loss: 0.038654208183288574
Batch 53/64 loss: 0.012769103050231934
Batch 54/64 loss: -0.004435479640960693
Batch 55/64 loss: 0.00851297378540039
Batch 56/64 loss: -0.0004404783248901367
Batch 57/64 loss: 0.029944181442260742
Batch 58/64 loss: 0.030424535274505615
Batch 59/64 loss: 0.013582289218902588
Batch 60/64 loss: 0.00530397891998291
Batch 61/64 loss: 0.007875442504882812
Batch 62/64 loss: 0.057541728019714355
Batch 63/64 loss: 0.032640933990478516
Batch 64/64 loss: 0.014592349529266357
Epoch 414  Train loss: 0.014751919811847163  Val loss: 0.12249606089903317
Epoch 415
-------------------------------
Batch 1/64 loss: 0.02108055353164673
Batch 2/64 loss: 0.008000433444976807
Batch 3/64 loss: 0.028843939304351807
Batch 4/64 loss: 0.018870949745178223
Batch 5/64 loss: 0.0018412470817565918
Batch 6/64 loss: 0.006447732448577881
Batch 7/64 loss: 0.024605154991149902
Batch 8/64 loss: 0.04589998722076416
Batch 9/64 loss: 0.007413923740386963
Batch 10/64 loss: -0.0078051090240478516
Batch 11/64 loss: 0.022998154163360596
Batch 12/64 loss: -0.009795606136322021
Batch 13/64 loss: 0.015443921089172363
Batch 14/64 loss: -0.004827976226806641
Batch 15/64 loss: 0.044870615005493164
Batch 16/64 loss: 0.027725636959075928
Batch 17/64 loss: 0.029705703258514404
Batch 18/64 loss: 0.005464434623718262
Batch 19/64 loss: 0.0514066219329834
Batch 20/64 loss: 0.015091300010681152
Batch 21/64 loss: 0.03551524877548218
Batch 22/64 loss: 0.0022305846214294434
Batch 23/64 loss: 0.01669532060623169
Batch 24/64 loss: 0.020471394062042236
Batch 25/64 loss: 0.012030482292175293
Batch 26/64 loss: 0.015150904655456543
Batch 27/64 loss: 0.02435159683227539
Batch 28/64 loss: 0.017345905303955078
Batch 29/64 loss: -0.03241229057312012
Batch 30/64 loss: 0.020374953746795654
Batch 31/64 loss: 0.0348130464553833
Batch 32/64 loss: 0.014985203742980957
Batch 33/64 loss: 0.023683607578277588
Batch 34/64 loss: 0.007564365863800049
Batch 35/64 loss: 0.004592716693878174
Batch 36/64 loss: 0.005752921104431152
Batch 37/64 loss: 0.028709590435028076
Batch 38/64 loss: 0.005303919315338135
Batch 39/64 loss: 0.03562474250793457
Batch 40/64 loss: 0.011523604393005371
Batch 41/64 loss: 0.015823304653167725
Batch 42/64 loss: 0.04375302791595459
Batch 43/64 loss: 0.02491658926010132
Batch 44/64 loss: 0.030962467193603516
Batch 45/64 loss: 0.013247668743133545
Batch 46/64 loss: 0.01898980140686035
Batch 47/64 loss: 0.020823419094085693
Batch 48/64 loss: 0.054327309131622314
Batch 49/64 loss: 0.0041959285736083984
Batch 50/64 loss: 0.039825260639190674
Batch 51/64 loss: 0.009710311889648438
Batch 52/64 loss: 0.023274660110473633
Batch 53/64 loss: 0.010002613067626953
Batch 54/64 loss: 0.014786779880523682
Batch 55/64 loss: 0.03802090883255005
Batch 56/64 loss: -0.0001271963119506836
Batch 57/64 loss: 0.006759822368621826
Batch 58/64 loss: -0.001965165138244629
Batch 59/64 loss: -0.010983824729919434
Batch 60/64 loss: 0.0015274882316589355
Batch 61/64 loss: 0.002938210964202881
Batch 62/64 loss: 0.008037447929382324
Batch 63/64 loss: -0.0013270378112792969
Batch 64/64 loss: -0.0013544559478759766
Epoch 415  Train loss: 0.0160642100315468  Val loss: 0.12213371852828875
Epoch 416
-------------------------------
Batch 1/64 loss: 0.003737211227416992
Batch 2/64 loss: 0.010696768760681152
Batch 3/64 loss: -0.016325712203979492
Batch 4/64 loss: 0.007052361965179443
Batch 5/64 loss: 0.002415001392364502
Batch 6/64 loss: -0.011290431022644043
Batch 7/64 loss: -0.01164853572845459
Batch 8/64 loss: 0.000209808349609375
Batch 9/64 loss: 0.017539262771606445
Batch 10/64 loss: 0.029206931591033936
Batch 11/64 loss: 0.0030765533447265625
Batch 12/64 loss: 0.018839359283447266
Batch 13/64 loss: 0.013613998889923096
Batch 14/64 loss: 0.008814573287963867
Batch 15/64 loss: -0.0013977289199829102
Batch 16/64 loss: 0.0032674074172973633
Batch 17/64 loss: 0.0297926664352417
Batch 18/64 loss: 0.04399216175079346
Batch 19/64 loss: 0.021930694580078125
Batch 20/64 loss: 0.0009895563125610352
Batch 21/64 loss: 0.03484213352203369
Batch 22/64 loss: 0.007729947566986084
Batch 23/64 loss: 0.012455463409423828
Batch 24/64 loss: 0.019314050674438477
Batch 25/64 loss: 0.01319652795791626
Batch 26/64 loss: -0.011323690414428711
Batch 27/64 loss: 0.037539780139923096
Batch 28/64 loss: 0.05700325965881348
Batch 29/64 loss: 0.006907403469085693
Batch 30/64 loss: 0.025615811347961426
Batch 31/64 loss: 0.013139665126800537
Batch 32/64 loss: 0.009747147560119629
Batch 33/64 loss: 0.004616916179656982
Batch 34/64 loss: 0.02174687385559082
Batch 35/64 loss: 0.003984808921813965
Batch 36/64 loss: 0.017462193965911865
Batch 37/64 loss: 0.04021930694580078
Batch 38/64 loss: 0.008500158786773682
Batch 39/64 loss: 0.020960748195648193
Batch 40/64 loss: 0.009856700897216797
Batch 41/64 loss: -0.0015799403190612793
Batch 42/64 loss: 0.015696287155151367
Batch 43/64 loss: 0.016439735889434814
Batch 44/64 loss: 0.0383906364440918
Batch 45/64 loss: 0.02719259262084961
Batch 46/64 loss: 0.0023039579391479492
Batch 47/64 loss: 0.013998031616210938
Batch 48/64 loss: -0.009752273559570312
Batch 49/64 loss: 0.015566647052764893
Batch 50/64 loss: 0.029388070106506348
Batch 51/64 loss: -0.004162967205047607
Batch 52/64 loss: 0.02003002166748047
Batch 53/64 loss: 0.024770379066467285
Batch 54/64 loss: 0.012616634368896484
Batch 55/64 loss: -0.0014308691024780273
Batch 56/64 loss: 0.012475728988647461
Batch 57/64 loss: 0.05875813961029053
Batch 58/64 loss: -0.0058536529541015625
Batch 59/64 loss: 0.03088557720184326
Batch 60/64 loss: 0.007072091102600098
Batch 61/64 loss: 0.012339293956756592
Batch 62/64 loss: 0.043031930923461914
Batch 63/64 loss: 0.024974703788757324
Batch 64/64 loss: 0.06760537624359131
Epoch 416  Train loss: 0.015088343620300293  Val loss: 0.12321297819262109
Epoch 417
-------------------------------
Batch 1/64 loss: -0.007801413536071777
Batch 2/64 loss: 0.0004191398620605469
Batch 3/64 loss: 0.007872223854064941
Batch 4/64 loss: 0.001770317554473877
Batch 5/64 loss: 0.04496967792510986
Batch 6/64 loss: 0.01934492588043213
Batch 7/64 loss: 0.016897380352020264
Batch 8/64 loss: 0.038167715072631836
Batch 9/64 loss: -0.0037189722061157227
Batch 10/64 loss: 0.005703449249267578
Batch 11/64 loss: 0.0077381134033203125
Batch 12/64 loss: 0.01887112855911255
Batch 13/64 loss: 0.0068076252937316895
Batch 14/64 loss: 0.02353304624557495
Batch 15/64 loss: 0.03014671802520752
Batch 16/64 loss: -0.006541430950164795
Batch 17/64 loss: 0.045783817768096924
Batch 18/64 loss: 0.002325892448425293
Batch 19/64 loss: 0.0053081512451171875
Batch 20/64 loss: 0.03486686944961548
Batch 21/64 loss: 0.026221156120300293
Batch 22/64 loss: -0.0038352608680725098
Batch 23/64 loss: 0.01787400245666504
Batch 24/64 loss: 0.004578769207000732
Batch 25/64 loss: 0.013030469417572021
Batch 26/64 loss: 0.01271730661392212
Batch 27/64 loss: 0.0010764598846435547
Batch 28/64 loss: -0.009785890579223633
Batch 29/64 loss: -0.014691531658172607
Batch 30/64 loss: 0.004255533218383789
Batch 31/64 loss: 0.02080780267715454
Batch 32/64 loss: -0.014412462711334229
Batch 33/64 loss: 0.026457786560058594
Batch 34/64 loss: 0.016553103923797607
Batch 35/64 loss: 0.000440061092376709
Batch 36/64 loss: 0.039939939975738525
Batch 37/64 loss: 0.010052859783172607
Batch 38/64 loss: 0.014407873153686523
Batch 39/64 loss: 0.025506019592285156
Batch 40/64 loss: 0.009506464004516602
Batch 41/64 loss: 0.02295994758605957
Batch 42/64 loss: 0.04601484537124634
Batch 43/64 loss: 0.029932141304016113
Batch 44/64 loss: -0.022056221961975098
Batch 45/64 loss: 0.022954106330871582
Batch 46/64 loss: 0.05600178241729736
Batch 47/64 loss: -0.0016170740127563477
Batch 48/64 loss: 0.004707396030426025
Batch 49/64 loss: 0.020091891288757324
Batch 50/64 loss: 0.028281867504119873
Batch 51/64 loss: 0.030281543731689453
Batch 52/64 loss: 0.020253002643585205
Batch 53/64 loss: 0.0021016597747802734
Batch 54/64 loss: 0.006273627281188965
Batch 55/64 loss: -0.0013241171836853027
Batch 56/64 loss: 0.013340353965759277
Batch 57/64 loss: 0.01691579818725586
Batch 58/64 loss: 0.0187985897064209
Batch 59/64 loss: 0.036621272563934326
Batch 60/64 loss: 0.02154242992401123
Batch 61/64 loss: 0.021657586097717285
Batch 62/64 loss: -0.013040304183959961
Batch 63/64 loss: 0.006413936614990234
Batch 64/64 loss: 0.015757620334625244
Epoch 417  Train loss: 0.013993554723029043  Val loss: 0.11973667943600527
Epoch 418
-------------------------------
Batch 1/64 loss: 0.013086676597595215
Batch 2/64 loss: 0.0026494264602661133
Batch 3/64 loss: -0.0019310712814331055
Batch 4/64 loss: 0.028638720512390137
Batch 5/64 loss: 0.01516258716583252
Batch 6/64 loss: 0.01377803087234497
Batch 7/64 loss: -0.01113659143447876
Batch 8/64 loss: 0.004729270935058594
Batch 9/64 loss: 0.03245973587036133
Batch 10/64 loss: 0.018466711044311523
Batch 11/64 loss: 0.009496927261352539
Batch 12/64 loss: -0.002459108829498291
Batch 13/64 loss: -0.01040273904800415
Batch 14/64 loss: -0.010694324970245361
Batch 15/64 loss: 0.01345813274383545
Batch 16/64 loss: 0.027371585369110107
Batch 17/64 loss: 0.012771666049957275
Batch 18/64 loss: 0.01586061716079712
Batch 19/64 loss: -0.006944239139556885
Batch 20/64 loss: -0.003931760787963867
Batch 21/64 loss: 0.013154983520507812
Batch 22/64 loss: -0.0048972368240356445
Batch 23/64 loss: 0.02834099531173706
Batch 24/64 loss: -0.003984630107879639
Batch 25/64 loss: 0.014312922954559326
Batch 26/64 loss: 0.05628788471221924
Batch 27/64 loss: 0.015828371047973633
Batch 28/64 loss: 0.004358410835266113
Batch 29/64 loss: -0.004791617393493652
Batch 30/64 loss: -0.013878405094146729
Batch 31/64 loss: 0.006017446517944336
Batch 32/64 loss: 0.04119133949279785
Batch 33/64 loss: 0.012575984001159668
Batch 34/64 loss: 0.011228442192077637
Batch 35/64 loss: 0.028249144554138184
Batch 36/64 loss: 0.01679140329360962
Batch 37/64 loss: 0.010969221591949463
Batch 38/64 loss: 0.024765968322753906
Batch 39/64 loss: 0.008652269840240479
Batch 40/64 loss: 0.008159518241882324
Batch 41/64 loss: 0.03544062376022339
Batch 42/64 loss: 0.01577991247177124
Batch 43/64 loss: 0.03918111324310303
Batch 44/64 loss: 0.046842992305755615
Batch 45/64 loss: 0.017314791679382324
Batch 46/64 loss: -0.0038765668869018555
Batch 47/64 loss: 0.002045929431915283
Batch 48/64 loss: 0.023351848125457764
Batch 49/64 loss: 0.024172842502593994
Batch 50/64 loss: 0.023790955543518066
Batch 51/64 loss: 0.024250566959381104
Batch 52/64 loss: 0.017528057098388672
Batch 53/64 loss: 0.020930826663970947
Batch 54/64 loss: 0.004184305667877197
Batch 55/64 loss: 0.05555671453475952
Batch 56/64 loss: -0.011217772960662842
Batch 57/64 loss: 0.026975035667419434
Batch 58/64 loss: 0.04763740301132202
Batch 59/64 loss: 0.044491469860076904
Batch 60/64 loss: 0.03806746006011963
Batch 61/64 loss: 0.012944340705871582
Batch 62/64 loss: 0.01248544454574585
Batch 63/64 loss: 0.016537189483642578
Batch 64/64 loss: 0.01544332504272461
Epoch 418  Train loss: 0.015368794459922641  Val loss: 0.12028283720573608
Epoch 419
-------------------------------
Batch 1/64 loss: 0.019260406494140625
Batch 2/64 loss: 0.0106162428855896
Batch 3/64 loss: 0.02037811279296875
Batch 4/64 loss: 0.015743553638458252
Batch 5/64 loss: -0.009565651416778564
Batch 6/64 loss: 0.006645083427429199
Batch 7/64 loss: 0.020757198333740234
Batch 8/64 loss: 0.015804588794708252
Batch 9/64 loss: 0.00020313262939453125
Batch 10/64 loss: 0.010775566101074219
Batch 11/64 loss: -0.0058574676513671875
Batch 12/64 loss: 0.02094024419784546
Batch 13/64 loss: 0.01395571231842041
Batch 14/64 loss: 0.02105855941772461
Batch 15/64 loss: -0.007996678352355957
Batch 16/64 loss: 0.04394090175628662
Batch 17/64 loss: 0.003204941749572754
Batch 18/64 loss: 0.003183424472808838
Batch 19/64 loss: 0.050733089447021484
Batch 20/64 loss: 0.01713871955871582
Batch 21/64 loss: 0.035299479961395264
Batch 22/64 loss: 0.01465141773223877
Batch 23/64 loss: 0.024752020835876465
Batch 24/64 loss: -0.024613022804260254
Batch 25/64 loss: 0.03544420003890991
Batch 26/64 loss: 0.015956580638885498
Batch 27/64 loss: 0.011542320251464844
Batch 28/64 loss: 0.00905519723892212
Batch 29/64 loss: 0.008128643035888672
Batch 30/64 loss: 1.5676021575927734e-05
Batch 31/64 loss: 0.037456512451171875
Batch 32/64 loss: 0.03316640853881836
Batch 33/64 loss: 0.027181267738342285
Batch 34/64 loss: 0.01855635643005371
Batch 35/64 loss: 0.003061652183532715
Batch 36/64 loss: 0.022584617137908936
Batch 37/64 loss: -0.0032570362091064453
Batch 38/64 loss: -0.009502887725830078
Batch 39/64 loss: 0.03572583198547363
Batch 40/64 loss: -0.0014494061470031738
Batch 41/64 loss: 0.03711128234863281
Batch 42/64 loss: 0.004463493824005127
Batch 43/64 loss: 0.022349774837493896
Batch 44/64 loss: 0.033118605613708496
Batch 45/64 loss: -0.00314486026763916
Batch 46/64 loss: 0.017300426959991455
Batch 47/64 loss: -0.002338886260986328
Batch 48/64 loss: 0.03324156999588013
Batch 49/64 loss: -0.0006156563758850098
Batch 50/64 loss: 0.032711803913116455
Batch 51/64 loss: 0.025618910789489746
Batch 52/64 loss: 0.01922476291656494
Batch 53/64 loss: 0.01393139362335205
Batch 54/64 loss: -0.0037282705307006836
Batch 55/64 loss: 0.022580862045288086
Batch 56/64 loss: 0.0036803483963012695
Batch 57/64 loss: 0.0016938447952270508
Batch 58/64 loss: -0.01838064193725586
Batch 59/64 loss: -0.0015186667442321777
Batch 60/64 loss: 0.033148884773254395
Batch 61/64 loss: 0.032496869564056396
Batch 62/64 loss: 0.00542902946472168
Batch 63/64 loss: -0.01397550106048584
Batch 64/64 loss: -0.0015387535095214844
Epoch 419  Train loss: 0.013457581576179056  Val loss: 0.11681817856031596
Epoch 420
-------------------------------
Batch 1/64 loss: 0.008511722087860107
Batch 2/64 loss: 0.02427816390991211
Batch 3/64 loss: 0.006476402282714844
Batch 4/64 loss: 0.004497826099395752
Batch 5/64 loss: -0.007079184055328369
Batch 6/64 loss: 0.04363834857940674
Batch 7/64 loss: 0.010725736618041992
Batch 8/64 loss: -0.002732396125793457
Batch 9/64 loss: 0.02234339714050293
Batch 10/64 loss: 0.0077245235443115234
Batch 11/64 loss: 0.016484737396240234
Batch 12/64 loss: 0.030185222625732422
Batch 13/64 loss: 0.02941727638244629
Batch 14/64 loss: 0.021221518516540527
Batch 15/64 loss: -0.008994221687316895
Batch 16/64 loss: 0.021839797496795654
Batch 17/64 loss: -0.002768397331237793
Batch 18/64 loss: 0.032732605934143066
Batch 19/64 loss: 0.015007495880126953
Batch 20/64 loss: -0.00043839216232299805
Batch 21/64 loss: -0.0013860464096069336
Batch 22/64 loss: -0.00920790433883667
Batch 23/64 loss: 0.0036823153495788574
Batch 24/64 loss: 0.016507208347320557
Batch 25/64 loss: 0.023976624011993408
Batch 26/64 loss: 0.02551257610321045
Batch 27/64 loss: -0.005620300769805908
Batch 28/64 loss: -0.003426492214202881
Batch 29/64 loss: 0.021041393280029297
Batch 30/64 loss: 0.002308070659637451
Batch 31/64 loss: 0.012326478958129883
Batch 32/64 loss: 0.028598904609680176
Batch 33/64 loss: -0.013492166996002197
Batch 34/64 loss: 0.02780282497406006
Batch 35/64 loss: 0.024837255477905273
Batch 36/64 loss: 0.012382984161376953
Batch 37/64 loss: -0.011917829513549805
Batch 38/64 loss: 0.016621410846710205
Batch 39/64 loss: -0.00650632381439209
Batch 40/64 loss: 0.0200960636138916
Batch 41/64 loss: 0.01886153221130371
Batch 42/64 loss: 0.026802420616149902
Batch 43/64 loss: -0.011591792106628418
Batch 44/64 loss: 0.01959162950515747
Batch 45/64 loss: 0.02142965793609619
Batch 46/64 loss: 0.05035209655761719
Batch 47/64 loss: 0.009411334991455078
Batch 48/64 loss: 0.009188234806060791
Batch 49/64 loss: 0.0337558388710022
Batch 50/64 loss: 0.04088127613067627
Batch 51/64 loss: 0.02875494956970215
Batch 52/64 loss: 0.012279748916625977
Batch 53/64 loss: 0.039860427379608154
Batch 54/64 loss: 0.010641634464263916
Batch 55/64 loss: 0.022080540657043457
Batch 56/64 loss: -0.0006788969039916992
Batch 57/64 loss: 0.013891935348510742
Batch 58/64 loss: -0.0014376640319824219
Batch 59/64 loss: 0.024105727672576904
Batch 60/64 loss: 0.01728355884552002
Batch 61/64 loss: 0.008254170417785645
Batch 62/64 loss: 0.004171431064605713
Batch 63/64 loss: 0.02151280641555786
Batch 64/64 loss: 0.04880499839782715
Epoch 420  Train loss: 0.01432495023690018  Val loss: 0.12187295537633994
Epoch 421
-------------------------------
Batch 1/64 loss: 0.03967934846878052
Batch 2/64 loss: 0.004380702972412109
Batch 3/64 loss: 0.01635509729385376
Batch 4/64 loss: 0.007543742656707764
Batch 5/64 loss: 0.0035987496376037598
Batch 6/64 loss: -0.011051297187805176
Batch 7/64 loss: 0.0017778873443603516
Batch 8/64 loss: 0.00196760892868042
Batch 9/64 loss: -0.011361360549926758
Batch 10/64 loss: 0.020761489868164062
Batch 11/64 loss: 0.0034939050674438477
Batch 12/64 loss: 0.006575584411621094
Batch 13/64 loss: 0.029645800590515137
Batch 14/64 loss: 0.00194549560546875
Batch 15/64 loss: -0.02039015293121338
Batch 16/64 loss: -0.011004269123077393
Batch 17/64 loss: 0.01750922203063965
Batch 18/64 loss: 0.01237940788269043
Batch 19/64 loss: 0.012355148792266846
Batch 20/64 loss: 0.04583275318145752
Batch 21/64 loss: -0.0020678043365478516
Batch 22/64 loss: 0.020857572555541992
Batch 23/64 loss: 0.03403961658477783
Batch 24/64 loss: 0.01910698413848877
Batch 25/64 loss: -0.0066890716552734375
Batch 26/64 loss: -0.0052474141120910645
Batch 27/64 loss: 0.028544127941131592
Batch 28/64 loss: -0.02125781774520874
Batch 29/64 loss: 0.0060440897941589355
Batch 30/64 loss: 0.012739360332489014
Batch 31/64 loss: 0.005904972553253174
Batch 32/64 loss: 0.01250302791595459
Batch 33/64 loss: 0.012984037399291992
Batch 34/64 loss: 0.011060953140258789
Batch 35/64 loss: 0.044374167919158936
Batch 36/64 loss: -0.01861727237701416
Batch 37/64 loss: 0.00018227100372314453
Batch 38/64 loss: 0.026411592960357666
Batch 39/64 loss: 0.005550861358642578
Batch 40/64 loss: 0.03904151916503906
Batch 41/64 loss: 0.0007061362266540527
Batch 42/64 loss: 0.02341938018798828
Batch 43/64 loss: 0.0005702972412109375
Batch 44/64 loss: 0.012925863265991211
Batch 45/64 loss: 0.03849518299102783
Batch 46/64 loss: 0.03538781404495239
Batch 47/64 loss: 0.019518733024597168
Batch 48/64 loss: 0.014972686767578125
Batch 49/64 loss: 0.009478271007537842
Batch 50/64 loss: -0.0019055604934692383
Batch 51/64 loss: 0.05745089054107666
Batch 52/64 loss: 0.020268559455871582
Batch 53/64 loss: 0.007715284824371338
Batch 54/64 loss: 0.003609895706176758
Batch 55/64 loss: 0.010361194610595703
Batch 56/64 loss: 0.037539124488830566
Batch 57/64 loss: 0.020897746086120605
Batch 58/64 loss: 0.04408919811248779
Batch 59/64 loss: 0.03936570882797241
Batch 60/64 loss: 0.013236701488494873
Batch 61/64 loss: 0.012638449668884277
Batch 62/64 loss: 0.004716157913208008
Batch 63/64 loss: 0.03684777021408081
Batch 64/64 loss: 0.01502913236618042
Epoch 421  Train loss: 0.01366334078358669  Val loss: 0.12099403409204122
Epoch 422
-------------------------------
Batch 1/64 loss: 0.008573353290557861
Batch 2/64 loss: -0.012492358684539795
Batch 3/64 loss: 0.009104132652282715
Batch 4/64 loss: -0.008624613285064697
Batch 5/64 loss: 0.02963930368423462
Batch 6/64 loss: 0.005498826503753662
Batch 7/64 loss: 0.036364078521728516
Batch 8/64 loss: -0.0015704631805419922
Batch 9/64 loss: -0.014992117881774902
Batch 10/64 loss: -0.008534908294677734
Batch 11/64 loss: 0.02032715082168579
Batch 12/64 loss: 0.019580602645874023
Batch 13/64 loss: 0.0018420219421386719
Batch 14/64 loss: 0.004909813404083252
Batch 15/64 loss: -0.012925446033477783
Batch 16/64 loss: -0.0007933974266052246
Batch 17/64 loss: 0.013798952102661133
Batch 18/64 loss: 0.019577622413635254
Batch 19/64 loss: -0.003098726272583008
Batch 20/64 loss: 0.0025932788848876953
Batch 21/64 loss: 0.036412715911865234
Batch 22/64 loss: 0.009929180145263672
Batch 23/64 loss: 0.029903113842010498
Batch 24/64 loss: 0.008484840393066406
Batch 25/64 loss: -0.011409461498260498
Batch 26/64 loss: 0.02564150094985962
Batch 27/64 loss: -0.0024477243423461914
Batch 28/64 loss: 0.02773982286453247
Batch 29/64 loss: 0.0041030049324035645
Batch 30/64 loss: 0.000731348991394043
Batch 31/64 loss: 0.013134002685546875
Batch 32/64 loss: 0.0256955623626709
Batch 33/64 loss: 0.01696610450744629
Batch 34/64 loss: 0.0047067999839782715
Batch 35/64 loss: -0.004438579082489014
Batch 36/64 loss: 0.007745623588562012
Batch 37/64 loss: 0.008777141571044922
Batch 38/64 loss: 0.006755709648132324
Batch 39/64 loss: -0.006345391273498535
Batch 40/64 loss: 0.025264739990234375
Batch 41/64 loss: 0.039711177349090576
Batch 42/64 loss: 0.0137673020362854
Batch 43/64 loss: 0.03864932060241699
Batch 44/64 loss: 0.037186503410339355
Batch 45/64 loss: 0.01792609691619873
Batch 46/64 loss: 0.03376263380050659
Batch 47/64 loss: 0.0455930233001709
Batch 48/64 loss: -0.00767594575881958
Batch 49/64 loss: 0.001687169075012207
Batch 50/64 loss: 0.021991312503814697
Batch 51/64 loss: 0.01644265651702881
Batch 52/64 loss: -0.0017497539520263672
Batch 53/64 loss: 0.019468605518341064
Batch 54/64 loss: 0.03244972229003906
Batch 55/64 loss: 0.01694542169570923
Batch 56/64 loss: 0.008579134941101074
Batch 57/64 loss: 0.017802953720092773
Batch 58/64 loss: 0.0017556548118591309
Batch 59/64 loss: 0.027488231658935547
Batch 60/64 loss: 0.01085197925567627
Batch 61/64 loss: -0.003922700881958008
Batch 62/64 loss: 0.052623093128204346
Batch 63/64 loss: 0.01161724328994751
Batch 64/64 loss: 0.028747260570526123
Epoch 422  Train loss: 0.01271589713938096  Val loss: 0.11743730101798408
Epoch 423
-------------------------------
Batch 1/64 loss: 0.024906814098358154
Batch 2/64 loss: 0.0059967041015625
Batch 3/64 loss: -0.01597416400909424
Batch 4/64 loss: 0.0027273893356323242
Batch 5/64 loss: -0.028668105602264404
Batch 6/64 loss: 0.01594388484954834
Batch 7/64 loss: 0.004396319389343262
Batch 8/64 loss: 0.025003790855407715
Batch 9/64 loss: 0.06190037727355957
Batch 10/64 loss: -0.023559868335723877
Batch 11/64 loss: -0.02017754316329956
Batch 12/64 loss: 0.01400226354598999
Batch 13/64 loss: 0.01826566457748413
Batch 14/64 loss: -0.004014372825622559
Batch 15/64 loss: 0.030125737190246582
Batch 16/64 loss: 0.0007156729698181152
Batch 17/64 loss: 0.004718184471130371
Batch 18/64 loss: 0.025251388549804688
Batch 19/64 loss: -0.006270170211791992
Batch 20/64 loss: 0.0190657377243042
Batch 21/64 loss: -0.006379663944244385
Batch 22/64 loss: 0.006929993629455566
Batch 23/64 loss: 0.004172861576080322
Batch 24/64 loss: 0.004960477352142334
Batch 25/64 loss: -0.020810425281524658
Batch 26/64 loss: 0.012537717819213867
Batch 27/64 loss: -0.006185770034790039
Batch 28/64 loss: -0.01328730583190918
Batch 29/64 loss: -0.003885984420776367
Batch 30/64 loss: 0.03970038890838623
Batch 31/64 loss: 0.003944993019104004
Batch 32/64 loss: 0.010363996028900146
Batch 33/64 loss: 0.006831943988800049
Batch 34/64 loss: 0.04014986753463745
Batch 35/64 loss: 0.012468993663787842
Batch 36/64 loss: 0.018320322036743164
Batch 37/64 loss: 0.03668630123138428
Batch 38/64 loss: 0.030704975128173828
Batch 39/64 loss: 0.016341686248779297
Batch 40/64 loss: 0.025281906127929688
Batch 41/64 loss: 0.023342549800872803
Batch 42/64 loss: 0.004493415355682373
Batch 43/64 loss: 0.023931443691253662
Batch 44/64 loss: 0.023893654346466064
Batch 45/64 loss: 0.023626208305358887
Batch 46/64 loss: -0.004038870334625244
Batch 47/64 loss: 0.02604687213897705
Batch 48/64 loss: 0.06175762414932251
Batch 49/64 loss: 0.007718682289123535
Batch 50/64 loss: 0.022569715976715088
Batch 51/64 loss: 0.016017675399780273
Batch 52/64 loss: 0.008119523525238037
Batch 53/64 loss: -0.003987133502960205
Batch 54/64 loss: -0.007898509502410889
Batch 55/64 loss: 0.025975048542022705
Batch 56/64 loss: 0.03357088565826416
Batch 57/64 loss: 0.04977560043334961
Batch 58/64 loss: 0.0259857177734375
Batch 59/64 loss: 0.026348471641540527
Batch 60/64 loss: 0.010015547275543213
Batch 61/64 loss: 0.015483260154724121
Batch 62/64 loss: -0.007695317268371582
Batch 63/64 loss: 0.014584660530090332
Batch 64/64 loss: 0.013482451438903809
Epoch 423  Train loss: 0.012595318345462575  Val loss: 0.12027446449417428
Epoch 424
-------------------------------
Batch 1/64 loss: -0.0013658404350280762
Batch 2/64 loss: 0.0005916357040405273
Batch 3/64 loss: -0.006385385990142822
Batch 4/64 loss: -0.007302641868591309
Batch 5/64 loss: 0.015416741371154785
Batch 6/64 loss: 0.0046887993812561035
Batch 7/64 loss: 0.01700127124786377
Batch 8/64 loss: 0.026222825050354004
Batch 9/64 loss: 0.021428227424621582
Batch 10/64 loss: -0.0007590055465698242
Batch 11/64 loss: 0.011640489101409912
Batch 12/64 loss: 0.010447442531585693
Batch 13/64 loss: -0.01636672019958496
Batch 14/64 loss: 0.03133893013000488
Batch 15/64 loss: 0.008259773254394531
Batch 16/64 loss: 0.005873143672943115
Batch 17/64 loss: 0.01716846227645874
Batch 18/64 loss: 0.017675936222076416
Batch 19/64 loss: -0.004905343055725098
Batch 20/64 loss: 0.01596301794052124
Batch 21/64 loss: 0.008195817470550537
Batch 22/64 loss: 0.017290055751800537
Batch 23/64 loss: 0.03202009201049805
Batch 24/64 loss: 0.030257225036621094
Batch 25/64 loss: 0.016294121742248535
Batch 26/64 loss: 0.004590392112731934
Batch 27/64 loss: 0.008035421371459961
Batch 28/64 loss: -0.008125841617584229
Batch 29/64 loss: -0.004337787628173828
Batch 30/64 loss: -0.006692767143249512
Batch 31/64 loss: -0.02213364839553833
Batch 32/64 loss: 0.0008125901222229004
Batch 33/64 loss: 0.025749623775482178
Batch 34/64 loss: 0.024210631847381592
Batch 35/64 loss: 0.01504361629486084
Batch 36/64 loss: 0.018195807933807373
Batch 37/64 loss: 0.01856774091720581
Batch 38/64 loss: -0.00022792816162109375
Batch 39/64 loss: 0.031558096408843994
Batch 40/64 loss: 0.03852534294128418
Batch 41/64 loss: 0.02922898530960083
Batch 42/64 loss: 0.0036046504974365234
Batch 43/64 loss: 0.023683607578277588
Batch 44/64 loss: 0.008971691131591797
Batch 45/64 loss: 0.03854638338088989
Batch 46/64 loss: 0.043594956398010254
Batch 47/64 loss: 0.021556615829467773
Batch 48/64 loss: -0.006310462951660156
Batch 49/64 loss: 0.010381937026977539
Batch 50/64 loss: 0.022020339965820312
Batch 51/64 loss: 0.021267235279083252
Batch 52/64 loss: 0.013582468032836914
Batch 53/64 loss: 0.015128135681152344
Batch 54/64 loss: 0.01977682113647461
Batch 55/64 loss: 0.02581113576889038
Batch 56/64 loss: 0.01945018768310547
Batch 57/64 loss: -0.0005906224250793457
Batch 58/64 loss: 0.007750451564788818
Batch 59/64 loss: 0.0010905265808105469
Batch 60/64 loss: 0.03182423114776611
Batch 61/64 loss: 0.020766019821166992
Batch 62/64 loss: 0.07379072904586792
Batch 63/64 loss: 0.03960442543029785
Batch 64/64 loss: 0.0006496310234069824
Epoch 424  Train loss: 0.014109459344078513  Val loss: 0.12187067851987492
Epoch 425
-------------------------------
Batch 1/64 loss: 0.0034697651863098145
Batch 2/64 loss: 0.003605663776397705
Batch 3/64 loss: 0.0024968981742858887
Batch 4/64 loss: -0.03029733896255493
Batch 5/64 loss: 0.02044743299484253
Batch 6/64 loss: 0.01971745491027832
Batch 7/64 loss: 0.01941859722137451
Batch 8/64 loss: -0.016144931316375732
Batch 9/64 loss: 0.07435309886932373
Batch 10/64 loss: -0.0005743503570556641
Batch 11/64 loss: -0.030202388763427734
Batch 12/64 loss: 0.009087562561035156
Batch 13/64 loss: 0.03343111276626587
Batch 14/64 loss: 0.016681671142578125
Batch 15/64 loss: 0.006428778171539307
Batch 16/64 loss: 0.01787465810775757
Batch 17/64 loss: 0.0020995736122131348
Batch 18/64 loss: 0.012445390224456787
Batch 19/64 loss: 0.0028933286666870117
Batch 20/64 loss: 0.03417313098907471
Batch 21/64 loss: 0.017857015132904053
Batch 22/64 loss: -0.005808770656585693
Batch 23/64 loss: 0.0036843419075012207
Batch 24/64 loss: -0.009963512420654297
Batch 25/64 loss: 0.03885573148727417
Batch 26/64 loss: 0.01766568422317505
Batch 27/64 loss: 0.01699197292327881
Batch 28/64 loss: 0.05562925338745117
Batch 29/64 loss: 0.010617554187774658
Batch 30/64 loss: 0.0004553794860839844
Batch 31/64 loss: 0.009797334671020508
Batch 32/64 loss: -0.004929304122924805
Batch 33/64 loss: 0.028120577335357666
Batch 34/64 loss: 0.011993050575256348
Batch 35/64 loss: 0.002392709255218506
Batch 36/64 loss: 0.02710103988647461
Batch 37/64 loss: 0.021376848220825195
Batch 38/64 loss: 0.017969489097595215
Batch 39/64 loss: 0.036011457443237305
Batch 40/64 loss: 0.024409949779510498
Batch 41/64 loss: 0.03276050090789795
Batch 42/64 loss: 0.038568735122680664
Batch 43/64 loss: 0.025821685791015625
Batch 44/64 loss: -0.012228786945343018
Batch 45/64 loss: 0.010544657707214355
Batch 46/64 loss: 0.06701713800430298
Batch 47/64 loss: -0.009249746799468994
Batch 48/64 loss: 0.02104926109313965
Batch 49/64 loss: 0.009562373161315918
Batch 50/64 loss: -0.00023889541625976562
Batch 51/64 loss: 0.01927858591079712
Batch 52/64 loss: -0.016624867916107178
Batch 53/64 loss: 0.012381374835968018
Batch 54/64 loss: 0.0007497072219848633
Batch 55/64 loss: 0.01941704750061035
Batch 56/64 loss: 0.019498467445373535
Batch 57/64 loss: 0.023648977279663086
Batch 58/64 loss: -0.009258389472961426
Batch 59/64 loss: 0.044866204261779785
Batch 60/64 loss: 0.02366262674331665
Batch 61/64 loss: 0.02615177631378174
Batch 62/64 loss: -0.007321000099182129
Batch 63/64 loss: 0.0409277081489563
Batch 64/64 loss: 0.008411705493927002
Epoch 425  Train loss: 0.014257675759932574  Val loss: 0.11881533766939878
Epoch 426
-------------------------------
Batch 1/64 loss: -0.0030595064163208008
Batch 2/64 loss: 0.0006062984466552734
Batch 3/64 loss: -0.0011982917785644531
Batch 4/64 loss: 0.0016062259674072266
Batch 5/64 loss: -0.0034197568893432617
Batch 6/64 loss: -0.003927886486053467
Batch 7/64 loss: -0.0047997236251831055
Batch 8/64 loss: 0.029666781425476074
Batch 9/64 loss: -0.004317164421081543
Batch 10/64 loss: -0.015729248523712158
Batch 11/64 loss: 0.011976480484008789
Batch 12/64 loss: 0.010459363460540771
Batch 13/64 loss: 0.00874239206314087
Batch 14/64 loss: 0.0055506229400634766
Batch 15/64 loss: 0.022575318813323975
Batch 16/64 loss: -0.0018993616104125977
Batch 17/64 loss: -0.013978123664855957
Batch 18/64 loss: 0.04097628593444824
Batch 19/64 loss: -0.013521790504455566
Batch 20/64 loss: 0.036928772926330566
Batch 21/64 loss: 0.009439051151275635
Batch 22/64 loss: 0.028322339057922363
Batch 23/64 loss: 0.040492117404937744
Batch 24/64 loss: 0.004937410354614258
Batch 25/64 loss: 0.03631526231765747
Batch 26/64 loss: 0.02266669273376465
Batch 27/64 loss: 0.024742424488067627
Batch 28/64 loss: 0.014592766761779785
Batch 29/64 loss: 0.0014647841453552246
Batch 30/64 loss: 0.014161109924316406
Batch 31/64 loss: 0.022282183170318604
Batch 32/64 loss: 0.009930670261383057
Batch 33/64 loss: 0.03228271007537842
Batch 34/64 loss: 0.014688372611999512
Batch 35/64 loss: 0.034683406352996826
Batch 36/64 loss: 0.03148108720779419
Batch 37/64 loss: 0.019567430019378662
Batch 38/64 loss: 0.015335440635681152
Batch 39/64 loss: 0.02551943063735962
Batch 40/64 loss: 0.023303866386413574
Batch 41/64 loss: 0.007380485534667969
Batch 42/64 loss: 0.014094352722167969
Batch 43/64 loss: 0.024089932441711426
Batch 44/64 loss: -0.0011888742446899414
Batch 45/64 loss: 0.008885324001312256
Batch 46/64 loss: -0.001088857650756836
Batch 47/64 loss: 0.0145302414894104
Batch 48/64 loss: 0.025944828987121582
Batch 49/64 loss: 0.01604020595550537
Batch 50/64 loss: 0.005370080471038818
Batch 51/64 loss: -0.00034797191619873047
Batch 52/64 loss: 0.010655224323272705
Batch 53/64 loss: -0.003311753273010254
Batch 54/64 loss: 0.025226473808288574
Batch 55/64 loss: 0.037745535373687744
Batch 56/64 loss: 0.011691093444824219
Batch 57/64 loss: 0.04116541147232056
Batch 58/64 loss: 0.0031655430793762207
Batch 59/64 loss: 0.02821892499923706
Batch 60/64 loss: 0.008536458015441895
Batch 61/64 loss: 0.049065589904785156
Batch 62/64 loss: -0.003109276294708252
Batch 63/64 loss: 0.0036429166793823242
Batch 64/64 loss: 0.02590000629425049
Epoch 426  Train loss: 0.01372977471819111  Val loss: 0.12262096171526565
Epoch 427
-------------------------------
Batch 1/64 loss: -0.0054395198822021484
Batch 2/64 loss: -0.007024168968200684
Batch 3/64 loss: 0.017639517784118652
Batch 4/64 loss: 0.01914191246032715
Batch 5/64 loss: 0.004224061965942383
Batch 6/64 loss: 0.031943678855895996
Batch 7/64 loss: 0.00498121976852417
Batch 8/64 loss: 0.007527828216552734
Batch 9/64 loss: 0.01712024211883545
Batch 10/64 loss: 0.003589153289794922
Batch 11/64 loss: 0.016080737113952637
Batch 12/64 loss: 0.006851792335510254
Batch 13/64 loss: 0.011568725109100342
Batch 14/64 loss: 0.02129882574081421
Batch 15/64 loss: 0.02023625373840332
Batch 16/64 loss: 0.008656799793243408
Batch 17/64 loss: 0.020212411880493164
Batch 18/64 loss: 0.018522560596466064
Batch 19/64 loss: 0.01314699649810791
Batch 20/64 loss: -0.010713040828704834
Batch 21/64 loss: 0.025676727294921875
Batch 22/64 loss: -0.005681872367858887
Batch 23/64 loss: 0.0053372979164123535
Batch 24/64 loss: -0.022275447845458984
Batch 25/64 loss: -0.009804487228393555
Batch 26/64 loss: 0.02416449785232544
Batch 27/64 loss: 0.03415936231613159
Batch 28/64 loss: 0.006453752517700195
Batch 29/64 loss: 0.027649402618408203
Batch 30/64 loss: 0.007393598556518555
Batch 31/64 loss: 0.009564042091369629
Batch 32/64 loss: 0.009741246700286865
Batch 33/64 loss: 0.02666652202606201
Batch 34/64 loss: 0.03257852792739868
Batch 35/64 loss: -0.008255064487457275
Batch 36/64 loss: -0.00777512788772583
Batch 37/64 loss: 0.027746200561523438
Batch 38/64 loss: 0.0399782657623291
Batch 39/64 loss: 0.01531440019607544
Batch 40/64 loss: 0.010664939880371094
Batch 41/64 loss: 0.01627826690673828
Batch 42/64 loss: 0.052666425704956055
Batch 43/64 loss: 0.013000845909118652
Batch 44/64 loss: -0.01697230339050293
Batch 45/64 loss: 0.002453625202178955
Batch 46/64 loss: 0.008883237838745117
Batch 47/64 loss: -0.0065462589263916016
Batch 48/64 loss: 0.014503777027130127
Batch 49/64 loss: 0.04159337282180786
Batch 50/64 loss: -0.0053424835205078125
Batch 51/64 loss: 0.0172308087348938
Batch 52/64 loss: 0.02456754446029663
Batch 53/64 loss: 0.02007967233657837
Batch 54/64 loss: 0.010005056858062744
Batch 55/64 loss: 0.03398996591567993
Batch 56/64 loss: 0.016229569911956787
Batch 57/64 loss: 0.026387453079223633
Batch 58/64 loss: -0.00018793344497680664
Batch 59/64 loss: 0.04247105121612549
Batch 60/64 loss: -0.0037633180618286133
Batch 61/64 loss: 0.009441733360290527
Batch 62/64 loss: -0.0020978450775146484
Batch 63/64 loss: -0.0003933906555175781
Batch 64/64 loss: -0.020358800888061523
Epoch 427  Train loss: 0.01204819679260254  Val loss: 0.11892326509010341
Epoch 428
-------------------------------
Batch 1/64 loss: 0.007760584354400635
Batch 2/64 loss: -0.0005649328231811523
Batch 3/64 loss: -0.01146608591079712
Batch 4/64 loss: 0.017145156860351562
Batch 5/64 loss: -0.007353663444519043
Batch 6/64 loss: 0.02551358938217163
Batch 7/64 loss: 0.021043062210083008
Batch 8/64 loss: 0.02456068992614746
Batch 9/64 loss: 0.03965413570404053
Batch 10/64 loss: 0.013168573379516602
Batch 11/64 loss: 0.03401452302932739
Batch 12/64 loss: -0.0008781552314758301
Batch 13/64 loss: -0.00525134801864624
Batch 14/64 loss: -0.011882126331329346
Batch 15/64 loss: 0.02593815326690674
Batch 16/64 loss: 0.0011788010597229004
Batch 17/64 loss: -0.007292389869689941
Batch 18/64 loss: -0.0023931264877319336
Batch 19/64 loss: -0.022193312644958496
Batch 20/64 loss: -0.0014586448669433594
Batch 21/64 loss: 0.0023092031478881836
Batch 22/64 loss: 0.0014393329620361328
Batch 23/64 loss: 0.018530607223510742
Batch 24/64 loss: 0.012252867221832275
Batch 25/64 loss: 0.03422236442565918
Batch 26/64 loss: -0.013230621814727783
Batch 27/64 loss: -0.005937457084655762
Batch 28/64 loss: -0.012036323547363281
Batch 29/64 loss: -0.00016629695892333984
Batch 30/64 loss: 0.020905911922454834
Batch 31/64 loss: 0.024446606636047363
Batch 32/64 loss: 0.0267869234085083
Batch 33/64 loss: -0.009759962558746338
Batch 34/64 loss: 0.004252493381500244
Batch 35/64 loss: 0.036710917949676514
Batch 36/64 loss: 0.03609555959701538
Batch 37/64 loss: 0.01347041130065918
Batch 38/64 loss: 0.026634812355041504
Batch 39/64 loss: 0.020547807216644287
Batch 40/64 loss: 0.025133967399597168
Batch 41/64 loss: -0.01717078685760498
Batch 42/64 loss: -0.008382201194763184
Batch 43/64 loss: 0.052532315254211426
Batch 44/64 loss: 0.00855165719985962
Batch 45/64 loss: 0.026734769344329834
Batch 46/64 loss: -0.014255702495574951
Batch 47/64 loss: 0.02372598648071289
Batch 48/64 loss: -0.005095064640045166
Batch 49/64 loss: -0.004235446453094482
Batch 50/64 loss: 0.027944087982177734
Batch 51/64 loss: -0.010936617851257324
Batch 52/64 loss: 0.01912027597427368
Batch 53/64 loss: 0.031539738178253174
Batch 54/64 loss: 0.017752408981323242
Batch 55/64 loss: 0.026374101638793945
Batch 56/64 loss: 0.030371367931365967
Batch 57/64 loss: 0.0053375959396362305
Batch 58/64 loss: 0.018775880336761475
Batch 59/64 loss: -0.008463263511657715
Batch 60/64 loss: 0.014693737030029297
Batch 61/64 loss: 0.051917433738708496
Batch 62/64 loss: 0.04159754514694214
Batch 63/64 loss: 0.027779579162597656
Batch 64/64 loss: 0.057523369789123535
Epoch 428  Train loss: 0.012567914233488195  Val loss: 0.122292638849147
Epoch 429
-------------------------------
Batch 1/64 loss: -0.018015384674072266
Batch 2/64 loss: 0.023424983024597168
Batch 3/64 loss: 0.020106732845306396
Batch 4/64 loss: 0.042776525020599365
Batch 5/64 loss: -0.022745609283447266
Batch 6/64 loss: 0.026951193809509277
Batch 7/64 loss: -0.010529696941375732
Batch 8/64 loss: 0.012933969497680664
Batch 9/64 loss: 0.020241975784301758
Batch 10/64 loss: 0.012492358684539795
Batch 11/64 loss: -0.0049114227294921875
Batch 12/64 loss: 0.03362232446670532
Batch 13/64 loss: -0.004048645496368408
Batch 14/64 loss: -0.013050317764282227
Batch 15/64 loss: 0.022841811180114746
Batch 16/64 loss: 0.026745200157165527
Batch 17/64 loss: 0.012707293033599854
Batch 18/64 loss: 0.025502026081085205
Batch 19/64 loss: 0.013301849365234375
Batch 20/64 loss: 0.004714071750640869
Batch 21/64 loss: 0.01488107442855835
Batch 22/64 loss: 0.029815733432769775
Batch 23/64 loss: 0.058771729469299316
Batch 24/64 loss: 0.012620687484741211
Batch 25/64 loss: 0.00391310453414917
Batch 26/64 loss: 0.023742258548736572
Batch 27/64 loss: 0.008446872234344482
Batch 28/64 loss: 0.005449533462524414
Batch 29/64 loss: -0.00023996829986572266
Batch 30/64 loss: 0.025477051734924316
Batch 31/64 loss: 0.006963253021240234
Batch 32/64 loss: 0.004188060760498047
Batch 33/64 loss: 0.008554339408874512
Batch 34/64 loss: 0.025267362594604492
Batch 35/64 loss: 0.007834196090698242
Batch 36/64 loss: 0.011338531970977783
Batch 37/64 loss: 0.026751041412353516
Batch 38/64 loss: 0.03564530611038208
Batch 39/64 loss: 0.003970682621002197
Batch 40/64 loss: 0.043363332748413086
Batch 41/64 loss: 0.018801391124725342
Batch 42/64 loss: 0.008411884307861328
Batch 43/64 loss: 0.007649421691894531
Batch 44/64 loss: -0.005873441696166992
Batch 45/64 loss: 0.03153204917907715
Batch 46/64 loss: 0.02901822328567505
Batch 47/64 loss: 0.007879197597503662
Batch 48/64 loss: -0.006301283836364746
Batch 49/64 loss: 0.009230852127075195
Batch 50/64 loss: 0.012322306632995605
Batch 51/64 loss: -0.007271111011505127
Batch 52/64 loss: 0.012006580829620361
Batch 53/64 loss: 0.03396272659301758
Batch 54/64 loss: 0.008549213409423828
Batch 55/64 loss: 0.022750377655029297
Batch 56/64 loss: 0.0005896687507629395
Batch 57/64 loss: 0.01867198944091797
Batch 58/64 loss: -0.0014655590057373047
Batch 59/64 loss: 0.01474684476852417
Batch 60/64 loss: -0.0012539029121398926
Batch 61/64 loss: -0.006352245807647705
Batch 62/64 loss: 0.03614383935928345
Batch 63/64 loss: 0.022542178630828857
Batch 64/64 loss: 0.01555490493774414
Epoch 429  Train loss: 0.013486632178811466  Val loss: 0.12059245793680146
Epoch 430
-------------------------------
Batch 1/64 loss: 0.01269829273223877
Batch 2/64 loss: 0.027922630310058594
Batch 3/64 loss: 0.014902114868164062
Batch 4/64 loss: -0.003458082675933838
Batch 5/64 loss: 0.013231098651885986
Batch 6/64 loss: 0.05177384614944458
Batch 7/64 loss: 0.012447953224182129
Batch 8/64 loss: 0.014204144477844238
Batch 9/64 loss: 7.069110870361328e-05
Batch 10/64 loss: 0.03560197353363037
Batch 11/64 loss: 0.023623228073120117
Batch 12/64 loss: 0.006393015384674072
Batch 13/64 loss: 0.013661861419677734
Batch 14/64 loss: 0.007313907146453857
Batch 15/64 loss: 0.006600618362426758
Batch 16/64 loss: 0.005298614501953125
Batch 17/64 loss: 0.03522557020187378
Batch 18/64 loss: 0.017509281635284424
Batch 19/64 loss: 0.008248329162597656
Batch 20/64 loss: -0.01262819766998291
Batch 21/64 loss: -0.0009705424308776855
Batch 22/64 loss: 0.007969558238983154
Batch 23/64 loss: 0.0094374418258667
Batch 24/64 loss: 0.010409712791442871
Batch 25/64 loss: -1.5079975128173828e-05
Batch 26/64 loss: 0.01698213815689087
Batch 27/64 loss: -0.007220566272735596
Batch 28/64 loss: -0.016666412353515625
Batch 29/64 loss: 0.002812504768371582
Batch 30/64 loss: -0.0005058646202087402
Batch 31/64 loss: 0.03622293472290039
Batch 32/64 loss: 0.021506130695343018
Batch 33/64 loss: 0.028395116329193115
Batch 34/64 loss: 0.023629069328308105
Batch 35/64 loss: 0.010641157627105713
Batch 36/64 loss: 0.0066794753074646
Batch 37/64 loss: -0.001829385757446289
Batch 38/64 loss: 0.0006646513938903809
Batch 39/64 loss: 0.018233776092529297
Batch 40/64 loss: -0.0025908946990966797
Batch 41/64 loss: 0.0133894681930542
Batch 42/64 loss: 0.02578568458557129
Batch 43/64 loss: -0.01902604103088379
Batch 44/64 loss: 0.011732220649719238
Batch 45/64 loss: 0.017843186855316162
Batch 46/64 loss: -0.010779261589050293
Batch 47/64 loss: -0.0022281408309936523
Batch 48/64 loss: 0.052170634269714355
Batch 49/64 loss: 0.010414540767669678
Batch 50/64 loss: 0.006290435791015625
Batch 51/64 loss: 0.008295059204101562
Batch 52/64 loss: 0.04218006134033203
Batch 53/64 loss: -0.003953337669372559
Batch 54/64 loss: 0.02118784189224243
Batch 55/64 loss: 0.05070960521697998
Batch 56/64 loss: 0.0051247477531433105
Batch 57/64 loss: 0.016747653484344482
Batch 58/64 loss: 0.03498870134353638
Batch 59/64 loss: 0.04809141159057617
Batch 60/64 loss: -0.0242345929145813
Batch 61/64 loss: 0.0036308765411376953
Batch 62/64 loss: 0.029406368732452393
Batch 63/64 loss: -0.012713909149169922
Batch 64/64 loss: 0.012007832527160645
Epoch 430  Train loss: 0.012368390606898887  Val loss: 0.11990610615084671
Epoch 431
-------------------------------
Batch 1/64 loss: -0.005207359790802002
Batch 2/64 loss: 0.022050678730010986
Batch 3/64 loss: 0.023427486419677734
Batch 4/64 loss: 0.02199077606201172
Batch 5/64 loss: -0.011017680168151855
Batch 6/64 loss: 0.0003502368927001953
Batch 7/64 loss: -0.006894469261169434
Batch 8/64 loss: 0.008331477642059326
Batch 9/64 loss: 0.00991445779800415
Batch 10/64 loss: 0.006524384021759033
Batch 11/64 loss: -0.00412142276763916
Batch 12/64 loss: 0.013614416122436523
Batch 13/64 loss: -0.0036339759826660156
Batch 14/64 loss: -0.01118922233581543
Batch 15/64 loss: -0.005959749221801758
Batch 16/64 loss: 0.011191010475158691
Batch 17/64 loss: 0.01547330617904663
Batch 18/64 loss: 0.005923032760620117
Batch 19/64 loss: 0.018351316452026367
Batch 20/64 loss: -0.005436062812805176
Batch 21/64 loss: 0.005343079566955566
Batch 22/64 loss: 0.025730431079864502
Batch 23/64 loss: 0.013417303562164307
Batch 24/64 loss: 0.03662794828414917
Batch 25/64 loss: 0.010904014110565186
Batch 26/64 loss: -0.0003845691680908203
Batch 27/64 loss: 0.03426861763000488
Batch 28/64 loss: -0.00015163421630859375
Batch 29/64 loss: 0.0040746331214904785
Batch 30/64 loss: 0.01883244514465332
Batch 31/64 loss: 0.030253946781158447
Batch 32/64 loss: 0.014339327812194824
Batch 33/64 loss: 0.04560065269470215
Batch 34/64 loss: -0.0031671524047851562
Batch 35/64 loss: 0.011545121669769287
Batch 36/64 loss: 0.006295323371887207
Batch 37/64 loss: 0.01728224754333496
Batch 38/64 loss: 0.04173612594604492
Batch 39/64 loss: -0.0028571486473083496
Batch 40/64 loss: -0.022713899612426758
Batch 41/64 loss: 0.000507056713104248
Batch 42/64 loss: 0.04151582717895508
Batch 43/64 loss: -0.003258943557739258
Batch 44/64 loss: 0.012871861457824707
Batch 45/64 loss: -0.0009648799896240234
Batch 46/64 loss: 0.04802066087722778
Batch 47/64 loss: -0.015032052993774414
Batch 48/64 loss: 0.030589818954467773
Batch 49/64 loss: -0.010155856609344482
Batch 50/64 loss: 0.044159650802612305
Batch 51/64 loss: 0.014283418655395508
Batch 52/64 loss: 0.005814552307128906
Batch 53/64 loss: 0.0030814409255981445
Batch 54/64 loss: 0.04707080125808716
Batch 55/64 loss: 0.017007768154144287
Batch 56/64 loss: 0.06841009855270386
Batch 57/64 loss: 0.0072596073150634766
Batch 58/64 loss: -0.00269925594329834
Batch 59/64 loss: 0.012563943862915039
Batch 60/64 loss: -0.01120913028717041
Batch 61/64 loss: 0.031916916370391846
Batch 62/64 loss: 0.006299138069152832
Batch 63/64 loss: 0.037850141525268555
Batch 64/64 loss: 0.02588576078414917
Epoch 431  Train loss: 0.012485903618382473  Val loss: 0.11877877994910958
Epoch 432
-------------------------------
Batch 1/64 loss: -0.013016819953918457
Batch 2/64 loss: 0.03127396106719971
Batch 3/64 loss: 0.02411717176437378
Batch 4/64 loss: -0.01357501745223999
Batch 5/64 loss: 0.018548130989074707
Batch 6/64 loss: 0.014400720596313477
Batch 7/64 loss: 0.01791965961456299
Batch 8/64 loss: 0.00925743579864502
Batch 9/64 loss: 0.028567016124725342
Batch 10/64 loss: 0.019834280014038086
Batch 11/64 loss: 0.012759029865264893
Batch 12/64 loss: 0.00024300813674926758
Batch 13/64 loss: -0.000933527946472168
Batch 14/64 loss: 0.02054917812347412
Batch 15/64 loss: 0.050973713397979736
Batch 16/64 loss: 0.012256145477294922
Batch 17/64 loss: 0.008992493152618408
Batch 18/64 loss: -0.010509252548217773
Batch 19/64 loss: 0.027726352214813232
Batch 20/64 loss: 0.021320462226867676
Batch 21/64 loss: 0.009633004665374756
Batch 22/64 loss: 0.01733267307281494
Batch 23/64 loss: 0.02222144603729248
Batch 24/64 loss: -0.022903859615325928
Batch 25/64 loss: -0.006667733192443848
Batch 26/64 loss: -0.009808838367462158
Batch 27/64 loss: 0.01969987154006958
Batch 28/64 loss: 0.0011788606643676758
Batch 29/64 loss: 0.011803865432739258
Batch 30/64 loss: 0.009407520294189453
Batch 31/64 loss: -0.00168687105178833
Batch 32/64 loss: -0.009976863861083984
Batch 33/64 loss: 0.02977997064590454
Batch 34/64 loss: 0.04562199115753174
Batch 35/64 loss: 0.022075414657592773
Batch 36/64 loss: 0.03997260332107544
Batch 37/64 loss: 0.031816959381103516
Batch 38/64 loss: 0.030542194843292236
Batch 39/64 loss: -0.001846909523010254
Batch 40/64 loss: 0.0022178292274475098
Batch 41/64 loss: 0.040026307106018066
Batch 42/64 loss: 0.027414381504058838
Batch 43/64 loss: -0.02231121063232422
Batch 44/64 loss: 0.02232813835144043
Batch 45/64 loss: 0.017696857452392578
Batch 46/64 loss: -0.008471250534057617
Batch 47/64 loss: -0.009529709815979004
Batch 48/64 loss: 0.012844443321228027
Batch 49/64 loss: 0.004699528217315674
Batch 50/64 loss: 0.029087722301483154
Batch 51/64 loss: 0.030923843383789062
Batch 52/64 loss: 0.02159029245376587
Batch 53/64 loss: -0.02487999200820923
Batch 54/64 loss: -0.01590895652770996
Batch 55/64 loss: 0.0052787065505981445
Batch 56/64 loss: -0.010228216648101807
Batch 57/64 loss: 0.011778116226196289
Batch 58/64 loss: 0.015715599060058594
Batch 59/64 loss: 0.015346646308898926
Batch 60/64 loss: -0.0015205740928649902
Batch 61/64 loss: 0.028190791606903076
Batch 62/64 loss: 0.02362304925918579
Batch 63/64 loss: 0.015714526176452637
Batch 64/64 loss: 0.02295464277267456
Epoch 432  Train loss: 0.01204301633086859  Val loss: 0.12067192956754022
Epoch 433
-------------------------------
Batch 1/64 loss: -0.01359260082244873
Batch 2/64 loss: 0.00015300512313842773
Batch 3/64 loss: -0.006282389163970947
Batch 4/64 loss: -0.009377658367156982
Batch 5/64 loss: 0.008992314338684082
Batch 6/64 loss: -0.016425907611846924
Batch 7/64 loss: 0.03109973669052124
Batch 8/64 loss: -0.025197148323059082
Batch 9/64 loss: 0.026264309883117676
Batch 10/64 loss: 0.0032483339309692383
Batch 11/64 loss: -0.0037854909896850586
Batch 12/64 loss: 0.001767873764038086
Batch 13/64 loss: 0.03150075674057007
Batch 14/64 loss: 0.04366260766983032
Batch 15/64 loss: 0.010310232639312744
Batch 16/64 loss: -0.000460207462310791
Batch 17/64 loss: 0.012110471725463867
Batch 18/64 loss: 0.0255964994430542
Batch 19/64 loss: -0.004438340663909912
Batch 20/64 loss: -0.00969994068145752
Batch 21/64 loss: 0.005322515964508057
Batch 22/64 loss: 0.01254194974899292
Batch 23/64 loss: -4.953145980834961e-05
Batch 24/64 loss: 0.01438295841217041
Batch 25/64 loss: 0.005892157554626465
Batch 26/64 loss: 0.009782850742340088
Batch 27/64 loss: 0.035990238189697266
Batch 28/64 loss: 0.04591035842895508
Batch 29/64 loss: 0.03600478172302246
Batch 30/64 loss: 0.017913997173309326
Batch 31/64 loss: -0.015201449394226074
Batch 32/64 loss: 0.0386427640914917
Batch 33/64 loss: 0.0027753710746765137
Batch 34/64 loss: 0.02706444263458252
Batch 35/64 loss: 0.017487168312072754
Batch 36/64 loss: 0.016122639179229736
Batch 37/64 loss: 0.02432727813720703
Batch 38/64 loss: 0.019181787967681885
Batch 39/64 loss: 0.022683262825012207
Batch 40/64 loss: 0.029376566410064697
Batch 41/64 loss: -0.004057526588439941
Batch 42/64 loss: -0.017346560955047607
Batch 43/64 loss: 0.028483033180236816
Batch 44/64 loss: -0.0026181936264038086
Batch 45/64 loss: 0.00702744722366333
Batch 46/64 loss: 0.0009765625
Batch 47/64 loss: 0.02086198329925537
Batch 48/64 loss: -0.0030859708786010742
Batch 49/64 loss: -0.0017049908638000488
Batch 50/64 loss: -0.013742506504058838
Batch 51/64 loss: -0.0024660825729370117
Batch 52/64 loss: -0.008100450038909912
Batch 53/64 loss: 0.012983620166778564
Batch 54/64 loss: 0.003940105438232422
Batch 55/64 loss: 0.047196030616760254
Batch 56/64 loss: -0.010525763034820557
Batch 57/64 loss: 0.015910744667053223
Batch 58/64 loss: 0.054977357387542725
Batch 59/64 loss: 0.004588127136230469
Batch 60/64 loss: 0.041956305503845215
Batch 61/64 loss: 0.028692960739135742
Batch 62/64 loss: 0.0062590837478637695
Batch 63/64 loss: 0.010933279991149902
Batch 64/64 loss: 0.03618878126144409
Epoch 433  Train loss: 0.01129221565583173  Val loss: 0.1226901928174127
Epoch 434
-------------------------------
Batch 1/64 loss: -0.006273090839385986
Batch 2/64 loss: 0.02760624885559082
Batch 3/64 loss: 0.02964681386947632
Batch 4/64 loss: -0.00921332836151123
Batch 5/64 loss: -0.014701128005981445
Batch 6/64 loss: 0.010387241840362549
Batch 7/64 loss: 0.04054522514343262
Batch 8/64 loss: 0.022022664546966553
Batch 9/64 loss: 0.021442174911499023
Batch 10/64 loss: 0.019740700721740723
Batch 11/64 loss: 0.014017462730407715
Batch 12/64 loss: -0.010320663452148438
Batch 13/64 loss: 0.025131165981292725
Batch 14/64 loss: -0.017194151878356934
Batch 15/64 loss: 0.0007156729698181152
Batch 16/64 loss: 0.005282998085021973
Batch 17/64 loss: 0.019468307495117188
Batch 18/64 loss: 0.018588900566101074
Batch 19/64 loss: -0.0016012787818908691
Batch 20/64 loss: -0.005525052547454834
Batch 21/64 loss: -0.0023831725120544434
Batch 22/64 loss: -0.006593525409698486
Batch 23/64 loss: 0.011555671691894531
Batch 24/64 loss: -0.008047938346862793
Batch 25/64 loss: -0.006464958190917969
Batch 26/64 loss: 0.0022282004356384277
Batch 27/64 loss: 0.002384185791015625
Batch 28/64 loss: 0.02098315954208374
Batch 29/64 loss: 0.012270212173461914
Batch 30/64 loss: 0.027032017707824707
Batch 31/64 loss: 0.016410410404205322
Batch 32/64 loss: 0.02541130781173706
Batch 33/64 loss: 0.04534834623336792
Batch 34/64 loss: -0.013430476188659668
Batch 35/64 loss: 0.0017565488815307617
Batch 36/64 loss: 0.04987519979476929
Batch 37/64 loss: 0.02946329116821289
Batch 38/64 loss: -0.00541996955871582
Batch 39/64 loss: -0.004596114158630371
Batch 40/64 loss: -0.0028612613677978516
Batch 41/64 loss: 0.017064273357391357
Batch 42/64 loss: 0.010357141494750977
Batch 43/64 loss: 0.015928328037261963
Batch 44/64 loss: 0.010731816291809082
Batch 45/64 loss: -0.01597154140472412
Batch 46/64 loss: 0.01280122995376587
Batch 47/64 loss: 0.012208938598632812
Batch 48/64 loss: 0.04320657253265381
Batch 49/64 loss: 0.014899969100952148
Batch 50/64 loss: 0.0045833587646484375
Batch 51/64 loss: 0.008195579051971436
Batch 52/64 loss: 0.009380817413330078
Batch 53/64 loss: -0.005861639976501465
Batch 54/64 loss: 0.03640151023864746
Batch 55/64 loss: 0.032456159591674805
Batch 56/64 loss: 0.014611601829528809
Batch 57/64 loss: 0.027312517166137695
Batch 58/64 loss: 0.0475008487701416
Batch 59/64 loss: 0.023862481117248535
Batch 60/64 loss: 0.006691455841064453
Batch 61/64 loss: 0.00728219747543335
Batch 62/64 loss: 0.019611001014709473
Batch 63/64 loss: 0.009503006935119629
Batch 64/64 loss: -0.014059722423553467
Epoch 434  Train loss: 0.011559229037340949  Val loss: 0.12050920929695733
Epoch 435
-------------------------------
Batch 1/64 loss: 0.01533442735671997
Batch 2/64 loss: -0.007363438606262207
Batch 3/64 loss: 0.02353435754776001
Batch 4/64 loss: 0.027505159378051758
Batch 5/64 loss: 0.011544108390808105
Batch 6/64 loss: 0.03369063138961792
Batch 7/64 loss: -0.02258777618408203
Batch 8/64 loss: 0.02350538969039917
Batch 9/64 loss: -0.01376497745513916
Batch 10/64 loss: 0.02207505702972412
Batch 11/64 loss: 0.0001996755599975586
Batch 12/64 loss: 0.007838070392608643
Batch 13/64 loss: 0.028322994709014893
Batch 14/64 loss: 0.03297513723373413
Batch 15/64 loss: -0.012453913688659668
Batch 16/64 loss: 0.029758810997009277
Batch 17/64 loss: 0.002831578254699707
Batch 18/64 loss: 0.006622672080993652
Batch 19/64 loss: 0.01420438289642334
Batch 20/64 loss: 0.01257944107055664
Batch 21/64 loss: 0.010449528694152832
Batch 22/64 loss: 0.04848325252532959
Batch 23/64 loss: 0.021244287490844727
Batch 24/64 loss: 0.018337726593017578
Batch 25/64 loss: 0.033082544803619385
Batch 26/64 loss: 0.0048795342445373535
Batch 27/64 loss: 0.0012029409408569336
Batch 28/64 loss: 0.022794127464294434
Batch 29/64 loss: 0.01680171489715576
Batch 30/64 loss: 0.006565093994140625
Batch 31/64 loss: 0.014712929725646973
Batch 32/64 loss: 0.017849206924438477
Batch 33/64 loss: 0.011008262634277344
Batch 34/64 loss: -0.012282013893127441
Batch 35/64 loss: 0.004807114601135254
Batch 36/64 loss: -0.019527852535247803
Batch 37/64 loss: 0.019435346126556396
Batch 38/64 loss: -0.006240189075469971
Batch 39/64 loss: -0.013054430484771729
Batch 40/64 loss: 0.012258827686309814
Batch 41/64 loss: -0.0034933090209960938
Batch 42/64 loss: -0.004270374774932861
Batch 43/64 loss: 0.01902294158935547
Batch 44/64 loss: -0.01040416955947876
Batch 45/64 loss: -0.004229485988616943
Batch 46/64 loss: 0.02805989980697632
Batch 47/64 loss: 0.056006550788879395
Batch 48/64 loss: 0.03569352626800537
Batch 49/64 loss: 0.0006334781646728516
Batch 50/64 loss: 0.04320073127746582
Batch 51/64 loss: 9.483098983764648e-05
Batch 52/64 loss: 0.007566869258880615
Batch 53/64 loss: 0.020364344120025635
Batch 54/64 loss: 0.020235419273376465
Batch 55/64 loss: 0.0075647830963134766
Batch 56/64 loss: 0.015086829662322998
Batch 57/64 loss: 0.03773033618927002
Batch 58/64 loss: -0.001659393310546875
Batch 59/64 loss: -0.022460103034973145
Batch 60/64 loss: 0.021444320678710938
Batch 61/64 loss: 0.03903305530548096
Batch 62/64 loss: -0.024410367012023926
Batch 63/64 loss: -0.0032662153244018555
Batch 64/64 loss: -0.004473745822906494
Epoch 435  Train loss: 0.011346649889852487  Val loss: 0.12111803797102466
Epoch 436
-------------------------------
Batch 1/64 loss: -0.02042686939239502
Batch 2/64 loss: -0.02093130350112915
Batch 3/64 loss: -0.015478730201721191
Batch 4/64 loss: 0.025865495204925537
Batch 5/64 loss: -0.014736533164978027
Batch 6/64 loss: 0.00419992208480835
Batch 7/64 loss: -0.0077378153800964355
Batch 8/64 loss: -0.029236972332000732
Batch 9/64 loss: 0.02221280336380005
Batch 10/64 loss: 0.016562044620513916
Batch 11/64 loss: 0.03764605522155762
Batch 12/64 loss: 0.036117613315582275
Batch 13/64 loss: 0.00599980354309082
Batch 14/64 loss: 0.009727776050567627
Batch 15/64 loss: -0.00017082691192626953
Batch 16/64 loss: 0.008564889430999756
Batch 17/64 loss: 0.021778404712677002
Batch 18/64 loss: 0.016702473163604736
Batch 19/64 loss: 0.004153072834014893
Batch 20/64 loss: 0.014104962348937988
Batch 21/64 loss: 0.001385509967803955
Batch 22/64 loss: 0.01914769411087036
Batch 23/64 loss: 0.004789233207702637
Batch 24/64 loss: 0.011781454086303711
Batch 25/64 loss: 0.01042085886001587
Batch 26/64 loss: 0.03625839948654175
Batch 27/64 loss: 0.0016416311264038086
Batch 28/64 loss: -0.032868146896362305
Batch 29/64 loss: 0.028278708457946777
Batch 30/64 loss: 0.0005255341529846191
Batch 31/64 loss: 0.008704543113708496
Batch 32/64 loss: -0.006669044494628906
Batch 33/64 loss: 0.017107605934143066
Batch 34/64 loss: 0.024561405181884766
Batch 35/64 loss: 0.022120654582977295
Batch 36/64 loss: 0.00012439489364624023
Batch 37/64 loss: 0.027659475803375244
Batch 38/64 loss: -0.012499451637268066
Batch 39/64 loss: 0.0017952919006347656
Batch 40/64 loss: 0.01785367727279663
Batch 41/64 loss: 0.05920594930648804
Batch 42/64 loss: -0.0020980238914489746
Batch 43/64 loss: 0.021973848342895508
Batch 44/64 loss: -0.020903289318084717
Batch 45/64 loss: 0.04352527856826782
Batch 46/64 loss: 0.036679863929748535
Batch 47/64 loss: -0.015218019485473633
Batch 48/64 loss: -0.01283031702041626
Batch 49/64 loss: 0.0048272013664245605
Batch 50/64 loss: 0.01827859878540039
Batch 51/64 loss: 0.029611587524414062
Batch 52/64 loss: 0.028641223907470703
Batch 53/64 loss: 0.031001269817352295
Batch 54/64 loss: -0.00013208389282226562
Batch 55/64 loss: 0.016183018684387207
Batch 56/64 loss: 0.013820111751556396
Batch 57/64 loss: -0.013934016227722168
Batch 58/64 loss: -0.00455319881439209
Batch 59/64 loss: 0.07668215036392212
Batch 60/64 loss: 0.03225737810134888
Batch 61/64 loss: 0.028383493423461914
Batch 62/64 loss: 0.01608264446258545
Batch 63/64 loss: 0.024974822998046875
Batch 64/64 loss: -0.005169808864593506
Epoch 436  Train loss: 0.011068514982859294  Val loss: 0.11992911518234567
Epoch 437
-------------------------------
Batch 1/64 loss: 0.023546338081359863
Batch 2/64 loss: 0.022817134857177734
Batch 3/64 loss: -0.02234363555908203
Batch 4/64 loss: -0.01191788911819458
Batch 5/64 loss: 0.006187379360198975
Batch 6/64 loss: -0.0016179680824279785
Batch 7/64 loss: 0.037878453731536865
Batch 8/64 loss: -0.0107346773147583
Batch 9/64 loss: 0.015612483024597168
Batch 10/64 loss: 0.033667683601379395
Batch 11/64 loss: 0.04200011491775513
Batch 12/64 loss: 0.0052078962326049805
Batch 13/64 loss: 0.009824872016906738
Batch 14/64 loss: 0.01621842384338379
Batch 15/64 loss: 0.028731167316436768
Batch 16/64 loss: 0.013513505458831787
Batch 17/64 loss: 0.008525490760803223
Batch 18/64 loss: -0.000594794750213623
Batch 19/64 loss: 0.03929871320724487
Batch 20/64 loss: -0.0019584298133850098
Batch 21/64 loss: 0.008353233337402344
Batch 22/64 loss: -0.0006363391876220703
Batch 23/64 loss: 0.04896104335784912
Batch 24/64 loss: 0.006589829921722412
Batch 25/64 loss: 0.0285261869430542
Batch 26/64 loss: 0.02259039878845215
Batch 27/64 loss: 0.017182528972625732
Batch 28/64 loss: 0.028990209102630615
Batch 29/64 loss: 0.022437453269958496
Batch 30/64 loss: -0.026818811893463135
Batch 31/64 loss: 0.03271263837814331
Batch 32/64 loss: 0.0018743276596069336
Batch 33/64 loss: -0.011482596397399902
Batch 34/64 loss: -0.0223085880279541
Batch 35/64 loss: 0.02920377254486084
Batch 36/64 loss: 0.009102463722229004
Batch 37/64 loss: -0.004725456237792969
Batch 38/64 loss: 0.021187424659729004
Batch 39/64 loss: 0.006550967693328857
Batch 40/64 loss: 0.021565735340118408
Batch 41/64 loss: 0.025636792182922363
Batch 42/64 loss: 0.017830610275268555
Batch 43/64 loss: -0.002899348735809326
Batch 44/64 loss: -0.006942093372344971
Batch 45/64 loss: -0.00674891471862793
Batch 46/64 loss: 0.023881733417510986
Batch 47/64 loss: -0.007855892181396484
Batch 48/64 loss: 0.04134190082550049
Batch 49/64 loss: 0.020433127880096436
Batch 50/64 loss: 0.036779820919036865
Batch 51/64 loss: 0.005030155181884766
Batch 52/64 loss: 0.005679607391357422
Batch 53/64 loss: 0.022681772708892822
Batch 54/64 loss: -0.023159921169281006
Batch 55/64 loss: -0.025268971920013428
Batch 56/64 loss: 0.02737271785736084
Batch 57/64 loss: -0.017254650592803955
Batch 58/64 loss: 0.013834714889526367
Batch 59/64 loss: 0.024894654750823975
Batch 60/64 loss: 0.022140324115753174
Batch 61/64 loss: 0.0032262206077575684
Batch 62/64 loss: 0.008133471012115479
Batch 63/64 loss: 0.0160520076751709
Batch 64/64 loss: 0.011474370956420898
Epoch 437  Train loss: 0.011406185112747491  Val loss: 0.11972331242872677
Epoch 438
-------------------------------
Batch 1/64 loss: -0.0238073468208313
Batch 2/64 loss: -0.006780862808227539
Batch 3/64 loss: -0.013173460960388184
Batch 4/64 loss: 0.013021707534790039
Batch 5/64 loss: 0.00694960355758667
Batch 6/64 loss: 0.02442467212677002
Batch 7/64 loss: 0.07001131772994995
Batch 8/64 loss: 0.05068182945251465
Batch 9/64 loss: 0.0012996196746826172
Batch 10/64 loss: -0.0038844943046569824
Batch 11/64 loss: 0.014999032020568848
Batch 12/64 loss: 0.039206087589263916
Batch 13/64 loss: 0.011566519737243652
Batch 14/64 loss: 0.0018474459648132324
Batch 15/64 loss: 0.0051683783531188965
Batch 16/64 loss: -0.017449617385864258
Batch 17/64 loss: -0.01794123649597168
Batch 18/64 loss: -0.01019740104675293
Batch 19/64 loss: 0.016442477703094482
Batch 20/64 loss: -0.014094352722167969
Batch 21/64 loss: -0.0025760531425476074
Batch 22/64 loss: -0.005195319652557373
Batch 23/64 loss: -0.006132662296295166
Batch 24/64 loss: 0.004811406135559082
Batch 25/64 loss: 0.02818286418914795
Batch 26/64 loss: 0.00945746898651123
Batch 27/64 loss: 0.02799999713897705
Batch 28/64 loss: 0.013367295265197754
Batch 29/64 loss: 0.05328631401062012
Batch 30/64 loss: 0.03775358200073242
Batch 31/64 loss: 0.021433591842651367
Batch 32/64 loss: 0.014697432518005371
Batch 33/64 loss: 0.02847433090209961
Batch 34/64 loss: -0.019575953483581543
Batch 35/64 loss: 0.002663254737854004
Batch 36/64 loss: -0.0023223161697387695
Batch 37/64 loss: 0.0007910728454589844
Batch 38/64 loss: 0.0012703537940979004
Batch 39/64 loss: 0.012041866779327393
Batch 40/64 loss: -0.01061391830444336
Batch 41/64 loss: 0.025125980377197266
Batch 42/64 loss: -0.0015518665313720703
Batch 43/64 loss: 0.011422574520111084
Batch 44/64 loss: 0.03582799434661865
Batch 45/64 loss: 0.03452003002166748
Batch 46/64 loss: 0.016275107860565186
Batch 47/64 loss: 0.014502882957458496
Batch 48/64 loss: 2.682209014892578e-06
Batch 49/64 loss: -0.0012431740760803223
Batch 50/64 loss: 0.00369948148727417
Batch 51/64 loss: 0.041022539138793945
Batch 52/64 loss: 0.03707700967788696
Batch 53/64 loss: 0.008787035942077637
Batch 54/64 loss: 0.002961397171020508
Batch 55/64 loss: 0.01663053035736084
Batch 56/64 loss: -0.006495356559753418
Batch 57/64 loss: 0.006192684173583984
Batch 58/64 loss: -0.004902303218841553
Batch 59/64 loss: 0.008225679397583008
Batch 60/64 loss: 0.0020824670791625977
Batch 61/64 loss: 0.001268148422241211
Batch 62/64 loss: -0.01727348566055298
Batch 63/64 loss: 0.04450505971908569
Batch 64/64 loss: -0.02081054449081421
Epoch 438  Train loss: 0.009743681842205571  Val loss: 0.11876941177853194
Epoch 439
-------------------------------
Batch 1/64 loss: 0.004733920097351074
Batch 2/64 loss: -0.018128275871276855
Batch 3/64 loss: -0.0002606511116027832
Batch 4/64 loss: -0.0029956698417663574
Batch 5/64 loss: 0.02365291118621826
Batch 6/64 loss: 0.04125475883483887
Batch 7/64 loss: 0.013138115406036377
Batch 8/64 loss: 0.002852499485015869
Batch 9/64 loss: 0.00026035308837890625
Batch 10/64 loss: -0.0056798458099365234
Batch 11/64 loss: 0.02473825216293335
Batch 12/64 loss: 0.016404449939727783
Batch 13/64 loss: 0.010672450065612793
Batch 14/64 loss: -0.0065991878509521484
Batch 15/64 loss: 0.01652461290359497
Batch 16/64 loss: 0.007550716400146484
Batch 17/64 loss: 0.04343229532241821
Batch 18/64 loss: 0.03402841091156006
Batch 19/64 loss: 0.013300061225891113
Batch 20/64 loss: 0.034840524196624756
Batch 21/64 loss: 0.0062067508697509766
Batch 22/64 loss: -0.008485257625579834
Batch 23/64 loss: 0.03247034549713135
Batch 24/64 loss: 0.02715015411376953
Batch 25/64 loss: 0.004017472267150879
Batch 26/64 loss: -0.006196439266204834
Batch 27/64 loss: 0.06358540058135986
Batch 28/64 loss: -0.006279885768890381
Batch 29/64 loss: 0.011018991470336914
Batch 30/64 loss: 0.02153235673904419
Batch 31/64 loss: -3.451108932495117e-05
Batch 32/64 loss: 0.03095722198486328
Batch 33/64 loss: -0.012498736381530762
Batch 34/64 loss: 0.03332698345184326
Batch 35/64 loss: -0.002226710319519043
Batch 36/64 loss: 0.028297603130340576
Batch 37/64 loss: 0.01696324348449707
Batch 38/64 loss: 0.008068203926086426
Batch 39/64 loss: 0.01205354928970337
Batch 40/64 loss: 0.002926647663116455
Batch 41/64 loss: 0.020565152168273926
Batch 42/64 loss: 0.021707653999328613
Batch 43/64 loss: 0.025720536708831787
Batch 44/64 loss: -0.011105716228485107
Batch 45/64 loss: 0.014226555824279785
Batch 46/64 loss: -0.00557398796081543
Batch 47/64 loss: 0.015961170196533203
Batch 48/64 loss: 0.01818305253982544
Batch 49/64 loss: 0.0016633868217468262
Batch 50/64 loss: 0.0246884822845459
Batch 51/64 loss: 0.020549654960632324
Batch 52/64 loss: 0.0046844482421875
Batch 53/64 loss: -0.006309688091278076
Batch 54/64 loss: -0.001976609230041504
Batch 55/64 loss: 0.0015015006065368652
Batch 56/64 loss: -0.008982419967651367
Batch 57/64 loss: 0.0015917420387268066
Batch 58/64 loss: 0.02813166379928589
Batch 59/64 loss: 0.0004818439483642578
Batch 60/64 loss: 0.018426597118377686
Batch 61/64 loss: 0.0211828351020813
Batch 62/64 loss: 0.0005290508270263672
Batch 63/64 loss: -0.014317750930786133
Batch 64/64 loss: 0.0051825642585754395
Epoch 439  Train loss: 0.011168473140866149  Val loss: 0.12138935544646483
Epoch 440
-------------------------------
Batch 1/64 loss: -0.01751643419265747
Batch 2/64 loss: 0.00812232494354248
Batch 3/64 loss: 0.02621680498123169
Batch 4/64 loss: 0.021596550941467285
Batch 5/64 loss: -0.0015594959259033203
Batch 6/64 loss: 0.020152568817138672
Batch 7/64 loss: 0.017258644104003906
Batch 8/64 loss: -0.007369637489318848
Batch 9/64 loss: 0.01997154951095581
Batch 10/64 loss: -0.0011081099510192871
Batch 11/64 loss: -0.02001011371612549
Batch 12/64 loss: -0.002207934856414795
Batch 13/64 loss: 0.03008812665939331
Batch 14/64 loss: 0.03379112482070923
Batch 15/64 loss: -0.005516469478607178
Batch 16/64 loss: 0.00981670618057251
Batch 17/64 loss: 0.0010392069816589355
Batch 18/64 loss: 0.021167099475860596
Batch 19/64 loss: 0.015850186347961426
Batch 20/64 loss: -0.010852932929992676
Batch 21/64 loss: -0.013317644596099854
Batch 22/64 loss: -0.005638718605041504
Batch 23/64 loss: 0.023246049880981445
Batch 24/64 loss: -0.005022764205932617
Batch 25/64 loss: 0.025168240070343018
Batch 26/64 loss: 0.04621845483779907
Batch 27/64 loss: -0.008605241775512695
Batch 28/64 loss: -0.008523285388946533
Batch 29/64 loss: -0.011649668216705322
Batch 30/64 loss: -0.0015004277229309082
Batch 31/64 loss: -0.005423784255981445
Batch 32/64 loss: 0.05813497304916382
Batch 33/64 loss: 0.025475680828094482
Batch 34/64 loss: 0.003086686134338379
Batch 35/64 loss: 0.006819605827331543
Batch 36/64 loss: 0.001749277114868164
Batch 37/64 loss: -0.015148520469665527
Batch 38/64 loss: -0.0007922649383544922
Batch 39/64 loss: 0.029629826545715332
Batch 40/64 loss: 0.010706484317779541
Batch 41/64 loss: -0.007723033428192139
Batch 42/64 loss: -0.00202864408493042
Batch 43/64 loss: 0.014504849910736084
Batch 44/64 loss: 0.0050743818283081055
Batch 45/64 loss: 0.03297305107116699
Batch 46/64 loss: 0.008562803268432617
Batch 47/64 loss: 0.00581049919128418
Batch 48/64 loss: 0.00635528564453125
Batch 49/64 loss: -0.014396488666534424
Batch 50/64 loss: 0.019528627395629883
Batch 51/64 loss: 0.00554502010345459
Batch 52/64 loss: 0.04255956411361694
Batch 53/64 loss: 0.032402217388153076
Batch 54/64 loss: 0.04317271709442139
Batch 55/64 loss: 0.04931062459945679
Batch 56/64 loss: 0.029569804668426514
Batch 57/64 loss: -0.0010129213333129883
Batch 58/64 loss: 0.010618329048156738
Batch 59/64 loss: -0.008510828018188477
Batch 60/64 loss: 0.030648112297058105
Batch 61/64 loss: 0.01796823740005493
Batch 62/64 loss: -0.007125973701477051
Batch 63/64 loss: 0.010076820850372314
Batch 64/64 loss: 0.02791774272918701
Epoch 440  Train loss: 0.01032727980146221  Val loss: 0.1217252005826157
Epoch 441
-------------------------------
Batch 1/64 loss: -0.012786507606506348
Batch 2/64 loss: 0.011936545372009277
Batch 3/64 loss: -0.02323436737060547
Batch 4/64 loss: 0.0032787322998046875
Batch 5/64 loss: -0.007886230945587158
Batch 6/64 loss: -0.0017650723457336426
Batch 7/64 loss: 0.011380672454833984
Batch 8/64 loss: -0.012041568756103516
Batch 9/64 loss: -0.004052221775054932
Batch 10/64 loss: 0.007091879844665527
Batch 11/64 loss: 0.010581016540527344
Batch 12/64 loss: 0.034682273864746094
Batch 13/64 loss: -0.011446893215179443
Batch 14/64 loss: -3.2842159271240234e-05
Batch 15/64 loss: 0.024158477783203125
Batch 16/64 loss: 0.012666881084442139
Batch 17/64 loss: -0.008751630783081055
Batch 18/64 loss: 0.031935274600982666
Batch 19/64 loss: 0.03830420970916748
Batch 20/64 loss: 0.00943613052368164
Batch 21/64 loss: 0.029799818992614746
Batch 22/64 loss: 0.027348756790161133
Batch 23/64 loss: 0.0004794001579284668
Batch 24/64 loss: 0.012011289596557617
Batch 25/64 loss: -0.019325733184814453
Batch 26/64 loss: 0.015213608741760254
Batch 27/64 loss: 0.013872325420379639
Batch 28/64 loss: -0.013193368911743164
Batch 29/64 loss: 0.003578662872314453
Batch 30/64 loss: 0.009123563766479492
Batch 31/64 loss: 0.007074475288391113
Batch 32/64 loss: -0.0009099245071411133
Batch 33/64 loss: 0.04027831554412842
Batch 34/64 loss: 0.022279024124145508
Batch 35/64 loss: 0.019776761531829834
Batch 36/64 loss: 0.018047749996185303
Batch 37/64 loss: 0.021528959274291992
Batch 38/64 loss: 0.0076947808265686035
Batch 39/64 loss: 0.005697846412658691
Batch 40/64 loss: 0.038182079792022705
Batch 41/64 loss: 0.02682250738143921
Batch 42/64 loss: -0.0059871673583984375
Batch 43/64 loss: 0.04378455877304077
Batch 44/64 loss: 0.005199074745178223
Batch 45/64 loss: 0.0005425810813903809
Batch 46/64 loss: 0.01857471466064453
Batch 47/64 loss: -0.01924222707748413
Batch 48/64 loss: 0.02761995792388916
Batch 49/64 loss: 0.017295897006988525
Batch 50/64 loss: 0.03904300928115845
Batch 51/64 loss: -0.0005761384963989258
Batch 52/64 loss: 0.005499005317687988
Batch 53/64 loss: 0.019198060035705566
Batch 54/64 loss: 0.018638134002685547
Batch 55/64 loss: -0.002436041831970215
Batch 56/64 loss: -0.007234454154968262
Batch 57/64 loss: -0.015244722366333008
Batch 58/64 loss: -0.017207205295562744
Batch 59/64 loss: -0.006024360656738281
Batch 60/64 loss: -0.002303898334503174
Batch 61/64 loss: 0.04021120071411133
Batch 62/64 loss: -0.004273414611816406
Batch 63/64 loss: 0.01198279857635498
Batch 64/64 loss: 0.055534422397613525
Epoch 441  Train loss: 0.009530130788391713  Val loss: 0.12004910875431861
Epoch 442
-------------------------------
Batch 1/64 loss: 0.01720738410949707
Batch 2/64 loss: 0.013114750385284424
Batch 3/64 loss: 0.0064008235931396484
Batch 4/64 loss: -0.007152259349822998
Batch 5/64 loss: 0.03481709957122803
Batch 6/64 loss: 0.04739445447921753
Batch 7/64 loss: 0.019981086254119873
Batch 8/64 loss: 0.02898585796356201
Batch 9/64 loss: -0.001393735408782959
Batch 10/64 loss: -0.000330507755279541
Batch 11/64 loss: 0.014473259449005127
Batch 12/64 loss: 0.010656952857971191
Batch 13/64 loss: -0.006425082683563232
Batch 14/64 loss: 0.002902388572692871
Batch 15/64 loss: 0.025456666946411133
Batch 16/64 loss: 0.021162986755371094
Batch 17/64 loss: 0.0001849532127380371
Batch 18/64 loss: -0.01924920082092285
Batch 19/64 loss: 0.013721764087677002
Batch 20/64 loss: 0.012520670890808105
Batch 21/64 loss: -0.006305694580078125
Batch 22/64 loss: -0.0018239617347717285
Batch 23/64 loss: 0.006696820259094238
Batch 24/64 loss: 0.005158364772796631
Batch 25/64 loss: 0.002105414867401123
Batch 26/64 loss: 0.01767563819885254
Batch 27/64 loss: 0.004877567291259766
Batch 28/64 loss: 0.06182670593261719
Batch 29/64 loss: -0.005755782127380371
Batch 30/64 loss: 0.010368585586547852
Batch 31/64 loss: -0.006497025489807129
Batch 32/64 loss: -0.039666175842285156
Batch 33/64 loss: 0.05066835880279541
Batch 34/64 loss: 0.02238255739212036
Batch 35/64 loss: 0.00021201372146606445
Batch 36/64 loss: 0.0017119050025939941
Batch 37/64 loss: 0.007961630821228027
Batch 38/64 loss: 0.004968404769897461
Batch 39/64 loss: 0.013176143169403076
Batch 40/64 loss: 0.024497032165527344
Batch 41/64 loss: -0.003045320510864258
Batch 42/64 loss: 0.0071991682052612305
Batch 43/64 loss: -0.002539396286010742
Batch 44/64 loss: 0.007953643798828125
Batch 45/64 loss: -0.010240375995635986
Batch 46/64 loss: 0.013170480728149414
Batch 47/64 loss: 0.013328135013580322
Batch 48/64 loss: -0.001034557819366455
Batch 49/64 loss: 0.0010192394256591797
Batch 50/64 loss: -0.012557625770568848
Batch 51/64 loss: -0.017153918743133545
Batch 52/64 loss: 0.0008977055549621582
Batch 53/64 loss: -0.005240082740783691
Batch 54/64 loss: -0.018828213214874268
Batch 55/64 loss: 0.022194206714630127
Batch 56/64 loss: 0.031605064868927
Batch 57/64 loss: 0.017477571964263916
Batch 58/64 loss: 0.002471923828125
Batch 59/64 loss: 0.01869487762451172
Batch 60/64 loss: 0.0009777545928955078
Batch 61/64 loss: 0.0006565451622009277
Batch 62/64 loss: 0.01409292221069336
Batch 63/64 loss: 0.02289038896560669
Batch 64/64 loss: 0.03035825490951538
Epoch 442  Train loss: 0.008398865017236449  Val loss: 0.12261310721590757
Epoch 443
-------------------------------
Batch 1/64 loss: -0.007628262042999268
Batch 2/64 loss: -0.0016137361526489258
Batch 3/64 loss: 0.007112085819244385
Batch 4/64 loss: -0.01630634069442749
Batch 5/64 loss: -0.0084991455078125
Batch 6/64 loss: -0.01561582088470459
Batch 7/64 loss: 0.029783189296722412
Batch 8/64 loss: 0.027292966842651367
Batch 9/64 loss: -0.013601601123809814
Batch 10/64 loss: -0.01253211498260498
Batch 11/64 loss: -0.0016785264015197754
Batch 12/64 loss: -0.00092315673828125
Batch 13/64 loss: 0.005259096622467041
Batch 14/64 loss: 0.02452397346496582
Batch 15/64 loss: -0.004265904426574707
Batch 16/64 loss: 0.01431584358215332
Batch 17/64 loss: 0.02023947238922119
Batch 18/64 loss: 0.022370994091033936
Batch 19/64 loss: -0.01965630054473877
Batch 20/64 loss: 0.004953920841217041
Batch 21/64 loss: -0.011314749717712402
Batch 22/64 loss: 0.0490267276763916
Batch 23/64 loss: 0.01592397689819336
Batch 24/64 loss: 0.005442261695861816
Batch 25/64 loss: -0.007113993167877197
Batch 26/64 loss: 0.014135003089904785
Batch 27/64 loss: 0.040910959243774414
Batch 28/64 loss: -0.002714812755584717
Batch 29/64 loss: 0.006544589996337891
Batch 30/64 loss: 0.0012148022651672363
Batch 31/64 loss: 0.031347036361694336
Batch 32/64 loss: 0.009190618991851807
Batch 33/64 loss: -0.0013040900230407715
Batch 34/64 loss: 0.0401533842086792
Batch 35/64 loss: 0.014592468738555908
Batch 36/64 loss: 0.03232896327972412
Batch 37/64 loss: -0.008634686470031738
Batch 38/64 loss: 0.012625515460968018
Batch 39/64 loss: 0.031624674797058105
Batch 40/64 loss: 0.021667003631591797
Batch 41/64 loss: 0.01280057430267334
Batch 42/64 loss: 0.022560417652130127
Batch 43/64 loss: 0.03760427236557007
Batch 44/64 loss: -0.005704402923583984
Batch 45/64 loss: 0.013888359069824219
Batch 46/64 loss: 0.009328186511993408
Batch 47/64 loss: 0.027045726776123047
Batch 48/64 loss: -0.012256085872650146
Batch 49/64 loss: 0.009852290153503418
Batch 50/64 loss: 0.003847181797027588
Batch 51/64 loss: -0.015548944473266602
Batch 52/64 loss: 0.017984747886657715
Batch 53/64 loss: -0.007011890411376953
Batch 54/64 loss: 0.030147135257720947
Batch 55/64 loss: -0.0009670853614807129
Batch 56/64 loss: 0.040669798851013184
Batch 57/64 loss: -0.013462662696838379
Batch 58/64 loss: -0.0027881264686584473
Batch 59/64 loss: 0.025228679180145264
Batch 60/64 loss: 0.008031725883483887
Batch 61/64 loss: 0.006706655025482178
Batch 62/64 loss: 0.006759285926818848
Batch 63/64 loss: 0.01766347885131836
Batch 64/64 loss: 0.0013547539710998535
Epoch 443  Train loss: 0.009138379143733603  Val loss: 0.11897992391356897
Epoch 444
-------------------------------
Batch 1/64 loss: 0.03951841592788696
Batch 2/64 loss: 0.013193249702453613
Batch 3/64 loss: 0.007811248302459717
Batch 4/64 loss: 0.010769248008728027
Batch 5/64 loss: 0.0013952851295471191
Batch 6/64 loss: 0.027137041091918945
Batch 7/64 loss: 0.025812268257141113
Batch 8/64 loss: -0.004034817218780518
Batch 9/64 loss: -0.015441954135894775
Batch 10/64 loss: 0.0016512870788574219
Batch 11/64 loss: -0.00797051191329956
Batch 12/64 loss: -0.02057778835296631
Batch 13/64 loss: -0.0033516883850097656
Batch 14/64 loss: 0.007794976234436035
Batch 15/64 loss: 0.006042897701263428
Batch 16/64 loss: 0.005441904067993164
Batch 17/64 loss: 0.002165079116821289
Batch 18/64 loss: 0.0028302669525146484
Batch 19/64 loss: 0.013086438179016113
Batch 20/64 loss: 0.006004691123962402
Batch 21/64 loss: 0.0020877718925476074
Batch 22/64 loss: -0.003437817096710205
Batch 23/64 loss: 0.012218475341796875
Batch 24/64 loss: 0.026276230812072754
Batch 25/64 loss: 0.028980493545532227
Batch 26/64 loss: -0.0018805265426635742
Batch 27/64 loss: 0.017245173454284668
Batch 28/64 loss: 0.031667888164520264
Batch 29/64 loss: 0.03100508451461792
Batch 30/64 loss: -0.00369340181350708
Batch 31/64 loss: 0.025246620178222656
Batch 32/64 loss: 0.001048266887664795
Batch 33/64 loss: -0.005376338958740234
Batch 34/64 loss: 0.01081627607345581
Batch 35/64 loss: 0.007880091667175293
Batch 36/64 loss: 0.020375370979309082
Batch 37/64 loss: 0.011416912078857422
Batch 38/64 loss: -0.008681297302246094
Batch 39/64 loss: -0.004063785076141357
Batch 40/64 loss: 0.012343168258666992
Batch 41/64 loss: 0.016452312469482422
Batch 42/64 loss: 0.021288156509399414
Batch 43/64 loss: -0.004813253879547119
Batch 44/64 loss: -0.016064226627349854
Batch 45/64 loss: 0.028037548065185547
Batch 46/64 loss: -0.008728623390197754
Batch 47/64 loss: -0.011431276798248291
Batch 48/64 loss: 0.012203991413116455
Batch 49/64 loss: -0.0040798187255859375
Batch 50/64 loss: 0.0031838417053222656
Batch 51/64 loss: 0.01092994213104248
Batch 52/64 loss: 0.028259873390197754
Batch 53/64 loss: -0.003989517688751221
Batch 54/64 loss: -0.0038049817085266113
Batch 55/64 loss: -0.015876293182373047
Batch 56/64 loss: 0.03721493482589722
Batch 57/64 loss: 0.018278837203979492
Batch 58/64 loss: -0.00042057037353515625
Batch 59/64 loss: 0.0005385279655456543
Batch 60/64 loss: 0.03514409065246582
Batch 61/64 loss: 0.006585359573364258
Batch 62/64 loss: 0.017880618572235107
Batch 63/64 loss: 0.03935033082962036
Batch 64/64 loss: 0.012820243835449219
Epoch 444  Train loss: 0.008572661642934762  Val loss: 0.11797792067642474
Epoch 445
-------------------------------
Batch 1/64 loss: 0.008522987365722656
Batch 2/64 loss: 0.0016666650772094727
Batch 3/64 loss: 0.00827115774154663
Batch 4/64 loss: 0.021471500396728516
Batch 5/64 loss: 0.0007274746894836426
Batch 6/64 loss: 0.012657761573791504
Batch 7/64 loss: -0.00927579402923584
Batch 8/64 loss: -0.005294382572174072
Batch 9/64 loss: -0.005549907684326172
Batch 10/64 loss: 0.017599403858184814
Batch 11/64 loss: -0.0015985369682312012
Batch 12/64 loss: -0.0028122663497924805
Batch 13/64 loss: 0.011681914329528809
Batch 14/64 loss: 0.005732595920562744
Batch 15/64 loss: 0.01785987615585327
Batch 16/64 loss: -0.00719141960144043
Batch 17/64 loss: -0.020684361457824707
Batch 18/64 loss: -0.000402987003326416
Batch 19/64 loss: -0.0005223751068115234
Batch 20/64 loss: -0.02002882957458496
Batch 21/64 loss: 0.008288145065307617
Batch 22/64 loss: -0.00745314359664917
Batch 23/64 loss: 0.01981133222579956
Batch 24/64 loss: 0.041655540466308594
Batch 25/64 loss: 0.03655761480331421
Batch 26/64 loss: 0.0021012425422668457
Batch 27/64 loss: 0.05302613973617554
Batch 28/64 loss: 0.03690791130065918
Batch 29/64 loss: 0.009210348129272461
Batch 30/64 loss: 0.016527533531188965
Batch 31/64 loss: 0.00261765718460083
Batch 32/64 loss: 0.02624690532684326
Batch 33/64 loss: 0.01403886079788208
Batch 34/64 loss: 0.016947269439697266
Batch 35/64 loss: 0.004478931427001953
Batch 36/64 loss: 0.0039844512939453125
Batch 37/64 loss: -0.0021163225173950195
Batch 38/64 loss: 0.005174875259399414
Batch 39/64 loss: 0.0005081295967102051
Batch 40/64 loss: 0.014897167682647705
Batch 41/64 loss: 0.012481212615966797
Batch 42/64 loss: 0.005400598049163818
Batch 43/64 loss: 0.011622905731201172
Batch 44/64 loss: -0.0007740259170532227
Batch 45/64 loss: 0.009150385856628418
Batch 46/64 loss: -0.0041956305503845215
Batch 47/64 loss: -0.0029747486114501953
Batch 48/64 loss: 0.011303305625915527
Batch 49/64 loss: -0.009009778499603271
Batch 50/64 loss: 0.002654135227203369
Batch 51/64 loss: 0.012722134590148926
Batch 52/64 loss: 0.008183956146240234
Batch 53/64 loss: 0.028933048248291016
Batch 54/64 loss: -0.005274653434753418
Batch 55/64 loss: 0.015287041664123535
Batch 56/64 loss: -0.008566200733184814
Batch 57/64 loss: 0.02308732271194458
Batch 58/64 loss: 0.02163916826248169
Batch 59/64 loss: 0.016240954399108887
Batch 60/64 loss: 0.035895586013793945
Batch 61/64 loss: 0.012293875217437744
Batch 62/64 loss: 0.002181828022003174
Batch 63/64 loss: 0.005543112754821777
Batch 64/64 loss: 0.010744094848632812
Epoch 445  Train loss: 0.008598034054625269  Val loss: 0.12053346306188
Epoch 446
-------------------------------
Batch 1/64 loss: 0.0241854190826416
Batch 2/64 loss: -0.0027243494987487793
Batch 3/64 loss: -0.0006788969039916992
Batch 4/64 loss: 0.02779477834701538
Batch 5/64 loss: -0.03654980659484863
Batch 6/64 loss: 0.012489676475524902
Batch 7/64 loss: 0.008401095867156982
Batch 8/64 loss: -0.00010669231414794922
Batch 9/64 loss: 0.0010212063789367676
Batch 10/64 loss: 0.006957948207855225
Batch 11/64 loss: 0.012421131134033203
Batch 12/64 loss: -0.011861979961395264
Batch 13/64 loss: -0.014501631259918213
Batch 14/64 loss: 0.022221922874450684
Batch 15/64 loss: 0.005773305892944336
Batch 16/64 loss: 0.038121163845062256
Batch 17/64 loss: 0.037127137184143066
Batch 18/64 loss: -0.007044613361358643
Batch 19/64 loss: -0.006662905216217041
Batch 20/64 loss: 0.04429870843887329
Batch 21/64 loss: 0.012295365333557129
Batch 22/64 loss: -0.008529305458068848
Batch 23/64 loss: -0.0029146671295166016
Batch 24/64 loss: -0.005583047866821289
Batch 25/64 loss: -0.027780354022979736
Batch 26/64 loss: 0.002774477005004883
Batch 27/64 loss: -0.02003622055053711
Batch 28/64 loss: 0.04555976390838623
Batch 29/64 loss: -0.010090410709381104
Batch 30/64 loss: 0.01195138692855835
Batch 31/64 loss: 0.05528682470321655
Batch 32/64 loss: 0.026112675666809082
Batch 33/64 loss: 0.005432724952697754
Batch 34/64 loss: 0.008207082748413086
Batch 35/64 loss: 0.0034132003784179688
Batch 36/64 loss: 0.02052462100982666
Batch 37/64 loss: -0.00289154052734375
Batch 38/64 loss: 0.013677239418029785
Batch 39/64 loss: -0.011878907680511475
Batch 40/64 loss: 0.021278977394104004
Batch 41/64 loss: -0.009520411491394043
Batch 42/64 loss: 0.0011717677116394043
Batch 43/64 loss: -0.0030984878540039062
Batch 44/64 loss: 0.013059258460998535
Batch 45/64 loss: 0.013894855976104736
Batch 46/64 loss: 0.037423014640808105
Batch 47/64 loss: 0.0056705474853515625
Batch 48/64 loss: -0.011195480823516846
Batch 49/64 loss: 0.017700493335723877
Batch 50/64 loss: 0.008986949920654297
Batch 51/64 loss: -0.005235016345977783
Batch 52/64 loss: -0.0006604790687561035
Batch 53/64 loss: 0.007347524166107178
Batch 54/64 loss: 0.006817758083343506
Batch 55/64 loss: 0.023017406463623047
Batch 56/64 loss: -0.0052337646484375
Batch 57/64 loss: 0.02117818593978882
Batch 58/64 loss: 0.0033550262451171875
Batch 59/64 loss: 0.024034202098846436
Batch 60/64 loss: 0.004926443099975586
Batch 61/64 loss: 0.023883163928985596
Batch 62/64 loss: 0.019765794277191162
Batch 63/64 loss: 0.01554262638092041
Batch 64/64 loss: -0.0009784102439880371
Epoch 446  Train loss: 0.007993569794823142  Val loss: 0.12019657269376251
Epoch 447
-------------------------------
Batch 1/64 loss: 0.007730722427368164
Batch 2/64 loss: -0.00991898775100708
Batch 3/64 loss: 0.0022721290588378906
Batch 4/64 loss: 0.005714237689971924
Batch 5/64 loss: -0.03084784746170044
Batch 6/64 loss: -0.0023545026779174805
Batch 7/64 loss: 0.012082815170288086
Batch 8/64 loss: 0.024979710578918457
Batch 9/64 loss: 0.02719956636428833
Batch 10/64 loss: 0.04587066173553467
Batch 11/64 loss: 0.030046820640563965
Batch 12/64 loss: -0.0016680359840393066
Batch 13/64 loss: 0.027820348739624023
Batch 14/64 loss: -0.00472027063369751
Batch 15/64 loss: 0.05892050266265869
Batch 16/64 loss: 0.0007145404815673828
Batch 17/64 loss: 0.013541340827941895
Batch 18/64 loss: 0.014274835586547852
Batch 19/64 loss: -0.010794460773468018
Batch 20/64 loss: 0.02097451686859131
Batch 21/64 loss: 0.0013841986656188965
Batch 22/64 loss: 0.0003656744956970215
Batch 23/64 loss: 0.009169995784759521
Batch 24/64 loss: 0.012491226196289062
Batch 25/64 loss: -0.0287095308303833
Batch 26/64 loss: 0.009625434875488281
Batch 27/64 loss: -0.006816208362579346
Batch 28/64 loss: 0.001511991024017334
Batch 29/64 loss: 0.007571816444396973
Batch 30/64 loss: 0.010687530040740967
Batch 31/64 loss: -0.02280104160308838
Batch 32/64 loss: 0.05326789617538452
Batch 33/64 loss: -0.012316703796386719
Batch 34/64 loss: 0.009397685527801514
Batch 35/64 loss: 0.014671921730041504
Batch 36/64 loss: 0.02041804790496826
Batch 37/64 loss: 0.015020668506622314
Batch 38/64 loss: -0.0012457966804504395
Batch 39/64 loss: 0.019442617893218994
Batch 40/64 loss: 0.032033324241638184
Batch 41/64 loss: 0.0030218958854675293
Batch 42/64 loss: 0.009687840938568115
Batch 43/64 loss: 0.014567673206329346
Batch 44/64 loss: 0.013215422630310059
Batch 45/64 loss: 0.0031204819679260254
Batch 46/64 loss: 0.051185786724090576
Batch 47/64 loss: 0.028714120388031006
Batch 48/64 loss: 0.016407668590545654
Batch 49/64 loss: 0.0052585601806640625
Batch 50/64 loss: 0.03576469421386719
Batch 51/64 loss: 0.012402951717376709
Batch 52/64 loss: -0.0052939653396606445
Batch 53/64 loss: -0.012596487998962402
Batch 54/64 loss: 0.014327287673950195
Batch 55/64 loss: 0.0321657657623291
Batch 56/64 loss: 0.018207669258117676
Batch 57/64 loss: 0.020321309566497803
Batch 58/64 loss: -0.01397240161895752
Batch 59/64 loss: -0.007875800132751465
Batch 60/64 loss: 0.017791271209716797
Batch 61/64 loss: 0.0062787532806396484
Batch 62/64 loss: 0.008850038051605225
Batch 63/64 loss: 0.012162744998931885
Batch 64/64 loss: -0.019680380821228027
Epoch 447  Train loss: 0.010132743330562816  Val loss: 0.12185160974456682
Epoch 448
-------------------------------
Batch 1/64 loss: 0.00021439790725708008
Batch 2/64 loss: 0.011060774326324463
Batch 3/64 loss: 0.009671270847320557
Batch 4/64 loss: 9.882450103759766e-05
Batch 5/64 loss: 0.008821547031402588
Batch 6/64 loss: -0.013758718967437744
Batch 7/64 loss: 0.01856386661529541
Batch 8/64 loss: 0.01852726936340332
Batch 9/64 loss: 0.015883266925811768
Batch 10/64 loss: -0.0017985105514526367
Batch 11/64 loss: 0.029964685440063477
Batch 12/64 loss: -0.002022862434387207
Batch 13/64 loss: 0.01692110300064087
Batch 14/64 loss: -0.0390017032623291
Batch 15/64 loss: -0.0068801045417785645
Batch 16/64 loss: 0.009362459182739258
Batch 17/64 loss: 0.015891313552856445
Batch 18/64 loss: 0.007060050964355469
Batch 19/64 loss: 0.016876161098480225
Batch 20/64 loss: -0.00793600082397461
Batch 21/64 loss: 0.00791555643081665
Batch 22/64 loss: 0.03539913892745972
Batch 23/64 loss: 0.010169029235839844
Batch 24/64 loss: -0.0028702616691589355
Batch 25/64 loss: 0.02454841136932373
Batch 26/64 loss: 0.03513067960739136
Batch 27/64 loss: 0.030018210411071777
Batch 28/64 loss: 0.03434211015701294
Batch 29/64 loss: -0.006379365921020508
Batch 30/64 loss: 0.021530985832214355
Batch 31/64 loss: 0.0028412342071533203
Batch 32/64 loss: 0.013052940368652344
Batch 33/64 loss: -0.006974339485168457
Batch 34/64 loss: 0.01617264747619629
Batch 35/64 loss: 0.023340463638305664
Batch 36/64 loss: -0.018891215324401855
Batch 37/64 loss: 0.0032311677932739258
Batch 38/64 loss: -0.0019951462745666504
Batch 39/64 loss: -0.0009093284606933594
Batch 40/64 loss: -0.0020222067832946777
Batch 41/64 loss: 0.031156420707702637
Batch 42/64 loss: 0.009086966514587402
Batch 43/64 loss: 0.024961888790130615
Batch 44/64 loss: -0.010113239288330078
Batch 45/64 loss: 0.026836931705474854
Batch 46/64 loss: 0.0009941458702087402
Batch 47/64 loss: 0.014521658420562744
Batch 48/64 loss: 0.0014935135841369629
Batch 49/64 loss: -0.026547908782958984
Batch 50/64 loss: -0.002597033977508545
Batch 51/64 loss: 0.010139226913452148
Batch 52/64 loss: 0.02833235263824463
Batch 53/64 loss: 0.023610591888427734
Batch 54/64 loss: -0.0019589662551879883
Batch 55/64 loss: 0.028093576431274414
Batch 56/64 loss: 1.728534698486328e-05
Batch 57/64 loss: 0.03630703687667847
Batch 58/64 loss: 0.04253268241882324
Batch 59/64 loss: 0.004987239837646484
Batch 60/64 loss: -0.01168745756149292
Batch 61/64 loss: 0.005706906318664551
Batch 62/64 loss: -0.009995222091674805
Batch 63/64 loss: -0.02124488353729248
Batch 64/64 loss: 0.002322852611541748
Epoch 448  Train loss: 0.00833797104218427  Val loss: 0.1173497755912571
Epoch 449
-------------------------------
Batch 1/64 loss: 0.004600822925567627
Batch 2/64 loss: -0.01939988136291504
Batch 3/64 loss: -0.0024852752685546875
Batch 4/64 loss: -0.02612173557281494
Batch 5/64 loss: 0.02087104320526123
Batch 6/64 loss: 0.0072154998779296875
Batch 7/64 loss: -0.014810562133789062
Batch 8/64 loss: -0.006703972816467285
Batch 9/64 loss: -0.009439945220947266
Batch 10/64 loss: -0.007405638694763184
Batch 11/64 loss: -0.012721359729766846
Batch 12/64 loss: -0.0056801438331604
Batch 13/64 loss: 8.940696716308594e-06
Batch 14/64 loss: 0.004933297634124756
Batch 15/64 loss: 0.036899447441101074
Batch 16/64 loss: 0.02136838436126709
Batch 17/64 loss: 0.025593161582946777
Batch 18/64 loss: 0.01709514856338501
Batch 19/64 loss: 0.015009582042694092
Batch 20/64 loss: 0.015597343444824219
Batch 21/64 loss: 0.03721415996551514
Batch 22/64 loss: 0.03495842218399048
Batch 23/64 loss: -0.010746896266937256
Batch 24/64 loss: 0.022732019424438477
Batch 25/64 loss: 0.028914690017700195
Batch 26/64 loss: 0.008049190044403076
Batch 27/64 loss: 0.004817068576812744
Batch 28/64 loss: -4.2557716369628906e-05
Batch 29/64 loss: 0.02375507354736328
Batch 30/64 loss: 0.007354259490966797
Batch 31/64 loss: 0.010987460613250732
Batch 32/64 loss: 0.013068914413452148
Batch 33/64 loss: 0.01388096809387207
Batch 34/64 loss: 0.005898773670196533
Batch 35/64 loss: 0.019021451473236084
Batch 36/64 loss: 0.018544435501098633
Batch 37/64 loss: 0.023140013217926025
Batch 38/64 loss: 0.01595604419708252
Batch 39/64 loss: -0.0048220157623291016
Batch 40/64 loss: 0.002887547016143799
Batch 41/64 loss: 0.024914443492889404
Batch 42/64 loss: -0.0041925907135009766
Batch 43/64 loss: 0.00880444049835205
Batch 44/64 loss: 0.004073977470397949
Batch 45/64 loss: 0.007704496383666992
Batch 46/64 loss: 0.0004843473434448242
Batch 47/64 loss: -0.00035262107849121094
Batch 48/64 loss: -0.017979919910430908
Batch 49/64 loss: 0.022391915321350098
Batch 50/64 loss: -0.002462327480316162
Batch 51/64 loss: 0.0069945454597473145
Batch 52/64 loss: 0.020318806171417236
Batch 53/64 loss: 0.017824769020080566
Batch 54/64 loss: 0.056172966957092285
Batch 55/64 loss: 0.009853243827819824
Batch 56/64 loss: 0.016191184520721436
Batch 57/64 loss: 0.0038652420043945312
Batch 58/64 loss: 0.022778630256652832
Batch 59/64 loss: 0.019951939582824707
Batch 60/64 loss: -0.0017598271369934082
Batch 61/64 loss: 0.005523681640625
Batch 62/64 loss: 0.02077960968017578
Batch 63/64 loss: -0.006279110908508301
Batch 64/64 loss: 0.019732952117919922
Epoch 449  Train loss: 0.009261093887628294  Val loss: 0.11760527202763509
Epoch 450
-------------------------------
Batch 1/64 loss: 0.008626103401184082
Batch 2/64 loss: 0.012017905712127686
Batch 3/64 loss: 0.028264522552490234
Batch 4/64 loss: 0.00851660966873169
Batch 5/64 loss: 0.01157921552658081
Batch 6/64 loss: -0.015821993350982666
Batch 7/64 loss: 0.02074187994003296
Batch 8/64 loss: 0.03545117378234863
Batch 9/64 loss: -0.01795142889022827
Batch 10/64 loss: 0.015953361988067627
Batch 11/64 loss: 0.010202109813690186
Batch 12/64 loss: -0.003910481929779053
Batch 13/64 loss: 0.012876391410827637
Batch 14/64 loss: 0.000863194465637207
Batch 15/64 loss: -0.0013428926467895508
Batch 16/64 loss: 0.027834057807922363
Batch 17/64 loss: 0.00010061264038085938
Batch 18/64 loss: -0.0018451213836669922
Batch 19/64 loss: 0.00760197639465332
Batch 20/64 loss: 0.0006461739540100098
Batch 21/64 loss: -0.0003854036331176758
Batch 22/64 loss: -0.017556846141815186
Batch 23/64 loss: 0.02375316619873047
Batch 24/64 loss: 0.002877950668334961
Batch 25/64 loss: -0.00575411319732666
Batch 26/64 loss: 0.0369640588760376
Batch 27/64 loss: 0.02934563159942627
Batch 28/64 loss: -0.007282674312591553
Batch 29/64 loss: 0.046075284481048584
Batch 30/64 loss: -0.024835526943206787
Batch 31/64 loss: -0.0005294084548950195
Batch 32/64 loss: -0.02441352605819702
Batch 33/64 loss: 0.01003885269165039
Batch 34/64 loss: 0.006582379341125488
Batch 35/64 loss: 0.04686236381530762
Batch 36/64 loss: 0.014574289321899414
Batch 37/64 loss: 0.017613351345062256
Batch 38/64 loss: 0.005526363849639893
Batch 39/64 loss: 0.005684971809387207
Batch 40/64 loss: 0.0016946792602539062
Batch 41/64 loss: 0.020251810550689697
Batch 42/64 loss: 0.062026023864746094
Batch 43/64 loss: 0.014505505561828613
Batch 44/64 loss: -0.005345046520233154
Batch 45/64 loss: 0.00889289379119873
Batch 46/64 loss: -0.0125962495803833
Batch 47/64 loss: -0.02257436513900757
Batch 48/64 loss: 0.010388970375061035
Batch 49/64 loss: 0.026283621788024902
Batch 50/64 loss: 0.021877586841583252
Batch 51/64 loss: 0.039482951164245605
Batch 52/64 loss: 0.028660476207733154
Batch 53/64 loss: 0.008956670761108398
Batch 54/64 loss: -0.01007699966430664
Batch 55/64 loss: -0.00894784927368164
Batch 56/64 loss: -0.014600872993469238
Batch 57/64 loss: -0.011516094207763672
Batch 58/64 loss: 0.01780158281326294
Batch 59/64 loss: 0.03516799211502075
Batch 60/64 loss: -0.011465966701507568
Batch 61/64 loss: 0.00615006685256958
Batch 62/64 loss: -0.013584494590759277
Batch 63/64 loss: 0.013418793678283691
Batch 64/64 loss: -0.009105324745178223
Epoch 450  Train loss: 0.008212819286421234  Val loss: 0.12046567353186328
Epoch 451
-------------------------------
Batch 1/64 loss: -0.028383255004882812
Batch 2/64 loss: -0.024017035961151123
Batch 3/64 loss: -0.0040441155433654785
Batch 4/64 loss: 0.004095971584320068
Batch 5/64 loss: -0.016727685928344727
Batch 6/64 loss: -0.005818486213684082
Batch 7/64 loss: 0.00044649839401245117
Batch 8/64 loss: -0.007063150405883789
Batch 9/64 loss: 0.020589709281921387
Batch 10/64 loss: 0.017648398876190186
Batch 11/64 loss: 0.0029897093772888184
Batch 12/64 loss: 0.029899120330810547
Batch 13/64 loss: 0.023737013339996338
Batch 14/64 loss: 0.002557098865509033
Batch 15/64 loss: 0.006352543830871582
Batch 16/64 loss: -0.014443755149841309
Batch 17/64 loss: 0.010691165924072266
Batch 18/64 loss: 0.0038245320320129395
Batch 19/64 loss: -0.023839354515075684
Batch 20/64 loss: 0.028283298015594482
Batch 21/64 loss: 0.001706242561340332
Batch 22/64 loss: 0.009662985801696777
Batch 23/64 loss: 0.033741533756256104
Batch 24/64 loss: -0.007948696613311768
Batch 25/64 loss: -0.023334741592407227
Batch 26/64 loss: -0.005893409252166748
Batch 27/64 loss: -0.012541234493255615
Batch 28/64 loss: -0.036465346813201904
Batch 29/64 loss: 0.0009794235229492188
Batch 30/64 loss: -0.004757344722747803
Batch 31/64 loss: 0.01745462417602539
Batch 32/64 loss: 0.01894354820251465
Batch 33/64 loss: -0.0066277384757995605
Batch 34/64 loss: 0.04172921180725098
Batch 35/64 loss: 0.026033639907836914
Batch 36/64 loss: -0.005573570728302002
Batch 37/64 loss: 0.007254302501678467
Batch 38/64 loss: -0.012675583362579346
Batch 39/64 loss: -0.002622246742248535
Batch 40/64 loss: 0.009700298309326172
Batch 41/64 loss: 0.019857406616210938
Batch 42/64 loss: -0.005835413932800293
Batch 43/64 loss: 0.015445530414581299
Batch 44/64 loss: 0.017644524574279785
Batch 45/64 loss: 0.02547973394393921
Batch 46/64 loss: 0.028701603412628174
Batch 47/64 loss: 0.015187084674835205
Batch 48/64 loss: 0.006462812423706055
Batch 49/64 loss: 0.013738274574279785
Batch 50/64 loss: 0.03826439380645752
Batch 51/64 loss: 0.03537559509277344
Batch 52/64 loss: 0.01784127950668335
Batch 53/64 loss: 0.026425600051879883
Batch 54/64 loss: 0.041108667850494385
Batch 55/64 loss: 0.04165303707122803
Batch 56/64 loss: -0.004771411418914795
Batch 57/64 loss: 0.014251947402954102
Batch 58/64 loss: 0.008716702461242676
Batch 59/64 loss: 0.025151610374450684
Batch 60/64 loss: 0.002507925033569336
Batch 61/64 loss: -0.0036334991455078125
Batch 62/64 loss: -0.025432229042053223
Batch 63/64 loss: 0.0042285919189453125
Batch 64/64 loss: 0.03946661949157715
Epoch 451  Train loss: 0.00727080550848269  Val loss: 0.12015303508522585
Epoch 452
-------------------------------
Batch 1/64 loss: 0.021138668060302734
Batch 2/64 loss: 0.018446147441864014
Batch 3/64 loss: 0.03510701656341553
Batch 4/64 loss: -0.01376497745513916
Batch 5/64 loss: 0.005260825157165527
Batch 6/64 loss: 0.03711700439453125
Batch 7/64 loss: -0.00599902868270874
Batch 8/64 loss: 0.027813255786895752
Batch 9/64 loss: 0.013068079948425293
Batch 10/64 loss: -0.013910651206970215
Batch 11/64 loss: 0.02165120840072632
Batch 12/64 loss: 0.03326594829559326
Batch 13/64 loss: 0.0024489760398864746
Batch 14/64 loss: 0.028845906257629395
Batch 15/64 loss: 0.008736252784729004
Batch 16/64 loss: -0.0015169382095336914
Batch 17/64 loss: 0.002839028835296631
Batch 18/64 loss: -0.004723072052001953
Batch 19/64 loss: 0.027139723300933838
Batch 20/64 loss: 0.06337010860443115
Batch 21/64 loss: 0.0040552616119384766
Batch 22/64 loss: -0.021630823612213135
Batch 23/64 loss: 0.004118561744689941
Batch 24/64 loss: -0.0033507943153381348
Batch 25/64 loss: -0.013181447982788086
Batch 26/64 loss: -0.01676422357559204
Batch 27/64 loss: -0.010279297828674316
Batch 28/64 loss: 0.0005357265472412109
Batch 29/64 loss: -0.0006150007247924805
Batch 30/64 loss: 0.0026706457138061523
Batch 31/64 loss: 0.0024848580360412598
Batch 32/64 loss: 0.02596747875213623
Batch 33/64 loss: 0.039036691188812256
Batch 34/64 loss: 0.041267991065979004
Batch 35/64 loss: -0.017703115940093994
Batch 36/64 loss: -0.008938074111938477
Batch 37/64 loss: 0.031137466430664062
Batch 38/64 loss: 0.014103293418884277
Batch 39/64 loss: 0.023793935775756836
Batch 40/64 loss: 0.027320384979248047
Batch 41/64 loss: -6.29425048828125e-05
Batch 42/64 loss: -0.017102956771850586
Batch 43/64 loss: 0.028140664100646973
Batch 44/64 loss: 0.017736852169036865
Batch 45/64 loss: 0.010269403457641602
Batch 46/64 loss: 0.022224009037017822
Batch 47/64 loss: -0.0027243494987487793
Batch 48/64 loss: -0.0006320476531982422
Batch 49/64 loss: 0.016303837299346924
Batch 50/64 loss: -0.010613679885864258
Batch 51/64 loss: 0.004617929458618164
Batch 52/64 loss: -0.00029838085174560547
Batch 53/64 loss: 0.02683025598526001
Batch 54/64 loss: -0.007008612155914307
Batch 55/64 loss: 0.026513993740081787
Batch 56/64 loss: 0.012828528881072998
Batch 57/64 loss: 0.03282308578491211
Batch 58/64 loss: -0.001202225685119629
Batch 59/64 loss: -0.010170996189117432
Batch 60/64 loss: -0.00075531005859375
Batch 61/64 loss: -0.00047528743743896484
Batch 62/64 loss: -0.015989065170288086
Batch 63/64 loss: 0.0042231082916259766
Batch 64/64 loss: -0.004339933395385742
Epoch 452  Train loss: 0.008824844921336455  Val loss: 0.11994164117013466
Epoch 453
-------------------------------
Batch 1/64 loss: 0.01949518918991089
Batch 2/64 loss: -0.003009200096130371
Batch 3/64 loss: 0.01834893226623535
Batch 4/64 loss: 0.0033664703369140625
Batch 5/64 loss: -0.0005474686622619629
Batch 6/64 loss: 0.013163745403289795
Batch 7/64 loss: -0.0023377537727355957
Batch 8/64 loss: -0.007052302360534668
Batch 9/64 loss: 0.01041269302368164
Batch 10/64 loss: 0.015982866287231445
Batch 11/64 loss: -0.015258729457855225
Batch 12/64 loss: -0.0053032636642456055
Batch 13/64 loss: -0.02155357599258423
Batch 14/64 loss: -0.01955568790435791
Batch 15/64 loss: -0.0106123685836792
Batch 16/64 loss: -0.026675164699554443
Batch 17/64 loss: 0.01606428623199463
Batch 18/64 loss: -0.014652609825134277
Batch 19/64 loss: -0.013547241687774658
Batch 20/64 loss: 0.012857258319854736
Batch 21/64 loss: -0.004107832908630371
Batch 22/64 loss: -0.020800769329071045
Batch 23/64 loss: 0.012373268604278564
Batch 24/64 loss: 0.011662006378173828
Batch 25/64 loss: 0.0032863616943359375
Batch 26/64 loss: -0.016572654247283936
Batch 27/64 loss: 0.03775674104690552
Batch 28/64 loss: -0.0018818378448486328
Batch 29/64 loss: 0.021254539489746094
Batch 30/64 loss: 0.036440372467041016
Batch 31/64 loss: 0.03209269046783447
Batch 32/64 loss: 0.04373776912689209
Batch 33/64 loss: 0.015624940395355225
Batch 34/64 loss: 0.0017128586769104004
Batch 35/64 loss: 0.0023195743560791016
Batch 36/64 loss: -5.549192428588867e-05
Batch 37/64 loss: 0.006150662899017334
Batch 38/64 loss: 0.0023398399353027344
Batch 39/64 loss: 0.0262298583984375
Batch 40/64 loss: 0.012990951538085938
Batch 41/64 loss: -0.0012854933738708496
Batch 42/64 loss: 0.014387011528015137
Batch 43/64 loss: 0.05182194709777832
Batch 44/64 loss: -0.008802652359008789
Batch 45/64 loss: 0.022739827632904053
Batch 46/64 loss: 0.016612231731414795
Batch 47/64 loss: 0.016581237316131592
Batch 48/64 loss: 0.03607523441314697
Batch 49/64 loss: 0.039101600646972656
Batch 50/64 loss: 0.00023126602172851562
Batch 51/64 loss: 0.0005450844764709473
Batch 52/64 loss: -0.0002465248107910156
Batch 53/64 loss: 0.004593312740325928
Batch 54/64 loss: 0.025714397430419922
Batch 55/64 loss: -0.01274561882019043
Batch 56/64 loss: 0.005473494529724121
Batch 57/64 loss: 0.005663692951202393
Batch 58/64 loss: 0.00467073917388916
Batch 59/64 loss: 0.0033568143844604492
Batch 60/64 loss: 0.004974424839019775
Batch 61/64 loss: 0.03479403257369995
Batch 62/64 loss: -0.0024883151054382324
Batch 63/64 loss: 0.038851916790008545
Batch 64/64 loss: 0.022634029388427734
Epoch 453  Train loss: 0.007995844822303921  Val loss: 0.12043579870073247
Epoch 454
-------------------------------
Batch 1/64 loss: -0.02003180980682373
Batch 2/64 loss: 0.01980358362197876
Batch 3/64 loss: -0.014209628105163574
Batch 4/64 loss: -0.004266858100891113
Batch 5/64 loss: 0.009234368801116943
Batch 6/64 loss: 0.01218479871749878
Batch 7/64 loss: 0.010719537734985352
Batch 8/64 loss: 0.010837137699127197
Batch 9/64 loss: 0.018612980842590332
Batch 10/64 loss: -0.009051084518432617
Batch 11/64 loss: -0.012267768383026123
Batch 12/64 loss: -0.003265857696533203
Batch 13/64 loss: -0.019313931465148926
Batch 14/64 loss: -0.010627150535583496
Batch 15/64 loss: 0.017226576805114746
Batch 16/64 loss: 0.03205132484436035
Batch 17/64 loss: -0.0012724995613098145
Batch 18/64 loss: 0.018399596214294434
Batch 19/64 loss: 0.02732771635055542
Batch 20/64 loss: 0.018035531044006348
Batch 21/64 loss: 0.006731867790222168
Batch 22/64 loss: -0.0038314461708068848
Batch 23/64 loss: 0.008842945098876953
Batch 24/64 loss: 0.0069702863693237305
Batch 25/64 loss: 0.007160186767578125
Batch 26/64 loss: 0.005375027656555176
Batch 27/64 loss: 0.006527125835418701
Batch 28/64 loss: 0.00788891315460205
Batch 29/64 loss: -0.010192811489105225
Batch 30/64 loss: -0.00022518634796142578
Batch 31/64 loss: -0.021821796894073486
Batch 32/64 loss: -0.012527406215667725
Batch 33/64 loss: 0.005340516567230225
Batch 34/64 loss: 0.03152233362197876
Batch 35/64 loss: 0.01239025592803955
Batch 36/64 loss: -0.002850770950317383
Batch 37/64 loss: 0.004262387752532959
Batch 38/64 loss: 0.0019211769104003906
Batch 39/64 loss: 0.00705409049987793
Batch 40/64 loss: 0.023034334182739258
Batch 41/64 loss: 0.005464017391204834
Batch 42/64 loss: 0.013629257678985596
Batch 43/64 loss: 0.050350069999694824
Batch 44/64 loss: 0.022181272506713867
Batch 45/64 loss: 0.0014035701751708984
Batch 46/64 loss: -0.011526703834533691
Batch 47/64 loss: 0.04601055383682251
Batch 48/64 loss: 0.008493244647979736
Batch 49/64 loss: -0.012462973594665527
Batch 50/64 loss: 0.027420639991760254
Batch 51/64 loss: -0.017452538013458252
Batch 52/64 loss: 0.0015944242477416992
Batch 53/64 loss: 0.0031827688217163086
Batch 54/64 loss: 0.013014495372772217
Batch 55/64 loss: -0.005702197551727295
Batch 56/64 loss: -0.012436628341674805
Batch 57/64 loss: 0.017160356044769287
Batch 58/64 loss: 0.026957809925079346
Batch 59/64 loss: -0.0013346672058105469
Batch 60/64 loss: 0.013797879219055176
Batch 61/64 loss: -0.0005308389663696289
Batch 62/64 loss: 0.006479442119598389
Batch 63/64 loss: 0.004698455333709717
Batch 64/64 loss: 0.017792582511901855
Epoch 454  Train loss: 0.006234270451115627  Val loss: 0.11830974362560154
Epoch 455
-------------------------------
Batch 1/64 loss: -0.025601327419281006
Batch 2/64 loss: 0.024216890335083008
Batch 3/64 loss: 0.02846837043762207
Batch 4/64 loss: -0.0240553617477417
Batch 5/64 loss: 0.022579073905944824
Batch 6/64 loss: -0.05094444751739502
Batch 7/64 loss: 0.001216888427734375
Batch 8/64 loss: 0.0031447410583496094
Batch 9/64 loss: 0.03358924388885498
Batch 10/64 loss: -0.010796666145324707
Batch 11/64 loss: 0.002351999282836914
Batch 12/64 loss: 0.02560943365097046
Batch 13/64 loss: -0.00792461633682251
Batch 14/64 loss: 0.027644693851470947
Batch 15/64 loss: 0.005618274211883545
Batch 16/64 loss: 0.012004673480987549
Batch 17/64 loss: -0.012354433536529541
Batch 18/64 loss: 0.011109888553619385
Batch 19/64 loss: 0.011402726173400879
Batch 20/64 loss: 0.004616379737854004
Batch 21/64 loss: 0.04121410846710205
Batch 22/64 loss: 0.030028343200683594
Batch 23/64 loss: -0.011079847812652588
Batch 24/64 loss: -0.0068468451499938965
Batch 25/64 loss: -0.011163890361785889
Batch 26/64 loss: -0.022291898727416992
Batch 27/64 loss: -0.001746833324432373
Batch 28/64 loss: -0.004350185394287109
Batch 29/64 loss: -0.019743740558624268
Batch 30/64 loss: 0.02810722589492798
Batch 31/64 loss: 0.02284228801727295
Batch 32/64 loss: 0.010489821434020996
Batch 33/64 loss: 0.013463973999023438
Batch 34/64 loss: 0.011492729187011719
Batch 35/64 loss: 0.0011354684829711914
Batch 36/64 loss: 0.012657105922698975
Batch 37/64 loss: 0.03667789697647095
Batch 38/64 loss: 0.019544243812561035
Batch 39/64 loss: 0.03792393207550049
Batch 40/64 loss: 0.015009939670562744
Batch 41/64 loss: 0.008739173412322998
Batch 42/64 loss: 0.007174491882324219
Batch 43/64 loss: 0.02723824977874756
Batch 44/64 loss: -0.008547067642211914
Batch 45/64 loss: 0.015029370784759521
Batch 46/64 loss: -0.009282112121582031
Batch 47/64 loss: 0.017205119132995605
Batch 48/64 loss: 0.012196242809295654
Batch 49/64 loss: -0.02768886089324951
Batch 50/64 loss: -0.023993074893951416
Batch 51/64 loss: -0.012318313121795654
Batch 52/64 loss: 0.027614593505859375
Batch 53/64 loss: -0.014155447483062744
Batch 54/64 loss: -0.01245272159576416
Batch 55/64 loss: 0.03086721897125244
Batch 56/64 loss: 0.028766155242919922
Batch 57/64 loss: 0.010561585426330566
Batch 58/64 loss: 0.02898871898651123
Batch 59/64 loss: -0.0007625818252563477
Batch 60/64 loss: -0.0012077093124389648
Batch 61/64 loss: 0.015292525291442871
Batch 62/64 loss: 0.012496352195739746
Batch 63/64 loss: 0.03596895933151245
Batch 64/64 loss: -0.010079264640808105
Epoch 455  Train loss: 0.006987163599799661  Val loss: 0.12121725942670684
Epoch 456
-------------------------------
Batch 1/64 loss: -0.002376377582550049
Batch 2/64 loss: 0.030087590217590332
Batch 3/64 loss: 0.023504257202148438
Batch 4/64 loss: -0.00811147689819336
Batch 5/64 loss: -0.012003123760223389
Batch 6/64 loss: 0.0291481614112854
Batch 7/64 loss: -0.017346620559692383
Batch 8/64 loss: -0.012811541557312012
Batch 9/64 loss: 0.01646566390991211
Batch 10/64 loss: 0.007365822792053223
Batch 11/64 loss: -0.00851351022720337
Batch 12/64 loss: 0.03593099117279053
Batch 13/64 loss: -0.004782915115356445
Batch 14/64 loss: 0.02445220947265625
Batch 15/64 loss: 0.005037665367126465
Batch 16/64 loss: -0.0031975507736206055
Batch 17/64 loss: -0.017244338989257812
Batch 18/64 loss: 0.0294838547706604
Batch 19/64 loss: -0.0004100203514099121
Batch 20/64 loss: 0.005556225776672363
Batch 21/64 loss: -0.01117616891860962
Batch 22/64 loss: 0.027272343635559082
Batch 23/64 loss: 0.012829840183258057
Batch 24/64 loss: 0.028851985931396484
Batch 25/64 loss: -0.0032225847244262695
Batch 26/64 loss: -0.002756834030151367
Batch 27/64 loss: 0.03912830352783203
Batch 28/64 loss: -0.002473294734954834
Batch 29/64 loss: 0.0014702677726745605
Batch 30/64 loss: -0.002980649471282959
Batch 31/64 loss: 0.02498406171798706
Batch 32/64 loss: -0.00348508358001709
Batch 33/64 loss: 0.020323634147644043
Batch 34/64 loss: -0.011745214462280273
Batch 35/64 loss: 0.014415085315704346
Batch 36/64 loss: 0.003428637981414795
Batch 37/64 loss: -0.01798105239868164
Batch 38/64 loss: 0.008014976978302002
Batch 39/64 loss: -0.01251918077468872
Batch 40/64 loss: -0.0018517374992370605
Batch 41/64 loss: 0.0034804940223693848
Batch 42/64 loss: 0.018619120121002197
Batch 43/64 loss: -0.010747432708740234
Batch 44/64 loss: 0.020276248455047607
Batch 45/64 loss: 0.00973975658416748
Batch 46/64 loss: 0.0034582018852233887
Batch 47/64 loss: 0.000985562801361084
Batch 48/64 loss: -0.026619672775268555
Batch 49/64 loss: 0.01687026023864746
Batch 50/64 loss: 0.026432454586029053
Batch 51/64 loss: 0.013954222202301025
Batch 52/64 loss: 0.03235054016113281
Batch 53/64 loss: 0.024398505687713623
Batch 54/64 loss: 0.0021224021911621094
Batch 55/64 loss: 0.02436310052871704
Batch 56/64 loss: 0.022307991981506348
Batch 57/64 loss: 0.00033468008041381836
Batch 58/64 loss: 0.009842932224273682
Batch 59/64 loss: -0.008289217948913574
Batch 60/64 loss: 0.008427798748016357
Batch 61/64 loss: 0.044311583042144775
Batch 62/64 loss: -0.03162294626235962
Batch 63/64 loss: 0.030401647090911865
Batch 64/64 loss: 0.022910475730895996
Epoch 456  Train loss: 0.007581857138989018  Val loss: 0.1200369602626132
Epoch 457
-------------------------------
Batch 1/64 loss: -0.014333128929138184
Batch 2/64 loss: 0.010260343551635742
Batch 3/64 loss: 0.01750016212463379
Batch 4/64 loss: 0.023304760456085205
Batch 5/64 loss: 0.011575937271118164
Batch 6/64 loss: -0.004335999488830566
Batch 7/64 loss: 0.008675813674926758
Batch 8/64 loss: 0.027946949005126953
Batch 9/64 loss: -0.013849616050720215
Batch 10/64 loss: 0.004769444465637207
Batch 11/64 loss: 0.03277057409286499
Batch 12/64 loss: 0.015446901321411133
Batch 13/64 loss: -0.011180758476257324
Batch 14/64 loss: 0.028595924377441406
Batch 15/64 loss: 0.023151040077209473
Batch 16/64 loss: 0.0013815760612487793
Batch 17/64 loss: 0.0023026466369628906
Batch 18/64 loss: 0.0045351386070251465
Batch 19/64 loss: 0.02980726957321167
Batch 20/64 loss: -0.01569455862045288
Batch 21/64 loss: 0.01892024278640747
Batch 22/64 loss: 0.017874479293823242
Batch 23/64 loss: 0.01890939474105835
Batch 24/64 loss: 0.017209410667419434
Batch 25/64 loss: 0.009394466876983643
Batch 26/64 loss: 0.012595832347869873
Batch 27/64 loss: -0.0088614821434021
Batch 28/64 loss: -0.015505015850067139
Batch 29/64 loss: -0.00813513994216919
Batch 30/64 loss: -0.00803309679031372
Batch 31/64 loss: 0.022288084030151367
Batch 32/64 loss: 0.004579901695251465
Batch 33/64 loss: 0.01554882526397705
Batch 34/64 loss: 0.010689377784729004
Batch 35/64 loss: 0.01876235008239746
Batch 36/64 loss: 0.0005362033843994141
Batch 37/64 loss: 0.009703397750854492
Batch 38/64 loss: 0.004077017307281494
Batch 39/64 loss: -0.0039101243019104
Batch 40/64 loss: 0.009167194366455078
Batch 41/64 loss: -0.008333563804626465
Batch 42/64 loss: -0.01305246353149414
Batch 43/64 loss: 0.025388062000274658
Batch 44/64 loss: -0.02282392978668213
Batch 45/64 loss: 0.006275177001953125
Batch 46/64 loss: 0.01625502109527588
Batch 47/64 loss: 0.008156657218933105
Batch 48/64 loss: 0.006879746913909912
Batch 49/64 loss: -0.018361330032348633
Batch 50/64 loss: 0.01319277286529541
Batch 51/64 loss: 0.031131982803344727
Batch 52/64 loss: -0.017019331455230713
Batch 53/64 loss: -0.004216432571411133
Batch 54/64 loss: -0.007540702819824219
Batch 55/64 loss: 0.015021562576293945
Batch 56/64 loss: -0.014468908309936523
Batch 57/64 loss: -0.003447294235229492
Batch 58/64 loss: -0.010104715824127197
Batch 59/64 loss: -0.0030891895294189453
Batch 60/64 loss: 0.03588056564331055
Batch 61/64 loss: 0.02915889024734497
Batch 62/64 loss: 0.020597517490386963
Batch 63/64 loss: 0.0030291080474853516
Batch 64/64 loss: 0.009601473808288574
Epoch 457  Train loss: 0.006653365434384813  Val loss: 0.12057016006450064
Epoch 458
-------------------------------
Batch 1/64 loss: 0.009677648544311523
Batch 2/64 loss: -0.008736789226531982
Batch 3/64 loss: -0.00940006971359253
Batch 4/64 loss: -0.00600820779800415
Batch 5/64 loss: 0.04999345541000366
Batch 6/64 loss: -0.0019251108169555664
Batch 7/64 loss: 0.005188643932342529
Batch 8/64 loss: 0.002747476100921631
Batch 9/64 loss: 0.0006802082061767578
Batch 10/64 loss: -0.0014248490333557129
Batch 11/64 loss: 0.014721333980560303
Batch 12/64 loss: -0.01316612958908081
Batch 13/64 loss: 0.031130194664001465
Batch 14/64 loss: 0.0023268461227416992
Batch 15/64 loss: 0.013694286346435547
Batch 16/64 loss: 0.003435969352722168
Batch 17/64 loss: 0.01806098222732544
Batch 18/64 loss: 0.01033550500869751
Batch 19/64 loss: -0.002006828784942627
Batch 20/64 loss: 0.01049816608428955
Batch 21/64 loss: 0.03523111343383789
Batch 22/64 loss: 0.04190176725387573
Batch 23/64 loss: 0.013052701950073242
Batch 24/64 loss: 0.009759187698364258
Batch 25/64 loss: 0.049171388149261475
Batch 26/64 loss: -0.004940330982208252
Batch 27/64 loss: 0.009094715118408203
Batch 28/64 loss: 0.06528383493423462
Batch 29/64 loss: -0.001359403133392334
Batch 30/64 loss: 0.025244474411010742
Batch 31/64 loss: 0.0016301870346069336
Batch 32/64 loss: -0.004542052745819092
Batch 33/64 loss: 0.004233896732330322
Batch 34/64 loss: -0.008762180805206299
Batch 35/64 loss: -0.005151510238647461
Batch 36/64 loss: -0.008895039558410645
Batch 37/64 loss: -0.01839756965637207
Batch 38/64 loss: 0.008273839950561523
Batch 39/64 loss: 0.015623331069946289
Batch 40/64 loss: 0.02973353862762451
Batch 41/64 loss: -0.03466451168060303
Batch 42/64 loss: 0.02314174175262451
Batch 43/64 loss: 0.005695164203643799
Batch 44/64 loss: -0.004275202751159668
Batch 45/64 loss: 0.01695072650909424
Batch 46/64 loss: -0.022902071475982666
Batch 47/64 loss: 0.03534865379333496
Batch 48/64 loss: 0.009379386901855469
Batch 49/64 loss: 0.005585432052612305
Batch 50/64 loss: 0.005837798118591309
Batch 51/64 loss: 0.015693187713623047
Batch 52/64 loss: 0.02736109495162964
Batch 53/64 loss: -0.009133458137512207
Batch 54/64 loss: 0.0010738968849182129
Batch 55/64 loss: -0.0039376020431518555
Batch 56/64 loss: -0.010798096656799316
Batch 57/64 loss: -0.01027148962020874
Batch 58/64 loss: 0.02330988645553589
Batch 59/64 loss: -0.0006527304649353027
Batch 60/64 loss: -0.01147550344467163
Batch 61/64 loss: 0.019158780574798584
Batch 62/64 loss: -0.016883671283721924
Batch 63/64 loss: 0.035129427909851074
Batch 64/64 loss: -0.0006479620933532715
Epoch 458  Train loss: 0.007595191983615651  Val loss: 0.12091143311503827
Epoch 459
-------------------------------
Batch 1/64 loss: -0.006665289402008057
Batch 2/64 loss: 0.011470139026641846
Batch 3/64 loss: 0.03941011428833008
Batch 4/64 loss: -0.008579730987548828
Batch 5/64 loss: 0.007947385311126709
Batch 6/64 loss: 0.02154242992401123
Batch 7/64 loss: 0.00968468189239502
Batch 8/64 loss: 0.005252063274383545
Batch 9/64 loss: 0.026528477668762207
Batch 10/64 loss: -0.006794571876525879
Batch 11/64 loss: 0.014370560646057129
Batch 12/64 loss: -0.0010794401168823242
Batch 13/64 loss: 0.004866957664489746
Batch 14/64 loss: 0.022104263305664062
Batch 15/64 loss: -0.005227982997894287
Batch 16/64 loss: -0.009467720985412598
Batch 17/64 loss: 0.026713132858276367
Batch 18/64 loss: 0.0007499456405639648
Batch 19/64 loss: 0.02204054594039917
Batch 20/64 loss: -0.011371493339538574
Batch 21/64 loss: 0.0009472966194152832
Batch 22/64 loss: -0.007450878620147705
Batch 23/64 loss: 0.018522262573242188
Batch 24/64 loss: -0.0036238431930541992
Batch 25/64 loss: 0.021941721439361572
Batch 26/64 loss: -0.006020486354827881
Batch 27/64 loss: 0.005982756614685059
Batch 28/64 loss: 0.00010991096496582031
Batch 29/64 loss: 0.011836409568786621
Batch 30/64 loss: 0.00390017032623291
Batch 31/64 loss: 0.02460634708404541
Batch 32/64 loss: 0.015050828456878662
Batch 33/64 loss: 0.0017651915550231934
Batch 34/64 loss: -0.01654452085494995
Batch 35/64 loss: 0.020699620246887207
Batch 36/64 loss: 0.0203169584274292
Batch 37/64 loss: -0.0029087066650390625
Batch 38/64 loss: 0.013051867485046387
Batch 39/64 loss: -0.024560391902923584
Batch 40/64 loss: -0.00304257869720459
Batch 41/64 loss: 0.013309895992279053
Batch 42/64 loss: -0.01873040199279785
Batch 43/64 loss: -0.0019698739051818848
Batch 44/64 loss: -0.02928006649017334
Batch 45/64 loss: 0.03646397590637207
Batch 46/64 loss: 0.01049119234085083
Batch 47/64 loss: 0.04088246822357178
Batch 48/64 loss: -0.022080957889556885
Batch 49/64 loss: -0.009029507637023926
Batch 50/64 loss: -0.031063854694366455
Batch 51/64 loss: -0.012056469917297363
Batch 52/64 loss: 0.00863802433013916
Batch 53/64 loss: -0.0168914794921875
Batch 54/64 loss: 0.025753378868103027
Batch 55/64 loss: 0.012201130390167236
Batch 56/64 loss: -0.018896877765655518
Batch 57/64 loss: 0.03791600465774536
Batch 58/64 loss: -0.002959609031677246
Batch 59/64 loss: -0.005285799503326416
Batch 60/64 loss: 0.00693511962890625
Batch 61/64 loss: 0.0060936808586120605
Batch 62/64 loss: 0.022772371768951416
Batch 63/64 loss: -0.022511720657348633
Batch 64/64 loss: 0.042375385761260986
Epoch 459  Train loss: 0.0050283382920657885  Val loss: 0.11846771293489385
Epoch 460
-------------------------------
Batch 1/64 loss: 0.003949224948883057
Batch 2/64 loss: -0.003518342971801758
Batch 3/64 loss: -0.01455777883529663
Batch 4/64 loss: -0.012899816036224365
Batch 5/64 loss: -0.006139516830444336
Batch 6/64 loss: -0.005186259746551514
Batch 7/64 loss: -0.00520855188369751
Batch 8/64 loss: 0.0011316537857055664
Batch 9/64 loss: 0.012966454029083252
Batch 10/64 loss: 0.016136527061462402
Batch 11/64 loss: 0.025578677654266357
Batch 12/64 loss: -0.014438092708587646
Batch 13/64 loss: -0.011789858341217041
Batch 14/64 loss: 0.009293556213378906
Batch 15/64 loss: -0.027539730072021484
Batch 16/64 loss: -0.013253986835479736
Batch 17/64 loss: -0.01689964532852173
Batch 18/64 loss: -0.0001239776611328125
Batch 19/64 loss: 0.003006458282470703
Batch 20/64 loss: 0.004886984825134277
Batch 21/64 loss: -0.029482901096343994
Batch 22/64 loss: -0.02588176727294922
Batch 23/64 loss: -0.00896376371383667
Batch 24/64 loss: 0.013264000415802002
Batch 25/64 loss: -0.001109004020690918
Batch 26/64 loss: 0.016289830207824707
Batch 27/64 loss: 0.02747666835784912
Batch 28/64 loss: 0.012391865253448486
Batch 29/64 loss: 0.031423091888427734
Batch 30/64 loss: 0.0023539066314697266
Batch 31/64 loss: 0.0022551417350769043
Batch 32/64 loss: 0.010884881019592285
Batch 33/64 loss: 0.01050633192062378
Batch 34/64 loss: 0.004109501838684082
Batch 35/64 loss: 0.002055943012237549
Batch 36/64 loss: 0.0004534125328063965
Batch 37/64 loss: -0.007531940937042236
Batch 38/64 loss: 0.03909802436828613
Batch 39/64 loss: 0.011478185653686523
Batch 40/64 loss: -0.007491648197174072
Batch 41/64 loss: -0.0004171133041381836
Batch 42/64 loss: 0.020630955696105957
Batch 43/64 loss: 0.006283760070800781
Batch 44/64 loss: 0.0017609000205993652
Batch 45/64 loss: 0.02610039710998535
Batch 46/64 loss: 0.04560178518295288
Batch 47/64 loss: -0.011624336242675781
Batch 48/64 loss: -0.0038511157035827637
Batch 49/64 loss: -0.011010050773620605
Batch 50/64 loss: 0.010706722736358643
Batch 51/64 loss: -0.005237281322479248
Batch 52/64 loss: 0.0022978782653808594
Batch 53/64 loss: 0.00985175371170044
Batch 54/64 loss: 0.007010281085968018
Batch 55/64 loss: 0.04208242893218994
Batch 56/64 loss: 0.04897356033325195
Batch 57/64 loss: 0.007839560508728027
Batch 58/64 loss: 0.029873967170715332
Batch 59/64 loss: -0.025738954544067383
Batch 60/64 loss: 0.004145026206970215
Batch 61/64 loss: -0.01852273941040039
Batch 62/64 loss: 0.023335695266723633
Batch 63/64 loss: 0.028985679149627686
Batch 64/64 loss: 0.03821873664855957
Epoch 460  Train loss: 0.004968102773030599  Val loss: 0.11968679321590568
Epoch 461
-------------------------------
Batch 1/64 loss: -0.0213811993598938
Batch 2/64 loss: 0.007022202014923096
Batch 3/64 loss: -0.023281991481781006
Batch 4/64 loss: 0.0214996337890625
Batch 5/64 loss: 0.014521300792694092
Batch 6/64 loss: -0.0057220458984375
Batch 7/64 loss: -0.004122316837310791
Batch 8/64 loss: 0.005248546600341797
Batch 9/64 loss: 0.008751153945922852
Batch 10/64 loss: 0.006444036960601807
Batch 11/64 loss: 0.02209681272506714
Batch 12/64 loss: 0.028594553470611572
Batch 13/64 loss: 0.015637516975402832
Batch 14/64 loss: -0.006089687347412109
Batch 15/64 loss: -0.008836746215820312
Batch 16/64 loss: -0.000555872917175293
Batch 17/64 loss: 0.007980585098266602
Batch 18/64 loss: -0.009434401988983154
Batch 19/64 loss: 0.0026494860649108887
Batch 20/64 loss: 0.01254570484161377
Batch 21/64 loss: -0.016234993934631348
Batch 22/64 loss: 0.02237701416015625
Batch 23/64 loss: 0.026247680187225342
Batch 24/64 loss: -0.005672812461853027
Batch 25/64 loss: 0.006898760795593262
Batch 26/64 loss: -0.006406545639038086
Batch 27/64 loss: 0.008238255977630615
Batch 28/64 loss: -0.004740118980407715
Batch 29/64 loss: 0.005095958709716797
Batch 30/64 loss: -0.018604576587677002
Batch 31/64 loss: 0.022885799407958984
Batch 32/64 loss: 0.03228408098220825
Batch 33/64 loss: 0.0023849010467529297
Batch 34/64 loss: 0.008102595806121826
Batch 35/64 loss: 0.020554065704345703
Batch 36/64 loss: -0.021912753582000732
Batch 37/64 loss: 0.010917127132415771
Batch 38/64 loss: -0.014805734157562256
Batch 39/64 loss: 0.058991193771362305
Batch 40/64 loss: 0.016300976276397705
Batch 41/64 loss: -0.017050445079803467
Batch 42/64 loss: 0.004587829113006592
Batch 43/64 loss: 0.017248868942260742
Batch 44/64 loss: -0.011633038520812988
Batch 45/64 loss: 0.01585984230041504
Batch 46/64 loss: -0.012830078601837158
Batch 47/64 loss: -0.03322255611419678
Batch 48/64 loss: -0.0022736787796020508
Batch 49/64 loss: 0.02882927656173706
Batch 50/64 loss: 0.010561168193817139
Batch 51/64 loss: -0.013043999671936035
Batch 52/64 loss: 0.026180624961853027
Batch 53/64 loss: 0.010707557201385498
Batch 54/64 loss: 8.153915405273438e-05
Batch 55/64 loss: 0.03369849920272827
Batch 56/64 loss: 0.0030902624130249023
Batch 57/64 loss: 0.004200756549835205
Batch 58/64 loss: -0.0021021366119384766
Batch 59/64 loss: 0.019888699054718018
Batch 60/64 loss: 0.028355121612548828
Batch 61/64 loss: -0.006451129913330078
Batch 62/64 loss: 0.003268003463745117
Batch 63/64 loss: 0.004389524459838867
Batch 64/64 loss: 0.002259552478790283
Epoch 461  Train loss: 0.005341228550555659  Val loss: 0.1191203383235997
Epoch 462
-------------------------------
Batch 1/64 loss: -0.007528960704803467
Batch 2/64 loss: -0.017959773540496826
Batch 3/64 loss: 0.013431787490844727
Batch 4/64 loss: 0.0023925304412841797
Batch 5/64 loss: 0.010418117046356201
Batch 6/64 loss: 0.024576306343078613
Batch 7/64 loss: 0.013238906860351562
Batch 8/64 loss: -0.03053528070449829
Batch 9/64 loss: -0.018400251865386963
Batch 10/64 loss: -0.015224218368530273
Batch 11/64 loss: 0.028201818466186523
Batch 12/64 loss: 0.021166443824768066
Batch 13/64 loss: -0.0119856595993042
Batch 14/64 loss: -0.007627248764038086
Batch 15/64 loss: 0.026640117168426514
Batch 16/64 loss: 0.00686335563659668
Batch 17/64 loss: -0.0035033822059631348
Batch 18/64 loss: -0.0046686530113220215
Batch 19/64 loss: 0.017923831939697266
Batch 20/64 loss: 0.0339619517326355
Batch 21/64 loss: 0.010743021965026855
Batch 22/64 loss: 0.004116535186767578
Batch 23/64 loss: 0.023330092430114746
Batch 24/64 loss: -0.007655918598175049
Batch 25/64 loss: -0.0029764175415039062
Batch 26/64 loss: 0.01886463165283203
Batch 27/64 loss: -0.005779445171356201
Batch 28/64 loss: 0.007366001605987549
Batch 29/64 loss: 0.004148960113525391
Batch 30/64 loss: 0.011849284172058105
Batch 31/64 loss: 0.006697595119476318
Batch 32/64 loss: 0.028545916080474854
Batch 33/64 loss: -0.024802803993225098
Batch 34/64 loss: 0.02691704034805298
Batch 35/64 loss: -0.013790428638458252
Batch 36/64 loss: -0.008410811424255371
Batch 37/64 loss: 0.0078352689743042
Batch 38/64 loss: 0.006446242332458496
Batch 39/64 loss: 0.01955103874206543
Batch 40/64 loss: -0.02096468210220337
Batch 41/64 loss: -0.01418602466583252
Batch 42/64 loss: 0.008778393268585205
Batch 43/64 loss: 0.020106613636016846
Batch 44/64 loss: -0.012241244316101074
Batch 45/64 loss: 0.026174604892730713
Batch 46/64 loss: 0.023130476474761963
Batch 47/64 loss: 0.012378871440887451
Batch 48/64 loss: 0.017796754837036133
Batch 49/64 loss: -0.0006708502769470215
Batch 50/64 loss: 0.013831615447998047
Batch 51/64 loss: -0.02398890256881714
Batch 52/64 loss: -0.013568639755249023
Batch 53/64 loss: -0.01591193675994873
Batch 54/64 loss: -0.03373408317565918
Batch 55/64 loss: 0.024528324604034424
Batch 56/64 loss: 0.003173649311065674
Batch 57/64 loss: 0.00839245319366455
Batch 58/64 loss: 0.022089719772338867
Batch 59/64 loss: 0.014331400394439697
Batch 60/64 loss: -0.0062291622161865234
Batch 61/64 loss: 0.005246579647064209
Batch 62/64 loss: -0.001910090446472168
Batch 63/64 loss: 0.013610422611236572
Batch 64/64 loss: -0.0008415579795837402
Epoch 462  Train loss: 0.004139774687149945  Val loss: 0.12077862495409254
Epoch 463
-------------------------------
Batch 1/64 loss: 0.0033402442932128906
Batch 2/64 loss: 0.03532993793487549
Batch 3/64 loss: -0.006547331809997559
Batch 4/64 loss: 0.01870715618133545
Batch 5/64 loss: -0.0034232735633850098
Batch 6/64 loss: 0.0019936561584472656
Batch 7/64 loss: 0.014393210411071777
Batch 8/64 loss: 0.012958407402038574
Batch 9/64 loss: -0.013064384460449219
Batch 10/64 loss: -0.007476329803466797
Batch 11/64 loss: -0.006763756275177002
Batch 12/64 loss: -0.005739927291870117
Batch 13/64 loss: 0.01937013864517212
Batch 14/64 loss: -0.007631063461303711
Batch 15/64 loss: -0.009593188762664795
Batch 16/64 loss: -0.016589641571044922
Batch 17/64 loss: 0.0010007619857788086
Batch 18/64 loss: 0.014236927032470703
Batch 19/64 loss: -0.005709528923034668
Batch 20/64 loss: 0.014612078666687012
Batch 21/64 loss: 0.01363593339920044
Batch 22/64 loss: -4.9054622650146484e-05
Batch 23/64 loss: -0.020138263702392578
Batch 24/64 loss: 0.012851953506469727
Batch 25/64 loss: 0.0060836076736450195
Batch 26/64 loss: 0.006736576557159424
Batch 27/64 loss: 0.00345611572265625
Batch 28/64 loss: 0.0145111083984375
Batch 29/64 loss: 0.0011432766914367676
Batch 30/64 loss: 0.018087446689605713
Batch 31/64 loss: 0.022025346755981445
Batch 32/64 loss: 0.00013303756713867188
Batch 33/64 loss: -0.0010706782341003418
Batch 34/64 loss: 0.024557173252105713
Batch 35/64 loss: 0.027138829231262207
Batch 36/64 loss: -0.004018545150756836
Batch 37/64 loss: 0.00450819730758667
Batch 38/64 loss: -0.018644511699676514
Batch 39/64 loss: -0.011645317077636719
Batch 40/64 loss: -0.006248176097869873
Batch 41/64 loss: -0.020160794258117676
Batch 42/64 loss: -0.011325418949127197
Batch 43/64 loss: 0.006193280220031738
Batch 44/64 loss: -0.0010142326354980469
Batch 45/64 loss: -0.009137630462646484
Batch 46/64 loss: -0.023771047592163086
Batch 47/64 loss: 0.001348733901977539
Batch 48/64 loss: 0.01775527000427246
Batch 49/64 loss: 0.009563922882080078
Batch 50/64 loss: 0.031157374382019043
Batch 51/64 loss: 0.02626490592956543
Batch 52/64 loss: 0.03282606601715088
Batch 53/64 loss: 0.04981184005737305
Batch 54/64 loss: -0.004907846450805664
Batch 55/64 loss: -0.003666222095489502
Batch 56/64 loss: -0.008356571197509766
Batch 57/64 loss: 0.007322192192077637
Batch 58/64 loss: 0.0110434889793396
Batch 59/64 loss: -0.0054929256439208984
Batch 60/64 loss: -0.01401209831237793
Batch 61/64 loss: 0.004370987415313721
Batch 62/64 loss: 0.01260840892791748
Batch 63/64 loss: 0.004965722560882568
Batch 64/64 loss: 0.004600644111633301
Epoch 463  Train loss: 0.004130133928037157  Val loss: 0.12075735633725562
Epoch 464
-------------------------------
Batch 1/64 loss: 0.003644227981567383
Batch 2/64 loss: -0.032824158668518066
Batch 3/64 loss: -0.012034058570861816
Batch 4/64 loss: -0.007475852966308594
Batch 5/64 loss: 0.029229044914245605
Batch 6/64 loss: 0.012391090393066406
Batch 7/64 loss: 0.008445024490356445
Batch 8/64 loss: -0.0077187418937683105
Batch 9/64 loss: 0.008509039878845215
Batch 10/64 loss: 0.008666455745697021
Batch 11/64 loss: 0.01856362819671631
Batch 12/64 loss: -0.0006103515625
Batch 13/64 loss: 0.02358722686767578
Batch 14/64 loss: -0.00954979658126831
Batch 15/64 loss: 0.004202842712402344
Batch 16/64 loss: 0.02025449275970459
Batch 17/64 loss: -0.026420950889587402
Batch 18/64 loss: -0.0012217164039611816
Batch 19/64 loss: -0.01314854621887207
Batch 20/64 loss: -0.02488255500793457
Batch 21/64 loss: 0.03137081861495972
Batch 22/64 loss: 0.022300243377685547
Batch 23/64 loss: 0.008732974529266357
Batch 24/64 loss: -0.0010281801223754883
Batch 25/64 loss: 0.01117241382598877
Batch 26/64 loss: 0.051747918128967285
Batch 27/64 loss: -0.020451843738555908
Batch 28/64 loss: 0.009131193161010742
Batch 29/64 loss: 0.02039945125579834
Batch 30/64 loss: 0.019414067268371582
Batch 31/64 loss: 0.004218578338623047
Batch 32/64 loss: 0.023626327514648438
Batch 33/64 loss: 0.02323448657989502
Batch 34/64 loss: -0.010883986949920654
Batch 35/64 loss: 0.009910404682159424
Batch 36/64 loss: -0.0008777379989624023
Batch 37/64 loss: 0.007470190525054932
Batch 38/64 loss: -0.006018519401550293
Batch 39/64 loss: -0.007395684719085693
Batch 40/64 loss: 0.027564585208892822
Batch 41/64 loss: -0.008540689945220947
Batch 42/64 loss: 0.013290107250213623
Batch 43/64 loss: 0.018131792545318604
Batch 44/64 loss: -0.0031291842460632324
Batch 45/64 loss: 0.048673391342163086
Batch 46/64 loss: -0.015201807022094727
Batch 47/64 loss: 0.021867454051971436
Batch 48/64 loss: 0.01584458351135254
Batch 49/64 loss: -0.010206520557403564
Batch 50/64 loss: -0.02191627025604248
Batch 51/64 loss: -0.016573548316955566
Batch 52/64 loss: -0.008521854877471924
Batch 53/64 loss: 0.0172693133354187
Batch 54/64 loss: 0.004627883434295654
Batch 55/64 loss: 0.024926483631134033
Batch 56/64 loss: -0.0033459067344665527
Batch 57/64 loss: -0.007882535457611084
Batch 58/64 loss: 0.0123215913772583
Batch 59/64 loss: -0.00830233097076416
Batch 60/64 loss: 0.0014838576316833496
Batch 61/64 loss: -0.007584512233734131
Batch 62/64 loss: 0.0030034780502319336
Batch 63/64 loss: 0.006550192832946777
Batch 64/64 loss: -0.017203867435455322
Epoch 464  Train loss: 0.004535311577366847  Val loss: 0.12304796447458956
Epoch 465
-------------------------------
Batch 1/64 loss: -0.011518120765686035
Batch 2/64 loss: 0.029434382915496826
Batch 3/64 loss: 0.016095280647277832
Batch 4/64 loss: 0.026045560836791992
Batch 5/64 loss: 0.02297884225845337
Batch 6/64 loss: 0.011975586414337158
Batch 7/64 loss: 0.013708233833312988
Batch 8/64 loss: 0.02319997549057007
Batch 9/64 loss: 0.026074647903442383
Batch 10/64 loss: -0.006658375263214111
Batch 11/64 loss: -0.008713960647583008
Batch 12/64 loss: -0.011875569820404053
Batch 13/64 loss: -0.014851570129394531
Batch 14/64 loss: -0.015601396560668945
Batch 15/64 loss: 0.022060871124267578
Batch 16/64 loss: -0.002491295337677002
Batch 17/64 loss: 0.03295135498046875
Batch 18/64 loss: 0.02044677734375
Batch 19/64 loss: 0.005284667015075684
Batch 20/64 loss: -0.028162121772766113
Batch 21/64 loss: -0.007416129112243652
Batch 22/64 loss: -0.017772912979125977
Batch 23/64 loss: 0.03133898973464966
Batch 24/64 loss: -0.012421667575836182
Batch 25/64 loss: -0.005748748779296875
Batch 26/64 loss: -0.010604381561279297
Batch 27/64 loss: 0.00860506296157837
Batch 28/64 loss: 0.018289268016815186
Batch 29/64 loss: -0.016425669193267822
Batch 30/64 loss: -0.020365118980407715
Batch 31/64 loss: 0.016170978546142578
Batch 32/64 loss: 0.0014978647232055664
Batch 33/64 loss: 0.0069838762283325195
Batch 34/64 loss: -0.010589241981506348
Batch 35/64 loss: -0.0016021728515625
Batch 36/64 loss: -0.006797313690185547
Batch 37/64 loss: 0.018973231315612793
Batch 38/64 loss: -0.008530497550964355
Batch 39/64 loss: -0.0030075907707214355
Batch 40/64 loss: 0.007124662399291992
Batch 41/64 loss: 0.0013642311096191406
Batch 42/64 loss: -0.01443624496459961
Batch 43/64 loss: 0.026762425899505615
Batch 44/64 loss: -0.009913921356201172
Batch 45/64 loss: 0.024769604206085205
Batch 46/64 loss: 0.01914161443710327
Batch 47/64 loss: 0.004754245281219482
Batch 48/64 loss: 0.025245249271392822
Batch 49/64 loss: 0.0011823177337646484
Batch 50/64 loss: 0.002228379249572754
Batch 51/64 loss: -0.01962977647781372
Batch 52/64 loss: 0.03916805982589722
Batch 53/64 loss: -0.00142747163772583
Batch 54/64 loss: -0.006561696529388428
Batch 55/64 loss: 0.002323150634765625
Batch 56/64 loss: -0.010996222496032715
Batch 57/64 loss: 0.012820541858673096
Batch 58/64 loss: -0.02144038677215576
Batch 59/64 loss: -0.0027406811714172363
Batch 60/64 loss: -0.015294969081878662
Batch 61/64 loss: 0.012289762496948242
Batch 62/64 loss: -0.00021791458129882812
Batch 63/64 loss: 0.027016878128051758
Batch 64/64 loss: -0.027445495128631592
Epoch 465  Train loss: 0.003355440205218745  Val loss: 0.12062597356711056
Epoch 466
-------------------------------
Batch 1/64 loss: 0.007818281650543213
Batch 2/64 loss: 0.020454823970794678
Batch 3/64 loss: -0.017108798027038574
Batch 4/64 loss: -0.012331068515777588
Batch 5/64 loss: -0.008017897605895996
Batch 6/64 loss: 0.02834343910217285
Batch 7/64 loss: -0.002844572067260742
Batch 8/64 loss: 0.012650251388549805
Batch 9/64 loss: -0.003573298454284668
Batch 10/64 loss: -0.008558571338653564
Batch 11/64 loss: -0.011988699436187744
Batch 12/64 loss: 0.020927608013153076
Batch 13/64 loss: -0.021773815155029297
Batch 14/64 loss: 0.0009505152702331543
Batch 15/64 loss: 0.005075693130493164
Batch 16/64 loss: 0.004658520221710205
Batch 17/64 loss: 0.03321868181228638
Batch 18/64 loss: -0.020489811897277832
Batch 19/64 loss: 0.03905463218688965
Batch 20/64 loss: 0.001237034797668457
Batch 21/64 loss: -1.6510486602783203e-05
Batch 22/64 loss: -0.001608431339263916
Batch 23/64 loss: 0.006447434425354004
Batch 24/64 loss: -0.02071458101272583
Batch 25/64 loss: 0.013552844524383545
Batch 26/64 loss: 0.030972421169281006
Batch 27/64 loss: 0.007165968418121338
Batch 28/64 loss: -0.015352487564086914
Batch 29/64 loss: -0.013738036155700684
Batch 30/64 loss: -0.023321926593780518
Batch 31/64 loss: -0.012098908424377441
Batch 32/64 loss: 0.027663886547088623
Batch 33/64 loss: -0.014262557029724121
Batch 34/64 loss: 0.014776110649108887
Batch 35/64 loss: 0.0026528239250183105
Batch 36/64 loss: 0.020579814910888672
Batch 37/64 loss: 0.019343018531799316
Batch 38/64 loss: -0.008281111717224121
Batch 39/64 loss: -0.0021792054176330566
Batch 40/64 loss: 0.015758275985717773
Batch 41/64 loss: 0.004840970039367676
Batch 42/64 loss: 0.025996506214141846
Batch 43/64 loss: -0.020987629890441895
Batch 44/64 loss: 0.0016741752624511719
Batch 45/64 loss: -0.013779163360595703
Batch 46/64 loss: -0.00236356258392334
Batch 47/64 loss: 0.004301548004150391
Batch 48/64 loss: 0.00859057903289795
Batch 49/64 loss: -0.0030018091201782227
Batch 50/64 loss: 0.004069805145263672
Batch 51/64 loss: -0.0032929182052612305
Batch 52/64 loss: 0.011647999286651611
Batch 53/64 loss: 0.030864059925079346
Batch 54/64 loss: 0.01820659637451172
Batch 55/64 loss: 0.00957256555557251
Batch 56/64 loss: 0.005113661289215088
Batch 57/64 loss: -0.00031960010528564453
Batch 58/64 loss: -0.015346348285675049
Batch 59/64 loss: 0.014786839485168457
Batch 60/64 loss: 0.00579380989074707
Batch 61/64 loss: 0.004388749599456787
Batch 62/64 loss: 0.01776278018951416
Batch 63/64 loss: 0.004771530628204346
Batch 64/64 loss: 0.02577197551727295
Epoch 466  Train loss: 0.00388489283767401  Val loss: 0.1241420128501158
Epoch 467
-------------------------------
Batch 1/64 loss: 0.024985313415527344
Batch 2/64 loss: 0.0030798912048339844
Batch 3/64 loss: -0.0022487640380859375
Batch 4/64 loss: 0.016900718212127686
Batch 5/64 loss: 0.01455456018447876
Batch 6/64 loss: 0.002446472644805908
Batch 7/64 loss: 0.016215026378631592
Batch 8/64 loss: 0.014128804206848145
Batch 9/64 loss: -0.032768428325653076
Batch 10/64 loss: -0.013966262340545654
Batch 11/64 loss: -0.011705636978149414
Batch 12/64 loss: 0.0015410780906677246
Batch 13/64 loss: -0.033073365688323975
Batch 14/64 loss: -0.026439011096954346
Batch 15/64 loss: -0.02609241008758545
Batch 16/64 loss: 0.020479321479797363
Batch 17/64 loss: 0.006538093090057373
Batch 18/64 loss: 0.011645793914794922
Batch 19/64 loss: -0.009198784828186035
Batch 20/64 loss: 0.0261346697807312
Batch 21/64 loss: 0.01085662841796875
Batch 22/64 loss: 0.012588918209075928
Batch 23/64 loss: -0.01294398307800293
Batch 24/64 loss: 0.023855984210968018
Batch 25/64 loss: -0.008168458938598633
Batch 26/64 loss: 0.004747748374938965
Batch 27/64 loss: 0.003876805305480957
Batch 28/64 loss: -0.01883101463317871
Batch 29/64 loss: 0.0016901493072509766
Batch 30/64 loss: 0.009705007076263428
Batch 31/64 loss: 0.0019621849060058594
Batch 32/64 loss: 0.0658564567565918
Batch 33/64 loss: -0.006506085395812988
Batch 34/64 loss: 0.004266500473022461
Batch 35/64 loss: 0.02010643482208252
Batch 36/64 loss: 0.01127767562866211
Batch 37/64 loss: 0.0016705989837646484
Batch 38/64 loss: -0.010856032371520996
Batch 39/64 loss: 0.03406268358230591
Batch 40/64 loss: -0.0009740591049194336
Batch 41/64 loss: 0.003336310386657715
Batch 42/64 loss: -0.004601180553436279
Batch 43/64 loss: 0.01967722177505493
Batch 44/64 loss: -0.012770354747772217
Batch 45/64 loss: -0.010008096694946289
Batch 46/64 loss: -0.005073070526123047
Batch 47/64 loss: 0.03234440088272095
Batch 48/64 loss: -0.014518558979034424
Batch 49/64 loss: 0.004864931106567383
Batch 50/64 loss: 0.008561432361602783
Batch 51/64 loss: -0.022939682006835938
Batch 52/64 loss: 0.019222557544708252
Batch 53/64 loss: 0.0332375168800354
Batch 54/64 loss: 0.01109844446182251
Batch 55/64 loss: -0.010785102844238281
Batch 56/64 loss: 0.018090665340423584
Batch 57/64 loss: -0.006822526454925537
Batch 58/64 loss: 0.02068430185317993
Batch 59/64 loss: 0.05173462629318237
Batch 60/64 loss: 0.03282588720321655
Batch 61/64 loss: 0.03760123252868652
Batch 62/64 loss: 0.01633632183074951
Batch 63/64 loss: 0.005156219005584717
Batch 64/64 loss: 0.005776047706604004
Epoch 467  Train loss: 0.006007635359670601  Val loss: 0.12273425949398185
Epoch 468
-------------------------------
Batch 1/64 loss: -0.01630192995071411
Batch 2/64 loss: 0.0032472610473632812
Batch 3/64 loss: 0.009952306747436523
Batch 4/64 loss: -0.003111600875854492
Batch 5/64 loss: 0.012286782264709473
Batch 6/64 loss: -0.004715561866760254
Batch 7/64 loss: -0.00782853364944458
Batch 8/64 loss: -0.0021498799324035645
Batch 9/64 loss: -0.044522106647491455
Batch 10/64 loss: 0.0008417367935180664
Batch 11/64 loss: 0.002240121364593506
Batch 12/64 loss: 0.012132585048675537
Batch 13/64 loss: 0.02230757474899292
Batch 14/64 loss: -0.014937996864318848
Batch 15/64 loss: -0.01664954423904419
Batch 16/64 loss: 0.020845651626586914
Batch 17/64 loss: -0.011741459369659424
Batch 18/64 loss: 0.013936638832092285
Batch 19/64 loss: 0.02156120538711548
Batch 20/64 loss: 0.01386481523513794
Batch 21/64 loss: -0.005409717559814453
Batch 22/64 loss: -0.0027422308921813965
Batch 23/64 loss: 0.014379322528839111
Batch 24/64 loss: -0.021929502487182617
Batch 25/64 loss: -0.0023946166038513184
Batch 26/64 loss: -0.019263923168182373
Batch 27/64 loss: 0.01120305061340332
Batch 28/64 loss: -0.011198461055755615
Batch 29/64 loss: -0.018485665321350098
Batch 30/64 loss: 0.015851974487304688
Batch 31/64 loss: -0.0169827938079834
Batch 32/64 loss: 0.003076910972595215
Batch 33/64 loss: -0.001638650894165039
Batch 34/64 loss: -0.010391950607299805
Batch 35/64 loss: 0.01998668909072876
Batch 36/64 loss: 0.014971733093261719
Batch 37/64 loss: 0.027399003505706787
Batch 38/64 loss: 0.01950913667678833
Batch 39/64 loss: -0.004472553730010986
Batch 40/64 loss: -0.010121405124664307
Batch 41/64 loss: 0.011880695819854736
Batch 42/64 loss: 0.031397223472595215
Batch 43/64 loss: 0.01184546947479248
Batch 44/64 loss: -0.011659026145935059
Batch 45/64 loss: 0.050092220306396484
Batch 46/64 loss: -0.002304375171661377
Batch 47/64 loss: -0.02642214298248291
Batch 48/64 loss: -0.01192706823348999
Batch 49/64 loss: 0.02981787919998169
Batch 50/64 loss: 0.01271730661392212
Batch 51/64 loss: -0.002970099449157715
Batch 52/64 loss: 0.033866167068481445
Batch 53/64 loss: 0.0022168755531311035
Batch 54/64 loss: 0.03221297264099121
Batch 55/64 loss: 0.014315366744995117
Batch 56/64 loss: -0.003561854362487793
Batch 57/64 loss: 0.005452871322631836
Batch 58/64 loss: 0.007837116718292236
Batch 59/64 loss: 0.021979331970214844
Batch 60/64 loss: 0.006209611892700195
Batch 61/64 loss: -0.012614011764526367
Batch 62/64 loss: 0.007949650287628174
Batch 63/64 loss: 0.004619598388671875
Batch 64/64 loss: -0.0015699267387390137
Epoch 468  Train loss: 0.0035196666624031816  Val loss: 0.11968888367983893
Epoch 469
-------------------------------
Batch 1/64 loss: 0.015049219131469727
Batch 2/64 loss: -0.003977298736572266
Batch 3/64 loss: -0.03138214349746704
Batch 4/64 loss: 0.026398897171020508
Batch 5/64 loss: 0.021849513053894043
Batch 6/64 loss: 0.01749563217163086
Batch 7/64 loss: 0.003929793834686279
Batch 8/64 loss: 0.0025818943977355957
Batch 9/64 loss: -0.008028686046600342
Batch 10/64 loss: -0.017747938632965088
Batch 11/64 loss: -0.02120530605316162
Batch 12/64 loss: -0.024809837341308594
Batch 13/64 loss: 3.993511199951172e-06
Batch 14/64 loss: 0.0222129225730896
Batch 15/64 loss: 0.03353714942932129
Batch 16/64 loss: 0.026872217655181885
Batch 17/64 loss: 0.018831849098205566
Batch 18/64 loss: 0.015408992767333984
Batch 19/64 loss: 0.034345149993896484
Batch 20/64 loss: 0.02296668291091919
Batch 21/64 loss: 0.0025333762168884277
Batch 22/64 loss: -0.011378765106201172
Batch 23/64 loss: -0.019593536853790283
Batch 24/64 loss: -0.017830312252044678
Batch 25/64 loss: -0.0017932653427124023
Batch 26/64 loss: -0.011456906795501709
Batch 27/64 loss: 0.02427208423614502
Batch 28/64 loss: -0.00012624263763427734
Batch 29/64 loss: 0.0023849010467529297
Batch 30/64 loss: 0.004959464073181152
Batch 31/64 loss: 0.0011135339736938477
Batch 32/64 loss: -0.003010094165802002
Batch 33/64 loss: -0.021962642669677734
Batch 34/64 loss: -0.008420944213867188
Batch 35/64 loss: 0.006738603115081787
Batch 36/64 loss: -0.008798778057098389
Batch 37/64 loss: 0.013169169425964355
Batch 38/64 loss: 0.013835251331329346
Batch 39/64 loss: 0.008440554141998291
Batch 40/64 loss: -0.000952601432800293
Batch 41/64 loss: 0.0004349350929260254
Batch 42/64 loss: -0.0265083909034729
Batch 43/64 loss: -0.0005118846893310547
Batch 44/64 loss: -0.005021333694458008
Batch 45/64 loss: -0.0030881762504577637
Batch 46/64 loss: 0.04909205436706543
Batch 47/64 loss: -0.013735413551330566
Batch 48/64 loss: 0.04306483268737793
Batch 49/64 loss: 0.00516432523727417
Batch 50/64 loss: -0.021051764488220215
Batch 51/64 loss: 0.010190606117248535
Batch 52/64 loss: -0.015594124794006348
Batch 53/64 loss: 0.024920642375946045
Batch 54/64 loss: 0.005696535110473633
Batch 55/64 loss: 0.002285599708557129
Batch 56/64 loss: 0.0024657249450683594
Batch 57/64 loss: 0.03732645511627197
Batch 58/64 loss: 0.006469666957855225
Batch 59/64 loss: -0.013901591300964355
Batch 60/64 loss: 0.001449286937713623
Batch 61/64 loss: 0.004354357719421387
Batch 62/64 loss: 0.01253819465637207
Batch 63/64 loss: -0.023066997528076172
Batch 64/64 loss: 0.01951974630355835
Epoch 469  Train loss: 0.0035148061958013795  Val loss: 0.1201470707290361
Epoch 470
-------------------------------
Batch 1/64 loss: -0.0056258440017700195
Batch 2/64 loss: -0.018978774547576904
Batch 3/64 loss: 0.014188945293426514
Batch 4/64 loss: -0.0094260573387146
Batch 5/64 loss: 0.0373692512512207
Batch 6/64 loss: 0.006727933883666992
Batch 7/64 loss: 0.007286190986633301
Batch 8/64 loss: -0.005790412425994873
Batch 9/64 loss: 0.013114213943481445
Batch 10/64 loss: -0.010538339614868164
Batch 11/64 loss: 0.02121204137802124
Batch 12/64 loss: -0.040023982524871826
Batch 13/64 loss: -0.022631287574768066
Batch 14/64 loss: 0.007187545299530029
Batch 15/64 loss: -0.020953714847564697
Batch 16/64 loss: -0.005414247512817383
Batch 17/64 loss: -0.0004410743713378906
Batch 18/64 loss: 0.02722257375717163
Batch 19/64 loss: -0.003551959991455078
Batch 20/64 loss: 0.018932878971099854
Batch 21/64 loss: -0.0030037760734558105
Batch 22/64 loss: 0.014863073825836182
Batch 23/64 loss: 0.04106539487838745
Batch 24/64 loss: -0.024731814861297607
Batch 25/64 loss: 0.0002086162567138672
Batch 26/64 loss: 0.031794190406799316
Batch 27/64 loss: -0.012828171253204346
Batch 28/64 loss: -0.002949059009552002
Batch 29/64 loss: -0.010682165622711182
Batch 30/64 loss: 0.007084250450134277
Batch 31/64 loss: -0.00698244571685791
Batch 32/64 loss: 0.009705066680908203
Batch 33/64 loss: 0.009043097496032715
Batch 34/64 loss: -0.002288520336151123
Batch 35/64 loss: 0.006773233413696289
Batch 36/64 loss: 0.0009233355522155762
Batch 37/64 loss: 0.03259611129760742
Batch 38/64 loss: 0.01255124807357788
Batch 39/64 loss: 0.008180379867553711
Batch 40/64 loss: -0.0062847137451171875
Batch 41/64 loss: -0.0015353560447692871
Batch 42/64 loss: 0.02267557382583618
Batch 43/64 loss: -0.0002518296241760254
Batch 44/64 loss: 0.0013248920440673828
Batch 45/64 loss: -0.003065168857574463
Batch 46/64 loss: 0.026882290840148926
Batch 47/64 loss: -0.015427827835083008
Batch 48/64 loss: -0.006653249263763428
Batch 49/64 loss: -0.024535179138183594
Batch 50/64 loss: -0.008981108665466309
Batch 51/64 loss: -0.006061673164367676
Batch 52/64 loss: -0.009416341781616211
Batch 53/64 loss: -0.0094451904296875
Batch 54/64 loss: 0.01633167266845703
Batch 55/64 loss: -0.006334424018859863
Batch 56/64 loss: 0.011514425277709961
Batch 57/64 loss: 0.007171928882598877
Batch 58/64 loss: 0.017821669578552246
Batch 59/64 loss: 0.010864317417144775
Batch 60/64 loss: -0.027300000190734863
Batch 61/64 loss: 0.013487517833709717
Batch 62/64 loss: 0.009918212890625
Batch 63/64 loss: 0.015877246856689453
Batch 64/64 loss: -0.012467563152313232
Epoch 470  Train loss: 0.00220258726793177  Val loss: 0.11946418748278798
Epoch 471
-------------------------------
Batch 1/64 loss: 0.0058519840240478516
Batch 2/64 loss: 0.0074928998947143555
Batch 3/64 loss: 0.004397094249725342
Batch 4/64 loss: -0.02004295587539673
Batch 5/64 loss: 0.01679927110671997
Batch 6/64 loss: 0.020249485969543457
Batch 7/64 loss: -0.030753731727600098
Batch 8/64 loss: -0.013902544975280762
Batch 9/64 loss: -0.007079064846038818
Batch 10/64 loss: 0.011748135089874268
Batch 11/64 loss: -7.331371307373047e-05
Batch 12/64 loss: 0.012321829795837402
Batch 13/64 loss: -0.020504355430603027
Batch 14/64 loss: -0.003096461296081543
Batch 15/64 loss: 0.0017336010932922363
Batch 16/64 loss: -0.021441876888275146
Batch 17/64 loss: 0.025127887725830078
Batch 18/64 loss: 0.006448149681091309
Batch 19/64 loss: 0.00854039192199707
Batch 20/64 loss: 0.022984087467193604
Batch 21/64 loss: 0.022461354732513428
Batch 22/64 loss: -0.008479893207550049
Batch 23/64 loss: 0.012309849262237549
Batch 24/64 loss: -0.011909186840057373
Batch 25/64 loss: 0.003667473793029785
Batch 26/64 loss: 0.01427692174911499
Batch 27/64 loss: 0.022340714931488037
Batch 28/64 loss: -0.028235197067260742
Batch 29/64 loss: -0.003000795841217041
Batch 30/64 loss: 0.001756429672241211
Batch 31/64 loss: -0.001363515853881836
Batch 32/64 loss: 0.04021191596984863
Batch 33/64 loss: 0.023887574672698975
Batch 34/64 loss: 0.003125429153442383
Batch 35/64 loss: 0.01362544298171997
Batch 36/64 loss: -0.009281396865844727
Batch 37/64 loss: -0.016593635082244873
Batch 38/64 loss: -0.019239366054534912
Batch 39/64 loss: -0.016105234622955322
Batch 40/64 loss: 0.020927011966705322
Batch 41/64 loss: -0.012571334838867188
Batch 42/64 loss: 0.015554726123809814
Batch 43/64 loss: -0.011952996253967285
Batch 44/64 loss: -0.017624974250793457
Batch 45/64 loss: 0.040325284004211426
Batch 46/64 loss: 0.00396573543548584
Batch 47/64 loss: -0.006799638271331787
Batch 48/64 loss: 0.0021275877952575684
Batch 49/64 loss: 0.02768230438232422
Batch 50/64 loss: 0.013020694255828857
Batch 51/64 loss: -0.010289192199707031
Batch 52/64 loss: 0.028714418411254883
Batch 53/64 loss: -0.005655884742736816
Batch 54/64 loss: -0.02041482925415039
Batch 55/64 loss: 0.00977623462677002
Batch 56/64 loss: 0.008298635482788086
Batch 57/64 loss: 0.02012038230895996
Batch 58/64 loss: 0.01503145694732666
Batch 59/64 loss: 0.019398272037506104
Batch 60/64 loss: 0.0021188855171203613
Batch 61/64 loss: 0.003396451473236084
Batch 62/64 loss: -0.00461733341217041
Batch 63/64 loss: 0.022834718227386475
Batch 64/64 loss: 0.02550530433654785
Epoch 471  Train loss: 0.003964721455293543  Val loss: 0.11956382002617486
Epoch 472
-------------------------------
Batch 1/64 loss: 0.013815581798553467
Batch 2/64 loss: 0.040126681327819824
Batch 3/64 loss: -0.020375311374664307
Batch 4/64 loss: -0.006907522678375244
Batch 5/64 loss: -0.007399916648864746
Batch 6/64 loss: -0.012223362922668457
Batch 7/64 loss: 0.00682830810546875
Batch 8/64 loss: 0.005978226661682129
Batch 9/64 loss: 8.165836334228516e-05
Batch 10/64 loss: -0.032609760761260986
Batch 11/64 loss: 0.0012701153755187988
Batch 12/64 loss: 0.010185718536376953
Batch 13/64 loss: -0.005212187767028809
Batch 14/64 loss: -0.012133955955505371
Batch 15/64 loss: -0.01660972833633423
Batch 16/64 loss: -0.00098419189453125
Batch 17/64 loss: 0.016996562480926514
Batch 18/64 loss: 0.012961983680725098
Batch 19/64 loss: -0.005719006061553955
Batch 20/64 loss: -0.0012569427490234375
Batch 21/64 loss: 0.022217392921447754
Batch 22/64 loss: -0.019887208938598633
Batch 23/64 loss: 0.023928701877593994
Batch 24/64 loss: 0.012903332710266113
Batch 25/64 loss: -0.0035424232482910156
Batch 26/64 loss: -0.002456843852996826
Batch 27/64 loss: 0.0019598007202148438
Batch 28/64 loss: -0.025326550006866455
Batch 29/64 loss: 0.02028632164001465
Batch 30/64 loss: -0.010174095630645752
Batch 31/64 loss: 0.0008793473243713379
Batch 32/64 loss: 0.013835251331329346
Batch 33/64 loss: -0.023437440395355225
Batch 34/64 loss: 0.005852222442626953
Batch 35/64 loss: -0.005053997039794922
Batch 36/64 loss: -0.0018002986907958984
Batch 37/64 loss: -0.004130959510803223
Batch 38/64 loss: -0.018647432327270508
Batch 39/64 loss: -0.014863729476928711
Batch 40/64 loss: 0.022248268127441406
Batch 41/64 loss: 0.003419339656829834
Batch 42/64 loss: -0.022558510303497314
Batch 43/64 loss: 0.004634559154510498
Batch 44/64 loss: 0.023916900157928467
Batch 45/64 loss: -0.014448761940002441
Batch 46/64 loss: 0.020150303840637207
Batch 47/64 loss: 0.0032407045364379883
Batch 48/64 loss: 0.021744132041931152
Batch 49/64 loss: -0.0014581084251403809
Batch 50/64 loss: -0.003407597541809082
Batch 51/64 loss: -6.633996963500977e-05
Batch 52/64 loss: 0.018164992332458496
Batch 53/64 loss: -0.010750174522399902
Batch 54/64 loss: 0.0004620552062988281
Batch 55/64 loss: 0.0031310319900512695
Batch 56/64 loss: -0.007189512252807617
Batch 57/64 loss: -0.006587862968444824
Batch 58/64 loss: 0.003631591796875
Batch 59/64 loss: 0.012213289737701416
Batch 60/64 loss: 0.007715344429016113
Batch 61/64 loss: 0.02426314353942871
Batch 62/64 loss: -0.017161011695861816
Batch 63/64 loss: 0.010161340236663818
Batch 64/64 loss: 0.03495067358016968
Epoch 472  Train loss: 0.0012711602098801557  Val loss: 0.11988447294202458
Epoch 473
-------------------------------
Batch 1/64 loss: 0.0025974512100219727
Batch 2/64 loss: -0.03356778621673584
Batch 3/64 loss: -0.006510555744171143
Batch 4/64 loss: -0.013587653636932373
Batch 5/64 loss: -0.00336301326751709
Batch 6/64 loss: -0.00896298885345459
Batch 7/64 loss: 0.015015065670013428
Batch 8/64 loss: 0.03641694784164429
Batch 9/64 loss: -0.012823104858398438
Batch 10/64 loss: -0.02125835418701172
Batch 11/64 loss: 0.006277859210968018
Batch 12/64 loss: 0.024535298347473145
Batch 13/64 loss: -0.012040793895721436
Batch 14/64 loss: 0.0024871826171875
Batch 15/64 loss: 0.017228007316589355
Batch 16/64 loss: 0.012563467025756836
Batch 17/64 loss: -0.024876892566680908
Batch 18/64 loss: -0.0017122626304626465
Batch 19/64 loss: 0.03098595142364502
Batch 20/64 loss: 3.3855438232421875e-05
Batch 21/64 loss: -0.01714235544204712
Batch 22/64 loss: 0.01566457748413086
Batch 23/64 loss: -0.0036357641220092773
Batch 24/64 loss: 0.016875267028808594
Batch 25/64 loss: 0.013336181640625
Batch 26/64 loss: 0.004725396633148193
Batch 27/64 loss: 0.0020644664764404297
Batch 28/64 loss: -0.0007014274597167969
Batch 29/64 loss: 0.02252495288848877
Batch 30/64 loss: 0.009574949741363525
Batch 31/64 loss: 0.012942373752593994
Batch 32/64 loss: 0.026155829429626465
Batch 33/64 loss: 0.014135420322418213
Batch 34/64 loss: -0.007522463798522949
Batch 35/64 loss: 0.03308820724487305
Batch 36/64 loss: 0.008709430694580078
Batch 37/64 loss: 0.05277281999588013
Batch 38/64 loss: -0.0075203776359558105
Batch 39/64 loss: 0.01856088638305664
Batch 40/64 loss: 0.022107243537902832
Batch 41/64 loss: -0.027665793895721436
Batch 42/64 loss: 0.041724205017089844
Batch 43/64 loss: 0.01072603464126587
Batch 44/64 loss: -0.012903094291687012
Batch 45/64 loss: 0.01536792516708374
Batch 46/64 loss: 0.013561427593231201
Batch 47/64 loss: 0.007659554481506348
Batch 48/64 loss: -0.0031664371490478516
Batch 49/64 loss: 0.018721818923950195
Batch 50/64 loss: -0.0059296488761901855
Batch 51/64 loss: 0.00779193639755249
Batch 52/64 loss: -0.014672636985778809
Batch 53/64 loss: 0.022889792919158936
Batch 54/64 loss: -0.020990431308746338
Batch 55/64 loss: -0.006401002407073975
Batch 56/64 loss: -0.014972448348999023
Batch 57/64 loss: 0.004691123962402344
Batch 58/64 loss: -0.02678292989730835
Batch 59/64 loss: -0.0005782842636108398
Batch 60/64 loss: -0.005607128143310547
Batch 61/64 loss: 0.02491861581802368
Batch 62/64 loss: -0.0024381279945373535
Batch 63/64 loss: -0.0077634453773498535
Batch 64/64 loss: 0.01901942491531372
Epoch 473  Train loss: 0.0043701786620944155  Val loss: 0.11924086895185648
Epoch 474
-------------------------------
Batch 1/64 loss: -0.017367303371429443
Batch 2/64 loss: -0.003813326358795166
Batch 3/64 loss: 0.016362369060516357
Batch 4/64 loss: -0.024250030517578125
Batch 5/64 loss: 0.0058632493019104
Batch 6/64 loss: -0.02665656805038452
Batch 7/64 loss: 0.02709364891052246
Batch 8/64 loss: -0.00401461124420166
Batch 9/64 loss: -0.017360687255859375
Batch 10/64 loss: 0.009523093700408936
Batch 11/64 loss: 0.03597128391265869
Batch 12/64 loss: -0.004585683345794678
Batch 13/64 loss: -0.018095731735229492
Batch 14/64 loss: -0.004330456256866455
Batch 15/64 loss: 0.026256442070007324
Batch 16/64 loss: 0.0022472143173217773
Batch 17/64 loss: 0.003475785255432129
Batch 18/64 loss: -0.003224313259124756
Batch 19/64 loss: 0.0266878604888916
Batch 20/64 loss: 0.014708518981933594
Batch 21/64 loss: -0.021151185035705566
Batch 22/64 loss: 0.0002638101577758789
Batch 23/64 loss: -0.010528087615966797
Batch 24/64 loss: 0.03544670343399048
Batch 25/64 loss: -0.01999795436859131
Batch 26/64 loss: 0.04250478744506836
Batch 27/64 loss: -0.011376559734344482
Batch 28/64 loss: 0.015644550323486328
Batch 29/64 loss: 0.02316594123840332
Batch 30/64 loss: 0.017298400402069092
Batch 31/64 loss: 0.006114184856414795
Batch 32/64 loss: 0.00013709068298339844
Batch 33/64 loss: 0.017236053943634033
Batch 34/64 loss: 0.023835957050323486
Batch 35/64 loss: 0.016293883323669434
Batch 36/64 loss: -0.04692572355270386
Batch 37/64 loss: -0.032630205154418945
Batch 38/64 loss: 0.010873377323150635
Batch 39/64 loss: -0.0025804638862609863
Batch 40/64 loss: 0.0036089420318603516
Batch 41/64 loss: -0.00644993782043457
Batch 42/64 loss: -0.00668412446975708
Batch 43/64 loss: 0.0066474080085754395
Batch 44/64 loss: -0.016418635845184326
Batch 45/64 loss: -0.007848799228668213
Batch 46/64 loss: 0.017083048820495605
Batch 47/64 loss: 0.01739192008972168
Batch 48/64 loss: 0.014507055282592773
Batch 49/64 loss: -0.0042035579681396484
Batch 50/64 loss: 0.02654176950454712
Batch 51/64 loss: 0.0009610652923583984
Batch 52/64 loss: 0.014629244804382324
Batch 53/64 loss: 0.026413321495056152
Batch 54/64 loss: -0.00040543079376220703
Batch 55/64 loss: -0.011091470718383789
Batch 56/64 loss: -0.005358636379241943
Batch 57/64 loss: 0.01280069351196289
Batch 58/64 loss: 0.012107789516448975
Batch 59/64 loss: -0.008998453617095947
Batch 60/64 loss: 0.004307091236114502
Batch 61/64 loss: -0.013041079044342041
Batch 62/64 loss: 0.015869736671447754
Batch 63/64 loss: -0.0023918747901916504
Batch 64/64 loss: 0.00706791877746582
Epoch 474  Train loss: 0.003190483766443589  Val loss: 0.12050531922337115
Epoch 475
-------------------------------
Batch 1/64 loss: -0.016563057899475098
Batch 2/64 loss: 0.01598680019378662
Batch 3/64 loss: 0.006330966949462891
Batch 4/64 loss: 0.02467411756515503
Batch 5/64 loss: -0.01881963014602661
Batch 6/64 loss: 0.021201610565185547
Batch 7/64 loss: 0.016674280166625977
Batch 8/64 loss: -0.004086434841156006
Batch 9/64 loss: -0.0021445155143737793
Batch 10/64 loss: -0.0005180835723876953
Batch 11/64 loss: 8.893013000488281e-05
Batch 12/64 loss: -0.008713960647583008
Batch 13/64 loss: 0.011358320713043213
Batch 14/64 loss: 0.02932053804397583
Batch 15/64 loss: 0.003792881965637207
Batch 16/64 loss: 0.011911213397979736
Batch 17/64 loss: 0.029386460781097412
Batch 18/64 loss: -0.004628956317901611
Batch 19/64 loss: 0.0036934614181518555
Batch 20/64 loss: -0.0011032819747924805
Batch 21/64 loss: 0.017147481441497803
Batch 22/64 loss: 0.024812698364257812
Batch 23/64 loss: -0.025362133979797363
Batch 24/64 loss: 0.019327402114868164
Batch 25/64 loss: -0.0014917850494384766
Batch 26/64 loss: 0.0031976699829101562
Batch 27/64 loss: -0.01387113332748413
Batch 28/64 loss: 0.013935327529907227
Batch 29/64 loss: 0.004398465156555176
Batch 30/64 loss: -0.0001201629638671875
Batch 31/64 loss: 0.011100292205810547
Batch 32/64 loss: -0.003798842430114746
Batch 33/64 loss: 0.013279438018798828
Batch 34/64 loss: -0.01799643039703369
Batch 35/64 loss: 0.007344067096710205
Batch 36/64 loss: 0.003099799156188965
Batch 37/64 loss: 0.01800769567489624
Batch 38/64 loss: -0.0027723312377929688
Batch 39/64 loss: 0.025833487510681152
Batch 40/64 loss: 0.004131734371185303
Batch 41/64 loss: -0.004025816917419434
Batch 42/64 loss: -0.007721304893493652
Batch 43/64 loss: -0.031803131103515625
Batch 44/64 loss: -0.0198214054107666
Batch 45/64 loss: 0.039279818534851074
Batch 46/64 loss: 0.014819204807281494
Batch 47/64 loss: -0.005161881446838379
Batch 48/64 loss: 0.012077152729034424
Batch 49/64 loss: 0.01725870370864868
Batch 50/64 loss: -0.010768771171569824
Batch 51/64 loss: -0.0018903613090515137
Batch 52/64 loss: 0.018522024154663086
Batch 53/64 loss: 0.017927110195159912
Batch 54/64 loss: 0.01747196912765503
Batch 55/64 loss: -0.004915773868560791
Batch 56/64 loss: 0.009612977504730225
Batch 57/64 loss: -0.0019728541374206543
Batch 58/64 loss: -0.008600890636444092
Batch 59/64 loss: 0.0010973215103149414
Batch 60/64 loss: -0.023077785968780518
Batch 61/64 loss: -0.01992511749267578
Batch 62/64 loss: -0.006994724273681641
Batch 63/64 loss: -0.0056816935539245605
Batch 64/64 loss: 0.01624119281768799
Epoch 475  Train loss: 0.003544001018299776  Val loss: 0.121538095457857
Epoch 476
-------------------------------
Batch 1/64 loss: -0.004038989543914795
Batch 2/64 loss: 0.01726168394088745
Batch 3/64 loss: -0.013159036636352539
Batch 4/64 loss: -0.021573901176452637
Batch 5/64 loss: 0.0008817911148071289
Batch 6/64 loss: -0.019692718982696533
Batch 7/64 loss: -0.005548000335693359
Batch 8/64 loss: 0.004439175128936768
Batch 9/64 loss: 0.003861725330352783
Batch 10/64 loss: 0.0023016929626464844
Batch 11/64 loss: 0.0031464695930480957
Batch 12/64 loss: -0.0026886463165283203
Batch 13/64 loss: 0.004359841346740723
Batch 14/64 loss: 0.014745116233825684
Batch 15/64 loss: -0.007166028022766113
Batch 16/64 loss: -0.012813270092010498
Batch 17/64 loss: -0.0047653913497924805
Batch 18/64 loss: -0.023938417434692383
Batch 19/64 loss: 0.0005339980125427246
Batch 20/64 loss: 0.02113783359527588
Batch 21/64 loss: -0.013523221015930176
Batch 22/64 loss: 0.025314033031463623
Batch 23/64 loss: 0.01777482032775879
Batch 24/64 loss: -0.02701127529144287
Batch 25/64 loss: -0.02239018678665161
Batch 26/64 loss: -0.004780471324920654
Batch 27/64 loss: -0.023190319538116455
Batch 28/64 loss: -0.02012842893600464
Batch 29/64 loss: -0.024098217487335205
Batch 30/64 loss: -0.0025757551193237305
Batch 31/64 loss: -0.016062557697296143
Batch 32/64 loss: -0.012136697769165039
Batch 33/64 loss: 0.010463893413543701
Batch 34/64 loss: 0.02185767889022827
Batch 35/64 loss: 0.023264765739440918
Batch 36/64 loss: 0.030960440635681152
Batch 37/64 loss: 0.008296668529510498
Batch 38/64 loss: 0.015076279640197754
Batch 39/64 loss: 0.03203582763671875
Batch 40/64 loss: -0.00975888967514038
Batch 41/64 loss: 0.00897151231765747
Batch 42/64 loss: -0.0022928714752197266
Batch 43/64 loss: -0.014410555362701416
Batch 44/64 loss: 0.03286254405975342
Batch 45/64 loss: 0.01347362995147705
Batch 46/64 loss: -0.012443184852600098
Batch 47/64 loss: -0.0070778727531433105
Batch 48/64 loss: -0.005309641361236572
Batch 49/64 loss: 0.02952098846435547
Batch 50/64 loss: 0.005196213722229004
Batch 51/64 loss: 0.004891693592071533
Batch 52/64 loss: 0.015007972717285156
Batch 53/64 loss: -0.0027265548706054688
Batch 54/64 loss: -0.0024988651275634766
Batch 55/64 loss: 0.013210177421569824
Batch 56/64 loss: 0.007837057113647461
Batch 57/64 loss: 0.015532135963439941
Batch 58/64 loss: -0.008497178554534912
Batch 59/64 loss: -0.012437939643859863
Batch 60/64 loss: 0.017601311206817627
Batch 61/64 loss: 0.007261872291564941
Batch 62/64 loss: 0.0400693416595459
Batch 63/64 loss: 0.03308039903640747
Batch 64/64 loss: -0.025545597076416016
Epoch 476  Train loss: 0.0019503733691047219  Val loss: 0.12143319435545669
Epoch 477
-------------------------------
Batch 1/64 loss: -0.0061800479888916016
Batch 2/64 loss: -0.0031011104583740234
Batch 3/64 loss: -0.0022065043449401855
Batch 4/64 loss: -0.021723806858062744
Batch 5/64 loss: -0.005565047264099121
Batch 6/64 loss: -0.016751348972320557
Batch 7/64 loss: 0.006456255912780762
Batch 8/64 loss: 0.048290789127349854
Batch 9/64 loss: -0.01996636390686035
Batch 10/64 loss: -0.016578316688537598
Batch 11/64 loss: 0.0036249756813049316
Batch 12/64 loss: -0.012151598930358887
Batch 13/64 loss: -0.032100796699523926
Batch 14/64 loss: 0.004512965679168701
Batch 15/64 loss: 0.04142087697982788
Batch 16/64 loss: 0.0006568431854248047
Batch 17/64 loss: -0.0051631927490234375
Batch 18/64 loss: -0.0155714750289917
Batch 19/64 loss: -0.005967915058135986
Batch 20/64 loss: 0.01770561933517456
Batch 21/64 loss: 0.04544329643249512
Batch 22/64 loss: 0.02158033847808838
Batch 23/64 loss: -0.009285926818847656
Batch 24/64 loss: 0.009897589683532715
Batch 25/64 loss: -0.027652502059936523
Batch 26/64 loss: -0.024854004383087158
Batch 27/64 loss: 0.02898317575454712
Batch 28/64 loss: -0.05941814184188843
Batch 29/64 loss: -0.016296863555908203
Batch 30/64 loss: -0.012335538864135742
Batch 31/64 loss: 0.007203578948974609
Batch 32/64 loss: -0.0066765546798706055
Batch 33/64 loss: 0.005150973796844482
Batch 34/64 loss: 0.003489255905151367
Batch 35/64 loss: -0.00888603925704956
Batch 36/64 loss: 0.02073514461517334
Batch 37/64 loss: 0.009024500846862793
Batch 38/64 loss: 0.015830278396606445
Batch 39/64 loss: 0.0066724419593811035
Batch 40/64 loss: 0.005193173885345459
Batch 41/64 loss: -0.017213761806488037
Batch 42/64 loss: 0.018524765968322754
Batch 43/64 loss: -0.015242695808410645
Batch 44/64 loss: 0.005398809909820557
Batch 45/64 loss: -0.0022115111351013184
Batch 46/64 loss: 0.0050383806228637695
Batch 47/64 loss: -0.01489877700805664
Batch 48/64 loss: -0.017935514450073242
Batch 49/64 loss: 0.016710281372070312
Batch 50/64 loss: 0.04359787702560425
Batch 51/64 loss: 0.0021205544471740723
Batch 52/64 loss: 0.002768397331237793
Batch 53/64 loss: 0.02883899211883545
Batch 54/64 loss: -0.01507711410522461
Batch 55/64 loss: 0.019281506538391113
Batch 56/64 loss: 0.0001665353775024414
Batch 57/64 loss: 0.024019837379455566
Batch 58/64 loss: -0.021100759506225586
Batch 59/64 loss: 0.052491068840026855
Batch 60/64 loss: -0.0019782185554504395
Batch 61/64 loss: 0.003996014595031738
Batch 62/64 loss: 0.03329193592071533
Batch 63/64 loss: 0.03810536861419678
Batch 64/64 loss: -0.0053095221519470215
Epoch 477  Train loss: 0.002480765651254093  Val loss: 0.12056453367279157
Epoch 478
-------------------------------
Batch 1/64 loss: -0.012468576431274414
Batch 2/64 loss: 0.0033309459686279297
Batch 3/64 loss: 0.0019071102142333984
Batch 4/64 loss: -0.004965424537658691
Batch 5/64 loss: -0.015260636806488037
Batch 6/64 loss: -0.008365392684936523
Batch 7/64 loss: -0.02759605646133423
Batch 8/64 loss: 0.015502691268920898
Batch 9/64 loss: 0.018224716186523438
Batch 10/64 loss: -0.0069533586502075195
Batch 11/64 loss: 0.0038371682167053223
Batch 12/64 loss: -0.019618511199951172
Batch 13/64 loss: -0.01270383596420288
Batch 14/64 loss: -0.024668514728546143
Batch 15/64 loss: 0.018555045127868652
Batch 16/64 loss: -0.004465818405151367
Batch 17/64 loss: 0.009693741798400879
Batch 18/64 loss: 0.005569338798522949
Batch 19/64 loss: 0.0003788471221923828
Batch 20/64 loss: 0.02389514446258545
Batch 21/64 loss: -0.013721227645874023
Batch 22/64 loss: -0.004698812961578369
Batch 23/64 loss: 0.004360675811767578
Batch 24/64 loss: 0.0007098913192749023
Batch 25/64 loss: 0.009835004806518555
Batch 26/64 loss: -0.03423571586608887
Batch 27/64 loss: 0.01457059383392334
Batch 28/64 loss: -0.011680960655212402
Batch 29/64 loss: -0.004321873188018799
Batch 30/64 loss: 0.012235522270202637
Batch 31/64 loss: 0.010282635688781738
Batch 32/64 loss: -0.0050806403160095215
Batch 33/64 loss: 0.004499316215515137
Batch 34/64 loss: -0.015067517757415771
Batch 35/64 loss: 0.02325284481048584
Batch 36/64 loss: -0.02196258306503296
Batch 37/64 loss: 0.015773296356201172
Batch 38/64 loss: 0.015092968940734863
Batch 39/64 loss: 0.009295344352722168
Batch 40/64 loss: -0.02002537250518799
Batch 41/64 loss: -0.019173800945281982
Batch 42/64 loss: 0.038013577461242676
Batch 43/64 loss: 0.034092485904693604
Batch 44/64 loss: 0.012209415435791016
Batch 45/64 loss: 0.015499293804168701
Batch 46/64 loss: 0.003991365432739258
Batch 47/64 loss: -0.0034742355346679688
Batch 48/64 loss: -0.006051957607269287
Batch 49/64 loss: 0.010009288787841797
Batch 50/64 loss: 0.027010083198547363
Batch 51/64 loss: -0.0067272186279296875
Batch 52/64 loss: -0.01072394847869873
Batch 53/64 loss: 0.028957009315490723
Batch 54/64 loss: 0.008978426456451416
Batch 55/64 loss: 0.016310811042785645
Batch 56/64 loss: -0.00704193115234375
Batch 57/64 loss: 0.03615778684616089
Batch 58/64 loss: -0.021573185920715332
Batch 59/64 loss: -0.013757586479187012
Batch 60/64 loss: -0.0016794800758361816
Batch 61/64 loss: -0.006260395050048828
Batch 62/64 loss: -0.0029510855674743652
Batch 63/64 loss: 0.016902923583984375
Batch 64/64 loss: 0.0112992525100708
Epoch 478  Train loss: 0.001727593646329992  Val loss: 0.1232748777186338
Epoch 479
-------------------------------
Batch 1/64 loss: 0.0011529922485351562
Batch 2/64 loss: -0.012310445308685303
Batch 3/64 loss: 0.00260084867477417
Batch 4/64 loss: -0.009333252906799316
Batch 5/64 loss: 0.003479599952697754
Batch 6/64 loss: -0.003934264183044434
Batch 7/64 loss: 0.007432103157043457
Batch 8/64 loss: -0.0021334290504455566
Batch 9/64 loss: -0.04152184724807739
Batch 10/64 loss: 0.014997363090515137
Batch 11/64 loss: -0.005816102027893066
Batch 12/64 loss: -0.005157768726348877
Batch 13/64 loss: 0.004025697708129883
Batch 14/64 loss: -0.02759009599685669
Batch 15/64 loss: 0.0026487112045288086
Batch 16/64 loss: 0.004820525646209717
Batch 17/64 loss: 0.03754866123199463
Batch 18/64 loss: -0.014438152313232422
Batch 19/64 loss: 0.01088714599609375
Batch 20/64 loss: -0.007213771343231201
Batch 21/64 loss: 0.01757591962814331
Batch 22/64 loss: -0.016154885292053223
Batch 23/64 loss: -0.002205967903137207
Batch 24/64 loss: -0.0007052421569824219
Batch 25/64 loss: 0.010592341423034668
Batch 26/64 loss: 0.01788961887359619
Batch 27/64 loss: -0.0051441192626953125
Batch 28/64 loss: 0.004985213279724121
Batch 29/64 loss: 0.02081310749053955
Batch 30/64 loss: -0.00829392671585083
Batch 31/64 loss: 0.00844275951385498
Batch 32/64 loss: -0.007126450538635254
Batch 33/64 loss: 0.0011749863624572754
Batch 34/64 loss: 0.011745572090148926
Batch 35/64 loss: 0.0058983564376831055
Batch 36/64 loss: 0.002183377742767334
Batch 37/64 loss: 0.03158998489379883
Batch 38/64 loss: -0.006632208824157715
Batch 39/64 loss: -0.004780173301696777
Batch 40/64 loss: -0.0023769140243530273
Batch 41/64 loss: 0.016421079635620117
Batch 42/64 loss: -0.043308377265930176
Batch 43/64 loss: 0.02702552080154419
Batch 44/64 loss: 0.019650578498840332
Batch 45/64 loss: -0.02524322271347046
Batch 46/64 loss: 0.015209853649139404
Batch 47/64 loss: 0.031130433082580566
Batch 48/64 loss: -0.004094600677490234
Batch 49/64 loss: 0.028627753257751465
Batch 50/64 loss: -0.0047959089279174805
Batch 51/64 loss: 0.014829635620117188
Batch 52/64 loss: 0.009365618228912354
Batch 53/64 loss: 0.006505846977233887
Batch 54/64 loss: -0.0029457807540893555
Batch 55/64 loss: 0.0074558258056640625
Batch 56/64 loss: 0.025540947914123535
Batch 57/64 loss: 0.01174628734588623
Batch 58/64 loss: -0.014370143413543701
Batch 59/64 loss: -0.0035863518714904785
Batch 60/64 loss: -0.02258354425430298
Batch 61/64 loss: 0.019428253173828125
Batch 62/64 loss: -0.0026394128799438477
Batch 63/64 loss: 0.00020694732666015625
Batch 64/64 loss: 0.01417773962020874
Epoch 479  Train loss: 0.0025070809850505755  Val loss: 0.12124708018352076
Epoch 480
-------------------------------
Batch 1/64 loss: 0.013930857181549072
Batch 2/64 loss: -0.021180808544158936
Batch 3/64 loss: -0.014876306056976318
Batch 4/64 loss: -0.005744218826293945
Batch 5/64 loss: 0.022734403610229492
Batch 6/64 loss: -0.006733715534210205
Batch 7/64 loss: 0.015388071537017822
Batch 8/64 loss: 0.0022901296615600586
Batch 9/64 loss: -0.015427947044372559
Batch 10/64 loss: -0.01586127281188965
Batch 11/64 loss: 0.009359359741210938
Batch 12/64 loss: -0.016849935054779053
Batch 13/64 loss: 0.005803942680358887
Batch 14/64 loss: -0.01123344898223877
Batch 15/64 loss: -0.015651822090148926
Batch 16/64 loss: 0.03893023729324341
Batch 17/64 loss: 0.012862324714660645
Batch 18/64 loss: -0.0030525922775268555
Batch 19/64 loss: 0.017193973064422607
Batch 20/64 loss: -0.008680343627929688
Batch 21/64 loss: 0.003716707229614258
Batch 22/64 loss: -0.03064626455307007
Batch 23/64 loss: 0.015001654624938965
Batch 24/64 loss: -0.018436551094055176
Batch 25/64 loss: -0.006210625171661377
Batch 26/64 loss: -0.018541395664215088
Batch 27/64 loss: 0.01685047149658203
Batch 28/64 loss: -0.027400851249694824
Batch 29/64 loss: 0.020097553730010986
Batch 30/64 loss: -0.012824594974517822
Batch 31/64 loss: -0.008797824382781982
Batch 32/64 loss: 0.0272219181060791
Batch 33/64 loss: 0.022496402263641357
Batch 34/64 loss: -0.006049931049346924
Batch 35/64 loss: -0.03128474950790405
Batch 36/64 loss: 0.005730926990509033
Batch 37/64 loss: 0.007238626480102539
Batch 38/64 loss: -0.0036129355430603027
Batch 39/64 loss: 0.017772555351257324
Batch 40/64 loss: -0.0014591813087463379
Batch 41/64 loss: -0.0090598464012146
Batch 42/64 loss: 0.007059812545776367
Batch 43/64 loss: -0.0006574392318725586
Batch 44/64 loss: 0.0031067728996276855
Batch 45/64 loss: 0.018074750900268555
Batch 46/64 loss: 0.013312876224517822
Batch 47/64 loss: 0.005586206912994385
Batch 48/64 loss: 0.004225194454193115
Batch 49/64 loss: 0.0206298828125
Batch 50/64 loss: 0.009557247161865234
Batch 51/64 loss: 0.004669547080993652
Batch 52/64 loss: -0.009330809116363525
Batch 53/64 loss: 0.015603423118591309
Batch 54/64 loss: -0.012768387794494629
Batch 55/64 loss: -0.003210127353668213
Batch 56/64 loss: -0.019265413284301758
Batch 57/64 loss: 0.006162524223327637
Batch 58/64 loss: -0.02979564666748047
Batch 59/64 loss: 0.020438969135284424
Batch 60/64 loss: -0.009601593017578125
Batch 61/64 loss: 0.04646176099777222
Batch 62/64 loss: -0.013701260089874268
Batch 63/64 loss: -0.0005540251731872559
Batch 64/64 loss: 0.01995617151260376
Epoch 480  Train loss: 0.0008780290098751293  Val loss: 0.12279151702664562
Epoch 481
-------------------------------
Batch 1/64 loss: -0.015519380569458008
Batch 2/64 loss: 0.014204919338226318
Batch 3/64 loss: -0.007426917552947998
Batch 4/64 loss: 0.03044259548187256
Batch 5/64 loss: 0.0395045280456543
Batch 6/64 loss: -0.005620777606964111
Batch 7/64 loss: -0.005053818225860596
Batch 8/64 loss: -0.013348698616027832
Batch 9/64 loss: -0.010616183280944824
Batch 10/64 loss: -0.0019699931144714355
Batch 11/64 loss: -0.0069359540939331055
Batch 12/64 loss: 0.02276623249053955
Batch 13/64 loss: 0.020492136478424072
Batch 14/64 loss: 0.0013797879219055176
Batch 15/64 loss: -0.036670148372650146
Batch 16/64 loss: 0.011557042598724365
Batch 17/64 loss: -0.028191030025482178
Batch 18/64 loss: 0.0003330707550048828
Batch 19/64 loss: 0.003358602523803711
Batch 20/64 loss: -0.012522101402282715
Batch 21/64 loss: -0.007704496383666992
Batch 22/64 loss: -0.007034659385681152
Batch 23/64 loss: -0.01099252700805664
Batch 24/64 loss: -0.020091772079467773
Batch 25/64 loss: -0.0035611391067504883
Batch 26/64 loss: 0.05331510305404663
Batch 27/64 loss: 0.007155895233154297
Batch 28/64 loss: 0.008185267448425293
Batch 29/64 loss: -0.011984467506408691
Batch 30/64 loss: 0.04446983337402344
Batch 31/64 loss: 0.02298671007156372
Batch 32/64 loss: 0.0025852322578430176
Batch 33/64 loss: 0.0099448561668396
Batch 34/64 loss: -0.01706528663635254
Batch 35/64 loss: 0.010357022285461426
Batch 36/64 loss: 0.018690168857574463
Batch 37/64 loss: -0.015051722526550293
Batch 38/64 loss: 0.004276752471923828
Batch 39/64 loss: -0.015829920768737793
Batch 40/64 loss: -0.009063482284545898
Batch 41/64 loss: 0.025267422199249268
Batch 42/64 loss: 0.009707868099212646
Batch 43/64 loss: -0.03139227628707886
Batch 44/64 loss: -0.024273335933685303
Batch 45/64 loss: -0.002234339714050293
Batch 46/64 loss: 0.0002944469451904297
Batch 47/64 loss: -0.0003536343574523926
Batch 48/64 loss: -0.0011399388313293457
Batch 49/64 loss: 0.014529526233673096
Batch 50/64 loss: 0.002431035041809082
Batch 51/64 loss: 0.019961953163146973
Batch 52/64 loss: -0.009844481945037842
Batch 53/64 loss: 0.020575344562530518
Batch 54/64 loss: 0.026264071464538574
Batch 55/64 loss: -0.0033102035522460938
Batch 56/64 loss: 0.007009267807006836
Batch 57/64 loss: 0.022287964820861816
Batch 58/64 loss: -0.016345858573913574
Batch 59/64 loss: 0.00708693265914917
Batch 60/64 loss: 0.00473940372467041
Batch 61/64 loss: -0.0032936930656433105
Batch 62/64 loss: 0.002961575984954834
Batch 63/64 loss: -0.010025084018707275
Batch 64/64 loss: 0.023220300674438477
Epoch 481  Train loss: 0.002228556427301145  Val loss: 0.12315661022343587
Epoch 482
-------------------------------
Batch 1/64 loss: -0.026712417602539062
Batch 2/64 loss: 0.01442098617553711
Batch 3/64 loss: 0.025171399116516113
Batch 4/64 loss: 0.009390294551849365
Batch 5/64 loss: 0.009160637855529785
Batch 6/64 loss: 0.022213995456695557
Batch 7/64 loss: 0.017764687538146973
Batch 8/64 loss: -0.004922151565551758
Batch 9/64 loss: -0.005645751953125
Batch 10/64 loss: -0.019315123558044434
Batch 11/64 loss: -0.006408274173736572
Batch 12/64 loss: -0.003412008285522461
Batch 13/64 loss: 0.002816617488861084
Batch 14/64 loss: 0.011015057563781738
Batch 15/64 loss: -0.023356378078460693
Batch 16/64 loss: 0.030799269676208496
Batch 17/64 loss: -0.027434110641479492
Batch 18/64 loss: -0.02892845869064331
Batch 19/64 loss: 0.005874693393707275
Batch 20/64 loss: -0.0021785497665405273
Batch 21/64 loss: -0.017012834548950195
Batch 22/64 loss: -0.0014033317565917969
Batch 23/64 loss: -0.018325507640838623
Batch 24/64 loss: 0.006637096405029297
Batch 25/64 loss: 0.004872739315032959
Batch 26/64 loss: 0.006525516510009766
Batch 27/64 loss: 0.004682183265686035
Batch 28/64 loss: -0.010658740997314453
Batch 29/64 loss: 0.0030165910720825195
Batch 30/64 loss: 0.011759042739868164
Batch 31/64 loss: -0.017179906368255615
Batch 32/64 loss: -0.004737555980682373
Batch 33/64 loss: -0.01116645336151123
Batch 34/64 loss: 0.04853808879852295
Batch 35/64 loss: 0.0014340877532958984
Batch 36/64 loss: 0.0026859045028686523
Batch 37/64 loss: 0.040754616260528564
Batch 38/64 loss: -0.0032276511192321777
Batch 39/64 loss: -0.01520395278930664
Batch 40/64 loss: -0.003282904624938965
Batch 41/64 loss: -0.0013313889503479004
Batch 42/64 loss: -0.008647024631500244
Batch 43/64 loss: 0.018053948879241943
Batch 44/64 loss: -0.0024601221084594727
Batch 45/64 loss: 0.003496110439300537
Batch 46/64 loss: 0.0033599138259887695
Batch 47/64 loss: 0.003782033920288086
Batch 48/64 loss: 0.005680084228515625
Batch 49/64 loss: -0.015356659889221191
Batch 50/64 loss: 0.032887160778045654
Batch 51/64 loss: 0.014577090740203857
Batch 52/64 loss: 0.012698888778686523
Batch 53/64 loss: -0.010576426982879639
Batch 54/64 loss: 0.013794898986816406
Batch 55/64 loss: 0.011571824550628662
Batch 56/64 loss: 0.026282429695129395
Batch 57/64 loss: 0.014666199684143066
Batch 58/64 loss: 0.020983576774597168
Batch 59/64 loss: 0.009423434734344482
Batch 60/64 loss: -0.012115240097045898
Batch 61/64 loss: -0.013508200645446777
Batch 62/64 loss: 0.013382196426391602
Batch 63/64 loss: 0.0318569540977478
Batch 64/64 loss: 0.0150299072265625
Epoch 482  Train loss: 0.003337969499475816  Val loss: 0.12522501204022018
Epoch 483
-------------------------------
Batch 1/64 loss: 0.012006700038909912
Batch 2/64 loss: -0.002224445343017578
Batch 3/64 loss: 0.014225542545318604
Batch 4/64 loss: -0.03998458385467529
Batch 5/64 loss: -0.004920899868011475
Batch 6/64 loss: -0.0004177689552307129
Batch 7/64 loss: -0.017210006713867188
Batch 8/64 loss: -0.021307647228240967
Batch 9/64 loss: -0.0062833428382873535
Batch 10/64 loss: -0.02285933494567871
Batch 11/64 loss: -0.03334122896194458
Batch 12/64 loss: -0.03292769193649292
Batch 13/64 loss: 0.02715325355529785
Batch 14/64 loss: -0.002891242504119873
Batch 15/64 loss: 0.00655817985534668
Batch 16/64 loss: -0.004787921905517578
Batch 17/64 loss: 0.009665489196777344
Batch 18/64 loss: -0.008744001388549805
Batch 19/64 loss: 0.009694993495941162
Batch 20/64 loss: 0.024592995643615723
Batch 21/64 loss: 0.007348060607910156
Batch 22/64 loss: 0.004721224308013916
Batch 23/64 loss: 0.009719312191009521
Batch 24/64 loss: -0.001470029354095459
Batch 25/64 loss: 0.03585958480834961
Batch 26/64 loss: 0.032154858112335205
Batch 27/64 loss: 0.029243826866149902
Batch 28/64 loss: -0.0007311701774597168
Batch 29/64 loss: 0.0010107755661010742
Batch 30/64 loss: -0.006616711616516113
Batch 31/64 loss: -0.0029680728912353516
Batch 32/64 loss: -0.02387869358062744
Batch 33/64 loss: -0.045760929584503174
Batch 34/64 loss: -0.014788210391998291
Batch 35/64 loss: -0.006423830986022949
Batch 36/64 loss: 0.02595663070678711
Batch 37/64 loss: 0.021294355392456055
Batch 38/64 loss: 0.0267488956451416
Batch 39/64 loss: 0.0019201040267944336
Batch 40/64 loss: -0.04478055238723755
Batch 41/64 loss: -0.0024794340133666992
Batch 42/64 loss: 0.00044143199920654297
Batch 43/64 loss: 0.014842987060546875
Batch 44/64 loss: 0.021911919116973877
Batch 45/64 loss: -0.01470935344696045
Batch 46/64 loss: 0.01660221815109253
Batch 47/64 loss: -0.028718650341033936
Batch 48/64 loss: -0.0032932162284851074
Batch 49/64 loss: 0.027379631996154785
Batch 50/64 loss: 0.024910688400268555
Batch 51/64 loss: -0.0028516650199890137
Batch 52/64 loss: -0.0023823976516723633
Batch 53/64 loss: 0.018687725067138672
Batch 54/64 loss: 0.019190192222595215
Batch 55/64 loss: -0.011159181594848633
Batch 56/64 loss: -0.006488323211669922
Batch 57/64 loss: -0.001186668872833252
Batch 58/64 loss: 0.02431100606918335
Batch 59/64 loss: -0.008612573146820068
Batch 60/64 loss: 0.047865867614746094
Batch 61/64 loss: 0.012185275554656982
Batch 62/64 loss: -0.007133841514587402
Batch 63/64 loss: -0.015210092067718506
Batch 64/64 loss: 0.03628420829772949
Epoch 483  Train loss: 0.001660755568859624  Val loss: 0.12217196893855878
Epoch 484
-------------------------------
Batch 1/64 loss: -0.02897101640701294
Batch 2/64 loss: -0.0063970088958740234
Batch 3/64 loss: -0.02206563949584961
Batch 4/64 loss: -0.01373600959777832
Batch 5/64 loss: -0.014024913311004639
Batch 6/64 loss: -0.0026120543479919434
Batch 7/64 loss: 0.0072512030601501465
Batch 8/64 loss: 0.0016605854034423828
Batch 9/64 loss: -0.011020362377166748
Batch 10/64 loss: 0.02412313222885132
Batch 11/64 loss: -0.014005780220031738
Batch 12/64 loss: -0.004948973655700684
Batch 13/64 loss: -0.0129128098487854
Batch 14/64 loss: -0.007307708263397217
Batch 15/64 loss: -0.02453446388244629
Batch 16/64 loss: -0.023724138736724854
Batch 17/64 loss: -0.004259586334228516
Batch 18/64 loss: -0.008863627910614014
Batch 19/64 loss: 0.0067726969718933105
Batch 20/64 loss: -0.03949308395385742
Batch 21/64 loss: 0.010412991046905518
Batch 22/64 loss: 0.043520331382751465
Batch 23/64 loss: 0.0005668997764587402
Batch 24/64 loss: 0.03550863265991211
Batch 25/64 loss: -0.01517939567565918
Batch 26/64 loss: 0.022186338901519775
Batch 27/64 loss: -0.014517366886138916
Batch 28/64 loss: 0.023234307765960693
Batch 29/64 loss: -0.0029552578926086426
Batch 30/64 loss: -0.025482594966888428
Batch 31/64 loss: -0.007885932922363281
Batch 32/64 loss: 0.009664714336395264
Batch 33/64 loss: -0.009219765663146973
Batch 34/64 loss: -0.0033481717109680176
Batch 35/64 loss: -0.014293432235717773
Batch 36/64 loss: 0.008773624897003174
Batch 37/64 loss: -0.0063474178314208984
Batch 38/64 loss: -0.015346705913543701
Batch 39/64 loss: -0.015322446823120117
Batch 40/64 loss: -0.0003535151481628418
Batch 41/64 loss: 0.0094374418258667
Batch 42/64 loss: -0.0005916357040405273
Batch 43/64 loss: -0.006037890911102295
Batch 44/64 loss: 0.008078336715698242
Batch 45/64 loss: 0.007539987564086914
Batch 46/64 loss: -0.0008225440979003906
Batch 47/64 loss: 0.004831790924072266
Batch 48/64 loss: -0.02058553695678711
Batch 49/64 loss: 0.01928555965423584
Batch 50/64 loss: 0.019524574279785156
Batch 51/64 loss: 0.003544449806213379
Batch 52/64 loss: 0.0012584924697875977
Batch 53/64 loss: 0.03191608190536499
Batch 54/64 loss: 0.04646503925323486
Batch 55/64 loss: 0.0005733966827392578
Batch 56/64 loss: 0.02436840534210205
Batch 57/64 loss: -0.011117815971374512
Batch 58/64 loss: 0.012471318244934082
Batch 59/64 loss: -0.02456456422805786
Batch 60/64 loss: 0.017392396926879883
Batch 61/64 loss: 0.011413216590881348
Batch 62/64 loss: 0.016381502151489258
Batch 63/64 loss: 0.032614707946777344
Batch 64/64 loss: -0.01791858673095703
Epoch 484  Train loss: 0.000227200751211129  Val loss: 0.12292343992547891
Epoch 485
-------------------------------
Batch 1/64 loss: -0.0036031603813171387
Batch 2/64 loss: 0.0239717960357666
Batch 3/64 loss: -0.008668005466461182
Batch 4/64 loss: -0.0198553204536438
Batch 5/64 loss: 0.007118821144104004
Batch 6/64 loss: 0.00811678171157837
Batch 7/64 loss: 0.0023049116134643555
Batch 8/64 loss: 0.007169127464294434
Batch 9/64 loss: -0.005114138126373291
Batch 10/64 loss: -0.0030015110969543457
Batch 11/64 loss: -0.03288072347640991
Batch 12/64 loss: 0.0003561973571777344
Batch 13/64 loss: -0.0016368627548217773
Batch 14/64 loss: -0.004767060279846191
Batch 15/64 loss: 0.007136344909667969
Batch 16/64 loss: -0.0029309988021850586
Batch 17/64 loss: -0.016312837600708008
Batch 18/64 loss: 0.012983500957489014
Batch 19/64 loss: 0.008844733238220215
Batch 20/64 loss: 0.013334989547729492
Batch 21/64 loss: -0.003610670566558838
Batch 22/64 loss: -0.010872066020965576
Batch 23/64 loss: 0.012860000133514404
Batch 24/64 loss: -0.032280683517456055
Batch 25/64 loss: 0.015211820602416992
Batch 26/64 loss: -0.00024056434631347656
Batch 27/64 loss: 0.0020638108253479004
Batch 28/64 loss: -0.0405084490776062
Batch 29/64 loss: 0.01538175344467163
Batch 30/64 loss: -0.010769665241241455
Batch 31/64 loss: -0.010392546653747559
Batch 32/64 loss: -0.005439400672912598
Batch 33/64 loss: 0.0028066635131835938
Batch 34/64 loss: 0.007405102252960205
Batch 35/64 loss: -0.016670942306518555
Batch 36/64 loss: 0.0370936393737793
Batch 37/64 loss: -0.006989836692810059
Batch 38/64 loss: -0.024371206760406494
Batch 39/64 loss: 0.02068108320236206
Batch 40/64 loss: -0.018274784088134766
Batch 41/64 loss: 0.023034214973449707
Batch 42/64 loss: -0.0026903152465820312
Batch 43/64 loss: -0.011499285697937012
Batch 44/64 loss: 0.03515362739562988
Batch 45/64 loss: 0.01325535774230957
Batch 46/64 loss: 0.04073536396026611
Batch 47/64 loss: -0.004750669002532959
Batch 48/64 loss: -0.02291804552078247
Batch 49/64 loss: 0.004924476146697998
Batch 50/64 loss: -0.008735895156860352
Batch 51/64 loss: -0.0037277936935424805
Batch 52/64 loss: 0.01505500078201294
Batch 53/64 loss: 0.03658264875411987
Batch 54/64 loss: -0.016198813915252686
Batch 55/64 loss: -0.033403635025024414
Batch 56/64 loss: -0.007472991943359375
Batch 57/64 loss: 0.029585599899291992
Batch 58/64 loss: 0.02970588207244873
Batch 59/64 loss: -0.0034886598587036133
Batch 60/64 loss: -0.03044116497039795
Batch 61/64 loss: -0.023934543132781982
Batch 62/64 loss: 0.017443060874938965
Batch 63/64 loss: 0.030551791191101074
Batch 64/64 loss: -0.0012737512588500977
Epoch 485  Train loss: 0.0004934829824111041  Val loss: 0.12004660290131454
Epoch 486
-------------------------------
Batch 1/64 loss: 0.020016908645629883
Batch 2/64 loss: -0.020652413368225098
Batch 3/64 loss: -0.028755545616149902
Batch 4/64 loss: -0.04074060916900635
Batch 5/64 loss: 0.01202845573425293
Batch 6/64 loss: -0.016118526458740234
Batch 7/64 loss: -0.038802146911621094
Batch 8/64 loss: -0.006244659423828125
Batch 9/64 loss: 0.008521854877471924
Batch 10/64 loss: -0.0027276277542114258
Batch 11/64 loss: 0.01342231035232544
Batch 12/64 loss: -0.007270872592926025
Batch 13/64 loss: -0.009823322296142578
Batch 14/64 loss: 0.01234745979309082
Batch 15/64 loss: -0.00038993358612060547
Batch 16/64 loss: -0.008158445358276367
Batch 17/64 loss: -0.015215814113616943
Batch 18/64 loss: -0.003349006175994873
Batch 19/64 loss: -0.013038277626037598
Batch 20/64 loss: -0.01743561029434204
Batch 21/64 loss: 0.018371164798736572
Batch 22/64 loss: -0.01137244701385498
Batch 23/64 loss: 0.01578676700592041
Batch 24/64 loss: 0.0300370454788208
Batch 25/64 loss: 0.0010250210762023926
Batch 26/64 loss: -0.0011442899703979492
Batch 27/64 loss: -0.004128456115722656
Batch 28/64 loss: 0.000410616397857666
Batch 29/64 loss: 0.03017747402191162
Batch 30/64 loss: -0.011611342430114746
Batch 31/64 loss: 0.03333836793899536
Batch 32/64 loss: -0.028293073177337646
Batch 33/64 loss: -0.0035729408264160156
Batch 34/64 loss: 0.012103557586669922
Batch 35/64 loss: -0.0004304647445678711
Batch 36/64 loss: 0.017121613025665283
Batch 37/64 loss: -0.010061323642730713
Batch 38/64 loss: -0.012363553047180176
Batch 39/64 loss: 0.03358644247055054
Batch 40/64 loss: -0.010048985481262207
Batch 41/64 loss: -0.012913107872009277
Batch 42/64 loss: -0.002944767475128174
Batch 43/64 loss: -0.0039980411529541016
Batch 44/64 loss: 0.03001713752746582
Batch 45/64 loss: 0.01542973518371582
Batch 46/64 loss: -0.015685975551605225
Batch 47/64 loss: 0.019363582134246826
Batch 48/64 loss: 0.02971494197845459
Batch 49/64 loss: -0.006015360355377197
Batch 50/64 loss: 0.0018687248229980469
Batch 51/64 loss: -0.022687017917633057
Batch 52/64 loss: -0.0033167600631713867
Batch 53/64 loss: 0.04780733585357666
Batch 54/64 loss: 0.0022467970848083496
Batch 55/64 loss: 0.02016383409500122
Batch 56/64 loss: -0.014248073101043701
Batch 57/64 loss: 0.018850207328796387
Batch 58/64 loss: -0.010990798473358154
Batch 59/64 loss: 0.017220377922058105
Batch 60/64 loss: -0.022687137126922607
Batch 61/64 loss: -0.019338250160217285
Batch 62/64 loss: 0.0027564167976379395
Batch 63/64 loss: -0.014719724655151367
Batch 64/64 loss: 0.02473348379135132
Epoch 486  Train loss: 0.00017238528120751474  Val loss: 0.12158756813232842
Epoch 487
-------------------------------
Batch 1/64 loss: -0.019977450370788574
Batch 2/64 loss: 0.024876058101654053
Batch 3/64 loss: -0.0008193850517272949
Batch 4/64 loss: -0.01386481523513794
Batch 5/64 loss: -0.026561856269836426
Batch 6/64 loss: -0.0012438297271728516
Batch 7/64 loss: -0.011205136775970459
Batch 8/64 loss: 0.0028687119483947754
Batch 9/64 loss: 0.0026427507400512695
Batch 10/64 loss: 0.009067416191101074
Batch 11/64 loss: 0.022895216941833496
Batch 12/64 loss: -0.009469330310821533
Batch 13/64 loss: 0.01020890474319458
Batch 14/64 loss: -0.01671546697616577
Batch 15/64 loss: 0.04068171977996826
Batch 16/64 loss: 0.000681459903717041
Batch 17/64 loss: -0.015612423419952393
Batch 18/64 loss: 0.013759851455688477
Batch 19/64 loss: 0.007516622543334961
Batch 20/64 loss: 0.01616227626800537
Batch 21/64 loss: 0.02908635139465332
Batch 22/64 loss: -0.007606029510498047
Batch 23/64 loss: 0.01440882682800293
Batch 24/64 loss: -0.016637146472930908
Batch 25/64 loss: -0.0007977485656738281
Batch 26/64 loss: 0.013229846954345703
Batch 27/64 loss: -0.026822686195373535
Batch 28/64 loss: -0.016674816608428955
Batch 29/64 loss: -0.004200577735900879
Batch 30/64 loss: 0.007956266403198242
Batch 31/64 loss: -0.009338617324829102
Batch 32/64 loss: -0.03767579793930054
Batch 33/64 loss: -0.001952826976776123
Batch 34/64 loss: 0.008032739162445068
Batch 35/64 loss: 0.017160475254058838
Batch 36/64 loss: -0.0021238327026367188
Batch 37/64 loss: -0.012251615524291992
Batch 38/64 loss: -0.02752143144607544
Batch 39/64 loss: 0.009688854217529297
Batch 40/64 loss: -0.03156059980392456
Batch 41/64 loss: 0.028651058673858643
Batch 42/64 loss: -0.021906256675720215
Batch 43/64 loss: -0.0051850080490112305
Batch 44/64 loss: -0.010270833969116211
Batch 45/64 loss: -0.024256587028503418
Batch 46/64 loss: -0.0026568174362182617
Batch 47/64 loss: 0.015520811080932617
Batch 48/64 loss: -0.0005185604095458984
Batch 49/64 loss: 0.01581740379333496
Batch 50/64 loss: 0.013745665550231934
Batch 51/64 loss: 0.01929318904876709
Batch 52/64 loss: -0.008162498474121094
Batch 53/64 loss: -0.0051313042640686035
Batch 54/64 loss: -0.0034435391426086426
Batch 55/64 loss: 0.03234177827835083
Batch 56/64 loss: -0.01972365379333496
Batch 57/64 loss: 0.0014754533767700195
Batch 58/64 loss: 0.004642665386199951
Batch 59/64 loss: -0.03132188320159912
Batch 60/64 loss: 0.005881309509277344
Batch 61/64 loss: -0.01651322841644287
Batch 62/64 loss: 0.020177483558654785
Batch 63/64 loss: -0.0031485557556152344
Batch 64/64 loss: 0.05257272720336914
Epoch 487  Train loss: -0.00023484604031431908  Val loss: 0.12318366499700907
Epoch 488
-------------------------------
Batch 1/64 loss: -0.009935855865478516
Batch 2/64 loss: 0.02843034267425537
Batch 3/64 loss: -0.006603658199310303
Batch 4/64 loss: -0.0028142929077148438
Batch 5/64 loss: -0.006119728088378906
Batch 6/64 loss: -0.012743949890136719
Batch 7/64 loss: 0.000933229923248291
Batch 8/64 loss: 0.01569688320159912
Batch 9/64 loss: 0.03561586141586304
Batch 10/64 loss: 0.0302506685256958
Batch 11/64 loss: 0.02012181282043457
Batch 12/64 loss: 0.006622493267059326
Batch 13/64 loss: 0.015989601612091064
Batch 14/64 loss: -0.004460453987121582
Batch 15/64 loss: 0.02371370792388916
Batch 16/64 loss: -0.007031679153442383
Batch 17/64 loss: -0.03310459852218628
Batch 18/64 loss: -0.004118800163269043
Batch 19/64 loss: 0.028046369552612305
Batch 20/64 loss: -0.016679763793945312
Batch 21/64 loss: -0.0014557242393493652
Batch 22/64 loss: -0.005978047847747803
Batch 23/64 loss: -0.016443610191345215
Batch 24/64 loss: -0.0003254413604736328
Batch 25/64 loss: -0.0033038854598999023
Batch 26/64 loss: 0.01786738634109497
Batch 27/64 loss: -0.007167637348175049
Batch 28/64 loss: -0.01193094253540039
Batch 29/64 loss: 0.005723059177398682
Batch 30/64 loss: 0.02430802583694458
Batch 31/64 loss: 0.00538712739944458
Batch 32/64 loss: -0.016743063926696777
Batch 33/64 loss: 0.006764411926269531
Batch 34/64 loss: -0.023396849632263184
Batch 35/64 loss: -0.004204154014587402
Batch 36/64 loss: 0.0025900602340698242
Batch 37/64 loss: 0.0030675530433654785
Batch 38/64 loss: -0.045696914196014404
Batch 39/64 loss: -0.013412237167358398
Batch 40/64 loss: -0.0025254487991333008
Batch 41/64 loss: -0.021252572536468506
Batch 42/64 loss: 0.007258176803588867
Batch 43/64 loss: 0.007195472717285156
Batch 44/64 loss: -0.009514093399047852
Batch 45/64 loss: 0.0002651214599609375
Batch 46/64 loss: 0.00028651952743530273
Batch 47/64 loss: 0.01745450496673584
Batch 48/64 loss: -0.01425325870513916
Batch 49/64 loss: -0.004730880260467529
Batch 50/64 loss: -0.002777278423309326
Batch 51/64 loss: -0.011338531970977783
Batch 52/64 loss: -0.015775024890899658
Batch 53/64 loss: 0.03007042407989502
Batch 54/64 loss: -0.005468785762786865
Batch 55/64 loss: 0.019132912158966064
Batch 56/64 loss: -0.014997363090515137
Batch 57/64 loss: -0.013074517250061035
Batch 58/64 loss: -0.006872653961181641
Batch 59/64 loss: 0.0011766552925109863
Batch 60/64 loss: 0.006705045700073242
Batch 61/64 loss: -0.0016347169876098633
Batch 62/64 loss: 0.0034572482109069824
Batch 63/64 loss: -0.01804518699645996
Batch 64/64 loss: 0.0020579099655151367
Epoch 488  Train loss: -0.0004746273452160405  Val loss: 0.11934581060999448
Epoch 489
-------------------------------
Batch 1/64 loss: 0.009495019912719727
Batch 2/64 loss: -0.014301300048828125
Batch 3/64 loss: -0.012120664119720459
Batch 4/64 loss: -0.004031538963317871
Batch 5/64 loss: 0.02419072389602661
Batch 6/64 loss: 0.028079986572265625
Batch 7/64 loss: -0.016460716724395752
Batch 8/64 loss: 0.04682004451751709
Batch 9/64 loss: -0.01764601469039917
Batch 10/64 loss: -0.01867753267288208
Batch 11/64 loss: -0.005269110202789307
Batch 12/64 loss: -0.008317887783050537
Batch 13/64 loss: -0.019581973552703857
Batch 14/64 loss: -0.02352452278137207
Batch 15/64 loss: 0.028817355632781982
Batch 16/64 loss: 0.005266070365905762
Batch 17/64 loss: 0.007648646831512451
Batch 18/64 loss: -0.013271689414978027
Batch 19/64 loss: -0.008027911186218262
Batch 20/64 loss: -0.02844679355621338
Batch 21/64 loss: -0.01314246654510498
Batch 22/64 loss: 0.00466465950012207
Batch 23/64 loss: -0.01246720552444458
Batch 24/64 loss: -0.018121778964996338
Batch 25/64 loss: 0.03239274024963379
Batch 26/64 loss: -0.022643744945526123
Batch 27/64 loss: -0.009243369102478027
Batch 28/64 loss: -0.025577843189239502
Batch 29/64 loss: 0.03315114974975586
Batch 30/64 loss: -0.027035176753997803
Batch 31/64 loss: 0.007807374000549316
Batch 32/64 loss: 0.005730628967285156
Batch 33/64 loss: -0.028600633144378662
Batch 34/64 loss: 0.02155447006225586
Batch 35/64 loss: 0.012326836585998535
Batch 36/64 loss: -0.008856236934661865
Batch 37/64 loss: -0.002294778823852539
Batch 38/64 loss: -0.023755371570587158
Batch 39/64 loss: 0.015955448150634766
Batch 40/64 loss: -0.022173047065734863
Batch 41/64 loss: -0.00644451379776001
Batch 42/64 loss: 0.0008205771446228027
Batch 43/64 loss: -0.009870469570159912
Batch 44/64 loss: 0.01470094919204712
Batch 45/64 loss: 0.008453011512756348
Batch 46/64 loss: -0.017306387424468994
Batch 47/64 loss: 0.00609743595123291
Batch 48/64 loss: -0.01895296573638916
Batch 49/64 loss: 0.02118825912475586
Batch 50/64 loss: 0.0092201828956604
Batch 51/64 loss: 0.02792757749557495
Batch 52/64 loss: 0.026824891567230225
Batch 53/64 loss: 0.001667022705078125
Batch 54/64 loss: 0.009459853172302246
Batch 55/64 loss: -0.0011435747146606445
Batch 56/64 loss: 0.0023949742317199707
Batch 57/64 loss: 0.013126611709594727
Batch 58/64 loss: -0.014065384864807129
Batch 59/64 loss: -0.005849003791809082
Batch 60/64 loss: -0.00010091066360473633
Batch 61/64 loss: 0.008687496185302734
Batch 62/64 loss: -0.02014780044555664
Batch 63/64 loss: 0.015637516975402832
Batch 64/64 loss: -0.007778286933898926
Epoch 489  Train loss: -0.0008344552096198587  Val loss: 0.12166175461307015
Epoch 490
-------------------------------
Batch 1/64 loss: -0.00251162052154541
Batch 2/64 loss: -0.029068946838378906
Batch 3/64 loss: -0.0008096694946289062
Batch 4/64 loss: 0.011978328227996826
Batch 5/64 loss: -0.02357017993927002
Batch 6/64 loss: 0.005519270896911621
Batch 7/64 loss: 0.01313638687133789
Batch 8/64 loss: -0.02254199981689453
Batch 9/64 loss: 0.008147776126861572
Batch 10/64 loss: -0.023062288761138916
Batch 11/64 loss: 0.009867608547210693
Batch 12/64 loss: 0.04306215047836304
Batch 13/64 loss: 0.0227813720703125
Batch 14/64 loss: -0.01215893030166626
Batch 15/64 loss: -0.03302299976348877
Batch 16/64 loss: -0.015405833721160889
Batch 17/64 loss: 0.005946636199951172
Batch 18/64 loss: 0.005709052085876465
Batch 19/64 loss: 0.019304275512695312
Batch 20/64 loss: 0.008596539497375488
Batch 21/64 loss: -0.012359261512756348
Batch 22/64 loss: -0.011172056198120117
Batch 23/64 loss: -0.009151577949523926
Batch 24/64 loss: -0.03423798084259033
Batch 25/64 loss: -0.012648522853851318
Batch 26/64 loss: 0.015541553497314453
Batch 27/64 loss: -0.009773015975952148
Batch 28/64 loss: 0.02100980281829834
Batch 29/64 loss: 0.008364737033843994
Batch 30/64 loss: 0.008218169212341309
Batch 31/64 loss: -0.020977020263671875
Batch 32/64 loss: -0.011086702346801758
Batch 33/64 loss: 0.012474894523620605
Batch 34/64 loss: 0.0010592341423034668
Batch 35/64 loss: 0.022391796112060547
Batch 36/64 loss: 0.014026522636413574
Batch 37/64 loss: -0.001671433448791504
Batch 38/64 loss: -0.024702489376068115
Batch 39/64 loss: -0.01483219861984253
Batch 40/64 loss: 0.041567325592041016
Batch 41/64 loss: 0.0043073296546936035
Batch 42/64 loss: -0.016874194145202637
Batch 43/64 loss: 0.029866456985473633
Batch 44/64 loss: 0.004611015319824219
Batch 45/64 loss: 0.0033097267150878906
Batch 46/64 loss: -0.006829679012298584
Batch 47/64 loss: -0.007090568542480469
Batch 48/64 loss: 0.03148698806762695
Batch 49/64 loss: 0.020019710063934326
Batch 50/64 loss: 0.004398047924041748
Batch 51/64 loss: 0.02041107416152954
Batch 52/64 loss: 0.0032547712326049805
Batch 53/64 loss: -0.015607357025146484
Batch 54/64 loss: 0.01776653528213501
Batch 55/64 loss: 0.011712312698364258
Batch 56/64 loss: 0.0022146105766296387
Batch 57/64 loss: -0.0028312206268310547
Batch 58/64 loss: -0.024398088455200195
Batch 59/64 loss: -0.014756619930267334
Batch 60/64 loss: 0.019844532012939453
Batch 61/64 loss: -0.01763439178466797
Batch 62/64 loss: -0.019733071327209473
Batch 63/64 loss: 0.027980566024780273
Batch 64/64 loss: 0.004384160041809082
Epoch 490  Train loss: 0.0008259656382542031  Val loss: 0.1229946547767141
Epoch 491
-------------------------------
Batch 1/64 loss: -0.023131966590881348
Batch 2/64 loss: 0.003169536590576172
Batch 3/64 loss: 0.030479788780212402
Batch 4/64 loss: -0.010539352893829346
Batch 5/64 loss: -0.025052964687347412
Batch 6/64 loss: 0.005146384239196777
Batch 7/64 loss: 0.005738198757171631
Batch 8/64 loss: -0.02807295322418213
Batch 9/64 loss: -0.015733778476715088
Batch 10/64 loss: -0.02098691463470459
Batch 11/64 loss: 0.003940582275390625
Batch 12/64 loss: 0.0012253522872924805
Batch 13/64 loss: -0.02279078960418701
Batch 14/64 loss: -0.0029450058937072754
Batch 15/64 loss: -0.01766437292098999
Batch 16/64 loss: -0.000405728816986084
Batch 17/64 loss: 0.01232379674911499
Batch 18/64 loss: -0.01300060749053955
Batch 19/64 loss: -0.010410487651824951
Batch 20/64 loss: -0.007583975791931152
Batch 21/64 loss: -0.008607625961303711
Batch 22/64 loss: 0.006739318370819092
Batch 23/64 loss: 0.03614389896392822
Batch 24/64 loss: 0.008919060230255127
Batch 25/64 loss: 0.01528233289718628
Batch 26/64 loss: 0.021031498908996582
Batch 27/64 loss: 0.03665482997894287
Batch 28/64 loss: 0.028317928314208984
Batch 29/64 loss: -0.011200547218322754
Batch 30/64 loss: 0.00015783309936523438
Batch 31/64 loss: -0.012565851211547852
Batch 32/64 loss: -0.009374856948852539
Batch 33/64 loss: 0.01584148406982422
Batch 34/64 loss: 0.004388988018035889
Batch 35/64 loss: -0.012628018856048584
Batch 36/64 loss: 0.0037186145782470703
Batch 37/64 loss: 0.0022101402282714844
Batch 38/64 loss: 0.0010259151458740234
Batch 39/64 loss: 0.0006132125854492188
Batch 40/64 loss: -0.007221817970275879
Batch 41/64 loss: -0.03958630561828613
Batch 42/64 loss: 0.009940505027770996
Batch 43/64 loss: -0.025704145431518555
Batch 44/64 loss: -0.009106040000915527
Batch 45/64 loss: 0.03718143701553345
Batch 46/64 loss: -0.02234065532684326
Batch 47/64 loss: -0.00934147834777832
Batch 48/64 loss: -0.02578604221343994
Batch 49/64 loss: -0.004324018955230713
Batch 50/64 loss: -0.022886991500854492
Batch 51/64 loss: -0.019023478031158447
Batch 52/64 loss: -0.023900389671325684
Batch 53/64 loss: 0.016870617866516113
Batch 54/64 loss: 0.0024306178092956543
Batch 55/64 loss: -0.012191534042358398
Batch 56/64 loss: -0.008710265159606934
Batch 57/64 loss: -0.014560580253601074
Batch 58/64 loss: 0.011722862720489502
Batch 59/64 loss: -0.022428035736083984
Batch 60/64 loss: -0.01334691047668457
Batch 61/64 loss: 0.03618001937866211
Batch 62/64 loss: -0.005440115928649902
Batch 63/64 loss: 0.05745440721511841
Batch 64/64 loss: -0.005327403545379639
Epoch 491  Train loss: -0.002003780299541997  Val loss: 0.12223969538187243
Epoch 492
-------------------------------
Batch 1/64 loss: 0.004095196723937988
Batch 2/64 loss: -0.0029922127723693848
Batch 3/64 loss: -0.02632319927215576
Batch 4/64 loss: -0.014006078243255615
Batch 5/64 loss: 0.003468036651611328
Batch 6/64 loss: -0.00951230525970459
Batch 7/64 loss: -0.020273804664611816
Batch 8/64 loss: 0.00167083740234375
Batch 9/64 loss: -0.008250653743743896
Batch 10/64 loss: 0.001110672950744629
Batch 11/64 loss: -0.029143810272216797
Batch 12/64 loss: 0.013994753360748291
Batch 13/64 loss: -0.0046204328536987305
Batch 14/64 loss: 0.02620452642440796
Batch 15/64 loss: -0.016748428344726562
Batch 16/64 loss: 0.015505075454711914
Batch 17/64 loss: -0.018102288246154785
Batch 18/64 loss: -0.004647672176361084
Batch 19/64 loss: -0.0070917606353759766
Batch 20/64 loss: -0.014568209648132324
Batch 21/64 loss: -0.02857750654220581
Batch 22/64 loss: -0.009683728218078613
Batch 23/64 loss: 0.005948662757873535
Batch 24/64 loss: -0.021402478218078613
Batch 25/64 loss: -0.003815889358520508
Batch 26/64 loss: 0.026427865028381348
Batch 27/64 loss: 0.009457826614379883
Batch 28/64 loss: -0.00963979959487915
Batch 29/64 loss: -0.011608242988586426
Batch 30/64 loss: 0.012749850749969482
Batch 31/64 loss: -0.006424665451049805
Batch 32/64 loss: 0.0041329264640808105
Batch 33/64 loss: -0.00496065616607666
Batch 34/64 loss: 0.0026161670684814453
Batch 35/64 loss: 0.01378720998764038
Batch 36/64 loss: 0.0010355710983276367
Batch 37/64 loss: -0.02674233913421631
Batch 38/64 loss: -0.0015043020248413086
Batch 39/64 loss: 0.007219135761260986
Batch 40/64 loss: 0.016994059085845947
Batch 41/64 loss: -0.038011908531188965
Batch 42/64 loss: 0.0007930994033813477
Batch 43/64 loss: 0.0007596015930175781
Batch 44/64 loss: 0.00602412223815918
Batch 45/64 loss: 0.005297362804412842
Batch 46/64 loss: 0.012423694133758545
Batch 47/64 loss: -0.005308210849761963
Batch 48/64 loss: -0.018591701984405518
Batch 49/64 loss: 0.008040010929107666
Batch 50/64 loss: 0.038653552532196045
Batch 51/64 loss: -0.003110051155090332
Batch 52/64 loss: -0.01867055892944336
Batch 53/64 loss: -0.008836984634399414
Batch 54/64 loss: -0.00936037302017212
Batch 55/64 loss: -0.0036060214042663574
Batch 56/64 loss: -0.008836865425109863
Batch 57/64 loss: 0.0015273094177246094
Batch 58/64 loss: 0.0031206607818603516
Batch 59/64 loss: 0.004125654697418213
Batch 60/64 loss: 0.025881052017211914
Batch 61/64 loss: 0.02670222520828247
Batch 62/64 loss: 0.00800943374633789
Batch 63/64 loss: -0.022147297859191895
Batch 64/64 loss: 0.02623283863067627
Epoch 492  Train loss: -0.0017203083225325042  Val loss: 0.12139063848252968
Epoch 493
-------------------------------
Batch 1/64 loss: -0.0308646559715271
Batch 2/64 loss: 0.02559185028076172
Batch 3/64 loss: -0.027981102466583252
Batch 4/64 loss: -0.04257631301879883
Batch 5/64 loss: -0.01833796501159668
Batch 6/64 loss: -0.008056402206420898
Batch 7/64 loss: -0.0074454545974731445
Batch 8/64 loss: -0.01823943853378296
Batch 9/64 loss: 5.0067901611328125e-05
Batch 10/64 loss: -0.015662312507629395
Batch 11/64 loss: -0.009401559829711914
Batch 12/64 loss: -0.011366486549377441
Batch 13/64 loss: 0.005074262619018555
Batch 14/64 loss: 0.016520142555236816
Batch 15/64 loss: 0.006658971309661865
Batch 16/64 loss: 0.023438572883605957
Batch 17/64 loss: -0.01756840944290161
Batch 18/64 loss: 0.02504909038543701
Batch 19/64 loss: -0.028165161609649658
Batch 20/64 loss: -0.006584763526916504
Batch 21/64 loss: -0.001529693603515625
Batch 22/64 loss: -0.01912456750869751
Batch 23/64 loss: -0.000895380973815918
Batch 24/64 loss: 0.01069551706314087
Batch 25/64 loss: -0.005737423896789551
Batch 26/64 loss: -0.02153956890106201
Batch 27/64 loss: -0.0022125244140625
Batch 28/64 loss: -0.009345650672912598
Batch 29/64 loss: 0.018009543418884277
Batch 30/64 loss: -0.013506531715393066
Batch 31/64 loss: 0.03832292556762695
Batch 32/64 loss: 0.012857437133789062
Batch 33/64 loss: -0.015741288661956787
Batch 34/64 loss: -0.0030684471130371094
Batch 35/64 loss: -0.004630982875823975
Batch 36/64 loss: 0.015317797660827637
Batch 37/64 loss: 0.009499728679656982
Batch 38/64 loss: 0.0379677414894104
Batch 39/64 loss: -0.0025449395179748535
Batch 40/64 loss: 0.018572568893432617
Batch 41/64 loss: -0.0009706020355224609
Batch 42/64 loss: -0.0013480782508850098
Batch 43/64 loss: -0.003628969192504883
Batch 44/64 loss: -0.016092300415039062
Batch 45/64 loss: 0.008723199367523193
Batch 46/64 loss: -0.02474159002304077
Batch 47/64 loss: -0.00351870059967041
Batch 48/64 loss: 0.015269935131072998
Batch 49/64 loss: -0.023949921131134033
Batch 50/64 loss: -0.029467225074768066
Batch 51/64 loss: 0.011494636535644531
Batch 52/64 loss: -0.0023301243782043457
Batch 53/64 loss: 0.006331682205200195
Batch 54/64 loss: -0.017117440700531006
Batch 55/64 loss: 0.02106034755706787
Batch 56/64 loss: -0.011551260948181152
Batch 57/64 loss: 0.008163034915924072
Batch 58/64 loss: 0.015798628330230713
Batch 59/64 loss: 0.004770219326019287
Batch 60/64 loss: -0.0021077990531921387
Batch 61/64 loss: 0.001981496810913086
Batch 62/64 loss: 0.005906105041503906
Batch 63/64 loss: 0.03283989429473877
Batch 64/64 loss: -0.001681208610534668
Epoch 493  Train loss: -0.0013215144475301106  Val loss: 0.12450099851667266
Epoch 494
-------------------------------
Batch 1/64 loss: 0.032525718212127686
Batch 2/64 loss: 0.04847985506057739
Batch 3/64 loss: 0.02660369873046875
Batch 4/64 loss: -0.00044673681259155273
Batch 5/64 loss: -0.005909323692321777
Batch 6/64 loss: -0.014077901840209961
Batch 7/64 loss: 0.04428696632385254
Batch 8/64 loss: -0.009677886962890625
Batch 9/64 loss: 0.011570155620574951
Batch 10/64 loss: 0.006821691989898682
Batch 11/64 loss: -0.00686269998550415
Batch 12/64 loss: -0.004326462745666504
Batch 13/64 loss: -0.02586662769317627
Batch 14/64 loss: -0.011432051658630371
Batch 15/64 loss: 0.008804082870483398
Batch 16/64 loss: -0.02326446771621704
Batch 17/64 loss: -0.027664005756378174
Batch 18/64 loss: -0.031131386756896973
Batch 19/64 loss: -0.01307833194732666
Batch 20/64 loss: -0.001955866813659668
Batch 21/64 loss: -0.011462926864624023
Batch 22/64 loss: 0.012007713317871094
Batch 23/64 loss: 0.018456578254699707
Batch 24/64 loss: -0.03032141923904419
Batch 25/64 loss: -0.0033483505249023438
Batch 26/64 loss: -0.027135133743286133
Batch 27/64 loss: -0.009617447853088379
Batch 28/64 loss: 0.002856135368347168
Batch 29/64 loss: 0.023481428623199463
Batch 30/64 loss: 0.006738007068634033
Batch 31/64 loss: -0.003912091255187988
Batch 32/64 loss: 0.004115879535675049
Batch 33/64 loss: -0.0010933279991149902
Batch 34/64 loss: 0.007342219352722168
Batch 35/64 loss: 0.040494322776794434
Batch 36/64 loss: 0.011541247367858887
Batch 37/64 loss: -0.0038458704948425293
Batch 38/64 loss: 0.001369476318359375
Batch 39/64 loss: -0.0100022554397583
Batch 40/64 loss: -0.017116844654083252
Batch 41/64 loss: 0.014280200004577637
Batch 42/64 loss: -0.015212297439575195
Batch 43/64 loss: -0.01727849245071411
Batch 44/64 loss: 0.002528667449951172
Batch 45/64 loss: -0.0067441463470458984
Batch 46/64 loss: 0.009538769721984863
Batch 47/64 loss: 0.0025205016136169434
Batch 48/64 loss: -0.017900943756103516
Batch 49/64 loss: -0.0043364763259887695
Batch 50/64 loss: -0.026627063751220703
Batch 51/64 loss: -0.022138774394989014
Batch 52/64 loss: 0.0004788637161254883
Batch 53/64 loss: 0.03227865695953369
Batch 54/64 loss: -0.00035446882247924805
Batch 55/64 loss: -0.01741337776184082
Batch 56/64 loss: 0.020987272262573242
Batch 57/64 loss: -0.0160520076751709
Batch 58/64 loss: -0.024687767028808594
Batch 59/64 loss: -0.013352453708648682
Batch 60/64 loss: 0.01568204164505005
Batch 61/64 loss: 0.020681321620941162
Batch 62/64 loss: 0.00783008337020874
Batch 63/64 loss: 0.0033522844314575195
Batch 64/64 loss: -0.0001285076141357422
Epoch 494  Train loss: -0.0005974937887752757  Val loss: 0.12144804123750667
Epoch 495
-------------------------------
Batch 1/64 loss: 0.044909656047821045
Batch 2/64 loss: -0.0036977529525756836
Batch 3/64 loss: -0.00988847017288208
Batch 4/64 loss: -0.022495746612548828
Batch 5/64 loss: -0.002635478973388672
Batch 6/64 loss: -0.00583493709564209
Batch 7/64 loss: 0.00546795129776001
Batch 8/64 loss: 0.018095195293426514
Batch 9/64 loss: 0.017364084720611572
Batch 10/64 loss: -0.017670929431915283
Batch 11/64 loss: -0.031130552291870117
Batch 12/64 loss: -0.014798283576965332
Batch 13/64 loss: -0.019640207290649414
Batch 14/64 loss: -0.004154562950134277
Batch 15/64 loss: 0.003954052925109863
Batch 16/64 loss: 0.01687908172607422
Batch 17/64 loss: -0.002015352249145508
Batch 18/64 loss: -0.015829741954803467
Batch 19/64 loss: 0.0016765594482421875
Batch 20/64 loss: -0.02211982011795044
Batch 21/64 loss: -0.008347749710083008
Batch 22/64 loss: 4.9173831939697266e-05
Batch 23/64 loss: -0.034886300563812256
Batch 24/64 loss: -0.008542418479919434
Batch 25/64 loss: -0.01695793867111206
Batch 26/64 loss: -0.012201011180877686
Batch 27/64 loss: -0.025364279747009277
Batch 28/64 loss: -0.03289985656738281
Batch 29/64 loss: -0.0075637102127075195
Batch 30/64 loss: -0.005803406238555908
Batch 31/64 loss: 0.0015186667442321777
Batch 32/64 loss: -0.0017480254173278809
Batch 33/64 loss: 0.0004870295524597168
Batch 34/64 loss: 0.014800190925598145
Batch 35/64 loss: -0.006278693675994873
Batch 36/64 loss: 0.01842474937438965
Batch 37/64 loss: 0.015081167221069336
Batch 38/64 loss: -0.01750361919403076
Batch 39/64 loss: -0.005395174026489258
Batch 40/64 loss: 0.016116797924041748
Batch 41/64 loss: -0.019632935523986816
Batch 42/64 loss: 0.0001976490020751953
Batch 43/64 loss: -0.002938985824584961
Batch 44/64 loss: 0.02147090435028076
Batch 45/64 loss: 0.014466166496276855
Batch 46/64 loss: -0.011678099632263184
Batch 47/64 loss: 0.001968860626220703
Batch 48/64 loss: -0.004810154438018799
Batch 49/64 loss: 0.020944058895111084
Batch 50/64 loss: 0.0063315629959106445
Batch 51/64 loss: 0.026763200759887695
Batch 52/64 loss: -0.012953102588653564
Batch 53/64 loss: -0.0006662607192993164
Batch 54/64 loss: -0.014043092727661133
Batch 55/64 loss: -0.001750349998474121
Batch 56/64 loss: 0.032629966735839844
Batch 57/64 loss: -0.012504756450653076
Batch 58/64 loss: 0.02605670690536499
Batch 59/64 loss: -0.00264585018157959
Batch 60/64 loss: 0.027467668056488037
Batch 61/64 loss: 0.009266912937164307
Batch 62/64 loss: 0.03996390104293823
Batch 63/64 loss: 0.0158158540725708
Batch 64/64 loss: -0.04386621713638306
Epoch 495  Train loss: -0.0008432862805385215  Val loss: 0.12098232946035378
Epoch 496
-------------------------------
Batch 1/64 loss: 0.0015519261360168457
Batch 2/64 loss: -0.013733148574829102
Batch 3/64 loss: -0.009508490562438965
Batch 4/64 loss: -0.0036998987197875977
Batch 5/64 loss: -0.008485913276672363
Batch 6/64 loss: -0.024896085262298584
Batch 7/64 loss: -0.01550835371017456
Batch 8/64 loss: -0.008073687553405762
Batch 9/64 loss: 0.015149295330047607
Batch 10/64 loss: 0.0003802776336669922
Batch 11/64 loss: 0.028832674026489258
Batch 12/64 loss: -0.0025754570960998535
Batch 13/64 loss: -0.012728393077850342
Batch 14/64 loss: -0.008762121200561523
Batch 15/64 loss: 0.0002834796905517578
Batch 16/64 loss: 0.01805025339126587
Batch 17/64 loss: 0.007645726203918457
Batch 18/64 loss: 0.01893395185470581
Batch 19/64 loss: 0.026127219200134277
Batch 20/64 loss: 0.002902507781982422
Batch 21/64 loss: -0.033799052238464355
Batch 22/64 loss: -0.020872890949249268
Batch 23/64 loss: -0.006261110305786133
Batch 24/64 loss: -0.028676748275756836
Batch 25/64 loss: -0.009867429733276367
Batch 26/64 loss: -0.014368891716003418
Batch 27/64 loss: -0.00154954195022583
Batch 28/64 loss: 0.01812809705734253
Batch 29/64 loss: -0.008498191833496094
Batch 30/64 loss: 0.015013933181762695
Batch 31/64 loss: 0.0044476985931396484
Batch 32/64 loss: -0.005212664604187012
Batch 33/64 loss: -0.0007500052452087402
Batch 34/64 loss: -0.016356945037841797
Batch 35/64 loss: 0.01492762565612793
Batch 36/64 loss: 0.02499312162399292
Batch 37/64 loss: -0.021796584129333496
Batch 38/64 loss: -0.006685078144073486
Batch 39/64 loss: 0.007626056671142578
Batch 40/64 loss: -0.01220935583114624
Batch 41/64 loss: -0.03808176517486572
Batch 42/64 loss: 0.03715568780899048
Batch 43/64 loss: 0.0093039870262146
Batch 44/64 loss: 0.02046811580657959
Batch 45/64 loss: -0.0008489489555358887
Batch 46/64 loss: 0.037109971046447754
Batch 47/64 loss: -0.013772547245025635
Batch 48/64 loss: 0.028596460819244385
Batch 49/64 loss: 0.0018081068992614746
Batch 50/64 loss: -0.0001913309097290039
Batch 51/64 loss: 0.0007560253143310547
Batch 52/64 loss: -0.00895535945892334
Batch 53/64 loss: 0.018314599990844727
Batch 54/64 loss: 0.008681893348693848
Batch 55/64 loss: -0.009651422500610352
Batch 56/64 loss: -0.0025638341903686523
Batch 57/64 loss: -0.02770775556564331
Batch 58/64 loss: -0.01862657070159912
Batch 59/64 loss: 8.845329284667969e-05
Batch 60/64 loss: -0.010464787483215332
Batch 61/64 loss: 0.010670483112335205
Batch 62/64 loss: 0.008406996726989746
Batch 63/64 loss: 0.029659688472747803
Batch 64/64 loss: -0.016520380973815918
Epoch 496  Train loss: -0.00034692287445068357  Val loss: 0.12322723701647467
Epoch 497
-------------------------------
Batch 1/64 loss: -0.014487206935882568
Batch 2/64 loss: -0.010618269443511963
Batch 3/64 loss: -0.02020549774169922
Batch 4/64 loss: 0.005511283874511719
Batch 5/64 loss: -0.017481088638305664
Batch 6/64 loss: -0.021986663341522217
Batch 7/64 loss: -0.022981762886047363
Batch 8/64 loss: -0.0027060508728027344
Batch 9/64 loss: 0.009372353553771973
Batch 10/64 loss: 0.0068604350090026855
Batch 11/64 loss: 0.0067258477210998535
Batch 12/64 loss: 0.0034537911415100098
Batch 13/64 loss: -0.002406477928161621
Batch 14/64 loss: -0.046058058738708496
Batch 15/64 loss: 0.018440425395965576
Batch 16/64 loss: -0.019141554832458496
Batch 17/64 loss: -0.0016796588897705078
Batch 18/64 loss: 0.007633686065673828
Batch 19/64 loss: -0.01806199550628662
Batch 20/64 loss: -0.018221378326416016
Batch 21/64 loss: 0.04242062568664551
Batch 22/64 loss: -0.00019538402557373047
Batch 23/64 loss: -0.00831913948059082
Batch 24/64 loss: 0.018072128295898438
Batch 25/64 loss: 0.006817340850830078
Batch 26/64 loss: -0.018078207969665527
Batch 27/64 loss: 0.006884574890136719
Batch 28/64 loss: -0.015126526355743408
Batch 29/64 loss: 0.009792625904083252
Batch 30/64 loss: 0.04069489240646362
Batch 31/64 loss: 0.0014688968658447266
Batch 32/64 loss: -0.010717511177062988
Batch 33/64 loss: -0.018574535846710205
Batch 34/64 loss: -0.007732748985290527
Batch 35/64 loss: 0.018976807594299316
Batch 36/64 loss: -0.0020055174827575684
Batch 37/64 loss: 0.00414353609085083
Batch 38/64 loss: -0.0113450288772583
Batch 39/64 loss: -0.018626511096954346
Batch 40/64 loss: -0.007562816143035889
Batch 41/64 loss: -0.0008741021156311035
Batch 42/64 loss: -0.0014203786849975586
Batch 43/64 loss: -0.013064563274383545
Batch 44/64 loss: -0.018692493438720703
Batch 45/64 loss: 0.005110621452331543
Batch 46/64 loss: -0.021795332431793213
Batch 47/64 loss: 0.027888834476470947
Batch 48/64 loss: 0.01089543104171753
Batch 49/64 loss: -0.0059580206871032715
Batch 50/64 loss: 0.012821733951568604
Batch 51/64 loss: 0.013862669467926025
Batch 52/64 loss: -0.02791774272918701
Batch 53/64 loss: 0.02732408046722412
Batch 54/64 loss: 0.028125464916229248
Batch 55/64 loss: -0.008047938346862793
Batch 56/64 loss: -0.012770354747772217
Batch 57/64 loss: -0.014576971530914307
Batch 58/64 loss: -0.010207653045654297
Batch 59/64 loss: 0.018778085708618164
Batch 60/64 loss: -0.010266363620758057
Batch 61/64 loss: -0.00621110200881958
Batch 62/64 loss: 0.001148521900177002
Batch 63/64 loss: 0.012536704540252686
Batch 64/64 loss: -0.006988525390625
Epoch 497  Train loss: -0.001970236909155752  Val loss: 0.12155790468261823
Epoch 498
-------------------------------
Batch 1/64 loss: 0.003771066665649414
Batch 2/64 loss: -0.022197604179382324
Batch 3/64 loss: -0.03219425678253174
Batch 4/64 loss: 0.0029349327087402344
Batch 5/64 loss: -0.028044283390045166
Batch 6/64 loss: 0.02434772253036499
Batch 7/64 loss: -0.016648292541503906
Batch 8/64 loss: -0.013863205909729004
Batch 9/64 loss: 0.017931997776031494
Batch 10/64 loss: 0.03081202507019043
Batch 11/64 loss: 0.002287447452545166
Batch 12/64 loss: 0.02450859546661377
Batch 13/64 loss: 0.0016078352928161621
Batch 14/64 loss: -0.010437965393066406
Batch 15/64 loss: 0.0024526119232177734
Batch 16/64 loss: 0.023144781589508057
Batch 17/64 loss: -0.01402735710144043
Batch 18/64 loss: 0.006326913833618164
Batch 19/64 loss: 0.018415212631225586
Batch 20/64 loss: -0.011835217475891113
Batch 21/64 loss: 0.018367350101470947
Batch 22/64 loss: -0.017751038074493408
Batch 23/64 loss: 0.02076667547225952
Batch 24/64 loss: 0.007986783981323242
Batch 25/64 loss: -0.02562814950942993
Batch 26/64 loss: -0.0022316575050354004
Batch 27/64 loss: -0.00861811637878418
Batch 28/64 loss: -0.007364869117736816
Batch 29/64 loss: -0.024688363075256348
Batch 30/64 loss: -0.027373194694519043
Batch 31/64 loss: 0.03771066665649414
Batch 32/64 loss: -0.015275657176971436
Batch 33/64 loss: 0.003336489200592041
Batch 34/64 loss: -0.012242376804351807
Batch 35/64 loss: 0.048606693744659424
Batch 36/64 loss: -0.01594775915145874
Batch 37/64 loss: -0.013793349266052246
Batch 38/64 loss: -0.007644057273864746
Batch 39/64 loss: -0.013643383979797363
Batch 40/64 loss: 0.032479166984558105
Batch 41/64 loss: 0.019152581691741943
Batch 42/64 loss: 0.013551712036132812
Batch 43/64 loss: -0.014553546905517578
Batch 44/64 loss: 0.014424324035644531
Batch 45/64 loss: 0.005330324172973633
Batch 46/64 loss: 0.003773629665374756
Batch 47/64 loss: 0.010894417762756348
Batch 48/64 loss: -0.01197892427444458
Batch 49/64 loss: -0.016805708408355713
Batch 50/64 loss: -0.012629687786102295
Batch 51/64 loss: -0.001526176929473877
Batch 52/64 loss: -0.013923168182373047
Batch 53/64 loss: -0.023069679737091064
Batch 54/64 loss: -0.0002295374870300293
Batch 55/64 loss: -0.021836042404174805
Batch 56/64 loss: 0.0027390718460083008
Batch 57/64 loss: -0.0048438310623168945
Batch 58/64 loss: -0.018513977527618408
Batch 59/64 loss: 0.0026630163192749023
Batch 60/64 loss: 0.006429791450500488
Batch 61/64 loss: -0.01725214719772339
Batch 62/64 loss: -0.00869894027709961
Batch 63/64 loss: 0.007353365421295166
Batch 64/64 loss: 0.017436504364013672
Epoch 498  Train loss: -0.0012568931953579772  Val loss: 0.12181056651872457
Epoch 499
-------------------------------
Batch 1/64 loss: 0.0036525726318359375
Batch 2/64 loss: 0.0014389753341674805
Batch 3/64 loss: 0.030680418014526367
Batch 4/64 loss: -0.003331601619720459
Batch 5/64 loss: 0.016293585300445557
Batch 6/64 loss: -0.0112992525100708
Batch 7/64 loss: -0.006885409355163574
Batch 8/64 loss: -0.004418909549713135
Batch 9/64 loss: 0.029341161251068115
Batch 10/64 loss: -0.005576610565185547
Batch 11/64 loss: -0.024149179458618164
Batch 12/64 loss: 0.039077579975128174
Batch 13/64 loss: -0.012568235397338867
Batch 14/64 loss: -0.015760958194732666
Batch 15/64 loss: -0.010478019714355469
Batch 16/64 loss: 0.00840693712234497
Batch 17/64 loss: -0.02144014835357666
Batch 18/64 loss: -0.002473592758178711
Batch 19/64 loss: -0.013981521129608154
Batch 20/64 loss: -0.0038420557975769043
Batch 21/64 loss: -0.0016439557075500488
Batch 22/64 loss: -0.0009770393371582031
Batch 23/64 loss: -0.016824841499328613
Batch 24/64 loss: -0.01150655746459961
Batch 25/64 loss: -0.010094881057739258
Batch 26/64 loss: -0.015768706798553467
Batch 27/64 loss: -0.03269320726394653
Batch 28/64 loss: -0.0139809250831604
Batch 29/64 loss: -0.01139742136001587
Batch 30/64 loss: 0.0011205077171325684
Batch 31/64 loss: -0.008284151554107666
Batch 32/64 loss: -0.0020908117294311523
Batch 33/64 loss: -0.026064515113830566
Batch 34/64 loss: -0.012439191341400146
Batch 35/64 loss: 0.005022108554840088
Batch 36/64 loss: 0.015340805053710938
Batch 37/64 loss: 0.028744637966156006
Batch 38/64 loss: -0.04323774576187134
Batch 39/64 loss: 0.029477953910827637
Batch 40/64 loss: -0.03213059902191162
Batch 41/64 loss: 0.0339808464050293
Batch 42/64 loss: -0.03020387887954712
Batch 43/64 loss: -0.008875489234924316
Batch 44/64 loss: 0.003000974655151367
Batch 45/64 loss: -0.02950996160507202
Batch 46/64 loss: -0.010812222957611084
Batch 47/64 loss: 0.022398948669433594
Batch 48/64 loss: -0.004464685916900635
Batch 49/64 loss: -0.012445807456970215
Batch 50/64 loss: 0.01578623056411743
Batch 51/64 loss: 0.005045890808105469
Batch 52/64 loss: 0.01197880506515503
Batch 53/64 loss: -0.007970571517944336
Batch 54/64 loss: -0.030678510665893555
Batch 55/64 loss: 0.009296774864196777
Batch 56/64 loss: 0.027413368225097656
Batch 57/64 loss: -0.01333552598953247
Batch 58/64 loss: 0.022256195545196533
Batch 59/64 loss: 0.016823172569274902
Batch 60/64 loss: -0.009329557418823242
Batch 61/64 loss: -0.03375893831253052
Batch 62/64 loss: 0.031787216663360596
Batch 63/64 loss: -0.0188448429107666
Batch 64/64 loss: 0.04156339168548584
Epoch 499  Train loss: -0.0022906953213261624  Val loss: 0.12281023800577905
Epoch 500
-------------------------------
Batch 1/64 loss: -0.019610226154327393
Batch 2/64 loss: 0.004875838756561279
Batch 3/64 loss: -0.006894826889038086
Batch 4/64 loss: -0.01372915506362915
Batch 5/64 loss: -0.015200614929199219
Batch 6/64 loss: 0.013122797012329102
Batch 7/64 loss: -0.03057175874710083
Batch 8/64 loss: 0.01973867416381836
Batch 9/64 loss: 0.013478636741638184
Batch 10/64 loss: 0.007581651210784912
Batch 11/64 loss: -0.013412714004516602
Batch 12/64 loss: -0.016960442066192627
Batch 13/64 loss: 0.0010332465171813965
Batch 14/64 loss: -0.01415020227432251
Batch 15/64 loss: 0.020083308219909668
Batch 16/64 loss: -0.0031853914260864258
Batch 17/64 loss: -0.02509009838104248
Batch 18/64 loss: 0.021020174026489258
Batch 19/64 loss: -0.025513291358947754
Batch 20/64 loss: -0.01367419958114624
Batch 21/64 loss: -0.0007548332214355469
Batch 22/64 loss: 0.019176483154296875
Batch 23/64 loss: -0.01758754253387451
Batch 24/64 loss: 0.026442289352416992
Batch 25/64 loss: -0.034012675285339355
Batch 26/64 loss: 0.00044971704483032227
Batch 27/64 loss: 0.019009888172149658
Batch 28/64 loss: 0.00567626953125
Batch 29/64 loss: -0.0013592243194580078
Batch 30/64 loss: 0.005487978458404541
Batch 31/64 loss: -0.012199044227600098
Batch 32/64 loss: -0.021122634410858154
Batch 33/64 loss: -0.012648046016693115
Batch 34/64 loss: -0.0192335844039917
Batch 35/64 loss: -0.01165306568145752
Batch 36/64 loss: -0.008949995040893555
Batch 37/64 loss: 0.013039231300354004
Batch 38/64 loss: -0.01509547233581543
Batch 39/64 loss: -0.01651167869567871
Batch 40/64 loss: 0.0020887255668640137
Batch 41/64 loss: -0.021824419498443604
Batch 42/64 loss: -0.021364271640777588
Batch 43/64 loss: -0.0268784761428833
Batch 44/64 loss: 0.0071877241134643555
Batch 45/64 loss: 0.005653262138366699
Batch 46/64 loss: -0.010651171207427979
Batch 47/64 loss: -0.03590601682662964
Batch 48/64 loss: -0.003076314926147461
Batch 49/64 loss: 0.01723933219909668
Batch 50/64 loss: 0.005193054676055908
Batch 51/64 loss: 0.020757615566253662
Batch 52/64 loss: -0.01088494062423706
Batch 53/64 loss: -0.0316694974899292
Batch 54/64 loss: 0.024064302444458008
Batch 55/64 loss: 0.0010704994201660156
Batch 56/64 loss: 0.012645244598388672
Batch 57/64 loss: 0.03802347183227539
Batch 58/64 loss: -0.01306450366973877
Batch 59/64 loss: 0.0035228729248046875
Batch 60/64 loss: -0.002094268798828125
Batch 61/64 loss: 0.0016707777976989746
Batch 62/64 loss: 0.010193407535552979
Batch 63/64 loss: -0.00736081600189209
Batch 64/64 loss: 0.018944084644317627
Epoch 500  Train loss: -0.003139778445748722  Val loss: 0.12071747099820691
SLIC undersegmentation error: 0.12412920962199316
SLIC inter-cluster variation: 0.13904419774313004
SLIC number of superpixels: 21483
SLIC superpixels per image: 73.82474226804123
Model loaded
Test metrics:
0.11024822013074999 0.3231450171821306 25.00617633749752 tensor(0.2687, dtype=torch.float64) 0.8938592330426534 5.066078009267705 27407
Inference time: 0.002596842054648907 seconds
Relabeled undersegmentation error: 0.07667353951890034
Relabeled inter-cluster variation: 0.03229780887005841
Relabeled mean superpixels count: 477.1340206185567
Original mean superpixels count: 94.18213058419244
Done!
Job id: 420571
Job id: 422902
