Started preprocessing dataset
Number of training samples: 2040
Number of validation samples: 582
Number of testing samples: 291
Using cuda device
Epoch 1
-------------------------------
Batch 1/64 loss: 4.644289493560791
Batch 2/64 loss: 4.6687517166137695
Batch 3/64 loss: 4.625485897064209
Batch 4/64 loss: 4.621085166931152
Batch 5/64 loss: 4.623441219329834
Batch 6/64 loss: 4.609669208526611
Batch 7/64 loss: 4.6089935302734375
Batch 8/64 loss: 4.603084087371826
Batch 9/64 loss: 4.597145080566406
Batch 10/64 loss: 4.599065780639648
Batch 11/64 loss: 4.594170570373535
Batch 12/64 loss: 4.592911243438721
Batch 13/64 loss: 4.591066360473633
Batch 14/64 loss: 4.595184803009033
Batch 15/64 loss: 4.590997219085693
Batch 16/64 loss: 4.592930793762207
Batch 17/64 loss: 4.58644437789917
Batch 18/64 loss: 4.5862650871276855
Batch 19/64 loss: 4.586391448974609
Batch 20/64 loss: 4.590076446533203
Batch 21/64 loss: 4.588493824005127
Batch 22/64 loss: 4.584644317626953
Batch 23/64 loss: 4.582306861877441
Batch 24/64 loss: 4.581621170043945
Batch 25/64 loss: 4.585258483886719
Batch 26/64 loss: 4.5824294090271
Batch 27/64 loss: 4.582420825958252
Batch 28/64 loss: 4.5823469161987305
Batch 29/64 loss: 4.581462383270264
Batch 30/64 loss: 4.580185890197754
Batch 31/64 loss: 4.5817437171936035
Batch 32/64 loss: 4.5808892250061035
Batch 33/64 loss: 4.581786155700684
Batch 34/64 loss: 4.58107328414917
Batch 35/64 loss: 4.580172061920166
Batch 36/64 loss: 4.581625938415527
Batch 37/64 loss: 4.580366134643555
Batch 38/64 loss: 4.58004093170166
Batch 39/64 loss: 4.5799360275268555
Batch 40/64 loss: 4.579310417175293
Batch 41/64 loss: 4.579931259155273
Batch 42/64 loss: 4.579765796661377
Batch 43/64 loss: 4.579851150512695
Batch 44/64 loss: 4.579545497894287
Batch 45/64 loss: 4.579627513885498
Batch 46/64 loss: 4.578678131103516
Batch 47/64 loss: 4.5789594650268555
Batch 48/64 loss: 4.579949855804443
Batch 49/64 loss: 4.579682350158691
Batch 50/64 loss: 4.57887077331543
Batch 51/64 loss: 4.579593181610107
Batch 52/64 loss: 4.579084873199463
Batch 53/64 loss: 4.5791850090026855
Batch 54/64 loss: 4.57853889465332
Batch 55/64 loss: 4.5794997215271
Batch 56/64 loss: 4.57990026473999
Batch 57/64 loss: 4.579349040985107
Batch 58/64 loss: 4.578581809997559
Batch 59/64 loss: 4.578979015350342
Batch 60/64 loss: 4.579016208648682
Batch 61/64 loss: 4.578736305236816
Batch 62/64 loss: 4.578732013702393
Batch 63/64 loss: 4.579474925994873
Batch 64/64 loss: 3.793609142303467
Epoch 1  Train loss: 4.579047896815282  Val loss: 4.543436629665676
Saving best model, epoch: 1
Epoch 2
-------------------------------
Batch 1/64 loss: 4.5785746574401855
Batch 2/64 loss: 4.578186988830566
Batch 3/64 loss: 4.5784149169921875
Batch 4/64 loss: 4.582289695739746
Batch 5/64 loss: 4.5776286125183105
Batch 6/64 loss: 4.578174591064453
Batch 7/64 loss: 4.577815055847168
Batch 8/64 loss: 4.580167293548584
Batch 9/64 loss: 4.579296588897705
Batch 10/64 loss: 4.5783467292785645
Batch 11/64 loss: 4.5780029296875
Batch 12/64 loss: 4.578675270080566
Batch 13/64 loss: 4.577276229858398
Batch 14/64 loss: 4.578757286071777
Batch 15/64 loss: 4.577821731567383
Batch 16/64 loss: 4.578490734100342
Batch 17/64 loss: 4.578242778778076
Batch 18/64 loss: 4.577932357788086
Batch 19/64 loss: 4.578085422515869
Batch 20/64 loss: 4.577391624450684
Batch 21/64 loss: 4.576821804046631
Batch 22/64 loss: 4.578390121459961
Batch 23/64 loss: 4.576611042022705
Batch 24/64 loss: 4.576715469360352
Batch 25/64 loss: 4.576704025268555
Batch 26/64 loss: 4.575164794921875
Batch 27/64 loss: 4.57671594619751
Batch 28/64 loss: 4.575103759765625
Batch 29/64 loss: 4.577602863311768
Batch 30/64 loss: 4.576278209686279
Batch 31/64 loss: 4.577134609222412
Batch 32/64 loss: 4.576462268829346
Batch 33/64 loss: 4.577248573303223
Batch 34/64 loss: 4.575675964355469
Batch 35/64 loss: 4.573025226593018
Batch 36/64 loss: 4.576825141906738
Batch 37/64 loss: 4.577909469604492
Batch 38/64 loss: 4.575803756713867
Batch 39/64 loss: 4.574924468994141
Batch 40/64 loss: 4.576538562774658
Batch 41/64 loss: 4.5776567459106445
Batch 42/64 loss: 4.575566291809082
Batch 43/64 loss: 4.570940971374512
Batch 44/64 loss: 4.5729570388793945
Batch 45/64 loss: 4.572505950927734
Batch 46/64 loss: 4.580272197723389
Batch 47/64 loss: 4.572455406188965
Batch 48/64 loss: 4.570108413696289
Batch 49/64 loss: 4.571852207183838
Batch 50/64 loss: 4.571704864501953
Batch 51/64 loss: 4.569900035858154
Batch 52/64 loss: 4.565963268280029
Batch 53/64 loss: 4.5656914710998535
Batch 54/64 loss: 4.565918922424316
Batch 55/64 loss: 4.566795825958252
Batch 56/64 loss: 4.563271999359131
Batch 57/64 loss: 4.563513278961182
Batch 58/64 loss: 4.559491157531738
Batch 59/64 loss: 4.573625087738037
Batch 60/64 loss: 4.563259124755859
Batch 61/64 loss: 4.5733137130737305
Batch 62/64 loss: 4.562893867492676
Batch 63/64 loss: 4.552964687347412
Batch 64/64 loss: 3.7462635040283203
Epoch 2  Train loss: 4.564541925168505  Val loss: 4.610849501341069
Epoch 3
-------------------------------
Batch 1/64 loss: 4.600753307342529
Batch 2/64 loss: 4.557016849517822
Batch 3/64 loss: 4.563708782196045
Batch 4/64 loss: 4.5809454917907715
Batch 5/64 loss: 4.580584526062012
Batch 6/64 loss: 4.551673889160156
Batch 7/64 loss: 4.565188407897949
Batch 8/64 loss: 4.550466537475586
Batch 9/64 loss: 4.5517778396606445
Batch 10/64 loss: 4.548211574554443
Batch 11/64 loss: 4.555314540863037
Batch 12/64 loss: 4.575184345245361
Batch 13/64 loss: 4.548079967498779
Batch 14/64 loss: 4.553593158721924
Batch 15/64 loss: 4.530275821685791
Batch 16/64 loss: 4.522721290588379
Batch 17/64 loss: 4.5717597007751465
Batch 18/64 loss: 4.534141540527344
Batch 19/64 loss: 4.533154010772705
Batch 20/64 loss: 4.518223762512207
Batch 21/64 loss: 4.497385501861572
Batch 22/64 loss: 4.534160614013672
Batch 23/64 loss: 4.488790035247803
Batch 24/64 loss: 4.476975917816162
Batch 25/64 loss: 4.470371723175049
Batch 26/64 loss: 4.518989562988281
Batch 27/64 loss: 4.466977119445801
Batch 28/64 loss: 4.4808454513549805
Batch 29/64 loss: 4.442967891693115
Batch 30/64 loss: 4.45436429977417
Batch 31/64 loss: 4.458252906799316
Batch 32/64 loss: 4.448066234588623
Batch 33/64 loss: 4.427483081817627
Batch 34/64 loss: 4.567688465118408
Batch 35/64 loss: 4.467492580413818
Batch 36/64 loss: 4.424598217010498
Batch 37/64 loss: 4.48555326461792
Batch 38/64 loss: 4.457132816314697
Batch 39/64 loss: 4.454862594604492
Batch 40/64 loss: 4.383674621582031
Batch 41/64 loss: 4.455798149108887
Batch 42/64 loss: 4.393187046051025
Batch 43/64 loss: 4.439163684844971
Batch 44/64 loss: 4.387331008911133
Batch 45/64 loss: 4.3365864753723145
Batch 46/64 loss: 4.432557582855225
Batch 47/64 loss: 4.526719570159912
Batch 48/64 loss: 4.400317192077637
Batch 49/64 loss: 4.375241279602051
Batch 50/64 loss: 4.389023780822754
Batch 51/64 loss: 4.367143630981445
Batch 52/64 loss: 4.399414539337158
Batch 53/64 loss: 4.403225421905518
Batch 54/64 loss: 4.419373512268066
Batch 55/64 loss: 4.356477737426758
Batch 56/64 loss: 4.447356700897217
Batch 57/64 loss: 4.361453533172607
Batch 58/64 loss: 4.382673263549805
Batch 59/64 loss: 4.397128105163574
Batch 60/64 loss: 4.266012191772461
Batch 61/64 loss: 4.324331283569336
Batch 62/64 loss: 4.270442962646484
Batch 63/64 loss: 4.3799028396606445
Batch 64/64 loss: 3.303401470184326
Epoch 3  Train loss: 4.453142287684422  Val loss: 4.3323929326231125
Saving best model, epoch: 3
Epoch 4
-------------------------------
Batch 1/64 loss: 4.271751403808594
Batch 2/64 loss: 4.332072734832764
Batch 3/64 loss: 4.313963413238525
Batch 4/64 loss: 4.351316452026367
Batch 5/64 loss: 4.298705101013184
Batch 6/64 loss: 4.237734317779541
Batch 7/64 loss: 4.2808918952941895
Batch 8/64 loss: 4.278522491455078
Batch 9/64 loss: 4.19252347946167
Batch 10/64 loss: 4.2479472160339355
Batch 11/64 loss: 4.171937465667725
Batch 12/64 loss: 4.287811756134033
Batch 13/64 loss: 4.241208076477051
Batch 14/64 loss: 4.193012237548828
Batch 15/64 loss: 4.144213676452637
Batch 16/64 loss: 4.126491069793701
Batch 17/64 loss: 4.118194580078125
Batch 18/64 loss: 4.109776496887207
Batch 19/64 loss: 4.108011245727539
Batch 20/64 loss: 4.110780715942383
Batch 21/64 loss: 4.2830047607421875
Batch 22/64 loss: 4.119585037231445
Batch 23/64 loss: 4.1761088371276855
Batch 24/64 loss: 4.111946105957031
Batch 25/64 loss: 4.043688774108887
Batch 26/64 loss: 4.091786861419678
Batch 27/64 loss: 4.219120502471924
Batch 28/64 loss: 4.039551258087158
Batch 29/64 loss: 4.102119445800781
Batch 30/64 loss: 4.0866169929504395
Batch 31/64 loss: 4.1131486892700195
Batch 32/64 loss: 4.082841396331787
Batch 33/64 loss: 4.130553722381592
Batch 34/64 loss: 4.039189338684082
Batch 35/64 loss: 4.098033905029297
Batch 36/64 loss: 4.127198219299316
Batch 37/64 loss: 4.012166976928711
Batch 38/64 loss: 4.008213520050049
Batch 39/64 loss: 4.089563846588135
Batch 40/64 loss: 4.026232719421387
Batch 41/64 loss: 3.992086410522461
Batch 42/64 loss: 4.054200172424316
Batch 43/64 loss: 4.110000133514404
Batch 44/64 loss: 4.133199691772461
Batch 45/64 loss: 4.015997886657715
Batch 46/64 loss: 4.0680317878723145
Batch 47/64 loss: 4.020443916320801
Batch 48/64 loss: 4.10830020904541
Batch 49/64 loss: 4.000901699066162
Batch 50/64 loss: 3.9493274688720703
Batch 51/64 loss: 4.069423675537109
Batch 52/64 loss: 4.027841091156006
Batch 53/64 loss: 3.8873167037963867
Batch 54/64 loss: 4.0357160568237305
Batch 55/64 loss: 3.8640236854553223
Batch 56/64 loss: 4.087803840637207
Batch 57/64 loss: 4.0474982261657715
Batch 58/64 loss: 3.942575454711914
Batch 59/64 loss: 4.040586948394775
Batch 60/64 loss: 3.989924907684326
Batch 61/64 loss: 4.053956031799316
Batch 62/64 loss: 3.867720127105713
Batch 63/64 loss: 3.8620288372039795
Batch 64/64 loss: 2.471703290939331
Epoch 4  Train loss: 4.086277947706335  Val loss: 3.919058445802669
Saving best model, epoch: 4
Epoch 5
-------------------------------
Batch 1/64 loss: 3.997661828994751
Batch 2/64 loss: 4.02001953125
Batch 3/64 loss: 3.940648078918457
Batch 4/64 loss: 3.863746404647827
Batch 5/64 loss: 3.855800151824951
Batch 6/64 loss: 3.820736885070801
Batch 7/64 loss: 3.9360604286193848
Batch 8/64 loss: 3.8004894256591797
Batch 9/64 loss: 3.912594795227051
Batch 10/64 loss: 3.7866554260253906
Batch 11/64 loss: 3.875117778778076
Batch 12/64 loss: 3.8280820846557617
Batch 13/64 loss: 3.8980674743652344
Batch 14/64 loss: 3.896486520767212
Batch 15/64 loss: 3.9148130416870117
Batch 16/64 loss: 3.7525062561035156
Batch 17/64 loss: 3.745304584503174
Batch 18/64 loss: 3.785264492034912
Batch 19/64 loss: 3.7583296298980713
Batch 20/64 loss: 3.954123020172119
Batch 21/64 loss: 3.730055809020996
Batch 22/64 loss: 3.861783742904663
Batch 23/64 loss: 3.9293317794799805
Batch 24/64 loss: 3.75445818901062
Batch 25/64 loss: 3.815547466278076
Batch 26/64 loss: 3.6438050270080566
Batch 27/64 loss: 3.8186533451080322
Batch 28/64 loss: 3.8605711460113525
Batch 29/64 loss: 3.946333885192871
Batch 30/64 loss: 3.6809020042419434
Batch 31/64 loss: 3.7262895107269287
Batch 32/64 loss: 3.6534934043884277
Batch 33/64 loss: 3.762742280960083
Batch 34/64 loss: 3.752880096435547
Batch 35/64 loss: 3.6982498168945312
Batch 36/64 loss: 3.7860708236694336
Batch 37/64 loss: 3.854410171508789
Batch 38/64 loss: 3.62069034576416
Batch 39/64 loss: 3.825608253479004
Batch 40/64 loss: 3.8399367332458496
Batch 41/64 loss: 3.8222978115081787
Batch 42/64 loss: 3.7460696697235107
Batch 43/64 loss: 3.6027636528015137
Batch 44/64 loss: 3.8900299072265625
Batch 45/64 loss: 3.6248345375061035
Batch 46/64 loss: 3.7139551639556885
Batch 47/64 loss: 3.613553762435913
Batch 48/64 loss: 3.6234242916107178
Batch 49/64 loss: 3.745593786239624
Batch 50/64 loss: 3.618861436843872
Batch 51/64 loss: 3.6164796352386475
Batch 52/64 loss: 3.6542789936065674
Batch 53/64 loss: 3.5185532569885254
Batch 54/64 loss: 3.622605085372925
Batch 55/64 loss: 3.5500271320343018
Batch 56/64 loss: 3.566098928451538
Batch 57/64 loss: 3.547863721847534
Batch 58/64 loss: 3.485105276107788
Batch 59/64 loss: 3.461779832839966
Batch 60/64 loss: 3.5094056129455566
Batch 61/64 loss: 3.348893642425537
Batch 62/64 loss: 3.4426627159118652
Batch 63/64 loss: 3.3585362434387207
Batch 64/64 loss: 1.667283058166504
Epoch 5  Train loss: 3.715113067626953  Val loss: 3.7435255017886866
Saving best model, epoch: 5
Epoch 6
-------------------------------
Batch 1/64 loss: 3.301146984100342
Batch 2/64 loss: 3.3282065391540527
Batch 3/64 loss: 3.6082663536071777
Batch 4/64 loss: 3.699077844619751
Batch 5/64 loss: 3.491553783416748
Batch 6/64 loss: 3.819976568222046
Batch 7/64 loss: 3.761197805404663
Batch 8/64 loss: 3.7228004932403564
Batch 9/64 loss: 3.867532730102539
Batch 10/64 loss: 3.7272565364837646
Batch 11/64 loss: 3.423607110977173
Batch 12/64 loss: 3.534665107727051
Batch 13/64 loss: 3.5923397541046143
Batch 14/64 loss: 3.594130754470825
Batch 15/64 loss: 3.3326616287231445
Batch 16/64 loss: 3.5597383975982666
Batch 17/64 loss: 3.391160726547241
Batch 18/64 loss: 3.5121047496795654
Batch 19/64 loss: 3.415339946746826
Batch 20/64 loss: 3.5882012844085693
Batch 21/64 loss: 3.699751377105713
Batch 22/64 loss: 3.432279586791992
Batch 23/64 loss: 3.441338300704956
Batch 24/64 loss: 3.2279772758483887
Batch 25/64 loss: 3.286932945251465
Batch 26/64 loss: 3.553044319152832
Batch 27/64 loss: 3.2928850650787354
Batch 28/64 loss: 3.378141403198242
Batch 29/64 loss: 3.3010098934173584
Batch 30/64 loss: 3.345954418182373
Batch 31/64 loss: 3.3347535133361816
Batch 32/64 loss: 3.5368072986602783
Batch 33/64 loss: 3.423187255859375
Batch 34/64 loss: 3.475879430770874
Batch 35/64 loss: 3.540771484375
Batch 36/64 loss: 3.4653477668762207
Batch 37/64 loss: 3.1740992069244385
Batch 38/64 loss: 3.3770089149475098
Batch 39/64 loss: 3.1355814933776855
Batch 40/64 loss: 3.3581273555755615
Batch 41/64 loss: 3.3281147480010986
Batch 42/64 loss: 3.402656316757202
Batch 43/64 loss: 3.5274105072021484
Batch 44/64 loss: 3.320376396179199
Batch 45/64 loss: 3.2926878929138184
Batch 46/64 loss: 3.267275333404541
Batch 47/64 loss: 3.0819945335388184
Batch 48/64 loss: 3.2562646865844727
Batch 49/64 loss: 3.193218469619751
Batch 50/64 loss: 3.016594409942627
Batch 51/64 loss: 3.1633448600769043
Batch 52/64 loss: 3.6927456855773926
Batch 53/64 loss: 3.2245805263519287
Batch 54/64 loss: 3.172865867614746
Batch 55/64 loss: 3.2107911109924316
Batch 56/64 loss: 4.427544593811035
Batch 57/64 loss: 3.2577431201934814
Batch 58/64 loss: 3.549489736557007
Batch 59/64 loss: 3.2791693210601807
Batch 60/64 loss: 3.3343143463134766
Batch 61/64 loss: 3.430070638656616
Batch 62/64 loss: 3.3032937049865723
Batch 63/64 loss: 3.308472156524658
Batch 64/64 loss: 1.5654869079589844
Epoch 6  Train loss: 3.408109451742733  Val loss: 3.7259926877890255
Saving best model, epoch: 6
Epoch 7
-------------------------------
Batch 1/64 loss: 3.3238556385040283
Batch 2/64 loss: 3.382884979248047
Batch 3/64 loss: 3.3520984649658203
Batch 4/64 loss: 3.5267832279205322
Batch 5/64 loss: 3.4630682468414307
Batch 6/64 loss: 3.3476994037628174
Batch 7/64 loss: 3.2305891513824463
Batch 8/64 loss: 3.5464553833007812
Batch 9/64 loss: 3.7643954753875732
Batch 10/64 loss: 3.427518367767334
Batch 11/64 loss: 3.163421869277954
Batch 12/64 loss: 3.567408561706543
Batch 13/64 loss: 3.3975958824157715
Batch 14/64 loss: 3.3281145095825195
Batch 15/64 loss: 3.2896382808685303
Batch 16/64 loss: 3.152012825012207
Batch 17/64 loss: 3.1703901290893555
Batch 18/64 loss: 3.4636261463165283
Batch 19/64 loss: 3.124955892562866
Batch 20/64 loss: 3.635652780532837
Batch 21/64 loss: 3.385254144668579
Batch 22/64 loss: 3.3309366703033447
Batch 23/64 loss: 3.154635429382324
Batch 24/64 loss: 3.32875919342041
Batch 25/64 loss: 3.4565317630767822
Batch 26/64 loss: 3.298708915710449
Batch 27/64 loss: 3.213437795639038
Batch 28/64 loss: 3.368447780609131
Batch 29/64 loss: 3.2782816886901855
Batch 30/64 loss: 3.034619092941284
Batch 31/64 loss: 3.268594980239868
Batch 32/64 loss: 3.17085337638855
Batch 33/64 loss: 3.3751509189605713
Batch 34/64 loss: 2.8878462314605713
Batch 35/64 loss: 3.320810556411743
Batch 36/64 loss: 3.0049262046813965
Batch 37/64 loss: 3.1747896671295166
Batch 38/64 loss: 3.1621029376983643
Batch 39/64 loss: 3.086960792541504
Batch 40/64 loss: 3.1698389053344727
Batch 41/64 loss: 2.8812687397003174
Batch 42/64 loss: 3.036402702331543
Batch 43/64 loss: 2.8852715492248535
Batch 44/64 loss: 2.8630857467651367
Batch 45/64 loss: 2.688931941986084
Batch 46/64 loss: 3.0676047801971436
Batch 47/64 loss: 3.1781468391418457
Batch 48/64 loss: 3.113381862640381
Batch 49/64 loss: 2.9716782569885254
Batch 50/64 loss: 3.041093587875366
Batch 51/64 loss: 3.098120927810669
Batch 52/64 loss: 2.962661027908325
Batch 53/64 loss: 3.047802209854126
Batch 54/64 loss: 3.243849515914917
Batch 55/64 loss: 3.2323622703552246
Batch 56/64 loss: 3.1191177368164062
Batch 57/64 loss: 2.974283456802368
Batch 58/64 loss: 3.257789134979248
Batch 59/64 loss: 3.0481534004211426
Batch 60/64 loss: 3.3084845542907715
Batch 61/64 loss: 3.634878396987915
Batch 62/64 loss: 2.970385789871216
Batch 63/64 loss: 2.9324097633361816
Batch 64/64 loss: 1.1455731391906738
Epoch 7  Train loss: 3.1928783734639485  Val loss: 3.1807596167338263
Saving best model, epoch: 7
Epoch 8
-------------------------------
Batch 1/64 loss: 2.8949084281921387
Batch 2/64 loss: 3.086326837539673
Batch 3/64 loss: 3.012040376663208
Batch 4/64 loss: 3.047142505645752
Batch 5/64 loss: 3.1072816848754883
Batch 6/64 loss: 2.91294527053833
Batch 7/64 loss: 2.968123197555542
Batch 8/64 loss: 3.253472089767456
Batch 9/64 loss: 2.8929059505462646
Batch 10/64 loss: 3.045402765274048
Batch 11/64 loss: 3.0118722915649414
Batch 12/64 loss: 3.001805067062378
Batch 13/64 loss: 3.287149667739868
Batch 14/64 loss: 2.7593939304351807
Batch 15/64 loss: 3.3185412883758545
Batch 16/64 loss: 3.1598169803619385
Batch 17/64 loss: 3.2125940322875977
Batch 18/64 loss: 3.0741641521453857
Batch 19/64 loss: 3.089385986328125
Batch 20/64 loss: 3.273900270462036
Batch 21/64 loss: 3.1407463550567627
Batch 22/64 loss: 3.1243810653686523
Batch 23/64 loss: 3.019916296005249
Batch 24/64 loss: 3.3045284748077393
Batch 25/64 loss: 3.087329626083374
Batch 26/64 loss: 2.810842990875244
Batch 27/64 loss: 3.0134623050689697
Batch 28/64 loss: 2.996373414993286
Batch 29/64 loss: 2.6827552318573
Batch 30/64 loss: 2.833399772644043
Batch 31/64 loss: 3.0836172103881836
Batch 32/64 loss: 2.6899895668029785
Batch 33/64 loss: 2.8453705310821533
Batch 34/64 loss: 3.26397705078125
Batch 35/64 loss: 4.057218074798584
Batch 36/64 loss: 2.8070743083953857
Batch 37/64 loss: 3.0015342235565186
Batch 38/64 loss: 3.127746820449829
Batch 39/64 loss: 2.9890689849853516
Batch 40/64 loss: 3.193030834197998
Batch 41/64 loss: 3.103975534439087
Batch 42/64 loss: 3.2728097438812256
Batch 43/64 loss: 3.1049399375915527
Batch 44/64 loss: 2.795793056488037
Batch 45/64 loss: 3.151480197906494
Batch 46/64 loss: 3.163006544113159
Batch 47/64 loss: 3.086249589920044
Batch 48/64 loss: 3.2353341579437256
Batch 49/64 loss: 3.322070598602295
Batch 50/64 loss: 3.0115907192230225
Batch 51/64 loss: 3.0940494537353516
Batch 52/64 loss: 3.123765468597412
Batch 53/64 loss: 3.2806553840637207
Batch 54/64 loss: 2.801673412322998
Batch 55/64 loss: 3.110539197921753
Batch 56/64 loss: 2.8208611011505127
Batch 57/64 loss: 2.965498447418213
Batch 58/64 loss: 3.1111793518066406
Batch 59/64 loss: 2.8384175300598145
Batch 60/64 loss: 2.8382153511047363
Batch 61/64 loss: 2.853935480117798
Batch 62/64 loss: 2.7917873859405518
Batch 63/64 loss: 2.754531145095825
Batch 64/64 loss: 0.7031769752502441
Epoch 8  Train loss: 3.0218239597245757  Val loss: 3.1411382635844123
Saving best model, epoch: 8
Epoch 9
-------------------------------
Batch 1/64 loss: 2.759821891784668
Batch 2/64 loss: 2.8412413597106934
Batch 3/64 loss: 3.176551580429077
Batch 4/64 loss: 2.9823825359344482
Batch 5/64 loss: 2.718043088912964
Batch 6/64 loss: 2.9681789875030518
Batch 7/64 loss: 3.0366573333740234
Batch 8/64 loss: 2.850724458694458
Batch 9/64 loss: 3.051220417022705
Batch 10/64 loss: 2.76720929145813
Batch 11/64 loss: 2.6739821434020996
Batch 12/64 loss: 2.7152047157287598
Batch 13/64 loss: 2.737877607345581
Batch 14/64 loss: 2.8045246601104736
Batch 15/64 loss: 2.895207643508911
Batch 16/64 loss: 2.88256573677063
Batch 17/64 loss: 2.6590964794158936
Batch 18/64 loss: 2.5675861835479736
Batch 19/64 loss: 2.6048903465270996
Batch 20/64 loss: 2.620908498764038
Batch 21/64 loss: 2.7637877464294434
Batch 22/64 loss: 2.653277635574341
Batch 23/64 loss: 2.5219545364379883
Batch 24/64 loss: 2.6167056560516357
Batch 25/64 loss: 3.228834629058838
Batch 26/64 loss: 3.5728554725646973
Batch 27/64 loss: 3.028385639190674
Batch 28/64 loss: 2.886483907699585
Batch 29/64 loss: 2.9443931579589844
Batch 30/64 loss: 2.7553741931915283
Batch 31/64 loss: 3.1707420349121094
Batch 32/64 loss: 2.905113458633423
Batch 33/64 loss: 2.9153428077697754
Batch 34/64 loss: 2.8286826610565186
Batch 35/64 loss: 2.695006847381592
Batch 36/64 loss: 2.8212900161743164
Batch 37/64 loss: 3.1986823081970215
Batch 38/64 loss: 3.043510675430298
Batch 39/64 loss: 3.2361419200897217
Batch 40/64 loss: 3.1734416484832764
Batch 41/64 loss: 2.832399845123291
Batch 42/64 loss: 2.9458608627319336
Batch 43/64 loss: 2.8090593814849854
Batch 44/64 loss: 2.9784603118896484
Batch 45/64 loss: 2.876021146774292
Batch 46/64 loss: 2.887528896331787
Batch 47/64 loss: 2.8314640522003174
Batch 48/64 loss: 2.9811437129974365
Batch 49/64 loss: 2.656344413757324
Batch 50/64 loss: 2.7964026927948
Batch 51/64 loss: 3.060136556625366
Batch 52/64 loss: 2.698260545730591
Batch 53/64 loss: 2.660712718963623
Batch 54/64 loss: 3.0688095092773438
Batch 55/64 loss: 2.6970856189727783
Batch 56/64 loss: 3.0416946411132812
Batch 57/64 loss: 2.807922124862671
Batch 58/64 loss: 2.7782819271087646
Batch 59/64 loss: 2.8932530879974365
Batch 60/64 loss: 3.1392822265625
Batch 61/64 loss: 2.612022638320923
Batch 62/64 loss: 2.7173073291778564
Batch 63/64 loss: 2.7999441623687744
Batch 64/64 loss: 0.5067310333251953
Epoch 9  Train loss: 2.842718879849303  Val loss: 2.8768496202029725
Saving best model, epoch: 9
Epoch 10
-------------------------------
Batch 1/64 loss: 2.624335527420044
Batch 2/64 loss: 2.6743996143341064
Batch 3/64 loss: 3.031496047973633
Batch 4/64 loss: 2.979775905609131
Batch 5/64 loss: 3.1018877029418945
Batch 6/64 loss: 2.568119525909424
Batch 7/64 loss: 3.1648054122924805
Batch 8/64 loss: 2.840261220932007
Batch 9/64 loss: 2.9096758365631104
Batch 10/64 loss: 2.939951181411743
Batch 11/64 loss: 2.9071309566497803
Batch 12/64 loss: 2.600104808807373
Batch 13/64 loss: 2.763608932495117
Batch 14/64 loss: 2.788292646408081
Batch 15/64 loss: 2.7838258743286133
Batch 16/64 loss: 2.9358723163604736
Batch 17/64 loss: 2.70491886138916
Batch 18/64 loss: 2.723520278930664
Batch 19/64 loss: 2.6258351802825928
Batch 20/64 loss: 2.669593572616577
Batch 21/64 loss: 2.705415725708008
Batch 22/64 loss: 3.0123274326324463
Batch 23/64 loss: 2.6272499561309814
Batch 24/64 loss: 2.5227041244506836
Batch 25/64 loss: 2.671525716781616
Batch 26/64 loss: 2.6601741313934326
Batch 27/64 loss: 2.6322758197784424
Batch 28/64 loss: 2.7159383296966553
Batch 29/64 loss: 2.6372814178466797
Batch 30/64 loss: 2.6601150035858154
Batch 31/64 loss: 2.789490222930908
Batch 32/64 loss: 2.5373783111572266
Batch 33/64 loss: 2.609553337097168
Batch 34/64 loss: 2.6944284439086914
Batch 35/64 loss: 2.7523117065429688
Batch 36/64 loss: 2.797954797744751
Batch 37/64 loss: 2.7432966232299805
Batch 38/64 loss: 2.5977227687835693
Batch 39/64 loss: 2.7995524406433105
Batch 40/64 loss: 2.801497459411621
Batch 41/64 loss: 2.7053515911102295
Batch 42/64 loss: 2.962885856628418
Batch 43/64 loss: 2.2998008728027344
Batch 44/64 loss: 2.4606857299804688
Batch 45/64 loss: 2.744962692260742
Batch 46/64 loss: 2.6297316551208496
Batch 47/64 loss: 2.7288975715637207
Batch 48/64 loss: 3.006226062774658
Batch 49/64 loss: 2.758236885070801
Batch 50/64 loss: 2.3984410762786865
Batch 51/64 loss: 2.8068718910217285
Batch 52/64 loss: 2.6063568592071533
Batch 53/64 loss: 2.872246503829956
Batch 54/64 loss: 2.454127073287964
Batch 55/64 loss: 2.6176183223724365
Batch 56/64 loss: 2.83512806892395
Batch 57/64 loss: 2.7709784507751465
Batch 58/64 loss: 2.7528023719787598
Batch 59/64 loss: 2.7174088954925537
Batch 60/64 loss: 2.462902784347534
Batch 61/64 loss: 2.639679431915283
Batch 62/64 loss: 2.3770365715026855
Batch 63/64 loss: 2.543947696685791
Batch 64/64 loss: 0.825932502746582
Epoch 10  Train loss: 2.699253011217304  Val loss: 2.680141806192824
Saving best model, epoch: 10
Epoch 11
-------------------------------
Batch 1/64 loss: 2.7621257305145264
Batch 2/64 loss: 2.9264583587646484
Batch 3/64 loss: 2.6061503887176514
Batch 4/64 loss: 2.3876724243164062
Batch 5/64 loss: 2.4343371391296387
Batch 6/64 loss: 2.520142078399658
Batch 7/64 loss: 2.6573944091796875
Batch 8/64 loss: 2.402670383453369
Batch 9/64 loss: 2.8693056106567383
Batch 10/64 loss: 2.6391992568969727
Batch 11/64 loss: 2.4056975841522217
Batch 12/64 loss: 2.780186176300049
Batch 13/64 loss: 3.0109596252441406
Batch 14/64 loss: 2.6672215461730957
Batch 15/64 loss: 2.727726697921753
Batch 16/64 loss: 2.8389201164245605
Batch 17/64 loss: 2.9269258975982666
Batch 18/64 loss: 2.883378028869629
Batch 19/64 loss: 2.600238084793091
Batch 20/64 loss: 2.5099985599517822
Batch 21/64 loss: 2.8224036693573
Batch 22/64 loss: 3.046660900115967
Batch 23/64 loss: 2.6869688034057617
Batch 24/64 loss: 2.550624370574951
Batch 25/64 loss: 2.727424144744873
Batch 26/64 loss: 2.780012607574463
Batch 27/64 loss: 2.4170796871185303
Batch 28/64 loss: 3.070434093475342
Batch 29/64 loss: 2.4597623348236084
Batch 30/64 loss: 2.8177218437194824
Batch 31/64 loss: 2.5866646766662598
Batch 32/64 loss: 2.722022771835327
Batch 33/64 loss: 2.3816874027252197
Batch 34/64 loss: 2.8013319969177246
Batch 35/64 loss: 2.70373272895813
Batch 36/64 loss: 2.771819591522217
Batch 37/64 loss: 2.4952571392059326
Batch 38/64 loss: 2.5401735305786133
Batch 39/64 loss: 2.5110666751861572
Batch 40/64 loss: 2.4938621520996094
Batch 41/64 loss: 2.633233070373535
Batch 42/64 loss: 2.558281421661377
Batch 43/64 loss: 2.643497943878174
Batch 44/64 loss: 2.4659276008605957
Batch 45/64 loss: 2.5361151695251465
Batch 46/64 loss: 2.537158966064453
Batch 47/64 loss: 2.318164825439453
Batch 48/64 loss: 2.753100872039795
Batch 49/64 loss: 2.5123960971832275
Batch 50/64 loss: 2.5495805740356445
Batch 51/64 loss: 2.440889358520508
Batch 52/64 loss: 2.7469024658203125
Batch 53/64 loss: 2.5625882148742676
Batch 54/64 loss: 2.431339740753174
Batch 55/64 loss: 2.54256010055542
Batch 56/64 loss: 2.4311418533325195
Batch 57/64 loss: 2.6246910095214844
Batch 58/64 loss: 2.626156806945801
Batch 59/64 loss: 2.202450752258301
Batch 60/64 loss: 2.548285961151123
Batch 61/64 loss: 2.753718852996826
Batch 62/64 loss: 2.6503076553344727
Batch 63/64 loss: 2.336733341217041
Batch 64/64 loss: -0.12530088424682617
Epoch 11  Train loss: 2.592230058183857  Val loss: 2.572808521310079
Saving best model, epoch: 11
Epoch 12
-------------------------------
Batch 1/64 loss: 2.5159788131713867
Batch 2/64 loss: 2.8220748901367188
Batch 3/64 loss: 2.1758646965026855
Batch 4/64 loss: 2.6040844917297363
Batch 5/64 loss: 2.5956506729125977
Batch 6/64 loss: 2.4283862113952637
Batch 7/64 loss: 2.5282015800476074
Batch 8/64 loss: 2.8136038780212402
Batch 9/64 loss: 2.704620361328125
Batch 10/64 loss: 2.3088884353637695
Batch 11/64 loss: 2.5907697677612305
Batch 12/64 loss: 2.5215940475463867
Batch 13/64 loss: 2.503386974334717
Batch 14/64 loss: 2.3827500343322754
Batch 15/64 loss: 2.3391780853271484
Batch 16/64 loss: 2.4514989852905273
Batch 17/64 loss: 2.329192638397217
Batch 18/64 loss: 2.2966666221618652
Batch 19/64 loss: 2.282691478729248
Batch 20/64 loss: 2.169424057006836
Batch 21/64 loss: 2.929807662963867
Batch 22/64 loss: 2.529083728790283
Batch 23/64 loss: 2.576953411102295
Batch 24/64 loss: 2.895756244659424
Batch 25/64 loss: 2.4609837532043457
Batch 26/64 loss: 2.5142288208007812
Batch 27/64 loss: 2.4345288276672363
Batch 28/64 loss: 2.3532886505126953
Batch 29/64 loss: 2.6085777282714844
Batch 30/64 loss: 2.6658921241760254
Batch 31/64 loss: 2.4278478622436523
Batch 32/64 loss: 2.482142448425293
Batch 33/64 loss: 2.3468058109283447
Batch 34/64 loss: 2.626316547393799
Batch 35/64 loss: 2.579296112060547
Batch 36/64 loss: 2.8406708240509033
Batch 37/64 loss: 2.6694343090057373
Batch 38/64 loss: 2.687483787536621
Batch 39/64 loss: 2.6875152587890625
Batch 40/64 loss: 2.3085227012634277
Batch 41/64 loss: 2.4004759788513184
Batch 42/64 loss: 2.581024169921875
Batch 43/64 loss: 2.535885810852051
Batch 44/64 loss: 2.4338321685791016
Batch 45/64 loss: 2.544857978820801
Batch 46/64 loss: 2.4485363960266113
Batch 47/64 loss: 2.657453775405884
Batch 48/64 loss: 2.5657968521118164
Batch 49/64 loss: 2.612199306488037
Batch 50/64 loss: 2.404111385345459
Batch 51/64 loss: 2.604546308517456
Batch 52/64 loss: 2.400373935699463
Batch 53/64 loss: 2.785144805908203
Batch 54/64 loss: 2.5694546699523926
Batch 55/64 loss: 2.4726600646972656
Batch 56/64 loss: 2.5026679039001465
Batch 57/64 loss: 2.599733352661133
Batch 58/64 loss: 2.3788743019104004
Batch 59/64 loss: 2.6238784790039062
Batch 60/64 loss: 3.25321102142334
Batch 61/64 loss: 2.7026498317718506
Batch 62/64 loss: 2.314622402191162
Batch 63/64 loss: 2.619722366333008
Batch 64/64 loss: 0.5521292686462402
Epoch 12  Train loss: 2.5163208400501924  Val loss: 2.6887979278040097
Epoch 13
-------------------------------
Batch 1/64 loss: 2.4120020866394043
Batch 2/64 loss: 2.539586067199707
Batch 3/64 loss: 2.3878350257873535
Batch 4/64 loss: 2.365988254547119
Batch 5/64 loss: 2.4472599029541016
Batch 6/64 loss: 2.461134433746338
Batch 7/64 loss: 2.490171432495117
Batch 8/64 loss: 2.6152124404907227
Batch 9/64 loss: 2.4452784061431885
Batch 10/64 loss: 2.228923797607422
Batch 11/64 loss: 2.3446402549743652
Batch 12/64 loss: 2.519125461578369
Batch 13/64 loss: 2.4806671142578125
Batch 14/64 loss: 2.359783172607422
Batch 15/64 loss: 2.3235507011413574
Batch 16/64 loss: 2.1771535873413086
Batch 17/64 loss: 2.786440849304199
Batch 18/64 loss: 2.3206424713134766
Batch 19/64 loss: 2.473507881164551
Batch 20/64 loss: 2.4513535499572754
Batch 21/64 loss: 2.555302619934082
Batch 22/64 loss: 2.263169765472412
Batch 23/64 loss: 3.073143482208252
Batch 24/64 loss: 2.357959747314453
Batch 25/64 loss: 2.392448902130127
Batch 26/64 loss: 2.4499402046203613
Batch 27/64 loss: 2.675678253173828
Batch 28/64 loss: 2.657639980316162
Batch 29/64 loss: 2.4580078125
Batch 30/64 loss: 2.313629150390625
Batch 31/64 loss: 2.6102218627929688
Batch 32/64 loss: 2.2942538261413574
Batch 33/64 loss: 2.4215731620788574
Batch 34/64 loss: 2.2686052322387695
Batch 35/64 loss: 2.9736456871032715
Batch 36/64 loss: 2.1368308067321777
Batch 37/64 loss: 2.5769877433776855
Batch 38/64 loss: 2.293437957763672
Batch 39/64 loss: 2.478498935699463
Batch 40/64 loss: 2.679286479949951
Batch 41/64 loss: 2.4932122230529785
Batch 42/64 loss: 2.3426103591918945
Batch 43/64 loss: 2.3611483573913574
Batch 44/64 loss: 2.232494831085205
Batch 45/64 loss: 2.3536553382873535
Batch 46/64 loss: 2.2840681076049805
Batch 47/64 loss: 3.8726353645324707
Batch 48/64 loss: 2.6055045127868652
Batch 49/64 loss: 2.5659852027893066
Batch 50/64 loss: 2.725465774536133
Batch 51/64 loss: 2.3231258392333984
Batch 52/64 loss: 2.567044734954834
Batch 53/64 loss: 2.410123825073242
Batch 54/64 loss: 2.7168784141540527
Batch 55/64 loss: 2.4312009811401367
Batch 56/64 loss: 2.638582468032837
Batch 57/64 loss: 2.3272204399108887
Batch 58/64 loss: 2.24215030670166
Batch 59/64 loss: 2.479332447052002
Batch 60/64 loss: 2.2524795532226562
Batch 61/64 loss: 2.7278189659118652
Batch 62/64 loss: 2.693753957748413
Batch 63/64 loss: 2.641608238220215
Batch 64/64 loss: 0.06027030944824219
Epoch 13  Train loss: 2.461079552594353  Val loss: 2.5623721093246616
Saving best model, epoch: 13
Epoch 14
-------------------------------
Batch 1/64 loss: 2.853679656982422
Batch 2/64 loss: 2.3718953132629395
Batch 3/64 loss: 2.5202231407165527
Batch 4/64 loss: 2.547400951385498
Batch 5/64 loss: 3.4260330200195312
Batch 6/64 loss: 2.7475762367248535
Batch 7/64 loss: 2.500429153442383
Batch 8/64 loss: 2.6339731216430664
Batch 9/64 loss: 2.655263662338257
Batch 10/64 loss: 2.5953969955444336
Batch 11/64 loss: 2.48472261428833
Batch 12/64 loss: 2.7150421142578125
Batch 13/64 loss: 3.507981300354004
Batch 14/64 loss: 3.6456940174102783
Batch 15/64 loss: 3.349443197250366
Batch 16/64 loss: 3.964931011199951
Batch 17/64 loss: 3.1940064430236816
Batch 18/64 loss: 3.4175682067871094
Batch 19/64 loss: 3.6111459732055664
Batch 20/64 loss: 3.8500800132751465
Batch 21/64 loss: 3.8154845237731934
Batch 22/64 loss: 3.5481486320495605
Batch 23/64 loss: 3.408106803894043
Batch 24/64 loss: 3.425942897796631
Batch 25/64 loss: 3.5791244506835938
Batch 26/64 loss: 3.1494781970977783
Batch 27/64 loss: 3.1547257900238037
Batch 28/64 loss: 3.5714077949523926
Batch 29/64 loss: 3.8406689167022705
Batch 30/64 loss: 3.333937883377075
Batch 31/64 loss: 3.2642288208007812
Batch 32/64 loss: 2.9871342182159424
Batch 33/64 loss: 2.815706253051758
Batch 34/64 loss: 3.288543939590454
Batch 35/64 loss: 3.1736795902252197
Batch 36/64 loss: 3.1366662979125977
Batch 37/64 loss: 3.361621618270874
Batch 38/64 loss: 3.1798343658447266
Batch 39/64 loss: 3.43500018119812
Batch 40/64 loss: 3.18402361869812
Batch 41/64 loss: 2.9336321353912354
Batch 42/64 loss: 2.849792242050171
Batch 43/64 loss: 2.765017032623291
Batch 44/64 loss: 2.7511062622070312
Batch 45/64 loss: 2.684739351272583
Batch 46/64 loss: 2.8152554035186768
Batch 47/64 loss: 3.1402502059936523
Batch 48/64 loss: 3.2569401264190674
Batch 49/64 loss: 2.6916003227233887
Batch 50/64 loss: 3.014517307281494
Batch 51/64 loss: 2.695751190185547
Batch 52/64 loss: 2.7247531414031982
Batch 53/64 loss: 3.0067713260650635
Batch 54/64 loss: 3.093904495239258
Batch 55/64 loss: 2.8750486373901367
Batch 56/64 loss: 2.688115119934082
Batch 57/64 loss: 2.8045077323913574
Batch 58/64 loss: 3.0928802490234375
Batch 59/64 loss: 3.0070981979370117
Batch 60/64 loss: 2.504462957382202
Batch 61/64 loss: 3.3856372833251953
Batch 62/64 loss: 2.732229232788086
Batch 63/64 loss: 2.6314194202423096
Batch 64/64 loss: 0.8123483657836914
Epoch 14  Train loss: 3.0431473189709233  Val loss: 2.8590844505021664
Epoch 15
-------------------------------
Batch 1/64 loss: 2.822202205657959
Batch 2/64 loss: 2.587733268737793
Batch 3/64 loss: 2.7800962924957275
Batch 4/64 loss: 3.025681734085083
Batch 5/64 loss: 2.8020224571228027
Batch 6/64 loss: 2.833336591720581
Batch 7/64 loss: 2.6417226791381836
Batch 8/64 loss: 2.743598699569702
Batch 9/64 loss: 2.583437442779541
Batch 10/64 loss: 2.7970383167266846
Batch 11/64 loss: 2.93583345413208
Batch 12/64 loss: 2.5781874656677246
Batch 13/64 loss: 2.7171013355255127
Batch 14/64 loss: 2.4693098068237305
Batch 15/64 loss: 2.745159387588501
Batch 16/64 loss: 2.513585090637207
Batch 17/64 loss: 2.9652836322784424
Batch 18/64 loss: 2.6679623126983643
Batch 19/64 loss: 2.9573233127593994
Batch 20/64 loss: 2.819075584411621
Batch 21/64 loss: 2.844395160675049
Batch 22/64 loss: 2.741513252258301
Batch 23/64 loss: 2.902893543243408
Batch 24/64 loss: 2.781515121459961
Batch 25/64 loss: 2.553121328353882
Batch 26/64 loss: 2.569429636001587
Batch 27/64 loss: 2.636169672012329
Batch 28/64 loss: 2.726372241973877
Batch 29/64 loss: 2.92546010017395
Batch 30/64 loss: 2.590437889099121
Batch 31/64 loss: 2.597684144973755
Batch 32/64 loss: 2.7302966117858887
Batch 33/64 loss: 2.658489942550659
Batch 34/64 loss: 2.68908429145813
Batch 35/64 loss: 2.5569369792938232
Batch 36/64 loss: 2.4164302349090576
Batch 37/64 loss: 2.4197516441345215
Batch 38/64 loss: 2.5541982650756836
Batch 39/64 loss: 2.546550989151001
Batch 40/64 loss: 2.383580446243286
Batch 41/64 loss: 2.605571746826172
Batch 42/64 loss: 2.4995429515838623
Batch 43/64 loss: 2.612651824951172
Batch 44/64 loss: 2.894266128540039
Batch 45/64 loss: 2.632132053375244
Batch 46/64 loss: 2.5944058895111084
Batch 47/64 loss: 2.6262893676757812
Batch 48/64 loss: 2.5278096199035645
Batch 49/64 loss: 2.6152005195617676
Batch 50/64 loss: 2.6345953941345215
Batch 51/64 loss: 2.5208969116210938
Batch 52/64 loss: 2.424382209777832
Batch 53/64 loss: 2.8810081481933594
Batch 54/64 loss: 2.6992592811584473
Batch 55/64 loss: 3.2626094818115234
Batch 56/64 loss: 2.3324084281921387
Batch 57/64 loss: 2.5044097900390625
Batch 58/64 loss: 2.6245994567871094
Batch 59/64 loss: 2.7163023948669434
Batch 60/64 loss: 2.375734329223633
Batch 61/64 loss: 2.516341209411621
Batch 62/64 loss: 2.4885926246643066
Batch 63/64 loss: 2.828691244125366
Batch 64/64 loss: 0.4216279983520508
Epoch 15  Train loss: 2.6438262677660176  Val loss: 2.5014645389674866
Saving best model, epoch: 15
Epoch 16
-------------------------------
Batch 1/64 loss: 2.5401315689086914
Batch 2/64 loss: 2.6586966514587402
Batch 3/64 loss: 2.501040458679199
Batch 4/64 loss: 2.5320816040039062
Batch 5/64 loss: 2.6867709159851074
Batch 6/64 loss: 2.6468305587768555
Batch 7/64 loss: 2.440248489379883
Batch 8/64 loss: 2.4573426246643066
Batch 9/64 loss: 2.6768407821655273
Batch 10/64 loss: 2.257760524749756
Batch 11/64 loss: 2.4535746574401855
Batch 12/64 loss: 2.4778590202331543
Batch 13/64 loss: 2.647531032562256
Batch 14/64 loss: 2.426884174346924
Batch 15/64 loss: 2.444972515106201
Batch 16/64 loss: 2.61181640625
Batch 17/64 loss: 2.313138008117676
Batch 18/64 loss: 2.8246636390686035
Batch 19/64 loss: 2.290243625640869
Batch 20/64 loss: 2.6390156745910645
Batch 21/64 loss: 2.592862129211426
Batch 22/64 loss: 2.6133370399475098
Batch 23/64 loss: 2.336543083190918
Batch 24/64 loss: 2.9207754135131836
Batch 25/64 loss: 2.577568531036377
Batch 26/64 loss: 2.5849947929382324
Batch 27/64 loss: 2.410759449005127
Batch 28/64 loss: 2.43314790725708
Batch 29/64 loss: 2.662426471710205
Batch 30/64 loss: 2.474565029144287
Batch 31/64 loss: 2.6708335876464844
Batch 32/64 loss: 2.672639846801758
Batch 33/64 loss: 2.5301766395568848
Batch 34/64 loss: 2.6126561164855957
Batch 35/64 loss: 2.1267194747924805
Batch 36/64 loss: 2.715796947479248
Batch 37/64 loss: 2.361448287963867
Batch 38/64 loss: 2.5031027793884277
Batch 39/64 loss: 2.802084445953369
Batch 40/64 loss: 2.1793460845947266
Batch 41/64 loss: 2.3522391319274902
Batch 42/64 loss: 2.7722010612487793
Batch 43/64 loss: 2.5654044151306152
Batch 44/64 loss: 2.6291399002075195
Batch 45/64 loss: 2.319952964782715
Batch 46/64 loss: 2.400416851043701
Batch 47/64 loss: 2.581796646118164
Batch 48/64 loss: 2.372410774230957
Batch 49/64 loss: 2.473601818084717
Batch 50/64 loss: 2.1409645080566406
Batch 51/64 loss: 2.512521266937256
Batch 52/64 loss: 2.9020752906799316
Batch 53/64 loss: 2.5208511352539062
Batch 54/64 loss: 2.393383026123047
Batch 55/64 loss: 2.5009994506835938
Batch 56/64 loss: 2.5122904777526855
Batch 57/64 loss: 2.419323444366455
Batch 58/64 loss: 2.3078818321228027
Batch 59/64 loss: 2.66713547706604
Batch 60/64 loss: 2.6978869438171387
Batch 61/64 loss: 2.316850185394287
Batch 62/64 loss: 2.300422191619873
Batch 63/64 loss: 2.1082167625427246
Batch 64/64 loss: -0.36335277557373047
Epoch 16  Train loss: 2.475367497462852  Val loss: 2.3271280629528346
Saving best model, epoch: 16
Epoch 17
-------------------------------
Batch 1/64 loss: 2.459031581878662
Batch 2/64 loss: 2.480093002319336
Batch 3/64 loss: 2.753718376159668
Batch 4/64 loss: 2.507950782775879
Batch 5/64 loss: 2.3476643562316895
Batch 6/64 loss: 2.3819313049316406
Batch 7/64 loss: 2.4803647994995117
Batch 8/64 loss: 2.2119383811950684
Batch 9/64 loss: 2.114327907562256
Batch 10/64 loss: 2.3174338340759277
Batch 11/64 loss: 2.4350781440734863
Batch 12/64 loss: 2.40301513671875
Batch 13/64 loss: 2.366159439086914
Batch 14/64 loss: 1.981926441192627
Batch 15/64 loss: 2.247419834136963
Batch 16/64 loss: 2.6269702911376953
Batch 17/64 loss: 2.155874252319336
Batch 18/64 loss: 2.145658493041992
Batch 19/64 loss: 2.559546947479248
Batch 20/64 loss: 2.4647650718688965
Batch 21/64 loss: 2.791412353515625
Batch 22/64 loss: 2.46622896194458
Batch 23/64 loss: 2.579440116882324
Batch 24/64 loss: 2.203216552734375
Batch 25/64 loss: 2.4313783645629883
Batch 26/64 loss: 2.38820743560791
Batch 27/64 loss: 2.2951550483703613
Batch 28/64 loss: 2.126491069793701
Batch 29/64 loss: 2.393584728240967
Batch 30/64 loss: 2.2245240211486816
Batch 31/64 loss: 2.2916626930236816
Batch 32/64 loss: 2.214796543121338
Batch 33/64 loss: 2.5205307006835938
Batch 34/64 loss: 2.5223617553710938
Batch 35/64 loss: 2.4537105560302734
Batch 36/64 loss: 2.3768410682678223
Batch 37/64 loss: 2.532736301422119
Batch 38/64 loss: 2.1650476455688477
Batch 39/64 loss: 2.449845314025879
Batch 40/64 loss: 2.9319100379943848
Batch 41/64 loss: 2.3931713104248047
Batch 42/64 loss: 2.3257994651794434
Batch 43/64 loss: 2.852510929107666
Batch 44/64 loss: 2.4528441429138184
Batch 45/64 loss: 2.393928050994873
Batch 46/64 loss: 2.2504491806030273
Batch 47/64 loss: 2.5521750450134277
Batch 48/64 loss: 2.1111350059509277
Batch 49/64 loss: 2.1687941551208496
Batch 50/64 loss: 2.726135730743408
Batch 51/64 loss: 2.4267477989196777
Batch 52/64 loss: 2.4157114028930664
Batch 53/64 loss: 2.336174488067627
Batch 54/64 loss: 2.670278310775757
Batch 55/64 loss: 2.3561644554138184
Batch 56/64 loss: 2.268259048461914
Batch 57/64 loss: 2.3308753967285156
Batch 58/64 loss: 2.3501243591308594
Batch 59/64 loss: 2.53696870803833
Batch 60/64 loss: 2.489230155944824
Batch 61/64 loss: 2.3709826469421387
Batch 62/64 loss: 2.3317441940307617
Batch 63/64 loss: 2.3193321228027344
Batch 64/64 loss: 0.07279348373413086
Epoch 17  Train loss: 2.373083621380376  Val loss: 2.27075479120733
Saving best model, epoch: 17
Epoch 18
-------------------------------
Batch 1/64 loss: 2.2445826530456543
Batch 2/64 loss: 2.1544594764709473
Batch 3/64 loss: 2.1120924949645996
Batch 4/64 loss: 2.4403882026672363
Batch 5/64 loss: 2.3302836418151855
Batch 6/64 loss: 2.363614082336426
Batch 7/64 loss: 2.1312050819396973
Batch 8/64 loss: 2.291614532470703
Batch 9/64 loss: 2.206976890563965
Batch 10/64 loss: 2.4525303840637207
Batch 11/64 loss: 2.222222328186035
Batch 12/64 loss: 2.2168726921081543
Batch 13/64 loss: 2.7622504234313965
Batch 14/64 loss: 2.804494857788086
Batch 15/64 loss: 2.4970335960388184
Batch 16/64 loss: 2.612163543701172
Batch 17/64 loss: 2.6264872550964355
Batch 18/64 loss: 2.9598007202148438
Batch 19/64 loss: 2.639007091522217
Batch 20/64 loss: 2.4403038024902344
Batch 21/64 loss: 2.575681686401367
Batch 22/64 loss: 2.573683738708496
Batch 23/64 loss: 2.5016250610351562
Batch 24/64 loss: 2.7093186378479004
Batch 25/64 loss: 2.2635765075683594
Batch 26/64 loss: 2.2435851097106934
Batch 27/64 loss: 2.5252909660339355
Batch 28/64 loss: 2.283820152282715
Batch 29/64 loss: 2.8053231239318848
Batch 30/64 loss: 2.3655166625976562
Batch 31/64 loss: 2.215348243713379
Batch 32/64 loss: 2.1883816719055176
Batch 33/64 loss: 2.668397903442383
Batch 34/64 loss: 2.715559959411621
Batch 35/64 loss: 2.261439800262451
Batch 36/64 loss: 2.150468349456787
Batch 37/64 loss: 2.3636841773986816
Batch 38/64 loss: 2.205594062805176
Batch 39/64 loss: 2.2686519622802734
Batch 40/64 loss: 2.326420307159424
Batch 41/64 loss: 2.4925098419189453
Batch 42/64 loss: 2.447317123413086
Batch 43/64 loss: 2.465757369995117
Batch 44/64 loss: 2.1709213256835938
Batch 45/64 loss: 2.2649078369140625
Batch 46/64 loss: 2.4279494285583496
Batch 47/64 loss: 2.893033981323242
Batch 48/64 loss: 2.384901523590088
Batch 49/64 loss: 2.3269453048706055
Batch 50/64 loss: 1.983191967010498
Batch 51/64 loss: 2.305687427520752
Batch 52/64 loss: 2.5012927055358887
Batch 53/64 loss: 2.471527099609375
Batch 54/64 loss: 2.4115967750549316
Batch 55/64 loss: 2.109679698944092
Batch 56/64 loss: 2.315019130706787
Batch 57/64 loss: 2.1450843811035156
Batch 58/64 loss: 2.4879798889160156
Batch 59/64 loss: 2.2863197326660156
Batch 60/64 loss: 2.1713380813598633
Batch 61/64 loss: 2.3178844451904297
Batch 62/64 loss: 2.3683080673217773
Batch 63/64 loss: 2.165792942047119
Batch 64/64 loss: -0.4056210517883301
Epoch 18  Train loss: 2.3581252098083496  Val loss: 2.3144315870357133
Epoch 19
-------------------------------
Batch 1/64 loss: 2.295229434967041
Batch 2/64 loss: 2.196014881134033
Batch 3/64 loss: 2.5436863899230957
Batch 4/64 loss: 2.0890889167785645
Batch 5/64 loss: 2.780571460723877
Batch 6/64 loss: 2.302978992462158
Batch 7/64 loss: 2.012449264526367
Batch 8/64 loss: 2.443366527557373
Batch 9/64 loss: 2.438438892364502
Batch 10/64 loss: 2.4094104766845703
Batch 11/64 loss: 2.3912296295166016
Batch 12/64 loss: 2.1707096099853516
Batch 13/64 loss: 2.0344791412353516
Batch 14/64 loss: 2.1980485916137695
Batch 15/64 loss: 2.3123526573181152
Batch 16/64 loss: 2.525930881500244
Batch 17/64 loss: 2.2876148223876953
Batch 18/64 loss: 2.754031181335449
Batch 19/64 loss: 2.1509342193603516
Batch 20/64 loss: 2.3873801231384277
Batch 21/64 loss: 2.1199827194213867
Batch 22/64 loss: 2.431218147277832
Batch 23/64 loss: 2.407501697540283
Batch 24/64 loss: 2.4031729698181152
Batch 25/64 loss: 2.2602086067199707
Batch 26/64 loss: 2.4044227600097656
Batch 27/64 loss: 2.0233702659606934
Batch 28/64 loss: 2.45369291305542
Batch 29/64 loss: 2.5999250411987305
Batch 30/64 loss: 2.348029613494873
Batch 31/64 loss: 2.2570390701293945
Batch 32/64 loss: 2.246994972229004
Batch 33/64 loss: 2.1513657569885254
Batch 34/64 loss: 2.0390868186950684
Batch 35/64 loss: 2.3279008865356445
Batch 36/64 loss: 2.4184579849243164
Batch 37/64 loss: 2.3649773597717285
Batch 38/64 loss: 1.9322614669799805
Batch 39/64 loss: 2.2240662574768066
Batch 40/64 loss: 2.4890260696411133
Batch 41/64 loss: 2.1488633155822754
Batch 42/64 loss: 2.336951732635498
Batch 43/64 loss: 2.1637706756591797
Batch 44/64 loss: 2.5320796966552734
Batch 45/64 loss: 2.259674549102783
Batch 46/64 loss: 2.377203941345215
Batch 47/64 loss: 2.2451443672180176
Batch 48/64 loss: 2.4641451835632324
Batch 49/64 loss: 2.124150276184082
Batch 50/64 loss: 2.325016975402832
Batch 51/64 loss: 2.3471832275390625
Batch 52/64 loss: 2.3574419021606445
Batch 53/64 loss: 2.38889741897583
Batch 54/64 loss: 2.333019733428955
Batch 55/64 loss: 2.013309955596924
Batch 56/64 loss: 2.425840377807617
Batch 57/64 loss: 2.0431365966796875
Batch 58/64 loss: 2.4037365913391113
Batch 59/64 loss: 2.28511905670166
Batch 60/64 loss: 2.349234104156494
Batch 61/64 loss: 2.376533031463623
Batch 62/64 loss: 2.2721214294433594
Batch 63/64 loss: 2.1868748664855957
Batch 64/64 loss: -0.5981307029724121
Epoch 19  Train loss: 2.2735293874553606  Val loss: 2.276991375533166
Epoch 20
-------------------------------
Batch 1/64 loss: 2.0456247329711914
Batch 2/64 loss: 1.9099440574645996
Batch 3/64 loss: 2.2985997200012207
Batch 4/64 loss: 2.1824846267700195
Batch 5/64 loss: 2.375596046447754
Batch 6/64 loss: 2.153149127960205
Batch 7/64 loss: 2.4227890968322754
Batch 8/64 loss: 2.4255504608154297
Batch 9/64 loss: 2.3578124046325684
Batch 10/64 loss: 2.37733793258667
Batch 11/64 loss: 2.576037883758545
Batch 12/64 loss: 2.375699996948242
Batch 13/64 loss: 2.3570408821105957
Batch 14/64 loss: 2.123650074005127
Batch 15/64 loss: 2.308187484741211
Batch 16/64 loss: 2.194155216217041
Batch 17/64 loss: 2.16672945022583
Batch 18/64 loss: 2.079166889190674
Batch 19/64 loss: 2.2261815071105957
Batch 20/64 loss: 2.46018123626709
Batch 21/64 loss: 2.2345285415649414
Batch 22/64 loss: 2.699369430541992
Batch 23/64 loss: 2.194408416748047
Batch 24/64 loss: 2.434539794921875
Batch 25/64 loss: 2.163177967071533
Batch 26/64 loss: 2.095946788787842
Batch 27/64 loss: 2.1847381591796875
Batch 28/64 loss: 1.9356703758239746
Batch 29/64 loss: 2.1875123977661133
Batch 30/64 loss: 2.3604483604431152
Batch 31/64 loss: 2.146650791168213
Batch 32/64 loss: 2.341465473175049
Batch 33/64 loss: 2.1139235496520996
Batch 34/64 loss: 1.9003868103027344
Batch 35/64 loss: 1.9898028373718262
Batch 36/64 loss: 2.573841094970703
Batch 37/64 loss: 2.16032075881958
Batch 38/64 loss: 2.296541690826416
Batch 39/64 loss: 2.082083225250244
Batch 40/64 loss: 2.1162877082824707
Batch 41/64 loss: 1.9664101600646973
Batch 42/64 loss: 2.4968509674072266
Batch 43/64 loss: 2.0138540267944336
Batch 44/64 loss: 2.1254048347473145
Batch 45/64 loss: 2.6778688430786133
Batch 46/64 loss: 2.621527671813965
Batch 47/64 loss: 2.098785877227783
Batch 48/64 loss: 2.4193577766418457
Batch 49/64 loss: 2.142357349395752
Batch 50/64 loss: 2.2006850242614746
Batch 51/64 loss: 2.2502236366271973
Batch 52/64 loss: 2.947470188140869
Batch 53/64 loss: 2.17679500579834
Batch 54/64 loss: 2.3451199531555176
Batch 55/64 loss: 2.1663155555725098
Batch 56/64 loss: 1.9621648788452148
Batch 57/64 loss: 2.0049691200256348
Batch 58/64 loss: 2.312126636505127
Batch 59/64 loss: 2.3929734230041504
Batch 60/64 loss: 2.4285831451416016
Batch 61/64 loss: 2.669780731201172
Batch 62/64 loss: 2.2279739379882812
Batch 63/64 loss: 2.3731837272644043
Batch 64/64 loss: -0.1721820831298828
Epoch 20  Train loss: 2.235595433852252  Val loss: 2.3081340724250294
Epoch 21
-------------------------------
Batch 1/64 loss: 2.8185787200927734
Batch 2/64 loss: 2.4120659828186035
Batch 3/64 loss: 2.860076904296875
Batch 4/64 loss: 2.1960010528564453
Batch 5/64 loss: 2.451169967651367
Batch 6/64 loss: 2.305140495300293
Batch 7/64 loss: 2.194784164428711
Batch 8/64 loss: 2.3564395904541016
Batch 9/64 loss: 2.246325969696045
Batch 10/64 loss: 2.2674641609191895
Batch 11/64 loss: 2.35530948638916
Batch 12/64 loss: 2.1293869018554688
Batch 13/64 loss: 2.3054537773132324
Batch 14/64 loss: 2.313478469848633
Batch 15/64 loss: 2.1712136268615723
Batch 16/64 loss: 2.4066286087036133
Batch 17/64 loss: 2.0238184928894043
Batch 18/64 loss: 2.742248058319092
Batch 19/64 loss: 2.1055030822753906
Batch 20/64 loss: 2.4514851570129395
Batch 21/64 loss: 1.933882713317871
Batch 22/64 loss: 2.1087722778320312
Batch 23/64 loss: 2.112635612487793
Batch 24/64 loss: 2.352853775024414
Batch 25/64 loss: 2.180014133453369
Batch 26/64 loss: 2.1794166564941406
Batch 27/64 loss: 2.3122425079345703
Batch 28/64 loss: 1.9323887825012207
Batch 29/64 loss: 2.3828959465026855
Batch 30/64 loss: 1.9593110084533691
Batch 31/64 loss: 2.5603137016296387
Batch 32/64 loss: 1.9925308227539062
Batch 33/64 loss: 2.3333048820495605
Batch 34/64 loss: 2.4145426750183105
Batch 35/64 loss: 2.201878547668457
Batch 36/64 loss: 2.60762882232666
Batch 37/64 loss: 2.1886935234069824
Batch 38/64 loss: 2.1190552711486816
Batch 39/64 loss: 2.1551194190979004
Batch 40/64 loss: 1.9318242073059082
Batch 41/64 loss: 2.1942625045776367
Batch 42/64 loss: 1.969233512878418
Batch 43/64 loss: 2.0394935607910156
Batch 44/64 loss: 2.2281322479248047
Batch 45/64 loss: 2.316702365875244
Batch 46/64 loss: 2.2046756744384766
Batch 47/64 loss: 2.4471778869628906
Batch 48/64 loss: 2.123116970062256
Batch 49/64 loss: 2.103597640991211
Batch 50/64 loss: 2.0709800720214844
Batch 51/64 loss: 2.1443228721618652
Batch 52/64 loss: 1.9362115859985352
Batch 53/64 loss: 2.465618133544922
Batch 54/64 loss: 2.115440845489502
Batch 55/64 loss: 2.213193416595459
Batch 56/64 loss: 2.684134006500244
Batch 57/64 loss: 2.091055393218994
Batch 58/64 loss: 2.292933940887451
Batch 59/64 loss: 1.9710993766784668
Batch 60/64 loss: 2.180145263671875
Batch 61/64 loss: 2.1022653579711914
Batch 62/64 loss: 2.111213207244873
Batch 63/64 loss: 3.1123485565185547
Batch 64/64 loss: -0.2294025421142578
Epoch 21  Train loss: 2.2277204775342754  Val loss: 2.3592507536058984
Epoch 22
-------------------------------
Batch 1/64 loss: 2.271838665008545
Batch 2/64 loss: 2.228158950805664
Batch 3/64 loss: 2.415583610534668
Batch 4/64 loss: 2.628554344177246
Batch 5/64 loss: 2.847365379333496
Batch 6/64 loss: 2.210218906402588
Batch 7/64 loss: 2.5159196853637695
Batch 8/64 loss: 2.4988651275634766
Batch 9/64 loss: 2.4240975379943848
Batch 10/64 loss: 2.5438427925109863
Batch 11/64 loss: 2.48746395111084
Batch 12/64 loss: 2.3492884635925293
Batch 13/64 loss: 2.65083646774292
Batch 14/64 loss: 2.3786821365356445
Batch 15/64 loss: 2.012301445007324
Batch 16/64 loss: 2.482577323913574
Batch 17/64 loss: 2.3802366256713867
Batch 18/64 loss: 2.5565147399902344
Batch 19/64 loss: 2.3463144302368164
Batch 20/64 loss: 2.5641989707946777
Batch 21/64 loss: 2.506031036376953
Batch 22/64 loss: 2.105306625366211
Batch 23/64 loss: 2.2733359336853027
Batch 24/64 loss: 2.3167567253112793
Batch 25/64 loss: 2.5291953086853027
Batch 26/64 loss: 2.4452996253967285
Batch 27/64 loss: 2.4303479194641113
Batch 28/64 loss: 2.154378890991211
Batch 29/64 loss: 2.244752883911133
Batch 30/64 loss: 2.322460174560547
Batch 31/64 loss: 2.6400928497314453
Batch 32/64 loss: 2.374021053314209
Batch 33/64 loss: 2.0120668411254883
Batch 34/64 loss: 2.4635372161865234
Batch 35/64 loss: 2.524735927581787
Batch 36/64 loss: 2.07645320892334
Batch 37/64 loss: 2.308668613433838
Batch 38/64 loss: 2.3086719512939453
Batch 39/64 loss: 2.409735679626465
Batch 40/64 loss: 2.5154871940612793
Batch 41/64 loss: 2.419513702392578
Batch 42/64 loss: 2.19122314453125
Batch 43/64 loss: 2.802671432495117
Batch 44/64 loss: 2.0418319702148438
Batch 45/64 loss: 2.4192700386047363
Batch 46/64 loss: 2.3060803413391113
Batch 47/64 loss: 2.3537330627441406
Batch 48/64 loss: 1.877410888671875
Batch 49/64 loss: 2.3880748748779297
Batch 50/64 loss: 2.291975498199463
Batch 51/64 loss: 2.0171380043029785
Batch 52/64 loss: 2.1391258239746094
Batch 53/64 loss: 2.4947314262390137
Batch 54/64 loss: 2.0699210166931152
Batch 55/64 loss: 2.3351454734802246
Batch 56/64 loss: 2.135756492614746
Batch 57/64 loss: 2.229067325592041
Batch 58/64 loss: 2.652827262878418
Batch 59/64 loss: 2.0524516105651855
Batch 60/64 loss: 2.139252185821533
Batch 61/64 loss: 2.3732752799987793
Batch 62/64 loss: 2.0080814361572266
Batch 63/64 loss: 2.704792022705078
Batch 64/64 loss: -0.2679929733276367
Epoch 22  Train loss: 2.3215145223280964  Val loss: 2.3530181150665808
Epoch 23
-------------------------------
Batch 1/64 loss: 2.2367897033691406
Batch 2/64 loss: 2.0304126739501953
Batch 3/64 loss: 1.798713207244873
Batch 4/64 loss: 2.177532196044922
Batch 5/64 loss: 2.250904083251953
Batch 6/64 loss: 2.1927685737609863
Batch 7/64 loss: 2.569459915161133
Batch 8/64 loss: 2.0854220390319824
Batch 9/64 loss: 2.217906951904297
Batch 10/64 loss: 2.079552173614502
Batch 11/64 loss: 2.1325221061706543
Batch 12/64 loss: 1.888974666595459
Batch 13/64 loss: 2.2333426475524902
Batch 14/64 loss: 2.247605800628662
Batch 15/64 loss: 1.9841275215148926
Batch 16/64 loss: 2.3588967323303223
Batch 17/64 loss: 2.1866703033447266
Batch 18/64 loss: 2.210452079772949
Batch 19/64 loss: 2.077404022216797
Batch 20/64 loss: 2.1666345596313477
Batch 21/64 loss: 2.3079748153686523
Batch 22/64 loss: 2.0677552223205566
Batch 23/64 loss: 2.1431851387023926
Batch 24/64 loss: 3.0913472175598145
Batch 25/64 loss: 2.868614673614502
Batch 26/64 loss: 2.090928554534912
Batch 27/64 loss: 2.438408374786377
Batch 28/64 loss: 2.1990437507629395
Batch 29/64 loss: 2.45454740524292
Batch 30/64 loss: 2.4447779655456543
Batch 31/64 loss: 2.300943374633789
Batch 32/64 loss: 2.379823684692383
Batch 33/64 loss: 2.2186059951782227
Batch 34/64 loss: 2.2259836196899414
Batch 35/64 loss: 2.3225646018981934
Batch 36/64 loss: 2.2917609214782715
Batch 37/64 loss: 2.7379202842712402
Batch 38/64 loss: 2.2628917694091797
Batch 39/64 loss: 2.4392123222351074
Batch 40/64 loss: 2.211289405822754
Batch 41/64 loss: 2.1140518188476562
Batch 42/64 loss: 2.3354716300964355
Batch 43/64 loss: 2.408719062805176
Batch 44/64 loss: 2.5290427207946777
Batch 45/64 loss: 2.1090855598449707
Batch 46/64 loss: 2.4404892921447754
Batch 47/64 loss: 2.043139934539795
Batch 48/64 loss: 1.9953155517578125
Batch 49/64 loss: 2.3174567222595215
Batch 50/64 loss: 1.970008373260498
Batch 51/64 loss: 2.4725117683410645
Batch 52/64 loss: 2.2442755699157715
Batch 53/64 loss: 2.1021199226379395
Batch 54/64 loss: 2.085639476776123
Batch 55/64 loss: 2.276516914367676
Batch 56/64 loss: 2.007265567779541
Batch 57/64 loss: 3.2147231101989746
Batch 58/64 loss: 2.2360548973083496
Batch 59/64 loss: 2.3098082542419434
Batch 60/64 loss: 2.4968032836914062
Batch 61/64 loss: 2.0108094215393066
Batch 62/64 loss: 2.412548065185547
Batch 63/64 loss: 2.2238659858703613
Batch 64/64 loss: -0.16287565231323242
Epoch 23  Train loss: 2.2408978387421254  Val loss: 3.217173074938587
Epoch 24
-------------------------------
Batch 1/64 loss: 2.4064674377441406
Batch 2/64 loss: 2.1402220726013184
Batch 3/64 loss: 1.964406967163086
Batch 4/64 loss: 2.1478629112243652
Batch 5/64 loss: 2.0548529624938965
Batch 6/64 loss: 2.4018473625183105
Batch 7/64 loss: 2.220086097717285
Batch 8/64 loss: 2.345271587371826
Batch 9/64 loss: 2.247105598449707
Batch 10/64 loss: 2.5833020210266113
Batch 11/64 loss: 2.1252942085266113
Batch 12/64 loss: 2.3161277770996094
Batch 13/64 loss: 2.256436824798584
Batch 14/64 loss: 2.0469374656677246
Batch 15/64 loss: 2.390500068664551
Batch 16/64 loss: 2.209604263305664
Batch 17/64 loss: 2.1592397689819336
Batch 18/64 loss: 2.0953006744384766
Batch 19/64 loss: 2.2794151306152344
Batch 20/64 loss: 2.285727024078369
Batch 21/64 loss: 2.1381802558898926
Batch 22/64 loss: 2.0723018646240234
Batch 23/64 loss: 2.527784824371338
Batch 24/64 loss: 2.1995511054992676
Batch 25/64 loss: 2.0148820877075195
Batch 26/64 loss: 1.9804363250732422
Batch 27/64 loss: 2.3209919929504395
Batch 28/64 loss: 2.067756175994873
Batch 29/64 loss: 2.250148296356201
Batch 30/64 loss: 2.263810157775879
Batch 31/64 loss: 2.1153979301452637
Batch 32/64 loss: 2.29647159576416
Batch 33/64 loss: 2.1771769523620605
Batch 34/64 loss: 1.9952287673950195
Batch 35/64 loss: 2.151235580444336
Batch 36/64 loss: 1.864119052886963
Batch 37/64 loss: 2.131701946258545
Batch 38/64 loss: 2.3828353881835938
Batch 39/64 loss: 1.8833088874816895
Batch 40/64 loss: 2.3573899269104004
Batch 41/64 loss: 2.2281084060668945
Batch 42/64 loss: 1.9947118759155273
Batch 43/64 loss: 2.406296730041504
Batch 44/64 loss: 2.0895609855651855
Batch 45/64 loss: 2.9843106269836426
Batch 46/64 loss: 1.962061882019043
Batch 47/64 loss: 1.997321605682373
Batch 48/64 loss: 2.052297592163086
Batch 49/64 loss: 2.293915271759033
Batch 50/64 loss: 2.0723748207092285
Batch 51/64 loss: 2.170048236846924
Batch 52/64 loss: 2.3280420303344727
Batch 53/64 loss: 2.5245542526245117
Batch 54/64 loss: 2.061147689819336
Batch 55/64 loss: 2.2697596549987793
Batch 56/64 loss: 2.1951956748962402
Batch 57/64 loss: 2.4131827354431152
Batch 58/64 loss: 2.139802932739258
Batch 59/64 loss: 2.282360553741455
Batch 60/64 loss: 2.096238136291504
Batch 61/64 loss: 2.4401369094848633
Batch 62/64 loss: 1.960890293121338
Batch 63/64 loss: 2.367198944091797
Batch 64/64 loss: -0.25351762771606445
Epoch 24  Train loss: 2.1804877954370836  Val loss: 2.158885818166831
Saving best model, epoch: 24
Epoch 25
-------------------------------
Batch 1/64 loss: 2.0451769828796387
Batch 2/64 loss: 2.167496681213379
Batch 3/64 loss: 1.8488373756408691
Batch 4/64 loss: 1.9873571395874023
Batch 5/64 loss: 2.321688652038574
Batch 6/64 loss: 2.2097911834716797
Batch 7/64 loss: 1.8181772232055664
Batch 8/64 loss: 2.025940418243408
Batch 9/64 loss: 1.8745856285095215
Batch 10/64 loss: 2.1655421257019043
Batch 11/64 loss: 2.510960578918457
Batch 12/64 loss: 2.050300121307373
Batch 13/64 loss: 2.210411548614502
Batch 14/64 loss: 2.144050121307373
Batch 15/64 loss: 2.067305088043213
Batch 16/64 loss: 2.1256260871887207
Batch 17/64 loss: 2.0127291679382324
Batch 18/64 loss: 2.1963396072387695
Batch 19/64 loss: 2.083555221557617
Batch 20/64 loss: 1.9178881645202637
Batch 21/64 loss: 1.8979439735412598
Batch 22/64 loss: 2.0768637657165527
Batch 23/64 loss: 2.7331290245056152
Batch 24/64 loss: 2.2691283226013184
Batch 25/64 loss: 2.487983226776123
Batch 26/64 loss: 2.77018404006958
Batch 27/64 loss: 2.2265377044677734
Batch 28/64 loss: 2.327040672302246
Batch 29/64 loss: 2.5324816703796387
Batch 30/64 loss: 2.5054550170898438
Batch 31/64 loss: 2.5254859924316406
Batch 32/64 loss: 2.5434322357177734
Batch 33/64 loss: 2.7214951515197754
Batch 34/64 loss: 2.4169764518737793
Batch 35/64 loss: 2.238471508026123
Batch 36/64 loss: 2.3891825675964355
Batch 37/64 loss: 2.239990711212158
Batch 38/64 loss: 2.4467411041259766
Batch 39/64 loss: 2.234984874725342
Batch 40/64 loss: 2.283956527709961
Batch 41/64 loss: 2.412078380584717
Batch 42/64 loss: 2.4673495292663574
Batch 43/64 loss: 2.319937229156494
Batch 44/64 loss: 2.6495394706726074
Batch 45/64 loss: 2.124415874481201
Batch 46/64 loss: 2.4808106422424316
Batch 47/64 loss: 2.424959659576416
Batch 48/64 loss: 2.2187328338623047
Batch 49/64 loss: 2.510979175567627
Batch 50/64 loss: 2.4043021202087402
Batch 51/64 loss: 2.3865699768066406
Batch 52/64 loss: 2.386533737182617
Batch 53/64 loss: 1.947005271911621
Batch 54/64 loss: 2.105024814605713
Batch 55/64 loss: 2.069652557373047
Batch 56/64 loss: 2.253873348236084
Batch 57/64 loss: 1.9963274002075195
Batch 58/64 loss: 2.4329700469970703
Batch 59/64 loss: 2.0994229316711426
Batch 60/64 loss: 2.110020160675049
Batch 61/64 loss: 2.305668830871582
Batch 62/64 loss: 2.2533164024353027
Batch 63/64 loss: 2.2772579193115234
Batch 64/64 loss: -0.2557806968688965
Epoch 25  Train loss: 2.2289590031492943  Val loss: 2.1434856690082356
Saving best model, epoch: 25
Epoch 26
-------------------------------
Batch 1/64 loss: 1.8847265243530273
Batch 2/64 loss: 1.93939208984375
Batch 3/64 loss: 1.9702258110046387
Batch 4/64 loss: 2.4086241722106934
Batch 5/64 loss: 2.383939266204834
Batch 6/64 loss: 2.272012710571289
Batch 7/64 loss: 2.248819351196289
Batch 8/64 loss: 1.97900390625
Batch 9/64 loss: 2.0937905311584473
Batch 10/64 loss: 2.0263490676879883
Batch 11/64 loss: 2.1717777252197266
Batch 12/64 loss: 2.207186222076416
Batch 13/64 loss: 1.978424072265625
Batch 14/64 loss: 2.1350531578063965
Batch 15/64 loss: 2.7874574661254883
Batch 16/64 loss: 2.074807643890381
Batch 17/64 loss: 1.9506349563598633
Batch 18/64 loss: 2.075503349304199
Batch 19/64 loss: 2.4382896423339844
Batch 20/64 loss: 2.2190351486206055
Batch 21/64 loss: 2.477187156677246
Batch 22/64 loss: 2.0081534385681152
Batch 23/64 loss: 2.1812562942504883
Batch 24/64 loss: 2.121011734008789
Batch 25/64 loss: 2.271172046661377
Batch 26/64 loss: 1.9879412651062012
Batch 27/64 loss: 2.292275905609131
Batch 28/64 loss: 2.577387809753418
Batch 29/64 loss: 2.045565128326416
Batch 30/64 loss: 2.195641040802002
Batch 31/64 loss: 2.3829870223999023
Batch 32/64 loss: 2.2884573936462402
Batch 33/64 loss: 2.1430931091308594
Batch 34/64 loss: 2.102506637573242
Batch 35/64 loss: 2.2989706993103027
Batch 36/64 loss: 1.8235526084899902
Batch 37/64 loss: 2.0251502990722656
Batch 38/64 loss: 1.9015436172485352
Batch 39/64 loss: 2.0697479248046875
Batch 40/64 loss: 2.150139331817627
Batch 41/64 loss: 2.0089168548583984
Batch 42/64 loss: 2.184065818786621
Batch 43/64 loss: 2.3780717849731445
Batch 44/64 loss: 2.074251174926758
Batch 45/64 loss: 1.9176793098449707
Batch 46/64 loss: 2.207336902618408
Batch 47/64 loss: 1.9601130485534668
Batch 48/64 loss: 2.0686731338500977
Batch 49/64 loss: 2.284820556640625
Batch 50/64 loss: 2.1113052368164062
Batch 51/64 loss: 2.0904345512390137
Batch 52/64 loss: 2.208372116088867
Batch 53/64 loss: 1.923424243927002
Batch 54/64 loss: 2.2146353721618652
Batch 55/64 loss: 2.2460484504699707
Batch 56/64 loss: 2.0453104972839355
Batch 57/64 loss: 2.3265366554260254
Batch 58/64 loss: 1.4908289909362793
Batch 59/64 loss: 2.1277999877929688
Batch 60/64 loss: 1.9896583557128906
Batch 61/64 loss: 2.215305805206299
Batch 62/64 loss: 2.092742443084717
Batch 63/64 loss: 1.9314169883728027
Batch 64/64 loss: -0.4249720573425293
Epoch 26  Train loss: 2.107730392381257  Val loss: 2.0806819614266203
Saving best model, epoch: 26
Epoch 27
-------------------------------
Batch 1/64 loss: 1.975766658782959
Batch 2/64 loss: 2.130906581878662
Batch 3/64 loss: 2.0278186798095703
Batch 4/64 loss: 1.8746528625488281
Batch 5/64 loss: 2.256932258605957
Batch 6/64 loss: 2.274209499359131
Batch 7/64 loss: 2.210623264312744
Batch 8/64 loss: 2.1282711029052734
Batch 9/64 loss: 2.1485390663146973
Batch 10/64 loss: 1.8935785293579102
Batch 11/64 loss: 1.966573715209961
Batch 12/64 loss: 2.033721446990967
Batch 13/64 loss: 2.1644372940063477
Batch 14/64 loss: 1.9724173545837402
Batch 15/64 loss: 2.214832305908203
Batch 16/64 loss: 1.8954334259033203
Batch 17/64 loss: 2.2558398246765137
Batch 18/64 loss: 2.1484947204589844
Batch 19/64 loss: 2.1946911811828613
Batch 20/64 loss: 1.9964709281921387
Batch 21/64 loss: 2.0059309005737305
Batch 22/64 loss: 1.9841818809509277
Batch 23/64 loss: 2.1182169914245605
Batch 24/64 loss: 2.216176986694336
Batch 25/64 loss: 2.2405710220336914
Batch 26/64 loss: 2.0619754791259766
Batch 27/64 loss: 2.1749796867370605
Batch 28/64 loss: 2.086514949798584
Batch 29/64 loss: 2.2295193672180176
Batch 30/64 loss: 2.4637107849121094
Batch 31/64 loss: 2.0079426765441895
Batch 32/64 loss: 2.263711929321289
Batch 33/64 loss: 1.8602027893066406
Batch 34/64 loss: 2.132467269897461
Batch 35/64 loss: 2.0995969772338867
Batch 36/64 loss: 2.0966033935546875
Batch 37/64 loss: 2.0631842613220215
Batch 38/64 loss: 2.3840560913085938
Batch 39/64 loss: 2.1636757850646973
Batch 40/64 loss: 2.137068271636963
Batch 41/64 loss: 1.9268722534179688
Batch 42/64 loss: 2.2446560859680176
Batch 43/64 loss: 1.8101391792297363
Batch 44/64 loss: 2.1458897590637207
Batch 45/64 loss: 1.9561572074890137
Batch 46/64 loss: 2.141387939453125
Batch 47/64 loss: 2.2575411796569824
Batch 48/64 loss: 1.842367172241211
Batch 49/64 loss: 2.284663677215576
Batch 50/64 loss: 2.1650443077087402
Batch 51/64 loss: 2.3382301330566406
Batch 52/64 loss: 1.7251958847045898
Batch 53/64 loss: 2.3259921073913574
Batch 54/64 loss: 2.1506972312927246
Batch 55/64 loss: 2.165095329284668
Batch 56/64 loss: 1.8207135200500488
Batch 57/64 loss: 1.9684762954711914
Batch 58/64 loss: 2.1710281372070312
Batch 59/64 loss: 2.035721778869629
Batch 60/64 loss: 1.786055564880371
Batch 61/64 loss: 1.9487600326538086
Batch 62/64 loss: 2.2165536880493164
Batch 63/64 loss: 1.9418530464172363
Batch 64/64 loss: -0.4938077926635742
Epoch 27  Train loss: 2.0635801390105604  Val loss: 2.003302426682305
Saving best model, epoch: 27
Epoch 28
-------------------------------
Batch 1/64 loss: 2.2048745155334473
Batch 2/64 loss: 2.1329293251037598
Batch 3/64 loss: 2.2716779708862305
Batch 4/64 loss: 2.121643543243408
Batch 5/64 loss: 2.0841550827026367
Batch 6/64 loss: 2.2614245414733887
Batch 7/64 loss: 1.9826421737670898
Batch 8/64 loss: 2.206108570098877
Batch 9/64 loss: 2.467719078063965
Batch 10/64 loss: 2.185197353363037
Batch 11/64 loss: 1.9261364936828613
Batch 12/64 loss: 1.9167804718017578
Batch 13/64 loss: 2.163231372833252
Batch 14/64 loss: 2.2915878295898438
Batch 15/64 loss: 2.11691951751709
Batch 16/64 loss: 1.759453296661377
Batch 17/64 loss: 2.1299967765808105
Batch 18/64 loss: 2.453798294067383
Batch 19/64 loss: 1.6846771240234375
Batch 20/64 loss: 2.1281256675720215
Batch 21/64 loss: 1.964599609375
Batch 22/64 loss: 2.035614013671875
Batch 23/64 loss: 1.9930682182312012
Batch 24/64 loss: 2.034945487976074
Batch 25/64 loss: 2.1720728874206543
Batch 26/64 loss: 2.035818099975586
Batch 27/64 loss: 1.751966953277588
Batch 28/64 loss: 1.9652605056762695
Batch 29/64 loss: 2.201242446899414
Batch 30/64 loss: 2.0128092765808105
Batch 31/64 loss: 1.8718185424804688
Batch 32/64 loss: 2.193984031677246
Batch 33/64 loss: 2.1278305053710938
Batch 34/64 loss: 2.1678714752197266
Batch 35/64 loss: 2.129056453704834
Batch 36/64 loss: 1.9042868614196777
Batch 37/64 loss: 2.192862033843994
Batch 38/64 loss: 1.8222436904907227
Batch 39/64 loss: 1.7537074089050293
Batch 40/64 loss: 2.318697452545166
Batch 41/64 loss: 2.3895645141601562
Batch 42/64 loss: 1.9252171516418457
Batch 43/64 loss: 1.816328525543213
Batch 44/64 loss: 1.717456340789795
Batch 45/64 loss: 2.0359959602355957
Batch 46/64 loss: 2.03387451171875
Batch 47/64 loss: 2.0012288093566895
Batch 48/64 loss: 1.9722843170166016
Batch 49/64 loss: 2.1110119819641113
Batch 50/64 loss: 2.043715000152588
Batch 51/64 loss: 1.8573079109191895
Batch 52/64 loss: 1.8659510612487793
Batch 53/64 loss: 2.2059903144836426
Batch 54/64 loss: 2.4650697708129883
Batch 55/64 loss: 2.2086563110351562
Batch 56/64 loss: 1.9755725860595703
Batch 57/64 loss: 2.000828266143799
Batch 58/64 loss: 2.2130846977233887
Batch 59/64 loss: 1.7099947929382324
Batch 60/64 loss: 1.949385166168213
Batch 61/64 loss: 1.8528146743774414
Batch 62/64 loss: 2.2366838455200195
Batch 63/64 loss: 1.7691659927368164
Batch 64/64 loss: -0.5913410186767578
Epoch 28  Train loss: 2.0243531245811313  Val loss: 2.0205603334092603
Epoch 29
-------------------------------
Batch 1/64 loss: 2.0794644355773926
Batch 2/64 loss: 1.8295397758483887
Batch 3/64 loss: 1.8859343528747559
Batch 4/64 loss: 1.8442344665527344
Batch 5/64 loss: 2.0592808723449707
Batch 6/64 loss: 2.02675199508667
Batch 7/64 loss: 2.3186469078063965
Batch 8/64 loss: 1.9674444198608398
Batch 9/64 loss: 1.8980088233947754
Batch 10/64 loss: 1.7886090278625488
Batch 11/64 loss: 2.6206116676330566
Batch 12/64 loss: 2.0906357765197754
Batch 13/64 loss: 2.141231060028076
Batch 14/64 loss: 2.3181252479553223
Batch 15/64 loss: 2.5662841796875
Batch 16/64 loss: 3.1314358711242676
Batch 17/64 loss: 2.962851047515869
Batch 18/64 loss: 2.8599801063537598
Batch 19/64 loss: 2.496084213256836
Batch 20/64 loss: 2.7027711868286133
Batch 21/64 loss: 2.6565771102905273
Batch 22/64 loss: 6.756563186645508
Batch 23/64 loss: 2.909942150115967
Batch 24/64 loss: 2.9218287467956543
Batch 25/64 loss: 2.736156940460205
Batch 26/64 loss: 2.8814632892608643
Batch 27/64 loss: 2.959434986114502
Batch 28/64 loss: 2.656330108642578
Batch 29/64 loss: 2.666829824447632
Batch 30/64 loss: 3.19588303565979
Batch 31/64 loss: 2.846769332885742
Batch 32/64 loss: 2.843991279602051
Batch 33/64 loss: 2.6698076725006104
Batch 34/64 loss: 3.0256779193878174
Batch 35/64 loss: 2.795900583267212
Batch 36/64 loss: 2.8808441162109375
Batch 37/64 loss: 2.7988998889923096
Batch 38/64 loss: 2.607809066772461
Batch 39/64 loss: 2.3698794841766357
Batch 40/64 loss: 2.478520393371582
Batch 41/64 loss: 2.5348634719848633
Batch 42/64 loss: 2.3942179679870605
Batch 43/64 loss: 2.3748137950897217
Batch 44/64 loss: 2.6492486000061035
Batch 45/64 loss: 2.640918731689453
Batch 46/64 loss: 2.3756656646728516
Batch 47/64 loss: 2.673544406890869
Batch 48/64 loss: 2.308434009552002
Batch 49/64 loss: 2.3921871185302734
Batch 50/64 loss: 3.078993797302246
Batch 51/64 loss: 2.3651132583618164
Batch 52/64 loss: 2.6438145637512207
Batch 53/64 loss: 2.3787546157836914
Batch 54/64 loss: 2.2967305183410645
Batch 55/64 loss: 2.460391044616699
Batch 56/64 loss: 2.4517369270324707
Batch 57/64 loss: 2.0858702659606934
Batch 58/64 loss: 2.4409255981445312
Batch 59/64 loss: 2.244016647338867
Batch 60/64 loss: 2.12418270111084
Batch 61/64 loss: 2.3744540214538574
Batch 62/64 loss: 2.135448455810547
Batch 63/64 loss: 2.1032967567443848
Batch 64/64 loss: 0.14904117584228516
Epoch 29  Train loss: 2.5237088371725642  Val loss: 2.2632180767780317
Epoch 30
-------------------------------
Batch 1/64 loss: 2.155552864074707
Batch 2/64 loss: 2.1774072647094727
Batch 3/64 loss: 2.0351834297180176
Batch 4/64 loss: 2.38907527923584
Batch 5/64 loss: 2.3480734825134277
Batch 6/64 loss: 2.0614070892333984
Batch 7/64 loss: 2.554506301879883
Batch 8/64 loss: 2.153451442718506
Batch 9/64 loss: 2.3508753776550293
Batch 10/64 loss: 2.3048901557922363
Batch 11/64 loss: 2.2490315437316895
Batch 12/64 loss: 2.2213315963745117
Batch 13/64 loss: 2.169626235961914
Batch 14/64 loss: 2.0599212646484375
Batch 15/64 loss: 2.0740966796875
Batch 16/64 loss: 2.113706588745117
Batch 17/64 loss: 2.280867576599121
Batch 18/64 loss: 2.2127747535705566
Batch 19/64 loss: 2.287114143371582
Batch 20/64 loss: 2.370121955871582
Batch 21/64 loss: 2.3489937782287598
Batch 22/64 loss: 2.044987678527832
Batch 23/64 loss: 2.200143337249756
Batch 24/64 loss: 2.3722901344299316
Batch 25/64 loss: 2.293336868286133
Batch 26/64 loss: 2.1493330001831055
Batch 27/64 loss: 2.2653250694274902
Batch 28/64 loss: 1.8868474960327148
Batch 29/64 loss: 2.454378128051758
Batch 30/64 loss: 2.1231698989868164
Batch 31/64 loss: 2.161764144897461
Batch 32/64 loss: 2.033339500427246
Batch 33/64 loss: 2.4117517471313477
Batch 34/64 loss: 2.147587776184082
Batch 35/64 loss: 2.077455520629883
Batch 36/64 loss: 2.1970314979553223
Batch 37/64 loss: 2.2789320945739746
Batch 38/64 loss: 1.9608607292175293
Batch 39/64 loss: 2.6903867721557617
Batch 40/64 loss: 1.8589978218078613
Batch 41/64 loss: 1.9430146217346191
Batch 42/64 loss: 1.978703498840332
Batch 43/64 loss: 1.803159236907959
Batch 44/64 loss: 2.3240137100219727
Batch 45/64 loss: 2.250929832458496
Batch 46/64 loss: 2.371725082397461
Batch 47/64 loss: 2.0799174308776855
Batch 48/64 loss: 2.164618492126465
Batch 49/64 loss: 1.9702796936035156
Batch 50/64 loss: 2.530362606048584
Batch 51/64 loss: 1.9497852325439453
Batch 52/64 loss: 2.0374197959899902
Batch 53/64 loss: 2.090402603149414
Batch 54/64 loss: 2.1269607543945312
Batch 55/64 loss: 2.1487174034118652
Batch 56/64 loss: 2.2410435676574707
Batch 57/64 loss: 2.78816556930542
Batch 58/64 loss: 2.095076560974121
Batch 59/64 loss: 2.27978515625
Batch 60/64 loss: 2.047837734222412
Batch 61/64 loss: 2.0341830253601074
Batch 62/64 loss: 2.183135986328125
Batch 63/64 loss: 2.004762649536133
Batch 64/64 loss: -0.36901283264160156
Epoch 30  Train loss: 2.1598928414139094  Val loss: 2.0519529388532605
Epoch 31
-------------------------------
Batch 1/64 loss: 1.9547314643859863
Batch 2/64 loss: 2.0412187576293945
Batch 3/64 loss: 2.020167350769043
Batch 4/64 loss: 1.992784023284912
Batch 5/64 loss: 2.3381099700927734
Batch 6/64 loss: 2.4241743087768555
Batch 7/64 loss: 1.8822922706604004
Batch 8/64 loss: 2.104154586791992
Batch 9/64 loss: 1.965916633605957
Batch 10/64 loss: 2.3423995971679688
Batch 11/64 loss: 2.3562545776367188
Batch 12/64 loss: 1.985565185546875
Batch 13/64 loss: 1.8781228065490723
Batch 14/64 loss: 1.9247150421142578
Batch 15/64 loss: 1.9196934700012207
Batch 16/64 loss: 1.9850997924804688
Batch 17/64 loss: 1.8619794845581055
Batch 18/64 loss: 2.3504748344421387
Batch 19/64 loss: 2.1421704292297363
Batch 20/64 loss: 2.209580421447754
Batch 21/64 loss: 2.2354025840759277
Batch 22/64 loss: 2.3851795196533203
Batch 23/64 loss: 1.929931640625
Batch 24/64 loss: 2.053112506866455
Batch 25/64 loss: 2.2677369117736816
Batch 26/64 loss: 1.9670672416687012
Batch 27/64 loss: 2.2006101608276367
Batch 28/64 loss: 2.650737762451172
Batch 29/64 loss: 2.2855224609375
Batch 30/64 loss: 2.375570297241211
Batch 31/64 loss: 2.225271701812744
Batch 32/64 loss: 1.9526410102844238
Batch 33/64 loss: 2.2627620697021484
Batch 34/64 loss: 1.856865406036377
Batch 35/64 loss: 2.080648899078369
Batch 36/64 loss: 2.0427637100219727
Batch 37/64 loss: 2.2659525871276855
Batch 38/64 loss: 1.9768543243408203
Batch 39/64 loss: 2.092482566833496
Batch 40/64 loss: 2.23561954498291
Batch 41/64 loss: 2.301149368286133
Batch 42/64 loss: 2.236860752105713
Batch 43/64 loss: 2.00471830368042
Batch 44/64 loss: 2.022110939025879
Batch 45/64 loss: 2.211414337158203
Batch 46/64 loss: 2.6195168495178223
Batch 47/64 loss: 2.19706392288208
Batch 48/64 loss: 2.125617027282715
Batch 49/64 loss: 2.0343923568725586
Batch 50/64 loss: 2.183668613433838
Batch 51/64 loss: 2.0632619857788086
Batch 52/64 loss: 2.0261735916137695
Batch 53/64 loss: 2.235039234161377
Batch 54/64 loss: 2.0597290992736816
Batch 55/64 loss: 2.0664916038513184
Batch 56/64 loss: 2.2062630653381348
Batch 57/64 loss: 2.1479368209838867
Batch 58/64 loss: 2.104033946990967
Batch 59/64 loss: 2.25431489944458
Batch 60/64 loss: 2.1765999794006348
Batch 61/64 loss: 2.0677499771118164
Batch 62/64 loss: 2.149034023284912
Batch 63/64 loss: 2.1039280891418457
Batch 64/64 loss: -0.3121180534362793
Epoch 31  Train loss: 2.108004990745993  Val loss: 2.0832925514666893
Epoch 32
-------------------------------
Batch 1/64 loss: 2.1524105072021484
Batch 2/64 loss: 1.980916976928711
Batch 3/64 loss: 2.262864112854004
Batch 4/64 loss: 2.0854763984680176
Batch 5/64 loss: 1.8008379936218262
Batch 6/64 loss: 2.034726142883301
Batch 7/64 loss: 1.8013315200805664
Batch 8/64 loss: 1.9539222717285156
Batch 9/64 loss: 2.050604820251465
Batch 10/64 loss: 2.4310336112976074
Batch 11/64 loss: 2.005096435546875
Batch 12/64 loss: 2.0400028228759766
Batch 13/64 loss: 2.06198787689209
Batch 14/64 loss: 2.0376830101013184
Batch 15/64 loss: 2.08426570892334
Batch 16/64 loss: 2.08172607421875
Batch 17/64 loss: 2.2481608390808105
Batch 18/64 loss: 2.3024940490722656
Batch 19/64 loss: 2.1392626762390137
Batch 20/64 loss: 2.2911124229431152
Batch 21/64 loss: 2.3912649154663086
Batch 22/64 loss: 2.277092933654785
Batch 23/64 loss: 2.566953659057617
Batch 24/64 loss: 1.7112884521484375
Batch 25/64 loss: 2.436063289642334
Batch 26/64 loss: 2.0889992713928223
Batch 27/64 loss: 2.063945770263672
Batch 28/64 loss: 1.9534897804260254
Batch 29/64 loss: 2.264349937438965
Batch 30/64 loss: 2.088627815246582
Batch 31/64 loss: 2.33760929107666
Batch 32/64 loss: 2.0520553588867188
Batch 33/64 loss: 2.145625114440918
Batch 34/64 loss: 2.395700454711914
Batch 35/64 loss: 2.0681676864624023
Batch 36/64 loss: 1.9795966148376465
Batch 37/64 loss: 2.2614517211914062
Batch 38/64 loss: 2.2624034881591797
Batch 39/64 loss: 2.4039835929870605
Batch 40/64 loss: 2.3666744232177734
Batch 41/64 loss: 2.075622081756592
Batch 42/64 loss: 2.208298683166504
Batch 43/64 loss: 2.2302536964416504
Batch 44/64 loss: 2.1651358604431152
Batch 45/64 loss: 1.9295859336853027
Batch 46/64 loss: 1.9588522911071777
Batch 47/64 loss: 1.9281911849975586
Batch 48/64 loss: 2.1226768493652344
Batch 49/64 loss: 1.9466133117675781
Batch 50/64 loss: 2.180250644683838
Batch 51/64 loss: 2.307356357574463
Batch 52/64 loss: 1.9143662452697754
Batch 53/64 loss: 1.9818711280822754
Batch 54/64 loss: 2.0605921745300293
Batch 55/64 loss: 2.394031047821045
Batch 56/64 loss: 2.0000147819519043
Batch 57/64 loss: 1.9659347534179688
Batch 58/64 loss: 2.459174633026123
Batch 59/64 loss: 2.07918643951416
Batch 60/64 loss: 2.018667221069336
Batch 61/64 loss: 2.092641830444336
Batch 62/64 loss: 2.0985031127929688
Batch 63/64 loss: 2.461646556854248
Batch 64/64 loss: -0.4889869689941406
Epoch 32  Train loss: 2.104689983293122  Val loss: 1.9916947092796928
Saving best model, epoch: 32
Epoch 33
-------------------------------
Batch 1/64 loss: 1.9934349060058594
Batch 2/64 loss: 1.8381056785583496
Batch 3/64 loss: 2.1452903747558594
Batch 4/64 loss: 1.9445137977600098
Batch 5/64 loss: 2.3028321266174316
Batch 6/64 loss: 1.9421544075012207
Batch 7/64 loss: 2.1771669387817383
Batch 8/64 loss: 2.020443916320801
Batch 9/64 loss: 2.2524123191833496
Batch 10/64 loss: 1.9752602577209473
Batch 11/64 loss: 2.014981269836426
Batch 12/64 loss: 1.9304766654968262
Batch 13/64 loss: 1.8255786895751953
Batch 14/64 loss: 1.9042105674743652
Batch 15/64 loss: 1.9742398262023926
Batch 16/64 loss: 2.2490968704223633
Batch 17/64 loss: 1.9500250816345215
Batch 18/64 loss: 2.1078615188598633
Batch 19/64 loss: 2.216823101043701
Batch 20/64 loss: 2.0033249855041504
Batch 21/64 loss: 1.9447760581970215
Batch 22/64 loss: 1.8964428901672363
Batch 23/64 loss: 2.0700621604919434
Batch 24/64 loss: 2.0211682319641113
Batch 25/64 loss: 2.2646303176879883
Batch 26/64 loss: 2.689074993133545
Batch 27/64 loss: 2.110792636871338
Batch 28/64 loss: 2.1996569633483887
Batch 29/64 loss: 1.8677964210510254
Batch 30/64 loss: 1.9436373710632324
Batch 31/64 loss: 1.9095458984375
Batch 32/64 loss: 2.0458054542541504
Batch 33/64 loss: 1.8298583030700684
Batch 34/64 loss: 2.3174166679382324
Batch 35/64 loss: 1.9888787269592285
Batch 36/64 loss: 1.9440321922302246
Batch 37/64 loss: 2.2988228797912598
Batch 38/64 loss: 2.403233528137207
Batch 39/64 loss: 2.1715712547302246
Batch 40/64 loss: 1.890512466430664
Batch 41/64 loss: 2.142228126525879
Batch 42/64 loss: 1.909372329711914
Batch 43/64 loss: 2.3712143898010254
Batch 44/64 loss: 2.1574368476867676
Batch 45/64 loss: 1.9378924369812012
Batch 46/64 loss: 2.1876282691955566
Batch 47/64 loss: 2.0840377807617188
Batch 48/64 loss: 2.1300249099731445
Batch 49/64 loss: 1.9173288345336914
Batch 50/64 loss: 1.9759387969970703
Batch 51/64 loss: 1.9957318305969238
Batch 52/64 loss: 1.82655668258667
Batch 53/64 loss: 2.1448488235473633
Batch 54/64 loss: 1.8978972434997559
Batch 55/64 loss: 1.828697681427002
Batch 56/64 loss: 2.028055191040039
Batch 57/64 loss: 2.2367348670959473
Batch 58/64 loss: 1.987769603729248
Batch 59/64 loss: 2.185049533843994
Batch 60/64 loss: 1.9837722778320312
Batch 61/64 loss: 1.6855249404907227
Batch 62/64 loss: 2.010622978210449
Batch 63/64 loss: 2.232421398162842
Batch 64/64 loss: -0.5894112586975098
Epoch 33  Train loss: 2.0233831536536124  Val loss: 1.9410282803564956
Saving best model, epoch: 33
Epoch 34
-------------------------------
Batch 1/64 loss: 1.8164873123168945
Batch 2/64 loss: 1.8882246017456055
Batch 3/64 loss: 2.210845947265625
Batch 4/64 loss: 2.272918224334717
Batch 5/64 loss: 2.008350372314453
Batch 6/64 loss: 2.3244800567626953
Batch 7/64 loss: 1.9294638633728027
Batch 8/64 loss: 2.081113338470459
Batch 9/64 loss: 2.0479040145874023
Batch 10/64 loss: 1.7889313697814941
Batch 11/64 loss: 2.0607972145080566
Batch 12/64 loss: 1.9647226333618164
Batch 13/64 loss: 1.8917121887207031
Batch 14/64 loss: 2.4706673622131348
Batch 15/64 loss: 2.1128311157226562
Batch 16/64 loss: 1.5975961685180664
Batch 17/64 loss: 1.9722185134887695
Batch 18/64 loss: 2.020984172821045
Batch 19/64 loss: 1.9820480346679688
Batch 20/64 loss: 1.9904699325561523
Batch 21/64 loss: 2.0303311347961426
Batch 22/64 loss: 1.8630342483520508
Batch 23/64 loss: 1.970931053161621
Batch 24/64 loss: 2.097994804382324
Batch 25/64 loss: 1.7870450019836426
Batch 26/64 loss: 1.7015671730041504
Batch 27/64 loss: 1.9488439559936523
Batch 28/64 loss: 2.198269844055176
Batch 29/64 loss: 1.9546875953674316
Batch 30/64 loss: 1.769050121307373
Batch 31/64 loss: 1.8065705299377441
Batch 32/64 loss: 1.8635282516479492
Batch 33/64 loss: 2.084249973297119
Batch 34/64 loss: 1.784367561340332
Batch 35/64 loss: 1.8449358940124512
Batch 36/64 loss: 2.1058497428894043
Batch 37/64 loss: 1.8896756172180176
Batch 38/64 loss: 1.998011589050293
Batch 39/64 loss: 1.8831076622009277
Batch 40/64 loss: 1.929689884185791
Batch 41/64 loss: 1.7359929084777832
Batch 42/64 loss: 2.141080379486084
Batch 43/64 loss: 1.9172396659851074
Batch 44/64 loss: 2.061924457550049
Batch 45/64 loss: 2.140498638153076
Batch 46/64 loss: 2.0656867027282715
Batch 47/64 loss: 2.013580322265625
Batch 48/64 loss: 2.227768898010254
Batch 49/64 loss: 1.8581652641296387
Batch 50/64 loss: 2.325615406036377
Batch 51/64 loss: 1.8608269691467285
Batch 52/64 loss: 1.631141185760498
Batch 53/64 loss: 2.012653350830078
Batch 54/64 loss: 2.1926937103271484
Batch 55/64 loss: 1.9863805770874023
Batch 56/64 loss: 1.9057259559631348
Batch 57/64 loss: 2.078613758087158
Batch 58/64 loss: 2.0119829177856445
Batch 59/64 loss: 2.1852102279663086
Batch 60/64 loss: 2.0072059631347656
Batch 61/64 loss: 2.068582534790039
Batch 62/64 loss: 1.9336657524108887
Batch 63/64 loss: 2.298318386077881
Batch 64/64 loss: -0.5571780204772949
Epoch 34  Train loss: 1.9637204469418994  Val loss: 2.150662936705494
Epoch 35
-------------------------------
Batch 1/64 loss: 1.6953058242797852
Batch 2/64 loss: 2.0083236694335938
Batch 3/64 loss: 1.9631266593933105
Batch 4/64 loss: 1.8485932350158691
Batch 5/64 loss: 1.9266209602355957
Batch 6/64 loss: 2.1221423149108887
Batch 7/64 loss: 2.3953909873962402
Batch 8/64 loss: 1.8989787101745605
Batch 9/64 loss: 2.1664209365844727
Batch 10/64 loss: 2.176934242248535
Batch 11/64 loss: 2.2966108322143555
Batch 12/64 loss: 2.068735122680664
Batch 13/64 loss: 2.1902952194213867
Batch 14/64 loss: 1.5998773574829102
Batch 15/64 loss: 2.0413784980773926
Batch 16/64 loss: 1.7186145782470703
Batch 17/64 loss: 1.999007225036621
Batch 18/64 loss: 1.7816367149353027
Batch 19/64 loss: 1.9726629257202148
Batch 20/64 loss: 2.2175660133361816
Batch 21/64 loss: 1.9820890426635742
Batch 22/64 loss: 1.7837944030761719
Batch 23/64 loss: 1.948488712310791
Batch 24/64 loss: 1.8448858261108398
Batch 25/64 loss: 1.8359417915344238
Batch 26/64 loss: 2.138237953186035
Batch 27/64 loss: 1.9483280181884766
Batch 28/64 loss: 1.9937467575073242
Batch 29/64 loss: 2.19525146484375
Batch 30/64 loss: 2.283512592315674
Batch 31/64 loss: 2.337677478790283
Batch 32/64 loss: 2.1313581466674805
Batch 33/64 loss: 1.8528614044189453
Batch 34/64 loss: 1.8662948608398438
Batch 35/64 loss: 2.1201224327087402
Batch 36/64 loss: 1.9917254447937012
Batch 37/64 loss: 2.24733304977417
Batch 38/64 loss: 2.212343692779541
Batch 39/64 loss: 2.508695125579834
Batch 40/64 loss: 2.0104799270629883
Batch 41/64 loss: 2.069307804107666
Batch 42/64 loss: 2.018338680267334
Batch 43/64 loss: 2.278864860534668
Batch 44/64 loss: 2.2435083389282227
Batch 45/64 loss: 1.9546122550964355
Batch 46/64 loss: 1.9608206748962402
Batch 47/64 loss: 2.2408838272094727
Batch 48/64 loss: 2.1271252632141113
Batch 49/64 loss: 2.0308518409729004
Batch 50/64 loss: 1.8267111778259277
Batch 51/64 loss: 2.2415108680725098
Batch 52/64 loss: 2.470944881439209
Batch 53/64 loss: 2.1279091835021973
Batch 54/64 loss: 2.33571720123291
Batch 55/64 loss: 1.9787907600402832
Batch 56/64 loss: 1.799163818359375
Batch 57/64 loss: 2.244760036468506
Batch 58/64 loss: 2.335815906524658
Batch 59/64 loss: 2.072587490081787
Batch 60/64 loss: 1.8417725563049316
Batch 61/64 loss: 2.093489170074463
Batch 62/64 loss: 1.9733672142028809
Batch 63/64 loss: 1.925954818725586
Batch 64/64 loss: -0.3839302062988281
Epoch 35  Train loss: 2.0270784490248737  Val loss: 2.0166702204963185
Epoch 36
-------------------------------
Batch 1/64 loss: 1.9047465324401855
Batch 2/64 loss: 1.8966283798217773
Batch 3/64 loss: 1.7931256294250488
Batch 4/64 loss: 2.153501510620117
Batch 5/64 loss: 2.014754295349121
Batch 6/64 loss: 1.8711986541748047
Batch 7/64 loss: 1.9712653160095215
Batch 8/64 loss: 2.022305488586426
Batch 9/64 loss: 2.1341915130615234
Batch 10/64 loss: 2.099393367767334
Batch 11/64 loss: 1.937598705291748
Batch 12/64 loss: 1.8897662162780762
Batch 13/64 loss: 2.05051326751709
Batch 14/64 loss: 1.7583818435668945
Batch 15/64 loss: 2.2576117515563965
Batch 16/64 loss: 1.789729118347168
Batch 17/64 loss: 1.721369743347168
Batch 18/64 loss: 2.985677719116211
Batch 19/64 loss: 2.1626663208007812
Batch 20/64 loss: 2.101229667663574
Batch 21/64 loss: 1.9688630104064941
Batch 22/64 loss: 2.2204694747924805
Batch 23/64 loss: 2.2272849082946777
Batch 24/64 loss: 2.183877944946289
Batch 25/64 loss: 1.986720085144043
Batch 26/64 loss: 2.1365275382995605
Batch 27/64 loss: 1.8394532203674316
Batch 28/64 loss: 1.929534912109375
Batch 29/64 loss: 2.350619316101074
Batch 30/64 loss: 1.846562385559082
Batch 31/64 loss: 2.2718300819396973
Batch 32/64 loss: 2.5075254440307617
Batch 33/64 loss: 2.0604605674743652
Batch 34/64 loss: 2.1349310874938965
Batch 35/64 loss: 1.8960728645324707
Batch 36/64 loss: 2.169597625732422
Batch 37/64 loss: 1.9188852310180664
Batch 38/64 loss: 1.7316203117370605
Batch 39/64 loss: 2.1473050117492676
Batch 40/64 loss: 1.781407356262207
Batch 41/64 loss: 1.6840591430664062
Batch 42/64 loss: 1.8121728897094727
Batch 43/64 loss: 1.855875015258789
Batch 44/64 loss: 2.08414888381958
Batch 45/64 loss: 2.0754246711730957
Batch 46/64 loss: 1.9147076606750488
Batch 47/64 loss: 2.0121498107910156
Batch 48/64 loss: 2.2357864379882812
Batch 49/64 loss: 1.7036800384521484
Batch 50/64 loss: 2.0068602561950684
Batch 51/64 loss: 2.0775928497314453
Batch 52/64 loss: 1.9039840698242188
Batch 53/64 loss: 1.8408284187316895
Batch 54/64 loss: 2.090728282928467
Batch 55/64 loss: 2.006315231323242
Batch 56/64 loss: 1.9526095390319824
Batch 57/64 loss: 1.8936967849731445
Batch 58/64 loss: 1.7885665893554688
Batch 59/64 loss: 1.9924125671386719
Batch 60/64 loss: 1.9911680221557617
Batch 61/64 loss: 1.7105321884155273
Batch 62/64 loss: 2.251880645751953
Batch 63/64 loss: 1.9422640800476074
Batch 64/64 loss: -0.8514261245727539
Epoch 36  Train loss: 1.9766914180680817  Val loss: 1.8588187620811856
Saving best model, epoch: 36
Epoch 37
-------------------------------
Batch 1/64 loss: 2.0140185356140137
Batch 2/64 loss: 1.8536639213562012
Batch 3/64 loss: 1.851552963256836
Batch 4/64 loss: 1.8116445541381836
Batch 5/64 loss: 1.9321050643920898
Batch 6/64 loss: 2.391873359680176
Batch 7/64 loss: 1.904313087463379
Batch 8/64 loss: 1.7918438911437988
Batch 9/64 loss: 2.056328773498535
Batch 10/64 loss: 1.993241786956787
Batch 11/64 loss: 1.6866655349731445
Batch 12/64 loss: 2.0150208473205566
Batch 13/64 loss: 1.8232078552246094
Batch 14/64 loss: 1.930861473083496
Batch 15/64 loss: 2.1972455978393555
Batch 16/64 loss: 2.1068129539489746
Batch 17/64 loss: 1.9626426696777344
Batch 18/64 loss: 1.6908283233642578
Batch 19/64 loss: 1.793837070465088
Batch 20/64 loss: 1.792367935180664
Batch 21/64 loss: 1.830655574798584
Batch 22/64 loss: 1.604578971862793
Batch 23/64 loss: 1.8615326881408691
Batch 24/64 loss: 1.8279995918273926
Batch 25/64 loss: 1.7098402976989746
Batch 26/64 loss: 1.9600262641906738
Batch 27/64 loss: 2.0990777015686035
Batch 28/64 loss: 1.9830632209777832
Batch 29/64 loss: 2.063936710357666
Batch 30/64 loss: 2.1940956115722656
Batch 31/64 loss: 1.9247851371765137
Batch 32/64 loss: 1.8248658180236816
Batch 33/64 loss: 1.9827933311462402
Batch 34/64 loss: 1.7308459281921387
Batch 35/64 loss: 1.9351272583007812
Batch 36/64 loss: 2.0311431884765625
Batch 37/64 loss: 1.7412900924682617
Batch 38/64 loss: 2.1159467697143555
Batch 39/64 loss: 2.202975273132324
Batch 40/64 loss: 2.270834445953369
Batch 41/64 loss: 1.9052023887634277
Batch 42/64 loss: 1.885178565979004
Batch 43/64 loss: 1.9110188484191895
Batch 44/64 loss: 1.9524831771850586
Batch 45/64 loss: 1.8930344581604004
Batch 46/64 loss: 1.836988925933838
Batch 47/64 loss: 1.7811388969421387
Batch 48/64 loss: 2.1621227264404297
Batch 49/64 loss: 1.6887216567993164
Batch 50/64 loss: 2.0818772315979004
Batch 51/64 loss: 1.946357250213623
Batch 52/64 loss: 2.26698637008667
Batch 53/64 loss: 1.8292388916015625
Batch 54/64 loss: 1.9239239692687988
Batch 55/64 loss: 1.8131084442138672
Batch 56/64 loss: 2.103764533996582
Batch 57/64 loss: 2.204312801361084
Batch 58/64 loss: 1.8311452865600586
Batch 59/64 loss: 1.6537127494812012
Batch 60/64 loss: 2.20858097076416
Batch 61/64 loss: 1.7635111808776855
Batch 62/64 loss: 1.7779655456542969
Batch 63/64 loss: 1.858062744140625
Batch 64/64 loss: -1.257153034210205
Epoch 37  Train loss: 1.89538922216378  Val loss: 1.8660773444421512
Epoch 38
-------------------------------
Batch 1/64 loss: 2.0538291931152344
Batch 2/64 loss: 1.6420378684997559
Batch 3/64 loss: 1.8234877586364746
Batch 4/64 loss: 1.9097938537597656
Batch 5/64 loss: 1.7704057693481445
Batch 6/64 loss: 2.3353285789489746
Batch 7/64 loss: 1.8158516883850098
Batch 8/64 loss: 1.7738580703735352
Batch 9/64 loss: 1.806058406829834
Batch 10/64 loss: 1.9477653503417969
Batch 11/64 loss: 1.8323054313659668
Batch 12/64 loss: 1.9085183143615723
Batch 13/64 loss: 1.6692442893981934
Batch 14/64 loss: 1.8196039199829102
Batch 15/64 loss: 1.9411077499389648
Batch 16/64 loss: 2.0182061195373535
Batch 17/64 loss: 1.83538818359375
Batch 18/64 loss: 2.139662265777588
Batch 19/64 loss: 1.809138298034668
Batch 20/64 loss: 2.102421283721924
Batch 21/64 loss: 1.9021787643432617
Batch 22/64 loss: 1.7394800186157227
Batch 23/64 loss: 1.7965455055236816
Batch 24/64 loss: 1.8022747039794922
Batch 25/64 loss: 2.017406940460205
Batch 26/64 loss: 1.878443717956543
Batch 27/64 loss: 1.773099422454834
Batch 28/64 loss: 1.893918514251709
Batch 29/64 loss: 1.774548053741455
Batch 30/64 loss: 1.7687792778015137
Batch 31/64 loss: 1.9424448013305664
Batch 32/64 loss: 2.1265664100646973
Batch 33/64 loss: 2.1460819244384766
Batch 34/64 loss: 1.708963394165039
Batch 35/64 loss: 2.0996298789978027
Batch 36/64 loss: 2.2732529640197754
Batch 37/64 loss: 1.9309892654418945
Batch 38/64 loss: 1.8696398735046387
Batch 39/64 loss: 1.7837543487548828
Batch 40/64 loss: 1.6299285888671875
Batch 41/64 loss: 1.785440444946289
Batch 42/64 loss: 2.075528144836426
Batch 43/64 loss: 1.952704906463623
Batch 44/64 loss: 1.8389534950256348
Batch 45/64 loss: 1.8536896705627441
Batch 46/64 loss: 1.791715145111084
Batch 47/64 loss: 1.762563705444336
Batch 48/64 loss: 1.6487140655517578
Batch 49/64 loss: 1.7065939903259277
Batch 50/64 loss: 2.235379695892334
Batch 51/64 loss: 2.0258750915527344
Batch 52/64 loss: 2.00418758392334
Batch 53/64 loss: 1.673487663269043
Batch 54/64 loss: 1.7253351211547852
Batch 55/64 loss: 1.621934413909912
Batch 56/64 loss: 1.9085378646850586
Batch 57/64 loss: 2.021998405456543
Batch 58/64 loss: 1.8713569641113281
Batch 59/64 loss: 2.327657699584961
Batch 60/64 loss: 1.6974177360534668
Batch 61/64 loss: 1.616325855255127
Batch 62/64 loss: 1.664604663848877
Batch 63/64 loss: 2.1268625259399414
Batch 64/64 loss: -1.00258207321167
Epoch 38  Train loss: 1.8509312480103737  Val loss: 1.8306126610929614
Saving best model, epoch: 38
Epoch 39
-------------------------------
Batch 1/64 loss: 2.0577878952026367
Batch 2/64 loss: 1.5304932594299316
Batch 3/64 loss: 1.987314224243164
Batch 4/64 loss: 2.139791965484619
Batch 5/64 loss: 2.002951145172119
Batch 6/64 loss: 1.7348623275756836
Batch 7/64 loss: 1.5084848403930664
Batch 8/64 loss: 2.3062820434570312
Batch 9/64 loss: 1.963183879852295
Batch 10/64 loss: 1.819803237915039
Batch 11/64 loss: 1.7656779289245605
Batch 12/64 loss: 1.9238743782043457
Batch 13/64 loss: 2.319139003753662
Batch 14/64 loss: 1.9368066787719727
Batch 15/64 loss: 2.010061264038086
Batch 16/64 loss: 1.587066650390625
Batch 17/64 loss: 1.8088312149047852
Batch 18/64 loss: 2.0411810874938965
Batch 19/64 loss: 1.880659580230713
Batch 20/64 loss: 1.9812679290771484
Batch 21/64 loss: 2.4716897010803223
Batch 22/64 loss: 1.7833681106567383
Batch 23/64 loss: 1.793039321899414
Batch 24/64 loss: 2.2680654525756836
Batch 25/64 loss: 1.8381714820861816
Batch 26/64 loss: 2.0103464126586914
Batch 27/64 loss: 2.1197900772094727
Batch 28/64 loss: 1.7552571296691895
Batch 29/64 loss: 1.755589485168457
Batch 30/64 loss: 1.7110323905944824
Batch 31/64 loss: 1.7668681144714355
Batch 32/64 loss: 1.6374320983886719
Batch 33/64 loss: 1.739201545715332
Batch 34/64 loss: 1.7743077278137207
Batch 35/64 loss: 1.7937860488891602
Batch 36/64 loss: 1.800644874572754
Batch 37/64 loss: 1.6324925422668457
Batch 38/64 loss: 1.9881305694580078
Batch 39/64 loss: 1.9639439582824707
Batch 40/64 loss: 2.2812156677246094
Batch 41/64 loss: 1.7204804420471191
Batch 42/64 loss: 1.9120287895202637
Batch 43/64 loss: 1.8841848373413086
Batch 44/64 loss: 1.7429723739624023
Batch 45/64 loss: 1.9932470321655273
Batch 46/64 loss: 2.060555934906006
Batch 47/64 loss: 1.7997822761535645
Batch 48/64 loss: 2.020653247833252
Batch 49/64 loss: 1.7951064109802246
Batch 50/64 loss: 2.086418628692627
Batch 51/64 loss: 1.7368865013122559
Batch 52/64 loss: 1.8920588493347168
Batch 53/64 loss: 1.6604690551757812
Batch 54/64 loss: 1.7152299880981445
Batch 55/64 loss: 2.0252251625061035
Batch 56/64 loss: 1.8521728515625
Batch 57/64 loss: 1.6426753997802734
Batch 58/64 loss: 1.6055593490600586
Batch 59/64 loss: 1.520857334136963
Batch 60/64 loss: 1.8354640007019043
Batch 61/64 loss: 1.8625516891479492
Batch 62/64 loss: 1.7136821746826172
Batch 63/64 loss: 2.03965425491333
Batch 64/64 loss: -1.2059836387634277
Epoch 39  Train loss: 1.8416207388335584  Val loss: 6.139806714664211
Epoch 40
-------------------------------
Batch 1/64 loss: 1.7317657470703125
Batch 2/64 loss: 1.5165190696716309
Batch 3/64 loss: 1.5276002883911133
Batch 4/64 loss: 1.9424424171447754
Batch 5/64 loss: 1.7763419151306152
Batch 6/64 loss: 2.3694300651550293
Batch 7/64 loss: 1.6997623443603516
Batch 8/64 loss: 1.828904151916504
Batch 9/64 loss: 2.127554416656494
Batch 10/64 loss: 1.6001739501953125
Batch 11/64 loss: 1.636871337890625
Batch 12/64 loss: 1.9031286239624023
Batch 13/64 loss: 2.076592445373535
Batch 14/64 loss: 1.8884553909301758
Batch 15/64 loss: 1.8279647827148438
Batch 16/64 loss: 1.91361665725708
Batch 17/64 loss: 1.7104082107543945
Batch 18/64 loss: 2.018679618835449
Batch 19/64 loss: 1.7680907249450684
Batch 20/64 loss: 1.6587958335876465
Batch 21/64 loss: 2.0364179611206055
Batch 22/64 loss: 2.088960647583008
Batch 23/64 loss: 2.0319485664367676
Batch 24/64 loss: 1.7429776191711426
Batch 25/64 loss: 1.7710304260253906
Batch 26/64 loss: 2.1542186737060547
Batch 27/64 loss: 1.7405829429626465
Batch 28/64 loss: 1.9896430969238281
Batch 29/64 loss: 1.9847850799560547
Batch 30/64 loss: 1.738807201385498
Batch 31/64 loss: 2.123106002807617
Batch 32/64 loss: 2.7994556427001953
Batch 33/64 loss: 2.7306880950927734
Batch 34/64 loss: 3.0082406997680664
Batch 35/64 loss: 3.6549954414367676
Batch 36/64 loss: 2.839874744415283
Batch 37/64 loss: 3.5499134063720703
Batch 38/64 loss: 3.2230052947998047
Batch 39/64 loss: 2.706936836242676
Batch 40/64 loss: 4.335679531097412
Batch 41/64 loss: 2.7573599815368652
Batch 42/64 loss: 2.6522364616394043
Batch 43/64 loss: 2.880019187927246
Batch 44/64 loss: 3.0075316429138184
Batch 45/64 loss: 2.632349967956543
Batch 46/64 loss: 2.9578661918640137
Batch 47/64 loss: 2.5095419883728027
Batch 48/64 loss: 2.6258230209350586
Batch 49/64 loss: 2.5925121307373047
Batch 50/64 loss: 2.443113327026367
Batch 51/64 loss: 2.375746726989746
Batch 52/64 loss: 2.5257091522216797
Batch 53/64 loss: 3.137439727783203
Batch 54/64 loss: 2.4979043006896973
Batch 55/64 loss: 2.707327365875244
Batch 56/64 loss: 3.118001937866211
Batch 57/64 loss: 2.551297664642334
Batch 58/64 loss: 2.576054096221924
Batch 59/64 loss: 2.5385751724243164
Batch 60/64 loss: 2.6213603019714355
Batch 61/64 loss: 2.406622886657715
Batch 62/64 loss: 2.33071231842041
Batch 63/64 loss: 2.485748291015625
Batch 64/64 loss: -0.24277448654174805
Epoch 40  Train loss: 2.314088453031054  Val loss: 2.557749315635445
Epoch 41
-------------------------------
Batch 1/64 loss: 2.4526891708374023
Batch 2/64 loss: 2.322876453399658
Batch 3/64 loss: 2.3772687911987305
Batch 4/64 loss: 2.2859368324279785
Batch 5/64 loss: 2.068075180053711
Batch 6/64 loss: 2.102421283721924
Batch 7/64 loss: 2.320371627807617
Batch 8/64 loss: 2.39778470993042
Batch 9/64 loss: 2.5642685890197754
Batch 10/64 loss: 2.302767753601074
Batch 11/64 loss: 2.6194467544555664
Batch 12/64 loss: 2.512094497680664
Batch 13/64 loss: 2.301255226135254
Batch 14/64 loss: 2.1930336952209473
Batch 15/64 loss: 2.286062717437744
Batch 16/64 loss: 2.4863877296447754
Batch 17/64 loss: 2.597353458404541
Batch 18/64 loss: 2.283679485321045
Batch 19/64 loss: 2.2641210556030273
Batch 20/64 loss: 2.761636257171631
Batch 21/64 loss: 2.154932975769043
Batch 22/64 loss: 2.2532644271850586
Batch 23/64 loss: 2.1276135444641113
Batch 24/64 loss: 2.26090669631958
Batch 25/64 loss: 2.235764503479004
Batch 26/64 loss: 2.374907970428467
Batch 27/64 loss: 1.9663233757019043
Batch 28/64 loss: 2.166299343109131
Batch 29/64 loss: 2.2454981803894043
Batch 30/64 loss: 2.1813011169433594
Batch 31/64 loss: 1.970449447631836
Batch 32/64 loss: 2.215707778930664
Batch 33/64 loss: 3.2683987617492676
Batch 34/64 loss: 2.316434860229492
Batch 35/64 loss: 2.0550994873046875
Batch 36/64 loss: 2.598318099975586
Batch 37/64 loss: 2.124403476715088
Batch 38/64 loss: 2.230983257293701
Batch 39/64 loss: 2.1516709327697754
Batch 40/64 loss: 2.3156042098999023
Batch 41/64 loss: 2.0743794441223145
Batch 42/64 loss: 2.4182634353637695
Batch 43/64 loss: 2.5598106384277344
Batch 44/64 loss: 2.5289230346679688
Batch 45/64 loss: 2.156813621520996
Batch 46/64 loss: 2.259793281555176
Batch 47/64 loss: 2.3307642936706543
Batch 48/64 loss: 2.0293288230895996
Batch 49/64 loss: 2.371140956878662
Batch 50/64 loss: 2.2882351875305176
Batch 51/64 loss: 2.0503721237182617
Batch 52/64 loss: 2.405348300933838
Batch 53/64 loss: 2.109374523162842
Batch 54/64 loss: 2.2920384407043457
Batch 55/64 loss: 2.293886184692383
Batch 56/64 loss: 2.0977988243103027
Batch 57/64 loss: 1.959477424621582
Batch 58/64 loss: 2.1989660263061523
Batch 59/64 loss: 2.0880961418151855
Batch 60/64 loss: 2.530735492706299
Batch 61/64 loss: 2.1342830657958984
Batch 62/64 loss: 2.0521841049194336
Batch 63/64 loss: 2.0747313499450684
Batch 64/64 loss: -0.4481649398803711
Epoch 41  Train loss: 2.253835838916255  Val loss: 2.0411494146917284
Epoch 42
-------------------------------
Batch 1/64 loss: 2.0072264671325684
Batch 2/64 loss: 2.197885036468506
Batch 3/64 loss: 2.5251388549804688
Batch 4/64 loss: 2.26151704788208
Batch 5/64 loss: 1.9978399276733398
Batch 6/64 loss: 2.2289719581604004
Batch 7/64 loss: 2.1578798294067383
Batch 8/64 loss: 2.0093541145324707
Batch 9/64 loss: 2.203406810760498
Batch 10/64 loss: 2.008331298828125
Batch 11/64 loss: 2.009315013885498
Batch 12/64 loss: 1.8691434860229492
Batch 13/64 loss: 2.0824079513549805
Batch 14/64 loss: 2.09273099899292
Batch 15/64 loss: 2.0324478149414062
Batch 16/64 loss: 2.089848041534424
Batch 17/64 loss: 1.9445767402648926
Batch 18/64 loss: 1.8896126747131348
Batch 19/64 loss: 1.8948101997375488
Batch 20/64 loss: 1.915555477142334
Batch 21/64 loss: 2.115504264831543
Batch 22/64 loss: 2.1313796043395996
Batch 23/64 loss: 2.1367130279541016
Batch 24/64 loss: 1.9563817977905273
Batch 25/64 loss: 1.8570146560668945
Batch 26/64 loss: 1.872185230255127
Batch 27/64 loss: 1.936020851135254
Batch 28/64 loss: 2.1505789756774902
Batch 29/64 loss: 2.1405062675476074
Batch 30/64 loss: 2.19864559173584
Batch 31/64 loss: 1.7892231941223145
Batch 32/64 loss: 2.0521140098571777
Batch 33/64 loss: 2.042095184326172
Batch 34/64 loss: 1.6971487998962402
Batch 35/64 loss: 2.298837661743164
Batch 36/64 loss: 2.0855531692504883
Batch 37/64 loss: 1.9714469909667969
Batch 38/64 loss: 2.285590648651123
Batch 39/64 loss: 2.144195079803467
Batch 40/64 loss: 1.8251957893371582
Batch 41/64 loss: 1.8223638534545898
Batch 42/64 loss: 1.9269037246704102
Batch 43/64 loss: 1.8131132125854492
Batch 44/64 loss: 2.005547046661377
Batch 45/64 loss: 1.8024511337280273
Batch 46/64 loss: 2.038691520690918
Batch 47/64 loss: 1.9687986373901367
Batch 48/64 loss: 1.7565116882324219
Batch 49/64 loss: 2.3416638374328613
Batch 50/64 loss: 1.826291561126709
Batch 51/64 loss: 2.1092987060546875
Batch 52/64 loss: 2.0918283462524414
Batch 53/64 loss: 2.024507999420166
Batch 54/64 loss: 1.933647632598877
Batch 55/64 loss: 2.1330714225769043
Batch 56/64 loss: 2.1399288177490234
Batch 57/64 loss: 2.1874780654907227
Batch 58/64 loss: 2.8307852745056152
Batch 59/64 loss: 2.3066587448120117
Batch 60/64 loss: 2.0393991470336914
Batch 61/64 loss: 2.3122944831848145
Batch 62/64 loss: 2.2719597816467285
Batch 63/64 loss: 2.3394508361816406
Batch 64/64 loss: -0.4166727066040039
Epoch 42  Train loss: 2.036305435031068  Val loss: 4.17407191823848
Epoch 43
-------------------------------
Batch 1/64 loss: 2.6961259841918945
Batch 2/64 loss: 2.2701754570007324
Batch 3/64 loss: 2.5561013221740723
Batch 4/64 loss: 2.3043646812438965
Batch 5/64 loss: 2.295828342437744
Batch 6/64 loss: 2.3613743782043457
Batch 7/64 loss: 2.4443655014038086
Batch 8/64 loss: 2.297297477722168
Batch 9/64 loss: 2.27927827835083
Batch 10/64 loss: 2.3731188774108887
Batch 11/64 loss: 2.5276031494140625
Batch 12/64 loss: 2.1240601539611816
Batch 13/64 loss: 2.303953170776367
Batch 14/64 loss: 2.249668598175049
Batch 15/64 loss: 2.317535400390625
Batch 16/64 loss: 2.2684874534606934
Batch 17/64 loss: 2.4422402381896973
Batch 18/64 loss: 2.3163962364196777
Batch 19/64 loss: 2.357893943786621
Batch 20/64 loss: 2.2776079177856445
Batch 21/64 loss: 2.4608025550842285
Batch 22/64 loss: 2.090322494506836
Batch 23/64 loss: 2.1115479469299316
Batch 24/64 loss: 2.1154942512512207
Batch 25/64 loss: 2.0098509788513184
Batch 26/64 loss: 2.358170509338379
Batch 27/64 loss: 2.1773428916931152
Batch 28/64 loss: 2.291264057159424
Batch 29/64 loss: 2.242799758911133
Batch 30/64 loss: 2.10862398147583
Batch 31/64 loss: 2.2758712768554688
Batch 32/64 loss: 2.0157203674316406
Batch 33/64 loss: 1.8774542808532715
Batch 34/64 loss: 2.251767635345459
Batch 35/64 loss: 2.122187614440918
Batch 36/64 loss: 1.94810152053833
Batch 37/64 loss: 2.160526752471924
Batch 38/64 loss: 2.315016746520996
Batch 39/64 loss: 2.0313568115234375
Batch 40/64 loss: 2.1510801315307617
Batch 41/64 loss: 2.1247048377990723
Batch 42/64 loss: 2.267089366912842
Batch 43/64 loss: 2.2428741455078125
Batch 44/64 loss: 1.929830551147461
Batch 45/64 loss: 2.204988479614258
Batch 46/64 loss: 2.0686960220336914
Batch 47/64 loss: 2.049004554748535
Batch 48/64 loss: 1.9895758628845215
Batch 49/64 loss: 1.8245587348937988
Batch 50/64 loss: 2.0028090476989746
Batch 51/64 loss: 1.9521403312683105
Batch 52/64 loss: 1.9914546012878418
Batch 53/64 loss: 2.000208854675293
Batch 54/64 loss: 1.7153587341308594
Batch 55/64 loss: 1.942244052886963
Batch 56/64 loss: 2.391730308532715
Batch 57/64 loss: 2.010749340057373
Batch 58/64 loss: 1.8041138648986816
Batch 59/64 loss: 1.8590116500854492
Batch 60/64 loss: 2.0876221656799316
Batch 61/64 loss: 2.148043632507324
Batch 62/64 loss: 2.0244250297546387
Batch 63/64 loss: 2.1146130561828613
Batch 64/64 loss: -0.8571815490722656
Epoch 43  Train loss: 2.1377527872721354  Val loss: 1.9325000461434172
Epoch 44
-------------------------------
Batch 1/64 loss: 1.8864150047302246
Batch 2/64 loss: 2.011227607727051
Batch 3/64 loss: 1.847066879272461
Batch 4/64 loss: 1.8507962226867676
Batch 5/64 loss: 1.7664546966552734
Batch 6/64 loss: 1.8743805885314941
Batch 7/64 loss: 2.152395725250244
Batch 8/64 loss: 1.8265085220336914
Batch 9/64 loss: 1.9884161949157715
Batch 10/64 loss: 1.6884808540344238
Batch 11/64 loss: 1.75114107131958
Batch 12/64 loss: 1.7356057167053223
Batch 13/64 loss: 1.603844165802002
Batch 14/64 loss: 1.9336504936218262
Batch 15/64 loss: 1.8905391693115234
Batch 16/64 loss: 1.7530627250671387
Batch 17/64 loss: 1.6596670150756836
Batch 18/64 loss: 2.202751636505127
Batch 19/64 loss: 2.086880683898926
Batch 20/64 loss: 2.072758674621582
Batch 21/64 loss: 2.39786958694458
Batch 22/64 loss: 2.210094451904297
Batch 23/64 loss: 2.2477831840515137
Batch 24/64 loss: 2.660994052886963
Batch 25/64 loss: 2.1547536849975586
Batch 26/64 loss: 1.6482124328613281
Batch 27/64 loss: 1.8023195266723633
Batch 28/64 loss: 1.936025619506836
Batch 29/64 loss: 1.8858189582824707
Batch 30/64 loss: 1.8256072998046875
Batch 31/64 loss: 2.071328639984131
Batch 32/64 loss: 1.9588289260864258
Batch 33/64 loss: 2.29835844039917
Batch 34/64 loss: 1.7766804695129395
Batch 35/64 loss: 1.9945826530456543
Batch 36/64 loss: 1.9268207550048828
Batch 37/64 loss: 1.834916591644287
Batch 38/64 loss: 1.9561352729797363
Batch 39/64 loss: 2.2489013671875
Batch 40/64 loss: 1.8742432594299316
Batch 41/64 loss: 2.04347562789917
Batch 42/64 loss: 1.8876638412475586
Batch 43/64 loss: 1.7389006614685059
Batch 44/64 loss: 1.9694623947143555
Batch 45/64 loss: 1.778578281402588
Batch 46/64 loss: 2.011265754699707
Batch 47/64 loss: 1.8430285453796387
Batch 48/64 loss: 2.0994749069213867
Batch 49/64 loss: 1.6822795867919922
Batch 50/64 loss: 1.865560531616211
Batch 51/64 loss: 1.8915929794311523
Batch 52/64 loss: 2.0069966316223145
Batch 53/64 loss: 1.8794097900390625
Batch 54/64 loss: 1.888685703277588
Batch 55/64 loss: 1.729712963104248
Batch 56/64 loss: 1.8985114097595215
Batch 57/64 loss: 1.9333868026733398
Batch 58/64 loss: 1.9040513038635254
Batch 59/64 loss: 1.940413475036621
Batch 60/64 loss: 1.879692554473877
Batch 61/64 loss: 1.7403678894042969
Batch 62/64 loss: 1.9166722297668457
Batch 63/64 loss: 2.0281357765197754
Batch 64/64 loss: -0.9155545234680176
Epoch 44  Train loss: 1.9005956481484805  Val loss: 2.0912808028283396
Epoch 45
-------------------------------
Batch 1/64 loss: 1.6743659973144531
Batch 2/64 loss: 1.7586455345153809
Batch 3/64 loss: 2.2629342079162598
Batch 4/64 loss: 2.013577461242676
Batch 5/64 loss: 1.843763828277588
Batch 6/64 loss: 1.9142017364501953
Batch 7/64 loss: 1.660806655883789
Batch 8/64 loss: 1.9511528015136719
Batch 9/64 loss: 1.760706901550293
Batch 10/64 loss: 1.9257569313049316
Batch 11/64 loss: 1.966688632965088
Batch 12/64 loss: 1.934244155883789
Batch 13/64 loss: 1.8062763214111328
Batch 14/64 loss: 2.1631016731262207
Batch 15/64 loss: 1.6008458137512207
Batch 16/64 loss: 1.548551082611084
Batch 17/64 loss: 1.7496252059936523
Batch 18/64 loss: 1.9700927734375
Batch 19/64 loss: 1.7795777320861816
Batch 20/64 loss: 1.8741111755371094
Batch 21/64 loss: 1.9367437362670898
Batch 22/64 loss: 1.6571612358093262
Batch 23/64 loss: 1.7715067863464355
Batch 24/64 loss: 1.8706727027893066
Batch 25/64 loss: 1.7707209587097168
Batch 26/64 loss: 1.7081217765808105
Batch 27/64 loss: 1.9215307235717773
Batch 28/64 loss: 2.0275864601135254
Batch 29/64 loss: 1.847620964050293
Batch 30/64 loss: 1.951523780822754
Batch 31/64 loss: 1.8615198135375977
Batch 32/64 loss: 1.800649642944336
Batch 33/64 loss: 1.9371671676635742
Batch 34/64 loss: 1.7056035995483398
Batch 35/64 loss: 1.7434868812561035
Batch 36/64 loss: 1.8114137649536133
Batch 37/64 loss: 1.6482806205749512
Batch 38/64 loss: 1.8452372550964355
Batch 39/64 loss: 1.6536688804626465
Batch 40/64 loss: 1.881425380706787
Batch 41/64 loss: 1.8326821327209473
Batch 42/64 loss: 2.0548601150512695
Batch 43/64 loss: 1.7112598419189453
Batch 44/64 loss: 2.270803451538086
Batch 45/64 loss: 2.832205295562744
Batch 46/64 loss: 2.4085135459899902
Batch 47/64 loss: 1.8858752250671387
Batch 48/64 loss: 2.103470802307129
Batch 49/64 loss: 2.3607797622680664
Batch 50/64 loss: 2.4051241874694824
Batch 51/64 loss: 2.270874500274658
Batch 52/64 loss: 2.0588855743408203
Batch 53/64 loss: 2.0959620475769043
Batch 54/64 loss: 2.1990909576416016
Batch 55/64 loss: 2.1566901206970215
Batch 56/64 loss: 2.232922077178955
Batch 57/64 loss: 2.2195687294006348
Batch 58/64 loss: 2.223285675048828
Batch 59/64 loss: 1.8595080375671387
Batch 60/64 loss: 2.3356785774230957
Batch 61/64 loss: 1.993034839630127
Batch 62/64 loss: 2.051640510559082
Batch 63/64 loss: 2.338724136352539
Batch 64/64 loss: -0.6398577690124512
Epoch 45  Train loss: 1.9283484795514274  Val loss: 3.22765649225294
Epoch 46
-------------------------------
Batch 1/64 loss: 2.0036730766296387
Batch 2/64 loss: 1.9097929000854492
Batch 3/64 loss: 2.2188291549682617
Batch 4/64 loss: 2.0464372634887695
Batch 5/64 loss: 2.3386178016662598
Batch 6/64 loss: 2.058199882507324
Batch 7/64 loss: 2.079747200012207
Batch 8/64 loss: 2.0042476654052734
Batch 9/64 loss: 2.1764750480651855
Batch 10/64 loss: 1.7645635604858398
Batch 11/64 loss: 2.090872287750244
Batch 12/64 loss: 2.2948660850524902
Batch 13/64 loss: 2.336411476135254
Batch 14/64 loss: 2.007556915283203
Batch 15/64 loss: 1.8116769790649414
Batch 16/64 loss: 2.191728115081787
Batch 17/64 loss: 2.0528736114501953
Batch 18/64 loss: 1.974583625793457
Batch 19/64 loss: 1.9117546081542969
Batch 20/64 loss: 2.288045883178711
Batch 21/64 loss: 2.164581775665283
Batch 22/64 loss: 1.8970608711242676
Batch 23/64 loss: 2.0292811393737793
Batch 24/64 loss: 2.1408534049987793
Batch 25/64 loss: 2.2660422325134277
Batch 26/64 loss: 2.0231313705444336
Batch 27/64 loss: 1.9597969055175781
Batch 28/64 loss: 1.9089274406433105
Batch 29/64 loss: 1.790858268737793
Batch 30/64 loss: 1.9211435317993164
Batch 31/64 loss: 2.3857669830322266
Batch 32/64 loss: 1.6184911727905273
Batch 33/64 loss: 1.7782588005065918
Batch 34/64 loss: 1.7064051628112793
Batch 35/64 loss: 1.8549981117248535
Batch 36/64 loss: 1.7388505935668945
Batch 37/64 loss: 1.6089472770690918
Batch 38/64 loss: 1.9096083641052246
Batch 39/64 loss: 1.9044699668884277
Batch 40/64 loss: 1.7813653945922852
Batch 41/64 loss: 1.8502874374389648
Batch 42/64 loss: 1.7667455673217773
Batch 43/64 loss: 2.037931442260742
Batch 44/64 loss: 1.866417407989502
Batch 45/64 loss: 2.3246583938598633
Batch 46/64 loss: 1.9765300750732422
Batch 47/64 loss: 1.7856764793395996
Batch 48/64 loss: 1.9622507095336914
Batch 49/64 loss: 1.8510046005249023
Batch 50/64 loss: 1.8488397598266602
Batch 51/64 loss: 2.01832914352417
Batch 52/64 loss: 2.6665139198303223
Batch 53/64 loss: 1.668825626373291
Batch 54/64 loss: 1.9148330688476562
Batch 55/64 loss: 1.6310324668884277
Batch 56/64 loss: 1.802915096282959
Batch 57/64 loss: 2.1570005416870117
Batch 58/64 loss: 1.7905559539794922
Batch 59/64 loss: 1.6483192443847656
Batch 60/64 loss: 2.0138602256774902
Batch 61/64 loss: 2.161011219024658
Batch 62/64 loss: 1.5369620323181152
Batch 63/64 loss: 2.26792049407959
Batch 64/64 loss: -0.9738035202026367
Epoch 46  Train loss: 1.9414566301832012  Val loss: 1.8253673671447124
Saving best model, epoch: 46
Epoch 47
-------------------------------
Batch 1/64 loss: 1.6300334930419922
Batch 2/64 loss: 1.7289834022521973
Batch 3/64 loss: 2.0798778533935547
Batch 4/64 loss: 1.546597957611084
Batch 5/64 loss: 1.7821545600891113
Batch 6/64 loss: 1.817037582397461
Batch 7/64 loss: 1.8999371528625488
Batch 8/64 loss: 1.998866081237793
Batch 9/64 loss: 1.7954316139221191
Batch 10/64 loss: 1.9445524215698242
Batch 11/64 loss: 1.8462624549865723
Batch 12/64 loss: 1.7011184692382812
Batch 13/64 loss: 1.5907368659973145
Batch 14/64 loss: 2.2622437477111816
Batch 15/64 loss: 1.756605625152588
Batch 16/64 loss: 2.192049026489258
Batch 17/64 loss: 1.922996997833252
Batch 18/64 loss: 1.8188714981079102
Batch 19/64 loss: 2.010622978210449
Batch 20/64 loss: 2.432476043701172
Batch 21/64 loss: 1.8773012161254883
Batch 22/64 loss: 1.7008600234985352
Batch 23/64 loss: 1.802441120147705
Batch 24/64 loss: 1.7768840789794922
Batch 25/64 loss: 1.8358983993530273
Batch 26/64 loss: 1.6932497024536133
Batch 27/64 loss: 1.8783988952636719
Batch 28/64 loss: 1.5478219985961914
Batch 29/64 loss: 1.9737358093261719
Batch 30/64 loss: 1.489212989807129
Batch 31/64 loss: 2.2070846557617188
Batch 32/64 loss: 1.9389586448669434
Batch 33/64 loss: 1.6490273475646973
Batch 34/64 loss: 1.8335089683532715
Batch 35/64 loss: 1.731025218963623
Batch 36/64 loss: 1.5625553131103516
Batch 37/64 loss: 1.6109366416931152
Batch 38/64 loss: 1.8704495429992676
Batch 39/64 loss: 1.862884521484375
Batch 40/64 loss: 1.7564120292663574
Batch 41/64 loss: 1.9303340911865234
Batch 42/64 loss: 1.9701905250549316
Batch 43/64 loss: 1.6998190879821777
Batch 44/64 loss: 1.7610015869140625
Batch 45/64 loss: 1.6039657592773438
Batch 46/64 loss: 1.8695950508117676
Batch 47/64 loss: 1.7565245628356934
Batch 48/64 loss: 1.6452865600585938
Batch 49/64 loss: 1.6943893432617188
Batch 50/64 loss: 1.7761964797973633
Batch 51/64 loss: 2.063345432281494
Batch 52/64 loss: 1.9403672218322754
Batch 53/64 loss: 1.8909573554992676
Batch 54/64 loss: 1.710583209991455
Batch 55/64 loss: 1.712446689605713
Batch 56/64 loss: 1.8088555335998535
Batch 57/64 loss: 1.776259422302246
Batch 58/64 loss: 2.034763813018799
Batch 59/64 loss: 1.9899711608886719
Batch 60/64 loss: 1.837972640991211
Batch 61/64 loss: 2.174924850463867
Batch 62/64 loss: 2.02970027923584
Batch 63/64 loss: 1.847792625427246
Batch 64/64 loss: -0.5315122604370117
Epoch 47  Train loss: 1.8114935220456592  Val loss: 1.7744129679047365
Saving best model, epoch: 47
Epoch 48
-------------------------------
Batch 1/64 loss: 1.568427562713623
Batch 2/64 loss: 1.8488125801086426
Batch 3/64 loss: 1.8223505020141602
Batch 4/64 loss: 1.937291145324707
Batch 5/64 loss: 1.5907344818115234
Batch 6/64 loss: 1.6373023986816406
Batch 7/64 loss: 1.724769115447998
Batch 8/64 loss: 1.9040946960449219
Batch 9/64 loss: 1.7912983894348145
Batch 10/64 loss: 1.623551845550537
Batch 11/64 loss: 1.7730193138122559
Batch 12/64 loss: 1.7414216995239258
Batch 13/64 loss: 2.172987937927246
Batch 14/64 loss: 1.853269100189209
Batch 15/64 loss: 2.151878833770752
Batch 16/64 loss: 1.8502721786499023
Batch 17/64 loss: 1.970724105834961
Batch 18/64 loss: 1.6995797157287598
Batch 19/64 loss: 1.734034538269043
Batch 20/64 loss: 1.9353647232055664
Batch 21/64 loss: 1.4776301383972168
Batch 22/64 loss: 2.0541725158691406
Batch 23/64 loss: 1.995285987854004
Batch 24/64 loss: 1.792691707611084
Batch 25/64 loss: 1.684338092803955
Batch 26/64 loss: 1.6754388809204102
Batch 27/64 loss: 2.00191068649292
Batch 28/64 loss: 1.6086235046386719
Batch 29/64 loss: 1.5757617950439453
Batch 30/64 loss: 1.263230800628662
Batch 31/64 loss: 2.1661176681518555
Batch 32/64 loss: 1.6750264167785645
Batch 33/64 loss: 1.80584716796875
Batch 34/64 loss: 1.9178681373596191
Batch 35/64 loss: 1.90034818649292
Batch 36/64 loss: 1.7865018844604492
Batch 37/64 loss: 1.7149953842163086
Batch 38/64 loss: 1.7338619232177734
Batch 39/64 loss: 2.327970027923584
Batch 40/64 loss: 1.954451084136963
Batch 41/64 loss: 1.7959704399108887
Batch 42/64 loss: 1.8926749229431152
Batch 43/64 loss: 2.1280722618103027
Batch 44/64 loss: 1.7184185981750488
Batch 45/64 loss: 1.640920639038086
Batch 46/64 loss: 1.6846261024475098
Batch 47/64 loss: 1.8609590530395508
Batch 48/64 loss: 1.5155954360961914
Batch 49/64 loss: 1.9428825378417969
Batch 50/64 loss: 1.8238096237182617
Batch 51/64 loss: 1.954087257385254
Batch 52/64 loss: 1.6611919403076172
Batch 53/64 loss: 2.0037569999694824
Batch 54/64 loss: 2.1900391578674316
Batch 55/64 loss: 1.7889776229858398
Batch 56/64 loss: 1.8540692329406738
Batch 57/64 loss: 2.2609610557556152
Batch 58/64 loss: 1.9203529357910156
Batch 59/64 loss: 1.6074552536010742
Batch 60/64 loss: 1.665670394897461
Batch 61/64 loss: 1.965907096862793
Batch 62/64 loss: 1.5962276458740234
Batch 63/64 loss: 1.8663902282714844
Batch 64/64 loss: -1.0148305892944336
Epoch 48  Train loss: 1.7885670643226772  Val loss: 1.9276582776885671
Epoch 49
-------------------------------
Batch 1/64 loss: 1.753072738647461
Batch 2/64 loss: 1.7309064865112305
Batch 3/64 loss: 1.8036017417907715
Batch 4/64 loss: 1.8861737251281738
Batch 5/64 loss: 1.7903013229370117
Batch 6/64 loss: 1.9098963737487793
Batch 7/64 loss: 1.82318115234375
Batch 8/64 loss: 1.7924904823303223
Batch 9/64 loss: 1.9854106903076172
Batch 10/64 loss: 1.8585281372070312
Batch 11/64 loss: 1.7857732772827148
Batch 12/64 loss: 1.4184236526489258
Batch 13/64 loss: 1.7166972160339355
Batch 14/64 loss: 1.516474723815918
Batch 15/64 loss: 1.8559932708740234
Batch 16/64 loss: 1.8574891090393066
Batch 17/64 loss: 1.791964054107666
Batch 18/64 loss: 1.8591394424438477
Batch 19/64 loss: 1.9104628562927246
Batch 20/64 loss: 1.739326000213623
Batch 21/64 loss: 2.004446506500244
Batch 22/64 loss: 1.7220377922058105
Batch 23/64 loss: 1.724045753479004
Batch 24/64 loss: 1.903646469116211
Batch 25/64 loss: 1.6352596282958984
Batch 26/64 loss: 1.8123183250427246
Batch 27/64 loss: 2.014585018157959
Batch 28/64 loss: 1.9660124778747559
Batch 29/64 loss: 1.6275954246520996
Batch 30/64 loss: 1.5824909210205078
Batch 31/64 loss: 1.8225808143615723
Batch 32/64 loss: 1.5830106735229492
Batch 33/64 loss: 1.9038410186767578
Batch 34/64 loss: 1.8069086074829102
Batch 35/64 loss: 1.427943229675293
Batch 36/64 loss: 1.5578207969665527
Batch 37/64 loss: 1.8922333717346191
Batch 38/64 loss: 1.9874267578125
Batch 39/64 loss: 1.6442232131958008
Batch 40/64 loss: 1.7895903587341309
Batch 41/64 loss: 1.915583610534668
Batch 42/64 loss: 1.4932656288146973
Batch 43/64 loss: 1.4820008277893066
Batch 44/64 loss: 1.68821382522583
Batch 45/64 loss: 1.6140975952148438
Batch 46/64 loss: 2.1213722229003906
Batch 47/64 loss: 2.1438546180725098
Batch 48/64 loss: 1.5933942794799805
Batch 49/64 loss: 1.764732837677002
Batch 50/64 loss: 1.4522829055786133
Batch 51/64 loss: 1.9945111274719238
Batch 52/64 loss: 1.70048189163208
Batch 53/64 loss: 1.9398932456970215
Batch 54/64 loss: 1.596461296081543
Batch 55/64 loss: 1.8915152549743652
Batch 56/64 loss: 1.7230987548828125
Batch 57/64 loss: 1.781552791595459
Batch 58/64 loss: 1.8101253509521484
Batch 59/64 loss: 1.6312274932861328
Batch 60/64 loss: 1.587388515472412
Batch 61/64 loss: 1.7822742462158203
Batch 62/64 loss: 1.9768900871276855
Batch 63/64 loss: 1.7603754997253418
Batch 64/64 loss: -0.7490425109863281
Epoch 49  Train loss: 1.7423707550647212  Val loss: 1.735530040518115
Saving best model, epoch: 49
Epoch 50
-------------------------------
Batch 1/64 loss: 1.892533302307129
Batch 2/64 loss: 1.4692516326904297
Batch 3/64 loss: 2.042442798614502
Batch 4/64 loss: 2.1474719047546387
Batch 5/64 loss: 2.031497001647949
Batch 6/64 loss: 1.6751108169555664
Batch 7/64 loss: 1.8035855293273926
Batch 8/64 loss: 1.89176607131958
Batch 9/64 loss: 1.8339381217956543
Batch 10/64 loss: 1.6482830047607422
Batch 11/64 loss: 1.5711731910705566
Batch 12/64 loss: 1.875382900238037
Batch 13/64 loss: 1.5013561248779297
Batch 14/64 loss: 1.802055835723877
Batch 15/64 loss: 1.7714653015136719
Batch 16/64 loss: 1.725855827331543
Batch 17/64 loss: 1.6329827308654785
Batch 18/64 loss: 1.6165432929992676
Batch 19/64 loss: 1.8454017639160156
Batch 20/64 loss: 1.9902515411376953
Batch 21/64 loss: 1.8104429244995117
Batch 22/64 loss: 2.021519184112549
Batch 23/64 loss: 1.5847196578979492
Batch 24/64 loss: 1.7535362243652344
Batch 25/64 loss: 1.4985690116882324
Batch 26/64 loss: 1.8241004943847656
Batch 27/64 loss: 1.6922554969787598
Batch 28/64 loss: 1.479701042175293
Batch 29/64 loss: 1.8506040573120117
Batch 30/64 loss: 1.5610113143920898
Batch 31/64 loss: 1.687577724456787
Batch 32/64 loss: 1.5445475578308105
Batch 33/64 loss: 1.760514259338379
Batch 34/64 loss: 1.6831579208374023
Batch 35/64 loss: 2.00991153717041
Batch 36/64 loss: 1.5690479278564453
Batch 37/64 loss: 1.3790011405944824
Batch 38/64 loss: 1.4871187210083008
Batch 39/64 loss: 2.00998592376709
Batch 40/64 loss: 1.7605862617492676
Batch 41/64 loss: 1.7749419212341309
Batch 42/64 loss: 1.5453505516052246
Batch 43/64 loss: 2.219191551208496
Batch 44/64 loss: 1.8925786018371582
Batch 45/64 loss: 1.7309517860412598
Batch 46/64 loss: 1.887202262878418
Batch 47/64 loss: 1.823300838470459
Batch 48/64 loss: 1.4845309257507324
Batch 49/64 loss: 1.742931842803955
Batch 50/64 loss: 1.495020866394043
Batch 51/64 loss: 1.7002263069152832
Batch 52/64 loss: 1.6125411987304688
Batch 53/64 loss: 1.671412467956543
Batch 54/64 loss: 1.8579049110412598
Batch 55/64 loss: 1.6627283096313477
Batch 56/64 loss: 2.025144577026367
Batch 57/64 loss: 1.8152384757995605
Batch 58/64 loss: 1.7391867637634277
Batch 59/64 loss: 1.711799144744873
Batch 60/64 loss: 1.754532814025879
Batch 61/64 loss: 1.693669319152832
Batch 62/64 loss: 1.7334399223327637
Batch 63/64 loss: 1.5322108268737793
Batch 64/64 loss: -1.4051055908203125
Epoch 50  Train loss: 1.7064857108920228  Val loss: 1.783214070952635
Epoch 51
-------------------------------
Batch 1/64 loss: 1.510958194732666
Batch 2/64 loss: 1.7915072441101074
Batch 3/64 loss: 1.366285800933838
Batch 4/64 loss: 1.7988429069519043
Batch 5/64 loss: 1.703075885772705
Batch 6/64 loss: 1.521242618560791
Batch 7/64 loss: 1.881676197052002
Batch 8/64 loss: 2.259889602661133
Batch 9/64 loss: 1.7457313537597656
Batch 10/64 loss: 1.9292502403259277
Batch 11/64 loss: 1.4279112815856934
Batch 12/64 loss: 1.9940948486328125
Batch 13/64 loss: 1.9519314765930176
Batch 14/64 loss: 1.7908291816711426
Batch 15/64 loss: 1.6814398765563965
Batch 16/64 loss: 1.5918822288513184
Batch 17/64 loss: 1.4751930236816406
Batch 18/64 loss: 1.434525966644287
Batch 19/64 loss: 1.6722431182861328
Batch 20/64 loss: 2.0061593055725098
Batch 21/64 loss: 1.6592459678649902
Batch 22/64 loss: 1.846470832824707
Batch 23/64 loss: 1.5842485427856445
Batch 24/64 loss: 1.8865280151367188
Batch 25/64 loss: 2.0600342750549316
Batch 26/64 loss: 1.5933823585510254
Batch 27/64 loss: 1.638315200805664
Batch 28/64 loss: 1.9267868995666504
Batch 29/64 loss: 1.8550543785095215
Batch 30/64 loss: 1.5966620445251465
Batch 31/64 loss: 1.9371562004089355
Batch 32/64 loss: 1.8247895240783691
Batch 33/64 loss: 1.4021568298339844
Batch 34/64 loss: 1.9748988151550293
Batch 35/64 loss: 1.7048592567443848
Batch 36/64 loss: 1.4463410377502441
Batch 37/64 loss: 1.7056336402893066
Batch 38/64 loss: 1.7523512840270996
Batch 39/64 loss: 1.4769768714904785
Batch 40/64 loss: 1.6145415306091309
Batch 41/64 loss: 1.9365439414978027
Batch 42/64 loss: 1.847334384918213
Batch 43/64 loss: 1.8770160675048828
Batch 44/64 loss: 1.6488351821899414
Batch 45/64 loss: 1.7398991584777832
Batch 46/64 loss: 1.8098368644714355
Batch 47/64 loss: 1.6109542846679688
Batch 48/64 loss: 1.7064590454101562
Batch 49/64 loss: 1.6900525093078613
Batch 50/64 loss: 1.6279296875
Batch 51/64 loss: 1.5436043739318848
Batch 52/64 loss: 1.6998271942138672
Batch 53/64 loss: 1.89003324508667
Batch 54/64 loss: 1.8158106803894043
Batch 55/64 loss: 1.6230721473693848
Batch 56/64 loss: 1.4583783149719238
Batch 57/64 loss: 1.5322771072387695
Batch 58/64 loss: 1.5531058311462402
Batch 59/64 loss: 2.215193271636963
Batch 60/64 loss: 2.176560401916504
Batch 61/64 loss: 1.6305327415466309
Batch 62/64 loss: 1.568453311920166
Batch 63/64 loss: 1.6975669860839844
Batch 64/64 loss: -1.352369785308838
Epoch 51  Train loss: 1.6926447569155225  Val loss: 1.689004806308812
Saving best model, epoch: 51
Epoch 52
-------------------------------
Batch 1/64 loss: 1.5517053604125977
Batch 2/64 loss: 2.155078887939453
Batch 3/64 loss: 1.710836410522461
Batch 4/64 loss: 1.9402508735656738
Batch 5/64 loss: 1.6908149719238281
Batch 6/64 loss: 2.0980429649353027
Batch 7/64 loss: 1.8428726196289062
Batch 8/64 loss: 2.2268123626708984
Batch 9/64 loss: 1.6242432594299316
Batch 10/64 loss: 1.6747560501098633
Batch 11/64 loss: 1.790410041809082
Batch 12/64 loss: 1.4084038734436035
Batch 13/64 loss: 1.9928021430969238
Batch 14/64 loss: 1.7935681343078613
Batch 15/64 loss: 1.7854104042053223
Batch 16/64 loss: 1.9710869789123535
Batch 17/64 loss: 1.5017600059509277
Batch 18/64 loss: 1.7339868545532227
Batch 19/64 loss: 1.9275860786437988
Batch 20/64 loss: 1.7609148025512695
Batch 21/64 loss: 2.053016185760498
Batch 22/64 loss: 1.591372013092041
Batch 23/64 loss: 1.717278003692627
Batch 24/64 loss: 1.7216277122497559
Batch 25/64 loss: 1.686244010925293
Batch 26/64 loss: 1.6047859191894531
Batch 27/64 loss: 1.3233628273010254
Batch 28/64 loss: 2.0420708656311035
Batch 29/64 loss: 1.5388121604919434
Batch 30/64 loss: 1.6557035446166992
Batch 31/64 loss: 1.4542436599731445
Batch 32/64 loss: 1.4962177276611328
Batch 33/64 loss: 1.633882999420166
Batch 34/64 loss: 1.8448758125305176
Batch 35/64 loss: 1.6698174476623535
Batch 36/64 loss: 1.80375337600708
Batch 37/64 loss: 1.4572672843933105
Batch 38/64 loss: 1.6184463500976562
Batch 39/64 loss: 1.8381762504577637
Batch 40/64 loss: 1.7153997421264648
Batch 41/64 loss: 1.8355712890625
Batch 42/64 loss: 1.8989129066467285
Batch 43/64 loss: 1.7274422645568848
Batch 44/64 loss: 1.6518750190734863
Batch 45/64 loss: 1.8520121574401855
Batch 46/64 loss: 1.661165714263916
Batch 47/64 loss: 1.520871639251709
Batch 48/64 loss: 1.7737970352172852
Batch 49/64 loss: 1.851491928100586
Batch 50/64 loss: 1.5120387077331543
Batch 51/64 loss: 1.565889835357666
Batch 52/64 loss: 1.5237436294555664
Batch 53/64 loss: 1.725179672241211
Batch 54/64 loss: 1.4454808235168457
Batch 55/64 loss: 1.699084758758545
Batch 56/64 loss: 1.792241096496582
Batch 57/64 loss: 1.831540584564209
Batch 58/64 loss: 1.4572763442993164
Batch 59/64 loss: 2.5381526947021484
Batch 60/64 loss: 1.6290359497070312
Batch 61/64 loss: 1.7499713897705078
Batch 62/64 loss: 1.5723614692687988
Batch 63/64 loss: 2.0665764808654785
Batch 64/64 loss: -0.9611988067626953
Epoch 52  Train loss: 1.7068001765830845  Val loss: 2.5148201513126542
Epoch 53
-------------------------------
Batch 1/64 loss: 1.9918689727783203
Batch 2/64 loss: 2.356415271759033
Batch 3/64 loss: 1.6941328048706055
Batch 4/64 loss: 1.6587328910827637
Batch 5/64 loss: 2.386765956878662
Batch 6/64 loss: 1.5562381744384766
Batch 7/64 loss: 2.4608993530273438
Batch 8/64 loss: 1.6684966087341309
Batch 9/64 loss: 2.301568031311035
Batch 10/64 loss: 1.956047534942627
Batch 11/64 loss: 1.792006015777588
Batch 12/64 loss: 1.9686970710754395
Batch 13/64 loss: 2.0018744468688965
Batch 14/64 loss: 1.9251365661621094
Batch 15/64 loss: 2.05930233001709
Batch 16/64 loss: 1.715825080871582
Batch 17/64 loss: 1.749082088470459
Batch 18/64 loss: 1.900224208831787
Batch 19/64 loss: 1.778623104095459
Batch 20/64 loss: 1.8545427322387695
Batch 21/64 loss: 2.074032783508301
Batch 22/64 loss: 1.8797616958618164
Batch 23/64 loss: 1.9495229721069336
Batch 24/64 loss: 2.753908157348633
Batch 25/64 loss: 2.2673587799072266
Batch 26/64 loss: 2.017268657684326
Batch 27/64 loss: 1.8595333099365234
Batch 28/64 loss: 2.242858409881592
Batch 29/64 loss: 2.2033462524414062
Batch 30/64 loss: 1.7013297080993652
Batch 31/64 loss: 1.752063274383545
Batch 32/64 loss: 1.6304636001586914
Batch 33/64 loss: 2.014820098876953
Batch 34/64 loss: 2.0093488693237305
Batch 35/64 loss: 1.808103084564209
Batch 36/64 loss: 2.133592128753662
Batch 37/64 loss: 2.473421096801758
Batch 38/64 loss: 1.7174887657165527
Batch 39/64 loss: 1.8632330894470215
Batch 40/64 loss: 2.0175042152404785
Batch 41/64 loss: 1.6201510429382324
Batch 42/64 loss: 1.8454322814941406
Batch 43/64 loss: 1.7802472114562988
Batch 44/64 loss: 1.6956710815429688
Batch 45/64 loss: 1.6236205101013184
Batch 46/64 loss: 2.0541296005249023
Batch 47/64 loss: 1.6109328269958496
Batch 48/64 loss: 2.2772507667541504
Batch 49/64 loss: 1.6402273178100586
Batch 50/64 loss: 1.9036293029785156
Batch 51/64 loss: 1.9485583305358887
Batch 52/64 loss: 1.4996719360351562
Batch 53/64 loss: 1.6832795143127441
Batch 54/64 loss: 2.04252290725708
Batch 55/64 loss: 2.2351837158203125
Batch 56/64 loss: 1.9760303497314453
Batch 57/64 loss: 1.7720274925231934
Batch 58/64 loss: 2.026200771331787
Batch 59/64 loss: 1.8070940971374512
Batch 60/64 loss: 1.6999750137329102
Batch 61/64 loss: 1.8049373626708984
Batch 62/64 loss: 1.7050399780273438
Batch 63/64 loss: 1.8443818092346191
Batch 64/64 loss: -1.1533088684082031
Epoch 53  Train loss: 1.888261204139859  Val loss: 1.7160229961487026
Epoch 54
-------------------------------
Batch 1/64 loss: 1.5368876457214355
Batch 2/64 loss: 1.8153505325317383
Batch 3/64 loss: 1.7900123596191406
Batch 4/64 loss: 1.7928037643432617
Batch 5/64 loss: 2.1464743614196777
Batch 6/64 loss: 1.484591007232666
Batch 7/64 loss: 1.6747846603393555
Batch 8/64 loss: 1.5307974815368652
Batch 9/64 loss: 2.2520623207092285
Batch 10/64 loss: 1.8904566764831543
Batch 11/64 loss: 2.102323055267334
Batch 12/64 loss: 1.6414055824279785
Batch 13/64 loss: 1.8634295463562012
Batch 14/64 loss: 1.4746570587158203
Batch 15/64 loss: 1.6110434532165527
Batch 16/64 loss: 1.913973331451416
Batch 17/64 loss: 1.9974489212036133
Batch 18/64 loss: 1.6292304992675781
Batch 19/64 loss: 1.5496606826782227
Batch 20/64 loss: 1.8804211616516113
Batch 21/64 loss: 1.695547103881836
Batch 22/64 loss: 1.769360065460205
Batch 23/64 loss: 1.9338560104370117
Batch 24/64 loss: 1.6273183822631836
Batch 25/64 loss: 1.7802114486694336
Batch 26/64 loss: 1.6987414360046387
Batch 27/64 loss: 1.881199836730957
Batch 28/64 loss: 1.898625373840332
Batch 29/64 loss: 1.7796010971069336
Batch 30/64 loss: 1.828150749206543
Batch 31/64 loss: 1.6492137908935547
Batch 32/64 loss: 1.6653962135314941
Batch 33/64 loss: 1.5989437103271484
Batch 34/64 loss: 1.5714826583862305
Batch 35/64 loss: 1.553349494934082
Batch 36/64 loss: 2.032625198364258
Batch 37/64 loss: 1.9222092628479004
Batch 38/64 loss: 1.7521581649780273
Batch 39/64 loss: 1.4596118927001953
Batch 40/64 loss: 1.661025047302246
Batch 41/64 loss: 1.9714221954345703
Batch 42/64 loss: 1.5271577835083008
Batch 43/64 loss: 1.7493762969970703
Batch 44/64 loss: 1.9531784057617188
Batch 45/64 loss: 1.635159969329834
Batch 46/64 loss: 1.7391762733459473
Batch 47/64 loss: 1.7687053680419922
Batch 48/64 loss: 1.6703248023986816
Batch 49/64 loss: 1.5024189949035645
Batch 50/64 loss: 1.8807930946350098
Batch 51/64 loss: 1.5015430450439453
Batch 52/64 loss: 1.9348745346069336
Batch 53/64 loss: 1.872215747833252
Batch 54/64 loss: 1.5490598678588867
Batch 55/64 loss: 1.7268352508544922
Batch 56/64 loss: 1.5569839477539062
Batch 57/64 loss: 1.6803312301635742
Batch 58/64 loss: 1.8383231163024902
Batch 59/64 loss: 1.605273723602295
Batch 60/64 loss: 1.6124210357666016
Batch 61/64 loss: 1.8379039764404297
Batch 62/64 loss: 1.6781120300292969
Batch 63/64 loss: 1.7878785133361816
Batch 64/64 loss: -1.2883086204528809
Epoch 54  Train loss: 1.7090145690768372  Val loss: 1.7221779315332366
Epoch 55
-------------------------------
Batch 1/64 loss: 1.8519434928894043
Batch 2/64 loss: 1.6797523498535156
Batch 3/64 loss: 2.524674892425537
Batch 4/64 loss: 1.7445626258850098
Batch 5/64 loss: 2.136932849884033
Batch 6/64 loss: 1.7975735664367676
Batch 7/64 loss: 2.000314712524414
Batch 8/64 loss: 1.9086289405822754
Batch 9/64 loss: 2.21835994720459
Batch 10/64 loss: 1.7005300521850586
Batch 11/64 loss: 1.7673263549804688
Batch 12/64 loss: 1.6117868423461914
Batch 13/64 loss: 1.9967865943908691
Batch 14/64 loss: 1.6382603645324707
Batch 15/64 loss: 1.8651785850524902
Batch 16/64 loss: 1.9983186721801758
Batch 17/64 loss: 1.7523598670959473
Batch 18/64 loss: 2.027942180633545
Batch 19/64 loss: 1.8169279098510742
Batch 20/64 loss: 1.7914977073669434
Batch 21/64 loss: 1.426468849182129
Batch 22/64 loss: 1.6320619583129883
Batch 23/64 loss: 1.9309806823730469
Batch 24/64 loss: 1.6795144081115723
Batch 25/64 loss: 1.8176522254943848
Batch 26/64 loss: 2.04024076461792
Batch 27/64 loss: 1.7721953392028809
Batch 28/64 loss: 1.6634025573730469
Batch 29/64 loss: 1.6746912002563477
Batch 30/64 loss: 1.8766498565673828
Batch 31/64 loss: 2.063835620880127
Batch 32/64 loss: 1.8173737525939941
Batch 33/64 loss: 2.0955147743225098
Batch 34/64 loss: 1.6636228561401367
Batch 35/64 loss: 1.639601707458496
Batch 36/64 loss: 1.555945873260498
Batch 37/64 loss: 1.9541239738464355
Batch 38/64 loss: 1.841578483581543
Batch 39/64 loss: 1.6786761283874512
Batch 40/64 loss: 1.703880786895752
Batch 41/64 loss: 1.6398744583129883
Batch 42/64 loss: 1.7633318901062012
Batch 43/64 loss: 1.4699759483337402
Batch 44/64 loss: 1.6935534477233887
Batch 45/64 loss: 1.5309514999389648
Batch 46/64 loss: 1.7133960723876953
Batch 47/64 loss: 1.7746214866638184
Batch 48/64 loss: 1.422563076019287
Batch 49/64 loss: 1.8235440254211426
Batch 50/64 loss: 1.6143169403076172
Batch 51/64 loss: 1.5877900123596191
Batch 52/64 loss: 1.813066005706787
Batch 53/64 loss: 1.862022876739502
Batch 54/64 loss: 1.947411060333252
Batch 55/64 loss: 1.3601365089416504
Batch 56/64 loss: 1.6115951538085938
Batch 57/64 loss: 1.6595096588134766
Batch 58/64 loss: 1.4872736930847168
Batch 59/64 loss: 1.4678001403808594
Batch 60/64 loss: 1.7132868766784668
Batch 61/64 loss: 2.263092041015625
Batch 62/64 loss: 1.8489446640014648
Batch 63/64 loss: 1.8925132751464844
Batch 64/64 loss: -1.6446185111999512
Epoch 55  Train loss: 1.7425063095840754  Val loss: 1.6750511811770934
Saving best model, epoch: 55
Epoch 56
-------------------------------
Batch 1/64 loss: 1.3513298034667969
Batch 2/64 loss: 1.660395622253418
Batch 3/64 loss: 1.4670968055725098
Batch 4/64 loss: 1.488924503326416
Batch 5/64 loss: 1.7406001091003418
Batch 6/64 loss: 1.6661896705627441
Batch 7/64 loss: 1.8612537384033203
Batch 8/64 loss: 1.972275733947754
Batch 9/64 loss: 1.626760482788086
Batch 10/64 loss: 1.625093936920166
Batch 11/64 loss: 1.609090805053711
Batch 12/64 loss: 1.5358386039733887
Batch 13/64 loss: 1.693389892578125
Batch 14/64 loss: 1.5685324668884277
Batch 15/64 loss: 1.6903724670410156
Batch 16/64 loss: 1.5171637535095215
Batch 17/64 loss: 1.5887489318847656
Batch 18/64 loss: 1.7302002906799316
Batch 19/64 loss: 1.7090001106262207
Batch 20/64 loss: 1.757094383239746
Batch 21/64 loss: 1.492764949798584
Batch 22/64 loss: 1.5110936164855957
Batch 23/64 loss: 1.487339973449707
Batch 24/64 loss: 2.3842215538024902
Batch 25/64 loss: 1.7520246505737305
Batch 26/64 loss: 1.4343647956848145
Batch 27/64 loss: 1.385127067565918
Batch 28/64 loss: 1.8463091850280762
Batch 29/64 loss: 1.8904757499694824
Batch 30/64 loss: 2.061633586883545
Batch 31/64 loss: 1.5935845375061035
Batch 32/64 loss: 1.8519525527954102
Batch 33/64 loss: 1.5413365364074707
Batch 34/64 loss: 1.6138310432434082
Batch 35/64 loss: 1.4217286109924316
Batch 36/64 loss: 1.5787229537963867
Batch 37/64 loss: 1.7311525344848633
Batch 38/64 loss: 1.4907035827636719
Batch 39/64 loss: 1.458639144897461
Batch 40/64 loss: 1.6375398635864258
Batch 41/64 loss: 1.5350141525268555
Batch 42/64 loss: 1.9861207008361816
Batch 43/64 loss: 2.0531468391418457
Batch 44/64 loss: 1.6827688217163086
Batch 45/64 loss: 1.7276382446289062
Batch 46/64 loss: 1.7381439208984375
Batch 47/64 loss: 1.6572599411010742
Batch 48/64 loss: 1.7948508262634277
Batch 49/64 loss: 2.201310157775879
Batch 50/64 loss: 1.4805083274841309
Batch 51/64 loss: 1.686821460723877
Batch 52/64 loss: 1.6648902893066406
Batch 53/64 loss: 2.0204458236694336
Batch 54/64 loss: 1.473245620727539
Batch 55/64 loss: 1.6949787139892578
Batch 56/64 loss: 1.567399024963379
Batch 57/64 loss: 1.7613439559936523
Batch 58/64 loss: 1.79579496383667
Batch 59/64 loss: 1.6189260482788086
Batch 60/64 loss: 1.4391722679138184
Batch 61/64 loss: 1.6533808708190918
Batch 62/64 loss: 1.7270722389221191
Batch 63/64 loss: 1.325854778289795
Batch 64/64 loss: -1.0490355491638184
Epoch 56  Train loss: 1.6395797635994704  Val loss: 1.6178550851304097
Saving best model, epoch: 56
Epoch 57
-------------------------------
Batch 1/64 loss: 1.5941390991210938
Batch 2/64 loss: 1.545182228088379
Batch 3/64 loss: 1.3980112075805664
Batch 4/64 loss: 1.5654077529907227
Batch 5/64 loss: 1.995680332183838
Batch 6/64 loss: 1.4880051612854004
Batch 7/64 loss: 1.6064815521240234
Batch 8/64 loss: 1.6323952674865723
Batch 9/64 loss: 1.6221280097961426
Batch 10/64 loss: 1.4402389526367188
Batch 11/64 loss: 1.6498417854309082
Batch 12/64 loss: 1.6647491455078125
Batch 13/64 loss: 1.6660809516906738
Batch 14/64 loss: 1.5416898727416992
Batch 15/64 loss: 1.8823418617248535
Batch 16/64 loss: 1.5896062850952148
Batch 17/64 loss: 1.5799951553344727
Batch 18/64 loss: 1.6922664642333984
Batch 19/64 loss: 1.5927772521972656
Batch 20/64 loss: 1.633284568786621
Batch 21/64 loss: 1.7187657356262207
Batch 22/64 loss: 1.6161675453186035
Batch 23/64 loss: 1.6491189002990723
Batch 24/64 loss: 1.716796875
Batch 25/64 loss: 1.6376433372497559
Batch 26/64 loss: 1.8126301765441895
Batch 27/64 loss: 1.9666519165039062
Batch 28/64 loss: 1.5008273124694824
Batch 29/64 loss: 1.8424553871154785
Batch 30/64 loss: 1.6382055282592773
Batch 31/64 loss: 2.0608158111572266
Batch 32/64 loss: 1.6221885681152344
Batch 33/64 loss: 1.8125019073486328
Batch 34/64 loss: 1.317378044128418
Batch 35/64 loss: 1.619119644165039
Batch 36/64 loss: 1.708385944366455
Batch 37/64 loss: 1.932948112487793
Batch 38/64 loss: 1.5158038139343262
Batch 39/64 loss: 1.8436532020568848
Batch 40/64 loss: 1.560765266418457
Batch 41/64 loss: 1.623220443725586
Batch 42/64 loss: 1.7918977737426758
Batch 43/64 loss: 1.506699562072754
Batch 44/64 loss: 1.7034363746643066
Batch 45/64 loss: 1.2843737602233887
Batch 46/64 loss: 1.47674560546875
Batch 47/64 loss: 1.6995372772216797
Batch 48/64 loss: 1.7636585235595703
Batch 49/64 loss: 1.5226821899414062
Batch 50/64 loss: 1.559351921081543
Batch 51/64 loss: 1.547743320465088
Batch 52/64 loss: 1.558669090270996
Batch 53/64 loss: 1.5875029563903809
Batch 54/64 loss: 1.4620485305786133
Batch 55/64 loss: 1.8016667366027832
Batch 56/64 loss: 1.33665132522583
Batch 57/64 loss: 1.83699369430542
Batch 58/64 loss: 1.7376747131347656
Batch 59/64 loss: 1.5220460891723633
Batch 60/64 loss: 1.6928648948669434
Batch 61/64 loss: 1.5784974098205566
Batch 62/64 loss: 1.6659302711486816
Batch 63/64 loss: 1.4521899223327637
Batch 64/64 loss: -1.5105233192443848
Epoch 57  Train loss: 1.600820640489167  Val loss: 1.5187956950918506
Saving best model, epoch: 57
Epoch 58
-------------------------------
Batch 1/64 loss: 1.611182689666748
Batch 2/64 loss: 2.214412212371826
Batch 3/64 loss: 1.3432364463806152
Batch 4/64 loss: 1.601630687713623
Batch 5/64 loss: 1.611842155456543
Batch 6/64 loss: 1.5312566757202148
Batch 7/64 loss: 1.5769133567810059
Batch 8/64 loss: 1.5262041091918945
Batch 9/64 loss: 1.8487234115600586
Batch 10/64 loss: 1.5600953102111816
Batch 11/64 loss: 1.7125391960144043
Batch 12/64 loss: 1.9887938499450684
Batch 13/64 loss: 1.3974404335021973
Batch 14/64 loss: 1.6507573127746582
Batch 15/64 loss: 1.4844169616699219
Batch 16/64 loss: 1.527402400970459
Batch 17/64 loss: 1.1713128089904785
Batch 18/64 loss: 1.4428482055664062
Batch 19/64 loss: 1.5752344131469727
Batch 20/64 loss: 1.661881446838379
Batch 21/64 loss: 1.4284858703613281
Batch 22/64 loss: 1.7356090545654297
Batch 23/64 loss: 2.3468918800354004
Batch 24/64 loss: 1.538996696472168
Batch 25/64 loss: 1.5023880004882812
Batch 26/64 loss: 1.612809181213379
Batch 27/64 loss: 1.5735363960266113
Batch 28/64 loss: 1.4899826049804688
Batch 29/64 loss: 1.5254979133605957
Batch 30/64 loss: 1.3956594467163086
Batch 31/64 loss: 1.963181972503662
Batch 32/64 loss: 1.4982662200927734
Batch 33/64 loss: 1.949275016784668
Batch 34/64 loss: 1.6211700439453125
Batch 35/64 loss: 1.490647792816162
Batch 36/64 loss: 1.672335147857666
Batch 37/64 loss: 1.6451115608215332
Batch 38/64 loss: 1.7302117347717285
Batch 39/64 loss: 1.6510190963745117
Batch 40/64 loss: 1.5723676681518555
Batch 41/64 loss: 1.6505742073059082
Batch 42/64 loss: 2.1195335388183594
Batch 43/64 loss: 1.5546855926513672
Batch 44/64 loss: 1.433115005493164
Batch 45/64 loss: 1.2682838439941406
Batch 46/64 loss: 1.5655975341796875
Batch 47/64 loss: 1.752089023590088
Batch 48/64 loss: 1.45820951461792
Batch 49/64 loss: 1.5027575492858887
Batch 50/64 loss: 1.5124459266662598
Batch 51/64 loss: 1.4531664848327637
Batch 52/64 loss: 1.71022367477417
Batch 53/64 loss: 1.505941390991211
Batch 54/64 loss: 1.6040287017822266
Batch 55/64 loss: 1.5341224670410156
Batch 56/64 loss: 1.5067534446716309
Batch 57/64 loss: 1.9275884628295898
Batch 58/64 loss: 1.864051342010498
Batch 59/64 loss: 1.4234604835510254
Batch 60/64 loss: 1.7024827003479004
Batch 61/64 loss: 1.5561347007751465
Batch 62/64 loss: 1.438460350036621
Batch 63/64 loss: 1.6101078987121582
Batch 64/64 loss: -1.301973819732666
Epoch 58  Train loss: 1.5789631506975959  Val loss: 1.7067488116497027
Epoch 59
-------------------------------
Batch 1/64 loss: 1.642592430114746
Batch 2/64 loss: 1.5843195915222168
Batch 3/64 loss: 1.626347541809082
Batch 4/64 loss: 1.681985855102539
Batch 5/64 loss: 1.7198357582092285
Batch 6/64 loss: 1.8296122550964355
Batch 7/64 loss: 1.7775630950927734
Batch 8/64 loss: 1.8472819328308105
Batch 9/64 loss: 1.6638259887695312
Batch 10/64 loss: 1.7150588035583496
Batch 11/64 loss: 1.2141852378845215
Batch 12/64 loss: 1.7105669975280762
Batch 13/64 loss: 1.925806999206543
Batch 14/64 loss: 1.6550488471984863
Batch 15/64 loss: 1.6828627586364746
Batch 16/64 loss: 1.6414942741394043
Batch 17/64 loss: 1.3638839721679688
Batch 18/64 loss: 1.412801742553711
Batch 19/64 loss: 1.578099250793457
Batch 20/64 loss: 1.6519927978515625
Batch 21/64 loss: 1.6941189765930176
Batch 22/64 loss: 1.6316018104553223
Batch 23/64 loss: 1.315208911895752
Batch 24/64 loss: 1.7904229164123535
Batch 25/64 loss: 1.674565315246582
Batch 26/64 loss: 1.5344109535217285
Batch 27/64 loss: 1.829716682434082
Batch 28/64 loss: 1.3103771209716797
Batch 29/64 loss: 1.4348134994506836
Batch 30/64 loss: 1.5676250457763672
Batch 31/64 loss: 1.4859585762023926
Batch 32/64 loss: 1.548851490020752
Batch 33/64 loss: 2.0483274459838867
Batch 34/64 loss: 1.4985628128051758
Batch 35/64 loss: 1.59647798538208
Batch 36/64 loss: 1.7148966789245605
Batch 37/64 loss: 1.7945399284362793
Batch 38/64 loss: 1.7066259384155273
Batch 39/64 loss: 1.3397603034973145
Batch 40/64 loss: 1.2643961906433105
Batch 41/64 loss: 1.7940349578857422
Batch 42/64 loss: 1.8335857391357422
Batch 43/64 loss: 1.7764873504638672
Batch 44/64 loss: 1.6486454010009766
Batch 45/64 loss: 1.5539026260375977
Batch 46/64 loss: 1.758584976196289
Batch 47/64 loss: 1.5006842613220215
Batch 48/64 loss: 1.6187825202941895
Batch 49/64 loss: 1.619980812072754
Batch 50/64 loss: 1.2733044624328613
Batch 51/64 loss: 1.240898609161377
Batch 52/64 loss: 1.7981700897216797
Batch 53/64 loss: 1.502549171447754
Batch 54/64 loss: 1.257230281829834
Batch 55/64 loss: 1.5939373970031738
Batch 56/64 loss: 1.5097465515136719
Batch 57/64 loss: 1.4592552185058594
Batch 58/64 loss: 1.695319652557373
Batch 59/64 loss: 1.4942317008972168
Batch 60/64 loss: 1.6546273231506348
Batch 61/64 loss: 1.469316005706787
Batch 62/64 loss: 1.7136950492858887
Batch 63/64 loss: 1.6775236129760742
Batch 64/64 loss: -1.4559845924377441
Epoch 59  Train loss: 1.5695518437553855  Val loss: 1.9287955490584225
Epoch 60
-------------------------------
Batch 1/64 loss: 1.5155415534973145
Batch 2/64 loss: 1.6760177612304688
Batch 3/64 loss: 1.3284549713134766
Batch 4/64 loss: 1.4880294799804688
Batch 5/64 loss: 1.9621682167053223
Batch 6/64 loss: 1.4137239456176758
Batch 7/64 loss: 1.4440670013427734
Batch 8/64 loss: 1.47700834274292
Batch 9/64 loss: 1.3297743797302246
Batch 10/64 loss: 1.666656494140625
Batch 11/64 loss: 1.2581028938293457
Batch 12/64 loss: 1.747586727142334
Batch 13/64 loss: 1.556870460510254
Batch 14/64 loss: 1.452378273010254
Batch 15/64 loss: 1.427495002746582
Batch 16/64 loss: 1.5243477821350098
Batch 17/64 loss: 1.546750545501709
Batch 18/64 loss: 1.465658187866211
Batch 19/64 loss: 1.291003704071045
Batch 20/64 loss: 1.7955217361450195
Batch 21/64 loss: 1.6437716484069824
Batch 22/64 loss: 1.6282835006713867
Batch 23/64 loss: 1.6287970542907715
Batch 24/64 loss: 1.6874070167541504
Batch 25/64 loss: 1.6205048561096191
Batch 26/64 loss: 1.6452617645263672
Batch 27/64 loss: 1.6596779823303223
Batch 28/64 loss: 1.8215513229370117
Batch 29/64 loss: 1.5923371315002441
Batch 30/64 loss: 1.685595989227295
Batch 31/64 loss: 1.5649824142456055
Batch 32/64 loss: 1.4694781303405762
Batch 33/64 loss: 1.5235304832458496
Batch 34/64 loss: 1.6779956817626953
Batch 35/64 loss: 1.851262092590332
Batch 36/64 loss: 1.664046287536621
Batch 37/64 loss: 1.6620264053344727
Batch 38/64 loss: 1.5388741493225098
Batch 39/64 loss: 1.660050868988037
Batch 40/64 loss: 1.5716819763183594
Batch 41/64 loss: 1.6941256523132324
Batch 42/64 loss: 1.358414649963379
Batch 43/64 loss: 1.4171724319458008
Batch 44/64 loss: 1.3475565910339355
Batch 45/64 loss: 1.465165138244629
Batch 46/64 loss: 1.9960393905639648
Batch 47/64 loss: 1.5362792015075684
Batch 48/64 loss: 1.6947717666625977
Batch 49/64 loss: 1.3880934715270996
Batch 50/64 loss: 1.3990449905395508
Batch 51/64 loss: 1.3678250312805176
Batch 52/64 loss: 1.5671558380126953
Batch 53/64 loss: 1.2486519813537598
Batch 54/64 loss: 1.9070472717285156
Batch 55/64 loss: 1.6144256591796875
Batch 56/64 loss: 1.5542259216308594
Batch 57/64 loss: 1.8025879859924316
Batch 58/64 loss: 1.5755534172058105
Batch 59/64 loss: 1.630129337310791
Batch 60/64 loss: 1.6845054626464844
Batch 61/64 loss: 1.7779827117919922
Batch 62/64 loss: 1.594344139099121
Batch 63/64 loss: 1.470019817352295
Batch 64/64 loss: -1.8680267333984375
Epoch 60  Train loss: 1.534970541561351  Val loss: 1.5450131131201674
Epoch 61
-------------------------------
Batch 1/64 loss: 1.3211073875427246
Batch 2/64 loss: 1.6382107734680176
Batch 3/64 loss: 1.7558159828186035
Batch 4/64 loss: 1.4650988578796387
Batch 5/64 loss: 1.5903635025024414
Batch 6/64 loss: 1.4906468391418457
Batch 7/64 loss: 1.5422534942626953
Batch 8/64 loss: 1.6518654823303223
Batch 9/64 loss: 1.3037095069885254
Batch 10/64 loss: 1.6958155632019043
Batch 11/64 loss: 1.4915351867675781
Batch 12/64 loss: 1.3977694511413574
Batch 13/64 loss: 1.4734101295471191
Batch 14/64 loss: 1.3966727256774902
Batch 15/64 loss: 1.4784283638000488
Batch 16/64 loss: 1.460075855255127
Batch 17/64 loss: 1.1784563064575195
Batch 18/64 loss: 1.6331663131713867
Batch 19/64 loss: 2.13004732131958
Batch 20/64 loss: 1.2997422218322754
Batch 21/64 loss: 2.0664219856262207
Batch 22/64 loss: 1.7740802764892578
Batch 23/64 loss: 2.219325065612793
Batch 24/64 loss: 1.7354512214660645
Batch 25/64 loss: 1.7668471336364746
Batch 26/64 loss: 1.7739782333374023
Batch 27/64 loss: 1.7889461517333984
Batch 28/64 loss: 1.7324600219726562
Batch 29/64 loss: 1.9978346824645996
Batch 30/64 loss: 1.5684738159179688
Batch 31/64 loss: 1.6021037101745605
Batch 32/64 loss: 2.241696834564209
Batch 33/64 loss: 1.5151643753051758
Batch 34/64 loss: 1.8084759712219238
Batch 35/64 loss: 2.2265658378601074
Batch 36/64 loss: 1.7999215126037598
Batch 37/64 loss: 1.7031569480895996
Batch 38/64 loss: 2.2330522537231445
Batch 39/64 loss: 1.7398204803466797
Batch 40/64 loss: 1.7197380065917969
Batch 41/64 loss: 1.9159245491027832
Batch 42/64 loss: 2.0037426948547363
Batch 43/64 loss: 1.622450828552246
Batch 44/64 loss: 2.150639533996582
Batch 45/64 loss: 1.6000680923461914
Batch 46/64 loss: 1.757730484008789
Batch 47/64 loss: 1.8188214302062988
Batch 48/64 loss: 1.902796745300293
Batch 49/64 loss: 1.7533226013183594
Batch 50/64 loss: 1.9163260459899902
Batch 51/64 loss: 1.8014464378356934
Batch 52/64 loss: 1.7966833114624023
Batch 53/64 loss: 1.6651248931884766
Batch 54/64 loss: 2.0129737854003906
Batch 55/64 loss: 1.8281850814819336
Batch 56/64 loss: 1.6411309242248535
Batch 57/64 loss: 1.5764894485473633
Batch 58/64 loss: 1.7695865631103516
Batch 59/64 loss: 1.811141014099121
Batch 60/64 loss: 1.7477598190307617
Batch 61/64 loss: 1.7499504089355469
Batch 62/64 loss: 1.6015968322753906
Batch 63/64 loss: 1.7038226127624512
Batch 64/64 loss: -0.7035026550292969
Epoch 61  Train loss: 1.6945536145976945  Val loss: 1.7105634695885517
Epoch 62
-------------------------------
Batch 1/64 loss: 1.518282413482666
Batch 2/64 loss: 1.7508039474487305
Batch 3/64 loss: 1.576064109802246
Batch 4/64 loss: 1.8715758323669434
Batch 5/64 loss: 1.7068462371826172
Batch 6/64 loss: 1.757007122039795
Batch 7/64 loss: 1.7664623260498047
Batch 8/64 loss: 1.8482937812805176
Batch 9/64 loss: 1.7808465957641602
Batch 10/64 loss: 1.614840030670166
Batch 11/64 loss: 1.3859553337097168
Batch 12/64 loss: 1.8314924240112305
Batch 13/64 loss: 1.5416040420532227
Batch 14/64 loss: 1.4599275588989258
Batch 15/64 loss: 1.4459028244018555
Batch 16/64 loss: 1.9751486778259277
Batch 17/64 loss: 1.7037429809570312
Batch 18/64 loss: 1.6355290412902832
Batch 19/64 loss: 1.5835919380187988
Batch 20/64 loss: 1.5573797225952148
Batch 21/64 loss: 1.7685637474060059
Batch 22/64 loss: 1.5080318450927734
Batch 23/64 loss: 1.5903215408325195
Batch 24/64 loss: 1.634078025817871
Batch 25/64 loss: 1.4408564567565918
Batch 26/64 loss: 1.8507771492004395
Batch 27/64 loss: 1.638695240020752
Batch 28/64 loss: 1.8013501167297363
Batch 29/64 loss: 1.4947128295898438
Batch 30/64 loss: 1.5230445861816406
Batch 31/64 loss: 1.999007225036621
Batch 32/64 loss: 1.603309154510498
Batch 33/64 loss: 1.858269214630127
Batch 34/64 loss: 1.818925380706787
Batch 35/64 loss: 1.6359615325927734
Batch 36/64 loss: 1.7395496368408203
Batch 37/64 loss: 1.7021331787109375
Batch 38/64 loss: 1.4998207092285156
Batch 39/64 loss: 1.6069350242614746
Batch 40/64 loss: 1.736443042755127
Batch 41/64 loss: 1.3675470352172852
Batch 42/64 loss: 1.8217968940734863
Batch 43/64 loss: 1.7990741729736328
Batch 44/64 loss: 1.5282649993896484
Batch 45/64 loss: 1.9808030128479004
Batch 46/64 loss: 2.013521194458008
Batch 47/64 loss: 1.7516064643859863
Batch 48/64 loss: 1.8693113327026367
Batch 49/64 loss: 2.074983596801758
Batch 50/64 loss: 1.8672337532043457
Batch 51/64 loss: 1.6181492805480957
Batch 52/64 loss: 1.6813197135925293
Batch 53/64 loss: 1.816441535949707
Batch 54/64 loss: 1.5829553604125977
Batch 55/64 loss: 1.284334659576416
Batch 56/64 loss: 1.4000163078308105
Batch 57/64 loss: 1.9558429718017578
Batch 58/64 loss: 1.8890624046325684
Batch 59/64 loss: 1.475142478942871
Batch 60/64 loss: 1.538517951965332
Batch 61/64 loss: 1.3343229293823242
Batch 62/64 loss: 1.859799861907959
Batch 63/64 loss: 1.5513691902160645
Batch 64/64 loss: -1.1788721084594727
Epoch 62  Train loss: 1.6461073819328758  Val loss: 1.5228879083063185
Epoch 63
-------------------------------
Batch 1/64 loss: 1.5719304084777832
Batch 2/64 loss: 1.5289244651794434
Batch 3/64 loss: 1.7788496017456055
Batch 4/64 loss: 1.4953875541687012
Batch 5/64 loss: 1.6352033615112305
Batch 6/64 loss: 1.562727451324463
Batch 7/64 loss: 1.4636716842651367
Batch 8/64 loss: 1.714881420135498
Batch 9/64 loss: 1.465014934539795
Batch 10/64 loss: 1.6034541130065918
Batch 11/64 loss: 1.5575227737426758
Batch 12/64 loss: 1.737473964691162
Batch 13/64 loss: 1.825310230255127
Batch 14/64 loss: 1.4988722801208496
Batch 15/64 loss: 1.6549444198608398
Batch 16/64 loss: 1.6185431480407715
Batch 17/64 loss: 1.5446839332580566
Batch 18/64 loss: 1.646571159362793
Batch 19/64 loss: 1.9477052688598633
Batch 20/64 loss: 1.503260612487793
Batch 21/64 loss: 1.2799797058105469
Batch 22/64 loss: 1.3036761283874512
Batch 23/64 loss: 1.576718807220459
Batch 24/64 loss: 1.9565997123718262
Batch 25/64 loss: 1.7029342651367188
Batch 26/64 loss: 1.3952898979187012
Batch 27/64 loss: 1.4992475509643555
Batch 28/64 loss: 1.7658390998840332
Batch 29/64 loss: 1.9012503623962402
Batch 30/64 loss: 1.339914321899414
Batch 31/64 loss: 1.315840244293213
Batch 32/64 loss: 1.4588561058044434
Batch 33/64 loss: 1.7099952697753906
Batch 34/64 loss: 1.4685969352722168
Batch 35/64 loss: 1.5086932182312012
Batch 36/64 loss: 1.7061614990234375
Batch 37/64 loss: 1.844193935394287
Batch 38/64 loss: 1.5817480087280273
Batch 39/64 loss: 1.520853042602539
Batch 40/64 loss: 1.6691441535949707
Batch 41/64 loss: 1.7900376319885254
Batch 42/64 loss: 1.5838346481323242
Batch 43/64 loss: 1.712294578552246
Batch 44/64 loss: 1.5024995803833008
Batch 45/64 loss: 1.6241345405578613
Batch 46/64 loss: 1.540501594543457
Batch 47/64 loss: 1.5716943740844727
Batch 48/64 loss: 1.6294207572937012
Batch 49/64 loss: 1.6625404357910156
Batch 50/64 loss: 1.6128602027893066
Batch 51/64 loss: 1.2195606231689453
Batch 52/64 loss: 1.4659900665283203
Batch 53/64 loss: 2.021059513092041
Batch 54/64 loss: 1.5078988075256348
Batch 55/64 loss: 1.607900619506836
Batch 56/64 loss: 1.6997451782226562
Batch 57/64 loss: 1.9516396522521973
Batch 58/64 loss: 1.810704231262207
Batch 59/64 loss: 1.8106298446655273
Batch 60/64 loss: 1.4131879806518555
Batch 61/64 loss: 1.6493396759033203
Batch 62/64 loss: 1.5324993133544922
Batch 63/64 loss: 1.7526259422302246
Batch 64/64 loss: -1.0203819274902344
Epoch 63  Train loss: 1.580671033672258  Val loss: 1.6246262776482965
Epoch 64
-------------------------------
Batch 1/64 loss: 1.6750178337097168
Batch 2/64 loss: 1.4253358840942383
Batch 3/64 loss: 2.0384769439697266
Batch 4/64 loss: 1.3403100967407227
Batch 5/64 loss: 1.5349392890930176
Batch 6/64 loss: 1.677049160003662
Batch 7/64 loss: 1.423879623413086
Batch 8/64 loss: 1.6897087097167969
Batch 9/64 loss: 1.9629292488098145
Batch 10/64 loss: 1.4347009658813477
Batch 11/64 loss: 1.6614885330200195
Batch 12/64 loss: 1.571516990661621
Batch 13/64 loss: 1.7742457389831543
Batch 14/64 loss: 1.7435317039489746
Batch 15/64 loss: 1.5807414054870605
Batch 16/64 loss: 1.3833556175231934
Batch 17/64 loss: 1.8767218589782715
Batch 18/64 loss: 1.3669319152832031
Batch 19/64 loss: 1.4829726219177246
Batch 20/64 loss: 1.2721352577209473
Batch 21/64 loss: 1.4459571838378906
Batch 22/64 loss: 1.250831127166748
Batch 23/64 loss: 1.8126468658447266
Batch 24/64 loss: 1.775115966796875
Batch 25/64 loss: 1.8126683235168457
Batch 26/64 loss: 1.6267476081848145
Batch 27/64 loss: 1.60359525680542
Batch 28/64 loss: 1.6885895729064941
Batch 29/64 loss: 1.4462957382202148
Batch 30/64 loss: 1.3738350868225098
Batch 31/64 loss: 1.37554931640625
Batch 32/64 loss: 1.8639588356018066
Batch 33/64 loss: 1.4976353645324707
Batch 34/64 loss: 2.0509629249572754
Batch 35/64 loss: 2.0543575286865234
Batch 36/64 loss: 1.703725814819336
Batch 37/64 loss: 1.741159439086914
Batch 38/64 loss: 1.2363238334655762
Batch 39/64 loss: 1.5731215476989746
Batch 40/64 loss: 1.6510062217712402
Batch 41/64 loss: 1.5492048263549805
Batch 42/64 loss: 1.9268651008605957
Batch 43/64 loss: 1.4069404602050781
Batch 44/64 loss: 1.5385918617248535
Batch 45/64 loss: 1.6046319007873535
Batch 46/64 loss: 1.8232417106628418
Batch 47/64 loss: 1.6847953796386719
Batch 48/64 loss: 1.7505617141723633
Batch 49/64 loss: 1.564389705657959
Batch 50/64 loss: 1.5257444381713867
Batch 51/64 loss: 1.6788640022277832
Batch 52/64 loss: 1.463210105895996
Batch 53/64 loss: 1.7847156524658203
Batch 54/64 loss: 1.6404390335083008
Batch 55/64 loss: 1.5616192817687988
Batch 56/64 loss: 1.6157946586608887
Batch 57/64 loss: 1.895721435546875
Batch 58/64 loss: 1.3997673988342285
Batch 59/64 loss: 1.608931541442871
Batch 60/64 loss: 1.4791221618652344
Batch 61/64 loss: 1.738208293914795
Batch 62/64 loss: 1.4656271934509277
Batch 63/64 loss: 1.4254236221313477
Batch 64/64 loss: -1.7943763732910156
Epoch 64  Train loss: 1.5731247397030101  Val loss: 1.6220560237714106
Epoch 65
-------------------------------
Batch 1/64 loss: 1.5229473114013672
Batch 2/64 loss: 1.4941234588623047
Batch 3/64 loss: 1.5006914138793945
Batch 4/64 loss: 1.933070182800293
Batch 5/64 loss: 1.548940658569336
Batch 6/64 loss: 1.7504630088806152
Batch 7/64 loss: 1.605318546295166
Batch 8/64 loss: 1.4238204956054688
Batch 9/64 loss: 1.8338494300842285
Batch 10/64 loss: 1.4228296279907227
Batch 11/64 loss: 1.3482418060302734
Batch 12/64 loss: 1.4827766418457031
Batch 13/64 loss: 1.3621015548706055
Batch 14/64 loss: 1.8237605094909668
Batch 15/64 loss: 1.3090357780456543
Batch 16/64 loss: 1.5570988655090332
Batch 17/64 loss: 1.274085521697998
Batch 18/64 loss: 1.4762282371520996
Batch 19/64 loss: 1.404545783996582
Batch 20/64 loss: 1.4685163497924805
Batch 21/64 loss: 1.52854585647583
Batch 22/64 loss: 1.6408352851867676
Batch 23/64 loss: 1.4769282341003418
Batch 24/64 loss: 1.8347125053405762
Batch 25/64 loss: 1.1863532066345215
Batch 26/64 loss: 1.5351133346557617
Batch 27/64 loss: 1.5363879203796387
Batch 28/64 loss: 1.742527961730957
Batch 29/64 loss: 1.5536842346191406
Batch 30/64 loss: 1.6234321594238281
Batch 31/64 loss: 1.8087172508239746
Batch 32/64 loss: 1.756624698638916
Batch 33/64 loss: 1.5598645210266113
Batch 34/64 loss: 1.8298468589782715
Batch 35/64 loss: 1.6177911758422852
Batch 36/64 loss: 1.4408926963806152
Batch 37/64 loss: 1.709062099456787
Batch 38/64 loss: 2.1794261932373047
Batch 39/64 loss: 1.469141960144043
Batch 40/64 loss: 1.619492530822754
Batch 41/64 loss: 1.4435162544250488
Batch 42/64 loss: 2.0254149436950684
Batch 43/64 loss: 1.2934298515319824
Batch 44/64 loss: 1.7842411994934082
Batch 45/64 loss: 1.43294095993042
Batch 46/64 loss: 1.2844386100769043
Batch 47/64 loss: 1.7154459953308105
Batch 48/64 loss: 1.4928679466247559
Batch 49/64 loss: 1.3174500465393066
Batch 50/64 loss: 1.4587726593017578
Batch 51/64 loss: 1.9021973609924316
Batch 52/64 loss: 1.7140979766845703
Batch 53/64 loss: 1.2894315719604492
Batch 54/64 loss: 1.4808745384216309
Batch 55/64 loss: 1.3470673561096191
Batch 56/64 loss: 1.2324681282043457
Batch 57/64 loss: 1.6781697273254395
Batch 58/64 loss: 1.7982797622680664
Batch 59/64 loss: 1.3120393753051758
Batch 60/64 loss: 1.821622371673584
Batch 61/64 loss: 1.7445192337036133
Batch 62/64 loss: 1.8969802856445312
Batch 63/64 loss: 1.212972640991211
Batch 64/64 loss: -1.3336701393127441
Epoch 65  Train loss: 1.5352284244462555  Val loss: 1.521716252225371
Epoch 66
-------------------------------
Batch 1/64 loss: 1.8039836883544922
Batch 2/64 loss: 1.451343059539795
Batch 3/64 loss: 1.4251832962036133
Batch 4/64 loss: 1.5641660690307617
Batch 5/64 loss: 1.7001910209655762
Batch 6/64 loss: 1.3537802696228027
Batch 7/64 loss: 1.3737893104553223
Batch 8/64 loss: 1.2448043823242188
Batch 9/64 loss: 1.603731632232666
Batch 10/64 loss: 2.1135244369506836
Batch 11/64 loss: 1.452458381652832
Batch 12/64 loss: 1.5108509063720703
Batch 13/64 loss: 1.698641300201416
Batch 14/64 loss: 1.5093941688537598
Batch 15/64 loss: 2.0570077896118164
Batch 16/64 loss: 1.4855494499206543
Batch 17/64 loss: 1.466038703918457
Batch 18/64 loss: 1.676663875579834
Batch 19/64 loss: 1.760354995727539
Batch 20/64 loss: 1.5786256790161133
Batch 21/64 loss: 1.8024659156799316
Batch 22/64 loss: 1.6813650131225586
Batch 23/64 loss: 1.8592681884765625
Batch 24/64 loss: 1.370811939239502
Batch 25/64 loss: 1.3820123672485352
Batch 26/64 loss: 1.7061586380004883
Batch 27/64 loss: 1.631281852722168
Batch 28/64 loss: 1.7379274368286133
Batch 29/64 loss: 1.4322361946105957
Batch 30/64 loss: 1.3676443099975586
Batch 31/64 loss: 1.423445701599121
Batch 32/64 loss: 1.432448387145996
Batch 33/64 loss: 1.7075963020324707
Batch 34/64 loss: 1.3042869567871094
Batch 35/64 loss: 1.5277862548828125
Batch 36/64 loss: 1.2974982261657715
Batch 37/64 loss: 1.542658805847168
Batch 38/64 loss: 1.3645234107971191
Batch 39/64 loss: 1.5886788368225098
Batch 40/64 loss: 2.1150217056274414
Batch 41/64 loss: 1.8932561874389648
Batch 42/64 loss: 1.3785357475280762
Batch 43/64 loss: 2.001746654510498
Batch 44/64 loss: 1.793022632598877
Batch 45/64 loss: 1.689159870147705
Batch 46/64 loss: 1.6517915725708008
Batch 47/64 loss: 1.510901927947998
Batch 48/64 loss: 1.4875531196594238
Batch 49/64 loss: 1.427663803100586
Batch 50/64 loss: 1.4976577758789062
Batch 51/64 loss: 1.4307708740234375
Batch 52/64 loss: 1.690147876739502
Batch 53/64 loss: 1.2904481887817383
Batch 54/64 loss: 1.5709586143493652
Batch 55/64 loss: 1.2270302772521973
Batch 56/64 loss: 1.1974282264709473
Batch 57/64 loss: 1.3883647918701172
Batch 58/64 loss: 1.300534725189209
Batch 59/64 loss: 1.578946590423584
Batch 60/64 loss: 1.6878457069396973
Batch 61/64 loss: 1.902540683746338
Batch 62/64 loss: 1.3691964149475098
Batch 63/64 loss: 1.9414381980895996
Batch 64/64 loss: -1.3387866020202637
Epoch 66  Train loss: 1.5373811665703268  Val loss: 1.6698110652543425
Epoch 67
-------------------------------
Batch 1/64 loss: 1.5522089004516602
Batch 2/64 loss: 1.58500337600708
Batch 3/64 loss: 1.3091039657592773
Batch 4/64 loss: 1.4429998397827148
Batch 5/64 loss: 1.5325026512145996
Batch 6/64 loss: 1.9116907119750977
Batch 7/64 loss: 1.4958744049072266
Batch 8/64 loss: 1.7516942024230957
Batch 9/64 loss: 1.3451871871948242
Batch 10/64 loss: 1.427565097808838
Batch 11/64 loss: 1.5568170547485352
Batch 12/64 loss: 1.6455574035644531
Batch 13/64 loss: 1.849900245666504
Batch 14/64 loss: 1.520437240600586
Batch 15/64 loss: 1.242037296295166
Batch 16/64 loss: 1.47166109085083
Batch 17/64 loss: 1.5396337509155273
Batch 18/64 loss: 1.2522788047790527
Batch 19/64 loss: 1.4128694534301758
Batch 20/64 loss: 1.5175628662109375
Batch 21/64 loss: 1.7486333847045898
Batch 22/64 loss: 1.445793628692627
Batch 23/64 loss: 1.4754538536071777
Batch 24/64 loss: 2.008129119873047
Batch 25/64 loss: 1.4265117645263672
Batch 26/64 loss: 1.6682429313659668
Batch 27/64 loss: 1.569328784942627
Batch 28/64 loss: 1.6445727348327637
Batch 29/64 loss: 1.2576069831848145
Batch 30/64 loss: 1.5577192306518555
Batch 31/64 loss: 1.3349432945251465
Batch 32/64 loss: 1.8787884712219238
Batch 33/64 loss: 1.494004726409912
Batch 34/64 loss: 1.491739273071289
Batch 35/64 loss: 1.704470157623291
Batch 36/64 loss: 1.8414230346679688
Batch 37/64 loss: 1.6639790534973145
Batch 38/64 loss: 1.774655818939209
Batch 39/64 loss: 1.3704261779785156
Batch 40/64 loss: 1.4581327438354492
Batch 41/64 loss: 1.4423880577087402
Batch 42/64 loss: 1.5977296829223633
Batch 43/64 loss: 1.4227452278137207
Batch 44/64 loss: 1.8277411460876465
Batch 45/64 loss: 1.5086278915405273
Batch 46/64 loss: 1.3418774604797363
Batch 47/64 loss: 1.5082049369812012
Batch 48/64 loss: 1.8192143440246582
Batch 49/64 loss: 1.3322458267211914
Batch 50/64 loss: 1.7495498657226562
Batch 51/64 loss: 1.4064512252807617
Batch 52/64 loss: 1.5595159530639648
Batch 53/64 loss: 1.4986863136291504
Batch 54/64 loss: 1.6342854499816895
Batch 55/64 loss: 1.5978035926818848
Batch 56/64 loss: 1.7239813804626465
Batch 57/64 loss: 1.4233555793762207
Batch 58/64 loss: 1.6399316787719727
Batch 59/64 loss: 1.4617176055908203
Batch 60/64 loss: 1.674269676208496
Batch 61/64 loss: 1.6740188598632812
Batch 62/64 loss: 1.4913339614868164
Batch 63/64 loss: 1.5119342803955078
Batch 64/64 loss: -1.6786532402038574
Epoch 67  Train loss: 1.517894286735385  Val loss: 1.629281637185218
Epoch 68
-------------------------------
Batch 1/64 loss: 1.4233379364013672
Batch 2/64 loss: 1.3187460899353027
Batch 3/64 loss: 1.6863174438476562
Batch 4/64 loss: 1.6257176399230957
Batch 5/64 loss: 1.7694354057312012
Batch 6/64 loss: 1.6097526550292969
Batch 7/64 loss: 1.5807456970214844
Batch 8/64 loss: 1.4901747703552246
Batch 9/64 loss: 1.4811615943908691
Batch 10/64 loss: 1.4160470962524414
Batch 11/64 loss: 1.5632872581481934
Batch 12/64 loss: 1.2665467262268066
Batch 13/64 loss: 1.2906413078308105
Batch 14/64 loss: 1.2076210975646973
Batch 15/64 loss: 1.7277483940124512
Batch 16/64 loss: 1.2843656539916992
Batch 17/64 loss: 1.4752836227416992
Batch 18/64 loss: 1.339561939239502
Batch 19/64 loss: 1.3692641258239746
Batch 20/64 loss: 1.439530372619629
Batch 21/64 loss: 1.5862078666687012
Batch 22/64 loss: 1.6572866439819336
Batch 23/64 loss: 1.6449437141418457
Batch 24/64 loss: 1.3663372993469238
Batch 25/64 loss: 1.454361915588379
Batch 26/64 loss: 1.3284931182861328
Batch 27/64 loss: 1.7184700965881348
Batch 28/64 loss: 1.5889511108398438
Batch 29/64 loss: 1.9524765014648438
Batch 30/64 loss: 1.3555974960327148
Batch 31/64 loss: 1.5834341049194336
Batch 32/64 loss: 1.4347748756408691
Batch 33/64 loss: 1.3203744888305664
Batch 34/64 loss: 1.2888946533203125
Batch 35/64 loss: 1.314199447631836
Batch 36/64 loss: 1.8866910934448242
Batch 37/64 loss: 1.2860913276672363
Batch 38/64 loss: 1.7054071426391602
Batch 39/64 loss: 1.5923042297363281
Batch 40/64 loss: 1.7892284393310547
Batch 41/64 loss: 1.4397006034851074
Batch 42/64 loss: 1.5452094078063965
Batch 43/64 loss: 1.5045061111450195
Batch 44/64 loss: 1.2947864532470703
Batch 45/64 loss: 2.035627841949463
Batch 46/64 loss: 1.462569236755371
Batch 47/64 loss: 1.2778267860412598
Batch 48/64 loss: 2.224395275115967
Batch 49/64 loss: 1.4509754180908203
Batch 50/64 loss: 1.515185832977295
Batch 51/64 loss: 1.510047435760498
Batch 52/64 loss: 1.4572110176086426
Batch 53/64 loss: 1.4070348739624023
Batch 54/64 loss: 1.6582627296447754
Batch 55/64 loss: 1.844616413116455
Batch 56/64 loss: 1.3750128746032715
Batch 57/64 loss: 1.5576224327087402
Batch 58/64 loss: 1.636120319366455
Batch 59/64 loss: 2.0371880531311035
Batch 60/64 loss: 1.6625380516052246
Batch 61/64 loss: 1.6621737480163574
Batch 62/64 loss: 1.526998519897461
Batch 63/64 loss: 1.823979377746582
Batch 64/64 loss: -1.457387924194336
Epoch 68  Train loss: 1.5064527100207759  Val loss: 1.7576576901465346
Epoch 69
-------------------------------
Batch 1/64 loss: 1.5576424598693848
Batch 2/64 loss: 1.7750177383422852
Batch 3/64 loss: 1.5434341430664062
Batch 4/64 loss: 1.7694602012634277
Batch 5/64 loss: 1.725870132446289
Batch 6/64 loss: 1.8160786628723145
Batch 7/64 loss: 1.4580559730529785
Batch 8/64 loss: 1.5475492477416992
Batch 9/64 loss: 1.5493879318237305
Batch 10/64 loss: 1.389338493347168
Batch 11/64 loss: 1.4215121269226074
Batch 12/64 loss: 1.447251796722412
Batch 13/64 loss: 1.4635376930236816
Batch 14/64 loss: 1.5145654678344727
Batch 15/64 loss: 1.5726280212402344
Batch 16/64 loss: 1.374427318572998
Batch 17/64 loss: 1.2878689765930176
Batch 18/64 loss: 1.5198278427124023
Batch 19/64 loss: 1.4611825942993164
Batch 20/64 loss: 1.1410069465637207
Batch 21/64 loss: 1.560105323791504
Batch 22/64 loss: 1.576979637145996
Batch 23/64 loss: 1.683887004852295
Batch 24/64 loss: 1.6697192192077637
Batch 25/64 loss: 1.6147246360778809
Batch 26/64 loss: 1.2576122283935547
Batch 27/64 loss: 1.3989133834838867
Batch 28/64 loss: 1.6032204627990723
Batch 29/64 loss: 1.2689499855041504
Batch 30/64 loss: 1.660031795501709
Batch 31/64 loss: 1.1830148696899414
Batch 32/64 loss: 1.6615910530090332
Batch 33/64 loss: 1.4979305267333984
Batch 34/64 loss: 1.6929826736450195
Batch 35/64 loss: 1.3454689979553223
Batch 36/64 loss: 2.6425013542175293
Batch 37/64 loss: 1.657515525817871
Batch 38/64 loss: 1.2657899856567383
Batch 39/64 loss: 1.3229541778564453
Batch 40/64 loss: 1.6803393363952637
Batch 41/64 loss: 1.3618474006652832
Batch 42/64 loss: 1.8196296691894531
Batch 43/64 loss: 1.3601956367492676
Batch 44/64 loss: 1.2856788635253906
Batch 45/64 loss: 1.4580788612365723
Batch 46/64 loss: 1.173830509185791
Batch 47/64 loss: 1.685256004333496
Batch 48/64 loss: 1.460240364074707
Batch 49/64 loss: 1.640777587890625
Batch 50/64 loss: 2.0261292457580566
Batch 51/64 loss: 1.5340847969055176
Batch 52/64 loss: 1.4262704849243164
Batch 53/64 loss: 1.2410712242126465
Batch 54/64 loss: 1.5182361602783203
Batch 55/64 loss: 1.4898066520690918
Batch 56/64 loss: 1.523087501525879
Batch 57/64 loss: 1.8437485694885254
Batch 58/64 loss: 1.7719120979309082
Batch 59/64 loss: 1.5366334915161133
Batch 60/64 loss: 1.4197168350219727
Batch 61/64 loss: 1.420351505279541
Batch 62/64 loss: 1.3967247009277344
Batch 63/64 loss: 1.3278765678405762
Batch 64/64 loss: -1.3974056243896484
Epoch 69  Train loss: 1.4941648071887446  Val loss: 1.5115134970019364
Saving best model, epoch: 69
Epoch 70
-------------------------------
Batch 1/64 loss: 1.535935401916504
Batch 2/64 loss: 1.4564509391784668
Batch 3/64 loss: 1.2909646034240723
Batch 4/64 loss: 1.867990493774414
Batch 5/64 loss: 1.2212581634521484
Batch 6/64 loss: 1.5412640571594238
Batch 7/64 loss: 1.401477336883545
Batch 8/64 loss: 1.5730781555175781
Batch 9/64 loss: 1.495481014251709
Batch 10/64 loss: 1.6236815452575684
Batch 11/64 loss: 1.3848509788513184
Batch 12/64 loss: 1.4395909309387207
Batch 13/64 loss: 1.460024356842041
Batch 14/64 loss: 1.195347785949707
Batch 15/64 loss: 1.4708938598632812
Batch 16/64 loss: 1.1807966232299805
Batch 17/64 loss: 1.3623147010803223
Batch 18/64 loss: 1.2786002159118652
Batch 19/64 loss: 1.5535054206848145
Batch 20/64 loss: 1.2371702194213867
Batch 21/64 loss: 1.3596796989440918
Batch 22/64 loss: 2.073537826538086
Batch 23/64 loss: 1.4272117614746094
Batch 24/64 loss: 1.579127311706543
Batch 25/64 loss: 1.3277397155761719
Batch 26/64 loss: 1.528454303741455
Batch 27/64 loss: 1.5112228393554688
Batch 28/64 loss: 1.465245246887207
Batch 29/64 loss: 1.5637593269348145
Batch 30/64 loss: 1.8260016441345215
Batch 31/64 loss: 1.5384979248046875
Batch 32/64 loss: 2.4223713874816895
Batch 33/64 loss: 1.5399699211120605
Batch 34/64 loss: 1.508486270904541
Batch 35/64 loss: 1.716200351715088
Batch 36/64 loss: 1.2786526679992676
Batch 37/64 loss: 1.3395013809204102
Batch 38/64 loss: 1.4892034530639648
Batch 39/64 loss: 1.5891432762145996
Batch 40/64 loss: 1.3399896621704102
Batch 41/64 loss: 1.5660734176635742
Batch 42/64 loss: 1.6799249649047852
Batch 43/64 loss: 1.3463411331176758
Batch 44/64 loss: 1.5337114334106445
Batch 45/64 loss: 1.4086103439331055
Batch 46/64 loss: 1.4977836608886719
Batch 47/64 loss: 1.2162270545959473
Batch 48/64 loss: 1.973897933959961
Batch 49/64 loss: 1.5998749732971191
Batch 50/64 loss: 1.418635368347168
Batch 51/64 loss: 1.4212040901184082
Batch 52/64 loss: 1.2478690147399902
Batch 53/64 loss: 1.602391242980957
Batch 54/64 loss: 1.3363971710205078
Batch 55/64 loss: 1.3566937446594238
Batch 56/64 loss: 1.4249629974365234
Batch 57/64 loss: 1.5100984573364258
Batch 58/64 loss: 1.8097615242004395
Batch 59/64 loss: 1.1981501579284668
Batch 60/64 loss: 1.4687042236328125
Batch 61/64 loss: 1.323084831237793
Batch 62/64 loss: 1.4088478088378906
Batch 63/64 loss: 1.4624896049499512
Batch 64/64 loss: -1.6500396728515625
Epoch 70  Train loss: 1.4520608340992647  Val loss: 1.7041990011418398
Epoch 71
-------------------------------
Batch 1/64 loss: 1.4268617630004883
Batch 2/64 loss: 1.553978443145752
Batch 3/64 loss: 1.5762619972229004
Batch 4/64 loss: 1.1099786758422852
Batch 5/64 loss: 1.3398284912109375
Batch 6/64 loss: 1.509079933166504
Batch 7/64 loss: 1.622331142425537
Batch 8/64 loss: 1.7994036674499512
Batch 9/64 loss: 1.3660674095153809
Batch 10/64 loss: 1.4441843032836914
Batch 11/64 loss: 2.5351104736328125
Batch 12/64 loss: 1.644721508026123
Batch 13/64 loss: 2.3814406394958496
Batch 14/64 loss: 2.13308048248291
Batch 15/64 loss: 1.851935863494873
Batch 16/64 loss: 2.3696999549865723
Batch 17/64 loss: 1.924952507019043
Batch 18/64 loss: 1.6721930503845215
Batch 19/64 loss: 1.8759031295776367
Batch 20/64 loss: 2.172586441040039
Batch 21/64 loss: 1.5584406852722168
Batch 22/64 loss: 2.0113654136657715
Batch 23/64 loss: 2.030093193054199
Batch 24/64 loss: 1.9312553405761719
Batch 25/64 loss: 2.1426191329956055
Batch 26/64 loss: 1.8663082122802734
Batch 27/64 loss: 1.7118902206420898
Batch 28/64 loss: 2.026620864868164
Batch 29/64 loss: 1.6648526191711426
Batch 30/64 loss: 1.7504849433898926
Batch 31/64 loss: 1.9727048873901367
Batch 32/64 loss: 1.7832145690917969
Batch 33/64 loss: 2.016110897064209
Batch 34/64 loss: 2.0294508934020996
Batch 35/64 loss: 1.7445058822631836
Batch 36/64 loss: 1.8016486167907715
Batch 37/64 loss: 2.141657829284668
Batch 38/64 loss: 1.5446782112121582
Batch 39/64 loss: 1.819098949432373
Batch 40/64 loss: 1.4302277565002441
Batch 41/64 loss: 1.5453472137451172
Batch 42/64 loss: 1.7197060585021973
Batch 43/64 loss: 1.6078529357910156
Batch 44/64 loss: 1.7507009506225586
Batch 45/64 loss: 1.5684847831726074
Batch 46/64 loss: 1.5795965194702148
Batch 47/64 loss: 1.5385394096374512
Batch 48/64 loss: 1.9819836616516113
Batch 49/64 loss: 1.5171995162963867
Batch 50/64 loss: 1.7165956497192383
Batch 51/64 loss: 1.434044361114502
Batch 52/64 loss: 1.7627577781677246
Batch 53/64 loss: 1.5954508781433105
Batch 54/64 loss: 1.3680129051208496
Batch 55/64 loss: 1.684034824371338
Batch 56/64 loss: 1.508951187133789
Batch 57/64 loss: 1.5547518730163574
Batch 58/64 loss: 1.7693772315979004
Batch 59/64 loss: 1.449136734008789
Batch 60/64 loss: 1.5490946769714355
Batch 61/64 loss: 1.9682621955871582
Batch 62/64 loss: 1.4418034553527832
Batch 63/64 loss: 1.6142034530639648
Batch 64/64 loss: -1.395789623260498
Epoch 71  Train loss: 1.7014254906598258  Val loss: 1.616928277556429
Epoch 72
-------------------------------
Batch 1/64 loss: 1.525686264038086
Batch 2/64 loss: 1.2714781761169434
Batch 3/64 loss: 1.4269604682922363
Batch 4/64 loss: 1.1360926628112793
Batch 5/64 loss: 1.3975086212158203
Batch 6/64 loss: 1.5054240226745605
Batch 7/64 loss: 1.941739559173584
Batch 8/64 loss: 1.6649060249328613
Batch 9/64 loss: 1.3264470100402832
Batch 10/64 loss: 1.202284336090088
Batch 11/64 loss: 1.699106216430664
Batch 12/64 loss: 1.3157267570495605
Batch 13/64 loss: 1.3767743110656738
Batch 14/64 loss: 1.5509648323059082
Batch 15/64 loss: 1.6704902648925781
Batch 16/64 loss: 1.2311148643493652
Batch 17/64 loss: 1.5267210006713867
Batch 18/64 loss: 1.795712947845459
Batch 19/64 loss: 1.362360954284668
Batch 20/64 loss: 1.4997262954711914
Batch 21/64 loss: 1.572251319885254
Batch 22/64 loss: 1.7385287284851074
Batch 23/64 loss: 1.7305755615234375
Batch 24/64 loss: 1.583022117614746
Batch 25/64 loss: 1.7497730255126953
Batch 26/64 loss: 1.8110718727111816
Batch 27/64 loss: 1.399552345275879
Batch 28/64 loss: 2.0255327224731445
Batch 29/64 loss: 1.5969223976135254
Batch 30/64 loss: 1.3171377182006836
Batch 31/64 loss: 1.4705767631530762
Batch 32/64 loss: 1.1677956581115723
Batch 33/64 loss: 1.5684690475463867
Batch 34/64 loss: 1.5539970397949219
Batch 35/64 loss: 1.2349295616149902
Batch 36/64 loss: 1.4408016204833984
Batch 37/64 loss: 1.3450870513916016
Batch 38/64 loss: 1.4769954681396484
Batch 39/64 loss: 1.7243375778198242
Batch 40/64 loss: 1.5266094207763672
Batch 41/64 loss: 1.5399222373962402
Batch 42/64 loss: 1.3678817749023438
Batch 43/64 loss: 1.604438304901123
Batch 44/64 loss: 1.648022174835205
Batch 45/64 loss: 1.2998127937316895
Batch 46/64 loss: 1.3073410987854004
Batch 47/64 loss: 1.8242449760437012
Batch 48/64 loss: 1.9163575172424316
Batch 49/64 loss: 1.39093017578125
Batch 50/64 loss: 1.4407105445861816
Batch 51/64 loss: 1.4404821395874023
Batch 52/64 loss: 1.5255308151245117
Batch 53/64 loss: 1.854459285736084
Batch 54/64 loss: 1.6747779846191406
Batch 55/64 loss: 1.3291211128234863
Batch 56/64 loss: 1.676537036895752
Batch 57/64 loss: 1.4527602195739746
Batch 58/64 loss: 1.4894070625305176
Batch 59/64 loss: 1.5477819442749023
Batch 60/64 loss: 1.7900214195251465
Batch 61/64 loss: 1.3275351524353027
Batch 62/64 loss: 1.3980937004089355
Batch 63/64 loss: 1.4981942176818848
Batch 64/64 loss: -1.5634655952453613
Epoch 72  Train loss: 1.4844385427587172  Val loss: 1.5817366662304015
Epoch 73
-------------------------------
Batch 1/64 loss: 1.5862360000610352
Batch 2/64 loss: 1.379809856414795
Batch 3/64 loss: 1.330406665802002
Batch 4/64 loss: 1.1461844444274902
Batch 5/64 loss: 1.3795151710510254
Batch 6/64 loss: 1.6166949272155762
Batch 7/64 loss: 1.3564887046813965
Batch 8/64 loss: 1.3700337409973145
Batch 9/64 loss: 1.877488613128662
Batch 10/64 loss: 1.2222113609313965
Batch 11/64 loss: 1.5889387130737305
Batch 12/64 loss: 1.1907143592834473
Batch 13/64 loss: 1.8475589752197266
Batch 14/64 loss: 1.390977382659912
Batch 15/64 loss: 1.5441303253173828
Batch 16/64 loss: 1.4330172538757324
Batch 17/64 loss: 1.3330721855163574
Batch 18/64 loss: 1.3407864570617676
Batch 19/64 loss: 1.388303279876709
Batch 20/64 loss: 1.5829243659973145
Batch 21/64 loss: 1.5252714157104492
Batch 22/64 loss: 1.6630606651306152
Batch 23/64 loss: 1.500894546508789
Batch 24/64 loss: 1.6580610275268555
Batch 25/64 loss: 1.7384681701660156
Batch 26/64 loss: 1.5940780639648438
Batch 27/64 loss: 1.3627581596374512
Batch 28/64 loss: 1.6790170669555664
Batch 29/64 loss: 1.5147709846496582
Batch 30/64 loss: 1.3889431953430176
Batch 31/64 loss: 1.4801740646362305
Batch 32/64 loss: 1.3685622215270996
Batch 33/64 loss: 1.419527530670166
Batch 34/64 loss: 1.5230693817138672
Batch 35/64 loss: 1.4187731742858887
Batch 36/64 loss: 1.331125259399414
Batch 37/64 loss: 1.6111974716186523
Batch 38/64 loss: 1.5882554054260254
Batch 39/64 loss: 1.3697257041931152
Batch 40/64 loss: 1.269331455230713
Batch 41/64 loss: 1.9131498336791992
Batch 42/64 loss: 1.727363109588623
Batch 43/64 loss: 1.3129701614379883
Batch 44/64 loss: 1.218163013458252
Batch 45/64 loss: 1.274268627166748
Batch 46/64 loss: 1.386085033416748
Batch 47/64 loss: 1.444800853729248
Batch 48/64 loss: 1.3279380798339844
Batch 49/64 loss: 1.2917613983154297
Batch 50/64 loss: 1.2154145240783691
Batch 51/64 loss: 1.6327252388000488
Batch 52/64 loss: 1.3644232749938965
Batch 53/64 loss: 1.62037992477417
Batch 54/64 loss: 1.899195671081543
Batch 55/64 loss: 1.3270082473754883
Batch 56/64 loss: 1.4298276901245117
Batch 57/64 loss: 1.1788554191589355
Batch 58/64 loss: 1.210014820098877
Batch 59/64 loss: 1.6898155212402344
Batch 60/64 loss: 1.3677797317504883
Batch 61/64 loss: 1.35408353805542
Batch 62/64 loss: 1.5950579643249512
Batch 63/64 loss: 1.108910083770752
Batch 64/64 loss: -1.3467836380004883
Epoch 73  Train loss: 1.4241641063316195  Val loss: 1.421293337320544
Saving best model, epoch: 73
Epoch 74
-------------------------------
Batch 1/64 loss: 1.2682604789733887
Batch 2/64 loss: 1.3623733520507812
Batch 3/64 loss: 1.2456645965576172
Batch 4/64 loss: 1.0698270797729492
Batch 5/64 loss: 1.321007251739502
Batch 6/64 loss: 1.565554141998291
Batch 7/64 loss: 1.006563663482666
Batch 8/64 loss: 1.680126667022705
Batch 9/64 loss: 1.2515559196472168
Batch 10/64 loss: 1.1231379508972168
Batch 11/64 loss: 1.249028205871582
Batch 12/64 loss: 1.590322494506836
Batch 13/64 loss: 1.7475261688232422
Batch 14/64 loss: 1.3494834899902344
Batch 15/64 loss: 1.3027091026306152
Batch 16/64 loss: 1.0799760818481445
Batch 17/64 loss: 1.2348861694335938
Batch 18/64 loss: 1.3676810264587402
Batch 19/64 loss: 1.5777969360351562
Batch 20/64 loss: 1.3291687965393066
Batch 21/64 loss: 1.720390796661377
Batch 22/64 loss: 1.6703009605407715
Batch 23/64 loss: 1.7181849479675293
Batch 24/64 loss: 1.3997130393981934
Batch 25/64 loss: 1.6358728408813477
Batch 26/64 loss: 1.4123210906982422
Batch 27/64 loss: 1.4200425148010254
Batch 28/64 loss: 1.1619338989257812
Batch 29/64 loss: 1.3543424606323242
Batch 30/64 loss: 1.4427027702331543
Batch 31/64 loss: 1.231398582458496
Batch 32/64 loss: 2.2247133255004883
Batch 33/64 loss: 1.5765676498413086
Batch 34/64 loss: 1.5196800231933594
Batch 35/64 loss: 1.7471728324890137
Batch 36/64 loss: 1.6376457214355469
Batch 37/64 loss: 1.368095874786377
Batch 38/64 loss: 1.4067015647888184
Batch 39/64 loss: 1.4141149520874023
Batch 40/64 loss: 1.3729476928710938
Batch 41/64 loss: 1.603635311126709
Batch 42/64 loss: 1.3395261764526367
Batch 43/64 loss: 1.320500373840332
Batch 44/64 loss: 1.7807817459106445
Batch 45/64 loss: 1.3582735061645508
Batch 46/64 loss: 1.8214731216430664
Batch 47/64 loss: 2.106740951538086
Batch 48/64 loss: 1.4159507751464844
Batch 49/64 loss: 1.183000087738037
Batch 50/64 loss: 1.243093490600586
Batch 51/64 loss: 1.9616098403930664
Batch 52/64 loss: 1.330564022064209
Batch 53/64 loss: 1.5772347450256348
Batch 54/64 loss: 1.6555194854736328
Batch 55/64 loss: 1.4685101509094238
Batch 56/64 loss: 1.6938228607177734
Batch 57/64 loss: 1.6143102645874023
Batch 58/64 loss: 1.5035419464111328
Batch 59/64 loss: 1.5762934684753418
Batch 60/64 loss: 1.5553083419799805
Batch 61/64 loss: 1.7143182754516602
Batch 62/64 loss: 1.5286364555358887
Batch 63/64 loss: 1.3336243629455566
Batch 64/64 loss: -1.7240643501281738
Epoch 74  Train loss: 1.4360896409726611  Val loss: 1.3977810705650304
Saving best model, epoch: 74
Epoch 75
-------------------------------
Batch 1/64 loss: 1.407881259918213
Batch 2/64 loss: 1.3656725883483887
Batch 3/64 loss: 1.182469367980957
Batch 4/64 loss: 1.637298583984375
Batch 5/64 loss: 1.3633008003234863
Batch 6/64 loss: 1.5023431777954102
Batch 7/64 loss: 1.723496913909912
Batch 8/64 loss: 1.5062808990478516
Batch 9/64 loss: 1.2280693054199219
Batch 10/64 loss: 1.4464149475097656
Batch 11/64 loss: 1.541938304901123
Batch 12/64 loss: 1.6107702255249023
Batch 13/64 loss: 1.3024115562438965
Batch 14/64 loss: 1.9910459518432617
Batch 15/64 loss: 1.506307601928711
Batch 16/64 loss: 1.6829862594604492
Batch 17/64 loss: 1.685586929321289
Batch 18/64 loss: 1.4226984977722168
Batch 19/64 loss: 2.16190242767334
Batch 20/64 loss: 1.5275564193725586
Batch 21/64 loss: 1.4383940696716309
Batch 22/64 loss: 1.4282031059265137
Batch 23/64 loss: 1.8934760093688965
Batch 24/64 loss: 1.7142071723937988
Batch 25/64 loss: 1.5283408164978027
Batch 26/64 loss: 1.7100324630737305
Batch 27/64 loss: 1.727829933166504
Batch 28/64 loss: 1.4885153770446777
Batch 29/64 loss: 1.6174578666687012
Batch 30/64 loss: 1.3725361824035645
Batch 31/64 loss: 1.6169438362121582
Batch 32/64 loss: 1.7243695259094238
Batch 33/64 loss: 1.6876778602600098
Batch 34/64 loss: 1.6017823219299316
Batch 35/64 loss: 1.30267333984375
Batch 36/64 loss: 1.6776528358459473
Batch 37/64 loss: 1.5578618049621582
Batch 38/64 loss: 1.6416568756103516
Batch 39/64 loss: 1.6405549049377441
Batch 40/64 loss: 1.4817423820495605
Batch 41/64 loss: 1.40325927734375
Batch 42/64 loss: 1.519437313079834
Batch 43/64 loss: 1.206108570098877
Batch 44/64 loss: 1.397066593170166
Batch 45/64 loss: 1.989941120147705
Batch 46/64 loss: 1.3274531364440918
Batch 47/64 loss: 1.57645845413208
Batch 48/64 loss: 1.3720002174377441
Batch 49/64 loss: 1.3169455528259277
Batch 50/64 loss: 1.1712961196899414
Batch 51/64 loss: 1.4989471435546875
Batch 52/64 loss: 1.558668613433838
Batch 53/64 loss: 1.5661983489990234
Batch 54/64 loss: 1.6790380477905273
Batch 55/64 loss: 1.5541009902954102
Batch 56/64 loss: 1.4796109199523926
Batch 57/64 loss: 1.0620689392089844
Batch 58/64 loss: 1.769731044769287
Batch 59/64 loss: 1.3458046913146973
Batch 60/64 loss: 1.5039000511169434
Batch 61/64 loss: 1.507007122039795
Batch 62/64 loss: 1.3399229049682617
Batch 63/64 loss: 1.1927990913391113
Batch 64/64 loss: -0.9768195152282715
Epoch 75  Train loss: 1.4941723973143335  Val loss: 1.368253386717072
Saving best model, epoch: 75
Epoch 76
-------------------------------
Batch 1/64 loss: 1.230269432067871
Batch 2/64 loss: 1.2528929710388184
Batch 3/64 loss: 1.459390640258789
Batch 4/64 loss: 1.4911537170410156
Batch 5/64 loss: 1.2397994995117188
Batch 6/64 loss: 1.2236394882202148
Batch 7/64 loss: 1.4545516967773438
Batch 8/64 loss: 1.512955665588379
Batch 9/64 loss: 1.042241096496582
Batch 10/64 loss: 1.5578117370605469
Batch 11/64 loss: 1.5212817192077637
Batch 12/64 loss: 1.2982144355773926
Batch 13/64 loss: 1.2576751708984375
Batch 14/64 loss: 1.461402416229248
Batch 15/64 loss: 1.4963254928588867
Batch 16/64 loss: 1.5975885391235352
Batch 17/64 loss: 1.5466008186340332
Batch 18/64 loss: 1.7436442375183105
Batch 19/64 loss: 1.166292667388916
Batch 20/64 loss: 1.5006685256958008
Batch 21/64 loss: 1.1130752563476562
Batch 22/64 loss: 1.4148612022399902
Batch 23/64 loss: 1.753129005432129
Batch 24/64 loss: 1.7019338607788086
Batch 25/64 loss: 1.317953109741211
Batch 26/64 loss: 1.7514095306396484
Batch 27/64 loss: 1.3852734565734863
Batch 28/64 loss: 1.2454462051391602
Batch 29/64 loss: 1.5448718070983887
Batch 30/64 loss: 1.3467488288879395
Batch 31/64 loss: 1.8493237495422363
Batch 32/64 loss: 1.2432169914245605
Batch 33/64 loss: 1.4046034812927246
Batch 34/64 loss: 1.2310657501220703
Batch 35/64 loss: 1.4564380645751953
Batch 36/64 loss: 1.3282513618469238
Batch 37/64 loss: 1.4612927436828613
Batch 38/64 loss: 1.4412932395935059
Batch 39/64 loss: 1.3562698364257812
Batch 40/64 loss: 1.9691095352172852
Batch 41/64 loss: 1.4573755264282227
Batch 42/64 loss: 1.119004249572754
Batch 43/64 loss: 1.450979232788086
Batch 44/64 loss: 1.3444218635559082
Batch 45/64 loss: 1.624831199645996
Batch 46/64 loss: 1.5640039443969727
Batch 47/64 loss: 1.1866817474365234
Batch 48/64 loss: 1.2181525230407715
Batch 49/64 loss: 1.3789095878601074
Batch 50/64 loss: 1.435819149017334
Batch 51/64 loss: 1.2559428215026855
Batch 52/64 loss: 1.2502293586730957
Batch 53/64 loss: 1.3926944732666016
Batch 54/64 loss: 1.509239673614502
Batch 55/64 loss: 1.2464032173156738
Batch 56/64 loss: 1.4351873397827148
Batch 57/64 loss: 1.4804210662841797
Batch 58/64 loss: 1.270164966583252
Batch 59/64 loss: 1.4986791610717773
Batch 60/64 loss: 1.3182697296142578
Batch 61/64 loss: 1.2360539436340332
Batch 62/64 loss: 1.983994483947754
Batch 63/64 loss: 1.1889567375183105
Batch 64/64 loss: -1.6349034309387207
Epoch 76  Train loss: 1.3802385161904727  Val loss: 1.2773014999337213
Saving best model, epoch: 76
Epoch 77
-------------------------------
Batch 1/64 loss: 1.473299503326416
Batch 2/64 loss: 1.4465103149414062
Batch 3/64 loss: 1.4152002334594727
Batch 4/64 loss: 1.3155179023742676
Batch 5/64 loss: 1.3097147941589355
Batch 6/64 loss: 1.3444876670837402
Batch 7/64 loss: 1.1485142707824707
Batch 8/64 loss: 1.1855878829956055
Batch 9/64 loss: 1.391892433166504
Batch 10/64 loss: 1.1224522590637207
Batch 11/64 loss: 1.246243953704834
Batch 12/64 loss: 1.3088479042053223
Batch 13/64 loss: 0.9607706069946289
Batch 14/64 loss: 1.6357536315917969
Batch 15/64 loss: 0.9357881546020508
Batch 16/64 loss: 1.6011338233947754
Batch 17/64 loss: 1.4355406761169434
Batch 18/64 loss: 1.396308422088623
Batch 19/64 loss: 1.5001039505004883
Batch 20/64 loss: 1.3907575607299805
Batch 21/64 loss: 1.3297014236450195
Batch 22/64 loss: 1.457183837890625
Batch 23/64 loss: 1.6378135681152344
Batch 24/64 loss: 1.4106478691101074
Batch 25/64 loss: 1.4558658599853516
Batch 26/64 loss: 1.431117057800293
Batch 27/64 loss: 1.48948335647583
Batch 28/64 loss: 1.3470816612243652
Batch 29/64 loss: 1.7538037300109863
Batch 30/64 loss: 1.5278663635253906
Batch 31/64 loss: 1.380843162536621
Batch 32/64 loss: 1.7094058990478516
Batch 33/64 loss: 1.122079849243164
Batch 34/64 loss: 1.4903454780578613
Batch 35/64 loss: 1.4716553688049316
Batch 36/64 loss: 1.4143176078796387
Batch 37/64 loss: 1.336991310119629
Batch 38/64 loss: 1.1652255058288574
Batch 39/64 loss: 1.552718162536621
Batch 40/64 loss: 1.5577387809753418
Batch 41/64 loss: 1.43021821975708
Batch 42/64 loss: 1.5519390106201172
Batch 43/64 loss: 1.399275302886963
Batch 44/64 loss: 1.0287189483642578
Batch 45/64 loss: 1.2640795707702637
Batch 46/64 loss: 1.157038688659668
Batch 47/64 loss: 1.4406275749206543
Batch 48/64 loss: 1.3149824142456055
Batch 49/64 loss: 1.515979290008545
Batch 50/64 loss: 1.24422025680542
Batch 51/64 loss: 1.3713593482971191
Batch 52/64 loss: 1.44962739944458
Batch 53/64 loss: 1.6632599830627441
Batch 54/64 loss: 1.3513307571411133
Batch 55/64 loss: 1.5495562553405762
Batch 56/64 loss: 2.176145553588867
Batch 57/64 loss: 1.2264065742492676
Batch 58/64 loss: 1.370340347290039
Batch 59/64 loss: 1.592287540435791
Batch 60/64 loss: 1.1108579635620117
Batch 61/64 loss: 1.6112895011901855
Batch 62/64 loss: 1.3927831649780273
Batch 63/64 loss: 1.0955705642700195
Batch 64/64 loss: -1.3427162170410156
Epoch 77  Train loss: 1.3632497076894723  Val loss: 1.4580759002580674
Epoch 78
-------------------------------
Batch 1/64 loss: 1.673774242401123
Batch 2/64 loss: 1.1719951629638672
Batch 3/64 loss: 1.6042184829711914
Batch 4/64 loss: 1.4434614181518555
Batch 5/64 loss: 1.5851140022277832
Batch 6/64 loss: 1.7282776832580566
Batch 7/64 loss: 1.283250331878662
Batch 8/64 loss: 1.4658541679382324
Batch 9/64 loss: 1.4694981575012207
Batch 10/64 loss: 1.36317777633667
Batch 11/64 loss: 1.5175437927246094
Batch 12/64 loss: 1.5496349334716797
Batch 13/64 loss: 1.3802547454833984
Batch 14/64 loss: 1.3758339881896973
Batch 15/64 loss: 1.370772361755371
Batch 16/64 loss: 1.2800955772399902
Batch 17/64 loss: 1.4501557350158691
Batch 18/64 loss: 1.1895394325256348
Batch 19/64 loss: 1.4060230255126953
Batch 20/64 loss: 2.346280574798584
Batch 21/64 loss: 1.6802220344543457
Batch 22/64 loss: 1.3871216773986816
Batch 23/64 loss: 1.299877643585205
Batch 24/64 loss: 1.139547348022461
Batch 25/64 loss: 1.3442530632019043
Batch 26/64 loss: 1.6294655799865723
Batch 27/64 loss: 1.5918536186218262
Batch 28/64 loss: 1.62677001953125
Batch 29/64 loss: 1.273836612701416
Batch 30/64 loss: 1.4341087341308594
Batch 31/64 loss: 1.5576000213623047
Batch 32/64 loss: 1.6474127769470215
Batch 33/64 loss: 1.2511272430419922
Batch 34/64 loss: 1.591714859008789
Batch 35/64 loss: 1.6983647346496582
Batch 36/64 loss: 1.234581470489502
Batch 37/64 loss: 1.7584714889526367
Batch 38/64 loss: 1.5822772979736328
Batch 39/64 loss: 1.3104510307312012
Batch 40/64 loss: 1.9005236625671387
Batch 41/64 loss: 1.6582832336425781
Batch 42/64 loss: 1.3331546783447266
Batch 43/64 loss: 1.421175479888916
Batch 44/64 loss: 1.2970890998840332
Batch 45/64 loss: 1.2972044944763184
Batch 46/64 loss: 1.6024165153503418
Batch 47/64 loss: 1.5096654891967773
Batch 48/64 loss: 2.0015854835510254
Batch 49/64 loss: 1.320981502532959
Batch 50/64 loss: 1.9239115715026855
Batch 51/64 loss: 1.2615113258361816
Batch 52/64 loss: 1.5082988739013672
Batch 53/64 loss: 1.6523652076721191
Batch 54/64 loss: 1.332249641418457
Batch 55/64 loss: 1.5841383934020996
Batch 56/64 loss: 1.237077236175537
Batch 57/64 loss: 1.5443477630615234
Batch 58/64 loss: 1.6874141693115234
Batch 59/64 loss: 1.5676116943359375
Batch 60/64 loss: 1.583756923675537
Batch 61/64 loss: 1.4442181587219238
Batch 62/64 loss: 1.3500142097473145
Batch 63/64 loss: 1.3872251510620117
Batch 64/64 loss: -1.704695701599121
Epoch 78  Train loss: 1.456023702434465  Val loss: 1.4231026377464897
Epoch 79
-------------------------------
Batch 1/64 loss: 1.222851276397705
Batch 2/64 loss: 1.1950268745422363
Batch 3/64 loss: 1.5599942207336426
Batch 4/64 loss: 1.2498321533203125
Batch 5/64 loss: 1.2388076782226562
Batch 6/64 loss: 1.709263801574707
Batch 7/64 loss: 1.2653274536132812
Batch 8/64 loss: 1.2546658515930176
Batch 9/64 loss: 1.505629062652588
Batch 10/64 loss: 1.1001920700073242
Batch 11/64 loss: 1.3981447219848633
Batch 12/64 loss: 1.7224740982055664
Batch 13/64 loss: 1.5250959396362305
Batch 14/64 loss: 1.1159253120422363
Batch 15/64 loss: 1.5293612480163574
Batch 16/64 loss: 1.2958049774169922
Batch 17/64 loss: 1.7576904296875
Batch 18/64 loss: 1.3280682563781738
Batch 19/64 loss: 1.3850955963134766
Batch 20/64 loss: 1.3675293922424316
Batch 21/64 loss: 1.491255760192871
Batch 22/64 loss: 1.3203010559082031
Batch 23/64 loss: 1.236534595489502
Batch 24/64 loss: 1.2442870140075684
Batch 25/64 loss: 1.2099032402038574
Batch 26/64 loss: 1.3561811447143555
Batch 27/64 loss: 1.4978623390197754
Batch 28/64 loss: 1.6587815284729004
Batch 29/64 loss: 1.3878626823425293
Batch 30/64 loss: 1.1959242820739746
Batch 31/64 loss: 1.1510272026062012
Batch 32/64 loss: 1.6211090087890625
Batch 33/64 loss: 1.4352412223815918
Batch 34/64 loss: 1.2658100128173828
Batch 35/64 loss: 1.2671613693237305
Batch 36/64 loss: 1.308056354522705
Batch 37/64 loss: 2.116447925567627
Batch 38/64 loss: 1.3389582633972168
Batch 39/64 loss: 1.403144359588623
Batch 40/64 loss: 1.4565439224243164
Batch 41/64 loss: 1.4603261947631836
Batch 42/64 loss: 1.3911309242248535
Batch 43/64 loss: 1.4501638412475586
Batch 44/64 loss: 1.5601024627685547
Batch 45/64 loss: 1.4245514869689941
Batch 46/64 loss: 1.6540608406066895
Batch 47/64 loss: 1.3190937042236328
Batch 48/64 loss: 1.1002230644226074
Batch 49/64 loss: 1.1409235000610352
Batch 50/64 loss: 1.3170232772827148
Batch 51/64 loss: 1.2659339904785156
Batch 52/64 loss: 0.91802978515625
Batch 53/64 loss: 1.4625978469848633
Batch 54/64 loss: 1.33837890625
Batch 55/64 loss: 1.599916934967041
Batch 56/64 loss: 1.5382394790649414
Batch 57/64 loss: 1.665822982788086
Batch 58/64 loss: 1.5773921012878418
Batch 59/64 loss: 1.6121349334716797
Batch 60/64 loss: 1.4196290969848633
Batch 61/64 loss: 1.3525848388671875
Batch 62/64 loss: 1.5383739471435547
Batch 63/64 loss: 1.2572174072265625
Batch 64/64 loss: -1.203005313873291
Epoch 79  Train loss: 1.3670709217295927  Val loss: 1.4896684037041419
Epoch 80
-------------------------------
Batch 1/64 loss: 1.2381277084350586
Batch 2/64 loss: 1.450681209564209
Batch 3/64 loss: 1.5919923782348633
Batch 4/64 loss: 1.419715404510498
Batch 5/64 loss: 1.2491822242736816
Batch 6/64 loss: 1.3607478141784668
Batch 7/64 loss: 1.1514949798583984
Batch 8/64 loss: 1.217012882232666
Batch 9/64 loss: 1.199124813079834
Batch 10/64 loss: 1.344423770904541
Batch 11/64 loss: 1.789808750152588
Batch 12/64 loss: 1.1069731712341309
Batch 13/64 loss: 1.3647356033325195
Batch 14/64 loss: 1.3319964408874512
Batch 15/64 loss: 1.3817472457885742
Batch 16/64 loss: 1.5159435272216797
Batch 17/64 loss: 1.3259596824645996
Batch 18/64 loss: 1.3399076461791992
Batch 19/64 loss: 1.1370720863342285
Batch 20/64 loss: 1.5040035247802734
Batch 21/64 loss: 1.4342570304870605
Batch 22/64 loss: 1.3763747215270996
Batch 23/64 loss: 1.2358441352844238
Batch 24/64 loss: 1.3973608016967773
Batch 25/64 loss: 1.5502395629882812
Batch 26/64 loss: 1.3145771026611328
Batch 27/64 loss: 1.4540882110595703
Batch 28/64 loss: 1.2411293983459473
Batch 29/64 loss: 1.2746963500976562
Batch 30/64 loss: 1.5890402793884277
Batch 31/64 loss: 1.087890625
Batch 32/64 loss: 1.1900010108947754
Batch 33/64 loss: 1.5778579711914062
Batch 34/64 loss: 1.5589814186096191
Batch 35/64 loss: 1.4393925666809082
Batch 36/64 loss: 1.3406767845153809
Batch 37/64 loss: 1.3869147300720215
Batch 38/64 loss: 1.447791576385498
Batch 39/64 loss: 1.3877315521240234
Batch 40/64 loss: 1.387336254119873
Batch 41/64 loss: 1.249910831451416
Batch 42/64 loss: 1.7640862464904785
Batch 43/64 loss: 1.504587173461914
Batch 44/64 loss: 1.582575798034668
Batch 45/64 loss: 1.4899201393127441
Batch 46/64 loss: 1.440007209777832
Batch 47/64 loss: 1.3802833557128906
Batch 48/64 loss: 1.3419175148010254
Batch 49/64 loss: 1.3380403518676758
Batch 50/64 loss: 1.10758638381958
Batch 51/64 loss: 1.2134742736816406
Batch 52/64 loss: 1.2558584213256836
Batch 53/64 loss: 1.490778923034668
Batch 54/64 loss: 1.287435531616211
Batch 55/64 loss: 1.3414835929870605
Batch 56/64 loss: 1.2220849990844727
Batch 57/64 loss: 1.3795170783996582
Batch 58/64 loss: 1.5466008186340332
Batch 59/64 loss: 1.4889321327209473
Batch 60/64 loss: 1.535184383392334
Batch 61/64 loss: 1.3802757263183594
Batch 62/64 loss: 1.5546598434448242
Batch 63/64 loss: 1.5757293701171875
Batch 64/64 loss: -1.4769001007080078
Epoch 80  Train loss: 1.349899419148763  Val loss: 1.3748323106274163
Epoch 81
-------------------------------
Batch 1/64 loss: 1.4723949432373047
Batch 2/64 loss: 1.2209820747375488
Batch 3/64 loss: 1.0983037948608398
Batch 4/64 loss: 1.3672847747802734
Batch 5/64 loss: 1.2866144180297852
Batch 6/64 loss: 1.060375690460205
Batch 7/64 loss: 1.3486747741699219
Batch 8/64 loss: 1.33917236328125
Batch 9/64 loss: 1.5267024040222168
Batch 10/64 loss: 1.1248855590820312
Batch 11/64 loss: 1.3083853721618652
Batch 12/64 loss: 1.6850132942199707
Batch 13/64 loss: 1.1648564338684082
Batch 14/64 loss: 1.3264918327331543
Batch 15/64 loss: 1.1562628746032715
Batch 16/64 loss: 1.2969179153442383
Batch 17/64 loss: 1.2021756172180176
Batch 18/64 loss: 1.1721668243408203
Batch 19/64 loss: 1.129971981048584
Batch 20/64 loss: 1.4480805397033691
Batch 21/64 loss: 1.2139053344726562
Batch 22/64 loss: 1.6147007942199707
Batch 23/64 loss: 1.1804089546203613
Batch 24/64 loss: 1.2643871307373047
Batch 25/64 loss: 1.2439522743225098
Batch 26/64 loss: 1.1349644660949707
Batch 27/64 loss: 1.4355888366699219
Batch 28/64 loss: 1.3931736946105957
Batch 29/64 loss: 1.274550437927246
Batch 30/64 loss: 1.6351008415222168
Batch 31/64 loss: 1.4019694328308105
Batch 32/64 loss: 1.5601654052734375
Batch 33/64 loss: 1.4007196426391602
Batch 34/64 loss: 1.40576171875
Batch 35/64 loss: 1.4656891822814941
Batch 36/64 loss: 1.2871341705322266
Batch 37/64 loss: 1.4110760688781738
Batch 38/64 loss: 1.5496015548706055
Batch 39/64 loss: 1.451150894165039
Batch 40/64 loss: 1.3671073913574219
Batch 41/64 loss: 1.3517475128173828
Batch 42/64 loss: 1.4452743530273438
Batch 43/64 loss: 1.4957094192504883
Batch 44/64 loss: 1.5882225036621094
Batch 45/64 loss: 1.5607962608337402
Batch 46/64 loss: 1.0703697204589844
Batch 47/64 loss: 1.4301023483276367
Batch 48/64 loss: 1.5737080574035645
Batch 49/64 loss: 1.5482220649719238
Batch 50/64 loss: 1.6191558837890625
Batch 51/64 loss: 1.305551528930664
Batch 52/64 loss: 1.2563848495483398
Batch 53/64 loss: 1.447697639465332
Batch 54/64 loss: 1.3192086219787598
Batch 55/64 loss: 1.5076055526733398
Batch 56/64 loss: 1.4865403175354004
Batch 57/64 loss: 1.3822126388549805
Batch 58/64 loss: 1.3198533058166504
Batch 59/64 loss: 1.090226650238037
Batch 60/64 loss: 1.4198565483093262
Batch 61/64 loss: 1.3720803260803223
Batch 62/64 loss: 1.0603089332580566
Batch 63/64 loss: 1.6343107223510742
Batch 64/64 loss: -1.322828769683838
Epoch 81  Train loss: 1.3289387590744917  Val loss: 1.3724364906651867
Epoch 82
-------------------------------
Batch 1/64 loss: 1.6155142784118652
Batch 2/64 loss: 1.3846168518066406
Batch 3/64 loss: 1.389951229095459
Batch 4/64 loss: 1.1227293014526367
Batch 5/64 loss: 0.9322147369384766
Batch 6/64 loss: 2.3587164878845215
Batch 7/64 loss: 1.2097454071044922
Batch 8/64 loss: 1.9099478721618652
Batch 9/64 loss: 1.6024608612060547
Batch 10/64 loss: 1.688199520111084
Batch 11/64 loss: 1.51224946975708
Batch 12/64 loss: 1.6098670959472656
Batch 13/64 loss: 1.3935751914978027
Batch 14/64 loss: 1.5963101387023926
Batch 15/64 loss: 1.5623087882995605
Batch 16/64 loss: 1.8100886344909668
Batch 17/64 loss: 1.406297206878662
Batch 18/64 loss: 1.6368565559387207
Batch 19/64 loss: 1.9553899765014648
Batch 20/64 loss: 1.896604061126709
Batch 21/64 loss: 1.4954915046691895
Batch 22/64 loss: 1.1322498321533203
Batch 23/64 loss: 1.5189852714538574
Batch 24/64 loss: 1.839360237121582
Batch 25/64 loss: 1.312253475189209
Batch 26/64 loss: 1.4389557838439941
Batch 27/64 loss: 1.6557092666625977
Batch 28/64 loss: 1.7598609924316406
Batch 29/64 loss: 1.3923239707946777
Batch 30/64 loss: 1.3683018684387207
Batch 31/64 loss: 1.243332862854004
Batch 32/64 loss: 1.5665273666381836
Batch 33/64 loss: 1.3595819473266602
Batch 34/64 loss: 1.5150041580200195
Batch 35/64 loss: 1.640392780303955
Batch 36/64 loss: 1.0096120834350586
Batch 37/64 loss: 1.3994865417480469
Batch 38/64 loss: 1.6903305053710938
Batch 39/64 loss: 1.2130107879638672
Batch 40/64 loss: 1.6886906623840332
Batch 41/64 loss: 1.605687141418457
Batch 42/64 loss: 1.5257740020751953
Batch 43/64 loss: 1.1326303482055664
Batch 44/64 loss: 1.250298023223877
Batch 45/64 loss: 1.2368602752685547
Batch 46/64 loss: 1.7322206497192383
Batch 47/64 loss: 1.5865106582641602
Batch 48/64 loss: 1.2382044792175293
Batch 49/64 loss: 1.1876020431518555
Batch 50/64 loss: 1.2727923393249512
Batch 51/64 loss: 1.5293354988098145
Batch 52/64 loss: 1.1984095573425293
Batch 53/64 loss: 2.0115699768066406
Batch 54/64 loss: 1.1239638328552246
Batch 55/64 loss: 1.1684603691101074
Batch 56/64 loss: 1.125317096710205
Batch 57/64 loss: 1.5102205276489258
Batch 58/64 loss: 1.6493830680847168
Batch 59/64 loss: 1.5918388366699219
Batch 60/64 loss: 1.1998744010925293
Batch 61/64 loss: 1.5829696655273438
Batch 62/64 loss: 1.7071237564086914
Batch 63/64 loss: 1.4491419792175293
Batch 64/64 loss: -1.7736396789550781
Epoch 82  Train loss: 1.4450049306832107  Val loss: 1.3504060830447273
Epoch 83
-------------------------------
Batch 1/64 loss: 1.462646484375
Batch 2/64 loss: 1.43603515625
Batch 3/64 loss: 1.591158390045166
Batch 4/64 loss: 1.994363784790039
Batch 5/64 loss: 1.2541484832763672
Batch 6/64 loss: 1.070481300354004
Batch 7/64 loss: 1.5968961715698242
Batch 8/64 loss: 1.2440075874328613
Batch 9/64 loss: 1.665590763092041
Batch 10/64 loss: 1.6148090362548828
Batch 11/64 loss: 1.4814915657043457
Batch 12/64 loss: 1.6025934219360352
Batch 13/64 loss: 1.6644988059997559
Batch 14/64 loss: 1.4092750549316406
Batch 15/64 loss: 1.2554526329040527
Batch 16/64 loss: 1.5750370025634766
Batch 17/64 loss: 1.3443188667297363
Batch 18/64 loss: 1.1937098503112793
Batch 19/64 loss: 1.4619321823120117
Batch 20/64 loss: 1.467787265777588
Batch 21/64 loss: 1.0356783866882324
Batch 22/64 loss: 1.4081792831420898
Batch 23/64 loss: 1.6001825332641602
Batch 24/64 loss: 0.9991850852966309
Batch 25/64 loss: 2.1460299491882324
Batch 26/64 loss: 1.3980731964111328
Batch 27/64 loss: 1.3758635520935059
Batch 28/64 loss: 1.795095443725586
Batch 29/64 loss: 1.0657343864440918
Batch 30/64 loss: 1.3304972648620605
Batch 31/64 loss: 1.428581714630127
Batch 32/64 loss: 1.3788652420043945
Batch 33/64 loss: 1.4079771041870117
Batch 34/64 loss: 1.2480106353759766
Batch 35/64 loss: 1.3668136596679688
Batch 36/64 loss: 1.6343417167663574
Batch 37/64 loss: 1.3348603248596191
Batch 38/64 loss: 1.2190866470336914
Batch 39/64 loss: 1.182833194732666
Batch 40/64 loss: 1.1305036544799805
Batch 41/64 loss: 1.3805599212646484
Batch 42/64 loss: 1.1171646118164062
Batch 43/64 loss: 1.283036708831787
Batch 44/64 loss: 1.3657183647155762
Batch 45/64 loss: 1.448523998260498
Batch 46/64 loss: 1.288130283355713
Batch 47/64 loss: 1.5847654342651367
Batch 48/64 loss: 1.4405269622802734
Batch 49/64 loss: 1.2958507537841797
Batch 50/64 loss: 1.4804773330688477
Batch 51/64 loss: 1.1136832237243652
Batch 52/64 loss: 1.632558822631836
Batch 53/64 loss: 1.3767809867858887
Batch 54/64 loss: 1.5361580848693848
Batch 55/64 loss: 1.363630771636963
Batch 56/64 loss: 1.4826745986938477
Batch 57/64 loss: 1.3825159072875977
Batch 58/64 loss: 1.4883885383605957
Batch 59/64 loss: 1.196394920349121
Batch 60/64 loss: 1.5413932800292969
Batch 61/64 loss: 1.5000057220458984
Batch 62/64 loss: 1.3914060592651367
Batch 63/64 loss: 1.4881114959716797
Batch 64/64 loss: -1.4889254570007324
Epoch 83  Train loss: 1.3793629721099254  Val loss: 1.3207497482037627
Epoch 84
-------------------------------
Batch 1/64 loss: 1.7597942352294922
Batch 2/64 loss: 1.9204249382019043
Batch 3/64 loss: 1.2591919898986816
Batch 4/64 loss: 1.3654913902282715
Batch 5/64 loss: 1.6784777641296387
Batch 6/64 loss: 1.5155129432678223
Batch 7/64 loss: 1.0476737022399902
Batch 8/64 loss: 1.3158893585205078
Batch 9/64 loss: 1.468836784362793
Batch 10/64 loss: 1.3271980285644531
Batch 11/64 loss: 1.1207971572875977
Batch 12/64 loss: 1.1043848991394043
Batch 13/64 loss: 1.277310848236084
Batch 14/64 loss: 1.014880657196045
Batch 15/64 loss: 1.114835262298584
Batch 16/64 loss: 1.3846569061279297
Batch 17/64 loss: 1.4894800186157227
Batch 18/64 loss: 1.1178460121154785
Batch 19/64 loss: 1.320986270904541
Batch 20/64 loss: 1.1046624183654785
Batch 21/64 loss: 1.257655143737793
Batch 22/64 loss: 1.1267809867858887
Batch 23/64 loss: 1.5411367416381836
Batch 24/64 loss: 1.6624579429626465
Batch 25/64 loss: 1.3566770553588867
Batch 26/64 loss: 1.511070728302002
Batch 27/64 loss: 1.4352269172668457
Batch 28/64 loss: 1.0656304359436035
Batch 29/64 loss: 1.4328227043151855
Batch 30/64 loss: 1.38258695602417
Batch 31/64 loss: 1.2124295234680176
Batch 32/64 loss: 1.401179313659668
Batch 33/64 loss: 1.3541851043701172
Batch 34/64 loss: 1.4306621551513672
Batch 35/64 loss: 1.4224166870117188
Batch 36/64 loss: 1.1624770164489746
Batch 37/64 loss: 1.1279206275939941
Batch 38/64 loss: 1.0901875495910645
Batch 39/64 loss: 1.6171660423278809
Batch 40/64 loss: 1.304670810699463
Batch 41/64 loss: 1.2440195083618164
Batch 42/64 loss: 0.950953483581543
Batch 43/64 loss: 1.2552051544189453
Batch 44/64 loss: 1.492992877960205
Batch 45/64 loss: 1.1237359046936035
Batch 46/64 loss: 1.2850408554077148
Batch 47/64 loss: 1.3319525718688965
Batch 48/64 loss: 1.417060375213623
Batch 49/64 loss: 1.0723228454589844
Batch 50/64 loss: 1.2125754356384277
Batch 51/64 loss: 1.3900318145751953
Batch 52/64 loss: 1.0932326316833496
Batch 53/64 loss: 1.5047039985656738
Batch 54/64 loss: 1.117114543914795
Batch 55/64 loss: 1.0541863441467285
Batch 56/64 loss: 1.4202866554260254
Batch 57/64 loss: 1.4344544410705566
Batch 58/64 loss: 1.288074016571045
Batch 59/64 loss: 1.8383936882019043
Batch 60/64 loss: 1.312166690826416
Batch 61/64 loss: 1.0708584785461426
Batch 62/64 loss: 1.4219202995300293
Batch 63/64 loss: 1.4137659072875977
Batch 64/64 loss: -1.3462085723876953
Epoch 84  Train loss: 1.2915931626862172  Val loss: 1.2291659456757746
Saving best model, epoch: 84
Epoch 85
-------------------------------
Batch 1/64 loss: 1.4696078300476074
Batch 2/64 loss: 1.5167756080627441
Batch 3/64 loss: 0.9714527130126953
Batch 4/64 loss: 1.285273551940918
Batch 5/64 loss: 1.3308382034301758
Batch 6/64 loss: 1.457895278930664
Batch 7/64 loss: 1.188420295715332
Batch 8/64 loss: 1.1935067176818848
Batch 9/64 loss: 1.3829622268676758
Batch 10/64 loss: 1.226140022277832
Batch 11/64 loss: 1.0800976753234863
Batch 12/64 loss: 1.4500484466552734
Batch 13/64 loss: 1.146796703338623
Batch 14/64 loss: 1.2064151763916016
Batch 15/64 loss: 1.2419414520263672
Batch 16/64 loss: 1.0467872619628906
Batch 17/64 loss: 1.3041119575500488
Batch 18/64 loss: 1.5137434005737305
Batch 19/64 loss: 1.496777057647705
Batch 20/64 loss: 1.332838535308838
Batch 21/64 loss: 1.6978049278259277
Batch 22/64 loss: 1.6001710891723633
Batch 23/64 loss: 1.3443689346313477
Batch 24/64 loss: 1.4057707786560059
Batch 25/64 loss: 1.154524803161621
Batch 26/64 loss: 1.257504940032959
Batch 27/64 loss: 1.6108946800231934
Batch 28/64 loss: 1.371100902557373
Batch 29/64 loss: 1.2769761085510254
Batch 30/64 loss: 1.4879374504089355
Batch 31/64 loss: 1.5425543785095215
Batch 32/64 loss: 1.1854548454284668
Batch 33/64 loss: 1.0286340713500977
Batch 34/64 loss: 1.2730531692504883
Batch 35/64 loss: 1.3283486366271973
Batch 36/64 loss: 1.3876609802246094
Batch 37/64 loss: 1.0196008682250977
Batch 38/64 loss: 1.5031843185424805
Batch 39/64 loss: 1.3119492530822754
Batch 40/64 loss: 1.2857189178466797
Batch 41/64 loss: 1.433448314666748
Batch 42/64 loss: 1.0153427124023438
Batch 43/64 loss: 1.124673843383789
Batch 44/64 loss: 1.70707368850708
Batch 45/64 loss: 1.1772923469543457
Batch 46/64 loss: 1.1506199836730957
Batch 47/64 loss: 1.3724637031555176
Batch 48/64 loss: 1.5341682434082031
Batch 49/64 loss: 1.416574478149414
Batch 50/64 loss: 1.319779872894287
Batch 51/64 loss: 1.4010562896728516
Batch 52/64 loss: 1.3363037109375
Batch 53/64 loss: 1.223132610321045
Batch 54/64 loss: 1.0309696197509766
Batch 55/64 loss: 1.2240028381347656
Batch 56/64 loss: 1.2400712966918945
Batch 57/64 loss: 1.0958147048950195
Batch 58/64 loss: 1.1804637908935547
Batch 59/64 loss: 1.3076086044311523
Batch 60/64 loss: 1.1300740242004395
Batch 61/64 loss: 1.216965675354004
Batch 62/64 loss: 1.158292293548584
Batch 63/64 loss: 1.460953712463379
Batch 64/64 loss: -1.8300275802612305
Epoch 85  Train loss: 1.267455213210162  Val loss: 1.2506611093213058
Epoch 86
-------------------------------
Batch 1/64 loss: 1.1402039527893066
Batch 2/64 loss: 1.2448434829711914
Batch 3/64 loss: 1.4306306838989258
Batch 4/64 loss: 1.3541793823242188
Batch 5/64 loss: 1.5170478820800781
Batch 6/64 loss: 1.7481498718261719
Batch 7/64 loss: 1.7107563018798828
Batch 8/64 loss: 1.0996580123901367
Batch 9/64 loss: 1.4262399673461914
Batch 10/64 loss: 1.4828920364379883
Batch 11/64 loss: 1.6298279762268066
Batch 12/64 loss: 1.4632363319396973
Batch 13/64 loss: 1.4020662307739258
Batch 14/64 loss: 1.1343059539794922
Batch 15/64 loss: 1.6348662376403809
Batch 16/64 loss: 1.1443758010864258
Batch 17/64 loss: 1.4636116027832031
Batch 18/64 loss: 1.0673065185546875
Batch 19/64 loss: 1.2269697189331055
Batch 20/64 loss: 1.6387310028076172
Batch 21/64 loss: 1.265359878540039
Batch 22/64 loss: 1.0310478210449219
Batch 23/64 loss: 1.3614435195922852
Batch 24/64 loss: 1.0249528884887695
Batch 25/64 loss: 0.926605224609375
Batch 26/64 loss: 1.3911242485046387
Batch 27/64 loss: 1.2828073501586914
Batch 28/64 loss: 0.8677034378051758
Batch 29/64 loss: 1.3364973068237305
Batch 30/64 loss: 1.471785545349121
Batch 31/64 loss: 0.9944744110107422
Batch 32/64 loss: 1.2316746711730957
Batch 33/64 loss: 1.550004482269287
Batch 34/64 loss: 1.144331455230713
Batch 35/64 loss: 1.3228893280029297
Batch 36/64 loss: 1.4825406074523926
Batch 37/64 loss: 1.3797454833984375
Batch 38/64 loss: 1.305429458618164
Batch 39/64 loss: 0.8669528961181641
Batch 40/64 loss: 1.4720478057861328
Batch 41/64 loss: 1.7145395278930664
Batch 42/64 loss: 1.4637670516967773
Batch 43/64 loss: 1.218942642211914
Batch 44/64 loss: 1.3395743370056152
Batch 45/64 loss: 1.5782032012939453
Batch 46/64 loss: 1.2851014137268066
Batch 47/64 loss: 0.9992480278015137
Batch 48/64 loss: 1.3185076713562012
Batch 49/64 loss: 1.5784764289855957
Batch 50/64 loss: 1.1205859184265137
Batch 51/64 loss: 1.7200407981872559
Batch 52/64 loss: 1.2610077857971191
Batch 53/64 loss: 1.3309640884399414
Batch 54/64 loss: 1.2220897674560547
Batch 55/64 loss: 0.8120975494384766
Batch 56/64 loss: 1.1653351783752441
Batch 57/64 loss: 0.9870076179504395
Batch 58/64 loss: 1.3766980171203613
Batch 59/64 loss: 1.4015893936157227
Batch 60/64 loss: 1.0232057571411133
Batch 61/64 loss: 1.3833260536193848
Batch 62/64 loss: 1.8435521125793457
Batch 63/64 loss: 1.3512663841247559
Batch 64/64 loss: -2.3811721801757812
Epoch 86  Train loss: 1.2765264997295305  Val loss: 1.2902135881771337
Epoch 87
-------------------------------
Batch 1/64 loss: 1.1068205833435059
Batch 2/64 loss: 1.1452641487121582
Batch 3/64 loss: 1.5004000663757324
Batch 4/64 loss: 1.4689240455627441
Batch 5/64 loss: 1.1812310218811035
Batch 6/64 loss: 1.5416855812072754
Batch 7/64 loss: 1.2404274940490723
Batch 8/64 loss: 1.4690256118774414
Batch 9/64 loss: 1.0240583419799805
Batch 10/64 loss: 1.2543601989746094
Batch 11/64 loss: 1.3127999305725098
Batch 12/64 loss: 1.1345996856689453
Batch 13/64 loss: 1.3449177742004395
Batch 14/64 loss: 0.9691448211669922
Batch 15/64 loss: 1.4996047019958496
Batch 16/64 loss: 1.4457993507385254
Batch 17/64 loss: 1.1630449295043945
Batch 18/64 loss: 1.5349845886230469
Batch 19/64 loss: 0.9780192375183105
Batch 20/64 loss: 1.224207878112793
Batch 21/64 loss: 1.176999568939209
Batch 22/64 loss: 1.4804115295410156
Batch 23/64 loss: 1.0198779106140137
Batch 24/64 loss: 1.5720806121826172
Batch 25/64 loss: 1.1040000915527344
Batch 26/64 loss: 0.9134860038757324
Batch 27/64 loss: 1.1637229919433594
Batch 28/64 loss: 0.8674993515014648
Batch 29/64 loss: 1.388444423675537
Batch 30/64 loss: 1.4086780548095703
Batch 31/64 loss: 1.5595731735229492
Batch 32/64 loss: 1.6559133529663086
Batch 33/64 loss: 0.9238691329956055
Batch 34/64 loss: 1.2185249328613281
Batch 35/64 loss: 1.1824226379394531
Batch 36/64 loss: 1.3887758255004883
Batch 37/64 loss: 1.2930212020874023
Batch 38/64 loss: 1.4812984466552734
Batch 39/64 loss: 1.2125744819641113
Batch 40/64 loss: 1.2401752471923828
Batch 41/64 loss: 1.2839665412902832
Batch 42/64 loss: 1.2452301979064941
Batch 43/64 loss: 1.1299409866333008
Batch 44/64 loss: 1.3085861206054688
Batch 45/64 loss: 1.2515931129455566
Batch 46/64 loss: 1.3627257347106934
Batch 47/64 loss: 1.1360926628112793
Batch 48/64 loss: 1.3484692573547363
Batch 49/64 loss: 1.11350679397583
Batch 50/64 loss: 1.1347136497497559
Batch 51/64 loss: 1.4923286437988281
Batch 52/64 loss: 1.5392265319824219
Batch 53/64 loss: 1.3958854675292969
Batch 54/64 loss: 1.5027337074279785
Batch 55/64 loss: 1.4245247840881348
Batch 56/64 loss: 1.127610206604004
Batch 57/64 loss: 1.019392490386963
Batch 58/64 loss: 1.052213191986084
Batch 59/64 loss: 1.51271390914917
Batch 60/64 loss: 1.4847908020019531
Batch 61/64 loss: 1.5893964767456055
Batch 62/64 loss: 1.251457691192627
Batch 63/64 loss: 1.3745250701904297
Batch 64/64 loss: -2.279264450073242
Epoch 87  Train loss: 1.241770112280752  Val loss: 1.3263960703951387
Epoch 88
-------------------------------
Batch 1/64 loss: 1.1468005180358887
Batch 2/64 loss: 1.3920154571533203
Batch 3/64 loss: 1.8753938674926758
Batch 4/64 loss: 1.199653148651123
Batch 5/64 loss: 1.3683581352233887
Batch 6/64 loss: 1.2234845161437988
Batch 7/64 loss: 1.114417552947998
Batch 8/64 loss: 1.3711910247802734
Batch 9/64 loss: 1.2257957458496094
Batch 10/64 loss: 1.2373099327087402
Batch 11/64 loss: 1.446375846862793
Batch 12/64 loss: 1.286041259765625
Batch 13/64 loss: 1.412435531616211
Batch 14/64 loss: 1.1701202392578125
Batch 15/64 loss: 1.2306809425354004
Batch 16/64 loss: 1.1709718704223633
Batch 17/64 loss: 1.2816290855407715
Batch 18/64 loss: 1.4650731086730957
Batch 19/64 loss: 1.2700562477111816
Batch 20/64 loss: 1.1796331405639648
Batch 21/64 loss: 1.1085448265075684
Batch 22/64 loss: 1.2782130241394043
Batch 23/64 loss: 1.0881738662719727
Batch 24/64 loss: 1.6157550811767578
Batch 25/64 loss: 1.6211657524108887
Batch 26/64 loss: 1.1260933876037598
Batch 27/64 loss: 1.397087574005127
Batch 28/64 loss: 1.2421073913574219
Batch 29/64 loss: 1.1823601722717285
Batch 30/64 loss: 1.5272274017333984
Batch 31/64 loss: 1.0752558708190918
Batch 32/64 loss: 1.2470107078552246
Batch 33/64 loss: 1.3430256843566895
Batch 34/64 loss: 1.113471508026123
Batch 35/64 loss: 1.3901352882385254
Batch 36/64 loss: 1.3248438835144043
Batch 37/64 loss: 1.266840934753418
Batch 38/64 loss: 1.234206199645996
Batch 39/64 loss: 1.7340517044067383
Batch 40/64 loss: 1.3515257835388184
Batch 41/64 loss: 1.0877413749694824
Batch 42/64 loss: 1.2541747093200684
Batch 43/64 loss: 1.5395960807800293
Batch 44/64 loss: 1.4304962158203125
Batch 45/64 loss: 1.2361736297607422
Batch 46/64 loss: 1.2343125343322754
Batch 47/64 loss: 1.4386696815490723
Batch 48/64 loss: 1.4307622909545898
Batch 49/64 loss: 1.3899116516113281
Batch 50/64 loss: 1.3071260452270508
Batch 51/64 loss: 1.1984601020812988
Batch 52/64 loss: 1.3534207344055176
Batch 53/64 loss: 1.185403823852539
Batch 54/64 loss: 1.4667601585388184
Batch 55/64 loss: 1.599421501159668
Batch 56/64 loss: 1.2443046569824219
Batch 57/64 loss: 1.1715521812438965
Batch 58/64 loss: 1.2855043411254883
Batch 59/64 loss: 1.310053825378418
Batch 60/64 loss: 1.2635498046875
Batch 61/64 loss: 1.5222063064575195
Batch 62/64 loss: 1.3546299934387207
Batch 63/64 loss: 0.8228425979614258
Batch 64/64 loss: -1.8061237335205078
Epoch 88  Train loss: 1.2722668965657553  Val loss: 1.2604549185107254
Epoch 89
-------------------------------
Batch 1/64 loss: 1.461747646331787
Batch 2/64 loss: 0.8770565986633301
Batch 3/64 loss: 1.184786319732666
Batch 4/64 loss: 1.1913394927978516
Batch 5/64 loss: 1.9477953910827637
Batch 6/64 loss: 1.1838183403015137
Batch 7/64 loss: 1.1839566230773926
Batch 8/64 loss: 1.2723431587219238
Batch 9/64 loss: 1.189953327178955
Batch 10/64 loss: 1.648259162902832
Batch 11/64 loss: 1.2246594429016113
Batch 12/64 loss: 1.3188962936401367
Batch 13/64 loss: 1.4870367050170898
Batch 14/64 loss: 1.290576457977295
Batch 15/64 loss: 1.3743896484375
Batch 16/64 loss: 1.235234260559082
Batch 17/64 loss: 1.1458945274353027
Batch 18/64 loss: 1.1545562744140625
Batch 19/64 loss: 1.2844552993774414
Batch 20/64 loss: 1.2268753051757812
Batch 21/64 loss: 1.2714109420776367
Batch 22/64 loss: 1.0780959129333496
Batch 23/64 loss: 1.2768254280090332
Batch 24/64 loss: 1.306617259979248
Batch 25/64 loss: 1.0540008544921875
Batch 26/64 loss: 1.2728538513183594
Batch 27/64 loss: 1.157754898071289
Batch 28/64 loss: 1.1272215843200684
Batch 29/64 loss: 1.6924958229064941
Batch 30/64 loss: 0.9682745933532715
Batch 31/64 loss: 1.1333770751953125
Batch 32/64 loss: 1.3703765869140625
Batch 33/64 loss: 1.2089881896972656
Batch 34/64 loss: 1.2572169303894043
Batch 35/64 loss: 1.392268180847168
Batch 36/64 loss: 1.2072057723999023
Batch 37/64 loss: 1.328582763671875
Batch 38/64 loss: 1.2675871849060059
Batch 39/64 loss: 1.2878398895263672
Batch 40/64 loss: 1.038668155670166
Batch 41/64 loss: 1.0292863845825195
Batch 42/64 loss: 1.015176773071289
Batch 43/64 loss: 1.2511286735534668
Batch 44/64 loss: 1.029003620147705
Batch 45/64 loss: 1.7377524375915527
Batch 46/64 loss: 1.3995776176452637
Batch 47/64 loss: 0.9094133377075195
Batch 48/64 loss: 1.3622827529907227
Batch 49/64 loss: 1.3582377433776855
Batch 50/64 loss: 1.1067218780517578
Batch 51/64 loss: 1.5624561309814453
Batch 52/64 loss: 1.5857295989990234
Batch 53/64 loss: 1.070876121520996
Batch 54/64 loss: 1.1842904090881348
Batch 55/64 loss: 1.1299409866333008
Batch 56/64 loss: 1.1291217803955078
Batch 57/64 loss: 1.2926626205444336
Batch 58/64 loss: 1.03346586227417
Batch 59/64 loss: 1.0237131118774414
Batch 60/64 loss: 1.0208792686462402
Batch 61/64 loss: 1.354987621307373
Batch 62/64 loss: 1.3118414878845215
Batch 63/64 loss: 1.1291847229003906
Batch 64/64 loss: -1.6636924743652344
Epoch 89  Train loss: 1.2135098625631893  Val loss: 1.3995959095119201
Epoch 90
-------------------------------
Batch 1/64 loss: 1.5188202857971191
Batch 2/64 loss: 1.3533501625061035
Batch 3/64 loss: 1.110541820526123
Batch 4/64 loss: 1.372382640838623
Batch 5/64 loss: 1.0903902053833008
Batch 6/64 loss: 1.0626616477966309
Batch 7/64 loss: 1.0202560424804688
Batch 8/64 loss: 3.7106595039367676
Batch 9/64 loss: 0.7822780609130859
Batch 10/64 loss: 1.154557704925537
Batch 11/64 loss: 1.0630569458007812
Batch 12/64 loss: 1.782404899597168
Batch 13/64 loss: 1.2209019660949707
Batch 14/64 loss: 1.3609004020690918
Batch 15/64 loss: 1.1310091018676758
Batch 16/64 loss: 1.4562005996704102
Batch 17/64 loss: 1.0644149780273438
Batch 18/64 loss: 0.9822025299072266
Batch 19/64 loss: 1.1429719924926758
Batch 20/64 loss: 1.3770432472229004
Batch 21/64 loss: 0.9854793548583984
Batch 22/64 loss: 0.9598021507263184
Batch 23/64 loss: 1.493567943572998
Batch 24/64 loss: 1.534247875213623
Batch 25/64 loss: 1.2861065864562988
Batch 26/64 loss: 1.3136911392211914
Batch 27/64 loss: 1.198533058166504
Batch 28/64 loss: 1.1288104057312012
Batch 29/64 loss: 2.3117103576660156
Batch 30/64 loss: 1.0253806114196777
Batch 31/64 loss: 1.4518132209777832
Batch 32/64 loss: 0.992490291595459
Batch 33/64 loss: 1.1931233406066895
Batch 34/64 loss: 1.376417636871338
Batch 35/64 loss: 1.4202017784118652
Batch 36/64 loss: 1.1043376922607422
Batch 37/64 loss: 1.867532730102539
Batch 38/64 loss: 1.1791529655456543
Batch 39/64 loss: 1.7885761260986328
Batch 40/64 loss: 1.3995380401611328
Batch 41/64 loss: 2.074751853942871
Batch 42/64 loss: 1.6771860122680664
Batch 43/64 loss: 1.5461039543151855
Batch 44/64 loss: 1.4182419776916504
Batch 45/64 loss: 1.1983375549316406
Batch 46/64 loss: 1.5674428939819336
Batch 47/64 loss: 1.430677890777588
Batch 48/64 loss: 1.3807487487792969
Batch 49/64 loss: 1.3699703216552734
Batch 50/64 loss: 1.0825114250183105
Batch 51/64 loss: 1.5333333015441895
Batch 52/64 loss: 1.6395020484924316
Batch 53/64 loss: 1.417243480682373
Batch 54/64 loss: 1.3924980163574219
Batch 55/64 loss: 1.477299690246582
Batch 56/64 loss: 1.2575359344482422
Batch 57/64 loss: 1.2493305206298828
Batch 58/64 loss: 1.7423038482666016
Batch 59/64 loss: 1.6390466690063477
Batch 60/64 loss: 1.2198333740234375
Batch 61/64 loss: 1.4266023635864258
Batch 62/64 loss: 1.5011253356933594
Batch 63/64 loss: 1.367177963256836
Batch 64/64 loss: -1.9561138153076172
Epoch 90  Train loss: 1.3475958880256205  Val loss: 1.3368922230304312
Epoch 91
-------------------------------
Batch 1/64 loss: 1.3992919921875
Batch 2/64 loss: 1.6624274253845215
Batch 3/64 loss: 1.1784658432006836
Batch 4/64 loss: 1.3731694221496582
Batch 5/64 loss: 1.248204231262207
Batch 6/64 loss: 1.5338730812072754
Batch 7/64 loss: 1.2998623847961426
Batch 8/64 loss: 1.2816171646118164
Batch 9/64 loss: 1.0653753280639648
Batch 10/64 loss: 1.187784194946289
Batch 11/64 loss: 1.2064342498779297
Batch 12/64 loss: 1.484105110168457
Batch 13/64 loss: 1.2046575546264648
Batch 14/64 loss: 1.5779380798339844
Batch 15/64 loss: 1.3883848190307617
Batch 16/64 loss: 1.486328125
Batch 17/64 loss: 1.013084888458252
Batch 18/64 loss: 1.337766170501709
Batch 19/64 loss: 1.0777559280395508
Batch 20/64 loss: 1.083275318145752
Batch 21/64 loss: 1.2506041526794434
Batch 22/64 loss: 1.3649921417236328
Batch 23/64 loss: 1.286172866821289
Batch 24/64 loss: 1.230489730834961
Batch 25/64 loss: 1.2468781471252441
Batch 26/64 loss: 1.4801335334777832
Batch 27/64 loss: 1.176905632019043
Batch 28/64 loss: 1.44386625289917
Batch 29/64 loss: 1.4119820594787598
Batch 30/64 loss: 1.2754454612731934
Batch 31/64 loss: 1.3062472343444824
Batch 32/64 loss: 1.09712553024292
Batch 33/64 loss: 1.4610204696655273
Batch 34/64 loss: 1.1297945976257324
Batch 35/64 loss: 1.4212336540222168
Batch 36/64 loss: 1.2153558731079102
Batch 37/64 loss: 1.6076102256774902
Batch 38/64 loss: 1.369279384613037
Batch 39/64 loss: 1.401956558227539
Batch 40/64 loss: 1.1774864196777344
Batch 41/64 loss: 1.3735880851745605
Batch 42/64 loss: 1.1387052536010742
Batch 43/64 loss: 1.4771828651428223
Batch 44/64 loss: 1.329965591430664
Batch 45/64 loss: 1.5868325233459473
Batch 46/64 loss: 1.0742835998535156
Batch 47/64 loss: 1.2070999145507812
Batch 48/64 loss: 1.6455497741699219
Batch 49/64 loss: 1.3601884841918945
Batch 50/64 loss: 1.2638025283813477
Batch 51/64 loss: 1.218003273010254
Batch 52/64 loss: 1.481250286102295
Batch 53/64 loss: 1.0887532234191895
Batch 54/64 loss: 1.3884730339050293
Batch 55/64 loss: 1.2445130348205566
Batch 56/64 loss: 1.1491427421569824
Batch 57/64 loss: 1.3053936958312988
Batch 58/64 loss: 1.162062644958496
Batch 59/64 loss: 1.2787103652954102
Batch 60/64 loss: 1.2361793518066406
Batch 61/64 loss: 1.2170090675354004
Batch 62/64 loss: 1.141608715057373
Batch 63/64 loss: 0.9624691009521484
Batch 64/64 loss: -2.357839584350586
Epoch 91  Train loss: 1.2550081365248735  Val loss: 1.2097562481857247
Saving best model, epoch: 91
Epoch 92
-------------------------------
Batch 1/64 loss: 1.615920066833496
Batch 2/64 loss: 0.9679327011108398
Batch 3/64 loss: 1.253422737121582
Batch 4/64 loss: 1.6447415351867676
Batch 5/64 loss: 1.0386128425598145
Batch 6/64 loss: 1.4452815055847168
Batch 7/64 loss: 1.0673346519470215
Batch 8/64 loss: 1.1223278045654297
Batch 9/64 loss: 1.0206894874572754
Batch 10/64 loss: 1.1843457221984863
Batch 11/64 loss: 1.0423698425292969
Batch 12/64 loss: 1.1135611534118652
Batch 13/64 loss: 1.5311155319213867
Batch 14/64 loss: 1.1076717376708984
Batch 15/64 loss: 1.2262167930603027
Batch 16/64 loss: 1.3662629127502441
Batch 17/64 loss: 1.1558947563171387
Batch 18/64 loss: 1.0377488136291504
Batch 19/64 loss: 1.051609992980957
Batch 20/64 loss: 1.613565444946289
Batch 21/64 loss: 1.448892593383789
Batch 22/64 loss: 1.818789005279541
Batch 23/64 loss: 1.291748046875
Batch 24/64 loss: 1.2142305374145508
Batch 25/64 loss: 1.2947359085083008
Batch 26/64 loss: 1.1447315216064453
Batch 27/64 loss: 1.2199020385742188
Batch 28/64 loss: 1.1101908683776855
Batch 29/64 loss: 1.0720362663269043
Batch 30/64 loss: 1.340291976928711
Batch 31/64 loss: 1.7133674621582031
Batch 32/64 loss: 1.0056281089782715
Batch 33/64 loss: 1.4612956047058105
Batch 34/64 loss: 1.2485103607177734
Batch 35/64 loss: 1.027470588684082
Batch 36/64 loss: 1.296773910522461
Batch 37/64 loss: 1.0698943138122559
Batch 38/64 loss: 1.0769343376159668
Batch 39/64 loss: 1.1653485298156738
Batch 40/64 loss: 1.2257061004638672
Batch 41/64 loss: 1.2572236061096191
Batch 42/64 loss: 1.2566099166870117
Batch 43/64 loss: 1.1508479118347168
Batch 44/64 loss: 1.1996545791625977
Batch 45/64 loss: 1.0854835510253906
Batch 46/64 loss: 1.2981033325195312
Batch 47/64 loss: 1.391911506652832
Batch 48/64 loss: 1.350968360900879
Batch 49/64 loss: 1.3073453903198242
Batch 50/64 loss: 1.1715970039367676
Batch 51/64 loss: 1.1963577270507812
Batch 52/64 loss: 2.041065216064453
Batch 53/64 loss: 1.1640348434448242
Batch 54/64 loss: 1.3651533126831055
Batch 55/64 loss: 1.274482250213623
Batch 56/64 loss: 1.475494384765625
Batch 57/64 loss: 1.2871389389038086
Batch 58/64 loss: 1.2183032035827637
Batch 59/64 loss: 1.4487571716308594
Batch 60/64 loss: 1.0804400444030762
Batch 61/64 loss: 1.188122272491455
Batch 62/64 loss: 1.2415204048156738
Batch 63/64 loss: 1.4615044593811035
Batch 64/64 loss: -1.7623238563537598
Epoch 92  Train loss: 1.230485970366235  Val loss: 1.2609115941417997
Epoch 93
-------------------------------
Batch 1/64 loss: 1.369823932647705
Batch 2/64 loss: 1.6866750717163086
Batch 3/64 loss: 1.1192693710327148
Batch 4/64 loss: 1.1839237213134766
Batch 5/64 loss: 1.3328838348388672
Batch 6/64 loss: 1.1043601036071777
Batch 7/64 loss: 0.9804434776306152
Batch 8/64 loss: 1.52362060546875
Batch 9/64 loss: 1.4746217727661133
Batch 10/64 loss: 1.6880640983581543
Batch 11/64 loss: 1.338564395904541
Batch 12/64 loss: 1.5795960426330566
Batch 13/64 loss: 1.027592658996582
Batch 14/64 loss: 1.5032825469970703
Batch 15/64 loss: 0.9919118881225586
Batch 16/64 loss: 1.5373620986938477
Batch 17/64 loss: 1.211287021636963
Batch 18/64 loss: 0.9273462295532227
Batch 19/64 loss: 1.3252911567687988
Batch 20/64 loss: 1.0890522003173828
Batch 21/64 loss: 0.8422112464904785
Batch 22/64 loss: 1.3644099235534668
Batch 23/64 loss: 1.184938907623291
Batch 24/64 loss: 0.907346248626709
Batch 25/64 loss: 1.1819419860839844
Batch 26/64 loss: 1.259202003479004
Batch 27/64 loss: 0.9832983016967773
Batch 28/64 loss: 1.3064703941345215
Batch 29/64 loss: 1.619504451751709
Batch 30/64 loss: 1.2080111503601074
Batch 31/64 loss: 1.070967674255371
Batch 32/64 loss: 1.5700874328613281
Batch 33/64 loss: 1.2108893394470215
Batch 34/64 loss: 1.194551944732666
Batch 35/64 loss: 0.939239501953125
Batch 36/64 loss: 1.1272077560424805
Batch 37/64 loss: 1.3313279151916504
Batch 38/64 loss: 1.1918864250183105
Batch 39/64 loss: 1.1950874328613281
Batch 40/64 loss: 1.3063993453979492
Batch 41/64 loss: 1.3034234046936035
Batch 42/64 loss: 1.243727684020996
Batch 43/64 loss: 1.3002057075500488
Batch 44/64 loss: 1.196181297302246
Batch 45/64 loss: 1.1370935440063477
Batch 46/64 loss: 1.3242130279541016
Batch 47/64 loss: 1.0960969924926758
Batch 48/64 loss: 1.2737674713134766
Batch 49/64 loss: 1.1194987297058105
Batch 50/64 loss: 1.4049773216247559
Batch 51/64 loss: 1.2919979095458984
Batch 52/64 loss: 1.2119812965393066
Batch 53/64 loss: 1.3775029182434082
Batch 54/64 loss: 1.534559726715088
Batch 55/64 loss: 1.2312850952148438
Batch 56/64 loss: 1.1054930686950684
Batch 57/64 loss: 1.3654375076293945
Batch 58/64 loss: 1.242485523223877
Batch 59/64 loss: 0.931795597076416
Batch 60/64 loss: 1.1125078201293945
Batch 61/64 loss: 1.1304640769958496
Batch 62/64 loss: 0.9808640480041504
Batch 63/64 loss: 1.5140891075134277
Batch 64/64 loss: -1.8714776039123535
Epoch 93  Train loss: 1.2080939966089586  Val loss: 1.1477238566605086
Saving best model, epoch: 93
Epoch 94
-------------------------------
Batch 1/64 loss: 1.3969454765319824
Batch 2/64 loss: 1.0689082145690918
Batch 3/64 loss: 1.580273151397705
Batch 4/64 loss: 1.2019414901733398
Batch 5/64 loss: 1.1148042678833008
Batch 6/64 loss: 1.2027244567871094
Batch 7/64 loss: 1.129220962524414
Batch 8/64 loss: 1.2775893211364746
Batch 9/64 loss: 1.4598522186279297
Batch 10/64 loss: 1.026298999786377
Batch 11/64 loss: 0.887359619140625
Batch 12/64 loss: 1.0519795417785645
Batch 13/64 loss: 1.3484759330749512
Batch 14/64 loss: 1.3567895889282227
Batch 15/64 loss: 1.211165428161621
Batch 16/64 loss: 1.2097373008728027
Batch 17/64 loss: 1.0661931037902832
Batch 18/64 loss: 1.642481803894043
Batch 19/64 loss: 1.33209228515625
Batch 20/64 loss: 0.964686393737793
Batch 21/64 loss: 1.0405163764953613
Batch 22/64 loss: 1.0084877014160156
Batch 23/64 loss: 1.0913996696472168
Batch 24/64 loss: 1.6179161071777344
Batch 25/64 loss: 1.5255012512207031
Batch 26/64 loss: 1.400439739227295
Batch 27/64 loss: 1.193338394165039
Batch 28/64 loss: 1.1415081024169922
Batch 29/64 loss: 1.5470008850097656
Batch 30/64 loss: 1.2046527862548828
Batch 31/64 loss: 1.0324530601501465
Batch 32/64 loss: 1.303750991821289
Batch 33/64 loss: 1.4297499656677246
Batch 34/64 loss: 1.2275843620300293
Batch 35/64 loss: 1.0461316108703613
Batch 36/64 loss: 1.3738813400268555
Batch 37/64 loss: 1.2685489654541016
Batch 38/64 loss: 1.2785110473632812
Batch 39/64 loss: 1.111372470855713
Batch 40/64 loss: 1.104705810546875
Batch 41/64 loss: 1.0999116897583008
Batch 42/64 loss: 1.105924129486084
Batch 43/64 loss: 1.0348210334777832
Batch 44/64 loss: 1.3951878547668457
Batch 45/64 loss: 0.9625406265258789
Batch 46/64 loss: 1.4006800651550293
Batch 47/64 loss: 1.0515480041503906
Batch 48/64 loss: 1.3106608390808105
Batch 49/64 loss: 1.3688020706176758
Batch 50/64 loss: 1.1660456657409668
Batch 51/64 loss: 1.2255687713623047
Batch 52/64 loss: 0.8597135543823242
Batch 53/64 loss: 1.035334587097168
Batch 54/64 loss: 1.0344281196594238
Batch 55/64 loss: 1.4127278327941895
Batch 56/64 loss: 0.9811458587646484
Batch 57/64 loss: 1.578423023223877
Batch 58/64 loss: 1.2394843101501465
Batch 59/64 loss: 1.0463027954101562
Batch 60/64 loss: 0.9500632286071777
Batch 61/64 loss: 1.132070541381836
Batch 62/64 loss: 1.1651239395141602
Batch 63/64 loss: 0.9134283065795898
Batch 64/64 loss: -2.452383518218994
Epoch 94  Train loss: 1.1624724612516515  Val loss: 1.1186174674542089
Saving best model, epoch: 94
Epoch 95
-------------------------------
Batch 1/64 loss: 0.848078727722168
Batch 2/64 loss: 1.1520118713378906
Batch 3/64 loss: 1.2350349426269531
Batch 4/64 loss: 0.9747133255004883
Batch 5/64 loss: 1.162583351135254
Batch 6/64 loss: 1.0498638153076172
Batch 7/64 loss: 1.1889829635620117
Batch 8/64 loss: 1.465200424194336
Batch 9/64 loss: 1.0515122413635254
Batch 10/64 loss: 1.4021949768066406
Batch 11/64 loss: 1.2402005195617676
Batch 12/64 loss: 0.9584708213806152
Batch 13/64 loss: 1.5123109817504883
Batch 14/64 loss: 1.1569113731384277
Batch 15/64 loss: 1.5048489570617676
Batch 16/64 loss: 1.0885963439941406
Batch 17/64 loss: 1.334704875946045
Batch 18/64 loss: 1.4973578453063965
Batch 19/64 loss: 1.3869338035583496
Batch 20/64 loss: 1.783301830291748
Batch 21/64 loss: 1.0788578987121582
Batch 22/64 loss: 1.5842399597167969
Batch 23/64 loss: 1.2052764892578125
Batch 24/64 loss: 1.3771090507507324
Batch 25/64 loss: 1.057805061340332
Batch 26/64 loss: 1.0016670227050781
Batch 27/64 loss: 0.9998264312744141
Batch 28/64 loss: 1.2187004089355469
Batch 29/64 loss: 1.2239251136779785
Batch 30/64 loss: 1.124608039855957
Batch 31/64 loss: 1.005082607269287
Batch 32/64 loss: 1.5107941627502441
Batch 33/64 loss: 1.3335251808166504
Batch 34/64 loss: 1.051774501800537
Batch 35/64 loss: 1.694706916809082
Batch 36/64 loss: 1.2028489112854004
Batch 37/64 loss: 1.4233274459838867
Batch 38/64 loss: 1.48744535446167
Batch 39/64 loss: 1.2796611785888672
Batch 40/64 loss: 1.0908632278442383
Batch 41/64 loss: 1.0204358100891113
Batch 42/64 loss: 1.338188648223877
Batch 43/64 loss: 1.188910961151123
Batch 44/64 loss: 1.2381272315979004
Batch 45/64 loss: 1.5308218002319336
Batch 46/64 loss: 1.2153739929199219
Batch 47/64 loss: 1.2555694580078125
Batch 48/64 loss: 1.199582576751709
Batch 49/64 loss: 1.440389633178711
Batch 50/64 loss: 0.9497637748718262
Batch 51/64 loss: 1.4695987701416016
Batch 52/64 loss: 1.1539926528930664
Batch 53/64 loss: 1.3431520462036133
Batch 54/64 loss: 1.4334559440612793
Batch 55/64 loss: 1.0925297737121582
Batch 56/64 loss: 1.3934111595153809
Batch 57/64 loss: 1.1461057662963867
Batch 58/64 loss: 1.3362407684326172
Batch 59/64 loss: 1.1715707778930664
Batch 60/64 loss: 1.2757062911987305
Batch 61/64 loss: 1.2430744171142578
Batch 62/64 loss: 1.3929762840270996
Batch 63/64 loss: 1.3965034484863281
Batch 64/64 loss: -1.661980152130127
Epoch 95  Train loss: 1.222350679659376  Val loss: 1.2147495164903988
Epoch 96
-------------------------------
Batch 1/64 loss: 1.0779027938842773
Batch 2/64 loss: 1.4438300132751465
Batch 3/64 loss: 1.213895320892334
Batch 4/64 loss: 1.1324787139892578
Batch 5/64 loss: 1.0174636840820312
Batch 6/64 loss: 1.3373889923095703
Batch 7/64 loss: 1.5552029609680176
Batch 8/64 loss: 1.0459489822387695
Batch 9/64 loss: 1.404083251953125
Batch 10/64 loss: 1.1136016845703125
Batch 11/64 loss: 1.0322437286376953
Batch 12/64 loss: 1.2506604194641113
Batch 13/64 loss: 1.0226974487304688
Batch 14/64 loss: 1.1262297630310059
Batch 15/64 loss: 1.0762319564819336
Batch 16/64 loss: 0.9087715148925781
Batch 17/64 loss: 1.6527152061462402
Batch 18/64 loss: 1.0776824951171875
Batch 19/64 loss: 1.173429012298584
Batch 20/64 loss: 0.8480114936828613
Batch 21/64 loss: 1.293243408203125
Batch 22/64 loss: 1.1863651275634766
Batch 23/64 loss: 1.3085651397705078
Batch 24/64 loss: 1.4728760719299316
Batch 25/64 loss: 1.2146544456481934
Batch 26/64 loss: 0.9733362197875977
Batch 27/64 loss: 1.1689538955688477
Batch 28/64 loss: 1.3130321502685547
Batch 29/64 loss: 1.0776686668395996
Batch 30/64 loss: 1.1219367980957031
Batch 31/64 loss: 1.555412769317627
Batch 32/64 loss: 0.7941145896911621
Batch 33/64 loss: 1.041369915008545
Batch 34/64 loss: 1.0991835594177246
Batch 35/64 loss: 0.8729510307312012
Batch 36/64 loss: 0.9849791526794434
Batch 37/64 loss: 1.3367266654968262
Batch 38/64 loss: 1.2808938026428223
Batch 39/64 loss: 1.2930889129638672
Batch 40/64 loss: 1.2004733085632324
Batch 41/64 loss: 1.0289902687072754
Batch 42/64 loss: 1.141401767730713
Batch 43/64 loss: 1.3944287300109863
Batch 44/64 loss: 1.1686735153198242
Batch 45/64 loss: 1.1932392120361328
Batch 46/64 loss: 1.5790362358093262
Batch 47/64 loss: 1.4713826179504395
Batch 48/64 loss: 1.218285083770752
Batch 49/64 loss: 1.3832054138183594
Batch 50/64 loss: 1.274055004119873
Batch 51/64 loss: 1.1358904838562012
Batch 52/64 loss: 1.6204266548156738
Batch 53/64 loss: 1.165428638458252
Batch 54/64 loss: 1.3973779678344727
Batch 55/64 loss: 1.3590426445007324
Batch 56/64 loss: 1.2273058891296387
Batch 57/64 loss: 1.1435532569885254
Batch 58/64 loss: 0.9186582565307617
Batch 59/64 loss: 1.2030830383300781
Batch 60/64 loss: 1.093874454498291
Batch 61/64 loss: 1.1313738822937012
Batch 62/64 loss: 1.15653657913208
Batch 63/64 loss: 1.1640453338623047
Batch 64/64 loss: -1.965937614440918
Epoch 96  Train loss: 1.1638452829099168  Val loss: 1.112742197882269
Saving best model, epoch: 96
Epoch 97
-------------------------------
Batch 1/64 loss: 0.9873685836791992
Batch 2/64 loss: 1.2475476264953613
Batch 3/64 loss: 1.033940315246582
Batch 4/64 loss: 1.048283576965332
Batch 5/64 loss: 1.0024008750915527
Batch 6/64 loss: 1.1041450500488281
Batch 7/64 loss: 1.099316120147705
Batch 8/64 loss: 1.1183314323425293
Batch 9/64 loss: 0.9156079292297363
Batch 10/64 loss: 1.1610217094421387
Batch 11/64 loss: 1.3439178466796875
Batch 12/64 loss: 1.0021929740905762
Batch 13/64 loss: 1.2326793670654297
Batch 14/64 loss: 1.043809413909912
Batch 15/64 loss: 1.110102653503418
Batch 16/64 loss: 1.0570502281188965
Batch 17/64 loss: 0.9140129089355469
Batch 18/64 loss: 1.1407732963562012
Batch 19/64 loss: 1.2225680351257324
Batch 20/64 loss: 1.2434186935424805
Batch 21/64 loss: 1.396000862121582
Batch 22/64 loss: 1.3815035820007324
Batch 23/64 loss: 0.9477806091308594
Batch 24/64 loss: 0.9478769302368164
Batch 25/64 loss: 1.3247003555297852
Batch 26/64 loss: 1.3232040405273438
Batch 27/64 loss: 1.3416938781738281
Batch 28/64 loss: 1.4787468910217285
Batch 29/64 loss: 1.1652369499206543
Batch 30/64 loss: 0.8875594139099121
Batch 31/64 loss: 1.0031194686889648
Batch 32/64 loss: 1.1048173904418945
Batch 33/64 loss: 1.0132436752319336
Batch 34/64 loss: 1.1654057502746582
Batch 35/64 loss: 1.4940590858459473
Batch 36/64 loss: 1.1538376808166504
Batch 37/64 loss: 1.3937478065490723
Batch 38/64 loss: 1.1310391426086426
Batch 39/64 loss: 1.0325002670288086
Batch 40/64 loss: 0.9668102264404297
Batch 41/64 loss: 0.9029417037963867
Batch 42/64 loss: 0.8372411727905273
Batch 43/64 loss: 1.0148096084594727
Batch 44/64 loss: 1.3366403579711914
Batch 45/64 loss: 0.9412331581115723
Batch 46/64 loss: 1.0491962432861328
Batch 47/64 loss: 1.3001203536987305
Batch 48/64 loss: 1.2559027671813965
Batch 49/64 loss: 1.1338434219360352
Batch 50/64 loss: 1.5237841606140137
Batch 51/64 loss: 1.2267589569091797
Batch 52/64 loss: 1.2171101570129395
Batch 53/64 loss: 1.0431008338928223
Batch 54/64 loss: 1.1529192924499512
Batch 55/64 loss: 1.424041748046875
Batch 56/64 loss: 1.286085605621338
Batch 57/64 loss: 1.348803997039795
Batch 58/64 loss: 1.2089920043945312
Batch 59/64 loss: 1.1371235847473145
Batch 60/64 loss: 1.4862666130065918
Batch 61/64 loss: 1.4059066772460938
Batch 62/64 loss: 1.1313161849975586
Batch 63/64 loss: 1.178375244140625
Batch 64/64 loss: -2.3664512634277344
Epoch 97  Train loss: 1.120769381055645  Val loss: 1.2854701104442687
Epoch 98
-------------------------------
Batch 1/64 loss: 1.3984375
Batch 2/64 loss: 1.7699604034423828
Batch 3/64 loss: 1.3114595413208008
Batch 4/64 loss: 0.9868044853210449
Batch 5/64 loss: 0.994384765625
Batch 6/64 loss: 1.1011910438537598
Batch 7/64 loss: 1.0559124946594238
Batch 8/64 loss: 1.1944456100463867
Batch 9/64 loss: 1.3266220092773438
Batch 10/64 loss: 1.5975103378295898
Batch 11/64 loss: 1.24180269241333
Batch 12/64 loss: 0.9712662696838379
Batch 13/64 loss: 1.1745052337646484
Batch 14/64 loss: 1.3561720848083496
Batch 15/64 loss: 1.1268925666809082
Batch 16/64 loss: 1.0037689208984375
Batch 17/64 loss: 0.9119782447814941
Batch 18/64 loss: 1.56203031539917
Batch 19/64 loss: 0.9124665260314941
Batch 20/64 loss: 1.4560823440551758
Batch 21/64 loss: 1.2110300064086914
Batch 22/64 loss: 1.0618700981140137
Batch 23/64 loss: 1.184675693511963
Batch 24/64 loss: 0.9813127517700195
Batch 25/64 loss: 1.2063298225402832
Batch 26/64 loss: 1.0076689720153809
Batch 27/64 loss: 1.1415510177612305
Batch 28/64 loss: 1.4222040176391602
Batch 29/64 loss: 1.069666862487793
Batch 30/64 loss: 1.309861660003662
Batch 31/64 loss: 1.151719570159912
Batch 32/64 loss: 0.9879274368286133
Batch 33/64 loss: 1.2307701110839844
Batch 34/64 loss: 1.5737357139587402
Batch 35/64 loss: 1.152268886566162
Batch 36/64 loss: 0.959052562713623
Batch 37/64 loss: 0.9639348983764648
Batch 38/64 loss: 1.0035433769226074
Batch 39/64 loss: 1.0360870361328125
Batch 40/64 loss: 0.9785313606262207
Batch 41/64 loss: 1.1387710571289062
Batch 42/64 loss: 1.3701171875
Batch 43/64 loss: 1.6041522026062012
Batch 44/64 loss: 1.2217035293579102
Batch 45/64 loss: 1.1158447265625
Batch 46/64 loss: 1.1406302452087402
Batch 47/64 loss: 1.1931419372558594
Batch 48/64 loss: 0.8592300415039062
Batch 49/64 loss: 0.8260293006896973
Batch 50/64 loss: 1.1795916557312012
Batch 51/64 loss: 1.1387577056884766
Batch 52/64 loss: 0.8787436485290527
Batch 53/64 loss: 0.9723930358886719
Batch 54/64 loss: 1.2724113464355469
Batch 55/64 loss: 0.9322366714477539
Batch 56/64 loss: 1.1420092582702637
Batch 57/64 loss: 1.1156229972839355
Batch 58/64 loss: 0.9995970726013184
Batch 59/64 loss: 1.3730902671813965
Batch 60/64 loss: 1.2131218910217285
Batch 61/64 loss: 1.2715888023376465
Batch 62/64 loss: 1.281053066253662
Batch 63/64 loss: 1.1896963119506836
Batch 64/64 loss: -2.0634799003601074
Epoch 98  Train loss: 1.1289311259400612  Val loss: 1.3681610081204025
Epoch 99
-------------------------------
Batch 1/64 loss: 1.0945420265197754
Batch 2/64 loss: 1.0493736267089844
Batch 3/64 loss: 1.1009340286254883
Batch 4/64 loss: 1.1471014022827148
Batch 5/64 loss: 1.0190544128417969
Batch 6/64 loss: 2.536156177520752
Batch 7/64 loss: 1.0725812911987305
Batch 8/64 loss: 1.3778762817382812
Batch 9/64 loss: 1.4306230545043945
Batch 10/64 loss: 1.4333529472351074
Batch 11/64 loss: 1.3127193450927734
Batch 12/64 loss: 1.1153626441955566
Batch 13/64 loss: 1.196566104888916
Batch 14/64 loss: 1.296736717224121
Batch 15/64 loss: 1.4171175956726074
Batch 16/64 loss: 1.2175703048706055
Batch 17/64 loss: 1.2188196182250977
Batch 18/64 loss: 2.251964569091797
Batch 19/64 loss: 1.3112163543701172
Batch 20/64 loss: 1.0222735404968262
Batch 21/64 loss: 1.248086929321289
Batch 22/64 loss: 1.370452880859375
Batch 23/64 loss: 1.2721219062805176
Batch 24/64 loss: 1.5050745010375977
Batch 25/64 loss: 1.311915397644043
Batch 26/64 loss: 1.5566835403442383
Batch 27/64 loss: 1.22819185256958
Batch 28/64 loss: 1.4060001373291016
Batch 29/64 loss: 1.3934574127197266
Batch 30/64 loss: 1.4972386360168457
Batch 31/64 loss: 1.2668547630310059
Batch 32/64 loss: 1.4639315605163574
Batch 33/64 loss: 1.031381607055664
Batch 34/64 loss: 1.3082804679870605
Batch 35/64 loss: 1.6404671669006348
Batch 36/64 loss: 1.0843796730041504
Batch 37/64 loss: 1.1259779930114746
Batch 38/64 loss: 1.3917741775512695
Batch 39/64 loss: 1.202378749847412
Batch 40/64 loss: 1.097364902496338
Batch 41/64 loss: 1.2593584060668945
Batch 42/64 loss: 1.2377920150756836
Batch 43/64 loss: 1.151991844177246
Batch 44/64 loss: 1.242006778717041
Batch 45/64 loss: 1.0687265396118164
Batch 46/64 loss: 1.2987799644470215
Batch 47/64 loss: 0.9503884315490723
Batch 48/64 loss: 1.3988380432128906
Batch 49/64 loss: 1.037583351135254
Batch 50/64 loss: 1.0830602645874023
Batch 51/64 loss: 1.100578784942627
Batch 52/64 loss: 1.7471656799316406
Batch 53/64 loss: 0.945526123046875
Batch 54/64 loss: 1.3822593688964844
Batch 55/64 loss: 1.1242518424987793
Batch 56/64 loss: 1.2697529792785645
Batch 57/64 loss: 1.1376729011535645
Batch 58/64 loss: 1.0350332260131836
Batch 59/64 loss: 1.1562623977661133
Batch 60/64 loss: 1.3000297546386719
Batch 61/64 loss: 1.0935349464416504
Batch 62/64 loss: 1.3257689476013184
Batch 63/64 loss: 1.2815775871276855
Batch 64/64 loss: -1.8141331672668457
Epoch 99  Train loss: 1.2437850447262035  Val loss: 1.2162063572414963
Epoch 100
-------------------------------
Batch 1/64 loss: 1.1734962463378906
Batch 2/64 loss: 1.2899479866027832
Batch 3/64 loss: 1.0341897010803223
Batch 4/64 loss: 1.4496736526489258
Batch 5/64 loss: 1.104382038116455
Batch 6/64 loss: 1.1407537460327148
Batch 7/64 loss: 1.0170392990112305
Batch 8/64 loss: 1.2530198097229004
Batch 9/64 loss: 1.1591782569885254
Batch 10/64 loss: 1.3518686294555664
Batch 11/64 loss: 1.0559897422790527
Batch 12/64 loss: 1.4674072265625
Batch 13/64 loss: 0.873417854309082
Batch 14/64 loss: 1.395979404449463
Batch 15/64 loss: 1.3260273933410645
Batch 16/64 loss: 1.187612533569336
Batch 17/64 loss: 1.4497132301330566
Batch 18/64 loss: 1.6226954460144043
Batch 19/64 loss: 1.2158632278442383
Batch 20/64 loss: 1.2470879554748535
Batch 21/64 loss: 1.070753574371338
Batch 22/64 loss: 1.0384759902954102
Batch 23/64 loss: 1.126220703125
Batch 24/64 loss: 1.1749038696289062
Batch 25/64 loss: 1.0432186126708984
Batch 26/64 loss: 1.3377785682678223
Batch 27/64 loss: 0.952582836151123
Batch 28/64 loss: 1.242419719696045
Batch 29/64 loss: 1.4815797805786133
Batch 30/64 loss: 1.0403800010681152
Batch 31/64 loss: 1.0455546379089355
Batch 32/64 loss: 0.9503474235534668
Batch 33/64 loss: 1.4416813850402832
Batch 34/64 loss: 1.285585880279541
Batch 35/64 loss: 1.2562370300292969
Batch 36/64 loss: 1.3389263153076172
Batch 37/64 loss: 1.0910115242004395
Batch 38/64 loss: 1.5746636390686035
Batch 39/64 loss: 1.2090344429016113
Batch 40/64 loss: 1.2169189453125
Batch 41/64 loss: 1.1203832626342773
Batch 42/64 loss: 1.0719847679138184
Batch 43/64 loss: 1.132561206817627
Batch 44/64 loss: 1.004134178161621
Batch 45/64 loss: 1.053426742553711
Batch 46/64 loss: 1.1634373664855957
Batch 47/64 loss: 1.318258285522461
Batch 48/64 loss: 1.2098870277404785
Batch 49/64 loss: 1.1805596351623535
Batch 50/64 loss: 1.192328929901123
Batch 51/64 loss: 1.318833351135254
Batch 52/64 loss: 1.019782543182373
Batch 53/64 loss: 1.3410520553588867
Batch 54/64 loss: 0.8548445701599121
Batch 55/64 loss: 1.53788423538208
Batch 56/64 loss: 1.0640859603881836
Batch 57/64 loss: 1.0800132751464844
Batch 58/64 loss: 0.9050416946411133
Batch 59/64 loss: 1.0215668678283691
Batch 60/64 loss: 1.2032885551452637
Batch 61/64 loss: 1.0646953582763672
Batch 62/64 loss: 1.182417392730713
Batch 63/64 loss: 1.572171688079834
Batch 64/64 loss: -2.130457878112793
Epoch 100  Train loss: 1.1568378635481291  Val loss: 1.12127695378569
Epoch 101
-------------------------------
Batch 1/64 loss: 1.100813388824463
Batch 2/64 loss: 1.111217975616455
Batch 3/64 loss: 1.1595792770385742
Batch 4/64 loss: 1.123070240020752
Batch 5/64 loss: 0.969353199005127
Batch 6/64 loss: 1.1892447471618652
Batch 7/64 loss: 1.211195468902588
Batch 8/64 loss: 1.0267152786254883
Batch 9/64 loss: 0.966062068939209
Batch 10/64 loss: 0.9181361198425293
Batch 11/64 loss: 1.2952165603637695
Batch 12/64 loss: 1.1786885261535645
Batch 13/64 loss: 1.2187142372131348
Batch 14/64 loss: 1.1693272590637207
Batch 15/64 loss: 0.8606796264648438
Batch 16/64 loss: 1.3188138008117676
Batch 17/64 loss: 0.8420042991638184
Batch 18/64 loss: 1.2088985443115234
Batch 19/64 loss: 1.0919303894042969
Batch 20/64 loss: 1.0834226608276367
Batch 21/64 loss: 1.67185640335083
Batch 22/64 loss: 0.9971714019775391
Batch 23/64 loss: 1.0749945640563965
Batch 24/64 loss: 1.023329734802246
Batch 25/64 loss: 1.1047539710998535
Batch 26/64 loss: 0.9818787574768066
Batch 27/64 loss: 0.9197030067443848
Batch 28/64 loss: 1.1637449264526367
Batch 29/64 loss: 1.1355056762695312
Batch 30/64 loss: 1.1653947830200195
Batch 31/64 loss: 1.2743115425109863
Batch 32/64 loss: 1.1545805931091309
Batch 33/64 loss: 1.1681709289550781
Batch 34/64 loss: 1.1091270446777344
Batch 35/64 loss: 0.928581714630127
Batch 36/64 loss: 1.052628993988037
Batch 37/64 loss: 1.2283034324645996
Batch 38/64 loss: 1.199653148651123
Batch 39/64 loss: 1.343724250793457
Batch 40/64 loss: 0.9804134368896484
Batch 41/64 loss: 1.1817622184753418
Batch 42/64 loss: 1.7259154319763184
Batch 43/64 loss: 1.201612949371338
Batch 44/64 loss: 1.3756580352783203
Batch 45/64 loss: 1.4990744590759277
Batch 46/64 loss: 1.1146278381347656
Batch 47/64 loss: 1.2362403869628906
Batch 48/64 loss: 1.1626067161560059
Batch 49/64 loss: 1.469522476196289
Batch 50/64 loss: 1.1093850135803223
Batch 51/64 loss: 0.9397988319396973
Batch 52/64 loss: 1.1753196716308594
Batch 53/64 loss: 1.1100025177001953
Batch 54/64 loss: 1.3207597732543945
Batch 55/64 loss: 1.192577838897705
Batch 56/64 loss: 0.8385429382324219
Batch 57/64 loss: 1.1554350852966309
Batch 58/64 loss: 1.2026801109313965
Batch 59/64 loss: 1.122999668121338
Batch 60/64 loss: 0.930417537689209
Batch 61/64 loss: 1.2296371459960938
Batch 62/64 loss: 1.0899734497070312
Batch 63/64 loss: 0.9114241600036621
Batch 64/64 loss: -2.4088211059570312
Epoch 101  Train loss: 1.1013375749775007  Val loss: 1.1810268192356805
Epoch 102
-------------------------------
Batch 1/64 loss: 0.9900431632995605
Batch 2/64 loss: 1.62608003616333
Batch 3/64 loss: 1.0423774719238281
Batch 4/64 loss: 0.9632978439331055
Batch 5/64 loss: 1.2738986015319824
Batch 6/64 loss: 0.8709197044372559
Batch 7/64 loss: 1.0481781959533691
Batch 8/64 loss: 1.166266918182373
Batch 9/64 loss: 1.1701374053955078
Batch 10/64 loss: 0.9558143615722656
Batch 11/64 loss: 0.9174728393554688
Batch 12/64 loss: 1.2992172241210938
Batch 13/64 loss: 1.397409439086914
Batch 14/64 loss: 1.2933073043823242
Batch 15/64 loss: 1.4473786354064941
Batch 16/64 loss: 0.9039225578308105
Batch 17/64 loss: 0.9820880889892578
Batch 18/64 loss: 1.3859405517578125
Batch 19/64 loss: 1.8031044006347656
Batch 20/64 loss: 1.4229230880737305
Batch 21/64 loss: 1.5160140991210938
Batch 22/64 loss: 1.4733262062072754
Batch 23/64 loss: 1.2716960906982422
Batch 24/64 loss: 1.4004731178283691
Batch 25/64 loss: 1.486978530883789
Batch 26/64 loss: 1.1889033317565918
Batch 27/64 loss: 1.5951037406921387
Batch 28/64 loss: 1.186232566833496
Batch 29/64 loss: 1.2427420616149902
Batch 30/64 loss: 1.310053825378418
Batch 31/64 loss: 1.548469066619873
Batch 32/64 loss: 1.3167948722839355
Batch 33/64 loss: 1.3305001258850098
Batch 34/64 loss: 1.531745433807373
Batch 35/64 loss: 1.1520466804504395
Batch 36/64 loss: 1.6037826538085938
Batch 37/64 loss: 1.2374982833862305
Batch 38/64 loss: 1.3342771530151367
Batch 39/64 loss: 1.3342337608337402
Batch 40/64 loss: 1.2847094535827637
Batch 41/64 loss: 1.1717262268066406
Batch 42/64 loss: 1.3928494453430176
Batch 43/64 loss: 1.6372413635253906
Batch 44/64 loss: 1.5114421844482422
Batch 45/64 loss: 1.5087995529174805
Batch 46/64 loss: 1.484889030456543
Batch 47/64 loss: 1.1423850059509277
Batch 48/64 loss: 1.3026676177978516
Batch 49/64 loss: 1.7033720016479492
Batch 50/64 loss: 1.3198366165161133
Batch 51/64 loss: 1.0508227348327637
Batch 52/64 loss: 1.199885368347168
Batch 53/64 loss: 1.1259269714355469
Batch 54/64 loss: 1.25443696975708
Batch 55/64 loss: 1.1147041320800781
Batch 56/64 loss: 1.699549674987793
Batch 57/64 loss: 1.0434298515319824
Batch 58/64 loss: 1.0321264266967773
Batch 59/64 loss: 1.3563122749328613
Batch 60/64 loss: 1.0678110122680664
Batch 61/64 loss: 0.9595003128051758
Batch 62/64 loss: 1.1138005256652832
Batch 63/64 loss: 1.8719682693481445
Batch 64/64 loss: -1.7964792251586914
Epoch 102  Train loss: 1.2552703221638997  Val loss: 1.2196807468060367
Epoch 103
-------------------------------
Batch 1/64 loss: 1.476426124572754
Batch 2/64 loss: 1.2770533561706543
Batch 3/64 loss: 1.0463595390319824
Batch 4/64 loss: 0.9151568412780762
Batch 5/64 loss: 1.4585270881652832
Batch 6/64 loss: 1.004267692565918
Batch 7/64 loss: 1.0242290496826172
Batch 8/64 loss: 1.257636547088623
Batch 9/64 loss: 1.3008136749267578
Batch 10/64 loss: 1.5440688133239746
Batch 11/64 loss: 1.1229662895202637
Batch 12/64 loss: 1.2317628860473633
Batch 13/64 loss: 1.0124659538269043
Batch 14/64 loss: 1.3421673774719238
Batch 15/64 loss: 1.2096657752990723
Batch 16/64 loss: 1.1691498756408691
Batch 17/64 loss: 1.3164305686950684
Batch 18/64 loss: 1.3883185386657715
Batch 19/64 loss: 1.5839333534240723
Batch 20/64 loss: 1.001986026763916
Batch 21/64 loss: 1.4112606048583984
Batch 22/64 loss: 1.3437657356262207
Batch 23/64 loss: 1.2155342102050781
Batch 24/64 loss: 1.1639151573181152
Batch 25/64 loss: 1.5210332870483398
Batch 26/64 loss: 1.1302990913391113
Batch 27/64 loss: 1.028937816619873
Batch 28/64 loss: 1.0353021621704102
Batch 29/64 loss: 1.049842357635498
Batch 30/64 loss: 1.1589455604553223
Batch 31/64 loss: 1.632643699645996
Batch 32/64 loss: 1.1054205894470215
Batch 33/64 loss: 0.872126579284668
Batch 34/64 loss: 1.468104362487793
Batch 35/64 loss: 1.363889217376709
Batch 36/64 loss: 1.0956201553344727
Batch 37/64 loss: 1.4168400764465332
Batch 38/64 loss: 1.0233325958251953
Batch 39/64 loss: 1.2004051208496094
Batch 40/64 loss: 1.147714614868164
Batch 41/64 loss: 1.3318710327148438
Batch 42/64 loss: 1.3141593933105469
Batch 43/64 loss: 1.544349193572998
Batch 44/64 loss: 1.3844141960144043
Batch 45/64 loss: 1.3262276649475098
Batch 46/64 loss: 0.9196391105651855
Batch 47/64 loss: 1.6152658462524414
Batch 48/64 loss: 1.053239345550537
Batch 49/64 loss: 1.4704432487487793
Batch 50/64 loss: 0.8779163360595703
Batch 51/64 loss: 1.2458558082580566
Batch 52/64 loss: 1.2396154403686523
Batch 53/64 loss: 1.3920722007751465
Batch 54/64 loss: 1.2063102722167969
Batch 55/64 loss: 1.1619501113891602
Batch 56/64 loss: 1.201209545135498
Batch 57/64 loss: 1.5807757377624512
Batch 58/64 loss: 1.4468460083007812
Batch 59/64 loss: 1.1374082565307617
Batch 60/64 loss: 1.25984525680542
Batch 61/64 loss: 1.2693428993225098
Batch 62/64 loss: 1.1245231628417969
Batch 63/64 loss: 1.0455718040466309
Batch 64/64 loss: -1.7804694175720215
Epoch 103  Train loss: 1.205989304710837  Val loss: 1.2123441007948412
Epoch 104
-------------------------------
Batch 1/64 loss: 1.0842409133911133
Batch 2/64 loss: 1.1003451347351074
Batch 3/64 loss: 1.308457851409912
Batch 4/64 loss: 1.2027406692504883
Batch 5/64 loss: 1.3477931022644043
Batch 6/64 loss: 1.1784687042236328
Batch 7/64 loss: 1.0465450286865234
Batch 8/64 loss: 1.1776447296142578
Batch 9/64 loss: 1.2238154411315918
Batch 10/64 loss: 1.201028823852539
Batch 11/64 loss: 1.3391623497009277
Batch 12/64 loss: 1.0338115692138672
Batch 13/64 loss: 1.0991544723510742
Batch 14/64 loss: 0.9178495407104492
Batch 15/64 loss: 0.8502411842346191
Batch 16/64 loss: 1.2434844970703125
Batch 17/64 loss: 1.1077513694763184
Batch 18/64 loss: 1.5272631645202637
Batch 19/64 loss: 1.3428263664245605
Batch 20/64 loss: 1.1399431228637695
Batch 21/64 loss: 1.1964802742004395
Batch 22/64 loss: 1.3650670051574707
Batch 23/64 loss: 1.2534198760986328
Batch 24/64 loss: 1.0877528190612793
Batch 25/64 loss: 1.3416829109191895
Batch 26/64 loss: 1.4426031112670898
Batch 27/64 loss: 1.2018070220947266
Batch 28/64 loss: 1.1274547576904297
Batch 29/64 loss: 1.3432321548461914
Batch 30/64 loss: 1.0327205657958984
Batch 31/64 loss: 0.9391236305236816
Batch 32/64 loss: 1.2198972702026367
Batch 33/64 loss: 1.203230857849121
Batch 34/64 loss: 1.200131893157959
Batch 35/64 loss: 0.9205894470214844
Batch 36/64 loss: 1.0207514762878418
Batch 37/64 loss: 1.1758661270141602
Batch 38/64 loss: 1.1211285591125488
Batch 39/64 loss: 1.0148487091064453
Batch 40/64 loss: 0.9717183113098145
Batch 41/64 loss: 1.3259711265563965
Batch 42/64 loss: 1.3783073425292969
Batch 43/64 loss: 0.9805765151977539
Batch 44/64 loss: 1.1957497596740723
Batch 45/64 loss: 1.376347541809082
Batch 46/64 loss: 0.9925212860107422
Batch 47/64 loss: 0.9415116310119629
Batch 48/64 loss: 1.14949369430542
Batch 49/64 loss: 1.0964412689208984
Batch 50/64 loss: 1.155479907989502
Batch 51/64 loss: 1.0130305290222168
Batch 52/64 loss: 0.7328095436096191
Batch 53/64 loss: 0.9179496765136719
Batch 54/64 loss: 0.7984857559204102
Batch 55/64 loss: 1.1401915550231934
Batch 56/64 loss: 1.721691608428955
Batch 57/64 loss: 1.1981120109558105
Batch 58/64 loss: 1.3570327758789062
Batch 59/64 loss: 1.6784648895263672
Batch 60/64 loss: 0.978205680847168
Batch 61/64 loss: 1.267838478088379
Batch 62/64 loss: 1.3117470741271973
Batch 63/64 loss: 1.1779074668884277
Batch 64/64 loss: -1.862708568572998
Epoch 104  Train loss: 1.1316221255882113  Val loss: 1.4616580714065184
Epoch 105
-------------------------------
Batch 1/64 loss: 1.3908743858337402
Batch 2/64 loss: 1.4293193817138672
Batch 3/64 loss: 0.8762693405151367
Batch 4/64 loss: 1.120471477508545
Batch 5/64 loss: 1.2158355712890625
Batch 6/64 loss: 0.774104118347168
Batch 7/64 loss: 1.4647607803344727
Batch 8/64 loss: 1.3940792083740234
Batch 9/64 loss: 1.2894034385681152
Batch 10/64 loss: 1.0407171249389648
Batch 11/64 loss: 1.4134039878845215
Batch 12/64 loss: 1.5136299133300781
Batch 13/64 loss: 1.316213607788086
Batch 14/64 loss: 1.366633415222168
Batch 15/64 loss: 0.7844729423522949
Batch 16/64 loss: 1.2443571090698242
Batch 17/64 loss: 1.281325340270996
Batch 18/64 loss: 1.079756259918213
Batch 19/64 loss: 1.3839678764343262
Batch 20/64 loss: 0.8214058876037598
Batch 21/64 loss: 0.9248690605163574
Batch 22/64 loss: 0.8998007774353027
Batch 23/64 loss: 1.2771010398864746
Batch 24/64 loss: 1.0503301620483398
Batch 25/64 loss: 1.6978254318237305
Batch 26/64 loss: 1.190286636352539
Batch 27/64 loss: 1.0458059310913086
Batch 28/64 loss: 1.180516242980957
Batch 29/64 loss: 1.377873420715332
Batch 30/64 loss: 0.97625732421875
Batch 31/64 loss: 1.2512946128845215
Batch 32/64 loss: 1.0235934257507324
Batch 33/64 loss: 1.0068473815917969
Batch 34/64 loss: 0.9393272399902344
Batch 35/64 loss: 1.0714025497436523
Batch 36/64 loss: 1.1579036712646484
Batch 37/64 loss: 1.36259126663208
Batch 38/64 loss: 1.22265625
Batch 39/64 loss: 1.276872158050537
Batch 40/64 loss: 0.853360652923584
Batch 41/64 loss: 0.8983211517333984
Batch 42/64 loss: 1.0743517875671387
Batch 43/64 loss: 1.1212420463562012
Batch 44/64 loss: 1.4544768333435059
Batch 45/64 loss: 1.050487995147705
Batch 46/64 loss: 1.2471542358398438
Batch 47/64 loss: 1.6891274452209473
Batch 48/64 loss: 1.145071029663086
Batch 49/64 loss: 1.2594218254089355
Batch 50/64 loss: 1.0750365257263184
Batch 51/64 loss: 1.0533595085144043
Batch 52/64 loss: 1.2902789115905762
Batch 53/64 loss: 1.3058862686157227
Batch 54/64 loss: 1.0294079780578613
Batch 55/64 loss: 1.1326565742492676
Batch 56/64 loss: 1.215590000152588
Batch 57/64 loss: 1.3275814056396484
Batch 58/64 loss: 1.4643259048461914
Batch 59/64 loss: 1.3552823066711426
Batch 60/64 loss: 1.1065731048583984
Batch 61/64 loss: 0.9582290649414062
Batch 62/64 loss: 1.1688408851623535
Batch 63/64 loss: 1.3441028594970703
Batch 64/64 loss: -1.9487905502319336
Epoch 105  Train loss: 1.149689868852204  Val loss: 1.129080749459283
Epoch 106
-------------------------------
Batch 1/64 loss: 1.1210436820983887
Batch 2/64 loss: 0.9908766746520996
Batch 3/64 loss: 0.9904818534851074
Batch 4/64 loss: 1.341689109802246
Batch 5/64 loss: 1.3400797843933105
Batch 6/64 loss: 1.484731674194336
Batch 7/64 loss: 0.9274368286132812
Batch 8/64 loss: 1.100034236907959
Batch 9/64 loss: 1.1528964042663574
Batch 10/64 loss: 0.9324960708618164
Batch 11/64 loss: 1.2665104866027832
Batch 12/64 loss: 0.8717255592346191
Batch 13/64 loss: 1.1189513206481934
Batch 14/64 loss: 1.1967015266418457
Batch 15/64 loss: 1.0277104377746582
Batch 16/64 loss: 1.1087217330932617
Batch 17/64 loss: 1.1454052925109863
Batch 18/64 loss: 0.8958425521850586
Batch 19/64 loss: 1.2009544372558594
Batch 20/64 loss: 1.231165885925293
Batch 21/64 loss: 1.3483381271362305
Batch 22/64 loss: 1.033440113067627
Batch 23/64 loss: 1.0307788848876953
Batch 24/64 loss: 1.0237269401550293
Batch 25/64 loss: 1.2294459342956543
Batch 26/64 loss: 1.000910758972168
Batch 27/64 loss: 1.1413021087646484
Batch 28/64 loss: 1.3236160278320312
Batch 29/64 loss: 1.2627654075622559
Batch 30/64 loss: 0.8626155853271484
Batch 31/64 loss: 0.8483047485351562
Batch 32/64 loss: 1.013817310333252
Batch 33/64 loss: 1.2543511390686035
Batch 34/64 loss: 1.2392582893371582
Batch 35/64 loss: 0.898350715637207
Batch 36/64 loss: 0.8180279731750488
Batch 37/64 loss: 1.057680606842041
Batch 38/64 loss: 1.6312174797058105
Batch 39/64 loss: 1.0098729133605957
Batch 40/64 loss: 1.0293502807617188
Batch 41/64 loss: 1.3091211318969727
Batch 42/64 loss: 1.0998868942260742
Batch 43/64 loss: 1.2897558212280273
Batch 44/64 loss: 1.4849214553833008
Batch 45/64 loss: 1.1794013977050781
Batch 46/64 loss: 0.9108161926269531
Batch 47/64 loss: 1.0397648811340332
Batch 48/64 loss: 1.3561110496520996
Batch 49/64 loss: 0.9810342788696289
Batch 50/64 loss: 1.2389497756958008
Batch 51/64 loss: 1.1632146835327148
Batch 52/64 loss: 1.602759838104248
Batch 53/64 loss: 1.1335477828979492
Batch 54/64 loss: 1.2158513069152832
Batch 55/64 loss: 1.0310592651367188
Batch 56/64 loss: 1.0253586769104004
Batch 57/64 loss: 1.4791712760925293
Batch 58/64 loss: 1.0727386474609375
Batch 59/64 loss: 1.1659622192382812
Batch 60/64 loss: 1.1641592979431152
Batch 61/64 loss: 0.585968017578125
Batch 62/64 loss: 1.4588065147399902
Batch 63/64 loss: 1.314328670501709
Batch 64/64 loss: -2.4865474700927734
Epoch 106  Train loss: 1.097104397942038  Val loss: 1.0959883555513887
Saving best model, epoch: 106
Epoch 107
-------------------------------
Batch 1/64 loss: 1.056593418121338
Batch 2/64 loss: 1.158057689666748
Batch 3/64 loss: 1.1773791313171387
Batch 4/64 loss: 1.2979483604431152
Batch 5/64 loss: 1.0354266166687012
Batch 6/64 loss: 1.252610683441162
Batch 7/64 loss: 1.2247648239135742
Batch 8/64 loss: 1.3964314460754395
Batch 9/64 loss: 1.3331093788146973
Batch 10/64 loss: 0.8913578987121582
Batch 11/64 loss: 1.1491279602050781
Batch 12/64 loss: 0.9873852729797363
Batch 13/64 loss: 0.771240234375
Batch 14/64 loss: 1.018259048461914
Batch 15/64 loss: 0.8438410758972168
Batch 16/64 loss: 1.0125174522399902
Batch 17/64 loss: 0.8507819175720215
Batch 18/64 loss: 1.0827956199645996
Batch 19/64 loss: 1.3459162712097168
Batch 20/64 loss: 0.8610382080078125
Batch 21/64 loss: 1.3145561218261719
Batch 22/64 loss: 0.8460931777954102
Batch 23/64 loss: 1.1485238075256348
Batch 24/64 loss: 1.1438841819763184
Batch 25/64 loss: 1.1898736953735352
Batch 26/64 loss: 1.0642848014831543
Batch 27/64 loss: 0.9017276763916016
Batch 28/64 loss: 1.3313989639282227
Batch 29/64 loss: 0.9438519477844238
Batch 30/64 loss: 1.358473777770996
Batch 31/64 loss: 1.1892223358154297
Batch 32/64 loss: 1.1733689308166504
Batch 33/64 loss: 0.9695491790771484
Batch 34/64 loss: 0.8205642700195312
Batch 35/64 loss: 1.1601996421813965
Batch 36/64 loss: 1.0832324028015137
Batch 37/64 loss: 1.3322315216064453
Batch 38/64 loss: 1.3678216934204102
Batch 39/64 loss: 1.3875017166137695
Batch 40/64 loss: 1.3209996223449707
Batch 41/64 loss: 1.260690689086914
Batch 42/64 loss: 1.085604190826416
Batch 43/64 loss: 0.8203091621398926
Batch 44/64 loss: 1.253209114074707
Batch 45/64 loss: 1.0060863494873047
Batch 46/64 loss: 0.9325265884399414
Batch 47/64 loss: 1.08128023147583
Batch 48/64 loss: 1.131883144378662
Batch 49/64 loss: 1.0481371879577637
Batch 50/64 loss: 0.9570951461791992
Batch 51/64 loss: 1.296201229095459
Batch 52/64 loss: 1.271000862121582
Batch 53/64 loss: 1.5690999031066895
Batch 54/64 loss: 1.0618391036987305
Batch 55/64 loss: 1.1948695182800293
Batch 56/64 loss: 0.7795600891113281
Batch 57/64 loss: 0.947232723236084
Batch 58/64 loss: 1.3921165466308594
Batch 59/64 loss: 1.3767571449279785
Batch 60/64 loss: 1.1531982421875
Batch 61/64 loss: 1.328195571899414
Batch 62/64 loss: 0.9871935844421387
Batch 63/64 loss: 1.2793421745300293
Batch 64/64 loss: -2.3401031494140625
Epoch 107  Train loss: 1.0863104801551968  Val loss: 1.2711433725258738
Epoch 108
-------------------------------
Batch 1/64 loss: 0.9696149826049805
Batch 2/64 loss: 0.9899301528930664
Batch 3/64 loss: 1.0288023948669434
Batch 4/64 loss: 1.1235594749450684
Batch 5/64 loss: 1.526534080505371
Batch 6/64 loss: 0.9349336624145508
Batch 7/64 loss: 1.3047833442687988
Batch 8/64 loss: 1.2587623596191406
Batch 9/64 loss: 1.0968313217163086
Batch 10/64 loss: 1.1930408477783203
Batch 11/64 loss: 1.0227470397949219
Batch 12/64 loss: 1.312868595123291
Batch 13/64 loss: 1.3929805755615234
Batch 14/64 loss: 0.9937005043029785
Batch 15/64 loss: 1.5827527046203613
Batch 16/64 loss: 0.9805498123168945
Batch 17/64 loss: 1.29941987991333
Batch 18/64 loss: 1.0057554244995117
Batch 19/64 loss: 1.049257755279541
Batch 20/64 loss: 1.0114970207214355
Batch 21/64 loss: 1.0652046203613281
Batch 22/64 loss: 1.1552386283874512
Batch 23/64 loss: 0.8416528701782227
Batch 24/64 loss: 1.0872011184692383
Batch 25/64 loss: 0.8396782875061035
Batch 26/64 loss: 0.8044743537902832
Batch 27/64 loss: 1.015716552734375
Batch 28/64 loss: 1.0306792259216309
Batch 29/64 loss: 0.9791154861450195
Batch 30/64 loss: 1.1294207572937012
Batch 31/64 loss: 1.2727360725402832
Batch 32/64 loss: 1.291684627532959
Batch 33/64 loss: 0.785336971282959
Batch 34/64 loss: 1.2688369750976562
Batch 35/64 loss: 1.1592907905578613
Batch 36/64 loss: 0.9227700233459473
Batch 37/64 loss: 0.9909000396728516
Batch 38/64 loss: 1.1019339561462402
Batch 39/64 loss: 0.8993539810180664
Batch 40/64 loss: 1.2558331489562988
Batch 41/64 loss: 0.9339232444763184
Batch 42/64 loss: 0.7452983856201172
Batch 43/64 loss: 1.1852750778198242
Batch 44/64 loss: 1.3100314140319824
Batch 45/64 loss: 1.3988356590270996
Batch 46/64 loss: 1.5862226486206055
Batch 47/64 loss: 0.8407378196716309
Batch 48/64 loss: 1.0636720657348633
Batch 49/64 loss: 1.1733417510986328
Batch 50/64 loss: 0.9991135597229004
Batch 51/64 loss: 0.7953648567199707
Batch 52/64 loss: 0.947960376739502
Batch 53/64 loss: 1.2221097946166992
Batch 54/64 loss: 0.878727912902832
Batch 55/64 loss: 1.0661239624023438
Batch 56/64 loss: 1.1269927024841309
Batch 57/64 loss: 1.2394461631774902
Batch 58/64 loss: 1.028998851776123
Batch 59/64 loss: 1.3370203971862793
Batch 60/64 loss: 1.0006484985351562
Batch 61/64 loss: 1.033928394317627
Batch 62/64 loss: 1.1093978881835938
Batch 63/64 loss: 1.0413289070129395
Batch 64/64 loss: -2.298363208770752
Epoch 108  Train loss: 1.05593895444683  Val loss: 1.240114854373473
Epoch 109
-------------------------------
Batch 1/64 loss: 0.937352180480957
Batch 2/64 loss: 1.3921403884887695
Batch 3/64 loss: 1.1463046073913574
Batch 4/64 loss: 1.0856328010559082
Batch 5/64 loss: 1.2967987060546875
Batch 6/64 loss: 2.088742256164551
Batch 7/64 loss: 1.0462150573730469
Batch 8/64 loss: 1.0547900199890137
Batch 9/64 loss: 1.6470403671264648
Batch 10/64 loss: 1.283498764038086
Batch 11/64 loss: 1.0836091041564941
Batch 12/64 loss: 0.9675235748291016
Batch 13/64 loss: 1.3432502746582031
Batch 14/64 loss: 1.2582902908325195
Batch 15/64 loss: 0.780205249786377
Batch 16/64 loss: 0.9972710609436035
Batch 17/64 loss: 1.1342353820800781
Batch 18/64 loss: 1.1593832969665527
Batch 19/64 loss: 1.256901741027832
Batch 20/64 loss: 1.1034650802612305
Batch 21/64 loss: 1.138646125793457
Batch 22/64 loss: 1.2414922714233398
Batch 23/64 loss: 1.340780258178711
Batch 24/64 loss: 1.2327280044555664
Batch 25/64 loss: 1.1527762413024902
Batch 26/64 loss: 0.9619569778442383
Batch 27/64 loss: 1.1032018661499023
Batch 28/64 loss: 1.1866068840026855
Batch 29/64 loss: 1.091914176940918
Batch 30/64 loss: 1.353494644165039
Batch 31/64 loss: 1.1118144989013672
Batch 32/64 loss: 1.4168171882629395
Batch 33/64 loss: 0.996180534362793
Batch 34/64 loss: 1.183089256286621
Batch 35/64 loss: 1.0037341117858887
Batch 36/64 loss: 1.0631623268127441
Batch 37/64 loss: 1.0007147789001465
Batch 38/64 loss: 1.2568936347961426
Batch 39/64 loss: 1.0122785568237305
Batch 40/64 loss: 1.1054167747497559
Batch 41/64 loss: 1.127103328704834
Batch 42/64 loss: 0.9410810470581055
Batch 43/64 loss: 0.8254399299621582
Batch 44/64 loss: 1.2740345001220703
Batch 45/64 loss: 1.1909570693969727
Batch 46/64 loss: 1.1268835067749023
Batch 47/64 loss: 0.7653617858886719
Batch 48/64 loss: 1.2489166259765625
Batch 49/64 loss: 1.0252666473388672
Batch 50/64 loss: 0.8701300621032715
Batch 51/64 loss: 0.6008005142211914
Batch 52/64 loss: 1.0236682891845703
Batch 53/64 loss: 0.7961578369140625
Batch 54/64 loss: 1.0981836318969727
Batch 55/64 loss: 0.9975724220275879
Batch 56/64 loss: 0.8094801902770996
Batch 57/64 loss: 1.2167701721191406
Batch 58/64 loss: 1.5755500793457031
Batch 59/64 loss: 1.140589714050293
Batch 60/64 loss: 1.061288833618164
Batch 61/64 loss: 1.391800880432129
Batch 62/64 loss: 1.1906838417053223
Batch 63/64 loss: 1.0271835327148438
Batch 64/64 loss: -2.2971134185791016
Epoch 109  Train loss: 1.0920536265653722  Val loss: 1.0607904126144356
Saving best model, epoch: 109
Epoch 110
-------------------------------
Batch 1/64 loss: 1.1403546333312988
Batch 2/64 loss: 1.081164836883545
Batch 3/64 loss: 0.8106989860534668
Batch 4/64 loss: 1.0239458084106445
Batch 5/64 loss: 1.1829018592834473
Batch 6/64 loss: 0.9259209632873535
Batch 7/64 loss: 0.9995012283325195
Batch 8/64 loss: 0.7930378913879395
Batch 9/64 loss: 1.3493895530700684
Batch 10/64 loss: 1.2264957427978516
Batch 11/64 loss: 0.9856100082397461
Batch 12/64 loss: 0.953300952911377
Batch 13/64 loss: 0.8356261253356934
Batch 14/64 loss: 1.073410987854004
Batch 15/64 loss: 1.1011161804199219
Batch 16/64 loss: 0.9287996292114258
Batch 17/64 loss: 1.3058409690856934
Batch 18/64 loss: 0.89593505859375
Batch 19/64 loss: 0.854248046875
Batch 20/64 loss: 0.9134297370910645
Batch 21/64 loss: 1.0322375297546387
Batch 22/64 loss: 1.178117275238037
Batch 23/64 loss: 1.0835747718811035
Batch 24/64 loss: 0.997955322265625
Batch 25/64 loss: 0.9782686233520508
Batch 26/64 loss: 1.0384130477905273
Batch 27/64 loss: 1.050112247467041
Batch 28/64 loss: 0.8208069801330566
Batch 29/64 loss: 0.9122805595397949
Batch 30/64 loss: 0.8656539916992188
Batch 31/64 loss: 1.332737922668457
Batch 32/64 loss: 1.0894432067871094
Batch 33/64 loss: 0.9669256210327148
Batch 34/64 loss: 1.125053882598877
Batch 35/64 loss: 1.2730937004089355
Batch 36/64 loss: 0.6743965148925781
Batch 37/64 loss: 1.292036533355713
Batch 38/64 loss: 1.1305222511291504
Batch 39/64 loss: 1.130441665649414
Batch 40/64 loss: 0.684302806854248
Batch 41/64 loss: 1.015096664428711
Batch 42/64 loss: 1.1677803993225098
Batch 43/64 loss: 1.2060213088989258
Batch 44/64 loss: 0.9985251426696777
Batch 45/64 loss: 0.9644594192504883
Batch 46/64 loss: 0.8911900520324707
Batch 47/64 loss: 1.1700329780578613
Batch 48/64 loss: 1.0089106559753418
Batch 49/64 loss: 1.1938042640686035
Batch 50/64 loss: 1.106752872467041
Batch 51/64 loss: 1.1543140411376953
Batch 52/64 loss: 1.473991870880127
Batch 53/64 loss: 1.2362627983093262
Batch 54/64 loss: 1.2172784805297852
Batch 55/64 loss: 1.6440892219543457
Batch 56/64 loss: 1.226818561553955
Batch 57/64 loss: 1.081782341003418
Batch 58/64 loss: 1.0875611305236816
Batch 59/64 loss: 0.9051752090454102
Batch 60/64 loss: 0.9804210662841797
Batch 61/64 loss: 0.8109273910522461
Batch 62/64 loss: 1.0708322525024414
Batch 63/64 loss: 1.2972712516784668
Batch 64/64 loss: -1.8190550804138184
Epoch 110  Train loss: 1.0287390073140463  Val loss: 1.5842291317444896
Epoch 111
-------------------------------
Batch 1/64 loss: 1.4453697204589844
Batch 2/64 loss: 1.1331167221069336
Batch 3/64 loss: 1.1960391998291016
Batch 4/64 loss: 1.3317432403564453
Batch 5/64 loss: 1.040745735168457
Batch 6/64 loss: 1.2147197723388672
Batch 7/64 loss: 1.6471576690673828
Batch 8/64 loss: 0.8689332008361816
Batch 9/64 loss: 1.2036166191101074
Batch 10/64 loss: 1.0040297508239746
Batch 11/64 loss: 1.392866611480713
Batch 12/64 loss: 1.4179601669311523
Batch 13/64 loss: 1.1252989768981934
Batch 14/64 loss: 1.1109066009521484
Batch 15/64 loss: 1.2459158897399902
Batch 16/64 loss: 1.3165464401245117
Batch 17/64 loss: 1.4500536918640137
Batch 18/64 loss: 1.0952496528625488
Batch 19/64 loss: 1.5136432647705078
Batch 20/64 loss: 1.3928070068359375
Batch 21/64 loss: 1.1691746711730957
Batch 22/64 loss: 1.1650681495666504
Batch 23/64 loss: 1.127640724182129
Batch 24/64 loss: 1.196012020111084
Batch 25/64 loss: 1.8022408485412598
Batch 26/64 loss: 1.0723767280578613
Batch 27/64 loss: 1.1482973098754883
Batch 28/64 loss: 1.2257843017578125
Batch 29/64 loss: 1.0620694160461426
Batch 30/64 loss: 1.1138029098510742
Batch 31/64 loss: 1.0830936431884766
Batch 32/64 loss: 1.3775906562805176
Batch 33/64 loss: 1.5572395324707031
Batch 34/64 loss: 1.3694701194763184
Batch 35/64 loss: 1.1529817581176758
Batch 36/64 loss: 1.1057252883911133
Batch 37/64 loss: 1.1025032997131348
Batch 38/64 loss: 1.4147381782531738
Batch 39/64 loss: 1.0593757629394531
Batch 40/64 loss: 1.238661289215088
Batch 41/64 loss: 1.0806045532226562
Batch 42/64 loss: 1.243393898010254
Batch 43/64 loss: 0.9174981117248535
Batch 44/64 loss: 1.8476333618164062
Batch 45/64 loss: 0.8945474624633789
Batch 46/64 loss: 1.4464712142944336
Batch 47/64 loss: 1.2075018882751465
Batch 48/64 loss: 1.0403285026550293
Batch 49/64 loss: 1.3290448188781738
Batch 50/64 loss: 1.0473861694335938
Batch 51/64 loss: 0.9809226989746094
Batch 52/64 loss: 1.4586162567138672
Batch 53/64 loss: 1.0686726570129395
Batch 54/64 loss: 0.979212760925293
Batch 55/64 loss: 1.2553329467773438
Batch 56/64 loss: 1.2480664253234863
Batch 57/64 loss: 1.6517705917358398
Batch 58/64 loss: 1.250537395477295
Batch 59/64 loss: 0.8104596138000488
Batch 60/64 loss: 0.8704156875610352
Batch 61/64 loss: 0.9536395072937012
Batch 62/64 loss: 1.3278107643127441
Batch 63/64 loss: 0.9062223434448242
Batch 64/64 loss: -2.2217769622802734
Epoch 111  Train loss: 1.1739658580106846  Val loss: 1.0401300908773625
Saving best model, epoch: 111
Epoch 112
-------------------------------
Batch 1/64 loss: 1.0884923934936523
Batch 2/64 loss: 1.2352633476257324
Batch 3/64 loss: 1.4360928535461426
Batch 4/64 loss: 1.0056633949279785
Batch 5/64 loss: 1.0864291191101074
Batch 6/64 loss: 1.0326967239379883
Batch 7/64 loss: 1.1152124404907227
Batch 8/64 loss: 1.1063518524169922
Batch 9/64 loss: 0.7204594612121582
Batch 10/64 loss: 0.8518991470336914
Batch 11/64 loss: 0.8842802047729492
Batch 12/64 loss: 0.9255609512329102
Batch 13/64 loss: 1.1085577011108398
Batch 14/64 loss: 0.8834714889526367
Batch 15/64 loss: 0.971837043762207
Batch 16/64 loss: 0.699256420135498
Batch 17/64 loss: 1.2658205032348633
Batch 18/64 loss: 1.0749974250793457
Batch 19/64 loss: 0.8981027603149414
Batch 20/64 loss: 0.9415440559387207
Batch 21/64 loss: 0.6744775772094727
Batch 22/64 loss: 1.2569451332092285
Batch 23/64 loss: 0.8185162544250488
Batch 24/64 loss: 0.814579963684082
Batch 25/64 loss: 1.0327682495117188
Batch 26/64 loss: 1.066420078277588
Batch 27/64 loss: 1.1413183212280273
Batch 28/64 loss: 1.2367100715637207
Batch 29/64 loss: 0.7799034118652344
Batch 30/64 loss: 0.8651123046875
Batch 31/64 loss: 0.665438175201416
Batch 32/64 loss: 1.2080039978027344
Batch 33/64 loss: 1.0186924934387207
Batch 34/64 loss: 1.3766322135925293
Batch 35/64 loss: 0.8110284805297852
Batch 36/64 loss: 1.3628854751586914
Batch 37/64 loss: 1.1646628379821777
Batch 38/64 loss: 1.1109800338745117
Batch 39/64 loss: 1.0567402839660645
Batch 40/64 loss: 0.9420552253723145
Batch 41/64 loss: 1.2325496673583984
Batch 42/64 loss: 1.2623963356018066
Batch 43/64 loss: 0.8931102752685547
Batch 44/64 loss: 1.340498924255371
Batch 45/64 loss: 0.8180956840515137
Batch 46/64 loss: 1.088045597076416
Batch 47/64 loss: 0.9742426872253418
Batch 48/64 loss: 1.1105551719665527
Batch 49/64 loss: 1.2212028503417969
Batch 50/64 loss: 0.7620391845703125
Batch 51/64 loss: 1.0401396751403809
Batch 52/64 loss: 1.0199847221374512
Batch 53/64 loss: 1.4807558059692383
Batch 54/64 loss: 1.2503037452697754
Batch 55/64 loss: 0.9444313049316406
Batch 56/64 loss: 0.7654013633728027
Batch 57/64 loss: 1.29231595993042
Batch 58/64 loss: 1.2929649353027344
Batch 59/64 loss: 1.6011357307434082
Batch 60/64 loss: 0.8854665756225586
Batch 61/64 loss: 1.2205238342285156
Batch 62/64 loss: 1.0389881134033203
Batch 63/64 loss: 1.4881482124328613
Batch 64/64 loss: -2.196706771850586
Epoch 112  Train loss: 1.0213588415407666  Val loss: 1.2335244339356308
Epoch 113
-------------------------------
Batch 1/64 loss: 0.9090781211853027
Batch 2/64 loss: 1.0768451690673828
Batch 3/64 loss: 1.619758129119873
Batch 4/64 loss: 0.9312644004821777
Batch 5/64 loss: 0.954315185546875
Batch 6/64 loss: 1.2766304016113281
Batch 7/64 loss: 1.0475316047668457
Batch 8/64 loss: 1.064833164215088
Batch 9/64 loss: 0.9718642234802246
Batch 10/64 loss: 1.2678985595703125
Batch 11/64 loss: 0.8982305526733398
Batch 12/64 loss: 1.1925888061523438
Batch 13/64 loss: 1.1045265197753906
Batch 14/64 loss: 0.9877638816833496
Batch 15/64 loss: 1.2343049049377441
Batch 16/64 loss: 0.9307241439819336
Batch 17/64 loss: 1.1998391151428223
Batch 18/64 loss: 1.0857996940612793
Batch 19/64 loss: 1.0527219772338867
Batch 20/64 loss: 1.5393805503845215
Batch 21/64 loss: 1.3533148765563965
Batch 22/64 loss: 0.9686150550842285
Batch 23/64 loss: 0.8226194381713867
Batch 24/64 loss: 0.8609676361083984
Batch 25/64 loss: 1.113901138305664
Batch 26/64 loss: 0.9788336753845215
Batch 27/64 loss: 1.5172410011291504
Batch 28/64 loss: 1.3136415481567383
Batch 29/64 loss: 0.9988799095153809
Batch 30/64 loss: 1.0306172370910645
Batch 31/64 loss: 0.9114227294921875
Batch 32/64 loss: 1.1232051849365234
Batch 33/64 loss: 1.0653471946716309
Batch 34/64 loss: 1.062720775604248
Batch 35/64 loss: 1.2331905364990234
Batch 36/64 loss: 0.9041166305541992
Batch 37/64 loss: 1.1179795265197754
Batch 38/64 loss: 1.1507325172424316
Batch 39/64 loss: 0.863563060760498
Batch 40/64 loss: 0.7580132484436035
Batch 41/64 loss: 1.316558837890625
Batch 42/64 loss: 1.5389432907104492
Batch 43/64 loss: 1.2079615592956543
Batch 44/64 loss: 0.9518046379089355
Batch 45/64 loss: 0.8957357406616211
Batch 46/64 loss: 1.0520682334899902
Batch 47/64 loss: 0.8876237869262695
Batch 48/64 loss: 0.9771862030029297
Batch 49/64 loss: 0.9640469551086426
Batch 50/64 loss: 1.088874340057373
Batch 51/64 loss: 1.1937203407287598
Batch 52/64 loss: 1.2349128723144531
Batch 53/64 loss: 1.078260898590088
Batch 54/64 loss: 1.1178617477416992
Batch 55/64 loss: 0.8026857376098633
Batch 56/64 loss: 1.128772258758545
Batch 57/64 loss: 0.707122802734375
Batch 58/64 loss: 1.0849223136901855
Batch 59/64 loss: 1.0692996978759766
Batch 60/64 loss: 1.0751533508300781
Batch 61/64 loss: 0.755427360534668
Batch 62/64 loss: 0.8764886856079102
Batch 63/64 loss: 0.7526340484619141
Batch 64/64 loss: -2.1187233924865723
Epoch 113  Train loss: 1.0300211008857278  Val loss: 1.0440075012416774
Epoch 114
-------------------------------
Batch 1/64 loss: 0.8177957534790039
Batch 2/64 loss: 1.4099693298339844
Batch 3/64 loss: 0.8182330131530762
Batch 4/64 loss: 0.7741389274597168
Batch 5/64 loss: 0.6964702606201172
Batch 6/64 loss: 1.2933874130249023
Batch 7/64 loss: 0.6509661674499512
Batch 8/64 loss: 1.050410270690918
Batch 9/64 loss: 0.9398283958435059
Batch 10/64 loss: 0.8934526443481445
Batch 11/64 loss: 0.782109260559082
Batch 12/64 loss: 0.5919380187988281
Batch 13/64 loss: 1.2925376892089844
Batch 14/64 loss: 1.413055419921875
Batch 15/64 loss: 1.0142183303833008
Batch 16/64 loss: 0.9690151214599609
Batch 17/64 loss: 0.9647402763366699
Batch 18/64 loss: 1.2898964881896973
Batch 19/64 loss: 1.0084900856018066
Batch 20/64 loss: 1.1159892082214355
Batch 21/64 loss: 0.8949723243713379
Batch 22/64 loss: 1.1096162796020508
Batch 23/64 loss: 0.8842225074768066
Batch 24/64 loss: 0.8739843368530273
Batch 25/64 loss: 1.0415043830871582
Batch 26/64 loss: 1.2639636993408203
Batch 27/64 loss: 1.2166686058044434
Batch 28/64 loss: 1.3824992179870605
Batch 29/64 loss: 0.6849050521850586
Batch 30/64 loss: 1.147148609161377
Batch 31/64 loss: 0.8808608055114746
Batch 32/64 loss: 1.0835881233215332
Batch 33/64 loss: 1.2780580520629883
Batch 34/64 loss: 1.1324386596679688
Batch 35/64 loss: 1.0302300453186035
Batch 36/64 loss: 0.9031558036804199
Batch 37/64 loss: 0.954167366027832
Batch 38/64 loss: 1.0353689193725586
Batch 39/64 loss: 1.0201373100280762
Batch 40/64 loss: 0.8584723472595215
Batch 41/64 loss: 0.7521233558654785
Batch 42/64 loss: 1.0856175422668457
Batch 43/64 loss: 0.8758678436279297
Batch 44/64 loss: 0.8679141998291016
Batch 45/64 loss: 0.6752614974975586
Batch 46/64 loss: 1.3415875434875488
Batch 47/64 loss: 0.939692497253418
Batch 48/64 loss: 1.1736888885498047
Batch 49/64 loss: 0.8195409774780273
Batch 50/64 loss: 0.7651352882385254
Batch 51/64 loss: 1.0753846168518066
Batch 52/64 loss: 0.7621579170227051
Batch 53/64 loss: 0.8616914749145508
Batch 54/64 loss: 0.855292797088623
Batch 55/64 loss: 0.6254129409790039
Batch 56/64 loss: 1.0452399253845215
Batch 57/64 loss: 0.9835643768310547
Batch 58/64 loss: 1.301154613494873
Batch 59/64 loss: 1.0806636810302734
Batch 60/64 loss: 0.9797143936157227
Batch 61/64 loss: 0.990382194519043
Batch 62/64 loss: 1.0788073539733887
Batch 63/64 loss: 1.2963175773620605
Batch 64/64 loss: -2.5502142906188965
Epoch 114  Train loss: 0.9534456047357297  Val loss: 1.0937822086294902
Epoch 115
-------------------------------
Batch 1/64 loss: 1.3078351020812988
Batch 2/64 loss: 1.0038542747497559
Batch 3/64 loss: 0.8738503456115723
Batch 4/64 loss: 0.892305850982666
Batch 5/64 loss: 0.7302432060241699
Batch 6/64 loss: 0.9292654991149902
Batch 7/64 loss: 1.0353264808654785
Batch 8/64 loss: 0.7523341178894043
Batch 9/64 loss: 0.970003604888916
Batch 10/64 loss: 1.292820930480957
Batch 11/64 loss: 0.7606043815612793
Batch 12/64 loss: 0.9126944541931152
Batch 13/64 loss: 1.3325486183166504
Batch 14/64 loss: 1.155466079711914
Batch 15/64 loss: 1.0799064636230469
Batch 16/64 loss: 0.7986440658569336
Batch 17/64 loss: 0.9262495040893555
Batch 18/64 loss: 1.0552024841308594
Batch 19/64 loss: 0.8087596893310547
Batch 20/64 loss: 0.8120322227478027
Batch 21/64 loss: 0.8771295547485352
Batch 22/64 loss: 0.709681510925293
Batch 23/64 loss: 1.1658282279968262
Batch 24/64 loss: 0.8159070014953613
Batch 25/64 loss: 1.1038179397583008
Batch 26/64 loss: 1.0889959335327148
Batch 27/64 loss: 1.0956473350524902
Batch 28/64 loss: 0.9988079071044922
Batch 29/64 loss: 1.150075912475586
Batch 30/64 loss: 1.0501775741577148
Batch 31/64 loss: 0.6019563674926758
Batch 32/64 loss: 0.9649243354797363
Batch 33/64 loss: 0.7290797233581543
Batch 34/64 loss: 1.1340460777282715
Batch 35/64 loss: 0.7637348175048828
Batch 36/64 loss: 0.8825922012329102
Batch 37/64 loss: 0.6783199310302734
Batch 38/64 loss: 1.9164328575134277
Batch 39/64 loss: 1.3690786361694336
Batch 40/64 loss: 0.6572761535644531
Batch 41/64 loss: 0.9258627891540527
Batch 42/64 loss: 1.0597467422485352
Batch 43/64 loss: 0.8151755332946777
Batch 44/64 loss: 0.7645668983459473
Batch 45/64 loss: 0.7801451683044434
Batch 46/64 loss: 0.828519344329834
Batch 47/64 loss: 0.7663350105285645
Batch 48/64 loss: 0.9025349617004395
Batch 49/64 loss: 1.1507182121276855
Batch 50/64 loss: 1.2257862091064453
Batch 51/64 loss: 1.0804781913757324
Batch 52/64 loss: 1.1503005027770996
Batch 53/64 loss: 1.0453410148620605
Batch 54/64 loss: 1.164107322692871
Batch 55/64 loss: 1.1905755996704102
Batch 56/64 loss: 0.9448952674865723
Batch 57/64 loss: 1.1587262153625488
Batch 58/64 loss: 0.9034380912780762
Batch 59/64 loss: 0.8676943778991699
Batch 60/64 loss: 0.9578161239624023
Batch 61/64 loss: 1.0192914009094238
Batch 62/64 loss: 1.1983356475830078
Batch 63/64 loss: 0.7902255058288574
Batch 64/64 loss: -2.126091480255127
Epoch 115  Train loss: 0.9455608611013375  Val loss: 1.0329683179298217
Saving best model, epoch: 115
Epoch 116
-------------------------------
Batch 1/64 loss: 1.0939083099365234
Batch 2/64 loss: 1.0415215492248535
Batch 3/64 loss: 0.9070234298706055
Batch 4/64 loss: 1.2501373291015625
Batch 5/64 loss: 1.6969718933105469
Batch 6/64 loss: 0.757972240447998
Batch 7/64 loss: 1.1115355491638184
Batch 8/64 loss: 1.126509666442871
Batch 9/64 loss: 1.1583890914916992
Batch 10/64 loss: 1.0552325248718262
Batch 11/64 loss: 0.805748462677002
Batch 12/64 loss: 0.8180017471313477
Batch 13/64 loss: 1.305922031402588
Batch 14/64 loss: 0.8008451461791992
Batch 15/64 loss: 1.1674432754516602
Batch 16/64 loss: 1.123537540435791
Batch 17/64 loss: 1.1181011199951172
Batch 18/64 loss: 1.1563425064086914
Batch 19/64 loss: 0.9872341156005859
Batch 20/64 loss: 0.720362663269043
Batch 21/64 loss: 1.3874177932739258
Batch 22/64 loss: 0.7005047798156738
Batch 23/64 loss: 1.284886360168457
Batch 24/64 loss: 0.8659448623657227
Batch 25/64 loss: 0.6664156913757324
Batch 26/64 loss: 0.7512168884277344
Batch 27/64 loss: 1.3221888542175293
Batch 28/64 loss: 1.1368284225463867
Batch 29/64 loss: 1.063232421875
Batch 30/64 loss: 0.7497434616088867
Batch 31/64 loss: 0.74505615234375
Batch 32/64 loss: 1.2570667266845703
Batch 33/64 loss: 0.9682331085205078
Batch 34/64 loss: 1.2125015258789062
Batch 35/64 loss: 0.912966251373291
Batch 36/64 loss: 0.9598493576049805
Batch 37/64 loss: 1.0194659233093262
Batch 38/64 loss: 1.0576043128967285
Batch 39/64 loss: 1.1135239601135254
Batch 40/64 loss: 1.2895498275756836
Batch 41/64 loss: 0.6535444259643555
Batch 42/64 loss: 0.5466961860656738
Batch 43/64 loss: 0.8722944259643555
Batch 44/64 loss: 0.8828277587890625
Batch 45/64 loss: 1.0855579376220703
Batch 46/64 loss: 0.6910381317138672
Batch 47/64 loss: 1.3553123474121094
Batch 48/64 loss: 1.4112262725830078
Batch 49/64 loss: 0.8773760795593262
Batch 50/64 loss: 0.413999080657959
Batch 51/64 loss: 0.7066411972045898
Batch 52/64 loss: 0.9539055824279785
Batch 53/64 loss: 0.9798736572265625
Batch 54/64 loss: 0.9283175468444824
Batch 55/64 loss: 1.0215764045715332
Batch 56/64 loss: 0.9107975959777832
Batch 57/64 loss: 0.7980875968933105
Batch 58/64 loss: 0.7772965431213379
Batch 59/64 loss: 0.7592787742614746
Batch 60/64 loss: 0.7084102630615234
Batch 61/64 loss: 0.9561185836791992
Batch 62/64 loss: 1.1427063941955566
Batch 63/64 loss: 0.7658224105834961
Batch 64/64 loss: -2.37459659576416
Epoch 116  Train loss: 0.9425050137089748  Val loss: 0.9495350552588394
Saving best model, epoch: 116
Epoch 117
-------------------------------
Batch 1/64 loss: 0.8416008949279785
Batch 2/64 loss: 1.0990824699401855
Batch 3/64 loss: 1.0501770973205566
Batch 4/64 loss: 0.7159852981567383
Batch 5/64 loss: 0.8787736892700195
Batch 6/64 loss: 1.0704736709594727
Batch 7/64 loss: 1.1686129570007324
Batch 8/64 loss: 0.8826150894165039
Batch 9/64 loss: 0.7705564498901367
Batch 10/64 loss: 0.6303458213806152
Batch 11/64 loss: 0.9781308174133301
Batch 12/64 loss: 0.993403434753418
Batch 13/64 loss: 1.2551703453063965
Batch 14/64 loss: 0.9390072822570801
Batch 15/64 loss: 0.8981804847717285
Batch 16/64 loss: 0.8417925834655762
Batch 17/64 loss: 1.281982421875
Batch 18/64 loss: 0.7810039520263672
Batch 19/64 loss: 0.9960203170776367
Batch 20/64 loss: 0.9092745780944824
Batch 21/64 loss: 0.9834752082824707
Batch 22/64 loss: 0.9387507438659668
Batch 23/64 loss: 1.2402896881103516
Batch 24/64 loss: 1.4024457931518555
Batch 25/64 loss: 1.331183910369873
Batch 26/64 loss: 0.8131775856018066
Batch 27/64 loss: 1.0364770889282227
Batch 28/64 loss: 1.2432656288146973
Batch 29/64 loss: 0.8889374732971191
Batch 30/64 loss: 1.137491226196289
Batch 31/64 loss: 0.6142921447753906
Batch 32/64 loss: 0.6862006187438965
Batch 33/64 loss: 1.4633183479309082
Batch 34/64 loss: 0.6010885238647461
Batch 35/64 loss: 1.274099349975586
Batch 36/64 loss: 0.8189482688903809
Batch 37/64 loss: 0.7732968330383301
Batch 38/64 loss: 0.8826918601989746
Batch 39/64 loss: 0.778897762298584
Batch 40/64 loss: 0.9160375595092773
Batch 41/64 loss: 0.9344797134399414
Batch 42/64 loss: 0.7799873352050781
Batch 43/64 loss: 0.822911262512207
Batch 44/64 loss: 0.9619674682617188
Batch 45/64 loss: 0.9376974105834961
Batch 46/64 loss: 0.819737434387207
Batch 47/64 loss: 0.9609169960021973
Batch 48/64 loss: 0.9369540214538574
Batch 49/64 loss: 0.8385229110717773
Batch 50/64 loss: 0.950406551361084
Batch 51/64 loss: 0.8838520050048828
Batch 52/64 loss: 0.8925657272338867
Batch 53/64 loss: 0.7353448867797852
Batch 54/64 loss: 1.044468879699707
Batch 55/64 loss: 0.9070982933044434
Batch 56/64 loss: 0.7427759170532227
Batch 57/64 loss: 0.8506217002868652
Batch 58/64 loss: 1.1449623107910156
Batch 59/64 loss: 0.7588753700256348
Batch 60/64 loss: 0.790104866027832
Batch 61/64 loss: 0.8642845153808594
Batch 62/64 loss: 1.2899742126464844
Batch 63/64 loss: 0.8218121528625488
Batch 64/64 loss: 0.047092437744140625
Epoch 117  Train loss: 0.9335246516209023  Val loss: 1.1093088064816399
Epoch 118
-------------------------------
Batch 1/64 loss: 1.0773334503173828
Batch 2/64 loss: 0.882636547088623
Batch 3/64 loss: 0.7782273292541504
Batch 4/64 loss: 1.0044984817504883
Batch 5/64 loss: 1.1161918640136719
Batch 6/64 loss: 1.0562686920166016
Batch 7/64 loss: 0.9204754829406738
Batch 8/64 loss: 0.940673828125
Batch 9/64 loss: 1.243664264678955
Batch 10/64 loss: 1.2571067810058594
Batch 11/64 loss: 0.7292289733886719
Batch 12/64 loss: 0.9058818817138672
Batch 13/64 loss: 0.8535995483398438
Batch 14/64 loss: 1.01434326171875
Batch 15/64 loss: 1.1597256660461426
Batch 16/64 loss: 0.6612434387207031
Batch 17/64 loss: 0.8644165992736816
Batch 18/64 loss: 1.1274909973144531
Batch 19/64 loss: 0.9895663261413574
Batch 20/64 loss: 1.2331366539001465
Batch 21/64 loss: 1.228508472442627
Batch 22/64 loss: 1.1788363456726074
Batch 23/64 loss: 0.9178047180175781
Batch 24/64 loss: 0.6493768692016602
Batch 25/64 loss: 1.131986141204834
Batch 26/64 loss: 1.3221893310546875
Batch 27/64 loss: 0.8745260238647461
Batch 28/64 loss: 0.9092435836791992
Batch 29/64 loss: 0.9092550277709961
Batch 30/64 loss: 0.9720797538757324
Batch 31/64 loss: 1.3723969459533691
Batch 32/64 loss: 0.9236860275268555
Batch 33/64 loss: 0.7505769729614258
Batch 34/64 loss: 1.1727313995361328
Batch 35/64 loss: 1.4619598388671875
Batch 36/64 loss: 1.1219744682312012
Batch 37/64 loss: 1.125260829925537
Batch 38/64 loss: 1.3250112533569336
Batch 39/64 loss: 0.9071736335754395
Batch 40/64 loss: 0.866063117980957
Batch 41/64 loss: 1.0211529731750488
Batch 42/64 loss: 0.8347969055175781
Batch 43/64 loss: 0.9499754905700684
Batch 44/64 loss: 1.2047767639160156
Batch 45/64 loss: 0.780271053314209
Batch 46/64 loss: 0.9497203826904297
Batch 47/64 loss: 0.9984588623046875
Batch 48/64 loss: 0.6706037521362305
Batch 49/64 loss: 0.809638500213623
Batch 50/64 loss: 0.7051043510437012
Batch 51/64 loss: 0.9143438339233398
Batch 52/64 loss: 1.0793509483337402
Batch 53/64 loss: 0.6580653190612793
Batch 54/64 loss: 0.8664569854736328
Batch 55/64 loss: 1.0606188774108887
Batch 56/64 loss: 0.9644026756286621
Batch 57/64 loss: 1.823298454284668
Batch 58/64 loss: 1.4876008033752441
Batch 59/64 loss: 0.5953249931335449
Batch 60/64 loss: 1.9284396171569824
Batch 61/64 loss: 1.580322265625
Batch 62/64 loss: 1.328080177307129
Batch 63/64 loss: 1.271355152130127
Batch 64/64 loss: -2.0673141479492188
Epoch 118  Train loss: 1.0018513623405905  Val loss: 7.6254109648085135
Epoch 119
-------------------------------
Batch 1/64 loss: 1.209731101989746
Batch 2/64 loss: 1.4274754524230957
Batch 3/64 loss: 2.9544434547424316
Batch 4/64 loss: 2.8726372718811035
Batch 5/64 loss: 1.7404446601867676
Batch 6/64 loss: 1.2649288177490234
Batch 7/64 loss: 1.9745912551879883
Batch 8/64 loss: 1.4407119750976562
Batch 9/64 loss: 1.6481056213378906
Batch 10/64 loss: 1.4821891784667969
Batch 11/64 loss: 1.4340925216674805
Batch 12/64 loss: 1.7603626251220703
Batch 13/64 loss: 1.7845549583435059
Batch 14/64 loss: 1.640376091003418
Batch 15/64 loss: 1.2221851348876953
Batch 16/64 loss: 1.3977127075195312
Batch 17/64 loss: 1.428999423980713
Batch 18/64 loss: 1.4008970260620117
Batch 19/64 loss: 1.4796075820922852
Batch 20/64 loss: 1.2572193145751953
Batch 21/64 loss: 1.528581142425537
Batch 22/64 loss: 1.4052190780639648
Batch 23/64 loss: 1.5995068550109863
Batch 24/64 loss: 1.7387990951538086
Batch 25/64 loss: 1.4529824256896973
Batch 26/64 loss: 2.570845603942871
Batch 27/64 loss: 1.2365822792053223
Batch 28/64 loss: 1.3046445846557617
Batch 29/64 loss: 1.4811592102050781
Batch 30/64 loss: 2.915353298187256
Batch 31/64 loss: 1.4694523811340332
Batch 32/64 loss: 1.270136833190918
Batch 33/64 loss: 1.855978012084961
Batch 34/64 loss: 1.609912395477295
Batch 35/64 loss: 1.4803967475891113
Batch 36/64 loss: 1.431626319885254
Batch 37/64 loss: 1.3410530090332031
Batch 38/64 loss: 1.2856144905090332
Batch 39/64 loss: 1.3611674308776855
Batch 40/64 loss: 1.281348705291748
Batch 41/64 loss: 1.2537879943847656
Batch 42/64 loss: 1.3147172927856445
Batch 43/64 loss: 1.2339472770690918
Batch 44/64 loss: 1.4104571342468262
Batch 45/64 loss: 1.3581972122192383
Batch 46/64 loss: 1.1598129272460938
Batch 47/64 loss: 1.3408417701721191
Batch 48/64 loss: 2.751711845397949
Batch 49/64 loss: 1.6650629043579102
Batch 50/64 loss: 2.588527202606201
Batch 51/64 loss: 1.2453289031982422
Batch 52/64 loss: 1.232071876525879
Batch 53/64 loss: 1.3962664604187012
Batch 54/64 loss: 1.5175628662109375
Batch 55/64 loss: 2.4961719512939453
Batch 56/64 loss: 1.267505168914795
Batch 57/64 loss: 1.0190777778625488
Batch 58/64 loss: 1.1808762550354004
Batch 59/64 loss: 1.2981510162353516
Batch 60/64 loss: 1.5217528343200684
Batch 61/64 loss: 1.349130630493164
Batch 62/64 loss: 1.5143976211547852
Batch 63/64 loss: 1.4556760787963867
Batch 64/64 loss: -2.025163173675537
Epoch 119  Train loss: 1.5293143010606953  Val loss: 1.3906310432145685
Epoch 120
-------------------------------
Batch 1/64 loss: 1.0767021179199219
Batch 2/64 loss: 1.581974983215332
Batch 3/64 loss: 2.0462470054626465
Batch 4/64 loss: 1.0702099800109863
Batch 5/64 loss: 2.1105847358703613
Batch 6/64 loss: 1.0484881401062012
Batch 7/64 loss: 1.7209067344665527
Batch 8/64 loss: 1.2226648330688477
Batch 9/64 loss: 1.3295249938964844
Batch 10/64 loss: 1.0774421691894531
Batch 11/64 loss: 1.4191927909851074
Batch 12/64 loss: 1.4645476341247559
Batch 13/64 loss: 1.3553366661071777
Batch 14/64 loss: 1.6479253768920898
Batch 15/64 loss: 1.1083812713623047
Batch 16/64 loss: 1.616541862487793
Batch 17/64 loss: 1.4669766426086426
Batch 18/64 loss: 1.0668816566467285
Batch 19/64 loss: 1.2458782196044922
Batch 20/64 loss: 1.5764360427856445
Batch 21/64 loss: 1.312659740447998
Batch 22/64 loss: 1.307483196258545
Batch 23/64 loss: 1.0914483070373535
Batch 24/64 loss: 1.3595623970031738
Batch 25/64 loss: 1.1434998512268066
Batch 26/64 loss: 1.3749799728393555
Batch 27/64 loss: 1.3896002769470215
Batch 28/64 loss: 1.1129589080810547
Batch 29/64 loss: 1.2269010543823242
Batch 30/64 loss: 1.3378853797912598
Batch 31/64 loss: 1.097825527191162
Batch 32/64 loss: 1.5446124076843262
Batch 33/64 loss: 1.234774112701416
Batch 34/64 loss: 1.3821253776550293
Batch 35/64 loss: 1.2176384925842285
Batch 36/64 loss: 1.0699048042297363
Batch 37/64 loss: 1.3533987998962402
Batch 38/64 loss: 1.0659708976745605
Batch 39/64 loss: 1.1787915229797363
Batch 40/64 loss: 1.1807289123535156
Batch 41/64 loss: 1.063323974609375
Batch 42/64 loss: 0.9100580215454102
Batch 43/64 loss: 1.482269287109375
Batch 44/64 loss: 1.0002598762512207
Batch 45/64 loss: 1.0796265602111816
Batch 46/64 loss: 1.322521686553955
Batch 47/64 loss: 1.179830551147461
Batch 48/64 loss: 0.9999032020568848
Batch 49/64 loss: 1.0844411849975586
Batch 50/64 loss: 0.8417119979858398
Batch 51/64 loss: 1.2943711280822754
Batch 52/64 loss: 0.8212180137634277
Batch 53/64 loss: 0.8211159706115723
Batch 54/64 loss: 0.952664852142334
Batch 55/64 loss: 0.8345432281494141
Batch 56/64 loss: 1.1081938743591309
Batch 57/64 loss: 0.9266886711120605
Batch 58/64 loss: 1.2073159217834473
Batch 59/64 loss: 0.7615056037902832
Batch 60/64 loss: 1.1045112609863281
Batch 61/64 loss: 0.986600399017334
Batch 62/64 loss: 1.1455516815185547
Batch 63/64 loss: 0.9141483306884766
Batch 64/64 loss: -2.281829833984375
Epoch 120  Train loss: 1.182221124686447  Val loss: 1.032309129066074
Epoch 121
-------------------------------
Batch 1/64 loss: 1.131742000579834
Batch 2/64 loss: 1.1240925788879395
Batch 3/64 loss: 1.3487873077392578
Batch 4/64 loss: 0.8904180526733398
Batch 5/64 loss: 0.9744701385498047
Batch 6/64 loss: 0.8472094535827637
Batch 7/64 loss: 1.179610252380371
Batch 8/64 loss: 1.055288314819336
Batch 9/64 loss: 1.1151838302612305
Batch 10/64 loss: 1.0984597206115723
Batch 11/64 loss: 1.1111488342285156
Batch 12/64 loss: 1.0920519828796387
Batch 13/64 loss: 1.0044646263122559
Batch 14/64 loss: 0.7871918678283691
Batch 15/64 loss: 1.065840244293213
Batch 16/64 loss: 0.8619508743286133
Batch 17/64 loss: 0.8934335708618164
Batch 18/64 loss: 0.7825307846069336
Batch 19/64 loss: 1.031041145324707
Batch 20/64 loss: 0.9840803146362305
Batch 21/64 loss: 1.2302169799804688
Batch 22/64 loss: 0.8503975868225098
Batch 23/64 loss: 0.8853607177734375
Batch 24/64 loss: 1.0718140602111816
Batch 25/64 loss: 1.0242772102355957
Batch 26/64 loss: 1.081611156463623
Batch 27/64 loss: 0.8607363700866699
Batch 28/64 loss: 1.195432186126709
Batch 29/64 loss: 1.0976762771606445
Batch 30/64 loss: 0.9919099807739258
Batch 31/64 loss: 1.3459582328796387
Batch 32/64 loss: 1.0312342643737793
Batch 33/64 loss: 1.0277719497680664
Batch 34/64 loss: 1.0567522048950195
Batch 35/64 loss: 0.9001731872558594
Batch 36/64 loss: 1.1396560668945312
Batch 37/64 loss: 1.160627841949463
Batch 38/64 loss: 1.3012704849243164
Batch 39/64 loss: 0.8326396942138672
Batch 40/64 loss: 1.1638035774230957
Batch 41/64 loss: 0.6294021606445312
Batch 42/64 loss: 1.11696195602417
Batch 43/64 loss: 1.0827484130859375
Batch 44/64 loss: 1.2035408020019531
Batch 45/64 loss: 1.0908102989196777
Batch 46/64 loss: 1.1646342277526855
Batch 47/64 loss: 0.6642050743103027
Batch 48/64 loss: 1.046968936920166
Batch 49/64 loss: 1.064936637878418
Batch 50/64 loss: 0.8242321014404297
Batch 51/64 loss: 1.229269027709961
Batch 52/64 loss: 1.353665828704834
Batch 53/64 loss: 1.1369209289550781
Batch 54/64 loss: 1.15480375289917
Batch 55/64 loss: 1.095128059387207
Batch 56/64 loss: 0.8530588150024414
Batch 57/64 loss: 1.0520601272583008
Batch 58/64 loss: 1.0936212539672852
Batch 59/64 loss: 0.8530707359313965
Batch 60/64 loss: 1.4823355674743652
Batch 61/64 loss: 0.9216375350952148
Batch 62/64 loss: 0.9849042892456055
Batch 63/64 loss: 0.7558245658874512
Batch 64/64 loss: -2.18601655960083
Epoch 121  Train loss: 1.0014673663120643  Val loss: 1.0078543699074447
Epoch 122
-------------------------------
Batch 1/64 loss: 0.8740935325622559
Batch 2/64 loss: 0.834418773651123
Batch 3/64 loss: 0.9545612335205078
Batch 4/64 loss: 1.1192622184753418
Batch 5/64 loss: 0.8872342109680176
Batch 6/64 loss: 1.3906326293945312
Batch 7/64 loss: 0.9164218902587891
Batch 8/64 loss: 0.8617968559265137
Batch 9/64 loss: 0.9388303756713867
Batch 10/64 loss: 0.9034738540649414
Batch 11/64 loss: 1.0970592498779297
Batch 12/64 loss: 0.8604001998901367
Batch 13/64 loss: 0.9990649223327637
Batch 14/64 loss: 1.0306239128112793
Batch 15/64 loss: 1.1964402198791504
Batch 16/64 loss: 1.0168190002441406
Batch 17/64 loss: 1.0756325721740723
Batch 18/64 loss: 1.130171298980713
Batch 19/64 loss: 1.2282824516296387
Batch 20/64 loss: 0.7500596046447754
Batch 21/64 loss: 1.176743984222412
Batch 22/64 loss: 1.217574119567871
Batch 23/64 loss: 1.159045696258545
Batch 24/64 loss: 0.9169831275939941
Batch 25/64 loss: 0.8820195198059082
Batch 26/64 loss: 1.0774998664855957
Batch 27/64 loss: 1.0298562049865723
Batch 28/64 loss: 1.2768988609313965
Batch 29/64 loss: 1.0126113891601562
Batch 30/64 loss: 0.895897388458252
Batch 31/64 loss: 1.2863860130310059
Batch 32/64 loss: 0.8476462364196777
Batch 33/64 loss: 0.6375269889831543
Batch 34/64 loss: 0.9626150131225586
Batch 35/64 loss: 0.9271764755249023
Batch 36/64 loss: 1.1498665809631348
Batch 37/64 loss: 1.2190866470336914
Batch 38/64 loss: 1.0651607513427734
Batch 39/64 loss: 0.9253683090209961
Batch 40/64 loss: 0.8607439994812012
Batch 41/64 loss: 0.9427700042724609
Batch 42/64 loss: 1.2260394096374512
Batch 43/64 loss: 0.9830093383789062
Batch 44/64 loss: 0.9220056533813477
Batch 45/64 loss: 0.7785634994506836
Batch 46/64 loss: 0.9472265243530273
Batch 47/64 loss: 0.5516524314880371
Batch 48/64 loss: 0.7804756164550781
Batch 49/64 loss: 0.9308419227600098
Batch 50/64 loss: 1.1552329063415527
Batch 51/64 loss: 0.9818015098571777
Batch 52/64 loss: 0.8545613288879395
Batch 53/64 loss: 0.8580198287963867
Batch 54/64 loss: 0.5319719314575195
Batch 55/64 loss: 1.0011701583862305
Batch 56/64 loss: 1.1467747688293457
Batch 57/64 loss: 1.1674456596374512
Batch 58/64 loss: 0.9370036125183105
Batch 59/64 loss: 0.8559165000915527
Batch 60/64 loss: 0.826472282409668
Batch 61/64 loss: 0.9104323387145996
Batch 62/64 loss: 1.2631287574768066
Batch 63/64 loss: 0.7783594131469727
Batch 64/64 loss: -2.140894889831543
Epoch 122  Train loss: 0.946152006411085  Val loss: 1.0146345944748711
Epoch 123
-------------------------------
Batch 1/64 loss: 1.057736873626709
Batch 2/64 loss: 0.9473233222961426
Batch 3/64 loss: 0.9065823554992676
Batch 4/64 loss: 0.8701825141906738
Batch 5/64 loss: 0.7751193046569824
Batch 6/64 loss: 0.9537997245788574
Batch 7/64 loss: 1.1130290031433105
Batch 8/64 loss: 0.9249334335327148
Batch 9/64 loss: 0.9813308715820312
Batch 10/64 loss: 0.8725881576538086
Batch 11/64 loss: 1.2190380096435547
Batch 12/64 loss: 0.8528838157653809
Batch 13/64 loss: 1.1422133445739746
Batch 14/64 loss: 0.6219258308410645
Batch 15/64 loss: 1.200955867767334
Batch 16/64 loss: 0.715980052947998
Batch 17/64 loss: 0.8638272285461426
Batch 18/64 loss: 1.0005607604980469
Batch 19/64 loss: 0.8511600494384766
Batch 20/64 loss: 1.100785732269287
Batch 21/64 loss: 0.7575345039367676
Batch 22/64 loss: 1.0501642227172852
Batch 23/64 loss: 0.9449410438537598
Batch 24/64 loss: 1.0757527351379395
Batch 25/64 loss: 0.8532443046569824
Batch 26/64 loss: 0.981205940246582
Batch 27/64 loss: 1.3234210014343262
Batch 28/64 loss: 1.0072088241577148
Batch 29/64 loss: 0.91357421875
Batch 30/64 loss: 1.0609040260314941
Batch 31/64 loss: 1.0093870162963867
Batch 32/64 loss: 1.2010574340820312
Batch 33/64 loss: 1.0053014755249023
Batch 34/64 loss: 0.563758373260498
Batch 35/64 loss: 0.9229207038879395
Batch 36/64 loss: 0.9417543411254883
Batch 37/64 loss: 1.1738576889038086
Batch 38/64 loss: 0.7395257949829102
Batch 39/64 loss: 1.228686809539795
Batch 40/64 loss: 0.6441974639892578
Batch 41/64 loss: 0.8471274375915527
Batch 42/64 loss: 1.0486412048339844
Batch 43/64 loss: 1.0458617210388184
Batch 44/64 loss: 1.0154781341552734
Batch 45/64 loss: 0.781548023223877
Batch 46/64 loss: 0.9188919067382812
Batch 47/64 loss: 1.0541329383850098
Batch 48/64 loss: 1.1900362968444824
Batch 49/64 loss: 1.001190185546875
Batch 50/64 loss: 0.8589892387390137
Batch 51/64 loss: 1.2566299438476562
Batch 52/64 loss: 1.065915584564209
Batch 53/64 loss: 0.9029693603515625
Batch 54/64 loss: 1.1274762153625488
Batch 55/64 loss: 0.9984793663024902
Batch 56/64 loss: 0.9307489395141602
Batch 57/64 loss: 0.7699565887451172
Batch 58/64 loss: 0.8509750366210938
Batch 59/64 loss: 0.8674654960632324
Batch 60/64 loss: 0.8166389465332031
Batch 61/64 loss: 1.0446429252624512
Batch 62/64 loss: 0.9960207939147949
Batch 63/64 loss: 0.9834871292114258
Batch 64/64 loss: -2.067080020904541
Epoch 123  Train loss: 0.9285230991887111  Val loss: 0.9209376200777558
Saving best model, epoch: 123
Epoch 124
-------------------------------
Batch 1/64 loss: 1.2645716667175293
Batch 2/64 loss: 1.01076078414917
Batch 3/64 loss: 0.8380308151245117
Batch 4/64 loss: 0.8236846923828125
Batch 5/64 loss: 0.7787618637084961
Batch 6/64 loss: 1.02667236328125
Batch 7/64 loss: 0.7226858139038086
Batch 8/64 loss: 0.864138126373291
Batch 9/64 loss: 1.3983945846557617
Batch 10/64 loss: 0.7672619819641113
Batch 11/64 loss: 0.8858308792114258
Batch 12/64 loss: 0.6190066337585449
Batch 13/64 loss: 0.8862013816833496
Batch 14/64 loss: 0.6905884742736816
Batch 15/64 loss: 0.6615495681762695
Batch 16/64 loss: 1.041830062866211
Batch 17/64 loss: 1.0044488906860352
Batch 18/64 loss: 0.8740658760070801
Batch 19/64 loss: 0.9712491035461426
Batch 20/64 loss: 0.8093843460083008
Batch 21/64 loss: 0.9979195594787598
Batch 22/64 loss: 1.3053479194641113
Batch 23/64 loss: 1.050830364227295
Batch 24/64 loss: 0.7819175720214844
Batch 25/64 loss: 0.7570757865905762
Batch 26/64 loss: 0.7186779975891113
Batch 27/64 loss: 0.6579294204711914
Batch 28/64 loss: 1.268632411956787
Batch 29/64 loss: 1.025834083557129
Batch 30/64 loss: 1.457902431488037
Batch 31/64 loss: 0.8188767433166504
Batch 32/64 loss: 0.9201946258544922
Batch 33/64 loss: 0.7844657897949219
Batch 34/64 loss: 0.8880081176757812
Batch 35/64 loss: 0.9897851943969727
Batch 36/64 loss: 0.6791653633117676
Batch 37/64 loss: 0.8998608589172363
Batch 38/64 loss: 1.4390549659729004
Batch 39/64 loss: 0.8207526206970215
Batch 40/64 loss: 0.6573486328125
Batch 41/64 loss: 0.6819195747375488
Batch 42/64 loss: 1.134951114654541
Batch 43/64 loss: 1.0557641983032227
Batch 44/64 loss: 0.7178120613098145
Batch 45/64 loss: 0.8207449913024902
Batch 46/64 loss: 0.9555377960205078
Batch 47/64 loss: 0.8366055488586426
Batch 48/64 loss: 0.8155288696289062
Batch 49/64 loss: 0.8792190551757812
Batch 50/64 loss: 1.0476799011230469
Batch 51/64 loss: 0.7126560211181641
Batch 52/64 loss: 1.4876718521118164
Batch 53/64 loss: 0.8322758674621582
Batch 54/64 loss: 1.1385908126831055
Batch 55/64 loss: 0.8111391067504883
Batch 56/64 loss: 1.2078819274902344
Batch 57/64 loss: 0.9542527198791504
Batch 58/64 loss: 1.0547904968261719
Batch 59/64 loss: 0.6898345947265625
Batch 60/64 loss: 1.2241325378417969
Batch 61/64 loss: 1.7360124588012695
Batch 62/64 loss: 1.2441596984863281
Batch 63/64 loss: 0.8695254325866699
Batch 64/64 loss: -2.3630385398864746
Epoch 124  Train loss: 0.909727154526056  Val loss: 1.3643435710893874
Epoch 125
-------------------------------
Batch 1/64 loss: 0.981666088104248
Batch 2/64 loss: 0.842254638671875
Batch 3/64 loss: 1.0931401252746582
Batch 4/64 loss: 0.7917518615722656
Batch 5/64 loss: 0.9630885124206543
Batch 6/64 loss: 1.0444726943969727
Batch 7/64 loss: 1.0744647979736328
Batch 8/64 loss: 0.9091043472290039
Batch 9/64 loss: 0.6947078704833984
Batch 10/64 loss: 0.9634771347045898
Batch 11/64 loss: 1.1211585998535156
Batch 12/64 loss: 0.8329887390136719
Batch 13/64 loss: 0.6852755546569824
Batch 14/64 loss: 0.8827190399169922
Batch 15/64 loss: 0.7253894805908203
Batch 16/64 loss: 1.1375765800476074
Batch 17/64 loss: 0.758448600769043
Batch 18/64 loss: 1.2475404739379883
Batch 19/64 loss: 1.0380549430847168
Batch 20/64 loss: 1.1225318908691406
Batch 21/64 loss: 1.0747733116149902
Batch 22/64 loss: 0.9278092384338379
Batch 23/64 loss: 1.0845532417297363
Batch 24/64 loss: 1.2408552169799805
Batch 25/64 loss: 0.43668651580810547
Batch 26/64 loss: 1.027994155883789
Batch 27/64 loss: 0.953406810760498
Batch 28/64 loss: 1.0140514373779297
Batch 29/64 loss: 1.3004398345947266
Batch 30/64 loss: 0.7552433013916016
Batch 31/64 loss: 0.8363175392150879
Batch 32/64 loss: 0.8341598510742188
Batch 33/64 loss: 1.0754361152648926
Batch 34/64 loss: 1.092592716217041
Batch 35/64 loss: 1.2104668617248535
Batch 36/64 loss: 1.0190439224243164
Batch 37/64 loss: 1.1512184143066406
Batch 38/64 loss: 1.0375900268554688
Batch 39/64 loss: 1.0933599472045898
Batch 40/64 loss: 0.6886496543884277
Batch 41/64 loss: 0.965510368347168
Batch 42/64 loss: 1.149552822113037
Batch 43/64 loss: 0.9694323539733887
Batch 44/64 loss: 1.305837631225586
Batch 45/64 loss: 0.8354487419128418
Batch 46/64 loss: 1.2463250160217285
Batch 47/64 loss: 0.973930835723877
Batch 48/64 loss: 0.951540470123291
Batch 49/64 loss: 1.0130925178527832
Batch 50/64 loss: 1.0956859588623047
Batch 51/64 loss: 0.5810317993164062
Batch 52/64 loss: 0.8084244728088379
Batch 53/64 loss: 0.9775810241699219
Batch 54/64 loss: 0.999152660369873
Batch 55/64 loss: 0.7848324775695801
Batch 56/64 loss: 0.9257736206054688
Batch 57/64 loss: 1.4942936897277832
Batch 58/64 loss: 0.9708318710327148
Batch 59/64 loss: 1.0559501647949219
Batch 60/64 loss: 1.2166223526000977
Batch 61/64 loss: 0.7138891220092773
Batch 62/64 loss: 0.9926910400390625
Batch 63/64 loss: 0.8571181297302246
Batch 64/64 loss: -2.4191646575927734
Epoch 125  Train loss: 0.9385825213264016  Val loss: 1.2794859057029908
Epoch 126
-------------------------------
Batch 1/64 loss: 0.9203767776489258
Batch 2/64 loss: 0.7690792083740234
Batch 3/64 loss: 1.083080768585205
Batch 4/64 loss: 0.565587043762207
Batch 5/64 loss: 1.6085762977600098
Batch 6/64 loss: 0.9160556793212891
Batch 7/64 loss: 1.1756467819213867
Batch 8/64 loss: 1.0042057037353516
Batch 9/64 loss: 0.7458019256591797
Batch 10/64 loss: 0.7393569946289062
Batch 11/64 loss: 0.9670257568359375
Batch 12/64 loss: 0.9820585250854492
Batch 13/64 loss: 1.0152254104614258
Batch 14/64 loss: 0.955324649810791
Batch 15/64 loss: 0.7150015830993652
Batch 16/64 loss: 1.068315029144287
Batch 17/64 loss: 0.9877486228942871
Batch 18/64 loss: 1.0518760681152344
Batch 19/64 loss: 1.138810157775879
Batch 20/64 loss: 0.8732872009277344
Batch 21/64 loss: 0.6966266632080078
Batch 22/64 loss: 1.1797051429748535
Batch 23/64 loss: 0.8661432266235352
Batch 24/64 loss: 1.382418155670166
Batch 25/64 loss: 1.0828571319580078
Batch 26/64 loss: 1.1732783317565918
Batch 27/64 loss: 0.8677377700805664
Batch 28/64 loss: 0.9367465972900391
Batch 29/64 loss: 0.8145556449890137
Batch 30/64 loss: 0.855964183807373
Batch 31/64 loss: 1.2757816314697266
Batch 32/64 loss: 1.2025952339172363
Batch 33/64 loss: 1.2506904602050781
Batch 34/64 loss: 0.9773373603820801
Batch 35/64 loss: 1.0564918518066406
Batch 36/64 loss: 1.3808588981628418
Batch 37/64 loss: 1.140355110168457
Batch 38/64 loss: 1.1701078414916992
Batch 39/64 loss: 0.9299044609069824
Batch 40/64 loss: 0.9874668121337891
Batch 41/64 loss: 1.0117383003234863
Batch 42/64 loss: 1.4652714729309082
Batch 43/64 loss: 1.1001019477844238
Batch 44/64 loss: 1.070836067199707
Batch 45/64 loss: 0.7998347282409668
Batch 46/64 loss: 0.920743465423584
Batch 47/64 loss: 1.0030407905578613
Batch 48/64 loss: 1.0918898582458496
Batch 49/64 loss: 1.110424518585205
Batch 50/64 loss: 0.7662844657897949
Batch 51/64 loss: 1.0086121559143066
Batch 52/64 loss: 0.786311149597168
Batch 53/64 loss: 1.0320062637329102
Batch 54/64 loss: 1.0480480194091797
Batch 55/64 loss: 1.0221295356750488
Batch 56/64 loss: 0.8485994338989258
Batch 57/64 loss: 0.7157559394836426
Batch 58/64 loss: 0.6595759391784668
Batch 59/64 loss: 0.7945456504821777
Batch 60/64 loss: 0.6707701683044434
Batch 61/64 loss: 0.8609938621520996
Batch 62/64 loss: 1.0675597190856934
Batch 63/64 loss: 0.7192082405090332
Batch 64/64 loss: -2.61220645904541
Epoch 126  Train loss: 0.9431402281218884  Val loss: 0.9407012782146021
Epoch 127
-------------------------------
Batch 1/64 loss: 0.707883358001709
Batch 2/64 loss: 0.9044833183288574
Batch 3/64 loss: 0.8341012001037598
Batch 4/64 loss: 0.8102726936340332
Batch 5/64 loss: 0.9753298759460449
Batch 6/64 loss: 1.0226993560791016
Batch 7/64 loss: 0.8980746269226074
Batch 8/64 loss: 1.1398639678955078
Batch 9/64 loss: 0.9214744567871094
Batch 10/64 loss: 0.8104381561279297
Batch 11/64 loss: 1.1402974128723145
Batch 12/64 loss: 0.8778448104858398
Batch 13/64 loss: 0.9237070083618164
Batch 14/64 loss: 1.2625536918640137
Batch 15/64 loss: 1.0634698867797852
Batch 16/64 loss: 0.7697920799255371
Batch 17/64 loss: 1.1359825134277344
Batch 18/64 loss: 0.9763855934143066
Batch 19/64 loss: 0.8905425071716309
Batch 20/64 loss: 0.8654370307922363
Batch 21/64 loss: 0.7485432624816895
Batch 22/64 loss: 0.9119434356689453
Batch 23/64 loss: 1.1732177734375
Batch 24/64 loss: 1.1740450859069824
Batch 25/64 loss: 0.7075991630554199
Batch 26/64 loss: 1.0376920700073242
Batch 27/64 loss: 0.814277172088623
Batch 28/64 loss: 1.229881763458252
Batch 29/64 loss: 0.9562454223632812
Batch 30/64 loss: 0.6438641548156738
Batch 31/64 loss: 1.0634522438049316
Batch 32/64 loss: 0.8587226867675781
Batch 33/64 loss: 0.9976167678833008
Batch 34/64 loss: 0.9426355361938477
Batch 35/64 loss: 1.018801212310791
Batch 36/64 loss: 0.9800863265991211
Batch 37/64 loss: 0.7871417999267578
Batch 38/64 loss: 0.9982352256774902
Batch 39/64 loss: 0.7932133674621582
Batch 40/64 loss: 0.9263315200805664
Batch 41/64 loss: 1.2455463409423828
Batch 42/64 loss: 1.0311927795410156
Batch 43/64 loss: 0.795107364654541
Batch 44/64 loss: 0.8659687042236328
Batch 45/64 loss: 0.7483382225036621
Batch 46/64 loss: 1.0369997024536133
Batch 47/64 loss: 1.3132410049438477
Batch 48/64 loss: 1.1742463111877441
Batch 49/64 loss: 1.1744909286499023
Batch 50/64 loss: 1.3312978744506836
Batch 51/64 loss: 0.9423437118530273
Batch 52/64 loss: 0.9533071517944336
Batch 53/64 loss: 0.9717926979064941
Batch 54/64 loss: 1.136124610900879
Batch 55/64 loss: 1.1519651412963867
Batch 56/64 loss: 0.8959693908691406
Batch 57/64 loss: 1.393357753753662
Batch 58/64 loss: 1.2005200386047363
Batch 59/64 loss: 0.8890109062194824
Batch 60/64 loss: 1.0653791427612305
Batch 61/64 loss: 1.1729812622070312
Batch 62/64 loss: 1.5917267799377441
Batch 63/64 loss: 1.165492057800293
Batch 64/64 loss: -1.6650686264038086
Epoch 127  Train loss: 0.9677141638363109  Val loss: 2.413354309973438
Epoch 128
-------------------------------
Batch 1/64 loss: 1.6869359016418457
Batch 2/64 loss: 1.1778440475463867
Batch 3/64 loss: 1.0308780670166016
Batch 4/64 loss: 1.2051634788513184
Batch 5/64 loss: 1.2153568267822266
Batch 6/64 loss: 0.985595703125
Batch 7/64 loss: 1.580723762512207
Batch 8/64 loss: 0.9851169586181641
Batch 9/64 loss: 1.4075241088867188
Batch 10/64 loss: 0.9761672019958496
Batch 11/64 loss: 1.060236930847168
Batch 12/64 loss: 1.2410483360290527
Batch 13/64 loss: 1.0927515029907227
Batch 14/64 loss: 1.271986484527588
Batch 15/64 loss: 1.0157241821289062
Batch 16/64 loss: 0.9427042007446289
Batch 17/64 loss: 1.2078723907470703
Batch 18/64 loss: 1.208482265472412
Batch 19/64 loss: 1.0896153450012207
Batch 20/64 loss: 1.1078858375549316
Batch 21/64 loss: 1.4327125549316406
Batch 22/64 loss: 0.9082741737365723
Batch 23/64 loss: 1.1174488067626953
Batch 24/64 loss: 1.0679874420166016
Batch 25/64 loss: 1.1952881813049316
Batch 26/64 loss: 1.1829757690429688
Batch 27/64 loss: 0.9330220222473145
Batch 28/64 loss: 1.2076911926269531
Batch 29/64 loss: 1.4362101554870605
Batch 30/64 loss: 1.0803513526916504
Batch 31/64 loss: 1.5979185104370117
Batch 32/64 loss: 1.2348613739013672
Batch 33/64 loss: 1.2100591659545898
Batch 34/64 loss: 1.431992530822754
Batch 35/64 loss: 1.203333854675293
Batch 36/64 loss: 1.0234308242797852
Batch 37/64 loss: 1.1916627883911133
Batch 38/64 loss: 1.1799349784851074
Batch 39/64 loss: 1.1175942420959473
Batch 40/64 loss: 1.4030070304870605
Batch 41/64 loss: 1.2072243690490723
Batch 42/64 loss: 1.1479406356811523
Batch 43/64 loss: 1.2810134887695312
Batch 44/64 loss: 0.842379093170166
Batch 45/64 loss: 0.7573142051696777
Batch 46/64 loss: 1.5664787292480469
Batch 47/64 loss: 1.0408978462219238
Batch 48/64 loss: 0.9759955406188965
Batch 49/64 loss: 0.9307126998901367
Batch 50/64 loss: 0.899569034576416
Batch 51/64 loss: 0.9100861549377441
Batch 52/64 loss: 0.8519802093505859
Batch 53/64 loss: 1.0596380233764648
Batch 54/64 loss: 0.8483395576477051
Batch 55/64 loss: 1.1143150329589844
Batch 56/64 loss: 0.951667308807373
Batch 57/64 loss: 0.6383252143859863
Batch 58/64 loss: 0.9145627021789551
Batch 59/64 loss: 1.2857260704040527
Batch 60/64 loss: 0.9597940444946289
Batch 61/64 loss: 0.9531807899475098
Batch 62/64 loss: 0.881415843963623
Batch 63/64 loss: 0.8088440895080566
Batch 64/64 loss: -2.1083855628967285
Epoch 128  Train loss: 1.0806506979699229  Val loss: 1.0370614684324493
Epoch 129
-------------------------------
Batch 1/64 loss: 0.7582225799560547
Batch 2/64 loss: 0.7912592887878418
Batch 3/64 loss: 0.8527326583862305
Batch 4/64 loss: 1.2467951774597168
Batch 5/64 loss: 1.0343332290649414
Batch 6/64 loss: 0.6333794593811035
Batch 7/64 loss: 1.1407861709594727
Batch 8/64 loss: 1.2102727890014648
Batch 9/64 loss: 1.1288814544677734
Batch 10/64 loss: 0.693385124206543
Batch 11/64 loss: 1.0581493377685547
Batch 12/64 loss: 0.8355722427368164
Batch 13/64 loss: 1.202104091644287
Batch 14/64 loss: 0.7822093963623047
Batch 15/64 loss: 1.0827879905700684
Batch 16/64 loss: 1.2655267715454102
Batch 17/64 loss: 1.075087070465088
Batch 18/64 loss: 0.9436602592468262
Batch 19/64 loss: 1.0148959159851074
Batch 20/64 loss: 0.9582576751708984
Batch 21/64 loss: 1.0679497718811035
Batch 22/64 loss: 0.8836727142333984
Batch 23/64 loss: 1.0773496627807617
Batch 24/64 loss: 0.9585971832275391
Batch 25/64 loss: 1.4474873542785645
Batch 26/64 loss: 0.904411792755127
Batch 27/64 loss: 1.0711379051208496
Batch 28/64 loss: 0.8282957077026367
Batch 29/64 loss: 0.8848457336425781
Batch 30/64 loss: 0.8424153327941895
Batch 31/64 loss: 0.8795933723449707
Batch 32/64 loss: 1.522737979888916
Batch 33/64 loss: 1.0532679557800293
Batch 34/64 loss: 1.0554118156433105
Batch 35/64 loss: 0.6847681999206543
Batch 36/64 loss: 0.832207202911377
Batch 37/64 loss: 0.793576717376709
Batch 38/64 loss: 1.1371235847473145
Batch 39/64 loss: 0.9342203140258789
Batch 40/64 loss: 0.760986328125
Batch 41/64 loss: 0.8577747344970703
Batch 42/64 loss: 1.1590442657470703
Batch 43/64 loss: 1.1780929565429688
Batch 44/64 loss: 1.1044058799743652
Batch 45/64 loss: 0.8671483993530273
Batch 46/64 loss: 0.6849865913391113
Batch 47/64 loss: 0.9343223571777344
Batch 48/64 loss: 1.0453662872314453
Batch 49/64 loss: 1.2648577690124512
Batch 50/64 loss: 1.073491096496582
Batch 51/64 loss: 0.9314446449279785
Batch 52/64 loss: 0.7799887657165527
Batch 53/64 loss: 0.8769221305847168
Batch 54/64 loss: 0.8222842216491699
Batch 55/64 loss: 1.1087555885314941
Batch 56/64 loss: 1.1247830390930176
Batch 57/64 loss: 0.8200182914733887
Batch 58/64 loss: 0.9479770660400391
Batch 59/64 loss: 0.9761457443237305
Batch 60/64 loss: 0.8726582527160645
Batch 61/64 loss: 1.043405532836914
Batch 62/64 loss: 0.8839402198791504
Batch 63/64 loss: 1.0741233825683594
Batch 64/64 loss: -2.5591049194335938
Epoch 129  Train loss: 0.938681785733092  Val loss: 0.9640823967268377
Epoch 130
-------------------------------
Batch 1/64 loss: 0.6618728637695312
Batch 2/64 loss: 0.9764885902404785
Batch 3/64 loss: 1.208695411682129
Batch 4/64 loss: 0.8270201683044434
Batch 5/64 loss: 0.8087811470031738
Batch 6/64 loss: 0.8235063552856445
Batch 7/64 loss: 1.1741681098937988
Batch 8/64 loss: 1.2142605781555176
Batch 9/64 loss: 1.3608431816101074
Batch 10/64 loss: 1.5125036239624023
Batch 11/64 loss: 0.8437023162841797
Batch 12/64 loss: 1.0159568786621094
Batch 13/64 loss: 1.1428337097167969
Batch 14/64 loss: 0.8884091377258301
Batch 15/64 loss: 1.0668258666992188
Batch 16/64 loss: 0.9108667373657227
Batch 17/64 loss: 1.3046035766601562
Batch 18/64 loss: 0.8385519981384277
Batch 19/64 loss: 1.0279107093811035
Batch 20/64 loss: 0.6456155776977539
Batch 21/64 loss: 1.0475835800170898
Batch 22/64 loss: 0.9989414215087891
Batch 23/64 loss: 0.9302463531494141
Batch 24/64 loss: 1.2654953002929688
Batch 25/64 loss: 1.0225310325622559
Batch 26/64 loss: 1.2175045013427734
Batch 27/64 loss: 0.9280099868774414
Batch 28/64 loss: 0.7001848220825195
Batch 29/64 loss: 0.910621166229248
Batch 30/64 loss: 0.9367594718933105
Batch 31/64 loss: 1.0383954048156738
Batch 32/64 loss: 1.4185881614685059
Batch 33/64 loss: 0.9481601715087891
Batch 34/64 loss: 1.165151596069336
Batch 35/64 loss: 0.9835605621337891
Batch 36/64 loss: 0.8923912048339844
Batch 37/64 loss: 1.2844104766845703
Batch 38/64 loss: 0.78472900390625
Batch 39/64 loss: 0.7669720649719238
Batch 40/64 loss: 1.2997040748596191
Batch 41/64 loss: 1.1259264945983887
Batch 42/64 loss: 1.1311345100402832
Batch 43/64 loss: 0.9638829231262207
Batch 44/64 loss: 1.274226188659668
Batch 45/64 loss: 0.8787393569946289
Batch 46/64 loss: 0.9316864013671875
Batch 47/64 loss: 0.7170877456665039
Batch 48/64 loss: 1.0267434120178223
Batch 49/64 loss: 0.9389443397521973
Batch 50/64 loss: 0.9731841087341309
Batch 51/64 loss: 1.0373868942260742
Batch 52/64 loss: 0.7034811973571777
Batch 53/64 loss: 1.0144472122192383
Batch 54/64 loss: 0.9944133758544922
Batch 55/64 loss: 1.2064790725708008
Batch 56/64 loss: 1.1805496215820312
Batch 57/64 loss: 1.0227875709533691
Batch 58/64 loss: 0.7891793251037598
Batch 59/64 loss: 0.9930353164672852
Batch 60/64 loss: 0.6815547943115234
Batch 61/64 loss: 1.3319182395935059
Batch 62/64 loss: 0.7715206146240234
Batch 63/64 loss: 0.9580001831054688
Batch 64/64 loss: -2.245333671569824
Epoch 130  Train loss: 0.9687163222069833  Val loss: 1.312165506107291
Epoch 131
-------------------------------
Batch 1/64 loss: 1.0509963035583496
Batch 2/64 loss: 0.7182779312133789
Batch 3/64 loss: 1.5159106254577637
Batch 4/64 loss: 0.9331827163696289
Batch 5/64 loss: 0.47144412994384766
Batch 6/64 loss: 0.8401799201965332
Batch 7/64 loss: 0.6390080451965332
Batch 8/64 loss: 0.9467306137084961
Batch 9/64 loss: 1.0532612800598145
Batch 10/64 loss: 0.896608829498291
Batch 11/64 loss: 0.9221692085266113
Batch 12/64 loss: 1.123486042022705
Batch 13/64 loss: 0.9920997619628906
Batch 14/64 loss: 0.9955849647521973
Batch 15/64 loss: 0.9682135581970215
Batch 16/64 loss: 0.9024744033813477
Batch 17/64 loss: 1.2247614860534668
Batch 18/64 loss: 0.8398652076721191
Batch 19/64 loss: 1.0086193084716797
Batch 20/64 loss: 0.8312220573425293
Batch 21/64 loss: 0.8292908668518066
Batch 22/64 loss: 0.6860437393188477
Batch 23/64 loss: 1.3076996803283691
Batch 24/64 loss: 0.7220687866210938
Batch 25/64 loss: 1.1216344833374023
Batch 26/64 loss: 0.8577256202697754
Batch 27/64 loss: 0.7946667671203613
Batch 28/64 loss: 1.004380226135254
Batch 29/64 loss: 0.8225111961364746
Batch 30/64 loss: 0.9554562568664551
Batch 31/64 loss: 1.019150733947754
Batch 32/64 loss: 0.9137215614318848
Batch 33/64 loss: 1.4049034118652344
Batch 34/64 loss: 0.971074104309082
Batch 35/64 loss: 0.7224783897399902
Batch 36/64 loss: 1.0764894485473633
Batch 37/64 loss: 1.1175804138183594
Batch 38/64 loss: 1.1318659782409668
Batch 39/64 loss: 0.901756763458252
Batch 40/64 loss: 1.2076148986816406
Batch 41/64 loss: 1.0865278244018555
Batch 42/64 loss: 1.0773282051086426
Batch 43/64 loss: 1.0090851783752441
Batch 44/64 loss: 1.0629920959472656
Batch 45/64 loss: 1.263866901397705
Batch 46/64 loss: 1.1573119163513184
Batch 47/64 loss: 0.9300394058227539
Batch 48/64 loss: 0.9108099937438965
Batch 49/64 loss: 0.9408864974975586
Batch 50/64 loss: 0.7772665023803711
Batch 51/64 loss: 0.7151775360107422
Batch 52/64 loss: 0.8482522964477539
Batch 53/64 loss: 0.8594832420349121
Batch 54/64 loss: 0.9604263305664062
Batch 55/64 loss: 0.7218093872070312
Batch 56/64 loss: 1.093949794769287
Batch 57/64 loss: 0.7633094787597656
Batch 58/64 loss: 0.9549813270568848
Batch 59/64 loss: 0.8863353729248047
Batch 60/64 loss: 0.6794729232788086
Batch 61/64 loss: 1.2202515602111816
Batch 62/64 loss: 1.0893139839172363
Batch 63/64 loss: 0.9246559143066406
Batch 64/64 loss: -2.196662425994873
Epoch 131  Train loss: 0.9212273971707213  Val loss: 0.9459907623500758
Epoch 132
-------------------------------
Batch 1/64 loss: 0.7463712692260742
Batch 2/64 loss: 1.194974422454834
Batch 3/64 loss: 0.8552036285400391
Batch 4/64 loss: 0.8733258247375488
Batch 5/64 loss: 0.9307832717895508
Batch 6/64 loss: 0.8947105407714844
Batch 7/64 loss: 1.316093921661377
Batch 8/64 loss: 1.014772891998291
Batch 9/64 loss: 0.8654775619506836
Batch 10/64 loss: 0.6754837036132812
Batch 11/64 loss: 1.0133662223815918
Batch 12/64 loss: 1.0238795280456543
Batch 13/64 loss: 0.9678897857666016
Batch 14/64 loss: 0.5561714172363281
Batch 15/64 loss: 0.7936978340148926
Batch 16/64 loss: 0.9473824501037598
Batch 17/64 loss: 0.861328125
Batch 18/64 loss: 0.750999927520752
Batch 19/64 loss: 1.1378707885742188
Batch 20/64 loss: 1.1446199417114258
Batch 21/64 loss: 0.7307095527648926
Batch 22/64 loss: 1.190145492553711
Batch 23/64 loss: 0.6656231880187988
Batch 24/64 loss: 0.6490879058837891
Batch 25/64 loss: 1.0094056129455566
Batch 26/64 loss: 0.9709959030151367
Batch 27/64 loss: 0.9349517822265625
Batch 28/64 loss: 0.848351001739502
Batch 29/64 loss: 0.7171812057495117
Batch 30/64 loss: 0.894233226776123
Batch 31/64 loss: 1.0565547943115234
Batch 32/64 loss: 1.1329708099365234
Batch 33/64 loss: 0.9771490097045898
Batch 34/64 loss: 0.9261765480041504
Batch 35/64 loss: 1.1071062088012695
Batch 36/64 loss: 0.8218297958374023
Batch 37/64 loss: 0.604736328125
Batch 38/64 loss: 0.8720488548278809
Batch 39/64 loss: 0.7902894020080566
Batch 40/64 loss: 0.718132495880127
Batch 41/64 loss: 0.9467244148254395
Batch 42/64 loss: 1.1389594078063965
Batch 43/64 loss: 0.6429581642150879
Batch 44/64 loss: 1.0841336250305176
Batch 45/64 loss: 0.9077930450439453
Batch 46/64 loss: 0.7349052429199219
Batch 47/64 loss: 0.5115056037902832
Batch 48/64 loss: 0.7885732650756836
Batch 49/64 loss: 0.7594571113586426
Batch 50/64 loss: 1.06807279586792
Batch 51/64 loss: 0.8314032554626465
Batch 52/64 loss: 0.7831110954284668
Batch 53/64 loss: 0.8046727180480957
Batch 54/64 loss: 1.0180144309997559
Batch 55/64 loss: 0.7681264877319336
Batch 56/64 loss: 0.9316468238830566
Batch 57/64 loss: 1.2376666069030762
Batch 58/64 loss: 0.9193429946899414
Batch 59/64 loss: 0.7747936248779297
Batch 60/64 loss: 0.9595041275024414
Batch 61/64 loss: 0.4870414733886719
Batch 62/64 loss: 0.7822375297546387
Batch 63/64 loss: 0.8912901878356934
Batch 64/64 loss: -2.1935229301452637
Epoch 132  Train loss: 0.8519039060555252  Val loss: 0.9185637247931097
Saving best model, epoch: 132
Epoch 133
-------------------------------
Batch 1/64 loss: 0.9574966430664062
Batch 2/64 loss: 0.6369028091430664
Batch 3/64 loss: 0.8453621864318848
Batch 4/64 loss: 0.9573268890380859
Batch 5/64 loss: 0.7415785789489746
Batch 6/64 loss: 0.7846989631652832
Batch 7/64 loss: 0.9605941772460938
Batch 8/64 loss: 0.8501811027526855
Batch 9/64 loss: 0.6833095550537109
Batch 10/64 loss: 0.6275105476379395
Batch 11/64 loss: 0.6837773323059082
Batch 12/64 loss: 0.8818354606628418
Batch 13/64 loss: 0.5596394538879395
Batch 14/64 loss: 0.7323646545410156
Batch 15/64 loss: 0.8099503517150879
Batch 16/64 loss: 0.7635002136230469
Batch 17/64 loss: 0.7491006851196289
Batch 18/64 loss: 0.8818807601928711
Batch 19/64 loss: 0.9071683883666992
Batch 20/64 loss: 1.018712043762207
Batch 21/64 loss: 0.7292814254760742
Batch 22/64 loss: 0.8893380165100098
Batch 23/64 loss: 0.6665186882019043
Batch 24/64 loss: 0.7731270790100098
Batch 25/64 loss: 0.3339347839355469
Batch 26/64 loss: 0.8498167991638184
Batch 27/64 loss: 1.0093960762023926
Batch 28/64 loss: 1.2698640823364258
Batch 29/64 loss: 0.9487528800964355
Batch 30/64 loss: 0.9953126907348633
Batch 31/64 loss: 1.0919189453125
Batch 32/64 loss: 0.951143741607666
Batch 33/64 loss: 0.9264335632324219
Batch 34/64 loss: 0.7942700386047363
Batch 35/64 loss: 1.1078782081604004
Batch 36/64 loss: 0.6675786972045898
Batch 37/64 loss: 1.1666431427001953
Batch 38/64 loss: 0.694878101348877
Batch 39/64 loss: 0.8414039611816406
Batch 40/64 loss: 0.8474240303039551
Batch 41/64 loss: 0.9705524444580078
Batch 42/64 loss: 0.87646484375
Batch 43/64 loss: 0.8084583282470703
Batch 44/64 loss: 0.8835487365722656
Batch 45/64 loss: 0.7896618843078613
Batch 46/64 loss: 1.134415626525879
Batch 47/64 loss: 1.1126532554626465
Batch 48/64 loss: 1.035067081451416
Batch 49/64 loss: 0.9828963279724121
Batch 50/64 loss: 0.8875570297241211
Batch 51/64 loss: 0.9639267921447754
Batch 52/64 loss: 1.0217571258544922
Batch 53/64 loss: 1.0923104286193848
Batch 54/64 loss: 0.7562689781188965
Batch 55/64 loss: 1.1516494750976562
Batch 56/64 loss: 1.033797264099121
Batch 57/64 loss: 0.575249195098877
Batch 58/64 loss: 0.6695752143859863
Batch 59/64 loss: 1.0179343223571777
Batch 60/64 loss: 0.757291316986084
Batch 61/64 loss: 1.0282044410705566
Batch 62/64 loss: 1.0020341873168945
Batch 63/64 loss: 1.2447528839111328
Batch 64/64 loss: -2.6914138793945312
Epoch 133  Train loss: 0.8371023140701593  Val loss: 0.9147745506050661
Saving best model, epoch: 133
Epoch 134
-------------------------------
Batch 1/64 loss: 0.6961326599121094
Batch 2/64 loss: 0.7885785102844238
Batch 3/64 loss: 0.7913289070129395
Batch 4/64 loss: 1.119455337524414
Batch 5/64 loss: 0.6931853294372559
Batch 6/64 loss: 0.8818058967590332
Batch 7/64 loss: 0.7616968154907227
Batch 8/64 loss: 0.9466347694396973
Batch 9/64 loss: 1.0593109130859375
Batch 10/64 loss: 0.6485576629638672
Batch 11/64 loss: 0.8761992454528809
Batch 12/64 loss: 1.0420913696289062
Batch 13/64 loss: 0.7617020606994629
Batch 14/64 loss: 0.9225611686706543
Batch 15/64 loss: 0.6750917434692383
Batch 16/64 loss: 0.8847899436950684
Batch 17/64 loss: 0.7760658264160156
Batch 18/64 loss: 0.8480501174926758
Batch 19/64 loss: 1.1977052688598633
Batch 20/64 loss: 1.4102182388305664
Batch 21/64 loss: 0.9013509750366211
Batch 22/64 loss: 0.9382619857788086
Batch 23/64 loss: 0.8772287368774414
Batch 24/64 loss: 1.0234785079956055
Batch 25/64 loss: 0.8856878280639648
Batch 26/64 loss: 1.115954875946045
Batch 27/64 loss: 0.8141765594482422
Batch 28/64 loss: 0.884305477142334
Batch 29/64 loss: 0.9542317390441895
Batch 30/64 loss: 0.701289176940918
Batch 31/64 loss: 0.9224772453308105
Batch 32/64 loss: 0.7955265045166016
Batch 33/64 loss: 0.9573054313659668
Batch 34/64 loss: 1.4749488830566406
Batch 35/64 loss: 1.4357757568359375
Batch 36/64 loss: 1.1750388145446777
Batch 37/64 loss: 0.8391647338867188
Batch 38/64 loss: 0.607086181640625
Batch 39/64 loss: 0.776939868927002
Batch 40/64 loss: 1.064542293548584
Batch 41/64 loss: 0.8451380729675293
Batch 42/64 loss: 0.5934572219848633
Batch 43/64 loss: 0.8000802993774414
Batch 44/64 loss: 0.7361268997192383
Batch 45/64 loss: 0.47199153900146484
Batch 46/64 loss: 0.7729144096374512
Batch 47/64 loss: 0.8427844047546387
Batch 48/64 loss: 0.8623666763305664
Batch 49/64 loss: 0.4727911949157715
Batch 50/64 loss: 1.1802620887756348
Batch 51/64 loss: 0.8478827476501465
Batch 52/64 loss: 0.6733927726745605
Batch 53/64 loss: 0.9357151985168457
Batch 54/64 loss: 1.1075057983398438
Batch 55/64 loss: 0.7516379356384277
Batch 56/64 loss: 0.7714953422546387
Batch 57/64 loss: 0.8963570594787598
Batch 58/64 loss: 0.9189906120300293
Batch 59/64 loss: 0.7209339141845703
Batch 60/64 loss: 0.6773386001586914
Batch 61/64 loss: 0.9168744087219238
Batch 62/64 loss: 0.7529749870300293
Batch 63/64 loss: 0.817802906036377
Batch 64/64 loss: -2.0849061012268066
Epoch 134  Train loss: 0.8428089235343185  Val loss: 0.8724177252386034
Saving best model, epoch: 134
Epoch 135
-------------------------------
Batch 1/64 loss: 0.8773908615112305
Batch 2/64 loss: 0.8945422172546387
Batch 3/64 loss: 0.6237649917602539
Batch 4/64 loss: 0.6782212257385254
Batch 5/64 loss: 0.9437956809997559
Batch 6/64 loss: 0.5487337112426758
Batch 7/64 loss: 1.0848326683044434
Batch 8/64 loss: 0.8928084373474121
Batch 9/64 loss: 0.8359246253967285
Batch 10/64 loss: 0.7899694442749023
Batch 11/64 loss: 0.6002864837646484
Batch 12/64 loss: 0.8139019012451172
Batch 13/64 loss: 0.9981107711791992
Batch 14/64 loss: 0.6677522659301758
Batch 15/64 loss: 0.6450247764587402
Batch 16/64 loss: 0.671290397644043
Batch 17/64 loss: 0.7985019683837891
Batch 18/64 loss: 0.5447368621826172
Batch 19/64 loss: 1.6096320152282715
Batch 20/64 loss: 0.8609027862548828
Batch 21/64 loss: 0.5413789749145508
Batch 22/64 loss: 0.8454742431640625
Batch 23/64 loss: 0.9012398719787598
Batch 24/64 loss: 1.1691474914550781
Batch 25/64 loss: 0.9441337585449219
Batch 26/64 loss: 1.1161398887634277
Batch 27/64 loss: 0.8655581474304199
Batch 28/64 loss: 0.945033073425293
Batch 29/64 loss: 1.129955768585205
Batch 30/64 loss: 0.9248924255371094
Batch 31/64 loss: 0.9670019149780273
Batch 32/64 loss: 0.9590272903442383
Batch 33/64 loss: 0.585395336151123
Batch 34/64 loss: 0.9717249870300293
Batch 35/64 loss: 1.1742396354675293
Batch 36/64 loss: 0.8466038703918457
Batch 37/64 loss: 1.0644965171813965
Batch 38/64 loss: 1.0908360481262207
Batch 39/64 loss: 1.0529828071594238
Batch 40/64 loss: 0.9312286376953125
Batch 41/64 loss: 0.8666725158691406
Batch 42/64 loss: 0.7516822814941406
Batch 43/64 loss: 1.145155906677246
Batch 44/64 loss: 1.134242057800293
Batch 45/64 loss: 0.7987642288208008
Batch 46/64 loss: 0.8831124305725098
Batch 47/64 loss: 0.8996057510375977
Batch 48/64 loss: 1.065321922302246
Batch 49/64 loss: 0.8082695007324219
Batch 50/64 loss: 0.8318839073181152
Batch 51/64 loss: 0.9761924743652344
Batch 52/64 loss: 1.1332378387451172
Batch 53/64 loss: 1.2002835273742676
Batch 54/64 loss: 0.8525552749633789
Batch 55/64 loss: 0.7628116607666016
Batch 56/64 loss: 0.8077497482299805
Batch 57/64 loss: 1.1069293022155762
Batch 58/64 loss: 0.861912727355957
Batch 59/64 loss: 0.9741783142089844
Batch 60/64 loss: 0.8181581497192383
Batch 61/64 loss: 0.7203607559204102
Batch 62/64 loss: 1.049454689025879
Batch 63/64 loss: 1.028139591217041
Batch 64/64 loss: -2.5069894790649414
Epoch 135  Train loss: 0.8627928976919137  Val loss: 0.9754009050192293
Epoch 136
-------------------------------
Batch 1/64 loss: 1.1256895065307617
Batch 2/64 loss: 0.7598376274108887
Batch 3/64 loss: 0.933617115020752
Batch 4/64 loss: 1.0036211013793945
Batch 5/64 loss: 0.9239053726196289
Batch 6/64 loss: 0.8530077934265137
Batch 7/64 loss: 0.7651596069335938
Batch 8/64 loss: 1.2593812942504883
Batch 9/64 loss: 0.7562136650085449
Batch 10/64 loss: 0.8838462829589844
Batch 11/64 loss: 1.0060935020446777
Batch 12/64 loss: 0.7663002014160156
Batch 13/64 loss: 1.242593765258789
Batch 14/64 loss: 0.8611922264099121
Batch 15/64 loss: 0.8585233688354492
Batch 16/64 loss: 0.9015917778015137
Batch 17/64 loss: 0.7513852119445801
Batch 18/64 loss: 0.8557977676391602
Batch 19/64 loss: 0.5644950866699219
Batch 20/64 loss: 0.82354736328125
Batch 21/64 loss: 0.9939079284667969
Batch 22/64 loss: 1.2251086235046387
Batch 23/64 loss: 0.6819624900817871
Batch 24/64 loss: 0.792963981628418
Batch 25/64 loss: 0.6348185539245605
Batch 26/64 loss: 1.2266449928283691
Batch 27/64 loss: 0.7664308547973633
Batch 28/64 loss: 1.0206632614135742
Batch 29/64 loss: 0.9981489181518555
Batch 30/64 loss: 0.5950908660888672
Batch 31/64 loss: 0.8323497772216797
Batch 32/64 loss: 0.9987959861755371
Batch 33/64 loss: 1.159130573272705
Batch 34/64 loss: 0.8578009605407715
Batch 35/64 loss: 0.8398523330688477
Batch 36/64 loss: 1.0827207565307617
Batch 37/64 loss: 0.9563422203063965
Batch 38/64 loss: 0.7254848480224609
Batch 39/64 loss: 1.0295543670654297
Batch 40/64 loss: 0.4900636672973633
Batch 41/64 loss: 0.9424448013305664
Batch 42/64 loss: 0.7570204734802246
Batch 43/64 loss: 0.8330888748168945
Batch 44/64 loss: 0.6448488235473633
Batch 45/64 loss: 0.7467060089111328
Batch 46/64 loss: 0.9937057495117188
Batch 47/64 loss: 1.1517386436462402
Batch 48/64 loss: 0.5189781188964844
Batch 49/64 loss: 0.7519445419311523
Batch 50/64 loss: 0.7187981605529785
Batch 51/64 loss: 0.9305672645568848
Batch 52/64 loss: 0.7297496795654297
Batch 53/64 loss: 0.8642921447753906
Batch 54/64 loss: 1.1315526962280273
Batch 55/64 loss: 0.795745849609375
Batch 56/64 loss: 0.8538875579833984
Batch 57/64 loss: 0.7211737632751465
Batch 58/64 loss: 0.8238568305969238
Batch 59/64 loss: 0.7252120971679688
Batch 60/64 loss: 1.0458850860595703
Batch 61/64 loss: 0.8064780235290527
Batch 62/64 loss: 0.9317636489868164
Batch 63/64 loss: 0.8165555000305176
Batch 64/64 loss: -2.6844520568847656
Epoch 136  Train loss: 0.8317849551930147  Val loss: 0.8700886428151343
Saving best model, epoch: 136
Epoch 137
-------------------------------
Batch 1/64 loss: 0.7771363258361816
Batch 2/64 loss: 0.5852341651916504
Batch 3/64 loss: 0.6148977279663086
Batch 4/64 loss: 0.8189911842346191
Batch 5/64 loss: 0.9782519340515137
Batch 6/64 loss: 1.0770916938781738
Batch 7/64 loss: 0.7917070388793945
Batch 8/64 loss: 0.6457638740539551
Batch 9/64 loss: 0.827458381652832
Batch 10/64 loss: 0.7644062042236328
Batch 11/64 loss: 0.6795735359191895
Batch 12/64 loss: 0.9998750686645508
Batch 13/64 loss: 1.227151870727539
Batch 14/64 loss: 1.1990857124328613
Batch 15/64 loss: 0.8825359344482422
Batch 16/64 loss: 0.9284849166870117
Batch 17/64 loss: 0.8660240173339844
Batch 18/64 loss: 0.6575174331665039
Batch 19/64 loss: 0.8802618980407715
Batch 20/64 loss: 0.9887585639953613
Batch 21/64 loss: 1.0032281875610352
Batch 22/64 loss: 1.1257305145263672
Batch 23/64 loss: 0.63128662109375
Batch 24/64 loss: 1.0139880180358887
Batch 25/64 loss: 0.983665943145752
Batch 26/64 loss: 0.7399120330810547
Batch 27/64 loss: 1.3235092163085938
Batch 28/64 loss: 0.9032225608825684
Batch 29/64 loss: 0.7205810546875
Batch 30/64 loss: 0.8219680786132812
Batch 31/64 loss: 0.7474122047424316
Batch 32/64 loss: 0.8237857818603516
Batch 33/64 loss: 0.8502750396728516
Batch 34/64 loss: 0.7211050987243652
Batch 35/64 loss: 0.9455223083496094
Batch 36/64 loss: 0.9817214012145996
Batch 37/64 loss: 0.7381253242492676
Batch 38/64 loss: 0.8312954902648926
Batch 39/64 loss: 1.11668062210083
Batch 40/64 loss: 0.6426568031311035
Batch 41/64 loss: 0.8572492599487305
Batch 42/64 loss: 0.6187763214111328
Batch 43/64 loss: 0.8706541061401367
Batch 44/64 loss: 0.8697514533996582
Batch 45/64 loss: 1.046525001525879
Batch 46/64 loss: 0.6821541786193848
Batch 47/64 loss: 0.5638232231140137
Batch 48/64 loss: 0.8385639190673828
Batch 49/64 loss: 0.8488783836364746
Batch 50/64 loss: 0.8446316719055176
Batch 51/64 loss: 0.7119536399841309
Batch 52/64 loss: 0.9034543037414551
Batch 53/64 loss: 0.7435140609741211
Batch 54/64 loss: 0.6105141639709473
Batch 55/64 loss: 0.9652266502380371
Batch 56/64 loss: 0.8117694854736328
Batch 57/64 loss: 0.960453987121582
Batch 58/64 loss: 0.894463062286377
Batch 59/64 loss: 0.5646138191223145
Batch 60/64 loss: 0.8224644660949707
Batch 61/64 loss: 0.8939461708068848
Batch 62/64 loss: 0.8714876174926758
Batch 63/64 loss: 0.5530357360839844
Batch 64/64 loss: -2.2605671882629395
Epoch 137  Train loss: 0.8079742599936093  Val loss: 0.9474698423929641
Epoch 138
-------------------------------
Batch 1/64 loss: 1.126366138458252
Batch 2/64 loss: 0.7964110374450684
Batch 3/64 loss: 0.5265974998474121
Batch 4/64 loss: 0.8162264823913574
Batch 5/64 loss: 0.7117924690246582
Batch 6/64 loss: 0.7500438690185547
Batch 7/64 loss: 1.0384716987609863
Batch 8/64 loss: 0.8639225959777832
Batch 9/64 loss: 0.7832140922546387
Batch 10/64 loss: 0.8414292335510254
Batch 11/64 loss: 0.939544677734375
Batch 12/64 loss: 1.2305865287780762
Batch 13/64 loss: 0.6685056686401367
Batch 14/64 loss: 0.8229842185974121
Batch 15/64 loss: 0.9127311706542969
Batch 16/64 loss: 1.2040200233459473
Batch 17/64 loss: 0.8230776786804199
Batch 18/64 loss: 0.8227314949035645
Batch 19/64 loss: 1.1204547882080078
Batch 20/64 loss: 1.255969524383545
Batch 21/64 loss: 1.31441068649292
Batch 22/64 loss: 0.9485650062561035
Batch 23/64 loss: 1.154350757598877
Batch 24/64 loss: 1.0085177421569824
Batch 25/64 loss: 0.9663276672363281
Batch 26/64 loss: 0.8828387260437012
Batch 27/64 loss: 0.6763467788696289
Batch 28/64 loss: 1.0478744506835938
Batch 29/64 loss: 0.8010172843933105
Batch 30/64 loss: 0.7769289016723633
Batch 31/64 loss: 0.8574700355529785
Batch 32/64 loss: 1.1629633903503418
Batch 33/64 loss: 0.8487896919250488
Batch 34/64 loss: 0.8475770950317383
Batch 35/64 loss: 0.7338852882385254
Batch 36/64 loss: 1.0112218856811523
Batch 37/64 loss: 0.6645016670227051
Batch 38/64 loss: 0.815673828125
Batch 39/64 loss: 0.8914623260498047
Batch 40/64 loss: 0.7389278411865234
Batch 41/64 loss: 1.0246191024780273
Batch 42/64 loss: 1.0783448219299316
Batch 43/64 loss: 0.8420724868774414
Batch 44/64 loss: 0.864342212677002
Batch 45/64 loss: 1.1768155097961426
Batch 46/64 loss: 0.9528636932373047
Batch 47/64 loss: 0.8469028472900391
Batch 48/64 loss: 0.6841859817504883
Batch 49/64 loss: 0.700615406036377
Batch 50/64 loss: 0.8209071159362793
Batch 51/64 loss: 0.725804328918457
Batch 52/64 loss: 1.013631820678711
Batch 53/64 loss: 0.9219279289245605
Batch 54/64 loss: 0.7295827865600586
Batch 55/64 loss: 0.6612205505371094
Batch 56/64 loss: 0.6890048980712891
Batch 57/64 loss: 0.9430174827575684
Batch 58/64 loss: 0.7374973297119141
Batch 59/64 loss: 1.0566048622131348
Batch 60/64 loss: 0.9048271179199219
Batch 61/64 loss: 0.8311500549316406
Batch 62/64 loss: 0.5924739837646484
Batch 63/64 loss: 0.9587850570678711
Batch 64/64 loss: -2.4640798568725586
Epoch 138  Train loss: 0.8488450106452493  Val loss: 0.8652080391690493
Saving best model, epoch: 138
Epoch 139
-------------------------------
Batch 1/64 loss: 1.071037769317627
Batch 2/64 loss: 0.9449319839477539
Batch 3/64 loss: 1.278141975402832
Batch 4/64 loss: 0.7048592567443848
Batch 5/64 loss: 0.8792238235473633
Batch 6/64 loss: 0.8975706100463867
Batch 7/64 loss: 0.895693302154541
Batch 8/64 loss: 0.9463562965393066
Batch 9/64 loss: 0.6919522285461426
Batch 10/64 loss: 0.9371500015258789
Batch 11/64 loss: 0.527618408203125
Batch 12/64 loss: 1.0484962463378906
Batch 13/64 loss: 0.9680333137512207
Batch 14/64 loss: 0.7198596000671387
Batch 15/64 loss: 0.8819279670715332
Batch 16/64 loss: 0.8117012977600098
Batch 17/64 loss: 0.6742768287658691
Batch 18/64 loss: 0.7845325469970703
Batch 19/64 loss: 0.9000005722045898
Batch 20/64 loss: 0.7967367172241211
Batch 21/64 loss: 0.6769194602966309
Batch 22/64 loss: 0.6450948715209961
Batch 23/64 loss: 0.7546830177307129
Batch 24/64 loss: 0.4352264404296875
Batch 25/64 loss: 0.9911174774169922
Batch 26/64 loss: 0.7752933502197266
Batch 27/64 loss: 0.48706769943237305
Batch 28/64 loss: 1.0520048141479492
Batch 29/64 loss: 0.9405426979064941
Batch 30/64 loss: 1.142171859741211
Batch 31/64 loss: 1.162463665008545
Batch 32/64 loss: 0.8068990707397461
Batch 33/64 loss: 0.7918186187744141
Batch 34/64 loss: 0.6512150764465332
Batch 35/64 loss: 0.8196964263916016
Batch 36/64 loss: 0.5786805152893066
Batch 37/64 loss: 0.8580594062805176
Batch 38/64 loss: 0.6891107559204102
Batch 39/64 loss: 0.5279884338378906
Batch 40/64 loss: 1.0252833366394043
Batch 41/64 loss: 0.6487569808959961
Batch 42/64 loss: 0.9341492652893066
Batch 43/64 loss: 1.0851898193359375
Batch 44/64 loss: 0.9384951591491699
Batch 45/64 loss: 0.604590892791748
Batch 46/64 loss: 1.1712040901184082
Batch 47/64 loss: 0.7055625915527344
Batch 48/64 loss: 0.563084602355957
Batch 49/64 loss: 0.5893149375915527
Batch 50/64 loss: 0.9376935958862305
Batch 51/64 loss: 0.6153535842895508
Batch 52/64 loss: 0.8606357574462891
Batch 53/64 loss: 0.9330101013183594
Batch 54/64 loss: 0.6756887435913086
Batch 55/64 loss: 0.6715922355651855
Batch 56/64 loss: 0.5666537284851074
Batch 57/64 loss: 1.0155506134033203
Batch 58/64 loss: 0.5176591873168945
Batch 59/64 loss: 1.2344741821289062
Batch 60/64 loss: 0.5371832847595215
Batch 61/64 loss: 0.797234058380127
Batch 62/64 loss: 1.0246448516845703
Batch 63/64 loss: 0.9036459922790527
Batch 64/64 loss: -3.1472983360290527
Epoch 139  Train loss: 0.7739973685320686  Val loss: 1.7093746145975959
Epoch 140
-------------------------------
Batch 1/64 loss: 1.009477138519287
Batch 2/64 loss: 0.8716588020324707
Batch 3/64 loss: 1.1270313262939453
Batch 4/64 loss: 0.5750088691711426
Batch 5/64 loss: 1.0029377937316895
Batch 6/64 loss: 0.9722566604614258
Batch 7/64 loss: 0.8233256340026855
Batch 8/64 loss: 0.5148091316223145
Batch 9/64 loss: 0.7867217063903809
Batch 10/64 loss: 0.7422890663146973
Batch 11/64 loss: 0.809420108795166
Batch 12/64 loss: 0.9266066551208496
Batch 13/64 loss: 0.7475404739379883
Batch 14/64 loss: 0.967592716217041
Batch 15/64 loss: 0.7184638977050781
Batch 16/64 loss: 0.5784473419189453
Batch 17/64 loss: 0.6511592864990234
Batch 18/64 loss: 1.1051850318908691
Batch 19/64 loss: 0.6286430358886719
Batch 20/64 loss: 0.8867683410644531
Batch 21/64 loss: 0.7471051216125488
Batch 22/64 loss: 0.8362011909484863
Batch 23/64 loss: 0.7802486419677734
Batch 24/64 loss: 0.7869386672973633
Batch 25/64 loss: 0.7844133377075195
Batch 26/64 loss: 1.065657138824463
Batch 27/64 loss: 0.536226749420166
Batch 28/64 loss: 0.46706581115722656
Batch 29/64 loss: 0.6483988761901855
Batch 30/64 loss: 1.2750053405761719
Batch 31/64 loss: 0.6704597473144531
Batch 32/64 loss: 0.9126744270324707
Batch 33/64 loss: 0.9909181594848633
Batch 34/64 loss: 0.5103235244750977
Batch 35/64 loss: 1.003580093383789
Batch 36/64 loss: 0.8638262748718262
Batch 37/64 loss: 0.5508551597595215
Batch 38/64 loss: 0.5930666923522949
Batch 39/64 loss: 1.0819921493530273
Batch 40/64 loss: 1.1016473770141602
Batch 41/64 loss: 1.0814862251281738
Batch 42/64 loss: 0.8634123802185059
Batch 43/64 loss: 0.8732194900512695
Batch 44/64 loss: 0.8555421829223633
Batch 45/64 loss: 0.7694797515869141
Batch 46/64 loss: 0.9554200172424316
Batch 47/64 loss: 0.8235445022583008
Batch 48/64 loss: 0.5944271087646484
Batch 49/64 loss: 0.8885302543640137
Batch 50/64 loss: 0.8698363304138184
Batch 51/64 loss: 0.9081258773803711
Batch 52/64 loss: 0.9285926818847656
Batch 53/64 loss: 1.1575264930725098
Batch 54/64 loss: 0.9615516662597656
Batch 55/64 loss: 0.5344600677490234
Batch 56/64 loss: 0.8206954002380371
Batch 57/64 loss: 0.715296745300293
Batch 58/64 loss: 0.8268246650695801
Batch 59/64 loss: 0.6807575225830078
Batch 60/64 loss: 1.0112013816833496
Batch 61/64 loss: 1.0034823417663574
Batch 62/64 loss: 1.0645833015441895
Batch 63/64 loss: 0.6260204315185547
Batch 64/64 loss: -2.6544971466064453
Epoch 140  Train loss: 0.7917661405077168  Val loss: 0.8631886485516
Saving best model, epoch: 140
Epoch 141
-------------------------------
Batch 1/64 loss: 0.835446834564209
Batch 2/64 loss: 0.9566035270690918
Batch 3/64 loss: 0.8195023536682129
Batch 4/64 loss: 0.7599835395812988
Batch 5/64 loss: 0.58538818359375
Batch 6/64 loss: 0.9165005683898926
Batch 7/64 loss: 0.9420523643493652
Batch 8/64 loss: 1.2564406394958496
Batch 9/64 loss: 0.6887640953063965
Batch 10/64 loss: 1.4040155410766602
Batch 11/64 loss: 0.9885320663452148
Batch 12/64 loss: 0.6434035301208496
Batch 13/64 loss: 0.9098830223083496
Batch 14/64 loss: 1.1503705978393555
Batch 15/64 loss: 0.8035941123962402
Batch 16/64 loss: 0.6942462921142578
Batch 17/64 loss: 1.276176929473877
Batch 18/64 loss: 1.0659079551696777
Batch 19/64 loss: 0.8297734260559082
Batch 20/64 loss: 0.6369695663452148
Batch 21/64 loss: 0.8821811676025391
Batch 22/64 loss: 1.2968482971191406
Batch 23/64 loss: 1.0555047988891602
Batch 24/64 loss: 0.9748601913452148
Batch 25/64 loss: 0.5942068099975586
Batch 26/64 loss: 1.1719212532043457
Batch 27/64 loss: 0.6122255325317383
Batch 28/64 loss: 0.755272388458252
Batch 29/64 loss: 0.5821552276611328
Batch 30/64 loss: 0.8906741142272949
Batch 31/64 loss: 0.866053581237793
Batch 32/64 loss: 1.0028376579284668
Batch 33/64 loss: 0.9556241035461426
Batch 34/64 loss: 1.1405773162841797
Batch 35/64 loss: 1.247459888458252
Batch 36/64 loss: 0.7476205825805664
Batch 37/64 loss: 0.878089427947998
Batch 38/64 loss: 0.9728364944458008
Batch 39/64 loss: 1.0609126091003418
Batch 40/64 loss: 0.6304020881652832
Batch 41/64 loss: 0.5855464935302734
Batch 42/64 loss: 0.6335320472717285
Batch 43/64 loss: 0.7094931602478027
Batch 44/64 loss: 1.2417354583740234
Batch 45/64 loss: 0.930138111114502
Batch 46/64 loss: 1.5476155281066895
Batch 47/64 loss: 0.6650853157043457
Batch 48/64 loss: 0.9860467910766602
Batch 49/64 loss: 0.7369198799133301
Batch 50/64 loss: 0.8161787986755371
Batch 51/64 loss: 0.9828519821166992
Batch 52/64 loss: 1.18247652053833
Batch 53/64 loss: 0.48015356063842773
Batch 54/64 loss: 0.6474437713623047
Batch 55/64 loss: 0.8606305122375488
Batch 56/64 loss: 1.0753068923950195
Batch 57/64 loss: 1.0027637481689453
Batch 58/64 loss: 1.0241479873657227
Batch 59/64 loss: 1.085564136505127
Batch 60/64 loss: 0.7225847244262695
Batch 61/64 loss: 0.8628463745117188
Batch 62/64 loss: 0.9399361610412598
Batch 63/64 loss: 0.8755474090576172
Batch 64/64 loss: -2.873676300048828
Epoch 141  Train loss: 0.861507557887657  Val loss: 0.925473924354999
Epoch 142
-------------------------------
Batch 1/64 loss: 0.903104305267334
Batch 2/64 loss: 0.7102656364440918
Batch 3/64 loss: 0.7849607467651367
Batch 4/64 loss: 0.8960204124450684
Batch 5/64 loss: 0.9253988265991211
Batch 6/64 loss: 1.201249599456787
Batch 7/64 loss: 1.2560577392578125
Batch 8/64 loss: 0.750330924987793
Batch 9/64 loss: 1.0886058807373047
Batch 10/64 loss: 0.8462882041931152
Batch 11/64 loss: 0.8731818199157715
Batch 12/64 loss: 1.1078953742980957
Batch 13/64 loss: 1.1272845268249512
Batch 14/64 loss: 0.7094988822937012
Batch 15/64 loss: 0.7456626892089844
Batch 16/64 loss: 0.8157591819763184
Batch 17/64 loss: 0.730257511138916
Batch 18/64 loss: 0.9418163299560547
Batch 19/64 loss: 0.9211502075195312
Batch 20/64 loss: 0.7066936492919922
Batch 21/64 loss: 1.021677017211914
Batch 22/64 loss: 0.5003728866577148
Batch 23/64 loss: 0.5588269233703613
Batch 24/64 loss: 0.8266372680664062
Batch 25/64 loss: 0.9810428619384766
Batch 26/64 loss: 1.0868892669677734
Batch 27/64 loss: 0.8157162666320801
Batch 28/64 loss: 0.9750957489013672
Batch 29/64 loss: 1.037930965423584
Batch 30/64 loss: 0.5691804885864258
Batch 31/64 loss: 1.0107746124267578
Batch 32/64 loss: 0.9633045196533203
Batch 33/64 loss: 0.642463207244873
Batch 34/64 loss: 0.6857538223266602
Batch 35/64 loss: 0.7748832702636719
Batch 36/64 loss: 0.9700255393981934
Batch 37/64 loss: 0.8420276641845703
Batch 38/64 loss: 1.1885766983032227
Batch 39/64 loss: 1.0626134872436523
Batch 40/64 loss: 0.9855270385742188
Batch 41/64 loss: 0.8819746971130371
Batch 42/64 loss: 0.5560317039489746
Batch 43/64 loss: 1.0347375869750977
Batch 44/64 loss: 0.8636517524719238
Batch 45/64 loss: 0.7238373756408691
Batch 46/64 loss: 0.8090977668762207
Batch 47/64 loss: 0.7461843490600586
Batch 48/64 loss: 0.749760627746582
Batch 49/64 loss: 0.7311258316040039
Batch 50/64 loss: 0.9049267768859863
Batch 51/64 loss: 0.7913274765014648
Batch 52/64 loss: 0.852475643157959
Batch 53/64 loss: 0.8407320976257324
Batch 54/64 loss: 0.6147165298461914
Batch 55/64 loss: 1.1910862922668457
Batch 56/64 loss: 0.8460803031921387
Batch 57/64 loss: 0.5903654098510742
Batch 58/64 loss: 0.9789271354675293
Batch 59/64 loss: 0.7714662551879883
Batch 60/64 loss: 0.7088794708251953
Batch 61/64 loss: 0.5688023567199707
Batch 62/64 loss: 0.636085033416748
Batch 63/64 loss: 0.5793070793151855
Batch 64/64 loss: -2.8376593589782715
Epoch 142  Train loss: 0.8060256789712344  Val loss: 0.9309904157500906
Epoch 143
-------------------------------
Batch 1/64 loss: 1.2168498039245605
Batch 2/64 loss: 0.6626944541931152
Batch 3/64 loss: 0.8965582847595215
Batch 4/64 loss: 1.053168773651123
Batch 5/64 loss: 1.0762295722961426
Batch 6/64 loss: 0.8120570182800293
Batch 7/64 loss: 0.9340906143188477
Batch 8/64 loss: 0.5431442260742188
Batch 9/64 loss: 0.9027876853942871
Batch 10/64 loss: 0.5239372253417969
Batch 11/64 loss: 0.838315486907959
Batch 12/64 loss: 0.9131135940551758
Batch 13/64 loss: 0.6976461410522461
Batch 14/64 loss: 0.7652597427368164
Batch 15/64 loss: 0.6873292922973633
Batch 16/64 loss: 0.5938267707824707
Batch 17/64 loss: 0.6267719268798828
Batch 18/64 loss: 0.694007396697998
Batch 19/64 loss: 0.611910343170166
Batch 20/64 loss: 0.4205050468444824
Batch 21/64 loss: 0.7123961448669434
Batch 22/64 loss: 0.8887639045715332
Batch 23/64 loss: 0.889714241027832
Batch 24/64 loss: 1.3471741676330566
Batch 25/64 loss: 0.717475414276123
Batch 26/64 loss: 0.8187575340270996
Batch 27/64 loss: 0.6617112159729004
Batch 28/64 loss: 0.8583240509033203
Batch 29/64 loss: 0.8346385955810547
Batch 30/64 loss: 0.7348155975341797
Batch 31/64 loss: 0.8514308929443359
Batch 32/64 loss: 0.889620304107666
Batch 33/64 loss: 0.8588376045227051
Batch 34/64 loss: 0.7507281303405762
Batch 35/64 loss: 1.2044014930725098
Batch 36/64 loss: 1.1801958084106445
Batch 37/64 loss: 0.7575931549072266
Batch 38/64 loss: 0.7257223129272461
Batch 39/64 loss: 0.7718462944030762
Batch 40/64 loss: 0.87542724609375
Batch 41/64 loss: 0.9118819236755371
Batch 42/64 loss: 0.7086033821105957
Batch 43/64 loss: 0.7008256912231445
Batch 44/64 loss: 1.6887712478637695
Batch 45/64 loss: 0.7117424011230469
Batch 46/64 loss: 1.0329856872558594
Batch 47/64 loss: 0.7017397880554199
Batch 48/64 loss: 0.6699819564819336
Batch 49/64 loss: 0.9039649963378906
Batch 50/64 loss: 1.5181365013122559
Batch 51/64 loss: 0.7497978210449219
Batch 52/64 loss: 1.1853947639465332
Batch 53/64 loss: 0.9065227508544922
Batch 54/64 loss: 1.17144775390625
Batch 55/64 loss: 0.5576515197753906
Batch 56/64 loss: 0.85052490234375
Batch 57/64 loss: 0.6026029586791992
Batch 58/64 loss: 0.9033331871032715
Batch 59/64 loss: 1.2961668968200684
Batch 60/64 loss: 0.9491229057312012
Batch 61/64 loss: 1.1710057258605957
Batch 62/64 loss: 0.5737442970275879
Batch 63/64 loss: 0.815493106842041
Batch 64/64 loss: -2.5550193786621094
Epoch 143  Train loss: 0.8182738061044731  Val loss: 1.0028262318614423
Epoch 144
-------------------------------
Batch 1/64 loss: 0.8209824562072754
Batch 2/64 loss: 0.8594355583190918
Batch 3/64 loss: 0.9804463386535645
Batch 4/64 loss: 0.9091534614562988
Batch 5/64 loss: 1.034360408782959
Batch 6/64 loss: 0.9069509506225586
Batch 7/64 loss: 0.9121475219726562
Batch 8/64 loss: 0.6080560684204102
Batch 9/64 loss: 0.670379638671875
Batch 10/64 loss: 0.7929487228393555
Batch 11/64 loss: 0.4844632148742676
Batch 12/64 loss: 0.841463565826416
Batch 13/64 loss: 0.9059886932373047
Batch 14/64 loss: 1.1107120513916016
Batch 15/64 loss: 1.4516286849975586
Batch 16/64 loss: 0.5690641403198242
Batch 17/64 loss: 1.1461777687072754
Batch 18/64 loss: 1.1062960624694824
Batch 19/64 loss: 0.6095781326293945
Batch 20/64 loss: 0.873042106628418
Batch 21/64 loss: 0.8684983253479004
Batch 22/64 loss: 0.6320476531982422
Batch 23/64 loss: 0.8807969093322754
Batch 24/64 loss: 1.0674715042114258
Batch 25/64 loss: 0.8112874031066895
Batch 26/64 loss: 0.6056728363037109
Batch 27/64 loss: 1.0368270874023438
Batch 28/64 loss: 1.0623831748962402
Batch 29/64 loss: 0.6804800033569336
Batch 30/64 loss: 1.0821661949157715
Batch 31/64 loss: 0.5968117713928223
Batch 32/64 loss: 1.053532600402832
Batch 33/64 loss: 0.5082030296325684
Batch 34/64 loss: 0.7320103645324707
Batch 35/64 loss: 0.7878513336181641
Batch 36/64 loss: 0.9604630470275879
Batch 37/64 loss: 1.4317026138305664
Batch 38/64 loss: 0.8334035873413086
Batch 39/64 loss: 0.6634583473205566
Batch 40/64 loss: 0.6780343055725098
Batch 41/64 loss: 0.8955879211425781
Batch 42/64 loss: 0.7291150093078613
Batch 43/64 loss: 0.7362561225891113
Batch 44/64 loss: 0.8370571136474609
Batch 45/64 loss: 1.0969038009643555
Batch 46/64 loss: 1.0752038955688477
Batch 47/64 loss: 1.5105080604553223
Batch 48/64 loss: 0.5426626205444336
Batch 49/64 loss: 1.1938157081604004
Batch 50/64 loss: 0.869420051574707
Batch 51/64 loss: 0.5957784652709961
Batch 52/64 loss: 0.8434224128723145
Batch 53/64 loss: 1.0082097053527832
Batch 54/64 loss: 0.8203701972961426
Batch 55/64 loss: 0.9478030204772949
Batch 56/64 loss: 0.6076512336730957
Batch 57/64 loss: 0.8580045700073242
Batch 58/64 loss: 0.7885642051696777
Batch 59/64 loss: 0.5050768852233887
Batch 60/64 loss: 0.6036958694458008
Batch 61/64 loss: 0.7292976379394531
Batch 62/64 loss: 0.6916952133178711
Batch 63/64 loss: 1.068345069885254
Batch 64/64 loss: -2.5257015228271484
Epoch 144  Train loss: 0.8192399417652804  Val loss: 0.9144271247575373
Epoch 145
-------------------------------
Batch 1/64 loss: 0.5991783142089844
Batch 2/64 loss: 0.6841926574707031
Batch 3/64 loss: 0.8736553192138672
Batch 4/64 loss: 0.6345281600952148
Batch 5/64 loss: 0.9887089729309082
Batch 6/64 loss: 0.7221121788024902
Batch 7/64 loss: 0.6007108688354492
Batch 8/64 loss: 0.48279380798339844
Batch 9/64 loss: 0.7890100479125977
Batch 10/64 loss: 0.5915970802307129
Batch 11/64 loss: 0.948305606842041
Batch 12/64 loss: 0.9354171752929688
Batch 13/64 loss: 0.630549430847168
Batch 14/64 loss: 0.7216043472290039
Batch 15/64 loss: 0.8213143348693848
Batch 16/64 loss: 0.6946210861206055
Batch 17/64 loss: 0.6006460189819336
Batch 18/64 loss: 0.9444069862365723
Batch 19/64 loss: 0.8570065498352051
Batch 20/64 loss: 0.8730449676513672
Batch 21/64 loss: 1.0504236221313477
Batch 22/64 loss: 0.795109748840332
Batch 23/64 loss: 0.8752803802490234
Batch 24/64 loss: 0.5834493637084961
Batch 25/64 loss: 1.1722345352172852
Batch 26/64 loss: 0.9411702156066895
Batch 27/64 loss: 0.8866219520568848
Batch 28/64 loss: 1.0604896545410156
Batch 29/64 loss: 0.6618733406066895
Batch 30/64 loss: 0.9499621391296387
Batch 31/64 loss: 0.6373286247253418
Batch 32/64 loss: 0.8991684913635254
Batch 33/64 loss: 0.8077692985534668
Batch 34/64 loss: 0.8213119506835938
Batch 35/64 loss: 0.37610578536987305
Batch 36/64 loss: 0.9616703987121582
Batch 37/64 loss: 0.5971131324768066
Batch 38/64 loss: 1.3849000930786133
Batch 39/64 loss: 1.018653392791748
Batch 40/64 loss: 0.7289056777954102
Batch 41/64 loss: 1.0855422019958496
Batch 42/64 loss: 0.8608264923095703
Batch 43/64 loss: 0.598297119140625
Batch 44/64 loss: 0.6915411949157715
Batch 45/64 loss: 0.9812450408935547
Batch 46/64 loss: 0.5729036331176758
Batch 47/64 loss: 0.6155266761779785
Batch 48/64 loss: 0.6854977607727051
Batch 49/64 loss: 0.6307821273803711
Batch 50/64 loss: 1.087468147277832
Batch 51/64 loss: 0.6546335220336914
Batch 52/64 loss: 0.7310032844543457
Batch 53/64 loss: 0.5447402000427246
Batch 54/64 loss: 0.7266435623168945
Batch 55/64 loss: 0.7877964973449707
Batch 56/64 loss: 1.1470627784729004
Batch 57/64 loss: 0.3794069290161133
Batch 58/64 loss: 0.8699793815612793
Batch 59/64 loss: 1.2648143768310547
Batch 60/64 loss: 0.8056011199951172
Batch 61/64 loss: 0.9194307327270508
Batch 62/64 loss: 0.6889090538024902
Batch 63/64 loss: 0.8533930778503418
Batch 64/64 loss: -2.9451241493225098
Epoch 145  Train loss: 0.7557199609045889  Val loss: 0.756426185267078
Saving best model, epoch: 145
Epoch 146
-------------------------------
Batch 1/64 loss: 1.0882086753845215
Batch 2/64 loss: 0.6094655990600586
Batch 3/64 loss: 0.7781577110290527
Batch 4/64 loss: 0.6204996109008789
Batch 5/64 loss: 0.8080077171325684
Batch 6/64 loss: 0.44940805435180664
Batch 7/64 loss: 1.2218499183654785
Batch 8/64 loss: 0.7340140342712402
Batch 9/64 loss: 0.7051305770874023
Batch 10/64 loss: 0.8911848068237305
Batch 11/64 loss: 0.7648296356201172
Batch 12/64 loss: 0.4416227340698242
Batch 13/64 loss: 0.5280032157897949
Batch 14/64 loss: 0.8424859046936035
Batch 15/64 loss: 0.5608596801757812
Batch 16/64 loss: 0.9930047988891602
Batch 17/64 loss: 0.7073307037353516
Batch 18/64 loss: 1.0840826034545898
Batch 19/64 loss: 0.8931102752685547
Batch 20/64 loss: 0.6466093063354492
Batch 21/64 loss: 0.7811784744262695
Batch 22/64 loss: 0.7995438575744629
Batch 23/64 loss: 0.6821155548095703
Batch 24/64 loss: 0.683314323425293
Batch 25/64 loss: 0.2872285842895508
Batch 26/64 loss: 0.8499536514282227
Batch 27/64 loss: 1.7074732780456543
Batch 28/64 loss: 0.4790916442871094
Batch 29/64 loss: 1.011556625366211
Batch 30/64 loss: 0.7333889007568359
Batch 31/64 loss: 1.0630207061767578
Batch 32/64 loss: 0.7661576271057129
Batch 33/64 loss: 0.7608547210693359
Batch 34/64 loss: 1.0135464668273926
Batch 35/64 loss: 1.2569432258605957
Batch 36/64 loss: 0.8515839576721191
Batch 37/64 loss: 0.884148120880127
Batch 38/64 loss: 0.8465852737426758
Batch 39/64 loss: 0.9149551391601562
Batch 40/64 loss: 1.112104892730713
Batch 41/64 loss: 0.7837285995483398
Batch 42/64 loss: 1.092729091644287
Batch 43/64 loss: 0.9396767616271973
Batch 44/64 loss: 1.6155447959899902
Batch 45/64 loss: 1.0130419731140137
Batch 46/64 loss: 0.7312092781066895
Batch 47/64 loss: 1.0093631744384766
Batch 48/64 loss: 1.307640552520752
Batch 49/64 loss: 0.7234721183776855
Batch 50/64 loss: 0.6909918785095215
Batch 51/64 loss: 0.8843750953674316
Batch 52/64 loss: 1.2070589065551758
Batch 53/64 loss: 0.6377263069152832
Batch 54/64 loss: 0.866239070892334
Batch 55/64 loss: 0.9079504013061523
Batch 56/64 loss: 0.9372053146362305
Batch 57/64 loss: 1.1376276016235352
Batch 58/64 loss: 0.767486572265625
Batch 59/64 loss: 0.8399925231933594
Batch 60/64 loss: 0.8021783828735352
Batch 61/64 loss: 0.833096981048584
Batch 62/64 loss: 1.1285691261291504
Batch 63/64 loss: 0.9153122901916504
Batch 64/64 loss: -2.4698596000671387
Epoch 146  Train loss: 0.8278028656454647  Val loss: 0.8716708966546861
Epoch 147
-------------------------------
Batch 1/64 loss: 0.729102611541748
Batch 2/64 loss: 0.6537628173828125
Batch 3/64 loss: 0.542572021484375
Batch 4/64 loss: 0.7315897941589355
Batch 5/64 loss: 1.1409835815429688
Batch 6/64 loss: 0.4853324890136719
Batch 7/64 loss: 0.747889518737793
Batch 8/64 loss: 0.9608955383300781
Batch 9/64 loss: 1.1485590934753418
Batch 10/64 loss: 0.883540153503418
Batch 11/64 loss: 0.8665380477905273
Batch 12/64 loss: 0.6101937294006348
Batch 13/64 loss: 0.9892163276672363
Batch 14/64 loss: 1.0696911811828613
Batch 15/64 loss: 0.964627742767334
Batch 16/64 loss: 1.2991490364074707
Batch 17/64 loss: 0.792086124420166
Batch 18/64 loss: 1.0050783157348633
Batch 19/64 loss: 0.9278569221496582
Batch 20/64 loss: 0.7777986526489258
Batch 21/64 loss: 0.780059814453125
Batch 22/64 loss: 0.9228272438049316
Batch 23/64 loss: 0.6139864921569824
Batch 24/64 loss: 0.6376852989196777
Batch 25/64 loss: 0.6927289962768555
Batch 26/64 loss: 0.7684898376464844
Batch 27/64 loss: 0.9834837913513184
Batch 28/64 loss: 0.7765984535217285
Batch 29/64 loss: 0.8085594177246094
Batch 30/64 loss: 0.9018735885620117
Batch 31/64 loss: 0.8954935073852539
Batch 32/64 loss: 0.46692371368408203
Batch 33/64 loss: 0.8558969497680664
Batch 34/64 loss: 0.9207262992858887
Batch 35/64 loss: 0.7464900016784668
Batch 36/64 loss: 0.6410050392150879
Batch 37/64 loss: 0.8896479606628418
Batch 38/64 loss: 0.9453821182250977
Batch 39/64 loss: 0.40335702896118164
Batch 40/64 loss: 1.6644492149353027
Batch 41/64 loss: 0.8768606185913086
Batch 42/64 loss: 0.6544041633605957
Batch 43/64 loss: 0.8340778350830078
Batch 44/64 loss: 0.8106255531311035
Batch 45/64 loss: 0.939906120300293
Batch 46/64 loss: 0.9537811279296875
Batch 47/64 loss: 0.6411480903625488
Batch 48/64 loss: 0.6578741073608398
Batch 49/64 loss: 0.865384578704834
Batch 50/64 loss: 0.8006143569946289
Batch 51/64 loss: 0.7082352638244629
Batch 52/64 loss: 0.9949727058410645
Batch 53/64 loss: 0.8100118637084961
Batch 54/64 loss: 1.0318622589111328
Batch 55/64 loss: 0.6236481666564941
Batch 56/64 loss: 0.9003148078918457
Batch 57/64 loss: 0.6110963821411133
Batch 58/64 loss: 0.6939830780029297
Batch 59/64 loss: 0.7950515747070312
Batch 60/64 loss: 0.8462753295898438
Batch 61/64 loss: 1.1237516403198242
Batch 62/64 loss: 0.8569245338439941
Batch 63/64 loss: 0.9061226844787598
Batch 64/64 loss: -2.7054853439331055
Epoch 147  Train loss: 0.7929402557073855  Val loss: 0.8608998826279264
Epoch 148
-------------------------------
Batch 1/64 loss: 0.7073855400085449
Batch 2/64 loss: 0.636141300201416
Batch 3/64 loss: 1.0842103958129883
Batch 4/64 loss: 0.8474287986755371
Batch 5/64 loss: 0.47187328338623047
Batch 6/64 loss: 0.9921960830688477
Batch 7/64 loss: 0.9751954078674316
Batch 8/64 loss: 0.8444976806640625
Batch 9/64 loss: 0.7356386184692383
Batch 10/64 loss: 0.5460801124572754
Batch 11/64 loss: 0.5549402236938477
Batch 12/64 loss: 0.7158074378967285
Batch 13/64 loss: 0.7397584915161133
Batch 14/64 loss: 0.7208561897277832
Batch 15/64 loss: 0.857029914855957
Batch 16/64 loss: 0.6978912353515625
Batch 17/64 loss: 0.6331725120544434
Batch 18/64 loss: 0.9646992683410645
Batch 19/64 loss: 0.7386932373046875
Batch 20/64 loss: 0.5698132514953613
Batch 21/64 loss: 0.8652992248535156
Batch 22/64 loss: 0.6516404151916504
Batch 23/64 loss: 0.8608241081237793
Batch 24/64 loss: 1.1159396171569824
Batch 25/64 loss: 0.9588274955749512
Batch 26/64 loss: 0.8728537559509277
Batch 27/64 loss: 0.47838783264160156
Batch 28/64 loss: 0.8061795234680176
Batch 29/64 loss: 0.6792612075805664
Batch 30/64 loss: 0.7855238914489746
Batch 31/64 loss: 0.7417268753051758
Batch 32/64 loss: 0.8869171142578125
Batch 33/64 loss: 0.7157130241394043
Batch 34/64 loss: 0.5608515739440918
Batch 35/64 loss: 1.3275985717773438
Batch 36/64 loss: 0.688906192779541
Batch 37/64 loss: 0.6675095558166504
Batch 38/64 loss: 0.7066140174865723
Batch 39/64 loss: 0.878382682800293
Batch 40/64 loss: 0.6215577125549316
Batch 41/64 loss: 1.051069736480713
Batch 42/64 loss: 0.7176733016967773
Batch 43/64 loss: 0.7978024482727051
Batch 44/64 loss: 0.5215668678283691
Batch 45/64 loss: 0.588172435760498
Batch 46/64 loss: 0.8255653381347656
Batch 47/64 loss: 0.8421430587768555
Batch 48/64 loss: 0.8393292427062988
Batch 49/64 loss: 0.6454038619995117
Batch 50/64 loss: 0.8343701362609863
Batch 51/64 loss: 0.669520378112793
Batch 52/64 loss: 0.9848847389221191
Batch 53/64 loss: 0.6017374992370605
Batch 54/64 loss: 0.9720978736877441
Batch 55/64 loss: 0.9363689422607422
Batch 56/64 loss: 1.0519423484802246
Batch 57/64 loss: 0.7184271812438965
Batch 58/64 loss: 0.7790927886962891
Batch 59/64 loss: 0.7374534606933594
Batch 60/64 loss: 0.8388853073120117
Batch 61/64 loss: 0.7318696975708008
Batch 62/64 loss: 0.7647786140441895
Batch 63/64 loss: 1.0357160568237305
Batch 64/64 loss: -1.9744672775268555
Epoch 148  Train loss: 0.75151128207936  Val loss: 0.7705390772868678
Epoch 149
-------------------------------
Batch 1/64 loss: 0.4085226058959961
Batch 2/64 loss: 0.8608055114746094
Batch 3/64 loss: 0.8555746078491211
Batch 4/64 loss: 0.7579045295715332
Batch 5/64 loss: 0.6681275367736816
Batch 6/64 loss: 0.3800477981567383
Batch 7/64 loss: 0.7428603172302246
Batch 8/64 loss: 0.8228654861450195
Batch 9/64 loss: 0.6592559814453125
Batch 10/64 loss: 0.5500068664550781
Batch 11/64 loss: 0.8912715911865234
Batch 12/64 loss: 0.8569927215576172
Batch 13/64 loss: 0.9000630378723145
Batch 14/64 loss: 0.3729534149169922
Batch 15/64 loss: 0.7812013626098633
Batch 16/64 loss: 1.0201940536499023
Batch 17/64 loss: 0.6706061363220215
Batch 18/64 loss: 0.4156365394592285
Batch 19/64 loss: 0.772374153137207
Batch 20/64 loss: 0.8964805603027344
Batch 21/64 loss: 0.5216913223266602
Batch 22/64 loss: 0.9973883628845215
Batch 23/64 loss: 0.5452113151550293
Batch 24/64 loss: 0.8369536399841309
Batch 25/64 loss: 0.8290972709655762
Batch 26/64 loss: 0.6103510856628418
Batch 27/64 loss: 0.5680856704711914
Batch 28/64 loss: 0.9187092781066895
Batch 29/64 loss: 0.8464694023132324
Batch 30/64 loss: 1.0783629417419434
Batch 31/64 loss: 0.633735179901123
Batch 32/64 loss: 0.6361727714538574
Batch 33/64 loss: 0.8300542831420898
Batch 34/64 loss: 0.5211410522460938
Batch 35/64 loss: 0.551063060760498
Batch 36/64 loss: 0.7564496994018555
Batch 37/64 loss: 0.6666173934936523
Batch 38/64 loss: 1.1449933052062988
Batch 39/64 loss: 0.9927020072937012
Batch 40/64 loss: 0.46327781677246094
Batch 41/64 loss: 1.077160358428955
Batch 42/64 loss: 0.6965146064758301
Batch 43/64 loss: 0.9554929733276367
Batch 44/64 loss: 0.8895626068115234
Batch 45/64 loss: 0.36536741256713867
Batch 46/64 loss: 1.1429815292358398
Batch 47/64 loss: 0.7663793563842773
Batch 48/64 loss: 0.7114872932434082
Batch 49/64 loss: 0.8299469947814941
Batch 50/64 loss: 0.8276429176330566
Batch 51/64 loss: 0.9571127891540527
Batch 52/64 loss: 0.9068160057067871
Batch 53/64 loss: 0.6267995834350586
Batch 54/64 loss: 0.4647560119628906
Batch 55/64 loss: 1.0951189994812012
Batch 56/64 loss: 0.8962259292602539
Batch 57/64 loss: 1.0900745391845703
Batch 58/64 loss: 0.6360273361206055
Batch 59/64 loss: 0.6248965263366699
Batch 60/64 loss: 0.6479449272155762
Batch 61/64 loss: 0.8383984565734863
Batch 62/64 loss: 0.7834873199462891
Batch 63/64 loss: 0.826993465423584
Batch 64/64 loss: -2.570181369781494
Epoch 149  Train loss: 0.7209697816886154  Val loss: 0.6996912546583877
Saving best model, epoch: 149
Epoch 150
-------------------------------
Batch 1/64 loss: 0.6500701904296875
Batch 2/64 loss: 0.779106616973877
Batch 3/64 loss: 0.5067648887634277
Batch 4/64 loss: 0.6176004409790039
Batch 5/64 loss: 0.6738214492797852
Batch 6/64 loss: 0.5222163200378418
Batch 7/64 loss: 0.963930606842041
Batch 8/64 loss: 0.7784161567687988
Batch 9/64 loss: 0.8153319358825684
Batch 10/64 loss: 0.4042677879333496
Batch 11/64 loss: 0.6703157424926758
Batch 12/64 loss: 0.8681492805480957
Batch 13/64 loss: 1.022956371307373
Batch 14/64 loss: 0.6315064430236816
Batch 15/64 loss: 0.7899060249328613
Batch 16/64 loss: 0.6659383773803711
Batch 17/64 loss: 0.700282096862793
Batch 18/64 loss: 0.8088688850402832
Batch 19/64 loss: 1.1577506065368652
Batch 20/64 loss: 0.6876592636108398
Batch 21/64 loss: 0.43596553802490234
Batch 22/64 loss: 0.6672177314758301
Batch 23/64 loss: 0.7710971832275391
Batch 24/64 loss: 0.715113639831543
Batch 25/64 loss: 0.9033341407775879
Batch 26/64 loss: 0.6515750885009766
Batch 27/64 loss: 0.673060417175293
Batch 28/64 loss: 0.7704801559448242
Batch 29/64 loss: 0.6174402236938477
Batch 30/64 loss: 0.3498196601867676
Batch 31/64 loss: 0.7019023895263672
Batch 32/64 loss: 0.8095951080322266
Batch 33/64 loss: 0.9143133163452148
Batch 34/64 loss: 0.9993577003479004
Batch 35/64 loss: 0.6842460632324219
Batch 36/64 loss: 0.7497477531433105
Batch 37/64 loss: 1.1545677185058594
Batch 38/64 loss: 0.6014003753662109
Batch 39/64 loss: 0.4179368019104004
Batch 40/64 loss: 0.7350749969482422
Batch 41/64 loss: 0.6042113304138184
Batch 42/64 loss: 0.8082647323608398
Batch 43/64 loss: 0.5506353378295898
Batch 44/64 loss: 0.5573596954345703
Batch 45/64 loss: 0.8253741264343262
Batch 46/64 loss: 0.5753173828125
Batch 47/64 loss: 0.7855072021484375
Batch 48/64 loss: 0.845977783203125
Batch 49/64 loss: 0.8100876808166504
Batch 50/64 loss: 1.0906100273132324
Batch 51/64 loss: 0.9224343299865723
Batch 52/64 loss: 0.6097745895385742
Batch 53/64 loss: 0.596646785736084
Batch 54/64 loss: 0.9590458869934082
Batch 55/64 loss: 0.6010751724243164
Batch 56/64 loss: 0.8186807632446289
Batch 57/64 loss: 0.49414682388305664
Batch 58/64 loss: 0.9400711059570312
Batch 59/64 loss: 0.7206206321716309
Batch 60/64 loss: 0.6380372047424316
Batch 61/64 loss: 0.9344215393066406
Batch 62/64 loss: 0.8290829658508301
Batch 63/64 loss: 0.8991193771362305
Batch 64/64 loss: -2.6809864044189453
Epoch 150  Train loss: 0.6971587162391812  Val loss: 0.7217688019742671
Epoch 151
-------------------------------
Batch 1/64 loss: 0.8262448310852051
Batch 2/64 loss: 1.0317721366882324
Batch 3/64 loss: 0.6515111923217773
Batch 4/64 loss: 0.5810585021972656
Batch 5/64 loss: 0.5507512092590332
Batch 6/64 loss: 0.5466065406799316
Batch 7/64 loss: 0.7466225624084473
Batch 8/64 loss: 0.4838275909423828
Batch 9/64 loss: 0.601250171661377
Batch 10/64 loss: 0.6295766830444336
Batch 11/64 loss: 0.4718937873840332
Batch 12/64 loss: 0.8748273849487305
Batch 13/64 loss: 0.754239559173584
Batch 14/64 loss: 0.8633508682250977
Batch 15/64 loss: 0.7847733497619629
Batch 16/64 loss: 0.5497035980224609
Batch 17/64 loss: 0.748816967010498
Batch 18/64 loss: 0.7990798950195312
Batch 19/64 loss: 0.6780829429626465
Batch 20/64 loss: 0.562408447265625
Batch 21/64 loss: 0.9164180755615234
Batch 22/64 loss: 0.9637665748596191
Batch 23/64 loss: 0.5973939895629883
Batch 24/64 loss: 1.279895305633545
Batch 25/64 loss: 1.2511253356933594
Batch 26/64 loss: 0.7090754508972168
Batch 27/64 loss: 1.0309481620788574
Batch 28/64 loss: 0.9575767517089844
Batch 29/64 loss: 0.6383209228515625
Batch 30/64 loss: 0.24409198760986328
Batch 31/64 loss: 0.777519702911377
Batch 32/64 loss: 0.959956169128418
Batch 33/64 loss: 0.7491979598999023
Batch 34/64 loss: 0.48192453384399414
Batch 35/64 loss: 0.3705711364746094
Batch 36/64 loss: 0.5241408348083496
Batch 37/64 loss: 0.7971153259277344
Batch 38/64 loss: 0.8040075302124023
Batch 39/64 loss: 0.7715878486633301
Batch 40/64 loss: 0.6540746688842773
Batch 41/64 loss: 0.6837120056152344
Batch 42/64 loss: 0.45665693283081055
Batch 43/64 loss: 0.7141022682189941
Batch 44/64 loss: 0.6716771125793457
Batch 45/64 loss: 0.7122478485107422
Batch 46/64 loss: 0.7339119911193848
Batch 47/64 loss: 0.6681933403015137
Batch 48/64 loss: 0.5819511413574219
Batch 49/64 loss: 1.1610817909240723
Batch 50/64 loss: 1.0045380592346191
Batch 51/64 loss: 1.05625581741333
Batch 52/64 loss: 0.6554789543151855
Batch 53/64 loss: 0.4170527458190918
Batch 54/64 loss: 0.5739235877990723
Batch 55/64 loss: 1.0124716758728027
Batch 56/64 loss: 0.5422158241271973
Batch 57/64 loss: 0.5790653228759766
Batch 58/64 loss: 0.4072098731994629
Batch 59/64 loss: 0.6051540374755859
Batch 60/64 loss: 0.5736532211303711
Batch 61/64 loss: 0.5795750617980957
Batch 62/64 loss: 0.926112174987793
Batch 63/64 loss: 0.6542701721191406
Batch 64/64 loss: -2.5436391830444336
Epoch 151  Train loss: 0.6788688323077033  Val loss: 0.6930541336741235
Saving best model, epoch: 151
Epoch 152
-------------------------------
Batch 1/64 loss: 0.8837776184082031
Batch 2/64 loss: 0.7586207389831543
Batch 3/64 loss: 0.6049118041992188
Batch 4/64 loss: 0.7419514656066895
Batch 5/64 loss: 0.5474405288696289
Batch 6/64 loss: 0.7669110298156738
Batch 7/64 loss: 0.4966721534729004
Batch 8/64 loss: 0.7706499099731445
Batch 9/64 loss: 0.8857722282409668
Batch 10/64 loss: 0.7824954986572266
Batch 11/64 loss: 0.6591830253601074
Batch 12/64 loss: 0.9813933372497559
Batch 13/64 loss: 0.572263240814209
Batch 14/64 loss: 0.6962776184082031
Batch 15/64 loss: 0.9262814521789551
Batch 16/64 loss: 0.9279251098632812
Batch 17/64 loss: 0.4331793785095215
Batch 18/64 loss: 0.7978920936584473
Batch 19/64 loss: 0.5452651977539062
Batch 20/64 loss: 0.47025632858276367
Batch 21/64 loss: 0.7136373519897461
Batch 22/64 loss: 0.8593730926513672
Batch 23/64 loss: 0.9932351112365723
Batch 24/64 loss: 0.2103734016418457
Batch 25/64 loss: 0.6882038116455078
Batch 26/64 loss: 0.5516390800476074
Batch 27/64 loss: 0.6333069801330566
Batch 28/64 loss: 0.5432767868041992
Batch 29/64 loss: 0.7889680862426758
Batch 30/64 loss: 0.5442900657653809
Batch 31/64 loss: 0.7086052894592285
Batch 32/64 loss: 0.6223196983337402
Batch 33/64 loss: 0.547149658203125
Batch 34/64 loss: 0.9413094520568848
Batch 35/64 loss: 0.6622605323791504
Batch 36/64 loss: 0.4645814895629883
Batch 37/64 loss: 0.5656018257141113
Batch 38/64 loss: 0.8245086669921875
Batch 39/64 loss: 0.996706485748291
Batch 40/64 loss: 0.9865469932556152
Batch 41/64 loss: 0.27344751358032227
Batch 42/64 loss: 0.6587715148925781
Batch 43/64 loss: 0.6114511489868164
Batch 44/64 loss: 0.4776778221130371
Batch 45/64 loss: 0.40368223190307617
Batch 46/64 loss: 0.7202472686767578
Batch 47/64 loss: 0.991858959197998
Batch 48/64 loss: 0.6071829795837402
Batch 49/64 loss: 0.7867717742919922
Batch 50/64 loss: 0.8287959098815918
Batch 51/64 loss: 0.5772318840026855
Batch 52/64 loss: 0.7502241134643555
Batch 53/64 loss: 0.7745742797851562
Batch 54/64 loss: 0.6415419578552246
Batch 55/64 loss: 0.942659854888916
Batch 56/64 loss: 0.7126307487487793
Batch 57/64 loss: 2.649998664855957
Batch 58/64 loss: 0.7627325057983398
Batch 59/64 loss: 1.0714211463928223
Batch 60/64 loss: 0.836738109588623
Batch 61/64 loss: 0.776710033416748
Batch 62/64 loss: 1.018838882446289
Batch 63/64 loss: 1.1840338706970215
Batch 64/64 loss: -2.005329132080078
Epoch 152  Train loss: 0.7160821952071844  Val loss: 1.598321468969391
Epoch 153
-------------------------------
Batch 1/64 loss: 0.9534392356872559
Batch 2/64 loss: 0.8686447143554688
Batch 3/64 loss: 0.8637876510620117
Batch 4/64 loss: 1.0454816818237305
Batch 5/64 loss: 0.9581356048583984
Batch 6/64 loss: 0.8654909133911133
Batch 7/64 loss: 0.9686169624328613
Batch 8/64 loss: 0.9503135681152344
Batch 9/64 loss: 0.7904376983642578
Batch 10/64 loss: 0.8525629043579102
Batch 11/64 loss: 1.3106646537780762
Batch 12/64 loss: 0.9331874847412109
Batch 13/64 loss: 0.9002952575683594
Batch 14/64 loss: 0.8093461990356445
Batch 15/64 loss: 0.897284984588623
Batch 16/64 loss: 1.389298915863037
Batch 17/64 loss: 0.8320999145507812
Batch 18/64 loss: 0.848386287689209
Batch 19/64 loss: 0.6579375267028809
Batch 20/64 loss: 0.8115782737731934
Batch 21/64 loss: 1.1245074272155762
Batch 22/64 loss: 1.0481948852539062
Batch 23/64 loss: 0.4877619743347168
Batch 24/64 loss: 0.9059910774230957
Batch 25/64 loss: 0.5929341316223145
Batch 26/64 loss: 0.6031937599182129
Batch 27/64 loss: 0.7088184356689453
Batch 28/64 loss: 0.8738365173339844
Batch 29/64 loss: 0.9807257652282715
Batch 30/64 loss: 0.7467422485351562
Batch 31/64 loss: 1.0671730041503906
Batch 32/64 loss: 0.9658918380737305
Batch 33/64 loss: 0.6086654663085938
Batch 34/64 loss: 0.780087947845459
Batch 35/64 loss: 0.49387168884277344
Batch 36/64 loss: 0.8577923774719238
Batch 37/64 loss: 0.7417111396789551
Batch 38/64 loss: 0.7404460906982422
Batch 39/64 loss: 0.7813539505004883
Batch 40/64 loss: 0.8745489120483398
Batch 41/64 loss: 0.7305121421813965
Batch 42/64 loss: 0.7966709136962891
Batch 43/64 loss: 0.6069192886352539
Batch 44/64 loss: 0.7386074066162109
Batch 45/64 loss: 0.8096075057983398
Batch 46/64 loss: 0.6235818862915039
Batch 47/64 loss: 0.3253660202026367
Batch 48/64 loss: 0.9218330383300781
Batch 49/64 loss: 0.5774526596069336
Batch 50/64 loss: 1.0093116760253906
Batch 51/64 loss: 1.0034112930297852
Batch 52/64 loss: 0.679415225982666
Batch 53/64 loss: 0.7475371360778809
Batch 54/64 loss: 0.6466841697692871
Batch 55/64 loss: 0.8067669868469238
Batch 56/64 loss: 0.5951642990112305
Batch 57/64 loss: 0.600736141204834
Batch 58/64 loss: 1.0895094871520996
Batch 59/64 loss: 0.9834232330322266
Batch 60/64 loss: 0.8604640960693359
Batch 61/64 loss: 0.7294101715087891
Batch 62/64 loss: 1.0684270858764648
Batch 63/64 loss: 0.5869784355163574
Batch 64/64 loss: -2.412715435028076
Epoch 153  Train loss: 0.7877567496954226  Val loss: 0.855259885493013
Epoch 154
-------------------------------
Batch 1/64 loss: 0.6530675888061523
Batch 2/64 loss: 0.9843964576721191
Batch 3/64 loss: 0.7071127891540527
Batch 4/64 loss: 0.7447032928466797
Batch 5/64 loss: 0.7016911506652832
Batch 6/64 loss: 0.8348417282104492
Batch 7/64 loss: 1.1387042999267578
Batch 8/64 loss: 0.6731233596801758
Batch 9/64 loss: 0.6464319229125977
Batch 10/64 loss: 0.860745906829834
Batch 11/64 loss: 0.7885313034057617
Batch 12/64 loss: 0.8131923675537109
Batch 13/64 loss: 0.8760876655578613
Batch 14/64 loss: 0.7958436012268066
Batch 15/64 loss: 1.0273947715759277
Batch 16/64 loss: 1.1111907958984375
Batch 17/64 loss: 0.9633121490478516
Batch 18/64 loss: 0.7024803161621094
Batch 19/64 loss: 0.5181493759155273
Batch 20/64 loss: 0.7471752166748047
Batch 21/64 loss: 0.549799919128418
Batch 22/64 loss: 0.6762166023254395
Batch 23/64 loss: 0.5790934562683105
Batch 24/64 loss: 0.3551959991455078
Batch 25/64 loss: 0.7434782981872559
Batch 26/64 loss: 0.6752567291259766
Batch 27/64 loss: 0.6220078468322754
Batch 28/64 loss: 0.41698169708251953
Batch 29/64 loss: 0.6667575836181641
Batch 30/64 loss: 0.9091877937316895
Batch 31/64 loss: 0.8750505447387695
Batch 32/64 loss: 0.8688945770263672
Batch 33/64 loss: 0.9522662162780762
Batch 34/64 loss: 0.9340109825134277
Batch 35/64 loss: 0.8435730934143066
Batch 36/64 loss: 0.5815005302429199
Batch 37/64 loss: 0.7018041610717773
Batch 38/64 loss: 0.5760245323181152
Batch 39/64 loss: 0.8208427429199219
Batch 40/64 loss: 0.5740680694580078
Batch 41/64 loss: 0.6693587303161621
Batch 42/64 loss: 0.7225375175476074
Batch 43/64 loss: 1.1841387748718262
Batch 44/64 loss: 0.7078614234924316
Batch 45/64 loss: 0.6640615463256836
Batch 46/64 loss: 0.5756473541259766
Batch 47/64 loss: 0.6641535758972168
Batch 48/64 loss: 0.9344425201416016
Batch 49/64 loss: 0.7290506362915039
Batch 50/64 loss: 0.8347430229187012
Batch 51/64 loss: 0.73980712890625
Batch 52/64 loss: 1.0000014305114746
Batch 53/64 loss: 0.901700496673584
Batch 54/64 loss: 0.5451836585998535
Batch 55/64 loss: 0.8779697418212891
Batch 56/64 loss: 0.7587852478027344
Batch 57/64 loss: 0.9334521293640137
Batch 58/64 loss: 0.42750120162963867
Batch 59/64 loss: 0.9690375328063965
Batch 60/64 loss: 1.0239982604980469
Batch 61/64 loss: 0.7580056190490723
Batch 62/64 loss: 0.7463116645812988
Batch 63/64 loss: 0.7832636833190918
Batch 64/64 loss: -3.0023341178894043
Epoch 154  Train loss: 0.7232854861839145  Val loss: 0.7556204844995872
Epoch 155
-------------------------------
Batch 1/64 loss: 0.8380069732666016
Batch 2/64 loss: 0.9994773864746094
Batch 3/64 loss: 1.0349555015563965
Batch 4/64 loss: 0.5389461517333984
Batch 5/64 loss: 0.7095093727111816
Batch 6/64 loss: 1.0531435012817383
Batch 7/64 loss: 0.32671117782592773
Batch 8/64 loss: 0.4932060241699219
Batch 9/64 loss: 0.572105884552002
Batch 10/64 loss: 0.5457663536071777
Batch 11/64 loss: 0.4759058952331543
Batch 12/64 loss: 0.5024957656860352
Batch 13/64 loss: 1.0472235679626465
Batch 14/64 loss: 0.7390918731689453
Batch 15/64 loss: 0.5633878707885742
Batch 16/64 loss: 0.5651092529296875
Batch 17/64 loss: 0.7514524459838867
Batch 18/64 loss: 1.090446949005127
Batch 19/64 loss: 1.0250921249389648
Batch 20/64 loss: 0.8095822334289551
Batch 21/64 loss: 0.722358226776123
Batch 22/64 loss: 0.6189756393432617
Batch 23/64 loss: 0.9255571365356445
Batch 24/64 loss: 0.8521728515625
Batch 25/64 loss: 0.8471965789794922
Batch 26/64 loss: 0.608757495880127
Batch 27/64 loss: 0.42835140228271484
Batch 28/64 loss: 0.6889753341674805
Batch 29/64 loss: 0.6349601745605469
Batch 30/64 loss: 0.7412362098693848
Batch 31/64 loss: 0.7291412353515625
Batch 32/64 loss: 0.7524185180664062
Batch 33/64 loss: 0.785405158996582
Batch 34/64 loss: 1.0001411437988281
Batch 35/64 loss: 0.45055723190307617
Batch 36/64 loss: 0.9032649993896484
Batch 37/64 loss: 0.927642822265625
Batch 38/64 loss: 0.45586729049682617
Batch 39/64 loss: 0.6931381225585938
Batch 40/64 loss: 0.7210969924926758
Batch 41/64 loss: 0.540771484375
Batch 42/64 loss: 0.637937068939209
Batch 43/64 loss: 0.9343500137329102
Batch 44/64 loss: 0.7839579582214355
Batch 45/64 loss: 0.6284017562866211
Batch 46/64 loss: 0.6529474258422852
Batch 47/64 loss: 0.8164892196655273
Batch 48/64 loss: 0.5244293212890625
Batch 49/64 loss: 0.7277188301086426
Batch 50/64 loss: 0.9024066925048828
Batch 51/64 loss: 0.7530374526977539
Batch 52/64 loss: 0.7721571922302246
Batch 53/64 loss: 0.5673274993896484
Batch 54/64 loss: 0.6649971008300781
Batch 55/64 loss: 0.5448832511901855
Batch 56/64 loss: 0.3429579734802246
Batch 57/64 loss: 0.7144980430603027
Batch 58/64 loss: 0.5527715682983398
Batch 59/64 loss: 0.7856287956237793
Batch 60/64 loss: 0.7316398620605469
Batch 61/64 loss: 0.26438379287719727
Batch 62/64 loss: 0.7450065612792969
Batch 63/64 loss: 0.4599289894104004
Batch 64/64 loss: -2.9272546768188477
Epoch 155  Train loss: 0.6591689053703756  Val loss: 0.6483909633151445
Saving best model, epoch: 155
Epoch 156
-------------------------------
Batch 1/64 loss: 0.7425980567932129
Batch 2/64 loss: 0.6735348701477051
Batch 3/64 loss: 0.42270994186401367
Batch 4/64 loss: 0.8766007423400879
Batch 5/64 loss: 0.5562472343444824
Batch 6/64 loss: 0.47212886810302734
Batch 7/64 loss: 0.602205753326416
Batch 8/64 loss: 0.3108649253845215
Batch 9/64 loss: 0.7673969268798828
Batch 10/64 loss: 0.9599208831787109
Batch 11/64 loss: 0.9670462608337402
Batch 12/64 loss: 0.8611621856689453
Batch 13/64 loss: 0.3816184997558594
Batch 14/64 loss: 0.5125670433044434
Batch 15/64 loss: 0.6309604644775391
Batch 16/64 loss: 0.7282800674438477
Batch 17/64 loss: 0.6087989807128906
Batch 18/64 loss: 0.26357269287109375
Batch 19/64 loss: 0.6978445053100586
Batch 20/64 loss: 0.6496033668518066
Batch 21/64 loss: 0.7665395736694336
Batch 22/64 loss: 0.6905660629272461
Batch 23/64 loss: 1.058976173400879
Batch 24/64 loss: 0.9128909111022949
Batch 25/64 loss: 0.488649845123291
Batch 26/64 loss: 0.6461796760559082
Batch 27/64 loss: 0.693328857421875
Batch 28/64 loss: 0.9341583251953125
Batch 29/64 loss: 0.41561365127563477
Batch 30/64 loss: 0.7631511688232422
Batch 31/64 loss: 1.0036540031433105
Batch 32/64 loss: 0.7031049728393555
Batch 33/64 loss: 0.4929237365722656
Batch 34/64 loss: 0.7430477142333984
Batch 35/64 loss: 0.46059513092041016
Batch 36/64 loss: 0.5906991958618164
Batch 37/64 loss: 0.8052983283996582
Batch 38/64 loss: 0.6539993286132812
Batch 39/64 loss: 0.7672209739685059
Batch 40/64 loss: 0.9596724510192871
Batch 41/64 loss: 1.040215015411377
Batch 42/64 loss: 0.6939482688903809
Batch 43/64 loss: 0.6226344108581543
Batch 44/64 loss: 0.5811543464660645
Batch 45/64 loss: 0.9144582748413086
Batch 46/64 loss: 0.6671075820922852
Batch 47/64 loss: 0.6127362251281738
Batch 48/64 loss: 0.8789029121398926
Batch 49/64 loss: 0.5759024620056152
Batch 50/64 loss: 0.7083401679992676
Batch 51/64 loss: 0.9418525695800781
Batch 52/64 loss: 0.9191036224365234
Batch 53/64 loss: 0.9300327301025391
Batch 54/64 loss: 0.3038063049316406
Batch 55/64 loss: 0.6500887870788574
Batch 56/64 loss: 0.7040953636169434
Batch 57/64 loss: 0.7119297981262207
Batch 58/64 loss: 1.0383610725402832
Batch 59/64 loss: 0.8247084617614746
Batch 60/64 loss: 0.8565616607666016
Batch 61/64 loss: 0.5950851440429688
Batch 62/64 loss: 1.0194644927978516
Batch 63/64 loss: 0.8173713684082031
Batch 64/64 loss: -2.8532629013061523
Epoch 156  Train loss: 0.6698642543717926  Val loss: 0.7726433481957085
Epoch 157
-------------------------------
Batch 1/64 loss: 0.4091463088989258
Batch 2/64 loss: 0.6607933044433594
Batch 3/64 loss: 0.509495735168457
Batch 4/64 loss: 0.9660754203796387
Batch 5/64 loss: 0.7733516693115234
Batch 6/64 loss: 0.44345951080322266
Batch 7/64 loss: 0.9232382774353027
Batch 8/64 loss: 0.6562490463256836
Batch 9/64 loss: 0.3781003952026367
Batch 10/64 loss: 0.6198902130126953
Batch 11/64 loss: 0.998262882232666
Batch 12/64 loss: 1.0028748512268066
Batch 13/64 loss: 0.7723031044006348
Batch 14/64 loss: 0.6579995155334473
Batch 15/64 loss: 0.5508227348327637
Batch 16/64 loss: 0.5832929611206055
Batch 17/64 loss: 0.8410434722900391
Batch 18/64 loss: 1.1182599067687988
Batch 19/64 loss: 0.5833644866943359
Batch 20/64 loss: 0.5159077644348145
Batch 21/64 loss: 0.9016604423522949
Batch 22/64 loss: 0.752845287322998
Batch 23/64 loss: 0.6493692398071289
Batch 24/64 loss: 0.483734130859375
Batch 25/64 loss: 0.6146979331970215
Batch 26/64 loss: 0.8228292465209961
Batch 27/64 loss: 0.871030330657959
Batch 28/64 loss: 0.4185523986816406
Batch 29/64 loss: 0.875220775604248
Batch 30/64 loss: 0.7635564804077148
Batch 31/64 loss: 0.8254618644714355
Batch 32/64 loss: 0.4752049446105957
Batch 33/64 loss: 0.6398563385009766
Batch 34/64 loss: 0.4325575828552246
Batch 35/64 loss: 1.043553352355957
Batch 36/64 loss: 0.5144157409667969
Batch 37/64 loss: 0.629417896270752
Batch 38/64 loss: 0.4478163719177246
Batch 39/64 loss: 0.48717308044433594
Batch 40/64 loss: 0.6692695617675781
Batch 41/64 loss: 0.8377351760864258
Batch 42/64 loss: 0.5296087265014648
Batch 43/64 loss: 0.7800297737121582
Batch 44/64 loss: 0.6282467842102051
Batch 45/64 loss: 0.7668623924255371
Batch 46/64 loss: 0.2549262046813965
Batch 47/64 loss: 1.0813746452331543
Batch 48/64 loss: 0.5331010818481445
Batch 49/64 loss: 0.7291831970214844
Batch 50/64 loss: 0.4170866012573242
Batch 51/64 loss: 0.7143135070800781
Batch 52/64 loss: 0.7466979026794434
Batch 53/64 loss: 0.907130241394043
Batch 54/64 loss: 0.848912239074707
Batch 55/64 loss: 0.5799956321716309
Batch 56/64 loss: 0.5615024566650391
Batch 57/64 loss: 0.9380702972412109
Batch 58/64 loss: 0.5034079551696777
Batch 59/64 loss: 0.9121999740600586
Batch 60/64 loss: 0.8989048004150391
Batch 61/64 loss: 0.776674747467041
Batch 62/64 loss: 0.5784358978271484
Batch 63/64 loss: 0.5717291831970215
Batch 64/64 loss: -3.148343563079834
Epoch 157  Train loss: 0.6434043341991948  Val loss: 0.6948848671929533
Epoch 158
-------------------------------
Batch 1/64 loss: 0.6124424934387207
Batch 2/64 loss: 0.3527565002441406
Batch 3/64 loss: 0.7002654075622559
Batch 4/64 loss: 0.6138148307800293
Batch 5/64 loss: 0.6822080612182617
Batch 6/64 loss: 0.6501522064208984
Batch 7/64 loss: 0.4512004852294922
Batch 8/64 loss: 0.6629505157470703
Batch 9/64 loss: 1.4589457511901855
Batch 10/64 loss: 0.3780388832092285
Batch 11/64 loss: 0.6884055137634277
Batch 12/64 loss: 0.8958516120910645
Batch 13/64 loss: 0.6370744705200195
Batch 14/64 loss: 0.35980987548828125
Batch 15/64 loss: 0.41991281509399414
Batch 16/64 loss: 0.5463862419128418
Batch 17/64 loss: 0.36728525161743164
Batch 18/64 loss: 0.6203188896179199
Batch 19/64 loss: 0.6646976470947266
Batch 20/64 loss: 0.5658807754516602
Batch 21/64 loss: 0.9344935417175293
Batch 22/64 loss: 0.9911947250366211
Batch 23/64 loss: 0.5565886497497559
Batch 24/64 loss: 0.4869256019592285
Batch 25/64 loss: 1.0232148170471191
Batch 26/64 loss: 0.6505866050720215
Batch 27/64 loss: 0.5961246490478516
Batch 28/64 loss: 0.6940135955810547
Batch 29/64 loss: 0.668642520904541
Batch 30/64 loss: 0.6332921981811523
Batch 31/64 loss: 0.5533013343811035
Batch 32/64 loss: 0.880669116973877
Batch 33/64 loss: 0.604642391204834
Batch 34/64 loss: 0.5068459510803223
Batch 35/64 loss: 0.629033088684082
Batch 36/64 loss: 0.5422806739807129
Batch 37/64 loss: 1.0076775550842285
Batch 38/64 loss: 0.7331500053405762
Batch 39/64 loss: 1.1956758499145508
Batch 40/64 loss: 0.5307798385620117
Batch 41/64 loss: 0.6615490913391113
Batch 42/64 loss: 0.7645325660705566
Batch 43/64 loss: 0.6570882797241211
Batch 44/64 loss: 0.29967737197875977
Batch 45/64 loss: 0.9257383346557617
Batch 46/64 loss: 0.8749499320983887
Batch 47/64 loss: 0.7612066268920898
Batch 48/64 loss: 0.37743282318115234
Batch 49/64 loss: 0.5670261383056641
Batch 50/64 loss: 0.9589648246765137
Batch 51/64 loss: 0.7928237915039062
Batch 52/64 loss: 0.7448329925537109
Batch 53/64 loss: 0.3753786087036133
Batch 54/64 loss: 0.7816615104675293
Batch 55/64 loss: 0.4761991500854492
Batch 56/64 loss: 0.5111722946166992
Batch 57/64 loss: 0.25182437896728516
Batch 58/64 loss: 0.44625282287597656
Batch 59/64 loss: 0.5422968864440918
Batch 60/64 loss: 0.5588130950927734
Batch 61/64 loss: 0.5276994705200195
Batch 62/64 loss: 0.8220386505126953
Batch 63/64 loss: 0.6708488464355469
Batch 64/64 loss: -2.8931474685668945
Epoch 158  Train loss: 0.6106302560544481  Val loss: 1.19072202964337
Epoch 159
-------------------------------
Batch 1/64 loss: 0.6308116912841797
Batch 2/64 loss: 0.9096746444702148
Batch 3/64 loss: 1.10565185546875
Batch 4/64 loss: 0.45968151092529297
Batch 5/64 loss: 0.517554759979248
Batch 6/64 loss: 0.6931743621826172
Batch 7/64 loss: 0.5610785484313965
Batch 8/64 loss: 0.7938380241394043
Batch 9/64 loss: 0.535527229309082
Batch 10/64 loss: 0.3953437805175781
Batch 11/64 loss: 0.822845458984375
Batch 12/64 loss: 0.6792097091674805
Batch 13/64 loss: 0.6380572319030762
Batch 14/64 loss: 0.40491771697998047
Batch 15/64 loss: 0.9835643768310547
Batch 16/64 loss: 0.9174189567565918
Batch 17/64 loss: 0.6701531410217285
Batch 18/64 loss: 0.5829343795776367
Batch 19/64 loss: 0.6127791404724121
Batch 20/64 loss: 0.5254330635070801
Batch 21/64 loss: 0.6405549049377441
Batch 22/64 loss: 0.6965255737304688
Batch 23/64 loss: 0.5912289619445801
Batch 24/64 loss: 0.5322246551513672
Batch 25/64 loss: 0.5943679809570312
Batch 26/64 loss: 0.46775293350219727
Batch 27/64 loss: 0.40711402893066406
Batch 28/64 loss: 0.46401214599609375
Batch 29/64 loss: 0.6204037666320801
Batch 30/64 loss: 0.6493430137634277
Batch 31/64 loss: 0.6774353981018066
Batch 32/64 loss: 0.5558743476867676
Batch 33/64 loss: 0.6987905502319336
Batch 34/64 loss: 0.4632105827331543
Batch 35/64 loss: 0.47075986862182617
Batch 36/64 loss: 0.27024030685424805
Batch 37/64 loss: 0.7915496826171875
Batch 38/64 loss: 0.3815155029296875
Batch 39/64 loss: 0.8206028938293457
Batch 40/64 loss: 0.35798025131225586
Batch 41/64 loss: 1.3029823303222656
Batch 42/64 loss: 0.5037598609924316
Batch 43/64 loss: 0.9271135330200195
Batch 44/64 loss: 0.6753711700439453
Batch 45/64 loss: 0.8163003921508789
Batch 46/64 loss: 0.658660888671875
Batch 47/64 loss: 0.7462325096130371
Batch 48/64 loss: 1.0415964126586914
Batch 49/64 loss: 0.3793034553527832
Batch 50/64 loss: 0.4959096908569336
Batch 51/64 loss: 0.7108573913574219
Batch 52/64 loss: 0.7639632225036621
Batch 53/64 loss: 0.3683938980102539
Batch 54/64 loss: 0.7887883186340332
Batch 55/64 loss: 1.0067009925842285
Batch 56/64 loss: 0.2987198829650879
Batch 57/64 loss: 0.9128947257995605
Batch 58/64 loss: 0.49175500869750977
Batch 59/64 loss: 0.679558277130127
Batch 60/64 loss: 0.49560070037841797
Batch 61/64 loss: 0.27416133880615234
Batch 62/64 loss: 0.7820677757263184
Batch 63/64 loss: 0.6728758811950684
Batch 64/64 loss: -2.7549242973327637
Epoch 159  Train loss: 0.6010746881073596  Val loss: 0.6268195057243007
Saving best model, epoch: 159
Epoch 160
-------------------------------
Batch 1/64 loss: 0.7444624900817871
Batch 2/64 loss: 0.6671223640441895
Batch 3/64 loss: 0.6747446060180664
Batch 4/64 loss: 0.8791828155517578
Batch 5/64 loss: 0.8185114860534668
Batch 6/64 loss: 0.6952390670776367
Batch 7/64 loss: 0.523798942565918
Batch 8/64 loss: 0.6905388832092285
Batch 9/64 loss: 0.7038450241088867
Batch 10/64 loss: 0.8589687347412109
Batch 11/64 loss: 0.41931819915771484
Batch 12/64 loss: 0.5758867263793945
Batch 13/64 loss: 0.5980110168457031
Batch 14/64 loss: 0.6538071632385254
Batch 15/64 loss: 0.3993983268737793
Batch 16/64 loss: 0.5195050239562988
Batch 17/64 loss: 0.5260219573974609
Batch 18/64 loss: 0.5912466049194336
Batch 19/64 loss: 0.36017704010009766
Batch 20/64 loss: 0.5319290161132812
Batch 21/64 loss: 0.46404361724853516
Batch 22/64 loss: 0.5004634857177734
Batch 23/64 loss: 0.5187187194824219
Batch 24/64 loss: 0.3701663017272949
Batch 25/64 loss: 0.771430492401123
Batch 26/64 loss: 0.4886283874511719
Batch 27/64 loss: 0.5962214469909668
Batch 28/64 loss: 0.8560566902160645
Batch 29/64 loss: 0.5859808921813965
Batch 30/64 loss: 0.5983681678771973
Batch 31/64 loss: 0.7279939651489258
Batch 32/64 loss: 0.5229940414428711
Batch 33/64 loss: 0.8091616630554199
Batch 34/64 loss: 0.6409111022949219
Batch 35/64 loss: 0.6041364669799805
Batch 36/64 loss: 0.8116564750671387
Batch 37/64 loss: 0.5271725654602051
Batch 38/64 loss: 0.4550895690917969
Batch 39/64 loss: 0.6843681335449219
Batch 40/64 loss: 0.4099903106689453
Batch 41/64 loss: 0.6107149124145508
Batch 42/64 loss: 0.3862190246582031
Batch 43/64 loss: 0.5297012329101562
Batch 44/64 loss: 0.9804315567016602
Batch 45/64 loss: 0.5352554321289062
Batch 46/64 loss: 0.7203998565673828
Batch 47/64 loss: 0.6463508605957031
Batch 48/64 loss: 0.5562710762023926
Batch 49/64 loss: 0.4972825050354004
Batch 50/64 loss: 0.5396175384521484
Batch 51/64 loss: 0.4743824005126953
Batch 52/64 loss: 0.5251846313476562
Batch 53/64 loss: 0.6889448165893555
Batch 54/64 loss: 0.9333572387695312
Batch 55/64 loss: 0.7764849662780762
Batch 56/64 loss: 0.949953556060791
Batch 57/64 loss: 0.5830307006835938
Batch 58/64 loss: 0.9141159057617188
Batch 59/64 loss: 0.42029333114624023
Batch 60/64 loss: 0.8697075843811035
Batch 61/64 loss: 0.3209114074707031
Batch 62/64 loss: 0.3839545249938965
Batch 63/64 loss: 0.7446126937866211
Batch 64/64 loss: -3.021467685699463
Epoch 160  Train loss: 0.5756289407318713  Val loss: 0.5861266814556318
Saving best model, epoch: 160
Epoch 161
-------------------------------
Batch 1/64 loss: 0.31989097595214844
Batch 2/64 loss: 0.4877505302429199
Batch 3/64 loss: 0.32746410369873047
Batch 4/64 loss: 0.773503303527832
Batch 5/64 loss: 0.48195314407348633
Batch 6/64 loss: 0.8679051399230957
Batch 7/64 loss: 0.6435484886169434
Batch 8/64 loss: 0.7187271118164062
Batch 9/64 loss: 0.35608816146850586
Batch 10/64 loss: 0.2942662239074707
Batch 11/64 loss: 0.9052867889404297
Batch 12/64 loss: 0.9372596740722656
Batch 13/64 loss: 0.4035072326660156
Batch 14/64 loss: 0.26088380813598633
Batch 15/64 loss: 0.4311962127685547
Batch 16/64 loss: 0.668238639831543
Batch 17/64 loss: 0.2993607521057129
Batch 18/64 loss: 0.44649362564086914
Batch 19/64 loss: 0.7581310272216797
Batch 20/64 loss: 0.6261463165283203
Batch 21/64 loss: 0.6812138557434082
Batch 22/64 loss: 0.45869922637939453
Batch 23/64 loss: 0.3952927589416504
Batch 24/64 loss: 0.4575319290161133
Batch 25/64 loss: 0.4618673324584961
Batch 26/64 loss: 0.5463337898254395
Batch 27/64 loss: 0.5184025764465332
Batch 28/64 loss: 0.4817619323730469
Batch 29/64 loss: 0.813514232635498
Batch 30/64 loss: 0.22169065475463867
Batch 31/64 loss: 0.40313196182250977
Batch 32/64 loss: 0.687279224395752
Batch 33/64 loss: 0.3255128860473633
Batch 34/64 loss: 0.31120872497558594
Batch 35/64 loss: 0.37261486053466797
Batch 36/64 loss: 0.6410326957702637
Batch 37/64 loss: 0.8181314468383789
Batch 38/64 loss: 0.552180290222168
Batch 39/64 loss: 0.8066220283508301
Batch 40/64 loss: 0.8383784294128418
Batch 41/64 loss: 0.5271611213684082
Batch 42/64 loss: 0.7469234466552734
Batch 43/64 loss: 0.7727689743041992
Batch 44/64 loss: 0.8258028030395508
Batch 45/64 loss: 0.7939729690551758
Batch 46/64 loss: 0.6256375312805176
Batch 47/64 loss: 0.929020881652832
Batch 48/64 loss: 0.5279369354248047
Batch 49/64 loss: 0.8140621185302734
Batch 50/64 loss: 0.33001041412353516
Batch 51/64 loss: 0.6414752006530762
Batch 52/64 loss: 0.6583404541015625
Batch 53/64 loss: 0.7317099571228027
Batch 54/64 loss: 0.8707966804504395
Batch 55/64 loss: 0.891444206237793
Batch 56/64 loss: 0.4187893867492676
Batch 57/64 loss: 0.5716423988342285
Batch 58/64 loss: 0.6588654518127441
Batch 59/64 loss: 0.6020097732543945
Batch 60/64 loss: 0.6197090148925781
Batch 61/64 loss: 0.6875119209289551
Batch 62/64 loss: 0.7350983619689941
Batch 63/64 loss: 1.0528054237365723
Batch 64/64 loss: -3.1241540908813477
Epoch 161  Train loss: 0.5567118738211837  Val loss: 0.6532840532125886
Epoch 162
-------------------------------
Batch 1/64 loss: 0.6232595443725586
Batch 2/64 loss: 0.683354377746582
Batch 3/64 loss: 0.7080168724060059
Batch 4/64 loss: 0.735224723815918
Batch 5/64 loss: 0.7519502639770508
Batch 6/64 loss: 0.9613933563232422
Batch 7/64 loss: 0.7553715705871582
Batch 8/64 loss: 1.0502562522888184
Batch 9/64 loss: 0.7513370513916016
Batch 10/64 loss: 0.4187450408935547
Batch 11/64 loss: 1.0846223831176758
Batch 12/64 loss: 0.8104896545410156
Batch 13/64 loss: 0.8163881301879883
Batch 14/64 loss: 0.5086851119995117
Batch 15/64 loss: 0.7340550422668457
Batch 16/64 loss: 0.771355152130127
Batch 17/64 loss: 0.882936954498291
Batch 18/64 loss: 1.0060253143310547
Batch 19/64 loss: 0.49175167083740234
Batch 20/64 loss: 0.804558277130127
Batch 21/64 loss: 0.7508559226989746
Batch 22/64 loss: 0.8560223579406738
Batch 23/64 loss: 1.057939052581787
Batch 24/64 loss: 1.6419644355773926
Batch 25/64 loss: 0.7934370040893555
Batch 26/64 loss: 1.102907657623291
Batch 27/64 loss: 0.8672480583190918
Batch 28/64 loss: 1.0109281539916992
Batch 29/64 loss: 0.8902897834777832
Batch 30/64 loss: 1.2743744850158691
Batch 31/64 loss: 0.7807393074035645
Batch 32/64 loss: 0.8541607856750488
Batch 33/64 loss: 1.003612995147705
Batch 34/64 loss: 0.6497178077697754
Batch 35/64 loss: 0.8977928161621094
Batch 36/64 loss: 1.3126602172851562
Batch 37/64 loss: 0.7171835899353027
Batch 38/64 loss: 1.0412020683288574
Batch 39/64 loss: 1.0821785926818848
Batch 40/64 loss: 0.7804746627807617
Batch 41/64 loss: 0.9254446029663086
Batch 42/64 loss: 0.834770679473877
Batch 43/64 loss: 0.7517704963684082
Batch 44/64 loss: 0.8205776214599609
Batch 45/64 loss: 0.6877627372741699
Batch 46/64 loss: 0.774655818939209
Batch 47/64 loss: 1.1132144927978516
Batch 48/64 loss: 0.5535874366760254
Batch 49/64 loss: 1.0720863342285156
Batch 50/64 loss: 1.0320725440979004
Batch 51/64 loss: 0.7718071937561035
Batch 52/64 loss: 0.7499346733093262
Batch 53/64 loss: 1.0134429931640625
Batch 54/64 loss: 0.6705784797668457
Batch 55/64 loss: 0.5098562240600586
Batch 56/64 loss: 0.3700895309448242
Batch 57/64 loss: 0.7980532646179199
Batch 58/64 loss: 0.6003413200378418
Batch 59/64 loss: 0.924769401550293
Batch 60/64 loss: 0.6187505722045898
Batch 61/64 loss: 0.5903997421264648
Batch 62/64 loss: 1.2068352699279785
Batch 63/64 loss: 0.7700872421264648
Batch 64/64 loss: -2.771663188934326
Epoch 162  Train loss: 0.7968252200706333  Val loss: 0.7904022977114543
Epoch 163
-------------------------------
Batch 1/64 loss: 1.117321491241455
Batch 2/64 loss: 0.6045632362365723
Batch 3/64 loss: 0.9466743469238281
Batch 4/64 loss: 0.7776103019714355
Batch 5/64 loss: 0.6333084106445312
Batch 6/64 loss: 0.5876936912536621
Batch 7/64 loss: 0.7072262763977051
Batch 8/64 loss: 1.1543173789978027
Batch 9/64 loss: 0.4897899627685547
Batch 10/64 loss: 0.36385297775268555
Batch 11/64 loss: 1.3524980545043945
Batch 12/64 loss: 0.570188045501709
Batch 13/64 loss: 0.6886768341064453
Batch 14/64 loss: 0.4758749008178711
Batch 15/64 loss: 0.7108678817749023
Batch 16/64 loss: 0.5277037620544434
Batch 17/64 loss: 0.7651252746582031
Batch 18/64 loss: 0.32085180282592773
Batch 19/64 loss: 0.5931859016418457
Batch 20/64 loss: 0.5902900695800781
Batch 21/64 loss: 0.5365338325500488
Batch 22/64 loss: 0.619286060333252
Batch 23/64 loss: 1.0442209243774414
Batch 24/64 loss: 0.6225881576538086
Batch 25/64 loss: 0.6255297660827637
Batch 26/64 loss: 0.9986147880554199
Batch 27/64 loss: 0.6011414527893066
Batch 28/64 loss: 0.6205706596374512
Batch 29/64 loss: 0.5731925964355469
Batch 30/64 loss: 0.7401618957519531
Batch 31/64 loss: 0.674161434173584
Batch 32/64 loss: 0.6083002090454102
Batch 33/64 loss: 0.5956764221191406
Batch 34/64 loss: 1.3208932876586914
Batch 35/64 loss: 0.8167352676391602
Batch 36/64 loss: 0.4622359275817871
Batch 37/64 loss: 0.8626537322998047
Batch 38/64 loss: 0.6929898262023926
Batch 39/64 loss: 0.5128116607666016
Batch 40/64 loss: 0.3908209800720215
Batch 41/64 loss: 0.39621543884277344
Batch 42/64 loss: 0.5244894027709961
Batch 43/64 loss: 1.0081281661987305
Batch 44/64 loss: 0.5105195045471191
Batch 45/64 loss: 0.3946061134338379
Batch 46/64 loss: 0.5482463836669922
Batch 47/64 loss: 0.6004805564880371
Batch 48/64 loss: 0.6196508407592773
Batch 49/64 loss: 1.0222711563110352
Batch 50/64 loss: 0.9750394821166992
Batch 51/64 loss: 0.3517775535583496
Batch 52/64 loss: 0.6170902252197266
Batch 53/64 loss: 0.7798476219177246
Batch 54/64 loss: 0.6580252647399902
Batch 55/64 loss: 0.6841602325439453
Batch 56/64 loss: 0.5225114822387695
Batch 57/64 loss: 0.7021656036376953
Batch 58/64 loss: 0.6145792007446289
Batch 59/64 loss: 0.8811378479003906
Batch 60/64 loss: 0.7748656272888184
Batch 61/64 loss: 1.0321745872497559
Batch 62/64 loss: 0.44041967391967773
Batch 63/64 loss: 0.4477052688598633
Batch 64/64 loss: -2.414839267730713
Epoch 163  Train loss: 0.6461445845809637  Val loss: 0.6157262480955353
Epoch 164
-------------------------------
Batch 1/64 loss: 0.7950029373168945
Batch 2/64 loss: 0.799497127532959
Batch 3/64 loss: 0.8569860458374023
Batch 4/64 loss: 0.7473020553588867
Batch 5/64 loss: 1.1297316551208496
Batch 6/64 loss: 0.6578207015991211
Batch 7/64 loss: 0.3632030487060547
Batch 8/64 loss: 0.3943490982055664
Batch 9/64 loss: 0.3179154396057129
Batch 10/64 loss: 0.8698954582214355
Batch 11/64 loss: 0.7107820510864258
Batch 12/64 loss: 0.4550042152404785
Batch 13/64 loss: 0.3744206428527832
Batch 14/64 loss: 0.3599357604980469
Batch 15/64 loss: 0.7721376419067383
Batch 16/64 loss: 0.6573553085327148
Batch 17/64 loss: 0.7781777381896973
Batch 18/64 loss: 0.747802734375
Batch 19/64 loss: 0.9088764190673828
Batch 20/64 loss: 0.5187554359436035
Batch 21/64 loss: 0.4272150993347168
Batch 22/64 loss: 0.6184921264648438
Batch 23/64 loss: 0.8933591842651367
Batch 24/64 loss: 0.4455881118774414
Batch 25/64 loss: 0.7618541717529297
Batch 26/64 loss: 0.3554706573486328
Batch 27/64 loss: 0.47386646270751953
Batch 28/64 loss: 1.2727012634277344
Batch 29/64 loss: 0.7396464347839355
Batch 30/64 loss: 0.48314666748046875
Batch 31/64 loss: 0.4446883201599121
Batch 32/64 loss: 0.8715577125549316
Batch 33/64 loss: 0.6865434646606445
Batch 34/64 loss: 0.5806336402893066
Batch 35/64 loss: 0.42223119735717773
Batch 36/64 loss: 0.7503104209899902
Batch 37/64 loss: 0.6245102882385254
Batch 38/64 loss: 1.1644539833068848
Batch 39/64 loss: 0.38199567794799805
Batch 40/64 loss: 0.5428361892700195
Batch 41/64 loss: 0.9280767440795898
Batch 42/64 loss: 0.7680082321166992
Batch 43/64 loss: 0.496492862701416
Batch 44/64 loss: 0.6217250823974609
Batch 45/64 loss: 0.4747757911682129
Batch 46/64 loss: 0.6159815788269043
Batch 47/64 loss: 0.5571722984313965
Batch 48/64 loss: 0.5687451362609863
Batch 49/64 loss: 0.4700789451599121
Batch 50/64 loss: 0.4230508804321289
Batch 51/64 loss: 0.5332303047180176
Batch 52/64 loss: 0.9273738861083984
Batch 53/64 loss: 0.46266651153564453
Batch 54/64 loss: 1.0990018844604492
Batch 55/64 loss: 0.7438116073608398
Batch 56/64 loss: 0.46654510498046875
Batch 57/64 loss: 0.6165013313293457
Batch 58/64 loss: 0.6876101493835449
Batch 59/64 loss: 0.843015193939209
Batch 60/64 loss: 0.46893978118896484
Batch 61/64 loss: 0.8563094139099121
Batch 62/64 loss: 0.5917258262634277
Batch 63/64 loss: 0.9162969589233398
Batch 64/64 loss: -2.6010308265686035
Epoch 164  Train loss: 0.6171359211790796  Val loss: 0.6937724175731751
Epoch 165
-------------------------------
Batch 1/64 loss: 0.4392209053039551
Batch 2/64 loss: 0.6490154266357422
Batch 3/64 loss: 0.5856022834777832
Batch 4/64 loss: 0.6421833038330078
Batch 5/64 loss: 0.5774412155151367
Batch 6/64 loss: 0.5946831703186035
Batch 7/64 loss: 0.6407074928283691
Batch 8/64 loss: 0.4587974548339844
Batch 9/64 loss: 0.6381072998046875
Batch 10/64 loss: 0.4742259979248047
Batch 11/64 loss: 0.2479085922241211
Batch 12/64 loss: 0.5250334739685059
Batch 13/64 loss: 0.393404483795166
Batch 14/64 loss: 0.5006847381591797
Batch 15/64 loss: 0.694188117980957
Batch 16/64 loss: 0.9759402275085449
Batch 17/64 loss: 0.6191139221191406
Batch 18/64 loss: 0.42467594146728516
Batch 19/64 loss: 1.0135622024536133
Batch 20/64 loss: 0.38207530975341797
Batch 21/64 loss: 0.49732017517089844
Batch 22/64 loss: 0.09807777404785156
Batch 23/64 loss: 0.3959817886352539
Batch 24/64 loss: 0.6316847801208496
Batch 25/64 loss: 1.453516960144043
Batch 26/64 loss: 0.5749387741088867
Batch 27/64 loss: 0.4340353012084961
Batch 28/64 loss: 0.38705921173095703
Batch 29/64 loss: 1.0599918365478516
Batch 30/64 loss: 0.49011850357055664
Batch 31/64 loss: 0.757817268371582
Batch 32/64 loss: 0.5888938903808594
Batch 33/64 loss: 0.5273199081420898
Batch 34/64 loss: 0.27796125411987305
Batch 35/64 loss: 0.7509856224060059
Batch 36/64 loss: 0.9661884307861328
Batch 37/64 loss: 0.6338081359863281
Batch 38/64 loss: 0.670020580291748
Batch 39/64 loss: 0.5290107727050781
Batch 40/64 loss: 0.6837172508239746
Batch 41/64 loss: 0.607020378112793
Batch 42/64 loss: 0.48304224014282227
Batch 43/64 loss: 0.46709537506103516
Batch 44/64 loss: 0.6019926071166992
Batch 45/64 loss: 0.593264102935791
Batch 46/64 loss: 0.8880882263183594
Batch 47/64 loss: 0.2480788230895996
Batch 48/64 loss: 0.48702096939086914
Batch 49/64 loss: 0.7022953033447266
Batch 50/64 loss: 0.2755160331726074
Batch 51/64 loss: 0.5380043983459473
Batch 52/64 loss: 0.6247735023498535
Batch 53/64 loss: 1.09889554977417
Batch 54/64 loss: 0.35160064697265625
Batch 55/64 loss: 0.4879627227783203
Batch 56/64 loss: 0.7152996063232422
Batch 57/64 loss: 0.7997970581054688
Batch 58/64 loss: 0.5705533027648926
Batch 59/64 loss: 0.5943350791931152
Batch 60/64 loss: 0.8221025466918945
Batch 61/64 loss: 0.42943429946899414
Batch 62/64 loss: 0.8050851821899414
Batch 63/64 loss: 0.8513946533203125
Batch 64/64 loss: -2.3758859634399414
Epoch 165  Train loss: 0.566992280997482  Val loss: 0.5993871197258074
Epoch 166
-------------------------------
Batch 1/64 loss: 0.7017602920532227
Batch 2/64 loss: 0.492398738861084
Batch 3/64 loss: 0.4280214309692383
Batch 4/64 loss: 0.8436660766601562
Batch 5/64 loss: 0.4693880081176758
Batch 6/64 loss: 0.24208307266235352
Batch 7/64 loss: 0.42475128173828125
Batch 8/64 loss: 0.6844468116760254
Batch 9/64 loss: 0.6427483558654785
Batch 10/64 loss: 0.4725618362426758
Batch 11/64 loss: 0.6393909454345703
Batch 12/64 loss: 0.5422224998474121
Batch 13/64 loss: 0.45760154724121094
Batch 14/64 loss: 0.8239655494689941
Batch 15/64 loss: 0.7201528549194336
Batch 16/64 loss: 0.8498053550720215
Batch 17/64 loss: 0.1962418556213379
Batch 18/64 loss: 0.4357595443725586
Batch 19/64 loss: 0.43052053451538086
Batch 20/64 loss: 0.623283863067627
Batch 21/64 loss: 0.656527042388916
Batch 22/64 loss: 0.8297367095947266
Batch 23/64 loss: 0.36069822311401367
Batch 24/64 loss: 0.8358950614929199
Batch 25/64 loss: 0.33539247512817383
Batch 26/64 loss: 0.4526638984680176
Batch 27/64 loss: 0.6356477737426758
Batch 28/64 loss: 0.3451542854309082
Batch 29/64 loss: 0.5803074836730957
Batch 30/64 loss: 0.6128449440002441
Batch 31/64 loss: 0.5127506256103516
Batch 32/64 loss: 0.5285048484802246
Batch 33/64 loss: 0.720919132232666
Batch 34/64 loss: 0.35564661026000977
Batch 35/64 loss: 0.5641779899597168
Batch 36/64 loss: 0.3884143829345703
Batch 37/64 loss: 0.6660418510437012
Batch 38/64 loss: 0.4141964912414551
Batch 39/64 loss: 0.8085699081420898
Batch 40/64 loss: 0.6841883659362793
Batch 41/64 loss: 0.9945173263549805
Batch 42/64 loss: 0.4420280456542969
Batch 43/64 loss: 0.5223240852355957
Batch 44/64 loss: 0.7316646575927734
Batch 45/64 loss: 0.40730857849121094
Batch 46/64 loss: 0.4363870620727539
Batch 47/64 loss: 0.4069857597351074
Batch 48/64 loss: 0.8328151702880859
Batch 49/64 loss: 0.9219985008239746
Batch 50/64 loss: 0.9367408752441406
Batch 51/64 loss: 0.8165526390075684
Batch 52/64 loss: 0.7898917198181152
Batch 53/64 loss: 0.5503888130187988
Batch 54/64 loss: 0.7242422103881836
Batch 55/64 loss: 0.5519781112670898
Batch 56/64 loss: 0.30419349670410156
Batch 57/64 loss: 0.6375570297241211
Batch 58/64 loss: 0.8974566459655762
Batch 59/64 loss: 0.5445098876953125
Batch 60/64 loss: 0.6432409286499023
Batch 61/64 loss: 0.9123573303222656
Batch 62/64 loss: 0.5135679244995117
Batch 63/64 loss: 0.7862334251403809
Batch 64/64 loss: -2.750535011291504
Epoch 166  Train loss: 0.5592327454510857  Val loss: 2.7132161589422585
Epoch 167
-------------------------------
Batch 1/64 loss: 1.713200569152832
Batch 2/64 loss: 0.8150315284729004
Batch 3/64 loss: 0.9081850051879883
Batch 4/64 loss: 0.9067020416259766
Batch 5/64 loss: 0.8543305397033691
Batch 6/64 loss: 0.8634181022644043
Batch 7/64 loss: 1.3571696281433105
Batch 8/64 loss: 1.2392706871032715
Batch 9/64 loss: 1.2410669326782227
Batch 10/64 loss: 1.61098051071167
Batch 11/64 loss: 1.123605728149414
Batch 12/64 loss: 1.3113760948181152
Batch 13/64 loss: 1.1309657096862793
Batch 14/64 loss: 1.2582387924194336
Batch 15/64 loss: 1.3522529602050781
Batch 16/64 loss: 0.9755725860595703
Batch 17/64 loss: 0.9405980110168457
Batch 18/64 loss: 0.6489901542663574
Batch 19/64 loss: 0.9251117706298828
Batch 20/64 loss: 0.9477858543395996
Batch 21/64 loss: 0.9955263137817383
Batch 22/64 loss: 1.0691852569580078
Batch 23/64 loss: 1.128901481628418
Batch 24/64 loss: 1.4156255722045898
Batch 25/64 loss: 1.2921271324157715
Batch 26/64 loss: 0.9145998954772949
Batch 27/64 loss: 1.0156373977661133
Batch 28/64 loss: 0.5342421531677246
Batch 29/64 loss: 0.7440924644470215
Batch 30/64 loss: 0.8083114624023438
Batch 31/64 loss: 1.0931406021118164
Batch 32/64 loss: 0.9914026260375977
Batch 33/64 loss: 1.0337953567504883
Batch 34/64 loss: 0.5584845542907715
Batch 35/64 loss: 0.6169137954711914
Batch 36/64 loss: 1.3893442153930664
Batch 37/64 loss: 0.9307212829589844
Batch 38/64 loss: 1.0528202056884766
Batch 39/64 loss: 0.7615532875061035
Batch 40/64 loss: 0.9194455146789551
Batch 41/64 loss: 0.896580696105957
Batch 42/64 loss: 0.49312639236450195
Batch 43/64 loss: 0.5450153350830078
Batch 44/64 loss: 0.7813143730163574
Batch 45/64 loss: 0.9141154289245605
Batch 46/64 loss: 0.9236092567443848
Batch 47/64 loss: 1.2280921936035156
Batch 48/64 loss: 0.7643070220947266
Batch 49/64 loss: 1.117912769317627
Batch 50/64 loss: 0.8715271949768066
Batch 51/64 loss: 0.8888497352600098
Batch 52/64 loss: 1.1711368560791016
Batch 53/64 loss: 1.0338430404663086
Batch 54/64 loss: 1.018000602722168
Batch 55/64 loss: 0.4621090888977051
Batch 56/64 loss: 0.7639732360839844
Batch 57/64 loss: 0.9551219940185547
Batch 58/64 loss: 0.9859223365783691
Batch 59/64 loss: 0.8656330108642578
Batch 60/64 loss: 0.6712474822998047
Batch 61/64 loss: 0.6607761383056641
Batch 62/64 loss: 0.8225994110107422
Batch 63/64 loss: 0.6876349449157715
Batch 64/64 loss: -2.845561981201172
Epoch 167  Train loss: 0.9220078561820236  Val loss: 0.8747817495024901
Epoch 168
-------------------------------
Batch 1/64 loss: 0.8572998046875
Batch 2/64 loss: 0.9273648262023926
Batch 3/64 loss: 1.2689456939697266
Batch 4/64 loss: 0.4436454772949219
Batch 5/64 loss: 0.6299247741699219
Batch 6/64 loss: 0.5783534049987793
Batch 7/64 loss: 0.7576541900634766
Batch 8/64 loss: 0.7142319679260254
Batch 9/64 loss: 1.2812085151672363
Batch 10/64 loss: 0.9036736488342285
Batch 11/64 loss: 0.4790077209472656
Batch 12/64 loss: 0.8680210113525391
Batch 13/64 loss: 0.849024772644043
Batch 14/64 loss: 0.46938657760620117
Batch 15/64 loss: 0.9453554153442383
Batch 16/64 loss: 0.35930490493774414
Batch 17/64 loss: 0.7902035713195801
Batch 18/64 loss: 0.7408914566040039
Batch 19/64 loss: 1.0982117652893066
Batch 20/64 loss: 0.7720904350280762
Batch 21/64 loss: 0.8163566589355469
Batch 22/64 loss: 0.7195801734924316
Batch 23/64 loss: 1.0961828231811523
Batch 24/64 loss: 0.9098758697509766
Batch 25/64 loss: 0.8972897529602051
Batch 26/64 loss: 0.41858673095703125
Batch 27/64 loss: 0.5405921936035156
Batch 28/64 loss: 1.1511735916137695
Batch 29/64 loss: 0.6237406730651855
Batch 30/64 loss: 0.6242275238037109
Batch 31/64 loss: 0.6039214134216309
Batch 32/64 loss: 0.6798057556152344
Batch 33/64 loss: 0.9413595199584961
Batch 34/64 loss: 0.6855707168579102
Batch 35/64 loss: 0.5268621444702148
Batch 36/64 loss: 0.4981503486633301
Batch 37/64 loss: 0.8143930435180664
Batch 38/64 loss: 0.5038967132568359
Batch 39/64 loss: 0.5861086845397949
Batch 40/64 loss: 0.6897425651550293
Batch 41/64 loss: 0.371584415435791
Batch 42/64 loss: 0.5054478645324707
Batch 43/64 loss: 0.6255149841308594
Batch 44/64 loss: 0.42543554306030273
Batch 45/64 loss: 0.49387550354003906
Batch 46/64 loss: 0.4824333190917969
Batch 47/64 loss: 0.7148256301879883
Batch 48/64 loss: 0.555910587310791
Batch 49/64 loss: 0.6876363754272461
Batch 50/64 loss: 0.6659798622131348
Batch 51/64 loss: 0.29299449920654297
Batch 52/64 loss: 0.7424521446228027
Batch 53/64 loss: 0.9573535919189453
Batch 54/64 loss: 0.7538671493530273
Batch 55/64 loss: 0.7084355354309082
Batch 56/64 loss: 0.832880973815918
Batch 57/64 loss: 0.5129208564758301
Batch 58/64 loss: 0.5761871337890625
Batch 59/64 loss: 0.5859103202819824
Batch 60/64 loss: 0.6674003601074219
Batch 61/64 loss: 0.6153140068054199
Batch 62/64 loss: 0.8679628372192383
Batch 63/64 loss: 0.6984262466430664
Batch 64/64 loss: -2.539976119995117
Epoch 168  Train loss: 0.6666193644205729  Val loss: 0.6369828751816373
Epoch 169
-------------------------------
Batch 1/64 loss: 0.5778603553771973
Batch 2/64 loss: 0.4085416793823242
Batch 3/64 loss: 0.8769640922546387
Batch 4/64 loss: 0.623112678527832
Batch 5/64 loss: 0.6888871192932129
Batch 6/64 loss: 0.739506721496582
Batch 7/64 loss: 0.44382715225219727
Batch 8/64 loss: 0.757652759552002
Batch 9/64 loss: 0.76300048828125
Batch 10/64 loss: 0.8408517837524414
Batch 11/64 loss: 1.0109248161315918
Batch 12/64 loss: 1.3066115379333496
Batch 13/64 loss: 0.9380307197570801
Batch 14/64 loss: 0.49076032638549805
Batch 15/64 loss: 0.5700135231018066
Batch 16/64 loss: 0.8348069190979004
Batch 17/64 loss: 0.5758767127990723
Batch 18/64 loss: 0.5662355422973633
Batch 19/64 loss: 0.693603515625
Batch 20/64 loss: 0.7214322090148926
Batch 21/64 loss: 0.7454447746276855
Batch 22/64 loss: 0.7748336791992188
Batch 23/64 loss: 0.8386087417602539
Batch 24/64 loss: 0.5422506332397461
Batch 25/64 loss: 0.7497544288635254
Batch 26/64 loss: 0.6190586090087891
Batch 27/64 loss: 0.7275667190551758
Batch 28/64 loss: 0.5639042854309082
Batch 29/64 loss: 0.629427433013916
Batch 30/64 loss: 0.5658774375915527
Batch 31/64 loss: 0.5754938125610352
Batch 32/64 loss: 0.694735050201416
Batch 33/64 loss: 0.8345012664794922
Batch 34/64 loss: 0.4110875129699707
Batch 35/64 loss: 0.7617154121398926
Batch 36/64 loss: 0.47694826126098633
Batch 37/64 loss: 0.7601985931396484
Batch 38/64 loss: 0.6205005645751953
Batch 39/64 loss: 0.9466385841369629
Batch 40/64 loss: 0.7168760299682617
Batch 41/64 loss: 0.5971255302429199
Batch 42/64 loss: 0.8964791297912598
Batch 43/64 loss: 0.604896068572998
Batch 44/64 loss: 0.5566158294677734
Batch 45/64 loss: 0.2974891662597656
Batch 46/64 loss: 0.6020603179931641
Batch 47/64 loss: 0.7078619003295898
Batch 48/64 loss: 0.5250425338745117
Batch 49/64 loss: 1.03369140625
Batch 50/64 loss: 0.707855224609375
Batch 51/64 loss: 1.0863571166992188
Batch 52/64 loss: 0.44855260848999023
Batch 53/64 loss: 0.5067133903503418
Batch 54/64 loss: 0.9114999771118164
Batch 55/64 loss: 0.44728994369506836
Batch 56/64 loss: 0.20257139205932617
Batch 57/64 loss: 0.5472464561462402
Batch 58/64 loss: 0.5112977027893066
Batch 59/64 loss: 0.5199856758117676
Batch 60/64 loss: 0.5613298416137695
Batch 61/64 loss: 0.6312894821166992
Batch 62/64 loss: 0.968541145324707
Batch 63/64 loss: 1.068650722503662
Batch 64/64 loss: -2.9468135833740234
Epoch 169  Train loss: 0.6386549781350529  Val loss: 0.660358737014823
Epoch 170
-------------------------------
Batch 1/64 loss: 0.7729802131652832
Batch 2/64 loss: 0.7245473861694336
Batch 3/64 loss: 0.6328511238098145
Batch 4/64 loss: 0.7130904197692871
Batch 5/64 loss: 0.33670520782470703
Batch 6/64 loss: 0.5434994697570801
Batch 7/64 loss: 0.21017074584960938
Batch 8/64 loss: 0.5398011207580566
Batch 9/64 loss: 0.8180513381958008
Batch 10/64 loss: 0.9555521011352539
Batch 11/64 loss: 0.6305179595947266
Batch 12/64 loss: 0.6326236724853516
Batch 13/64 loss: 0.36835670471191406
Batch 14/64 loss: 0.3672027587890625
Batch 15/64 loss: 0.6701655387878418
Batch 16/64 loss: 0.8313679695129395
Batch 17/64 loss: 1.0124688148498535
Batch 18/64 loss: 0.259920597076416
Batch 19/64 loss: 1.1301159858703613
Batch 20/64 loss: 0.4166994094848633
Batch 21/64 loss: 0.6470170021057129
Batch 22/64 loss: 0.5405635833740234
Batch 23/64 loss: 0.46725988388061523
Batch 24/64 loss: 0.5950064659118652
Batch 25/64 loss: 0.7419614791870117
Batch 26/64 loss: 0.7296481132507324
Batch 27/64 loss: 0.28029346466064453
Batch 28/64 loss: 1.1128015518188477
Batch 29/64 loss: 0.8069100379943848
Batch 30/64 loss: 0.6447849273681641
Batch 31/64 loss: 0.3369255065917969
Batch 32/64 loss: 0.2556018829345703
Batch 33/64 loss: 0.5424890518188477
Batch 34/64 loss: 0.7511987686157227
Batch 35/64 loss: 0.9424228668212891
Batch 36/64 loss: 0.8437991142272949
Batch 37/64 loss: 0.4238128662109375
Batch 38/64 loss: 0.38019371032714844
Batch 39/64 loss: 0.568483829498291
Batch 40/64 loss: 0.2950248718261719
Batch 41/64 loss: 0.5454549789428711
Batch 42/64 loss: 0.5661630630493164
Batch 43/64 loss: 0.4348263740539551
Batch 44/64 loss: 0.6631612777709961
Batch 45/64 loss: 0.2275524139404297
Batch 46/64 loss: 0.20051956176757812
Batch 47/64 loss: 0.7802801132202148
Batch 48/64 loss: 0.6651754379272461
Batch 49/64 loss: 0.7191348075866699
Batch 50/64 loss: 0.9946212768554688
Batch 51/64 loss: 1.0718331336975098
Batch 52/64 loss: 0.4532780647277832
Batch 53/64 loss: 0.27363061904907227
Batch 54/64 loss: 0.45438051223754883
Batch 55/64 loss: 0.4330406188964844
Batch 56/64 loss: 0.5482735633850098
Batch 57/64 loss: 0.40607357025146484
Batch 58/64 loss: 0.4852943420410156
Batch 59/64 loss: 0.6763639450073242
Batch 60/64 loss: 0.6070747375488281
Batch 61/64 loss: 0.8199996948242188
Batch 62/64 loss: 0.7376389503479004
Batch 63/64 loss: 0.512479305267334
Batch 64/64 loss: -3.325410842895508
Epoch 170  Train loss: 0.5530208587646485  Val loss: 0.5797752301717541
Saving best model, epoch: 170
Epoch 171
-------------------------------
Batch 1/64 loss: 0.41251659393310547
Batch 2/64 loss: 0.9758362770080566
Batch 3/64 loss: 0.5998353958129883
Batch 4/64 loss: 0.36427927017211914
Batch 5/64 loss: 0.37687110900878906
Batch 6/64 loss: 0.3843240737915039
Batch 7/64 loss: 0.5899276733398438
Batch 8/64 loss: 0.5019311904907227
Batch 9/64 loss: 0.4906282424926758
Batch 10/64 loss: 0.5166082382202148
Batch 11/64 loss: 0.7585344314575195
Batch 12/64 loss: 0.8879451751708984
Batch 13/64 loss: 0.3833136558532715
Batch 14/64 loss: 0.5572729110717773
Batch 15/64 loss: 0.6461391448974609
Batch 16/64 loss: 0.39075422286987305
Batch 17/64 loss: 0.6254768371582031
Batch 18/64 loss: 0.5118985176086426
Batch 19/64 loss: 0.5365653038024902
Batch 20/64 loss: 0.36768102645874023
Batch 21/64 loss: 0.7072486877441406
Batch 22/64 loss: 0.19285106658935547
Batch 23/64 loss: 0.40842676162719727
Batch 24/64 loss: 0.5653338432312012
Batch 25/64 loss: 0.5603837966918945
Batch 26/64 loss: 0.42159557342529297
Batch 27/64 loss: 0.3072032928466797
Batch 28/64 loss: 0.7170624732971191
Batch 29/64 loss: 0.7941842079162598
Batch 30/64 loss: 0.4420595169067383
Batch 31/64 loss: 0.8260421752929688
Batch 32/64 loss: 0.6673111915588379
Batch 33/64 loss: 0.3769245147705078
Batch 34/64 loss: 0.6530814170837402
Batch 35/64 loss: 0.46675586700439453
Batch 36/64 loss: 0.6956000328063965
Batch 37/64 loss: 0.5341219902038574
Batch 38/64 loss: 0.32303810119628906
Batch 39/64 loss: 0.6750836372375488
Batch 40/64 loss: 0.464083194732666
Batch 41/64 loss: 0.5352029800415039
Batch 42/64 loss: 0.49088287353515625
Batch 43/64 loss: 0.08980274200439453
Batch 44/64 loss: 0.5023612976074219
Batch 45/64 loss: 0.6009087562561035
Batch 46/64 loss: 0.8586764335632324
Batch 47/64 loss: 0.569725513458252
Batch 48/64 loss: 0.4099001884460449
Batch 49/64 loss: 0.850578784942627
Batch 50/64 loss: 0.3647041320800781
Batch 51/64 loss: 0.4676952362060547
Batch 52/64 loss: 0.5237131118774414
Batch 53/64 loss: 0.24644899368286133
Batch 54/64 loss: 0.9856858253479004
Batch 55/64 loss: 0.5230717658996582
Batch 56/64 loss: 1.034097671508789
Batch 57/64 loss: 0.342531681060791
Batch 58/64 loss: 0.9373955726623535
Batch 59/64 loss: 0.8807268142700195
Batch 60/64 loss: 0.3277626037597656
Batch 61/64 loss: 0.31509971618652344
Batch 62/64 loss: 0.7419519424438477
Batch 63/64 loss: 0.45917367935180664
Batch 64/64 loss: -3.184640884399414
Epoch 171  Train loss: 0.5073936985988243  Val loss: 0.6441705448111308
Epoch 172
-------------------------------
Batch 1/64 loss: 0.6456031799316406
Batch 2/64 loss: 0.6150474548339844
Batch 3/64 loss: 0.7530417442321777
Batch 4/64 loss: 0.665257453918457
Batch 5/64 loss: 0.3878507614135742
Batch 6/64 loss: 0.5430822372436523
Batch 7/64 loss: 0.5239710807800293
Batch 8/64 loss: 0.6494979858398438
Batch 9/64 loss: 0.6109857559204102
Batch 10/64 loss: 0.690833568572998
Batch 11/64 loss: 0.794468879699707
Batch 12/64 loss: 0.5068836212158203
Batch 13/64 loss: 0.5023922920227051
Batch 14/64 loss: 0.44255876541137695
Batch 15/64 loss: 0.3985452651977539
Batch 16/64 loss: 0.6719789505004883
Batch 17/64 loss: 0.8097257614135742
Batch 18/64 loss: 0.45131349563598633
Batch 19/64 loss: 0.2607436180114746
Batch 20/64 loss: 0.881101131439209
Batch 21/64 loss: 0.49285316467285156
Batch 22/64 loss: 0.2744121551513672
Batch 23/64 loss: 0.5437402725219727
Batch 24/64 loss: 0.2316608428955078
Batch 25/64 loss: 0.4556694030761719
Batch 26/64 loss: 0.4237489700317383
Batch 27/64 loss: 0.1492619514465332
Batch 28/64 loss: 0.6226367950439453
Batch 29/64 loss: 0.6314053535461426
Batch 30/64 loss: 0.32924318313598633
Batch 31/64 loss: 0.6019201278686523
Batch 32/64 loss: 0.9143381118774414
Batch 33/64 loss: 0.4661550521850586
Batch 34/64 loss: 0.23270082473754883
Batch 35/64 loss: 0.5946168899536133
Batch 36/64 loss: 0.36701059341430664
Batch 37/64 loss: 0.6975741386413574
Batch 38/64 loss: 0.5804543495178223
Batch 39/64 loss: 0.6274909973144531
Batch 40/64 loss: 0.5536713600158691
Batch 41/64 loss: 0.46476078033447266
Batch 42/64 loss: 0.41420936584472656
Batch 43/64 loss: 0.505836009979248
Batch 44/64 loss: 0.4718475341796875
Batch 45/64 loss: 0.7032451629638672
Batch 46/64 loss: 0.48969364166259766
Batch 47/64 loss: 0.45262575149536133
Batch 48/64 loss: 0.5943784713745117
Batch 49/64 loss: 0.7348909378051758
Batch 50/64 loss: 0.4413900375366211
Batch 51/64 loss: 0.5290794372558594
Batch 52/64 loss: 0.722285270690918
Batch 53/64 loss: 0.8320088386535645
Batch 54/64 loss: 0.4905252456665039
Batch 55/64 loss: 0.4713754653930664
Batch 56/64 loss: 0.7483921051025391
Batch 57/64 loss: 0.6281213760375977
Batch 58/64 loss: 0.31740808486938477
Batch 59/64 loss: 0.517280101776123
Batch 60/64 loss: 0.4633970260620117
Batch 61/64 loss: 0.5511236190795898
Batch 62/64 loss: 0.4095463752746582
Batch 63/64 loss: 0.7081146240234375
Batch 64/64 loss: -2.719707489013672
Epoch 172  Train loss: 0.5053678774366192  Val loss: 0.5037194347053868
Saving best model, epoch: 172
Epoch 173
-------------------------------
Batch 1/64 loss: 0.6928310394287109
Batch 2/64 loss: 0.44620752334594727
Batch 3/64 loss: 0.2439274787902832
Batch 4/64 loss: 0.33951711654663086
Batch 5/64 loss: 0.55511474609375
Batch 6/64 loss: 0.8894448280334473
Batch 7/64 loss: 0.6885333061218262
Batch 8/64 loss: 0.19003820419311523
Batch 9/64 loss: 0.6909623146057129
Batch 10/64 loss: 0.28534793853759766
Batch 11/64 loss: 0.1677694320678711
Batch 12/64 loss: 0.6731839179992676
Batch 13/64 loss: 0.07193803787231445
Batch 14/64 loss: 0.5111684799194336
Batch 15/64 loss: 0.786555290222168
Batch 16/64 loss: 0.2551541328430176
Batch 17/64 loss: 0.7281694412231445
Batch 18/64 loss: 0.4495816230773926
Batch 19/64 loss: 0.6238522529602051
Batch 20/64 loss: 0.6160855293273926
Batch 21/64 loss: 0.4759950637817383
Batch 22/64 loss: 0.5147180557250977
Batch 23/64 loss: 0.9451532363891602
Batch 24/64 loss: 0.5215997695922852
Batch 25/64 loss: 0.4823741912841797
Batch 26/64 loss: 0.23697137832641602
Batch 27/64 loss: 0.529599666595459
Batch 28/64 loss: 0.6461539268493652
Batch 29/64 loss: 0.5119895935058594
Batch 30/64 loss: 0.572627067565918
Batch 31/64 loss: 0.5688915252685547
Batch 32/64 loss: 0.5470046997070312
Batch 33/64 loss: 0.456179141998291
Batch 34/64 loss: 0.3540525436401367
Batch 35/64 loss: 0.4607691764831543
Batch 36/64 loss: 0.6459507942199707
Batch 37/64 loss: 0.4998903274536133
Batch 38/64 loss: 0.28876399993896484
Batch 39/64 loss: 0.47772789001464844
Batch 40/64 loss: 0.4571194648742676
Batch 41/64 loss: 0.7026700973510742
Batch 42/64 loss: 0.2808060646057129
Batch 43/64 loss: 0.5028600692749023
Batch 44/64 loss: 0.5860910415649414
Batch 45/64 loss: 0.2389507293701172
Batch 46/64 loss: 0.15881586074829102
Batch 47/64 loss: 0.29496288299560547
Batch 48/64 loss: 0.47698974609375
Batch 49/64 loss: 0.5345044136047363
Batch 50/64 loss: 0.7889800071716309
Batch 51/64 loss: 0.5225887298583984
Batch 52/64 loss: 0.6405129432678223
Batch 53/64 loss: 0.562349796295166
Batch 54/64 loss: 0.5362148284912109
Batch 55/64 loss: 0.3243241310119629
Batch 56/64 loss: 0.5404553413391113
Batch 57/64 loss: 0.8593649864196777
Batch 58/64 loss: 0.1388835906982422
Batch 59/64 loss: 0.7104544639587402
Batch 60/64 loss: 0.3215675354003906
Batch 61/64 loss: 0.52154541015625
Batch 62/64 loss: 0.9432616233825684
Batch 63/64 loss: 0.6993203163146973
Batch 64/64 loss: -2.8845081329345703
Epoch 173  Train loss: 0.4677961985270182  Val loss: 0.517191621446118
Epoch 174
-------------------------------
Batch 1/64 loss: 0.1459355354309082
Batch 2/64 loss: 0.2661166191101074
Batch 3/64 loss: 0.585413932800293
Batch 4/64 loss: 0.26384973526000977
Batch 5/64 loss: 0.5198259353637695
Batch 6/64 loss: 0.501798152923584
Batch 7/64 loss: 0.7306013107299805
Batch 8/64 loss: 0.5047698020935059
Batch 9/64 loss: 0.28978729248046875
Batch 10/64 loss: 0.7136445045471191
Batch 11/64 loss: 0.42206239700317383
Batch 12/64 loss: 0.47363853454589844
Batch 13/64 loss: 0.49822139739990234
Batch 14/64 loss: 0.38001251220703125
Batch 15/64 loss: 0.48043394088745117
Batch 16/64 loss: 0.7303009033203125
Batch 17/64 loss: 0.3976569175720215
Batch 18/64 loss: 0.32265567779541016
Batch 19/64 loss: 0.288480281829834
Batch 20/64 loss: 0.26499176025390625
Batch 21/64 loss: 0.8969650268554688
Batch 22/64 loss: 0.705970287322998
Batch 23/64 loss: 0.15935516357421875
Batch 24/64 loss: 0.4299020767211914
Batch 25/64 loss: 0.9698290824890137
Batch 26/64 loss: 0.4653019905090332
Batch 27/64 loss: 0.7231941223144531
Batch 28/64 loss: 1.1340103149414062
Batch 29/64 loss: 0.8976407051086426
Batch 30/64 loss: 0.65936279296875
Batch 31/64 loss: 0.6442303657531738
Batch 32/64 loss: 0.6115179061889648
Batch 33/64 loss: 0.8668398857116699
Batch 34/64 loss: 0.9642696380615234
Batch 35/64 loss: 0.7643547058105469
Batch 36/64 loss: 0.5048661231994629
Batch 37/64 loss: 0.7383561134338379
Batch 38/64 loss: 0.5946445465087891
Batch 39/64 loss: 1.0998644828796387
Batch 40/64 loss: 0.5537128448486328
Batch 41/64 loss: 0.5974922180175781
Batch 42/64 loss: 0.3503880500793457
Batch 43/64 loss: 0.6334476470947266
Batch 44/64 loss: 0.4290432929992676
Batch 45/64 loss: 0.8432550430297852
Batch 46/64 loss: 0.517216682434082
Batch 47/64 loss: 0.486971378326416
Batch 48/64 loss: 0.6094560623168945
Batch 49/64 loss: 0.8785223960876465
Batch 50/64 loss: 0.5101184844970703
Batch 51/64 loss: 1.0678377151489258
Batch 52/64 loss: 0.3895597457885742
Batch 53/64 loss: 0.7485098838806152
Batch 54/64 loss: 0.6711826324462891
Batch 55/64 loss: 0.2967357635498047
Batch 56/64 loss: 0.5510101318359375
Batch 57/64 loss: 0.8647680282592773
Batch 58/64 loss: 0.43274593353271484
Batch 59/64 loss: 1.0459256172180176
Batch 60/64 loss: 0.7752475738525391
Batch 61/64 loss: 0.4908576011657715
Batch 62/64 loss: 0.439603328704834
Batch 63/64 loss: 0.7638731002807617
Batch 64/64 loss: -2.8129472732543945
Epoch 174  Train loss: 0.5560540105782303  Val loss: 0.6675025245168364
Epoch 175
-------------------------------
Batch 1/64 loss: 0.7479133605957031
Batch 2/64 loss: 0.4587216377258301
Batch 3/64 loss: 0.3995323181152344
Batch 4/64 loss: 0.32080602645874023
Batch 5/64 loss: 0.523308277130127
Batch 6/64 loss: 0.7250666618347168
Batch 7/64 loss: 0.8405203819274902
Batch 8/64 loss: 0.8542609214782715
Batch 9/64 loss: 0.9661016464233398
Batch 10/64 loss: 0.736659049987793
Batch 11/64 loss: 0.6870861053466797
Batch 12/64 loss: 0.40581274032592773
Batch 13/64 loss: 0.5864033699035645
Batch 14/64 loss: 0.8582425117492676
Batch 15/64 loss: 0.5179290771484375
Batch 16/64 loss: 0.5355677604675293
Batch 17/64 loss: 0.4822878837585449
Batch 18/64 loss: 0.6381101608276367
Batch 19/64 loss: 0.3609023094177246
Batch 20/64 loss: 0.7026424407958984
Batch 21/64 loss: 0.5604090690612793
Batch 22/64 loss: 0.6098871231079102
Batch 23/64 loss: 0.5407495498657227
Batch 24/64 loss: 0.42989063262939453
Batch 25/64 loss: 0.5565152168273926
Batch 26/64 loss: 0.823491096496582
Batch 27/64 loss: 0.5097670555114746
Batch 28/64 loss: 0.6665792465209961
Batch 29/64 loss: 0.27823305130004883
Batch 30/64 loss: 0.6406378746032715
Batch 31/64 loss: 0.8605766296386719
Batch 32/64 loss: 0.7565627098083496
Batch 33/64 loss: 0.562009334564209
Batch 34/64 loss: 0.4223971366882324
Batch 35/64 loss: 0.594825267791748
Batch 36/64 loss: 0.5771040916442871
Batch 37/64 loss: 0.821131706237793
Batch 38/64 loss: 0.5468888282775879
Batch 39/64 loss: 0.7138476371765137
Batch 40/64 loss: 1.0360207557678223
Batch 41/64 loss: 0.7570266723632812
Batch 42/64 loss: 0.46909189224243164
Batch 43/64 loss: 0.4364171028137207
Batch 44/64 loss: 0.2543172836303711
Batch 45/64 loss: 0.36158180236816406
Batch 46/64 loss: 0.17595958709716797
Batch 47/64 loss: 0.5820932388305664
Batch 48/64 loss: 0.6846556663513184
Batch 49/64 loss: 0.8060121536254883
Batch 50/64 loss: 0.3781862258911133
Batch 51/64 loss: 0.44197750091552734
Batch 52/64 loss: 0.47304630279541016
Batch 53/64 loss: 0.3224983215332031
Batch 54/64 loss: 0.21967840194702148
Batch 55/64 loss: 0.8143014907836914
Batch 56/64 loss: 0.5255126953125
Batch 57/64 loss: 1.1816315650939941
Batch 58/64 loss: 1.0122838020324707
Batch 59/64 loss: 0.45868349075317383
Batch 60/64 loss: 0.6295652389526367
Batch 61/64 loss: 0.19066476821899414
Batch 62/64 loss: 0.40744495391845703
Batch 63/64 loss: 0.4400978088378906
Batch 64/64 loss: -2.771817684173584
Epoch 175  Train loss: 0.545870797774371  Val loss: 0.6126933736899465
Epoch 176
-------------------------------
Batch 1/64 loss: 0.4779953956604004
Batch 2/64 loss: 0.28913164138793945
Batch 3/64 loss: 0.15125608444213867
Batch 4/64 loss: 0.7474417686462402
Batch 5/64 loss: 1.0304059982299805
Batch 6/64 loss: 0.32355213165283203
Batch 7/64 loss: 0.44863319396972656
Batch 8/64 loss: 0.21668434143066406
Batch 9/64 loss: 0.8945679664611816
Batch 10/64 loss: 0.254178524017334
Batch 11/64 loss: 0.6350369453430176
Batch 12/64 loss: 0.40359020233154297
Batch 13/64 loss: 0.8219928741455078
Batch 14/64 loss: 0.7938227653503418
Batch 15/64 loss: 0.4050431251525879
Batch 16/64 loss: 0.33197498321533203
Batch 17/64 loss: 0.29935121536254883
Batch 18/64 loss: 0.6773605346679688
Batch 19/64 loss: 0.37120676040649414
Batch 20/64 loss: 0.4529261589050293
Batch 21/64 loss: 0.42505502700805664
Batch 22/64 loss: 0.42407798767089844
Batch 23/64 loss: 0.6262550354003906
Batch 24/64 loss: 0.26221275329589844
Batch 25/64 loss: 0.4136471748352051
Batch 26/64 loss: 0.6105608940124512
Batch 27/64 loss: 0.42060279846191406
Batch 28/64 loss: 0.3526334762573242
Batch 29/64 loss: 0.6783838272094727
Batch 30/64 loss: 0.42690277099609375
Batch 31/64 loss: 0.45349645614624023
Batch 32/64 loss: 0.4044818878173828
Batch 33/64 loss: 0.4280672073364258
Batch 34/64 loss: 1.5845532417297363
Batch 35/64 loss: 0.49493885040283203
Batch 36/64 loss: 0.7008371353149414
Batch 37/64 loss: 0.7590255737304688
Batch 38/64 loss: 0.8473701477050781
Batch 39/64 loss: 0.4886307716369629
Batch 40/64 loss: 0.701810359954834
Batch 41/64 loss: 1.053349494934082
Batch 42/64 loss: 0.7020821571350098
Batch 43/64 loss: 0.7952251434326172
Batch 44/64 loss: 0.9051008224487305
Batch 45/64 loss: 0.6525392532348633
Batch 46/64 loss: 0.7234740257263184
Batch 47/64 loss: 0.9561152458190918
Batch 48/64 loss: 0.7881069183349609
Batch 49/64 loss: 1.007199764251709
Batch 50/64 loss: 0.5271620750427246
Batch 51/64 loss: 0.9626307487487793
Batch 52/64 loss: 0.7845277786254883
Batch 53/64 loss: 0.6194772720336914
Batch 54/64 loss: 0.6060609817504883
Batch 55/64 loss: 0.9532461166381836
Batch 56/64 loss: 0.4925508499145508
Batch 57/64 loss: 0.5795860290527344
Batch 58/64 loss: 0.47411155700683594
Batch 59/64 loss: 0.772188663482666
Batch 60/64 loss: 0.7239136695861816
Batch 61/64 loss: 1.0751490592956543
Batch 62/64 loss: 0.5436077117919922
Batch 63/64 loss: 0.6351761817932129
Batch 64/64 loss: -2.69242000579834
Epoch 176  Train loss: 0.5779288235832664  Val loss: 0.6953716015897665
Epoch 177
-------------------------------
Batch 1/64 loss: 0.4522705078125
Batch 2/64 loss: 3.2591681480407715
Batch 3/64 loss: 0.5645780563354492
Batch 4/64 loss: 0.9199066162109375
Batch 5/64 loss: 0.6589803695678711
Batch 6/64 loss: 0.8632760047912598
Batch 7/64 loss: 0.6521921157836914
Batch 8/64 loss: 0.8433542251586914
Batch 9/64 loss: 1.0414948463439941
Batch 10/64 loss: 0.8642940521240234
Batch 11/64 loss: 0.8141436576843262
Batch 12/64 loss: 0.6598992347717285
Batch 13/64 loss: 0.5392627716064453
Batch 14/64 loss: 0.6504583358764648
Batch 15/64 loss: 0.7800521850585938
Batch 16/64 loss: 0.6182608604431152
Batch 17/64 loss: 0.6733016967773438
Batch 18/64 loss: 0.835324764251709
Batch 19/64 loss: 0.7596826553344727
Batch 20/64 loss: 1.0636067390441895
Batch 21/64 loss: 0.6719379425048828
Batch 22/64 loss: 0.9272236824035645
Batch 23/64 loss: 0.5860228538513184
Batch 24/64 loss: 1.4065213203430176
Batch 25/64 loss: 0.8980321884155273
Batch 26/64 loss: 0.7061090469360352
Batch 27/64 loss: 0.5789775848388672
Batch 28/64 loss: 0.6376543045043945
Batch 29/64 loss: 1.199422836303711
Batch 30/64 loss: 0.6925115585327148
Batch 31/64 loss: 0.6545157432556152
Batch 32/64 loss: 0.2949085235595703
Batch 33/64 loss: 0.724602222442627
Batch 34/64 loss: 0.5360770225524902
Batch 35/64 loss: 0.8403148651123047
Batch 36/64 loss: 0.8938965797424316
Batch 37/64 loss: 0.3479328155517578
Batch 38/64 loss: 0.987675666809082
Batch 39/64 loss: 0.6147933006286621
Batch 40/64 loss: 0.6502151489257812
Batch 41/64 loss: 0.6128048896789551
Batch 42/64 loss: 0.6132087707519531
Batch 43/64 loss: 1.0013670921325684
Batch 44/64 loss: 0.964324951171875
Batch 45/64 loss: 0.37367677688598633
Batch 46/64 loss: 0.6930084228515625
Batch 47/64 loss: 0.35341739654541016
Batch 48/64 loss: 0.22466135025024414
Batch 49/64 loss: 0.9084811210632324
Batch 50/64 loss: 0.3641238212585449
Batch 51/64 loss: 0.3750457763671875
Batch 52/64 loss: 0.6242661476135254
Batch 53/64 loss: 0.48151636123657227
Batch 54/64 loss: 0.45918703079223633
Batch 55/64 loss: 0.6172957420349121
Batch 56/64 loss: 0.4591660499572754
Batch 57/64 loss: 0.8586602210998535
Batch 58/64 loss: 0.32637500762939453
Batch 59/64 loss: 0.5772476196289062
Batch 60/64 loss: 1.0782346725463867
Batch 61/64 loss: 0.7973484992980957
Batch 62/64 loss: 0.6397662162780762
Batch 63/64 loss: 0.9757671356201172
Batch 64/64 loss: -2.744961738586426
Epoch 177  Train loss: 0.7009111030429017  Val loss: 0.6612450773363671
Epoch 178
-------------------------------
Batch 1/64 loss: 0.4062690734863281
Batch 2/64 loss: 0.659998893737793
Batch 3/64 loss: 0.7342991828918457
Batch 4/64 loss: 0.6592617034912109
Batch 5/64 loss: 0.6097245216369629
Batch 6/64 loss: 0.6377596855163574
Batch 7/64 loss: 0.789304256439209
Batch 8/64 loss: 0.8715529441833496
Batch 9/64 loss: 0.3470458984375
Batch 10/64 loss: 1.0557966232299805
Batch 11/64 loss: 0.5799880027770996
Batch 12/64 loss: 0.7276387214660645
Batch 13/64 loss: 0.4178428649902344
Batch 14/64 loss: 0.5356540679931641
Batch 15/64 loss: 0.4844627380371094
Batch 16/64 loss: 0.6920328140258789
Batch 17/64 loss: 0.6004672050476074
Batch 18/64 loss: 0.7435140609741211
Batch 19/64 loss: 0.4673142433166504
Batch 20/64 loss: 0.9957833290100098
Batch 21/64 loss: 0.2100386619567871
Batch 22/64 loss: 0.3464851379394531
Batch 23/64 loss: 0.4520745277404785
Batch 24/64 loss: 0.6641788482666016
Batch 25/64 loss: 0.4761638641357422
Batch 26/64 loss: 0.8269038200378418
Batch 27/64 loss: 1.1847248077392578
Batch 28/64 loss: 0.7267875671386719
Batch 29/64 loss: 0.685701847076416
Batch 30/64 loss: 0.7075767517089844
Batch 31/64 loss: 0.26782751083374023
Batch 32/64 loss: 0.6902985572814941
Batch 33/64 loss: 0.7213010787963867
Batch 34/64 loss: 0.385317325592041
Batch 35/64 loss: 0.4760432243347168
Batch 36/64 loss: 0.6107916831970215
Batch 37/64 loss: 0.702967643737793
Batch 38/64 loss: 0.5694437026977539
Batch 39/64 loss: 0.6583094596862793
Batch 40/64 loss: 0.16984319686889648
Batch 41/64 loss: 0.7449855804443359
Batch 42/64 loss: 0.6556382179260254
Batch 43/64 loss: 0.6192584037780762
Batch 44/64 loss: 0.4018993377685547
Batch 45/64 loss: 0.4639401435852051
Batch 46/64 loss: 0.44409847259521484
Batch 47/64 loss: 0.3988990783691406
Batch 48/64 loss: 0.3764333724975586
Batch 49/64 loss: 0.6220130920410156
Batch 50/64 loss: 0.6495060920715332
Batch 51/64 loss: 0.19865083694458008
Batch 52/64 loss: 0.41904163360595703
Batch 53/64 loss: 0.6825628280639648
Batch 54/64 loss: 0.5011563301086426
Batch 55/64 loss: 0.2669997215270996
Batch 56/64 loss: 0.8768668174743652
Batch 57/64 loss: 0.9771661758422852
Batch 58/64 loss: 0.43955516815185547
Batch 59/64 loss: 0.7258906364440918
Batch 60/64 loss: 0.5087056159973145
Batch 61/64 loss: 0.44220399856567383
Batch 62/64 loss: 0.7483615875244141
Batch 63/64 loss: 0.6193480491638184
Batch 64/64 loss: -2.887723922729492
Epoch 178  Train loss: 0.5516216203278186  Val loss: 0.5783027308093723
Epoch 179
-------------------------------
Batch 1/64 loss: 0.46317052841186523
Batch 2/64 loss: 0.9600310325622559
Batch 3/64 loss: 0.7668700218200684
Batch 4/64 loss: 0.44640254974365234
Batch 5/64 loss: 0.5013670921325684
Batch 6/64 loss: 1.1712050437927246
Batch 7/64 loss: 0.8565769195556641
Batch 8/64 loss: 0.6418375968933105
Batch 9/64 loss: 0.9233183860778809
Batch 10/64 loss: 0.49600982666015625
Batch 11/64 loss: 0.7324719429016113
Batch 12/64 loss: 0.48799943923950195
Batch 13/64 loss: 0.2812809944152832
Batch 14/64 loss: 0.7679572105407715
Batch 15/64 loss: 0.34407806396484375
Batch 16/64 loss: 0.76837158203125
Batch 17/64 loss: 0.4725494384765625
Batch 18/64 loss: 0.70794677734375
Batch 19/64 loss: 0.6394095420837402
Batch 20/64 loss: 0.43615245819091797
Batch 21/64 loss: 0.4020204544067383
Batch 22/64 loss: 0.4102330207824707
Batch 23/64 loss: 0.6634731292724609
Batch 24/64 loss: 0.6141462326049805
Batch 25/64 loss: 0.39048051834106445
Batch 26/64 loss: 0.47873497009277344
Batch 27/64 loss: 0.38107967376708984
Batch 28/64 loss: 0.7824602127075195
Batch 29/64 loss: 0.5611414909362793
Batch 30/64 loss: 0.9725375175476074
Batch 31/64 loss: 0.5729498863220215
Batch 32/64 loss: 0.46573543548583984
Batch 33/64 loss: 0.15923357009887695
Batch 34/64 loss: 0.23551177978515625
Batch 35/64 loss: 0.37508726119995117
Batch 36/64 loss: 0.37476301193237305
Batch 37/64 loss: 1.1157817840576172
Batch 38/64 loss: 0.5316753387451172
Batch 39/64 loss: 0.5469245910644531
Batch 40/64 loss: 0.19457721710205078
Batch 41/64 loss: 0.5082807540893555
Batch 42/64 loss: 0.4185194969177246
Batch 43/64 loss: 0.3570570945739746
Batch 44/64 loss: 0.36935853958129883
Batch 45/64 loss: 0.48282909393310547
Batch 46/64 loss: 0.5901122093200684
Batch 47/64 loss: 0.35205745697021484
Batch 48/64 loss: 0.6181931495666504
Batch 49/64 loss: 0.522219181060791
Batch 50/64 loss: 0.659304141998291
Batch 51/64 loss: 0.31914758682250977
Batch 52/64 loss: 0.3781867027282715
Batch 53/64 loss: 0.6918272972106934
Batch 54/64 loss: 0.6666932106018066
Batch 55/64 loss: 0.49168825149536133
Batch 56/64 loss: 0.37512636184692383
Batch 57/64 loss: 0.32369041442871094
Batch 58/64 loss: 0.5692143440246582
Batch 59/64 loss: 0.6342158317565918
Batch 60/64 loss: 0.32767152786254883
Batch 61/64 loss: 0.41058349609375
Batch 62/64 loss: 0.36249732971191406
Batch 63/64 loss: 0.37901782989501953
Batch 64/64 loss: -3.4778146743774414
Epoch 179  Train loss: 0.4908656288595761  Val loss: 0.5434816760295855
Epoch 180
-------------------------------
Batch 1/64 loss: 0.28154420852661133
Batch 2/64 loss: 1.0864181518554688
Batch 3/64 loss: 0.9509739875793457
Batch 4/64 loss: 0.22737550735473633
Batch 5/64 loss: 0.8366451263427734
Batch 6/64 loss: 0.2156848907470703
Batch 7/64 loss: 0.4785909652709961
Batch 8/64 loss: 0.6007766723632812
Batch 9/64 loss: 0.3982839584350586
Batch 10/64 loss: 1.114072322845459
Batch 11/64 loss: 0.6290020942687988
Batch 12/64 loss: 0.5629086494445801
Batch 13/64 loss: 0.46381282806396484
Batch 14/64 loss: 0.5508546829223633
Batch 15/64 loss: 0.8226170539855957
Batch 16/64 loss: 0.30099058151245117
Batch 17/64 loss: 0.3894648551940918
Batch 18/64 loss: 0.5099654197692871
Batch 19/64 loss: 0.914649486541748
Batch 20/64 loss: 1.315180778503418
Batch 21/64 loss: 0.34166622161865234
Batch 22/64 loss: 0.4667344093322754
Batch 23/64 loss: 0.42512989044189453
Batch 24/64 loss: 0.5168437957763672
Batch 25/64 loss: 0.1819629669189453
Batch 26/64 loss: 0.6515994071960449
Batch 27/64 loss: 0.804591178894043
Batch 28/64 loss: 0.785639762878418
Batch 29/64 loss: 0.7082486152648926
Batch 30/64 loss: 1.6959261894226074
Batch 31/64 loss: 0.8825850486755371
Batch 32/64 loss: 0.7320675849914551
Batch 33/64 loss: 0.7222409248352051
Batch 34/64 loss: 0.49713897705078125
Batch 35/64 loss: 0.38787078857421875
Batch 36/64 loss: 0.5446314811706543
Batch 37/64 loss: 0.51678466796875
Batch 38/64 loss: 0.6483078002929688
Batch 39/64 loss: 0.5246076583862305
Batch 40/64 loss: 0.4071979522705078
Batch 41/64 loss: 0.2836017608642578
Batch 42/64 loss: 0.3359255790710449
Batch 43/64 loss: 0.49074459075927734
Batch 44/64 loss: 0.4968748092651367
Batch 45/64 loss: 0.40614938735961914
Batch 46/64 loss: 0.3726038932800293
Batch 47/64 loss: 0.4513864517211914
Batch 48/64 loss: 0.8828988075256348
Batch 49/64 loss: 0.5523123741149902
Batch 50/64 loss: 0.6876482963562012
Batch 51/64 loss: 2.0864405632019043
Batch 52/64 loss: 0.19158601760864258
Batch 53/64 loss: 0.48558473587036133
Batch 54/64 loss: 0.4509925842285156
Batch 55/64 loss: 0.26042938232421875
Batch 56/64 loss: 0.4405951499938965
Batch 57/64 loss: 0.3017721176147461
Batch 58/64 loss: 0.566835880279541
Batch 59/64 loss: 0.5109343528747559
Batch 60/64 loss: 0.5626368522644043
Batch 61/64 loss: 0.1956315040588379
Batch 62/64 loss: 0.6041030883789062
Batch 63/64 loss: 0.516049861907959
Batch 64/64 loss: -2.8868255615234375
Epoch 180  Train loss: 0.5499640221689261  Val loss: 0.8202527364095052
Epoch 181
-------------------------------
Batch 1/64 loss: 0.6804451942443848
Batch 2/64 loss: 0.41426706314086914
Batch 3/64 loss: 0.5279173851013184
Batch 4/64 loss: 0.5417757034301758
Batch 5/64 loss: 0.4761390686035156
Batch 6/64 loss: 0.46439456939697266
Batch 7/64 loss: 0.528557300567627
Batch 8/64 loss: 0.40726470947265625
Batch 9/64 loss: 0.35303831100463867
Batch 10/64 loss: 0.553889274597168
Batch 11/64 loss: 0.6629323959350586
Batch 12/64 loss: 0.784818172454834
Batch 13/64 loss: 0.49787425994873047
Batch 14/64 loss: 0.37714338302612305
Batch 15/64 loss: 0.5707216262817383
Batch 16/64 loss: 0.5955629348754883
Batch 17/64 loss: 0.6519322395324707
Batch 18/64 loss: 0.5891499519348145
Batch 19/64 loss: 0.46431636810302734
Batch 20/64 loss: 0.2886533737182617
Batch 21/64 loss: 0.6804447174072266
Batch 22/64 loss: 0.23152446746826172
Batch 23/64 loss: 0.9682912826538086
Batch 24/64 loss: 0.27870750427246094
Batch 25/64 loss: 0.6626782417297363
Batch 26/64 loss: 0.38417482376098633
Batch 27/64 loss: 0.21303224563598633
Batch 28/64 loss: 0.3977236747741699
Batch 29/64 loss: 0.4890437126159668
Batch 30/64 loss: 0.6418671607971191
Batch 31/64 loss: 0.44518327713012695
Batch 32/64 loss: 0.6530089378356934
Batch 33/64 loss: 0.42952728271484375
Batch 34/64 loss: 0.3131885528564453
Batch 35/64 loss: 1.0235519409179688
Batch 36/64 loss: 0.6889019012451172
Batch 37/64 loss: 0.7791519165039062
Batch 38/64 loss: 0.6346402168273926
Batch 39/64 loss: 0.40423154830932617
Batch 40/64 loss: 1.0015597343444824
Batch 41/64 loss: 0.3857245445251465
Batch 42/64 loss: 0.4205622673034668
Batch 43/64 loss: 0.6832113265991211
Batch 44/64 loss: 0.09589672088623047
Batch 45/64 loss: 0.3827385902404785
Batch 46/64 loss: 0.4122333526611328
Batch 47/64 loss: 0.517906665802002
Batch 48/64 loss: 0.5274662971496582
Batch 49/64 loss: 0.5552616119384766
Batch 50/64 loss: 0.28130292892456055
Batch 51/64 loss: 0.8395447731018066
Batch 52/64 loss: 2.3856186866760254
Batch 53/64 loss: 0.4999852180480957
Batch 54/64 loss: 0.3075122833251953
Batch 55/64 loss: 0.9177684783935547
Batch 56/64 loss: 0.3331718444824219
Batch 57/64 loss: 1.4845294952392578
Batch 58/64 loss: 0.7203588485717773
Batch 59/64 loss: 0.2608327865600586
Batch 60/64 loss: 0.6510295867919922
Batch 61/64 loss: 1.3438034057617188
Batch 62/64 loss: 0.5216593742370605
Batch 63/64 loss: 0.11110210418701172
Batch 64/64 loss: -3.2783188819885254
Epoch 181  Train loss: 0.5322620934131099  Val loss: 1.0951977497113938
Epoch 182
-------------------------------
Batch 1/64 loss: 0.5518527030944824
Batch 2/64 loss: 0.683326244354248
Batch 3/64 loss: 0.3334646224975586
Batch 4/64 loss: 0.6004796028137207
Batch 5/64 loss: 0.6813468933105469
Batch 6/64 loss: 0.6692895889282227
Batch 7/64 loss: 1.2864594459533691
Batch 8/64 loss: 0.7983131408691406
Batch 9/64 loss: 0.8838081359863281
Batch 10/64 loss: 1.0509347915649414
Batch 11/64 loss: 0.5869874954223633
Batch 12/64 loss: 1.1516375541687012
Batch 13/64 loss: 0.5387063026428223
Batch 14/64 loss: 1.0530586242675781
Batch 15/64 loss: 0.6892085075378418
Batch 16/64 loss: 0.4480924606323242
Batch 17/64 loss: 0.24787616729736328
Batch 18/64 loss: 0.733208179473877
Batch 19/64 loss: 0.39925527572631836
Batch 20/64 loss: 0.4843177795410156
Batch 21/64 loss: 0.5729794502258301
Batch 22/64 loss: 0.30101442337036133
Batch 23/64 loss: 0.4014248847961426
Batch 24/64 loss: 0.49527740478515625
Batch 25/64 loss: 0.20241928100585938
Batch 26/64 loss: 0.7733006477355957
Batch 27/64 loss: 0.23269939422607422
Batch 28/64 loss: 0.13994455337524414
Batch 29/64 loss: 0.45493268966674805
Batch 30/64 loss: 0.8127708435058594
Batch 31/64 loss: 0.769249439239502
Batch 32/64 loss: 0.6106600761413574
Batch 33/64 loss: 0.3995933532714844
Batch 34/64 loss: 0.6705322265625
Batch 35/64 loss: 1.2412528991699219
Batch 36/64 loss: 0.3500542640686035
Batch 37/64 loss: 0.3436417579650879
Batch 38/64 loss: 0.40998077392578125
Batch 39/64 loss: 0.31539201736450195
Batch 40/64 loss: 0.5469260215759277
Batch 41/64 loss: 0.6899137496948242
Batch 42/64 loss: 0.4762449264526367
Batch 43/64 loss: 0.9395270347595215
Batch 44/64 loss: 1.8644061088562012
Batch 45/64 loss: 1.004323959350586
Batch 46/64 loss: 0.6423306465148926
Batch 47/64 loss: 0.5738201141357422
Batch 48/64 loss: 0.47774696350097656
Batch 49/64 loss: 0.37375783920288086
Batch 50/64 loss: 0.4430856704711914
Batch 51/64 loss: 0.7069125175476074
Batch 52/64 loss: 0.44568395614624023
Batch 53/64 loss: 0.27433109283447266
Batch 54/64 loss: 0.8172616958618164
Batch 55/64 loss: 0.752692699432373
Batch 56/64 loss: 0.6832246780395508
Batch 57/64 loss: 0.6190590858459473
Batch 58/64 loss: 0.4736766815185547
Batch 59/64 loss: 0.3062291145324707
Batch 60/64 loss: 0.43569135665893555
Batch 61/64 loss: 0.536712646484375
Batch 62/64 loss: 0.4031710624694824
Batch 63/64 loss: 0.4024076461791992
Batch 64/64 loss: -3.379847526550293
Epoch 182  Train loss: 0.5603607140335383  Val loss: 0.5224951714584508
Epoch 183
-------------------------------
Batch 1/64 loss: 0.41225433349609375
Batch 2/64 loss: 0.5100135803222656
Batch 3/64 loss: 1.1808648109436035
Batch 4/64 loss: 0.6582164764404297
Batch 5/64 loss: 0.5479516983032227
Batch 6/64 loss: 0.4144754409790039
Batch 7/64 loss: 0.47641944885253906
Batch 8/64 loss: 0.6915059089660645
Batch 9/64 loss: 0.7760992050170898
Batch 10/64 loss: 0.502509593963623
Batch 11/64 loss: 0.7869768142700195
Batch 12/64 loss: 0.4651651382446289
Batch 13/64 loss: 0.7867889404296875
Batch 14/64 loss: 0.7179732322692871
Batch 15/64 loss: 0.47887372970581055
Batch 16/64 loss: 0.43436336517333984
Batch 17/64 loss: 0.3364567756652832
Batch 18/64 loss: 0.3843669891357422
Batch 19/64 loss: 0.1399087905883789
Batch 20/64 loss: 0.8307795524597168
Batch 21/64 loss: 0.36644840240478516
Batch 22/64 loss: 0.5523576736450195
Batch 23/64 loss: 0.5544095039367676
Batch 24/64 loss: 0.4467349052429199
Batch 25/64 loss: 0.5177216529846191
Batch 26/64 loss: 0.9142255783081055
Batch 27/64 loss: 0.8916912078857422
Batch 28/64 loss: 1.1067848205566406
Batch 29/64 loss: 0.5867834091186523
Batch 30/64 loss: 0.45002079010009766
Batch 31/64 loss: 0.2718791961669922
Batch 32/64 loss: 0.3690943717956543
Batch 33/64 loss: 0.378875732421875
Batch 34/64 loss: 0.3676424026489258
Batch 35/64 loss: 0.5093221664428711
Batch 36/64 loss: 0.17656326293945312
Batch 37/64 loss: 0.9332036972045898
Batch 38/64 loss: 0.0779733657836914
Batch 39/64 loss: 0.27229976654052734
Batch 40/64 loss: 0.0688166618347168
Batch 41/64 loss: 0.5084586143493652
Batch 42/64 loss: 0.2964339256286621
Batch 43/64 loss: 0.3405766487121582
Batch 44/64 loss: 0.28833436965942383
Batch 45/64 loss: 0.08719921112060547
Batch 46/64 loss: 0.6968650817871094
Batch 47/64 loss: 0.48516368865966797
Batch 48/64 loss: 0.6890945434570312
Batch 49/64 loss: 0.3133397102355957
Batch 50/64 loss: 0.2949223518371582
Batch 51/64 loss: 0.38010692596435547
Batch 52/64 loss: 0.5547733306884766
Batch 53/64 loss: 1.1087727546691895
Batch 54/64 loss: 0.11984872817993164
Batch 55/64 loss: 0.28336524963378906
Batch 56/64 loss: 0.368471622467041
Batch 57/64 loss: 0.2956733703613281
Batch 58/64 loss: 1.7308087348937988
Batch 59/64 loss: 0.332888126373291
Batch 60/64 loss: 0.32657766342163086
Batch 61/64 loss: 0.03676939010620117
Batch 62/64 loss: 0.4340858459472656
Batch 63/64 loss: 0.9484744071960449
Batch 64/64 loss: -3.0439276695251465
Epoch 183  Train loss: 0.4703195441002939  Val loss: 0.6368194003285411
Epoch 184
-------------------------------
Batch 1/64 loss: 0.6326870918273926
Batch 2/64 loss: 0.6375799179077148
Batch 3/64 loss: 0.3358941078186035
Batch 4/64 loss: 0.47316741943359375
Batch 5/64 loss: 0.8118634223937988
Batch 6/64 loss: 0.9468741416931152
Batch 7/64 loss: 0.7826614379882812
Batch 8/64 loss: 0.41138362884521484
Batch 9/64 loss: 0.8301291465759277
Batch 10/64 loss: 0.21122121810913086
Batch 11/64 loss: 0.8888912200927734
Batch 12/64 loss: 0.5159125328063965
Batch 13/64 loss: 0.7758269309997559
Batch 14/64 loss: 0.22187566757202148
Batch 15/64 loss: 0.4249696731567383
Batch 16/64 loss: 0.37007617950439453
Batch 17/64 loss: 0.5000157356262207
Batch 18/64 loss: 0.657494068145752
Batch 19/64 loss: 0.2588324546813965
Batch 20/64 loss: 0.18244504928588867
Batch 21/64 loss: 0.36783361434936523
Batch 22/64 loss: 0.3645052909851074
Batch 23/64 loss: 0.6600217819213867
Batch 24/64 loss: 0.8037309646606445
Batch 25/64 loss: 0.3609466552734375
Batch 26/64 loss: 0.16988706588745117
Batch 27/64 loss: 0.944373607635498
Batch 28/64 loss: 0.2906675338745117
Batch 29/64 loss: 0.16847753524780273
Batch 30/64 loss: 0.4524526596069336
Batch 31/64 loss: 0.35191917419433594
Batch 32/64 loss: 0.23399829864501953
Batch 33/64 loss: 0.522822380065918
Batch 34/64 loss: 0.34354305267333984
Batch 35/64 loss: 0.16607379913330078
Batch 36/64 loss: 0.18481111526489258
Batch 37/64 loss: 0.5026674270629883
Batch 38/64 loss: 0.5350961685180664
Batch 39/64 loss: 1.1394281387329102
Batch 40/64 loss: 0.780428409576416
Batch 41/64 loss: 0.34865808486938477
Batch 42/64 loss: 0.28395605087280273
Batch 43/64 loss: 0.3705716133117676
Batch 44/64 loss: 0.3276505470275879
Batch 45/64 loss: 0.5742964744567871
Batch 46/64 loss: 0.31526613235473633
Batch 47/64 loss: 0.15219879150390625
Batch 48/64 loss: 0.30241966247558594
Batch 49/64 loss: 0.1155095100402832
Batch 50/64 loss: 0.8811378479003906
Batch 51/64 loss: 0.1573319435119629
Batch 52/64 loss: 0.48206233978271484
Batch 53/64 loss: 0.21114301681518555
Batch 54/64 loss: 0.9399313926696777
Batch 55/64 loss: 0.18875885009765625
Batch 56/64 loss: 0.39825439453125
Batch 57/64 loss: 0.5890493392944336
Batch 58/64 loss: 0.6331911087036133
Batch 59/64 loss: 0.40662288665771484
Batch 60/64 loss: 0.20812416076660156
Batch 61/64 loss: 0.8676857948303223
Batch 62/64 loss: 0.5757231712341309
Batch 63/64 loss: 0.4107522964477539
Batch 64/64 loss: -3.4181108474731445
Epoch 184  Train loss: 0.4296501645854875  Val loss: 0.5180890781363261
Epoch 185
-------------------------------
Batch 1/64 loss: 0.6321358680725098
Batch 2/64 loss: 0.43230772018432617
Batch 3/64 loss: 0.7137632369995117
Batch 4/64 loss: 0.35560178756713867
Batch 5/64 loss: 0.37397336959838867
Batch 6/64 loss: 0.43000316619873047
Batch 7/64 loss: 0.36670446395874023
Batch 8/64 loss: 0.2475147247314453
Batch 9/64 loss: 0.34801578521728516
Batch 10/64 loss: 1.7147364616394043
Batch 11/64 loss: 0.23780584335327148
Batch 12/64 loss: 0.7207455635070801
Batch 13/64 loss: 0.6486248970031738
Batch 14/64 loss: 0.9568076133728027
Batch 15/64 loss: 0.6018581390380859
Batch 16/64 loss: 2.9277849197387695
Batch 17/64 loss: 1.1196870803833008
Batch 18/64 loss: 0.7577590942382812
Batch 19/64 loss: 0.7414860725402832
Batch 20/64 loss: 0.32418155670166016
Batch 21/64 loss: 3.2422494888305664
Batch 22/64 loss: 0.626884937286377
Batch 23/64 loss: 3.9190526008605957
Batch 24/64 loss: 1.1241841316223145
Batch 25/64 loss: 0.5707278251647949
Batch 26/64 loss: 0.7117748260498047
Batch 27/64 loss: 1.5994291305541992
Batch 28/64 loss: 1.1668391227722168
Batch 29/64 loss: 3.8534750938415527
Batch 30/64 loss: 1.4545674324035645
Batch 31/64 loss: 0.9173216819763184
Batch 32/64 loss: 1.2268962860107422
Batch 33/64 loss: 4.443319797515869
Batch 34/64 loss: 1.4526376724243164
Batch 35/64 loss: 1.0477113723754883
Batch 36/64 loss: 1.5031204223632812
Batch 37/64 loss: 1.279055118560791
Batch 38/64 loss: 1.682633876800537
Batch 39/64 loss: 2.594034194946289
Batch 40/64 loss: 3.6553540229797363
Batch 41/64 loss: 1.6666617393493652
Batch 42/64 loss: 2.275930404663086
Batch 43/64 loss: 1.309518814086914
Batch 44/64 loss: 0.8270769119262695
Batch 45/64 loss: 1.8044638633728027
Batch 46/64 loss: 1.322838306427002
Batch 47/64 loss: 1.5072417259216309
Batch 48/64 loss: 1.7790417671203613
Batch 49/64 loss: 1.7974662780761719
Batch 50/64 loss: 2.160339832305908
Batch 51/64 loss: 1.5237369537353516
Batch 52/64 loss: 0.9885516166687012
Batch 53/64 loss: 1.5638980865478516
Batch 54/64 loss: 1.1494331359863281
Batch 55/64 loss: 2.459599018096924
Batch 56/64 loss: 0.9347243309020996
Batch 57/64 loss: 0.742213249206543
Batch 58/64 loss: 1.0148043632507324
Batch 59/64 loss: 1.2625422477722168
Batch 60/64 loss: 0.8560976982116699
Batch 61/64 loss: 1.289933204650879
Batch 62/64 loss: 0.9486098289489746
Batch 63/64 loss: 0.9823932647705078
Batch 64/64 loss: -2.8101959228515625
Epoch 185  Train loss: 1.298544879988128  Val loss: 1.1953489493668283
Epoch 186
-------------------------------
Batch 1/64 loss: 0.6506218910217285
Batch 2/64 loss: 1.3651008605957031
Batch 3/64 loss: 2.4392457008361816
Batch 4/64 loss: 0.7860140800476074
Batch 5/64 loss: 0.8409810066223145
Batch 6/64 loss: 0.5913386344909668
Batch 7/64 loss: 0.8389406204223633
Batch 8/64 loss: 0.7869424819946289
Batch 9/64 loss: 0.6251192092895508
Batch 10/64 loss: 3.420177936553955
Batch 11/64 loss: 1.0377602577209473
Batch 12/64 loss: 0.8956999778747559
Batch 13/64 loss: 1.0210680961608887
Batch 14/64 loss: 1.8959002494812012
Batch 15/64 loss: 1.3311877250671387
Batch 16/64 loss: 1.226334571838379
Batch 17/64 loss: 1.8353610038757324
Batch 18/64 loss: 1.0824394226074219
Batch 19/64 loss: 1.238903522491455
Batch 20/64 loss: 1.592020034790039
Batch 21/64 loss: 1.101069450378418
Batch 22/64 loss: 1.2310914993286133
Batch 23/64 loss: 1.1144475936889648
Batch 24/64 loss: 2.3413357734680176
Batch 25/64 loss: 1.0109567642211914
Batch 26/64 loss: 1.199162483215332
Batch 27/64 loss: 1.2609686851501465
Batch 28/64 loss: 0.9127449989318848
Batch 29/64 loss: 1.5919852256774902
Batch 30/64 loss: 1.174356460571289
Batch 31/64 loss: 1.209498405456543
Batch 32/64 loss: 1.0642194747924805
Batch 33/64 loss: 0.9979290962219238
Batch 34/64 loss: 1.0699400901794434
Batch 35/64 loss: 0.942387580871582
Batch 36/64 loss: 0.9501900672912598
Batch 37/64 loss: 1.0562019348144531
Batch 38/64 loss: 0.8553094863891602
Batch 39/64 loss: 1.1692919731140137
Batch 40/64 loss: 0.7066488265991211
Batch 41/64 loss: 0.9824914932250977
Batch 42/64 loss: 0.8066539764404297
Batch 43/64 loss: 1.00604248046875
Batch 44/64 loss: 0.8171067237854004
Batch 45/64 loss: 1.348010540008545
Batch 46/64 loss: 0.999692440032959
Batch 47/64 loss: 1.0435676574707031
Batch 48/64 loss: 0.9511184692382812
Batch 49/64 loss: 1.3569798469543457
Batch 50/64 loss: 1.114494800567627
Batch 51/64 loss: 1.2956628799438477
Batch 52/64 loss: 0.9438190460205078
Batch 53/64 loss: 1.1843795776367188
Batch 54/64 loss: 1.4503426551818848
Batch 55/64 loss: 0.8252873420715332
Batch 56/64 loss: 1.4989104270935059
Batch 57/64 loss: 0.7838296890258789
Batch 58/64 loss: 0.7403521537780762
Batch 59/64 loss: 1.5425548553466797
Batch 60/64 loss: 0.7540841102600098
Batch 61/64 loss: 1.2622838020324707
Batch 62/64 loss: 1.0785284042358398
Batch 63/64 loss: 1.0942049026489258
Batch 64/64 loss: -2.5454764366149902
Epoch 186  Train loss: 1.1205048486298206  Val loss: 0.9542616159235898
Epoch 187
-------------------------------
Batch 1/64 loss: 0.7310404777526855
Batch 2/64 loss: 1.1674189567565918
Batch 3/64 loss: 0.6153669357299805
Batch 4/64 loss: 0.8787684440612793
Batch 5/64 loss: 0.7073149681091309
Batch 6/64 loss: 2.0341429710388184
Batch 7/64 loss: 0.9910321235656738
Batch 8/64 loss: 0.9778704643249512
Batch 9/64 loss: 0.8874316215515137
Batch 10/64 loss: 1.0187487602233887
Batch 11/64 loss: 1.186558723449707
Batch 12/64 loss: 0.6814303398132324
Batch 13/64 loss: 0.5680298805236816
Batch 14/64 loss: 0.7808742523193359
Batch 15/64 loss: 0.5179686546325684
Batch 16/64 loss: 0.8917374610900879
Batch 17/64 loss: 0.7363815307617188
Batch 18/64 loss: 1.1093196868896484
Batch 19/64 loss: 0.5211806297302246
Batch 20/64 loss: 0.8578028678894043
Batch 21/64 loss: 0.7945327758789062
Batch 22/64 loss: 0.7542266845703125
Batch 23/64 loss: 1.0456457138061523
Batch 24/64 loss: 0.4841609001159668
Batch 25/64 loss: 0.91949462890625
Batch 26/64 loss: 0.45621204376220703
Batch 27/64 loss: 1.008002758026123
Batch 28/64 loss: 0.3870878219604492
Batch 29/64 loss: 0.634068489074707
Batch 30/64 loss: 0.5709261894226074
Batch 31/64 loss: 0.7837691307067871
Batch 32/64 loss: 0.8102078437805176
Batch 33/64 loss: 0.5345206260681152
Batch 34/64 loss: 0.7264180183410645
Batch 35/64 loss: 0.5201053619384766
Batch 36/64 loss: 1.0086164474487305
Batch 37/64 loss: 0.6224031448364258
Batch 38/64 loss: 0.7524752616882324
Batch 39/64 loss: 0.30724000930786133
Batch 40/64 loss: 0.7558126449584961
Batch 41/64 loss: 0.8955683708190918
Batch 42/64 loss: 0.578822135925293
Batch 43/64 loss: 0.7561559677124023
Batch 44/64 loss: 0.6421833038330078
Batch 45/64 loss: 0.6777729988098145
Batch 46/64 loss: 1.2066106796264648
Batch 47/64 loss: 0.5570578575134277
Batch 48/64 loss: 0.3676586151123047
Batch 49/64 loss: 0.5913281440734863
Batch 50/64 loss: 1.5030040740966797
Batch 51/64 loss: 0.8742079734802246
Batch 52/64 loss: 0.4316868782043457
Batch 53/64 loss: 0.45384931564331055
Batch 54/64 loss: 0.49806737899780273
Batch 55/64 loss: 1.0173578262329102
Batch 56/64 loss: 0.9778594970703125
Batch 57/64 loss: 0.4713144302368164
Batch 58/64 loss: 0.46943235397338867
Batch 59/64 loss: 0.6391983032226562
Batch 60/64 loss: 0.48086118698120117
Batch 61/64 loss: 0.930241584777832
Batch 62/64 loss: 0.7179450988769531
Batch 63/64 loss: 0.7157559394836426
Batch 64/64 loss: -2.5774078369140625
Epoch 187  Train loss: 0.7256036122639974  Val loss: 0.7025101651850435
Epoch 188
-------------------------------
Batch 1/64 loss: 0.8366589546203613
Batch 2/64 loss: 0.7689018249511719
Batch 3/64 loss: 0.721076488494873
Batch 4/64 loss: 0.6398487091064453
Batch 5/64 loss: 0.8437409400939941
Batch 6/64 loss: 0.3240652084350586
Batch 7/64 loss: 0.5847182273864746
Batch 8/64 loss: 0.42131471633911133
Batch 9/64 loss: 1.042558193206787
Batch 10/64 loss: 0.44973230361938477
Batch 11/64 loss: 0.9674344062805176
Batch 12/64 loss: 0.49608325958251953
Batch 13/64 loss: 0.5959038734436035
Batch 14/64 loss: 1.0851821899414062
Batch 15/64 loss: 0.27372217178344727
Batch 16/64 loss: 0.35487794876098633
Batch 17/64 loss: 0.5439510345458984
Batch 18/64 loss: 0.5088133811950684
Batch 19/64 loss: 0.6489458084106445
Batch 20/64 loss: 0.47098398208618164
Batch 21/64 loss: 0.5137381553649902
Batch 22/64 loss: 0.5559220314025879
Batch 23/64 loss: 0.5888142585754395
Batch 24/64 loss: 0.6088528633117676
Batch 25/64 loss: 0.6431441307067871
Batch 26/64 loss: 0.5236749649047852
Batch 27/64 loss: 0.7098932266235352
Batch 28/64 loss: 0.5629100799560547
Batch 29/64 loss: 0.4171895980834961
Batch 30/64 loss: 0.4443955421447754
Batch 31/64 loss: 0.8421263694763184
Batch 32/64 loss: 0.8226833343505859
Batch 33/64 loss: 0.7806930541992188
Batch 34/64 loss: 0.7928524017333984
Batch 35/64 loss: 0.39287471771240234
Batch 36/64 loss: 0.9844279289245605
Batch 37/64 loss: 0.4649362564086914
Batch 38/64 loss: 0.7479557991027832
Batch 39/64 loss: 0.6260647773742676
Batch 40/64 loss: 0.7057409286499023
Batch 41/64 loss: 0.42289066314697266
Batch 42/64 loss: 0.5461406707763672
Batch 43/64 loss: 0.9152660369873047
Batch 44/64 loss: 0.5787835121154785
Batch 45/64 loss: 0.5988650321960449
Batch 46/64 loss: 0.36945247650146484
Batch 47/64 loss: 0.24724721908569336
Batch 48/64 loss: 0.7020583152770996
Batch 49/64 loss: 0.4254741668701172
Batch 50/64 loss: 1.2902312278747559
Batch 51/64 loss: 0.5437049865722656
Batch 52/64 loss: 0.20386266708374023
Batch 53/64 loss: 0.25153589248657227
Batch 54/64 loss: 0.3943924903869629
Batch 55/64 loss: 0.6169357299804688
Batch 56/64 loss: 0.5679326057434082
Batch 57/64 loss: 0.6264157295227051
Batch 58/64 loss: 0.5783662796020508
Batch 59/64 loss: 0.6165595054626465
Batch 60/64 loss: 0.6188907623291016
Batch 61/64 loss: 0.5759053230285645
Batch 62/64 loss: 0.5117144584655762
Batch 63/64 loss: 0.9383931159973145
Batch 64/64 loss: -2.9864392280578613
Epoch 188  Train loss: 0.5679779370625814  Val loss: 0.6071419273455119
Epoch 189
-------------------------------
Batch 1/64 loss: 1.0377802848815918
Batch 2/64 loss: 0.7072234153747559
Batch 3/64 loss: 0.24323272705078125
Batch 4/64 loss: 0.3864607810974121
Batch 5/64 loss: 0.44841718673706055
Batch 6/64 loss: 0.3203706741333008
Batch 7/64 loss: 0.47408056259155273
Batch 8/64 loss: 1.386937141418457
Batch 9/64 loss: 0.21166563034057617
Batch 10/64 loss: 0.5267233848571777
Batch 11/64 loss: 0.6592674255371094
Batch 12/64 loss: 0.44223880767822266
Batch 13/64 loss: 0.6520295143127441
Batch 14/64 loss: 0.5136923789978027
Batch 15/64 loss: 0.5883350372314453
Batch 16/64 loss: 0.7656927108764648
Batch 17/64 loss: 0.557645320892334
Batch 18/64 loss: 0.7937264442443848
Batch 19/64 loss: 0.37711000442504883
Batch 20/64 loss: 0.6043696403503418
Batch 21/64 loss: 0.6940240859985352
Batch 22/64 loss: 0.5183444023132324
Batch 23/64 loss: 0.5705299377441406
Batch 24/64 loss: 0.49215030670166016
Batch 25/64 loss: 0.7679805755615234
Batch 26/64 loss: 0.31011199951171875
Batch 27/64 loss: 0.3501873016357422
Batch 28/64 loss: 0.6720066070556641
Batch 29/64 loss: 1.1341204643249512
Batch 30/64 loss: 0.7581725120544434
Batch 31/64 loss: 0.6254825592041016
Batch 32/64 loss: 0.2833428382873535
Batch 33/64 loss: 0.5286149978637695
Batch 34/64 loss: 0.4388751983642578
Batch 35/64 loss: 0.6709418296813965
Batch 36/64 loss: 0.6858901977539062
Batch 37/64 loss: 0.5784797668457031
Batch 38/64 loss: 0.36013078689575195
Batch 39/64 loss: 0.851555347442627
Batch 40/64 loss: 0.5082697868347168
Batch 41/64 loss: 0.7580475807189941
Batch 42/64 loss: 0.39152002334594727
Batch 43/64 loss: 0.8073544502258301
Batch 44/64 loss: 0.12600994110107422
Batch 45/64 loss: 0.3253297805786133
Batch 46/64 loss: 0.5180702209472656
Batch 47/64 loss: 0.33492422103881836
Batch 48/64 loss: 0.3971824645996094
Batch 49/64 loss: 0.5948829650878906
Batch 50/64 loss: 0.410184383392334
Batch 51/64 loss: 0.4705162048339844
Batch 52/64 loss: 0.053455352783203125
Batch 53/64 loss: 0.6313095092773438
Batch 54/64 loss: 0.3836703300476074
Batch 55/64 loss: 0.4900493621826172
Batch 56/64 loss: 0.44516801834106445
Batch 57/64 loss: 0.735684871673584
Batch 58/64 loss: 0.40225648880004883
Batch 59/64 loss: 0.5855026245117188
Batch 60/64 loss: 0.9177947044372559
Batch 61/64 loss: 0.7922897338867188
Batch 62/64 loss: 0.7021055221557617
Batch 63/64 loss: 0.614069938659668
Batch 64/64 loss: -3.163395404815674
Epoch 189  Train loss: 0.5178202778685327  Val loss: 0.4713330744058406
Saving best model, epoch: 189
Epoch 190
-------------------------------
Batch 1/64 loss: 0.638638973236084
Batch 2/64 loss: 0.22761058807373047
Batch 3/64 loss: 0.8090496063232422
Batch 4/64 loss: 0.8460564613342285
Batch 5/64 loss: 0.28571605682373047
Batch 6/64 loss: 0.46540403366088867
Batch 7/64 loss: 0.18237733840942383
Batch 8/64 loss: 0.7089920043945312
Batch 9/64 loss: 0.7197666168212891
Batch 10/64 loss: 0.6759228706359863
Batch 11/64 loss: 0.3815445899963379
Batch 12/64 loss: 0.7937626838684082
Batch 13/64 loss: 0.6568827629089355
Batch 14/64 loss: 0.5765738487243652
Batch 15/64 loss: 0.3088111877441406
Batch 16/64 loss: 0.4825105667114258
Batch 17/64 loss: 0.7400026321411133
Batch 18/64 loss: 0.15397310256958008
Batch 19/64 loss: 0.4673762321472168
Batch 20/64 loss: 0.4515953063964844
Batch 21/64 loss: 0.29751110076904297
Batch 22/64 loss: 0.33336305618286133
Batch 23/64 loss: 0.5366249084472656
Batch 24/64 loss: 0.25665712356567383
Batch 25/64 loss: 0.33067798614501953
Batch 26/64 loss: 0.3658628463745117
Batch 27/64 loss: 0.39014530181884766
Batch 28/64 loss: 0.8007926940917969
Batch 29/64 loss: 0.5498299598693848
Batch 30/64 loss: 0.1296100616455078
Batch 31/64 loss: 0.4388699531555176
Batch 32/64 loss: 0.9490494728088379
Batch 33/64 loss: 0.7667145729064941
Batch 34/64 loss: 0.2176661491394043
Batch 35/64 loss: 0.21887683868408203
Batch 36/64 loss: 0.4882831573486328
Batch 37/64 loss: 0.32711124420166016
Batch 38/64 loss: 0.3915591239929199
Batch 39/64 loss: 0.1898660659790039
Batch 40/64 loss: 0.2710747718811035
Batch 41/64 loss: 0.4502243995666504
Batch 42/64 loss: 0.5708074569702148
Batch 43/64 loss: 0.34659624099731445
Batch 44/64 loss: 0.8364791870117188
Batch 45/64 loss: 0.5048861503601074
Batch 46/64 loss: 0.32080650329589844
Batch 47/64 loss: 0.12447166442871094
Batch 48/64 loss: 0.3714733123779297
Batch 49/64 loss: 0.5351991653442383
Batch 50/64 loss: 1.02178955078125
Batch 51/64 loss: 0.2696647644042969
Batch 52/64 loss: 0.5119180679321289
Batch 53/64 loss: 0.5416631698608398
Batch 54/64 loss: 0.17177343368530273
Batch 55/64 loss: 0.5355653762817383
Batch 56/64 loss: 0.5679216384887695
Batch 57/64 loss: 0.6773009300231934
Batch 58/64 loss: 0.26813268661499023
Batch 59/64 loss: 0.5077862739562988
Batch 60/64 loss: 0.6493186950683594
Batch 61/64 loss: 0.4275674819946289
Batch 62/64 loss: 0.6938066482543945
Batch 63/64 loss: 0.6368637084960938
Batch 64/64 loss: -3.3365068435668945
Epoch 190  Train loss: 0.43705647412468407  Val loss: 0.48995126154004914
Epoch 191
-------------------------------
Batch 1/64 loss: 0.799896240234375
Batch 2/64 loss: 0.561316967010498
Batch 3/64 loss: 0.2561030387878418
Batch 4/64 loss: 0.5070304870605469
Batch 5/64 loss: 0.5665397644042969
Batch 6/64 loss: 0.13853120803833008
Batch 7/64 loss: 0.5336227416992188
Batch 8/64 loss: 0.28621339797973633
Batch 9/64 loss: 0.41396617889404297
Batch 10/64 loss: 0.44054317474365234
Batch 11/64 loss: 0.8977503776550293
Batch 12/64 loss: 0.25885963439941406
Batch 13/64 loss: 0.48295068740844727
Batch 14/64 loss: 0.6623902320861816
Batch 15/64 loss: 0.06372404098510742
Batch 16/64 loss: 0.4609375
Batch 17/64 loss: 0.7260031700134277
Batch 18/64 loss: 0.8952512741088867
Batch 19/64 loss: 0.5327849388122559
Batch 20/64 loss: 0.7198047637939453
Batch 21/64 loss: 0.38341856002807617
Batch 22/64 loss: 0.4272341728210449
Batch 23/64 loss: 0.4759502410888672
Batch 24/64 loss: 0.539400577545166
Batch 25/64 loss: 0.4307136535644531
Batch 26/64 loss: 0.5528531074523926
Batch 27/64 loss: 0.38010644912719727
Batch 28/64 loss: 0.5753693580627441
Batch 29/64 loss: 0.456817626953125
Batch 30/64 loss: 0.8053760528564453
Batch 31/64 loss: 0.6125483512878418
Batch 32/64 loss: 0.24034404754638672
Batch 33/64 loss: 0.13705205917358398
Batch 34/64 loss: 0.6752729415893555
Batch 35/64 loss: 0.29248046875
Batch 36/64 loss: 0.5524659156799316
Batch 37/64 loss: 0.28887939453125
Batch 38/64 loss: 0.22210693359375
Batch 39/64 loss: 0.3160839080810547
Batch 40/64 loss: 0.4635639190673828
Batch 41/64 loss: 0.3729820251464844
Batch 42/64 loss: 0.5691890716552734
Batch 43/64 loss: 0.26705312728881836
Batch 44/64 loss: 0.4421672821044922
Batch 45/64 loss: 0.1768951416015625
Batch 46/64 loss: 0.22980451583862305
Batch 47/64 loss: 0.44684457778930664
Batch 48/64 loss: 0.7592535018920898
Batch 49/64 loss: 0.8423070907592773
Batch 50/64 loss: 0.38422203063964844
Batch 51/64 loss: 0.478240966796875
Batch 52/64 loss: 0.6226983070373535
Batch 53/64 loss: 0.7233490943908691
Batch 54/64 loss: 0.39383792877197266
Batch 55/64 loss: 0.4338850975036621
Batch 56/64 loss: 0.31543922424316406
Batch 57/64 loss: 0.41878271102905273
Batch 58/64 loss: 0.34316396713256836
Batch 59/64 loss: 0.617042064666748
Batch 60/64 loss: 0.2760605812072754
Batch 61/64 loss: 0.3388199806213379
Batch 62/64 loss: 0.142181396484375
Batch 63/64 loss: 0.14774227142333984
Batch 64/64 loss: -3.239858627319336
Epoch 191  Train loss: 0.413244322234509  Val loss: 0.5930000711552466
Epoch 192
-------------------------------
Batch 1/64 loss: 0.20534706115722656
Batch 2/64 loss: 0.401885986328125
Batch 3/64 loss: 0.3729572296142578
Batch 4/64 loss: 0.11432600021362305
Batch 5/64 loss: 0.10465621948242188
Batch 6/64 loss: 0.19911909103393555
Batch 7/64 loss: 0.12186002731323242
Batch 8/64 loss: 0.6441259384155273
Batch 9/64 loss: 0.3628411293029785
Batch 10/64 loss: 0.3490309715270996
Batch 11/64 loss: 0.09291458129882812
Batch 12/64 loss: 0.2660956382751465
Batch 13/64 loss: 0.48949718475341797
Batch 14/64 loss: 0.29548120498657227
Batch 15/64 loss: 1.0413079261779785
Batch 16/64 loss: 1.1027531623840332
Batch 17/64 loss: 0.3440432548522949
Batch 18/64 loss: 0.6963005065917969
Batch 19/64 loss: 0.4273710250854492
Batch 20/64 loss: 0.08276653289794922
Batch 21/64 loss: 0.22215795516967773
Batch 22/64 loss: 0.6272792816162109
Batch 23/64 loss: 0.316561222076416
Batch 24/64 loss: 0.6680340766906738
Batch 25/64 loss: 0.517643928527832
Batch 26/64 loss: 0.43524932861328125
Batch 27/64 loss: 0.27521228790283203
Batch 28/64 loss: 0.384124755859375
Batch 29/64 loss: 0.5276193618774414
Batch 30/64 loss: 0.5718035697937012
Batch 31/64 loss: 0.23740291595458984
Batch 32/64 loss: 0.47089147567749023
Batch 33/64 loss: 0.4465827941894531
Batch 34/64 loss: 0.5165438652038574
Batch 35/64 loss: 0.6467099189758301
Batch 36/64 loss: 0.2824563980102539
Batch 37/64 loss: 0.47202587127685547
Batch 38/64 loss: 0.33064746856689453
Batch 39/64 loss: 0.5091872215270996
Batch 40/64 loss: 0.8923873901367188
Batch 41/64 loss: 0.34583044052124023
Batch 42/64 loss: 0.5258193016052246
Batch 43/64 loss: 0.4251570701599121
Batch 44/64 loss: 0.2981853485107422
Batch 45/64 loss: 0.25899839401245117
Batch 46/64 loss: 0.615969181060791
Batch 47/64 loss: 0.4913201332092285
Batch 48/64 loss: 0.42852354049682617
Batch 49/64 loss: 0.41643428802490234
Batch 50/64 loss: 0.4135723114013672
Batch 51/64 loss: 0.5309181213378906
Batch 52/64 loss: 0.6429228782653809
Batch 53/64 loss: 0.17728042602539062
Batch 54/64 loss: 0.5645103454589844
Batch 55/64 loss: 0.24598407745361328
Batch 56/64 loss: 0.4873385429382324
Batch 57/64 loss: 0.35822534561157227
Batch 58/64 loss: 0.4621114730834961
Batch 59/64 loss: 0.2675189971923828
Batch 60/64 loss: 0.41428375244140625
Batch 61/64 loss: 0.5402741432189941
Batch 62/64 loss: 0.44981813430786133
Batch 63/64 loss: 0.4167900085449219
Batch 64/64 loss: -3.2133307456970215
Epoch 192  Train loss: 0.38326258752860276  Val loss: 0.5835319204428762
Epoch 193
-------------------------------
Batch 1/64 loss: 0.3100395202636719
Batch 2/64 loss: 0.39942073822021484
Batch 3/64 loss: 0.6547632217407227
Batch 4/64 loss: 0.28669166564941406
Batch 5/64 loss: 0.25437307357788086
Batch 6/64 loss: 0.4152555465698242
Batch 7/64 loss: 0.1092233657836914
Batch 8/64 loss: 0.1451873779296875
Batch 9/64 loss: 0.508882999420166
Batch 10/64 loss: 0.48770999908447266
Batch 11/64 loss: 0.24570369720458984
Batch 12/64 loss: 0.2357187271118164
Batch 13/64 loss: 0.2002553939819336
Batch 14/64 loss: -0.0025467872619628906
Batch 15/64 loss: 0.28570032119750977
Batch 16/64 loss: 0.48978328704833984
Batch 17/64 loss: 0.4971785545349121
Batch 18/64 loss: 0.4538111686706543
Batch 19/64 loss: 0.5000414848327637
Batch 20/64 loss: 0.3583540916442871
Batch 21/64 loss: 0.446075439453125
Batch 22/64 loss: 0.3503837585449219
Batch 23/64 loss: 0.5957937240600586
Batch 24/64 loss: 0.36211442947387695
Batch 25/64 loss: 0.4342665672302246
Batch 26/64 loss: 0.3839268684387207
Batch 27/64 loss: 0.41554975509643555
Batch 28/64 loss: 0.5746736526489258
Batch 29/64 loss: 0.3847661018371582
Batch 30/64 loss: 0.2606687545776367
Batch 31/64 loss: 1.2135019302368164
Batch 32/64 loss: 0.32433366775512695
Batch 33/64 loss: 0.21952247619628906
Batch 34/64 loss: 0.6575493812561035
Batch 35/64 loss: 0.804316520690918
Batch 36/64 loss: 0.23473024368286133
Batch 37/64 loss: 0.5842118263244629
Batch 38/64 loss: 0.39373016357421875
Batch 39/64 loss: 0.49085378646850586
Batch 40/64 loss: 0.9130363464355469
Batch 41/64 loss: 0.584475040435791
Batch 42/64 loss: 0.5827732086181641
Batch 43/64 loss: 0.2632465362548828
Batch 44/64 loss: 0.40911293029785156
Batch 45/64 loss: 0.19452714920043945
Batch 46/64 loss: 0.2501678466796875
Batch 47/64 loss: 0.48478031158447266
Batch 48/64 loss: 0.28134918212890625
Batch 49/64 loss: 0.346285343170166
Batch 50/64 loss: 0.6809592247009277
Batch 51/64 loss: 0.45160913467407227
Batch 52/64 loss: 0.6597599983215332
Batch 53/64 loss: 0.425992488861084
Batch 54/64 loss: 0.30168676376342773
Batch 55/64 loss: 0.49094486236572266
Batch 56/64 loss: 0.3271183967590332
Batch 57/64 loss: 0.29413890838623047
Batch 58/64 loss: 0.34241247177124023
Batch 59/64 loss: 0.281857967376709
Batch 60/64 loss: 0.2400193214416504
Batch 61/64 loss: 0.26935577392578125
Batch 62/64 loss: 0.5128178596496582
Batch 63/64 loss: 0.3042573928833008
Batch 64/64 loss: -3.0760765075683594
Epoch 193  Train loss: 0.3694453893923292  Val loss: 0.3916071993378839
Saving best model, epoch: 193
Epoch 194
-------------------------------
Batch 1/64 loss: 0.4446883201599121
Batch 2/64 loss: 0.09578514099121094
Batch 3/64 loss: 0.013465404510498047
Batch 4/64 loss: 0.3174552917480469
Batch 5/64 loss: 0.5250859260559082
Batch 6/64 loss: -0.026242733001708984
Batch 7/64 loss: -0.07504081726074219
Batch 8/64 loss: 0.2886238098144531
Batch 9/64 loss: 0.21300220489501953
Batch 10/64 loss: 0.5969734191894531
Batch 11/64 loss: 0.47109222412109375
Batch 12/64 loss: 0.35608911514282227
Batch 13/64 loss: 0.28677797317504883
Batch 14/64 loss: 0.33511924743652344
Batch 15/64 loss: 0.015325546264648438
Batch 16/64 loss: 0.28322792053222656
Batch 17/64 loss: 0.31691646575927734
Batch 18/64 loss: 0.5639381408691406
Batch 19/64 loss: 0.5206475257873535
Batch 20/64 loss: 0.2388162612915039
Batch 21/64 loss: 0.20919370651245117
Batch 22/64 loss: 0.6936650276184082
Batch 23/64 loss: 0.7863879203796387
Batch 24/64 loss: 0.5553693771362305
Batch 25/64 loss: 0.576268196105957
Batch 26/64 loss: 0.4159722328186035
Batch 27/64 loss: 0.5389413833618164
Batch 28/64 loss: 0.503657341003418
Batch 29/64 loss: 0.3610539436340332
Batch 30/64 loss: 0.2892723083496094
Batch 31/64 loss: 0.3757305145263672
Batch 32/64 loss: 0.42673349380493164
Batch 33/64 loss: 0.919651985168457
Batch 34/64 loss: 0.5879225730895996
Batch 35/64 loss: 0.785825252532959
Batch 36/64 loss: 0.3320584297180176
Batch 37/64 loss: 0.30579662322998047
Batch 38/64 loss: 0.504817008972168
Batch 39/64 loss: 0.2465801239013672
Batch 40/64 loss: 0.3321723937988281
Batch 41/64 loss: 0.6863875389099121
Batch 42/64 loss: 0.47881603240966797
Batch 43/64 loss: 0.3905372619628906
Batch 44/64 loss: 0.15356779098510742
Batch 45/64 loss: 0.14772987365722656
Batch 46/64 loss: 0.38916683197021484
Batch 47/64 loss: 0.4158482551574707
Batch 48/64 loss: 0.6600785255432129
Batch 49/64 loss: 0.5151386260986328
Batch 50/64 loss: 0.5998659133911133
Batch 51/64 loss: 0.40042924880981445
Batch 52/64 loss: 0.6039042472839355
Batch 53/64 loss: 0.24957561492919922
Batch 54/64 loss: 0.31269359588623047
Batch 55/64 loss: 0.5319452285766602
Batch 56/64 loss: 0.6834192276000977
Batch 57/64 loss: 0.2162036895751953
Batch 58/64 loss: 0.3105025291442871
Batch 59/64 loss: 0.17269325256347656
Batch 60/64 loss: 0.5266318321228027
Batch 61/64 loss: 0.18903064727783203
Batch 62/64 loss: 0.21535634994506836
Batch 63/64 loss: 0.22299432754516602
Batch 64/64 loss: -3.316997528076172
Epoch 194  Train loss: 0.3468797571518842  Val loss: 0.4144985618460219
Epoch 195
-------------------------------
Batch 1/64 loss: 0.39278697967529297
Batch 2/64 loss: 0.4042496681213379
Batch 3/64 loss: 0.3722958564758301
Batch 4/64 loss: 0.5663275718688965
Batch 5/64 loss: 0.37249755859375
Batch 6/64 loss: 0.3083229064941406
Batch 7/64 loss: 0.3737030029296875
Batch 8/64 loss: 0.11957120895385742
Batch 9/64 loss: 0.26830101013183594
Batch 10/64 loss: 0.37511348724365234
Batch 11/64 loss: 0.34035778045654297
Batch 12/64 loss: 0.0962381362915039
Batch 13/64 loss: 0.35822391510009766
Batch 14/64 loss: 0.28459978103637695
Batch 15/64 loss: 0.34996604919433594
Batch 16/64 loss: 0.34192943572998047
Batch 17/64 loss: 0.23851490020751953
Batch 18/64 loss: 0.21460533142089844
Batch 19/64 loss: 0.18547677993774414
Batch 20/64 loss: 0.7710628509521484
Batch 21/64 loss: 0.37508249282836914
Batch 22/64 loss: 0.14202356338500977
Batch 23/64 loss: 0.26648807525634766
Batch 24/64 loss: 0.4994478225708008
Batch 25/64 loss: 0.08652687072753906
Batch 26/64 loss: 0.43023109436035156
Batch 27/64 loss: 0.4403047561645508
Batch 28/64 loss: 0.5428600311279297
Batch 29/64 loss: 0.12209653854370117
Batch 30/64 loss: 0.33298635482788086
Batch 31/64 loss: 0.1492624282836914
Batch 32/64 loss: 0.17128324508666992
Batch 33/64 loss: 0.2190413475036621
Batch 34/64 loss: 0.47042274475097656
Batch 35/64 loss: 0.28049135208129883
Batch 36/64 loss: 0.4121270179748535
Batch 37/64 loss: 0.34636735916137695
Batch 38/64 loss: 0.4525308609008789
Batch 39/64 loss: 1.037740707397461
Batch 40/64 loss: 0.19979572296142578
Batch 41/64 loss: 0.09947872161865234
Batch 42/64 loss: 0.48564577102661133
Batch 43/64 loss: 0.2527775764465332
Batch 44/64 loss: 0.42739105224609375
Batch 45/64 loss: 0.26482343673706055
Batch 46/64 loss: 0.47602319717407227
Batch 47/64 loss: 0.28339147567749023
Batch 48/64 loss: 0.21555137634277344
Batch 49/64 loss: 0.3433036804199219
Batch 50/64 loss: 0.11005687713623047
Batch 51/64 loss: 0.6504635810852051
Batch 52/64 loss: 0.4079160690307617
Batch 53/64 loss: 0.22081613540649414
Batch 54/64 loss: 0.41724538803100586
Batch 55/64 loss: 0.25479555130004883
Batch 56/64 loss: 0.2876424789428711
Batch 57/64 loss: 0.27912092208862305
Batch 58/64 loss: 0.3812727928161621
Batch 59/64 loss: 0.8256993293762207
Batch 60/64 loss: 0.48290014266967773
Batch 61/64 loss: 0.565666675567627
Batch 62/64 loss: 0.3893723487854004
Batch 63/64 loss: 0.29317712783813477
Batch 64/64 loss: -3.2388553619384766
Epoch 195  Train loss: 0.3089669769885493  Val loss: 0.3855813409864288
Saving best model, epoch: 195
Epoch 196
-------------------------------
Batch 1/64 loss: 0.18752145767211914
Batch 2/64 loss: 0.1815052032470703
Batch 3/64 loss: 0.24562835693359375
Batch 4/64 loss: 0.3709707260131836
Batch 5/64 loss: 0.27207231521606445
Batch 6/64 loss: 0.1648097038269043
Batch 7/64 loss: 0.4047236442565918
Batch 8/64 loss: 0.540036678314209
Batch 9/64 loss: 0.467012882232666
Batch 10/64 loss: 0.5104804039001465
Batch 11/64 loss: 0.3823251724243164
Batch 12/64 loss: 0.4832124710083008
Batch 13/64 loss: -6.67572021484375e-05
Batch 14/64 loss: 0.6385326385498047
Batch 15/64 loss: 0.5031909942626953
Batch 16/64 loss: -0.04395771026611328
Batch 17/64 loss: 0.2468123435974121
Batch 18/64 loss: 0.11406612396240234
Batch 19/64 loss: 0.21821308135986328
Batch 20/64 loss: 0.3879399299621582
Batch 21/64 loss: 0.4671316146850586
Batch 22/64 loss: 0.07836723327636719
Batch 23/64 loss: 0.2854194641113281
Batch 24/64 loss: 0.3431839942932129
Batch 25/64 loss: 0.27957630157470703
Batch 26/64 loss: 0.14383935928344727
Batch 27/64 loss: 0.9916329383850098
Batch 28/64 loss: 0.6714072227478027
Batch 29/64 loss: 0.0017366409301757812
Batch 30/64 loss: 0.6291236877441406
Batch 31/64 loss: 0.19649314880371094
Batch 32/64 loss: 0.3241744041442871
Batch 33/64 loss: 0.1274266242980957
Batch 34/64 loss: 0.584869384765625
Batch 35/64 loss: 0.29560327529907227
Batch 36/64 loss: 0.22633886337280273
Batch 37/64 loss: 0.21816349029541016
Batch 38/64 loss: 0.40453529357910156
Batch 39/64 loss: 0.3229074478149414
Batch 40/64 loss: 0.44007015228271484
Batch 41/64 loss: 0.388638973236084
Batch 42/64 loss: 0.47769927978515625
Batch 43/64 loss: 0.46828174591064453
Batch 44/64 loss: 0.5491743087768555
Batch 45/64 loss: -0.03876304626464844
Batch 46/64 loss: 0.8216800689697266
Batch 47/64 loss: 0.33434295654296875
Batch 48/64 loss: -0.08956670761108398
Batch 49/64 loss: 0.25931787490844727
Batch 50/64 loss: 0.11312246322631836
Batch 51/64 loss: 0.5343751907348633
Batch 52/64 loss: 0.1539168357849121
Batch 53/64 loss: 0.2647428512573242
Batch 54/64 loss: 0.3995213508605957
Batch 55/64 loss: 0.27904558181762695
Batch 56/64 loss: -0.029909133911132812
Batch 57/64 loss: 0.4964118003845215
Batch 58/64 loss: 0.14059782028198242
Batch 59/64 loss: 0.18130826950073242
Batch 60/64 loss: 0.4980792999267578
Batch 61/64 loss: 0.2612452507019043
Batch 62/64 loss: 0.5432710647583008
Batch 63/64 loss: 0.36181163787841797
Batch 64/64 loss: -3.696730136871338
Epoch 196  Train loss: 0.2808286797766592  Val loss: 0.44542977244583604
Epoch 197
-------------------------------
Batch 1/64 loss: 0.3157048225402832
Batch 2/64 loss: 0.39458322525024414
Batch 3/64 loss: 0.6269001960754395
Batch 4/64 loss: 0.22957134246826172
Batch 5/64 loss: 0.32769775390625
Batch 6/64 loss: 0.261960506439209
Batch 7/64 loss: 0.1765289306640625
Batch 8/64 loss: 0.3572549819946289
Batch 9/64 loss: 0.3274564743041992
Batch 10/64 loss: 0.08939027786254883
Batch 11/64 loss: -0.00880575180053711
Batch 12/64 loss: 0.4259610176086426
Batch 13/64 loss: 0.33934688568115234
Batch 14/64 loss: 0.12580585479736328
Batch 15/64 loss: 0.29735517501831055
Batch 16/64 loss: 0.28240156173706055
Batch 17/64 loss: 0.45810365676879883
Batch 18/64 loss: 0.3691563606262207
Batch 19/64 loss: 0.3718147277832031
Batch 20/64 loss: 0.37253808975219727
Batch 21/64 loss: 0.16804885864257812
Batch 22/64 loss: 0.5522618293762207
Batch 23/64 loss: 0.21320104598999023
Batch 24/64 loss: 0.0950784683227539
Batch 25/64 loss: 0.4414854049682617
Batch 26/64 loss: 0.10805320739746094
Batch 27/64 loss: 0.37829065322875977
Batch 28/64 loss: 0.8212599754333496
Batch 29/64 loss: 0.15042448043823242
Batch 30/64 loss: 0.3070816993713379
Batch 31/64 loss: 0.42259836196899414
Batch 32/64 loss: 0.7097320556640625
Batch 33/64 loss: 0.08648061752319336
Batch 34/64 loss: 0.4510655403137207
Batch 35/64 loss: 0.142852783203125
Batch 36/64 loss: 0.6290578842163086
Batch 37/64 loss: 0.19853782653808594
Batch 38/64 loss: 0.2329273223876953
Batch 39/64 loss: 0.6229038238525391
Batch 40/64 loss: 0.0692892074584961
Batch 41/64 loss: 0.9454464912414551
Batch 42/64 loss: 0.7417197227478027
Batch 43/64 loss: 0.05293750762939453
Batch 44/64 loss: 0.13586950302124023
Batch 45/64 loss: 0.1442718505859375
Batch 46/64 loss: 0.09546852111816406
Batch 47/64 loss: 0.330960750579834
Batch 48/64 loss: 0.21718931198120117
Batch 49/64 loss: 0.5976104736328125
Batch 50/64 loss: 0.2362356185913086
Batch 51/64 loss: 0.2440962791442871
Batch 52/64 loss: 0.15579605102539062
Batch 53/64 loss: 0.23343610763549805
Batch 54/64 loss: 0.10288810729980469
Batch 55/64 loss: 0.5125613212585449
Batch 56/64 loss: 0.3259615898132324
Batch 57/64 loss: 0.557154655456543
Batch 58/64 loss: 0.35598278045654297
Batch 59/64 loss: 0.11681127548217773
Batch 60/64 loss: 0.26082897186279297
Batch 61/64 loss: 0.21361207962036133
Batch 62/64 loss: 0.41704750061035156
Batch 63/64 loss: 0.35005950927734375
Batch 64/64 loss: -3.590348243713379
Epoch 197  Train loss: 0.27596144208721085  Val loss: 0.5523544914534002
Epoch 198
-------------------------------
Batch 1/64 loss: 0.3416743278503418
Batch 2/64 loss: 0.29813146591186523
Batch 3/64 loss: 0.13872718811035156
Batch 4/64 loss: 0.3395724296569824
Batch 5/64 loss: -0.027782440185546875
Batch 6/64 loss: 0.6319046020507812
Batch 7/64 loss: 0.4319276809692383
Batch 8/64 loss: 0.3905673027038574
Batch 9/64 loss: 0.41512537002563477
Batch 10/64 loss: 0.4392423629760742
Batch 11/64 loss: 0.3885312080383301
Batch 12/64 loss: 0.23215436935424805
Batch 13/64 loss: 0.3629946708679199
Batch 14/64 loss: -0.0064182281494140625
Batch 15/64 loss: 0.4626617431640625
Batch 16/64 loss: 0.19059181213378906
Batch 17/64 loss: 0.8927245140075684
Batch 18/64 loss: 0.10713005065917969
Batch 19/64 loss: 0.15666580200195312
Batch 20/64 loss: 0.6104674339294434
Batch 21/64 loss: 0.2149066925048828
Batch 22/64 loss: -0.1571650505065918
Batch 23/64 loss: 0.351381778717041
Batch 24/64 loss: 0.7085065841674805
Batch 25/64 loss: 0.26212310791015625
Batch 26/64 loss: 0.12018442153930664
Batch 27/64 loss: 0.5903377532958984
Batch 28/64 loss: 0.2734651565551758
Batch 29/64 loss: 0.16129493713378906
Batch 30/64 loss: 0.21753883361816406
Batch 31/64 loss: 0.2081456184387207
Batch 32/64 loss: 0.8356161117553711
Batch 33/64 loss: 0.5360832214355469
Batch 34/64 loss: 0.3834114074707031
Batch 35/64 loss: 0.21513700485229492
Batch 36/64 loss: 0.5887298583984375
Batch 37/64 loss: 0.23490333557128906
Batch 38/64 loss: 0.14812994003295898
Batch 39/64 loss: 0.3808455467224121
Batch 40/64 loss: 0.21544933319091797
Batch 41/64 loss: 0.5458269119262695
Batch 42/64 loss: 0.2063431739807129
Batch 43/64 loss: -0.09705829620361328
Batch 44/64 loss: 0.3271055221557617
Batch 45/64 loss: 0.18413114547729492
Batch 46/64 loss: 0.5605378150939941
Batch 47/64 loss: 0.557863712310791
Batch 48/64 loss: 0.5824522972106934
Batch 49/64 loss: 0.4387092590332031
Batch 50/64 loss: 0.57940673828125
Batch 51/64 loss: 0.3214864730834961
Batch 52/64 loss: 0.30541229248046875
Batch 53/64 loss: 0.45266056060791016
Batch 54/64 loss: 0.32375669479370117
Batch 55/64 loss: 0.45418262481689453
Batch 56/64 loss: 0.4490671157836914
Batch 57/64 loss: 0.26430559158325195
Batch 58/64 loss: 0.17876482009887695
Batch 59/64 loss: 0.10116910934448242
Batch 60/64 loss: 0.38843393325805664
Batch 61/64 loss: 0.6022443771362305
Batch 62/64 loss: 0.547370433807373
Batch 63/64 loss: 0.3530397415161133
Batch 64/64 loss: -3.2487049102783203
Epoch 198  Train loss: 0.3055106293921377  Val loss: 0.8125680615402169
Epoch 199
-------------------------------
Batch 1/64 loss: 0.8219594955444336
Batch 2/64 loss: 0.612694263458252
Batch 3/64 loss: 0.48433399200439453
Batch 4/64 loss: 0.30092668533325195
Batch 5/64 loss: 0.3536043167114258
Batch 6/64 loss: 0.8612728118896484
Batch 7/64 loss: 0.36489200592041016
Batch 8/64 loss: 0.4645400047302246
Batch 9/64 loss: 0.6584272384643555
Batch 10/64 loss: 0.24611186981201172
Batch 11/64 loss: 0.5467844009399414
Batch 12/64 loss: 0.34616756439208984
Batch 13/64 loss: 0.480252742767334
Batch 14/64 loss: 0.37150144577026367
Batch 15/64 loss: 0.7790365219116211
Batch 16/64 loss: 0.47841882705688477
Batch 17/64 loss: 0.17798089981079102
Batch 18/64 loss: 0.5633964538574219
Batch 19/64 loss: 0.5321564674377441
Batch 20/64 loss: 0.5401706695556641
Batch 21/64 loss: 1.2451896667480469
Batch 22/64 loss: 0.2336416244506836
Batch 23/64 loss: 0.2638707160949707
Batch 24/64 loss: 0.4526686668395996
Batch 25/64 loss: 0.49123096466064453
Batch 26/64 loss: 0.4090852737426758
Batch 27/64 loss: 0.40667104721069336
Batch 28/64 loss: 0.05854034423828125
Batch 29/64 loss: 0.5227022171020508
Batch 30/64 loss: 0.2912602424621582
Batch 31/64 loss: 0.3380722999572754
Batch 32/64 loss: 0.17999744415283203
Batch 33/64 loss: 0.7787995338439941
Batch 34/64 loss: 1.0502290725708008
Batch 35/64 loss: 0.2103419303894043
Batch 36/64 loss: 0.2827873229980469
Batch 37/64 loss: 0.4422950744628906
Batch 38/64 loss: 0.30130672454833984
Batch 39/64 loss: 1.0378532409667969
Batch 40/64 loss: 0.7434029579162598
Batch 41/64 loss: 0.0884552001953125
Batch 42/64 loss: 0.44342565536499023
Batch 43/64 loss: 0.30164432525634766
Batch 44/64 loss: 0.8281426429748535
Batch 45/64 loss: 0.4622039794921875
Batch 46/64 loss: 0.35967397689819336
Batch 47/64 loss: 0.5124917030334473
Batch 48/64 loss: 0.6373162269592285
Batch 49/64 loss: 0.15024518966674805
Batch 50/64 loss: 0.30487680435180664
Batch 51/64 loss: 0.33915138244628906
Batch 52/64 loss: 0.24574756622314453
Batch 53/64 loss: 0.03280973434448242
Batch 54/64 loss: 0.2477726936340332
Batch 55/64 loss: 0.333709716796875
Batch 56/64 loss: 0.10107183456420898
Batch 57/64 loss: 0.6105098724365234
Batch 58/64 loss: 0.3147401809692383
Batch 59/64 loss: 0.44408226013183594
Batch 60/64 loss: 0.6891641616821289
Batch 61/64 loss: 0.12962579727172852
Batch 62/64 loss: 0.540036678314209
Batch 63/64 loss: 0.4929986000061035
Batch 64/64 loss: -3.390267848968506
Epoch 199  Train loss: 0.40457678963156307  Val loss: 0.7285641935682788
Epoch 200
-------------------------------
Batch 1/64 loss: 0.31683874130249023
Batch 2/64 loss: 0.5966043472290039
Batch 3/64 loss: 0.8060860633850098
Batch 4/64 loss: 0.343686580657959
Batch 5/64 loss: 0.9236550331115723
Batch 6/64 loss: 0.510282039642334
Batch 7/64 loss: 0.4616270065307617
Batch 8/64 loss: 0.7686252593994141
Batch 9/64 loss: 0.40096521377563477
Batch 10/64 loss: 0.7680754661560059
Batch 11/64 loss: 0.8279595375061035
Batch 12/64 loss: 0.30033016204833984
Batch 13/64 loss: 0.27391576766967773
Batch 14/64 loss: 1.033592700958252
Batch 15/64 loss: 0.3623647689819336
Batch 16/64 loss: 0.1532726287841797
Batch 17/64 loss: 0.12206697463989258
Batch 18/64 loss: 1.1822853088378906
Batch 19/64 loss: 0.21112489700317383
Batch 20/64 loss: 0.7136473655700684
Batch 21/64 loss: 0.2663583755493164
Batch 22/64 loss: 0.6859769821166992
Batch 23/64 loss: 0.25555849075317383
Batch 24/64 loss: 0.36722898483276367
Batch 25/64 loss: 0.6095781326293945
Batch 26/64 loss: 0.46985864639282227
Batch 27/64 loss: 0.6239113807678223
Batch 28/64 loss: 0.7411098480224609
Batch 29/64 loss: 0.9529995918273926
Batch 30/64 loss: 0.4616098403930664
Batch 31/64 loss: 0.6017813682556152
Batch 32/64 loss: 0.3270721435546875
Batch 33/64 loss: 0.24663257598876953
Batch 34/64 loss: 0.591285228729248
Batch 35/64 loss: 0.657315731048584
Batch 36/64 loss: 0.32858705520629883
Batch 37/64 loss: 1.2291340827941895
Batch 38/64 loss: 0.5695581436157227
Batch 39/64 loss: 0.3799095153808594
Batch 40/64 loss: 0.5058379173278809
Batch 41/64 loss: 0.484957218170166
Batch 42/64 loss: 0.9135603904724121
Batch 43/64 loss: 0.5418148040771484
Batch 44/64 loss: 1.0274548530578613
Batch 45/64 loss: 0.6567039489746094
Batch 46/64 loss: 0.5679507255554199
Batch 47/64 loss: 0.5719852447509766
Batch 48/64 loss: 0.5992417335510254
Batch 49/64 loss: 0.6913547515869141
Batch 50/64 loss: 0.5059871673583984
Batch 51/64 loss: 0.9143271446228027
Batch 52/64 loss: 0.519993782043457
Batch 53/64 loss: 0.3324136734008789
Batch 54/64 loss: 0.6340775489807129
Batch 55/64 loss: 0.42550086975097656
Batch 56/64 loss: 0.647160530090332
Batch 57/64 loss: 0.8851861953735352
Batch 58/64 loss: 1.074906349182129
Batch 59/64 loss: 0.5129666328430176
Batch 60/64 loss: 0.6664228439331055
Batch 61/64 loss: 0.8185381889343262
Batch 62/64 loss: 0.37090206146240234
Batch 63/64 loss: 0.1628265380859375
Batch 64/64 loss: -3.031770706176758
Epoch 200  Train loss: 0.5364818049412148  Val loss: 0.7063507591326212
Epoch 201
-------------------------------
Batch 1/64 loss: 0.4092240333557129
Batch 2/64 loss: 0.6219696998596191
Batch 3/64 loss: 0.503875732421875
Batch 4/64 loss: 0.1832127571105957
Batch 5/64 loss: 0.7727756500244141
Batch 6/64 loss: 0.2625889778137207
Batch 7/64 loss: 0.28470945358276367
Batch 8/64 loss: 0.3990507125854492
Batch 9/64 loss: 0.2516040802001953
Batch 10/64 loss: 0.21488285064697266
Batch 11/64 loss: 0.6250820159912109
Batch 12/64 loss: 0.7189064025878906
Batch 13/64 loss: 1.0645642280578613
Batch 14/64 loss: 0.7454776763916016
Batch 15/64 loss: 0.15410661697387695
Batch 16/64 loss: 1.2454285621643066
Batch 17/64 loss: 0.4018211364746094
Batch 18/64 loss: 0.8561139106750488
Batch 19/64 loss: 0.6968193054199219
Batch 20/64 loss: 0.6232004165649414
Batch 21/64 loss: 0.49991846084594727
Batch 22/64 loss: 0.967984676361084
Batch 23/64 loss: 0.7566919326782227
Batch 24/64 loss: 0.3778667449951172
Batch 25/64 loss: 0.40287065505981445
Batch 26/64 loss: 0.4829559326171875
Batch 27/64 loss: 0.30388307571411133
Batch 28/64 loss: 0.614098072052002
Batch 29/64 loss: 0.2154674530029297
Batch 30/64 loss: 0.9126729965209961
Batch 31/64 loss: 0.25919008255004883
Batch 32/64 loss: 0.3045773506164551
Batch 33/64 loss: -0.008721351623535156
Batch 34/64 loss: 0.5587897300720215
Batch 35/64 loss: 0.48206663131713867
Batch 36/64 loss: 0.22620677947998047
Batch 37/64 loss: 0.969456672668457
Batch 38/64 loss: 0.6104393005371094
Batch 39/64 loss: 0.5619587898254395
Batch 40/64 loss: 0.3240976333618164
Batch 41/64 loss: 0.6355085372924805
Batch 42/64 loss: 0.33599853515625
Batch 43/64 loss: 0.2496495246887207
Batch 44/64 loss: 0.7653665542602539
Batch 45/64 loss: 0.2858409881591797
Batch 46/64 loss: 0.7111563682556152
Batch 47/64 loss: 0.9559464454650879
Batch 48/64 loss: 1.1627931594848633
Batch 49/64 loss: 0.34213924407958984
Batch 50/64 loss: 1.5294976234436035
Batch 51/64 loss: 1.0476593971252441
Batch 52/64 loss: 0.8686132431030273
Batch 53/64 loss: 0.7995133399963379
Batch 54/64 loss: 0.5197272300720215
Batch 55/64 loss: 0.15924644470214844
Batch 56/64 loss: 0.5321707725524902
Batch 57/64 loss: 0.34727907180786133
Batch 58/64 loss: 0.13013410568237305
Batch 59/64 loss: 0.6343626976013184
Batch 60/64 loss: 0.1045079231262207
Batch 61/64 loss: 0.6406388282775879
Batch 62/64 loss: 0.6423296928405762
Batch 63/64 loss: 0.17125701904296875
Batch 64/64 loss: -2.9811654090881348
Epoch 201  Train loss: 0.5049937023836024  Val loss: 0.6117157362580709
Epoch 202
-------------------------------
Batch 1/64 loss: 0.7617425918579102
Batch 2/64 loss: 0.5865621566772461
Batch 3/64 loss: 0.38362693786621094
Batch 4/64 loss: 0.6012649536132812
Batch 5/64 loss: 0.5423331260681152
Batch 6/64 loss: 0.3031649589538574
Batch 7/64 loss: 0.3104424476623535
Batch 8/64 loss: 0.7871432304382324
Batch 9/64 loss: 0.427675724029541
Batch 10/64 loss: 0.27061939239501953
Batch 11/64 loss: 0.0345611572265625
Batch 12/64 loss: 0.37358665466308594
Batch 13/64 loss: 0.27940797805786133
Batch 14/64 loss: 0.4503469467163086
Batch 15/64 loss: 0.4512777328491211
Batch 16/64 loss: 0.6760144233703613
Batch 17/64 loss: 0.2702212333679199
Batch 18/64 loss: 0.9856986999511719
Batch 19/64 loss: 0.6282658576965332
Batch 20/64 loss: 0.3229832649230957
Batch 21/64 loss: 0.6049590110778809
Batch 22/64 loss: 0.46560049057006836
Batch 23/64 loss: 0.17440557479858398
Batch 24/64 loss: 0.5222034454345703
Batch 25/64 loss: 0.07496786117553711
Batch 26/64 loss: 0.5153441429138184
Batch 27/64 loss: 0.353391170501709
Batch 28/64 loss: 0.717597484588623
Batch 29/64 loss: 0.5603265762329102
Batch 30/64 loss: 0.2724738121032715
Batch 31/64 loss: 0.32242250442504883
Batch 32/64 loss: 0.3940095901489258
Batch 33/64 loss: 0.35896730422973633
Batch 34/64 loss: 0.43144702911376953
Batch 35/64 loss: 1.0217680931091309
Batch 36/64 loss: 0.36367321014404297
Batch 37/64 loss: 0.13875198364257812
Batch 38/64 loss: 0.3791189193725586
Batch 39/64 loss: 0.4618673324584961
Batch 40/64 loss: 0.30023765563964844
Batch 41/64 loss: 0.12146854400634766
Batch 42/64 loss: 0.39400196075439453
Batch 43/64 loss: 0.6734719276428223
Batch 44/64 loss: 0.32829761505126953
Batch 45/64 loss: 0.4014153480529785
Batch 46/64 loss: 0.2885012626647949
Batch 47/64 loss: 0.49809694290161133
Batch 48/64 loss: 0.11520671844482422
Batch 49/64 loss: 0.44861412048339844
Batch 50/64 loss: 0.18653011322021484
Batch 51/64 loss: 0.6918907165527344
Batch 52/64 loss: 0.1052560806274414
Batch 53/64 loss: 0.5168023109436035
Batch 54/64 loss: 0.7218427658081055
Batch 55/64 loss: 0.7351317405700684
Batch 56/64 loss: 0.34957456588745117
Batch 57/64 loss: 0.5446410179138184
Batch 58/64 loss: 0.49951982498168945
Batch 59/64 loss: 0.09242010116577148
Batch 60/64 loss: 0.4180622100830078
Batch 61/64 loss: 0.2415637969970703
Batch 62/64 loss: 0.4630403518676758
Batch 63/64 loss: 0.5396270751953125
Batch 64/64 loss: -2.758864402770996
Epoch 202  Train loss: 0.39507923874200557  Val loss: 0.47813063552699137
Epoch 203
-------------------------------
Batch 1/64 loss: 0.35955095291137695
Batch 2/64 loss: 0.31983041763305664
Batch 3/64 loss: 0.3118748664855957
Batch 4/64 loss: 0.15225553512573242
Batch 5/64 loss: 0.28264570236206055
Batch 6/64 loss: 0.28184938430786133
Batch 7/64 loss: 0.49703025817871094
Batch 8/64 loss: 0.564582347869873
Batch 9/64 loss: 0.2725491523742676
Batch 10/64 loss: 0.13437986373901367
Batch 11/64 loss: 0.6571321487426758
Batch 12/64 loss: 0.5770459175109863
Batch 13/64 loss: 0.33358049392700195
Batch 14/64 loss: 0.29962921142578125
Batch 15/64 loss: 0.41867971420288086
Batch 16/64 loss: 0.4499664306640625
Batch 17/64 loss: 0.4676690101623535
Batch 18/64 loss: 0.6095485687255859
Batch 19/64 loss: 0.3970632553100586
Batch 20/64 loss: 0.47779273986816406
Batch 21/64 loss: 0.5896573066711426
Batch 22/64 loss: 0.3076896667480469
Batch 23/64 loss: 0.3316841125488281
Batch 24/64 loss: 0.4347810745239258
Batch 25/64 loss: -0.035887718200683594
Batch 26/64 loss: 0.17578697204589844
Batch 27/64 loss: 0.016294479370117188
Batch 28/64 loss: 0.06931257247924805
Batch 29/64 loss: 0.1676921844482422
Batch 30/64 loss: 0.21040916442871094
Batch 31/64 loss: 0.14016056060791016
Batch 32/64 loss: 0.3663516044616699
Batch 33/64 loss: 0.30768918991088867
Batch 34/64 loss: 0.1483464241027832
Batch 35/64 loss: 0.07867431640625
Batch 36/64 loss: 0.5304059982299805
Batch 37/64 loss: 0.3813962936401367
Batch 38/64 loss: 0.21160221099853516
Batch 39/64 loss: 0.7580914497375488
Batch 40/64 loss: 0.6699771881103516
Batch 41/64 loss: 0.26165056228637695
Batch 42/64 loss: 0.48018741607666016
Batch 43/64 loss: 0.434751033782959
Batch 44/64 loss: 0.43193817138671875
Batch 45/64 loss: 0.5679335594177246
Batch 46/64 loss: 0.6835517883300781
Batch 47/64 loss: 0.7103657722473145
Batch 48/64 loss: 0.34714555740356445
Batch 49/64 loss: 0.26101207733154297
Batch 50/64 loss: 0.4363064765930176
Batch 51/64 loss: 0.13938283920288086
Batch 52/64 loss: 0.25417518615722656
Batch 53/64 loss: 0.4455256462097168
Batch 54/64 loss: 0.470644474029541
Batch 55/64 loss: 0.3866276741027832
Batch 56/64 loss: 0.5080604553222656
Batch 57/64 loss: 0.2544069290161133
Batch 58/64 loss: 0.2543501853942871
Batch 59/64 loss: 0.1943645477294922
Batch 60/64 loss: 0.3872523307800293
Batch 61/64 loss: 0.2470569610595703
Batch 62/64 loss: 0.40385866165161133
Batch 63/64 loss: 0.14771699905395508
Batch 64/64 loss: -3.3347811698913574
Epoch 203  Train loss: 0.3126580462736242  Val loss: 0.801018023408975
Epoch 204
-------------------------------
Batch 1/64 loss: 0.5707244873046875
Batch 2/64 loss: 0.31967782974243164
Batch 3/64 loss: 0.038988590240478516
Batch 4/64 loss: 0.9875874519348145
Batch 5/64 loss: 0.20863962173461914
Batch 6/64 loss: 0.48594141006469727
Batch 7/64 loss: 0.23337507247924805
Batch 8/64 loss: 0.05974626541137695
Batch 9/64 loss: 0.06321334838867188
Batch 10/64 loss: 0.07636547088623047
Batch 11/64 loss: 0.23814630508422852
Batch 12/64 loss: 0.11110734939575195
Batch 13/64 loss: 0.1404423713684082
Batch 14/64 loss: 0.1621856689453125
Batch 15/64 loss: 0.2888164520263672
Batch 16/64 loss: 0.6093249320983887
Batch 17/64 loss: 0.35395145416259766
Batch 18/64 loss: 0.16158580780029297
Batch 19/64 loss: 0.48096513748168945
Batch 20/64 loss: 0.08771133422851562
Batch 21/64 loss: 0.5701541900634766
Batch 22/64 loss: 0.47989940643310547
Batch 23/64 loss: 0.3594679832458496
Batch 24/64 loss: 0.31172800064086914
Batch 25/64 loss: 0.3254876136779785
Batch 26/64 loss: 0.5149345397949219
Batch 27/64 loss: 0.4339165687561035
Batch 28/64 loss: 0.7390522956848145
Batch 29/64 loss: 0.21978044509887695
Batch 30/64 loss: 0.20240449905395508
Batch 31/64 loss: 1.4857406616210938
Batch 32/64 loss: 0.5153317451477051
Batch 33/64 loss: 0.5784077644348145
Batch 34/64 loss: 0.40239715576171875
Batch 35/64 loss: 0.2948031425476074
Batch 36/64 loss: 0.43290185928344727
Batch 37/64 loss: 0.3980278968811035
Batch 38/64 loss: 0.38996076583862305
Batch 39/64 loss: 0.6267142295837402
Batch 40/64 loss: 0.45868492126464844
Batch 41/64 loss: 0.2714667320251465
Batch 42/64 loss: 0.17789602279663086
Batch 43/64 loss: 0.5141358375549316
Batch 44/64 loss: 0.8936891555786133
Batch 45/64 loss: 0.023858070373535156
Batch 46/64 loss: 0.48529052734375
Batch 47/64 loss: 0.7335138320922852
Batch 48/64 loss: 0.5559229850769043
Batch 49/64 loss: 0.901801586151123
Batch 50/64 loss: 0.3553800582885742
Batch 51/64 loss: 0.5070724487304688
Batch 52/64 loss: 0.24838972091674805
Batch 53/64 loss: 0.367462158203125
Batch 54/64 loss: 0.4070901870727539
Batch 55/64 loss: 0.9784035682678223
Batch 56/64 loss: 0.04215669631958008
Batch 57/64 loss: 0.5427169799804688
Batch 58/64 loss: 0.6288232803344727
Batch 59/64 loss: 0.5420184135437012
Batch 60/64 loss: 0.8070659637451172
Batch 61/64 loss: 0.1559009552001953
Batch 62/64 loss: 0.5058102607727051
Batch 63/64 loss: 0.4469771385192871
Batch 64/64 loss: -3.2052950859069824
Epoch 204  Train loss: 0.3781515813341328  Val loss: 0.655723244054211
Epoch 205
-------------------------------
Batch 1/64 loss: 0.5757722854614258
Batch 2/64 loss: 0.2593669891357422
Batch 3/64 loss: -0.006109714508056641
Batch 4/64 loss: 0.15360498428344727
Batch 5/64 loss: 0.13391971588134766
Batch 6/64 loss: 0.5460968017578125
Batch 7/64 loss: 0.44884634017944336
Batch 8/64 loss: 0.42498111724853516
Batch 9/64 loss: 0.5333085060119629
Batch 10/64 loss: 0.5738692283630371
Batch 11/64 loss: 1.8304262161254883
Batch 12/64 loss: 0.3501267433166504
Batch 13/64 loss: 0.5971355438232422
Batch 14/64 loss: 0.36531925201416016
Batch 15/64 loss: 0.6636734008789062
Batch 16/64 loss: 0.45344066619873047
Batch 17/64 loss: 0.4215579032897949
Batch 18/64 loss: 0.29604482650756836
Batch 19/64 loss: 0.18645524978637695
Batch 20/64 loss: 0.17230796813964844
Batch 21/64 loss: 0.5404767990112305
Batch 22/64 loss: 0.5055193901062012
Batch 23/64 loss: 1.3304295539855957
Batch 24/64 loss: 0.6727862358093262
Batch 25/64 loss: 0.3706192970275879
Batch 26/64 loss: 0.25711774826049805
Batch 27/64 loss: 0.4195590019226074
Batch 28/64 loss: 0.1920781135559082
Batch 29/64 loss: 0.4581456184387207
Batch 30/64 loss: 0.46231889724731445
Batch 31/64 loss: 0.30429792404174805
Batch 32/64 loss: 0.22967004776000977
Batch 33/64 loss: 0.32768726348876953
Batch 34/64 loss: 0.2613105773925781
Batch 35/64 loss: 0.5920295715332031
Batch 36/64 loss: 0.6438140869140625
Batch 37/64 loss: 0.8061361312866211
Batch 38/64 loss: 0.6436820030212402
Batch 39/64 loss: 0.4526200294494629
Batch 40/64 loss: 0.7941555976867676
Batch 41/64 loss: 0.21305274963378906
Batch 42/64 loss: 0.32482194900512695
Batch 43/64 loss: 0.49530601501464844
Batch 44/64 loss: 0.027297019958496094
Batch 45/64 loss: 0.43719911575317383
Batch 46/64 loss: 0.18201494216918945
Batch 47/64 loss: 0.15579700469970703
Batch 48/64 loss: 0.21821355819702148
Batch 49/64 loss: 0.0908975601196289
Batch 50/64 loss: 0.3777446746826172
Batch 51/64 loss: 0.8388700485229492
Batch 52/64 loss: 0.06662511825561523
Batch 53/64 loss: 0.3355979919433594
Batch 54/64 loss: 0.1336674690246582
Batch 55/64 loss: 0.08352041244506836
Batch 56/64 loss: 0.5051388740539551
Batch 57/64 loss: 0.4227299690246582
Batch 58/64 loss: 1.0195198059082031
Batch 59/64 loss: 0.2491316795349121
Batch 60/64 loss: -0.1920933723449707
Batch 61/64 loss: 0.4004988670349121
Batch 62/64 loss: 0.5266871452331543
Batch 63/64 loss: 0.12653541564941406
Batch 64/64 loss: -2.855935573577881
Epoch 205  Train loss: 0.37868894502228384  Val loss: 0.5694780906860771
Epoch 206
-------------------------------
Batch 1/64 loss: 0.01756572723388672
Batch 2/64 loss: 0.38076066970825195
Batch 3/64 loss: 0.48705530166625977
Batch 4/64 loss: 0.41976499557495117
Batch 5/64 loss: 0.5537271499633789
Batch 6/64 loss: 0.21254634857177734
Batch 7/64 loss: 0.2660956382751465
Batch 8/64 loss: 0.3070859909057617
Batch 9/64 loss: 0.02147197723388672
Batch 10/64 loss: 0.009521484375
Batch 11/64 loss: 0.30704164505004883
Batch 12/64 loss: 0.7063002586364746
Batch 13/64 loss: 1.030496597290039
Batch 14/64 loss: 0.09465169906616211
Batch 15/64 loss: -0.04340505599975586
Batch 16/64 loss: 0.5802149772644043
Batch 17/64 loss: 0.1786823272705078
Batch 18/64 loss: 0.3760995864868164
Batch 19/64 loss: 0.4356722831726074
Batch 20/64 loss: 0.11201667785644531
Batch 21/64 loss: 0.4402894973754883
Batch 22/64 loss: 0.23617029190063477
Batch 23/64 loss: 0.3601350784301758
Batch 24/64 loss: 0.27823352813720703
Batch 25/64 loss: 0.2745656967163086
Batch 26/64 loss: 0.12219810485839844
Batch 27/64 loss: 0.5765681266784668
Batch 28/64 loss: 0.18762922286987305
Batch 29/64 loss: 0.24372434616088867
Batch 30/64 loss: 0.362030029296875
Batch 31/64 loss: -0.018052101135253906
Batch 32/64 loss: 0.030076026916503906
Batch 33/64 loss: 0.2877054214477539
Batch 34/64 loss: 0.10306119918823242
Batch 35/64 loss: 0.052733421325683594
Batch 36/64 loss: 0.42185544967651367
Batch 37/64 loss: 0.42055320739746094
Batch 38/64 loss: -0.017604351043701172
Batch 39/64 loss: 0.12487936019897461
Batch 40/64 loss: 0.12260246276855469
Batch 41/64 loss: 0.22355890274047852
Batch 42/64 loss: 0.27730274200439453
Batch 43/64 loss: 0.4678535461425781
Batch 44/64 loss: 0.22914505004882812
Batch 45/64 loss: 0.41330623626708984
Batch 46/64 loss: 0.18072223663330078
Batch 47/64 loss: 0.3065662384033203
Batch 48/64 loss: 0.6725144386291504
Batch 49/64 loss: 1.262751579284668
Batch 50/64 loss: 0.501762866973877
Batch 51/64 loss: 0.3163766860961914
Batch 52/64 loss: 0.22236347198486328
Batch 53/64 loss: 0.18007802963256836
Batch 54/64 loss: 0.5977277755737305
Batch 55/64 loss: 0.36303138732910156
Batch 56/64 loss: 0.1374225616455078
Batch 57/64 loss: 0.3326849937438965
Batch 58/64 loss: 0.272186279296875
Batch 59/64 loss: 0.14108037948608398
Batch 60/64 loss: 0.9318618774414062
Batch 61/64 loss: 0.5591249465942383
Batch 62/64 loss: 0.5726585388183594
Batch 63/64 loss: 0.12488794326782227
Batch 64/64 loss: -3.3355441093444824
Epoch 206  Train loss: 0.27996911254583623  Val loss: 0.552104320722757
Epoch 207
-------------------------------
Batch 1/64 loss: 0.6793212890625
Batch 2/64 loss: 0.3295125961303711
Batch 3/64 loss: 0.32839155197143555
Batch 4/64 loss: 0.6966562271118164
Batch 5/64 loss: 0.14617013931274414
Batch 6/64 loss: 0.32025575637817383
Batch 7/64 loss: 0.6374845504760742
Batch 8/64 loss: 0.384249210357666
Batch 9/64 loss: 0.6081681251525879
Batch 10/64 loss: 0.29086780548095703
Batch 11/64 loss: 0.4015512466430664
Batch 12/64 loss: 0.11063146591186523
Batch 13/64 loss: 0.25244951248168945
Batch 14/64 loss: 0.008545875549316406
Batch 15/64 loss: 0.35449743270874023
Batch 16/64 loss: 0.1706223487854004
Batch 17/64 loss: 0.5205216407775879
Batch 18/64 loss: 0.24793004989624023
Batch 19/64 loss: 0.5768094062805176
Batch 20/64 loss: 0.1584310531616211
Batch 21/64 loss: 0.04704856872558594
Batch 22/64 loss: 0.31258058547973633
Batch 23/64 loss: 0.45577001571655273
Batch 24/64 loss: 0.45343589782714844
Batch 25/64 loss: 1.7876181602478027
Batch 26/64 loss: 0.19457578659057617
Batch 27/64 loss: 0.20998001098632812
Batch 28/64 loss: 0.29625797271728516
Batch 29/64 loss: 0.029898643493652344
Batch 30/64 loss: 0.39930152893066406
Batch 31/64 loss: 0.37961578369140625
Batch 32/64 loss: 0.34870338439941406
Batch 33/64 loss: 0.43499088287353516
Batch 34/64 loss: 0.4828329086303711
Batch 35/64 loss: 0.4339118003845215
Batch 36/64 loss: 0.15308713912963867
Batch 37/64 loss: 0.374783992767334
Batch 38/64 loss: 0.38561391830444336
Batch 39/64 loss: 0.2187814712524414
Batch 40/64 loss: 0.23647642135620117
Batch 41/64 loss: 0.4589500427246094
Batch 42/64 loss: 0.39363861083984375
Batch 43/64 loss: 0.03970670700073242
Batch 44/64 loss: 0.35005617141723633
Batch 45/64 loss: 0.5264310836791992
Batch 46/64 loss: 0.3728938102722168
Batch 47/64 loss: 0.3103957176208496
Batch 48/64 loss: 0.32014894485473633
Batch 49/64 loss: 0.3949112892150879
Batch 50/64 loss: 0.5425901412963867
Batch 51/64 loss: 0.22499847412109375
Batch 52/64 loss: 0.25977659225463867
Batch 53/64 loss: 0.2820014953613281
Batch 54/64 loss: 0.24545860290527344
Batch 55/64 loss: 0.2188882827758789
Batch 56/64 loss: 0.38745880126953125
Batch 57/64 loss: 0.44777917861938477
Batch 58/64 loss: 1.416029453277588
Batch 59/64 loss: 0.29313087463378906
Batch 60/64 loss: -0.043643951416015625
Batch 61/64 loss: 0.7707276344299316
Batch 62/64 loss: 0.6476788520812988
Batch 63/64 loss: 0.18809843063354492
Batch 64/64 loss: -3.2395358085632324
Epoch 207  Train loss: 0.33689075357773723  Val loss: 0.5261456728800875
Epoch 208
-------------------------------
Batch 1/64 loss: 0.26994848251342773
Batch 2/64 loss: 0.4889650344848633
Batch 3/64 loss: 0.3467555046081543
Batch 4/64 loss: 0.27024173736572266
Batch 5/64 loss: 0.16650676727294922
Batch 6/64 loss: 0.30911970138549805
Batch 7/64 loss: 0.24676227569580078
Batch 8/64 loss: 0.5026988983154297
Batch 9/64 loss: 0.384307861328125
Batch 10/64 loss: 0.168670654296875
Batch 11/64 loss: 0.17867660522460938
Batch 12/64 loss: 0.2540435791015625
Batch 13/64 loss: 0.15900850296020508
Batch 14/64 loss: -0.047820091247558594
Batch 15/64 loss: 0.332061767578125
Batch 16/64 loss: 4.854294300079346
Batch 17/64 loss: 0.16512060165405273
Batch 18/64 loss: 0.3632998466491699
Batch 19/64 loss: 0.23260784149169922
Batch 20/64 loss: 0.1883864402770996
Batch 21/64 loss: 0.45230627059936523
Batch 22/64 loss: 0.4023094177246094
Batch 23/64 loss: 0.4335031509399414
Batch 24/64 loss: 0.25534963607788086
Batch 25/64 loss: 0.5148153305053711
Batch 26/64 loss: 1.3102893829345703
Batch 27/64 loss: 1.242598533630371
Batch 28/64 loss: 0.6644768714904785
Batch 29/64 loss: 0.5560317039489746
Batch 30/64 loss: 1.0214800834655762
Batch 31/64 loss: 0.8668122291564941
Batch 32/64 loss: 0.8696866035461426
Batch 33/64 loss: 0.5510339736938477
Batch 34/64 loss: 0.9127216339111328
Batch 35/64 loss: 0.5386753082275391
Batch 36/64 loss: 0.5128016471862793
Batch 37/64 loss: 0.281980037689209
Batch 38/64 loss: 0.9549717903137207
Batch 39/64 loss: 0.6941046714782715
Batch 40/64 loss: 0.6633396148681641
Batch 41/64 loss: 0.4868607521057129
Batch 42/64 loss: 0.5363569259643555
Batch 43/64 loss: 0.3887166976928711
Batch 44/64 loss: 0.5451040267944336
Batch 45/64 loss: 0.32626867294311523
Batch 46/64 loss: 0.47064781188964844
Batch 47/64 loss: 0.40250301361083984
Batch 48/64 loss: 0.18392610549926758
Batch 49/64 loss: 0.5824613571166992
Batch 50/64 loss: 0.17275428771972656
Batch 51/64 loss: 0.2572917938232422
Batch 52/64 loss: 0.8769359588623047
Batch 53/64 loss: 0.5018749237060547
Batch 54/64 loss: 0.5082364082336426
Batch 55/64 loss: 0.4393010139465332
Batch 56/64 loss: 0.7709207534790039
Batch 57/64 loss: 0.3474736213684082
Batch 58/64 loss: 0.6073470115661621
Batch 59/64 loss: 0.32115793228149414
Batch 60/64 loss: 0.19913482666015625
Batch 61/64 loss: 0.7278828620910645
Batch 62/64 loss: 0.7347526550292969
Batch 63/64 loss: 0.614081859588623
Batch 64/64 loss: -3.607150077819824
Epoch 208  Train loss: 0.4992874182906805  Val loss: 0.5139537824388222
Epoch 209
-------------------------------
Batch 1/64 loss: 0.6637744903564453
Batch 2/64 loss: 0.2764010429382324
Batch 3/64 loss: 0.6362118721008301
Batch 4/64 loss: 1.1449308395385742
Batch 5/64 loss: 0.3418121337890625
Batch 6/64 loss: 0.3308272361755371
Batch 7/64 loss: 0.37135982513427734
Batch 8/64 loss: 0.00768280029296875
Batch 9/64 loss: 0.407015323638916
Batch 10/64 loss: 0.4951968193054199
Batch 11/64 loss: 0.5299725532531738
Batch 12/64 loss: 0.32250356674194336
Batch 13/64 loss: 0.5134124755859375
Batch 14/64 loss: 0.6934094429016113
Batch 15/64 loss: 2.1801819801330566
Batch 16/64 loss: 0.30339479446411133
Batch 17/64 loss: 0.3454623222351074
Batch 18/64 loss: 0.16758346557617188
Batch 19/64 loss: 0.15555143356323242
Batch 20/64 loss: 0.359494686126709
Batch 21/64 loss: 0.2623453140258789
Batch 22/64 loss: 0.2386775016784668
Batch 23/64 loss: 0.2793154716491699
Batch 24/64 loss: -0.02655506134033203
Batch 25/64 loss: 0.6705431938171387
Batch 26/64 loss: -0.005377292633056641
Batch 27/64 loss: 0.44361209869384766
Batch 28/64 loss: 0.17937707901000977
Batch 29/64 loss: 0.2286677360534668
Batch 30/64 loss: 0.5256505012512207
Batch 31/64 loss: 1.1863102912902832
Batch 32/64 loss: 0.4018263816833496
Batch 33/64 loss: 0.09784460067749023
Batch 34/64 loss: 0.2126927375793457
Batch 35/64 loss: 0.5660858154296875
Batch 36/64 loss: 0.5182132720947266
Batch 37/64 loss: 0.21295642852783203
Batch 38/64 loss: 0.17109107971191406
Batch 39/64 loss: 0.453033447265625
Batch 40/64 loss: 0.4362626075744629
Batch 41/64 loss: 0.7417864799499512
Batch 42/64 loss: 0.08919477462768555
Batch 43/64 loss: 0.18860578536987305
Batch 44/64 loss: 0.14711713790893555
Batch 45/64 loss: 0.2842845916748047
Batch 46/64 loss: 0.4730868339538574
Batch 47/64 loss: 0.26682281494140625
Batch 48/64 loss: 0.509554386138916
Batch 49/64 loss: 0.16200637817382812
Batch 50/64 loss: 0.3718743324279785
Batch 51/64 loss: 0.7724003791809082
Batch 52/64 loss: 0.10903596878051758
Batch 53/64 loss: 0.5600376129150391
Batch 54/64 loss: 0.10055971145629883
Batch 55/64 loss: 0.016399860382080078
Batch 56/64 loss: 0.12645244598388672
Batch 57/64 loss: 0.3742814064025879
Batch 58/64 loss: 0.2182474136352539
Batch 59/64 loss: 0.25359249114990234
Batch 60/64 loss: 0.4896087646484375
Batch 61/64 loss: 0.5294198989868164
Batch 62/64 loss: 0.16727113723754883
Batch 63/64 loss: 0.48838281631469727
Batch 64/64 loss: -3.419994831085205
Epoch 209  Train loss: 0.3478239938324573  Val loss: 0.5190110878436426
Epoch 210
-------------------------------
Batch 1/64 loss: 0.44080209732055664
Batch 2/64 loss: 0.0964202880859375
Batch 3/64 loss: 0.1238698959350586
Batch 4/64 loss: 0.585381031036377
Batch 5/64 loss: 0.2260732650756836
Batch 6/64 loss: 0.19610357284545898
Batch 7/64 loss: 0.46176862716674805
Batch 8/64 loss: 0.11649894714355469
Batch 9/64 loss: 0.27530717849731445
Batch 10/64 loss: 0.5249485969543457
Batch 11/64 loss: 0.025155067443847656
Batch 12/64 loss: 0.32094287872314453
Batch 13/64 loss: 0.12698698043823242
Batch 14/64 loss: 0.24236297607421875
Batch 15/64 loss: -0.04293346405029297
Batch 16/64 loss: 0.5045261383056641
Batch 17/64 loss: 0.6457757949829102
Batch 18/64 loss: 0.02850627899169922
Batch 19/64 loss: 0.43172550201416016
Batch 20/64 loss: 0.4578866958618164
Batch 21/64 loss: 0.4782109260559082
Batch 22/64 loss: 0.1525740623474121
Batch 23/64 loss: 0.12999629974365234
Batch 24/64 loss: 0.3667640686035156
Batch 25/64 loss: 0.5777645111083984
Batch 26/64 loss: 0.2338733673095703
Batch 27/64 loss: 0.10657978057861328
Batch 28/64 loss: 0.24619102478027344
Batch 29/64 loss: 0.540489673614502
Batch 30/64 loss: 0.08721399307250977
Batch 31/64 loss: 0.28731822967529297
Batch 32/64 loss: 0.3483705520629883
Batch 33/64 loss: 0.25763988494873047
Batch 34/64 loss: 0.20632314682006836
Batch 35/64 loss: 1.3771052360534668
Batch 36/64 loss: 0.650853157043457
Batch 37/64 loss: 0.34493350982666016
Batch 38/64 loss: 0.45137643814086914
Batch 39/64 loss: 0.47005414962768555
Batch 40/64 loss: 0.27713632583618164
Batch 41/64 loss: 0.01801013946533203
Batch 42/64 loss: 0.05294513702392578
Batch 43/64 loss: 0.5229854583740234
Batch 44/64 loss: 0.5766048431396484
Batch 45/64 loss: 0.4832000732421875
Batch 46/64 loss: 0.5912470817565918
Batch 47/64 loss: 0.1786346435546875
Batch 48/64 loss: 0.4921755790710449
Batch 49/64 loss: 0.33495330810546875
Batch 50/64 loss: 1.0174007415771484
Batch 51/64 loss: 0.2751436233520508
Batch 52/64 loss: 0.6739554405212402
Batch 53/64 loss: 0.023013591766357422
Batch 54/64 loss: 0.25270795822143555
Batch 55/64 loss: 0.21745920181274414
Batch 56/64 loss: 0.2093806266784668
Batch 57/64 loss: 0.39490747451782227
Batch 58/64 loss: 0.3380775451660156
Batch 59/64 loss: 0.1415548324584961
Batch 60/64 loss: 0.6045398712158203
Batch 61/64 loss: 0.16605377197265625
Batch 62/64 loss: 0.2862586975097656
Batch 63/64 loss: 0.4031381607055664
Batch 64/64 loss: -3.2285709381103516
Epoch 210  Train loss: 0.30136151033289293  Val loss: 0.5384464657183775
Epoch 211
-------------------------------
Batch 1/64 loss: 0.21689462661743164
Batch 2/64 loss: 0.4616379737854004
Batch 3/64 loss: 0.40760278701782227
Batch 4/64 loss: 0.3739442825317383
Batch 5/64 loss: 0.17724275588989258
Batch 6/64 loss: 0.34851503372192383
Batch 7/64 loss: 0.1473541259765625
Batch 8/64 loss: 0.11941909790039062
Batch 9/64 loss: 0.3020510673522949
Batch 10/64 loss: 0.3316373825073242
Batch 11/64 loss: 0.20809316635131836
Batch 12/64 loss: 0.3046579360961914
Batch 13/64 loss: 0.3195919990539551
Batch 14/64 loss: 0.2394413948059082
Batch 15/64 loss: 0.21356773376464844
Batch 16/64 loss: 0.07261323928833008
Batch 17/64 loss: 0.10225915908813477
Batch 18/64 loss: -0.06532955169677734
Batch 19/64 loss: 0.36039257049560547
Batch 20/64 loss: 0.2087259292602539
Batch 21/64 loss: 0.11377525329589844
Batch 22/64 loss: 0.34996557235717773
Batch 23/64 loss: 0.010613441467285156
Batch 24/64 loss: 0.34319543838500977
Batch 25/64 loss: 0.20795917510986328
Batch 26/64 loss: -0.0543065071105957
Batch 27/64 loss: 0.6687498092651367
Batch 28/64 loss: 0.5102376937866211
Batch 29/64 loss: 0.9200935363769531
Batch 30/64 loss: 0.46704721450805664
Batch 31/64 loss: 0.5680904388427734
Batch 32/64 loss: 0.17473936080932617
Batch 33/64 loss: -0.039331912994384766
Batch 34/64 loss: 0.3221926689147949
Batch 35/64 loss: 0.24462413787841797
Batch 36/64 loss: 0.4087347984313965
Batch 37/64 loss: 0.20789813995361328
Batch 38/64 loss: 0.038333892822265625
Batch 39/64 loss: 0.27875661849975586
Batch 40/64 loss: 0.11146211624145508
Batch 41/64 loss: 0.4038658142089844
Batch 42/64 loss: 0.23237037658691406
Batch 43/64 loss: 0.49788427352905273
Batch 44/64 loss: 0.23453283309936523
Batch 45/64 loss: 0.21203136444091797
Batch 46/64 loss: 0.3974151611328125
Batch 47/64 loss: 0.31030750274658203
Batch 48/64 loss: 0.2584648132324219
Batch 49/64 loss: 0.08112049102783203
Batch 50/64 loss: 0.8466548919677734
Batch 51/64 loss: 0.22267723083496094
Batch 52/64 loss: 0.40077972412109375
Batch 53/64 loss: 0.4460744857788086
Batch 54/64 loss: 0.0818934440612793
Batch 55/64 loss: 0.08735275268554688
Batch 56/64 loss: 0.43140649795532227
Batch 57/64 loss: 0.1332998275756836
Batch 58/64 loss: 0.22597885131835938
Batch 59/64 loss: 0.029599666595458984
Batch 60/64 loss: 0.582545280456543
Batch 61/64 loss: 0.28785228729248047
Batch 62/64 loss: 0.8392753601074219
Batch 63/64 loss: 0.3677692413330078
Batch 64/64 loss: -3.3468055725097656
Epoch 211  Train loss: 0.2479088876761642  Val loss: 0.34397548662428185
Saving best model, epoch: 211
Epoch 212
-------------------------------
Batch 1/64 loss: 0.302340030670166
Batch 2/64 loss: 0.3842587471008301
Batch 3/64 loss: 0.19803953170776367
Batch 4/64 loss: 0.17251062393188477
Batch 5/64 loss: 0.15528583526611328
Batch 6/64 loss: 0.13039398193359375
Batch 7/64 loss: 0.15212583541870117
Batch 8/64 loss: 0.2869715690612793
Batch 9/64 loss: 0.26416587829589844
Batch 10/64 loss: 0.22238922119140625
Batch 11/64 loss: 0.33746957778930664
Batch 12/64 loss: -0.02220010757446289
Batch 13/64 loss: -0.051746368408203125
Batch 14/64 loss: 0.04134368896484375
Batch 15/64 loss: 0.49578857421875
Batch 16/64 loss: -0.11031150817871094
Batch 17/64 loss: -0.06351280212402344
Batch 18/64 loss: 0.15322589874267578
Batch 19/64 loss: 0.21274757385253906
Batch 20/64 loss: 0.3057847023010254
Batch 21/64 loss: 0.5335988998413086
Batch 22/64 loss: 0.5276546478271484
Batch 23/64 loss: 0.20166587829589844
Batch 24/64 loss: 0.38989925384521484
Batch 25/64 loss: 0.4727921485900879
Batch 26/64 loss: 0.006159305572509766
Batch 27/64 loss: 0.26387977600097656
Batch 28/64 loss: -0.1404728889465332
Batch 29/64 loss: 0.14990663528442383
Batch 30/64 loss: 0.5675849914550781
Batch 31/64 loss: 0.08582019805908203
Batch 32/64 loss: 0.028562545776367188
Batch 33/64 loss: 1.0177583694458008
Batch 34/64 loss: 0.45345067977905273
Batch 35/64 loss: 0.17452526092529297
Batch 36/64 loss: 0.5327625274658203
Batch 37/64 loss: 0.5740361213684082
Batch 38/64 loss: -0.006779193878173828
Batch 39/64 loss: 0.42914390563964844
Batch 40/64 loss: 0.7114267349243164
Batch 41/64 loss: -0.05039691925048828
Batch 42/64 loss: 0.09650564193725586
Batch 43/64 loss: 0.17302608489990234
Batch 44/64 loss: 0.3061819076538086
Batch 45/64 loss: 0.29427433013916016
Batch 46/64 loss: 0.34501123428344727
Batch 47/64 loss: 0.05123758316040039
Batch 48/64 loss: 0.09262847900390625
Batch 49/64 loss: 0.1411881446838379
Batch 50/64 loss: 0.2595791816711426
Batch 51/64 loss: -0.0010366439819335938
Batch 52/64 loss: 0.06256246566772461
Batch 53/64 loss: 0.11455059051513672
Batch 54/64 loss: 0.37903308868408203
Batch 55/64 loss: 0.23919963836669922
Batch 56/64 loss: 0.37232208251953125
Batch 57/64 loss: 0.3446998596191406
Batch 58/64 loss: 0.21876859664916992
Batch 59/64 loss: 0.3394031524658203
Batch 60/64 loss: 0.1114044189453125
Batch 61/64 loss: 0.15077590942382812
Batch 62/64 loss: 0.2575674057006836
Batch 63/64 loss: 0.4121541976928711
Batch 64/64 loss: -3.578331470489502
Epoch 212  Train loss: 0.19713471543555167  Val loss: 0.37902502997224685
Epoch 213
-------------------------------
Batch 1/64 loss: 0.019967079162597656
Batch 2/64 loss: 0.09171295166015625
Batch 3/64 loss: 0.32522153854370117
Batch 4/64 loss: 0.00418853759765625
Batch 5/64 loss: 0.0957484245300293
Batch 6/64 loss: -0.026844024658203125
Batch 7/64 loss: 0.01307821273803711
Batch 8/64 loss: 0.07205581665039062
Batch 9/64 loss: 0.33838415145874023
Batch 10/64 loss: 0.015588760375976562
Batch 11/64 loss: 0.14414501190185547
Batch 12/64 loss: 0.41890764236450195
Batch 13/64 loss: -0.055069923400878906
Batch 14/64 loss: 0.04871082305908203
Batch 15/64 loss: 0.48619985580444336
Batch 16/64 loss: 0.35396766662597656
Batch 17/64 loss: 0.23048830032348633
Batch 18/64 loss: 0.1015615463256836
Batch 19/64 loss: 0.3815007209777832
Batch 20/64 loss: 0.5067272186279297
Batch 21/64 loss: 0.5620579719543457
Batch 22/64 loss: 0.6657686233520508
Batch 23/64 loss: -0.11596441268920898
Batch 24/64 loss: 0.18274450302124023
Batch 25/64 loss: 0.3836188316345215
Batch 26/64 loss: -0.05081462860107422
Batch 27/64 loss: 0.28838443756103516
Batch 28/64 loss: 0.30043983459472656
Batch 29/64 loss: 0.9770231246948242
Batch 30/64 loss: 0.23105335235595703
Batch 31/64 loss: 0.7141447067260742
Batch 32/64 loss: 0.10074281692504883
Batch 33/64 loss: -0.029591083526611328
Batch 34/64 loss: 0.24545049667358398
Batch 35/64 loss: 0.06502723693847656
Batch 36/64 loss: 0.019888877868652344
Batch 37/64 loss: 0.027585506439208984
Batch 38/64 loss: 0.274686336517334
Batch 39/64 loss: -0.2319040298461914
Batch 40/64 loss: 0.0825204849243164
Batch 41/64 loss: 0.20522832870483398
Batch 42/64 loss: 0.19186735153198242
Batch 43/64 loss: 0.4622926712036133
Batch 44/64 loss: 0.09600400924682617
Batch 45/64 loss: 0.15723323822021484
Batch 46/64 loss: 0.5909099578857422
Batch 47/64 loss: 0.2186746597290039
Batch 48/64 loss: 0.03470182418823242
Batch 49/64 loss: -0.1960620880126953
Batch 50/64 loss: 0.49646997451782227
Batch 51/64 loss: 0.5259051322937012
Batch 52/64 loss: 0.29709339141845703
Batch 53/64 loss: -0.10663986206054688
Batch 54/64 loss: -0.2784538269042969
Batch 55/64 loss: 0.0495762825012207
Batch 56/64 loss: 0.21545124053955078
Batch 57/64 loss: -0.18682432174682617
Batch 58/64 loss: 0.011472225189208984
Batch 59/64 loss: 0.13752174377441406
Batch 60/64 loss: -0.008281230926513672
Batch 61/64 loss: 0.30660057067871094
Batch 62/64 loss: 0.566411018371582
Batch 63/64 loss: 0.39987945556640625
Batch 64/64 loss: -3.026151180267334
Epoch 213  Train loss: 0.1596317122964298  Val loss: 0.3561623956739288
Epoch 214
-------------------------------
Batch 1/64 loss: 0.22005462646484375
Batch 2/64 loss: 0.5591845512390137
Batch 3/64 loss: -0.07019519805908203
Batch 4/64 loss: 0.012566089630126953
Batch 5/64 loss: 0.24417829513549805
Batch 6/64 loss: 0.3770766258239746
Batch 7/64 loss: 0.2859673500061035
Batch 8/64 loss: 0.21275663375854492
Batch 9/64 loss: 0.20243406295776367
Batch 10/64 loss: 0.07390499114990234
Batch 11/64 loss: 0.06584692001342773
Batch 12/64 loss: 0.4797186851501465
Batch 13/64 loss: 0.8068318367004395
Batch 14/64 loss: 0.31725358963012695
Batch 15/64 loss: 0.17518854141235352
Batch 16/64 loss: 0.622410774230957
Batch 17/64 loss: 0.21260499954223633
Batch 18/64 loss: 0.1751570701599121
Batch 19/64 loss: 0.3955063819885254
Batch 20/64 loss: 0.0008006095886230469
Batch 21/64 loss: 0.4343438148498535
Batch 22/64 loss: 0.32163190841674805
Batch 23/64 loss: 0.2664527893066406
Batch 24/64 loss: 0.03196001052856445
Batch 25/64 loss: -0.002166271209716797
Batch 26/64 loss: 0.12912845611572266
Batch 27/64 loss: 0.24219846725463867
Batch 28/64 loss: 0.5909633636474609
Batch 29/64 loss: 0.016028881072998047
Batch 30/64 loss: 0.2882552146911621
Batch 31/64 loss: 0.16048097610473633
Batch 32/64 loss: 0.08042478561401367
Batch 33/64 loss: 0.2416820526123047
Batch 34/64 loss: 0.0671539306640625
Batch 35/64 loss: 0.35613441467285156
Batch 36/64 loss: 0.34221458435058594
Batch 37/64 loss: 0.6274113655090332
Batch 38/64 loss: 0.32380151748657227
Batch 39/64 loss: 0.3858451843261719
Batch 40/64 loss: 0.058139801025390625
Batch 41/64 loss: 0.19358396530151367
Batch 42/64 loss: 0.25568580627441406
Batch 43/64 loss: 0.43578624725341797
Batch 44/64 loss: -0.11386728286743164
Batch 45/64 loss: 0.06673002243041992
Batch 46/64 loss: -0.14681529998779297
Batch 47/64 loss: 0.18538141250610352
Batch 48/64 loss: 0.16179275512695312
Batch 49/64 loss: 0.42828893661499023
Batch 50/64 loss: -0.1909480094909668
Batch 51/64 loss: -0.1781473159790039
Batch 52/64 loss: 0.531886100769043
Batch 53/64 loss: 0.023556232452392578
Batch 54/64 loss: 0.13025903701782227
Batch 55/64 loss: 0.2700824737548828
Batch 56/64 loss: -0.006752490997314453
Batch 57/64 loss: 0.8704671859741211
Batch 58/64 loss: 0.1793689727783203
Batch 59/64 loss: 0.6183643341064453
Batch 60/64 loss: -0.15392494201660156
Batch 61/64 loss: 0.2089385986328125
Batch 62/64 loss: 0.22201156616210938
Batch 63/64 loss: 0.19336843490600586
Batch 64/64 loss: -3.6568918228149414
Epoch 214  Train loss: 0.18468644011254404  Val loss: 0.23120733306989638
Saving best model, epoch: 214
Epoch 215
-------------------------------
Batch 1/64 loss: 0.4209623336791992
Batch 2/64 loss: 0.6689882278442383
Batch 3/64 loss: 0.058196067810058594
Batch 4/64 loss: 0.13547229766845703
Batch 5/64 loss: -0.1232309341430664
Batch 6/64 loss: 0.6972050666809082
Batch 7/64 loss: 0.2547621726989746
Batch 8/64 loss: 0.4333686828613281
Batch 9/64 loss: 0.03237199783325195
Batch 10/64 loss: 0.19583606719970703
Batch 11/64 loss: 0.2614588737487793
Batch 12/64 loss: 0.04217529296875
Batch 13/64 loss: 0.5749282836914062
Batch 14/64 loss: 0.09483098983764648
Batch 15/64 loss: 0.269650936126709
Batch 16/64 loss: -0.15693140029907227
Batch 17/64 loss: 0.15483427047729492
Batch 18/64 loss: 0.36353063583374023
Batch 19/64 loss: 0.06638336181640625
Batch 20/64 loss: 0.44373226165771484
Batch 21/64 loss: 0.4160346984863281
Batch 22/64 loss: 0.04645347595214844
Batch 23/64 loss: 0.30193614959716797
Batch 24/64 loss: -0.12983179092407227
Batch 25/64 loss: -0.03854846954345703
Batch 26/64 loss: -0.1994495391845703
Batch 27/64 loss: 0.4475889205932617
Batch 28/64 loss: 0.23614740371704102
Batch 29/64 loss: 0.28853416442871094
Batch 30/64 loss: -0.17607688903808594
Batch 31/64 loss: 0.14066743850708008
Batch 32/64 loss: 0.534031867980957
Batch 33/64 loss: 0.35071802139282227
Batch 34/64 loss: 0.29242610931396484
Batch 35/64 loss: 0.7637085914611816
Batch 36/64 loss: 0.09002161026000977
Batch 37/64 loss: 0.7518625259399414
Batch 38/64 loss: 0.4724464416503906
Batch 39/64 loss: 0.34835386276245117
Batch 40/64 loss: 1.041114330291748
Batch 41/64 loss: 0.006898403167724609
Batch 42/64 loss: 0.27668142318725586
Batch 43/64 loss: 0.2107844352722168
Batch 44/64 loss: 0.18442583084106445
Batch 45/64 loss: 0.05394744873046875
Batch 46/64 loss: 0.14919614791870117
Batch 47/64 loss: 1.392381191253662
Batch 48/64 loss: 0.2690277099609375
Batch 49/64 loss: 0.3864307403564453
Batch 50/64 loss: 0.48923254013061523
Batch 51/64 loss: 0.3033909797668457
Batch 52/64 loss: 0.43866777420043945
Batch 53/64 loss: 0.3052687644958496
Batch 54/64 loss: 0.5536260604858398
Batch 55/64 loss: 0.3967094421386719
Batch 56/64 loss: 0.5055131912231445
Batch 57/64 loss: 0.6210403442382812
Batch 58/64 loss: 1.015737533569336
Batch 59/64 loss: 0.4779658317565918
Batch 60/64 loss: 0.24700403213500977
Batch 61/64 loss: 0.5143222808837891
Batch 62/64 loss: 0.5653042793273926
Batch 63/64 loss: -0.027304649353027344
Batch 64/64 loss: -3.5503721237182617
Epoch 215  Train loss: 0.2751394047456629  Val loss: 0.5643508753825709
Epoch 216
-------------------------------
Batch 1/64 loss: 0.5892724990844727
Batch 2/64 loss: 0.5035114288330078
Batch 3/64 loss: 0.6899490356445312
Batch 4/64 loss: 0.6521763801574707
Batch 5/64 loss: 0.10437297821044922
Batch 6/64 loss: 0.48989343643188477
Batch 7/64 loss: 0.4964599609375
Batch 8/64 loss: 0.6235456466674805
Batch 9/64 loss: 0.4687471389770508
Batch 10/64 loss: 0.7480626106262207
Batch 11/64 loss: 0.21297550201416016
Batch 12/64 loss: 0.42975664138793945
Batch 13/64 loss: 0.20379304885864258
Batch 14/64 loss: 0.4586300849914551
Batch 15/64 loss: 0.33235740661621094
Batch 16/64 loss: 0.46281957626342773
Batch 17/64 loss: 0.24735784530639648
Batch 18/64 loss: 0.6262660026550293
Batch 19/64 loss: 0.4792361259460449
Batch 20/64 loss: 0.20808982849121094
Batch 21/64 loss: 0.2927865982055664
Batch 22/64 loss: 0.38028669357299805
Batch 23/64 loss: 0.6114282608032227
Batch 24/64 loss: 0.046360015869140625
Batch 25/64 loss: -0.03732919692993164
Batch 26/64 loss: 0.29679250717163086
Batch 27/64 loss: 0.28931665420532227
Batch 28/64 loss: 0.5452136993408203
Batch 29/64 loss: 0.017141342163085938
Batch 30/64 loss: 0.05577993392944336
Batch 31/64 loss: 0.09829378128051758
Batch 32/64 loss: 0.10747003555297852
Batch 33/64 loss: 0.05502462387084961
Batch 34/64 loss: 0.3450284004211426
Batch 35/64 loss: 0.2516765594482422
Batch 36/64 loss: 0.30182504653930664
Batch 37/64 loss: 0.032942771911621094
Batch 38/64 loss: 0.7689700126647949
Batch 39/64 loss: 0.49949216842651367
Batch 40/64 loss: 0.4205474853515625
Batch 41/64 loss: 0.9114828109741211
Batch 42/64 loss: 0.3252897262573242
Batch 43/64 loss: 0.26563358306884766
Batch 44/64 loss: 0.22676420211791992
Batch 45/64 loss: 0.5867424011230469
Batch 46/64 loss: 0.8232159614562988
Batch 47/64 loss: 0.4398331642150879
Batch 48/64 loss: 0.13451719284057617
Batch 49/64 loss: 0.37792205810546875
Batch 50/64 loss: 0.23614025115966797
Batch 51/64 loss: 0.42496299743652344
Batch 52/64 loss: 0.18889808654785156
Batch 53/64 loss: 0.14841604232788086
Batch 54/64 loss: 0.4205451011657715
Batch 55/64 loss: 0.19347000122070312
Batch 56/64 loss: -0.17928409576416016
Batch 57/64 loss: 0.18236446380615234
Batch 58/64 loss: 0.3272666931152344
Batch 59/64 loss: 0.8815808296203613
Batch 60/64 loss: 0.1542224884033203
Batch 61/64 loss: 0.1456432342529297
Batch 62/64 loss: 0.32079029083251953
Batch 63/64 loss: 0.27788400650024414
Batch 64/64 loss: -2.579188823699951
Epoch 216  Train loss: 0.31821537952797085  Val loss: 0.412016413056154
Epoch 217
-------------------------------
Batch 1/64 loss: 0.23483848571777344
Batch 2/64 loss: 0.2904057502746582
Batch 3/64 loss: 0.2814145088195801
Batch 4/64 loss: 0.4037470817565918
Batch 5/64 loss: 0.16825103759765625
Batch 6/64 loss: 0.32015228271484375
Batch 7/64 loss: 0.4610586166381836
Batch 8/64 loss: 0.036623477935791016
Batch 9/64 loss: 0.24433040618896484
Batch 10/64 loss: 0.30783510208129883
Batch 11/64 loss: 0.3284749984741211
Batch 12/64 loss: 0.33888959884643555
Batch 13/64 loss: 0.3800678253173828
Batch 14/64 loss: 0.6393861770629883
Batch 15/64 loss: 0.28267860412597656
Batch 16/64 loss: 0.1436634063720703
Batch 17/64 loss: 0.2670135498046875
Batch 18/64 loss: 0.3923768997192383
Batch 19/64 loss: 0.16303014755249023
Batch 20/64 loss: 0.47550153732299805
Batch 21/64 loss: 0.37919139862060547
Batch 22/64 loss: 0.19492483139038086
Batch 23/64 loss: 0.3074173927307129
Batch 24/64 loss: 0.23995637893676758
Batch 25/64 loss: 0.15284442901611328
Batch 26/64 loss: 0.09382009506225586
Batch 27/64 loss: 0.02178192138671875
Batch 28/64 loss: 0.4400515556335449
Batch 29/64 loss: -0.26349496841430664
Batch 30/64 loss: 0.3353390693664551
Batch 31/64 loss: 0.3964567184448242
Batch 32/64 loss: 0.4133291244506836
Batch 33/64 loss: -9.393692016601562e-05
Batch 34/64 loss: -0.11343193054199219
Batch 35/64 loss: 0.035462379455566406
Batch 36/64 loss: 0.16723966598510742
Batch 37/64 loss: 0.32101869583129883
Batch 38/64 loss: 0.040361881256103516
Batch 39/64 loss: 0.000301361083984375
Batch 40/64 loss: 0.7364177703857422
Batch 41/64 loss: 0.3783283233642578
Batch 42/64 loss: 0.4104161262512207
Batch 43/64 loss: 0.5787720680236816
Batch 44/64 loss: 0.2619051933288574
Batch 45/64 loss: -0.034171104431152344
Batch 46/64 loss: 0.1551227569580078
Batch 47/64 loss: 0.6138033866882324
Batch 48/64 loss: 0.28719425201416016
Batch 49/64 loss: 0.3206663131713867
Batch 50/64 loss: 0.2887897491455078
Batch 51/64 loss: 0.09347772598266602
Batch 52/64 loss: 0.036232948303222656
Batch 53/64 loss: 0.18404150009155273
Batch 54/64 loss: 0.780611515045166
Batch 55/64 loss: 0.12186622619628906
Batch 56/64 loss: 0.35298919677734375
Batch 57/64 loss: 0.18413066864013672
Batch 58/64 loss: 0.08428525924682617
Batch 59/64 loss: 0.18095922470092773
Batch 60/64 loss: 0.18065547943115234
Batch 61/64 loss: -0.035039424896240234
Batch 62/64 loss: 0.0669088363647461
Batch 63/64 loss: 0.35866689682006836
Batch 64/64 loss: -3.4292163848876953
Epoch 217  Train loss: 0.20921308480057063  Val loss: 0.22333685229324393
Saving best model, epoch: 217
Epoch 218
-------------------------------
Batch 1/64 loss: 0.2588038444519043
Batch 2/64 loss: 0.046172142028808594
Batch 3/64 loss: 0.05284690856933594
Batch 4/64 loss: -0.015489578247070312
Batch 5/64 loss: 0.3983473777770996
Batch 6/64 loss: 0.22694015502929688
Batch 7/64 loss: 0.25350141525268555
Batch 8/64 loss: 0.14473295211791992
Batch 9/64 loss: 0.24692869186401367
Batch 10/64 loss: -0.023324966430664062
Batch 11/64 loss: -0.14883899688720703
Batch 12/64 loss: 0.4017515182495117
Batch 13/64 loss: 0.18563270568847656
Batch 14/64 loss: 0.5692768096923828
Batch 15/64 loss: 0.16624164581298828
Batch 16/64 loss: 0.17309284210205078
Batch 17/64 loss: 0.2181110382080078
Batch 18/64 loss: 0.642611026763916
Batch 19/64 loss: 0.015656471252441406
Batch 20/64 loss: 0.11279916763305664
Batch 21/64 loss: -0.047269344329833984
Batch 22/64 loss: 0.34271717071533203
Batch 23/64 loss: 0.5294837951660156
Batch 24/64 loss: 0.29954957962036133
Batch 25/64 loss: 0.12892818450927734
Batch 26/64 loss: 0.3821854591369629
Batch 27/64 loss: 0.3081378936767578
Batch 28/64 loss: -0.0027151107788085938
Batch 29/64 loss: -0.03193235397338867
Batch 30/64 loss: -0.16096067428588867
Batch 31/64 loss: 0.7060704231262207
Batch 32/64 loss: 0.35543298721313477
Batch 33/64 loss: 0.213409423828125
Batch 34/64 loss: 0.3806905746459961
Batch 35/64 loss: 0.0010275840759277344
Batch 36/64 loss: 0.624547004699707
Batch 37/64 loss: -0.1802225112915039
Batch 38/64 loss: -0.012336254119873047
Batch 39/64 loss: 0.19256877899169922
Batch 40/64 loss: 0.20234394073486328
Batch 41/64 loss: 0.1235666275024414
Batch 42/64 loss: 0.03312206268310547
Batch 43/64 loss: -0.05187082290649414
Batch 44/64 loss: 0.042090415954589844
Batch 45/64 loss: 0.8894290924072266
Batch 46/64 loss: 0.09850788116455078
Batch 47/64 loss: 0.7856731414794922
Batch 48/64 loss: 0.39055919647216797
Batch 49/64 loss: 0.13649463653564453
Batch 50/64 loss: -0.19257545471191406
Batch 51/64 loss: -0.06518745422363281
Batch 52/64 loss: 0.750338077545166
Batch 53/64 loss: 0.08153581619262695
Batch 54/64 loss: 0.15947866439819336
Batch 55/64 loss: 0.22160911560058594
Batch 56/64 loss: 0.14056921005249023
Batch 57/64 loss: 0.20422697067260742
Batch 58/64 loss: -0.02265453338623047
Batch 59/64 loss: 0.1880025863647461
Batch 60/64 loss: 0.1560831069946289
Batch 61/64 loss: 0.15973186492919922
Batch 62/64 loss: 0.5996432304382324
Batch 63/64 loss: -0.01350259780883789
Batch 64/64 loss: -3.4375438690185547
Epoch 218  Train loss: 0.16304568870394837  Val loss: 0.20859992954739182
Saving best model, epoch: 218
Epoch 219
-------------------------------
Batch 1/64 loss: 0.6756625175476074
Batch 2/64 loss: 0.23830270767211914
Batch 3/64 loss: 0.11608409881591797
Batch 4/64 loss: 0.008053302764892578
Batch 5/64 loss: 0.0023860931396484375
Batch 6/64 loss: 0.3177666664123535
Batch 7/64 loss: 0.03480100631713867
Batch 8/64 loss: -0.22884273529052734
Batch 9/64 loss: 0.2374897003173828
Batch 10/64 loss: -0.08352184295654297
Batch 11/64 loss: 0.03745603561401367
Batch 12/64 loss: 0.5427780151367188
Batch 13/64 loss: -0.0971522331237793
Batch 14/64 loss: 0.17934703826904297
Batch 15/64 loss: 0.05359458923339844
Batch 16/64 loss: 0.20064783096313477
Batch 17/64 loss: 0.44466590881347656
Batch 18/64 loss: 0.3372669219970703
Batch 19/64 loss: 0.3478431701660156
Batch 20/64 loss: -0.0028896331787109375
Batch 21/64 loss: 0.08005189895629883
Batch 22/64 loss: 0.4226551055908203
Batch 23/64 loss: 0.42827510833740234
Batch 24/64 loss: 0.07915496826171875
Batch 25/64 loss: -0.12271928787231445
Batch 26/64 loss: 0.10881280899047852
Batch 27/64 loss: 0.36446189880371094
Batch 28/64 loss: -0.03223466873168945
Batch 29/64 loss: 0.1355276107788086
Batch 30/64 loss: 0.17566728591918945
Batch 31/64 loss: -0.09178495407104492
Batch 32/64 loss: 0.13993215560913086
Batch 33/64 loss: -0.16805410385131836
Batch 34/64 loss: 0.26787853240966797
Batch 35/64 loss: 0.05867195129394531
Batch 36/64 loss: 0.44173383712768555
Batch 37/64 loss: 0.06684303283691406
Batch 38/64 loss: 0.21741819381713867
Batch 39/64 loss: 0.428037166595459
Batch 40/64 loss: -0.20009231567382812
Batch 41/64 loss: -0.1724715232849121
Batch 42/64 loss: 0.4091520309448242
Batch 43/64 loss: 0.19694280624389648
Batch 44/64 loss: 0.06928157806396484
Batch 45/64 loss: 0.005275249481201172
Batch 46/64 loss: 0.6496191024780273
Batch 47/64 loss: 0.2988877296447754
Batch 48/64 loss: 0.23173999786376953
Batch 49/64 loss: 0.3839263916015625
Batch 50/64 loss: -0.06863546371459961
Batch 51/64 loss: -0.1607522964477539
Batch 52/64 loss: 0.31329345703125
Batch 53/64 loss: 0.5237808227539062
Batch 54/64 loss: 0.14088201522827148
Batch 55/64 loss: 0.06664657592773438
Batch 56/64 loss: 0.39020299911499023
Batch 57/64 loss: 0.0833578109741211
Batch 58/64 loss: 0.5235433578491211
Batch 59/64 loss: 0.16773462295532227
Batch 60/64 loss: 0.2974052429199219
Batch 61/64 loss: 0.39881277084350586
Batch 62/64 loss: 0.11448860168457031
Batch 63/64 loss: 0.3616170883178711
Batch 64/64 loss: -2.7943849563598633
Epoch 219  Train loss: 0.14573988447002337  Val loss: 0.2032004811919432
Saving best model, epoch: 219
Epoch 220
-------------------------------
Batch 1/64 loss: 0.40231943130493164
Batch 2/64 loss: 0.30733346939086914
Batch 3/64 loss: 0.01660442352294922
Batch 4/64 loss: 0.20465373992919922
Batch 5/64 loss: 0.1595478057861328
Batch 6/64 loss: -0.030413150787353516
Batch 7/64 loss: 0.36910390853881836
Batch 8/64 loss: 0.7479944229125977
Batch 9/64 loss: 0.3997340202331543
Batch 10/64 loss: 0.5826969146728516
Batch 11/64 loss: 0.1902928352355957
Batch 12/64 loss: 0.3093085289001465
Batch 13/64 loss: 0.1158289909362793
Batch 14/64 loss: 0.18552160263061523
Batch 15/64 loss: 0.2563800811767578
Batch 16/64 loss: 0.6013393402099609
Batch 17/64 loss: 0.38240528106689453
Batch 18/64 loss: 0.2853245735168457
Batch 19/64 loss: 0.13791227340698242
Batch 20/64 loss: 0.430203914642334
Batch 21/64 loss: 0.28560686111450195
Batch 22/64 loss: 0.28026390075683594
Batch 23/64 loss: 0.20119857788085938
Batch 24/64 loss: 0.2282123565673828
Batch 25/64 loss: 0.4324812889099121
Batch 26/64 loss: 0.27125024795532227
Batch 27/64 loss: 0.13655376434326172
Batch 28/64 loss: 0.5546603202819824
Batch 29/64 loss: 0.09624290466308594
Batch 30/64 loss: 0.018159866333007812
Batch 31/64 loss: 0.16460895538330078
Batch 32/64 loss: 0.5773239135742188
Batch 33/64 loss: 0.4100461006164551
Batch 34/64 loss: 0.3669767379760742
Batch 35/64 loss: 0.5941629409790039
Batch 36/64 loss: 0.5359539985656738
Batch 37/64 loss: -0.02604389190673828
Batch 38/64 loss: 0.5123739242553711
Batch 39/64 loss: -0.28203630447387695
Batch 40/64 loss: -0.1691722869873047
Batch 41/64 loss: 0.29018545150756836
Batch 42/64 loss: 0.12427234649658203
Batch 43/64 loss: 0.2288818359375
Batch 44/64 loss: 0.1919856071472168
Batch 45/64 loss: 0.0849604606628418
Batch 46/64 loss: 0.3006410598754883
Batch 47/64 loss: -0.08246946334838867
Batch 48/64 loss: 0.3268265724182129
Batch 49/64 loss: 0.10295343399047852
Batch 50/64 loss: 0.061757564544677734
Batch 51/64 loss: 0.2058391571044922
Batch 52/64 loss: 0.34383153915405273
Batch 53/64 loss: -0.07215118408203125
Batch 54/64 loss: 0.05599546432495117
Batch 55/64 loss: 0.4850602149963379
Batch 56/64 loss: 0.30501317977905273
Batch 57/64 loss: 0.2978973388671875
Batch 58/64 loss: -0.018468379974365234
Batch 59/64 loss: 0.3207125663757324
Batch 60/64 loss: 0.038683414459228516
Batch 61/64 loss: 0.2416706085205078
Batch 62/64 loss: 0.421142578125
Batch 63/64 loss: -0.002597332000732422
Batch 64/64 loss: -3.318060874938965
Epoch 220  Train loss: 0.20403129353242763  Val loss: 0.30987459687432883
Epoch 221
-------------------------------
Batch 1/64 loss: 0.11459684371948242
Batch 2/64 loss: 0.34225988388061523
Batch 3/64 loss: 0.3631420135498047
Batch 4/64 loss: 0.05268049240112305
Batch 5/64 loss: 0.15067720413208008
Batch 6/64 loss: 0.2164902687072754
Batch 7/64 loss: -0.22864103317260742
Batch 8/64 loss: 0.12895584106445312
Batch 9/64 loss: -0.1799602508544922
Batch 10/64 loss: -0.03752708435058594
Batch 11/64 loss: 0.20562982559204102
Batch 12/64 loss: 0.35050201416015625
Batch 13/64 loss: 0.40450620651245117
Batch 14/64 loss: 0.19901561737060547
Batch 15/64 loss: -0.09311819076538086
Batch 16/64 loss: -0.07046127319335938
Batch 17/64 loss: 0.1606125831604004
Batch 18/64 loss: -0.046511173248291016
Batch 19/64 loss: -0.04066133499145508
Batch 20/64 loss: 0.22410345077514648
Batch 21/64 loss: -0.043140411376953125
Batch 22/64 loss: 0.11275768280029297
Batch 23/64 loss: 0.20342683792114258
Batch 24/64 loss: -0.054053306579589844
Batch 25/64 loss: 0.30528688430786133
Batch 26/64 loss: 0.783503532409668
Batch 27/64 loss: 0.5392861366271973
Batch 28/64 loss: 0.039277076721191406
Batch 29/64 loss: -0.2415761947631836
Batch 30/64 loss: 0.24997282028198242
Batch 31/64 loss: 0.048374176025390625
Batch 32/64 loss: 0.1866154670715332
Batch 33/64 loss: -0.050187110900878906
Batch 34/64 loss: 0.18665552139282227
Batch 35/64 loss: 0.2733011245727539
Batch 36/64 loss: -0.26432180404663086
Batch 37/64 loss: -0.20716619491577148
Batch 38/64 loss: 0.2168259620666504
Batch 39/64 loss: 0.059015750885009766
Batch 40/64 loss: 0.16637516021728516
Batch 41/64 loss: 0.2894611358642578
Batch 42/64 loss: 0.3637428283691406
Batch 43/64 loss: 0.24206113815307617
Batch 44/64 loss: 0.04357337951660156
Batch 45/64 loss: 0.5263195037841797
Batch 46/64 loss: 0.13951730728149414
Batch 47/64 loss: 0.11591577529907227
Batch 48/64 loss: 0.2616868019104004
Batch 49/64 loss: 0.03610992431640625
Batch 50/64 loss: -0.15762615203857422
Batch 51/64 loss: 0.5778727531433105
Batch 52/64 loss: 0.2287273406982422
Batch 53/64 loss: 0.3843822479248047
Batch 54/64 loss: 0.3898892402648926
Batch 55/64 loss: 0.006004810333251953
Batch 56/64 loss: 0.2345752716064453
Batch 57/64 loss: -0.04478931427001953
Batch 58/64 loss: 0.0729222297668457
Batch 59/64 loss: 0.20398378372192383
Batch 60/64 loss: -0.05769824981689453
Batch 61/64 loss: 0.1381545066833496
Batch 62/64 loss: 0.2635002136230469
Batch 63/64 loss: 0.462033748626709
Batch 64/64 loss: -3.7697043418884277
Epoch 221  Train loss: 0.10383628209431967  Val loss: 0.18429502506846004
Saving best model, epoch: 221
Epoch 222
-------------------------------
Batch 1/64 loss: 0.21572542190551758
Batch 2/64 loss: 0.09074878692626953
Batch 3/64 loss: 0.13964319229125977
Batch 4/64 loss: 0.24125909805297852
Batch 5/64 loss: 0.27788686752319336
Batch 6/64 loss: 0.13001060485839844
Batch 7/64 loss: 0.022297382354736328
Batch 8/64 loss: 0.032073020935058594
Batch 9/64 loss: -0.009766578674316406
Batch 10/64 loss: 1.0541715621948242
Batch 11/64 loss: -0.040673255920410156
Batch 12/64 loss: 0.14419984817504883
Batch 13/64 loss: 0.41974878311157227
Batch 14/64 loss: 0.5385799407958984
Batch 15/64 loss: -0.08869457244873047
Batch 16/64 loss: 0.2678408622741699
Batch 17/64 loss: 0.06056690216064453
Batch 18/64 loss: 0.14957618713378906
Batch 19/64 loss: -0.13246631622314453
Batch 20/64 loss: 0.21813011169433594
Batch 21/64 loss: 0.41634130477905273
Batch 22/64 loss: 0.025408267974853516
Batch 23/64 loss: 0.17303800582885742
Batch 24/64 loss: 0.0238189697265625
Batch 25/64 loss: 0.021678447723388672
Batch 26/64 loss: 0.2844381332397461
Batch 27/64 loss: 0.07580709457397461
Batch 28/64 loss: 0.337615966796875
Batch 29/64 loss: 0.27489137649536133
Batch 30/64 loss: 0.09554910659790039
Batch 31/64 loss: -0.018959999084472656
Batch 32/64 loss: -0.08823347091674805
Batch 33/64 loss: 0.006908416748046875
Batch 34/64 loss: 0.221343994140625
Batch 35/64 loss: -0.1917433738708496
Batch 36/64 loss: -0.04496192932128906
Batch 37/64 loss: 0.16072416305541992
Batch 38/64 loss: -0.07695722579956055
Batch 39/64 loss: 0.47719621658325195
Batch 40/64 loss: -0.29103994369506836
Batch 41/64 loss: 0.17726802825927734
Batch 42/64 loss: 0.1019735336303711
Batch 43/64 loss: 0.5919466018676758
Batch 44/64 loss: 0.0727386474609375
Batch 45/64 loss: 0.629209041595459
Batch 46/64 loss: -0.10980510711669922
Batch 47/64 loss: 0.1363072395324707
Batch 48/64 loss: 0.5500044822692871
Batch 49/64 loss: -0.0023708343505859375
Batch 50/64 loss: -0.004809856414794922
Batch 51/64 loss: 0.22612810134887695
Batch 52/64 loss: 0.3773970603942871
Batch 53/64 loss: 0.2988433837890625
Batch 54/64 loss: 0.26780176162719727
Batch 55/64 loss: 0.10088253021240234
Batch 56/64 loss: 0.056659698486328125
Batch 57/64 loss: -0.08801746368408203
Batch 58/64 loss: 0.13658380508422852
Batch 59/64 loss: 0.15180587768554688
Batch 60/64 loss: -0.14652442932128906
Batch 61/64 loss: 0.0930318832397461
Batch 62/64 loss: 0.2380352020263672
Batch 63/64 loss: 0.35360145568847656
Batch 64/64 loss: -3.6853718757629395
Epoch 222  Train loss: 0.11071973501467237  Val loss: 0.36414191649132166
Epoch 223
-------------------------------
Batch 1/64 loss: 0.32511472702026367
Batch 2/64 loss: 0.28150510787963867
Batch 3/64 loss: -0.08445930480957031
Batch 4/64 loss: -0.10866355895996094
Batch 5/64 loss: 0.24463510513305664
Batch 6/64 loss: 0.028564453125
Batch 7/64 loss: -0.06978750228881836
Batch 8/64 loss: -0.1918172836303711
Batch 9/64 loss: 0.09393167495727539
Batch 10/64 loss: -0.12343835830688477
Batch 11/64 loss: 0.12882328033447266
Batch 12/64 loss: 0.21028566360473633
Batch 13/64 loss: -0.09313297271728516
Batch 14/64 loss: -0.03371000289916992
Batch 15/64 loss: -0.03380155563354492
Batch 16/64 loss: 0.4412670135498047
Batch 17/64 loss: 0.5101833343505859
Batch 18/64 loss: 0.04689598083496094
Batch 19/64 loss: 0.36925745010375977
Batch 20/64 loss: 0.31844377517700195
Batch 21/64 loss: 0.31206417083740234
Batch 22/64 loss: 0.05866718292236328
Batch 23/64 loss: 0.16320133209228516
Batch 24/64 loss: 0.20506763458251953
Batch 25/64 loss: 0.03612375259399414
Batch 26/64 loss: 0.08914995193481445
Batch 27/64 loss: 0.21462631225585938
Batch 28/64 loss: 0.6236104965209961
Batch 29/64 loss: 0.3559861183166504
Batch 30/64 loss: 0.18233013153076172
Batch 31/64 loss: -0.14739418029785156
Batch 32/64 loss: 0.38282012939453125
Batch 33/64 loss: 0.41629791259765625
Batch 34/64 loss: 0.009167194366455078
Batch 35/64 loss: 0.2679462432861328
Batch 36/64 loss: 0.1311478614807129
Batch 37/64 loss: 0.4583554267883301
Batch 38/64 loss: 0.38260984420776367
Batch 39/64 loss: -0.10999011993408203
Batch 40/64 loss: -0.2346663475036621
Batch 41/64 loss: 0.24178171157836914
Batch 42/64 loss: 0.28575897216796875
Batch 43/64 loss: 0.4165940284729004
Batch 44/64 loss: 0.11874771118164062
Batch 45/64 loss: 0.08108091354370117
Batch 46/64 loss: 0.10192394256591797
Batch 47/64 loss: 0.16620779037475586
Batch 48/64 loss: 0.1117558479309082
Batch 49/64 loss: -0.1327195167541504
Batch 50/64 loss: -0.16438865661621094
Batch 51/64 loss: -0.04339027404785156
Batch 52/64 loss: -0.009198188781738281
Batch 53/64 loss: 0.33531665802001953
Batch 54/64 loss: 0.2048935890197754
Batch 55/64 loss: -0.21318674087524414
Batch 56/64 loss: 0.09278488159179688
Batch 57/64 loss: -0.1368727684020996
Batch 58/64 loss: 0.19763517379760742
Batch 59/64 loss: 0.31456708908081055
Batch 60/64 loss: 0.09657001495361328
Batch 61/64 loss: 0.5627803802490234
Batch 62/64 loss: 0.32744359970092773
Batch 63/64 loss: -0.06130552291870117
Batch 64/64 loss: -3.4178924560546875
Epoch 223  Train loss: 0.10021300970339307  Val loss: 0.21455742485334783
Epoch 224
-------------------------------
Batch 1/64 loss: 0.16028165817260742
Batch 2/64 loss: 0.4141511917114258
Batch 3/64 loss: 0.06554269790649414
Batch 4/64 loss: 0.20653963088989258
Batch 5/64 loss: 0.07182741165161133
Batch 6/64 loss: 0.14484119415283203
Batch 7/64 loss: 0.006928443908691406
Batch 8/64 loss: 0.27002954483032227
Batch 9/64 loss: 0.32732105255126953
Batch 10/64 loss: 0.6868190765380859
Batch 11/64 loss: 0.0760655403137207
Batch 12/64 loss: -0.045569419860839844
Batch 13/64 loss: 0.49529361724853516
Batch 14/64 loss: 0.13861560821533203
Batch 15/64 loss: -0.10880756378173828
Batch 16/64 loss: 0.31577587127685547
Batch 17/64 loss: 0.3275413513183594
Batch 18/64 loss: 0.1311511993408203
Batch 19/64 loss: 0.18835735321044922
Batch 20/64 loss: 0.12750005722045898
Batch 21/64 loss: -0.09716510772705078
Batch 22/64 loss: -0.13569927215576172
Batch 23/64 loss: 0.6963891983032227
Batch 24/64 loss: -0.18948602676391602
Batch 25/64 loss: -0.11271953582763672
Batch 26/64 loss: 0.36815929412841797
Batch 27/64 loss: 0.061182498931884766
Batch 28/64 loss: -0.027679443359375
Batch 29/64 loss: 0.09981203079223633
Batch 30/64 loss: 0.3364906311035156
Batch 31/64 loss: 0.1275467872619629
Batch 32/64 loss: 0.06709527969360352
Batch 33/64 loss: -0.04689645767211914
Batch 34/64 loss: 0.40514326095581055
Batch 35/64 loss: 0.076812744140625
Batch 36/64 loss: 0.42373132705688477
Batch 37/64 loss: -0.07478713989257812
Batch 38/64 loss: 0.037607669830322266
Batch 39/64 loss: 0.11592531204223633
Batch 40/64 loss: -0.25902652740478516
Batch 41/64 loss: 0.9235720634460449
Batch 42/64 loss: -0.21766281127929688
Batch 43/64 loss: -0.05390214920043945
Batch 44/64 loss: 0.1980900764465332
Batch 45/64 loss: 0.1833171844482422
Batch 46/64 loss: 0.4184083938598633
Batch 47/64 loss: -0.020664215087890625
Batch 48/64 loss: 0.1854844093322754
Batch 49/64 loss: 0.13148880004882812
Batch 50/64 loss: 0.32016706466674805
Batch 51/64 loss: 0.28076839447021484
Batch 52/64 loss: 0.0004811286926269531
Batch 53/64 loss: 0.9216718673706055
Batch 54/64 loss: -0.3324756622314453
Batch 55/64 loss: 0.017787933349609375
Batch 56/64 loss: 0.31440067291259766
Batch 57/64 loss: -0.002318859100341797
Batch 58/64 loss: 0.021449565887451172
Batch 59/64 loss: 0.15621614456176758
Batch 60/64 loss: 0.49404382705688477
Batch 61/64 loss: 0.07085514068603516
Batch 62/64 loss: 0.3293113708496094
Batch 63/64 loss: -0.11443090438842773
Batch 64/64 loss: -3.289210796356201
Epoch 224  Train loss: 0.11971440595739027  Val loss: 0.34349141333930683
Epoch 225
-------------------------------
Batch 1/64 loss: 0.4873385429382324
Batch 2/64 loss: 0.05308389663696289
Batch 3/64 loss: 0.18665456771850586
Batch 4/64 loss: -0.09045791625976562
Batch 5/64 loss: 0.4524717330932617
Batch 6/64 loss: 0.6728529930114746
Batch 7/64 loss: 0.4727010726928711
Batch 8/64 loss: 0.09554147720336914
Batch 9/64 loss: 0.26807308197021484
Batch 10/64 loss: 0.4346446990966797
Batch 11/64 loss: 0.023950576782226562
Batch 12/64 loss: 0.26070594787597656
Batch 13/64 loss: 0.06445980072021484
Batch 14/64 loss: 0.44200944900512695
Batch 15/64 loss: 0.005740165710449219
Batch 16/64 loss: 0.06284809112548828
Batch 17/64 loss: 0.2883133888244629
Batch 18/64 loss: 0.3249073028564453
Batch 19/64 loss: 0.24779939651489258
Batch 20/64 loss: 0.30979299545288086
Batch 21/64 loss: 0.20577335357666016
Batch 22/64 loss: 0.08954095840454102
Batch 23/64 loss: 0.1627054214477539
Batch 24/64 loss: 0.10578775405883789
Batch 25/64 loss: 0.24253225326538086
Batch 26/64 loss: 0.22707700729370117
Batch 27/64 loss: 0.2544283866882324
Batch 28/64 loss: 0.0809473991394043
Batch 29/64 loss: 0.3512229919433594
Batch 30/64 loss: 0.18959569931030273
Batch 31/64 loss: 0.35003232955932617
Batch 32/64 loss: -0.05807161331176758
Batch 33/64 loss: 0.3639044761657715
Batch 34/64 loss: -0.20845699310302734
Batch 35/64 loss: -0.24940967559814453
Batch 36/64 loss: -0.042539119720458984
Batch 37/64 loss: 0.26165771484375
Batch 38/64 loss: 0.15541648864746094
Batch 39/64 loss: 0.32921791076660156
Batch 40/64 loss: 0.5044779777526855
Batch 41/64 loss: 0.043749332427978516
Batch 42/64 loss: -0.014071941375732422
Batch 43/64 loss: 0.19643640518188477
Batch 44/64 loss: 0.1159358024597168
Batch 45/64 loss: 0.2330937385559082
Batch 46/64 loss: 0.24647188186645508
Batch 47/64 loss: 0.005394935607910156
Batch 48/64 loss: 0.3578643798828125
Batch 49/64 loss: 0.1285867691040039
Batch 50/64 loss: -0.10341978073120117
Batch 51/64 loss: 0.29876184463500977
Batch 52/64 loss: 0.16357898712158203
Batch 53/64 loss: 0.2871837615966797
Batch 54/64 loss: 0.47940969467163086
Batch 55/64 loss: -0.044434547424316406
Batch 56/64 loss: 0.1865243911743164
Batch 57/64 loss: 0.014055728912353516
Batch 58/64 loss: 0.3618440628051758
Batch 59/64 loss: -0.002925395965576172
Batch 60/64 loss: 0.16141414642333984
Batch 61/64 loss: -0.15520620346069336
Batch 62/64 loss: 0.3684511184692383
Batch 63/64 loss: 0.15499401092529297
Batch 64/64 loss: -3.624502182006836
Epoch 225  Train loss: 0.1434445250268076  Val loss: 0.3306204281312084
Epoch 226
-------------------------------
Batch 1/64 loss: 0.3050827980041504
Batch 2/64 loss: -0.05608510971069336
Batch 3/64 loss: 0.02629375457763672
Batch 4/64 loss: 0.2006516456604004
Batch 5/64 loss: 0.3414278030395508
Batch 6/64 loss: -0.036858558654785156
Batch 7/64 loss: 0.9657726287841797
Batch 8/64 loss: -0.07825279235839844
Batch 9/64 loss: 0.20295429229736328
Batch 10/64 loss: 0.056265830993652344
Batch 11/64 loss: 0.6472649574279785
Batch 12/64 loss: 0.5817246437072754
Batch 13/64 loss: 0.005725383758544922
Batch 14/64 loss: -0.12648344039916992
Batch 15/64 loss: 0.25859975814819336
Batch 16/64 loss: 0.3476376533508301
Batch 17/64 loss: 0.1414780616760254
Batch 18/64 loss: -0.10505914688110352
Batch 19/64 loss: 0.3805208206176758
Batch 20/64 loss: 0.28264808654785156
Batch 21/64 loss: 0.7020635604858398
Batch 22/64 loss: 0.2602872848510742
Batch 23/64 loss: 0.27375316619873047
Batch 24/64 loss: 0.10203790664672852
Batch 25/64 loss: 0.8771414756774902
Batch 26/64 loss: 0.27739477157592773
Batch 27/64 loss: 0.6807332038879395
Batch 28/64 loss: -0.3175368309020996
Batch 29/64 loss: 0.09901094436645508
Batch 30/64 loss: 0.1295003890991211
Batch 31/64 loss: 0.1519327163696289
Batch 32/64 loss: 0.2728753089904785
Batch 33/64 loss: 0.11370420455932617
Batch 34/64 loss: 0.4069490432739258
Batch 35/64 loss: 0.29248046875
Batch 36/64 loss: 0.07343006134033203
Batch 37/64 loss: 0.12286949157714844
Batch 38/64 loss: -0.024662494659423828
Batch 39/64 loss: 0.07950305938720703
Batch 40/64 loss: -0.2062835693359375
Batch 41/64 loss: -0.04145383834838867
Batch 42/64 loss: 0.382753849029541
Batch 43/64 loss: 0.22990083694458008
Batch 44/64 loss: 0.12779474258422852
Batch 45/64 loss: 0.023941516876220703
Batch 46/64 loss: 0.2581925392150879
Batch 47/64 loss: -0.023773193359375
Batch 48/64 loss: -0.06438684463500977
Batch 49/64 loss: -0.1472001075744629
Batch 50/64 loss: 0.18509387969970703
Batch 51/64 loss: 0.27143096923828125
Batch 52/64 loss: 0.02504110336303711
Batch 53/64 loss: 0.12042617797851562
Batch 54/64 loss: 0.04913473129272461
Batch 55/64 loss: -0.08661937713623047
Batch 56/64 loss: 0.06589126586914062
Batch 57/64 loss: -0.08143997192382812
Batch 58/64 loss: -0.07366704940795898
Batch 59/64 loss: 0.09534835815429688
Batch 60/64 loss: 0.37540578842163086
Batch 61/64 loss: 0.017040252685546875
Batch 62/64 loss: 0.37686586380004883
Batch 63/64 loss: -0.20596981048583984
Batch 64/64 loss: -3.8837594985961914
Epoch 226  Train loss: 0.1203988280950808  Val loss: 0.1586696061072071
Saving best model, epoch: 226
Epoch 227
-------------------------------
Batch 1/64 loss: 0.07866811752319336
Batch 2/64 loss: 0.17902660369873047
Batch 3/64 loss: 0.013186931610107422
Batch 4/64 loss: 0.14966869354248047
Batch 5/64 loss: 0.42664337158203125
Batch 6/64 loss: 0.04914426803588867
Batch 7/64 loss: -0.21535730361938477
Batch 8/64 loss: -0.008898735046386719
Batch 9/64 loss: 0.014394283294677734
Batch 10/64 loss: 0.1793375015258789
Batch 11/64 loss: 0.11795854568481445
Batch 12/64 loss: 0.20532655715942383
Batch 13/64 loss: 0.17333650588989258
Batch 14/64 loss: 0.4843573570251465
Batch 15/64 loss: 0.09478282928466797
Batch 16/64 loss: -0.05913734436035156
Batch 17/64 loss: -0.3053016662597656
Batch 18/64 loss: 0.0395660400390625
Batch 19/64 loss: 0.19086503982543945
Batch 20/64 loss: 0.13459396362304688
Batch 21/64 loss: -0.11801862716674805
Batch 22/64 loss: 0.19960260391235352
Batch 23/64 loss: 0.5527443885803223
Batch 24/64 loss: -0.12490463256835938
Batch 25/64 loss: 0.40746641159057617
Batch 26/64 loss: -0.10403585433959961
Batch 27/64 loss: 0.05693960189819336
Batch 28/64 loss: 0.08446359634399414
Batch 29/64 loss: -0.010854721069335938
Batch 30/64 loss: -0.08647489547729492
Batch 31/64 loss: -0.07090282440185547
Batch 32/64 loss: 0.15540552139282227
Batch 33/64 loss: -0.06771469116210938
Batch 34/64 loss: -0.09205245971679688
Batch 35/64 loss: 0.0341339111328125
Batch 36/64 loss: 0.19195985794067383
Batch 37/64 loss: 0.4801912307739258
Batch 38/64 loss: 0.5162520408630371
Batch 39/64 loss: -0.26442623138427734
Batch 40/64 loss: 0.7800021171569824
Batch 41/64 loss: 0.044086456298828125
Batch 42/64 loss: 0.2888646125793457
Batch 43/64 loss: 0.22568035125732422
Batch 44/64 loss: 0.08717060089111328
Batch 45/64 loss: 0.26038074493408203
Batch 46/64 loss: 0.2908821105957031
Batch 47/64 loss: -0.11806488037109375
Batch 48/64 loss: 0.05325508117675781
Batch 49/64 loss: 0.17676782608032227
Batch 50/64 loss: -0.07265615463256836
Batch 51/64 loss: 0.29540300369262695
Batch 52/64 loss: 0.27440357208251953
Batch 53/64 loss: 0.04212522506713867
Batch 54/64 loss: 0.41651201248168945
Batch 55/64 loss: 0.11521196365356445
Batch 56/64 loss: 0.34334802627563477
Batch 57/64 loss: 0.20644235610961914
Batch 58/64 loss: 0.1001119613647461
Batch 59/64 loss: 0.3250432014465332
Batch 60/64 loss: -0.10122823715209961
Batch 61/64 loss: 0.33771657943725586
Batch 62/64 loss: 0.15393972396850586
Batch 63/64 loss: 0.2050333023071289
Batch 64/64 loss: -3.9068603515625
Epoch 227  Train loss: 0.08599564047420725  Val loss: 0.2664773292148236
Epoch 228
-------------------------------
Batch 1/64 loss: -0.0641331672668457
Batch 2/64 loss: -0.000621795654296875
Batch 3/64 loss: 0.5714230537414551
Batch 4/64 loss: 0.5386390686035156
Batch 5/64 loss: 0.26258087158203125
Batch 6/64 loss: -0.06182050704956055
Batch 7/64 loss: 0.35396528244018555
Batch 8/64 loss: 0.15364456176757812
Batch 9/64 loss: 0.3035464286804199
Batch 10/64 loss: -0.040273189544677734
Batch 11/64 loss: -0.010328292846679688
Batch 12/64 loss: 0.33797216415405273
Batch 13/64 loss: 0.24710941314697266
Batch 14/64 loss: 0.16679811477661133
Batch 15/64 loss: 0.5195260047912598
Batch 16/64 loss: 0.20963096618652344
Batch 17/64 loss: -0.2470550537109375
Batch 18/64 loss: -0.07598400115966797
Batch 19/64 loss: -0.23401355743408203
Batch 20/64 loss: 0.17851924896240234
Batch 21/64 loss: -0.21898174285888672
Batch 22/64 loss: 0.02058887481689453
Batch 23/64 loss: 0.43750667572021484
Batch 24/64 loss: -0.2367696762084961
Batch 25/64 loss: 0.01828289031982422
Batch 26/64 loss: 0.3522782325744629
Batch 27/64 loss: -0.1461925506591797
Batch 28/64 loss: -0.0673213005065918
Batch 29/64 loss: -0.1588592529296875
Batch 30/64 loss: 0.16068077087402344
Batch 31/64 loss: 0.023939132690429688
Batch 32/64 loss: -0.022259235382080078
Batch 33/64 loss: 0.29465293884277344
Batch 34/64 loss: 0.2880101203918457
Batch 35/64 loss: -0.07017660140991211
Batch 36/64 loss: 0.08893060684204102
Batch 37/64 loss: -0.015493392944335938
Batch 38/64 loss: 0.2966732978820801
Batch 39/64 loss: -0.2074146270751953
Batch 40/64 loss: 0.14344406127929688
Batch 41/64 loss: 0.2391057014465332
Batch 42/64 loss: 0.36045265197753906
Batch 43/64 loss: -0.05304288864135742
Batch 44/64 loss: 0.06534862518310547
Batch 45/64 loss: -0.06426095962524414
Batch 46/64 loss: 0.23729944229125977
Batch 47/64 loss: 0.3664569854736328
Batch 48/64 loss: 0.2644691467285156
Batch 49/64 loss: -0.2467656135559082
Batch 50/64 loss: -0.21431589126586914
Batch 51/64 loss: 0.278627872467041
Batch 52/64 loss: -0.11220645904541016
Batch 53/64 loss: 0.2717752456665039
Batch 54/64 loss: 0.1951131820678711
Batch 55/64 loss: -0.1629939079284668
Batch 56/64 loss: -0.028290271759033203
Batch 57/64 loss: 0.23604536056518555
Batch 58/64 loss: 0.6716980934143066
Batch 59/64 loss: -0.026517868041992188
Batch 60/64 loss: -0.05322837829589844
Batch 61/64 loss: 0.06334352493286133
Batch 62/64 loss: 0.3296666145324707
Batch 63/64 loss: 0.1921381950378418
Batch 64/64 loss: -2.998389720916748
Epoch 228  Train loss: 0.07296895606845033  Val loss: 0.14085705419586286
Saving best model, epoch: 228
Epoch 229
-------------------------------
Batch 1/64 loss: 0.04489612579345703
Batch 2/64 loss: 0.058695316314697266
Batch 3/64 loss: 0.23970985412597656
Batch 4/64 loss: -0.10454988479614258
Batch 5/64 loss: -0.048592567443847656
Batch 6/64 loss: 0.4246368408203125
Batch 7/64 loss: -0.01708984375
Batch 8/64 loss: 0.13279151916503906
Batch 9/64 loss: -0.05543804168701172
Batch 10/64 loss: 0.2517571449279785
Batch 11/64 loss: -0.07484102249145508
Batch 12/64 loss: -0.011426925659179688
Batch 13/64 loss: -0.011006832122802734
Batch 14/64 loss: -0.007025241851806641
Batch 15/64 loss: -0.023042678833007812
Batch 16/64 loss: -0.05495405197143555
Batch 17/64 loss: 0.16270875930786133
Batch 18/64 loss: -0.0192413330078125
Batch 19/64 loss: 0.24770498275756836
Batch 20/64 loss: 0.06953144073486328
Batch 21/64 loss: -0.2039647102355957
Batch 22/64 loss: 0.11656999588012695
Batch 23/64 loss: -0.08795881271362305
Batch 24/64 loss: -0.18285274505615234
Batch 25/64 loss: 0.19037199020385742
Batch 26/64 loss: 0.013896465301513672
Batch 27/64 loss: 0.16001367568969727
Batch 28/64 loss: -0.05626869201660156
Batch 29/64 loss: -0.0349421501159668
Batch 30/64 loss: -0.05944490432739258
Batch 31/64 loss: 0.19701576232910156
Batch 32/64 loss: 0.2910928726196289
Batch 33/64 loss: -0.06356954574584961
Batch 34/64 loss: 0.9687356948852539
Batch 35/64 loss: 0.3420267105102539
Batch 36/64 loss: -0.12108039855957031
Batch 37/64 loss: -0.36856555938720703
Batch 38/64 loss: -0.015413761138916016
Batch 39/64 loss: -0.12341451644897461
Batch 40/64 loss: 0.04506731033325195
Batch 41/64 loss: -0.2877182960510254
Batch 42/64 loss: 0.5228643417358398
Batch 43/64 loss: -0.10864496231079102
Batch 44/64 loss: -0.07241964340209961
Batch 45/64 loss: 0.30420875549316406
Batch 46/64 loss: 0.2599649429321289
Batch 47/64 loss: 0.20582914352416992
Batch 48/64 loss: 0.416475772857666
Batch 49/64 loss: 0.6562151908874512
Batch 50/64 loss: 0.13133907318115234
Batch 51/64 loss: 0.053381919860839844
Batch 52/64 loss: 0.05154848098754883
Batch 53/64 loss: -0.10002756118774414
Batch 54/64 loss: 0.19829988479614258
Batch 55/64 loss: 0.27509212493896484
Batch 56/64 loss: -0.017136573791503906
Batch 57/64 loss: 0.12394189834594727
Batch 58/64 loss: 0.340883731842041
Batch 59/64 loss: 0.24722862243652344
Batch 60/64 loss: 0.18086862564086914
Batch 61/64 loss: 0.3581581115722656
Batch 62/64 loss: -0.06763267517089844
Batch 63/64 loss: 0.25446414947509766
Batch 64/64 loss: -2.9999094009399414
Epoch 229  Train loss: 0.061016333336923634  Val loss: 0.2242346956967488
Epoch 230
-------------------------------
Batch 1/64 loss: 0.19898080825805664
Batch 2/64 loss: 0.19768953323364258
Batch 3/64 loss: 0.14174127578735352
Batch 4/64 loss: 0.40317583084106445
Batch 5/64 loss: 0.07473373413085938
Batch 6/64 loss: -0.168212890625
Batch 7/64 loss: -0.014367103576660156
Batch 8/64 loss: -0.07221078872680664
Batch 9/64 loss: -0.14061307907104492
Batch 10/64 loss: 0.17293167114257812
Batch 11/64 loss: 0.10372447967529297
Batch 12/64 loss: 0.2042989730834961
Batch 13/64 loss: -0.3466024398803711
Batch 14/64 loss: -0.1469736099243164
Batch 15/64 loss: 0.3462100028991699
Batch 16/64 loss: 0.2903757095336914
Batch 17/64 loss: -0.31157684326171875
Batch 18/64 loss: -0.030812740325927734
Batch 19/64 loss: 0.22422266006469727
Batch 20/64 loss: -0.1201629638671875
Batch 21/64 loss: -0.07414913177490234
Batch 22/64 loss: -0.20160341262817383
Batch 23/64 loss: -0.14187908172607422
Batch 24/64 loss: 0.14559650421142578
Batch 25/64 loss: 0.06456470489501953
Batch 26/64 loss: -0.04192686080932617
Batch 27/64 loss: 0.3379068374633789
Batch 28/64 loss: 0.25190114974975586
Batch 29/64 loss: 0.2726140022277832
Batch 30/64 loss: 0.1771383285522461
Batch 31/64 loss: 0.007476806640625
Batch 32/64 loss: 0.23950910568237305
Batch 33/64 loss: 0.1857767105102539
Batch 34/64 loss: 0.06290721893310547
Batch 35/64 loss: 0.4196896553039551
Batch 36/64 loss: 0.22287607192993164
Batch 37/64 loss: 0.10460138320922852
Batch 38/64 loss: -0.19032764434814453
Batch 39/64 loss: -0.08719635009765625
Batch 40/64 loss: -0.1466531753540039
Batch 41/64 loss: 0.05995368957519531
Batch 42/64 loss: 0.12115192413330078
Batch 43/64 loss: 0.3543691635131836
Batch 44/64 loss: 0.00202178955078125
Batch 45/64 loss: -0.1295185089111328
Batch 46/64 loss: 0.11934995651245117
Batch 47/64 loss: -0.21682119369506836
Batch 48/64 loss: 0.2170853614807129
Batch 49/64 loss: 0.457888126373291
Batch 50/64 loss: 0.016590118408203125
Batch 51/64 loss: 0.4032478332519531
Batch 52/64 loss: 0.02865886688232422
Batch 53/64 loss: 0.19463729858398438
Batch 54/64 loss: 0.45934629440307617
Batch 55/64 loss: -0.1718130111694336
Batch 56/64 loss: 0.2769484519958496
Batch 57/64 loss: 0.02981281280517578
Batch 58/64 loss: 0.27486324310302734
Batch 59/64 loss: 0.31919050216674805
Batch 60/64 loss: 0.09264183044433594
Batch 61/64 loss: 0.10945510864257812
Batch 62/64 loss: 0.1628570556640625
Batch 63/64 loss: -0.22445201873779297
Batch 64/64 loss: -3.4598140716552734
Epoch 230  Train loss: 0.046713398952110144  Val loss: 0.1487938661345911
Epoch 231
-------------------------------
Batch 1/64 loss: 0.3868560791015625
Batch 2/64 loss: 0.04390668869018555
Batch 3/64 loss: 0.14834117889404297
Batch 4/64 loss: -0.40730714797973633
Batch 5/64 loss: -0.002346515655517578
Batch 6/64 loss: -0.008713245391845703
Batch 7/64 loss: -0.027081966400146484
Batch 8/64 loss: 0.006058216094970703
Batch 9/64 loss: -0.05514383316040039
Batch 10/64 loss: 0.5373268127441406
Batch 11/64 loss: 0.11855030059814453
Batch 12/64 loss: -0.1104741096496582
Batch 13/64 loss: 0.04706144332885742
Batch 14/64 loss: 0.22894048690795898
Batch 15/64 loss: -0.33068370819091797
Batch 16/64 loss: -0.11572551727294922
Batch 17/64 loss: -0.2349715232849121
Batch 18/64 loss: -0.14581012725830078
Batch 19/64 loss: -0.010151386260986328
Batch 20/64 loss: -0.02444744110107422
Batch 21/64 loss: -0.13441085815429688
Batch 22/64 loss: 0.44066524505615234
Batch 23/64 loss: -0.0913853645324707
Batch 24/64 loss: 0.46618175506591797
Batch 25/64 loss: -0.18938636779785156
Batch 26/64 loss: -0.21898651123046875
Batch 27/64 loss: -0.20884275436401367
Batch 28/64 loss: 0.2671670913696289
Batch 29/64 loss: 0.4613800048828125
Batch 30/64 loss: 0.42656373977661133
Batch 31/64 loss: 0.2834773063659668
Batch 32/64 loss: 0.03955078125
Batch 33/64 loss: 0.120025634765625
Batch 34/64 loss: -0.03528594970703125
Batch 35/64 loss: -0.14056825637817383
Batch 36/64 loss: 0.17539691925048828
Batch 37/64 loss: 0.10535573959350586
Batch 38/64 loss: 0.31563472747802734
Batch 39/64 loss: -0.0049571990966796875
Batch 40/64 loss: -0.12539291381835938
Batch 41/64 loss: 0.03717613220214844
Batch 42/64 loss: 0.05478525161743164
Batch 43/64 loss: 0.2212080955505371
Batch 44/64 loss: -0.08283042907714844
Batch 45/64 loss: 0.11543846130371094
Batch 46/64 loss: 0.12631607055664062
Batch 47/64 loss: 0.25530576705932617
Batch 48/64 loss: 0.16509485244750977
Batch 49/64 loss: -0.13315057754516602
Batch 50/64 loss: 0.10963201522827148
Batch 51/64 loss: 0.07390451431274414
Batch 52/64 loss: -0.25980043411254883
Batch 53/64 loss: -0.03512716293334961
Batch 54/64 loss: -0.13098478317260742
Batch 55/64 loss: 0.4049253463745117
Batch 56/64 loss: 0.060338497161865234
Batch 57/64 loss: -0.20649099349975586
Batch 58/64 loss: 0.0794830322265625
Batch 59/64 loss: 0.11389589309692383
Batch 60/64 loss: 0.20447587966918945
Batch 61/64 loss: 0.2556486129760742
Batch 62/64 loss: 0.33512449264526367
Batch 63/64 loss: 0.017619609832763672
Batch 64/64 loss: -4.127476692199707
Epoch 231  Train loss: 0.010709773792940028  Val loss: 0.12710193752013532
Saving best model, epoch: 231
Epoch 232
-------------------------------
Batch 1/64 loss: 0.05678129196166992
Batch 2/64 loss: 0.09802389144897461
Batch 3/64 loss: 0.038367271423339844
Batch 4/64 loss: 0.2912588119506836
Batch 5/64 loss: -0.2502574920654297
Batch 6/64 loss: 0.02659749984741211
Batch 7/64 loss: -0.12515926361083984
Batch 8/64 loss: -0.07107353210449219
Batch 9/64 loss: 0.040671348571777344
Batch 10/64 loss: 0.215576171875
Batch 11/64 loss: 0.1098165512084961
Batch 12/64 loss: -0.39901018142700195
Batch 13/64 loss: -0.26327037811279297
Batch 14/64 loss: -0.23915863037109375
Batch 15/64 loss: 0.39286231994628906
Batch 16/64 loss: 0.09453868865966797
Batch 17/64 loss: 0.03362417221069336
Batch 18/64 loss: -0.2778358459472656
Batch 19/64 loss: 0.3525662422180176
Batch 20/64 loss: -0.023031234741210938
Batch 21/64 loss: -0.31332969665527344
Batch 22/64 loss: -0.1913127899169922
Batch 23/64 loss: 0.028547286987304688
Batch 24/64 loss: -0.2480316162109375
Batch 25/64 loss: 0.20336294174194336
Batch 26/64 loss: -0.09652423858642578
Batch 27/64 loss: 0.04062175750732422
Batch 28/64 loss: -0.08487796783447266
Batch 29/64 loss: 0.1453394889831543
Batch 30/64 loss: 0.17757320404052734
Batch 31/64 loss: 0.1195220947265625
Batch 32/64 loss: 0.034618377685546875
Batch 33/64 loss: -0.21570587158203125
Batch 34/64 loss: -0.003471851348876953
Batch 35/64 loss: -0.06009626388549805
Batch 36/64 loss: 0.06924295425415039
Batch 37/64 loss: 0.04507255554199219
Batch 38/64 loss: -0.029119014739990234
Batch 39/64 loss: 0.04797935485839844
Batch 40/64 loss: 0.32906293869018555
Batch 41/64 loss: 0.1657552719116211
Batch 42/64 loss: -0.2518181800842285
Batch 43/64 loss: 0.14238929748535156
Batch 44/64 loss: 0.3787965774536133
Batch 45/64 loss: 0.33982229232788086
Batch 46/64 loss: -0.1533050537109375
Batch 47/64 loss: -0.15917110443115234
Batch 48/64 loss: 0.12568330764770508
Batch 49/64 loss: 0.19512510299682617
Batch 50/64 loss: 0.17544078826904297
Batch 51/64 loss: -0.051873207092285156
Batch 52/64 loss: 0.20372629165649414
Batch 53/64 loss: 0.18298625946044922
Batch 54/64 loss: -0.19826316833496094
Batch 55/64 loss: 0.3545675277709961
Batch 56/64 loss: 0.1731114387512207
Batch 57/64 loss: 0.1327800750732422
Batch 58/64 loss: -0.06815862655639648
Batch 59/64 loss: 0.16513347625732422
Batch 60/64 loss: 0.32838869094848633
Batch 61/64 loss: -0.14751529693603516
Batch 62/64 loss: -0.03179359436035156
Batch 63/64 loss: -0.3202018737792969
Batch 64/64 loss: -3.332211494445801
Epoch 232  Train loss: -0.011250054602529489  Val loss: 0.10818891754674748
Saving best model, epoch: 232
Epoch 233
-------------------------------
Batch 1/64 loss: 0.10473251342773438
Batch 2/64 loss: 0.03930950164794922
Batch 3/64 loss: -0.21568965911865234
Batch 4/64 loss: -0.05956459045410156
Batch 5/64 loss: 0.38229942321777344
Batch 6/64 loss: -0.03570699691772461
Batch 7/64 loss: 0.0614171028137207
Batch 8/64 loss: -0.2073502540588379
Batch 9/64 loss: 0.1336536407470703
Batch 10/64 loss: 0.12462425231933594
Batch 11/64 loss: -0.09795427322387695
Batch 12/64 loss: -0.07657480239868164
Batch 13/64 loss: -0.027335166931152344
Batch 14/64 loss: -0.08761787414550781
Batch 15/64 loss: -0.33161163330078125
Batch 16/64 loss: -0.16536903381347656
Batch 17/64 loss: -0.173248291015625
Batch 18/64 loss: 0.14279556274414062
Batch 19/64 loss: 0.03699493408203125
Batch 20/64 loss: 0.07805204391479492
Batch 21/64 loss: -0.08660078048706055
Batch 22/64 loss: 0.25137948989868164
Batch 23/64 loss: 0.773597240447998
Batch 24/64 loss: -0.3116641044616699
Batch 25/64 loss: 0.43633460998535156
Batch 26/64 loss: -0.09140968322753906
Batch 27/64 loss: -0.11708927154541016
Batch 28/64 loss: 0.019464492797851562
Batch 29/64 loss: 0.0821537971496582
Batch 30/64 loss: -0.09406185150146484
Batch 31/64 loss: 0.1750802993774414
Batch 32/64 loss: 0.09296512603759766
Batch 33/64 loss: 0.08482646942138672
Batch 34/64 loss: -0.21471786499023438
Batch 35/64 loss: 0.1185140609741211
Batch 36/64 loss: 0.025670528411865234
Batch 37/64 loss: 0.014685630798339844
Batch 38/64 loss: 0.5293359756469727
Batch 39/64 loss: -0.3399796485900879
Batch 40/64 loss: -0.29149532318115234
Batch 41/64 loss: 0.17038488388061523
Batch 42/64 loss: 0.13301515579223633
Batch 43/64 loss: 0.5556521415710449
Batch 44/64 loss: -0.382049560546875
Batch 45/64 loss: 0.023432254791259766
Batch 46/64 loss: 0.0704493522644043
Batch 47/64 loss: -0.4358243942260742
Batch 48/64 loss: 0.42484045028686523
Batch 49/64 loss: 0.4884481430053711
Batch 50/64 loss: -0.06921958923339844
Batch 51/64 loss: 0.020194053649902344
Batch 52/64 loss: 0.08479833602905273
Batch 53/64 loss: 0.3356742858886719
Batch 54/64 loss: 0.2623896598815918
Batch 55/64 loss: -0.12037944793701172
Batch 56/64 loss: 0.07263994216918945
Batch 57/64 loss: -0.28417396545410156
Batch 58/64 loss: 0.12126016616821289
Batch 59/64 loss: 0.23307037353515625
Batch 60/64 loss: -0.05904960632324219
Batch 61/64 loss: 0.06687736511230469
Batch 62/64 loss: 0.1812448501586914
Batch 63/64 loss: -0.2009425163269043
Batch 64/64 loss: -2.965698719024658
Epoch 233  Train loss: 0.002373394311643114  Val loss: 0.09229328378369309
Saving best model, epoch: 233
Epoch 234
-------------------------------
Batch 1/64 loss: -0.029060840606689453
Batch 2/64 loss: -0.0014104843139648438
Batch 3/64 loss: 0.09583854675292969
Batch 4/64 loss: 0.020670413970947266
Batch 5/64 loss: -0.1276836395263672
Batch 6/64 loss: -0.09556913375854492
Batch 7/64 loss: 0.4047889709472656
Batch 8/64 loss: 0.3772463798522949
Batch 9/64 loss: 0.10521221160888672
Batch 10/64 loss: 0.1533827781677246
Batch 11/64 loss: -0.13672924041748047
Batch 12/64 loss: -0.17577362060546875
Batch 13/64 loss: 0.06926536560058594
Batch 14/64 loss: 0.26958131790161133
Batch 15/64 loss: -0.1042180061340332
Batch 16/64 loss: 0.15953636169433594
Batch 17/64 loss: 0.2204117774963379
Batch 18/64 loss: 0.1697239875793457
Batch 19/64 loss: 0.07742595672607422
Batch 20/64 loss: -0.20419645309448242
Batch 21/64 loss: -0.2998347282409668
Batch 22/64 loss: -0.2037057876586914
Batch 23/64 loss: -0.31195640563964844
Batch 24/64 loss: 0.1457819938659668
Batch 25/64 loss: -0.06961727142333984
Batch 26/64 loss: 0.01724529266357422
Batch 27/64 loss: -0.1602773666381836
Batch 28/64 loss: 0.0926675796508789
Batch 29/64 loss: 0.21024179458618164
Batch 30/64 loss: -0.42792320251464844
Batch 31/64 loss: 0.0073604583740234375
Batch 32/64 loss: -0.10396289825439453
Batch 33/64 loss: -0.11761474609375
Batch 34/64 loss: -0.18546581268310547
Batch 35/64 loss: 0.17702245712280273
Batch 36/64 loss: 0.3319888114929199
Batch 37/64 loss: 0.21572017669677734
Batch 38/64 loss: -0.1683659553527832
Batch 39/64 loss: 0.12375450134277344
Batch 40/64 loss: -0.031044960021972656
Batch 41/64 loss: -0.06439924240112305
Batch 42/64 loss: -0.04168510437011719
Batch 43/64 loss: -0.22469425201416016
Batch 44/64 loss: -0.08648014068603516
Batch 45/64 loss: -0.058110713958740234
Batch 46/64 loss: 0.21179580688476562
Batch 47/64 loss: 0.37105846405029297
Batch 48/64 loss: 0.0489497184753418
Batch 49/64 loss: 0.10400676727294922
Batch 50/64 loss: -0.23724699020385742
Batch 51/64 loss: -0.12615585327148438
Batch 52/64 loss: -0.2236652374267578
Batch 53/64 loss: -0.12003135681152344
Batch 54/64 loss: 0.045325279235839844
Batch 55/64 loss: 0.0454096794128418
Batch 56/64 loss: 0.42248058319091797
Batch 57/64 loss: -0.13083648681640625
Batch 58/64 loss: 0.16222763061523438
Batch 59/64 loss: -0.018197059631347656
Batch 60/64 loss: 0.14333677291870117
Batch 61/64 loss: 0.2652091979980469
Batch 62/64 loss: -0.07551813125610352
Batch 63/64 loss: 0.09148359298706055
Batch 64/64 loss: -3.4349775314331055
Epoch 234  Train loss: -0.024808057149251304  Val loss: 0.018346072062593966
Saving best model, epoch: 234
Epoch 235
-------------------------------
Batch 1/64 loss: 0.10170936584472656
Batch 2/64 loss: -0.08867740631103516
Batch 3/64 loss: -0.4836282730102539
Batch 4/64 loss: 0.309201717376709
Batch 5/64 loss: 0.02093219757080078
Batch 6/64 loss: 0.02714061737060547
Batch 7/64 loss: -0.12421607971191406
Batch 8/64 loss: -0.1311626434326172
Batch 9/64 loss: -0.2522420883178711
Batch 10/64 loss: -0.05365753173828125
Batch 11/64 loss: 0.3832244873046875
Batch 12/64 loss: -0.21857833862304688
Batch 13/64 loss: 0.0955810546875
Batch 14/64 loss: -0.2397632598876953
Batch 15/64 loss: 0.3300600051879883
Batch 16/64 loss: -0.16428375244140625
Batch 17/64 loss: -0.20885181427001953
Batch 18/64 loss: -0.0789494514465332
Batch 19/64 loss: -0.12192440032958984
Batch 20/64 loss: 0.06194114685058594
Batch 21/64 loss: -0.14164257049560547
Batch 22/64 loss: 0.2612948417663574
Batch 23/64 loss: -0.29227447509765625
Batch 24/64 loss: -0.405426025390625
Batch 25/64 loss: 0.33988189697265625
Batch 26/64 loss: 0.5697407722473145
Batch 27/64 loss: -0.0027070045471191406
Batch 28/64 loss: -0.22629117965698242
Batch 29/64 loss: 0.2165975570678711
Batch 30/64 loss: 0.10606193542480469
Batch 31/64 loss: 0.393033504486084
Batch 32/64 loss: 0.08074760437011719
Batch 33/64 loss: -0.1036062240600586
Batch 34/64 loss: 0.25231027603149414
Batch 35/64 loss: -0.06695032119750977
Batch 36/64 loss: 0.10000085830688477
Batch 37/64 loss: -0.3648557662963867
Batch 38/64 loss: 0.11261224746704102
Batch 39/64 loss: 0.15139150619506836
Batch 40/64 loss: 0.23204278945922852
Batch 41/64 loss: -0.3199892044067383
Batch 42/64 loss: -0.017844200134277344
Batch 43/64 loss: 0.11724662780761719
Batch 44/64 loss: 0.5198974609375
Batch 45/64 loss: -0.33495426177978516
Batch 46/64 loss: 0.0017609596252441406
Batch 47/64 loss: -0.24733781814575195
Batch 48/64 loss: -0.05101203918457031
Batch 49/64 loss: -0.21393203735351562
Batch 50/64 loss: -0.23797035217285156
Batch 51/64 loss: -0.007102012634277344
Batch 52/64 loss: 0.321624755859375
Batch 53/64 loss: 0.12002754211425781
Batch 54/64 loss: -0.3092002868652344
Batch 55/64 loss: -0.05000638961791992
Batch 56/64 loss: 0.6098251342773438
Batch 57/64 loss: -0.07715702056884766
Batch 58/64 loss: 0.20106887817382812
Batch 59/64 loss: -0.02976369857788086
Batch 60/64 loss: 0.25791120529174805
Batch 61/64 loss: -0.0783395767211914
Batch 62/64 loss: -0.20839691162109375
Batch 63/64 loss: 0.38067102432250977
Batch 64/64 loss: -2.8585128784179688
Epoch 235  Train loss: -0.022290809481751685  Val loss: 0.4768834851451756
Epoch 236
-------------------------------
Batch 1/64 loss: 0.033631324768066406
Batch 2/64 loss: -0.1896967887878418
Batch 3/64 loss: 0.2067708969116211
Batch 4/64 loss: 0.08594989776611328
Batch 5/64 loss: 0.01822185516357422
Batch 6/64 loss: 0.017313480377197266
Batch 7/64 loss: 0.17096567153930664
Batch 8/64 loss: -0.1527090072631836
Batch 9/64 loss: -0.08196830749511719
Batch 10/64 loss: 0.5283637046813965
Batch 11/64 loss: 0.1701064109802246
Batch 12/64 loss: 0.07485723495483398
Batch 13/64 loss: -0.25386810302734375
Batch 14/64 loss: 0.06449604034423828
Batch 15/64 loss: 0.13272333145141602
Batch 16/64 loss: -0.2339916229248047
Batch 17/64 loss: -0.20348882675170898
Batch 18/64 loss: 0.15890121459960938
Batch 19/64 loss: -0.047104835510253906
Batch 20/64 loss: -0.09575557708740234
Batch 21/64 loss: 0.07823657989501953
Batch 22/64 loss: 0.13672351837158203
Batch 23/64 loss: 0.3643941879272461
Batch 24/64 loss: 0.05231189727783203
Batch 25/64 loss: 0.12709474563598633
Batch 26/64 loss: -0.020353317260742188
Batch 27/64 loss: 0.40288782119750977
Batch 28/64 loss: 0.4235076904296875
Batch 29/64 loss: -0.06362676620483398
Batch 30/64 loss: 0.2407684326171875
Batch 31/64 loss: -0.09954547882080078
Batch 32/64 loss: 0.2676982879638672
Batch 33/64 loss: 0.018451690673828125
Batch 34/64 loss: 0.05954933166503906
Batch 35/64 loss: 0.026572227478027344
Batch 36/64 loss: 0.16845369338989258
Batch 37/64 loss: 0.1455554962158203
Batch 38/64 loss: -0.004076957702636719
Batch 39/64 loss: -8.678436279296875e-05
Batch 40/64 loss: 0.003403186798095703
Batch 41/64 loss: 0.23374557495117188
Batch 42/64 loss: 0.3030853271484375
Batch 43/64 loss: -0.09679698944091797
Batch 44/64 loss: -0.05463838577270508
Batch 45/64 loss: 0.17690086364746094
Batch 46/64 loss: 0.0867767333984375
Batch 47/64 loss: 0.5136723518371582
Batch 48/64 loss: 0.3048563003540039
Batch 49/64 loss: 0.12627553939819336
Batch 50/64 loss: -0.14232730865478516
Batch 51/64 loss: 0.7382264137268066
Batch 52/64 loss: -0.23276042938232422
Batch 53/64 loss: 0.11834239959716797
Batch 54/64 loss: 0.492124080657959
Batch 55/64 loss: -0.12287616729736328
Batch 56/64 loss: 0.4698977470397949
Batch 57/64 loss: 0.02930164337158203
Batch 58/64 loss: -0.0867776870727539
Batch 59/64 loss: -0.31702232360839844
Batch 60/64 loss: 0.07615852355957031
Batch 61/64 loss: 0.11959266662597656
Batch 62/64 loss: -0.1866741180419922
Batch 63/64 loss: -0.19870662689208984
Batch 64/64 loss: -3.722635269165039
Epoch 236  Train loss: 0.03592215145335478  Val loss: 0.22852679380436533
Epoch 237
-------------------------------
Batch 1/64 loss: -0.08627986907958984
Batch 2/64 loss: -0.10852766036987305
Batch 3/64 loss: -0.02738475799560547
Batch 4/64 loss: -0.1597452163696289
Batch 5/64 loss: 0.16282176971435547
Batch 6/64 loss: 0.26487255096435547
Batch 7/64 loss: 0.007662296295166016
Batch 8/64 loss: 0.7014670372009277
Batch 9/64 loss: -0.10398578643798828
Batch 10/64 loss: -0.03435325622558594
Batch 11/64 loss: 0.22354507446289062
Batch 12/64 loss: -0.03723287582397461
Batch 13/64 loss: 0.2239689826965332
Batch 14/64 loss: -0.12725448608398438
Batch 15/64 loss: -0.0643472671508789
Batch 16/64 loss: 0.1988511085510254
Batch 17/64 loss: 0.3149552345275879
Batch 18/64 loss: -0.15153121948242188
Batch 19/64 loss: 0.031124114990234375
Batch 20/64 loss: -0.11721611022949219
Batch 21/64 loss: -0.23437786102294922
Batch 22/64 loss: -0.13182497024536133
Batch 23/64 loss: 0.11279964447021484
Batch 24/64 loss: 0.04710960388183594
Batch 25/64 loss: -0.052393436431884766
Batch 26/64 loss: 0.0013523101806640625
Batch 27/64 loss: 0.12749767303466797
Batch 28/64 loss: -0.16672325134277344
Batch 29/64 loss: 0.3582592010498047
Batch 30/64 loss: -0.025491714477539062
Batch 31/64 loss: 0.05016469955444336
Batch 32/64 loss: -0.12050724029541016
Batch 33/64 loss: 0.0922384262084961
Batch 34/64 loss: -0.050930023193359375
Batch 35/64 loss: -0.39398193359375
Batch 36/64 loss: 0.010946273803710938
Batch 37/64 loss: 0.14487409591674805
Batch 38/64 loss: 0.28220510482788086
Batch 39/64 loss: -0.1086740493774414
Batch 40/64 loss: 0.16539907455444336
Batch 41/64 loss: -0.1674509048461914
Batch 42/64 loss: -0.16146421432495117
Batch 43/64 loss: 0.1329498291015625
Batch 44/64 loss: -0.17498254776000977
Batch 45/64 loss: 0.018314838409423828
Batch 46/64 loss: 0.08466339111328125
Batch 47/64 loss: -0.033167362213134766
Batch 48/64 loss: 0.009365081787109375
Batch 49/64 loss: 0.26413583755493164
Batch 50/64 loss: 0.1551671028137207
Batch 51/64 loss: 0.11473464965820312
Batch 52/64 loss: 0.023096084594726562
Batch 53/64 loss: -0.1736001968383789
Batch 54/64 loss: 0.01007699966430664
Batch 55/64 loss: 0.018219470977783203
Batch 56/64 loss: 0.10870647430419922
Batch 57/64 loss: 0.16591787338256836
Batch 58/64 loss: -0.0332341194152832
Batch 59/64 loss: -0.10164928436279297
Batch 60/64 loss: -0.13057994842529297
Batch 61/64 loss: -0.22553443908691406
Batch 62/64 loss: -0.21847057342529297
Batch 63/64 loss: 0.1255660057067871
Batch 64/64 loss: -3.738091468811035
Epoch 237  Train loss: -0.02781862371108111  Val loss: 0.14865390213904103
Epoch 238
-------------------------------
Batch 1/64 loss: -0.44594287872314453
Batch 2/64 loss: -0.07515192031860352
Batch 3/64 loss: -0.1878223419189453
Batch 4/64 loss: -0.20995616912841797
Batch 5/64 loss: -0.10703468322753906
Batch 6/64 loss: 0.2908968925476074
Batch 7/64 loss: -0.3612813949584961
Batch 8/64 loss: -0.3576316833496094
Batch 9/64 loss: 0.04032278060913086
Batch 10/64 loss: 0.001903533935546875
Batch 11/64 loss: 0.1677689552307129
Batch 12/64 loss: -0.023252487182617188
Batch 13/64 loss: 0.24444818496704102
Batch 14/64 loss: -0.1495189666748047
Batch 15/64 loss: 0.09177303314208984
Batch 16/64 loss: -0.11803007125854492
Batch 17/64 loss: 0.17250728607177734
Batch 18/64 loss: 0.30356359481811523
Batch 19/64 loss: 0.09906578063964844
Batch 20/64 loss: -0.23025894165039062
Batch 21/64 loss: -0.10003662109375
Batch 22/64 loss: 0.02736043930053711
Batch 23/64 loss: 0.16318702697753906
Batch 24/64 loss: 0.08801841735839844
Batch 25/64 loss: 0.4780731201171875
Batch 26/64 loss: 0.030101776123046875
Batch 27/64 loss: 0.26393699645996094
Batch 28/64 loss: -0.03048086166381836
Batch 29/64 loss: -0.06684589385986328
Batch 30/64 loss: 0.09315824508666992
Batch 31/64 loss: 0.5816774368286133
Batch 32/64 loss: 0.1481318473815918
Batch 33/64 loss: 0.35146284103393555
Batch 34/64 loss: -0.10154914855957031
Batch 35/64 loss: -0.17371559143066406
Batch 36/64 loss: -0.09515666961669922
Batch 37/64 loss: -0.0673375129699707
Batch 38/64 loss: 0.15010309219360352
Batch 39/64 loss: -0.3504347801208496
Batch 40/64 loss: -0.1306004524230957
Batch 41/64 loss: 0.2333240509033203
Batch 42/64 loss: 0.044922828674316406
Batch 43/64 loss: -0.21120643615722656
Batch 44/64 loss: -0.23897981643676758
Batch 45/64 loss: -0.19902420043945312
Batch 46/64 loss: -0.021701812744140625
Batch 47/64 loss: 0.14814996719360352
Batch 48/64 loss: 0.333712100982666
Batch 49/64 loss: -0.25420284271240234
Batch 50/64 loss: -0.050862789154052734
Batch 51/64 loss: 0.2452259063720703
Batch 52/64 loss: 0.16291093826293945
Batch 53/64 loss: 0.017586708068847656
Batch 54/64 loss: 0.1324477195739746
Batch 55/64 loss: -0.15181732177734375
Batch 56/64 loss: 0.4150509834289551
Batch 57/64 loss: 0.09231996536254883
Batch 58/64 loss: -0.24024105072021484
Batch 59/64 loss: -0.4855670928955078
Batch 60/64 loss: -0.028881549835205078
Batch 61/64 loss: -0.3337697982788086
Batch 62/64 loss: -0.2126150131225586
Batch 63/64 loss: 0.1295485496520996
Batch 64/64 loss: -3.7774763107299805
Epoch 238  Train loss: -0.045511451421999465  Val loss: 0.08732728204366677
Epoch 239
-------------------------------
Batch 1/64 loss: -0.12579870223999023
Batch 2/64 loss: 0.09990835189819336
Batch 3/64 loss: -0.24338817596435547
Batch 4/64 loss: 0.16080236434936523
Batch 5/64 loss: 0.1730356216430664
Batch 6/64 loss: 0.03778648376464844
Batch 7/64 loss: -0.19652366638183594
Batch 8/64 loss: -0.13405799865722656
Batch 9/64 loss: 0.20724010467529297
Batch 10/64 loss: -0.4405841827392578
Batch 11/64 loss: -0.31394004821777344
Batch 12/64 loss: 0.24190044403076172
Batch 13/64 loss: -0.3588752746582031
Batch 14/64 loss: -0.15845680236816406
Batch 15/64 loss: -0.17815542221069336
Batch 16/64 loss: -0.0710306167602539
Batch 17/64 loss: -0.30532264709472656
Batch 18/64 loss: 0.07419538497924805
Batch 19/64 loss: 0.048191070556640625
Batch 20/64 loss: -0.1576671600341797
Batch 21/64 loss: -0.11211347579956055
Batch 22/64 loss: -0.1657247543334961
Batch 23/64 loss: 0.3916645050048828
Batch 24/64 loss: -0.09656000137329102
Batch 25/64 loss: 0.1027984619140625
Batch 26/64 loss: -0.09795284271240234
Batch 27/64 loss: -0.09017515182495117
Batch 28/64 loss: 0.3359107971191406
Batch 29/64 loss: 0.20772981643676758
Batch 30/64 loss: -0.15736865997314453
Batch 31/64 loss: -0.046370506286621094
Batch 32/64 loss: 0.17116165161132812
Batch 33/64 loss: 0.0731649398803711
Batch 34/64 loss: -0.22447824478149414
Batch 35/64 loss: -0.14779186248779297
Batch 36/64 loss: 0.059882164001464844
Batch 37/64 loss: -0.07976150512695312
Batch 38/64 loss: -0.4362630844116211
Batch 39/64 loss: 0.42085886001586914
Batch 40/64 loss: 0.46396780014038086
Batch 41/64 loss: 0.004216670989990234
Batch 42/64 loss: -0.15480756759643555
Batch 43/64 loss: -0.4237546920776367
Batch 44/64 loss: 0.1432027816772461
Batch 45/64 loss: 0.08744382858276367
Batch 46/64 loss: 0.2835416793823242
Batch 47/64 loss: 0.26165008544921875
Batch 48/64 loss: 0.03496122360229492
Batch 49/64 loss: -0.014233589172363281
Batch 50/64 loss: -0.17038869857788086
Batch 51/64 loss: -0.068084716796875
Batch 52/64 loss: -0.1756610870361328
Batch 53/64 loss: 0.04158496856689453
Batch 54/64 loss: -0.0851583480834961
Batch 55/64 loss: -0.04874372482299805
Batch 56/64 loss: 0.19022464752197266
Batch 57/64 loss: -0.34581851959228516
Batch 58/64 loss: 0.09275674819946289
Batch 59/64 loss: -0.18133068084716797
Batch 60/64 loss: 0.14874505996704102
Batch 61/64 loss: 0.023636817932128906
Batch 62/64 loss: 0.29972410202026367
Batch 63/64 loss: -0.05160236358642578
Batch 64/64 loss: -3.484821319580078
Epoch 239  Train loss: -0.059445856131759345  Val loss: 0.208937104215327
Epoch 240
-------------------------------
Batch 1/64 loss: -0.3376045227050781
Batch 2/64 loss: -0.15869140625
Batch 3/64 loss: -0.10674762725830078
Batch 4/64 loss: 0.14021539688110352
Batch 5/64 loss: -0.09885692596435547
Batch 6/64 loss: 0.39589357376098633
Batch 7/64 loss: 0.037878990173339844
Batch 8/64 loss: 0.19591188430786133
Batch 9/64 loss: -0.2313861846923828
Batch 10/64 loss: -0.1392197608947754
Batch 11/64 loss: -0.4899110794067383
Batch 12/64 loss: -0.19939231872558594
Batch 13/64 loss: 0.6446523666381836
Batch 14/64 loss: -0.18050289154052734
Batch 15/64 loss: 0.152740478515625
Batch 16/64 loss: 0.09024858474731445
Batch 17/64 loss: 0.16357851028442383
Batch 18/64 loss: -0.2859821319580078
Batch 19/64 loss: 0.23015117645263672
Batch 20/64 loss: -0.33819007873535156
Batch 21/64 loss: 0.23750638961791992
Batch 22/64 loss: 0.004207134246826172
Batch 23/64 loss: -0.14812850952148438
Batch 24/64 loss: -0.16993236541748047
Batch 25/64 loss: 0.0464634895324707
Batch 26/64 loss: 0.22844362258911133
Batch 27/64 loss: -0.19651222229003906
Batch 28/64 loss: -0.2965555191040039
Batch 29/64 loss: -0.11958885192871094
Batch 30/64 loss: -0.3190145492553711
Batch 31/64 loss: -0.05645608901977539
Batch 32/64 loss: 0.18146800994873047
Batch 33/64 loss: -0.18609905242919922
Batch 34/64 loss: -0.0689077377319336
Batch 35/64 loss: -0.5001926422119141
Batch 36/64 loss: 0.27968311309814453
Batch 37/64 loss: 0.09959983825683594
Batch 38/64 loss: -0.35196876525878906
Batch 39/64 loss: 0.5256824493408203
Batch 40/64 loss: -0.1226205825805664
Batch 41/64 loss: -0.3716697692871094
Batch 42/64 loss: -0.0041484832763671875
Batch 43/64 loss: -0.13700008392333984
Batch 44/64 loss: -0.20493745803833008
Batch 45/64 loss: 0.3134422302246094
Batch 46/64 loss: -0.3233652114868164
Batch 47/64 loss: 0.06002664566040039
Batch 48/64 loss: 0.21466875076293945
Batch 49/64 loss: 0.31998729705810547
Batch 50/64 loss: -0.22550487518310547
Batch 51/64 loss: -0.05647468566894531
Batch 52/64 loss: -0.05336713790893555
Batch 53/64 loss: -0.2850675582885742
Batch 54/64 loss: 0.1790480613708496
Batch 55/64 loss: 0.03743314743041992
Batch 56/64 loss: -0.05149650573730469
Batch 57/64 loss: 0.06464719772338867
Batch 58/64 loss: 0.06982183456420898
Batch 59/64 loss: 0.4086294174194336
Batch 60/64 loss: -0.05842113494873047
Batch 61/64 loss: -0.2972559928894043
Batch 62/64 loss: -0.10478687286376953
Batch 63/64 loss: -0.1380753517150879
Batch 64/64 loss: -3.366738796234131
Epoch 240  Train loss: -0.07242443047317804  Val loss: 0.14193557791693515
Epoch 241
-------------------------------
Batch 1/64 loss: 0.18009519577026367
Batch 2/64 loss: -0.18969964981079102
Batch 3/64 loss: -0.12739896774291992
Batch 4/64 loss: -0.038295745849609375
Batch 5/64 loss: 0.01150655746459961
Batch 6/64 loss: 0.24748754501342773
Batch 7/64 loss: -0.18827581405639648
Batch 8/64 loss: 0.0020771026611328125
Batch 9/64 loss: 0.18415451049804688
Batch 10/64 loss: 0.14402532577514648
Batch 11/64 loss: 0.10812234878540039
Batch 12/64 loss: -0.20298051834106445
Batch 13/64 loss: -0.1156163215637207
Batch 14/64 loss: -0.04332447052001953
Batch 15/64 loss: 0.10775423049926758
Batch 16/64 loss: -0.12197685241699219
Batch 17/64 loss: 0.010783195495605469
Batch 18/64 loss: 0.5094947814941406
Batch 19/64 loss: -0.19181060791015625
Batch 20/64 loss: -0.046547889709472656
Batch 21/64 loss: -0.1543102264404297
Batch 22/64 loss: 0.1938309669494629
Batch 23/64 loss: 0.3953986167907715
Batch 24/64 loss: 0.007344245910644531
Batch 25/64 loss: 0.24231863021850586
Batch 26/64 loss: 0.42337989807128906
Batch 27/64 loss: -0.1651158332824707
Batch 28/64 loss: 0.03684568405151367
Batch 29/64 loss: -0.19161462783813477
Batch 30/64 loss: -0.2586965560913086
Batch 31/64 loss: -0.23813152313232422
Batch 32/64 loss: -0.10149002075195312
Batch 33/64 loss: 0.08331584930419922
Batch 34/64 loss: -0.09604740142822266
Batch 35/64 loss: 0.0142059326171875
Batch 36/64 loss: 0.3662900924682617
Batch 37/64 loss: -0.18720722198486328
Batch 38/64 loss: 0.19331979751586914
Batch 39/64 loss: -0.20225048065185547
Batch 40/64 loss: 0.16557836532592773
Batch 41/64 loss: 0.11071395874023438
Batch 42/64 loss: -0.12302589416503906
Batch 43/64 loss: 0.08775186538696289
Batch 44/64 loss: -0.22676944732666016
Batch 45/64 loss: 0.19515228271484375
Batch 46/64 loss: -0.012192726135253906
Batch 47/64 loss: 0.3866901397705078
Batch 48/64 loss: -0.13916730880737305
Batch 49/64 loss: -0.08397340774536133
Batch 50/64 loss: 0.21351194381713867
Batch 51/64 loss: -0.055850982666015625
Batch 52/64 loss: -0.14602375030517578
Batch 53/64 loss: 0.09114313125610352
Batch 54/64 loss: -0.062073707580566406
Batch 55/64 loss: 0.16071653366088867
Batch 56/64 loss: 0.1465916633605957
Batch 57/64 loss: -0.04551839828491211
Batch 58/64 loss: -0.1487441062927246
Batch 59/64 loss: 0.0584564208984375
Batch 60/64 loss: 0.07238626480102539
Batch 61/64 loss: -0.3403282165527344
Batch 62/64 loss: -0.017338275909423828
Batch 63/64 loss: -0.124420166015625
Batch 64/64 loss: -3.8517627716064453
Epoch 241  Train loss: -0.03332699794395297  Val loss: 0.07791920298153591
Epoch 242
-------------------------------
Batch 1/64 loss: 0.19519710540771484
Batch 2/64 loss: -0.015282630920410156
Batch 3/64 loss: 0.010193824768066406
Batch 4/64 loss: 0.016115665435791016
Batch 5/64 loss: -0.16171932220458984
Batch 6/64 loss: 0.10384893417358398
Batch 7/64 loss: -0.1674938201904297
Batch 8/64 loss: -0.2098388671875
Batch 9/64 loss: -0.2597169876098633
Batch 10/64 loss: 0.14239883422851562
Batch 11/64 loss: -0.21054458618164062
Batch 12/64 loss: 0.11326122283935547
Batch 13/64 loss: -0.1342153549194336
Batch 14/64 loss: 0.09837579727172852
Batch 15/64 loss: 0.06775760650634766
Batch 16/64 loss: -0.2920875549316406
Batch 17/64 loss: 0.1905350685119629
Batch 18/64 loss: 0.09299755096435547
Batch 19/64 loss: -0.02395486831665039
Batch 20/64 loss: 0.0024933815002441406
Batch 21/64 loss: 0.19079160690307617
Batch 22/64 loss: -0.06543302536010742
Batch 23/64 loss: -0.1605987548828125
Batch 24/64 loss: -0.10637474060058594
Batch 25/64 loss: -0.03776884078979492
Batch 26/64 loss: -0.040972232818603516
Batch 27/64 loss: 0.06217241287231445
Batch 28/64 loss: 0.044579505920410156
Batch 29/64 loss: -0.2213582992553711
Batch 30/64 loss: -0.3359246253967285
Batch 31/64 loss: 0.13316011428833008
Batch 32/64 loss: 0.03314495086669922
Batch 33/64 loss: -0.27773141860961914
Batch 34/64 loss: 0.0964360237121582
Batch 35/64 loss: -0.3265399932861328
Batch 36/64 loss: 0.1116337776184082
Batch 37/64 loss: -0.04532766342163086
Batch 38/64 loss: -0.22315263748168945
Batch 39/64 loss: -0.2903923988342285
Batch 40/64 loss: -0.24657773971557617
Batch 41/64 loss: -0.25569725036621094
Batch 42/64 loss: -0.10348033905029297
Batch 43/64 loss: -0.12617778778076172
Batch 44/64 loss: -0.36214160919189453
Batch 45/64 loss: -0.009943008422851562
Batch 46/64 loss: -0.1526932716369629
Batch 47/64 loss: -0.11365413665771484
Batch 48/64 loss: 0.2563018798828125
Batch 49/64 loss: -0.16326093673706055
Batch 50/64 loss: 0.08080577850341797
Batch 51/64 loss: 0.23072576522827148
Batch 52/64 loss: -0.20982074737548828
Batch 53/64 loss: 0.23252058029174805
Batch 54/64 loss: 0.1401224136352539
Batch 55/64 loss: 0.03391695022583008
Batch 56/64 loss: 0.13629865646362305
Batch 57/64 loss: 0.13666868209838867
Batch 58/64 loss: 0.06717634201049805
Batch 59/64 loss: 0.03323936462402344
Batch 60/64 loss: 0.19468402862548828
Batch 61/64 loss: 0.0028247833251953125
Batch 62/64 loss: -0.019924163818359375
Batch 63/64 loss: -0.11071300506591797
Batch 64/64 loss: -3.6845297813415527
Epoch 242  Train loss: -0.07832990347170363  Val loss: 0.04393061739472589
Epoch 243
-------------------------------
Batch 1/64 loss: -0.059525489807128906
Batch 2/64 loss: 0.16667509078979492
Batch 3/64 loss: -0.19030332565307617
Batch 4/64 loss: 0.05661201477050781
Batch 5/64 loss: 0.3734002113342285
Batch 6/64 loss: -0.34180736541748047
Batch 7/64 loss: -0.24163532257080078
Batch 8/64 loss: -0.5262937545776367
Batch 9/64 loss: 0.2935948371887207
Batch 10/64 loss: -0.1376781463623047
Batch 11/64 loss: 0.16539382934570312
Batch 12/64 loss: 0.0016956329345703125
Batch 13/64 loss: -0.30754852294921875
Batch 14/64 loss: -0.011295795440673828
Batch 15/64 loss: 0.10938167572021484
Batch 16/64 loss: -0.0745391845703125
Batch 17/64 loss: -0.3348665237426758
Batch 18/64 loss: -0.28003978729248047
Batch 19/64 loss: -0.1158285140991211
Batch 20/64 loss: -0.06534957885742188
Batch 21/64 loss: -0.1378331184387207
Batch 22/64 loss: -0.0032806396484375
Batch 23/64 loss: 0.2927570343017578
Batch 24/64 loss: -0.3291645050048828
Batch 25/64 loss: -0.26632118225097656
Batch 26/64 loss: 0.004473686218261719
Batch 27/64 loss: 0.3562436103820801
Batch 28/64 loss: -0.09816551208496094
Batch 29/64 loss: 0.092041015625
Batch 30/64 loss: 0.17046833038330078
Batch 31/64 loss: -0.3806328773498535
Batch 32/64 loss: 0.37109375
Batch 33/64 loss: 0.10615968704223633
Batch 34/64 loss: 0.3278846740722656
Batch 35/64 loss: 0.16848516464233398
Batch 36/64 loss: 0.15926313400268555
Batch 37/64 loss: 0.009411811828613281
Batch 38/64 loss: -0.10529518127441406
Batch 39/64 loss: 0.09549140930175781
Batch 40/64 loss: 0.09735822677612305
Batch 41/64 loss: 0.07483577728271484
Batch 42/64 loss: 0.036464691162109375
Batch 43/64 loss: 0.16004276275634766
Batch 44/64 loss: 0.02783060073852539
Batch 45/64 loss: 0.10883235931396484
Batch 46/64 loss: -0.22184371948242188
Batch 47/64 loss: 0.13472652435302734
Batch 48/64 loss: 0.11975622177124023
Batch 49/64 loss: 0.09157752990722656
Batch 50/64 loss: -0.32225942611694336
Batch 51/64 loss: -0.11531543731689453
Batch 52/64 loss: -0.27954769134521484
Batch 53/64 loss: -0.07213354110717773
Batch 54/64 loss: 0.19527530670166016
Batch 55/64 loss: -0.027493953704833984
Batch 56/64 loss: -0.21088314056396484
Batch 57/64 loss: -0.15732192993164062
Batch 58/64 loss: 0.27725887298583984
Batch 59/64 loss: 0.41364097595214844
Batch 60/64 loss: 0.024852275848388672
Batch 61/64 loss: 0.02264261245727539
Batch 62/64 loss: 0.08141803741455078
Batch 63/64 loss: 0.0015773773193359375
Batch 64/64 loss: -3.536684989929199
Epoch 243  Train loss: -0.045146669126024434  Val loss: 0.11085054063305412
Epoch 244
-------------------------------
Batch 1/64 loss: 0.02610301971435547
Batch 2/64 loss: 0.09098243713378906
Batch 3/64 loss: -0.17264080047607422
Batch 4/64 loss: -0.04314231872558594
Batch 5/64 loss: 0.24790477752685547
Batch 6/64 loss: -0.18520784378051758
Batch 7/64 loss: 0.09392833709716797
Batch 8/64 loss: -0.17925167083740234
Batch 9/64 loss: -0.20112991333007812
Batch 10/64 loss: 0.06008195877075195
Batch 11/64 loss: 0.19183874130249023
Batch 12/64 loss: 0.08388710021972656
Batch 13/64 loss: -0.1589035987854004
Batch 14/64 loss: 0.10660743713378906
Batch 15/64 loss: 0.17331695556640625
Batch 16/64 loss: 0.1443009376525879
Batch 17/64 loss: -0.26001930236816406
Batch 18/64 loss: 0.4318675994873047
Batch 19/64 loss: -0.05265665054321289
Batch 20/64 loss: -0.24005699157714844
Batch 21/64 loss: -0.11960172653198242
Batch 22/64 loss: -0.28422975540161133
Batch 23/64 loss: -0.11211633682250977
Batch 24/64 loss: 0.34545230865478516
Batch 25/64 loss: 0.011504173278808594
Batch 26/64 loss: -0.020421504974365234
Batch 27/64 loss: -0.06318283081054688
Batch 28/64 loss: -0.1033487319946289
Batch 29/64 loss: 0.26915454864501953
Batch 30/64 loss: -0.2287445068359375
Batch 31/64 loss: -0.43016862869262695
Batch 32/64 loss: -0.28639936447143555
Batch 33/64 loss: -0.2800636291503906
Batch 34/64 loss: -0.15460634231567383
Batch 35/64 loss: -0.30156707763671875
Batch 36/64 loss: 0.09909486770629883
Batch 37/64 loss: -0.0023965835571289062
Batch 38/64 loss: 0.1374807357788086
Batch 39/64 loss: -0.01087188720703125
Batch 40/64 loss: 0.028605937957763672
Batch 41/64 loss: -0.018489837646484375
Batch 42/64 loss: 0.014476776123046875
Batch 43/64 loss: -0.12638092041015625
Batch 44/64 loss: -0.09748077392578125
Batch 45/64 loss: -0.37926197052001953
Batch 46/64 loss: -0.10584163665771484
Batch 47/64 loss: 0.13902664184570312
Batch 48/64 loss: -0.13077068328857422
Batch 49/64 loss: -0.1823892593383789
Batch 50/64 loss: -0.030374526977539062
Batch 51/64 loss: 0.11278486251831055
Batch 52/64 loss: -0.3208026885986328
Batch 53/64 loss: 0.014763355255126953
Batch 54/64 loss: 0.27620840072631836
Batch 55/64 loss: -0.23188066482543945
Batch 56/64 loss: -0.22965145111083984
Batch 57/64 loss: 0.04258012771606445
Batch 58/64 loss: -0.11034727096557617
Batch 59/64 loss: 0.28353118896484375
Batch 60/64 loss: 0.06348180770874023
Batch 61/64 loss: 0.21263551712036133
Batch 62/64 loss: 0.05846118927001953
Batch 63/64 loss: 0.055907249450683594
Batch 64/64 loss: -3.401336669921875
Epoch 244  Train loss: -0.07199110891304764  Val loss: -0.023700150427539732
Saving best model, epoch: 244
Epoch 245
-------------------------------
Batch 1/64 loss: -0.0520167350769043
Batch 2/64 loss: 0.02118968963623047
Batch 3/64 loss: -0.13092422485351562
Batch 4/64 loss: -0.24956035614013672
Batch 5/64 loss: 0.1961221694946289
Batch 6/64 loss: 0.07498979568481445
Batch 7/64 loss: -0.1484975814819336
Batch 8/64 loss: -0.061052799224853516
Batch 9/64 loss: 0.31771183013916016
Batch 10/64 loss: -0.17966032028198242
Batch 11/64 loss: -0.15378093719482422
Batch 12/64 loss: 0.14557218551635742
Batch 13/64 loss: -0.002852916717529297
Batch 14/64 loss: 0.1364612579345703
Batch 15/64 loss: -0.2502126693725586
Batch 16/64 loss: -0.22182846069335938
Batch 17/64 loss: -0.1023550033569336
Batch 18/64 loss: 0.26911497116088867
Batch 19/64 loss: -0.27339839935302734
Batch 20/64 loss: -0.023118019104003906
Batch 21/64 loss: 0.18947267532348633
Batch 22/64 loss: 0.04350757598876953
Batch 23/64 loss: -0.2917466163635254
Batch 24/64 loss: -0.19639968872070312
Batch 25/64 loss: -0.2821054458618164
Batch 26/64 loss: -0.17552995681762695
Batch 27/64 loss: -0.14156246185302734
Batch 28/64 loss: -0.04757356643676758
Batch 29/64 loss: -0.349945068359375
Batch 30/64 loss: -0.18346309661865234
Batch 31/64 loss: -0.12100410461425781
Batch 32/64 loss: -0.28700828552246094
Batch 33/64 loss: -0.23926258087158203
Batch 34/64 loss: -0.18758106231689453
Batch 35/64 loss: 0.13624906539916992
Batch 36/64 loss: 0.09575986862182617
Batch 37/64 loss: -0.08662271499633789
Batch 38/64 loss: -0.03374195098876953
Batch 39/64 loss: 0.16686439514160156
Batch 40/64 loss: -0.2630577087402344
Batch 41/64 loss: -0.16595745086669922
Batch 42/64 loss: 0.21013402938842773
Batch 43/64 loss: -0.14163637161254883
Batch 44/64 loss: -0.5660362243652344
Batch 45/64 loss: -0.41542530059814453
Batch 46/64 loss: -0.2164325714111328
Batch 47/64 loss: 0.0779581069946289
Batch 48/64 loss: 0.19394493103027344
Batch 49/64 loss: 0.49459266662597656
Batch 50/64 loss: -0.32410430908203125
Batch 51/64 loss: 0.13278961181640625
Batch 52/64 loss: 0.21689558029174805
Batch 53/64 loss: 0.1475214958190918
Batch 54/64 loss: 0.16526174545288086
Batch 55/64 loss: -0.023800373077392578
Batch 56/64 loss: -0.050679683685302734
Batch 57/64 loss: -0.1595449447631836
Batch 58/64 loss: -0.026024818420410156
Batch 59/64 loss: -0.14053583145141602
Batch 60/64 loss: -0.29810047149658203
Batch 61/64 loss: -0.08056354522705078
Batch 62/64 loss: -0.1312389373779297
Batch 63/64 loss: -0.08206462860107422
Batch 64/64 loss: -3.8167333602905273
Epoch 245  Train loss: -0.1096226598702225  Val loss: -0.024952760676747746
Saving best model, epoch: 245
Epoch 246
-------------------------------
Batch 1/64 loss: -0.30776405334472656
Batch 2/64 loss: 0.10811185836791992
Batch 3/64 loss: -0.06455755233764648
Batch 4/64 loss: -0.1599273681640625
Batch 5/64 loss: -0.12187957763671875
Batch 6/64 loss: 0.39179039001464844
Batch 7/64 loss: 0.0764622688293457
Batch 8/64 loss: -0.05180692672729492
Batch 9/64 loss: -0.19652605056762695
Batch 10/64 loss: -0.0031061172485351562
Batch 11/64 loss: -0.0336461067199707
Batch 12/64 loss: 0.023471355438232422
Batch 13/64 loss: -0.1815481185913086
Batch 14/64 loss: -0.21346044540405273
Batch 15/64 loss: -0.036604881286621094
Batch 16/64 loss: 0.27585792541503906
Batch 17/64 loss: 0.14371776580810547
Batch 18/64 loss: 0.4207329750061035
Batch 19/64 loss: -0.2992124557495117
Batch 20/64 loss: -0.31457042694091797
Batch 21/64 loss: -0.4724559783935547
Batch 22/64 loss: -0.20562362670898438
Batch 23/64 loss: -0.20680522918701172
Batch 24/64 loss: -0.2568826675415039
Batch 25/64 loss: 0.014391422271728516
Batch 26/64 loss: -0.26184844970703125
Batch 27/64 loss: -0.054705142974853516
Batch 28/64 loss: 0.06580305099487305
Batch 29/64 loss: -0.16219520568847656
Batch 30/64 loss: -0.15981197357177734
Batch 31/64 loss: -0.19047164916992188
Batch 32/64 loss: 0.048490047454833984
Batch 33/64 loss: 0.04619789123535156
Batch 34/64 loss: 0.026803016662597656
Batch 35/64 loss: 0.2106938362121582
Batch 36/64 loss: 0.06635522842407227
Batch 37/64 loss: -0.32430267333984375
Batch 38/64 loss: -0.13177108764648438
Batch 39/64 loss: 0.28688621520996094
Batch 40/64 loss: -0.057600975036621094
Batch 41/64 loss: -0.20303726196289062
Batch 42/64 loss: -0.31892871856689453
Batch 43/64 loss: -0.3037757873535156
Batch 44/64 loss: 0.006965160369873047
Batch 45/64 loss: -0.12996673583984375
Batch 46/64 loss: -0.3261251449584961
Batch 47/64 loss: -0.2843961715698242
Batch 48/64 loss: -0.13474559783935547
Batch 49/64 loss: 0.00043773651123046875
Batch 50/64 loss: 0.10515260696411133
Batch 51/64 loss: -0.1342334747314453
Batch 52/64 loss: 0.15869140625
Batch 53/64 loss: 0.10502910614013672
Batch 54/64 loss: -0.1933755874633789
Batch 55/64 loss: 0.31136608123779297
Batch 56/64 loss: 0.1466836929321289
Batch 57/64 loss: -0.09579944610595703
Batch 58/64 loss: -0.0877218246459961
Batch 59/64 loss: 0.11475181579589844
Batch 60/64 loss: -0.1743168830871582
Batch 61/64 loss: -0.24499034881591797
Batch 62/64 loss: 0.4032468795776367
Batch 63/64 loss: -0.1162567138671875
Batch 64/64 loss: -3.7718658447265625
Epoch 246  Train loss: -0.10176571116727941  Val loss: -0.006169505955017719
Epoch 247
-------------------------------
Batch 1/64 loss: -0.19914817810058594
Batch 2/64 loss: 0.19292163848876953
Batch 3/64 loss: -0.10870933532714844
Batch 4/64 loss: 0.12022542953491211
Batch 5/64 loss: -0.02779674530029297
Batch 6/64 loss: -0.18174457550048828
Batch 7/64 loss: 0.22233963012695312
Batch 8/64 loss: -0.16663455963134766
Batch 9/64 loss: -0.1932964324951172
Batch 10/64 loss: 0.26159191131591797
Batch 11/64 loss: -0.06945037841796875
Batch 12/64 loss: -0.1760578155517578
Batch 13/64 loss: -0.01791667938232422
Batch 14/64 loss: -0.3579864501953125
Batch 15/64 loss: -0.3280372619628906
Batch 16/64 loss: -0.13484764099121094
Batch 17/64 loss: -0.08091163635253906
Batch 18/64 loss: -0.013651847839355469
Batch 19/64 loss: -0.3881044387817383
Batch 20/64 loss: -0.22948169708251953
Batch 21/64 loss: 0.1453104019165039
Batch 22/64 loss: -0.2956266403198242
Batch 23/64 loss: -0.1725149154663086
Batch 24/64 loss: 0.0787959098815918
Batch 25/64 loss: 0.18074417114257812
Batch 26/64 loss: -0.12935686111450195
Batch 27/64 loss: -0.06510543823242188
Batch 28/64 loss: -0.10521125793457031
Batch 29/64 loss: -0.2087688446044922
Batch 30/64 loss: 0.037026405334472656
Batch 31/64 loss: -0.051544189453125
Batch 32/64 loss: 0.32405805587768555
Batch 33/64 loss: -0.32088565826416016
Batch 34/64 loss: -0.03371906280517578
Batch 35/64 loss: -0.354888916015625
Batch 36/64 loss: -0.32167816162109375
Batch 37/64 loss: 0.2161426544189453
Batch 38/64 loss: -0.28328609466552734
Batch 39/64 loss: 0.031009674072265625
Batch 40/64 loss: 0.2623929977416992
Batch 41/64 loss: -0.41812658309936523
Batch 42/64 loss: -0.02159595489501953
Batch 43/64 loss: -0.3841695785522461
Batch 44/64 loss: -0.02130270004272461
Batch 45/64 loss: -0.3116765022277832
Batch 46/64 loss: -0.007731437683105469
Batch 47/64 loss: 0.026253700256347656
Batch 48/64 loss: -0.37960338592529297
Batch 49/64 loss: -0.10142183303833008
Batch 50/64 loss: 0.11423397064208984
Batch 51/64 loss: 0.09498167037963867
Batch 52/64 loss: -0.5774726867675781
Batch 53/64 loss: -0.23508739471435547
Batch 54/64 loss: -0.07886123657226562
Batch 55/64 loss: 0.25924062728881836
Batch 56/64 loss: -0.000232696533203125
Batch 57/64 loss: 0.24514102935791016
Batch 58/64 loss: -0.0302276611328125
Batch 59/64 loss: 0.7994017601013184
Batch 60/64 loss: 0.2476787567138672
Batch 61/64 loss: -0.1372089385986328
Batch 62/64 loss: 0.38893556594848633
Batch 63/64 loss: -0.01601266860961914
Batch 64/64 loss: -3.9154539108276367
Epoch 247  Train loss: -0.10078835206873277  Val loss: 0.13229978371321952
Epoch 248
-------------------------------
Batch 1/64 loss: 0.0299224853515625
Batch 2/64 loss: -0.3270549774169922
Batch 3/64 loss: 0.3112931251525879
Batch 4/64 loss: -0.045350074768066406
Batch 5/64 loss: 0.4680061340332031
Batch 6/64 loss: -0.09137821197509766
Batch 7/64 loss: -0.22882366180419922
Batch 8/64 loss: -0.31247711181640625
Batch 9/64 loss: 0.2807283401489258
Batch 10/64 loss: 0.261197566986084
Batch 11/64 loss: 0.10432195663452148
Batch 12/64 loss: -0.20572185516357422
Batch 13/64 loss: -0.3768329620361328
Batch 14/64 loss: 0.0131988525390625
Batch 15/64 loss: -0.05968809127807617
Batch 16/64 loss: -0.3023824691772461
Batch 17/64 loss: -0.2891693115234375
Batch 18/64 loss: 0.06370162963867188
Batch 19/64 loss: -0.29728221893310547
Batch 20/64 loss: 0.0645914077758789
Batch 21/64 loss: -0.15261173248291016
Batch 22/64 loss: -0.11191082000732422
Batch 23/64 loss: 0.30449867248535156
Batch 24/64 loss: -0.07338619232177734
Batch 25/64 loss: -0.17133188247680664
Batch 26/64 loss: -0.00104522705078125
Batch 27/64 loss: -0.11656951904296875
Batch 28/64 loss: -0.02168560028076172
Batch 29/64 loss: -0.43714237213134766
Batch 30/64 loss: -0.06266021728515625
Batch 31/64 loss: 0.20568275451660156
Batch 32/64 loss: -0.1872854232788086
Batch 33/64 loss: -0.017267227172851562
Batch 34/64 loss: -0.02170085906982422
Batch 35/64 loss: -0.18149662017822266
Batch 36/64 loss: 0.338409423828125
Batch 37/64 loss: 0.148834228515625
Batch 38/64 loss: 0.04695463180541992
Batch 39/64 loss: -0.06957626342773438
Batch 40/64 loss: -0.27013111114501953
Batch 41/64 loss: 0.12360858917236328
Batch 42/64 loss: 0.14712238311767578
Batch 43/64 loss: 0.029539108276367188
Batch 44/64 loss: 0.4090256690979004
Batch 45/64 loss: 0.5252442359924316
Batch 46/64 loss: -0.061115264892578125
Batch 47/64 loss: 0.27962779998779297
Batch 48/64 loss: 0.09545087814331055
Batch 49/64 loss: -0.20459699630737305
Batch 50/64 loss: 0.07300233840942383
Batch 51/64 loss: -0.22803020477294922
Batch 52/64 loss: 0.20024538040161133
Batch 53/64 loss: 0.017412185668945312
Batch 54/64 loss: -0.023817062377929688
Batch 55/64 loss: 0.24483728408813477
Batch 56/64 loss: 0.003497600555419922
Batch 57/64 loss: -0.20102453231811523
Batch 58/64 loss: -0.2743997573852539
Batch 59/64 loss: 0.15092754364013672
Batch 60/64 loss: -0.08611631393432617
Batch 61/64 loss: 0.4410867691040039
Batch 62/64 loss: -0.21086359024047852
Batch 63/64 loss: 0.1033172607421875
Batch 64/64 loss: -4.0684051513671875
Epoch 248  Train loss: -0.05157558216768152  Val loss: 0.10954478680063359
Epoch 249
-------------------------------
Batch 1/64 loss: 0.047771453857421875
Batch 2/64 loss: -0.02936267852783203
Batch 3/64 loss: 0.13330650329589844
Batch 4/64 loss: -0.15270042419433594
Batch 5/64 loss: 0.08278179168701172
Batch 6/64 loss: -0.11211919784545898
Batch 7/64 loss: -0.23276329040527344
Batch 8/64 loss: -0.016404151916503906
Batch 9/64 loss: 0.16283702850341797
Batch 10/64 loss: -0.1968250274658203
Batch 11/64 loss: 0.13717412948608398
Batch 12/64 loss: -0.10837984085083008
Batch 13/64 loss: -0.22057533264160156
Batch 14/64 loss: -0.25060558319091797
Batch 15/64 loss: -0.028668880462646484
Batch 16/64 loss: -0.08081436157226562
Batch 17/64 loss: -0.07692337036132812
Batch 18/64 loss: -0.27059364318847656
Batch 19/64 loss: 0.13900089263916016
Batch 20/64 loss: -0.17328500747680664
Batch 21/64 loss: -0.3229026794433594
Batch 22/64 loss: 0.22014904022216797
Batch 23/64 loss: 1.3007593154907227
Batch 24/64 loss: -0.31875133514404297
Batch 25/64 loss: -0.3506021499633789
Batch 26/64 loss: -0.36443328857421875
Batch 27/64 loss: -0.04398536682128906
Batch 28/64 loss: -0.09095430374145508
Batch 29/64 loss: 1.0107946395874023
Batch 30/64 loss: -0.100677490234375
Batch 31/64 loss: -0.08141040802001953
Batch 32/64 loss: 0.20379400253295898
Batch 33/64 loss: 0.1914224624633789
Batch 34/64 loss: 0.4330620765686035
Batch 35/64 loss: 0.25807905197143555
Batch 36/64 loss: 0.22417688369750977
Batch 37/64 loss: 0.22277164459228516
Batch 38/64 loss: 0.3336057662963867
Batch 39/64 loss: 0.6877584457397461
Batch 40/64 loss: 0.3570375442504883
Batch 41/64 loss: 0.503626823425293
Batch 42/64 loss: 0.06021595001220703
Batch 43/64 loss: -0.13577842712402344
Batch 44/64 loss: 0.20711088180541992
Batch 45/64 loss: 0.38188934326171875
Batch 46/64 loss: 0.3325777053833008
Batch 47/64 loss: 0.2120351791381836
Batch 48/64 loss: 0.4901242256164551
Batch 49/64 loss: 0.10562515258789062
Batch 50/64 loss: 0.3932533264160156
Batch 51/64 loss: 0.23522615432739258
Batch 52/64 loss: 0.18604183197021484
Batch 53/64 loss: 0.18618106842041016
Batch 54/64 loss: 0.08252573013305664
Batch 55/64 loss: 0.2634263038635254
Batch 56/64 loss: 0.13199090957641602
Batch 57/64 loss: 0.4219222068786621
Batch 58/64 loss: 0.602849006652832
Batch 59/64 loss: 0.3353886604309082
Batch 60/64 loss: 0.06555414199829102
Batch 61/64 loss: 0.17151165008544922
Batch 62/64 loss: 0.17078113555908203
Batch 63/64 loss: -0.1031489372253418
Batch 64/64 loss: -3.482297897338867
Epoch 249  Train loss: 0.08175296409457337  Val loss: 0.13694034327346435
Epoch 250
-------------------------------
Batch 1/64 loss: 0.13419771194458008
Batch 2/64 loss: 0.33992481231689453
Batch 3/64 loss: -0.1185617446899414
Batch 4/64 loss: -0.08362483978271484
Batch 5/64 loss: 0.15773677825927734
Batch 6/64 loss: -0.13407182693481445
Batch 7/64 loss: -0.11484003067016602
Batch 8/64 loss: 0.2882804870605469
Batch 9/64 loss: -0.07945966720581055
Batch 10/64 loss: 0.29538488388061523
Batch 11/64 loss: 0.36602020263671875
Batch 12/64 loss: -0.1573643684387207
Batch 13/64 loss: 0.3323068618774414
Batch 14/64 loss: -0.21112394332885742
Batch 15/64 loss: -0.07473230361938477
Batch 16/64 loss: 0.005207538604736328
Batch 17/64 loss: -0.19747304916381836
Batch 18/64 loss: -0.07172679901123047
Batch 19/64 loss: 0.057245731353759766
Batch 20/64 loss: -0.20129966735839844
Batch 21/64 loss: 0.2865447998046875
Batch 22/64 loss: -0.057369232177734375
Batch 23/64 loss: -0.05770587921142578
Batch 24/64 loss: 0.23500585556030273
Batch 25/64 loss: 0.29922008514404297
Batch 26/64 loss: -0.011354446411132812
Batch 27/64 loss: 0.3840975761413574
Batch 28/64 loss: 0.16580677032470703
Batch 29/64 loss: 0.32558584213256836
Batch 30/64 loss: 0.0649709701538086
Batch 31/64 loss: -0.018892765045166016
Batch 32/64 loss: -0.25139856338500977
Batch 33/64 loss: 0.20197820663452148
Batch 34/64 loss: -0.11716747283935547
Batch 35/64 loss: -0.04605531692504883
Batch 36/64 loss: -0.015976905822753906
Batch 37/64 loss: -0.044764041900634766
Batch 38/64 loss: -0.23232078552246094
Batch 39/64 loss: 0.27212953567504883
Batch 40/64 loss: 0.08995342254638672
Batch 41/64 loss: -0.06281614303588867
Batch 42/64 loss: -0.29830074310302734
Batch 43/64 loss: 0.17376708984375
Batch 44/64 loss: -0.08959197998046875
Batch 45/64 loss: -0.3291053771972656
Batch 46/64 loss: -0.049625396728515625
Batch 47/64 loss: -0.24249935150146484
Batch 48/64 loss: 0.06981754302978516
Batch 49/64 loss: 0.1386547088623047
Batch 50/64 loss: -0.00704193115234375
Batch 51/64 loss: 0.18922805786132812
Batch 52/64 loss: -0.0805048942565918
Batch 53/64 loss: 0.08685541152954102
Batch 54/64 loss: 0.38115692138671875
Batch 55/64 loss: -0.21085071563720703
Batch 56/64 loss: -0.17502784729003906
Batch 57/64 loss: 0.30513668060302734
Batch 58/64 loss: -0.0036344528198242188
Batch 59/64 loss: 0.2133197784423828
Batch 60/64 loss: -0.28757333755493164
Batch 61/64 loss: 0.20147275924682617
Batch 62/64 loss: -0.3762245178222656
Batch 63/64 loss: 0.18969488143920898
Batch 64/64 loss: -3.3764734268188477
Epoch 250  Train loss: -0.012419349072026272  Val loss: -0.004524362046284364
Epoch 251
-------------------------------
Batch 1/64 loss: -0.09659910202026367
Batch 2/64 loss: 0.05324268341064453
Batch 3/64 loss: 0.24079132080078125
Batch 4/64 loss: -0.2728409767150879
Batch 5/64 loss: -0.19768619537353516
Batch 6/64 loss: 0.006827354431152344
Batch 7/64 loss: -0.2456979751586914
Batch 8/64 loss: -0.3117103576660156
Batch 9/64 loss: 0.0027627944946289062
Batch 10/64 loss: -0.07074451446533203
Batch 11/64 loss: -0.1228322982788086
Batch 12/64 loss: 0.24405956268310547
Batch 13/64 loss: -0.2139596939086914
Batch 14/64 loss: -0.3371295928955078
Batch 15/64 loss: -0.18455028533935547
Batch 16/64 loss: -0.15109014511108398
Batch 17/64 loss: -0.06671428680419922
Batch 18/64 loss: -0.2586517333984375
Batch 19/64 loss: -0.02694559097290039
Batch 20/64 loss: 0.011478424072265625
Batch 21/64 loss: -0.02242136001586914
Batch 22/64 loss: -0.0632314682006836
Batch 23/64 loss: 0.15150880813598633
Batch 24/64 loss: -0.0924229621887207
Batch 25/64 loss: 0.43130064010620117
Batch 26/64 loss: 0.2587008476257324
Batch 27/64 loss: -0.19033432006835938
Batch 28/64 loss: 0.1735515594482422
Batch 29/64 loss: 0.5217127799987793
Batch 30/64 loss: 0.0735006332397461
Batch 31/64 loss: 0.10367393493652344
Batch 32/64 loss: 0.0038690567016601562
Batch 33/64 loss: 0.08234739303588867
Batch 34/64 loss: 0.15779495239257812
Batch 35/64 loss: -0.35235118865966797
Batch 36/64 loss: -0.2681093215942383
Batch 37/64 loss: -0.12601375579833984
Batch 38/64 loss: 0.08432674407958984
Batch 39/64 loss: 0.4461236000061035
Batch 40/64 loss: 0.04268455505371094
Batch 41/64 loss: -0.1455535888671875
Batch 42/64 loss: -0.13002300262451172
Batch 43/64 loss: 0.2786288261413574
Batch 44/64 loss: -0.32120418548583984
Batch 45/64 loss: 0.5717530250549316
Batch 46/64 loss: 0.41691160202026367
Batch 47/64 loss: 0.0020895004272460938
Batch 48/64 loss: 1.1088910102844238
Batch 49/64 loss: -0.17270755767822266
Batch 50/64 loss: 0.3126497268676758
Batch 51/64 loss: -0.03799009323120117
Batch 52/64 loss: 0.14080810546875
Batch 53/64 loss: 0.15710115432739258
Batch 54/64 loss: 0.46253156661987305
Batch 55/64 loss: 0.5760951042175293
Batch 56/64 loss: 0.3447246551513672
Batch 57/64 loss: -0.07142448425292969
Batch 58/64 loss: 0.17833614349365234
Batch 59/64 loss: 0.17259693145751953
Batch 60/64 loss: 0.3117818832397461
Batch 61/64 loss: 0.43039989471435547
Batch 62/64 loss: 0.6616768836975098
Batch 63/64 loss: -0.0076389312744140625
Batch 64/64 loss: -3.4126334190368652
Epoch 251  Train loss: 0.0329283078511556  Val loss: 0.49125892927556514
Epoch 252
-------------------------------
Batch 1/64 loss: 0.5331058502197266
Batch 2/64 loss: -0.02208995819091797
Batch 3/64 loss: 0.3870868682861328
Batch 4/64 loss: 0.4596548080444336
Batch 5/64 loss: 0.2091078758239746
Batch 6/64 loss: 0.20259428024291992
Batch 7/64 loss: 0.2040271759033203
Batch 8/64 loss: 0.23722410202026367
Batch 9/64 loss: 0.05437278747558594
Batch 10/64 loss: -0.06208944320678711
Batch 11/64 loss: 0.27100419998168945
Batch 12/64 loss: 0.044245243072509766
Batch 13/64 loss: 0.06420278549194336
Batch 14/64 loss: 0.42630624771118164
Batch 15/64 loss: 0.0007252693176269531
Batch 16/64 loss: 0.3741598129272461
Batch 17/64 loss: 0.2698817253112793
Batch 18/64 loss: 0.34522247314453125
Batch 19/64 loss: 0.1932997703552246
Batch 20/64 loss: 0.3174934387207031
Batch 21/64 loss: 0.2890291213989258
Batch 22/64 loss: -0.10924863815307617
Batch 23/64 loss: 0.5121989250183105
Batch 24/64 loss: 0.1422576904296875
Batch 25/64 loss: 0.4049110412597656
Batch 26/64 loss: -0.09882020950317383
Batch 27/64 loss: -0.0034308433532714844
Batch 28/64 loss: 0.3052487373352051
Batch 29/64 loss: -0.047109127044677734
Batch 30/64 loss: 0.03241586685180664
Batch 31/64 loss: 0.13783931732177734
Batch 32/64 loss: 0.2840900421142578
Batch 33/64 loss: 0.006341457366943359
Batch 34/64 loss: -0.004390716552734375
Batch 35/64 loss: 0.06468486785888672
Batch 36/64 loss: -0.10021448135375977
Batch 37/64 loss: 0.013747692108154297
Batch 38/64 loss: -0.07387399673461914
Batch 39/64 loss: 0.12277507781982422
Batch 40/64 loss: 0.22344446182250977
Batch 41/64 loss: -0.05993175506591797
Batch 42/64 loss: 0.3414020538330078
Batch 43/64 loss: -0.008251667022705078
Batch 44/64 loss: 0.2601780891418457
Batch 45/64 loss: -0.01358938217163086
Batch 46/64 loss: 0.29638099670410156
Batch 47/64 loss: -0.12253856658935547
Batch 48/64 loss: 0.04461669921875
Batch 49/64 loss: 0.18099403381347656
Batch 50/64 loss: 0.7137818336486816
Batch 51/64 loss: -0.04375171661376953
Batch 52/64 loss: -0.018156051635742188
Batch 53/64 loss: 0.011178970336914062
Batch 54/64 loss: -0.012112617492675781
Batch 55/64 loss: 0.04187345504760742
Batch 56/64 loss: 0.1601266860961914
Batch 57/64 loss: 0.16739749908447266
Batch 58/64 loss: 0.010901927947998047
Batch 59/64 loss: 0.18418407440185547
Batch 60/64 loss: 0.016154766082763672
Batch 61/64 loss: -0.10175037384033203
Batch 62/64 loss: -0.06257200241088867
Batch 63/64 loss: 0.18100929260253906
Batch 64/64 loss: -4.263973236083984
Epoch 252  Train loss: 0.08754475163478477  Val loss: 0.009994939430472777
Epoch 253
-------------------------------
Batch 1/64 loss: -0.29698610305786133
Batch 2/64 loss: -0.28972578048706055
Batch 3/64 loss: 0.372067928314209
Batch 4/64 loss: -0.16057682037353516
Batch 5/64 loss: 0.018472671508789062
Batch 6/64 loss: 0.31958675384521484
Batch 7/64 loss: -0.04475259780883789
Batch 8/64 loss: 0.2742619514465332
Batch 9/64 loss: -0.24054718017578125
Batch 10/64 loss: 0.17890024185180664
Batch 11/64 loss: 0.19600152969360352
Batch 12/64 loss: 0.14373207092285156
Batch 13/64 loss: 0.5204696655273438
Batch 14/64 loss: -0.18241119384765625
Batch 15/64 loss: 0.1082005500793457
Batch 16/64 loss: -0.07363224029541016
Batch 17/64 loss: -0.1984391212463379
Batch 18/64 loss: -0.15093135833740234
Batch 19/64 loss: 0.038738250732421875
Batch 20/64 loss: -0.010046958923339844
Batch 21/64 loss: -0.32788562774658203
Batch 22/64 loss: -0.17130756378173828
Batch 23/64 loss: 0.16639328002929688
Batch 24/64 loss: -0.1669635772705078
Batch 25/64 loss: -0.040332794189453125
Batch 26/64 loss: 0.23830604553222656
Batch 27/64 loss: 0.12103271484375
Batch 28/64 loss: -0.2606782913208008
Batch 29/64 loss: -0.1770009994506836
Batch 30/64 loss: 0.1316695213317871
Batch 31/64 loss: 0.12416458129882812
Batch 32/64 loss: -0.009354591369628906
Batch 33/64 loss: -0.06397056579589844
Batch 34/64 loss: -0.20416545867919922
Batch 35/64 loss: -0.21471357345581055
Batch 36/64 loss: 0.4538230895996094
Batch 37/64 loss: 0.06536340713500977
Batch 38/64 loss: 0.14194917678833008
Batch 39/64 loss: -0.09566736221313477
Batch 40/64 loss: -0.03154897689819336
Batch 41/64 loss: -0.30184364318847656
Batch 42/64 loss: -0.2498316764831543
Batch 43/64 loss: -0.30805397033691406
Batch 44/64 loss: 0.37357664108276367
Batch 45/64 loss: 0.06926584243774414
Batch 46/64 loss: -0.1912064552307129
Batch 47/64 loss: -0.02980184555053711
Batch 48/64 loss: -0.13942337036132812
Batch 49/64 loss: 0.10176849365234375
Batch 50/64 loss: -0.01267242431640625
Batch 51/64 loss: 0.5899457931518555
Batch 52/64 loss: -0.2535538673400879
Batch 53/64 loss: -0.12718772888183594
Batch 54/64 loss: -0.1699666976928711
Batch 55/64 loss: 0.011320114135742188
Batch 56/64 loss: 0.3256344795227051
Batch 57/64 loss: 0.29139232635498047
Batch 58/64 loss: -0.34023094177246094
Batch 59/64 loss: -0.19034957885742188
Batch 60/64 loss: -0.32407093048095703
Batch 61/64 loss: -0.21322917938232422
Batch 62/64 loss: -0.14574384689331055
Batch 63/64 loss: 0.049970149993896484
Batch 64/64 loss: -3.5352344512939453
Epoch 253  Train loss: -0.057007426841586245  Val loss: -0.03676953922022659
Saving best model, epoch: 253
Epoch 254
-------------------------------
Batch 1/64 loss: -0.4785165786743164
Batch 2/64 loss: -0.1363673210144043
Batch 3/64 loss: 0.049248695373535156
Batch 4/64 loss: -0.04181098937988281
Batch 5/64 loss: -0.028044700622558594
Batch 6/64 loss: 0.1997685432434082
Batch 7/64 loss: 0.08926010131835938
Batch 8/64 loss: -0.3329648971557617
Batch 9/64 loss: -0.2786855697631836
Batch 10/64 loss: -0.3279705047607422
Batch 11/64 loss: -0.07310914993286133
Batch 12/64 loss: -0.03285360336303711
Batch 13/64 loss: 0.11028718948364258
Batch 14/64 loss: -0.10163497924804688
Batch 15/64 loss: 0.010139942169189453
Batch 16/64 loss: 0.06255674362182617
Batch 17/64 loss: -0.13569259643554688
Batch 18/64 loss: 0.04319953918457031
Batch 19/64 loss: 0.1983633041381836
Batch 20/64 loss: 0.09379768371582031
Batch 21/64 loss: -0.058101654052734375
Batch 22/64 loss: -0.21712684631347656
Batch 23/64 loss: 0.24972248077392578
Batch 24/64 loss: -0.030341148376464844
Batch 25/64 loss: -0.3923940658569336
Batch 26/64 loss: -0.10451793670654297
Batch 27/64 loss: -0.13419532775878906
Batch 28/64 loss: -0.14821243286132812
Batch 29/64 loss: -0.11150264739990234
Batch 30/64 loss: 0.5094509124755859
Batch 31/64 loss: -0.17863845825195312
Batch 32/64 loss: 0.15954113006591797
Batch 33/64 loss: -0.1380910873413086
Batch 34/64 loss: -0.018555641174316406
Batch 35/64 loss: -0.3274984359741211
Batch 36/64 loss: -0.37511539459228516
Batch 37/64 loss: -0.058391571044921875
Batch 38/64 loss: -0.008408069610595703
Batch 39/64 loss: -0.1839008331298828
Batch 40/64 loss: -0.3210601806640625
Batch 41/64 loss: -0.38957881927490234
Batch 42/64 loss: -0.07387495040893555
Batch 43/64 loss: -0.3867464065551758
Batch 44/64 loss: 0.6778345108032227
Batch 45/64 loss: -0.2644777297973633
Batch 46/64 loss: 0.23996543884277344
Batch 47/64 loss: 0.32941389083862305
Batch 48/64 loss: -0.09991598129272461
Batch 49/64 loss: 0.06244373321533203
Batch 50/64 loss: 0.1351633071899414
Batch 51/64 loss: 0.21869373321533203
Batch 52/64 loss: 0.05978059768676758
Batch 53/64 loss: -0.060736656188964844
Batch 54/64 loss: -0.1422901153564453
Batch 55/64 loss: -0.19388914108276367
Batch 56/64 loss: -0.03080129623413086
Batch 57/64 loss: -0.24906063079833984
Batch 58/64 loss: -0.20911264419555664
Batch 59/64 loss: -0.1386728286743164
Batch 60/64 loss: -0.11912059783935547
Batch 61/64 loss: 0.32591867446899414
Batch 62/64 loss: -0.2655153274536133
Batch 63/64 loss: -0.023775577545166016
Batch 64/64 loss: -3.4201855659484863
Epoch 254  Train loss: -0.09665663289088829  Val loss: -0.05144952990345119
Saving best model, epoch: 254
Epoch 255
-------------------------------
Batch 1/64 loss: -0.15346622467041016
Batch 2/64 loss: 0.20254278182983398
Batch 3/64 loss: -0.2089376449584961
Batch 4/64 loss: 0.45474767684936523
Batch 5/64 loss: -0.2257547378540039
Batch 6/64 loss: -0.11827707290649414
Batch 7/64 loss: -0.1065826416015625
Batch 8/64 loss: -0.07303762435913086
Batch 9/64 loss: -0.24903059005737305
Batch 10/64 loss: -0.12044620513916016
Batch 11/64 loss: -0.3459620475769043
Batch 12/64 loss: 0.11531400680541992
Batch 13/64 loss: 0.012965202331542969
Batch 14/64 loss: -0.08069419860839844
Batch 15/64 loss: 0.024295806884765625
Batch 16/64 loss: -0.033143043518066406
Batch 17/64 loss: -0.3016347885131836
Batch 18/64 loss: -0.1144719123840332
Batch 19/64 loss: -0.23883390426635742
Batch 20/64 loss: -0.09926700592041016
Batch 21/64 loss: -0.27454471588134766
Batch 22/64 loss: -0.19467449188232422
Batch 23/64 loss: -0.32178783416748047
Batch 24/64 loss: 0.07023048400878906
Batch 25/64 loss: -0.12972164154052734
Batch 26/64 loss: 0.6060791015625
Batch 27/64 loss: -0.06559467315673828
Batch 28/64 loss: -0.3899869918823242
Batch 29/64 loss: 0.31369829177856445
Batch 30/64 loss: 0.4287090301513672
Batch 31/64 loss: 0.07357597351074219
Batch 32/64 loss: 0.04747962951660156
Batch 33/64 loss: 0.32507753372192383
Batch 34/64 loss: -0.09826850891113281
Batch 35/64 loss: -0.11205053329467773
Batch 36/64 loss: -0.1782846450805664
Batch 37/64 loss: 0.028590679168701172
Batch 38/64 loss: -0.11607599258422852
Batch 39/64 loss: -0.3523712158203125
Batch 40/64 loss: 0.24094009399414062
Batch 41/64 loss: 0.023799896240234375
Batch 42/64 loss: -0.022418498992919922
Batch 43/64 loss: -0.08728408813476562
Batch 44/64 loss: 0.11668157577514648
Batch 45/64 loss: 0.4148736000061035
Batch 46/64 loss: -0.07104873657226562
Batch 47/64 loss: -0.07602071762084961
Batch 48/64 loss: 0.15100955963134766
Batch 49/64 loss: -0.26721811294555664
Batch 50/64 loss: 0.0845193862915039
Batch 51/64 loss: -0.2497396469116211
Batch 52/64 loss: -0.2945847511291504
Batch 53/64 loss: -0.06661653518676758
Batch 54/64 loss: -0.5249166488647461
Batch 55/64 loss: -0.27876949310302734
Batch 56/64 loss: -0.2566366195678711
Batch 57/64 loss: 0.2678050994873047
Batch 58/64 loss: -0.33493947982788086
Batch 59/64 loss: 0.032314300537109375
Batch 60/64 loss: 0.0723104476928711
Batch 61/64 loss: -0.4152956008911133
Batch 62/64 loss: -0.24295520782470703
Batch 63/64 loss: -0.052875518798828125
Batch 64/64 loss: -3.8823680877685547
Epoch 255  Train loss: -0.10585782668169808  Val loss: -0.0012793786746939434
Epoch 256
-------------------------------
Batch 1/64 loss: -0.4362316131591797
Batch 2/64 loss: -0.039105892181396484
Batch 3/64 loss: 0.06760311126708984
Batch 4/64 loss: -0.3529243469238281
Batch 5/64 loss: -0.6332263946533203
Batch 6/64 loss: -0.2712101936340332
Batch 7/64 loss: -0.106231689453125
Batch 8/64 loss: -0.1360335350036621
Batch 9/64 loss: -0.46001243591308594
Batch 10/64 loss: 0.022489070892333984
Batch 11/64 loss: -0.46240234375
Batch 12/64 loss: 0.12359428405761719
Batch 13/64 loss: 0.1360330581665039
Batch 14/64 loss: -0.10240364074707031
Batch 15/64 loss: -0.2561483383178711
Batch 16/64 loss: -0.2663421630859375
Batch 17/64 loss: -0.12987661361694336
Batch 18/64 loss: 0.05068635940551758
Batch 19/64 loss: 0.3784360885620117
Batch 20/64 loss: -0.26874256134033203
Batch 21/64 loss: -0.1354684829711914
Batch 22/64 loss: -0.39156341552734375
Batch 23/64 loss: 0.22126531600952148
Batch 24/64 loss: 0.20144081115722656
Batch 25/64 loss: 0.06129121780395508
Batch 26/64 loss: 0.4828205108642578
Batch 27/64 loss: -0.30574703216552734
Batch 28/64 loss: -0.19158077239990234
Batch 29/64 loss: -0.16560792922973633
Batch 30/64 loss: 0.1953744888305664
Batch 31/64 loss: 0.17624282836914062
Batch 32/64 loss: -0.33910226821899414
Batch 33/64 loss: -0.019742965698242188
Batch 34/64 loss: 0.1132650375366211
Batch 35/64 loss: -0.054300785064697266
Batch 36/64 loss: 0.2747321128845215
Batch 37/64 loss: 0.012172698974609375
Batch 38/64 loss: 0.4049201011657715
Batch 39/64 loss: -0.04785919189453125
Batch 40/64 loss: 0.1927323341369629
Batch 41/64 loss: 0.09641170501708984
Batch 42/64 loss: -0.232086181640625
Batch 43/64 loss: 0.016808032989501953
Batch 44/64 loss: -0.2847328186035156
Batch 45/64 loss: -0.19217538833618164
Batch 46/64 loss: 0.37291669845581055
Batch 47/64 loss: 0.1321730613708496
Batch 48/64 loss: -0.1630268096923828
Batch 49/64 loss: -0.1642603874206543
Batch 50/64 loss: -0.20602893829345703
Batch 51/64 loss: -0.03503608703613281
Batch 52/64 loss: -0.3874349594116211
Batch 53/64 loss: -0.36608028411865234
Batch 54/64 loss: 0.04300117492675781
Batch 55/64 loss: -0.39151906967163086
Batch 56/64 loss: 0.11063766479492188
Batch 57/64 loss: -0.27189111709594727
Batch 58/64 loss: 0.014086723327636719
Batch 59/64 loss: 0.10891485214233398
Batch 60/64 loss: 0.2542600631713867
Batch 61/64 loss: -0.10151863098144531
Batch 62/64 loss: -0.1114192008972168
Batch 63/64 loss: -0.12296485900878906
Batch 64/64 loss: -3.692572593688965
Epoch 256  Train loss: -0.11148485295912798  Val loss: 0.037755042007288984
Epoch 257
-------------------------------
Batch 1/64 loss: -0.3191404342651367
Batch 2/64 loss: -0.22333049774169922
Batch 3/64 loss: -0.053277015686035156
Batch 4/64 loss: -0.11416482925415039
Batch 5/64 loss: 0.3906059265136719
Batch 6/64 loss: -0.3906126022338867
Batch 7/64 loss: 0.31408262252807617
Batch 8/64 loss: -0.09395027160644531
Batch 9/64 loss: -0.2272024154663086
Batch 10/64 loss: -0.19970417022705078
Batch 11/64 loss: -0.060303688049316406
Batch 12/64 loss: -0.2570676803588867
Batch 13/64 loss: -0.18574810028076172
Batch 14/64 loss: -0.05661344528198242
Batch 15/64 loss: -0.2881946563720703
Batch 16/64 loss: -0.20258426666259766
Batch 17/64 loss: 0.036952972412109375
Batch 18/64 loss: -0.2204303741455078
Batch 19/64 loss: -0.4482088088989258
Batch 20/64 loss: 0.10824871063232422
Batch 21/64 loss: 0.1644153594970703
Batch 22/64 loss: -7.724761962890625e-05
Batch 23/64 loss: -0.01612377166748047
Batch 24/64 loss: -0.3062458038330078
Batch 25/64 loss: 0.25815725326538086
Batch 26/64 loss: 0.031031131744384766
Batch 27/64 loss: 0.1644725799560547
Batch 28/64 loss: -0.23285961151123047
Batch 29/64 loss: -0.006752490997314453
Batch 30/64 loss: 0.03323841094970703
Batch 31/64 loss: -0.43611717224121094
Batch 32/64 loss: 2.144577980041504
Batch 33/64 loss: 0.31255149841308594
Batch 34/64 loss: 0.21160554885864258
Batch 35/64 loss: 0.0021576881408691406
Batch 36/64 loss: -0.14553451538085938
Batch 37/64 loss: -0.09614276885986328
Batch 38/64 loss: 0.15393733978271484
Batch 39/64 loss: -0.20583820343017578
Batch 40/64 loss: 0.17920637130737305
Batch 41/64 loss: 0.4563918113708496
Batch 42/64 loss: 0.4306211471557617
Batch 43/64 loss: -0.0897216796875
Batch 44/64 loss: 0.44415855407714844
Batch 45/64 loss: 0.32163143157958984
Batch 46/64 loss: 0.04145383834838867
Batch 47/64 loss: 0.39430761337280273
Batch 48/64 loss: -0.26653385162353516
Batch 49/64 loss: 0.3260927200317383
Batch 50/64 loss: 1.0373172760009766
Batch 51/64 loss: -0.1327953338623047
Batch 52/64 loss: 0.6651706695556641
Batch 53/64 loss: 0.06273317337036133
Batch 54/64 loss: 0.4102654457092285
Batch 55/64 loss: 0.5682806968688965
Batch 56/64 loss: 0.13835430145263672
Batch 57/64 loss: 2.675527572631836
Batch 58/64 loss: 0.6906623840332031
Batch 59/64 loss: 0.7893638610839844
Batch 60/64 loss: 0.1998124122619629
Batch 61/64 loss: 0.6533050537109375
Batch 62/64 loss: 0.5001606941223145
Batch 63/64 loss: 0.2607865333557129
Batch 64/64 loss: -3.4441986083984375
Epoch 257  Train loss: 0.12099159091126685  Val loss: 0.4226530016083078
Epoch 258
-------------------------------
Batch 1/64 loss: 0.25695037841796875
Batch 2/64 loss: 0.8033437728881836
Batch 3/64 loss: 0.03769254684448242
Batch 4/64 loss: 0.14287948608398438
Batch 5/64 loss: 0.1777324676513672
Batch 6/64 loss: 1.7772579193115234
Batch 7/64 loss: -0.1405348777770996
Batch 8/64 loss: 0.16723251342773438
Batch 9/64 loss: 0.45357370376586914
Batch 10/64 loss: 0.15395116806030273
Batch 11/64 loss: 1.3245458602905273
Batch 12/64 loss: 0.35272216796875
Batch 13/64 loss: 0.0434565544128418
Batch 14/64 loss: 1.0743370056152344
Batch 15/64 loss: -0.0012345314025878906
Batch 16/64 loss: 0.8515496253967285
Batch 17/64 loss: 0.3496856689453125
Batch 18/64 loss: 0.745762825012207
Batch 19/64 loss: 0.42681360244750977
Batch 20/64 loss: 0.3602137565612793
Batch 21/64 loss: 0.588381290435791
Batch 22/64 loss: 0.3424654006958008
Batch 23/64 loss: 0.26925182342529297
Batch 24/64 loss: 0.3503561019897461
Batch 25/64 loss: 0.40476036071777344
Batch 26/64 loss: 0.336122989654541
Batch 27/64 loss: 0.12363004684448242
Batch 28/64 loss: 0.22496891021728516
Batch 29/64 loss: 0.5231947898864746
Batch 30/64 loss: 0.13507080078125
Batch 31/64 loss: 0.24652481079101562
Batch 32/64 loss: -0.11814498901367188
Batch 33/64 loss: 0.4412240982055664
Batch 34/64 loss: 0.30733251571655273
Batch 35/64 loss: 0.09187650680541992
Batch 36/64 loss: 0.4906797409057617
Batch 37/64 loss: 0.33522939682006836
Batch 38/64 loss: 0.2435598373413086
Batch 39/64 loss: -0.12499141693115234
Batch 40/64 loss: -0.12313461303710938
Batch 41/64 loss: 0.29814577102661133
Batch 42/64 loss: 0.1201314926147461
Batch 43/64 loss: 0.1276397705078125
Batch 44/64 loss: 0.010203838348388672
Batch 45/64 loss: -0.015637874603271484
Batch 46/64 loss: 0.26601457595825195
Batch 47/64 loss: -0.044040679931640625
Batch 48/64 loss: 1.5232329368591309
Batch 49/64 loss: 0.5830817222595215
Batch 50/64 loss: 0.1429767608642578
Batch 51/64 loss: 0.3769516944885254
Batch 52/64 loss: 0.360107421875
Batch 53/64 loss: -0.07262039184570312
Batch 54/64 loss: 2.0321555137634277
Batch 55/64 loss: -0.10399866104125977
Batch 56/64 loss: 0.12125205993652344
Batch 57/64 loss: 0.09194087982177734
Batch 58/64 loss: 0.589385986328125
Batch 59/64 loss: 0.40110254287719727
Batch 60/64 loss: -0.017479419708251953
Batch 61/64 loss: 0.2973814010620117
Batch 62/64 loss: 0.23787784576416016
Batch 63/64 loss: 0.3914060592651367
Batch 64/64 loss: -3.4781551361083984
Epoch 258  Train loss: 0.30674329271503525  Val loss: 0.15796391333091708
Epoch 259
-------------------------------
Batch 1/64 loss: -0.2574162483215332
Batch 2/64 loss: 2.8289222717285156
Batch 3/64 loss: 0.5168952941894531
Batch 4/64 loss: -0.012223243713378906
Batch 5/64 loss: 0.14538049697875977
Batch 6/64 loss: 0.010900497436523438
Batch 7/64 loss: -0.06051492691040039
Batch 8/64 loss: -0.10998249053955078
Batch 9/64 loss: 0.004294872283935547
Batch 10/64 loss: 0.40871143341064453
Batch 11/64 loss: -0.22221899032592773
Batch 12/64 loss: -0.1206517219543457
Batch 13/64 loss: 0.26087236404418945
Batch 14/64 loss: 0.115264892578125
Batch 15/64 loss: 0.12467670440673828
Batch 16/64 loss: 0.17052936553955078
Batch 17/64 loss: -0.06316757202148438
Batch 18/64 loss: 0.10948944091796875
Batch 19/64 loss: -0.025333881378173828
Batch 20/64 loss: 0.18605613708496094
Batch 21/64 loss: -0.0017004013061523438
Batch 22/64 loss: 0.13090896606445312
Batch 23/64 loss: -0.2240447998046875
Batch 24/64 loss: -0.1537332534790039
Batch 25/64 loss: 0.2003183364868164
Batch 26/64 loss: 0.09749794006347656
Batch 27/64 loss: 0.5295701026916504
Batch 28/64 loss: 0.29802751541137695
Batch 29/64 loss: 0.7615413665771484
Batch 30/64 loss: 0.06373882293701172
Batch 31/64 loss: 0.2432723045349121
Batch 32/64 loss: 0.2249751091003418
Batch 33/64 loss: 0.20239496231079102
Batch 34/64 loss: -0.21892547607421875
Batch 35/64 loss: -0.1429123878479004
Batch 36/64 loss: 1.8922719955444336
Batch 37/64 loss: 0.2899775505065918
Batch 38/64 loss: -0.15778350830078125
Batch 39/64 loss: 0.29696178436279297
Batch 40/64 loss: -0.14379262924194336
Batch 41/64 loss: 0.21694183349609375
Batch 42/64 loss: 0.029401779174804688
Batch 43/64 loss: 0.3493804931640625
Batch 44/64 loss: 0.23491621017456055
Batch 45/64 loss: 0.19100666046142578
Batch 46/64 loss: 0.25003814697265625
Batch 47/64 loss: -0.0078277587890625
Batch 48/64 loss: 0.08332061767578125
Batch 49/64 loss: 0.3271336555480957
Batch 50/64 loss: 0.05564069747924805
Batch 51/64 loss: 0.19529342651367188
Batch 52/64 loss: 0.3598904609680176
Batch 53/64 loss: 0.08777761459350586
Batch 54/64 loss: 0.0552973747253418
Batch 55/64 loss: 0.10896492004394531
Batch 56/64 loss: 0.3102254867553711
Batch 57/64 loss: 0.3385047912597656
Batch 58/64 loss: 0.21914148330688477
Batch 59/64 loss: 0.1964702606201172
Batch 60/64 loss: -0.1770458221435547
Batch 61/64 loss: 0.06655454635620117
Batch 62/64 loss: 0.17980194091796875
Batch 63/64 loss: 0.026981353759765625
Batch 64/64 loss: -3.804934024810791
Epoch 259  Train loss: 0.14185346902585497  Val loss: 0.11313394500627551
Epoch 260
-------------------------------
Batch 1/64 loss: -0.15559148788452148
Batch 2/64 loss: 0.24116086959838867
Batch 3/64 loss: 0.11113786697387695
Batch 4/64 loss: 1.4729251861572266
Batch 5/64 loss: 0.0865030288696289
Batch 6/64 loss: 0.007586956024169922
Batch 7/64 loss: 0.04281282424926758
Batch 8/64 loss: 0.3343772888183594
Batch 9/64 loss: 0.13599777221679688
Batch 10/64 loss: -0.4241628646850586
Batch 11/64 loss: -0.40667247772216797
Batch 12/64 loss: -0.19427967071533203
Batch 13/64 loss: 0.18988513946533203
Batch 14/64 loss: 0.1500225067138672
Batch 15/64 loss: -0.3803882598876953
Batch 16/64 loss: -0.3719635009765625
Batch 17/64 loss: 0.5274815559387207
Batch 18/64 loss: -0.2218184471130371
Batch 19/64 loss: 0.09277725219726562
Batch 20/64 loss: -0.010029315948486328
Batch 21/64 loss: -0.09845447540283203
Batch 22/64 loss: 0.2662181854248047
Batch 23/64 loss: -0.11960411071777344
Batch 24/64 loss: -0.12213516235351562
Batch 25/64 loss: 0.1314373016357422
Batch 26/64 loss: -0.016237735748291016
Batch 27/64 loss: -0.30909061431884766
Batch 28/64 loss: 0.050868988037109375
Batch 29/64 loss: 0.23486566543579102
Batch 30/64 loss: 0.30377960205078125
Batch 31/64 loss: 0.3347787857055664
Batch 32/64 loss: 0.18970394134521484
Batch 33/64 loss: -0.007726192474365234
Batch 34/64 loss: -0.019319534301757812
Batch 35/64 loss: -0.10800027847290039
Batch 36/64 loss: -0.06902837753295898
Batch 37/64 loss: 0.44507551193237305
Batch 38/64 loss: -0.22740650177001953
Batch 39/64 loss: 0.013086318969726562
Batch 40/64 loss: 0.014554023742675781
Batch 41/64 loss: -0.24889516830444336
Batch 42/64 loss: -0.03232526779174805
Batch 43/64 loss: 0.11915969848632812
Batch 44/64 loss: 0.11968183517456055
Batch 45/64 loss: 0.013423442840576172
Batch 46/64 loss: -0.06516075134277344
Batch 47/64 loss: -0.04014110565185547
Batch 48/64 loss: 0.0073089599609375
Batch 49/64 loss: 0.02353191375732422
Batch 50/64 loss: 0.24861717224121094
Batch 51/64 loss: -0.10328435897827148
Batch 52/64 loss: -0.005209922790527344
Batch 53/64 loss: 0.38344860076904297
Batch 54/64 loss: 0.09748363494873047
Batch 55/64 loss: -0.16747665405273438
Batch 56/64 loss: -0.09999752044677734
Batch 57/64 loss: -0.0716404914855957
Batch 58/64 loss: -0.04371213912963867
Batch 59/64 loss: -0.04180479049682617
Batch 60/64 loss: -0.012494087219238281
Batch 61/64 loss: 0.1162099838256836
Batch 62/64 loss: -0.20121097564697266
Batch 63/64 loss: 0.045046329498291016
Batch 64/64 loss: -3.6107711791992188
Epoch 260  Train loss: -0.008664980121687347  Val loss: -0.030479667112999355
Epoch 261
-------------------------------
Batch 1/64 loss: -0.37337779998779297
Batch 2/64 loss: 0.22167348861694336
Batch 3/64 loss: -0.29738521575927734
Batch 4/64 loss: -0.016637325286865234
Batch 5/64 loss: -0.026669025421142578
Batch 6/64 loss: -0.05370044708251953
Batch 7/64 loss: -0.21310186386108398
Batch 8/64 loss: -0.46720123291015625
Batch 9/64 loss: -0.5114889144897461
Batch 10/64 loss: -0.26082515716552734
Batch 11/64 loss: 0.08856439590454102
Batch 12/64 loss: -0.07983922958374023
Batch 13/64 loss: -0.4348182678222656
Batch 14/64 loss: -0.39368629455566406
Batch 15/64 loss: 0.21195745468139648
Batch 16/64 loss: -0.24176311492919922
Batch 17/64 loss: 0.11582517623901367
Batch 18/64 loss: 0.1117715835571289
Batch 19/64 loss: 0.07186508178710938
Batch 20/64 loss: -0.06586551666259766
Batch 21/64 loss: -0.13542747497558594
Batch 22/64 loss: -0.01150369644165039
Batch 23/64 loss: -0.25727415084838867
Batch 24/64 loss: -0.2383575439453125
Batch 25/64 loss: -0.36849498748779297
Batch 26/64 loss: -0.07050561904907227
Batch 27/64 loss: -0.021459102630615234
Batch 28/64 loss: -0.3650627136230469
Batch 29/64 loss: -0.3036470413208008
Batch 30/64 loss: 0.34786367416381836
Batch 31/64 loss: -0.31994152069091797
Batch 32/64 loss: -0.20296716690063477
Batch 33/64 loss: 0.1060934066772461
Batch 34/64 loss: 0.0014276504516601562
Batch 35/64 loss: -0.2396869659423828
Batch 36/64 loss: -0.3691291809082031
Batch 37/64 loss: -0.031281471252441406
Batch 38/64 loss: -0.2738056182861328
Batch 39/64 loss: -0.10334968566894531
Batch 40/64 loss: 0.4528775215148926
Batch 41/64 loss: -0.13179588317871094
Batch 42/64 loss: -0.16973209381103516
Batch 43/64 loss: 0.035611629486083984
Batch 44/64 loss: -0.21108531951904297
Batch 45/64 loss: -0.05990028381347656
Batch 46/64 loss: -0.02285289764404297
Batch 47/64 loss: 0.12944793701171875
Batch 48/64 loss: 0.18088388442993164
Batch 49/64 loss: 0.7497782707214355
Batch 50/64 loss: 0.0058002471923828125
Batch 51/64 loss: 0.007161140441894531
Batch 52/64 loss: -0.2739391326904297
Batch 53/64 loss: -0.0158843994140625
Batch 54/64 loss: 0.3022336959838867
Batch 55/64 loss: 0.3687901496887207
Batch 56/64 loss: 0.07614850997924805
Batch 57/64 loss: 0.11930179595947266
Batch 58/64 loss: 0.21835565567016602
Batch 59/64 loss: 0.49208736419677734
Batch 60/64 loss: -0.19017744064331055
Batch 61/64 loss: -0.12662029266357422
Batch 62/64 loss: 0.1988821029663086
Batch 63/64 loss: 0.16752862930297852
Batch 64/64 loss: -3.536068916320801
Epoch 261  Train loss: -0.09129980124679266  Val loss: 0.08300832374808714
Epoch 262
-------------------------------
Batch 1/64 loss: -0.12979936599731445
Batch 2/64 loss: 0.04818868637084961
Batch 3/64 loss: -0.26093101501464844
Batch 4/64 loss: -0.002883434295654297
Batch 5/64 loss: 0.2459855079650879
Batch 6/64 loss: 0.027993202209472656
Batch 7/64 loss: 0.052512168884277344
Batch 8/64 loss: -0.2695913314819336
Batch 9/64 loss: -0.13540267944335938
Batch 10/64 loss: 0.013024330139160156
Batch 11/64 loss: 0.3601713180541992
Batch 12/64 loss: 0.26225805282592773
Batch 13/64 loss: -0.24567317962646484
Batch 14/64 loss: -0.3558807373046875
Batch 15/64 loss: 0.18560314178466797
Batch 16/64 loss: 0.5551028251647949
Batch 17/64 loss: 0.12133646011352539
Batch 18/64 loss: -0.24737024307250977
Batch 19/64 loss: -0.4555521011352539
Batch 20/64 loss: 0.046298980712890625
Batch 21/64 loss: -0.10206842422485352
Batch 22/64 loss: 0.5623874664306641
Batch 23/64 loss: -0.2676258087158203
Batch 24/64 loss: -0.19311285018920898
Batch 25/64 loss: -0.29076480865478516
Batch 26/64 loss: -0.1304187774658203
Batch 27/64 loss: -0.20448017120361328
Batch 28/64 loss: 0.04750633239746094
Batch 29/64 loss: 0.19189739227294922
Batch 30/64 loss: -0.29221153259277344
Batch 31/64 loss: 0.14933109283447266
Batch 32/64 loss: -0.20880603790283203
Batch 33/64 loss: 0.053999900817871094
Batch 34/64 loss: -0.030848979949951172
Batch 35/64 loss: 0.07901716232299805
Batch 36/64 loss: -0.12327051162719727
Batch 37/64 loss: 0.019867420196533203
Batch 38/64 loss: -0.1199026107788086
Batch 39/64 loss: -0.5200338363647461
Batch 40/64 loss: -0.3125581741333008
Batch 41/64 loss: -0.3902587890625
Batch 42/64 loss: -0.08231019973754883
Batch 43/64 loss: -0.18524646759033203
Batch 44/64 loss: -0.2904987335205078
Batch 45/64 loss: 0.07503509521484375
Batch 46/64 loss: 0.07540225982666016
Batch 47/64 loss: -0.014119625091552734
Batch 48/64 loss: -0.11259174346923828
Batch 49/64 loss: -0.05448627471923828
Batch 50/64 loss: -0.19292020797729492
Batch 51/64 loss: -0.1531076431274414
Batch 52/64 loss: -0.19197463989257812
Batch 53/64 loss: 0.4406094551086426
Batch 54/64 loss: 0.16037988662719727
Batch 55/64 loss: -0.2672557830810547
Batch 56/64 loss: -0.09765481948852539
Batch 57/64 loss: 0.07028007507324219
Batch 58/64 loss: -0.14116907119750977
Batch 59/64 loss: -0.09511566162109375
Batch 60/64 loss: 0.13532352447509766
Batch 61/64 loss: -0.06090211868286133
Batch 62/64 loss: -0.15987396240234375
Batch 63/64 loss: -0.07992172241210938
Batch 64/64 loss: -4.011490821838379
Epoch 262  Train loss: -0.10192471298516965  Val loss: -0.04491381301093347
Epoch 263
-------------------------------
Batch 1/64 loss: 0.050162315368652344
Batch 2/64 loss: 0.08927583694458008
Batch 3/64 loss: 0.28009986877441406
Batch 4/64 loss: -0.13125944137573242
Batch 5/64 loss: -0.48624610900878906
Batch 6/64 loss: -0.14719438552856445
Batch 7/64 loss: -0.29773426055908203
Batch 8/64 loss: -0.22735309600830078
Batch 9/64 loss: -0.08544492721557617
Batch 10/64 loss: -0.01562976837158203
Batch 11/64 loss: -0.004235744476318359
Batch 12/64 loss: -0.0681314468383789
Batch 13/64 loss: 0.06519842147827148
Batch 14/64 loss: -0.3008537292480469
Batch 15/64 loss: -0.08388996124267578
Batch 16/64 loss: -0.21462631225585938
Batch 17/64 loss: -0.10319280624389648
Batch 18/64 loss: -0.21369171142578125
Batch 19/64 loss: -0.13139772415161133
Batch 20/64 loss: -0.1495380401611328
Batch 21/64 loss: -0.09611845016479492
Batch 22/64 loss: -0.2901887893676758
Batch 23/64 loss: -0.19817447662353516
Batch 24/64 loss: -0.3161582946777344
Batch 25/64 loss: -0.1295309066772461
Batch 26/64 loss: -0.34513187408447266
Batch 27/64 loss: 0.4603266716003418
Batch 28/64 loss: -0.027782440185546875
Batch 29/64 loss: 0.008940696716308594
Batch 30/64 loss: 0.23386287689208984
Batch 31/64 loss: -0.2290477752685547
Batch 32/64 loss: -0.3502168655395508
Batch 33/64 loss: -0.23130130767822266
Batch 34/64 loss: 0.16465473175048828
Batch 35/64 loss: -0.5880746841430664
Batch 36/64 loss: 0.007675647735595703
Batch 37/64 loss: -0.22560453414916992
Batch 38/64 loss: -0.028347492218017578
Batch 39/64 loss: 0.042670249938964844
Batch 40/64 loss: -0.2157001495361328
Batch 41/64 loss: -0.0890035629272461
Batch 42/64 loss: 0.022209644317626953
Batch 43/64 loss: -0.048247337341308594
Batch 44/64 loss: -0.2831459045410156
Batch 45/64 loss: -0.2494192123413086
Batch 46/64 loss: -0.04407501220703125
Batch 47/64 loss: 0.12883520126342773
Batch 48/64 loss: -0.11420726776123047
Batch 49/64 loss: -0.15914630889892578
Batch 50/64 loss: 0.1294722557067871
Batch 51/64 loss: -0.3310508728027344
Batch 52/64 loss: 0.40471649169921875
Batch 53/64 loss: 0.0687708854675293
Batch 54/64 loss: 0.029143333435058594
Batch 55/64 loss: -0.11044645309448242
Batch 56/64 loss: -0.1488337516784668
Batch 57/64 loss: 0.0955357551574707
Batch 58/64 loss: -0.1064295768737793
Batch 59/64 loss: 0.1484375
Batch 60/64 loss: -0.3498878479003906
Batch 61/64 loss: -0.07873010635375977
Batch 62/64 loss: -0.2563495635986328
Batch 63/64 loss: -0.3271350860595703
Batch 64/64 loss: -4.045157432556152
Epoch 263  Train loss: -0.1448123146505917  Val loss: -0.0030784738022846866
Epoch 264
-------------------------------
Batch 1/64 loss: -0.3888587951660156
Batch 2/64 loss: -0.04498863220214844
Batch 3/64 loss: 0.0838155746459961
Batch 4/64 loss: -0.03911638259887695
Batch 5/64 loss: -0.10590839385986328
Batch 6/64 loss: -0.19196081161499023
Batch 7/64 loss: -0.2351360321044922
Batch 8/64 loss: -0.1681842803955078
Batch 9/64 loss: 0.2069563865661621
Batch 10/64 loss: 0.09320592880249023
Batch 11/64 loss: -0.3127117156982422
Batch 12/64 loss: 0.05698680877685547
Batch 13/64 loss: -0.41562557220458984
Batch 14/64 loss: -0.1436901092529297
Batch 15/64 loss: -0.07530069351196289
Batch 16/64 loss: -0.21000099182128906
Batch 17/64 loss: 0.3133811950683594
Batch 18/64 loss: -0.12783145904541016
Batch 19/64 loss: 0.0804595947265625
Batch 20/64 loss: -0.04627370834350586
Batch 21/64 loss: -0.47094249725341797
Batch 22/64 loss: -0.12246990203857422
Batch 23/64 loss: -0.09268903732299805
Batch 24/64 loss: 0.6135587692260742
Batch 25/64 loss: -0.08871936798095703
Batch 26/64 loss: -0.2466573715209961
Batch 27/64 loss: 0.2276453971862793
Batch 28/64 loss: -0.35193729400634766
Batch 29/64 loss: 0.04819011688232422
Batch 30/64 loss: -0.34402942657470703
Batch 31/64 loss: -0.3148641586303711
Batch 32/64 loss: -0.4848442077636719
Batch 33/64 loss: -0.3215312957763672
Batch 34/64 loss: 0.04479217529296875
Batch 35/64 loss: -0.2789793014526367
Batch 36/64 loss: -0.35916614532470703
Batch 37/64 loss: -0.1453094482421875
Batch 38/64 loss: -0.5919189453125
Batch 39/64 loss: -0.23582172393798828
Batch 40/64 loss: 0.12091445922851562
Batch 41/64 loss: 0.1926126480102539
Batch 42/64 loss: -0.19788742065429688
Batch 43/64 loss: 0.21120929718017578
Batch 44/64 loss: 0.04516315460205078
Batch 45/64 loss: -0.11820125579833984
Batch 46/64 loss: -0.0509490966796875
Batch 47/64 loss: -0.2716989517211914
Batch 48/64 loss: 0.04133129119873047
Batch 49/64 loss: -0.2179851531982422
Batch 50/64 loss: -0.35425281524658203
Batch 51/64 loss: -0.1015768051147461
Batch 52/64 loss: -0.19181537628173828
Batch 53/64 loss: -0.01435995101928711
Batch 54/64 loss: -0.01765155792236328
Batch 55/64 loss: -0.08093833923339844
Batch 56/64 loss: -0.13713455200195312
Batch 57/64 loss: 1.1130099296569824
Batch 58/64 loss: 0.34203243255615234
Batch 59/64 loss: -0.0366358757019043
Batch 60/64 loss: 0.30229806900024414
Batch 61/64 loss: -0.2015533447265625
Batch 62/64 loss: -0.14233684539794922
Batch 63/64 loss: 0.10778522491455078
Batch 64/64 loss: -3.908111572265625
Epoch 264  Train loss: -0.12197929831112132  Val loss: 0.05936434506550687
Epoch 265
-------------------------------
Batch 1/64 loss: -0.026469707489013672
Batch 2/64 loss: -0.35088348388671875
Batch 3/64 loss: -0.4337739944458008
Batch 4/64 loss: 0.3343677520751953
Batch 5/64 loss: -0.06449651718139648
Batch 6/64 loss: -0.16180419921875
Batch 7/64 loss: 0.3489370346069336
Batch 8/64 loss: 0.014962196350097656
Batch 9/64 loss: 0.12023258209228516
Batch 10/64 loss: -0.11344051361083984
Batch 11/64 loss: 0.45850086212158203
Batch 12/64 loss: 0.4157123565673828
Batch 13/64 loss: -0.23479938507080078
Batch 14/64 loss: -0.052416324615478516
Batch 15/64 loss: 0.12512922286987305
Batch 16/64 loss: 0.15077877044677734
Batch 17/64 loss: 0.1298666000366211
Batch 18/64 loss: -0.1541452407836914
Batch 19/64 loss: -0.08262825012207031
Batch 20/64 loss: -0.051810264587402344
Batch 21/64 loss: 2.292342185974121
Batch 22/64 loss: 0.2708315849304199
Batch 23/64 loss: 0.14270305633544922
Batch 24/64 loss: 0.30286693572998047
Batch 25/64 loss: -0.21858501434326172
Batch 26/64 loss: 0.042014122009277344
Batch 27/64 loss: -0.3287057876586914
Batch 28/64 loss: -0.2766885757446289
Batch 29/64 loss: 0.6128530502319336
Batch 30/64 loss: 0.05556774139404297
Batch 31/64 loss: -0.1837902069091797
Batch 32/64 loss: -0.1861586570739746
Batch 33/64 loss: 0.1302638053894043
Batch 34/64 loss: 0.10311555862426758
Batch 35/64 loss: 0.10842561721801758
Batch 36/64 loss: 2.5404067039489746
Batch 37/64 loss: -0.21247291564941406
Batch 38/64 loss: 0.7366681098937988
Batch 39/64 loss: 0.10847711563110352
Batch 40/64 loss: -0.024764537811279297
Batch 41/64 loss: 0.07756996154785156
Batch 42/64 loss: 0.03101491928100586
Batch 43/64 loss: 0.28253936767578125
Batch 44/64 loss: 0.05187082290649414
Batch 45/64 loss: 0.13660955429077148
Batch 46/64 loss: -0.07323694229125977
Batch 47/64 loss: 0.05574178695678711
Batch 48/64 loss: 0.17032909393310547
Batch 49/64 loss: -0.07885980606079102
Batch 50/64 loss: 0.21225452423095703
Batch 51/64 loss: -0.10960054397583008
Batch 52/64 loss: 0.36659860610961914
Batch 53/64 loss: -0.21457862854003906
Batch 54/64 loss: -0.14212560653686523
Batch 55/64 loss: 0.2050762176513672
Batch 56/64 loss: -0.3170170783996582
Batch 57/64 loss: 0.09393501281738281
Batch 58/64 loss: 0.10975503921508789
Batch 59/64 loss: 0.05169200897216797
Batch 60/64 loss: -0.06745195388793945
Batch 61/64 loss: -0.12388849258422852
Batch 62/64 loss: -0.19875431060791016
Batch 63/64 loss: -0.03949260711669922
Batch 64/64 loss: -3.235799789428711
Epoch 265  Train loss: 0.06965208614573759  Val loss: 0.05461389338437634
Epoch 266
-------------------------------
Batch 1/64 loss: -0.24347305297851562
Batch 2/64 loss: -0.043843746185302734
Batch 3/64 loss: -0.06889915466308594
Batch 4/64 loss: -0.03870439529418945
Batch 5/64 loss: -0.2612800598144531
Batch 6/64 loss: -0.02016592025756836
Batch 7/64 loss: -0.055113792419433594
Batch 8/64 loss: -0.005195140838623047
Batch 9/64 loss: 0.29451799392700195
Batch 10/64 loss: 1.5021119117736816
Batch 11/64 loss: -0.27123546600341797
Batch 12/64 loss: -0.0033864974975585938
Batch 13/64 loss: 0.041535377502441406
Batch 14/64 loss: -0.20904779434204102
Batch 15/64 loss: 0.27851247787475586
Batch 16/64 loss: 0.4920225143432617
Batch 17/64 loss: 0.01734638214111328
Batch 18/64 loss: 0.2114262580871582
Batch 19/64 loss: -0.07731437683105469
Batch 20/64 loss: -0.31286144256591797
Batch 21/64 loss: -0.18373870849609375
Batch 22/64 loss: -0.21943187713623047
Batch 23/64 loss: 0.2584266662597656
Batch 24/64 loss: -0.16043853759765625
Batch 25/64 loss: -0.08141279220581055
Batch 26/64 loss: 0.3336973190307617
Batch 27/64 loss: 0.1147623062133789
Batch 28/64 loss: -0.10719680786132812
Batch 29/64 loss: 0.3259544372558594
Batch 30/64 loss: -0.019103050231933594
Batch 31/64 loss: 0.37584877014160156
Batch 32/64 loss: -0.21445751190185547
Batch 33/64 loss: -0.26348209381103516
Batch 34/64 loss: -0.18600749969482422
Batch 35/64 loss: 0.10128450393676758
Batch 36/64 loss: 0.16482019424438477
Batch 37/64 loss: -0.2093205451965332
Batch 38/64 loss: -0.15469932556152344
Batch 39/64 loss: -0.4226551055908203
Batch 40/64 loss: -0.2871980667114258
Batch 41/64 loss: -0.35653018951416016
Batch 42/64 loss: -0.3612051010131836
Batch 43/64 loss: -0.3366403579711914
Batch 44/64 loss: -0.21504497528076172
Batch 45/64 loss: 0.11307334899902344
Batch 46/64 loss: 0.07733345031738281
Batch 47/64 loss: -0.004496574401855469
Batch 48/64 loss: 0.2743854522705078
Batch 49/64 loss: 0.09797525405883789
Batch 50/64 loss: -0.2851591110229492
Batch 51/64 loss: -0.2247171401977539
Batch 52/64 loss: 0.2688312530517578
Batch 53/64 loss: -0.23321008682250977
Batch 54/64 loss: -0.1577167510986328
Batch 55/64 loss: -0.0030918121337890625
Batch 56/64 loss: -0.058917999267578125
Batch 57/64 loss: -0.3439340591430664
Batch 58/64 loss: -0.05245542526245117
Batch 59/64 loss: 0.2710719108581543
Batch 60/64 loss: 0.6336007118225098
Batch 61/64 loss: 0.7128028869628906
Batch 62/64 loss: 0.5076475143432617
Batch 63/64 loss: -0.34343624114990234
Batch 64/64 loss: -3.1730427742004395
Epoch 266  Train loss: -0.031482537587483726  Val loss: 0.002002191707440668
Epoch 267
-------------------------------
Batch 1/64 loss: -0.35350894927978516
Batch 2/64 loss: 0.278165340423584
Batch 3/64 loss: 0.010733604431152344
Batch 4/64 loss: -0.3415675163269043
Batch 5/64 loss: 0.06162881851196289
Batch 6/64 loss: -0.042835235595703125
Batch 7/64 loss: 0.058017730712890625
Batch 8/64 loss: -0.2834796905517578
Batch 9/64 loss: -0.3076314926147461
Batch 10/64 loss: -0.038761138916015625
Batch 11/64 loss: -0.12419605255126953
Batch 12/64 loss: -0.055291175842285156
Batch 13/64 loss: 0.4151148796081543
Batch 14/64 loss: -0.09582901000976562
Batch 15/64 loss: 0.40944337844848633
Batch 16/64 loss: 0.33998632431030273
Batch 17/64 loss: -0.1477046012878418
Batch 18/64 loss: -0.2128143310546875
Batch 19/64 loss: -0.2143869400024414
Batch 20/64 loss: -0.41109561920166016
Batch 21/64 loss: -0.2769947052001953
Batch 22/64 loss: 0.020304203033447266
Batch 23/64 loss: -0.4132528305053711
Batch 24/64 loss: 0.054717063903808594
Batch 25/64 loss: -0.5178050994873047
Batch 26/64 loss: 0.21304655075073242
Batch 27/64 loss: -0.1479330062866211
Batch 28/64 loss: -0.1591930389404297
Batch 29/64 loss: 0.32367992401123047
Batch 30/64 loss: -0.4313669204711914
Batch 31/64 loss: -0.10786962509155273
Batch 32/64 loss: -0.13221263885498047
Batch 33/64 loss: -0.4099597930908203
Batch 34/64 loss: -0.2067103385925293
Batch 35/64 loss: 0.008218765258789062
Batch 36/64 loss: -0.0355219841003418
Batch 37/64 loss: -0.043322086334228516
Batch 38/64 loss: -0.337646484375
Batch 39/64 loss: -0.16726398468017578
Batch 40/64 loss: -0.10145092010498047
Batch 41/64 loss: 0.3948636054992676
Batch 42/64 loss: -0.2402663230895996
Batch 43/64 loss: 0.056871891021728516
Batch 44/64 loss: 0.3328280448913574
Batch 45/64 loss: -0.3110837936401367
Batch 46/64 loss: -0.12988996505737305
Batch 47/64 loss: 0.08038949966430664
Batch 48/64 loss: -0.46430015563964844
Batch 49/64 loss: -0.07191133499145508
Batch 50/64 loss: 0.12050390243530273
Batch 51/64 loss: -0.29958486557006836
Batch 52/64 loss: -0.21739864349365234
Batch 53/64 loss: -0.2061920166015625
Batch 54/64 loss: 0.16143798828125
Batch 55/64 loss: 0.14437294006347656
Batch 56/64 loss: 0.0019230842590332031
Batch 57/64 loss: 0.30382394790649414
Batch 58/64 loss: -0.40326690673828125
Batch 59/64 loss: 0.013143539428710938
Batch 60/64 loss: 0.1474137306213379
Batch 61/64 loss: -0.33688831329345703
Batch 62/64 loss: -0.11972999572753906
Batch 63/64 loss: -0.36863279342651367
Batch 64/64 loss: -4.082197189331055
Epoch 267  Train loss: -0.13172971688064875  Val loss: -0.04008340999432856
Epoch 268
-------------------------------
Batch 1/64 loss: 0.010671138763427734
Batch 2/64 loss: -0.02938985824584961
Batch 3/64 loss: -0.41261959075927734
Batch 4/64 loss: -0.1240234375
Batch 5/64 loss: -0.1981039047241211
Batch 6/64 loss: -0.5213556289672852
Batch 7/64 loss: -0.2326345443725586
Batch 8/64 loss: -0.37752294540405273
Batch 9/64 loss: -0.22790145874023438
Batch 10/64 loss: -0.31427001953125
Batch 11/64 loss: 0.32119083404541016
Batch 12/64 loss: -0.2104625701904297
Batch 13/64 loss: -0.0347294807434082
Batch 14/64 loss: -0.3145627975463867
Batch 15/64 loss: -0.28440284729003906
Batch 16/64 loss: -0.3860044479370117
Batch 17/64 loss: -0.022731304168701172
Batch 18/64 loss: 0.08192729949951172
Batch 19/64 loss: -0.29854583740234375
Batch 20/64 loss: -0.06396961212158203
Batch 21/64 loss: 0.34899330139160156
Batch 22/64 loss: -0.199188232421875
Batch 23/64 loss: -0.13784313201904297
Batch 24/64 loss: -0.2377166748046875
Batch 25/64 loss: -0.30692195892333984
Batch 26/64 loss: 0.13090085983276367
Batch 27/64 loss: -0.2699098587036133
Batch 28/64 loss: 0.32731056213378906
Batch 29/64 loss: -0.09122753143310547
Batch 30/64 loss: -0.14916419982910156
Batch 31/64 loss: 0.08453083038330078
Batch 32/64 loss: -0.021112918853759766
Batch 33/64 loss: -0.2156219482421875
Batch 34/64 loss: 0.2294635772705078
Batch 35/64 loss: 0.2755303382873535
Batch 36/64 loss: -0.3405647277832031
Batch 37/64 loss: -0.09755802154541016
Batch 38/64 loss: -0.016509056091308594
Batch 39/64 loss: -0.16959762573242188
Batch 40/64 loss: -0.22190380096435547
Batch 41/64 loss: -0.10821056365966797
Batch 42/64 loss: -0.1448535919189453
Batch 43/64 loss: 0.44948911666870117
Batch 44/64 loss: 0.05388307571411133
Batch 45/64 loss: 0.05692291259765625
Batch 46/64 loss: -0.2621316909790039
Batch 47/64 loss: -0.06192493438720703
Batch 48/64 loss: -0.2128582000732422
Batch 49/64 loss: -0.16314220428466797
Batch 50/64 loss: -0.019189834594726562
Batch 51/64 loss: -0.37526512145996094
Batch 52/64 loss: -0.21155738830566406
Batch 53/64 loss: 0.1142730712890625
Batch 54/64 loss: -0.029523849487304688
Batch 55/64 loss: -0.3598155975341797
Batch 56/64 loss: -0.08115577697753906
Batch 57/64 loss: -0.3444819450378418
Batch 58/64 loss: 0.5545039176940918
Batch 59/64 loss: -0.30673980712890625
Batch 60/64 loss: -0.3050422668457031
Batch 61/64 loss: 0.022596359252929688
Batch 62/64 loss: -0.14030075073242188
Batch 63/64 loss: 0.023056983947753906
Batch 64/64 loss: -4.0291290283203125
Epoch 268  Train loss: -0.1504449582567402  Val loss: -0.10524247684019947
Saving best model, epoch: 268
Epoch 269
-------------------------------
Batch 1/64 loss: -0.12525272369384766
Batch 2/64 loss: -0.15280818939208984
Batch 3/64 loss: 0.14808177947998047
Batch 4/64 loss: 0.027556896209716797
Batch 5/64 loss: -0.058910369873046875
Batch 6/64 loss: -0.2440481185913086
Batch 7/64 loss: -0.2984590530395508
Batch 8/64 loss: -0.26656532287597656
Batch 9/64 loss: 0.1275339126586914
Batch 10/64 loss: 0.11777400970458984
Batch 11/64 loss: -0.43337154388427734
Batch 12/64 loss: -0.06328868865966797
Batch 13/64 loss: -0.19039058685302734
Batch 14/64 loss: -0.28484535217285156
Batch 15/64 loss: 0.05767631530761719
Batch 16/64 loss: 0.006981372833251953
Batch 17/64 loss: 0.26746129989624023
Batch 18/64 loss: -0.37874412536621094
Batch 19/64 loss: 0.36521339416503906
Batch 20/64 loss: -0.34147167205810547
Batch 21/64 loss: -0.28081512451171875
Batch 22/64 loss: -0.15726852416992188
Batch 23/64 loss: -0.06056976318359375
Batch 24/64 loss: -0.12323760986328125
Batch 25/64 loss: -0.31072998046875
Batch 26/64 loss: -0.14215469360351562
Batch 27/64 loss: -0.3650035858154297
Batch 28/64 loss: -0.2937660217285156
Batch 29/64 loss: -0.03770160675048828
Batch 30/64 loss: -0.19664382934570312
Batch 31/64 loss: -0.16471385955810547
Batch 32/64 loss: -0.34574413299560547
Batch 33/64 loss: -0.049434661865234375
Batch 34/64 loss: -0.28526782989501953
Batch 35/64 loss: -0.33292579650878906
Batch 36/64 loss: -0.23185110092163086
Batch 37/64 loss: -0.4848976135253906
Batch 38/64 loss: -0.17994308471679688
Batch 39/64 loss: 0.364654541015625
Batch 40/64 loss: -0.317535400390625
Batch 41/64 loss: -0.08827495574951172
Batch 42/64 loss: -0.4427509307861328
Batch 43/64 loss: -0.13061141967773438
Batch 44/64 loss: 0.47215747833251953
Batch 45/64 loss: 0.20566844940185547
Batch 46/64 loss: 0.22574996948242188
Batch 47/64 loss: -0.2956123352050781
Batch 48/64 loss: -0.05647706985473633
Batch 49/64 loss: 0.07617378234863281
Batch 50/64 loss: -0.06821155548095703
Batch 51/64 loss: -0.01538848876953125
Batch 52/64 loss: 0.47457027435302734
Batch 53/64 loss: -0.09647130966186523
Batch 54/64 loss: -0.2908611297607422
Batch 55/64 loss: -0.2611579895019531
Batch 56/64 loss: 0.061679840087890625
Batch 57/64 loss: 0.04605245590209961
Batch 58/64 loss: 0.18765592575073242
Batch 59/64 loss: -0.4067420959472656
Batch 60/64 loss: -0.16412830352783203
Batch 61/64 loss: -0.004046440124511719
Batch 62/64 loss: 0.3498711585998535
Batch 63/64 loss: 0.006993770599365234
Batch 64/64 loss: -4.456569671630859
Epoch 269  Train loss: -0.1454433665556066  Val loss: -0.06854519401628946
Epoch 270
-------------------------------
Batch 1/64 loss: 0.13289785385131836
Batch 2/64 loss: -0.27184391021728516
Batch 3/64 loss: 0.0022077560424804688
Batch 4/64 loss: -0.2518339157104492
Batch 5/64 loss: -0.29933834075927734
Batch 6/64 loss: -0.14353418350219727
Batch 7/64 loss: -0.20055103302001953
Batch 8/64 loss: -0.1963672637939453
Batch 9/64 loss: -0.5595760345458984
Batch 10/64 loss: -0.29551029205322266
Batch 11/64 loss: -0.3284425735473633
Batch 12/64 loss: -0.07874822616577148
Batch 13/64 loss: -0.1340646743774414
Batch 14/64 loss: -0.1725606918334961
Batch 15/64 loss: -0.1947164535522461
Batch 16/64 loss: 0.134429931640625
Batch 17/64 loss: -0.20560264587402344
Batch 18/64 loss: -0.22768211364746094
Batch 19/64 loss: -0.13548946380615234
Batch 20/64 loss: -0.05906820297241211
Batch 21/64 loss: -0.3254585266113281
Batch 22/64 loss: -0.30266475677490234
Batch 23/64 loss: -0.31574535369873047
Batch 24/64 loss: -0.553863525390625
Batch 25/64 loss: -0.5743303298950195
Batch 26/64 loss: 0.14030694961547852
Batch 27/64 loss: -0.15037775039672852
Batch 28/64 loss: 0.005896091461181641
Batch 29/64 loss: -0.24691295623779297
Batch 30/64 loss: -0.5496377944946289
Batch 31/64 loss: -0.2794456481933594
Batch 32/64 loss: -0.03879261016845703
Batch 33/64 loss: 0.22295188903808594
Batch 34/64 loss: 0.09427213668823242
Batch 35/64 loss: -0.1890416145324707
Batch 36/64 loss: -0.08024406433105469
Batch 37/64 loss: -0.1722240447998047
Batch 38/64 loss: -0.09763097763061523
Batch 39/64 loss: 0.438967227935791
Batch 40/64 loss: -0.08022832870483398
Batch 41/64 loss: -0.14320659637451172
Batch 42/64 loss: 0.17017269134521484
Batch 43/64 loss: -0.004919528961181641
Batch 44/64 loss: -0.020396709442138672
Batch 45/64 loss: 0.011142730712890625
Batch 46/64 loss: -0.38159942626953125
Batch 47/64 loss: -0.40429115295410156
Batch 48/64 loss: -0.031169891357421875
Batch 49/64 loss: -0.10115718841552734
Batch 50/64 loss: 0.5146641731262207
Batch 51/64 loss: 0.0038194656372070312
Batch 52/64 loss: -0.03522157669067383
Batch 53/64 loss: 0.17894887924194336
Batch 54/64 loss: -0.19696521759033203
Batch 55/64 loss: 0.048304080963134766
Batch 56/64 loss: 0.3954963684082031
Batch 57/64 loss: -0.07281684875488281
Batch 58/64 loss: -0.12849044799804688
Batch 59/64 loss: 0.15222644805908203
Batch 60/64 loss: -0.1670522689819336
Batch 61/64 loss: -0.06306791305541992
Batch 62/64 loss: -0.1424570083618164
Batch 63/64 loss: -0.17525005340576172
Batch 64/64 loss: -3.710577964782715
Epoch 270  Train loss: -0.15554225772034888  Val loss: -0.10642854618452668
Saving best model, epoch: 270
Epoch 271
-------------------------------
Batch 1/64 loss: -0.20491361618041992
Batch 2/64 loss: -0.06031656265258789
Batch 3/64 loss: -0.16783714294433594
Batch 4/64 loss: -0.39580535888671875
Batch 5/64 loss: 0.09989023208618164
Batch 6/64 loss: -0.5283269882202148
Batch 7/64 loss: 0.056972503662109375
Batch 8/64 loss: -0.2675209045410156
Batch 9/64 loss: -0.29445743560791016
Batch 10/64 loss: 0.18133258819580078
Batch 11/64 loss: -0.4304819107055664
Batch 12/64 loss: 0.08512067794799805
Batch 13/64 loss: 1.1299805641174316
Batch 14/64 loss: 0.2756223678588867
Batch 15/64 loss: 0.0648641586303711
Batch 16/64 loss: 2.2074103355407715
Batch 17/64 loss: 0.00829315185546875
Batch 18/64 loss: 0.2401566505432129
Batch 19/64 loss: 0.38043928146362305
Batch 20/64 loss: 0.6032748222351074
Batch 21/64 loss: 1.422147274017334
Batch 22/64 loss: 0.8007426261901855
Batch 23/64 loss: 1.1551222801208496
Batch 24/64 loss: 1.202439308166504
Batch 25/64 loss: 1.432279109954834
Batch 26/64 loss: 0.9168438911437988
Batch 27/64 loss: 0.9690613746643066
Batch 28/64 loss: 0.9916667938232422
Batch 29/64 loss: 1.4908232688903809
Batch 30/64 loss: 0.935096263885498
Batch 31/64 loss: 1.0890960693359375
Batch 32/64 loss: 0.855567455291748
Batch 33/64 loss: 1.4108052253723145
Batch 34/64 loss: 1.1319231986999512
Batch 35/64 loss: 0.7750000953674316
Batch 36/64 loss: 1.4214820861816406
Batch 37/64 loss: 1.5960822105407715
Batch 38/64 loss: 0.7770609855651855
Batch 39/64 loss: 0.6308712959289551
Batch 40/64 loss: 0.9199128150939941
Batch 41/64 loss: 0.75970458984375
Batch 42/64 loss: 0.6606054306030273
Batch 43/64 loss: 0.9921879768371582
Batch 44/64 loss: 0.9778738021850586
Batch 45/64 loss: 0.6832733154296875
Batch 46/64 loss: 0.42160701751708984
Batch 47/64 loss: 0.8694453239440918
Batch 48/64 loss: 0.6749987602233887
Batch 49/64 loss: 0.91400146484375
Batch 50/64 loss: 0.49161195755004883
Batch 51/64 loss: 0.7779045104980469
Batch 52/64 loss: 0.9600777626037598
Batch 53/64 loss: 0.2587437629699707
Batch 54/64 loss: 1.074063777923584
Batch 55/64 loss: 0.4065432548522949
Batch 56/64 loss: 0.4585080146789551
Batch 57/64 loss: 0.4223008155822754
Batch 58/64 loss: 0.5309405326843262
Batch 59/64 loss: 0.43583106994628906
Batch 60/64 loss: 0.35602712631225586
Batch 61/64 loss: 0.29053306579589844
Batch 62/64 loss: 0.5642976760864258
Batch 63/64 loss: 0.9764013290405273
Batch 64/64 loss: -2.901630401611328
Epoch 271  Train loss: 0.5911996953627643  Val loss: 0.5005786869534102
Epoch 272
-------------------------------
Batch 1/64 loss: 0.5268568992614746
Batch 2/64 loss: 0.29689884185791016
Batch 3/64 loss: 0.6783714294433594
Batch 4/64 loss: 0.7294368743896484
Batch 5/64 loss: 0.47458553314208984
Batch 6/64 loss: 0.3949089050292969
Batch 7/64 loss: 0.44962501525878906
Batch 8/64 loss: 0.14597415924072266
Batch 9/64 loss: 0.3448915481567383
Batch 10/64 loss: 0.21628189086914062
Batch 11/64 loss: 0.18659639358520508
Batch 12/64 loss: 0.4445047378540039
Batch 13/64 loss: 0.5509462356567383
Batch 14/64 loss: 0.43906736373901367
Batch 15/64 loss: 0.33792686462402344
Batch 16/64 loss: 0.5828566551208496
Batch 17/64 loss: 0.14680910110473633
Batch 18/64 loss: 0.3659210205078125
Batch 19/64 loss: 0.29958105087280273
Batch 20/64 loss: 0.8275089263916016
Batch 21/64 loss: 0.33602237701416016
Batch 22/64 loss: 0.267855167388916
Batch 23/64 loss: 0.40024757385253906
Batch 24/64 loss: 0.694310188293457
Batch 25/64 loss: 0.23033428192138672
Batch 26/64 loss: 0.2915010452270508
Batch 27/64 loss: 0.6178755760192871
Batch 28/64 loss: 0.6300320625305176
Batch 29/64 loss: -0.04216480255126953
Batch 30/64 loss: 0.40992116928100586
Batch 31/64 loss: 0.05930185317993164
Batch 32/64 loss: 0.14348936080932617
Batch 33/64 loss: 0.5287656784057617
Batch 34/64 loss: 0.6715121269226074
Batch 35/64 loss: 0.1456003189086914
Batch 36/64 loss: 0.1796112060546875
Batch 37/64 loss: 0.20078372955322266
Batch 38/64 loss: 0.23645687103271484
Batch 39/64 loss: 0.30335044860839844
Batch 40/64 loss: 0.4225645065307617
Batch 41/64 loss: 0.05283308029174805
Batch 42/64 loss: 0.24728679656982422
Batch 43/64 loss: 0.5270543098449707
Batch 44/64 loss: 0.5505480766296387
Batch 45/64 loss: 0.3557095527648926
Batch 46/64 loss: 0.3139805793762207
Batch 47/64 loss: 0.24077558517456055
Batch 48/64 loss: 0.3857550621032715
Batch 49/64 loss: 0.29691410064697266
Batch 50/64 loss: 0.18024206161499023
Batch 51/64 loss: 0.005764007568359375
Batch 52/64 loss: 0.3512306213378906
Batch 53/64 loss: -0.07670021057128906
Batch 54/64 loss: 0.18375396728515625
Batch 55/64 loss: 0.5243015289306641
Batch 56/64 loss: 0.3380098342895508
Batch 57/64 loss: 0.22870445251464844
Batch 58/64 loss: 0.22829914093017578
Batch 59/64 loss: 0.09915542602539062
Batch 60/64 loss: 0.0008792877197265625
Batch 61/64 loss: 0.5395135879516602
Batch 62/64 loss: 0.1642913818359375
Batch 63/64 loss: 0.1848306655883789
Batch 64/64 loss: -3.323749542236328
Epoch 272  Train loss: 0.29172142627192477  Val loss: 0.23597424956121804
Epoch 273
-------------------------------
Batch 1/64 loss: 0.1117558479309082
Batch 2/64 loss: 0.23512887954711914
Batch 3/64 loss: 0.026262283325195312
Batch 4/64 loss: 0.3723564147949219
Batch 5/64 loss: 0.04288959503173828
Batch 6/64 loss: -0.03742790222167969
Batch 7/64 loss: -0.2188105583190918
Batch 8/64 loss: 0.6283073425292969
Batch 9/64 loss: 0.6396241188049316
Batch 10/64 loss: 0.6106114387512207
Batch 11/64 loss: 0.3313908576965332
Batch 12/64 loss: -0.29237985610961914
Batch 13/64 loss: 0.04887247085571289
Batch 14/64 loss: 0.32448673248291016
Batch 15/64 loss: 0.0645589828491211
Batch 16/64 loss: 0.8884477615356445
Batch 17/64 loss: -0.07263612747192383
Batch 18/64 loss: 0.39372777938842773
Batch 19/64 loss: -0.0911245346069336
Batch 20/64 loss: 0.32306528091430664
Batch 21/64 loss: 0.04885578155517578
Batch 22/64 loss: 0.06636667251586914
Batch 23/64 loss: 0.4268341064453125
Batch 24/64 loss: 0.05521440505981445
Batch 25/64 loss: 0.5176105499267578
Batch 26/64 loss: 0.009411811828613281
Batch 27/64 loss: 0.009296417236328125
Batch 28/64 loss: 0.3420705795288086
Batch 29/64 loss: 0.3599734306335449
Batch 30/64 loss: 0.18484163284301758
Batch 31/64 loss: 0.5619673728942871
Batch 32/64 loss: 0.4435005187988281
Batch 33/64 loss: 0.173675537109375
Batch 34/64 loss: 0.2510690689086914
Batch 35/64 loss: 0.3640155792236328
Batch 36/64 loss: 0.2206258773803711
Batch 37/64 loss: -0.1330585479736328
Batch 38/64 loss: 0.1137380599975586
Batch 39/64 loss: 0.5613713264465332
Batch 40/64 loss: 0.37590980529785156
Batch 41/64 loss: 0.18657350540161133
Batch 42/64 loss: 0.1593647003173828
Batch 43/64 loss: -0.17995929718017578
Batch 44/64 loss: -0.06684398651123047
Batch 45/64 loss: 0.3932046890258789
Batch 46/64 loss: 0.04527711868286133
Batch 47/64 loss: 0.3248891830444336
Batch 48/64 loss: 0.25444746017456055
Batch 49/64 loss: -0.0028710365295410156
Batch 50/64 loss: 0.10733747482299805
Batch 51/64 loss: 0.41562414169311523
Batch 52/64 loss: 0.13884878158569336
Batch 53/64 loss: -0.2176990509033203
Batch 54/64 loss: 0.024096965789794922
Batch 55/64 loss: 0.1525120735168457
Batch 56/64 loss: -0.009511470794677734
Batch 57/64 loss: 0.1250171661376953
Batch 58/64 loss: 0.025708675384521484
Batch 59/64 loss: 0.24161481857299805
Batch 60/64 loss: 0.19571733474731445
Batch 61/64 loss: -0.040496826171875
Batch 62/64 loss: 0.13800048828125
Batch 63/64 loss: 0.2726731300354004
Batch 64/64 loss: -4.052538871765137
Epoch 273  Train loss: 0.1400238224104339  Val loss: 0.12278755423948937
Epoch 274
-------------------------------
Batch 1/64 loss: -0.08003902435302734
Batch 2/64 loss: 0.060968875885009766
Batch 3/64 loss: 0.2472696304321289
Batch 4/64 loss: -0.05996274948120117
Batch 5/64 loss: 0.4280576705932617
Batch 6/64 loss: 0.7058625221252441
Batch 7/64 loss: -0.09355497360229492
Batch 8/64 loss: 0.22935199737548828
Batch 9/64 loss: 0.31322717666625977
Batch 10/64 loss: 0.0875554084777832
Batch 11/64 loss: 0.024781227111816406
Batch 12/64 loss: -0.012495994567871094
Batch 13/64 loss: 0.042514801025390625
Batch 14/64 loss: 0.28752756118774414
Batch 15/64 loss: 0.25942182540893555
Batch 16/64 loss: 0.5402579307556152
Batch 17/64 loss: 0.05941152572631836
Batch 18/64 loss: 0.13262557983398438
Batch 19/64 loss: 0.04706621170043945
Batch 20/64 loss: 0.061893463134765625
Batch 21/64 loss: 0.2699308395385742
Batch 22/64 loss: 0.3458747863769531
Batch 23/64 loss: 0.24589252471923828
Batch 24/64 loss: -0.09510135650634766
Batch 25/64 loss: -0.20106792449951172
Batch 26/64 loss: -0.31466102600097656
Batch 27/64 loss: 0.07896614074707031
Batch 28/64 loss: 0.01519155502319336
Batch 29/64 loss: -0.22585201263427734
Batch 30/64 loss: -0.15316295623779297
Batch 31/64 loss: 0.21944522857666016
Batch 32/64 loss: -0.17774677276611328
Batch 33/64 loss: -0.06501007080078125
Batch 34/64 loss: -0.34839630126953125
Batch 35/64 loss: 0.1907343864440918
Batch 36/64 loss: 0.014993667602539062
Batch 37/64 loss: 0.12288951873779297
Batch 38/64 loss: -0.3691434860229492
Batch 39/64 loss: -0.1497211456298828
Batch 40/64 loss: -0.04240083694458008
Batch 41/64 loss: 0.08294868469238281
Batch 42/64 loss: -0.09327077865600586
Batch 43/64 loss: 0.20334815979003906
Batch 44/64 loss: 0.014528751373291016
Batch 45/64 loss: 0.016542434692382812
Batch 46/64 loss: 0.37078142166137695
Batch 47/64 loss: -0.35007286071777344
Batch 48/64 loss: -0.1378469467163086
Batch 49/64 loss: 0.05598926544189453
Batch 50/64 loss: -0.004707813262939453
Batch 51/64 loss: 0.15566349029541016
Batch 52/64 loss: 0.5062055587768555
Batch 53/64 loss: 0.2379779815673828
Batch 54/64 loss: 0.0687098503112793
Batch 55/64 loss: 0.07098913192749023
Batch 56/64 loss: 0.34674835205078125
Batch 57/64 loss: 0.5813841819763184
Batch 58/64 loss: -0.29321718215942383
Batch 59/64 loss: -0.1456003189086914
Batch 60/64 loss: 0.04117870330810547
Batch 61/64 loss: 0.03903388977050781
Batch 62/64 loss: 0.08526420593261719
Batch 63/64 loss: 0.04315900802612305
Batch 64/64 loss: -3.7017555236816406
Epoch 274  Train loss: 0.0276520149380553  Val loss: 0.03106614732250725
Epoch 275
-------------------------------
Batch 1/64 loss: -0.0694437026977539
Batch 2/64 loss: -0.4372725486755371
Batch 3/64 loss: 0.11647272109985352
Batch 4/64 loss: -0.035925865173339844
Batch 5/64 loss: 0.03932905197143555
Batch 6/64 loss: -0.3111906051635742
Batch 7/64 loss: 0.3745245933532715
Batch 8/64 loss: -0.03590583801269531
Batch 9/64 loss: 0.10213184356689453
Batch 10/64 loss: -0.22854995727539062
Batch 11/64 loss: 0.34260082244873047
Batch 12/64 loss: 0.049773216247558594
Batch 13/64 loss: 0.2599978446960449
Batch 14/64 loss: -0.029683589935302734
Batch 15/64 loss: 0.01785898208618164
Batch 16/64 loss: 0.23964595794677734
Batch 17/64 loss: 0.03827047348022461
Batch 18/64 loss: -0.029662132263183594
Batch 19/64 loss: -0.011128902435302734
Batch 20/64 loss: -0.025690078735351562
Batch 21/64 loss: -0.02644634246826172
Batch 22/64 loss: -0.3187265396118164
Batch 23/64 loss: 0.07941102981567383
Batch 24/64 loss: 0.19615650177001953
Batch 25/64 loss: 0.17249488830566406
Batch 26/64 loss: -0.0016236305236816406
Batch 27/64 loss: -0.1196432113647461
Batch 28/64 loss: -0.17793989181518555
Batch 29/64 loss: 0.009083747863769531
Batch 30/64 loss: -0.26349735260009766
Batch 31/64 loss: 0.1360635757446289
Batch 32/64 loss: -0.05428504943847656
Batch 33/64 loss: -0.1851210594177246
Batch 34/64 loss: -0.23755264282226562
Batch 35/64 loss: -0.07981395721435547
Batch 36/64 loss: -0.08465003967285156
Batch 37/64 loss: 0.23009252548217773
Batch 38/64 loss: 0.25997257232666016
Batch 39/64 loss: 0.09107160568237305
Batch 40/64 loss: -0.21358585357666016
Batch 41/64 loss: 0.03565645217895508
Batch 42/64 loss: 0.2701869010925293
Batch 43/64 loss: -0.10446548461914062
Batch 44/64 loss: -0.22053909301757812
Batch 45/64 loss: 0.10976791381835938
Batch 46/64 loss: 0.07650041580200195
Batch 47/64 loss: -0.026880264282226562
Batch 48/64 loss: -0.056345462799072266
Batch 49/64 loss: -0.17444801330566406
Batch 50/64 loss: 0.32680606842041016
Batch 51/64 loss: -0.14346981048583984
Batch 52/64 loss: 0.1205606460571289
Batch 53/64 loss: -0.2050762176513672
Batch 54/64 loss: 0.02777099609375
Batch 55/64 loss: -0.08476448059082031
Batch 56/64 loss: 0.18630743026733398
Batch 57/64 loss: -0.28899145126342773
Batch 58/64 loss: -0.05691957473754883
Batch 59/64 loss: 0.08831071853637695
Batch 60/64 loss: -0.21542692184448242
Batch 61/64 loss: -0.42310047149658203
Batch 62/64 loss: 0.0654592514038086
Batch 63/64 loss: 0.3458375930786133
Batch 64/64 loss: -3.885258674621582
Epoch 275  Train loss: -0.0546446070951574  Val loss: -0.029997927216729756
Epoch 276
-------------------------------
Batch 1/64 loss: -0.27720069885253906
Batch 2/64 loss: 0.15395784378051758
Batch 3/64 loss: -0.23914146423339844
Batch 4/64 loss: 0.03949117660522461
Batch 5/64 loss: -0.18886470794677734
Batch 6/64 loss: 0.1399698257446289
Batch 7/64 loss: -0.0694732666015625
Batch 8/64 loss: -0.0965127944946289
Batch 9/64 loss: -0.11845541000366211
Batch 10/64 loss: -0.0393366813659668
Batch 11/64 loss: -0.3767280578613281
Batch 12/64 loss: -0.11910104751586914
Batch 13/64 loss: -0.3726692199707031
Batch 14/64 loss: -0.36253929138183594
Batch 15/64 loss: -0.027046680450439453
Batch 16/64 loss: 0.05466651916503906
Batch 17/64 loss: -0.05058574676513672
Batch 18/64 loss: 0.3056950569152832
Batch 19/64 loss: -0.014044761657714844
Batch 20/64 loss: -0.18580245971679688
Batch 21/64 loss: -0.5031147003173828
Batch 22/64 loss: -0.041916847229003906
Batch 23/64 loss: -0.27422237396240234
Batch 24/64 loss: 0.09941959381103516
Batch 25/64 loss: -0.2908143997192383
Batch 26/64 loss: 0.8565487861633301
Batch 27/64 loss: 0.322052001953125
Batch 28/64 loss: -0.06349420547485352
Batch 29/64 loss: -0.23361682891845703
Batch 30/64 loss: -0.34400177001953125
Batch 31/64 loss: -0.048609256744384766
Batch 32/64 loss: -0.20874786376953125
Batch 33/64 loss: -0.1755533218383789
Batch 34/64 loss: -0.03673696517944336
Batch 35/64 loss: -0.21101045608520508
Batch 36/64 loss: -0.17279767990112305
Batch 37/64 loss: 0.010973453521728516
Batch 38/64 loss: -0.21826744079589844
Batch 39/64 loss: 0.15374183654785156
Batch 40/64 loss: -0.3802766799926758
Batch 41/64 loss: 0.04097747802734375
Batch 42/64 loss: -0.22007179260253906
Batch 43/64 loss: 0.24502134323120117
Batch 44/64 loss: 0.0702509880065918
Batch 45/64 loss: 0.5310263633728027
Batch 46/64 loss: 0.21942138671875
Batch 47/64 loss: 0.16061019897460938
Batch 48/64 loss: -0.057099342346191406
Batch 49/64 loss: -0.14444780349731445
Batch 50/64 loss: 0.4407219886779785
Batch 51/64 loss: -0.19938039779663086
Batch 52/64 loss: -0.03056192398071289
Batch 53/64 loss: 0.0283355712890625
Batch 54/64 loss: 0.47639894485473633
Batch 55/64 loss: 0.14914798736572266
Batch 56/64 loss: 0.09478187561035156
Batch 57/64 loss: -0.1555032730102539
Batch 58/64 loss: 0.15577030181884766
Batch 59/64 loss: -0.08489131927490234
Batch 60/64 loss: -0.14528846740722656
Batch 61/64 loss: -0.23585176467895508
Batch 62/64 loss: 0.2918834686279297
Batch 63/64 loss: -0.011676788330078125
Batch 64/64 loss: -3.3899898529052734
Epoch 276  Train loss: -0.07101308785232843  Val loss: 0.030214552207501074
Epoch 277
-------------------------------
Batch 1/64 loss: -0.12702417373657227
Batch 2/64 loss: 0.04174613952636719
Batch 3/64 loss: -0.05311775207519531
Batch 4/64 loss: -0.07567310333251953
Batch 5/64 loss: 0.049796104431152344
Batch 6/64 loss: 1.941995620727539
Batch 7/64 loss: -0.2053070068359375
Batch 8/64 loss: -0.17073631286621094
Batch 9/64 loss: 0.12735700607299805
Batch 10/64 loss: -0.08326148986816406
Batch 11/64 loss: -0.06661558151245117
Batch 12/64 loss: 0.22565078735351562
Batch 13/64 loss: -0.06020641326904297
Batch 14/64 loss: 0.28079652786254883
Batch 15/64 loss: -0.017560482025146484
Batch 16/64 loss: 0.35036516189575195
Batch 17/64 loss: 1.9483003616333008
Batch 18/64 loss: 0.08294677734375
Batch 19/64 loss: -0.18021011352539062
Batch 20/64 loss: 0.531792163848877
Batch 21/64 loss: 0.3621792793273926
Batch 22/64 loss: 0.17806625366210938
Batch 23/64 loss: 0.10715293884277344
Batch 24/64 loss: 0.02385854721069336
Batch 25/64 loss: 0.16147851943969727
Batch 26/64 loss: -0.1790475845336914
Batch 27/64 loss: 0.06089973449707031
Batch 28/64 loss: 0.34915971755981445
Batch 29/64 loss: -0.18269729614257812
Batch 30/64 loss: -0.06046295166015625
Batch 31/64 loss: 0.31217288970947266
Batch 32/64 loss: 0.41663360595703125
Batch 33/64 loss: 0.21524953842163086
Batch 34/64 loss: 0.5734143257141113
Batch 35/64 loss: -0.17677545547485352
Batch 36/64 loss: -0.21977758407592773
Batch 37/64 loss: -0.0642232894897461
Batch 38/64 loss: 0.14611387252807617
Batch 39/64 loss: 0.08092641830444336
Batch 40/64 loss: 0.15998268127441406
Batch 41/64 loss: 0.04628324508666992
Batch 42/64 loss: 0.23603200912475586
Batch 43/64 loss: -0.1394481658935547
Batch 44/64 loss: -0.30611133575439453
Batch 45/64 loss: -0.22900056838989258
Batch 46/64 loss: 0.10759878158569336
Batch 47/64 loss: 0.47240543365478516
Batch 48/64 loss: 0.1942305564880371
Batch 49/64 loss: 0.1965947151184082
Batch 50/64 loss: -0.21327590942382812
Batch 51/64 loss: 0.4178295135498047
Batch 52/64 loss: 0.20680761337280273
Batch 53/64 loss: -0.0519866943359375
Batch 54/64 loss: 0.13518762588500977
Batch 55/64 loss: 0.031164169311523438
Batch 56/64 loss: 0.06196022033691406
Batch 57/64 loss: -0.15543365478515625
Batch 58/64 loss: 0.052231788635253906
Batch 59/64 loss: 0.1814570426940918
Batch 60/64 loss: 0.13475847244262695
Batch 61/64 loss: -0.1560535430908203
Batch 62/64 loss: -0.08330917358398438
Batch 63/64 loss: -0.196075439453125
Batch 64/64 loss: -4.333956718444824
Epoch 277  Train loss: 0.0705681183758904  Val loss: 0.16431542360495865
Epoch 278
-------------------------------
Batch 1/64 loss: 0.0038580894470214844
Batch 2/64 loss: -0.0935521125793457
Batch 3/64 loss: -0.1234889030456543
Batch 4/64 loss: -0.013184070587158203
Batch 5/64 loss: -0.14519596099853516
Batch 6/64 loss: 0.01488637924194336
Batch 7/64 loss: -0.13483619689941406
Batch 8/64 loss: 0.2501564025878906
Batch 9/64 loss: 1.936471939086914
Batch 10/64 loss: 0.27205848693847656
Batch 11/64 loss: -0.1597146987915039
Batch 12/64 loss: -0.08083534240722656
Batch 13/64 loss: -0.17809009552001953
Batch 14/64 loss: -0.02171802520751953
Batch 15/64 loss: -0.2769279479980469
Batch 16/64 loss: -0.026786327362060547
Batch 17/64 loss: 0.21886682510375977
Batch 18/64 loss: 0.05120563507080078
Batch 19/64 loss: -0.049957275390625
Batch 20/64 loss: 0.3139204978942871
Batch 21/64 loss: 0.2720460891723633
Batch 22/64 loss: -0.11641073226928711
Batch 23/64 loss: 0.1409597396850586
Batch 24/64 loss: -0.2513093948364258
Batch 25/64 loss: 0.48117685317993164
Batch 26/64 loss: 0.027296066284179688
Batch 27/64 loss: -0.1661849021911621
Batch 28/64 loss: 2.9373083114624023
Batch 29/64 loss: 0.6370744705200195
Batch 30/64 loss: 0.18910646438598633
Batch 31/64 loss: -0.024469375610351562
Batch 32/64 loss: 0.40877628326416016
Batch 33/64 loss: 0.9686331748962402
Batch 34/64 loss: 0.2939567565917969
Batch 35/64 loss: 0.0911855697631836
Batch 36/64 loss: 0.29073190689086914
Batch 37/64 loss: 0.16744422912597656
Batch 38/64 loss: 0.47212743759155273
Batch 39/64 loss: 0.47789478302001953
Batch 40/64 loss: 1.2664985656738281
Batch 41/64 loss: 0.3301525115966797
Batch 42/64 loss: 0.31478309631347656
Batch 43/64 loss: 2.531996726989746
Batch 44/64 loss: 0.10292339324951172
Batch 45/64 loss: 0.3914937973022461
Batch 46/64 loss: 0.36893367767333984
Batch 47/64 loss: 0.3603219985961914
Batch 48/64 loss: 0.0874638557434082
Batch 49/64 loss: 0.619870662689209
Batch 50/64 loss: 0.5429706573486328
Batch 51/64 loss: 0.1925945281982422
Batch 52/64 loss: 0.7918939590454102
Batch 53/64 loss: 0.4265146255493164
Batch 54/64 loss: -0.10219097137451172
Batch 55/64 loss: 3.622541904449463
Batch 56/64 loss: 0.20720481872558594
Batch 57/64 loss: 0.009766578674316406
Batch 58/64 loss: 0.20218658447265625
Batch 59/64 loss: 1.4012699127197266
Batch 60/64 loss: 0.8573794364929199
Batch 61/64 loss: 0.3898744583129883
Batch 62/64 loss: 0.36936187744140625
Batch 63/64 loss: 0.7464189529418945
Batch 64/64 loss: -3.758023262023926
Epoch 278  Train loss: 0.3493049284991096  Val loss: 0.9132168825549358
Epoch 279
-------------------------------
Batch 1/64 loss: 0.5110044479370117
Batch 2/64 loss: 0.23157024383544922
Batch 3/64 loss: 2.131208896636963
Batch 4/64 loss: 0.6492443084716797
Batch 5/64 loss: 4.178323745727539
Batch 6/64 loss: 0.1771864891052246
Batch 7/64 loss: 0.6837611198425293
Batch 8/64 loss: 0.055728912353515625
Batch 9/64 loss: 0.32410478591918945
Batch 10/64 loss: 0.5557651519775391
Batch 11/64 loss: 0.4906282424926758
Batch 12/64 loss: 0.1391277313232422
Batch 13/64 loss: 0.8024554252624512
Batch 14/64 loss: 0.5076889991760254
Batch 15/64 loss: 0.29045772552490234
Batch 16/64 loss: 0.29402828216552734
Batch 17/64 loss: 0.4525785446166992
Batch 18/64 loss: 0.09185981750488281
Batch 19/64 loss: 0.4881911277770996
Batch 20/64 loss: 0.5005388259887695
Batch 21/64 loss: 0.2952260971069336
Batch 22/64 loss: -0.026094436645507812
Batch 23/64 loss: 0.4921121597290039
Batch 24/64 loss: 0.44859790802001953
Batch 25/64 loss: 0.09306192398071289
Batch 26/64 loss: 0.4207878112792969
Batch 27/64 loss: 0.5516676902770996
Batch 28/64 loss: 0.7096028327941895
Batch 29/64 loss: 0.14980030059814453
Batch 30/64 loss: 1.1694293022155762
Batch 31/64 loss: 0.041810035705566406
Batch 32/64 loss: -0.03446006774902344
Batch 33/64 loss: 0.41900110244750977
Batch 34/64 loss: 0.25879812240600586
Batch 35/64 loss: 0.3209395408630371
Batch 36/64 loss: 0.302640438079834
Batch 37/64 loss: 0.5310015678405762
Batch 38/64 loss: 0.5961599349975586
Batch 39/64 loss: 0.37497377395629883
Batch 40/64 loss: 0.5718722343444824
Batch 41/64 loss: -0.06621932983398438
Batch 42/64 loss: 0.2760758399963379
Batch 43/64 loss: 0.4534420967102051
Batch 44/64 loss: -0.10959625244140625
Batch 45/64 loss: 0.3839583396911621
Batch 46/64 loss: 0.018463611602783203
Batch 47/64 loss: 0.29169797897338867
Batch 48/64 loss: -0.08230876922607422
Batch 49/64 loss: 0.36965465545654297
Batch 50/64 loss: 0.07986688613891602
Batch 51/64 loss: 0.45923900604248047
Batch 52/64 loss: 0.07287979125976562
Batch 53/64 loss: -0.11387395858764648
Batch 54/64 loss: 0.25824832916259766
Batch 55/64 loss: 0.24844074249267578
Batch 56/64 loss: 0.39430856704711914
Batch 57/64 loss: 0.17703580856323242
Batch 58/64 loss: -0.03641462326049805
Batch 59/64 loss: 0.22026920318603516
Batch 60/64 loss: 0.655663013458252
Batch 61/64 loss: 0.024897098541259766
Batch 62/64 loss: -0.024398326873779297
Batch 63/64 loss: 0.2558150291442871
Batch 64/64 loss: -3.3735432624816895
Epoch 279  Train loss: 0.3595195040983312  Val loss: 0.30933176938610796
Epoch 280
-------------------------------
Batch 1/64 loss: 0.228607177734375
Batch 2/64 loss: -0.07256364822387695
Batch 3/64 loss: 0.2813386917114258
Batch 4/64 loss: 0.20096540451049805
Batch 5/64 loss: -0.014444351196289062
Batch 6/64 loss: 0.07702207565307617
Batch 7/64 loss: 0.5750532150268555
Batch 8/64 loss: 0.4741692543029785
Batch 9/64 loss: 0.10451745986938477
Batch 10/64 loss: 0.4083566665649414
Batch 11/64 loss: 2.389529228210449
Batch 12/64 loss: 0.8029112815856934
Batch 13/64 loss: 0.08127546310424805
Batch 14/64 loss: 0.7607560157775879
Batch 15/64 loss: 0.09751510620117188
Batch 16/64 loss: -0.07283353805541992
Batch 17/64 loss: 0.0017056465148925781
Batch 18/64 loss: 0.5036354064941406
Batch 19/64 loss: 0.4665546417236328
Batch 20/64 loss: -0.1662917137145996
Batch 21/64 loss: -0.011490345001220703
Batch 22/64 loss: 0.22112560272216797
Batch 23/64 loss: 0.14943981170654297
Batch 24/64 loss: 0.4813690185546875
Batch 25/64 loss: -0.08853387832641602
Batch 26/64 loss: -0.03549766540527344
Batch 27/64 loss: 0.206878662109375
Batch 28/64 loss: -0.22682523727416992
Batch 29/64 loss: 0.21188020706176758
Batch 30/64 loss: 0.17623138427734375
Batch 31/64 loss: 0.046029090881347656
Batch 32/64 loss: 0.008631706237792969
Batch 33/64 loss: 0.30065488815307617
Batch 34/64 loss: 0.397855281829834
Batch 35/64 loss: 0.2778053283691406
Batch 36/64 loss: -0.05060529708862305
Batch 37/64 loss: 0.1948261260986328
Batch 38/64 loss: 0.07928752899169922
Batch 39/64 loss: -0.01605224609375
Batch 40/64 loss: 0.043548583984375
Batch 41/64 loss: 0.04368448257446289
Batch 42/64 loss: 0.04804182052612305
Batch 43/64 loss: 0.43932247161865234
Batch 44/64 loss: -0.17701339721679688
Batch 45/64 loss: 0.2737894058227539
Batch 46/64 loss: -0.17949867248535156
Batch 47/64 loss: -0.20135164260864258
Batch 48/64 loss: 0.017606258392333984
Batch 49/64 loss: -0.10596179962158203
Batch 50/64 loss: 0.04798603057861328
Batch 51/64 loss: 0.2022104263305664
Batch 52/64 loss: 0.35778379440307617
Batch 53/64 loss: 0.21381855010986328
Batch 54/64 loss: 0.273101806640625
Batch 55/64 loss: -0.2021484375
Batch 56/64 loss: 0.2133488655090332
Batch 57/64 loss: -0.04608583450317383
Batch 58/64 loss: 1.6991076469421387
Batch 59/64 loss: -0.25667381286621094
Batch 60/64 loss: -0.23313665390014648
Batch 61/64 loss: 0.17555570602416992
Batch 62/64 loss: -0.023620128631591797
Batch 63/64 loss: 0.2638859748840332
Batch 64/64 loss: -3.7907190322875977
Epoch 280  Train loss: 0.14894198623358035  Val loss: 0.12938806854982146
Epoch 281
-------------------------------
Batch 1/64 loss: 0.12887239456176758
Batch 2/64 loss: -0.18068170547485352
Batch 3/64 loss: 0.0815877914428711
Batch 4/64 loss: 0.014461994171142578
Batch 5/64 loss: -0.0026369094848632812
Batch 6/64 loss: -0.027356624603271484
Batch 7/64 loss: -0.22425079345703125
Batch 8/64 loss: 0.43539857864379883
Batch 9/64 loss: 0.15897846221923828
Batch 10/64 loss: 0.09909343719482422
Batch 11/64 loss: 0.2540273666381836
Batch 12/64 loss: 0.6541552543640137
Batch 13/64 loss: 0.09753894805908203
Batch 14/64 loss: 0.36957454681396484
Batch 15/64 loss: 0.033884525299072266
Batch 16/64 loss: -0.055698394775390625
Batch 17/64 loss: -0.1672358512878418
Batch 18/64 loss: -0.0992593765258789
Batch 19/64 loss: 0.13321590423583984
Batch 20/64 loss: -0.2883720397949219
Batch 21/64 loss: -0.18909454345703125
Batch 22/64 loss: -0.05812883377075195
Batch 23/64 loss: 0.19988632202148438
Batch 24/64 loss: -0.0024766921997070312
Batch 25/64 loss: 0.28328752517700195
Batch 26/64 loss: -0.21733427047729492
Batch 27/64 loss: -0.2967691421508789
Batch 28/64 loss: -0.1993403434753418
Batch 29/64 loss: -0.026985645294189453
Batch 30/64 loss: 0.08925247192382812
Batch 31/64 loss: 0.3468751907348633
Batch 32/64 loss: 0.23993206024169922
Batch 33/64 loss: 0.11148548126220703
Batch 34/64 loss: 0.1510782241821289
Batch 35/64 loss: -0.10739612579345703
Batch 36/64 loss: -0.19141530990600586
Batch 37/64 loss: 0.0008311271667480469
Batch 38/64 loss: -0.13402891159057617
Batch 39/64 loss: -0.02590179443359375
Batch 40/64 loss: -0.04466581344604492
Batch 41/64 loss: -0.15403985977172852
Batch 42/64 loss: -0.1318669319152832
Batch 43/64 loss: -0.08767557144165039
Batch 44/64 loss: 0.12133169174194336
Batch 45/64 loss: -0.060509681701660156
Batch 46/64 loss: 0.04399299621582031
Batch 47/64 loss: 0.2389678955078125
Batch 48/64 loss: -0.027330875396728516
Batch 49/64 loss: 0.3758277893066406
Batch 50/64 loss: 0.39342308044433594
Batch 51/64 loss: 0.040671348571777344
Batch 52/64 loss: 0.16542816162109375
Batch 53/64 loss: 0.0065898895263671875
Batch 54/64 loss: -0.06351661682128906
Batch 55/64 loss: -0.1679692268371582
Batch 56/64 loss: -0.10179615020751953
Batch 57/64 loss: -0.1647047996520996
Batch 58/64 loss: -0.04777812957763672
Batch 59/64 loss: -0.32723522186279297
Batch 60/64 loss: -0.006773471832275391
Batch 61/64 loss: 0.26345396041870117
Batch 62/64 loss: -0.0500035285949707
Batch 63/64 loss: -0.05426359176635742
Batch 64/64 loss: -3.715641498565674
Epoch 281  Train loss: -0.019421482086181642  Val loss: -0.03364650818080837
Epoch 282
-------------------------------
Batch 1/64 loss: 0.027512073516845703
Batch 2/64 loss: 0.32241106033325195
Batch 3/64 loss: -0.2587575912475586
Batch 4/64 loss: -0.03909778594970703
Batch 5/64 loss: -0.1233224868774414
Batch 6/64 loss: -0.20856571197509766
Batch 7/64 loss: 0.16249847412109375
Batch 8/64 loss: -0.027387619018554688
Batch 9/64 loss: 0.09720516204833984
Batch 10/64 loss: 0.07196283340454102
Batch 11/64 loss: -0.1256265640258789
Batch 12/64 loss: 0.009598731994628906
Batch 13/64 loss: -0.3077578544616699
Batch 14/64 loss: -0.14376163482666016
Batch 15/64 loss: -0.023199081420898438
Batch 16/64 loss: -0.2605247497558594
Batch 17/64 loss: -0.24159669876098633
Batch 18/64 loss: 0.38697338104248047
Batch 19/64 loss: -0.13258647918701172
Batch 20/64 loss: -0.02340841293334961
Batch 21/64 loss: -0.078704833984375
Batch 22/64 loss: 0.5772867202758789
Batch 23/64 loss: 0.0401616096496582
Batch 24/64 loss: -0.2879600524902344
Batch 25/64 loss: -0.01791238784790039
Batch 26/64 loss: -0.14102697372436523
Batch 27/64 loss: -0.25977420806884766
Batch 28/64 loss: 0.21505975723266602
Batch 29/64 loss: 0.5884513854980469
Batch 30/64 loss: 0.31046295166015625
Batch 31/64 loss: -0.16062402725219727
Batch 32/64 loss: 0.011515617370605469
Batch 33/64 loss: 0.22150278091430664
Batch 34/64 loss: -0.00021696090698242188
Batch 35/64 loss: -0.26467227935791016
Batch 36/64 loss: 0.11267948150634766
Batch 37/64 loss: -0.30084705352783203
Batch 38/64 loss: -0.049732208251953125
Batch 39/64 loss: 0.04081249237060547
Batch 40/64 loss: 0.06950998306274414
Batch 41/64 loss: 0.0021829605102539062
Batch 42/64 loss: 0.20634889602661133
Batch 43/64 loss: 0.040076255798339844
Batch 44/64 loss: -0.41013622283935547
Batch 45/64 loss: 0.10196638107299805
Batch 46/64 loss: -0.11859893798828125
Batch 47/64 loss: -0.17109966278076172
Batch 48/64 loss: -0.0999307632446289
Batch 49/64 loss: -0.4171628952026367
Batch 50/64 loss: 0.4013195037841797
Batch 51/64 loss: 0.021432876586914062
Batch 52/64 loss: 0.24712657928466797
Batch 53/64 loss: 0.05749225616455078
Batch 54/64 loss: 0.3011155128479004
Batch 55/64 loss: -0.09747171401977539
Batch 56/64 loss: 0.37369346618652344
Batch 57/64 loss: 0.13250350952148438
Batch 58/64 loss: -0.20567750930786133
Batch 59/64 loss: -0.07764816284179688
Batch 60/64 loss: 0.3191251754760742
Batch 61/64 loss: -0.15763139724731445
Batch 62/64 loss: 0.057042598724365234
Batch 63/64 loss: 0.26543521881103516
Batch 64/64 loss: -3.4469099044799805
Epoch 282  Train loss: -0.031766865300197226  Val loss: 0.04661636090360556
Epoch 283
-------------------------------
Batch 1/64 loss: -0.21482372283935547
Batch 2/64 loss: 0.0035457611083984375
Batch 3/64 loss: 0.24129390716552734
Batch 4/64 loss: 0.32197093963623047
Batch 5/64 loss: 0.27643680572509766
Batch 6/64 loss: 0.03968191146850586
Batch 7/64 loss: 0.15232038497924805
Batch 8/64 loss: -0.3218212127685547
Batch 9/64 loss: -0.04156208038330078
Batch 10/64 loss: 0.2339615821838379
Batch 11/64 loss: -0.25577354431152344
Batch 12/64 loss: -0.25711679458618164
Batch 13/64 loss: 0.32242536544799805
Batch 14/64 loss: -0.28069400787353516
Batch 15/64 loss: -0.000247955322265625
Batch 16/64 loss: -0.028154850006103516
Batch 17/64 loss: 0.26131296157836914
Batch 18/64 loss: 0.09118843078613281
Batch 19/64 loss: 0.08387994766235352
Batch 20/64 loss: -0.16678142547607422
Batch 21/64 loss: -0.2313375473022461
Batch 22/64 loss: -0.2370443344116211
Batch 23/64 loss: 0.29090213775634766
Batch 24/64 loss: -0.3472118377685547
Batch 25/64 loss: -0.2967052459716797
Batch 26/64 loss: -0.31927013397216797
Batch 27/64 loss: -0.30460643768310547
Batch 28/64 loss: -0.19554376602172852
Batch 29/64 loss: -0.08693790435791016
Batch 30/64 loss: 0.16875314712524414
Batch 31/64 loss: -0.18857336044311523
Batch 32/64 loss: -0.051752567291259766
Batch 33/64 loss: 0.2110590934753418
Batch 34/64 loss: -0.08624553680419922
Batch 35/64 loss: -0.03372001647949219
Batch 36/64 loss: -0.4303722381591797
Batch 37/64 loss: 0.01806640625
Batch 38/64 loss: -0.12728595733642578
Batch 39/64 loss: 0.03681373596191406
Batch 40/64 loss: 0.011631011962890625
Batch 41/64 loss: -0.06407928466796875
Batch 42/64 loss: -0.004014015197753906
Batch 43/64 loss: -0.2545738220214844
Batch 44/64 loss: -0.09812259674072266
Batch 45/64 loss: 0.5369787216186523
Batch 46/64 loss: 0.2832803726196289
Batch 47/64 loss: 0.2550821304321289
Batch 48/64 loss: -0.3329916000366211
Batch 49/64 loss: -0.10387706756591797
Batch 50/64 loss: 0.2548966407775879
Batch 51/64 loss: 0.25514984130859375
Batch 52/64 loss: -0.09317302703857422
Batch 53/64 loss: 0.06266975402832031
Batch 54/64 loss: -0.2108464241027832
Batch 55/64 loss: 0.003960609436035156
Batch 56/64 loss: 0.11461210250854492
Batch 57/64 loss: 0.35846710205078125
Batch 58/64 loss: -0.0739450454711914
Batch 59/64 loss: -0.09699726104736328
Batch 60/64 loss: -0.22170543670654297
Batch 61/64 loss: -0.2765769958496094
Batch 62/64 loss: 0.20656967163085938
Batch 63/64 loss: -0.2706308364868164
Batch 64/64 loss: -4.2011823654174805
Epoch 283  Train loss: -0.07308379902559169  Val loss: 0.14161426504862676
Epoch 284
-------------------------------
Batch 1/64 loss: -0.33336448669433594
Batch 2/64 loss: -0.12746858596801758
Batch 3/64 loss: -0.011912345886230469
Batch 4/64 loss: 0.05267047882080078
Batch 5/64 loss: -0.16532611846923828
Batch 6/64 loss: 0.013803958892822266
Batch 7/64 loss: -0.057439327239990234
Batch 8/64 loss: 0.06021451950073242
Batch 9/64 loss: -0.19350433349609375
Batch 10/64 loss: 0.03255176544189453
Batch 11/64 loss: -0.07158613204956055
Batch 12/64 loss: -0.18663358688354492
Batch 13/64 loss: -0.17056655883789062
Batch 14/64 loss: -0.20315837860107422
Batch 15/64 loss: -0.004967689514160156
Batch 16/64 loss: 0.023030757904052734
Batch 17/64 loss: -0.01534271240234375
Batch 18/64 loss: -0.21000194549560547
Batch 19/64 loss: 0.04984903335571289
Batch 20/64 loss: 0.40163660049438477
Batch 21/64 loss: 0.027766704559326172
Batch 22/64 loss: -0.3458719253540039
Batch 23/64 loss: 0.11531448364257812
Batch 24/64 loss: -0.23581981658935547
Batch 25/64 loss: 0.08591127395629883
Batch 26/64 loss: -0.28302860260009766
Batch 27/64 loss: -0.08303308486938477
Batch 28/64 loss: 0.11118364334106445
Batch 29/64 loss: -0.18772411346435547
Batch 30/64 loss: 0.33862781524658203
Batch 31/64 loss: -0.29673004150390625
Batch 32/64 loss: 0.1020212173461914
Batch 33/64 loss: 0.2254652976989746
Batch 34/64 loss: 0.1402597427368164
Batch 35/64 loss: 0.2964358329772949
Batch 36/64 loss: -0.18401813507080078
Batch 37/64 loss: -0.06442594528198242
Batch 38/64 loss: 0.2503018379211426
Batch 39/64 loss: -0.08817052841186523
Batch 40/64 loss: 0.23350954055786133
Batch 41/64 loss: -0.13800430297851562
Batch 42/64 loss: -0.026352882385253906
Batch 43/64 loss: -0.2126483917236328
Batch 44/64 loss: -0.3111438751220703
Batch 45/64 loss: 0.07055377960205078
Batch 46/64 loss: -0.0777435302734375
Batch 47/64 loss: -0.1586160659790039
Batch 48/64 loss: -0.4571685791015625
Batch 49/64 loss: -0.34875011444091797
Batch 50/64 loss: 0.13297271728515625
Batch 51/64 loss: -0.3413667678833008
Batch 52/64 loss: -0.113128662109375
Batch 53/64 loss: -0.14992427825927734
Batch 54/64 loss: 0.01403045654296875
Batch 55/64 loss: -0.03133392333984375
Batch 56/64 loss: -0.28903961181640625
Batch 57/64 loss: -0.2868204116821289
Batch 58/64 loss: -0.3791656494140625
Batch 59/64 loss: -0.20151519775390625
Batch 60/64 loss: -0.1819171905517578
Batch 61/64 loss: 0.14019107818603516
Batch 62/64 loss: 0.022663593292236328
Batch 63/64 loss: 0.08574533462524414
Batch 64/64 loss: -3.538581371307373
Epoch 284  Train loss: -0.10748170029883292  Val loss: 0.04043285461635524
Epoch 285
-------------------------------
Batch 1/64 loss: -0.3656005859375
Batch 2/64 loss: -0.1030278205871582
Batch 3/64 loss: 0.038381099700927734
Batch 4/64 loss: -0.18502235412597656
Batch 5/64 loss: 0.07211589813232422
Batch 6/64 loss: -0.07148456573486328
Batch 7/64 loss: 0.11734247207641602
Batch 8/64 loss: -0.08663082122802734
Batch 9/64 loss: -0.23541927337646484
Batch 10/64 loss: -0.31763315200805664
Batch 11/64 loss: -0.32523441314697266
Batch 12/64 loss: -0.176025390625
Batch 13/64 loss: -0.30132389068603516
Batch 14/64 loss: -0.1967020034790039
Batch 15/64 loss: -0.01861095428466797
Batch 16/64 loss: -0.09457588195800781
Batch 17/64 loss: -0.049015045166015625
Batch 18/64 loss: -0.19959115982055664
Batch 19/64 loss: -0.5522851943969727
Batch 20/64 loss: 0.4087686538696289
Batch 21/64 loss: -0.2700643539428711
Batch 22/64 loss: 0.05116128921508789
Batch 23/64 loss: 0.014713764190673828
Batch 24/64 loss: 0.055796146392822266
Batch 25/64 loss: -0.38754749298095703
Batch 26/64 loss: 0.27787208557128906
Batch 27/64 loss: 0.24247980117797852
Batch 28/64 loss: -0.3705158233642578
Batch 29/64 loss: -0.32480335235595703
Batch 30/64 loss: -0.39127159118652344
Batch 31/64 loss: -0.4880809783935547
Batch 32/64 loss: 0.9108715057373047
Batch 33/64 loss: -0.3526344299316406
Batch 34/64 loss: 0.15712356567382812
Batch 35/64 loss: -0.33144474029541016
Batch 36/64 loss: 0.21826457977294922
Batch 37/64 loss: -0.0014176368713378906
Batch 38/64 loss: 0.08711528778076172
Batch 39/64 loss: -0.12261390686035156
Batch 40/64 loss: -0.29156017303466797
Batch 41/64 loss: -0.40340423583984375
Batch 42/64 loss: -0.3925600051879883
Batch 43/64 loss: -0.20354652404785156
Batch 44/64 loss: -0.024232864379882812
Batch 45/64 loss: 0.14481258392333984
Batch 46/64 loss: -0.42934608459472656
Batch 47/64 loss: 0.01959848403930664
Batch 48/64 loss: -0.25826358795166016
Batch 49/64 loss: 0.07434320449829102
Batch 50/64 loss: -0.15258455276489258
Batch 51/64 loss: -0.5495443344116211
Batch 52/64 loss: -0.34603214263916016
Batch 53/64 loss: -0.015192508697509766
Batch 54/64 loss: 0.3596487045288086
Batch 55/64 loss: 0.13936376571655273
Batch 56/64 loss: 0.25732851028442383
Batch 57/64 loss: -0.0524749755859375
Batch 58/64 loss: 0.2481975555419922
Batch 59/64 loss: 0.1170649528503418
Batch 60/64 loss: -0.09301233291625977
Batch 61/64 loss: 0.08060884475708008
Batch 62/64 loss: 0.1313786506652832
Batch 63/64 loss: -0.06327342987060547
Batch 64/64 loss: -3.8923535346984863
Epoch 285  Train loss: -0.1300159734838149  Val loss: 0.08649525855415056
Epoch 286
-------------------------------
Batch 1/64 loss: -0.024602890014648438
Batch 2/64 loss: 0.01149892807006836
Batch 3/64 loss: -0.3561878204345703
Batch 4/64 loss: -0.41918468475341797
Batch 5/64 loss: -0.1338047981262207
Batch 6/64 loss: -0.162017822265625
Batch 7/64 loss: -0.1512141227722168
Batch 8/64 loss: -0.12244319915771484
Batch 9/64 loss: 0.2692070007324219
Batch 10/64 loss: -0.1505427360534668
Batch 11/64 loss: -0.09169483184814453
Batch 12/64 loss: -0.41907596588134766
Batch 13/64 loss: -0.19060993194580078
Batch 14/64 loss: 0.06476593017578125
Batch 15/64 loss: -0.14691162109375
Batch 16/64 loss: -0.046174049377441406
Batch 17/64 loss: -0.06707191467285156
Batch 18/64 loss: -0.2569465637207031
Batch 19/64 loss: -0.10959053039550781
Batch 20/64 loss: 0.10490560531616211
Batch 21/64 loss: -0.580409049987793
Batch 22/64 loss: -0.23218631744384766
Batch 23/64 loss: 0.07262659072875977
Batch 24/64 loss: 0.02167034149169922
Batch 25/64 loss: -0.4319725036621094
Batch 26/64 loss: 0.5142660140991211
Batch 27/64 loss: -0.18840408325195312
Batch 28/64 loss: 0.39842748641967773
Batch 29/64 loss: -0.43227291107177734
Batch 30/64 loss: 0.2953066825866699
Batch 31/64 loss: -0.10439872741699219
Batch 32/64 loss: 0.0163726806640625
Batch 33/64 loss: -0.1967153549194336
Batch 34/64 loss: -0.024021625518798828
Batch 35/64 loss: -0.08954906463623047
Batch 36/64 loss: -0.06313514709472656
Batch 37/64 loss: -0.4537467956542969
Batch 38/64 loss: 0.007482051849365234
Batch 39/64 loss: -0.0051937103271484375
Batch 40/64 loss: -0.1301250457763672
Batch 41/64 loss: -0.20960330963134766
Batch 42/64 loss: -0.05498981475830078
Batch 43/64 loss: -0.0842428207397461
Batch 44/64 loss: 0.1360483169555664
Batch 45/64 loss: -0.36600303649902344
Batch 46/64 loss: -0.07151317596435547
Batch 47/64 loss: -0.3270597457885742
Batch 48/64 loss: 0.10130167007446289
Batch 49/64 loss: -0.32806873321533203
Batch 50/64 loss: 0.031114578247070312
Batch 51/64 loss: 0.03079986572265625
Batch 52/64 loss: -0.06543159484863281
Batch 53/64 loss: -0.20218944549560547
Batch 54/64 loss: -0.2320699691772461
Batch 55/64 loss: -0.5891609191894531
Batch 56/64 loss: 0.048685550689697266
Batch 57/64 loss: -0.38232898712158203
Batch 58/64 loss: -0.1972179412841797
Batch 59/64 loss: -0.11447334289550781
Batch 60/64 loss: 0.11475658416748047
Batch 61/64 loss: 0.1269683837890625
Batch 62/64 loss: -0.2980489730834961
Batch 63/64 loss: 0.0959172248840332
Batch 64/64 loss: -3.9178266525268555
Epoch 286  Train loss: -0.15339379030115463  Val loss: 0.07541436264195393
Epoch 287
-------------------------------
Batch 1/64 loss: 0.15633869171142578
Batch 2/64 loss: -0.1687030792236328
Batch 3/64 loss: -0.2971506118774414
Batch 4/64 loss: -0.048987388610839844
Batch 5/64 loss: -0.3390970230102539
Batch 6/64 loss: -0.16423606872558594
Batch 7/64 loss: 0.047617435455322266
Batch 8/64 loss: -0.4402799606323242
Batch 9/64 loss: -0.6653375625610352
Batch 10/64 loss: -0.3487663269042969
Batch 11/64 loss: 0.0049877166748046875
Batch 12/64 loss: -0.02359294891357422
Batch 13/64 loss: 0.33760929107666016
Batch 14/64 loss: -0.3985776901245117
Batch 15/64 loss: 0.09194087982177734
Batch 16/64 loss: 0.15237808227539062
Batch 17/64 loss: -0.18134403228759766
Batch 18/64 loss: -0.18557453155517578
Batch 19/64 loss: -0.16038990020751953
Batch 20/64 loss: -0.14584589004516602
Batch 21/64 loss: -0.09740066528320312
Batch 22/64 loss: -0.3035163879394531
Batch 23/64 loss: -0.30106449127197266
Batch 24/64 loss: 0.34560489654541016
Batch 25/64 loss: 0.10872173309326172
Batch 26/64 loss: 0.0005235671997070312
Batch 27/64 loss: -0.2828855514526367
Batch 28/64 loss: 0.11394357681274414
Batch 29/64 loss: -0.4278144836425781
Batch 30/64 loss: 0.1135244369506836
Batch 31/64 loss: -0.5003623962402344
Batch 32/64 loss: -0.09444618225097656
Batch 33/64 loss: -0.004372596740722656
Batch 34/64 loss: -0.3050994873046875
Batch 35/64 loss: 0.3465862274169922
Batch 36/64 loss: -0.2926816940307617
Batch 37/64 loss: -0.30021095275878906
Batch 38/64 loss: 0.029578208923339844
Batch 39/64 loss: -0.28934383392333984
Batch 40/64 loss: -0.07228946685791016
Batch 41/64 loss: -0.14762449264526367
Batch 42/64 loss: 0.15254688262939453
Batch 43/64 loss: -0.25175952911376953
Batch 44/64 loss: 0.37198495864868164
Batch 45/64 loss: -0.29895496368408203
Batch 46/64 loss: 0.12161540985107422
Batch 47/64 loss: 0.20513439178466797
Batch 48/64 loss: -0.2645869255065918
Batch 49/64 loss: 0.014875411987304688
Batch 50/64 loss: -0.1299877166748047
Batch 51/64 loss: -0.22332382202148438
Batch 52/64 loss: -0.017199993133544922
Batch 53/64 loss: -0.23119068145751953
Batch 54/64 loss: -0.1273970603942871
Batch 55/64 loss: -0.13883686065673828
Batch 56/64 loss: -0.4239082336425781
Batch 57/64 loss: -0.17152690887451172
Batch 58/64 loss: -0.3385047912597656
Batch 59/64 loss: -0.09485864639282227
Batch 60/64 loss: -0.2713804244995117
Batch 61/64 loss: -0.11566925048828125
Batch 62/64 loss: -0.1495685577392578
Batch 63/64 loss: 0.0075435638427734375
Batch 64/64 loss: -3.904714584350586
Epoch 287  Train loss: -0.16378244138231465  Val loss: 0.03566686885873067
Epoch 288
-------------------------------
Batch 1/64 loss: 0.09550142288208008
Batch 2/64 loss: 0.029434680938720703
Batch 3/64 loss: -0.1974468231201172
Batch 4/64 loss: -0.14966297149658203
Batch 5/64 loss: -0.22716140747070312
Batch 6/64 loss: 0.004771232604980469
Batch 7/64 loss: 0.11944866180419922
Batch 8/64 loss: -0.03847026824951172
Batch 9/64 loss: -0.5111246109008789
Batch 10/64 loss: -0.0586400032043457
Batch 11/64 loss: 0.06470394134521484
Batch 12/64 loss: 0.05377817153930664
Batch 13/64 loss: -0.422393798828125
Batch 14/64 loss: 0.28788280487060547
Batch 15/64 loss: -0.048302650451660156
Batch 16/64 loss: 0.21347761154174805
Batch 17/64 loss: 0.34432411193847656
Batch 18/64 loss: -0.20785903930664062
Batch 19/64 loss: -0.33773136138916016
Batch 20/64 loss: -0.3596467971801758
Batch 21/64 loss: -0.07859039306640625
Batch 22/64 loss: 0.029981613159179688
Batch 23/64 loss: -0.15267467498779297
Batch 24/64 loss: -0.41323184967041016
Batch 25/64 loss: -0.5342979431152344
Batch 26/64 loss: -0.20221233367919922
Batch 27/64 loss: -0.18228912353515625
Batch 28/64 loss: -0.4039287567138672
Batch 29/64 loss: -0.29517364501953125
Batch 30/64 loss: -0.47375965118408203
Batch 31/64 loss: -0.05205821990966797
Batch 32/64 loss: -0.29698944091796875
Batch 33/64 loss: -0.03338813781738281
Batch 34/64 loss: -0.5140657424926758
Batch 35/64 loss: -0.2119607925415039
Batch 36/64 loss: -0.23462200164794922
Batch 37/64 loss: -0.13257312774658203
Batch 38/64 loss: 0.09800243377685547
Batch 39/64 loss: 0.07077312469482422
Batch 40/64 loss: 0.15833663940429688
Batch 41/64 loss: 0.47591686248779297
Batch 42/64 loss: -0.16054916381835938
Batch 43/64 loss: 0.3805422782897949
Batch 44/64 loss: 0.17155170440673828
Batch 45/64 loss: 0.10436534881591797
Batch 46/64 loss: 0.02508401870727539
Batch 47/64 loss: 0.33855104446411133
Batch 48/64 loss: 0.36612987518310547
Batch 49/64 loss: -0.07141256332397461
Batch 50/64 loss: 0.22725296020507812
Batch 51/64 loss: -0.12109375
Batch 52/64 loss: 0.08255815505981445
Batch 53/64 loss: -0.14534759521484375
Batch 54/64 loss: 0.03004932403564453
Batch 55/64 loss: 0.5820422172546387
Batch 56/64 loss: -0.09495115280151367
Batch 57/64 loss: 0.03569221496582031
Batch 58/64 loss: -0.33481264114379883
Batch 59/64 loss: -0.03632545471191406
Batch 60/64 loss: 0.49556827545166016
Batch 61/64 loss: -0.25742149353027344
Batch 62/64 loss: 0.18518543243408203
Batch 63/64 loss: 0.1799454689025879
Batch 64/64 loss: -3.774003505706787
Epoch 288  Train loss: -0.08740110397338867  Val loss: -0.02209915737925526
Epoch 289
-------------------------------
Batch 1/64 loss: -0.1777935028076172
Batch 2/64 loss: -0.18949079513549805
Batch 3/64 loss: -0.12174749374389648
Batch 4/64 loss: 0.09706449508666992
Batch 5/64 loss: 0.039069175720214844
Batch 6/64 loss: -0.06247234344482422
Batch 7/64 loss: -0.25807952880859375
Batch 8/64 loss: 0.010241031646728516
Batch 9/64 loss: 0.597480297088623
Batch 10/64 loss: 0.06431007385253906
Batch 11/64 loss: -0.35998010635375977
Batch 12/64 loss: 0.1254897117614746
Batch 13/64 loss: -0.11852121353149414
Batch 14/64 loss: -0.33724260330200195
Batch 15/64 loss: -0.34766387939453125
Batch 16/64 loss: -0.25374794006347656
Batch 17/64 loss: -0.13247299194335938
Batch 18/64 loss: -0.2854194641113281
Batch 19/64 loss: -0.007631778717041016
Batch 20/64 loss: -0.32312870025634766
Batch 21/64 loss: -0.21189641952514648
Batch 22/64 loss: -0.21209478378295898
Batch 23/64 loss: -0.3864154815673828
Batch 24/64 loss: -0.18708562850952148
Batch 25/64 loss: 0.1183772087097168
Batch 26/64 loss: 0.022184371948242188
Batch 27/64 loss: 0.02627706527709961
Batch 28/64 loss: -0.0007042884826660156
Batch 29/64 loss: -0.1242074966430664
Batch 30/64 loss: -0.10003900527954102
Batch 31/64 loss: -0.03250455856323242
Batch 32/64 loss: -0.2720632553100586
Batch 33/64 loss: -0.1801319122314453
Batch 34/64 loss: -0.05655384063720703
Batch 35/64 loss: 0.03724861145019531
Batch 36/64 loss: -0.41283607482910156
Batch 37/64 loss: -0.15014076232910156
Batch 38/64 loss: 0.19453191757202148
Batch 39/64 loss: -0.0388946533203125
Batch 40/64 loss: -0.03679609298706055
Batch 41/64 loss: -0.21735382080078125
Batch 42/64 loss: -0.06707572937011719
Batch 43/64 loss: 0.29248571395874023
Batch 44/64 loss: -0.22238492965698242
Batch 45/64 loss: -0.1481642723083496
Batch 46/64 loss: 0.1166224479675293
Batch 47/64 loss: -0.04756927490234375
Batch 48/64 loss: -0.2764406204223633
Batch 49/64 loss: 0.10107088088989258
Batch 50/64 loss: 0.14284610748291016
Batch 51/64 loss: -0.5045881271362305
Batch 52/64 loss: -0.26684093475341797
Batch 53/64 loss: -0.14755964279174805
Batch 54/64 loss: -0.36782360076904297
Batch 55/64 loss: 0.15208959579467773
Batch 56/64 loss: -0.1607818603515625
Batch 57/64 loss: 0.17972040176391602
Batch 58/64 loss: -0.03740501403808594
Batch 59/64 loss: 0.008709907531738281
Batch 60/64 loss: 0.1689314842224121
Batch 61/64 loss: -0.1847667694091797
Batch 62/64 loss: 0.22759580612182617
Batch 63/64 loss: 0.06968307495117188
Batch 64/64 loss: -4.539679050445557
Epoch 289  Train loss: -0.1355175074409036  Val loss: -0.09748260917532485
Epoch 290
-------------------------------
Batch 1/64 loss: 0.20322418212890625
Batch 2/64 loss: -0.09065914154052734
Batch 3/64 loss: -0.22153377532958984
Batch 4/64 loss: 0.016925811767578125
Batch 5/64 loss: -0.3881711959838867
Batch 6/64 loss: 0.005402565002441406
Batch 7/64 loss: -0.1332101821899414
Batch 8/64 loss: -0.24365615844726562
Batch 9/64 loss: -0.40085601806640625
Batch 10/64 loss: -0.37830352783203125
Batch 11/64 loss: -0.09781312942504883
Batch 12/64 loss: -0.24003124237060547
Batch 13/64 loss: 0.1755537986755371
Batch 14/64 loss: 0.20421600341796875
Batch 15/64 loss: -0.3104410171508789
Batch 16/64 loss: -0.1359410285949707
Batch 17/64 loss: 0.048143863677978516
Batch 18/64 loss: -0.22652816772460938
Batch 19/64 loss: 0.023153305053710938
Batch 20/64 loss: 0.08190155029296875
Batch 21/64 loss: -0.19077396392822266
Batch 22/64 loss: -0.3540658950805664
Batch 23/64 loss: 0.15958833694458008
Batch 24/64 loss: -0.14713287353515625
Batch 25/64 loss: -0.38839244842529297
Batch 26/64 loss: -0.1127614974975586
Batch 27/64 loss: 0.2100996971130371
Batch 28/64 loss: -0.14882707595825195
Batch 29/64 loss: -0.5281438827514648
Batch 30/64 loss: 0.3681449890136719
Batch 31/64 loss: 0.08416557312011719
Batch 32/64 loss: -0.41550540924072266
Batch 33/64 loss: -0.23690128326416016
Batch 34/64 loss: -0.04326343536376953
Batch 35/64 loss: -0.24608612060546875
Batch 36/64 loss: 0.18912506103515625
Batch 37/64 loss: 0.030659198760986328
Batch 38/64 loss: -0.35854244232177734
Batch 39/64 loss: -0.17375946044921875
Batch 40/64 loss: -0.40452098846435547
Batch 41/64 loss: -0.10636568069458008
Batch 42/64 loss: 0.055891990661621094
Batch 43/64 loss: 0.18640899658203125
Batch 44/64 loss: -0.16671085357666016
Batch 45/64 loss: -0.20783519744873047
Batch 46/64 loss: -0.2890748977661133
Batch 47/64 loss: -0.19205951690673828
Batch 48/64 loss: -0.14391613006591797
Batch 49/64 loss: 0.02642965316772461
Batch 50/64 loss: -0.21550846099853516
Batch 51/64 loss: -0.10752391815185547
Batch 52/64 loss: -0.2671031951904297
Batch 53/64 loss: -0.2486715316772461
Batch 54/64 loss: -0.23593711853027344
Batch 55/64 loss: -0.2655448913574219
Batch 56/64 loss: 0.2003178596496582
Batch 57/64 loss: -0.5206146240234375
Batch 58/64 loss: -0.3387317657470703
Batch 59/64 loss: -0.34868335723876953
Batch 60/64 loss: 0.04900932312011719
Batch 61/64 loss: -0.4324073791503906
Batch 62/64 loss: -0.2780599594116211
Batch 63/64 loss: -0.5209541320800781
Batch 64/64 loss: -4.169119834899902
Epoch 290  Train loss: -0.19309807197720397  Val loss: -0.1629420539357818
Saving best model, epoch: 290
Epoch 291
-------------------------------
Batch 1/64 loss: -0.26131629943847656
Batch 2/64 loss: -0.4251670837402344
Batch 3/64 loss: -0.21151494979858398
Batch 4/64 loss: -0.2664985656738281
Batch 5/64 loss: 0.13330078125
Batch 6/64 loss: -0.35924625396728516
Batch 7/64 loss: -0.29985904693603516
Batch 8/64 loss: -0.029601097106933594
Batch 9/64 loss: -0.40538787841796875
Batch 10/64 loss: -0.2421426773071289
Batch 11/64 loss: 0.034824371337890625
Batch 12/64 loss: -0.25876760482788086
Batch 13/64 loss: -0.32459259033203125
Batch 14/64 loss: 0.41494274139404297
Batch 15/64 loss: -0.2991476058959961
Batch 16/64 loss: -0.3684511184692383
Batch 17/64 loss: -0.36754322052001953
Batch 18/64 loss: -0.08003807067871094
Batch 19/64 loss: 0.02126312255859375
Batch 20/64 loss: -0.29927825927734375
Batch 21/64 loss: 0.14107704162597656
Batch 22/64 loss: -0.16756343841552734
Batch 23/64 loss: -0.17157459259033203
Batch 24/64 loss: 0.02013111114501953
Batch 25/64 loss: -0.4154481887817383
Batch 26/64 loss: -0.26645565032958984
Batch 27/64 loss: 0.033478736877441406
Batch 28/64 loss: -0.37696075439453125
Batch 29/64 loss: -0.14202594757080078
Batch 30/64 loss: -0.5232276916503906
Batch 31/64 loss: -0.17203330993652344
Batch 32/64 loss: -0.061351776123046875
Batch 33/64 loss: -0.3365316390991211
Batch 34/64 loss: -0.3242979049682617
Batch 35/64 loss: 0.014810562133789062
Batch 36/64 loss: -0.26713085174560547
Batch 37/64 loss: -0.2180032730102539
Batch 38/64 loss: -0.030099868774414062
Batch 39/64 loss: -0.36559486389160156
Batch 40/64 loss: -0.1065359115600586
Batch 41/64 loss: -0.3217906951904297
Batch 42/64 loss: -0.42241477966308594
Batch 43/64 loss: -0.19260597229003906
Batch 44/64 loss: -0.30198001861572266
Batch 45/64 loss: -0.03906393051147461
Batch 46/64 loss: -0.2878751754760742
Batch 47/64 loss: -0.2051239013671875
Batch 48/64 loss: 0.4215421676635742
Batch 49/64 loss: -0.39505672454833984
Batch 50/64 loss: -0.04593181610107422
Batch 51/64 loss: 0.22065019607543945
Batch 52/64 loss: -0.43103885650634766
Batch 53/64 loss: -0.21982860565185547
Batch 54/64 loss: -0.18600940704345703
Batch 55/64 loss: -0.10268497467041016
Batch 56/64 loss: 0.11884212493896484
Batch 57/64 loss: -0.25081825256347656
Batch 58/64 loss: 0.013193130493164062
Batch 59/64 loss: -0.0730595588684082
Batch 60/64 loss: -0.27905750274658203
Batch 61/64 loss: 0.013302803039550781
Batch 62/64 loss: -0.30750083923339844
Batch 63/64 loss: -0.19430828094482422
Batch 64/64 loss: -3.976360321044922
Epoch 291  Train loss: -0.22086978239171645  Val loss: -0.12116849709212575
Epoch 292
-------------------------------
Batch 1/64 loss: -0.3518362045288086
Batch 2/64 loss: -0.21591758728027344
Batch 3/64 loss: -0.3303699493408203
Batch 4/64 loss: -0.2576313018798828
Batch 5/64 loss: -0.5838394165039062
Batch 6/64 loss: 0.05194282531738281
Batch 7/64 loss: -0.0762491226196289
Batch 8/64 loss: -0.4013862609863281
Batch 9/64 loss: -0.04386711120605469
Batch 10/64 loss: 0.06185150146484375
Batch 11/64 loss: 0.0172119140625
Batch 12/64 loss: -0.10694026947021484
Batch 13/64 loss: -0.1554107666015625
Batch 14/64 loss: -0.35834598541259766
Batch 15/64 loss: 0.05328798294067383
Batch 16/64 loss: -0.6390542984008789
Batch 17/64 loss: -0.1381206512451172
Batch 18/64 loss: -0.48474693298339844
Batch 19/64 loss: -0.06793785095214844
Batch 20/64 loss: -0.44015979766845703
Batch 21/64 loss: -0.010001182556152344
Batch 22/64 loss: -0.1867694854736328
Batch 23/64 loss: -0.021374225616455078
Batch 24/64 loss: -0.2749290466308594
Batch 25/64 loss: -0.4549846649169922
Batch 26/64 loss: 0.0380244255065918
Batch 27/64 loss: 0.4724879264831543
Batch 28/64 loss: -0.33156585693359375
Batch 29/64 loss: -0.21091747283935547
Batch 30/64 loss: -0.17473602294921875
Batch 31/64 loss: -0.26354217529296875
Batch 32/64 loss: -0.4685821533203125
Batch 33/64 loss: 0.45258235931396484
Batch 34/64 loss: -0.2872943878173828
Batch 35/64 loss: -0.3689708709716797
Batch 36/64 loss: -0.30760669708251953
Batch 37/64 loss: 0.07914972305297852
Batch 38/64 loss: -0.07802486419677734
Batch 39/64 loss: -0.2579345703125
Batch 40/64 loss: -0.2785148620605469
Batch 41/64 loss: -0.2703380584716797
Batch 42/64 loss: 0.05367469787597656
Batch 43/64 loss: -0.5369415283203125
Batch 44/64 loss: -0.34288501739501953
Batch 45/64 loss: -0.33216285705566406
Batch 46/64 loss: -0.06639862060546875
Batch 47/64 loss: -0.19057655334472656
Batch 48/64 loss: -0.35335731506347656
Batch 49/64 loss: -0.3921041488647461
Batch 50/64 loss: 0.22179889678955078
Batch 51/64 loss: -0.3648242950439453
Batch 52/64 loss: -0.31424903869628906
Batch 53/64 loss: -0.17163562774658203
Batch 54/64 loss: -0.4346780776977539
Batch 55/64 loss: -0.11974239349365234
Batch 56/64 loss: -0.3950338363647461
Batch 57/64 loss: -0.010201454162597656
Batch 58/64 loss: 0.15385770797729492
Batch 59/64 loss: -0.07574176788330078
Batch 60/64 loss: -0.23685455322265625
Batch 61/64 loss: 0.09997320175170898
Batch 62/64 loss: -0.2947111129760742
Batch 63/64 loss: -0.5136823654174805
Batch 64/64 loss: -4.316849231719971
Epoch 292  Train loss: -0.2435368537902832  Val loss: -0.16448615424821467
Saving best model, epoch: 292
Epoch 293
-------------------------------
Batch 1/64 loss: 0.3575477600097656
Batch 2/64 loss: 0.13037395477294922
Batch 3/64 loss: -0.22521400451660156
Batch 4/64 loss: -0.12699604034423828
Batch 5/64 loss: 0.07741069793701172
Batch 6/64 loss: 0.0059795379638671875
Batch 7/64 loss: -0.23418807983398438
Batch 8/64 loss: -0.28323841094970703
Batch 9/64 loss: -0.12377166748046875
Batch 10/64 loss: -0.056206703186035156
Batch 11/64 loss: -0.24260616302490234
Batch 12/64 loss: -0.2760448455810547
Batch 13/64 loss: -0.2756366729736328
Batch 14/64 loss: 0.5214672088623047
Batch 15/64 loss: -0.21363162994384766
Batch 16/64 loss: -0.5032310485839844
Batch 17/64 loss: -0.3538961410522461
Batch 18/64 loss: -0.08351325988769531
Batch 19/64 loss: -0.31252002716064453
Batch 20/64 loss: -0.6087503433227539
Batch 21/64 loss: -0.13323974609375
Batch 22/64 loss: -0.6021862030029297
Batch 23/64 loss: -0.2447671890258789
Batch 24/64 loss: 0.016435623168945312
Batch 25/64 loss: -0.48621368408203125
Batch 26/64 loss: 0.02124786376953125
Batch 27/64 loss: 0.1347036361694336
Batch 28/64 loss: -0.25430870056152344
Batch 29/64 loss: -0.24407196044921875
Batch 30/64 loss: -0.40932464599609375
Batch 31/64 loss: -0.23933982849121094
Batch 32/64 loss: -0.42882442474365234
Batch 33/64 loss: -0.2624626159667969
Batch 34/64 loss: -0.09236717224121094
Batch 35/64 loss: -0.15986347198486328
Batch 36/64 loss: -0.10390663146972656
Batch 37/64 loss: -0.5331554412841797
Batch 38/64 loss: -0.10006904602050781
Batch 39/64 loss: -0.20046520233154297
Batch 40/64 loss: -0.18731403350830078
Batch 41/64 loss: -0.3395271301269531
Batch 42/64 loss: -0.14116191864013672
Batch 43/64 loss: -0.5021467208862305
Batch 44/64 loss: -0.007651329040527344
Batch 45/64 loss: -0.3227863311767578
Batch 46/64 loss: 0.13453912734985352
Batch 47/64 loss: -0.2211170196533203
Batch 48/64 loss: 0.12307357788085938
Batch 49/64 loss: -0.2816753387451172
Batch 50/64 loss: -0.21930503845214844
Batch 51/64 loss: 0.09217596054077148
Batch 52/64 loss: -0.36023902893066406
Batch 53/64 loss: -0.32940673828125
Batch 54/64 loss: -0.34171581268310547
Batch 55/64 loss: -0.12866973876953125
Batch 56/64 loss: -0.12761783599853516
Batch 57/64 loss: -0.48033714294433594
Batch 58/64 loss: -0.30364131927490234
Batch 59/64 loss: -0.1259765625
Batch 60/64 loss: -0.18632984161376953
Batch 61/64 loss: -0.15577030181884766
Batch 62/64 loss: -0.15325260162353516
Batch 63/64 loss: -0.28856563568115234
Batch 64/64 loss: -4.107894420623779
Epoch 293  Train loss: -0.23661465551338945  Val loss: -0.037846555414888045
Epoch 294
-------------------------------
Batch 1/64 loss: -0.21910905838012695
Batch 2/64 loss: -0.3177967071533203
Batch 3/64 loss: 0.5015945434570312
Batch 4/64 loss: 0.45845508575439453
Batch 5/64 loss: -0.2548246383666992
Batch 6/64 loss: -0.04517936706542969
Batch 7/64 loss: 0.1354374885559082
Batch 8/64 loss: -0.4219522476196289
Batch 9/64 loss: -0.23536062240600586
Batch 10/64 loss: -0.22501182556152344
Batch 11/64 loss: -0.5138463973999023
Batch 12/64 loss: -0.3229055404663086
Batch 13/64 loss: -0.22115564346313477
Batch 14/64 loss: -0.3330373764038086
Batch 15/64 loss: 0.016473770141601562
Batch 16/64 loss: -0.009804725646972656
Batch 17/64 loss: -0.4720726013183594
Batch 18/64 loss: -0.13587713241577148
Batch 19/64 loss: 1.6045098304748535
Batch 20/64 loss: 0.003395557403564453
Batch 21/64 loss: -0.42605018615722656
Batch 22/64 loss: -0.22169876098632812
Batch 23/64 loss: 0.012796878814697266
Batch 24/64 loss: 0.28406715393066406
Batch 25/64 loss: -0.4954557418823242
Batch 26/64 loss: -0.02305746078491211
Batch 27/64 loss: 0.0019807815551757812
Batch 28/64 loss: -0.1324462890625
Batch 29/64 loss: 0.06762886047363281
Batch 30/64 loss: 0.01606607437133789
Batch 31/64 loss: 0.07857799530029297
Batch 32/64 loss: 0.06572246551513672
Batch 33/64 loss: 0.11161088943481445
Batch 34/64 loss: -0.1633586883544922
Batch 35/64 loss: -0.11775016784667969
Batch 36/64 loss: -0.1723165512084961
Batch 37/64 loss: 0.11331367492675781
Batch 38/64 loss: 0.052292823791503906
Batch 39/64 loss: -0.2470850944519043
Batch 40/64 loss: -0.03194093704223633
Batch 41/64 loss: -0.12730741500854492
Batch 42/64 loss: -0.2786712646484375
Batch 43/64 loss: -0.2848358154296875
Batch 44/64 loss: -0.01722240447998047
Batch 45/64 loss: 0.17798709869384766
Batch 46/64 loss: 0.2505016326904297
Batch 47/64 loss: 0.16147851943969727
Batch 48/64 loss: 0.019083499908447266
Batch 49/64 loss: -0.3364901542663574
Batch 50/64 loss: 0.08466911315917969
Batch 51/64 loss: -0.22188329696655273
Batch 52/64 loss: -0.15703630447387695
Batch 53/64 loss: -0.2656402587890625
Batch 54/64 loss: 0.11670255661010742
Batch 55/64 loss: -0.11671638488769531
Batch 56/64 loss: -0.08245086669921875
Batch 57/64 loss: -0.17850780487060547
Batch 58/64 loss: -0.12733221054077148
Batch 59/64 loss: -0.1673436164855957
Batch 60/64 loss: 0.22867870330810547
Batch 61/64 loss: -0.31721019744873047
Batch 62/64 loss: 0.18877363204956055
Batch 63/64 loss: -0.30343103408813477
Batch 64/64 loss: -3.309506416320801
Epoch 294  Train loss: -0.10151378781187768  Val loss: -0.09448078325933607
Epoch 295
-------------------------------
Batch 1/64 loss: -0.5084443092346191
Batch 2/64 loss: -0.03834247589111328
Batch 3/64 loss: -0.25910472869873047
Batch 4/64 loss: -0.008885383605957031
Batch 5/64 loss: -0.2853841781616211
Batch 6/64 loss: -0.28403615951538086
Batch 7/64 loss: -0.20481109619140625
Batch 8/64 loss: -0.3218679428100586
Batch 9/64 loss: 0.1874380111694336
Batch 10/64 loss: -0.292877197265625
Batch 11/64 loss: -0.3239421844482422
Batch 12/64 loss: -0.24189996719360352
Batch 13/64 loss: -0.3786330223083496
Batch 14/64 loss: -0.1968531608581543
Batch 15/64 loss: 0.050353050231933594
Batch 16/64 loss: -0.3284015655517578
Batch 17/64 loss: -0.4031953811645508
Batch 18/64 loss: -0.0827932357788086
Batch 19/64 loss: 0.004749298095703125
Batch 20/64 loss: -0.43630409240722656
Batch 21/64 loss: 0.22501134872436523
Batch 22/64 loss: -0.2433338165283203
Batch 23/64 loss: -0.20686054229736328
Batch 24/64 loss: -0.4806337356567383
Batch 25/64 loss: -0.2507929801940918
Batch 26/64 loss: -0.2840409278869629
Batch 27/64 loss: 0.024372100830078125
Batch 28/64 loss: -0.033463478088378906
Batch 29/64 loss: -0.30480241775512695
Batch 30/64 loss: -0.17055702209472656
Batch 31/64 loss: -0.2871274948120117
Batch 32/64 loss: 0.307100772857666
Batch 33/64 loss: -0.3276329040527344
Batch 34/64 loss: -0.11882209777832031
Batch 35/64 loss: 0.12256240844726562
Batch 36/64 loss: -0.03930377960205078
Batch 37/64 loss: -0.18842506408691406
Batch 38/64 loss: -0.3570899963378906
Batch 39/64 loss: -0.21062850952148438
Batch 40/64 loss: -0.25472545623779297
Batch 41/64 loss: -0.044051170349121094
Batch 42/64 loss: -0.3360285758972168
Batch 43/64 loss: -0.06057405471801758
Batch 44/64 loss: -0.0028166770935058594
Batch 45/64 loss: -0.24387264251708984
Batch 46/64 loss: -0.026102066040039062
Batch 47/64 loss: 0.03741741180419922
Batch 48/64 loss: -0.1688232421875
Batch 49/64 loss: 0.029111862182617188
Batch 50/64 loss: -0.28774261474609375
Batch 51/64 loss: -0.2613968849182129
Batch 52/64 loss: 0.11386919021606445
Batch 53/64 loss: -0.2276172637939453
Batch 54/64 loss: -0.11075687408447266
Batch 55/64 loss: -0.04649925231933594
Batch 56/64 loss: -0.0022172927856445312
Batch 57/64 loss: -0.20972919464111328
Batch 58/64 loss: 0.1652693748474121
Batch 59/64 loss: -0.1260848045349121
Batch 60/64 loss: -0.19719314575195312
Batch 61/64 loss: 0.2322397232055664
Batch 62/64 loss: 0.021313190460205078
Batch 63/64 loss: -0.1646413803100586
Batch 64/64 loss: -3.7333550453186035
Epoch 295  Train loss: -0.19057838402542412  Val loss: -0.08628800644497692
Epoch 296
-------------------------------
Batch 1/64 loss: -0.10956096649169922
Batch 2/64 loss: -0.27658939361572266
Batch 3/64 loss: -0.032876014709472656
Batch 4/64 loss: -0.2070446014404297
Batch 5/64 loss: -0.3893299102783203
Batch 6/64 loss: 0.10261058807373047
Batch 7/64 loss: -0.3398275375366211
Batch 8/64 loss: -0.44288063049316406
Batch 9/64 loss: -0.20697689056396484
Batch 10/64 loss: -0.34429264068603516
Batch 11/64 loss: -0.2530708312988281
Batch 12/64 loss: -0.31154537200927734
Batch 13/64 loss: -0.32399845123291016
Batch 14/64 loss: -0.22052764892578125
Batch 15/64 loss: 0.019701004028320312
Batch 16/64 loss: -0.36786746978759766
Batch 17/64 loss: -0.397705078125
Batch 18/64 loss: 0.08684444427490234
Batch 19/64 loss: -0.08850479125976562
Batch 20/64 loss: -0.1945514678955078
Batch 21/64 loss: -0.5976457595825195
Batch 22/64 loss: -0.05893135070800781
Batch 23/64 loss: -0.13133811950683594
Batch 24/64 loss: 0.10376167297363281
Batch 25/64 loss: -0.4766387939453125
Batch 26/64 loss: 0.06504058837890625
Batch 27/64 loss: -0.11658287048339844
Batch 28/64 loss: -0.33280181884765625
Batch 29/64 loss: -0.1588273048400879
Batch 30/64 loss: -0.039775848388671875
Batch 31/64 loss: -0.070709228515625
Batch 32/64 loss: -0.47293853759765625
Batch 33/64 loss: -0.4840536117553711
Batch 34/64 loss: -0.2848968505859375
Batch 35/64 loss: -0.46773529052734375
Batch 36/64 loss: -0.23721790313720703
Batch 37/64 loss: 0.030858993530273438
Batch 38/64 loss: 0.043824195861816406
Batch 39/64 loss: -0.40784645080566406
Batch 40/64 loss: 0.037484169006347656
Batch 41/64 loss: 0.04817962646484375
Batch 42/64 loss: 0.05244112014770508
Batch 43/64 loss: -0.2696084976196289
Batch 44/64 loss: -0.18526840209960938
Batch 45/64 loss: -0.2865610122680664
Batch 46/64 loss: 0.1868133544921875
Batch 47/64 loss: -0.4602193832397461
Batch 48/64 loss: -0.2323155403137207
Batch 49/64 loss: -0.08077287673950195
Batch 50/64 loss: -0.16291522979736328
Batch 51/64 loss: -0.2126626968383789
Batch 52/64 loss: -0.2395782470703125
Batch 53/64 loss: 0.040477752685546875
Batch 54/64 loss: -0.3409299850463867
Batch 55/64 loss: -0.35555458068847656
Batch 56/64 loss: -0.22339630126953125
Batch 57/64 loss: -0.3524656295776367
Batch 58/64 loss: -0.22800636291503906
Batch 59/64 loss: 0.02468585968017578
Batch 60/64 loss: -0.0053882598876953125
Batch 61/64 loss: -0.2926950454711914
Batch 62/64 loss: -0.32445812225341797
Batch 63/64 loss: -0.10203313827514648
Batch 64/64 loss: -4.311638355255127
Epoch 296  Train loss: -0.2445635159810384  Val loss: -0.18790393842454628
Saving best model, epoch: 296
Epoch 297
-------------------------------
Batch 1/64 loss: -0.5491714477539062
Batch 2/64 loss: 0.22620582580566406
Batch 3/64 loss: -0.07475519180297852
Batch 4/64 loss: -0.545903205871582
Batch 5/64 loss: -0.248626708984375
Batch 6/64 loss: -0.28118896484375
Batch 7/64 loss: -0.3917274475097656
Batch 8/64 loss: -0.45371150970458984
Batch 9/64 loss: -0.4483318328857422
Batch 10/64 loss: -0.33085107803344727
Batch 11/64 loss: 0.10979938507080078
Batch 12/64 loss: -0.08768939971923828
Batch 13/64 loss: -0.6430683135986328
Batch 14/64 loss: -0.40329647064208984
Batch 15/64 loss: -0.3020801544189453
Batch 16/64 loss: -0.02781391143798828
Batch 17/64 loss: -0.2862215042114258
Batch 18/64 loss: 0.027610301971435547
Batch 19/64 loss: -0.28546619415283203
Batch 20/64 loss: -0.5369997024536133
Batch 21/64 loss: -0.371462345123291
Batch 22/64 loss: -0.1870260238647461
Batch 23/64 loss: -0.38018131256103516
Batch 24/64 loss: 0.038344383239746094
Batch 25/64 loss: -0.35018348693847656
Batch 26/64 loss: -0.4999532699584961
Batch 27/64 loss: -0.19047260284423828
Batch 28/64 loss: -0.04975748062133789
Batch 29/64 loss: -0.6799755096435547
Batch 30/64 loss: 0.2308330535888672
Batch 31/64 loss: -0.2875032424926758
Batch 32/64 loss: -0.4905662536621094
Batch 33/64 loss: -0.4129657745361328
Batch 34/64 loss: -0.06757736206054688
Batch 35/64 loss: -0.18598365783691406
Batch 36/64 loss: -0.3366212844848633
Batch 37/64 loss: -0.27697277069091797
Batch 38/64 loss: -0.2786750793457031
Batch 39/64 loss: -0.2824687957763672
Batch 40/64 loss: -0.008385658264160156
Batch 41/64 loss: -0.2900400161743164
Batch 42/64 loss: -0.4881305694580078
Batch 43/64 loss: -0.11496829986572266
Batch 44/64 loss: -0.3331928253173828
Batch 45/64 loss: -0.2741250991821289
Batch 46/64 loss: -0.3044700622558594
Batch 47/64 loss: -0.2892446517944336
Batch 48/64 loss: -0.3823232650756836
Batch 49/64 loss: -0.38949012756347656
Batch 50/64 loss: -0.1232614517211914
Batch 51/64 loss: -0.2652149200439453
Batch 52/64 loss: 0.21654987335205078
Batch 53/64 loss: 0.7625269889831543
Batch 54/64 loss: -0.4522542953491211
Batch 55/64 loss: 0.008415699005126953
Batch 56/64 loss: -0.1204233169555664
Batch 57/64 loss: 0.22839689254760742
Batch 58/64 loss: -0.5223922729492188
Batch 59/64 loss: -0.17545223236083984
Batch 60/64 loss: -0.031348228454589844
Batch 61/64 loss: -0.17425823211669922
Batch 62/64 loss: -0.4402294158935547
Batch 63/64 loss: -0.27140140533447266
Batch 64/64 loss: -4.364468097686768
Epoch 297  Train loss: -0.2839297930399577  Val loss: -0.1388810279033438
Epoch 298
-------------------------------
Batch 1/64 loss: -0.16357040405273438
Batch 2/64 loss: -0.14885711669921875
Batch 3/64 loss: -0.39409542083740234
Batch 4/64 loss: -0.048111915588378906
Batch 5/64 loss: -0.3113679885864258
Batch 6/64 loss: 0.1830730438232422
Batch 7/64 loss: -0.23705005645751953
Batch 8/64 loss: -0.3175182342529297
Batch 9/64 loss: 0.09326887130737305
Batch 10/64 loss: -0.35415172576904297
Batch 11/64 loss: -0.3944683074951172
Batch 12/64 loss: -0.2868518829345703
Batch 13/64 loss: -0.46594810485839844
Batch 14/64 loss: 0.0029897689819335938
Batch 15/64 loss: -0.1866617202758789
Batch 16/64 loss: -0.15442657470703125
Batch 17/64 loss: -0.18167591094970703
Batch 18/64 loss: -0.4218320846557617
Batch 19/64 loss: -0.19100570678710938
Batch 20/64 loss: -0.3320283889770508
Batch 21/64 loss: -0.31342220306396484
Batch 22/64 loss: -0.02123260498046875
Batch 23/64 loss: -0.5280017852783203
Batch 24/64 loss: -0.1645793914794922
Batch 25/64 loss: 0.06815910339355469
Batch 26/64 loss: -0.4825763702392578
Batch 27/64 loss: 0.26642894744873047
Batch 28/64 loss: -0.2926311492919922
Batch 29/64 loss: -0.3422269821166992
Batch 30/64 loss: -0.3330850601196289
Batch 31/64 loss: -0.10467147827148438
Batch 32/64 loss: -0.44263553619384766
Batch 33/64 loss: -0.10069942474365234
Batch 34/64 loss: -0.4362945556640625
Batch 35/64 loss: 0.23475980758666992
Batch 36/64 loss: -0.2158060073852539
Batch 37/64 loss: 0.21452903747558594
Batch 38/64 loss: -0.3475484848022461
Batch 39/64 loss: -0.5403661727905273
Batch 40/64 loss: -0.16493892669677734
Batch 41/64 loss: -0.12369918823242188
Batch 42/64 loss: -0.2124004364013672
Batch 43/64 loss: -0.0811147689819336
Batch 44/64 loss: 0.11902809143066406
Batch 45/64 loss: -0.2458057403564453
Batch 46/64 loss: -0.48882389068603516
Batch 47/64 loss: 0.16987037658691406
Batch 48/64 loss: -0.1401352882385254
Batch 49/64 loss: 0.143218994140625
Batch 50/64 loss: -0.23811626434326172
Batch 51/64 loss: -0.5904722213745117
Batch 52/64 loss: -0.3577861785888672
Batch 53/64 loss: -0.39468860626220703
Batch 54/64 loss: -0.32910633087158203
Batch 55/64 loss: -0.22345638275146484
Batch 56/64 loss: -0.47951412200927734
Batch 57/64 loss: -0.03952503204345703
Batch 58/64 loss: -0.5135631561279297
Batch 59/64 loss: -0.3559989929199219
Batch 60/64 loss: -0.11585235595703125
Batch 61/64 loss: -0.010594844818115234
Batch 62/64 loss: -0.30786991119384766
Batch 63/64 loss: -0.4766216278076172
Batch 64/64 loss: -4.555696964263916
Epoch 298  Train loss: -0.26771654428220265  Val loss: -0.07257760431348663
Epoch 299
-------------------------------
Batch 1/64 loss: 0.07222461700439453
Batch 2/64 loss: -0.633636474609375
Batch 3/64 loss: -0.19493961334228516
Batch 4/64 loss: -0.38979148864746094
Batch 5/64 loss: -0.1473531723022461
Batch 6/64 loss: -0.2388772964477539
Batch 7/64 loss: -0.4467487335205078
Batch 8/64 loss: -0.35477638244628906
Batch 9/64 loss: 0.16890525817871094
Batch 10/64 loss: -0.4943552017211914
Batch 11/64 loss: -0.16699600219726562
Batch 12/64 loss: 0.12709903717041016
Batch 13/64 loss: 0.030246257781982422
Batch 14/64 loss: 0.061959266662597656
Batch 15/64 loss: -0.28855419158935547
Batch 16/64 loss: -0.43134498596191406
Batch 17/64 loss: 0.0650186538696289
Batch 18/64 loss: 0.21248960494995117
Batch 19/64 loss: -0.3077077865600586
Batch 20/64 loss: 0.12582063674926758
Batch 21/64 loss: -0.4063377380371094
Batch 22/64 loss: -0.4915037155151367
Batch 23/64 loss: -0.08265066146850586
Batch 24/64 loss: -0.36892032623291016
Batch 25/64 loss: -0.4597282409667969
Batch 26/64 loss: -0.04905223846435547
Batch 27/64 loss: 0.39061832427978516
Batch 28/64 loss: -0.34067583084106445
Batch 29/64 loss: -0.13625574111938477
Batch 30/64 loss: -0.05411338806152344
Batch 31/64 loss: -0.5703144073486328
Batch 32/64 loss: -0.5080499649047852
Batch 33/64 loss: -0.0649409294128418
Batch 34/64 loss: -0.3771400451660156
Batch 35/64 loss: -0.3198375701904297
Batch 36/64 loss: -0.5869312286376953
Batch 37/64 loss: -0.1226348876953125
Batch 38/64 loss: -0.07997322082519531
Batch 39/64 loss: 0.3019237518310547
Batch 40/64 loss: -0.18266773223876953
Batch 41/64 loss: -0.4707632064819336
Batch 42/64 loss: -0.2817268371582031
Batch 43/64 loss: -0.4637413024902344
Batch 44/64 loss: 0.0032033920288085938
Batch 45/64 loss: -0.2978248596191406
Batch 46/64 loss: -0.2837820053100586
Batch 47/64 loss: 0.5018982887268066
Batch 48/64 loss: -0.36078548431396484
Batch 49/64 loss: -0.22434663772583008
Batch 50/64 loss: -0.11766910552978516
Batch 51/64 loss: -0.21312713623046875
Batch 52/64 loss: -0.014000415802001953
Batch 53/64 loss: -0.19528579711914062
Batch 54/64 loss: -0.43478870391845703
Batch 55/64 loss: 0.19838333129882812
Batch 56/64 loss: -0.12138175964355469
Batch 57/64 loss: -0.2298145294189453
Batch 58/64 loss: -0.3181343078613281
Batch 59/64 loss: -0.04237079620361328
Batch 60/64 loss: 0.09178447723388672
Batch 61/64 loss: 0.07375001907348633
Batch 62/64 loss: -0.14093303680419922
Batch 63/64 loss: -0.12092208862304688
Batch 64/64 loss: -4.106294631958008
Epoch 299  Train loss: -0.22404083551145068  Val loss: -0.17849799611724118
Epoch 300
-------------------------------
Batch 1/64 loss: -0.12368202209472656
Batch 2/64 loss: -0.18709087371826172
Batch 3/64 loss: -0.5152273178100586
Batch 4/64 loss: -0.20955371856689453
Batch 5/64 loss: 0.22891616821289062
Batch 6/64 loss: 0.10433149337768555
Batch 7/64 loss: -0.17898178100585938
Batch 8/64 loss: 0.19616079330444336
Batch 9/64 loss: -0.12989139556884766
Batch 10/64 loss: -0.4019479751586914
Batch 11/64 loss: -0.22183513641357422
Batch 12/64 loss: -0.39542198181152344
Batch 13/64 loss: -0.27157115936279297
Batch 14/64 loss: -0.1382884979248047
Batch 15/64 loss: -0.1604323387145996
Batch 16/64 loss: 0.10509204864501953
Batch 17/64 loss: -0.3981304168701172
Batch 18/64 loss: -0.1845235824584961
Batch 19/64 loss: -0.2742953300476074
Batch 20/64 loss: -0.6499061584472656
Batch 21/64 loss: -0.5416736602783203
Batch 22/64 loss: -0.14074134826660156
Batch 23/64 loss: -0.570500373840332
Batch 24/64 loss: 0.04432344436645508
Batch 25/64 loss: -0.41255855560302734
Batch 26/64 loss: -0.39273834228515625
Batch 27/64 loss: -0.32285118103027344
Batch 28/64 loss: 0.056496620178222656
Batch 29/64 loss: -0.3865337371826172
Batch 30/64 loss: -0.2797870635986328
Batch 31/64 loss: -0.45186519622802734
Batch 32/64 loss: -0.1393289566040039
Batch 33/64 loss: -0.1475229263305664
Batch 34/64 loss: -0.2687230110168457
Batch 35/64 loss: -0.3946409225463867
Batch 36/64 loss: -0.11306619644165039
Batch 37/64 loss: -0.3863396644592285
Batch 38/64 loss: -0.16321086883544922
Batch 39/64 loss: -0.30374622344970703
Batch 40/64 loss: -0.2312793731689453
Batch 41/64 loss: -0.1767435073852539
Batch 42/64 loss: -0.25071048736572266
Batch 43/64 loss: 0.02078104019165039
Batch 44/64 loss: -0.42916011810302734
Batch 45/64 loss: -0.05961942672729492
Batch 46/64 loss: -0.366973876953125
Batch 47/64 loss: -0.38181209564208984
Batch 48/64 loss: -0.4455118179321289
Batch 49/64 loss: -0.5065069198608398
Batch 50/64 loss: -0.2377767562866211
Batch 51/64 loss: -0.6391696929931641
Batch 52/64 loss: -0.4846162796020508
Batch 53/64 loss: 0.13976049423217773
Batch 54/64 loss: -0.44029998779296875
Batch 55/64 loss: -0.12456321716308594
Batch 56/64 loss: -0.42467403411865234
Batch 57/64 loss: -0.2614154815673828
Batch 58/64 loss: -0.024076461791992188
Batch 59/64 loss: -0.3406076431274414
Batch 60/64 loss: -0.2313232421875
Batch 61/64 loss: -0.34770679473876953
Batch 62/64 loss: -0.04039573669433594
Batch 63/64 loss: -0.5454444885253906
Batch 64/64 loss: -4.016657829284668
Epoch 300  Train loss: -0.2974686529122147  Val loss: -0.22010130734787775
Saving best model, epoch: 300
Epoch 301
-------------------------------
Batch 1/64 loss: -0.43309974670410156
Batch 2/64 loss: -0.05663108825683594
Batch 3/64 loss: -0.667597770690918
Batch 4/64 loss: -0.3260049819946289
Batch 5/64 loss: -0.47103023529052734
Batch 6/64 loss: -0.20713520050048828
Batch 7/64 loss: -0.2542247772216797
Batch 8/64 loss: 0.3645515441894531
Batch 9/64 loss: -0.5704002380371094
Batch 10/64 loss: -0.18572044372558594
Batch 11/64 loss: -0.6717967987060547
Batch 12/64 loss: -0.04137706756591797
Batch 13/64 loss: -0.37300872802734375
Batch 14/64 loss: -0.3694887161254883
Batch 15/64 loss: -0.2654447555541992
Batch 16/64 loss: -0.25655174255371094
Batch 17/64 loss: -0.4692678451538086
Batch 18/64 loss: -0.7944793701171875
Batch 19/64 loss: -0.2270193099975586
Batch 20/64 loss: -0.27717113494873047
Batch 21/64 loss: -0.2599220275878906
Batch 22/64 loss: -0.33423423767089844
Batch 23/64 loss: 0.1885061264038086
Batch 24/64 loss: -0.31377410888671875
Batch 25/64 loss: -0.3245525360107422
Batch 26/64 loss: -0.44464874267578125
Batch 27/64 loss: -0.5748786926269531
Batch 28/64 loss: -0.4778919219970703
Batch 29/64 loss: -0.43631553649902344
Batch 30/64 loss: -0.08237171173095703
Batch 31/64 loss: -0.3107185363769531
Batch 32/64 loss: -0.18040180206298828
Batch 33/64 loss: -0.48865604400634766
Batch 34/64 loss: 0.15749645233154297
Batch 35/64 loss: 0.1675090789794922
Batch 36/64 loss: -0.10205745697021484
Batch 37/64 loss: 0.16887855529785156
Batch 38/64 loss: -0.3381938934326172
Batch 39/64 loss: -0.26050281524658203
Batch 40/64 loss: -0.2827301025390625
Batch 41/64 loss: -0.21313762664794922
Batch 42/64 loss: 0.3370485305786133
Batch 43/64 loss: -0.3315715789794922
Batch 44/64 loss: -0.3176431655883789
Batch 45/64 loss: 0.2249927520751953
Batch 46/64 loss: -0.1458282470703125
Batch 47/64 loss: -0.5242938995361328
Batch 48/64 loss: -0.4733695983886719
Batch 49/64 loss: -0.15927791595458984
Batch 50/64 loss: -0.08022403717041016
Batch 51/64 loss: -0.2944297790527344
Batch 52/64 loss: -0.19707393646240234
Batch 53/64 loss: -0.42564964294433594
Batch 54/64 loss: -0.09164142608642578
Batch 55/64 loss: -0.15972137451171875
Batch 56/64 loss: 0.13693571090698242
Batch 57/64 loss: -0.1139059066772461
Batch 58/64 loss: -0.18964290618896484
Batch 59/64 loss: -0.45115184783935547
Batch 60/64 loss: -0.1978168487548828
Batch 61/64 loss: -0.46829795837402344
Batch 62/64 loss: -0.4598217010498047
Batch 63/64 loss: -0.33347320556640625
Batch 64/64 loss: -4.158001899719238
Epoch 301  Train loss: -0.3000761630488377  Val loss: -0.17221626465263234
Epoch 302
-------------------------------
Batch 1/64 loss: -0.39891815185546875
Batch 2/64 loss: 0.09889888763427734
Batch 3/64 loss: -0.10957717895507812
Batch 4/64 loss: -0.16340255737304688
Batch 5/64 loss: -0.2598552703857422
Batch 6/64 loss: -0.07856559753417969
Batch 7/64 loss: -0.10959339141845703
Batch 8/64 loss: -0.24022388458251953
Batch 9/64 loss: 0.22136497497558594
Batch 10/64 loss: -0.19915390014648438
Batch 11/64 loss: 0.32351112365722656
Batch 12/64 loss: -0.12059497833251953
Batch 13/64 loss: -0.3654317855834961
Batch 14/64 loss: -0.11792755126953125
Batch 15/64 loss: -0.3362455368041992
Batch 16/64 loss: -0.47429943084716797
Batch 17/64 loss: -0.3865079879760742
Batch 18/64 loss: -0.11720752716064453
Batch 19/64 loss: -0.18827152252197266
Batch 20/64 loss: -0.17715167999267578
Batch 21/64 loss: -0.6194295883178711
Batch 22/64 loss: -0.23035430908203125
Batch 23/64 loss: -0.4302377700805664
Batch 24/64 loss: -0.19233131408691406
Batch 25/64 loss: -0.19508647918701172
Batch 26/64 loss: -0.5342617034912109
Batch 27/64 loss: -0.3921823501586914
Batch 28/64 loss: 0.008769989013671875
Batch 29/64 loss: -0.37630176544189453
Batch 30/64 loss: -0.5855741500854492
Batch 31/64 loss: -0.4834280014038086
Batch 32/64 loss: -0.16884422302246094
Batch 33/64 loss: -0.06674480438232422
Batch 34/64 loss: -0.33272266387939453
Batch 35/64 loss: -0.18741130828857422
Batch 36/64 loss: 0.05213165283203125
Batch 37/64 loss: -0.15031051635742188
Batch 38/64 loss: -0.18442916870117188
Batch 39/64 loss: -0.4937429428100586
Batch 40/64 loss: -0.18266773223876953
Batch 41/64 loss: -0.08089637756347656
Batch 42/64 loss: -0.08908367156982422
Batch 43/64 loss: -0.13360881805419922
Batch 44/64 loss: -0.3517923355102539
Batch 45/64 loss: -0.026480674743652344
Batch 46/64 loss: 0.05172157287597656
Batch 47/64 loss: -0.2890920639038086
Batch 48/64 loss: -0.22291183471679688
Batch 49/64 loss: -0.3394355773925781
Batch 50/64 loss: -0.38962745666503906
Batch 51/64 loss: 0.06485939025878906
Batch 52/64 loss: -0.07678413391113281
Batch 53/64 loss: -0.28647899627685547
Batch 54/64 loss: -0.3278541564941406
Batch 55/64 loss: -0.596165657043457
Batch 56/64 loss: -0.5176658630371094
Batch 57/64 loss: -0.6207180023193359
Batch 58/64 loss: -0.18926048278808594
Batch 59/64 loss: -0.5141563415527344
Batch 60/64 loss: -0.05160188674926758
Batch 61/64 loss: -0.532841682434082
Batch 62/64 loss: -0.28934574127197266
Batch 63/64 loss: -0.4538145065307617
Batch 64/64 loss: -4.54862642288208
Epoch 302  Train loss: -0.2920598778070188  Val loss: -0.12688714777890758
Epoch 303
-------------------------------
Batch 1/64 loss: -0.11772346496582031
Batch 2/64 loss: -0.36965179443359375
Batch 3/64 loss: -0.40788745880126953
Batch 4/64 loss: -0.2037639617919922
Batch 5/64 loss: -0.4357290267944336
Batch 6/64 loss: -0.7221288681030273
Batch 7/64 loss: 0.031426429748535156
Batch 8/64 loss: -0.17511653900146484
Batch 9/64 loss: -0.30054759979248047
Batch 10/64 loss: -0.6443614959716797
Batch 11/64 loss: -0.3877267837524414
Batch 12/64 loss: -0.4546842575073242
Batch 13/64 loss: -0.45743465423583984
Batch 14/64 loss: 0.044989585876464844
Batch 15/64 loss: 0.0913991928100586
Batch 16/64 loss: -0.2935295104980469
Batch 17/64 loss: -0.38109588623046875
Batch 18/64 loss: -0.21582317352294922
Batch 19/64 loss: -0.09989166259765625
Batch 20/64 loss: 0.11488914489746094
Batch 21/64 loss: -0.17854547500610352
Batch 22/64 loss: -0.38186073303222656
Batch 23/64 loss: -0.07926464080810547
Batch 24/64 loss: -0.4155998229980469
Batch 25/64 loss: -0.2512502670288086
Batch 26/64 loss: 0.16480302810668945
Batch 27/64 loss: -0.09596729278564453
Batch 28/64 loss: -0.39214420318603516
Batch 29/64 loss: -0.3878002166748047
Batch 30/64 loss: -0.35179710388183594
Batch 31/64 loss: -0.36270904541015625
Batch 32/64 loss: 0.10739994049072266
Batch 33/64 loss: -0.45921802520751953
Batch 34/64 loss: -0.03869009017944336
Batch 35/64 loss: -0.09769678115844727
Batch 36/64 loss: -0.2359771728515625
Batch 37/64 loss: -0.1941661834716797
Batch 38/64 loss: -0.6823692321777344
Batch 39/64 loss: -0.1443777084350586
Batch 40/64 loss: -0.24948883056640625
Batch 41/64 loss: -0.039606571197509766
Batch 42/64 loss: -0.0476222038269043
Batch 43/64 loss: -0.40436458587646484
Batch 44/64 loss: -0.06258869171142578
Batch 45/64 loss: 0.029475688934326172
Batch 46/64 loss: -0.47581005096435547
Batch 47/64 loss: -0.13314342498779297
Batch 48/64 loss: -0.3776226043701172
Batch 49/64 loss: -0.4245719909667969
Batch 50/64 loss: -0.010907173156738281
Batch 51/64 loss: -0.38617706298828125
Batch 52/64 loss: -0.29561424255371094
Batch 53/64 loss: -0.3825225830078125
Batch 54/64 loss: -0.3945579528808594
Batch 55/64 loss: -0.37943172454833984
Batch 56/64 loss: -0.3480396270751953
Batch 57/64 loss: -0.4522275924682617
Batch 58/64 loss: -0.18957948684692383
Batch 59/64 loss: -0.2771329879760742
Batch 60/64 loss: -0.5484609603881836
Batch 61/64 loss: -0.1816091537475586
Batch 62/64 loss: -0.5076303482055664
Batch 63/64 loss: -0.28485870361328125
Batch 64/64 loss: -3.450167179107666
Epoch 303  Train loss: -0.30232691858329025  Val loss: -0.08283418478425016
Epoch 304
-------------------------------
Batch 1/64 loss: -0.24967670440673828
Batch 2/64 loss: 0.039839744567871094
Batch 3/64 loss: -0.3715972900390625
Batch 4/64 loss: -0.6973285675048828
Batch 5/64 loss: -0.3857889175415039
Batch 6/64 loss: -0.3451995849609375
Batch 7/64 loss: -0.6124105453491211
Batch 8/64 loss: 0.15637445449829102
Batch 9/64 loss: 0.39455413818359375
Batch 10/64 loss: -0.4032764434814453
Batch 11/64 loss: -0.3482093811035156
Batch 12/64 loss: -0.010293006896972656
Batch 13/64 loss: -0.1134176254272461
Batch 14/64 loss: -0.278076171875
Batch 15/64 loss: -0.5081443786621094
Batch 16/64 loss: -0.2905616760253906
Batch 17/64 loss: -0.16290664672851562
Batch 18/64 loss: -0.41123199462890625
Batch 19/64 loss: -0.4999713897705078
Batch 20/64 loss: -0.4185647964477539
Batch 21/64 loss: -0.1324291229248047
Batch 22/64 loss: 0.03015422821044922
Batch 23/64 loss: -0.09451866149902344
Batch 24/64 loss: -0.10457420349121094
Batch 25/64 loss: -0.500737190246582
Batch 26/64 loss: -0.14652156829833984
Batch 27/64 loss: -0.4302663803100586
Batch 28/64 loss: -0.22049236297607422
Batch 29/64 loss: -0.44387054443359375
Batch 30/64 loss: -0.05729103088378906
Batch 31/64 loss: -0.5199089050292969
Batch 32/64 loss: -0.09101486206054688
Batch 33/64 loss: -0.062489986419677734
Batch 34/64 loss: -0.10226964950561523
Batch 35/64 loss: -0.4364314079284668
Batch 36/64 loss: -0.5513792037963867
Batch 37/64 loss: 0.0682835578918457
Batch 38/64 loss: -0.5524196624755859
Batch 39/64 loss: 0.08798980712890625
Batch 40/64 loss: -0.5594320297241211
Batch 41/64 loss: -0.2942361831665039
Batch 42/64 loss: -0.49176979064941406
Batch 43/64 loss: 0.40084171295166016
Batch 44/64 loss: -0.3605489730834961
Batch 45/64 loss: -0.4963970184326172
Batch 46/64 loss: -0.5325746536254883
Batch 47/64 loss: -0.1865978240966797
Batch 48/64 loss: -0.5753850936889648
Batch 49/64 loss: -0.5284948348999023
Batch 50/64 loss: -0.03588724136352539
Batch 51/64 loss: -0.2699289321899414
Batch 52/64 loss: -0.05876445770263672
Batch 53/64 loss: -0.2214670181274414
Batch 54/64 loss: -0.47479724884033203
Batch 55/64 loss: -0.4251737594604492
Batch 56/64 loss: -0.5691671371459961
Batch 57/64 loss: -0.3840169906616211
Batch 58/64 loss: 0.9130048751831055
Batch 59/64 loss: -0.4087247848510742
Batch 60/64 loss: -0.3932943344116211
Batch 61/64 loss: -0.3364381790161133
Batch 62/64 loss: -0.27208900451660156
Batch 63/64 loss: -0.5132083892822266
Batch 64/64 loss: -4.179110527038574
Epoch 304  Train loss: -0.3134894763722139  Val loss: -0.207676707264484
Epoch 305
-------------------------------
Batch 1/64 loss: -0.12978458404541016
Batch 2/64 loss: -0.15247726440429688
Batch 3/64 loss: -0.5159320831298828
Batch 4/64 loss: 0.010209083557128906
Batch 5/64 loss: -0.4880800247192383
Batch 6/64 loss: -0.20903873443603516
Batch 7/64 loss: -0.33975791931152344
Batch 8/64 loss: -0.2545027732849121
Batch 9/64 loss: -0.23579883575439453
Batch 10/64 loss: -0.5262660980224609
Batch 11/64 loss: -0.06700658798217773
Batch 12/64 loss: 0.010378360748291016
Batch 13/64 loss: -0.25581979751586914
Batch 14/64 loss: -0.32204341888427734
Batch 15/64 loss: 0.12626361846923828
Batch 16/64 loss: -0.49111175537109375
Batch 17/64 loss: -0.3424234390258789
Batch 18/64 loss: -0.43637943267822266
Batch 19/64 loss: -0.10378122329711914
Batch 20/64 loss: -0.2726726531982422
Batch 21/64 loss: 0.0021810531616210938
Batch 22/64 loss: -0.17625999450683594
Batch 23/64 loss: -0.24805545806884766
Batch 24/64 loss: 0.12503623962402344
Batch 25/64 loss: -0.10501384735107422
Batch 26/64 loss: -0.32836341857910156
Batch 27/64 loss: -0.32642459869384766
Batch 28/64 loss: -0.12720108032226562
Batch 29/64 loss: -0.5267372131347656
Batch 30/64 loss: -0.12978839874267578
Batch 31/64 loss: -0.7655391693115234
Batch 32/64 loss: -0.05619335174560547
Batch 33/64 loss: -0.35167598724365234
Batch 34/64 loss: -0.44928646087646484
Batch 35/64 loss: -0.5424728393554688
Batch 36/64 loss: -0.08931636810302734
Batch 37/64 loss: -0.6492624282836914
Batch 38/64 loss: -0.22769546508789062
Batch 39/64 loss: 0.22366619110107422
Batch 40/64 loss: 0.13880538940429688
Batch 41/64 loss: -0.45412635803222656
Batch 42/64 loss: -0.3034477233886719
Batch 43/64 loss: -0.5732078552246094
Batch 44/64 loss: -0.034702301025390625
Batch 45/64 loss: -0.5502300262451172
Batch 46/64 loss: -0.49146461486816406
Batch 47/64 loss: -0.5857276916503906
Batch 48/64 loss: -0.1848430633544922
Batch 49/64 loss: -0.15208053588867188
Batch 50/64 loss: -0.5464305877685547
Batch 51/64 loss: -0.3597230911254883
Batch 52/64 loss: 0.07930421829223633
Batch 53/64 loss: -0.3541545867919922
Batch 54/64 loss: -0.4329977035522461
Batch 55/64 loss: -0.41385746002197266
Batch 56/64 loss: -0.43717193603515625
Batch 57/64 loss: -0.2805976867675781
Batch 58/64 loss: 0.6442403793334961
Batch 59/64 loss: -0.6524658203125
Batch 60/64 loss: -0.26880455017089844
Batch 61/64 loss: -0.03491783142089844
Batch 62/64 loss: -0.09669017791748047
Batch 63/64 loss: -0.28818511962890625
Batch 64/64 loss: -4.143675327301025
Epoch 305  Train loss: -0.30565743352852615  Val loss: -0.17433562393450655
Epoch 306
-------------------------------
Batch 1/64 loss: -0.015455245971679688
Batch 2/64 loss: -0.1889028549194336
Batch 3/64 loss: -0.38500022888183594
Batch 4/64 loss: -0.0515594482421875
Batch 5/64 loss: -0.11214351654052734
Batch 6/64 loss: -0.3741340637207031
Batch 7/64 loss: -0.2644038200378418
Batch 8/64 loss: -0.09610271453857422
Batch 9/64 loss: 0.18469619750976562
Batch 10/64 loss: -0.586369514465332
Batch 11/64 loss: -0.44717884063720703
Batch 12/64 loss: -0.04160785675048828
Batch 13/64 loss: -0.40486764907836914
Batch 14/64 loss: -0.03514671325683594
Batch 15/64 loss: 3.3923940658569336
Batch 16/64 loss: 0.0286712646484375
Batch 17/64 loss: -0.03999519348144531
Batch 18/64 loss: -0.259124755859375
Batch 19/64 loss: -0.07741165161132812
Batch 20/64 loss: -0.1938486099243164
Batch 21/64 loss: 0.13199329376220703
Batch 22/64 loss: 0.01578044891357422
Batch 23/64 loss: -0.1805105209350586
Batch 24/64 loss: -0.24442100524902344
Batch 25/64 loss: -0.30550098419189453
Batch 26/64 loss: -0.018776893615722656
Batch 27/64 loss: -0.09798145294189453
Batch 28/64 loss: -0.07646369934082031
Batch 29/64 loss: -0.2913360595703125
Batch 30/64 loss: -0.25597715377807617
Batch 31/64 loss: -0.44759559631347656
Batch 32/64 loss: -0.1700754165649414
Batch 33/64 loss: -0.13250160217285156
Batch 34/64 loss: -0.2354297637939453
Batch 35/64 loss: -0.3180508613586426
Batch 36/64 loss: -0.0832672119140625
Batch 37/64 loss: -0.6850032806396484
Batch 38/64 loss: -0.03244209289550781
Batch 39/64 loss: 0.48492908477783203
Batch 40/64 loss: -0.45020389556884766
Batch 41/64 loss: -0.15309715270996094
Batch 42/64 loss: -0.32338714599609375
Batch 43/64 loss: -0.4509315490722656
Batch 44/64 loss: -0.31603431701660156
Batch 45/64 loss: -0.5673103332519531
Batch 46/64 loss: -0.09403514862060547
Batch 47/64 loss: 0.04629802703857422
Batch 48/64 loss: -0.24827098846435547
Batch 49/64 loss: -0.12521648406982422
Batch 50/64 loss: 0.13349437713623047
Batch 51/64 loss: -0.2312335968017578
Batch 52/64 loss: -0.45407581329345703
Batch 53/64 loss: 0.32872438430786133
Batch 54/64 loss: 0.06002998352050781
Batch 55/64 loss: 0.10505294799804688
Batch 56/64 loss: -0.44787025451660156
Batch 57/64 loss: -0.3232002258300781
Batch 58/64 loss: -0.5218696594238281
Batch 59/64 loss: -0.17180490493774414
Batch 60/64 loss: 0.1670069694519043
Batch 61/64 loss: -0.5506792068481445
Batch 62/64 loss: 0.05720663070678711
Batch 63/64 loss: -0.2699728012084961
Batch 64/64 loss: -4.34675407409668
Epoch 306  Train loss: -0.1721030216591031  Val loss: -0.02882027380245248
Epoch 307
-------------------------------
Batch 1/64 loss: 0.06488943099975586
Batch 2/64 loss: 0.0923013687133789
Batch 3/64 loss: -0.22414875030517578
Batch 4/64 loss: -0.07266664505004883
Batch 5/64 loss: 0.40564680099487305
Batch 6/64 loss: -0.5556440353393555
Batch 7/64 loss: -0.13645505905151367
Batch 8/64 loss: -0.38599205017089844
Batch 9/64 loss: -0.31110191345214844
Batch 10/64 loss: -0.20399856567382812
Batch 11/64 loss: -0.2130422592163086
Batch 12/64 loss: -0.2567558288574219
Batch 13/64 loss: -0.14624452590942383
Batch 14/64 loss: -0.5336723327636719
Batch 15/64 loss: -0.3542308807373047
Batch 16/64 loss: -0.6183500289916992
Batch 17/64 loss: -0.17204761505126953
Batch 18/64 loss: -0.0657196044921875
Batch 19/64 loss: 0.27004432678222656
Batch 20/64 loss: -0.24046754837036133
Batch 21/64 loss: -0.33322811126708984
Batch 22/64 loss: -0.07841873168945312
Batch 23/64 loss: -0.21955490112304688
Batch 24/64 loss: -0.45583152770996094
Batch 25/64 loss: -0.4837799072265625
Batch 26/64 loss: -0.005684375762939453
Batch 27/64 loss: -0.4495210647583008
Batch 28/64 loss: 0.045775413513183594
Batch 29/64 loss: -0.11494016647338867
Batch 30/64 loss: -0.6493282318115234
Batch 31/64 loss: -0.22182607650756836
Batch 32/64 loss: 0.08649873733520508
Batch 33/64 loss: 0.03112030029296875
Batch 34/64 loss: -0.2105274200439453
Batch 35/64 loss: 0.09101390838623047
Batch 36/64 loss: -0.5634374618530273
Batch 37/64 loss: -0.13026809692382812
Batch 38/64 loss: -0.2864065170288086
Batch 39/64 loss: -0.3197669982910156
Batch 40/64 loss: -0.23074865341186523
Batch 41/64 loss: -0.5143814086914062
Batch 42/64 loss: -0.009547233581542969
Batch 43/64 loss: -0.29290771484375
Batch 44/64 loss: -0.003303050994873047
Batch 45/64 loss: -0.23412036895751953
Batch 46/64 loss: 2.6526575088500977
Batch 47/64 loss: -0.47475242614746094
Batch 48/64 loss: -0.27713966369628906
Batch 49/64 loss: -0.12408828735351562
Batch 50/64 loss: -0.21433258056640625
Batch 51/64 loss: -0.6262435913085938
Batch 52/64 loss: -0.03540849685668945
Batch 53/64 loss: -0.610814094543457
Batch 54/64 loss: -0.46204662322998047
Batch 55/64 loss: 0.019555091857910156
Batch 56/64 loss: -0.20812129974365234
Batch 57/64 loss: -0.27695465087890625
Batch 58/64 loss: -0.33419132232666016
Batch 59/64 loss: -0.3679351806640625
Batch 60/64 loss: 0.0804910659790039
Batch 61/64 loss: -0.21773910522460938
Batch 62/64 loss: -0.13164520263671875
Batch 63/64 loss: -0.24385929107666016
Batch 64/64 loss: -4.705678462982178
Epoch 307  Train loss: -0.22890356662226657  Val loss: -0.13832914214773276
Epoch 308
-------------------------------
Batch 1/64 loss: -0.14752864837646484
Batch 2/64 loss: 0.023949146270751953
Batch 3/64 loss: -0.2705364227294922
Batch 4/64 loss: -0.02780771255493164
Batch 5/64 loss: -0.6544733047485352
Batch 6/64 loss: -0.35492801666259766
Batch 7/64 loss: -0.602604866027832
Batch 8/64 loss: -0.07000255584716797
Batch 9/64 loss: -0.14336729049682617
Batch 10/64 loss: -0.05947446823120117
Batch 11/64 loss: -0.322451114654541
Batch 12/64 loss: 0.2576313018798828
Batch 13/64 loss: -0.6994924545288086
Batch 14/64 loss: -0.7644014358520508
Batch 15/64 loss: -0.23847675323486328
Batch 16/64 loss: -0.2765922546386719
Batch 17/64 loss: -0.5051956176757812
Batch 18/64 loss: -0.24010753631591797
Batch 19/64 loss: -0.27198314666748047
Batch 20/64 loss: -0.5678730010986328
Batch 21/64 loss: -0.36527061462402344
Batch 22/64 loss: -0.07308721542358398
Batch 23/64 loss: -0.44268321990966797
Batch 24/64 loss: -0.17239856719970703
Batch 25/64 loss: -0.17449283599853516
Batch 26/64 loss: -0.5868711471557617
Batch 27/64 loss: -0.27832794189453125
Batch 28/64 loss: -0.41307640075683594
Batch 29/64 loss: -0.31939125061035156
Batch 30/64 loss: -0.5088996887207031
Batch 31/64 loss: -0.6121482849121094
Batch 32/64 loss: -0.32765769958496094
Batch 33/64 loss: -0.13601255416870117
Batch 34/64 loss: -0.4016685485839844
Batch 35/64 loss: -0.1710214614868164
Batch 36/64 loss: 0.19856643676757812
Batch 37/64 loss: -0.2557048797607422
Batch 38/64 loss: -0.47244739532470703
Batch 39/64 loss: -0.5181818008422852
Batch 40/64 loss: -0.3621406555175781
Batch 41/64 loss: -0.32254981994628906
Batch 42/64 loss: -0.4263334274291992
Batch 43/64 loss: -0.029949188232421875
Batch 44/64 loss: -0.2174835205078125
Batch 45/64 loss: 0.06322193145751953
Batch 46/64 loss: -0.21046066284179688
Batch 47/64 loss: -0.08088302612304688
Batch 48/64 loss: -0.3886747360229492
Batch 49/64 loss: -0.14601898193359375
Batch 50/64 loss: 0.12397289276123047
Batch 51/64 loss: -0.5303106307983398
Batch 52/64 loss: -0.5770483016967773
Batch 53/64 loss: -0.3376493453979492
Batch 54/64 loss: -0.12073040008544922
Batch 55/64 loss: -0.41200828552246094
Batch 56/64 loss: -0.30610179901123047
Batch 57/64 loss: -0.2657489776611328
Batch 58/64 loss: -0.35751914978027344
Batch 59/64 loss: -0.17068862915039062
Batch 60/64 loss: -0.12904644012451172
Batch 61/64 loss: -0.2116856575012207
Batch 62/64 loss: -6.29425048828125e-05
Batch 63/64 loss: -0.25644874572753906
Batch 64/64 loss: -4.5738935470581055
Epoch 308  Train loss: -0.33049819422703164  Val loss: -0.24997172404810325
Saving best model, epoch: 308
Epoch 309
-------------------------------
Batch 1/64 loss: 0.2409205436706543
Batch 2/64 loss: -0.23433208465576172
Batch 3/64 loss: -0.3959541320800781
Batch 4/64 loss: -0.6257905960083008
Batch 5/64 loss: -0.16200637817382812
Batch 6/64 loss: -0.18267154693603516
Batch 7/64 loss: 0.008169174194335938
Batch 8/64 loss: -0.22613906860351562
Batch 9/64 loss: -0.4235372543334961
Batch 10/64 loss: -0.4681682586669922
Batch 11/64 loss: -0.1723337173461914
Batch 12/64 loss: -0.013556480407714844
Batch 13/64 loss: -0.4678354263305664
Batch 14/64 loss: -0.35449790954589844
Batch 15/64 loss: -0.5340423583984375
Batch 16/64 loss: -0.5557565689086914
Batch 17/64 loss: -0.21767807006835938
Batch 18/64 loss: -0.16411876678466797
Batch 19/64 loss: -0.19960403442382812
Batch 20/64 loss: -0.42569732666015625
Batch 21/64 loss: -0.4880800247192383
Batch 22/64 loss: -0.47786426544189453
Batch 23/64 loss: -0.6173210144042969
Batch 24/64 loss: -0.4517536163330078
Batch 25/64 loss: -0.4207897186279297
Batch 26/64 loss: -0.43166637420654297
Batch 27/64 loss: -0.07462024688720703
Batch 28/64 loss: -0.4684934616088867
Batch 29/64 loss: -0.37608814239501953
Batch 30/64 loss: -0.2800569534301758
Batch 31/64 loss: -0.030241012573242188
Batch 32/64 loss: -0.4375944137573242
Batch 33/64 loss: -0.20318317413330078
Batch 34/64 loss: -0.4328784942626953
Batch 35/64 loss: -0.3968210220336914
Batch 36/64 loss: 0.01081085205078125
Batch 37/64 loss: -0.37157535552978516
Batch 38/64 loss: -0.3614492416381836
Batch 39/64 loss: -0.4368715286254883
Batch 40/64 loss: -0.4845905303955078
Batch 41/64 loss: -0.5941247940063477
Batch 42/64 loss: -0.45237159729003906
Batch 43/64 loss: -0.3161592483520508
Batch 44/64 loss: -0.11809730529785156
Batch 45/64 loss: -0.5669918060302734
Batch 46/64 loss: -0.2480020523071289
Batch 47/64 loss: -0.45580291748046875
Batch 48/64 loss: -0.2874574661254883
Batch 49/64 loss: -0.22435760498046875
Batch 50/64 loss: -0.20992660522460938
Batch 51/64 loss: -0.4431734085083008
Batch 52/64 loss: -0.21294260025024414
Batch 53/64 loss: -0.39434814453125
Batch 54/64 loss: -0.2210245132446289
Batch 55/64 loss: -0.0042247772216796875
Batch 56/64 loss: -0.38525390625
Batch 57/64 loss: -0.5459051132202148
Batch 58/64 loss: -0.2883481979370117
Batch 59/64 loss: -0.261627197265625
Batch 60/64 loss: -0.1981964111328125
Batch 61/64 loss: -0.3434295654296875
Batch 62/64 loss: -0.25541019439697266
Batch 63/64 loss: -0.24707603454589844
Batch 64/64 loss: -4.23907470703125
Epoch 309  Train loss: -0.3648520076976103  Val loss: -0.30780126302922306
Saving best model, epoch: 309
Epoch 310
-------------------------------
Batch 1/64 loss: -0.3735618591308594
Batch 2/64 loss: -0.0383296012878418
Batch 3/64 loss: -0.56134033203125
Batch 4/64 loss: -0.03730010986328125
Batch 5/64 loss: -0.4000101089477539
Batch 6/64 loss: -0.192718505859375
Batch 7/64 loss: -0.5214567184448242
Batch 8/64 loss: -0.6077737808227539
Batch 9/64 loss: -0.16613101959228516
Batch 10/64 loss: -0.6482133865356445
Batch 11/64 loss: -0.6272373199462891
Batch 12/64 loss: -0.45659923553466797
Batch 13/64 loss: -0.31403160095214844
Batch 14/64 loss: -0.2752189636230469
Batch 15/64 loss: -0.3419933319091797
Batch 16/64 loss: -0.40242576599121094
Batch 17/64 loss: -0.14983177185058594
Batch 18/64 loss: -0.3494272232055664
Batch 19/64 loss: -0.2829303741455078
Batch 20/64 loss: -0.48987293243408203
Batch 21/64 loss: -0.2868471145629883
Batch 22/64 loss: 0.2285175323486328
Batch 23/64 loss: -0.42568111419677734
Batch 24/64 loss: -0.1813039779663086
Batch 25/64 loss: -0.5196723937988281
Batch 26/64 loss: -0.16970062255859375
Batch 27/64 loss: -0.19887447357177734
Batch 28/64 loss: 0.06168651580810547
Batch 29/64 loss: -0.1166696548461914
Batch 30/64 loss: -0.7100677490234375
Batch 31/64 loss: -0.029970169067382812
Batch 32/64 loss: -0.24954605102539062
Batch 33/64 loss: -0.4306049346923828
Batch 34/64 loss: -0.33965206146240234
Batch 35/64 loss: 0.0819406509399414
Batch 36/64 loss: -0.4820117950439453
Batch 37/64 loss: -0.4080362319946289
Batch 38/64 loss: -0.5956258773803711
Batch 39/64 loss: -0.3462810516357422
Batch 40/64 loss: -0.2227010726928711
Batch 41/64 loss: -0.1987161636352539
Batch 42/64 loss: -0.44071197509765625
Batch 43/64 loss: -0.1631631851196289
Batch 44/64 loss: -0.42287445068359375
Batch 45/64 loss: -0.5529165267944336
Batch 46/64 loss: -0.3200235366821289
Batch 47/64 loss: -0.22368431091308594
Batch 48/64 loss: -0.33623790740966797
Batch 49/64 loss: -0.3723316192626953
Batch 50/64 loss: -0.3536663055419922
Batch 51/64 loss: -0.7002887725830078
Batch 52/64 loss: -0.112823486328125
Batch 53/64 loss: -0.3206777572631836
Batch 54/64 loss: -0.6554965972900391
Batch 55/64 loss: -0.41109275817871094
Batch 56/64 loss: -0.18985509872436523
Batch 57/64 loss: -0.32351207733154297
Batch 58/64 loss: -0.36161231994628906
Batch 59/64 loss: -0.3320808410644531
Batch 60/64 loss: -0.4393587112426758
Batch 61/64 loss: -0.026006698608398438
Batch 62/64 loss: 0.5918169021606445
Batch 63/64 loss: -0.41542816162109375
Batch 64/64 loss: -4.1099853515625
Epoch 310  Train loss: -0.3567174425312117  Val loss: -0.2581842953396827
Epoch 311
-------------------------------
Batch 1/64 loss: -0.654973030090332
Batch 2/64 loss: 0.024559974670410156
Batch 3/64 loss: -0.5267724990844727
Batch 4/64 loss: -0.26085948944091797
Batch 5/64 loss: -0.7112407684326172
Batch 6/64 loss: 0.1844191551208496
Batch 7/64 loss: -0.3233013153076172
Batch 8/64 loss: -0.28131771087646484
Batch 9/64 loss: 0.05946826934814453
Batch 10/64 loss: -0.45137882232666016
Batch 11/64 loss: -0.23820209503173828
Batch 12/64 loss: -0.2660493850708008
Batch 13/64 loss: -0.2820444107055664
Batch 14/64 loss: -0.28528594970703125
Batch 15/64 loss: -0.5833377838134766
Batch 16/64 loss: -0.26995277404785156
Batch 17/64 loss: -0.1611042022705078
Batch 18/64 loss: -0.13582324981689453
Batch 19/64 loss: -0.5798044204711914
Batch 20/64 loss: -0.35733795166015625
Batch 21/64 loss: -0.07375717163085938
Batch 22/64 loss: -0.4168519973754883
Batch 23/64 loss: -0.5158939361572266
Batch 24/64 loss: -0.32970428466796875
Batch 25/64 loss: -0.1816730499267578
Batch 26/64 loss: -0.3473014831542969
Batch 27/64 loss: -0.2474050521850586
Batch 28/64 loss: 0.09632587432861328
Batch 29/64 loss: -0.4442434310913086
Batch 30/64 loss: -0.2584962844848633
Batch 31/64 loss: 0.034661293029785156
Batch 32/64 loss: -0.2855043411254883
Batch 33/64 loss: -0.554682731628418
Batch 34/64 loss: -0.4142436981201172
Batch 35/64 loss: -0.5967473983764648
Batch 36/64 loss: -0.03338003158569336
Batch 37/64 loss: -0.5420198440551758
Batch 38/64 loss: -0.4268980026245117
Batch 39/64 loss: -0.002941131591796875
Batch 40/64 loss: -0.7346429824829102
Batch 41/64 loss: -0.20186710357666016
Batch 42/64 loss: -0.20418977737426758
Batch 43/64 loss: -0.24617958068847656
Batch 44/64 loss: -0.02977275848388672
Batch 45/64 loss: 0.023397445678710938
Batch 46/64 loss: -0.21252822875976562
Batch 47/64 loss: -0.6618976593017578
Batch 48/64 loss: 0.05374908447265625
Batch 49/64 loss: -0.5531940460205078
Batch 50/64 loss: -0.2957763671875
Batch 51/64 loss: -0.4732398986816406
Batch 52/64 loss: -0.5726518630981445
Batch 53/64 loss: -0.5608549118041992
Batch 54/64 loss: -0.45367431640625
Batch 55/64 loss: -0.4206552505493164
Batch 56/64 loss: -0.5368852615356445
Batch 57/64 loss: -0.46800899505615234
Batch 58/64 loss: -0.2436542510986328
Batch 59/64 loss: -0.04170370101928711
Batch 60/64 loss: -0.46279335021972656
Batch 61/64 loss: -0.559443473815918
Batch 62/64 loss: -0.26427459716796875
Batch 63/64 loss: -0.2418813705444336
Batch 64/64 loss: -4.672728538513184
Epoch 311  Train loss: -0.3687570945889342  Val loss: -0.27057292453202186
Epoch 312
-------------------------------
Batch 1/64 loss: 0.008978843688964844
Batch 2/64 loss: -0.7378721237182617
Batch 3/64 loss: -0.5823993682861328
Batch 4/64 loss: -0.16957950592041016
Batch 5/64 loss: 0.0016942024230957031
Batch 6/64 loss: -0.21435832977294922
Batch 7/64 loss: -0.5492591857910156
Batch 8/64 loss: -0.7975606918334961
Batch 9/64 loss: -0.30857372283935547
Batch 10/64 loss: -0.2666177749633789
Batch 11/64 loss: -0.2119150161743164
Batch 12/64 loss: -0.008448600769042969
Batch 13/64 loss: -0.5687236785888672
Batch 14/64 loss: 0.20523738861083984
Batch 15/64 loss: -0.7878580093383789
Batch 16/64 loss: -0.3757171630859375
Batch 17/64 loss: -0.44333553314208984
Batch 18/64 loss: -0.05668973922729492
Batch 19/64 loss: -0.1850872039794922
Batch 20/64 loss: -0.6064863204956055
Batch 21/64 loss: -0.2570610046386719
Batch 22/64 loss: -0.45433998107910156
Batch 23/64 loss: -0.3872966766357422
Batch 24/64 loss: -0.5337991714477539
Batch 25/64 loss: -0.5052251815795898
Batch 26/64 loss: -0.2665090560913086
Batch 27/64 loss: -0.34052467346191406
Batch 28/64 loss: -0.2585296630859375
Batch 29/64 loss: -0.08231163024902344
Batch 30/64 loss: -0.4993782043457031
Batch 31/64 loss: -0.5587863922119141
Batch 32/64 loss: -0.3298015594482422
Batch 33/64 loss: -0.4383993148803711
Batch 34/64 loss: -0.1929473876953125
Batch 35/64 loss: -0.3904733657836914
Batch 36/64 loss: -0.3977212905883789
Batch 37/64 loss: -0.31749629974365234
Batch 38/64 loss: -0.22606277465820312
Batch 39/64 loss: -0.17913150787353516
Batch 40/64 loss: -0.4459714889526367
Batch 41/64 loss: -0.2466754913330078
Batch 42/64 loss: -0.3695974349975586
Batch 43/64 loss: -0.2424325942993164
Batch 44/64 loss: 0.3116025924682617
Batch 45/64 loss: -0.10911083221435547
Batch 46/64 loss: -0.37909698486328125
Batch 47/64 loss: -0.36643028259277344
Batch 48/64 loss: -0.4456357955932617
Batch 49/64 loss: -0.44045352935791016
Batch 50/64 loss: -0.1504039764404297
Batch 51/64 loss: -0.018596172332763672
Batch 52/64 loss: -0.2715339660644531
Batch 53/64 loss: -0.29906749725341797
Batch 54/64 loss: -0.6897668838500977
Batch 55/64 loss: -0.22472000122070312
Batch 56/64 loss: -0.3679637908935547
Batch 57/64 loss: -0.03412818908691406
Batch 58/64 loss: -0.30968761444091797
Batch 59/64 loss: -0.31089305877685547
Batch 60/64 loss: -0.4747018814086914
Batch 61/64 loss: -0.19333648681640625
Batch 62/64 loss: -0.3879575729370117
Batch 63/64 loss: -0.5735244750976562
Batch 64/64 loss: -4.373011589050293
Epoch 312  Train loss: -0.3700424904916801  Val loss: -0.27112852011349603
Epoch 313
-------------------------------
Batch 1/64 loss: -0.2238292694091797
Batch 2/64 loss: -0.5359869003295898
Batch 3/64 loss: -0.17213106155395508
Batch 4/64 loss: -0.1920309066772461
Batch 5/64 loss: 0.05857276916503906
Batch 6/64 loss: -0.5225028991699219
Batch 7/64 loss: -0.2648611068725586
Batch 8/64 loss: -0.8340988159179688
Batch 9/64 loss: -0.598637580871582
Batch 10/64 loss: -0.5193166732788086
Batch 11/64 loss: -0.30611515045166016
Batch 12/64 loss: -0.3311147689819336
Batch 13/64 loss: -0.24767112731933594
Batch 14/64 loss: -0.3907909393310547
Batch 15/64 loss: -0.22089290618896484
Batch 16/64 loss: -0.4798440933227539
Batch 17/64 loss: -0.21185302734375
Batch 18/64 loss: -0.2577505111694336
Batch 19/64 loss: -0.43190479278564453
Batch 20/64 loss: -0.24306392669677734
Batch 21/64 loss: -0.3467063903808594
Batch 22/64 loss: 0.13810396194458008
Batch 23/64 loss: -0.29095935821533203
Batch 24/64 loss: -0.5566720962524414
Batch 25/64 loss: -0.3550834655761719
Batch 26/64 loss: -0.42545127868652344
Batch 27/64 loss: -0.12444210052490234
Batch 28/64 loss: -0.2766761779785156
Batch 29/64 loss: -0.28704261779785156
Batch 30/64 loss: -0.1510772705078125
Batch 31/64 loss: -0.041802406311035156
Batch 32/64 loss: 0.09552192687988281
Batch 33/64 loss: 0.778803825378418
Batch 34/64 loss: -0.5154905319213867
Batch 35/64 loss: 0.408419132232666
Batch 36/64 loss: -0.3459482192993164
Batch 37/64 loss: -0.4507303237915039
Batch 38/64 loss: -0.32112884521484375
Batch 39/64 loss: -0.14123916625976562
Batch 40/64 loss: -0.6461181640625
Batch 41/64 loss: -0.4534273147583008
Batch 42/64 loss: -0.03496837615966797
Batch 43/64 loss: 0.13411808013916016
Batch 44/64 loss: -0.19938087463378906
Batch 45/64 loss: -0.05955934524536133
Batch 46/64 loss: -0.29147911071777344
Batch 47/64 loss: -0.3646669387817383
Batch 48/64 loss: -0.14557170867919922
Batch 49/64 loss: -0.21004152297973633
Batch 50/64 loss: 0.1295633316040039
Batch 51/64 loss: -0.12560367584228516
Batch 52/64 loss: -0.251495361328125
Batch 53/64 loss: -0.6260433197021484
Batch 54/64 loss: -0.4786872863769531
Batch 55/64 loss: -0.5901718139648438
Batch 56/64 loss: 0.006409645080566406
Batch 57/64 loss: -0.2406444549560547
Batch 58/64 loss: 0.09792232513427734
Batch 59/64 loss: -0.49066638946533203
Batch 60/64 loss: -0.4093503952026367
Batch 61/64 loss: -0.45144081115722656
Batch 62/64 loss: -0.2760915756225586
Batch 63/64 loss: -0.531494140625
Batch 64/64 loss: -4.241497039794922
Epoch 313  Train loss: -0.3109872481402229  Val loss: -0.25503807461138855
Epoch 314
-------------------------------
Batch 1/64 loss: -0.09527301788330078
Batch 2/64 loss: -0.3441276550292969
Batch 3/64 loss: -0.49495983123779297
Batch 4/64 loss: -0.2564716339111328
Batch 5/64 loss: -0.1898670196533203
Batch 6/64 loss: -0.6956110000610352
Batch 7/64 loss: -0.6632022857666016
Batch 8/64 loss: -0.5950641632080078
Batch 9/64 loss: -0.2417898178100586
Batch 10/64 loss: -0.11749553680419922
Batch 11/64 loss: -0.08168506622314453
Batch 12/64 loss: -0.5101232528686523
Batch 13/64 loss: -0.4041471481323242
Batch 14/64 loss: -0.7967844009399414
Batch 15/64 loss: -0.5355567932128906
Batch 16/64 loss: 0.15735149383544922
Batch 17/64 loss: -0.013131141662597656
Batch 18/64 loss: -0.19844627380371094
Batch 19/64 loss: -0.22270774841308594
Batch 20/64 loss: -0.566685676574707
Batch 21/64 loss: -0.054337501525878906
Batch 22/64 loss: -0.6142168045043945
Batch 23/64 loss: -0.6245899200439453
Batch 24/64 loss: -0.39308929443359375
Batch 25/64 loss: -0.48790454864501953
Batch 26/64 loss: -0.29842662811279297
Batch 27/64 loss: -0.5491771697998047
Batch 28/64 loss: -0.4420051574707031
Batch 29/64 loss: -0.29329967498779297
Batch 30/64 loss: -0.3064384460449219
Batch 31/64 loss: -0.10580682754516602
Batch 32/64 loss: -0.08680963516235352
Batch 33/64 loss: -0.2834615707397461
Batch 34/64 loss: -0.24764156341552734
Batch 35/64 loss: -0.09655094146728516
Batch 36/64 loss: -0.32017993927001953
Batch 37/64 loss: -0.4852571487426758
Batch 38/64 loss: -0.15987873077392578
Batch 39/64 loss: -0.3886833190917969
Batch 40/64 loss: -0.4609642028808594
Batch 41/64 loss: -0.1420745849609375
Batch 42/64 loss: -0.560511589050293
Batch 43/64 loss: -0.20352649688720703
Batch 44/64 loss: -0.08486366271972656
Batch 45/64 loss: -0.3121204376220703
Batch 46/64 loss: -0.31536197662353516
Batch 47/64 loss: -0.3854827880859375
Batch 48/64 loss: -0.4354104995727539
Batch 49/64 loss: -0.20937061309814453
Batch 50/64 loss: -0.4219474792480469
Batch 51/64 loss: -0.605952262878418
Batch 52/64 loss: -0.6018819808959961
Batch 53/64 loss: -0.489501953125
Batch 54/64 loss: -0.6719865798950195
Batch 55/64 loss: -0.09879016876220703
Batch 56/64 loss: -0.3760557174682617
Batch 57/64 loss: -0.38852977752685547
Batch 58/64 loss: 0.034949302673339844
Batch 59/64 loss: -0.4094390869140625
Batch 60/64 loss: -0.33923816680908203
Batch 61/64 loss: -0.4832735061645508
Batch 62/64 loss: 0.06763076782226562
Batch 63/64 loss: -0.1369638442993164
Batch 64/64 loss: -3.8156161308288574
Epoch 314  Train loss: -0.37640646579218845  Val loss: -0.28975735169505745
Epoch 315
-------------------------------
Batch 1/64 loss: -0.33461666107177734
Batch 2/64 loss: 0.14842605590820312
Batch 3/64 loss: -0.15399599075317383
Batch 4/64 loss: -0.534515380859375
Batch 5/64 loss: -0.5671663284301758
Batch 6/64 loss: -0.5161647796630859
Batch 7/64 loss: 0.19780635833740234
Batch 8/64 loss: -0.30782222747802734
Batch 9/64 loss: -0.3974437713623047
Batch 10/64 loss: -0.47623538970947266
Batch 11/64 loss: -0.4305381774902344
Batch 12/64 loss: -0.4679384231567383
Batch 13/64 loss: -0.07697582244873047
Batch 14/64 loss: -0.4787626266479492
Batch 15/64 loss: -0.5651788711547852
Batch 16/64 loss: -0.5422468185424805
Batch 17/64 loss: -0.20907878875732422
Batch 18/64 loss: -0.2683238983154297
Batch 19/64 loss: -0.5002527236938477
Batch 20/64 loss: -0.06672477722167969
Batch 21/64 loss: -0.16873836517333984
Batch 22/64 loss: -0.5065813064575195
Batch 23/64 loss: -0.14605426788330078
Batch 24/64 loss: -0.4014406204223633
Batch 25/64 loss: -0.5265426635742188
Batch 26/64 loss: -0.01568603515625
Batch 27/64 loss: -0.4512319564819336
Batch 28/64 loss: -0.7183504104614258
Batch 29/64 loss: -0.2526235580444336
Batch 30/64 loss: -0.3945331573486328
Batch 31/64 loss: -0.6627607345581055
Batch 32/64 loss: -0.2696514129638672
Batch 33/64 loss: -0.5476770401000977
Batch 34/64 loss: -0.01073455810546875
Batch 35/64 loss: -0.5967998504638672
Batch 36/64 loss: -0.26426029205322266
Batch 37/64 loss: -0.44109058380126953
Batch 38/64 loss: -0.25161266326904297
Batch 39/64 loss: -0.3053302764892578
Batch 40/64 loss: -0.174224853515625
Batch 41/64 loss: -0.51788330078125
Batch 42/64 loss: -0.02050495147705078
Batch 43/64 loss: -0.6484699249267578
Batch 44/64 loss: -0.2566995620727539
Batch 45/64 loss: -0.5136528015136719
Batch 46/64 loss: -0.4879903793334961
Batch 47/64 loss: -0.20147323608398438
Batch 48/64 loss: -0.5375776290893555
Batch 49/64 loss: -0.031212806701660156
Batch 50/64 loss: -0.011562347412109375
Batch 51/64 loss: -0.1865081787109375
Batch 52/64 loss: -0.3466529846191406
Batch 53/64 loss: -0.4796466827392578
Batch 54/64 loss: -0.5505762100219727
Batch 55/64 loss: -0.4414939880371094
Batch 56/64 loss: -0.2815055847167969
Batch 57/64 loss: -0.4761981964111328
Batch 58/64 loss: 0.1542215347290039
Batch 59/64 loss: -0.24482250213623047
Batch 60/64 loss: -0.04927539825439453
Batch 61/64 loss: -0.4885749816894531
Batch 62/64 loss: -0.5079402923583984
Batch 63/64 loss: -0.3087129592895508
Batch 64/64 loss: -4.446146488189697
Epoch 315  Train loss: -0.3831058333901798  Val loss: -0.3074048491278055
Epoch 316
-------------------------------
Batch 1/64 loss: -0.4645547866821289
Batch 2/64 loss: -0.2527132034301758
Batch 3/64 loss: -0.1666421890258789
Batch 4/64 loss: -0.12755393981933594
Batch 5/64 loss: -0.17987728118896484
Batch 6/64 loss: -0.30472373962402344
Batch 7/64 loss: -0.699070930480957
Batch 8/64 loss: -0.5041007995605469
Batch 9/64 loss: -0.42281341552734375
Batch 10/64 loss: -0.22434329986572266
Batch 11/64 loss: -0.5039768218994141
Batch 12/64 loss: -0.4838438034057617
Batch 13/64 loss: -0.35908031463623047
Batch 14/64 loss: -0.33985137939453125
Batch 15/64 loss: -0.3890409469604492
Batch 16/64 loss: -0.3690910339355469
Batch 17/64 loss: -0.45844364166259766
Batch 18/64 loss: -0.745112419128418
Batch 19/64 loss: -0.17749309539794922
Batch 20/64 loss: -0.2793722152709961
Batch 21/64 loss: -0.4691028594970703
Batch 22/64 loss: -0.6004657745361328
Batch 23/64 loss: -0.2628631591796875
Batch 24/64 loss: 0.10593032836914062
Batch 25/64 loss: 0.014043331146240234
Batch 26/64 loss: -0.3391904830932617
Batch 27/64 loss: -0.07358074188232422
Batch 28/64 loss: -0.48073482513427734
Batch 29/64 loss: -0.4930257797241211
Batch 30/64 loss: -0.4536857604980469
Batch 31/64 loss: -0.2560281753540039
Batch 32/64 loss: -0.27251243591308594
Batch 33/64 loss: -0.1128244400024414
Batch 34/64 loss: -0.319244384765625
Batch 35/64 loss: -0.4002189636230469
Batch 36/64 loss: -0.5690765380859375
Batch 37/64 loss: -0.08495712280273438
Batch 38/64 loss: -0.47637939453125
Batch 39/64 loss: -0.45595741271972656
Batch 40/64 loss: -0.5607433319091797
Batch 41/64 loss: -0.4004049301147461
Batch 42/64 loss: -0.3875236511230469
Batch 43/64 loss: -0.6017093658447266
Batch 44/64 loss: -0.1489696502685547
Batch 45/64 loss: -0.5885448455810547
Batch 46/64 loss: -0.1554269790649414
Batch 47/64 loss: 0.05313396453857422
Batch 48/64 loss: -0.45050811767578125
Batch 49/64 loss: -0.8067665100097656
Batch 50/64 loss: 0.0185546875
Batch 51/64 loss: -0.534541130065918
Batch 52/64 loss: -0.3313179016113281
Batch 53/64 loss: -0.40222835540771484
Batch 54/64 loss: -0.5158119201660156
Batch 55/64 loss: -0.512354850769043
Batch 56/64 loss: -0.4351387023925781
Batch 57/64 loss: -0.26148033142089844
Batch 58/64 loss: -0.29207515716552734
Batch 59/64 loss: -0.7813749313354492
Batch 60/64 loss: -0.35372352600097656
Batch 61/64 loss: -0.20070266723632812
Batch 62/64 loss: -0.06712913513183594
Batch 63/64 loss: -0.4359712600708008
Batch 64/64 loss: -3.8541698455810547
Epoch 316  Train loss: -0.3999209385292203  Val loss: -0.2692389471834058
Epoch 317
-------------------------------
Batch 1/64 loss: -0.37454700469970703
Batch 2/64 loss: -0.35299110412597656
Batch 3/64 loss: -0.4067354202270508
Batch 4/64 loss: -0.3269157409667969
Batch 5/64 loss: -0.49253082275390625
Batch 6/64 loss: -0.6366720199584961
Batch 7/64 loss: -0.2818784713745117
Batch 8/64 loss: -0.56915283203125
Batch 9/64 loss: -0.37976646423339844
Batch 10/64 loss: -0.3376913070678711
Batch 11/64 loss: -0.6938495635986328
Batch 12/64 loss: 0.07602787017822266
Batch 13/64 loss: -0.4133167266845703
Batch 14/64 loss: -0.16327667236328125
Batch 15/64 loss: -0.5193986892700195
Batch 16/64 loss: -0.5951328277587891
Batch 17/64 loss: -0.5951128005981445
Batch 18/64 loss: -0.2639884948730469
Batch 19/64 loss: -0.5129823684692383
Batch 20/64 loss: -0.35750579833984375
Batch 21/64 loss: -0.35068702697753906
Batch 22/64 loss: -0.21860980987548828
Batch 23/64 loss: -0.7879877090454102
Batch 24/64 loss: 0.10776424407958984
Batch 25/64 loss: -0.6458921432495117
Batch 26/64 loss: -0.3696632385253906
Batch 27/64 loss: -0.1823415756225586
Batch 28/64 loss: -0.4420652389526367
Batch 29/64 loss: -0.31548023223876953
Batch 30/64 loss: -0.2873086929321289
Batch 31/64 loss: -0.17618083953857422
Batch 32/64 loss: -0.7091445922851562
Batch 33/64 loss: -0.05019950866699219
Batch 34/64 loss: -0.43276405334472656
Batch 35/64 loss: -0.2513246536254883
Batch 36/64 loss: -0.2630949020385742
Batch 37/64 loss: -0.4666709899902344
Batch 38/64 loss: -0.40567588806152344
Batch 39/64 loss: -0.4158601760864258
Batch 40/64 loss: -0.6014080047607422
Batch 41/64 loss: -0.4559364318847656
Batch 42/64 loss: -0.2288656234741211
Batch 43/64 loss: -0.32465457916259766
Batch 44/64 loss: -0.5901126861572266
Batch 45/64 loss: -0.4458770751953125
Batch 46/64 loss: -0.4372701644897461
Batch 47/64 loss: -0.10724544525146484
Batch 48/64 loss: -0.2508583068847656
Batch 49/64 loss: -0.3249979019165039
Batch 50/64 loss: -0.30660152435302734
Batch 51/64 loss: -0.2243213653564453
Batch 52/64 loss: -0.5489835739135742
Batch 53/64 loss: -0.07171249389648438
Batch 54/64 loss: -0.2989816665649414
Batch 55/64 loss: 0.019829750061035156
Batch 56/64 loss: -0.15125751495361328
Batch 57/64 loss: -0.3824806213378906
Batch 58/64 loss: -0.44752025604248047
Batch 59/64 loss: -0.1245431900024414
Batch 60/64 loss: -0.11655950546264648
Batch 61/64 loss: -0.2050313949584961
Batch 62/64 loss: -0.3739633560180664
Batch 63/64 loss: -0.3240499496459961
Batch 64/64 loss: -4.1889142990112305
Epoch 317  Train loss: -0.3972657708560719  Val loss: -0.3384216282375899
Saving best model, epoch: 317
Epoch 318
-------------------------------
Batch 1/64 loss: 0.0401611328125
Batch 2/64 loss: -0.44539928436279297
Batch 3/64 loss: -0.029464244842529297
Batch 4/64 loss: -0.3931550979614258
Batch 5/64 loss: -0.06949043273925781
Batch 6/64 loss: -0.09682750701904297
Batch 7/64 loss: -0.4654397964477539
Batch 8/64 loss: -0.5889968872070312
Batch 9/64 loss: -0.6621599197387695
Batch 10/64 loss: -0.5003690719604492
Batch 11/64 loss: -0.7539005279541016
Batch 12/64 loss: -0.40100860595703125
Batch 13/64 loss: -0.1168375015258789
Batch 14/64 loss: -0.010679244995117188
Batch 15/64 loss: -0.17592954635620117
Batch 16/64 loss: -0.21464252471923828
Batch 17/64 loss: -0.47205352783203125
Batch 18/64 loss: -0.4478940963745117
Batch 19/64 loss: -0.27027416229248047
Batch 20/64 loss: -0.05074119567871094
Batch 21/64 loss: 0.08681058883666992
Batch 22/64 loss: -0.3877391815185547
Batch 23/64 loss: 0.43146610260009766
Batch 24/64 loss: 0.025757789611816406
Batch 25/64 loss: -0.16046810150146484
Batch 26/64 loss: -0.5822830200195312
Batch 27/64 loss: -0.5529994964599609
Batch 28/64 loss: -0.4945087432861328
Batch 29/64 loss: -0.6880111694335938
Batch 30/64 loss: -0.07966804504394531
Batch 31/64 loss: -0.4613056182861328
Batch 32/64 loss: -0.26285648345947266
Batch 33/64 loss: -0.6067628860473633
Batch 34/64 loss: -0.23219013214111328
Batch 35/64 loss: -0.11589908599853516
Batch 36/64 loss: 0.07339143753051758
Batch 37/64 loss: 0.1359882354736328
Batch 38/64 loss: -0.055713653564453125
Batch 39/64 loss: -0.5763254165649414
Batch 40/64 loss: -0.3881072998046875
Batch 41/64 loss: -0.12428951263427734
Batch 42/64 loss: -0.2861299514770508
Batch 43/64 loss: -0.3827695846557617
Batch 44/64 loss: -0.04664897918701172
Batch 45/64 loss: -0.1791219711303711
Batch 46/64 loss: -0.5085115432739258
Batch 47/64 loss: 0.1657266616821289
Batch 48/64 loss: -0.3811469078063965
Batch 49/64 loss: -0.2869248390197754
Batch 50/64 loss: -0.4863591194152832
Batch 51/64 loss: -0.4702796936035156
Batch 52/64 loss: -0.2362985610961914
Batch 53/64 loss: -0.4012136459350586
Batch 54/64 loss: -0.4708681106567383
Batch 55/64 loss: -0.35718822479248047
Batch 56/64 loss: -0.2688426971435547
Batch 57/64 loss: -0.17034244537353516
Batch 58/64 loss: 0.05702400207519531
Batch 59/64 loss: -0.41672182083129883
Batch 60/64 loss: -0.4830493927001953
Batch 61/64 loss: -0.23661279678344727
Batch 62/64 loss: -0.15956354141235352
Batch 63/64 loss: -0.22141790390014648
Batch 64/64 loss: -4.474667072296143
Epoch 318  Train loss: -0.3250835624395632  Val loss: -0.2157261248716374
Epoch 319
-------------------------------
Batch 1/64 loss: -0.2805194854736328
Batch 2/64 loss: -0.1455821990966797
Batch 3/64 loss: -0.43822669982910156
Batch 4/64 loss: -0.022565364837646484
Batch 5/64 loss: -0.47525882720947266
Batch 6/64 loss: -0.2803840637207031
Batch 7/64 loss: -0.3957042694091797
Batch 8/64 loss: 0.6524701118469238
Batch 9/64 loss: -0.1665945053100586
Batch 10/64 loss: -0.24750995635986328
Batch 11/64 loss: -0.42577552795410156
Batch 12/64 loss: -0.1099095344543457
Batch 13/64 loss: -0.5484037399291992
Batch 14/64 loss: -0.16740083694458008
Batch 15/64 loss: 0.12649154663085938
Batch 16/64 loss: -0.3894615173339844
Batch 17/64 loss: -0.11966562271118164
Batch 18/64 loss: -0.11659955978393555
Batch 19/64 loss: -0.21729469299316406
Batch 20/64 loss: -0.1817793846130371
Batch 21/64 loss: -0.2367258071899414
Batch 22/64 loss: -0.25034332275390625
Batch 23/64 loss: -0.5741729736328125
Batch 24/64 loss: -0.45002079010009766
Batch 25/64 loss: -0.12713241577148438
Batch 26/64 loss: -0.26259708404541016
Batch 27/64 loss: -0.1939992904663086
Batch 28/64 loss: -0.04414176940917969
Batch 29/64 loss: -0.1399707794189453
Batch 30/64 loss: -0.24996185302734375
Batch 31/64 loss: -0.3078336715698242
Batch 32/64 loss: -0.4390449523925781
Batch 33/64 loss: -0.7302703857421875
Batch 34/64 loss: -0.43434810638427734
Batch 35/64 loss: -0.1243429183959961
Batch 36/64 loss: -0.30311107635498047
Batch 37/64 loss: -0.595005989074707
Batch 38/64 loss: -0.23214244842529297
Batch 39/64 loss: -0.42620372772216797
Batch 40/64 loss: -0.43311405181884766
Batch 41/64 loss: -0.47566699981689453
Batch 42/64 loss: -0.5466670989990234
Batch 43/64 loss: -0.41559791564941406
Batch 44/64 loss: -0.47121429443359375
Batch 45/64 loss: -0.3654470443725586
Batch 46/64 loss: -0.2421112060546875
Batch 47/64 loss: -0.6141424179077148
Batch 48/64 loss: -0.5518703460693359
Batch 49/64 loss: -0.20940399169921875
Batch 50/64 loss: -0.24586105346679688
Batch 51/64 loss: -0.24674224853515625
Batch 52/64 loss: -0.2541389465332031
Batch 53/64 loss: -0.5277748107910156
Batch 54/64 loss: -0.5091619491577148
Batch 55/64 loss: -0.46619415283203125
Batch 56/64 loss: -0.2670936584472656
Batch 57/64 loss: -0.13500022888183594
Batch 58/64 loss: -0.47216320037841797
Batch 59/64 loss: -0.0683431625366211
Batch 60/64 loss: -0.21436405181884766
Batch 61/64 loss: -0.6865262985229492
Batch 62/64 loss: -0.29728221893310547
Batch 63/64 loss: -0.5121870040893555
Batch 64/64 loss: -4.160706043243408
Epoch 319  Train loss: -0.3516806490281049  Val loss: -0.3237704116454239
Epoch 320
-------------------------------
Batch 1/64 loss: -0.2936420440673828
Batch 2/64 loss: -0.5884389877319336
Batch 3/64 loss: -0.5821990966796875
Batch 4/64 loss: -0.20257854461669922
Batch 5/64 loss: -0.41614246368408203
Batch 6/64 loss: -0.2375812530517578
Batch 7/64 loss: -0.3775310516357422
Batch 8/64 loss: -0.49257755279541016
Batch 9/64 loss: 0.0016508102416992188
Batch 10/64 loss: -0.3298768997192383
Batch 11/64 loss: -0.3450918197631836
Batch 12/64 loss: -0.6372385025024414
Batch 13/64 loss: -0.3251504898071289
Batch 14/64 loss: -0.02319622039794922
Batch 15/64 loss: -0.20210742950439453
Batch 16/64 loss: -0.2255563735961914
Batch 17/64 loss: -0.6761407852172852
Batch 18/64 loss: -0.1241140365600586
Batch 19/64 loss: -0.5887355804443359
Batch 20/64 loss: -0.40225982666015625
Batch 21/64 loss: -0.35607433319091797
Batch 22/64 loss: -0.5802698135375977
Batch 23/64 loss: -0.5692148208618164
Batch 24/64 loss: -0.4164152145385742
Batch 25/64 loss: -0.7926654815673828
Batch 26/64 loss: -0.5941562652587891
Batch 27/64 loss: -0.7341642379760742
Batch 28/64 loss: -0.42623138427734375
Batch 29/64 loss: 0.10561180114746094
Batch 30/64 loss: -0.13299560546875
Batch 31/64 loss: -0.18299102783203125
Batch 32/64 loss: -0.1289072036743164
Batch 33/64 loss: -0.7441110610961914
Batch 34/64 loss: -0.22770404815673828
Batch 35/64 loss: -0.47208690643310547
Batch 36/64 loss: -0.4098501205444336
Batch 37/64 loss: -0.5855569839477539
Batch 38/64 loss: -0.476715087890625
Batch 39/64 loss: -0.2587757110595703
Batch 40/64 loss: -0.12418556213378906
Batch 41/64 loss: -0.31495189666748047
Batch 42/64 loss: -0.6068687438964844
Batch 43/64 loss: -0.32715320587158203
Batch 44/64 loss: -0.222991943359375
Batch 45/64 loss: -0.3739957809448242
Batch 46/64 loss: -0.19854736328125
Batch 47/64 loss: -0.7863883972167969
Batch 48/64 loss: -0.3114738464355469
Batch 49/64 loss: -0.2511882781982422
Batch 50/64 loss: -0.283236026763916
Batch 51/64 loss: -0.2616109848022461
Batch 52/64 loss: -0.35323143005371094
Batch 53/64 loss: -0.41420841217041016
Batch 54/64 loss: -0.6149015426635742
Batch 55/64 loss: -0.1329641342163086
Batch 56/64 loss: -0.36388397216796875
Batch 57/64 loss: -0.41795825958251953
Batch 58/64 loss: 0.13466882705688477
Batch 59/64 loss: -0.5133514404296875
Batch 60/64 loss: -0.6418542861938477
Batch 61/64 loss: -0.2005777359008789
Batch 62/64 loss: -0.5456256866455078
Batch 63/64 loss: -0.3112812042236328
Batch 64/64 loss: -4.604697227478027
Epoch 320  Train loss: -0.4226363275565353  Val loss: -0.2831315109410237
Epoch 321
-------------------------------
Batch 1/64 loss: -0.5342731475830078
Batch 2/64 loss: -0.2851381301879883
Batch 3/64 loss: -0.7444610595703125
Batch 4/64 loss: -0.06436586380004883
Batch 5/64 loss: -0.5218162536621094
Batch 6/64 loss: -0.5317544937133789
Batch 7/64 loss: -0.3659496307373047
Batch 8/64 loss: -0.2541532516479492
Batch 9/64 loss: -0.45832061767578125
Batch 10/64 loss: -0.4301118850708008
Batch 11/64 loss: -0.29554176330566406
Batch 12/64 loss: -0.43169593811035156
Batch 13/64 loss: -0.2546663284301758
Batch 14/64 loss: -0.6000652313232422
Batch 15/64 loss: -0.16907310485839844
Batch 16/64 loss: -0.4357614517211914
Batch 17/64 loss: -0.16890907287597656
Batch 18/64 loss: -0.5117778778076172
Batch 19/64 loss: 0.28183937072753906
Batch 20/64 loss: -0.4857921600341797
Batch 21/64 loss: -0.3560295104980469
Batch 22/64 loss: -0.4447021484375
Batch 23/64 loss: -0.3416118621826172
Batch 24/64 loss: -0.4086313247680664
Batch 25/64 loss: -0.3459291458129883
Batch 26/64 loss: -0.19830322265625
Batch 27/64 loss: -0.5014839172363281
Batch 28/64 loss: -0.13450908660888672
Batch 29/64 loss: -0.40179920196533203
Batch 30/64 loss: -0.22130489349365234
Batch 31/64 loss: -0.28985023498535156
Batch 32/64 loss: -0.6114797592163086
Batch 33/64 loss: -0.32485103607177734
Batch 34/64 loss: -0.13667774200439453
Batch 35/64 loss: -0.27364349365234375
Batch 36/64 loss: -0.45468711853027344
Batch 37/64 loss: -0.2597665786743164
Batch 38/64 loss: -0.6390800476074219
Batch 39/64 loss: -0.5276422500610352
Batch 40/64 loss: -0.4782733917236328
Batch 41/64 loss: -0.4277677536010742
Batch 42/64 loss: 0.0646367073059082
Batch 43/64 loss: -0.4517803192138672
Batch 44/64 loss: -0.5529012680053711
Batch 45/64 loss: -0.0019197463989257812
Batch 46/64 loss: -0.1258373260498047
Batch 47/64 loss: -0.3788633346557617
Batch 48/64 loss: -0.28682804107666016
Batch 49/64 loss: -0.473419189453125
Batch 50/64 loss: -0.25237560272216797
Batch 51/64 loss: -0.4373159408569336
Batch 52/64 loss: -0.12964916229248047
Batch 53/64 loss: -0.2583951950073242
Batch 54/64 loss: -0.6407709121704102
Batch 55/64 loss: -0.14696788787841797
Batch 56/64 loss: -0.5542831420898438
Batch 57/64 loss: -0.3432035446166992
Batch 58/64 loss: -0.4523658752441406
Batch 59/64 loss: -0.4246854782104492
Batch 60/64 loss: -0.527679443359375
Batch 61/64 loss: -0.5071277618408203
Batch 62/64 loss: 0.048563480377197266
Batch 63/64 loss: -0.056084632873535156
Batch 64/64 loss: -4.076125144958496
Epoch 321  Train loss: -0.39193975037219475  Val loss: -0.36970252597454895
Saving best model, epoch: 321
Epoch 322
-------------------------------
Batch 1/64 loss: -0.30990123748779297
Batch 2/64 loss: -0.4118185043334961
Batch 3/64 loss: -0.25252532958984375
Batch 4/64 loss: -0.16099071502685547
Batch 5/64 loss: -0.17612171173095703
Batch 6/64 loss: -0.021450042724609375
Batch 7/64 loss: -0.5753412246704102
Batch 8/64 loss: -0.20873069763183594
Batch 9/64 loss: -0.322723388671875
Batch 10/64 loss: -0.5789098739624023
Batch 11/64 loss: -0.15884780883789062
Batch 12/64 loss: -0.37218666076660156
Batch 13/64 loss: -0.26047325134277344
Batch 14/64 loss: -0.2502403259277344
Batch 15/64 loss: -0.25704193115234375
Batch 16/64 loss: -0.39736080169677734
Batch 17/64 loss: -0.3151845932006836
Batch 18/64 loss: -0.5144195556640625
Batch 19/64 loss: -0.09496164321899414
Batch 20/64 loss: -0.5970973968505859
Batch 21/64 loss: 0.007563591003417969
Batch 22/64 loss: -0.6647806167602539
Batch 23/64 loss: -0.3194265365600586
Batch 24/64 loss: -0.09801673889160156
Batch 25/64 loss: -0.3494911193847656
Batch 26/64 loss: 0.3533449172973633
Batch 27/64 loss: -0.6601619720458984
Batch 28/64 loss: -0.39298152923583984
Batch 29/64 loss: -0.4522428512573242
Batch 30/64 loss: -0.5741539001464844
Batch 31/64 loss: -0.734980583190918
Batch 32/64 loss: -0.25162410736083984
Batch 33/64 loss: -0.06660747528076172
Batch 34/64 loss: -0.4838218688964844
Batch 35/64 loss: -0.18887901306152344
Batch 36/64 loss: -0.8539180755615234
Batch 37/64 loss: -0.3990507125854492
Batch 38/64 loss: -0.3655118942260742
Batch 39/64 loss: -0.027144432067871094
Batch 40/64 loss: -0.4459505081176758
Batch 41/64 loss: -0.37279701232910156
Batch 42/64 loss: -0.16122055053710938
Batch 43/64 loss: -0.30721282958984375
Batch 44/64 loss: -0.765289306640625
Batch 45/64 loss: -0.5202674865722656
Batch 46/64 loss: -0.5777721405029297
Batch 47/64 loss: -0.5159578323364258
Batch 48/64 loss: -0.13435840606689453
Batch 49/64 loss: -0.40341758728027344
Batch 50/64 loss: -0.37857627868652344
Batch 51/64 loss: -0.3888397216796875
Batch 52/64 loss: -0.5062971115112305
Batch 53/64 loss: -0.31977367401123047
Batch 54/64 loss: -0.5242156982421875
Batch 55/64 loss: -0.06084871292114258
Batch 56/64 loss: -0.35195064544677734
Batch 57/64 loss: -0.5775604248046875
Batch 58/64 loss: -0.3305034637451172
Batch 59/64 loss: -0.47994518280029297
Batch 60/64 loss: -0.3512306213378906
Batch 61/64 loss: -0.6100492477416992
Batch 62/64 loss: -0.27875709533691406
Batch 63/64 loss: -0.2215404510498047
Batch 64/64 loss: -4.5177202224731445
Epoch 322  Train loss: -0.4036209218642291  Val loss: -0.35379856804392185
Epoch 323
-------------------------------
Batch 1/64 loss: -0.4377584457397461
Batch 2/64 loss: -0.44975757598876953
Batch 3/64 loss: -0.6587467193603516
Batch 4/64 loss: -0.5247611999511719
Batch 5/64 loss: -0.27264881134033203
Batch 6/64 loss: -0.5884056091308594
Batch 7/64 loss: -0.41453981399536133
Batch 8/64 loss: -0.5799188613891602
Batch 9/64 loss: -0.3151683807373047
Batch 10/64 loss: -0.3716869354248047
Batch 11/64 loss: 0.07659530639648438
Batch 12/64 loss: -0.6913032531738281
Batch 13/64 loss: -0.2738828659057617
Batch 14/64 loss: -0.26189136505126953
Batch 15/64 loss: -0.3351936340332031
Batch 16/64 loss: -0.23394298553466797
Batch 17/64 loss: -0.21581745147705078
Batch 18/64 loss: -0.4662799835205078
Batch 19/64 loss: -0.30985260009765625
Batch 20/64 loss: -0.30536365509033203
Batch 21/64 loss: -0.41684913635253906
Batch 22/64 loss: -0.037096500396728516
Batch 23/64 loss: -0.29773807525634766
Batch 24/64 loss: -0.49509525299072266
Batch 25/64 loss: -0.2917604446411133
Batch 26/64 loss: -0.3854694366455078
Batch 27/64 loss: -0.4248313903808594
Batch 28/64 loss: -0.13603925704956055
Batch 29/64 loss: -0.5144567489624023
Batch 30/64 loss: -0.4342823028564453
Batch 31/64 loss: -0.5267667770385742
Batch 32/64 loss: -0.6759729385375977
Batch 33/64 loss: -0.41837406158447266
Batch 34/64 loss: 0.2013721466064453
Batch 35/64 loss: -0.715968132019043
Batch 36/64 loss: -0.3405923843383789
Batch 37/64 loss: -0.4694795608520508
Batch 38/64 loss: -0.3933572769165039
Batch 39/64 loss: -0.6355810165405273
Batch 40/64 loss: -0.516850471496582
Batch 41/64 loss: -0.33748626708984375
Batch 42/64 loss: -0.3509397506713867
Batch 43/64 loss: -0.3124122619628906
Batch 44/64 loss: -0.6249332427978516
Batch 45/64 loss: -0.12937545776367188
Batch 46/64 loss: -0.015172004699707031
Batch 47/64 loss: -0.5720882415771484
Batch 48/64 loss: -0.3116273880004883
Batch 49/64 loss: -0.5049352645874023
Batch 50/64 loss: -0.3395652770996094
Batch 51/64 loss: -0.7155275344848633
Batch 52/64 loss: -0.5637388229370117
Batch 53/64 loss: -0.7295570373535156
Batch 54/64 loss: -0.3234415054321289
Batch 55/64 loss: -0.41065216064453125
Batch 56/64 loss: -0.4595603942871094
Batch 57/64 loss: -0.3217582702636719
Batch 58/64 loss: -0.5033092498779297
Batch 59/64 loss: -0.4645233154296875
Batch 60/64 loss: 0.06264305114746094
Batch 61/64 loss: -0.37077999114990234
Batch 62/64 loss: -0.06669235229492188
Batch 63/64 loss: -0.30890369415283203
Batch 64/64 loss: -4.414113998413086
Epoch 323  Train loss: -0.4320068359375  Val loss: -0.377628300198165
Saving best model, epoch: 323
Epoch 324
-------------------------------
Batch 1/64 loss: -0.4599618911743164
Batch 2/64 loss: -0.7063159942626953
Batch 3/64 loss: -0.5550308227539062
Batch 4/64 loss: -0.5698614120483398
Batch 5/64 loss: -0.7043018341064453
Batch 6/64 loss: -0.823756217956543
Batch 7/64 loss: -0.32459163665771484
Batch 8/64 loss: -0.7074460983276367
Batch 9/64 loss: 0.3824930191040039
Batch 10/64 loss: -0.529789924621582
Batch 11/64 loss: -0.3300647735595703
Batch 12/64 loss: -0.32817554473876953
Batch 13/64 loss: -0.3013944625854492
Batch 14/64 loss: 0.22293615341186523
Batch 15/64 loss: -0.43306732177734375
Batch 16/64 loss: -0.4277229309082031
Batch 17/64 loss: -0.5552377700805664
Batch 18/64 loss: -0.40505313873291016
Batch 19/64 loss: -0.2522745132446289
Batch 20/64 loss: 0.06676483154296875
Batch 21/64 loss: 0.25452327728271484
Batch 22/64 loss: -0.3211660385131836
Batch 23/64 loss: -0.6736259460449219
Batch 24/64 loss: -0.5033941268920898
Batch 25/64 loss: -0.66717529296875
Batch 26/64 loss: 0.004855155944824219
Batch 27/64 loss: -0.9272775650024414
Batch 28/64 loss: -0.36820411682128906
Batch 29/64 loss: -0.1512584686279297
Batch 30/64 loss: -0.12557411193847656
Batch 31/64 loss: -0.4167203903198242
Batch 32/64 loss: 0.10369110107421875
Batch 33/64 loss: -0.6864690780639648
Batch 34/64 loss: -0.6403188705444336
Batch 35/64 loss: -0.4775867462158203
Batch 36/64 loss: -0.7860422134399414
Batch 37/64 loss: -0.27181529998779297
Batch 38/64 loss: -0.29189395904541016
Batch 39/64 loss: -0.5954046249389648
Batch 40/64 loss: -0.5441474914550781
Batch 41/64 loss: 3.2599267959594727
Batch 42/64 loss: -0.43695068359375
Batch 43/64 loss: 0.011553287506103516
Batch 44/64 loss: -0.3750791549682617
Batch 45/64 loss: -0.4421243667602539
Batch 46/64 loss: -0.5031404495239258
Batch 47/64 loss: -0.7072515487670898
Batch 48/64 loss: -0.35091209411621094
Batch 49/64 loss: -0.3244285583496094
Batch 50/64 loss: -0.4995603561401367
Batch 51/64 loss: -0.0649251937866211
Batch 52/64 loss: -0.46918678283691406
Batch 53/64 loss: -0.5877342224121094
Batch 54/64 loss: -0.3814563751220703
Batch 55/64 loss: -0.5812654495239258
Batch 56/64 loss: -0.46526432037353516
Batch 57/64 loss: -0.5738039016723633
Batch 58/64 loss: -0.004946231842041016
Batch 59/64 loss: -0.05125999450683594
Batch 60/64 loss: -0.4311952590942383
Batch 61/64 loss: 0.06626319885253906
Batch 62/64 loss: -0.008184432983398438
Batch 63/64 loss: -0.38693809509277344
Batch 64/64 loss: -4.901910305023193
Epoch 324  Train loss: -0.3735082981633205  Val loss: -0.33066819705504324
Epoch 325
-------------------------------
Batch 1/64 loss: -0.6497640609741211
Batch 2/64 loss: -0.4696521759033203
Batch 3/64 loss: -0.6207313537597656
Batch 4/64 loss: -0.515202522277832
Batch 5/64 loss: -0.5414447784423828
Batch 6/64 loss: -0.39383697509765625
Batch 7/64 loss: -0.24203968048095703
Batch 8/64 loss: -0.5257272720336914
Batch 9/64 loss: -0.41245365142822266
Batch 10/64 loss: -0.2567720413208008
Batch 11/64 loss: -0.08432865142822266
Batch 12/64 loss: -0.4460735321044922
Batch 13/64 loss: -0.3022289276123047
Batch 14/64 loss: -0.7134904861450195
Batch 15/64 loss: -0.07590293884277344
Batch 16/64 loss: -0.45942020416259766
Batch 17/64 loss: -0.20759868621826172
Batch 18/64 loss: -0.5648508071899414
Batch 19/64 loss: -0.46428871154785156
Batch 20/64 loss: 0.032871246337890625
Batch 21/64 loss: -0.5035810470581055
Batch 22/64 loss: -0.5832786560058594
Batch 23/64 loss: -0.1488351821899414
Batch 24/64 loss: -0.41907691955566406
Batch 25/64 loss: -0.4185514450073242
Batch 26/64 loss: -0.8786916732788086
Batch 27/64 loss: -0.1645803451538086
Batch 28/64 loss: -0.17066574096679688
Batch 29/64 loss: -0.8317480087280273
Batch 30/64 loss: -0.2319774627685547
Batch 31/64 loss: -0.17542457580566406
Batch 32/64 loss: -0.608393669128418
Batch 33/64 loss: -0.3705778121948242
Batch 34/64 loss: -0.3591880798339844
Batch 35/64 loss: -0.2205667495727539
Batch 36/64 loss: -0.594390869140625
Batch 37/64 loss: -0.7019567489624023
Batch 38/64 loss: -0.13097429275512695
Batch 39/64 loss: -0.1395721435546875
Batch 40/64 loss: -0.38491058349609375
Batch 41/64 loss: -0.22467374801635742
Batch 42/64 loss: 0.050917625427246094
Batch 43/64 loss: -0.14769840240478516
Batch 44/64 loss: -0.3983469009399414
Batch 45/64 loss: -0.6698493957519531
Batch 46/64 loss: -0.13003063201904297
Batch 47/64 loss: -0.3236417770385742
Batch 48/64 loss: -0.675908088684082
Batch 49/64 loss: -0.5916976928710938
Batch 50/64 loss: -0.34174156188964844
Batch 51/64 loss: -0.25453758239746094
Batch 52/64 loss: -0.15660858154296875
Batch 53/64 loss: -0.5316495895385742
Batch 54/64 loss: -0.06193828582763672
Batch 55/64 loss: -0.29172420501708984
Batch 56/64 loss: -0.3464069366455078
Batch 57/64 loss: -0.17483234405517578
Batch 58/64 loss: -0.4253578186035156
Batch 59/64 loss: -0.21301746368408203
Batch 60/64 loss: -0.46454429626464844
Batch 61/64 loss: -0.8040637969970703
Batch 62/64 loss: -0.38431262969970703
Batch 63/64 loss: -0.505040168762207
Batch 64/64 loss: -4.834345817565918
Epoch 325  Train loss: -0.4336053623872645  Val loss: -0.3605264748904304
Epoch 326
-------------------------------
Batch 1/64 loss: -0.06249046325683594
Batch 2/64 loss: -0.41118621826171875
Batch 3/64 loss: -0.4465522766113281
Batch 4/64 loss: -0.3796882629394531
Batch 5/64 loss: -0.03421306610107422
Batch 6/64 loss: -0.0839395523071289
Batch 7/64 loss: -0.3412132263183594
Batch 8/64 loss: -0.5002222061157227
Batch 9/64 loss: -0.3420257568359375
Batch 10/64 loss: -0.43757057189941406
Batch 11/64 loss: -0.13129568099975586
Batch 12/64 loss: 0.1695394515991211
Batch 13/64 loss: -0.3095889091491699
Batch 14/64 loss: -0.6657695770263672
Batch 15/64 loss: -0.1849370002746582
Batch 16/64 loss: -0.39191436767578125
Batch 17/64 loss: -0.5029630661010742
Batch 18/64 loss: -0.3933887481689453
Batch 19/64 loss: -0.2713003158569336
Batch 20/64 loss: -0.6104698181152344
Batch 21/64 loss: -0.09031295776367188
Batch 22/64 loss: -0.5555257797241211
Batch 23/64 loss: -0.6249227523803711
Batch 24/64 loss: -0.4073143005371094
Batch 25/64 loss: -0.6567173004150391
Batch 26/64 loss: -0.27199363708496094
Batch 27/64 loss: -0.4799537658691406
Batch 28/64 loss: -0.3837704658508301
Batch 29/64 loss: -0.7420940399169922
Batch 30/64 loss: -0.06083393096923828
Batch 31/64 loss: -0.4897317886352539
Batch 32/64 loss: -0.5682220458984375
Batch 33/64 loss: -0.37514781951904297
Batch 34/64 loss: -0.6181612014770508
Batch 35/64 loss: -0.5065717697143555
Batch 36/64 loss: -0.5884685516357422
Batch 37/64 loss: -0.7286462783813477
Batch 38/64 loss: -0.4102926254272461
Batch 39/64 loss: -0.38451671600341797
Batch 40/64 loss: -0.4744291305541992
Batch 41/64 loss: -0.5622091293334961
Batch 42/64 loss: -0.23674869537353516
Batch 43/64 loss: -0.5438766479492188
Batch 44/64 loss: -0.2580595016479492
Batch 45/64 loss: -0.27954769134521484
Batch 46/64 loss: -0.2583885192871094
Batch 47/64 loss: -0.2742156982421875
Batch 48/64 loss: -0.5645360946655273
Batch 49/64 loss: -0.2788543701171875
Batch 50/64 loss: -0.13907146453857422
Batch 51/64 loss: -0.6305675506591797
Batch 52/64 loss: -0.02756977081298828
Batch 53/64 loss: -0.3376007080078125
Batch 54/64 loss: -0.6186151504516602
Batch 55/64 loss: -0.3685131072998047
Batch 56/64 loss: -0.6338119506835938
Batch 57/64 loss: -0.3957996368408203
Batch 58/64 loss: -0.4130058288574219
Batch 59/64 loss: -0.7093620300292969
Batch 60/64 loss: 0.019481658935546875
Batch 61/64 loss: -0.5261440277099609
Batch 62/64 loss: -0.6763362884521484
Batch 63/64 loss: -0.3449583053588867
Batch 64/64 loss: -4.464931011199951
Epoch 326  Train loss: -0.4416600040360993  Val loss: -0.339247582294687
Epoch 327
-------------------------------
Batch 1/64 loss: -0.17418193817138672
Batch 2/64 loss: -0.31142330169677734
Batch 3/64 loss: -0.37491893768310547
Batch 4/64 loss: -0.5298728942871094
Batch 5/64 loss: -0.4150581359863281
Batch 6/64 loss: -0.26406097412109375
Batch 7/64 loss: -0.15198993682861328
Batch 8/64 loss: -0.5315408706665039
Batch 9/64 loss: -0.19975662231445312
Batch 10/64 loss: -0.5561971664428711
Batch 11/64 loss: -0.39536094665527344
Batch 12/64 loss: -0.1783275604248047
Batch 13/64 loss: -0.47279834747314453
Batch 14/64 loss: -0.035910606384277344
Batch 15/64 loss: -0.5304794311523438
Batch 16/64 loss: -0.598963737487793
Batch 17/64 loss: -0.2708559036254883
Batch 18/64 loss: -0.118133544921875
Batch 19/64 loss: -0.5808582305908203
Batch 20/64 loss: -0.1696014404296875
Batch 21/64 loss: -0.5779438018798828
Batch 22/64 loss: -0.17534923553466797
Batch 23/64 loss: -0.6389446258544922
Batch 24/64 loss: -0.551300048828125
Batch 25/64 loss: -0.834345817565918
Batch 26/64 loss: -0.30431270599365234
Batch 27/64 loss: -0.42777538299560547
Batch 28/64 loss: -0.14653587341308594
Batch 29/64 loss: -0.24375629425048828
Batch 30/64 loss: -0.4841909408569336
Batch 31/64 loss: -0.2618870735168457
Batch 32/64 loss: -0.4591999053955078
Batch 33/64 loss: -0.7447052001953125
Batch 34/64 loss: -0.6173210144042969
Batch 35/64 loss: -0.513702392578125
Batch 36/64 loss: -0.5730094909667969
Batch 37/64 loss: -0.6272602081298828
Batch 38/64 loss: -0.5864124298095703
Batch 39/64 loss: -0.38294410705566406
Batch 40/64 loss: -0.6845502853393555
Batch 41/64 loss: -0.912785530090332
Batch 42/64 loss: -0.5816020965576172
Batch 43/64 loss: -0.523005485534668
Batch 44/64 loss: -0.4037456512451172
Batch 45/64 loss: -0.47666358947753906
Batch 46/64 loss: -0.700098991394043
Batch 47/64 loss: -0.17922496795654297
Batch 48/64 loss: -0.41355228424072266
Batch 49/64 loss: -0.49182796478271484
Batch 50/64 loss: -0.3172788619995117
Batch 51/64 loss: -0.3328123092651367
Batch 52/64 loss: -0.37915611267089844
Batch 53/64 loss: -0.31221771240234375
Batch 54/64 loss: -0.5346078872680664
Batch 55/64 loss: -0.4266633987426758
Batch 56/64 loss: -0.4336671829223633
Batch 57/64 loss: -0.3749208450317383
Batch 58/64 loss: -0.592921257019043
Batch 59/64 loss: -0.7444572448730469
Batch 60/64 loss: -0.4653959274291992
Batch 61/64 loss: -0.6745376586914062
Batch 62/64 loss: -0.5422468185424805
Batch 63/64 loss: -0.4899454116821289
Batch 64/64 loss: -4.733985900878906
Epoch 327  Train loss: -0.49489508984135644  Val loss: -0.3937532877184681
Saving best model, epoch: 327
Epoch 328
-------------------------------
Batch 1/64 loss: -0.2995424270629883
Batch 2/64 loss: -0.48767662048339844
Batch 3/64 loss: -0.7061386108398438
Batch 4/64 loss: -0.7228984832763672
Batch 5/64 loss: 0.01045989990234375
Batch 6/64 loss: -0.9139423370361328
Batch 7/64 loss: -0.3995990753173828
Batch 8/64 loss: -0.7277870178222656
Batch 9/64 loss: -0.6393041610717773
Batch 10/64 loss: -0.3017902374267578
Batch 11/64 loss: -0.5880365371704102
Batch 12/64 loss: -0.46507740020751953
Batch 13/64 loss: -0.8527154922485352
Batch 14/64 loss: -0.42739105224609375
Batch 15/64 loss: -0.6460704803466797
Batch 16/64 loss: -0.5352411270141602
Batch 17/64 loss: -0.7905550003051758
Batch 18/64 loss: -0.4569272994995117
Batch 19/64 loss: 0.13133621215820312
Batch 20/64 loss: -0.2581195831298828
Batch 21/64 loss: -0.29340648651123047
Batch 22/64 loss: -0.33370113372802734
Batch 23/64 loss: -0.6042966842651367
Batch 24/64 loss: -0.29174232482910156
Batch 25/64 loss: -0.03975105285644531
Batch 26/64 loss: -0.5419406890869141
Batch 27/64 loss: -0.6004905700683594
Batch 28/64 loss: -0.1342630386352539
Batch 29/64 loss: -0.0029659271240234375
Batch 30/64 loss: -0.11358642578125
Batch 31/64 loss: -0.42359352111816406
Batch 32/64 loss: -0.6168403625488281
Batch 33/64 loss: -0.7675876617431641
Batch 34/64 loss: -0.005329132080078125
Batch 35/64 loss: -0.7387962341308594
Batch 36/64 loss: -0.2501201629638672
Batch 37/64 loss: -0.5793046951293945
Batch 38/64 loss: -0.5141592025756836
Batch 39/64 loss: -0.4496164321899414
Batch 40/64 loss: -0.7362480163574219
Batch 41/64 loss: -0.4551229476928711
Batch 42/64 loss: -0.47048377990722656
Batch 43/64 loss: -0.6142921447753906
Batch 44/64 loss: -0.2026348114013672
Batch 45/64 loss: -0.7339630126953125
Batch 46/64 loss: -0.2527914047241211
Batch 47/64 loss: -0.1874103546142578
Batch 48/64 loss: 0.07001113891601562
Batch 49/64 loss: -0.4474906921386719
Batch 50/64 loss: -0.3468132019042969
Batch 51/64 loss: -0.3862161636352539
Batch 52/64 loss: -0.6653690338134766
Batch 53/64 loss: -0.2830209732055664
Batch 54/64 loss: -0.4312162399291992
Batch 55/64 loss: -0.7027273178100586
Batch 56/64 loss: -0.6949567794799805
Batch 57/64 loss: 0.027060508728027344
Batch 58/64 loss: -0.48673343658447266
Batch 59/64 loss: -0.5827064514160156
Batch 60/64 loss: -0.5759706497192383
Batch 61/64 loss: -0.4334545135498047
Batch 62/64 loss: -0.6092033386230469
Batch 63/64 loss: -0.7283134460449219
Batch 64/64 loss: -4.166755199432373
Epoch 328  Train loss: -0.4930767377217611  Val loss: -0.4278728314691393
Saving best model, epoch: 328
Epoch 329
-------------------------------
Batch 1/64 loss: -0.3270406723022461
Batch 2/64 loss: -0.5459280014038086
Batch 3/64 loss: -0.5511789321899414
Batch 4/64 loss: -0.2365255355834961
Batch 5/64 loss: -0.4843292236328125
Batch 6/64 loss: -0.4053058624267578
Batch 7/64 loss: -0.12307882308959961
Batch 8/64 loss: -0.564082145690918
Batch 9/64 loss: -0.2272510528564453
Batch 10/64 loss: -0.21547603607177734
Batch 11/64 loss: -0.8218917846679688
Batch 12/64 loss: -0.6121120452880859
Batch 13/64 loss: -0.12140846252441406
Batch 14/64 loss: -0.4518413543701172
Batch 15/64 loss: -0.9437026977539062
Batch 16/64 loss: -0.5273532867431641
Batch 17/64 loss: -0.7130756378173828
Batch 18/64 loss: -0.5389556884765625
Batch 19/64 loss: -0.18717670440673828
Batch 20/64 loss: -0.31612586975097656
Batch 21/64 loss: -0.4111976623535156
Batch 22/64 loss: -0.47405433654785156
Batch 23/64 loss: -0.1873016357421875
Batch 24/64 loss: -0.7793521881103516
Batch 25/64 loss: -0.5912895202636719
Batch 26/64 loss: -0.6118917465209961
Batch 27/64 loss: -0.8205242156982422
Batch 28/64 loss: -0.6184930801391602
Batch 29/64 loss: -0.5421590805053711
Batch 30/64 loss: -0.4795360565185547
Batch 31/64 loss: -0.6619892120361328
Batch 32/64 loss: -0.5896062850952148
Batch 33/64 loss: -0.3501605987548828
Batch 34/64 loss: -0.48027992248535156
Batch 35/64 loss: -0.8717002868652344
Batch 36/64 loss: -0.16277647018432617
Batch 37/64 loss: -0.5745935440063477
Batch 38/64 loss: -0.39096832275390625
Batch 39/64 loss: -0.38869380950927734
Batch 40/64 loss: -0.8758153915405273
Batch 41/64 loss: -0.773045539855957
Batch 42/64 loss: -0.5183134078979492
Batch 43/64 loss: -0.025964736938476562
Batch 44/64 loss: -0.2043018341064453
Batch 45/64 loss: -0.3616933822631836
Batch 46/64 loss: -0.7355985641479492
Batch 47/64 loss: -0.542475700378418
Batch 48/64 loss: -0.7265090942382812
Batch 49/64 loss: -0.28455638885498047
Batch 50/64 loss: -0.5277738571166992
Batch 51/64 loss: -0.12234878540039062
Batch 52/64 loss: -0.32203102111816406
Batch 53/64 loss: -0.4314889907836914
Batch 54/64 loss: -0.09791851043701172
Batch 55/64 loss: 0.1638789176940918
Batch 56/64 loss: -0.26585960388183594
Batch 57/64 loss: -0.20494556427001953
Batch 58/64 loss: -0.5864276885986328
Batch 59/64 loss: -0.6709995269775391
Batch 60/64 loss: -0.031246185302734375
Batch 61/64 loss: -0.4075937271118164
Batch 62/64 loss: -0.5450248718261719
Batch 63/64 loss: -0.4692411422729492
Batch 64/64 loss: -4.573637962341309
Epoch 329  Train loss: -0.5003596997728534  Val loss: -0.42292353049995973
Epoch 330
-------------------------------
Batch 1/64 loss: -0.36727333068847656
Batch 2/64 loss: -0.7224521636962891
Batch 3/64 loss: -0.45078372955322266
Batch 4/64 loss: -0.5025529861450195
Batch 5/64 loss: -0.527949333190918
Batch 6/64 loss: -0.33690452575683594
Batch 7/64 loss: -0.5713138580322266
Batch 8/64 loss: 0.1424245834350586
Batch 9/64 loss: -0.4508495330810547
Batch 10/64 loss: -0.2786235809326172
Batch 11/64 loss: -0.3669404983520508
Batch 12/64 loss: -0.6583633422851562
Batch 13/64 loss: -0.5494327545166016
Batch 14/64 loss: -0.3032951354980469
Batch 15/64 loss: -0.24158096313476562
Batch 16/64 loss: -0.3050060272216797
Batch 17/64 loss: -0.44606781005859375
Batch 18/64 loss: -0.6361370086669922
Batch 19/64 loss: -0.36543846130371094
Batch 20/64 loss: -0.7816705703735352
Batch 21/64 loss: -0.2571430206298828
Batch 22/64 loss: -0.1042928695678711
Batch 23/64 loss: -0.5362386703491211
Batch 24/64 loss: -0.48029613494873047
Batch 25/64 loss: -0.5943117141723633
Batch 26/64 loss: -0.3825411796569824
Batch 27/64 loss: -0.33776187896728516
Batch 28/64 loss: 0.0901021957397461
Batch 29/64 loss: -0.5481348037719727
Batch 30/64 loss: -0.08710718154907227
Batch 31/64 loss: -0.3343219757080078
Batch 32/64 loss: -0.13647031784057617
Batch 33/64 loss: -0.4569978713989258
Batch 34/64 loss: -0.06736278533935547
Batch 35/64 loss: -0.6000213623046875
Batch 36/64 loss: -0.2183837890625
Batch 37/64 loss: -0.1241750717163086
Batch 38/64 loss: 0.09527969360351562
Batch 39/64 loss: -0.2623882293701172
Batch 40/64 loss: -0.2420344352722168
Batch 41/64 loss: -0.45455360412597656
Batch 42/64 loss: -0.2521829605102539
Batch 43/64 loss: -0.2951078414916992
Batch 44/64 loss: -0.6172380447387695
Batch 45/64 loss: -0.2312021255493164
Batch 46/64 loss: -0.4103717803955078
Batch 47/64 loss: -0.3712139129638672
Batch 48/64 loss: -0.3520832061767578
Batch 49/64 loss: -0.5474557876586914
Batch 50/64 loss: -0.44957923889160156
Batch 51/64 loss: -0.6379575729370117
Batch 52/64 loss: -0.280062198638916
Batch 53/64 loss: -0.6690778732299805
Batch 54/64 loss: -0.49028682708740234
Batch 55/64 loss: -0.7645645141601562
Batch 56/64 loss: -0.3057708740234375
Batch 57/64 loss: -0.3384227752685547
Batch 58/64 loss: -0.4180727005004883
Batch 59/64 loss: -0.5830564498901367
Batch 60/64 loss: -0.4781942367553711
Batch 61/64 loss: -0.6082363128662109
Batch 62/64 loss: -0.3573312759399414
Batch 63/64 loss: -0.6303310394287109
Batch 64/64 loss: -4.17425537109375
Epoch 330  Train loss: -0.4388683468687768  Val loss: -0.4058316679754618
Epoch 331
-------------------------------
Batch 1/64 loss: -0.2561607360839844
Batch 2/64 loss: -0.4744272232055664
Batch 3/64 loss: -0.2190098762512207
Batch 4/64 loss: -0.3916969299316406
Batch 5/64 loss: -0.5991659164428711
Batch 6/64 loss: -0.528071403503418
Batch 7/64 loss: -0.49190235137939453
Batch 8/64 loss: -0.2621145248413086
Batch 9/64 loss: -0.1002964973449707
Batch 10/64 loss: -0.4166297912597656
Batch 11/64 loss: -0.5488271713256836
Batch 12/64 loss: -0.5847663879394531
Batch 13/64 loss: -0.5942449569702148
Batch 14/64 loss: -0.37589168548583984
Batch 15/64 loss: -0.163909912109375
Batch 16/64 loss: -0.5655422210693359
Batch 17/64 loss: -0.5836114883422852
Batch 18/64 loss: -0.10564899444580078
Batch 19/64 loss: -0.6080942153930664
Batch 20/64 loss: -0.3950824737548828
Batch 21/64 loss: -0.20314311981201172
Batch 22/64 loss: -0.40532588958740234
Batch 23/64 loss: -0.1184377670288086
Batch 24/64 loss: -0.6441917419433594
Batch 25/64 loss: -0.8185720443725586
Batch 26/64 loss: -0.4059591293334961
Batch 27/64 loss: -0.34569263458251953
Batch 28/64 loss: -0.7605838775634766
Batch 29/64 loss: -0.5677394866943359
Batch 30/64 loss: -0.28390979766845703
Batch 31/64 loss: -0.4984703063964844
Batch 32/64 loss: -0.5148124694824219
Batch 33/64 loss: -0.6970090866088867
Batch 34/64 loss: -0.43019676208496094
Batch 35/64 loss: -0.7306623458862305
Batch 36/64 loss: -0.22294902801513672
Batch 37/64 loss: -0.6457834243774414
Batch 38/64 loss: -0.5723896026611328
Batch 39/64 loss: -0.23589420318603516
Batch 40/64 loss: -0.46555233001708984
Batch 41/64 loss: -0.6310110092163086
Batch 42/64 loss: -0.3182210922241211
Batch 43/64 loss: -0.1412353515625
Batch 44/64 loss: -0.7500076293945312
Batch 45/64 loss: -0.7776956558227539
Batch 46/64 loss: -1.0003490447998047
Batch 47/64 loss: -0.28110790252685547
Batch 48/64 loss: -0.35022449493408203
Batch 49/64 loss: -0.8267965316772461
Batch 50/64 loss: -0.3712644577026367
Batch 51/64 loss: -0.48671436309814453
Batch 52/64 loss: -0.3292274475097656
Batch 53/64 loss: -0.640477180480957
Batch 54/64 loss: -0.6589117050170898
Batch 55/64 loss: -0.6152591705322266
Batch 56/64 loss: -0.5728788375854492
Batch 57/64 loss: -0.1923975944519043
Batch 58/64 loss: -0.44990062713623047
Batch 59/64 loss: -0.5786628723144531
Batch 60/64 loss: -0.26602745056152344
Batch 61/64 loss: -0.7149257659912109
Batch 62/64 loss: -0.4382896423339844
Batch 63/64 loss: -0.7115726470947266
Batch 64/64 loss: -4.14731502532959
Epoch 331  Train loss: -0.5183688556446748  Val loss: -0.49041390173214
Saving best model, epoch: 331
Epoch 332
-------------------------------
Batch 1/64 loss: -0.6245527267456055
Batch 2/64 loss: -0.15781497955322266
Batch 3/64 loss: -0.6890583038330078
Batch 4/64 loss: 0.02850961685180664
Batch 5/64 loss: -0.39949512481689453
Batch 6/64 loss: -0.43883609771728516
Batch 7/64 loss: -0.5758609771728516
Batch 8/64 loss: -0.0896158218383789
Batch 9/64 loss: -0.5013294219970703
Batch 10/64 loss: -0.4174947738647461
Batch 11/64 loss: -0.563929557800293
Batch 12/64 loss: -0.4701671600341797
Batch 13/64 loss: 0.13507461547851562
Batch 14/64 loss: -0.8545961380004883
Batch 15/64 loss: -0.46501827239990234
Batch 16/64 loss: -0.6306877136230469
Batch 17/64 loss: -0.8221035003662109
Batch 18/64 loss: -0.6978940963745117
Batch 19/64 loss: -0.5493221282958984
Batch 20/64 loss: -0.484161376953125
Batch 21/64 loss: -0.8174352645874023
Batch 22/64 loss: -0.6294002532958984
Batch 23/64 loss: -0.61322021484375
Batch 24/64 loss: -0.4255352020263672
Batch 25/64 loss: -0.7323684692382812
Batch 26/64 loss: -0.6453380584716797
Batch 27/64 loss: 0.5126328468322754
Batch 28/64 loss: -0.34382057189941406
Batch 29/64 loss: -0.5609197616577148
Batch 30/64 loss: -0.33692264556884766
Batch 31/64 loss: -0.7595481872558594
Batch 32/64 loss: -0.6204090118408203
Batch 33/64 loss: -0.32787418365478516
Batch 34/64 loss: -0.1829838752746582
Batch 35/64 loss: -0.8752908706665039
Batch 36/64 loss: -0.3629932403564453
Batch 37/64 loss: -0.3805818557739258
Batch 38/64 loss: -0.17436504364013672
Batch 39/64 loss: -0.5892782211303711
Batch 40/64 loss: -0.31676197052001953
Batch 41/64 loss: -0.8066682815551758
Batch 42/64 loss: -0.7228460311889648
Batch 43/64 loss: -0.6854667663574219
Batch 44/64 loss: 0.5363349914550781
Batch 45/64 loss: -0.3534870147705078
Batch 46/64 loss: -0.2549610137939453
Batch 47/64 loss: -0.6747283935546875
Batch 48/64 loss: -0.38327789306640625
Batch 49/64 loss: -0.4525327682495117
Batch 50/64 loss: -0.2379474639892578
Batch 51/64 loss: -0.5993986129760742
Batch 52/64 loss: -0.39780616760253906
Batch 53/64 loss: -0.4384918212890625
Batch 54/64 loss: -0.7653799057006836
Batch 55/64 loss: -0.6648435592651367
Batch 56/64 loss: -0.725712776184082
Batch 57/64 loss: -0.6598949432373047
Batch 58/64 loss: -0.40236759185791016
Batch 59/64 loss: -0.7371330261230469
Batch 60/64 loss: -0.2541980743408203
Batch 61/64 loss: -0.6171550750732422
Batch 62/64 loss: -0.5586137771606445
Batch 63/64 loss: -0.3165435791015625
Batch 64/64 loss: -4.378915786743164
Epoch 332  Train loss: -0.5162050882975261  Val loss: -0.338983489885363
Epoch 333
-------------------------------
Batch 1/64 loss: -0.26737403869628906
Batch 2/64 loss: -0.25833702087402344
Batch 3/64 loss: -0.5662870407104492
Batch 4/64 loss: -0.6777582168579102
Batch 5/64 loss: -0.3716745376586914
Batch 6/64 loss: -0.2585620880126953
Batch 7/64 loss: -0.4640235900878906
Batch 8/64 loss: -0.35448265075683594
Batch 9/64 loss: -0.4412574768066406
Batch 10/64 loss: -0.6079273223876953
Batch 11/64 loss: -0.36333751678466797
Batch 12/64 loss: -0.3417816162109375
Batch 13/64 loss: -0.6051006317138672
Batch 14/64 loss: -0.8332195281982422
Batch 15/64 loss: -0.565826416015625
Batch 16/64 loss: -0.5019016265869141
Batch 17/64 loss: -0.2142486572265625
Batch 18/64 loss: -0.4752969741821289
Batch 19/64 loss: -0.7329835891723633
Batch 20/64 loss: -0.2086796760559082
Batch 21/64 loss: -0.5383663177490234
Batch 22/64 loss: -0.3837471008300781
Batch 23/64 loss: -0.3835430145263672
Batch 24/64 loss: -0.5916814804077148
Batch 25/64 loss: -0.45499706268310547
Batch 26/64 loss: -0.08580970764160156
Batch 27/64 loss: -0.5685548782348633
Batch 28/64 loss: -0.46770668029785156
Batch 29/64 loss: -0.8165044784545898
Batch 30/64 loss: -0.774449348449707
Batch 31/64 loss: -0.3934602737426758
Batch 32/64 loss: -0.7997093200683594
Batch 33/64 loss: -0.4856739044189453
Batch 34/64 loss: -0.6866416931152344
Batch 35/64 loss: -0.431365966796875
Batch 36/64 loss: -0.0649423599243164
Batch 37/64 loss: -0.34214305877685547
Batch 38/64 loss: -0.5727462768554688
Batch 39/64 loss: -0.3961496353149414
Batch 40/64 loss: -0.3200407028198242
Batch 41/64 loss: -0.395721435546875
Batch 42/64 loss: -0.5232534408569336
Batch 43/64 loss: -0.6352853775024414
Batch 44/64 loss: -0.8252439498901367
Batch 45/64 loss: -0.4391164779663086
Batch 46/64 loss: -0.44934558868408203
Batch 47/64 loss: -0.6078395843505859
Batch 48/64 loss: -0.5451936721801758
Batch 49/64 loss: -0.8621978759765625
Batch 50/64 loss: -0.5586309432983398
Batch 51/64 loss: -0.23813199996948242
Batch 52/64 loss: -0.6892814636230469
Batch 53/64 loss: -0.7126893997192383
Batch 54/64 loss: -0.3431253433227539
Batch 55/64 loss: -0.08309459686279297
Batch 56/64 loss: -0.21926355361938477
Batch 57/64 loss: -0.5026750564575195
Batch 58/64 loss: -0.4462919235229492
Batch 59/64 loss: -0.9077062606811523
Batch 60/64 loss: -0.496429443359375
Batch 61/64 loss: -0.6013126373291016
Batch 62/64 loss: -0.3845806121826172
Batch 63/64 loss: -0.4936037063598633
Batch 64/64 loss: -4.3558349609375
Epoch 333  Train loss: -0.5316891614128562  Val loss: -0.37963311369066793
Epoch 334
-------------------------------
Batch 1/64 loss: -0.5230894088745117
Batch 2/64 loss: -0.26783084869384766
Batch 3/64 loss: -0.6442489624023438
Batch 4/64 loss: -0.430755615234375
Batch 5/64 loss: -0.7202844619750977
Batch 6/64 loss: -0.6107645034790039
Batch 7/64 loss: -0.7350597381591797
Batch 8/64 loss: -0.5933675765991211
Batch 9/64 loss: -0.6205253601074219
Batch 10/64 loss: -0.5119781494140625
Batch 11/64 loss: -0.32411861419677734
Batch 12/64 loss: -0.6776132583618164
Batch 13/64 loss: -0.46427154541015625
Batch 14/64 loss: -0.589625358581543
Batch 15/64 loss: -0.4796266555786133
Batch 16/64 loss: -0.7083396911621094
Batch 17/64 loss: -0.3330526351928711
Batch 18/64 loss: -0.581629753112793
Batch 19/64 loss: -0.2728118896484375
Batch 20/64 loss: -0.571904182434082
Batch 21/64 loss: -0.15689563751220703
Batch 22/64 loss: -0.47363948822021484
Batch 23/64 loss: -0.2681112289428711
Batch 24/64 loss: -0.06789112091064453
Batch 25/64 loss: -0.3148155212402344
Batch 26/64 loss: -0.6028223037719727
Batch 27/64 loss: -0.564915657043457
Batch 28/64 loss: -0.5060558319091797
Batch 29/64 loss: -0.7816333770751953
Batch 30/64 loss: -0.5938358306884766
Batch 31/64 loss: -0.40418529510498047
Batch 32/64 loss: -0.5135784149169922
Batch 33/64 loss: -0.16124248504638672
Batch 34/64 loss: -0.685246467590332
Batch 35/64 loss: -0.21539974212646484
Batch 36/64 loss: -0.7499837875366211
Batch 37/64 loss: -0.35477447509765625
Batch 38/64 loss: -0.48702239990234375
Batch 39/64 loss: -0.32877254486083984
Batch 40/64 loss: -0.536102294921875
Batch 41/64 loss: -0.9393720626831055
Batch 42/64 loss: -0.4014148712158203
Batch 43/64 loss: -0.5986719131469727
Batch 44/64 loss: -0.7569522857666016
Batch 45/64 loss: -0.801875114440918
Batch 46/64 loss: -0.5574016571044922
Batch 47/64 loss: 0.22070980072021484
Batch 48/64 loss: -0.5731697082519531
Batch 49/64 loss: -0.6426916122436523
Batch 50/64 loss: -0.6001148223876953
Batch 51/64 loss: -0.36423587799072266
Batch 52/64 loss: -0.6388149261474609
Batch 53/64 loss: -0.3065919876098633
Batch 54/64 loss: -0.7034025192260742
Batch 55/64 loss: -0.7901725769042969
Batch 56/64 loss: -0.446685791015625
Batch 57/64 loss: -0.26633167266845703
Batch 58/64 loss: -0.4738025665283203
Batch 59/64 loss: -0.7900428771972656
Batch 60/64 loss: -0.44889068603515625
Batch 61/64 loss: -0.5249099731445312
Batch 62/64 loss: -0.5032844543457031
Batch 63/64 loss: -0.6632242202758789
Batch 64/64 loss: -4.663481712341309
Epoch 334  Train loss: -0.5568122003592697  Val loss: -0.32072560156333896
Epoch 335
-------------------------------
Batch 1/64 loss: -0.10421276092529297
Batch 2/64 loss: -0.6316909790039062
Batch 3/64 loss: -0.5026435852050781
Batch 4/64 loss: -0.2944622039794922
Batch 5/64 loss: -0.1652202606201172
Batch 6/64 loss: -0.4217720031738281
Batch 7/64 loss: -0.5599822998046875
Batch 8/64 loss: -0.7091102600097656
Batch 9/64 loss: -0.6372108459472656
Batch 10/64 loss: -0.7020082473754883
Batch 11/64 loss: -0.4350624084472656
Batch 12/64 loss: -0.24506378173828125
Batch 13/64 loss: -0.5748558044433594
Batch 14/64 loss: -0.5867013931274414
Batch 15/64 loss: -0.7242965698242188
Batch 16/64 loss: -0.9297704696655273
Batch 17/64 loss: -0.5799331665039062
Batch 18/64 loss: -0.8238592147827148
Batch 19/64 loss: -0.5123157501220703
Batch 20/64 loss: -0.6058502197265625
Batch 21/64 loss: -0.8935613632202148
Batch 22/64 loss: -0.5702171325683594
Batch 23/64 loss: -0.8550443649291992
Batch 24/64 loss: -0.6141262054443359
Batch 25/64 loss: -0.5114955902099609
Batch 26/64 loss: -0.7823343276977539
Batch 27/64 loss: -0.3905315399169922
Batch 28/64 loss: -0.587864875793457
Batch 29/64 loss: -0.1683502197265625
Batch 30/64 loss: -0.3487968444824219
Batch 31/64 loss: -0.3559989929199219
Batch 32/64 loss: -0.22683238983154297
Batch 33/64 loss: -0.3155488967895508
Batch 34/64 loss: -0.6798505783081055
Batch 35/64 loss: -0.5714969635009766
Batch 36/64 loss: -0.5132036209106445
Batch 37/64 loss: -0.40198326110839844
Batch 38/64 loss: -0.004357337951660156
Batch 39/64 loss: -0.8229713439941406
Batch 40/64 loss: -0.1115875244140625
Batch 41/64 loss: -0.8017120361328125
Batch 42/64 loss: -0.676182746887207
Batch 43/64 loss: -0.28460025787353516
Batch 44/64 loss: -0.1163325309753418
Batch 45/64 loss: -0.6958131790161133
Batch 46/64 loss: -0.5374507904052734
Batch 47/64 loss: -0.04819297790527344
Batch 48/64 loss: -0.37240028381347656
Batch 49/64 loss: -0.29743480682373047
Batch 50/64 loss: -0.6846809387207031
Batch 51/64 loss: -0.8271703720092773
Batch 52/64 loss: -0.4311838150024414
Batch 53/64 loss: -0.6477899551391602
Batch 54/64 loss: -0.6004934310913086
Batch 55/64 loss: -0.7548341751098633
Batch 56/64 loss: -0.5767765045166016
Batch 57/64 loss: -0.3014049530029297
Batch 58/64 loss: -0.41762828826904297
Batch 59/64 loss: 0.07153081893920898
Batch 60/64 loss: -0.7737855911254883
Batch 61/64 loss: -0.3373451232910156
Batch 62/64 loss: -0.537322998046875
Batch 63/64 loss: -0.2191324234008789
Batch 64/64 loss: -4.735173225402832
Epoch 335  Train loss: -0.5473206725775027  Val loss: -0.4625848462081857
Epoch 336
-------------------------------
Batch 1/64 loss: -0.585113525390625
Batch 2/64 loss: -0.7859039306640625
Batch 3/64 loss: -0.7817010879516602
Batch 4/64 loss: -0.7506580352783203
Batch 5/64 loss: -0.28054332733154297
Batch 6/64 loss: -0.6269550323486328
Batch 7/64 loss: -0.4260673522949219
Batch 8/64 loss: -0.6495046615600586
Batch 9/64 loss: -0.8973512649536133
Batch 10/64 loss: -0.5215625762939453
Batch 11/64 loss: -0.5924386978149414
Batch 12/64 loss: -0.5005636215209961
Batch 13/64 loss: -0.5262918472290039
Batch 14/64 loss: -0.09721708297729492
Batch 15/64 loss: -0.6621608734130859
Batch 16/64 loss: -0.5008964538574219
Batch 17/64 loss: -0.8518600463867188
Batch 18/64 loss: -0.6986293792724609
Batch 19/64 loss: -0.6209640502929688
Batch 20/64 loss: -0.5034732818603516
Batch 21/64 loss: -0.32890892028808594
Batch 22/64 loss: -0.5126914978027344
Batch 23/64 loss: -0.26979541778564453
Batch 24/64 loss: -0.7421808242797852
Batch 25/64 loss: -0.7955532073974609
Batch 26/64 loss: -0.5923118591308594
Batch 27/64 loss: -0.7787904739379883
Batch 28/64 loss: -0.6412124633789062
Batch 29/64 loss: -0.22785568237304688
Batch 30/64 loss: -0.5534477233886719
Batch 31/64 loss: -0.41005992889404297
Batch 32/64 loss: -0.5643386840820312
Batch 33/64 loss: -0.4903583526611328
Batch 34/64 loss: -0.10048866271972656
Batch 35/64 loss: -0.1080465316772461
Batch 36/64 loss: -0.30017852783203125
Batch 37/64 loss: -0.7140188217163086
Batch 38/64 loss: -0.48578453063964844
Batch 39/64 loss: -0.2040252685546875
Batch 40/64 loss: -0.47835826873779297
Batch 41/64 loss: -0.23439598083496094
Batch 42/64 loss: -0.18503475189208984
Batch 43/64 loss: -0.582362174987793
Batch 44/64 loss: -0.6280736923217773
Batch 45/64 loss: -0.6009111404418945
Batch 46/64 loss: -0.0870199203491211
Batch 47/64 loss: -0.6802463531494141
Batch 48/64 loss: -0.008515357971191406
Batch 49/64 loss: -0.19562244415283203
Batch 50/64 loss: -0.2666645050048828
Batch 51/64 loss: -0.26259803771972656
Batch 52/64 loss: -0.5886926651000977
Batch 53/64 loss: -0.8686084747314453
Batch 54/64 loss: -0.5633077621459961
Batch 55/64 loss: -0.5608587265014648
Batch 56/64 loss: -0.2687492370605469
Batch 57/64 loss: -0.17755889892578125
Batch 58/64 loss: -0.39958858489990234
Batch 59/64 loss: -0.6297712326049805
Batch 60/64 loss: -0.2600879669189453
Batch 61/64 loss: -0.42952537536621094
Batch 62/64 loss: -0.6926641464233398
Batch 63/64 loss: -0.32637596130371094
Batch 64/64 loss: -4.305127143859863
Epoch 336  Train loss: -0.5315190670537013  Val loss: 0.04612669010752255
Epoch 337
-------------------------------
Batch 1/64 loss: -0.2626371383666992
Batch 2/64 loss: -0.7386598587036133
Batch 3/64 loss: 0.034493446350097656
Batch 4/64 loss: -0.6464471817016602
Batch 5/64 loss: -0.6940221786499023
Batch 6/64 loss: -0.5976505279541016
Batch 7/64 loss: -0.7472496032714844
Batch 8/64 loss: -0.09554576873779297
Batch 9/64 loss: -0.35246849060058594
Batch 10/64 loss: -0.5739336013793945
Batch 11/64 loss: -0.23668289184570312
Batch 12/64 loss: -0.32021141052246094
Batch 13/64 loss: -0.3142433166503906
Batch 14/64 loss: -0.6104574203491211
Batch 15/64 loss: -0.9606685638427734
Batch 16/64 loss: -0.8575649261474609
Batch 17/64 loss: -0.11412239074707031
Batch 18/64 loss: -0.21481800079345703
Batch 19/64 loss: -0.6652317047119141
Batch 20/64 loss: -0.5693845748901367
Batch 21/64 loss: -0.6201667785644531
Batch 22/64 loss: -0.8462314605712891
Batch 23/64 loss: -0.6971940994262695
Batch 24/64 loss: -0.36424827575683594
Batch 25/64 loss: -0.8974828720092773
Batch 26/64 loss: -0.8367681503295898
Batch 27/64 loss: -0.5397167205810547
Batch 28/64 loss: -0.4165821075439453
Batch 29/64 loss: 0.03832864761352539
Batch 30/64 loss: -0.4513101577758789
Batch 31/64 loss: -0.47555065155029297
Batch 32/64 loss: -0.29441022872924805
Batch 33/64 loss: -0.6104850769042969
Batch 34/64 loss: -0.39543819427490234
Batch 35/64 loss: -0.21093368530273438
Batch 36/64 loss: -0.4080848693847656
Batch 37/64 loss: -0.6342954635620117
Batch 38/64 loss: -0.7726516723632812
Batch 39/64 loss: -0.35349369049072266
Batch 40/64 loss: -0.4052906036376953
Batch 41/64 loss: -0.7446680068969727
Batch 42/64 loss: -0.2709474563598633
Batch 43/64 loss: -0.48012256622314453
Batch 44/64 loss: 0.0019183158874511719
Batch 45/64 loss: -0.7249832153320312
Batch 46/64 loss: -0.6148872375488281
Batch 47/64 loss: -0.22829246520996094
Batch 48/64 loss: -0.6715335845947266
Batch 49/64 loss: -0.4346127510070801
Batch 50/64 loss: -0.7348728179931641
Batch 51/64 loss: -0.33861351013183594
Batch 52/64 loss: -0.13103437423706055
Batch 53/64 loss: -0.5945444107055664
Batch 54/64 loss: -0.6077861785888672
Batch 55/64 loss: -0.18105030059814453
Batch 56/64 loss: -0.5375404357910156
Batch 57/64 loss: 0.017566204071044922
Batch 58/64 loss: -0.614619255065918
Batch 59/64 loss: -0.4324493408203125
Batch 60/64 loss: -0.672947883605957
Batch 61/64 loss: -0.23610496520996094
Batch 62/64 loss: -0.7268486022949219
Batch 63/64 loss: -0.671757698059082
Batch 64/64 loss: -4.407829284667969
Epoch 337  Train loss: -0.5280959484623927  Val loss: -0.3937514262510739
Epoch 338
-------------------------------
Batch 1/64 loss: -0.806117057800293
Batch 2/64 loss: -0.722259521484375
Batch 3/64 loss: -0.7193517684936523
Batch 4/64 loss: -0.7452850341796875
Batch 5/64 loss: -0.1795511245727539
Batch 6/64 loss: -0.35442352294921875
Batch 7/64 loss: -0.43184375762939453
Batch 8/64 loss: -0.5379219055175781
Batch 9/64 loss: -0.6834745407104492
Batch 10/64 loss: -0.6537570953369141
Batch 11/64 loss: -0.7656211853027344
Batch 12/64 loss: -0.5901403427124023
Batch 13/64 loss: -0.5528087615966797
Batch 14/64 loss: -0.2457752227783203
Batch 15/64 loss: -0.587061882019043
Batch 16/64 loss: -0.4939584732055664
Batch 17/64 loss: -0.2688455581665039
Batch 18/64 loss: -0.5265884399414062
Batch 19/64 loss: -0.7174482345581055
Batch 20/64 loss: -0.9460039138793945
Batch 21/64 loss: -0.7241725921630859
Batch 22/64 loss: -0.9829139709472656
Batch 23/64 loss: -0.7805747985839844
Batch 24/64 loss: -0.6539096832275391
Batch 25/64 loss: -0.4296855926513672
Batch 26/64 loss: -0.29724884033203125
Batch 27/64 loss: -0.40900421142578125
Batch 28/64 loss: -0.7413597106933594
Batch 29/64 loss: -0.5088701248168945
Batch 30/64 loss: 0.11535215377807617
Batch 31/64 loss: -0.5784492492675781
Batch 32/64 loss: -0.32923221588134766
Batch 33/64 loss: -0.5425529479980469
Batch 34/64 loss: -0.1885538101196289
Batch 35/64 loss: -0.21181964874267578
Batch 36/64 loss: -0.8890476226806641
Batch 37/64 loss: -0.6973562240600586
Batch 38/64 loss: -0.5834798812866211
Batch 39/64 loss: -0.4214591979980469
Batch 40/64 loss: -0.07340049743652344
Batch 41/64 loss: -0.7941217422485352
Batch 42/64 loss: -0.7567129135131836
Batch 43/64 loss: -0.7032556533813477
Batch 44/64 loss: -0.26906299591064453
Batch 45/64 loss: -0.6421842575073242
Batch 46/64 loss: -0.7421598434448242
Batch 47/64 loss: -0.6335287094116211
Batch 48/64 loss: -0.26916027069091797
Batch 49/64 loss: -0.30167102813720703
Batch 50/64 loss: -0.6988391876220703
Batch 51/64 loss: -0.6445789337158203
Batch 52/64 loss: -0.7227754592895508
Batch 53/64 loss: -0.7251243591308594
Batch 54/64 loss: -0.15722942352294922
Batch 55/64 loss: -0.48613929748535156
Batch 56/64 loss: -0.47690296173095703
Batch 57/64 loss: -0.5935754776000977
Batch 58/64 loss: -0.475494384765625
Batch 59/64 loss: -0.03282642364501953
Batch 60/64 loss: -0.5737409591674805
Batch 61/64 loss: -0.1655721664428711
Batch 62/64 loss: -0.5384683609008789
Batch 63/64 loss: -0.40802764892578125
Batch 64/64 loss: -4.687021255493164
Epoch 338  Train loss: -0.5769787358302696  Val loss: -0.271211525828568
Epoch 339
-------------------------------
Batch 1/64 loss: -0.8687343597412109
Batch 2/64 loss: -0.45688438415527344
Batch 3/64 loss: -0.5418977737426758
Batch 4/64 loss: -0.06910991668701172
Batch 5/64 loss: -0.7153959274291992
Batch 6/64 loss: -0.4798707962036133
Batch 7/64 loss: -0.3618955612182617
Batch 8/64 loss: -0.26455116271972656
Batch 9/64 loss: -0.23703670501708984
Batch 10/64 loss: 0.40453624725341797
Batch 11/64 loss: -0.505101203918457
Batch 12/64 loss: -0.3459358215332031
Batch 13/64 loss: -0.32309913635253906
Batch 14/64 loss: 0.16299057006835938
Batch 15/64 loss: -0.18463420867919922
Batch 16/64 loss: -0.2123556137084961
Batch 17/64 loss: -0.21982288360595703
Batch 18/64 loss: -0.5532369613647461
Batch 19/64 loss: -0.3767814636230469
Batch 20/64 loss: -0.7783288955688477
Batch 21/64 loss: -0.5254325866699219
Batch 22/64 loss: -0.44768810272216797
Batch 23/64 loss: -0.30623531341552734
Batch 24/64 loss: -0.261962890625
Batch 25/64 loss: -0.5704860687255859
Batch 26/64 loss: -0.41281604766845703
Batch 27/64 loss: -0.19454193115234375
Batch 28/64 loss: -0.7902460098266602
Batch 29/64 loss: -0.12572288513183594
Batch 30/64 loss: -0.7663726806640625
Batch 31/64 loss: -0.31550121307373047
Batch 32/64 loss: -0.41199684143066406
Batch 33/64 loss: -0.32690906524658203
Batch 34/64 loss: -0.637812614440918
Batch 35/64 loss: -0.6452140808105469
Batch 36/64 loss: -0.5495786666870117
Batch 37/64 loss: -0.19062328338623047
Batch 38/64 loss: -0.4745750427246094
Batch 39/64 loss: -0.4937162399291992
Batch 40/64 loss: -0.8269338607788086
Batch 41/64 loss: -0.6208686828613281
Batch 42/64 loss: -0.4958524703979492
Batch 43/64 loss: -0.3603801727294922
Batch 44/64 loss: -0.85430908203125
Batch 45/64 loss: -0.6806106567382812
Batch 46/64 loss: -0.5303993225097656
Batch 47/64 loss: -0.5989580154418945
Batch 48/64 loss: -0.7420587539672852
Batch 49/64 loss: -0.024446487426757812
Batch 50/64 loss: -0.45035457611083984
Batch 51/64 loss: -0.40023326873779297
Batch 52/64 loss: -0.6399307250976562
Batch 53/64 loss: -0.7292051315307617
Batch 54/64 loss: -0.380828857421875
Batch 55/64 loss: -0.6354703903198242
Batch 56/64 loss: -0.8079633712768555
Batch 57/64 loss: -0.4797811508178711
Batch 58/64 loss: -0.5199384689331055
Batch 59/64 loss: -0.16121292114257812
Batch 60/64 loss: -0.2551727294921875
Batch 61/64 loss: -0.22228336334228516
Batch 62/64 loss: -0.40783214569091797
Batch 63/64 loss: -0.3721733093261719
Batch 64/64 loss: -4.80809211730957
Epoch 339  Train loss: -0.48906422783346737  Val loss: -0.16119331019031224
Epoch 340
-------------------------------
Batch 1/64 loss: -0.6615190505981445
Batch 2/64 loss: -0.6151237487792969
Batch 3/64 loss: -0.5788507461547852
Batch 4/64 loss: -0.5980930328369141
Batch 5/64 loss: -0.3805360794067383
Batch 6/64 loss: -0.4129457473754883
Batch 7/64 loss: -0.7359962463378906
Batch 8/64 loss: -0.12804508209228516
Batch 9/64 loss: -0.4891042709350586
Batch 10/64 loss: -0.569091796875
Batch 11/64 loss: -0.03757667541503906
Batch 12/64 loss: -0.6651592254638672
Batch 13/64 loss: -0.5275468826293945
Batch 14/64 loss: -0.4268455505371094
Batch 15/64 loss: -0.5979204177856445
Batch 16/64 loss: -0.7727775573730469
Batch 17/64 loss: -0.21247577667236328
Batch 18/64 loss: -0.3244342803955078
Batch 19/64 loss: -0.3688488006591797
Batch 20/64 loss: -0.6179046630859375
Batch 21/64 loss: -0.5128641128540039
Batch 22/64 loss: -0.5386495590209961
Batch 23/64 loss: -0.4548788070678711
Batch 24/64 loss: -0.4041461944580078
Batch 25/64 loss: -0.4400348663330078
Batch 26/64 loss: -0.6474781036376953
Batch 27/64 loss: -0.8215742111206055
Batch 28/64 loss: -0.5893363952636719
Batch 29/64 loss: -0.27596282958984375
Batch 30/64 loss: -0.5324201583862305
Batch 31/64 loss: -0.3631429672241211
Batch 32/64 loss: -0.4510469436645508
Batch 33/64 loss: -0.5764894485473633
Batch 34/64 loss: -0.6106605529785156
Batch 35/64 loss: -0.5927104949951172
Batch 36/64 loss: -0.5673103332519531
Batch 37/64 loss: -0.6276321411132812
Batch 38/64 loss: -0.6823062896728516
Batch 39/64 loss: -0.45006752014160156
Batch 40/64 loss: -0.3835268020629883
Batch 41/64 loss: -0.8437061309814453
Batch 42/64 loss: -0.5697879791259766
Batch 43/64 loss: -0.2982759475708008
Batch 44/64 loss: -0.6199483871459961
Batch 45/64 loss: -0.5847816467285156
Batch 46/64 loss: -0.6209611892700195
Batch 47/64 loss: -0.6650505065917969
Batch 48/64 loss: -0.3307933807373047
Batch 49/64 loss: -0.4903097152709961
Batch 50/64 loss: -0.6152181625366211
Batch 51/64 loss: -0.33249664306640625
Batch 52/64 loss: -0.7723522186279297
Batch 53/64 loss: -0.42453765869140625
Batch 54/64 loss: -0.48949337005615234
Batch 55/64 loss: -0.45030784606933594
Batch 56/64 loss: -0.44831371307373047
Batch 57/64 loss: -0.7692098617553711
Batch 58/64 loss: -0.6747264862060547
Batch 59/64 loss: -0.9631624221801758
Batch 60/64 loss: -0.2191762924194336
Batch 61/64 loss: -0.7024869918823242
Batch 62/64 loss: -0.5932655334472656
Batch 63/64 loss: -0.4139585494995117
Batch 64/64 loss: -4.792243480682373
Epoch 340  Train loss: -0.5761496095096363  Val loss: -0.30456852339387347
Epoch 341
-------------------------------
Batch 1/64 loss: -0.35779666900634766
Batch 2/64 loss: -0.5492525100708008
Batch 3/64 loss: -0.25030994415283203
Batch 4/64 loss: 0.30089664459228516
Batch 5/64 loss: -0.5630598068237305
Batch 6/64 loss: -0.8800287246704102
Batch 7/64 loss: -0.8537826538085938
Batch 8/64 loss: -0.3608226776123047
Batch 9/64 loss: -0.6710186004638672
Batch 10/64 loss: -0.42397022247314453
Batch 11/64 loss: -0.34912776947021484
Batch 12/64 loss: -0.4671592712402344
Batch 13/64 loss: -0.8035554885864258
Batch 14/64 loss: -0.7770538330078125
Batch 15/64 loss: -0.8299665451049805
Batch 16/64 loss: 0.08733272552490234
Batch 17/64 loss: -0.4240751266479492
Batch 18/64 loss: -0.6500453948974609
Batch 19/64 loss: -0.4177398681640625
Batch 20/64 loss: -0.5098047256469727
Batch 21/64 loss: -0.6755485534667969
Batch 22/64 loss: -0.9217634201049805
Batch 23/64 loss: -0.03962421417236328
Batch 24/64 loss: -0.6611166000366211
Batch 25/64 loss: -0.4058246612548828
Batch 26/64 loss: 0.10058021545410156
Batch 27/64 loss: -0.5834131240844727
Batch 28/64 loss: -0.8080511093139648
Batch 29/64 loss: -0.6764860153198242
Batch 30/64 loss: -0.8072147369384766
Batch 31/64 loss: -0.5190496444702148
Batch 32/64 loss: -0.5312232971191406
Batch 33/64 loss: -0.5991058349609375
Batch 34/64 loss: -0.06331348419189453
Batch 35/64 loss: -0.7218561172485352
Batch 36/64 loss: -0.8870887756347656
Batch 37/64 loss: -0.6173563003540039
Batch 38/64 loss: -0.3667278289794922
Batch 39/64 loss: -0.7227573394775391
Batch 40/64 loss: -0.42708301544189453
Batch 41/64 loss: -0.8582401275634766
Batch 42/64 loss: -0.4308290481567383
Batch 43/64 loss: -0.6425743103027344
Batch 44/64 loss: -0.8741645812988281
Batch 45/64 loss: -0.6569433212280273
Batch 46/64 loss: -0.39096641540527344
Batch 47/64 loss: -0.9461660385131836
Batch 48/64 loss: -0.6826286315917969
Batch 49/64 loss: -0.4717063903808594
Batch 50/64 loss: -0.8247232437133789
Batch 51/64 loss: -0.8048315048217773
Batch 52/64 loss: -0.2555074691772461
Batch 53/64 loss: -0.7679595947265625
Batch 54/64 loss: -0.47061920166015625
Batch 55/64 loss: -0.44173431396484375
Batch 56/64 loss: -0.9146480560302734
Batch 57/64 loss: -0.13983726501464844
Batch 58/64 loss: -0.6472568511962891
Batch 59/64 loss: -0.5766773223876953
Batch 60/64 loss: -0.686798095703125
Batch 61/64 loss: 0.0848093032836914
Batch 62/64 loss: -0.3538846969604492
Batch 63/64 loss: -0.5269317626953125
Batch 64/64 loss: -4.016572952270508
Epoch 341  Train loss: -0.5800409878001493  Val loss: -0.4642970160520363
Epoch 342
-------------------------------
Batch 1/64 loss: -0.5516567230224609
Batch 2/64 loss: -0.6226530075073242
Batch 3/64 loss: -0.4784584045410156
Batch 4/64 loss: -0.7394704818725586
Batch 5/64 loss: -0.1401987075805664
Batch 6/64 loss: -0.9372825622558594
Batch 7/64 loss: -0.6820354461669922
Batch 8/64 loss: -0.5945110321044922
Batch 9/64 loss: -0.622410774230957
Batch 10/64 loss: -0.32332515716552734
Batch 11/64 loss: -0.5502405166625977
Batch 12/64 loss: -0.6860113143920898
Batch 13/64 loss: -0.28504180908203125
Batch 14/64 loss: -0.7601594924926758
Batch 15/64 loss: -0.5148773193359375
Batch 16/64 loss: -0.7407941818237305
Batch 17/64 loss: -0.7526874542236328
Batch 18/64 loss: -0.4346904754638672
Batch 19/64 loss: -0.21251583099365234
Batch 20/64 loss: -0.48796749114990234
Batch 21/64 loss: -0.4466428756713867
Batch 22/64 loss: -0.039493560791015625
Batch 23/64 loss: -0.48149967193603516
Batch 24/64 loss: -0.572479248046875
Batch 25/64 loss: -0.9876537322998047
Batch 26/64 loss: -0.5805835723876953
Batch 27/64 loss: -0.45578765869140625
Batch 28/64 loss: -0.6448345184326172
Batch 29/64 loss: -0.5451135635375977
Batch 30/64 loss: -0.6496620178222656
Batch 31/64 loss: -0.6446676254272461
Batch 32/64 loss: -0.4152355194091797
Batch 33/64 loss: -0.3262300491333008
Batch 34/64 loss: -0.44368457794189453
Batch 35/64 loss: -0.6490230560302734
Batch 36/64 loss: -0.3913249969482422
Batch 37/64 loss: -0.8722515106201172
Batch 38/64 loss: -0.64996337890625
Batch 39/64 loss: -0.6389293670654297
Batch 40/64 loss: -0.42286109924316406
Batch 41/64 loss: -0.8296689987182617
Batch 42/64 loss: -0.3702688217163086
Batch 43/64 loss: -0.6532716751098633
Batch 44/64 loss: -0.2800140380859375
Batch 45/64 loss: -0.6420536041259766
Batch 46/64 loss: -0.1577739715576172
Batch 47/64 loss: -0.1358203887939453
Batch 48/64 loss: -0.35474586486816406
Batch 49/64 loss: -0.6119709014892578
Batch 50/64 loss: -0.26354503631591797
Batch 51/64 loss: -0.5653982162475586
Batch 52/64 loss: -0.5441675186157227
Batch 53/64 loss: -0.33189868927001953
Batch 54/64 loss: -0.8402423858642578
Batch 55/64 loss: -0.76617431640625
Batch 56/64 loss: -0.7853336334228516
Batch 57/64 loss: -0.31655025482177734
Batch 58/64 loss: -0.4990262985229492
Batch 59/64 loss: -0.5620079040527344
Batch 60/64 loss: -0.8066120147705078
Batch 61/64 loss: -0.7466583251953125
Batch 62/64 loss: -0.7022409439086914
Batch 63/64 loss: -0.6406459808349609
Batch 64/64 loss: -4.65618371963501
Epoch 342  Train loss: -0.5940884290956984  Val loss: -0.4020628257305761
Epoch 343
-------------------------------
Batch 1/64 loss: -0.10185813903808594
Batch 2/64 loss: -0.47303199768066406
Batch 3/64 loss: -0.9578094482421875
Batch 4/64 loss: -0.6062812805175781
Batch 5/64 loss: -0.13091278076171875
Batch 6/64 loss: -0.7296009063720703
Batch 7/64 loss: -0.6804904937744141
Batch 8/64 loss: -0.7137250900268555
Batch 9/64 loss: -0.3440895080566406
Batch 10/64 loss: -0.3792715072631836
Batch 11/64 loss: -0.74560546875
Batch 12/64 loss: -0.9153251647949219
Batch 13/64 loss: -0.366973876953125
Batch 14/64 loss: -0.28777122497558594
Batch 15/64 loss: -0.9828195571899414
Batch 16/64 loss: -0.33565521240234375
Batch 17/64 loss: -0.563410758972168
Batch 18/64 loss: -0.6334896087646484
Batch 19/64 loss: -0.7312259674072266
Batch 20/64 loss: -0.9052286148071289
Batch 21/64 loss: -0.7818546295166016
Batch 22/64 loss: -0.5537147521972656
Batch 23/64 loss: 0.9527349472045898
Batch 24/64 loss: -0.16487503051757812
Batch 25/64 loss: -0.2234477996826172
Batch 26/64 loss: -0.47775745391845703
Batch 27/64 loss: -0.896449089050293
Batch 28/64 loss: -0.5170488357543945
Batch 29/64 loss: -0.5827236175537109
Batch 30/64 loss: -0.5400762557983398
Batch 31/64 loss: -0.7858772277832031
Batch 32/64 loss: -0.5309944152832031
Batch 33/64 loss: -0.651123046875
Batch 34/64 loss: -0.6744155883789062
Batch 35/64 loss: -0.8482761383056641
Batch 36/64 loss: -0.4620018005371094
Batch 37/64 loss: -1.0458478927612305
Batch 38/64 loss: -0.7992477416992188
Batch 39/64 loss: -0.5421714782714844
Batch 40/64 loss: -0.4775114059448242
Batch 41/64 loss: -0.19402217864990234
Batch 42/64 loss: -0.8232154846191406
Batch 43/64 loss: -0.8915081024169922
Batch 44/64 loss: -0.8410959243774414
Batch 45/64 loss: -0.24418258666992188
Batch 46/64 loss: 0.5251131057739258
Batch 47/64 loss: -0.8982200622558594
Batch 48/64 loss: -0.23961925506591797
Batch 49/64 loss: -0.6556129455566406
Batch 50/64 loss: -0.7338151931762695
Batch 51/64 loss: -0.6086206436157227
Batch 52/64 loss: -0.9539260864257812
Batch 53/64 loss: -0.22641849517822266
Batch 54/64 loss: -0.41541481018066406
Batch 55/64 loss: -0.2298107147216797
Batch 56/64 loss: -0.17908382415771484
Batch 57/64 loss: -0.44434261322021484
Batch 58/64 loss: -0.7552661895751953
Batch 59/64 loss: -0.38239097595214844
Batch 60/64 loss: -0.4822807312011719
Batch 61/64 loss: -0.5073280334472656
Batch 62/64 loss: -0.48171234130859375
Batch 63/64 loss: -0.5521297454833984
Batch 64/64 loss: -5.085648536682129
Epoch 343  Train loss: -0.583786605386173  Val loss: -0.36093344147672357
Epoch 344
-------------------------------
Batch 1/64 loss: -0.5259981155395508
Batch 2/64 loss: -0.40182971954345703
Batch 3/64 loss: -0.5495376586914062
Batch 4/64 loss: -0.6994037628173828
Batch 5/64 loss: -0.6477727890014648
Batch 6/64 loss: -0.6751270294189453
Batch 7/64 loss: -0.5316867828369141
Batch 8/64 loss: -0.5095863342285156
Batch 9/64 loss: -0.3712453842163086
Batch 10/64 loss: -0.8607091903686523
Batch 11/64 loss: -0.40435123443603516
Batch 12/64 loss: -0.4555797576904297
Batch 13/64 loss: -0.7274293899536133
Batch 14/64 loss: -0.6492605209350586
Batch 15/64 loss: -0.8118143081665039
Batch 16/64 loss: -0.3161954879760742
Batch 17/64 loss: -0.6885662078857422
Batch 18/64 loss: -0.3083477020263672
Batch 19/64 loss: -0.7638778686523438
Batch 20/64 loss: -0.4527006149291992
Batch 21/64 loss: -0.8686227798461914
Batch 22/64 loss: -0.7472553253173828
Batch 23/64 loss: 0.05108833312988281
Batch 24/64 loss: -0.5056114196777344
Batch 25/64 loss: -0.5409374237060547
Batch 26/64 loss: -0.47971534729003906
Batch 27/64 loss: -0.5007724761962891
Batch 28/64 loss: -0.4542551040649414
Batch 29/64 loss: -0.6982746124267578
Batch 30/64 loss: -0.6504278182983398
Batch 31/64 loss: -0.5563240051269531
Batch 32/64 loss: -0.5832738876342773
Batch 33/64 loss: -0.662287712097168
Batch 34/64 loss: -0.1506185531616211
Batch 35/64 loss: -0.676487922668457
Batch 36/64 loss: -0.5938024520874023
Batch 37/64 loss: -0.02544689178466797
Batch 38/64 loss: -0.05890369415283203
Batch 39/64 loss: -0.8818235397338867
Batch 40/64 loss: -0.7565546035766602
Batch 41/64 loss: -0.6844215393066406
Batch 42/64 loss: -0.8218879699707031
Batch 43/64 loss: -0.8662786483764648
Batch 44/64 loss: -0.1360793113708496
Batch 45/64 loss: -0.6257057189941406
Batch 46/64 loss: -0.49173545837402344
Batch 47/64 loss: -0.7303695678710938
Batch 48/64 loss: -1.0453195571899414
Batch 49/64 loss: -0.8522281646728516
Batch 50/64 loss: 0.2819080352783203
Batch 51/64 loss: -0.6147356033325195
Batch 52/64 loss: -0.7024602890014648
Batch 53/64 loss: -0.7750110626220703
Batch 54/64 loss: -0.6736116409301758
Batch 55/64 loss: -0.3359565734863281
Batch 56/64 loss: -0.9616518020629883
Batch 57/64 loss: 0.1876354217529297
Batch 58/64 loss: -0.6389236450195312
Batch 59/64 loss: -0.41931915283203125
Batch 60/64 loss: -0.7844009399414062
Batch 61/64 loss: -0.2987098693847656
Batch 62/64 loss: -0.7286338806152344
Batch 63/64 loss: -0.5784521102905273
Batch 64/64 loss: -4.751774311065674
Epoch 344  Train loss: -0.6047295196383607  Val loss: -0.415676523320044
Epoch 345
-------------------------------
Batch 1/64 loss: -0.0942392349243164
Batch 2/64 loss: -0.6145868301391602
Batch 3/64 loss: -0.8235321044921875
Batch 4/64 loss: -0.8267011642456055
Batch 5/64 loss: -0.7628231048583984
Batch 6/64 loss: -0.5043258666992188
Batch 7/64 loss: -0.6059551239013672
Batch 8/64 loss: -0.6490421295166016
Batch 9/64 loss: -0.6690092086791992
Batch 10/64 loss: -0.7112636566162109
Batch 11/64 loss: -0.69677734375
Batch 12/64 loss: -0.7703304290771484
Batch 13/64 loss: -0.8101768493652344
Batch 14/64 loss: -0.8928737640380859
Batch 15/64 loss: -0.6780405044555664
Batch 16/64 loss: -0.7837533950805664
Batch 17/64 loss: -0.3047304153442383
Batch 18/64 loss: -0.882563591003418
Batch 19/64 loss: -0.5890789031982422
Batch 20/64 loss: -0.6863470077514648
Batch 21/64 loss: -0.6801528930664062
Batch 22/64 loss: -0.8267936706542969
Batch 23/64 loss: -0.045144081115722656
Batch 24/64 loss: -0.3107147216796875
Batch 25/64 loss: -0.5112419128417969
Batch 26/64 loss: -0.45804500579833984
Batch 27/64 loss: 0.25750160217285156
Batch 28/64 loss: -0.49098682403564453
Batch 29/64 loss: -0.34979915618896484
Batch 30/64 loss: -0.760655403137207
Batch 31/64 loss: -0.5487899780273438
Batch 32/64 loss: -0.587803840637207
Batch 33/64 loss: -0.3904762268066406
Batch 34/64 loss: -0.7033958435058594
Batch 35/64 loss: -0.41506099700927734
Batch 36/64 loss: -0.45533180236816406
Batch 37/64 loss: -0.47672462463378906
Batch 38/64 loss: -0.46593761444091797
Batch 39/64 loss: -0.36670970916748047
Batch 40/64 loss: -0.5885095596313477
Batch 41/64 loss: -0.3476982116699219
Batch 42/64 loss: -0.5658683776855469
Batch 43/64 loss: -0.5849170684814453
Batch 44/64 loss: -0.6012840270996094
Batch 45/64 loss: -0.0886831283569336
Batch 46/64 loss: -0.6052541732788086
Batch 47/64 loss: -0.2666740417480469
Batch 48/64 loss: -0.12847232818603516
Batch 49/64 loss: -0.6695680618286133
Batch 50/64 loss: -0.8185148239135742
Batch 51/64 loss: -0.1985158920288086
Batch 52/64 loss: -0.7534942626953125
Batch 53/64 loss: -0.30408668518066406
Batch 54/64 loss: -0.43200111389160156
Batch 55/64 loss: -0.593632698059082
Batch 56/64 loss: -0.10314369201660156
Batch 57/64 loss: -0.953155517578125
Batch 58/64 loss: -0.37374305725097656
Batch 59/64 loss: -0.1852102279663086
Batch 60/64 loss: -0.6762304306030273
Batch 61/64 loss: -0.8523530960083008
Batch 62/64 loss: -0.5622749328613281
Batch 63/64 loss: -0.7120332717895508
Batch 64/64 loss: -4.484923839569092
Epoch 345  Train loss: -0.5846498021892473  Val loss: -0.4896481307511477
Epoch 346
-------------------------------
Batch 1/64 loss: -0.5736818313598633
Batch 2/64 loss: -0.8823795318603516
Batch 3/64 loss: -0.737879753112793
Batch 4/64 loss: -0.5090370178222656
Batch 5/64 loss: -0.7479019165039062
Batch 6/64 loss: -0.8736019134521484
Batch 7/64 loss: -0.40380096435546875
Batch 8/64 loss: -0.1179494857788086
Batch 9/64 loss: -0.10875129699707031
Batch 10/64 loss: -0.5663785934448242
Batch 11/64 loss: -0.32541465759277344
Batch 12/64 loss: -0.8130064010620117
Batch 13/64 loss: -0.4878683090209961
Batch 14/64 loss: -0.5448722839355469
Batch 15/64 loss: -0.7033901214599609
Batch 16/64 loss: -0.730128288269043
Batch 17/64 loss: -0.39608287811279297
Batch 18/64 loss: -0.2952098846435547
Batch 19/64 loss: -0.7094440460205078
Batch 20/64 loss: -0.9092626571655273
Batch 21/64 loss: -0.6074810028076172
Batch 22/64 loss: -0.469451904296875
Batch 23/64 loss: -0.5631914138793945
Batch 24/64 loss: -0.5836515426635742
Batch 25/64 loss: -0.015358448028564453
Batch 26/64 loss: -0.3564167022705078
Batch 27/64 loss: -0.34136390686035156
Batch 28/64 loss: -0.6848917007446289
Batch 29/64 loss: -0.5781354904174805
Batch 30/64 loss: -0.541071891784668
Batch 31/64 loss: -0.718815803527832
Batch 32/64 loss: -0.535527229309082
Batch 33/64 loss: -0.4139127731323242
Batch 34/64 loss: -0.42568016052246094
Batch 35/64 loss: -0.33172607421875
Batch 36/64 loss: -0.49094581604003906
Batch 37/64 loss: -0.5465774536132812
Batch 38/64 loss: -0.4238576889038086
Batch 39/64 loss: -0.4968414306640625
Batch 40/64 loss: -0.3947162628173828
Batch 41/64 loss: -0.9191799163818359
Batch 42/64 loss: -0.8805627822875977
Batch 43/64 loss: -0.8490839004516602
Batch 44/64 loss: -0.2125253677368164
Batch 45/64 loss: -0.5763225555419922
Batch 46/64 loss: -0.5055093765258789
Batch 47/64 loss: -0.6731061935424805
Batch 48/64 loss: -0.2748575210571289
Batch 49/64 loss: -0.17558956146240234
Batch 50/64 loss: -0.550633430480957
Batch 51/64 loss: -0.6536340713500977
Batch 52/64 loss: -0.3070106506347656
Batch 53/64 loss: -0.5914525985717773
Batch 54/64 loss: -0.8230705261230469
Batch 55/64 loss: -0.6417388916015625
Batch 56/64 loss: -0.7826976776123047
Batch 57/64 loss: -0.7466678619384766
Batch 58/64 loss: -0.4456634521484375
Batch 59/64 loss: -0.6694145202636719
Batch 60/64 loss: -0.4425067901611328
Batch 61/64 loss: -0.5180702209472656
Batch 62/64 loss: -0.5484867095947266
Batch 63/64 loss: -0.8358907699584961
Batch 64/64 loss: -4.710894584655762
Epoch 346  Train loss: -0.5978431813857135  Val loss: -0.17754915899427487
Epoch 347
-------------------------------
Batch 1/64 loss: -0.616663932800293
Batch 2/64 loss: -0.44702720642089844
Batch 3/64 loss: 0.2690696716308594
Batch 4/64 loss: -0.43765830993652344
Batch 5/64 loss: -0.6470956802368164
Batch 6/64 loss: -0.59814453125
Batch 7/64 loss: -0.40578556060791016
Batch 8/64 loss: 1.190251350402832
Batch 9/64 loss: -0.7878732681274414
Batch 10/64 loss: -0.5735359191894531
Batch 11/64 loss: -0.5185327529907227
Batch 12/64 loss: -0.20215606689453125
Batch 13/64 loss: -0.2973814010620117
Batch 14/64 loss: -0.25514793395996094
Batch 15/64 loss: 0.014806747436523438
Batch 16/64 loss: -0.15294647216796875
Batch 17/64 loss: -0.5273523330688477
Batch 18/64 loss: 0.08607769012451172
Batch 19/64 loss: -0.32918453216552734
Batch 20/64 loss: -0.36577796936035156
Batch 21/64 loss: -0.014584064483642578
Batch 22/64 loss: -0.5386419296264648
Batch 23/64 loss: -0.5002422332763672
Batch 24/64 loss: 0.024270057678222656
Batch 25/64 loss: -0.08323097229003906
Batch 26/64 loss: -0.45618104934692383
Batch 27/64 loss: -0.047544002532958984
Batch 28/64 loss: -0.4631052017211914
Batch 29/64 loss: -0.6310386657714844
Batch 30/64 loss: -0.3783559799194336
Batch 31/64 loss: -0.3063545227050781
Batch 32/64 loss: -0.5225591659545898
Batch 33/64 loss: -0.2430434226989746
Batch 34/64 loss: -0.4460725784301758
Batch 35/64 loss: -0.8440847396850586
Batch 36/64 loss: -0.5611228942871094
Batch 37/64 loss: -0.34476184844970703
Batch 38/64 loss: -0.1410980224609375
Batch 39/64 loss: -0.4347381591796875
Batch 40/64 loss: -0.3407926559448242
Batch 41/64 loss: -0.3322925567626953
Batch 42/64 loss: -0.19594430923461914
Batch 43/64 loss: -0.34415578842163086
Batch 44/64 loss: -0.3963479995727539
Batch 45/64 loss: -0.4086933135986328
Batch 46/64 loss: -0.3617982864379883
Batch 47/64 loss: -0.4568309783935547
Batch 48/64 loss: -0.45041751861572266
Batch 49/64 loss: -0.48966026306152344
Batch 50/64 loss: -0.3406848907470703
Batch 51/64 loss: -0.5141172409057617
Batch 52/64 loss: -0.21906566619873047
Batch 53/64 loss: -0.27740478515625
Batch 54/64 loss: -0.7966642379760742
Batch 55/64 loss: -0.5225095748901367
Batch 56/64 loss: -0.2814207077026367
Batch 57/64 loss: -0.6612434387207031
Batch 58/64 loss: -0.48419666290283203
Batch 59/64 loss: -0.14116191864013672
Batch 60/64 loss: -0.5046977996826172
Batch 61/64 loss: -0.5839347839355469
Batch 62/64 loss: -0.20650768280029297
Batch 63/64 loss: -0.5197668075561523
Batch 64/64 loss: -4.397324562072754
Epoch 347  Train loss: -0.4025545269835229  Val loss: -0.3643733414587696
Epoch 348
-------------------------------
Batch 1/64 loss: -0.6394252777099609
Batch 2/64 loss: -0.5444374084472656
Batch 3/64 loss: -0.35974740982055664
Batch 4/64 loss: -0.5611038208007812
Batch 5/64 loss: -0.1748514175415039
Batch 6/64 loss: -0.46558570861816406
Batch 7/64 loss: -0.35682010650634766
Batch 8/64 loss: -0.37976741790771484
Batch 9/64 loss: -0.48990726470947266
Batch 10/64 loss: -0.26891040802001953
Batch 11/64 loss: -0.7709493637084961
Batch 12/64 loss: -0.1857595443725586
Batch 13/64 loss: -0.29052734375
Batch 14/64 loss: -0.6402168273925781
Batch 15/64 loss: -0.6266002655029297
Batch 16/64 loss: -0.40694332122802734
Batch 17/64 loss: -0.7971115112304688
Batch 18/64 loss: -0.6649150848388672
Batch 19/64 loss: -0.6886825561523438
Batch 20/64 loss: -0.1668257713317871
Batch 21/64 loss: -0.6839218139648438
Batch 22/64 loss: -0.545654296875
Batch 23/64 loss: -0.7774496078491211
Batch 24/64 loss: -0.8097324371337891
Batch 25/64 loss: -0.6128568649291992
Batch 26/64 loss: -0.29890918731689453
Batch 27/64 loss: -0.03596353530883789
Batch 28/64 loss: -0.9576444625854492
Batch 29/64 loss: 0.1873455047607422
Batch 30/64 loss: -0.5859832763671875
Batch 31/64 loss: -0.4446859359741211
Batch 32/64 loss: -0.6427040100097656
Batch 33/64 loss: 0.007119178771972656
Batch 34/64 loss: -0.5029277801513672
Batch 35/64 loss: 0.1955242156982422
Batch 36/64 loss: -0.5430841445922852
Batch 37/64 loss: -0.4971284866333008
Batch 38/64 loss: -0.4093189239501953
Batch 39/64 loss: -0.4951925277709961
Batch 40/64 loss: -0.47267913818359375
Batch 41/64 loss: -0.6885795593261719
Batch 42/64 loss: -0.7180233001708984
Batch 43/64 loss: -0.4900960922241211
Batch 44/64 loss: -0.6993255615234375
Batch 45/64 loss: -0.6691913604736328
Batch 46/64 loss: -0.6266164779663086
Batch 47/64 loss: -0.5426397323608398
Batch 48/64 loss: -0.3110628128051758
Batch 49/64 loss: -0.28312015533447266
Batch 50/64 loss: -0.7705488204956055
Batch 51/64 loss: -0.13724708557128906
Batch 52/64 loss: -0.5611410140991211
Batch 53/64 loss: -0.3901805877685547
Batch 54/64 loss: -0.6521329879760742
Batch 55/64 loss: 0.04272890090942383
Batch 56/64 loss: -0.5740213394165039
Batch 57/64 loss: -0.6230382919311523
Batch 58/64 loss: -0.6109676361083984
Batch 59/64 loss: -0.38519763946533203
Batch 60/64 loss: 0.9111108779907227
Batch 61/64 loss: 0.18905353546142578
Batch 62/64 loss: 0.24340152740478516
Batch 63/64 loss: -0.0752573013305664
Batch 64/64 loss: -4.444522857666016
Epoch 348  Train loss: -0.4731046340044807  Val loss: -0.3465117228399847
Epoch 349
-------------------------------
Batch 1/64 loss: -0.5476236343383789
Batch 2/64 loss: -0.3708524703979492
Batch 3/64 loss: -0.2626667022705078
Batch 4/64 loss: -0.8506402969360352
Batch 5/64 loss: -0.7575788497924805
Batch 6/64 loss: -0.29075145721435547
Batch 7/64 loss: -0.48249053955078125
Batch 8/64 loss: -0.5038938522338867
Batch 9/64 loss: -0.3487567901611328
Batch 10/64 loss: -0.5638370513916016
Batch 11/64 loss: -0.6819496154785156
Batch 12/64 loss: 0.1974782943725586
Batch 13/64 loss: 0.14006662368774414
Batch 14/64 loss: -0.5255060195922852
Batch 15/64 loss: -0.4956932067871094
Batch 16/64 loss: 0.12580204010009766
Batch 17/64 loss: -0.6150360107421875
Batch 18/64 loss: -0.6314783096313477
Batch 19/64 loss: -0.6850185394287109
Batch 20/64 loss: -0.36649131774902344
Batch 21/64 loss: -0.1777048110961914
Batch 22/64 loss: -0.3812398910522461
Batch 23/64 loss: -0.5794401168823242
Batch 24/64 loss: -0.5768566131591797
Batch 25/64 loss: -0.4667186737060547
Batch 26/64 loss: -0.048155784606933594
Batch 27/64 loss: -0.7226362228393555
Batch 28/64 loss: -0.3844480514526367
Batch 29/64 loss: -0.3024883270263672
Batch 30/64 loss: -0.6433277130126953
Batch 31/64 loss: -0.35060596466064453
Batch 32/64 loss: -0.6511735916137695
Batch 33/64 loss: -0.8886404037475586
Batch 34/64 loss: 0.4880943298339844
Batch 35/64 loss: -0.5872077941894531
Batch 36/64 loss: -0.3601827621459961
Batch 37/64 loss: -0.16579151153564453
Batch 38/64 loss: -0.7129096984863281
Batch 39/64 loss: -0.4996671676635742
Batch 40/64 loss: -0.515376091003418
Batch 41/64 loss: -0.4162740707397461
Batch 42/64 loss: -0.38524723052978516
Batch 43/64 loss: -0.21948528289794922
Batch 44/64 loss: -0.6009426116943359
Batch 45/64 loss: -0.6876354217529297
Batch 46/64 loss: -0.6677465438842773
Batch 47/64 loss: -0.3148536682128906
Batch 48/64 loss: -0.3901786804199219
Batch 49/64 loss: -0.6682720184326172
Batch 50/64 loss: -0.560638427734375
Batch 51/64 loss: -0.3228025436401367
Batch 52/64 loss: -0.7821254730224609
Batch 53/64 loss: -0.2456521987915039
Batch 54/64 loss: -0.3489799499511719
Batch 55/64 loss: -0.44979095458984375
Batch 56/64 loss: -0.7751703262329102
Batch 57/64 loss: -0.7140140533447266
Batch 58/64 loss: -0.7525644302368164
Batch 59/64 loss: -0.39292049407958984
Batch 60/64 loss: -0.6420154571533203
Batch 61/64 loss: -0.6827917098999023
Batch 62/64 loss: -0.5977725982666016
Batch 63/64 loss: -0.358428955078125
Batch 64/64 loss: -4.616677284240723
Epoch 349  Train loss: -0.5095247941858628  Val loss: -0.48133711143047947
Epoch 350
-------------------------------
Batch 1/64 loss: -0.5565004348754883
Batch 2/64 loss: -0.47136974334716797
Batch 3/64 loss: -0.9641246795654297
Batch 4/64 loss: -0.559330940246582
Batch 5/64 loss: -0.6989297866821289
Batch 6/64 loss: -0.27999210357666016
Batch 7/64 loss: -0.08083915710449219
Batch 8/64 loss: -0.6537046432495117
Batch 9/64 loss: -0.27771854400634766
Batch 10/64 loss: -0.4415302276611328
Batch 11/64 loss: -0.42322635650634766
Batch 12/64 loss: 0.3343172073364258
Batch 13/64 loss: -0.2613525390625
Batch 14/64 loss: -0.5788125991821289
Batch 15/64 loss: -0.5696010589599609
Batch 16/64 loss: -0.3538932800292969
Batch 17/64 loss: -0.5261268615722656
Batch 18/64 loss: -0.4777402877807617
Batch 19/64 loss: -0.7136468887329102
Batch 20/64 loss: -0.4180927276611328
Batch 21/64 loss: -0.26452064514160156
Batch 22/64 loss: -0.7752647399902344
Batch 23/64 loss: -0.6164369583129883
Batch 24/64 loss: -0.06755352020263672
Batch 25/64 loss: -0.6620512008666992
Batch 26/64 loss: -0.48648738861083984
Batch 27/64 loss: -0.6844310760498047
Batch 28/64 loss: -0.8779697418212891
Batch 29/64 loss: -0.07961750030517578
Batch 30/64 loss: -0.8353481292724609
Batch 31/64 loss: -0.654536247253418
Batch 32/64 loss: -0.4600391387939453
Batch 33/64 loss: -0.47341251373291016
Batch 34/64 loss: -0.3127002716064453
Batch 35/64 loss: -0.38141822814941406
Batch 36/64 loss: -0.3963603973388672
Batch 37/64 loss: -0.33587646484375
Batch 38/64 loss: -0.5683603286743164
Batch 39/64 loss: -0.700653076171875
Batch 40/64 loss: -0.596287727355957
Batch 41/64 loss: -0.4820566177368164
Batch 42/64 loss: -0.30530548095703125
Batch 43/64 loss: -0.35099315643310547
Batch 44/64 loss: -0.6235980987548828
Batch 45/64 loss: -0.38003063201904297
Batch 46/64 loss: -0.14922809600830078
Batch 47/64 loss: -0.5125417709350586
Batch 48/64 loss: -0.7073574066162109
Batch 49/64 loss: -0.6728925704956055
Batch 50/64 loss: -0.889892578125
Batch 51/64 loss: -0.6859149932861328
Batch 52/64 loss: -0.3256387710571289
Batch 53/64 loss: -0.8087835311889648
Batch 54/64 loss: -0.40583133697509766
Batch 55/64 loss: -0.5273227691650391
Batch 56/64 loss: -0.7196998596191406
Batch 57/64 loss: -0.3533010482788086
Batch 58/64 loss: -0.30803585052490234
Batch 59/64 loss: -0.5829153060913086
Batch 60/64 loss: -0.8549604415893555
Batch 61/64 loss: -0.06028938293457031
Batch 62/64 loss: -0.6803407669067383
Batch 63/64 loss: -0.7864885330200195
Batch 64/64 loss: -5.04111385345459
Epoch 350  Train loss: -0.5514634562473671  Val loss: -0.5311141915337736
Saving best model, epoch: 350
Epoch 351
-------------------------------
Batch 1/64 loss: -0.7253446578979492
Batch 2/64 loss: -0.5803585052490234
Batch 3/64 loss: -0.6079988479614258
Batch 4/64 loss: -0.6505470275878906
Batch 5/64 loss: -0.33748435974121094
Batch 6/64 loss: -0.8913917541503906
Batch 7/64 loss: -0.48769664764404297
Batch 8/64 loss: -0.35224246978759766
Batch 9/64 loss: -0.9468669891357422
Batch 10/64 loss: -0.5260171890258789
Batch 11/64 loss: -0.5939970016479492
Batch 12/64 loss: -0.1735992431640625
Batch 13/64 loss: -0.19297313690185547
Batch 14/64 loss: -0.5422677993774414
Batch 15/64 loss: -0.7001934051513672
Batch 16/64 loss: -0.642730712890625
Batch 17/64 loss: -0.7384395599365234
Batch 18/64 loss: -0.5268154144287109
Batch 19/64 loss: -0.5574169158935547
Batch 20/64 loss: -0.5739212036132812
Batch 21/64 loss: -0.34727954864501953
Batch 22/64 loss: -0.5480794906616211
Batch 23/64 loss: -0.5517673492431641
Batch 24/64 loss: -0.5117340087890625
Batch 25/64 loss: -0.5982856750488281
Batch 26/64 loss: -0.46665191650390625
Batch 27/64 loss: -0.49435997009277344
Batch 28/64 loss: -0.513096809387207
Batch 29/64 loss: -0.3513936996459961
Batch 30/64 loss: -0.6022672653198242
Batch 31/64 loss: -0.7397193908691406
Batch 32/64 loss: -0.7119855880737305
Batch 33/64 loss: -0.6593542098999023
Batch 34/64 loss: -0.4488554000854492
Batch 35/64 loss: -0.6135244369506836
Batch 36/64 loss: -0.5389423370361328
Batch 37/64 loss: -0.4702444076538086
Batch 38/64 loss: -0.6387119293212891
Batch 39/64 loss: -0.6109809875488281
Batch 40/64 loss: -0.5407171249389648
Batch 41/64 loss: -0.8496780395507812
Batch 42/64 loss: -0.2673301696777344
Batch 43/64 loss: -0.8873043060302734
Batch 44/64 loss: -0.7863121032714844
Batch 45/64 loss: -0.4495582580566406
Batch 46/64 loss: -0.6972751617431641
Batch 47/64 loss: -0.7111825942993164
Batch 48/64 loss: -0.4998941421508789
Batch 49/64 loss: -0.36573219299316406
Batch 50/64 loss: -0.2905731201171875
Batch 51/64 loss: -0.6369075775146484
Batch 52/64 loss: 0.2012624740600586
Batch 53/64 loss: -0.502720832824707
Batch 54/64 loss: -0.36638355255126953
Batch 55/64 loss: -0.7232046127319336
Batch 56/64 loss: -0.37708377838134766
Batch 57/64 loss: -0.11770868301391602
Batch 58/64 loss: -0.4119844436645508
Batch 59/64 loss: -0.8500432968139648
Batch 60/64 loss: -0.6669149398803711
Batch 61/64 loss: -0.38254737854003906
Batch 62/64 loss: -0.31532764434814453
Batch 63/64 loss: -0.6548309326171875
Batch 64/64 loss: -4.590170860290527
Epoch 351  Train loss: -0.5860096912758023  Val loss: -0.5759706398875443
Saving best model, epoch: 351
Epoch 352
-------------------------------
Batch 1/64 loss: -0.08326911926269531
Batch 2/64 loss: -0.5861291885375977
Batch 3/64 loss: -0.788691520690918
Batch 4/64 loss: -0.7410135269165039
Batch 5/64 loss: -0.4258079528808594
Batch 6/64 loss: -0.8042573928833008
Batch 7/64 loss: -0.8777647018432617
Batch 8/64 loss: -0.7016048431396484
Batch 9/64 loss: -0.2512493133544922
Batch 10/64 loss: -0.6122598648071289
Batch 11/64 loss: -0.6628856658935547
Batch 12/64 loss: -0.8896274566650391
Batch 13/64 loss: -0.6251707077026367
Batch 14/64 loss: -0.6856918334960938
Batch 15/64 loss: -0.7948484420776367
Batch 16/64 loss: -0.6579675674438477
Batch 17/64 loss: -0.8062162399291992
Batch 18/64 loss: -0.17290878295898438
Batch 19/64 loss: -0.6560764312744141
Batch 20/64 loss: -0.7848396301269531
Batch 21/64 loss: -0.6798896789550781
Batch 22/64 loss: -0.9757051467895508
Batch 23/64 loss: -0.6643924713134766
Batch 24/64 loss: -0.4970703125
Batch 25/64 loss: -0.5173063278198242
Batch 26/64 loss: -0.5669689178466797
Batch 27/64 loss: -0.6509552001953125
Batch 28/64 loss: -0.4189176559448242
Batch 29/64 loss: -0.7286596298217773
Batch 30/64 loss: -0.7082624435424805
Batch 31/64 loss: -0.46825313568115234
Batch 32/64 loss: -0.5603170394897461
Batch 33/64 loss: -0.626312255859375
Batch 34/64 loss: -0.4592266082763672
Batch 35/64 loss: -0.3862190246582031
Batch 36/64 loss: -0.5314626693725586
Batch 37/64 loss: -0.45398712158203125
Batch 38/64 loss: -0.4924030303955078
Batch 39/64 loss: -0.29279279708862305
Batch 40/64 loss: -0.6453733444213867
Batch 41/64 loss: -0.8944015502929688
Batch 42/64 loss: -0.6325197219848633
Batch 43/64 loss: -0.3510313034057617
Batch 44/64 loss: -0.34082889556884766
Batch 45/64 loss: -0.5937204360961914
Batch 46/64 loss: -0.3888978958129883
Batch 47/64 loss: -0.6495914459228516
Batch 48/64 loss: -0.4895153045654297
Batch 49/64 loss: -0.4701204299926758
Batch 50/64 loss: -0.4892463684082031
Batch 51/64 loss: -0.011593818664550781
Batch 52/64 loss: -0.5937252044677734
Batch 53/64 loss: -0.823206901550293
Batch 54/64 loss: -0.666651725769043
Batch 55/64 loss: -0.4365243911743164
Batch 56/64 loss: -0.5245389938354492
Batch 57/64 loss: -0.4551219940185547
Batch 58/64 loss: -0.4075345993041992
Batch 59/64 loss: -0.30534839630126953
Batch 60/64 loss: -0.09936380386352539
Batch 61/64 loss: -0.7685632705688477
Batch 62/64 loss: -0.12322807312011719
Batch 63/64 loss: -0.36420345306396484
Batch 64/64 loss: -4.680607318878174
Epoch 352  Train loss: -0.6011402111427456  Val loss: -0.41541977190889445
Epoch 353
-------------------------------
Batch 1/64 loss: -0.6886749267578125
Batch 2/64 loss: -0.5903100967407227
Batch 3/64 loss: -0.34591197967529297
Batch 4/64 loss: -0.3400897979736328
Batch 5/64 loss: -0.2986717224121094
Batch 6/64 loss: -0.3127279281616211
Batch 7/64 loss: -0.6709928512573242
Batch 8/64 loss: -0.6724424362182617
Batch 9/64 loss: -0.6916980743408203
Batch 10/64 loss: -0.3349132537841797
Batch 11/64 loss: -0.4665975570678711
Batch 12/64 loss: -0.8704652786254883
Batch 13/64 loss: -0.29639339447021484
Batch 14/64 loss: -0.42420482635498047
Batch 15/64 loss: 0.015803813934326172
Batch 16/64 loss: -0.3395671844482422
Batch 17/64 loss: -1.0640678405761719
Batch 18/64 loss: -0.38312339782714844
Batch 19/64 loss: -0.48933887481689453
Batch 20/64 loss: -0.4276285171508789
Batch 21/64 loss: -0.1019601821899414
Batch 22/64 loss: -0.5259666442871094
Batch 23/64 loss: -0.4559478759765625
Batch 24/64 loss: -0.40296077728271484
Batch 25/64 loss: -0.4587688446044922
Batch 26/64 loss: -0.6108913421630859
Batch 27/64 loss: -0.755955696105957
Batch 28/64 loss: -0.7411298751831055
Batch 29/64 loss: -0.690948486328125
Batch 30/64 loss: -0.9011430740356445
Batch 31/64 loss: -0.5111255645751953
Batch 32/64 loss: -0.8217439651489258
Batch 33/64 loss: -0.7469244003295898
Batch 34/64 loss: -0.12476348876953125
Batch 35/64 loss: -0.6666698455810547
Batch 36/64 loss: -0.29865360260009766
Batch 37/64 loss: -0.8093280792236328
Batch 38/64 loss: -0.2825045585632324
Batch 39/64 loss: -0.47104549407958984
Batch 40/64 loss: -0.5040988922119141
Batch 41/64 loss: -0.7429428100585938
Batch 42/64 loss: -0.29033756256103516
Batch 43/64 loss: -0.5925359725952148
Batch 44/64 loss: -0.7873544692993164
Batch 45/64 loss: -0.4302864074707031
Batch 46/64 loss: -0.3469200134277344
Batch 47/64 loss: -0.7739171981811523
Batch 48/64 loss: -0.29599475860595703
Batch 49/64 loss: -0.30139732360839844
Batch 50/64 loss: -0.5469198226928711
Batch 51/64 loss: -0.6524944305419922
Batch 52/64 loss: -0.5161561965942383
Batch 53/64 loss: -1.0717363357543945
Batch 54/64 loss: -0.4361457824707031
Batch 55/64 loss: -0.5188827514648438
Batch 56/64 loss: -0.48726844787597656
Batch 57/64 loss: -0.3920907974243164
Batch 58/64 loss: -0.6342592239379883
Batch 59/64 loss: -0.7836627960205078
Batch 60/64 loss: -0.7129812240600586
Batch 61/64 loss: -0.6992301940917969
Batch 62/64 loss: -0.7308349609375
Batch 63/64 loss: -0.4646463394165039
Batch 64/64 loss: -4.910933494567871
Epoch 353  Train loss: -0.5877136118271772  Val loss: -0.624480191784626
Saving best model, epoch: 353
Epoch 354
-------------------------------
Batch 1/64 loss: -0.43641090393066406
Batch 2/64 loss: -0.8787765502929688
Batch 3/64 loss: -0.37164306640625
Batch 4/64 loss: -0.9124460220336914
Batch 5/64 loss: -0.5925817489624023
Batch 6/64 loss: -0.2821187973022461
Batch 7/64 loss: -0.21477699279785156
Batch 8/64 loss: -0.6860122680664062
Batch 9/64 loss: -0.43567943572998047
Batch 10/64 loss: -0.48285865783691406
Batch 11/64 loss: -0.7279233932495117
Batch 12/64 loss: -0.7750759124755859
Batch 13/64 loss: -0.6309404373168945
Batch 14/64 loss: -0.531672477722168
Batch 15/64 loss: -0.47210693359375
Batch 16/64 loss: -0.5392446517944336
Batch 17/64 loss: -0.7159595489501953
Batch 18/64 loss: -0.6928186416625977
Batch 19/64 loss: -0.1827096939086914
Batch 20/64 loss: -0.6737403869628906
Batch 21/64 loss: -0.9200973510742188
Batch 22/64 loss: -0.6563034057617188
Batch 23/64 loss: -0.15222835540771484
Batch 24/64 loss: -0.6193075180053711
Batch 25/64 loss: -0.28394126892089844
Batch 26/64 loss: -0.5606346130371094
Batch 27/64 loss: -0.6196622848510742
Batch 28/64 loss: -0.4710578918457031
Batch 29/64 loss: -0.5636672973632812
Batch 30/64 loss: -0.811976432800293
Batch 31/64 loss: -0.7747335433959961
Batch 32/64 loss: -0.8898229598999023
Batch 33/64 loss: -0.6570701599121094
Batch 34/64 loss: -0.7475290298461914
Batch 35/64 loss: -0.8256759643554688
Batch 36/64 loss: -0.7142915725708008
Batch 37/64 loss: -0.614781379699707
Batch 38/64 loss: -0.9766445159912109
Batch 39/64 loss: -0.6904573440551758
Batch 40/64 loss: -0.6322917938232422
Batch 41/64 loss: -0.8427505493164062
Batch 42/64 loss: -0.6076736450195312
Batch 43/64 loss: -0.6304235458374023
Batch 44/64 loss: -0.5746822357177734
Batch 45/64 loss: -0.49425792694091797
Batch 46/64 loss: -0.5528440475463867
Batch 47/64 loss: -0.4914979934692383
Batch 48/64 loss: -0.6959199905395508
Batch 49/64 loss: -0.8394527435302734
Batch 50/64 loss: -0.4428730010986328
Batch 51/64 loss: -0.5651941299438477
Batch 52/64 loss: -0.6080598831176758
Batch 53/64 loss: -0.19200992584228516
Batch 54/64 loss: -0.6406126022338867
Batch 55/64 loss: -0.5755701065063477
Batch 56/64 loss: -0.7383947372436523
Batch 57/64 loss: -0.7132434844970703
Batch 58/64 loss: -0.7900152206420898
Batch 59/64 loss: -0.3453035354614258
Batch 60/64 loss: -0.7875747680664062
Batch 61/64 loss: -0.5157203674316406
Batch 62/64 loss: -0.4617910385131836
Batch 63/64 loss: -0.6184024810791016
Batch 64/64 loss: -4.66475248336792
Epoch 354  Train loss: -0.6531843690311208  Val loss: -0.14727066144910464
Epoch 355
-------------------------------
Batch 1/64 loss: 0.09306812286376953
Batch 2/64 loss: -0.7231464385986328
Batch 3/64 loss: -0.7587108612060547
Batch 4/64 loss: -0.8906002044677734
Batch 5/64 loss: -0.5561866760253906
Batch 6/64 loss: -0.6829652786254883
Batch 7/64 loss: -0.7879047393798828
Batch 8/64 loss: -0.7500581741333008
Batch 9/64 loss: -0.7623138427734375
Batch 10/64 loss: -0.5920820236206055
Batch 11/64 loss: -0.8112754821777344
Batch 12/64 loss: -0.6781444549560547
Batch 13/64 loss: -0.8629627227783203
Batch 14/64 loss: -0.47640419006347656
Batch 15/64 loss: -0.579376220703125
Batch 16/64 loss: -0.27347755432128906
Batch 17/64 loss: -0.6034975051879883
Batch 18/64 loss: -0.8657779693603516
Batch 19/64 loss: -0.538334846496582
Batch 20/64 loss: 0.0030460357666015625
Batch 21/64 loss: -0.6614236831665039
Batch 22/64 loss: -0.5010919570922852
Batch 23/64 loss: -0.49802684783935547
Batch 24/64 loss: -0.2448863983154297
Batch 25/64 loss: -0.34257984161376953
Batch 26/64 loss: -0.5724620819091797
Batch 27/64 loss: -0.2862844467163086
Batch 28/64 loss: -0.8616876602172852
Batch 29/64 loss: -0.35187816619873047
Batch 30/64 loss: -0.5319766998291016
Batch 31/64 loss: -0.5514612197875977
Batch 32/64 loss: -0.2591085433959961
Batch 33/64 loss: -0.5860576629638672
Batch 34/64 loss: -0.7753047943115234
Batch 35/64 loss: -0.20875883102416992
Batch 36/64 loss: -0.746551513671875
Batch 37/64 loss: -0.3787975311279297
Batch 38/64 loss: -0.5684871673583984
Batch 39/64 loss: -0.45398712158203125
Batch 40/64 loss: -0.3541545867919922
Batch 41/64 loss: -0.8042354583740234
Batch 42/64 loss: -0.39891719818115234
Batch 43/64 loss: -0.7129220962524414
Batch 44/64 loss: -0.7920141220092773
Batch 45/64 loss: -0.5530738830566406
Batch 46/64 loss: -0.9356527328491211
Batch 47/64 loss: -0.5332670211791992
Batch 48/64 loss: -0.23648834228515625
Batch 49/64 loss: -0.5631637573242188
Batch 50/64 loss: -0.3325948715209961
Batch 51/64 loss: -0.18744373321533203
Batch 52/64 loss: -0.8093976974487305
Batch 53/64 loss: -0.2262864112854004
Batch 54/64 loss: -0.3837919235229492
Batch 55/64 loss: -0.5289821624755859
Batch 56/64 loss: -0.3419771194458008
Batch 57/64 loss: -0.32011985778808594
Batch 58/64 loss: -0.6648092269897461
Batch 59/64 loss: -0.8351631164550781
Batch 60/64 loss: -0.4522113800048828
Batch 61/64 loss: -0.8228645324707031
Batch 62/64 loss: -0.2835569381713867
Batch 63/64 loss: -0.5784158706665039
Batch 64/64 loss: -3.8625378608703613
Epoch 355  Train loss: -0.5808050922319001  Val loss: -0.5265551694889659
Epoch 356
-------------------------------
Batch 1/64 loss: -0.7629728317260742
Batch 2/64 loss: -0.724949836730957
Batch 3/64 loss: -0.46707725524902344
Batch 4/64 loss: -0.7422466278076172
Batch 5/64 loss: -0.5512838363647461
Batch 6/64 loss: -0.34157466888427734
Batch 7/64 loss: -0.43543052673339844
Batch 8/64 loss: -0.8408050537109375
Batch 9/64 loss: -0.5879926681518555
Batch 10/64 loss: -0.4213991165161133
Batch 11/64 loss: -0.6831426620483398
Batch 12/64 loss: -0.4780454635620117
Batch 13/64 loss: -0.8366575241088867
Batch 14/64 loss: -0.7168970108032227
Batch 15/64 loss: -0.23992347717285156
Batch 16/64 loss: -0.7155590057373047
Batch 17/64 loss: -0.8619108200073242
Batch 18/64 loss: -0.7880773544311523
Batch 19/64 loss: -0.4925813674926758
Batch 20/64 loss: -0.7027044296264648
Batch 21/64 loss: -0.5603828430175781
Batch 22/64 loss: -0.5598440170288086
Batch 23/64 loss: -0.814640998840332
Batch 24/64 loss: -0.5937881469726562
Batch 25/64 loss: -0.5246429443359375
Batch 26/64 loss: -0.6992168426513672
Batch 27/64 loss: -0.7971992492675781
Batch 28/64 loss: -0.6707010269165039
Batch 29/64 loss: -0.47686004638671875
Batch 30/64 loss: -0.6950044631958008
Batch 31/64 loss: 0.17661285400390625
Batch 32/64 loss: -0.37206363677978516
Batch 33/64 loss: -0.6497764587402344
Batch 34/64 loss: -0.5588922500610352
Batch 35/64 loss: -0.4618854522705078
Batch 36/64 loss: -0.8275594711303711
Batch 37/64 loss: -0.7587089538574219
Batch 38/64 loss: -0.36951255798339844
Batch 39/64 loss: -0.13434648513793945
Batch 40/64 loss: -0.7811269760131836
Batch 41/64 loss: -0.6542463302612305
Batch 42/64 loss: -0.7707738876342773
Batch 43/64 loss: -0.42476654052734375
Batch 44/64 loss: -0.49991703033447266
Batch 45/64 loss: -0.4347562789916992
Batch 46/64 loss: -0.7101869583129883
Batch 47/64 loss: -0.6470947265625
Batch 48/64 loss: -0.33208370208740234
Batch 49/64 loss: -0.2946147918701172
Batch 50/64 loss: -0.5839614868164062
Batch 51/64 loss: -0.6541824340820312
Batch 52/64 loss: -0.05754566192626953
Batch 53/64 loss: -0.53125
Batch 54/64 loss: -0.9624004364013672
Batch 55/64 loss: -0.3103752136230469
Batch 56/64 loss: -0.4096822738647461
Batch 57/64 loss: -0.36882877349853516
Batch 58/64 loss: -0.37935638427734375
Batch 59/64 loss: -0.38570594787597656
Batch 60/64 loss: -0.6964035034179688
Batch 61/64 loss: -0.10962486267089844
Batch 62/64 loss: -0.6156492233276367
Batch 63/64 loss: -0.7192745208740234
Batch 64/64 loss: -5.027176380157471
Epoch 356  Train loss: -0.6093150550243901  Val loss: -0.29219609683321923
Epoch 357
-------------------------------
Batch 1/64 loss: -0.5086021423339844
Batch 2/64 loss: -0.28383827209472656
Batch 3/64 loss: -0.4316530227661133
Batch 4/64 loss: -0.06718158721923828
Batch 5/64 loss: -0.4951286315917969
Batch 6/64 loss: -0.7253303527832031
Batch 7/64 loss: -0.5613937377929688
Batch 8/64 loss: -0.5464859008789062
Batch 9/64 loss: -0.5279054641723633
Batch 10/64 loss: -0.20769786834716797
Batch 11/64 loss: -0.4169425964355469
Batch 12/64 loss: -0.8592557907104492
Batch 13/64 loss: -0.5377101898193359
Batch 14/64 loss: -0.5020647048950195
Batch 15/64 loss: -0.8086338043212891
Batch 16/64 loss: -0.42284488677978516
Batch 17/64 loss: -0.918004035949707
Batch 18/64 loss: -0.764190673828125
Batch 19/64 loss: -0.4051933288574219
Batch 20/64 loss: -0.4702777862548828
Batch 21/64 loss: -0.6022357940673828
Batch 22/64 loss: -0.6971988677978516
Batch 23/64 loss: -0.6217432022094727
Batch 24/64 loss: -0.6227273941040039
Batch 25/64 loss: -0.7295045852661133
Batch 26/64 loss: -0.8381729125976562
Batch 27/64 loss: -0.8520545959472656
Batch 28/64 loss: -0.7838582992553711
Batch 29/64 loss: -0.5842809677124023
Batch 30/64 loss: -0.6207141876220703
Batch 31/64 loss: -0.7394542694091797
Batch 32/64 loss: -0.554234504699707
Batch 33/64 loss: -0.5956153869628906
Batch 34/64 loss: -0.2852811813354492
Batch 35/64 loss: -0.405120849609375
Batch 36/64 loss: -0.3834524154663086
Batch 37/64 loss: -0.8973731994628906
Batch 38/64 loss: -0.6355152130126953
Batch 39/64 loss: 0.6460247039794922
Batch 40/64 loss: -0.32787227630615234
Batch 41/64 loss: -0.6115741729736328
Batch 42/64 loss: -0.4930896759033203
Batch 43/64 loss: -0.4900693893432617
Batch 44/64 loss: -0.6549606323242188
Batch 45/64 loss: -0.6113014221191406
Batch 46/64 loss: -0.19074058532714844
Batch 47/64 loss: 0.42252445220947266
Batch 48/64 loss: -0.8820381164550781
Batch 49/64 loss: -0.9013185501098633
Batch 50/64 loss: -0.7246904373168945
Batch 51/64 loss: -0.45246028900146484
Batch 52/64 loss: -0.5451946258544922
Batch 53/64 loss: -0.6532983779907227
Batch 54/64 loss: -0.2842588424682617
Batch 55/64 loss: 1.0333776473999023
Batch 56/64 loss: -0.5903844833374023
Batch 57/64 loss: -0.2582855224609375
Batch 58/64 loss: -0.20878314971923828
Batch 59/64 loss: -0.14852046966552734
Batch 60/64 loss: -0.3508920669555664
Batch 61/64 loss: -0.4887819290161133
Batch 62/64 loss: -0.28818607330322266
Batch 63/64 loss: -0.23426342010498047
Batch 64/64 loss: -4.447502613067627
Epoch 357  Train loss: -0.5260162708806057  Val loss: -0.058219293548479115
Epoch 358
-------------------------------
Batch 1/64 loss: -0.2888460159301758
Batch 2/64 loss: -0.5042181015014648
Batch 3/64 loss: -0.3249855041503906
Batch 4/64 loss: -0.02257823944091797
Batch 5/64 loss: -0.4996147155761719
Batch 6/64 loss: -0.31530189514160156
Batch 7/64 loss: 0.5824298858642578
Batch 8/64 loss: -0.6950168609619141
Batch 9/64 loss: -0.16891002655029297
Batch 10/64 loss: -0.5310440063476562
Batch 11/64 loss: -0.01860523223876953
Batch 12/64 loss: -0.21518421173095703
Batch 13/64 loss: -0.5928754806518555
Batch 14/64 loss: 0.22596263885498047
Batch 15/64 loss: -0.4313316345214844
Batch 16/64 loss: -0.042244911193847656
Batch 17/64 loss: -0.14658832550048828
Batch 18/64 loss: -0.4085092544555664
Batch 19/64 loss: -0.3257770538330078
Batch 20/64 loss: -0.4811563491821289
Batch 21/64 loss: -0.5030651092529297
Batch 22/64 loss: -0.40219879150390625
Batch 23/64 loss: -0.6901760101318359
Batch 24/64 loss: -0.7752962112426758
Batch 25/64 loss: -0.4298553466796875
Batch 26/64 loss: -0.34724903106689453
Batch 27/64 loss: -0.28737831115722656
Batch 28/64 loss: -0.7502126693725586
Batch 29/64 loss: -0.4617738723754883
Batch 30/64 loss: -0.1109762191772461
Batch 31/64 loss: -0.4110546112060547
Batch 32/64 loss: -0.6785469055175781
Batch 33/64 loss: -0.45925426483154297
Batch 34/64 loss: -0.11259651184082031
Batch 35/64 loss: -0.58953857421875
Batch 36/64 loss: -0.2766456604003906
Batch 37/64 loss: -0.7104969024658203
Batch 38/64 loss: -0.6885166168212891
Batch 39/64 loss: -0.6358261108398438
Batch 40/64 loss: -0.6468687057495117
Batch 41/64 loss: -0.8910589218139648
Batch 42/64 loss: -0.6412105560302734
Batch 43/64 loss: -0.6847124099731445
Batch 44/64 loss: -0.3387174606323242
Batch 45/64 loss: -0.5564641952514648
Batch 46/64 loss: -0.6870326995849609
Batch 47/64 loss: -0.5601606369018555
Batch 48/64 loss: -0.060352325439453125
Batch 49/64 loss: -0.5102930068969727
Batch 50/64 loss: -0.4700641632080078
Batch 51/64 loss: -0.7588710784912109
Batch 52/64 loss: 0.15721845626831055
Batch 53/64 loss: -0.4790821075439453
Batch 54/64 loss: -0.6155433654785156
Batch 55/64 loss: -0.4299497604370117
Batch 56/64 loss: -0.5418624877929688
Batch 57/64 loss: 0.6949925422668457
Batch 58/64 loss: -0.5261058807373047
Batch 59/64 loss: -0.5986166000366211
Batch 60/64 loss: -0.8700599670410156
Batch 61/64 loss: -0.35071754455566406
Batch 62/64 loss: -0.6044788360595703
Batch 63/64 loss: -0.6521854400634766
Batch 64/64 loss: -4.673752307891846
Epoch 358  Train loss: -0.4651382651983523  Val loss: 0.4413271900714468
Epoch 359
-------------------------------
Batch 1/64 loss: -0.2635231018066406
Batch 2/64 loss: -0.4589691162109375
Batch 3/64 loss: -0.5561017990112305
Batch 4/64 loss: 0.13748836517333984
Batch 5/64 loss: -0.4811286926269531
Batch 6/64 loss: -0.5683574676513672
Batch 7/64 loss: -0.8348703384399414
Batch 8/64 loss: -0.6734523773193359
Batch 9/64 loss: -0.5826168060302734
Batch 10/64 loss: -0.26878929138183594
Batch 11/64 loss: -0.20830345153808594
Batch 12/64 loss: -0.4222421646118164
Batch 13/64 loss: -0.8580875396728516
Batch 14/64 loss: -0.788599967956543
Batch 15/64 loss: -0.43691158294677734
Batch 16/64 loss: -0.1584644317626953
Batch 17/64 loss: -0.7957773208618164
Batch 18/64 loss: -0.6003141403198242
Batch 19/64 loss: -0.7657089233398438
Batch 20/64 loss: -0.4486427307128906
Batch 21/64 loss: -0.8699445724487305
Batch 22/64 loss: -0.4294710159301758
Batch 23/64 loss: -0.2962322235107422
Batch 24/64 loss: -0.557469367980957
Batch 25/64 loss: -0.5607519149780273
Batch 26/64 loss: -0.8169059753417969
Batch 27/64 loss: -0.6804313659667969
Batch 28/64 loss: -1.1128253936767578
Batch 29/64 loss: -0.4354219436645508
Batch 30/64 loss: -0.8834781646728516
Batch 31/64 loss: -0.7946596145629883
Batch 32/64 loss: -0.4103250503540039
Batch 33/64 loss: -0.6269998550415039
Batch 34/64 loss: -0.14077520370483398
Batch 35/64 loss: 0.11990737915039062
Batch 36/64 loss: -0.6324739456176758
Batch 37/64 loss: -0.3749246597290039
Batch 38/64 loss: -0.6162958145141602
Batch 39/64 loss: -0.4683542251586914
Batch 40/64 loss: -0.5435209274291992
Batch 41/64 loss: -0.3207416534423828
Batch 42/64 loss: -0.5576801300048828
Batch 43/64 loss: -0.7627887725830078
Batch 44/64 loss: 0.09870147705078125
Batch 45/64 loss: -0.7375822067260742
Batch 46/64 loss: -0.8494939804077148
Batch 47/64 loss: -0.5780954360961914
Batch 48/64 loss: -0.6428937911987305
Batch 49/64 loss: -0.7253894805908203
Batch 50/64 loss: -0.31467628479003906
Batch 51/64 loss: 0.13159942626953125
Batch 52/64 loss: -0.8022966384887695
Batch 53/64 loss: -0.4960803985595703
Batch 54/64 loss: -0.09945106506347656
Batch 55/64 loss: -0.5123758316040039
Batch 56/64 loss: -0.5872974395751953
Batch 57/64 loss: -0.5780925750732422
Batch 58/64 loss: -0.81939697265625
Batch 59/64 loss: -0.8548870086669922
Batch 60/64 loss: -0.5332841873168945
Batch 61/64 loss: -0.39926624298095703
Batch 62/64 loss: -0.4596977233886719
Batch 63/64 loss: -0.817723274230957
Batch 64/64 loss: -4.251555919647217
Epoch 359  Train loss: -0.5736828841415106  Val loss: -0.5211268159532055
Epoch 360
-------------------------------
Batch 1/64 loss: -0.6861104965209961
Batch 2/64 loss: -0.6122627258300781
Batch 3/64 loss: -0.818791389465332
Batch 4/64 loss: -0.1646890640258789
Batch 5/64 loss: -0.39024829864501953
Batch 6/64 loss: -0.5460319519042969
Batch 7/64 loss: -0.6772022247314453
Batch 8/64 loss: -0.5735740661621094
Batch 9/64 loss: 0.008554458618164062
Batch 10/64 loss: -0.7633152008056641
Batch 11/64 loss: -0.12680625915527344
Batch 12/64 loss: -0.9593029022216797
Batch 13/64 loss: -0.8095579147338867
Batch 14/64 loss: -0.1945018768310547
Batch 15/64 loss: -0.5787220001220703
Batch 16/64 loss: -0.33646440505981445
Batch 17/64 loss: -0.6027860641479492
Batch 18/64 loss: -0.7553920745849609
Batch 19/64 loss: -0.5417537689208984
Batch 20/64 loss: -0.4282798767089844
Batch 21/64 loss: -0.6479501724243164
Batch 22/64 loss: -0.7760725021362305
Batch 23/64 loss: 0.23648643493652344
Batch 24/64 loss: -0.49316978454589844
Batch 25/64 loss: -0.6147518157958984
Batch 26/64 loss: -0.042507171630859375
Batch 27/64 loss: -0.8169097900390625
Batch 28/64 loss: -0.7188711166381836
Batch 29/64 loss: -0.11806106567382812
Batch 30/64 loss: -0.29512977600097656
Batch 31/64 loss: -0.5495510101318359
Batch 32/64 loss: -0.8085947036743164
Batch 33/64 loss: -0.8108205795288086
Batch 34/64 loss: -0.6640405654907227
Batch 35/64 loss: -0.9300556182861328
Batch 36/64 loss: -0.7766780853271484
Batch 37/64 loss: -0.4817085266113281
Batch 38/64 loss: -0.9381504058837891
Batch 39/64 loss: -0.7446861267089844
Batch 40/64 loss: -0.45890045166015625
Batch 41/64 loss: -0.5807867050170898
Batch 42/64 loss: -0.6851034164428711
Batch 43/64 loss: -0.8373651504516602
Batch 44/64 loss: -0.8438224792480469
Batch 45/64 loss: -0.8161325454711914
Batch 46/64 loss: -0.32676219940185547
Batch 47/64 loss: -0.3162984848022461
Batch 48/64 loss: -0.6030645370483398
Batch 49/64 loss: -0.32242488861083984
Batch 50/64 loss: -0.989959716796875
Batch 51/64 loss: -0.8027715682983398
Batch 52/64 loss: -0.6713056564331055
Batch 53/64 loss: -0.8895759582519531
Batch 54/64 loss: -0.7617588043212891
Batch 55/64 loss: -0.1836838722229004
Batch 56/64 loss: -0.7974348068237305
Batch 57/64 loss: -0.43764591217041016
Batch 58/64 loss: -0.4994192123413086
Batch 59/64 loss: -0.2551450729370117
Batch 60/64 loss: -0.7850179672241211
Batch 61/64 loss: -0.8785724639892578
Batch 62/64 loss: -0.7043533325195312
Batch 63/64 loss: -0.5027828216552734
Batch 64/64 loss: -4.877255439758301
Epoch 360  Train loss: -0.6299057268628887  Val loss: -0.5858052964882343
Epoch 361
-------------------------------
Batch 1/64 loss: -0.4698343276977539
Batch 2/64 loss: -0.8825139999389648
Batch 3/64 loss: -0.4723854064941406
Batch 4/64 loss: -0.4742155075073242
Batch 5/64 loss: -0.2457895278930664
Batch 6/64 loss: -0.7560720443725586
Batch 7/64 loss: -0.5281438827514648
Batch 8/64 loss: -0.3473625183105469
Batch 9/64 loss: -0.663726806640625
Batch 10/64 loss: -0.4520988464355469
Batch 11/64 loss: -0.6130218505859375
Batch 12/64 loss: -0.6686372756958008
Batch 13/64 loss: -0.7063169479370117
Batch 14/64 loss: -0.6939964294433594
Batch 15/64 loss: -0.9000520706176758
Batch 16/64 loss: -0.5266885757446289
Batch 17/64 loss: -0.3113288879394531
Batch 18/64 loss: -0.6568088531494141
Batch 19/64 loss: -0.6771841049194336
Batch 20/64 loss: -0.7079696655273438
Batch 21/64 loss: -0.8476476669311523
Batch 22/64 loss: -0.6301851272583008
Batch 23/64 loss: -0.9874544143676758
Batch 24/64 loss: -0.5911188125610352
Batch 25/64 loss: -0.5763759613037109
Batch 26/64 loss: -0.4237241744995117
Batch 27/64 loss: -0.5181159973144531
Batch 28/64 loss: -0.48250770568847656
Batch 29/64 loss: -0.4820404052734375
Batch 30/64 loss: -0.8636569976806641
Batch 31/64 loss: -0.4354534149169922
Batch 32/64 loss: -0.9688825607299805
Batch 33/64 loss: -0.6010990142822266
Batch 34/64 loss: -0.5269870758056641
Batch 35/64 loss: -0.6583328247070312
Batch 36/64 loss: -0.3210906982421875
Batch 37/64 loss: -0.9003849029541016
Batch 38/64 loss: -0.4399585723876953
Batch 39/64 loss: -0.7319393157958984
Batch 40/64 loss: -0.6311550140380859
Batch 41/64 loss: -0.4816398620605469
Batch 42/64 loss: -0.5240840911865234
Batch 43/64 loss: -0.40320491790771484
Batch 44/64 loss: -0.748011589050293
Batch 45/64 loss: 0.022971153259277344
Batch 46/64 loss: -0.6345281600952148
Batch 47/64 loss: -0.5477209091186523
Batch 48/64 loss: -0.7470283508300781
Batch 49/64 loss: -0.8280735015869141
Batch 50/64 loss: -0.6921424865722656
Batch 51/64 loss: -0.6879844665527344
Batch 52/64 loss: -0.46922779083251953
Batch 53/64 loss: -0.9659910202026367
Batch 54/64 loss: -0.9021835327148438
Batch 55/64 loss: -0.5648365020751953
Batch 56/64 loss: -0.5691442489624023
Batch 57/64 loss: -0.5934963226318359
Batch 58/64 loss: -0.8009891510009766
Batch 59/64 loss: -0.8882484436035156
Batch 60/64 loss: -0.30614566802978516
Batch 61/64 loss: -0.4208841323852539
Batch 62/64 loss: -0.25518798828125
Batch 63/64 loss: -0.04059934616088867
Batch 64/64 loss: -4.743379592895508
Epoch 361  Train loss: -0.6427948895622703  Val loss: -0.6333440538124531
Saving best model, epoch: 361
Epoch 362
-------------------------------
Batch 1/64 loss: -0.889378547668457
Batch 2/64 loss: -0.5846138000488281
Batch 3/64 loss: -0.37871551513671875
Batch 4/64 loss: -0.7587127685546875
Batch 5/64 loss: -0.717808723449707
Batch 6/64 loss: -0.43741703033447266
Batch 7/64 loss: -0.8408355712890625
Batch 8/64 loss: -0.9572820663452148
Batch 9/64 loss: -0.620640754699707
Batch 10/64 loss: -0.4999732971191406
Batch 11/64 loss: -0.6256113052368164
Batch 12/64 loss: -0.5895776748657227
Batch 13/64 loss: -0.5812520980834961
Batch 14/64 loss: -0.4222984313964844
Batch 15/64 loss: -0.4705324172973633
Batch 16/64 loss: -0.8486614227294922
Batch 17/64 loss: -0.856903076171875
Batch 18/64 loss: -0.6956205368041992
Batch 19/64 loss: -0.8628950119018555
Batch 20/64 loss: -0.8860225677490234
Batch 21/64 loss: -0.46278858184814453
Batch 22/64 loss: -0.014371871948242188
Batch 23/64 loss: -0.4698667526245117
Batch 24/64 loss: -0.9942207336425781
Batch 25/64 loss: -0.8097953796386719
Batch 26/64 loss: -0.5537605285644531
Batch 27/64 loss: -0.8328828811645508
Batch 28/64 loss: -0.7687578201293945
Batch 29/64 loss: -0.7224063873291016
Batch 30/64 loss: -0.7587966918945312
Batch 31/64 loss: -0.6706790924072266
Batch 32/64 loss: -0.5213394165039062
Batch 33/64 loss: -1.0329294204711914
Batch 34/64 loss: -0.5482149124145508
Batch 35/64 loss: -0.847020149230957
Batch 36/64 loss: -0.8032903671264648
Batch 37/64 loss: -0.7688503265380859
Batch 38/64 loss: -0.5358905792236328
Batch 39/64 loss: -0.9294090270996094
Batch 40/64 loss: -0.7213821411132812
Batch 41/64 loss: -0.1131429672241211
Batch 42/64 loss: -1.1057424545288086
Batch 43/64 loss: -0.2187356948852539
Batch 44/64 loss: -0.6331186294555664
Batch 45/64 loss: -0.6638555526733398
Batch 46/64 loss: -0.8764266967773438
Batch 47/64 loss: -0.0033655166625976562
Batch 48/64 loss: -0.12510251998901367
Batch 49/64 loss: -0.720606803894043
Batch 50/64 loss: -0.16615533828735352
Batch 51/64 loss: -0.5053005218505859
Batch 52/64 loss: -0.5915632247924805
Batch 53/64 loss: -0.8050765991210938
Batch 54/64 loss: -0.8381052017211914
Batch 55/64 loss: -0.4052438735961914
Batch 56/64 loss: -0.1688709259033203
Batch 57/64 loss: -0.5628252029418945
Batch 58/64 loss: -0.18412160873413086
Batch 59/64 loss: -0.4584989547729492
Batch 60/64 loss: -0.6611137390136719
Batch 61/64 loss: -0.669581413269043
Batch 62/64 loss: -0.6114969253540039
Batch 63/64 loss: -0.42206382751464844
Batch 64/64 loss: -3.9115805625915527
Epoch 362  Train loss: -0.6546698869443407  Val loss: 1.5903730621862249
Epoch 363
-------------------------------
Batch 1/64 loss: -0.15205764770507812
Batch 2/64 loss: -0.3929157257080078
Batch 3/64 loss: -0.5864677429199219
Batch 4/64 loss: -0.4080314636230469
Batch 5/64 loss: -0.3230743408203125
Batch 6/64 loss: -0.7263879776000977
Batch 7/64 loss: -0.967625617980957
Batch 8/64 loss: -0.8064022064208984
Batch 9/64 loss: 0.06819009780883789
Batch 10/64 loss: -0.8772821426391602
Batch 11/64 loss: -0.4756202697753906
Batch 12/64 loss: -0.5006113052368164
Batch 13/64 loss: -0.4367790222167969
Batch 14/64 loss: -0.5034465789794922
Batch 15/64 loss: -0.4488677978515625
Batch 16/64 loss: -0.6601600646972656
Batch 17/64 loss: -0.5759544372558594
Batch 18/64 loss: -0.25573015213012695
Batch 19/64 loss: -0.8198661804199219
Batch 20/64 loss: -0.5783662796020508
Batch 21/64 loss: -0.6829462051391602
Batch 22/64 loss: -0.31731367111206055
Batch 23/64 loss: -0.2702951431274414
Batch 24/64 loss: -0.29573917388916016
Batch 25/64 loss: -0.7228450775146484
Batch 26/64 loss: -0.8339414596557617
Batch 27/64 loss: -0.5380573272705078
Batch 28/64 loss: -0.8626937866210938
Batch 29/64 loss: -0.3194718360900879
Batch 30/64 loss: -0.23340988159179688
Batch 31/64 loss: -0.40524959564208984
Batch 32/64 loss: -0.7596883773803711
Batch 33/64 loss: -0.25878238677978516
Batch 34/64 loss: -0.45522594451904297
Batch 35/64 loss: -0.569218635559082
Batch 36/64 loss: -0.6992597579956055
Batch 37/64 loss: -0.4897575378417969
Batch 38/64 loss: -0.20322418212890625
Batch 39/64 loss: -0.3741798400878906
Batch 40/64 loss: -0.31977272033691406
Batch 41/64 loss: -0.7175750732421875
Batch 42/64 loss: -0.5981388092041016
Batch 43/64 loss: -0.3622932434082031
Batch 44/64 loss: -0.26226329803466797
Batch 45/64 loss: -0.6719999313354492
Batch 46/64 loss: -0.5730409622192383
Batch 47/64 loss: -0.7667760848999023
Batch 48/64 loss: -0.7373085021972656
Batch 49/64 loss: -0.561741828918457
Batch 50/64 loss: -0.27469921112060547
Batch 51/64 loss: -0.8260965347290039
Batch 52/64 loss: -0.6155214309692383
Batch 53/64 loss: -0.7993621826171875
Batch 54/64 loss: -0.29619455337524414
Batch 55/64 loss: -0.6643466949462891
Batch 56/64 loss: -0.5814838409423828
Batch 57/64 loss: -0.9555568695068359
Batch 58/64 loss: -0.41425323486328125
Batch 59/64 loss: -0.618408203125
Batch 60/64 loss: -0.7593288421630859
Batch 61/64 loss: -0.842198371887207
Batch 62/64 loss: -0.43402862548828125
Batch 63/64 loss: -0.4671297073364258
Batch 64/64 loss: -4.99314022064209
Epoch 363  Train loss: -0.5895393034991097  Val loss: -0.5996511531449675
Epoch 364
-------------------------------
Batch 1/64 loss: -0.6896762847900391
Batch 2/64 loss: -0.7424907684326172
Batch 3/64 loss: -0.7149429321289062
Batch 4/64 loss: -0.26190853118896484
Batch 5/64 loss: -0.6484794616699219
Batch 6/64 loss: -0.7282657623291016
Batch 7/64 loss: -0.18454742431640625
Batch 8/64 loss: -0.2194538116455078
Batch 9/64 loss: -0.6140737533569336
Batch 10/64 loss: -0.01439666748046875
Batch 11/64 loss: -0.7702388763427734
Batch 12/64 loss: -0.5158252716064453
Batch 13/64 loss: -0.6225366592407227
Batch 14/64 loss: -0.3124542236328125
Batch 15/64 loss: -0.5419111251831055
Batch 16/64 loss: -0.22026634216308594
Batch 17/64 loss: -0.7897872924804688
Batch 18/64 loss: -0.3849630355834961
Batch 19/64 loss: -0.7289180755615234
Batch 20/64 loss: -0.6904134750366211
Batch 21/64 loss: -0.7919435501098633
Batch 22/64 loss: -0.7623481750488281
Batch 23/64 loss: -0.5191640853881836
Batch 24/64 loss: -0.4670877456665039
Batch 25/64 loss: -0.759974479675293
Batch 26/64 loss: -0.5231714248657227
Batch 27/64 loss: -0.7721452713012695
Batch 28/64 loss: -1.0420169830322266
Batch 29/64 loss: -0.18115615844726562
Batch 30/64 loss: -0.7901229858398438
Batch 31/64 loss: -0.4554147720336914
Batch 32/64 loss: -0.5572175979614258
Batch 33/64 loss: -0.6817684173583984
Batch 34/64 loss: -0.6362714767456055
Batch 35/64 loss: -0.43103599548339844
Batch 36/64 loss: -0.6606473922729492
Batch 37/64 loss: -0.9273414611816406
Batch 38/64 loss: -0.6573085784912109
Batch 39/64 loss: -0.8259029388427734
Batch 40/64 loss: -0.6288213729858398
Batch 41/64 loss: -0.658024787902832
Batch 42/64 loss: -0.5213766098022461
Batch 43/64 loss: -0.6404085159301758
Batch 44/64 loss: -0.7088346481323242
Batch 45/64 loss: -0.6964797973632812
Batch 46/64 loss: -0.7977991104125977
Batch 47/64 loss: -0.2499713897705078
Batch 48/64 loss: -1.0147991180419922
Batch 49/64 loss: -0.29883575439453125
Batch 50/64 loss: -0.7278003692626953
Batch 51/64 loss: -0.8173580169677734
Batch 52/64 loss: -0.6460275650024414
Batch 53/64 loss: -0.6406326293945312
Batch 54/64 loss: -0.6248750686645508
Batch 55/64 loss: -0.7592840194702148
Batch 56/64 loss: 0.020928382873535156
Batch 57/64 loss: -0.6354656219482422
Batch 58/64 loss: -0.44239044189453125
Batch 59/64 loss: -0.8302946090698242
Batch 60/64 loss: -0.6314201354980469
Batch 61/64 loss: -0.44727230072021484
Batch 62/64 loss: -0.7370214462280273
Batch 63/64 loss: -0.3912992477416992
Batch 64/64 loss: -4.118927955627441
Epoch 364  Train loss: -0.6345466576370539  Val loss: -0.5734046726292351
Epoch 365
-------------------------------
Batch 1/64 loss: -0.8390979766845703
Batch 2/64 loss: -0.4491004943847656
Batch 3/64 loss: -0.45567798614501953
Batch 4/64 loss: -0.3548126220703125
Batch 5/64 loss: -0.17220306396484375
Batch 6/64 loss: -0.06610107421875
Batch 7/64 loss: -0.7041101455688477
Batch 8/64 loss: -0.46067333221435547
Batch 9/64 loss: -0.6784162521362305
Batch 10/64 loss: -0.6133937835693359
Batch 11/64 loss: -0.5255336761474609
Batch 12/64 loss: -0.3828277587890625
Batch 13/64 loss: -0.4218568801879883
Batch 14/64 loss: -0.728907585144043
Batch 15/64 loss: -0.7144804000854492
Batch 16/64 loss: -0.20463275909423828
Batch 17/64 loss: -0.7303876876831055
Batch 18/64 loss: -0.16904973983764648
Batch 19/64 loss: -0.24322032928466797
Batch 20/64 loss: -0.600886344909668
Batch 21/64 loss: -0.5208683013916016
Batch 22/64 loss: -0.6685667037963867
Batch 23/64 loss: -0.7755575180053711
Batch 24/64 loss: -0.8426265716552734
Batch 25/64 loss: -0.22957992553710938
Batch 26/64 loss: -0.5870513916015625
Batch 27/64 loss: -0.4460010528564453
Batch 28/64 loss: -0.5733747482299805
Batch 29/64 loss: -0.5796899795532227
Batch 30/64 loss: -0.3368234634399414
Batch 31/64 loss: -0.516871452331543
Batch 32/64 loss: -0.6753644943237305
Batch 33/64 loss: -0.7729511260986328
Batch 34/64 loss: -0.7874164581298828
Batch 35/64 loss: -0.4477500915527344
Batch 36/64 loss: -1.0697860717773438
Batch 37/64 loss: -0.9150457382202148
Batch 38/64 loss: -0.6680259704589844
Batch 39/64 loss: -0.7709369659423828
Batch 40/64 loss: -0.45327091217041016
Batch 41/64 loss: -1.0314264297485352
Batch 42/64 loss: -0.6711893081665039
Batch 43/64 loss: -0.5664739608764648
Batch 44/64 loss: -0.777552604675293
Batch 45/64 loss: -0.5919580459594727
Batch 46/64 loss: -0.6391220092773438
Batch 47/64 loss: -0.5445947647094727
Batch 48/64 loss: -0.5997333526611328
Batch 49/64 loss: -0.9832229614257812
Batch 50/64 loss: -0.5903863906860352
Batch 51/64 loss: -0.6976909637451172
Batch 52/64 loss: -0.2520880699157715
Batch 53/64 loss: -0.8898210525512695
Batch 54/64 loss: -0.8234615325927734
Batch 55/64 loss: -0.5784673690795898
Batch 56/64 loss: -0.9108133316040039
Batch 57/64 loss: -0.7419672012329102
Batch 58/64 loss: -0.6851377487182617
Batch 59/64 loss: -0.6955127716064453
Batch 60/64 loss: -0.9036140441894531
Batch 61/64 loss: -0.5045080184936523
Batch 62/64 loss: -0.7476034164428711
Batch 63/64 loss: -0.8387451171875
Batch 64/64 loss: -4.667168140411377
Epoch 365  Train loss: -0.657543457255644  Val loss: -0.6451452589526618
Saving best model, epoch: 365
Epoch 366
-------------------------------
Batch 1/64 loss: -0.7545700073242188
Batch 2/64 loss: -0.8140945434570312
Batch 3/64 loss: -0.6940212249755859
Batch 4/64 loss: -0.9053668975830078
Batch 5/64 loss: -0.5667352676391602
Batch 6/64 loss: -0.8042688369750977
Batch 7/64 loss: -0.42060184478759766
Batch 8/64 loss: -0.8576478958129883
Batch 9/64 loss: -0.7678670883178711
Batch 10/64 loss: -0.7864990234375
Batch 11/64 loss: -0.3122749328613281
Batch 12/64 loss: -0.37848949432373047
Batch 13/64 loss: -0.9369192123413086
Batch 14/64 loss: -0.8175630569458008
Batch 15/64 loss: -0.3368568420410156
Batch 16/64 loss: -0.5103635787963867
Batch 17/64 loss: -0.7415666580200195
Batch 18/64 loss: -0.7018013000488281
Batch 19/64 loss: -0.5524969100952148
Batch 20/64 loss: -0.6308784484863281
Batch 21/64 loss: -0.8871212005615234
Batch 22/64 loss: -0.7939844131469727
Batch 23/64 loss: -0.5416660308837891
Batch 24/64 loss: -1.0337657928466797
Batch 25/64 loss: -1.0000553131103516
Batch 26/64 loss: -0.6288690567016602
Batch 27/64 loss: -0.5861186981201172
Batch 28/64 loss: -0.49280357360839844
Batch 29/64 loss: -0.477752685546875
Batch 30/64 loss: -0.5402116775512695
Batch 31/64 loss: -0.5063543319702148
Batch 32/64 loss: -0.3410367965698242
Batch 33/64 loss: -0.8998661041259766
Batch 34/64 loss: -0.5720548629760742
Batch 35/64 loss: -0.9659833908081055
Batch 36/64 loss: -0.3914833068847656
Batch 37/64 loss: -0.6693687438964844
Batch 38/64 loss: -0.3532075881958008
Batch 39/64 loss: 0.11974048614501953
Batch 40/64 loss: -0.41229724884033203
Batch 41/64 loss: -0.8549938201904297
Batch 42/64 loss: -0.6640005111694336
Batch 43/64 loss: -0.9317293167114258
Batch 44/64 loss: -0.8281383514404297
Batch 45/64 loss: -0.544520378112793
Batch 46/64 loss: -0.6529254913330078
Batch 47/64 loss: -0.4717226028442383
Batch 48/64 loss: -0.6473674774169922
Batch 49/64 loss: -0.67205810546875
Batch 50/64 loss: -0.6786661148071289
Batch 51/64 loss: -0.7866592407226562
Batch 52/64 loss: -0.7932710647583008
Batch 53/64 loss: -0.44825267791748047
Batch 54/64 loss: -0.7797174453735352
Batch 55/64 loss: -0.711151123046875
Batch 56/64 loss: -0.48925113677978516
Batch 57/64 loss: -0.4577960968017578
Batch 58/64 loss: -0.1152796745300293
Batch 59/64 loss: -0.7895135879516602
Batch 60/64 loss: -0.7758665084838867
Batch 61/64 loss: -0.8088617324829102
Batch 62/64 loss: -0.5958957672119141
Batch 63/64 loss: -0.9408321380615234
Batch 64/64 loss: -4.737248420715332
Epoch 366  Train loss: -0.6942203933117437  Val loss: -0.6503692574517423
Saving best model, epoch: 366
Epoch 367
-------------------------------
Batch 1/64 loss: -0.6143894195556641
Batch 2/64 loss: -0.8501768112182617
Batch 3/64 loss: -0.9837541580200195
Batch 4/64 loss: -0.5130071640014648
Batch 5/64 loss: -0.5696430206298828
Batch 6/64 loss: -0.5294895172119141
Batch 7/64 loss: -0.2942466735839844
Batch 8/64 loss: -0.7033233642578125
Batch 9/64 loss: -0.6979436874389648
Batch 10/64 loss: -0.8296318054199219
Batch 11/64 loss: -0.7848577499389648
Batch 12/64 loss: -0.8480463027954102
Batch 13/64 loss: -0.7087574005126953
Batch 14/64 loss: -0.7463493347167969
Batch 15/64 loss: -0.6071500778198242
Batch 16/64 loss: -0.8973875045776367
Batch 17/64 loss: -0.6727714538574219
Batch 18/64 loss: -0.6640329360961914
Batch 19/64 loss: -0.8397531509399414
Batch 20/64 loss: -0.8844184875488281
Batch 21/64 loss: -0.30092525482177734
Batch 22/64 loss: -0.7552680969238281
Batch 23/64 loss: -0.7487897872924805
Batch 24/64 loss: -0.9780550003051758
Batch 25/64 loss: -0.25797462463378906
Batch 26/64 loss: -0.1615438461303711
Batch 27/64 loss: -0.39928627014160156
Batch 28/64 loss: -0.6165790557861328
Batch 29/64 loss: -0.7258138656616211
Batch 30/64 loss: -0.9235162734985352
Batch 31/64 loss: -0.6561641693115234
Batch 32/64 loss: -0.5430173873901367
Batch 33/64 loss: -0.8353805541992188
Batch 34/64 loss: -0.7041025161743164
Batch 35/64 loss: -0.16350889205932617
Batch 36/64 loss: -0.6950168609619141
Batch 37/64 loss: -0.6785955429077148
Batch 38/64 loss: -0.2110452651977539
Batch 39/64 loss: -0.9146814346313477
Batch 40/64 loss: -0.5819845199584961
Batch 41/64 loss: -0.33966732025146484
Batch 42/64 loss: -0.7285089492797852
Batch 43/64 loss: -0.3747272491455078
Batch 44/64 loss: -0.508021354675293
Batch 45/64 loss: -0.4511260986328125
Batch 46/64 loss: -0.03914642333984375
Batch 47/64 loss: -0.823481559753418
Batch 48/64 loss: -0.754119873046875
Batch 49/64 loss: -0.22799015045166016
Batch 50/64 loss: -0.35114383697509766
Batch 51/64 loss: -0.6526584625244141
Batch 52/64 loss: -0.7613115310668945
Batch 53/64 loss: -0.5953340530395508
Batch 54/64 loss: -0.0705099105834961
Batch 55/64 loss: -0.3708920478820801
Batch 56/64 loss: -1.0152788162231445
Batch 57/64 loss: -0.7993698120117188
Batch 58/64 loss: -0.4871377944946289
Batch 59/64 loss: -0.8007087707519531
Batch 60/64 loss: -0.3395729064941406
Batch 61/64 loss: -0.6690282821655273
Batch 62/64 loss: -0.6612081527709961
Batch 63/64 loss: -0.357572078704834
Batch 64/64 loss: -4.226479530334473
Epoch 367  Train loss: -0.6500196756101122  Val loss: -0.39960246397457583
Epoch 368
-------------------------------
Batch 1/64 loss: -0.4601287841796875
Batch 2/64 loss: -0.5904617309570312
Batch 3/64 loss: -0.2063312530517578
Batch 4/64 loss: -0.796661376953125
Batch 5/64 loss: -0.9394054412841797
Batch 6/64 loss: -0.27343177795410156
Batch 7/64 loss: -0.8382596969604492
Batch 8/64 loss: -0.42339420318603516
Batch 9/64 loss: -0.10147857666015625
Batch 10/64 loss: -0.4665679931640625
Batch 11/64 loss: -0.9663715362548828
Batch 12/64 loss: -0.7258539199829102
Batch 13/64 loss: -0.7408294677734375
Batch 14/64 loss: -0.5890626907348633
Batch 15/64 loss: -0.16324234008789062
Batch 16/64 loss: -0.6803112030029297
Batch 17/64 loss: -0.6357202529907227
Batch 18/64 loss: -0.7110357284545898
Batch 19/64 loss: -0.226470947265625
Batch 20/64 loss: -0.7244119644165039
Batch 21/64 loss: -0.7092733383178711
Batch 22/64 loss: -0.5573234558105469
Batch 23/64 loss: -0.6311712265014648
Batch 24/64 loss: -0.10416650772094727
Batch 25/64 loss: -0.5725116729736328
Batch 26/64 loss: -0.30644989013671875
Batch 27/64 loss: -0.8099193572998047
Batch 28/64 loss: -0.8478689193725586
Batch 29/64 loss: -1.0045795440673828
Batch 30/64 loss: -0.48421764373779297
Batch 31/64 loss: -1.0153388977050781
Batch 32/64 loss: -0.6538419723510742
Batch 33/64 loss: -0.498779296875
Batch 34/64 loss: -0.5415735244750977
Batch 35/64 loss: -0.659912109375
Batch 36/64 loss: -0.6839237213134766
Batch 37/64 loss: -0.17956066131591797
Batch 38/64 loss: -0.42927074432373047
Batch 39/64 loss: -0.8544454574584961
Batch 40/64 loss: -0.8477277755737305
Batch 41/64 loss: -0.3175663948059082
Batch 42/64 loss: -0.5413618087768555
Batch 43/64 loss: -0.9709663391113281
Batch 44/64 loss: -0.4736928939819336
Batch 45/64 loss: -0.39719295501708984
Batch 46/64 loss: -0.6693525314331055
Batch 47/64 loss: -1.0286140441894531
Batch 48/64 loss: -0.5957412719726562
Batch 49/64 loss: -0.7705841064453125
Batch 50/64 loss: -0.5393638610839844
Batch 51/64 loss: -0.5389509201049805
Batch 52/64 loss: -0.7587852478027344
Batch 53/64 loss: -0.6235113143920898
Batch 54/64 loss: -0.3025178909301758
Batch 55/64 loss: -0.5744466781616211
Batch 56/64 loss: -0.2799863815307617
Batch 57/64 loss: -0.8050718307495117
Batch 58/64 loss: -0.5569000244140625
Batch 59/64 loss: -0.5492429733276367
Batch 60/64 loss: -1.0725889205932617
Batch 61/64 loss: -0.47589588165283203
Batch 62/64 loss: 0.031978607177734375
Batch 63/64 loss: -0.4729022979736328
Batch 64/64 loss: -4.666684627532959
Epoch 368  Train loss: -0.6342675770030303  Val loss: -0.5390736688043654
Epoch 369
-------------------------------
Batch 1/64 loss: -0.5305366516113281
Batch 2/64 loss: -0.8913660049438477
Batch 3/64 loss: -0.7478466033935547
Batch 4/64 loss: -0.8337240219116211
Batch 5/64 loss: -0.6790857315063477
Batch 6/64 loss: -0.39105224609375
Batch 7/64 loss: -0.8467798233032227
Batch 8/64 loss: -0.5546340942382812
Batch 9/64 loss: -0.4235820770263672
Batch 10/64 loss: -0.6828098297119141
Batch 11/64 loss: -0.6533470153808594
Batch 12/64 loss: -0.43241405487060547
Batch 13/64 loss: -0.46219825744628906
Batch 14/64 loss: -0.48740673065185547
Batch 15/64 loss: -0.8377895355224609
Batch 16/64 loss: -0.3932485580444336
Batch 17/64 loss: -0.8367366790771484
Batch 18/64 loss: -0.8073244094848633
Batch 19/64 loss: -0.4679727554321289
Batch 20/64 loss: -0.6867580413818359
Batch 21/64 loss: -0.02154541015625
Batch 22/64 loss: -1.0005674362182617
Batch 23/64 loss: -0.5331640243530273
Batch 24/64 loss: -0.3660707473754883
Batch 25/64 loss: -0.4456472396850586
Batch 26/64 loss: -0.4402599334716797
Batch 27/64 loss: -0.6914796829223633
Batch 28/64 loss: -0.615992546081543
Batch 29/64 loss: -0.35892486572265625
Batch 30/64 loss: -0.6522560119628906
Batch 31/64 loss: -0.6325044631958008
Batch 32/64 loss: -0.2881498336791992
Batch 33/64 loss: -0.6084165573120117
Batch 34/64 loss: -0.7039699554443359
Batch 35/64 loss: -0.8042201995849609
Batch 36/64 loss: -0.4726676940917969
Batch 37/64 loss: -0.6342458724975586
Batch 38/64 loss: -0.641880989074707
Batch 39/64 loss: -0.5907812118530273
Batch 40/64 loss: -0.49425601959228516
Batch 41/64 loss: -0.8214130401611328
Batch 42/64 loss: -0.34732532501220703
Batch 43/64 loss: -0.5615520477294922
Batch 44/64 loss: -0.32712745666503906
Batch 45/64 loss: -1.240523338317871
Batch 46/64 loss: -0.661407470703125
Batch 47/64 loss: -0.938603401184082
Batch 48/64 loss: -0.6221294403076172
Batch 49/64 loss: -0.44155120849609375
Batch 50/64 loss: -0.6853475570678711
Batch 51/64 loss: -0.8058948516845703
Batch 52/64 loss: -0.5504436492919922
Batch 53/64 loss: -0.5869665145874023
Batch 54/64 loss: -0.8977184295654297
Batch 55/64 loss: -0.7842521667480469
Batch 56/64 loss: -0.7447748184204102
Batch 57/64 loss: -0.833683967590332
Batch 58/64 loss: -0.7420816421508789
Batch 59/64 loss: -0.4115753173828125
Batch 60/64 loss: -0.9141855239868164
Batch 61/64 loss: -0.6660785675048828
Batch 62/64 loss: -0.5482559204101562
Batch 63/64 loss: -0.2996683120727539
Batch 64/64 loss: -4.9893059730529785
Epoch 369  Train loss: -0.6716573060727586  Val loss: -0.5699557143797989
Epoch 370
-------------------------------
Batch 1/64 loss: -0.6834220886230469
Batch 2/64 loss: -0.7634916305541992
Batch 3/64 loss: 0.11162853240966797
Batch 4/64 loss: -0.6642999649047852
Batch 5/64 loss: -0.7145013809204102
Batch 6/64 loss: -0.8258466720581055
Batch 7/64 loss: -1.0130510330200195
Batch 8/64 loss: -0.5308084487915039
Batch 9/64 loss: -0.7259664535522461
Batch 10/64 loss: -0.49698543548583984
Batch 11/64 loss: -0.5659770965576172
Batch 12/64 loss: -0.535771369934082
Batch 13/64 loss: -0.7109498977661133
Batch 14/64 loss: -0.7861919403076172
Batch 15/64 loss: -0.6242752075195312
Batch 16/64 loss: -0.6395301818847656
Batch 17/64 loss: -0.6246891021728516
Batch 18/64 loss: -0.49295806884765625
Batch 19/64 loss: -0.28861141204833984
Batch 20/64 loss: -0.9513978958129883
Batch 21/64 loss: -0.7588357925415039
Batch 22/64 loss: -0.47573280334472656
Batch 23/64 loss: -0.8323450088500977
Batch 24/64 loss: -0.2899351119995117
Batch 25/64 loss: -0.4947834014892578
Batch 26/64 loss: -0.6372604370117188
Batch 27/64 loss: -0.3647308349609375
Batch 28/64 loss: -0.9628438949584961
Batch 29/64 loss: -0.9799861907958984
Batch 30/64 loss: -0.5635662078857422
Batch 31/64 loss: -0.8733997344970703
Batch 32/64 loss: -0.5942878723144531
Batch 33/64 loss: -0.2397451400756836
Batch 34/64 loss: -0.9268789291381836
Batch 35/64 loss: -0.8299884796142578
Batch 36/64 loss: -0.9618759155273438
Batch 37/64 loss: -0.5924015045166016
Batch 38/64 loss: -0.5235052108764648
Batch 39/64 loss: -0.4979991912841797
Batch 40/64 loss: -0.6802444458007812
Batch 41/64 loss: -0.5895462036132812
Batch 42/64 loss: -0.6345682144165039
Batch 43/64 loss: -0.6026115417480469
Batch 44/64 loss: -0.5994930267333984
Batch 45/64 loss: -0.7223930358886719
Batch 46/64 loss: -0.7673053741455078
Batch 47/64 loss: -0.651158332824707
Batch 48/64 loss: -0.5792245864868164
Batch 49/64 loss: -1.1122655868530273
Batch 50/64 loss: -0.5714530944824219
Batch 51/64 loss: -0.7085599899291992
Batch 52/64 loss: -0.6941442489624023
Batch 53/64 loss: -1.0568618774414062
Batch 54/64 loss: -0.34966468811035156
Batch 55/64 loss: -0.8443260192871094
Batch 56/64 loss: -0.6762161254882812
Batch 57/64 loss: -0.9223537445068359
Batch 58/64 loss: -1.154097557067871
Batch 59/64 loss: -0.6105222702026367
Batch 60/64 loss: -0.49141407012939453
Batch 61/64 loss: -0.7276248931884766
Batch 62/64 loss: -0.4748401641845703
Batch 63/64 loss: -0.6661500930786133
Batch 64/64 loss: -5.11674165725708
Epoch 370  Train loss: -0.716106570000742  Val loss: -0.5935678252649471
Epoch 371
-------------------------------
Batch 1/64 loss: -0.34534358978271484
Batch 2/64 loss: -0.7739019393920898
Batch 3/64 loss: -0.5918960571289062
Batch 4/64 loss: -0.48968982696533203
Batch 5/64 loss: -0.9672145843505859
Batch 6/64 loss: -0.6864767074584961
Batch 7/64 loss: -0.5609455108642578
Batch 8/64 loss: -0.5367288589477539
Batch 9/64 loss: -0.5173749923706055
Batch 10/64 loss: -0.6331291198730469
Batch 11/64 loss: -0.8490877151489258
Batch 12/64 loss: -0.6705589294433594
Batch 13/64 loss: -0.9016857147216797
Batch 14/64 loss: -0.8155908584594727
Batch 15/64 loss: -0.15265750885009766
Batch 16/64 loss: -0.6391868591308594
Batch 17/64 loss: -0.42792320251464844
Batch 18/64 loss: -0.7485876083374023
Batch 19/64 loss: -0.7770929336547852
Batch 20/64 loss: -0.6263866424560547
Batch 21/64 loss: -0.4441404342651367
Batch 22/64 loss: -0.5917291641235352
Batch 23/64 loss: -0.6221256256103516
Batch 24/64 loss: -0.1463613510131836
Batch 25/64 loss: -0.7074670791625977
Batch 26/64 loss: -0.8510560989379883
Batch 27/64 loss: -0.6636781692504883
Batch 28/64 loss: -0.7650384902954102
Batch 29/64 loss: -0.7564182281494141
Batch 30/64 loss: -0.8444614410400391
Batch 31/64 loss: -0.8088197708129883
Batch 32/64 loss: -0.6301069259643555
Batch 33/64 loss: -0.8367385864257812
Batch 34/64 loss: -0.7210044860839844
Batch 35/64 loss: -0.9454889297485352
Batch 36/64 loss: -0.8017215728759766
Batch 37/64 loss: -0.8091640472412109
Batch 38/64 loss: -1.0161142349243164
Batch 39/64 loss: 0.1554865837097168
Batch 40/64 loss: -0.5389575958251953
Batch 41/64 loss: -0.5387725830078125
Batch 42/64 loss: -0.9080667495727539
Batch 43/64 loss: -0.971156120300293
Batch 44/64 loss: -0.7072267532348633
Batch 45/64 loss: -0.6482810974121094
Batch 46/64 loss: -0.9214677810668945
Batch 47/64 loss: -0.7956485748291016
Batch 48/64 loss: -0.6969013214111328
Batch 49/64 loss: -0.30809497833251953
Batch 50/64 loss: -0.24129581451416016
Batch 51/64 loss: -0.8018388748168945
Batch 52/64 loss: -0.5647964477539062
Batch 53/64 loss: -1.1637849807739258
Batch 54/64 loss: -1.0324010848999023
Batch 55/64 loss: -1.0265531539916992
Batch 56/64 loss: -0.5445880889892578
Batch 57/64 loss: -0.9170169830322266
Batch 58/64 loss: -1.0284185409545898
Batch 59/64 loss: -0.9817800521850586
Batch 60/64 loss: -0.7100563049316406
Batch 61/64 loss: -0.9688444137573242
Batch 62/64 loss: -1.0493097305297852
Batch 63/64 loss: -0.686406135559082
Batch 64/64 loss: -4.619024276733398
Epoch 371  Train loss: -0.7487614051968443  Val loss: -0.637154923271887
Epoch 372
-------------------------------
Batch 1/64 loss: -0.7297029495239258
Batch 2/64 loss: -0.7175283432006836
Batch 3/64 loss: -0.50799560546875
Batch 4/64 loss: -0.5102014541625977
Batch 5/64 loss: -0.5399904251098633
Batch 6/64 loss: -0.3095846176147461
Batch 7/64 loss: -0.5438995361328125
Batch 8/64 loss: -0.9690952301025391
Batch 9/64 loss: -0.40315914154052734
Batch 10/64 loss: -0.5442056655883789
Batch 11/64 loss: -0.6385164260864258
Batch 12/64 loss: -0.8043489456176758
Batch 13/64 loss: -0.5850515365600586
Batch 14/64 loss: -0.3055543899536133
Batch 15/64 loss: -0.573277473449707
Batch 16/64 loss: -0.3264760971069336
Batch 17/64 loss: -0.33568572998046875
Batch 18/64 loss: -0.49727916717529297
Batch 19/64 loss: -0.5776538848876953
Batch 20/64 loss: -0.567143440246582
Batch 21/64 loss: -0.6577053070068359
Batch 22/64 loss: -0.4444904327392578
Batch 23/64 loss: -1.0286474227905273
Batch 24/64 loss: -0.28569698333740234
Batch 25/64 loss: -0.31610107421875
Batch 26/64 loss: -0.8879871368408203
Batch 27/64 loss: -0.8320455551147461
Batch 28/64 loss: -0.8475322723388672
Batch 29/64 loss: -0.566584587097168
Batch 30/64 loss: -0.6975364685058594
Batch 31/64 loss: -1.0898218154907227
Batch 32/64 loss: -0.9621763229370117
Batch 33/64 loss: -0.9829006195068359
Batch 34/64 loss: -0.7931632995605469
Batch 35/64 loss: -0.7656650543212891
Batch 36/64 loss: -0.6465940475463867
Batch 37/64 loss: -0.7245082855224609
Batch 38/64 loss: -0.739171028137207
Batch 39/64 loss: -0.7900323867797852
Batch 40/64 loss: -0.8588838577270508
Batch 41/64 loss: -0.7596597671508789
Batch 42/64 loss: -1.0266141891479492
Batch 43/64 loss: -0.6895465850830078
Batch 44/64 loss: -0.8480539321899414
Batch 45/64 loss: -0.8140802383422852
Batch 46/64 loss: -0.8290004730224609
Batch 47/64 loss: -0.9900751113891602
Batch 48/64 loss: -1.0108938217163086
Batch 49/64 loss: -0.5035743713378906
Batch 50/64 loss: -0.5302305221557617
Batch 51/64 loss: -0.6439962387084961
Batch 52/64 loss: -0.528019905090332
Batch 53/64 loss: -1.0176572799682617
Batch 54/64 loss: -0.7886686325073242
Batch 55/64 loss: -0.6134681701660156
Batch 56/64 loss: -0.35865020751953125
Batch 57/64 loss: 0.3442220687866211
Batch 58/64 loss: -0.928584098815918
Batch 59/64 loss: -0.7248001098632812
Batch 60/64 loss: -0.7148370742797852
Batch 61/64 loss: -0.6371698379516602
Batch 62/64 loss: -0.5450973510742188
Batch 63/64 loss: -0.6566085815429688
Batch 64/64 loss: -4.960330486297607
Epoch 372  Train loss: -0.7127671578351189  Val loss: -0.6069640064567225
Epoch 373
-------------------------------
Batch 1/64 loss: -0.5454597473144531
Batch 2/64 loss: -0.2158527374267578
Batch 3/64 loss: -0.8703784942626953
Batch 4/64 loss: -0.7342863082885742
Batch 5/64 loss: -0.203125
Batch 6/64 loss: -0.5512847900390625
Batch 7/64 loss: -0.8671932220458984
Batch 8/64 loss: -0.7656364440917969
Batch 9/64 loss: -0.6925544738769531
Batch 10/64 loss: -0.48923587799072266
Batch 11/64 loss: -0.7372045516967773
Batch 12/64 loss: -0.6750526428222656
Batch 13/64 loss: -0.6625251770019531
Batch 14/64 loss: -0.8684844970703125
Batch 15/64 loss: -0.6611537933349609
Batch 16/64 loss: -0.3924980163574219
Batch 17/64 loss: -0.7108497619628906
Batch 18/64 loss: -0.6122560501098633
Batch 19/64 loss: -0.7958211898803711
Batch 20/64 loss: -0.06326866149902344
Batch 21/64 loss: -0.5567626953125
Batch 22/64 loss: -0.9624528884887695
Batch 23/64 loss: -0.5332221984863281
Batch 24/64 loss: -0.21355867385864258
Batch 25/64 loss: -0.32216548919677734
Batch 26/64 loss: -0.7870817184448242
Batch 27/64 loss: -0.3365011215209961
Batch 28/64 loss: -0.8816032409667969
Batch 29/64 loss: -0.7854127883911133
Batch 30/64 loss: -0.8610763549804688
Batch 31/64 loss: -0.839350700378418
Batch 32/64 loss: -0.5739679336547852
Batch 33/64 loss: -0.7383403778076172
Batch 34/64 loss: -0.8426141738891602
Batch 35/64 loss: -0.946476936340332
Batch 36/64 loss: -1.0119028091430664
Batch 37/64 loss: -0.7445259094238281
Batch 38/64 loss: -0.46828651428222656
Batch 39/64 loss: -0.7427997589111328
Batch 40/64 loss: -0.882664680480957
Batch 41/64 loss: -1.0282831192016602
Batch 42/64 loss: -0.8295392990112305
Batch 43/64 loss: -0.6433925628662109
Batch 44/64 loss: -0.501948356628418
Batch 45/64 loss: -0.5421142578125
Batch 46/64 loss: -1.1979751586914062
Batch 47/64 loss: -0.6550664901733398
Batch 48/64 loss: -0.5147914886474609
Batch 49/64 loss: -0.7262077331542969
Batch 50/64 loss: -0.7196283340454102
Batch 51/64 loss: -0.38666248321533203
Batch 52/64 loss: -0.6267690658569336
Batch 53/64 loss: -0.948760986328125
Batch 54/64 loss: -0.6725606918334961
Batch 55/64 loss: -1.132756233215332
Batch 56/64 loss: -0.7108755111694336
Batch 57/64 loss: -0.8407573699951172
Batch 58/64 loss: -0.8324356079101562
Batch 59/64 loss: -0.5507955551147461
Batch 60/64 loss: -0.6935205459594727
Batch 61/64 loss: -0.7704935073852539
Batch 62/64 loss: -0.6197748184204102
Batch 63/64 loss: -0.6307649612426758
Batch 64/64 loss: -5.108833312988281
Epoch 373  Train loss: -0.7333707846847235  Val loss: -0.7102414947195151
Saving best model, epoch: 373
Epoch 374
-------------------------------
Batch 1/64 loss: -0.6483230590820312
Batch 2/64 loss: -0.576848030090332
Batch 3/64 loss: 0.01064300537109375
Batch 4/64 loss: -0.8181123733520508
Batch 5/64 loss: -0.1751842498779297
Batch 6/64 loss: -0.34997081756591797
Batch 7/64 loss: -0.6975593566894531
Batch 8/64 loss: -0.8077154159545898
Batch 9/64 loss: -0.7932510375976562
Batch 10/64 loss: -0.6208934783935547
Batch 11/64 loss: -0.5381984710693359
Batch 12/64 loss: -0.5756549835205078
Batch 13/64 loss: -0.7034883499145508
Batch 14/64 loss: -0.31209754943847656
Batch 15/64 loss: -0.9153070449829102
Batch 16/64 loss: -0.7167577743530273
Batch 17/64 loss: -0.8016119003295898
Batch 18/64 loss: -0.3456583023071289
Batch 19/64 loss: -0.7441005706787109
Batch 20/64 loss: -0.4950590133666992
Batch 21/64 loss: -0.7059001922607422
Batch 22/64 loss: -0.6326971054077148
Batch 23/64 loss: -0.8852710723876953
Batch 24/64 loss: -0.7115297317504883
Batch 25/64 loss: 0.12034225463867188
Batch 26/64 loss: -0.7685489654541016
Batch 27/64 loss: -0.5786218643188477
Batch 28/64 loss: -0.5235404968261719
Batch 29/64 loss: -0.6512432098388672
Batch 30/64 loss: -0.7792329788208008
Batch 31/64 loss: -0.5929193496704102
Batch 32/64 loss: -0.23401451110839844
Batch 33/64 loss: -0.3501777648925781
Batch 34/64 loss: -0.6266508102416992
Batch 35/64 loss: -0.4207429885864258
Batch 36/64 loss: -0.7541027069091797
Batch 37/64 loss: -0.6511287689208984
Batch 38/64 loss: -0.5356645584106445
Batch 39/64 loss: -0.8672914505004883
Batch 40/64 loss: -0.5956668853759766
Batch 41/64 loss: -0.9043254852294922
Batch 42/64 loss: -0.630640983581543
Batch 43/64 loss: -0.9336328506469727
Batch 44/64 loss: -0.5947599411010742
Batch 45/64 loss: -0.5932979583740234
Batch 46/64 loss: -0.8712921142578125
Batch 47/64 loss: -0.44391441345214844
Batch 48/64 loss: -0.6415767669677734
Batch 49/64 loss: -0.25453758239746094
Batch 50/64 loss: -0.6846532821655273
Batch 51/64 loss: -0.47472667694091797
Batch 52/64 loss: -0.8874616622924805
Batch 53/64 loss: -0.8503866195678711
Batch 54/64 loss: -0.4829883575439453
Batch 55/64 loss: -0.43346595764160156
Batch 56/64 loss: -0.8213281631469727
Batch 57/64 loss: -0.8876476287841797
Batch 58/64 loss: -0.4490938186645508
Batch 59/64 loss: -0.6963529586791992
Batch 60/64 loss: -0.7467060089111328
Batch 61/64 loss: -0.6898822784423828
Batch 62/64 loss: -0.6194925308227539
Batch 63/64 loss: -0.9126148223876953
Batch 64/64 loss: -4.636168479919434
Epoch 374  Train loss: -0.6643397200341318  Val loss: -0.49445011361767743
Epoch 375
-------------------------------
Batch 1/64 loss: -0.6980438232421875
Batch 2/64 loss: -0.5284147262573242
Batch 3/64 loss: -0.8608760833740234
Batch 4/64 loss: -0.5819387435913086
Batch 5/64 loss: -0.4042778015136719
Batch 6/64 loss: -1.0907793045043945
Batch 7/64 loss: -0.43415069580078125
Batch 8/64 loss: -0.44567298889160156
Batch 9/64 loss: -0.8834209442138672
Batch 10/64 loss: -0.5994367599487305
Batch 11/64 loss: -0.7061080932617188
Batch 12/64 loss: -0.5479183197021484
Batch 13/64 loss: -0.7354984283447266
Batch 14/64 loss: -0.7688808441162109
Batch 15/64 loss: -0.7934255599975586
Batch 16/64 loss: -0.7890253067016602
Batch 17/64 loss: -1.0099287033081055
Batch 18/64 loss: -0.6735506057739258
Batch 19/64 loss: -0.9165773391723633
Batch 20/64 loss: -0.671025276184082
Batch 21/64 loss: 0.020239830017089844
Batch 22/64 loss: -0.5996379852294922
Batch 23/64 loss: -0.5747337341308594
Batch 24/64 loss: 0.1794881820678711
Batch 25/64 loss: -0.6608076095581055
Batch 26/64 loss: -0.6335000991821289
Batch 27/64 loss: -0.4652366638183594
Batch 28/64 loss: -0.7805271148681641
Batch 29/64 loss: -0.45891857147216797
Batch 30/64 loss: -0.7549057006835938
Batch 31/64 loss: -0.43973255157470703
Batch 32/64 loss: -0.6819982528686523
Batch 33/64 loss: -0.2258586883544922
Batch 34/64 loss: -0.2688770294189453
Batch 35/64 loss: -0.58544921875
Batch 36/64 loss: -0.2277212142944336
Batch 37/64 loss: -0.6494655609130859
Batch 38/64 loss: -0.4504203796386719
Batch 39/64 loss: -0.3485298156738281
Batch 40/64 loss: -0.5747551918029785
Batch 41/64 loss: -0.8659820556640625
Batch 42/64 loss: -0.519744873046875
Batch 43/64 loss: -0.3429417610168457
Batch 44/64 loss: -0.3932652473449707
Batch 45/64 loss: -0.2968921661376953
Batch 46/64 loss: -0.4558877944946289
Batch 47/64 loss: -0.6833419799804688
Batch 48/64 loss: -0.7172565460205078
Batch 49/64 loss: -0.3511228561401367
Batch 50/64 loss: -0.6849393844604492
Batch 51/64 loss: -0.4499835968017578
Batch 52/64 loss: -0.6304807662963867
Batch 53/64 loss: -0.7266941070556641
Batch 54/64 loss: -0.5299472808837891
Batch 55/64 loss: -0.7731151580810547
Batch 56/64 loss: -0.41798877716064453
Batch 57/64 loss: -0.196136474609375
Batch 58/64 loss: -0.21851444244384766
Batch 59/64 loss: -0.47466564178466797
Batch 60/64 loss: -0.4487152099609375
Batch 61/64 loss: -0.7703685760498047
Batch 62/64 loss: -0.35472869873046875
Batch 63/64 loss: -0.5460805892944336
Batch 64/64 loss: -4.695729732513428
Epoch 375  Train loss: -0.6069159058963551  Val loss: -0.3968022795477274
Epoch 376
-------------------------------
Batch 1/64 loss: -0.8262948989868164
Batch 2/64 loss: -0.8489999771118164
Batch 3/64 loss: -0.34181976318359375
Batch 4/64 loss: -0.6860065460205078
Batch 5/64 loss: -0.516998291015625
Batch 6/64 loss: -0.7754793167114258
Batch 7/64 loss: -0.5841503143310547
Batch 8/64 loss: 0.24867868423461914
Batch 9/64 loss: -0.8096723556518555
Batch 10/64 loss: -0.8147344589233398
Batch 11/64 loss: -0.7419872283935547
Batch 12/64 loss: -0.4839305877685547
Batch 13/64 loss: -0.6828775405883789
Batch 14/64 loss: 1.4106016159057617
Batch 15/64 loss: -0.6381340026855469
Batch 16/64 loss: -0.5446386337280273
Batch 17/64 loss: -0.47957324981689453
Batch 18/64 loss: -0.29250049591064453
Batch 19/64 loss: -0.3766641616821289
Batch 20/64 loss: -0.3535938262939453
Batch 21/64 loss: -0.5036935806274414
Batch 22/64 loss: -0.1675863265991211
Batch 23/64 loss: -0.4864635467529297
Batch 24/64 loss: -0.0740671157836914
Batch 25/64 loss: -0.43438720703125
Batch 26/64 loss: -0.17589282989501953
Batch 27/64 loss: -0.602351188659668
Batch 28/64 loss: -0.6530313491821289
Batch 29/64 loss: -0.8221702575683594
Batch 30/64 loss: -0.5473861694335938
Batch 31/64 loss: -0.24934673309326172
Batch 32/64 loss: -0.5790882110595703
Batch 33/64 loss: -0.40651702880859375
Batch 34/64 loss: -0.26414012908935547
Batch 35/64 loss: -0.1618814468383789
Batch 36/64 loss: -0.25464963912963867
Batch 37/64 loss: -0.6012449264526367
Batch 38/64 loss: -0.6583566665649414
Batch 39/64 loss: -0.11596298217773438
Batch 40/64 loss: -0.5899143218994141
Batch 41/64 loss: -0.5322093963623047
Batch 42/64 loss: -0.5671262741088867
Batch 43/64 loss: -0.8942222595214844
Batch 44/64 loss: -0.5961437225341797
Batch 45/64 loss: -0.31350231170654297
Batch 46/64 loss: -0.3852205276489258
Batch 47/64 loss: -0.69818115234375
Batch 48/64 loss: -0.8271541595458984
Batch 49/64 loss: -0.5972490310668945
Batch 50/64 loss: -0.7315654754638672
Batch 51/64 loss: -0.38509273529052734
Batch 52/64 loss: -0.4768209457397461
Batch 53/64 loss: -0.5705404281616211
Batch 54/64 loss: -0.26966094970703125
Batch 55/64 loss: -0.4189643859863281
Batch 56/64 loss: -0.4864349365234375
Batch 57/64 loss: -0.502955436706543
Batch 58/64 loss: -0.5770378112792969
Batch 59/64 loss: -0.3101387023925781
Batch 60/64 loss: -0.686737060546875
Batch 61/64 loss: -0.2600584030151367
Batch 62/64 loss: -0.5849409103393555
Batch 63/64 loss: -0.4792156219482422
Batch 64/64 loss: -4.681790351867676
Epoch 376  Train loss: -0.5199909397200042  Val loss: -0.5514286604943553
Epoch 377
-------------------------------
Batch 1/64 loss: -0.4365081787109375
Batch 2/64 loss: -0.11087608337402344
Batch 3/64 loss: -0.7455902099609375
Batch 4/64 loss: -0.7033157348632812
Batch 5/64 loss: -0.34209632873535156
Batch 6/64 loss: -0.1912841796875
Batch 7/64 loss: -0.6078891754150391
Batch 8/64 loss: -0.7950305938720703
Batch 9/64 loss: -0.9207487106323242
Batch 10/64 loss: -0.3797426223754883
Batch 11/64 loss: -0.7266311645507812
Batch 12/64 loss: -0.4830007553100586
Batch 13/64 loss: -0.5174598693847656
Batch 14/64 loss: -0.7152175903320312
Batch 15/64 loss: -0.7129945755004883
Batch 16/64 loss: -0.8222551345825195
Batch 17/64 loss: -0.6623039245605469
Batch 18/64 loss: -0.7266674041748047
Batch 19/64 loss: -0.8134336471557617
Batch 20/64 loss: -0.6713399887084961
Batch 21/64 loss: -0.9577627182006836
Batch 22/64 loss: -0.7069272994995117
Batch 23/64 loss: 0.043587684631347656
Batch 24/64 loss: -0.3963594436645508
Batch 25/64 loss: 0.15370941162109375
Batch 26/64 loss: -0.2881040573120117
Batch 27/64 loss: -0.45856189727783203
Batch 28/64 loss: -0.9984502792358398
Batch 29/64 loss: -0.9291706085205078
Batch 30/64 loss: -0.6490583419799805
Batch 31/64 loss: -0.7220773696899414
Batch 32/64 loss: -0.32499217987060547
Batch 33/64 loss: -0.8900432586669922
Batch 34/64 loss: -0.4235725402832031
Batch 35/64 loss: -0.5644264221191406
Batch 36/64 loss: -0.621429443359375
Batch 37/64 loss: -0.30437660217285156
Batch 38/64 loss: -0.49227428436279297
Batch 39/64 loss: -0.7501258850097656
Batch 40/64 loss: -0.8534278869628906
Batch 41/64 loss: -0.9880380630493164
Batch 42/64 loss: -0.7294130325317383
Batch 43/64 loss: -0.3396625518798828
Batch 44/64 loss: -0.5721397399902344
Batch 45/64 loss: -0.8304309844970703
Batch 46/64 loss: -0.7831134796142578
Batch 47/64 loss: -0.5571279525756836
Batch 48/64 loss: -0.6093311309814453
Batch 49/64 loss: -0.8484869003295898
Batch 50/64 loss: -1.1286087036132812
Batch 51/64 loss: -0.6105318069458008
Batch 52/64 loss: -0.3905038833618164
Batch 53/64 loss: -0.4328289031982422
Batch 54/64 loss: -0.54931640625
Batch 55/64 loss: -0.1717667579650879
Batch 56/64 loss: -0.7769184112548828
Batch 57/64 loss: -0.4297466278076172
Batch 58/64 loss: -0.7722558975219727
Batch 59/64 loss: -1.099797248840332
Batch 60/64 loss: -0.602177619934082
Batch 61/64 loss: -0.5006675720214844
Batch 62/64 loss: -0.35431671142578125
Batch 63/64 loss: -1.007181167602539
Batch 64/64 loss: -4.672861099243164
Epoch 377  Train loss: -0.6557997909246707  Val loss: -0.6253930711254632
Epoch 378
-------------------------------
Batch 1/64 loss: -0.1320333480834961
Batch 2/64 loss: -0.8253650665283203
Batch 3/64 loss: -0.41348934173583984
Batch 4/64 loss: -0.2004556655883789
Batch 5/64 loss: -0.7186107635498047
Batch 6/64 loss: -0.5588693618774414
Batch 7/64 loss: -0.5144462585449219
Batch 8/64 loss: -0.4153423309326172
Batch 9/64 loss: -0.5597429275512695
Batch 10/64 loss: -0.3781309127807617
Batch 11/64 loss: -0.3325214385986328
Batch 12/64 loss: -1.0600166320800781
Batch 13/64 loss: -0.8481502532958984
Batch 14/64 loss: -0.2700681686401367
Batch 15/64 loss: -0.8567323684692383
Batch 16/64 loss: -0.3532123565673828
Batch 17/64 loss: -0.7163543701171875
Batch 18/64 loss: -1.142568588256836
Batch 19/64 loss: -0.5723981857299805
Batch 20/64 loss: -0.7257280349731445
Batch 21/64 loss: -0.7703542709350586
Batch 22/64 loss: -0.36855030059814453
Batch 23/64 loss: -0.9458904266357422
Batch 24/64 loss: -0.5656337738037109
Batch 25/64 loss: -0.8933849334716797
Batch 26/64 loss: -0.5251684188842773
Batch 27/64 loss: -0.9074344635009766
Batch 28/64 loss: -0.6854562759399414
Batch 29/64 loss: -0.7366905212402344
Batch 30/64 loss: -0.8732986450195312
Batch 31/64 loss: -0.798670768737793
Batch 32/64 loss: -0.6987371444702148
Batch 33/64 loss: -0.6713523864746094
Batch 34/64 loss: -0.720005989074707
Batch 35/64 loss: -0.5961837768554688
Batch 36/64 loss: -1.0106353759765625
Batch 37/64 loss: -0.7288446426391602
Batch 38/64 loss: -0.6817502975463867
Batch 39/64 loss: -0.6725759506225586
Batch 40/64 loss: -0.4181995391845703
Batch 41/64 loss: -0.7968149185180664
Batch 42/64 loss: -0.35904979705810547
Batch 43/64 loss: -0.5095624923706055
Batch 44/64 loss: -0.38959693908691406
Batch 45/64 loss: -0.7497320175170898
Batch 46/64 loss: -0.9840660095214844
Batch 47/64 loss: -0.2455005645751953
Batch 48/64 loss: -0.37917423248291016
Batch 49/64 loss: -0.657628059387207
Batch 50/64 loss: -1.003270149230957
Batch 51/64 loss: -0.6935720443725586
Batch 52/64 loss: -0.6866874694824219
Batch 53/64 loss: -0.8513641357421875
Batch 54/64 loss: -0.2758150100708008
Batch 55/64 loss: -0.7245731353759766
Batch 56/64 loss: -0.567378044128418
Batch 57/64 loss: -0.8149690628051758
Batch 58/64 loss: -0.7817468643188477
Batch 59/64 loss: -0.9020195007324219
Batch 60/64 loss: -0.41390323638916016
Batch 61/64 loss: -0.601348876953125
Batch 62/64 loss: -0.5872011184692383
Batch 63/64 loss: -0.8694610595703125
Batch 64/64 loss: -4.9565205574035645
Epoch 378  Train loss: -0.6968608538309733  Val loss: -0.6662492260490496
Epoch 379
-------------------------------
Batch 1/64 loss: -0.49209117889404297
Batch 2/64 loss: -1.1020259857177734
Batch 3/64 loss: -0.9729986190795898
Batch 4/64 loss: -0.2854938507080078
Batch 5/64 loss: -0.6229972839355469
Batch 6/64 loss: -0.5940132141113281
Batch 7/64 loss: -0.7519893646240234
Batch 8/64 loss: -0.5896339416503906
Batch 9/64 loss: -0.6118755340576172
Batch 10/64 loss: -0.527679443359375
Batch 11/64 loss: -0.6076631546020508
Batch 12/64 loss: -0.7302532196044922
Batch 13/64 loss: -1.0090723037719727
Batch 14/64 loss: -0.6515636444091797
Batch 15/64 loss: -0.7861413955688477
Batch 16/64 loss: -0.6270351409912109
Batch 17/64 loss: -0.3783884048461914
Batch 18/64 loss: -0.5256080627441406
Batch 19/64 loss: -0.7706289291381836
Batch 20/64 loss: -0.7064304351806641
Batch 21/64 loss: -0.6995229721069336
Batch 22/64 loss: -0.9356880187988281
Batch 23/64 loss: -0.7121677398681641
Batch 24/64 loss: -0.42584800720214844
Batch 25/64 loss: -0.5564393997192383
Batch 26/64 loss: -0.9851188659667969
Batch 27/64 loss: -0.8295993804931641
Batch 28/64 loss: -0.8418540954589844
Batch 29/64 loss: -0.43609142303466797
Batch 30/64 loss: -0.4550342559814453
Batch 31/64 loss: -0.756566047668457
Batch 32/64 loss: -0.5975017547607422
Batch 33/64 loss: -0.8437738418579102
Batch 34/64 loss: -0.36521434783935547
Batch 35/64 loss: -0.25678062438964844
Batch 36/64 loss: -0.737116813659668
Batch 37/64 loss: -0.4419231414794922
Batch 38/64 loss: -0.5671234130859375
Batch 39/64 loss: -1.0317716598510742
Batch 40/64 loss: -0.6645708084106445
Batch 41/64 loss: -0.7065763473510742
Batch 42/64 loss: -0.7289524078369141
Batch 43/64 loss: -0.7280540466308594
Batch 44/64 loss: -0.6905832290649414
Batch 45/64 loss: -0.547877311706543
Batch 46/64 loss: -0.9638547897338867
Batch 47/64 loss: -0.7051458358764648
Batch 48/64 loss: -0.5063028335571289
Batch 49/64 loss: -0.7035760879516602
Batch 50/64 loss: -0.5272006988525391
Batch 51/64 loss: -0.6620035171508789
Batch 52/64 loss: -0.5803232192993164
Batch 53/64 loss: -0.8555002212524414
Batch 54/64 loss: -0.5627737045288086
Batch 55/64 loss: -0.9109392166137695
Batch 56/64 loss: -0.675628662109375
Batch 57/64 loss: -0.7347259521484375
Batch 58/64 loss: -0.9161911010742188
Batch 59/64 loss: -0.49968528747558594
Batch 60/64 loss: -0.8507986068725586
Batch 61/64 loss: -0.6358022689819336
Batch 62/64 loss: -0.42728328704833984
Batch 63/64 loss: -0.7009801864624023
Batch 64/64 loss: -4.954143524169922
Epoch 379  Train loss: -0.7218769597072228  Val loss: -0.6429367065429688
Epoch 380
-------------------------------
Batch 1/64 loss: -0.6955537796020508
Batch 2/64 loss: -0.9362812042236328
Batch 3/64 loss: -0.3766508102416992
Batch 4/64 loss: -0.9982500076293945
Batch 5/64 loss: -0.8158016204833984
Batch 6/64 loss: -0.6639747619628906
Batch 7/64 loss: -0.5704946517944336
Batch 8/64 loss: -0.9111442565917969
Batch 9/64 loss: -1.006159782409668
Batch 10/64 loss: -0.858454704284668
Batch 11/64 loss: -0.7731761932373047
Batch 12/64 loss: -0.7654132843017578
Batch 13/64 loss: -0.793731689453125
Batch 14/64 loss: -0.5854740142822266
Batch 15/64 loss: -0.6298065185546875
Batch 16/64 loss: -0.7511453628540039
Batch 17/64 loss: -0.44516563415527344
Batch 18/64 loss: -0.3976163864135742
Batch 19/64 loss: -0.810145378112793
Batch 20/64 loss: -0.6344213485717773
Batch 21/64 loss: -0.5749216079711914
Batch 22/64 loss: -0.9013128280639648
Batch 23/64 loss: -0.8263893127441406
Batch 24/64 loss: -0.7242832183837891
Batch 25/64 loss: -0.8515796661376953
Batch 26/64 loss: -0.5061330795288086
Batch 27/64 loss: -0.7958974838256836
Batch 28/64 loss: -0.8744516372680664
Batch 29/64 loss: -0.7342567443847656
Batch 30/64 loss: -0.4163351058959961
Batch 31/64 loss: -0.5119743347167969
Batch 32/64 loss: -0.7663068771362305
Batch 33/64 loss: -0.8424501419067383
Batch 34/64 loss: -0.42557811737060547
Batch 35/64 loss: -0.5357789993286133
Batch 36/64 loss: -0.6590728759765625
Batch 37/64 loss: -0.5036401748657227
Batch 38/64 loss: -0.40177249908447266
Batch 39/64 loss: -0.6586856842041016
Batch 40/64 loss: -0.13605546951293945
Batch 41/64 loss: -0.9162073135375977
Batch 42/64 loss: -0.605952262878418
Batch 43/64 loss: -0.6055126190185547
Batch 44/64 loss: -0.2487945556640625
Batch 45/64 loss: -0.07522821426391602
Batch 46/64 loss: -0.6521902084350586
Batch 47/64 loss: -0.48660945892333984
Batch 48/64 loss: -0.3008127212524414
Batch 49/64 loss: -0.6146106719970703
Batch 50/64 loss: -0.9216632843017578
Batch 51/64 loss: -0.1305375099182129
Batch 52/64 loss: -0.609217643737793
Batch 53/64 loss: -0.583317756652832
Batch 54/64 loss: -0.4783010482788086
Batch 55/64 loss: -0.42037248611450195
Batch 56/64 loss: -0.04660511016845703
Batch 57/64 loss: -0.8990402221679688
Batch 58/64 loss: -0.5423660278320312
Batch 59/64 loss: -0.5319614410400391
Batch 60/64 loss: -0.4212350845336914
Batch 61/64 loss: -0.8172292709350586
Batch 62/64 loss: -0.6519088745117188
Batch 63/64 loss: -0.8241729736328125
Batch 64/64 loss: -4.7891130447387695
Epoch 380  Train loss: -0.6751595104441923  Val loss: -0.5976130429821735
Epoch 381
-------------------------------
Batch 1/64 loss: -0.04281187057495117
Batch 2/64 loss: -0.663416862487793
Batch 3/64 loss: -0.32993364334106445
Batch 4/64 loss: -0.8839635848999023
Batch 5/64 loss: -0.3540191650390625
Batch 6/64 loss: -0.37741661071777344
Batch 7/64 loss: -0.9291048049926758
Batch 8/64 loss: -0.6737394332885742
Batch 9/64 loss: -0.6013889312744141
Batch 10/64 loss: -0.8147335052490234
Batch 11/64 loss: -0.5279664993286133
Batch 12/64 loss: -0.44547271728515625
Batch 13/64 loss: -0.43421268463134766
Batch 14/64 loss: -0.8258275985717773
Batch 15/64 loss: -0.47785282135009766
Batch 16/64 loss: -0.9393529891967773
Batch 17/64 loss: -0.9053230285644531
Batch 18/64 loss: -0.9825973510742188
Batch 19/64 loss: -0.6561489105224609
Batch 20/64 loss: -0.8551959991455078
Batch 21/64 loss: -0.7807092666625977
Batch 22/64 loss: -0.69586181640625
Batch 23/64 loss: -1.0093612670898438
Batch 24/64 loss: -0.34879493713378906
Batch 25/64 loss: -0.5038604736328125
Batch 26/64 loss: -0.7516946792602539
Batch 27/64 loss: -0.7579507827758789
Batch 28/64 loss: -0.9688320159912109
Batch 29/64 loss: -0.5605783462524414
Batch 30/64 loss: -0.874629020690918
Batch 31/64 loss: -1.0262975692749023
Batch 32/64 loss: -0.8642425537109375
Batch 33/64 loss: -0.44643211364746094
Batch 34/64 loss: -0.6655645370483398
Batch 35/64 loss: -0.4700613021850586
Batch 36/64 loss: -0.8796148300170898
Batch 37/64 loss: -0.2524881362915039
Batch 38/64 loss: -1.082108497619629
Batch 39/64 loss: -0.8703222274780273
Batch 40/64 loss: -0.4927940368652344
Batch 41/64 loss: -0.9319620132446289
Batch 42/64 loss: -0.9356050491333008
Batch 43/64 loss: -0.6910524368286133
Batch 44/64 loss: -0.8541698455810547
Batch 45/64 loss: -0.8790159225463867
Batch 46/64 loss: -0.0835561752319336
Batch 47/64 loss: 0.10094451904296875
Batch 48/64 loss: -1.0059022903442383
Batch 49/64 loss: -0.13989973068237305
Batch 50/64 loss: -0.8022003173828125
Batch 51/64 loss: -0.40140342712402344
Batch 52/64 loss: -1.1555147171020508
Batch 53/64 loss: -0.39493465423583984
Batch 54/64 loss: -0.21306514739990234
Batch 55/64 loss: -0.5542545318603516
Batch 56/64 loss: -0.6035881042480469
Batch 57/64 loss: -0.7386722564697266
Batch 58/64 loss: -0.6831579208374023
Batch 59/64 loss: -0.7557573318481445
Batch 60/64 loss: -0.6649379730224609
Batch 61/64 loss: -0.6456546783447266
Batch 62/64 loss: -0.3362407684326172
Batch 63/64 loss: -0.710576057434082
Batch 64/64 loss: -4.564919471740723
Epoch 381  Train loss: -0.6984555898928175  Val loss: -0.6446159205485865
Epoch 382
-------------------------------
Batch 1/64 loss: -0.6811847686767578
Batch 2/64 loss: -0.7653923034667969
Batch 3/64 loss: -0.7431831359863281
Batch 4/64 loss: -0.9525136947631836
Batch 5/64 loss: -0.6981029510498047
Batch 6/64 loss: -0.35442495346069336
Batch 7/64 loss: -0.9475460052490234
Batch 8/64 loss: -1.048818588256836
Batch 9/64 loss: -0.8276872634887695
Batch 10/64 loss: -0.7732429504394531
Batch 11/64 loss: -0.8172388076782227
Batch 12/64 loss: -0.8589267730712891
Batch 13/64 loss: -0.34360504150390625
Batch 14/64 loss: -0.1371312141418457
Batch 15/64 loss: -1.0172204971313477
Batch 16/64 loss: -0.2957291603088379
Batch 17/64 loss: -0.8322048187255859
Batch 18/64 loss: -0.22513580322265625
Batch 19/64 loss: -0.8245086669921875
Batch 20/64 loss: -0.9112510681152344
Batch 21/64 loss: -0.37335681915283203
Batch 22/64 loss: -0.26499366760253906
Batch 23/64 loss: -0.3580770492553711
Batch 24/64 loss: -0.8857192993164062
Batch 25/64 loss: -1.0153141021728516
Batch 26/64 loss: -0.7346649169921875
Batch 27/64 loss: -0.6639404296875
Batch 28/64 loss: -1.057908058166504
Batch 29/64 loss: -1.040806770324707
Batch 30/64 loss: -0.5484170913696289
Batch 31/64 loss: -0.4798860549926758
Batch 32/64 loss: -0.6011219024658203
Batch 33/64 loss: -0.8787555694580078
Batch 34/64 loss: -0.3504452705383301
Batch 35/64 loss: -0.9920740127563477
Batch 36/64 loss: -0.5161905288696289
Batch 37/64 loss: -0.7612533569335938
Batch 38/64 loss: -0.6909379959106445
Batch 39/64 loss: -0.9105672836303711
Batch 40/64 loss: -0.4617776870727539
Batch 41/64 loss: -0.7200708389282227
Batch 42/64 loss: -0.9502992630004883
Batch 43/64 loss: -0.6228933334350586
Batch 44/64 loss: -0.8967828750610352
Batch 45/64 loss: -0.8210477828979492
Batch 46/64 loss: -0.24907779693603516
Batch 47/64 loss: -0.7739934921264648
Batch 48/64 loss: -0.7374382019042969
Batch 49/64 loss: -0.4270315170288086
Batch 50/64 loss: -0.6122636795043945
Batch 51/64 loss: -0.5519351959228516
Batch 52/64 loss: -0.9503898620605469
Batch 53/64 loss: -0.7117729187011719
Batch 54/64 loss: -0.43872737884521484
Batch 55/64 loss: -1.0208873748779297
Batch 56/64 loss: -0.29531002044677734
Batch 57/64 loss: -0.7874021530151367
Batch 58/64 loss: -0.5069894790649414
Batch 59/64 loss: -1.1841239929199219
Batch 60/64 loss: -0.6635494232177734
Batch 61/64 loss: -0.8825597763061523
Batch 62/64 loss: -0.5559301376342773
Batch 63/64 loss: -0.5908565521240234
Batch 64/64 loss: -4.242475509643555
Epoch 382  Train loss: -0.7337168001661114  Val loss: -0.6544841753248497
Epoch 383
-------------------------------
Batch 1/64 loss: -0.8942909240722656
Batch 2/64 loss: -0.48802661895751953
Batch 3/64 loss: -0.17493391036987305
Batch 4/64 loss: -1.055002212524414
Batch 5/64 loss: -0.6914796829223633
Batch 6/64 loss: -0.6143293380737305
Batch 7/64 loss: -0.9694623947143555
Batch 8/64 loss: -0.7796573638916016
Batch 9/64 loss: -0.8929443359375
Batch 10/64 loss: -0.852290153503418
Batch 11/64 loss: -0.19901132583618164
Batch 12/64 loss: -0.7965221405029297
Batch 13/64 loss: -0.9684906005859375
Batch 14/64 loss: -0.5774259567260742
Batch 15/64 loss: -1.0377731323242188
Batch 16/64 loss: -0.6491680145263672
Batch 17/64 loss: -0.6328516006469727
Batch 18/64 loss: -0.6490058898925781
Batch 19/64 loss: -0.7330741882324219
Batch 20/64 loss: -0.7441291809082031
Batch 21/64 loss: -0.983820915222168
Batch 22/64 loss: -0.6062688827514648
Batch 23/64 loss: -0.8675498962402344
Batch 24/64 loss: -0.8661737442016602
Batch 25/64 loss: -0.057417869567871094
Batch 26/64 loss: -0.6954870223999023
Batch 27/64 loss: -0.7969779968261719
Batch 28/64 loss: -0.663325309753418
Batch 29/64 loss: -0.9128828048706055
Batch 30/64 loss: -0.3948535919189453
Batch 31/64 loss: -1.0079965591430664
Batch 32/64 loss: -0.6014423370361328
Batch 33/64 loss: -0.9192924499511719
Batch 34/64 loss: -0.8497371673583984
Batch 35/64 loss: -0.6935367584228516
Batch 36/64 loss: -0.8478813171386719
Batch 37/64 loss: -0.7879085540771484
Batch 38/64 loss: -0.7772073745727539
Batch 39/64 loss: -0.9319210052490234
Batch 40/64 loss: -0.6143875122070312
Batch 41/64 loss: -0.5202798843383789
Batch 42/64 loss: -0.8161296844482422
Batch 43/64 loss: -0.8092527389526367
Batch 44/64 loss: -0.7659578323364258
Batch 45/64 loss: -0.5305404663085938
Batch 46/64 loss: -0.2928504943847656
Batch 47/64 loss: -0.5735645294189453
Batch 48/64 loss: -0.4544553756713867
Batch 49/64 loss: -0.7747535705566406
Batch 50/64 loss: -1.071274757385254
Batch 51/64 loss: -0.7094612121582031
Batch 52/64 loss: -0.6046342849731445
Batch 53/64 loss: -0.8632354736328125
Batch 54/64 loss: -0.5755987167358398
Batch 55/64 loss: -0.8051090240478516
Batch 56/64 loss: -0.6459064483642578
Batch 57/64 loss: -0.46237945556640625
Batch 58/64 loss: -0.9504413604736328
Batch 59/64 loss: -0.7396183013916016
Batch 60/64 loss: -0.5319910049438477
Batch 61/64 loss: -0.7879190444946289
Batch 62/64 loss: -0.7752780914306641
Batch 63/64 loss: -0.643092155456543
Batch 64/64 loss: -5.368353366851807
Epoch 383  Train loss: -0.768720423006544  Val loss: -0.7095620460116986
Epoch 384
-------------------------------
Batch 1/64 loss: -0.967717170715332
Batch 2/64 loss: -0.7397661209106445
Batch 3/64 loss: -1.0030784606933594
Batch 4/64 loss: -1.0011882781982422
Batch 5/64 loss: -0.8415422439575195
Batch 6/64 loss: -0.643035888671875
Batch 7/64 loss: -0.6458702087402344
Batch 8/64 loss: -0.6388654708862305
Batch 9/64 loss: -0.7844724655151367
Batch 10/64 loss: -0.6112747192382812
Batch 11/64 loss: -0.5388431549072266
Batch 12/64 loss: -0.6874046325683594
Batch 13/64 loss: -0.4464282989501953
Batch 14/64 loss: -0.9562358856201172
Batch 15/64 loss: -0.8918695449829102
Batch 16/64 loss: -0.9820518493652344
Batch 17/64 loss: -0.9687814712524414
Batch 18/64 loss: -0.9872369766235352
Batch 19/64 loss: -0.43137073516845703
Batch 20/64 loss: -0.4853534698486328
Batch 21/64 loss: -0.4561767578125
Batch 22/64 loss: -0.43234825134277344
Batch 23/64 loss: -0.7230539321899414
Batch 24/64 loss: -0.5175304412841797
Batch 25/64 loss: -0.5532159805297852
Batch 26/64 loss: -0.5928468704223633
Batch 27/64 loss: -0.5979852676391602
Batch 28/64 loss: -0.8170309066772461
Batch 29/64 loss: -1.0049238204956055
Batch 30/64 loss: -0.9432754516601562
Batch 31/64 loss: -0.8518915176391602
Batch 32/64 loss: -1.0259389877319336
Batch 33/64 loss: -0.7815628051757812
Batch 34/64 loss: -0.834040641784668
Batch 35/64 loss: -0.966343879699707
Batch 36/64 loss: -0.5503664016723633
Batch 37/64 loss: -1.0380620956420898
Batch 38/64 loss: -0.8251152038574219
Batch 39/64 loss: -0.7050228118896484
Batch 40/64 loss: -0.9342260360717773
Batch 41/64 loss: -0.8821945190429688
Batch 42/64 loss: -0.777008056640625
Batch 43/64 loss: -1.0683650970458984
Batch 44/64 loss: -1.1107149124145508
Batch 45/64 loss: -0.9377031326293945
Batch 46/64 loss: -0.6528491973876953
Batch 47/64 loss: -0.5411891937255859
Batch 48/64 loss: -0.6662883758544922
Batch 49/64 loss: -0.8920202255249023
Batch 50/64 loss: -0.6191396713256836
Batch 51/64 loss: -0.7950525283813477
Batch 52/64 loss: -0.5417385101318359
Batch 53/64 loss: -0.6856155395507812
Batch 54/64 loss: -0.596038818359375
Batch 55/64 loss: -0.3709716796875
Batch 56/64 loss: -0.8143377304077148
Batch 57/64 loss: -0.9468145370483398
Batch 58/64 loss: -0.5973491668701172
Batch 59/64 loss: -0.7290048599243164
Batch 60/64 loss: -0.6933469772338867
Batch 61/64 loss: -1.0144367218017578
Batch 62/64 loss: -0.6159305572509766
Batch 63/64 loss: -0.6378269195556641
Batch 64/64 loss: -5.23501443862915
Epoch 384  Train loss: -0.8080869468988157  Val loss: -0.6243068000295318
Epoch 385
-------------------------------
Batch 1/64 loss: -0.8646478652954102
Batch 2/64 loss: -0.19216442108154297
Batch 3/64 loss: 0.8428125381469727
Batch 4/64 loss: 3.4129457473754883
Batch 5/64 loss: -0.22094440460205078
Batch 6/64 loss: -0.7405099868774414
Batch 7/64 loss: -0.6901540756225586
Batch 8/64 loss: -0.9819869995117188
Batch 9/64 loss: -0.5463008880615234
Batch 10/64 loss: -0.35428714752197266
Batch 11/64 loss: -0.3918447494506836
Batch 12/64 loss: -0.8263120651245117
Batch 13/64 loss: -0.8249626159667969
Batch 14/64 loss: -0.4071340560913086
Batch 15/64 loss: -0.6154880523681641
Batch 16/64 loss: -0.5076093673706055
Batch 17/64 loss: -0.5841236114501953
Batch 18/64 loss: -0.6225652694702148
Batch 19/64 loss: -0.5813465118408203
Batch 20/64 loss: -0.7801170349121094
Batch 21/64 loss: -0.3027210235595703
Batch 22/64 loss: -0.293792724609375
Batch 23/64 loss: -0.5800342559814453
Batch 24/64 loss: -0.488525390625
Batch 25/64 loss: -0.3518209457397461
Batch 26/64 loss: -0.723475456237793
Batch 27/64 loss: -0.2860088348388672
Batch 28/64 loss: -0.4942293167114258
Batch 29/64 loss: -0.8115873336791992
Batch 30/64 loss: -0.3764200210571289
Batch 31/64 loss: -0.328887939453125
Batch 32/64 loss: -0.47623634338378906
Batch 33/64 loss: -0.669276237487793
Batch 34/64 loss: -0.6810684204101562
Batch 35/64 loss: -0.7402114868164062
Batch 36/64 loss: -0.5523767471313477
Batch 37/64 loss: -0.6273660659790039
Batch 38/64 loss: -0.8285331726074219
Batch 39/64 loss: -0.8360767364501953
Batch 40/64 loss: -0.7356843948364258
Batch 41/64 loss: -0.7349281311035156
Batch 42/64 loss: -0.4706268310546875
Batch 43/64 loss: -0.2825508117675781
Batch 44/64 loss: -0.2546424865722656
Batch 45/64 loss: -0.45174121856689453
Batch 46/64 loss: -0.6488332748413086
Batch 47/64 loss: -0.7508306503295898
Batch 48/64 loss: -0.27490806579589844
Batch 49/64 loss: -0.6930465698242188
Batch 50/64 loss: -0.3563690185546875
Batch 51/64 loss: -0.7669706344604492
Batch 52/64 loss: -0.34943675994873047
Batch 53/64 loss: -0.8831367492675781
Batch 54/64 loss: -0.6500759124755859
Batch 55/64 loss: -0.6068916320800781
Batch 56/64 loss: -0.2910490036010742
Batch 57/64 loss: -0.7186422348022461
Batch 58/64 loss: -0.703953742980957
Batch 59/64 loss: -0.6428070068359375
Batch 60/64 loss: -0.7824497222900391
Batch 61/64 loss: -0.29700660705566406
Batch 62/64 loss: -0.428131103515625
Batch 63/64 loss: -0.5498647689819336
Batch 64/64 loss: -4.384006977081299
Epoch 385  Train loss: -0.5260858330072141  Val loss: -0.6131605167978817
Epoch 386
-------------------------------
Batch 1/64 loss: -0.45931339263916016
Batch 2/64 loss: -0.7034006118774414
Batch 3/64 loss: -0.8394613265991211
Batch 4/64 loss: -0.28221607208251953
Batch 5/64 loss: -0.9766883850097656
Batch 6/64 loss: -0.711237907409668
Batch 7/64 loss: -0.7645444869995117
Batch 8/64 loss: -0.7335309982299805
Batch 9/64 loss: -0.6771945953369141
Batch 10/64 loss: -0.8912973403930664
Batch 11/64 loss: -0.9581804275512695
Batch 12/64 loss: -0.6247854232788086
Batch 13/64 loss: -0.5500297546386719
Batch 14/64 loss: -0.7394189834594727
Batch 15/64 loss: -0.5843467712402344
Batch 16/64 loss: -0.4991741180419922
Batch 17/64 loss: -0.8531169891357422
Batch 18/64 loss: -0.6640768051147461
Batch 19/64 loss: -0.4670372009277344
Batch 20/64 loss: -0.6330099105834961
Batch 21/64 loss: -0.8588180541992188
Batch 22/64 loss: -0.6215457916259766
Batch 23/64 loss: -0.8000822067260742
Batch 24/64 loss: -0.7491855621337891
Batch 25/64 loss: -0.789677619934082
Batch 26/64 loss: -0.7486686706542969
Batch 27/64 loss: -0.8651294708251953
Batch 28/64 loss: -0.5830907821655273
Batch 29/64 loss: -0.5337162017822266
Batch 30/64 loss: -1.0087766647338867
Batch 31/64 loss: -0.7110691070556641
Batch 32/64 loss: -0.9184474945068359
Batch 33/64 loss: -0.5125589370727539
Batch 34/64 loss: -0.7746572494506836
Batch 35/64 loss: -1.1098966598510742
Batch 36/64 loss: -1.1081571578979492
Batch 37/64 loss: -0.8122081756591797
Batch 38/64 loss: -0.7686281204223633
Batch 39/64 loss: -0.7566394805908203
Batch 40/64 loss: -0.07829475402832031
Batch 41/64 loss: -0.8267450332641602
Batch 42/64 loss: -0.5524711608886719
Batch 43/64 loss: -0.5621204376220703
Batch 44/64 loss: -0.28153038024902344
Batch 45/64 loss: -0.8826141357421875
Batch 46/64 loss: -0.6178579330444336
Batch 47/64 loss: -0.893646240234375
Batch 48/64 loss: -0.8434019088745117
Batch 49/64 loss: -0.8436775207519531
Batch 50/64 loss: -0.9510593414306641
Batch 51/64 loss: -0.9691448211669922
Batch 52/64 loss: -0.6401853561401367
Batch 53/64 loss: -0.9034214019775391
Batch 54/64 loss: -1.0761232376098633
Batch 55/64 loss: -0.8068361282348633
Batch 56/64 loss: -1.0307798385620117
Batch 57/64 loss: -0.7180862426757812
Batch 58/64 loss: -0.856236457824707
Batch 59/64 loss: -0.538142204284668
Batch 60/64 loss: -0.24558067321777344
Batch 61/64 loss: -0.5823659896850586
Batch 62/64 loss: -0.4709768295288086
Batch 63/64 loss: -0.517573356628418
Batch 64/64 loss: -5.148748397827148
Epoch 386  Train loss: -0.7716619229784198  Val loss: -0.4858655765703863
Epoch 387
-------------------------------
Batch 1/64 loss: -0.796992301940918
Batch 2/64 loss: -0.8804311752319336
Batch 3/64 loss: -1.0132856369018555
Batch 4/64 loss: -0.647791862487793
Batch 5/64 loss: -1.107447624206543
Batch 6/64 loss: -0.5598316192626953
Batch 7/64 loss: -0.39828014373779297
Batch 8/64 loss: -0.3753194808959961
Batch 9/64 loss: -0.7807912826538086
Batch 10/64 loss: -0.8110485076904297
Batch 11/64 loss: -0.4709968566894531
Batch 12/64 loss: -0.7218694686889648
Batch 13/64 loss: -0.6339931488037109
Batch 14/64 loss: -0.7003040313720703
Batch 15/64 loss: -0.45753955841064453
Batch 16/64 loss: -0.6765327453613281
Batch 17/64 loss: -0.5953512191772461
Batch 18/64 loss: -0.8905038833618164
Batch 19/64 loss: -0.735346794128418
Batch 20/64 loss: -0.9585103988647461
Batch 21/64 loss: -0.5533447265625
Batch 22/64 loss: -0.3215351104736328
Batch 23/64 loss: -0.994379997253418
Batch 24/64 loss: -0.5305767059326172
Batch 25/64 loss: -1.1164522171020508
Batch 26/64 loss: -0.6027822494506836
Batch 27/64 loss: -0.7260408401489258
Batch 28/64 loss: -0.6392717361450195
Batch 29/64 loss: -0.7093830108642578
Batch 30/64 loss: -0.3937997817993164
Batch 31/64 loss: -0.3756704330444336
Batch 32/64 loss: -0.6953516006469727
Batch 33/64 loss: -0.8376474380493164
Batch 34/64 loss: -0.562647819519043
Batch 35/64 loss: -0.4329671859741211
Batch 36/64 loss: -0.8360977172851562
Batch 37/64 loss: -0.4151296615600586
Batch 38/64 loss: -0.5319585800170898
Batch 39/64 loss: -0.6188507080078125
Batch 40/64 loss: -0.5698385238647461
Batch 41/64 loss: -0.8535213470458984
Batch 42/64 loss: -0.5453195571899414
Batch 43/64 loss: -0.7699050903320312
Batch 44/64 loss: -0.5854568481445312
Batch 45/64 loss: -0.5863304138183594
Batch 46/64 loss: -0.5278568267822266
Batch 47/64 loss: -0.887420654296875
Batch 48/64 loss: -0.8676528930664062
Batch 49/64 loss: -1.07427978515625
Batch 50/64 loss: -0.6186752319335938
Batch 51/64 loss: -0.37899208068847656
Batch 52/64 loss: -0.6799240112304688
Batch 53/64 loss: -0.9035806655883789
Batch 54/64 loss: -0.8865890502929688
Batch 55/64 loss: -0.7262678146362305
Batch 56/64 loss: -0.7037220001220703
Batch 57/64 loss: -0.39861392974853516
Batch 58/64 loss: -0.7247791290283203
Batch 59/64 loss: -0.7131214141845703
Batch 60/64 loss: -0.22037601470947266
Batch 61/64 loss: -0.6482648849487305
Batch 62/64 loss: -0.057567596435546875
Batch 63/64 loss: -0.989771842956543
Batch 64/64 loss: -4.828190803527832
Epoch 387  Train loss: -0.7160004073498296  Val loss: -0.5654618961294902
Epoch 388
-------------------------------
Batch 1/64 loss: -0.9136724472045898
Batch 2/64 loss: -0.7044649124145508
Batch 3/64 loss: -0.6985645294189453
Batch 4/64 loss: -0.8034448623657227
Batch 5/64 loss: -0.9734220504760742
Batch 6/64 loss: -0.6127490997314453
Batch 7/64 loss: -0.6569337844848633
Batch 8/64 loss: -0.8244209289550781
Batch 9/64 loss: -0.5384807586669922
Batch 10/64 loss: -0.71917724609375
Batch 11/64 loss: -0.5945930480957031
Batch 12/64 loss: -0.7896203994750977
Batch 13/64 loss: -1.119272232055664
Batch 14/64 loss: -0.7547874450683594
Batch 15/64 loss: -0.5025653839111328
Batch 16/64 loss: -0.8039512634277344
Batch 17/64 loss: -0.7142858505249023
Batch 18/64 loss: -0.7775058746337891
Batch 19/64 loss: -1.1681346893310547
Batch 20/64 loss: -0.7696008682250977
Batch 21/64 loss: -0.9974069595336914
Batch 22/64 loss: -0.9524192810058594
Batch 23/64 loss: -0.9719886779785156
Batch 24/64 loss: -0.8387813568115234
Batch 25/64 loss: -0.42875099182128906
Batch 26/64 loss: -0.805750846862793
Batch 27/64 loss: -0.5196323394775391
Batch 28/64 loss: -0.9131965637207031
Batch 29/64 loss: -0.6349010467529297
Batch 30/64 loss: -0.8011589050292969
Batch 31/64 loss: -0.6878747940063477
Batch 32/64 loss: -0.9535951614379883
Batch 33/64 loss: -1.0630416870117188
Batch 34/64 loss: -1.159409523010254
Batch 35/64 loss: -0.32342529296875
Batch 36/64 loss: -0.881251335144043
Batch 37/64 loss: -0.7170829772949219
Batch 38/64 loss: -0.7673187255859375
Batch 39/64 loss: -0.9828071594238281
Batch 40/64 loss: -0.8086433410644531
Batch 41/64 loss: -0.82000732421875
Batch 42/64 loss: -0.7400531768798828
Batch 43/64 loss: -0.5768327713012695
Batch 44/64 loss: -0.9117927551269531
Batch 45/64 loss: -0.7160625457763672
Batch 46/64 loss: -0.153167724609375
Batch 47/64 loss: -0.45658302307128906
Batch 48/64 loss: -0.1711263656616211
Batch 49/64 loss: -0.2833833694458008
Batch 50/64 loss: -0.9090747833251953
Batch 51/64 loss: -0.9216699600219727
Batch 52/64 loss: -0.7378835678100586
Batch 53/64 loss: -0.05960512161254883
Batch 54/64 loss: -0.7768135070800781
Batch 55/64 loss: -0.8753623962402344
Batch 56/64 loss: -0.9352045059204102
Batch 57/64 loss: -0.3640890121459961
Batch 58/64 loss: -0.9909334182739258
Batch 59/64 loss: -0.9780721664428711
Batch 60/64 loss: -0.6431379318237305
Batch 61/64 loss: 0.05980110168457031
Batch 62/64 loss: -0.8288955688476562
Batch 63/64 loss: -0.8773946762084961
Batch 64/64 loss: -3.54386043548584
Epoch 388  Train loss: -0.7682090123494466  Val loss: 0.09761301676432292
Epoch 389
-------------------------------
Batch 1/64 loss: -0.6597156524658203
Batch 2/64 loss: 0.44463443756103516
Batch 3/64 loss: -0.1931467056274414
Batch 4/64 loss: -0.7175178527832031
Batch 5/64 loss: -0.2665987014770508
Batch 6/64 loss: -0.7304058074951172
Batch 7/64 loss: -0.4162931442260742
Batch 8/64 loss: -0.27541255950927734
Batch 9/64 loss: -0.028970718383789062
Batch 10/64 loss: -0.6328649520874023
Batch 11/64 loss: -0.2549457550048828
Batch 12/64 loss: -0.13659381866455078
Batch 13/64 loss: -0.7484798431396484
Batch 14/64 loss: -0.5801248550415039
Batch 15/64 loss: -0.08417844772338867
Batch 16/64 loss: -0.6175155639648438
Batch 17/64 loss: -0.7096834182739258
Batch 18/64 loss: -0.10154914855957031
Batch 19/64 loss: -0.773411750793457
Batch 20/64 loss: -0.4826812744140625
Batch 21/64 loss: 0.061425209045410156
Batch 22/64 loss: -0.6050310134887695
Batch 23/64 loss: -0.22017192840576172
Batch 24/64 loss: -0.43999576568603516
Batch 25/64 loss: -0.6480722427368164
Batch 26/64 loss: -0.4618711471557617
Batch 27/64 loss: -0.5872325897216797
Batch 28/64 loss: -0.5725917816162109
Batch 29/64 loss: -0.6488037109375
Batch 30/64 loss: -0.3281526565551758
Batch 31/64 loss: -0.1515669822692871
Batch 32/64 loss: -0.8809728622436523
Batch 33/64 loss: -0.7439031600952148
Batch 34/64 loss: -0.42719268798828125
Batch 35/64 loss: -0.14393949508666992
Batch 36/64 loss: -0.5933446884155273
Batch 37/64 loss: -0.6611881256103516
Batch 38/64 loss: -0.7056970596313477
Batch 39/64 loss: -0.731450080871582
Batch 40/64 loss: -0.20837640762329102
Batch 41/64 loss: 0.23398971557617188
Batch 42/64 loss: 0.1093606948852539
Batch 43/64 loss: -0.7186079025268555
Batch 44/64 loss: -0.36148738861083984
Batch 45/64 loss: -0.7355318069458008
Batch 46/64 loss: -0.5410537719726562
Batch 47/64 loss: -0.37339305877685547
Batch 48/64 loss: -0.9539995193481445
Batch 49/64 loss: -0.6022758483886719
Batch 50/64 loss: -0.3279104232788086
Batch 51/64 loss: -0.5493927001953125
Batch 52/64 loss: -0.2602958679199219
Batch 53/64 loss: -0.7719516754150391
Batch 54/64 loss: -0.15842723846435547
Batch 55/64 loss: -0.5389461517333984
Batch 56/64 loss: -0.8433952331542969
Batch 57/64 loss: -0.21970462799072266
Batch 58/64 loss: -0.6090564727783203
Batch 59/64 loss: -0.7974462509155273
Batch 60/64 loss: -0.689910888671875
Batch 61/64 loss: -0.8210697174072266
Batch 62/64 loss: -1.0411310195922852
Batch 63/64 loss: -0.5088100433349609
Batch 64/64 loss: -4.85587215423584
Epoch 389  Train loss: -0.5237009572047814  Val loss: -0.3514311616773048
Epoch 390
-------------------------------
Batch 1/64 loss: -0.8487281799316406
Batch 2/64 loss: -0.8594150543212891
Batch 3/64 loss: -0.4099903106689453
Batch 4/64 loss: -0.5952205657958984
Batch 5/64 loss: -0.45850372314453125
Batch 6/64 loss: -0.2842898368835449
Batch 7/64 loss: -0.3983936309814453
Batch 8/64 loss: -0.7317905426025391
Batch 9/64 loss: -0.4747638702392578
Batch 10/64 loss: -0.4388895034790039
Batch 11/64 loss: 0.03761100769042969
Batch 12/64 loss: -0.7345199584960938
Batch 13/64 loss: -0.4290304183959961
Batch 14/64 loss: -0.6591854095458984
Batch 15/64 loss: -0.7047185897827148
Batch 16/64 loss: -0.8013229370117188
Batch 17/64 loss: -0.4930591583251953
Batch 18/64 loss: -0.08799123764038086
Batch 19/64 loss: -0.4795370101928711
Batch 20/64 loss: -0.2059321403503418
Batch 21/64 loss: -0.7957706451416016
Batch 22/64 loss: -0.281984806060791
Batch 23/64 loss: -0.638789176940918
Batch 24/64 loss: -0.4253520965576172
Batch 25/64 loss: -0.7497806549072266
Batch 26/64 loss: -0.44234466552734375
Batch 27/64 loss: -0.5644741058349609
Batch 28/64 loss: -0.5783300399780273
Batch 29/64 loss: -0.7823724746704102
Batch 30/64 loss: -0.5532159805297852
Batch 31/64 loss: -0.24236679077148438
Batch 32/64 loss: -0.46639347076416016
Batch 33/64 loss: -0.9130172729492188
Batch 34/64 loss: -0.6090316772460938
Batch 35/64 loss: -0.5107431411743164
Batch 36/64 loss: -0.9283971786499023
Batch 37/64 loss: -0.6629085540771484
Batch 38/64 loss: -0.790949821472168
Batch 39/64 loss: -0.5180253982543945
Batch 40/64 loss: -1.0983867645263672
Batch 41/64 loss: -0.7546529769897461
Batch 42/64 loss: -0.3460683822631836
Batch 43/64 loss: -0.1707000732421875
Batch 44/64 loss: -0.27353858947753906
Batch 45/64 loss: -1.180023193359375
Batch 46/64 loss: -0.5507431030273438
Batch 47/64 loss: -0.6173648834228516
Batch 48/64 loss: -0.9341411590576172
Batch 49/64 loss: -0.6674556732177734
Batch 50/64 loss: -0.8934516906738281
Batch 51/64 loss: -0.8247852325439453
Batch 52/64 loss: -0.6397533416748047
Batch 53/64 loss: -0.8245172500610352
Batch 54/64 loss: -0.3886232376098633
Batch 55/64 loss: -0.7741928100585938
Batch 56/64 loss: -0.6724138259887695
Batch 57/64 loss: -0.5690212249755859
Batch 58/64 loss: -0.8624181747436523
Batch 59/64 loss: -0.8947429656982422
Batch 60/64 loss: -0.46712398529052734
Batch 61/64 loss: -0.8048467636108398
Batch 62/64 loss: -0.7294292449951172
Batch 63/64 loss: -0.5748081207275391
Batch 64/64 loss: -4.4394850730896
Epoch 390  Train loss: -0.6487017332338819  Val loss: -0.4963506718271786
Epoch 391
-------------------------------
Batch 1/64 loss: -0.9243736267089844
Batch 2/64 loss: -0.818760871887207
Batch 3/64 loss: -0.6290550231933594
Batch 4/64 loss: -0.9865446090698242
Batch 5/64 loss: -0.3677635192871094
Batch 6/64 loss: -0.09946823120117188
Batch 7/64 loss: -0.6067495346069336
Batch 8/64 loss: -0.8714132308959961
Batch 9/64 loss: -1.002823829650879
Batch 10/64 loss: -0.41160011291503906
Batch 11/64 loss: -0.4689140319824219
Batch 12/64 loss: -0.8074674606323242
Batch 13/64 loss: -0.5335702896118164
Batch 14/64 loss: -0.7161798477172852
Batch 15/64 loss: -0.7938728332519531
Batch 16/64 loss: -1.0261478424072266
Batch 17/64 loss: -0.5871171951293945
Batch 18/64 loss: -0.8076677322387695
Batch 19/64 loss: -0.7903432846069336
Batch 20/64 loss: -0.4620332717895508
Batch 21/64 loss: -0.9081964492797852
Batch 22/64 loss: -0.8436603546142578
Batch 23/64 loss: -1.0603179931640625
Batch 24/64 loss: -0.21639347076416016
Batch 25/64 loss: -0.6115827560424805
Batch 26/64 loss: -1.0079822540283203
Batch 27/64 loss: -0.7896623611450195
Batch 28/64 loss: -0.6167669296264648
Batch 29/64 loss: -0.6117305755615234
Batch 30/64 loss: -0.38516998291015625
Batch 31/64 loss: -0.849207878112793
Batch 32/64 loss: -0.2272496223449707
Batch 33/64 loss: -0.7471504211425781
Batch 34/64 loss: -0.509678840637207
Batch 35/64 loss: -0.8242683410644531
Batch 36/64 loss: -0.6869745254516602
Batch 37/64 loss: -0.9166488647460938
Batch 38/64 loss: -0.5967922210693359
Batch 39/64 loss: -0.8186531066894531
Batch 40/64 loss: -0.8540430068969727
Batch 41/64 loss: -0.5971574783325195
Batch 42/64 loss: -0.7864112854003906
Batch 43/64 loss: -0.7353982925415039
Batch 44/64 loss: -0.39283180236816406
Batch 45/64 loss: -0.5708189010620117
Batch 46/64 loss: -0.81219482421875
Batch 47/64 loss: -0.6890802383422852
Batch 48/64 loss: -0.2919282913208008
Batch 49/64 loss: -0.2234506607055664
Batch 50/64 loss: -1.0023193359375
Batch 51/64 loss: -1.0173368453979492
Batch 52/64 loss: -0.8180704116821289
Batch 53/64 loss: -0.5801258087158203
Batch 54/64 loss: -0.9970502853393555
Batch 55/64 loss: -0.7118167877197266
Batch 56/64 loss: -0.7447261810302734
Batch 57/64 loss: -1.0086565017700195
Batch 58/64 loss: -0.48566246032714844
Batch 59/64 loss: -1.2251949310302734
Batch 60/64 loss: -0.28792285919189453
Batch 61/64 loss: -0.7801284790039062
Batch 62/64 loss: -0.9365739822387695
Batch 63/64 loss: -0.7798557281494141
Batch 64/64 loss: -4.822569847106934
Epoch 391  Train loss: -0.7511472328036439  Val loss: -0.6864734531677875
Epoch 392
-------------------------------
Batch 1/64 loss: -0.6513462066650391
Batch 2/64 loss: -0.3915700912475586
Batch 3/64 loss: -1.207413673400879
Batch 4/64 loss: -1.1938190460205078
Batch 5/64 loss: -0.71014404296875
Batch 6/64 loss: -0.9908113479614258
Batch 7/64 loss: -0.7859792709350586
Batch 8/64 loss: -0.719609260559082
Batch 9/64 loss: -0.5237998962402344
Batch 10/64 loss: -0.7720251083374023
Batch 11/64 loss: -0.8003053665161133
Batch 12/64 loss: -0.9462032318115234
Batch 13/64 loss: -0.3777303695678711
Batch 14/64 loss: -0.6800193786621094
Batch 15/64 loss: -0.8430700302124023
Batch 16/64 loss: -1.106461524963379
Batch 17/64 loss: -0.49163818359375
Batch 18/64 loss: -0.5745296478271484
Batch 19/64 loss: -0.7206869125366211
Batch 20/64 loss: -0.7337589263916016
Batch 21/64 loss: -0.7149295806884766
Batch 22/64 loss: -0.6885671615600586
Batch 23/64 loss: -0.8752813339233398
Batch 24/64 loss: -0.7472686767578125
Batch 25/64 loss: -0.6297788619995117
Batch 26/64 loss: -0.7165346145629883
Batch 27/64 loss: -0.7872304916381836
Batch 28/64 loss: -0.7128381729125977
Batch 29/64 loss: -1.0281295776367188
Batch 30/64 loss: -0.25370311737060547
Batch 31/64 loss: -0.6239490509033203
Batch 32/64 loss: -0.9069786071777344
Batch 33/64 loss: -0.5287790298461914
Batch 34/64 loss: -0.7144556045532227
Batch 35/64 loss: -0.6178169250488281
Batch 36/64 loss: -0.9808998107910156
Batch 37/64 loss: -0.6111507415771484
Batch 38/64 loss: -0.6960573196411133
Batch 39/64 loss: -0.9572076797485352
Batch 40/64 loss: -0.5671062469482422
Batch 41/64 loss: -0.7523574829101562
Batch 42/64 loss: -0.42652320861816406
Batch 43/64 loss: -1.125554084777832
Batch 44/64 loss: -0.7522716522216797
Batch 45/64 loss: -1.048151969909668
Batch 46/64 loss: -0.9180088043212891
Batch 47/64 loss: -0.6748189926147461
Batch 48/64 loss: -0.9659500122070312
Batch 49/64 loss: -1.080179214477539
Batch 50/64 loss: -0.9011125564575195
Batch 51/64 loss: -0.5300760269165039
Batch 52/64 loss: -0.828312873840332
Batch 53/64 loss: -0.45262813568115234
Batch 54/64 loss: -0.9222640991210938
Batch 55/64 loss: -0.7556905746459961
Batch 56/64 loss: -0.9961490631103516
Batch 57/64 loss: -0.7535915374755859
Batch 58/64 loss: -0.48447608947753906
Batch 59/64 loss: -0.5188016891479492
Batch 60/64 loss: -0.5016851425170898
Batch 61/64 loss: -0.5968809127807617
Batch 62/64 loss: -0.6595697402954102
Batch 63/64 loss: -0.6008844375610352
Batch 64/64 loss: -4.654653072357178
Epoch 392  Train loss: -0.7892786231695437  Val loss: -0.48792312726941717
Epoch 393
-------------------------------
Batch 1/64 loss: -0.8100156784057617
Batch 2/64 loss: -0.8700447082519531
Batch 3/64 loss: -0.8901996612548828
Batch 4/64 loss: -0.6187372207641602
Batch 5/64 loss: -0.042522430419921875
Batch 6/64 loss: -0.8599939346313477
Batch 7/64 loss: -0.9309759140014648
Batch 8/64 loss: -0.8629283905029297
Batch 9/64 loss: -0.8670177459716797
Batch 10/64 loss: -0.5377588272094727
Batch 11/64 loss: -0.6482906341552734
Batch 12/64 loss: -1.1275367736816406
Batch 13/64 loss: -0.8745460510253906
Batch 14/64 loss: -0.6610012054443359
Batch 15/64 loss: -0.9953489303588867
Batch 16/64 loss: -0.7007989883422852
Batch 17/64 loss: -0.7443685531616211
Batch 18/64 loss: -0.1002798080444336
Batch 19/64 loss: -0.8196182250976562
Batch 20/64 loss: -0.9170017242431641
Batch 21/64 loss: -0.7282991409301758
Batch 22/64 loss: -0.9046039581298828
Batch 23/64 loss: -0.4877805709838867
Batch 24/64 loss: -0.41103363037109375
Batch 25/64 loss: -0.7148942947387695
Batch 26/64 loss: -1.2480554580688477
Batch 27/64 loss: -0.7450466156005859
Batch 28/64 loss: -0.34016895294189453
Batch 29/64 loss: -0.5858573913574219
Batch 30/64 loss: -0.8374614715576172
Batch 31/64 loss: -0.7618837356567383
Batch 32/64 loss: -0.5874061584472656
Batch 33/64 loss: -0.5449810028076172
Batch 34/64 loss: -0.7110137939453125
Batch 35/64 loss: -0.945469856262207
Batch 36/64 loss: -1.1968269348144531
Batch 37/64 loss: -0.8196945190429688
Batch 38/64 loss: -0.9635963439941406
Batch 39/64 loss: -0.9745607376098633
Batch 40/64 loss: -0.4566459655761719
Batch 41/64 loss: -0.8715343475341797
Batch 42/64 loss: -0.14387798309326172
Batch 43/64 loss: -0.4043102264404297
Batch 44/64 loss: -0.8234853744506836
Batch 45/64 loss: -0.807708740234375
Batch 46/64 loss: -0.6454277038574219
Batch 47/64 loss: -0.9903755187988281
Batch 48/64 loss: -0.3703298568725586
Batch 49/64 loss: -0.9950275421142578
Batch 50/64 loss: -0.49585819244384766
Batch 51/64 loss: -0.6328210830688477
Batch 52/64 loss: -1.1168174743652344
Batch 53/64 loss: -0.4952878952026367
Batch 54/64 loss: -1.0780658721923828
Batch 55/64 loss: -0.786778450012207
Batch 56/64 loss: -0.7752017974853516
Batch 57/64 loss: -1.2886152267456055
Batch 58/64 loss: -0.5400028228759766
Batch 59/64 loss: -0.4718647003173828
Batch 60/64 loss: -0.74066162109375
Batch 61/64 loss: -0.8928709030151367
Batch 62/64 loss: -0.10168838500976562
Batch 63/64 loss: -0.8488378524780273
Batch 64/64 loss: -5.385221481323242
Epoch 393  Train loss: -0.7874608881333295  Val loss: -0.5341801069856099
Epoch 394
-------------------------------
Batch 1/64 loss: -0.9887962341308594
Batch 2/64 loss: -1.0144119262695312
Batch 3/64 loss: -0.7674827575683594
Batch 4/64 loss: -0.7668132781982422
Batch 5/64 loss: -0.6976957321166992
Batch 6/64 loss: -0.9246835708618164
Batch 7/64 loss: -1.2182273864746094
Batch 8/64 loss: -0.6294631958007812
Batch 9/64 loss: -0.5203304290771484
Batch 10/64 loss: -0.7127780914306641
Batch 11/64 loss: -0.8660154342651367
Batch 12/64 loss: -0.6847391128540039
Batch 13/64 loss: -0.7351245880126953
Batch 14/64 loss: -0.9127378463745117
Batch 15/64 loss: -0.6140766143798828
Batch 16/64 loss: -0.7822532653808594
Batch 17/64 loss: -0.9875402450561523
Batch 18/64 loss: -0.48598766326904297
Batch 19/64 loss: -1.158792495727539
Batch 20/64 loss: -0.9079036712646484
Batch 21/64 loss: -0.6450405120849609
Batch 22/64 loss: -0.5884294509887695
Batch 23/64 loss: -0.7144126892089844
Batch 24/64 loss: -0.32196712493896484
Batch 25/64 loss: -0.7814702987670898
Batch 26/64 loss: 0.18805694580078125
Batch 27/64 loss: -0.6601228713989258
Batch 28/64 loss: -0.6902942657470703
Batch 29/64 loss: -0.6846475601196289
Batch 30/64 loss: -0.8795604705810547
Batch 31/64 loss: -0.3495464324951172
Batch 32/64 loss: -0.4509849548339844
Batch 33/64 loss: -0.33536624908447266
Batch 34/64 loss: -0.5605077743530273
Batch 35/64 loss: -0.6665573120117188
Batch 36/64 loss: -0.2835350036621094
Batch 37/64 loss: -0.7365474700927734
Batch 38/64 loss: -0.7740278244018555
Batch 39/64 loss: -0.9494190216064453
Batch 40/64 loss: -0.21839618682861328
Batch 41/64 loss: -0.7559223175048828
Batch 42/64 loss: -0.9098997116088867
Batch 43/64 loss: -0.14731979370117188
Batch 44/64 loss: 0.01881122589111328
Batch 45/64 loss: -0.14890003204345703
Batch 46/64 loss: -0.15013790130615234
Batch 47/64 loss: -0.5641956329345703
Batch 48/64 loss: -0.40180492401123047
Batch 49/64 loss: -0.6575021743774414
Batch 50/64 loss: -0.5488224029541016
Batch 51/64 loss: -0.5961151123046875
Batch 52/64 loss: -0.5860843658447266
Batch 53/64 loss: -0.30791187286376953
Batch 54/64 loss: -0.5786314010620117
Batch 55/64 loss: -0.4316129684448242
Batch 56/64 loss: -0.44058799743652344
Batch 57/64 loss: -0.7263574600219727
Batch 58/64 loss: -0.22104597091674805
Batch 59/64 loss: -0.15503358840942383
Batch 60/64 loss: -0.2541313171386719
Batch 61/64 loss: -0.3437347412109375
Batch 62/64 loss: -0.45206165313720703
Batch 63/64 loss: -0.5016498565673828
Batch 64/64 loss: -4.67140531539917
Epoch 394  Train loss: -0.6406719376059139  Val loss: -0.34307433977159846
Epoch 395
-------------------------------
Batch 1/64 loss: -0.5277833938598633
Batch 2/64 loss: -0.4033479690551758
Batch 3/64 loss: -0.8223495483398438
Batch 4/64 loss: -0.5392074584960938
Batch 5/64 loss: -0.0233612060546875
Batch 6/64 loss: -0.767338752746582
Batch 7/64 loss: -0.8058261871337891
Batch 8/64 loss: -0.3183250427246094
Batch 9/64 loss: -0.5270853042602539
Batch 10/64 loss: -0.5737857818603516
Batch 11/64 loss: -0.5377435684204102
Batch 12/64 loss: -0.5884571075439453
Batch 13/64 loss: -0.4793357849121094
Batch 14/64 loss: -0.8205318450927734
Batch 15/64 loss: -0.8440427780151367
Batch 16/64 loss: -0.6569414138793945
Batch 17/64 loss: -0.8871288299560547
Batch 18/64 loss: -0.6222314834594727
Batch 19/64 loss: -0.5343294143676758
Batch 20/64 loss: -0.7434492111206055
Batch 21/64 loss: -0.43546104431152344
Batch 22/64 loss: -0.5739030838012695
Batch 23/64 loss: -0.4195537567138672
Batch 24/64 loss: -0.7258100509643555
Batch 25/64 loss: -0.6099433898925781
Batch 26/64 loss: -0.7470178604125977
Batch 27/64 loss: -0.7603302001953125
Batch 28/64 loss: -0.26428890228271484
Batch 29/64 loss: -0.6198940277099609
Batch 30/64 loss: -0.6198348999023438
Batch 31/64 loss: -0.6812896728515625
Batch 32/64 loss: -0.5644588470458984
Batch 33/64 loss: -0.5240335464477539
Batch 34/64 loss: -0.4424781799316406
Batch 35/64 loss: -0.53375244140625
Batch 36/64 loss: -0.6533622741699219
Batch 37/64 loss: -0.5516233444213867
Batch 38/64 loss: -0.4760313034057617
Batch 39/64 loss: -0.7466335296630859
Batch 40/64 loss: -0.8717842102050781
Batch 41/64 loss: -0.7593021392822266
Batch 42/64 loss: -1.0065841674804688
Batch 43/64 loss: -0.6691617965698242
Batch 44/64 loss: -0.6562814712524414
Batch 45/64 loss: -0.6550083160400391
Batch 46/64 loss: -0.6709117889404297
Batch 47/64 loss: -0.5752925872802734
Batch 48/64 loss: -0.7180662155151367
Batch 49/64 loss: -0.7116298675537109
Batch 50/64 loss: -0.5688724517822266
Batch 51/64 loss: -0.5381994247436523
Batch 52/64 loss: -0.8307647705078125
Batch 53/64 loss: -0.7390060424804688
Batch 54/64 loss: -0.6398029327392578
Batch 55/64 loss: -0.8963832855224609
Batch 56/64 loss: -1.0744218826293945
Batch 57/64 loss: -0.5654211044311523
Batch 58/64 loss: -0.7201957702636719
Batch 59/64 loss: -0.4852170944213867
Batch 60/64 loss: -0.9329919815063477
Batch 61/64 loss: -0.45145511627197266
Batch 62/64 loss: -0.4623231887817383
Batch 63/64 loss: -0.6587791442871094
Batch 64/64 loss: -5.0538330078125
Epoch 395  Train loss: -0.6842436696968827  Val loss: -0.5848875013004053
Epoch 396
-------------------------------
Batch 1/64 loss: -0.8686628341674805
Batch 2/64 loss: -0.9470329284667969
Batch 3/64 loss: -0.9874019622802734
Batch 4/64 loss: -0.9569587707519531
Batch 5/64 loss: -0.8927097320556641
Batch 6/64 loss: -0.5439329147338867
Batch 7/64 loss: -0.561802864074707
Batch 8/64 loss: -0.9212312698364258
Batch 9/64 loss: -0.6212291717529297
Batch 10/64 loss: -0.19359397888183594
Batch 11/64 loss: -0.8590259552001953
Batch 12/64 loss: -0.8616342544555664
Batch 13/64 loss: -0.9930276870727539
Batch 14/64 loss: -0.5837516784667969
Batch 15/64 loss: -0.9016733169555664
Batch 16/64 loss: -0.4805335998535156
Batch 17/64 loss: -0.5231161117553711
Batch 18/64 loss: -0.4680023193359375
Batch 19/64 loss: -1.0234737396240234
Batch 20/64 loss: -0.6250638961791992
Batch 21/64 loss: -0.8680896759033203
Batch 22/64 loss: -0.6707706451416016
Batch 23/64 loss: -0.6166458129882812
Batch 24/64 loss: -0.7280807495117188
Batch 25/64 loss: -0.9175395965576172
Batch 26/64 loss: -0.66033935546875
Batch 27/64 loss: -1.0537109375
Batch 28/64 loss: -0.6667146682739258
Batch 29/64 loss: -1.1117019653320312
Batch 30/64 loss: -0.7889289855957031
Batch 31/64 loss: -0.6711111068725586
Batch 32/64 loss: -0.6124668121337891
Batch 33/64 loss: -0.6634616851806641
Batch 34/64 loss: -0.9804048538208008
Batch 35/64 loss: -1.0040836334228516
Batch 36/64 loss: -1.0083255767822266
Batch 37/64 loss: -0.977294921875
Batch 38/64 loss: -0.7822265625
Batch 39/64 loss: -0.5495777130126953
Batch 40/64 loss: 0.05647087097167969
Batch 41/64 loss: -1.109145164489746
Batch 42/64 loss: -0.5187721252441406
Batch 43/64 loss: -0.5528440475463867
Batch 44/64 loss: -0.4161491394042969
Batch 45/64 loss: -0.6073141098022461
Batch 46/64 loss: -1.0195722579956055
Batch 47/64 loss: -1.019796371459961
Batch 48/64 loss: -0.8619680404663086
Batch 49/64 loss: -0.8678503036499023
Batch 50/64 loss: -0.6731204986572266
Batch 51/64 loss: -0.4323854446411133
Batch 52/64 loss: -0.8325099945068359
Batch 53/64 loss: -0.8757762908935547
Batch 54/64 loss: -0.7273893356323242
Batch 55/64 loss: -0.7307262420654297
Batch 56/64 loss: -0.8858709335327148
Batch 57/64 loss: -0.7011528015136719
Batch 58/64 loss: -0.8513002395629883
Batch 59/64 loss: -0.1698780059814453
Batch 60/64 loss: -1.0598020553588867
Batch 61/64 loss: -0.3314981460571289
Batch 62/64 loss: -0.4923229217529297
Batch 63/64 loss: -0.23587799072265625
Batch 64/64 loss: -5.0260539054870605
Epoch 396  Train loss: -0.7816694315742044  Val loss: -0.5798560335873738
Epoch 397
-------------------------------
Batch 1/64 loss: -0.6227607727050781
Batch 2/64 loss: -1.0129470825195312
Batch 3/64 loss: -0.4382486343383789
Batch 4/64 loss: -0.6311798095703125
Batch 5/64 loss: -0.4892578125
Batch 6/64 loss: -0.5437593460083008
Batch 7/64 loss: -1.000615119934082
Batch 8/64 loss: -0.31258201599121094
Batch 9/64 loss: -0.8053293228149414
Batch 10/64 loss: -0.6595020294189453
Batch 11/64 loss: -0.7086601257324219
Batch 12/64 loss: -1.0680522918701172
Batch 13/64 loss: -0.7363567352294922
Batch 14/64 loss: -0.8507661819458008
Batch 15/64 loss: -0.5779190063476562
Batch 16/64 loss: -0.6781101226806641
Batch 17/64 loss: -0.6610832214355469
Batch 18/64 loss: -0.9887466430664062
Batch 19/64 loss: -0.8084201812744141
Batch 20/64 loss: -0.9834136962890625
Batch 21/64 loss: -0.9353952407836914
Batch 22/64 loss: -0.6751079559326172
Batch 23/64 loss: -0.8398656845092773
Batch 24/64 loss: -0.7290258407592773
Batch 25/64 loss: -0.5113544464111328
Batch 26/64 loss: -0.6691951751708984
Batch 27/64 loss: -0.7782297134399414
Batch 28/64 loss: -0.8996810913085938
Batch 29/64 loss: -0.738189697265625
Batch 30/64 loss: -0.5784883499145508
Batch 31/64 loss: -0.4533090591430664
Batch 32/64 loss: -0.6836395263671875
Batch 33/64 loss: -0.44429683685302734
Batch 34/64 loss: -0.385528564453125
Batch 35/64 loss: -0.8777866363525391
Batch 36/64 loss: -0.4906473159790039
Batch 37/64 loss: -0.7526130676269531
Batch 38/64 loss: -0.8022041320800781
Batch 39/64 loss: -0.6866550445556641
Batch 40/64 loss: -0.7434539794921875
Batch 41/64 loss: -0.8372297286987305
Batch 42/64 loss: -0.6879281997680664
Batch 43/64 loss: -0.7978401184082031
Batch 44/64 loss: -0.7419395446777344
Batch 45/64 loss: -0.7188968658447266
Batch 46/64 loss: -0.8933401107788086
Batch 47/64 loss: -0.5543193817138672
Batch 48/64 loss: -0.7790021896362305
Batch 49/64 loss: -0.9852828979492188
Batch 50/64 loss: -0.7928628921508789
Batch 51/64 loss: -1.0947847366333008
Batch 52/64 loss: -1.1068735122680664
Batch 53/64 loss: -1.0330381393432617
Batch 54/64 loss: -0.8322954177856445
Batch 55/64 loss: -0.46603965759277344
Batch 56/64 loss: -0.8966169357299805
Batch 57/64 loss: -0.8619050979614258
Batch 58/64 loss: -0.4402313232421875
Batch 59/64 loss: -0.544459342956543
Batch 60/64 loss: -0.17032623291015625
Batch 61/64 loss: -0.9641914367675781
Batch 62/64 loss: -0.8024454116821289
Batch 63/64 loss: -0.48635101318359375
Batch 64/64 loss: -5.088254451751709
Epoch 397  Train loss: -0.7773610750834147  Val loss: -0.6829483976069185
Epoch 398
-------------------------------
Batch 1/64 loss: -1.0604734420776367
Batch 2/64 loss: -0.6908864974975586
Batch 3/64 loss: -0.684697151184082
Batch 4/64 loss: -0.9669189453125
Batch 5/64 loss: -1.094137191772461
Batch 6/64 loss: -0.6071348190307617
Batch 7/64 loss: -0.8287944793701172
Batch 8/64 loss: -0.8392362594604492
Batch 9/64 loss: -0.7418375015258789
Batch 10/64 loss: -0.9107513427734375
Batch 11/64 loss: -1.0569257736206055
Batch 12/64 loss: -1.0325021743774414
Batch 13/64 loss: -0.848170280456543
Batch 14/64 loss: -0.7138166427612305
Batch 15/64 loss: -0.5180435180664062
Batch 16/64 loss: -0.7953310012817383
Batch 17/64 loss: -0.3299131393432617
Batch 18/64 loss: -0.6717710494995117
Batch 19/64 loss: -0.896031379699707
Batch 20/64 loss: -0.581913948059082
Batch 21/64 loss: -0.7032718658447266
Batch 22/64 loss: -0.5024232864379883
Batch 23/64 loss: -0.9317026138305664
Batch 24/64 loss: -0.9356517791748047
Batch 25/64 loss: -0.5706186294555664
Batch 26/64 loss: -0.5621957778930664
Batch 27/64 loss: -0.6647415161132812
Batch 28/64 loss: -0.7347192764282227
Batch 29/64 loss: -0.24872541427612305
Batch 30/64 loss: -0.4652976989746094
Batch 31/64 loss: -0.865234375
Batch 32/64 loss: -0.7782526016235352
Batch 33/64 loss: -0.8829545974731445
Batch 34/64 loss: -0.6345643997192383
Batch 35/64 loss: -0.708038330078125
Batch 36/64 loss: -0.7203311920166016
Batch 37/64 loss: -0.5062732696533203
Batch 38/64 loss: -0.8805837631225586
Batch 39/64 loss: -0.7810268402099609
Batch 40/64 loss: -0.7755422592163086
Batch 41/64 loss: -0.8573923110961914
Batch 42/64 loss: -0.8263120651245117
Batch 43/64 loss: -0.5948266983032227
Batch 44/64 loss: -0.8109827041625977
Batch 45/64 loss: -0.29500484466552734
Batch 46/64 loss: -0.5884323120117188
Batch 47/64 loss: -1.067953109741211
Batch 48/64 loss: -1.0176286697387695
Batch 49/64 loss: -0.7068300247192383
Batch 50/64 loss: -0.28584766387939453
Batch 51/64 loss: -0.6244783401489258
Batch 52/64 loss: -0.6128358840942383
Batch 53/64 loss: -0.3325204849243164
Batch 54/64 loss: -0.6528701782226562
Batch 55/64 loss: -1.0967473983764648
Batch 56/64 loss: -0.42740821838378906
Batch 57/64 loss: -1.0425500869750977
Batch 58/64 loss: -1.2017250061035156
Batch 59/64 loss: -0.6609659194946289
Batch 60/64 loss: -1.1013660430908203
Batch 61/64 loss: -0.5216207504272461
Batch 62/64 loss: -0.8458824157714844
Batch 63/64 loss: -0.8550577163696289
Batch 64/64 loss: -3.946187973022461
Epoch 398  Train loss: -0.7797382878322228  Val loss: -0.775405988660465
Saving best model, epoch: 398
Epoch 399
-------------------------------
Batch 1/64 loss: -0.6210422515869141
Batch 2/64 loss: -0.26010799407958984
Batch 3/64 loss: -1.0238943099975586
Batch 4/64 loss: -0.6932077407836914
Batch 5/64 loss: -0.22174835205078125
Batch 6/64 loss: -0.7928171157836914
Batch 7/64 loss: -0.8072214126586914
Batch 8/64 loss: -0.9571723937988281
Batch 9/64 loss: -1.0708436965942383
Batch 10/64 loss: -0.7909507751464844
Batch 11/64 loss: -0.44667530059814453
Batch 12/64 loss: -0.7918052673339844
Batch 13/64 loss: -0.5279264450073242
Batch 14/64 loss: -0.9294166564941406
Batch 15/64 loss: -0.4461221694946289
Batch 16/64 loss: -1.040635108947754
Batch 17/64 loss: -0.8672113418579102
Batch 18/64 loss: -1.0793275833129883
Batch 19/64 loss: -0.7159194946289062
Batch 20/64 loss: -0.8939638137817383
Batch 21/64 loss: -0.3785572052001953
Batch 22/64 loss: -0.8799581527709961
Batch 23/64 loss: -0.6867132186889648
Batch 24/64 loss: -0.5103549957275391
Batch 25/64 loss: -0.6891345977783203
Batch 26/64 loss: -1.005356788635254
Batch 27/64 loss: -0.7078971862792969
Batch 28/64 loss: -0.7847967147827148
Batch 29/64 loss: -0.8585090637207031
Batch 30/64 loss: -0.9227666854858398
Batch 31/64 loss: -0.6991090774536133
Batch 32/64 loss: -0.8481788635253906
Batch 33/64 loss: -0.4598226547241211
Batch 34/64 loss: -1.0568151473999023
Batch 35/64 loss: -0.8589239120483398
Batch 36/64 loss: -0.40907955169677734
Batch 37/64 loss: -0.8456735610961914
Batch 38/64 loss: -0.8973798751831055
Batch 39/64 loss: -0.8750934600830078
Batch 40/64 loss: -0.8177013397216797
Batch 41/64 loss: -0.8714590072631836
Batch 42/64 loss: -0.38564586639404297
Batch 43/64 loss: -0.6354761123657227
Batch 44/64 loss: -0.7830991744995117
Batch 45/64 loss: -0.8298826217651367
Batch 46/64 loss: -0.4965667724609375
Batch 47/64 loss: -1.0115327835083008
Batch 48/64 loss: -0.15663623809814453
Batch 49/64 loss: -0.9621267318725586
Batch 50/64 loss: -0.9032497406005859
Batch 51/64 loss: -0.9884710311889648
Batch 52/64 loss: -0.9170398712158203
Batch 53/64 loss: -0.42000293731689453
Batch 54/64 loss: -0.4743537902832031
Batch 55/64 loss: -0.4849090576171875
Batch 56/64 loss: -1.0353269577026367
Batch 57/64 loss: -0.9687843322753906
Batch 58/64 loss: -0.3806114196777344
Batch 59/64 loss: -0.7925443649291992
Batch 60/64 loss: -0.7036151885986328
Batch 61/64 loss: -0.569758415222168
Batch 62/64 loss: -0.9002456665039062
Batch 63/64 loss: -0.8875055313110352
Batch 64/64 loss: -5.179086685180664
Epoch 399  Train loss: -0.7939292533724915  Val loss: -0.521430956129356
Epoch 400
-------------------------------
Batch 1/64 loss: -0.4042339324951172
Batch 2/64 loss: -1.250838279724121
Batch 3/64 loss: -0.41597557067871094
Batch 4/64 loss: -0.45674610137939453
Batch 5/64 loss: -0.7792348861694336
Batch 6/64 loss: -0.6489429473876953
Batch 7/64 loss: -0.9119939804077148
Batch 8/64 loss: -0.4538078308105469
Batch 9/64 loss: -0.9571695327758789
Batch 10/64 loss: -0.7441625595092773
Batch 11/64 loss: -1.1393146514892578
Batch 12/64 loss: -1.0722179412841797
Batch 13/64 loss: -0.9080343246459961
Batch 14/64 loss: -0.8155117034912109
Batch 15/64 loss: -0.7580928802490234
Batch 16/64 loss: -0.5691337585449219
Batch 17/64 loss: -0.5898590087890625
Batch 18/64 loss: -0.6121559143066406
Batch 19/64 loss: -0.8097953796386719
Batch 20/64 loss: -0.7931737899780273
Batch 21/64 loss: -0.8354101181030273
Batch 22/64 loss: -0.9959344863891602
Batch 23/64 loss: -0.8521051406860352
Batch 24/64 loss: -0.8291511535644531
Batch 25/64 loss: -0.5716733932495117
Batch 26/64 loss: -0.4891958236694336
Batch 27/64 loss: -0.6832189559936523
Batch 28/64 loss: -0.4486379623413086
Batch 29/64 loss: -0.6263837814331055
Batch 30/64 loss: -0.9697389602661133
Batch 31/64 loss: -1.0483312606811523
Batch 32/64 loss: -0.9949455261230469
Batch 33/64 loss: -0.9815521240234375
Batch 34/64 loss: -0.9351062774658203
Batch 35/64 loss: -0.7438983917236328
Batch 36/64 loss: -0.9700174331665039
Batch 37/64 loss: -0.5228719711303711
Batch 38/64 loss: -0.6262483596801758
Batch 39/64 loss: -0.7421951293945312
Batch 40/64 loss: -0.7312231063842773
Batch 41/64 loss: -0.6632709503173828
Batch 42/64 loss: -0.23399066925048828
Batch 43/64 loss: -0.8902006149291992
Batch 44/64 loss: -0.6739082336425781
Batch 45/64 loss: -0.8477602005004883
Batch 46/64 loss: -0.703709602355957
Batch 47/64 loss: -1.1104202270507812
Batch 48/64 loss: -0.9017210006713867
Batch 49/64 loss: -0.7525320053100586
Batch 50/64 loss: -0.07791614532470703
Batch 51/64 loss: -1.0092048645019531
Batch 52/64 loss: -0.8348302841186523
Batch 53/64 loss: -0.8685827255249023
Batch 54/64 loss: -0.9002599716186523
Batch 55/64 loss: -0.541839599609375
Batch 56/64 loss: -0.9649591445922852
Batch 57/64 loss: -0.48809051513671875
Batch 58/64 loss: -0.7259273529052734
Batch 59/64 loss: -0.9444198608398438
Batch 60/64 loss: -0.7758464813232422
Batch 61/64 loss: -0.3895273208618164
Batch 62/64 loss: -0.7050819396972656
Batch 63/64 loss: -0.3360462188720703
Batch 64/64 loss: -5.318483352661133
Epoch 400  Train loss: -0.8002689062380324  Val loss: -0.652964116781438
Epoch 401
-------------------------------
Batch 1/64 loss: -0.5226325988769531
Batch 2/64 loss: -0.9112091064453125
Batch 3/64 loss: -0.8194303512573242
Batch 4/64 loss: -0.8164243698120117
Batch 5/64 loss: -1.0761404037475586
Batch 6/64 loss: -0.9066791534423828
Batch 7/64 loss: -1.0500059127807617
Batch 8/64 loss: -0.910191535949707
Batch 9/64 loss: -0.5518169403076172
Batch 10/64 loss: -0.4035005569458008
Batch 11/64 loss: -1.0418157577514648
Batch 12/64 loss: -0.7977848052978516
Batch 13/64 loss: -0.7150459289550781
Batch 14/64 loss: -1.216099739074707
Batch 15/64 loss: -0.7910003662109375
Batch 16/64 loss: -0.42403316497802734
Batch 17/64 loss: -0.960444450378418
Batch 18/64 loss: -0.762843132019043
Batch 19/64 loss: -0.40167903900146484
Batch 20/64 loss: -0.952855110168457
Batch 21/64 loss: -1.0763053894042969
Batch 22/64 loss: -0.6653051376342773
Batch 23/64 loss: -0.7731847763061523
Batch 24/64 loss: -0.38317298889160156
Batch 25/64 loss: -1.0077400207519531
Batch 26/64 loss: -0.950465202331543
Batch 27/64 loss: -0.8114995956420898
Batch 28/64 loss: -0.7892885208129883
Batch 29/64 loss: -0.4647712707519531
Batch 30/64 loss: -0.7668876647949219
Batch 31/64 loss: -0.6560649871826172
Batch 32/64 loss: -0.8095026016235352
Batch 33/64 loss: -0.9438686370849609
Batch 34/64 loss: -0.20032691955566406
Batch 35/64 loss: -0.6624660491943359
Batch 36/64 loss: -0.8846673965454102
Batch 37/64 loss: -0.4800376892089844
Batch 38/64 loss: -0.9460077285766602
Batch 39/64 loss: -0.13220882415771484
Batch 40/64 loss: -0.6459903717041016
Batch 41/64 loss: -0.7291908264160156
Batch 42/64 loss: -0.5597209930419922
Batch 43/64 loss: -1.1160831451416016
Batch 44/64 loss: -0.35997676849365234
Batch 45/64 loss: -1.1110639572143555
Batch 46/64 loss: -1.0754461288452148
Batch 47/64 loss: -0.8569650650024414
Batch 48/64 loss: -0.9667263031005859
Batch 49/64 loss: -0.7286224365234375
Batch 50/64 loss: -1.0660781860351562
Batch 51/64 loss: -0.8656082153320312
Batch 52/64 loss: -0.722285270690918
Batch 53/64 loss: -0.29055023193359375
Batch 54/64 loss: -0.8612995147705078
Batch 55/64 loss: -0.7587432861328125
Batch 56/64 loss: -0.8120260238647461
Batch 57/64 loss: -1.070652961730957
Batch 58/64 loss: -0.8545103073120117
Batch 59/64 loss: -0.5955371856689453
Batch 60/64 loss: -0.8671493530273438
Batch 61/64 loss: -0.9131813049316406
Batch 62/64 loss: -0.3151826858520508
Batch 63/64 loss: -0.9736423492431641
Batch 64/64 loss: -5.2556915283203125
Epoch 401  Train loss: -0.8234259661506204  Val loss: -0.7993975636066031
Saving best model, epoch: 401
Epoch 402
-------------------------------
Batch 1/64 loss: -1.2523975372314453
Batch 2/64 loss: -0.7615327835083008
Batch 3/64 loss: -0.8370952606201172
Batch 4/64 loss: -0.8398618698120117
Batch 5/64 loss: -0.7783126831054688
Batch 6/64 loss: -0.8199281692504883
Batch 7/64 loss: -1.0631961822509766
Batch 8/64 loss: -0.32451820373535156
Batch 9/64 loss: -0.6354761123657227
Batch 10/64 loss: -0.683013916015625
Batch 11/64 loss: -0.8498916625976562
Batch 12/64 loss: -0.8942298889160156
Batch 13/64 loss: -0.8640432357788086
Batch 14/64 loss: -1.02960205078125
Batch 15/64 loss: -1.2190217971801758
Batch 16/64 loss: -0.8253355026245117
Batch 17/64 loss: -1.185256004333496
Batch 18/64 loss: -0.8589286804199219
Batch 19/64 loss: -0.6673707962036133
Batch 20/64 loss: -0.3231849670410156
Batch 21/64 loss: -0.6475353240966797
Batch 22/64 loss: -0.9234828948974609
Batch 23/64 loss: -1.0854997634887695
Batch 24/64 loss: -1.062830924987793
Batch 25/64 loss: -0.8814725875854492
Batch 26/64 loss: -0.4954986572265625
Batch 27/64 loss: -0.6613311767578125
Batch 28/64 loss: -1.0259456634521484
Batch 29/64 loss: -0.7520875930786133
Batch 30/64 loss: -0.8852415084838867
Batch 31/64 loss: -0.4819488525390625
Batch 32/64 loss: -1.0668668746948242
Batch 33/64 loss: -0.6539888381958008
Batch 34/64 loss: -0.5992202758789062
Batch 35/64 loss: -1.1136608123779297
Batch 36/64 loss: -1.0787343978881836
Batch 37/64 loss: -1.1836929321289062
Batch 38/64 loss: -0.8200521469116211
Batch 39/64 loss: -1.0168142318725586
Batch 40/64 loss: -0.7801380157470703
Batch 41/64 loss: -0.47669410705566406
Batch 42/64 loss: -0.6373491287231445
Batch 43/64 loss: -1.1338787078857422
Batch 44/64 loss: -1.050577163696289
Batch 45/64 loss: -0.4682464599609375
Batch 46/64 loss: -0.579716682434082
Batch 47/64 loss: -1.0533323287963867
Batch 48/64 loss: -0.9992847442626953
Batch 49/64 loss: -1.106919288635254
Batch 50/64 loss: -0.3972797393798828
Batch 51/64 loss: -0.6519289016723633
Batch 52/64 loss: -0.5003147125244141
Batch 53/64 loss: -0.5670995712280273
Batch 54/64 loss: -0.6991996765136719
Batch 55/64 loss: -0.658482551574707
Batch 56/64 loss: -0.7012786865234375
Batch 57/64 loss: -0.39807605743408203
Batch 58/64 loss: -0.9062185287475586
Batch 59/64 loss: -0.6891813278198242
Batch 60/64 loss: -0.9997377395629883
Batch 61/64 loss: -1.0310297012329102
Batch 62/64 loss: -0.6035737991333008
Batch 63/64 loss: -0.899714469909668
Batch 64/64 loss: -4.743216514587402
Epoch 402  Train loss: -0.8579571331248564  Val loss: -0.5938756752669606
Epoch 403
-------------------------------
Batch 1/64 loss: -0.3648357391357422
Batch 2/64 loss: -0.5940275192260742
Batch 3/64 loss: -0.6374654769897461
Batch 4/64 loss: -1.0944204330444336
Batch 5/64 loss: -0.7656688690185547
Batch 6/64 loss: -0.712158203125
Batch 7/64 loss: -0.7510995864868164
Batch 8/64 loss: -0.7149448394775391
Batch 9/64 loss: -0.8911781311035156
Batch 10/64 loss: -0.5244007110595703
Batch 11/64 loss: -0.06779623031616211
Batch 12/64 loss: -1.0044775009155273
Batch 13/64 loss: -0.6035690307617188
Batch 14/64 loss: -0.7901334762573242
Batch 15/64 loss: -0.8301925659179688
Batch 16/64 loss: -0.9198856353759766
Batch 17/64 loss: -0.5879487991333008
Batch 18/64 loss: -0.4321098327636719
Batch 19/64 loss: -0.9563055038452148
Batch 20/64 loss: -0.6224765777587891
Batch 21/64 loss: -0.6170282363891602
Batch 22/64 loss: -1.0963249206542969
Batch 23/64 loss: -0.43352222442626953
Batch 24/64 loss: -0.22609663009643555
Batch 25/64 loss: -0.6448297500610352
Batch 26/64 loss: -0.7332668304443359
Batch 27/64 loss: -0.5433177947998047
Batch 28/64 loss: -0.4325847625732422
Batch 29/64 loss: -0.8591623306274414
Batch 30/64 loss: -0.744898796081543
Batch 31/64 loss: -0.8591709136962891
Batch 32/64 loss: -1.0009393692016602
Batch 33/64 loss: -0.9229307174682617
Batch 34/64 loss: -0.7949562072753906
Batch 35/64 loss: -0.9641923904418945
Batch 36/64 loss: -0.8493127822875977
Batch 37/64 loss: -0.9707574844360352
Batch 38/64 loss: -0.7427759170532227
Batch 39/64 loss: -0.36254215240478516
Batch 40/64 loss: -0.4720611572265625
Batch 41/64 loss: -0.9297008514404297
Batch 42/64 loss: -0.562098503112793
Batch 43/64 loss: -0.8567676544189453
Batch 44/64 loss: -0.12536859512329102
Batch 45/64 loss: -0.5988759994506836
Batch 46/64 loss: -0.6217021942138672
Batch 47/64 loss: -0.8507070541381836
Batch 48/64 loss: -0.5636739730834961
Batch 49/64 loss: -0.5561866760253906
Batch 50/64 loss: -1.0346012115478516
Batch 51/64 loss: -0.8184890747070312
Batch 52/64 loss: -0.9737758636474609
Batch 53/64 loss: -1.018026351928711
Batch 54/64 loss: -0.6411046981811523
Batch 55/64 loss: -0.8422565460205078
Batch 56/64 loss: -1.0349798202514648
Batch 57/64 loss: -0.9406137466430664
Batch 58/64 loss: -1.0370187759399414
Batch 59/64 loss: -0.5120048522949219
Batch 60/64 loss: -0.9691352844238281
Batch 61/64 loss: -0.959834098815918
Batch 62/64 loss: -0.7959938049316406
Batch 63/64 loss: -0.7318172454833984
Batch 64/64 loss: -5.011913299560547
Epoch 403  Train loss: -0.7822656294878791  Val loss: -0.5998474068658048
Epoch 404
-------------------------------
Batch 1/64 loss: -0.43149757385253906
Batch 2/64 loss: -0.7236499786376953
Batch 3/64 loss: -0.7105255126953125
Batch 4/64 loss: -0.972804069519043
Batch 5/64 loss: -0.6888885498046875
Batch 6/64 loss: -0.5274105072021484
Batch 7/64 loss: -0.5532569885253906
Batch 8/64 loss: -1.066725730895996
Batch 9/64 loss: -0.8519659042358398
Batch 10/64 loss: -0.6841831207275391
Batch 11/64 loss: -0.8676557540893555
Batch 12/64 loss: -0.876220703125
Batch 13/64 loss: -0.8563327789306641
Batch 14/64 loss: -0.5531997680664062
Batch 15/64 loss: -0.969172477722168
Batch 16/64 loss: -0.8486042022705078
Batch 17/64 loss: -0.6137790679931641
Batch 18/64 loss: -1.3148870468139648
Batch 19/64 loss: -0.9085454940795898
Batch 20/64 loss: -0.8045711517333984
Batch 21/64 loss: -1.060892105102539
Batch 22/64 loss: -0.7936372756958008
Batch 23/64 loss: -1.0984058380126953
Batch 24/64 loss: -0.9544610977172852
Batch 25/64 loss: -0.8199729919433594
Batch 26/64 loss: -1.1780881881713867
Batch 27/64 loss: -0.8443937301635742
Batch 28/64 loss: -0.6020469665527344
Batch 29/64 loss: -0.6241559982299805
Batch 30/64 loss: -1.1048669815063477
Batch 31/64 loss: -0.2150096893310547
Batch 32/64 loss: -0.6670761108398438
Batch 33/64 loss: -0.8331394195556641
Batch 34/64 loss: -1.2808570861816406
Batch 35/64 loss: -0.4454317092895508
Batch 36/64 loss: -0.8659286499023438
Batch 37/64 loss: -0.9332962036132812
Batch 38/64 loss: -1.0191669464111328
Batch 39/64 loss: -0.6353473663330078
Batch 40/64 loss: -0.69976806640625
Batch 41/64 loss: -0.6938800811767578
Batch 42/64 loss: -0.631561279296875
Batch 43/64 loss: -0.5244474411010742
Batch 44/64 loss: -0.989903450012207
Batch 45/64 loss: -0.7272939682006836
Batch 46/64 loss: -0.6565685272216797
Batch 47/64 loss: -0.6953821182250977
Batch 48/64 loss: -0.6762113571166992
Batch 49/64 loss: -0.9145183563232422
Batch 50/64 loss: -0.7734451293945312
Batch 51/64 loss: -1.0531501770019531
Batch 52/64 loss: -0.684443473815918
Batch 53/64 loss: -0.6769447326660156
Batch 54/64 loss: -0.07888603210449219
Batch 55/64 loss: -0.9939126968383789
Batch 56/64 loss: -0.8094701766967773
Batch 57/64 loss: -0.8114080429077148
Batch 58/64 loss: -0.9755697250366211
Batch 59/64 loss: -0.7999515533447266
Batch 60/64 loss: -0.8660974502563477
Batch 61/64 loss: -0.9284038543701172
Batch 62/64 loss: -0.6793498992919922
Batch 63/64 loss: -0.4164848327636719
Batch 64/64 loss: -4.628437042236328
Epoch 404  Train loss: -0.8318185245289522  Val loss: -0.5942676845694735
Epoch 405
-------------------------------
Batch 1/64 loss: -1.0401363372802734
Batch 2/64 loss: -0.33625364303588867
Batch 3/64 loss: -0.5622224807739258
Batch 4/64 loss: -0.9772844314575195
Batch 5/64 loss: -0.7699356079101562
Batch 6/64 loss: -0.9086313247680664
Batch 7/64 loss: -0.9744987487792969
Batch 8/64 loss: -0.5110015869140625
Batch 9/64 loss: -0.9293918609619141
Batch 10/64 loss: -0.49074649810791016
Batch 11/64 loss: -0.5537137985229492
Batch 12/64 loss: -0.9194269180297852
Batch 13/64 loss: -1.0210113525390625
Batch 14/64 loss: -0.7172088623046875
Batch 15/64 loss: -1.0532236099243164
Batch 16/64 loss: -0.5474348068237305
Batch 17/64 loss: -1.3180255889892578
Batch 18/64 loss: -0.7853260040283203
Batch 19/64 loss: -0.7342128753662109
Batch 20/64 loss: -0.9390573501586914
Batch 21/64 loss: -0.9528350830078125
Batch 22/64 loss: -0.9825010299682617
Batch 23/64 loss: -0.7182455062866211
Batch 24/64 loss: -0.9656801223754883
Batch 25/64 loss: -0.918701171875
Batch 26/64 loss: -0.5035915374755859
Batch 27/64 loss: -0.7788410186767578
Batch 28/64 loss: -0.24913406372070312
Batch 29/64 loss: -0.834467887878418
Batch 30/64 loss: -0.7332448959350586
Batch 31/64 loss: -0.6194858551025391
Batch 32/64 loss: -0.775303840637207
Batch 33/64 loss: -0.9826231002807617
Batch 34/64 loss: -0.56854248046875
Batch 35/64 loss: -0.6644382476806641
Batch 36/64 loss: -0.7427053451538086
Batch 37/64 loss: -0.5686330795288086
Batch 38/64 loss: -1.0741777420043945
Batch 39/64 loss: -0.8160829544067383
Batch 40/64 loss: -0.6410188674926758
Batch 41/64 loss: -0.8404264450073242
Batch 42/64 loss: -0.9007167816162109
Batch 43/64 loss: -0.7755632400512695
Batch 44/64 loss: -0.5561208724975586
Batch 45/64 loss: -0.48925209045410156
Batch 46/64 loss: -0.6049118041992188
Batch 47/64 loss: -0.681370735168457
Batch 48/64 loss: -1.0640907287597656
Batch 49/64 loss: -1.2295160293579102
Batch 50/64 loss: -0.8638525009155273
Batch 51/64 loss: -0.47147083282470703
Batch 52/64 loss: -0.6562099456787109
Batch 53/64 loss: -0.6939401626586914
Batch 54/64 loss: -0.4433717727661133
Batch 55/64 loss: -0.34923362731933594
Batch 56/64 loss: -0.3879108428955078
Batch 57/64 loss: -0.6304779052734375
Batch 58/64 loss: -1.0831079483032227
Batch 59/64 loss: -0.4430866241455078
Batch 60/64 loss: -0.8189640045166016
Batch 61/64 loss: -0.6300582885742188
Batch 62/64 loss: -0.8908462524414062
Batch 63/64 loss: -0.44763660430908203
Batch 64/64 loss: -5.049598693847656
Epoch 405  Train loss: -0.798718942380419  Val loss: -0.4998886527884047
Epoch 406
-------------------------------
Batch 1/64 loss: -0.953465461730957
Batch 2/64 loss: -0.4933595657348633
Batch 3/64 loss: -1.081986427307129
Batch 4/64 loss: -0.70123291015625
Batch 5/64 loss: -0.760960578918457
Batch 6/64 loss: -0.6489953994750977
Batch 7/64 loss: -0.7279653549194336
Batch 8/64 loss: -0.4962730407714844
Batch 9/64 loss: -0.7945117950439453
Batch 10/64 loss: -0.8500089645385742
Batch 11/64 loss: -0.6912069320678711
Batch 12/64 loss: -0.8068170547485352
Batch 13/64 loss: -0.8320512771606445
Batch 14/64 loss: -0.8534450531005859
Batch 15/64 loss: -0.6071643829345703
Batch 16/64 loss: -0.6906843185424805
Batch 17/64 loss: -0.6960544586181641
Batch 18/64 loss: -0.913121223449707
Batch 19/64 loss: -0.6362400054931641
Batch 20/64 loss: -0.5466737747192383
Batch 21/64 loss: -0.27466678619384766
Batch 22/64 loss: -0.8729696273803711
Batch 23/64 loss: -0.5447845458984375
Batch 24/64 loss: -0.7423210144042969
Batch 25/64 loss: -0.877685546875
Batch 26/64 loss: -0.29642391204833984
Batch 27/64 loss: -0.4446420669555664
Batch 28/64 loss: -1.0586252212524414
Batch 29/64 loss: -1.0142173767089844
Batch 30/64 loss: 0.04925060272216797
Batch 31/64 loss: -1.0146856307983398
Batch 32/64 loss: -0.8543062210083008
Batch 33/64 loss: -0.48301029205322266
Batch 34/64 loss: -0.44834423065185547
Batch 35/64 loss: -0.45343017578125
Batch 36/64 loss: -0.2358999252319336
Batch 37/64 loss: -0.3832511901855469
Batch 38/64 loss: -0.33500099182128906
Batch 39/64 loss: -0.7849769592285156
Batch 40/64 loss: -0.8431415557861328
Batch 41/64 loss: -0.6279888153076172
Batch 42/64 loss: -0.8896284103393555
Batch 43/64 loss: -0.9731693267822266
Batch 44/64 loss: -1.068704605102539
Batch 45/64 loss: -0.7422685623168945
Batch 46/64 loss: -0.8244419097900391
Batch 47/64 loss: -0.8256845474243164
Batch 48/64 loss: -0.08861541748046875
Batch 49/64 loss: -1.0333385467529297
Batch 50/64 loss: -1.0150928497314453
Batch 51/64 loss: -0.39502429962158203
Batch 52/64 loss: -0.7110767364501953
Batch 53/64 loss: -1.0443639755249023
Batch 54/64 loss: -0.6539096832275391
Batch 55/64 loss: -0.8095693588256836
Batch 56/64 loss: -1.082575798034668
Batch 57/64 loss: -1.0896272659301758
Batch 58/64 loss: -0.4157905578613281
Batch 59/64 loss: -0.7732057571411133
Batch 60/64 loss: -0.5318117141723633
Batch 61/64 loss: -0.921595573425293
Batch 62/64 loss: -0.45592308044433594
Batch 63/64 loss: -1.0537853240966797
Batch 64/64 loss: -5.007806777954102
Epoch 406  Train loss: -0.7604454564113243  Val loss: -0.5106005717798606
Epoch 407
-------------------------------
Batch 1/64 loss: -0.5883569717407227
Batch 2/64 loss: -1.253469467163086
Batch 3/64 loss: -0.6983461380004883
Batch 4/64 loss: -0.8657560348510742
Batch 5/64 loss: -0.6963977813720703
Batch 6/64 loss: -0.8877506256103516
Batch 7/64 loss: -0.6164569854736328
Batch 8/64 loss: -0.5567846298217773
Batch 9/64 loss: -0.7164230346679688
Batch 10/64 loss: -0.9225702285766602
Batch 11/64 loss: -1.0835065841674805
Batch 12/64 loss: -0.9831123352050781
Batch 13/64 loss: -0.4175376892089844
Batch 14/64 loss: -0.3807353973388672
Batch 15/64 loss: -0.207366943359375
Batch 16/64 loss: -0.9158649444580078
Batch 17/64 loss: -0.917057991027832
Batch 18/64 loss: -0.5369148254394531
Batch 19/64 loss: -0.9860782623291016
Batch 20/64 loss: -0.3514080047607422
Batch 21/64 loss: -1.0293455123901367
Batch 22/64 loss: -0.9426164627075195
Batch 23/64 loss: -0.846552848815918
Batch 24/64 loss: -1.0245532989501953
Batch 25/64 loss: -0.8324337005615234
Batch 26/64 loss: -0.85015869140625
Batch 27/64 loss: -0.7342672348022461
Batch 28/64 loss: -0.723388671875
Batch 29/64 loss: -1.0990781784057617
Batch 30/64 loss: -1.0041837692260742
Batch 31/64 loss: -0.7421998977661133
Batch 32/64 loss: -0.8226118087768555
Batch 33/64 loss: -0.7593975067138672
Batch 34/64 loss: -0.9245758056640625
Batch 35/64 loss: 0.010956764221191406
Batch 36/64 loss: -1.1902751922607422
Batch 37/64 loss: -0.711029052734375
Batch 38/64 loss: -0.4432239532470703
Batch 39/64 loss: -0.8193912506103516
Batch 40/64 loss: -0.7908840179443359
Batch 41/64 loss: -0.43190479278564453
Batch 42/64 loss: -0.8370332717895508
Batch 43/64 loss: -0.5788297653198242
Batch 44/64 loss: -0.8393325805664062
Batch 45/64 loss: -0.6671152114868164
Batch 46/64 loss: -0.7781524658203125
Batch 47/64 loss: -1.0286235809326172
Batch 48/64 loss: -0.8330087661743164
Batch 49/64 loss: -0.913731575012207
Batch 50/64 loss: -0.7267494201660156
Batch 51/64 loss: -0.8983249664306641
Batch 52/64 loss: -0.8382396697998047
Batch 53/64 loss: -0.8292560577392578
Batch 54/64 loss: -0.8058376312255859
Batch 55/64 loss: -0.6779632568359375
Batch 56/64 loss: -0.2846231460571289
Batch 57/64 loss: -0.41922664642333984
Batch 58/64 loss: -0.36751461029052734
Batch 59/64 loss: -0.9379444122314453
Batch 60/64 loss: -0.8256902694702148
Batch 61/64 loss: -0.9994955062866211
Batch 62/64 loss: -0.8435831069946289
Batch 63/64 loss: -1.2088146209716797
Batch 64/64 loss: -4.486149787902832
Epoch 407  Train loss: -0.8124974531285903  Val loss: -0.7827665191335776
Epoch 408
-------------------------------
Batch 1/64 loss: -0.8646841049194336
Batch 2/64 loss: -0.6688766479492188
Batch 3/64 loss: -0.9802818298339844
Batch 4/64 loss: -0.6955661773681641
Batch 5/64 loss: -0.8689661026000977
Batch 6/64 loss: -0.9766788482666016
Batch 7/64 loss: -0.8394250869750977
Batch 8/64 loss: -0.7979307174682617
Batch 9/64 loss: -0.6976833343505859
Batch 10/64 loss: -0.7188262939453125
Batch 11/64 loss: -0.8537321090698242
Batch 12/64 loss: -0.736689567565918
Batch 13/64 loss: -0.3239431381225586
Batch 14/64 loss: -0.8547468185424805
Batch 15/64 loss: -0.7944889068603516
Batch 16/64 loss: -0.6709489822387695
Batch 17/64 loss: -0.7141284942626953
Batch 18/64 loss: -0.912724494934082
Batch 19/64 loss: -1.078841209411621
Batch 20/64 loss: -0.9343748092651367
Batch 21/64 loss: -0.8887386322021484
Batch 22/64 loss: -0.3496122360229492
Batch 23/64 loss: -0.7396783828735352
Batch 24/64 loss: -0.9765205383300781
Batch 25/64 loss: -0.9800891876220703
Batch 26/64 loss: -0.7634296417236328
Batch 27/64 loss: -0.5871057510375977
Batch 28/64 loss: -0.9467449188232422
Batch 29/64 loss: -1.2689886093139648
Batch 30/64 loss: -0.8484973907470703
Batch 31/64 loss: -0.6057662963867188
Batch 32/64 loss: -0.6854343414306641
Batch 33/64 loss: -1.1648283004760742
Batch 34/64 loss: -0.8053369522094727
Batch 35/64 loss: -0.7473678588867188
Batch 36/64 loss: -0.786041259765625
Batch 37/64 loss: -0.36473751068115234
Batch 38/64 loss: -0.7618770599365234
Batch 39/64 loss: -0.0051021575927734375
Batch 40/64 loss: -1.0299177169799805
Batch 41/64 loss: -0.6308860778808594
Batch 42/64 loss: -0.5960617065429688
Batch 43/64 loss: -0.9676351547241211
Batch 44/64 loss: -0.6971073150634766
Batch 45/64 loss: -0.3352394104003906
Batch 46/64 loss: -0.27292537689208984
Batch 47/64 loss: -0.7619152069091797
Batch 48/64 loss: -0.22430992126464844
Batch 49/64 loss: -0.589085578918457
Batch 50/64 loss: -0.7710866928100586
Batch 51/64 loss: -0.6917800903320312
Batch 52/64 loss: -0.6439714431762695
Batch 53/64 loss: -0.7976388931274414
Batch 54/64 loss: -0.7252998352050781
Batch 55/64 loss: -1.037607192993164
Batch 56/64 loss: -0.9363365173339844
Batch 57/64 loss: -0.8796205520629883
Batch 58/64 loss: -0.3719816207885742
Batch 59/64 loss: -1.080124855041504
Batch 60/64 loss: -0.7379369735717773
Batch 61/64 loss: -0.71429443359375
Batch 62/64 loss: -0.9652986526489258
Batch 63/64 loss: -0.7728462219238281
Batch 64/64 loss: -5.108581066131592
Epoch 408  Train loss: -0.8050474970948462  Val loss: -0.7439807878736778
Epoch 409
-------------------------------
Batch 1/64 loss: -0.384124755859375
Batch 2/64 loss: -0.5135478973388672
Batch 3/64 loss: -0.7837533950805664
Batch 4/64 loss: -0.6447353363037109
Batch 5/64 loss: -0.7646007537841797
Batch 6/64 loss: -0.6609039306640625
Batch 7/64 loss: -0.8463659286499023
Batch 8/64 loss: -0.9936342239379883
Batch 9/64 loss: -0.5999870300292969
Batch 10/64 loss: -0.8425178527832031
Batch 11/64 loss: -0.6864843368530273
Batch 12/64 loss: -0.6759195327758789
Batch 13/64 loss: -0.9336328506469727
Batch 14/64 loss: -0.9895420074462891
Batch 15/64 loss: -0.9963846206665039
Batch 16/64 loss: -1.188070297241211
Batch 17/64 loss: 0.03406238555908203
Batch 18/64 loss: -0.5382165908813477
Batch 19/64 loss: -1.2053213119506836
Batch 20/64 loss: -0.8911619186401367
Batch 21/64 loss: -0.8842525482177734
Batch 22/64 loss: -0.8645305633544922
Batch 23/64 loss: -1.009786605834961
Batch 24/64 loss: -1.1346368789672852
Batch 25/64 loss: -0.5121059417724609
Batch 26/64 loss: -1.195967674255371
Batch 27/64 loss: -0.23578357696533203
Batch 28/64 loss: -0.8200349807739258
Batch 29/64 loss: -0.8617591857910156
Batch 30/64 loss: -0.7005910873413086
Batch 31/64 loss: -0.9524145126342773
Batch 32/64 loss: -0.9797487258911133
Batch 33/64 loss: -0.9553098678588867
Batch 34/64 loss: -0.9943561553955078
Batch 35/64 loss: -0.847320556640625
Batch 36/64 loss: -0.0250091552734375
Batch 37/64 loss: -0.8228616714477539
Batch 38/64 loss: -0.4669952392578125
Batch 39/64 loss: -0.9531831741333008
Batch 40/64 loss: -0.5421714782714844
Batch 41/64 loss: -0.8641567230224609
Batch 42/64 loss: -0.34728145599365234
Batch 43/64 loss: -1.006357192993164
Batch 44/64 loss: -0.8368062973022461
Batch 45/64 loss: -1.069570541381836
Batch 46/64 loss: -0.9021511077880859
Batch 47/64 loss: -0.4154806137084961
Batch 48/64 loss: -1.190739631652832
Batch 49/64 loss: -1.1650409698486328
Batch 50/64 loss: -0.7675189971923828
Batch 51/64 loss: -0.9714021682739258
Batch 52/64 loss: -0.8155689239501953
Batch 53/64 loss: -0.8380832672119141
Batch 54/64 loss: -1.042337417602539
Batch 55/64 loss: -0.33723974227905273
Batch 56/64 loss: -0.734405517578125
Batch 57/64 loss: -0.5296306610107422
Batch 58/64 loss: -0.8027257919311523
Batch 59/64 loss: -0.9820041656494141
Batch 60/64 loss: -0.7077121734619141
Batch 61/64 loss: -0.6445894241333008
Batch 62/64 loss: -1.0839309692382812
Batch 63/64 loss: -0.8539094924926758
Batch 64/64 loss: -5.079498291015625
Epoch 409  Train loss: -0.8405008428237017  Val loss: -0.83232884882242
Saving best model, epoch: 409
Epoch 410
-------------------------------
Batch 1/64 loss: -0.9893913269042969
Batch 2/64 loss: -1.1612253189086914
Batch 3/64 loss: -0.8306140899658203
Batch 4/64 loss: -0.8272171020507812
Batch 5/64 loss: -0.8244361877441406
Batch 6/64 loss: -0.4651651382446289
Batch 7/64 loss: -0.9114551544189453
Batch 8/64 loss: -0.3183012008666992
Batch 9/64 loss: -0.7858085632324219
Batch 10/64 loss: -0.9697494506835938
Batch 11/64 loss: -0.9103851318359375
Batch 12/64 loss: -0.8014345169067383
Batch 13/64 loss: -0.9095592498779297
Batch 14/64 loss: -0.8241386413574219
Batch 15/64 loss: -0.7063741683959961
Batch 16/64 loss: -0.3067798614501953
Batch 17/64 loss: -0.7906322479248047
Batch 18/64 loss: -0.8021163940429688
Batch 19/64 loss: -0.46563053131103516
Batch 20/64 loss: -1.0345773696899414
Batch 21/64 loss: -1.1423397064208984
Batch 22/64 loss: -0.9097776412963867
Batch 23/64 loss: -0.9888753890991211
Batch 24/64 loss: -0.5754384994506836
Batch 25/64 loss: -0.7170534133911133
Batch 26/64 loss: -0.8221454620361328
Batch 27/64 loss: -1.1135492324829102
Batch 28/64 loss: -0.9179172515869141
Batch 29/64 loss: -0.814152717590332
Batch 30/64 loss: -0.912445068359375
Batch 31/64 loss: -0.6938438415527344
Batch 32/64 loss: -1.0524187088012695
Batch 33/64 loss: -0.9992609024047852
Batch 34/64 loss: -0.5166912078857422
Batch 35/64 loss: -0.9112720489501953
Batch 36/64 loss: -0.9490461349487305
Batch 37/64 loss: -0.9509468078613281
Batch 38/64 loss: -0.4769325256347656
Batch 39/64 loss: -0.8650913238525391
Batch 40/64 loss: -0.7094573974609375
Batch 41/64 loss: -0.45175743103027344
Batch 42/64 loss: -1.13043212890625
Batch 43/64 loss: -0.8183221817016602
Batch 44/64 loss: -1.1315460205078125
Batch 45/64 loss: -1.0062284469604492
Batch 46/64 loss: -0.7853097915649414
Batch 47/64 loss: -0.8954458236694336
Batch 48/64 loss: -0.5541925430297852
Batch 49/64 loss: -0.6791458129882812
Batch 50/64 loss: -0.851287841796875
Batch 51/64 loss: -0.9400825500488281
Batch 52/64 loss: -0.9702682495117188
Batch 53/64 loss: -1.089193344116211
Batch 54/64 loss: -1.1437129974365234
Batch 55/64 loss: -0.9803857803344727
Batch 56/64 loss: -0.9013938903808594
Batch 57/64 loss: -0.8318338394165039
Batch 58/64 loss: -1.0905752182006836
Batch 59/64 loss: -0.5466012954711914
Batch 60/64 loss: -0.42362213134765625
Batch 61/64 loss: -0.8110685348510742
Batch 62/64 loss: -0.5021228790283203
Batch 63/64 loss: -0.8071374893188477
Batch 64/64 loss: -4.480134963989258
Epoch 410  Train loss: -0.8686339509253408  Val loss: -0.6662951597233409
Epoch 411
-------------------------------
Batch 1/64 loss: -0.8810462951660156
Batch 2/64 loss: -0.8170948028564453
Batch 3/64 loss: -0.8107929229736328
Batch 4/64 loss: -1.0616636276245117
Batch 5/64 loss: -0.811711311340332
Batch 6/64 loss: -1.1505804061889648
Batch 7/64 loss: -0.98626708984375
Batch 8/64 loss: -0.47292041778564453
Batch 9/64 loss: -0.7202205657958984
Batch 10/64 loss: -0.956573486328125
Batch 11/64 loss: -0.5451250076293945
Batch 12/64 loss: -0.9080162048339844
Batch 13/64 loss: -0.8400125503540039
Batch 14/64 loss: -0.7835588455200195
Batch 15/64 loss: -0.5572500228881836
Batch 16/64 loss: -0.45394420623779297
Batch 17/64 loss: -0.7124471664428711
Batch 18/64 loss: -0.9609537124633789
Batch 19/64 loss: -0.3951530456542969
Batch 20/64 loss: -0.9065046310424805
Batch 21/64 loss: -0.5748805999755859
Batch 22/64 loss: -0.35486698150634766
Batch 23/64 loss: -0.6508693695068359
Batch 24/64 loss: -0.8462972640991211
Batch 25/64 loss: -0.6461639404296875
Batch 26/64 loss: -1.1195926666259766
Batch 27/64 loss: -0.8161659240722656
Batch 28/64 loss: -1.2469463348388672
Batch 29/64 loss: -0.4048166275024414
Batch 30/64 loss: -0.4304838180541992
Batch 31/64 loss: -0.9539966583251953
Batch 32/64 loss: -0.6899538040161133
Batch 33/64 loss: -0.7640476226806641
Batch 34/64 loss: -0.9293308258056641
Batch 35/64 loss: -0.8284273147583008
Batch 36/64 loss: -0.9151611328125
Batch 37/64 loss: -1.0119752883911133
Batch 38/64 loss: -0.7165651321411133
Batch 39/64 loss: -0.958827018737793
Batch 40/64 loss: -1.1236810684204102
Batch 41/64 loss: -0.8215703964233398
Batch 42/64 loss: -0.5178241729736328
Batch 43/64 loss: -0.8428878784179688
Batch 44/64 loss: -0.35037994384765625
Batch 45/64 loss: -0.9327001571655273
Batch 46/64 loss: -0.9792461395263672
Batch 47/64 loss: -1.0938549041748047
Batch 48/64 loss: -1.057448387145996
Batch 49/64 loss: -0.3209981918334961
Batch 50/64 loss: -0.8057937622070312
Batch 51/64 loss: -0.7555818557739258
Batch 52/64 loss: -0.7439022064208984
Batch 53/64 loss: -0.6882381439208984
Batch 54/64 loss: -0.3225288391113281
Batch 55/64 loss: -0.3822364807128906
Batch 56/64 loss: -0.8289623260498047
Batch 57/64 loss: -0.6567964553833008
Batch 58/64 loss: -0.7167930603027344
Batch 59/64 loss: -0.7522010803222656
Batch 60/64 loss: -1.1754083633422852
Batch 61/64 loss: -0.83734130859375
Batch 62/64 loss: -1.1018667221069336
Batch 63/64 loss: -0.6668815612792969
Batch 64/64 loss: -5.168622970581055
Epoch 411  Train loss: -0.8304752200257545  Val loss: -0.5678169014527625
Epoch 412
-------------------------------
Batch 1/64 loss: -0.7495441436767578
Batch 2/64 loss: -1.0484085083007812
Batch 3/64 loss: -0.8798360824584961
Batch 4/64 loss: -0.7213430404663086
Batch 5/64 loss: -0.6632661819458008
Batch 6/64 loss: -0.8870391845703125
Batch 7/64 loss: -0.7997894287109375
Batch 8/64 loss: -0.6855201721191406
Batch 9/64 loss: -0.6443595886230469
Batch 10/64 loss: -0.6459903717041016
Batch 11/64 loss: -0.9441404342651367
Batch 12/64 loss: -0.9430704116821289
Batch 13/64 loss: -0.7461776733398438
Batch 14/64 loss: -1.2107973098754883
Batch 15/64 loss: -0.8293647766113281
Batch 16/64 loss: -0.9586601257324219
Batch 17/64 loss: -0.6278982162475586
Batch 18/64 loss: -0.8683691024780273
Batch 19/64 loss: -0.9483871459960938
Batch 20/64 loss: -1.0434684753417969
Batch 21/64 loss: -0.5406723022460938
Batch 22/64 loss: -0.6877555847167969
Batch 23/64 loss: -0.3288402557373047
Batch 24/64 loss: -0.9384946823120117
Batch 25/64 loss: -0.8142938613891602
Batch 26/64 loss: -0.6468734741210938
Batch 27/64 loss: -0.7996959686279297
Batch 28/64 loss: -0.5423793792724609
Batch 29/64 loss: -0.9836111068725586
Batch 30/64 loss: -1.0792627334594727
Batch 31/64 loss: -0.8757181167602539
Batch 32/64 loss: -0.9931344985961914
Batch 33/64 loss: -0.5596179962158203
Batch 34/64 loss: -1.158757209777832
Batch 35/64 loss: -0.49848270416259766
Batch 36/64 loss: -0.8126468658447266
Batch 37/64 loss: -0.8809833526611328
Batch 38/64 loss: -1.1271095275878906
Batch 39/64 loss: -0.33878326416015625
Batch 40/64 loss: -0.9465599060058594
Batch 41/64 loss: -1.0532722473144531
Batch 42/64 loss: -1.1746721267700195
Batch 43/64 loss: -0.4292573928833008
Batch 44/64 loss: -0.9982748031616211
Batch 45/64 loss: -0.8726797103881836
Batch 46/64 loss: -0.6816415786743164
Batch 47/64 loss: -0.747553825378418
Batch 48/64 loss: -0.9463262557983398
Batch 49/64 loss: -0.9479970932006836
Batch 50/64 loss: -0.9083404541015625
Batch 51/64 loss: -0.6466684341430664
Batch 52/64 loss: -0.8156147003173828
Batch 53/64 loss: -0.7429370880126953
Batch 54/64 loss: -0.8850955963134766
Batch 55/64 loss: -0.9860086441040039
Batch 56/64 loss: -1.1038827896118164
Batch 57/64 loss: -0.6339664459228516
Batch 58/64 loss: -0.6454439163208008
Batch 59/64 loss: -0.7558784484863281
Batch 60/64 loss: -0.5999603271484375
Batch 61/64 loss: -0.9542913436889648
Batch 62/64 loss: -0.8096837997436523
Batch 63/64 loss: -0.7584257125854492
Batch 64/64 loss: -4.799849510192871
Epoch 412  Train loss: -0.8642645181394091  Val loss: -0.5911003191446521
Epoch 413
-------------------------------
Batch 1/64 loss: -0.614436149597168
Batch 2/64 loss: -1.0092716217041016
Batch 3/64 loss: -0.8218851089477539
Batch 4/64 loss: -0.06796073913574219
Batch 5/64 loss: -1.0776042938232422
Batch 6/64 loss: -0.9318513870239258
Batch 7/64 loss: -0.5726480484008789
Batch 8/64 loss: -0.8966922760009766
Batch 9/64 loss: -0.9993791580200195
Batch 10/64 loss: -0.17803192138671875
Batch 11/64 loss: -0.7129993438720703
Batch 12/64 loss: -0.8090047836303711
Batch 13/64 loss: -0.953272819519043
Batch 14/64 loss: -0.9279413223266602
Batch 15/64 loss: -0.8066110610961914
Batch 16/64 loss: -1.0441741943359375
Batch 17/64 loss: -1.200124740600586
Batch 18/64 loss: -0.8263149261474609
Batch 19/64 loss: -0.6615781784057617
Batch 20/64 loss: -0.8688459396362305
Batch 21/64 loss: -1.047307014465332
Batch 22/64 loss: -0.7703847885131836
Batch 23/64 loss: -0.9704380035400391
Batch 24/64 loss: -0.7291479110717773
Batch 25/64 loss: -0.9088973999023438
Batch 26/64 loss: -0.8553495407104492
Batch 27/64 loss: -0.7969417572021484
Batch 28/64 loss: 0.040584564208984375
Batch 29/64 loss: -0.8549995422363281
Batch 30/64 loss: -0.8746414184570312
Batch 31/64 loss: -1.061117172241211
Batch 32/64 loss: -0.5394439697265625
Batch 33/64 loss: -1.1403160095214844
Batch 34/64 loss: -0.9239063262939453
Batch 35/64 loss: -0.8709621429443359
Batch 36/64 loss: -0.9455289840698242
Batch 37/64 loss: -0.907832145690918
Batch 38/64 loss: -0.8302898406982422
Batch 39/64 loss: -0.999964714050293
Batch 40/64 loss: -0.8980493545532227
Batch 41/64 loss: -0.9651756286621094
Batch 42/64 loss: -1.1266250610351562
Batch 43/64 loss: -0.6393823623657227
Batch 44/64 loss: -0.7119112014770508
Batch 45/64 loss: -1.0218420028686523
Batch 46/64 loss: -0.6647834777832031
Batch 47/64 loss: -0.99652099609375
Batch 48/64 loss: -0.8801488876342773
Batch 49/64 loss: -0.9031276702880859
Batch 50/64 loss: -0.9525718688964844
Batch 51/64 loss: -0.8233156204223633
Batch 52/64 loss: -0.4163246154785156
Batch 53/64 loss: -0.8825817108154297
Batch 54/64 loss: -0.4577035903930664
Batch 55/64 loss: -0.7879877090454102
Batch 56/64 loss: -1.1454286575317383
Batch 57/64 loss: -0.5123214721679688
Batch 58/64 loss: -0.8032455444335938
Batch 59/64 loss: -0.9709711074829102
Batch 60/64 loss: -0.9171390533447266
Batch 61/64 loss: -0.8148736953735352
Batch 62/64 loss: -1.1090373992919922
Batch 63/64 loss: -0.9884748458862305
Batch 64/64 loss: -4.7794189453125
Epoch 413  Train loss: -0.8775156058517157  Val loss: -0.6530699582444024
Epoch 414
-------------------------------
Batch 1/64 loss: -0.9679231643676758
Batch 2/64 loss: -0.6626062393188477
Batch 3/64 loss: -1.073603630065918
Batch 4/64 loss: -0.7881183624267578
Batch 5/64 loss: -1.0509395599365234
Batch 6/64 loss: -1.0750713348388672
Batch 7/64 loss: -0.6911325454711914
Batch 8/64 loss: -0.4838743209838867
Batch 9/64 loss: -0.9691591262817383
Batch 10/64 loss: -1.163832664489746
Batch 11/64 loss: -0.8836345672607422
Batch 12/64 loss: -0.9982795715332031
Batch 13/64 loss: -1.2251615524291992
Batch 14/64 loss: -0.9823102951049805
Batch 15/64 loss: -0.29320812225341797
Batch 16/64 loss: -1.234665870666504
Batch 17/64 loss: -0.9103250503540039
Batch 18/64 loss: -0.7940349578857422
Batch 19/64 loss: -1.1359977722167969
Batch 20/64 loss: -1.2157859802246094
Batch 21/64 loss: -0.9480209350585938
Batch 22/64 loss: -1.1148176193237305
Batch 23/64 loss: -0.7832679748535156
Batch 24/64 loss: -0.5697393417358398
Batch 25/64 loss: -0.7831745147705078
Batch 26/64 loss: -1.0205316543579102
Batch 27/64 loss: -0.5977783203125
Batch 28/64 loss: -1.3097381591796875
Batch 29/64 loss: -0.7791252136230469
Batch 30/64 loss: -0.7065439224243164
Batch 31/64 loss: -0.7137823104858398
Batch 32/64 loss: -0.9417018890380859
Batch 33/64 loss: -0.8614358901977539
Batch 34/64 loss: -1.0115938186645508
Batch 35/64 loss: -1.0655708312988281
Batch 36/64 loss: -0.5436544418334961
Batch 37/64 loss: -1.0058135986328125
Batch 38/64 loss: -0.8714265823364258
Batch 39/64 loss: -1.0187149047851562
Batch 40/64 loss: -0.2035655975341797
Batch 41/64 loss: -0.9137039184570312
Batch 42/64 loss: -0.6682147979736328
Batch 43/64 loss: -0.823211669921875
Batch 44/64 loss: -0.6393375396728516
Batch 45/64 loss: -0.8884983062744141
Batch 46/64 loss: -1.0837993621826172
Batch 47/64 loss: -0.6373281478881836
Batch 48/64 loss: -0.510218620300293
Batch 49/64 loss: -1.0883722305297852
Batch 50/64 loss: -0.7105731964111328
Batch 51/64 loss: -0.21774005889892578
Batch 52/64 loss: -1.103494644165039
Batch 53/64 loss: -1.110957145690918
Batch 54/64 loss: -0.8958959579467773
Batch 55/64 loss: -1.0132818222045898
Batch 56/64 loss: -0.45216846466064453
Batch 57/64 loss: -0.6226596832275391
Batch 58/64 loss: -0.8242616653442383
Batch 59/64 loss: -0.8185205459594727
Batch 60/64 loss: -0.8205671310424805
Batch 61/64 loss: -0.7659912109375
Batch 62/64 loss: -0.5200433731079102
Batch 63/64 loss: -0.9288291931152344
Batch 64/64 loss: -4.179503440856934
Epoch 414  Train loss: -0.8885013094135359  Val loss: -0.45310692279199555
Epoch 415
-------------------------------
Batch 1/64 loss: -0.8546295166015625
Batch 2/64 loss: -0.6118316650390625
Batch 3/64 loss: -0.8423929214477539
Batch 4/64 loss: -0.6273488998413086
Batch 5/64 loss: -0.8326015472412109
Batch 6/64 loss: -0.7514677047729492
Batch 7/64 loss: -0.9895133972167969
Batch 8/64 loss: -0.9601583480834961
Batch 9/64 loss: -0.7495269775390625
Batch 10/64 loss: -0.8250370025634766
Batch 11/64 loss: -0.37021541595458984
Batch 12/64 loss: -0.771026611328125
Batch 13/64 loss: -0.9133205413818359
Batch 14/64 loss: -0.7617530822753906
Batch 15/64 loss: -0.7028408050537109
Batch 16/64 loss: -0.5655632019042969
Batch 17/64 loss: -0.837285041809082
Batch 18/64 loss: -0.9662837982177734
Batch 19/64 loss: -0.9975786209106445
Batch 20/64 loss: -0.7764387130737305
Batch 21/64 loss: -1.2104120254516602
Batch 22/64 loss: -0.8664464950561523
Batch 23/64 loss: -1.1258487701416016
Batch 24/64 loss: -0.7099008560180664
Batch 25/64 loss: -0.9942512512207031
Batch 26/64 loss: -1.1004571914672852
Batch 27/64 loss: -0.9636249542236328
Batch 28/64 loss: -0.8830375671386719
Batch 29/64 loss: -1.0469236373901367
Batch 30/64 loss: -0.7863378524780273
Batch 31/64 loss: -0.5139970779418945
Batch 32/64 loss: -0.6863269805908203
Batch 33/64 loss: -0.42337799072265625
Batch 34/64 loss: -0.812169075012207
Batch 35/64 loss: -0.9542427062988281
Batch 36/64 loss: -0.7823143005371094
Batch 37/64 loss: -1.0237092971801758
Batch 38/64 loss: -0.6683950424194336
Batch 39/64 loss: -0.9123497009277344
Batch 40/64 loss: -0.794896125793457
Batch 41/64 loss: -0.7518768310546875
Batch 42/64 loss: -0.8340425491333008
Batch 43/64 loss: -1.1202707290649414
Batch 44/64 loss: -1.0020179748535156
Batch 45/64 loss: -0.9039030075073242
Batch 46/64 loss: -0.6528825759887695
Batch 47/64 loss: -1.0806446075439453
Batch 48/64 loss: -0.9290637969970703
Batch 49/64 loss: -1.0619115829467773
Batch 50/64 loss: -1.1153059005737305
Batch 51/64 loss: -1.059854507446289
Batch 52/64 loss: -0.581599235534668
Batch 53/64 loss: -1.089756965637207
Batch 54/64 loss: -0.8749532699584961
Batch 55/64 loss: -0.46021080017089844
Batch 56/64 loss: -0.7004365921020508
Batch 57/64 loss: -0.8509798049926758
Batch 58/64 loss: -1.1394424438476562
Batch 59/64 loss: -1.1218585968017578
Batch 60/64 loss: -0.7449054718017578
Batch 61/64 loss: -0.9751319885253906
Batch 62/64 loss: -0.7949705123901367
Batch 63/64 loss: -0.6907377243041992
Batch 64/64 loss: -4.077535629272461
Epoch 415  Train loss: -0.8872273239434935  Val loss: -0.6309627257671553
Epoch 416
-------------------------------
Batch 1/64 loss: -1.3865184783935547
Batch 2/64 loss: -0.8026742935180664
Batch 3/64 loss: -0.8994388580322266
Batch 4/64 loss: -1.027388572692871
Batch 5/64 loss: -0.9261016845703125
Batch 6/64 loss: -0.5636129379272461
Batch 7/64 loss: -0.7293519973754883
Batch 8/64 loss: -0.895233154296875
Batch 9/64 loss: -0.9459009170532227
Batch 10/64 loss: -0.5359086990356445
Batch 11/64 loss: -0.8789558410644531
Batch 12/64 loss: -1.1880121231079102
Batch 13/64 loss: -1.1655597686767578
Batch 14/64 loss: -0.9996728897094727
Batch 15/64 loss: -0.6868371963500977
Batch 16/64 loss: -0.8389320373535156
Batch 17/64 loss: -0.9884662628173828
Batch 18/64 loss: -0.8614826202392578
Batch 19/64 loss: -1.0808448791503906
Batch 20/64 loss: -0.8613166809082031
Batch 21/64 loss: -1.047323226928711
Batch 22/64 loss: -0.844548225402832
Batch 23/64 loss: -0.6704158782958984
Batch 24/64 loss: -0.6444339752197266
Batch 25/64 loss: -0.7790040969848633
Batch 26/64 loss: -0.8177471160888672
Batch 27/64 loss: -0.48659324645996094
Batch 28/64 loss: -0.6851959228515625
Batch 29/64 loss: -0.8105373382568359
Batch 30/64 loss: -1.2377138137817383
Batch 31/64 loss: -0.6005325317382812
Batch 32/64 loss: -1.1927604675292969
Batch 33/64 loss: -0.9110307693481445
Batch 34/64 loss: -0.6211385726928711
Batch 35/64 loss: -0.721501350402832
Batch 36/64 loss: -0.7794170379638672
Batch 37/64 loss: -0.741917610168457
Batch 38/64 loss: -0.9946155548095703
Batch 39/64 loss: -0.8403234481811523
Batch 40/64 loss: -0.8326034545898438
Batch 41/64 loss: -1.128331184387207
Batch 42/64 loss: -1.184988021850586
Batch 43/64 loss: -0.6779384613037109
Batch 44/64 loss: -0.7938652038574219
Batch 45/64 loss: -1.0258779525756836
Batch 46/64 loss: -0.8294496536254883
Batch 47/64 loss: -0.9090461730957031
Batch 48/64 loss: -0.31758880615234375
Batch 49/64 loss: -0.6402912139892578
Batch 50/64 loss: -0.9874982833862305
Batch 51/64 loss: -0.7223958969116211
Batch 52/64 loss: -0.6788616180419922
Batch 53/64 loss: -0.929469108581543
Batch 54/64 loss: -0.7450895309448242
Batch 55/64 loss: -0.4903287887573242
Batch 56/64 loss: -0.7642755508422852
Batch 57/64 loss: -0.6847515106201172
Batch 58/64 loss: -1.1077709197998047
Batch 59/64 loss: -1.1741561889648438
Batch 60/64 loss: -0.7359209060668945
Batch 61/64 loss: -0.5110111236572266
Batch 62/64 loss: -0.46417999267578125
Batch 63/64 loss: -1.0496625900268555
Batch 64/64 loss: -5.153843402862549
Epoch 416  Train loss: -0.8931716825447831  Val loss: -0.6487044632639672
Epoch 417
-------------------------------
Batch 1/64 loss: -0.4980640411376953
Batch 2/64 loss: -1.2740631103515625
Batch 3/64 loss: -1.0002937316894531
Batch 4/64 loss: -1.0176334381103516
Batch 5/64 loss: -0.903599739074707
Batch 6/64 loss: -0.3923606872558594
Batch 7/64 loss: -1.1163883209228516
Batch 8/64 loss: -0.7870674133300781
Batch 9/64 loss: -0.8251533508300781
Batch 10/64 loss: -0.8006420135498047
Batch 11/64 loss: -1.0824623107910156
Batch 12/64 loss: -0.6467885971069336
Batch 13/64 loss: -0.7720260620117188
Batch 14/64 loss: -0.8229141235351562
Batch 15/64 loss: -0.8304986953735352
Batch 16/64 loss: -0.9183282852172852
Batch 17/64 loss: -0.8599185943603516
Batch 18/64 loss: -0.7262887954711914
Batch 19/64 loss: -0.8275279998779297
Batch 20/64 loss: -0.9506330490112305
Batch 21/64 loss: -0.7990474700927734
Batch 22/64 loss: -0.5986232757568359
Batch 23/64 loss: -1.0664443969726562
Batch 24/64 loss: -0.6529331207275391
Batch 25/64 loss: -0.9288311004638672
Batch 26/64 loss: -0.8051586151123047
Batch 27/64 loss: -1.0990800857543945
Batch 28/64 loss: -0.8794164657592773
Batch 29/64 loss: -0.5792503356933594
Batch 30/64 loss: -0.9463682174682617
Batch 31/64 loss: -0.9893741607666016
Batch 32/64 loss: -1.179844856262207
Batch 33/64 loss: -0.720703125
Batch 34/64 loss: -0.7984323501586914
Batch 35/64 loss: -0.81243896484375
Batch 36/64 loss: -0.9120826721191406
Batch 37/64 loss: -0.6402053833007812
Batch 38/64 loss: -0.9538698196411133
Batch 39/64 loss: -1.1597166061401367
Batch 40/64 loss: -0.8000526428222656
Batch 41/64 loss: -0.45368194580078125
Batch 42/64 loss: -1.048105239868164
Batch 43/64 loss: -0.48668956756591797
Batch 44/64 loss: -1.0095405578613281
Batch 45/64 loss: -0.7874460220336914
Batch 46/64 loss: -1.1475715637207031
Batch 47/64 loss: -0.8305244445800781
Batch 48/64 loss: -0.9419927597045898
Batch 49/64 loss: -0.5016822814941406
Batch 50/64 loss: -0.4454965591430664
Batch 51/64 loss: -0.4248647689819336
Batch 52/64 loss: -0.2535848617553711
Batch 53/64 loss: -0.5588464736938477
Batch 54/64 loss: -0.590388298034668
Batch 55/64 loss: -0.4779090881347656
Batch 56/64 loss: -0.21809911727905273
Batch 57/64 loss: -0.5354194641113281
Batch 58/64 loss: -0.4947538375854492
Batch 59/64 loss: -0.391693115234375
Batch 60/64 loss: -0.5568819046020508
Batch 61/64 loss: -0.6384201049804688
Batch 62/64 loss: -0.7572689056396484
Batch 63/64 loss: -0.6675014495849609
Batch 64/64 loss: -4.935673713684082
Epoch 417  Train loss: -0.8203081355375402  Val loss: -0.008947837803371992
Epoch 418
-------------------------------
Batch 1/64 loss: -0.6800413131713867
Batch 2/64 loss: -0.8748989105224609
Batch 3/64 loss: -0.7621746063232422
Batch 4/64 loss: -0.5204067230224609
Batch 5/64 loss: -0.8335866928100586
Batch 6/64 loss: -0.7897100448608398
Batch 7/64 loss: -0.773716926574707
Batch 8/64 loss: -0.6769351959228516
Batch 9/64 loss: -0.5467538833618164
Batch 10/64 loss: -0.7917213439941406
Batch 11/64 loss: -0.5902013778686523
Batch 12/64 loss: -0.9981546401977539
Batch 13/64 loss: -0.8054466247558594
Batch 14/64 loss: -0.6641416549682617
Batch 15/64 loss: -0.8857526779174805
Batch 16/64 loss: -0.7915868759155273
Batch 17/64 loss: -0.44130516052246094
Batch 18/64 loss: -1.0639581680297852
Batch 19/64 loss: -0.3659791946411133
Batch 20/64 loss: -0.9449434280395508
Batch 21/64 loss: -0.9768953323364258
Batch 22/64 loss: -0.4180126190185547
Batch 23/64 loss: -0.8356313705444336
Batch 24/64 loss: -0.8817148208618164
Batch 25/64 loss: -0.746668815612793
Batch 26/64 loss: -1.0221424102783203
Batch 27/64 loss: -0.6721353530883789
Batch 28/64 loss: -1.1217594146728516
Batch 29/64 loss: -0.8333568572998047
Batch 30/64 loss: -0.19922876358032227
Batch 31/64 loss: -0.8801860809326172
Batch 32/64 loss: -0.9023847579956055
Batch 33/64 loss: -1.0488967895507812
Batch 34/64 loss: -0.9082221984863281
Batch 35/64 loss: -0.8707094192504883
Batch 36/64 loss: -0.9339399337768555
Batch 37/64 loss: -0.7169885635375977
Batch 38/64 loss: -0.7717876434326172
Batch 39/64 loss: -1.0420293807983398
Batch 40/64 loss: -0.47097015380859375
Batch 41/64 loss: -0.35919857025146484
Batch 42/64 loss: -0.7244110107421875
Batch 43/64 loss: -0.7747364044189453
Batch 44/64 loss: -0.8608407974243164
Batch 45/64 loss: -0.5770740509033203
Batch 46/64 loss: -0.3470029830932617
Batch 47/64 loss: -0.15964603424072266
Batch 48/64 loss: -0.5836505889892578
Batch 49/64 loss: -0.8410673141479492
Batch 50/64 loss: -0.2938518524169922
Batch 51/64 loss: -0.7325859069824219
Batch 52/64 loss: -0.8502721786499023
Batch 53/64 loss: -0.9433479309082031
Batch 54/64 loss: -0.9209089279174805
Batch 55/64 loss: -0.866459846496582
Batch 56/64 loss: -1.180978775024414
Batch 57/64 loss: -0.9615459442138672
Batch 58/64 loss: -0.7623519897460938
Batch 59/64 loss: -0.5007705688476562
Batch 60/64 loss: -0.6243314743041992
Batch 61/64 loss: -0.2514162063598633
Batch 62/64 loss: -0.8343486785888672
Batch 63/64 loss: -0.6995248794555664
Batch 64/64 loss: -4.295091152191162
Epoch 418  Train loss: -0.7831641945184445  Val loss: -0.5626686594330568
Epoch 419
-------------------------------
Batch 1/64 loss: -0.8115043640136719
Batch 2/64 loss: -0.8030614852905273
Batch 3/64 loss: -0.5398283004760742
Batch 4/64 loss: -0.6751718521118164
Batch 5/64 loss: -0.6924552917480469
Batch 6/64 loss: -0.6691579818725586
Batch 7/64 loss: -0.7519245147705078
Batch 8/64 loss: -0.6961307525634766
Batch 9/64 loss: -0.602381706237793
Batch 10/64 loss: -0.8304357528686523
Batch 11/64 loss: -0.8499412536621094
Batch 12/64 loss: -1.0559167861938477
Batch 13/64 loss: -0.731806755065918
Batch 14/64 loss: -0.7922077178955078
Batch 15/64 loss: -1.1991605758666992
Batch 16/64 loss: -0.5714406967163086
Batch 17/64 loss: -0.8060750961303711
Batch 18/64 loss: -0.5361013412475586
Batch 19/64 loss: -0.9557123184204102
Batch 20/64 loss: 0.2242279052734375
Batch 21/64 loss: -0.9368095397949219
Batch 22/64 loss: -0.7290449142456055
Batch 23/64 loss: -0.877812385559082
Batch 24/64 loss: -0.9141817092895508
Batch 25/64 loss: -0.27587890625
Batch 26/64 loss: -0.6786527633666992
Batch 27/64 loss: -0.6122827529907227
Batch 28/64 loss: -0.7692270278930664
Batch 29/64 loss: -0.6746997833251953
Batch 30/64 loss: -0.9049139022827148
Batch 31/64 loss: -0.6265802383422852
Batch 32/64 loss: -0.7438459396362305
Batch 33/64 loss: -0.8828525543212891
Batch 34/64 loss: -0.6529293060302734
Batch 35/64 loss: -0.5263252258300781
Batch 36/64 loss: -0.4849672317504883
Batch 37/64 loss: -0.4926328659057617
Batch 38/64 loss: -0.8815460205078125
Batch 39/64 loss: -0.5161275863647461
Batch 40/64 loss: -0.4938163757324219
Batch 41/64 loss: -0.4810361862182617
Batch 42/64 loss: -0.6960220336914062
Batch 43/64 loss: -1.1409673690795898
Batch 44/64 loss: -0.8587312698364258
Batch 45/64 loss: -0.6695442199707031
Batch 46/64 loss: -0.6099948883056641
Batch 47/64 loss: -0.7830076217651367
Batch 48/64 loss: -0.1256418228149414
Batch 49/64 loss: -0.4119882583618164
Batch 50/64 loss: -0.5340213775634766
Batch 51/64 loss: -0.6493406295776367
Batch 52/64 loss: -0.85467529296875
Batch 53/64 loss: -0.7176704406738281
Batch 54/64 loss: -0.6945152282714844
Batch 55/64 loss: -0.9093637466430664
Batch 56/64 loss: -0.8294296264648438
Batch 57/64 loss: -0.7495174407958984
Batch 58/64 loss: -0.5960445404052734
Batch 59/64 loss: -0.6770133972167969
Batch 60/64 loss: -0.16081809997558594
Batch 61/64 loss: -0.9712162017822266
Batch 62/64 loss: -0.7472248077392578
Batch 63/64 loss: -0.5319786071777344
Batch 64/64 loss: -4.308185577392578
Epoch 419  Train loss: -0.7317994660022212  Val loss: -0.6751351372892505
Epoch 420
-------------------------------
Batch 1/64 loss: -0.6278486251831055
Batch 2/64 loss: -0.5336418151855469
Batch 3/64 loss: -0.6644992828369141
Batch 4/64 loss: -0.8939609527587891
Batch 5/64 loss: -0.5508890151977539
Batch 6/64 loss: -0.41774463653564453
Batch 7/64 loss: -0.8747797012329102
Batch 8/64 loss: -0.5581531524658203
Batch 9/64 loss: -0.08358478546142578
Batch 10/64 loss: -0.1990184783935547
Batch 11/64 loss: -0.8619060516357422
Batch 12/64 loss: -0.7089033126831055
Batch 13/64 loss: -0.9091768264770508
Batch 14/64 loss: -0.5001497268676758
Batch 15/64 loss: -0.6425952911376953
Batch 16/64 loss: -0.7498874664306641
Batch 17/64 loss: -0.9059934616088867
Batch 18/64 loss: -0.9137468338012695
Batch 19/64 loss: -0.37552928924560547
Batch 20/64 loss: -0.7236175537109375
Batch 21/64 loss: -0.5493955612182617
Batch 22/64 loss: -0.8950891494750977
Batch 23/64 loss: -0.8209772109985352
Batch 24/64 loss: -0.5442829132080078
Batch 25/64 loss: -0.7898359298706055
Batch 26/64 loss: -0.5660429000854492
Batch 27/64 loss: -0.6981058120727539
Batch 28/64 loss: -1.0489768981933594
Batch 29/64 loss: -0.7126550674438477
Batch 30/64 loss: -0.40280961990356445
Batch 31/64 loss: -0.5090265274047852
Batch 32/64 loss: -0.3850669860839844
Batch 33/64 loss: -0.783991813659668
Batch 34/64 loss: -0.6632919311523438
Batch 35/64 loss: -1.010040283203125
Batch 36/64 loss: -0.842066764831543
Batch 37/64 loss: -0.7257270812988281
Batch 38/64 loss: -0.5405006408691406
Batch 39/64 loss: -0.8140544891357422
Batch 40/64 loss: -0.8249263763427734
Batch 41/64 loss: -0.7947416305541992
Batch 42/64 loss: -1.0907020568847656
Batch 43/64 loss: -0.5990924835205078
Batch 44/64 loss: -0.7535152435302734
Batch 45/64 loss: -0.8363656997680664
Batch 46/64 loss: -0.9151477813720703
Batch 47/64 loss: -0.6575336456298828
Batch 48/64 loss: -0.3916158676147461
Batch 49/64 loss: -0.5821170806884766
Batch 50/64 loss: -0.6356630325317383
Batch 51/64 loss: -0.6669769287109375
Batch 52/64 loss: -0.7029647827148438
Batch 53/64 loss: -0.9863576889038086
Batch 54/64 loss: -0.5972681045532227
Batch 55/64 loss: -0.8502483367919922
Batch 56/64 loss: -0.9297904968261719
Batch 57/64 loss: -0.807830810546875
Batch 58/64 loss: -0.6921606063842773
Batch 59/64 loss: -0.40610790252685547
Batch 60/64 loss: -0.5882024765014648
Batch 61/64 loss: -0.7871360778808594
Batch 62/64 loss: -0.8126640319824219
Batch 63/64 loss: -0.7522792816162109
Batch 64/64 loss: -4.889519214630127
Epoch 420  Train loss: -0.7423703792048436  Val loss: -0.7652395189422923
Epoch 421
-------------------------------
Batch 1/64 loss: -0.8256006240844727
Batch 2/64 loss: -1.0830669403076172
Batch 3/64 loss: -0.9234209060668945
Batch 4/64 loss: -0.6702165603637695
Batch 5/64 loss: -0.9063262939453125
Batch 6/64 loss: -0.3416461944580078
Batch 7/64 loss: -0.6463088989257812
Batch 8/64 loss: -0.5211124420166016
Batch 9/64 loss: -0.6997690200805664
Batch 10/64 loss: -0.8466320037841797
Batch 11/64 loss: -0.6860923767089844
Batch 12/64 loss: -0.5437259674072266
Batch 13/64 loss: -0.5285234451293945
Batch 14/64 loss: -0.9243879318237305
Batch 15/64 loss: -0.8464326858520508
Batch 16/64 loss: -0.8621397018432617
Batch 17/64 loss: -0.6769351959228516
Batch 18/64 loss: -1.0847759246826172
Batch 19/64 loss: -0.9847707748413086
Batch 20/64 loss: -0.6774196624755859
Batch 21/64 loss: -0.884495735168457
Batch 22/64 loss: -0.9772977828979492
Batch 23/64 loss: -0.8188810348510742
Batch 24/64 loss: -1.2752838134765625
Batch 25/64 loss: -0.9122400283813477
Batch 26/64 loss: -0.8546638488769531
Batch 27/64 loss: -0.0706024169921875
Batch 28/64 loss: -0.6049528121948242
Batch 29/64 loss: -0.8291845321655273
Batch 30/64 loss: -0.6808528900146484
Batch 31/64 loss: -0.4435138702392578
Batch 32/64 loss: -1.0740737915039062
Batch 33/64 loss: -0.6444845199584961
Batch 34/64 loss: -0.8274574279785156
Batch 35/64 loss: -1.0508394241333008
Batch 36/64 loss: -0.6172695159912109
Batch 37/64 loss: -0.8498039245605469
Batch 38/64 loss: -1.032027244567871
Batch 39/64 loss: -0.9277553558349609
Batch 40/64 loss: -0.8809070587158203
Batch 41/64 loss: -0.2284679412841797
Batch 42/64 loss: -0.8259801864624023
Batch 43/64 loss: -0.3043050765991211
Batch 44/64 loss: -0.6955280303955078
Batch 45/64 loss: -0.789759635925293
Batch 46/64 loss: -0.4140663146972656
Batch 47/64 loss: -0.21133041381835938
Batch 48/64 loss: -0.06378746032714844
Batch 49/64 loss: -0.7475128173828125
Batch 50/64 loss: -0.8837909698486328
Batch 51/64 loss: -0.16559982299804688
Batch 52/64 loss: -0.7032432556152344
Batch 53/64 loss: -0.11742687225341797
Batch 54/64 loss: -0.7562398910522461
Batch 55/64 loss: -0.5583972930908203
Batch 56/64 loss: -0.23581314086914062
Batch 57/64 loss: -0.980987548828125
Batch 58/64 loss: -0.5718679428100586
Batch 59/64 loss: -0.984196662902832
Batch 60/64 loss: -1.0073461532592773
Batch 61/64 loss: -0.7575550079345703
Batch 62/64 loss: -0.6100902557373047
Batch 63/64 loss: -0.9084987640380859
Batch 64/64 loss: -4.819738864898682
Epoch 421  Train loss: -0.7634899793886671  Val loss: -0.7205315816033747
Epoch 422
-------------------------------
Batch 1/64 loss: -0.9540424346923828
Batch 2/64 loss: -0.806884765625
Batch 3/64 loss: -0.8751392364501953
Batch 4/64 loss: -0.3800830841064453
Batch 5/64 loss: -0.43553924560546875
Batch 6/64 loss: -1.0703763961791992
Batch 7/64 loss: 0.0026731491088867188
Batch 8/64 loss: -0.4659748077392578
Batch 9/64 loss: -1.0930814743041992
Batch 10/64 loss: -0.7238903045654297
Batch 11/64 loss: -0.8112554550170898
Batch 12/64 loss: -1.0181779861450195
Batch 13/64 loss: -0.8711795806884766
Batch 14/64 loss: -0.8955268859863281
Batch 15/64 loss: -0.9271030426025391
Batch 16/64 loss: -0.6794137954711914
Batch 17/64 loss: -0.9779119491577148
Batch 18/64 loss: -0.6053524017333984
Batch 19/64 loss: -1.0178747177124023
Batch 20/64 loss: -0.761688232421875
Batch 21/64 loss: -1.0266733169555664
Batch 22/64 loss: -1.0624313354492188
Batch 23/64 loss: -0.8649253845214844
Batch 24/64 loss: -0.6390581130981445
Batch 25/64 loss: -0.9225378036499023
Batch 26/64 loss: -0.5200119018554688
Batch 27/64 loss: -0.9289178848266602
Batch 28/64 loss: -0.6632966995239258
Batch 29/64 loss: -0.9136648178100586
Batch 30/64 loss: -0.7839565277099609
Batch 31/64 loss: -1.0704565048217773
Batch 32/64 loss: -0.8211870193481445
Batch 33/64 loss: -1.155135154724121
Batch 34/64 loss: -0.868560791015625
Batch 35/64 loss: -0.5804996490478516
Batch 36/64 loss: -0.8558168411254883
Batch 37/64 loss: -0.7431545257568359
Batch 38/64 loss: -0.6801061630249023
Batch 39/64 loss: -0.540130615234375
Batch 40/64 loss: -0.5133152008056641
Batch 41/64 loss: -0.5448818206787109
Batch 42/64 loss: -0.7677116394042969
Batch 43/64 loss: -0.7644548416137695
Batch 44/64 loss: -1.0945453643798828
Batch 45/64 loss: -0.7472457885742188
Batch 46/64 loss: -0.9990615844726562
Batch 47/64 loss: -0.9968948364257812
Batch 48/64 loss: -0.13823366165161133
Batch 49/64 loss: -0.7117795944213867
Batch 50/64 loss: -0.6733932495117188
Batch 51/64 loss: -0.7342081069946289
Batch 52/64 loss: -0.6258506774902344
Batch 53/64 loss: -0.9496192932128906
Batch 54/64 loss: -0.9194850921630859
Batch 55/64 loss: -0.9311351776123047
Batch 56/64 loss: -0.6350650787353516
Batch 57/64 loss: -0.5404577255249023
Batch 58/64 loss: -1.097071647644043
Batch 59/64 loss: -0.6160907745361328
Batch 60/64 loss: -0.7407474517822266
Batch 61/64 loss: -1.0345678329467773
Batch 62/64 loss: -0.952357292175293
Batch 63/64 loss: -1.1929492950439453
Batch 64/64 loss: -5.223311424255371
Epoch 422  Train loss: -0.8446580438052906  Val loss: -0.6910592567470065
Epoch 423
-------------------------------
Batch 1/64 loss: -0.6881990432739258
Batch 2/64 loss: -0.7182931900024414
Batch 3/64 loss: -0.6015481948852539
Batch 4/64 loss: -0.6930656433105469
Batch 5/64 loss: -0.7263612747192383
Batch 6/64 loss: -0.5883312225341797
Batch 7/64 loss: -0.9119548797607422
Batch 8/64 loss: -0.8866338729858398
Batch 9/64 loss: -0.926701545715332
Batch 10/64 loss: -0.8968076705932617
Batch 11/64 loss: -1.0106229782104492
Batch 12/64 loss: -0.6610841751098633
Batch 13/64 loss: -0.9203433990478516
Batch 14/64 loss: -1.0490131378173828
Batch 15/64 loss: -1.1040115356445312
Batch 16/64 loss: -0.8188161849975586
Batch 17/64 loss: -0.7421627044677734
Batch 18/64 loss: -0.9100427627563477
Batch 19/64 loss: -0.6687221527099609
Batch 20/64 loss: -0.8418788909912109
Batch 21/64 loss: -0.5324497222900391
Batch 22/64 loss: -1.045058250427246
Batch 23/64 loss: -0.5262069702148438
Batch 24/64 loss: -0.8970623016357422
Batch 25/64 loss: -0.8291435241699219
Batch 26/64 loss: -0.48422718048095703
Batch 27/64 loss: -1.1603479385375977
Batch 28/64 loss: -0.9609594345092773
Batch 29/64 loss: -1.0156526565551758
Batch 30/64 loss: -0.7250528335571289
Batch 31/64 loss: -0.9926128387451172
Batch 32/64 loss: -0.6426286697387695
Batch 33/64 loss: -0.6712770462036133
Batch 34/64 loss: -0.8577003479003906
Batch 35/64 loss: -0.6910028457641602
Batch 36/64 loss: -0.7482290267944336
Batch 37/64 loss: -0.9438810348510742
Batch 38/64 loss: -0.9315071105957031
Batch 39/64 loss: -1.1110515594482422
Batch 40/64 loss: -0.672882080078125
Batch 41/64 loss: -0.7099285125732422
Batch 42/64 loss: -0.7507114410400391
Batch 43/64 loss: -1.0042238235473633
Batch 44/64 loss: -0.7647457122802734
Batch 45/64 loss: -0.8579950332641602
Batch 46/64 loss: -0.5573539733886719
Batch 47/64 loss: -0.6063337326049805
Batch 48/64 loss: -0.7950172424316406
Batch 49/64 loss: -0.45359039306640625
Batch 50/64 loss: -0.9514579772949219
Batch 51/64 loss: -0.6288442611694336
Batch 52/64 loss: -0.5651845932006836
Batch 53/64 loss: -0.7894315719604492
Batch 54/64 loss: -1.0228261947631836
Batch 55/64 loss: -0.8107080459594727
Batch 56/64 loss: -1.0343389511108398
Batch 57/64 loss: -1.1614513397216797
Batch 58/64 loss: -0.8384456634521484
Batch 59/64 loss: -0.6724061965942383
Batch 60/64 loss: -1.083608627319336
Batch 61/64 loss: -0.8692474365234375
Batch 62/64 loss: -1.167098045349121
Batch 63/64 loss: -0.9731664657592773
Batch 64/64 loss: -4.639486312866211
Epoch 423  Train loss: -0.8682550243302888  Val loss: -0.7652419572024002
Epoch 424
-------------------------------
Batch 1/64 loss: -0.9177284240722656
Batch 2/64 loss: -0.737060546875
Batch 3/64 loss: -0.791935920715332
Batch 4/64 loss: -0.8639326095581055
Batch 5/64 loss: -0.8048486709594727
Batch 6/64 loss: -0.7709779739379883
Batch 7/64 loss: -0.8807163238525391
Batch 8/64 loss: -1.1405696868896484
Batch 9/64 loss: -0.5465984344482422
Batch 10/64 loss: -0.9341917037963867
Batch 11/64 loss: -0.8257284164428711
Batch 12/64 loss: -0.7828865051269531
Batch 13/64 loss: -0.9805202484130859
Batch 14/64 loss: -0.7455253601074219
Batch 15/64 loss: -0.7567739486694336
Batch 16/64 loss: -0.8212862014770508
Batch 17/64 loss: -0.9605722427368164
Batch 18/64 loss: -0.8240690231323242
Batch 19/64 loss: -0.9822292327880859
Batch 20/64 loss: -0.8039369583129883
Batch 21/64 loss: -0.8425970077514648
Batch 22/64 loss: -0.5392856597900391
Batch 23/64 loss: -0.6476678848266602
Batch 24/64 loss: -0.99456787109375
Batch 25/64 loss: -0.5499114990234375
Batch 26/64 loss: -1.0203533172607422
Batch 27/64 loss: -0.9359750747680664
Batch 28/64 loss: -0.5912380218505859
Batch 29/64 loss: -0.7734098434448242
Batch 30/64 loss: -0.8301181793212891
Batch 31/64 loss: -0.8810052871704102
Batch 32/64 loss: -1.0452423095703125
Batch 33/64 loss: -1.001286506652832
Batch 34/64 loss: -0.9058599472045898
Batch 35/64 loss: -0.8621578216552734
Batch 36/64 loss: -1.0881729125976562
Batch 37/64 loss: -0.849639892578125
Batch 38/64 loss: -0.8554363250732422
Batch 39/64 loss: -0.8673515319824219
Batch 40/64 loss: -1.0435256958007812
Batch 41/64 loss: -0.6428165435791016
Batch 42/64 loss: -0.6946229934692383
Batch 43/64 loss: -0.7630901336669922
Batch 44/64 loss: -1.1669378280639648
Batch 45/64 loss: -1.0806283950805664
Batch 46/64 loss: -0.9316520690917969
Batch 47/64 loss: -0.7870016098022461
Batch 48/64 loss: -0.8741121292114258
Batch 49/64 loss: -1.0739612579345703
Batch 50/64 loss: -0.985076904296875
Batch 51/64 loss: -1.1013259887695312
Batch 52/64 loss: -0.6232290267944336
Batch 53/64 loss: -1.1403217315673828
Batch 54/64 loss: -0.9875173568725586
Batch 55/64 loss: -0.6810932159423828
Batch 56/64 loss: -0.8133821487426758
Batch 57/64 loss: -0.9698591232299805
Batch 58/64 loss: -0.5861291885375977
Batch 59/64 loss: -0.4918041229248047
Batch 60/64 loss: -1.0008153915405273
Batch 61/64 loss: -0.9001426696777344
Batch 62/64 loss: -0.9458351135253906
Batch 63/64 loss: -0.4772987365722656
Batch 64/64 loss: -5.121401786804199
Epoch 424  Train loss: -0.902848565344717  Val loss: -0.833400162634571
Saving best model, epoch: 424
Epoch 425
-------------------------------
Batch 1/64 loss: -1.072906494140625
Batch 2/64 loss: -0.9775819778442383
Batch 3/64 loss: -0.8096694946289062
Batch 4/64 loss: -0.942835807800293
Batch 5/64 loss: -0.9860877990722656
Batch 6/64 loss: -0.8702735900878906
Batch 7/64 loss: -1.0222969055175781
Batch 8/64 loss: -0.9407024383544922
Batch 9/64 loss: -0.5433168411254883
Batch 10/64 loss: -0.06884956359863281
Batch 11/64 loss: -0.8377790451049805
Batch 12/64 loss: -0.9791431427001953
Batch 13/64 loss: -0.5460634231567383
Batch 14/64 loss: -1.0167036056518555
Batch 15/64 loss: -0.9513397216796875
Batch 16/64 loss: -0.9939107894897461
Batch 17/64 loss: -0.8903293609619141
Batch 18/64 loss: -0.9415092468261719
Batch 19/64 loss: -0.8442726135253906
Batch 20/64 loss: -1.0688447952270508
Batch 21/64 loss: -1.1069107055664062
Batch 22/64 loss: -1.1514787673950195
Batch 23/64 loss: -0.5096960067749023
Batch 24/64 loss: -1.2556571960449219
Batch 25/64 loss: -0.8652229309082031
Batch 26/64 loss: -0.7120275497436523
Batch 27/64 loss: -1.0829486846923828
Batch 28/64 loss: -0.8589792251586914
Batch 29/64 loss: -0.9186086654663086
Batch 30/64 loss: -0.46804046630859375
Batch 31/64 loss: -0.7744712829589844
Batch 32/64 loss: -0.9880580902099609
Batch 33/64 loss: -0.8795280456542969
Batch 34/64 loss: -0.9480295181274414
Batch 35/64 loss: -1.0682592391967773
Batch 36/64 loss: -0.6294527053833008
Batch 37/64 loss: -1.0709953308105469
Batch 38/64 loss: -1.0201282501220703
Batch 39/64 loss: -1.1722431182861328
Batch 40/64 loss: -0.7285280227661133
Batch 41/64 loss: -0.6973199844360352
Batch 42/64 loss: -0.9276142120361328
Batch 43/64 loss: -0.5058765411376953
Batch 44/64 loss: -0.709599494934082
Batch 45/64 loss: -1.001786231994629
Batch 46/64 loss: -0.3256568908691406
Batch 47/64 loss: -1.0273494720458984
Batch 48/64 loss: -1.3431501388549805
Batch 49/64 loss: -0.731114387512207
Batch 50/64 loss: -0.8350200653076172
Batch 51/64 loss: -0.9282207489013672
Batch 52/64 loss: -0.8766918182373047
Batch 53/64 loss: -1.0811138153076172
Batch 54/64 loss: -1.1219816207885742
Batch 55/64 loss: -0.962275505065918
Batch 56/64 loss: -0.8768281936645508
Batch 57/64 loss: -0.9119348526000977
Batch 58/64 loss: -1.112257957458496
Batch 59/64 loss: -0.44379138946533203
Batch 60/64 loss: -0.9533014297485352
Batch 61/64 loss: -1.0265827178955078
Batch 62/64 loss: -1.0370378494262695
Batch 63/64 loss: -0.7320556640625
Batch 64/64 loss: -5.609089374542236
Epoch 425  Train loss: -0.9394362113055061  Val loss: -0.862363153306889
Saving best model, epoch: 425
Epoch 426
-------------------------------
Batch 1/64 loss: -0.6810216903686523
Batch 2/64 loss: -0.9082851409912109
Batch 3/64 loss: -1.0322513580322266
Batch 4/64 loss: -0.7173480987548828
Batch 5/64 loss: -1.1088447570800781
Batch 6/64 loss: -1.2393712997436523
Batch 7/64 loss: -0.38489437103271484
Batch 8/64 loss: -0.8530168533325195
Batch 9/64 loss: -1.164402961730957
Batch 10/64 loss: -0.7259368896484375
Batch 11/64 loss: -0.5612335205078125
Batch 12/64 loss: -0.812809944152832
Batch 13/64 loss: -0.6544075012207031
Batch 14/64 loss: -1.0921659469604492
Batch 15/64 loss: -0.9359855651855469
Batch 16/64 loss: -0.7811822891235352
Batch 17/64 loss: -1.2740917205810547
Batch 18/64 loss: -0.94915771484375
Batch 19/64 loss: -0.9745931625366211
Batch 20/64 loss: -0.7889204025268555
Batch 21/64 loss: -0.687103271484375
Batch 22/64 loss: -0.7106199264526367
Batch 23/64 loss: -0.9136648178100586
Batch 24/64 loss: -1.1865673065185547
Batch 25/64 loss: -0.6855812072753906
Batch 26/64 loss: -0.12181758880615234
Batch 27/64 loss: -1.026118278503418
Batch 28/64 loss: -1.1444101333618164
Batch 29/64 loss: -0.14569473266601562
Batch 30/64 loss: -0.07629680633544922
Batch 31/64 loss: -1.0376663208007812
Batch 32/64 loss: -1.0163965225219727
Batch 33/64 loss: -0.8836965560913086
Batch 34/64 loss: -1.0941162109375
Batch 35/64 loss: -0.8339405059814453
Batch 36/64 loss: -0.7670078277587891
Batch 37/64 loss: -1.0664100646972656
Batch 38/64 loss: -0.9625425338745117
Batch 39/64 loss: -0.8266992568969727
Batch 40/64 loss: -0.8357572555541992
Batch 41/64 loss: -0.6894588470458984
Batch 42/64 loss: -0.47528553009033203
Batch 43/64 loss: -0.5090789794921875
Batch 44/64 loss: -1.0521841049194336
Batch 45/64 loss: -0.9927949905395508
Batch 46/64 loss: -0.8938560485839844
Batch 47/64 loss: -0.6391983032226562
Batch 48/64 loss: -0.9280385971069336
Batch 49/64 loss: -0.8780593872070312
Batch 50/64 loss: -0.9854192733764648
Batch 51/64 loss: -0.6629447937011719
Batch 52/64 loss: -0.9986124038696289
Batch 53/64 loss: -1.0067224502563477
Batch 54/64 loss: -1.3108577728271484
Batch 55/64 loss: -0.8612842559814453
Batch 56/64 loss: -1.0300931930541992
Batch 57/64 loss: -1.2679328918457031
Batch 58/64 loss: -0.8287858963012695
Batch 59/64 loss: -1.1007070541381836
Batch 60/64 loss: -0.7977495193481445
Batch 61/64 loss: -0.42974281311035156
Batch 62/64 loss: -1.0123968124389648
Batch 63/64 loss: -0.8286399841308594
Batch 64/64 loss: -5.043601036071777
Epoch 426  Train loss: -0.9039148704678405  Val loss: -0.8811778825582918
Saving best model, epoch: 426
Epoch 427
-------------------------------
Batch 1/64 loss: -1.3241510391235352
Batch 2/64 loss: -0.4706258773803711
Batch 3/64 loss: -0.7353019714355469
Batch 4/64 loss: -1.1158819198608398
Batch 5/64 loss: -0.7157506942749023
Batch 6/64 loss: -0.8263359069824219
Batch 7/64 loss: -0.8067646026611328
Batch 8/64 loss: -0.8763465881347656
Batch 9/64 loss: -0.47791099548339844
Batch 10/64 loss: -0.9450397491455078
Batch 11/64 loss: -0.6406412124633789
Batch 12/64 loss: -0.8052253723144531
Batch 13/64 loss: -0.7403554916381836
Batch 14/64 loss: -0.9850616455078125
Batch 15/64 loss: -0.6841611862182617
Batch 16/64 loss: -0.793452262878418
Batch 17/64 loss: -1.0201492309570312
Batch 18/64 loss: -0.7958841323852539
Batch 19/64 loss: -1.149261474609375
Batch 20/64 loss: -0.7992696762084961
Batch 21/64 loss: -0.7760334014892578
Batch 22/64 loss: -0.7989215850830078
Batch 23/64 loss: -1.0866069793701172
Batch 24/64 loss: -0.554926872253418
Batch 25/64 loss: -1.101109504699707
Batch 26/64 loss: -1.1518068313598633
Batch 27/64 loss: -0.9534463882446289
Batch 28/64 loss: -0.998321533203125
Batch 29/64 loss: -0.628291130065918
Batch 30/64 loss: -1.1008853912353516
Batch 31/64 loss: -1.316035270690918
Batch 32/64 loss: -1.0423526763916016
Batch 33/64 loss: -0.2261066436767578
Batch 34/64 loss: -1.1086149215698242
Batch 35/64 loss: -1.1044645309448242
Batch 36/64 loss: -0.9147138595581055
Batch 37/64 loss: -1.022242546081543
Batch 38/64 loss: -0.7893304824829102
Batch 39/64 loss: -0.9488382339477539
Batch 40/64 loss: -1.0035638809204102
Batch 41/64 loss: -0.6899328231811523
Batch 42/64 loss: -0.39981651306152344
Batch 43/64 loss: -0.910893440246582
Batch 44/64 loss: -0.9350872039794922
Batch 45/64 loss: -0.7861967086791992
Batch 46/64 loss: -0.23309898376464844
Batch 47/64 loss: -0.4119729995727539
Batch 48/64 loss: -1.0018110275268555
Batch 49/64 loss: -0.9896535873413086
Batch 50/64 loss: -1.0417890548706055
Batch 51/64 loss: -0.7225284576416016
Batch 52/64 loss: -0.8023586273193359
Batch 53/64 loss: -0.8702030181884766
Batch 54/64 loss: -1.142167091369629
Batch 55/64 loss: -0.8746881484985352
Batch 56/64 loss: -0.6760234832763672
Batch 57/64 loss: -1.2100677490234375
Batch 58/64 loss: -1.2177047729492188
Batch 59/64 loss: -0.788905143737793
Batch 60/64 loss: -0.6142616271972656
Batch 61/64 loss: -0.6793031692504883
Batch 62/64 loss: -0.5635604858398438
Batch 63/64 loss: -0.6712303161621094
Batch 64/64 loss: -4.27592658996582
Epoch 427  Train loss: -0.8905785579307407  Val loss: -0.5792782577042727
Epoch 428
-------------------------------
Batch 1/64 loss: -0.789341926574707
Batch 2/64 loss: -1.019974708557129
Batch 3/64 loss: -0.7435770034790039
Batch 4/64 loss: -1.3075876235961914
Batch 5/64 loss: -0.9237327575683594
Batch 6/64 loss: -0.6337928771972656
Batch 7/64 loss: -0.6272706985473633
Batch 8/64 loss: -0.6411266326904297
Batch 9/64 loss: -0.4030723571777344
Batch 10/64 loss: -0.9352397918701172
Batch 11/64 loss: -0.4290952682495117
Batch 12/64 loss: -0.969024658203125
Batch 13/64 loss: -0.7439432144165039
Batch 14/64 loss: -0.7814617156982422
Batch 15/64 loss: -0.14020633697509766
Batch 16/64 loss: -0.5588016510009766
Batch 17/64 loss: -0.39090633392333984
Batch 18/64 loss: -0.7615909576416016
Batch 19/64 loss: -0.8209075927734375
Batch 20/64 loss: -0.8414716720581055
Batch 21/64 loss: -0.4218769073486328
Batch 22/64 loss: -0.8315496444702148
Batch 23/64 loss: -0.7360076904296875
Batch 24/64 loss: -0.8459634780883789
Batch 25/64 loss: -0.15328216552734375
Batch 26/64 loss: -0.9642877578735352
Batch 27/64 loss: -0.6491880416870117
Batch 28/64 loss: -0.6717491149902344
Batch 29/64 loss: -0.6637973785400391
Batch 30/64 loss: -0.9658718109130859
Batch 31/64 loss: -1.1305675506591797
Batch 32/64 loss: -0.686884880065918
Batch 33/64 loss: -0.9055366516113281
Batch 34/64 loss: -0.8498077392578125
Batch 35/64 loss: -0.6259269714355469
Batch 36/64 loss: -0.8646879196166992
Batch 37/64 loss: -0.7063102722167969
Batch 38/64 loss: -0.6214237213134766
Batch 39/64 loss: -1.0599985122680664
Batch 40/64 loss: -0.7838878631591797
Batch 41/64 loss: -0.41434288024902344
Batch 42/64 loss: -0.8402175903320312
Batch 43/64 loss: -0.8415460586547852
Batch 44/64 loss: -0.8429393768310547
Batch 45/64 loss: -0.863978385925293
Batch 46/64 loss: -0.6569833755493164
Batch 47/64 loss: -0.5563440322875977
Batch 48/64 loss: -0.5066852569580078
Batch 49/64 loss: -0.48403167724609375
Batch 50/64 loss: -0.573089599609375
Batch 51/64 loss: -0.9695806503295898
Batch 52/64 loss: -0.8671779632568359
Batch 53/64 loss: -0.5535612106323242
Batch 54/64 loss: -1.0464820861816406
Batch 55/64 loss: -0.6140604019165039
Batch 56/64 loss: -0.9616060256958008
Batch 57/64 loss: -0.8555412292480469
Batch 58/64 loss: -0.9714345932006836
Batch 59/64 loss: -1.1193513870239258
Batch 60/64 loss: -1.2423934936523438
Batch 61/64 loss: -0.6887302398681641
Batch 62/64 loss: -0.8070907592773438
Batch 63/64 loss: -1.0223979949951172
Batch 64/64 loss: -4.510286331176758
Epoch 428  Train loss: -0.8044394175211589  Val loss: -0.785402959974361
Epoch 429
-------------------------------
Batch 1/64 loss: -0.9077634811401367
Batch 2/64 loss: -0.5995092391967773
Batch 3/64 loss: -0.8333587646484375
Batch 4/64 loss: -0.47873878479003906
Batch 5/64 loss: -0.8509063720703125
Batch 6/64 loss: -0.8395175933837891
Batch 7/64 loss: -0.34226226806640625
Batch 8/64 loss: -0.9261636734008789
Batch 9/64 loss: -0.6953315734863281
Batch 10/64 loss: -0.3564004898071289
Batch 11/64 loss: -1.0623207092285156
Batch 12/64 loss: -0.4230985641479492
Batch 13/64 loss: -0.7732601165771484
Batch 14/64 loss: -0.85296630859375
Batch 15/64 loss: -0.884343147277832
Batch 16/64 loss: -0.6597023010253906
Batch 17/64 loss: -0.25661468505859375
Batch 18/64 loss: -0.9838037490844727
Batch 19/64 loss: -1.046905517578125
Batch 20/64 loss: -0.9340038299560547
Batch 21/64 loss: -0.6895895004272461
Batch 22/64 loss: -0.8355321884155273
Batch 23/64 loss: -1.104353904724121
Batch 24/64 loss: -0.7772045135498047
Batch 25/64 loss: -0.8673830032348633
Batch 26/64 loss: -1.2478952407836914
Batch 27/64 loss: -0.7774887084960938
Batch 28/64 loss: -1.0759077072143555
Batch 29/64 loss: -0.8567352294921875
Batch 30/64 loss: -1.079946517944336
Batch 31/64 loss: -0.5363588333129883
Batch 32/64 loss: -0.8553400039672852
Batch 33/64 loss: -1.1807737350463867
Batch 34/64 loss: -0.8074026107788086
Batch 35/64 loss: 0.24501895904541016
Batch 36/64 loss: -0.5633535385131836
Batch 37/64 loss: -0.8488864898681641
Batch 38/64 loss: -1.0140323638916016
Batch 39/64 loss: -0.3909454345703125
Batch 40/64 loss: -1.1888198852539062
Batch 41/64 loss: -1.0275096893310547
Batch 42/64 loss: -0.853515625
Batch 43/64 loss: -0.6484775543212891
Batch 44/64 loss: -0.37001991271972656
Batch 45/64 loss: -0.43642234802246094
Batch 46/64 loss: -0.8858814239501953
Batch 47/64 loss: -1.4140520095825195
Batch 48/64 loss: -1.0369977951049805
Batch 49/64 loss: -0.9945735931396484
Batch 50/64 loss: -0.5040407180786133
Batch 51/64 loss: -0.9838361740112305
Batch 52/64 loss: -0.9129648208618164
Batch 53/64 loss: -0.8132762908935547
Batch 54/64 loss: -0.7108774185180664
Batch 55/64 loss: -0.7521343231201172
Batch 56/64 loss: -0.6841821670532227
Batch 57/64 loss: -1.1786956787109375
Batch 58/64 loss: -0.9817228317260742
Batch 59/64 loss: -0.7008810043334961
Batch 60/64 loss: -0.8789176940917969
Batch 61/64 loss: -0.8459510803222656
Batch 62/64 loss: -0.9195766448974609
Batch 63/64 loss: -1.0768747329711914
Batch 64/64 loss: -4.9113359451293945
Epoch 429  Train loss: -0.8541927075853535  Val loss: -0.817604012505705
Epoch 430
-------------------------------
Batch 1/64 loss: -0.6161079406738281
Batch 2/64 loss: -0.9257278442382812
Batch 3/64 loss: -0.5011787414550781
Batch 4/64 loss: -1.0385627746582031
Batch 5/64 loss: -0.7591953277587891
Batch 6/64 loss: -1.1006231307983398
Batch 7/64 loss: -0.48184776306152344
Batch 8/64 loss: -0.8883914947509766
Batch 9/64 loss: -1.107895851135254
Batch 10/64 loss: -0.5471343994140625
Batch 11/64 loss: -0.918879508972168
Batch 12/64 loss: -0.9835309982299805
Batch 13/64 loss: -1.000680923461914
Batch 14/64 loss: -0.7269306182861328
Batch 15/64 loss: -0.7020854949951172
Batch 16/64 loss: -1.0447826385498047
Batch 17/64 loss: -0.9076271057128906
Batch 18/64 loss: -0.4277191162109375
Batch 19/64 loss: -0.4830045700073242
Batch 20/64 loss: -0.33100223541259766
Batch 21/64 loss: -0.538203239440918
Batch 22/64 loss: -1.0131025314331055
Batch 23/64 loss: -1.311751365661621
Batch 24/64 loss: -0.7758903503417969
Batch 25/64 loss: -1.1277236938476562
Batch 26/64 loss: -0.9551305770874023
Batch 27/64 loss: -0.9377412796020508
Batch 28/64 loss: -0.7718753814697266
Batch 29/64 loss: -0.7778043746948242
Batch 30/64 loss: -0.9221305847167969
Batch 31/64 loss: -0.5309925079345703
Batch 32/64 loss: -0.7452983856201172
Batch 33/64 loss: -0.8734235763549805
Batch 34/64 loss: -1.0549421310424805
Batch 35/64 loss: -0.8758707046508789
Batch 36/64 loss: -0.9358539581298828
Batch 37/64 loss: -0.9513435363769531
Batch 38/64 loss: -0.7432565689086914
Batch 39/64 loss: -0.8293352127075195
Batch 40/64 loss: -0.8734512329101562
Batch 41/64 loss: -0.9317703247070312
Batch 42/64 loss: -0.5781698226928711
Batch 43/64 loss: -0.8061227798461914
Batch 44/64 loss: -0.7086086273193359
Batch 45/64 loss: -1.0262832641601562
Batch 46/64 loss: -0.7535638809204102
Batch 47/64 loss: -0.5966196060180664
Batch 48/64 loss: -0.8122634887695312
Batch 49/64 loss: -0.4598674774169922
Batch 50/64 loss: -0.9044637680053711
Batch 51/64 loss: -0.8380622863769531
Batch 52/64 loss: -0.9232568740844727
Batch 53/64 loss: -0.6214923858642578
Batch 54/64 loss: -0.8100471496582031
Batch 55/64 loss: -0.8707313537597656
Batch 56/64 loss: -0.7411918640136719
Batch 57/64 loss: -1.0612850189208984
Batch 58/64 loss: -0.9276618957519531
Batch 59/64 loss: -1.3591623306274414
Batch 60/64 loss: -0.8484830856323242
Batch 61/64 loss: -0.8153171539306641
Batch 62/64 loss: -0.8219900131225586
Batch 63/64 loss: -0.6999282836914062
Batch 64/64 loss: -4.686832427978516
Epoch 430  Train loss: -0.8701097525802313  Val loss: -0.8405978278196145
Epoch 431
-------------------------------
Batch 1/64 loss: -0.4945659637451172
Batch 2/64 loss: -0.7894229888916016
Batch 3/64 loss: -0.05933380126953125
Batch 4/64 loss: -1.2134180068969727
Batch 5/64 loss: -1.0240106582641602
Batch 6/64 loss: -0.6331710815429688
Batch 7/64 loss: -0.8780803680419922
Batch 8/64 loss: -0.8506479263305664
Batch 9/64 loss: -0.9105892181396484
Batch 10/64 loss: -1.108367919921875
Batch 11/64 loss: -0.7413921356201172
Batch 12/64 loss: -0.9108781814575195
Batch 13/64 loss: -1.0434236526489258
Batch 14/64 loss: -0.8685455322265625
Batch 15/64 loss: -0.5576620101928711
Batch 16/64 loss: -0.6500034332275391
Batch 17/64 loss: -0.8318490982055664
Batch 18/64 loss: -1.0802392959594727
Batch 19/64 loss: -0.8018360137939453
Batch 20/64 loss: 0.39799976348876953
Batch 21/64 loss: -0.9708290100097656
Batch 22/64 loss: -0.6183338165283203
Batch 23/64 loss: -0.9779014587402344
Batch 24/64 loss: -0.8164491653442383
Batch 25/64 loss: -0.6430559158325195
Batch 26/64 loss: -0.6749591827392578
Batch 27/64 loss: -0.958439826965332
Batch 28/64 loss: -1.001978874206543
Batch 29/64 loss: -0.9371261596679688
Batch 30/64 loss: -0.9902210235595703
Batch 31/64 loss: -0.8285207748413086
Batch 32/64 loss: -1.0290794372558594
Batch 33/64 loss: -1.3550605773925781
Batch 34/64 loss: -0.6906061172485352
Batch 35/64 loss: -1.1743602752685547
Batch 36/64 loss: -0.9126968383789062
Batch 37/64 loss: -0.28018665313720703
Batch 38/64 loss: -0.4466361999511719
Batch 39/64 loss: -0.8374414443969727
Batch 40/64 loss: -1.1874542236328125
Batch 41/64 loss: -0.7535839080810547
Batch 42/64 loss: -0.8384113311767578
Batch 43/64 loss: -0.8335227966308594
Batch 44/64 loss: -0.708317756652832
Batch 45/64 loss: -1.1519536972045898
Batch 46/64 loss: -0.8623409271240234
Batch 47/64 loss: -0.821502685546875
Batch 48/64 loss: -1.0793190002441406
Batch 49/64 loss: -0.8565120697021484
Batch 50/64 loss: -0.4271240234375
Batch 51/64 loss: -0.9253273010253906
Batch 52/64 loss: -0.7297267913818359
Batch 53/64 loss: -1.2374114990234375
Batch 54/64 loss: -1.228729248046875
Batch 55/64 loss: -0.6765232086181641
Batch 56/64 loss: -0.8033304214477539
Batch 57/64 loss: -0.7703056335449219
Batch 58/64 loss: -0.7533359527587891
Batch 59/64 loss: -0.7067317962646484
Batch 60/64 loss: -0.555150032043457
Batch 61/64 loss: -0.6309738159179688
Batch 62/64 loss: -0.9274692535400391
Batch 63/64 loss: -0.9585084915161133
Batch 64/64 loss: -4.412168502807617
Epoch 431  Train loss: -0.8615845100552428  Val loss: -0.8605919670812863
Epoch 432
-------------------------------
Batch 1/64 loss: -0.7637853622436523
Batch 2/64 loss: -0.9281091690063477
Batch 3/64 loss: -1.231278419494629
Batch 4/64 loss: -0.6680364608764648
Batch 5/64 loss: -0.6469945907592773
Batch 6/64 loss: -0.9946298599243164
Batch 7/64 loss: -0.6055612564086914
Batch 8/64 loss: 0.013039112091064453
Batch 9/64 loss: -0.7819576263427734
Batch 10/64 loss: -0.8147106170654297
Batch 11/64 loss: -1.0893497467041016
Batch 12/64 loss: -1.3260889053344727
Batch 13/64 loss: -0.8398704528808594
Batch 14/64 loss: -0.9079885482788086
Batch 15/64 loss: -0.8376827239990234
Batch 16/64 loss: -0.8845968246459961
Batch 17/64 loss: -1.2396459579467773
Batch 18/64 loss: -0.6927719116210938
Batch 19/64 loss: -0.9550495147705078
Batch 20/64 loss: -0.8438329696655273
Batch 21/64 loss: -0.2180490493774414
Batch 22/64 loss: -0.8606767654418945
Batch 23/64 loss: -0.5297183990478516
Batch 24/64 loss: -0.9921092987060547
Batch 25/64 loss: -0.8612089157104492
Batch 26/64 loss: -0.30977344512939453
Batch 27/64 loss: -0.8170738220214844
Batch 28/64 loss: -0.6123046875
Batch 29/64 loss: -1.1657476425170898
Batch 30/64 loss: -0.5441770553588867
Batch 31/64 loss: -0.794886589050293
Batch 32/64 loss: -1.0789833068847656
Batch 33/64 loss: -0.6240043640136719
Batch 34/64 loss: -1.1744804382324219
Batch 35/64 loss: -0.8046903610229492
Batch 36/64 loss: -0.9089460372924805
Batch 37/64 loss: -0.3407411575317383
Batch 38/64 loss: -1.0587196350097656
Batch 39/64 loss: -1.225006103515625
Batch 40/64 loss: -0.9459810256958008
Batch 41/64 loss: -1.0239315032958984
Batch 42/64 loss: -1.031407356262207
Batch 43/64 loss: -0.7240114212036133
Batch 44/64 loss: -1.0421171188354492
Batch 45/64 loss: -0.4776935577392578
Batch 46/64 loss: -1.3535833358764648
Batch 47/64 loss: -0.7569427490234375
Batch 48/64 loss: -0.7756252288818359
Batch 49/64 loss: -1.148040771484375
Batch 50/64 loss: -0.5762786865234375
Batch 51/64 loss: -1.1167869567871094
Batch 52/64 loss: -0.661524772644043
Batch 53/64 loss: -0.835240364074707
Batch 54/64 loss: -0.5366983413696289
Batch 55/64 loss: -0.8368053436279297
Batch 56/64 loss: -0.9819126129150391
Batch 57/64 loss: -1.0073938369750977
Batch 58/64 loss: -0.6038188934326172
Batch 59/64 loss: -0.6635875701904297
Batch 60/64 loss: -0.6114101409912109
Batch 61/64 loss: -0.8755970001220703
Batch 62/64 loss: -1.2970695495605469
Batch 63/64 loss: -0.9570693969726562
Batch 64/64 loss: -4.558104038238525
Epoch 432  Train loss: -0.8818714422338149  Val loss: -0.7076760518182185
Epoch 433
-------------------------------
Batch 1/64 loss: -0.8502740859985352
Batch 2/64 loss: -0.8503103256225586
Batch 3/64 loss: -1.0153112411499023
Batch 4/64 loss: -0.7803630828857422
Batch 5/64 loss: -0.3326988220214844
Batch 6/64 loss: -0.5634069442749023
Batch 7/64 loss: -0.8482065200805664
Batch 8/64 loss: -1.01629638671875
Batch 9/64 loss: -0.8552112579345703
Batch 10/64 loss: -0.6882505416870117
Batch 11/64 loss: -0.7149934768676758
Batch 12/64 loss: -0.9527626037597656
Batch 13/64 loss: -0.8834028244018555
Batch 14/64 loss: -0.7064895629882812
Batch 15/64 loss: -0.8751420974731445
Batch 16/64 loss: -0.7835111618041992
Batch 17/64 loss: -0.9177579879760742
Batch 18/64 loss: -0.634796142578125
Batch 19/64 loss: -0.5345516204833984
Batch 20/64 loss: -0.6670103073120117
Batch 21/64 loss: -0.9606208801269531
Batch 22/64 loss: -0.6305942535400391
Batch 23/64 loss: -0.7941274642944336
Batch 24/64 loss: -0.7125673294067383
Batch 25/64 loss: -0.7239036560058594
Batch 26/64 loss: -0.5108470916748047
Batch 27/64 loss: -0.7935504913330078
Batch 28/64 loss: -0.8846836090087891
Batch 29/64 loss: -0.4691171646118164
Batch 30/64 loss: -1.131913185119629
Batch 31/64 loss: -0.6488552093505859
Batch 32/64 loss: -0.7980642318725586
Batch 33/64 loss: -1.0620403289794922
Batch 34/64 loss: -0.5660934448242188
Batch 35/64 loss: -1.015131950378418
Batch 36/64 loss: -0.9140386581420898
Batch 37/64 loss: -1.1507034301757812
Batch 38/64 loss: -0.9743423461914062
Batch 39/64 loss: -0.8241796493530273
Batch 40/64 loss: -1.0955877304077148
Batch 41/64 loss: -1.1212959289550781
Batch 42/64 loss: -0.7930994033813477
Batch 43/64 loss: -0.9431276321411133
Batch 44/64 loss: -0.9441375732421875
Batch 45/64 loss: -0.8497838973999023
Batch 46/64 loss: -0.6112041473388672
Batch 47/64 loss: -0.982757568359375
Batch 48/64 loss: -1.0745468139648438
Batch 49/64 loss: 0.06342792510986328
Batch 50/64 loss: -1.2767953872680664
Batch 51/64 loss: -0.9251117706298828
Batch 52/64 loss: -0.9692649841308594
Batch 53/64 loss: -0.8238706588745117
Batch 54/64 loss: -0.9376726150512695
Batch 55/64 loss: -0.8494768142700195
Batch 56/64 loss: -0.8742380142211914
Batch 57/64 loss: -1.0787992477416992
Batch 58/64 loss: -1.1026897430419922
Batch 59/64 loss: -0.8262348175048828
Batch 60/64 loss: -0.3647470474243164
Batch 61/64 loss: -0.4130716323852539
Batch 62/64 loss: -0.9074869155883789
Batch 63/64 loss: -0.7492647171020508
Batch 64/64 loss: -4.139627933502197
Epoch 433  Train loss: -0.8563400698643104  Val loss: -0.7124960332392007
Epoch 434
-------------------------------
Batch 1/64 loss: -0.7067527770996094
Batch 2/64 loss: -1.0036077499389648
Batch 3/64 loss: -0.8763494491577148
Batch 4/64 loss: -0.7178525924682617
Batch 5/64 loss: -0.8697700500488281
Batch 6/64 loss: -0.6928853988647461
Batch 7/64 loss: -0.619664192199707
Batch 8/64 loss: -0.6378087997436523
Batch 9/64 loss: -0.39195823669433594
Batch 10/64 loss: -0.9760847091674805
Batch 11/64 loss: -1.1630535125732422
Batch 12/64 loss: -0.771575927734375
Batch 13/64 loss: -1.1759777069091797
Batch 14/64 loss: -0.919917106628418
Batch 15/64 loss: -0.8117923736572266
Batch 16/64 loss: -0.7932548522949219
Batch 17/64 loss: -0.8459196090698242
Batch 18/64 loss: -0.7358026504516602
Batch 19/64 loss: -0.3876323699951172
Batch 20/64 loss: -0.9636936187744141
Batch 21/64 loss: -0.6855640411376953
Batch 22/64 loss: -0.638463020324707
Batch 23/64 loss: -0.7867317199707031
Batch 24/64 loss: -0.6436834335327148
Batch 25/64 loss: -1.2298336029052734
Batch 26/64 loss: -0.8584194183349609
Batch 27/64 loss: -0.6990251541137695
Batch 28/64 loss: -0.9944257736206055
Batch 29/64 loss: -0.7907257080078125
Batch 30/64 loss: -0.9121780395507812
Batch 31/64 loss: -0.9581241607666016
Batch 32/64 loss: -0.9063758850097656
Batch 33/64 loss: -0.9244565963745117
Batch 34/64 loss: -0.31656742095947266
Batch 35/64 loss: -0.7878503799438477
Batch 36/64 loss: -0.8013114929199219
Batch 37/64 loss: -0.8388595581054688
Batch 38/64 loss: -0.8501186370849609
Batch 39/64 loss: -0.550267219543457
Batch 40/64 loss: -0.8582439422607422
Batch 41/64 loss: -1.088486671447754
Batch 42/64 loss: -1.1181221008300781
Batch 43/64 loss: -0.5809412002563477
Batch 44/64 loss: -0.6835556030273438
Batch 45/64 loss: -0.9169406890869141
Batch 46/64 loss: -0.7481966018676758
Batch 47/64 loss: -0.6004047393798828
Batch 48/64 loss: -0.7858190536499023
Batch 49/64 loss: -1.077962875366211
Batch 50/64 loss: -0.9994449615478516
Batch 51/64 loss: -0.911707878112793
Batch 52/64 loss: -0.8392524719238281
Batch 53/64 loss: -0.9300870895385742
Batch 54/64 loss: -1.0374927520751953
Batch 55/64 loss: -0.5606603622436523
Batch 56/64 loss: -1.0443248748779297
Batch 57/64 loss: -1.1614933013916016
Batch 58/64 loss: -0.9440546035766602
Batch 59/64 loss: -0.39220333099365234
Batch 60/64 loss: -0.920170783996582
Batch 61/64 loss: -0.6364917755126953
Batch 62/64 loss: -0.680516242980957
Batch 63/64 loss: -0.9492616653442383
Batch 64/64 loss: -4.821393013000488
Epoch 434  Train loss: -0.86770533767401  Val loss: -0.7813975278454548
Epoch 435
-------------------------------
Batch 1/64 loss: -0.90911865234375
Batch 2/64 loss: -1.053910255432129
Batch 3/64 loss: -0.9485483169555664
Batch 4/64 loss: -0.8190450668334961
Batch 5/64 loss: -0.6991615295410156
Batch 6/64 loss: -1.3066329956054688
Batch 7/64 loss: -0.6856203079223633
Batch 8/64 loss: -0.9767608642578125
Batch 9/64 loss: -1.0967893600463867
Batch 10/64 loss: -0.5368671417236328
Batch 11/64 loss: -0.7362480163574219
Batch 12/64 loss: -1.1019220352172852
Batch 13/64 loss: -1.2709875106811523
Batch 14/64 loss: -1.1542329788208008
Batch 15/64 loss: -1.2013845443725586
Batch 16/64 loss: -0.8970670700073242
Batch 17/64 loss: -0.42115211486816406
Batch 18/64 loss: -1.0823335647583008
Batch 19/64 loss: -1.2992734909057617
Batch 20/64 loss: 0.8757953643798828
Batch 21/64 loss: -0.7044305801391602
Batch 22/64 loss: -1.216695785522461
Batch 23/64 loss: -1.0292377471923828
Batch 24/64 loss: -0.4752616882324219
Batch 25/64 loss: -0.7215700149536133
Batch 26/64 loss: -0.7016849517822266
Batch 27/64 loss: -0.6223621368408203
Batch 28/64 loss: -0.45502376556396484
Batch 29/64 loss: -0.2014007568359375
Batch 30/64 loss: -0.8510665893554688
Batch 31/64 loss: -0.34009838104248047
Batch 32/64 loss: -0.8696660995483398
Batch 33/64 loss: -0.7682018280029297
Batch 34/64 loss: -0.8065776824951172
Batch 35/64 loss: -0.8523159027099609
Batch 36/64 loss: -1.0475702285766602
Batch 37/64 loss: -0.9456796646118164
Batch 38/64 loss: -0.5618267059326172
Batch 39/64 loss: -0.6955242156982422
Batch 40/64 loss: -0.9103403091430664
Batch 41/64 loss: -0.6536645889282227
Batch 42/64 loss: -0.7211332321166992
Batch 43/64 loss: -0.839533805847168
Batch 44/64 loss: -0.6955976486206055
Batch 45/64 loss: -0.8885335922241211
Batch 46/64 loss: -1.0518836975097656
Batch 47/64 loss: -0.41309165954589844
Batch 48/64 loss: -0.7548599243164062
Batch 49/64 loss: -0.9090738296508789
Batch 50/64 loss: -0.6105937957763672
Batch 51/64 loss: -0.7777957916259766
Batch 52/64 loss: -0.5649013519287109
Batch 53/64 loss: -0.6862583160400391
Batch 54/64 loss: -0.8201122283935547
Batch 55/64 loss: -0.6858577728271484
Batch 56/64 loss: -0.8666849136352539
Batch 57/64 loss: -1.1406688690185547
Batch 58/64 loss: -0.9856424331665039
Batch 59/64 loss: -0.9301252365112305
Batch 60/64 loss: -0.7181863784790039
Batch 61/64 loss: -0.5349493026733398
Batch 62/64 loss: -1.0613536834716797
Batch 63/64 loss: -0.6459693908691406
Batch 64/64 loss: -5.401369571685791
Epoch 435  Train loss: -0.848710495817895  Val loss: -0.8723548941595858
Epoch 436
-------------------------------
Batch 1/64 loss: -1.0969085693359375
Batch 2/64 loss: -1.2297372817993164
Batch 3/64 loss: -0.4933309555053711
Batch 4/64 loss: -0.2645149230957031
Batch 5/64 loss: -0.7955389022827148
Batch 6/64 loss: -0.88348388671875
Batch 7/64 loss: -0.8864946365356445
Batch 8/64 loss: -0.9453935623168945
Batch 9/64 loss: -0.9730606079101562
Batch 10/64 loss: -0.4612579345703125
Batch 11/64 loss: -0.8280792236328125
Batch 12/64 loss: -1.128509521484375
Batch 13/64 loss: -0.8537998199462891
Batch 14/64 loss: -1.1207294464111328
Batch 15/64 loss: -0.9275264739990234
Batch 16/64 loss: -0.9642562866210938
Batch 17/64 loss: -0.7929878234863281
Batch 18/64 loss: -1.2304515838623047
Batch 19/64 loss: -1.2859077453613281
Batch 20/64 loss: -0.8749685287475586
Batch 21/64 loss: -0.9180784225463867
Batch 22/64 loss: -0.881627082824707
Batch 23/64 loss: -0.9979152679443359
Batch 24/64 loss: -0.6910600662231445
Batch 25/64 loss: -0.8490409851074219
Batch 26/64 loss: -0.9738979339599609
Batch 27/64 loss: -0.8774280548095703
Batch 28/64 loss: -0.9787063598632812
Batch 29/64 loss: -0.8692169189453125
Batch 30/64 loss: -0.7982473373413086
Batch 31/64 loss: -1.2101411819458008
Batch 32/64 loss: -0.7496538162231445
Batch 33/64 loss: -0.9874305725097656
Batch 34/64 loss: -0.347320556640625
Batch 35/64 loss: -0.2717256546020508
Batch 36/64 loss: -0.7795314788818359
Batch 37/64 loss: -0.9883909225463867
Batch 38/64 loss: -1.172316551208496
Batch 39/64 loss: -0.8762397766113281
Batch 40/64 loss: -0.5925140380859375
Batch 41/64 loss: -0.7138986587524414
Batch 42/64 loss: -0.9439916610717773
Batch 43/64 loss: -0.4616565704345703
Batch 44/64 loss: -0.7952957153320312
Batch 45/64 loss: -1.0636367797851562
Batch 46/64 loss: -1.0464057922363281
Batch 47/64 loss: -0.5361194610595703
Batch 48/64 loss: -0.8970556259155273
Batch 49/64 loss: -1.1199407577514648
Batch 50/64 loss: -0.38043785095214844
Batch 51/64 loss: -0.8343877792358398
Batch 52/64 loss: -0.8982419967651367
Batch 53/64 loss: -0.6339731216430664
Batch 54/64 loss: -0.9464902877807617
Batch 55/64 loss: -0.9592475891113281
Batch 56/64 loss: -1.0022611618041992
Batch 57/64 loss: -1.2574653625488281
Batch 58/64 loss: -0.9574670791625977
Batch 59/64 loss: -1.1282663345336914
Batch 60/64 loss: -0.9580173492431641
Batch 61/64 loss: -0.7182445526123047
Batch 62/64 loss: -0.9511661529541016
Batch 63/64 loss: -0.7780790328979492
Batch 64/64 loss: -4.936856269836426
Epoch 436  Train loss: -0.918146032445571  Val loss: -0.8552755375498349
Epoch 437
-------------------------------
Batch 1/64 loss: -1.1503171920776367
Batch 2/64 loss: -0.6021881103515625
Batch 3/64 loss: -0.9453163146972656
Batch 4/64 loss: -1.265523910522461
Batch 5/64 loss: -0.6399316787719727
Batch 6/64 loss: -0.34174633026123047
Batch 7/64 loss: -0.8877391815185547
Batch 8/64 loss: -0.8222970962524414
Batch 9/64 loss: -0.7619619369506836
Batch 10/64 loss: -0.808588981628418
Batch 11/64 loss: -0.8424091339111328
Batch 12/64 loss: -0.9083547592163086
Batch 13/64 loss: -1.0884218215942383
Batch 14/64 loss: -0.6790351867675781
Batch 15/64 loss: -1.1955623626708984
Batch 16/64 loss: -0.7187223434448242
Batch 17/64 loss: -0.7682533264160156
Batch 18/64 loss: -1.0413227081298828
Batch 19/64 loss: -0.9218235015869141
Batch 20/64 loss: -0.9173717498779297
Batch 21/64 loss: -1.0522527694702148
Batch 22/64 loss: -0.9060592651367188
Batch 23/64 loss: -0.8784389495849609
Batch 24/64 loss: -0.9328584671020508
Batch 25/64 loss: -0.745661735534668
Batch 26/64 loss: -0.9780368804931641
Batch 27/64 loss: -0.9677562713623047
Batch 28/64 loss: -0.7242031097412109
Batch 29/64 loss: -0.6430377960205078
Batch 30/64 loss: -1.1156339645385742
Batch 31/64 loss: -0.937744140625
Batch 32/64 loss: -0.9796695709228516
Batch 33/64 loss: -0.9342737197875977
Batch 34/64 loss: -0.6309747695922852
Batch 35/64 loss: -0.7583074569702148
Batch 36/64 loss: -0.9343538284301758
Batch 37/64 loss: -1.014481544494629
Batch 38/64 loss: -1.068160057067871
Batch 39/64 loss: -0.5698089599609375
Batch 40/64 loss: -1.027914047241211
Batch 41/64 loss: -1.016587257385254
Batch 42/64 loss: -1.088705062866211
Batch 43/64 loss: -0.8496675491333008
Batch 44/64 loss: -0.7675638198852539
Batch 45/64 loss: -1.019454002380371
Batch 46/64 loss: -0.7033367156982422
Batch 47/64 loss: -0.9492044448852539
Batch 48/64 loss: -0.7744865417480469
Batch 49/64 loss: -1.0687780380249023
Batch 50/64 loss: -0.11732006072998047
Batch 51/64 loss: -0.6349163055419922
Batch 52/64 loss: -0.9674205780029297
Batch 53/64 loss: -1.1045341491699219
Batch 54/64 loss: -1.084543228149414
Batch 55/64 loss: -0.8339567184448242
Batch 56/64 loss: -0.8824272155761719
Batch 57/64 loss: -1.1008920669555664
Batch 58/64 loss: -0.8307323455810547
Batch 59/64 loss: -0.8548364639282227
Batch 60/64 loss: -1.047123908996582
Batch 61/64 loss: -0.8840808868408203
Batch 62/64 loss: -0.9957914352416992
Batch 63/64 loss: -0.8250293731689453
Batch 64/64 loss: -4.965624809265137
Epoch 437  Train loss: -0.9291313208785712  Val loss: -0.8868276720604127
Saving best model, epoch: 437
Epoch 438
-------------------------------
Batch 1/64 loss: -0.8449716567993164
Batch 2/64 loss: -0.8874835968017578
Batch 3/64 loss: -1.1114740371704102
Batch 4/64 loss: -1.0291166305541992
Batch 5/64 loss: -1.145613670349121
Batch 6/64 loss: -0.870915412902832
Batch 7/64 loss: -0.5411100387573242
Batch 8/64 loss: -0.8758049011230469
Batch 9/64 loss: -0.9034099578857422
Batch 10/64 loss: -1.0286617279052734
Batch 11/64 loss: -0.39328765869140625
Batch 12/64 loss: -0.9677543640136719
Batch 13/64 loss: -0.788905143737793
Batch 14/64 loss: -0.7731351852416992
Batch 15/64 loss: -0.6816377639770508
Batch 16/64 loss: -1.0700063705444336
Batch 17/64 loss: -0.9470977783203125
Batch 18/64 loss: -0.7702980041503906
Batch 19/64 loss: -0.6606264114379883
Batch 20/64 loss: -0.9954233169555664
Batch 21/64 loss: -1.001673698425293
Batch 22/64 loss: -0.8714265823364258
Batch 23/64 loss: -1.1730337142944336
Batch 24/64 loss: -1.1019906997680664
Batch 25/64 loss: -0.8230438232421875
Batch 26/64 loss: -0.5884971618652344
Batch 27/64 loss: -1.256866455078125
Batch 28/64 loss: -0.8745546340942383
Batch 29/64 loss: -0.9302291870117188
Batch 30/64 loss: -1.0519495010375977
Batch 31/64 loss: -1.012451171875
Batch 32/64 loss: -1.2217025756835938
Batch 33/64 loss: -0.5827960968017578
Batch 34/64 loss: -0.9388742446899414
Batch 35/64 loss: -0.15884971618652344
Batch 36/64 loss: -1.1210508346557617
Batch 37/64 loss: -0.8594999313354492
Batch 38/64 loss: -0.7829904556274414
Batch 39/64 loss: -1.1581296920776367
Batch 40/64 loss: -0.7657785415649414
Batch 41/64 loss: -0.7239494323730469
Batch 42/64 loss: -0.9853305816650391
Batch 43/64 loss: -1.0672225952148438
Batch 44/64 loss: -0.7823381423950195
Batch 45/64 loss: -0.4281444549560547
Batch 46/64 loss: -1.3858413696289062
Batch 47/64 loss: -1.2245092391967773
Batch 48/64 loss: -0.6347026824951172
Batch 49/64 loss: -1.0982885360717773
Batch 50/64 loss: -0.9892253875732422
Batch 51/64 loss: -0.6097984313964844
Batch 52/64 loss: -1.1348838806152344
Batch 53/64 loss: -0.7542228698730469
Batch 54/64 loss: -0.4567232131958008
Batch 55/64 loss: -0.597193717956543
Batch 56/64 loss: -1.105377197265625
Batch 57/64 loss: -0.8185672760009766
Batch 58/64 loss: -1.080327033996582
Batch 59/64 loss: -1.0453720092773438
Batch 60/64 loss: -0.4606904983520508
Batch 61/64 loss: -1.0056123733520508
Batch 62/64 loss: -0.9804239273071289
Batch 63/64 loss: -0.7387189865112305
Batch 64/64 loss: -5.038054466247559
Epoch 438  Train loss: -0.9325196397070791  Val loss: -0.919452287077494
Saving best model, epoch: 438
Epoch 439
-------------------------------
Batch 1/64 loss: -1.0847959518432617
Batch 2/64 loss: -1.0967388153076172
Batch 3/64 loss: -1.1343193054199219
Batch 4/64 loss: -0.7105197906494141
Batch 5/64 loss: -1.0384883880615234
Batch 6/64 loss: -1.0288105010986328
Batch 7/64 loss: -1.0679550170898438
Batch 8/64 loss: -1.1670093536376953
Batch 9/64 loss: -0.9712276458740234
Batch 10/64 loss: -1.0648136138916016
Batch 11/64 loss: -1.127638816833496
Batch 12/64 loss: -0.8785400390625
Batch 13/64 loss: -0.8970470428466797
Batch 14/64 loss: -1.190323829650879
Batch 15/64 loss: -0.4207344055175781
Batch 16/64 loss: -0.9940395355224609
Batch 17/64 loss: -1.0643339157104492
Batch 18/64 loss: -1.049128532409668
Batch 19/64 loss: -0.9725666046142578
Batch 20/64 loss: -0.7020769119262695
Batch 21/64 loss: -0.8337526321411133
Batch 22/64 loss: -1.1076631546020508
Batch 23/64 loss: -1.191253662109375
Batch 24/64 loss: -0.7921829223632812
Batch 25/64 loss: -0.9550752639770508
Batch 26/64 loss: -1.0776300430297852
Batch 27/64 loss: -0.7486171722412109
Batch 28/64 loss: -1.3375320434570312
Batch 29/64 loss: -1.2348718643188477
Batch 30/64 loss: -0.8593940734863281
Batch 31/64 loss: -0.735753059387207
Batch 32/64 loss: -0.7296485900878906
Batch 33/64 loss: -1.0454912185668945
Batch 34/64 loss: -0.6593360900878906
Batch 35/64 loss: -0.7218542098999023
Batch 36/64 loss: -0.7717924118041992
Batch 37/64 loss: -1.0361671447753906
Batch 38/64 loss: -1.1862421035766602
Batch 39/64 loss: -1.1287117004394531
Batch 40/64 loss: -0.7163305282592773
Batch 41/64 loss: -0.8636112213134766
Batch 42/64 loss: -1.0324974060058594
Batch 43/64 loss: -0.9713830947875977
Batch 44/64 loss: -0.712193489074707
Batch 45/64 loss: -1.0504350662231445
Batch 46/64 loss: -0.6774435043334961
Batch 47/64 loss: -0.9238977432250977
Batch 48/64 loss: -0.8612756729125977
Batch 49/64 loss: -0.9594831466674805
Batch 50/64 loss: -1.216050148010254
Batch 51/64 loss: -0.9277429580688477
Batch 52/64 loss: -0.9664039611816406
Batch 53/64 loss: -0.8569860458374023
Batch 54/64 loss: -0.5562629699707031
Batch 55/64 loss: -0.9850616455078125
Batch 56/64 loss: -0.4751291275024414
Batch 57/64 loss: -1.0481147766113281
Batch 58/64 loss: -0.7273731231689453
Batch 59/64 loss: -0.7829179763793945
Batch 60/64 loss: -0.5237569808959961
Batch 61/64 loss: -0.5359697341918945
Batch 62/64 loss: -0.7008628845214844
Batch 63/64 loss: -0.3963947296142578
Batch 64/64 loss: -4.90125846862793
Epoch 439  Train loss: -0.9561976339302811  Val loss: -0.8124356482856462
Epoch 440
-------------------------------
Batch 1/64 loss: -0.701171875
Batch 2/64 loss: -1.017592430114746
Batch 3/64 loss: -0.7743949890136719
Batch 4/64 loss: -1.0492467880249023
Batch 5/64 loss: -0.8233566284179688
Batch 6/64 loss: -0.6016073226928711
Batch 7/64 loss: -0.6993751525878906
Batch 8/64 loss: -1.0482921600341797
Batch 9/64 loss: -0.7064857482910156
Batch 10/64 loss: -1.0767803192138672
Batch 11/64 loss: -0.9395618438720703
Batch 12/64 loss: -0.9746761322021484
Batch 13/64 loss: -0.8401250839233398
Batch 14/64 loss: -0.9080362319946289
Batch 15/64 loss: -0.5512237548828125
Batch 16/64 loss: -1.0085515975952148
Batch 17/64 loss: -0.8181924819946289
Batch 18/64 loss: -1.109243392944336
Batch 19/64 loss: -0.5802421569824219
Batch 20/64 loss: -1.1144981384277344
Batch 21/64 loss: -0.8969402313232422
Batch 22/64 loss: -1.1354084014892578
Batch 23/64 loss: -0.7686672210693359
Batch 24/64 loss: -0.9440135955810547
Batch 25/64 loss: -0.5704689025878906
Batch 26/64 loss: -0.4556236267089844
Batch 27/64 loss: -1.014388084411621
Batch 28/64 loss: -1.1160402297973633
Batch 29/64 loss: -0.7676839828491211
Batch 30/64 loss: -0.517237663269043
Batch 31/64 loss: -0.9732847213745117
Batch 32/64 loss: -0.7436399459838867
Batch 33/64 loss: -0.5071849822998047
Batch 34/64 loss: -0.8269853591918945
Batch 35/64 loss: -1.0266227722167969
Batch 36/64 loss: -0.7012710571289062
Batch 37/64 loss: -1.1285247802734375
Batch 38/64 loss: -0.6481838226318359
Batch 39/64 loss: -0.32706785202026367
Batch 40/64 loss: -0.8717517852783203
Batch 41/64 loss: -0.3828721046447754
Batch 42/64 loss: -0.9581060409545898
Batch 43/64 loss: -1.1900014877319336
Batch 44/64 loss: -0.7319240570068359
Batch 45/64 loss: -0.7805290222167969
Batch 46/64 loss: -0.7851686477661133
Batch 47/64 loss: -0.9450654983520508
Batch 48/64 loss: -0.9007740020751953
Batch 49/64 loss: -1.1549873352050781
Batch 50/64 loss: -0.5275812149047852
Batch 51/64 loss: -1.2244501113891602
Batch 52/64 loss: -1.1859331130981445
Batch 53/64 loss: -0.5269155502319336
Batch 54/64 loss: -0.7047882080078125
Batch 55/64 loss: -0.7761383056640625
Batch 56/64 loss: -1.058670997619629
Batch 57/64 loss: -0.8242092132568359
Batch 58/64 loss: -1.0280160903930664
Batch 59/64 loss: -0.880885124206543
Batch 60/64 loss: -0.6453208923339844
Batch 61/64 loss: -1.0078582763671875
Batch 62/64 loss: -0.95245361328125
Batch 63/64 loss: -0.2704486846923828
Batch 64/64 loss: -4.91558837890625
Epoch 440  Train loss: -0.884916582294539  Val loss: -0.9108528713999745
Epoch 441
-------------------------------
Batch 1/64 loss: -1.2154045104980469
Batch 2/64 loss: -1.0132989883422852
Batch 3/64 loss: -1.075068473815918
Batch 4/64 loss: -1.0402307510375977
Batch 5/64 loss: -1.0410003662109375
Batch 6/64 loss: -0.7183341979980469
Batch 7/64 loss: -0.8799562454223633
Batch 8/64 loss: -0.8929433822631836
Batch 9/64 loss: -0.8566770553588867
Batch 10/64 loss: -0.9106025695800781
Batch 11/64 loss: -0.7848720550537109
Batch 12/64 loss: -1.248183250427246
Batch 13/64 loss: -0.6547250747680664
Batch 14/64 loss: -0.39971446990966797
Batch 15/64 loss: -0.9672441482543945
Batch 16/64 loss: -0.49061012268066406
Batch 17/64 loss: -0.8141412734985352
Batch 18/64 loss: -1.0335922241210938
Batch 19/64 loss: -0.33051013946533203
Batch 20/64 loss: -0.9636631011962891
Batch 21/64 loss: -0.9466867446899414
Batch 22/64 loss: -1.032003402709961
Batch 23/64 loss: -1.046677589416504
Batch 24/64 loss: -0.9367046356201172
Batch 25/64 loss: -0.6994047164916992
Batch 26/64 loss: -0.7609844207763672
Batch 27/64 loss: -1.0983495712280273
Batch 28/64 loss: -1.1798830032348633
Batch 29/64 loss: -0.8666896820068359
Batch 30/64 loss: -0.8682785034179688
Batch 31/64 loss: -0.5977945327758789
Batch 32/64 loss: -1.0716991424560547
Batch 33/64 loss: -0.9405813217163086
Batch 34/64 loss: -0.8658819198608398
Batch 35/64 loss: -1.1677827835083008
Batch 36/64 loss: -0.8358373641967773
Batch 37/64 loss: -0.8392162322998047
Batch 38/64 loss: -1.0354366302490234
Batch 39/64 loss: -0.8853530883789062
Batch 40/64 loss: -0.9025440216064453
Batch 41/64 loss: -1.0077991485595703
Batch 42/64 loss: -1.0309886932373047
Batch 43/64 loss: -1.0534067153930664
Batch 44/64 loss: -0.8213119506835938
Batch 45/64 loss: -0.40868186950683594
Batch 46/64 loss: -0.7553462982177734
Batch 47/64 loss: -0.8085975646972656
Batch 48/64 loss: -0.9053831100463867
Batch 49/64 loss: -0.8174667358398438
Batch 50/64 loss: -0.5215063095092773
Batch 51/64 loss: -1.144850730895996
Batch 52/64 loss: -0.6256265640258789
Batch 53/64 loss: -1.0048513412475586
Batch 54/64 loss: -1.2526397705078125
Batch 55/64 loss: -0.9676542282104492
Batch 56/64 loss: -0.8079824447631836
Batch 57/64 loss: -0.6649885177612305
Batch 58/64 loss: -0.9608650207519531
Batch 59/64 loss: -0.9721403121948242
Batch 60/64 loss: -0.8566150665283203
Batch 61/64 loss: -0.862034797668457
Batch 62/64 loss: -1.0115890502929688
Batch 63/64 loss: -0.5580215454101562
Batch 64/64 loss: -5.250969886779785
Epoch 441  Train loss: -0.935955088746314  Val loss: -0.8843681886024082
Epoch 442
-------------------------------
Batch 1/64 loss: -1.1262235641479492
Batch 2/64 loss: -0.9163570404052734
Batch 3/64 loss: -0.9153966903686523
Batch 4/64 loss: -0.8475723266601562
Batch 5/64 loss: -0.5795745849609375
Batch 6/64 loss: -0.34989452362060547
Batch 7/64 loss: -0.9546546936035156
Batch 8/64 loss: -1.1760997772216797
Batch 9/64 loss: -1.1252918243408203
Batch 10/64 loss: -0.9999370574951172
Batch 11/64 loss: -0.8267154693603516
Batch 12/64 loss: -0.7525968551635742
Batch 13/64 loss: -0.9772109985351562
Batch 14/64 loss: -1.1292123794555664
Batch 15/64 loss: -1.1543607711791992
Batch 16/64 loss: -0.5143165588378906
Batch 17/64 loss: -0.3081493377685547
Batch 18/64 loss: -0.37071895599365234
Batch 19/64 loss: -1.035395622253418
Batch 20/64 loss: -0.9210376739501953
Batch 21/64 loss: -0.7380228042602539
Batch 22/64 loss: -0.8635702133178711
Batch 23/64 loss: -0.8698024749755859
Batch 24/64 loss: -1.0006952285766602
Batch 25/64 loss: -1.1411504745483398
Batch 26/64 loss: -0.9596691131591797
Batch 27/64 loss: 1.244497299194336
Batch 28/64 loss: -0.7725000381469727
Batch 29/64 loss: -1.1512870788574219
Batch 30/64 loss: -1.2480297088623047
Batch 31/64 loss: -0.650324821472168
Batch 32/64 loss: -0.9577093124389648
Batch 33/64 loss: -1.0162277221679688
Batch 34/64 loss: -0.6333866119384766
Batch 35/64 loss: -0.9164400100708008
Batch 36/64 loss: -1.0910091400146484
Batch 37/64 loss: -0.7867107391357422
Batch 38/64 loss: -0.7165441513061523
Batch 39/64 loss: -0.9066600799560547
Batch 40/64 loss: -1.1561603546142578
Batch 41/64 loss: -0.9464387893676758
Batch 42/64 loss: -1.1071796417236328
Batch 43/64 loss: -0.618870735168457
Batch 44/64 loss: -0.8174428939819336
Batch 45/64 loss: -0.9995012283325195
Batch 46/64 loss: -0.6545505523681641
Batch 47/64 loss: -0.9816207885742188
Batch 48/64 loss: -0.8003053665161133
Batch 49/64 loss: -1.236781120300293
Batch 50/64 loss: -0.9810237884521484
Batch 51/64 loss: -0.8157548904418945
Batch 52/64 loss: -1.1271371841430664
Batch 53/64 loss: -0.8055543899536133
Batch 54/64 loss: -0.9394645690917969
Batch 55/64 loss: -0.5476284027099609
Batch 56/64 loss: -1.308842658996582
Batch 57/64 loss: -1.1461410522460938
Batch 58/64 loss: -1.0065488815307617
Batch 59/64 loss: -0.7709417343139648
Batch 60/64 loss: -1.3187456130981445
Batch 61/64 loss: -0.963709831237793
Batch 62/64 loss: -0.6255130767822266
Batch 63/64 loss: -0.7646036148071289
Batch 64/64 loss: -4.181629180908203
Epoch 442  Train loss: -0.9056100733139936  Val loss: -0.8498429105044231
Epoch 443
-------------------------------
Batch 1/64 loss: -0.48804283142089844
Batch 2/64 loss: -0.6538419723510742
Batch 3/64 loss: -0.7288084030151367
Batch 4/64 loss: -1.0947809219360352
Batch 5/64 loss: -1.0432014465332031
Batch 6/64 loss: -0.9488668441772461
Batch 7/64 loss: -1.1027469635009766
Batch 8/64 loss: -0.8376083374023438
Batch 9/64 loss: -0.9079179763793945
Batch 10/64 loss: -0.7736530303955078
Batch 11/64 loss: -0.9064922332763672
Batch 12/64 loss: -1.1265897750854492
Batch 13/64 loss: -0.9800510406494141
Batch 14/64 loss: -0.9977140426635742
Batch 15/64 loss: -1.1211004257202148
Batch 16/64 loss: -1.039337158203125
Batch 17/64 loss: -0.8846721649169922
Batch 18/64 loss: -1.075819969177246
Batch 19/64 loss: -1.0540094375610352
Batch 20/64 loss: -1.1658639907836914
Batch 21/64 loss: -0.9125146865844727
Batch 22/64 loss: -0.9544563293457031
Batch 23/64 loss: -0.8052511215209961
Batch 24/64 loss: -0.3506479263305664
Batch 25/64 loss: -1.126373291015625
Batch 26/64 loss: -0.5829343795776367
Batch 27/64 loss: -0.9534826278686523
Batch 28/64 loss: -0.8939599990844727
Batch 29/64 loss: -0.8930635452270508
Batch 30/64 loss: -0.835296630859375
Batch 31/64 loss: -1.1893682479858398
Batch 32/64 loss: -1.0148334503173828
Batch 33/64 loss: -0.8155450820922852
Batch 34/64 loss: -1.2001285552978516
Batch 35/64 loss: -1.2312641143798828
Batch 36/64 loss: -1.025533676147461
Batch 37/64 loss: -0.6916189193725586
Batch 38/64 loss: -0.5696620941162109
Batch 39/64 loss: -1.0795402526855469
Batch 40/64 loss: -0.7093524932861328
Batch 41/64 loss: -0.8709697723388672
Batch 42/64 loss: -0.8068218231201172
Batch 43/64 loss: -0.7607479095458984
Batch 44/64 loss: -0.6677093505859375
Batch 45/64 loss: -0.9954500198364258
Batch 46/64 loss: -0.6028175354003906
Batch 47/64 loss: -1.081751823425293
Batch 48/64 loss: -0.5848236083984375
Batch 49/64 loss: -0.9354724884033203
Batch 50/64 loss: -0.9629087448120117
Batch 51/64 loss: -0.6705074310302734
Batch 52/64 loss: -0.8926315307617188
Batch 53/64 loss: -1.210158348083496
Batch 54/64 loss: -0.942744255065918
Batch 55/64 loss: -0.4897470474243164
Batch 56/64 loss: -1.3501157760620117
Batch 57/64 loss: -0.6514501571655273
Batch 58/64 loss: -1.3146295547485352
Batch 59/64 loss: -1.0561389923095703
Batch 60/64 loss: -1.1366767883300781
Batch 61/64 loss: -0.8306760787963867
Batch 62/64 loss: -1.1319799423217773
Batch 63/64 loss: -1.1139240264892578
Batch 64/64 loss: -5.0394368171691895
Epoch 443  Train loss: -0.9663745412639543  Val loss: -0.926413532794546
Saving best model, epoch: 443
Epoch 444
-------------------------------
Batch 1/64 loss: -1.072432518005371
Batch 2/64 loss: -1.1717729568481445
Batch 3/64 loss: -0.9146642684936523
Batch 4/64 loss: -0.7516880035400391
Batch 5/64 loss: -0.8250236511230469
Batch 6/64 loss: -1.036910057067871
Batch 7/64 loss: -1.0613155364990234
Batch 8/64 loss: -1.049555778503418
Batch 9/64 loss: -1.036703109741211
Batch 10/64 loss: -1.0106277465820312
Batch 11/64 loss: -0.9779443740844727
Batch 12/64 loss: -0.9917364120483398
Batch 13/64 loss: -0.8382225036621094
Batch 14/64 loss: -0.7245655059814453
Batch 15/64 loss: -0.9554595947265625
Batch 16/64 loss: -0.7463474273681641
Batch 17/64 loss: -1.2791423797607422
Batch 18/64 loss: -1.060194969177246
Batch 19/64 loss: -1.2101669311523438
Batch 20/64 loss: -1.1825284957885742
Batch 21/64 loss: -0.7959623336791992
Batch 22/64 loss: -0.32065391540527344
Batch 23/64 loss: -0.8587179183959961
Batch 24/64 loss: -0.9355173110961914
Batch 25/64 loss: -1.0290565490722656
Batch 26/64 loss: -0.6676883697509766
Batch 27/64 loss: -0.6660251617431641
Batch 28/64 loss: -0.8291606903076172
Batch 29/64 loss: -1.1161136627197266
Batch 30/64 loss: -0.8976736068725586
Batch 31/64 loss: -0.6122236251831055
Batch 32/64 loss: -1.00579833984375
Batch 33/64 loss: -1.2980356216430664
Batch 34/64 loss: -0.9890460968017578
Batch 35/64 loss: -0.8072443008422852
Batch 36/64 loss: -0.6430845260620117
Batch 37/64 loss: -0.8215761184692383
Batch 38/64 loss: -1.0184974670410156
Batch 39/64 loss: -1.1664724349975586
Batch 40/64 loss: -0.9363203048706055
Batch 41/64 loss: -0.9291152954101562
Batch 42/64 loss: -0.9806795120239258
Batch 43/64 loss: -0.9627504348754883
Batch 44/64 loss: -1.1750202178955078
Batch 45/64 loss: -1.261277198791504
Batch 46/64 loss: -1.3302392959594727
Batch 47/64 loss: -0.9636516571044922
Batch 48/64 loss: -0.9832582473754883
Batch 49/64 loss: -0.6464605331420898
Batch 50/64 loss: -0.8578081130981445
Batch 51/64 loss: -1.2427244186401367
Batch 52/64 loss: -1.0217742919921875
Batch 53/64 loss: -0.831721305847168
Batch 54/64 loss: -0.8856039047241211
Batch 55/64 loss: -1.321223258972168
Batch 56/64 loss: -0.7082462310791016
Batch 57/64 loss: -1.1072940826416016
Batch 58/64 loss: -0.7096157073974609
Batch 59/64 loss: -0.9911975860595703
Batch 60/64 loss: -0.5131387710571289
Batch 61/64 loss: -0.9970121383666992
Batch 62/64 loss: -0.7379999160766602
Batch 63/64 loss: -0.8933906555175781
Batch 64/64 loss: -5.342736721038818
Epoch 444  Train loss: -0.9940411904278923  Val loss: -0.8913444309300164
Epoch 445
-------------------------------
Batch 1/64 loss: -0.8621950149536133
Batch 2/64 loss: -0.8416156768798828
Batch 3/64 loss: -1.1870508193969727
Batch 4/64 loss: -0.3252725601196289
Batch 5/64 loss: -1.327406883239746
Batch 6/64 loss: -0.9955768585205078
Batch 7/64 loss: -0.7412595748901367
Batch 8/64 loss: -0.9242610931396484
Batch 9/64 loss: -0.6560945510864258
Batch 10/64 loss: -0.6292896270751953
Batch 11/64 loss: -0.8045320510864258
Batch 12/64 loss: -0.25202274322509766
Batch 13/64 loss: -0.5754299163818359
Batch 14/64 loss: -0.8600692749023438
Batch 15/64 loss: -0.6480445861816406
Batch 16/64 loss: -1.0562419891357422
Batch 17/64 loss: -0.6863784790039062
Batch 18/64 loss: -0.5524082183837891
Batch 19/64 loss: -0.7533855438232422
Batch 20/64 loss: -1.1291046142578125
Batch 21/64 loss: -1.0080671310424805
Batch 22/64 loss: -0.7331905364990234
Batch 23/64 loss: -1.0702886581420898
Batch 24/64 loss: -1.258378028869629
Batch 25/64 loss: -0.9445858001708984
Batch 26/64 loss: -1.2953433990478516
Batch 27/64 loss: -1.0259027481079102
Batch 28/64 loss: -0.46146583557128906
Batch 29/64 loss: -1.0616254806518555
Batch 30/64 loss: -1.1933631896972656
Batch 31/64 loss: -1.1369571685791016
Batch 32/64 loss: -1.1165027618408203
Batch 33/64 loss: -0.6482038497924805
Batch 34/64 loss: -1.1247472763061523
Batch 35/64 loss: -0.7934455871582031
Batch 36/64 loss: -1.1391973495483398
Batch 37/64 loss: -1.0696706771850586
Batch 38/64 loss: -0.6932172775268555
Batch 39/64 loss: -0.8765859603881836
Batch 40/64 loss: -1.092930793762207
Batch 41/64 loss: -1.1003799438476562
Batch 42/64 loss: -1.1652584075927734
Batch 43/64 loss: -0.9872655868530273
Batch 44/64 loss: -1.2645025253295898
Batch 45/64 loss: -0.9918508529663086
Batch 46/64 loss: -1.0905570983886719
Batch 47/64 loss: -1.0985965728759766
Batch 48/64 loss: -0.5090608596801758
Batch 49/64 loss: -0.9789905548095703
Batch 50/64 loss: -1.0108165740966797
Batch 51/64 loss: -1.006988525390625
Batch 52/64 loss: -1.07403564453125
Batch 53/64 loss: -0.6764411926269531
Batch 54/64 loss: -0.8575668334960938
Batch 55/64 loss: -0.9808340072631836
Batch 56/64 loss: -0.49468231201171875
Batch 57/64 loss: -0.9673728942871094
Batch 58/64 loss: -0.6691446304321289
Batch 59/64 loss: -1.1796159744262695
Batch 60/64 loss: -1.1302509307861328
Batch 61/64 loss: -1.2794294357299805
Batch 62/64 loss: -1.0250616073608398
Batch 63/64 loss: -0.8046236038208008
Batch 64/64 loss: -5.291646480560303
Epoch 445  Train loss: -0.9704058198367849  Val loss: -0.8522950005285519
Epoch 446
-------------------------------
Batch 1/64 loss: -1.4039525985717773
Batch 2/64 loss: -0.8807153701782227
Batch 3/64 loss: -1.0812454223632812
Batch 4/64 loss: -1.1599493026733398
Batch 5/64 loss: -1.302973747253418
Batch 6/64 loss: -0.844172477722168
Batch 7/64 loss: -1.1140985488891602
Batch 8/64 loss: -0.752655029296875
Batch 9/64 loss: -0.6146278381347656
Batch 10/64 loss: -0.6402015686035156
Batch 11/64 loss: -1.13372802734375
Batch 12/64 loss: -0.7466259002685547
Batch 13/64 loss: -0.7649641036987305
Batch 14/64 loss: -0.6934185028076172
Batch 15/64 loss: -0.5612125396728516
Batch 16/64 loss: -0.9317893981933594
Batch 17/64 loss: -1.2458620071411133
Batch 18/64 loss: -1.1093559265136719
Batch 19/64 loss: -0.7314014434814453
Batch 20/64 loss: -0.9786710739135742
Batch 21/64 loss: -1.0034370422363281
Batch 22/64 loss: -0.7660245895385742
Batch 23/64 loss: -0.8809814453125
Batch 24/64 loss: -0.6785440444946289
Batch 25/64 loss: -0.922245979309082
Batch 26/64 loss: -0.9046554565429688
Batch 27/64 loss: -0.5524969100952148
Batch 28/64 loss: -0.9544744491577148
Batch 29/64 loss: -0.6658363342285156
Batch 30/64 loss: -0.8680934906005859
Batch 31/64 loss: -0.9814996719360352
Batch 32/64 loss: -1.1253623962402344
Batch 33/64 loss: -0.6701469421386719
Batch 34/64 loss: -1.0133867263793945
Batch 35/64 loss: -0.8031501770019531
Batch 36/64 loss: -1.0419158935546875
Batch 37/64 loss: -0.960484504699707
Batch 38/64 loss: -0.9703989028930664
Batch 39/64 loss: -1.2500333786010742
Batch 40/64 loss: -0.7675046920776367
Batch 41/64 loss: -0.9958467483520508
Batch 42/64 loss: -0.7345304489135742
Batch 43/64 loss: -0.6530609130859375
Batch 44/64 loss: -1.0645465850830078
Batch 45/64 loss: -1.2033958435058594
Batch 46/64 loss: -1.096151351928711
Batch 47/64 loss: -1.2267284393310547
Batch 48/64 loss: -0.994903564453125
Batch 49/64 loss: -0.9837751388549805
Batch 50/64 loss: -0.6506233215332031
Batch 51/64 loss: -0.8735389709472656
Batch 52/64 loss: -0.750279426574707
Batch 53/64 loss: -0.9077014923095703
Batch 54/64 loss: -0.7998714447021484
Batch 55/64 loss: -0.800114631652832
Batch 56/64 loss: -0.671238899230957
Batch 57/64 loss: -1.2432498931884766
Batch 58/64 loss: -0.6566381454467773
Batch 59/64 loss: -0.9631175994873047
Batch 60/64 loss: -1.1162290573120117
Batch 61/64 loss: -0.9346256256103516
Batch 62/64 loss: -1.205204963684082
Batch 63/64 loss: -0.8980636596679688
Batch 64/64 loss: -5.5092926025390625
Epoch 446  Train loss: -0.9729835211061963  Val loss: -0.8971348267650276
Epoch 447
-------------------------------
Batch 1/64 loss: -1.2097692489624023
Batch 2/64 loss: -1.0436182022094727
Batch 3/64 loss: -0.843562126159668
Batch 4/64 loss: -0.6183347702026367
Batch 5/64 loss: -0.8443126678466797
Batch 6/64 loss: -1.1760263442993164
Batch 7/64 loss: -0.9142417907714844
Batch 8/64 loss: -0.8893194198608398
Batch 9/64 loss: -0.5591802597045898
Batch 10/64 loss: -1.1883354187011719
Batch 11/64 loss: -0.7536678314208984
Batch 12/64 loss: -0.6980562210083008
Batch 13/64 loss: -1.0669116973876953
Batch 14/64 loss: -0.626194953918457
Batch 15/64 loss: -0.8727788925170898
Batch 16/64 loss: -1.187591552734375
Batch 17/64 loss: -0.4919919967651367
Batch 18/64 loss: -0.6041727066040039
Batch 19/64 loss: -0.8318185806274414
Batch 20/64 loss: -0.6374835968017578
Batch 21/64 loss: -1.0895023345947266
Batch 22/64 loss: -1.1335229873657227
Batch 23/64 loss: -0.8837642669677734
Batch 24/64 loss: -0.7602033615112305
Batch 25/64 loss: -1.168283462524414
Batch 26/64 loss: -0.7946434020996094
Batch 27/64 loss: -0.9837408065795898
Batch 28/64 loss: -0.7629880905151367
Batch 29/64 loss: -1.0334644317626953
Batch 30/64 loss: -0.6948652267456055
Batch 31/64 loss: -0.9230947494506836
Batch 32/64 loss: -1.256082534790039
Batch 33/64 loss: -0.9608945846557617
Batch 34/64 loss: -1.2714929580688477
Batch 35/64 loss: -1.2598676681518555
Batch 36/64 loss: -1.216263771057129
Batch 37/64 loss: -0.8869180679321289
Batch 38/64 loss: -0.8134632110595703
Batch 39/64 loss: -1.0134525299072266
Batch 40/64 loss: -0.8839941024780273
Batch 41/64 loss: -0.9193458557128906
Batch 42/64 loss: -1.1751470565795898
Batch 43/64 loss: -0.8709993362426758
Batch 44/64 loss: -0.5969409942626953
Batch 45/64 loss: -0.8582496643066406
Batch 46/64 loss: -1.1369457244873047
Batch 47/64 loss: -1.0849523544311523
Batch 48/64 loss: -0.36368846893310547
Batch 49/64 loss: -0.9524240493774414
Batch 50/64 loss: -0.25957632064819336
Batch 51/64 loss: -0.9885234832763672
Batch 52/64 loss: -0.7547845840454102
Batch 53/64 loss: -0.7738161087036133
Batch 54/64 loss: -0.929255485534668
Batch 55/64 loss: -0.7762031555175781
Batch 56/64 loss: -0.7533473968505859
Batch 57/64 loss: -0.9618635177612305
Batch 58/64 loss: -1.0696401596069336
Batch 59/64 loss: -0.976933479309082
Batch 60/64 loss: -0.5852108001708984
Batch 61/64 loss: -0.9038486480712891
Batch 62/64 loss: -0.7848005294799805
Batch 63/64 loss: -0.673182487487793
Batch 64/64 loss: -4.637351989746094
Epoch 447  Train loss: -0.9329500310561236  Val loss: -0.761964909399498
Epoch 448
-------------------------------
Batch 1/64 loss: -0.6599750518798828
Batch 2/64 loss: -0.8391075134277344
Batch 3/64 loss: -0.7274894714355469
Batch 4/64 loss: -0.7746067047119141
Batch 5/64 loss: -0.44556570053100586
Batch 6/64 loss: -0.6150245666503906
Batch 7/64 loss: -0.9144372940063477
Batch 8/64 loss: -1.2463064193725586
Batch 9/64 loss: -0.9871673583984375
Batch 10/64 loss: -0.9463510513305664
Batch 11/64 loss: -0.21677684783935547
Batch 12/64 loss: -0.9178743362426758
Batch 13/64 loss: -0.5025234222412109
Batch 14/64 loss: -0.6997766494750977
Batch 15/64 loss: -1.1127405166625977
Batch 16/64 loss: -0.9165983200073242
Batch 17/64 loss: -1.140817642211914
Batch 18/64 loss: -1.1198434829711914
Batch 19/64 loss: -0.5030088424682617
Batch 20/64 loss: -0.38425731658935547
Batch 21/64 loss: -0.8388576507568359
Batch 22/64 loss: -0.892878532409668
Batch 23/64 loss: -0.8358917236328125
Batch 24/64 loss: -0.8386316299438477
Batch 25/64 loss: -0.6791496276855469
Batch 26/64 loss: -0.49806976318359375
Batch 27/64 loss: -1.186558723449707
Batch 28/64 loss: -0.7713088989257812
Batch 29/64 loss: -1.1607170104980469
Batch 30/64 loss: -1.008183479309082
Batch 31/64 loss: -1.2268247604370117
Batch 32/64 loss: 1.1439590454101562
Batch 33/64 loss: -1.0677356719970703
Batch 34/64 loss: -0.6915740966796875
Batch 35/64 loss: -0.9444093704223633
Batch 36/64 loss: -0.7378253936767578
Batch 37/64 loss: -0.8226261138916016
Batch 38/64 loss: -0.5089173316955566
Batch 39/64 loss: -1.0222949981689453
Batch 40/64 loss: -1.0546445846557617
Batch 41/64 loss: -0.1557483673095703
Batch 42/64 loss: -0.5785789489746094
Batch 43/64 loss: -0.5154504776000977
Batch 44/64 loss: -0.45076847076416016
Batch 45/64 loss: -0.8604974746704102
Batch 46/64 loss: -0.7745075225830078
Batch 47/64 loss: -0.5035314559936523
Batch 48/64 loss: -0.7023458480834961
Batch 49/64 loss: -0.36090898513793945
Batch 50/64 loss: -0.8006687164306641
Batch 51/64 loss: -0.9542140960693359
Batch 52/64 loss: -1.078317642211914
Batch 53/64 loss: -0.9027919769287109
Batch 54/64 loss: -0.7957572937011719
Batch 55/64 loss: -0.49152326583862305
Batch 56/64 loss: -0.8065576553344727
Batch 57/64 loss: -0.8418159484863281
Batch 58/64 loss: -0.8969402313232422
Batch 59/64 loss: -0.731292724609375
Batch 60/64 loss: -0.9402179718017578
Batch 61/64 loss: -0.8812122344970703
Batch 62/64 loss: -1.0685062408447266
Batch 63/64 loss: -0.8242368698120117
Batch 64/64 loss: -5.203597068786621
Epoch 448  Train loss: -0.8177643458048502  Val loss: -0.6648106001496724
Epoch 449
-------------------------------
Batch 1/64 loss: -0.7942266464233398
Batch 2/64 loss: -0.9225597381591797
Batch 3/64 loss: -0.8562870025634766
Batch 4/64 loss: -0.7594022750854492
Batch 5/64 loss: -0.6153736114501953
Batch 6/64 loss: -0.8520345687866211
Batch 7/64 loss: -0.8026895523071289
Batch 8/64 loss: -0.9462375640869141
Batch 9/64 loss: -0.723383903503418
Batch 10/64 loss: -0.7409524917602539
Batch 11/64 loss: -0.5907449722290039
Batch 12/64 loss: -0.9137058258056641
Batch 13/64 loss: -1.1691503524780273
Batch 14/64 loss: -1.1777687072753906
Batch 15/64 loss: -0.7930850982666016
Batch 16/64 loss: -0.7284069061279297
Batch 17/64 loss: -0.9556369781494141
Batch 18/64 loss: -0.8372898101806641
Batch 19/64 loss: -0.8946151733398438
Batch 20/64 loss: -0.5729131698608398
Batch 21/64 loss: -0.7084121704101562
Batch 22/64 loss: -0.6266250610351562
Batch 23/64 loss: -0.48901891708374023
Batch 24/64 loss: -0.9456472396850586
Batch 25/64 loss: -0.7416372299194336
Batch 26/64 loss: -0.8451833724975586
Batch 27/64 loss: -0.9631862640380859
Batch 28/64 loss: -1.0261211395263672
Batch 29/64 loss: -1.0414199829101562
Batch 30/64 loss: -0.5320911407470703
Batch 31/64 loss: -0.5136308670043945
Batch 32/64 loss: -1.0854434967041016
Batch 33/64 loss: -1.1380681991577148
Batch 34/64 loss: -0.7440776824951172
Batch 35/64 loss: -1.0469818115234375
Batch 36/64 loss: -0.6464643478393555
Batch 37/64 loss: -0.7227039337158203
Batch 38/64 loss: -1.216893196105957
Batch 39/64 loss: -0.32347965240478516
Batch 40/64 loss: -0.7250995635986328
Batch 41/64 loss: -0.6882314682006836
Batch 42/64 loss: -0.8733015060424805
Batch 43/64 loss: -0.6069164276123047
Batch 44/64 loss: -0.9006519317626953
Batch 45/64 loss: -0.549309253692627
Batch 46/64 loss: -0.42511844635009766
Batch 47/64 loss: -0.7012186050415039
Batch 48/64 loss: -0.6166553497314453
Batch 49/64 loss: -0.9078502655029297
Batch 50/64 loss: -0.8400259017944336
Batch 51/64 loss: -0.6187992095947266
Batch 52/64 loss: -0.9559040069580078
Batch 53/64 loss: -0.7065019607543945
Batch 54/64 loss: -0.7486429214477539
Batch 55/64 loss: -0.9019374847412109
Batch 56/64 loss: -0.28391456604003906
Batch 57/64 loss: -0.8135623931884766
Batch 58/64 loss: -0.9035348892211914
Batch 59/64 loss: -0.9087839126586914
Batch 60/64 loss: -0.8867950439453125
Batch 61/64 loss: -1.137312889099121
Batch 62/64 loss: -0.6371612548828125
Batch 63/64 loss: -0.8955564498901367
Batch 64/64 loss: -5.135973930358887
Epoch 449  Train loss: -0.8484441757202148  Val loss: -0.7143732182348717
Epoch 450
-------------------------------
Batch 1/64 loss: -0.9978790283203125
Batch 2/64 loss: -0.9181108474731445
Batch 3/64 loss: -0.9638862609863281
Batch 4/64 loss: -0.9424381256103516
Batch 5/64 loss: -0.43775081634521484
Batch 6/64 loss: -1.0368003845214844
Batch 7/64 loss: -1.0248775482177734
Batch 8/64 loss: -0.5423765182495117
Batch 9/64 loss: -1.0534038543701172
Batch 10/64 loss: -0.37694835662841797
Batch 11/64 loss: -0.6887397766113281
Batch 12/64 loss: -0.9065446853637695
Batch 13/64 loss: -0.8165655136108398
Batch 14/64 loss: -1.1857719421386719
Batch 15/64 loss: -0.900822639465332
Batch 16/64 loss: -0.9646825790405273
Batch 17/64 loss: -0.9848966598510742
Batch 18/64 loss: -0.8809099197387695
Batch 19/64 loss: -1.0892066955566406
Batch 20/64 loss: -1.097914695739746
Batch 21/64 loss: -0.9563760757446289
Batch 22/64 loss: -0.7007293701171875
Batch 23/64 loss: -1.3036975860595703
Batch 24/64 loss: -1.0757417678833008
Batch 25/64 loss: -0.4783458709716797
Batch 26/64 loss: -0.9364376068115234
Batch 27/64 loss: -0.6169404983520508
Batch 28/64 loss: -1.053025245666504
Batch 29/64 loss: -0.5117921829223633
Batch 30/64 loss: -0.7248830795288086
Batch 31/64 loss: -0.9409894943237305
Batch 32/64 loss: -1.194075584411621
Batch 33/64 loss: -1.0348224639892578
Batch 34/64 loss: -1.2055139541625977
Batch 35/64 loss: -0.46753978729248047
Batch 36/64 loss: -0.9947319030761719
Batch 37/64 loss: -0.7420787811279297
Batch 38/64 loss: -0.8037557601928711
Batch 39/64 loss: -0.8563251495361328
Batch 40/64 loss: -0.7792530059814453
Batch 41/64 loss: -1.063720703125
Batch 42/64 loss: -0.985804557800293
Batch 43/64 loss: -0.5354547500610352
Batch 44/64 loss: -0.7879934310913086
Batch 45/64 loss: -1.2477989196777344
Batch 46/64 loss: -1.1475210189819336
Batch 47/64 loss: -1.1354942321777344
Batch 48/64 loss: -1.042673110961914
Batch 49/64 loss: -1.265462875366211
Batch 50/64 loss: -0.6274328231811523
Batch 51/64 loss: -0.7273998260498047
Batch 52/64 loss: -0.9582071304321289
Batch 53/64 loss: -1.331181526184082
Batch 54/64 loss: -0.9143285751342773
Batch 55/64 loss: -0.720184326171875
Batch 56/64 loss: -0.9857673645019531
Batch 57/64 loss: -1.0287485122680664
Batch 58/64 loss: -1.1884441375732422
Batch 59/64 loss: -0.45281314849853516
Batch 60/64 loss: -0.9309682846069336
Batch 61/64 loss: -0.9456987380981445
Batch 62/64 loss: -0.9837799072265625
Batch 63/64 loss: -1.2193803787231445
Batch 64/64 loss: -4.624939918518066
Epoch 450  Train loss: -0.9550203173768287  Val loss: -0.842442502680513
Epoch 451
-------------------------------
Batch 1/64 loss: -1.2396411895751953
Batch 2/64 loss: -0.9036359786987305
Batch 3/64 loss: -0.4371662139892578
Batch 4/64 loss: -1.1035175323486328
Batch 5/64 loss: -1.1591272354125977
Batch 6/64 loss: -1.049062728881836
Batch 7/64 loss: -1.2336654663085938
Batch 8/64 loss: -0.8098745346069336
Batch 9/64 loss: -1.1307172775268555
Batch 10/64 loss: -1.556863784790039
Batch 11/64 loss: -0.4640045166015625
Batch 12/64 loss: -0.8866796493530273
Batch 13/64 loss: -0.9969930648803711
Batch 14/64 loss: -1.0671777725219727
Batch 15/64 loss: -0.9756278991699219
Batch 16/64 loss: -0.9169435501098633
Batch 17/64 loss: -1.0697154998779297
Batch 18/64 loss: -0.7524042129516602
Batch 19/64 loss: -1.0020580291748047
Batch 20/64 loss: -1.1127824783325195
Batch 21/64 loss: -1.2566003799438477
Batch 22/64 loss: -1.1821775436401367
Batch 23/64 loss: -1.2515697479248047
Batch 24/64 loss: -0.8163604736328125
Batch 25/64 loss: -0.6372194290161133
Batch 26/64 loss: -0.973088264465332
Batch 27/64 loss: -1.0366830825805664
Batch 28/64 loss: -0.6415672302246094
Batch 29/64 loss: -0.7817201614379883
Batch 30/64 loss: -0.971461296081543
Batch 31/64 loss: -0.7229461669921875
Batch 32/64 loss: -0.9438314437866211
Batch 33/64 loss: -0.7023115158081055
Batch 34/64 loss: -0.9394569396972656
Batch 35/64 loss: -1.1288337707519531
Batch 36/64 loss: -0.6266899108886719
Batch 37/64 loss: -1.163381576538086
Batch 38/64 loss: -0.6322507858276367
Batch 39/64 loss: -0.6688299179077148
Batch 40/64 loss: -0.7710905075073242
Batch 41/64 loss: -1.084620475769043
Batch 42/64 loss: -1.027094841003418
Batch 43/64 loss: -1.1152677536010742
Batch 44/64 loss: -0.5548601150512695
Batch 45/64 loss: -0.5533885955810547
Batch 46/64 loss: -0.6408729553222656
Batch 47/64 loss: -0.6563272476196289
Batch 48/64 loss: 0.0729365348815918
Batch 49/64 loss: -0.03134632110595703
Batch 50/64 loss: -0.31356239318847656
Batch 51/64 loss: -0.6663589477539062
Batch 52/64 loss: -0.7980661392211914
Batch 53/64 loss: -0.8050069808959961
Batch 54/64 loss: -0.4189491271972656
Batch 55/64 loss: -0.2019672393798828
Batch 56/64 loss: -0.07744503021240234
Batch 57/64 loss: -0.4741525650024414
Batch 58/64 loss: -0.6836729049682617
Batch 59/64 loss: -0.8475742340087891
Batch 60/64 loss: -0.7535772323608398
Batch 61/64 loss: -0.8652124404907227
Batch 62/64 loss: -0.6443548202514648
Batch 63/64 loss: -0.5965061187744141
Batch 64/64 loss: -4.273710250854492
Epoch 451  Train loss: -0.8573844610476027  Val loss: -0.39643041866341816
Epoch 452
-------------------------------
Batch 1/64 loss: -0.4956989288330078
Batch 2/64 loss: -0.5006046295166016
Batch 3/64 loss: -0.5497007369995117
Batch 4/64 loss: -0.8153352737426758
Batch 5/64 loss: -0.7814607620239258
Batch 6/64 loss: -0.8309764862060547
Batch 7/64 loss: -0.4730720520019531
Batch 8/64 loss: -0.8697061538696289
Batch 9/64 loss: -0.6666765213012695
Batch 10/64 loss: -1.144474983215332
Batch 11/64 loss: -0.5803813934326172
Batch 12/64 loss: -0.48810386657714844
Batch 13/64 loss: -0.5200014114379883
Batch 14/64 loss: -0.6532983779907227
Batch 15/64 loss: -0.9360675811767578
Batch 16/64 loss: -0.6002025604248047
Batch 17/64 loss: -0.6841764450073242
Batch 18/64 loss: -0.47893619537353516
Batch 19/64 loss: 0.026134490966796875
Batch 20/64 loss: -0.7319660186767578
Batch 21/64 loss: -0.46692752838134766
Batch 22/64 loss: -0.9398612976074219
Batch 23/64 loss: -0.9932565689086914
Batch 24/64 loss: -0.8551387786865234
Batch 25/64 loss: -0.44057464599609375
Batch 26/64 loss: -0.9947195053100586
Batch 27/64 loss: -0.8133401870727539
Batch 28/64 loss: -0.8824291229248047
Batch 29/64 loss: -0.7448406219482422
Batch 30/64 loss: -0.7837495803833008
Batch 31/64 loss: -0.546107292175293
Batch 32/64 loss: -0.9709653854370117
Batch 33/64 loss: -0.834019660949707
Batch 34/64 loss: -0.7875747680664062
Batch 35/64 loss: -0.7926912307739258
Batch 36/64 loss: -0.7179574966430664
Batch 37/64 loss: -1.053278923034668
Batch 38/64 loss: -0.7116260528564453
Batch 39/64 loss: -0.5929231643676758
Batch 40/64 loss: -0.8916921615600586
Batch 41/64 loss: -0.8118762969970703
Batch 42/64 loss: -0.8468093872070312
Batch 43/64 loss: -0.8042802810668945
Batch 44/64 loss: -1.1219816207885742
Batch 45/64 loss: -1.2599306106567383
Batch 46/64 loss: -0.5858421325683594
Batch 47/64 loss: -0.6201944351196289
Batch 48/64 loss: -1.0942153930664062
Batch 49/64 loss: 0.6744403839111328
Batch 50/64 loss: -0.9987306594848633
Batch 51/64 loss: -0.3514995574951172
Batch 52/64 loss: -0.8627443313598633
Batch 53/64 loss: -0.7990436553955078
Batch 54/64 loss: -0.4856281280517578
Batch 55/64 loss: -0.18621826171875
Batch 56/64 loss: -0.6129875183105469
Batch 57/64 loss: -0.26431989669799805
Batch 58/64 loss: -0.16342592239379883
Batch 59/64 loss: -0.3009986877441406
Batch 60/64 loss: -0.2953500747680664
Batch 61/64 loss: -0.5250349044799805
Batch 62/64 loss: -0.03262186050415039
Batch 63/64 loss: -0.28438282012939453
Batch 64/64 loss: -3.8787174224853516
Epoch 452  Train loss: -0.6922524545706955  Val loss: -0.040347699037532214
Epoch 453
-------------------------------
Batch 1/64 loss: -0.39632558822631836
Batch 2/64 loss: -0.5415902137756348
Batch 3/64 loss: -0.07095718383789062
Batch 4/64 loss: -0.17999839782714844
Batch 5/64 loss: -0.5320043563842773
Batch 6/64 loss: -0.6803121566772461
Batch 7/64 loss: 0.29239320755004883
Batch 8/64 loss: -0.17627191543579102
Batch 9/64 loss: -0.13232421875
Batch 10/64 loss: -0.7811670303344727
Batch 11/64 loss: -0.10231161117553711
Batch 12/64 loss: -0.5777311325073242
Batch 13/64 loss: -0.30608654022216797
Batch 14/64 loss: -0.5952434539794922
Batch 15/64 loss: -0.5474987030029297
Batch 16/64 loss: -0.5817079544067383
Batch 17/64 loss: -1.0837764739990234
Batch 18/64 loss: -0.7033853530883789
Batch 19/64 loss: -0.39101600646972656
Batch 20/64 loss: -0.6170368194580078
Batch 21/64 loss: -0.6222209930419922
Batch 22/64 loss: -0.8109502792358398
Batch 23/64 loss: -0.5956535339355469
Batch 24/64 loss: -0.7927703857421875
Batch 25/64 loss: -0.5970258712768555
Batch 26/64 loss: -0.6320991516113281
Batch 27/64 loss: -0.9145956039428711
Batch 28/64 loss: -1.066976547241211
Batch 29/64 loss: -1.1114721298217773
Batch 30/64 loss: -0.6111955642700195
Batch 31/64 loss: -0.7446699142456055
Batch 32/64 loss: -0.9584293365478516
Batch 33/64 loss: -0.7794637680053711
Batch 34/64 loss: -0.42520713806152344
Batch 35/64 loss: -0.5690784454345703
Batch 36/64 loss: -0.9559946060180664
Batch 37/64 loss: -0.8684911727905273
Batch 38/64 loss: -0.8175582885742188
Batch 39/64 loss: -0.3378715515136719
Batch 40/64 loss: -0.5770778656005859
Batch 41/64 loss: -0.3433690071105957
Batch 42/64 loss: -0.4772043228149414
Batch 43/64 loss: -0.6269264221191406
Batch 44/64 loss: -1.0640811920166016
Batch 45/64 loss: -0.2250518798828125
Batch 46/64 loss: -0.44161033630371094
Batch 47/64 loss: -0.531376838684082
Batch 48/64 loss: -0.6252412796020508
Batch 49/64 loss: -0.916447639465332
Batch 50/64 loss: -0.7035751342773438
Batch 51/64 loss: -0.9330387115478516
Batch 52/64 loss: -0.8139495849609375
Batch 53/64 loss: -0.9565095901489258
Batch 54/64 loss: -0.9905519485473633
Batch 55/64 loss: -0.7467098236083984
Batch 56/64 loss: -0.751708984375
Batch 57/64 loss: -0.7365188598632812
Batch 58/64 loss: -0.8536968231201172
Batch 59/64 loss: -0.7439498901367188
Batch 60/64 loss: -1.0484390258789062
Batch 61/64 loss: -0.5672445297241211
Batch 62/64 loss: -1.2379789352416992
Batch 63/64 loss: -0.5807094573974609
Batch 64/64 loss: -5.239883899688721
Epoch 453  Train loss: -0.695513053968841  Val loss: -0.7389422413409781
Epoch 454
-------------------------------
Batch 1/64 loss: -0.7880277633666992
Batch 2/64 loss: -1.083756446838379
Batch 3/64 loss: -1.04931640625
Batch 4/64 loss: -0.9473781585693359
Batch 5/64 loss: -0.7864170074462891
Batch 6/64 loss: -0.47760868072509766
Batch 7/64 loss: -0.22051334381103516
Batch 8/64 loss: -0.15102672576904297
Batch 9/64 loss: -0.6961488723754883
Batch 10/64 loss: -0.6603584289550781
Batch 11/64 loss: -1.0830917358398438
Batch 12/64 loss: -1.0550708770751953
Batch 13/64 loss: -0.9754753112792969
Batch 14/64 loss: -0.6195201873779297
Batch 15/64 loss: -0.6793222427368164
Batch 16/64 loss: -0.8081903457641602
Batch 17/64 loss: -1.0027036666870117
Batch 18/64 loss: -1.0157623291015625
Batch 19/64 loss: -0.9886474609375
Batch 20/64 loss: -1.1110458374023438
Batch 21/64 loss: -0.9923791885375977
Batch 22/64 loss: -1.1206302642822266
Batch 23/64 loss: -0.478546142578125
Batch 24/64 loss: -1.0587053298950195
Batch 25/64 loss: -0.7930078506469727
Batch 26/64 loss: -0.986445426940918
Batch 27/64 loss: -0.36270904541015625
Batch 28/64 loss: -0.8617229461669922
Batch 29/64 loss: -0.3755960464477539
Batch 30/64 loss: -0.9812755584716797
Batch 31/64 loss: -0.73193359375
Batch 32/64 loss: -0.5237493515014648
Batch 33/64 loss: -0.735539436340332
Batch 34/64 loss: -1.0285615921020508
Batch 35/64 loss: -1.2337846755981445
Batch 36/64 loss: -0.6919012069702148
Batch 37/64 loss: -0.714569091796875
Batch 38/64 loss: -0.7926130294799805
Batch 39/64 loss: -0.7789697647094727
Batch 40/64 loss: -0.8100214004516602
Batch 41/64 loss: -0.42002391815185547
Batch 42/64 loss: -1.1309738159179688
Batch 43/64 loss: -0.6693592071533203
Batch 44/64 loss: -0.7922115325927734
Batch 45/64 loss: -0.8327417373657227
Batch 46/64 loss: -0.7767715454101562
Batch 47/64 loss: -0.8224163055419922
Batch 48/64 loss: -0.9695444107055664
Batch 49/64 loss: -0.6224269866943359
Batch 50/64 loss: -1.0418920516967773
Batch 51/64 loss: -0.7356195449829102
Batch 52/64 loss: -1.0532035827636719
Batch 53/64 loss: -0.6713104248046875
Batch 54/64 loss: -0.2756385803222656
Batch 55/64 loss: -0.7458705902099609
Batch 56/64 loss: -0.866455078125
Batch 57/64 loss: -0.8228616714477539
Batch 58/64 loss: -0.6481609344482422
Batch 59/64 loss: -0.8294649124145508
Batch 60/64 loss: -0.9126977920532227
Batch 61/64 loss: -0.8998022079467773
Batch 62/64 loss: -0.6278886795043945
Batch 63/64 loss: -1.222559928894043
Batch 64/64 loss: -5.142915725708008
Epoch 454  Train loss: -0.8548882353539561  Val loss: -0.7529200655488214
Epoch 455
-------------------------------
Batch 1/64 loss: -0.9427633285522461
Batch 2/64 loss: -1.0279178619384766
Batch 3/64 loss: -0.900792121887207
Batch 4/64 loss: -0.7777471542358398
Batch 5/64 loss: -0.9827089309692383
Batch 6/64 loss: -0.6522216796875
Batch 7/64 loss: -0.7652502059936523
Batch 8/64 loss: -0.7733335494995117
Batch 9/64 loss: -0.6686830520629883
Batch 10/64 loss: -0.2963600158691406
Batch 11/64 loss: -0.6839637756347656
Batch 12/64 loss: -0.5303964614868164
Batch 13/64 loss: -0.7630691528320312
Batch 14/64 loss: -1.066208839416504
Batch 15/64 loss: -1.037562370300293
Batch 16/64 loss: -0.6706027984619141
Batch 17/64 loss: -0.4174785614013672
Batch 18/64 loss: -0.8134765625
Batch 19/64 loss: -0.9477643966674805
Batch 20/64 loss: -0.7209644317626953
Batch 21/64 loss: -1.0788202285766602
Batch 22/64 loss: -0.7976198196411133
Batch 23/64 loss: -1.2619619369506836
Batch 24/64 loss: -0.8262977600097656
Batch 25/64 loss: -0.807154655456543
Batch 26/64 loss: -0.7839260101318359
Batch 27/64 loss: -1.0961999893188477
Batch 28/64 loss: -0.983729362487793
Batch 29/64 loss: -0.2468719482421875
Batch 30/64 loss: -0.7202987670898438
Batch 31/64 loss: -0.745941162109375
Batch 32/64 loss: -0.7424173355102539
Batch 33/64 loss: -0.9172697067260742
Batch 34/64 loss: -0.502659797668457
Batch 35/64 loss: -1.035292625427246
Batch 36/64 loss: -0.6186113357543945
Batch 37/64 loss: -1.1210718154907227
Batch 38/64 loss: -0.9130945205688477
Batch 39/64 loss: -0.7408838272094727
Batch 40/64 loss: -0.2802848815917969
Batch 41/64 loss: -1.2059497833251953
Batch 42/64 loss: -0.7536821365356445
Batch 43/64 loss: -0.8858051300048828
Batch 44/64 loss: -0.9832229614257812
Batch 45/64 loss: -0.8721065521240234
Batch 46/64 loss: -1.3341236114501953
Batch 47/64 loss: -1.097132682800293
Batch 48/64 loss: -0.7738218307495117
Batch 49/64 loss: -1.2030105590820312
Batch 50/64 loss: -0.447784423828125
Batch 51/64 loss: -0.8407192230224609
Batch 52/64 loss: -0.8460865020751953
Batch 53/64 loss: -1.0788984298706055
Batch 54/64 loss: -1.0534801483154297
Batch 55/64 loss: -0.9825735092163086
Batch 56/64 loss: -0.9479122161865234
Batch 57/64 loss: -0.9124059677124023
Batch 58/64 loss: -0.594822883605957
Batch 59/64 loss: -1.062006950378418
Batch 60/64 loss: -0.9246406555175781
Batch 61/64 loss: -0.9121665954589844
Batch 62/64 loss: -0.5165557861328125
Batch 63/64 loss: -0.9859542846679688
Batch 64/64 loss: -4.525115966796875
Epoch 455  Train loss: -0.8826411378149893  Val loss: -0.8286974536594247
Epoch 456
-------------------------------
Batch 1/64 loss: -0.6224937438964844
Batch 2/64 loss: -0.9015665054321289
Batch 3/64 loss: -0.8720216751098633
Batch 4/64 loss: -0.8322696685791016
Batch 5/64 loss: -0.7334375381469727
Batch 6/64 loss: -0.9927225112915039
Batch 7/64 loss: -1.2182464599609375
Batch 8/64 loss: -1.102555274963379
Batch 9/64 loss: -0.9471807479858398
Batch 10/64 loss: -0.8673133850097656
Batch 11/64 loss: -0.8878545761108398
Batch 12/64 loss: -0.8290090560913086
Batch 13/64 loss: -0.7187671661376953
Batch 14/64 loss: -1.0390682220458984
Batch 15/64 loss: -0.8778295516967773
Batch 16/64 loss: -0.7153749465942383
Batch 17/64 loss: -0.6280717849731445
Batch 18/64 loss: -0.8515701293945312
Batch 19/64 loss: -0.9090948104858398
Batch 20/64 loss: -0.7455167770385742
Batch 21/64 loss: -0.8796558380126953
Batch 22/64 loss: -1.1202831268310547
Batch 23/64 loss: -0.9537057876586914
Batch 24/64 loss: -1.0323762893676758
Batch 25/64 loss: -0.877985954284668
Batch 26/64 loss: -0.8558626174926758
Batch 27/64 loss: -1.0273027420043945
Batch 28/64 loss: -0.6718826293945312
Batch 29/64 loss: -0.847050666809082
Batch 30/64 loss: -0.731816291809082
Batch 31/64 loss: -0.8528985977172852
Batch 32/64 loss: -0.8443021774291992
Batch 33/64 loss: -0.7798538208007812
Batch 34/64 loss: -0.7728233337402344
Batch 35/64 loss: -0.8260917663574219
Batch 36/64 loss: -0.47996997833251953
Batch 37/64 loss: -1.062607765197754
Batch 38/64 loss: -0.4104938507080078
Batch 39/64 loss: -0.9551362991333008
Batch 40/64 loss: -0.9136714935302734
Batch 41/64 loss: -1.0098199844360352
Batch 42/64 loss: -1.006791114807129
Batch 43/64 loss: -0.9815216064453125
Batch 44/64 loss: -1.0820751190185547
Batch 45/64 loss: -1.075068473815918
Batch 46/64 loss: -1.1734037399291992
Batch 47/64 loss: -0.6845035552978516
Batch 48/64 loss: -0.7795839309692383
Batch 49/64 loss: -0.7006359100341797
Batch 50/64 loss: -0.7252693176269531
Batch 51/64 loss: -0.8153343200683594
Batch 52/64 loss: -1.1155576705932617
Batch 53/64 loss: -1.1077346801757812
Batch 54/64 loss: -0.432159423828125
Batch 55/64 loss: -0.6717815399169922
Batch 56/64 loss: -1.1285505294799805
Batch 57/64 loss: -0.824244499206543
Batch 58/64 loss: -0.6082677841186523
Batch 59/64 loss: -0.8581733703613281
Batch 60/64 loss: -1.0775489807128906
Batch 61/64 loss: -0.8021125793457031
Batch 62/64 loss: -0.8534603118896484
Batch 63/64 loss: -1.083806037902832
Batch 64/64 loss: -5.076864719390869
Epoch 456  Train loss: -0.9191025659149769  Val loss: -0.7954230554325065
Epoch 457
-------------------------------
Batch 1/64 loss: -0.873692512512207
Batch 2/64 loss: -0.8249025344848633
Batch 3/64 loss: -1.0230588912963867
Batch 4/64 loss: -0.8202066421508789
Batch 5/64 loss: -1.0286331176757812
Batch 6/64 loss: -0.9618806838989258
Batch 7/64 loss: -1.1314983367919922
Batch 8/64 loss: -1.0887279510498047
Batch 9/64 loss: -1.230484962463379
Batch 10/64 loss: -0.9685497283935547
Batch 11/64 loss: -0.8906621932983398
Batch 12/64 loss: -0.7985296249389648
Batch 13/64 loss: -0.9472751617431641
Batch 14/64 loss: -0.6422977447509766
Batch 15/64 loss: -0.7448940277099609
Batch 16/64 loss: -0.6354084014892578
Batch 17/64 loss: -0.4374666213989258
Batch 18/64 loss: -0.9347448348999023
Batch 19/64 loss: -0.8775129318237305
Batch 20/64 loss: -1.1927156448364258
Batch 21/64 loss: -1.1154985427856445
Batch 22/64 loss: -0.8621501922607422
Batch 23/64 loss: -1.1858863830566406
Batch 24/64 loss: -0.27560901641845703
Batch 25/64 loss: -0.7532157897949219
Batch 26/64 loss: -0.7720050811767578
Batch 27/64 loss: -0.8757658004760742
Batch 28/64 loss: -0.9243001937866211
Batch 29/64 loss: -1.1484508514404297
Batch 30/64 loss: -0.8825674057006836
Batch 31/64 loss: -0.7267513275146484
Batch 32/64 loss: -0.6408147811889648
Batch 33/64 loss: -0.9656124114990234
Batch 34/64 loss: -1.0487899780273438
Batch 35/64 loss: -1.0238122940063477
Batch 36/64 loss: -0.6234674453735352
Batch 37/64 loss: -0.9105062484741211
Batch 38/64 loss: -1.0479059219360352
Batch 39/64 loss: -0.6945095062255859
Batch 40/64 loss: -0.648869514465332
Batch 41/64 loss: -0.8637895584106445
Batch 42/64 loss: -1.1126012802124023
Batch 43/64 loss: -0.9302301406860352
Batch 44/64 loss: -1.0965967178344727
Batch 45/64 loss: -0.8845510482788086
Batch 46/64 loss: -0.9548940658569336
Batch 47/64 loss: -1.0138330459594727
Batch 48/64 loss: -0.8872308731079102
Batch 49/64 loss: -0.9735794067382812
Batch 50/64 loss: -1.2734289169311523
Batch 51/64 loss: -1.043405532836914
Batch 52/64 loss: -0.7513504028320312
Batch 53/64 loss: -0.7869110107421875
Batch 54/64 loss: -0.8754005432128906
Batch 55/64 loss: -1.0054149627685547
Batch 56/64 loss: -1.224909782409668
Batch 57/64 loss: -0.7146329879760742
Batch 58/64 loss: -0.9814949035644531
Batch 59/64 loss: -1.0898256301879883
Batch 60/64 loss: -1.084085464477539
Batch 61/64 loss: -1.1721124649047852
Batch 62/64 loss: -0.8809909820556641
Batch 63/64 loss: -1.0681037902832031
Batch 64/64 loss: -4.7414469718933105
Epoch 457  Train loss: -0.9632170976377001  Val loss: -0.8544696663663149
Epoch 458
-------------------------------
Batch 1/64 loss: -0.8046588897705078
Batch 2/64 loss: -0.9177398681640625
Batch 3/64 loss: -1.0615911483764648
Batch 4/64 loss: -0.9106168746948242
Batch 5/64 loss: -1.2114028930664062
Batch 6/64 loss: -1.3835563659667969
Batch 7/64 loss: -0.9477310180664062
Batch 8/64 loss: -0.8672761917114258
Batch 9/64 loss: -1.1032981872558594
Batch 10/64 loss: -1.432499885559082
Batch 11/64 loss: -0.8137912750244141
Batch 12/64 loss: -0.8741617202758789
Batch 13/64 loss: -1.0511341094970703
Batch 14/64 loss: -0.8116750717163086
Batch 15/64 loss: -0.8398370742797852
Batch 16/64 loss: -1.280226707458496
Batch 17/64 loss: -0.6147699356079102
Batch 18/64 loss: -0.7246885299682617
Batch 19/64 loss: -0.6938276290893555
Batch 20/64 loss: -0.7022256851196289
Batch 21/64 loss: -0.9539680480957031
Batch 22/64 loss: -1.1043481826782227
Batch 23/64 loss: -0.7052335739135742
Batch 24/64 loss: -1.1012983322143555
Batch 25/64 loss: -1.0377426147460938
Batch 26/64 loss: -0.8562955856323242
Batch 27/64 loss: -1.218989372253418
Batch 28/64 loss: -1.0200614929199219
Batch 29/64 loss: -0.8327693939208984
Batch 30/64 loss: -1.0367746353149414
Batch 31/64 loss: -1.0554046630859375
Batch 32/64 loss: -0.3922748565673828
Batch 33/64 loss: -0.841731071472168
Batch 34/64 loss: -0.7394447326660156
Batch 35/64 loss: -0.8630695343017578
Batch 36/64 loss: -0.695744514465332
Batch 37/64 loss: -0.6538524627685547
Batch 38/64 loss: -0.6067514419555664
Batch 39/64 loss: -0.46590518951416016
Batch 40/64 loss: -0.18006134033203125
Batch 41/64 loss: -0.4904317855834961
Batch 42/64 loss: -1.178375244140625
Batch 43/64 loss: -0.8884801864624023
Batch 44/64 loss: -0.5126323699951172
Batch 45/64 loss: -0.8803195953369141
Batch 46/64 loss: -0.9850931167602539
Batch 47/64 loss: -0.8568630218505859
Batch 48/64 loss: -1.1087608337402344
Batch 49/64 loss: -0.9542627334594727
Batch 50/64 loss: -0.5135383605957031
Batch 51/64 loss: -0.3314704895019531
Batch 52/64 loss: -0.5065069198608398
Batch 53/64 loss: -0.6086359024047852
Batch 54/64 loss: -0.8484983444213867
Batch 55/64 loss: -0.7910871505737305
Batch 56/64 loss: -0.7491283416748047
Batch 57/64 loss: -0.6315813064575195
Batch 58/64 loss: -0.6862859725952148
Batch 59/64 loss: -0.8540115356445312
Batch 60/64 loss: -0.43139171600341797
Batch 61/64 loss: -0.9339332580566406
Batch 62/64 loss: -0.9909133911132812
Batch 63/64 loss: -0.9098310470581055
Batch 64/64 loss: -4.425910949707031
Epoch 458  Train loss: -0.8842336617264093  Val loss: -0.6725821871937755
Epoch 459
-------------------------------
Batch 1/64 loss: -0.8493776321411133
Batch 2/64 loss: -0.28448009490966797
Batch 3/64 loss: -0.8850116729736328
Batch 4/64 loss: -0.8712196350097656
Batch 5/64 loss: -0.7860469818115234
Batch 6/64 loss: -1.1587409973144531
Batch 7/64 loss: -0.8182878494262695
Batch 8/64 loss: -0.8390283584594727
Batch 9/64 loss: -0.9533615112304688
Batch 10/64 loss: -0.5734548568725586
Batch 11/64 loss: -0.9940824508666992
Batch 12/64 loss: -0.9894933700561523
Batch 13/64 loss: -0.9111299514770508
Batch 14/64 loss: -0.7622346878051758
Batch 15/64 loss: -1.2656173706054688
Batch 16/64 loss: -1.1268682479858398
Batch 17/64 loss: -0.9212017059326172
Batch 18/64 loss: -1.0170469284057617
Batch 19/64 loss: -1.0380525588989258
Batch 20/64 loss: -1.096693992614746
Batch 21/64 loss: -0.8542442321777344
Batch 22/64 loss: -1.0706357955932617
Batch 23/64 loss: -0.8807973861694336
Batch 24/64 loss: -0.5933609008789062
Batch 25/64 loss: -0.8345117568969727
Batch 26/64 loss: -0.7257204055786133
Batch 27/64 loss: -0.9531059265136719
Batch 28/64 loss: -0.5671815872192383
Batch 29/64 loss: -0.7020292282104492
Batch 30/64 loss: -0.9739246368408203
Batch 31/64 loss: -1.145442008972168
Batch 32/64 loss: -0.7072668075561523
Batch 33/64 loss: -0.9364795684814453
Batch 34/64 loss: -0.9607963562011719
Batch 35/64 loss: -0.3833484649658203
Batch 36/64 loss: -1.3042879104614258
Batch 37/64 loss: -1.2561025619506836
Batch 38/64 loss: -0.9278144836425781
Batch 39/64 loss: -1.0351219177246094
Batch 40/64 loss: -1.2768049240112305
Batch 41/64 loss: -0.8161506652832031
Batch 42/64 loss: -1.062668800354004
Batch 43/64 loss: -0.8368215560913086
Batch 44/64 loss: -0.7210025787353516
Batch 45/64 loss: -0.770716667175293
Batch 46/64 loss: -1.167363166809082
Batch 47/64 loss: -0.9814701080322266
Batch 48/64 loss: -1.0452537536621094
Batch 49/64 loss: -0.6695365905761719
Batch 50/64 loss: -0.7095861434936523
Batch 51/64 loss: -0.9832744598388672
Batch 52/64 loss: -1.1734790802001953
Batch 53/64 loss: -0.4831247329711914
Batch 54/64 loss: -1.0570430755615234
Batch 55/64 loss: -0.5414848327636719
Batch 56/64 loss: -0.9660634994506836
Batch 57/64 loss: -0.8840265274047852
Batch 58/64 loss: -1.08807373046875
Batch 59/64 loss: -0.8013410568237305
Batch 60/64 loss: -1.0976333618164062
Batch 61/64 loss: -1.1543960571289062
Batch 62/64 loss: -0.9240131378173828
Batch 63/64 loss: -0.6955757141113281
Batch 64/64 loss: -5.43187952041626
Epoch 459  Train loss: -0.9558339866937375  Val loss: -0.7415041448324406
Epoch 460
-------------------------------
Batch 1/64 loss: -0.8401603698730469
Batch 2/64 loss: -0.48694467544555664
Batch 3/64 loss: -0.9508771896362305
Batch 4/64 loss: -1.2023134231567383
Batch 5/64 loss: -1.037827491760254
Batch 6/64 loss: -0.7945728302001953
Batch 7/64 loss: -1.117985725402832
Batch 8/64 loss: -0.35109710693359375
Batch 9/64 loss: -0.8510026931762695
Batch 10/64 loss: -0.8243217468261719
Batch 11/64 loss: -0.8335390090942383
Batch 12/64 loss: -0.5679922103881836
Batch 13/64 loss: -1.1632013320922852
Batch 14/64 loss: -0.8379068374633789
Batch 15/64 loss: -0.915125846862793
Batch 16/64 loss: -1.1222362518310547
Batch 17/64 loss: -0.9462928771972656
Batch 18/64 loss: -1.0721302032470703
Batch 19/64 loss: -1.0618867874145508
Batch 20/64 loss: -1.0601978302001953
Batch 21/64 loss: -0.8689441680908203
Batch 22/64 loss: -0.5591011047363281
Batch 23/64 loss: -1.25408935546875
Batch 24/64 loss: -0.24276447296142578
Batch 25/64 loss: -1.1189231872558594
Batch 26/64 loss: -1.0442008972167969
Batch 27/64 loss: -0.9668607711791992
Batch 28/64 loss: -1.1840219497680664
Batch 29/64 loss: -1.2257881164550781
Batch 30/64 loss: -1.049966812133789
Batch 31/64 loss: -0.8307209014892578
Batch 32/64 loss: -0.97900390625
Batch 33/64 loss: -0.9467153549194336
Batch 34/64 loss: -1.0092048645019531
Batch 35/64 loss: -1.015615463256836
Batch 36/64 loss: -0.4750375747680664
Batch 37/64 loss: -1.096186637878418
Batch 38/64 loss: -0.8941640853881836
Batch 39/64 loss: -0.9760875701904297
Batch 40/64 loss: -0.8392305374145508
Batch 41/64 loss: -1.085805892944336
Batch 42/64 loss: -1.3180055618286133
Batch 43/64 loss: -0.7204666137695312
Batch 44/64 loss: -1.0142545700073242
Batch 45/64 loss: -1.0297975540161133
Batch 46/64 loss: -0.7939023971557617
Batch 47/64 loss: -1.0978899002075195
Batch 48/64 loss: -0.5197553634643555
Batch 49/64 loss: -0.9332637786865234
Batch 50/64 loss: -1.0628156661987305
Batch 51/64 loss: -1.1618919372558594
Batch 52/64 loss: -0.8977994918823242
Batch 53/64 loss: -1.1111879348754883
Batch 54/64 loss: -0.44973087310791016
Batch 55/64 loss: -0.7264223098754883
Batch 56/64 loss: -0.8190784454345703
Batch 57/64 loss: -0.9190607070922852
Batch 58/64 loss: -1.0065717697143555
Batch 59/64 loss: -0.8750810623168945
Batch 60/64 loss: -1.198410987854004
Batch 61/64 loss: -0.5875434875488281
Batch 62/64 loss: -0.017800331115722656
Batch 63/64 loss: -1.0853395462036133
Batch 64/64 loss: -5.0122175216674805
Epoch 460  Train loss: -0.9538083057777554  Val loss: 0.3760591225116114
Epoch 461
-------------------------------
Batch 1/64 loss: -0.8498878479003906
Batch 2/64 loss: -1.1333093643188477
Batch 3/64 loss: -0.9281034469604492
Batch 4/64 loss: -0.14379405975341797
Batch 5/64 loss: -0.7026758193969727
Batch 6/64 loss: -0.5526618957519531
Batch 7/64 loss: -0.6713943481445312
Batch 8/64 loss: -1.0952482223510742
Batch 9/64 loss: -0.7628374099731445
Batch 10/64 loss: -0.29055118560791016
Batch 11/64 loss: -0.5952873229980469
Batch 12/64 loss: -0.5047588348388672
Batch 13/64 loss: -0.7374801635742188
Batch 14/64 loss: -0.8735246658325195
Batch 15/64 loss: -0.7347917556762695
Batch 16/64 loss: -0.31677913665771484
Batch 17/64 loss: -0.8871946334838867
Batch 18/64 loss: -0.8137712478637695
Batch 19/64 loss: -0.8421096801757812
Batch 20/64 loss: -0.7609977722167969
Batch 21/64 loss: -0.6332159042358398
Batch 22/64 loss: -0.8745012283325195
Batch 23/64 loss: -0.7881889343261719
Batch 24/64 loss: -0.1597118377685547
Batch 25/64 loss: -0.9412088394165039
Batch 26/64 loss: -0.8647317886352539
Batch 27/64 loss: -0.39151811599731445
Batch 28/64 loss: -0.41157054901123047
Batch 29/64 loss: -0.8535642623901367
Batch 30/64 loss: -1.2327861785888672
Batch 31/64 loss: -0.9584712982177734
Batch 32/64 loss: -0.804351806640625
Batch 33/64 loss: -0.8157939910888672
Batch 34/64 loss: -0.9134912490844727
Batch 35/64 loss: -0.8451023101806641
Batch 36/64 loss: -0.4820089340209961
Batch 37/64 loss: -0.6358070373535156
Batch 38/64 loss: -0.7217206954956055
Batch 39/64 loss: -1.0848503112792969
Batch 40/64 loss: -0.8052940368652344
Batch 41/64 loss: -0.5823974609375
Batch 42/64 loss: -1.1104555130004883
Batch 43/64 loss: -0.4450874328613281
Batch 44/64 loss: -0.972900390625
Batch 45/64 loss: -0.8142547607421875
Batch 46/64 loss: -0.9113435745239258
Batch 47/64 loss: -1.0463619232177734
Batch 48/64 loss: -0.9415473937988281
Batch 49/64 loss: -0.9680290222167969
Batch 50/64 loss: -0.6669111251831055
Batch 51/64 loss: -0.9634294509887695
Batch 52/64 loss: -0.7938117980957031
Batch 53/64 loss: -0.9516658782958984
Batch 54/64 loss: -0.9450101852416992
Batch 55/64 loss: -0.9662046432495117
Batch 56/64 loss: -0.7698001861572266
Batch 57/64 loss: -0.975189208984375
Batch 58/64 loss: -0.33949947357177734
Batch 59/64 loss: -1.2282476425170898
Batch 60/64 loss: -0.7985506057739258
Batch 61/64 loss: -0.7551364898681641
Batch 62/64 loss: -0.6359043121337891
Batch 63/64 loss: -0.6123952865600586
Batch 64/64 loss: -5.322878837585449
Epoch 461  Train loss: -0.8251190746531767  Val loss: -0.9192697715103831
Epoch 462
-------------------------------
Batch 1/64 loss: -1.051222801208496
Batch 2/64 loss: -0.9545803070068359
Batch 3/64 loss: -0.72125244140625
Batch 4/64 loss: -0.81317138671875
Batch 5/64 loss: -0.8624000549316406
Batch 6/64 loss: -0.9050884246826172
Batch 7/64 loss: -0.7309122085571289
Batch 8/64 loss: -1.101858139038086
Batch 9/64 loss: -0.1528768539428711
Batch 10/64 loss: -0.9065189361572266
Batch 11/64 loss: -0.8828487396240234
Batch 12/64 loss: -1.131394386291504
Batch 13/64 loss: -1.0811271667480469
Batch 14/64 loss: -1.127894401550293
Batch 15/64 loss: -0.7919759750366211
Batch 16/64 loss: -1.0904407501220703
Batch 17/64 loss: -0.49265289306640625
Batch 18/64 loss: -0.7662696838378906
Batch 19/64 loss: -0.9008617401123047
Batch 20/64 loss: -0.907374382019043
Batch 21/64 loss: -0.4719276428222656
Batch 22/64 loss: -1.0966291427612305
Batch 23/64 loss: -1.107783317565918
Batch 24/64 loss: -0.8653745651245117
Batch 25/64 loss: -0.9450607299804688
Batch 26/64 loss: -0.9314947128295898
Batch 27/64 loss: -1.1574344635009766
Batch 28/64 loss: -1.0233736038208008
Batch 29/64 loss: -0.920811653137207
Batch 30/64 loss: -0.9791536331176758
Batch 31/64 loss: -0.8849344253540039
Batch 32/64 loss: -0.6621198654174805
Batch 33/64 loss: -0.867274284362793
Batch 34/64 loss: -1.08209228515625
Batch 35/64 loss: -0.9816741943359375
Batch 36/64 loss: -1.2223014831542969
Batch 37/64 loss: -0.9184551239013672
Batch 38/64 loss: -0.7220516204833984
Batch 39/64 loss: -0.9506654739379883
Batch 40/64 loss: -0.7860937118530273
Batch 41/64 loss: -0.632023811340332
Batch 42/64 loss: -0.5285816192626953
Batch 43/64 loss: -1.0046072006225586
Batch 44/64 loss: -0.8948631286621094
Batch 45/64 loss: -1.0583229064941406
Batch 46/64 loss: -0.6408052444458008
Batch 47/64 loss: -0.6925239562988281
Batch 48/64 loss: -1.0702447891235352
Batch 49/64 loss: -1.0402164459228516
Batch 50/64 loss: -1.0195484161376953
Batch 51/64 loss: -0.8098907470703125
Batch 52/64 loss: -1.1116275787353516
Batch 53/64 loss: -1.1194076538085938
Batch 54/64 loss: -1.1499862670898438
Batch 55/64 loss: -1.276371955871582
Batch 56/64 loss: -1.084721565246582
Batch 57/64 loss: -0.9249458312988281
Batch 58/64 loss: -0.8848915100097656
Batch 59/64 loss: -1.0474882125854492
Batch 60/64 loss: -0.9785957336425781
Batch 61/64 loss: -0.9623298645019531
Batch 62/64 loss: -0.3883857727050781
Batch 63/64 loss: -0.7272987365722656
Batch 64/64 loss: -4.900690078735352
Epoch 462  Train loss: -0.9517274370380476  Val loss: -0.9195441544260766
Epoch 463
-------------------------------
Batch 1/64 loss: -1.0510482788085938
Batch 2/64 loss: -1.4009170532226562
Batch 3/64 loss: -1.0692873001098633
Batch 4/64 loss: -1.2937202453613281
Batch 5/64 loss: -1.1711101531982422
Batch 6/64 loss: -1.0470552444458008
Batch 7/64 loss: -1.0415916442871094
Batch 8/64 loss: -1.143610954284668
Batch 9/64 loss: -1.1874189376831055
Batch 10/64 loss: -1.0396451950073242
Batch 11/64 loss: -0.5884780883789062
Batch 12/64 loss: -0.7179117202758789
Batch 13/64 loss: -1.136190414428711
Batch 14/64 loss: -0.962061882019043
Batch 15/64 loss: -1.3328619003295898
Batch 16/64 loss: -0.8599786758422852
Batch 17/64 loss: -0.8080053329467773
Batch 18/64 loss: -0.8857030868530273
Batch 19/64 loss: -0.5777263641357422
Batch 20/64 loss: -1.110032081604004
Batch 21/64 loss: -0.7826290130615234
Batch 22/64 loss: -1.1958560943603516
Batch 23/64 loss: -0.725733757019043
Batch 24/64 loss: -0.6228761672973633
Batch 25/64 loss: -0.38678836822509766
Batch 26/64 loss: -1.0782346725463867
Batch 27/64 loss: -0.9005184173583984
Batch 28/64 loss: -0.6474523544311523
Batch 29/64 loss: -0.8935260772705078
Batch 30/64 loss: -0.832463264465332
Batch 31/64 loss: -1.0996856689453125
Batch 32/64 loss: -1.296651840209961
Batch 33/64 loss: -1.235426902770996
Batch 34/64 loss: -1.2644062042236328
Batch 35/64 loss: -1.1624059677124023
Batch 36/64 loss: -0.48424530029296875
Batch 37/64 loss: -0.9702863693237305
Batch 38/64 loss: -0.9424772262573242
Batch 39/64 loss: -0.5669670104980469
Batch 40/64 loss: -1.0267763137817383
Batch 41/64 loss: -0.6809978485107422
Batch 42/64 loss: -0.8912715911865234
Batch 43/64 loss: -0.9316005706787109
Batch 44/64 loss: -0.683441162109375
Batch 45/64 loss: -1.3146686553955078
Batch 46/64 loss: -1.1022262573242188
Batch 47/64 loss: -0.7312202453613281
Batch 48/64 loss: -0.5601778030395508
Batch 49/64 loss: -0.5182790756225586
Batch 50/64 loss: -1.1566333770751953
Batch 51/64 loss: -0.9645090103149414
Batch 52/64 loss: -0.8014793395996094
Batch 53/64 loss: -1.1355342864990234
Batch 54/64 loss: -0.5544958114624023
Batch 55/64 loss: -1.002211570739746
Batch 56/64 loss: -0.9039726257324219
Batch 57/64 loss: -0.5544242858886719
Batch 58/64 loss: -1.1267528533935547
Batch 59/64 loss: -0.9767055511474609
Batch 60/64 loss: -0.8909492492675781
Batch 61/64 loss: -0.4867105484008789
Batch 62/64 loss: -1.0651416778564453
Batch 63/64 loss: -1.1121635437011719
Batch 64/64 loss: -4.837613105773926
Epoch 463  Train loss: -0.9774672676535213  Val loss: -0.9978440012718803
Saving best model, epoch: 463
Epoch 464
-------------------------------
Batch 1/64 loss: -0.38985157012939453
Batch 2/64 loss: -1.3030672073364258
Batch 3/64 loss: -0.9995136260986328
Batch 4/64 loss: -0.8704490661621094
Batch 5/64 loss: -0.44428157806396484
Batch 6/64 loss: -0.8375005722045898
Batch 7/64 loss: -1.1174020767211914
Batch 8/64 loss: -0.8511619567871094
Batch 9/64 loss: -0.7336568832397461
Batch 10/64 loss: -0.9427204132080078
Batch 11/64 loss: -0.7968358993530273
Batch 12/64 loss: -0.9092311859130859
Batch 13/64 loss: -1.295506477355957
Batch 14/64 loss: -0.6941194534301758
Batch 15/64 loss: -1.053558349609375
Batch 16/64 loss: -1.3389472961425781
Batch 17/64 loss: -0.9619617462158203
Batch 18/64 loss: -0.9987058639526367
Batch 19/64 loss: -0.9487695693969727
Batch 20/64 loss: -1.0819034576416016
Batch 21/64 loss: -1.0027694702148438
Batch 22/64 loss: -0.8202447891235352
Batch 23/64 loss: -0.7010297775268555
Batch 24/64 loss: -0.9683904647827148
Batch 25/64 loss: -0.8762054443359375
Batch 26/64 loss: -1.0965442657470703
Batch 27/64 loss: -1.020066261291504
Batch 28/64 loss: -0.8279542922973633
Batch 29/64 loss: -0.9789285659790039
Batch 30/64 loss: -0.9292497634887695
Batch 31/64 loss: -0.8768186569213867
Batch 32/64 loss: -1.1091842651367188
Batch 33/64 loss: -0.5720062255859375
Batch 34/64 loss: -1.1013603210449219
Batch 35/64 loss: -1.086740493774414
Batch 36/64 loss: -0.8216333389282227
Batch 37/64 loss: -0.9136419296264648
Batch 38/64 loss: -1.050917625427246
Batch 39/64 loss: -1.223973274230957
Batch 40/64 loss: -1.2215709686279297
Batch 41/64 loss: -0.8377227783203125
Batch 42/64 loss: -0.9097566604614258
Batch 43/64 loss: -0.47578907012939453
Batch 44/64 loss: -1.1411218643188477
Batch 45/64 loss: -1.2589778900146484
Batch 46/64 loss: -0.764988899230957
Batch 47/64 loss: -0.6360311508178711
Batch 48/64 loss: -0.8190860748291016
Batch 49/64 loss: -0.5746383666992188
Batch 50/64 loss: -0.8170461654663086
Batch 51/64 loss: -1.3119316101074219
Batch 52/64 loss: -1.1598825454711914
Batch 53/64 loss: -1.1755695343017578
Batch 54/64 loss: -1.1084537506103516
Batch 55/64 loss: -1.1368131637573242
Batch 56/64 loss: -0.8844490051269531
Batch 57/64 loss: -1.1880998611450195
Batch 58/64 loss: -1.0984735488891602
Batch 59/64 loss: -0.9189643859863281
Batch 60/64 loss: -0.9871721267700195
Batch 61/64 loss: -1.0160207748413086
Batch 62/64 loss: -0.941986083984375
Batch 63/64 loss: -0.7577791213989258
Batch 64/64 loss: -4.559596061706543
Epoch 464  Train loss: -0.989942367404115  Val loss: -0.8593990024422452
Epoch 465
-------------------------------
Batch 1/64 loss: -0.8392333984375
Batch 2/64 loss: -0.756892204284668
Batch 3/64 loss: -1.2301101684570312
Batch 4/64 loss: -0.6452302932739258
Batch 5/64 loss: -0.8462858200073242
Batch 6/64 loss: -1.0507965087890625
Batch 7/64 loss: -0.9909296035766602
Batch 8/64 loss: -0.690185546875
Batch 9/64 loss: -0.8497419357299805
Batch 10/64 loss: -1.0958271026611328
Batch 11/64 loss: -0.7488336563110352
Batch 12/64 loss: -1.0283203125
Batch 13/64 loss: -1.0108823776245117
Batch 14/64 loss: -1.1840457916259766
Batch 15/64 loss: -1.0731515884399414
Batch 16/64 loss: -1.1288070678710938
Batch 17/64 loss: -0.6516475677490234
Batch 18/64 loss: -0.9616708755493164
Batch 19/64 loss: -0.7462406158447266
Batch 20/64 loss: -1.2328157424926758
Batch 21/64 loss: -1.1533851623535156
Batch 22/64 loss: -1.1320409774780273
Batch 23/64 loss: -0.8817501068115234
Batch 24/64 loss: -0.8253593444824219
Batch 25/64 loss: -0.9675674438476562
Batch 26/64 loss: -1.1766347885131836
Batch 27/64 loss: -0.9793262481689453
Batch 28/64 loss: -1.0694770812988281
Batch 29/64 loss: -1.3204803466796875
Batch 30/64 loss: -0.6124973297119141
Batch 31/64 loss: -1.1319141387939453
Batch 32/64 loss: -0.5857877731323242
Batch 33/64 loss: -1.1824493408203125
Batch 34/64 loss: -1.098780632019043
Batch 35/64 loss: -1.1151971817016602
Batch 36/64 loss: -0.9600286483764648
Batch 37/64 loss: -1.0639400482177734
Batch 38/64 loss: -1.092320442199707
Batch 39/64 loss: -0.6346979141235352
Batch 40/64 loss: -1.2555904388427734
Batch 41/64 loss: -0.8187799453735352
Batch 42/64 loss: -0.6146326065063477
Batch 43/64 loss: -1.2210874557495117
Batch 44/64 loss: -1.0686206817626953
Batch 45/64 loss: -0.9470338821411133
Batch 46/64 loss: -1.1332283020019531
Batch 47/64 loss: -1.0452165603637695
Batch 48/64 loss: -1.130131721496582
Batch 49/64 loss: -0.4956531524658203
Batch 50/64 loss: -0.7802152633666992
Batch 51/64 loss: -0.7089433670043945
Batch 52/64 loss: -0.6361856460571289
Batch 53/64 loss: -1.1652517318725586
Batch 54/64 loss: -1.1970739364624023
Batch 55/64 loss: -0.9559831619262695
Batch 56/64 loss: -0.5483665466308594
Batch 57/64 loss: -1.0040473937988281
Batch 58/64 loss: -0.9268274307250977
Batch 59/64 loss: -1.1142854690551758
Batch 60/64 loss: -1.2251901626586914
Batch 61/64 loss: -1.3842496871948242
Batch 62/64 loss: -0.7079458236694336
Batch 63/64 loss: -0.6747331619262695
Batch 64/64 loss: -5.36538028717041
Epoch 465  Train loss: -1.0122132058237112  Val loss: -0.9946041434900867
Epoch 466
-------------------------------
Batch 1/64 loss: -1.312662124633789
Batch 2/64 loss: -1.032938003540039
Batch 3/64 loss: -0.9496145248413086
Batch 4/64 loss: -1.3033132553100586
Batch 5/64 loss: -1.0060720443725586
Batch 6/64 loss: -1.0898752212524414
Batch 7/64 loss: -1.2641353607177734
Batch 8/64 loss: -0.5467815399169922
Batch 9/64 loss: -1.1477775573730469
Batch 10/64 loss: -1.0999155044555664
Batch 11/64 loss: -1.1006574630737305
Batch 12/64 loss: -1.3087148666381836
Batch 13/64 loss: -0.9520273208618164
Batch 14/64 loss: -1.222884178161621
Batch 15/64 loss: -1.107813835144043
Batch 16/64 loss: -0.9459743499755859
Batch 17/64 loss: -0.5935707092285156
Batch 18/64 loss: -0.9099655151367188
Batch 19/64 loss: -1.054642677307129
Batch 20/64 loss: -0.9082107543945312
Batch 21/64 loss: -0.2710299491882324
Batch 22/64 loss: -1.2594833374023438
Batch 23/64 loss: -0.8898525238037109
Batch 24/64 loss: -1.1391363143920898
Batch 25/64 loss: -0.9740419387817383
Batch 26/64 loss: -0.5702705383300781
Batch 27/64 loss: -0.8426437377929688
Batch 28/64 loss: -0.7620143890380859
Batch 29/64 loss: -0.6390132904052734
Batch 30/64 loss: -0.6041793823242188
Batch 31/64 loss: -1.2222604751586914
Batch 32/64 loss: -0.8424758911132812
Batch 33/64 loss: -1.067636489868164
Batch 34/64 loss: -0.8343391418457031
Batch 35/64 loss: -1.0793266296386719
Batch 36/64 loss: -0.804224967956543
Batch 37/64 loss: -1.1883697509765625
Batch 38/64 loss: -1.0499296188354492
Batch 39/64 loss: -0.9869813919067383
Batch 40/64 loss: -0.7923812866210938
Batch 41/64 loss: -0.7370700836181641
Batch 42/64 loss: -0.8708877563476562
Batch 43/64 loss: -1.0796527862548828
Batch 44/64 loss: -0.6234369277954102
Batch 45/64 loss: -1.2358036041259766
Batch 46/64 loss: -0.6923637390136719
Batch 47/64 loss: -1.029327392578125
Batch 48/64 loss: -0.36652183532714844
Batch 49/64 loss: -0.8793888092041016
Batch 50/64 loss: -0.863499641418457
Batch 51/64 loss: -0.7947282791137695
Batch 52/64 loss: -0.9273567199707031
Batch 53/64 loss: -0.7849187850952148
Batch 54/64 loss: -1.0618486404418945
Batch 55/64 loss: -1.1403388977050781
Batch 56/64 loss: -0.9534111022949219
Batch 57/64 loss: -0.8531560897827148
Batch 58/64 loss: -1.2465801239013672
Batch 59/64 loss: -1.3096370697021484
Batch 60/64 loss: -0.7934560775756836
Batch 61/64 loss: -0.030115127563476562
Batch 62/64 loss: -1.0105648040771484
Batch 63/64 loss: -0.9256668090820312
Batch 64/64 loss: -4.0078887939453125
Epoch 466  Train loss: -0.9708672243006089  Val loss: -0.9117207084734416
Epoch 467
-------------------------------
Batch 1/64 loss: -0.7553033828735352
Batch 2/64 loss: -1.1255426406860352
Batch 3/64 loss: -0.46277427673339844
Batch 4/64 loss: -0.8272438049316406
Batch 5/64 loss: -0.8894357681274414
Batch 6/64 loss: -1.3073968887329102
Batch 7/64 loss: -1.2116107940673828
Batch 8/64 loss: -1.0760393142700195
Batch 9/64 loss: -0.8713436126708984
Batch 10/64 loss: -0.8719482421875
Batch 11/64 loss: -0.7057619094848633
Batch 12/64 loss: -0.7053165435791016
Batch 13/64 loss: -0.6583194732666016
Batch 14/64 loss: -0.7589349746704102
Batch 15/64 loss: -0.6909284591674805
Batch 16/64 loss: -1.114267349243164
Batch 17/64 loss: -0.9716386795043945
Batch 18/64 loss: -0.7763738632202148
Batch 19/64 loss: -0.8907814025878906
Batch 20/64 loss: -1.0165719985961914
Batch 21/64 loss: -1.01092529296875
Batch 22/64 loss: -0.7568473815917969
Batch 23/64 loss: -0.46292591094970703
Batch 24/64 loss: -0.8759336471557617
Batch 25/64 loss: -1.1121454238891602
Batch 26/64 loss: -0.9644241333007812
Batch 27/64 loss: -0.5890274047851562
Batch 28/64 loss: -0.9614439010620117
Batch 29/64 loss: -1.0031490325927734
Batch 30/64 loss: -0.6207151412963867
Batch 31/64 loss: -0.5793867111206055
Batch 32/64 loss: -1.0846748352050781
Batch 33/64 loss: -1.3103094100952148
Batch 34/64 loss: -0.8287982940673828
Batch 35/64 loss: -0.8488922119140625
Batch 36/64 loss: -1.0141000747680664
Batch 37/64 loss: -0.6256895065307617
Batch 38/64 loss: -1.2590274810791016
Batch 39/64 loss: -0.5086221694946289
Batch 40/64 loss: -0.6697626113891602
Batch 41/64 loss: -0.954066276550293
Batch 42/64 loss: -0.4568338394165039
Batch 43/64 loss: -0.6318941116333008
Batch 44/64 loss: -0.7735137939453125
Batch 45/64 loss: -1.194540023803711
Batch 46/64 loss: -0.9501819610595703
Batch 47/64 loss: -0.8214750289916992
Batch 48/64 loss: -1.1559982299804688
Batch 49/64 loss: -0.8621072769165039
Batch 50/64 loss: -0.7747383117675781
Batch 51/64 loss: -1.190420150756836
Batch 52/64 loss: -0.8195829391479492
Batch 53/64 loss: -0.9859580993652344
Batch 54/64 loss: -1.2393970489501953
Batch 55/64 loss: -0.8973979949951172
Batch 56/64 loss: -0.7202377319335938
Batch 57/64 loss: -0.7857532501220703
Batch 58/64 loss: -0.8768177032470703
Batch 59/64 loss: -1.282191276550293
Batch 60/64 loss: -0.922149658203125
Batch 61/64 loss: -0.967930793762207
Batch 62/64 loss: -1.1533279418945312
Batch 63/64 loss: -1.0404138565063477
Batch 64/64 loss: -4.944110870361328
Epoch 467  Train loss: -0.9402250102922028  Val loss: -1.003997881387927
Saving best model, epoch: 467
Epoch 468
-------------------------------
Batch 1/64 loss: -0.8224582672119141
Batch 2/64 loss: -1.1825323104858398
Batch 3/64 loss: -0.9893522262573242
Batch 4/64 loss: -0.6713314056396484
Batch 5/64 loss: -0.8955545425415039
Batch 6/64 loss: -0.41921138763427734
Batch 7/64 loss: -1.0670156478881836
Batch 8/64 loss: -1.1221399307250977
Batch 9/64 loss: -1.1520347595214844
Batch 10/64 loss: -1.1074962615966797
Batch 11/64 loss: -0.6659860610961914
Batch 12/64 loss: -0.8075017929077148
Batch 13/64 loss: -0.8055801391601562
Batch 14/64 loss: -1.3451528549194336
Batch 15/64 loss: -0.8230724334716797
Batch 16/64 loss: -1.1883354187011719
Batch 17/64 loss: -0.998051643371582
Batch 18/64 loss: -1.1668872833251953
Batch 19/64 loss: -0.7864274978637695
Batch 20/64 loss: -1.114527702331543
Batch 21/64 loss: -0.9940090179443359
Batch 22/64 loss: -0.8769197463989258
Batch 23/64 loss: -0.4868459701538086
Batch 24/64 loss: -1.037628173828125
Batch 25/64 loss: -0.9430418014526367
Batch 26/64 loss: -1.2400798797607422
Batch 27/64 loss: -0.6320295333862305
Batch 28/64 loss: -1.1889305114746094
Batch 29/64 loss: -1.0576105117797852
Batch 30/64 loss: -0.8156843185424805
Batch 31/64 loss: -0.8836755752563477
Batch 32/64 loss: -0.96466064453125
Batch 33/64 loss: -0.8536520004272461
Batch 34/64 loss: -1.2204313278198242
Batch 35/64 loss: -0.6415386199951172
Batch 36/64 loss: -0.9428024291992188
Batch 37/64 loss: -1.300039291381836
Batch 38/64 loss: -0.9550905227661133
Batch 39/64 loss: -1.2448482513427734
Batch 40/64 loss: -1.1668453216552734
Batch 41/64 loss: -0.5514745712280273
Batch 42/64 loss: -0.8594293594360352
Batch 43/64 loss: -1.1005973815917969
Batch 44/64 loss: -0.9962425231933594
Batch 45/64 loss: -0.3782320022583008
Batch 46/64 loss: -0.9688949584960938
Batch 47/64 loss: -1.1890668869018555
Batch 48/64 loss: -0.6196441650390625
Batch 49/64 loss: -0.7290544509887695
Batch 50/64 loss: -0.9929380416870117
Batch 51/64 loss: -0.8874597549438477
Batch 52/64 loss: -0.7915744781494141
Batch 53/64 loss: 0.43245697021484375
Batch 54/64 loss: -1.2649812698364258
Batch 55/64 loss: -1.0227785110473633
Batch 56/64 loss: -0.5697965621948242
Batch 57/64 loss: -0.9911556243896484
Batch 58/64 loss: -1.0471887588500977
Batch 59/64 loss: -1.2675590515136719
Batch 60/64 loss: -0.9789419174194336
Batch 61/64 loss: -1.0455083847045898
Batch 62/64 loss: -0.9545373916625977
Batch 63/64 loss: -1.1204948425292969
Batch 64/64 loss: -4.72990083694458
Epoch 468  Train loss: -0.9728554127263088  Val loss: -0.7769773424286204
Epoch 469
-------------------------------
Batch 1/64 loss: -0.4253511428833008
Batch 2/64 loss: -0.936223030090332
Batch 3/64 loss: -1.0762195587158203
Batch 4/64 loss: -0.8900566101074219
Batch 5/64 loss: -0.7868013381958008
Batch 6/64 loss: -1.1469860076904297
Batch 7/64 loss: -0.8460016250610352
Batch 8/64 loss: -0.6098775863647461
Batch 9/64 loss: -0.990565299987793
Batch 10/64 loss: -0.9157896041870117
Batch 11/64 loss: -0.7338600158691406
Batch 12/64 loss: -0.8314533233642578
Batch 13/64 loss: -0.9533205032348633
Batch 14/64 loss: -0.8431377410888672
Batch 15/64 loss: -1.0671100616455078
Batch 16/64 loss: -0.24280786514282227
Batch 17/64 loss: -0.6101236343383789
Batch 18/64 loss: -0.6035270690917969
Batch 19/64 loss: -1.08685302734375
Batch 20/64 loss: -1.0248346328735352
Batch 21/64 loss: -0.7556476593017578
Batch 22/64 loss: -0.5634374618530273
Batch 23/64 loss: -1.1733551025390625
Batch 24/64 loss: -1.0689868927001953
Batch 25/64 loss: -1.0485029220581055
Batch 26/64 loss: -0.9680767059326172
Batch 27/64 loss: -0.8227682113647461
Batch 28/64 loss: -0.35536861419677734
Batch 29/64 loss: -0.8581085205078125
Batch 30/64 loss: -0.4333763122558594
Batch 31/64 loss: -0.7785882949829102
Batch 32/64 loss: -0.5753469467163086
Batch 33/64 loss: -1.0560579299926758
Batch 34/64 loss: -1.0067462921142578
Batch 35/64 loss: -1.0675840377807617
Batch 36/64 loss: -0.8205900192260742
Batch 37/64 loss: -0.9791727066040039
Batch 38/64 loss: -1.0114765167236328
Batch 39/64 loss: -1.0894012451171875
Batch 40/64 loss: -0.833308219909668
Batch 41/64 loss: -0.9974336624145508
Batch 42/64 loss: -0.7095403671264648
Batch 43/64 loss: -0.806035041809082
Batch 44/64 loss: -1.4047794342041016
Batch 45/64 loss: -1.085850715637207
Batch 46/64 loss: -0.784541130065918
Batch 47/64 loss: -0.5791006088256836
Batch 48/64 loss: -1.2410449981689453
Batch 49/64 loss: -1.051854133605957
Batch 50/64 loss: -1.0793523788452148
Batch 51/64 loss: -0.5790739059448242
Batch 52/64 loss: -1.1289739608764648
Batch 53/64 loss: -1.132589340209961
Batch 54/64 loss: -1.0149927139282227
Batch 55/64 loss: -0.99176025390625
Batch 56/64 loss: -0.9530458450317383
Batch 57/64 loss: -0.8880901336669922
Batch 58/64 loss: -0.9318017959594727
Batch 59/64 loss: -0.9066343307495117
Batch 60/64 loss: -0.2915167808532715
Batch 61/64 loss: -0.3446016311645508
Batch 62/64 loss: -0.9704341888427734
Batch 63/64 loss: -0.8857841491699219
Batch 64/64 loss: -5.442899227142334
Epoch 469  Train loss: -0.9212204895767511  Val loss: -0.9723501434850529
Epoch 470
-------------------------------
Batch 1/64 loss: -0.6485538482666016
Batch 2/64 loss: -0.8741016387939453
Batch 3/64 loss: -0.9251031875610352
Batch 4/64 loss: -1.0818290710449219
Batch 5/64 loss: -1.2735366821289062
Batch 6/64 loss: -0.9098482131958008
Batch 7/64 loss: -0.7858676910400391
Batch 8/64 loss: -1.0776491165161133
Batch 9/64 loss: -1.1327190399169922
Batch 10/64 loss: -0.8803777694702148
Batch 11/64 loss: -0.8544273376464844
Batch 12/64 loss: -1.04107666015625
Batch 13/64 loss: -0.8569622039794922
Batch 14/64 loss: -0.8275165557861328
Batch 15/64 loss: -0.034154415130615234
Batch 16/64 loss: -0.7087917327880859
Batch 17/64 loss: -0.9121713638305664
Batch 18/64 loss: -1.1552867889404297
Batch 19/64 loss: -1.1698265075683594
Batch 20/64 loss: -1.1232948303222656
Batch 21/64 loss: -0.6441621780395508
Batch 22/64 loss: -0.8150234222412109
Batch 23/64 loss: -0.45520973205566406
Batch 24/64 loss: -0.8432111740112305
Batch 25/64 loss: -0.9754714965820312
Batch 26/64 loss: -0.7892513275146484
Batch 27/64 loss: -0.6642227172851562
Batch 28/64 loss: -1.2980108261108398
Batch 29/64 loss: -0.6665124893188477
Batch 30/64 loss: -0.960698127746582
Batch 31/64 loss: -1.0183296203613281
Batch 32/64 loss: -0.7346982955932617
Batch 33/64 loss: -0.974299430847168
Batch 34/64 loss: -1.1647567749023438
Batch 35/64 loss: -1.108231544494629
Batch 36/64 loss: -0.80413818359375
Batch 37/64 loss: -1.1166410446166992
Batch 38/64 loss: -0.7666015625
Batch 39/64 loss: -0.39428138732910156
Batch 40/64 loss: -0.7205028533935547
Batch 41/64 loss: -0.9085149765014648
Batch 42/64 loss: -1.1347112655639648
Batch 43/64 loss: -0.06762981414794922
Batch 44/64 loss: -1.2091264724731445
Batch 45/64 loss: -1.1446952819824219
Batch 46/64 loss: -1.2554121017456055
Batch 47/64 loss: -1.141702651977539
Batch 48/64 loss: -1.0047998428344727
Batch 49/64 loss: -0.9942874908447266
Batch 50/64 loss: -1.0315971374511719
Batch 51/64 loss: -1.1099319458007812
Batch 52/64 loss: -0.5807361602783203
Batch 53/64 loss: -0.8507013320922852
Batch 54/64 loss: -1.3300647735595703
Batch 55/64 loss: -1.055436134338379
Batch 56/64 loss: -1.3124113082885742
Batch 57/64 loss: -1.0716800689697266
Batch 58/64 loss: -1.1031990051269531
Batch 59/64 loss: -1.2896318435668945
Batch 60/64 loss: -0.5300083160400391
Batch 61/64 loss: -1.3082151412963867
Batch 62/64 loss: -1.1241817474365234
Batch 63/64 loss: -1.2516870498657227
Batch 64/64 loss: -5.179653167724609
Epoch 470  Train loss: -0.9863913816564224  Val loss: -0.992967572818507
Epoch 471
-------------------------------
Batch 1/64 loss: -1.376936912536621
Batch 2/64 loss: -1.2386484146118164
Batch 3/64 loss: -0.666473388671875
Batch 4/64 loss: -0.7526082992553711
Batch 5/64 loss: -0.37095165252685547
Batch 6/64 loss: -0.5741176605224609
Batch 7/64 loss: -1.0068950653076172
Batch 8/64 loss: -1.2286224365234375
Batch 9/64 loss: -1.0016260147094727
Batch 10/64 loss: -1.4857778549194336
Batch 11/64 loss: -0.9154567718505859
Batch 12/64 loss: -1.176722526550293
Batch 13/64 loss: -1.1895256042480469
Batch 14/64 loss: -0.8304281234741211
Batch 15/64 loss: -1.1822261810302734
Batch 16/64 loss: -1.0645227432250977
Batch 17/64 loss: -0.5079584121704102
Batch 18/64 loss: -1.0453615188598633
Batch 19/64 loss: -1.202897071838379
Batch 20/64 loss: -1.0307512283325195
Batch 21/64 loss: -1.185471534729004
Batch 22/64 loss: -0.9618406295776367
Batch 23/64 loss: -0.9010629653930664
Batch 24/64 loss: -0.6868753433227539
Batch 25/64 loss: -0.9293966293334961
Batch 26/64 loss: -0.6512956619262695
Batch 27/64 loss: -0.7539472579956055
Batch 28/64 loss: -1.2076444625854492
Batch 29/64 loss: -0.5180730819702148
Batch 30/64 loss: -1.1830978393554688
Batch 31/64 loss: -1.0221443176269531
Batch 32/64 loss: -0.6161441802978516
Batch 33/64 loss: -1.094757080078125
Batch 34/64 loss: -1.0015945434570312
Batch 35/64 loss: -0.6076822280883789
Batch 36/64 loss: -1.1466102600097656
Batch 37/64 loss: -0.905177116394043
Batch 38/64 loss: -1.0092096328735352
Batch 39/64 loss: -1.3426942825317383
Batch 40/64 loss: -0.9151191711425781
Batch 41/64 loss: -1.0087203979492188
Batch 42/64 loss: -0.8374462127685547
Batch 43/64 loss: -1.265401840209961
Batch 44/64 loss: -1.1993722915649414
Batch 45/64 loss: -0.978419303894043
Batch 46/64 loss: -1.227604866027832
Batch 47/64 loss: -1.0152826309204102
Batch 48/64 loss: -1.077596664428711
Batch 49/64 loss: -1.1005258560180664
Batch 50/64 loss: -1.1884803771972656
Batch 51/64 loss: -1.1279125213623047
Batch 52/64 loss: -1.2074565887451172
Batch 53/64 loss: -0.4886655807495117
Batch 54/64 loss: -0.8973941802978516
Batch 55/64 loss: -1.3430671691894531
Batch 56/64 loss: -1.1929903030395508
Batch 57/64 loss: -0.44700050354003906
Batch 58/64 loss: -0.9405975341796875
Batch 59/64 loss: -0.933842658996582
Batch 60/64 loss: -1.3086910247802734
Batch 61/64 loss: -0.3401346206665039
Batch 62/64 loss: -1.3621931076049805
Batch 63/64 loss: -1.0449886322021484
Batch 64/64 loss: -5.322458267211914
Epoch 471  Train loss: -1.035513328103458  Val loss: -1.041779062592287
Saving best model, epoch: 471
Epoch 472
-------------------------------
Batch 1/64 loss: -0.935328483581543
Batch 2/64 loss: -1.2941312789916992
Batch 3/64 loss: -0.8879423141479492
Batch 4/64 loss: -0.8954248428344727
Batch 5/64 loss: -0.7661161422729492
Batch 6/64 loss: -1.089655876159668
Batch 7/64 loss: -0.5623435974121094
Batch 8/64 loss: -0.7741966247558594
Batch 9/64 loss: -0.9596767425537109
Batch 10/64 loss: -1.0033369064331055
Batch 11/64 loss: -0.7095527648925781
Batch 12/64 loss: -0.726283073425293
Batch 13/64 loss: -1.2318706512451172
Batch 14/64 loss: -1.0267829895019531
Batch 15/64 loss: -0.8843917846679688
Batch 16/64 loss: -0.6672887802124023
Batch 17/64 loss: -0.8511533737182617
Batch 18/64 loss: -1.1847686767578125
Batch 19/64 loss: -0.9880542755126953
Batch 20/64 loss: -1.2532463073730469
Batch 21/64 loss: -0.8592672348022461
Batch 22/64 loss: -0.7858762741088867
Batch 23/64 loss: -1.2320232391357422
Batch 24/64 loss: -0.865696907043457
Batch 25/64 loss: -0.819727897644043
Batch 26/64 loss: -1.0552253723144531
Batch 27/64 loss: -0.7441158294677734
Batch 28/64 loss: -1.1349821090698242
Batch 29/64 loss: -0.5874605178833008
Batch 30/64 loss: -0.8952531814575195
Batch 31/64 loss: -0.9726543426513672
Batch 32/64 loss: -0.88311767578125
Batch 33/64 loss: -1.3184080123901367
Batch 34/64 loss: -0.5261468887329102
Batch 35/64 loss: -0.560882568359375
Batch 36/64 loss: -1.3341197967529297
Batch 37/64 loss: -0.9277477264404297
Batch 38/64 loss: -0.458648681640625
Batch 39/64 loss: -0.8861551284790039
Batch 40/64 loss: -0.6129608154296875
Batch 41/64 loss: -0.8569650650024414
Batch 42/64 loss: -0.8770427703857422
Batch 43/64 loss: -1.1095333099365234
Batch 44/64 loss: -0.92095947265625
Batch 45/64 loss: -0.94488525390625
Batch 46/64 loss: -0.8316431045532227
Batch 47/64 loss: -0.9891948699951172
Batch 48/64 loss: -0.9907083511352539
Batch 49/64 loss: -1.1708135604858398
Batch 50/64 loss: -1.1242809295654297
Batch 51/64 loss: -1.2151288986206055
Batch 52/64 loss: -0.9962301254272461
Batch 53/64 loss: -0.5170383453369141
Batch 54/64 loss: -0.6659164428710938
Batch 55/64 loss: -0.6618833541870117
Batch 56/64 loss: -1.1283855438232422
Batch 57/64 loss: -1.1119251251220703
Batch 58/64 loss: -1.1528615951538086
Batch 59/64 loss: -1.0110158920288086
Batch 60/64 loss: -1.0494089126586914
Batch 61/64 loss: -1.113302230834961
Batch 62/64 loss: -1.1788320541381836
Batch 63/64 loss: -0.7954530715942383
Batch 64/64 loss: -5.369594097137451
Epoch 472  Train loss: -0.981845012365603  Val loss: -0.5325312794688641
Epoch 473
-------------------------------
Batch 1/64 loss: -0.7238645553588867
Batch 2/64 loss: -0.9456586837768555
Batch 3/64 loss: -1.1375436782836914
Batch 4/64 loss: -0.41869163513183594
Batch 5/64 loss: 0.02101421356201172
Batch 6/64 loss: -1.040085792541504
Batch 7/64 loss: -0.761836051940918
Batch 8/64 loss: -0.3769359588623047
Batch 9/64 loss: -0.5636482238769531
Batch 10/64 loss: -0.8253374099731445
Batch 11/64 loss: -0.7554588317871094
Batch 12/64 loss: -0.6059379577636719
Batch 13/64 loss: -1.2457351684570312
Batch 14/64 loss: -1.0019302368164062
Batch 15/64 loss: -1.084040641784668
Batch 16/64 loss: -1.0707435607910156
Batch 17/64 loss: -0.942662239074707
Batch 18/64 loss: -0.8278665542602539
Batch 19/64 loss: -1.1467742919921875
Batch 20/64 loss: -0.9971065521240234
Batch 21/64 loss: -0.9742841720581055
Batch 22/64 loss: -0.7877988815307617
Batch 23/64 loss: -0.8470830917358398
Batch 24/64 loss: -0.5991430282592773
Batch 25/64 loss: -0.998143196105957
Batch 26/64 loss: -0.891627311706543
Batch 27/64 loss: -0.3823971748352051
Batch 28/64 loss: -1.2365236282348633
Batch 29/64 loss: -1.258946418762207
Batch 30/64 loss: -0.9177541732788086
Batch 31/64 loss: -1.0687942504882812
Batch 32/64 loss: -0.882603645324707
Batch 33/64 loss: -0.8249988555908203
Batch 34/64 loss: -0.7242984771728516
Batch 35/64 loss: -0.9407081604003906
Batch 36/64 loss: -0.7003345489501953
Batch 37/64 loss: -1.0008459091186523
Batch 38/64 loss: -1.2447242736816406
Batch 39/64 loss: -1.1495676040649414
Batch 40/64 loss: -0.8952083587646484
Batch 41/64 loss: -1.0355720520019531
Batch 42/64 loss: -0.7284860610961914
Batch 43/64 loss: -0.6352920532226562
Batch 44/64 loss: -0.9852895736694336
Batch 45/64 loss: -0.8919277191162109
Batch 46/64 loss: -1.058481216430664
Batch 47/64 loss: -0.755854606628418
Batch 48/64 loss: -0.9344749450683594
Batch 49/64 loss: -0.5516586303710938
Batch 50/64 loss: -1.0830202102661133
Batch 51/64 loss: -1.0943584442138672
Batch 52/64 loss: -0.9319000244140625
Batch 53/64 loss: -1.1321773529052734
Batch 54/64 loss: -1.0508747100830078
Batch 55/64 loss: -0.5834112167358398
Batch 56/64 loss: -0.9853754043579102
Batch 57/64 loss: -1.0332136154174805
Batch 58/64 loss: -0.8976850509643555
Batch 59/64 loss: -0.8777942657470703
Batch 60/64 loss: -0.6915197372436523
Batch 61/64 loss: -0.9175376892089844
Batch 62/64 loss: -1.149683952331543
Batch 63/64 loss: -0.9501686096191406
Batch 64/64 loss: -5.369546413421631
Epoch 473  Train loss: -0.9374046419181076  Val loss: -0.9504480001443031
Epoch 474
-------------------------------
Batch 1/64 loss: -0.6803750991821289
Batch 2/64 loss: -0.8662796020507812
Batch 3/64 loss: -0.8819942474365234
Batch 4/64 loss: -0.48596763610839844
Batch 5/64 loss: -1.111093521118164
Batch 6/64 loss: -0.7941751480102539
Batch 7/64 loss: -0.9936189651489258
Batch 8/64 loss: -0.9307756423950195
Batch 9/64 loss: -1.1561288833618164
Batch 10/64 loss: -1.2336959838867188
Batch 11/64 loss: -0.6028032302856445
Batch 12/64 loss: -0.9865169525146484
Batch 13/64 loss: -1.3328275680541992
Batch 14/64 loss: -1.0254402160644531
Batch 15/64 loss: -0.5788536071777344
Batch 16/64 loss: -0.9550313949584961
Batch 17/64 loss: -1.1222238540649414
Batch 18/64 loss: -1.3159313201904297
Batch 19/64 loss: -0.7875852584838867
Batch 20/64 loss: -1.0963211059570312
Batch 21/64 loss: -1.2592573165893555
Batch 22/64 loss: -0.46079063415527344
Batch 23/64 loss: -1.2325754165649414
Batch 24/64 loss: -1.2543134689331055
Batch 25/64 loss: -1.3219327926635742
Batch 26/64 loss: -1.0050764083862305
Batch 27/64 loss: -0.9771490097045898
Batch 28/64 loss: -1.0867671966552734
Batch 29/64 loss: -0.6047754287719727
Batch 30/64 loss: -1.1860370635986328
Batch 31/64 loss: -0.9071245193481445
Batch 32/64 loss: -0.7289400100708008
Batch 33/64 loss: -0.9418058395385742
Batch 34/64 loss: -1.0773754119873047
Batch 35/64 loss: -0.8447389602661133
Batch 36/64 loss: -0.9817914962768555
Batch 37/64 loss: -1.060856819152832
Batch 38/64 loss: -1.415400505065918
Batch 39/64 loss: -1.3202924728393555
Batch 40/64 loss: -1.0035991668701172
Batch 41/64 loss: -1.1430330276489258
Batch 42/64 loss: -0.46442604064941406
Batch 43/64 loss: -0.6881523132324219
Batch 44/64 loss: -0.8879423141479492
Batch 45/64 loss: -0.4671592712402344
Batch 46/64 loss: -0.9645175933837891
Batch 47/64 loss: -1.4123477935791016
Batch 48/64 loss: -1.065800666809082
Batch 49/64 loss: -0.8595123291015625
Batch 50/64 loss: -1.1640939712524414
Batch 51/64 loss: -1.2638664245605469
Batch 52/64 loss: -1.1682605743408203
Batch 53/64 loss: -0.9862775802612305
Batch 54/64 loss: -0.544642448425293
Batch 55/64 loss: -1.1971254348754883
Batch 56/64 loss: -1.2573957443237305
Batch 57/64 loss: -0.5662956237792969
Batch 58/64 loss: -1.4068231582641602
Batch 59/64 loss: -1.1725969314575195
Batch 60/64 loss: -0.8889493942260742
Batch 61/64 loss: -1.0688915252685547
Batch 62/64 loss: -0.9240865707397461
Batch 63/64 loss: -0.8426980972290039
Batch 64/64 loss: -5.090982437133789
Epoch 474  Train loss: -1.0326489542044845  Val loss: -1.0454926376080595
Saving best model, epoch: 474
Epoch 475
-------------------------------
Batch 1/64 loss: -1.0635528564453125
Batch 2/64 loss: -0.7207651138305664
Batch 3/64 loss: -1.122450828552246
Batch 4/64 loss: -0.9198112487792969
Batch 5/64 loss: -0.7499141693115234
Batch 6/64 loss: -1.1534004211425781
Batch 7/64 loss: -1.1190557479858398
Batch 8/64 loss: -1.253274917602539
Batch 9/64 loss: -0.7411327362060547
Batch 10/64 loss: -1.027791976928711
Batch 11/64 loss: -1.2025985717773438
Batch 12/64 loss: -0.7020483016967773
Batch 13/64 loss: -1.0830488204956055
Batch 14/64 loss: -1.0005512237548828
Batch 15/64 loss: -0.5075645446777344
Batch 16/64 loss: -0.9142837524414062
Batch 17/64 loss: -1.1151866912841797
Batch 18/64 loss: -0.9360980987548828
Batch 19/64 loss: -1.050461769104004
Batch 20/64 loss: -1.0101652145385742
Batch 21/64 loss: -1.046731948852539
Batch 22/64 loss: -1.0566883087158203
Batch 23/64 loss: -1.078317642211914
Batch 24/64 loss: -0.7281265258789062
Batch 25/64 loss: -0.8726968765258789
Batch 26/64 loss: -0.6930913925170898
Batch 27/64 loss: -0.7303323745727539
Batch 28/64 loss: -0.7942581176757812
Batch 29/64 loss: -0.7913408279418945
Batch 30/64 loss: -1.3967046737670898
Batch 31/64 loss: -1.0263423919677734
Batch 32/64 loss: -1.1799802780151367
Batch 33/64 loss: -1.3580312728881836
Batch 34/64 loss: -0.8174953460693359
Batch 35/64 loss: -0.5179786682128906
Batch 36/64 loss: -1.1627607345581055
Batch 37/64 loss: -1.4587440490722656
Batch 38/64 loss: -0.7803030014038086
Batch 39/64 loss: -0.6761150360107422
Batch 40/64 loss: -0.8169307708740234
Batch 41/64 loss: -0.5340137481689453
Batch 42/64 loss: -1.166996955871582
Batch 43/64 loss: -0.8924942016601562
Batch 44/64 loss: -0.42775440216064453
Batch 45/64 loss: -0.961338996887207
Batch 46/64 loss: -1.0852174758911133
Batch 47/64 loss: -1.1468791961669922
Batch 48/64 loss: -0.9035739898681641
Batch 49/64 loss: -0.8493289947509766
Batch 50/64 loss: -1.0483989715576172
Batch 51/64 loss: -1.159658432006836
Batch 52/64 loss: -1.066157341003418
Batch 53/64 loss: -0.9446554183959961
Batch 54/64 loss: -0.8936643600463867
Batch 55/64 loss: -0.7472143173217773
Batch 56/64 loss: -1.2773046493530273
Batch 57/64 loss: -0.86175537109375
Batch 58/64 loss: -1.3039474487304688
Batch 59/64 loss: -0.924290657043457
Batch 60/64 loss: -1.1663541793823242
Batch 61/64 loss: -0.9692220687866211
Batch 62/64 loss: -0.8869857788085938
Batch 63/64 loss: -1.037825584411621
Batch 64/64 loss: -5.412176132202148
Epoch 475  Train loss: -1.0147496765735102  Val loss: -1.0098087402553493
Epoch 476
-------------------------------
Batch 1/64 loss: -0.9245576858520508
Batch 2/64 loss: -1.106562614440918
Batch 3/64 loss: -1.2000083923339844
Batch 4/64 loss: -0.9728708267211914
Batch 5/64 loss: -0.6792335510253906
Batch 6/64 loss: -0.5839662551879883
Batch 7/64 loss: -0.9054164886474609
Batch 8/64 loss: -0.9353866577148438
Batch 9/64 loss: -1.3195133209228516
Batch 10/64 loss: -0.8001585006713867
Batch 11/64 loss: -1.159952163696289
Batch 12/64 loss: -0.866002082824707
Batch 13/64 loss: -0.9002304077148438
Batch 14/64 loss: -1.1927413940429688
Batch 15/64 loss: -0.628809928894043
Batch 16/64 loss: -1.0761480331420898
Batch 17/64 loss: -0.7783546447753906
Batch 18/64 loss: -0.8791799545288086
Batch 19/64 loss: -0.9886903762817383
Batch 20/64 loss: -1.100595474243164
Batch 21/64 loss: -1.1545639038085938
Batch 22/64 loss: -1.3115415573120117
Batch 23/64 loss: -0.9931249618530273
Batch 24/64 loss: -1.1581964492797852
Batch 25/64 loss: -1.1831302642822266
Batch 26/64 loss: -1.046736717224121
Batch 27/64 loss: -0.9080266952514648
Batch 28/64 loss: -1.1792526245117188
Batch 29/64 loss: -1.0921564102172852
Batch 30/64 loss: -1.0755205154418945
Batch 31/64 loss: -1.104665756225586
Batch 32/64 loss: -0.24016857147216797
Batch 33/64 loss: -0.8357524871826172
Batch 34/64 loss: -1.3611974716186523
Batch 35/64 loss: -1.4300193786621094
Batch 36/64 loss: -1.1226310729980469
Batch 37/64 loss: -0.9970788955688477
Batch 38/64 loss: -1.1714324951171875
Batch 39/64 loss: -0.9277238845825195
Batch 40/64 loss: -1.10760498046875
Batch 41/64 loss: -1.1719341278076172
Batch 42/64 loss: -1.2316837310791016
Batch 43/64 loss: -0.8770856857299805
Batch 44/64 loss: -1.0050878524780273
Batch 45/64 loss: -1.253469467163086
Batch 46/64 loss: -0.9121999740600586
Batch 47/64 loss: -0.9273490905761719
Batch 48/64 loss: -0.8139686584472656
Batch 49/64 loss: -0.9617223739624023
Batch 50/64 loss: -0.5832691192626953
Batch 51/64 loss: -1.0989046096801758
Batch 52/64 loss: -1.1007499694824219
Batch 53/64 loss: -0.8792552947998047
Batch 54/64 loss: -1.1672191619873047
Batch 55/64 loss: -1.0604829788208008
Batch 56/64 loss: -1.1921138763427734
Batch 57/64 loss: -1.3053550720214844
Batch 58/64 loss: -0.7666330337524414
Batch 59/64 loss: -0.7456998825073242
Batch 60/64 loss: -1.0335302352905273
Batch 61/64 loss: -1.0031518936157227
Batch 62/64 loss: -0.7359828948974609
Batch 63/64 loss: -1.2599430084228516
Batch 64/64 loss: -5.2776994705200195
Epoch 476  Train loss: -1.0579446343814625  Val loss: -1.0255468571718616
Epoch 477
-------------------------------
Batch 1/64 loss: -1.0477914810180664
Batch 2/64 loss: -1.2754364013671875
Batch 3/64 loss: -0.819488525390625
Batch 4/64 loss: -1.194209098815918
Batch 5/64 loss: -1.1366539001464844
Batch 6/64 loss: -0.8968753814697266
Batch 7/64 loss: -0.975189208984375
Batch 8/64 loss: -1.008650779724121
Batch 9/64 loss: -0.9685478210449219
Batch 10/64 loss: -1.015110969543457
Batch 11/64 loss: -0.9473295211791992
Batch 12/64 loss: -1.0805234909057617
Batch 13/64 loss: -1.0631704330444336
Batch 14/64 loss: -1.1425361633300781
Batch 15/64 loss: -1.0117168426513672
Batch 16/64 loss: -0.5255937576293945
Batch 17/64 loss: -0.9577503204345703
Batch 18/64 loss: -1.1534795761108398
Batch 19/64 loss: -1.069016456604004
Batch 20/64 loss: -1.2600021362304688
Batch 21/64 loss: -0.7434320449829102
Batch 22/64 loss: -1.4416618347167969
Batch 23/64 loss: -1.1588315963745117
Batch 24/64 loss: -0.8224163055419922
Batch 25/64 loss: -1.0286073684692383
Batch 26/64 loss: -1.298954963684082
Batch 27/64 loss: -1.015242576599121
Batch 28/64 loss: -1.0969066619873047
Batch 29/64 loss: -0.8018703460693359
Batch 30/64 loss: -0.5604791641235352
Batch 31/64 loss: -0.5317468643188477
Batch 32/64 loss: -0.8270692825317383
Batch 33/64 loss: -1.101973533630371
Batch 34/64 loss: -0.5194234848022461
Batch 35/64 loss: -1.1378507614135742
Batch 36/64 loss: -1.1255207061767578
Batch 37/64 loss: -0.5550642013549805
Batch 38/64 loss: -1.3488073348999023
Batch 39/64 loss: -0.9340858459472656
Batch 40/64 loss: -1.1826152801513672
Batch 41/64 loss: -1.1171073913574219
Batch 42/64 loss: -0.9602394104003906
Batch 43/64 loss: -1.0801506042480469
Batch 44/64 loss: -0.9836044311523438
Batch 45/64 loss: -0.9911699295043945
Batch 46/64 loss: -1.1664142608642578
Batch 47/64 loss: -0.44865989685058594
Batch 48/64 loss: -0.7382926940917969
Batch 49/64 loss: -1.236037254333496
Batch 50/64 loss: -1.1208229064941406
Batch 51/64 loss: -0.7309637069702148
Batch 52/64 loss: -0.8129415512084961
Batch 53/64 loss: -1.20635986328125
Batch 54/64 loss: -0.9159755706787109
Batch 55/64 loss: -0.9507055282592773
Batch 56/64 loss: -1.3241682052612305
Batch 57/64 loss: -1.3558807373046875
Batch 58/64 loss: -1.0178594589233398
Batch 59/64 loss: -0.25104713439941406
Batch 60/64 loss: -1.010828971862793
Batch 61/64 loss: -1.0693655014038086
Batch 62/64 loss: -0.9295597076416016
Batch 63/64 loss: -0.8354587554931641
Batch 64/64 loss: -5.468527793884277
Epoch 477  Train loss: -1.0374375175027286  Val loss: -0.9781763791218656
Epoch 478
-------------------------------
Batch 1/64 loss: -1.1751976013183594
Batch 2/64 loss: -1.0862483978271484
Batch 3/64 loss: -0.9928121566772461
Batch 4/64 loss: -1.0106792449951172
Batch 5/64 loss: -0.3752880096435547
Batch 6/64 loss: -1.1709985733032227
Batch 7/64 loss: -0.4621124267578125
Batch 8/64 loss: -1.3548507690429688
Batch 9/64 loss: -0.6766119003295898
Batch 10/64 loss: -0.8594646453857422
Batch 11/64 loss: -0.7231073379516602
Batch 12/64 loss: -1.0916862487792969
Batch 13/64 loss: -1.0919427871704102
Batch 14/64 loss: -1.2589521408081055
Batch 15/64 loss: -1.1751775741577148
Batch 16/64 loss: -0.9215879440307617
Batch 17/64 loss: -1.2996158599853516
Batch 18/64 loss: -0.8557682037353516
Batch 19/64 loss: -0.8940553665161133
Batch 20/64 loss: -0.9737701416015625
Batch 21/64 loss: -1.3781919479370117
Batch 22/64 loss: -0.765472412109375
Batch 23/64 loss: -1.2340679168701172
Batch 24/64 loss: -1.005819320678711
Batch 25/64 loss: -0.5237112045288086
Batch 26/64 loss: -0.9340648651123047
Batch 27/64 loss: -1.2471418380737305
Batch 28/64 loss: -1.0202674865722656
Batch 29/64 loss: -0.8864870071411133
Batch 30/64 loss: -0.8154754638671875
Batch 31/64 loss: -1.2466306686401367
Batch 32/64 loss: -1.2122278213500977
Batch 33/64 loss: -0.8109846115112305
Batch 34/64 loss: -0.9239311218261719
Batch 35/64 loss: -1.271993637084961
Batch 36/64 loss: -0.9377498626708984
Batch 37/64 loss: -1.0555267333984375
Batch 38/64 loss: -1.157435417175293
Batch 39/64 loss: -0.8867006301879883
Batch 40/64 loss: -0.9962062835693359
Batch 41/64 loss: -1.1636133193969727
Batch 42/64 loss: -1.123337745666504
Batch 43/64 loss: -0.542205810546875
Batch 44/64 loss: -1.0693979263305664
Batch 45/64 loss: -1.0908260345458984
Batch 46/64 loss: -1.229020118713379
Batch 47/64 loss: -0.7210369110107422
Batch 48/64 loss: -1.327606201171875
Batch 49/64 loss: -1.1048908233642578
Batch 50/64 loss: -1.058797836303711
Batch 51/64 loss: -1.4219417572021484
Batch 52/64 loss: -1.1957588195800781
Batch 53/64 loss: -0.7660799026489258
Batch 54/64 loss: -0.7191915512084961
Batch 55/64 loss: -0.4489622116088867
Batch 56/64 loss: -0.9241914749145508
Batch 57/64 loss: -1.00811767578125
Batch 58/64 loss: -1.0245952606201172
Batch 59/64 loss: -1.2295331954956055
Batch 60/64 loss: -1.2439908981323242
Batch 61/64 loss: -1.0711908340454102
Batch 62/64 loss: -0.5508155822753906
Batch 63/64 loss: -1.3873462677001953
Batch 64/64 loss: -5.263160705566406
Epoch 478  Train loss: -1.0530169767491957  Val loss: -0.9559544566570688
Epoch 479
-------------------------------
Batch 1/64 loss: -1.1284990310668945
Batch 2/64 loss: -0.9757413864135742
Batch 3/64 loss: -1.0810956954956055
Batch 4/64 loss: -0.8932867050170898
Batch 5/64 loss: -1.255448341369629
Batch 6/64 loss: -1.1003732681274414
Batch 7/64 loss: -1.082895278930664
Batch 8/64 loss: -0.636265754699707
Batch 9/64 loss: -1.1002883911132812
Batch 10/64 loss: -1.0228948593139648
Batch 11/64 loss: -0.9898862838745117
Batch 12/64 loss: -1.1327409744262695
Batch 13/64 loss: -0.7497024536132812
Batch 14/64 loss: -0.8854513168334961
Batch 15/64 loss: -0.9818572998046875
Batch 16/64 loss: -1.350484848022461
Batch 17/64 loss: -0.9204063415527344
Batch 18/64 loss: -0.6879091262817383
Batch 19/64 loss: -0.9157123565673828
Batch 20/64 loss: -1.1058015823364258
Batch 21/64 loss: -1.0797405242919922
Batch 22/64 loss: -0.9419841766357422
Batch 23/64 loss: -1.1704530715942383
Batch 24/64 loss: -0.796055793762207
Batch 25/64 loss: -1.1791505813598633
Batch 26/64 loss: -1.3611488342285156
Batch 27/64 loss: -1.154439926147461
Batch 28/64 loss: -1.380472183227539
Batch 29/64 loss: -1.1078310012817383
Batch 30/64 loss: -1.1710634231567383
Batch 31/64 loss: -0.6624994277954102
Batch 32/64 loss: -1.069188117980957
Batch 33/64 loss: -0.9566411972045898
Batch 34/64 loss: -0.755162239074707
Batch 35/64 loss: -1.0967893600463867
Batch 36/64 loss: -1.248758316040039
Batch 37/64 loss: -0.765711784362793
Batch 38/64 loss: -0.9094171524047852
Batch 39/64 loss: -0.7549095153808594
Batch 40/64 loss: -1.2204856872558594
Batch 41/64 loss: -0.977229118347168
Batch 42/64 loss: -0.9065589904785156
Batch 43/64 loss: -0.8113975524902344
Batch 44/64 loss: -0.8233642578125
Batch 45/64 loss: -0.9705295562744141
Batch 46/64 loss: -0.8195343017578125
Batch 47/64 loss: -0.6299962997436523
Batch 48/64 loss: -1.2824888229370117
Batch 49/64 loss: -1.1392555236816406
Batch 50/64 loss: -0.5908041000366211
Batch 51/64 loss: -1.0400094985961914
Batch 52/64 loss: -1.236856460571289
Batch 53/64 loss: -0.7016706466674805
Batch 54/64 loss: -1.0263032913208008
Batch 55/64 loss: -1.1341571807861328
Batch 56/64 loss: -1.289597511291504
Batch 57/64 loss: 0.024382591247558594
Batch 58/64 loss: -1.1990242004394531
Batch 59/64 loss: -0.9177160263061523
Batch 60/64 loss: -0.6819972991943359
Batch 61/64 loss: -1.2546939849853516
Batch 62/64 loss: -1.041764259338379
Batch 63/64 loss: -1.1027183532714844
Batch 64/64 loss: -4.778891563415527
Epoch 479  Train loss: -1.033977990991929  Val loss: -0.9311178934942815
Epoch 480
-------------------------------
Batch 1/64 loss: -1.0181083679199219
Batch 2/64 loss: -1.404646873474121
Batch 3/64 loss: -0.9302740097045898
Batch 4/64 loss: -0.6958837509155273
Batch 5/64 loss: -1.2883853912353516
Batch 6/64 loss: -1.3263349533081055
Batch 7/64 loss: -0.8257102966308594
Batch 8/64 loss: -1.4243688583374023
Batch 9/64 loss: -0.8428058624267578
Batch 10/64 loss: -0.946660041809082
Batch 11/64 loss: -1.411280632019043
Batch 12/64 loss: -1.2569160461425781
Batch 13/64 loss: -1.0775718688964844
Batch 14/64 loss: -0.8201208114624023
Batch 15/64 loss: -0.29778575897216797
Batch 16/64 loss: -1.1448707580566406
Batch 17/64 loss: -0.5432910919189453
Batch 18/64 loss: -1.365610122680664
Batch 19/64 loss: -0.6958465576171875
Batch 20/64 loss: -1.3413753509521484
Batch 21/64 loss: -1.279409408569336
Batch 22/64 loss: -1.085897445678711
Batch 23/64 loss: -0.4777097702026367
Batch 24/64 loss: -0.8956584930419922
Batch 25/64 loss: -0.8981037139892578
Batch 26/64 loss: -1.2673053741455078
Batch 27/64 loss: -1.0114860534667969
Batch 28/64 loss: -0.9404659271240234
Batch 29/64 loss: -0.8182401657104492
Batch 30/64 loss: -1.1245336532592773
Batch 31/64 loss: -0.9322099685668945
Batch 32/64 loss: -1.1119756698608398
Batch 33/64 loss: -1.2588224411010742
Batch 34/64 loss: -0.9373617172241211
Batch 35/64 loss: -0.9599275588989258
Batch 36/64 loss: -1.1905765533447266
Batch 37/64 loss: -1.0586729049682617
Batch 38/64 loss: -1.1794548034667969
Batch 39/64 loss: -1.2388696670532227
Batch 40/64 loss: -1.1120872497558594
Batch 41/64 loss: -1.3690900802612305
Batch 42/64 loss: -1.055246353149414
Batch 43/64 loss: -1.3222179412841797
Batch 44/64 loss: -0.8997697830200195
Batch 45/64 loss: -1.1935453414916992
Batch 46/64 loss: -1.1077337265014648
Batch 47/64 loss: -0.9301424026489258
Batch 48/64 loss: -1.1760692596435547
Batch 49/64 loss: -0.7093000411987305
Batch 50/64 loss: -1.3346242904663086
Batch 51/64 loss: -1.1892213821411133
Batch 52/64 loss: -1.3133420944213867
Batch 53/64 loss: -1.0601816177368164
Batch 54/64 loss: -0.5334014892578125
Batch 55/64 loss: -1.309286117553711
Batch 56/64 loss: -1.2166757583618164
Batch 57/64 loss: -1.050663948059082
Batch 58/64 loss: -0.7539577484130859
Batch 59/64 loss: -1.0380659103393555
Batch 60/64 loss: -1.2733087539672852
Batch 61/64 loss: -1.1374187469482422
Batch 62/64 loss: -0.6924924850463867
Batch 63/64 loss: -0.5544919967651367
Batch 64/64 loss: -5.619669437408447
Epoch 480  Train loss: -1.09602533789242  Val loss: -0.9993965306232885
Epoch 481
-------------------------------
Batch 1/64 loss: -1.079298973083496
Batch 2/64 loss: -0.9727354049682617
Batch 3/64 loss: -0.7326765060424805
Batch 4/64 loss: -1.0936145782470703
Batch 5/64 loss: -0.982630729675293
Batch 6/64 loss: -1.2225170135498047
Batch 7/64 loss: -1.2371644973754883
Batch 8/64 loss: -1.202524185180664
Batch 9/64 loss: -1.0494575500488281
Batch 10/64 loss: -1.1791563034057617
Batch 11/64 loss: -1.069753646850586
Batch 12/64 loss: -0.8156156539916992
Batch 13/64 loss: -1.2937145233154297
Batch 14/64 loss: -1.0110702514648438
Batch 15/64 loss: -0.92523193359375
Batch 16/64 loss: -0.9448823928833008
Batch 17/64 loss: -1.2855749130249023
Batch 18/64 loss: -0.9595575332641602
Batch 19/64 loss: -1.068197250366211
Batch 20/64 loss: -1.4400854110717773
Batch 21/64 loss: -1.1031036376953125
Batch 22/64 loss: -1.0983467102050781
Batch 23/64 loss: -1.3335018157958984
Batch 24/64 loss: -1.1149005889892578
Batch 25/64 loss: -1.132929801940918
Batch 26/64 loss: -0.9216241836547852
Batch 27/64 loss: -1.0798912048339844
Batch 28/64 loss: -0.4797630310058594
Batch 29/64 loss: -1.3767509460449219
Batch 30/64 loss: -1.132309913635254
Batch 31/64 loss: -1.2661113739013672
Batch 32/64 loss: -0.9752712249755859
Batch 33/64 loss: -1.5007038116455078
Batch 34/64 loss: -1.1843395233154297
Batch 35/64 loss: -1.2661819458007812
Batch 36/64 loss: -0.8650217056274414
Batch 37/64 loss: -1.1732416152954102
Batch 38/64 loss: -1.3845100402832031
Batch 39/64 loss: -1.3433609008789062
Batch 40/64 loss: -1.046426773071289
Batch 41/64 loss: -0.9588527679443359
Batch 42/64 loss: -0.939366340637207
Batch 43/64 loss: -1.1797904968261719
Batch 44/64 loss: -1.1908674240112305
Batch 45/64 loss: -0.8648529052734375
Batch 46/64 loss: -1.176457405090332
Batch 47/64 loss: -1.0606565475463867
Batch 48/64 loss: -0.9397811889648438
Batch 49/64 loss: -0.80169677734375
Batch 50/64 loss: -1.0522279739379883
Batch 51/64 loss: -0.676936149597168
Batch 52/64 loss: -0.8261804580688477
Batch 53/64 loss: -0.6916475296020508
Batch 54/64 loss: -1.2778816223144531
Batch 55/64 loss: -0.9467573165893555
Batch 56/64 loss: -0.6258211135864258
Batch 57/64 loss: -0.9898958206176758
Batch 58/64 loss: -1.0760736465454102
Batch 59/64 loss: -0.8393039703369141
Batch 60/64 loss: -0.9459438323974609
Batch 61/64 loss: -1.338761329650879
Batch 62/64 loss: -1.0515661239624023
Batch 63/64 loss: -1.3468666076660156
Batch 64/64 loss: -4.816734790802002
Epoch 481  Train loss: -1.1098742971233293  Val loss: -1.063501482567017
Saving best model, epoch: 481
Epoch 482
-------------------------------
Batch 1/64 loss: -1.0542516708374023
Batch 2/64 loss: -0.7046585083007812
Batch 3/64 loss: -1.0750846862792969
Batch 4/64 loss: -1.1820087432861328
Batch 5/64 loss: -1.0033302307128906
Batch 6/64 loss: -1.0738763809204102
Batch 7/64 loss: -1.0371789932250977
Batch 8/64 loss: -1.1152191162109375
Batch 9/64 loss: -1.1020174026489258
Batch 10/64 loss: -0.45843982696533203
Batch 11/64 loss: -0.9843292236328125
Batch 12/64 loss: -1.3357429504394531
Batch 13/64 loss: -1.1217422485351562
Batch 14/64 loss: -1.1549949645996094
Batch 15/64 loss: -1.1462211608886719
Batch 16/64 loss: -1.2373476028442383
Batch 17/64 loss: -1.1178913116455078
Batch 18/64 loss: -1.2925024032592773
Batch 19/64 loss: -1.3446578979492188
Batch 20/64 loss: -0.9614067077636719
Batch 21/64 loss: -1.2870569229125977
Batch 22/64 loss: -0.49744606018066406
Batch 23/64 loss: -0.5435218811035156
Batch 24/64 loss: -1.0270233154296875
Batch 25/64 loss: -0.9213008880615234
Batch 26/64 loss: -1.3310737609863281
Batch 27/64 loss: -1.2181463241577148
Batch 28/64 loss: -1.0456171035766602
Batch 29/64 loss: -0.9057626724243164
Batch 30/64 loss: -1.229447364807129
Batch 31/64 loss: -1.2973318099975586
Batch 32/64 loss: -1.0423002243041992
Batch 33/64 loss: -0.6258554458618164
Batch 34/64 loss: -1.1681222915649414
Batch 35/64 loss: -0.9199075698852539
Batch 36/64 loss: -0.7705392837524414
Batch 37/64 loss: -0.7173881530761719
Batch 38/64 loss: -1.3222360610961914
Batch 39/64 loss: -0.9106836318969727
Batch 40/64 loss: -1.210479736328125
Batch 41/64 loss: -0.6087503433227539
Batch 42/64 loss: -0.80206298828125
Batch 43/64 loss: -0.9629926681518555
Batch 44/64 loss: -0.9563465118408203
Batch 45/64 loss: -1.29638671875
Batch 46/64 loss: -1.117823600769043
Batch 47/64 loss: -1.2058544158935547
Batch 48/64 loss: -1.1630020141601562
Batch 49/64 loss: -0.8913755416870117
Batch 50/64 loss: -1.1858510971069336
Batch 51/64 loss: -1.0162553787231445
Batch 52/64 loss: -0.9625453948974609
Batch 53/64 loss: -1.045475959777832
Batch 54/64 loss: -1.180302619934082
Batch 55/64 loss: -1.1570920944213867
Batch 56/64 loss: -1.0631542205810547
Batch 57/64 loss: -0.9592571258544922
Batch 58/64 loss: -1.2043285369873047
Batch 59/64 loss: -0.6181058883666992
Batch 60/64 loss: -1.1687545776367188
Batch 61/64 loss: -0.7450885772705078
Batch 62/64 loss: -0.6871223449707031
Batch 63/64 loss: -0.9889612197875977
Batch 64/64 loss: -5.4805402755737305
Epoch 482  Train loss: -1.0759441188737457  Val loss: -1.038285586432493
Epoch 483
-------------------------------
Batch 1/64 loss: -1.1236200332641602
Batch 2/64 loss: -0.9708833694458008
Batch 3/64 loss: -0.9713239669799805
Batch 4/64 loss: -1.0193090438842773
Batch 5/64 loss: -1.2348756790161133
Batch 6/64 loss: -1.1199989318847656
Batch 7/64 loss: -0.7855243682861328
Batch 8/64 loss: -1.2367992401123047
Batch 9/64 loss: -1.2689704895019531
Batch 10/64 loss: -1.0627660751342773
Batch 11/64 loss: -1.2717418670654297
Batch 12/64 loss: -0.8715991973876953
Batch 13/64 loss: -1.3664436340332031
Batch 14/64 loss: -1.143798828125
Batch 15/64 loss: -1.1222381591796875
Batch 16/64 loss: -0.8316574096679688
Batch 17/64 loss: -0.7922763824462891
Batch 18/64 loss: -1.1979522705078125
Batch 19/64 loss: -0.5478610992431641
Batch 20/64 loss: -1.0501594543457031
Batch 21/64 loss: -1.1236801147460938
Batch 22/64 loss: -0.9211063385009766
Batch 23/64 loss: -0.9886579513549805
Batch 24/64 loss: -1.0382051467895508
Batch 25/64 loss: -0.8827915191650391
Batch 26/64 loss: -0.5437240600585938
Batch 27/64 loss: -1.138406753540039
Batch 28/64 loss: -1.0343503952026367
Batch 29/64 loss: -1.3758554458618164
Batch 30/64 loss: -1.0191268920898438
Batch 31/64 loss: -1.0864410400390625
Batch 32/64 loss: -1.2399835586547852
Batch 33/64 loss: -0.6524133682250977
Batch 34/64 loss: -1.2808856964111328
Batch 35/64 loss: -1.1274900436401367
Batch 36/64 loss: -1.1776666641235352
Batch 37/64 loss: -0.9667263031005859
Batch 38/64 loss: -0.9826526641845703
Batch 39/64 loss: -0.9347047805786133
Batch 40/64 loss: -0.7999477386474609
Batch 41/64 loss: -1.0453243255615234
Batch 42/64 loss: -0.7470827102661133
Batch 43/64 loss: -1.0022039413452148
Batch 44/64 loss: -1.3602666854858398
Batch 45/64 loss: -1.311202049255371
Batch 46/64 loss: -0.7735042572021484
Batch 47/64 loss: -0.9965877532958984
Batch 48/64 loss: -0.9788761138916016
Batch 49/64 loss: -0.6035337448120117
Batch 50/64 loss: -1.0936155319213867
Batch 51/64 loss: -0.9470710754394531
Batch 52/64 loss: -1.0503807067871094
Batch 53/64 loss: -1.137603759765625
Batch 54/64 loss: -1.0998706817626953
Batch 55/64 loss: -1.2479486465454102
Batch 56/64 loss: -1.0346202850341797
Batch 57/64 loss: -0.961090087890625
Batch 58/64 loss: -1.309737205505371
Batch 59/64 loss: -1.0511951446533203
Batch 60/64 loss: -0.7589845657348633
Batch 61/64 loss: -0.9776878356933594
Batch 62/64 loss: -0.8652229309082031
Batch 63/64 loss: -1.2816381454467773
Batch 64/64 loss: -5.092902183532715
Epoch 483  Train loss: -1.0785810320985083  Val loss: -1.0538623586962723
Epoch 484
-------------------------------
Batch 1/64 loss: -1.2695674896240234
Batch 2/64 loss: -0.6406240463256836
Batch 3/64 loss: -0.9116306304931641
Batch 4/64 loss: -0.7870569229125977
Batch 5/64 loss: -1.0638618469238281
Batch 6/64 loss: -1.2258548736572266
Batch 7/64 loss: -1.4147491455078125
Batch 8/64 loss: -0.8408746719360352
Batch 9/64 loss: -0.8903465270996094
Batch 10/64 loss: -1.0793581008911133
Batch 11/64 loss: -1.273604393005371
Batch 12/64 loss: -0.7134504318237305
Batch 13/64 loss: -1.2477788925170898
Batch 14/64 loss: -1.009251594543457
Batch 15/64 loss: -1.087325096130371
Batch 16/64 loss: -0.9521312713623047
Batch 17/64 loss: -0.9279422760009766
Batch 18/64 loss: -0.7520990371704102
Batch 19/64 loss: -0.7962455749511719
Batch 20/64 loss: -0.760462760925293
Batch 21/64 loss: -1.1286191940307617
Batch 22/64 loss: -1.4435100555419922
Batch 23/64 loss: -1.4241199493408203
Batch 24/64 loss: -1.0647172927856445
Batch 25/64 loss: -1.0344219207763672
Batch 26/64 loss: -0.9199762344360352
Batch 27/64 loss: -0.8396940231323242
Batch 28/64 loss: -1.281235694885254
Batch 29/64 loss: -1.04327392578125
Batch 30/64 loss: -0.9674215316772461
Batch 31/64 loss: -1.1666450500488281
Batch 32/64 loss: -1.0533342361450195
Batch 33/64 loss: -0.8782815933227539
Batch 34/64 loss: -1.0513477325439453
Batch 35/64 loss: -0.9829072952270508
Batch 36/64 loss: -1.0641670227050781
Batch 37/64 loss: -0.7193059921264648
Batch 38/64 loss: -1.2426471710205078
Batch 39/64 loss: -1.044407844543457
Batch 40/64 loss: -0.996211051940918
Batch 41/64 loss: -1.0222673416137695
Batch 42/64 loss: -1.1645221710205078
Batch 43/64 loss: -0.8327951431274414
Batch 44/64 loss: -0.781702995300293
Batch 45/64 loss: -1.0036029815673828
Batch 46/64 loss: -1.0420026779174805
Batch 47/64 loss: -1.0708856582641602
Batch 48/64 loss: -1.3626298904418945
Batch 49/64 loss: -0.9174947738647461
Batch 50/64 loss: -1.0201215744018555
Batch 51/64 loss: -1.2129106521606445
Batch 52/64 loss: -0.6780405044555664
Batch 53/64 loss: -0.9088935852050781
Batch 54/64 loss: -0.96502685546875
Batch 55/64 loss: -1.195927619934082
Batch 56/64 loss: -1.0249433517456055
Batch 57/64 loss: -1.2702484130859375
Batch 58/64 loss: -1.2893486022949219
Batch 59/64 loss: -1.2863645553588867
Batch 60/64 loss: -0.9758749008178711
Batch 61/64 loss: -1.2231483459472656
Batch 62/64 loss: -1.252401351928711
Batch 63/64 loss: -0.6663427352905273
Batch 64/64 loss: -5.264435768127441
Epoch 484  Train loss: -1.084020139656815  Val loss: -1.0731631144625216
Saving best model, epoch: 484
Epoch 485
-------------------------------
Batch 1/64 loss: -1.2213115692138672
Batch 2/64 loss: -0.8875551223754883
Batch 3/64 loss: -1.1988344192504883
Batch 4/64 loss: -1.0822210311889648
Batch 5/64 loss: -1.2244491577148438
Batch 6/64 loss: -1.3956222534179688
Batch 7/64 loss: -1.2708005905151367
Batch 8/64 loss: -1.1244678497314453
Batch 9/64 loss: -1.2314023971557617
Batch 10/64 loss: -1.0139436721801758
Batch 11/64 loss: -0.8599605560302734
Batch 12/64 loss: -0.8583316802978516
Batch 13/64 loss: -1.1401243209838867
Batch 14/64 loss: -1.5336523056030273
Batch 15/64 loss: -0.9959945678710938
Batch 16/64 loss: -0.903533935546875
Batch 17/64 loss: -0.7929744720458984
Batch 18/64 loss: -1.2287473678588867
Batch 19/64 loss: -0.9273290634155273
Batch 20/64 loss: -0.810847282409668
Batch 21/64 loss: -0.7677888870239258
Batch 22/64 loss: -1.0063457489013672
Batch 23/64 loss: -1.1415328979492188
Batch 24/64 loss: -1.0951309204101562
Batch 25/64 loss: -1.2732572555541992
Batch 26/64 loss: -1.3056211471557617
Batch 27/64 loss: -1.022242546081543
Batch 28/64 loss: -0.929356575012207
Batch 29/64 loss: -1.232198715209961
Batch 30/64 loss: -1.1662216186523438
Batch 31/64 loss: -1.4203577041625977
Batch 32/64 loss: -1.2286300659179688
Batch 33/64 loss: -1.0436124801635742
Batch 34/64 loss: -1.0404014587402344
Batch 35/64 loss: -0.3249063491821289
Batch 36/64 loss: -1.056889533996582
Batch 37/64 loss: -0.927337646484375
Batch 38/64 loss: -0.6491575241088867
Batch 39/64 loss: -1.0648183822631836
Batch 40/64 loss: -1.2471179962158203
Batch 41/64 loss: -0.7971572875976562
Batch 42/64 loss: -1.2258281707763672
Batch 43/64 loss: -0.7925100326538086
Batch 44/64 loss: -0.6410646438598633
Batch 45/64 loss: -1.1504478454589844
Batch 46/64 loss: -1.0068111419677734
Batch 47/64 loss: -1.1969232559204102
Batch 48/64 loss: -1.1913948059082031
Batch 49/64 loss: -0.9300556182861328
Batch 50/64 loss: -0.9535379409790039
Batch 51/64 loss: -1.0552997589111328
Batch 52/64 loss: -0.8872346878051758
Batch 53/64 loss: -0.9817237854003906
Batch 54/64 loss: -1.1086254119873047
Batch 55/64 loss: -0.6955280303955078
Batch 56/64 loss: -1.4062480926513672
Batch 57/64 loss: -1.075876235961914
Batch 58/64 loss: -0.9353046417236328
Batch 59/64 loss: -1.0374956130981445
Batch 60/64 loss: -0.7774877548217773
Batch 61/64 loss: -1.2788419723510742
Batch 62/64 loss: -1.2346315383911133
Batch 63/64 loss: -1.1693716049194336
Batch 64/64 loss: -4.753825664520264
Epoch 485  Train loss: -1.0939576186385809  Val loss: -0.9718622292849616
Epoch 486
-------------------------------
Batch 1/64 loss: -0.8033256530761719
Batch 2/64 loss: -1.1445856094360352
Batch 3/64 loss: -1.1876325607299805
Batch 4/64 loss: -1.2927064895629883
Batch 5/64 loss: -1.3155155181884766
Batch 6/64 loss: -1.0396289825439453
Batch 7/64 loss: -1.2283029556274414
Batch 8/64 loss: -0.560612678527832
Batch 9/64 loss: -1.1896991729736328
Batch 10/64 loss: -1.3884353637695312
Batch 11/64 loss: -0.8271770477294922
Batch 12/64 loss: -1.1492347717285156
Batch 13/64 loss: -1.2491216659545898
Batch 14/64 loss: -0.7292909622192383
Batch 15/64 loss: -1.4378671646118164
Batch 16/64 loss: -0.6317234039306641
Batch 17/64 loss: -1.271885871887207
Batch 18/64 loss: -1.335611343383789
Batch 19/64 loss: -1.2392168045043945
Batch 20/64 loss: -1.083648681640625
Batch 21/64 loss: -1.2617549896240234
Batch 22/64 loss: -1.1423912048339844
Batch 23/64 loss: -1.3214998245239258
Batch 24/64 loss: -1.1235589981079102
Batch 25/64 loss: -0.8095312118530273
Batch 26/64 loss: -1.0836524963378906
Batch 27/64 loss: -1.345794677734375
Batch 28/64 loss: -1.2103700637817383
Batch 29/64 loss: -0.9059247970581055
Batch 30/64 loss: -0.8861160278320312
Batch 31/64 loss: -1.2581720352172852
Batch 32/64 loss: -0.8380413055419922
Batch 33/64 loss: -1.0199575424194336
Batch 34/64 loss: -0.8805484771728516
Batch 35/64 loss: -1.2245187759399414
Batch 36/64 loss: -0.4115266799926758
Batch 37/64 loss: -1.1407527923583984
Batch 38/64 loss: -0.9235830307006836
Batch 39/64 loss: -0.7408742904663086
Batch 40/64 loss: -1.0738143920898438
Batch 41/64 loss: -0.8875322341918945
Batch 42/64 loss: -0.8277158737182617
Batch 43/64 loss: -0.9243965148925781
Batch 44/64 loss: -1.051854133605957
Batch 45/64 loss: -1.2962779998779297
Batch 46/64 loss: -0.9797801971435547
Batch 47/64 loss: -0.7746782302856445
Batch 48/64 loss: -0.6422386169433594
Batch 49/64 loss: -0.9188756942749023
Batch 50/64 loss: -0.9321794509887695
Batch 51/64 loss: -1.3686952590942383
Batch 52/64 loss: -0.9554367065429688
Batch 53/64 loss: -0.7015924453735352
Batch 54/64 loss: -0.6188068389892578
Batch 55/64 loss: -0.866755485534668
Batch 56/64 loss: -1.2617712020874023
Batch 57/64 loss: -1.0185661315917969
Batch 58/64 loss: -0.8584518432617188
Batch 59/64 loss: -1.0419321060180664
Batch 60/64 loss: -1.0824394226074219
Batch 61/64 loss: -0.7519426345825195
Batch 62/64 loss: -1.076645851135254
Batch 63/64 loss: -1.010446548461914
Batch 64/64 loss: -5.327117919921875
Epoch 486  Train loss: -1.0753248027726716  Val loss: -1.0439011747484763
Epoch 487
-------------------------------
Batch 1/64 loss: -0.8149633407592773
Batch 2/64 loss: -1.1709175109863281
Batch 3/64 loss: -1.1543865203857422
Batch 4/64 loss: -0.9782533645629883
Batch 5/64 loss: -1.0172643661499023
Batch 6/64 loss: -1.1783227920532227
Batch 7/64 loss: -0.9320106506347656
Batch 8/64 loss: -1.2234487533569336
Batch 9/64 loss: -1.3490180969238281
Batch 10/64 loss: -1.3169116973876953
Batch 11/64 loss: -1.123042106628418
Batch 12/64 loss: -0.8592500686645508
Batch 13/64 loss: -0.9913978576660156
Batch 14/64 loss: -0.7931861877441406
Batch 15/64 loss: -1.210230827331543
Batch 16/64 loss: -1.1183528900146484
Batch 17/64 loss: -1.0707283020019531
Batch 18/64 loss: -0.7885208129882812
Batch 19/64 loss: -0.7280721664428711
Batch 20/64 loss: -1.1508350372314453
Batch 21/64 loss: -0.8234405517578125
Batch 22/64 loss: -1.0719366073608398
Batch 23/64 loss: -0.6984224319458008
Batch 24/64 loss: -1.1603431701660156
Batch 25/64 loss: -1.1139240264892578
Batch 26/64 loss: -1.1224784851074219
Batch 27/64 loss: -1.338210105895996
Batch 28/64 loss: -1.1949329376220703
Batch 29/64 loss: -1.2262496948242188
Batch 30/64 loss: -0.9087047576904297
Batch 31/64 loss: -0.2628793716430664
Batch 32/64 loss: -1.0281143188476562
Batch 33/64 loss: -1.037949562072754
Batch 34/64 loss: -1.1151647567749023
Batch 35/64 loss: -1.3252239227294922
Batch 36/64 loss: -1.4885797500610352
Batch 37/64 loss: -1.2299184799194336
Batch 38/64 loss: -1.2558069229125977
Batch 39/64 loss: -0.6922798156738281
Batch 40/64 loss: -1.0756301879882812
Batch 41/64 loss: -0.5832700729370117
Batch 42/64 loss: -0.9847774505615234
Batch 43/64 loss: -1.0133867263793945
Batch 44/64 loss: -1.1380367279052734
Batch 45/64 loss: -1.0673093795776367
Batch 46/64 loss: -1.3320274353027344
Batch 47/64 loss: -1.2137880325317383
Batch 48/64 loss: -1.4619245529174805
Batch 49/64 loss: -1.210923194885254
Batch 50/64 loss: -0.9235601425170898
Batch 51/64 loss: -1.2189693450927734
Batch 52/64 loss: -1.0325403213500977
Batch 53/64 loss: -1.4286003112792969
Batch 54/64 loss: -1.025996208190918
Batch 55/64 loss: -1.163595199584961
Batch 56/64 loss: -0.8597955703735352
Batch 57/64 loss: -1.1741857528686523
Batch 58/64 loss: -0.6292095184326172
Batch 59/64 loss: -0.8640470504760742
Batch 60/64 loss: -1.0197534561157227
Batch 61/64 loss: -1.0925273895263672
Batch 62/64 loss: -0.9490318298339844
Batch 63/64 loss: -0.9172544479370117
Batch 64/64 loss: -4.959431171417236
Epoch 487  Train loss: -1.1006021443535299  Val loss: -0.8018943026303426
Epoch 488
-------------------------------
Batch 1/64 loss: -0.9894485473632812
Batch 2/64 loss: -1.2744340896606445
Batch 3/64 loss: -1.2640552520751953
Batch 4/64 loss: -1.033747673034668
Batch 5/64 loss: -0.7721586227416992
Batch 6/64 loss: -0.4713716506958008
Batch 7/64 loss: -1.1158666610717773
Batch 8/64 loss: -1.1943674087524414
Batch 9/64 loss: -0.23198795318603516
Batch 10/64 loss: -1.2929935455322266
Batch 11/64 loss: -1.0872983932495117
Batch 12/64 loss: -0.5516357421875
Batch 13/64 loss: -0.8481454849243164
Batch 14/64 loss: -1.3178205490112305
Batch 15/64 loss: -1.0559349060058594
Batch 16/64 loss: -0.8931283950805664
Batch 17/64 loss: -1.0394773483276367
Batch 18/64 loss: -0.7178630828857422
Batch 19/64 loss: -1.0698795318603516
Batch 20/64 loss: -1.3022851943969727
Batch 21/64 loss: -0.9125041961669922
Batch 22/64 loss: -1.1139411926269531
Batch 23/64 loss: -0.6966943740844727
Batch 24/64 loss: -1.0823297500610352
Batch 25/64 loss: -1.1504650115966797
Batch 26/64 loss: -0.7351198196411133
Batch 27/64 loss: -0.935399055480957
Batch 28/64 loss: -0.9616622924804688
Batch 29/64 loss: -1.1147270202636719
Batch 30/64 loss: -1.3446073532104492
Batch 31/64 loss: -1.2463788986206055
Batch 32/64 loss: -0.8870420455932617
Batch 33/64 loss: -1.1987791061401367
Batch 34/64 loss: -0.6864166259765625
Batch 35/64 loss: -1.0706062316894531
Batch 36/64 loss: -1.0528717041015625
Batch 37/64 loss: -0.8561868667602539
Batch 38/64 loss: -1.3112058639526367
Batch 39/64 loss: -0.8143825531005859
Batch 40/64 loss: -0.8895502090454102
Batch 41/64 loss: -0.9145946502685547
Batch 42/64 loss: -1.2137107849121094
Batch 43/64 loss: -0.9958429336547852
Batch 44/64 loss: -1.0940666198730469
Batch 45/64 loss: -1.307194709777832
Batch 46/64 loss: -1.019322395324707
Batch 47/64 loss: -0.7862348556518555
Batch 48/64 loss: -0.8454399108886719
Batch 49/64 loss: -0.9354095458984375
Batch 50/64 loss: -1.1073999404907227
Batch 51/64 loss: -1.2529354095458984
Batch 52/64 loss: -0.8723773956298828
Batch 53/64 loss: -0.8539924621582031
Batch 54/64 loss: -0.7713747024536133
Batch 55/64 loss: -0.8829050064086914
Batch 56/64 loss: -1.3239383697509766
Batch 57/64 loss: -0.7242469787597656
Batch 58/64 loss: -0.8892936706542969
Batch 59/64 loss: -1.1229076385498047
Batch 60/64 loss: -0.9309797286987305
Batch 61/64 loss: -0.6532964706420898
Batch 62/64 loss: -1.0068416595458984
Batch 63/64 loss: -1.115962028503418
Batch 64/64 loss: -5.338794708251953
Epoch 488  Train loss: -1.0385746525783164  Val loss: -1.0399369964075251
Epoch 489
-------------------------------
Batch 1/64 loss: -0.4925498962402344
Batch 2/64 loss: -1.0771369934082031
Batch 3/64 loss: -1.2018165588378906
Batch 4/64 loss: -1.3643417358398438
Batch 5/64 loss: -1.142526626586914
Batch 6/64 loss: -1.166015625
Batch 7/64 loss: -0.6457557678222656
Batch 8/64 loss: -0.7474899291992188
Batch 9/64 loss: -1.2462892532348633
Batch 10/64 loss: -1.2309646606445312
Batch 11/64 loss: -0.7291784286499023
Batch 12/64 loss: -1.0502986907958984
Batch 13/64 loss: -0.9628429412841797
Batch 14/64 loss: -1.0655975341796875
Batch 15/64 loss: -0.7651796340942383
Batch 16/64 loss: -1.3086309432983398
Batch 17/64 loss: -1.0913152694702148
Batch 18/64 loss: -1.0591201782226562
Batch 19/64 loss: -1.3219594955444336
Batch 20/64 loss: -1.2120466232299805
Batch 21/64 loss: -1.299020767211914
Batch 22/64 loss: -1.0653133392333984
Batch 23/64 loss: -0.8405084609985352
Batch 24/64 loss: -0.48677825927734375
Batch 25/64 loss: -0.7220602035522461
Batch 26/64 loss: -1.2252082824707031
Batch 27/64 loss: -1.2780828475952148
Batch 28/64 loss: -1.123011589050293
Batch 29/64 loss: -1.0387439727783203
Batch 30/64 loss: -0.9832305908203125
Batch 31/64 loss: -0.9870691299438477
Batch 32/64 loss: -0.9942045211791992
Batch 33/64 loss: -1.3198680877685547
Batch 34/64 loss: -1.1296253204345703
Batch 35/64 loss: -1.074732780456543
Batch 36/64 loss: -1.1576423645019531
Batch 37/64 loss: -1.183502197265625
Batch 38/64 loss: -1.197657585144043
Batch 39/64 loss: -0.7683992385864258
Batch 40/64 loss: -1.0326452255249023
Batch 41/64 loss: -1.1488189697265625
Batch 42/64 loss: -0.5650901794433594
Batch 43/64 loss: -1.394021987915039
Batch 44/64 loss: -1.2442865371704102
Batch 45/64 loss: -0.9006586074829102
Batch 46/64 loss: -1.0628557205200195
Batch 47/64 loss: -1.1575202941894531
Batch 48/64 loss: -1.0246562957763672
Batch 49/64 loss: -0.93292236328125
Batch 50/64 loss: -1.1274528503417969
Batch 51/64 loss: -1.3285388946533203
Batch 52/64 loss: -1.1748666763305664
Batch 53/64 loss: -0.9899272918701172
Batch 54/64 loss: -1.1061363220214844
Batch 55/64 loss: -1.1329479217529297
Batch 56/64 loss: -0.7171964645385742
Batch 57/64 loss: -0.8628606796264648
Batch 58/64 loss: -1.0486831665039062
Batch 59/64 loss: -0.8349189758300781
Batch 60/64 loss: -1.0007381439208984
Batch 61/64 loss: -0.9997482299804688
Batch 62/64 loss: -0.6697416305541992
Batch 63/64 loss: -1.055715560913086
Batch 64/64 loss: -5.481322765350342
Epoch 489  Train loss: -1.0883083511801328  Val loss: -1.1726697285970051
Saving best model, epoch: 489
Epoch 490
-------------------------------
Batch 1/64 loss: -1.2385826110839844
Batch 2/64 loss: -0.9444656372070312
Batch 3/64 loss: -1.0260114669799805
Batch 4/64 loss: -1.2282094955444336
Batch 5/64 loss: -0.9813337326049805
Batch 6/64 loss: -1.0056686401367188
Batch 7/64 loss: -1.2498836517333984
Batch 8/64 loss: -1.0266571044921875
Batch 9/64 loss: -1.0241432189941406
Batch 10/64 loss: -1.2076711654663086
Batch 11/64 loss: -1.0446243286132812
Batch 12/64 loss: -0.8250627517700195
Batch 13/64 loss: -0.6245203018188477
Batch 14/64 loss: -1.0258636474609375
Batch 15/64 loss: -1.138474464416504
Batch 16/64 loss: -1.3906946182250977
Batch 17/64 loss: -1.2736740112304688
Batch 18/64 loss: -1.2200937271118164
Batch 19/64 loss: -1.340092658996582
Batch 20/64 loss: -1.3919658660888672
Batch 21/64 loss: -1.311793327331543
Batch 22/64 loss: -0.9362373352050781
Batch 23/64 loss: -1.3059091567993164
Batch 24/64 loss: -1.1019096374511719
Batch 25/64 loss: -0.7266988754272461
Batch 26/64 loss: -1.2905664443969727
Batch 27/64 loss: -1.3751258850097656
Batch 28/64 loss: -0.7899026870727539
Batch 29/64 loss: -1.150496482849121
Batch 30/64 loss: -1.3337936401367188
Batch 31/64 loss: -1.2568588256835938
Batch 32/64 loss: -1.4260015487670898
Batch 33/64 loss: -1.4660978317260742
Batch 34/64 loss: -0.9130954742431641
Batch 35/64 loss: -0.9698905944824219
Batch 36/64 loss: -1.0071029663085938
Batch 37/64 loss: -1.4029302597045898
Batch 38/64 loss: -1.0707483291625977
Batch 39/64 loss: -1.0608034133911133
Batch 40/64 loss: -1.2314348220825195
Batch 41/64 loss: -1.050668716430664
Batch 42/64 loss: -1.2314453125
Batch 43/64 loss: -1.2448453903198242
Batch 44/64 loss: -0.8539810180664062
Batch 45/64 loss: -0.8604001998901367
Batch 46/64 loss: -0.9934310913085938
Batch 47/64 loss: -0.8539752960205078
Batch 48/64 loss: -1.1765689849853516
Batch 49/64 loss: -0.9914865493774414
Batch 50/64 loss: -0.8689651489257812
Batch 51/64 loss: -1.1765356063842773
Batch 52/64 loss: -0.7839031219482422
Batch 53/64 loss: -1.1684207916259766
Batch 54/64 loss: -1.1846799850463867
Batch 55/64 loss: -1.0871152877807617
Batch 56/64 loss: -1.1628217697143555
Batch 57/64 loss: -1.0799837112426758
Batch 58/64 loss: -1.0918455123901367
Batch 59/64 loss: -1.2983016967773438
Batch 60/64 loss: -1.0071992874145508
Batch 61/64 loss: -1.3065223693847656
Batch 62/64 loss: -0.923405647277832
Batch 63/64 loss: -0.9034328460693359
Batch 64/64 loss: -4.661762237548828
Epoch 490  Train loss: -1.1471583946078432  Val loss: -1.136311245947769
Epoch 491
-------------------------------
Batch 1/64 loss: -1.3189153671264648
Batch 2/64 loss: -1.4556055068969727
Batch 3/64 loss: -1.020395278930664
Batch 4/64 loss: -1.1223878860473633
Batch 5/64 loss: -1.2435197830200195
Batch 6/64 loss: -1.1252784729003906
Batch 7/64 loss: -0.9805192947387695
Batch 8/64 loss: -1.4616422653198242
Batch 9/64 loss: -0.9611291885375977
Batch 10/64 loss: -1.122100830078125
Batch 11/64 loss: -1.3260860443115234
Batch 12/64 loss: -0.9498796463012695
Batch 13/64 loss: -1.0804061889648438
Batch 14/64 loss: -1.1467905044555664
Batch 15/64 loss: -1.0603570938110352
Batch 16/64 loss: -0.9307975769042969
Batch 17/64 loss: -1.2667341232299805
Batch 18/64 loss: -1.0520954132080078
Batch 19/64 loss: -0.9925880432128906
Batch 20/64 loss: -1.1086053848266602
Batch 21/64 loss: -1.1955699920654297
Batch 22/64 loss: -1.280050277709961
Batch 23/64 loss: -1.088688850402832
Batch 24/64 loss: -1.2449846267700195
Batch 25/64 loss: -1.1123332977294922
Batch 26/64 loss: -1.0249509811401367
Batch 27/64 loss: -0.937678337097168
Batch 28/64 loss: -0.7811470031738281
Batch 29/64 loss: -1.1953926086425781
Batch 30/64 loss: -1.0928058624267578
Batch 31/64 loss: -1.1433277130126953
Batch 32/64 loss: -1.0738821029663086
Batch 33/64 loss: -1.1490612030029297
Batch 34/64 loss: -0.898900032043457
Batch 35/64 loss: -1.1765403747558594
Batch 36/64 loss: -1.4225139617919922
Batch 37/64 loss: -1.012033462524414
Batch 38/64 loss: -0.40343284606933594
Batch 39/64 loss: -1.22186279296875
Batch 40/64 loss: -1.0753288269042969
Batch 41/64 loss: -0.7820186614990234
Batch 42/64 loss: -1.3690786361694336
Batch 43/64 loss: -1.086012840270996
Batch 44/64 loss: -1.136545181274414
Batch 45/64 loss: -0.8785190582275391
Batch 46/64 loss: -1.1906213760375977
Batch 47/64 loss: -0.8892879486083984
Batch 48/64 loss: -1.1110258102416992
Batch 49/64 loss: -1.3406915664672852
Batch 50/64 loss: -0.9870700836181641
Batch 51/64 loss: -1.0260839462280273
Batch 52/64 loss: -0.9033527374267578
Batch 53/64 loss: -1.0605688095092773
Batch 54/64 loss: -1.3169946670532227
Batch 55/64 loss: -1.2740020751953125
Batch 56/64 loss: -1.209538459777832
Batch 57/64 loss: -0.6242589950561523
Batch 58/64 loss: -0.8243246078491211
Batch 59/64 loss: -0.9770803451538086
Batch 60/64 loss: -1.1662311553955078
Batch 61/64 loss: -1.1589622497558594
Batch 62/64 loss: -0.8495016098022461
Batch 63/64 loss: -0.8733091354370117
Batch 64/64 loss: -5.551637172698975
Epoch 491  Train loss: -1.136551009907442  Val loss: -1.0451313556264765
Epoch 492
-------------------------------
Batch 1/64 loss: -1.2994804382324219
Batch 2/64 loss: -0.8475456237792969
Batch 3/64 loss: -1.2581682205200195
Batch 4/64 loss: -0.9116144180297852
Batch 5/64 loss: -0.9466333389282227
Batch 6/64 loss: -1.092665672302246
Batch 7/64 loss: -0.7349815368652344
Batch 8/64 loss: -1.2204084396362305
Batch 9/64 loss: -0.935063362121582
Batch 10/64 loss: -0.08856773376464844
Batch 11/64 loss: -1.0471696853637695
Batch 12/64 loss: -1.2156143188476562
Batch 13/64 loss: -1.167677879333496
Batch 14/64 loss: -1.1947097778320312
Batch 15/64 loss: -1.2089824676513672
Batch 16/64 loss: -1.0071849822998047
Batch 17/64 loss: -1.1807050704956055
Batch 18/64 loss: -0.8181743621826172
Batch 19/64 loss: -1.1527166366577148
Batch 20/64 loss: -0.9642972946166992
Batch 21/64 loss: -0.7114620208740234
Batch 22/64 loss: -1.2621650695800781
Batch 23/64 loss: -0.47347164154052734
Batch 24/64 loss: -1.3891830444335938
Batch 25/64 loss: -1.1305303573608398
Batch 26/64 loss: -0.4898109436035156
Batch 27/64 loss: -1.000462532043457
Batch 28/64 loss: -1.0043182373046875
Batch 29/64 loss: -1.2412605285644531
Batch 30/64 loss: -1.119318962097168
Batch 31/64 loss: -1.0708770751953125
Batch 32/64 loss: -0.8550882339477539
Batch 33/64 loss: -1.2565793991088867
Batch 34/64 loss: -1.4498310089111328
Batch 35/64 loss: -0.4607095718383789
Batch 36/64 loss: -1.0655317306518555
Batch 37/64 loss: -0.8450956344604492
Batch 38/64 loss: -0.8133573532104492
Batch 39/64 loss: -0.8025941848754883
Batch 40/64 loss: -0.48472118377685547
Batch 41/64 loss: -1.3461933135986328
Batch 42/64 loss: -0.8191738128662109
Batch 43/64 loss: -1.030691146850586
Batch 44/64 loss: -0.8309736251831055
Batch 45/64 loss: -1.2475004196166992
Batch 46/64 loss: -1.103754997253418
Batch 47/64 loss: -1.0306015014648438
Batch 48/64 loss: -0.8929576873779297
Batch 49/64 loss: -1.395218849182129
Batch 50/64 loss: -1.2571954727172852
Batch 51/64 loss: -1.235137939453125
Batch 52/64 loss: -1.4353208541870117
Batch 53/64 loss: -1.1895666122436523
Batch 54/64 loss: -1.241598129272461
Batch 55/64 loss: -1.0194950103759766
Batch 56/64 loss: -1.2439136505126953
Batch 57/64 loss: -1.0499982833862305
Batch 58/64 loss: -1.1893386840820312
Batch 59/64 loss: -1.0287885665893555
Batch 60/64 loss: -0.8912410736083984
Batch 61/64 loss: -1.2370052337646484
Batch 62/64 loss: -1.1293182373046875
Batch 63/64 loss: -0.9328708648681641
Batch 64/64 loss: -5.3046770095825195
Epoch 492  Train loss: -1.0819622226789887  Val loss: -0.9884253170891726
Epoch 493
-------------------------------
Batch 1/64 loss: -1.142019271850586
Batch 2/64 loss: -1.3593111038208008
Batch 3/64 loss: -1.247060775756836
Batch 4/64 loss: -1.3818464279174805
Batch 5/64 loss: -1.0285520553588867
Batch 6/64 loss: -0.774871826171875
Batch 7/64 loss: -1.270197868347168
Batch 8/64 loss: -1.098855972290039
Batch 9/64 loss: -0.6762580871582031
Batch 10/64 loss: -0.8450155258178711
Batch 11/64 loss: -0.8994779586791992
Batch 12/64 loss: -0.8500480651855469
Batch 13/64 loss: -1.2602825164794922
Batch 14/64 loss: -1.1629419326782227
Batch 15/64 loss: -0.8587427139282227
Batch 16/64 loss: -1.4361076354980469
Batch 17/64 loss: -1.1651105880737305
Batch 18/64 loss: -1.1166324615478516
Batch 19/64 loss: -1.1587562561035156
Batch 20/64 loss: -0.9787740707397461
Batch 21/64 loss: -1.2169599533081055
Batch 22/64 loss: -0.8903055191040039
Batch 23/64 loss: -1.1861982345581055
Batch 24/64 loss: -1.0525779724121094
Batch 25/64 loss: -0.6047506332397461
Batch 26/64 loss: -1.359013557434082
Batch 27/64 loss: -1.2173891067504883
Batch 28/64 loss: -0.8802194595336914
Batch 29/64 loss: -1.044581413269043
Batch 30/64 loss: -1.2281217575073242
Batch 31/64 loss: -0.8031997680664062
Batch 32/64 loss: -0.998504638671875
Batch 33/64 loss: -1.2943449020385742
Batch 34/64 loss: -0.9965887069702148
Batch 35/64 loss: -0.973658561706543
Batch 36/64 loss: -0.7188291549682617
Batch 37/64 loss: -1.0952749252319336
Batch 38/64 loss: -1.0262985229492188
Batch 39/64 loss: -1.3770332336425781
Batch 40/64 loss: -1.166050910949707
Batch 41/64 loss: -0.6228694915771484
Batch 42/64 loss: -1.3001689910888672
Batch 43/64 loss: -0.7832717895507812
Batch 44/64 loss: -0.6270170211791992
Batch 45/64 loss: -1.168313980102539
Batch 46/64 loss: -0.9960117340087891
Batch 47/64 loss: -1.1351490020751953
Batch 48/64 loss: -0.940974235534668
Batch 49/64 loss: -0.5769872665405273
Batch 50/64 loss: -1.0247020721435547
Batch 51/64 loss: -1.2325401306152344
Batch 52/64 loss: -0.548893928527832
Batch 53/64 loss: -1.1432781219482422
Batch 54/64 loss: -1.100682258605957
Batch 55/64 loss: -0.9870357513427734
Batch 56/64 loss: -1.1455774307250977
Batch 57/64 loss: -1.1590394973754883
Batch 58/64 loss: -1.2878503799438477
Batch 59/64 loss: -1.0667476654052734
Batch 60/64 loss: -1.042327880859375
Batch 61/64 loss: -0.7972774505615234
Batch 62/64 loss: -1.2052812576293945
Batch 63/64 loss: -1.2265233993530273
Batch 64/64 loss: -5.710400581359863
Epoch 493  Train loss: -1.1018366308773264  Val loss: -1.0242362137102998
Epoch 494
-------------------------------
Batch 1/64 loss: -1.1662025451660156
Batch 2/64 loss: -0.821080207824707
Batch 3/64 loss: -1.0974273681640625
Batch 4/64 loss: -0.9257850646972656
Batch 5/64 loss: -1.433370590209961
Batch 6/64 loss: -0.8535928726196289
Batch 7/64 loss: -1.3214588165283203
Batch 8/64 loss: -0.8774070739746094
Batch 9/64 loss: -0.9983062744140625
Batch 10/64 loss: -1.3265256881713867
Batch 11/64 loss: -0.8858823776245117
Batch 12/64 loss: -0.9379138946533203
Batch 13/64 loss: -1.3087997436523438
Batch 14/64 loss: -1.2080984115600586
Batch 15/64 loss: -0.6243143081665039
Batch 16/64 loss: -1.1524238586425781
Batch 17/64 loss: -1.0241823196411133
Batch 18/64 loss: -0.4489593505859375
Batch 19/64 loss: -0.6335830688476562
Batch 20/64 loss: -1.031473159790039
Batch 21/64 loss: -1.1504039764404297
Batch 22/64 loss: -1.0607481002807617
Batch 23/64 loss: -1.3416290283203125
Batch 24/64 loss: -1.195448875427246
Batch 25/64 loss: -1.2513008117675781
Batch 26/64 loss: -0.9604024887084961
Batch 27/64 loss: -0.8541831970214844
Batch 28/64 loss: -0.813624382019043
Batch 29/64 loss: -0.7557821273803711
Batch 30/64 loss: -1.0547103881835938
Batch 31/64 loss: -0.7378597259521484
Batch 32/64 loss: -0.6967983245849609
Batch 33/64 loss: -1.3471946716308594
Batch 34/64 loss: -0.6889858245849609
Batch 35/64 loss: -0.8125448226928711
Batch 36/64 loss: -1.113865852355957
Batch 37/64 loss: -1.0950136184692383
Batch 38/64 loss: -1.3340930938720703
Batch 39/64 loss: -0.9413166046142578
Batch 40/64 loss: -0.8941974639892578
Batch 41/64 loss: -1.0972471237182617
Batch 42/64 loss: -0.8301725387573242
Batch 43/64 loss: -1.012040138244629
Batch 44/64 loss: -1.3795528411865234
Batch 45/64 loss: -0.8699178695678711
Batch 46/64 loss: -1.2069072723388672
Batch 47/64 loss: -1.471846580505371
Batch 48/64 loss: -1.0921392440795898
Batch 49/64 loss: -1.1847076416015625
Batch 50/64 loss: -0.9893884658813477
Batch 51/64 loss: -1.3601036071777344
Batch 52/64 loss: -1.0493755340576172
Batch 53/64 loss: -1.4107704162597656
Batch 54/64 loss: -0.7252340316772461
Batch 55/64 loss: -0.8653125762939453
Batch 56/64 loss: -1.0771427154541016
Batch 57/64 loss: -1.4329719543457031
Batch 58/64 loss: -1.0971965789794922
Batch 59/64 loss: -0.9287548065185547
Batch 60/64 loss: -1.0748863220214844
Batch 61/64 loss: -0.6691961288452148
Batch 62/64 loss: -1.309457778930664
Batch 63/64 loss: -1.1709327697753906
Batch 64/64 loss: -5.198605537414551
Epoch 494  Train loss: -1.088330971960928  Val loss: -1.1049351118684225
Epoch 495
-------------------------------
Batch 1/64 loss: -1.183828353881836
Batch 2/64 loss: -1.2842578887939453
Batch 3/64 loss: -0.8925304412841797
Batch 4/64 loss: -1.037109375
Batch 5/64 loss: -1.062978744506836
Batch 6/64 loss: -1.1747703552246094
Batch 7/64 loss: -1.423013687133789
Batch 8/64 loss: -0.8019723892211914
Batch 9/64 loss: -1.3382692337036133
Batch 10/64 loss: -1.482004165649414
Batch 11/64 loss: -1.0474023818969727
Batch 12/64 loss: -1.4276113510131836
Batch 13/64 loss: -1.0925073623657227
Batch 14/64 loss: -1.470963478088379
Batch 15/64 loss: -0.867192268371582
Batch 16/64 loss: -1.3348989486694336
Batch 17/64 loss: -1.4169244766235352
Batch 18/64 loss: -1.2664861679077148
Batch 19/64 loss: -1.0877885818481445
Batch 20/64 loss: -1.278355598449707
Batch 21/64 loss: -1.1878271102905273
Batch 22/64 loss: -1.288651466369629
Batch 23/64 loss: -1.2356586456298828
Batch 24/64 loss: -1.3125066757202148
Batch 25/64 loss: -1.162632942199707
Batch 26/64 loss: -1.4058027267456055
Batch 27/64 loss: -1.3636474609375
Batch 28/64 loss: -0.7674665451049805
Batch 29/64 loss: -1.1766166687011719
Batch 30/64 loss: -1.0222644805908203
Batch 31/64 loss: -1.0874967575073242
Batch 32/64 loss: -1.2534408569335938
Batch 33/64 loss: -0.6604423522949219
Batch 34/64 loss: -0.7227153778076172
Batch 35/64 loss: -1.2613134384155273
Batch 36/64 loss: -1.1953020095825195
Batch 37/64 loss: -0.9597053527832031
Batch 38/64 loss: -0.8508377075195312
Batch 39/64 loss: -0.9850215911865234
Batch 40/64 loss: -0.8022212982177734
Batch 41/64 loss: -0.7772274017333984
Batch 42/64 loss: -0.8066539764404297
Batch 43/64 loss: -0.7722110748291016
Batch 44/64 loss: -1.327733039855957
Batch 45/64 loss: -1.2589702606201172
Batch 46/64 loss: -1.2196340560913086
Batch 47/64 loss: -0.6901378631591797
Batch 48/64 loss: -0.9866628646850586
Batch 49/64 loss: -1.0731687545776367
Batch 50/64 loss: -1.2812957763671875
Batch 51/64 loss: -0.8447780609130859
Batch 52/64 loss: -1.119497299194336
Batch 53/64 loss: -0.8532505035400391
Batch 54/64 loss: -1.233992576599121
Batch 55/64 loss: -1.387613296508789
Batch 56/64 loss: -1.1239233016967773
Batch 57/64 loss: -1.0915660858154297
Batch 58/64 loss: -1.1117744445800781
Batch 59/64 loss: -0.8503046035766602
Batch 60/64 loss: -0.8539266586303711
Batch 61/64 loss: -1.1346406936645508
Batch 62/64 loss: -0.8375997543334961
Batch 63/64 loss: -1.0297870635986328
Batch 64/64 loss: -5.755802154541016
Epoch 495  Train loss: -1.155382552801394  Val loss: -1.1491092537686587
Epoch 496
-------------------------------
Batch 1/64 loss: -0.8800020217895508
Batch 2/64 loss: -0.8142633438110352
Batch 3/64 loss: -1.1632461547851562
Batch 4/64 loss: -0.9547405242919922
Batch 5/64 loss: -1.1472835540771484
Batch 6/64 loss: -0.7147626876831055
Batch 7/64 loss: -1.2572040557861328
Batch 8/64 loss: -1.0454607009887695
Batch 9/64 loss: -1.3618783950805664
Batch 10/64 loss: -1.2437191009521484
Batch 11/64 loss: -1.4803085327148438
Batch 12/64 loss: -0.9872665405273438
Batch 13/64 loss: -1.0347185134887695
Batch 14/64 loss: -1.0548601150512695
Batch 15/64 loss: -1.1940326690673828
Batch 16/64 loss: -1.2219276428222656
Batch 17/64 loss: -0.6863527297973633
Batch 18/64 loss: -0.933349609375
Batch 19/64 loss: -0.9356288909912109
Batch 20/64 loss: -0.9660024642944336
Batch 21/64 loss: -1.2158012390136719
Batch 22/64 loss: -1.596510887145996
Batch 23/64 loss: -1.0274276733398438
Batch 24/64 loss: -0.9418172836303711
Batch 25/64 loss: -1.6685991287231445
Batch 26/64 loss: -0.7820072174072266
Batch 27/64 loss: -0.9753885269165039
Batch 28/64 loss: -0.6723003387451172
Batch 29/64 loss: -0.9595165252685547
Batch 30/64 loss: -0.7997665405273438
Batch 31/64 loss: -1.065535545349121
Batch 32/64 loss: -1.2233781814575195
Batch 33/64 loss: -1.0746803283691406
Batch 34/64 loss: -1.273427963256836
Batch 35/64 loss: -1.310530662536621
Batch 36/64 loss: -1.5414581298828125
Batch 37/64 loss: -1.1058073043823242
Batch 38/64 loss: -1.5135383605957031
Batch 39/64 loss: -0.922144889831543
Batch 40/64 loss: -1.1867332458496094
Batch 41/64 loss: -0.9557991027832031
Batch 42/64 loss: -1.2639636993408203
Batch 43/64 loss: -0.5398416519165039
Batch 44/64 loss: -0.9361763000488281
Batch 45/64 loss: -1.034928321838379
Batch 46/64 loss: -0.8865375518798828
Batch 47/64 loss: -1.4637422561645508
Batch 48/64 loss: -0.9970254898071289
Batch 49/64 loss: -0.9451522827148438
Batch 50/64 loss: -1.4462404251098633
Batch 51/64 loss: -1.0170421600341797
Batch 52/64 loss: -0.9255962371826172
Batch 53/64 loss: -1.1391572952270508
Batch 54/64 loss: -1.221475601196289
Batch 55/64 loss: -1.0628118515014648
Batch 56/64 loss: -1.4379043579101562
Batch 57/64 loss: -1.0672264099121094
Batch 58/64 loss: -0.9334144592285156
Batch 59/64 loss: -1.1272764205932617
Batch 60/64 loss: -1.1592397689819336
Batch 61/64 loss: -1.1035432815551758
Batch 62/64 loss: -1.280501365661621
Batch 63/64 loss: -1.0468339920043945
Batch 64/64 loss: -5.552435874938965
Epoch 496  Train loss: -1.1464962417004154  Val loss: -1.1195440652853845
Epoch 497
-------------------------------
Batch 1/64 loss: -1.055851936340332
Batch 2/64 loss: -0.939234733581543
Batch 3/64 loss: -1.449519157409668
Batch 4/64 loss: -0.8099956512451172
Batch 5/64 loss: -1.0601511001586914
Batch 6/64 loss: -0.8277568817138672
Batch 7/64 loss: -1.3475351333618164
Batch 8/64 loss: -1.0768909454345703
Batch 9/64 loss: -1.2817049026489258
Batch 10/64 loss: -1.041539192199707
Batch 11/64 loss: -1.2719793319702148
Batch 12/64 loss: -1.0051584243774414
Batch 13/64 loss: -1.0933218002319336
Batch 14/64 loss: -1.098048210144043
Batch 15/64 loss: -1.3583869934082031
Batch 16/64 loss: -1.0300312042236328
Batch 17/64 loss: -1.0156898498535156
Batch 18/64 loss: -1.2747364044189453
Batch 19/64 loss: -1.1926774978637695
Batch 20/64 loss: -1.1356925964355469
Batch 21/64 loss: -0.9064836502075195
Batch 22/64 loss: -1.252427101135254
Batch 23/64 loss: -0.9109945297241211
Batch 24/64 loss: -0.7179574966430664
Batch 25/64 loss: -0.8632183074951172
Batch 26/64 loss: -0.9377708435058594
Batch 27/64 loss: -1.0732851028442383
Batch 28/64 loss: -1.4475641250610352
Batch 29/64 loss: -1.0607414245605469
Batch 30/64 loss: -1.576756477355957
Batch 31/64 loss: -1.1228971481323242
Batch 32/64 loss: -1.227630615234375
Batch 33/64 loss: -1.5236892700195312
Batch 34/64 loss: -0.41120243072509766
Batch 35/64 loss: -0.8745536804199219
Batch 36/64 loss: -1.0497455596923828
Batch 37/64 loss: -0.4657745361328125
Batch 38/64 loss: -1.3788976669311523
Batch 39/64 loss: -0.7374992370605469
Batch 40/64 loss: -0.6074419021606445
Batch 41/64 loss: -1.294001579284668
Batch 42/64 loss: -0.817169189453125
Batch 43/64 loss: -0.7519083023071289
Batch 44/64 loss: -1.1945838928222656
Batch 45/64 loss: -1.1522083282470703
Batch 46/64 loss: -1.0785245895385742
Batch 47/64 loss: -0.49509620666503906
Batch 48/64 loss: -0.9308338165283203
Batch 49/64 loss: -0.989837646484375
Batch 50/64 loss: -0.8377962112426758
Batch 51/64 loss: -1.1009235382080078
Batch 52/64 loss: -0.7249765396118164
Batch 53/64 loss: -0.9457530975341797
Batch 54/64 loss: -1.1074495315551758
Batch 55/64 loss: -1.2232847213745117
Batch 56/64 loss: -1.0965213775634766
Batch 57/64 loss: -1.23956298828125
Batch 58/64 loss: -1.2410202026367188
Batch 59/64 loss: -0.8012733459472656
Batch 60/64 loss: -0.5915813446044922
Batch 61/64 loss: -0.5127487182617188
Batch 62/64 loss: -1.2168693542480469
Batch 63/64 loss: -1.304417610168457
Batch 64/64 loss: -4.927268981933594
Epoch 497  Train loss: -1.080097677193436  Val loss: -0.9419992846721637
Epoch 498
-------------------------------
Batch 1/64 loss: -0.28232526779174805
Batch 2/64 loss: -0.8095741271972656
Batch 3/64 loss: -1.3771371841430664
Batch 4/64 loss: -1.0287809371948242
Batch 5/64 loss: -0.9341907501220703
Batch 6/64 loss: -1.1848278045654297
Batch 7/64 loss: -1.0417728424072266
Batch 8/64 loss: -1.2318334579467773
Batch 9/64 loss: -0.5563125610351562
Batch 10/64 loss: -1.1433706283569336
Batch 11/64 loss: -1.2933378219604492
Batch 12/64 loss: -1.0936470031738281
Batch 13/64 loss: -1.3412609100341797
Batch 14/64 loss: -1.2724313735961914
Batch 15/64 loss: -1.1868877410888672
Batch 16/64 loss: -0.8809537887573242
Batch 17/64 loss: -1.0794658660888672
Batch 18/64 loss: -1.1292800903320312
Batch 19/64 loss: -0.8750114440917969
Batch 20/64 loss: -1.089615821838379
Batch 21/64 loss: -0.8261394500732422
Batch 22/64 loss: -0.5647735595703125
Batch 23/64 loss: -0.8592681884765625
Batch 24/64 loss: -1.2799129486083984
Batch 25/64 loss: -0.7741365432739258
Batch 26/64 loss: -0.5981340408325195
Batch 27/64 loss: -0.8172788619995117
Batch 28/64 loss: -0.47894859313964844
Batch 29/64 loss: -1.1326875686645508
Batch 30/64 loss: -0.9733982086181641
Batch 31/64 loss: -1.1845512390136719
Batch 32/64 loss: -1.0109777450561523
Batch 33/64 loss: -1.254446029663086
Batch 34/64 loss: -1.1420927047729492
Batch 35/64 loss: -0.8906631469726562
Batch 36/64 loss: -1.2534265518188477
Batch 37/64 loss: -0.9323434829711914
Batch 38/64 loss: -0.9187583923339844
Batch 39/64 loss: -0.7712469100952148
Batch 40/64 loss: -0.7088394165039062
Batch 41/64 loss: -1.1403532028198242
Batch 42/64 loss: -1.0205297470092773
Batch 43/64 loss: -0.9006547927856445
Batch 44/64 loss: -1.3084678649902344
Batch 45/64 loss: -0.9359884262084961
Batch 46/64 loss: -0.8816947937011719
Batch 47/64 loss: -1.0914115905761719
Batch 48/64 loss: -0.6536159515380859
Batch 49/64 loss: -1.3257026672363281
Batch 50/64 loss: -1.1302814483642578
Batch 51/64 loss: -0.9562540054321289
Batch 52/64 loss: -0.6801528930664062
Batch 53/64 loss: -0.8756675720214844
Batch 54/64 loss: -1.0019636154174805
Batch 55/64 loss: -0.9641323089599609
Batch 56/64 loss: -1.0735855102539062
Batch 57/64 loss: -1.144144058227539
Batch 58/64 loss: -0.9336538314819336
Batch 59/64 loss: -0.9468317031860352
Batch 60/64 loss: -1.340810775756836
Batch 61/64 loss: -1.3105335235595703
Batch 62/64 loss: -0.8443584442138672
Batch 63/64 loss: -1.1656150817871094
Batch 64/64 loss: -5.49044942855835
Epoch 498  Train loss: -1.050169127595191  Val loss: -1.0077413709712602
Epoch 499
-------------------------------
Batch 1/64 loss: -1.1373252868652344
Batch 2/64 loss: -0.5177736282348633
Batch 3/64 loss: -1.1945152282714844
Batch 4/64 loss: -0.8933277130126953
Batch 5/64 loss: -1.0087718963623047
Batch 6/64 loss: -1.2973642349243164
Batch 7/64 loss: -1.1738662719726562
Batch 8/64 loss: -1.1925926208496094
Batch 9/64 loss: -0.9869871139526367
Batch 10/64 loss: -1.1816234588623047
Batch 11/64 loss: -1.4000368118286133
Batch 12/64 loss: -0.5538997650146484
Batch 13/64 loss: -1.072601318359375
Batch 14/64 loss: -1.3097429275512695
Batch 15/64 loss: -1.1742134094238281
Batch 16/64 loss: -1.3564271926879883
Batch 17/64 loss: -1.0954017639160156
Batch 18/64 loss: -1.0504302978515625
Batch 19/64 loss: -0.9850664138793945
Batch 20/64 loss: -1.2580327987670898
Batch 21/64 loss: -1.1740751266479492
Batch 22/64 loss: -0.9179811477661133
Batch 23/64 loss: -0.5970773696899414
Batch 24/64 loss: -0.8460826873779297
Batch 25/64 loss: -1.0798835754394531
Batch 26/64 loss: -1.196080207824707
Batch 27/64 loss: -1.3866376876831055
Batch 28/64 loss: -1.3466300964355469
Batch 29/64 loss: -0.8472385406494141
Batch 30/64 loss: -0.7426624298095703
Batch 31/64 loss: -1.0842485427856445
Batch 32/64 loss: -1.2358951568603516
Batch 33/64 loss: -1.2799091339111328
Batch 34/64 loss: -0.8818464279174805
Batch 35/64 loss: -0.7858209609985352
Batch 36/64 loss: -0.8134908676147461
Batch 37/64 loss: -1.2404279708862305
Batch 38/64 loss: -1.0185861587524414
Batch 39/64 loss: -0.8248348236083984
Batch 40/64 loss: -1.364506721496582
Batch 41/64 loss: -0.8666219711303711
Batch 42/64 loss: -1.3791322708129883
Batch 43/64 loss: -1.372117042541504
Batch 44/64 loss: -0.8478479385375977
Batch 45/64 loss: -1.0256757736206055
Batch 46/64 loss: -1.3101310729980469
Batch 47/64 loss: -1.3221406936645508
Batch 48/64 loss: -1.156336784362793
Batch 49/64 loss: -1.4568042755126953
Batch 50/64 loss: -0.7043695449829102
Batch 51/64 loss: -0.9894247055053711
Batch 52/64 loss: -1.0844879150390625
Batch 53/64 loss: -0.7226200103759766
Batch 54/64 loss: -1.015955924987793
Batch 55/64 loss: -1.2775030136108398
Batch 56/64 loss: -1.300562858581543
Batch 57/64 loss: -0.8284912109375
Batch 58/64 loss: -0.9512596130371094
Batch 59/64 loss: -1.249272346496582
Batch 60/64 loss: -1.061121940612793
Batch 61/64 loss: -0.9271974563598633
Batch 62/64 loss: -0.9504003524780273
Batch 63/64 loss: -1.1435699462890625
Batch 64/64 loss: -4.960989952087402
Epoch 499  Train loss: -1.1163561241299498  Val loss: -0.8148065940620973
Epoch 500
-------------------------------
Batch 1/64 loss: -0.8495645523071289
Batch 2/64 loss: -1.343735694885254
Batch 3/64 loss: -0.7987728118896484
Batch 4/64 loss: -1.1490144729614258
Batch 5/64 loss: -0.950108528137207
Batch 6/64 loss: -0.4522991180419922
Batch 7/64 loss: -0.9788532257080078
Batch 8/64 loss: -1.2634239196777344
Batch 9/64 loss: -1.2538890838623047
Batch 10/64 loss: -0.9242658615112305
Batch 11/64 loss: -0.7199726104736328
Batch 12/64 loss: -0.4790000915527344
Batch 13/64 loss: -1.3476362228393555
Batch 14/64 loss: -1.199110984802246
Batch 15/64 loss: -1.327779769897461
Batch 16/64 loss: -1.1120405197143555
Batch 17/64 loss: -1.2130870819091797
Batch 18/64 loss: -0.8833017349243164
Batch 19/64 loss: -1.211446762084961
Batch 20/64 loss: -1.4673986434936523
Batch 21/64 loss: -1.3029708862304688
Batch 22/64 loss: -1.0551176071166992
Batch 23/64 loss: -1.4571552276611328
Batch 24/64 loss: -1.219472885131836
Batch 25/64 loss: -1.3185796737670898
Batch 26/64 loss: -1.4985055923461914
Batch 27/64 loss: -0.3628883361816406
Batch 28/64 loss: -1.1494474411010742
Batch 29/64 loss: -1.0493888854980469
Batch 30/64 loss: -0.8614482879638672
Batch 31/64 loss: -1.3807029724121094
Batch 32/64 loss: -1.2981805801391602
Batch 33/64 loss: -1.3702125549316406
Batch 34/64 loss: -0.6706008911132812
Batch 35/64 loss: -1.4084692001342773
Batch 36/64 loss: -1.112391471862793
Batch 37/64 loss: -1.0456390380859375
Batch 38/64 loss: -0.9785594940185547
Batch 39/64 loss: -0.8967113494873047
Batch 40/64 loss: -1.153773307800293
Batch 41/64 loss: -1.2642097473144531
Batch 42/64 loss: -1.4296035766601562
Batch 43/64 loss: -1.2163200378417969
Batch 44/64 loss: -1.0331058502197266
Batch 45/64 loss: -1.0550260543823242
Batch 46/64 loss: -1.1891183853149414
Batch 47/64 loss: -0.21903228759765625
Batch 48/64 loss: -1.0722999572753906
Batch 49/64 loss: -0.9722461700439453
Batch 50/64 loss: -1.1186199188232422
Batch 51/64 loss: -1.0418996810913086
Batch 52/64 loss: -0.9934196472167969
Batch 53/64 loss: -0.7896442413330078
Batch 54/64 loss: -1.2890825271606445
Batch 55/64 loss: -1.1610021591186523
Batch 56/64 loss: -1.0344200134277344
Batch 57/64 loss: -1.2853412628173828
Batch 58/64 loss: -1.1716432571411133
Batch 59/64 loss: -1.3063173294067383
Batch 60/64 loss: -1.1959943771362305
Batch 61/64 loss: -1.0176963806152344
Batch 62/64 loss: -0.9899473190307617
Batch 63/64 loss: -1.1541881561279297
Batch 64/64 loss: -4.967384338378906
Epoch 500  Train loss: -1.1331864151300168  Val loss: -1.1463731064419567
SLIC undersegmentation error: 0.12412920962199316
SLIC inter-cluster variation: 0.13904419774313004
SLIC number of superpixels: 21483
SLIC superpixels per image: 73.82474226804123
Model loaded
Test metrics:
-1.9280192581648679 0.31940481099656354 49.54471814923535 tensor(0.3157, dtype=torch.float64) 0.7316208551483421 3.1015488656195465 18336
Inference time: 0.002185135772547771 seconds
Relabeled undersegmentation error: 0.1351491408934708
Relabeled inter-cluster variation: 0.08506741000365564
Relabeled mean superpixels count: 195.42611683848799
Original mean superpixels count: 63.01030927835052
Done!
Job id: 488558
Job id: 492261
