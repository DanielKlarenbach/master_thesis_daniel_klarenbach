Started preprocessing dataset
Number of training samples: 2040
Number of validation samples: 582
Number of testing samples: 291
Using cuda device
Epoch 1
-------------------------------
Batch 1/64 loss: 4.670834541320801
Batch 2/64 loss: 4.669632911682129
Batch 3/64 loss: 4.673595905303955
Batch 4/64 loss: 4.650175094604492
Batch 5/64 loss: 4.646002292633057
Batch 6/64 loss: 4.633694648742676
Batch 7/64 loss: 4.632309436798096
Batch 8/64 loss: 4.630188941955566
Batch 9/64 loss: 4.629258155822754
Batch 10/64 loss: 4.6293840408325195
Batch 11/64 loss: 4.626319885253906
Batch 12/64 loss: 4.623775005340576
Batch 13/64 loss: 4.622813701629639
Batch 14/64 loss: 4.621981143951416
Batch 15/64 loss: 4.623656749725342
Batch 16/64 loss: 4.619918346405029
Batch 17/64 loss: 4.620916366577148
Batch 18/64 loss: 4.622006416320801
Batch 19/64 loss: 4.620173931121826
Batch 20/64 loss: 4.62033748626709
Batch 21/64 loss: 4.619932651519775
Batch 22/64 loss: 4.62055778503418
Batch 23/64 loss: 4.6196675300598145
Batch 24/64 loss: 4.618819236755371
Batch 25/64 loss: 4.618699550628662
Batch 26/64 loss: 4.618366241455078
Batch 27/64 loss: 4.618741512298584
Batch 28/64 loss: 4.618200302124023
Batch 29/64 loss: 4.618711471557617
Batch 30/64 loss: 4.61800479888916
Batch 31/64 loss: 4.618090629577637
Batch 32/64 loss: 4.617948055267334
Batch 33/64 loss: 4.617747783660889
Batch 34/64 loss: 4.61787748336792
Batch 35/64 loss: 4.617770195007324
Batch 36/64 loss: 4.617743968963623
Batch 37/64 loss: 4.617856979370117
Batch 38/64 loss: 4.617421627044678
Batch 39/64 loss: 4.617451190948486
Batch 40/64 loss: 4.617276191711426
Batch 41/64 loss: 4.617242813110352
Batch 42/64 loss: 4.616977214813232
Batch 43/64 loss: 4.617023944854736
Batch 44/64 loss: 4.616973876953125
Batch 45/64 loss: 4.616686820983887
Batch 46/64 loss: 4.616209506988525
Batch 47/64 loss: 4.616341590881348
Batch 48/64 loss: 4.616629123687744
Batch 49/64 loss: 4.615830898284912
Batch 50/64 loss: 4.616121292114258
Batch 51/64 loss: 4.615578651428223
Batch 52/64 loss: 4.615614891052246
Batch 53/64 loss: 4.614799976348877
Batch 54/64 loss: 4.61557149887085
Batch 55/64 loss: 4.614524841308594
Batch 56/64 loss: 4.6148810386657715
Batch 57/64 loss: 4.6147141456604
Batch 58/64 loss: 4.613981246948242
Batch 59/64 loss: 4.613616466522217
Batch 60/64 loss: 4.613142490386963
Batch 61/64 loss: 4.612851619720459
Batch 62/64 loss: 4.611656665802002
Batch 63/64 loss: 4.610928058624268
Batch 64/64 loss: 3.836009979248047
Epoch 1  Train loss: 4.613000256407495  Val loss: 4.587497360517888
Saving best model, epoch: 1
Epoch 2
-------------------------------
Batch 1/64 loss: 4.609376430511475
Batch 2/64 loss: 4.609093189239502
Batch 3/64 loss: 4.608461380004883
Batch 4/64 loss: 4.607119083404541
Batch 5/64 loss: 4.606684684753418
Batch 6/64 loss: 4.6079864501953125
Batch 7/64 loss: 4.603014945983887
Batch 8/64 loss: 4.601720809936523
Batch 9/64 loss: 4.602192401885986
Batch 10/64 loss: 4.599215507507324
Batch 11/64 loss: 4.601770877838135
Batch 12/64 loss: 4.593258857727051
Batch 13/64 loss: 4.6005682945251465
Batch 14/64 loss: 4.595036506652832
Batch 15/64 loss: 4.5950846672058105
Batch 16/64 loss: 4.597790241241455
Batch 17/64 loss: 4.586963176727295
Batch 18/64 loss: 4.594731330871582
Batch 19/64 loss: 4.598647594451904
Batch 20/64 loss: 4.588907241821289
Batch 21/64 loss: 4.582263946533203
Batch 22/64 loss: 4.596482753753662
Batch 23/64 loss: 4.590163707733154
Batch 24/64 loss: 4.587035179138184
Batch 25/64 loss: 4.588254928588867
Batch 26/64 loss: 4.586273670196533
Batch 27/64 loss: 4.580021858215332
Batch 28/64 loss: 4.574975967407227
Batch 29/64 loss: 4.569296836853027
Batch 30/64 loss: 4.56369686126709
Batch 31/64 loss: 4.57387638092041
Batch 32/64 loss: 4.560909748077393
Batch 33/64 loss: 4.566938400268555
Batch 34/64 loss: 4.555250644683838
Batch 35/64 loss: 4.547474384307861
Batch 36/64 loss: 4.5743255615234375
Batch 37/64 loss: 4.569808006286621
Batch 38/64 loss: 4.602055549621582
Batch 39/64 loss: 4.57706356048584
Batch 40/64 loss: 4.562732696533203
Batch 41/64 loss: 4.603219509124756
Batch 42/64 loss: 4.599827766418457
Batch 43/64 loss: 4.585697174072266
Batch 44/64 loss: 4.566490173339844
Batch 45/64 loss: 4.578951358795166
Batch 46/64 loss: 4.558969974517822
Batch 47/64 loss: 4.56793212890625
Batch 48/64 loss: 4.5586323738098145
Batch 49/64 loss: 4.557362079620361
Batch 50/64 loss: 4.553389072418213
Batch 51/64 loss: 4.541004657745361
Batch 52/64 loss: 4.557727336883545
Batch 53/64 loss: 4.594522476196289
Batch 54/64 loss: 4.530822277069092
Batch 55/64 loss: 4.555708885192871
Batch 56/64 loss: 4.545295715332031
Batch 57/64 loss: 4.53830623626709
Batch 58/64 loss: 4.539634704589844
Batch 59/64 loss: 4.514810562133789
Batch 60/64 loss: 4.5796661376953125
Batch 61/64 loss: 4.528255462646484
Batch 62/64 loss: 4.507632255554199
Batch 63/64 loss: 4.553117275238037
Batch 64/64 loss: 3.6715140342712402
Epoch 2  Train loss: 4.56607271269256  Val loss: 4.514295558134715
Saving best model, epoch: 2
Epoch 3
-------------------------------
Batch 1/64 loss: 4.538954734802246
Batch 2/64 loss: 4.526788711547852
Batch 3/64 loss: 4.520079135894775
Batch 4/64 loss: 4.544181823730469
Batch 5/64 loss: 4.515997409820557
Batch 6/64 loss: 4.540858268737793
Batch 7/64 loss: 4.514668941497803
Batch 8/64 loss: 4.534624099731445
Batch 9/64 loss: 4.553205490112305
Batch 10/64 loss: 4.551600933074951
Batch 11/64 loss: 4.550920009613037
Batch 12/64 loss: 4.541162967681885
Batch 13/64 loss: 4.540018081665039
Batch 14/64 loss: 4.5459723472595215
Batch 15/64 loss: 4.530903339385986
Batch 16/64 loss: 4.531580924987793
Batch 17/64 loss: 4.532401084899902
Batch 18/64 loss: 4.55613899230957
Batch 19/64 loss: 4.5076823234558105
Batch 20/64 loss: 4.4970502853393555
Batch 21/64 loss: 4.530498027801514
Batch 22/64 loss: 4.504351615905762
Batch 23/64 loss: 4.484705448150635
Batch 24/64 loss: 4.4815521240234375
Batch 25/64 loss: 4.5048041343688965
Batch 26/64 loss: 4.4719319343566895
Batch 27/64 loss: 4.5208420753479
Batch 28/64 loss: 4.514704704284668
Batch 29/64 loss: 4.475335121154785
Batch 30/64 loss: 4.49220609664917
Batch 31/64 loss: 4.489073753356934
Batch 32/64 loss: 4.508684158325195
Batch 33/64 loss: 4.494864463806152
Batch 34/64 loss: 4.5076069831848145
Batch 35/64 loss: 4.501933574676514
Batch 36/64 loss: 4.548449993133545
Batch 37/64 loss: 4.500415802001953
Batch 38/64 loss: 4.56351375579834
Batch 39/64 loss: 4.777154922485352
Batch 40/64 loss: 4.490847110748291
Batch 41/64 loss: 4.525325298309326
Batch 42/64 loss: 4.693011283874512
Batch 43/64 loss: 4.523636341094971
Batch 44/64 loss: 4.59009313583374
Batch 45/64 loss: 4.608526229858398
Batch 46/64 loss: 4.583029270172119
Batch 47/64 loss: 4.567514419555664
Batch 48/64 loss: 4.551663875579834
Batch 49/64 loss: 4.54140567779541
Batch 50/64 loss: 4.534811973571777
Batch 51/64 loss: 4.515294075012207
Batch 52/64 loss: 4.5038652420043945
Batch 53/64 loss: 4.537051200866699
Batch 54/64 loss: 4.546914577484131
Batch 55/64 loss: 4.505344390869141
Batch 56/64 loss: 4.455539703369141
Batch 57/64 loss: 4.620723247528076
Batch 58/64 loss: 4.5200347900390625
Batch 59/64 loss: 4.4980788230896
Batch 60/64 loss: 4.492988109588623
Batch 61/64 loss: 4.5377655029296875
Batch 62/64 loss: 4.495827674865723
Batch 63/64 loss: 4.460015296936035
Batch 64/64 loss: 3.5370311737060547
Epoch 3  Train loss: 4.519207830990062  Val loss: 4.472865579873836
Saving best model, epoch: 3
Epoch 4
-------------------------------
Batch 1/64 loss: 4.463995456695557
Batch 2/64 loss: 4.399271011352539
Batch 3/64 loss: 4.405943393707275
Batch 4/64 loss: 4.441366195678711
Batch 5/64 loss: 4.39919376373291
Batch 6/64 loss: 4.437856674194336
Batch 7/64 loss: 4.525902271270752
Batch 8/64 loss: 4.4674763679504395
Batch 9/64 loss: 4.446439266204834
Batch 10/64 loss: 4.45580530166626
Batch 11/64 loss: 4.427828311920166
Batch 12/64 loss: 4.414692401885986
Batch 13/64 loss: 4.4842753410339355
Batch 14/64 loss: 4.501917362213135
Batch 15/64 loss: 4.49647855758667
Batch 16/64 loss: 4.438904762268066
Batch 17/64 loss: 4.454738616943359
Batch 18/64 loss: 4.433486461639404
Batch 19/64 loss: 4.418608665466309
Batch 20/64 loss: 4.449468612670898
Batch 21/64 loss: 4.403623580932617
Batch 22/64 loss: 4.479162216186523
Batch 23/64 loss: 4.387673377990723
Batch 24/64 loss: 4.4642014503479
Batch 25/64 loss: 4.482338905334473
Batch 26/64 loss: 4.462561130523682
Batch 27/64 loss: 4.429018974304199
Batch 28/64 loss: 4.459378242492676
Batch 29/64 loss: 4.508099555969238
Batch 30/64 loss: 4.378707408905029
Batch 31/64 loss: 4.435047626495361
Batch 32/64 loss: 4.411717891693115
Batch 33/64 loss: 4.458817958831787
Batch 34/64 loss: 4.469707489013672
Batch 35/64 loss: 4.574202060699463
Batch 36/64 loss: 4.4185333251953125
Batch 37/64 loss: 4.39407205581665
Batch 38/64 loss: 4.441773891448975
Batch 39/64 loss: 4.409816741943359
Batch 40/64 loss: 4.37481689453125
Batch 41/64 loss: 4.3983001708984375
Batch 42/64 loss: 4.420109272003174
Batch 43/64 loss: 4.385550498962402
Batch 44/64 loss: 4.464703559875488
Batch 45/64 loss: 4.450037956237793
Batch 46/64 loss: 4.337188720703125
Batch 47/64 loss: 4.399326324462891
Batch 48/64 loss: 4.341556549072266
Batch 49/64 loss: 4.386373996734619
Batch 50/64 loss: 4.434334754943848
Batch 51/64 loss: 4.412191867828369
Batch 52/64 loss: 4.352640151977539
Batch 53/64 loss: 4.360628604888916
Batch 54/64 loss: 4.367270469665527
Batch 55/64 loss: 4.398700714111328
Batch 56/64 loss: 4.460375785827637
Batch 57/64 loss: 4.376414775848389
Batch 58/64 loss: 4.355825901031494
Batch 59/64 loss: 4.363475322723389
Batch 60/64 loss: 4.320025444030762
Batch 61/64 loss: 4.338931560516357
Batch 62/64 loss: 4.411907196044922
Batch 63/64 loss: 4.35101318359375
Batch 64/64 loss: 3.31339168548584
Epoch 4  Train loss: 4.410648566601323  Val loss: 4.431083201542752
Saving best model, epoch: 4
Epoch 5
-------------------------------
Batch 1/64 loss: 4.333900451660156
Batch 2/64 loss: 4.410519599914551
Batch 3/64 loss: 4.379730224609375
Batch 4/64 loss: 4.327450275421143
Batch 5/64 loss: 4.322460174560547
Batch 6/64 loss: 4.403646469116211
Batch 7/64 loss: 4.315375328063965
Batch 8/64 loss: 4.37153959274292
Batch 9/64 loss: 4.369132995605469
Batch 10/64 loss: 4.359167575836182
Batch 11/64 loss: 4.3192644119262695
Batch 12/64 loss: 4.273650646209717
Batch 13/64 loss: 4.4562087059021
Batch 14/64 loss: 4.337913990020752
Batch 15/64 loss: 4.270363807678223
Batch 16/64 loss: 4.264087200164795
Batch 17/64 loss: 4.28080940246582
Batch 18/64 loss: 4.349689960479736
Batch 19/64 loss: 4.313549041748047
Batch 20/64 loss: 4.367306709289551
Batch 21/64 loss: 4.294307231903076
Batch 22/64 loss: 4.288151741027832
Batch 23/64 loss: 4.324572563171387
Batch 24/64 loss: 4.280396461486816
Batch 25/64 loss: 4.279345512390137
Batch 26/64 loss: 4.277803897857666
Batch 27/64 loss: 4.346621990203857
Batch 28/64 loss: 4.278537750244141
Batch 29/64 loss: 4.322793960571289
Batch 30/64 loss: 4.215546607971191
Batch 31/64 loss: 4.30104398727417
Batch 32/64 loss: 4.321757793426514
Batch 33/64 loss: 4.269455432891846
Batch 34/64 loss: 4.32811975479126
Batch 35/64 loss: 4.253612041473389
Batch 36/64 loss: 4.359850883483887
Batch 37/64 loss: 4.335176467895508
Batch 38/64 loss: 4.3214192390441895
Batch 39/64 loss: 4.293424606323242
Batch 40/64 loss: 4.375328063964844
Batch 41/64 loss: 4.356273651123047
Batch 42/64 loss: 4.33665132522583
Batch 43/64 loss: 4.343593120574951
Batch 44/64 loss: 4.222602367401123
Batch 45/64 loss: 4.338107109069824
Batch 46/64 loss: 4.307347297668457
Batch 47/64 loss: 4.335956573486328
Batch 48/64 loss: 4.245993137359619
Batch 49/64 loss: 4.342677116394043
Batch 50/64 loss: 4.275751113891602
Batch 51/64 loss: 4.315847873687744
Batch 52/64 loss: 4.317011833190918
Batch 53/64 loss: 4.281498432159424
Batch 54/64 loss: 4.269570827484131
Batch 55/64 loss: 4.276566505432129
Batch 56/64 loss: 4.2235517501831055
Batch 57/64 loss: 4.179295539855957
Batch 58/64 loss: 4.237770080566406
Batch 59/64 loss: 4.273749351501465
Batch 60/64 loss: 4.241645812988281
Batch 61/64 loss: 4.245636463165283
Batch 62/64 loss: 4.214740753173828
Batch 63/64 loss: 4.3490891456604
Batch 64/64 loss: 3.204069137573242
Epoch 5  Train loss: 4.295325671925264  Val loss: 4.286554777335465
Saving best model, epoch: 5
Epoch 6
-------------------------------
Batch 1/64 loss: 4.292466163635254
Batch 2/64 loss: 4.2930779457092285
Batch 3/64 loss: 4.214203834533691
Batch 4/64 loss: 4.30385160446167
Batch 5/64 loss: 4.282118797302246
Batch 6/64 loss: 4.348443031311035
Batch 7/64 loss: 4.315163612365723
Batch 8/64 loss: 4.220384120941162
Batch 9/64 loss: 4.323283672332764
Batch 10/64 loss: 4.342414855957031
Batch 11/64 loss: 4.281669616699219
Batch 12/64 loss: 4.300879955291748
Batch 13/64 loss: 4.224469184875488
Batch 14/64 loss: 4.228152751922607
Batch 15/64 loss: 4.281529426574707
Batch 16/64 loss: 4.3052215576171875
Batch 17/64 loss: 4.298882484436035
Batch 18/64 loss: 4.258244037628174
Batch 19/64 loss: 4.228510856628418
Batch 20/64 loss: 4.234992027282715
Batch 21/64 loss: 4.263586044311523
Batch 22/64 loss: 4.221097469329834
Batch 23/64 loss: 4.266270637512207
Batch 24/64 loss: 4.132471084594727
Batch 25/64 loss: 4.241039276123047
Batch 26/64 loss: 4.218960762023926
Batch 27/64 loss: 4.2723517417907715
Batch 28/64 loss: 4.149948596954346
Batch 29/64 loss: 4.239509105682373
Batch 30/64 loss: 4.208920478820801
Batch 31/64 loss: 4.175784111022949
Batch 32/64 loss: 4.21083402633667
Batch 33/64 loss: 4.229929447174072
Batch 34/64 loss: 4.552187442779541
Batch 35/64 loss: 4.205041408538818
Batch 36/64 loss: 4.160667896270752
Batch 37/64 loss: 4.2076311111450195
Batch 38/64 loss: 4.3192830085754395
Batch 39/64 loss: 4.269787311553955
Batch 40/64 loss: 4.207459926605225
Batch 41/64 loss: 4.358177185058594
Batch 42/64 loss: 4.301975727081299
Batch 43/64 loss: 4.337322235107422
Batch 44/64 loss: 4.289314270019531
Batch 45/64 loss: 4.298206806182861
Batch 46/64 loss: 4.369709014892578
Batch 47/64 loss: 4.375332832336426
Batch 48/64 loss: 4.309210300445557
Batch 49/64 loss: 4.274587154388428
Batch 50/64 loss: 4.230485916137695
Batch 51/64 loss: 4.246567726135254
Batch 52/64 loss: 4.220576763153076
Batch 53/64 loss: 4.2050676345825195
Batch 54/64 loss: 4.341433048248291
Batch 55/64 loss: 4.218996524810791
Batch 56/64 loss: 4.264353275299072
Batch 57/64 loss: 4.205804824829102
Batch 58/64 loss: 4.187619209289551
Batch 59/64 loss: 4.208987236022949
Batch 60/64 loss: 4.314477920532227
Batch 61/64 loss: 4.253905296325684
Batch 62/64 loss: 4.174784183502197
Batch 63/64 loss: 4.266515731811523
Batch 64/64 loss: 3.043778419494629
Epoch 6  Train loss: 4.248893569497501  Val loss: 4.214665181038716
Saving best model, epoch: 6
Epoch 7
-------------------------------
Batch 1/64 loss: 4.168213367462158
Batch 2/64 loss: 4.210258483886719
Batch 3/64 loss: 4.147812843322754
Batch 4/64 loss: 4.135554313659668
Batch 5/64 loss: 4.297635078430176
Batch 6/64 loss: 4.191769599914551
Batch 7/64 loss: 4.242185592651367
Batch 8/64 loss: 4.249173164367676
Batch 9/64 loss: 4.290012359619141
Batch 10/64 loss: 4.253405570983887
Batch 11/64 loss: 4.289580821990967
Batch 12/64 loss: 4.238389015197754
Batch 13/64 loss: 4.241351127624512
Batch 14/64 loss: 4.293613433837891
Batch 15/64 loss: 4.175060272216797
Batch 16/64 loss: 4.145432472229004
Batch 17/64 loss: 4.252256870269775
Batch 18/64 loss: 4.141050815582275
Batch 19/64 loss: 4.230624675750732
Batch 20/64 loss: 4.188047409057617
Batch 21/64 loss: 4.1671857833862305
Batch 22/64 loss: 4.192411422729492
Batch 23/64 loss: 4.281467437744141
Batch 24/64 loss: 4.305212020874023
Batch 25/64 loss: 4.165005207061768
Batch 26/64 loss: 4.168829917907715
Batch 27/64 loss: 4.261375904083252
Batch 28/64 loss: 4.152057647705078
Batch 29/64 loss: 4.185844421386719
Batch 30/64 loss: 4.211267948150635
Batch 31/64 loss: 4.218523025512695
Batch 32/64 loss: 4.188858985900879
Batch 33/64 loss: 4.177459716796875
Batch 34/64 loss: 4.296548843383789
Batch 35/64 loss: 4.253218173980713
Batch 36/64 loss: 4.227712631225586
Batch 37/64 loss: 4.266379356384277
Batch 38/64 loss: 4.142142295837402
Batch 39/64 loss: 4.191334247589111
Batch 40/64 loss: 4.185513973236084
Batch 41/64 loss: 4.1329665184021
Batch 42/64 loss: 4.131031036376953
Batch 43/64 loss: 4.147233009338379
Batch 44/64 loss: 4.142661094665527
Batch 45/64 loss: 4.0652174949646
Batch 46/64 loss: 4.206918716430664
Batch 47/64 loss: 4.17220401763916
Batch 48/64 loss: 4.211693286895752
Batch 49/64 loss: 4.118605613708496
Batch 50/64 loss: 4.344806671142578
Batch 51/64 loss: 4.099958419799805
Batch 52/64 loss: 4.147012233734131
Batch 53/64 loss: 4.17553186416626
Batch 54/64 loss: 4.2366766929626465
Batch 55/64 loss: 4.123341083526611
Batch 56/64 loss: 4.151062965393066
Batch 57/64 loss: 4.197281360626221
Batch 58/64 loss: 4.101589679718018
Batch 59/64 loss: 4.209840774536133
Batch 60/64 loss: 4.163190841674805
Batch 61/64 loss: 4.1374664306640625
Batch 62/64 loss: 4.335987091064453
Batch 63/64 loss: 4.283889293670654
Batch 64/64 loss: 2.829738140106201
Epoch 7  Train loss: 4.184756800707649  Val loss: 4.241160968734636
Epoch 8
-------------------------------
Batch 1/64 loss: 4.234063148498535
Batch 2/64 loss: 4.072588920593262
Batch 3/64 loss: 4.2707366943359375
Batch 4/64 loss: 4.135852336883545
Batch 5/64 loss: 4.049785137176514
Batch 6/64 loss: 4.179576873779297
Batch 7/64 loss: 4.060847759246826
Batch 8/64 loss: 4.122804164886475
Batch 9/64 loss: 4.239069938659668
Batch 10/64 loss: 4.109635829925537
Batch 11/64 loss: 4.1429290771484375
Batch 12/64 loss: 4.021761894226074
Batch 13/64 loss: 4.1097846031188965
Batch 14/64 loss: 4.155657768249512
Batch 15/64 loss: 4.022103309631348
Batch 16/64 loss: 4.114720344543457
Batch 17/64 loss: 4.055366516113281
Batch 18/64 loss: 4.255918502807617
Batch 19/64 loss: 4.137932777404785
Batch 20/64 loss: 4.246309280395508
Batch 21/64 loss: 4.042155742645264
Batch 22/64 loss: 4.258551597595215
Batch 23/64 loss: 4.124488353729248
Batch 24/64 loss: 4.160636901855469
Batch 25/64 loss: 4.180938243865967
Batch 26/64 loss: 4.200182914733887
Batch 27/64 loss: 4.158754348754883
Batch 28/64 loss: 4.1350321769714355
Batch 29/64 loss: 4.180106163024902
Batch 30/64 loss: 4.064866065979004
Batch 31/64 loss: 4.098215103149414
Batch 32/64 loss: 4.167506694793701
Batch 33/64 loss: 4.182847023010254
Batch 34/64 loss: 4.05440616607666
Batch 35/64 loss: 4.042954444885254
Batch 36/64 loss: 4.130962371826172
Batch 37/64 loss: 4.284618377685547
Batch 38/64 loss: 4.052546501159668
Batch 39/64 loss: 4.112148284912109
Batch 40/64 loss: 4.117107391357422
Batch 41/64 loss: 4.112356662750244
Batch 42/64 loss: 4.128002166748047
Batch 43/64 loss: 4.4040374755859375
Batch 44/64 loss: 4.215676307678223
Batch 45/64 loss: 4.175723075866699
Batch 46/64 loss: 4.195690155029297
Batch 47/64 loss: 4.04813814163208
Batch 48/64 loss: 4.159841060638428
Batch 49/64 loss: 4.072658538818359
Batch 50/64 loss: 4.124229431152344
Batch 51/64 loss: 4.067506313323975
Batch 52/64 loss: 4.095672130584717
Batch 53/64 loss: 4.082002639770508
Batch 54/64 loss: 4.018856048583984
Batch 55/64 loss: 4.078372001647949
Batch 56/64 loss: 4.130579471588135
Batch 57/64 loss: 4.033777236938477
Batch 58/64 loss: 4.036680221557617
Batch 59/64 loss: 4.052202224731445
Batch 60/64 loss: 3.940159797668457
Batch 61/64 loss: 3.958441972732544
Batch 62/64 loss: 4.11207914352417
Batch 63/64 loss: 4.0157694816589355
Batch 64/64 loss: 2.63252854347229
Epoch 8  Train loss: 4.105401064367856  Val loss: 4.0545252246135695
Saving best model, epoch: 8
Epoch 9
-------------------------------
Batch 1/64 loss: 4.095263481140137
Batch 2/64 loss: 4.2048563957214355
Batch 3/64 loss: 3.9325990676879883
Batch 4/64 loss: 3.998857259750366
Batch 5/64 loss: 4.0753173828125
Batch 6/64 loss: 4.045136451721191
Batch 7/64 loss: 3.9841527938842773
Batch 8/64 loss: 3.994035005569458
Batch 9/64 loss: 4.2365522384643555
Batch 10/64 loss: 4.010682582855225
Batch 11/64 loss: 3.9522838592529297
Batch 12/64 loss: 4.15792179107666
Batch 13/64 loss: 3.9854354858398438
Batch 14/64 loss: 3.903334617614746
Batch 15/64 loss: 3.919248342514038
Batch 16/64 loss: 3.8819096088409424
Batch 17/64 loss: 3.960742950439453
Batch 18/64 loss: 4.07747220993042
Batch 19/64 loss: 4.091087341308594
Batch 20/64 loss: 4.100456237792969
Batch 21/64 loss: 3.9277029037475586
Batch 22/64 loss: 4.000308990478516
Batch 23/64 loss: 4.0260748863220215
Batch 24/64 loss: 4.049794673919678
Batch 25/64 loss: 3.9325618743896484
Batch 26/64 loss: 3.9993481636047363
Batch 27/64 loss: 4.102849960327148
Batch 28/64 loss: 4.130410194396973
Batch 29/64 loss: 4.042998313903809
Batch 30/64 loss: 4.071786880493164
Batch 31/64 loss: 4.013934135437012
Batch 32/64 loss: 4.0498199462890625
Batch 33/64 loss: 4.010964870452881
Batch 34/64 loss: 4.012692928314209
Batch 35/64 loss: 4.009481430053711
Batch 36/64 loss: 3.905989408493042
Batch 37/64 loss: 4.004111289978027
Batch 38/64 loss: 4.008127689361572
Batch 39/64 loss: 3.9034974575042725
Batch 40/64 loss: 4.101373195648193
Batch 41/64 loss: 4.126195907592773
Batch 42/64 loss: 3.984701633453369
Batch 43/64 loss: 4.065521717071533
Batch 44/64 loss: 3.9986376762390137
Batch 45/64 loss: 4.001977920532227
Batch 46/64 loss: 3.9461307525634766
Batch 47/64 loss: 3.9423766136169434
Batch 48/64 loss: 4.083087921142578
Batch 49/64 loss: 4.14365291595459
Batch 50/64 loss: 3.9270827770233154
Batch 51/64 loss: 4.023645877838135
Batch 52/64 loss: 4.088743209838867
Batch 53/64 loss: 3.9752206802368164
Batch 54/64 loss: 3.845449447631836
Batch 55/64 loss: 4.038716793060303
Batch 56/64 loss: 4.286009788513184
Batch 57/64 loss: 3.807241439819336
Batch 58/64 loss: 4.104034423828125
Batch 59/64 loss: 4.006472110748291
Batch 60/64 loss: 3.967573642730713
Batch 61/64 loss: 4.047478675842285
Batch 62/64 loss: 4.040034770965576
Batch 63/64 loss: 4.0402398109436035
Batch 64/64 loss: 2.5662875175476074
Epoch 9  Train loss: 4.005115552042045  Val loss: 4.0578099206550835
Epoch 10
-------------------------------
Batch 1/64 loss: 3.9749410152435303
Batch 2/64 loss: 3.926419734954834
Batch 3/64 loss: 3.978774309158325
Batch 4/64 loss: 4.025498867034912
Batch 5/64 loss: 3.881260871887207
Batch 6/64 loss: 3.856311321258545
Batch 7/64 loss: 3.9185261726379395
Batch 8/64 loss: 4.21191930770874
Batch 9/64 loss: 3.984966993331909
Batch 10/64 loss: 3.9267516136169434
Batch 11/64 loss: 4.17177152633667
Batch 12/64 loss: 3.94882869720459
Batch 13/64 loss: 3.8806049823760986
Batch 14/64 loss: 3.8689656257629395
Batch 15/64 loss: 3.9525303840637207
Batch 16/64 loss: 3.9578003883361816
Batch 17/64 loss: 3.9365134239196777
Batch 18/64 loss: 4.09539794921875
Batch 19/64 loss: 3.9626431465148926
Batch 20/64 loss: 3.8061723709106445
Batch 21/64 loss: 4.029657363891602
Batch 22/64 loss: 3.88688325881958
Batch 23/64 loss: 3.798203468322754
Batch 24/64 loss: 3.8526012897491455
Batch 25/64 loss: 3.9108853340148926
Batch 26/64 loss: 4.11152982711792
Batch 27/64 loss: 3.855651378631592
Batch 28/64 loss: 3.9440417289733887
Batch 29/64 loss: 3.9868204593658447
Batch 30/64 loss: 3.9269018173217773
Batch 31/64 loss: 3.9567530155181885
Batch 32/64 loss: 3.8672025203704834
Batch 33/64 loss: 4.049980163574219
Batch 34/64 loss: 3.9437146186828613
Batch 35/64 loss: 3.7823781967163086
Batch 36/64 loss: 3.9300103187561035
Batch 37/64 loss: 3.9281258583068848
Batch 38/64 loss: 3.74387264251709
Batch 39/64 loss: 3.886228561401367
Batch 40/64 loss: 3.8050694465637207
Batch 41/64 loss: 3.953514575958252
Batch 42/64 loss: 3.863567352294922
Batch 43/64 loss: 3.885712146759033
Batch 44/64 loss: 4.0420331954956055
Batch 45/64 loss: 3.8080177307128906
Batch 46/64 loss: 3.811018943786621
Batch 47/64 loss: 3.7527427673339844
Batch 48/64 loss: 3.9378397464752197
Batch 49/64 loss: 3.999739646911621
Batch 50/64 loss: 3.8105080127716064
Batch 51/64 loss: 3.668006658554077
Batch 52/64 loss: 3.9357595443725586
Batch 53/64 loss: 3.8944344520568848
Batch 54/64 loss: 4.009585380554199
Batch 55/64 loss: 3.7593915462493896
Batch 56/64 loss: 3.9310319423675537
Batch 57/64 loss: 3.754570484161377
Batch 58/64 loss: 3.8463122844696045
Batch 59/64 loss: 3.78159236907959
Batch 60/64 loss: 3.7117457389831543
Batch 61/64 loss: 3.842216968536377
Batch 62/64 loss: 4.051603317260742
Batch 63/64 loss: 3.9895262718200684
Batch 64/64 loss: 2.3719184398651123
Epoch 10  Train loss: 3.8946277627757953  Val loss: 4.197854728633186
Epoch 11
-------------------------------
Batch 1/64 loss: 3.9094038009643555
Batch 2/64 loss: 4.035462856292725
Batch 3/64 loss: 3.9194326400756836
Batch 4/64 loss: 3.8844268321990967
Batch 5/64 loss: 3.9024124145507812
Batch 6/64 loss: 3.859180450439453
Batch 7/64 loss: 3.8986806869506836
Batch 8/64 loss: 3.811889171600342
Batch 9/64 loss: 3.81390380859375
Batch 10/64 loss: 3.8509178161621094
Batch 11/64 loss: 3.7992377281188965
Batch 12/64 loss: 3.722576141357422
Batch 13/64 loss: 3.8695216178894043
Batch 14/64 loss: 3.7616147994995117
Batch 15/64 loss: 3.766510486602783
Batch 16/64 loss: 3.8298912048339844
Batch 17/64 loss: 3.7231249809265137
Batch 18/64 loss: 3.674996852874756
Batch 19/64 loss: 3.874147415161133
Batch 20/64 loss: 3.7420547008514404
Batch 21/64 loss: 3.788508892059326
Batch 22/64 loss: 3.548658609390259
Batch 23/64 loss: 3.7606191635131836
Batch 24/64 loss: 3.735295534133911
Batch 25/64 loss: 3.661130905151367
Batch 26/64 loss: 3.763235569000244
Batch 27/64 loss: 3.7172634601593018
Batch 28/64 loss: 3.7024238109588623
Batch 29/64 loss: 3.811911106109619
Batch 30/64 loss: 3.7584469318389893
Batch 31/64 loss: 3.7798659801483154
Batch 32/64 loss: 3.6617236137390137
Batch 33/64 loss: 3.7721664905548096
Batch 34/64 loss: 3.7161853313446045
Batch 35/64 loss: 3.752802610397339
Batch 36/64 loss: 3.7024223804473877
Batch 37/64 loss: 3.868393898010254
Batch 38/64 loss: 3.6794376373291016
Batch 39/64 loss: 3.59110951423645
Batch 40/64 loss: 3.82651424407959
Batch 41/64 loss: 3.5679569244384766
Batch 42/64 loss: 3.6573920249938965
Batch 43/64 loss: 3.612231492996216
Batch 44/64 loss: 3.719053030014038
Batch 45/64 loss: 3.6764204502105713
Batch 46/64 loss: 3.6280791759490967
Batch 47/64 loss: 3.712350606918335
Batch 48/64 loss: 3.710092067718506
Batch 49/64 loss: 3.7272777557373047
Batch 50/64 loss: 3.655662775039673
Batch 51/64 loss: 3.7515835762023926
Batch 52/64 loss: 3.7621426582336426
Batch 53/64 loss: 3.887939929962158
Batch 54/64 loss: 3.6615424156188965
Batch 55/64 loss: 3.8079652786254883
Batch 56/64 loss: 3.5903775691986084
Batch 57/64 loss: 3.66379976272583
Batch 58/64 loss: 3.8067214488983154
Batch 59/64 loss: 3.736842155456543
Batch 60/64 loss: 3.6619815826416016
Batch 61/64 loss: 3.6715810298919678
Batch 62/64 loss: 3.775991916656494
Batch 63/64 loss: 3.846888780593872
Batch 64/64 loss: 2.0716612339019775
Epoch 11  Train loss: 3.734794077218748  Val loss: 4.176590218167124
Epoch 12
-------------------------------
Batch 1/64 loss: 3.8327479362487793
Batch 2/64 loss: 3.8489296436309814
Batch 3/64 loss: 3.7878150939941406
Batch 4/64 loss: 3.790560007095337
Batch 5/64 loss: 3.6424167156219482
Batch 6/64 loss: 3.617396116256714
Batch 7/64 loss: 3.5801055431365967
Batch 8/64 loss: 3.673326253890991
Batch 9/64 loss: 3.6612486839294434
Batch 10/64 loss: 3.738142967224121
Batch 11/64 loss: 3.687938690185547
Batch 12/64 loss: 3.824028730392456
Batch 13/64 loss: 3.730207920074463
Batch 14/64 loss: 3.5418262481689453
Batch 15/64 loss: 3.652470588684082
Batch 16/64 loss: 3.7736551761627197
Batch 17/64 loss: 3.59712290763855
Batch 18/64 loss: 3.6078035831451416
Batch 19/64 loss: 3.6607820987701416
Batch 20/64 loss: 3.743602991104126
Batch 21/64 loss: 3.762657880783081
Batch 22/64 loss: 3.7259304523468018
Batch 23/64 loss: 3.701838970184326
Batch 24/64 loss: 3.650381565093994
Batch 25/64 loss: 3.689056396484375
Batch 26/64 loss: 3.6628079414367676
Batch 27/64 loss: 3.653122901916504
Batch 28/64 loss: 3.535644769668579
Batch 29/64 loss: 3.874964475631714
Batch 30/64 loss: 3.556959867477417
Batch 31/64 loss: 3.6056268215179443
Batch 32/64 loss: 3.7433037757873535
Batch 33/64 loss: 3.697558879852295
Batch 34/64 loss: 3.706326961517334
Batch 35/64 loss: 3.594191551208496
Batch 36/64 loss: 3.6671249866485596
Batch 37/64 loss: 3.572338342666626
Batch 38/64 loss: 3.6847407817840576
Batch 39/64 loss: 3.543165683746338
Batch 40/64 loss: 3.784684896469116
Batch 41/64 loss: 3.5561399459838867
Batch 42/64 loss: 3.510045051574707
Batch 43/64 loss: 3.517667770385742
Batch 44/64 loss: 3.60568904876709
Batch 45/64 loss: 3.5838584899902344
Batch 46/64 loss: 3.585845947265625
Batch 47/64 loss: 3.531582832336426
Batch 48/64 loss: 3.4173359870910645
Batch 49/64 loss: 3.39373779296875
Batch 50/64 loss: 3.5804173946380615
Batch 51/64 loss: 3.5440995693206787
Batch 52/64 loss: 3.536097288131714
Batch 53/64 loss: 3.7776119709014893
Batch 54/64 loss: 3.3713865280151367
Batch 55/64 loss: 3.5296804904937744
Batch 56/64 loss: 3.3726882934570312
Batch 57/64 loss: 3.436289072036743
Batch 58/64 loss: 3.4749271869659424
Batch 59/64 loss: 3.5490036010742188
Batch 60/64 loss: 3.3482279777526855
Batch 61/64 loss: 3.5782907009124756
Batch 62/64 loss: 3.3199777603149414
Batch 63/64 loss: 3.5241456031799316
Batch 64/64 loss: 1.5768496990203857
Epoch 12  Train loss: 3.5957951274572633  Val loss: 3.9691382273775604
Saving best model, epoch: 12
Epoch 13
-------------------------------
Batch 1/64 loss: 3.3078606128692627
Batch 2/64 loss: 3.4876179695129395
Batch 3/64 loss: 3.3326165676116943
Batch 4/64 loss: 3.6474215984344482
Batch 5/64 loss: 3.290374517440796
Batch 6/64 loss: 3.5793120861053467
Batch 7/64 loss: 3.9360029697418213
Batch 8/64 loss: 3.5987801551818848
Batch 9/64 loss: 3.4862561225891113
Batch 10/64 loss: 3.5565342903137207
Batch 11/64 loss: 3.504180669784546
Batch 12/64 loss: 3.5140955448150635
Batch 13/64 loss: 3.297894239425659
Batch 14/64 loss: 3.457563638687134
Batch 15/64 loss: 3.492238759994507
Batch 16/64 loss: 3.3433282375335693
Batch 17/64 loss: 3.4651851654052734
Batch 18/64 loss: 3.406771183013916
Batch 19/64 loss: 3.475750207901001
Batch 20/64 loss: 3.498558282852173
Batch 21/64 loss: 3.4760522842407227
Batch 22/64 loss: 3.3898820877075195
Batch 23/64 loss: 3.479626178741455
Batch 24/64 loss: 3.4057419300079346
Batch 25/64 loss: 3.219695568084717
Batch 26/64 loss: 3.4279062747955322
Batch 27/64 loss: 3.337099313735962
Batch 28/64 loss: 3.435619354248047
Batch 29/64 loss: 3.4564361572265625
Batch 30/64 loss: 3.3650126457214355
Batch 31/64 loss: 3.4182775020599365
Batch 32/64 loss: 3.299079179763794
Batch 33/64 loss: 3.559962272644043
Batch 34/64 loss: 3.296255111694336
Batch 35/64 loss: 3.3966379165649414
Batch 36/64 loss: 3.4288480281829834
Batch 37/64 loss: 3.37530779838562
Batch 38/64 loss: 3.330655813217163
Batch 39/64 loss: 3.5278053283691406
Batch 40/64 loss: 3.251600503921509
Batch 41/64 loss: 3.2442026138305664
Batch 42/64 loss: 3.3370282649993896
Batch 43/64 loss: 3.313877582550049
Batch 44/64 loss: 3.4720702171325684
Batch 45/64 loss: 3.2363250255584717
Batch 46/64 loss: 3.31164813041687
Batch 47/64 loss: 3.3468124866485596
Batch 48/64 loss: 3.532498598098755
Batch 49/64 loss: 3.3422133922576904
Batch 50/64 loss: 3.3750932216644287
Batch 51/64 loss: 3.4642205238342285
Batch 52/64 loss: 3.379225015640259
Batch 53/64 loss: 3.4624359607696533
Batch 54/64 loss: 3.3247077465057373
Batch 55/64 loss: 3.481761932373047
Batch 56/64 loss: 3.454658031463623
Batch 57/64 loss: 3.6463985443115234
Batch 58/64 loss: 3.1749372482299805
Batch 59/64 loss: 3.3039891719818115
Batch 60/64 loss: 3.327029228210449
Batch 61/64 loss: 3.342320680618286
Batch 62/64 loss: 3.39968204498291
Batch 63/64 loss: 3.228978157043457
Batch 64/64 loss: 1.3800034523010254
Epoch 13  Train loss: 3.3896934976764754  Val loss: 4.863656565086129
Epoch 14
-------------------------------
Batch 1/64 loss: 3.4092862606048584
Batch 2/64 loss: 3.567054271697998
Batch 3/64 loss: 3.1724748611450195
Batch 4/64 loss: 3.165898323059082
Batch 5/64 loss: 3.4281063079833984
Batch 6/64 loss: 3.15663480758667
Batch 7/64 loss: 3.316033124923706
Batch 8/64 loss: 3.1940255165100098
Batch 9/64 loss: 3.407085418701172
Batch 10/64 loss: 3.316523551940918
Batch 11/64 loss: 3.0860705375671387
Batch 12/64 loss: 3.1969656944274902
Batch 13/64 loss: 3.4541802406311035
Batch 14/64 loss: 3.2590086460113525
Batch 15/64 loss: 3.221755027770996
Batch 16/64 loss: 3.037790060043335
Batch 17/64 loss: 3.155024290084839
Batch 18/64 loss: 3.0469815731048584
Batch 19/64 loss: 3.137040376663208
Batch 20/64 loss: 3.457411766052246
Batch 21/64 loss: 3.052821636199951
Batch 22/64 loss: 3.1268768310546875
Batch 23/64 loss: 2.9702272415161133
Batch 24/64 loss: 3.1821072101593018
Batch 25/64 loss: 3.204258441925049
Batch 26/64 loss: 3.199202537536621
Batch 27/64 loss: 3.0570778846740723
Batch 28/64 loss: 2.9964654445648193
Batch 29/64 loss: 3.084442138671875
Batch 30/64 loss: 2.9885168075561523
Batch 31/64 loss: 2.8749077320098877
Batch 32/64 loss: 3.1971476078033447
Batch 33/64 loss: 2.9783949851989746
Batch 34/64 loss: 3.163337230682373
Batch 35/64 loss: 3.0220959186553955
Batch 36/64 loss: 3.0022947788238525
Batch 37/64 loss: 2.9286231994628906
Batch 38/64 loss: 3.1865851879119873
Batch 39/64 loss: 2.9782769680023193
Batch 40/64 loss: 2.967024564743042
Batch 41/64 loss: 3.24340558052063
Batch 42/64 loss: 3.116607427597046
Batch 43/64 loss: 2.8704323768615723
Batch 44/64 loss: 2.811439037322998
Batch 45/64 loss: 3.043121814727783
Batch 46/64 loss: 2.959214210510254
Batch 47/64 loss: 2.8926870822906494
Batch 48/64 loss: 2.7801742553710938
Batch 49/64 loss: 2.975735664367676
Batch 50/64 loss: 3.0733578205108643
Batch 51/64 loss: 2.903959274291992
Batch 52/64 loss: 2.6990318298339844
Batch 53/64 loss: 2.9293036460876465
Batch 54/64 loss: 2.9872400760650635
Batch 55/64 loss: 2.9571218490600586
Batch 56/64 loss: 2.8545584678649902
Batch 57/64 loss: 2.8505425453186035
Batch 58/64 loss: 2.9150266647338867
Batch 59/64 loss: 2.857512950897217
Batch 60/64 loss: 3.0701961517333984
Batch 61/64 loss: 3.1944539546966553
Batch 62/64 loss: 2.968324661254883
Batch 63/64 loss: 2.978670835494995
Batch 64/64 loss: 0.6330900192260742
Epoch 14  Train loss: 3.054948559929343  Val loss: 3.0970875487704457
Saving best model, epoch: 14
Epoch 15
-------------------------------
Batch 1/64 loss: 2.9670498371124268
Batch 2/64 loss: 2.7045865058898926
Batch 3/64 loss: 2.910163402557373
Batch 4/64 loss: 3.0004515647888184
Batch 5/64 loss: 2.984245777130127
Batch 6/64 loss: 2.8037214279174805
Batch 7/64 loss: 2.830854892730713
Batch 8/64 loss: 2.709672451019287
Batch 9/64 loss: 2.9904911518096924
Batch 10/64 loss: 2.798487424850464
Batch 11/64 loss: 3.0303964614868164
Batch 12/64 loss: 2.6922523975372314
Batch 13/64 loss: 2.707186698913574
Batch 14/64 loss: 2.7403931617736816
Batch 15/64 loss: 3.227477550506592
Batch 16/64 loss: 3.139866352081299
Batch 17/64 loss: 2.7416253089904785
Batch 18/64 loss: 2.626049041748047
Batch 19/64 loss: 2.7347412109375
Batch 20/64 loss: 2.5917210578918457
Batch 21/64 loss: 2.5647783279418945
Batch 22/64 loss: 2.7325916290283203
Batch 23/64 loss: 2.6644577980041504
Batch 24/64 loss: 3.0418167114257812
Batch 25/64 loss: 2.757761001586914
Batch 26/64 loss: 2.780792236328125
Batch 27/64 loss: 2.8436086177825928
Batch 28/64 loss: 2.8193788528442383
Batch 29/64 loss: 2.830700159072876
Batch 30/64 loss: 2.986311912536621
Batch 31/64 loss: 2.792943239212036
Batch 32/64 loss: 2.837876319885254
Batch 33/64 loss: 2.689789295196533
Batch 34/64 loss: 2.8682806491851807
Batch 35/64 loss: 2.846743583679199
Batch 36/64 loss: 2.8313233852386475
Batch 37/64 loss: 2.9016342163085938
Batch 38/64 loss: 2.904050827026367
Batch 39/64 loss: 2.6630940437316895
Batch 40/64 loss: 2.7671918869018555
Batch 41/64 loss: 2.760784149169922
Batch 42/64 loss: 2.5730834007263184
Batch 43/64 loss: 2.833871364593506
Batch 44/64 loss: 3.2247841358184814
Batch 45/64 loss: 2.623281478881836
Batch 46/64 loss: 2.8925657272338867
Batch 47/64 loss: 2.841902732849121
Batch 48/64 loss: 2.9054269790649414
Batch 49/64 loss: 2.756028652191162
Batch 50/64 loss: 2.9290895462036133
Batch 51/64 loss: 2.9101624488830566
Batch 52/64 loss: 2.7328615188598633
Batch 53/64 loss: 2.751339912414551
Batch 54/64 loss: 2.5786924362182617
Batch 55/64 loss: 2.841588020324707
Batch 56/64 loss: 2.9333839416503906
Batch 57/64 loss: 2.7935471534729004
Batch 58/64 loss: 2.7224273681640625
Batch 59/64 loss: 2.679683208465576
Batch 60/64 loss: 3.17197322845459
Batch 61/64 loss: 2.6454615592956543
Batch 62/64 loss: 2.5743751525878906
Batch 63/64 loss: 2.513216972351074
Batch 64/64 loss: 0.0012402534484863281
Epoch 15  Train loss: 2.780345406251795  Val loss: 3.0060141389722266
Saving best model, epoch: 15
Epoch 16
-------------------------------
Batch 1/64 loss: 2.6953816413879395
Batch 2/64 loss: 2.6014914512634277
Batch 3/64 loss: 2.457352638244629
Batch 4/64 loss: 2.5297999382019043
Batch 5/64 loss: 2.4886717796325684
Batch 6/64 loss: 2.384152889251709
Batch 7/64 loss: 2.6144208908081055
Batch 8/64 loss: 2.664926528930664
Batch 9/64 loss: 2.903968334197998
Batch 10/64 loss: 2.3881964683532715
Batch 11/64 loss: 2.459714412689209
Batch 12/64 loss: 2.630347728729248
Batch 13/64 loss: 2.5933022499084473
Batch 14/64 loss: 2.6088242530822754
Batch 15/64 loss: 2.5561389923095703
Batch 16/64 loss: 2.7078347206115723
Batch 17/64 loss: 2.3861470222473145
Batch 18/64 loss: 2.436863899230957
Batch 19/64 loss: 2.571526050567627
Batch 20/64 loss: 2.523052215576172
Batch 21/64 loss: 2.325863838195801
Batch 22/64 loss: 2.9361701011657715
Batch 23/64 loss: 2.5550475120544434
Batch 24/64 loss: 2.4293594360351562
Batch 25/64 loss: 2.350463390350342
Batch 26/64 loss: 2.2594962120056152
Batch 27/64 loss: 2.5210771560668945
Batch 28/64 loss: 2.4116711616516113
Batch 29/64 loss: 2.439772129058838
Batch 30/64 loss: 2.496346950531006
Batch 31/64 loss: 2.275768280029297
Batch 32/64 loss: 2.302861213684082
Batch 33/64 loss: 2.5500712394714355
Batch 34/64 loss: 2.482685089111328
Batch 35/64 loss: 2.428485870361328
Batch 36/64 loss: 2.6022043228149414
Batch 37/64 loss: 2.3481149673461914
Batch 38/64 loss: 2.366179943084717
Batch 39/64 loss: 2.358551025390625
Batch 40/64 loss: 2.358427047729492
Batch 41/64 loss: 2.345289707183838
Batch 42/64 loss: 2.2598538398742676
Batch 43/64 loss: 2.6936707496643066
Batch 44/64 loss: 2.474177837371826
Batch 45/64 loss: 2.865023136138916
Batch 46/64 loss: 2.3944826126098633
Batch 47/64 loss: 2.200801372528076
Batch 48/64 loss: 2.391596794128418
Batch 49/64 loss: 2.1815433502197266
Batch 50/64 loss: 2.606614112854004
Batch 51/64 loss: 2.3902392387390137
Batch 52/64 loss: 2.402012348175049
Batch 53/64 loss: 2.309117317199707
Batch 54/64 loss: 2.2080907821655273
Batch 55/64 loss: 2.2101569175720215
Batch 56/64 loss: 2.720294952392578
Batch 57/64 loss: 2.2217564582824707
Batch 58/64 loss: 2.410280227661133
Batch 59/64 loss: 2.7625856399536133
Batch 60/64 loss: 2.1337008476257324
Batch 61/64 loss: 2.4502806663513184
Batch 62/64 loss: 2.6544651985168457
Batch 63/64 loss: 2.2498998641967773
Batch 64/64 loss: 0.019320964813232422
Epoch 16  Train loss: 2.44001812841378  Val loss: 2.939067080258504
Saving best model, epoch: 16
Epoch 17
-------------------------------
Batch 1/64 loss: 2.3043570518493652
Batch 2/64 loss: 2.320502281188965
Batch 3/64 loss: 2.3937435150146484
Batch 4/64 loss: 2.5797810554504395
Batch 5/64 loss: 2.450406074523926
Batch 6/64 loss: 2.2659878730773926
Batch 7/64 loss: 2.2007079124450684
Batch 8/64 loss: 2.588350296020508
Batch 9/64 loss: 2.4843850135803223
Batch 10/64 loss: 2.5090508460998535
Batch 11/64 loss: 2.111541271209717
Batch 12/64 loss: 2.323518753051758
Batch 13/64 loss: 2.3649401664733887
Batch 14/64 loss: 2.393345832824707
Batch 15/64 loss: 2.5196971893310547
Batch 16/64 loss: 2.3731794357299805
Batch 17/64 loss: 2.438917636871338
Batch 18/64 loss: 2.634800910949707
Batch 19/64 loss: 2.324606418609619
Batch 20/64 loss: 2.2937393188476562
Batch 21/64 loss: 2.2575020790100098
Batch 22/64 loss: 2.2880334854125977
Batch 23/64 loss: 2.2249741554260254
Batch 24/64 loss: 2.3515658378601074
Batch 25/64 loss: 2.036220073699951
Batch 26/64 loss: 2.2256202697753906
Batch 27/64 loss: 2.3805460929870605
Batch 28/64 loss: 2.387389659881592
Batch 29/64 loss: 2.334036350250244
Batch 30/64 loss: 2.3906755447387695
Batch 31/64 loss: 2.4213123321533203
Batch 32/64 loss: 2.6664199829101562
Batch 33/64 loss: 2.1252055168151855
Batch 34/64 loss: 2.116799831390381
Batch 35/64 loss: 2.355381488800049
Batch 36/64 loss: 2.267207622528076
Batch 37/64 loss: 2.21157169342041
Batch 38/64 loss: 2.2434611320495605
Batch 39/64 loss: 2.5509395599365234
Batch 40/64 loss: 2.3007044792175293
Batch 41/64 loss: 2.6412267684936523
Batch 42/64 loss: 2.5126614570617676
Batch 43/64 loss: 2.434509754180908
Batch 44/64 loss: 2.518481731414795
Batch 45/64 loss: 2.356823444366455
Batch 46/64 loss: 2.2285447120666504
Batch 47/64 loss: 2.3618931770324707
Batch 48/64 loss: 2.2736058235168457
Batch 49/64 loss: 2.261219024658203
Batch 50/64 loss: 2.4756078720092773
Batch 51/64 loss: 2.1274757385253906
Batch 52/64 loss: 2.2066650390625
Batch 53/64 loss: 2.1544833183288574
Batch 54/64 loss: 2.0025062561035156
Batch 55/64 loss: 2.0018739700317383
Batch 56/64 loss: 2.783566951751709
Batch 57/64 loss: 2.280484199523926
Batch 58/64 loss: 1.8112363815307617
Batch 59/64 loss: 2.3256564140319824
Batch 60/64 loss: 2.0907773971557617
Batch 61/64 loss: 2.1965346336364746
Batch 62/64 loss: 2.0305261611938477
Batch 63/64 loss: 2.2108211517333984
Batch 64/64 loss: -0.4012742042541504
Epoch 17  Train loss: 2.2901545375001198  Val loss: 2.4259516463656605
Saving best model, epoch: 17
Epoch 18
-------------------------------
Batch 1/64 loss: 2.2525410652160645
Batch 2/64 loss: 2.3719701766967773
Batch 3/64 loss: 2.411449909210205
Batch 4/64 loss: 2.273134708404541
Batch 5/64 loss: 2.3644356727600098
Batch 6/64 loss: 2.5758538246154785
Batch 7/64 loss: 2.0814313888549805
Batch 8/64 loss: 1.961359977722168
Batch 9/64 loss: 2.2478551864624023
Batch 10/64 loss: 1.9509196281433105
Batch 11/64 loss: 2.0317487716674805
Batch 12/64 loss: 2.260118007659912
Batch 13/64 loss: 2.1974716186523438
Batch 14/64 loss: 2.0611796379089355
Batch 15/64 loss: 1.9179272651672363
Batch 16/64 loss: 2.302170753479004
Batch 17/64 loss: 2.199042320251465
Batch 18/64 loss: 2.0850396156311035
Batch 19/64 loss: 2.2842016220092773
Batch 20/64 loss: 2.4498157501220703
Batch 21/64 loss: 2.269759178161621
Batch 22/64 loss: 2.2313599586486816
Batch 23/64 loss: 2.156229019165039
Batch 24/64 loss: 2.1408777236938477
Batch 25/64 loss: 2.1817731857299805
Batch 26/64 loss: 2.529402732849121
Batch 27/64 loss: 2.181154251098633
Batch 28/64 loss: 1.9801573753356934
Batch 29/64 loss: 2.238156318664551
Batch 30/64 loss: 2.2119650840759277
Batch 31/64 loss: 2.381047248840332
Batch 32/64 loss: 2.1373629570007324
Batch 33/64 loss: 2.113636016845703
Batch 34/64 loss: 1.9137511253356934
Batch 35/64 loss: 1.9397053718566895
Batch 36/64 loss: 1.9192614555358887
Batch 37/64 loss: 2.155196189880371
Batch 38/64 loss: 2.1597108840942383
Batch 39/64 loss: 2.1405091285705566
Batch 40/64 loss: 2.0693764686584473
Batch 41/64 loss: 1.8103618621826172
Batch 42/64 loss: 2.6191582679748535
Batch 43/64 loss: 1.9059967994689941
Batch 44/64 loss: 2.1071457862854004
Batch 45/64 loss: 2.069868564605713
Batch 46/64 loss: 1.7192788124084473
Batch 47/64 loss: 2.242551803588867
Batch 48/64 loss: 1.9880218505859375
Batch 49/64 loss: 1.965214729309082
Batch 50/64 loss: 1.8080711364746094
Batch 51/64 loss: 1.998392105102539
Batch 52/64 loss: 1.94301176071167
Batch 53/64 loss: 1.9149646759033203
Batch 54/64 loss: 1.8114118576049805
Batch 55/64 loss: 1.8199543952941895
Batch 56/64 loss: 1.7669143676757812
Batch 57/64 loss: 1.808553695678711
Batch 58/64 loss: 1.8049373626708984
Batch 59/64 loss: 2.1067404747009277
Batch 60/64 loss: 1.7543106079101562
Batch 61/64 loss: 1.719912052154541
Batch 62/64 loss: 1.9032282829284668
Batch 63/64 loss: 1.7749500274658203
Batch 64/64 loss: -0.908515453338623
Epoch 18  Train loss: 2.055084286484064  Val loss: 1.8843820054096865
Saving best model, epoch: 18
Epoch 19
-------------------------------
Batch 1/64 loss: 1.624387264251709
Batch 2/64 loss: 1.8063139915466309
Batch 3/64 loss: 1.9668564796447754
Batch 4/64 loss: 2.1442675590515137
Batch 5/64 loss: 2.4080557823181152
Batch 6/64 loss: 1.7185916900634766
Batch 7/64 loss: 1.8530831336975098
Batch 8/64 loss: 1.7983293533325195
Batch 9/64 loss: 1.767014503479004
Batch 10/64 loss: 1.7740321159362793
Batch 11/64 loss: 1.7513933181762695
Batch 12/64 loss: 2.6226353645324707
Batch 13/64 loss: 1.9898509979248047
Batch 14/64 loss: 1.6449308395385742
Batch 15/64 loss: 1.8362321853637695
Batch 16/64 loss: 1.7479910850524902
Batch 17/64 loss: 2.485260009765625
Batch 18/64 loss: 1.817671298980713
Batch 19/64 loss: 1.72993803024292
Batch 20/64 loss: 1.5246853828430176
Batch 21/64 loss: 2.1178932189941406
Batch 22/64 loss: 1.8541579246520996
Batch 23/64 loss: 1.794682502746582
Batch 24/64 loss: 1.5596346855163574
Batch 25/64 loss: 1.644383430480957
Batch 26/64 loss: 1.787137508392334
Batch 27/64 loss: 1.4541168212890625
Batch 28/64 loss: 2.0308146476745605
Batch 29/64 loss: 1.4882221221923828
Batch 30/64 loss: 1.949777603149414
Batch 31/64 loss: 1.5236334800720215
Batch 32/64 loss: 1.6822047233581543
Batch 33/64 loss: 1.5735588073730469
Batch 34/64 loss: 1.7682666778564453
Batch 35/64 loss: 1.5472192764282227
Batch 36/64 loss: 1.8733549118041992
Batch 37/64 loss: 1.668036937713623
Batch 38/64 loss: 1.7725152969360352
Batch 39/64 loss: 1.6990647315979004
Batch 40/64 loss: 1.605794906616211
Batch 41/64 loss: 1.5619497299194336
Batch 42/64 loss: 1.671433925628662
Batch 43/64 loss: 1.406766414642334
Batch 44/64 loss: 1.472825527191162
Batch 45/64 loss: 1.8046493530273438
Batch 46/64 loss: 1.7748336791992188
Batch 47/64 loss: 1.5145277976989746
Batch 48/64 loss: 1.5950398445129395
Batch 49/64 loss: 1.7252063751220703
Batch 50/64 loss: 1.7756590843200684
Batch 51/64 loss: 1.5774989128112793
Batch 52/64 loss: 1.5966906547546387
Batch 53/64 loss: 1.4898629188537598
Batch 54/64 loss: 1.4522490501403809
Batch 55/64 loss: 1.498957633972168
Batch 56/64 loss: 1.6218318939208984
Batch 57/64 loss: 1.8502922058105469
Batch 58/64 loss: 1.8153281211853027
Batch 59/64 loss: 1.5987391471862793
Batch 60/64 loss: 1.5451579093933105
Batch 61/64 loss: 1.6229948997497559
Batch 62/64 loss: 1.702707290649414
Batch 63/64 loss: 1.6654672622680664
Batch 64/64 loss: -1.2744665145874023
Epoch 19  Train loss: 1.7065225152408376  Val loss: 2.1266741146336714
Epoch 20
-------------------------------
Batch 1/64 loss: 1.4276928901672363
Batch 2/64 loss: 1.5945873260498047
Batch 3/64 loss: 1.5689888000488281
Batch 4/64 loss: 1.4959115982055664
Batch 5/64 loss: 1.1839790344238281
Batch 6/64 loss: 1.4620637893676758
Batch 7/64 loss: 1.648073673248291
Batch 8/64 loss: 1.4551167488098145
Batch 9/64 loss: 1.5375733375549316
Batch 10/64 loss: 1.3186712265014648
Batch 11/64 loss: 1.5295052528381348
Batch 12/64 loss: 1.5661954879760742
Batch 13/64 loss: 1.4151201248168945
Batch 14/64 loss: 1.7257990837097168
Batch 15/64 loss: 1.556203842163086
Batch 16/64 loss: 1.502159595489502
Batch 17/64 loss: 1.6339988708496094
Batch 18/64 loss: 1.603797435760498
Batch 19/64 loss: 1.4600143432617188
Batch 20/64 loss: 1.5834283828735352
Batch 21/64 loss: 1.5403037071228027
Batch 22/64 loss: 1.455655574798584
Batch 23/64 loss: 1.546609878540039
Batch 24/64 loss: 1.6727776527404785
Batch 25/64 loss: 1.390500545501709
Batch 26/64 loss: 1.6815814971923828
Batch 27/64 loss: 1.6378068923950195
Batch 28/64 loss: 1.8066740036010742
Batch 29/64 loss: 1.4482884407043457
Batch 30/64 loss: 1.7412209510803223
Batch 31/64 loss: 1.5235724449157715
Batch 32/64 loss: 1.6925439834594727
Batch 33/64 loss: 1.5772881507873535
Batch 34/64 loss: 1.8673973083496094
Batch 35/64 loss: 1.8084731101989746
Batch 36/64 loss: 1.4475221633911133
Batch 37/64 loss: 1.7329068183898926
Batch 38/64 loss: 1.9046869277954102
Batch 39/64 loss: 1.62872314453125
Batch 40/64 loss: 1.898777961730957
Batch 41/64 loss: 1.7758903503417969
Batch 42/64 loss: 1.704535961151123
Batch 43/64 loss: 2.2294864654541016
Batch 44/64 loss: 1.509261131286621
Batch 45/64 loss: 1.7008328437805176
Batch 46/64 loss: 1.4641776084899902
Batch 47/64 loss: 1.6787056922912598
Batch 48/64 loss: 1.634857177734375
Batch 49/64 loss: 2.0519227981567383
Batch 50/64 loss: 1.551805019378662
Batch 51/64 loss: 1.653696060180664
Batch 52/64 loss: 1.3898873329162598
Batch 53/64 loss: 1.380289077758789
Batch 54/64 loss: 1.4130468368530273
Batch 55/64 loss: 1.4465794563293457
Batch 56/64 loss: 1.387998104095459
Batch 57/64 loss: 1.654693603515625
Batch 58/64 loss: 1.7317500114440918
Batch 59/64 loss: 1.6743731498718262
Batch 60/64 loss: 1.7231884002685547
Batch 61/64 loss: 1.3839178085327148
Batch 62/64 loss: 1.4804105758666992
Batch 63/64 loss: 1.3949995040893555
Batch 64/64 loss: -1.3102855682373047
Epoch 20  Train loss: 1.5577377693325865  Val loss: 1.7805646850481065
Saving best model, epoch: 20
Epoch 21
-------------------------------
Batch 1/64 loss: 1.8426299095153809
Batch 2/64 loss: 1.3528265953063965
Batch 3/64 loss: 1.3359107971191406
Batch 4/64 loss: 1.5913166999816895
Batch 5/64 loss: 1.396634578704834
Batch 6/64 loss: 1.4872336387634277
Batch 7/64 loss: 1.6426124572753906
Batch 8/64 loss: 1.5276856422424316
Batch 9/64 loss: 1.629636287689209
Batch 10/64 loss: 1.5161333084106445
Batch 11/64 loss: 1.3500714302062988
Batch 12/64 loss: 1.5290346145629883
Batch 13/64 loss: 1.4457783699035645
Batch 14/64 loss: 1.556856632232666
Batch 15/64 loss: 1.4849748611450195
Batch 16/64 loss: 1.3430304527282715
Batch 17/64 loss: 1.6323490142822266
Batch 18/64 loss: 1.7589716911315918
Batch 19/64 loss: 1.5891733169555664
Batch 20/64 loss: 1.6965351104736328
Batch 21/64 loss: 1.3095664978027344
Batch 22/64 loss: 1.474813461303711
Batch 23/64 loss: 1.4757657051086426
Batch 24/64 loss: 1.2155895233154297
Batch 25/64 loss: 1.3904495239257812
Batch 26/64 loss: 1.253901481628418
Batch 27/64 loss: 1.685497760772705
Batch 28/64 loss: 1.4442520141601562
Batch 29/64 loss: 1.283846378326416
Batch 30/64 loss: 1.4977421760559082
Batch 31/64 loss: 1.1868724822998047
Batch 32/64 loss: 1.3485627174377441
Batch 33/64 loss: 1.5667977333068848
Batch 34/64 loss: 1.3424086570739746
Batch 35/64 loss: 1.551450252532959
Batch 36/64 loss: 1.3853473663330078
Batch 37/64 loss: 1.1980643272399902
Batch 38/64 loss: 1.188950538635254
Batch 39/64 loss: 1.3038148880004883
Batch 40/64 loss: 1.5394444465637207
Batch 41/64 loss: 1.4137701988220215
Batch 42/64 loss: 1.362525463104248
Batch 43/64 loss: 1.2540149688720703
Batch 44/64 loss: 1.2113299369812012
Batch 45/64 loss: 1.3102383613586426
Batch 46/64 loss: 1.1582770347595215
Batch 47/64 loss: 1.5391106605529785
Batch 48/64 loss: 1.0961828231811523
Batch 49/64 loss: 1.0195116996765137
Batch 50/64 loss: 1.1414661407470703
Batch 51/64 loss: 1.6155223846435547
Batch 52/64 loss: 1.3507161140441895
Batch 53/64 loss: 1.2066121101379395
Batch 54/64 loss: 1.1349282264709473
Batch 55/64 loss: 1.3446226119995117
Batch 56/64 loss: 1.283350944519043
Batch 57/64 loss: 1.166429042816162
Batch 58/64 loss: 1.3131380081176758
Batch 59/64 loss: 1.185807228088379
Batch 60/64 loss: 1.1391611099243164
Batch 61/64 loss: 1.5244159698486328
Batch 62/64 loss: 1.272839069366455
Batch 63/64 loss: 0.995366096496582
Batch 64/64 loss: -1.6791253089904785
Epoch 21  Train loss: 1.351098440207687  Val loss: 1.365683801395377
Saving best model, epoch: 21
Epoch 22
-------------------------------
Batch 1/64 loss: 1.5221433639526367
Batch 2/64 loss: 0.9792938232421875
Batch 3/64 loss: 1.4631619453430176
Batch 4/64 loss: 1.2653522491455078
Batch 5/64 loss: 1.2197799682617188
Batch 6/64 loss: 0.9145941734313965
Batch 7/64 loss: 1.6007356643676758
Batch 8/64 loss: 1.3702988624572754
Batch 9/64 loss: 1.0677032470703125
Batch 10/64 loss: 1.4101829528808594
Batch 11/64 loss: 1.6089134216308594
Batch 12/64 loss: 1.157679557800293
Batch 13/64 loss: 1.0828523635864258
Batch 14/64 loss: 1.6734580993652344
Batch 15/64 loss: 1.1343755722045898
Batch 16/64 loss: 1.3460941314697266
Batch 17/64 loss: 1.6562743186950684
Batch 18/64 loss: 1.30208158493042
Batch 19/64 loss: 1.1923351287841797
Batch 20/64 loss: 1.6447176933288574
Batch 21/64 loss: 1.1628446578979492
Batch 22/64 loss: 1.0523653030395508
Batch 23/64 loss: 1.4561986923217773
Batch 24/64 loss: 1.5369911193847656
Batch 25/64 loss: 1.2851238250732422
Batch 26/64 loss: 1.1445589065551758
Batch 27/64 loss: 1.102768898010254
Batch 28/64 loss: 0.9389972686767578
Batch 29/64 loss: 1.4133424758911133
Batch 30/64 loss: 1.0308380126953125
Batch 31/64 loss: 0.8827595710754395
Batch 32/64 loss: 1.4340367317199707
Batch 33/64 loss: 1.1501140594482422
Batch 34/64 loss: 1.505833625793457
Batch 35/64 loss: 1.4716134071350098
Batch 36/64 loss: 1.4711318016052246
Batch 37/64 loss: 1.0547428131103516
Batch 38/64 loss: 1.094984531402588
Batch 39/64 loss: 1.1214475631713867
Batch 40/64 loss: 1.078017234802246
Batch 41/64 loss: 1.040666103363037
Batch 42/64 loss: 1.4983201026916504
Batch 43/64 loss: 0.9572787284851074
Batch 44/64 loss: 0.9280962944030762
Batch 45/64 loss: 1.1334619522094727
Batch 46/64 loss: 1.0970144271850586
Batch 47/64 loss: 1.4651293754577637
Batch 48/64 loss: 0.853269100189209
Batch 49/64 loss: 1.4095697402954102
Batch 50/64 loss: 1.1648073196411133
Batch 51/64 loss: 1.256338119506836
Batch 52/64 loss: 1.1842460632324219
Batch 53/64 loss: 1.1515827178955078
Batch 54/64 loss: 1.1319832801818848
Batch 55/64 loss: 1.305872917175293
Batch 56/64 loss: 1.0428376197814941
Batch 57/64 loss: 1.1305866241455078
Batch 58/64 loss: 1.0353069305419922
Batch 59/64 loss: 1.2867975234985352
Batch 60/64 loss: 1.1630916595458984
Batch 61/64 loss: 1.1118626594543457
Batch 62/64 loss: 1.060898780822754
Batch 63/64 loss: 1.0735273361206055
Batch 64/64 loss: -2.0062432289123535
Epoch 22  Train loss: 1.191821201174867  Val loss: 1.1632338651676768
Saving best model, epoch: 22
Epoch 23
-------------------------------
Batch 1/64 loss: 1.1355338096618652
Batch 2/64 loss: 0.9341626167297363
Batch 3/64 loss: 1.1827454566955566
Batch 4/64 loss: 1.0317845344543457
Batch 5/64 loss: 1.001399040222168
Batch 6/64 loss: 1.1171908378601074
Batch 7/64 loss: 1.432511806488037
Batch 8/64 loss: 1.43123197555542
Batch 9/64 loss: 1.1333341598510742
Batch 10/64 loss: 1.293471336364746
Batch 11/64 loss: 1.2966666221618652
Batch 12/64 loss: 1.0478529930114746
Batch 13/64 loss: 1.2722134590148926
Batch 14/64 loss: 1.2358160018920898
Batch 15/64 loss: 1.3046255111694336
Batch 16/64 loss: 1.0664000511169434
Batch 17/64 loss: 1.2897138595581055
Batch 18/64 loss: 1.0241470336914062
Batch 19/64 loss: 1.081270694732666
Batch 20/64 loss: 1.0145959854125977
Batch 21/64 loss: 1.2928013801574707
Batch 22/64 loss: 1.1994190216064453
Batch 23/64 loss: 0.9893069267272949
Batch 24/64 loss: 1.039011001586914
Batch 25/64 loss: 1.3243184089660645
Batch 26/64 loss: 1.5422935485839844
Batch 27/64 loss: 0.9980673789978027
Batch 28/64 loss: 0.8941993713378906
Batch 29/64 loss: 1.0351300239562988
Batch 30/64 loss: 1.1781458854675293
Batch 31/64 loss: 1.0119848251342773
Batch 32/64 loss: 1.2266645431518555
Batch 33/64 loss: 1.193021297454834
Batch 34/64 loss: 0.9606471061706543
Batch 35/64 loss: 1.1342554092407227
Batch 36/64 loss: 1.1141357421875
Batch 37/64 loss: 0.8190078735351562
Batch 38/64 loss: 0.9082260131835938
Batch 39/64 loss: 1.7671451568603516
Batch 40/64 loss: 1.0688586235046387
Batch 41/64 loss: 1.3081202507019043
Batch 42/64 loss: 1.5144696235656738
Batch 43/64 loss: 0.9721612930297852
Batch 44/64 loss: 1.8022541999816895
Batch 45/64 loss: 1.474259853363037
Batch 46/64 loss: 1.2140750885009766
Batch 47/64 loss: 1.3914504051208496
Batch 48/64 loss: 1.1088080406188965
Batch 49/64 loss: 1.5963764190673828
Batch 50/64 loss: 1.3014822006225586
Batch 51/64 loss: 1.0524530410766602
Batch 52/64 loss: 1.2515974044799805
Batch 53/64 loss: 1.0722174644470215
Batch 54/64 loss: 1.1683874130249023
Batch 55/64 loss: 1.2086753845214844
Batch 56/64 loss: 1.0832915306091309
Batch 57/64 loss: 1.080045223236084
Batch 58/64 loss: 1.4825124740600586
Batch 59/64 loss: 1.1421713829040527
Batch 60/64 loss: 1.2385797500610352
Batch 61/64 loss: 1.1488890647888184
Batch 62/64 loss: 1.2499713897705078
Batch 63/64 loss: 1.0068755149841309
Batch 64/64 loss: -2.0247802734375
Epoch 23  Train loss: 1.1509622985241459  Val loss: 1.572751605633608
Epoch 24
-------------------------------
Batch 1/64 loss: 1.327223777770996
Batch 2/64 loss: 1.4038853645324707
Batch 3/64 loss: 1.0742197036743164
Batch 4/64 loss: 0.8719072341918945
Batch 5/64 loss: 1.1981067657470703
Batch 6/64 loss: 1.0068693161010742
Batch 7/64 loss: 1.0285611152648926
Batch 8/64 loss: 1.1184697151184082
Batch 9/64 loss: 1.0530691146850586
Batch 10/64 loss: 1.0801501274108887
Batch 11/64 loss: 0.8103184700012207
Batch 12/64 loss: 1.0341944694519043
Batch 13/64 loss: 0.7843441963195801
Batch 14/64 loss: 0.8456511497497559
Batch 15/64 loss: 0.8132305145263672
Batch 16/64 loss: 1.1720294952392578
Batch 17/64 loss: 1.1594147682189941
Batch 18/64 loss: 1.0682616233825684
Batch 19/64 loss: 1.2019023895263672
Batch 20/64 loss: 0.9125404357910156
Batch 21/64 loss: 0.9034385681152344
Batch 22/64 loss: 1.1095795631408691
Batch 23/64 loss: 1.0671381950378418
Batch 24/64 loss: 1.3616886138916016
Batch 25/64 loss: 0.9823546409606934
Batch 26/64 loss: 1.125779151916504
Batch 27/64 loss: 1.114171028137207
Batch 28/64 loss: 0.9112381935119629
Batch 29/64 loss: 1.0150699615478516
Batch 30/64 loss: 0.9959564208984375
Batch 31/64 loss: 1.1780920028686523
Batch 32/64 loss: 0.7668929100036621
Batch 33/64 loss: 1.0218696594238281
Batch 34/64 loss: 0.8788633346557617
Batch 35/64 loss: 0.8781394958496094
Batch 36/64 loss: 0.8530993461608887
Batch 37/64 loss: 1.339822769165039
Batch 38/64 loss: 1.2134408950805664
Batch 39/64 loss: 1.3311762809753418
Batch 40/64 loss: 0.8432831764221191
Batch 41/64 loss: 1.016448974609375
Batch 42/64 loss: 1.0910053253173828
Batch 43/64 loss: 1.4620304107666016
Batch 44/64 loss: 0.9163460731506348
Batch 45/64 loss: 1.225052833557129
Batch 46/64 loss: 0.8992958068847656
Batch 47/64 loss: 1.2176403999328613
Batch 48/64 loss: 1.086036205291748
Batch 49/64 loss: 1.3548903465270996
Batch 50/64 loss: 1.2410688400268555
Batch 51/64 loss: 1.0491256713867188
Batch 52/64 loss: 1.5195794105529785
Batch 53/64 loss: 1.1481003761291504
Batch 54/64 loss: 1.0706291198730469
Batch 55/64 loss: 1.3269600868225098
Batch 56/64 loss: 1.2996187210083008
Batch 57/64 loss: 1.1772956848144531
Batch 58/64 loss: 0.9838299751281738
Batch 59/64 loss: 0.8870601654052734
Batch 60/64 loss: 1.7930831909179688
Batch 61/64 loss: 1.05452299118042
Batch 62/64 loss: 0.7950825691223145
Batch 63/64 loss: 1.1368451118469238
Batch 64/64 loss: -2.284050941467285
Epoch 24  Train loss: 1.049316926096  Val loss: 2.0699732147950898
Epoch 25
-------------------------------
Batch 1/64 loss: 1.2269806861877441
Batch 2/64 loss: 0.9200944900512695
Batch 3/64 loss: 1.1597247123718262
Batch 4/64 loss: 1.328920841217041
Batch 5/64 loss: 0.9266571998596191
Batch 6/64 loss: 0.9821414947509766
Batch 7/64 loss: 1.3870019912719727
Batch 8/64 loss: 0.9905247688293457
Batch 9/64 loss: 0.9153356552124023
Batch 10/64 loss: 0.9066686630249023
Batch 11/64 loss: 1.3457670211791992
Batch 12/64 loss: 1.0409808158874512
Batch 13/64 loss: 1.090505599975586
Batch 14/64 loss: 1.089787483215332
Batch 15/64 loss: 1.2851753234863281
Batch 16/64 loss: 1.3993945121765137
Batch 17/64 loss: 1.164750576019287
Batch 18/64 loss: 0.8331074714660645
Batch 19/64 loss: 1.0771288871765137
Batch 20/64 loss: 1.1392459869384766
Batch 21/64 loss: 0.9730663299560547
Batch 22/64 loss: 0.8853335380554199
Batch 23/64 loss: 1.57999849319458
Batch 24/64 loss: 0.8552675247192383
Batch 25/64 loss: 0.9370245933532715
Batch 26/64 loss: 1.2357826232910156
Batch 27/64 loss: 0.847198486328125
Batch 28/64 loss: 0.7166318893432617
Batch 29/64 loss: 0.7869076728820801
Batch 30/64 loss: 1.0460877418518066
Batch 31/64 loss: 0.9356145858764648
Batch 32/64 loss: 1.1007471084594727
Batch 33/64 loss: 1.0822534561157227
Batch 34/64 loss: 0.7137947082519531
Batch 35/64 loss: 0.9143581390380859
Batch 36/64 loss: 0.816373348236084
Batch 37/64 loss: 0.8141975402832031
Batch 38/64 loss: 1.0182299613952637
Batch 39/64 loss: 0.8184733390808105
Batch 40/64 loss: 0.695563793182373
Batch 41/64 loss: 1.1895866394042969
Batch 42/64 loss: 0.9520235061645508
Batch 43/64 loss: 1.2698907852172852
Batch 44/64 loss: 1.0468158721923828
Batch 45/64 loss: 0.7115116119384766
Batch 46/64 loss: 1.082601547241211
Batch 47/64 loss: 1.1573600769042969
Batch 48/64 loss: 1.1889801025390625
Batch 49/64 loss: 1.387770652770996
Batch 50/64 loss: 2.2854056358337402
Batch 51/64 loss: 1.1766324043273926
Batch 52/64 loss: 1.3717460632324219
Batch 53/64 loss: 2.304014205932617
Batch 54/64 loss: 2.4786853790283203
Batch 55/64 loss: 1.6090407371520996
Batch 56/64 loss: 2.062084197998047
Batch 57/64 loss: 2.459179401397705
Batch 58/64 loss: 1.6221985816955566
Batch 59/64 loss: 1.6068305969238281
Batch 60/64 loss: 1.8018279075622559
Batch 61/64 loss: 1.5422329902648926
Batch 62/64 loss: 2.077284336090088
Batch 63/64 loss: 1.501582145690918
Batch 64/64 loss: -1.0498180389404297
Epoch 25  Train loss: 1.1934230729645374  Val loss: 2.4482166906402694
Epoch 26
-------------------------------
Batch 1/64 loss: 1.6966347694396973
Batch 2/64 loss: 2.1444363594055176
Batch 3/64 loss: 1.6571154594421387
Batch 4/64 loss: 1.6635160446166992
Batch 5/64 loss: 1.7166728973388672
Batch 6/64 loss: 1.53751802444458
Batch 7/64 loss: 1.3858208656311035
Batch 8/64 loss: 1.4431939125061035
Batch 9/64 loss: 1.3303890228271484
Batch 10/64 loss: 1.7612805366516113
Batch 11/64 loss: 1.4598069190979004
Batch 12/64 loss: 2.2143540382385254
Batch 13/64 loss: 1.6007933616638184
Batch 14/64 loss: 1.1584539413452148
Batch 15/64 loss: 1.6215910911560059
Batch 16/64 loss: 1.2560691833496094
Batch 17/64 loss: 1.3597445487976074
Batch 18/64 loss: 1.3158893585205078
Batch 19/64 loss: 1.5717191696166992
Batch 20/64 loss: 1.5383338928222656
Batch 21/64 loss: 1.3701305389404297
Batch 22/64 loss: 1.3525753021240234
Batch 23/64 loss: 1.3499789237976074
Batch 24/64 loss: 1.1143183708190918
Batch 25/64 loss: 1.149613380432129
Batch 26/64 loss: 1.2015137672424316
Batch 27/64 loss: 1.4128694534301758
Batch 28/64 loss: 1.2470040321350098
Batch 29/64 loss: 0.8225727081298828
Batch 30/64 loss: 1.0969996452331543
Batch 31/64 loss: 1.0624642372131348
Batch 32/64 loss: 1.0318102836608887
Batch 33/64 loss: 1.0597124099731445
Batch 34/64 loss: 0.9318099021911621
Batch 35/64 loss: 1.1268253326416016
Batch 36/64 loss: 0.9521527290344238
Batch 37/64 loss: 1.3249306678771973
Batch 38/64 loss: 1.463118076324463
Batch 39/64 loss: 0.9549074172973633
Batch 40/64 loss: 1.042919635772705
Batch 41/64 loss: 1.1527161598205566
Batch 42/64 loss: 0.8482890129089355
Batch 43/64 loss: 1.1634469032287598
Batch 44/64 loss: 0.8848652839660645
Batch 45/64 loss: 0.7733702659606934
Batch 46/64 loss: 0.9666743278503418
Batch 47/64 loss: 1.0244174003601074
Batch 48/64 loss: 0.9308090209960938
Batch 49/64 loss: 0.7720060348510742
Batch 50/64 loss: 0.9940919876098633
Batch 51/64 loss: 0.8369073867797852
Batch 52/64 loss: 1.0297422409057617
Batch 53/64 loss: 0.8795866966247559
Batch 54/64 loss: 0.7165002822875977
Batch 55/64 loss: 0.9294219017028809
Batch 56/64 loss: 1.028489589691162
Batch 57/64 loss: 0.7007942199707031
Batch 58/64 loss: 0.7894864082336426
Batch 59/64 loss: 1.001814842224121
Batch 60/64 loss: 0.8061375617980957
Batch 61/64 loss: 0.6695313453674316
Batch 62/64 loss: 1.1731085777282715
Batch 63/64 loss: 1.2013177871704102
Batch 64/64 loss: -2.268928050994873
Epoch 26  Train loss: 1.161935520172119  Val loss: 0.9312772193725166
Saving best model, epoch: 26
Epoch 27
-------------------------------
Batch 1/64 loss: 1.571230411529541
Batch 2/64 loss: 0.8637480735778809
Batch 3/64 loss: 1.1512398719787598
Batch 4/64 loss: 0.780085563659668
Batch 5/64 loss: 1.3662347793579102
Batch 6/64 loss: 0.7556428909301758
Batch 7/64 loss: 0.7130250930786133
Batch 8/64 loss: 0.7725539207458496
Batch 9/64 loss: 0.9591169357299805
Batch 10/64 loss: 0.8332390785217285
Batch 11/64 loss: 0.7679786682128906
Batch 12/64 loss: 1.0671696662902832
Batch 13/64 loss: 0.7718038558959961
Batch 14/64 loss: 0.7641925811767578
Batch 15/64 loss: 0.9575910568237305
Batch 16/64 loss: 0.7501144409179688
Batch 17/64 loss: 0.7368006706237793
Batch 18/64 loss: 0.8630270957946777
Batch 19/64 loss: 1.217071533203125
Batch 20/64 loss: 0.7115788459777832
Batch 21/64 loss: 0.9868273735046387
Batch 22/64 loss: 0.5613417625427246
Batch 23/64 loss: 0.9513697624206543
Batch 24/64 loss: 0.6842241287231445
Batch 25/64 loss: 0.8175668716430664
Batch 26/64 loss: 0.8495454788208008
Batch 27/64 loss: 0.8153700828552246
Batch 28/64 loss: 0.8836770057678223
Batch 29/64 loss: 0.6322183609008789
Batch 30/64 loss: 0.7031641006469727
Batch 31/64 loss: 0.7169246673583984
Batch 32/64 loss: 0.6427488327026367
Batch 33/64 loss: 0.9688196182250977
Batch 34/64 loss: 0.6067724227905273
Batch 35/64 loss: 0.7761726379394531
Batch 36/64 loss: 0.5837416648864746
Batch 37/64 loss: 0.8948707580566406
Batch 38/64 loss: 0.6608967781066895
Batch 39/64 loss: 0.6217021942138672
Batch 40/64 loss: 0.8669862747192383
Batch 41/64 loss: 0.9161443710327148
Batch 42/64 loss: 0.8385767936706543
Batch 43/64 loss: 0.7785019874572754
Batch 44/64 loss: 0.7305679321289062
Batch 45/64 loss: 0.870476245880127
Batch 46/64 loss: 0.8501815795898438
Batch 47/64 loss: 0.7299456596374512
Batch 48/64 loss: 0.9076528549194336
Batch 49/64 loss: 0.6527161598205566
Batch 50/64 loss: 0.530911922454834
Batch 51/64 loss: 0.5423460006713867
Batch 52/64 loss: 0.7363934516906738
Batch 53/64 loss: 0.8163762092590332
Batch 54/64 loss: 0.7081475257873535
Batch 55/64 loss: 0.8356895446777344
Batch 56/64 loss: 0.653378963470459
Batch 57/64 loss: 0.932915210723877
Batch 58/64 loss: 0.5902738571166992
Batch 59/64 loss: 0.6700639724731445
Batch 60/64 loss: 0.7596359252929688
Batch 61/64 loss: 0.7713656425476074
Batch 62/64 loss: 1.0908961296081543
Batch 63/64 loss: 0.6461915969848633
Batch 64/64 loss: -2.789639949798584
Epoch 27  Train loss: 0.7696549864376292  Val loss: 0.7251888419344663
Saving best model, epoch: 27
Epoch 28
-------------------------------
Batch 1/64 loss: 0.8123898506164551
Batch 2/64 loss: 0.6848087310791016
Batch 3/64 loss: 0.9065723419189453
Batch 4/64 loss: 0.5744028091430664
Batch 5/64 loss: 0.8640255928039551
Batch 6/64 loss: 0.6536159515380859
Batch 7/64 loss: 0.5651063919067383
Batch 8/64 loss: 0.654688835144043
Batch 9/64 loss: 0.730679988861084
Batch 10/64 loss: 0.8070063591003418
Batch 11/64 loss: 1.0426173210144043
Batch 12/64 loss: 0.5734977722167969
Batch 13/64 loss: 0.6741161346435547
Batch 14/64 loss: 1.3811836242675781
Batch 15/64 loss: 0.6414222717285156
Batch 16/64 loss: 0.6711935997009277
Batch 17/64 loss: 0.5905466079711914
Batch 18/64 loss: 0.9588966369628906
Batch 19/64 loss: 0.5213804244995117
Batch 20/64 loss: 0.6040034294128418
Batch 21/64 loss: 0.8046541213989258
Batch 22/64 loss: 0.8290495872497559
Batch 23/64 loss: 0.9235024452209473
Batch 24/64 loss: 0.9203062057495117
Batch 25/64 loss: 0.596163272857666
Batch 26/64 loss: 0.8524971008300781
Batch 27/64 loss: 0.6116676330566406
Batch 28/64 loss: 0.5995655059814453
Batch 29/64 loss: 0.6773700714111328
Batch 30/64 loss: 0.8238062858581543
Batch 31/64 loss: 0.8920221328735352
Batch 32/64 loss: 0.6915545463562012
Batch 33/64 loss: 0.5679078102111816
Batch 34/64 loss: 0.7022085189819336
Batch 35/64 loss: 0.6527538299560547
Batch 36/64 loss: 0.5658535957336426
Batch 37/64 loss: 0.7898507118225098
Batch 38/64 loss: 0.864809513092041
Batch 39/64 loss: 0.7535419464111328
Batch 40/64 loss: 0.9080100059509277
Batch 41/64 loss: 0.6436557769775391
Batch 42/64 loss: 0.6745338439941406
Batch 43/64 loss: 0.4781913757324219
Batch 44/64 loss: 0.6882705688476562
Batch 45/64 loss: 0.7660970687866211
Batch 46/64 loss: 0.4799509048461914
Batch 47/64 loss: 0.5663180351257324
Batch 48/64 loss: 0.5491409301757812
Batch 49/64 loss: 0.7458691596984863
Batch 50/64 loss: 0.8329477310180664
Batch 51/64 loss: 0.566277027130127
Batch 52/64 loss: 0.5156326293945312
Batch 53/64 loss: 0.8059258460998535
Batch 54/64 loss: 0.631525993347168
Batch 55/64 loss: 0.3028254508972168
Batch 56/64 loss: 0.699366569519043
Batch 57/64 loss: 0.6515107154846191
Batch 58/64 loss: 0.7113137245178223
Batch 59/64 loss: 0.5959320068359375
Batch 60/64 loss: 0.7129473686218262
Batch 61/64 loss: 0.7539329528808594
Batch 62/64 loss: 0.7870545387268066
Batch 63/64 loss: 1.0014667510986328
Batch 64/64 loss: -2.6397013664245605
Epoch 28  Train loss: 0.6763946738897585  Val loss: 0.6926840621581192
Saving best model, epoch: 28
Epoch 29
-------------------------------
Batch 1/64 loss: 0.617072582244873
Batch 2/64 loss: 0.4864821434020996
Batch 3/64 loss: 0.8107919692993164
Batch 4/64 loss: 0.6427435874938965
Batch 5/64 loss: 0.5297999382019043
Batch 6/64 loss: 0.7383880615234375
Batch 7/64 loss: 0.5279459953308105
Batch 8/64 loss: 0.49788665771484375
Batch 9/64 loss: 0.8082332611083984
Batch 10/64 loss: 0.8129396438598633
Batch 11/64 loss: 0.6917743682861328
Batch 12/64 loss: 0.4982175827026367
Batch 13/64 loss: 0.5935006141662598
Batch 14/64 loss: 0.3995194435119629
Batch 15/64 loss: 0.963414192199707
Batch 16/64 loss: 0.3820972442626953
Batch 17/64 loss: 0.6427206993103027
Batch 18/64 loss: 0.7539196014404297
Batch 19/64 loss: 0.43293285369873047
Batch 20/64 loss: 0.4611940383911133
Batch 21/64 loss: 0.4141998291015625
Batch 22/64 loss: 0.7232413291931152
Batch 23/64 loss: 0.7207274436950684
Batch 24/64 loss: 0.9170007705688477
Batch 25/64 loss: 0.5886359214782715
Batch 26/64 loss: 0.40448904037475586
Batch 27/64 loss: 0.4499993324279785
Batch 28/64 loss: 0.662569522857666
Batch 29/64 loss: 0.4716033935546875
Batch 30/64 loss: 0.6763877868652344
Batch 31/64 loss: 0.7787632942199707
Batch 32/64 loss: 0.9480586051940918
Batch 33/64 loss: 0.32350730895996094
Batch 34/64 loss: 0.5651154518127441
Batch 35/64 loss: 0.658566951751709
Batch 36/64 loss: 0.45508480072021484
Batch 37/64 loss: 0.554903507232666
Batch 38/64 loss: 0.781766414642334
Batch 39/64 loss: 0.6188979148864746
Batch 40/64 loss: 0.6938910484313965
Batch 41/64 loss: 0.49037742614746094
Batch 42/64 loss: 1.2557320594787598
Batch 43/64 loss: 0.4732508659362793
Batch 44/64 loss: 0.4729766845703125
Batch 45/64 loss: 0.8734092712402344
Batch 46/64 loss: 0.6288814544677734
Batch 47/64 loss: 0.564786434173584
Batch 48/64 loss: 0.7095432281494141
Batch 49/64 loss: 0.49690866470336914
Batch 50/64 loss: 0.8223905563354492
Batch 51/64 loss: 0.4475855827331543
Batch 52/64 loss: 0.7038564682006836
Batch 53/64 loss: 0.6714859008789062
Batch 54/64 loss: 0.5266928672790527
Batch 55/64 loss: 0.6152992248535156
Batch 56/64 loss: 0.6657366752624512
Batch 57/64 loss: 0.5065984725952148
Batch 58/64 loss: 0.4874424934387207
Batch 59/64 loss: 0.4272270202636719
Batch 60/64 loss: 0.43520259857177734
Batch 61/64 loss: 0.5634341239929199
Batch 62/64 loss: 0.5360679626464844
Batch 63/64 loss: 0.6372761726379395
Batch 64/64 loss: -2.3584799766540527
Epoch 29  Train loss: 0.5794868450538785  Val loss: 0.627388157795385
Saving best model, epoch: 29
Epoch 30
-------------------------------
Batch 1/64 loss: 0.4808192253112793
Batch 2/64 loss: 0.7563204765319824
Batch 3/64 loss: 0.35588741302490234
Batch 4/64 loss: 0.4411931037902832
Batch 5/64 loss: 0.36437368392944336
Batch 6/64 loss: 0.4227309226989746
Batch 7/64 loss: 0.37677669525146484
Batch 8/64 loss: 0.5999307632446289
Batch 9/64 loss: 0.47980260848999023
Batch 10/64 loss: 0.3603811264038086
Batch 11/64 loss: 0.42682695388793945
Batch 12/64 loss: 0.5892343521118164
Batch 13/64 loss: 0.4184403419494629
Batch 14/64 loss: 0.36094045639038086
Batch 15/64 loss: 0.46849584579467773
Batch 16/64 loss: 0.47190427780151367
Batch 17/64 loss: 0.5953779220581055
Batch 18/64 loss: 0.40248537063598633
Batch 19/64 loss: 0.43218994140625
Batch 20/64 loss: 0.49076080322265625
Batch 21/64 loss: 0.2795100212097168
Batch 22/64 loss: 0.4352073669433594
Batch 23/64 loss: 0.6991472244262695
Batch 24/64 loss: 0.42016124725341797
Batch 25/64 loss: 0.495455265045166
Batch 26/64 loss: 0.49970436096191406
Batch 27/64 loss: 0.5294771194458008
Batch 28/64 loss: 0.5495352745056152
Batch 29/64 loss: 0.36978626251220703
Batch 30/64 loss: 0.6814408302307129
Batch 31/64 loss: 0.39306116104125977
Batch 32/64 loss: 0.8954100608825684
Batch 33/64 loss: 0.4861764907836914
Batch 34/64 loss: 0.32374143600463867
Batch 35/64 loss: 0.7437529563903809
Batch 36/64 loss: 0.670586109161377
Batch 37/64 loss: 0.5277533531188965
Batch 38/64 loss: 0.5457162857055664
Batch 39/64 loss: 0.4811210632324219
Batch 40/64 loss: 0.5638976097106934
Batch 41/64 loss: 0.5166435241699219
Batch 42/64 loss: 0.7889976501464844
Batch 43/64 loss: 0.36999082565307617
Batch 44/64 loss: 0.352719783782959
Batch 45/64 loss: 0.4857215881347656
Batch 46/64 loss: 0.6964569091796875
Batch 47/64 loss: 0.4135704040527344
Batch 48/64 loss: 0.7020664215087891
Batch 49/64 loss: 0.7376065254211426
Batch 50/64 loss: 0.3326272964477539
Batch 51/64 loss: 0.6699633598327637
Batch 52/64 loss: 0.49648523330688477
Batch 53/64 loss: 0.6100950241088867
Batch 54/64 loss: 0.37562131881713867
Batch 55/64 loss: 0.39623355865478516
Batch 56/64 loss: 0.4792442321777344
Batch 57/64 loss: 0.6863679885864258
Batch 58/64 loss: 0.3965277671813965
Batch 59/64 loss: 0.6689481735229492
Batch 60/64 loss: 0.9290375709533691
Batch 61/64 loss: 0.611485481262207
Batch 62/64 loss: 0.34721803665161133
Batch 63/64 loss: 0.5749111175537109
Batch 64/64 loss: -2.886638641357422
Epoch 30  Train loss: 0.4762207779229856  Val loss: 0.3425362052786391
Saving best model, epoch: 30
Epoch 31
-------------------------------
Batch 1/64 loss: 0.6409091949462891
Batch 2/64 loss: 0.3128218650817871
Batch 3/64 loss: 0.27428674697875977
Batch 4/64 loss: 0.1910839080810547
Batch 5/64 loss: 0.3074822425842285
Batch 6/64 loss: 0.36577796936035156
Batch 7/64 loss: 0.3993978500366211
Batch 8/64 loss: 0.2844080924987793
Batch 9/64 loss: 0.33618831634521484
Batch 10/64 loss: 0.37555551528930664
Batch 11/64 loss: 0.3488302230834961
Batch 12/64 loss: 0.41820383071899414
Batch 13/64 loss: 0.5349512100219727
Batch 14/64 loss: 0.5683331489562988
Batch 15/64 loss: 0.3155655860900879
Batch 16/64 loss: 0.45772266387939453
Batch 17/64 loss: 0.6357846260070801
Batch 18/64 loss: 0.22179079055786133
Batch 19/64 loss: 0.32666778564453125
Batch 20/64 loss: 0.33411741256713867
Batch 21/64 loss: 0.5715913772583008
Batch 22/64 loss: 0.43363094329833984
Batch 23/64 loss: 0.30923032760620117
Batch 24/64 loss: 0.5226259231567383
Batch 25/64 loss: 0.7665038108825684
Batch 26/64 loss: 0.5171051025390625
Batch 27/64 loss: 0.3558030128479004
Batch 28/64 loss: 0.17924118041992188
Batch 29/64 loss: 0.4413261413574219
Batch 30/64 loss: 0.4341745376586914
Batch 31/64 loss: 0.49770164489746094
Batch 32/64 loss: 0.2583589553833008
Batch 33/64 loss: 0.34495067596435547
Batch 34/64 loss: 0.48415184020996094
Batch 35/64 loss: 0.23822689056396484
Batch 36/64 loss: 0.19913959503173828
Batch 37/64 loss: 0.41365957260131836
Batch 38/64 loss: 0.1844325065612793
Batch 39/64 loss: 0.7668280601501465
Batch 40/64 loss: 0.5004701614379883
Batch 41/64 loss: 0.21508121490478516
Batch 42/64 loss: 0.787600040435791
Batch 43/64 loss: 0.31031036376953125
Batch 44/64 loss: 0.3960747718811035
Batch 45/64 loss: 0.5482301712036133
Batch 46/64 loss: 0.686241626739502
Batch 47/64 loss: 1.2801003456115723
Batch 48/64 loss: 0.21962356567382812
Batch 49/64 loss: 0.534060001373291
Batch 50/64 loss: 0.5180721282958984
Batch 51/64 loss: 0.4001913070678711
Batch 52/64 loss: 0.23772287368774414
Batch 53/64 loss: 0.6837091445922852
Batch 54/64 loss: 0.7210912704467773
Batch 55/64 loss: 0.7053389549255371
Batch 56/64 loss: 0.4031357765197754
Batch 57/64 loss: 0.7674860954284668
Batch 58/64 loss: 0.30739736557006836
Batch 59/64 loss: 0.6121783256530762
Batch 60/64 loss: 0.435488224029541
Batch 61/64 loss: 0.484891414642334
Batch 62/64 loss: 0.48756885528564453
Batch 63/64 loss: 0.5489521026611328
Batch 64/64 loss: -3.0434746742248535
Epoch 31  Train loss: 0.4090505282084147  Val loss: 0.43058148937946333
Epoch 32
-------------------------------
Batch 1/64 loss: 0.23751544952392578
Batch 2/64 loss: 0.2422161102294922
Batch 3/64 loss: 0.5818724632263184
Batch 4/64 loss: 0.42429447174072266
Batch 5/64 loss: 0.42136049270629883
Batch 6/64 loss: 0.414156436920166
Batch 7/64 loss: 0.5005888938903809
Batch 8/64 loss: 0.5638532638549805
Batch 9/64 loss: 0.6176199913024902
Batch 10/64 loss: 0.3836832046508789
Batch 11/64 loss: 0.23453950881958008
Batch 12/64 loss: 0.3971114158630371
Batch 13/64 loss: 0.4199352264404297
Batch 14/64 loss: 0.4376335144042969
Batch 15/64 loss: 0.49214935302734375
Batch 16/64 loss: 0.43031835556030273
Batch 17/64 loss: 0.4528985023498535
Batch 18/64 loss: 0.41283416748046875
Batch 19/64 loss: 0.35976362228393555
Batch 20/64 loss: 0.7270717620849609
Batch 21/64 loss: 0.41493892669677734
Batch 22/64 loss: 0.4605712890625
Batch 23/64 loss: 0.40955495834350586
Batch 24/64 loss: 0.37465524673461914
Batch 25/64 loss: 0.26268911361694336
Batch 26/64 loss: 0.31829071044921875
Batch 27/64 loss: 0.5339369773864746
Batch 28/64 loss: 0.3751029968261719
Batch 29/64 loss: 0.3034377098083496
Batch 30/64 loss: 0.8126668930053711
Batch 31/64 loss: 0.3859410285949707
Batch 32/64 loss: 0.3807220458984375
Batch 33/64 loss: 0.4395427703857422
Batch 34/64 loss: 0.4889497756958008
Batch 35/64 loss: 0.40055227279663086
Batch 36/64 loss: 0.323275089263916
Batch 37/64 loss: 0.3929581642150879
Batch 38/64 loss: 0.3440089225769043
Batch 39/64 loss: 0.32991838455200195
Batch 40/64 loss: 0.40001964569091797
Batch 41/64 loss: 0.2636284828186035
Batch 42/64 loss: 0.32896852493286133
Batch 43/64 loss: 0.2870330810546875
Batch 44/64 loss: 0.2965250015258789
Batch 45/64 loss: 0.3980417251586914
Batch 46/64 loss: 0.6418380737304688
Batch 47/64 loss: 0.2714967727661133
Batch 48/64 loss: 0.7513341903686523
Batch 49/64 loss: 0.4426121711730957
Batch 50/64 loss: 1.0238566398620605
Batch 51/64 loss: 0.37132930755615234
Batch 52/64 loss: 0.39414262771606445
Batch 53/64 loss: 0.35155820846557617
Batch 54/64 loss: 0.5096273422241211
Batch 55/64 loss: 1.0819573402404785
Batch 56/64 loss: 0.2895994186401367
Batch 57/64 loss: 0.31397056579589844
Batch 58/64 loss: 0.6855192184448242
Batch 59/64 loss: 0.6527299880981445
Batch 60/64 loss: 0.3445706367492676
Batch 61/64 loss: 0.3142433166503906
Batch 62/64 loss: 0.4584956169128418
Batch 63/64 loss: 0.33785295486450195
Batch 64/64 loss: -3.0202083587646484
Epoch 32  Train loss: 0.3991988088570389  Val loss: 0.2184138478282391
Saving best model, epoch: 32
Epoch 33
-------------------------------
Batch 1/64 loss: 0.22153615951538086
Batch 2/64 loss: 0.4837160110473633
Batch 3/64 loss: 0.4009723663330078
Batch 4/64 loss: 0.4113731384277344
Batch 5/64 loss: 0.6565413475036621
Batch 6/64 loss: 0.6466231346130371
Batch 7/64 loss: 0.7350602149963379
Batch 8/64 loss: 0.7095274925231934
Batch 9/64 loss: 0.5123834609985352
Batch 10/64 loss: 0.5433721542358398
Batch 11/64 loss: 0.7620515823364258
Batch 12/64 loss: 0.6615381240844727
Batch 13/64 loss: 0.7310585975646973
Batch 14/64 loss: 0.7045159339904785
Batch 15/64 loss: 0.5668911933898926
Batch 16/64 loss: 0.7685928344726562
Batch 17/64 loss: 1.571901798248291
Batch 18/64 loss: 0.9793329238891602
Batch 19/64 loss: 0.7687292098999023
Batch 20/64 loss: 0.7086410522460938
Batch 21/64 loss: 0.6163606643676758
Batch 22/64 loss: 0.7906622886657715
Batch 23/64 loss: 1.913741111755371
Batch 24/64 loss: 0.7692055702209473
Batch 25/64 loss: 1.1082277297973633
Batch 26/64 loss: 1.0370988845825195
Batch 27/64 loss: 0.8675718307495117
Batch 28/64 loss: 0.8359603881835938
Batch 29/64 loss: 0.7660632133483887
Batch 30/64 loss: 0.7615156173706055
Batch 31/64 loss: 1.5045771598815918
Batch 32/64 loss: 1.1219635009765625
Batch 33/64 loss: 1.2017326354980469
Batch 34/64 loss: 0.8925542831420898
Batch 35/64 loss: 0.6634392738342285
Batch 36/64 loss: 0.6455569267272949
Batch 37/64 loss: 0.6419053077697754
Batch 38/64 loss: 0.620180606842041
Batch 39/64 loss: 0.9462070465087891
Batch 40/64 loss: 0.5014772415161133
Batch 41/64 loss: 0.834169864654541
Batch 42/64 loss: 1.7024273872375488
Batch 43/64 loss: 0.6742091178894043
Batch 44/64 loss: 0.6683802604675293
Batch 45/64 loss: 0.8976635932922363
Batch 46/64 loss: 0.6747326850891113
Batch 47/64 loss: 0.57177734375
Batch 48/64 loss: 0.951845645904541
Batch 49/64 loss: 1.0698442459106445
Batch 50/64 loss: 0.5507183074951172
Batch 51/64 loss: 0.5320630073547363
Batch 52/64 loss: 0.6675691604614258
Batch 53/64 loss: 0.7789149284362793
Batch 54/64 loss: 0.6422281265258789
Batch 55/64 loss: 0.687201976776123
Batch 56/64 loss: 0.43752288818359375
Batch 57/64 loss: 0.629389762878418
Batch 58/64 loss: 0.4689650535583496
Batch 59/64 loss: 0.42143774032592773
Batch 60/64 loss: 0.6054010391235352
Batch 61/64 loss: 0.5099101066589355
Batch 62/64 loss: 0.4598884582519531
Batch 63/64 loss: 0.34941530227661133
Batch 64/64 loss: -2.7864136695861816
Epoch 33  Train loss: 0.7128819727430157  Val loss: 0.4851732417889887
Epoch 34
-------------------------------
Batch 1/64 loss: 0.5081787109375
Batch 2/64 loss: 0.7601475715637207
Batch 3/64 loss: 0.847832202911377
Batch 4/64 loss: 0.5065598487854004
Batch 5/64 loss: 1.137263298034668
Batch 6/64 loss: 0.4159979820251465
Batch 7/64 loss: 0.6751012802124023
Batch 8/64 loss: 1.1066641807556152
Batch 9/64 loss: 0.3941650390625
Batch 10/64 loss: 0.43283510208129883
Batch 11/64 loss: 0.70965576171875
Batch 12/64 loss: 0.5099101066589355
Batch 13/64 loss: 0.3690328598022461
Batch 14/64 loss: 0.6954808235168457
Batch 15/64 loss: 0.7376751899719238
Batch 16/64 loss: 0.518007755279541
Batch 17/64 loss: 0.38254785537719727
Batch 18/64 loss: 0.6994233131408691
Batch 19/64 loss: 1.8718032836914062
Batch 20/64 loss: 0.4361305236816406
Batch 21/64 loss: 1.2755913734436035
Batch 22/64 loss: 1.278226375579834
Batch 23/64 loss: 0.5590558052062988
Batch 24/64 loss: 0.494199275970459
Batch 25/64 loss: 0.5978059768676758
Batch 26/64 loss: 0.6564922332763672
Batch 27/64 loss: 0.6828351020812988
Batch 28/64 loss: 1.7043867111206055
Batch 29/64 loss: 0.3992338180541992
Batch 30/64 loss: 0.8478217124938965
Batch 31/64 loss: 0.5464978218078613
Batch 32/64 loss: 0.5351147651672363
Batch 33/64 loss: 0.5099601745605469
Batch 34/64 loss: 0.5465288162231445
Batch 35/64 loss: 0.5198040008544922
Batch 36/64 loss: 0.6972112655639648
Batch 37/64 loss: 0.48366212844848633
Batch 38/64 loss: 0.9840083122253418
Batch 39/64 loss: 0.8474020957946777
Batch 40/64 loss: 0.46820068359375
Batch 41/64 loss: 0.6270384788513184
Batch 42/64 loss: 0.5661087036132812
Batch 43/64 loss: 0.7811112403869629
Batch 44/64 loss: 0.4471273422241211
Batch 45/64 loss: 0.46579790115356445
Batch 46/64 loss: 0.38823986053466797
Batch 47/64 loss: 0.42392539978027344
Batch 48/64 loss: 0.9748091697692871
Batch 49/64 loss: 0.5879416465759277
Batch 50/64 loss: 0.6072978973388672
Batch 51/64 loss: 0.5360102653503418
Batch 52/64 loss: 1.2712950706481934
Batch 53/64 loss: 0.7130608558654785
Batch 54/64 loss: 1.4259676933288574
Batch 55/64 loss: 0.8564982414245605
Batch 56/64 loss: 1.2221450805664062
Batch 57/64 loss: 0.6591768264770508
Batch 58/64 loss: 0.5778307914733887
Batch 59/64 loss: 0.39264822006225586
Batch 60/64 loss: 0.4768037796020508
Batch 61/64 loss: 0.5045137405395508
Batch 62/64 loss: 0.616117000579834
Batch 63/64 loss: 1.2255620956420898
Batch 64/64 loss: -2.9221296310424805
Epoch 34  Train loss: 0.6667275484870462  Val loss: 2.324087097063097
Epoch 35
-------------------------------
Batch 1/64 loss: 0.9263057708740234
Batch 2/64 loss: 0.8606696128845215
Batch 3/64 loss: 1.1013708114624023
Batch 4/64 loss: 2.0694494247436523
Batch 5/64 loss: 0.7569236755371094
Batch 6/64 loss: 0.8494377136230469
Batch 7/64 loss: 0.6350417137145996
Batch 8/64 loss: 0.5123624801635742
Batch 9/64 loss: 0.694828987121582
Batch 10/64 loss: 1.3473711013793945
Batch 11/64 loss: 0.4525752067565918
Batch 12/64 loss: 1.3865880966186523
Batch 13/64 loss: 1.977381706237793
Batch 14/64 loss: 0.6394519805908203
Batch 15/64 loss: 0.49201059341430664
Batch 16/64 loss: 0.5336132049560547
Batch 17/64 loss: 1.5715689659118652
Batch 18/64 loss: 0.4647855758666992
Batch 19/64 loss: 0.5849480628967285
Batch 20/64 loss: 0.4954953193664551
Batch 21/64 loss: 0.5654397010803223
Batch 22/64 loss: 1.1584243774414062
Batch 23/64 loss: 1.6814651489257812
Batch 24/64 loss: 0.5980143547058105
Batch 25/64 loss: 0.6353569030761719
Batch 26/64 loss: 0.7174868583679199
Batch 27/64 loss: 0.6052498817443848
Batch 28/64 loss: 0.5221462249755859
Batch 29/64 loss: 0.6044692993164062
Batch 30/64 loss: 0.343080997467041
Batch 31/64 loss: 0.9095263481140137
Batch 32/64 loss: 0.5374341011047363
Batch 33/64 loss: 0.2705564498901367
Batch 34/64 loss: 0.5840425491333008
Batch 35/64 loss: 0.661808967590332
Batch 36/64 loss: 0.48842430114746094
Batch 37/64 loss: 0.7228288650512695
Batch 38/64 loss: 0.7385883331298828
Batch 39/64 loss: 0.5498056411743164
Batch 40/64 loss: 0.5160250663757324
Batch 41/64 loss: 0.5618906021118164
Batch 42/64 loss: 1.3067045211791992
Batch 43/64 loss: 0.5193891525268555
Batch 44/64 loss: 0.5405006408691406
Batch 45/64 loss: 0.2741093635559082
Batch 46/64 loss: 0.4160118103027344
Batch 47/64 loss: 0.3971695899963379
Batch 48/64 loss: 0.5388989448547363
Batch 49/64 loss: 0.3323640823364258
Batch 50/64 loss: 0.37256622314453125
Batch 51/64 loss: 0.5769205093383789
Batch 52/64 loss: 0.3161449432373047
Batch 53/64 loss: 0.5440216064453125
Batch 54/64 loss: 0.5743579864501953
Batch 55/64 loss: 0.47328662872314453
Batch 56/64 loss: 0.5364398956298828
Batch 57/64 loss: 0.9964175224304199
Batch 58/64 loss: 0.3222341537475586
Batch 59/64 loss: 0.8857254981994629
Batch 60/64 loss: 0.4329047203063965
Batch 61/64 loss: 0.2921180725097656
Batch 62/64 loss: 0.6603412628173828
Batch 63/64 loss: 0.5173296928405762
Batch 64/64 loss: -2.851975440979004
Epoch 35  Train loss: 0.6589995328117819  Val loss: 0.27353217265859914
Epoch 36
-------------------------------
Batch 1/64 loss: 0.21003007888793945
Batch 2/64 loss: 0.536717414855957
Batch 3/64 loss: 0.32857179641723633
Batch 4/64 loss: 0.3281865119934082
Batch 5/64 loss: 0.36752986907958984
Batch 6/64 loss: 0.4298582077026367
Batch 7/64 loss: 0.8241176605224609
Batch 8/64 loss: 2.3303489685058594
Batch 9/64 loss: 0.28171777725219727
Batch 10/64 loss: 0.41849756240844727
Batch 11/64 loss: 0.4721841812133789
Batch 12/64 loss: 0.29230546951293945
Batch 13/64 loss: 0.20392751693725586
Batch 14/64 loss: 0.2630500793457031
Batch 15/64 loss: 0.3895549774169922
Batch 16/64 loss: 0.7820596694946289
Batch 17/64 loss: 0.5275940895080566
Batch 18/64 loss: 0.49421167373657227
Batch 19/64 loss: 0.6092648506164551
Batch 20/64 loss: 0.5234236717224121
Batch 21/64 loss: 0.5955309867858887
Batch 22/64 loss: 0.5658316612243652
Batch 23/64 loss: 0.8746929168701172
Batch 24/64 loss: 0.9409360885620117
Batch 25/64 loss: 0.9080891609191895
Batch 26/64 loss: 0.4516787528991699
Batch 27/64 loss: 0.3671693801879883
Batch 28/64 loss: 0.42116832733154297
Batch 29/64 loss: 0.42747974395751953
Batch 30/64 loss: 0.2746767997741699
Batch 31/64 loss: 0.40906810760498047
Batch 32/64 loss: 0.8755273818969727
Batch 33/64 loss: 0.3525505065917969
Batch 34/64 loss: 0.8866457939147949
Batch 35/64 loss: 0.19295501708984375
Batch 36/64 loss: 0.8400025367736816
Batch 37/64 loss: 0.38905811309814453
Batch 38/64 loss: 0.2245349884033203
Batch 39/64 loss: 0.4122343063354492
Batch 40/64 loss: 0.3645925521850586
Batch 41/64 loss: 0.42353153228759766
Batch 42/64 loss: 0.27966880798339844
Batch 43/64 loss: 0.2541642189025879
Batch 44/64 loss: 0.23969602584838867
Batch 45/64 loss: 0.3981962203979492
Batch 46/64 loss: 0.3925037384033203
Batch 47/64 loss: 0.8122673034667969
Batch 48/64 loss: 0.2481245994567871
Batch 49/64 loss: 0.5391182899475098
Batch 50/64 loss: 1.0734119415283203
Batch 51/64 loss: 0.8734064102172852
Batch 52/64 loss: 0.25174951553344727
Batch 53/64 loss: 1.259310245513916
Batch 54/64 loss: 0.2766866683959961
Batch 55/64 loss: 0.19711828231811523
Batch 56/64 loss: 0.26062822341918945
Batch 57/64 loss: 0.4853968620300293
Batch 58/64 loss: 0.4274139404296875
Batch 59/64 loss: 0.23331594467163086
Batch 60/64 loss: 0.3121509552001953
Batch 61/64 loss: 0.37843751907348633
Batch 62/64 loss: 0.586491584777832
Batch 63/64 loss: 0.5206785202026367
Batch 64/64 loss: -3.04740571975708
Epoch 36  Train loss: 0.46785079544665764  Val loss: 0.21084670758329307
Saving best model, epoch: 36
Epoch 37
-------------------------------
Batch 1/64 loss: 0.5183706283569336
Batch 2/64 loss: 0.36452341079711914
Batch 3/64 loss: 0.2687978744506836
Batch 4/64 loss: 0.4554743766784668
Batch 5/64 loss: 0.5174484252929688
Batch 6/64 loss: 0.5619087219238281
Batch 7/64 loss: 0.2783989906311035
Batch 8/64 loss: 1.003401756286621
Batch 9/64 loss: 0.6407794952392578
Batch 10/64 loss: 0.5977215766906738
Batch 11/64 loss: 1.178682804107666
Batch 12/64 loss: 0.16987276077270508
Batch 13/64 loss: 0.36849308013916016
Batch 14/64 loss: 0.28230810165405273
Batch 15/64 loss: 0.4025392532348633
Batch 16/64 loss: 0.6013936996459961
Batch 17/64 loss: 0.5189852714538574
Batch 18/64 loss: 0.42127466201782227
Batch 19/64 loss: 0.21425962448120117
Batch 20/64 loss: 0.6309909820556641
Batch 21/64 loss: 0.42185306549072266
Batch 22/64 loss: 0.6563267707824707
Batch 23/64 loss: 0.5397953987121582
Batch 24/64 loss: 0.7659025192260742
Batch 25/64 loss: 0.5337162017822266
Batch 26/64 loss: 0.39336585998535156
Batch 27/64 loss: 0.32522106170654297
Batch 28/64 loss: 0.6296143531799316
Batch 29/64 loss: 0.28840017318725586
Batch 30/64 loss: 0.32321929931640625
Batch 31/64 loss: 0.4330453872680664
Batch 32/64 loss: 0.20799732208251953
Batch 33/64 loss: 0.626225471496582
Batch 34/64 loss: 1.698939323425293
Batch 35/64 loss: 0.3333425521850586
Batch 36/64 loss: 0.22905302047729492
Batch 37/64 loss: 0.32294225692749023
Batch 38/64 loss: 0.24887466430664062
Batch 39/64 loss: 0.9319429397583008
Batch 40/64 loss: 0.4287753105163574
Batch 41/64 loss: 0.2978801727294922
Batch 42/64 loss: 0.42519187927246094
Batch 43/64 loss: 0.6199131011962891
Batch 44/64 loss: 0.19193220138549805
Batch 45/64 loss: 0.4112701416015625
Batch 46/64 loss: 0.5535092353820801
Batch 47/64 loss: 0.34519290924072266
Batch 48/64 loss: 0.8937292098999023
Batch 49/64 loss: 0.7757172584533691
Batch 50/64 loss: 0.5680856704711914
Batch 51/64 loss: 0.21793174743652344
Batch 52/64 loss: 0.7670660018920898
Batch 53/64 loss: 0.28296852111816406
Batch 54/64 loss: 0.8708314895629883
Batch 55/64 loss: 1.3401298522949219
Batch 56/64 loss: 0.35178184509277344
Batch 57/64 loss: 0.4079585075378418
Batch 58/64 loss: 0.35669565200805664
Batch 59/64 loss: 0.5137791633605957
Batch 60/64 loss: 0.3220553398132324
Batch 61/64 loss: 0.39110612869262695
Batch 62/64 loss: 0.31295251846313477
Batch 63/64 loss: 0.2018594741821289
Batch 64/64 loss: -3.0736923217773438
Epoch 37  Train loss: 0.46193642709769456  Val loss: 0.23287911431486255
Epoch 38
-------------------------------
Batch 1/64 loss: 0.2244572639465332
Batch 2/64 loss: 0.4610586166381836
Batch 3/64 loss: 1.6754412651062012
Batch 4/64 loss: 0.1601266860961914
Batch 5/64 loss: 0.33123254776000977
Batch 6/64 loss: 0.21799850463867188
Batch 7/64 loss: 0.5407686233520508
Batch 8/64 loss: 0.16740894317626953
Batch 9/64 loss: 0.279541015625
Batch 10/64 loss: 0.7320027351379395
Batch 11/64 loss: 0.11072778701782227
Batch 12/64 loss: 0.1985940933227539
Batch 13/64 loss: 0.28086280822753906
Batch 14/64 loss: 0.48166751861572266
Batch 15/64 loss: 0.40300416946411133
Batch 16/64 loss: 0.518951416015625
Batch 17/64 loss: 0.20105695724487305
Batch 18/64 loss: 0.1578044891357422
Batch 19/64 loss: 0.250607967376709
Batch 20/64 loss: 1.4077773094177246
Batch 21/64 loss: 0.49547433853149414
Batch 22/64 loss: 0.4086790084838867
Batch 23/64 loss: 0.4508638381958008
Batch 24/64 loss: 0.19458436965942383
Batch 25/64 loss: 0.045696258544921875
Batch 26/64 loss: 0.1918482780456543
Batch 27/64 loss: 0.4892597198486328
Batch 28/64 loss: 0.37383556365966797
Batch 29/64 loss: 0.7122597694396973
Batch 30/64 loss: 0.35047483444213867
Batch 31/64 loss: 0.2894878387451172
Batch 32/64 loss: 0.22264766693115234
Batch 33/64 loss: 0.5962076187133789
Batch 34/64 loss: 0.16351842880249023
Batch 35/64 loss: 1.4681239128112793
Batch 36/64 loss: 0.4225625991821289
Batch 37/64 loss: 0.4261178970336914
Batch 38/64 loss: 0.3988614082336426
Batch 39/64 loss: 0.30844593048095703
Batch 40/64 loss: 0.511744499206543
Batch 41/64 loss: 0.6928291320800781
Batch 42/64 loss: 0.48502016067504883
Batch 43/64 loss: 0.3289189338684082
Batch 44/64 loss: 0.3835916519165039
Batch 45/64 loss: 0.3492417335510254
Batch 46/64 loss: 0.22806501388549805
Batch 47/64 loss: 0.12999582290649414
Batch 48/64 loss: 0.153778076171875
Batch 49/64 loss: 0.24562740325927734
Batch 50/64 loss: 0.3615689277648926
Batch 51/64 loss: 0.9649486541748047
Batch 52/64 loss: 0.18804550170898438
Batch 53/64 loss: 0.18794870376586914
Batch 54/64 loss: 0.043244361877441406
Batch 55/64 loss: 0.5320992469787598
Batch 56/64 loss: 0.15788984298706055
Batch 57/64 loss: 0.015418529510498047
Batch 58/64 loss: 0.4337272644042969
Batch 59/64 loss: 0.16260337829589844
Batch 60/64 loss: 0.19085073471069336
Batch 61/64 loss: 0.6889901161193848
Batch 62/64 loss: 0.4369063377380371
Batch 63/64 loss: 0.09746026992797852
Batch 64/64 loss: -2.8400039672851562
Epoch 38  Train loss: 0.35530276579015396  Val loss: 0.21449017606650023
Epoch 39
-------------------------------
Batch 1/64 loss: 0.18675756454467773
Batch 2/64 loss: 0.16984891891479492
Batch 3/64 loss: 0.2295217514038086
Batch 4/64 loss: 0.2736945152282715
Batch 5/64 loss: 0.3188209533691406
Batch 6/64 loss: 0.16727685928344727
Batch 7/64 loss: 0.3480668067932129
Batch 8/64 loss: 0.4951200485229492
Batch 9/64 loss: 0.13032007217407227
Batch 10/64 loss: 0.2670321464538574
Batch 11/64 loss: 0.28648948669433594
Batch 12/64 loss: 0.26908445358276367
Batch 13/64 loss: 0.22504568099975586
Batch 14/64 loss: 0.2822380065917969
Batch 15/64 loss: 0.25110530853271484
Batch 16/64 loss: 0.24083900451660156
Batch 17/64 loss: 0.15967941284179688
Batch 18/64 loss: 0.1816244125366211
Batch 19/64 loss: 0.2315220832824707
Batch 20/64 loss: 0.1189112663269043
Batch 21/64 loss: 0.4248042106628418
Batch 22/64 loss: 0.5086097717285156
Batch 23/64 loss: 0.1778731346130371
Batch 24/64 loss: 0.14277076721191406
Batch 25/64 loss: 0.28861522674560547
Batch 26/64 loss: 1.441941738128662
Batch 27/64 loss: 0.4103569984436035
Batch 28/64 loss: 0.054058074951171875
Batch 29/64 loss: 1.5782842636108398
Batch 30/64 loss: 0.06546974182128906
Batch 31/64 loss: 0.2422776222229004
Batch 32/64 loss: 0.15188074111938477
Batch 33/64 loss: 0.27319812774658203
Batch 34/64 loss: 0.3393993377685547
Batch 35/64 loss: 0.020534992218017578
Batch 36/64 loss: 0.07338094711303711
Batch 37/64 loss: 0.1915435791015625
Batch 38/64 loss: 0.37899017333984375
Batch 39/64 loss: 0.28730201721191406
Batch 40/64 loss: 0.5764741897583008
Batch 41/64 loss: 0.1414194107055664
Batch 42/64 loss: 0.3832883834838867
Batch 43/64 loss: 0.26053476333618164
Batch 44/64 loss: 0.476931095123291
Batch 45/64 loss: 0.4801974296569824
Batch 46/64 loss: 0.30423784255981445
Batch 47/64 loss: 0.6083512306213379
Batch 48/64 loss: 0.9150586128234863
Batch 49/64 loss: 0.43259572982788086
Batch 50/64 loss: 1.2414593696594238
Batch 51/64 loss: 0.4441537857055664
Batch 52/64 loss: 0.1888713836669922
Batch 53/64 loss: 0.5110712051391602
Batch 54/64 loss: 0.7392177581787109
Batch 55/64 loss: 0.4956989288330078
Batch 56/64 loss: 0.8852553367614746
Batch 57/64 loss: 0.5557818412780762
Batch 58/64 loss: 0.41268205642700195
Batch 59/64 loss: 0.17646408081054688
Batch 60/64 loss: 0.4434528350830078
Batch 61/64 loss: 0.4334988594055176
Batch 62/64 loss: 0.29000425338745117
Batch 63/64 loss: 0.1575484275817871
Batch 64/64 loss: -3.152595043182373
Epoch 39  Train loss: 0.3305740038553874  Val loss: 0.22734962542032458
Epoch 40
-------------------------------
Batch 1/64 loss: 0.3843045234680176
Batch 2/64 loss: 0.4969453811645508
Batch 3/64 loss: 0.5801854133605957
Batch 4/64 loss: 0.5531611442565918
Batch 5/64 loss: 0.2571425437927246
Batch 6/64 loss: 0.6396808624267578
Batch 7/64 loss: 0.3417325019836426
Batch 8/64 loss: 0.2684316635131836
Batch 9/64 loss: 0.6606712341308594
Batch 10/64 loss: 0.5264368057250977
Batch 11/64 loss: 0.2681260108947754
Batch 12/64 loss: 0.37004804611206055
Batch 13/64 loss: 0.1708240509033203
Batch 14/64 loss: 0.434537410736084
Batch 15/64 loss: 0.3367733955383301
Batch 16/64 loss: 0.34649038314819336
Batch 17/64 loss: 0.5098752975463867
Batch 18/64 loss: 0.4285273551940918
Batch 19/64 loss: 0.8951511383056641
Batch 20/64 loss: 0.1275010108947754
Batch 21/64 loss: 1.4594612121582031
Batch 22/64 loss: 0.28107261657714844
Batch 23/64 loss: 0.09212684631347656
Batch 24/64 loss: 0.08996963500976562
Batch 25/64 loss: 0.18280839920043945
Batch 26/64 loss: 0.08825445175170898
Batch 27/64 loss: 0.5166449546813965
Batch 28/64 loss: 0.27219295501708984
Batch 29/64 loss: 0.14437532424926758
Batch 30/64 loss: 0.35849952697753906
Batch 31/64 loss: 0.17338085174560547
Batch 32/64 loss: 0.16379499435424805
Batch 33/64 loss: 0.04925060272216797
Batch 34/64 loss: 0.647343635559082
Batch 35/64 loss: 0.13524627685546875
Batch 36/64 loss: 0.089111328125
Batch 37/64 loss: 0.2597517967224121
Batch 38/64 loss: 0.5891890525817871
Batch 39/64 loss: 0.8815412521362305
Batch 40/64 loss: 0.17383050918579102
Batch 41/64 loss: 0.6839618682861328
Batch 42/64 loss: 0.2241654396057129
Batch 43/64 loss: 0.16296052932739258
Batch 44/64 loss: 0.28027963638305664
Batch 45/64 loss: 0.29967164993286133
Batch 46/64 loss: 0.292208194732666
Batch 47/64 loss: 0.2804298400878906
Batch 48/64 loss: 1.1760845184326172
Batch 49/64 loss: 0.009784221649169922
Batch 50/64 loss: 0.797091007232666
Batch 51/64 loss: 0.13976192474365234
Batch 52/64 loss: 0.2332015037536621
Batch 53/64 loss: 0.18759441375732422
Batch 54/64 loss: 0.31270265579223633
Batch 55/64 loss: -0.0054988861083984375
Batch 56/64 loss: 0.2944765090942383
Batch 57/64 loss: 0.22589445114135742
Batch 58/64 loss: 0.21322965621948242
Batch 59/64 loss: 0.1327838897705078
Batch 60/64 loss: 1.3744478225708008
Batch 61/64 loss: 0.11809635162353516
Batch 62/64 loss: 0.06519603729248047
Batch 63/64 loss: 0.22429609298706055
Batch 64/64 loss: -3.270289897918701
Epoch 40  Train loss: 0.32179598901786055  Val loss: -0.04979266661548942
Saving best model, epoch: 40
Epoch 41
-------------------------------
Batch 1/64 loss: 0.045932769775390625
Batch 2/64 loss: 0.19794607162475586
Batch 3/64 loss: 0.11925554275512695
Batch 4/64 loss: 0.23365449905395508
Batch 5/64 loss: 0.27512502670288086
Batch 6/64 loss: 0.6201577186584473
Batch 7/64 loss: 0.1773209571838379
Batch 8/64 loss: 0.5455780029296875
Batch 9/64 loss: 0.29242944717407227
Batch 10/64 loss: 0.18029022216796875
Batch 11/64 loss: 0.25396728515625
Batch 12/64 loss: 0.1538686752319336
Batch 13/64 loss: 0.4376511573791504
Batch 14/64 loss: 0.26548051834106445
Batch 15/64 loss: 0.16097021102905273
Batch 16/64 loss: 0.09709548950195312
Batch 17/64 loss: 0.1835927963256836
Batch 18/64 loss: 0.0807790756225586
Batch 19/64 loss: 0.1089482307434082
Batch 20/64 loss: -0.05754280090332031
Batch 21/64 loss: 0.19408273696899414
Batch 22/64 loss: 0.0845937728881836
Batch 23/64 loss: 0.024978160858154297
Batch 24/64 loss: -0.027953147888183594
Batch 25/64 loss: 0.11014413833618164
Batch 26/64 loss: 0.6773896217346191
Batch 27/64 loss: 0.3788323402404785
Batch 28/64 loss: 1.8021316528320312
Batch 29/64 loss: 0.18704605102539062
Batch 30/64 loss: 0.5398597717285156
Batch 31/64 loss: 0.20029067993164062
Batch 32/64 loss: 0.20800161361694336
Batch 33/64 loss: 0.6496782302856445
Batch 34/64 loss: 0.09925413131713867
Batch 35/64 loss: 0.2264575958251953
Batch 36/64 loss: 0.07924318313598633
Batch 37/64 loss: 0.3499126434326172
Batch 38/64 loss: -0.03585100173950195
Batch 39/64 loss: 0.08696508407592773
Batch 40/64 loss: 0.8766121864318848
Batch 41/64 loss: 0.2954831123352051
Batch 42/64 loss: 0.08101987838745117
Batch 43/64 loss: 0.37107372283935547
Batch 44/64 loss: 0.08096599578857422
Batch 45/64 loss: 0.07051420211791992
Batch 46/64 loss: 0.5572395324707031
Batch 47/64 loss: 0.20373153686523438
Batch 48/64 loss: 0.2704477310180664
Batch 49/64 loss: 0.12956857681274414
Batch 50/64 loss: 0.2849850654602051
Batch 51/64 loss: 0.13070106506347656
Batch 52/64 loss: 0.22518205642700195
Batch 53/64 loss: 0.5833573341369629
Batch 54/64 loss: 0.6411981582641602
Batch 55/64 loss: 0.0405426025390625
Batch 56/64 loss: 0.2993607521057129
Batch 57/64 loss: 0.1473522186279297
Batch 58/64 loss: 1.2806358337402344
Batch 59/64 loss: 1.498870849609375
Batch 60/64 loss: 0.08324861526489258
Batch 61/64 loss: 0.2742938995361328
Batch 62/64 loss: 0.3624539375305176
Batch 63/64 loss: 0.21726322174072266
Batch 64/64 loss: -3.3918395042419434
Epoch 41  Train loss: 0.2614867546979119  Val loss: 0.4973494015198803
Epoch 42
-------------------------------
Batch 1/64 loss: 0.08366966247558594
Batch 2/64 loss: 0.08562898635864258
Batch 3/64 loss: 0.2647976875305176
Batch 4/64 loss: 0.16452312469482422
Batch 5/64 loss: -0.04233503341674805
Batch 6/64 loss: 0.1083059310913086
Batch 7/64 loss: 0.32666921615600586
Batch 8/64 loss: 0.561213493347168
Batch 9/64 loss: 0.2542552947998047
Batch 10/64 loss: 0.24727535247802734
Batch 11/64 loss: -0.007860660552978516
Batch 12/64 loss: 0.24486827850341797
Batch 13/64 loss: 0.39507007598876953
Batch 14/64 loss: 0.3358726501464844
Batch 15/64 loss: 0.5071043968200684
Batch 16/64 loss: 1.6969695091247559
Batch 17/64 loss: 0.09506082534790039
Batch 18/64 loss: -0.0031909942626953125
Batch 19/64 loss: 0.7767672538757324
Batch 20/64 loss: 0.23033857345581055
Batch 21/64 loss: 0.4583010673522949
Batch 22/64 loss: 0.18840932846069336
Batch 23/64 loss: 0.1631765365600586
Batch 24/64 loss: 0.401885986328125
Batch 25/64 loss: 0.23038387298583984
Batch 26/64 loss: 0.1734161376953125
Batch 27/64 loss: 0.14790964126586914
Batch 28/64 loss: 0.5129561424255371
Batch 29/64 loss: 0.16774606704711914
Batch 30/64 loss: 0.5952019691467285
Batch 31/64 loss: 0.38296031951904297
Batch 32/64 loss: 0.6313810348510742
Batch 33/64 loss: 0.29999542236328125
Batch 34/64 loss: 0.43367862701416016
Batch 35/64 loss: 0.03302288055419922
Batch 36/64 loss: 0.12363529205322266
Batch 37/64 loss: 0.5099086761474609
Batch 38/64 loss: 0.5860786437988281
Batch 39/64 loss: 0.31820154190063477
Batch 40/64 loss: 0.6127033233642578
Batch 41/64 loss: 0.23750066757202148
Batch 42/64 loss: 0.27150917053222656
Batch 43/64 loss: 0.28914737701416016
Batch 44/64 loss: 0.2389082908630371
Batch 45/64 loss: 0.019764423370361328
Batch 46/64 loss: 0.4117240905761719
Batch 47/64 loss: 0.2169504165649414
Batch 48/64 loss: 0.16142702102661133
Batch 49/64 loss: 0.3984098434448242
Batch 50/64 loss: 0.15571880340576172
Batch 51/64 loss: 0.04373502731323242
Batch 52/64 loss: 0.040859222412109375
Batch 53/64 loss: -0.04652070999145508
Batch 54/64 loss: 0.022219181060791016
Batch 55/64 loss: 0.01585674285888672
Batch 56/64 loss: 0.02656078338623047
Batch 57/64 loss: 0.20322418212890625
Batch 58/64 loss: 1.6518802642822266
Batch 59/64 loss: 1.1977710723876953
Batch 60/64 loss: 0.2579836845397949
Batch 61/64 loss: 0.18746089935302734
Batch 62/64 loss: 0.07611322402954102
Batch 63/64 loss: 0.0998544692993164
Batch 64/64 loss: -2.78444766998291
Epoch 42  Train loss: 0.27271652595669615  Val loss: 0.20489254194436615
Epoch 43
-------------------------------
Batch 1/64 loss: 0.23523664474487305
Batch 2/64 loss: 0.2435898780822754
Batch 3/64 loss: 0.10578107833862305
Batch 4/64 loss: 0.23744678497314453
Batch 5/64 loss: 0.18462800979614258
Batch 6/64 loss: 0.2212824821472168
Batch 7/64 loss: 0.09761333465576172
Batch 8/64 loss: 0.03246879577636719
Batch 9/64 loss: 0.09932708740234375
Batch 10/64 loss: 0.21781682968139648
Batch 11/64 loss: 0.14980363845825195
Batch 12/64 loss: -0.03149080276489258
Batch 13/64 loss: 0.1757359504699707
Batch 14/64 loss: 0.09637022018432617
Batch 15/64 loss: -0.04205656051635742
Batch 16/64 loss: -0.04973554611206055
Batch 17/64 loss: -0.04818105697631836
Batch 18/64 loss: -0.13712739944458008
Batch 19/64 loss: 0.16782855987548828
Batch 20/64 loss: 0.23592233657836914
Batch 21/64 loss: 0.42865800857543945
Batch 22/64 loss: 0.19462871551513672
Batch 23/64 loss: 0.19995927810668945
Batch 24/64 loss: 0.3885345458984375
Batch 25/64 loss: 0.0804910659790039
Batch 26/64 loss: 0.6792058944702148
Batch 27/64 loss: 0.5436115264892578
Batch 28/64 loss: 0.37270069122314453
Batch 29/64 loss: 0.9310412406921387
Batch 30/64 loss: 0.1065683364868164
Batch 31/64 loss: 0.21441316604614258
Batch 32/64 loss: 0.40355634689331055
Batch 33/64 loss: 0.1758408546447754
Batch 34/64 loss: 0.6117234230041504
Batch 35/64 loss: 0.8004631996154785
Batch 36/64 loss: 0.3262486457824707
Batch 37/64 loss: 0.15005159378051758
Batch 38/64 loss: 0.39443397521972656
Batch 39/64 loss: 0.05714750289916992
Batch 40/64 loss: 0.1047964096069336
Batch 41/64 loss: 0.6928987503051758
Batch 42/64 loss: 0.2155303955078125
Batch 43/64 loss: 0.07384777069091797
Batch 44/64 loss: 0.15705013275146484
Batch 45/64 loss: 0.36666297912597656
Batch 46/64 loss: -0.0092315673828125
Batch 47/64 loss: 0.17181015014648438
Batch 48/64 loss: 1.4420104026794434
Batch 49/64 loss: 0.1341385841369629
Batch 50/64 loss: 0.15817785263061523
Batch 51/64 loss: 0.23695993423461914
Batch 52/64 loss: 0.15560102462768555
Batch 53/64 loss: 0.2411174774169922
Batch 54/64 loss: 0.24861478805541992
Batch 55/64 loss: 0.06982183456420898
Batch 56/64 loss: 0.10075521469116211
Batch 57/64 loss: -0.01946544647216797
Batch 58/64 loss: 0.09904766082763672
Batch 59/64 loss: -0.13929271697998047
Batch 60/64 loss: 1.4670844078063965
Batch 61/64 loss: 0.14792919158935547
Batch 62/64 loss: -0.1192169189453125
Batch 63/64 loss: 0.14325618743896484
Batch 64/64 loss: -3.5263237953186035
Epoch 43  Train loss: 0.19994823418411553  Val loss: 0.00881396945809171
Epoch 44
-------------------------------
Batch 1/64 loss: 0.21199417114257812
Batch 2/64 loss: 0.1297168731689453
Batch 3/64 loss: 0.16588640213012695
Batch 4/64 loss: 0.5039691925048828
Batch 5/64 loss: 0.027534008026123047
Batch 6/64 loss: 0.08641386032104492
Batch 7/64 loss: 0.012942790985107422
Batch 8/64 loss: 0.5031557083129883
Batch 9/64 loss: 0.03294515609741211
Batch 10/64 loss: 0.14168643951416016
Batch 11/64 loss: -0.053734779357910156
Batch 12/64 loss: 0.4336400032043457
Batch 13/64 loss: 0.4340553283691406
Batch 14/64 loss: 0.05044889450073242
Batch 15/64 loss: 0.009862422943115234
Batch 16/64 loss: 0.47892332077026367
Batch 17/64 loss: 0.07300376892089844
Batch 18/64 loss: 0.15949296951293945
Batch 19/64 loss: 1.4173541069030762
Batch 20/64 loss: 0.4299139976501465
Batch 21/64 loss: 0.08140277862548828
Batch 22/64 loss: 0.11006546020507812
Batch 23/64 loss: 0.0029935836791992188
Batch 24/64 loss: 0.10821676254272461
Batch 25/64 loss: 0.05850696563720703
Batch 26/64 loss: 0.15568304061889648
Batch 27/64 loss: 1.5095953941345215
Batch 28/64 loss: 0.3505721092224121
Batch 29/64 loss: 0.10964822769165039
Batch 30/64 loss: 0.22019338607788086
Batch 31/64 loss: -0.060079097747802734
Batch 32/64 loss: 0.14287805557250977
Batch 33/64 loss: 0.36673736572265625
Batch 34/64 loss: 0.1261458396911621
Batch 35/64 loss: 0.20546817779541016
Batch 36/64 loss: 0.018861770629882812
Batch 37/64 loss: 0.07263326644897461
Batch 38/64 loss: 0.23091745376586914
Batch 39/64 loss: 0.14606142044067383
Batch 40/64 loss: 0.061448097229003906
Batch 41/64 loss: 0.21480798721313477
Batch 42/64 loss: 0.4155898094177246
Batch 43/64 loss: 0.2374401092529297
Batch 44/64 loss: 0.03175544738769531
Batch 45/64 loss: 0.1488947868347168
Batch 46/64 loss: 0.30446338653564453
Batch 47/64 loss: 0.17030811309814453
Batch 48/64 loss: 0.08632516860961914
Batch 49/64 loss: 0.046547889709472656
Batch 50/64 loss: 0.17978715896606445
Batch 51/64 loss: -0.031548500061035156
Batch 52/64 loss: -0.0383906364440918
Batch 53/64 loss: 0.5282177925109863
Batch 54/64 loss: 0.07262229919433594
Batch 55/64 loss: 0.1431722640991211
Batch 56/64 loss: 0.2082042694091797
Batch 57/64 loss: 0.15114927291870117
Batch 58/64 loss: 0.40723562240600586
Batch 59/64 loss: 1.0859360694885254
Batch 60/64 loss: 0.025559425354003906
Batch 61/64 loss: 0.07534933090209961
Batch 62/64 loss: -0.06017446517944336
Batch 63/64 loss: 0.048587799072265625
Batch 64/64 loss: -3.2891902923583984
Epoch 44  Train loss: 0.17650356666714537  Val loss: 0.24709976825517477
Epoch 45
-------------------------------
Batch 1/64 loss: -0.06782245635986328
Batch 2/64 loss: 0.1301569938659668
Batch 3/64 loss: 0.1777505874633789
Batch 4/64 loss: 0.44719934463500977
Batch 5/64 loss: 0.0539555549621582
Batch 6/64 loss: 0.2662935256958008
Batch 7/64 loss: 0.08501720428466797
Batch 8/64 loss: 0.2111501693725586
Batch 9/64 loss: 0.18185138702392578
Batch 10/64 loss: 0.3453793525695801
Batch 11/64 loss: 0.32789087295532227
Batch 12/64 loss: 0.21380615234375
Batch 13/64 loss: 0.4404792785644531
Batch 14/64 loss: 0.14676332473754883
Batch 15/64 loss: 0.033072471618652344
Batch 16/64 loss: -0.0527348518371582
Batch 17/64 loss: 0.5472898483276367
Batch 18/64 loss: 0.23714971542358398
Batch 19/64 loss: 0.1310129165649414
Batch 20/64 loss: 0.3831753730773926
Batch 21/64 loss: 0.43323659896850586
Batch 22/64 loss: -0.08103561401367188
Batch 23/64 loss: 0.0963587760925293
Batch 24/64 loss: 0.22802019119262695
Batch 25/64 loss: 0.4734921455383301
Batch 26/64 loss: 0.6070494651794434
Batch 27/64 loss: 0.19275426864624023
Batch 28/64 loss: 0.4676060676574707
Batch 29/64 loss: 0.6236848831176758
Batch 30/64 loss: 0.33394956588745117
Batch 31/64 loss: 0.2551388740539551
Batch 32/64 loss: 1.2401399612426758
Batch 33/64 loss: 0.23005294799804688
Batch 34/64 loss: 0.6564850807189941
Batch 35/64 loss: 0.19798803329467773
Batch 36/64 loss: 0.14354944229125977
Batch 37/64 loss: 0.13875102996826172
Batch 38/64 loss: 2.040095806121826
Batch 39/64 loss: 1.0166535377502441
Batch 40/64 loss: 0.5495696067810059
Batch 41/64 loss: 0.24911165237426758
Batch 42/64 loss: 0.13857221603393555
Batch 43/64 loss: 0.19286203384399414
Batch 44/64 loss: 1.0690484046936035
Batch 45/64 loss: 0.11741352081298828
Batch 46/64 loss: 0.10674524307250977
Batch 47/64 loss: 0.09992599487304688
Batch 48/64 loss: 0.07776021957397461
Batch 49/64 loss: 0.11009979248046875
Batch 50/64 loss: 0.8401966094970703
Batch 51/64 loss: 0.1432490348815918
Batch 52/64 loss: 0.10087156295776367
Batch 53/64 loss: 0.31592702865600586
Batch 54/64 loss: 0.4630270004272461
Batch 55/64 loss: 0.8698210716247559
Batch 56/64 loss: 0.23719358444213867
Batch 57/64 loss: 0.3645906448364258
Batch 58/64 loss: 0.006942272186279297
Batch 59/64 loss: 0.26851844787597656
Batch 60/64 loss: 1.3119726181030273
Batch 61/64 loss: 0.0013942718505859375
Batch 62/64 loss: 0.06924152374267578
Batch 63/64 loss: 0.23871326446533203
Batch 64/64 loss: -3.3053159713745117
Epoch 45  Train loss: 0.29798570146747666  Val loss: -0.010894093726508805
Epoch 46
-------------------------------
Batch 1/64 loss: 0.307894229888916
Batch 2/64 loss: 0.07334184646606445
Batch 3/64 loss: 0.22393131256103516
Batch 4/64 loss: 0.444155216217041
Batch 5/64 loss: 0.1709737777709961
Batch 6/64 loss: 0.2967796325683594
Batch 7/64 loss: 0.21381044387817383
Batch 8/64 loss: -0.04081583023071289
Batch 9/64 loss: 0.4674968719482422
Batch 10/64 loss: 1.0336570739746094
Batch 11/64 loss: 0.38040828704833984
Batch 12/64 loss: 0.10798120498657227
Batch 13/64 loss: 0.0901026725769043
Batch 14/64 loss: 0.09539413452148438
Batch 15/64 loss: 0.14696407318115234
Batch 16/64 loss: 0.06717538833618164
Batch 17/64 loss: 0.08169889450073242
Batch 18/64 loss: -0.12994956970214844
Batch 19/64 loss: 0.06941461563110352
Batch 20/64 loss: 0.14844322204589844
Batch 21/64 loss: 1.261373519897461
Batch 22/64 loss: -0.14857721328735352
Batch 23/64 loss: 0.29765939712524414
Batch 24/64 loss: 0.0539402961730957
Batch 25/64 loss: 0.15398931503295898
Batch 26/64 loss: 0.17848682403564453
Batch 27/64 loss: 0.09698152542114258
Batch 28/64 loss: 0.1457843780517578
Batch 29/64 loss: 0.04164600372314453
Batch 30/64 loss: 0.29773950576782227
Batch 31/64 loss: 1.2814936637878418
Batch 32/64 loss: 1.5169811248779297
Batch 33/64 loss: 0.021245956420898438
Batch 34/64 loss: 0.0061626434326171875
Batch 35/64 loss: 0.015285491943359375
Batch 36/64 loss: -0.13126564025878906
Batch 37/64 loss: 0.1483440399169922
Batch 38/64 loss: -0.09728813171386719
Batch 39/64 loss: -0.048655033111572266
Batch 40/64 loss: 0.0031805038452148438
Batch 41/64 loss: 0.039614200592041016
Batch 42/64 loss: 0.016499996185302734
Batch 43/64 loss: -0.02552318572998047
Batch 44/64 loss: 0.04595375061035156
Batch 45/64 loss: 0.1274428367614746
Batch 46/64 loss: 0.09224081039428711
Batch 47/64 loss: 0.050963401794433594
Batch 48/64 loss: 0.0977630615234375
Batch 49/64 loss: 0.7019648551940918
Batch 50/64 loss: 0.013411521911621094
Batch 51/64 loss: -0.11514520645141602
Batch 52/64 loss: 0.6840801239013672
Batch 53/64 loss: 0.11855506896972656
Batch 54/64 loss: 0.1298213005065918
Batch 55/64 loss: -0.006552219390869141
Batch 56/64 loss: 0.14016103744506836
Batch 57/64 loss: 0.28007030487060547
Batch 58/64 loss: 0.2682490348815918
Batch 59/64 loss: -0.04262733459472656
Batch 60/64 loss: 0.12706756591796875
Batch 61/64 loss: 0.2771129608154297
Batch 62/64 loss: 0.08048677444458008
Batch 63/64 loss: -0.0796360969543457
Batch 64/64 loss: -3.190387725830078
Epoch 46  Train loss: 0.15643214805453431  Val loss: -0.11230291779508296
Saving best model, epoch: 46
Epoch 47
-------------------------------
Batch 1/64 loss: 0.18541431427001953
Batch 2/64 loss: -0.06773090362548828
Batch 3/64 loss: 0.023523330688476562
Batch 4/64 loss: 0.17593908309936523
Batch 5/64 loss: 0.20794296264648438
Batch 6/64 loss: -0.0562443733215332
Batch 7/64 loss: 0.26640748977661133
Batch 8/64 loss: 0.005175590515136719
Batch 9/64 loss: 0.11994361877441406
Batch 10/64 loss: -0.1857619285583496
Batch 11/64 loss: 1.5932936668395996
Batch 12/64 loss: 0.28194284439086914
Batch 13/64 loss: 0.05508899688720703
Batch 14/64 loss: -0.017976760864257812
Batch 15/64 loss: -0.07236671447753906
Batch 16/64 loss: 0.13695716857910156
Batch 17/64 loss: 0.604668140411377
Batch 18/64 loss: -0.08797979354858398
Batch 19/64 loss: 0.2848844528198242
Batch 20/64 loss: -0.054477691650390625
Batch 21/64 loss: 0.045281410217285156
Batch 22/64 loss: -0.08980846405029297
Batch 23/64 loss: 1.037506103515625
Batch 24/64 loss: -0.07724809646606445
Batch 25/64 loss: 0.006162166595458984
Batch 26/64 loss: 1.4436917304992676
Batch 27/64 loss: 0.12993717193603516
Batch 28/64 loss: -0.049848079681396484
Batch 29/64 loss: -0.12396574020385742
Batch 30/64 loss: -0.014951705932617188
Batch 31/64 loss: 0.06141376495361328
Batch 32/64 loss: 0.18206214904785156
Batch 33/64 loss: 0.3053879737854004
Batch 34/64 loss: 0.07247686386108398
Batch 35/64 loss: 0.010119438171386719
Batch 36/64 loss: -0.026506423950195312
Batch 37/64 loss: -0.056944847106933594
Batch 38/64 loss: 0.23871994018554688
Batch 39/64 loss: 0.3819279670715332
Batch 40/64 loss: -0.10761690139770508
Batch 41/64 loss: 0.1587686538696289
Batch 42/64 loss: 0.05841827392578125
Batch 43/64 loss: 0.24848365783691406
Batch 44/64 loss: 0.48696374893188477
Batch 45/64 loss: 0.16223478317260742
Batch 46/64 loss: 0.21409845352172852
Batch 47/64 loss: 0.1546335220336914
Batch 48/64 loss: 0.22515535354614258
Batch 49/64 loss: 0.10628747940063477
Batch 50/64 loss: -0.10001659393310547
Batch 51/64 loss: -0.07276344299316406
Batch 52/64 loss: 0.01992654800415039
Batch 53/64 loss: 0.25890493392944336
Batch 54/64 loss: 0.1084756851196289
Batch 55/64 loss: 0.6345529556274414
Batch 56/64 loss: 0.17197322845458984
Batch 57/64 loss: 0.1311779022216797
Batch 58/64 loss: -0.11574697494506836
Batch 59/64 loss: -0.09142303466796875
Batch 60/64 loss: 0.014925479888916016
Batch 61/64 loss: 0.6130790710449219
Batch 62/64 loss: -0.07674121856689453
Batch 63/64 loss: 0.008057117462158203
Batch 64/64 loss: -3.323357105255127
Epoch 47  Train loss: 0.1191113359787885  Val loss: -0.16308760233351455
Saving best model, epoch: 47
Epoch 48
-------------------------------
Batch 1/64 loss: -0.043558597564697266
Batch 2/64 loss: -0.00991964340209961
Batch 3/64 loss: 0.25509023666381836
Batch 4/64 loss: 0.16718482971191406
Batch 5/64 loss: 0.007873058319091797
Batch 6/64 loss: -0.006439208984375
Batch 7/64 loss: 1.3999066352844238
Batch 8/64 loss: 0.33279895782470703
Batch 9/64 loss: 0.12207984924316406
Batch 10/64 loss: -0.023013591766357422
Batch 11/64 loss: 0.6900262832641602
Batch 12/64 loss: 0.0932459831237793
Batch 13/64 loss: -0.017821311950683594
Batch 14/64 loss: 0.10961341857910156
Batch 15/64 loss: 0.20192575454711914
Batch 16/64 loss: 0.03758096694946289
Batch 17/64 loss: 0.3316826820373535
Batch 18/64 loss: 0.17909669876098633
Batch 19/64 loss: 0.3891167640686035
Batch 20/64 loss: 0.3325223922729492
Batch 21/64 loss: 0.01638174057006836
Batch 22/64 loss: 0.21628713607788086
Batch 23/64 loss: -0.048180580139160156
Batch 24/64 loss: 0.14433765411376953
Batch 25/64 loss: -0.011234760284423828
Batch 26/64 loss: 0.2538285255432129
Batch 27/64 loss: -0.034847259521484375
Batch 28/64 loss: -0.02215290069580078
Batch 29/64 loss: 0.852696418762207
Batch 30/64 loss: -0.10382556915283203
Batch 31/64 loss: -0.114105224609375
Batch 32/64 loss: 0.07956266403198242
Batch 33/64 loss: -0.1534113883972168
Batch 34/64 loss: -0.11337614059448242
Batch 35/64 loss: -0.13046884536743164
Batch 36/64 loss: -0.03925752639770508
Batch 37/64 loss: -0.06867742538452148
Batch 38/64 loss: -0.05975818634033203
Batch 39/64 loss: -0.08915042877197266
Batch 40/64 loss: 0.14358901977539062
Batch 41/64 loss: 0.0576777458190918
Batch 42/64 loss: 0.10183572769165039
Batch 43/64 loss: 0.29703521728515625
Batch 44/64 loss: -0.017600059509277344
Batch 45/64 loss: 0.1400318145751953
Batch 46/64 loss: 1.143545150756836
Batch 47/64 loss: 0.1403970718383789
Batch 48/64 loss: 0.33524179458618164
Batch 49/64 loss: 0.25888824462890625
Batch 50/64 loss: 0.23030328750610352
Batch 51/64 loss: 0.02363872528076172
Batch 52/64 loss: -0.09737253189086914
Batch 53/64 loss: 0.07492733001708984
Batch 54/64 loss: 0.45968055725097656
Batch 55/64 loss: -0.06040334701538086
Batch 56/64 loss: -0.1056509017944336
Batch 57/64 loss: 0.22785615921020508
Batch 58/64 loss: 0.04640054702758789
Batch 59/64 loss: -0.011188507080078125
Batch 60/64 loss: 0.1137847900390625
Batch 61/64 loss: -0.05318784713745117
Batch 62/64 loss: 0.1119232177734375
Batch 63/64 loss: 0.0006728172302246094
Batch 64/64 loss: -3.4299850463867188
Epoch 48  Train loss: 0.09589297724705116  Val loss: -0.13458157568862758
Epoch 49
-------------------------------
Batch 1/64 loss: 0.0607609748840332
Batch 2/64 loss: -0.24651098251342773
Batch 3/64 loss: -0.030708789825439453
Batch 4/64 loss: 0.35935211181640625
Batch 5/64 loss: 0.17200326919555664
Batch 6/64 loss: 0.065460205078125
Batch 7/64 loss: -0.06682205200195312
Batch 8/64 loss: -0.02422809600830078
Batch 9/64 loss: -0.018466949462890625
Batch 10/64 loss: -0.15326404571533203
Batch 11/64 loss: -0.21481609344482422
Batch 12/64 loss: 0.38681936264038086
Batch 13/64 loss: 0.062170982360839844
Batch 14/64 loss: 0.1837787628173828
Batch 15/64 loss: -0.07118463516235352
Batch 16/64 loss: 1.2261171340942383
Batch 17/64 loss: -0.0590662956237793
Batch 18/64 loss: 0.5731658935546875
Batch 19/64 loss: 0.12134027481079102
Batch 20/64 loss: 0.07134246826171875
Batch 21/64 loss: -0.030031681060791016
Batch 22/64 loss: 0.2658982276916504
Batch 23/64 loss: 0.0034303665161132812
Batch 24/64 loss: -0.1693744659423828
Batch 25/64 loss: 0.13526248931884766
Batch 26/64 loss: -0.08738327026367188
Batch 27/64 loss: 0.6239027976989746
Batch 28/64 loss: 0.020400524139404297
Batch 29/64 loss: 0.21360492706298828
Batch 30/64 loss: -0.0074005126953125
Batch 31/64 loss: 0.1519012451171875
Batch 32/64 loss: 0.0647134780883789
Batch 33/64 loss: -0.028701305389404297
Batch 34/64 loss: 0.9233322143554688
Batch 35/64 loss: 0.08833599090576172
Batch 36/64 loss: 0.9010028839111328
Batch 37/64 loss: -0.025458812713623047
Batch 38/64 loss: 0.5623936653137207
Batch 39/64 loss: -0.06887292861938477
Batch 40/64 loss: 0.28673601150512695
Batch 41/64 loss: 0.10210275650024414
Batch 42/64 loss: 0.5091061592102051
Batch 43/64 loss: -0.00170135498046875
Batch 44/64 loss: -0.006358623504638672
Batch 45/64 loss: 0.18471860885620117
Batch 46/64 loss: 0.06291723251342773
Batch 47/64 loss: 0.016553878784179688
Batch 48/64 loss: -0.1481618881225586
Batch 49/64 loss: 0.2761406898498535
Batch 50/64 loss: 0.09688091278076172
Batch 51/64 loss: 0.2152862548828125
Batch 52/64 loss: 0.10525321960449219
Batch 53/64 loss: 0.8953948020935059
Batch 54/64 loss: 0.04544973373413086
Batch 55/64 loss: 0.19196271896362305
Batch 56/64 loss: -0.005358219146728516
Batch 57/64 loss: 0.11588144302368164
Batch 58/64 loss: 0.6259341239929199
Batch 59/64 loss: 0.4311990737915039
Batch 60/64 loss: 0.054874420166015625
Batch 61/64 loss: 0.105682373046875
Batch 62/64 loss: -0.06548357009887695
Batch 63/64 loss: -0.032161712646484375
Batch 64/64 loss: -3.3533668518066406
Epoch 49  Train loss: 0.11736507041781556  Val loss: -0.014767230581172143
Epoch 50
-------------------------------
Batch 1/64 loss: 0.11083793640136719
Batch 2/64 loss: 0.129608154296875
Batch 3/64 loss: 0.13774728775024414
Batch 4/64 loss: 0.15138626098632812
Batch 5/64 loss: 0.04914665222167969
Batch 6/64 loss: 0.1186518669128418
Batch 7/64 loss: 0.412076473236084
Batch 8/64 loss: 0.012683391571044922
Batch 9/64 loss: 0.10126543045043945
Batch 10/64 loss: 0.10831928253173828
Batch 11/64 loss: -0.032019615173339844
Batch 12/64 loss: -0.008088111877441406
Batch 13/64 loss: 0.03421163558959961
Batch 14/64 loss: -0.00455474853515625
Batch 15/64 loss: 0.3602437973022461
Batch 16/64 loss: 0.2341327667236328
Batch 17/64 loss: 0.19147729873657227
Batch 18/64 loss: -0.033480167388916016
Batch 19/64 loss: 1.0766992568969727
Batch 20/64 loss: 0.05307292938232422
Batch 21/64 loss: 0.056765079498291016
Batch 22/64 loss: -0.017696857452392578
Batch 23/64 loss: 0.2285904884338379
Batch 24/64 loss: 0.035822391510009766
Batch 25/64 loss: 0.2742338180541992
Batch 26/64 loss: 0.05198097229003906
Batch 27/64 loss: 0.05000448226928711
Batch 28/64 loss: 0.1519608497619629
Batch 29/64 loss: 0.05164480209350586
Batch 30/64 loss: -0.16617822647094727
Batch 31/64 loss: 0.22261619567871094
Batch 32/64 loss: 0.010686397552490234
Batch 33/64 loss: -0.15134572982788086
Batch 34/64 loss: -0.061762332916259766
Batch 35/64 loss: -0.18686914443969727
Batch 36/64 loss: 0.1455988883972168
Batch 37/64 loss: -0.0656590461730957
Batch 38/64 loss: 0.1497955322265625
Batch 39/64 loss: -0.012631416320800781
Batch 40/64 loss: -0.14754676818847656
Batch 41/64 loss: 0.1763019561767578
Batch 42/64 loss: 0.9989066123962402
Batch 43/64 loss: 0.17713212966918945
Batch 44/64 loss: 2.755215644836426
Batch 45/64 loss: -0.013406753540039062
Batch 46/64 loss: 0.06748723983764648
Batch 47/64 loss: -0.11992120742797852
Batch 48/64 loss: -0.1380634307861328
Batch 49/64 loss: -0.030556201934814453
Batch 50/64 loss: -0.0744619369506836
Batch 51/64 loss: 0.28095388412475586
Batch 52/64 loss: -0.03180074691772461
Batch 53/64 loss: 0.4298224449157715
Batch 54/64 loss: 0.16809797286987305
Batch 55/64 loss: 0.14985275268554688
Batch 56/64 loss: 0.09583616256713867
Batch 57/64 loss: 0.35375070571899414
Batch 58/64 loss: 0.2644643783569336
Batch 59/64 loss: -0.10687446594238281
Batch 60/64 loss: -0.13980674743652344
Batch 61/64 loss: 0.4849848747253418
Batch 62/64 loss: 0.0821232795715332
Batch 63/64 loss: -0.19735336303710938
Batch 64/64 loss: -3.4416327476501465
Epoch 50  Train loss: 0.10784139259188782  Val loss: -0.08158272812046956
Epoch 51
-------------------------------
Batch 1/64 loss: -0.07433128356933594
Batch 2/64 loss: 0.23836088180541992
Batch 3/64 loss: 0.04594707489013672
Batch 4/64 loss: 0.07751035690307617
Batch 5/64 loss: 0.4122047424316406
Batch 6/64 loss: 0.37412500381469727
Batch 7/64 loss: 0.1818857192993164
Batch 8/64 loss: 0.09951543807983398
Batch 9/64 loss: -0.06576204299926758
Batch 10/64 loss: -0.03997325897216797
Batch 11/64 loss: 0.03142070770263672
Batch 12/64 loss: 0.14170408248901367
Batch 13/64 loss: 0.02971649169921875
Batch 14/64 loss: 0.09833955764770508
Batch 15/64 loss: 1.4877209663391113
Batch 16/64 loss: -0.08265972137451172
Batch 17/64 loss: 0.03564596176147461
Batch 18/64 loss: 0.36199378967285156
Batch 19/64 loss: 0.38937997817993164
Batch 20/64 loss: 0.09066390991210938
Batch 21/64 loss: 0.2182631492614746
Batch 22/64 loss: -0.0014629364013671875
Batch 23/64 loss: 0.24270105361938477
Batch 24/64 loss: -0.04275703430175781
Batch 25/64 loss: 0.03180217742919922
Batch 26/64 loss: 0.32112789154052734
Batch 27/64 loss: 0.32354736328125
Batch 28/64 loss: 0.4525923728942871
Batch 29/64 loss: 0.31562137603759766
Batch 30/64 loss: 0.011914730072021484
Batch 31/64 loss: 0.40556859970092773
Batch 32/64 loss: 0.24187231063842773
Batch 33/64 loss: 0.10166788101196289
Batch 34/64 loss: 0.09043645858764648
Batch 35/64 loss: 0.1188664436340332
Batch 36/64 loss: -0.04200887680053711
Batch 37/64 loss: 0.19528913497924805
Batch 38/64 loss: 0.11660575866699219
Batch 39/64 loss: 0.6270532608032227
Batch 40/64 loss: 0.06314945220947266
Batch 41/64 loss: 0.14899682998657227
Batch 42/64 loss: -0.10892200469970703
Batch 43/64 loss: 0.2476177215576172
Batch 44/64 loss: 0.1323080062866211
Batch 45/64 loss: -0.06969165802001953
Batch 46/64 loss: 0.0755319595336914
Batch 47/64 loss: 0.18074989318847656
Batch 48/64 loss: -0.03730487823486328
Batch 49/64 loss: 0.40508127212524414
Batch 50/64 loss: 0.16195154190063477
Batch 51/64 loss: 0.13265752792358398
Batch 52/64 loss: 0.10656547546386719
Batch 53/64 loss: 0.7789297103881836
Batch 54/64 loss: 0.07535123825073242
Batch 55/64 loss: 0.028905391693115234
Batch 56/64 loss: 0.11640691757202148
Batch 57/64 loss: 0.1652374267578125
Batch 58/64 loss: 0.004706382751464844
Batch 59/64 loss: 0.07033157348632812
Batch 60/64 loss: 0.3325505256652832
Batch 61/64 loss: 0.045336246490478516
Batch 62/64 loss: -0.021780014038085938
Batch 63/64 loss: 1.698885440826416
Batch 64/64 loss: -2.1723432540893555
Epoch 51  Train loss: 0.16731614131553502  Val loss: 0.2955883327628329
Epoch 52
-------------------------------
Batch 1/64 loss: 0.16161203384399414
Batch 2/64 loss: 0.10340213775634766
Batch 3/64 loss: 0.041156768798828125
Batch 4/64 loss: 0.18208837509155273
Batch 5/64 loss: 0.4372711181640625
Batch 6/64 loss: 0.12674188613891602
Batch 7/64 loss: 0.21358919143676758
Batch 8/64 loss: 0.02792835235595703
Batch 9/64 loss: 1.592738151550293
Batch 10/64 loss: -0.13213586807250977
Batch 11/64 loss: 0.605928897857666
Batch 12/64 loss: 0.04850959777832031
Batch 13/64 loss: 0.4511685371398926
Batch 14/64 loss: 0.11010217666625977
Batch 15/64 loss: 0.3854050636291504
Batch 16/64 loss: 0.42891883850097656
Batch 17/64 loss: 0.37836599349975586
Batch 18/64 loss: -0.029175758361816406
Batch 19/64 loss: 0.14410972595214844
Batch 20/64 loss: 0.18922185897827148
Batch 21/64 loss: 0.0782175064086914
Batch 22/64 loss: -0.15451765060424805
Batch 23/64 loss: -0.0845036506652832
Batch 24/64 loss: 0.11909055709838867
Batch 25/64 loss: 0.08951473236083984
Batch 26/64 loss: 0.0006666183471679688
Batch 27/64 loss: 0.28717517852783203
Batch 28/64 loss: 0.2256450653076172
Batch 29/64 loss: -0.08461952209472656
Batch 30/64 loss: -0.01073598861694336
Batch 31/64 loss: 0.32697296142578125
Batch 32/64 loss: 1.7890939712524414
Batch 33/64 loss: 0.1801004409790039
Batch 34/64 loss: 0.25237560272216797
Batch 35/64 loss: 1.243908405303955
Batch 36/64 loss: 0.2226862907409668
Batch 37/64 loss: 0.2198028564453125
Batch 38/64 loss: 0.05371427536010742
Batch 39/64 loss: -0.005343914031982422
Batch 40/64 loss: 0.06718063354492188
Batch 41/64 loss: 0.12925100326538086
Batch 42/64 loss: 0.2917518615722656
Batch 43/64 loss: 0.2404637336730957
Batch 44/64 loss: 0.18326759338378906
Batch 45/64 loss: 0.1891798973083496
Batch 46/64 loss: 0.4327836036682129
Batch 47/64 loss: 0.06116819381713867
Batch 48/64 loss: 0.21881723403930664
Batch 49/64 loss: 0.1068572998046875
Batch 50/64 loss: -0.05405092239379883
Batch 51/64 loss: 0.11666202545166016
Batch 52/64 loss: 0.015002250671386719
Batch 53/64 loss: -0.18250513076782227
Batch 54/64 loss: -0.06304645538330078
Batch 55/64 loss: 0.0066738128662109375
Batch 56/64 loss: 0.035704612731933594
Batch 57/64 loss: 0.29447507858276367
Batch 58/64 loss: -0.04114246368408203
Batch 59/64 loss: 0.061749935150146484
Batch 60/64 loss: -0.07004976272583008
Batch 61/64 loss: -0.10227775573730469
Batch 62/64 loss: 0.44205522537231445
Batch 63/64 loss: -0.06978893280029297
Batch 64/64 loss: -2.4311132431030273
Epoch 52  Train loss: 0.16789079927930645  Val loss: -0.08378384776951112
Epoch 53
-------------------------------
Batch 1/64 loss: -0.09710025787353516
Batch 2/64 loss: 0.005650520324707031
Batch 3/64 loss: 0.19045162200927734
Batch 4/64 loss: -0.002418994903564453
Batch 5/64 loss: 0.20065832138061523
Batch 6/64 loss: 0.7163739204406738
Batch 7/64 loss: 0.07830286026000977
Batch 8/64 loss: -0.03970050811767578
Batch 9/64 loss: 0.1845874786376953
Batch 10/64 loss: 0.4701576232910156
Batch 11/64 loss: 0.04611921310424805
Batch 12/64 loss: -0.029195308685302734
Batch 13/64 loss: 0.17159032821655273
Batch 14/64 loss: -0.04650259017944336
Batch 15/64 loss: 0.8646583557128906
Batch 16/64 loss: -0.053301334381103516
Batch 17/64 loss: 0.12040424346923828
Batch 18/64 loss: 0.04904651641845703
Batch 19/64 loss: 0.1844649314880371
Batch 20/64 loss: 0.09240150451660156
Batch 21/64 loss: 0.060694217681884766
Batch 22/64 loss: 0.019928455352783203
Batch 23/64 loss: -0.11504030227661133
Batch 24/64 loss: 1.0389885902404785
Batch 25/64 loss: 0.24235057830810547
Batch 26/64 loss: 0.436647891998291
Batch 27/64 loss: -0.08201360702514648
Batch 28/64 loss: 0.17911958694458008
Batch 29/64 loss: 0.11862707138061523
Batch 30/64 loss: -0.017308712005615234
Batch 31/64 loss: 0.1458144187927246
Batch 32/64 loss: 0.05307579040527344
Batch 33/64 loss: 0.039406776428222656
Batch 34/64 loss: 0.14107799530029297
Batch 35/64 loss: 0.1897144317626953
Batch 36/64 loss: 0.2602047920227051
Batch 37/64 loss: 0.06669950485229492
Batch 38/64 loss: -0.0496983528137207
Batch 39/64 loss: 1.3015551567077637
Batch 40/64 loss: 0.12028741836547852
Batch 41/64 loss: 0.3326258659362793
Batch 42/64 loss: 1.4086003303527832
Batch 43/64 loss: 0.16598844528198242
Batch 44/64 loss: -0.013209342956542969
Batch 45/64 loss: 0.12068891525268555
Batch 46/64 loss: -0.11884498596191406
Batch 47/64 loss: 0.13305902481079102
Batch 48/64 loss: 0.05832815170288086
Batch 49/64 loss: 0.1708531379699707
Batch 50/64 loss: 0.018588542938232422
Batch 51/64 loss: 0.2302989959716797
Batch 52/64 loss: 0.5700016021728516
Batch 53/64 loss: -0.011577129364013672
Batch 54/64 loss: 0.31397199630737305
Batch 55/64 loss: 0.3999142646789551
Batch 56/64 loss: 0.037097930908203125
Batch 57/64 loss: -0.011144161224365234
Batch 58/64 loss: 0.053437232971191406
Batch 59/64 loss: -0.028772830963134766
Batch 60/64 loss: -0.01874399185180664
Batch 61/64 loss: -0.08254528045654297
Batch 62/64 loss: -0.07920360565185547
Batch 63/64 loss: 0.16282033920288086
Batch 64/64 loss: -3.149853229522705
Epoch 53  Train loss: 0.13657448899512198  Val loss: -0.006447624914424936
Epoch 54
-------------------------------
Batch 1/64 loss: -0.0010499954223632812
Batch 2/64 loss: 0.04315948486328125
Batch 3/64 loss: 0.012519359588623047
Batch 4/64 loss: 0.1452045440673828
Batch 5/64 loss: 0.10705089569091797
Batch 6/64 loss: 0.3420572280883789
Batch 7/64 loss: 0.03191423416137695
Batch 8/64 loss: -0.11450529098510742
Batch 9/64 loss: 0.23053979873657227
Batch 10/64 loss: 0.0858297348022461
Batch 11/64 loss: 0.09901714324951172
Batch 12/64 loss: 1.3186230659484863
Batch 13/64 loss: -0.046227455139160156
Batch 14/64 loss: 0.1277470588684082
Batch 15/64 loss: 0.0030303001403808594
Batch 16/64 loss: 0.21975946426391602
Batch 17/64 loss: 0.015038013458251953
Batch 18/64 loss: -0.07467126846313477
Batch 19/64 loss: -0.10164785385131836
Batch 20/64 loss: -0.14664077758789062
Batch 21/64 loss: 0.030885696411132812
Batch 22/64 loss: -0.015675067901611328
Batch 23/64 loss: 0.12228822708129883
Batch 24/64 loss: -0.13243341445922852
Batch 25/64 loss: 0.01785564422607422
Batch 26/64 loss: -0.10918760299682617
Batch 27/64 loss: 0.8571925163269043
Batch 28/64 loss: 0.32018375396728516
Batch 29/64 loss: 0.02590799331665039
Batch 30/64 loss: 0.003926277160644531
Batch 31/64 loss: -0.08642387390136719
Batch 32/64 loss: 0.02093648910522461
Batch 33/64 loss: 0.014721393585205078
Batch 34/64 loss: -0.1697072982788086
Batch 35/64 loss: 0.20096206665039062
Batch 36/64 loss: 0.08105134963989258
Batch 37/64 loss: -0.003949642181396484
Batch 38/64 loss: -0.05332803726196289
Batch 39/64 loss: 0.111663818359375
Batch 40/64 loss: 0.24825716018676758
Batch 41/64 loss: 0.2298111915588379
Batch 42/64 loss: 0.06753158569335938
Batch 43/64 loss: 0.12858057022094727
Batch 44/64 loss: 0.0944986343383789
Batch 45/64 loss: -0.16373586654663086
Batch 46/64 loss: 0.18869829177856445
Batch 47/64 loss: -0.10534286499023438
Batch 48/64 loss: 0.24748897552490234
Batch 49/64 loss: 0.40158939361572266
Batch 50/64 loss: 1.7981128692626953
Batch 51/64 loss: -0.05307912826538086
Batch 52/64 loss: 0.0017938613891601562
Batch 53/64 loss: 0.05018424987792969
Batch 54/64 loss: 0.1776871681213379
Batch 55/64 loss: 0.19158506393432617
Batch 56/64 loss: 0.7440028190612793
Batch 57/64 loss: 0.09556102752685547
Batch 58/64 loss: 0.21383953094482422
Batch 59/64 loss: 0.07921075820922852
Batch 60/64 loss: 0.028202533721923828
Batch 61/64 loss: 0.060459136962890625
Batch 62/64 loss: 0.06435298919677734
Batch 63/64 loss: 0.13399505615234375
Batch 64/64 loss: -3.381265640258789
Epoch 54  Train loss: 0.09287770589192708  Val loss: 0.022980903022477718
Epoch 55
-------------------------------
Batch 1/64 loss: 0.11711359024047852
Batch 2/64 loss: 0.11551332473754883
Batch 3/64 loss: 0.02159404754638672
Batch 4/64 loss: 0.19959688186645508
Batch 5/64 loss: 0.3937873840332031
Batch 6/64 loss: 0.1545095443725586
Batch 7/64 loss: -0.0008683204650878906
Batch 8/64 loss: 0.17711877822875977
Batch 9/64 loss: 0.035671234130859375
Batch 10/64 loss: 0.02296924591064453
Batch 11/64 loss: -0.07367658615112305
Batch 12/64 loss: -0.009705066680908203
Batch 13/64 loss: 0.021036624908447266
Batch 14/64 loss: -0.05921173095703125
Batch 15/64 loss: 1.2185091972351074
Batch 16/64 loss: 1.4226222038269043
Batch 17/64 loss: 0.05603170394897461
Batch 18/64 loss: -0.011187076568603516
Batch 19/64 loss: 0.09042835235595703
Batch 20/64 loss: -0.06797170639038086
Batch 21/64 loss: -0.03497171401977539
Batch 22/64 loss: 0.12885284423828125
Batch 23/64 loss: -0.10202884674072266
Batch 24/64 loss: 0.055735111236572266
Batch 25/64 loss: 0.2947878837585449
Batch 26/64 loss: 0.17978811264038086
Batch 27/64 loss: 0.12786626815795898
Batch 28/64 loss: -0.05266618728637695
Batch 29/64 loss: -0.07483386993408203
Batch 30/64 loss: -0.17629575729370117
Batch 31/64 loss: -0.17010259628295898
Batch 32/64 loss: 0.07762384414672852
Batch 33/64 loss: -0.12630510330200195
Batch 34/64 loss: -0.027280330657958984
Batch 35/64 loss: -0.0883016586303711
Batch 36/64 loss: -0.08524084091186523
Batch 37/64 loss: 0.13095331192016602
Batch 38/64 loss: -0.06707048416137695
Batch 39/64 loss: 0.054522037506103516
Batch 40/64 loss: -0.08668661117553711
Batch 41/64 loss: 0.023333072662353516
Batch 42/64 loss: 0.24927330017089844
Batch 43/64 loss: 0.19418954849243164
Batch 44/64 loss: -0.04632234573364258
Batch 45/64 loss: 0.3400712013244629
Batch 46/64 loss: 0.012206554412841797
Batch 47/64 loss: 0.02203083038330078
Batch 48/64 loss: 0.06178712844848633
Batch 49/64 loss: -0.03076648712158203
Batch 50/64 loss: 0.268435001373291
Batch 51/64 loss: -0.22518444061279297
Batch 52/64 loss: 0.19721078872680664
Batch 53/64 loss: 0.41591596603393555
Batch 54/64 loss: -0.02932596206665039
Batch 55/64 loss: 0.15169000625610352
Batch 56/64 loss: 0.13423871994018555
Batch 57/64 loss: -0.04449748992919922
Batch 58/64 loss: 0.07104730606079102
Batch 59/64 loss: -0.022084712982177734
Batch 60/64 loss: -0.007075309753417969
Batch 61/64 loss: 0.33369874954223633
Batch 62/64 loss: 0.15327739715576172
Batch 63/64 loss: 0.4416656494140625
Batch 64/64 loss: -2.0599870681762695
Epoch 55  Train loss: 0.07689492094750498  Val loss: -0.04527817558996456
Epoch 56
-------------------------------
Batch 1/64 loss: 0.18845510482788086
Batch 2/64 loss: 0.047556400299072266
Batch 3/64 loss: 0.1754741668701172
Batch 4/64 loss: 0.25532102584838867
Batch 5/64 loss: 0.24448776245117188
Batch 6/64 loss: 0.10140657424926758
Batch 7/64 loss: 0.09743452072143555
Batch 8/64 loss: -0.1154475212097168
Batch 9/64 loss: 0.08276700973510742
Batch 10/64 loss: 0.03518533706665039
Batch 11/64 loss: -0.07569646835327148
Batch 12/64 loss: 0.1109628677368164
Batch 13/64 loss: -0.12016105651855469
Batch 14/64 loss: 1.109583854675293
Batch 15/64 loss: 0.20271015167236328
Batch 16/64 loss: -0.07086610794067383
Batch 17/64 loss: -0.03515291213989258
Batch 18/64 loss: 0.2942342758178711
Batch 19/64 loss: -0.01720905303955078
Batch 20/64 loss: 0.11284875869750977
Batch 21/64 loss: -0.04021930694580078
Batch 22/64 loss: 0.05130910873413086
Batch 23/64 loss: 2.386293888092041
Batch 24/64 loss: 0.048789024353027344
Batch 25/64 loss: -0.004980564117431641
Batch 26/64 loss: -0.2064199447631836
Batch 27/64 loss: -0.03895711898803711
Batch 28/64 loss: -0.06795501708984375
Batch 29/64 loss: -0.2022876739501953
Batch 30/64 loss: -0.15421772003173828
Batch 31/64 loss: -0.09556818008422852
Batch 32/64 loss: -0.03867197036743164
Batch 33/64 loss: -0.026895523071289062
Batch 34/64 loss: -0.0008711814880371094
Batch 35/64 loss: 1.0183906555175781
Batch 36/64 loss: 0.12967491149902344
Batch 37/64 loss: 0.029779434204101562
Batch 38/64 loss: -0.16247940063476562
Batch 39/64 loss: -0.06828069686889648
Batch 40/64 loss: -0.035067081451416016
Batch 41/64 loss: 0.15987300872802734
Batch 42/64 loss: 0.056729793548583984
Batch 43/64 loss: 0.22376489639282227
Batch 44/64 loss: -0.11087417602539062
Batch 45/64 loss: 0.02129364013671875
Batch 46/64 loss: -0.039046287536621094
Batch 47/64 loss: 0.08584260940551758
Batch 48/64 loss: -0.12442779541015625
Batch 49/64 loss: -0.08282852172851562
Batch 50/64 loss: 0.47840070724487305
Batch 51/64 loss: -0.14220714569091797
Batch 52/64 loss: -0.11401033401489258
Batch 53/64 loss: 0.38415050506591797
Batch 54/64 loss: 1.1378498077392578
Batch 55/64 loss: 0.43851137161254883
Batch 56/64 loss: 0.016377925872802734
Batch 57/64 loss: -0.11495304107666016
Batch 58/64 loss: 0.015876293182373047
Batch 59/64 loss: -0.05172443389892578
Batch 60/64 loss: 0.06235170364379883
Batch 61/64 loss: -0.19194316864013672
Batch 62/64 loss: 0.2623481750488281
Batch 63/64 loss: -0.08440446853637695
Batch 64/64 loss: -3.235748767852783
Epoch 56  Train loss: 0.07851607565786324  Val loss: -0.15740068835491167
Epoch 57
-------------------------------
Batch 1/64 loss: 0.15714406967163086
Batch 2/64 loss: -0.15128469467163086
Batch 3/64 loss: -0.06812715530395508
Batch 4/64 loss: -0.2759699821472168
Batch 5/64 loss: -0.08403158187866211
Batch 6/64 loss: 0.0379796028137207
Batch 7/64 loss: 1.0557408332824707
Batch 8/64 loss: 0.19427013397216797
Batch 9/64 loss: -0.14761066436767578
Batch 10/64 loss: 0.28656911849975586
Batch 11/64 loss: 0.0872945785522461
Batch 12/64 loss: -0.15025091171264648
Batch 13/64 loss: -0.041863441467285156
Batch 14/64 loss: -0.1326274871826172
Batch 15/64 loss: 0.10665464401245117
Batch 16/64 loss: -0.10098743438720703
Batch 17/64 loss: 0.6091370582580566
Batch 18/64 loss: 0.06656169891357422
Batch 19/64 loss: 0.23383712768554688
Batch 20/64 loss: 0.3260207176208496
Batch 21/64 loss: -0.14588308334350586
Batch 22/64 loss: -0.031481266021728516
Batch 23/64 loss: -0.09763956069946289
Batch 24/64 loss: -0.062081336975097656
Batch 25/64 loss: -0.14744138717651367
Batch 26/64 loss: -0.19691181182861328
Batch 27/64 loss: -0.10084962844848633
Batch 28/64 loss: 0.008590221405029297
Batch 29/64 loss: 1.0552887916564941
Batch 30/64 loss: -0.005330562591552734
Batch 31/64 loss: 1.975447177886963
Batch 32/64 loss: -0.10601186752319336
Batch 33/64 loss: 0.04793596267700195
Batch 34/64 loss: 0.42263269424438477
Batch 35/64 loss: -0.1140146255493164
Batch 36/64 loss: 0.09875679016113281
Batch 37/64 loss: 0.1853809356689453
Batch 38/64 loss: -0.1527414321899414
Batch 39/64 loss: 0.16648626327514648
Batch 40/64 loss: 1.6127643585205078
Batch 41/64 loss: -0.04447174072265625
Batch 42/64 loss: 0.052609920501708984
Batch 43/64 loss: 0.37691593170166016
Batch 44/64 loss: 0.10242319107055664
Batch 45/64 loss: 0.5256900787353516
Batch 46/64 loss: 0.10333824157714844
Batch 47/64 loss: 0.21263408660888672
Batch 48/64 loss: 0.8401780128479004
Batch 49/64 loss: 0.7893824577331543
Batch 50/64 loss: 0.10450172424316406
Batch 51/64 loss: 0.7550721168518066
Batch 52/64 loss: 0.4633007049560547
Batch 53/64 loss: -0.015691757202148438
Batch 54/64 loss: 0.14373159408569336
Batch 55/64 loss: 0.21832036972045898
Batch 56/64 loss: 0.3133583068847656
Batch 57/64 loss: 0.06565570831298828
Batch 58/64 loss: 0.14908647537231445
Batch 59/64 loss: 0.10864686965942383
Batch 60/64 loss: 0.1309337615966797
Batch 61/64 loss: 0.08474349975585938
Batch 62/64 loss: 0.20611000061035156
Batch 63/64 loss: 0.06683349609375
Batch 64/64 loss: -2.593627452850342
Epoch 57  Train loss: 0.16046173058304133  Val loss: -0.05054788491160599
Epoch 58
-------------------------------
Batch 1/64 loss: 0.3359827995300293
Batch 2/64 loss: 0.13202762603759766
Batch 3/64 loss: 0.6012892723083496
Batch 4/64 loss: -0.009148597717285156
Batch 5/64 loss: -0.09346771240234375
Batch 6/64 loss: 0.21748685836791992
Batch 7/64 loss: 0.13044404983520508
Batch 8/64 loss: 0.1842942237854004
Batch 9/64 loss: 0.021270751953125
Batch 10/64 loss: 0.09621572494506836
Batch 11/64 loss: -0.01457834243774414
Batch 12/64 loss: -0.09108448028564453
Batch 13/64 loss: 0.04628419876098633
Batch 14/64 loss: 0.8452401161193848
Batch 15/64 loss: -0.01171112060546875
Batch 16/64 loss: 0.16266489028930664
Batch 17/64 loss: 0.18704748153686523
Batch 18/64 loss: 0.057333946228027344
Batch 19/64 loss: -0.03469514846801758
Batch 20/64 loss: 0.06698083877563477
Batch 21/64 loss: 0.35869359970092773
Batch 22/64 loss: -0.08712482452392578
Batch 23/64 loss: -0.1600937843322754
Batch 24/64 loss: 0.11299514770507812
Batch 25/64 loss: -0.0027647018432617188
Batch 26/64 loss: -0.06643056869506836
Batch 27/64 loss: 0.16236495971679688
Batch 28/64 loss: 0.0742654800415039
Batch 29/64 loss: 1.4273157119750977
Batch 30/64 loss: -0.02610492706298828
Batch 31/64 loss: 0.020558834075927734
Batch 32/64 loss: -0.0528111457824707
Batch 33/64 loss: 0.05570363998413086
Batch 34/64 loss: 0.1388554573059082
Batch 35/64 loss: 1.1886701583862305
Batch 36/64 loss: 0.0662999153137207
Batch 37/64 loss: 0.1627979278564453
Batch 38/64 loss: -0.028351306915283203
Batch 39/64 loss: 0.07668828964233398
Batch 40/64 loss: 0.36298704147338867
Batch 41/64 loss: -0.026927471160888672
Batch 42/64 loss: 0.10754156112670898
Batch 43/64 loss: -0.2139110565185547
Batch 44/64 loss: -0.19133758544921875
Batch 45/64 loss: -0.037578582763671875
Batch 46/64 loss: 0.8326168060302734
Batch 47/64 loss: -0.055785179138183594
Batch 48/64 loss: 0.04763364791870117
Batch 49/64 loss: 0.11632347106933594
Batch 50/64 loss: 0.6474995613098145
Batch 51/64 loss: 0.11968421936035156
Batch 52/64 loss: 0.1921830177307129
Batch 53/64 loss: -0.04600811004638672
Batch 54/64 loss: 0.6987810134887695
Batch 55/64 loss: 0.1850132942199707
Batch 56/64 loss: 0.1805257797241211
Batch 57/64 loss: 0.05343484878540039
Batch 58/64 loss: 0.02736949920654297
Batch 59/64 loss: 0.39922428131103516
Batch 60/64 loss: 0.6228842735290527
Batch 61/64 loss: 0.13167905807495117
Batch 62/64 loss: -0.04569530487060547
Batch 63/64 loss: 1.221114158630371
Batch 64/64 loss: -2.266911029815674
Epoch 58  Train loss: 0.15498783074173272  Val loss: 0.5866955760418344
Epoch 59
-------------------------------
Batch 1/64 loss: 0.3731546401977539
Batch 2/64 loss: 0.13782644271850586
Batch 3/64 loss: 0.14383459091186523
Batch 4/64 loss: 0.22313642501831055
Batch 5/64 loss: 0.1911487579345703
Batch 6/64 loss: 0.19087457656860352
Batch 7/64 loss: -0.04136848449707031
Batch 8/64 loss: 0.1150360107421875
Batch 9/64 loss: 0.12691640853881836
Batch 10/64 loss: 0.16636323928833008
Batch 11/64 loss: 0.1386404037475586
Batch 12/64 loss: 0.1727924346923828
Batch 13/64 loss: 0.32448911666870117
Batch 14/64 loss: 0.09544086456298828
Batch 15/64 loss: 0.1399521827697754
Batch 16/64 loss: 0.21324920654296875
Batch 17/64 loss: 0.08554220199584961
Batch 18/64 loss: 0.25142908096313477
Batch 19/64 loss: 0.31701087951660156
Batch 20/64 loss: 0.2410287857055664
Batch 21/64 loss: 0.16383981704711914
Batch 22/64 loss: 0.27166271209716797
Batch 23/64 loss: 0.6024160385131836
Batch 24/64 loss: 0.12198352813720703
Batch 25/64 loss: 0.1062169075012207
Batch 26/64 loss: 0.4886155128479004
Batch 27/64 loss: -0.11730670928955078
Batch 28/64 loss: 0.6614766120910645
Batch 29/64 loss: 0.22834396362304688
Batch 30/64 loss: 0.08653736114501953
Batch 31/64 loss: 0.28166913986206055
Batch 32/64 loss: 0.12775897979736328
Batch 33/64 loss: -0.015274524688720703
Batch 34/64 loss: 0.019830703735351562
Batch 35/64 loss: 0.3103213310241699
Batch 36/64 loss: -0.028030872344970703
Batch 37/64 loss: 0.322965145111084
Batch 38/64 loss: 0.0041141510009765625
Batch 39/64 loss: 0.04520082473754883
Batch 40/64 loss: 0.21362066268920898
Batch 41/64 loss: 0.07567167282104492
Batch 42/64 loss: 0.1751265525817871
Batch 43/64 loss: 0.0989999771118164
Batch 44/64 loss: 0.08219051361083984
Batch 45/64 loss: 0.0482783317565918
Batch 46/64 loss: 1.3572187423706055
Batch 47/64 loss: -0.022349834442138672
Batch 48/64 loss: 0.6470465660095215
Batch 49/64 loss: 0.13758373260498047
Batch 50/64 loss: 0.12060928344726562
Batch 51/64 loss: 0.44333362579345703
Batch 52/64 loss: 0.04808235168457031
Batch 53/64 loss: 0.1558361053466797
Batch 54/64 loss: -0.08999252319335938
Batch 55/64 loss: 0.10046815872192383
Batch 56/64 loss: -0.08548212051391602
Batch 57/64 loss: 0.08134317398071289
Batch 58/64 loss: 0.11339569091796875
Batch 59/64 loss: 1.5929832458496094
Batch 60/64 loss: 0.37255382537841797
Batch 61/64 loss: 1.1498017311096191
Batch 62/64 loss: 0.9946932792663574
Batch 63/64 loss: -0.06950712203979492
Batch 64/64 loss: -3.5388264656066895
Epoch 59  Train loss: 0.19413684957167682  Val loss: -0.08170478532404424
Epoch 60
-------------------------------
Batch 1/64 loss: 0.08421468734741211
Batch 2/64 loss: 0.04661130905151367
Batch 3/64 loss: -0.09960031509399414
Batch 4/64 loss: -0.06799507141113281
Batch 5/64 loss: 0.09607172012329102
Batch 6/64 loss: 1.1391148567199707
Batch 7/64 loss: 0.019687175750732422
Batch 8/64 loss: 0.11065483093261719
Batch 9/64 loss: -0.18338394165039062
Batch 10/64 loss: -0.11637210845947266
Batch 11/64 loss: -0.08526468276977539
Batch 12/64 loss: -0.12704896926879883
Batch 13/64 loss: -0.061185359954833984
Batch 14/64 loss: -0.12279558181762695
Batch 15/64 loss: -0.035686492919921875
Batch 16/64 loss: -0.027640819549560547
Batch 17/64 loss: -0.012499809265136719
Batch 18/64 loss: -0.1682267189025879
Batch 19/64 loss: -0.18079423904418945
Batch 20/64 loss: -0.031487464904785156
Batch 21/64 loss: -0.12746000289916992
Batch 22/64 loss: 0.04708147048950195
Batch 23/64 loss: 0.38024187088012695
Batch 24/64 loss: 0.0315852165222168
Batch 25/64 loss: -0.00984048843383789
Batch 26/64 loss: 0.059130191802978516
Batch 27/64 loss: -0.011583805084228516
Batch 28/64 loss: 0.017110347747802734
Batch 29/64 loss: -0.10338354110717773
Batch 30/64 loss: 0.23825597763061523
Batch 31/64 loss: -0.11142778396606445
Batch 32/64 loss: 0.03688955307006836
Batch 33/64 loss: 0.014410972595214844
Batch 34/64 loss: 0.07276201248168945
Batch 35/64 loss: 0.06366395950317383
Batch 36/64 loss: -0.0507354736328125
Batch 37/64 loss: 0.0524139404296875
Batch 38/64 loss: -0.1540083885192871
Batch 39/64 loss: -0.09347295761108398
Batch 40/64 loss: -0.05903768539428711
Batch 41/64 loss: 0.05721569061279297
Batch 42/64 loss: -0.1369647979736328
Batch 43/64 loss: -0.09247970581054688
Batch 44/64 loss: -0.23198604583740234
Batch 45/64 loss: -0.024268627166748047
Batch 46/64 loss: -0.12689685821533203
Batch 47/64 loss: -0.13470983505249023
Batch 48/64 loss: 1.2907133102416992
Batch 49/64 loss: 0.5002970695495605
Batch 50/64 loss: -0.07349824905395508
Batch 51/64 loss: 0.4523148536682129
Batch 52/64 loss: 0.7319130897521973
Batch 53/64 loss: 0.9086270332336426
Batch 54/64 loss: -0.18828058242797852
Batch 55/64 loss: -0.07687854766845703
Batch 56/64 loss: -0.03189516067504883
Batch 57/64 loss: 0.017133235931396484
Batch 58/64 loss: 0.034698486328125
Batch 59/64 loss: 0.0775914192199707
Batch 60/64 loss: -0.023144245147705078
Batch 61/64 loss: 0.2318863868713379
Batch 62/64 loss: -0.17128753662109375
Batch 63/64 loss: 0.08702564239501953
Batch 64/64 loss: -3.4815673828125
Epoch 60  Train loss: 0.014665394203335631  Val loss: -0.16236771259111227
Epoch 61
-------------------------------
Batch 1/64 loss: -0.12392950057983398
Batch 2/64 loss: -0.19663238525390625
Batch 3/64 loss: -0.1893610954284668
Batch 4/64 loss: 0.7606983184814453
Batch 5/64 loss: -0.29280900955200195
Batch 6/64 loss: -0.11198854446411133
Batch 7/64 loss: -0.23749494552612305
Batch 8/64 loss: 1.009211540222168
Batch 9/64 loss: -0.10837459564208984
Batch 10/64 loss: -0.048183441162109375
Batch 11/64 loss: -0.2070631980895996
Batch 12/64 loss: -0.1823110580444336
Batch 13/64 loss: -0.17676115036010742
Batch 14/64 loss: 0.23354434967041016
Batch 15/64 loss: -0.24976778030395508
Batch 16/64 loss: -0.30895233154296875
Batch 17/64 loss: -0.003292560577392578
Batch 18/64 loss: 0.0015001296997070312
Batch 19/64 loss: -0.1996908187866211
Batch 20/64 loss: -0.16509294509887695
Batch 21/64 loss: -0.13684606552124023
Batch 22/64 loss: -0.00993204116821289
Batch 23/64 loss: 0.007440090179443359
Batch 24/64 loss: 0.32047462463378906
Batch 25/64 loss: 0.08415508270263672
Batch 26/64 loss: 0.9403038024902344
Batch 27/64 loss: -0.049384117126464844
Batch 28/64 loss: 0.08682584762573242
Batch 29/64 loss: -0.06254768371582031
Batch 30/64 loss: -0.12238931655883789
Batch 31/64 loss: -0.10447406768798828
Batch 32/64 loss: 0.088958740234375
Batch 33/64 loss: 0.04479646682739258
Batch 34/64 loss: -0.12068462371826172
Batch 35/64 loss: -0.13176918029785156
Batch 36/64 loss: 1.477323055267334
Batch 37/64 loss: -0.16871404647827148
Batch 38/64 loss: -0.07649993896484375
Batch 39/64 loss: 0.16262006759643555
Batch 40/64 loss: 0.33323097229003906
Batch 41/64 loss: 0.024625778198242188
Batch 42/64 loss: 0.0520482063293457
Batch 43/64 loss: -0.14917802810668945
Batch 44/64 loss: -0.20644140243530273
Batch 45/64 loss: 0.06414127349853516
Batch 46/64 loss: 0.19683408737182617
Batch 47/64 loss: -0.06540107727050781
Batch 48/64 loss: 0.03540945053100586
Batch 49/64 loss: 0.05386066436767578
Batch 50/64 loss: 0.012714862823486328
Batch 51/64 loss: -0.03879404067993164
Batch 52/64 loss: 0.032755374908447266
Batch 53/64 loss: 1.2678875923156738
Batch 54/64 loss: -0.08551168441772461
Batch 55/64 loss: 0.3315916061401367
Batch 56/64 loss: 0.4324197769165039
Batch 57/64 loss: 0.11246919631958008
Batch 58/64 loss: 0.3100295066833496
Batch 59/64 loss: 0.14677000045776367
Batch 60/64 loss: 0.09442520141601562
Batch 61/64 loss: 0.2016305923461914
Batch 62/64 loss: 0.3206820487976074
Batch 63/64 loss: -0.1905994415283203
Batch 64/64 loss: -3.4763588905334473
Epoch 61  Train loss: 0.033148816052605126  Val loss: 0.6628191053252859
Epoch 62
-------------------------------
Batch 1/64 loss: -0.13917207717895508
Batch 2/64 loss: -0.14429473876953125
Batch 3/64 loss: 0.20234155654907227
Batch 4/64 loss: 0.6079740524291992
Batch 5/64 loss: 0.029857158660888672
Batch 6/64 loss: 0.6378879547119141
Batch 7/64 loss: -0.07721471786499023
Batch 8/64 loss: -0.09616613388061523
Batch 9/64 loss: 0.15433168411254883
Batch 10/64 loss: 0.24506282806396484
Batch 11/64 loss: -0.0424041748046875
Batch 12/64 loss: 0.006438732147216797
Batch 13/64 loss: 0.2653360366821289
Batch 14/64 loss: 0.9921603202819824
Batch 15/64 loss: -0.1594066619873047
Batch 16/64 loss: 0.017215251922607422
Batch 17/64 loss: -0.0961155891418457
Batch 18/64 loss: -0.1148233413696289
Batch 19/64 loss: -0.09025812149047852
Batch 20/64 loss: 0.06977462768554688
Batch 21/64 loss: -0.10075998306274414
Batch 22/64 loss: 0.06907176971435547
Batch 23/64 loss: 0.0004086494445800781
Batch 24/64 loss: 1.220296859741211
Batch 25/64 loss: -0.09277629852294922
Batch 26/64 loss: -0.02858114242553711
Batch 27/64 loss: -0.04864311218261719
Batch 28/64 loss: 0.22168207168579102
Batch 29/64 loss: -0.13591575622558594
Batch 30/64 loss: -0.15192747116088867
Batch 31/64 loss: -0.053137779235839844
Batch 32/64 loss: 0.2740316390991211
Batch 33/64 loss: 0.13417625427246094
Batch 34/64 loss: 0.018223285675048828
Batch 35/64 loss: 0.5171823501586914
Batch 36/64 loss: 0.012145042419433594
Batch 37/64 loss: 0.21564292907714844
Batch 38/64 loss: 0.14425325393676758
Batch 39/64 loss: -0.23216629028320312
Batch 40/64 loss: 0.3362421989440918
Batch 41/64 loss: 0.10743045806884766
Batch 42/64 loss: -0.20847463607788086
Batch 43/64 loss: 0.002209186553955078
Batch 44/64 loss: 0.9454665184020996
Batch 45/64 loss: -0.03255271911621094
Batch 46/64 loss: 0.11946678161621094
Batch 47/64 loss: 0.11485576629638672
Batch 48/64 loss: 0.24483394622802734
Batch 49/64 loss: 0.2886481285095215
Batch 50/64 loss: -0.1773080825805664
Batch 51/64 loss: 0.017368793487548828
Batch 52/64 loss: -0.10504627227783203
Batch 53/64 loss: 0.012433528900146484
Batch 54/64 loss: 0.3428001403808594
Batch 55/64 loss: -0.06525230407714844
Batch 56/64 loss: -0.0882267951965332
Batch 57/64 loss: 0.29729413986206055
Batch 58/64 loss: -0.1062922477722168
Batch 59/64 loss: -0.1406111717224121
Batch 60/64 loss: -0.04191923141479492
Batch 61/64 loss: -0.04931020736694336
Batch 62/64 loss: 0.08810567855834961
Batch 63/64 loss: 0.10581302642822266
Batch 64/64 loss: -3.5175089836120605
Epoch 62  Train loss: 0.056809000875435625  Val loss: -0.3240230468540257
Saving best model, epoch: 62
Epoch 63
-------------------------------
Batch 1/64 loss: -0.17600679397583008
Batch 2/64 loss: -0.1604447364807129
Batch 3/64 loss: 0.014278411865234375
Batch 4/64 loss: 0.07700395584106445
Batch 5/64 loss: 0.21786785125732422
Batch 6/64 loss: -0.1688094139099121
Batch 7/64 loss: 0.345339298248291
Batch 8/64 loss: 0.41945457458496094
Batch 9/64 loss: -0.044857025146484375
Batch 10/64 loss: -0.26428747177124023
Batch 11/64 loss: -0.2383413314819336
Batch 12/64 loss: -0.026299476623535156
Batch 13/64 loss: 0.2713608741760254
Batch 14/64 loss: -0.16008520126342773
Batch 15/64 loss: 0.021176815032958984
Batch 16/64 loss: 0.19188928604125977
Batch 17/64 loss: 0.04610252380371094
Batch 18/64 loss: 0.401639461517334
Batch 19/64 loss: -0.05543375015258789
Batch 20/64 loss: -0.10786104202270508
Batch 21/64 loss: 0.1669926643371582
Batch 22/64 loss: -0.056250572204589844
Batch 23/64 loss: 0.024674415588378906
Batch 24/64 loss: -0.11061382293701172
Batch 25/64 loss: 0.12429046630859375
Batch 26/64 loss: -0.0635976791381836
Batch 27/64 loss: 0.24876785278320312
Batch 28/64 loss: 0.1608743667602539
Batch 29/64 loss: -0.039156436920166016
Batch 30/64 loss: 0.16028451919555664
Batch 31/64 loss: -0.040194034576416016
Batch 32/64 loss: 0.04440450668334961
Batch 33/64 loss: -0.08920431137084961
Batch 34/64 loss: 0.1444239616394043
Batch 35/64 loss: 1.081106185913086
Batch 36/64 loss: -0.03674888610839844
Batch 37/64 loss: 0.2579498291015625
Batch 38/64 loss: -0.1458754539489746
Batch 39/64 loss: 0.3467674255371094
Batch 40/64 loss: -0.1048884391784668
Batch 41/64 loss: 0.39528942108154297
Batch 42/64 loss: 0.08893156051635742
Batch 43/64 loss: 0.0864100456237793
Batch 44/64 loss: -0.09109115600585938
Batch 45/64 loss: 2.581880569458008
Batch 46/64 loss: -0.06580877304077148
Batch 47/64 loss: 0.18858957290649414
Batch 48/64 loss: 0.09912109375
Batch 49/64 loss: 0.1647968292236328
Batch 50/64 loss: 0.5982294082641602
Batch 51/64 loss: 0.28228282928466797
Batch 52/64 loss: 0.05068254470825195
Batch 53/64 loss: 0.524418830871582
Batch 54/64 loss: 0.3224353790283203
Batch 55/64 loss: 0.3780508041381836
Batch 56/64 loss: 0.5965986251831055
Batch 57/64 loss: 0.5639824867248535
Batch 58/64 loss: 0.20270729064941406
Batch 59/64 loss: 0.18224382400512695
Batch 60/64 loss: 0.15053749084472656
Batch 61/64 loss: 0.1959071159362793
Batch 62/64 loss: 0.0773468017578125
Batch 63/64 loss: 0.017960548400878906
Batch 64/64 loss: -2.241852283477783
Epoch 63  Train loss: 0.1347107027091232  Val loss: 0.4361006615497812
Epoch 64
-------------------------------
Batch 1/64 loss: 0.4380002021789551
Batch 2/64 loss: 0.21256351470947266
Batch 3/64 loss: 0.17163658142089844
Batch 4/64 loss: 0.8067331314086914
Batch 5/64 loss: 0.10894012451171875
Batch 6/64 loss: 0.1702713966369629
Batch 7/64 loss: 0.10869169235229492
Batch 8/64 loss: 0.3394932746887207
Batch 9/64 loss: 0.35439062118530273
Batch 10/64 loss: 1.3221635818481445
Batch 11/64 loss: 0.1367645263671875
Batch 12/64 loss: 0.39434051513671875
Batch 13/64 loss: 1.4005317687988281
Batch 14/64 loss: 0.36024045944213867
Batch 15/64 loss: 0.24830150604248047
Batch 16/64 loss: 0.4465508460998535
Batch 17/64 loss: 0.2664484977722168
Batch 18/64 loss: 0.07682228088378906
Batch 19/64 loss: 0.29222726821899414
Batch 20/64 loss: 0.22760534286499023
Batch 21/64 loss: 0.29318761825561523
Batch 22/64 loss: 0.39789438247680664
Batch 23/64 loss: 0.14666414260864258
Batch 24/64 loss: 0.23693275451660156
Batch 25/64 loss: 0.08225297927856445
Batch 26/64 loss: 0.6142463684082031
Batch 27/64 loss: 2.1933255195617676
Batch 28/64 loss: 0.15880489349365234
Batch 29/64 loss: 0.05127096176147461
Batch 30/64 loss: 0.08663702011108398
Batch 31/64 loss: 0.15055227279663086
Batch 32/64 loss: 0.48018598556518555
Batch 33/64 loss: 0.16185474395751953
Batch 34/64 loss: 0.1730189323425293
Batch 35/64 loss: 0.3437509536743164
Batch 36/64 loss: 0.7746548652648926
Batch 37/64 loss: 0.037804603576660156
Batch 38/64 loss: 0.44577741622924805
Batch 39/64 loss: 0.41228294372558594
Batch 40/64 loss: 0.8384542465209961
Batch 41/64 loss: 0.18015432357788086
Batch 42/64 loss: 0.08548116683959961
Batch 43/64 loss: 0.29869651794433594
Batch 44/64 loss: 0.12892580032348633
Batch 45/64 loss: 0.19816875457763672
Batch 46/64 loss: -0.06372690200805664
Batch 47/64 loss: -0.005691051483154297
Batch 48/64 loss: 0.18738031387329102
Batch 49/64 loss: -0.058516502380371094
Batch 50/64 loss: -0.0375676155090332
Batch 51/64 loss: 0.1535325050354004
Batch 52/64 loss: 0.43645286560058594
Batch 53/64 loss: -0.02815103530883789
Batch 54/64 loss: -0.1003870964050293
Batch 55/64 loss: 0.5710301399230957
Batch 56/64 loss: 0.036942481994628906
Batch 57/64 loss: -0.05641365051269531
Batch 58/64 loss: 0.16845369338989258
Batch 59/64 loss: 0.18154048919677734
Batch 60/64 loss: 1.465529441833496
Batch 61/64 loss: 0.16997957229614258
Batch 62/64 loss: -0.21486663818359375
Batch 63/64 loss: 0.07850885391235352
Batch 64/64 loss: -3.3561973571777344
Epoch 64  Train loss: 0.27012673172296264  Val loss: -0.1671888344885967
Epoch 65
-------------------------------
Batch 1/64 loss: 0.5735564231872559
Batch 2/64 loss: 0.265408992767334
Batch 3/64 loss: 0.3500504493713379
Batch 4/64 loss: 0.1957249641418457
Batch 5/64 loss: 0.07207107543945312
Batch 6/64 loss: 0.10373592376708984
Batch 7/64 loss: 1.247997760772705
Batch 8/64 loss: 0.06439447402954102
Batch 9/64 loss: 0.9718279838562012
Batch 10/64 loss: -0.0980839729309082
Batch 11/64 loss: -0.15250682830810547
Batch 12/64 loss: 0.05280637741088867
Batch 13/64 loss: -0.043514251708984375
Batch 14/64 loss: -0.11844635009765625
Batch 15/64 loss: 0.002643585205078125
Batch 16/64 loss: -0.04252433776855469
Batch 17/64 loss: 0.21293258666992188
Batch 18/64 loss: 0.1768965721130371
Batch 19/64 loss: -0.03198719024658203
Batch 20/64 loss: -0.040224552154541016
Batch 21/64 loss: 0.08146429061889648
Batch 22/64 loss: -0.15282201766967773
Batch 23/64 loss: 0.4409055709838867
Batch 24/64 loss: -0.1180419921875
Batch 25/64 loss: -0.05581331253051758
Batch 26/64 loss: -0.07706308364868164
Batch 27/64 loss: -0.0031709671020507812
Batch 28/64 loss: 0.005429744720458984
Batch 29/64 loss: 0.11650753021240234
Batch 30/64 loss: 0.058197975158691406
Batch 31/64 loss: 0.0977787971496582
Batch 32/64 loss: 0.040793418884277344
Batch 33/64 loss: 0.19702482223510742
Batch 34/64 loss: -0.01908731460571289
Batch 35/64 loss: -0.13389015197753906
Batch 36/64 loss: -0.17659235000610352
Batch 37/64 loss: -0.03188467025756836
Batch 38/64 loss: 0.24578380584716797
Batch 39/64 loss: 0.22343921661376953
Batch 40/64 loss: -0.09651899337768555
Batch 41/64 loss: 0.004832267761230469
Batch 42/64 loss: -0.06577920913696289
Batch 43/64 loss: -0.23387575149536133
Batch 44/64 loss: -0.10749483108520508
Batch 45/64 loss: -0.19709444046020508
Batch 46/64 loss: -0.1940760612487793
Batch 47/64 loss: 0.10465335845947266
Batch 48/64 loss: 1.5601544380187988
Batch 49/64 loss: -0.024638652801513672
Batch 50/64 loss: -0.2497868537902832
Batch 51/64 loss: 0.02361917495727539
Batch 52/64 loss: 0.07165956497192383
Batch 53/64 loss: -0.12749910354614258
Batch 54/64 loss: 0.17600011825561523
Batch 55/64 loss: 0.09188365936279297
Batch 56/64 loss: 0.15820884704589844
Batch 57/64 loss: -0.03748655319213867
Batch 58/64 loss: 0.25615644454956055
Batch 59/64 loss: 0.13125085830688477
Batch 60/64 loss: 0.12175178527832031
Batch 61/64 loss: 0.09629392623901367
Batch 62/64 loss: -0.15463829040527344
Batch 63/64 loss: 0.09659624099731445
Batch 64/64 loss: -3.549891471862793
Epoch 65  Train loss: 0.05087799745447495  Val loss: -0.13559433193141243
Epoch 66
-------------------------------
Batch 1/64 loss: -0.14493274688720703
Batch 2/64 loss: -0.08967399597167969
Batch 3/64 loss: 0.11384391784667969
Batch 4/64 loss: 0.3542466163635254
Batch 5/64 loss: -0.05802345275878906
Batch 6/64 loss: -0.08232879638671875
Batch 7/64 loss: -0.1759958267211914
Batch 8/64 loss: -0.08006000518798828
Batch 9/64 loss: -0.06640338897705078
Batch 10/64 loss: -0.13680648803710938
Batch 11/64 loss: 0.023390769958496094
Batch 12/64 loss: 0.02311849594116211
Batch 13/64 loss: -0.11881685256958008
Batch 14/64 loss: 0.23914766311645508
Batch 15/64 loss: -0.16031646728515625
Batch 16/64 loss: -0.0885152816772461
Batch 17/64 loss: -0.2010817527770996
Batch 18/64 loss: -0.1625995635986328
Batch 19/64 loss: -0.08569717407226562
Batch 20/64 loss: -0.07298040390014648
Batch 21/64 loss: 0.07689619064331055
Batch 22/64 loss: -0.002635478973388672
Batch 23/64 loss: 0.22600603103637695
Batch 24/64 loss: -0.3511314392089844
Batch 25/64 loss: -0.23059511184692383
Batch 26/64 loss: -0.1326465606689453
Batch 27/64 loss: 0.9411449432373047
Batch 28/64 loss: -0.29491615295410156
Batch 29/64 loss: -0.05411529541015625
Batch 30/64 loss: -0.2613530158996582
Batch 31/64 loss: -0.18764877319335938
Batch 32/64 loss: 0.19952392578125
Batch 33/64 loss: -0.21917247772216797
Batch 34/64 loss: -0.12121343612670898
Batch 35/64 loss: -0.013714790344238281
Batch 36/64 loss: -0.12568378448486328
Batch 37/64 loss: 0.25005435943603516
Batch 38/64 loss: 0.6932969093322754
Batch 39/64 loss: -0.00022125244140625
Batch 40/64 loss: -0.12734365463256836
Batch 41/64 loss: -0.11426782608032227
Batch 42/64 loss: -0.1407170295715332
Batch 43/64 loss: -0.15931987762451172
Batch 44/64 loss: 1.046626091003418
Batch 45/64 loss: 0.47000837326049805
Batch 46/64 loss: -0.05398082733154297
Batch 47/64 loss: -0.0914006233215332
Batch 48/64 loss: 0.09471845626831055
Batch 49/64 loss: -0.004885673522949219
Batch 50/64 loss: -0.028885364532470703
Batch 51/64 loss: -0.30468130111694336
Batch 52/64 loss: -0.08240032196044922
Batch 53/64 loss: -0.3223414421081543
Batch 54/64 loss: -0.1554403305053711
Batch 55/64 loss: -0.022754669189453125
Batch 56/64 loss: 0.011675834655761719
Batch 57/64 loss: -0.0027627944946289062
Batch 58/64 loss: -0.23214054107666016
Batch 59/64 loss: 0.10089111328125
Batch 60/64 loss: -0.02311086654663086
Batch 61/64 loss: 1.3045616149902344
Batch 62/64 loss: -0.19112920761108398
Batch 63/64 loss: 0.6121807098388672
Batch 64/64 loss: -3.4103870391845703
Epoch 66  Train loss: -0.024365496167949603  Val loss: -0.3741111886460347
Saving best model, epoch: 66
Epoch 67
-------------------------------
Batch 1/64 loss: -0.0094146728515625
Batch 2/64 loss: -0.01879739761352539
Batch 3/64 loss: -0.13579559326171875
Batch 4/64 loss: -0.30289602279663086
Batch 5/64 loss: -0.2542991638183594
Batch 6/64 loss: -0.01576519012451172
Batch 7/64 loss: -0.07381057739257812
Batch 8/64 loss: -0.08458471298217773
Batch 9/64 loss: 0.3729124069213867
Batch 10/64 loss: 0.04965972900390625
Batch 11/64 loss: 0.5258998870849609
Batch 12/64 loss: -0.17522811889648438
Batch 13/64 loss: -0.2803010940551758
Batch 14/64 loss: -0.22211885452270508
Batch 15/64 loss: -0.13235235214233398
Batch 16/64 loss: -0.22650814056396484
Batch 17/64 loss: -0.17872953414916992
Batch 18/64 loss: -0.1893916130065918
Batch 19/64 loss: 1.3638248443603516
Batch 20/64 loss: -0.10802698135375977
Batch 21/64 loss: 0.23826026916503906
Batch 22/64 loss: 0.9069223403930664
Batch 23/64 loss: -0.14017963409423828
Batch 24/64 loss: -0.18294906616210938
Batch 25/64 loss: -0.06641817092895508
Batch 26/64 loss: -0.13592195510864258
Batch 27/64 loss: -0.20590448379516602
Batch 28/64 loss: -0.02858734130859375
Batch 29/64 loss: -0.2476491928100586
Batch 30/64 loss: 0.10526895523071289
Batch 31/64 loss: -0.1425800323486328
Batch 32/64 loss: -0.3230733871459961
Batch 33/64 loss: 0.019550800323486328
Batch 34/64 loss: -0.028551578521728516
Batch 35/64 loss: -0.16119098663330078
Batch 36/64 loss: -0.10780668258666992
Batch 37/64 loss: -0.20442581176757812
Batch 38/64 loss: -0.04825019836425781
Batch 39/64 loss: -0.14399337768554688
Batch 40/64 loss: -0.022750377655029297
Batch 41/64 loss: 0.8131914138793945
Batch 42/64 loss: -0.19180917739868164
Batch 43/64 loss: -0.23068618774414062
Batch 44/64 loss: -0.10702276229858398
Batch 45/64 loss: 0.10079288482666016
Batch 46/64 loss: -0.04112529754638672
Batch 47/64 loss: -0.17322635650634766
Batch 48/64 loss: 0.11211347579956055
Batch 49/64 loss: -0.16903066635131836
Batch 50/64 loss: -0.060425758361816406
Batch 51/64 loss: 0.23078250885009766
Batch 52/64 loss: -0.028768539428710938
Batch 53/64 loss: 1.272730827331543
Batch 54/64 loss: -0.12416458129882812
Batch 55/64 loss: 0.03820180892944336
Batch 56/64 loss: -0.2108621597290039
Batch 57/64 loss: -0.12764501571655273
Batch 58/64 loss: 0.1201171875
Batch 59/64 loss: 0.26027631759643555
Batch 60/64 loss: -0.18338537216186523
Batch 61/64 loss: 0.008526325225830078
Batch 62/64 loss: -0.12434577941894531
Batch 63/64 loss: 0.16687345504760742
Batch 64/64 loss: -3.614063262939453
Epoch 67  Train loss: -0.03726105035520067  Val loss: -0.261101830865919
Epoch 68
-------------------------------
Batch 1/64 loss: -0.12418270111083984
Batch 2/64 loss: -0.1405329704284668
Batch 3/64 loss: 0.13170623779296875
Batch 4/64 loss: 0.13117074966430664
Batch 5/64 loss: -0.0720071792602539
Batch 6/64 loss: -0.17101383209228516
Batch 7/64 loss: -0.05104637145996094
Batch 8/64 loss: -0.08501291275024414
Batch 9/64 loss: -0.13354063034057617
Batch 10/64 loss: -0.24097156524658203
Batch 11/64 loss: 0.022755146026611328
Batch 12/64 loss: -0.06451177597045898
Batch 13/64 loss: 0.05641937255859375
Batch 14/64 loss: -0.025094985961914062
Batch 15/64 loss: 0.5917620658874512
Batch 16/64 loss: 0.05757713317871094
Batch 17/64 loss: 0.009430885314941406
Batch 18/64 loss: 0.14630603790283203
Batch 19/64 loss: -0.25488758087158203
Batch 20/64 loss: -0.057707786560058594
Batch 21/64 loss: 0.17311477661132812
Batch 22/64 loss: -0.0028367042541503906
Batch 23/64 loss: -0.11866521835327148
Batch 24/64 loss: 0.19819879531860352
Batch 25/64 loss: 0.04965496063232422
Batch 26/64 loss: -0.08367300033569336
Batch 27/64 loss: -0.16333961486816406
Batch 28/64 loss: -0.11453914642333984
Batch 29/64 loss: 0.06458377838134766
Batch 30/64 loss: 1.0254340171813965
Batch 31/64 loss: -0.13964033126831055
Batch 32/64 loss: -0.10106897354125977
Batch 33/64 loss: -0.15398597717285156
Batch 34/64 loss: -0.04941606521606445
Batch 35/64 loss: 1.774843692779541
Batch 36/64 loss: -0.28017425537109375
Batch 37/64 loss: -0.0755925178527832
Batch 38/64 loss: 0.4492206573486328
Batch 39/64 loss: 0.08182430267333984
Batch 40/64 loss: 0.0885629653930664
Batch 41/64 loss: 0.08595132827758789
Batch 42/64 loss: 0.38706493377685547
Batch 43/64 loss: 0.23884344100952148
Batch 44/64 loss: 0.027184486389160156
Batch 45/64 loss: 0.05928373336791992
Batch 46/64 loss: -0.019956588745117188
Batch 47/64 loss: -0.09334230422973633
Batch 48/64 loss: 0.4451413154602051
Batch 49/64 loss: -0.15375041961669922
Batch 50/64 loss: 0.012041568756103516
Batch 51/64 loss: 0.2906966209411621
Batch 52/64 loss: -0.06343603134155273
Batch 53/64 loss: 0.3992156982421875
Batch 54/64 loss: 0.14261817932128906
Batch 55/64 loss: 0.061595916748046875
Batch 56/64 loss: 0.2718544006347656
Batch 57/64 loss: 0.502230167388916
Batch 58/64 loss: 0.31544923782348633
Batch 59/64 loss: -0.025780200958251953
Batch 60/64 loss: -0.08265018463134766
Batch 61/64 loss: 0.004164695739746094
Batch 62/64 loss: -0.11150598526000977
Batch 63/64 loss: 1.2958073616027832
Batch 64/64 loss: -3.401707172393799
Epoch 68  Train loss: 0.05939708971509747  Val loss: -0.22480876175398679
Epoch 69
-------------------------------
Batch 1/64 loss: 1.0423474311828613
Batch 2/64 loss: -0.25073909759521484
Batch 3/64 loss: -0.29033851623535156
Batch 4/64 loss: 1.2483677864074707
Batch 5/64 loss: -0.16072368621826172
Batch 6/64 loss: -0.0006513595581054688
Batch 7/64 loss: -0.046886444091796875
Batch 8/64 loss: 0.11178159713745117
Batch 9/64 loss: -0.17190265655517578
Batch 10/64 loss: -0.1794910430908203
Batch 11/64 loss: -0.14584779739379883
Batch 12/64 loss: -0.18166780471801758
Batch 13/64 loss: 0.2643899917602539
Batch 14/64 loss: -0.21291446685791016
Batch 15/64 loss: -0.07827568054199219
Batch 16/64 loss: 0.043642520904541016
Batch 17/64 loss: -0.2103133201599121
Batch 18/64 loss: -0.06549453735351562
Batch 19/64 loss: -0.25924253463745117
Batch 20/64 loss: -0.26142215728759766
Batch 21/64 loss: 0.051700592041015625
Batch 22/64 loss: -0.17120027542114258
Batch 23/64 loss: -0.37717247009277344
Batch 24/64 loss: -0.2126760482788086
Batch 25/64 loss: -0.26415491104125977
Batch 26/64 loss: -0.16249608993530273
Batch 27/64 loss: -0.0029354095458984375
Batch 28/64 loss: -0.0814356803894043
Batch 29/64 loss: 0.034549713134765625
Batch 30/64 loss: -0.11837244033813477
Batch 31/64 loss: -0.2269439697265625
Batch 32/64 loss: -0.1382594108581543
Batch 33/64 loss: -0.28035879135131836
Batch 34/64 loss: -0.02771472930908203
Batch 35/64 loss: -0.15248584747314453
Batch 36/64 loss: 0.15693998336791992
Batch 37/64 loss: 1.4034218788146973
Batch 38/64 loss: -0.05523109436035156
Batch 39/64 loss: 0.018910884857177734
Batch 40/64 loss: 0.3646678924560547
Batch 41/64 loss: 0.5005221366882324
Batch 42/64 loss: 0.034044742584228516
Batch 43/64 loss: -0.14289331436157227
Batch 44/64 loss: 0.12746334075927734
Batch 45/64 loss: -0.2997012138366699
Batch 46/64 loss: 0.2707638740539551
Batch 47/64 loss: -0.09895133972167969
Batch 48/64 loss: 0.0189361572265625
Batch 49/64 loss: -0.15253591537475586
Batch 50/64 loss: -0.08358621597290039
Batch 51/64 loss: 0.1508941650390625
Batch 52/64 loss: 1.1669421195983887
Batch 53/64 loss: 0.3339409828186035
Batch 54/64 loss: -0.10187911987304688
Batch 55/64 loss: -0.16601848602294922
Batch 56/64 loss: -0.030577659606933594
Batch 57/64 loss: -0.17648887634277344
Batch 58/64 loss: 0.03746795654296875
Batch 59/64 loss: 0.15404939651489258
Batch 60/64 loss: -0.17490863800048828
Batch 61/64 loss: 0.06461477279663086
Batch 62/64 loss: -0.16628551483154297
Batch 63/64 loss: -0.19346857070922852
Batch 64/64 loss: -3.827446937561035
Epoch 69  Train loss: -0.028939112494973575  Val loss: -0.3498482327280995
Epoch 70
-------------------------------
Batch 1/64 loss: -0.12901020050048828
Batch 2/64 loss: 0.0023331642150878906
Batch 3/64 loss: -0.27669811248779297
Batch 4/64 loss: -0.21617889404296875
Batch 5/64 loss: -0.002040386199951172
Batch 6/64 loss: -0.2048940658569336
Batch 7/64 loss: 0.015666961669921875
Batch 8/64 loss: -0.1845569610595703
Batch 9/64 loss: -0.046236515045166016
Batch 10/64 loss: -0.004558086395263672
Batch 11/64 loss: -0.2268524169921875
Batch 12/64 loss: -0.28598976135253906
Batch 13/64 loss: 0.020126819610595703
Batch 14/64 loss: -0.20369863510131836
Batch 15/64 loss: -0.030668258666992188
Batch 16/64 loss: 0.028427600860595703
Batch 17/64 loss: -0.2130446434020996
Batch 18/64 loss: 0.0025463104248046875
Batch 19/64 loss: -0.07060527801513672
Batch 20/64 loss: -0.010149002075195312
Batch 21/64 loss: 0.5207328796386719
Batch 22/64 loss: -0.0974421501159668
Batch 23/64 loss: -0.09821081161499023
Batch 24/64 loss: 0.2338252067565918
Batch 25/64 loss: -0.15027856826782227
Batch 26/64 loss: 0.9094409942626953
Batch 27/64 loss: -0.20288658142089844
Batch 28/64 loss: -0.15296125411987305
Batch 29/64 loss: -0.1764512062072754
Batch 30/64 loss: -0.20682334899902344
Batch 31/64 loss: -0.24876880645751953
Batch 32/64 loss: -0.20401620864868164
Batch 33/64 loss: 1.2261366844177246
Batch 34/64 loss: -0.2206869125366211
Batch 35/64 loss: 0.18935728073120117
Batch 36/64 loss: -0.32974720001220703
Batch 37/64 loss: -0.010142326354980469
Batch 38/64 loss: -0.3868865966796875
Batch 39/64 loss: -0.2266254425048828
Batch 40/64 loss: -0.34683847427368164
Batch 41/64 loss: 0.7290892601013184
Batch 42/64 loss: -0.23012256622314453
Batch 43/64 loss: -0.3161354064941406
Batch 44/64 loss: -0.19837188720703125
Batch 45/64 loss: 0.17193937301635742
Batch 46/64 loss: 0.7976298332214355
Batch 47/64 loss: -0.26904821395874023
Batch 48/64 loss: -0.23211669921875
Batch 49/64 loss: -0.18400955200195312
Batch 50/64 loss: 0.9999504089355469
Batch 51/64 loss: 0.796760082244873
Batch 52/64 loss: -0.22953319549560547
Batch 53/64 loss: -0.21530389785766602
Batch 54/64 loss: -0.23790884017944336
Batch 55/64 loss: -0.1559138298034668
Batch 56/64 loss: -0.2531256675720215
Batch 57/64 loss: -0.053637027740478516
Batch 58/64 loss: 0.003693103790283203
Batch 59/64 loss: -0.11200380325317383
Batch 60/64 loss: -0.3551301956176758
Batch 61/64 loss: -0.023885250091552734
Batch 62/64 loss: -0.2897615432739258
Batch 63/64 loss: -0.14825725555419922
Batch 64/64 loss: -3.51662540435791
Epoch 70  Train loss: -0.0730670592364143  Val loss: -0.47690284702786057
Saving best model, epoch: 70
Epoch 71
-------------------------------
Batch 1/64 loss: 0.3339996337890625
Batch 2/64 loss: -0.20558595657348633
Batch 3/64 loss: -0.29270219802856445
Batch 4/64 loss: -0.24434185028076172
Batch 5/64 loss: -0.2806859016418457
Batch 6/64 loss: -0.19512033462524414
Batch 7/64 loss: -0.316957950592041
Batch 8/64 loss: -0.14175033569335938
Batch 9/64 loss: -0.2998690605163574
Batch 10/64 loss: -0.31675195693969727
Batch 11/64 loss: 0.11910486221313477
Batch 12/64 loss: 0.5019588470458984
Batch 13/64 loss: -0.21875905990600586
Batch 14/64 loss: -0.31613588333129883
Batch 15/64 loss: 0.6469531059265137
Batch 16/64 loss: -0.18582773208618164
Batch 17/64 loss: -0.3697061538696289
Batch 18/64 loss: -0.1277918815612793
Batch 19/64 loss: -0.05822467803955078
Batch 20/64 loss: 0.04946088790893555
Batch 21/64 loss: -0.15587854385375977
Batch 22/64 loss: -0.23807811737060547
Batch 23/64 loss: -0.24077749252319336
Batch 24/64 loss: -0.2198486328125
Batch 25/64 loss: -0.02909708023071289
Batch 26/64 loss: -0.21880674362182617
Batch 27/64 loss: -0.09635210037231445
Batch 28/64 loss: 0.044246673583984375
Batch 29/64 loss: -0.09751319885253906
Batch 30/64 loss: -0.15433311462402344
Batch 31/64 loss: -0.17658042907714844
Batch 32/64 loss: 0.8007445335388184
Batch 33/64 loss: -0.12596940994262695
Batch 34/64 loss: -0.11294078826904297
Batch 35/64 loss: -0.1943650245666504
Batch 36/64 loss: -0.3525047302246094
Batch 37/64 loss: -0.09680557250976562
Batch 38/64 loss: -0.16193294525146484
Batch 39/64 loss: -0.2349257469177246
Batch 40/64 loss: -0.2683558464050293
Batch 41/64 loss: 0.4332404136657715
Batch 42/64 loss: 0.10082530975341797
Batch 43/64 loss: -0.35660219192504883
Batch 44/64 loss: -0.33373546600341797
Batch 45/64 loss: -0.15825223922729492
Batch 46/64 loss: -0.12027835845947266
Batch 47/64 loss: -0.22594451904296875
Batch 48/64 loss: -0.36025285720825195
Batch 49/64 loss: -0.38118457794189453
Batch 50/64 loss: -0.2595820426940918
Batch 51/64 loss: -0.338836669921875
Batch 52/64 loss: -0.45267152786254883
Batch 53/64 loss: -0.21500444412231445
Batch 54/64 loss: 2.41269588470459
Batch 55/64 loss: -0.29500722885131836
Batch 56/64 loss: -0.1698918342590332
Batch 57/64 loss: -0.2579946517944336
Batch 58/64 loss: -0.37970495223999023
Batch 59/64 loss: -0.37337493896484375
Batch 60/64 loss: 0.07820653915405273
Batch 61/64 loss: -0.17053985595703125
Batch 62/64 loss: -0.24019432067871094
Batch 63/64 loss: -0.22258949279785156
Batch 64/64 loss: -3.82979679107666
Epoch 71  Train loss: -0.1475737964405733  Val loss: -0.26241811116536456
Epoch 72
-------------------------------
Batch 1/64 loss: -0.3275899887084961
Batch 2/64 loss: -0.18706703186035156
Batch 3/64 loss: 0.5365839004516602
Batch 4/64 loss: -0.2874155044555664
Batch 5/64 loss: -0.21709728240966797
Batch 6/64 loss: -0.2475285530090332
Batch 7/64 loss: -0.16663789749145508
Batch 8/64 loss: -0.04362773895263672
Batch 9/64 loss: -0.2638707160949707
Batch 10/64 loss: -0.34087085723876953
Batch 11/64 loss: 0.3494434356689453
Batch 12/64 loss: -0.08929872512817383
Batch 13/64 loss: 0.01862192153930664
Batch 14/64 loss: -0.05857372283935547
Batch 15/64 loss: 0.29018259048461914
Batch 16/64 loss: -0.15490007400512695
Batch 17/64 loss: -0.2475733757019043
Batch 18/64 loss: -0.07974958419799805
Batch 19/64 loss: -0.08765459060668945
Batch 20/64 loss: -0.03528738021850586
Batch 21/64 loss: -0.2156510353088379
Batch 22/64 loss: 0.8709702491760254
Batch 23/64 loss: -0.2796630859375
Batch 24/64 loss: -0.24046754837036133
Batch 25/64 loss: -0.1492776870727539
Batch 26/64 loss: -0.3070068359375
Batch 27/64 loss: -0.19801092147827148
Batch 28/64 loss: 0.42957448959350586
Batch 29/64 loss: -0.23758316040039062
Batch 30/64 loss: -0.31157970428466797
Batch 31/64 loss: -0.23558759689331055
Batch 32/64 loss: 0.01622152328491211
Batch 33/64 loss: -0.11167001724243164
Batch 34/64 loss: -0.17920684814453125
Batch 35/64 loss: -0.2778744697570801
Batch 36/64 loss: 0.19725513458251953
Batch 37/64 loss: -0.22073745727539062
Batch 38/64 loss: -0.14803028106689453
Batch 39/64 loss: 1.2030787467956543
Batch 40/64 loss: -0.23046636581420898
Batch 41/64 loss: -0.17069578170776367
Batch 42/64 loss: -0.13399362564086914
Batch 43/64 loss: -0.3289008140563965
Batch 44/64 loss: -0.267214298248291
Batch 45/64 loss: 0.4380354881286621
Batch 46/64 loss: -0.12095499038696289
Batch 47/64 loss: -0.12289667129516602
Batch 48/64 loss: 0.12222862243652344
Batch 49/64 loss: -0.05525970458984375
Batch 50/64 loss: -0.041960716247558594
Batch 51/64 loss: 0.0038003921508789062
Batch 52/64 loss: -0.02638864517211914
Batch 53/64 loss: 0.03662729263305664
Batch 54/64 loss: -0.1590876579284668
Batch 55/64 loss: -0.22817134857177734
Batch 56/64 loss: -0.2449502944946289
Batch 57/64 loss: -0.1894068717956543
Batch 58/64 loss: 1.4056735038757324
Batch 59/64 loss: -0.07122135162353516
Batch 60/64 loss: -0.1310863494873047
Batch 61/64 loss: 0.7180604934692383
Batch 62/64 loss: -0.3095059394836426
Batch 63/64 loss: -0.07387638092041016
Batch 64/64 loss: -3.121004104614258
Epoch 72  Train loss: -0.0714905533136106  Val loss: -0.3690728256382893
Epoch 73
-------------------------------
Batch 1/64 loss: -0.2066507339477539
Batch 2/64 loss: -0.17815113067626953
Batch 3/64 loss: -0.2644171714782715
Batch 4/64 loss: -0.30556297302246094
Batch 5/64 loss: -0.29966163635253906
Batch 6/64 loss: -0.16008377075195312
Batch 7/64 loss: -0.23553991317749023
Batch 8/64 loss: -0.172271728515625
Batch 9/64 loss: 0.2864656448364258
Batch 10/64 loss: -0.3109707832336426
Batch 11/64 loss: -0.17701053619384766
Batch 12/64 loss: -0.21933555603027344
Batch 13/64 loss: -0.24985885620117188
Batch 14/64 loss: -0.12420463562011719
Batch 15/64 loss: -0.24890995025634766
Batch 16/64 loss: -0.2993588447570801
Batch 17/64 loss: -0.17464590072631836
Batch 18/64 loss: -0.2850813865661621
Batch 19/64 loss: 0.11468267440795898
Batch 20/64 loss: -0.23407411575317383
Batch 21/64 loss: -0.18867206573486328
Batch 22/64 loss: -0.09655523300170898
Batch 23/64 loss: 0.10527849197387695
Batch 24/64 loss: -0.04184436798095703
Batch 25/64 loss: -0.20395755767822266
Batch 26/64 loss: 0.08001565933227539
Batch 27/64 loss: 0.966041088104248
Batch 28/64 loss: -0.033658504486083984
Batch 29/64 loss: -0.1617279052734375
Batch 30/64 loss: 0.09177732467651367
Batch 31/64 loss: 0.027634620666503906
Batch 32/64 loss: 0.8865752220153809
Batch 33/64 loss: -0.11236572265625
Batch 34/64 loss: 0.1521286964416504
Batch 35/64 loss: -0.07007980346679688
Batch 36/64 loss: -0.014626502990722656
Batch 37/64 loss: 0.09717512130737305
Batch 38/64 loss: 0.3498573303222656
Batch 39/64 loss: -0.1037130355834961
Batch 40/64 loss: 0.9710865020751953
Batch 41/64 loss: -0.1873311996459961
Batch 42/64 loss: 1.3526058197021484
Batch 43/64 loss: -0.2482624053955078
Batch 44/64 loss: -0.30014657974243164
Batch 45/64 loss: 0.0937185287475586
Batch 46/64 loss: -0.03385305404663086
Batch 47/64 loss: 0.35729312896728516
Batch 48/64 loss: 0.21234703063964844
Batch 49/64 loss: -0.2604069709777832
Batch 50/64 loss: -0.04909229278564453
Batch 51/64 loss: -0.2635021209716797
Batch 52/64 loss: 0.4171457290649414
Batch 53/64 loss: -0.08237648010253906
Batch 54/64 loss: -0.22571372985839844
Batch 55/64 loss: -0.24997901916503906
Batch 56/64 loss: 0.3416309356689453
Batch 57/64 loss: -0.10241556167602539
Batch 58/64 loss: -0.2881946563720703
Batch 59/64 loss: 0.4537992477416992
Batch 60/64 loss: 0.1696004867553711
Batch 61/64 loss: 0.06837749481201172
Batch 62/64 loss: 0.5470829010009766
Batch 63/64 loss: 0.013608455657958984
Batch 64/64 loss: -3.6388211250305176
Epoch 73  Train loss: -0.031960032967960135  Val loss: -0.27397482173959004
Epoch 74
-------------------------------
Batch 1/64 loss: 0.6811285018920898
Batch 2/64 loss: -0.23029661178588867
Batch 3/64 loss: 0.00011157989501953125
Batch 4/64 loss: 0.2990093231201172
Batch 5/64 loss: -0.27017784118652344
Batch 6/64 loss: -0.10733366012573242
Batch 7/64 loss: 0.10730361938476562
Batch 8/64 loss: 0.37125635147094727
Batch 9/64 loss: -0.21019649505615234
Batch 10/64 loss: -0.3317580223083496
Batch 11/64 loss: -0.20276832580566406
Batch 12/64 loss: -0.23364830017089844
Batch 13/64 loss: -0.13907861709594727
Batch 14/64 loss: -0.13471221923828125
Batch 15/64 loss: -0.11533355712890625
Batch 16/64 loss: 0.7974176406860352
Batch 17/64 loss: 0.29436445236206055
Batch 18/64 loss: -0.18606996536254883
Batch 19/64 loss: 0.20862817764282227
Batch 20/64 loss: -0.1809372901916504
Batch 21/64 loss: -0.11300134658813477
Batch 22/64 loss: -0.18424606323242188
Batch 23/64 loss: -0.04616689682006836
Batch 24/64 loss: 0.03598308563232422
Batch 25/64 loss: -0.26880979537963867
Batch 26/64 loss: -0.02357959747314453
Batch 27/64 loss: 0.09194231033325195
Batch 28/64 loss: 0.13780927658081055
Batch 29/64 loss: -0.1036839485168457
Batch 30/64 loss: -0.08324623107910156
Batch 31/64 loss: -0.19128942489624023
Batch 32/64 loss: -0.2513728141784668
Batch 33/64 loss: -0.03031635284423828
Batch 34/64 loss: -0.060396671295166016
Batch 35/64 loss: 0.015864849090576172
Batch 36/64 loss: 0.1668257713317871
Batch 37/64 loss: 0.544278621673584
Batch 38/64 loss: -0.028419971466064453
Batch 39/64 loss: -0.28160953521728516
Batch 40/64 loss: 0.01908588409423828
Batch 41/64 loss: -0.20342540740966797
Batch 42/64 loss: -0.22744178771972656
Batch 43/64 loss: -0.1562042236328125
Batch 44/64 loss: 0.008684635162353516
Batch 45/64 loss: 0.23274803161621094
Batch 46/64 loss: 0.013337135314941406
Batch 47/64 loss: -0.0567317008972168
Batch 48/64 loss: -0.13597583770751953
Batch 49/64 loss: -0.0786600112915039
Batch 50/64 loss: -0.25617218017578125
Batch 51/64 loss: 0.31095075607299805
Batch 52/64 loss: 0.15655851364135742
Batch 53/64 loss: 0.1889190673828125
Batch 54/64 loss: 1.6493382453918457
Batch 55/64 loss: -0.08239316940307617
Batch 56/64 loss: 0.30345964431762695
Batch 57/64 loss: -0.13479232788085938
Batch 58/64 loss: -0.10470199584960938
Batch 59/64 loss: 0.24751996994018555
Batch 60/64 loss: 0.28949928283691406
Batch 61/64 loss: -0.2417912483215332
Batch 62/64 loss: 0.005066394805908203
Batch 63/64 loss: 0.04763460159301758
Batch 64/64 loss: -2.183979034423828
Epoch 74  Train loss: -0.00156859603582644  Val loss: -0.2544355228594488
Epoch 75
-------------------------------
Batch 1/64 loss: 0.15158700942993164
Batch 2/64 loss: -0.15111970901489258
Batch 3/64 loss: -0.07494831085205078
Batch 4/64 loss: 0.6889452934265137
Batch 5/64 loss: -0.01390695571899414
Batch 6/64 loss: -0.0037250518798828125
Batch 7/64 loss: -0.11573076248168945
Batch 8/64 loss: -0.0563969612121582
Batch 9/64 loss: -0.10657691955566406
Batch 10/64 loss: -0.20943689346313477
Batch 11/64 loss: -0.21429920196533203
Batch 12/64 loss: 0.1542673110961914
Batch 13/64 loss: 0.18412399291992188
Batch 14/64 loss: 0.2699899673461914
Batch 15/64 loss: -0.005708217620849609
Batch 16/64 loss: -0.10276985168457031
Batch 17/64 loss: -0.3009676933288574
Batch 18/64 loss: 0.3746376037597656
Batch 19/64 loss: -0.10693836212158203
Batch 20/64 loss: -0.2608909606933594
Batch 21/64 loss: -0.1720438003540039
Batch 22/64 loss: -0.10949039459228516
Batch 23/64 loss: -0.07178258895874023
Batch 24/64 loss: -0.2229900360107422
Batch 25/64 loss: -0.0039615631103515625
Batch 26/64 loss: 0.36609411239624023
Batch 27/64 loss: -0.1507701873779297
Batch 28/64 loss: 0.13837289810180664
Batch 29/64 loss: 1.3652839660644531
Batch 30/64 loss: -0.0473175048828125
Batch 31/64 loss: -0.21391725540161133
Batch 32/64 loss: -0.16100788116455078
Batch 33/64 loss: 0.3679642677307129
Batch 34/64 loss: -0.16138839721679688
Batch 35/64 loss: -0.3004183769226074
Batch 36/64 loss: -0.20014381408691406
Batch 37/64 loss: 0.5143570899963379
Batch 38/64 loss: -0.2147235870361328
Batch 39/64 loss: 0.13913965225219727
Batch 40/64 loss: 0.5458717346191406
Batch 41/64 loss: -0.21654272079467773
Batch 42/64 loss: -0.17397832870483398
Batch 43/64 loss: 0.23461008071899414
Batch 44/64 loss: -0.1591801643371582
Batch 45/64 loss: -0.16136932373046875
Batch 46/64 loss: 0.019992828369140625
Batch 47/64 loss: -0.059535980224609375
Batch 48/64 loss: -0.16288137435913086
Batch 49/64 loss: 0.27419614791870117
Batch 50/64 loss: -0.1513690948486328
Batch 51/64 loss: -0.0021462440490722656
Batch 52/64 loss: 1.7161812782287598
Batch 53/64 loss: 0.17032241821289062
Batch 54/64 loss: 0.08397722244262695
Batch 55/64 loss: 0.030830860137939453
Batch 56/64 loss: 0.023822784423828125
Batch 57/64 loss: -0.1813678741455078
Batch 58/64 loss: -0.34446287155151367
Batch 59/64 loss: -0.0912942886352539
Batch 60/64 loss: 0.08128881454467773
Batch 61/64 loss: 0.010640144348144531
Batch 62/64 loss: 1.087022304534912
Batch 63/64 loss: 0.3869943618774414
Batch 64/64 loss: -3.548774242401123
Epoch 75  Train loss: 0.019787199356976676  Val loss: -0.07041062030595602
Epoch 76
-------------------------------
Batch 1/64 loss: 0.12165021896362305
Batch 2/64 loss: 1.4421439170837402
Batch 3/64 loss: 0.1565690040588379
Batch 4/64 loss: 0.4866204261779785
Batch 5/64 loss: 0.09789085388183594
Batch 6/64 loss: -0.026069164276123047
Batch 7/64 loss: 0.007950305938720703
Batch 8/64 loss: 1.3733506202697754
Batch 9/64 loss: -0.13180112838745117
Batch 10/64 loss: -0.19588613510131836
Batch 11/64 loss: -0.05109214782714844
Batch 12/64 loss: 0.11793756484985352
Batch 13/64 loss: 0.32090091705322266
Batch 14/64 loss: -0.06253767013549805
Batch 15/64 loss: -0.06760358810424805
Batch 16/64 loss: 0.1525130271911621
Batch 17/64 loss: 0.3469371795654297
Batch 18/64 loss: 0.1426715850830078
Batch 19/64 loss: 0.016998291015625
Batch 20/64 loss: 0.01732492446899414
Batch 21/64 loss: -0.15389680862426758
Batch 22/64 loss: 0.052764892578125
Batch 23/64 loss: -0.0028247833251953125
Batch 24/64 loss: 0.11501073837280273
Batch 25/64 loss: -0.14235210418701172
Batch 26/64 loss: -0.15619325637817383
Batch 27/64 loss: -0.035846710205078125
Batch 28/64 loss: 1.2620344161987305
Batch 29/64 loss: 0.5306439399719238
Batch 30/64 loss: -0.189056396484375
Batch 31/64 loss: 2.1560873985290527
Batch 32/64 loss: 0.0007090568542480469
Batch 33/64 loss: 0.1558527946472168
Batch 34/64 loss: -0.03368806838989258
Batch 35/64 loss: -0.029997825622558594
Batch 36/64 loss: 0.09313488006591797
Batch 37/64 loss: -0.07653236389160156
Batch 38/64 loss: 0.013792991638183594
Batch 39/64 loss: -0.17442941665649414
Batch 40/64 loss: -0.11800336837768555
Batch 41/64 loss: -0.0350794792175293
Batch 42/64 loss: -0.10690498352050781
Batch 43/64 loss: -0.12130355834960938
Batch 44/64 loss: 0.40019655227661133
Batch 45/64 loss: -0.30544185638427734
Batch 46/64 loss: -0.17409467697143555
Batch 47/64 loss: 0.11336278915405273
Batch 48/64 loss: 0.4674258232116699
Batch 49/64 loss: -0.22426223754882812
Batch 50/64 loss: -0.1725473403930664
Batch 51/64 loss: -0.33104944229125977
Batch 52/64 loss: 0.36060142517089844
Batch 53/64 loss: -0.035747528076171875
Batch 54/64 loss: -0.12029409408569336
Batch 55/64 loss: -0.21799993515014648
Batch 56/64 loss: -0.32434797286987305
Batch 57/64 loss: -0.0715179443359375
Batch 58/64 loss: -0.27913475036621094
Batch 59/64 loss: -0.10959291458129883
Batch 60/64 loss: 0.09481096267700195
Batch 61/64 loss: -0.050188541412353516
Batch 62/64 loss: -0.1535191535949707
Batch 63/64 loss: -0.17353343963623047
Batch 64/64 loss: -3.7213339805603027
Epoch 76  Train loss: 0.04976496041989794  Val loss: -0.19264820269292982
Epoch 77
-------------------------------
Batch 1/64 loss: 0.23070096969604492
Batch 2/64 loss: -0.22803401947021484
Batch 3/64 loss: -0.2599935531616211
Batch 4/64 loss: -0.1646742820739746
Batch 5/64 loss: -0.26981639862060547
Batch 6/64 loss: -0.09003829956054688
Batch 7/64 loss: -0.2606697082519531
Batch 8/64 loss: -0.3535184860229492
Batch 9/64 loss: -0.13194513320922852
Batch 10/64 loss: -0.18725872039794922
Batch 11/64 loss: -0.21070289611816406
Batch 12/64 loss: -0.15652179718017578
Batch 13/64 loss: -0.20824623107910156
Batch 14/64 loss: -0.1674518585205078
Batch 15/64 loss: -0.1402454376220703
Batch 16/64 loss: -0.11846303939819336
Batch 17/64 loss: -0.3444709777832031
Batch 18/64 loss: -0.24672842025756836
Batch 19/64 loss: 0.04667997360229492
Batch 20/64 loss: 0.02021503448486328
Batch 21/64 loss: -0.15605640411376953
Batch 22/64 loss: 0.08369874954223633
Batch 23/64 loss: 0.33009958267211914
Batch 24/64 loss: -0.05045604705810547
Batch 25/64 loss: 0.061754703521728516
Batch 26/64 loss: 0.1197967529296875
Batch 27/64 loss: -0.1741337776184082
Batch 28/64 loss: -0.037813663482666016
Batch 29/64 loss: 1.4130182266235352
Batch 30/64 loss: -0.0034842491149902344
Batch 31/64 loss: -0.1338977813720703
Batch 32/64 loss: -0.14611434936523438
Batch 33/64 loss: -0.39884138107299805
Batch 34/64 loss: -0.12103128433227539
Batch 35/64 loss: 0.10845041275024414
Batch 36/64 loss: -0.2688765525817871
Batch 37/64 loss: 1.786595344543457
Batch 38/64 loss: -0.2831578254699707
Batch 39/64 loss: -0.12408256530761719
Batch 40/64 loss: -0.1753549575805664
Batch 41/64 loss: -0.10902881622314453
Batch 42/64 loss: 0.0489964485168457
Batch 43/64 loss: -0.1496272087097168
Batch 44/64 loss: -0.24111461639404297
Batch 45/64 loss: -0.2190861701965332
Batch 46/64 loss: -0.2164597511291504
Batch 47/64 loss: -0.1591477394104004
Batch 48/64 loss: 0.7006993293762207
Batch 49/64 loss: 0.002849102020263672
Batch 50/64 loss: -0.21184778213500977
Batch 51/64 loss: -0.32906627655029297
Batch 52/64 loss: -0.08826589584350586
Batch 53/64 loss: -0.22497034072875977
Batch 54/64 loss: -0.16282987594604492
Batch 55/64 loss: 0.2635788917541504
Batch 56/64 loss: -0.17509174346923828
Batch 57/64 loss: -0.21593904495239258
Batch 58/64 loss: 0.15864038467407227
Batch 59/64 loss: -0.17737531661987305
Batch 60/64 loss: 0.44818735122680664
Batch 61/64 loss: -0.057830810546875
Batch 62/64 loss: -0.19736862182617188
Batch 63/64 loss: 0.3926577568054199
Batch 64/64 loss: -3.655170440673828
Epoch 77  Train loss: -0.07955904193952971  Val loss: -0.2033699271605187
Epoch 78
-------------------------------
Batch 1/64 loss: -0.10489177703857422
Batch 2/64 loss: -0.050096988677978516
Batch 3/64 loss: -0.12476396560668945
Batch 4/64 loss: 0.02904033660888672
Batch 5/64 loss: -0.256319522857666
Batch 6/64 loss: -0.23803234100341797
Batch 7/64 loss: -0.06571054458618164
Batch 8/64 loss: -0.09559392929077148
Batch 9/64 loss: 0.2619190216064453
Batch 10/64 loss: -0.024516582489013672
Batch 11/64 loss: -0.1595597267150879
Batch 12/64 loss: -0.2602858543395996
Batch 13/64 loss: -0.06944704055786133
Batch 14/64 loss: -0.004385471343994141
Batch 15/64 loss: -0.3914761543273926
Batch 16/64 loss: 0.8911309242248535
Batch 17/64 loss: -0.3069620132446289
Batch 18/64 loss: -0.07478141784667969
Batch 19/64 loss: -0.030182361602783203
Batch 20/64 loss: -0.12766122817993164
Batch 21/64 loss: -0.16020917892456055
Batch 22/64 loss: -0.2058420181274414
Batch 23/64 loss: -0.1878514289855957
Batch 24/64 loss: -0.15164995193481445
Batch 25/64 loss: -0.15221166610717773
Batch 26/64 loss: -0.2744283676147461
Batch 27/64 loss: 0.12641143798828125
Batch 28/64 loss: -0.3958134651184082
Batch 29/64 loss: -0.31250667572021484
Batch 30/64 loss: -0.10703659057617188
Batch 31/64 loss: -0.23665237426757812
Batch 32/64 loss: 0.23950958251953125
Batch 33/64 loss: -0.13570165634155273
Batch 34/64 loss: -0.2429032325744629
Batch 35/64 loss: -0.2601795196533203
Batch 36/64 loss: -0.17818546295166016
Batch 37/64 loss: 0.09797954559326172
Batch 38/64 loss: -0.33132123947143555
Batch 39/64 loss: -0.17644119262695312
Batch 40/64 loss: 0.0054759979248046875
Batch 41/64 loss: 0.02460956573486328
Batch 42/64 loss: 0.23805522918701172
Batch 43/64 loss: -0.3090701103210449
Batch 44/64 loss: -0.0908350944519043
Batch 45/64 loss: -0.05143308639526367
Batch 46/64 loss: -0.3221168518066406
Batch 47/64 loss: -0.11146068572998047
Batch 48/64 loss: -0.2136216163635254
Batch 49/64 loss: -0.2860121726989746
Batch 50/64 loss: 0.3705625534057617
Batch 51/64 loss: 0.6961398124694824
Batch 52/64 loss: -0.11603927612304688
Batch 53/64 loss: -0.15561628341674805
Batch 54/64 loss: -0.20079851150512695
Batch 55/64 loss: -0.07032155990600586
Batch 56/64 loss: -0.1762995719909668
Batch 57/64 loss: -0.15778827667236328
Batch 58/64 loss: 1.139986515045166
Batch 59/64 loss: -0.2832913398742676
Batch 60/64 loss: -0.26764678955078125
Batch 61/64 loss: 0.020030498504638672
Batch 62/64 loss: -0.1414794921875
Batch 63/64 loss: -0.1266922950744629
Batch 64/64 loss: -3.8099961280822754
Epoch 78  Train loss: -0.12063952988269282  Val loss: -0.24128151595387673
Epoch 79
-------------------------------
Batch 1/64 loss: -0.3176612854003906
Batch 2/64 loss: 0.360044002532959
Batch 3/64 loss: -0.23856687545776367
Batch 4/64 loss: -0.3530449867248535
Batch 5/64 loss: -0.2704195976257324
Batch 6/64 loss: -0.11344718933105469
Batch 7/64 loss: -0.2840843200683594
Batch 8/64 loss: -0.2669644355773926
Batch 9/64 loss: -0.22130823135375977
Batch 10/64 loss: -0.059081077575683594
Batch 11/64 loss: -0.20566225051879883
Batch 12/64 loss: -0.21443891525268555
Batch 13/64 loss: -0.17327213287353516
Batch 14/64 loss: -0.14424753189086914
Batch 15/64 loss: -0.16568422317504883
Batch 16/64 loss: -0.05605888366699219
Batch 17/64 loss: 0.9022684097290039
Batch 18/64 loss: 0.28995704650878906
Batch 19/64 loss: -0.18096017837524414
Batch 20/64 loss: -0.33580684661865234
Batch 21/64 loss: -0.32497644424438477
Batch 22/64 loss: -0.2515387535095215
Batch 23/64 loss: -0.1524982452392578
Batch 24/64 loss: -0.05399322509765625
Batch 25/64 loss: -0.09878396987915039
Batch 26/64 loss: -0.14678335189819336
Batch 27/64 loss: 0.42667484283447266
Batch 28/64 loss: -0.220916748046875
Batch 29/64 loss: -0.1948986053466797
Batch 30/64 loss: -0.2993659973144531
Batch 31/64 loss: -0.20353937149047852
Batch 32/64 loss: -0.12854671478271484
Batch 33/64 loss: 0.0255126953125
Batch 34/64 loss: -0.3476080894470215
Batch 35/64 loss: -0.20576095581054688
Batch 36/64 loss: -0.23148632049560547
Batch 37/64 loss: -0.29065418243408203
Batch 38/64 loss: -0.19768857955932617
Batch 39/64 loss: 0.2903299331665039
Batch 40/64 loss: -0.3019752502441406
Batch 41/64 loss: -0.15038824081420898
Batch 42/64 loss: -0.21376323699951172
Batch 43/64 loss: -0.1264195442199707
Batch 44/64 loss: -0.2030777931213379
Batch 45/64 loss: -0.1250753402709961
Batch 46/64 loss: -0.035860538482666016
Batch 47/64 loss: -0.30010271072387695
Batch 48/64 loss: -0.12367486953735352
Batch 49/64 loss: 0.7417445182800293
Batch 50/64 loss: -0.3127312660217285
Batch 51/64 loss: 0.5130462646484375
Batch 52/64 loss: -0.008118629455566406
Batch 53/64 loss: -0.040268898010253906
Batch 54/64 loss: 1.1475000381469727
Batch 55/64 loss: 0.22690629959106445
Batch 56/64 loss: 0.5677661895751953
Batch 57/64 loss: 1.473757266998291
Batch 58/64 loss: 1.130542278289795
Batch 59/64 loss: 1.0062618255615234
Batch 60/64 loss: 1.064375877380371
Batch 61/64 loss: 1.178849697113037
Batch 62/64 loss: 0.9454045295715332
Batch 63/64 loss: 1.3395018577575684
Batch 64/64 loss: -2.166679859161377
Epoch 79  Train loss: 0.04885064854341395  Val loss: 2.412406842733167
Epoch 80
-------------------------------
Batch 1/64 loss: 0.7389483451843262
Batch 2/64 loss: 1.1896390914916992
Batch 3/64 loss: 0.6399402618408203
Batch 4/64 loss: 0.6012721061706543
Batch 5/64 loss: 1.3262715339660645
Batch 6/64 loss: 0.7841544151306152
Batch 7/64 loss: 0.483583927154541
Batch 8/64 loss: 0.5244569778442383
Batch 9/64 loss: 0.500370979309082
Batch 10/64 loss: 0.5387158393859863
Batch 11/64 loss: 0.5292987823486328
Batch 12/64 loss: 0.3367652893066406
Batch 13/64 loss: 0.4435391426086426
Batch 14/64 loss: 0.5289955139160156
Batch 15/64 loss: 0.6340951919555664
Batch 16/64 loss: 0.633880615234375
Batch 17/64 loss: 0.6105751991271973
Batch 18/64 loss: 0.2830510139465332
Batch 19/64 loss: 0.4957089424133301
Batch 20/64 loss: 0.1556720733642578
Batch 21/64 loss: 0.40645408630371094
Batch 22/64 loss: 0.2161259651184082
Batch 23/64 loss: 0.28079748153686523
Batch 24/64 loss: 0.3518400192260742
Batch 25/64 loss: 0.6347389221191406
Batch 26/64 loss: 0.15685749053955078
Batch 27/64 loss: 0.26138734817504883
Batch 28/64 loss: 0.29473257064819336
Batch 29/64 loss: 0.24653863906860352
Batch 30/64 loss: 0.29411840438842773
Batch 31/64 loss: 0.21737241744995117
Batch 32/64 loss: -0.023933887481689453
Batch 33/64 loss: 0.1231837272644043
Batch 34/64 loss: 0.20476722717285156
Batch 35/64 loss: 1.3399381637573242
Batch 36/64 loss: 0.20134878158569336
Batch 37/64 loss: 0.019310951232910156
Batch 38/64 loss: 0.0487971305847168
Batch 39/64 loss: 0.32172346115112305
Batch 40/64 loss: 0.09247303009033203
Batch 41/64 loss: -0.00724029541015625
Batch 42/64 loss: 0.2431197166442871
Batch 43/64 loss: 0.19814586639404297
Batch 44/64 loss: -0.0011749267578125
Batch 45/64 loss: 0.01708984375
Batch 46/64 loss: 0.04094743728637695
Batch 47/64 loss: 0.037235260009765625
Batch 48/64 loss: -0.006150245666503906
Batch 49/64 loss: 0.054108619689941406
Batch 50/64 loss: 0.021702289581298828
Batch 51/64 loss: 0.022792816162109375
Batch 52/64 loss: -0.06980609893798828
Batch 53/64 loss: -0.15723657608032227
Batch 54/64 loss: 1.2943720817565918
Batch 55/64 loss: -0.029917240142822266
Batch 56/64 loss: 0.07735347747802734
Batch 57/64 loss: -0.05049896240234375
Batch 58/64 loss: 0.007450580596923828
Batch 59/64 loss: 0.15328168869018555
Batch 60/64 loss: -0.009577751159667969
Batch 61/64 loss: 1.529006004333496
Batch 62/64 loss: -0.03145790100097656
Batch 63/64 loss: 0.19526910781860352
Batch 64/64 loss: -3.328855037689209
Epoch 80  Train loss: 0.2933283244862276  Val loss: -0.07611613584957581
Epoch 81
-------------------------------
Batch 1/64 loss: -0.07208633422851562
Batch 2/64 loss: 0.03674650192260742
Batch 3/64 loss: -0.22838926315307617
Batch 4/64 loss: -0.1012105941772461
Batch 5/64 loss: 0.9888372421264648
Batch 6/64 loss: -0.0631265640258789
Batch 7/64 loss: -0.006119728088378906
Batch 8/64 loss: 0.04587507247924805
Batch 9/64 loss: -0.0792388916015625
Batch 10/64 loss: -0.06235456466674805
Batch 11/64 loss: -0.024307727813720703
Batch 12/64 loss: 0.13264083862304688
Batch 13/64 loss: 0.05173444747924805
Batch 14/64 loss: 0.3775629997253418
Batch 15/64 loss: -0.03188180923461914
Batch 16/64 loss: -0.057500362396240234
Batch 17/64 loss: -0.07848739624023438
Batch 18/64 loss: 0.09009599685668945
Batch 19/64 loss: 0.07558345794677734
Batch 20/64 loss: -0.05655050277709961
Batch 21/64 loss: -0.1098785400390625
Batch 22/64 loss: 1.0769004821777344
Batch 23/64 loss: -0.10992956161499023
Batch 24/64 loss: -0.007089138031005859
Batch 25/64 loss: 0.14520692825317383
Batch 26/64 loss: -0.1773533821105957
Batch 27/64 loss: -0.07172966003417969
Batch 28/64 loss: -0.09030389785766602
Batch 29/64 loss: 0.2858128547668457
Batch 30/64 loss: 0.21715593338012695
Batch 31/64 loss: 0.21546316146850586
Batch 32/64 loss: -0.15383481979370117
Batch 33/64 loss: -0.08408641815185547
Batch 34/64 loss: -0.12389564514160156
Batch 35/64 loss: -0.04047870635986328
Batch 36/64 loss: -0.010135650634765625
Batch 37/64 loss: 2.411196231842041
Batch 38/64 loss: 0.1413254737854004
Batch 39/64 loss: 0.0273284912109375
Batch 40/64 loss: 0.03932809829711914
Batch 41/64 loss: 0.3767232894897461
Batch 42/64 loss: -0.08593893051147461
Batch 43/64 loss: 0.13152170181274414
Batch 44/64 loss: 0.8739495277404785
Batch 45/64 loss: -0.1108856201171875
Batch 46/64 loss: -0.011198043823242188
Batch 47/64 loss: -0.10914421081542969
Batch 48/64 loss: 0.07308292388916016
Batch 49/64 loss: -0.05943012237548828
Batch 50/64 loss: 0.11581945419311523
Batch 51/64 loss: 0.20391225814819336
Batch 52/64 loss: 0.26560115814208984
Batch 53/64 loss: 0.19864559173583984
Batch 54/64 loss: -0.19219303131103516
Batch 55/64 loss: -0.024363040924072266
Batch 56/64 loss: 0.21500539779663086
Batch 57/64 loss: -0.030262470245361328
Batch 58/64 loss: 0.2133007049560547
Batch 59/64 loss: -0.020994186401367188
Batch 60/64 loss: 0.4896697998046875
Batch 61/64 loss: -0.017833709716796875
Batch 62/64 loss: 0.11896276473999023
Batch 63/64 loss: -0.18351173400878906
Batch 64/64 loss: -3.4009385108947754
Epoch 81  Train loss: 0.06899702969719382  Val loss: -0.1412863977176627
Epoch 82
-------------------------------
Batch 1/64 loss: 0.26288604736328125
Batch 2/64 loss: -0.04770088195800781
Batch 3/64 loss: 0.01875019073486328
Batch 4/64 loss: 0.06439828872680664
Batch 5/64 loss: -0.1599588394165039
Batch 6/64 loss: -0.07255172729492188
Batch 7/64 loss: -0.10193204879760742
Batch 8/64 loss: -0.07255125045776367
Batch 9/64 loss: -0.0627145767211914
Batch 10/64 loss: -0.28014659881591797
Batch 11/64 loss: -0.3036351203918457
Batch 12/64 loss: -0.21210336685180664
Batch 13/64 loss: 0.6813011169433594
Batch 14/64 loss: 0.30521249771118164
Batch 15/64 loss: -0.021858692169189453
Batch 16/64 loss: 1.100226879119873
Batch 17/64 loss: -0.10822916030883789
Batch 18/64 loss: -0.058980464935302734
Batch 19/64 loss: 0.03702068328857422
Batch 20/64 loss: -0.29506349563598633
Batch 21/64 loss: 0.1926426887512207
Batch 22/64 loss: 0.08706903457641602
Batch 23/64 loss: 0.06310462951660156
Batch 24/64 loss: 0.28372859954833984
Batch 25/64 loss: 0.10021543502807617
Batch 26/64 loss: 0.34776782989501953
Batch 27/64 loss: -0.15792226791381836
Batch 28/64 loss: 0.020046710968017578
Batch 29/64 loss: -0.20989084243774414
Batch 30/64 loss: -0.039403438568115234
Batch 31/64 loss: -0.04142332077026367
Batch 32/64 loss: -0.1722860336303711
Batch 33/64 loss: -0.08629274368286133
Batch 34/64 loss: -0.05670738220214844
Batch 35/64 loss: 0.007161140441894531
Batch 36/64 loss: -0.14820575714111328
Batch 37/64 loss: -0.11308622360229492
Batch 38/64 loss: -0.0337986946105957
Batch 39/64 loss: -0.24988555908203125
Batch 40/64 loss: -0.25895071029663086
Batch 41/64 loss: -0.21985435485839844
Batch 42/64 loss: -0.16202306747436523
Batch 43/64 loss: 1.2608275413513184
Batch 44/64 loss: -0.30922508239746094
Batch 45/64 loss: -0.2795729637145996
Batch 46/64 loss: -0.24092912673950195
Batch 47/64 loss: -0.21058177947998047
Batch 48/64 loss: -0.2572946548461914
Batch 49/64 loss: -0.314608097076416
Batch 50/64 loss: -0.13126659393310547
Batch 51/64 loss: -0.09884214401245117
Batch 52/64 loss: 1.0823793411254883
Batch 53/64 loss: -0.012294769287109375
Batch 54/64 loss: 0.33632421493530273
Batch 55/64 loss: -0.18484878540039062
Batch 56/64 loss: -0.26455068588256836
Batch 57/64 loss: 0.12989568710327148
Batch 58/64 loss: -0.22745609283447266
Batch 59/64 loss: -0.1247262954711914
Batch 60/64 loss: -0.17687177658081055
Batch 61/64 loss: -0.048517704010009766
Batch 62/64 loss: 0.9972171783447266
Batch 63/64 loss: -0.0543217658996582
Batch 64/64 loss: -3.338472843170166
Epoch 82  Train loss: -0.02837245230581246  Val loss: -0.3762397372845522
Epoch 83
-------------------------------
Batch 1/64 loss: 0.21436548233032227
Batch 2/64 loss: 0.08352947235107422
Batch 3/64 loss: 0.4229731559753418
Batch 4/64 loss: -0.14387989044189453
Batch 5/64 loss: -0.25167417526245117
Batch 6/64 loss: -0.20464611053466797
Batch 7/64 loss: -0.31871604919433594
Batch 8/64 loss: 0.29091501235961914
Batch 9/64 loss: -0.21962833404541016
Batch 10/64 loss: -0.1433401107788086
Batch 11/64 loss: -0.007160186767578125
Batch 12/64 loss: 0.06733846664428711
Batch 13/64 loss: -0.28421831130981445
Batch 14/64 loss: 1.39927339553833
Batch 15/64 loss: -0.1790008544921875
Batch 16/64 loss: -0.2866368293762207
Batch 17/64 loss: -0.011700630187988281
Batch 18/64 loss: -0.22069692611694336
Batch 19/64 loss: -0.11332893371582031
Batch 20/64 loss: -0.14159107208251953
Batch 21/64 loss: -0.2335348129272461
Batch 22/64 loss: -0.09621334075927734
Batch 23/64 loss: -0.13754034042358398
Batch 24/64 loss: -0.1757516860961914
Batch 25/64 loss: -0.23186683654785156
Batch 26/64 loss: -0.23099136352539062
Batch 27/64 loss: 0.8501496315002441
Batch 28/64 loss: -0.03307390213012695
Batch 29/64 loss: 0.5683951377868652
Batch 30/64 loss: -0.21457147598266602
Batch 31/64 loss: -0.02475738525390625
Batch 32/64 loss: -0.18729066848754883
Batch 33/64 loss: 0.42524242401123047
Batch 34/64 loss: 0.03997087478637695
Batch 35/64 loss: -0.1737232208251953
Batch 36/64 loss: -0.1898965835571289
Batch 37/64 loss: 0.023571491241455078
Batch 38/64 loss: 0.34417152404785156
Batch 39/64 loss: -0.011329174041748047
Batch 40/64 loss: -0.029059886932373047
Batch 41/64 loss: 0.19663286209106445
Batch 42/64 loss: -0.1205449104309082
Batch 43/64 loss: -0.07613134384155273
Batch 44/64 loss: -3.8623809814453125e-05
Batch 45/64 loss: 0.0914607048034668
Batch 46/64 loss: 0.021247386932373047
Batch 47/64 loss: -0.2723984718322754
Batch 48/64 loss: -0.2120819091796875
Batch 49/64 loss: -0.1285719871520996
Batch 50/64 loss: 0.09452676773071289
Batch 51/64 loss: 0.043277740478515625
Batch 52/64 loss: 1.0478053092956543
Batch 53/64 loss: 0.0839548110961914
Batch 54/64 loss: 0.04554462432861328
Batch 55/64 loss: -0.2565455436706543
Batch 56/64 loss: -0.2582826614379883
Batch 57/64 loss: -0.1757044792175293
Batch 58/64 loss: 0.07791328430175781
Batch 59/64 loss: 0.20918798446655273
Batch 60/64 loss: -0.31206750869750977
Batch 61/64 loss: -0.16912508010864258
Batch 62/64 loss: -0.11948919296264648
Batch 63/64 loss: -0.1964278221130371
Batch 64/64 loss: -3.7429914474487305
Epoch 83  Train loss: -0.04641607321944891  Val loss: -0.37951780758362863
Epoch 84
-------------------------------
Batch 1/64 loss: -0.20815324783325195
Batch 2/64 loss: -0.16430902481079102
Batch 3/64 loss: -0.058277130126953125
Batch 4/64 loss: -0.18707704544067383
Batch 5/64 loss: 0.17415475845336914
Batch 6/64 loss: -0.22841167449951172
Batch 7/64 loss: -0.33234214782714844
Batch 8/64 loss: -0.27973413467407227
Batch 9/64 loss: 0.4756298065185547
Batch 10/64 loss: -0.16185426712036133
Batch 11/64 loss: 0.2863602638244629
Batch 12/64 loss: -0.19415712356567383
Batch 13/64 loss: 0.005789756774902344
Batch 14/64 loss: -0.13748455047607422
Batch 15/64 loss: 0.118804931640625
Batch 16/64 loss: -0.09097862243652344
Batch 17/64 loss: -0.15631961822509766
Batch 18/64 loss: -0.2039804458618164
Batch 19/64 loss: 1.3775582313537598
Batch 20/64 loss: -0.2970919609069824
Batch 21/64 loss: -0.17987823486328125
Batch 22/64 loss: -0.04703378677368164
Batch 23/64 loss: 0.9193873405456543
Batch 24/64 loss: -0.25351715087890625
Batch 25/64 loss: 0.295133113861084
Batch 26/64 loss: -0.13620662689208984
Batch 27/64 loss: -0.18217802047729492
Batch 28/64 loss: -0.09582901000976562
Batch 29/64 loss: -0.3066291809082031
Batch 30/64 loss: 0.9479303359985352
Batch 31/64 loss: -0.0780940055847168
Batch 32/64 loss: 0.06540584564208984
Batch 33/64 loss: -0.12908458709716797
Batch 34/64 loss: -0.18587541580200195
Batch 35/64 loss: -0.1726078987121582
Batch 36/64 loss: -0.15753602981567383
Batch 37/64 loss: -0.33347177505493164
Batch 38/64 loss: -0.1699357032775879
Batch 39/64 loss: -0.08203315734863281
Batch 40/64 loss: -0.3016033172607422
Batch 41/64 loss: -0.07963991165161133
Batch 42/64 loss: -0.15448427200317383
Batch 43/64 loss: -0.05962371826171875
Batch 44/64 loss: -0.029650211334228516
Batch 45/64 loss: -0.07722902297973633
Batch 46/64 loss: -0.2639627456665039
Batch 47/64 loss: -0.01943683624267578
Batch 48/64 loss: -0.14751958847045898
Batch 49/64 loss: -0.08430957794189453
Batch 50/64 loss: 0.10696744918823242
Batch 51/64 loss: 0.3402519226074219
Batch 52/64 loss: -0.1546926498413086
Batch 53/64 loss: -0.13033485412597656
Batch 54/64 loss: 0.36939239501953125
Batch 55/64 loss: -0.1597304344177246
Batch 56/64 loss: -0.1784534454345703
Batch 57/64 loss: -0.1935896873474121
Batch 58/64 loss: -0.16055011749267578
Batch 59/64 loss: -0.2926492691040039
Batch 60/64 loss: -0.2362513542175293
Batch 61/64 loss: -0.0809621810913086
Batch 62/64 loss: -0.3154754638671875
Batch 63/64 loss: -0.2726883888244629
Batch 64/64 loss: -3.5573372840881348
Epoch 84  Train loss: -0.09079459508260092  Val loss: -0.3652916085679097
Epoch 85
-------------------------------
Batch 1/64 loss: -0.08254051208496094
Batch 2/64 loss: -0.08762168884277344
Batch 3/64 loss: 0.918663501739502
Batch 4/64 loss: -0.3378782272338867
Batch 5/64 loss: -0.30954742431640625
Batch 6/64 loss: -0.026161670684814453
Batch 7/64 loss: 0.5223960876464844
Batch 8/64 loss: 0.45969104766845703
Batch 9/64 loss: -0.1461195945739746
Batch 10/64 loss: -0.1665821075439453
Batch 11/64 loss: 0.22350597381591797
Batch 12/64 loss: -0.20326948165893555
Batch 13/64 loss: -0.15619373321533203
Batch 14/64 loss: -0.22370624542236328
Batch 15/64 loss: -0.08866643905639648
Batch 16/64 loss: -0.19918155670166016
Batch 17/64 loss: 0.6791677474975586
Batch 18/64 loss: -0.19355058670043945
Batch 19/64 loss: -0.07667016983032227
Batch 20/64 loss: -0.13144540786743164
Batch 21/64 loss: -0.07699823379516602
Batch 22/64 loss: 0.11802339553833008
Batch 23/64 loss: -0.20102167129516602
Batch 24/64 loss: -0.052958011627197266
Batch 25/64 loss: 0.021315574645996094
Batch 26/64 loss: -0.3022003173828125
Batch 27/64 loss: 0.10601472854614258
Batch 28/64 loss: -0.009981155395507812
Batch 29/64 loss: -0.26149511337280273
Batch 30/64 loss: -0.015270709991455078
Batch 31/64 loss: -0.3637361526489258
Batch 32/64 loss: 0.0650944709777832
Batch 33/64 loss: -0.10346412658691406
Batch 34/64 loss: -0.02194070816040039
Batch 35/64 loss: 0.05304288864135742
Batch 36/64 loss: -0.13344669342041016
Batch 37/64 loss: 1.3436861038208008
Batch 38/64 loss: -0.2787590026855469
Batch 39/64 loss: -0.19932079315185547
Batch 40/64 loss: -0.1538839340209961
Batch 41/64 loss: -0.09108877182006836
Batch 42/64 loss: 0.17005300521850586
Batch 43/64 loss: -0.023739337921142578
Batch 44/64 loss: -0.17110443115234375
Batch 45/64 loss: -0.0958261489868164
Batch 46/64 loss: -0.12305498123168945
Batch 47/64 loss: -0.22365713119506836
Batch 48/64 loss: -0.2253856658935547
Batch 49/64 loss: -0.20331192016601562
Batch 50/64 loss: -0.010771751403808594
Batch 51/64 loss: -0.3328256607055664
Batch 52/64 loss: 0.7672796249389648
Batch 53/64 loss: -0.038481712341308594
Batch 54/64 loss: 0.0532832145690918
Batch 55/64 loss: -0.1931161880493164
Batch 56/64 loss: -0.2405533790588379
Batch 57/64 loss: -0.3119072914123535
Batch 58/64 loss: -0.2427372932434082
Batch 59/64 loss: -0.15044212341308594
Batch 60/64 loss: 0.05591869354248047
Batch 61/64 loss: -0.11676168441772461
Batch 62/64 loss: -0.2925872802734375
Batch 63/64 loss: -0.2811455726623535
Batch 64/64 loss: -3.5834450721740723
Epoch 85  Train loss: -0.08004011827356675  Val loss: 0.021200003083219232
Epoch 86
-------------------------------
Batch 1/64 loss: -0.17054271697998047
Batch 2/64 loss: -0.2022719383239746
Batch 3/64 loss: -0.10525798797607422
Batch 4/64 loss: 0.021509647369384766
Batch 5/64 loss: -0.14250516891479492
Batch 6/64 loss: 0.015812397003173828
Batch 7/64 loss: 1.1741719245910645
Batch 8/64 loss: -0.1758098602294922
Batch 9/64 loss: -0.21831226348876953
Batch 10/64 loss: -0.23705720901489258
Batch 11/64 loss: -0.2166886329650879
Batch 12/64 loss: 0.23622798919677734
Batch 13/64 loss: -0.4040703773498535
Batch 14/64 loss: -0.3132510185241699
Batch 15/64 loss: -0.22082996368408203
Batch 16/64 loss: -0.1319289207458496
Batch 17/64 loss: -0.41701316833496094
Batch 18/64 loss: -0.43263912200927734
Batch 19/64 loss: 0.09690427780151367
Batch 20/64 loss: -0.26827287673950195
Batch 21/64 loss: -0.014179706573486328
Batch 22/64 loss: 0.016721725463867188
Batch 23/64 loss: -0.23374414443969727
Batch 24/64 loss: -0.0005049705505371094
Batch 25/64 loss: 0.22414207458496094
Batch 26/64 loss: 0.18582725524902344
Batch 27/64 loss: -0.2859487533569336
Batch 28/64 loss: -0.2495427131652832
Batch 29/64 loss: -0.2917613983154297
Batch 30/64 loss: 1.0390243530273438
Batch 31/64 loss: -0.06847190856933594
Batch 32/64 loss: -0.17540979385375977
Batch 33/64 loss: -0.18218231201171875
Batch 34/64 loss: -0.25160884857177734
Batch 35/64 loss: -0.1587986946105957
Batch 36/64 loss: -0.2290654182434082
Batch 37/64 loss: -0.1798858642578125
Batch 38/64 loss: 0.043944358825683594
Batch 39/64 loss: -0.1922459602355957
Batch 40/64 loss: -0.33290529251098633
Batch 41/64 loss: -0.0676112174987793
Batch 42/64 loss: -0.1745285987854004
Batch 43/64 loss: -0.28055667877197266
Batch 44/64 loss: 0.1831059455871582
Batch 45/64 loss: -0.17031049728393555
Batch 46/64 loss: -0.19765233993530273
Batch 47/64 loss: 1.1784958839416504
Batch 48/64 loss: -0.16412830352783203
Batch 49/64 loss: 0.016991138458251953
Batch 50/64 loss: -0.10694551467895508
Batch 51/64 loss: -0.3293924331665039
Batch 52/64 loss: -0.346923828125
Batch 53/64 loss: 0.21667146682739258
Batch 54/64 loss: -0.16492700576782227
Batch 55/64 loss: 0.21510791778564453
Batch 56/64 loss: -0.10340118408203125
Batch 57/64 loss: -0.0019659996032714844
Batch 58/64 loss: -0.009418964385986328
Batch 59/64 loss: -0.1657099723815918
Batch 60/64 loss: 0.5748372077941895
Batch 61/64 loss: -0.27899885177612305
Batch 62/64 loss: -0.033277034759521484
Batch 63/64 loss: -0.2435588836669922
Batch 64/64 loss: -3.7968645095825195
Epoch 86  Train loss: -0.10588497461057177  Val loss: -0.4106244877031988
Epoch 87
-------------------------------
Batch 1/64 loss: -0.1541767120361328
Batch 2/64 loss: -0.2061138153076172
Batch 3/64 loss: -0.15911006927490234
Batch 4/64 loss: -0.3131532669067383
Batch 5/64 loss: 0.13471269607543945
Batch 6/64 loss: -0.2980966567993164
Batch 7/64 loss: -0.24379301071166992
Batch 8/64 loss: 0.06115913391113281
Batch 9/64 loss: 0.5223603248596191
Batch 10/64 loss: -0.08841180801391602
Batch 11/64 loss: -0.1889667510986328
Batch 12/64 loss: -0.19476890563964844
Batch 13/64 loss: -0.269073486328125
Batch 14/64 loss: 1.1401448249816895
Batch 15/64 loss: -0.12945938110351562
Batch 16/64 loss: 1.1286511421203613
Batch 17/64 loss: 0.037217140197753906
Batch 18/64 loss: -0.11303901672363281
Batch 19/64 loss: 0.1080484390258789
Batch 20/64 loss: -0.15720796585083008
Batch 21/64 loss: -0.1314530372619629
Batch 22/64 loss: -0.20217609405517578
Batch 23/64 loss: 0.05054950714111328
Batch 24/64 loss: 0.14241266250610352
Batch 25/64 loss: -0.1710805892944336
Batch 26/64 loss: -0.25763750076293945
Batch 27/64 loss: -0.1337285041809082
Batch 28/64 loss: -0.0958871841430664
Batch 29/64 loss: -0.27996826171875
Batch 30/64 loss: 0.3238534927368164
Batch 31/64 loss: -0.23180103302001953
Batch 32/64 loss: -0.13038158416748047
Batch 33/64 loss: -0.023040294647216797
Batch 34/64 loss: -0.03380107879638672
Batch 35/64 loss: 0.030771732330322266
Batch 36/64 loss: -0.06527566909790039
Batch 37/64 loss: 0.10460805892944336
Batch 38/64 loss: 0.08321142196655273
Batch 39/64 loss: -0.24969863891601562
Batch 40/64 loss: -0.08699989318847656
Batch 41/64 loss: -0.11402225494384766
Batch 42/64 loss: -0.07986783981323242
Batch 43/64 loss: -0.03180551528930664
Batch 44/64 loss: -0.2111802101135254
Batch 45/64 loss: -0.09233760833740234
Batch 46/64 loss: -0.15044164657592773
Batch 47/64 loss: -0.02310657501220703
Batch 48/64 loss: 0.05974149703979492
Batch 49/64 loss: -0.23707246780395508
Batch 50/64 loss: 0.058501243591308594
Batch 51/64 loss: -0.23876571655273438
Batch 52/64 loss: -0.36395740509033203
Batch 53/64 loss: -0.19405078887939453
Batch 54/64 loss: -0.27964115142822266
Batch 55/64 loss: 1.200516700744629
Batch 56/64 loss: -0.0774831771850586
Batch 57/64 loss: -0.3217334747314453
Batch 58/64 loss: -0.3223705291748047
Batch 59/64 loss: -0.2256946563720703
Batch 60/64 loss: -0.14341068267822266
Batch 61/64 loss: -0.07271814346313477
Batch 62/64 loss: -0.301577091217041
Batch 63/64 loss: -0.03888702392578125
Batch 64/64 loss: -3.6858763694763184
Epoch 87  Train loss: -0.08951170865227194  Val loss: -0.43333018194768846
Epoch 88
-------------------------------
Batch 1/64 loss: 0.13405466079711914
Batch 2/64 loss: 0.1907968521118164
Batch 3/64 loss: -0.04681587219238281
Batch 4/64 loss: -0.26175403594970703
Batch 5/64 loss: 0.08370590209960938
Batch 6/64 loss: -0.2713489532470703
Batch 7/64 loss: -0.028380870819091797
Batch 8/64 loss: -0.345700740814209
Batch 9/64 loss: -0.2267460823059082
Batch 10/64 loss: -0.08867788314819336
Batch 11/64 loss: -0.31612253189086914
Batch 12/64 loss: -0.1418595314025879
Batch 13/64 loss: -0.2283949851989746
Batch 14/64 loss: -0.1992788314819336
Batch 15/64 loss: -0.22823238372802734
Batch 16/64 loss: -0.1833662986755371
Batch 17/64 loss: -0.19767522811889648
Batch 18/64 loss: -0.013311386108398438
Batch 19/64 loss: -0.28347253799438477
Batch 20/64 loss: 0.3716740608215332
Batch 21/64 loss: -0.20944452285766602
Batch 22/64 loss: 0.1364574432373047
Batch 23/64 loss: 1.596116542816162
Batch 24/64 loss: -0.30329179763793945
Batch 25/64 loss: -0.07301473617553711
Batch 26/64 loss: -0.3259749412536621
Batch 27/64 loss: -0.3089423179626465
Batch 28/64 loss: -0.37353992462158203
Batch 29/64 loss: -0.31307125091552734
Batch 30/64 loss: -0.3287959098815918
Batch 31/64 loss: -0.08874177932739258
Batch 32/64 loss: -0.31637048721313477
Batch 33/64 loss: -0.33770132064819336
Batch 34/64 loss: 0.07507753372192383
Batch 35/64 loss: -0.18115568161010742
Batch 36/64 loss: -0.11892890930175781
Batch 37/64 loss: -0.17256546020507812
Batch 38/64 loss: 0.042208194732666016
Batch 39/64 loss: -0.3607606887817383
Batch 40/64 loss: -0.30165672302246094
Batch 41/64 loss: -0.2767624855041504
Batch 42/64 loss: -0.3230886459350586
Batch 43/64 loss: -0.06689977645874023
Batch 44/64 loss: -0.29328393936157227
Batch 45/64 loss: -0.11696767807006836
Batch 46/64 loss: -0.18782329559326172
Batch 47/64 loss: -0.20846223831176758
Batch 48/64 loss: -0.18777179718017578
Batch 49/64 loss: -0.04995107650756836
Batch 50/64 loss: 0.6652183532714844
Batch 51/64 loss: -0.25672006607055664
Batch 52/64 loss: -0.3059682846069336
Batch 53/64 loss: -0.37322187423706055
Batch 54/64 loss: -0.20534849166870117
Batch 55/64 loss: -0.3573737144470215
Batch 56/64 loss: 0.19689273834228516
Batch 57/64 loss: 0.8989944458007812
Batch 58/64 loss: -0.19879961013793945
Batch 59/64 loss: -0.31178760528564453
Batch 60/64 loss: 0.058078765869140625
Batch 61/64 loss: -0.11579704284667969
Batch 62/64 loss: 0.24928998947143555
Batch 63/64 loss: 0.15691280364990234
Batch 64/64 loss: -3.5841894149780273
Epoch 88  Train loss: -0.1387260549208697  Val loss: -0.2830070679130423
Epoch 89
-------------------------------
Batch 1/64 loss: -0.2832174301147461
Batch 2/64 loss: -0.3070664405822754
Batch 3/64 loss: -0.14140033721923828
Batch 4/64 loss: 0.00945281982421875
Batch 5/64 loss: -0.1652088165283203
Batch 6/64 loss: -0.059388160705566406
Batch 7/64 loss: 0.37239599227905273
Batch 8/64 loss: -0.12748289108276367
Batch 9/64 loss: -0.12185287475585938
Batch 10/64 loss: -0.013854026794433594
Batch 11/64 loss: -0.0988774299621582
Batch 12/64 loss: -0.238555908203125
Batch 13/64 loss: -0.08751392364501953
Batch 14/64 loss: -0.19127845764160156
Batch 15/64 loss: -0.06329488754272461
Batch 16/64 loss: -0.2492351531982422
Batch 17/64 loss: -0.3529205322265625
Batch 18/64 loss: -0.0385746955871582
Batch 19/64 loss: -0.31484460830688477
Batch 20/64 loss: -0.16196918487548828
Batch 21/64 loss: -0.043163299560546875
Batch 22/64 loss: -0.038564205169677734
Batch 23/64 loss: 1.2209491729736328
Batch 24/64 loss: -0.024800777435302734
Batch 25/64 loss: -0.20777654647827148
Batch 26/64 loss: -0.28015851974487305
Batch 27/64 loss: 0.6754703521728516
Batch 28/64 loss: -0.1361069679260254
Batch 29/64 loss: -0.17592382431030273
Batch 30/64 loss: -0.12560033798217773
Batch 31/64 loss: -0.21485185623168945
Batch 32/64 loss: -0.08490514755249023
Batch 33/64 loss: -0.1877140998840332
Batch 34/64 loss: 0.02986764907836914
Batch 35/64 loss: 0.08186578750610352
Batch 36/64 loss: 1.165834903717041
Batch 37/64 loss: -0.24510812759399414
Batch 38/64 loss: -0.013055801391601562
Batch 39/64 loss: -0.15792417526245117
Batch 40/64 loss: -0.23542451858520508
Batch 41/64 loss: -0.08671855926513672
Batch 42/64 loss: -0.28837013244628906
Batch 43/64 loss: -0.04471874237060547
Batch 44/64 loss: -0.379758358001709
Batch 45/64 loss: -0.22429704666137695
Batch 46/64 loss: -0.29587316513061523
Batch 47/64 loss: -0.3824753761291504
Batch 48/64 loss: -0.02885293960571289
Batch 49/64 loss: -0.41924285888671875
Batch 50/64 loss: -0.3548550605773926
Batch 51/64 loss: -0.3035264015197754
Batch 52/64 loss: -0.18528032302856445
Batch 53/64 loss: -0.18373966217041016
Batch 54/64 loss: -0.34093761444091797
Batch 55/64 loss: -0.04108095169067383
Batch 56/64 loss: -0.38146448135375977
Batch 57/64 loss: -0.2471928596496582
Batch 58/64 loss: -0.36640405654907227
Batch 59/64 loss: -0.37163448333740234
Batch 60/64 loss: -0.36811065673828125
Batch 61/64 loss: -0.17920589447021484
Batch 62/64 loss: -0.0874323844909668
Batch 63/64 loss: 0.4183011054992676
Batch 64/64 loss: -3.8140292167663574
Epoch 89  Train loss: -0.15113992317050112  Val loss: -0.48305982085028054
Saving best model, epoch: 89
Epoch 90
-------------------------------
Batch 1/64 loss: -0.39891767501831055
Batch 2/64 loss: -0.16452646255493164
Batch 3/64 loss: -0.199249267578125
Batch 4/64 loss: -0.10913801193237305
Batch 5/64 loss: -0.2584257125854492
Batch 6/64 loss: -0.3283729553222656
Batch 7/64 loss: -0.42139148712158203
Batch 8/64 loss: -0.34059619903564453
Batch 9/64 loss: -0.1885662078857422
Batch 10/64 loss: -0.06699180603027344
Batch 11/64 loss: 0.025391578674316406
Batch 12/64 loss: -0.33341312408447266
Batch 13/64 loss: -0.1949601173400879
Batch 14/64 loss: -0.14252853393554688
Batch 15/64 loss: 0.0716714859008789
Batch 16/64 loss: -0.3074941635131836
Batch 17/64 loss: -0.10904264450073242
Batch 18/64 loss: -0.1946578025817871
Batch 19/64 loss: -0.1320476531982422
Batch 20/64 loss: -0.02188587188720703
Batch 21/64 loss: 1.124098777770996
Batch 22/64 loss: 0.0565333366394043
Batch 23/64 loss: -0.18412351608276367
Batch 24/64 loss: -0.30685949325561523
Batch 25/64 loss: -0.3193974494934082
Batch 26/64 loss: 0.04589080810546875
Batch 27/64 loss: -0.24172544479370117
Batch 28/64 loss: -0.2172689437866211
Batch 29/64 loss: 0.25625133514404297
Batch 30/64 loss: -0.2159891128540039
Batch 31/64 loss: -0.21754026412963867
Batch 32/64 loss: 0.08109092712402344
Batch 33/64 loss: 1.3029417991638184
Batch 34/64 loss: -0.3416924476623535
Batch 35/64 loss: 0.24220991134643555
Batch 36/64 loss: -0.2639317512512207
Batch 37/64 loss: -0.12001943588256836
Batch 38/64 loss: -0.15882110595703125
Batch 39/64 loss: -0.2718195915222168
Batch 40/64 loss: -0.25867700576782227
Batch 41/64 loss: -0.23353052139282227
Batch 42/64 loss: -0.3053603172302246
Batch 43/64 loss: -0.11030197143554688
Batch 44/64 loss: -0.2676372528076172
Batch 45/64 loss: -0.21755361557006836
Batch 46/64 loss: 0.04169273376464844
Batch 47/64 loss: -0.23975276947021484
Batch 48/64 loss: -0.15972328186035156
Batch 49/64 loss: -0.1363368034362793
Batch 50/64 loss: -0.23234033584594727
Batch 51/64 loss: -0.1392526626586914
Batch 52/64 loss: 0.039542198181152344
Batch 53/64 loss: -0.3131847381591797
Batch 54/64 loss: 0.38548946380615234
Batch 55/64 loss: -0.31922197341918945
Batch 56/64 loss: -0.12250328063964844
Batch 57/64 loss: -0.2031869888305664
Batch 58/64 loss: 0.26503610610961914
Batch 59/64 loss: -0.05926704406738281
Batch 60/64 loss: 0.7484922409057617
Batch 61/64 loss: 0.04270601272583008
Batch 62/64 loss: 0.03298330307006836
Batch 63/64 loss: -0.2347879409790039
Batch 64/64 loss: -3.5465760231018066
Epoch 90  Train loss: -0.1289713373371199  Val loss: -0.17707081430966093
Epoch 91
-------------------------------
Batch 1/64 loss: -0.02944183349609375
Batch 2/64 loss: -0.21419954299926758
Batch 3/64 loss: -0.19324779510498047
Batch 4/64 loss: -0.005370616912841797
Batch 5/64 loss: 0.08922290802001953
Batch 6/64 loss: -0.0631723403930664
Batch 7/64 loss: 0.07909107208251953
Batch 8/64 loss: -0.020373821258544922
Batch 9/64 loss: -0.10178184509277344
Batch 10/64 loss: -0.17714500427246094
Batch 11/64 loss: -0.11806869506835938
Batch 12/64 loss: 0.5443930625915527
Batch 13/64 loss: -0.06575822830200195
Batch 14/64 loss: -0.13466310501098633
Batch 15/64 loss: 0.15384340286254883
Batch 16/64 loss: -0.19382238388061523
Batch 17/64 loss: -0.06300210952758789
Batch 18/64 loss: 0.37662315368652344
Batch 19/64 loss: -0.1346302032470703
Batch 20/64 loss: 0.05875444412231445
Batch 21/64 loss: 0.8599734306335449
Batch 22/64 loss: 0.07783937454223633
Batch 23/64 loss: -0.03635978698730469
Batch 24/64 loss: 0.20213079452514648
Batch 25/64 loss: 0.2220931053161621
Batch 26/64 loss: -0.041147708892822266
Batch 27/64 loss: 0.1328887939453125
Batch 28/64 loss: 1.0296387672424316
Batch 29/64 loss: -0.022190093994140625
Batch 30/64 loss: -0.1763310432434082
Batch 31/64 loss: 0.1898789405822754
Batch 32/64 loss: -0.01100921630859375
Batch 33/64 loss: 0.26497507095336914
Batch 34/64 loss: 0.09160327911376953
Batch 35/64 loss: 0.0604853630065918
Batch 36/64 loss: 0.031311988830566406
Batch 37/64 loss: 0.029651165008544922
Batch 38/64 loss: 0.19292211532592773
Batch 39/64 loss: 0.17694568634033203
Batch 40/64 loss: -0.05107879638671875
Batch 41/64 loss: -0.10814714431762695
Batch 42/64 loss: 0.14467096328735352
Batch 43/64 loss: -0.03480100631713867
Batch 44/64 loss: 0.280458927154541
Batch 45/64 loss: -0.07774019241333008
Batch 46/64 loss: 0.006487369537353516
Batch 47/64 loss: -0.10482358932495117
Batch 48/64 loss: 0.43379735946655273
Batch 49/64 loss: 0.13234949111938477
Batch 50/64 loss: -0.035489559173583984
Batch 51/64 loss: -0.23420143127441406
Batch 52/64 loss: 0.005846977233886719
Batch 53/64 loss: 0.48012256622314453
Batch 54/64 loss: 1.4577546119689941
Batch 55/64 loss: -0.39965009689331055
Batch 56/64 loss: -0.16217803955078125
Batch 57/64 loss: -0.212249755859375
Batch 58/64 loss: -0.01914358139038086
Batch 59/64 loss: 0.053148746490478516
Batch 60/64 loss: -0.042389869689941406
Batch 61/64 loss: -0.10595083236694336
Batch 62/64 loss: 0.029243946075439453
Batch 63/64 loss: -0.24605607986450195
Batch 64/64 loss: -3.6290626525878906
Epoch 91  Train loss: 0.024011522180893842  Val loss: -0.22375805517242536
Epoch 92
-------------------------------
Batch 1/64 loss: -0.08018636703491211
Batch 2/64 loss: -0.050341129302978516
Batch 3/64 loss: -0.08483648300170898
Batch 4/64 loss: 0.1802387237548828
Batch 5/64 loss: -0.2625861167907715
Batch 6/64 loss: -0.2713584899902344
Batch 7/64 loss: -0.21235227584838867
Batch 8/64 loss: -0.04741859436035156
Batch 9/64 loss: 0.16589736938476562
Batch 10/64 loss: -0.09607267379760742
Batch 11/64 loss: 0.08704376220703125
Batch 12/64 loss: -0.1137857437133789
Batch 13/64 loss: -0.2990751266479492
Batch 14/64 loss: -0.19316816329956055
Batch 15/64 loss: -0.1474323272705078
Batch 16/64 loss: -0.11804389953613281
Batch 17/64 loss: -0.26011037826538086
Batch 18/64 loss: -0.17755460739135742
Batch 19/64 loss: -0.13804864883422852
Batch 20/64 loss: -0.29081106185913086
Batch 21/64 loss: -0.13606929779052734
Batch 22/64 loss: 0.330474853515625
Batch 23/64 loss: -0.30694150924682617
Batch 24/64 loss: -0.05658102035522461
Batch 25/64 loss: 0.7789392471313477
Batch 26/64 loss: -0.2366652488708496
Batch 27/64 loss: -0.3413224220275879
Batch 28/64 loss: -0.3486819267272949
Batch 29/64 loss: 0.2542991638183594
Batch 30/64 loss: -0.00983428955078125
Batch 31/64 loss: 0.24186277389526367
Batch 32/64 loss: 0.05143880844116211
Batch 33/64 loss: -0.1308269500732422
Batch 34/64 loss: 0.8606224060058594
Batch 35/64 loss: -0.4012150764465332
Batch 36/64 loss: -0.21762561798095703
Batch 37/64 loss: -0.06196737289428711
Batch 38/64 loss: -0.12826156616210938
Batch 39/64 loss: -0.2472543716430664
Batch 40/64 loss: -0.23528671264648438
Batch 41/64 loss: -0.0939946174621582
Batch 42/64 loss: -0.39673471450805664
Batch 43/64 loss: 0.13733243942260742
Batch 44/64 loss: -0.2844209671020508
Batch 45/64 loss: -0.21844959259033203
Batch 46/64 loss: -0.23090744018554688
Batch 47/64 loss: -0.25609493255615234
Batch 48/64 loss: -0.1561417579650879
Batch 49/64 loss: 1.3064451217651367
Batch 50/64 loss: -0.23990297317504883
Batch 51/64 loss: 0.12559223175048828
Batch 52/64 loss: -0.30750083923339844
Batch 53/64 loss: -0.08729743957519531
Batch 54/64 loss: -0.18160009384155273
Batch 55/64 loss: -0.21444940567016602
Batch 56/64 loss: -0.34847593307495117
Batch 57/64 loss: 0.04371166229248047
Batch 58/64 loss: -0.3243880271911621
Batch 59/64 loss: -0.19245481491088867
Batch 60/64 loss: -0.188934326171875
Batch 61/64 loss: -0.019787311553955078
Batch 62/64 loss: -0.10691070556640625
Batch 63/64 loss: -0.39337587356567383
Batch 64/64 loss: -3.671816825866699
Epoch 92  Train loss: -0.1275843339807847  Val loss: -0.465203393365919
Epoch 93
-------------------------------
Batch 1/64 loss: -0.2635025978088379
Batch 2/64 loss: -0.41071653366088867
Batch 3/64 loss: -0.3426637649536133
Batch 4/64 loss: -0.2768979072570801
Batch 5/64 loss: 0.6560702323913574
Batch 6/64 loss: -0.09081697463989258
Batch 7/64 loss: -0.023578643798828125
Batch 8/64 loss: -0.27062559127807617
Batch 9/64 loss: -0.19893407821655273
Batch 10/64 loss: -0.13277244567871094
Batch 11/64 loss: -0.1991100311279297
Batch 12/64 loss: -0.16658258438110352
Batch 13/64 loss: -0.3429546356201172
Batch 14/64 loss: -0.14476537704467773
Batch 15/64 loss: -0.1250147819519043
Batch 16/64 loss: -0.3198428153991699
Batch 17/64 loss: -0.19854116439819336
Batch 18/64 loss: -0.33207178115844727
Batch 19/64 loss: -0.015352249145507812
Batch 20/64 loss: -0.35628652572631836
Batch 21/64 loss: 0.037926673889160156
Batch 22/64 loss: -0.28896522521972656
Batch 23/64 loss: 0.04701805114746094
Batch 24/64 loss: -0.05577278137207031
Batch 25/64 loss: -0.08309745788574219
Batch 26/64 loss: -0.43463850021362305
Batch 27/64 loss: -0.3607010841369629
Batch 28/64 loss: -0.03549528121948242
Batch 29/64 loss: -0.20507049560546875
Batch 30/64 loss: -0.20654010772705078
Batch 31/64 loss: -0.34047365188598633
Batch 32/64 loss: 0.8235197067260742
Batch 33/64 loss: -0.21390914916992188
Batch 34/64 loss: -0.4015007019042969
Batch 35/64 loss: -0.11402559280395508
Batch 36/64 loss: -0.19594573974609375
Batch 37/64 loss: -0.06742525100708008
Batch 38/64 loss: 0.08488655090332031
Batch 39/64 loss: -0.03569507598876953
Batch 40/64 loss: -0.10709762573242188
Batch 41/64 loss: -0.17437458038330078
Batch 42/64 loss: 0.003923892974853516
Batch 43/64 loss: 1.3891315460205078
Batch 44/64 loss: -0.2343611717224121
Batch 45/64 loss: 0.057691097259521484
Batch 46/64 loss: -0.31147050857543945
Batch 47/64 loss: -0.2159256935119629
Batch 48/64 loss: -0.0799102783203125
Batch 49/64 loss: 0.1575312614440918
Batch 50/64 loss: 0.9148850440979004
Batch 51/64 loss: -0.31003427505493164
Batch 52/64 loss: -0.3134727478027344
Batch 53/64 loss: 0.2920551300048828
Batch 54/64 loss: 0.15174388885498047
Batch 55/64 loss: -0.13462114334106445
Batch 56/64 loss: -0.2931842803955078
Batch 57/64 loss: -0.26612138748168945
Batch 58/64 loss: -0.17932748794555664
Batch 59/64 loss: -0.03013467788696289
Batch 60/64 loss: 0.024827957153320312
Batch 61/64 loss: -0.1205601692199707
Batch 62/64 loss: 0.09405994415283203
Batch 63/64 loss: 0.44142913818359375
Batch 64/64 loss: -3.7682089805603027
Epoch 93  Train loss: -0.12031904669368969  Val loss: -0.09294717008715234
Epoch 94
-------------------------------
Batch 1/64 loss: -0.2881355285644531
Batch 2/64 loss: -0.22142553329467773
Batch 3/64 loss: -0.14842557907104492
Batch 4/64 loss: -0.24346637725830078
Batch 5/64 loss: -0.21219348907470703
Batch 6/64 loss: -0.1115422248840332
Batch 7/64 loss: -0.1094808578491211
Batch 8/64 loss: -0.27590465545654297
Batch 9/64 loss: 0.5495390892028809
Batch 10/64 loss: -0.2008371353149414
Batch 11/64 loss: -0.2730832099914551
Batch 12/64 loss: -0.08423328399658203
Batch 13/64 loss: -0.2856731414794922
Batch 14/64 loss: -0.3914928436279297
Batch 15/64 loss: -0.25490236282348633
Batch 16/64 loss: -0.12009239196777344
Batch 17/64 loss: -0.008809089660644531
Batch 18/64 loss: -0.013315677642822266
Batch 19/64 loss: 1.1464791297912598
Batch 20/64 loss: -0.11641645431518555
Batch 21/64 loss: -0.35710906982421875
Batch 22/64 loss: -0.25553226470947266
Batch 23/64 loss: 0.15433406829833984
Batch 24/64 loss: -0.3070240020751953
Batch 25/64 loss: -0.10547208786010742
Batch 26/64 loss: 0.08511161804199219
Batch 27/64 loss: -0.16246652603149414
Batch 28/64 loss: 0.8193812370300293
Batch 29/64 loss: -0.11102867126464844
Batch 30/64 loss: -0.09202957153320312
Batch 31/64 loss: -0.0052032470703125
Batch 32/64 loss: 0.909522533416748
Batch 33/64 loss: -0.2950129508972168
Batch 34/64 loss: -0.29764747619628906
Batch 35/64 loss: -0.08648967742919922
Batch 36/64 loss: -0.1986069679260254
Batch 37/64 loss: -0.004107952117919922
Batch 38/64 loss: -0.22872400283813477
Batch 39/64 loss: -0.24855518341064453
Batch 40/64 loss: -0.25275373458862305
Batch 41/64 loss: -0.29358577728271484
Batch 42/64 loss: -0.17874765396118164
Batch 43/64 loss: -0.09626913070678711
Batch 44/64 loss: -0.13695764541625977
Batch 45/64 loss: -0.3359694480895996
Batch 46/64 loss: -0.17996740341186523
Batch 47/64 loss: -0.09436845779418945
Batch 48/64 loss: -0.16202735900878906
Batch 49/64 loss: -0.20191526412963867
Batch 50/64 loss: -0.12396478652954102
Batch 51/64 loss: -0.32481813430786133
Batch 52/64 loss: 0.11281061172485352
Batch 53/64 loss: -0.39730262756347656
Batch 54/64 loss: -0.3270583152770996
Batch 55/64 loss: -0.2945384979248047
Batch 56/64 loss: -0.34041690826416016
Batch 57/64 loss: -0.30382347106933594
Batch 58/64 loss: 0.22736787796020508
Batch 59/64 loss: -0.280210018157959
Batch 60/64 loss: -0.40117979049682617
Batch 61/64 loss: -0.4020814895629883
Batch 62/64 loss: -0.06910276412963867
Batch 63/64 loss: -0.2983889579772949
Batch 64/64 loss: -3.6694445610046387
Epoch 94  Train loss: -0.16246940201404048  Val loss: -0.49603287215085373
Saving best model, epoch: 94
Epoch 95
-------------------------------
Batch 1/64 loss: -0.11697053909301758
Batch 2/64 loss: -0.2584543228149414
Batch 3/64 loss: -0.20416927337646484
Batch 4/64 loss: -0.3344402313232422
Batch 5/64 loss: -0.38684654235839844
Batch 6/64 loss: -0.23301410675048828
Batch 7/64 loss: -0.34899044036865234
Batch 8/64 loss: -0.1555919647216797
Batch 9/64 loss: -0.12356758117675781
Batch 10/64 loss: -0.24725008010864258
Batch 11/64 loss: -0.2864093780517578
Batch 12/64 loss: -0.35829734802246094
Batch 13/64 loss: -0.14257287979125977
Batch 14/64 loss: -0.14478302001953125
Batch 15/64 loss: -0.25878334045410156
Batch 16/64 loss: -0.2341775894165039
Batch 17/64 loss: -0.18291807174682617
Batch 18/64 loss: 0.6232767105102539
Batch 19/64 loss: -0.04302215576171875
Batch 20/64 loss: -0.3102736473083496
Batch 21/64 loss: -0.011606693267822266
Batch 22/64 loss: -0.18994569778442383
Batch 23/64 loss: -0.30542802810668945
Batch 24/64 loss: -0.22330760955810547
Batch 25/64 loss: -0.37764549255371094
Batch 26/64 loss: -0.31071949005126953
Batch 27/64 loss: -0.1799921989440918
Batch 28/64 loss: -0.2342381477355957
Batch 29/64 loss: -0.20670270919799805
Batch 30/64 loss: -0.255948543548584
Batch 31/64 loss: -0.2600398063659668
Batch 32/64 loss: -0.10385894775390625
Batch 33/64 loss: -0.28113842010498047
Batch 34/64 loss: -0.39806461334228516
Batch 35/64 loss: -0.08968877792358398
Batch 36/64 loss: -0.23899269104003906
Batch 37/64 loss: -0.36046600341796875
Batch 38/64 loss: -0.20493221282958984
Batch 39/64 loss: -0.26064538955688477
Batch 40/64 loss: -0.40497398376464844
Batch 41/64 loss: 0.024453163146972656
Batch 42/64 loss: -0.13656902313232422
Batch 43/64 loss: -0.2570462226867676
Batch 44/64 loss: -0.3849802017211914
Batch 45/64 loss: 0.13408374786376953
Batch 46/64 loss: -0.27736616134643555
Batch 47/64 loss: -0.35097742080688477
Batch 48/64 loss: 1.1575655937194824
Batch 49/64 loss: -0.23158550262451172
Batch 50/64 loss: -0.017529010772705078
Batch 51/64 loss: 0.4126601219177246
Batch 52/64 loss: -0.04699516296386719
Batch 53/64 loss: 0.7917447090148926
Batch 54/64 loss: 0.10002708435058594
Batch 55/64 loss: 0.1608877182006836
Batch 56/64 loss: 0.3009967803955078
Batch 57/64 loss: -0.10720586776733398
Batch 58/64 loss: 0.21733427047729492
Batch 59/64 loss: -0.18724298477172852
Batch 60/64 loss: 0.12609529495239258
Batch 61/64 loss: 0.1065053939819336
Batch 62/64 loss: 2.9343996047973633
Batch 63/64 loss: 0.01800394058227539
Batch 64/64 loss: -3.4418249130249023
Epoch 95  Train loss: -0.10572078555238013  Val loss: -0.13490724072013935
Epoch 96
-------------------------------
Batch 1/64 loss: 1.385848045349121
Batch 2/64 loss: 0.38437652587890625
Batch 3/64 loss: -0.2243971824645996
Batch 4/64 loss: 0.2848482131958008
Batch 5/64 loss: -0.10262537002563477
Batch 6/64 loss: -0.06184196472167969
Batch 7/64 loss: -0.05083417892456055
Batch 8/64 loss: 0.030602455139160156
Batch 9/64 loss: -0.2823810577392578
Batch 10/64 loss: -0.015722274780273438
Batch 11/64 loss: 0.061446189880371094
Batch 12/64 loss: 1.4618940353393555
Batch 13/64 loss: -0.12682056427001953
Batch 14/64 loss: -0.09407854080200195
Batch 15/64 loss: -0.1992783546447754
Batch 16/64 loss: -0.1625828742980957
Batch 17/64 loss: -0.10699701309204102
Batch 18/64 loss: 0.05486774444580078
Batch 19/64 loss: -0.26611328125
Batch 20/64 loss: -0.23507118225097656
Batch 21/64 loss: -0.2376723289489746
Batch 22/64 loss: -0.10552406311035156
Batch 23/64 loss: -0.09858322143554688
Batch 24/64 loss: -0.24304914474487305
Batch 25/64 loss: -0.13485956192016602
Batch 26/64 loss: -0.3724827766418457
Batch 27/64 loss: 0.46327638626098633
Batch 28/64 loss: -0.10283088684082031
Batch 29/64 loss: -0.2602529525756836
Batch 30/64 loss: -0.21801280975341797
Batch 31/64 loss: -0.2800612449645996
Batch 32/64 loss: -0.2489948272705078
Batch 33/64 loss: -0.37528371810913086
Batch 34/64 loss: -0.17745161056518555
Batch 35/64 loss: -0.13045358657836914
Batch 36/64 loss: -0.07400035858154297
Batch 37/64 loss: -0.01743936538696289
Batch 38/64 loss: -0.1609506607055664
Batch 39/64 loss: -0.22158050537109375
Batch 40/64 loss: -0.14413213729858398
Batch 41/64 loss: -0.3511233329772949
Batch 42/64 loss: -0.14664649963378906
Batch 43/64 loss: 1.2766227722167969
Batch 44/64 loss: -0.019965171813964844
Batch 45/64 loss: 0.060912132263183594
Batch 46/64 loss: -0.2832303047180176
Batch 47/64 loss: -0.269716739654541
Batch 48/64 loss: -0.19786500930786133
Batch 49/64 loss: -0.0041294097900390625
Batch 50/64 loss: -0.2237863540649414
Batch 51/64 loss: -0.3141632080078125
Batch 52/64 loss: -0.09912681579589844
Batch 53/64 loss: -0.27336740493774414
Batch 54/64 loss: -0.06680107116699219
Batch 55/64 loss: -0.26127004623413086
Batch 56/64 loss: -0.2816743850708008
Batch 57/64 loss: -0.14074420928955078
Batch 58/64 loss: 1.1816067695617676
Batch 59/64 loss: 0.08756828308105469
Batch 60/64 loss: -0.016497135162353516
Batch 61/64 loss: 0.10509014129638672
Batch 62/64 loss: 0.48090314865112305
Batch 63/64 loss: -0.1917133331298828
Batch 64/64 loss: -3.7637453079223633
Epoch 96  Train loss: -0.06552354775223078  Val loss: -0.3498914790727019
Epoch 97
-------------------------------
Batch 1/64 loss: -0.01551055908203125
Batch 2/64 loss: -0.16010522842407227
Batch 3/64 loss: -0.3002796173095703
Batch 4/64 loss: -0.01432943344116211
Batch 5/64 loss: -0.16275548934936523
Batch 6/64 loss: -0.040986061096191406
Batch 7/64 loss: -0.08957147598266602
Batch 8/64 loss: -0.17735767364501953
Batch 9/64 loss: -0.12258195877075195
Batch 10/64 loss: -0.18420696258544922
Batch 11/64 loss: -0.18729114532470703
Batch 12/64 loss: -0.10398340225219727
Batch 13/64 loss: -0.2009291648864746
Batch 14/64 loss: -0.1517467498779297
Batch 15/64 loss: -0.14412498474121094
Batch 16/64 loss: 0.04191017150878906
Batch 17/64 loss: -0.11993169784545898
Batch 18/64 loss: -0.17611408233642578
Batch 19/64 loss: -0.27921295166015625
Batch 20/64 loss: -0.1867227554321289
Batch 21/64 loss: -0.25972795486450195
Batch 22/64 loss: -0.2592964172363281
Batch 23/64 loss: -0.018850326538085938
Batch 24/64 loss: -0.3097686767578125
Batch 25/64 loss: -0.3232688903808594
Batch 26/64 loss: -0.15236425399780273
Batch 27/64 loss: -0.3529038429260254
Batch 28/64 loss: 0.3209114074707031
Batch 29/64 loss: 0.2624540328979492
Batch 30/64 loss: -0.28325796127319336
Batch 31/64 loss: 0.1044459342956543
Batch 32/64 loss: -0.2428436279296875
Batch 33/64 loss: -0.25080251693725586
Batch 34/64 loss: 0.028720378875732422
Batch 35/64 loss: -0.24471616744995117
Batch 36/64 loss: 0.32730865478515625
Batch 37/64 loss: -0.12828636169433594
Batch 38/64 loss: -0.36652326583862305
Batch 39/64 loss: 0.23061609268188477
Batch 40/64 loss: 0.7272524833679199
Batch 41/64 loss: -0.19896793365478516
Batch 42/64 loss: -0.23172235488891602
Batch 43/64 loss: -0.23682737350463867
Batch 44/64 loss: -0.21454143524169922
Batch 45/64 loss: -0.08433914184570312
Batch 46/64 loss: -0.3099050521850586
Batch 47/64 loss: 1.3601469993591309
Batch 48/64 loss: -0.36751842498779297
Batch 49/64 loss: -0.17596197128295898
Batch 50/64 loss: -0.17975950241088867
Batch 51/64 loss: -0.03630208969116211
Batch 52/64 loss: -0.16913127899169922
Batch 53/64 loss: 0.058746337890625
Batch 54/64 loss: -0.12395143508911133
Batch 55/64 loss: -0.19111156463623047
Batch 56/64 loss: 0.06726980209350586
Batch 57/64 loss: -0.2664947509765625
Batch 58/64 loss: -0.16528081893920898
Batch 59/64 loss: -0.22392034530639648
Batch 60/64 loss: -0.30611228942871094
Batch 61/64 loss: -0.16427373886108398
Batch 62/64 loss: -0.07439613342285156
Batch 63/64 loss: 0.8677215576171875
Batch 64/64 loss: -3.7057900428771973
Epoch 97  Train loss: -0.12725816427492628  Val loss: -0.4145856509913284
Epoch 98
-------------------------------
Batch 1/64 loss: -0.12989568710327148
Batch 2/64 loss: -0.16415739059448242
Batch 3/64 loss: -0.19645452499389648
Batch 4/64 loss: 0.03896760940551758
Batch 5/64 loss: 0.018547534942626953
Batch 6/64 loss: -0.24129629135131836
Batch 7/64 loss: -0.24314451217651367
Batch 8/64 loss: 0.2596321105957031
Batch 9/64 loss: -0.24858331680297852
Batch 10/64 loss: -0.01157522201538086
Batch 11/64 loss: -0.12421369552612305
Batch 12/64 loss: -0.21511077880859375
Batch 13/64 loss: -0.11093759536743164
Batch 14/64 loss: -0.2602992057800293
Batch 15/64 loss: -0.32135486602783203
Batch 16/64 loss: -0.1834573745727539
Batch 17/64 loss: -0.16231870651245117
Batch 18/64 loss: 0.0483403205871582
Batch 19/64 loss: -0.0031538009643554688
Batch 20/64 loss: 0.18149995803833008
Batch 21/64 loss: -0.3520379066467285
Batch 22/64 loss: -0.335599422454834
Batch 23/64 loss: -0.30619192123413086
Batch 24/64 loss: -0.039597511291503906
Batch 25/64 loss: -0.15169048309326172
Batch 26/64 loss: -0.4211440086364746
Batch 27/64 loss: 0.7675037384033203
Batch 28/64 loss: -0.06627416610717773
Batch 29/64 loss: -0.20640087127685547
Batch 30/64 loss: -0.26346397399902344
Batch 31/64 loss: -0.0756983757019043
Batch 32/64 loss: -0.2785496711730957
Batch 33/64 loss: 1.4454336166381836
Batch 34/64 loss: -0.20638275146484375
Batch 35/64 loss: -0.16268491744995117
Batch 36/64 loss: 0.053049564361572266
Batch 37/64 loss: -0.3287205696105957
Batch 38/64 loss: -0.3303976058959961
Batch 39/64 loss: -0.19870376586914062
Batch 40/64 loss: -0.0962529182434082
Batch 41/64 loss: -0.11981487274169922
Batch 42/64 loss: 0.1638035774230957
Batch 43/64 loss: -0.2606930732727051
Batch 44/64 loss: -0.14557790756225586
Batch 45/64 loss: -0.25841426849365234
Batch 46/64 loss: -0.12983274459838867
Batch 47/64 loss: -0.2678680419921875
Batch 48/64 loss: -0.2629055976867676
Batch 49/64 loss: -0.34792089462280273
Batch 50/64 loss: -0.41624975204467773
Batch 51/64 loss: 0.02683734893798828
Batch 52/64 loss: -0.2996997833251953
Batch 53/64 loss: -0.23677968978881836
Batch 54/64 loss: -0.380216121673584
Batch 55/64 loss: -0.23189592361450195
Batch 56/64 loss: -0.4408140182495117
Batch 57/64 loss: 0.10382080078125
Batch 58/64 loss: 0.9009089469909668
Batch 59/64 loss: -0.2971644401550293
Batch 60/64 loss: -0.40142011642456055
Batch 61/64 loss: -0.28903627395629883
Batch 62/64 loss: -0.21123647689819336
Batch 63/64 loss: 0.46942567825317383
Batch 64/64 loss: -3.7577896118164062
Epoch 98  Train loss: -0.15331537583295038  Val loss: -0.278195253352529
Epoch 99
-------------------------------
Batch 1/64 loss: -0.11208820343017578
Batch 2/64 loss: 0.1033468246459961
Batch 3/64 loss: -0.2250676155090332
Batch 4/64 loss: -0.15297555923461914
Batch 5/64 loss: -0.14766263961791992
Batch 6/64 loss: -0.3854994773864746
Batch 7/64 loss: -0.08280515670776367
Batch 8/64 loss: -0.017560958862304688
Batch 9/64 loss: -0.13646316528320312
Batch 10/64 loss: -0.06474971771240234
Batch 11/64 loss: 0.04746294021606445
Batch 12/64 loss: 1.5738372802734375
Batch 13/64 loss: 0.16443967819213867
Batch 14/64 loss: -0.2580432891845703
Batch 15/64 loss: -0.21274757385253906
Batch 16/64 loss: -0.04628896713256836
Batch 17/64 loss: -0.009372711181640625
Batch 18/64 loss: -0.21920347213745117
Batch 19/64 loss: -0.07116222381591797
Batch 20/64 loss: 1.0726571083068848
Batch 21/64 loss: 0.10912942886352539
Batch 22/64 loss: -0.21950197219848633
Batch 23/64 loss: -0.28814697265625
Batch 24/64 loss: -0.3036508560180664
Batch 25/64 loss: 0.652008056640625
Batch 26/64 loss: -0.38344335556030273
Batch 27/64 loss: -0.3099498748779297
Batch 28/64 loss: -0.1697540283203125
Batch 29/64 loss: -0.3456435203552246
Batch 30/64 loss: -0.2655982971191406
Batch 31/64 loss: -0.2760448455810547
Batch 32/64 loss: -0.34789609909057617
Batch 33/64 loss: -0.24105072021484375
Batch 34/64 loss: -0.07704448699951172
Batch 35/64 loss: -0.40250110626220703
Batch 36/64 loss: -0.2993302345275879
Batch 37/64 loss: -0.36754846572875977
Batch 38/64 loss: -0.3375678062438965
Batch 39/64 loss: -0.2791013717651367
Batch 40/64 loss: -0.3788609504699707
Batch 41/64 loss: -0.3626070022583008
Batch 42/64 loss: -0.32224035263061523
Batch 43/64 loss: 0.021940231323242188
Batch 44/64 loss: -0.20862102508544922
Batch 45/64 loss: -0.3866429328918457
Batch 46/64 loss: -0.11834859848022461
Batch 47/64 loss: -0.30983495712280273
Batch 48/64 loss: -0.2542753219604492
Batch 49/64 loss: -0.11642646789550781
Batch 50/64 loss: -0.23142290115356445
Batch 51/64 loss: -0.37594175338745117
Batch 52/64 loss: 0.4376864433288574
Batch 53/64 loss: -0.3185420036315918
Batch 54/64 loss: 0.4644160270690918
Batch 55/64 loss: -0.3602151870727539
Batch 56/64 loss: -0.2754216194152832
Batch 57/64 loss: -0.3077964782714844
Batch 58/64 loss: -0.10917806625366211
Batch 59/64 loss: -0.06267166137695312
Batch 60/64 loss: -0.24057483673095703
Batch 61/64 loss: -0.30253171920776367
Batch 62/64 loss: 0.02787923812866211
Batch 63/64 loss: -0.29587841033935547
Batch 64/64 loss: -3.4480056762695312
Epoch 99  Train loss: -0.16164232141831342  Val loss: -0.43288993180002955
Epoch 100
-------------------------------
Batch 1/64 loss: -0.23596954345703125
Batch 2/64 loss: -0.38588953018188477
Batch 3/64 loss: -0.1353898048400879
Batch 4/64 loss: -0.3704514503479004
Batch 5/64 loss: -0.29536008834838867
Batch 6/64 loss: 0.903928279876709
Batch 7/64 loss: -0.34978580474853516
Batch 8/64 loss: -0.3642158508300781
Batch 9/64 loss: -0.3049168586730957
Batch 10/64 loss: -0.21234369277954102
Batch 11/64 loss: -0.2045269012451172
Batch 12/64 loss: -0.29869556427001953
Batch 13/64 loss: 0.04209613800048828
Batch 14/64 loss: -0.3900423049926758
Batch 15/64 loss: -0.4092569351196289
Batch 16/64 loss: -0.23723220825195312
Batch 17/64 loss: -0.36308765411376953
Batch 18/64 loss: -0.2506542205810547
Batch 19/64 loss: -0.1318955421447754
Batch 20/64 loss: -0.3503756523132324
Batch 21/64 loss: -0.32808876037597656
Batch 22/64 loss: -0.18613433837890625
Batch 23/64 loss: -0.3876495361328125
Batch 24/64 loss: -0.47107982635498047
Batch 25/64 loss: -0.27318906784057617
Batch 26/64 loss: -0.10250377655029297
Batch 27/64 loss: -0.44341325759887695
Batch 28/64 loss: -0.395754337310791
Batch 29/64 loss: -0.39531946182250977
Batch 30/64 loss: -0.37462615966796875
Batch 31/64 loss: -0.08276796340942383
Batch 32/64 loss: 0.7544794082641602
Batch 33/64 loss: -0.23018264770507812
Batch 34/64 loss: -0.41367483139038086
Batch 35/64 loss: -0.3317737579345703
Batch 36/64 loss: -0.38076114654541016
Batch 37/64 loss: -0.27057361602783203
Batch 38/64 loss: 0.14078569412231445
Batch 39/64 loss: 1.3288288116455078
Batch 40/64 loss: 0.39565467834472656
Batch 41/64 loss: -0.3357234001159668
Batch 42/64 loss: -0.21686744689941406
Batch 43/64 loss: -0.37430381774902344
Batch 44/64 loss: -0.25339508056640625
Batch 45/64 loss: -0.3767366409301758
Batch 46/64 loss: -0.15561628341674805
Batch 47/64 loss: 0.2411327362060547
Batch 48/64 loss: -0.44266271591186523
Batch 49/64 loss: -0.27595996856689453
Batch 50/64 loss: -0.3019089698791504
Batch 51/64 loss: -0.2336716651916504
Batch 52/64 loss: -0.2422637939453125
Batch 53/64 loss: 0.38774585723876953
Batch 54/64 loss: -0.30399608612060547
Batch 55/64 loss: -0.11522150039672852
Batch 56/64 loss: -0.11863994598388672
Batch 57/64 loss: -0.2299351692199707
Batch 58/64 loss: -0.023406028747558594
Batch 59/64 loss: -0.23635482788085938
Batch 60/64 loss: -0.1221165657043457
Batch 61/64 loss: -0.36577415466308594
Batch 62/64 loss: -0.20943069458007812
Batch 63/64 loss: -0.10138416290283203
Batch 64/64 loss: -3.5930862426757812
Epoch 100  Train loss: -0.21793120141122854  Val loss: -0.1831038563521867
Epoch 101
-------------------------------
Batch 1/64 loss: -0.20569944381713867
Batch 2/64 loss: -0.2085704803466797
Batch 3/64 loss: -0.10400962829589844
Batch 4/64 loss: -0.24354171752929688
Batch 5/64 loss: -0.04539918899536133
Batch 6/64 loss: 0.3050370216369629
Batch 7/64 loss: -0.3316655158996582
Batch 8/64 loss: -0.30967140197753906
Batch 9/64 loss: -0.12707138061523438
Batch 10/64 loss: -0.2540102005004883
Batch 11/64 loss: 0.17147445678710938
Batch 12/64 loss: -0.39675283432006836
Batch 13/64 loss: 0.0023241043090820312
Batch 14/64 loss: -0.15221786499023438
Batch 15/64 loss: -0.24055767059326172
Batch 16/64 loss: -0.33798837661743164
Batch 17/64 loss: 0.20403432846069336
Batch 18/64 loss: 1.2016253471374512
Batch 19/64 loss: 0.42775535583496094
Batch 20/64 loss: -0.16575288772583008
Batch 21/64 loss: 0.04254436492919922
Batch 22/64 loss: -0.14207696914672852
Batch 23/64 loss: 0.1034393310546875
Batch 24/64 loss: -0.07125043869018555
Batch 25/64 loss: -0.03176689147949219
Batch 26/64 loss: -0.3883090019226074
Batch 27/64 loss: -0.04457855224609375
Batch 28/64 loss: -0.33408260345458984
Batch 29/64 loss: -0.3234882354736328
Batch 30/64 loss: 0.08761787414550781
Batch 31/64 loss: -0.36160755157470703
Batch 32/64 loss: 0.6429400444030762
Batch 33/64 loss: -0.3061337471008301
Batch 34/64 loss: -0.17513751983642578
Batch 35/64 loss: -0.2505173683166504
Batch 36/64 loss: 0.8795709609985352
Batch 37/64 loss: 0.31513214111328125
Batch 38/64 loss: -0.22333192825317383
Batch 39/64 loss: -0.408693790435791
Batch 40/64 loss: -0.17290878295898438
Batch 41/64 loss: -0.1731724739074707
Batch 42/64 loss: -0.3902583122253418
Batch 43/64 loss: -0.4011688232421875
Batch 44/64 loss: -0.36308813095092773
Batch 45/64 loss: -0.2395009994506836
Batch 46/64 loss: -0.29860401153564453
Batch 47/64 loss: -0.35111284255981445
Batch 48/64 loss: -0.04240131378173828
Batch 49/64 loss: -0.2454977035522461
Batch 50/64 loss: -0.31230783462524414
Batch 51/64 loss: -0.46294689178466797
Batch 52/64 loss: -0.46173524856567383
Batch 53/64 loss: -0.4028658866882324
Batch 54/64 loss: -0.4259486198425293
Batch 55/64 loss: 0.02071237564086914
Batch 56/64 loss: -0.14942026138305664
Batch 57/64 loss: -0.41832494735717773
Batch 58/64 loss: -0.28748512268066406
Batch 59/64 loss: -0.12559986114501953
Batch 60/64 loss: -0.274658203125
Batch 61/64 loss: -0.10318326950073242
Batch 62/64 loss: -0.18023252487182617
Batch 63/64 loss: 1.077047348022461
Batch 64/64 loss: -3.7866311073303223
Epoch 101  Train loss: -0.15411801618688248  Val loss: -0.5621347591229731
Saving best model, epoch: 101
Epoch 102
-------------------------------
Batch 1/64 loss: -0.21140384674072266
Batch 2/64 loss: -0.1058349609375
Batch 3/64 loss: -0.008317947387695312
Batch 4/64 loss: 0.8308601379394531
Batch 5/64 loss: -0.4004654884338379
Batch 6/64 loss: 0.04548025131225586
Batch 7/64 loss: -0.18264532089233398
Batch 8/64 loss: -0.10550689697265625
Batch 9/64 loss: -0.13722467422485352
Batch 10/64 loss: -0.12279844284057617
Batch 11/64 loss: -0.19711637496948242
Batch 12/64 loss: -0.287261962890625
Batch 13/64 loss: -0.2539939880371094
Batch 14/64 loss: -0.02256488800048828
Batch 15/64 loss: -0.41667604446411133
Batch 16/64 loss: -0.2221841812133789
Batch 17/64 loss: 0.3014044761657715
Batch 18/64 loss: -0.19188976287841797
Batch 19/64 loss: 0.9421992301940918
Batch 20/64 loss: -0.3760857582092285
Batch 21/64 loss: -0.46902990341186523
Batch 22/64 loss: -0.3530254364013672
Batch 23/64 loss: -0.23436594009399414
Batch 24/64 loss: -0.3845643997192383
Batch 25/64 loss: -0.4528923034667969
Batch 26/64 loss: -0.33379650115966797
Batch 27/64 loss: -0.34087038040161133
Batch 28/64 loss: -0.35794782638549805
Batch 29/64 loss: -0.40007734298706055
Batch 30/64 loss: -0.29827308654785156
Batch 31/64 loss: -0.3482842445373535
Batch 32/64 loss: -0.1318826675415039
Batch 33/64 loss: -0.3812541961669922
Batch 34/64 loss: -0.2744765281677246
Batch 35/64 loss: 0.8938517570495605
Batch 36/64 loss: -0.3078765869140625
Batch 37/64 loss: -0.4418320655822754
Batch 38/64 loss: -0.5042886734008789
Batch 39/64 loss: -0.29294681549072266
Batch 40/64 loss: -0.3562436103820801
Batch 41/64 loss: -0.33283281326293945
Batch 42/64 loss: -0.2782902717590332
Batch 43/64 loss: -0.4142732620239258
Batch 44/64 loss: -0.20247316360473633
Batch 45/64 loss: -0.25817155838012695
Batch 46/64 loss: -0.035083770751953125
Batch 47/64 loss: -0.2549567222595215
Batch 48/64 loss: -0.13929176330566406
Batch 49/64 loss: -0.32773685455322266
Batch 50/64 loss: -0.2277841567993164
Batch 51/64 loss: -0.3443765640258789
Batch 52/64 loss: -0.391787052154541
Batch 53/64 loss: -0.26964712142944336
Batch 54/64 loss: -0.3167757987976074
Batch 55/64 loss: -0.38280200958251953
Batch 56/64 loss: -0.2914876937866211
Batch 57/64 loss: 1.273759365081787
Batch 58/64 loss: -0.3377861976623535
Batch 59/64 loss: -0.45861339569091797
Batch 60/64 loss: -0.3834686279296875
Batch 61/64 loss: -0.32507896423339844
Batch 62/64 loss: -0.3642578125
Batch 63/64 loss: -0.2608952522277832
Batch 64/64 loss: -3.5696544647216797
Epoch 102  Train loss: -0.23362283145680146  Val loss: -0.274221780783532
Epoch 103
-------------------------------
Batch 1/64 loss: -0.35846376419067383
Batch 2/64 loss: -0.24393415451049805
Batch 3/64 loss: -0.4648895263671875
Batch 4/64 loss: -0.40305519104003906
Batch 5/64 loss: -0.24869203567504883
Batch 6/64 loss: -0.2759819030761719
Batch 7/64 loss: 0.36687564849853516
Batch 8/64 loss: -0.4500894546508789
Batch 9/64 loss: -0.333768367767334
Batch 10/64 loss: -0.3270244598388672
Batch 11/64 loss: 0.6976580619812012
Batch 12/64 loss: -0.3514223098754883
Batch 13/64 loss: 0.5572061538696289
Batch 14/64 loss: -0.22531986236572266
Batch 15/64 loss: -0.2978549003601074
Batch 16/64 loss: -0.2485218048095703
Batch 17/64 loss: -0.3225560188293457
Batch 18/64 loss: -0.28095149993896484
Batch 19/64 loss: 0.5222692489624023
Batch 20/64 loss: -0.2590184211730957
Batch 21/64 loss: -0.4232673645019531
Batch 22/64 loss: -0.4286961555480957
Batch 23/64 loss: -0.2828850746154785
Batch 24/64 loss: -0.4543595314025879
Batch 25/64 loss: -0.2203369140625
Batch 26/64 loss: -0.4688124656677246
Batch 27/64 loss: -0.41901636123657227
Batch 28/64 loss: -0.31310510635375977
Batch 29/64 loss: -0.35581398010253906
Batch 30/64 loss: -0.3128533363342285
Batch 31/64 loss: -0.4518413543701172
Batch 32/64 loss: -0.22795486450195312
Batch 33/64 loss: -0.28668880462646484
Batch 34/64 loss: -0.15084457397460938
Batch 35/64 loss: -0.28333473205566406
Batch 36/64 loss: -0.30707359313964844
Batch 37/64 loss: -0.4607248306274414
Batch 38/64 loss: -0.43295955657958984
Batch 39/64 loss: -0.18186473846435547
Batch 40/64 loss: 0.9488224983215332
Batch 41/64 loss: -0.24236822128295898
Batch 42/64 loss: -0.0699315071105957
Batch 43/64 loss: -0.19051265716552734
Batch 44/64 loss: -0.14598703384399414
Batch 45/64 loss: -0.34464025497436523
Batch 46/64 loss: -0.32290172576904297
Batch 47/64 loss: -0.3723177909851074
Batch 48/64 loss: -0.1292734146118164
Batch 49/64 loss: 1.2959928512573242
Batch 50/64 loss: -0.16521883010864258
Batch 51/64 loss: -0.34328269958496094
Batch 52/64 loss: -0.5540475845336914
Batch 53/64 loss: -0.33140993118286133
Batch 54/64 loss: -0.27446413040161133
Batch 55/64 loss: -0.08500003814697266
Batch 56/64 loss: -0.26913976669311523
Batch 57/64 loss: -0.29337263107299805
Batch 58/64 loss: 0.5569744110107422
Batch 59/64 loss: -0.29712533950805664
Batch 60/64 loss: -0.08857870101928711
Batch 61/64 loss: -0.40817832946777344
Batch 62/64 loss: -0.23699474334716797
Batch 63/64 loss: -0.26143980026245117
Batch 64/64 loss: -3.486431121826172
Epoch 103  Train loss: -0.22979116252824372  Val loss: -0.45355874812070446
Epoch 104
-------------------------------
Batch 1/64 loss: 0.6070890426635742
Batch 2/64 loss: -0.43451786041259766
Batch 3/64 loss: -0.20238733291625977
Batch 4/64 loss: -0.4349212646484375
Batch 5/64 loss: -0.38939952850341797
Batch 6/64 loss: -0.33679866790771484
Batch 7/64 loss: -0.3464622497558594
Batch 8/64 loss: -0.1843881607055664
Batch 9/64 loss: -0.46710920333862305
Batch 10/64 loss: -0.36884212493896484
Batch 11/64 loss: -0.31888437271118164
Batch 12/64 loss: -0.42272281646728516
Batch 13/64 loss: 0.10854053497314453
Batch 14/64 loss: -0.40068483352661133
Batch 15/64 loss: -0.42897939682006836
Batch 16/64 loss: -0.33243703842163086
Batch 17/64 loss: -0.3419504165649414
Batch 18/64 loss: -0.17970657348632812
Batch 19/64 loss: -0.3283677101135254
Batch 20/64 loss: -0.3809375762939453
Batch 21/64 loss: -0.34783315658569336
Batch 22/64 loss: 0.15097522735595703
Batch 23/64 loss: -0.30988025665283203
Batch 24/64 loss: -0.3449702262878418
Batch 25/64 loss: 0.0846552848815918
Batch 26/64 loss: -0.17905235290527344
Batch 27/64 loss: -0.47190189361572266
Batch 28/64 loss: -0.18645048141479492
Batch 29/64 loss: -0.3974127769470215
Batch 30/64 loss: 0.08019351959228516
Batch 31/64 loss: -0.25618410110473633
Batch 32/64 loss: -0.40122079849243164
Batch 33/64 loss: -0.28560781478881836
Batch 34/64 loss: -0.3034672737121582
Batch 35/64 loss: -0.27573108673095703
Batch 36/64 loss: -0.4387493133544922
Batch 37/64 loss: -0.33956193923950195
Batch 38/64 loss: -0.36069440841674805
Batch 39/64 loss: -0.38341236114501953
Batch 40/64 loss: -0.3791999816894531
Batch 41/64 loss: -0.3835268020629883
Batch 42/64 loss: -0.480621337890625
Batch 43/64 loss: -0.42052507400512695
Batch 44/64 loss: -0.4034438133239746
Batch 45/64 loss: -0.17525386810302734
Batch 46/64 loss: -0.32863855361938477
Batch 47/64 loss: -0.46015167236328125
Batch 48/64 loss: -0.291501522064209
Batch 49/64 loss: -0.417508602142334
Batch 50/64 loss: -0.43079423904418945
Batch 51/64 loss: -0.2583894729614258
Batch 52/64 loss: -0.3946552276611328
Batch 53/64 loss: -0.36353254318237305
Batch 54/64 loss: 2.3154549598693848
Batch 55/64 loss: -0.35507965087890625
Batch 56/64 loss: -0.3594813346862793
Batch 57/64 loss: -0.27382659912109375
Batch 58/64 loss: 2.6702880859375e-05
Batch 59/64 loss: 2.343980312347412
Batch 60/64 loss: 0.4334077835083008
Batch 61/64 loss: -0.03490495681762695
Batch 62/64 loss: -0.022861480712890625
Batch 63/64 loss: -0.10564756393432617
Batch 64/64 loss: -2.5167765617370605
Epoch 104  Train loss: -0.2146577367595598  Val loss: 0.23549696669955433
Epoch 105
-------------------------------
Batch 1/64 loss: -0.0022096633911132812
Batch 2/64 loss: 0.19542407989501953
Batch 3/64 loss: -0.16093683242797852
Batch 4/64 loss: -0.24561357498168945
Batch 5/64 loss: 0.04535198211669922
Batch 6/64 loss: 0.03050708770751953
Batch 7/64 loss: 0.7056746482849121
Batch 8/64 loss: -0.2893800735473633
Batch 9/64 loss: 0.07884693145751953
Batch 10/64 loss: -0.14014339447021484
Batch 11/64 loss: -0.09843730926513672
Batch 12/64 loss: -0.23136663436889648
Batch 13/64 loss: -0.2595505714416504
Batch 14/64 loss: -0.2716517448425293
Batch 15/64 loss: -0.20357656478881836
Batch 16/64 loss: 1.4382143020629883
Batch 17/64 loss: -0.05638933181762695
Batch 18/64 loss: -0.16412830352783203
Batch 19/64 loss: -0.18193674087524414
Batch 20/64 loss: -0.2385120391845703
Batch 21/64 loss: -0.217803955078125
Batch 22/64 loss: -0.04376220703125
Batch 23/64 loss: 0.1069340705871582
Batch 24/64 loss: -0.2289586067199707
Batch 25/64 loss: -0.2838735580444336
Batch 26/64 loss: -0.16421842575073242
Batch 27/64 loss: -0.23201322555541992
Batch 28/64 loss: -0.4534740447998047
Batch 29/64 loss: -0.22638320922851562
Batch 30/64 loss: -0.2440347671508789
Batch 31/64 loss: -0.43030214309692383
Batch 32/64 loss: 0.41187191009521484
Batch 33/64 loss: -0.2163105010986328
Batch 34/64 loss: 1.1282882690429688
Batch 35/64 loss: -0.45415592193603516
Batch 36/64 loss: -0.16152048110961914
Batch 37/64 loss: -0.24872350692749023
Batch 38/64 loss: -0.14901256561279297
Batch 39/64 loss: -0.32344865798950195
Batch 40/64 loss: 0.23508262634277344
Batch 41/64 loss: 0.16895818710327148
Batch 42/64 loss: -0.34300708770751953
Batch 43/64 loss: -0.22069787979125977
Batch 44/64 loss: -0.33910226821899414
Batch 45/64 loss: -0.3378105163574219
Batch 46/64 loss: -0.03783845901489258
Batch 47/64 loss: 1.1152677536010742
Batch 48/64 loss: -0.026590824127197266
Batch 49/64 loss: -0.27977609634399414
Batch 50/64 loss: -0.2749447822570801
Batch 51/64 loss: -0.06724405288696289
Batch 52/64 loss: -0.25482988357543945
Batch 53/64 loss: 0.531735897064209
Batch 54/64 loss: 0.04992961883544922
Batch 55/64 loss: -0.08487510681152344
Batch 56/64 loss: 0.5513567924499512
Batch 57/64 loss: -0.31873273849487305
Batch 58/64 loss: -0.19247674942016602
Batch 59/64 loss: 0.5976886749267578
Batch 60/64 loss: -0.3872685432434082
Batch 61/64 loss: -0.16364812850952148
Batch 62/64 loss: 0.3445248603820801
Batch 63/64 loss: -0.15932512283325195
Batch 64/64 loss: -3.6160497665405273
Epoch 105  Train loss: -0.07978629691928041  Val loss: 0.12443183295915217
Epoch 106
-------------------------------
Batch 1/64 loss: 1.327282428741455
Batch 2/64 loss: 0.0411686897277832
Batch 3/64 loss: 0.04741334915161133
Batch 4/64 loss: 0.008348941802978516
Batch 5/64 loss: 0.021735191345214844
Batch 6/64 loss: 0.09598493576049805
Batch 7/64 loss: 0.2092900276184082
Batch 8/64 loss: 1.4138069152832031
Batch 9/64 loss: 0.46057748794555664
Batch 10/64 loss: 0.758415699005127
Batch 11/64 loss: 0.8795666694641113
Batch 12/64 loss: 0.5263156890869141
Batch 13/64 loss: 1.2322678565979004
Batch 14/64 loss: 0.23519515991210938
Batch 15/64 loss: 0.5389180183410645
Batch 16/64 loss: 1.4263806343078613
Batch 17/64 loss: 0.6170573234558105
Batch 18/64 loss: 0.2034621238708496
Batch 19/64 loss: 0.44437074661254883
Batch 20/64 loss: 0.4738907814025879
Batch 21/64 loss: -0.0427861213684082
Batch 22/64 loss: 0.23005914688110352
Batch 23/64 loss: -0.09443235397338867
Batch 24/64 loss: -0.08792829513549805
Batch 25/64 loss: 0.19298267364501953
Batch 26/64 loss: 0.4496021270751953
Batch 27/64 loss: 0.2274951934814453
Batch 28/64 loss: 0.016425609588623047
Batch 29/64 loss: 0.04570960998535156
Batch 30/64 loss: 0.17755889892578125
Batch 31/64 loss: 0.22759151458740234
Batch 32/64 loss: 0.5400104522705078
Batch 33/64 loss: -0.17486000061035156
Batch 34/64 loss: 0.21451759338378906
Batch 35/64 loss: -0.005403995513916016
Batch 36/64 loss: 1.2984700202941895
Batch 37/64 loss: -0.04644584655761719
Batch 38/64 loss: -0.1736292839050293
Batch 39/64 loss: 0.04262399673461914
Batch 40/64 loss: -0.25353193283081055
Batch 41/64 loss: 1.2411994934082031
Batch 42/64 loss: -0.021897315979003906
Batch 43/64 loss: -0.15413522720336914
Batch 44/64 loss: -0.06462240219116211
Batch 45/64 loss: -0.1521906852722168
Batch 46/64 loss: 0.023565292358398438
Batch 47/64 loss: -0.30257225036621094
Batch 48/64 loss: 0.6095895767211914
Batch 49/64 loss: -0.2586345672607422
Batch 50/64 loss: -0.10811328887939453
Batch 51/64 loss: -0.16101312637329102
Batch 52/64 loss: -0.2014145851135254
Batch 53/64 loss: -0.15334653854370117
Batch 54/64 loss: -0.22743892669677734
Batch 55/64 loss: -0.2150282859802246
Batch 56/64 loss: 0.8438787460327148
Batch 57/64 loss: -0.16210508346557617
Batch 58/64 loss: -0.027521133422851562
Batch 59/64 loss: -0.24003171920776367
Batch 60/64 loss: -0.19804763793945312
Batch 61/64 loss: -0.17527341842651367
Batch 62/64 loss: 0.022228240966796875
Batch 63/64 loss: -0.15752935409545898
Batch 64/64 loss: -3.8113207817077637
Epoch 106  Train loss: 0.16700443754009173  Val loss: -0.3465446655692923
Epoch 107
-------------------------------
Batch 1/64 loss: -0.026404857635498047
Batch 2/64 loss: -0.1035609245300293
Batch 3/64 loss: 0.4966874122619629
Batch 4/64 loss: 0.12893104553222656
Batch 5/64 loss: -0.3035249710083008
Batch 6/64 loss: -0.12860584259033203
Batch 7/64 loss: -0.01848125457763672
Batch 8/64 loss: -0.07013320922851562
Batch 9/64 loss: 0.2705354690551758
Batch 10/64 loss: -0.1281118392944336
Batch 11/64 loss: -0.19905471801757812
Batch 12/64 loss: -0.17174577713012695
Batch 13/64 loss: -0.14095163345336914
Batch 14/64 loss: -0.21117591857910156
Batch 15/64 loss: -0.042101383209228516
Batch 16/64 loss: -0.19936656951904297
Batch 17/64 loss: -0.3313145637512207
Batch 18/64 loss: -0.3854789733886719
Batch 19/64 loss: -0.21050786972045898
Batch 20/64 loss: -0.21082782745361328
Batch 21/64 loss: -0.23683738708496094
Batch 22/64 loss: -0.3062419891357422
Batch 23/64 loss: 1.0936026573181152
Batch 24/64 loss: -0.27187347412109375
Batch 25/64 loss: -0.16658258438110352
Batch 26/64 loss: -0.32953405380249023
Batch 27/64 loss: -0.1255478858947754
Batch 28/64 loss: -0.10725736618041992
Batch 29/64 loss: -0.11556291580200195
Batch 30/64 loss: -0.25688695907592773
Batch 31/64 loss: -0.26984500885009766
Batch 32/64 loss: -0.2126307487487793
Batch 33/64 loss: 0.6954226493835449
Batch 34/64 loss: -0.32798099517822266
Batch 35/64 loss: -0.29733896255493164
Batch 36/64 loss: 0.2553553581237793
Batch 37/64 loss: -0.14323139190673828
Batch 38/64 loss: -0.16828489303588867
Batch 39/64 loss: -0.004413127899169922
Batch 40/64 loss: 0.37461280822753906
Batch 41/64 loss: -0.11467552185058594
Batch 42/64 loss: -0.19308090209960938
Batch 43/64 loss: 0.11831951141357422
Batch 44/64 loss: -0.24428319931030273
Batch 45/64 loss: 0.20215272903442383
Batch 46/64 loss: -0.2414722442626953
Batch 47/64 loss: -0.2510547637939453
Batch 48/64 loss: -0.24423646926879883
Batch 49/64 loss: -0.21732664108276367
Batch 50/64 loss: -0.22297048568725586
Batch 51/64 loss: 0.044200897216796875
Batch 52/64 loss: -0.3986983299255371
Batch 53/64 loss: -0.24343347549438477
Batch 54/64 loss: -0.2697772979736328
Batch 55/64 loss: -0.3824930191040039
Batch 56/64 loss: -0.17403507232666016
Batch 57/64 loss: -0.45963048934936523
Batch 58/64 loss: -0.06801748275756836
Batch 59/64 loss: -0.4193720817565918
Batch 60/64 loss: 0.2396092414855957
Batch 61/64 loss: -0.3539443016052246
Batch 62/64 loss: -0.23675870895385742
Batch 63/64 loss: 1.0912761688232422
Batch 64/64 loss: -3.6847238540649414
Epoch 107  Train loss: -0.13661953421200024  Val loss: -0.34267417671754186
Epoch 108
-------------------------------
Batch 1/64 loss: -0.06675148010253906
Batch 2/64 loss: 0.027582645416259766
Batch 3/64 loss: -0.38378238677978516
Batch 4/64 loss: -0.30640554428100586
Batch 5/64 loss: -0.2228846549987793
Batch 6/64 loss: -0.294985294342041
Batch 7/64 loss: -0.2973365783691406
Batch 8/64 loss: -0.22250699996948242
Batch 9/64 loss: -0.10096502304077148
Batch 10/64 loss: -0.24629497528076172
Batch 11/64 loss: -0.3479771614074707
Batch 12/64 loss: -0.1976785659790039
Batch 13/64 loss: -0.20340871810913086
Batch 14/64 loss: -0.3210105895996094
Batch 15/64 loss: -0.34293556213378906
Batch 16/64 loss: -0.26697444915771484
Batch 17/64 loss: -0.08817529678344727
Batch 18/64 loss: -0.3006277084350586
Batch 19/64 loss: -0.4794120788574219
Batch 20/64 loss: -0.33113574981689453
Batch 21/64 loss: -0.3724536895751953
Batch 22/64 loss: -0.1657576560974121
Batch 23/64 loss: -0.3012700080871582
Batch 24/64 loss: 0.3824782371520996
Batch 25/64 loss: 1.2985000610351562
Batch 26/64 loss: -0.14662504196166992
Batch 27/64 loss: 0.08194637298583984
Batch 28/64 loss: -0.0026617050170898438
Batch 29/64 loss: -0.11972999572753906
Batch 30/64 loss: 0.05693769454956055
Batch 31/64 loss: -0.0684051513671875
Batch 32/64 loss: 0.8673014640808105
Batch 33/64 loss: -0.26908302307128906
Batch 34/64 loss: -0.3220067024230957
Batch 35/64 loss: -0.38582420349121094
Batch 36/64 loss: -0.0820302963256836
Batch 37/64 loss: -0.23096942901611328
Batch 38/64 loss: -0.2778949737548828
Batch 39/64 loss: -0.1353917121887207
Batch 40/64 loss: -0.21039628982543945
Batch 41/64 loss: -0.07862710952758789
Batch 42/64 loss: -0.3343696594238281
Batch 43/64 loss: -0.345395565032959
Batch 44/64 loss: -0.2500324249267578
Batch 45/64 loss: 0.039362430572509766
Batch 46/64 loss: -0.31023502349853516
Batch 47/64 loss: -0.2710132598876953
Batch 48/64 loss: -0.20734930038452148
Batch 49/64 loss: 0.7902421951293945
Batch 50/64 loss: -0.24517536163330078
Batch 51/64 loss: -0.20605993270874023
Batch 52/64 loss: 0.13669633865356445
Batch 53/64 loss: -0.26222991943359375
Batch 54/64 loss: -0.25594186782836914
Batch 55/64 loss: -0.3048396110534668
Batch 56/64 loss: -0.39406871795654297
Batch 57/64 loss: 0.3467893600463867
Batch 58/64 loss: -0.036971092224121094
Batch 59/64 loss: -0.19508647918701172
Batch 60/64 loss: -0.13394832611083984
Batch 61/64 loss: -0.3036775588989258
Batch 62/64 loss: -0.37542104721069336
Batch 63/64 loss: -0.1006932258605957
Batch 64/64 loss: -3.702272891998291
Epoch 108  Train loss: -0.17994905172609815  Val loss: -0.09628590849257007
Epoch 109
-------------------------------
Batch 1/64 loss: -0.370974063873291
Batch 2/64 loss: 0.8108916282653809
Batch 3/64 loss: -0.045203208923339844
Batch 4/64 loss: -0.05252838134765625
Batch 5/64 loss: -0.4337167739868164
Batch 6/64 loss: -0.3357572555541992
Batch 7/64 loss: -0.28920745849609375
Batch 8/64 loss: -0.2774639129638672
Batch 9/64 loss: -0.3103666305541992
Batch 10/64 loss: -0.1241292953491211
Batch 11/64 loss: -0.10159826278686523
Batch 12/64 loss: 0.19134140014648438
Batch 13/64 loss: 0.15811634063720703
Batch 14/64 loss: -0.397885799407959
Batch 15/64 loss: -0.23001670837402344
Batch 16/64 loss: -0.061075687408447266
Batch 17/64 loss: -0.0410313606262207
Batch 18/64 loss: -0.005385875701904297
Batch 19/64 loss: 0.3385348320007324
Batch 20/64 loss: -0.3955111503601074
Batch 21/64 loss: -0.2692723274230957
Batch 22/64 loss: -0.15704727172851562
Batch 23/64 loss: -0.20685386657714844
Batch 24/64 loss: -0.27988719940185547
Batch 25/64 loss: 0.016911983489990234
Batch 26/64 loss: -0.19964075088500977
Batch 27/64 loss: 1.5726299285888672
Batch 28/64 loss: -0.37589359283447266
Batch 29/64 loss: -0.328951358795166
Batch 30/64 loss: -0.2851691246032715
Batch 31/64 loss: -0.32217931747436523
Batch 32/64 loss: -0.28082752227783203
Batch 33/64 loss: 0.1028437614440918
Batch 34/64 loss: -0.2085404396057129
Batch 35/64 loss: -0.23898839950561523
Batch 36/64 loss: -0.3067665100097656
Batch 37/64 loss: -0.15268707275390625
Batch 38/64 loss: -0.22594976425170898
Batch 39/64 loss: -0.05774259567260742
Batch 40/64 loss: -0.31978607177734375
Batch 41/64 loss: -0.33284997940063477
Batch 42/64 loss: 0.06498241424560547
Batch 43/64 loss: -0.45317983627319336
Batch 44/64 loss: 0.8220663070678711
Batch 45/64 loss: -0.29100894927978516
Batch 46/64 loss: -0.4292316436767578
Batch 47/64 loss: -0.1473541259765625
Batch 48/64 loss: -0.2865633964538574
Batch 49/64 loss: 0.06522226333618164
Batch 50/64 loss: -0.2672758102416992
Batch 51/64 loss: -0.3404383659362793
Batch 52/64 loss: -0.31173276901245117
Batch 53/64 loss: -0.4553341865539551
Batch 54/64 loss: -0.18276691436767578
Batch 55/64 loss: -0.4704442024230957
Batch 56/64 loss: -0.216827392578125
Batch 57/64 loss: -0.37979698181152344
Batch 58/64 loss: -0.29700469970703125
Batch 59/64 loss: -0.5012998580932617
Batch 60/64 loss: 0.6619410514831543
Batch 61/64 loss: -0.3230628967285156
Batch 62/64 loss: -0.2986431121826172
Batch 63/64 loss: -0.49620962142944336
Batch 64/64 loss: -3.657845973968506
Epoch 109  Train loss: -0.18991313448139266  Val loss: -0.3982836536525451
Epoch 110
-------------------------------
Batch 1/64 loss: 0.07096719741821289
Batch 2/64 loss: -0.49384546279907227
Batch 3/64 loss: -0.131256103515625
Batch 4/64 loss: 0.22962665557861328
Batch 5/64 loss: -0.18770456314086914
Batch 6/64 loss: -0.2176508903503418
Batch 7/64 loss: -0.13283967971801758
Batch 8/64 loss: -0.013539314270019531
Batch 9/64 loss: -0.27711963653564453
Batch 10/64 loss: 0.06131553649902344
Batch 11/64 loss: -0.22733831405639648
Batch 12/64 loss: -0.2493457794189453
Batch 13/64 loss: -0.07223320007324219
Batch 14/64 loss: -0.14396333694458008
Batch 15/64 loss: -0.3331928253173828
Batch 16/64 loss: -0.12342071533203125
Batch 17/64 loss: 0.14638805389404297
Batch 18/64 loss: -0.1976776123046875
Batch 19/64 loss: 0.044272422790527344
Batch 20/64 loss: -0.34784841537475586
Batch 21/64 loss: -0.33557939529418945
Batch 22/64 loss: -0.15438270568847656
Batch 23/64 loss: -0.3836379051208496
Batch 24/64 loss: -0.05913591384887695
Batch 25/64 loss: -0.06153726577758789
Batch 26/64 loss: -0.3396611213684082
Batch 27/64 loss: -0.16034317016601562
Batch 28/64 loss: -0.12572908401489258
Batch 29/64 loss: -0.3913607597351074
Batch 30/64 loss: -0.4016914367675781
Batch 31/64 loss: 2.1315574645996094
Batch 32/64 loss: -0.2359447479248047
Batch 33/64 loss: 0.24401617050170898
Batch 34/64 loss: -0.26398134231567383
Batch 35/64 loss: -0.2523493766784668
Batch 36/64 loss: -0.16782808303833008
Batch 37/64 loss: -0.33911752700805664
Batch 38/64 loss: 0.13974380493164062
Batch 39/64 loss: -0.13374900817871094
Batch 40/64 loss: -0.08886241912841797
Batch 41/64 loss: 0.14457941055297852
Batch 42/64 loss: 0.16294193267822266
Batch 43/64 loss: -0.37625598907470703
Batch 44/64 loss: -0.3744034767150879
Batch 45/64 loss: -0.07608938217163086
Batch 46/64 loss: 0.24933815002441406
Batch 47/64 loss: -0.1498403549194336
Batch 48/64 loss: -0.3624587059020996
Batch 49/64 loss: -0.3609137535095215
Batch 50/64 loss: -0.06556224822998047
Batch 51/64 loss: -0.29811525344848633
Batch 52/64 loss: -0.03667736053466797
Batch 53/64 loss: 0.1476426124572754
Batch 54/64 loss: 1.5262136459350586
Batch 55/64 loss: -0.2189927101135254
Batch 56/64 loss: -0.14499187469482422
Batch 57/64 loss: -0.26407384872436523
Batch 58/64 loss: 0.4066324234008789
Batch 59/64 loss: 0.3889627456665039
Batch 60/64 loss: -0.26507139205932617
Batch 61/64 loss: -0.19966411590576172
Batch 62/64 loss: -0.19672250747680664
Batch 63/64 loss: -0.20766496658325195
Batch 64/64 loss: -3.7525949478149414
Epoch 110  Train loss: -0.11547628290512983  Val loss: -0.48463007346870973
Epoch 111
-------------------------------
Batch 1/64 loss: -0.18392562866210938
Batch 2/64 loss: -0.1737060546875
Batch 3/64 loss: 0.09260940551757812
Batch 4/64 loss: -0.15654706954956055
Batch 5/64 loss: -0.22940301895141602
Batch 6/64 loss: -0.21805381774902344
Batch 7/64 loss: -0.1864461898803711
Batch 8/64 loss: -0.01905679702758789
Batch 9/64 loss: -0.26186132431030273
Batch 10/64 loss: 0.6050868034362793
Batch 11/64 loss: -0.11057710647583008
Batch 12/64 loss: -0.2744140625
Batch 13/64 loss: -0.43641185760498047
Batch 14/64 loss: -0.11196422576904297
Batch 15/64 loss: -0.3263053894042969
Batch 16/64 loss: 0.2536602020263672
Batch 17/64 loss: -0.05490684509277344
Batch 18/64 loss: -0.32393407821655273
Batch 19/64 loss: -0.3118753433227539
Batch 20/64 loss: -0.38544607162475586
Batch 21/64 loss: -0.34245777130126953
Batch 22/64 loss: -0.291750431060791
Batch 23/64 loss: -0.18262434005737305
Batch 24/64 loss: 3.4281272888183594
Batch 25/64 loss: -0.3535628318786621
Batch 26/64 loss: -0.39791107177734375
Batch 27/64 loss: -0.29292821884155273
Batch 28/64 loss: -0.15176916122436523
Batch 29/64 loss: -0.4753227233886719
Batch 30/64 loss: -0.06123971939086914
Batch 31/64 loss: -0.36930274963378906
Batch 32/64 loss: 0.40816497802734375
Batch 33/64 loss: -0.4065251350402832
Batch 34/64 loss: -0.37492942810058594
Batch 35/64 loss: -0.36926698684692383
Batch 36/64 loss: -0.14412927627563477
Batch 37/64 loss: -0.17010164260864258
Batch 38/64 loss: -0.3464527130126953
Batch 39/64 loss: -0.3927288055419922
Batch 40/64 loss: -0.030647754669189453
Batch 41/64 loss: -0.3585071563720703
Batch 42/64 loss: -0.47357749938964844
Batch 43/64 loss: -0.2889747619628906
Batch 44/64 loss: -0.12028789520263672
Batch 45/64 loss: 0.5161981582641602
Batch 46/64 loss: -0.17679548263549805
Batch 47/64 loss: 0.2881617546081543
Batch 48/64 loss: -0.2536783218383789
Batch 49/64 loss: -0.11644983291625977
Batch 50/64 loss: 0.5569353103637695
Batch 51/64 loss: -0.16620969772338867
Batch 52/64 loss: -0.31039905548095703
Batch 53/64 loss: -0.42467451095581055
Batch 54/64 loss: -0.330078125
Batch 55/64 loss: -0.3207430839538574
Batch 56/64 loss: -0.2083606719970703
Batch 57/64 loss: 0.20359086990356445
Batch 58/64 loss: -0.23563814163208008
Batch 59/64 loss: -0.3486747741699219
Batch 60/64 loss: -0.4271373748779297
Batch 61/64 loss: -0.3235812187194824
Batch 62/64 loss: -0.1073298454284668
Batch 63/64 loss: -0.33212757110595703
Batch 64/64 loss: -3.7384800910949707
Epoch 111  Train loss: -0.16773389741486194  Val loss: -0.47762791479576083
Epoch 112
-------------------------------
Batch 1/64 loss: -0.11048126220703125
Batch 2/64 loss: 0.10459423065185547
Batch 3/64 loss: -0.04572916030883789
Batch 4/64 loss: -0.18346309661865234
Batch 5/64 loss: -0.16249656677246094
Batch 6/64 loss: -0.11850690841674805
Batch 7/64 loss: -0.17279767990112305
Batch 8/64 loss: -0.39599037170410156
Batch 9/64 loss: -0.144378662109375
Batch 10/64 loss: 0.36934375762939453
Batch 11/64 loss: -0.07691001892089844
Batch 12/64 loss: 0.3223137855529785
Batch 13/64 loss: -0.3493080139160156
Batch 14/64 loss: -0.3447141647338867
Batch 15/64 loss: -0.24842309951782227
Batch 16/64 loss: -0.26589488983154297
Batch 17/64 loss: -0.1577925682067871
Batch 18/64 loss: -0.0015101432800292969
Batch 19/64 loss: -0.10979366302490234
Batch 20/64 loss: 0.0402684211730957
Batch 21/64 loss: 1.5223679542541504
Batch 22/64 loss: 0.14603233337402344
Batch 23/64 loss: -0.3810453414916992
Batch 24/64 loss: -0.18945026397705078
Batch 25/64 loss: -0.1728529930114746
Batch 26/64 loss: -0.22789812088012695
Batch 27/64 loss: 1.1920509338378906
Batch 28/64 loss: -0.16241836547851562
Batch 29/64 loss: 1.0443763732910156
Batch 30/64 loss: -0.07989072799682617
Batch 31/64 loss: -0.08321714401245117
Batch 32/64 loss: -0.10312938690185547
Batch 33/64 loss: -0.24041175842285156
Batch 34/64 loss: -0.2302565574645996
Batch 35/64 loss: -0.1290760040283203
Batch 36/64 loss: 0.020519256591796875
Batch 37/64 loss: -0.23198652267456055
Batch 38/64 loss: -0.16631221771240234
Batch 39/64 loss: -0.07891178131103516
Batch 40/64 loss: -0.0754542350769043
Batch 41/64 loss: -0.14536571502685547
Batch 42/64 loss: -0.25780391693115234
Batch 43/64 loss: -0.3245244026184082
Batch 44/64 loss: -0.210693359375
Batch 45/64 loss: -0.3600449562072754
Batch 46/64 loss: -0.23067426681518555
Batch 47/64 loss: 0.261563777923584
Batch 48/64 loss: -0.2051553726196289
Batch 49/64 loss: 0.016463279724121094
Batch 50/64 loss: -0.06922626495361328
Batch 51/64 loss: -0.3383927345275879
Batch 52/64 loss: -0.3550229072570801
Batch 53/64 loss: -0.4076218605041504
Batch 54/64 loss: -0.09379816055297852
Batch 55/64 loss: -0.4033937454223633
Batch 56/64 loss: -0.2869296073913574
Batch 57/64 loss: -0.13026142120361328
Batch 58/64 loss: 0.7192254066467285
Batch 59/64 loss: -0.07871580123901367
Batch 60/64 loss: 0.01433563232421875
Batch 61/64 loss: -0.24500131607055664
Batch 62/64 loss: -0.2119593620300293
Batch 63/64 loss: -0.10139226913452148
Batch 64/64 loss: -3.698112964630127
Epoch 112  Train loss: -0.10818209741629806  Val loss: -0.3989204459173982
Epoch 113
-------------------------------
Batch 1/64 loss: 0.38819313049316406
Batch 2/64 loss: -0.3657989501953125
Batch 3/64 loss: -0.08978939056396484
Batch 4/64 loss: 1.0603418350219727
Batch 5/64 loss: -0.20235824584960938
Batch 6/64 loss: -0.43711423873901367
Batch 7/64 loss: 0.2863802909851074
Batch 8/64 loss: -0.23940563201904297
Batch 9/64 loss: 0.029444217681884766
Batch 10/64 loss: -0.2360401153564453
Batch 11/64 loss: -0.12296247482299805
Batch 12/64 loss: -0.26326656341552734
Batch 13/64 loss: -0.12955856323242188
Batch 14/64 loss: -0.31357383728027344
Batch 15/64 loss: -0.17373323440551758
Batch 16/64 loss: 0.585719108581543
Batch 17/64 loss: -0.2962021827697754
Batch 18/64 loss: -0.116851806640625
Batch 19/64 loss: -0.001956462860107422
Batch 20/64 loss: -0.11109685897827148
Batch 21/64 loss: -0.3842153549194336
Batch 22/64 loss: -0.3518228530883789
Batch 23/64 loss: -0.41160058975219727
Batch 24/64 loss: -0.3541984558105469
Batch 25/64 loss: -0.24181890487670898
Batch 26/64 loss: -0.12166738510131836
Batch 27/64 loss: 0.9469642639160156
Batch 28/64 loss: 0.0036754608154296875
Batch 29/64 loss: -0.1810436248779297
Batch 30/64 loss: -0.29282712936401367
Batch 31/64 loss: 0.17603254318237305
Batch 32/64 loss: -0.18019580841064453
Batch 33/64 loss: -0.2501091957092285
Batch 34/64 loss: -0.30584239959716797
Batch 35/64 loss: -0.2681279182434082
Batch 36/64 loss: -0.09566831588745117
Batch 37/64 loss: -0.09691524505615234
Batch 38/64 loss: 0.10351133346557617
Batch 39/64 loss: -0.47475194931030273
Batch 40/64 loss: -0.2989230155944824
Batch 41/64 loss: -0.35564756393432617
Batch 42/64 loss: -0.4414830207824707
Batch 43/64 loss: -0.342071533203125
Batch 44/64 loss: -0.4378695487976074
Batch 45/64 loss: 0.03835296630859375
Batch 46/64 loss: -0.19850444793701172
Batch 47/64 loss: -0.22918128967285156
Batch 48/64 loss: -0.18904447555541992
Batch 49/64 loss: -0.2523965835571289
Batch 50/64 loss: -0.023772716522216797
Batch 51/64 loss: -0.29579639434814453
Batch 52/64 loss: -0.29517221450805664
Batch 53/64 loss: 0.47690248489379883
Batch 54/64 loss: -0.25906848907470703
Batch 55/64 loss: -0.24700403213500977
Batch 56/64 loss: -0.3690052032470703
Batch 57/64 loss: 1.3636159896850586
Batch 58/64 loss: -0.04949188232421875
Batch 59/64 loss: -0.0030159950256347656
Batch 60/64 loss: -0.005700588226318359
Batch 61/64 loss: -0.1336808204650879
Batch 62/64 loss: -0.2408156394958496
Batch 63/64 loss: -0.21360445022583008
Batch 64/64 loss: -3.6631698608398438
Epoch 113  Train loss: -0.14556874293907016  Val loss: -0.36277396080829843
Epoch 114
-------------------------------
Batch 1/64 loss: -0.2214832305908203
Batch 2/64 loss: -0.3599100112915039
Batch 3/64 loss: -0.2887115478515625
Batch 4/64 loss: -0.3580799102783203
Batch 5/64 loss: -0.43059301376342773
Batch 6/64 loss: -0.3820171356201172
Batch 7/64 loss: -0.2208414077758789
Batch 8/64 loss: -0.21615839004516602
Batch 9/64 loss: -0.22829151153564453
Batch 10/64 loss: -0.4060988426208496
Batch 11/64 loss: -0.4125857353210449
Batch 12/64 loss: -0.45075368881225586
Batch 13/64 loss: -0.15062904357910156
Batch 14/64 loss: -0.3620181083679199
Batch 15/64 loss: -0.19798994064331055
Batch 16/64 loss: -0.003437519073486328
Batch 17/64 loss: -0.23277044296264648
Batch 18/64 loss: -0.17958354949951172
Batch 19/64 loss: -0.27020978927612305
Batch 20/64 loss: -0.19776344299316406
Batch 21/64 loss: -0.321016788482666
Batch 22/64 loss: -0.2833900451660156
Batch 23/64 loss: -0.2581057548522949
Batch 24/64 loss: -0.33660173416137695
Batch 25/64 loss: -0.15159845352172852
Batch 26/64 loss: -0.25481224060058594
Batch 27/64 loss: -0.06094694137573242
Batch 28/64 loss: 0.02908802032470703
Batch 29/64 loss: -0.10988998413085938
Batch 30/64 loss: -0.27718544006347656
Batch 31/64 loss: 0.49245357513427734
Batch 32/64 loss: 0.25335073471069336
Batch 33/64 loss: -0.2575554847717285
Batch 34/64 loss: -0.11891746520996094
Batch 35/64 loss: -0.27910470962524414
Batch 36/64 loss: -0.23124456405639648
Batch 37/64 loss: -0.2707638740539551
Batch 38/64 loss: -0.27405738830566406
Batch 39/64 loss: 0.48412132263183594
Batch 40/64 loss: -0.02088451385498047
Batch 41/64 loss: -0.15451431274414062
Batch 42/64 loss: 0.07118749618530273
Batch 43/64 loss: -0.19894742965698242
Batch 44/64 loss: -0.1254267692565918
Batch 45/64 loss: 1.3101844787597656
Batch 46/64 loss: 0.22633790969848633
Batch 47/64 loss: 0.14397621154785156
Batch 48/64 loss: 0.9111323356628418
Batch 49/64 loss: -0.16465330123901367
Batch 50/64 loss: 0.012812614440917969
Batch 51/64 loss: -0.20472288131713867
Batch 52/64 loss: -0.2322831153869629
Batch 53/64 loss: -0.21161127090454102
Batch 54/64 loss: -0.24408483505249023
Batch 55/64 loss: 0.897921085357666
Batch 56/64 loss: -0.2590456008911133
Batch 57/64 loss: -0.29525136947631836
Batch 58/64 loss: -0.0823354721069336
Batch 59/64 loss: -0.18995904922485352
Batch 60/64 loss: -0.23759984970092773
Batch 61/64 loss: -0.2789125442504883
Batch 62/64 loss: -0.3732333183288574
Batch 63/64 loss: -0.30008935928344727
Batch 64/64 loss: -3.771904945373535
Epoch 114  Train loss: -0.16666721643186083  Val loss: -0.4365760829440507
Epoch 115
-------------------------------
Batch 1/64 loss: -0.3805985450744629
Batch 2/64 loss: 0.04410409927368164
Batch 3/64 loss: -0.4562797546386719
Batch 4/64 loss: -0.32721614837646484
Batch 5/64 loss: -0.43186521530151367
Batch 6/64 loss: -0.08748626708984375
Batch 7/64 loss: 1.1605134010314941
Batch 8/64 loss: -0.2439570426940918
Batch 9/64 loss: 0.13413047790527344
Batch 10/64 loss: -0.4773564338684082
Batch 11/64 loss: -0.40717124938964844
Batch 12/64 loss: -0.4923248291015625
Batch 13/64 loss: -0.16994047164916992
Batch 14/64 loss: -0.30440282821655273
Batch 15/64 loss: -0.35473060607910156
Batch 16/64 loss: -0.3792719841003418
Batch 17/64 loss: -0.4151144027709961
Batch 18/64 loss: -0.3069181442260742
Batch 19/64 loss: -0.33088064193725586
Batch 20/64 loss: -0.07074165344238281
Batch 21/64 loss: -0.3294520378112793
Batch 22/64 loss: -0.3322014808654785
Batch 23/64 loss: -0.21224069595336914
Batch 24/64 loss: 0.9174613952636719
Batch 25/64 loss: -0.3541998863220215
Batch 26/64 loss: -0.626859188079834
Batch 27/64 loss: 0.2100996971130371
Batch 28/64 loss: -0.24207592010498047
Batch 29/64 loss: -0.48419666290283203
Batch 30/64 loss: -0.24903011322021484
Batch 31/64 loss: -0.1291031837463379
Batch 32/64 loss: -0.206817626953125
Batch 33/64 loss: -0.4212331771850586
Batch 34/64 loss: -0.24251985549926758
Batch 35/64 loss: -0.19110488891601562
Batch 36/64 loss: -0.21816396713256836
Batch 37/64 loss: 1.0123882293701172
Batch 38/64 loss: -0.014564037322998047
Batch 39/64 loss: -0.2917766571044922
Batch 40/64 loss: -0.21213769912719727
Batch 41/64 loss: -0.24936914443969727
Batch 42/64 loss: -0.22449827194213867
Batch 43/64 loss: 0.043242454528808594
Batch 44/64 loss: -0.23490095138549805
Batch 45/64 loss: 0.0598297119140625
Batch 46/64 loss: -0.36050844192504883
Batch 47/64 loss: 0.32754039764404297
Batch 48/64 loss: -0.3471651077270508
Batch 49/64 loss: -0.3440732955932617
Batch 50/64 loss: -0.1813359260559082
Batch 51/64 loss: -0.34772777557373047
Batch 52/64 loss: -0.33837461471557617
Batch 53/64 loss: -0.3050670623779297
Batch 54/64 loss: -0.2445073127746582
Batch 55/64 loss: -0.2127680778503418
Batch 56/64 loss: -0.17460393905639648
Batch 57/64 loss: -0.34638023376464844
Batch 58/64 loss: -0.1365513801574707
Batch 59/64 loss: -0.18186283111572266
Batch 60/64 loss: -0.15963220596313477
Batch 61/64 loss: -0.39565277099609375
Batch 62/64 loss: -0.4251570701599121
Batch 63/64 loss: -0.39609718322753906
Batch 64/64 loss: -3.921631336212158
Epoch 115  Train loss: -0.23579734166463215  Val loss: -0.4185761389453796
Epoch 116
-------------------------------
Batch 1/64 loss: -0.34970808029174805
Batch 2/64 loss: -0.17952203750610352
Batch 3/64 loss: -0.3644280433654785
Batch 4/64 loss: -0.18822860717773438
Batch 5/64 loss: -0.3911776542663574
Batch 6/64 loss: 1.2465534210205078
Batch 7/64 loss: 0.06429910659790039
Batch 8/64 loss: -0.25638246536254883
Batch 9/64 loss: -0.38218069076538086
Batch 10/64 loss: -0.23778343200683594
Batch 11/64 loss: -0.27701377868652344
Batch 12/64 loss: -0.41556310653686523
Batch 13/64 loss: -0.24287986755371094
Batch 14/64 loss: -0.23761892318725586
Batch 15/64 loss: -0.3610496520996094
Batch 16/64 loss: -0.2791633605957031
Batch 17/64 loss: -0.41795778274536133
Batch 18/64 loss: -0.14104700088500977
Batch 19/64 loss: -0.41832494735717773
Batch 20/64 loss: -0.03420591354370117
Batch 21/64 loss: -0.36153316497802734
Batch 22/64 loss: -0.3769111633300781
Batch 23/64 loss: -0.38367128372192383
Batch 24/64 loss: -0.19453716278076172
Batch 25/64 loss: -0.2631959915161133
Batch 26/64 loss: -0.47241783142089844
Batch 27/64 loss: -0.2264866828918457
Batch 28/64 loss: -0.4273405075073242
Batch 29/64 loss: -0.27837610244750977
Batch 30/64 loss: -0.17501401901245117
Batch 31/64 loss: -0.3794088363647461
Batch 32/64 loss: -0.4779024124145508
Batch 33/64 loss: -0.5132455825805664
Batch 34/64 loss: -0.30254411697387695
Batch 35/64 loss: -0.4139370918273926
Batch 36/64 loss: -0.046541690826416016
Batch 37/64 loss: 0.7273979187011719
Batch 38/64 loss: -0.5062289237976074
Batch 39/64 loss: -0.4946556091308594
Batch 40/64 loss: -0.20214223861694336
Batch 41/64 loss: 0.9387063980102539
Batch 42/64 loss: -0.3683915138244629
Batch 43/64 loss: -0.3292045593261719
Batch 44/64 loss: -0.3280344009399414
Batch 45/64 loss: -0.4762883186340332
Batch 46/64 loss: -0.44373607635498047
Batch 47/64 loss: -0.2675766944885254
Batch 48/64 loss: -0.3636474609375
Batch 49/64 loss: -0.13325786590576172
Batch 50/64 loss: -0.03623199462890625
Batch 51/64 loss: -0.23568058013916016
Batch 52/64 loss: -0.49114322662353516
Batch 53/64 loss: -0.3264460563659668
Batch 54/64 loss: -0.4019198417663574
Batch 55/64 loss: -0.39668989181518555
Batch 56/64 loss: -0.47954845428466797
Batch 57/64 loss: -0.1899261474609375
Batch 58/64 loss: -0.3329453468322754
Batch 59/64 loss: -0.34356069564819336
Batch 60/64 loss: -0.5204892158508301
Batch 61/64 loss: 0.28600454330444336
Batch 62/64 loss: 0.49489593505859375
Batch 63/64 loss: -0.26934194564819336
Batch 64/64 loss: -3.9508056640625
Epoch 116  Train loss: -0.27622953676709944  Val loss: -0.5423978300848368
Epoch 117
-------------------------------
Batch 1/64 loss: -0.485292911529541
Batch 2/64 loss: -0.2923612594604492
Batch 3/64 loss: -0.23508453369140625
Batch 4/64 loss: -0.17142963409423828
Batch 5/64 loss: 0.21640300750732422
Batch 6/64 loss: -0.3249030113220215
Batch 7/64 loss: -0.4085960388183594
Batch 8/64 loss: -0.42099905014038086
Batch 9/64 loss: 0.8214139938354492
Batch 10/64 loss: -0.3497781753540039
Batch 11/64 loss: 0.06794118881225586
Batch 12/64 loss: -0.3725581169128418
Batch 13/64 loss: 0.17888975143432617
Batch 14/64 loss: -0.4073910713195801
Batch 15/64 loss: 0.05727052688598633
Batch 16/64 loss: -0.37744760513305664
Batch 17/64 loss: -0.08829116821289062
Batch 18/64 loss: -0.3535275459289551
Batch 19/64 loss: -0.40451860427856445
Batch 20/64 loss: 0.22078418731689453
Batch 21/64 loss: -0.10545825958251953
Batch 22/64 loss: -0.06313133239746094
Batch 23/64 loss: -0.22965478897094727
Batch 24/64 loss: -0.008446216583251953
Batch 25/64 loss: -0.19780874252319336
Batch 26/64 loss: -0.21447515487670898
Batch 27/64 loss: -0.3403353691101074
Batch 28/64 loss: 0.05004262924194336
Batch 29/64 loss: 0.9611272811889648
Batch 30/64 loss: -0.14011859893798828
Batch 31/64 loss: -0.21767663955688477
Batch 32/64 loss: -0.25647449493408203
Batch 33/64 loss: -0.3426628112792969
Batch 34/64 loss: -0.28635501861572266
Batch 35/64 loss: -0.41585445404052734
Batch 36/64 loss: -0.4657602310180664
Batch 37/64 loss: -0.29799699783325195
Batch 38/64 loss: -0.2854623794555664
Batch 39/64 loss: -0.42249584197998047
Batch 40/64 loss: -0.11791086196899414
Batch 41/64 loss: -0.36922359466552734
Batch 42/64 loss: -0.38071107864379883
Batch 43/64 loss: -0.35142993927001953
Batch 44/64 loss: -0.353238582611084
Batch 45/64 loss: -0.2841205596923828
Batch 46/64 loss: -0.44163084030151367
Batch 47/64 loss: -0.2853531837463379
Batch 48/64 loss: -0.3869509696960449
Batch 49/64 loss: -0.4291691780090332
Batch 50/64 loss: -0.46243906021118164
Batch 51/64 loss: -0.6088132858276367
Batch 52/64 loss: -0.37691736221313477
Batch 53/64 loss: 1.4629969596862793
Batch 54/64 loss: -0.374392032623291
Batch 55/64 loss: -0.2377772331237793
Batch 56/64 loss: -0.4367074966430664
Batch 57/64 loss: -0.19571971893310547
Batch 58/64 loss: -0.27892303466796875
Batch 59/64 loss: -0.03654956817626953
Batch 60/64 loss: -0.4318962097167969
Batch 61/64 loss: -0.3061699867248535
Batch 62/64 loss: -0.2160782814025879
Batch 63/64 loss: -0.0354924201965332
Batch 64/64 loss: -3.8557558059692383
Epoch 117  Train loss: -0.23897894691018498  Val loss: -0.373747769909626
Epoch 118
-------------------------------
Batch 1/64 loss: -0.44992971420288086
Batch 2/64 loss: -0.30524730682373047
Batch 3/64 loss: -0.39168262481689453
Batch 4/64 loss: -0.333223819732666
Batch 5/64 loss: -0.1676778793334961
Batch 6/64 loss: 1.1273107528686523
Batch 7/64 loss: -0.34072113037109375
Batch 8/64 loss: -0.2570915222167969
Batch 9/64 loss: -0.4074883460998535
Batch 10/64 loss: -0.4164705276489258
Batch 11/64 loss: -0.32126474380493164
Batch 12/64 loss: -0.48552417755126953
Batch 13/64 loss: -0.2735171318054199
Batch 14/64 loss: -0.31833648681640625
Batch 15/64 loss: -0.3751096725463867
Batch 16/64 loss: -0.2816915512084961
Batch 17/64 loss: -0.3112525939941406
Batch 18/64 loss: 0.06060647964477539
Batch 19/64 loss: -0.31508350372314453
Batch 20/64 loss: -0.4187483787536621
Batch 21/64 loss: -0.22718238830566406
Batch 22/64 loss: -0.33791017532348633
Batch 23/64 loss: -0.1850757598876953
Batch 24/64 loss: 0.09248208999633789
Batch 25/64 loss: -0.38438940048217773
Batch 26/64 loss: -0.37853574752807617
Batch 27/64 loss: -0.38002586364746094
Batch 28/64 loss: -0.37320756912231445
Batch 29/64 loss: -0.33202409744262695
Batch 30/64 loss: -0.2610182762145996
Batch 31/64 loss: -0.33481597900390625
Batch 32/64 loss: -0.36016082763671875
Batch 33/64 loss: 0.8544678688049316
Batch 34/64 loss: -0.3334789276123047
Batch 35/64 loss: -0.48490476608276367
Batch 36/64 loss: -0.35111570358276367
Batch 37/64 loss: -0.39834165573120117
Batch 38/64 loss: -0.3658766746520996
Batch 39/64 loss: -0.07599210739135742
Batch 40/64 loss: -0.37123727798461914
Batch 41/64 loss: -0.4617905616760254
Batch 42/64 loss: -0.14634990692138672
Batch 43/64 loss: 0.24158430099487305
Batch 44/64 loss: -0.4310464859008789
Batch 45/64 loss: -0.44210386276245117
Batch 46/64 loss: -0.45225095748901367
Batch 47/64 loss: -0.44258975982666016
Batch 48/64 loss: -0.3908991813659668
Batch 49/64 loss: -0.31261396408081055
Batch 50/64 loss: -0.5061492919921875
Batch 51/64 loss: -0.24249505996704102
Batch 52/64 loss: -0.332244873046875
Batch 53/64 loss: -0.42903995513916016
Batch 54/64 loss: 0.7623443603515625
Batch 55/64 loss: -0.23465776443481445
Batch 56/64 loss: -0.4292430877685547
Batch 57/64 loss: -0.4829883575439453
Batch 58/64 loss: -0.3131899833679199
Batch 59/64 loss: -0.0020194053649902344
Batch 60/64 loss: -0.31667566299438477
Batch 61/64 loss: 0.2915205955505371
Batch 62/64 loss: -0.2109518051147461
Batch 63/64 loss: -0.2783207893371582
Batch 64/64 loss: -3.8001184463500977
Epoch 118  Train loss: -0.28835682214475145  Val loss: -0.6133627744065118
Saving best model, epoch: 118
Epoch 119
-------------------------------
Batch 1/64 loss: -0.42995595932006836
Batch 2/64 loss: -0.41599130630493164
Batch 3/64 loss: 0.09168767929077148
Batch 4/64 loss: -0.5300068855285645
Batch 5/64 loss: 1.1040253639221191
Batch 6/64 loss: -0.4273042678833008
Batch 7/64 loss: -0.30507421493530273
Batch 8/64 loss: -0.19304561614990234
Batch 9/64 loss: -0.5979652404785156
Batch 10/64 loss: -0.41657066345214844
Batch 11/64 loss: -0.4345078468322754
Batch 12/64 loss: -0.4191141128540039
Batch 13/64 loss: -0.49277400970458984
Batch 14/64 loss: -0.32700634002685547
Batch 15/64 loss: -0.4883742332458496
Batch 16/64 loss: -0.3639688491821289
Batch 17/64 loss: 1.3128738403320312
Batch 18/64 loss: -0.35927677154541016
Batch 19/64 loss: -0.4255995750427246
Batch 20/64 loss: -0.2564883232116699
Batch 21/64 loss: -0.2543020248413086
Batch 22/64 loss: -0.2983536720275879
Batch 23/64 loss: -0.3397073745727539
Batch 24/64 loss: -0.27502918243408203
Batch 25/64 loss: -0.4430522918701172
Batch 26/64 loss: -0.28198671340942383
Batch 27/64 loss: -0.46768951416015625
Batch 28/64 loss: -0.4902653694152832
Batch 29/64 loss: -0.06008481979370117
Batch 30/64 loss: -0.3101978302001953
Batch 31/64 loss: 0.7309260368347168
Batch 32/64 loss: -0.4517979621887207
Batch 33/64 loss: -0.36806631088256836
Batch 34/64 loss: -0.3662705421447754
Batch 35/64 loss: 0.08935832977294922
Batch 36/64 loss: -0.23997068405151367
Batch 37/64 loss: -0.3060646057128906
Batch 38/64 loss: -0.45489978790283203
Batch 39/64 loss: -0.19672203063964844
Batch 40/64 loss: -0.19620037078857422
Batch 41/64 loss: -0.18715667724609375
Batch 42/64 loss: -0.33905744552612305
Batch 43/64 loss: -0.4128389358520508
Batch 44/64 loss: -0.449892520904541
Batch 45/64 loss: -0.1327505111694336
Batch 46/64 loss: -0.279296875
Batch 47/64 loss: -0.35726356506347656
Batch 48/64 loss: -0.508537769317627
Batch 49/64 loss: -0.30972862243652344
Batch 50/64 loss: 0.12656354904174805
Batch 51/64 loss: 0.014322280883789062
Batch 52/64 loss: -0.0636129379272461
Batch 53/64 loss: -0.3169436454772949
Batch 54/64 loss: -0.4479193687438965
Batch 55/64 loss: -0.2711958885192871
Batch 56/64 loss: -0.37143373489379883
Batch 57/64 loss: -0.336550235748291
Batch 58/64 loss: -0.21074390411376953
Batch 59/64 loss: -0.1627335548400879
Batch 60/64 loss: -0.19246864318847656
Batch 61/64 loss: -0.4085817337036133
Batch 62/64 loss: 0.16667509078979492
Batch 63/64 loss: -0.42681121826171875
Batch 64/64 loss: -3.9507107734680176
Epoch 119  Train loss: -0.28542437646903246  Val loss: -0.4657419473444883
Epoch 120
-------------------------------
Batch 1/64 loss: -0.41683292388916016
Batch 2/64 loss: -0.1783146858215332
Batch 3/64 loss: 0.36629295349121094
Batch 4/64 loss: -0.3173065185546875
Batch 5/64 loss: -0.37202882766723633
Batch 6/64 loss: -0.41415977478027344
Batch 7/64 loss: -0.2876315116882324
Batch 8/64 loss: -0.22632551193237305
Batch 9/64 loss: 0.0279998779296875
Batch 10/64 loss: -0.2736544609069824
Batch 11/64 loss: -0.1320810317993164
Batch 12/64 loss: -0.2093658447265625
Batch 13/64 loss: -0.4086627960205078
Batch 14/64 loss: -0.16452836990356445
Batch 15/64 loss: -0.24395132064819336
Batch 16/64 loss: -0.13394546508789062
Batch 17/64 loss: -0.2728705406188965
Batch 18/64 loss: -0.3360614776611328
Batch 19/64 loss: -0.31860923767089844
Batch 20/64 loss: -0.30322742462158203
Batch 21/64 loss: -0.3609147071838379
Batch 22/64 loss: 0.07777547836303711
Batch 23/64 loss: -0.19579410552978516
Batch 24/64 loss: 0.8524961471557617
Batch 25/64 loss: -0.23682308197021484
Batch 26/64 loss: -0.31421327590942383
Batch 27/64 loss: -0.14316415786743164
Batch 28/64 loss: -0.4407033920288086
Batch 29/64 loss: -0.41146373748779297
Batch 30/64 loss: -0.3346714973449707
Batch 31/64 loss: -0.39858293533325195
Batch 32/64 loss: -0.3065190315246582
Batch 33/64 loss: 0.06566619873046875
Batch 34/64 loss: -0.3600296974182129
Batch 35/64 loss: -0.21930217742919922
Batch 36/64 loss: -0.33980846405029297
Batch 37/64 loss: -0.30253124237060547
Batch 38/64 loss: -0.42894554138183594
Batch 39/64 loss: -0.2833743095397949
Batch 40/64 loss: -0.32579898834228516
Batch 41/64 loss: -0.4520888328552246
Batch 42/64 loss: 0.06229543685913086
Batch 43/64 loss: -0.19586706161499023
Batch 44/64 loss: 1.1996679306030273
Batch 45/64 loss: -0.27687644958496094
Batch 46/64 loss: -0.2439436912536621
Batch 47/64 loss: -0.24896001815795898
Batch 48/64 loss: -0.31302356719970703
Batch 49/64 loss: -0.32790184020996094
Batch 50/64 loss: -0.43547534942626953
Batch 51/64 loss: -0.038039207458496094
Batch 52/64 loss: -0.3074522018432617
Batch 53/64 loss: -0.05253267288208008
Batch 54/64 loss: 0.12287139892578125
Batch 55/64 loss: -0.41595458984375
Batch 56/64 loss: -0.5772318840026855
Batch 57/64 loss: -0.2644014358520508
Batch 58/64 loss: -0.24096155166625977
Batch 59/64 loss: -0.4611239433288574
Batch 60/64 loss: -0.2208724021911621
Batch 61/64 loss: 0.6097683906555176
Batch 62/64 loss: -0.20660734176635742
Batch 63/64 loss: -0.3036017417907715
Batch 64/64 loss: -3.3113913536071777
Epoch 120  Train loss: -0.23676585964128083  Val loss: -0.556625431755564
Epoch 121
-------------------------------
Batch 1/64 loss: -0.3046598434448242
Batch 2/64 loss: -0.08464527130126953
Batch 3/64 loss: -0.38811540603637695
Batch 4/64 loss: -0.35392045974731445
Batch 5/64 loss: -0.35659360885620117
Batch 6/64 loss: -0.4623837471008301
Batch 7/64 loss: -0.4666938781738281
Batch 8/64 loss: -0.41148853302001953
Batch 9/64 loss: -0.44089317321777344
Batch 10/64 loss: -0.5159649848937988
Batch 11/64 loss: -0.39186859130859375
Batch 12/64 loss: -0.17762994766235352
Batch 13/64 loss: -0.44426488876342773
Batch 14/64 loss: -0.3461771011352539
Batch 15/64 loss: 1.4029645919799805
Batch 16/64 loss: -0.38776206970214844
Batch 17/64 loss: -0.3534708023071289
Batch 18/64 loss: -0.2437448501586914
Batch 19/64 loss: -0.5068378448486328
Batch 20/64 loss: -0.45967960357666016
Batch 21/64 loss: -0.49581050872802734
Batch 22/64 loss: -0.43210649490356445
Batch 23/64 loss: 0.9187421798706055
Batch 24/64 loss: -0.4291353225708008
Batch 25/64 loss: -0.0538482666015625
Batch 26/64 loss: -0.47812700271606445
Batch 27/64 loss: -0.43726682662963867
Batch 28/64 loss: -0.5218634605407715
Batch 29/64 loss: -0.20624780654907227
Batch 30/64 loss: 0.7943334579467773
Batch 31/64 loss: -0.5257840156555176
Batch 32/64 loss: -0.3290824890136719
Batch 33/64 loss: -0.4421119689941406
Batch 34/64 loss: -0.44925594329833984
Batch 35/64 loss: -0.5353083610534668
Batch 36/64 loss: -0.3902311325073242
Batch 37/64 loss: -0.2526249885559082
Batch 38/64 loss: -0.5530533790588379
Batch 39/64 loss: -0.31823301315307617
Batch 40/64 loss: -0.19471168518066406
Batch 41/64 loss: -0.08525991439819336
Batch 42/64 loss: -0.34798097610473633
Batch 43/64 loss: -0.31200218200683594
Batch 44/64 loss: -0.44861602783203125
Batch 45/64 loss: -0.4002866744995117
Batch 46/64 loss: -0.37599897384643555
Batch 47/64 loss: -0.2151937484741211
Batch 48/64 loss: -0.2866344451904297
Batch 49/64 loss: -0.47343015670776367
Batch 50/64 loss: -0.2917327880859375
Batch 51/64 loss: -0.33907413482666016
Batch 52/64 loss: 0.008913993835449219
Batch 53/64 loss: -0.5159039497375488
Batch 54/64 loss: -0.33589887619018555
Batch 55/64 loss: -0.36266660690307617
Batch 56/64 loss: -0.379824161529541
Batch 57/64 loss: -0.17966794967651367
Batch 58/64 loss: -0.46808481216430664
Batch 59/64 loss: -0.4957151412963867
Batch 60/64 loss: -0.3735384941101074
Batch 61/64 loss: 0.19132137298583984
Batch 62/64 loss: 0.1476144790649414
Batch 63/64 loss: -0.34529829025268555
Batch 64/64 loss: -3.9046473503112793
Epoch 121  Train loss: -0.3237490354799757  Val loss: -0.47750767973280445
Epoch 122
-------------------------------
Batch 1/64 loss: 0.519871711730957
Batch 2/64 loss: -0.4552125930786133
Batch 3/64 loss: -0.3718876838684082
Batch 4/64 loss: -0.3408942222595215
Batch 5/64 loss: 1.1076593399047852
Batch 6/64 loss: -0.2596440315246582
Batch 7/64 loss: -0.4226565361022949
Batch 8/64 loss: -0.2704787254333496
Batch 9/64 loss: -0.24855327606201172
Batch 10/64 loss: -0.5458765029907227
Batch 11/64 loss: -0.4303250312805176
Batch 12/64 loss: -0.4125986099243164
Batch 13/64 loss: -0.45281982421875
Batch 14/64 loss: -0.44846296310424805
Batch 15/64 loss: -0.47591209411621094
Batch 16/64 loss: -0.24225664138793945
Batch 17/64 loss: -0.31475067138671875
Batch 18/64 loss: -0.45795679092407227
Batch 19/64 loss: -0.41754150390625
Batch 20/64 loss: 1.2124571800231934
Batch 21/64 loss: -0.4261155128479004
Batch 22/64 loss: -0.2868971824645996
Batch 23/64 loss: -0.4530062675476074
Batch 24/64 loss: -0.4538002014160156
Batch 25/64 loss: -0.04752492904663086
Batch 26/64 loss: -0.03433036804199219
Batch 27/64 loss: -0.3890085220336914
Batch 28/64 loss: -0.4823017120361328
Batch 29/64 loss: -0.4040870666503906
Batch 30/64 loss: -0.4695110321044922
Batch 31/64 loss: -0.5127878189086914
Batch 32/64 loss: -0.3920440673828125
Batch 33/64 loss: -0.42682647705078125
Batch 34/64 loss: -0.2090458869934082
Batch 35/64 loss: -0.2970156669616699
Batch 36/64 loss: -0.21414422988891602
Batch 37/64 loss: 0.4558849334716797
Batch 38/64 loss: -0.5421600341796875
Batch 39/64 loss: -0.3632526397705078
Batch 40/64 loss: -0.5320792198181152
Batch 41/64 loss: -0.48654890060424805
Batch 42/64 loss: -0.3781924247741699
Batch 43/64 loss: -0.326568603515625
Batch 44/64 loss: -0.300048828125
Batch 45/64 loss: -0.1734304428100586
Batch 46/64 loss: -0.020648956298828125
Batch 47/64 loss: -0.45844125747680664
Batch 48/64 loss: -0.25690746307373047
Batch 49/64 loss: -0.33515262603759766
Batch 50/64 loss: -0.3306727409362793
Batch 51/64 loss: -0.28955841064453125
Batch 52/64 loss: -0.3658928871154785
Batch 53/64 loss: -0.5994439125061035
Batch 54/64 loss: -0.415891170501709
Batch 55/64 loss: -0.21423625946044922
Batch 56/64 loss: -0.4685325622558594
Batch 57/64 loss: -0.41665077209472656
Batch 58/64 loss: -0.32242822647094727
Batch 59/64 loss: -0.40538501739501953
Batch 60/64 loss: -0.11728096008300781
Batch 61/64 loss: -0.5236573219299316
Batch 62/64 loss: -0.2939882278442383
Batch 63/64 loss: 0.17634105682373047
Batch 64/64 loss: -3.7885332107543945
Epoch 122  Train loss: -0.3195687873690736  Val loss: -0.6025408977495436
Epoch 123
-------------------------------
Batch 1/64 loss: -0.2910013198852539
Batch 2/64 loss: -0.31733179092407227
Batch 3/64 loss: -0.49482202529907227
Batch 4/64 loss: -0.228240966796875
Batch 5/64 loss: -0.08334112167358398
Batch 6/64 loss: -0.09481239318847656
Batch 7/64 loss: -0.2635049819946289
Batch 8/64 loss: -0.33759212493896484
Batch 9/64 loss: -0.3775520324707031
Batch 10/64 loss: -0.3264145851135254
Batch 11/64 loss: -0.44127416610717773
Batch 12/64 loss: -0.43442678451538086
Batch 13/64 loss: -0.22763872146606445
Batch 14/64 loss: -0.30502986907958984
Batch 15/64 loss: -0.5216050148010254
Batch 16/64 loss: -0.5266323089599609
Batch 17/64 loss: -0.5362300872802734
Batch 18/64 loss: -0.2848849296569824
Batch 19/64 loss: -0.4809756278991699
Batch 20/64 loss: 0.1761007308959961
Batch 21/64 loss: -0.5171699523925781
Batch 22/64 loss: -0.41040992736816406
Batch 23/64 loss: -0.3974480628967285
Batch 24/64 loss: -0.48264455795288086
Batch 25/64 loss: -0.5034232139587402
Batch 26/64 loss: 0.1849513053894043
Batch 27/64 loss: -0.39583492279052734
Batch 28/64 loss: -0.45777082443237305
Batch 29/64 loss: -0.38835668563842773
Batch 30/64 loss: -0.31885814666748047
Batch 31/64 loss: -0.3389115333557129
Batch 32/64 loss: -0.4474945068359375
Batch 33/64 loss: -0.4580349922180176
Batch 34/64 loss: 0.5739870071411133
Batch 35/64 loss: -0.4674043655395508
Batch 36/64 loss: -0.42638254165649414
Batch 37/64 loss: -0.5989055633544922
Batch 38/64 loss: -0.5582160949707031
Batch 39/64 loss: -0.32709264755249023
Batch 40/64 loss: -0.5324773788452148
Batch 41/64 loss: -0.45595216751098633
Batch 42/64 loss: -0.42026758193969727
Batch 43/64 loss: -0.4006004333496094
Batch 44/64 loss: -0.43738317489624023
Batch 45/64 loss: 0.016349315643310547
Batch 46/64 loss: -0.20943546295166016
Batch 47/64 loss: -0.3753480911254883
Batch 48/64 loss: -0.3037548065185547
Batch 49/64 loss: 1.0228157043457031
Batch 50/64 loss: -0.09180259704589844
Batch 51/64 loss: -0.2084035873413086
Batch 52/64 loss: -0.4659767150878906
Batch 53/64 loss: -0.33074426651000977
Batch 54/64 loss: 0.6659865379333496
Batch 55/64 loss: -0.5073976516723633
Batch 56/64 loss: -0.5738186836242676
Batch 57/64 loss: 0.004364490509033203
Batch 58/64 loss: -0.21605777740478516
Batch 59/64 loss: -0.38740062713623047
Batch 60/64 loss: -0.5103616714477539
Batch 61/64 loss: -0.29714012145996094
Batch 62/64 loss: -0.3697695732116699
Batch 63/64 loss: -0.20597362518310547
Batch 64/64 loss: -3.703519344329834
Epoch 123  Train loss: -0.3372677616044587  Val loss: -0.6388568026093683
Saving best model, epoch: 123
Epoch 124
-------------------------------
Batch 1/64 loss: -0.5110816955566406
Batch 2/64 loss: -0.4927244186401367
Batch 3/64 loss: -0.3572821617126465
Batch 4/64 loss: -0.491208553314209
Batch 5/64 loss: -0.24863433837890625
Batch 6/64 loss: -0.40258073806762695
Batch 7/64 loss: -0.15544891357421875
Batch 8/64 loss: -0.49800872802734375
Batch 9/64 loss: -0.24975061416625977
Batch 10/64 loss: -0.5175952911376953
Batch 11/64 loss: -0.0008831024169921875
Batch 12/64 loss: -0.41238832473754883
Batch 13/64 loss: -0.4498147964477539
Batch 14/64 loss: 0.04836463928222656
Batch 15/64 loss: 2.2704081535339355
Batch 16/64 loss: -0.3007526397705078
Batch 17/64 loss: -0.41899824142456055
Batch 18/64 loss: -0.24846887588500977
Batch 19/64 loss: -0.44489383697509766
Batch 20/64 loss: -0.47557497024536133
Batch 21/64 loss: 0.9768524169921875
Batch 22/64 loss: -0.25868701934814453
Batch 23/64 loss: -0.3034706115722656
Batch 24/64 loss: -0.10553789138793945
Batch 25/64 loss: -0.4063081741333008
Batch 26/64 loss: -0.4734511375427246
Batch 27/64 loss: -0.2601165771484375
Batch 28/64 loss: -0.2732052803039551
Batch 29/64 loss: -0.4520606994628906
Batch 30/64 loss: -0.46938133239746094
Batch 31/64 loss: -0.5986447334289551
Batch 32/64 loss: -0.24732255935668945
Batch 33/64 loss: -0.32797908782958984
Batch 34/64 loss: -0.46831321716308594
Batch 35/64 loss: -0.46256589889526367
Batch 36/64 loss: -0.29805469512939453
Batch 37/64 loss: -0.33882665634155273
Batch 38/64 loss: -0.28434038162231445
Batch 39/64 loss: -0.2619452476501465
Batch 40/64 loss: -0.5389184951782227
Batch 41/64 loss: -0.15137195587158203
Batch 42/64 loss: -0.5291085243225098
Batch 43/64 loss: -0.46466684341430664
Batch 44/64 loss: -0.21964740753173828
Batch 45/64 loss: -0.4535818099975586
Batch 46/64 loss: -0.429415225982666
Batch 47/64 loss: -0.38370180130004883
Batch 48/64 loss: -0.5507054328918457
Batch 49/64 loss: -0.47815561294555664
Batch 50/64 loss: -0.3588294982910156
Batch 51/64 loss: -0.320925235748291
Batch 52/64 loss: -0.5739202499389648
Batch 53/64 loss: -0.2695789337158203
Batch 54/64 loss: -0.24412202835083008
Batch 55/64 loss: -0.227752685546875
Batch 56/64 loss: -0.20694208145141602
Batch 57/64 loss: -0.6087994575500488
Batch 58/64 loss: -0.3824596405029297
Batch 59/64 loss: -0.587303638458252
Batch 60/64 loss: -0.3541536331176758
Batch 61/64 loss: -0.4539651870727539
Batch 62/64 loss: 0.18208074569702148
Batch 63/64 loss: -0.37640905380249023
Batch 64/64 loss: -3.8118882179260254
Epoch 124  Train loss: -0.3374422914841596  Val loss: -0.5105718763423539
Epoch 125
-------------------------------
Batch 1/64 loss: -0.26091718673706055
Batch 2/64 loss: -0.4827117919921875
Batch 3/64 loss: -0.548973560333252
Batch 4/64 loss: -0.3775782585144043
Batch 5/64 loss: -0.5065650939941406
Batch 6/64 loss: -0.4745368957519531
Batch 7/64 loss: 0.24530982971191406
Batch 8/64 loss: 0.9325542449951172
Batch 9/64 loss: 0.992424488067627
Batch 10/64 loss: -0.4107937812805176
Batch 11/64 loss: -0.4285445213317871
Batch 12/64 loss: -0.3914928436279297
Batch 13/64 loss: -0.47235918045043945
Batch 14/64 loss: -0.39779043197631836
Batch 15/64 loss: -0.46169424057006836
Batch 16/64 loss: -0.37248849868774414
Batch 17/64 loss: -0.3890647888183594
Batch 18/64 loss: -0.23815250396728516
Batch 19/64 loss: -0.3251042366027832
Batch 20/64 loss: -0.20496177673339844
Batch 21/64 loss: -0.36870861053466797
Batch 22/64 loss: -0.13243341445922852
Batch 23/64 loss: -0.3736991882324219
Batch 24/64 loss: -0.15765142440795898
Batch 25/64 loss: -0.32651329040527344
Batch 26/64 loss: -0.3266634941101074
Batch 27/64 loss: 0.8711023330688477
Batch 28/64 loss: -0.29412412643432617
Batch 29/64 loss: -0.0801248550415039
Batch 30/64 loss: -0.14923095703125
Batch 31/64 loss: 0.5435194969177246
Batch 32/64 loss: 0.14146900177001953
Batch 33/64 loss: -0.15186166763305664
Batch 34/64 loss: -0.14125347137451172
Batch 35/64 loss: -0.015566825866699219
Batch 36/64 loss: -0.15983963012695312
Batch 37/64 loss: -0.21539831161499023
Batch 38/64 loss: -0.2096695899963379
Batch 39/64 loss: -0.11345052719116211
Batch 40/64 loss: -0.2141132354736328
Batch 41/64 loss: -0.14244318008422852
Batch 42/64 loss: -0.08863019943237305
Batch 43/64 loss: -0.3265824317932129
Batch 44/64 loss: -0.05637073516845703
Batch 45/64 loss: -0.2638826370239258
Batch 46/64 loss: 1.5907950401306152
Batch 47/64 loss: -0.25208473205566406
Batch 48/64 loss: 0.07388639450073242
Batch 49/64 loss: -0.10524368286132812
Batch 50/64 loss: -0.3277111053466797
Batch 51/64 loss: 0.18696212768554688
Batch 52/64 loss: 0.07493257522583008
Batch 53/64 loss: -0.19489145278930664
Batch 54/64 loss: 0.23388910293579102
Batch 55/64 loss: -0.15047168731689453
Batch 56/64 loss: -0.08290386199951172
Batch 57/64 loss: 0.01661539077758789
Batch 58/64 loss: -0.19860601425170898
Batch 59/64 loss: 0.01672649383544922
Batch 60/64 loss: -0.3858356475830078
Batch 61/64 loss: -0.11035633087158203
Batch 62/64 loss: -0.14728975296020508
Batch 63/64 loss: -0.17549467086791992
Batch 64/64 loss: -3.0960302352905273
Epoch 125  Train loss: -0.15034770965576172  Val loss: -0.39247586227364556
Epoch 126
-------------------------------
Batch 1/64 loss: 0.19299793243408203
Batch 2/64 loss: -0.23986339569091797
Batch 3/64 loss: -0.2402482032775879
Batch 4/64 loss: -0.20651483535766602
Batch 5/64 loss: 0.09653663635253906
Batch 6/64 loss: -0.1892991065979004
Batch 7/64 loss: -0.19452714920043945
Batch 8/64 loss: -0.46950578689575195
Batch 9/64 loss: -0.4055314064025879
Batch 10/64 loss: -0.28423213958740234
Batch 11/64 loss: -0.14064884185791016
Batch 12/64 loss: -0.28920841217041016
Batch 13/64 loss: 0.18715715408325195
Batch 14/64 loss: -0.4973587989807129
Batch 15/64 loss: -0.10327482223510742
Batch 16/64 loss: -0.1947464942932129
Batch 17/64 loss: 1.6656455993652344
Batch 18/64 loss: 0.7945990562438965
Batch 19/64 loss: 0.06668615341186523
Batch 20/64 loss: -0.1117558479309082
Batch 21/64 loss: 0.1458430290222168
Batch 22/64 loss: 0.11851978302001953
Batch 23/64 loss: 0.23092222213745117
Batch 24/64 loss: -0.0435481071472168
Batch 25/64 loss: -0.12145805358886719
Batch 26/64 loss: 0.15489864349365234
Batch 27/64 loss: -0.22215032577514648
Batch 28/64 loss: -0.18026351928710938
Batch 29/64 loss: -0.22539663314819336
Batch 30/64 loss: -0.26754093170166016
Batch 31/64 loss: -0.23557281494140625
Batch 32/64 loss: 0.9460997581481934
Batch 33/64 loss: -0.004870891571044922
Batch 34/64 loss: 0.01950359344482422
Batch 35/64 loss: -0.2863130569458008
Batch 36/64 loss: -0.2708401679992676
Batch 37/64 loss: 0.24043655395507812
Batch 38/64 loss: -0.12229585647583008
Batch 39/64 loss: -0.26523780822753906
Batch 40/64 loss: 0.10576629638671875
Batch 41/64 loss: -0.13205528259277344
Batch 42/64 loss: 0.9504528045654297
Batch 43/64 loss: -0.3183097839355469
Batch 44/64 loss: -0.04080677032470703
Batch 45/64 loss: -0.04109334945678711
Batch 46/64 loss: -0.1866769790649414
Batch 47/64 loss: -0.1457386016845703
Batch 48/64 loss: -0.24404287338256836
Batch 49/64 loss: -0.05949068069458008
Batch 50/64 loss: -0.3945937156677246
Batch 51/64 loss: 1.4922857284545898
Batch 52/64 loss: -0.23909568786621094
Batch 53/64 loss: -0.07399415969848633
Batch 54/64 loss: -0.1679844856262207
Batch 55/64 loss: 0.19478273391723633
Batch 56/64 loss: -0.34801340103149414
Batch 57/64 loss: 0.06025123596191406
Batch 58/64 loss: -0.12164115905761719
Batch 59/64 loss: -0.2698807716369629
Batch 60/64 loss: -0.3128209114074707
Batch 61/64 loss: 0.14416790008544922
Batch 62/64 loss: -0.2891979217529297
Batch 63/64 loss: -0.30449342727661133
Batch 64/64 loss: -3.45101261138916
Epoch 126  Train loss: -0.06718180413339653  Val loss: -0.4243881645071547
Epoch 127
-------------------------------
Batch 1/64 loss: -0.3385014533996582
Batch 2/64 loss: -0.36372804641723633
Batch 3/64 loss: -0.3398466110229492
Batch 4/64 loss: -0.3317108154296875
Batch 5/64 loss: 0.02959728240966797
Batch 6/64 loss: -0.26293420791625977
Batch 7/64 loss: -0.3568301200866699
Batch 8/64 loss: -0.27139997482299805
Batch 9/64 loss: -0.3217306137084961
Batch 10/64 loss: -0.27221107482910156
Batch 11/64 loss: -0.2776632308959961
Batch 12/64 loss: 0.03024768829345703
Batch 13/64 loss: -0.38246870040893555
Batch 14/64 loss: -0.030788898468017578
Batch 15/64 loss: 0.25278711318969727
Batch 16/64 loss: -0.28868865966796875
Batch 17/64 loss: -0.25389957427978516
Batch 18/64 loss: -0.5100564956665039
Batch 19/64 loss: -0.28432321548461914
Batch 20/64 loss: -0.42490673065185547
Batch 21/64 loss: 1.1691207885742188
Batch 22/64 loss: -0.33478260040283203
Batch 23/64 loss: -0.26899099349975586
Batch 24/64 loss: -0.4169282913208008
Batch 25/64 loss: 0.012764453887939453
Batch 26/64 loss: -0.31387901306152344
Batch 27/64 loss: -0.36629533767700195
Batch 28/64 loss: -0.37919139862060547
Batch 29/64 loss: -0.30205440521240234
Batch 30/64 loss: -0.39629077911376953
Batch 31/64 loss: 0.03403282165527344
Batch 32/64 loss: -0.43199920654296875
Batch 33/64 loss: -0.25087833404541016
Batch 34/64 loss: -0.4063882827758789
Batch 35/64 loss: -0.5205550193786621
Batch 36/64 loss: -0.26788997650146484
Batch 37/64 loss: -0.5241680145263672
Batch 38/64 loss: 0.8787755966186523
Batch 39/64 loss: -0.5069069862365723
Batch 40/64 loss: -0.08161401748657227
Batch 41/64 loss: -0.1969618797302246
Batch 42/64 loss: -0.2339634895324707
Batch 43/64 loss: -0.442779541015625
Batch 44/64 loss: -0.29838991165161133
Batch 45/64 loss: -0.22915267944335938
Batch 46/64 loss: -0.2811126708984375
Batch 47/64 loss: -0.41085004806518555
Batch 48/64 loss: -0.1941089630126953
Batch 49/64 loss: -0.38942909240722656
Batch 50/64 loss: -0.4270920753479004
Batch 51/64 loss: -0.43705034255981445
Batch 52/64 loss: -0.539543628692627
Batch 53/64 loss: -0.3466963768005371
Batch 54/64 loss: 0.3986940383911133
Batch 55/64 loss: -0.4082159996032715
Batch 56/64 loss: 0.6107287406921387
Batch 57/64 loss: 0.11271858215332031
Batch 58/64 loss: -0.42394399642944336
Batch 59/64 loss: -0.3251309394836426
Batch 60/64 loss: -0.3133234977722168
Batch 61/64 loss: -0.3327755928039551
Batch 62/64 loss: -0.2347278594970703
Batch 63/64 loss: -0.49359130859375
Batch 64/64 loss: -3.6457786560058594
Epoch 127  Train loss: -0.2704973781810087  Val loss: -0.7277933625421164
Saving best model, epoch: 127
Epoch 128
-------------------------------
Batch 1/64 loss: -0.16083002090454102
Batch 2/64 loss: -0.18861007690429688
Batch 3/64 loss: -0.4304046630859375
Batch 4/64 loss: -0.5268406867980957
Batch 5/64 loss: -0.38869190216064453
Batch 6/64 loss: -0.4695887565612793
Batch 7/64 loss: -0.42720651626586914
Batch 8/64 loss: -0.44762659072875977
Batch 9/64 loss: 0.7490973472595215
Batch 10/64 loss: -0.38076114654541016
Batch 11/64 loss: -0.16057968139648438
Batch 12/64 loss: -0.15408849716186523
Batch 13/64 loss: -0.11986875534057617
Batch 14/64 loss: -0.17061710357666016
Batch 15/64 loss: -0.16019058227539062
Batch 16/64 loss: -0.1260838508605957
Batch 17/64 loss: -0.17159414291381836
Batch 18/64 loss: -0.06918144226074219
Batch 19/64 loss: -0.2121901512145996
Batch 20/64 loss: -0.2964000701904297
Batch 21/64 loss: -0.23635149002075195
Batch 22/64 loss: -0.2909202575683594
Batch 23/64 loss: -0.33061885833740234
Batch 24/64 loss: -0.15594911575317383
Batch 25/64 loss: -0.23724603652954102
Batch 26/64 loss: -0.17659330368041992
Batch 27/64 loss: -0.3307170867919922
Batch 28/64 loss: -0.29605913162231445
Batch 29/64 loss: -0.23338031768798828
Batch 30/64 loss: -0.03904294967651367
Batch 31/64 loss: -0.17881536483764648
Batch 32/64 loss: -0.49459028244018555
Batch 33/64 loss: 0.05503416061401367
Batch 34/64 loss: -0.2069377899169922
Batch 35/64 loss: -0.13481378555297852
Batch 36/64 loss: -0.2919020652770996
Batch 37/64 loss: -0.45023679733276367
Batch 38/64 loss: -0.2953653335571289
Batch 39/64 loss: -0.38123512268066406
Batch 40/64 loss: 0.8779067993164062
Batch 41/64 loss: 1.4935598373413086
Batch 42/64 loss: -0.29059553146362305
Batch 43/64 loss: -0.3169379234313965
Batch 44/64 loss: -0.3016047477722168
Batch 45/64 loss: -0.279510498046875
Batch 46/64 loss: -0.2879061698913574
Batch 47/64 loss: -0.24742794036865234
Batch 48/64 loss: 1.3764429092407227
Batch 49/64 loss: -0.24638938903808594
Batch 50/64 loss: 0.2810688018798828
Batch 51/64 loss: -0.29514455795288086
Batch 52/64 loss: -0.3421964645385742
Batch 53/64 loss: -0.13260889053344727
Batch 54/64 loss: -0.38773393630981445
Batch 55/64 loss: -0.27425527572631836
Batch 56/64 loss: -0.3167304992675781
Batch 57/64 loss: -0.10678958892822266
Batch 58/64 loss: -0.38336944580078125
Batch 59/64 loss: -0.22767305374145508
Batch 60/64 loss: -0.15276288986206055
Batch 61/64 loss: -0.19697809219360352
Batch 62/64 loss: -0.2835102081298828
Batch 63/64 loss: -0.21732616424560547
Batch 64/64 loss: -3.442047119140625
Epoch 128  Train loss: -0.20169421925264247  Val loss: -0.4205388727876329
Epoch 129
-------------------------------
Batch 1/64 loss: -0.21790790557861328
Batch 2/64 loss: 0.2373638153076172
Batch 3/64 loss: -0.12790203094482422
Batch 4/64 loss: -0.21270036697387695
Batch 5/64 loss: -0.22568845748901367
Batch 6/64 loss: -0.3749527931213379
Batch 7/64 loss: -0.33013439178466797
Batch 8/64 loss: -0.3059406280517578
Batch 9/64 loss: -0.3924570083618164
Batch 10/64 loss: -0.13656997680664062
Batch 11/64 loss: -0.1441650390625
Batch 12/64 loss: -0.1723017692565918
Batch 13/64 loss: -0.15676164627075195
Batch 14/64 loss: -0.34876537322998047
Batch 15/64 loss: 0.03727293014526367
Batch 16/64 loss: -0.4356656074523926
Batch 17/64 loss: -0.28095102310180664
Batch 18/64 loss: -0.33659839630126953
Batch 19/64 loss: -0.3835453987121582
Batch 20/64 loss: -0.37961578369140625
Batch 21/64 loss: -0.3717350959777832
Batch 22/64 loss: -0.4702010154724121
Batch 23/64 loss: -0.3244633674621582
Batch 24/64 loss: -0.4002842903137207
Batch 25/64 loss: -0.3886547088623047
Batch 26/64 loss: -0.2387399673461914
Batch 27/64 loss: -0.40473127365112305
Batch 28/64 loss: 0.27642822265625
Batch 29/64 loss: 1.0128421783447266
Batch 30/64 loss: -0.33484458923339844
Batch 31/64 loss: -0.4589705467224121
Batch 32/64 loss: -0.5401935577392578
Batch 33/64 loss: 0.2903275489807129
Batch 34/64 loss: -0.43401527404785156
Batch 35/64 loss: -0.20126962661743164
Batch 36/64 loss: 0.2993965148925781
Batch 37/64 loss: -0.3457307815551758
Batch 38/64 loss: -0.2958235740661621
Batch 39/64 loss: -0.48921871185302734
Batch 40/64 loss: -0.23972845077514648
Batch 41/64 loss: -0.47803735733032227
Batch 42/64 loss: -0.06022167205810547
Batch 43/64 loss: 1.264822006225586
Batch 44/64 loss: -0.2183823585510254
Batch 45/64 loss: 0.9594473838806152
Batch 46/64 loss: 0.11880016326904297
Batch 47/64 loss: -0.02663135528564453
Batch 48/64 loss: -0.30458927154541016
Batch 49/64 loss: -0.36716747283935547
Batch 50/64 loss: -0.2980937957763672
Batch 51/64 loss: -0.29358434677124023
Batch 52/64 loss: -0.41109180450439453
Batch 53/64 loss: 0.06282234191894531
Batch 54/64 loss: -0.27923059463500977
Batch 55/64 loss: -0.32242393493652344
Batch 56/64 loss: -0.4001593589782715
Batch 57/64 loss: -0.4343719482421875
Batch 58/64 loss: -0.14932012557983398
Batch 59/64 loss: -0.47777414321899414
Batch 60/64 loss: -0.4668698310852051
Batch 61/64 loss: -0.25940990447998047
Batch 62/64 loss: -0.30788087844848633
Batch 63/64 loss: 0.17185640335083008
Batch 64/64 loss: -3.8830604553222656
Epoch 129  Train loss: -0.22960603003408395  Val loss: -0.4750836428088421
Epoch 130
-------------------------------
Batch 1/64 loss: -0.48209238052368164
Batch 2/64 loss: 1.0315589904785156
Batch 3/64 loss: -0.31352853775024414
Batch 4/64 loss: -0.1253352165222168
Batch 5/64 loss: 0.5120468139648438
Batch 6/64 loss: -0.3974466323852539
Batch 7/64 loss: -0.4276919364929199
Batch 8/64 loss: -0.3281407356262207
Batch 9/64 loss: 0.49315595626831055
Batch 10/64 loss: -0.5218324661254883
Batch 11/64 loss: -0.36280345916748047
Batch 12/64 loss: -0.2575244903564453
Batch 13/64 loss: 0.060799598693847656
Batch 14/64 loss: -0.44521522521972656
Batch 15/64 loss: -0.249359130859375
Batch 16/64 loss: -0.32964181900024414
Batch 17/64 loss: -0.20913028717041016
Batch 18/64 loss: 0.06059455871582031
Batch 19/64 loss: -0.12678050994873047
Batch 20/64 loss: -0.28775835037231445
Batch 21/64 loss: -0.22979164123535156
Batch 22/64 loss: -0.30821943283081055
Batch 23/64 loss: -0.22204828262329102
Batch 24/64 loss: -0.3174257278442383
Batch 25/64 loss: -0.3828158378601074
Batch 26/64 loss: 0.4241652488708496
Batch 27/64 loss: -0.3724937438964844
Batch 28/64 loss: -0.2287750244140625
Batch 29/64 loss: -0.2664942741394043
Batch 30/64 loss: -0.27541255950927734
Batch 31/64 loss: -0.5499286651611328
Batch 32/64 loss: 0.049675941467285156
Batch 33/64 loss: -0.01142740249633789
Batch 34/64 loss: -0.44028472900390625
Batch 35/64 loss: -0.3496241569519043
Batch 36/64 loss: -0.3782162666320801
Batch 37/64 loss: -0.32121896743774414
Batch 38/64 loss: -0.39174890518188477
Batch 39/64 loss: -0.448305606842041
Batch 40/64 loss: -0.24922752380371094
Batch 41/64 loss: -0.5343384742736816
Batch 42/64 loss: -0.46021175384521484
Batch 43/64 loss: -0.3303523063659668
Batch 44/64 loss: -0.3789091110229492
Batch 45/64 loss: -0.3699650764465332
Batch 46/64 loss: -0.34584522247314453
Batch 47/64 loss: -0.516535758972168
Batch 48/64 loss: -0.43178319931030273
Batch 49/64 loss: -0.4735751152038574
Batch 50/64 loss: -0.19559526443481445
Batch 51/64 loss: -0.43422555923461914
Batch 52/64 loss: -0.3951268196105957
Batch 53/64 loss: -0.4257030487060547
Batch 54/64 loss: -0.4577631950378418
Batch 55/64 loss: -0.3905167579650879
Batch 56/64 loss: -0.4390220642089844
Batch 57/64 loss: 1.11204195022583
Batch 58/64 loss: -0.2797422409057617
Batch 59/64 loss: -0.43086862564086914
Batch 60/64 loss: -0.35991954803466797
Batch 61/64 loss: -0.44373083114624023
Batch 62/64 loss: -0.6064233779907227
Batch 63/64 loss: 0.559356689453125
Batch 64/64 loss: -3.9509544372558594
Epoch 130  Train loss: -0.2818464840159697  Val loss: -0.5043923551683983
Epoch 131
-------------------------------
Batch 1/64 loss: -0.044173240661621094
Batch 2/64 loss: -0.36849212646484375
Batch 3/64 loss: -0.43433380126953125
Batch 4/64 loss: -0.41604137420654297
Batch 5/64 loss: -0.48463010787963867
Batch 6/64 loss: -0.20682668685913086
Batch 7/64 loss: -0.18753719329833984
Batch 8/64 loss: -0.4261903762817383
Batch 9/64 loss: -0.3456435203552246
Batch 10/64 loss: -0.5030045509338379
Batch 11/64 loss: -0.3851909637451172
Batch 12/64 loss: -0.4542660713195801
Batch 13/64 loss: -0.3579444885253906
Batch 14/64 loss: 1.138986587524414
Batch 15/64 loss: -0.4277501106262207
Batch 16/64 loss: -0.3426976203918457
Batch 17/64 loss: -0.414614200592041
Batch 18/64 loss: -0.04771900177001953
Batch 19/64 loss: -0.27338123321533203
Batch 20/64 loss: -0.13770532608032227
Batch 21/64 loss: -0.32318878173828125
Batch 22/64 loss: -0.40416669845581055
Batch 23/64 loss: -0.10410737991333008
Batch 24/64 loss: 1.7300748825073242
Batch 25/64 loss: -0.3990192413330078
Batch 26/64 loss: -0.45547056198120117
Batch 27/64 loss: 0.44694995880126953
Batch 28/64 loss: -0.45844030380249023
Batch 29/64 loss: -0.3886260986328125
Batch 30/64 loss: 0.6311216354370117
Batch 31/64 loss: -0.45966196060180664
Batch 32/64 loss: -0.3380270004272461
Batch 33/64 loss: -0.36168956756591797
Batch 34/64 loss: -0.32309722900390625
Batch 35/64 loss: -0.3027982711791992
Batch 36/64 loss: -0.1956157684326172
Batch 37/64 loss: -0.28012990951538086
Batch 38/64 loss: -0.45596981048583984
Batch 39/64 loss: -0.3855266571044922
Batch 40/64 loss: -0.4673738479614258
Batch 41/64 loss: -0.40099287033081055
Batch 42/64 loss: -0.4520130157470703
Batch 43/64 loss: -0.3972010612487793
Batch 44/64 loss: -0.42270946502685547
Batch 45/64 loss: -0.3661494255065918
Batch 46/64 loss: -0.49849939346313477
Batch 47/64 loss: -0.36576271057128906
Batch 48/64 loss: -0.4811258316040039
Batch 49/64 loss: -0.3868875503540039
Batch 50/64 loss: -0.19898366928100586
Batch 51/64 loss: -0.15873956680297852
Batch 52/64 loss: -0.22211408615112305
Batch 53/64 loss: -0.47678470611572266
Batch 54/64 loss: 0.03460264205932617
Batch 55/64 loss: -0.03201103210449219
Batch 56/64 loss: -0.26238346099853516
Batch 57/64 loss: -0.29627323150634766
Batch 58/64 loss: -0.28307342529296875
Batch 59/64 loss: -0.467923641204834
Batch 60/64 loss: 0.24475526809692383
Batch 61/64 loss: -0.34207773208618164
Batch 62/64 loss: -0.3821849822998047
Batch 63/64 loss: -0.3513021469116211
Batch 64/64 loss: -3.838491916656494
Epoch 131  Train loss: -0.28640975765153476  Val loss: -0.5239731044703743
Epoch 132
-------------------------------
Batch 1/64 loss: -0.05678415298461914
Batch 2/64 loss: -0.2775440216064453
Batch 3/64 loss: -0.220672607421875
Batch 4/64 loss: -0.44309139251708984
Batch 5/64 loss: -0.08753585815429688
Batch 6/64 loss: -0.22487163543701172
Batch 7/64 loss: -0.35956478118896484
Batch 8/64 loss: -0.2671527862548828
Batch 9/64 loss: -0.3812556266784668
Batch 10/64 loss: -0.4709649085998535
Batch 11/64 loss: -0.46054649353027344
Batch 12/64 loss: 0.706519603729248
Batch 13/64 loss: -0.5464668273925781
Batch 14/64 loss: -0.3566460609436035
Batch 15/64 loss: -0.2736544609069824
Batch 16/64 loss: -0.5145678520202637
Batch 17/64 loss: -0.3446311950683594
Batch 18/64 loss: -0.5533204078674316
Batch 19/64 loss: -0.3210487365722656
Batch 20/64 loss: -0.39223289489746094
Batch 21/64 loss: -0.3846111297607422
Batch 22/64 loss: -0.5688009262084961
Batch 23/64 loss: 0.17412710189819336
Batch 24/64 loss: -0.2801179885864258
Batch 25/64 loss: -0.5821347236633301
Batch 26/64 loss: -0.5639023780822754
Batch 27/64 loss: -0.11029481887817383
Batch 28/64 loss: -0.3491811752319336
Batch 29/64 loss: -0.5247626304626465
Batch 30/64 loss: -0.32138633728027344
Batch 31/64 loss: -0.15871667861938477
Batch 32/64 loss: -0.34320688247680664
Batch 33/64 loss: -0.3711671829223633
Batch 34/64 loss: -0.37760353088378906
Batch 35/64 loss: -0.3044276237487793
Batch 36/64 loss: -0.377164363861084
Batch 37/64 loss: -0.1304330825805664
Batch 38/64 loss: 0.027605056762695312
Batch 39/64 loss: -0.26742124557495117
Batch 40/64 loss: -0.5288333892822266
Batch 41/64 loss: -0.5064234733581543
Batch 42/64 loss: 0.9059891700744629
Batch 43/64 loss: 1.0470514297485352
Batch 44/64 loss: -0.34580421447753906
Batch 45/64 loss: -0.5844807624816895
Batch 46/64 loss: -0.5083980560302734
Batch 47/64 loss: -0.40352964401245117
Batch 48/64 loss: -0.3797297477722168
Batch 49/64 loss: -0.2586650848388672
Batch 50/64 loss: -0.4617781639099121
Batch 51/64 loss: -0.5278120040893555
Batch 52/64 loss: -0.23854732513427734
Batch 53/64 loss: -0.4650588035583496
Batch 54/64 loss: -0.4819459915161133
Batch 55/64 loss: -0.2752203941345215
Batch 56/64 loss: -0.2424635887145996
Batch 57/64 loss: -0.2857379913330078
Batch 58/64 loss: -0.08881235122680664
Batch 59/64 loss: -0.456942081451416
Batch 60/64 loss: -0.5650491714477539
Batch 61/64 loss: -0.41962242126464844
Batch 62/64 loss: 0.20145845413208008
Batch 63/64 loss: -0.20124197006225586
Batch 64/64 loss: -4.06329870223999
Epoch 132  Train loss: -0.3259404743418974  Val loss: -0.7122860938003382
Epoch 133
-------------------------------
Batch 1/64 loss: -0.4011960029602051
Batch 2/64 loss: -0.5275416374206543
Batch 3/64 loss: -0.4582219123840332
Batch 4/64 loss: -0.6178345680236816
Batch 5/64 loss: -0.46978282928466797
Batch 6/64 loss: -0.4305868148803711
Batch 7/64 loss: 0.6941213607788086
Batch 8/64 loss: -0.4303398132324219
Batch 9/64 loss: -0.44341230392456055
Batch 10/64 loss: -0.3192930221557617
Batch 11/64 loss: -0.45877742767333984
Batch 12/64 loss: -0.5779576301574707
Batch 13/64 loss: -0.4819164276123047
Batch 14/64 loss: -0.22362136840820312
Batch 15/64 loss: -0.3616185188293457
Batch 16/64 loss: -0.17159032821655273
Batch 17/64 loss: 0.8981938362121582
Batch 18/64 loss: -0.31816816329956055
Batch 19/64 loss: -0.2087874412536621
Batch 20/64 loss: -0.15744590759277344
Batch 21/64 loss: -0.36777782440185547
Batch 22/64 loss: -0.38333654403686523
Batch 23/64 loss: 1.0974111557006836
Batch 24/64 loss: -0.2554292678833008
Batch 25/64 loss: -0.3711395263671875
Batch 26/64 loss: -0.3769826889038086
Batch 27/64 loss: -0.3243827819824219
Batch 28/64 loss: -0.19339561462402344
Batch 29/64 loss: -0.3491654396057129
Batch 30/64 loss: -0.4749574661254883
Batch 31/64 loss: 0.14748096466064453
Batch 32/64 loss: -0.15088987350463867
Batch 33/64 loss: -0.05362701416015625
Batch 34/64 loss: 0.13073396682739258
Batch 35/64 loss: -0.45922374725341797
Batch 36/64 loss: -0.00968170166015625
Batch 37/64 loss: -0.40287303924560547
Batch 38/64 loss: -0.2595639228820801
Batch 39/64 loss: -0.40227222442626953
Batch 40/64 loss: -0.23559141159057617
Batch 41/64 loss: -0.39629554748535156
Batch 42/64 loss: 0.25726842880249023
Batch 43/64 loss: -0.4300227165222168
Batch 44/64 loss: -0.4277009963989258
Batch 45/64 loss: -0.3787107467651367
Batch 46/64 loss: -0.2883591651916504
Batch 47/64 loss: -0.3113083839416504
Batch 48/64 loss: -0.47002601623535156
Batch 49/64 loss: -0.3791813850402832
Batch 50/64 loss: -0.528630256652832
Batch 51/64 loss: -0.521395206451416
Batch 52/64 loss: -0.5739264488220215
Batch 53/64 loss: -0.4976329803466797
Batch 54/64 loss: -0.5296664237976074
Batch 55/64 loss: -0.32979440689086914
Batch 56/64 loss: -0.3095998764038086
Batch 57/64 loss: -0.3656902313232422
Batch 58/64 loss: -0.48781585693359375
Batch 59/64 loss: -0.3694777488708496
Batch 60/64 loss: -0.29381322860717773
Batch 61/64 loss: -0.5857553482055664
Batch 62/64 loss: -0.4864788055419922
Batch 63/64 loss: -0.4790768623352051
Batch 64/64 loss: -4.076015472412109
Epoch 133  Train loss: -0.33569478801652497  Val loss: -0.7205140156434574
Epoch 134
-------------------------------
Batch 1/64 loss: -0.4082179069519043
Batch 2/64 loss: -0.5723452568054199
Batch 3/64 loss: -0.5734596252441406
Batch 4/64 loss: -0.4329795837402344
Batch 5/64 loss: -0.5716791152954102
Batch 6/64 loss: 1.0328588485717773
Batch 7/64 loss: -0.5483016967773438
Batch 8/64 loss: -0.5294690132141113
Batch 9/64 loss: -0.46643877029418945
Batch 10/64 loss: -0.2665395736694336
Batch 11/64 loss: -0.4524111747741699
Batch 12/64 loss: -0.4660177230834961
Batch 13/64 loss: -0.36521291732788086
Batch 14/64 loss: 0.13620805740356445
Batch 15/64 loss: -0.28650856018066406
Batch 16/64 loss: -0.48199033737182617
Batch 17/64 loss: -0.5006518363952637
Batch 18/64 loss: -0.34791135787963867
Batch 19/64 loss: -0.561793327331543
Batch 20/64 loss: 0.2780289649963379
Batch 21/64 loss: -0.5496048927307129
Batch 22/64 loss: -0.3836665153503418
Batch 23/64 loss: -0.5690450668334961
Batch 24/64 loss: -0.517331600189209
Batch 25/64 loss: -0.3366518020629883
Batch 26/64 loss: -0.41063547134399414
Batch 27/64 loss: -0.5519790649414062
Batch 28/64 loss: -0.4973583221435547
Batch 29/64 loss: 0.7149572372436523
Batch 30/64 loss: -0.47374439239501953
Batch 31/64 loss: -0.42512989044189453
Batch 32/64 loss: -0.4903583526611328
Batch 33/64 loss: 0.07701683044433594
Batch 34/64 loss: -0.3152179718017578
Batch 35/64 loss: -0.4494967460632324
Batch 36/64 loss: -0.44295501708984375
Batch 37/64 loss: -0.4528498649597168
Batch 38/64 loss: -0.4407777786254883
Batch 39/64 loss: -0.4313812255859375
Batch 40/64 loss: -0.376312255859375
Batch 41/64 loss: -0.47605180740356445
Batch 42/64 loss: -0.3268718719482422
Batch 43/64 loss: -0.5157980918884277
Batch 44/64 loss: 0.7612752914428711
Batch 45/64 loss: -0.4000740051269531
Batch 46/64 loss: 0.08874320983886719
Batch 47/64 loss: -0.5012001991271973
Batch 48/64 loss: -0.5800213813781738
Batch 49/64 loss: -0.3973994255065918
Batch 50/64 loss: -0.5247502326965332
Batch 51/64 loss: -0.6018471717834473
Batch 52/64 loss: -0.4747438430786133
Batch 53/64 loss: -0.4530763626098633
Batch 54/64 loss: -0.3914375305175781
Batch 55/64 loss: -0.5368428230285645
Batch 56/64 loss: -0.5499048233032227
Batch 57/64 loss: -0.46332359313964844
Batch 58/64 loss: -0.561161994934082
Batch 59/64 loss: -0.6255326271057129
Batch 60/64 loss: -0.21212530136108398
Batch 61/64 loss: -0.08786964416503906
Batch 62/64 loss: -0.4301156997680664
Batch 63/64 loss: -0.13951826095581055
Batch 64/64 loss: -4.0435566902160645
Epoch 134  Train loss: -0.3943477611915738  Val loss: -0.6802760514197072
Epoch 135
-------------------------------
Batch 1/64 loss: -0.5079421997070312
Batch 2/64 loss: -0.11695051193237305
Batch 3/64 loss: -0.4988999366760254
Batch 4/64 loss: -0.26680612564086914
Batch 5/64 loss: -0.43289804458618164
Batch 6/64 loss: -0.5791239738464355
Batch 7/64 loss: -0.34851741790771484
Batch 8/64 loss: -0.2555875778198242
Batch 9/64 loss: -0.16852569580078125
Batch 10/64 loss: -0.48468732833862305
Batch 11/64 loss: -0.5340676307678223
Batch 12/64 loss: 0.42209863662719727
Batch 13/64 loss: -0.515782356262207
Batch 14/64 loss: -0.2017817497253418
Batch 15/64 loss: -0.49929189682006836
Batch 16/64 loss: -0.39371538162231445
Batch 17/64 loss: -0.3450942039489746
Batch 18/64 loss: -0.5366353988647461
Batch 19/64 loss: -0.017040252685546875
Batch 20/64 loss: -0.40930747985839844
Batch 21/64 loss: -0.45821380615234375
Batch 22/64 loss: -0.04891347885131836
Batch 23/64 loss: -0.5165896415710449
Batch 24/64 loss: -0.1724224090576172
Batch 25/64 loss: -0.5325837135314941
Batch 26/64 loss: -0.37117481231689453
Batch 27/64 loss: -0.4371514320373535
Batch 28/64 loss: -0.32404375076293945
Batch 29/64 loss: -0.5142116546630859
Batch 30/64 loss: -0.4414949417114258
Batch 31/64 loss: -0.28402233123779297
Batch 32/64 loss: -0.38142919540405273
Batch 33/64 loss: -0.5579771995544434
Batch 34/64 loss: -0.18415451049804688
Batch 35/64 loss: -0.3548011779785156
Batch 36/64 loss: -0.4456663131713867
Batch 37/64 loss: -0.5009512901306152
Batch 38/64 loss: -0.42899417877197266
Batch 39/64 loss: -0.3639979362487793
Batch 40/64 loss: -0.39522743225097656
Batch 41/64 loss: -0.3934025764465332
Batch 42/64 loss: 1.2131290435791016
Batch 43/64 loss: -0.45639610290527344
Batch 44/64 loss: -0.4755096435546875
Batch 45/64 loss: -0.5094232559204102
Batch 46/64 loss: -0.33305883407592773
Batch 47/64 loss: 0.18310022354125977
Batch 48/64 loss: -0.45151424407958984
Batch 49/64 loss: -0.3514671325683594
Batch 50/64 loss: -0.40083885192871094
Batch 51/64 loss: -0.296750545501709
Batch 52/64 loss: -0.2324366569519043
Batch 53/64 loss: -0.2959623336791992
Batch 54/64 loss: -0.20319414138793945
Batch 55/64 loss: -0.21332931518554688
Batch 56/64 loss: -0.36525535583496094
Batch 57/64 loss: 1.3473305702209473
Batch 58/64 loss: -0.3045220375061035
Batch 59/64 loss: -0.397554874420166
Batch 60/64 loss: -0.42649364471435547
Batch 61/64 loss: 0.1482381820678711
Batch 62/64 loss: -0.36339902877807617
Batch 63/64 loss: -0.3140106201171875
Batch 64/64 loss: -3.863708972930908
Epoch 135  Train loss: -0.3324718531440286  Val loss: -0.38553515824255663
Epoch 136
-------------------------------
Batch 1/64 loss: -0.4102444648742676
Batch 2/64 loss: -0.2028489112854004
Batch 3/64 loss: -0.17585420608520508
Batch 4/64 loss: -0.08218765258789062
Batch 5/64 loss: -0.2954554557800293
Batch 6/64 loss: 0.16263675689697266
Batch 7/64 loss: -0.3759617805480957
Batch 8/64 loss: -0.4358096122741699
Batch 9/64 loss: -0.19635486602783203
Batch 10/64 loss: -0.37636423110961914
Batch 11/64 loss: -0.04201173782348633
Batch 12/64 loss: -0.28988027572631836
Batch 13/64 loss: -0.2969236373901367
Batch 14/64 loss: -0.3457474708557129
Batch 15/64 loss: -0.41945981979370117
Batch 16/64 loss: -0.3689136505126953
Batch 17/64 loss: -0.23485755920410156
Batch 18/64 loss: -0.3315725326538086
Batch 19/64 loss: -0.18462324142456055
Batch 20/64 loss: 0.33914804458618164
Batch 21/64 loss: -0.2729053497314453
Batch 22/64 loss: -0.3924446105957031
Batch 23/64 loss: -0.3807101249694824
Batch 24/64 loss: -0.33425283432006836
Batch 25/64 loss: -0.3603067398071289
Batch 26/64 loss: -0.42859792709350586
Batch 27/64 loss: -0.34654664993286133
Batch 28/64 loss: -0.29554080963134766
Batch 29/64 loss: 0.2649564743041992
Batch 30/64 loss: -0.20394086837768555
Batch 31/64 loss: -0.11417198181152344
Batch 32/64 loss: -0.462069034576416
Batch 33/64 loss: -0.36150169372558594
Batch 34/64 loss: 1.0406837463378906
Batch 35/64 loss: -0.42981815338134766
Batch 36/64 loss: -0.29565906524658203
Batch 37/64 loss: -0.31151723861694336
Batch 38/64 loss: -0.37616586685180664
Batch 39/64 loss: -0.45525074005126953
Batch 40/64 loss: -0.34533262252807617
Batch 41/64 loss: -0.4435267448425293
Batch 42/64 loss: 0.06959247589111328
Batch 43/64 loss: -0.358823299407959
Batch 44/64 loss: 0.058531761169433594
Batch 45/64 loss: -0.5117731094360352
Batch 46/64 loss: 0.12276506423950195
Batch 47/64 loss: -0.5219106674194336
Batch 48/64 loss: -0.3750948905944824
Batch 49/64 loss: -0.5621023178100586
Batch 50/64 loss: -0.503840446472168
Batch 51/64 loss: -0.42551708221435547
Batch 52/64 loss: -0.3406968116760254
Batch 53/64 loss: -0.38191938400268555
Batch 54/64 loss: -0.2481856346130371
Batch 55/64 loss: -0.15249300003051758
Batch 56/64 loss: 0.582646369934082
Batch 57/64 loss: 0.07155084609985352
Batch 58/64 loss: -0.4717264175415039
Batch 59/64 loss: -0.4157094955444336
Batch 60/64 loss: 0.28955936431884766
Batch 61/64 loss: 0.7552409172058105
Batch 62/64 loss: -0.5156278610229492
Batch 63/64 loss: -0.11189603805541992
Batch 64/64 loss: -3.9204559326171875
Epoch 136  Train loss: -0.26283414504107305  Val loss: -0.47407247274601994
Epoch 137
-------------------------------
Batch 1/64 loss: -0.35773658752441406
Batch 2/64 loss: -0.21227645874023438
Batch 3/64 loss: -0.4303121566772461
Batch 4/64 loss: -0.414309024810791
Batch 5/64 loss: -0.16769027709960938
Batch 6/64 loss: -0.39539527893066406
Batch 7/64 loss: -0.48433685302734375
Batch 8/64 loss: -0.3695368766784668
Batch 9/64 loss: -0.33842992782592773
Batch 10/64 loss: -0.40206098556518555
Batch 11/64 loss: -0.3024611473083496
Batch 12/64 loss: -0.434323787689209
Batch 13/64 loss: 0.009341239929199219
Batch 14/64 loss: -0.28916501998901367
Batch 15/64 loss: -0.31519651412963867
Batch 16/64 loss: -0.49013233184814453
Batch 17/64 loss: -0.5185031890869141
Batch 18/64 loss: -0.4829874038696289
Batch 19/64 loss: -0.3396153450012207
Batch 20/64 loss: -0.3610682487487793
Batch 21/64 loss: -0.3831596374511719
Batch 22/64 loss: -0.47877931594848633
Batch 23/64 loss: -0.5653600692749023
Batch 24/64 loss: -0.18761491775512695
Batch 25/64 loss: -0.38211727142333984
Batch 26/64 loss: -0.03320455551147461
Batch 27/64 loss: 0.9879517555236816
Batch 28/64 loss: -0.2438945770263672
Batch 29/64 loss: -0.5105795860290527
Batch 30/64 loss: 0.12539958953857422
Batch 31/64 loss: -0.4809412956237793
Batch 32/64 loss: -0.35002756118774414
Batch 33/64 loss: 0.17586946487426758
Batch 34/64 loss: -0.42023277282714844
Batch 35/64 loss: -0.33232927322387695
Batch 36/64 loss: -0.4983201026916504
Batch 37/64 loss: -0.35357189178466797
Batch 38/64 loss: -0.3728165626525879
Batch 39/64 loss: -0.2725543975830078
Batch 40/64 loss: -0.3817873001098633
Batch 41/64 loss: -0.5005617141723633
Batch 42/64 loss: -0.10761547088623047
Batch 43/64 loss: -0.3216981887817383
Batch 44/64 loss: -0.2795224189758301
Batch 45/64 loss: -0.412050724029541
Batch 46/64 loss: -0.4715118408203125
Batch 47/64 loss: -0.4836764335632324
Batch 48/64 loss: 0.16425561904907227
Batch 49/64 loss: 0.740020751953125
Batch 50/64 loss: -0.5300912857055664
Batch 51/64 loss: -0.36349916458129883
Batch 52/64 loss: -0.4053163528442383
Batch 53/64 loss: -0.36269378662109375
Batch 54/64 loss: -0.43497419357299805
Batch 55/64 loss: -0.5288591384887695
Batch 56/64 loss: -0.24708127975463867
Batch 57/64 loss: -0.515449047088623
Batch 58/64 loss: -0.23405027389526367
Batch 59/64 loss: -0.30980825424194336
Batch 60/64 loss: 0.8198332786560059
Batch 61/64 loss: 0.07581377029418945
Batch 62/64 loss: -0.28508996963500977
Batch 63/64 loss: -0.44699668884277344
Batch 64/64 loss: -3.9576950073242188
Epoch 137  Train loss: -0.32099075317382814  Val loss: -0.5530214473553949
Epoch 138
-------------------------------
Batch 1/64 loss: -0.34538984298706055
Batch 2/64 loss: -0.34603166580200195
Batch 3/64 loss: -0.34225034713745117
Batch 4/64 loss: 1.0008134841918945
Batch 5/64 loss: -0.28040027618408203
Batch 6/64 loss: -0.3561835289001465
Batch 7/64 loss: -0.09228658676147461
Batch 8/64 loss: -0.004724025726318359
Batch 9/64 loss: -0.16316509246826172
Batch 10/64 loss: -0.18974065780639648
Batch 11/64 loss: -0.21893930435180664
Batch 12/64 loss: -0.12710952758789062
Batch 13/64 loss: -0.2923460006713867
Batch 14/64 loss: -0.06705856323242188
Batch 15/64 loss: -0.37628746032714844
Batch 16/64 loss: -0.31658029556274414
Batch 17/64 loss: -0.3326840400695801
Batch 18/64 loss: -0.44452667236328125
Batch 19/64 loss: 0.1905217170715332
Batch 20/64 loss: -0.4022660255432129
Batch 21/64 loss: 0.1557002067565918
Batch 22/64 loss: -0.2923154830932617
Batch 23/64 loss: 0.04086923599243164
Batch 24/64 loss: -0.4212608337402344
Batch 25/64 loss: 0.08002614974975586
Batch 26/64 loss: -0.5641036033630371
Batch 27/64 loss: -0.3519158363342285
Batch 28/64 loss: -0.6075429916381836
Batch 29/64 loss: -0.36615943908691406
Batch 30/64 loss: -0.3628678321838379
Batch 31/64 loss: -0.34057140350341797
Batch 32/64 loss: 0.19717025756835938
Batch 33/64 loss: -0.47716665267944336
Batch 34/64 loss: -0.3484792709350586
Batch 35/64 loss: -0.30954694747924805
Batch 36/64 loss: 0.7425470352172852
Batch 37/64 loss: -0.3190751075744629
Batch 38/64 loss: -0.2844047546386719
Batch 39/64 loss: -0.2977180480957031
Batch 40/64 loss: -0.3581504821777344
Batch 41/64 loss: -0.27451276779174805
Batch 42/64 loss: -0.33231353759765625
Batch 43/64 loss: -0.24677276611328125
Batch 44/64 loss: -0.5024232864379883
Batch 45/64 loss: -0.368654727935791
Batch 46/64 loss: -0.3837862014770508
Batch 47/64 loss: -0.6145439147949219
Batch 48/64 loss: -0.5744762420654297
Batch 49/64 loss: -0.6131129264831543
Batch 50/64 loss: -0.44208431243896484
Batch 51/64 loss: -0.14310121536254883
Batch 52/64 loss: -0.3797574043273926
Batch 53/64 loss: -0.34858131408691406
Batch 54/64 loss: -0.1903519630432129
Batch 55/64 loss: -0.4418973922729492
Batch 56/64 loss: -0.39124107360839844
Batch 57/64 loss: -0.41000986099243164
Batch 58/64 loss: -0.06543827056884766
Batch 59/64 loss: -0.5729622840881348
Batch 60/64 loss: 1.0751090049743652
Batch 61/64 loss: -0.3453083038330078
Batch 62/64 loss: -0.46547985076904297
Batch 63/64 loss: -0.4909820556640625
Batch 64/64 loss: -3.2745370864868164
Epoch 138  Train loss: -0.28188527051140283  Val loss: -0.6750610587523156
Epoch 139
-------------------------------
Batch 1/64 loss: 0.23595476150512695
Batch 2/64 loss: -0.38938474655151367
Batch 3/64 loss: 0.02376699447631836
Batch 4/64 loss: -0.4968390464782715
Batch 5/64 loss: -0.42250585556030273
Batch 6/64 loss: -0.5004773139953613
Batch 7/64 loss: -0.5574946403503418
Batch 8/64 loss: -0.08466482162475586
Batch 9/64 loss: -0.10296344757080078
Batch 10/64 loss: -0.5939040184020996
Batch 11/64 loss: -0.4927239418029785
Batch 12/64 loss: -0.3831043243408203
Batch 13/64 loss: -0.023129940032958984
Batch 14/64 loss: -0.45494937896728516
Batch 15/64 loss: -0.44817590713500977
Batch 16/64 loss: -0.30676889419555664
Batch 17/64 loss: 0.2515869140625
Batch 18/64 loss: -0.4726700782775879
Batch 19/64 loss: -0.5632929801940918
Batch 20/64 loss: -0.3957056999206543
Batch 21/64 loss: -0.5168609619140625
Batch 22/64 loss: -0.38426685333251953
Batch 23/64 loss: -0.3121957778930664
Batch 24/64 loss: -0.17205572128295898
Batch 25/64 loss: -0.4606165885925293
Batch 26/64 loss: -0.516502857208252
Batch 27/64 loss: -0.5267653465270996
Batch 28/64 loss: 1.0653939247131348
Batch 29/64 loss: 0.5602269172668457
Batch 30/64 loss: -0.5239377021789551
Batch 31/64 loss: -0.4434175491333008
Batch 32/64 loss: -0.46705007553100586
Batch 33/64 loss: -0.39357900619506836
Batch 34/64 loss: -0.542426586151123
Batch 35/64 loss: -0.4802570343017578
Batch 36/64 loss: -0.49186277389526367
Batch 37/64 loss: -0.1432790756225586
Batch 38/64 loss: -0.4994387626647949
Batch 39/64 loss: 0.8695497512817383
Batch 40/64 loss: -0.44043684005737305
Batch 41/64 loss: -0.3959951400756836
Batch 42/64 loss: -0.4258108139038086
Batch 43/64 loss: -0.354581356048584
Batch 44/64 loss: -0.5373601913452148
Batch 45/64 loss: -0.38791370391845703
Batch 46/64 loss: -0.43442821502685547
Batch 47/64 loss: -0.40400218963623047
Batch 48/64 loss: -0.6152629852294922
Batch 49/64 loss: -0.18130826950073242
Batch 50/64 loss: -0.24483585357666016
Batch 51/64 loss: -0.4891848564147949
Batch 52/64 loss: -0.48612213134765625
Batch 53/64 loss: -0.4055767059326172
Batch 54/64 loss: -0.5358772277832031
Batch 55/64 loss: -0.4427967071533203
Batch 56/64 loss: -0.4981660842895508
Batch 57/64 loss: -0.5058250427246094
Batch 58/64 loss: -0.13169336318969727
Batch 59/64 loss: -0.5262155532836914
Batch 60/64 loss: -0.34089136123657227
Batch 61/64 loss: -0.2981376647949219
Batch 62/64 loss: -0.40570497512817383
Batch 63/64 loss: -0.3565640449523926
Batch 64/64 loss: -4.008168697357178
Epoch 139  Train loss: -0.3671781371621525  Val loss: -0.6193282268301318
Epoch 140
-------------------------------
Batch 1/64 loss: -0.1857156753540039
Batch 2/64 loss: -0.46821069717407227
Batch 3/64 loss: 0.5212926864624023
Batch 4/64 loss: -0.30025720596313477
Batch 5/64 loss: -0.44378137588500977
Batch 6/64 loss: 0.1438755989074707
Batch 7/64 loss: -0.4644474983215332
Batch 8/64 loss: -0.48546552658081055
Batch 9/64 loss: -0.5069937705993652
Batch 10/64 loss: -0.5322694778442383
Batch 11/64 loss: -0.6001038551330566
Batch 12/64 loss: -0.3773674964904785
Batch 13/64 loss: -0.5445880889892578
Batch 14/64 loss: -0.5626130104064941
Batch 15/64 loss: -0.36161184310913086
Batch 16/64 loss: -0.3089261054992676
Batch 17/64 loss: -0.5961771011352539
Batch 18/64 loss: -0.5722322463989258
Batch 19/64 loss: -0.40855884552001953
Batch 20/64 loss: -0.5182709693908691
Batch 21/64 loss: -0.3906993865966797
Batch 22/64 loss: -0.5225157737731934
Batch 23/64 loss: -0.398282527923584
Batch 24/64 loss: -0.5132598876953125
Batch 25/64 loss: -0.5634241104125977
Batch 26/64 loss: 0.9042878150939941
Batch 27/64 loss: -0.5473065376281738
Batch 28/64 loss: -0.3463764190673828
Batch 29/64 loss: -0.401700496673584
Batch 30/64 loss: -0.5802407264709473
Batch 31/64 loss: -0.5471673011779785
Batch 32/64 loss: -0.47693872451782227
Batch 33/64 loss: -0.4152369499206543
Batch 34/64 loss: 0.15379714965820312
Batch 35/64 loss: -0.4906454086303711
Batch 36/64 loss: -0.5573897361755371
Batch 37/64 loss: -0.5466418266296387
Batch 38/64 loss: -0.4567842483520508
Batch 39/64 loss: -0.524538516998291
Batch 40/64 loss: -0.35929441452026367
Batch 41/64 loss: -0.4681396484375
Batch 42/64 loss: -0.4660453796386719
Batch 43/64 loss: 1.1909141540527344
Batch 44/64 loss: -0.29660511016845703
Batch 45/64 loss: -0.31723690032958984
Batch 46/64 loss: -0.4605674743652344
Batch 47/64 loss: -0.4841580390930176
Batch 48/64 loss: -0.45253801345825195
Batch 49/64 loss: -0.43152666091918945
Batch 50/64 loss: -0.4822072982788086
Batch 51/64 loss: -0.41051197052001953
Batch 52/64 loss: -0.3816184997558594
Batch 53/64 loss: -0.5367889404296875
Batch 54/64 loss: -0.37689733505249023
Batch 55/64 loss: -0.26145172119140625
Batch 56/64 loss: 0.19569826126098633
Batch 57/64 loss: -0.395263671875
Batch 58/64 loss: -0.31925392150878906
Batch 59/64 loss: 0.19002389907836914
Batch 60/64 loss: -0.3359503746032715
Batch 61/64 loss: -0.30161571502685547
Batch 62/64 loss: -0.36422157287597656
Batch 63/64 loss: -0.15192127227783203
Batch 64/64 loss: -3.768232822418213
Epoch 140  Train loss: -0.377989621255912  Val loss: -0.6210484848808997
Epoch 141
-------------------------------
Batch 1/64 loss: -0.008234977722167969
Batch 2/64 loss: -0.4497199058532715
Batch 3/64 loss: -0.43791961669921875
Batch 4/64 loss: -0.37862730026245117
Batch 5/64 loss: -0.3557009696960449
Batch 6/64 loss: -0.5190691947937012
Batch 7/64 loss: -0.4747796058654785
Batch 8/64 loss: -0.4444413185119629
Batch 9/64 loss: -0.2868156433105469
Batch 10/64 loss: 0.06855201721191406
Batch 11/64 loss: -0.4454517364501953
Batch 12/64 loss: -0.4233584403991699
Batch 13/64 loss: 1.1388301849365234
Batch 14/64 loss: -0.289276123046875
Batch 15/64 loss: -0.05060720443725586
Batch 16/64 loss: -0.380979061126709
Batch 17/64 loss: -0.34490013122558594
Batch 18/64 loss: -0.33811187744140625
Batch 19/64 loss: -0.3606681823730469
Batch 20/64 loss: -0.26124095916748047
Batch 21/64 loss: -0.22925424575805664
Batch 22/64 loss: -0.4398174285888672
Batch 23/64 loss: -0.3994593620300293
Batch 24/64 loss: -0.2943887710571289
Batch 25/64 loss: -0.2089524269104004
Batch 26/64 loss: 0.1999969482421875
Batch 27/64 loss: -0.43692779541015625
Batch 28/64 loss: 0.13109445571899414
Batch 29/64 loss: -0.43762826919555664
Batch 30/64 loss: -0.3670921325683594
Batch 31/64 loss: -0.38282060623168945
Batch 32/64 loss: -0.4423213005065918
Batch 33/64 loss: -0.5356969833374023
Batch 34/64 loss: -0.03960561752319336
Batch 35/64 loss: -0.4972681999206543
Batch 36/64 loss: -0.15647649765014648
Batch 37/64 loss: -0.2887887954711914
Batch 38/64 loss: -0.20595407485961914
Batch 39/64 loss: -0.4304628372192383
Batch 40/64 loss: -0.3380560874938965
Batch 41/64 loss: -0.18012237548828125
Batch 42/64 loss: -0.4646768569946289
Batch 43/64 loss: -0.5882377624511719
Batch 44/64 loss: 0.7372031211853027
Batch 45/64 loss: -0.49593400955200195
Batch 46/64 loss: -0.40881824493408203
Batch 47/64 loss: -0.42162418365478516
Batch 48/64 loss: -0.5258026123046875
Batch 49/64 loss: -0.23450994491577148
Batch 50/64 loss: -0.5227141380310059
Batch 51/64 loss: -0.39136552810668945
Batch 52/64 loss: -0.3434624671936035
Batch 53/64 loss: -0.45129966735839844
Batch 54/64 loss: -0.442629337310791
Batch 55/64 loss: 0.10350275039672852
Batch 56/64 loss: -0.39290523529052734
Batch 57/64 loss: -0.42015933990478516
Batch 58/64 loss: -0.5019474029541016
Batch 59/64 loss: 0.47178220748901367
Batch 60/64 loss: -0.41290712356567383
Batch 61/64 loss: -0.25167036056518555
Batch 62/64 loss: -0.5225462913513184
Batch 63/64 loss: -0.341890811920166
Batch 64/64 loss: -3.9193873405456543
Epoch 141  Train loss: -0.3260341363794663  Val loss: -0.7142784669227207
Epoch 142
-------------------------------
Batch 1/64 loss: -0.3370804786682129
Batch 2/64 loss: -0.4189457893371582
Batch 3/64 loss: -0.4685235023498535
Batch 4/64 loss: -0.370208740234375
Batch 5/64 loss: -0.5513038635253906
Batch 6/64 loss: -0.5512871742248535
Batch 7/64 loss: -0.39493513107299805
Batch 8/64 loss: -0.4528365135192871
Batch 9/64 loss: -0.338076114654541
Batch 10/64 loss: 1.1617326736450195
Batch 11/64 loss: -0.48937129974365234
Batch 12/64 loss: -0.4801340103149414
Batch 13/64 loss: -0.20812416076660156
Batch 14/64 loss: -0.5004138946533203
Batch 15/64 loss: -0.4575071334838867
Batch 16/64 loss: -0.4375643730163574
Batch 17/64 loss: -0.47075986862182617
Batch 18/64 loss: -0.2627744674682617
Batch 19/64 loss: -0.5772843360900879
Batch 20/64 loss: -0.2065105438232422
Batch 21/64 loss: -0.3439927101135254
Batch 22/64 loss: 0.6844158172607422
Batch 23/64 loss: -0.521237850189209
Batch 24/64 loss: -0.41125059127807617
Batch 25/64 loss: -0.377352237701416
Batch 26/64 loss: -0.4408230781555176
Batch 27/64 loss: -0.43169450759887695
Batch 28/64 loss: -0.33446311950683594
Batch 29/64 loss: -0.4568452835083008
Batch 30/64 loss: -0.47010087966918945
Batch 31/64 loss: -0.4170188903808594
Batch 32/64 loss: -0.619044303894043
Batch 33/64 loss: -0.046053409576416016
Batch 34/64 loss: 0.12729978561401367
Batch 35/64 loss: -0.4230513572692871
Batch 36/64 loss: 0.02634429931640625
Batch 37/64 loss: -0.24387741088867188
Batch 38/64 loss: -0.47737550735473633
Batch 39/64 loss: 0.044058799743652344
Batch 40/64 loss: -0.4630584716796875
Batch 41/64 loss: -0.20177412033081055
Batch 42/64 loss: -0.4007248878479004
Batch 43/64 loss: -0.021105289459228516
Batch 44/64 loss: -0.21497488021850586
Batch 45/64 loss: -0.40277528762817383
Batch 46/64 loss: -0.25765419006347656
Batch 47/64 loss: -0.16597318649291992
Batch 48/64 loss: -0.25641393661499023
Batch 49/64 loss: -0.1808156967163086
Batch 50/64 loss: -0.3945474624633789
Batch 51/64 loss: -0.31130123138427734
Batch 52/64 loss: -0.3805832862854004
Batch 53/64 loss: -0.2403874397277832
Batch 54/64 loss: -0.4821891784667969
Batch 55/64 loss: -0.2027592658996582
Batch 56/64 loss: -0.4674696922302246
Batch 57/64 loss: -0.38678503036499023
Batch 58/64 loss: -0.34145689010620117
Batch 59/64 loss: -0.468442440032959
Batch 60/64 loss: -0.35556697845458984
Batch 61/64 loss: -0.43268537521362305
Batch 62/64 loss: 0.9697990417480469
Batch 63/64 loss: -0.27156972885131836
Batch 64/64 loss: -4.030283451080322
Epoch 142  Train loss: -0.3340846847085392  Val loss: -0.6678031187286901
Epoch 143
-------------------------------
Batch 1/64 loss: -0.3212738037109375
Batch 2/64 loss: -0.46935129165649414
Batch 3/64 loss: 0.0931539535522461
Batch 4/64 loss: -0.4492197036743164
Batch 5/64 loss: -0.2091832160949707
Batch 6/64 loss: -0.46134138107299805
Batch 7/64 loss: -0.40319108963012695
Batch 8/64 loss: -0.25158071517944336
Batch 9/64 loss: -0.4514942169189453
Batch 10/64 loss: -0.21962404251098633
Batch 11/64 loss: -0.40132999420166016
Batch 12/64 loss: -0.2600135803222656
Batch 13/64 loss: -0.4525465965270996
Batch 14/64 loss: -0.08127498626708984
Batch 15/64 loss: 0.710421085357666
Batch 16/64 loss: 0.24675464630126953
Batch 17/64 loss: 0.07785797119140625
Batch 18/64 loss: -0.49983882904052734
Batch 19/64 loss: -0.08525562286376953
Batch 20/64 loss: -0.42440319061279297
Batch 21/64 loss: -0.19411087036132812
Batch 22/64 loss: 0.06112194061279297
Batch 23/64 loss: -0.3713698387145996
Batch 24/64 loss: -0.29204702377319336
Batch 25/64 loss: -0.12217998504638672
Batch 26/64 loss: -0.40232372283935547
Batch 27/64 loss: -0.14855480194091797
Batch 28/64 loss: -0.3626217842102051
Batch 29/64 loss: 1.3443660736083984
Batch 30/64 loss: -0.09641122817993164
Batch 31/64 loss: -0.14162349700927734
Batch 32/64 loss: -0.3378734588623047
Batch 33/64 loss: 0.087066650390625
Batch 34/64 loss: 0.41797304153442383
Batch 35/64 loss: -0.29172563552856445
Batch 36/64 loss: -0.38606739044189453
Batch 37/64 loss: -0.48810434341430664
Batch 38/64 loss: -0.43289995193481445
Batch 39/64 loss: -0.5147662162780762
Batch 40/64 loss: -0.43848228454589844
Batch 41/64 loss: -0.4549999237060547
Batch 42/64 loss: -0.32229042053222656
Batch 43/64 loss: -0.5022692680358887
Batch 44/64 loss: -0.3892335891723633
Batch 45/64 loss: -0.3029499053955078
Batch 46/64 loss: -0.24692153930664062
Batch 47/64 loss: -0.3617572784423828
Batch 48/64 loss: -0.4119424819946289
Batch 49/64 loss: -0.08981847763061523
Batch 50/64 loss: -0.3669624328613281
Batch 51/64 loss: -0.29322195053100586
Batch 52/64 loss: -0.40343618392944336
Batch 53/64 loss: -0.35552167892456055
Batch 54/64 loss: -0.36101436614990234
Batch 55/64 loss: -0.24901056289672852
Batch 56/64 loss: -0.3860359191894531
Batch 57/64 loss: -0.13591861724853516
Batch 58/64 loss: -0.4514641761779785
Batch 59/64 loss: -0.3375067710876465
Batch 60/64 loss: -0.07499313354492188
Batch 61/64 loss: -0.36378049850463867
Batch 62/64 loss: -0.29044580459594727
Batch 63/64 loss: 0.5786008834838867
Batch 64/64 loss: -3.960043430328369
Epoch 143  Train loss: -0.2661379701950971  Val loss: -0.6504626978713622
Epoch 144
-------------------------------
Batch 1/64 loss: -0.5233774185180664
Batch 2/64 loss: -0.438295841217041
Batch 3/64 loss: -0.5829439163208008
Batch 4/64 loss: -0.4945950508117676
Batch 5/64 loss: -0.4538755416870117
Batch 6/64 loss: -0.4782562255859375
Batch 7/64 loss: -0.2809910774230957
Batch 8/64 loss: -0.545870304107666
Batch 9/64 loss: -0.17741632461547852
Batch 10/64 loss: -0.4400906562805176
Batch 11/64 loss: -0.47703981399536133
Batch 12/64 loss: -0.4825153350830078
Batch 13/64 loss: -0.4861612319946289
Batch 14/64 loss: -0.5325155258178711
Batch 15/64 loss: -0.49172163009643555
Batch 16/64 loss: 0.8628578186035156
Batch 17/64 loss: -0.3289155960083008
Batch 18/64 loss: -0.3982887268066406
Batch 19/64 loss: -0.1902909278869629
Batch 20/64 loss: -0.4262819290161133
Batch 21/64 loss: -0.12163639068603516
Batch 22/64 loss: -0.19007015228271484
Batch 23/64 loss: -0.4175100326538086
Batch 24/64 loss: -0.3597087860107422
Batch 25/64 loss: -0.39576053619384766
Batch 26/64 loss: -0.39971494674682617
Batch 27/64 loss: -0.5538902282714844
Batch 28/64 loss: 0.07208871841430664
Batch 29/64 loss: -0.4392061233520508
Batch 30/64 loss: -0.5903143882751465
Batch 31/64 loss: -0.3372683525085449
Batch 32/64 loss: -0.5758514404296875
Batch 33/64 loss: -0.2609119415283203
Batch 34/64 loss: -0.4190187454223633
Batch 35/64 loss: -0.5362839698791504
Batch 36/64 loss: -0.35813474655151367
Batch 37/64 loss: -0.4055333137512207
Batch 38/64 loss: -0.45331621170043945
Batch 39/64 loss: 0.6330814361572266
Batch 40/64 loss: -0.4672727584838867
Batch 41/64 loss: -0.4052863121032715
Batch 42/64 loss: -0.23506736755371094
Batch 43/64 loss: -0.5190153121948242
Batch 44/64 loss: -0.4739236831665039
Batch 45/64 loss: -0.42635536193847656
Batch 46/64 loss: -0.3992948532104492
Batch 47/64 loss: -0.4651613235473633
Batch 48/64 loss: -0.5549240112304688
Batch 49/64 loss: -0.5381903648376465
Batch 50/64 loss: -0.5017876625061035
Batch 51/64 loss: -0.10197925567626953
Batch 52/64 loss: -0.3692183494567871
Batch 53/64 loss: -0.26904964447021484
Batch 54/64 loss: 0.6544642448425293
Batch 55/64 loss: 0.003009319305419922
Batch 56/64 loss: -0.29644155502319336
Batch 57/64 loss: -0.47104835510253906
Batch 58/64 loss: -0.32396507263183594
Batch 59/64 loss: 1.1904053688049316
Batch 60/64 loss: -0.3074660301208496
Batch 61/64 loss: -0.451840877532959
Batch 62/64 loss: -0.09930706024169922
Batch 63/64 loss: 0.0840444564819336
Batch 64/64 loss: -3.8873820304870605
Epoch 144  Train loss: -0.34722750981648765  Val loss: -0.26903874767604974
Epoch 145
-------------------------------
Batch 1/64 loss: -0.35400819778442383
Batch 2/64 loss: -0.32817506790161133
Batch 3/64 loss: -0.4449172019958496
Batch 4/64 loss: 0.9065604209899902
Batch 5/64 loss: -0.43944311141967773
Batch 6/64 loss: -0.17820262908935547
Batch 7/64 loss: -0.44974565505981445
Batch 8/64 loss: -0.37691736221313477
Batch 9/64 loss: -0.512352466583252
Batch 10/64 loss: -0.42557430267333984
Batch 11/64 loss: -0.41829442977905273
Batch 12/64 loss: -0.3568401336669922
Batch 13/64 loss: -0.48659753799438477
Batch 14/64 loss: -0.39544248580932617
Batch 15/64 loss: -0.5079388618469238
Batch 16/64 loss: -0.5646672248840332
Batch 17/64 loss: -0.46010732650756836
Batch 18/64 loss: -0.37880516052246094
Batch 19/64 loss: 0.15229320526123047
Batch 20/64 loss: 0.1537923812866211
Batch 21/64 loss: -0.20541000366210938
Batch 22/64 loss: -0.3179044723510742
Batch 23/64 loss: -0.19608736038208008
Batch 24/64 loss: -0.371917724609375
Batch 25/64 loss: -0.5669827461242676
Batch 26/64 loss: -0.37776756286621094
Batch 27/64 loss: -0.2590146064758301
Batch 28/64 loss: -0.4421854019165039
Batch 29/64 loss: -0.2675933837890625
Batch 30/64 loss: -0.31798601150512695
Batch 31/64 loss: 0.03597593307495117
Batch 32/64 loss: -0.5456085205078125
Batch 33/64 loss: -0.37630748748779297
Batch 34/64 loss: -0.5088191032409668
Batch 35/64 loss: -0.20900535583496094
Batch 36/64 loss: -0.25556039810180664
Batch 37/64 loss: -0.5464615821838379
Batch 38/64 loss: -0.27129077911376953
Batch 39/64 loss: -0.4494013786315918
Batch 40/64 loss: -0.20148181915283203
Batch 41/64 loss: -0.1792149543762207
Batch 42/64 loss: -0.3551139831542969
Batch 43/64 loss: -0.2662324905395508
Batch 44/64 loss: -0.4905967712402344
Batch 45/64 loss: -0.09076261520385742
Batch 46/64 loss: -0.09120750427246094
Batch 47/64 loss: -0.5165634155273438
Batch 48/64 loss: -0.4610443115234375
Batch 49/64 loss: -0.2704939842224121
Batch 50/64 loss: -0.2864398956298828
Batch 51/64 loss: 0.653602123260498
Batch 52/64 loss: -0.4169735908508301
Batch 53/64 loss: -0.44298839569091797
Batch 54/64 loss: -0.3853440284729004
Batch 55/64 loss: -0.22185564041137695
Batch 56/64 loss: -0.17156553268432617
Batch 57/64 loss: 1.491919994354248
Batch 58/64 loss: -0.35207653045654297
Batch 59/64 loss: -0.29573917388916016
Batch 60/64 loss: 0.1959829330444336
Batch 61/64 loss: -0.30355310440063477
Batch 62/64 loss: -0.4657769203186035
Batch 63/64 loss: -0.36811304092407227
Batch 64/64 loss: -3.7072272300720215
Epoch 145  Train loss: -0.30410610460767556  Val loss: -0.6246738106114758
Epoch 146
-------------------------------
Batch 1/64 loss: -0.19482183456420898
Batch 2/64 loss: -0.4866628646850586
Batch 3/64 loss: -0.40301084518432617
Batch 4/64 loss: -0.3842616081237793
Batch 5/64 loss: -0.17841291427612305
Batch 6/64 loss: -0.5420827865600586
Batch 7/64 loss: -0.4790325164794922
Batch 8/64 loss: -0.18236160278320312
Batch 9/64 loss: -0.3865518569946289
Batch 10/64 loss: -0.23236322402954102
Batch 11/64 loss: -0.5032777786254883
Batch 12/64 loss: -0.32863473892211914
Batch 13/64 loss: -0.25166988372802734
Batch 14/64 loss: -0.3591279983520508
Batch 15/64 loss: -0.26131486892700195
Batch 16/64 loss: -0.5341248512268066
Batch 17/64 loss: 0.012770652770996094
Batch 18/64 loss: -0.4415774345397949
Batch 19/64 loss: -0.3245840072631836
Batch 20/64 loss: -0.2169651985168457
Batch 21/64 loss: -0.4544687271118164
Batch 22/64 loss: -0.4959278106689453
Batch 23/64 loss: -0.3390207290649414
Batch 24/64 loss: -0.5707316398620605
Batch 25/64 loss: -0.45519495010375977
Batch 26/64 loss: 0.08482789993286133
Batch 27/64 loss: -0.2120046615600586
Batch 28/64 loss: -0.16967105865478516
Batch 29/64 loss: -0.4288978576660156
Batch 30/64 loss: -0.2809138298034668
Batch 31/64 loss: -0.430450439453125
Batch 32/64 loss: 0.08970165252685547
Batch 33/64 loss: -0.4354419708251953
Batch 34/64 loss: -0.3450765609741211
Batch 35/64 loss: -0.19168806076049805
Batch 36/64 loss: -0.13264751434326172
Batch 37/64 loss: -0.4626736640930176
Batch 38/64 loss: -0.44074058532714844
Batch 39/64 loss: 0.47529077529907227
Batch 40/64 loss: -0.31334924697875977
Batch 41/64 loss: -0.5646781921386719
Batch 42/64 loss: -0.5162205696105957
Batch 43/64 loss: -0.5231690406799316
Batch 44/64 loss: -0.39937448501586914
Batch 45/64 loss: -0.44870471954345703
Batch 46/64 loss: 1.037343978881836
Batch 47/64 loss: -0.07416725158691406
Batch 48/64 loss: -0.1820216178894043
Batch 49/64 loss: -0.2213282585144043
Batch 50/64 loss: -0.1807093620300293
Batch 51/64 loss: -0.1470961570739746
Batch 52/64 loss: -0.0350651741027832
Batch 53/64 loss: -0.3753499984741211
Batch 54/64 loss: -0.3991851806640625
Batch 55/64 loss: -0.3106575012207031
Batch 56/64 loss: -0.43303632736206055
Batch 57/64 loss: 0.8930401802062988
Batch 58/64 loss: 0.006846904754638672
Batch 59/64 loss: -0.3835287094116211
Batch 60/64 loss: 0.27755022048950195
Batch 61/64 loss: -0.3930377960205078
Batch 62/64 loss: 0.4249610900878906
Batch 63/64 loss: -0.3210010528564453
Batch 64/64 loss: -3.8754453659057617
Epoch 146  Train loss: -0.2880363950542375  Val loss: -0.5163998554662331
Epoch 147
-------------------------------
Batch 1/64 loss: -0.321840763092041
Batch 2/64 loss: -0.28259801864624023
Batch 3/64 loss: -0.33896684646606445
Batch 4/64 loss: -0.15645217895507812
Batch 5/64 loss: -0.4268951416015625
Batch 6/64 loss: 0.04402971267700195
Batch 7/64 loss: -0.3489055633544922
Batch 8/64 loss: -0.2755751609802246
Batch 9/64 loss: -0.35650205612182617
Batch 10/64 loss: -0.13459205627441406
Batch 11/64 loss: -0.37229347229003906
Batch 12/64 loss: -0.3380594253540039
Batch 13/64 loss: -0.11150407791137695
Batch 14/64 loss: -0.31767702102661133
Batch 15/64 loss: -0.3307795524597168
Batch 16/64 loss: 0.32574892044067383
Batch 17/64 loss: -0.13521718978881836
Batch 18/64 loss: -0.1450648307800293
Batch 19/64 loss: -0.38837671279907227
Batch 20/64 loss: -0.5036382675170898
Batch 21/64 loss: -0.28946495056152344
Batch 22/64 loss: 0.7279219627380371
Batch 23/64 loss: -0.39188146591186523
Batch 24/64 loss: -0.4613914489746094
Batch 25/64 loss: -0.27484130859375
Batch 26/64 loss: -0.33231401443481445
Batch 27/64 loss: 0.17723321914672852
Batch 28/64 loss: -0.05603742599487305
Batch 29/64 loss: -0.1821422576904297
Batch 30/64 loss: -0.330810546875
Batch 31/64 loss: 0.08340263366699219
Batch 32/64 loss: -0.11793375015258789
Batch 33/64 loss: -0.3064250946044922
Batch 34/64 loss: -0.3084869384765625
Batch 35/64 loss: -0.22664403915405273
Batch 36/64 loss: 0.009792804718017578
Batch 37/64 loss: 0.7668595314025879
Batch 38/64 loss: -0.297027587890625
Batch 39/64 loss: -0.2918515205383301
Batch 40/64 loss: -0.4656343460083008
Batch 41/64 loss: 0.6878604888916016
Batch 42/64 loss: -0.4138927459716797
Batch 43/64 loss: -0.406982421875
Batch 44/64 loss: 0.3803987503051758
Batch 45/64 loss: -0.16944217681884766
Batch 46/64 loss: -0.3528003692626953
Batch 47/64 loss: -0.31786537170410156
Batch 48/64 loss: -0.3289813995361328
Batch 49/64 loss: 0.06300115585327148
Batch 50/64 loss: -0.061679840087890625
Batch 51/64 loss: -0.32037782669067383
Batch 52/64 loss: -0.20333242416381836
Batch 53/64 loss: 1.1280169486999512
Batch 54/64 loss: -0.23470735549926758
Batch 55/64 loss: -0.5332870483398438
Batch 56/64 loss: -0.1451711654663086
Batch 57/64 loss: -0.42313098907470703
Batch 58/64 loss: -0.16244792938232422
Batch 59/64 loss: -0.30539798736572266
Batch 60/64 loss: -0.25928592681884766
Batch 61/64 loss: 0.22844314575195312
Batch 62/64 loss: -0.45069360733032227
Batch 63/64 loss: -0.24833106994628906
Batch 64/64 loss: -3.9637012481689453
Epoch 147  Train loss: -0.2087168525247013  Val loss: -0.6228668501286981
Epoch 148
-------------------------------
Batch 1/64 loss: -0.3319053649902344
Batch 2/64 loss: -0.3327326774597168
Batch 3/64 loss: -0.2875990867614746
Batch 4/64 loss: -0.5366654396057129
Batch 5/64 loss: -0.2721538543701172
Batch 6/64 loss: -0.287017822265625
Batch 7/64 loss: -0.2164773941040039
Batch 8/64 loss: -0.4056968688964844
Batch 9/64 loss: -0.4531440734863281
Batch 10/64 loss: -0.440244197845459
Batch 11/64 loss: -0.4588813781738281
Batch 12/64 loss: -0.33271312713623047
Batch 13/64 loss: -0.3278360366821289
Batch 14/64 loss: -0.3760828971862793
Batch 15/64 loss: 0.13400840759277344
Batch 16/64 loss: 0.29828739166259766
Batch 17/64 loss: -0.3914680480957031
Batch 18/64 loss: -0.019941329956054688
Batch 19/64 loss: -0.4322643280029297
Batch 20/64 loss: -0.3032212257385254
Batch 21/64 loss: -0.27485084533691406
Batch 22/64 loss: -0.29056787490844727
Batch 23/64 loss: -0.30771446228027344
Batch 24/64 loss: -0.4112372398376465
Batch 25/64 loss: 0.10453605651855469
Batch 26/64 loss: -0.47279930114746094
Batch 27/64 loss: -0.39974546432495117
Batch 28/64 loss: -0.2680091857910156
Batch 29/64 loss: -0.4064154624938965
Batch 30/64 loss: -0.36119890213012695
Batch 31/64 loss: -0.5490355491638184
Batch 32/64 loss: 0.6417608261108398
Batch 33/64 loss: -0.46411609649658203
Batch 34/64 loss: -0.48508167266845703
Batch 35/64 loss: -0.5109820365905762
Batch 36/64 loss: -0.5708584785461426
Batch 37/64 loss: -0.5111255645751953
Batch 38/64 loss: -0.47813892364501953
Batch 39/64 loss: -0.2347254753112793
Batch 40/64 loss: -0.5942258834838867
Batch 41/64 loss: -0.39856815338134766
Batch 42/64 loss: -0.3355255126953125
Batch 43/64 loss: -0.48716259002685547
Batch 44/64 loss: -0.4636878967285156
Batch 45/64 loss: -0.5785555839538574
Batch 46/64 loss: -0.4803004264831543
Batch 47/64 loss: 0.05100536346435547
Batch 48/64 loss: -0.5310583114624023
Batch 49/64 loss: -0.6347355842590332
Batch 50/64 loss: -0.5987777709960938
Batch 51/64 loss: -0.40697431564331055
Batch 52/64 loss: -0.5352392196655273
Batch 53/64 loss: -0.26824188232421875
Batch 54/64 loss: -0.35720205307006836
Batch 55/64 loss: -0.40879344940185547
Batch 56/64 loss: -0.3543682098388672
Batch 57/64 loss: -0.3287525177001953
Batch 58/64 loss: -0.5850386619567871
Batch 59/64 loss: -0.4440574645996094
Batch 60/64 loss: 1.1073379516601562
Batch 61/64 loss: -0.4984307289123535
Batch 62/64 loss: -0.44406843185424805
Batch 63/64 loss: -0.3667426109313965
Batch 64/64 loss: -2.7264223098754883
Epoch 148  Train loss: -0.3609574224434647  Val loss: -0.7481335118873832
Saving best model, epoch: 148
Epoch 149
-------------------------------
Batch 1/64 loss: -0.5919303894042969
Batch 2/64 loss: -0.5384511947631836
Batch 3/64 loss: -0.22335386276245117
Batch 4/64 loss: -0.6458816528320312
Batch 5/64 loss: -0.3582887649536133
Batch 6/64 loss: -0.5289106369018555
Batch 7/64 loss: -0.4401712417602539
Batch 8/64 loss: -0.33445072174072266
Batch 9/64 loss: -0.5079913139343262
Batch 10/64 loss: -0.5211181640625
Batch 11/64 loss: -0.4177436828613281
Batch 12/64 loss: -0.44185876846313477
Batch 13/64 loss: -0.3775796890258789
Batch 14/64 loss: -0.5197353363037109
Batch 15/64 loss: -0.44171714782714844
Batch 16/64 loss: 0.5911941528320312
Batch 17/64 loss: -0.11801767349243164
Batch 18/64 loss: -0.535426139831543
Batch 19/64 loss: -0.3089570999145508
Batch 20/64 loss: -0.5532493591308594
Batch 21/64 loss: -0.6005749702453613
Batch 22/64 loss: -0.5528378486633301
Batch 23/64 loss: -0.47794485092163086
Batch 24/64 loss: -0.08475875854492188
Batch 25/64 loss: -0.4861578941345215
Batch 26/64 loss: -0.458651065826416
Batch 27/64 loss: -0.5015616416931152
Batch 28/64 loss: -0.4920830726623535
Batch 29/64 loss: 0.9848318099975586
Batch 30/64 loss: -0.4573650360107422
Batch 31/64 loss: -0.4635610580444336
Batch 32/64 loss: -0.5040225982666016
Batch 33/64 loss: -0.5171666145324707
Batch 34/64 loss: -0.5093550682067871
Batch 35/64 loss: -0.47014427185058594
Batch 36/64 loss: -0.21680355072021484
Batch 37/64 loss: -0.31905269622802734
Batch 38/64 loss: -0.25303220748901367
Batch 39/64 loss: -0.48974037170410156
Batch 40/64 loss: -0.5098004341125488
Batch 41/64 loss: -0.5019187927246094
Batch 42/64 loss: -0.3067746162414551
Batch 43/64 loss: -0.3979506492614746
Batch 44/64 loss: -0.5827813148498535
Batch 45/64 loss: 0.08503913879394531
Batch 46/64 loss: -0.5897307395935059
Batch 47/64 loss: -0.18309593200683594
Batch 48/64 loss: -0.43558359146118164
Batch 49/64 loss: -0.35121679306030273
Batch 50/64 loss: -0.059220314025878906
Batch 51/64 loss: -0.5063071250915527
Batch 52/64 loss: -0.538292407989502
Batch 53/64 loss: -0.5533385276794434
Batch 54/64 loss: -0.43553733825683594
Batch 55/64 loss: -0.4772496223449707
Batch 56/64 loss: 0.0796518325805664
Batch 57/64 loss: -0.45746707916259766
Batch 58/64 loss: -0.4281482696533203
Batch 59/64 loss: -0.39775562286376953
Batch 60/64 loss: 0.5430312156677246
Batch 61/64 loss: -0.37289905548095703
Batch 62/64 loss: -0.5942244529724121
Batch 63/64 loss: -0.4375782012939453
Batch 64/64 loss: -3.8043556213378906
Epoch 149  Train loss: -0.4069966409720627  Val loss: -0.3083967621793452
Epoch 150
-------------------------------
Batch 1/64 loss: -0.11608076095581055
Batch 2/64 loss: 0.01912689208984375
Batch 3/64 loss: 0.1591196060180664
Batch 4/64 loss: -0.29019689559936523
Batch 5/64 loss: -0.21097755432128906
Batch 6/64 loss: -0.11321258544921875
Batch 7/64 loss: -0.23080682754516602
Batch 8/64 loss: -0.19355297088623047
Batch 9/64 loss: 0.13576412200927734
Batch 10/64 loss: -0.317716121673584
Batch 11/64 loss: 0.10624885559082031
Batch 12/64 loss: 1.5886564254760742
Batch 13/64 loss: -0.04819202423095703
Batch 14/64 loss: 0.04920482635498047
Batch 15/64 loss: 0.8856844902038574
Batch 16/64 loss: 0.19580841064453125
Batch 17/64 loss: 0.15379619598388672
Batch 18/64 loss: 0.2564406394958496
Batch 19/64 loss: 0.3946695327758789
Batch 20/64 loss: 0.010487079620361328
Batch 21/64 loss: 0.316495418548584
Batch 22/64 loss: 0.09858417510986328
Batch 23/64 loss: 0.09575223922729492
Batch 24/64 loss: 0.6176137924194336
Batch 25/64 loss: 0.17227602005004883
Batch 26/64 loss: 0.20921087265014648
Batch 27/64 loss: 0.4330434799194336
Batch 28/64 loss: -0.002113819122314453
Batch 29/64 loss: 0.11184167861938477
Batch 30/64 loss: 0.2445964813232422
Batch 31/64 loss: 0.47657060623168945
Batch 32/64 loss: 0.9902997016906738
Batch 33/64 loss: 0.14449119567871094
Batch 34/64 loss: 1.348818302154541
Batch 35/64 loss: 0.40552520751953125
Batch 36/64 loss: 0.06799840927124023
Batch 37/64 loss: -0.02857828140258789
Batch 38/64 loss: 0.15322065353393555
Batch 39/64 loss: 0.2543482780456543
Batch 40/64 loss: 0.09750223159790039
Batch 41/64 loss: -0.07401371002197266
Batch 42/64 loss: 0.30345869064331055
Batch 43/64 loss: -0.08859729766845703
Batch 44/64 loss: -0.05976581573486328
Batch 45/64 loss: -0.1318039894104004
Batch 46/64 loss: -0.16120529174804688
Batch 47/64 loss: -0.057822227478027344
Batch 48/64 loss: -0.14406681060791016
Batch 49/64 loss: 0.08142328262329102
Batch 50/64 loss: 0.0173797607421875
Batch 51/64 loss: -0.10129976272583008
Batch 52/64 loss: -0.20246648788452148
Batch 53/64 loss: -0.08022642135620117
Batch 54/64 loss: -0.1909041404724121
Batch 55/64 loss: -0.31769895553588867
Batch 56/64 loss: -0.15224266052246094
Batch 57/64 loss: -0.0031414031982421875
Batch 58/64 loss: -0.16678857803344727
Batch 59/64 loss: -0.18510675430297852
Batch 60/64 loss: 0.10461139678955078
Batch 61/64 loss: -0.1465001106262207
Batch 62/64 loss: -0.07477140426635742
Batch 63/64 loss: -0.1488356590270996
Batch 64/64 loss: -3.716139793395996
Epoch 150  Train loss: 0.060773000530168125  Val loss: -0.3827204491264632
Epoch 151
-------------------------------
Batch 1/64 loss: -0.003948211669921875
Batch 2/64 loss: 0.2752857208251953
Batch 3/64 loss: -0.19536781311035156
Batch 4/64 loss: -0.24554443359375
Batch 5/64 loss: -0.27509212493896484
Batch 6/64 loss: -0.10817766189575195
Batch 7/64 loss: -0.17232418060302734
Batch 8/64 loss: -0.191131591796875
Batch 9/64 loss: -0.19288015365600586
Batch 10/64 loss: -0.14451980590820312
Batch 11/64 loss: 0.36777687072753906
Batch 12/64 loss: -0.20422744750976562
Batch 13/64 loss: -0.008938312530517578
Batch 14/64 loss: 0.28839731216430664
Batch 15/64 loss: 0.7043929100036621
Batch 16/64 loss: 0.34102582931518555
Batch 17/64 loss: 1.638608455657959
Batch 18/64 loss: 0.28983211517333984
Batch 19/64 loss: 0.6123051643371582
Batch 20/64 loss: 0.6793727874755859
Batch 21/64 loss: 1.0697240829467773
Batch 22/64 loss: 1.6428351402282715
Batch 23/64 loss: 1.1264963150024414
Batch 24/64 loss: 1.5115442276000977
Batch 25/64 loss: 0.9228010177612305
Batch 26/64 loss: 1.217432975769043
Batch 27/64 loss: 1.17120361328125
Batch 28/64 loss: 0.5825085639953613
Batch 29/64 loss: 0.7207646369934082
Batch 30/64 loss: 1.2751922607421875
Batch 31/64 loss: 1.7104792594909668
Batch 32/64 loss: 1.1456317901611328
Batch 33/64 loss: 1.2399678230285645
Batch 34/64 loss: 1.9240317344665527
Batch 35/64 loss: 0.6406412124633789
Batch 36/64 loss: 1.0236735343933105
Batch 37/64 loss: 1.328512191772461
Batch 38/64 loss: 0.995185375213623
Batch 39/64 loss: 1.2692975997924805
Batch 40/64 loss: 0.8009486198425293
Batch 41/64 loss: 0.49075984954833984
Batch 42/64 loss: 1.240518569946289
Batch 43/64 loss: 0.5536646842956543
Batch 44/64 loss: 0.8411774635314941
Batch 45/64 loss: 0.4845237731933594
Batch 46/64 loss: 1.0396618843078613
Batch 47/64 loss: 1.319068431854248
Batch 48/64 loss: 0.8405213356018066
Batch 49/64 loss: 0.8007001876831055
Batch 50/64 loss: 0.463381290435791
Batch 51/64 loss: 0.96270751953125
Batch 52/64 loss: 0.8564400672912598
Batch 53/64 loss: 0.687166690826416
Batch 54/64 loss: 0.2977781295776367
Batch 55/64 loss: 0.6648321151733398
Batch 56/64 loss: 0.35997772216796875
Batch 57/64 loss: 1.5544743537902832
Batch 58/64 loss: 0.9385924339294434
Batch 59/64 loss: 0.5323262214660645
Batch 60/64 loss: 1.08823823928833
Batch 61/64 loss: 0.42534589767456055
Batch 62/64 loss: 0.37192535400390625
Batch 63/64 loss: 0.756162166595459
Batch 64/64 loss: -3.2204294204711914
Epoch 151  Train loss: 0.6576994465846642  Val loss: 0.6920775056294969
Epoch 152
-------------------------------
Batch 1/64 loss: 1.0279221534729004
Batch 2/64 loss: 0.05099153518676758
Batch 3/64 loss: 0.6173305511474609
Batch 4/64 loss: 0.8888859748840332
Batch 5/64 loss: 0.6875548362731934
Batch 6/64 loss: 0.2726578712463379
Batch 7/64 loss: 0.2559995651245117
Batch 8/64 loss: 0.5762538909912109
Batch 9/64 loss: 0.03851795196533203
Batch 10/64 loss: 0.2876291275024414
Batch 11/64 loss: 0.20454883575439453
Batch 12/64 loss: 0.928957462310791
Batch 13/64 loss: 0.21822261810302734
Batch 14/64 loss: 0.1120920181274414
Batch 15/64 loss: 0.14682435989379883
Batch 16/64 loss: 0.8007445335388184
Batch 17/64 loss: 0.32134246826171875
Batch 18/64 loss: 1.8496747016906738
Batch 19/64 loss: 0.37786102294921875
Batch 20/64 loss: -0.014446258544921875
Batch 21/64 loss: 0.4248180389404297
Batch 22/64 loss: 0.4976673126220703
Batch 23/64 loss: 0.9379334449768066
Batch 24/64 loss: -0.18858098983764648
Batch 25/64 loss: -0.016315460205078125
Batch 26/64 loss: 0.5771946907043457
Batch 27/64 loss: 0.3451089859008789
Batch 28/64 loss: 0.07834482192993164
Batch 29/64 loss: -0.06637430191040039
Batch 30/64 loss: 0.5410895347595215
Batch 31/64 loss: 0.5695009231567383
Batch 32/64 loss: 0.2480788230895996
Batch 33/64 loss: 0.8322453498840332
Batch 34/64 loss: 0.09065628051757812
Batch 35/64 loss: 0.04404640197753906
Batch 36/64 loss: 0.39113616943359375
Batch 37/64 loss: 0.34831857681274414
Batch 38/64 loss: 0.5032873153686523
Batch 39/64 loss: -0.020621299743652344
Batch 40/64 loss: 0.4382820129394531
Batch 41/64 loss: 0.20591354370117188
Batch 42/64 loss: 0.25354480743408203
Batch 43/64 loss: -0.060575008392333984
Batch 44/64 loss: -0.1863260269165039
Batch 45/64 loss: 0.15050125122070312
Batch 46/64 loss: 0.1890397071838379
Batch 47/64 loss: 0.027160167694091797
Batch 48/64 loss: 0.012867927551269531
Batch 49/64 loss: 0.07022905349731445
Batch 50/64 loss: 1.171459674835205
Batch 51/64 loss: -0.05919504165649414
Batch 52/64 loss: -0.05182647705078125
Batch 53/64 loss: 0.07945680618286133
Batch 54/64 loss: 0.21819067001342773
Batch 55/64 loss: 0.05925178527832031
Batch 56/64 loss: -0.021835803985595703
Batch 57/64 loss: 1.4493746757507324
Batch 58/64 loss: -0.056746482849121094
Batch 59/64 loss: 0.07927703857421875
Batch 60/64 loss: -0.2099299430847168
Batch 61/64 loss: -0.10388803482055664
Batch 62/64 loss: -0.05652284622192383
Batch 63/64 loss: 0.23968505859375
Batch 64/64 loss: -3.7800121307373047
Epoch 152  Train loss: 0.26336438048119637  Val loss: -0.2693860031075494
Epoch 153
-------------------------------
Batch 1/64 loss: -0.10610818862915039
Batch 2/64 loss: -0.26998090744018555
Batch 3/64 loss: -0.07225704193115234
Batch 4/64 loss: -0.22575092315673828
Batch 5/64 loss: -0.07249593734741211
Batch 6/64 loss: 0.32679224014282227
Batch 7/64 loss: 0.28067731857299805
Batch 8/64 loss: -0.29755258560180664
Batch 9/64 loss: 0.016515731811523438
Batch 10/64 loss: 0.6318364143371582
Batch 11/64 loss: 0.18136835098266602
Batch 12/64 loss: -0.31043386459350586
Batch 13/64 loss: -0.20907211303710938
Batch 14/64 loss: -0.07299137115478516
Batch 15/64 loss: -0.09063243865966797
Batch 16/64 loss: 0.18898677825927734
Batch 17/64 loss: -0.024040699005126953
Batch 18/64 loss: 0.01231527328491211
Batch 19/64 loss: -0.08864784240722656
Batch 20/64 loss: -0.12734365463256836
Batch 21/64 loss: -0.2504572868347168
Batch 22/64 loss: -0.03042316436767578
Batch 23/64 loss: -0.08928108215332031
Batch 24/64 loss: -0.03490161895751953
Batch 25/64 loss: 0.12314414978027344
Batch 26/64 loss: -0.2153177261352539
Batch 27/64 loss: 0.40901851654052734
Batch 28/64 loss: -0.15906667709350586
Batch 29/64 loss: -0.2315077781677246
Batch 30/64 loss: -0.1850566864013672
Batch 31/64 loss: 0.12755060195922852
Batch 32/64 loss: -0.16757917404174805
Batch 33/64 loss: 0.35781431198120117
Batch 34/64 loss: 0.09519815444946289
Batch 35/64 loss: -0.07021570205688477
Batch 36/64 loss: -0.16187429428100586
Batch 37/64 loss: -0.20384931564331055
Batch 38/64 loss: -0.2889986038208008
Batch 39/64 loss: 0.012270927429199219
Batch 40/64 loss: -0.3075699806213379
Batch 41/64 loss: -0.20420360565185547
Batch 42/64 loss: -0.026109695434570312
Batch 43/64 loss: -0.360506534576416
Batch 44/64 loss: -0.29517459869384766
Batch 45/64 loss: -0.2104635238647461
Batch 46/64 loss: -0.27623414993286133
Batch 47/64 loss: -0.048983097076416016
Batch 48/64 loss: -0.05988883972167969
Batch 49/64 loss: -0.07818222045898438
Batch 50/64 loss: 1.0341763496398926
Batch 51/64 loss: -0.36572742462158203
Batch 52/64 loss: -0.34795141220092773
Batch 53/64 loss: -0.24248027801513672
Batch 54/64 loss: -0.2369246482849121
Batch 55/64 loss: 0.37769460678100586
Batch 56/64 loss: -0.2485671043395996
Batch 57/64 loss: 0.24277830123901367
Batch 58/64 loss: -0.23870038986206055
Batch 59/64 loss: -0.09624052047729492
Batch 60/64 loss: 1.206984519958496
Batch 61/64 loss: -0.3008599281311035
Batch 62/64 loss: 1.0766019821166992
Batch 63/64 loss: -0.27152156829833984
Batch 64/64 loss: -3.096752643585205
Epoch 153  Train loss: -0.06106613570568608  Val loss: -0.4116084829638504
Epoch 154
-------------------------------
Batch 1/64 loss: 0.016496658325195312
Batch 2/64 loss: 0.36519908905029297
Batch 3/64 loss: -0.2779383659362793
Batch 4/64 loss: -0.37387752532958984
Batch 5/64 loss: -0.11238336563110352
Batch 6/64 loss: -0.25495481491088867
Batch 7/64 loss: -0.18736028671264648
Batch 8/64 loss: 0.09303522109985352
Batch 9/64 loss: 0.23487186431884766
Batch 10/64 loss: -0.23988580703735352
Batch 11/64 loss: -0.017264366149902344
Batch 12/64 loss: -0.08863401412963867
Batch 13/64 loss: -0.24423551559448242
Batch 14/64 loss: 0.036414146423339844
Batch 15/64 loss: 0.8642668724060059
Batch 16/64 loss: -0.14911127090454102
Batch 17/64 loss: -0.29843759536743164
Batch 18/64 loss: 0.11510658264160156
Batch 19/64 loss: -0.16324663162231445
Batch 20/64 loss: -0.0939474105834961
Batch 21/64 loss: -0.25653648376464844
Batch 22/64 loss: -0.35930681228637695
Batch 23/64 loss: -0.31868600845336914
Batch 24/64 loss: -0.3010554313659668
Batch 25/64 loss: -0.11726808547973633
Batch 26/64 loss: -0.11870765686035156
Batch 27/64 loss: -0.17060136795043945
Batch 28/64 loss: -0.17349481582641602
Batch 29/64 loss: 0.015215396881103516
Batch 30/64 loss: -0.35561037063598633
Batch 31/64 loss: -0.19096946716308594
Batch 32/64 loss: -0.24316692352294922
Batch 33/64 loss: 0.21960687637329102
Batch 34/64 loss: 0.7873239517211914
Batch 35/64 loss: -0.24693870544433594
Batch 36/64 loss: -0.015307903289794922
Batch 37/64 loss: 0.1810135841369629
Batch 38/64 loss: -0.1838374137878418
Batch 39/64 loss: -0.0732421875
Batch 40/64 loss: -0.27295970916748047
Batch 41/64 loss: -0.21900606155395508
Batch 42/64 loss: -0.27544260025024414
Batch 43/64 loss: -0.14485883712768555
Batch 44/64 loss: 1.2131433486938477
Batch 45/64 loss: 0.31674623489379883
Batch 46/64 loss: -0.37603092193603516
Batch 47/64 loss: -0.32318925857543945
Batch 48/64 loss: 0.25467634201049805
Batch 49/64 loss: -0.14242792129516602
Batch 50/64 loss: -0.16715097427368164
Batch 51/64 loss: 0.17314386367797852
Batch 52/64 loss: -0.2985835075378418
Batch 53/64 loss: -0.21129560470581055
Batch 54/64 loss: -0.33382081985473633
Batch 55/64 loss: -0.2860746383666992
Batch 56/64 loss: -0.19420957565307617
Batch 57/64 loss: -0.30246686935424805
Batch 58/64 loss: -0.3299593925476074
Batch 59/64 loss: -0.2782425880432129
Batch 60/64 loss: -0.1491861343383789
Batch 61/64 loss: -0.28844642639160156
Batch 62/64 loss: 0.24451875686645508
Batch 63/64 loss: -0.04754209518432617
Batch 64/64 loss: -3.8646087646484375
Epoch 154  Train loss: -0.12603260115081188  Val loss: -0.17082416232918546
Epoch 155
-------------------------------
Batch 1/64 loss: 0.10881853103637695
Batch 2/64 loss: -0.23592185974121094
Batch 3/64 loss: -0.1942000389099121
Batch 4/64 loss: -0.10595273971557617
Batch 5/64 loss: 0.09961080551147461
Batch 6/64 loss: 0.23850584030151367
Batch 7/64 loss: -0.3243064880371094
Batch 8/64 loss: -0.3000645637512207
Batch 9/64 loss: -0.14778661727905273
Batch 10/64 loss: -0.18325376510620117
Batch 11/64 loss: -0.22616338729858398
Batch 12/64 loss: -0.30971479415893555
Batch 13/64 loss: 0.4660196304321289
Batch 14/64 loss: -0.24712419509887695
Batch 15/64 loss: -0.3631172180175781
Batch 16/64 loss: 0.06849193572998047
Batch 17/64 loss: 0.14490556716918945
Batch 18/64 loss: -0.28960609436035156
Batch 19/64 loss: 0.08814620971679688
Batch 20/64 loss: -0.2047896385192871
Batch 21/64 loss: -0.28166723251342773
Batch 22/64 loss: -0.09199142456054688
Batch 23/64 loss: -0.34988927841186523
Batch 24/64 loss: 0.4040408134460449
Batch 25/64 loss: -0.08832406997680664
Batch 26/64 loss: -0.1501941680908203
Batch 27/64 loss: -0.25690460205078125
Batch 28/64 loss: -0.1506814956665039
Batch 29/64 loss: -0.29022884368896484
Batch 30/64 loss: 0.1568307876586914
Batch 31/64 loss: 0.17974567413330078
Batch 32/64 loss: 0.08374881744384766
Batch 33/64 loss: 1.526933193206787
Batch 34/64 loss: -0.011621475219726562
Batch 35/64 loss: 0.059908390045166016
Batch 36/64 loss: -0.20767450332641602
Batch 37/64 loss: -0.056934356689453125
Batch 38/64 loss: -0.017324447631835938
Batch 39/64 loss: 1.3605170249938965
Batch 40/64 loss: -0.16404247283935547
Batch 41/64 loss: -0.27815866470336914
Batch 42/64 loss: -0.08842897415161133
Batch 43/64 loss: 2.0631022453308105
Batch 44/64 loss: 0.5612602233886719
Batch 45/64 loss: -0.10809564590454102
Batch 46/64 loss: -0.17148065567016602
Batch 47/64 loss: -0.2785353660583496
Batch 48/64 loss: -0.19867706298828125
Batch 49/64 loss: -0.3060450553894043
Batch 50/64 loss: -0.19804954528808594
Batch 51/64 loss: 0.5332579612731934
Batch 52/64 loss: -0.17745018005371094
Batch 53/64 loss: -0.1027073860168457
Batch 54/64 loss: -0.14749622344970703
Batch 55/64 loss: -0.18583965301513672
Batch 56/64 loss: -0.3036203384399414
Batch 57/64 loss: 0.0453190803527832
Batch 58/64 loss: -0.13294744491577148
Batch 59/64 loss: -0.06670045852661133
Batch 60/64 loss: 0.10622596740722656
Batch 61/64 loss: -0.05562877655029297
Batch 62/64 loss: -0.2451796531677246
Batch 63/64 loss: -0.20024871826171875
Batch 64/64 loss: -3.524514675140381
Epoch 155  Train loss: -0.04459242166257372  Val loss: -0.3798360136366382
Epoch 156
-------------------------------
Batch 1/64 loss: -0.37617969512939453
Batch 2/64 loss: 0.007961750030517578
Batch 3/64 loss: -0.16753053665161133
Batch 4/64 loss: -0.2177600860595703
Batch 5/64 loss: -0.26193952560424805
Batch 6/64 loss: -0.33759164810180664
Batch 7/64 loss: 0.7206978797912598
Batch 8/64 loss: -0.09205198287963867
Batch 9/64 loss: -0.30813121795654297
Batch 10/64 loss: -0.28464174270629883
Batch 11/64 loss: 0.08555221557617188
Batch 12/64 loss: 1.369673728942871
Batch 13/64 loss: -0.1598496437072754
Batch 14/64 loss: -0.12038183212280273
Batch 15/64 loss: -0.16486167907714844
Batch 16/64 loss: 0.7655797004699707
Batch 17/64 loss: -0.02473163604736328
Batch 18/64 loss: -0.3681144714355469
Batch 19/64 loss: 0.18956995010375977
Batch 20/64 loss: -0.15729999542236328
Batch 21/64 loss: -0.23812532424926758
Batch 22/64 loss: 0.27644920349121094
Batch 23/64 loss: -0.2859663963317871
Batch 24/64 loss: -0.15245771408081055
Batch 25/64 loss: -0.41500282287597656
Batch 26/64 loss: 0.3641529083251953
Batch 27/64 loss: -0.3589324951171875
Batch 28/64 loss: -0.46303606033325195
Batch 29/64 loss: -0.4019455909729004
Batch 30/64 loss: -0.5127425193786621
Batch 31/64 loss: -0.49695396423339844
Batch 32/64 loss: -0.21895551681518555
Batch 33/64 loss: -0.05037355422973633
Batch 34/64 loss: -0.270200252532959
Batch 35/64 loss: -0.3329768180847168
Batch 36/64 loss: -0.16648435592651367
Batch 37/64 loss: -0.45542478561401367
Batch 38/64 loss: -0.29201698303222656
Batch 39/64 loss: -0.2145524024963379
Batch 40/64 loss: 0.8288354873657227
Batch 41/64 loss: -0.27463579177856445
Batch 42/64 loss: -0.14469671249389648
Batch 43/64 loss: 0.0556797981262207
Batch 44/64 loss: -0.16919851303100586
Batch 45/64 loss: -0.14434242248535156
Batch 46/64 loss: -0.3437075614929199
Batch 47/64 loss: -0.45632410049438477
Batch 48/64 loss: -0.40615415573120117
Batch 49/64 loss: -0.30349063873291016
Batch 50/64 loss: -0.2793436050415039
Batch 51/64 loss: -0.2692422866821289
Batch 52/64 loss: -0.4153470993041992
Batch 53/64 loss: -0.4052696228027344
Batch 54/64 loss: -0.30045509338378906
Batch 55/64 loss: 0.13946771621704102
Batch 56/64 loss: -0.3579587936401367
Batch 57/64 loss: -0.40213966369628906
Batch 58/64 loss: -0.32600975036621094
Batch 59/64 loss: -0.30252504348754883
Batch 60/64 loss: -0.33416318893432617
Batch 61/64 loss: -0.03291654586791992
Batch 62/64 loss: -0.3032045364379883
Batch 63/64 loss: 0.09547281265258789
Batch 64/64 loss: -3.29617977142334
Epoch 156  Train loss: -0.18684517729516123  Val loss: -0.45501267213592006
Epoch 157
-------------------------------
Batch 1/64 loss: -0.41656923294067383
Batch 2/64 loss: -0.4380936622619629
Batch 3/64 loss: -0.3141040802001953
Batch 4/64 loss: -0.24649953842163086
Batch 5/64 loss: -0.37360095977783203
Batch 6/64 loss: -0.1974496841430664
Batch 7/64 loss: 0.0435185432434082
Batch 8/64 loss: -0.20537042617797852
Batch 9/64 loss: -0.3077530860900879
Batch 10/64 loss: -0.353973388671875
Batch 11/64 loss: -0.4138765335083008
Batch 12/64 loss: -0.3081212043762207
Batch 13/64 loss: -0.03095102310180664
Batch 14/64 loss: -0.25760459899902344
Batch 15/64 loss: -0.32145166397094727
Batch 16/64 loss: -0.1374187469482422
Batch 17/64 loss: -0.3158426284790039
Batch 18/64 loss: 0.365664005279541
Batch 19/64 loss: -0.32863903045654297
Batch 20/64 loss: -0.3882279396057129
Batch 21/64 loss: -0.3700861930847168
Batch 22/64 loss: -0.3823122978210449
Batch 23/64 loss: 0.32018041610717773
Batch 24/64 loss: -0.3475632667541504
Batch 25/64 loss: -0.1129159927368164
Batch 26/64 loss: -0.34709787368774414
Batch 27/64 loss: 0.9643344879150391
Batch 28/64 loss: -0.4278984069824219
Batch 29/64 loss: -0.255617618560791
Batch 30/64 loss: -0.38846778869628906
Batch 31/64 loss: -0.2559943199157715
Batch 32/64 loss: 1.275763988494873
Batch 33/64 loss: -0.27014636993408203
Batch 34/64 loss: -0.11731386184692383
Batch 35/64 loss: -0.30632686614990234
Batch 36/64 loss: 0.7328829765319824
Batch 37/64 loss: -0.23968505859375
Batch 38/64 loss: -0.3340463638305664
Batch 39/64 loss: -0.20067644119262695
Batch 40/64 loss: -0.10295486450195312
Batch 41/64 loss: -0.2971787452697754
Batch 42/64 loss: -0.024745941162109375
Batch 43/64 loss: -0.3134284019470215
Batch 44/64 loss: -0.32109642028808594
Batch 45/64 loss: -0.19000720977783203
Batch 46/64 loss: -0.2669076919555664
Batch 47/64 loss: -0.3724064826965332
Batch 48/64 loss: -0.03770780563354492
Batch 49/64 loss: -0.27763986587524414
Batch 50/64 loss: 3.8231163024902344
Batch 51/64 loss: 1.5069270133972168
Batch 52/64 loss: 2.856912136077881
Batch 53/64 loss: 1.6286253929138184
Batch 54/64 loss: 0.8042464256286621
Batch 55/64 loss: 0.8796191215515137
Batch 56/64 loss: 0.6862235069274902
Batch 57/64 loss: 0.7921690940856934
Batch 58/64 loss: 0.691497802734375
Batch 59/64 loss: 0.7493610382080078
Batch 60/64 loss: 0.8696832656860352
Batch 61/64 loss: 0.2360091209411621
Batch 62/64 loss: 0.7337226867675781
Batch 63/64 loss: 0.745964527130127
Batch 64/64 loss: -3.290226936340332
Epoch 157  Train loss: 0.09918403251498353  Val loss: 0.447637013962998
Epoch 158
-------------------------------
Batch 1/64 loss: 0.2892274856567383
Batch 2/64 loss: 0.5021862983703613
Batch 3/64 loss: 0.8737754821777344
Batch 4/64 loss: 0.2657289505004883
Batch 5/64 loss: 0.9676275253295898
Batch 6/64 loss: 0.5294704437255859
Batch 7/64 loss: 0.4684920310974121
Batch 8/64 loss: 0.24437761306762695
Batch 9/64 loss: 0.42084789276123047
Batch 10/64 loss: 0.00977468490600586
Batch 11/64 loss: 0.48342418670654297
Batch 12/64 loss: 0.6057844161987305
Batch 13/64 loss: 0.36022377014160156
Batch 14/64 loss: 0.11549997329711914
Batch 15/64 loss: 0.4043898582458496
Batch 16/64 loss: 0.25606536865234375
Batch 17/64 loss: 0.15221786499023438
Batch 18/64 loss: -0.022663593292236328
Batch 19/64 loss: -0.17141294479370117
Batch 20/64 loss: 0.1579132080078125
Batch 21/64 loss: 1.428767204284668
Batch 22/64 loss: 0.9557833671569824
Batch 23/64 loss: -0.1102762222290039
Batch 24/64 loss: -0.2551999092102051
Batch 25/64 loss: 0.017778396606445312
Batch 26/64 loss: -0.2640676498413086
Batch 27/64 loss: -0.05752229690551758
Batch 28/64 loss: 0.023608684539794922
Batch 29/64 loss: -0.21930313110351562
Batch 30/64 loss: -0.1287546157836914
Batch 31/64 loss: -0.10480260848999023
Batch 32/64 loss: -0.16045284271240234
Batch 33/64 loss: -0.06556272506713867
Batch 34/64 loss: 0.04189920425415039
Batch 35/64 loss: 0.1571950912475586
Batch 36/64 loss: -0.3263072967529297
Batch 37/64 loss: -0.23364543914794922
Batch 38/64 loss: -0.14145898818969727
Batch 39/64 loss: -0.0014376640319824219
Batch 40/64 loss: -0.08474540710449219
Batch 41/64 loss: -0.23241281509399414
Batch 42/64 loss: 0.27607059478759766
Batch 43/64 loss: 0.27592039108276367
Batch 44/64 loss: -0.17910432815551758
Batch 45/64 loss: -0.3293757438659668
Batch 46/64 loss: 0.2819833755493164
Batch 47/64 loss: -0.057999610900878906
Batch 48/64 loss: -0.18817138671875
Batch 49/64 loss: -0.08076143264770508
Batch 50/64 loss: -0.2878565788269043
Batch 51/64 loss: -0.11963891983032227
Batch 52/64 loss: -0.2887234687805176
Batch 53/64 loss: -0.17713499069213867
Batch 54/64 loss: -0.26524925231933594
Batch 55/64 loss: 1.0936923027038574
Batch 56/64 loss: 0.014163494110107422
Batch 57/64 loss: -0.32370662689208984
Batch 58/64 loss: -0.22332096099853516
Batch 59/64 loss: -0.13536643981933594
Batch 60/64 loss: -0.2204751968383789
Batch 61/64 loss: 0.020194530487060547
Batch 62/64 loss: -0.3450145721435547
Batch 63/64 loss: -0.21965456008911133
Batch 64/64 loss: -3.7910284996032715
Epoch 158  Train loss: 0.04438011132034601  Val loss: -0.4905111175222495
Epoch 159
-------------------------------
Batch 1/64 loss: -0.36568212509155273
Batch 2/64 loss: 0.7636680603027344
Batch 3/64 loss: -0.11172914505004883
Batch 4/64 loss: -0.16332387924194336
Batch 5/64 loss: -0.21113967895507812
Batch 6/64 loss: -0.2581520080566406
Batch 7/64 loss: 0.5007576942443848
Batch 8/64 loss: -0.27843332290649414
Batch 9/64 loss: -0.14449167251586914
Batch 10/64 loss: -0.30095767974853516
Batch 11/64 loss: -0.0718240737915039
Batch 12/64 loss: -0.1523113250732422
Batch 13/64 loss: -0.24401473999023438
Batch 14/64 loss: -0.2137775421142578
Batch 15/64 loss: -0.2855839729309082
Batch 16/64 loss: -0.35999631881713867
Batch 17/64 loss: -0.14725303649902344
Batch 18/64 loss: -0.11179208755493164
Batch 19/64 loss: -0.2892642021179199
Batch 20/64 loss: -0.3771376609802246
Batch 21/64 loss: -0.03542184829711914
Batch 22/64 loss: -0.11560392379760742
Batch 23/64 loss: -0.23748540878295898
Batch 24/64 loss: 0.002872467041015625
Batch 25/64 loss: 1.5972695350646973
Batch 26/64 loss: 0.2400226593017578
Batch 27/64 loss: -0.09734678268432617
Batch 28/64 loss: -0.2944340705871582
Batch 29/64 loss: -0.21599388122558594
Batch 30/64 loss: -0.14142560958862305
Batch 31/64 loss: -0.22893142700195312
Batch 32/64 loss: 0.2992720603942871
Batch 33/64 loss: -0.16888427734375
Batch 34/64 loss: -0.0897512435913086
Batch 35/64 loss: -0.22003793716430664
Batch 36/64 loss: -0.28611278533935547
Batch 37/64 loss: 0.46590232849121094
Batch 38/64 loss: -0.17801904678344727
Batch 39/64 loss: 0.06741905212402344
Batch 40/64 loss: -0.1573781967163086
Batch 41/64 loss: -0.38485097885131836
Batch 42/64 loss: -0.34629344940185547
Batch 43/64 loss: -0.29753684997558594
Batch 44/64 loss: -0.09578990936279297
Batch 45/64 loss: -0.15270185470581055
Batch 46/64 loss: -0.22389745712280273
Batch 47/64 loss: -0.1624460220336914
Batch 48/64 loss: 0.19389724731445312
Batch 49/64 loss: -0.23841285705566406
Batch 50/64 loss: -0.1719374656677246
Batch 51/64 loss: -0.3589205741882324
Batch 52/64 loss: -0.3779292106628418
Batch 53/64 loss: -0.40459203720092773
Batch 54/64 loss: -0.055825233459472656
Batch 55/64 loss: -0.40700626373291016
Batch 56/64 loss: -0.008041858673095703
Batch 57/64 loss: 0.9952020645141602
Batch 58/64 loss: -0.23453950881958008
Batch 59/64 loss: -0.2157425880432129
Batch 60/64 loss: -0.31354236602783203
Batch 61/64 loss: -0.27031469345092773
Batch 62/64 loss: -0.04069709777832031
Batch 63/64 loss: -0.16947507858276367
Batch 64/64 loss: -3.5847010612487793
Epoch 159  Train loss: -0.14190473556518554  Val loss: -0.5312590844852408
Epoch 160
-------------------------------
Batch 1/64 loss: -0.2028212547302246
Batch 2/64 loss: -0.1105051040649414
Batch 3/64 loss: -0.09238052368164062
Batch 4/64 loss: -0.17307567596435547
Batch 5/64 loss: -0.4116806983947754
Batch 6/64 loss: 0.1706080436706543
Batch 7/64 loss: -0.22263383865356445
Batch 8/64 loss: -0.4576077461242676
Batch 9/64 loss: -0.20743846893310547
Batch 10/64 loss: -0.40927696228027344
Batch 11/64 loss: -0.13412237167358398
Batch 12/64 loss: -0.12080192565917969
Batch 13/64 loss: -0.3318338394165039
Batch 14/64 loss: 0.17606067657470703
Batch 15/64 loss: -0.40958547592163086
Batch 16/64 loss: -0.2931666374206543
Batch 17/64 loss: -0.21088600158691406
Batch 18/64 loss: 0.7237715721130371
Batch 19/64 loss: -0.4002203941345215
Batch 20/64 loss: -0.3340425491333008
Batch 21/64 loss: -0.4922966957092285
Batch 22/64 loss: 0.028339862823486328
Batch 23/64 loss: -0.36160850524902344
Batch 24/64 loss: -0.40726804733276367
Batch 25/64 loss: -0.25408029556274414
Batch 26/64 loss: -0.06557559967041016
Batch 27/64 loss: -0.5631475448608398
Batch 28/64 loss: -0.3347311019897461
Batch 29/64 loss: -0.5255355834960938
Batch 30/64 loss: -0.2778911590576172
Batch 31/64 loss: -0.5432767868041992
Batch 32/64 loss: -0.45429563522338867
Batch 33/64 loss: -0.3135647773742676
Batch 34/64 loss: -0.3114495277404785
Batch 35/64 loss: -0.43169546127319336
Batch 36/64 loss: -0.4066290855407715
Batch 37/64 loss: -0.32036828994750977
Batch 38/64 loss: -0.4120755195617676
Batch 39/64 loss: -0.31334733963012695
Batch 40/64 loss: -0.43590545654296875
Batch 41/64 loss: -0.2448887825012207
Batch 42/64 loss: -0.19408702850341797
Batch 43/64 loss: 0.9037647247314453
Batch 44/64 loss: 0.9952058792114258
Batch 45/64 loss: 0.3332529067993164
Batch 46/64 loss: 0.38904285430908203
Batch 47/64 loss: 1.0107402801513672
Batch 48/64 loss: 0.8090429306030273
Batch 49/64 loss: 1.0709500312805176
Batch 50/64 loss: 1.1695256233215332
Batch 51/64 loss: 1.2351102828979492
Batch 52/64 loss: 0.7804250717163086
Batch 53/64 loss: 1.7244796752929688
Batch 54/64 loss: 0.9536256790161133
Batch 55/64 loss: 1.0533084869384766
Batch 56/64 loss: 1.1641817092895508
Batch 57/64 loss: 0.5227136611938477
Batch 58/64 loss: 1.0429792404174805
Batch 59/64 loss: 0.7399387359619141
Batch 60/64 loss: 0.5846691131591797
Batch 61/64 loss: 0.21488285064697266
Batch 62/64 loss: 0.45700693130493164
Batch 63/64 loss: 0.29102611541748047
Batch 64/64 loss: -3.1667799949645996
Epoch 160  Train loss: 0.06249051374547622  Val loss: 0.5391534235059601
Epoch 161
-------------------------------
Batch 1/64 loss: 0.4261302947998047
Batch 2/64 loss: 0.4039463996887207
Batch 3/64 loss: 0.730949878692627
Batch 4/64 loss: 0.21929645538330078
Batch 5/64 loss: 0.09155988693237305
Batch 6/64 loss: 0.2529921531677246
Batch 7/64 loss: 0.25527238845825195
Batch 8/64 loss: 0.40462160110473633
Batch 9/64 loss: 1.235166072845459
Batch 10/64 loss: -0.06645011901855469
Batch 11/64 loss: 0.30246639251708984
Batch 12/64 loss: 0.27829980850219727
Batch 13/64 loss: 0.01575613021850586
Batch 14/64 loss: 0.026669025421142578
Batch 15/64 loss: -0.0679011344909668
Batch 16/64 loss: 0.15975713729858398
Batch 17/64 loss: 0.03039264678955078
Batch 18/64 loss: -0.18166017532348633
Batch 19/64 loss: 0.3391256332397461
Batch 20/64 loss: -0.11327886581420898
Batch 21/64 loss: -0.06573200225830078
Batch 22/64 loss: -0.30656003952026367
Batch 23/64 loss: -0.28366947174072266
Batch 24/64 loss: 0.022258758544921875
Batch 25/64 loss: -0.05511045455932617
Batch 26/64 loss: -0.21160078048706055
Batch 27/64 loss: 0.21213436126708984
Batch 28/64 loss: 0.1556839942932129
Batch 29/64 loss: -0.23566627502441406
Batch 30/64 loss: -0.2154560089111328
Batch 31/64 loss: 0.37775468826293945
Batch 32/64 loss: 0.14840173721313477
Batch 33/64 loss: -0.16304683685302734
Batch 34/64 loss: -0.3795337677001953
Batch 35/64 loss: 0.10253763198852539
Batch 36/64 loss: -0.1704692840576172
Batch 37/64 loss: -0.01491689682006836
Batch 38/64 loss: 0.784611701965332
Batch 39/64 loss: -0.3067798614501953
Batch 40/64 loss: -0.05244922637939453
Batch 41/64 loss: -0.08043813705444336
Batch 42/64 loss: -0.09045124053955078
Batch 43/64 loss: 0.012727737426757812
Batch 44/64 loss: -0.23636627197265625
Batch 45/64 loss: -0.09656000137329102
Batch 46/64 loss: -0.15735816955566406
Batch 47/64 loss: -0.030712127685546875
Batch 48/64 loss: -0.10987281799316406
Batch 49/64 loss: -0.24454116821289062
Batch 50/64 loss: -0.10924577713012695
Batch 51/64 loss: -0.3136324882507324
Batch 52/64 loss: 1.3934803009033203
Batch 53/64 loss: -0.05986833572387695
Batch 54/64 loss: -0.1519937515258789
Batch 55/64 loss: -0.1363520622253418
Batch 56/64 loss: -0.28160715103149414
Batch 57/64 loss: -0.027126312255859375
Batch 58/64 loss: -0.05238151550292969
Batch 59/64 loss: -0.08084583282470703
Batch 60/64 loss: -0.1886286735534668
Batch 61/64 loss: 0.3096017837524414
Batch 62/64 loss: -0.22619152069091797
Batch 63/64 loss: -0.4539475440979004
Batch 64/64 loss: -3.8300366401672363
Epoch 161  Train loss: -0.0031268232008990118  Val loss: -0.4934085570659834
Epoch 162
-------------------------------
Batch 1/64 loss: -0.4065537452697754
Batch 2/64 loss: -0.3005671501159668
Batch 3/64 loss: -0.014087677001953125
Batch 4/64 loss: 0.1301403045654297
Batch 5/64 loss: -0.18519973754882812
Batch 6/64 loss: -0.3978462219238281
Batch 7/64 loss: -0.3361220359802246
Batch 8/64 loss: -0.07520294189453125
Batch 9/64 loss: -0.4942479133605957
Batch 10/64 loss: 0.07847929000854492
Batch 11/64 loss: -0.4981198310852051
Batch 12/64 loss: -0.268068790435791
Batch 13/64 loss: -0.22985029220581055
Batch 14/64 loss: -0.0664973258972168
Batch 15/64 loss: -0.35213613510131836
Batch 16/64 loss: 0.0699000358581543
Batch 17/64 loss: -0.2493600845336914
Batch 18/64 loss: -0.3693375587463379
Batch 19/64 loss: 0.10720586776733398
Batch 20/64 loss: -0.18905401229858398
Batch 21/64 loss: 0.12360000610351562
Batch 22/64 loss: -0.40805816650390625
Batch 23/64 loss: 0.03554868698120117
Batch 24/64 loss: -0.3605685234069824
Batch 25/64 loss: 0.9688067436218262
Batch 26/64 loss: -0.3607950210571289
Batch 27/64 loss: -0.3762044906616211
Batch 28/64 loss: -0.12013769149780273
Batch 29/64 loss: 1.3861212730407715
Batch 30/64 loss: -0.284970760345459
Batch 31/64 loss: 0.1005864143371582
Batch 32/64 loss: -0.10662126541137695
Batch 33/64 loss: -0.21289396286010742
Batch 34/64 loss: -0.33178186416625977
Batch 35/64 loss: -0.41077709197998047
Batch 36/64 loss: -0.3650398254394531
Batch 37/64 loss: -0.11943340301513672
Batch 38/64 loss: -0.2820167541503906
Batch 39/64 loss: -0.3905954360961914
Batch 40/64 loss: -0.19933795928955078
Batch 41/64 loss: -0.3579111099243164
Batch 42/64 loss: -0.4747962951660156
Batch 43/64 loss: -0.1742715835571289
Batch 44/64 loss: -0.02320241928100586
Batch 45/64 loss: -0.35109376907348633
Batch 46/64 loss: -0.23772382736206055
Batch 47/64 loss: -0.052713871002197266
Batch 48/64 loss: -0.40586328506469727
Batch 49/64 loss: -0.48746252059936523
Batch 50/64 loss: 0.8125014305114746
Batch 51/64 loss: -0.29822778701782227
Batch 52/64 loss: -0.28457212448120117
Batch 53/64 loss: 0.04403829574584961
Batch 54/64 loss: -0.44610595703125
Batch 55/64 loss: -0.3741950988769531
Batch 56/64 loss: -0.31468725204467773
Batch 57/64 loss: -0.3088679313659668
Batch 58/64 loss: -0.46812868118286133
Batch 59/64 loss: -0.4508986473083496
Batch 60/64 loss: -0.44251394271850586
Batch 61/64 loss: -0.3463411331176758
Batch 62/64 loss: 0.33907175064086914
Batch 63/64 loss: -0.3206367492675781
Batch 64/64 loss: -3.827622413635254
Epoch 162  Train loss: -0.220492774364995  Val loss: -0.685613927152968
Epoch 163
-------------------------------
Batch 1/64 loss: -0.2905616760253906
Batch 2/64 loss: -0.39913225173950195
Batch 3/64 loss: -0.3123612403869629
Batch 4/64 loss: -0.2950725555419922
Batch 5/64 loss: -0.23471879959106445
Batch 6/64 loss: -0.07738828659057617
Batch 7/64 loss: -0.4494457244873047
Batch 8/64 loss: -0.37361621856689453
Batch 9/64 loss: -0.4871072769165039
Batch 10/64 loss: -0.48108768463134766
Batch 11/64 loss: -0.4651823043823242
Batch 12/64 loss: -0.5069799423217773
Batch 13/64 loss: -0.3720512390136719
Batch 14/64 loss: -0.37062978744506836
Batch 15/64 loss: -0.5839104652404785
Batch 16/64 loss: -0.37871217727661133
Batch 17/64 loss: -0.4782414436340332
Batch 18/64 loss: 0.9088454246520996
Batch 19/64 loss: -0.27992677688598633
Batch 20/64 loss: -0.25030946731567383
Batch 21/64 loss: -0.3457221984863281
Batch 22/64 loss: -0.3140091896057129
Batch 23/64 loss: -0.1806621551513672
Batch 24/64 loss: -0.501986026763916
Batch 25/64 loss: -0.4128227233886719
Batch 26/64 loss: -0.4563112258911133
Batch 27/64 loss: -0.014688491821289062
Batch 28/64 loss: 0.9936275482177734
Batch 29/64 loss: -0.5701103210449219
Batch 30/64 loss: -0.42546701431274414
Batch 31/64 loss: -0.2852816581726074
Batch 32/64 loss: 0.49279022216796875
Batch 33/64 loss: -0.11292552947998047
Batch 34/64 loss: -0.5004053115844727
Batch 35/64 loss: -0.5649166107177734
Batch 36/64 loss: -0.3325080871582031
Batch 37/64 loss: -0.39040470123291016
Batch 38/64 loss: -0.5536632537841797
Batch 39/64 loss: -0.3234257698059082
Batch 40/64 loss: -0.31644392013549805
Batch 41/64 loss: -0.4235835075378418
Batch 42/64 loss: -0.07509136199951172
Batch 43/64 loss: -0.43937110900878906
Batch 44/64 loss: -0.14891719818115234
Batch 45/64 loss: -0.3347468376159668
Batch 46/64 loss: -0.45732736587524414
Batch 47/64 loss: -0.08791208267211914
Batch 48/64 loss: -0.3974595069885254
Batch 49/64 loss: 0.3536081314086914
Batch 50/64 loss: -0.5766725540161133
Batch 51/64 loss: -0.21985387802124023
Batch 52/64 loss: -0.5594477653503418
Batch 53/64 loss: -0.5391449928283691
Batch 54/64 loss: -0.4091672897338867
Batch 55/64 loss: -0.4145174026489258
Batch 56/64 loss: -0.2747044563293457
Batch 57/64 loss: -0.3500332832336426
Batch 58/64 loss: -0.34419965744018555
Batch 59/64 loss: -0.5012912750244141
Batch 60/64 loss: -0.5882406234741211
Batch 61/64 loss: -0.4835014343261719
Batch 62/64 loss: -0.07512950897216797
Batch 63/64 loss: -0.11170101165771484
Batch 64/64 loss: -3.797238826751709
Epoch 163  Train loss: -0.3388119435777851  Val loss: -0.7488605984297815
Saving best model, epoch: 163
Epoch 164
-------------------------------
Batch 1/64 loss: -0.4185371398925781
Batch 2/64 loss: -0.4826350212097168
Batch 3/64 loss: -0.4432992935180664
Batch 4/64 loss: -0.4876837730407715
Batch 5/64 loss: -0.29004669189453125
Batch 6/64 loss: 0.05280256271362305
Batch 7/64 loss: -0.5578441619873047
Batch 8/64 loss: -0.3220701217651367
Batch 9/64 loss: -0.5167121887207031
Batch 10/64 loss: -0.4369339942932129
Batch 11/64 loss: -0.5036602020263672
Batch 12/64 loss: -0.504054069519043
Batch 13/64 loss: -0.6362829208374023
Batch 14/64 loss: -0.5777382850646973
Batch 15/64 loss: -0.4560990333557129
Batch 16/64 loss: -0.18128156661987305
Batch 17/64 loss: -0.5110630989074707
Batch 18/64 loss: -0.457766056060791
Batch 19/64 loss: -0.45424604415893555
Batch 20/64 loss: -0.4241828918457031
Batch 21/64 loss: -0.3061838150024414
Batch 22/64 loss: 0.3658719062805176
Batch 23/64 loss: -0.46242189407348633
Batch 24/64 loss: -0.2549304962158203
Batch 25/64 loss: 0.7445178031921387
Batch 26/64 loss: -0.43998003005981445
Batch 27/64 loss: -0.49250364303588867
Batch 28/64 loss: -0.4711151123046875
Batch 29/64 loss: 0.1257624626159668
Batch 30/64 loss: -0.5734848976135254
Batch 31/64 loss: -0.48249292373657227
Batch 32/64 loss: -0.28240251541137695
Batch 33/64 loss: -0.4916219711303711
Batch 34/64 loss: -0.429166316986084
Batch 35/64 loss: -0.4720449447631836
Batch 36/64 loss: 0.0631871223449707
Batch 37/64 loss: -0.5005879402160645
Batch 38/64 loss: -0.36465883255004883
Batch 39/64 loss: -0.46107912063598633
Batch 40/64 loss: 1.1003823280334473
Batch 41/64 loss: -0.2803182601928711
Batch 42/64 loss: -0.16335391998291016
Batch 43/64 loss: -0.36442136764526367
Batch 44/64 loss: -0.4208707809448242
Batch 45/64 loss: -0.38797616958618164
Batch 46/64 loss: -0.45345067977905273
Batch 47/64 loss: -0.4180150032043457
Batch 48/64 loss: -0.26319217681884766
Batch 49/64 loss: -0.44723081588745117
Batch 50/64 loss: -0.4777545928955078
Batch 51/64 loss: -0.4758925437927246
Batch 52/64 loss: -0.40888357162475586
Batch 53/64 loss: -0.44034719467163086
Batch 54/64 loss: -0.4650750160217285
Batch 55/64 loss: -0.4912447929382324
Batch 56/64 loss: -0.3481326103210449
Batch 57/64 loss: -0.4306049346923828
Batch 58/64 loss: -0.4820842742919922
Batch 59/64 loss: -0.5778698921203613
Batch 60/64 loss: -0.5309367179870605
Batch 61/64 loss: -0.44733524322509766
Batch 62/64 loss: -0.28910398483276367
Batch 63/64 loss: -0.48909902572631836
Batch 64/64 loss: -4.076796054840088
Epoch 164  Train loss: -0.3969423911150764  Val loss: -0.7096663930571776
Epoch 165
-------------------------------
Batch 1/64 loss: -0.42613935470581055
Batch 2/64 loss: -0.48294544219970703
Batch 3/64 loss: -0.4265718460083008
Batch 4/64 loss: -0.09087038040161133
Batch 5/64 loss: -0.3684511184692383
Batch 6/64 loss: -0.31256866455078125
Batch 7/64 loss: -0.13394832611083984
Batch 8/64 loss: -0.34331464767456055
Batch 9/64 loss: -0.48595428466796875
Batch 10/64 loss: -0.48055601119995117
Batch 11/64 loss: -0.5073370933532715
Batch 12/64 loss: -0.48050737380981445
Batch 13/64 loss: -0.527122974395752
Batch 14/64 loss: -0.5948939323425293
Batch 15/64 loss: 0.17902231216430664
Batch 16/64 loss: 0.18812942504882812
Batch 17/64 loss: -0.42023515701293945
Batch 18/64 loss: -0.475919246673584
Batch 19/64 loss: -0.5227193832397461
Batch 20/64 loss: -0.5461955070495605
Batch 21/64 loss: -0.5216727256774902
Batch 22/64 loss: -0.3409609794616699
Batch 23/64 loss: -0.5927495956420898
Batch 24/64 loss: -0.3059053421020508
Batch 25/64 loss: -0.31408119201660156
Batch 26/64 loss: 0.2339153289794922
Batch 27/64 loss: -0.5012660026550293
Batch 28/64 loss: -0.4019927978515625
Batch 29/64 loss: -0.17115163803100586
Batch 30/64 loss: -0.4958953857421875
Batch 31/64 loss: -0.45822572708129883
Batch 32/64 loss: -0.5152287483215332
Batch 33/64 loss: -0.3726687431335449
Batch 34/64 loss: -0.4733400344848633
Batch 35/64 loss: 1.3064260482788086
Batch 36/64 loss: -0.4669485092163086
Batch 37/64 loss: -0.3681478500366211
Batch 38/64 loss: -0.22106504440307617
Batch 39/64 loss: -0.49399757385253906
Batch 40/64 loss: 0.8945064544677734
Batch 41/64 loss: -0.2262425422668457
Batch 42/64 loss: -0.508204460144043
Batch 43/64 loss: -0.5331010818481445
Batch 44/64 loss: -0.5622763633728027
Batch 45/64 loss: -0.3667640686035156
Batch 46/64 loss: -0.2734980583190918
Batch 47/64 loss: -0.40009260177612305
Batch 48/64 loss: -0.3688778877258301
Batch 49/64 loss: -0.2732853889465332
Batch 50/64 loss: -0.1500844955444336
Batch 51/64 loss: -0.5214390754699707
Batch 52/64 loss: -0.26811838150024414
Batch 53/64 loss: -0.39049196243286133
Batch 54/64 loss: -0.3432579040527344
Batch 55/64 loss: -0.4938936233520508
Batch 56/64 loss: -0.4451937675476074
Batch 57/64 loss: -0.5163583755493164
Batch 58/64 loss: 0.12337589263916016
Batch 59/64 loss: -0.5333690643310547
Batch 60/64 loss: -0.25863027572631836
Batch 61/64 loss: 0.8514671325683594
Batch 62/64 loss: -0.5049996376037598
Batch 63/64 loss: -0.47474098205566406
Batch 64/64 loss: -3.789623737335205
Epoch 165  Train loss: -0.3469779425976323  Val loss: -0.6255082199253987
Epoch 166
-------------------------------
Batch 1/64 loss: -0.46709775924682617
Batch 2/64 loss: -0.5756373405456543
Batch 3/64 loss: -0.4073910713195801
Batch 4/64 loss: -0.6811370849609375
Batch 5/64 loss: -0.4651603698730469
Batch 6/64 loss: -0.5819602012634277
Batch 7/64 loss: 0.657808780670166
Batch 8/64 loss: -0.5324912071228027
Batch 9/64 loss: -0.49747371673583984
Batch 10/64 loss: -0.4269542694091797
Batch 11/64 loss: 0.2596626281738281
Batch 12/64 loss: -0.5589942932128906
Batch 13/64 loss: -0.23212718963623047
Batch 14/64 loss: 0.27225637435913086
Batch 15/64 loss: -0.45058727264404297
Batch 16/64 loss: -0.25540781021118164
Batch 17/64 loss: -0.491668701171875
Batch 18/64 loss: -0.34728527069091797
Batch 19/64 loss: -0.23274755477905273
Batch 20/64 loss: -0.40792274475097656
Batch 21/64 loss: -0.1234898567199707
Batch 22/64 loss: -0.5346474647521973
Batch 23/64 loss: 1.16871976852417
Batch 24/64 loss: -0.29123401641845703
Batch 25/64 loss: -0.437929630279541
Batch 26/64 loss: -0.3408393859863281
Batch 27/64 loss: -0.37259960174560547
Batch 28/64 loss: -0.10044193267822266
Batch 29/64 loss: -0.38352346420288086
Batch 30/64 loss: -0.4073066711425781
Batch 31/64 loss: -0.1280689239501953
Batch 32/64 loss: -0.19074392318725586
Batch 33/64 loss: -0.4054999351501465
Batch 34/64 loss: -0.39142894744873047
Batch 35/64 loss: -0.2991471290588379
Batch 36/64 loss: -0.5243401527404785
Batch 37/64 loss: -0.44089698791503906
Batch 38/64 loss: -0.34424686431884766
Batch 39/64 loss: -0.4894137382507324
Batch 40/64 loss: -0.4293961524963379
Batch 41/64 loss: -0.47745466232299805
Batch 42/64 loss: -0.43721866607666016
Batch 43/64 loss: -0.38329124450683594
Batch 44/64 loss: -0.3541865348815918
Batch 45/64 loss: -0.20549440383911133
Batch 46/64 loss: -0.6181130409240723
Batch 47/64 loss: -0.6296405792236328
Batch 48/64 loss: -0.2134556770324707
Batch 49/64 loss: -0.4113335609436035
Batch 50/64 loss: -0.4868597984313965
Batch 51/64 loss: -0.4514479637145996
Batch 52/64 loss: 0.04512929916381836
Batch 53/64 loss: -0.38137054443359375
Batch 54/64 loss: -0.35428428649902344
Batch 55/64 loss: -0.4717278480529785
Batch 56/64 loss: -0.44205522537231445
Batch 57/64 loss: -0.2810344696044922
Batch 58/64 loss: -0.5361614227294922
Batch 59/64 loss: 0.8185639381408691
Batch 60/64 loss: -0.4519071578979492
Batch 61/64 loss: -0.3269467353820801
Batch 62/64 loss: -0.34888267517089844
Batch 63/64 loss: -0.15000295639038086
Batch 64/64 loss: -3.7600278854370117
Epoch 166  Train loss: -0.3491449131685145  Val loss: -0.6140186925933943
Epoch 167
-------------------------------
Batch 1/64 loss: -0.2643594741821289
Batch 2/64 loss: -0.4628915786743164
Batch 3/64 loss: -0.43404626846313477
Batch 4/64 loss: -0.4311857223510742
Batch 5/64 loss: 0.2606515884399414
Batch 6/64 loss: -0.5532383918762207
Batch 7/64 loss: -0.6213836669921875
Batch 8/64 loss: -0.41500139236450195
Batch 9/64 loss: -0.32105112075805664
Batch 10/64 loss: -0.24154233932495117
Batch 11/64 loss: 0.9136762619018555
Batch 12/64 loss: -0.44144678115844727
Batch 13/64 loss: -0.441802978515625
Batch 14/64 loss: -0.5438575744628906
Batch 15/64 loss: -0.39008617401123047
Batch 16/64 loss: -0.1358814239501953
Batch 17/64 loss: 0.2199721336364746
Batch 18/64 loss: -0.376312255859375
Batch 19/64 loss: -0.30445384979248047
Batch 20/64 loss: -0.041025638580322266
Batch 21/64 loss: -0.0955052375793457
Batch 22/64 loss: -0.5907387733459473
Batch 23/64 loss: -0.4191451072692871
Batch 24/64 loss: -0.46728992462158203
Batch 25/64 loss: -0.40129947662353516
Batch 26/64 loss: -0.48800039291381836
Batch 27/64 loss: -0.5843319892883301
Batch 28/64 loss: -0.2252025604248047
Batch 29/64 loss: -0.5698299407958984
Batch 30/64 loss: 0.5842461585998535
Batch 31/64 loss: -0.4526095390319824
Batch 32/64 loss: -0.3054838180541992
Batch 33/64 loss: -0.596461296081543
Batch 34/64 loss: 1.809885025024414
Batch 35/64 loss: -0.5038285255432129
Batch 36/64 loss: -0.16881608963012695
Batch 37/64 loss: -0.3266282081604004
Batch 38/64 loss: -0.4453768730163574
Batch 39/64 loss: -0.41716814041137695
Batch 40/64 loss: -0.24109697341918945
Batch 41/64 loss: -0.5675559043884277
Batch 42/64 loss: -0.4570603370666504
Batch 43/64 loss: -0.35724544525146484
Batch 44/64 loss: -0.5997090339660645
Batch 45/64 loss: -0.5527615547180176
Batch 46/64 loss: -0.42665815353393555
Batch 47/64 loss: -0.4967217445373535
Batch 48/64 loss: -0.4569416046142578
Batch 49/64 loss: -0.2191600799560547
Batch 50/64 loss: -0.5242514610290527
Batch 51/64 loss: -0.5267624855041504
Batch 52/64 loss: -0.5121707916259766
Batch 53/64 loss: -0.4710874557495117
Batch 54/64 loss: -0.5472846031188965
Batch 55/64 loss: -0.5001702308654785
Batch 56/64 loss: -0.5066218376159668
Batch 57/64 loss: -0.5343203544616699
Batch 58/64 loss: -0.5980534553527832
Batch 59/64 loss: -0.5075163841247559
Batch 60/64 loss: -0.21506357192993164
Batch 61/64 loss: -0.28761911392211914
Batch 62/64 loss: -0.2755699157714844
Batch 63/64 loss: -0.7053790092468262
Batch 64/64 loss: -4.00866174697876
Epoch 167  Train loss: -0.3730530065648696  Val loss: -0.794139783406995
Saving best model, epoch: 167
Epoch 168
-------------------------------
Batch 1/64 loss: -0.5094480514526367
Batch 2/64 loss: 0.03549480438232422
Batch 3/64 loss: -0.6089382171630859
Batch 4/64 loss: -0.672389030456543
Batch 5/64 loss: 0.5417356491088867
Batch 6/64 loss: -0.5870957374572754
Batch 7/64 loss: -0.6237320899963379
Batch 8/64 loss: -0.48626184463500977
Batch 9/64 loss: -0.6391897201538086
Batch 10/64 loss: -0.42587852478027344
Batch 11/64 loss: -0.46578264236450195
Batch 12/64 loss: -0.2731647491455078
Batch 13/64 loss: -0.49591875076293945
Batch 14/64 loss: 0.15460443496704102
Batch 15/64 loss: -0.567145824432373
Batch 16/64 loss: -0.2392573356628418
Batch 17/64 loss: -0.4307875633239746
Batch 18/64 loss: 0.4726448059082031
Batch 19/64 loss: -0.6232113838195801
Batch 20/64 loss: -0.556647777557373
Batch 21/64 loss: -0.23002910614013672
Batch 22/64 loss: -0.35062551498413086
Batch 23/64 loss: -0.44140052795410156
Batch 24/64 loss: -0.6325654983520508
Batch 25/64 loss: -0.6762232780456543
Batch 26/64 loss: -0.647423267364502
Batch 27/64 loss: -0.5534248352050781
Batch 28/64 loss: -0.5138368606567383
Batch 29/64 loss: -0.6068310737609863
Batch 30/64 loss: -0.6506524085998535
Batch 31/64 loss: -0.6405458450317383
Batch 32/64 loss: -0.28888702392578125
Batch 33/64 loss: -0.44232606887817383
Batch 34/64 loss: -0.5273666381835938
Batch 35/64 loss: -0.6098308563232422
Batch 36/64 loss: -0.5107693672180176
Batch 37/64 loss: -0.6457395553588867
Batch 38/64 loss: -0.3630218505859375
Batch 39/64 loss: -0.5220165252685547
Batch 40/64 loss: -0.5423250198364258
Batch 41/64 loss: -0.14125299453735352
Batch 42/64 loss: -0.635831356048584
Batch 43/64 loss: -0.3645901679992676
Batch 44/64 loss: -0.13652610778808594
Batch 45/64 loss: -0.42123842239379883
Batch 46/64 loss: -0.5519452095031738
Batch 47/64 loss: -0.1818852424621582
Batch 48/64 loss: -0.5777397155761719
Batch 49/64 loss: -0.47550010681152344
Batch 50/64 loss: -0.4042391777038574
Batch 51/64 loss: -0.5231990814208984
Batch 52/64 loss: -0.5113639831542969
Batch 53/64 loss: -0.591362476348877
Batch 54/64 loss: -0.48301076889038086
Batch 55/64 loss: -0.48554325103759766
Batch 56/64 loss: -0.5819234848022461
Batch 57/64 loss: -0.5062723159790039
Batch 58/64 loss: -0.48950862884521484
Batch 59/64 loss: -0.4936237335205078
Batch 60/64 loss: -0.4924612045288086
Batch 61/64 loss: 0.991452693939209
Batch 62/64 loss: -0.5990314483642578
Batch 63/64 loss: -0.5467243194580078
Batch 64/64 loss: -4.068742752075195
Epoch 168  Train loss: -0.46511510961196  Val loss: -0.8069651757728603
Saving best model, epoch: 168
Epoch 169
-------------------------------
Batch 1/64 loss: -0.5388689041137695
Batch 2/64 loss: -0.6023521423339844
Batch 3/64 loss: -0.5953636169433594
Batch 4/64 loss: -0.6282110214233398
Batch 5/64 loss: -0.46660423278808594
Batch 6/64 loss: -0.49131298065185547
Batch 7/64 loss: -0.41218137741088867
Batch 8/64 loss: -0.659085750579834
Batch 9/64 loss: -0.6200237274169922
Batch 10/64 loss: -0.017742156982421875
Batch 11/64 loss: -0.5548653602600098
Batch 12/64 loss: -0.6413359642028809
Batch 13/64 loss: -0.59796142578125
Batch 14/64 loss: -0.626612663269043
Batch 15/64 loss: -0.4801168441772461
Batch 16/64 loss: -0.23477745056152344
Batch 17/64 loss: -0.4697890281677246
Batch 18/64 loss: -0.5292940139770508
Batch 19/64 loss: -0.4434471130371094
Batch 20/64 loss: -0.10177373886108398
Batch 21/64 loss: -0.5438418388366699
Batch 22/64 loss: -0.6404390335083008
Batch 23/64 loss: -0.5543203353881836
Batch 24/64 loss: -0.524177074432373
Batch 25/64 loss: -0.5344266891479492
Batch 26/64 loss: -0.545900821685791
Batch 27/64 loss: -0.32993173599243164
Batch 28/64 loss: -0.5421428680419922
Batch 29/64 loss: -0.5809922218322754
Batch 30/64 loss: -0.4397249221801758
Batch 31/64 loss: -0.294802188873291
Batch 32/64 loss: -0.5642566680908203
Batch 33/64 loss: -0.38970184326171875
Batch 34/64 loss: -0.48795223236083984
Batch 35/64 loss: -0.478909969329834
Batch 36/64 loss: -0.3932919502258301
Batch 37/64 loss: -0.5051751136779785
Batch 38/64 loss: -0.5060992240905762
Batch 39/64 loss: -0.40443849563598633
Batch 40/64 loss: -0.3734769821166992
Batch 41/64 loss: -0.46296167373657227
Batch 42/64 loss: 1.0487337112426758
Batch 43/64 loss: 0.3034062385559082
Batch 44/64 loss: 0.6113662719726562
Batch 45/64 loss: -0.26015615463256836
Batch 46/64 loss: -0.5425982475280762
Batch 47/64 loss: -0.11632251739501953
Batch 48/64 loss: -0.4650754928588867
Batch 49/64 loss: -0.6409149169921875
Batch 50/64 loss: 0.44875288009643555
Batch 51/64 loss: -0.4357762336730957
Batch 52/64 loss: -0.0026273727416992188
Batch 53/64 loss: -0.5777559280395508
Batch 54/64 loss: -0.18988943099975586
Batch 55/64 loss: -0.3060898780822754
Batch 56/64 loss: -0.4018378257751465
Batch 57/64 loss: -0.21630334854125977
Batch 58/64 loss: -0.5915470123291016
Batch 59/64 loss: -0.538358211517334
Batch 60/64 loss: 0.2996077537536621
Batch 61/64 loss: -0.6992835998535156
Batch 62/64 loss: -0.3736238479614258
Batch 63/64 loss: -0.4259343147277832
Batch 64/64 loss: -4.106456756591797
Epoch 169  Train loss: -0.4229137794644225  Val loss: -0.7898646482487315
Epoch 170
-------------------------------
Batch 1/64 loss: -0.3841066360473633
Batch 2/64 loss: -0.5961723327636719
Batch 3/64 loss: -0.5025901794433594
Batch 4/64 loss: -0.49892377853393555
Batch 5/64 loss: -0.46472692489624023
Batch 6/64 loss: -0.42356252670288086
Batch 7/64 loss: 0.18251991271972656
Batch 8/64 loss: -0.14803504943847656
Batch 9/64 loss: -0.2127094268798828
Batch 10/64 loss: -0.3170342445373535
Batch 11/64 loss: -0.48260498046875
Batch 12/64 loss: -0.3741641044616699
Batch 13/64 loss: -0.5031290054321289
Batch 14/64 loss: -0.4627084732055664
Batch 15/64 loss: -0.5473732948303223
Batch 16/64 loss: -0.4981570243835449
Batch 17/64 loss: -0.5980792045593262
Batch 18/64 loss: -0.5798978805541992
Batch 19/64 loss: -0.45585203170776367
Batch 20/64 loss: -0.6036696434020996
Batch 21/64 loss: -0.5381841659545898
Batch 22/64 loss: -0.4953742027282715
Batch 23/64 loss: -0.5550937652587891
Batch 24/64 loss: -0.6652522087097168
Batch 25/64 loss: -0.5522446632385254
Batch 26/64 loss: -0.6135072708129883
Batch 27/64 loss: -0.5056376457214355
Batch 28/64 loss: -0.4890866279602051
Batch 29/64 loss: -0.3875598907470703
Batch 30/64 loss: 1.0849757194519043
Batch 31/64 loss: -0.4576730728149414
Batch 32/64 loss: -0.42194032669067383
Batch 33/64 loss: -0.5575551986694336
Batch 34/64 loss: -0.5110697746276855
Batch 35/64 loss: -0.4893627166748047
Batch 36/64 loss: -0.5445833206176758
Batch 37/64 loss: -0.6072168350219727
Batch 38/64 loss: -0.46277666091918945
Batch 39/64 loss: -0.6167216300964355
Batch 40/64 loss: -0.5609760284423828
Batch 41/64 loss: -0.6023592948913574
Batch 42/64 loss: -0.6347236633300781
Batch 43/64 loss: 0.7548866271972656
Batch 44/64 loss: -0.3721737861633301
Batch 45/64 loss: -0.503018856048584
Batch 46/64 loss: -0.5466957092285156
Batch 47/64 loss: -0.5368003845214844
Batch 48/64 loss: -0.5917472839355469
Batch 49/64 loss: 0.13827943801879883
Batch 50/64 loss: -0.6816277503967285
Batch 51/64 loss: -0.4762396812438965
Batch 52/64 loss: -0.4899578094482422
Batch 53/64 loss: -0.6528840065002441
Batch 54/64 loss: -0.7078428268432617
Batch 55/64 loss: -0.5329227447509766
Batch 56/64 loss: -0.37564563751220703
Batch 57/64 loss: -0.7871198654174805
Batch 58/64 loss: 0.7365899085998535
Batch 59/64 loss: -0.06304359436035156
Batch 60/64 loss: -0.49619102478027344
Batch 61/64 loss: -0.41782140731811523
Batch 62/64 loss: -0.5487399101257324
Batch 63/64 loss: -0.42948436737060547
Batch 64/64 loss: -4.125333309173584
Epoch 170  Train loss: -0.46006432514564666  Val loss: -0.8316363567339186
Saving best model, epoch: 170
Epoch 171
-------------------------------
Batch 1/64 loss: -0.4912147521972656
Batch 2/64 loss: -0.6610980033874512
Batch 3/64 loss: -0.6188392639160156
Batch 4/64 loss: -0.5974445343017578
Batch 5/64 loss: -0.5600123405456543
Batch 6/64 loss: -0.5386204719543457
Batch 7/64 loss: -0.2328028678894043
Batch 8/64 loss: -0.12415695190429688
Batch 9/64 loss: -0.3659210205078125
Batch 10/64 loss: 0.4105410575866699
Batch 11/64 loss: -0.5776734352111816
Batch 12/64 loss: -0.5173077583312988
Batch 13/64 loss: -0.5247235298156738
Batch 14/64 loss: -0.6376819610595703
Batch 15/64 loss: -0.6772117614746094
Batch 16/64 loss: -0.6003999710083008
Batch 17/64 loss: -0.10905694961547852
Batch 18/64 loss: -0.5247607231140137
Batch 19/64 loss: -0.5009140968322754
Batch 20/64 loss: -0.6220712661743164
Batch 21/64 loss: -0.5501952171325684
Batch 22/64 loss: -0.6284646987915039
Batch 23/64 loss: -0.50006103515625
Batch 24/64 loss: -0.6551222801208496
Batch 25/64 loss: -0.4923419952392578
Batch 26/64 loss: -0.6001381874084473
Batch 27/64 loss: -0.7142624855041504
Batch 28/64 loss: -0.4088277816772461
Batch 29/64 loss: -0.45667314529418945
Batch 30/64 loss: -0.5490427017211914
Batch 31/64 loss: -0.6767005920410156
Batch 32/64 loss: -0.201995849609375
Batch 33/64 loss: -0.4200739860534668
Batch 34/64 loss: -0.5676007270812988
Batch 35/64 loss: -0.5292239189147949
Batch 36/64 loss: -0.5165228843688965
Batch 37/64 loss: -0.2987680435180664
Batch 38/64 loss: -0.4559941291809082
Batch 39/64 loss: -0.40070104598999023
Batch 40/64 loss: -0.35336780548095703
Batch 41/64 loss: -0.47721385955810547
Batch 42/64 loss: -0.4174485206604004
Batch 43/64 loss: 0.006324291229248047
Batch 44/64 loss: -0.3745150566101074
Batch 45/64 loss: -0.37372922897338867
Batch 46/64 loss: -0.2950286865234375
Batch 47/64 loss: -0.47730016708374023
Batch 48/64 loss: -0.4833827018737793
Batch 49/64 loss: -0.27236080169677734
Batch 50/64 loss: -0.4464859962463379
Batch 51/64 loss: -0.5080008506774902
Batch 52/64 loss: 0.7710990905761719
Batch 53/64 loss: -0.4704155921936035
Batch 54/64 loss: -0.10566234588623047
Batch 55/64 loss: -0.2533426284790039
Batch 56/64 loss: 0.019176959991455078
Batch 57/64 loss: -0.40889596939086914
Batch 58/64 loss: -0.38933706283569336
Batch 59/64 loss: -0.0878305435180664
Batch 60/64 loss: -0.4270596504211426
Batch 61/64 loss: -0.4931154251098633
Batch 62/64 loss: -0.25832033157348633
Batch 63/64 loss: -0.44297027587890625
Batch 64/64 loss: -2.532217025756836
Epoch 171  Train loss: -0.43313611348470055  Val loss: -0.626615032707293
Epoch 172
-------------------------------
Batch 1/64 loss: -0.5369648933410645
Batch 2/64 loss: -0.42085886001586914
Batch 3/64 loss: -0.4360084533691406
Batch 4/64 loss: -0.363187313079834
Batch 5/64 loss: -0.32891225814819336
Batch 6/64 loss: -0.3780245780944824
Batch 7/64 loss: -0.4398670196533203
Batch 8/64 loss: -0.5610461235046387
Batch 9/64 loss: -0.4609489440917969
Batch 10/64 loss: 0.5252265930175781
Batch 11/64 loss: -0.3736739158630371
Batch 12/64 loss: -0.3670382499694824
Batch 13/64 loss: -0.20460271835327148
Batch 14/64 loss: -0.48312902450561523
Batch 15/64 loss: -0.6556034088134766
Batch 16/64 loss: -0.3260197639465332
Batch 17/64 loss: -0.24216794967651367
Batch 18/64 loss: 0.03017139434814453
Batch 19/64 loss: 0.08272933959960938
Batch 20/64 loss: -0.4735574722290039
Batch 21/64 loss: -0.5365233421325684
Batch 22/64 loss: -0.44012928009033203
Batch 23/64 loss: -0.5272989273071289
Batch 24/64 loss: -0.5295753479003906
Batch 25/64 loss: -0.44582223892211914
Batch 26/64 loss: -0.6063523292541504
Batch 27/64 loss: -0.39278173446655273
Batch 28/64 loss: -0.5522770881652832
Batch 29/64 loss: -0.6173930168151855
Batch 30/64 loss: -0.35433006286621094
Batch 31/64 loss: -0.5139360427856445
Batch 32/64 loss: -0.22360658645629883
Batch 33/64 loss: -0.45075273513793945
Batch 34/64 loss: -0.608311653137207
Batch 35/64 loss: -0.3356947898864746
Batch 36/64 loss: -0.6492958068847656
Batch 37/64 loss: 0.6937170028686523
Batch 38/64 loss: 1.149057388305664
Batch 39/64 loss: -0.512993335723877
Batch 40/64 loss: -0.41954803466796875
Batch 41/64 loss: -0.5197892189025879
Batch 42/64 loss: -0.24365901947021484
Batch 43/64 loss: -0.30283403396606445
Batch 44/64 loss: 0.6044125556945801
Batch 45/64 loss: 0.1209573745727539
Batch 46/64 loss: -0.3889045715332031
Batch 47/64 loss: -0.4347071647644043
Batch 48/64 loss: -0.5628314018249512
Batch 49/64 loss: -0.5504355430603027
Batch 50/64 loss: -0.5192456245422363
Batch 51/64 loss: -0.5444436073303223
Batch 52/64 loss: -0.35770463943481445
Batch 53/64 loss: -0.44632816314697266
Batch 54/64 loss: -0.6486163139343262
Batch 55/64 loss: -0.27924299240112305
Batch 56/64 loss: -0.45739078521728516
Batch 57/64 loss: -0.25733041763305664
Batch 58/64 loss: -0.606529712677002
Batch 59/64 loss: -0.4107346534729004
Batch 60/64 loss: -0.5367803573608398
Batch 61/64 loss: -0.6383056640625
Batch 62/64 loss: -0.405611515045166
Batch 63/64 loss: -0.4160299301147461
Batch 64/64 loss: -4.226767539978027
Epoch 172  Train loss: -0.3962273354623832  Val loss: -0.8000783887515772
Epoch 173
-------------------------------
Batch 1/64 loss: -0.49625539779663086
Batch 2/64 loss: -0.5113439559936523
Batch 3/64 loss: -0.5300183296203613
Batch 4/64 loss: -0.6287856101989746
Batch 5/64 loss: -0.6773271560668945
Batch 6/64 loss: -0.3435506820678711
Batch 7/64 loss: -0.4527444839477539
Batch 8/64 loss: 0.09010839462280273
Batch 9/64 loss: 0.79119873046875
Batch 10/64 loss: -0.5959210395812988
Batch 11/64 loss: 0.3318610191345215
Batch 12/64 loss: -0.4547848701477051
Batch 13/64 loss: 1.0070257186889648
Batch 14/64 loss: -0.4108405113220215
Batch 15/64 loss: -0.4984583854675293
Batch 16/64 loss: -0.4356536865234375
Batch 17/64 loss: -0.37221431732177734
Batch 18/64 loss: -0.6242952346801758
Batch 19/64 loss: -0.5163531303405762
Batch 20/64 loss: -0.5766477584838867
Batch 21/64 loss: -0.5815763473510742
Batch 22/64 loss: -0.5073294639587402
Batch 23/64 loss: -0.3129115104675293
Batch 24/64 loss: -0.48416757583618164
Batch 25/64 loss: -0.047621726989746094
Batch 26/64 loss: -0.46294593811035156
Batch 27/64 loss: -0.5642518997192383
Batch 28/64 loss: -0.37271928787231445
Batch 29/64 loss: -0.5168924331665039
Batch 30/64 loss: -0.48079490661621094
Batch 31/64 loss: -0.5949773788452148
Batch 32/64 loss: -0.3835945129394531
Batch 33/64 loss: -0.416013240814209
Batch 34/64 loss: -0.3874068260192871
Batch 35/64 loss: -0.4959073066711426
Batch 36/64 loss: -0.5088605880737305
Batch 37/64 loss: -0.48407649993896484
Batch 38/64 loss: -0.4173769950866699
Batch 39/64 loss: -0.5541315078735352
Batch 40/64 loss: -0.5111708641052246
Batch 41/64 loss: -0.4287419319152832
Batch 42/64 loss: -0.4939608573913574
Batch 43/64 loss: -0.6094002723693848
Batch 44/64 loss: 1.035111904144287
Batch 45/64 loss: -0.5646119117736816
Batch 46/64 loss: -0.5639967918395996
Batch 47/64 loss: -0.548485279083252
Batch 48/64 loss: -0.6679902076721191
Batch 49/64 loss: -0.4412550926208496
Batch 50/64 loss: -0.6896195411682129
Batch 51/64 loss: -0.5385355949401855
Batch 52/64 loss: -0.05911445617675781
Batch 53/64 loss: -0.5669231414794922
Batch 54/64 loss: -0.3997220993041992
Batch 55/64 loss: -0.4879169464111328
Batch 56/64 loss: -0.48813819885253906
Batch 57/64 loss: -0.30216121673583984
Batch 58/64 loss: -0.5565943717956543
Batch 59/64 loss: -0.5245962142944336
Batch 60/64 loss: -0.40682315826416016
Batch 61/64 loss: -0.5309352874755859
Batch 62/64 loss: -0.6370878219604492
Batch 63/64 loss: -0.5956172943115234
Batch 64/64 loss: -4.117192268371582
Epoch 173  Train loss: -0.44148607815013213  Val loss: -0.810669194382081
Epoch 174
-------------------------------
Batch 1/64 loss: -0.2520632743835449
Batch 2/64 loss: -0.6108827590942383
Batch 3/64 loss: 0.4073052406311035
Batch 4/64 loss: -0.2679281234741211
Batch 5/64 loss: -0.6173205375671387
Batch 6/64 loss: -0.5046877861022949
Batch 7/64 loss: -0.40845489501953125
Batch 8/64 loss: -0.5701460838317871
Batch 9/64 loss: -0.4891514778137207
Batch 10/64 loss: -0.5892782211303711
Batch 11/64 loss: -0.15352630615234375
Batch 12/64 loss: -0.5331072807312012
Batch 13/64 loss: -0.6151275634765625
Batch 14/64 loss: -0.44750261306762695
Batch 15/64 loss: 0.8723964691162109
Batch 16/64 loss: -0.6623821258544922
Batch 17/64 loss: -0.5636587142944336
Batch 18/64 loss: -0.48705053329467773
Batch 19/64 loss: -0.5057234764099121
Batch 20/64 loss: -0.3685569763183594
Batch 21/64 loss: -0.45125675201416016
Batch 22/64 loss: -0.5686979293823242
Batch 23/64 loss: -0.4675278663635254
Batch 24/64 loss: -0.5359025001525879
Batch 25/64 loss: -0.3351454734802246
Batch 26/64 loss: -0.5027709007263184
Batch 27/64 loss: -0.4991297721862793
Batch 28/64 loss: -0.6295313835144043
Batch 29/64 loss: -0.5698966979980469
Batch 30/64 loss: -0.3911018371582031
Batch 31/64 loss: -0.5483121871948242
Batch 32/64 loss: -0.4104480743408203
Batch 33/64 loss: -0.2608661651611328
Batch 34/64 loss: -0.5152506828308105
Batch 35/64 loss: -0.413057804107666
Batch 36/64 loss: -0.3561892509460449
Batch 37/64 loss: 1.5871224403381348
Batch 38/64 loss: -0.4396829605102539
Batch 39/64 loss: -0.5944628715515137
Batch 40/64 loss: -0.512392520904541
Batch 41/64 loss: -0.5101690292358398
Batch 42/64 loss: 0.05233430862426758
Batch 43/64 loss: -0.46868038177490234
Batch 44/64 loss: -0.4499983787536621
Batch 45/64 loss: -0.5863714218139648
Batch 46/64 loss: -0.6558055877685547
Batch 47/64 loss: 0.07931995391845703
Batch 48/64 loss: -0.40084409713745117
Batch 49/64 loss: -0.591090202331543
Batch 50/64 loss: -0.5459208488464355
Batch 51/64 loss: -0.5403294563293457
Batch 52/64 loss: -0.6077680587768555
Batch 53/64 loss: -0.29442644119262695
Batch 54/64 loss: -0.3407883644104004
Batch 55/64 loss: -0.4985532760620117
Batch 56/64 loss: -0.48788928985595703
Batch 57/64 loss: -0.6415328979492188
Batch 58/64 loss: -0.5219650268554688
Batch 59/64 loss: -0.5565352439880371
Batch 60/64 loss: -0.5310516357421875
Batch 61/64 loss: -0.4765148162841797
Batch 62/64 loss: -0.6220769882202148
Batch 63/64 loss: -0.4808492660522461
Batch 64/64 loss: -4.0745439529418945
Epoch 174  Train loss: -0.4472903943529316  Val loss: -0.6902528743153995
Epoch 175
-------------------------------
Batch 1/64 loss: -0.5682902336120605
Batch 2/64 loss: -0.5580544471740723
Batch 3/64 loss: -0.5362582206726074
Batch 4/64 loss: -0.5479884147644043
Batch 5/64 loss: -0.4750251770019531
Batch 6/64 loss: -0.5978927612304688
Batch 7/64 loss: -0.33643484115600586
Batch 8/64 loss: -0.2799191474914551
Batch 9/64 loss: -0.5397553443908691
Batch 10/64 loss: -0.3714008331298828
Batch 11/64 loss: -0.38210248947143555
Batch 12/64 loss: -0.3787531852722168
Batch 13/64 loss: -0.5517034530639648
Batch 14/64 loss: -0.5017657279968262
Batch 15/64 loss: -0.5598516464233398
Batch 16/64 loss: -0.5288662910461426
Batch 17/64 loss: -0.5447258949279785
Batch 18/64 loss: -0.52349853515625
Batch 19/64 loss: -0.5537152290344238
Batch 20/64 loss: -0.5548825263977051
Batch 21/64 loss: 0.7061285972595215
Batch 22/64 loss: 0.05874824523925781
Batch 23/64 loss: -0.4730091094970703
Batch 24/64 loss: -0.6442937850952148
Batch 25/64 loss: -0.41798973083496094
Batch 26/64 loss: -0.5233116149902344
Batch 27/64 loss: -0.42736196517944336
Batch 28/64 loss: -0.604703426361084
Batch 29/64 loss: -0.47037744522094727
Batch 30/64 loss: -0.24881935119628906
Batch 31/64 loss: -0.6336617469787598
Batch 32/64 loss: -0.6384353637695312
Batch 33/64 loss: -0.36769866943359375
Batch 34/64 loss: -0.2221384048461914
Batch 35/64 loss: -0.5439376831054688
Batch 36/64 loss: -0.5264005661010742
Batch 37/64 loss: -0.6201157569885254
Batch 38/64 loss: -0.5680537223815918
Batch 39/64 loss: 0.06425619125366211
Batch 40/64 loss: -0.7105536460876465
Batch 41/64 loss: -0.542332649230957
Batch 42/64 loss: -0.3045783042907715
Batch 43/64 loss: -0.5606808662414551
Batch 44/64 loss: -0.5064535140991211
Batch 45/64 loss: -0.6197667121887207
Batch 46/64 loss: -0.5804433822631836
Batch 47/64 loss: -0.6791272163391113
Batch 48/64 loss: -0.42351293563842773
Batch 49/64 loss: -0.5086884498596191
Batch 50/64 loss: -0.3909764289855957
Batch 51/64 loss: -0.44680309295654297
Batch 52/64 loss: -0.1145467758178711
Batch 53/64 loss: -0.3424954414367676
Batch 54/64 loss: 0.4768533706665039
Batch 55/64 loss: -0.6538400650024414
Batch 56/64 loss: -0.47878456115722656
Batch 57/64 loss: -0.123626708984375
Batch 58/64 loss: -0.42118120193481445
Batch 59/64 loss: -0.4513731002807617
Batch 60/64 loss: 1.022477626800537
Batch 61/64 loss: 0.005893707275390625
Batch 62/64 loss: -0.4829435348510742
Batch 63/64 loss: -0.5773062705993652
Batch 64/64 loss: -3.9758615493774414
Epoch 175  Train loss: -0.4453136631086761  Val loss: -0.6862672497726388
Epoch 176
-------------------------------
Batch 1/64 loss: -0.5495572090148926
Batch 2/64 loss: -0.4180307388305664
Batch 3/64 loss: -0.3764786720275879
Batch 4/64 loss: -0.4973154067993164
Batch 5/64 loss: -0.5533933639526367
Batch 6/64 loss: -0.5438518524169922
Batch 7/64 loss: -0.15083646774291992
Batch 8/64 loss: -0.1859278678894043
Batch 9/64 loss: 0.03695535659790039
Batch 10/64 loss: -0.4634552001953125
Batch 11/64 loss: -0.4792633056640625
Batch 12/64 loss: -0.27746057510375977
Batch 13/64 loss: -0.41999387741088867
Batch 14/64 loss: -0.39702510833740234
Batch 15/64 loss: -0.5021390914916992
Batch 16/64 loss: -0.41594409942626953
Batch 17/64 loss: -0.42751312255859375
Batch 18/64 loss: -0.3359847068786621
Batch 19/64 loss: -0.4064326286315918
Batch 20/64 loss: -0.22367525100708008
Batch 21/64 loss: -0.38388776779174805
Batch 22/64 loss: -0.34615135192871094
Batch 23/64 loss: 0.031043529510498047
Batch 24/64 loss: -0.5114459991455078
Batch 25/64 loss: -0.3497138023376465
Batch 26/64 loss: -0.4882087707519531
Batch 27/64 loss: 0.034670352935791016
Batch 28/64 loss: 0.8420982360839844
Batch 29/64 loss: -0.47042322158813477
Batch 30/64 loss: 0.6559090614318848
Batch 31/64 loss: -0.4477357864379883
Batch 32/64 loss: 0.18582391738891602
Batch 33/64 loss: 0.5958538055419922
Batch 34/64 loss: 0.5360827445983887
Batch 35/64 loss: -0.02279949188232422
Batch 36/64 loss: 0.8377747535705566
Batch 37/64 loss: 1.2502055168151855
Batch 38/64 loss: -0.03800010681152344
Batch 39/64 loss: 0.16039371490478516
Batch 40/64 loss: 0.20193195343017578
Batch 41/64 loss: 0.23569202423095703
Batch 42/64 loss: 0.3500995635986328
Batch 43/64 loss: 0.47093868255615234
Batch 44/64 loss: -0.08710336685180664
Batch 45/64 loss: 0.09505891799926758
Batch 46/64 loss: 0.1623072624206543
Batch 47/64 loss: -0.043682098388671875
Batch 48/64 loss: 0.17946910858154297
Batch 49/64 loss: -0.1121821403503418
Batch 50/64 loss: 0.3842945098876953
Batch 51/64 loss: -0.09654426574707031
Batch 52/64 loss: 0.08498716354370117
Batch 53/64 loss: 0.05525970458984375
Batch 54/64 loss: -0.22448348999023438
Batch 55/64 loss: -0.16646242141723633
Batch 56/64 loss: -0.037229061126708984
Batch 57/64 loss: -0.25833940505981445
Batch 58/64 loss: -0.2849459648132324
Batch 59/64 loss: -0.2420496940612793
Batch 60/64 loss: 0.18075132369995117
Batch 61/64 loss: -0.03666543960571289
Batch 62/64 loss: -0.15282726287841797
Batch 63/64 loss: -0.27318239212036133
Batch 64/64 loss: -3.1562256813049316
Epoch 176  Train loss: -0.11761427299649108  Val loss: -0.2555349159896169
Epoch 177
-------------------------------
Batch 1/64 loss: -0.12325572967529297
Batch 2/64 loss: -0.39217281341552734
Batch 3/64 loss: -0.14337444305419922
Batch 4/64 loss: -0.3386974334716797
Batch 5/64 loss: -0.11586427688598633
Batch 6/64 loss: -0.1912670135498047
Batch 7/64 loss: -0.17920637130737305
Batch 8/64 loss: -0.17214441299438477
Batch 9/64 loss: -0.15086746215820312
Batch 10/64 loss: -0.37821531295776367
Batch 11/64 loss: -0.3470487594604492
Batch 12/64 loss: -0.4095306396484375
Batch 13/64 loss: -0.34790945053100586
Batch 14/64 loss: -0.3919181823730469
Batch 15/64 loss: 0.2689194679260254
Batch 16/64 loss: -0.23152542114257812
Batch 17/64 loss: -0.13560199737548828
Batch 18/64 loss: -0.24843645095825195
Batch 19/64 loss: -0.3203721046447754
Batch 20/64 loss: -0.0041713714599609375
Batch 21/64 loss: -0.2373485565185547
Batch 22/64 loss: -0.4965782165527344
Batch 23/64 loss: -0.2856273651123047
Batch 24/64 loss: -0.2762727737426758
Batch 25/64 loss: 1.0434589385986328
Batch 26/64 loss: -0.23318004608154297
Batch 27/64 loss: -0.18497419357299805
Batch 28/64 loss: -0.2255101203918457
Batch 29/64 loss: 1.5598907470703125
Batch 30/64 loss: -0.5084967613220215
Batch 31/64 loss: -0.13889598846435547
Batch 32/64 loss: 1.1451640129089355
Batch 33/64 loss: -0.27437400817871094
Batch 34/64 loss: -0.23179149627685547
Batch 35/64 loss: -0.35595273971557617
Batch 36/64 loss: -0.34175872802734375
Batch 37/64 loss: -0.07746362686157227
Batch 38/64 loss: -0.33864545822143555
Batch 39/64 loss: -0.2711820602416992
Batch 40/64 loss: -0.5269927978515625
Batch 41/64 loss: -0.5343494415283203
Batch 42/64 loss: 0.3108067512512207
Batch 43/64 loss: -0.24696922302246094
Batch 44/64 loss: -0.44655942916870117
Batch 45/64 loss: -0.4087948799133301
Batch 46/64 loss: -0.42088747024536133
Batch 47/64 loss: -0.11884212493896484
Batch 48/64 loss: -0.33754396438598633
Batch 49/64 loss: -0.4584016799926758
Batch 50/64 loss: -0.5408511161804199
Batch 51/64 loss: -0.5264744758605957
Batch 52/64 loss: -0.2712235450744629
Batch 53/64 loss: -0.37209510803222656
Batch 54/64 loss: -0.21034526824951172
Batch 55/64 loss: -0.11228513717651367
Batch 56/64 loss: -0.36794567108154297
Batch 57/64 loss: -0.2188124656677246
Batch 58/64 loss: -0.21893787384033203
Batch 59/64 loss: -0.23189115524291992
Batch 60/64 loss: -0.37215662002563477
Batch 61/64 loss: -0.022072792053222656
Batch 62/64 loss: -0.3622727394104004
Batch 63/64 loss: -0.25904226303100586
Batch 64/64 loss: -4.035751819610596
Epoch 177  Train loss: -0.24131690941604914  Val loss: -0.5665812410439822
Epoch 178
-------------------------------
Batch 1/64 loss: -0.28524351119995117
Batch 2/64 loss: -0.31984996795654297
Batch 3/64 loss: -0.21500062942504883
Batch 4/64 loss: 0.3346090316772461
Batch 5/64 loss: 1.298140048980713
Batch 6/64 loss: -0.38225841522216797
Batch 7/64 loss: -0.40195560455322266
Batch 8/64 loss: 0.6136116981506348
Batch 9/64 loss: -0.26926136016845703
Batch 10/64 loss: -0.3846011161804199
Batch 11/64 loss: 0.2146892547607422
Batch 12/64 loss: -0.33670806884765625
Batch 13/64 loss: -0.3531622886657715
Batch 14/64 loss: -0.42546939849853516
Batch 15/64 loss: -0.47092628479003906
Batch 16/64 loss: -0.12184333801269531
Batch 17/64 loss: 0.8499584197998047
Batch 18/64 loss: -0.3778409957885742
Batch 19/64 loss: -0.32656383514404297
Batch 20/64 loss: -0.38037538528442383
Batch 21/64 loss: 0.29776620864868164
Batch 22/64 loss: -0.4494810104370117
Batch 23/64 loss: -0.3157005310058594
Batch 24/64 loss: -0.42597198486328125
Batch 25/64 loss: -0.3745460510253906
Batch 26/64 loss: -0.44361019134521484
Batch 27/64 loss: -0.5187578201293945
Batch 28/64 loss: 0.16240501403808594
Batch 29/64 loss: -0.3802146911621094
Batch 30/64 loss: -0.3992276191711426
Batch 31/64 loss: -0.5946016311645508
Batch 32/64 loss: -0.4735445976257324
Batch 33/64 loss: -0.29608917236328125
Batch 34/64 loss: -0.5137805938720703
Batch 35/64 loss: -0.3880648612976074
Batch 36/64 loss: -0.47656869888305664
Batch 37/64 loss: -0.5404925346374512
Batch 38/64 loss: -0.4479985237121582
Batch 39/64 loss: -0.5239434242248535
Batch 40/64 loss: -0.45708751678466797
Batch 41/64 loss: -0.18152904510498047
Batch 42/64 loss: -0.49831676483154297
Batch 43/64 loss: 0.06574440002441406
Batch 44/64 loss: 0.4293832778930664
Batch 45/64 loss: -0.5194926261901855
Batch 46/64 loss: -0.3237009048461914
Batch 47/64 loss: -0.46970367431640625
Batch 48/64 loss: -0.5874333381652832
Batch 49/64 loss: -0.39372730255126953
Batch 50/64 loss: -0.5709333419799805
Batch 51/64 loss: -0.4716153144836426
Batch 52/64 loss: 0.23286199569702148
Batch 53/64 loss: -0.5025215148925781
Batch 54/64 loss: -0.3222618103027344
Batch 55/64 loss: -0.3090686798095703
Batch 56/64 loss: -0.26449060440063477
Batch 57/64 loss: -0.5373873710632324
Batch 58/64 loss: -0.4304184913635254
Batch 59/64 loss: -0.3294081687927246
Batch 60/64 loss: -0.42398595809936523
Batch 61/64 loss: -0.4077177047729492
Batch 62/64 loss: -0.10916709899902344
Batch 63/64 loss: -0.6266994476318359
Batch 64/64 loss: -4.0818963050842285
Epoch 178  Train loss: -0.31235409717933804  Val loss: -0.7037742326349737
Epoch 179
-------------------------------
Batch 1/64 loss: -0.5471391677856445
Batch 2/64 loss: -0.28262853622436523
Batch 3/64 loss: -0.45792722702026367
Batch 4/64 loss: -0.5901141166687012
Batch 5/64 loss: 1.0367331504821777
Batch 6/64 loss: -0.2113499641418457
Batch 7/64 loss: -0.3866138458251953
Batch 8/64 loss: -0.32315540313720703
Batch 9/64 loss: -0.2660059928894043
Batch 10/64 loss: -0.4630241394042969
Batch 11/64 loss: -0.22500371932983398
Batch 12/64 loss: -0.03379058837890625
Batch 13/64 loss: -0.5492434501647949
Batch 14/64 loss: -0.5295310020446777
Batch 15/64 loss: -0.49596118927001953
Batch 16/64 loss: -0.28570985794067383
Batch 17/64 loss: -0.4231595993041992
Batch 18/64 loss: -0.47113466262817383
Batch 19/64 loss: -0.437349796295166
Batch 20/64 loss: -0.5119233131408691
Batch 21/64 loss: -0.05808830261230469
Batch 22/64 loss: -0.12298154830932617
Batch 23/64 loss: -0.3962836265563965
Batch 24/64 loss: -0.5255093574523926
Batch 25/64 loss: -0.4971175193786621
Batch 26/64 loss: -0.21909141540527344
Batch 27/64 loss: -0.6034541130065918
Batch 28/64 loss: -0.43021631240844727
Batch 29/64 loss: -0.3868422508239746
Batch 30/64 loss: -0.42720842361450195
Batch 31/64 loss: -0.48407649993896484
Batch 32/64 loss: -0.3074169158935547
Batch 33/64 loss: -0.393646240234375
Batch 34/64 loss: -0.32622241973876953
Batch 35/64 loss: -0.4108443260192871
Batch 36/64 loss: -0.4466724395751953
Batch 37/64 loss: -0.41211748123168945
Batch 38/64 loss: -0.3784942626953125
Batch 39/64 loss: 0.053630828857421875
Batch 40/64 loss: -0.4402604103088379
Batch 41/64 loss: -0.5807023048400879
Batch 42/64 loss: -0.20767879486083984
Batch 43/64 loss: -0.3119053840637207
Batch 44/64 loss: -0.34561872482299805
Batch 45/64 loss: -0.14295339584350586
Batch 46/64 loss: -0.2444310188293457
Batch 47/64 loss: -0.31597232818603516
Batch 48/64 loss: -0.26325559616088867
Batch 49/64 loss: -0.12335634231567383
Batch 50/64 loss: -0.5396552085876465
Batch 51/64 loss: 0.1410355567932129
Batch 52/64 loss: -0.4833817481994629
Batch 53/64 loss: -0.36190271377563477
Batch 54/64 loss: 0.39632749557495117
Batch 55/64 loss: -0.4657750129699707
Batch 56/64 loss: -0.1388092041015625
Batch 57/64 loss: -0.6235685348510742
Batch 58/64 loss: -0.6198363304138184
Batch 59/64 loss: -0.2805495262145996
Batch 60/64 loss: -0.11681175231933594
Batch 61/64 loss: -0.5303621292114258
Batch 62/64 loss: 1.6115303039550781
Batch 63/64 loss: -0.43953704833984375
Batch 64/64 loss: -3.899683952331543
Epoch 179  Train loss: -0.33849220649868833  Val loss: -0.6936126656548673
Epoch 180
-------------------------------
Batch 1/64 loss: -0.6019306182861328
Batch 2/64 loss: -0.3552975654602051
Batch 3/64 loss: -0.6046338081359863
Batch 4/64 loss: -0.12146902084350586
Batch 5/64 loss: -0.4440889358520508
Batch 6/64 loss: 0.14863204956054688
Batch 7/64 loss: -0.6924233436584473
Batch 8/64 loss: 0.41967201232910156
Batch 9/64 loss: -0.5929861068725586
Batch 10/64 loss: -0.34706640243530273
Batch 11/64 loss: -0.5882353782653809
Batch 12/64 loss: -0.3634452819824219
Batch 13/64 loss: -0.24761247634887695
Batch 14/64 loss: -0.52178955078125
Batch 15/64 loss: -0.025558948516845703
Batch 16/64 loss: -0.2410888671875
Batch 17/64 loss: -0.07448720932006836
Batch 18/64 loss: 0.6019353866577148
Batch 19/64 loss: -0.03188800811767578
Batch 20/64 loss: 0.6777944564819336
Batch 21/64 loss: 0.2696413993835449
Batch 22/64 loss: 0.8519749641418457
Batch 23/64 loss: 1.1090712547302246
Batch 24/64 loss: 0.6430497169494629
Batch 25/64 loss: 0.6366806030273438
Batch 26/64 loss: 1.2850394248962402
Batch 27/64 loss: 0.3279232978820801
Batch 28/64 loss: -0.09234380722045898
Batch 29/64 loss: 0.06994962692260742
Batch 30/64 loss: 0.3443489074707031
Batch 31/64 loss: -0.035418033599853516
Batch 32/64 loss: 0.46152591705322266
Batch 33/64 loss: 1.2069687843322754
Batch 34/64 loss: -0.11846303939819336
Batch 35/64 loss: 0.21900606155395508
Batch 36/64 loss: 0.08390617370605469
Batch 37/64 loss: 0.02224111557006836
Batch 38/64 loss: 2.5562820434570312
Batch 39/64 loss: 1.1320152282714844
Batch 40/64 loss: 0.70123291015625
Batch 41/64 loss: 0.6313843727111816
Batch 42/64 loss: 0.09578752517700195
Batch 43/64 loss: -0.035872459411621094
Batch 44/64 loss: -0.047306060791015625
Batch 45/64 loss: 0.3157992362976074
Batch 46/64 loss: -0.11501502990722656
Batch 47/64 loss: 0.8126988410949707
Batch 48/64 loss: 1.2269630432128906
Batch 49/64 loss: -0.11580467224121094
Batch 50/64 loss: 0.2982168197631836
Batch 51/64 loss: -0.15773487091064453
Batch 52/64 loss: -0.1649184226989746
Batch 53/64 loss: -0.17966032028198242
Batch 54/64 loss: -0.14074420928955078
Batch 55/64 loss: 0.018558025360107422
Batch 56/64 loss: -0.18925952911376953
Batch 57/64 loss: -0.2607235908508301
Batch 58/64 loss: -0.023375988006591797
Batch 59/64 loss: -0.2514662742614746
Batch 60/64 loss: 1.097752571105957
Batch 61/64 loss: -0.18213272094726562
Batch 62/64 loss: -0.35161828994750977
Batch 63/64 loss: -0.06365299224853516
Batch 64/64 loss: -3.583489418029785
Epoch 180  Train loss: 0.11292428035362094  Val loss: -0.27503695930402305
Epoch 181
-------------------------------
Batch 1/64 loss: 0.07074832916259766
Batch 2/64 loss: 0.1802229881286621
Batch 3/64 loss: -0.21181964874267578
Batch 4/64 loss: 0.5512270927429199
Batch 5/64 loss: 0.5554018020629883
Batch 6/64 loss: -0.14813709259033203
Batch 7/64 loss: -0.21124601364135742
Batch 8/64 loss: 0.1043100357055664
Batch 9/64 loss: -0.026290416717529297
Batch 10/64 loss: -0.17649364471435547
Batch 11/64 loss: -0.23079252243041992
Batch 12/64 loss: 0.11116695404052734
Batch 13/64 loss: -0.12187051773071289
Batch 14/64 loss: -0.07358884811401367
Batch 15/64 loss: -0.17833471298217773
Batch 16/64 loss: -0.19407033920288086
Batch 17/64 loss: -0.16199302673339844
Batch 18/64 loss: -0.32701873779296875
Batch 19/64 loss: -0.33771371841430664
Batch 20/64 loss: -0.1223750114440918
Batch 21/64 loss: -0.2290639877319336
Batch 22/64 loss: -0.17220211029052734
Batch 23/64 loss: -0.14913320541381836
Batch 24/64 loss: -0.12975168228149414
Batch 25/64 loss: 0.9852824211120605
Batch 26/64 loss: -0.24215412139892578
Batch 27/64 loss: -0.16008663177490234
Batch 28/64 loss: -0.37079668045043945
Batch 29/64 loss: -0.16451787948608398
Batch 30/64 loss: -0.02846813201904297
Batch 31/64 loss: -0.1296677589416504
Batch 32/64 loss: -0.3305525779724121
Batch 33/64 loss: -0.2687816619873047
Batch 34/64 loss: -0.274446964263916
Batch 35/64 loss: -0.0412287712097168
Batch 36/64 loss: 1.2415647506713867
Batch 37/64 loss: -0.2867574691772461
Batch 38/64 loss: -0.2373809814453125
Batch 39/64 loss: 0.14777851104736328
Batch 40/64 loss: -0.1280059814453125
Batch 41/64 loss: 0.5115556716918945
Batch 42/64 loss: -0.257840633392334
Batch 43/64 loss: -0.18686485290527344
Batch 44/64 loss: -0.16130495071411133
Batch 45/64 loss: -0.3620610237121582
Batch 46/64 loss: -0.2367849349975586
Batch 47/64 loss: -0.017176151275634766
Batch 48/64 loss: -0.13236427307128906
Batch 49/64 loss: -0.35368967056274414
Batch 50/64 loss: -0.1346426010131836
Batch 51/64 loss: -0.24443960189819336
Batch 52/64 loss: -0.08622455596923828
Batch 53/64 loss: -0.39688634872436523
Batch 54/64 loss: -0.12859010696411133
Batch 55/64 loss: -0.37743425369262695
Batch 56/64 loss: -0.35956907272338867
Batch 57/64 loss: -0.1284799575805664
Batch 58/64 loss: 0.21809673309326172
Batch 59/64 loss: -0.1690073013305664
Batch 60/64 loss: -0.17544889450073242
Batch 61/64 loss: -0.2561302185058594
Batch 62/64 loss: -0.22269296646118164
Batch 63/64 loss: -0.3485093116760254
Batch 64/64 loss: -3.7064342498779297
Epoch 181  Train loss: -0.13652318318684895  Val loss: -0.48853357059439434
Epoch 182
-------------------------------
Batch 1/64 loss: -0.38193225860595703
Batch 2/64 loss: -0.28388023376464844
Batch 3/64 loss: -0.4089531898498535
Batch 4/64 loss: -0.3487710952758789
Batch 5/64 loss: -0.2857022285461426
Batch 6/64 loss: -0.25002241134643555
Batch 7/64 loss: 0.49927377700805664
Batch 8/64 loss: -0.0868692398071289
Batch 9/64 loss: -0.4739537239074707
Batch 10/64 loss: -0.1719660758972168
Batch 11/64 loss: -0.5441150665283203
Batch 12/64 loss: -0.2634010314941406
Batch 13/64 loss: -0.3780336380004883
Batch 14/64 loss: -0.31211185455322266
Batch 15/64 loss: -0.3745841979980469
Batch 16/64 loss: -0.20997285842895508
Batch 17/64 loss: 0.21896696090698242
Batch 18/64 loss: -0.3850574493408203
Batch 19/64 loss: 0.10840606689453125
Batch 20/64 loss: 0.1776127815246582
Batch 21/64 loss: -0.32015180587768555
Batch 22/64 loss: -0.477325439453125
Batch 23/64 loss: -0.3152809143066406
Batch 24/64 loss: -0.24068975448608398
Batch 25/64 loss: 1.1307015419006348
Batch 26/64 loss: 0.07207441329956055
Batch 27/64 loss: -0.2613334655761719
Batch 28/64 loss: -0.30245256423950195
Batch 29/64 loss: -0.1278843879699707
Batch 30/64 loss: -0.3910970687866211
Batch 31/64 loss: -0.2811131477355957
Batch 32/64 loss: -0.16550159454345703
Batch 33/64 loss: -0.263580322265625
Batch 34/64 loss: -0.09495162963867188
Batch 35/64 loss: 0.03142547607421875
Batch 36/64 loss: 1.840951919555664
Batch 37/64 loss: -0.17539262771606445
Batch 38/64 loss: 2.436586856842041
Batch 39/64 loss: 2.249831199645996
Batch 40/64 loss: 3.0787134170532227
Batch 41/64 loss: 7.895308971405029
Batch 42/64 loss: 1.6444921493530273
Batch 43/64 loss: 3.1546034812927246
Batch 44/64 loss: 1.9158082008361816
Batch 45/64 loss: 2.4692344665527344
Batch 46/64 loss: 1.7454957962036133
Batch 47/64 loss: 2.943953037261963
Batch 48/64 loss: 1.5973749160766602
Batch 49/64 loss: 1.4669909477233887
Batch 50/64 loss: 1.1940264701843262
Batch 51/64 loss: 1.4103732109069824
Batch 52/64 loss: 1.0879406929016113
Batch 53/64 loss: 1.6336383819580078
Batch 54/64 loss: 1.6533236503601074
Batch 55/64 loss: 2.005925178527832
Batch 56/64 loss: 1.1308631896972656
Batch 57/64 loss: 1.0401315689086914
Batch 58/64 loss: 0.9634885787963867
Batch 59/64 loss: 1.1726665496826172
Batch 60/64 loss: 0.896705150604248
Batch 61/64 loss: 0.9223952293395996
Batch 62/64 loss: 0.7068009376525879
Batch 63/64 loss: 1.1843881607055664
Batch 64/64 loss: -2.9952969551086426
Epoch 182  Train loss: 0.6722810876135733  Val loss: 1.0819268374098945
Epoch 183
-------------------------------
Batch 1/64 loss: 0.7396945953369141
Batch 2/64 loss: 0.5690040588378906
Batch 3/64 loss: 0.48740243911743164
Batch 4/64 loss: 0.8824343681335449
Batch 5/64 loss: 1.0510749816894531
Batch 6/64 loss: 0.34223222732543945
Batch 7/64 loss: 0.7140159606933594
Batch 8/64 loss: 0.32793331146240234
Batch 9/64 loss: 0.4566383361816406
Batch 10/64 loss: 0.4375333786010742
Batch 11/64 loss: 1.20302152633667
Batch 12/64 loss: 0.060945987701416016
Batch 13/64 loss: 0.4524564743041992
Batch 14/64 loss: 0.36241579055786133
Batch 15/64 loss: 1.4567017555236816
Batch 16/64 loss: 1.468440055847168
Batch 17/64 loss: 0.6226463317871094
Batch 18/64 loss: 0.8849272727966309
Batch 19/64 loss: 1.7013144493103027
Batch 20/64 loss: 0.8243546485900879
Batch 21/64 loss: 0.35858821868896484
Batch 22/64 loss: 0.3413534164428711
Batch 23/64 loss: 0.13141727447509766
Batch 24/64 loss: 0.3736429214477539
Batch 25/64 loss: 0.2021927833557129
Batch 26/64 loss: 0.3283381462097168
Batch 27/64 loss: 0.1188507080078125
Batch 28/64 loss: 0.08059310913085938
Batch 29/64 loss: 0.18578243255615234
Batch 30/64 loss: 0.01545858383178711
Batch 31/64 loss: 0.7905898094177246
Batch 32/64 loss: 0.3147416114807129
Batch 33/64 loss: 0.40623950958251953
Batch 34/64 loss: -0.022614479064941406
Batch 35/64 loss: 0.38254642486572266
Batch 36/64 loss: 0.740872859954834
Batch 37/64 loss: 0.08925771713256836
Batch 38/64 loss: -0.12500810623168945
Batch 39/64 loss: -0.19040584564208984
Batch 40/64 loss: 0.0011773109436035156
Batch 41/64 loss: -0.08948945999145508
Batch 42/64 loss: 0.5538244247436523
Batch 43/64 loss: 0.2730903625488281
Batch 44/64 loss: -0.10781288146972656
Batch 45/64 loss: -0.2651700973510742
Batch 46/64 loss: 1.292379379272461
Batch 47/64 loss: -0.05041790008544922
Batch 48/64 loss: 0.3057088851928711
Batch 49/64 loss: 0.022748470306396484
Batch 50/64 loss: 0.1011819839477539
Batch 51/64 loss: -0.15335369110107422
Batch 52/64 loss: 0.12347412109375
Batch 53/64 loss: 0.6058521270751953
Batch 54/64 loss: 0.17322635650634766
Batch 55/64 loss: -0.1524362564086914
Batch 56/64 loss: -0.07282733917236328
Batch 57/64 loss: -0.0561676025390625
Batch 58/64 loss: 0.01083517074584961
Batch 59/64 loss: -0.2756328582763672
Batch 60/64 loss: -0.07896995544433594
Batch 61/64 loss: 0.07052755355834961
Batch 62/64 loss: -0.024308204650878906
Batch 63/64 loss: 0.041652679443359375
Batch 64/64 loss: -3.8512210845947266
Epoch 183  Train loss: 0.29691453821518843  Val loss: -0.2248067036527129
Epoch 184
-------------------------------
Batch 1/64 loss: -0.18598079681396484
Batch 2/64 loss: -0.145416259765625
Batch 3/64 loss: -0.1156005859375
Batch 4/64 loss: -0.2958097457885742
Batch 5/64 loss: -0.039438724517822266
Batch 6/64 loss: -0.3467521667480469
Batch 7/64 loss: -0.27198362350463867
Batch 8/64 loss: -0.29405784606933594
Batch 9/64 loss: -0.12575912475585938
Batch 10/64 loss: -0.31734132766723633
Batch 11/64 loss: -0.15230178833007812
Batch 12/64 loss: -0.24599695205688477
Batch 13/64 loss: 0.11048126220703125
Batch 14/64 loss: -0.32159948348999023
Batch 15/64 loss: 0.7635512351989746
Batch 16/64 loss: 0.1623382568359375
Batch 17/64 loss: -0.12430143356323242
Batch 18/64 loss: -0.10738277435302734
Batch 19/64 loss: -0.28969621658325195
Batch 20/64 loss: -0.258333683013916
Batch 21/64 loss: -0.4148101806640625
Batch 22/64 loss: -0.24623966217041016
Batch 23/64 loss: -0.4884624481201172
Batch 24/64 loss: -0.24764013290405273
Batch 25/64 loss: -0.20282745361328125
Batch 26/64 loss: -0.1841573715209961
Batch 27/64 loss: -0.1305708885192871
Batch 28/64 loss: -0.10093402862548828
Batch 29/64 loss: -0.08570671081542969
Batch 30/64 loss: 1.3757143020629883
Batch 31/64 loss: -0.08849239349365234
Batch 32/64 loss: 0.10676240921020508
Batch 33/64 loss: -0.49356937408447266
Batch 34/64 loss: -0.19013357162475586
Batch 35/64 loss: -0.038311004638671875
Batch 36/64 loss: -0.37198591232299805
Batch 37/64 loss: -0.3925943374633789
Batch 38/64 loss: -0.18897438049316406
Batch 39/64 loss: -0.10267972946166992
Batch 40/64 loss: -0.26382875442504883
Batch 41/64 loss: -0.20986366271972656
Batch 42/64 loss: -0.40596771240234375
Batch 43/64 loss: -0.41416072845458984
Batch 44/64 loss: -0.1791543960571289
Batch 45/64 loss: 1.0407400131225586
Batch 46/64 loss: -0.20435857772827148
Batch 47/64 loss: -0.4149022102355957
Batch 48/64 loss: -0.3923611640930176
Batch 49/64 loss: -0.4076700210571289
Batch 50/64 loss: -0.3479800224304199
Batch 51/64 loss: -0.1526350975036621
Batch 52/64 loss: -0.3293576240539551
Batch 53/64 loss: -0.1342463493347168
Batch 54/64 loss: -0.11468172073364258
Batch 55/64 loss: -0.342989444732666
Batch 56/64 loss: -0.40442466735839844
Batch 57/64 loss: -0.353391170501709
Batch 58/64 loss: 0.7863888740539551
Batch 59/64 loss: -0.2552318572998047
Batch 60/64 loss: -0.26497602462768555
Batch 61/64 loss: -0.3858819007873535
Batch 62/64 loss: -0.26778507232666016
Batch 63/64 loss: -0.14731407165527344
Batch 64/64 loss: -2.8341445922851562
Epoch 184  Train loss: -0.18476292479271983  Val loss: -0.5453727827039371
Epoch 185
-------------------------------
Batch 1/64 loss: 1.3875880241394043
Batch 2/64 loss: -0.43024206161499023
Batch 3/64 loss: 0.08138418197631836
Batch 4/64 loss: -0.12411308288574219
Batch 5/64 loss: -0.44179296493530273
Batch 6/64 loss: -0.31571292877197266
Batch 7/64 loss: -0.22374439239501953
Batch 8/64 loss: -0.1363210678100586
Batch 9/64 loss: -0.43432092666625977
Batch 10/64 loss: -0.3811960220336914
Batch 11/64 loss: -0.4702572822570801
Batch 12/64 loss: -0.4243640899658203
Batch 13/64 loss: -0.3606758117675781
Batch 14/64 loss: -0.3113265037536621
Batch 15/64 loss: -0.41178226470947266
Batch 16/64 loss: 1.0623407363891602
Batch 17/64 loss: 0.12992477416992188
Batch 18/64 loss: -0.0791926383972168
Batch 19/64 loss: -0.516808032989502
Batch 20/64 loss: -0.3971099853515625
Batch 21/64 loss: -0.35889577865600586
Batch 22/64 loss: 0.209991455078125
Batch 23/64 loss: -0.3240017890930176
Batch 24/64 loss: -0.19831037521362305
Batch 25/64 loss: -0.055683135986328125
Batch 26/64 loss: -0.08426427841186523
Batch 27/64 loss: -0.4543442726135254
Batch 28/64 loss: -0.36690425872802734
Batch 29/64 loss: -0.4303889274597168
Batch 30/64 loss: -0.2249913215637207
Batch 31/64 loss: 0.0708303451538086
Batch 32/64 loss: -0.07790708541870117
Batch 33/64 loss: 0.6914339065551758
Batch 34/64 loss: -0.2893095016479492
Batch 35/64 loss: -0.29848670959472656
Batch 36/64 loss: -0.5212850570678711
Batch 37/64 loss: -0.42466306686401367
Batch 38/64 loss: -0.23737716674804688
Batch 39/64 loss: -0.34153223037719727
Batch 40/64 loss: -0.04292774200439453
Batch 41/64 loss: -0.31037473678588867
Batch 42/64 loss: -0.2664031982421875
Batch 43/64 loss: -0.324617862701416
Batch 44/64 loss: -0.2602100372314453
Batch 45/64 loss: -0.2779664993286133
Batch 46/64 loss: -0.4710373878479004
Batch 47/64 loss: 0.3183765411376953
Batch 48/64 loss: -0.019461631774902344
Batch 49/64 loss: 0.4282822608947754
Batch 50/64 loss: -0.40328550338745117
Batch 51/64 loss: -0.30109357833862305
Batch 52/64 loss: -0.30619287490844727
Batch 53/64 loss: -0.378082275390625
Batch 54/64 loss: -0.36057090759277344
Batch 55/64 loss: -0.34305715560913086
Batch 56/64 loss: -0.27024221420288086
Batch 57/64 loss: -0.32527637481689453
Batch 58/64 loss: 0.19304370880126953
Batch 59/64 loss: -0.32201528549194336
Batch 60/64 loss: -0.3991236686706543
Batch 61/64 loss: -0.3629121780395508
Batch 62/64 loss: -0.41050291061401367
Batch 63/64 loss: -0.2903151512145996
Batch 64/64 loss: -3.8647279739379883
Epoch 185  Train loss: -0.234012929131003  Val loss: -0.6059382199421781
Epoch 186
-------------------------------
Batch 1/64 loss: -0.2975125312805176
Batch 2/64 loss: -0.029137611389160156
Batch 3/64 loss: -0.2890806198120117
Batch 4/64 loss: -0.4900836944580078
Batch 5/64 loss: -0.3899803161621094
Batch 6/64 loss: -0.04688882827758789
Batch 7/64 loss: 0.14127063751220703
Batch 8/64 loss: -0.13522052764892578
Batch 9/64 loss: -0.2706027030944824
Batch 10/64 loss: -0.432586669921875
Batch 11/64 loss: -0.33358001708984375
Batch 12/64 loss: -0.2533388137817383
Batch 13/64 loss: -0.07002019882202148
Batch 14/64 loss: -0.2972688674926758
Batch 15/64 loss: -0.3206939697265625
Batch 16/64 loss: 0.17278242111206055
Batch 17/64 loss: -0.2900099754333496
Batch 18/64 loss: -0.4166250228881836
Batch 19/64 loss: -0.36737918853759766
Batch 20/64 loss: -0.24759721755981445
Batch 21/64 loss: -0.2947368621826172
Batch 22/64 loss: -0.5383563041687012
Batch 23/64 loss: -0.35262250900268555
Batch 24/64 loss: -0.2456650733947754
Batch 25/64 loss: 0.12334394454956055
Batch 26/64 loss: -0.10994863510131836
Batch 27/64 loss: -0.3285398483276367
Batch 28/64 loss: -0.49343013763427734
Batch 29/64 loss: -0.4280819892883301
Batch 30/64 loss: -0.12358617782592773
Batch 31/64 loss: -0.47450685501098633
Batch 32/64 loss: -0.3127751350402832
Batch 33/64 loss: -0.40425872802734375
Batch 34/64 loss: -0.31409597396850586
Batch 35/64 loss: -0.34837961196899414
Batch 36/64 loss: -0.43744659423828125
Batch 37/64 loss: -0.24355173110961914
Batch 38/64 loss: -0.4746065139770508
Batch 39/64 loss: -0.38463306427001953
Batch 40/64 loss: -0.430356502532959
Batch 41/64 loss: -0.014138221740722656
Batch 42/64 loss: -0.39991235733032227
Batch 43/64 loss: -0.4796786308288574
Batch 44/64 loss: -0.2511773109436035
Batch 45/64 loss: 1.1891264915466309
Batch 46/64 loss: 0.8743100166320801
Batch 47/64 loss: -0.1593012809753418
Batch 48/64 loss: -0.3428168296813965
Batch 49/64 loss: -0.44112062454223633
Batch 50/64 loss: -0.40323638916015625
Batch 51/64 loss: -0.3704380989074707
Batch 52/64 loss: 0.1948528289794922
Batch 53/64 loss: -0.5133562088012695
Batch 54/64 loss: -0.585085391998291
Batch 55/64 loss: -0.44784021377563477
Batch 56/64 loss: -0.030837535858154297
Batch 57/64 loss: 0.12674760818481445
Batch 58/64 loss: -0.349700927734375
Batch 59/64 loss: -0.36088991165161133
Batch 60/64 loss: -0.4796905517578125
Batch 61/64 loss: 0.9629311561584473
Batch 62/64 loss: -0.20383596420288086
Batch 63/64 loss: -0.4800581932067871
Batch 64/64 loss: -4.139583587646484
Epoch 186  Train loss: -0.2721509372486788  Val loss: -0.7020943238563144
Epoch 187
-------------------------------
Batch 1/64 loss: -0.5514869689941406
Batch 2/64 loss: -0.5547494888305664
Batch 3/64 loss: -0.34531211853027344
Batch 4/64 loss: -0.3219423294067383
Batch 5/64 loss: -0.39994192123413086
Batch 6/64 loss: 0.3050251007080078
Batch 7/64 loss: -0.45693540573120117
Batch 8/64 loss: -0.19031810760498047
Batch 9/64 loss: -0.5005345344543457
Batch 10/64 loss: -0.523256778717041
Batch 11/64 loss: 0.9838347434997559
Batch 12/64 loss: -0.3474569320678711
Batch 13/64 loss: -0.35221147537231445
Batch 14/64 loss: -0.16178607940673828
Batch 15/64 loss: -0.49905824661254883
Batch 16/64 loss: -0.01847219467163086
Batch 17/64 loss: -0.2070012092590332
Batch 18/64 loss: -0.4240608215332031
Batch 19/64 loss: -0.49678754806518555
Batch 20/64 loss: -0.3340425491333008
Batch 21/64 loss: -0.4217510223388672
Batch 22/64 loss: -0.5176210403442383
Batch 23/64 loss: -0.3761105537414551
Batch 24/64 loss: -0.5407595634460449
Batch 25/64 loss: -0.3664517402648926
Batch 26/64 loss: -0.4190526008605957
Batch 27/64 loss: -0.4159355163574219
Batch 28/64 loss: 0.5626492500305176
Batch 29/64 loss: -0.37343788146972656
Batch 30/64 loss: -0.26024723052978516
Batch 31/64 loss: -0.44947195053100586
Batch 32/64 loss: -0.5212316513061523
Batch 33/64 loss: -0.37978172302246094
Batch 34/64 loss: -0.2690916061401367
Batch 35/64 loss: -0.5361113548278809
Batch 36/64 loss: -0.31430578231811523
Batch 37/64 loss: 0.48984527587890625
Batch 38/64 loss: -0.13795757293701172
Batch 39/64 loss: -0.4507312774658203
Batch 40/64 loss: -0.29285287857055664
Batch 41/64 loss: -0.4564023017883301
Batch 42/64 loss: -0.28314638137817383
Batch 43/64 loss: -0.2985076904296875
Batch 44/64 loss: -0.3503689765930176
Batch 45/64 loss: -0.38863182067871094
Batch 46/64 loss: -0.1200551986694336
Batch 47/64 loss: -0.42935609817504883
Batch 48/64 loss: -0.46353816986083984
Batch 49/64 loss: -0.553372859954834
Batch 50/64 loss: 0.5207271575927734
Batch 51/64 loss: -0.39189863204956055
Batch 52/64 loss: -0.35033559799194336
Batch 53/64 loss: -0.3838205337524414
Batch 54/64 loss: -0.4113950729370117
Batch 55/64 loss: -0.05698108673095703
Batch 56/64 loss: -0.326810359954834
Batch 57/64 loss: -0.10170316696166992
Batch 58/64 loss: 1.0172748565673828
Batch 59/64 loss: -0.20240545272827148
Batch 60/64 loss: -0.3661642074584961
Batch 61/64 loss: -0.34549760818481445
Batch 62/64 loss: -0.2304234504699707
Batch 63/64 loss: -0.27953338623046875
Batch 64/64 loss: -3.965531349182129
Epoch 187  Train loss: -0.3076611425362381  Val loss: -0.19340532178321654
Epoch 188
-------------------------------
Batch 1/64 loss: 0.26867008209228516
Batch 2/64 loss: -0.2537364959716797
Batch 3/64 loss: 0.04253387451171875
Batch 4/64 loss: -0.16354131698608398
Batch 5/64 loss: -0.29021644592285156
Batch 6/64 loss: -0.08130884170532227
Batch 7/64 loss: 0.4783926010131836
Batch 8/64 loss: -0.3279085159301758
Batch 9/64 loss: -0.29372692108154297
Batch 10/64 loss: 0.9914560317993164
Batch 11/64 loss: -0.05340433120727539
Batch 12/64 loss: -0.32530689239501953
Batch 13/64 loss: 0.4541659355163574
Batch 14/64 loss: 0.2852463722229004
Batch 15/64 loss: -0.4112977981567383
Batch 16/64 loss: -0.11433649063110352
Batch 17/64 loss: 0.41047191619873047
Batch 18/64 loss: -0.4340858459472656
Batch 19/64 loss: -0.23276519775390625
Batch 20/64 loss: 0.7618436813354492
Batch 21/64 loss: -0.3126564025878906
Batch 22/64 loss: -0.18121004104614258
Batch 23/64 loss: -0.03024911880493164
Batch 24/64 loss: -0.07054662704467773
Batch 25/64 loss: -0.30773305892944336
Batch 26/64 loss: 0.006005764007568359
Batch 27/64 loss: -0.1404414176940918
Batch 28/64 loss: 1.23079252243042
Batch 29/64 loss: -0.30301809310913086
Batch 30/64 loss: -0.3676180839538574
Batch 31/64 loss: -0.3647170066833496
Batch 32/64 loss: -0.2619590759277344
Batch 33/64 loss: -0.22442054748535156
Batch 34/64 loss: -0.37946414947509766
Batch 35/64 loss: -0.3513951301574707
Batch 36/64 loss: 0.14780712127685547
Batch 37/64 loss: -0.23448801040649414
Batch 38/64 loss: -0.2878689765930176
Batch 39/64 loss: -0.17677927017211914
Batch 40/64 loss: 0.29693031311035156
Batch 41/64 loss: -0.4623537063598633
Batch 42/64 loss: -0.33984851837158203
Batch 43/64 loss: 0.04924583435058594
Batch 44/64 loss: -0.1293163299560547
Batch 45/64 loss: -0.5809445381164551
Batch 46/64 loss: -0.5984125137329102
Batch 47/64 loss: -0.22335529327392578
Batch 48/64 loss: -0.5091323852539062
Batch 49/64 loss: 0.4383988380432129
Batch 50/64 loss: -0.4168272018432617
Batch 51/64 loss: -0.5605168342590332
Batch 52/64 loss: -0.4116659164428711
Batch 53/64 loss: -0.3491859436035156
Batch 54/64 loss: -0.6906819343566895
Batch 55/64 loss: -0.10367250442504883
Batch 56/64 loss: -0.5038590431213379
Batch 57/64 loss: -0.47745418548583984
Batch 58/64 loss: -0.4581880569458008
Batch 59/64 loss: -0.5728998184204102
Batch 60/64 loss: -0.24959373474121094
Batch 61/64 loss: 0.4879488945007324
Batch 62/64 loss: -0.38684988021850586
Batch 63/64 loss: -0.3472576141357422
Batch 64/64 loss: -4.135014533996582
Epoch 188  Train loss: -0.1897971321554745  Val loss: -0.6984086184157539
Epoch 189
-------------------------------
Batch 1/64 loss: -0.3362398147583008
Batch 2/64 loss: -0.4048757553100586
Batch 3/64 loss: -0.5014257431030273
Batch 4/64 loss: -0.3617410659790039
Batch 5/64 loss: -0.336456298828125
Batch 6/64 loss: -0.14764881134033203
Batch 7/64 loss: -0.44333362579345703
Batch 8/64 loss: -0.5743546485900879
Batch 9/64 loss: -0.16937017440795898
Batch 10/64 loss: -0.3303642272949219
Batch 11/64 loss: -0.22826242446899414
Batch 12/64 loss: -0.3273134231567383
Batch 13/64 loss: -0.4826016426086426
Batch 14/64 loss: -0.46302270889282227
Batch 15/64 loss: -0.4697427749633789
Batch 16/64 loss: -0.024123668670654297
Batch 17/64 loss: -0.6163339614868164
Batch 18/64 loss: -0.5697689056396484
Batch 19/64 loss: -0.40419721603393555
Batch 20/64 loss: -0.483306884765625
Batch 21/64 loss: -0.5215373039245605
Batch 22/64 loss: -0.44066524505615234
Batch 23/64 loss: -0.5276479721069336
Batch 24/64 loss: -0.5797796249389648
Batch 25/64 loss: -0.47917985916137695
Batch 26/64 loss: -0.3074355125427246
Batch 27/64 loss: -0.5299396514892578
Batch 28/64 loss: -0.3167433738708496
Batch 29/64 loss: -0.24121713638305664
Batch 30/64 loss: -0.5200929641723633
Batch 31/64 loss: -0.32250547409057617
Batch 32/64 loss: -0.4987607002258301
Batch 33/64 loss: -0.591036319732666
Batch 34/64 loss: -0.04810047149658203
Batch 35/64 loss: -0.2731490135192871
Batch 36/64 loss: -0.4075798988342285
Batch 37/64 loss: -0.508885383605957
Batch 38/64 loss: 0.9628829956054688
Batch 39/64 loss: 0.7900457382202148
Batch 40/64 loss: -0.48491573333740234
Batch 41/64 loss: -0.38747167587280273
Batch 42/64 loss: -0.44590282440185547
Batch 43/64 loss: -0.3687129020690918
Batch 44/64 loss: -0.33971357345581055
Batch 45/64 loss: -0.3680887222290039
Batch 46/64 loss: -0.4462928771972656
Batch 47/64 loss: 0.006049633026123047
Batch 48/64 loss: -0.4827108383178711
Batch 49/64 loss: -0.5315570831298828
Batch 50/64 loss: 0.15595245361328125
Batch 51/64 loss: -0.3400297164916992
Batch 52/64 loss: -0.03253889083862305
Batch 53/64 loss: 0.674379825592041
Batch 54/64 loss: 0.14713001251220703
Batch 55/64 loss: -0.4241948127746582
Batch 56/64 loss: -0.5738315582275391
Batch 57/64 loss: -0.5261716842651367
Batch 58/64 loss: -0.5265798568725586
Batch 59/64 loss: 0.20955801010131836
Batch 60/64 loss: -0.5111498832702637
Batch 61/64 loss: -0.3651609420776367
Batch 62/64 loss: -0.29607725143432617
Batch 63/64 loss: -0.37941980361938477
Batch 64/64 loss: -3.7647247314453125
Epoch 189  Train loss: -0.3528910618202359  Val loss: -0.6364109786515383
Epoch 190
-------------------------------
Batch 1/64 loss: -0.5777382850646973
Batch 2/64 loss: -0.029657363891601562
Batch 3/64 loss: -0.44621992111206055
Batch 4/64 loss: -0.4179816246032715
Batch 5/64 loss: -0.38199949264526367
Batch 6/64 loss: -0.09526252746582031
Batch 7/64 loss: -0.274294376373291
Batch 8/64 loss: 0.03906583786010742
Batch 9/64 loss: 0.6540021896362305
Batch 10/64 loss: -0.2981438636779785
Batch 11/64 loss: -0.26761531829833984
Batch 12/64 loss: 0.7022366523742676
Batch 13/64 loss: -0.34838008880615234
Batch 14/64 loss: -0.4609184265136719
Batch 15/64 loss: -0.2719583511352539
Batch 16/64 loss: -0.34723424911499023
Batch 17/64 loss: -0.3751392364501953
Batch 18/64 loss: -0.3781924247741699
Batch 19/64 loss: -0.5622491836547852
Batch 20/64 loss: -0.5851650238037109
Batch 21/64 loss: -0.41324377059936523
Batch 22/64 loss: -0.3453507423400879
Batch 23/64 loss: -0.36846017837524414
Batch 24/64 loss: 1.2074270248413086
Batch 25/64 loss: -0.49353599548339844
Batch 26/64 loss: -0.5115499496459961
Batch 27/64 loss: 0.5647110939025879
Batch 28/64 loss: -0.257108211517334
Batch 29/64 loss: -0.4193868637084961
Batch 30/64 loss: -0.5775637626647949
Batch 31/64 loss: 0.9746222496032715
Batch 32/64 loss: -0.5564556121826172
Batch 33/64 loss: -0.5819425582885742
Batch 34/64 loss: -0.4672555923461914
Batch 35/64 loss: -0.43070125579833984
Batch 36/64 loss: -0.4185328483581543
Batch 37/64 loss: -0.2644181251525879
Batch 38/64 loss: -0.44451284408569336
Batch 39/64 loss: -0.48612213134765625
Batch 40/64 loss: -0.4652290344238281
Batch 41/64 loss: -0.3026161193847656
Batch 42/64 loss: -0.34392356872558594
Batch 43/64 loss: -0.5691280364990234
Batch 44/64 loss: 0.7938857078552246
Batch 45/64 loss: -0.46553707122802734
Batch 46/64 loss: -0.5151257514953613
Batch 47/64 loss: -0.43814945220947266
Batch 48/64 loss: -0.5710659027099609
Batch 49/64 loss: -0.48421287536621094
Batch 50/64 loss: -0.31804418563842773
Batch 51/64 loss: -0.5388212203979492
Batch 52/64 loss: -0.38535642623901367
Batch 53/64 loss: -0.21401596069335938
Batch 54/64 loss: -0.4846162796020508
Batch 55/64 loss: -0.18909692764282227
Batch 56/64 loss: -0.3451056480407715
Batch 57/64 loss: -0.49793148040771484
Batch 58/64 loss: -0.2930746078491211
Batch 59/64 loss: -0.21340513229370117
Batch 60/64 loss: 0.24392461776733398
Batch 61/64 loss: -0.5602006912231445
Batch 62/64 loss: -0.5251927375793457
Batch 63/64 loss: -0.20877313613891602
Batch 64/64 loss: -3.9330835342407227
Epoch 190  Train loss: -0.3114172505397423  Val loss: -0.7471398422398519
Epoch 191
-------------------------------
Batch 1/64 loss: -0.25447559356689453
Batch 2/64 loss: -0.5661869049072266
Batch 3/64 loss: -0.4385089874267578
Batch 4/64 loss: -0.15149974822998047
Batch 5/64 loss: -0.07747840881347656
Batch 6/64 loss: -0.34356212615966797
Batch 7/64 loss: -0.32183122634887695
Batch 8/64 loss: -0.1360330581665039
Batch 9/64 loss: 0.7374558448791504
Batch 10/64 loss: -0.493593692779541
Batch 11/64 loss: 0.07019901275634766
Batch 12/64 loss: 0.09242916107177734
Batch 13/64 loss: -0.01052713394165039
Batch 14/64 loss: 1.139479637145996
Batch 15/64 loss: -0.031066417694091797
Batch 16/64 loss: -0.2057342529296875
Batch 17/64 loss: 0.21776485443115234
Batch 18/64 loss: -0.010735034942626953
Batch 19/64 loss: 0.41350555419921875
Batch 20/64 loss: 0.09650135040283203
Batch 21/64 loss: -0.09533262252807617
Batch 22/64 loss: 0.13715505599975586
Batch 23/64 loss: -0.08551168441772461
Batch 24/64 loss: -0.18540191650390625
Batch 25/64 loss: 0.1381387710571289
Batch 26/64 loss: 0.15576696395874023
Batch 27/64 loss: -0.2093667984008789
Batch 28/64 loss: 0.032464027404785156
Batch 29/64 loss: 1.541994571685791
Batch 30/64 loss: 0.22799301147460938
Batch 31/64 loss: -0.1827530860900879
Batch 32/64 loss: -0.05324554443359375
Batch 33/64 loss: 0.3408498764038086
Batch 34/64 loss: -0.2996101379394531
Batch 35/64 loss: -0.3384556770324707
Batch 36/64 loss: -0.1795949935913086
Batch 37/64 loss: -0.20343780517578125
Batch 38/64 loss: -0.2325000762939453
Batch 39/64 loss: -0.21033382415771484
Batch 40/64 loss: 0.35825395584106445
Batch 41/64 loss: -0.3773488998413086
Batch 42/64 loss: -0.1427450180053711
Batch 43/64 loss: -0.3573775291442871
Batch 44/64 loss: -0.13622093200683594
Batch 45/64 loss: -0.2277688980102539
Batch 46/64 loss: 0.04498720169067383
Batch 47/64 loss: -0.3348259925842285
Batch 48/64 loss: -0.281280517578125
Batch 49/64 loss: -0.23131036758422852
Batch 50/64 loss: -0.37395811080932617
Batch 51/64 loss: 0.678947925567627
Batch 52/64 loss: -0.15343379974365234
Batch 53/64 loss: -0.3600444793701172
Batch 54/64 loss: -0.33913469314575195
Batch 55/64 loss: -0.25951242446899414
Batch 56/64 loss: -0.26531219482421875
Batch 57/64 loss: -0.29147958755493164
Batch 58/64 loss: 0.12484264373779297
Batch 59/64 loss: -0.21442890167236328
Batch 60/64 loss: -0.3292865753173828
Batch 61/64 loss: -0.1958146095275879
Batch 62/64 loss: -0.33682680130004883
Batch 63/64 loss: -0.23632383346557617
Batch 64/64 loss: -3.974851608276367
Epoch 191  Train loss: -0.11284110125373392  Val loss: -0.7059094012807735
Epoch 192
-------------------------------
Batch 1/64 loss: -0.32510852813720703
Batch 2/64 loss: -0.30880069732666016
Batch 3/64 loss: -0.3790621757507324
Batch 4/64 loss: -0.13149118423461914
Batch 5/64 loss: -0.39859628677368164
Batch 6/64 loss: -0.5034852027893066
Batch 7/64 loss: -0.1535778045654297
Batch 8/64 loss: -0.2746310234069824
Batch 9/64 loss: -0.0863037109375
Batch 10/64 loss: -0.3764362335205078
Batch 11/64 loss: -0.0872340202331543
Batch 12/64 loss: -0.3820514678955078
Batch 13/64 loss: -0.3915696144104004
Batch 14/64 loss: -0.26987123489379883
Batch 15/64 loss: -0.35793256759643555
Batch 16/64 loss: -0.10912895202636719
Batch 17/64 loss: -0.440399169921875
Batch 18/64 loss: 1.680433750152588
Batch 19/64 loss: -0.44267940521240234
Batch 20/64 loss: -0.46888303756713867
Batch 21/64 loss: -0.594325065612793
Batch 22/64 loss: -0.34523773193359375
Batch 23/64 loss: -0.0501093864440918
Batch 24/64 loss: -0.33366823196411133
Batch 25/64 loss: -0.4726829528808594
Batch 26/64 loss: -0.07232999801635742
Batch 27/64 loss: -0.43909740447998047
Batch 28/64 loss: -0.48505544662475586
Batch 29/64 loss: -0.4820222854614258
Batch 30/64 loss: -0.45954418182373047
Batch 31/64 loss: -0.30032968521118164
Batch 32/64 loss: -0.3243265151977539
Batch 33/64 loss: -0.48186397552490234
Batch 34/64 loss: -0.30286359786987305
Batch 35/64 loss: -0.29160213470458984
Batch 36/64 loss: -0.00035190582275390625
Batch 37/64 loss: -0.32706737518310547
Batch 38/64 loss: -0.4076056480407715
Batch 39/64 loss: -0.22959661483764648
Batch 40/64 loss: -0.19486141204833984
Batch 41/64 loss: -0.30469322204589844
Batch 42/64 loss: -0.38717126846313477
Batch 43/64 loss: -0.3435673713684082
Batch 44/64 loss: -0.37188053131103516
Batch 45/64 loss: -0.37804365158081055
Batch 46/64 loss: -0.43854188919067383
Batch 47/64 loss: -0.44708824157714844
Batch 48/64 loss: -0.5674910545349121
Batch 49/64 loss: 0.5621299743652344
Batch 50/64 loss: 0.18187475204467773
Batch 51/64 loss: -0.5180420875549316
Batch 52/64 loss: -0.4482593536376953
Batch 53/64 loss: -0.4505882263183594
Batch 54/64 loss: -0.4054999351501465
Batch 55/64 loss: -0.3575425148010254
Batch 56/64 loss: -0.4111781120300293
Batch 57/64 loss: 0.7306828498840332
Batch 58/64 loss: -0.3592672348022461
Batch 59/64 loss: 0.2617955207824707
Batch 60/64 loss: -0.3430509567260742
Batch 61/64 loss: -0.4446878433227539
Batch 62/64 loss: -0.46358203887939453
Batch 63/64 loss: -0.36909008026123047
Batch 64/64 loss: -3.759953022003174
Epoch 192  Train loss: -0.3089270180346919  Val loss: -0.6086340311056969
Epoch 193
-------------------------------
Batch 1/64 loss: -0.5365920066833496
Batch 2/64 loss: -0.2781662940979004
Batch 3/64 loss: 0.7594785690307617
Batch 4/64 loss: -0.47675132751464844
Batch 5/64 loss: -0.3790111541748047
Batch 6/64 loss: -0.3472895622253418
Batch 7/64 loss: -0.3688979148864746
Batch 8/64 loss: -0.22602605819702148
Batch 9/64 loss: -0.5622458457946777
Batch 10/64 loss: -0.4444570541381836
Batch 11/64 loss: -0.39762163162231445
Batch 12/64 loss: -0.29700422286987305
Batch 13/64 loss: 0.008137226104736328
Batch 14/64 loss: -0.38822507858276367
Batch 15/64 loss: -0.3875088691711426
Batch 16/64 loss: -0.2970867156982422
Batch 17/64 loss: -0.3194870948791504
Batch 18/64 loss: -0.3157811164855957
Batch 19/64 loss: -0.4620537757873535
Batch 20/64 loss: -0.41582441329956055
Batch 21/64 loss: -0.4384193420410156
Batch 22/64 loss: -0.35185670852661133
Batch 23/64 loss: -0.08663702011108398
Batch 24/64 loss: 0.023749351501464844
Batch 25/64 loss: -0.40097570419311523
Batch 26/64 loss: -0.43375253677368164
Batch 27/64 loss: -0.0887002944946289
Batch 28/64 loss: -0.40828418731689453
Batch 29/64 loss: -0.5010395050048828
Batch 30/64 loss: -0.29996633529663086
Batch 31/64 loss: -0.48529815673828125
Batch 32/64 loss: -0.35720157623291016
Batch 33/64 loss: -0.37272167205810547
Batch 34/64 loss: -0.48697900772094727
Batch 35/64 loss: -0.31119680404663086
Batch 36/64 loss: -0.5330734252929688
Batch 37/64 loss: -0.27730751037597656
Batch 38/64 loss: -0.3243751525878906
Batch 39/64 loss: -0.44100284576416016
Batch 40/64 loss: -0.4876527786254883
Batch 41/64 loss: -0.48546314239501953
Batch 42/64 loss: -0.27542734146118164
Batch 43/64 loss: -0.3366527557373047
Batch 44/64 loss: -0.3369274139404297
Batch 45/64 loss: -0.40535974502563477
Batch 46/64 loss: -0.2778968811035156
Batch 47/64 loss: 0.18416452407836914
Batch 48/64 loss: -0.36243438720703125
Batch 49/64 loss: -0.4175238609313965
Batch 50/64 loss: 1.0873804092407227
Batch 51/64 loss: -0.5085692405700684
Batch 52/64 loss: -0.3272829055786133
Batch 53/64 loss: 0.8865699768066406
Batch 54/64 loss: -0.4435858726501465
Batch 55/64 loss: 0.3975105285644531
Batch 56/64 loss: -0.4491086006164551
Batch 57/64 loss: -0.3842339515686035
Batch 58/64 loss: -0.5419902801513672
Batch 59/64 loss: -0.30174779891967773
Batch 60/64 loss: 0.04946756362915039
Batch 61/64 loss: -0.39342737197875977
Batch 62/64 loss: -0.12379598617553711
Batch 63/64 loss: -0.21173381805419922
Batch 64/64 loss: -3.8515100479125977
Epoch 193  Train loss: -0.3146950029859356  Val loss: -0.6721669357666855
Epoch 194
-------------------------------
Batch 1/64 loss: -0.4942641258239746
Batch 2/64 loss: -0.5165290832519531
Batch 3/64 loss: -0.35726118087768555
Batch 4/64 loss: -0.3488154411315918
Batch 5/64 loss: -0.4894728660583496
Batch 6/64 loss: -0.4595026969909668
Batch 7/64 loss: -0.31600379943847656
Batch 8/64 loss: -0.46569347381591797
Batch 9/64 loss: 0.03729677200317383
Batch 10/64 loss: -0.5199494361877441
Batch 11/64 loss: -0.03957080841064453
Batch 12/64 loss: -0.5185966491699219
Batch 13/64 loss: 0.5272550582885742
Batch 14/64 loss: -0.42533159255981445
Batch 15/64 loss: -0.37652587890625
Batch 16/64 loss: -0.49167346954345703
Batch 17/64 loss: -0.4122896194458008
Batch 18/64 loss: -0.44653987884521484
Batch 19/64 loss: -0.3515315055847168
Batch 20/64 loss: -0.056090354919433594
Batch 21/64 loss: -0.27480316162109375
Batch 22/64 loss: -0.23778247833251953
Batch 23/64 loss: -0.4472999572753906
Batch 24/64 loss: -0.18735980987548828
Batch 25/64 loss: -0.4681577682495117
Batch 26/64 loss: -0.03107595443725586
Batch 27/64 loss: -0.36289548873901367
Batch 28/64 loss: -0.48951244354248047
Batch 29/64 loss: -0.40811729431152344
Batch 30/64 loss: 0.4471893310546875
Batch 31/64 loss: -0.4282970428466797
Batch 32/64 loss: -0.33144664764404297
Batch 33/64 loss: -0.5883283615112305
Batch 34/64 loss: 1.3114628791809082
Batch 35/64 loss: -0.3258800506591797
Batch 36/64 loss: -0.3480362892150879
Batch 37/64 loss: -0.40732860565185547
Batch 38/64 loss: -0.5439057350158691
Batch 39/64 loss: -0.2429642677307129
Batch 40/64 loss: -0.43239355087280273
Batch 41/64 loss: -0.4704465866088867
Batch 42/64 loss: -0.4679083824157715
Batch 43/64 loss: -0.5488247871398926
Batch 44/64 loss: -0.3446521759033203
Batch 45/64 loss: -0.3527493476867676
Batch 46/64 loss: -0.4128899574279785
Batch 47/64 loss: -0.3339223861694336
Batch 48/64 loss: -0.47988367080688477
Batch 49/64 loss: -0.5488095283508301
Batch 50/64 loss: -0.3695974349975586
Batch 51/64 loss: -0.5104503631591797
Batch 52/64 loss: -0.5080828666687012
Batch 53/64 loss: -0.4905214309692383
Batch 54/64 loss: -0.5295515060424805
Batch 55/64 loss: 1.142723560333252
Batch 56/64 loss: -0.4171772003173828
Batch 57/64 loss: -0.398892879486084
Batch 58/64 loss: -0.36304140090942383
Batch 59/64 loss: -0.46945619583129883
Batch 60/64 loss: -0.2930412292480469
Batch 61/64 loss: 0.6805272102355957
Batch 62/64 loss: -0.5700993537902832
Batch 63/64 loss: -0.4985647201538086
Batch 64/64 loss: -3.998629093170166
Epoch 194  Train loss: -0.34309501460954256  Val loss: -0.47079804672818004
Epoch 195
-------------------------------
Batch 1/64 loss: -0.42717552185058594
Batch 2/64 loss: -0.3672924041748047
Batch 3/64 loss: -0.06780099868774414
Batch 4/64 loss: -0.19775724411010742
Batch 5/64 loss: -0.32047510147094727
Batch 6/64 loss: -0.45290470123291016
Batch 7/64 loss: -0.39838409423828125
Batch 8/64 loss: -0.3991212844848633
Batch 9/64 loss: -0.34600067138671875
Batch 10/64 loss: -0.049062252044677734
Batch 11/64 loss: -0.2907428741455078
Batch 12/64 loss: -0.08227252960205078
Batch 13/64 loss: -0.34813499450683594
Batch 14/64 loss: -0.46173810958862305
Batch 15/64 loss: -0.4309830665588379
Batch 16/64 loss: -0.5232577323913574
Batch 17/64 loss: -0.22382450103759766
Batch 18/64 loss: -0.49184131622314453
Batch 19/64 loss: 0.22759151458740234
Batch 20/64 loss: -0.36339426040649414
Batch 21/64 loss: -0.0179901123046875
Batch 22/64 loss: 0.2709670066833496
Batch 23/64 loss: -0.37262678146362305
Batch 24/64 loss: -0.5427460670471191
Batch 25/64 loss: -0.42814064025878906
Batch 26/64 loss: -0.2671198844909668
Batch 27/64 loss: -0.3892521858215332
Batch 28/64 loss: 0.29973554611206055
Batch 29/64 loss: -0.2678389549255371
Batch 30/64 loss: -0.3827509880065918
Batch 31/64 loss: -0.28459596633911133
Batch 32/64 loss: -0.12736988067626953
Batch 33/64 loss: 0.06631708145141602
Batch 34/64 loss: -0.25159502029418945
Batch 35/64 loss: -0.15474796295166016
Batch 36/64 loss: -0.36074256896972656
Batch 37/64 loss: 0.1147298812866211
Batch 38/64 loss: -0.35401058197021484
Batch 39/64 loss: -0.24439525604248047
Batch 40/64 loss: 0.8891172409057617
Batch 41/64 loss: 1.0461134910583496
Batch 42/64 loss: 0.0037283897399902344
Batch 43/64 loss: -0.40049219131469727
Batch 44/64 loss: -0.3431553840637207
Batch 45/64 loss: 0.18570947647094727
Batch 46/64 loss: -0.23793888092041016
Batch 47/64 loss: 0.5589914321899414
Batch 48/64 loss: -0.4405040740966797
Batch 49/64 loss: -0.4726219177246094
Batch 50/64 loss: -0.4420585632324219
Batch 51/64 loss: -0.49739503860473633
Batch 52/64 loss: -0.5373687744140625
Batch 53/64 loss: -0.2753300666809082
Batch 54/64 loss: -0.4591841697692871
Batch 55/64 loss: -0.49117612838745117
Batch 56/64 loss: -0.46119117736816406
Batch 57/64 loss: -0.5771045684814453
Batch 58/64 loss: -0.49140405654907227
Batch 59/64 loss: 0.08400630950927734
Batch 60/64 loss: -0.5121040344238281
Batch 61/64 loss: -0.42540884017944336
Batch 62/64 loss: -0.480438232421875
Batch 63/64 loss: -0.26694631576538086
Batch 64/64 loss: -3.9010825157165527
Epoch 195  Train loss: -0.2773131520140405  Val loss: -0.6710697318270444
Epoch 196
-------------------------------
Batch 1/64 loss: -0.3134794235229492
Batch 2/64 loss: -0.3369898796081543
Batch 3/64 loss: -0.03535032272338867
Batch 4/64 loss: -0.4542255401611328
Batch 5/64 loss: -0.45025110244750977
Batch 6/64 loss: -0.337766170501709
Batch 7/64 loss: 0.09310293197631836
Batch 8/64 loss: -0.2535133361816406
Batch 9/64 loss: -0.16855621337890625
Batch 10/64 loss: -0.5563507080078125
Batch 11/64 loss: -0.4837050437927246
Batch 12/64 loss: -0.25934600830078125
Batch 13/64 loss: -0.4898548126220703
Batch 14/64 loss: -0.41451406478881836
Batch 15/64 loss: -0.5179476737976074
Batch 16/64 loss: -0.2364349365234375
Batch 17/64 loss: -0.11445093154907227
Batch 18/64 loss: -0.3773188591003418
Batch 19/64 loss: -0.5408859252929688
Batch 20/64 loss: -0.2721061706542969
Batch 21/64 loss: -0.5558676719665527
Batch 22/64 loss: -0.30759429931640625
Batch 23/64 loss: -0.45409536361694336
Batch 24/64 loss: -0.42356252670288086
Batch 25/64 loss: -0.3613104820251465
Batch 26/64 loss: -0.4759206771850586
Batch 27/64 loss: -0.004344463348388672
Batch 28/64 loss: -0.3080153465270996
Batch 29/64 loss: -0.45303916931152344
Batch 30/64 loss: -0.5120439529418945
Batch 31/64 loss: 0.19573688507080078
Batch 32/64 loss: -0.22258472442626953
Batch 33/64 loss: -0.2879953384399414
Batch 34/64 loss: -0.513394832611084
Batch 35/64 loss: -0.42316627502441406
Batch 36/64 loss: -0.3099346160888672
Batch 37/64 loss: -0.4427027702331543
Batch 38/64 loss: -0.28508615493774414
Batch 39/64 loss: -0.37163734436035156
Batch 40/64 loss: -0.41916942596435547
Batch 41/64 loss: -0.4914722442626953
Batch 42/64 loss: 0.0028543472290039062
Batch 43/64 loss: -0.4808783531188965
Batch 44/64 loss: -0.2366318702697754
Batch 45/64 loss: 0.685549259185791
Batch 46/64 loss: -0.520789623260498
Batch 47/64 loss: -0.47922277450561523
Batch 48/64 loss: -0.5836706161499023
Batch 49/64 loss: -0.3166165351867676
Batch 50/64 loss: -0.3919086456298828
Batch 51/64 loss: 0.09287643432617188
Batch 52/64 loss: -0.46849822998046875
Batch 53/64 loss: 0.09972810745239258
Batch 54/64 loss: -0.3635678291320801
Batch 55/64 loss: 1.0184683799743652
Batch 56/64 loss: -0.5099825859069824
Batch 57/64 loss: -0.5669541358947754
Batch 58/64 loss: 0.9651093482971191
Batch 59/64 loss: -0.5471920967102051
Batch 60/64 loss: -0.3821139335632324
Batch 61/64 loss: -0.6145534515380859
Batch 62/64 loss: -0.23702239990234375
Batch 63/64 loss: -0.5287671089172363
Batch 64/64 loss: -4.122185230255127
Epoch 196  Train loss: -0.3357265603308584  Val loss: -0.7477969140121618
Epoch 197
-------------------------------
Batch 1/64 loss: -0.48178911209106445
Batch 2/64 loss: -0.5443768501281738
Batch 3/64 loss: -0.5448064804077148
Batch 4/64 loss: -0.32454490661621094
Batch 5/64 loss: -0.2572031021118164
Batch 6/64 loss: -0.4198908805847168
Batch 7/64 loss: -0.3650374412536621
Batch 8/64 loss: 1.2101831436157227
Batch 9/64 loss: -0.3305392265319824
Batch 10/64 loss: -0.3156108856201172
Batch 11/64 loss: -0.08545064926147461
Batch 12/64 loss: -0.28078269958496094
Batch 13/64 loss: -0.2838616371154785
Batch 14/64 loss: -0.21500778198242188
Batch 15/64 loss: -0.24214553833007812
Batch 16/64 loss: -0.28171586990356445
Batch 17/64 loss: -0.37154579162597656
Batch 18/64 loss: -0.3612632751464844
Batch 19/64 loss: -0.48056840896606445
Batch 20/64 loss: -0.5468182563781738
Batch 21/64 loss: -0.35272741317749023
Batch 22/64 loss: -0.40726709365844727
Batch 23/64 loss: -0.46186256408691406
Batch 24/64 loss: -0.37120962142944336
Batch 25/64 loss: 0.5013608932495117
Batch 26/64 loss: 0.9713006019592285
Batch 27/64 loss: -0.552894115447998
Batch 28/64 loss: -0.4748802185058594
Batch 29/64 loss: -0.5030531883239746
Batch 30/64 loss: -0.09546136856079102
Batch 31/64 loss: -0.3132171630859375
Batch 32/64 loss: -0.5298213958740234
Batch 33/64 loss: -0.35422706604003906
Batch 34/64 loss: -0.49069643020629883
Batch 35/64 loss: -0.396181583404541
Batch 36/64 loss: -0.5170011520385742
Batch 37/64 loss: 0.1374516487121582
Batch 38/64 loss: -0.3440871238708496
Batch 39/64 loss: -0.5216937065124512
Batch 40/64 loss: 0.8233671188354492
Batch 41/64 loss: -0.16130590438842773
Batch 42/64 loss: 1.5430865287780762
Batch 43/64 loss: 0.3220486640930176
Batch 44/64 loss: 0.9423050880432129
Batch 45/64 loss: -0.01945209503173828
Batch 46/64 loss: 0.6560397148132324
Batch 47/64 loss: 0.16382598876953125
Batch 48/64 loss: 0.671018123626709
Batch 49/64 loss: -0.007836341857910156
Batch 50/64 loss: 0.1565685272216797
Batch 51/64 loss: 0.27620935440063477
Batch 52/64 loss: 0.43871164321899414
Batch 53/64 loss: 0.1410822868347168
Batch 54/64 loss: 0.3251500129699707
Batch 55/64 loss: 0.1371450424194336
Batch 56/64 loss: 0.9778757095336914
Batch 57/64 loss: -0.018369197845458984
Batch 58/64 loss: -0.02395486831665039
Batch 59/64 loss: 0.15883493423461914
Batch 60/64 loss: 0.9613871574401855
Batch 61/64 loss: -0.010660648345947266
Batch 62/64 loss: 0.44354915618896484
Batch 63/64 loss: 0.1349024772644043
Batch 64/64 loss: -3.546799659729004
Epoch 197  Train loss: -0.06631396050546684  Val loss: 0.09166683446091066
Epoch 198
-------------------------------
Batch 1/64 loss: 0.2776312828063965
Batch 2/64 loss: 0.16573095321655273
Batch 3/64 loss: 2.158442497253418
Batch 4/64 loss: 0.4699831008911133
Batch 5/64 loss: -0.05612611770629883
Batch 6/64 loss: 0.06888675689697266
Batch 7/64 loss: -0.16608619689941406
Batch 8/64 loss: 0.6975913047790527
Batch 9/64 loss: 0.6220712661743164
Batch 10/64 loss: 0.19861888885498047
Batch 11/64 loss: 0.17060470581054688
Batch 12/64 loss: 0.14757680892944336
Batch 13/64 loss: 0.20770835876464844
Batch 14/64 loss: -0.27336931228637695
Batch 15/64 loss: -0.0002288818359375
Batch 16/64 loss: -0.07427215576171875
Batch 17/64 loss: -0.029443740844726562
Batch 18/64 loss: 1.4986348152160645
Batch 19/64 loss: 0.13231277465820312
Batch 20/64 loss: 0.0382843017578125
Batch 21/64 loss: -0.028244972229003906
Batch 22/64 loss: 0.3922610282897949
Batch 23/64 loss: 0.28580474853515625
Batch 24/64 loss: -0.2626218795776367
Batch 25/64 loss: 0.13747787475585938
Batch 26/64 loss: 0.25290346145629883
Batch 27/64 loss: -0.1876382827758789
Batch 28/64 loss: 1.2735986709594727
Batch 29/64 loss: -0.16176748275756836
Batch 30/64 loss: -0.3852100372314453
Batch 31/64 loss: -0.32016944885253906
Batch 32/64 loss: -0.31018638610839844
Batch 33/64 loss: -0.10820865631103516
Batch 34/64 loss: 0.7173099517822266
Batch 35/64 loss: -0.22036027908325195
Batch 36/64 loss: -0.39074182510375977
Batch 37/64 loss: -0.0028076171875
Batch 38/64 loss: -0.35370302200317383
Batch 39/64 loss: -0.4313511848449707
Batch 40/64 loss: -0.34918642044067383
Batch 41/64 loss: -0.26147890090942383
Batch 42/64 loss: -0.4294462203979492
Batch 43/64 loss: -0.11763477325439453
Batch 44/64 loss: -0.19209003448486328
Batch 45/64 loss: 0.04100990295410156
Batch 46/64 loss: -0.1134195327758789
Batch 47/64 loss: -0.3789973258972168
Batch 48/64 loss: 0.2465834617614746
Batch 49/64 loss: 0.8121776580810547
Batch 50/64 loss: -0.3248300552368164
Batch 51/64 loss: -0.36291980743408203
Batch 52/64 loss: -0.06649017333984375
Batch 53/64 loss: 0.04557991027832031
Batch 54/64 loss: -0.10129737854003906
Batch 55/64 loss: 0.05192899703979492
Batch 56/64 loss: -0.42684316635131836
Batch 57/64 loss: -0.38111257553100586
Batch 58/64 loss: -0.16927385330200195
Batch 59/64 loss: -0.07598686218261719
Batch 60/64 loss: -0.3305935859680176
Batch 61/64 loss: -0.39322853088378906
Batch 62/64 loss: -0.052387237548828125
Batch 63/64 loss: -0.3136739730834961
Batch 64/64 loss: -3.714186191558838
Epoch 198  Train loss: -0.004366338019277535  Val loss: -0.47883120926794726
Epoch 199
-------------------------------
Batch 1/64 loss: 0.8290200233459473
Batch 2/64 loss: -0.30025720596313477
Batch 3/64 loss: -0.41455554962158203
Batch 4/64 loss: -0.4818739891052246
Batch 5/64 loss: -0.13556385040283203
Batch 6/64 loss: -0.2648940086364746
Batch 7/64 loss: -0.39371156692504883
Batch 8/64 loss: -0.09719467163085938
Batch 9/64 loss: -0.21477699279785156
Batch 10/64 loss: 0.09216547012329102
Batch 11/64 loss: -0.502471923828125
Batch 12/64 loss: -0.2638711929321289
Batch 13/64 loss: -0.3483915328979492
Batch 14/64 loss: -0.497652530670166
Batch 15/64 loss: -0.4643378257751465
Batch 16/64 loss: -0.491729736328125
Batch 17/64 loss: -0.3977069854736328
Batch 18/64 loss: -0.18706178665161133
Batch 19/64 loss: -0.30947017669677734
Batch 20/64 loss: -0.3174762725830078
Batch 21/64 loss: -0.3321056365966797
Batch 22/64 loss: -0.49845457077026367
Batch 23/64 loss: -0.2555713653564453
Batch 24/64 loss: 0.1723623275756836
Batch 25/64 loss: -0.35570669174194336
Batch 26/64 loss: -0.2666144371032715
Batch 27/64 loss: -0.13854503631591797
Batch 28/64 loss: -0.4964027404785156
Batch 29/64 loss: -0.4182772636413574
Batch 30/64 loss: -0.337979793548584
Batch 31/64 loss: 0.0072765350341796875
Batch 32/64 loss: -0.43471193313598633
Batch 33/64 loss: -0.22137212753295898
Batch 34/64 loss: 0.06888198852539062
Batch 35/64 loss: -0.4574117660522461
Batch 36/64 loss: -0.44225120544433594
Batch 37/64 loss: -0.046634674072265625
Batch 38/64 loss: -0.46411705017089844
Batch 39/64 loss: -0.37578296661376953
Batch 40/64 loss: -0.3840508460998535
Batch 41/64 loss: -0.2620267868041992
Batch 42/64 loss: -0.5855131149291992
Batch 43/64 loss: -0.49814367294311523
Batch 44/64 loss: -0.3377542495727539
Batch 45/64 loss: -0.05038642883300781
Batch 46/64 loss: 0.10544395446777344
Batch 47/64 loss: 2.08681583404541
Batch 48/64 loss: -0.41465044021606445
Batch 49/64 loss: -0.452939510345459
Batch 50/64 loss: -0.37830448150634766
Batch 51/64 loss: -0.5013942718505859
Batch 52/64 loss: -0.04609870910644531
Batch 53/64 loss: 0.05886268615722656
Batch 54/64 loss: -0.4159245491027832
Batch 55/64 loss: -0.35030269622802734
Batch 56/64 loss: 0.19894981384277344
Batch 57/64 loss: -0.37355852127075195
Batch 58/64 loss: -0.5556521415710449
Batch 59/64 loss: -0.239715576171875
Batch 60/64 loss: -0.2775235176086426
Batch 61/64 loss: -0.517423152923584
Batch 62/64 loss: -0.39112424850463867
Batch 63/64 loss: -0.2749490737915039
Batch 64/64 loss: -3.0296945571899414
Epoch 199  Train loss: -0.2758410248101926  Val loss: -0.47457729745976296
Epoch 200
-------------------------------
Batch 1/64 loss: -0.3937187194824219
Batch 2/64 loss: -0.3010597229003906
Batch 3/64 loss: -0.4630274772644043
Batch 4/64 loss: -0.36305761337280273
Batch 5/64 loss: 0.5746321678161621
Batch 6/64 loss: -0.39215517044067383
Batch 7/64 loss: -0.14576005935668945
Batch 8/64 loss: -0.21100902557373047
Batch 9/64 loss: -0.41933536529541016
Batch 10/64 loss: 0.04737234115600586
Batch 11/64 loss: -0.3580303192138672
Batch 12/64 loss: -0.4234433174133301
Batch 13/64 loss: -0.1759357452392578
Batch 14/64 loss: -0.20228052139282227
Batch 15/64 loss: -0.07201242446899414
Batch 16/64 loss: 0.16227245330810547
Batch 17/64 loss: -0.3481717109680176
Batch 18/64 loss: 0.7700085639953613
Batch 19/64 loss: -0.2751932144165039
Batch 20/64 loss: -0.3379549980163574
Batch 21/64 loss: 0.31845712661743164
Batch 22/64 loss: -0.07153797149658203
Batch 23/64 loss: -0.4594740867614746
Batch 24/64 loss: -0.18850135803222656
Batch 25/64 loss: -0.19371700286865234
Batch 26/64 loss: -0.44290685653686523
Batch 27/64 loss: -0.10612010955810547
Batch 28/64 loss: -0.5418701171875
Batch 29/64 loss: -0.18382883071899414
Batch 30/64 loss: 1.4941987991333008
Batch 31/64 loss: -0.029042720794677734
Batch 32/64 loss: -0.4701542854309082
Batch 33/64 loss: -0.39353275299072266
Batch 34/64 loss: -0.047899723052978516
Batch 35/64 loss: -0.015399456024169922
Batch 36/64 loss: 1.2377171516418457
Batch 37/64 loss: 0.1851511001586914
Batch 38/64 loss: -0.2351822853088379
Batch 39/64 loss: 0.2189803123474121
Batch 40/64 loss: -0.4413108825683594
Batch 41/64 loss: -0.1262965202331543
Batch 42/64 loss: -0.45186281204223633
Batch 43/64 loss: -0.4395132064819336
Batch 44/64 loss: -0.4913454055786133
Batch 45/64 loss: -0.20788955688476562
Batch 46/64 loss: -0.2877473831176758
Batch 47/64 loss: -0.3547220230102539
Batch 48/64 loss: -0.42532968521118164
Batch 49/64 loss: -0.3677983283996582
Batch 50/64 loss: -0.3566093444824219
Batch 51/64 loss: -0.2864570617675781
Batch 52/64 loss: -0.48668861389160156
Batch 53/64 loss: -0.3484616279602051
Batch 54/64 loss: -0.45235300064086914
Batch 55/64 loss: -0.40085744857788086
Batch 56/64 loss: -0.24177074432373047
Batch 57/64 loss: -0.18302297592163086
Batch 58/64 loss: -0.23607349395751953
Batch 59/64 loss: -0.22740745544433594
Batch 60/64 loss: -0.3333559036254883
Batch 61/64 loss: -0.44046878814697266
Batch 62/64 loss: -0.32497262954711914
Batch 63/64 loss: -0.2829585075378418
Batch 64/64 loss: -3.7126665115356445
Epoch 200  Train loss: -0.22325170554366766  Val loss: -0.6121299848523746
Epoch 201
-------------------------------
Batch 1/64 loss: -0.3388514518737793
Batch 2/64 loss: -0.40299367904663086
Batch 3/64 loss: 0.196685791015625
Batch 4/64 loss: -0.21156549453735352
Batch 5/64 loss: -0.0066509246826171875
Batch 6/64 loss: -0.5329203605651855
Batch 7/64 loss: -0.40624046325683594
Batch 8/64 loss: -0.46465444564819336
Batch 9/64 loss: -0.24480009078979492
Batch 10/64 loss: -0.32371091842651367
Batch 11/64 loss: -0.1839437484741211
Batch 12/64 loss: -0.1579604148864746
Batch 13/64 loss: -0.27331018447875977
Batch 14/64 loss: 0.8504929542541504
Batch 15/64 loss: -0.30226850509643555
Batch 16/64 loss: -0.23890256881713867
Batch 17/64 loss: 0.4921722412109375
Batch 18/64 loss: -0.4797353744506836
Batch 19/64 loss: -0.2666487693786621
Batch 20/64 loss: -0.3316183090209961
Batch 21/64 loss: 1.2236151695251465
Batch 22/64 loss: -0.25208425521850586
Batch 23/64 loss: 0.40967893600463867
Batch 24/64 loss: -0.17830228805541992
Batch 25/64 loss: -0.08743143081665039
Batch 26/64 loss: -0.32279253005981445
Batch 27/64 loss: -0.3876042366027832
Batch 28/64 loss: 0.23294448852539062
Batch 29/64 loss: -0.04700946807861328
Batch 30/64 loss: 0.4048042297363281
Batch 31/64 loss: -0.47324514389038086
Batch 32/64 loss: 0.033016204833984375
Batch 33/64 loss: -0.32556676864624023
Batch 34/64 loss: -0.45061826705932617
Batch 35/64 loss: -0.49285221099853516
Batch 36/64 loss: -0.42948150634765625
Batch 37/64 loss: -0.4156761169433594
Batch 38/64 loss: 0.18620014190673828
Batch 39/64 loss: -0.5068998336791992
Batch 40/64 loss: -0.3683042526245117
Batch 41/64 loss: -0.3044242858886719
Batch 42/64 loss: -0.4593539237976074
Batch 43/64 loss: -0.4745936393737793
Batch 44/64 loss: -0.16387462615966797
Batch 45/64 loss: -0.21526336669921875
Batch 46/64 loss: -0.1366419792175293
Batch 47/64 loss: -0.3944225311279297
Batch 48/64 loss: -0.24209213256835938
Batch 49/64 loss: 0.1598949432373047
Batch 50/64 loss: -0.031487464904785156
Batch 51/64 loss: -0.22025394439697266
Batch 52/64 loss: -0.0055389404296875
Batch 53/64 loss: -0.40110063552856445
Batch 54/64 loss: -0.30440187454223633
Batch 55/64 loss: -0.35822391510009766
Batch 56/64 loss: -0.464536190032959
Batch 57/64 loss: -0.5278792381286621
Batch 58/64 loss: -0.25905561447143555
Batch 59/64 loss: -0.35277795791625977
Batch 60/64 loss: 0.008412361145019531
Batch 61/64 loss: -0.09453487396240234
Batch 62/64 loss: -0.418271541595459
Batch 63/64 loss: -0.47206687927246094
Batch 64/64 loss: -3.8080320358276367
Epoch 201  Train loss: -0.23315366483202168  Val loss: -0.6597159343077145
Epoch 202
-------------------------------
Batch 1/64 loss: -0.5253071784973145
Batch 2/64 loss: -0.3384251594543457
Batch 3/64 loss: -0.4196324348449707
Batch 4/64 loss: -0.46457529067993164
Batch 5/64 loss: -0.45171165466308594
Batch 6/64 loss: -0.2959251403808594
Batch 7/64 loss: -0.003822803497314453
Batch 8/64 loss: -0.31197071075439453
Batch 9/64 loss: -0.3854494094848633
Batch 10/64 loss: -0.47444581985473633
Batch 11/64 loss: -0.5809817314147949
Batch 12/64 loss: -0.5486364364624023
Batch 13/64 loss: -0.28948974609375
Batch 14/64 loss: -0.5768551826477051
Batch 15/64 loss: -0.5620207786560059
Batch 16/64 loss: -0.24306297302246094
Batch 17/64 loss: -0.35152387619018555
Batch 18/64 loss: -0.22117090225219727
Batch 19/64 loss: -0.5953140258789062
Batch 20/64 loss: -0.5028743743896484
Batch 21/64 loss: 0.7787861824035645
Batch 22/64 loss: -0.5483188629150391
Batch 23/64 loss: -0.4618358612060547
Batch 24/64 loss: -0.05645751953125
Batch 25/64 loss: -0.4125404357910156
Batch 26/64 loss: -0.43866920471191406
Batch 27/64 loss: -0.07641267776489258
Batch 28/64 loss: -0.2639484405517578
Batch 29/64 loss: -0.35080480575561523
Batch 30/64 loss: -0.3825550079345703
Batch 31/64 loss: -0.5792970657348633
Batch 32/64 loss: -0.35994958877563477
Batch 33/64 loss: -0.6857633590698242
Batch 34/64 loss: -0.42484045028686523
Batch 35/64 loss: -0.613673210144043
Batch 36/64 loss: -0.46634626388549805
Batch 37/64 loss: -0.41749048233032227
Batch 38/64 loss: -0.4810061454772949
Batch 39/64 loss: -0.39091968536376953
Batch 40/64 loss: 0.09913063049316406
Batch 41/64 loss: 1.29380464553833
Batch 42/64 loss: -0.42234182357788086
Batch 43/64 loss: -0.3820457458496094
Batch 44/64 loss: -0.5914216041564941
Batch 45/64 loss: -0.32932186126708984
Batch 46/64 loss: -0.14255094528198242
Batch 47/64 loss: -0.1801900863647461
Batch 48/64 loss: -0.1452021598815918
Batch 49/64 loss: -0.3435068130493164
Batch 50/64 loss: 0.733543872833252
Batch 51/64 loss: 0.09126806259155273
Batch 52/64 loss: 0.012396812438964844
Batch 53/64 loss: -0.3089113235473633
Batch 54/64 loss: -0.12974071502685547
Batch 55/64 loss: -0.1916041374206543
Batch 56/64 loss: -0.27680063247680664
Batch 57/64 loss: -0.39782285690307617
Batch 58/64 loss: -0.28724098205566406
Batch 59/64 loss: -0.14899635314941406
Batch 60/64 loss: -0.3666238784790039
Batch 61/64 loss: -0.32251739501953125
Batch 62/64 loss: -0.43918561935424805
Batch 63/64 loss: -0.3722410202026367
Batch 64/64 loss: -3.9269800186157227
Epoch 202  Train loss: -0.33362502677767886  Val loss: -0.6185621281260067
Epoch 203
-------------------------------
Batch 1/64 loss: -0.48447418212890625
Batch 2/64 loss: 0.00244140625
Batch 3/64 loss: -0.36082887649536133
Batch 4/64 loss: 0.06788158416748047
Batch 5/64 loss: -0.43461179733276367
Batch 6/64 loss: -0.2436990737915039
Batch 7/64 loss: -0.1271228790283203
Batch 8/64 loss: -0.3500485420227051
Batch 9/64 loss: -0.31108570098876953
Batch 10/64 loss: -0.45273733139038086
Batch 11/64 loss: -0.2640514373779297
Batch 12/64 loss: -0.4600796699523926
Batch 13/64 loss: -0.43009424209594727
Batch 14/64 loss: -0.13551855087280273
Batch 15/64 loss: -0.5693459510803223
Batch 16/64 loss: 0.6121392250061035
Batch 17/64 loss: -0.3514842987060547
Batch 18/64 loss: -0.5085663795471191
Batch 19/64 loss: -0.4747443199157715
Batch 20/64 loss: -0.46140623092651367
Batch 21/64 loss: -0.4253396987915039
Batch 22/64 loss: -0.49260854721069336
Batch 23/64 loss: 0.04795408248901367
Batch 24/64 loss: -0.4831972122192383
Batch 25/64 loss: -0.3162217140197754
Batch 26/64 loss: -0.28654956817626953
Batch 27/64 loss: -0.45230865478515625
Batch 28/64 loss: -0.46527910232543945
Batch 29/64 loss: -0.01077890396118164
Batch 30/64 loss: -0.39252328872680664
Batch 31/64 loss: -0.36540985107421875
Batch 32/64 loss: -0.5039777755737305
Batch 33/64 loss: -0.6155343055725098
Batch 34/64 loss: -0.37062692642211914
Batch 35/64 loss: -0.33667802810668945
Batch 36/64 loss: -0.33379697799682617
Batch 37/64 loss: -0.45494890213012695
Batch 38/64 loss: -0.38705921173095703
Batch 39/64 loss: 1.3476924896240234
Batch 40/64 loss: -0.5158977508544922
Batch 41/64 loss: -0.14493656158447266
Batch 42/64 loss: -0.4666905403137207
Batch 43/64 loss: -0.5628600120544434
Batch 44/64 loss: -0.3318514823913574
Batch 45/64 loss: -0.4697294235229492
Batch 46/64 loss: 0.8505954742431641
Batch 47/64 loss: -0.5872650146484375
Batch 48/64 loss: -0.5572991371154785
Batch 49/64 loss: -0.5517120361328125
Batch 50/64 loss: -0.4353013038635254
Batch 51/64 loss: -0.3990631103515625
Batch 52/64 loss: -0.5498294830322266
Batch 53/64 loss: -0.5503120422363281
Batch 54/64 loss: -0.37668895721435547
Batch 55/64 loss: -0.5016880035400391
Batch 56/64 loss: -0.25684309005737305
Batch 57/64 loss: -0.3835325241088867
Batch 58/64 loss: -0.4139094352722168
Batch 59/64 loss: -0.28366851806640625
Batch 60/64 loss: -0.45212841033935547
Batch 61/64 loss: -0.5077290534973145
Batch 62/64 loss: -0.3004722595214844
Batch 63/64 loss: -0.6217327117919922
Batch 64/64 loss: -3.5869765281677246
Epoch 203  Train loss: -0.3622808961307301  Val loss: -0.7382083250484925
Epoch 204
-------------------------------
Batch 1/64 loss: 1.3127679824829102
Batch 2/64 loss: -0.4673342704772949
Batch 3/64 loss: 1.0013113021850586
Batch 4/64 loss: -0.5631842613220215
Batch 5/64 loss: -0.5801010131835938
Batch 6/64 loss: -0.5610318183898926
Batch 7/64 loss: -0.05542421340942383
Batch 8/64 loss: -0.6319818496704102
Batch 9/64 loss: -0.6017627716064453
Batch 10/64 loss: -0.49848365783691406
Batch 11/64 loss: -0.4782681465148926
Batch 12/64 loss: -0.5690546035766602
Batch 13/64 loss: -0.4381241798400879
Batch 14/64 loss: -0.5714511871337891
Batch 15/64 loss: -0.4808931350708008
Batch 16/64 loss: -0.4603919982910156
Batch 17/64 loss: -0.4960498809814453
Batch 18/64 loss: -0.598271369934082
Batch 19/64 loss: -0.18367481231689453
Batch 20/64 loss: -0.6428394317626953
Batch 21/64 loss: -0.5958199501037598
Batch 22/64 loss: -0.5273675918579102
Batch 23/64 loss: -0.4771766662597656
Batch 24/64 loss: -0.5690927505493164
Batch 25/64 loss: -0.4795970916748047
Batch 26/64 loss: 0.19315052032470703
Batch 27/64 loss: -0.4235820770263672
Batch 28/64 loss: -0.2159733772277832
Batch 29/64 loss: -0.4593653678894043
Batch 30/64 loss: -0.19527769088745117
Batch 31/64 loss: -0.020721912384033203
Batch 32/64 loss: -0.5112652778625488
Batch 33/64 loss: -0.3706188201904297
Batch 34/64 loss: -0.31542158126831055
Batch 35/64 loss: -0.6394762992858887
Batch 36/64 loss: -0.49092960357666016
Batch 37/64 loss: -0.20345783233642578
Batch 38/64 loss: 0.01605701446533203
Batch 39/64 loss: -0.5039253234863281
Batch 40/64 loss: 0.21347951889038086
Batch 41/64 loss: -0.48699331283569336
Batch 42/64 loss: -0.200836181640625
Batch 43/64 loss: -0.42566919326782227
Batch 44/64 loss: -0.284970760345459
Batch 45/64 loss: -0.46109724044799805
Batch 46/64 loss: 0.7523126602172852
Batch 47/64 loss: -0.4781632423400879
Batch 48/64 loss: -0.16181421279907227
Batch 49/64 loss: -0.4341702461242676
Batch 50/64 loss: -0.3638572692871094
Batch 51/64 loss: -0.1287689208984375
Batch 52/64 loss: -0.40832948684692383
Batch 53/64 loss: -0.4716458320617676
Batch 54/64 loss: -0.45250463485717773
Batch 55/64 loss: -0.6072096824645996
Batch 56/64 loss: 0.05724143981933594
Batch 57/64 loss: -0.4246664047241211
Batch 58/64 loss: -0.5423922538757324
Batch 59/64 loss: -0.34307861328125
Batch 60/64 loss: -0.17627382278442383
Batch 61/64 loss: -0.35486650466918945
Batch 62/64 loss: -0.3787541389465332
Batch 63/64 loss: -0.5983791351318359
Batch 64/64 loss: -3.932645320892334
Epoch 204  Train loss: -0.36807837579764574  Val loss: -0.7025935248410988
Epoch 205
-------------------------------
Batch 1/64 loss: -0.4784693717956543
Batch 2/64 loss: -0.552548885345459
Batch 3/64 loss: -0.5140247344970703
Batch 4/64 loss: -0.48160409927368164
Batch 5/64 loss: -0.4409661293029785
Batch 6/64 loss: -0.4530296325683594
Batch 7/64 loss: -0.19701242446899414
Batch 8/64 loss: -0.5130467414855957
Batch 9/64 loss: -0.4361739158630371
Batch 10/64 loss: -0.5342135429382324
Batch 11/64 loss: -0.2068767547607422
Batch 12/64 loss: 0.2623414993286133
Batch 13/64 loss: 1.6233510971069336
Batch 14/64 loss: -0.6126885414123535
Batch 15/64 loss: 0.532707691192627
Batch 16/64 loss: -0.29328489303588867
Batch 17/64 loss: -0.5253219604492188
Batch 18/64 loss: 0.25194692611694336
Batch 19/64 loss: -0.14928913116455078
Batch 20/64 loss: -0.5164699554443359
Batch 21/64 loss: -0.4170050621032715
Batch 22/64 loss: -0.35888051986694336
Batch 23/64 loss: -0.547478199005127
Batch 24/64 loss: -0.5714154243469238
Batch 25/64 loss: -0.5681095123291016
Batch 26/64 loss: -0.5839629173278809
Batch 27/64 loss: -0.1750035285949707
Batch 28/64 loss: -0.47541332244873047
Batch 29/64 loss: -0.48224449157714844
Batch 30/64 loss: -0.2967958450317383
Batch 31/64 loss: -0.22288179397583008
Batch 32/64 loss: -0.5074305534362793
Batch 33/64 loss: -0.16820716857910156
Batch 34/64 loss: -0.07854127883911133
Batch 35/64 loss: -0.4344191551208496
Batch 36/64 loss: 0.055562496185302734
Batch 37/64 loss: -0.4989185333251953
Batch 38/64 loss: -0.5302615165710449
Batch 39/64 loss: -0.21411895751953125
Batch 40/64 loss: -0.4362201690673828
Batch 41/64 loss: -0.48357057571411133
Batch 42/64 loss: 0.6842069625854492
Batch 43/64 loss: -0.5378537178039551
Batch 44/64 loss: -0.5502586364746094
Batch 45/64 loss: -0.21177196502685547
Batch 46/64 loss: -0.5191407203674316
Batch 47/64 loss: -0.5623807907104492
Batch 48/64 loss: -0.5487160682678223
Batch 49/64 loss: -0.4981656074523926
Batch 50/64 loss: -0.5203242301940918
Batch 51/64 loss: -0.4915351867675781
Batch 52/64 loss: 0.01965045928955078
Batch 53/64 loss: -0.3908653259277344
Batch 54/64 loss: -0.4058103561401367
Batch 55/64 loss: -0.41989994049072266
Batch 56/64 loss: -0.33396291732788086
Batch 57/64 loss: -0.3985013961791992
Batch 58/64 loss: -0.3563523292541504
Batch 59/64 loss: -0.3388228416442871
Batch 60/64 loss: -0.2928905487060547
Batch 61/64 loss: -0.5222949981689453
Batch 62/64 loss: -0.33648252487182617
Batch 63/64 loss: -0.40861940383911133
Batch 64/64 loss: -3.783247947692871
Epoch 205  Train loss: -0.3609132168339748  Val loss: -0.6689422843382531
Epoch 206
-------------------------------
Batch 1/64 loss: -0.44719696044921875
Batch 2/64 loss: -0.3200087547302246
Batch 3/64 loss: -0.1596541404724121
Batch 4/64 loss: -0.3851022720336914
Batch 5/64 loss: -0.33540773391723633
Batch 6/64 loss: -0.36346435546875
Batch 7/64 loss: -0.2591266632080078
Batch 8/64 loss: -0.617487907409668
Batch 9/64 loss: -0.28714942932128906
Batch 10/64 loss: 0.7769136428833008
Batch 11/64 loss: -0.43152570724487305
Batch 12/64 loss: -0.5258779525756836
Batch 13/64 loss: -0.45603513717651367
Batch 14/64 loss: -0.4263763427734375
Batch 15/64 loss: 0.2374587059020996
Batch 16/64 loss: -0.4419560432434082
Batch 17/64 loss: -0.360809326171875
Batch 18/64 loss: -0.3905477523803711
Batch 19/64 loss: -0.5148730278015137
Batch 20/64 loss: -0.48029041290283203
Batch 21/64 loss: -0.5127825736999512
Batch 22/64 loss: -0.06316280364990234
Batch 23/64 loss: -0.47801780700683594
Batch 24/64 loss: -0.4363265037536621
Batch 25/64 loss: -0.4078636169433594
Batch 26/64 loss: 0.25519275665283203
Batch 27/64 loss: -0.4491844177246094
Batch 28/64 loss: -0.4539351463317871
Batch 29/64 loss: -0.05752754211425781
Batch 30/64 loss: -0.44165897369384766
Batch 31/64 loss: 0.6339411735534668
Batch 32/64 loss: -0.3395824432373047
Batch 33/64 loss: 1.1785879135131836
Batch 34/64 loss: -0.34174299240112305
Batch 35/64 loss: -0.491945743560791
Batch 36/64 loss: -0.32820653915405273
Batch 37/64 loss: -0.3815627098083496
Batch 38/64 loss: -0.37442970275878906
Batch 39/64 loss: -0.40767860412597656
Batch 40/64 loss: -0.47339630126953125
Batch 41/64 loss: -0.3733072280883789
Batch 42/64 loss: -0.48274850845336914
Batch 43/64 loss: -0.36736297607421875
Batch 44/64 loss: -0.4560055732727051
Batch 45/64 loss: -0.4631028175354004
Batch 46/64 loss: -0.14339685440063477
Batch 47/64 loss: -0.4079451560974121
Batch 48/64 loss: -0.22399616241455078
Batch 49/64 loss: -0.5662426948547363
Batch 50/64 loss: 0.8125715255737305
Batch 51/64 loss: -0.14920949935913086
Batch 52/64 loss: -0.18863248825073242
Batch 53/64 loss: -0.39793968200683594
Batch 54/64 loss: -0.5668721199035645
Batch 55/64 loss: -0.14281845092773438
Batch 56/64 loss: -0.33750438690185547
Batch 57/64 loss: -0.3660721778869629
Batch 58/64 loss: -0.34502077102661133
Batch 59/64 loss: -0.5695104598999023
Batch 60/64 loss: -0.429779052734375
Batch 61/64 loss: 0.031050682067871094
Batch 62/64 loss: -0.3333892822265625
Batch 63/64 loss: -0.3624696731567383
Batch 64/64 loss: -3.8555173873901367
Epoch 206  Train loss: -0.3181355607275869  Val loss: -0.6419041820408142
Epoch 207
-------------------------------
Batch 1/64 loss: -0.2145838737487793
Batch 2/64 loss: -0.08762788772583008
Batch 3/64 loss: -0.15532541275024414
Batch 4/64 loss: -0.5476937294006348
Batch 5/64 loss: 0.05729055404663086
Batch 6/64 loss: -0.45526790618896484
Batch 7/64 loss: -0.43691158294677734
Batch 8/64 loss: -0.27231359481811523
Batch 9/64 loss: -0.5745058059692383
Batch 10/64 loss: -0.513338565826416
Batch 11/64 loss: -0.2826828956604004
Batch 12/64 loss: 1.0134639739990234
Batch 13/64 loss: -0.5170207023620605
Batch 14/64 loss: 0.04939842224121094
Batch 15/64 loss: -0.46526622772216797
Batch 16/64 loss: -0.046773433685302734
Batch 17/64 loss: 0.10550737380981445
Batch 18/64 loss: -0.32898759841918945
Batch 19/64 loss: -0.3129897117614746
Batch 20/64 loss: -0.5277891159057617
Batch 21/64 loss: -0.5550260543823242
Batch 22/64 loss: -0.20281457901000977
Batch 23/64 loss: -0.2835526466369629
Batch 24/64 loss: 0.8034248352050781
Batch 25/64 loss: -0.21308612823486328
Batch 26/64 loss: 0.1139674186706543
Batch 27/64 loss: -0.21702194213867188
Batch 28/64 loss: -0.40447235107421875
Batch 29/64 loss: -0.1593775749206543
Batch 30/64 loss: -0.19408798217773438
Batch 31/64 loss: -0.4112515449523926
Batch 32/64 loss: -0.4513282775878906
Batch 33/64 loss: -0.07479715347290039
Batch 34/64 loss: -0.05766153335571289
Batch 35/64 loss: -0.3200058937072754
Batch 36/64 loss: -0.36565256118774414
Batch 37/64 loss: -0.47097063064575195
Batch 38/64 loss: 0.47588586807250977
Batch 39/64 loss: -0.1874384880065918
Batch 40/64 loss: -0.44254064559936523
Batch 41/64 loss: -0.5390410423278809
Batch 42/64 loss: -0.48282670974731445
Batch 43/64 loss: -0.3953261375427246
Batch 44/64 loss: -0.6065759658813477
Batch 45/64 loss: -0.496004581451416
Batch 46/64 loss: -0.38322877883911133
Batch 47/64 loss: -0.15323495864868164
Batch 48/64 loss: -0.6263728141784668
Batch 49/64 loss: -0.46410274505615234
Batch 50/64 loss: -0.3662095069885254
Batch 51/64 loss: -0.522362232208252
Batch 52/64 loss: -0.3692312240600586
Batch 53/64 loss: -0.534663200378418
Batch 54/64 loss: -0.24586153030395508
Batch 55/64 loss: -0.5204401016235352
Batch 56/64 loss: -0.2621479034423828
Batch 57/64 loss: -0.4808344841003418
Batch 58/64 loss: -0.30291271209716797
Batch 59/64 loss: -0.2506537437438965
Batch 60/64 loss: -0.5367627143859863
Batch 61/64 loss: -0.4202127456665039
Batch 62/64 loss: -0.4995245933532715
Batch 63/64 loss: -0.33953237533569336
Batch 64/64 loss: -3.936849594116211
Epoch 207  Train loss: -0.32755961698644304  Val loss: -0.6942492350679902
Epoch 208
-------------------------------
Batch 1/64 loss: -0.14959144592285156
Batch 2/64 loss: -0.3622612953186035
Batch 3/64 loss: -0.47118377685546875
Batch 4/64 loss: -0.48969030380249023
Batch 5/64 loss: -0.31715965270996094
Batch 6/64 loss: -0.0484013557434082
Batch 7/64 loss: 0.09749364852905273
Batch 8/64 loss: -0.4559006690979004
Batch 9/64 loss: -0.11593818664550781
Batch 10/64 loss: -0.44654130935668945
Batch 11/64 loss: -0.420654296875
Batch 12/64 loss: -0.45082855224609375
Batch 13/64 loss: -0.3635845184326172
Batch 14/64 loss: -0.16759777069091797
Batch 15/64 loss: -0.5595512390136719
Batch 16/64 loss: -0.5573611259460449
Batch 17/64 loss: -0.37648773193359375
Batch 18/64 loss: -0.377439022064209
Batch 19/64 loss: 0.6670246124267578
Batch 20/64 loss: 0.20381450653076172
Batch 21/64 loss: 0.04901838302612305
Batch 22/64 loss: -0.12116432189941406
Batch 23/64 loss: -0.4169173240661621
Batch 24/64 loss: 1.5017609596252441
Batch 25/64 loss: -0.35692596435546875
Batch 26/64 loss: -0.3555283546447754
Batch 27/64 loss: -0.2791709899902344
Batch 28/64 loss: -0.41026973724365234
Batch 29/64 loss: 0.050812721252441406
Batch 30/64 loss: -0.10549449920654297
Batch 31/64 loss: -0.11323690414428711
Batch 32/64 loss: -0.3374361991882324
Batch 33/64 loss: 0.06963729858398438
Batch 34/64 loss: -0.40485572814941406
Batch 35/64 loss: -0.3406362533569336
Batch 36/64 loss: -0.4793429374694824
Batch 37/64 loss: 0.2780451774597168
Batch 38/64 loss: -0.5011115074157715
Batch 39/64 loss: -0.3105049133300781
Batch 40/64 loss: -0.30019712448120117
Batch 41/64 loss: -0.07382726669311523
Batch 42/64 loss: -0.40883684158325195
Batch 43/64 loss: -0.38684558868408203
Batch 44/64 loss: -0.4965085983276367
Batch 45/64 loss: -0.26941490173339844
Batch 46/64 loss: -0.33634424209594727
Batch 47/64 loss: -0.45525169372558594
Batch 48/64 loss: -0.41501903533935547
Batch 49/64 loss: -0.20931148529052734
Batch 50/64 loss: -0.24401617050170898
Batch 51/64 loss: -0.28588104248046875
Batch 52/64 loss: 1.6664977073669434
Batch 53/64 loss: -0.4272909164428711
Batch 54/64 loss: -0.4053359031677246
Batch 55/64 loss: -0.36159563064575195
Batch 56/64 loss: -0.4692573547363281
Batch 57/64 loss: -0.27110815048217773
Batch 58/64 loss: -0.40448522567749023
Batch 59/64 loss: -0.4605865478515625
Batch 60/64 loss: -0.4925498962402344
Batch 61/64 loss: -0.4030938148498535
Batch 62/64 loss: -0.29935121536254883
Batch 63/64 loss: -0.5672707557678223
Batch 64/64 loss: -3.9917917251586914
Epoch 208  Train loss: -0.27475899715049595  Val loss: -0.5151682588242993
Epoch 209
-------------------------------
Batch 1/64 loss: 0.7125568389892578
Batch 2/64 loss: -0.5268783569335938
Batch 3/64 loss: -0.5162134170532227
Batch 4/64 loss: 0.06032228469848633
Batch 5/64 loss: -0.3772315979003906
Batch 6/64 loss: -0.6602687835693359
Batch 7/64 loss: -0.12332630157470703
Batch 8/64 loss: 0.37235593795776367
Batch 9/64 loss: -0.47358226776123047
Batch 10/64 loss: -0.4379234313964844
Batch 11/64 loss: -0.13105249404907227
Batch 12/64 loss: -0.41326141357421875
Batch 13/64 loss: -0.4810214042663574
Batch 14/64 loss: -0.2945737838745117
Batch 15/64 loss: 0.053379058837890625
Batch 16/64 loss: 1.280355453491211
Batch 17/64 loss: -0.3817176818847656
Batch 18/64 loss: -0.49984025955200195
Batch 19/64 loss: -0.4731292724609375
Batch 20/64 loss: -0.4517550468444824
Batch 21/64 loss: -0.46659088134765625
Batch 22/64 loss: -0.5438919067382812
Batch 23/64 loss: -0.3496432304382324
Batch 24/64 loss: -0.3662734031677246
Batch 25/64 loss: -0.2673988342285156
Batch 26/64 loss: -0.4863395690917969
Batch 27/64 loss: -0.3857569694519043
Batch 28/64 loss: -0.4104881286621094
Batch 29/64 loss: -0.41465234756469727
Batch 30/64 loss: -0.42304182052612305
Batch 31/64 loss: -0.1005105972290039
Batch 32/64 loss: -0.5467414855957031
Batch 33/64 loss: 0.024967670440673828
Batch 34/64 loss: -0.45716333389282227
Batch 35/64 loss: -0.015253543853759766
Batch 36/64 loss: -0.43387794494628906
Batch 37/64 loss: -0.44847774505615234
Batch 38/64 loss: -0.5243706703186035
Batch 39/64 loss: -0.4340634346008301
Batch 40/64 loss: -0.5308480262756348
Batch 41/64 loss: -0.5651278495788574
Batch 42/64 loss: 0.004895687103271484
Batch 43/64 loss: -0.356231689453125
Batch 44/64 loss: -0.2798638343811035
Batch 45/64 loss: -0.48818349838256836
Batch 46/64 loss: -0.5306539535522461
Batch 47/64 loss: -0.5962834358215332
Batch 48/64 loss: -0.4163994789123535
Batch 49/64 loss: -0.6026864051818848
Batch 50/64 loss: -0.5599770545959473
Batch 51/64 loss: -0.35773181915283203
Batch 52/64 loss: -0.45669984817504883
Batch 53/64 loss: -0.4105191230773926
Batch 54/64 loss: -0.4929628372192383
Batch 55/64 loss: -0.2661886215209961
Batch 56/64 loss: -0.39206838607788086
Batch 57/64 loss: 0.766636848449707
Batch 58/64 loss: -0.6041359901428223
Batch 59/64 loss: -0.39270448684692383
Batch 60/64 loss: -0.4637718200683594
Batch 61/64 loss: -0.4950857162475586
Batch 62/64 loss: -0.5512185096740723
Batch 63/64 loss: -0.1819462776184082
Batch 64/64 loss: -3.915170669555664
Epoch 209  Train loss: -0.36029032538918887  Val loss: -0.7332940773455957
Epoch 210
-------------------------------
Batch 1/64 loss: -0.47847414016723633
Batch 2/64 loss: 0.6328582763671875
Batch 3/64 loss: -0.6648473739624023
Batch 4/64 loss: -0.5895652770996094
Batch 5/64 loss: -0.2989811897277832
Batch 6/64 loss: 0.10843753814697266
Batch 7/64 loss: -0.3855900764465332
Batch 8/64 loss: -0.4807548522949219
Batch 9/64 loss: -0.5075783729553223
Batch 10/64 loss: -0.5379819869995117
Batch 11/64 loss: -0.45432090759277344
Batch 12/64 loss: -0.26889801025390625
Batch 13/64 loss: -0.5523204803466797
Batch 14/64 loss: -0.11047554016113281
Batch 15/64 loss: 1.0793509483337402
Batch 16/64 loss: -0.43947410583496094
Batch 17/64 loss: 0.41896963119506836
Batch 18/64 loss: -0.40288352966308594
Batch 19/64 loss: 0.5455541610717773
Batch 20/64 loss: -0.5300083160400391
Batch 21/64 loss: -0.5056214332580566
Batch 22/64 loss: -0.3986377716064453
Batch 23/64 loss: -0.09720659255981445
Batch 24/64 loss: -0.3176145553588867
Batch 25/64 loss: -0.3792724609375
Batch 26/64 loss: -0.30142688751220703
Batch 27/64 loss: -0.4702153205871582
Batch 28/64 loss: -0.2819538116455078
Batch 29/64 loss: -0.3956289291381836
Batch 30/64 loss: -0.24533891677856445
Batch 31/64 loss: -0.2665581703186035
Batch 32/64 loss: -0.5182967185974121
Batch 33/64 loss: -0.42321300506591797
Batch 34/64 loss: -0.058474063873291016
Batch 35/64 loss: -0.2730398178100586
Batch 36/64 loss: -0.12694883346557617
Batch 37/64 loss: -0.4247007369995117
Batch 38/64 loss: 0.35355186462402344
Batch 39/64 loss: -0.4274473190307617
Batch 40/64 loss: -0.16046714782714844
Batch 41/64 loss: -0.36519765853881836
Batch 42/64 loss: -0.03876686096191406
Batch 43/64 loss: -0.2638068199157715
Batch 44/64 loss: -0.3797917366027832
Batch 45/64 loss: -0.4118342399597168
Batch 46/64 loss: -0.33021068572998047
Batch 47/64 loss: -0.3918023109436035
Batch 48/64 loss: -0.4731578826904297
Batch 49/64 loss: -0.12571430206298828
Batch 50/64 loss: -0.46938371658325195
Batch 51/64 loss: -0.5253615379333496
Batch 52/64 loss: -0.5247817039489746
Batch 53/64 loss: -0.2978224754333496
Batch 54/64 loss: -0.4540519714355469
Batch 55/64 loss: -0.4123191833496094
Batch 56/64 loss: -0.023128509521484375
Batch 57/64 loss: 1.0494017601013184
Batch 58/64 loss: -0.4620237350463867
Batch 59/64 loss: -0.5481433868408203
Batch 60/64 loss: -0.3804206848144531
Batch 61/64 loss: -0.4330635070800781
Batch 62/64 loss: -0.3216547966003418
Batch 63/64 loss: -0.45592355728149414
Batch 64/64 loss: -4.079171657562256
Epoch 210  Train loss: -0.30955031338860006  Val loss: -0.6273413589320231
Epoch 211
-------------------------------
Batch 1/64 loss: 0.1279153823852539
Batch 2/64 loss: -0.4177837371826172
Batch 3/64 loss: -0.42915773391723633
Batch 4/64 loss: -0.5285606384277344
Batch 5/64 loss: -0.15236616134643555
Batch 6/64 loss: -0.4362492561340332
Batch 7/64 loss: 1.0537872314453125
Batch 8/64 loss: -0.3581414222717285
Batch 9/64 loss: -0.4416627883911133
Batch 10/64 loss: -0.3182950019836426
Batch 11/64 loss: -0.43650102615356445
Batch 12/64 loss: -0.4406852722167969
Batch 13/64 loss: -0.4809565544128418
Batch 14/64 loss: -0.5271530151367188
Batch 15/64 loss: -0.2811107635498047
Batch 16/64 loss: -0.4450860023498535
Batch 17/64 loss: -0.3657646179199219
Batch 18/64 loss: -0.47055959701538086
Batch 19/64 loss: -0.448822021484375
Batch 20/64 loss: -0.0495152473449707
Batch 21/64 loss: -0.38983964920043945
Batch 22/64 loss: -0.16005277633666992
Batch 23/64 loss: -0.31125736236572266
Batch 24/64 loss: -0.5001449584960938
Batch 25/64 loss: 0.03774404525756836
Batch 26/64 loss: -0.5199489593505859
Batch 27/64 loss: -0.3962211608886719
Batch 28/64 loss: -0.16428136825561523
Batch 29/64 loss: -0.4111142158508301
Batch 30/64 loss: -0.0884714126586914
Batch 31/64 loss: -0.35080528259277344
Batch 32/64 loss: -0.34070634841918945
Batch 33/64 loss: -0.3025975227355957
Batch 34/64 loss: -0.39842748641967773
Batch 35/64 loss: -0.29529714584350586
Batch 36/64 loss: -0.4076051712036133
Batch 37/64 loss: 0.4444761276245117
Batch 38/64 loss: -0.3870811462402344
Batch 39/64 loss: -0.4104771614074707
Batch 40/64 loss: -0.44178104400634766
Batch 41/64 loss: -0.23793363571166992
Batch 42/64 loss: -0.41040706634521484
Batch 43/64 loss: -0.34796571731567383
Batch 44/64 loss: -0.18822431564331055
Batch 45/64 loss: 0.7529315948486328
Batch 46/64 loss: -0.16203737258911133
Batch 47/64 loss: -0.34755420684814453
Batch 48/64 loss: -0.43359994888305664
Batch 49/64 loss: -0.25337839126586914
Batch 50/64 loss: 0.047583580017089844
Batch 51/64 loss: -0.12794065475463867
Batch 52/64 loss: -0.5145044326782227
Batch 53/64 loss: -0.5696902275085449
Batch 54/64 loss: -0.5169873237609863
Batch 55/64 loss: -0.5300817489624023
Batch 56/64 loss: -0.37075185775756836
Batch 57/64 loss: -0.43578386306762695
Batch 58/64 loss: -0.47515106201171875
Batch 59/64 loss: -0.6477255821228027
Batch 60/64 loss: -0.3592348098754883
Batch 61/64 loss: -0.2955198287963867
Batch 62/64 loss: -0.603421688079834
Batch 63/64 loss: -0.5415863990783691
Batch 64/64 loss: -3.9192776679992676
Epoch 211  Train loss: -0.34743500316844267  Val loss: -0.7148142680269746
Epoch 212
-------------------------------
Batch 1/64 loss: -0.4746732711791992
Batch 2/64 loss: -0.5234413146972656
Batch 3/64 loss: -0.4158201217651367
Batch 4/64 loss: -0.10943794250488281
Batch 5/64 loss: -0.41391897201538086
Batch 6/64 loss: -0.6105070114135742
Batch 7/64 loss: -0.526522159576416
Batch 8/64 loss: -0.667325496673584
Batch 9/64 loss: -0.34244585037231445
Batch 10/64 loss: -0.5475015640258789
Batch 11/64 loss: -0.5983147621154785
Batch 12/64 loss: -0.18618202209472656
Batch 13/64 loss: -0.5035886764526367
Batch 14/64 loss: -0.5209441184997559
Batch 15/64 loss: -0.5339274406433105
Batch 16/64 loss: -0.5990891456604004
Batch 17/64 loss: -0.5340867042541504
Batch 18/64 loss: -0.5393342971801758
Batch 19/64 loss: -0.6655788421630859
Batch 20/64 loss: 0.036852359771728516
Batch 21/64 loss: -0.1493358612060547
Batch 22/64 loss: -0.2045149803161621
Batch 23/64 loss: -0.708106517791748
Batch 24/64 loss: -0.5757951736450195
Batch 25/64 loss: -0.7012467384338379
Batch 26/64 loss: -0.22890424728393555
Batch 27/64 loss: 0.004939556121826172
Batch 28/64 loss: -0.5937137603759766
Batch 29/64 loss: -0.6943321228027344
Batch 30/64 loss: -0.5881357192993164
Batch 31/64 loss: -0.3781599998474121
Batch 32/64 loss: 0.851038932800293
Batch 33/64 loss: -0.6206216812133789
Batch 34/64 loss: 0.46963977813720703
Batch 35/64 loss: -0.5572628974914551
Batch 36/64 loss: -0.4107809066772461
Batch 37/64 loss: -0.6215801239013672
Batch 38/64 loss: -0.5272107124328613
Batch 39/64 loss: -0.5760736465454102
Batch 40/64 loss: -0.6716132164001465
Batch 41/64 loss: -0.5848050117492676
Batch 42/64 loss: -0.5246243476867676
Batch 43/64 loss: -0.46076297760009766
Batch 44/64 loss: -0.39127492904663086
Batch 45/64 loss: -0.5720109939575195
Batch 46/64 loss: -0.230743408203125
Batch 47/64 loss: -0.4507007598876953
Batch 48/64 loss: 0.03153038024902344
Batch 49/64 loss: -0.5217561721801758
Batch 50/64 loss: -0.3272523880004883
Batch 51/64 loss: -0.4446573257446289
Batch 52/64 loss: -0.5749578475952148
Batch 53/64 loss: -0.506950855255127
Batch 54/64 loss: -0.35651302337646484
Batch 55/64 loss: -0.23431777954101562
Batch 56/64 loss: -0.4711642265319824
Batch 57/64 loss: -0.4867877960205078
Batch 58/64 loss: -0.0012083053588867188
Batch 59/64 loss: -0.5528430938720703
Batch 60/64 loss: -0.4103693962097168
Batch 61/64 loss: -0.6186585426330566
Batch 62/64 loss: 0.8786263465881348
Batch 63/64 loss: -0.38690996170043945
Batch 64/64 loss: -3.982442855834961
Epoch 212  Train loss: -0.4383294423421224  Val loss: -0.7930657560473046
Epoch 213
-------------------------------
Batch 1/64 loss: -0.42707109451293945
Batch 2/64 loss: -0.6100554466247559
Batch 3/64 loss: 1.15836763381958
Batch 4/64 loss: -0.4142131805419922
Batch 5/64 loss: -0.49312591552734375
Batch 6/64 loss: -0.41142797470092773
Batch 7/64 loss: -0.40067148208618164
Batch 8/64 loss: -0.41481590270996094
Batch 9/64 loss: -0.42255353927612305
Batch 10/64 loss: -0.48709630966186523
Batch 11/64 loss: -0.6150779724121094
Batch 12/64 loss: 0.4915285110473633
Batch 13/64 loss: -0.2866086959838867
Batch 14/64 loss: -0.6170411109924316
Batch 15/64 loss: -0.506195068359375
Batch 16/64 loss: -0.4154090881347656
Batch 17/64 loss: -0.44304752349853516
Batch 18/64 loss: -0.34015417098999023
Batch 19/64 loss: -0.20339155197143555
Batch 20/64 loss: -0.4944267272949219
Batch 21/64 loss: -0.44361066818237305
Batch 22/64 loss: -0.047949790954589844
Batch 23/64 loss: -0.5476880073547363
Batch 24/64 loss: -0.6314206123352051
Batch 25/64 loss: -0.37493467330932617
Batch 26/64 loss: -0.5059428215026855
Batch 27/64 loss: -0.5806875228881836
Batch 28/64 loss: -0.5162696838378906
Batch 29/64 loss: -0.7245469093322754
Batch 30/64 loss: -0.7483730316162109
Batch 31/64 loss: 0.284574031829834
Batch 32/64 loss: -0.6262521743774414
Batch 33/64 loss: -0.36990928649902344
Batch 34/64 loss: -0.5466623306274414
Batch 35/64 loss: -0.6404104232788086
Batch 36/64 loss: -0.6526827812194824
Batch 37/64 loss: -0.6489596366882324
Batch 38/64 loss: -0.4026641845703125
Batch 39/64 loss: -0.6381468772888184
Batch 40/64 loss: -0.5227179527282715
Batch 41/64 loss: -0.6547384262084961
Batch 42/64 loss: -0.4922609329223633
Batch 43/64 loss: -0.6152324676513672
Batch 44/64 loss: 0.5317015647888184
Batch 45/64 loss: -0.47780466079711914
Batch 46/64 loss: -0.6486945152282715
Batch 47/64 loss: -0.5371065139770508
Batch 48/64 loss: -0.6992244720458984
Batch 49/64 loss: -0.636927604675293
Batch 50/64 loss: -0.3626389503479004
Batch 51/64 loss: -0.5388565063476562
Batch 52/64 loss: -0.4984745979309082
Batch 53/64 loss: -0.6857461929321289
Batch 54/64 loss: -0.5320267677307129
Batch 55/64 loss: -0.3314833641052246
Batch 56/64 loss: -0.7595572471618652
Batch 57/64 loss: -0.6054744720458984
Batch 58/64 loss: -0.6518502235412598
Batch 59/64 loss: -0.5226750373840332
Batch 60/64 loss: -0.4746685028076172
Batch 61/64 loss: -0.45381784439086914
Batch 62/64 loss: 0.43749380111694336
Batch 63/64 loss: -0.42295265197753906
Batch 64/64 loss: -4.050859451293945
Epoch 213  Train loss: -0.46915926465801167  Val loss: -0.7974693193468442
Epoch 214
-------------------------------
Batch 1/64 loss: -0.5040311813354492
Batch 2/64 loss: -0.5889158248901367
Batch 3/64 loss: 1.6666975021362305
Batch 4/64 loss: -0.06647014617919922
Batch 5/64 loss: -0.3993382453918457
Batch 6/64 loss: -0.3944215774536133
Batch 7/64 loss: -0.22554302215576172
Batch 8/64 loss: 0.07117509841918945
Batch 9/64 loss: -0.541135311126709
Batch 10/64 loss: -0.5137810707092285
Batch 11/64 loss: -0.28129100799560547
Batch 12/64 loss: -0.6180167198181152
Batch 13/64 loss: -0.39745616912841797
Batch 14/64 loss: -0.49758243560791016
Batch 15/64 loss: -0.3077869415283203
Batch 16/64 loss: -0.4173455238342285
Batch 17/64 loss: -0.3365049362182617
Batch 18/64 loss: -0.5231218338012695
Batch 19/64 loss: -0.572516918182373
Batch 20/64 loss: -0.5835871696472168
Batch 21/64 loss: 0.6243743896484375
Batch 22/64 loss: -0.4433879852294922
Batch 23/64 loss: -0.33379554748535156
Batch 24/64 loss: -0.19486236572265625
Batch 25/64 loss: -0.477081298828125
Batch 26/64 loss: -0.28566741943359375
Batch 27/64 loss: 0.7614202499389648
Batch 28/64 loss: 0.07018566131591797
Batch 29/64 loss: -0.4482860565185547
Batch 30/64 loss: -0.4639873504638672
Batch 31/64 loss: -0.5262317657470703
Batch 32/64 loss: -0.39168691635131836
Batch 33/64 loss: -0.3530116081237793
Batch 34/64 loss: -0.5847668647766113
Batch 35/64 loss: -0.5400371551513672
Batch 36/64 loss: -0.48821496963500977
Batch 37/64 loss: -0.5275163650512695
Batch 38/64 loss: -0.6136183738708496
Batch 39/64 loss: -0.5862865447998047
Batch 40/64 loss: -0.5550470352172852
Batch 41/64 loss: -0.5592575073242188
Batch 42/64 loss: -0.5800924301147461
Batch 43/64 loss: -0.6205496788024902
Batch 44/64 loss: -0.5378060340881348
Batch 45/64 loss: -0.42882585525512695
Batch 46/64 loss: -0.4916038513183594
Batch 47/64 loss: -0.44445276260375977
Batch 48/64 loss: -0.4031953811645508
Batch 49/64 loss: -0.5960574150085449
Batch 50/64 loss: -0.6482009887695312
Batch 51/64 loss: -0.3481559753417969
Batch 52/64 loss: -0.6240024566650391
Batch 53/64 loss: -0.5247673988342285
Batch 54/64 loss: -0.5911531448364258
Batch 55/64 loss: -0.4626622200012207
Batch 56/64 loss: -0.45980358123779297
Batch 57/64 loss: 0.06381368637084961
Batch 58/64 loss: -0.20459556579589844
Batch 59/64 loss: -0.6032719612121582
Batch 60/64 loss: -0.5216355323791504
Batch 61/64 loss: -0.5627322196960449
Batch 62/64 loss: -0.40407323837280273
Batch 63/64 loss: -0.37976884841918945
Batch 64/64 loss: -3.5814766883850098
Epoch 214  Train loss: -0.4079597903232948  Val loss: -0.7818996521205837
Epoch 215
-------------------------------
Batch 1/64 loss: -0.671870231628418
Batch 2/64 loss: -0.49038028717041016
Batch 3/64 loss: -0.4869861602783203
Batch 4/64 loss: -0.5428814888000488
Batch 5/64 loss: -0.5877566337585449
Batch 6/64 loss: -0.10359001159667969
Batch 7/64 loss: 0.950836181640625
Batch 8/64 loss: -0.47007036209106445
Batch 9/64 loss: -0.3158588409423828
Batch 10/64 loss: -0.2228074073791504
Batch 11/64 loss: -0.4645686149597168
Batch 12/64 loss: -0.5215187072753906
Batch 13/64 loss: 0.757267951965332
Batch 14/64 loss: 0.10541534423828125
Batch 15/64 loss: -0.5667786598205566
Batch 16/64 loss: -0.49291181564331055
Batch 17/64 loss: -0.20890569686889648
Batch 18/64 loss: -0.5509166717529297
Batch 19/64 loss: -0.37229442596435547
Batch 20/64 loss: -0.5610575675964355
Batch 21/64 loss: -0.42371654510498047
Batch 22/64 loss: -0.45453405380249023
Batch 23/64 loss: -0.4648733139038086
Batch 24/64 loss: 0.4463639259338379
Batch 25/64 loss: -0.5697855949401855
Batch 26/64 loss: -0.4241161346435547
Batch 27/64 loss: -0.4085373878479004
Batch 28/64 loss: -0.48014307022094727
Batch 29/64 loss: -0.5681047439575195
Batch 30/64 loss: -0.37792015075683594
Batch 31/64 loss: -0.5984687805175781
Batch 32/64 loss: -0.5033926963806152
Batch 33/64 loss: -0.6060662269592285
Batch 34/64 loss: -0.4726896286010742
Batch 35/64 loss: -0.3902430534362793
Batch 36/64 loss: -0.4123048782348633
Batch 37/64 loss: 0.008780479431152344
Batch 38/64 loss: -0.5868182182312012
Batch 39/64 loss: -0.5706686973571777
Batch 40/64 loss: -0.5658135414123535
Batch 41/64 loss: -0.506187915802002
Batch 42/64 loss: -0.584622859954834
Batch 43/64 loss: -0.6011319160461426
Batch 44/64 loss: -0.5201668739318848
Batch 45/64 loss: -0.45234203338623047
Batch 46/64 loss: -0.6330671310424805
Batch 47/64 loss: -0.08864498138427734
Batch 48/64 loss: -0.5242648124694824
Batch 49/64 loss: -0.6500792503356934
Batch 50/64 loss: -0.6633443832397461
Batch 51/64 loss: -0.40105295181274414
Batch 52/64 loss: -0.5185379981994629
Batch 53/64 loss: -0.6555557250976562
Batch 54/64 loss: -0.5306501388549805
Batch 55/64 loss: -0.37654590606689453
Batch 56/64 loss: -0.33112335205078125
Batch 57/64 loss: -0.5169410705566406
Batch 58/64 loss: -0.3842167854309082
Batch 59/64 loss: -0.16600656509399414
Batch 60/64 loss: -0.5328831672668457
Batch 61/64 loss: -0.5771160125732422
Batch 62/64 loss: 0.20012378692626953
Batch 63/64 loss: 0.08490896224975586
Batch 64/64 loss: -3.9905285835266113
Epoch 215  Train loss: -0.42608677546183266  Val loss: -0.7854235409871
Epoch 216
-------------------------------
Batch 1/64 loss: -0.6166234016418457
Batch 2/64 loss: -0.47809934616088867
Batch 3/64 loss: -0.4492192268371582
Batch 4/64 loss: -0.6636452674865723
Batch 5/64 loss: -0.4888172149658203
Batch 6/64 loss: 1.0453166961669922
Batch 7/64 loss: -0.5779614448547363
Batch 8/64 loss: -0.6264233589172363
Batch 9/64 loss: -0.5334153175354004
Batch 10/64 loss: -0.5411396026611328
Batch 11/64 loss: -0.5478348731994629
Batch 12/64 loss: -0.5234799385070801
Batch 13/64 loss: -0.27810096740722656
Batch 14/64 loss: -0.5099577903747559
Batch 15/64 loss: -0.24951457977294922
Batch 16/64 loss: -0.7086358070373535
Batch 17/64 loss: -0.5485124588012695
Batch 18/64 loss: -0.5750269889831543
Batch 19/64 loss: 0.2513580322265625
Batch 20/64 loss: 0.9514484405517578
Batch 21/64 loss: -0.607050895690918
Batch 22/64 loss: -0.6255664825439453
Batch 23/64 loss: -0.39603281021118164
Batch 24/64 loss: -0.5433063507080078
Batch 25/64 loss: -0.5296454429626465
Batch 26/64 loss: -0.6646056175231934
Batch 27/64 loss: -0.5577921867370605
Batch 28/64 loss: -0.5491180419921875
Batch 29/64 loss: -0.045853614807128906
Batch 30/64 loss: -0.18243646621704102
Batch 31/64 loss: -0.21405553817749023
Batch 32/64 loss: -0.621464729309082
Batch 33/64 loss: -0.28548574447631836
Batch 34/64 loss: -0.5474505424499512
Batch 35/64 loss: -0.5881915092468262
Batch 36/64 loss: -0.23249340057373047
Batch 37/64 loss: 0.09699392318725586
Batch 38/64 loss: -0.5209159851074219
Batch 39/64 loss: -0.5494585037231445
Batch 40/64 loss: -0.509516716003418
Batch 41/64 loss: -0.4633355140686035
Batch 42/64 loss: 0.7874879837036133
Batch 43/64 loss: -0.5271329879760742
Batch 44/64 loss: -0.006449699401855469
Batch 45/64 loss: -0.3273606300354004
Batch 46/64 loss: 0.21200323104858398
Batch 47/64 loss: -0.43126726150512695
Batch 48/64 loss: -0.4727663993835449
Batch 49/64 loss: -0.5423460006713867
Batch 50/64 loss: -0.5260806083679199
Batch 51/64 loss: -0.5407261848449707
Batch 52/64 loss: -0.39717864990234375
Batch 53/64 loss: -0.3818354606628418
Batch 54/64 loss: -0.22183609008789062
Batch 55/64 loss: -0.39591217041015625
Batch 56/64 loss: -0.2288808822631836
Batch 57/64 loss: -0.3737912178039551
Batch 58/64 loss: -0.6896076202392578
Batch 59/64 loss: -0.5403094291687012
Batch 60/64 loss: -0.2913055419921875
Batch 61/64 loss: -0.5885457992553711
Batch 62/64 loss: -0.0424504280090332
Batch 63/64 loss: -0.43389225006103516
Batch 64/64 loss: -4.050833702087402
Epoch 216  Train loss: -0.4047588385787665  Val loss: -0.8414668840231355
Saving best model, epoch: 216
Epoch 217
-------------------------------
Batch 1/64 loss: -0.26927757263183594
Batch 2/64 loss: -0.37148618698120117
Batch 3/64 loss: -0.46022748947143555
Batch 4/64 loss: -0.5978832244873047
Batch 5/64 loss: -0.5317258834838867
Batch 6/64 loss: -0.6986169815063477
Batch 7/64 loss: -0.3165621757507324
Batch 8/64 loss: -0.5665383338928223
Batch 9/64 loss: -0.6000585556030273
Batch 10/64 loss: -0.41181516647338867
Batch 11/64 loss: -0.32060861587524414
Batch 12/64 loss: -0.43050432205200195
Batch 13/64 loss: -0.44258642196655273
Batch 14/64 loss: -0.34120750427246094
Batch 15/64 loss: 1.4679155349731445
Batch 16/64 loss: -0.5036592483520508
Batch 17/64 loss: -0.5121874809265137
Batch 18/64 loss: -0.5086765289306641
Batch 19/64 loss: 0.3390622138977051
Batch 20/64 loss: -0.5280022621154785
Batch 21/64 loss: -0.5352234840393066
Batch 22/64 loss: -0.464749813079834
Batch 23/64 loss: -0.48758697509765625
Batch 24/64 loss: -0.2417130470275879
Batch 25/64 loss: -0.4358797073364258
Batch 26/64 loss: 0.7358946800231934
Batch 27/64 loss: -0.3991107940673828
Batch 28/64 loss: -0.3978404998779297
Batch 29/64 loss: -0.44783735275268555
Batch 30/64 loss: -0.5177578926086426
Batch 31/64 loss: -0.3824496269226074
Batch 32/64 loss: -0.34059858322143555
Batch 33/64 loss: -0.3189582824707031
Batch 34/64 loss: -0.5646533966064453
Batch 35/64 loss: 0.038481712341308594
Batch 36/64 loss: -0.25684118270874023
Batch 37/64 loss: -0.5062088966369629
Batch 38/64 loss: 1.0866703987121582
Batch 39/64 loss: -0.45356321334838867
Batch 40/64 loss: -0.17317962646484375
Batch 41/64 loss: -0.3484783172607422
Batch 42/64 loss: -0.05963850021362305
Batch 43/64 loss: 0.4744582176208496
Batch 44/64 loss: -0.48177576065063477
Batch 45/64 loss: -0.4013028144836426
Batch 46/64 loss: -0.5941705703735352
Batch 47/64 loss: 0.7469210624694824
Batch 48/64 loss: -0.5584797859191895
Batch 49/64 loss: -0.2863192558288574
Batch 50/64 loss: -0.49382448196411133
Batch 51/64 loss: -0.5680966377258301
Batch 52/64 loss: -0.4271278381347656
Batch 53/64 loss: -0.284576416015625
Batch 54/64 loss: -0.5840153694152832
Batch 55/64 loss: -0.4114108085632324
Batch 56/64 loss: -0.3747529983520508
Batch 57/64 loss: -0.4810009002685547
Batch 58/64 loss: -0.596827507019043
Batch 59/64 loss: -0.37680816650390625
Batch 60/64 loss: -0.5449471473693848
Batch 61/64 loss: -0.2422165870666504
Batch 62/64 loss: -0.605339527130127
Batch 63/64 loss: 0.08016633987426758
Batch 64/64 loss: -4.113772392272949
Epoch 217  Train loss: -0.347806193781834  Val loss: -0.8114219744180896
Epoch 218
-------------------------------
Batch 1/64 loss: -0.4501357078552246
Batch 2/64 loss: -0.5164008140563965
Batch 3/64 loss: -0.5266971588134766
Batch 4/64 loss: -0.3609800338745117
Batch 5/64 loss: -0.2904181480407715
Batch 6/64 loss: -0.391359806060791
Batch 7/64 loss: -0.4427790641784668
Batch 8/64 loss: -0.3950009346008301
Batch 9/64 loss: 0.6272993087768555
Batch 10/64 loss: -0.28725624084472656
Batch 11/64 loss: -0.4779505729675293
Batch 12/64 loss: -0.36385297775268555
Batch 13/64 loss: -0.20847845077514648
Batch 14/64 loss: -0.330629825592041
Batch 15/64 loss: -0.17478513717651367
Batch 16/64 loss: -0.009285926818847656
Batch 17/64 loss: -0.3898148536682129
Batch 18/64 loss: -0.38948535919189453
Batch 19/64 loss: -0.2913694381713867
Batch 20/64 loss: 0.6038408279418945
Batch 21/64 loss: -0.3397488594055176
Batch 22/64 loss: -0.5250287055969238
Batch 23/64 loss: -0.38759851455688477
Batch 24/64 loss: -0.4264564514160156
Batch 25/64 loss: -0.44991588592529297
Batch 26/64 loss: -0.5949106216430664
Batch 27/64 loss: -0.5057649612426758
Batch 28/64 loss: -0.4719734191894531
Batch 29/64 loss: -0.4842820167541504
Batch 30/64 loss: -0.27863645553588867
Batch 31/64 loss: -0.6148538589477539
Batch 32/64 loss: -0.5816197395324707
Batch 33/64 loss: -0.5594019889831543
Batch 34/64 loss: -0.5243511199951172
Batch 35/64 loss: -0.5052652359008789
Batch 36/64 loss: -0.23729181289672852
Batch 37/64 loss: -0.4750995635986328
Batch 38/64 loss: -0.5656776428222656
Batch 39/64 loss: 0.9019761085510254
Batch 40/64 loss: -0.4892892837524414
Batch 41/64 loss: -0.31691503524780273
Batch 42/64 loss: -0.3920736312866211
Batch 43/64 loss: -0.43866777420043945
Batch 44/64 loss: -0.48863792419433594
Batch 45/64 loss: 0.6378107070922852
Batch 46/64 loss: -0.459134578704834
Batch 47/64 loss: -0.30949831008911133
Batch 48/64 loss: -0.5688877105712891
Batch 49/64 loss: -0.37741565704345703
Batch 50/64 loss: -0.44832658767700195
Batch 51/64 loss: -0.5911064147949219
Batch 52/64 loss: -0.4785885810852051
Batch 53/64 loss: -0.42167186737060547
Batch 54/64 loss: 0.01855754852294922
Batch 55/64 loss: 1.0377545356750488
Batch 56/64 loss: -0.46615076065063477
Batch 57/64 loss: -0.40171051025390625
Batch 58/64 loss: -0.27657222747802734
Batch 59/64 loss: -0.5141739845275879
Batch 60/64 loss: -0.4268455505371094
Batch 61/64 loss: -0.2572040557861328
Batch 62/64 loss: -0.6071939468383789
Batch 63/64 loss: -0.3388986587524414
Batch 64/64 loss: -3.8869590759277344
Epoch 218  Train loss: -0.36049412746055454  Val loss: -0.7809887784453192
Epoch 219
-------------------------------
Batch 1/64 loss: -0.1600351333618164
Batch 2/64 loss: -0.14697742462158203
Batch 3/64 loss: -0.5860252380371094
Batch 4/64 loss: -0.4585747718811035
Batch 5/64 loss: -0.33327341079711914
Batch 6/64 loss: -0.3138446807861328
Batch 7/64 loss: -0.3773331642150879
Batch 8/64 loss: -0.5708227157592773
Batch 9/64 loss: -0.5184526443481445
Batch 10/64 loss: -0.4329805374145508
Batch 11/64 loss: -0.6121845245361328
Batch 12/64 loss: -0.47278547286987305
Batch 13/64 loss: 0.8772773742675781
Batch 14/64 loss: -0.30232810974121094
Batch 15/64 loss: -0.3390932083129883
Batch 16/64 loss: -0.3543672561645508
Batch 17/64 loss: -0.3116421699523926
Batch 18/64 loss: -0.3438849449157715
Batch 19/64 loss: -0.47483205795288086
Batch 20/64 loss: -0.4168267250061035
Batch 21/64 loss: -0.3521099090576172
Batch 22/64 loss: -0.02935171127319336
Batch 23/64 loss: -0.11563301086425781
Batch 24/64 loss: -0.48515748977661133
Batch 25/64 loss: -0.0465092658996582
Batch 26/64 loss: -0.4174370765686035
Batch 27/64 loss: -0.4022092819213867
Batch 28/64 loss: 0.014728546142578125
Batch 29/64 loss: -0.1886286735534668
Batch 30/64 loss: -0.08973550796508789
Batch 31/64 loss: -0.4202723503112793
Batch 32/64 loss: -0.41054534912109375
Batch 33/64 loss: -0.4294109344482422
Batch 34/64 loss: -0.40819263458251953
Batch 35/64 loss: -0.5199832916259766
Batch 36/64 loss: -0.5602788925170898
Batch 37/64 loss: -0.33698558807373047
Batch 38/64 loss: 0.007134437561035156
Batch 39/64 loss: -0.3533501625061035
Batch 40/64 loss: -0.3088984489440918
Batch 41/64 loss: -0.49486637115478516
Batch 42/64 loss: 0.24741411209106445
Batch 43/64 loss: -0.408754825592041
Batch 44/64 loss: -0.435244083404541
Batch 45/64 loss: 1.1345796585083008
Batch 46/64 loss: -0.27019500732421875
Batch 47/64 loss: -0.24743127822875977
Batch 48/64 loss: -0.3330397605895996
Batch 49/64 loss: -0.5335297584533691
Batch 50/64 loss: 0.14125919342041016
Batch 51/64 loss: -0.3270707130432129
Batch 52/64 loss: -0.3512845039367676
Batch 53/64 loss: -0.4227604866027832
Batch 54/64 loss: -0.31206417083740234
Batch 55/64 loss: -0.3094768524169922
Batch 56/64 loss: -0.21471309661865234
Batch 57/64 loss: -0.34891271591186523
Batch 58/64 loss: 0.7498860359191895
Batch 59/64 loss: -0.35511302947998047
Batch 60/64 loss: -0.4888486862182617
Batch 61/64 loss: -0.38563060760498047
Batch 62/64 loss: -0.5710997581481934
Batch 63/64 loss: -0.10298681259155273
Batch 64/64 loss: -4.049708843231201
Epoch 219  Train loss: -0.3165329970565497  Val loss: -0.6516334166641498
Epoch 220
-------------------------------
Batch 1/64 loss: -0.48081350326538086
Batch 2/64 loss: -0.25129270553588867
Batch 3/64 loss: -0.49826812744140625
Batch 4/64 loss: -0.45592832565307617
Batch 5/64 loss: -0.3434901237487793
Batch 6/64 loss: -0.4756903648376465
Batch 7/64 loss: -0.44826602935791016
Batch 8/64 loss: -0.40095090866088867
Batch 9/64 loss: -0.6002869606018066
Batch 10/64 loss: -0.3908557891845703
Batch 11/64 loss: -0.41796350479125977
Batch 12/64 loss: -0.42678070068359375
Batch 13/64 loss: -0.47805261611938477
Batch 14/64 loss: -0.35547447204589844
Batch 15/64 loss: -0.5528016090393066
Batch 16/64 loss: -0.30658578872680664
Batch 17/64 loss: -0.5254778861999512
Batch 18/64 loss: -0.18158245086669922
Batch 19/64 loss: -0.4251704216003418
Batch 20/64 loss: -0.24637460708618164
Batch 21/64 loss: -0.3984403610229492
Batch 22/64 loss: 0.16329193115234375
Batch 23/64 loss: -0.18927240371704102
Batch 24/64 loss: 0.1677103042602539
Batch 25/64 loss: -0.01752471923828125
Batch 26/64 loss: 0.6440815925598145
Batch 27/64 loss: -0.5167155265808105
Batch 28/64 loss: -0.38575124740600586
Batch 29/64 loss: 1.203216552734375
Batch 30/64 loss: -0.30566883087158203
Batch 31/64 loss: -0.3174281120300293
Batch 32/64 loss: -0.30492734909057617
Batch 33/64 loss: 0.7163996696472168
Batch 34/64 loss: -0.5888199806213379
Batch 35/64 loss: -0.1980123519897461
Batch 36/64 loss: -0.4694209098815918
Batch 37/64 loss: -0.4801769256591797
Batch 38/64 loss: -0.39380979537963867
Batch 39/64 loss: -0.47485780715942383
Batch 40/64 loss: -0.44627809524536133
Batch 41/64 loss: -0.22258853912353516
Batch 42/64 loss: -0.3621230125427246
Batch 43/64 loss: -0.378908634185791
Batch 44/64 loss: -0.3945193290710449
Batch 45/64 loss: -0.38422679901123047
Batch 46/64 loss: -0.44493722915649414
Batch 47/64 loss: -0.3763608932495117
Batch 48/64 loss: -0.526282787322998
Batch 49/64 loss: -0.4529838562011719
Batch 50/64 loss: 0.009380340576171875
Batch 51/64 loss: -0.3343954086303711
Batch 52/64 loss: -0.4357337951660156
Batch 53/64 loss: -0.37703752517700195
Batch 54/64 loss: 0.1844649314880371
Batch 55/64 loss: -0.42446279525756836
Batch 56/64 loss: -0.6805582046508789
Batch 57/64 loss: -0.3693985939025879
Batch 58/64 loss: -0.32452869415283203
Batch 59/64 loss: -0.5340862274169922
Batch 60/64 loss: -0.46822309494018555
Batch 61/64 loss: -0.4600553512573242
Batch 62/64 loss: -0.42006397247314453
Batch 63/64 loss: -0.5856561660766602
Batch 64/64 loss: -3.9491372108459473
Epoch 220  Train loss: -0.35419058519251206  Val loss: -0.6659727981410075
Epoch 221
-------------------------------
Batch 1/64 loss: -0.3484663963317871
Batch 2/64 loss: -0.15415477752685547
Batch 3/64 loss: 0.8864345550537109
Batch 4/64 loss: -0.4114398956298828
Batch 5/64 loss: -0.4926137924194336
Batch 6/64 loss: -0.37250232696533203
Batch 7/64 loss: -0.3977999687194824
Batch 8/64 loss: -0.19904708862304688
Batch 9/64 loss: -0.3508725166320801
Batch 10/64 loss: 2.287446975708008
Batch 11/64 loss: -0.5221571922302246
Batch 12/64 loss: -0.3870511054992676
Batch 13/64 loss: -0.4606184959411621
Batch 14/64 loss: -0.38039493560791016
Batch 15/64 loss: 0.07306528091430664
Batch 16/64 loss: -0.3345518112182617
Batch 17/64 loss: -0.3663330078125
Batch 18/64 loss: -0.30935001373291016
Batch 19/64 loss: 0.19830989837646484
Batch 20/64 loss: -0.40665435791015625
Batch 21/64 loss: -0.408444881439209
Batch 22/64 loss: -0.40734386444091797
Batch 23/64 loss: -0.4521956443786621
Batch 24/64 loss: 0.3653268814086914
Batch 25/64 loss: -0.4586043357849121
Batch 26/64 loss: -0.45746898651123047
Batch 27/64 loss: -0.22666025161743164
Batch 28/64 loss: -0.3201718330383301
Batch 29/64 loss: -0.5197763442993164
Batch 30/64 loss: -0.15280675888061523
Batch 31/64 loss: -0.5343332290649414
Batch 32/64 loss: -0.4311699867248535
Batch 33/64 loss: -0.30207157135009766
Batch 34/64 loss: -0.1736760139465332
Batch 35/64 loss: -0.34372520446777344
Batch 36/64 loss: -0.6191277503967285
Batch 37/64 loss: -0.31088685989379883
Batch 38/64 loss: -0.3457765579223633
Batch 39/64 loss: -0.2910933494567871
Batch 40/64 loss: -0.4055800437927246
Batch 41/64 loss: 0.06370973587036133
Batch 42/64 loss: -0.06370115280151367
Batch 43/64 loss: -0.4220266342163086
Batch 44/64 loss: -0.5166678428649902
Batch 45/64 loss: -0.48334360122680664
Batch 46/64 loss: -0.37951231002807617
Batch 47/64 loss: -0.43396711349487305
Batch 48/64 loss: -0.4250326156616211
Batch 49/64 loss: -0.3990488052368164
Batch 50/64 loss: -0.23729991912841797
Batch 51/64 loss: -0.49305105209350586
Batch 52/64 loss: 0.0348820686340332
Batch 53/64 loss: -0.6042437553405762
Batch 54/64 loss: -0.49898576736450195
Batch 55/64 loss: -0.2853889465332031
Batch 56/64 loss: -0.531641960144043
Batch 57/64 loss: -0.4420623779296875
Batch 58/64 loss: -0.3268575668334961
Batch 59/64 loss: -0.031980037689208984
Batch 60/64 loss: -0.33899879455566406
Batch 61/64 loss: -0.5533838272094727
Batch 62/64 loss: -0.4572014808654785
Batch 63/64 loss: 0.2680020332336426
Batch 64/64 loss: -3.7716383934020996
Epoch 221  Train loss: -0.3079351855259316  Val loss: -0.7033165607255759
Epoch 222
-------------------------------
Batch 1/64 loss: -0.5624642372131348
Batch 2/64 loss: -0.45456552505493164
Batch 3/64 loss: -0.4022488594055176
Batch 4/64 loss: -0.4652409553527832
Batch 5/64 loss: -0.4909243583679199
Batch 6/64 loss: -0.5594563484191895
Batch 7/64 loss: -0.5069036483764648
Batch 8/64 loss: -0.530268669128418
Batch 9/64 loss: -0.44255828857421875
Batch 10/64 loss: -0.4882230758666992
Batch 11/64 loss: -0.36863183975219727
Batch 12/64 loss: -0.42199087142944336
Batch 13/64 loss: -0.41942310333251953
Batch 14/64 loss: -0.39157533645629883
Batch 15/64 loss: -0.47502660751342773
Batch 16/64 loss: -0.3150339126586914
Batch 17/64 loss: -0.3442230224609375
Batch 18/64 loss: -0.596527099609375
Batch 19/64 loss: -0.41935300827026367
Batch 20/64 loss: -0.44194507598876953
Batch 21/64 loss: -0.48531675338745117
Batch 22/64 loss: -0.36946868896484375
Batch 23/64 loss: -0.2905611991882324
Batch 24/64 loss: -0.5142431259155273
Batch 25/64 loss: -0.2798595428466797
Batch 26/64 loss: -0.058335304260253906
Batch 27/64 loss: 0.8788490295410156
Batch 28/64 loss: -0.044574737548828125
Batch 29/64 loss: 0.13787126541137695
Batch 30/64 loss: -0.45043134689331055
Batch 31/64 loss: -0.6061086654663086
Batch 32/64 loss: -0.49268150329589844
Batch 33/64 loss: -0.5115776062011719
Batch 34/64 loss: -0.48461341857910156
Batch 35/64 loss: -0.22064828872680664
Batch 36/64 loss: 1.040452480316162
Batch 37/64 loss: -0.4113121032714844
Batch 38/64 loss: -0.4333534240722656
Batch 39/64 loss: -0.3762035369873047
Batch 40/64 loss: -0.26410722732543945
Batch 41/64 loss: -0.2877211570739746
Batch 42/64 loss: -0.4341702461242676
Batch 43/64 loss: -0.47666358947753906
Batch 44/64 loss: -0.42040348052978516
Batch 45/64 loss: -0.6139082908630371
Batch 46/64 loss: -0.07673025131225586
Batch 47/64 loss: -0.5293855667114258
Batch 48/64 loss: -0.5292677879333496
Batch 49/64 loss: -0.20831584930419922
Batch 50/64 loss: -0.4376072883605957
Batch 51/64 loss: -0.09523153305053711
Batch 52/64 loss: -0.552790641784668
Batch 53/64 loss: -0.45560312271118164
Batch 54/64 loss: 0.2691946029663086
Batch 55/64 loss: -0.5510430335998535
Batch 56/64 loss: -0.4205780029296875
Batch 57/64 loss: 0.6252827644348145
Batch 58/64 loss: -0.28634071350097656
Batch 59/64 loss: -0.45789527893066406
Batch 60/64 loss: -0.5796680450439453
Batch 61/64 loss: -0.544189453125
Batch 62/64 loss: -0.24750566482543945
Batch 63/64 loss: -0.27510547637939453
Batch 64/64 loss: -3.5693821907043457
Epoch 222  Train loss: -0.3701253535700779  Val loss: -0.6337335986370074
Epoch 223
-------------------------------
Batch 1/64 loss: -0.26822996139526367
Batch 2/64 loss: -0.49582672119140625
Batch 3/64 loss: -0.5396780967712402
Batch 4/64 loss: -0.2480459213256836
Batch 5/64 loss: -0.5334300994873047
Batch 6/64 loss: -0.29656505584716797
Batch 7/64 loss: -0.3902273178100586
Batch 8/64 loss: 0.22737836837768555
Batch 9/64 loss: -0.22585248947143555
Batch 10/64 loss: -0.5071196556091309
Batch 11/64 loss: -0.28737449645996094
Batch 12/64 loss: -0.44699668884277344
Batch 13/64 loss: -0.3272428512573242
Batch 14/64 loss: -0.03312253952026367
Batch 15/64 loss: -0.43698978424072266
Batch 16/64 loss: -0.3421630859375
Batch 17/64 loss: 0.24547910690307617
Batch 18/64 loss: -0.4997835159301758
Batch 19/64 loss: -0.2615084648132324
Batch 20/64 loss: -0.402193546295166
Batch 21/64 loss: -0.29416799545288086
Batch 22/64 loss: -0.46771955490112305
Batch 23/64 loss: -0.5278515815734863
Batch 24/64 loss: -0.5470609664916992
Batch 25/64 loss: -0.5190553665161133
Batch 26/64 loss: -0.4217386245727539
Batch 27/64 loss: -0.5317649841308594
Batch 28/64 loss: -0.3549957275390625
Batch 29/64 loss: -0.4486851692199707
Batch 30/64 loss: -0.4751400947570801
Batch 31/64 loss: -0.5925722122192383
Batch 32/64 loss: 0.591029167175293
Batch 33/64 loss: -0.5698699951171875
Batch 34/64 loss: -0.37061405181884766
Batch 35/64 loss: -0.46425914764404297
Batch 36/64 loss: -0.20532560348510742
Batch 37/64 loss: -0.5175714492797852
Batch 38/64 loss: -0.3809971809387207
Batch 39/64 loss: -0.32448434829711914
Batch 40/64 loss: -0.41156530380249023
Batch 41/64 loss: -0.4502744674682617
Batch 42/64 loss: -0.4155144691467285
Batch 43/64 loss: 0.830193042755127
Batch 44/64 loss: -0.5278797149658203
Batch 45/64 loss: -0.5119895935058594
Batch 46/64 loss: -0.343904972076416
Batch 47/64 loss: -0.4990720748901367
Batch 48/64 loss: -0.5538420677185059
Batch 49/64 loss: -0.5966329574584961
Batch 50/64 loss: 1.070876121520996
Batch 51/64 loss: -0.4514169692993164
Batch 52/64 loss: -0.573275089263916
Batch 53/64 loss: -0.5440821647644043
Batch 54/64 loss: -0.38924169540405273
Batch 55/64 loss: -0.3074979782104492
Batch 56/64 loss: -0.2236161231994629
Batch 57/64 loss: 0.1406998634338379
Batch 58/64 loss: -0.45118045806884766
Batch 59/64 loss: -0.41026830673217773
Batch 60/64 loss: -0.3992128372192383
Batch 61/64 loss: -0.5407943725585938
Batch 62/64 loss: -0.43959665298461914
Batch 63/64 loss: -0.38304853439331055
Batch 64/64 loss: -4.0465192794799805
Epoch 223  Train loss: -0.3750488617840935  Val loss: -0.7584893269227543
Epoch 224
-------------------------------
Batch 1/64 loss: -0.387418270111084
Batch 2/64 loss: -0.34679365158081055
Batch 3/64 loss: 0.5842862129211426
Batch 4/64 loss: -0.41536664962768555
Batch 5/64 loss: -0.6610755920410156
Batch 6/64 loss: -0.41339921951293945
Batch 7/64 loss: -0.44278812408447266
Batch 8/64 loss: -0.30231332778930664
Batch 9/64 loss: -0.4377107620239258
Batch 10/64 loss: -0.5732674598693848
Batch 11/64 loss: -0.5202794075012207
Batch 12/64 loss: -0.36375856399536133
Batch 13/64 loss: -0.33209800720214844
Batch 14/64 loss: -0.014375686645507812
Batch 15/64 loss: -0.529942512512207
Batch 16/64 loss: -0.6285524368286133
Batch 17/64 loss: 0.14474773406982422
Batch 18/64 loss: -0.5585613250732422
Batch 19/64 loss: -0.6549296379089355
Batch 20/64 loss: -0.4537358283996582
Batch 21/64 loss: 0.833745002746582
Batch 22/64 loss: -0.4079294204711914
Batch 23/64 loss: -0.29866933822631836
Batch 24/64 loss: -0.5544376373291016
Batch 25/64 loss: -0.45142507553100586
Batch 26/64 loss: -0.5658597946166992
Batch 27/64 loss: -0.498016357421875
Batch 28/64 loss: -0.5747666358947754
Batch 29/64 loss: -0.5788760185241699
Batch 30/64 loss: -0.5366172790527344
Batch 31/64 loss: 1.1436653137207031
Batch 32/64 loss: -0.3172626495361328
Batch 33/64 loss: -0.542473316192627
Batch 34/64 loss: -0.4699368476867676
Batch 35/64 loss: -0.1396346092224121
Batch 36/64 loss: -0.570958137512207
Batch 37/64 loss: -0.4698777198791504
Batch 38/64 loss: -0.3966498374938965
Batch 39/64 loss: -0.44906139373779297
Batch 40/64 loss: -0.6368765830993652
Batch 41/64 loss: -0.4633622169494629
Batch 42/64 loss: -0.5764431953430176
Batch 43/64 loss: -0.5922946929931641
Batch 44/64 loss: -0.5530762672424316
Batch 45/64 loss: -0.5519518852233887
Batch 46/64 loss: -0.4505605697631836
Batch 47/64 loss: -0.4766383171081543
Batch 48/64 loss: -0.5236749649047852
Batch 49/64 loss: -0.42877674102783203
Batch 50/64 loss: -0.5071325302124023
Batch 51/64 loss: -0.5879964828491211
Batch 52/64 loss: -0.511469841003418
Batch 53/64 loss: -0.31766843795776367
Batch 54/64 loss: -0.0027108192443847656
Batch 55/64 loss: -0.43184852600097656
Batch 56/64 loss: -0.561251163482666
Batch 57/64 loss: -0.48543691635131836
Batch 58/64 loss: -0.5714187622070312
Batch 59/64 loss: -0.5443301200866699
Batch 60/64 loss: -0.6025271415710449
Batch 61/64 loss: -0.29624223709106445
Batch 62/64 loss: -0.4799661636352539
Batch 63/64 loss: -0.6001639366149902
Batch 64/64 loss: -4.035747528076172
Epoch 224  Train loss: -0.4381647596172258  Val loss: -0.7317252536000255
Epoch 225
-------------------------------
Batch 1/64 loss: -0.5325627326965332
Batch 2/64 loss: -0.026210784912109375
Batch 3/64 loss: -0.6256103515625
Batch 4/64 loss: 0.1815037727355957
Batch 5/64 loss: -0.541557788848877
Batch 6/64 loss: -0.37561941146850586
Batch 7/64 loss: -0.5538954734802246
Batch 8/64 loss: -0.5999836921691895
Batch 9/64 loss: -0.37076854705810547
Batch 10/64 loss: -0.49781370162963867
Batch 11/64 loss: -0.5383090972900391
Batch 12/64 loss: -0.5358457565307617
Batch 13/64 loss: -0.3839111328125
Batch 14/64 loss: -0.42455387115478516
Batch 15/64 loss: -0.46117305755615234
Batch 16/64 loss: -0.37456226348876953
Batch 17/64 loss: -0.5576419830322266
Batch 18/64 loss: -0.5529565811157227
Batch 19/64 loss: -0.48430347442626953
Batch 20/64 loss: -0.5966997146606445
Batch 21/64 loss: -0.43572521209716797
Batch 22/64 loss: -0.3826446533203125
Batch 23/64 loss: -0.48827648162841797
Batch 24/64 loss: -0.3881855010986328
Batch 25/64 loss: 1.1428179740905762
Batch 26/64 loss: -0.46529293060302734
Batch 27/64 loss: -0.4851675033569336
Batch 28/64 loss: -0.497161865234375
Batch 29/64 loss: -0.6778936386108398
Batch 30/64 loss: -0.6110692024230957
Batch 31/64 loss: -0.25610828399658203
Batch 32/64 loss: -0.43506813049316406
Batch 33/64 loss: -0.48255205154418945
Batch 34/64 loss: -0.371950626373291
Batch 35/64 loss: 0.08648109436035156
Batch 36/64 loss: -0.5072250366210938
Batch 37/64 loss: -0.5096578598022461
Batch 38/64 loss: -0.09144783020019531
Batch 39/64 loss: -0.06357049942016602
Batch 40/64 loss: -0.4305105209350586
Batch 41/64 loss: -0.4596896171569824
Batch 42/64 loss: -0.3404836654663086
Batch 43/64 loss: -0.403933048248291
Batch 44/64 loss: -0.49351930618286133
Batch 45/64 loss: -0.389742374420166
Batch 46/64 loss: -0.5414180755615234
Batch 47/64 loss: -0.5078763961791992
Batch 48/64 loss: -0.5577192306518555
Batch 49/64 loss: 0.4286618232727051
Batch 50/64 loss: -0.515101432800293
Batch 51/64 loss: -0.418304443359375
Batch 52/64 loss: -0.31807804107666016
Batch 53/64 loss: -0.48830127716064453
Batch 54/64 loss: -0.4307212829589844
Batch 55/64 loss: 1.6443276405334473
Batch 56/64 loss: -0.4713411331176758
Batch 57/64 loss: -0.23331832885742188
Batch 58/64 loss: -0.2436084747314453
Batch 59/64 loss: -0.42190027236938477
Batch 60/64 loss: -0.49654340744018555
Batch 61/64 loss: -0.5425605773925781
Batch 62/64 loss: -0.37353944778442383
Batch 63/64 loss: -0.6345853805541992
Batch 64/64 loss: -4.056772708892822
Epoch 225  Train loss: -0.3992872107262705  Val loss: -0.8106201801103415
Epoch 226
-------------------------------
Batch 1/64 loss: -0.34538936614990234
Batch 2/64 loss: -0.591437816619873
Batch 3/64 loss: -0.40132951736450195
Batch 4/64 loss: -0.450653076171875
Batch 5/64 loss: -0.6233043670654297
Batch 6/64 loss: -0.5734949111938477
Batch 7/64 loss: -0.016324996948242188
Batch 8/64 loss: -0.5534758567810059
Batch 9/64 loss: -0.5784955024719238
Batch 10/64 loss: -0.4815101623535156
Batch 11/64 loss: -0.43607234954833984
Batch 12/64 loss: -0.48691892623901367
Batch 13/64 loss: -0.49751996994018555
Batch 14/64 loss: -0.6043643951416016
Batch 15/64 loss: -0.5543079376220703
Batch 16/64 loss: -0.2086172103881836
Batch 17/64 loss: -0.5883469581604004
Batch 18/64 loss: -0.6226329803466797
Batch 19/64 loss: -0.48578882217407227
Batch 20/64 loss: -0.6110219955444336
Batch 21/64 loss: -0.20672035217285156
Batch 22/64 loss: -0.5376968383789062
Batch 23/64 loss: -0.07022762298583984
Batch 24/64 loss: -0.2863888740539551
Batch 25/64 loss: -0.6104001998901367
Batch 26/64 loss: -0.43465709686279297
Batch 27/64 loss: -0.5277924537658691
Batch 28/64 loss: 0.6689834594726562
Batch 29/64 loss: -0.21370363235473633
Batch 30/64 loss: -0.6451969146728516
Batch 31/64 loss: -0.24800395965576172
Batch 32/64 loss: -0.623161792755127
Batch 33/64 loss: -0.5670390129089355
Batch 34/64 loss: -0.45864009857177734
Batch 35/64 loss: -0.5165395736694336
Batch 36/64 loss: -0.5301313400268555
Batch 37/64 loss: -0.6452789306640625
Batch 38/64 loss: -0.49852561950683594
Batch 39/64 loss: -0.49313831329345703
Batch 40/64 loss: -0.017848968505859375
Batch 41/64 loss: 0.22994279861450195
Batch 42/64 loss: -0.2716388702392578
Batch 43/64 loss: -0.6966910362243652
Batch 44/64 loss: -0.6534948348999023
Batch 45/64 loss: -0.4462709426879883
Batch 46/64 loss: 0.2588667869567871
Batch 47/64 loss: -0.4409809112548828
Batch 48/64 loss: -0.6439967155456543
Batch 49/64 loss: -0.6112527847290039
Batch 50/64 loss: -0.17007780075073242
Batch 51/64 loss: 0.46743249893188477
Batch 52/64 loss: -0.40077924728393555
Batch 53/64 loss: 1.049431324005127
Batch 54/64 loss: -0.5711112022399902
Batch 55/64 loss: -0.42893028259277344
Batch 56/64 loss: -0.4960784912109375
Batch 57/64 loss: -0.5478672981262207
Batch 58/64 loss: -0.5160398483276367
Batch 59/64 loss: -0.6570358276367188
Batch 60/64 loss: -0.5685844421386719
Batch 61/64 loss: -0.48390960693359375
Batch 62/64 loss: -0.4959893226623535
Batch 63/64 loss: -0.37285900115966797
Batch 64/64 loss: -4.047407627105713
Epoch 226  Train loss: -0.43414252599080405  Val loss: -0.8794828329708978
Saving best model, epoch: 226
Epoch 227
-------------------------------
Batch 1/64 loss: -0.6108808517456055
Batch 2/64 loss: -0.43661069869995117
Batch 3/64 loss: -0.6154346466064453
Batch 4/64 loss: -0.6603045463562012
Batch 5/64 loss: -0.019170761108398438
Batch 6/64 loss: -0.02348470687866211
Batch 7/64 loss: -0.5314831733703613
Batch 8/64 loss: -0.5238637924194336
Batch 9/64 loss: 0.45766210556030273
Batch 10/64 loss: -0.1486959457397461
Batch 11/64 loss: -0.6910223960876465
Batch 12/64 loss: -0.6153779029846191
Batch 13/64 loss: -0.5867013931274414
Batch 14/64 loss: -0.6122627258300781
Batch 15/64 loss: -0.4915342330932617
Batch 16/64 loss: -0.5181970596313477
Batch 17/64 loss: 0.14016294479370117
Batch 18/64 loss: -0.5946831703186035
Batch 19/64 loss: -0.519627571105957
Batch 20/64 loss: -0.4919300079345703
Batch 21/64 loss: -0.6291446685791016
Batch 22/64 loss: -0.5448989868164062
Batch 23/64 loss: -0.5861029624938965
Batch 24/64 loss: -0.39458799362182617
Batch 25/64 loss: -0.38817405700683594
Batch 26/64 loss: -0.4903278350830078
Batch 27/64 loss: -0.31913232803344727
Batch 28/64 loss: -0.4415121078491211
Batch 29/64 loss: -0.2735724449157715
Batch 30/64 loss: -0.5286493301391602
Batch 31/64 loss: -0.43175792694091797
Batch 32/64 loss: -0.5521106719970703
Batch 33/64 loss: -0.3274111747741699
Batch 34/64 loss: -0.5942564010620117
Batch 35/64 loss: -0.44070911407470703
Batch 36/64 loss: -0.5652050971984863
Batch 37/64 loss: 1.0346784591674805
Batch 38/64 loss: -0.39954090118408203
Batch 39/64 loss: -0.4892902374267578
Batch 40/64 loss: -0.4646878242492676
Batch 41/64 loss: -0.2450885772705078
Batch 42/64 loss: -0.5521659851074219
Batch 43/64 loss: -0.4078211784362793
Batch 44/64 loss: -0.567906379699707
Batch 45/64 loss: -0.4709196090698242
Batch 46/64 loss: -0.5876908302307129
Batch 47/64 loss: -0.32605791091918945
Batch 48/64 loss: -0.6055879592895508
Batch 49/64 loss: -0.44883251190185547
Batch 50/64 loss: -0.658780574798584
Batch 51/64 loss: -0.3576169013977051
Batch 52/64 loss: -0.6087379455566406
Batch 53/64 loss: -0.4587407112121582
Batch 54/64 loss: -0.6337928771972656
Batch 55/64 loss: -0.35327625274658203
Batch 56/64 loss: -0.4626741409301758
Batch 57/64 loss: -0.6687569618225098
Batch 58/64 loss: 1.149315357208252
Batch 59/64 loss: -0.4489002227783203
Batch 60/64 loss: -0.5345883369445801
Batch 61/64 loss: -0.371340274810791
Batch 62/64 loss: -0.05342245101928711
Batch 63/64 loss: -0.5016918182373047
Batch 64/64 loss: -3.7705345153808594
Epoch 227  Train loss: -0.43800486396340765  Val loss: -0.8118634174779519
Epoch 228
-------------------------------
Batch 1/64 loss: -0.4786052703857422
Batch 2/64 loss: -0.6003851890563965
Batch 3/64 loss: -0.617884635925293
Batch 4/64 loss: -0.40181398391723633
Batch 5/64 loss: -0.5546283721923828
Batch 6/64 loss: -0.5724377632141113
Batch 7/64 loss: -0.19380617141723633
Batch 8/64 loss: 0.09291505813598633
Batch 9/64 loss: 0.07167196273803711
Batch 10/64 loss: -0.4031963348388672
Batch 11/64 loss: 0.9270162582397461
Batch 12/64 loss: -0.6005792617797852
Batch 13/64 loss: -0.5772147178649902
Batch 14/64 loss: -0.37274932861328125
Batch 15/64 loss: -0.5121593475341797
Batch 16/64 loss: 0.5178790092468262
Batch 17/64 loss: -0.39168453216552734
Batch 18/64 loss: -0.4653143882751465
Batch 19/64 loss: -0.4717540740966797
Batch 20/64 loss: -0.5693626403808594
Batch 21/64 loss: -0.4639420509338379
Batch 22/64 loss: -0.5949258804321289
Batch 23/64 loss: -0.4192924499511719
Batch 24/64 loss: 0.7082014083862305
Batch 25/64 loss: -0.6259727478027344
Batch 26/64 loss: -0.6755023002624512
Batch 27/64 loss: -0.29914379119873047
Batch 28/64 loss: -0.6273007392883301
Batch 29/64 loss: -0.5085649490356445
Batch 30/64 loss: -0.6629447937011719
Batch 31/64 loss: -0.4498882293701172
Batch 32/64 loss: -0.4931821823120117
Batch 33/64 loss: -0.526641845703125
Batch 34/64 loss: -0.44550418853759766
Batch 35/64 loss: -0.45207881927490234
Batch 36/64 loss: -0.49036264419555664
Batch 37/64 loss: -0.4203042984008789
Batch 38/64 loss: -0.15015268325805664
Batch 39/64 loss: -0.5482082366943359
Batch 40/64 loss: -0.5231809616088867
Batch 41/64 loss: -0.3010849952697754
Batch 42/64 loss: -0.5221185684204102
Batch 43/64 loss: -0.5217595100402832
Batch 44/64 loss: -0.48409461975097656
Batch 45/64 loss: -0.3993372917175293
Batch 46/64 loss: -0.6101722717285156
Batch 47/64 loss: -0.33017778396606445
Batch 48/64 loss: -0.442349910736084
Batch 49/64 loss: -0.34423351287841797
Batch 50/64 loss: -0.6194858551025391
Batch 51/64 loss: -0.5148735046386719
Batch 52/64 loss: -0.43422889709472656
Batch 53/64 loss: -0.4105243682861328
Batch 54/64 loss: -0.4266233444213867
Batch 55/64 loss: -0.48772573471069336
Batch 56/64 loss: -0.5606098175048828
Batch 57/64 loss: -0.5309386253356934
Batch 58/64 loss: -0.5533156394958496
Batch 59/64 loss: -0.4017333984375
Batch 60/64 loss: -0.5427432060241699
Batch 61/64 loss: -0.47493934631347656
Batch 62/64 loss: 0.3363051414489746
Batch 63/64 loss: -0.4425544738769531
Batch 64/64 loss: -3.9394359588623047
Epoch 228  Train loss: -0.43634325663248696  Val loss: -0.8085861992590206
Epoch 229
-------------------------------
Batch 1/64 loss: -0.49098777770996094
Batch 2/64 loss: -0.6002993583679199
Batch 3/64 loss: -0.46889543533325195
Batch 4/64 loss: -0.6212444305419922
Batch 5/64 loss: -0.510221004486084
Batch 6/64 loss: -0.1218557357788086
Batch 7/64 loss: -0.425020694732666
Batch 8/64 loss: 0.5440211296081543
Batch 9/64 loss: -0.5374007225036621
Batch 10/64 loss: -0.5752110481262207
Batch 11/64 loss: -0.5808682441711426
Batch 12/64 loss: -0.5995121002197266
Batch 13/64 loss: -0.5224442481994629
Batch 14/64 loss: -0.49036645889282227
Batch 15/64 loss: -0.48624134063720703
Batch 16/64 loss: -0.6649894714355469
Batch 17/64 loss: -0.46651268005371094
Batch 18/64 loss: -0.5140137672424316
Batch 19/64 loss: -0.5763959884643555
Batch 20/64 loss: -0.606600284576416
Batch 21/64 loss: -0.0024051666259765625
Batch 22/64 loss: -0.4893035888671875
Batch 23/64 loss: -0.2882862091064453
Batch 24/64 loss: -0.6081929206848145
Batch 25/64 loss: -0.75634765625
Batch 26/64 loss: -0.6336021423339844
Batch 27/64 loss: -0.4313693046569824
Batch 28/64 loss: -0.45104408264160156
Batch 29/64 loss: -0.42545366287231445
Batch 30/64 loss: -0.31295013427734375
Batch 31/64 loss: -0.6907386779785156
Batch 32/64 loss: -0.6299209594726562
Batch 33/64 loss: -0.48424434661865234
Batch 34/64 loss: -0.5078282356262207
Batch 35/64 loss: -0.34836244583129883
Batch 36/64 loss: -0.5372333526611328
Batch 37/64 loss: -0.4818730354309082
Batch 38/64 loss: -0.5237679481506348
Batch 39/64 loss: -0.47428131103515625
Batch 40/64 loss: -0.4675283432006836
Batch 41/64 loss: 0.9723124504089355
Batch 42/64 loss: -0.4711637496948242
Batch 43/64 loss: -0.5035009384155273
Batch 44/64 loss: -0.4273509979248047
Batch 45/64 loss: -0.12799358367919922
Batch 46/64 loss: -0.08247232437133789
Batch 47/64 loss: 0.1073617935180664
Batch 48/64 loss: -0.4790797233581543
Batch 49/64 loss: -0.6575837135314941
Batch 50/64 loss: 0.6776590347290039
Batch 51/64 loss: -0.34938907623291016
Batch 52/64 loss: -0.560737133026123
Batch 53/64 loss: -0.5582337379455566
Batch 54/64 loss: -0.038419246673583984
Batch 55/64 loss: -0.3607749938964844
Batch 56/64 loss: -0.45452260971069336
Batch 57/64 loss: -0.6308927536010742
Batch 58/64 loss: -0.49584388732910156
Batch 59/64 loss: -0.3596620559692383
Batch 60/64 loss: -0.47060060501098633
Batch 61/64 loss: -0.6030192375183105
Batch 62/64 loss: -0.6177740097045898
Batch 63/64 loss: -0.47888994216918945
Batch 64/64 loss: -4.17136287689209
Epoch 229  Train loss: -0.45425704133276845  Val loss: -0.7727279925264444
Epoch 230
-------------------------------
Batch 1/64 loss: -0.28141212463378906
Batch 2/64 loss: -0.33148622512817383
Batch 3/64 loss: -0.51019287109375
Batch 4/64 loss: -0.5553946495056152
Batch 5/64 loss: -0.29196882247924805
Batch 6/64 loss: -0.5955419540405273
Batch 7/64 loss: -0.704310417175293
Batch 8/64 loss: -0.44476747512817383
Batch 9/64 loss: -0.5019721984863281
Batch 10/64 loss: -0.525052547454834
Batch 11/64 loss: -0.5729618072509766
Batch 12/64 loss: -0.43346452713012695
Batch 13/64 loss: -0.48804473876953125
Batch 14/64 loss: -0.4818401336669922
Batch 15/64 loss: -0.519935131072998
Batch 16/64 loss: -0.6180486679077148
Batch 17/64 loss: -0.5683379173278809
Batch 18/64 loss: -0.5542526245117188
Batch 19/64 loss: -0.465364933013916
Batch 20/64 loss: -0.4962453842163086
Batch 21/64 loss: -0.35698366165161133
Batch 22/64 loss: -0.5923013687133789
Batch 23/64 loss: -0.6123104095458984
Batch 24/64 loss: -0.6680922508239746
Batch 25/64 loss: -0.7140655517578125
Batch 26/64 loss: -0.4472651481628418
Batch 27/64 loss: -0.47636842727661133
Batch 28/64 loss: -0.6107401847839355
Batch 29/64 loss: -0.30708932876586914
Batch 30/64 loss: -0.16398048400878906
Batch 31/64 loss: -0.3964543342590332
Batch 32/64 loss: -0.6260404586791992
Batch 33/64 loss: 0.6422858238220215
Batch 34/64 loss: 0.7836856842041016
Batch 35/64 loss: -0.39923572540283203
Batch 36/64 loss: -0.5687437057495117
Batch 37/64 loss: -0.46767234802246094
Batch 38/64 loss: -0.01219320297241211
Batch 39/64 loss: -0.6344976425170898
Batch 40/64 loss: 1.32597017288208
Batch 41/64 loss: -0.5915460586547852
Batch 42/64 loss: -0.18673181533813477
Batch 43/64 loss: -0.34598827362060547
Batch 44/64 loss: -0.6213564872741699
Batch 45/64 loss: -0.5336995124816895
Batch 46/64 loss: -0.3659367561340332
Batch 47/64 loss: -0.4732637405395508
Batch 48/64 loss: -0.3123202323913574
Batch 49/64 loss: -0.48261356353759766
Batch 50/64 loss: 0.08977937698364258
Batch 51/64 loss: -0.620490550994873
Batch 52/64 loss: -0.6136689186096191
Batch 53/64 loss: -0.6349124908447266
Batch 54/64 loss: -0.5128593444824219
Batch 55/64 loss: -0.3552670478820801
Batch 56/64 loss: -0.5613265037536621
Batch 57/64 loss: -0.5927290916442871
Batch 58/64 loss: -0.6127562522888184
Batch 59/64 loss: -0.6165857315063477
Batch 60/64 loss: -0.5416531562805176
Batch 61/64 loss: -0.5188965797424316
Batch 62/64 loss: -0.6045832633972168
Batch 63/64 loss: -0.47266721725463867
Batch 64/64 loss: -4.105592727661133
Epoch 230  Train loss: -0.46123851701325064  Val loss: -0.8152690114024579
Epoch 231
-------------------------------
Batch 1/64 loss: -0.5948395729064941
Batch 2/64 loss: -0.4195561408996582
Batch 3/64 loss: -0.3234748840332031
Batch 4/64 loss: -0.5612349510192871
Batch 5/64 loss: -0.5308318138122559
Batch 6/64 loss: 0.2040252685546875
Batch 7/64 loss: -0.4448204040527344
Batch 8/64 loss: -0.46250391006469727
Batch 9/64 loss: -0.5615434646606445
Batch 10/64 loss: -0.4705028533935547
Batch 11/64 loss: -0.49537181854248047
Batch 12/64 loss: -0.6838111877441406
Batch 13/64 loss: -0.5785694122314453
Batch 14/64 loss: -0.5295214653015137
Batch 15/64 loss: 0.5768089294433594
Batch 16/64 loss: 0.8710942268371582
Batch 17/64 loss: -0.1779923439025879
Batch 18/64 loss: -0.3135719299316406
Batch 19/64 loss: -0.48482275009155273
Batch 20/64 loss: -0.5811953544616699
Batch 21/64 loss: 0.1502089500427246
Batch 22/64 loss: -0.24764394760131836
Batch 23/64 loss: -0.3338050842285156
Batch 24/64 loss: -0.22209739685058594
Batch 25/64 loss: -0.5826663970947266
Batch 26/64 loss: -0.3339223861694336
Batch 27/64 loss: 0.005960941314697266
Batch 28/64 loss: -0.49332714080810547
Batch 29/64 loss: -0.1296381950378418
Batch 30/64 loss: -0.4926586151123047
Batch 31/64 loss: -0.512397289276123
Batch 32/64 loss: -0.5769877433776855
Batch 33/64 loss: -0.3282179832458496
Batch 34/64 loss: -0.35662364959716797
Batch 35/64 loss: -0.43178367614746094
Batch 36/64 loss: -0.629704475402832
Batch 37/64 loss: -0.4823160171508789
Batch 38/64 loss: -0.19399738311767578
Batch 39/64 loss: -0.4699211120605469
Batch 40/64 loss: -0.5065889358520508
Batch 41/64 loss: -0.4438948631286621
Batch 42/64 loss: -0.19199895858764648
Batch 43/64 loss: -0.4464445114135742
Batch 44/64 loss: -0.45275115966796875
Batch 45/64 loss: -0.23870563507080078
Batch 46/64 loss: -0.3261680603027344
Batch 47/64 loss: -0.35378503799438477
Batch 48/64 loss: -0.4200711250305176
Batch 49/64 loss: -0.30527734756469727
Batch 50/64 loss: -0.18613958358764648
Batch 51/64 loss: 1.0260825157165527
Batch 52/64 loss: 0.252291202545166
Batch 53/64 loss: -0.5774412155151367
Batch 54/64 loss: -0.11187934875488281
Batch 55/64 loss: -0.0884857177734375
Batch 56/64 loss: -0.4348468780517578
Batch 57/64 loss: -0.41469812393188477
Batch 58/64 loss: -0.5576949119567871
Batch 59/64 loss: -0.42354297637939453
Batch 60/64 loss: -0.21858692169189453
Batch 61/64 loss: -0.2858901023864746
Batch 62/64 loss: -0.5447969436645508
Batch 63/64 loss: -0.11454439163208008
Batch 64/64 loss: -3.8849191665649414
Epoch 231  Train loss: -0.3529932994468539  Val loss: -0.3839627950871523
Epoch 232
-------------------------------
Batch 1/64 loss: -0.08566474914550781
Batch 2/64 loss: -0.3628063201904297
Batch 3/64 loss: -0.28604984283447266
Batch 4/64 loss: -0.4313373565673828
Batch 5/64 loss: -0.3777899742126465
Batch 6/64 loss: 0.39412832260131836
Batch 7/64 loss: -0.4408988952636719
Batch 8/64 loss: -0.08441972732543945
Batch 9/64 loss: -0.2937474250793457
Batch 10/64 loss: -0.15730714797973633
Batch 11/64 loss: -0.3507537841796875
Batch 12/64 loss: -0.39887380599975586
Batch 13/64 loss: 0.00997304916381836
Batch 14/64 loss: -0.5122418403625488
Batch 15/64 loss: -0.42516183853149414
Batch 16/64 loss: 0.12371063232421875
Batch 17/64 loss: -0.440216064453125
Batch 18/64 loss: -0.09896659851074219
Batch 19/64 loss: -0.45118045806884766
Batch 20/64 loss: -0.4587726593017578
Batch 21/64 loss: 0.5687150955200195
Batch 22/64 loss: 1.0537371635437012
Batch 23/64 loss: -0.4492783546447754
Batch 24/64 loss: -0.4699087142944336
Batch 25/64 loss: -0.4060983657836914
Batch 26/64 loss: -0.5091052055358887
Batch 27/64 loss: -0.14824962615966797
Batch 28/64 loss: -0.23976755142211914
Batch 29/64 loss: 0.10092878341674805
Batch 30/64 loss: -0.515871524810791
Batch 31/64 loss: -0.5436806678771973
Batch 32/64 loss: -0.5174708366394043
Batch 33/64 loss: -0.4831047058105469
Batch 34/64 loss: -0.3827328681945801
Batch 35/64 loss: -0.4969668388366699
Batch 36/64 loss: -0.4490947723388672
Batch 37/64 loss: -0.3387727737426758
Batch 38/64 loss: -0.24070310592651367
Batch 39/64 loss: -0.42217445373535156
Batch 40/64 loss: 0.35024309158325195
Batch 41/64 loss: -0.21650171279907227
Batch 42/64 loss: -0.5313677787780762
Batch 43/64 loss: -0.5225539207458496
Batch 44/64 loss: -0.5323057174682617
Batch 45/64 loss: -0.3088507652282715
Batch 46/64 loss: 1.0174846649169922
Batch 47/64 loss: -0.42583703994750977
Batch 48/64 loss: -0.5812773704528809
Batch 49/64 loss: -0.2743706703186035
Batch 50/64 loss: -0.5373845100402832
Batch 51/64 loss: -0.5543718338012695
Batch 52/64 loss: -0.19676971435546875
Batch 53/64 loss: -0.5704054832458496
Batch 54/64 loss: -0.4125032424926758
Batch 55/64 loss: -0.6899170875549316
Batch 56/64 loss: -0.5664377212524414
Batch 57/64 loss: -0.568730354309082
Batch 58/64 loss: -0.17258596420288086
Batch 59/64 loss: -0.13506126403808594
Batch 60/64 loss: -0.4942479133605957
Batch 61/64 loss: -0.20806264877319336
Batch 62/64 loss: -0.45928096771240234
Batch 63/64 loss: -0.516566276550293
Batch 64/64 loss: -3.963474750518799
Epoch 232  Train loss: -0.3309528481726553  Val loss: -0.636616841214629
Epoch 233
-------------------------------
Batch 1/64 loss: -0.5151047706604004
Batch 2/64 loss: -0.4029541015625
Batch 3/64 loss: -0.4698944091796875
Batch 4/64 loss: -0.5851526260375977
Batch 5/64 loss: -0.39015674591064453
Batch 6/64 loss: -0.6111917495727539
Batch 7/64 loss: -0.5621285438537598
Batch 8/64 loss: -0.519355297088623
Batch 9/64 loss: -0.6626663208007812
Batch 10/64 loss: -0.49112415313720703
Batch 11/64 loss: -0.5706453323364258
Batch 12/64 loss: -0.5150842666625977
Batch 13/64 loss: -0.4290323257446289
Batch 14/64 loss: -0.6226959228515625
Batch 15/64 loss: -0.6005921363830566
Batch 16/64 loss: -0.49651479721069336
Batch 17/64 loss: -0.433713436126709
Batch 18/64 loss: -0.24384021759033203
Batch 19/64 loss: 1.314436435699463
Batch 20/64 loss: -0.40947961807250977
Batch 21/64 loss: -0.5473251342773438
Batch 22/64 loss: -0.18494081497192383
Batch 23/64 loss: -0.5508861541748047
Batch 24/64 loss: -0.07731246948242188
Batch 25/64 loss: -0.4293041229248047
Batch 26/64 loss: -0.37578582763671875
Batch 27/64 loss: -0.3603091239929199
Batch 28/64 loss: -0.6087303161621094
Batch 29/64 loss: -0.5431737899780273
Batch 30/64 loss: -0.628265380859375
Batch 31/64 loss: -0.5236635208129883
Batch 32/64 loss: -0.5890002250671387
Batch 33/64 loss: -0.584195613861084
Batch 34/64 loss: -0.5630388259887695
Batch 35/64 loss: -0.3205575942993164
Batch 36/64 loss: -0.4349346160888672
Batch 37/64 loss: -0.045775413513183594
Batch 38/64 loss: 0.34839963912963867
Batch 39/64 loss: -0.6772036552429199
Batch 40/64 loss: -0.542323112487793
Batch 41/64 loss: -0.5364713668823242
Batch 42/64 loss: -0.5305538177490234
Batch 43/64 loss: 0.7945003509521484
Batch 44/64 loss: -0.6017169952392578
Batch 45/64 loss: -0.5645503997802734
Batch 46/64 loss: -0.47402143478393555
Batch 47/64 loss: -0.5251936912536621
Batch 48/64 loss: -0.6264286041259766
Batch 49/64 loss: -0.6418862342834473
Batch 50/64 loss: -0.616940975189209
Batch 51/64 loss: -0.13837385177612305
Batch 52/64 loss: -0.5470738410949707
Batch 53/64 loss: -0.49381065368652344
Batch 54/64 loss: -0.6599249839782715
Batch 55/64 loss: -0.43651628494262695
Batch 56/64 loss: 1.0318751335144043
Batch 57/64 loss: -0.530280590057373
Batch 58/64 loss: -0.3273143768310547
Batch 59/64 loss: -0.48310041427612305
Batch 60/64 loss: -0.5484127998352051
Batch 61/64 loss: -0.5134282112121582
Batch 62/64 loss: -0.3066248893737793
Batch 63/64 loss: -0.5125813484191895
Batch 64/64 loss: -4.209453582763672
Epoch 233  Train loss: -0.44550802941415824  Val loss: -0.8081769779375738
Epoch 234
-------------------------------
Batch 1/64 loss: -0.5766806602478027
Batch 2/64 loss: -0.5650649070739746
Batch 3/64 loss: -0.33328676223754883
Batch 4/64 loss: -0.4992704391479492
Batch 5/64 loss: -0.5149602890014648
Batch 6/64 loss: -0.3166966438293457
Batch 7/64 loss: -0.5407981872558594
Batch 8/64 loss: -0.7010273933410645
Batch 9/64 loss: -0.6569132804870605
Batch 10/64 loss: -0.6454524993896484
Batch 11/64 loss: -0.48866748809814453
Batch 12/64 loss: -0.5008106231689453
Batch 13/64 loss: -0.6084504127502441
Batch 14/64 loss: -0.37520647048950195
Batch 15/64 loss: -0.4691910743713379
Batch 16/64 loss: -0.30901288986206055
Batch 17/64 loss: -0.33771228790283203
Batch 18/64 loss: -0.5704798698425293
Batch 19/64 loss: -0.5773077011108398
Batch 20/64 loss: -0.34012699127197266
Batch 21/64 loss: 0.03840208053588867
Batch 22/64 loss: -0.4034724235534668
Batch 23/64 loss: -0.4864187240600586
Batch 24/64 loss: -0.4525876045227051
Batch 25/64 loss: -0.5565118789672852
Batch 26/64 loss: -0.49773311614990234
Batch 27/64 loss: -0.3790879249572754
Batch 28/64 loss: -0.530890941619873
Batch 29/64 loss: -0.5681757926940918
Batch 30/64 loss: -0.4574265480041504
Batch 31/64 loss: 0.4397859573364258
Batch 32/64 loss: -0.5953164100646973
Batch 33/64 loss: 0.8829874992370605
Batch 34/64 loss: -0.3065624237060547
Batch 35/64 loss: -0.5835471153259277
Batch 36/64 loss: 0.6398205757141113
Batch 37/64 loss: -0.5000252723693848
Batch 38/64 loss: -0.37804126739501953
Batch 39/64 loss: -0.5752582550048828
Batch 40/64 loss: -0.6460685729980469
Batch 41/64 loss: -0.6201825141906738
Batch 42/64 loss: -0.4073491096496582
Batch 43/64 loss: -0.5622463226318359
Batch 44/64 loss: -0.5916957855224609
Batch 45/64 loss: -0.48523712158203125
Batch 46/64 loss: -0.6067652702331543
Batch 47/64 loss: -0.633392333984375
Batch 48/64 loss: -0.30346155166625977
Batch 49/64 loss: -0.48090696334838867
Batch 50/64 loss: -0.04703092575073242
Batch 51/64 loss: -0.3787808418273926
Batch 52/64 loss: -0.45607852935791016
Batch 53/64 loss: -0.5765595436096191
Batch 54/64 loss: 0.03164386749267578
Batch 55/64 loss: -0.6367206573486328
Batch 56/64 loss: -0.3663749694824219
Batch 57/64 loss: -0.39147377014160156
Batch 58/64 loss: -0.5179295539855957
Batch 59/64 loss: 0.2185530662536621
Batch 60/64 loss: -0.47837257385253906
Batch 61/64 loss: -0.12828636169433594
Batch 62/64 loss: -0.33679771423339844
Batch 63/64 loss: 0.026931285858154297
Batch 64/64 loss: -4.188780784606934
Epoch 234  Train loss: -0.43471913431205  Val loss: -0.7936761207187298
Epoch 235
-------------------------------
Batch 1/64 loss: -0.6589608192443848
Batch 2/64 loss: -0.22371721267700195
Batch 3/64 loss: -0.5632052421569824
Batch 4/64 loss: -0.40644216537475586
Batch 5/64 loss: -0.658637523651123
Batch 6/64 loss: 1.259169101715088
Batch 7/64 loss: -0.46854066848754883
Batch 8/64 loss: -0.6620159149169922
Batch 9/64 loss: -0.4836006164550781
Batch 10/64 loss: -0.48966550827026367
Batch 11/64 loss: -0.7179169654846191
Batch 12/64 loss: -0.4797224998474121
Batch 13/64 loss: -0.38850975036621094
Batch 14/64 loss: -0.39147043228149414
Batch 15/64 loss: -0.18796491622924805
Batch 16/64 loss: -0.5612106323242188
Batch 17/64 loss: -0.5681695938110352
Batch 18/64 loss: -0.5826148986816406
Batch 19/64 loss: -0.5372810363769531
Batch 20/64 loss: -0.408541202545166
Batch 21/64 loss: -0.6081809997558594
Batch 22/64 loss: -0.5110807418823242
Batch 23/64 loss: -0.1501755714416504
Batch 24/64 loss: -0.18993616104125977
Batch 25/64 loss: -0.5254306793212891
Batch 26/64 loss: -0.30482006072998047
Batch 27/64 loss: -0.5697526931762695
Batch 28/64 loss: -0.5257868766784668
Batch 29/64 loss: -0.5439577102661133
Batch 30/64 loss: -0.37773990631103516
Batch 31/64 loss: -0.5653924942016602
Batch 32/64 loss: 0.1674489974975586
Batch 33/64 loss: -0.09231424331665039
Batch 34/64 loss: -0.5680727958679199
Batch 35/64 loss: -0.4831972122192383
Batch 36/64 loss: -0.05163097381591797
Batch 37/64 loss: -0.4495429992675781
Batch 38/64 loss: 0.04274702072143555
Batch 39/64 loss: -0.14790868759155273
Batch 40/64 loss: -0.5499119758605957
Batch 41/64 loss: -0.3082394599914551
Batch 42/64 loss: -0.5670900344848633
Batch 43/64 loss: -0.5669503211975098
Batch 44/64 loss: -0.3460211753845215
Batch 45/64 loss: -0.2600421905517578
Batch 46/64 loss: -0.30288028717041016
Batch 47/64 loss: -0.27498674392700195
Batch 48/64 loss: -0.5376148223876953
Batch 49/64 loss: -0.15474987030029297
Batch 50/64 loss: -0.3730616569519043
Batch 51/64 loss: -0.5385112762451172
Batch 52/64 loss: -0.40256261825561523
Batch 53/64 loss: -0.42505931854248047
Batch 54/64 loss: -0.1841282844543457
Batch 55/64 loss: -0.5654277801513672
Batch 56/64 loss: -0.45062255859375
Batch 57/64 loss: 0.877556324005127
Batch 58/64 loss: -0.5419445037841797
Batch 59/64 loss: -0.4393343925476074
Batch 60/64 loss: -0.4768104553222656
Batch 61/64 loss: 0.4891324043273926
Batch 62/64 loss: -0.49683141708374023
Batch 63/64 loss: -0.6202607154846191
Batch 64/64 loss: -4.182441234588623
Epoch 235  Train loss: -0.4045008210574879  Val loss: -0.8037292113418841
Epoch 236
-------------------------------
Batch 1/64 loss: -0.5995988845825195
Batch 2/64 loss: -0.5690436363220215
Batch 3/64 loss: -0.42095327377319336
Batch 4/64 loss: -0.549591064453125
Batch 5/64 loss: -0.4522104263305664
Batch 6/64 loss: -0.10131168365478516
Batch 7/64 loss: -0.23459815979003906
Batch 8/64 loss: -0.5274310111999512
Batch 9/64 loss: -0.19893980026245117
Batch 10/64 loss: -0.28592824935913086
Batch 11/64 loss: -0.437471866607666
Batch 12/64 loss: -0.502814769744873
Batch 13/64 loss: -0.5841226577758789
Batch 14/64 loss: -0.11143970489501953
Batch 15/64 loss: -0.47243547439575195
Batch 16/64 loss: -0.464111328125
Batch 17/64 loss: -0.5952582359313965
Batch 18/64 loss: -0.30068016052246094
Batch 19/64 loss: 0.6600446701049805
Batch 20/64 loss: -0.034212589263916016
Batch 21/64 loss: -0.5886950492858887
Batch 22/64 loss: 0.004574298858642578
Batch 23/64 loss: -0.4267868995666504
Batch 24/64 loss: -0.2761502265930176
Batch 25/64 loss: -0.5123376846313477
Batch 26/64 loss: -0.5903205871582031
Batch 27/64 loss: 0.9270410537719727
Batch 28/64 loss: -0.2232956886291504
Batch 29/64 loss: 0.4868755340576172
Batch 30/64 loss: -0.3206667900085449
Batch 31/64 loss: -0.639153003692627
Batch 32/64 loss: -0.4468269348144531
Batch 33/64 loss: -0.6593828201293945
Batch 34/64 loss: -0.5928449630737305
Batch 35/64 loss: -0.544802188873291
Batch 36/64 loss: -0.7386059761047363
Batch 37/64 loss: 0.34787464141845703
Batch 38/64 loss: -0.6016111373901367
Batch 39/64 loss: -0.5211319923400879
Batch 40/64 loss: -0.45411014556884766
Batch 41/64 loss: -0.5894074440002441
Batch 42/64 loss: -0.5609602928161621
Batch 43/64 loss: -0.5972132682800293
Batch 44/64 loss: -0.6472034454345703
Batch 45/64 loss: -0.658043384552002
Batch 46/64 loss: -0.658505916595459
Batch 47/64 loss: -0.4495983123779297
Batch 48/64 loss: -0.4418449401855469
Batch 49/64 loss: -0.12624168395996094
Batch 50/64 loss: -0.4874544143676758
Batch 51/64 loss: -0.4308896064758301
Batch 52/64 loss: -0.5836005210876465
Batch 53/64 loss: -0.5219597816467285
Batch 54/64 loss: -0.5278496742248535
Batch 55/64 loss: -0.5760698318481445
Batch 56/64 loss: -0.5387783050537109
Batch 57/64 loss: -0.5762300491333008
Batch 58/64 loss: -0.48636674880981445
Batch 59/64 loss: -0.6520495414733887
Batch 60/64 loss: -0.5529541969299316
Batch 61/64 loss: -0.7204484939575195
Batch 62/64 loss: -0.5428633689880371
Batch 63/64 loss: -0.6023221015930176
Batch 64/64 loss: -4.098840713500977
Epoch 236  Train loss: -0.45106589373420264  Val loss: -0.8828947978331051
Saving best model, epoch: 236
Epoch 237
-------------------------------
Batch 1/64 loss: -0.538607120513916
Batch 2/64 loss: -0.5672931671142578
Batch 3/64 loss: -0.5659875869750977
Batch 4/64 loss: -0.6104812622070312
Batch 5/64 loss: -0.7161040306091309
Batch 6/64 loss: -0.596961498260498
Batch 7/64 loss: -0.6556487083435059
Batch 8/64 loss: -0.6949753761291504
Batch 9/64 loss: -0.7414159774780273
Batch 10/64 loss: -0.6394281387329102
Batch 11/64 loss: -0.600799560546875
Batch 12/64 loss: -0.12619924545288086
Batch 13/64 loss: 0.5355625152587891
Batch 14/64 loss: -0.47162485122680664
Batch 15/64 loss: -0.679234504699707
Batch 16/64 loss: -0.658665657043457
Batch 17/64 loss: -0.4857802391052246
Batch 18/64 loss: -0.6581120491027832
Batch 19/64 loss: 0.8445267677307129
Batch 20/64 loss: -0.6102046966552734
Batch 21/64 loss: -0.40848493576049805
Batch 22/64 loss: -0.5473556518554688
Batch 23/64 loss: -0.35747623443603516
Batch 24/64 loss: -0.5272259712219238
Batch 25/64 loss: -0.4791908264160156
Batch 26/64 loss: -0.05713319778442383
Batch 27/64 loss: -0.41672706604003906
Batch 28/64 loss: -0.3271365165710449
Batch 29/64 loss: -0.4034085273742676
Batch 30/64 loss: -0.6559925079345703
Batch 31/64 loss: -0.31434202194213867
Batch 32/64 loss: -0.5556950569152832
Batch 33/64 loss: -0.2657742500305176
Batch 34/64 loss: -0.6283516883850098
Batch 35/64 loss: -0.5919332504272461
Batch 36/64 loss: -0.3543519973754883
Batch 37/64 loss: -0.5152082443237305
Batch 38/64 loss: -0.5046291351318359
Batch 39/64 loss: -0.5141892433166504
Batch 40/64 loss: -0.5146632194519043
Batch 41/64 loss: -0.5848846435546875
Batch 42/64 loss: -0.6175065040588379
Batch 43/64 loss: -0.4404749870300293
Batch 44/64 loss: -0.6628909111022949
Batch 45/64 loss: -0.522468090057373
Batch 46/64 loss: -0.563194751739502
Batch 47/64 loss: -0.41033363342285156
Batch 48/64 loss: -0.708897590637207
Batch 49/64 loss: -0.3771839141845703
Batch 50/64 loss: -0.49242734909057617
Batch 51/64 loss: -0.19565773010253906
Batch 52/64 loss: -0.5951838493347168
Batch 53/64 loss: -0.5956826210021973
Batch 54/64 loss: -0.5525326728820801
Batch 55/64 loss: -0.2599492073059082
Batch 56/64 loss: -0.7210426330566406
Batch 57/64 loss: -0.43118810653686523
Batch 58/64 loss: 0.5843238830566406
Batch 59/64 loss: -0.14823532104492188
Batch 60/64 loss: -0.46462535858154297
Batch 61/64 loss: -0.5851335525512695
Batch 62/64 loss: -0.6309552192687988
Batch 63/64 loss: -0.5639839172363281
Batch 64/64 loss: -3.213947296142578
Epoch 237  Train loss: -0.4882709503173828  Val loss: -0.906528276266511
Saving best model, epoch: 237
Epoch 238
-------------------------------
Batch 1/64 loss: -0.533195972442627
Batch 2/64 loss: -0.4927997589111328
Batch 3/64 loss: -0.5124664306640625
Batch 4/64 loss: -0.7226576805114746
Batch 5/64 loss: -0.7071852684020996
Batch 6/64 loss: -0.6542863845825195
Batch 7/64 loss: -0.5191783905029297
Batch 8/64 loss: -0.48342370986938477
Batch 9/64 loss: -0.5354671478271484
Batch 10/64 loss: -0.6083431243896484
Batch 11/64 loss: -0.6475319862365723
Batch 12/64 loss: -0.7101569175720215
Batch 13/64 loss: -0.5622916221618652
Batch 14/64 loss: -0.3581967353820801
Batch 15/64 loss: -0.5102758407592773
Batch 16/64 loss: 0.4277505874633789
Batch 17/64 loss: -0.4490079879760742
Batch 18/64 loss: -0.3294806480407715
Batch 19/64 loss: -0.21117019653320312
Batch 20/64 loss: -0.5996637344360352
Batch 21/64 loss: -0.6764278411865234
Batch 22/64 loss: -0.5568366050720215
Batch 23/64 loss: -0.5748004913330078
Batch 24/64 loss: -0.6311130523681641
Batch 25/64 loss: -0.47486162185668945
Batch 26/64 loss: -0.38272953033447266
Batch 27/64 loss: -0.5203976631164551
Batch 28/64 loss: -0.5827531814575195
Batch 29/64 loss: -0.560905933380127
Batch 30/64 loss: -0.6888008117675781
Batch 31/64 loss: -0.6843805313110352
Batch 32/64 loss: -0.48613786697387695
Batch 33/64 loss: -0.6536931991577148
Batch 34/64 loss: -0.5312933921813965
Batch 35/64 loss: 0.5300483703613281
Batch 36/64 loss: 0.14127588272094727
Batch 37/64 loss: -0.6516604423522949
Batch 38/64 loss: -0.41709375381469727
Batch 39/64 loss: 0.2408604621887207
Batch 40/64 loss: 0.8256306648254395
Batch 41/64 loss: -0.5708222389221191
Batch 42/64 loss: -0.6119794845581055
Batch 43/64 loss: 0.049166202545166016
Batch 44/64 loss: 0.08534717559814453
Batch 45/64 loss: -0.6135177612304688
Batch 46/64 loss: -0.612419605255127
Batch 47/64 loss: -0.5835661888122559
Batch 48/64 loss: -0.20018434524536133
Batch 49/64 loss: -0.5173497200012207
Batch 50/64 loss: -0.5155706405639648
Batch 51/64 loss: -0.5094089508056641
Batch 52/64 loss: -0.608522891998291
Batch 53/64 loss: -0.5672378540039062
Batch 54/64 loss: -0.2456517219543457
Batch 55/64 loss: -0.5040311813354492
Batch 56/64 loss: -0.4586811065673828
Batch 57/64 loss: -0.4643573760986328
Batch 58/64 loss: -0.1409597396850586
Batch 59/64 loss: -0.37679290771484375
Batch 60/64 loss: -0.44184112548828125
Batch 61/64 loss: -0.4033493995666504
Batch 62/64 loss: -0.3545041084289551
Batch 63/64 loss: -0.5294699668884277
Batch 64/64 loss: -4.115458011627197
Epoch 238  Train loss: -0.46803761463539273  Val loss: -0.6804233039777303
Epoch 239
-------------------------------
Batch 1/64 loss: -0.5282659530639648
Batch 2/64 loss: -0.5109672546386719
Batch 3/64 loss: -0.6327834129333496
Batch 4/64 loss: 0.8502392768859863
Batch 5/64 loss: -0.3986701965332031
Batch 6/64 loss: -0.3841590881347656
Batch 7/64 loss: -0.6453557014465332
Batch 8/64 loss: -0.5837745666503906
Batch 9/64 loss: -0.3895387649536133
Batch 10/64 loss: -0.03950023651123047
Batch 11/64 loss: -0.49804210662841797
Batch 12/64 loss: -0.45369911193847656
Batch 13/64 loss: -0.45375919342041016
Batch 14/64 loss: -0.5975756645202637
Batch 15/64 loss: -0.46677112579345703
Batch 16/64 loss: -0.34373950958251953
Batch 17/64 loss: -0.1582350730895996
Batch 18/64 loss: -0.3650798797607422
Batch 19/64 loss: -0.6620197296142578
Batch 20/64 loss: -0.6291656494140625
Batch 21/64 loss: -0.6741704940795898
Batch 22/64 loss: 0.17718267440795898
Batch 23/64 loss: -0.5840978622436523
Batch 24/64 loss: -0.5508089065551758
Batch 25/64 loss: -0.4023256301879883
Batch 26/64 loss: -0.5417532920837402
Batch 27/64 loss: -0.5823063850402832
Batch 28/64 loss: -0.6106610298156738
Batch 29/64 loss: 1.0164599418640137
Batch 30/64 loss: -0.24954700469970703
Batch 31/64 loss: -0.1691293716430664
Batch 32/64 loss: 0.4630751609802246
Batch 33/64 loss: 0.7642202377319336
Batch 34/64 loss: 0.49270009994506836
Batch 35/64 loss: 0.24402618408203125
Batch 36/64 loss: 1.3628759384155273
Batch 37/64 loss: 1.4449262619018555
Batch 38/64 loss: 0.7835445404052734
Batch 39/64 loss: 0.9276447296142578
Batch 40/64 loss: 0.4804682731628418
Batch 41/64 loss: 1.1702594757080078
Batch 42/64 loss: 0.5938587188720703
Batch 43/64 loss: 0.5659575462341309
Batch 44/64 loss: 0.5927233695983887
Batch 45/64 loss: 0.9902133941650391
Batch 46/64 loss: 0.38770151138305664
Batch 47/64 loss: 1.4047904014587402
Batch 48/64 loss: 0.8103861808776855
Batch 49/64 loss: 0.7964491844177246
Batch 50/64 loss: 0.3152174949645996
Batch 51/64 loss: 0.1698150634765625
Batch 52/64 loss: 0.796907901763916
Batch 53/64 loss: 0.6552162170410156
Batch 54/64 loss: -0.17028188705444336
Batch 55/64 loss: 0.3871736526489258
Batch 56/64 loss: 0.9857625961303711
Batch 57/64 loss: 0.7258186340332031
Batch 58/64 loss: 0.28772687911987305
Batch 59/64 loss: 0.18765640258789062
Batch 60/64 loss: 0.1489238739013672
Batch 61/64 loss: 0.09551239013671875
Batch 62/64 loss: 0.0032062530517578125
Batch 63/64 loss: 0.2955055236816406
Batch 64/64 loss: -3.698141098022461
Epoch 239  Train loss: 0.0835193110447304  Val loss: -0.11912319012933581
Epoch 240
-------------------------------
Batch 1/64 loss: -0.06974506378173828
Batch 2/64 loss: 0.48252344131469727
Batch 3/64 loss: 1.2135858535766602
Batch 4/64 loss: 0.25617218017578125
Batch 5/64 loss: 0.11648988723754883
Batch 6/64 loss: 0.0971827507019043
Batch 7/64 loss: 0.3120570182800293
Batch 8/64 loss: 0.15308713912963867
Batch 9/64 loss: 0.024381637573242188
Batch 10/64 loss: 0.1105198860168457
Batch 11/64 loss: -0.14942264556884766
Batch 12/64 loss: 0.7144851684570312
Batch 13/64 loss: -0.01694011688232422
Batch 14/64 loss: 0.022251129150390625
Batch 15/64 loss: -0.07538843154907227
Batch 16/64 loss: 0.5272989273071289
Batch 17/64 loss: -0.08144664764404297
Batch 18/64 loss: 0.2196636199951172
Batch 19/64 loss: -0.20743131637573242
Batch 20/64 loss: 0.11036062240600586
Batch 21/64 loss: -0.1940751075744629
Batch 22/64 loss: -0.14117002487182617
Batch 23/64 loss: -0.24254560470581055
Batch 24/64 loss: -0.10460376739501953
Batch 25/64 loss: -0.09443092346191406
Batch 26/64 loss: -0.23537111282348633
Batch 27/64 loss: -0.29423999786376953
Batch 28/64 loss: -0.3101015090942383
Batch 29/64 loss: -0.286893367767334
Batch 30/64 loss: 0.013454914093017578
Batch 31/64 loss: -0.11405038833618164
Batch 32/64 loss: -0.07242918014526367
Batch 33/64 loss: -0.4319453239440918
Batch 34/64 loss: 1.0519490242004395
Batch 35/64 loss: -0.106170654296875
Batch 36/64 loss: 0.05194664001464844
Batch 37/64 loss: -0.261568546295166
Batch 38/64 loss: -0.27922487258911133
Batch 39/64 loss: -0.1110544204711914
Batch 40/64 loss: -0.20906448364257812
Batch 41/64 loss: -0.21856975555419922
Batch 42/64 loss: 0.14991474151611328
Batch 43/64 loss: -0.1708083152770996
Batch 44/64 loss: 1.1902236938476562
Batch 45/64 loss: -0.13886022567749023
Batch 46/64 loss: -0.38793134689331055
Batch 47/64 loss: -0.30373477935791016
Batch 48/64 loss: -0.19357967376708984
Batch 49/64 loss: -0.45057153701782227
Batch 50/64 loss: -0.2190556526184082
Batch 51/64 loss: -0.31459999084472656
Batch 52/64 loss: -0.1633157730102539
Batch 53/64 loss: -0.2633657455444336
Batch 54/64 loss: -0.32041406631469727
Batch 55/64 loss: -0.32803916931152344
Batch 56/64 loss: -0.38145875930786133
Batch 57/64 loss: -0.22681474685668945
Batch 58/64 loss: -0.28311681747436523
Batch 59/64 loss: -0.08954763412475586
Batch 60/64 loss: -0.1658949851989746
Batch 61/64 loss: -0.19606924057006836
Batch 62/64 loss: -0.3066067695617676
Batch 63/64 loss: -0.02961111068725586
Batch 64/64 loss: -3.4851412773132324
Epoch 240  Train loss: -0.07902097702026367  Val loss: -0.20755075671009182
Epoch 241
-------------------------------
Batch 1/64 loss: -0.30533456802368164
Batch 2/64 loss: 0.1393289566040039
Batch 3/64 loss: -0.2873959541320801
Batch 4/64 loss: 0.37434911727905273
Batch 5/64 loss: -0.10111474990844727
Batch 6/64 loss: -0.1166839599609375
Batch 7/64 loss: -0.03752613067626953
Batch 8/64 loss: 0.6960134506225586
Batch 9/64 loss: -0.20510149002075195
Batch 10/64 loss: 0.3524947166442871
Batch 11/64 loss: -0.025094032287597656
Batch 12/64 loss: -0.19841480255126953
Batch 13/64 loss: -0.2549424171447754
Batch 14/64 loss: -0.09701251983642578
Batch 15/64 loss: -0.22748756408691406
Batch 16/64 loss: 0.3988194465637207
Batch 17/64 loss: 1.4189114570617676
Batch 18/64 loss: -0.4018430709838867
Batch 19/64 loss: -0.41809701919555664
Batch 20/64 loss: -0.33347272872924805
Batch 21/64 loss: -0.4765758514404297
Batch 22/64 loss: -0.41469478607177734
Batch 23/64 loss: -0.3525981903076172
Batch 24/64 loss: -0.295682430267334
Batch 25/64 loss: 0.32841014862060547
Batch 26/64 loss: -0.24874067306518555
Batch 27/64 loss: -0.49383974075317383
Batch 28/64 loss: -0.12175178527832031
Batch 29/64 loss: -0.35575294494628906
Batch 30/64 loss: -0.2650423049926758
Batch 31/64 loss: 0.2847480773925781
Batch 32/64 loss: 0.04994010925292969
Batch 33/64 loss: -0.1748495101928711
Batch 34/64 loss: -0.2917814254760742
Batch 35/64 loss: -0.17801189422607422
Batch 36/64 loss: -0.48067569732666016
Batch 37/64 loss: 0.3553647994995117
Batch 38/64 loss: -0.3545246124267578
Batch 39/64 loss: -0.4587392807006836
Batch 40/64 loss: -0.4498929977416992
Batch 41/64 loss: -0.018883705139160156
Batch 42/64 loss: -0.21846771240234375
Batch 43/64 loss: 0.0018434524536132812
Batch 44/64 loss: -0.3315305709838867
Batch 45/64 loss: 0.4356088638305664
Batch 46/64 loss: -0.5271611213684082
Batch 47/64 loss: -0.1846446990966797
Batch 48/64 loss: -0.22577714920043945
Batch 49/64 loss: -0.3645615577697754
Batch 50/64 loss: -0.3033018112182617
Batch 51/64 loss: 1.591115951538086
Batch 52/64 loss: -0.29936838150024414
Batch 53/64 loss: 0.06100940704345703
Batch 54/64 loss: -0.17972326278686523
Batch 55/64 loss: -0.40152835845947266
Batch 56/64 loss: 0.4113621711730957
Batch 57/64 loss: 0.3970308303833008
Batch 58/64 loss: 0.0830221176147461
Batch 59/64 loss: 0.09148263931274414
Batch 60/64 loss: 0.6116857528686523
Batch 61/64 loss: 0.19962549209594727
Batch 62/64 loss: 0.015868186950683594
Batch 63/64 loss: -0.016244888305664062
Batch 64/64 loss: -3.1394906044006348
Epoch 241  Train loss: -0.08706590054081935  Val loss: 0.3416023909840797
Epoch 242
-------------------------------
Batch 1/64 loss: 0.2998695373535156
Batch 2/64 loss: -0.03302288055419922
Batch 3/64 loss: 0.1500535011291504
Batch 4/64 loss: 0.12043952941894531
Batch 5/64 loss: 0.06905508041381836
Batch 6/64 loss: -0.1334834098815918
Batch 7/64 loss: 0.1542644500732422
Batch 8/64 loss: 0.04760551452636719
Batch 9/64 loss: -0.06070089340209961
Batch 10/64 loss: -0.2440171241760254
Batch 11/64 loss: -0.13675689697265625
Batch 12/64 loss: 0.1142115592956543
Batch 13/64 loss: 0.3881855010986328
Batch 14/64 loss: 1.1635956764221191
Batch 15/64 loss: -0.28787899017333984
Batch 16/64 loss: 0.3485445976257324
Batch 17/64 loss: -0.059042930603027344
Batch 18/64 loss: 0.35723400115966797
Batch 19/64 loss: -0.3286576271057129
Batch 20/64 loss: 0.048098087310791016
Batch 21/64 loss: 0.1020822525024414
Batch 22/64 loss: -0.2778940200805664
Batch 23/64 loss: -0.20279502868652344
Batch 24/64 loss: -0.17474842071533203
Batch 25/64 loss: 0.30486392974853516
Batch 26/64 loss: 0.22844409942626953
Batch 27/64 loss: -0.11878538131713867
Batch 28/64 loss: -0.3236546516418457
Batch 29/64 loss: -0.2627391815185547
Batch 30/64 loss: -0.015425682067871094
Batch 31/64 loss: -0.15241003036499023
Batch 32/64 loss: 0.19196653366088867
Batch 33/64 loss: 0.8867702484130859
Batch 34/64 loss: 0.8929119110107422
Batch 35/64 loss: -0.310330867767334
Batch 36/64 loss: -0.2913169860839844
Batch 37/64 loss: -0.23906946182250977
Batch 38/64 loss: -0.38107776641845703
Batch 39/64 loss: 0.00026988983154296875
Batch 40/64 loss: -0.30153799057006836
Batch 41/64 loss: -0.3198232650756836
Batch 42/64 loss: -0.3614387512207031
Batch 43/64 loss: 0.29615259170532227
Batch 44/64 loss: -0.2133021354675293
Batch 45/64 loss: -0.3303103446960449
Batch 46/64 loss: 0.38719749450683594
Batch 47/64 loss: -0.42339134216308594
Batch 48/64 loss: -0.30819272994995117
Batch 49/64 loss: -0.31963539123535156
Batch 50/64 loss: 0.28929853439331055
Batch 51/64 loss: 0.8709511756896973
Batch 52/64 loss: -0.2960233688354492
Batch 53/64 loss: -0.3741159439086914
Batch 54/64 loss: -0.23468255996704102
Batch 55/64 loss: -0.2501511573791504
Batch 56/64 loss: -0.3434910774230957
Batch 57/64 loss: 0.0595393180847168
Batch 58/64 loss: -0.13851356506347656
Batch 59/64 loss: -0.17591238021850586
Batch 60/64 loss: -0.30680370330810547
Batch 61/64 loss: -0.4251894950866699
Batch 62/64 loss: -0.06654691696166992
Batch 63/64 loss: 0.7122898101806641
Batch 64/64 loss: -3.8572640419006348
Epoch 242  Train loss: -0.056971349903181485  Val loss: -0.5586156943409714
Epoch 243
-------------------------------
Batch 1/64 loss: -0.06754827499389648
Batch 2/64 loss: -0.29270172119140625
Batch 3/64 loss: -0.44524669647216797
Batch 4/64 loss: 0.8305578231811523
Batch 5/64 loss: 0.15039777755737305
Batch 6/64 loss: -0.46437501907348633
Batch 7/64 loss: -0.36970996856689453
Batch 8/64 loss: -0.4057002067565918
Batch 9/64 loss: -0.24913644790649414
Batch 10/64 loss: -0.3772144317626953
Batch 11/64 loss: -0.035840511322021484
Batch 12/64 loss: -0.37707042694091797
Batch 13/64 loss: -0.4090433120727539
Batch 14/64 loss: -0.37584590911865234
Batch 15/64 loss: -0.29730987548828125
Batch 16/64 loss: 0.026096820831298828
Batch 17/64 loss: -0.370086669921875
Batch 18/64 loss: -0.39859485626220703
Batch 19/64 loss: -0.4664034843444824
Batch 20/64 loss: -0.15063190460205078
Batch 21/64 loss: 0.04131937026977539
Batch 22/64 loss: -0.10848093032836914
Batch 23/64 loss: 0.5559291839599609
Batch 24/64 loss: -0.4475383758544922
Batch 25/64 loss: -0.4763932228088379
Batch 26/64 loss: -0.5854678153991699
Batch 27/64 loss: -0.18913698196411133
Batch 28/64 loss: -0.45532703399658203
Batch 29/64 loss: -0.3437013626098633
Batch 30/64 loss: 0.0028443336486816406
Batch 31/64 loss: -0.2526426315307617
Batch 32/64 loss: -0.5621228218078613
Batch 33/64 loss: -0.4622330665588379
Batch 34/64 loss: -0.3894367218017578
Batch 35/64 loss: -0.41916847229003906
Batch 36/64 loss: 0.39336681365966797
Batch 37/64 loss: -0.3582444190979004
Batch 38/64 loss: -0.44828367233276367
Batch 39/64 loss: -0.1685328483581543
Batch 40/64 loss: -0.2942042350769043
Batch 41/64 loss: -0.3081955909729004
Batch 42/64 loss: -0.5643725395202637
Batch 43/64 loss: -0.34734392166137695
Batch 44/64 loss: -0.31368494033813477
Batch 45/64 loss: -0.2532963752746582
Batch 46/64 loss: -0.5301337242126465
Batch 47/64 loss: -0.20213937759399414
Batch 48/64 loss: 0.03541851043701172
Batch 49/64 loss: -0.43065881729125977
Batch 50/64 loss: -0.5030417442321777
Batch 51/64 loss: -0.3021531105041504
Batch 52/64 loss: -0.5582079887390137
Batch 53/64 loss: 0.26666784286499023
Batch 54/64 loss: -0.1800684928894043
Batch 55/64 loss: -0.4556894302368164
Batch 56/64 loss: -0.5961785316467285
Batch 57/64 loss: 0.8452396392822266
Batch 58/64 loss: -0.5498552322387695
Batch 59/64 loss: -0.44304990768432617
Batch 60/64 loss: -0.5810518264770508
Batch 61/64 loss: -0.4064188003540039
Batch 62/64 loss: -0.6034998893737793
Batch 63/64 loss: -0.46625709533691406
Batch 64/64 loss: -3.9866156578063965
Epoch 243  Train loss: -0.3129536516526166  Val loss: -0.7270565098503611
Epoch 244
-------------------------------
Batch 1/64 loss: -0.08686256408691406
Batch 2/64 loss: -0.31232166290283203
Batch 3/64 loss: -0.23475217819213867
Batch 4/64 loss: -0.18655014038085938
Batch 5/64 loss: -0.4541306495666504
Batch 6/64 loss: 0.06920957565307617
Batch 7/64 loss: -0.5437936782836914
Batch 8/64 loss: -0.10377645492553711
Batch 9/64 loss: -0.5786123275756836
Batch 10/64 loss: -0.41376733779907227
Batch 11/64 loss: -0.11860370635986328
Batch 12/64 loss: -0.370025634765625
Batch 13/64 loss: -0.4112367630004883
Batch 14/64 loss: -0.2885575294494629
Batch 15/64 loss: -0.48603343963623047
Batch 16/64 loss: -0.4981260299682617
Batch 17/64 loss: -0.5427103042602539
Batch 18/64 loss: -0.3502168655395508
Batch 19/64 loss: -0.6815047264099121
Batch 20/64 loss: -0.4888272285461426
Batch 21/64 loss: -0.6007704734802246
Batch 22/64 loss: -0.1119394302368164
Batch 23/64 loss: -0.4703068733215332
Batch 24/64 loss: -0.35246944427490234
Batch 25/64 loss: -0.48448657989501953
Batch 26/64 loss: -0.5930061340332031
Batch 27/64 loss: -0.3699793815612793
Batch 28/64 loss: -0.5338168144226074
Batch 29/64 loss: -0.44878530502319336
Batch 30/64 loss: -0.5224580764770508
Batch 31/64 loss: -0.4284186363220215
Batch 32/64 loss: -0.6010966300964355
Batch 33/64 loss: 0.8979182243347168
Batch 34/64 loss: -0.586127758026123
Batch 35/64 loss: -0.39067602157592773
Batch 36/64 loss: -0.48706626892089844
Batch 37/64 loss: -0.6340699195861816
Batch 38/64 loss: -0.5629010200500488
Batch 39/64 loss: -0.3323631286621094
Batch 40/64 loss: -0.4688291549682617
Batch 41/64 loss: -0.4542546272277832
Batch 42/64 loss: -0.5995278358459473
Batch 43/64 loss: -0.45286035537719727
Batch 44/64 loss: -0.5918245315551758
Batch 45/64 loss: 0.8430643081665039
Batch 46/64 loss: -0.47419166564941406
Batch 47/64 loss: -0.718630313873291
Batch 48/64 loss: -0.6599559783935547
Batch 49/64 loss: -0.5799798965454102
Batch 50/64 loss: -0.6364736557006836
Batch 51/64 loss: 0.4819021224975586
Batch 52/64 loss: -0.28516054153442383
Batch 53/64 loss: -0.2115488052368164
Batch 54/64 loss: 0.03270101547241211
Batch 55/64 loss: -0.45923328399658203
Batch 56/64 loss: -0.648735523223877
Batch 57/64 loss: -0.42819643020629883
Batch 58/64 loss: -0.37497854232788086
Batch 59/64 loss: -0.6652626991271973
Batch 60/64 loss: -0.5428495407104492
Batch 61/64 loss: -0.5230607986450195
Batch 62/64 loss: -0.4323453903198242
Batch 63/64 loss: -0.5235700607299805
Batch 64/64 loss: -4.1397705078125
Epoch 244  Train loss: -0.42623763738893994  Val loss: -0.6214338872850556
Epoch 245
-------------------------------
Batch 1/64 loss: 1.0972075462341309
Batch 2/64 loss: -0.2511134147644043
Batch 3/64 loss: -0.5452322959899902
Batch 4/64 loss: -0.4374089241027832
Batch 5/64 loss: -0.457003116607666
Batch 6/64 loss: -0.3908200263977051
Batch 7/64 loss: 1.3871216773986816
Batch 8/64 loss: -0.3772110939025879
Batch 9/64 loss: -0.43005847930908203
Batch 10/64 loss: -0.5950460433959961
Batch 11/64 loss: -0.18019866943359375
Batch 12/64 loss: -0.3472447395324707
Batch 13/64 loss: -0.6540312767028809
Batch 14/64 loss: -0.44898080825805664
Batch 15/64 loss: -0.4982309341430664
Batch 16/64 loss: -0.4446544647216797
Batch 17/64 loss: -0.48662281036376953
Batch 18/64 loss: -0.21701335906982422
Batch 19/64 loss: -0.5100240707397461
Batch 20/64 loss: -0.04758882522583008
Batch 21/64 loss: -0.35344934463500977
Batch 22/64 loss: -0.3828544616699219
Batch 23/64 loss: -0.06526803970336914
Batch 24/64 loss: -0.4455571174621582
Batch 25/64 loss: -0.49413061141967773
Batch 26/64 loss: -0.6508255004882812
Batch 27/64 loss: -0.3274569511413574
Batch 28/64 loss: 0.07292509078979492
Batch 29/64 loss: -0.5468502044677734
Batch 30/64 loss: -0.5153374671936035
Batch 31/64 loss: -0.5655217170715332
Batch 32/64 loss: -0.539252758026123
Batch 33/64 loss: -0.5453453063964844
Batch 34/64 loss: -0.4982566833496094
Batch 35/64 loss: -0.3906540870666504
Batch 36/64 loss: -0.21072149276733398
Batch 37/64 loss: -0.4794750213623047
Batch 38/64 loss: 0.3657832145690918
Batch 39/64 loss: -0.3125925064086914
Batch 40/64 loss: -0.4936671257019043
Batch 41/64 loss: -0.3535332679748535
Batch 42/64 loss: -0.6066560745239258
Batch 43/64 loss: -0.5952582359313965
Batch 44/64 loss: -0.09457254409790039
Batch 45/64 loss: -0.499880313873291
Batch 46/64 loss: -0.3232426643371582
Batch 47/64 loss: 0.27440929412841797
Batch 48/64 loss: -0.626619815826416
Batch 49/64 loss: -0.5683488845825195
Batch 50/64 loss: -0.5267057418823242
Batch 51/64 loss: -0.4216451644897461
Batch 52/64 loss: -0.6484384536743164
Batch 53/64 loss: -0.5016212463378906
Batch 54/64 loss: -0.4224815368652344
Batch 55/64 loss: 0.447908878326416
Batch 56/64 loss: -0.47083282470703125
Batch 57/64 loss: -0.6280674934387207
Batch 58/64 loss: -0.20388126373291016
Batch 59/64 loss: -0.37280797958374023
Batch 60/64 loss: -0.5692958831787109
Batch 61/64 loss: -0.4202089309692383
Batch 62/64 loss: -0.3305821418762207
Batch 63/64 loss: -0.3995633125305176
Batch 64/64 loss: -4.05928373336792
Epoch 245  Train loss: -0.37833804897233553  Val loss: -0.7568511831801372
Epoch 246
-------------------------------
Batch 1/64 loss: -0.10283231735229492
Batch 2/64 loss: 0.15188169479370117
Batch 3/64 loss: -0.3247966766357422
Batch 4/64 loss: -0.5142054557800293
Batch 5/64 loss: -0.43381738662719727
Batch 6/64 loss: -0.40632009506225586
Batch 7/64 loss: -0.5164594650268555
Batch 8/64 loss: -0.46129512786865234
Batch 9/64 loss: -0.37798309326171875
Batch 10/64 loss: -0.31122350692749023
Batch 11/64 loss: -0.2911405563354492
Batch 12/64 loss: -0.46356678009033203
Batch 13/64 loss: -0.6016526222229004
Batch 14/64 loss: -0.41910266876220703
Batch 15/64 loss: -0.4646477699279785
Batch 16/64 loss: -0.5289764404296875
Batch 17/64 loss: -0.38629722595214844
Batch 18/64 loss: -0.5060830116271973
Batch 19/64 loss: 0.502711296081543
Batch 20/64 loss: -0.42298078536987305
Batch 21/64 loss: -0.3708820343017578
Batch 22/64 loss: -0.25188159942626953
Batch 23/64 loss: -0.20636272430419922
Batch 24/64 loss: -0.2422924041748047
Batch 25/64 loss: -0.40406322479248047
Batch 26/64 loss: -0.5164971351623535
Batch 27/64 loss: -0.45173025131225586
Batch 28/64 loss: -0.1326894760131836
Batch 29/64 loss: -0.1628708839416504
Batch 30/64 loss: -0.5625529289245605
Batch 31/64 loss: -0.5838956832885742
Batch 32/64 loss: -0.38523197174072266
Batch 33/64 loss: -0.5867738723754883
Batch 34/64 loss: -0.5881175994873047
Batch 35/64 loss: -0.5095367431640625
Batch 36/64 loss: -0.5768327713012695
Batch 37/64 loss: -0.3655128479003906
Batch 38/64 loss: -0.23184967041015625
Batch 39/64 loss: -0.49096202850341797
Batch 40/64 loss: 0.07896041870117188
Batch 41/64 loss: -0.5272483825683594
Batch 42/64 loss: -0.05298948287963867
Batch 43/64 loss: -0.544499397277832
Batch 44/64 loss: 0.2602367401123047
Batch 45/64 loss: -0.248565673828125
Batch 46/64 loss: -0.3087277412414551
Batch 47/64 loss: -0.30605649948120117
Batch 48/64 loss: 0.03752851486206055
Batch 49/64 loss: 0.26061010360717773
Batch 50/64 loss: 0.9658713340759277
Batch 51/64 loss: -0.31003522872924805
Batch 52/64 loss: 0.980858325958252
Batch 53/64 loss: 0.17064857482910156
Batch 54/64 loss: 0.16110944747924805
Batch 55/64 loss: 1.6241869926452637
Batch 56/64 loss: 1.645291805267334
Batch 57/64 loss: 0.4471464157104492
Batch 58/64 loss: -0.03240251541137695
Batch 59/64 loss: 0.3327479362487793
Batch 60/64 loss: 1.3068346977233887
Batch 61/64 loss: 0.14266109466552734
Batch 62/64 loss: 0.28341197967529297
Batch 63/64 loss: 0.17942142486572266
Batch 64/64 loss: -2.9302902221679688
Epoch 246  Train loss: -0.1592163235533471  Val loss: 0.07166992921599817
Epoch 247
-------------------------------
Batch 1/64 loss: 0.9521322250366211
Batch 2/64 loss: 0.19146156311035156
Batch 3/64 loss: -0.1871500015258789
Batch 4/64 loss: 0.668210506439209
Batch 5/64 loss: 0.4503774642944336
Batch 6/64 loss: 0.2408595085144043
Batch 7/64 loss: -0.11611223220825195
Batch 8/64 loss: -0.1362476348876953
Batch 9/64 loss: -0.12505483627319336
Batch 10/64 loss: 0.4713587760925293
Batch 11/64 loss: 0.2816448211669922
Batch 12/64 loss: 0.8533749580383301
Batch 13/64 loss: -0.013708114624023438
Batch 14/64 loss: -0.1629505157470703
Batch 15/64 loss: -0.16661691665649414
Batch 16/64 loss: -0.26133251190185547
Batch 17/64 loss: -0.08661890029907227
Batch 18/64 loss: -0.39633607864379883
Batch 19/64 loss: 0.08384180068969727
Batch 20/64 loss: -0.3147740364074707
Batch 21/64 loss: 0.24078941345214844
Batch 22/64 loss: -0.1489582061767578
Batch 23/64 loss: -0.39144420623779297
Batch 24/64 loss: -0.3641037940979004
Batch 25/64 loss: -0.37601566314697266
Batch 26/64 loss: -0.29115724563598633
Batch 27/64 loss: -0.2586088180541992
Batch 28/64 loss: -0.28415775299072266
Batch 29/64 loss: -0.33255624771118164
Batch 30/64 loss: -0.3135833740234375
Batch 31/64 loss: 0.18096351623535156
Batch 32/64 loss: -0.26525306701660156
Batch 33/64 loss: -0.4566359519958496
Batch 34/64 loss: -0.4382209777832031
Batch 35/64 loss: -0.10322809219360352
Batch 36/64 loss: -0.46288013458251953
Batch 37/64 loss: -0.3804149627685547
Batch 38/64 loss: -0.19471502304077148
Batch 39/64 loss: -0.4922914505004883
Batch 40/64 loss: -0.17315101623535156
Batch 41/64 loss: -0.23351669311523438
Batch 42/64 loss: -0.42437171936035156
Batch 43/64 loss: -0.43304967880249023
Batch 44/64 loss: -0.4737734794616699
Batch 45/64 loss: -0.462801456451416
Batch 46/64 loss: -0.46869897842407227
Batch 47/64 loss: -0.24523401260375977
Batch 48/64 loss: -0.3281679153442383
Batch 49/64 loss: -0.43120431900024414
Batch 50/64 loss: -0.3002042770385742
Batch 51/64 loss: -0.3376312255859375
Batch 52/64 loss: 0.07419776916503906
Batch 53/64 loss: -0.49792909622192383
Batch 54/64 loss: -0.4060935974121094
Batch 55/64 loss: 0.6842536926269531
Batch 56/64 loss: -0.2748990058898926
Batch 57/64 loss: -0.4247555732727051
Batch 58/64 loss: 1.1237201690673828
Batch 59/64 loss: -0.3800215721130371
Batch 60/64 loss: -0.2684054374694824
Batch 61/64 loss: -0.3736844062805176
Batch 62/64 loss: -0.0466923713684082
Batch 63/64 loss: -0.23243236541748047
Batch 64/64 loss: -3.7020010948181152
Epoch 247  Train loss: -0.1728181895087747  Val loss: -0.582738161906344
Epoch 248
-------------------------------
Batch 1/64 loss: -0.37996625900268555
Batch 2/64 loss: -0.18871784210205078
Batch 3/64 loss: -0.46123838424682617
Batch 4/64 loss: -0.44710493087768555
Batch 5/64 loss: -0.28307437896728516
Batch 6/64 loss: 1.0822124481201172
Batch 7/64 loss: -0.31903505325317383
Batch 8/64 loss: -0.4330415725708008
Batch 9/64 loss: -0.5233116149902344
Batch 10/64 loss: -0.4540543556213379
Batch 11/64 loss: -0.36710023880004883
Batch 12/64 loss: -0.2387704849243164
Batch 13/64 loss: -0.16539287567138672
Batch 14/64 loss: -0.530128002166748
Batch 15/64 loss: -0.32415294647216797
Batch 16/64 loss: -0.21400976181030273
Batch 17/64 loss: -0.4870171546936035
Batch 18/64 loss: -0.4779949188232422
Batch 19/64 loss: -0.4494028091430664
Batch 20/64 loss: -0.5159087181091309
Batch 21/64 loss: -0.355954647064209
Batch 22/64 loss: -0.26229333877563477
Batch 23/64 loss: -0.30820465087890625
Batch 24/64 loss: -0.39542198181152344
Batch 25/64 loss: -0.27998924255371094
Batch 26/64 loss: -0.32650327682495117
Batch 27/64 loss: 0.46771812438964844
Batch 28/64 loss: -0.4785952568054199
Batch 29/64 loss: -0.29038286209106445
Batch 30/64 loss: 0.13713407516479492
Batch 31/64 loss: -0.37154150009155273
Batch 32/64 loss: -0.5071539878845215
Batch 33/64 loss: -0.46692419052124023
Batch 34/64 loss: -0.3906540870666504
Batch 35/64 loss: -0.4535684585571289
Batch 36/64 loss: 0.26789331436157227
Batch 37/64 loss: 0.05977010726928711
Batch 38/64 loss: -0.4637131690979004
Batch 39/64 loss: -0.28459978103637695
Batch 40/64 loss: -0.2916851043701172
Batch 41/64 loss: -0.2657499313354492
Batch 42/64 loss: -0.46182727813720703
Batch 43/64 loss: -0.1691431999206543
Batch 44/64 loss: -0.46935081481933594
Batch 45/64 loss: -0.3357505798339844
Batch 46/64 loss: -0.1725454330444336
Batch 47/64 loss: -0.37421178817749023
Batch 48/64 loss: -0.34810352325439453
Batch 49/64 loss: -0.25595664978027344
Batch 50/64 loss: -0.39526987075805664
Batch 51/64 loss: -0.45978546142578125
Batch 52/64 loss: -0.20667791366577148
Batch 53/64 loss: -0.39055728912353516
Batch 54/64 loss: -0.47266244888305664
Batch 55/64 loss: -0.3693656921386719
Batch 56/64 loss: 0.4554901123046875
Batch 57/64 loss: -0.08991241455078125
Batch 58/64 loss: -0.3413815498352051
Batch 59/64 loss: -0.06448173522949219
Batch 60/64 loss: -0.33312225341796875
Batch 61/64 loss: -0.5321793556213379
Batch 62/64 loss: -0.45131683349609375
Batch 63/64 loss: -0.35935163497924805
Batch 64/64 loss: -3.9788684844970703
Epoch 248  Train loss: -0.32971364189596736  Val loss: -0.6892182392762699
Epoch 249
-------------------------------
Batch 1/64 loss: -0.35198020935058594
Batch 2/64 loss: -0.4782257080078125
Batch 3/64 loss: 0.5474810600280762
Batch 4/64 loss: -0.4534463882446289
Batch 5/64 loss: -0.39194631576538086
Batch 6/64 loss: -0.5547170639038086
Batch 7/64 loss: -0.5823159217834473
Batch 8/64 loss: -0.050356388092041016
Batch 9/64 loss: -0.20162534713745117
Batch 10/64 loss: -0.4705543518066406
Batch 11/64 loss: -0.2887392044067383
Batch 12/64 loss: -0.13575172424316406
Batch 13/64 loss: -0.4440193176269531
Batch 14/64 loss: 0.6222491264343262
Batch 15/64 loss: -0.44780874252319336
Batch 16/64 loss: -0.4663252830505371
Batch 17/64 loss: -0.3804030418395996
Batch 18/64 loss: -0.4949150085449219
Batch 19/64 loss: -0.6040525436401367
Batch 20/64 loss: -0.6555037498474121
Batch 21/64 loss: -0.27603626251220703
Batch 22/64 loss: -0.23154735565185547
Batch 23/64 loss: -0.4423942565917969
Batch 24/64 loss: -0.5301027297973633
Batch 25/64 loss: -0.6379938125610352
Batch 26/64 loss: -0.5123081207275391
Batch 27/64 loss: 1.0349440574645996
Batch 28/64 loss: -0.5433669090270996
Batch 29/64 loss: -0.3747591972351074
Batch 30/64 loss: -0.42361879348754883
Batch 31/64 loss: -0.48932361602783203
Batch 32/64 loss: -0.5773453712463379
Batch 33/64 loss: -0.11762809753417969
Batch 34/64 loss: -0.06942176818847656
Batch 35/64 loss: -0.5142302513122559
Batch 36/64 loss: -0.5424189567565918
Batch 37/64 loss: -0.5160732269287109
Batch 38/64 loss: -0.5471282005310059
Batch 39/64 loss: -0.4877805709838867
Batch 40/64 loss: -0.34624671936035156
Batch 41/64 loss: -0.49216365814208984
Batch 42/64 loss: -0.675811767578125
Batch 43/64 loss: -0.4514641761779785
Batch 44/64 loss: -0.5192790031433105
Batch 45/64 loss: -0.6125874519348145
Batch 46/64 loss: -0.5817480087280273
Batch 47/64 loss: 0.043384552001953125
Batch 48/64 loss: -0.5724358558654785
Batch 49/64 loss: -0.6705121994018555
Batch 50/64 loss: -0.47381114959716797
Batch 51/64 loss: -0.5433096885681152
Batch 52/64 loss: -0.01212167739868164
Batch 53/64 loss: -0.545198917388916
Batch 54/64 loss: -0.5955057144165039
Batch 55/64 loss: -0.44205665588378906
Batch 56/64 loss: -0.5336089134216309
Batch 57/64 loss: -0.6198039054870605
Batch 58/64 loss: -0.47713422775268555
Batch 59/64 loss: -0.5136170387268066
Batch 60/64 loss: -0.5798683166503906
Batch 61/64 loss: -0.48540306091308594
Batch 62/64 loss: -0.4990234375
Batch 63/64 loss: -0.5257534980773926
Batch 64/64 loss: -4.066473960876465
Epoch 249  Train loss: -0.43696353762757545  Val loss: -0.7590012173472401
Epoch 250
-------------------------------
Batch 1/64 loss: -0.5446152687072754
Batch 2/64 loss: -0.3280301094055176
Batch 3/64 loss: -0.3795356750488281
Batch 4/64 loss: -0.2585878372192383
Batch 5/64 loss: -0.6742682456970215
Batch 6/64 loss: -0.6720948219299316
Batch 7/64 loss: -0.24665355682373047
Batch 8/64 loss: -0.5345568656921387
Batch 9/64 loss: -0.44737911224365234
Batch 10/64 loss: -0.5844173431396484
Batch 11/64 loss: -0.5446987152099609
Batch 12/64 loss: -0.4984893798828125
Batch 13/64 loss: -0.5107946395874023
Batch 14/64 loss: -0.5477485656738281
Batch 15/64 loss: -0.5331521034240723
Batch 16/64 loss: -0.15346527099609375
Batch 17/64 loss: -0.46074438095092773
Batch 18/64 loss: 0.8605785369873047
Batch 19/64 loss: -0.3772702217102051
Batch 20/64 loss: -0.4806852340698242
Batch 21/64 loss: -0.31096363067626953
Batch 22/64 loss: -0.3017549514770508
Batch 23/64 loss: -0.4980583190917969
Batch 24/64 loss: -0.5398163795471191
Batch 25/64 loss: -0.4430270195007324
Batch 26/64 loss: -0.06627321243286133
Batch 27/64 loss: -0.3161773681640625
Batch 28/64 loss: -0.011601924896240234
Batch 29/64 loss: 0.06289911270141602
Batch 30/64 loss: -0.5163722038269043
Batch 31/64 loss: 1.2021780014038086
Batch 32/64 loss: -0.5942220687866211
Batch 33/64 loss: -0.5351877212524414
Batch 34/64 loss: -0.5001039505004883
Batch 35/64 loss: -0.5509963035583496
Batch 36/64 loss: -0.5278034210205078
Batch 37/64 loss: 0.9676566123962402
Batch 38/64 loss: -0.43977832794189453
Batch 39/64 loss: -0.5289602279663086
Batch 40/64 loss: 0.48980045318603516
Batch 41/64 loss: -0.48113012313842773
Batch 42/64 loss: -0.5932679176330566
Batch 43/64 loss: -0.48316097259521484
Batch 44/64 loss: -0.40739870071411133
Batch 45/64 loss: -0.2891359329223633
Batch 46/64 loss: -0.4081435203552246
Batch 47/64 loss: -0.5880818367004395
Batch 48/64 loss: -0.5205512046813965
Batch 49/64 loss: -0.299410343170166
Batch 50/64 loss: -0.21825408935546875
Batch 51/64 loss: -0.268585205078125
Batch 52/64 loss: -0.473635196685791
Batch 53/64 loss: -0.5296826362609863
Batch 54/64 loss: -0.16994476318359375
Batch 55/64 loss: -0.5784344673156738
Batch 56/64 loss: -0.5408601760864258
Batch 57/64 loss: -0.45146608352661133
Batch 58/64 loss: -0.32753801345825195
Batch 59/64 loss: -0.4076871871948242
Batch 60/64 loss: -0.44974184036254883
Batch 61/64 loss: -0.43441152572631836
Batch 62/64 loss: -0.539618968963623
Batch 63/64 loss: -0.6499571800231934
Batch 64/64 loss: -4.135431289672852
Epoch 250  Train loss: -0.39351910609824986  Val loss: -0.7299575019128544
Epoch 251
-------------------------------
Batch 1/64 loss: -0.5313315391540527
Batch 2/64 loss: -0.22327375411987305
Batch 3/64 loss: -0.5163350105285645
Batch 4/64 loss: -0.5405268669128418
Batch 5/64 loss: -0.5419669151306152
Batch 6/64 loss: -0.2934880256652832
Batch 7/64 loss: -0.30727338790893555
Batch 8/64 loss: 0.7288241386413574
Batch 9/64 loss: -0.5032920837402344
Batch 10/64 loss: -0.5138721466064453
Batch 11/64 loss: -0.25132131576538086
Batch 12/64 loss: -0.367154598236084
Batch 13/64 loss: -0.41778087615966797
Batch 14/64 loss: -0.48847293853759766
Batch 15/64 loss: -0.4903392791748047
Batch 16/64 loss: -0.5102353096008301
Batch 17/64 loss: -0.6705093383789062
Batch 18/64 loss: -0.4730243682861328
Batch 19/64 loss: -0.33884620666503906
Batch 20/64 loss: 0.3885831832885742
Batch 21/64 loss: -0.04472827911376953
Batch 22/64 loss: -0.33406734466552734
Batch 23/64 loss: -0.47446107864379883
Batch 24/64 loss: -0.45761823654174805
Batch 25/64 loss: -0.6197648048400879
Batch 26/64 loss: 0.49363136291503906
Batch 27/64 loss: -0.5938692092895508
Batch 28/64 loss: -0.4758930206298828
Batch 29/64 loss: 0.048442840576171875
Batch 30/64 loss: -0.5347437858581543
Batch 31/64 loss: -0.3165445327758789
Batch 32/64 loss: -0.40755748748779297
Batch 33/64 loss: -0.5981483459472656
Batch 34/64 loss: -0.5574684143066406
Batch 35/64 loss: -0.5136861801147461
Batch 36/64 loss: -0.305941104888916
Batch 37/64 loss: -0.592750072479248
Batch 38/64 loss: -0.6717963218688965
Batch 39/64 loss: -0.6550412178039551
Batch 40/64 loss: -0.25621652603149414
Batch 41/64 loss: -0.38934755325317383
Batch 42/64 loss: -0.5438756942749023
Batch 43/64 loss: -0.49973297119140625
Batch 44/64 loss: -0.6243829727172852
Batch 45/64 loss: -0.450314998626709
Batch 46/64 loss: -0.6060667037963867
Batch 47/64 loss: -0.45470571517944336
Batch 48/64 loss: -0.4140605926513672
Batch 49/64 loss: -0.6383781433105469
Batch 50/64 loss: -0.5299944877624512
Batch 51/64 loss: -0.6300668716430664
Batch 52/64 loss: -0.5834226608276367
Batch 53/64 loss: -0.5202617645263672
Batch 54/64 loss: -0.6086182594299316
Batch 55/64 loss: -0.41304588317871094
Batch 56/64 loss: -0.4737696647644043
Batch 57/64 loss: -0.4581418037414551
Batch 58/64 loss: -0.6265687942504883
Batch 59/64 loss: 0.9502096176147461
Batch 60/64 loss: -0.43537282943725586
Batch 61/64 loss: -0.5699596405029297
Batch 62/64 loss: -0.19797134399414062
Batch 63/64 loss: -0.5316247940063477
Batch 64/64 loss: -4.070711612701416
Epoch 251  Train loss: -0.4397233981712192  Val loss: -0.8311043441090796
Epoch 252
-------------------------------
Batch 1/64 loss: -0.6427907943725586
Batch 2/64 loss: -0.34848976135253906
Batch 3/64 loss: -0.5026745796203613
Batch 4/64 loss: -0.4795665740966797
Batch 5/64 loss: -0.6645359992980957
Batch 6/64 loss: -0.5457472801208496
Batch 7/64 loss: -0.5908737182617188
Batch 8/64 loss: -0.5147013664245605
Batch 9/64 loss: -0.5759963989257812
Batch 10/64 loss: -0.48094654083251953
Batch 11/64 loss: -0.5954828262329102
Batch 12/64 loss: -0.5772910118103027
Batch 13/64 loss: 0.1779308319091797
Batch 14/64 loss: -0.7299189567565918
Batch 15/64 loss: -0.6351690292358398
Batch 16/64 loss: -0.5350236892700195
Batch 17/64 loss: 0.4762911796569824
Batch 18/64 loss: -0.4610018730163574
Batch 19/64 loss: -0.5717577934265137
Batch 20/64 loss: -0.5708179473876953
Batch 21/64 loss: -0.49720335006713867
Batch 22/64 loss: -0.6214280128479004
Batch 23/64 loss: -0.6112055778503418
Batch 24/64 loss: -0.5391755104064941
Batch 25/64 loss: -0.31409454345703125
Batch 26/64 loss: -0.6082501411437988
Batch 27/64 loss: -0.5984992980957031
Batch 28/64 loss: -0.5012173652648926
Batch 29/64 loss: -0.18955326080322266
Batch 30/64 loss: -0.6601190567016602
Batch 31/64 loss: -0.6222357749938965
Batch 32/64 loss: -0.6663050651550293
Batch 33/64 loss: -0.7062802314758301
Batch 34/64 loss: -0.25551843643188477
Batch 35/64 loss: 0.973353385925293
Batch 36/64 loss: -0.5327544212341309
Batch 37/64 loss: -0.5882744789123535
Batch 38/64 loss: -0.6767034530639648
Batch 39/64 loss: -0.5676608085632324
Batch 40/64 loss: -0.4174666404724121
Batch 41/64 loss: -0.10820484161376953
Batch 42/64 loss: -0.43804025650024414
Batch 43/64 loss: -0.5338783264160156
Batch 44/64 loss: -0.4844365119934082
Batch 45/64 loss: -0.5997257232666016
Batch 46/64 loss: -0.5886449813842773
Batch 47/64 loss: -0.2105083465576172
Batch 48/64 loss: -0.16440820693969727
Batch 49/64 loss: -0.4257650375366211
Batch 50/64 loss: -0.3931851387023926
Batch 51/64 loss: -0.5735325813293457
Batch 52/64 loss: -0.49643516540527344
Batch 53/64 loss: -0.5053682327270508
Batch 54/64 loss: -0.5425004959106445
Batch 55/64 loss: -0.4951043128967285
Batch 56/64 loss: -0.6464805603027344
Batch 57/64 loss: -0.5453028678894043
Batch 58/64 loss: -0.6253452301025391
Batch 59/64 loss: -0.4056825637817383
Batch 60/64 loss: -0.35161304473876953
Batch 61/64 loss: -0.7050657272338867
Batch 62/64 loss: -0.17609262466430664
Batch 63/64 loss: 0.556023120880127
Batch 64/64 loss: -4.1621503829956055
Epoch 252  Train loss: -0.48862849590825097  Val loss: -0.8608848597995195
Epoch 253
-------------------------------
Batch 1/64 loss: -0.5494451522827148
Batch 2/64 loss: 0.8819937705993652
Batch 3/64 loss: -0.5468807220458984
Batch 4/64 loss: -0.6305885314941406
Batch 5/64 loss: -0.6768550872802734
Batch 6/64 loss: -0.5727152824401855
Batch 7/64 loss: -0.5916228294372559
Batch 8/64 loss: -0.32886552810668945
Batch 9/64 loss: -0.11270284652709961
Batch 10/64 loss: -0.5361466407775879
Batch 11/64 loss: -0.5627560615539551
Batch 12/64 loss: -0.695551872253418
Batch 13/64 loss: -0.45534801483154297
Batch 14/64 loss: -0.6033458709716797
Batch 15/64 loss: -0.4998049736022949
Batch 16/64 loss: -0.33298301696777344
Batch 17/64 loss: -0.6414241790771484
Batch 18/64 loss: -0.6777529716491699
Batch 19/64 loss: -0.49663400650024414
Batch 20/64 loss: -0.5970854759216309
Batch 21/64 loss: -0.6004548072814941
Batch 22/64 loss: -0.48045873641967773
Batch 23/64 loss: -0.5784072875976562
Batch 24/64 loss: -0.5882267951965332
Batch 25/64 loss: -0.6022500991821289
Batch 26/64 loss: -0.5631070137023926
Batch 27/64 loss: 0.7696194648742676
Batch 28/64 loss: -0.6135330200195312
Batch 29/64 loss: -0.5567488670349121
Batch 30/64 loss: -0.6898813247680664
Batch 31/64 loss: -0.6620330810546875
Batch 32/64 loss: -0.4937620162963867
Batch 33/64 loss: -0.5700464248657227
Batch 34/64 loss: -0.6137199401855469
Batch 35/64 loss: -0.48452186584472656
Batch 36/64 loss: -0.6763262748718262
Batch 37/64 loss: -0.532902717590332
Batch 38/64 loss: -0.7167596817016602
Batch 39/64 loss: -0.44193315505981445
Batch 40/64 loss: -0.6123733520507812
Batch 41/64 loss: -0.42942237854003906
Batch 42/64 loss: -0.5049929618835449
Batch 43/64 loss: -0.6034421920776367
Batch 44/64 loss: -0.5596432685852051
Batch 45/64 loss: -0.5483493804931641
Batch 46/64 loss: -0.5463113784790039
Batch 47/64 loss: -0.7133359909057617
Batch 48/64 loss: -0.48316097259521484
Batch 49/64 loss: -0.7469534873962402
Batch 50/64 loss: -0.5726690292358398
Batch 51/64 loss: -0.6321148872375488
Batch 52/64 loss: -0.6513910293579102
Batch 53/64 loss: -0.4140810966491699
Batch 54/64 loss: -0.49033689498901367
Batch 55/64 loss: -0.568519115447998
Batch 56/64 loss: 0.11115169525146484
Batch 57/64 loss: -0.4950289726257324
Batch 58/64 loss: -0.5492300987243652
Batch 59/64 loss: -0.33128929138183594
Batch 60/64 loss: -0.6106815338134766
Batch 61/64 loss: -0.42579126358032227
Batch 62/64 loss: 0.4648475646972656
Batch 63/64 loss: -0.23099517822265625
Batch 64/64 loss: -4.169712066650391
Epoch 253  Train loss: -0.5206802742153991  Val loss: -0.8796575618363738
Epoch 254
-------------------------------
Batch 1/64 loss: -0.6560201644897461
Batch 2/64 loss: -0.6361136436462402
Batch 3/64 loss: -0.5961661338806152
Batch 4/64 loss: -0.3495149612426758
Batch 5/64 loss: -0.09458494186401367
Batch 6/64 loss: 0.8784995079040527
Batch 7/64 loss: -0.6625370979309082
Batch 8/64 loss: -0.3663902282714844
Batch 9/64 loss: -0.6686191558837891
Batch 10/64 loss: -0.5849823951721191
Batch 11/64 loss: -0.5508265495300293
Batch 12/64 loss: -0.50048828125
Batch 13/64 loss: -0.6026501655578613
Batch 14/64 loss: -0.6147894859313965
Batch 15/64 loss: -0.3416428565979004
Batch 16/64 loss: -0.49353647232055664
Batch 17/64 loss: -0.3815650939941406
Batch 18/64 loss: -0.48908281326293945
Batch 19/64 loss: -0.6476321220397949
Batch 20/64 loss: -0.6221604347229004
Batch 21/64 loss: 0.21963882446289062
Batch 22/64 loss: -0.7239346504211426
Batch 23/64 loss: -0.1833047866821289
Batch 24/64 loss: -0.5815529823303223
Batch 25/64 loss: -0.596560001373291
Batch 26/64 loss: -0.6021089553833008
Batch 27/64 loss: -0.6737580299377441
Batch 28/64 loss: -0.5360231399536133
Batch 29/64 loss: -0.3422274589538574
Batch 30/64 loss: -0.48127222061157227
Batch 31/64 loss: -0.4935750961303711
Batch 32/64 loss: -0.6187028884887695
Batch 33/64 loss: -0.691401481628418
Batch 34/64 loss: -0.723320484161377
Batch 35/64 loss: -0.6479358673095703
Batch 36/64 loss: -0.3673524856567383
Batch 37/64 loss: -0.7216305732727051
Batch 38/64 loss: -0.47667694091796875
Batch 39/64 loss: -0.2870912551879883
Batch 40/64 loss: -0.6817793846130371
Batch 41/64 loss: 0.8341250419616699
Batch 42/64 loss: -0.6615457534790039
Batch 43/64 loss: -0.4327688217163086
Batch 44/64 loss: -0.6096034049987793
Batch 45/64 loss: -0.5129494667053223
Batch 46/64 loss: -0.720250129699707
Batch 47/64 loss: -0.7188739776611328
Batch 48/64 loss: -0.6506915092468262
Batch 49/64 loss: -0.6340785026550293
Batch 50/64 loss: -0.5713663101196289
Batch 51/64 loss: -0.6214828491210938
Batch 52/64 loss: -0.6796555519104004
Batch 53/64 loss: 0.39484691619873047
Batch 54/64 loss: -0.7275357246398926
Batch 55/64 loss: -0.09081697463989258
Batch 56/64 loss: -0.5755276679992676
Batch 57/64 loss: -0.6292591094970703
Batch 58/64 loss: -0.474365234375
Batch 59/64 loss: -0.7450518608093262
Batch 60/64 loss: -0.7368259429931641
Batch 61/64 loss: -0.6192536354064941
Batch 62/64 loss: -0.5975618362426758
Batch 63/64 loss: -0.4771614074707031
Batch 64/64 loss: -3.909947395324707
Epoch 254  Train loss: -0.5236311781640146  Val loss: -0.8775606057078568
Epoch 255
-------------------------------
Batch 1/64 loss: -0.6260313987731934
Batch 2/64 loss: -0.6482162475585938
Batch 3/64 loss: -0.7487783432006836
Batch 4/64 loss: -0.5338826179504395
Batch 5/64 loss: -0.49321937561035156
Batch 6/64 loss: -0.4703636169433594
Batch 7/64 loss: -0.5433993339538574
Batch 8/64 loss: 0.39675092697143555
Batch 9/64 loss: -0.43335866928100586
Batch 10/64 loss: -0.5651674270629883
Batch 11/64 loss: -0.6991629600524902
Batch 12/64 loss: -0.6599364280700684
Batch 13/64 loss: -0.5854544639587402
Batch 14/64 loss: -0.7150006294250488
Batch 15/64 loss: -0.5195889472961426
Batch 16/64 loss: -0.5871291160583496
Batch 17/64 loss: -0.5596365928649902
Batch 18/64 loss: -0.42970895767211914
Batch 19/64 loss: -0.6250228881835938
Batch 20/64 loss: -0.05635786056518555
Batch 21/64 loss: -0.45540428161621094
Batch 22/64 loss: -0.5079855918884277
Batch 23/64 loss: -0.6898684501647949
Batch 24/64 loss: -0.6236920356750488
Batch 25/64 loss: -0.6237740516662598
Batch 26/64 loss: -0.472872257232666
Batch 27/64 loss: -0.6275615692138672
Batch 28/64 loss: -0.5391111373901367
Batch 29/64 loss: -0.5723433494567871
Batch 30/64 loss: -0.6402249336242676
Batch 31/64 loss: 0.6072564125061035
Batch 32/64 loss: -0.5419816970825195
Batch 33/64 loss: -0.6315116882324219
Batch 34/64 loss: -0.543403148651123
Batch 35/64 loss: -0.6274294853210449
Batch 36/64 loss: -0.10621023178100586
Batch 37/64 loss: -0.25630760192871094
Batch 38/64 loss: -0.45744848251342773
Batch 39/64 loss: -0.6190423965454102
Batch 40/64 loss: -0.47332000732421875
Batch 41/64 loss: -0.5911693572998047
Batch 42/64 loss: -0.6883020401000977
Batch 43/64 loss: -0.700812816619873
Batch 44/64 loss: -0.06823015213012695
Batch 45/64 loss: -0.19404935836791992
Batch 46/64 loss: -0.645240306854248
Batch 47/64 loss: -0.5740756988525391
Batch 48/64 loss: -0.21081924438476562
Batch 49/64 loss: -0.40895748138427734
Batch 50/64 loss: 1.2392420768737793
Batch 51/64 loss: -0.6755642890930176
Batch 52/64 loss: -0.6018948554992676
Batch 53/64 loss: -0.5563344955444336
Batch 54/64 loss: -0.4727320671081543
Batch 55/64 loss: -0.6032018661499023
Batch 56/64 loss: 0.31624317169189453
Batch 57/64 loss: -0.41737794876098633
Batch 58/64 loss: -0.36930084228515625
Batch 59/64 loss: -0.6162376403808594
Batch 60/64 loss: -0.6052122116088867
Batch 61/64 loss: -0.4865574836730957
Batch 62/64 loss: -0.5598492622375488
Batch 63/64 loss: -0.7186975479125977
Batch 64/64 loss: -4.2212815284729
Epoch 255  Train loss: -0.5000783340603697  Val loss: -0.7755104799041224
Epoch 256
-------------------------------
Batch 1/64 loss: -0.3880620002746582
Batch 2/64 loss: -0.4553351402282715
Batch 3/64 loss: -0.4747490882873535
Batch 4/64 loss: -0.6807050704956055
Batch 5/64 loss: -0.6924676895141602
Batch 6/64 loss: -0.7068085670471191
Batch 7/64 loss: -0.6044611930847168
Batch 8/64 loss: -0.5503859519958496
Batch 9/64 loss: -0.4451117515563965
Batch 10/64 loss: -0.4476499557495117
Batch 11/64 loss: -0.6417441368103027
Batch 12/64 loss: -0.3626241683959961
Batch 13/64 loss: -0.49895334243774414
Batch 14/64 loss: 0.9928584098815918
Batch 15/64 loss: -0.6314687728881836
Batch 16/64 loss: -0.3646864891052246
Batch 17/64 loss: -0.6937499046325684
Batch 18/64 loss: -0.532865047454834
Batch 19/64 loss: -0.6806750297546387
Batch 20/64 loss: -0.4835953712463379
Batch 21/64 loss: -0.6520791053771973
Batch 22/64 loss: -0.593806266784668
Batch 23/64 loss: -0.5467863082885742
Batch 24/64 loss: -0.0691370964050293
Batch 25/64 loss: 0.3809170722961426
Batch 26/64 loss: -0.641502857208252
Batch 27/64 loss: -0.5091152191162109
Batch 28/64 loss: -0.5767364501953125
Batch 29/64 loss: -0.6446199417114258
Batch 30/64 loss: -0.5237851142883301
Batch 31/64 loss: 0.21360445022583008
Batch 32/64 loss: -0.7244858741760254
Batch 33/64 loss: -0.6731376647949219
Batch 34/64 loss: 0.47561025619506836
Batch 35/64 loss: -0.6202163696289062
Batch 36/64 loss: -0.6899704933166504
Batch 37/64 loss: -0.5523567199707031
Batch 38/64 loss: -0.4395585060119629
Batch 39/64 loss: -0.2539687156677246
Batch 40/64 loss: -0.7631258964538574
Batch 41/64 loss: -0.6374163627624512
Batch 42/64 loss: -0.33919763565063477
Batch 43/64 loss: -0.5414152145385742
Batch 44/64 loss: -0.5460715293884277
Batch 45/64 loss: -0.5273513793945312
Batch 46/64 loss: -0.669611930847168
Batch 47/64 loss: -0.5438523292541504
Batch 48/64 loss: -0.36284351348876953
Batch 49/64 loss: -0.2831873893737793
Batch 50/64 loss: -0.5993165969848633
Batch 51/64 loss: -0.6962270736694336
Batch 52/64 loss: -0.6043529510498047
Batch 53/64 loss: -0.4940457344055176
Batch 54/64 loss: -0.7541365623474121
Batch 55/64 loss: -0.3029661178588867
Batch 56/64 loss: -0.5672640800476074
Batch 57/64 loss: -0.2615795135498047
Batch 58/64 loss: -0.4719719886779785
Batch 59/64 loss: -0.5062460899353027
Batch 60/64 loss: -0.6158418655395508
Batch 61/64 loss: -0.5235934257507324
Batch 62/64 loss: -0.42176294326782227
Batch 63/64 loss: 0.07836675643920898
Batch 64/64 loss: -4.011934757232666
Epoch 256  Train loss: -0.5011503313101974  Val loss: -0.8285118443859402
Epoch 257
-------------------------------
Batch 1/64 loss: -0.47908639907836914
Batch 2/64 loss: -0.5688567161560059
Batch 3/64 loss: -0.5058245658874512
Batch 4/64 loss: -0.49826574325561523
Batch 5/64 loss: -0.17872238159179688
Batch 6/64 loss: 0.4938364028930664
Batch 7/64 loss: -0.6350264549255371
Batch 8/64 loss: -0.5465421676635742
Batch 9/64 loss: -0.5753560066223145
Batch 10/64 loss: -0.5163922309875488
Batch 11/64 loss: -0.5880346298217773
Batch 12/64 loss: -0.5366406440734863
Batch 13/64 loss: -0.6676311492919922
Batch 14/64 loss: -0.6543316841125488
Batch 15/64 loss: -0.6877074241638184
Batch 16/64 loss: -0.46895647048950195
Batch 17/64 loss: -0.514589786529541
Batch 18/64 loss: -0.32158660888671875
Batch 19/64 loss: -0.3426480293273926
Batch 20/64 loss: -0.6931166648864746
Batch 21/64 loss: -0.0051822662353515625
Batch 22/64 loss: -0.6542987823486328
Batch 23/64 loss: -0.5556817054748535
Batch 24/64 loss: -0.617833137512207
Batch 25/64 loss: -0.6727910041809082
Batch 26/64 loss: -0.8350591659545898
Batch 27/64 loss: -0.6076540946960449
Batch 28/64 loss: -0.6449003219604492
Batch 29/64 loss: -0.6310305595397949
Batch 30/64 loss: -0.5410299301147461
Batch 31/64 loss: -0.5389046669006348
Batch 32/64 loss: -0.37715578079223633
Batch 33/64 loss: -0.6916804313659668
Batch 34/64 loss: -0.5173602104187012
Batch 35/64 loss: -0.5827651023864746
Batch 36/64 loss: -0.06859636306762695
Batch 37/64 loss: -0.485750675201416
Batch 38/64 loss: -0.5300846099853516
Batch 39/64 loss: -0.46359872817993164
Batch 40/64 loss: -0.5709934234619141
Batch 41/64 loss: -0.5748510360717773
Batch 42/64 loss: -0.6118960380554199
Batch 43/64 loss: -0.6999001502990723
Batch 44/64 loss: -0.5650992393493652
Batch 45/64 loss: -0.48633384704589844
Batch 46/64 loss: -0.41567134857177734
Batch 47/64 loss: -0.6404833793640137
Batch 48/64 loss: -0.4785637855529785
Batch 49/64 loss: -0.5843386650085449
Batch 50/64 loss: -0.6262550354003906
Batch 51/64 loss: 0.3697476387023926
Batch 52/64 loss: -0.567507266998291
Batch 53/64 loss: -0.506749153137207
Batch 54/64 loss: -0.6482939720153809
Batch 55/64 loss: -0.5342116355895996
Batch 56/64 loss: -0.46178627014160156
Batch 57/64 loss: -0.509152889251709
Batch 58/64 loss: -0.6317610740661621
Batch 59/64 loss: -0.6225099563598633
Batch 60/64 loss: -0.5793495178222656
Batch 61/64 loss: -0.20485973358154297
Batch 62/64 loss: -0.5104513168334961
Batch 63/64 loss: 0.9056763648986816
Batch 64/64 loss: -4.121067523956299
Epoch 257  Train loss: -0.52318795709049  Val loss: -0.6986832766188789
Epoch 258
-------------------------------
Batch 1/64 loss: -0.4291505813598633
Batch 2/64 loss: -0.6932597160339355
Batch 3/64 loss: -0.2115015983581543
Batch 4/64 loss: -0.5744237899780273
Batch 5/64 loss: -0.45264196395874023
Batch 6/64 loss: -0.4673800468444824
Batch 7/64 loss: -0.5804891586303711
Batch 8/64 loss: -0.626739501953125
Batch 9/64 loss: -0.5278291702270508
Batch 10/64 loss: -0.4771862030029297
Batch 11/64 loss: -0.6991724967956543
Batch 12/64 loss: -0.48630332946777344
Batch 13/64 loss: -0.5170221328735352
Batch 14/64 loss: -0.5891900062561035
Batch 15/64 loss: -0.604832649230957
Batch 16/64 loss: -0.6975979804992676
Batch 17/64 loss: -0.6006283760070801
Batch 18/64 loss: -0.7469539642333984
Batch 19/64 loss: -0.6998472213745117
Batch 20/64 loss: -0.6542320251464844
Batch 21/64 loss: -0.5166306495666504
Batch 22/64 loss: -0.6404800415039062
Batch 23/64 loss: -0.5492019653320312
Batch 24/64 loss: 0.5987496376037598
Batch 25/64 loss: -0.49405765533447266
Batch 26/64 loss: -0.6185002326965332
Batch 27/64 loss: -0.564995288848877
Batch 28/64 loss: -0.589348316192627
Batch 29/64 loss: -0.6725492477416992
Batch 30/64 loss: -0.7179455757141113
Batch 31/64 loss: -0.6052289009094238
Batch 32/64 loss: -0.5968408584594727
Batch 33/64 loss: -0.7293787002563477
Batch 34/64 loss: -0.6153078079223633
Batch 35/64 loss: -0.6284346580505371
Batch 36/64 loss: -0.5357241630554199
Batch 37/64 loss: -0.7893462181091309
Batch 38/64 loss: 1.3224010467529297
Batch 39/64 loss: -0.5355525016784668
Batch 40/64 loss: -0.05734539031982422
Batch 41/64 loss: -0.2213597297668457
Batch 42/64 loss: 0.43419933319091797
Batch 43/64 loss: -0.5380053520202637
Batch 44/64 loss: -0.7697577476501465
Batch 45/64 loss: -0.5532999038696289
Batch 46/64 loss: 0.10530710220336914
Batch 47/64 loss: -0.5389704704284668
Batch 48/64 loss: -0.44444990158081055
Batch 49/64 loss: -0.42284107208251953
Batch 50/64 loss: -0.5007724761962891
Batch 51/64 loss: -0.6711950302124023
Batch 52/64 loss: -0.3918800354003906
Batch 53/64 loss: -0.6863088607788086
Batch 54/64 loss: -0.6031441688537598
Batch 55/64 loss: -0.6734957695007324
Batch 56/64 loss: -0.594571590423584
Batch 57/64 loss: -0.5410280227661133
Batch 58/64 loss: -0.1951923370361328
Batch 59/64 loss: -0.7463159561157227
Batch 60/64 loss: -0.502532958984375
Batch 61/64 loss: -0.5740833282470703
Batch 62/64 loss: -0.5660400390625
Batch 63/64 loss: -0.6713261604309082
Batch 64/64 loss: -4.315976619720459
Epoch 258  Train loss: -0.5329591582803165  Val loss: -0.9282192217115685
Saving best model, epoch: 258
Epoch 259
-------------------------------
Batch 1/64 loss: -0.7556614875793457
Batch 2/64 loss: -0.6137895584106445
Batch 3/64 loss: -0.6015958786010742
Batch 4/64 loss: -0.7062044143676758
Batch 5/64 loss: -0.6546182632446289
Batch 6/64 loss: -0.7400422096252441
Batch 7/64 loss: -0.7129168510437012
Batch 8/64 loss: -0.7073345184326172
Batch 9/64 loss: -0.7068700790405273
Batch 10/64 loss: 0.6777305603027344
Batch 11/64 loss: -0.606940746307373
Batch 12/64 loss: -0.41603899002075195
Batch 13/64 loss: -0.6328978538513184
Batch 14/64 loss: -0.7191882133483887
Batch 15/64 loss: -0.629150390625
Batch 16/64 loss: -0.6892375946044922
Batch 17/64 loss: -0.18364715576171875
Batch 18/64 loss: -0.6692123413085938
Batch 19/64 loss: -0.6637449264526367
Batch 20/64 loss: -0.5495848655700684
Batch 21/64 loss: -0.35165882110595703
Batch 22/64 loss: -0.7720456123352051
Batch 23/64 loss: -0.7034749984741211
Batch 24/64 loss: -0.6590800285339355
Batch 25/64 loss: -0.5855264663696289
Batch 26/64 loss: -0.7015247344970703
Batch 27/64 loss: -0.7365350723266602
Batch 28/64 loss: -0.6495170593261719
Batch 29/64 loss: -0.6130342483520508
Batch 30/64 loss: -0.42244386672973633
Batch 31/64 loss: -0.7501955032348633
Batch 32/64 loss: -0.7591948509216309
Batch 33/64 loss: -0.7903265953063965
Batch 34/64 loss: -0.6108970642089844
Batch 35/64 loss: -0.6759428977966309
Batch 36/64 loss: -0.705472469329834
Batch 37/64 loss: -0.6496734619140625
Batch 38/64 loss: -0.7317976951599121
Batch 39/64 loss: -0.31856822967529297
Batch 40/64 loss: 0.641352653503418
Batch 41/64 loss: -0.7076678276062012
Batch 42/64 loss: -0.7120833396911621
Batch 43/64 loss: -0.5817141532897949
Batch 44/64 loss: -0.7371759414672852
Batch 45/64 loss: -0.40340518951416016
Batch 46/64 loss: -0.5450844764709473
Batch 47/64 loss: -0.5696520805358887
Batch 48/64 loss: -0.6319737434387207
Batch 49/64 loss: -0.43936920166015625
Batch 50/64 loss: -0.4949617385864258
Batch 51/64 loss: -0.6971144676208496
Batch 52/64 loss: 0.08400344848632812
Batch 53/64 loss: -0.701876163482666
Batch 54/64 loss: -0.7348484992980957
Batch 55/64 loss: -0.5284433364868164
Batch 56/64 loss: -0.7525992393493652
Batch 57/64 loss: -0.5229988098144531
Batch 58/64 loss: -0.8187856674194336
Batch 59/64 loss: -0.709808349609375
Batch 60/64 loss: -0.6855216026306152
Batch 61/64 loss: -0.5626020431518555
Batch 62/64 loss: 0.265134334564209
Batch 63/64 loss: -0.7141323089599609
Batch 64/64 loss: -3.6322388648986816
Epoch 259  Train loss: -0.6031899975795372  Val loss: -0.9349747752815587
Saving best model, epoch: 259
Epoch 260
-------------------------------
Batch 1/64 loss: -0.26164817810058594
Batch 2/64 loss: -0.6379156112670898
Batch 3/64 loss: -0.5605292320251465
Batch 4/64 loss: -0.7800121307373047
Batch 5/64 loss: -0.5145673751831055
Batch 6/64 loss: -0.7409853935241699
Batch 7/64 loss: -0.6415586471557617
Batch 8/64 loss: -0.679814338684082
Batch 9/64 loss: -0.19757509231567383
Batch 10/64 loss: -0.10988044738769531
Batch 11/64 loss: -0.6782317161560059
Batch 12/64 loss: -0.5368733406066895
Batch 13/64 loss: 0.013488292694091797
Batch 14/64 loss: -0.5442705154418945
Batch 15/64 loss: -0.23182344436645508
Batch 16/64 loss: -0.5471224784851074
Batch 17/64 loss: -0.7049093246459961
Batch 18/64 loss: -0.6594424247741699
Batch 19/64 loss: -0.5808148384094238
Batch 20/64 loss: 0.028210163116455078
Batch 21/64 loss: -0.7132668495178223
Batch 22/64 loss: 0.7385001182556152
Batch 23/64 loss: -0.5240392684936523
Batch 24/64 loss: -0.4673123359680176
Batch 25/64 loss: -0.4823493957519531
Batch 26/64 loss: -0.714665412902832
Batch 27/64 loss: -0.6581487655639648
Batch 28/64 loss: -0.6038837432861328
Batch 29/64 loss: -0.6867094039916992
Batch 30/64 loss: 0.8063473701477051
Batch 31/64 loss: -0.6665358543395996
Batch 32/64 loss: -0.5131058692932129
Batch 33/64 loss: -0.4214458465576172
Batch 34/64 loss: -0.7094025611877441
Batch 35/64 loss: -0.6664638519287109
Batch 36/64 loss: -0.5750713348388672
Batch 37/64 loss: -0.6595602035522461
Batch 38/64 loss: -0.5937290191650391
Batch 39/64 loss: -0.5779433250427246
Batch 40/64 loss: -0.6582093238830566
Batch 41/64 loss: -0.43968820571899414
Batch 42/64 loss: -0.3667793273925781
Batch 43/64 loss: -0.6020445823669434
Batch 44/64 loss: -0.6151552200317383
Batch 45/64 loss: -0.686042308807373
Batch 46/64 loss: -0.15785884857177734
Batch 47/64 loss: -0.5583968162536621
Batch 48/64 loss: -0.5843911170959473
Batch 49/64 loss: -0.44714832305908203
Batch 50/64 loss: -0.748164176940918
Batch 51/64 loss: -0.6982073783874512
Batch 52/64 loss: -0.535912036895752
Batch 53/64 loss: -0.6233015060424805
Batch 54/64 loss: -0.5772156715393066
Batch 55/64 loss: -0.7028732299804688
Batch 56/64 loss: -0.6095418930053711
Batch 57/64 loss: -0.6476025581359863
Batch 58/64 loss: -0.5352725982666016
Batch 59/64 loss: -0.42435312271118164
Batch 60/64 loss: 0.9028434753417969
Batch 61/64 loss: -0.6019129753112793
Batch 62/64 loss: -0.49213361740112305
Batch 63/64 loss: -0.5678348541259766
Batch 64/64 loss: -4.125505447387695
Epoch 260  Train loss: -0.5227668537813075  Val loss: -0.8232094938402733
Epoch 261
-------------------------------
Batch 1/64 loss: -0.33384084701538086
Batch 2/64 loss: -0.09810256958007812
Batch 3/64 loss: -0.36739015579223633
Batch 4/64 loss: -0.6174726486206055
Batch 5/64 loss: -0.4820575714111328
Batch 6/64 loss: -0.6227049827575684
Batch 7/64 loss: 0.612910270690918
Batch 8/64 loss: -0.4505887031555176
Batch 9/64 loss: -0.5286893844604492
Batch 10/64 loss: -0.5506248474121094
Batch 11/64 loss: -0.7029585838317871
Batch 12/64 loss: -0.13759851455688477
Batch 13/64 loss: -0.6413159370422363
Batch 14/64 loss: -0.5726585388183594
Batch 15/64 loss: -0.6285357475280762
Batch 16/64 loss: -0.46082448959350586
Batch 17/64 loss: -0.7146773338317871
Batch 18/64 loss: -0.6367130279541016
Batch 19/64 loss: -0.5396394729614258
Batch 20/64 loss: 0.2509803771972656
Batch 21/64 loss: -0.6705703735351562
Batch 22/64 loss: -0.4841156005859375
Batch 23/64 loss: -0.5740852355957031
Batch 24/64 loss: 0.8182578086853027
Batch 25/64 loss: -0.5827546119689941
Batch 26/64 loss: -0.6430015563964844
Batch 27/64 loss: -0.6117525100708008
Batch 28/64 loss: -0.8089804649353027
Batch 29/64 loss: -0.6588330268859863
Batch 30/64 loss: -0.5234427452087402
Batch 31/64 loss: -0.24031972885131836
Batch 32/64 loss: -0.6360025405883789
Batch 33/64 loss: -0.6108269691467285
Batch 34/64 loss: -0.47190237045288086
Batch 35/64 loss: -0.5212574005126953
Batch 36/64 loss: -0.5181431770324707
Batch 37/64 loss: -0.43186187744140625
Batch 38/64 loss: -0.5948691368103027
Batch 39/64 loss: -0.5615019798278809
Batch 40/64 loss: -0.6289749145507812
Batch 41/64 loss: -0.7319951057434082
Batch 42/64 loss: -0.6323528289794922
Batch 43/64 loss: -0.5685396194458008
Batch 44/64 loss: -0.6554980278015137
Batch 45/64 loss: -0.5273838043212891
Batch 46/64 loss: -0.7563552856445312
Batch 47/64 loss: -0.6132612228393555
Batch 48/64 loss: -0.7230873107910156
Batch 49/64 loss: -0.7227087020874023
Batch 50/64 loss: -0.8389706611633301
Batch 51/64 loss: -0.638831615447998
Batch 52/64 loss: -0.39995431900024414
Batch 53/64 loss: -0.815248966217041
Batch 54/64 loss: -0.6918854713439941
Batch 55/64 loss: -0.6937294006347656
Batch 56/64 loss: -0.5318236351013184
Batch 57/64 loss: -0.6771197319030762
Batch 58/64 loss: -0.663907527923584
Batch 59/64 loss: -0.2107071876525879
Batch 60/64 loss: -0.6273374557495117
Batch 61/64 loss: 0.0661158561706543
Batch 62/64 loss: -0.19752740859985352
Batch 63/64 loss: -0.7067675590515137
Batch 64/64 loss: -4.05996561050415
Epoch 261  Train loss: -0.5455888991262399  Val loss: -0.7996828793660062
Epoch 262
-------------------------------
Batch 1/64 loss: -0.5251150131225586
Batch 2/64 loss: -0.6953067779541016
Batch 3/64 loss: -0.43203067779541016
Batch 4/64 loss: -0.6902470588684082
Batch 5/64 loss: -0.5458245277404785
Batch 6/64 loss: -0.12274932861328125
Batch 7/64 loss: -0.4222888946533203
Batch 8/64 loss: -0.5105366706848145
Batch 9/64 loss: 0.2679562568664551
Batch 10/64 loss: -0.6664395332336426
Batch 11/64 loss: -0.5501856803894043
Batch 12/64 loss: -0.6394233703613281
Batch 13/64 loss: 0.4602947235107422
Batch 14/64 loss: -0.4638509750366211
Batch 15/64 loss: -0.6091818809509277
Batch 16/64 loss: 0.8562321662902832
Batch 17/64 loss: -0.08093023300170898
Batch 18/64 loss: -0.7132387161254883
Batch 19/64 loss: -0.25609445571899414
Batch 20/64 loss: -0.6738805770874023
Batch 21/64 loss: -0.2724456787109375
Batch 22/64 loss: -0.5150632858276367
Batch 23/64 loss: -0.6652941703796387
Batch 24/64 loss: -0.42104291915893555
Batch 25/64 loss: -0.5335531234741211
Batch 26/64 loss: -0.6563234329223633
Batch 27/64 loss: -0.5132546424865723
Batch 28/64 loss: 0.5015139579772949
Batch 29/64 loss: -0.48140954971313477
Batch 30/64 loss: -0.5535798072814941
Batch 31/64 loss: -0.16309118270874023
Batch 32/64 loss: -0.5564436912536621
Batch 33/64 loss: -0.5499796867370605
Batch 34/64 loss: -0.5991940498352051
Batch 35/64 loss: -0.6870489120483398
Batch 36/64 loss: -0.5115513801574707
Batch 37/64 loss: -0.07063055038452148
Batch 38/64 loss: -0.6309661865234375
Batch 39/64 loss: -0.22676706314086914
Batch 40/64 loss: -0.22423553466796875
Batch 41/64 loss: -0.5023341178894043
Batch 42/64 loss: -0.526005744934082
Batch 43/64 loss: -0.2010955810546875
Batch 44/64 loss: -0.6631407737731934
Batch 45/64 loss: -0.11960554122924805
Batch 46/64 loss: -0.5871429443359375
Batch 47/64 loss: -0.6398100852966309
Batch 48/64 loss: -0.32632017135620117
Batch 49/64 loss: -0.5404291152954102
Batch 50/64 loss: -0.4279446601867676
Batch 51/64 loss: -0.560676097869873
Batch 52/64 loss: -0.13202953338623047
Batch 53/64 loss: -0.5929446220397949
Batch 54/64 loss: -0.4818873405456543
Batch 55/64 loss: -0.5517969131469727
Batch 56/64 loss: -0.2605299949645996
Batch 57/64 loss: -0.5173759460449219
Batch 58/64 loss: -0.4569058418273926
Batch 59/64 loss: -0.5679998397827148
Batch 60/64 loss: -0.6465754508972168
Batch 61/64 loss: -0.24483919143676758
Batch 62/64 loss: -0.4500584602355957
Batch 63/64 loss: -0.716728687286377
Batch 64/64 loss: -3.9175467491149902
Epoch 262  Train loss: -0.45483198165893557  Val loss: -0.8301881744279894
Epoch 263
-------------------------------
Batch 1/64 loss: -0.3572239875793457
Batch 2/64 loss: -0.20203924179077148
Batch 3/64 loss: -0.2740974426269531
Batch 4/64 loss: -0.520777702331543
Batch 5/64 loss: -0.2729792594909668
Batch 6/64 loss: 0.4443035125732422
Batch 7/64 loss: -0.4953289031982422
Batch 8/64 loss: -0.4284648895263672
Batch 9/64 loss: 0.5710382461547852
Batch 10/64 loss: -0.4332265853881836
Batch 11/64 loss: -0.7343835830688477
Batch 12/64 loss: -0.3957676887512207
Batch 13/64 loss: -0.6850466728210449
Batch 14/64 loss: -0.5485310554504395
Batch 15/64 loss: -0.5621652603149414
Batch 16/64 loss: -0.4928169250488281
Batch 17/64 loss: -0.5539278984069824
Batch 18/64 loss: -0.45447492599487305
Batch 19/64 loss: -0.6615018844604492
Batch 20/64 loss: -0.6918840408325195
Batch 21/64 loss: -0.5201940536499023
Batch 22/64 loss: -0.5261712074279785
Batch 23/64 loss: -0.4780464172363281
Batch 24/64 loss: -0.629601001739502
Batch 25/64 loss: -0.25412559509277344
Batch 26/64 loss: 0.9265542030334473
Batch 27/64 loss: 0.27167463302612305
Batch 28/64 loss: -0.5754475593566895
Batch 29/64 loss: -0.6439905166625977
Batch 30/64 loss: -0.5807619094848633
Batch 31/64 loss: -0.48157358169555664
Batch 32/64 loss: -0.5647854804992676
Batch 33/64 loss: -0.44821929931640625
Batch 34/64 loss: -0.45211172103881836
Batch 35/64 loss: -0.44716787338256836
Batch 36/64 loss: -0.609168529510498
Batch 37/64 loss: -0.6002020835876465
Batch 38/64 loss: -0.6250405311584473
Batch 39/64 loss: -0.5661988258361816
Batch 40/64 loss: -0.449979305267334
Batch 41/64 loss: -0.3602108955383301
Batch 42/64 loss: -0.5939769744873047
Batch 43/64 loss: -0.6091752052307129
Batch 44/64 loss: -0.34340953826904297
Batch 45/64 loss: -0.591641902923584
Batch 46/64 loss: -0.6322202682495117
Batch 47/64 loss: -0.5770792961120605
Batch 48/64 loss: -0.5786690711975098
Batch 49/64 loss: -0.5407209396362305
Batch 50/64 loss: -0.5978078842163086
Batch 51/64 loss: -0.4152355194091797
Batch 52/64 loss: -0.5704140663146973
Batch 53/64 loss: -0.7352828979492188
Batch 54/64 loss: -0.560936450958252
Batch 55/64 loss: -0.6354818344116211
Batch 56/64 loss: -0.510312557220459
Batch 57/64 loss: -0.3842201232910156
Batch 58/64 loss: -0.5337481498718262
Batch 59/64 loss: -0.6391868591308594
Batch 60/64 loss: -0.6691460609436035
Batch 61/64 loss: -0.6802816390991211
Batch 62/64 loss: -0.7116050720214844
Batch 63/64 loss: -0.0741891860961914
Batch 64/64 loss: -4.290153503417969
Epoch 263  Train loss: -0.49829679002948835  Val loss: -0.889321920388343
Epoch 264
-------------------------------
Batch 1/64 loss: -0.5838208198547363
Batch 2/64 loss: -0.6215329170227051
Batch 3/64 loss: -0.5993156433105469
Batch 4/64 loss: -0.565000057220459
Batch 5/64 loss: -0.6005725860595703
Batch 6/64 loss: -0.5580682754516602
Batch 7/64 loss: -0.5242533683776855
Batch 8/64 loss: -0.5054945945739746
Batch 9/64 loss: -0.7094221115112305
Batch 10/64 loss: -0.6732978820800781
Batch 11/64 loss: -0.6938576698303223
Batch 12/64 loss: -0.6770563125610352
Batch 13/64 loss: -0.12853240966796875
Batch 14/64 loss: -0.6575078964233398
Batch 15/64 loss: -0.606107234954834
Batch 16/64 loss: -0.5609688758850098
Batch 17/64 loss: -0.5251607894897461
Batch 18/64 loss: -0.6704902648925781
Batch 19/64 loss: -0.5708866119384766
Batch 20/64 loss: -0.4315009117126465
Batch 21/64 loss: -0.42791080474853516
Batch 22/64 loss: -0.40639591217041016
Batch 23/64 loss: -0.6841096878051758
Batch 24/64 loss: -0.6047840118408203
Batch 25/64 loss: -0.6838288307189941
Batch 26/64 loss: -0.6521205902099609
Batch 27/64 loss: -0.6125736236572266
Batch 28/64 loss: -0.6422915458679199
Batch 29/64 loss: 0.5271787643432617
Batch 30/64 loss: -0.5847768783569336
Batch 31/64 loss: 0.6852865219116211
Batch 32/64 loss: -0.7171244621276855
Batch 33/64 loss: -0.49979066848754883
Batch 34/64 loss: -0.45316648483276367
Batch 35/64 loss: -0.7005901336669922
Batch 36/64 loss: -0.640352725982666
Batch 37/64 loss: -0.6263723373413086
Batch 38/64 loss: -0.16897249221801758
Batch 39/64 loss: -0.6021132469177246
Batch 40/64 loss: -0.7076835632324219
Batch 41/64 loss: -0.5856938362121582
Batch 42/64 loss: -0.6691007614135742
Batch 43/64 loss: -0.6740484237670898
Batch 44/64 loss: -0.5753574371337891
Batch 45/64 loss: -0.1267995834350586
Batch 46/64 loss: 0.2718353271484375
Batch 47/64 loss: -0.6616063117980957
Batch 48/64 loss: -0.31444787979125977
Batch 49/64 loss: -0.637509822845459
Batch 50/64 loss: -0.6708569526672363
Batch 51/64 loss: -0.5442466735839844
Batch 52/64 loss: -0.5462145805358887
Batch 53/64 loss: -0.539771556854248
Batch 54/64 loss: -0.3148193359375
Batch 55/64 loss: -0.7385931015014648
Batch 56/64 loss: -0.7083806991577148
Batch 57/64 loss: -0.544675350189209
Batch 58/64 loss: -0.558861255645752
Batch 59/64 loss: 0.6984467506408691
Batch 60/64 loss: -0.3452792167663574
Batch 61/64 loss: -0.4214010238647461
Batch 62/64 loss: -0.5030837059020996
Batch 63/64 loss: -0.4230375289916992
Batch 64/64 loss: -4.154296398162842
Epoch 264  Train loss: -0.531993179695279  Val loss: -0.8818232742781492
Epoch 265
-------------------------------
Batch 1/64 loss: -0.41847801208496094
Batch 2/64 loss: -0.6937661170959473
Batch 3/64 loss: -0.5313701629638672
Batch 4/64 loss: -0.5448470115661621
Batch 5/64 loss: -0.44415283203125
Batch 6/64 loss: -0.5844020843505859
Batch 7/64 loss: -0.43793582916259766
Batch 8/64 loss: -0.7156305313110352
Batch 9/64 loss: 0.22224187850952148
Batch 10/64 loss: -0.22555971145629883
Batch 11/64 loss: -0.5203509330749512
Batch 12/64 loss: -0.3845210075378418
Batch 13/64 loss: -0.6252613067626953
Batch 14/64 loss: -0.6100630760192871
Batch 15/64 loss: -0.6598386764526367
Batch 16/64 loss: -0.7300519943237305
Batch 17/64 loss: -0.7092242240905762
Batch 18/64 loss: -0.6479401588439941
Batch 19/64 loss: -0.6474499702453613
Batch 20/64 loss: -0.5003342628479004
Batch 21/64 loss: -0.5766892433166504
Batch 22/64 loss: -0.3476753234863281
Batch 23/64 loss: -0.6908860206604004
Batch 24/64 loss: -0.6764740943908691
Batch 25/64 loss: -0.6042904853820801
Batch 26/64 loss: -0.7486634254455566
Batch 27/64 loss: -0.6906461715698242
Batch 28/64 loss: 1.7685251235961914
Batch 29/64 loss: -0.4990062713623047
Batch 30/64 loss: -0.43790674209594727
Batch 31/64 loss: -0.5728673934936523
Batch 32/64 loss: -0.6136188507080078
Batch 33/64 loss: -0.7027530670166016
Batch 34/64 loss: -0.16583776473999023
Batch 35/64 loss: -0.1833195686340332
Batch 36/64 loss: -0.5136876106262207
Batch 37/64 loss: 0.9828348159790039
Batch 38/64 loss: -0.6880083084106445
Batch 39/64 loss: -0.6475272178649902
Batch 40/64 loss: -0.45290088653564453
Batch 41/64 loss: -0.5369958877563477
Batch 42/64 loss: -0.6717448234558105
Batch 43/64 loss: -0.6546669006347656
Batch 44/64 loss: -0.40537595748901367
Batch 45/64 loss: -0.46323633193969727
Batch 46/64 loss: -0.43018484115600586
Batch 47/64 loss: -0.6384515762329102
Batch 48/64 loss: -0.08609390258789062
Batch 49/64 loss: -0.47757863998413086
Batch 50/64 loss: -0.4149312973022461
Batch 51/64 loss: -0.485288143157959
Batch 52/64 loss: -0.5738463401794434
Batch 53/64 loss: -0.3767814636230469
Batch 54/64 loss: -0.3379392623901367
Batch 55/64 loss: -0.34868812561035156
Batch 56/64 loss: -0.6798243522644043
Batch 57/64 loss: -0.6466712951660156
Batch 58/64 loss: -0.6401476860046387
Batch 59/64 loss: -0.6004581451416016
Batch 60/64 loss: -0.04728078842163086
Batch 61/64 loss: -0.7692966461181641
Batch 62/64 loss: -0.4738173484802246
Batch 63/64 loss: -0.5809812545776367
Batch 64/64 loss: -4.112355709075928
Epoch 265  Train loss: -0.5003118795507094  Val loss: -0.8618351520131954
Epoch 266
-------------------------------
Batch 1/64 loss: -0.7624950408935547
Batch 2/64 loss: -0.6394329071044922
Batch 3/64 loss: -0.6562252044677734
Batch 4/64 loss: -0.587317943572998
Batch 5/64 loss: -0.5367913246154785
Batch 6/64 loss: -0.700920581817627
Batch 7/64 loss: -0.588707447052002
Batch 8/64 loss: -0.7391080856323242
Batch 9/64 loss: -0.505183219909668
Batch 10/64 loss: -0.6023397445678711
Batch 11/64 loss: -0.4859919548034668
Batch 12/64 loss: -0.4715547561645508
Batch 13/64 loss: -0.6958389282226562
Batch 14/64 loss: -0.7021889686584473
Batch 15/64 loss: -0.7457180023193359
Batch 16/64 loss: -0.7855033874511719
Batch 17/64 loss: -0.6710953712463379
Batch 18/64 loss: -0.6767096519470215
Batch 19/64 loss: -0.65411376953125
Batch 20/64 loss: -0.6064825057983398
Batch 21/64 loss: -0.5685749053955078
Batch 22/64 loss: -0.6589560508728027
Batch 23/64 loss: -0.7565083503723145
Batch 24/64 loss: -0.6954984664916992
Batch 25/64 loss: -0.7014446258544922
Batch 26/64 loss: -0.621757984161377
Batch 27/64 loss: -0.6953840255737305
Batch 28/64 loss: -0.6135549545288086
Batch 29/64 loss: -0.6227426528930664
Batch 30/64 loss: -0.4913811683654785
Batch 31/64 loss: 0.608405590057373
Batch 32/64 loss: -0.6613636016845703
Batch 33/64 loss: -0.5511140823364258
Batch 34/64 loss: -0.7458324432373047
Batch 35/64 loss: -0.7380871772766113
Batch 36/64 loss: -0.7300205230712891
Batch 37/64 loss: -0.7227439880371094
Batch 38/64 loss: -0.14434289932250977
Batch 39/64 loss: -0.6462106704711914
Batch 40/64 loss: -0.7500066757202148
Batch 41/64 loss: 0.11971664428710938
Batch 42/64 loss: -0.4935269355773926
Batch 43/64 loss: -0.724785327911377
Batch 44/64 loss: -0.7877750396728516
Batch 45/64 loss: -0.6002211570739746
Batch 46/64 loss: -0.6156792640686035
Batch 47/64 loss: -0.5531272888183594
Batch 48/64 loss: -0.3990621566772461
Batch 49/64 loss: -0.617499828338623
Batch 50/64 loss: -0.7286415100097656
Batch 51/64 loss: -0.382296085357666
Batch 52/64 loss: -0.6073532104492188
Batch 53/64 loss: -0.5306529998779297
Batch 54/64 loss: 1.208592414855957
Batch 55/64 loss: -0.48441457748413086
Batch 56/64 loss: 0.03944730758666992
Batch 57/64 loss: -0.666529655456543
Batch 58/64 loss: -0.6670527458190918
Batch 59/64 loss: -0.5990753173828125
Batch 60/64 loss: -0.7120509147644043
Batch 61/64 loss: -0.6045646667480469
Batch 62/64 loss: -0.5233922004699707
Batch 63/64 loss: 0.6901907920837402
Batch 64/64 loss: -4.054646015167236
Epoch 266  Train loss: -0.5741423793867523  Val loss: -0.9409166971842448
Saving best model, epoch: 266
Epoch 267
-------------------------------
Batch 1/64 loss: -0.6548690795898438
Batch 2/64 loss: -0.29215574264526367
Batch 3/64 loss: 0.5426502227783203
Batch 4/64 loss: -0.6423211097717285
Batch 5/64 loss: -0.511258602142334
Batch 6/64 loss: -0.6970481872558594
Batch 7/64 loss: -0.5519466400146484
Batch 8/64 loss: -0.5721216201782227
Batch 9/64 loss: -0.653724193572998
Batch 10/64 loss: -0.6528744697570801
Batch 11/64 loss: -0.6501941680908203
Batch 12/64 loss: 1.279355525970459
Batch 13/64 loss: -0.5894904136657715
Batch 14/64 loss: -0.5450301170349121
Batch 15/64 loss: -0.7457890510559082
Batch 16/64 loss: -0.6401410102844238
Batch 17/64 loss: 0.010512828826904297
Batch 18/64 loss: -0.5271039009094238
Batch 19/64 loss: -0.6602144241333008
Batch 20/64 loss: -0.6518063545227051
Batch 21/64 loss: -0.6989340782165527
Batch 22/64 loss: -0.4055471420288086
Batch 23/64 loss: -0.537621021270752
Batch 24/64 loss: -0.8179588317871094
Batch 25/64 loss: -0.601715087890625
Batch 26/64 loss: -0.7044806480407715
Batch 27/64 loss: -0.3757028579711914
Batch 28/64 loss: -0.5535669326782227
Batch 29/64 loss: -0.7142248153686523
Batch 30/64 loss: -0.6271100044250488
Batch 31/64 loss: -0.7675685882568359
Batch 32/64 loss: -0.45891761779785156
Batch 33/64 loss: -0.7407259941101074
Batch 34/64 loss: -0.7528896331787109
Batch 35/64 loss: -0.16653156280517578
Batch 36/64 loss: -0.6801247596740723
Batch 37/64 loss: -0.36049890518188477
Batch 38/64 loss: -0.4766674041748047
Batch 39/64 loss: -0.6956338882446289
Batch 40/64 loss: -0.41385793685913086
Batch 41/64 loss: -0.6075239181518555
Batch 42/64 loss: -0.5830302238464355
Batch 43/64 loss: -0.7145648002624512
Batch 44/64 loss: -0.3652610778808594
Batch 45/64 loss: -0.6637477874755859
Batch 46/64 loss: -0.6005282402038574
Batch 47/64 loss: -0.6440839767456055
Batch 48/64 loss: 0.5471224784851074
Batch 49/64 loss: -0.5958313941955566
Batch 50/64 loss: -0.6429233551025391
Batch 51/64 loss: -0.6450071334838867
Batch 52/64 loss: -0.7228803634643555
Batch 53/64 loss: -0.6938967704772949
Batch 54/64 loss: -0.590451717376709
Batch 55/64 loss: -0.7017617225646973
Batch 56/64 loss: -0.6604704856872559
Batch 57/64 loss: -0.4081907272338867
Batch 58/64 loss: -0.6032266616821289
Batch 59/64 loss: -0.36992502212524414
Batch 60/64 loss: -0.6435542106628418
Batch 61/64 loss: -0.5748677253723145
Batch 62/64 loss: -0.5831990242004395
Batch 63/64 loss: -0.6809506416320801
Batch 64/64 loss: -4.221812725067139
Epoch 267  Train loss: -0.5626817609749588  Val loss: -0.9657232802348448
Saving best model, epoch: 267
Epoch 268
-------------------------------
Batch 1/64 loss: -0.16021108627319336
Batch 2/64 loss: -0.682675838470459
Batch 3/64 loss: 0.7783026695251465
Batch 4/64 loss: -0.527376651763916
Batch 5/64 loss: -0.7152504920959473
Batch 6/64 loss: -0.6903233528137207
Batch 7/64 loss: -0.3451828956604004
Batch 8/64 loss: -0.3782644271850586
Batch 9/64 loss: -0.6113996505737305
Batch 10/64 loss: -0.6403117179870605
Batch 11/64 loss: 0.2960319519042969
Batch 12/64 loss: -0.63616943359375
Batch 13/64 loss: -0.6945157051086426
Batch 14/64 loss: -0.612236499786377
Batch 15/64 loss: -0.6939220428466797
Batch 16/64 loss: -0.5056924819946289
Batch 17/64 loss: -0.20496177673339844
Batch 18/64 loss: -0.6011314392089844
Batch 19/64 loss: -0.6842241287231445
Batch 20/64 loss: -0.6287617683410645
Batch 21/64 loss: -0.3074169158935547
Batch 22/64 loss: -0.6495275497436523
Batch 23/64 loss: -0.6801791191101074
Batch 24/64 loss: -0.6459612846374512
Batch 25/64 loss: -0.7141876220703125
Batch 26/64 loss: -0.6910042762756348
Batch 27/64 loss: -0.5340509414672852
Batch 28/64 loss: -0.5042529106140137
Batch 29/64 loss: -0.5082120895385742
Batch 30/64 loss: -0.6880702972412109
Batch 31/64 loss: -0.5864090919494629
Batch 32/64 loss: -0.4936089515686035
Batch 33/64 loss: -0.7958183288574219
Batch 34/64 loss: -0.7089667320251465
Batch 35/64 loss: -0.6030879020690918
Batch 36/64 loss: -0.7034878730773926
Batch 37/64 loss: -0.21289825439453125
Batch 38/64 loss: -0.7426285743713379
Batch 39/64 loss: 0.0731663703918457
Batch 40/64 loss: -0.678807258605957
Batch 41/64 loss: -0.6184206008911133
Batch 42/64 loss: -0.6259136199951172
Batch 43/64 loss: -0.47580528259277344
Batch 44/64 loss: -0.7184886932373047
Batch 45/64 loss: -0.6367721557617188
Batch 46/64 loss: -0.7052617073059082
Batch 47/64 loss: -0.5462894439697266
Batch 48/64 loss: -0.6017942428588867
Batch 49/64 loss: -0.6325592994689941
Batch 50/64 loss: -0.6774768829345703
Batch 51/64 loss: -0.6678915023803711
Batch 52/64 loss: -0.584047794342041
Batch 53/64 loss: -0.2402324676513672
Batch 54/64 loss: -0.5707273483276367
Batch 55/64 loss: -0.5382046699523926
Batch 56/64 loss: -0.5984420776367188
Batch 57/64 loss: -0.6736841201782227
Batch 58/64 loss: -0.6635499000549316
Batch 59/64 loss: -0.5033183097839355
Batch 60/64 loss: -0.7988996505737305
Batch 61/64 loss: -0.6791863441467285
Batch 62/64 loss: -0.6322388648986816
Batch 63/64 loss: -0.5820426940917969
Batch 64/64 loss: -2.5174179077148438
Epoch 268  Train loss: -0.5674195682301241  Val loss: -0.8204112233165204
Epoch 269
-------------------------------
Batch 1/64 loss: -0.6247191429138184
Batch 2/64 loss: -0.49102306365966797
Batch 3/64 loss: -0.5783638954162598
Batch 4/64 loss: -0.5879449844360352
Batch 5/64 loss: -0.7028670310974121
Batch 6/64 loss: -0.6278228759765625
Batch 7/64 loss: -0.1338644027709961
Batch 8/64 loss: -0.5569391250610352
Batch 9/64 loss: -0.5203075408935547
Batch 10/64 loss: -0.46826601028442383
Batch 11/64 loss: -0.4992823600769043
Batch 12/64 loss: -0.731593132019043
Batch 13/64 loss: -0.5825514793395996
Batch 14/64 loss: -0.7598938941955566
Batch 15/64 loss: -0.7303085327148438
Batch 16/64 loss: -0.28614187240600586
Batch 17/64 loss: -0.5628008842468262
Batch 18/64 loss: -0.45442676544189453
Batch 19/64 loss: -0.7453975677490234
Batch 20/64 loss: -0.614931583404541
Batch 21/64 loss: -0.4818539619445801
Batch 22/64 loss: -0.5671639442443848
Batch 23/64 loss: -0.5334253311157227
Batch 24/64 loss: -0.7082242965698242
Batch 25/64 loss: 1.444638729095459
Batch 26/64 loss: 0.026925086975097656
Batch 27/64 loss: -0.5702047348022461
Batch 28/64 loss: 0.06937313079833984
Batch 29/64 loss: -0.22410869598388672
Batch 30/64 loss: -0.5343418121337891
Batch 31/64 loss: -0.6316695213317871
Batch 32/64 loss: -0.2031402587890625
Batch 33/64 loss: -0.6138415336608887
Batch 34/64 loss: -0.37187719345092773
Batch 35/64 loss: 0.9974398612976074
Batch 36/64 loss: -0.40991926193237305
Batch 37/64 loss: -0.560032844543457
Batch 38/64 loss: -0.5861167907714844
Batch 39/64 loss: -0.4986076354980469
Batch 40/64 loss: -0.49469947814941406
Batch 41/64 loss: -0.5660290718078613
Batch 42/64 loss: -0.40953969955444336
Batch 43/64 loss: -0.6010913848876953
Batch 44/64 loss: -0.4714655876159668
Batch 45/64 loss: -0.5221819877624512
Batch 46/64 loss: -0.21535015106201172
Batch 47/64 loss: -0.2750849723815918
Batch 48/64 loss: -0.4915146827697754
Batch 49/64 loss: -0.5425195693969727
Batch 50/64 loss: -0.47154664993286133
Batch 51/64 loss: -0.6234283447265625
Batch 52/64 loss: -0.417724609375
Batch 53/64 loss: -0.611203670501709
Batch 54/64 loss: -0.44569969177246094
Batch 55/64 loss: -0.5900697708129883
Batch 56/64 loss: -0.6116604804992676
Batch 57/64 loss: -0.5746426582336426
Batch 58/64 loss: -0.3504014015197754
Batch 59/64 loss: -0.40186119079589844
Batch 60/64 loss: -0.44594860076904297
Batch 61/64 loss: -0.4755434989929199
Batch 62/64 loss: -0.271944522857666
Batch 63/64 loss: -0.15027093887329102
Batch 64/64 loss: -3.9779491424560547
Epoch 269  Train loss: -0.47420363332711013  Val loss: -0.7027576092592219
Epoch 270
-------------------------------
Batch 1/64 loss: -0.6725273132324219
Batch 2/64 loss: -0.45914411544799805
Batch 3/64 loss: -0.26558732986450195
Batch 4/64 loss: -0.5139307975769043
Batch 5/64 loss: -0.34073638916015625
Batch 6/64 loss: -0.30672311782836914
Batch 7/64 loss: -0.47513580322265625
Batch 8/64 loss: -0.3984413146972656
Batch 9/64 loss: -0.19771480560302734
Batch 10/64 loss: -0.4984569549560547
Batch 11/64 loss: -0.4963035583496094
Batch 12/64 loss: -0.6236753463745117
Batch 13/64 loss: -0.47129106521606445
Batch 14/64 loss: -0.5995640754699707
Batch 15/64 loss: 0.5396318435668945
Batch 16/64 loss: -0.32618141174316406
Batch 17/64 loss: -0.5992927551269531
Batch 18/64 loss: -0.4677290916442871
Batch 19/64 loss: -0.5328879356384277
Batch 20/64 loss: -0.47872400283813477
Batch 21/64 loss: -0.39748382568359375
Batch 22/64 loss: -0.5714106559753418
Batch 23/64 loss: 1.0101022720336914
Batch 24/64 loss: -0.4644346237182617
Batch 25/64 loss: -0.5407876968383789
Batch 26/64 loss: -0.5561103820800781
Batch 27/64 loss: -0.6853809356689453
Batch 28/64 loss: -0.6585121154785156
Batch 29/64 loss: -0.36441516876220703
Batch 30/64 loss: -0.5960288047790527
Batch 31/64 loss: -0.2262248992919922
Batch 32/64 loss: -0.7165508270263672
Batch 33/64 loss: -0.6537866592407227
Batch 34/64 loss: -0.6101408004760742
Batch 35/64 loss: -0.6591410636901855
Batch 36/64 loss: -0.5903339385986328
Batch 37/64 loss: -0.42333507537841797
Batch 38/64 loss: -0.6586170196533203
Batch 39/64 loss: -0.6515369415283203
Batch 40/64 loss: -0.3799118995666504
Batch 41/64 loss: -0.5517029762268066
Batch 42/64 loss: 0.5186042785644531
Batch 43/64 loss: -0.5899734497070312
Batch 44/64 loss: -0.7286052703857422
Batch 45/64 loss: -0.5923037528991699
Batch 46/64 loss: -0.6109943389892578
Batch 47/64 loss: -0.3553142547607422
Batch 48/64 loss: -0.4737076759338379
Batch 49/64 loss: -0.593721866607666
Batch 50/64 loss: -0.6061844825744629
Batch 51/64 loss: -0.4098067283630371
Batch 52/64 loss: 0.0905461311340332
Batch 53/64 loss: -0.42694902420043945
Batch 54/64 loss: -0.34053707122802734
Batch 55/64 loss: -0.6751813888549805
Batch 56/64 loss: -0.6885242462158203
Batch 57/64 loss: -0.6642279624938965
Batch 58/64 loss: -0.5262241363525391
Batch 59/64 loss: -0.2016282081604004
Batch 60/64 loss: -0.7072300910949707
Batch 61/64 loss: -0.41922426223754883
Batch 62/64 loss: -0.27585840225219727
Batch 63/64 loss: -0.0758514404296875
Batch 64/64 loss: -2.0690174102783203
Epoch 270  Train loss: -0.4554481581145642  Val loss: 0.34218858279723074
Epoch 271
-------------------------------
Batch 1/64 loss: -0.08687305450439453
Batch 2/64 loss: 1.3265528678894043
Batch 3/64 loss: 0.2434530258178711
Batch 4/64 loss: -0.03394937515258789
Batch 5/64 loss: 0.9827256202697754
Batch 6/64 loss: 1.4555702209472656
Batch 7/64 loss: 0.1467146873474121
Batch 8/64 loss: -0.04667520523071289
Batch 9/64 loss: 0.34744691848754883
Batch 10/64 loss: 0.23535823822021484
Batch 11/64 loss: 0.05193948745727539
Batch 12/64 loss: 0.18702936172485352
Batch 13/64 loss: 1.638279914855957
Batch 14/64 loss: -0.12354707717895508
Batch 15/64 loss: 0.9752669334411621
Batch 16/64 loss: 0.12146568298339844
Batch 17/64 loss: -0.17741870880126953
Batch 18/64 loss: 1.2098884582519531
Batch 19/64 loss: -0.02485942840576172
Batch 20/64 loss: -0.09395170211791992
Batch 21/64 loss: 0.25948476791381836
Batch 22/64 loss: -0.17371845245361328
Batch 23/64 loss: -0.020986557006835938
Batch 24/64 loss: -0.3141031265258789
Batch 25/64 loss: -0.11432218551635742
Batch 26/64 loss: -0.18027544021606445
Batch 27/64 loss: 0.7595453262329102
Batch 28/64 loss: -0.1070261001586914
Batch 29/64 loss: 0.14845561981201172
Batch 30/64 loss: -0.2981696128845215
Batch 31/64 loss: -0.2447834014892578
Batch 32/64 loss: 0.09216642379760742
Batch 33/64 loss: 1.2756342887878418
Batch 34/64 loss: -0.14029502868652344
Batch 35/64 loss: 0.8630790710449219
Batch 36/64 loss: -0.07637834548950195
Batch 37/64 loss: -0.38466835021972656
Batch 38/64 loss: -0.12906265258789062
Batch 39/64 loss: 0.4335761070251465
Batch 40/64 loss: -0.2552351951599121
Batch 41/64 loss: -0.30059385299682617
Batch 42/64 loss: -0.11478471755981445
Batch 43/64 loss: -0.32970571517944336
Batch 44/64 loss: -0.33054018020629883
Batch 45/64 loss: 0.27052831649780273
Batch 46/64 loss: -0.27980661392211914
Batch 47/64 loss: -0.29648447036743164
Batch 48/64 loss: -0.2965559959411621
Batch 49/64 loss: -0.26394176483154297
Batch 50/64 loss: 0.06338310241699219
Batch 51/64 loss: -0.4194626808166504
Batch 52/64 loss: -0.31281280517578125
Batch 53/64 loss: -0.3200650215148926
Batch 54/64 loss: -0.04749155044555664
Batch 55/64 loss: -0.2031874656677246
Batch 56/64 loss: -0.44988155364990234
Batch 57/64 loss: -0.2781519889831543
Batch 58/64 loss: 1.1323871612548828
Batch 59/64 loss: -0.3251476287841797
Batch 60/64 loss: -0.24605035781860352
Batch 61/64 loss: -0.0710763931274414
Batch 62/64 loss: -0.26938724517822266
Batch 63/64 loss: -0.30933570861816406
Batch 64/64 loss: -3.9082298278808594
Epoch 271  Train loss: 0.04389014150582108  Val loss: -0.5735150104535814
Epoch 272
-------------------------------
Batch 1/64 loss: -0.2988266944885254
Batch 2/64 loss: -0.25536584854125977
Batch 3/64 loss: -0.38240957260131836
Batch 4/64 loss: -0.3848724365234375
Batch 5/64 loss: -0.01144552230834961
Batch 6/64 loss: -0.4756741523742676
Batch 7/64 loss: -0.3748154640197754
Batch 8/64 loss: -0.5356802940368652
Batch 9/64 loss: -0.4210848808288574
Batch 10/64 loss: -0.5922122001647949
Batch 11/64 loss: 0.6918621063232422
Batch 12/64 loss: -0.5763893127441406
Batch 13/64 loss: -0.32522010803222656
Batch 14/64 loss: -0.46642208099365234
Batch 15/64 loss: -0.42905330657958984
Batch 16/64 loss: -0.5072507858276367
Batch 17/64 loss: 0.9385581016540527
Batch 18/64 loss: -0.25601673126220703
Batch 19/64 loss: -0.5565991401672363
Batch 20/64 loss: -0.23360347747802734
Batch 21/64 loss: -0.3778958320617676
Batch 22/64 loss: -0.4358029365539551
Batch 23/64 loss: -0.5417666435241699
Batch 24/64 loss: -0.5897436141967773
Batch 25/64 loss: -0.46831703186035156
Batch 26/64 loss: -0.39180517196655273
Batch 27/64 loss: -0.6524205207824707
Batch 28/64 loss: -0.049657344818115234
Batch 29/64 loss: -0.4906497001647949
Batch 30/64 loss: -0.37800025939941406
Batch 31/64 loss: -0.5127778053283691
Batch 32/64 loss: -0.6520333290100098
Batch 33/64 loss: -0.4254155158996582
Batch 34/64 loss: -0.5203099250793457
Batch 35/64 loss: -0.49463415145874023
Batch 36/64 loss: -0.5467276573181152
Batch 37/64 loss: -0.33611488342285156
Batch 38/64 loss: 0.17970991134643555
Batch 39/64 loss: 0.49385881423950195
Batch 40/64 loss: -0.6389560699462891
Batch 41/64 loss: -0.273287296295166
Batch 42/64 loss: -0.43744945526123047
Batch 43/64 loss: 0.14290142059326172
Batch 44/64 loss: -0.5948367118835449
Batch 45/64 loss: -0.4772305488586426
Batch 46/64 loss: -0.5340771675109863
Batch 47/64 loss: -0.5168566703796387
Batch 48/64 loss: -0.059094905853271484
Batch 49/64 loss: -0.4395127296447754
Batch 50/64 loss: -0.5359916687011719
Batch 51/64 loss: -0.48926639556884766
Batch 52/64 loss: -0.23652029037475586
Batch 53/64 loss: -0.33167266845703125
Batch 54/64 loss: -0.45957517623901367
Batch 55/64 loss: -0.3579444885253906
Batch 56/64 loss: -0.41946887969970703
Batch 57/64 loss: -0.47309112548828125
Batch 58/64 loss: -0.534510612487793
Batch 59/64 loss: -0.6385703086853027
Batch 60/64 loss: -0.2780303955078125
Batch 61/64 loss: -0.5408334732055664
Batch 62/64 loss: -0.573756217956543
Batch 63/64 loss: -0.32409143447875977
Batch 64/64 loss: -4.066567420959473
Epoch 272  Train loss: -0.4033674389708276  Val loss: -0.7783437381495315
Epoch 273
-------------------------------
Batch 1/64 loss: -0.7346596717834473
Batch 2/64 loss: -0.49344587326049805
Batch 3/64 loss: -0.6795167922973633
Batch 4/64 loss: 0.057109832763671875
Batch 5/64 loss: -0.6787915229797363
Batch 6/64 loss: -0.22129344940185547
Batch 7/64 loss: -0.5997529029846191
Batch 8/64 loss: -0.4524855613708496
Batch 9/64 loss: -0.577484130859375
Batch 10/64 loss: -0.5137815475463867
Batch 11/64 loss: -0.3724055290222168
Batch 12/64 loss: -0.6897983551025391
Batch 13/64 loss: -0.6498823165893555
Batch 14/64 loss: -0.2002410888671875
Batch 15/64 loss: -0.36263465881347656
Batch 16/64 loss: -0.2524561882019043
Batch 17/64 loss: -0.637077808380127
Batch 18/64 loss: -0.007090091705322266
Batch 19/64 loss: -0.4345512390136719
Batch 20/64 loss: -0.6252899169921875
Batch 21/64 loss: -0.42816162109375
Batch 22/64 loss: -0.2549610137939453
Batch 23/64 loss: -0.6632223129272461
Batch 24/64 loss: -0.6498193740844727
Batch 25/64 loss: -0.5728516578674316
Batch 26/64 loss: 0.30587053298950195
Batch 27/64 loss: -0.32483530044555664
Batch 28/64 loss: -0.3752322196960449
Batch 29/64 loss: -0.538304328918457
Batch 30/64 loss: -0.09337711334228516
Batch 31/64 loss: -0.4072895050048828
Batch 32/64 loss: -0.5309596061706543
Batch 33/64 loss: -0.6425747871398926
Batch 34/64 loss: -0.29564714431762695
Batch 35/64 loss: -0.4024057388305664
Batch 36/64 loss: -0.5895075798034668
Batch 37/64 loss: -0.5772600173950195
Batch 38/64 loss: -0.4126901626586914
Batch 39/64 loss: -0.4957289695739746
Batch 40/64 loss: -0.44245004653930664
Batch 41/64 loss: -0.5738224983215332
Batch 42/64 loss: -0.17194414138793945
Batch 43/64 loss: -0.28400564193725586
Batch 44/64 loss: 1.8869447708129883
Batch 45/64 loss: -0.6679620742797852
Batch 46/64 loss: 0.9333853721618652
Batch 47/64 loss: 1.1153759956359863
Batch 48/64 loss: -0.5215420722961426
Batch 49/64 loss: -0.435060977935791
Batch 50/64 loss: -0.41043853759765625
Batch 51/64 loss: -0.16460227966308594
Batch 52/64 loss: -0.45614004135131836
Batch 53/64 loss: -0.186370849609375
Batch 54/64 loss: -0.4599885940551758
Batch 55/64 loss: -0.4835333824157715
Batch 56/64 loss: -0.4683709144592285
Batch 57/64 loss: -0.39869260787963867
Batch 58/64 loss: -0.4326152801513672
Batch 59/64 loss: -0.45613622665405273
Batch 60/64 loss: -0.30695581436157227
Batch 61/64 loss: -0.2737860679626465
Batch 62/64 loss: -0.35799121856689453
Batch 63/64 loss: -0.552250862121582
Batch 64/64 loss: -3.9089417457580566
Epoch 273  Train loss: -0.3854925024743174  Val loss: -0.6819124254574072
Epoch 274
-------------------------------
Batch 1/64 loss: -0.49971437454223633
Batch 2/64 loss: -0.5857129096984863
Batch 3/64 loss: -0.36837053298950195
Batch 4/64 loss: -0.35452747344970703
Batch 5/64 loss: -0.5478963851928711
Batch 6/64 loss: -0.42420339584350586
Batch 7/64 loss: 0.9841985702514648
Batch 8/64 loss: -0.4484124183654785
Batch 9/64 loss: -0.45087337493896484
Batch 10/64 loss: -0.23121070861816406
Batch 11/64 loss: -0.3936123847961426
Batch 12/64 loss: -0.511939525604248
Batch 13/64 loss: 0.295896053314209
Batch 14/64 loss: 0.1875748634338379
Batch 15/64 loss: -0.3328237533569336
Batch 16/64 loss: -0.33583974838256836
Batch 17/64 loss: -0.6081595420837402
Batch 18/64 loss: -0.5039196014404297
Batch 19/64 loss: -0.27949094772338867
Batch 20/64 loss: -0.3396735191345215
Batch 21/64 loss: -0.597442626953125
Batch 22/64 loss: -0.4702749252319336
Batch 23/64 loss: -0.17847681045532227
Batch 24/64 loss: -0.52264404296875
Batch 25/64 loss: -0.43581104278564453
Batch 26/64 loss: -0.5836644172668457
Batch 27/64 loss: -0.38117265701293945
Batch 28/64 loss: -0.6272258758544922
Batch 29/64 loss: -0.24719953536987305
Batch 30/64 loss: -0.29892778396606445
Batch 31/64 loss: -0.2043752670288086
Batch 32/64 loss: -0.5742220878601074
Batch 33/64 loss: -0.5900983810424805
Batch 34/64 loss: -0.633641242980957
Batch 35/64 loss: -0.20839166641235352
Batch 36/64 loss: -0.436643123626709
Batch 37/64 loss: -0.37272071838378906
Batch 38/64 loss: -0.4324221611022949
Batch 39/64 loss: -0.273531436920166
Batch 40/64 loss: -0.6950879096984863
Batch 41/64 loss: -0.1830449104309082
Batch 42/64 loss: 0.4528350830078125
Batch 43/64 loss: -0.3587918281555176
Batch 44/64 loss: -0.5923724174499512
Batch 45/64 loss: -0.5914530754089355
Batch 46/64 loss: 0.689180850982666
Batch 47/64 loss: -0.2817702293395996
Batch 48/64 loss: -0.6654276847839355
Batch 49/64 loss: -0.6060500144958496
Batch 50/64 loss: -0.0837106704711914
Batch 51/64 loss: -0.3561592102050781
Batch 52/64 loss: -0.5477757453918457
Batch 53/64 loss: -0.1886286735534668
Batch 54/64 loss: -0.4442753791809082
Batch 55/64 loss: -0.654386043548584
Batch 56/64 loss: -0.7346038818359375
Batch 57/64 loss: -0.47570323944091797
Batch 58/64 loss: -0.6276707649230957
Batch 59/64 loss: -0.5973458290100098
Batch 60/64 loss: -0.5780186653137207
Batch 61/64 loss: -0.7501144409179688
Batch 62/64 loss: -0.3602256774902344
Batch 63/64 loss: -0.5099043846130371
Batch 64/64 loss: -3.708740711212158
Epoch 274  Train loss: -0.4131710931366565  Val loss: -0.8510251782604099
Epoch 275
-------------------------------
Batch 1/64 loss: -0.2781863212585449
Batch 2/64 loss: -0.33969688415527344
Batch 3/64 loss: -0.6455979347229004
Batch 4/64 loss: -0.6483798027038574
Batch 5/64 loss: -0.2948336601257324
Batch 6/64 loss: 0.574152946472168
Batch 7/64 loss: 0.9101929664611816
Batch 8/64 loss: -0.6850442886352539
Batch 9/64 loss: -0.45307016372680664
Batch 10/64 loss: -0.39781808853149414
Batch 11/64 loss: -0.6380219459533691
Batch 12/64 loss: -0.5468382835388184
Batch 13/64 loss: -0.5049729347229004
Batch 14/64 loss: -0.6350021362304688
Batch 15/64 loss: -0.5746212005615234
Batch 16/64 loss: -0.5769095420837402
Batch 17/64 loss: -0.607757568359375
Batch 18/64 loss: -0.5785927772521973
Batch 19/64 loss: -0.049031734466552734
Batch 20/64 loss: -0.6350955963134766
Batch 21/64 loss: -0.7082147598266602
Batch 22/64 loss: -0.45766496658325195
Batch 23/64 loss: -0.5043478012084961
Batch 24/64 loss: -0.7500371932983398
Batch 25/64 loss: -0.5781569480895996
Batch 26/64 loss: -0.5956053733825684
Batch 27/64 loss: -0.48294734954833984
Batch 28/64 loss: 0.07742834091186523
Batch 29/64 loss: -0.3571295738220215
Batch 30/64 loss: -0.48220300674438477
Batch 31/64 loss: -0.5193147659301758
Batch 32/64 loss: -0.40036582946777344
Batch 33/64 loss: -0.6333041191101074
Batch 34/64 loss: -0.6882495880126953
Batch 35/64 loss: -0.6057581901550293
Batch 36/64 loss: -0.7788815498352051
Batch 37/64 loss: -0.6336560249328613
Batch 38/64 loss: -0.6243338584899902
Batch 39/64 loss: -0.6122994422912598
Batch 40/64 loss: -0.3444027900695801
Batch 41/64 loss: -0.6975927352905273
Batch 42/64 loss: -0.5311746597290039
Batch 43/64 loss: -0.5773968696594238
Batch 44/64 loss: -0.5894036293029785
Batch 45/64 loss: -0.7453761100769043
Batch 46/64 loss: -0.645576000213623
Batch 47/64 loss: -0.3715653419494629
Batch 48/64 loss: -0.639765739440918
Batch 49/64 loss: -0.6283187866210938
Batch 50/64 loss: -0.6949863433837891
Batch 51/64 loss: -0.4652590751647949
Batch 52/64 loss: -0.7528433799743652
Batch 53/64 loss: -0.5730209350585938
Batch 54/64 loss: -0.5398526191711426
Batch 55/64 loss: -0.5314898490905762
Batch 56/64 loss: -0.49780702590942383
Batch 57/64 loss: -0.6791567802429199
Batch 58/64 loss: -0.5886940956115723
Batch 59/64 loss: 0.8870506286621094
Batch 60/64 loss: -0.6356043815612793
Batch 61/64 loss: -0.7032537460327148
Batch 62/64 loss: -0.6034226417541504
Batch 63/64 loss: -0.29244422912597656
Batch 64/64 loss: -4.126395225524902
Epoch 275  Train loss: -0.5251187530218386  Val loss: -0.8424961575118127
Epoch 276
-------------------------------
Batch 1/64 loss: -0.16700220108032227
Batch 2/64 loss: -0.6027402877807617
Batch 3/64 loss: -0.6765842437744141
Batch 4/64 loss: -0.5869994163513184
Batch 5/64 loss: -0.5799417495727539
Batch 6/64 loss: -0.674260139465332
Batch 7/64 loss: -0.6612839698791504
Batch 8/64 loss: -0.5096802711486816
Batch 9/64 loss: -0.6646928787231445
Batch 10/64 loss: -0.6399087905883789
Batch 11/64 loss: -0.6783261299133301
Batch 12/64 loss: -0.6231813430786133
Batch 13/64 loss: -0.6267299652099609
Batch 14/64 loss: -0.41721439361572266
Batch 15/64 loss: -0.5192980766296387
Batch 16/64 loss: -0.2524738311767578
Batch 17/64 loss: -0.4540553092956543
Batch 18/64 loss: -0.5336050987243652
Batch 19/64 loss: 0.8270602226257324
Batch 20/64 loss: -0.5548419952392578
Batch 21/64 loss: -0.5285458564758301
Batch 22/64 loss: -0.4743156433105469
Batch 23/64 loss: -0.4462885856628418
Batch 24/64 loss: -0.6127233505249023
Batch 25/64 loss: -0.5837759971618652
Batch 26/64 loss: -0.6615481376647949
Batch 27/64 loss: -0.7605977058410645
Batch 28/64 loss: -0.5398483276367188
Batch 29/64 loss: -0.3959660530090332
Batch 30/64 loss: -0.5519862174987793
Batch 31/64 loss: -0.6652331352233887
Batch 32/64 loss: -0.6156067848205566
Batch 33/64 loss: -0.5988254547119141
Batch 34/64 loss: -0.4537773132324219
Batch 35/64 loss: -0.49400758743286133
Batch 36/64 loss: -0.7174530029296875
Batch 37/64 loss: -0.7121067047119141
Batch 38/64 loss: -0.5327997207641602
Batch 39/64 loss: -0.6717872619628906
Batch 40/64 loss: -0.6081523895263672
Batch 41/64 loss: -0.590764045715332
Batch 42/64 loss: -0.34929800033569336
Batch 43/64 loss: 0.10463619232177734
Batch 44/64 loss: -0.26061582565307617
Batch 45/64 loss: 0.2488117218017578
Batch 46/64 loss: -0.6259016990661621
Batch 47/64 loss: -0.752657413482666
Batch 48/64 loss: -0.6162204742431641
Batch 49/64 loss: -0.12145280838012695
Batch 50/64 loss: -0.5110750198364258
Batch 51/64 loss: -0.6897163391113281
Batch 52/64 loss: -0.4347076416015625
Batch 53/64 loss: -0.6260175704956055
Batch 54/64 loss: -0.40120792388916016
Batch 55/64 loss: -0.7256784439086914
Batch 56/64 loss: -0.4982566833496094
Batch 57/64 loss: -0.44780731201171875
Batch 58/64 loss: -0.6105623245239258
Batch 59/64 loss: 0.6003937721252441
Batch 60/64 loss: -0.4677886962890625
Batch 61/64 loss: -0.6321549415588379
Batch 62/64 loss: -0.5103883743286133
Batch 63/64 loss: -0.6254568099975586
Batch 64/64 loss: -2.5094785690307617
Epoch 276  Train loss: -0.5121113833259133  Val loss: -0.7872431384738778
Epoch 277
-------------------------------
Batch 1/64 loss: -0.404754638671875
Batch 2/64 loss: -0.3019437789916992
Batch 3/64 loss: -0.5955023765563965
Batch 4/64 loss: -0.6268458366394043
Batch 5/64 loss: -0.3656888008117676
Batch 6/64 loss: 0.10192489624023438
Batch 7/64 loss: 0.8446741104125977
Batch 8/64 loss: -0.38659000396728516
Batch 9/64 loss: -0.6651558876037598
Batch 10/64 loss: -0.674126148223877
Batch 11/64 loss: -0.3757753372192383
Batch 12/64 loss: 0.7099871635437012
Batch 13/64 loss: -0.6225619316101074
Batch 14/64 loss: -0.725853443145752
Batch 15/64 loss: -0.19683122634887695
Batch 16/64 loss: -0.1644148826599121
Batch 17/64 loss: -0.5253963470458984
Batch 18/64 loss: -0.41983556747436523
Batch 19/64 loss: -0.25802183151245117
Batch 20/64 loss: -0.27735328674316406
Batch 21/64 loss: -0.3188810348510742
Batch 22/64 loss: -0.5327963829040527
Batch 23/64 loss: -0.3391847610473633
Batch 24/64 loss: -0.4821019172668457
Batch 25/64 loss: 1.144571304321289
Batch 26/64 loss: -0.4493117332458496
Batch 27/64 loss: -0.5852527618408203
Batch 28/64 loss: -0.44498777389526367
Batch 29/64 loss: -0.33123207092285156
Batch 30/64 loss: -0.3713951110839844
Batch 31/64 loss: -0.5487537384033203
Batch 32/64 loss: -0.46125078201293945
Batch 33/64 loss: -0.557316780090332
Batch 34/64 loss: -0.5907526016235352
Batch 35/64 loss: -0.43082237243652344
Batch 36/64 loss: -0.5231094360351562
Batch 37/64 loss: -0.6517715454101562
Batch 38/64 loss: -0.3862595558166504
Batch 39/64 loss: -0.5781941413879395
Batch 40/64 loss: -0.6604070663452148
Batch 41/64 loss: -0.5890159606933594
Batch 42/64 loss: -0.6254696846008301
Batch 43/64 loss: -0.6720237731933594
Batch 44/64 loss: -0.24207401275634766
Batch 45/64 loss: -0.516446590423584
Batch 46/64 loss: -0.5583000183105469
Batch 47/64 loss: -0.6807065010070801
Batch 48/64 loss: -0.5088558197021484
Batch 49/64 loss: 0.272127628326416
Batch 50/64 loss: -0.5468535423278809
Batch 51/64 loss: -0.47895145416259766
Batch 52/64 loss: -0.4787259101867676
Batch 53/64 loss: -0.42763280868530273
Batch 54/64 loss: -0.4523448944091797
Batch 55/64 loss: -0.6546821594238281
Batch 56/64 loss: -0.6121072769165039
Batch 57/64 loss: -0.6725144386291504
Batch 58/64 loss: -0.6522750854492188
Batch 59/64 loss: -0.6148514747619629
Batch 60/64 loss: 0.26822710037231445
Batch 61/64 loss: -0.33448362350463867
Batch 62/64 loss: -0.05664396286010742
Batch 63/64 loss: -0.5794882774353027
Batch 64/64 loss: -4.282571315765381
Epoch 277  Train loss: -0.43380857168459425  Val loss: -0.8524950951645055
Epoch 278
-------------------------------
Batch 1/64 loss: -0.5385150909423828
Batch 2/64 loss: -0.3642444610595703
Batch 3/64 loss: -0.07843637466430664
Batch 4/64 loss: 0.5120787620544434
Batch 5/64 loss: -0.23721027374267578
Batch 6/64 loss: -0.46697187423706055
Batch 7/64 loss: -0.6301956176757812
Batch 8/64 loss: -0.1286478042602539
Batch 9/64 loss: -0.6214418411254883
Batch 10/64 loss: -0.4722280502319336
Batch 11/64 loss: -0.6062860488891602
Batch 12/64 loss: -0.5117449760437012
Batch 13/64 loss: -0.670135498046875
Batch 14/64 loss: -0.5244913101196289
Batch 15/64 loss: -0.5252938270568848
Batch 16/64 loss: -0.7649493217468262
Batch 17/64 loss: -0.6869645118713379
Batch 18/64 loss: -0.7665290832519531
Batch 19/64 loss: 0.2781410217285156
Batch 20/64 loss: -0.5852670669555664
Batch 21/64 loss: -0.6575207710266113
Batch 22/64 loss: -0.43308115005493164
Batch 23/64 loss: 0.6469259262084961
Batch 24/64 loss: -0.7304754257202148
Batch 25/64 loss: -0.6360220909118652
Batch 26/64 loss: -0.7090544700622559
Batch 27/64 loss: -0.6841974258422852
Batch 28/64 loss: -0.5966396331787109
Batch 29/64 loss: -0.5050458908081055
Batch 30/64 loss: -0.48442888259887695
Batch 31/64 loss: -0.5580344200134277
Batch 32/64 loss: -0.3833732604980469
Batch 33/64 loss: -0.5829544067382812
Batch 34/64 loss: -0.7231221199035645
Batch 35/64 loss: -0.7215604782104492
Batch 36/64 loss: -0.5892481803894043
Batch 37/64 loss: -0.6423053741455078
Batch 38/64 loss: -0.5580778121948242
Batch 39/64 loss: -0.41114330291748047
Batch 40/64 loss: -0.22115564346313477
Batch 41/64 loss: -0.6132397651672363
Batch 42/64 loss: -0.4233255386352539
Batch 43/64 loss: -0.5322117805480957
Batch 44/64 loss: -0.6386089324951172
Batch 45/64 loss: -0.6212563514709473
Batch 46/64 loss: -0.5896592140197754
Batch 47/64 loss: -0.6635746955871582
Batch 48/64 loss: -0.6463613510131836
Batch 49/64 loss: -0.4929966926574707
Batch 50/64 loss: -0.5604953765869141
Batch 51/64 loss: -0.584953784942627
Batch 52/64 loss: -0.5519061088562012
Batch 53/64 loss: -0.467893123626709
Batch 54/64 loss: -0.5782437324523926
Batch 55/64 loss: -0.7219953536987305
Batch 56/64 loss: -0.576934814453125
Batch 57/64 loss: -0.5820860862731934
Batch 58/64 loss: 1.0247001647949219
Batch 59/64 loss: -0.6193275451660156
Batch 60/64 loss: -0.5744504928588867
Batch 61/64 loss: -0.5413870811462402
Batch 62/64 loss: -0.6214075088500977
Batch 63/64 loss: -0.5272526741027832
Batch 64/64 loss: -3.798981189727783
Epoch 278  Train loss: -0.5195914006700703  Val loss: -0.8246919035501906
Epoch 279
-------------------------------
Batch 1/64 loss: -0.48425817489624023
Batch 2/64 loss: -0.5588216781616211
Batch 3/64 loss: -0.4986138343811035
Batch 4/64 loss: -0.6085124015808105
Batch 5/64 loss: 0.8789205551147461
Batch 6/64 loss: -0.6092047691345215
Batch 7/64 loss: -0.4346790313720703
Batch 8/64 loss: -0.5953636169433594
Batch 9/64 loss: -0.579160213470459
Batch 10/64 loss: -0.37637948989868164
Batch 11/64 loss: -0.5082383155822754
Batch 12/64 loss: -0.5808343887329102
Batch 13/64 loss: -0.6227664947509766
Batch 14/64 loss: -0.6520171165466309
Batch 15/64 loss: -0.5485773086547852
Batch 16/64 loss: 0.07790756225585938
Batch 17/64 loss: 0.5542187690734863
Batch 18/64 loss: -0.48963212966918945
Batch 19/64 loss: -0.5051784515380859
Batch 20/64 loss: -0.5406708717346191
Batch 21/64 loss: -0.45531415939331055
Batch 22/64 loss: -0.44397926330566406
Batch 23/64 loss: -0.3748602867126465
Batch 24/64 loss: -0.47827577590942383
Batch 25/64 loss: -0.3983755111694336
Batch 26/64 loss: -0.293759822845459
Batch 27/64 loss: -0.25125980377197266
Batch 28/64 loss: -0.12616729736328125
Batch 29/64 loss: -0.3883066177368164
Batch 30/64 loss: -0.5517311096191406
Batch 31/64 loss: 0.017493247985839844
Batch 32/64 loss: -0.323819637298584
Batch 33/64 loss: -0.5473122596740723
Batch 34/64 loss: 0.08039188385009766
Batch 35/64 loss: -0.40802669525146484
Batch 36/64 loss: -0.4917325973510742
Batch 37/64 loss: -0.5085282325744629
Batch 38/64 loss: -0.42856788635253906
Batch 39/64 loss: -0.6340956687927246
Batch 40/64 loss: -0.5167684555053711
Batch 41/64 loss: -0.49503660202026367
Batch 42/64 loss: -0.43423938751220703
Batch 43/64 loss: -0.5124716758728027
Batch 44/64 loss: -0.5203027725219727
Batch 45/64 loss: -0.6107392311096191
Batch 46/64 loss: -0.36763668060302734
Batch 47/64 loss: -0.7473349571228027
Batch 48/64 loss: -0.3548622131347656
Batch 49/64 loss: -0.7037630081176758
Batch 50/64 loss: -0.6320748329162598
Batch 51/64 loss: -0.43305015563964844
Batch 52/64 loss: -0.28070831298828125
Batch 53/64 loss: -0.3679628372192383
Batch 54/64 loss: -0.49071836471557617
Batch 55/64 loss: -0.7097983360290527
Batch 56/64 loss: -0.556333065032959
Batch 57/64 loss: 0.4593639373779297
Batch 58/64 loss: -0.7171249389648438
Batch 59/64 loss: -0.4708123207092285
Batch 60/64 loss: -0.6449317932128906
Batch 61/64 loss: -0.6406726837158203
Batch 62/64 loss: -0.5322432518005371
Batch 63/64 loss: -0.6982474327087402
Batch 64/64 loss: -4.343173503875732
Epoch 279  Train loss: -0.4693951120563582  Val loss: -0.9307834192649606
Epoch 280
-------------------------------
Batch 1/64 loss: -0.4908928871154785
Batch 2/64 loss: -0.20509672164916992
Batch 3/64 loss: -0.6754002571105957
Batch 4/64 loss: -0.5893325805664062
Batch 5/64 loss: -0.621861457824707
Batch 6/64 loss: -0.6488685607910156
Batch 7/64 loss: -0.6499261856079102
Batch 8/64 loss: -0.7288031578063965
Batch 9/64 loss: -0.4333934783935547
Batch 10/64 loss: -0.6307401657104492
Batch 11/64 loss: -0.6497635841369629
Batch 12/64 loss: -0.657869815826416
Batch 13/64 loss: -0.39730215072631836
Batch 14/64 loss: -0.6095867156982422
Batch 15/64 loss: -0.4922213554382324
Batch 16/64 loss: -0.48666906356811523
Batch 17/64 loss: -0.23582839965820312
Batch 18/64 loss: -0.4539661407470703
Batch 19/64 loss: -0.49887800216674805
Batch 20/64 loss: -0.6689887046813965
Batch 21/64 loss: -0.666964054107666
Batch 22/64 loss: -0.6687588691711426
Batch 23/64 loss: -0.7257289886474609
Batch 24/64 loss: -0.7242403030395508
Batch 25/64 loss: -0.6843123435974121
Batch 26/64 loss: -0.5504012107849121
Batch 27/64 loss: -0.6248683929443359
Batch 28/64 loss: -0.6913280487060547
Batch 29/64 loss: -0.6115431785583496
Batch 30/64 loss: -0.706573486328125
Batch 31/64 loss: -0.3102731704711914
Batch 32/64 loss: -0.7739629745483398
Batch 33/64 loss: 0.3128089904785156
Batch 34/64 loss: -0.7436270713806152
Batch 35/64 loss: -0.7151942253112793
Batch 36/64 loss: -0.8294219970703125
Batch 37/64 loss: -0.7826571464538574
Batch 38/64 loss: -0.6190495491027832
Batch 39/64 loss: 0.6824417114257812
Batch 40/64 loss: -0.4056081771850586
Batch 41/64 loss: -0.6484813690185547
Batch 42/64 loss: -0.5601844787597656
Batch 43/64 loss: -0.6439323425292969
Batch 44/64 loss: -0.3563990592956543
Batch 45/64 loss: 0.43979310989379883
Batch 46/64 loss: -0.6379594802856445
Batch 47/64 loss: -0.6226892471313477
Batch 48/64 loss: -0.6899237632751465
Batch 49/64 loss: -0.6595268249511719
Batch 50/64 loss: -0.7826991081237793
Batch 51/64 loss: -0.7125205993652344
Batch 52/64 loss: -0.6068458557128906
Batch 53/64 loss: -0.5778589248657227
Batch 54/64 loss: -0.7962183952331543
Batch 55/64 loss: -0.6945877075195312
Batch 56/64 loss: -0.33960437774658203
Batch 57/64 loss: -0.7152295112609863
Batch 58/64 loss: -0.06132841110229492
Batch 59/64 loss: -0.7360072135925293
Batch 60/64 loss: -0.6727294921875
Batch 61/64 loss: -0.6497111320495605
Batch 62/64 loss: -0.7524690628051758
Batch 63/64 loss: -0.5287528038024902
Batch 64/64 loss: -4.079319477081299
Epoch 280  Train loss: -0.5918432553609212  Val loss: -0.7200318562615778
Epoch 281
-------------------------------
Batch 1/64 loss: 0.7357115745544434
Batch 2/64 loss: -0.6165485382080078
Batch 3/64 loss: -0.3792738914489746
Batch 4/64 loss: -0.47879552841186523
Batch 5/64 loss: -0.5615673065185547
Batch 6/64 loss: -0.7189998626708984
Batch 7/64 loss: -0.5642638206481934
Batch 8/64 loss: -0.12735652923583984
Batch 9/64 loss: -0.48644590377807617
Batch 10/64 loss: -0.6607170104980469
Batch 11/64 loss: -0.601287841796875
Batch 12/64 loss: -0.10540390014648438
Batch 13/64 loss: -0.615959644317627
Batch 14/64 loss: 0.7550430297851562
Batch 15/64 loss: -0.27962493896484375
Batch 16/64 loss: -0.7486810684204102
Batch 17/64 loss: -0.6804370880126953
Batch 18/64 loss: -0.5627665519714355
Batch 19/64 loss: -0.3911900520324707
Batch 20/64 loss: -0.6072311401367188
Batch 21/64 loss: -0.44591569900512695
Batch 22/64 loss: -0.7440891265869141
Batch 23/64 loss: -0.689300537109375
Batch 24/64 loss: -0.7713079452514648
Batch 25/64 loss: -0.7179203033447266
Batch 26/64 loss: -0.4963235855102539
Batch 27/64 loss: -0.529569149017334
Batch 28/64 loss: -0.3536033630371094
Batch 29/64 loss: -0.541409969329834
Batch 30/64 loss: -0.7112812995910645
Batch 31/64 loss: -0.4926748275756836
Batch 32/64 loss: -0.6091680526733398
Batch 33/64 loss: -0.6223359107971191
Batch 34/64 loss: -0.6204876899719238
Batch 35/64 loss: -0.5734081268310547
Batch 36/64 loss: -0.7727255821228027
Batch 37/64 loss: -0.5196051597595215
Batch 38/64 loss: -0.794344425201416
Batch 39/64 loss: -0.6462182998657227
Batch 40/64 loss: -0.7169580459594727
Batch 41/64 loss: -0.6651453971862793
Batch 42/64 loss: -0.11959314346313477
Batch 43/64 loss: -0.7000293731689453
Batch 44/64 loss: -0.47306156158447266
Batch 45/64 loss: -0.6567173004150391
Batch 46/64 loss: -0.6650409698486328
Batch 47/64 loss: -0.7195034027099609
Batch 48/64 loss: -0.4288506507873535
Batch 49/64 loss: -0.7810564041137695
Batch 50/64 loss: -0.8226971626281738
Batch 51/64 loss: 0.5706386566162109
Batch 52/64 loss: -0.5838065147399902
Batch 53/64 loss: -0.47397804260253906
Batch 54/64 loss: -0.734128475189209
Batch 55/64 loss: -0.6325697898864746
Batch 56/64 loss: -0.4592609405517578
Batch 57/64 loss: -0.6661744117736816
Batch 58/64 loss: -0.39819765090942383
Batch 59/64 loss: -0.6141562461853027
Batch 60/64 loss: -0.6743927001953125
Batch 61/64 loss: -0.7274889945983887
Batch 62/64 loss: -0.6549844741821289
Batch 63/64 loss: -0.5941638946533203
Batch 64/64 loss: -4.200404644012451
Epoch 281  Train loss: -0.5629663598303701  Val loss: -0.8972856056239596
Epoch 282
-------------------------------
Batch 1/64 loss: -0.47083616256713867
Batch 2/64 loss: -0.6036391258239746
Batch 3/64 loss: -0.4233684539794922
Batch 4/64 loss: -0.641059398651123
Batch 5/64 loss: -0.45722150802612305
Batch 6/64 loss: -0.4942030906677246
Batch 7/64 loss: -0.5860614776611328
Batch 8/64 loss: 0.4635024070739746
Batch 9/64 loss: -0.2843608856201172
Batch 10/64 loss: 0.1934657096862793
Batch 11/64 loss: -0.4207305908203125
Batch 12/64 loss: 0.9306254386901855
Batch 13/64 loss: -0.34700918197631836
Batch 14/64 loss: -0.43550729751586914
Batch 15/64 loss: -0.46942901611328125
Batch 16/64 loss: -0.6451268196105957
Batch 17/64 loss: -0.5865230560302734
Batch 18/64 loss: -0.5568461418151855
Batch 19/64 loss: -0.5198111534118652
Batch 20/64 loss: -0.6940245628356934
Batch 21/64 loss: -0.4618210792541504
Batch 22/64 loss: -0.6140923500061035
Batch 23/64 loss: -0.5322532653808594
Batch 24/64 loss: -0.5503997802734375
Batch 25/64 loss: -0.3181309700012207
Batch 26/64 loss: -0.33513975143432617
Batch 27/64 loss: -0.6183357238769531
Batch 28/64 loss: -0.5602574348449707
Batch 29/64 loss: 0.5026922225952148
Batch 30/64 loss: -0.682976245880127
Batch 31/64 loss: -0.6078939437866211
Batch 32/64 loss: -0.7398900985717773
Batch 33/64 loss: -0.5340838432312012
Batch 34/64 loss: -0.034207820892333984
Batch 35/64 loss: -0.6868371963500977
Batch 36/64 loss: -0.7013492584228516
Batch 37/64 loss: -0.7354207038879395
Batch 38/64 loss: -0.48540782928466797
Batch 39/64 loss: -0.6179180145263672
Batch 40/64 loss: -0.44778966903686523
Batch 41/64 loss: -0.4583277702331543
Batch 42/64 loss: 0.008830547332763672
Batch 43/64 loss: -0.6454610824584961
Batch 44/64 loss: -0.5857634544372559
Batch 45/64 loss: -0.45981693267822266
Batch 46/64 loss: -0.49703264236450195
Batch 47/64 loss: -0.5244874954223633
Batch 48/64 loss: -0.5957651138305664
Batch 49/64 loss: -0.5043954849243164
Batch 50/64 loss: -0.6291928291320801
Batch 51/64 loss: -0.5597195625305176
Batch 52/64 loss: -0.600797176361084
Batch 53/64 loss: -0.6696882247924805
Batch 54/64 loss: -0.48952198028564453
Batch 55/64 loss: -0.6265425682067871
Batch 56/64 loss: -0.5298504829406738
Batch 57/64 loss: -0.6794567108154297
Batch 58/64 loss: -0.6529736518859863
Batch 59/64 loss: -0.7201571464538574
Batch 60/64 loss: -0.687861442565918
Batch 61/64 loss: -0.5884518623352051
Batch 62/64 loss: -0.6474795341491699
Batch 63/64 loss: -0.5862464904785156
Batch 64/64 loss: -4.295750617980957
Epoch 282  Train loss: -0.5170454773248411  Val loss: -0.950837990672318
Epoch 283
-------------------------------
Batch 1/64 loss: -0.32777929306030273
Batch 2/64 loss: -0.7055187225341797
Batch 3/64 loss: -0.5426015853881836
Batch 4/64 loss: -0.6750755310058594
Batch 5/64 loss: -0.5789775848388672
Batch 6/64 loss: -0.580599308013916
Batch 7/64 loss: -0.7470030784606934
Batch 8/64 loss: -0.5235252380371094
Batch 9/64 loss: -0.6482176780700684
Batch 10/64 loss: -0.6754741668701172
Batch 11/64 loss: 0.0344691276550293
Batch 12/64 loss: -0.4837827682495117
Batch 13/64 loss: -0.4743056297302246
Batch 14/64 loss: -0.6798033714294434
Batch 15/64 loss: -0.49276304244995117
Batch 16/64 loss: 0.12388324737548828
Batch 17/64 loss: -0.3772106170654297
Batch 18/64 loss: -0.3975348472595215
Batch 19/64 loss: -0.3069744110107422
Batch 20/64 loss: -0.6610021591186523
Batch 21/64 loss: -0.5588259696960449
Batch 22/64 loss: -0.4474363327026367
Batch 23/64 loss: -0.6447067260742188
Batch 24/64 loss: -0.6082463264465332
Batch 25/64 loss: -0.5459122657775879
Batch 26/64 loss: -0.6293554306030273
Batch 27/64 loss: -0.6467776298522949
Batch 28/64 loss: -0.5683012008666992
Batch 29/64 loss: -0.4818897247314453
Batch 30/64 loss: -0.6144981384277344
Batch 31/64 loss: -0.6804709434509277
Batch 32/64 loss: -0.5598711967468262
Batch 33/64 loss: -0.6015615463256836
Batch 34/64 loss: -0.45302295684814453
Batch 35/64 loss: 0.529045581817627
Batch 36/64 loss: -0.49025630950927734
Batch 37/64 loss: -0.6217436790466309
Batch 38/64 loss: -0.44838905334472656
Batch 39/64 loss: -0.43769264221191406
Batch 40/64 loss: -0.5479717254638672
Batch 41/64 loss: -0.5175924301147461
Batch 42/64 loss: -0.6481552124023438
Batch 43/64 loss: -0.6955890655517578
Batch 44/64 loss: -0.5720596313476562
Batch 45/64 loss: -0.595238208770752
Batch 46/64 loss: -0.5685782432556152
Batch 47/64 loss: -0.4214468002319336
Batch 48/64 loss: -0.6452341079711914
Batch 49/64 loss: -0.5751075744628906
Batch 50/64 loss: -0.579226016998291
Batch 51/64 loss: -0.6589374542236328
Batch 52/64 loss: 0.7919039726257324
Batch 53/64 loss: -0.38982057571411133
Batch 54/64 loss: 0.3361477851867676
Batch 55/64 loss: -0.6856021881103516
Batch 56/64 loss: -0.628718376159668
Batch 57/64 loss: -0.6262650489807129
Batch 58/64 loss: -0.5078730583190918
Batch 59/64 loss: -0.47586488723754883
Batch 60/64 loss: -0.4653773307800293
Batch 61/64 loss: -0.6528921127319336
Batch 62/64 loss: -0.5725498199462891
Batch 63/64 loss: -0.22218799591064453
Batch 64/64 loss: -4.2529754638671875
Epoch 283  Train loss: -0.5261753231871362  Val loss: -0.9025585528501531
Epoch 284
-------------------------------
Batch 1/64 loss: -0.6366276741027832
Batch 2/64 loss: -0.7178363800048828
Batch 3/64 loss: -0.5639700889587402
Batch 4/64 loss: -0.5599274635314941
Batch 5/64 loss: -0.7186450958251953
Batch 6/64 loss: -0.6815834045410156
Batch 7/64 loss: -0.3137531280517578
Batch 8/64 loss: -0.4739189147949219
Batch 9/64 loss: -0.5375819206237793
Batch 10/64 loss: -0.31789445877075195
Batch 11/64 loss: -0.7002243995666504
Batch 12/64 loss: -0.5686917304992676
Batch 13/64 loss: -0.3728780746459961
Batch 14/64 loss: -0.5641269683837891
Batch 15/64 loss: 0.5164647102355957
Batch 16/64 loss: -0.6842303276062012
Batch 17/64 loss: -0.7097878456115723
Batch 18/64 loss: 0.9517583847045898
Batch 19/64 loss: 0.3576498031616211
Batch 20/64 loss: -0.719001293182373
Batch 21/64 loss: -0.7347707748413086
Batch 22/64 loss: -0.5650062561035156
Batch 23/64 loss: -0.574063777923584
Batch 24/64 loss: -0.716712474822998
Batch 25/64 loss: -0.6315693855285645
Batch 26/64 loss: -0.5681114196777344
Batch 27/64 loss: -0.30125999450683594
Batch 28/64 loss: -0.5627384185791016
Batch 29/64 loss: -0.7730178833007812
Batch 30/64 loss: -0.6984100341796875
Batch 31/64 loss: -0.6739354133605957
Batch 32/64 loss: -0.08275222778320312
Batch 33/64 loss: -0.4259786605834961
Batch 34/64 loss: -0.6294760704040527
Batch 35/64 loss: -0.7255630493164062
Batch 36/64 loss: -0.7611536979675293
Batch 37/64 loss: -0.7595558166503906
Batch 38/64 loss: -0.7704010009765625
Batch 39/64 loss: -0.59149169921875
Batch 40/64 loss: -0.7393808364868164
Batch 41/64 loss: -0.44198131561279297
Batch 42/64 loss: -0.6116795539855957
Batch 43/64 loss: -0.421450138092041
Batch 44/64 loss: -0.4679555892944336
Batch 45/64 loss: -0.7022209167480469
Batch 46/64 loss: -0.5844573974609375
Batch 47/64 loss: -0.5848231315612793
Batch 48/64 loss: -0.470090389251709
Batch 49/64 loss: -0.513646125793457
Batch 50/64 loss: -0.6138930320739746
Batch 51/64 loss: 0.31627845764160156
Batch 52/64 loss: -0.5884895324707031
Batch 53/64 loss: -0.4318089485168457
Batch 54/64 loss: -0.6109347343444824
Batch 55/64 loss: -0.5889153480529785
Batch 56/64 loss: -0.6474895477294922
Batch 57/64 loss: -0.5756926536560059
Batch 58/64 loss: -0.6451826095581055
Batch 59/64 loss: -0.5672340393066406
Batch 60/64 loss: -0.5773415565490723
Batch 61/64 loss: -0.6309142112731934
Batch 62/64 loss: -0.035822391510009766
Batch 63/64 loss: -0.6156406402587891
Batch 64/64 loss: -4.180711269378662
Epoch 284  Train loss: -0.5497580229067335  Val loss: -0.9701269221879363
Saving best model, epoch: 284
Epoch 285
-------------------------------
Batch 1/64 loss: -0.7393512725830078
Batch 2/64 loss: -0.21991777420043945
Batch 3/64 loss: -0.42552709579467773
Batch 4/64 loss: -0.769040584564209
Batch 5/64 loss: -0.4994211196899414
Batch 6/64 loss: -0.5789356231689453
Batch 7/64 loss: -0.6764450073242188
Batch 8/64 loss: -0.6605854034423828
Batch 9/64 loss: 1.0473084449768066
Batch 10/64 loss: -0.5945487022399902
Batch 11/64 loss: -0.6234307289123535
Batch 12/64 loss: -0.5137467384338379
Batch 13/64 loss: -0.6435384750366211
Batch 14/64 loss: -0.5239601135253906
Batch 15/64 loss: -0.18292665481567383
Batch 16/64 loss: -0.8243722915649414
Batch 17/64 loss: -0.6805810928344727
Batch 18/64 loss: -0.46140480041503906
Batch 19/64 loss: -0.768125057220459
Batch 20/64 loss: -0.722649097442627
Batch 21/64 loss: -0.7615776062011719
Batch 22/64 loss: -0.6840124130249023
Batch 23/64 loss: -0.5322871208190918
Batch 24/64 loss: -0.507875919342041
Batch 25/64 loss: -0.6597661972045898
Batch 26/64 loss: -0.06533956527709961
Batch 27/64 loss: -0.5651984214782715
Batch 28/64 loss: -0.5552377700805664
Batch 29/64 loss: 0.5250349044799805
Batch 30/64 loss: -0.7076830863952637
Batch 31/64 loss: -0.6274466514587402
Batch 32/64 loss: -0.5827817916870117
Batch 33/64 loss: -0.6391234397888184
Batch 34/64 loss: -0.6345424652099609
Batch 35/64 loss: -0.6606121063232422
Batch 36/64 loss: -0.8268656730651855
Batch 37/64 loss: 0.3124732971191406
Batch 38/64 loss: -0.6894974708557129
Batch 39/64 loss: -0.641148567199707
Batch 40/64 loss: -0.6005182266235352
Batch 41/64 loss: -0.6658744812011719
Batch 42/64 loss: -0.5799059867858887
Batch 43/64 loss: -0.7159609794616699
Batch 44/64 loss: -0.6449565887451172
Batch 45/64 loss: -0.7404646873474121
Batch 46/64 loss: -0.7390408515930176
Batch 47/64 loss: -0.5504293441772461
Batch 48/64 loss: -0.571169376373291
Batch 49/64 loss: -0.6585483551025391
Batch 50/64 loss: -3.337860107421875e-06
Batch 51/64 loss: -0.7095894813537598
Batch 52/64 loss: -0.5801353454589844
Batch 53/64 loss: -0.5739707946777344
Batch 54/64 loss: -0.6398138999938965
Batch 55/64 loss: -0.6722350120544434
Batch 56/64 loss: -0.23980998992919922
Batch 57/64 loss: -0.5538067817687988
Batch 58/64 loss: -0.5733428001403809
Batch 59/64 loss: -0.729586124420166
Batch 60/64 loss: -0.6683444976806641
Batch 61/64 loss: -0.6512966156005859
Batch 62/64 loss: -0.5707211494445801
Batch 63/64 loss: -0.6687831878662109
Batch 64/64 loss: -4.245892524719238
Epoch 285  Train loss: -0.5811359143724628  Val loss: -0.9080151036842582
Epoch 286
-------------------------------
Batch 1/64 loss: -0.527951717376709
Batch 2/64 loss: -0.6887650489807129
Batch 3/64 loss: -0.5900936126708984
Batch 4/64 loss: -0.7286281585693359
Batch 5/64 loss: -0.6369071006774902
Batch 6/64 loss: -0.49639320373535156
Batch 7/64 loss: -0.7028403282165527
Batch 8/64 loss: -0.5916547775268555
Batch 9/64 loss: -0.4826970100402832
Batch 10/64 loss: -0.5631170272827148
Batch 11/64 loss: -0.7107148170471191
Batch 12/64 loss: -0.6248140335083008
Batch 13/64 loss: -0.5472927093505859
Batch 14/64 loss: -0.6414222717285156
Batch 15/64 loss: -0.5119333267211914
Batch 16/64 loss: -0.008574485778808594
Batch 17/64 loss: -0.4969496726989746
Batch 18/64 loss: -0.6395831108093262
Batch 19/64 loss: -0.46210670471191406
Batch 20/64 loss: -0.6743369102478027
Batch 21/64 loss: -0.7266254425048828
Batch 22/64 loss: -0.45537519454956055
Batch 23/64 loss: -0.7497639656066895
Batch 24/64 loss: -0.6803336143493652
Batch 25/64 loss: -0.7598047256469727
Batch 26/64 loss: -0.833033561706543
Batch 27/64 loss: -0.4350614547729492
Batch 28/64 loss: 0.7222614288330078
Batch 29/64 loss: -0.7797737121582031
Batch 30/64 loss: -0.7853412628173828
Batch 31/64 loss: -0.5783858299255371
Batch 32/64 loss: -0.5079531669616699
Batch 33/64 loss: -0.47838449478149414
Batch 34/64 loss: -0.4772663116455078
Batch 35/64 loss: -0.549929141998291
Batch 36/64 loss: -0.6619358062744141
Batch 37/64 loss: -0.6683182716369629
Batch 38/64 loss: -0.18256187438964844
Batch 39/64 loss: -0.3645496368408203
Batch 40/64 loss: -0.4670271873474121
Batch 41/64 loss: -0.6846261024475098
Batch 42/64 loss: 0.8688979148864746
Batch 43/64 loss: -0.7144813537597656
Batch 44/64 loss: -0.6715607643127441
Batch 45/64 loss: -0.6649680137634277
Batch 46/64 loss: -0.6586980819702148
Batch 47/64 loss: -0.6775631904602051
Batch 48/64 loss: -0.46493053436279297
Batch 49/64 loss: 0.44788122177124023
Batch 50/64 loss: -0.4398679733276367
Batch 51/64 loss: -0.6214923858642578
Batch 52/64 loss: -0.2984776496887207
Batch 53/64 loss: -0.7021641731262207
Batch 54/64 loss: -0.6515688896179199
Batch 55/64 loss: -0.5631380081176758
Batch 56/64 loss: -0.7284364700317383
Batch 57/64 loss: -0.7691082954406738
Batch 58/64 loss: -0.6116394996643066
Batch 59/64 loss: -0.5582542419433594
Batch 60/64 loss: -0.6088829040527344
Batch 61/64 loss: -0.5881199836730957
Batch 62/64 loss: -0.657252311706543
Batch 63/64 loss: -0.11763191223144531
Batch 64/64 loss: -4.263859272003174
Epoch 286  Train loss: -0.5659594872418572  Val loss: -0.8321508689434668
Epoch 287
-------------------------------
Batch 1/64 loss: -0.596555233001709
Batch 2/64 loss: -0.5688624382019043
Batch 3/64 loss: -0.6122751235961914
Batch 4/64 loss: -0.6865196228027344
Batch 5/64 loss: -0.5542149543762207
Batch 6/64 loss: -0.6123533248901367
Batch 7/64 loss: 0.7870650291442871
Batch 8/64 loss: -0.34606122970581055
Batch 9/64 loss: -0.4041290283203125
Batch 10/64 loss: -0.0713810920715332
Batch 11/64 loss: -0.4547853469848633
Batch 12/64 loss: -0.4244990348815918
Batch 13/64 loss: -0.5611376762390137
Batch 14/64 loss: -0.23499679565429688
Batch 15/64 loss: -0.7509336471557617
Batch 16/64 loss: -0.542910099029541
Batch 17/64 loss: -0.641049861907959
Batch 18/64 loss: -0.6765871047973633
Batch 19/64 loss: -0.7040085792541504
Batch 20/64 loss: -0.3774099349975586
Batch 21/64 loss: -0.6098480224609375
Batch 22/64 loss: 0.03078603744506836
Batch 23/64 loss: -0.6955413818359375
Batch 24/64 loss: -0.6058611869812012
Batch 25/64 loss: -0.5079894065856934
Batch 26/64 loss: -0.6572837829589844
Batch 27/64 loss: -0.5932893753051758
Batch 28/64 loss: -0.30142831802368164
Batch 29/64 loss: -0.7537527084350586
Batch 30/64 loss: -0.6435880661010742
Batch 31/64 loss: -0.6047401428222656
Batch 32/64 loss: -0.6163291931152344
Batch 33/64 loss: 0.8790287971496582
Batch 34/64 loss: -0.5643582344055176
Batch 35/64 loss: -0.6730175018310547
Batch 36/64 loss: -0.7256712913513184
Batch 37/64 loss: -0.7118778228759766
Batch 38/64 loss: -0.5746850967407227
Batch 39/64 loss: -0.6524739265441895
Batch 40/64 loss: -0.7431936264038086
Batch 41/64 loss: -0.6775698661804199
Batch 42/64 loss: -0.6582789421081543
Batch 43/64 loss: -0.6932120323181152
Batch 44/64 loss: -0.44308042526245117
Batch 45/64 loss: -0.7603812217712402
Batch 46/64 loss: 0.6550784111022949
Batch 47/64 loss: -0.4600658416748047
Batch 48/64 loss: -0.45079755783081055
Batch 49/64 loss: -0.5611095428466797
Batch 50/64 loss: -0.5800352096557617
Batch 51/64 loss: -0.7170829772949219
Batch 52/64 loss: -0.5120010375976562
Batch 53/64 loss: -0.6124682426452637
Batch 54/64 loss: -0.538628101348877
Batch 55/64 loss: -0.5688133239746094
Batch 56/64 loss: -0.5007166862487793
Batch 57/64 loss: -0.5023155212402344
Batch 58/64 loss: -0.21145009994506836
Batch 59/64 loss: -0.5167288780212402
Batch 60/64 loss: -0.7122564315795898
Batch 61/64 loss: -0.34622669219970703
Batch 62/64 loss: -0.6569232940673828
Batch 63/64 loss: -0.5994048118591309
Batch 64/64 loss: -4.287334442138672
Epoch 287  Train loss: -0.5364500157973345  Val loss: -0.8719544033823964
Epoch 288
-------------------------------
Batch 1/64 loss: -0.40935754776000977
Batch 2/64 loss: -0.5672993659973145
Batch 3/64 loss: -0.5444879531860352
Batch 4/64 loss: -0.6669650077819824
Batch 5/64 loss: -0.48407697677612305
Batch 6/64 loss: -0.591850757598877
Batch 7/64 loss: -0.5465316772460938
Batch 8/64 loss: -0.7724361419677734
Batch 9/64 loss: -0.7281622886657715
Batch 10/64 loss: -0.6242985725402832
Batch 11/64 loss: -0.4216756820678711
Batch 12/64 loss: -0.7101364135742188
Batch 13/64 loss: -0.6186580657958984
Batch 14/64 loss: -0.6992893218994141
Batch 15/64 loss: 0.37900209426879883
Batch 16/64 loss: -0.6163167953491211
Batch 17/64 loss: -0.46101999282836914
Batch 18/64 loss: -0.5809769630432129
Batch 19/64 loss: -0.24577045440673828
Batch 20/64 loss: -0.46966028213500977
Batch 21/64 loss: -0.5574817657470703
Batch 22/64 loss: -0.38555002212524414
Batch 23/64 loss: -0.566340446472168
Batch 24/64 loss: -0.5888261795043945
Batch 25/64 loss: -0.648521900177002
Batch 26/64 loss: -0.6726112365722656
Batch 27/64 loss: -0.28556299209594727
Batch 28/64 loss: -0.7350764274597168
Batch 29/64 loss: -0.7400064468383789
Batch 30/64 loss: -0.7265501022338867
Batch 31/64 loss: -0.6016025543212891
Batch 32/64 loss: -0.6526856422424316
Batch 33/64 loss: -0.3891787528991699
Batch 34/64 loss: -0.5711302757263184
Batch 35/64 loss: -0.711552619934082
Batch 36/64 loss: -0.6639313697814941
Batch 37/64 loss: -0.2519187927246094
Batch 38/64 loss: -0.4217848777770996
Batch 39/64 loss: 0.7512779235839844
Batch 40/64 loss: -0.6621150970458984
Batch 41/64 loss: -0.39169740676879883
Batch 42/64 loss: -0.5612368583679199
Batch 43/64 loss: -0.0984954833984375
Batch 44/64 loss: -0.541196346282959
Batch 45/64 loss: -0.6937527656555176
Batch 46/64 loss: -0.44914674758911133
Batch 47/64 loss: -0.6054644584655762
Batch 48/64 loss: -0.574974536895752
Batch 49/64 loss: -0.6904850006103516
Batch 50/64 loss: -0.6383161544799805
Batch 51/64 loss: -0.705535888671875
Batch 52/64 loss: -0.8394732475280762
Batch 53/64 loss: -0.6919703483581543
Batch 54/64 loss: -0.6333599090576172
Batch 55/64 loss: 0.4918966293334961
Batch 56/64 loss: -0.6968836784362793
Batch 57/64 loss: -0.6365489959716797
Batch 58/64 loss: -0.5072412490844727
Batch 59/64 loss: 0.12361812591552734
Batch 60/64 loss: -0.7638564109802246
Batch 61/64 loss: -0.6731419563293457
Batch 62/64 loss: -0.6433196067810059
Batch 63/64 loss: -0.27767419815063477
Batch 64/64 loss: -4.425703525543213
Epoch 288  Train loss: -0.5565278726465562  Val loss: -0.8698313342746591
Epoch 289
-------------------------------
Batch 1/64 loss: -0.7134699821472168
Batch 2/64 loss: -0.7211804389953613
Batch 3/64 loss: -0.28537940979003906
Batch 4/64 loss: -0.6910715103149414
Batch 5/64 loss: -0.7019200325012207
Batch 6/64 loss: -0.6929669380187988
Batch 7/64 loss: 0.7243046760559082
Batch 8/64 loss: -0.5490927696228027
Batch 9/64 loss: -0.6925020217895508
Batch 10/64 loss: -0.4702916145324707
Batch 11/64 loss: -0.7554545402526855
Batch 12/64 loss: -0.6508994102478027
Batch 13/64 loss: -0.6091732978820801
Batch 14/64 loss: -0.7132391929626465
Batch 15/64 loss: -0.675018310546875
Batch 16/64 loss: -0.6361474990844727
Batch 17/64 loss: -0.6255383491516113
Batch 18/64 loss: -0.7111911773681641
Batch 19/64 loss: -0.6156792640686035
Batch 20/64 loss: -0.6879544258117676
Batch 21/64 loss: -0.5410690307617188
Batch 22/64 loss: -0.49919748306274414
Batch 23/64 loss: -0.6138277053833008
Batch 24/64 loss: -0.44784116744995117
Batch 25/64 loss: -0.639157772064209
Batch 26/64 loss: -0.6286392211914062
Batch 27/64 loss: -0.515195369720459
Batch 28/64 loss: -0.658907413482666
Batch 29/64 loss: -0.2460179328918457
Batch 30/64 loss: -0.5591740608215332
Batch 31/64 loss: -0.7415676116943359
Batch 32/64 loss: -0.5361318588256836
Batch 33/64 loss: -0.5834593772888184
Batch 34/64 loss: -0.7086362838745117
Batch 35/64 loss: -0.6328654289245605
Batch 36/64 loss: -0.5995807647705078
Batch 37/64 loss: -0.8361530303955078
Batch 38/64 loss: -0.6892666816711426
Batch 39/64 loss: -0.6209006309509277
Batch 40/64 loss: -0.7579360008239746
Batch 41/64 loss: -0.7666926383972168
Batch 42/64 loss: 0.37236928939819336
Batch 43/64 loss: -0.6337161064147949
Batch 44/64 loss: -0.6654014587402344
Batch 45/64 loss: -0.7324056625366211
Batch 46/64 loss: -0.45099925994873047
Batch 47/64 loss: 0.6276745796203613
Batch 48/64 loss: -0.680081844329834
Batch 49/64 loss: -0.6036844253540039
Batch 50/64 loss: -0.643394947052002
Batch 51/64 loss: -0.013583660125732422
Batch 52/64 loss: -0.19742155075073242
Batch 53/64 loss: -0.6011719703674316
Batch 54/64 loss: -0.6266674995422363
Batch 55/64 loss: -0.6986947059631348
Batch 56/64 loss: -0.6070003509521484
Batch 57/64 loss: -0.03238821029663086
Batch 58/64 loss: -0.5955524444580078
Batch 59/64 loss: -0.5343775749206543
Batch 60/64 loss: -0.69403076171875
Batch 61/64 loss: -0.5290617942810059
Batch 62/64 loss: -0.3864250183105469
Batch 63/64 loss: -0.4725985527038574
Batch 64/64 loss: -4.104501247406006
Epoch 289  Train loss: -0.5768325151181688  Val loss: -0.8749054453217286
Epoch 290
-------------------------------
Batch 1/64 loss: -0.7834477424621582
Batch 2/64 loss: -0.6975789070129395
Batch 3/64 loss: -0.551579475402832
Batch 4/64 loss: -0.4674196243286133
Batch 5/64 loss: -0.5960474014282227
Batch 6/64 loss: -0.70343017578125
Batch 7/64 loss: -0.677487850189209
Batch 8/64 loss: -0.4962143898010254
Batch 9/64 loss: -0.6310615539550781
Batch 10/64 loss: 1.0405850410461426
Batch 11/64 loss: -0.7633271217346191
Batch 12/64 loss: -0.5544309616088867
Batch 13/64 loss: -0.6893553733825684
Batch 14/64 loss: -0.5591659545898438
Batch 15/64 loss: -0.629241943359375
Batch 16/64 loss: -0.5113158226013184
Batch 17/64 loss: -0.7253561019897461
Batch 18/64 loss: -0.6812863349914551
Batch 19/64 loss: -0.44170618057250977
Batch 20/64 loss: -0.5598201751708984
Batch 21/64 loss: -0.5811104774475098
Batch 22/64 loss: -0.6677756309509277
Batch 23/64 loss: -0.7347502708435059
Batch 24/64 loss: -0.6417727470397949
Batch 25/64 loss: -0.6729912757873535
Batch 26/64 loss: -0.5755190849304199
Batch 27/64 loss: -0.6112833023071289
Batch 28/64 loss: -0.5638928413391113
Batch 29/64 loss: -0.612389087677002
Batch 30/64 loss: -0.7155294418334961
Batch 31/64 loss: -0.5732831954956055
Batch 32/64 loss: -0.6158266067504883
Batch 33/64 loss: -0.6042351722717285
Batch 34/64 loss: -0.6213221549987793
Batch 35/64 loss: -0.7330141067504883
Batch 36/64 loss: -0.7407960891723633
Batch 37/64 loss: -0.6165981292724609
Batch 38/64 loss: -0.6949372291564941
Batch 39/64 loss: -0.6113848686218262
Batch 40/64 loss: -0.5721840858459473
Batch 41/64 loss: -0.1945343017578125
Batch 42/64 loss: -0.1428966522216797
Batch 43/64 loss: -0.7626705169677734
Batch 44/64 loss: -0.5327048301696777
Batch 45/64 loss: -0.6323661804199219
Batch 46/64 loss: -0.6085691452026367
Batch 47/64 loss: -0.6211276054382324
Batch 48/64 loss: -0.6452059745788574
Batch 49/64 loss: 0.051201820373535156
Batch 50/64 loss: -0.5945029258728027
Batch 51/64 loss: -0.7570056915283203
Batch 52/64 loss: -0.6665463447570801
Batch 53/64 loss: -0.6152844429016113
Batch 54/64 loss: -0.48206138610839844
Batch 55/64 loss: -0.6471338272094727
Batch 56/64 loss: -0.7386054992675781
Batch 57/64 loss: -0.1902155876159668
Batch 58/64 loss: -0.5426769256591797
Batch 59/64 loss: -0.783207893371582
Batch 60/64 loss: -0.58172607421875
Batch 61/64 loss: 0.8332152366638184
Batch 62/64 loss: -0.3498563766479492
Batch 63/64 loss: -0.5971941947937012
Batch 64/64 loss: -2.9904117584228516
Epoch 290  Train loss: -0.5722943997850605  Val loss: -0.8233076141462293
Epoch 291
-------------------------------
Batch 1/64 loss: -0.651494026184082
Batch 2/64 loss: -0.33724117279052734
Batch 3/64 loss: -0.65557861328125
Batch 4/64 loss: -0.6141300201416016
Batch 5/64 loss: -0.14490079879760742
Batch 6/64 loss: -0.636599063873291
Batch 7/64 loss: -0.5376138687133789
Batch 8/64 loss: -0.5936985015869141
Batch 9/64 loss: -0.6429996490478516
Batch 10/64 loss: -0.5772438049316406
Batch 11/64 loss: -0.3809514045715332
Batch 12/64 loss: -0.6135516166687012
Batch 13/64 loss: -0.652763843536377
Batch 14/64 loss: -0.7311697006225586
Batch 15/64 loss: -0.6539692878723145
Batch 16/64 loss: -0.6784539222717285
Batch 17/64 loss: -0.6718349456787109
Batch 18/64 loss: -0.5873751640319824
Batch 19/64 loss: -0.5717997550964355
Batch 20/64 loss: -0.38917016983032227
Batch 21/64 loss: -0.4702901840209961
Batch 22/64 loss: -0.610987663269043
Batch 23/64 loss: -0.648068904876709
Batch 24/64 loss: -0.6126666069030762
Batch 25/64 loss: -0.12494087219238281
Batch 26/64 loss: 0.44383716583251953
Batch 27/64 loss: 1.2562761306762695
Batch 28/64 loss: -0.4989762306213379
Batch 29/64 loss: -0.6746363639831543
Batch 30/64 loss: -0.6677069664001465
Batch 31/64 loss: -0.40005922317504883
Batch 32/64 loss: -0.645087718963623
Batch 33/64 loss: -0.5812935829162598
Batch 34/64 loss: -0.682673454284668
Batch 35/64 loss: -0.5065441131591797
Batch 36/64 loss: -0.5647397041320801
Batch 37/64 loss: -0.6721982955932617
Batch 38/64 loss: -0.7432537078857422
Batch 39/64 loss: -0.43553924560546875
Batch 40/64 loss: -0.7217936515808105
Batch 41/64 loss: -0.5712051391601562
Batch 42/64 loss: -0.30980587005615234
Batch 43/64 loss: -0.7371220588684082
Batch 44/64 loss: -0.5782074928283691
Batch 45/64 loss: -0.666532039642334
Batch 46/64 loss: -0.7003765106201172
Batch 47/64 loss: -0.677701473236084
Batch 48/64 loss: -0.7115039825439453
Batch 49/64 loss: -0.7430281639099121
Batch 50/64 loss: -0.6509203910827637
Batch 51/64 loss: -0.6161026954650879
Batch 52/64 loss: -0.7156982421875
Batch 53/64 loss: 0.338925838470459
Batch 54/64 loss: -0.7555971145629883
Batch 55/64 loss: -0.5767278671264648
Batch 56/64 loss: -0.3108067512512207
Batch 57/64 loss: -0.7158632278442383
Batch 58/64 loss: -0.7124156951904297
Batch 59/64 loss: -0.5691447257995605
Batch 60/64 loss: -0.4647650718688965
Batch 61/64 loss: 0.24623584747314453
Batch 62/64 loss: -0.6379704475402832
Batch 63/64 loss: -0.42162656784057617
Batch 64/64 loss: -4.246030330657959
Epoch 291  Train loss: -0.5541390606001312  Val loss: -0.7291234203220642
Epoch 292
-------------------------------
Batch 1/64 loss: -0.6488966941833496
Batch 2/64 loss: -0.5694723129272461
Batch 3/64 loss: -0.4364595413208008
Batch 4/64 loss: 0.09574174880981445
Batch 5/64 loss: -0.6609578132629395
Batch 6/64 loss: -0.5033798217773438
Batch 7/64 loss: 0.1144399642944336
Batch 8/64 loss: -0.608757495880127
Batch 9/64 loss: -0.5264363288879395
Batch 10/64 loss: -0.6981472969055176
Batch 11/64 loss: -0.6743917465209961
Batch 12/64 loss: -0.4379310607910156
Batch 13/64 loss: -0.41332387924194336
Batch 14/64 loss: -0.4879579544067383
Batch 15/64 loss: -0.7203502655029297
Batch 16/64 loss: -0.6253666877746582
Batch 17/64 loss: -0.6502256393432617
Batch 18/64 loss: -0.5097360610961914
Batch 19/64 loss: -0.6617369651794434
Batch 20/64 loss: -0.634037971496582
Batch 21/64 loss: -0.7118363380432129
Batch 22/64 loss: -0.7467732429504395
Batch 23/64 loss: -0.6231827735900879
Batch 24/64 loss: -0.5386209487915039
Batch 25/64 loss: -0.5143499374389648
Batch 26/64 loss: 0.5743694305419922
Batch 27/64 loss: -0.6177458763122559
Batch 28/64 loss: -0.5874209403991699
Batch 29/64 loss: 0.6844315528869629
Batch 30/64 loss: -0.6881070137023926
Batch 31/64 loss: -0.054940223693847656
Batch 32/64 loss: -0.4730567932128906
Batch 33/64 loss: -0.6008381843566895
Batch 34/64 loss: -0.720984935760498
Batch 35/64 loss: -0.2403883934020996
Batch 36/64 loss: -0.5545711517333984
Batch 37/64 loss: -0.5630273818969727
Batch 38/64 loss: -0.6650552749633789
Batch 39/64 loss: -0.5826139450073242
Batch 40/64 loss: 0.8733291625976562
Batch 41/64 loss: -0.460416316986084
Batch 42/64 loss: -0.6376128196716309
Batch 43/64 loss: -0.5549769401550293
Batch 44/64 loss: -0.5462870597839355
Batch 45/64 loss: -0.5008115768432617
Batch 46/64 loss: -0.5983166694641113
Batch 47/64 loss: -0.535059928894043
Batch 48/64 loss: -0.45387840270996094
Batch 49/64 loss: -0.16134071350097656
Batch 50/64 loss: -0.722343921661377
Batch 51/64 loss: -0.5907001495361328
Batch 52/64 loss: -0.5384039878845215
Batch 53/64 loss: -0.6697635650634766
Batch 54/64 loss: -0.6901431083679199
Batch 55/64 loss: -0.43973875045776367
Batch 56/64 loss: -0.6289362907409668
Batch 57/64 loss: -0.5900964736938477
Batch 58/64 loss: -0.6413731575012207
Batch 59/64 loss: -0.2712078094482422
Batch 60/64 loss: -0.5769972801208496
Batch 61/64 loss: -0.6111874580383301
Batch 62/64 loss: -0.1935415267944336
Batch 63/64 loss: -0.43343114852905273
Batch 64/64 loss: -4.080533027648926
Epoch 292  Train loss: -0.5131879507326612  Val loss: -0.9135343807259786
Epoch 293
-------------------------------
Batch 1/64 loss: -0.6451849937438965
Batch 2/64 loss: -0.6200413703918457
Batch 3/64 loss: -0.4521193504333496
Batch 4/64 loss: -0.6551041603088379
Batch 5/64 loss: -0.22048425674438477
Batch 6/64 loss: -0.7523899078369141
Batch 7/64 loss: -0.36502742767333984
Batch 8/64 loss: 0.34197044372558594
Batch 9/64 loss: -0.37151384353637695
Batch 10/64 loss: -0.559295654296875
Batch 11/64 loss: -0.6200437545776367
Batch 12/64 loss: -0.5028595924377441
Batch 13/64 loss: -0.30704545974731445
Batch 14/64 loss: -0.37967729568481445
Batch 15/64 loss: 1.261472225189209
Batch 16/64 loss: -0.5151300430297852
Batch 17/64 loss: -0.6830410957336426
Batch 18/64 loss: -0.4962754249572754
Batch 19/64 loss: -0.5140848159790039
Batch 20/64 loss: -0.6059503555297852
Batch 21/64 loss: -0.5268616676330566
Batch 22/64 loss: -0.6842427253723145
Batch 23/64 loss: -0.552058219909668
Batch 24/64 loss: -0.6300797462463379
Batch 25/64 loss: -0.4069647789001465
Batch 26/64 loss: -0.6277117729187012
Batch 27/64 loss: 0.46056652069091797
Batch 28/64 loss: -0.35606908798217773
Batch 29/64 loss: -0.6979613304138184
Batch 30/64 loss: -0.1585984230041504
Batch 31/64 loss: -0.4905843734741211
Batch 32/64 loss: -0.4857354164123535
Batch 33/64 loss: -0.6266570091247559
Batch 34/64 loss: -0.4067215919494629
Batch 35/64 loss: -0.6842222213745117
Batch 36/64 loss: -0.32416534423828125
Batch 37/64 loss: -0.5245447158813477
Batch 38/64 loss: -0.4311814308166504
Batch 39/64 loss: -0.5307407379150391
Batch 40/64 loss: -0.19437122344970703
Batch 41/64 loss: -0.6501402854919434
Batch 42/64 loss: -0.6550979614257812
Batch 43/64 loss: -0.6220016479492188
Batch 44/64 loss: -0.44439697265625
Batch 45/64 loss: -0.6978349685668945
Batch 46/64 loss: -0.07807350158691406
Batch 47/64 loss: -0.1744394302368164
Batch 48/64 loss: -0.6202535629272461
Batch 49/64 loss: -0.23373889923095703
Batch 50/64 loss: -0.6927409172058105
Batch 51/64 loss: -0.5396122932434082
Batch 52/64 loss: -0.5383706092834473
Batch 53/64 loss: -0.553551197052002
Batch 54/64 loss: -0.38358449935913086
Batch 55/64 loss: -0.6372580528259277
Batch 56/64 loss: -0.7582721710205078
Batch 57/64 loss: -0.6542234420776367
Batch 58/64 loss: -0.5852632522583008
Batch 59/64 loss: -0.7984962463378906
Batch 60/64 loss: -0.6544232368469238
Batch 61/64 loss: -0.5696425437927246
Batch 62/64 loss: -0.5599746704101562
Batch 63/64 loss: 0.014081001281738281
Batch 64/64 loss: -4.15793514251709
Epoch 293  Train loss: -0.4979841905481675  Val loss: -0.8652353188426224
Epoch 294
-------------------------------
Batch 1/64 loss: -0.7667269706726074
Batch 2/64 loss: -0.765784740447998
Batch 3/64 loss: -0.45676279067993164
Batch 4/64 loss: -0.6023392677307129
Batch 5/64 loss: -0.5584430694580078
Batch 6/64 loss: -0.42603063583374023
Batch 7/64 loss: -0.6464433670043945
Batch 8/64 loss: -0.627255916595459
Batch 9/64 loss: -0.6424088478088379
Batch 10/64 loss: 0.8174433708190918
Batch 11/64 loss: -0.47536563873291016
Batch 12/64 loss: -0.45307397842407227
Batch 13/64 loss: -0.6515345573425293
Batch 14/64 loss: -0.2863750457763672
Batch 15/64 loss: -0.6952447891235352
Batch 16/64 loss: -0.47006797790527344
Batch 17/64 loss: -0.6287651062011719
Batch 18/64 loss: -0.635047435760498
Batch 19/64 loss: -0.7340641021728516
Batch 20/64 loss: -0.4467806816101074
Batch 21/64 loss: -0.7080001831054688
Batch 22/64 loss: -0.37291479110717773
Batch 23/64 loss: -0.7241559028625488
Batch 24/64 loss: -0.526756763458252
Batch 25/64 loss: -0.7398595809936523
Batch 26/64 loss: -0.5785121917724609
Batch 27/64 loss: -0.42989587783813477
Batch 28/64 loss: 0.48275279998779297
Batch 29/64 loss: -0.6806831359863281
Batch 30/64 loss: -0.4087996482849121
Batch 31/64 loss: -0.6802425384521484
Batch 32/64 loss: -0.5687804222106934
Batch 33/64 loss: -0.7658753395080566
Batch 34/64 loss: -0.6816048622131348
Batch 35/64 loss: -0.6065287590026855
Batch 36/64 loss: -0.6824445724487305
Batch 37/64 loss: 0.4245491027832031
Batch 38/64 loss: -0.2770571708679199
Batch 39/64 loss: -0.6638307571411133
Batch 40/64 loss: 0.20250892639160156
Batch 41/64 loss: -0.7942428588867188
Batch 42/64 loss: -0.6220474243164062
Batch 43/64 loss: -0.30039072036743164
Batch 44/64 loss: -0.5538649559020996
Batch 45/64 loss: -0.16705656051635742
Batch 46/64 loss: -0.5756111145019531
Batch 47/64 loss: -0.2487320899963379
Batch 48/64 loss: -0.6684246063232422
Batch 49/64 loss: -0.7139286994934082
Batch 50/64 loss: -0.5417332649230957
Batch 51/64 loss: -0.08338356018066406
Batch 52/64 loss: -0.7715649604797363
Batch 53/64 loss: -0.4192695617675781
Batch 54/64 loss: -0.4015979766845703
Batch 55/64 loss: -0.6612944602966309
Batch 56/64 loss: -0.7494239807128906
Batch 57/64 loss: -0.49970149993896484
Batch 58/64 loss: -0.5827312469482422
Batch 59/64 loss: -0.6330509185791016
Batch 60/64 loss: -0.6891603469848633
Batch 61/64 loss: -0.7210240364074707
Batch 62/64 loss: -0.6267142295837402
Batch 63/64 loss: -0.6854138374328613
Batch 64/64 loss: -4.136842250823975
Epoch 294  Train loss: -0.5482383971120797  Val loss: -0.9153495342870758
Epoch 295
-------------------------------
Batch 1/64 loss: -0.6935939788818359
Batch 2/64 loss: -0.20433664321899414
Batch 3/64 loss: -0.4522590637207031
Batch 4/64 loss: -0.6945466995239258
Batch 5/64 loss: -0.6390619277954102
Batch 6/64 loss: -0.5413608551025391
Batch 7/64 loss: -0.6805310249328613
Batch 8/64 loss: -0.7469677925109863
Batch 9/64 loss: -0.5050487518310547
Batch 10/64 loss: -0.28791189193725586
Batch 11/64 loss: -0.5286779403686523
Batch 12/64 loss: -0.5968599319458008
Batch 13/64 loss: -0.740570068359375
Batch 14/64 loss: -0.7580561637878418
Batch 15/64 loss: -0.6513819694519043
Batch 16/64 loss: -0.6399326324462891
Batch 17/64 loss: -0.5792460441589355
Batch 18/64 loss: -0.652534008026123
Batch 19/64 loss: -0.5130939483642578
Batch 20/64 loss: -0.554196834564209
Batch 21/64 loss: -0.6686663627624512
Batch 22/64 loss: -0.7065696716308594
Batch 23/64 loss: -0.7806921005249023
Batch 24/64 loss: -0.15955305099487305
Batch 25/64 loss: -0.6200113296508789
Batch 26/64 loss: -0.447812557220459
Batch 27/64 loss: -0.17168092727661133
Batch 28/64 loss: -0.6456108093261719
Batch 29/64 loss: -0.5237116813659668
Batch 30/64 loss: -0.35962533950805664
Batch 31/64 loss: -0.5401520729064941
Batch 32/64 loss: -0.5245161056518555
Batch 33/64 loss: -0.482607364654541
Batch 34/64 loss: -0.7107758522033691
Batch 35/64 loss: -0.7145466804504395
Batch 36/64 loss: -0.45368003845214844
Batch 37/64 loss: -0.7211174964904785
Batch 38/64 loss: -0.5821108818054199
Batch 39/64 loss: 0.3260655403137207
Batch 40/64 loss: -0.5412864685058594
Batch 41/64 loss: -0.6389713287353516
Batch 42/64 loss: 0.8759164810180664
Batch 43/64 loss: -0.5003986358642578
Batch 44/64 loss: -0.5713076591491699
Batch 45/64 loss: -0.669410228729248
Batch 46/64 loss: -0.5865554809570312
Batch 47/64 loss: -0.6796145439147949
Batch 48/64 loss: -0.6940245628356934
Batch 49/64 loss: -0.10285091400146484
Batch 50/64 loss: -0.6306271553039551
Batch 51/64 loss: 0.8297977447509766
Batch 52/64 loss: -0.6934418678283691
Batch 53/64 loss: -0.6018481254577637
Batch 54/64 loss: -0.7747926712036133
Batch 55/64 loss: -0.5736298561096191
Batch 56/64 loss: -0.6889495849609375
Batch 57/64 loss: -0.5885481834411621
Batch 58/64 loss: -0.7543439865112305
Batch 59/64 loss: -0.6555204391479492
Batch 60/64 loss: -0.32128381729125977
Batch 61/64 loss: -0.7000260353088379
Batch 62/64 loss: -0.5055680274963379
Batch 63/64 loss: -0.6159172058105469
Batch 64/64 loss: -4.157102584838867
Epoch 295  Train loss: -0.5591932932535807  Val loss: -0.9928105147843508
Saving best model, epoch: 295
Epoch 296
-------------------------------
Batch 1/64 loss: -0.5975279808044434
Batch 2/64 loss: -0.7372894287109375
Batch 3/64 loss: -0.7089295387268066
Batch 4/64 loss: -0.7369651794433594
Batch 5/64 loss: -0.7657079696655273
Batch 6/64 loss: -0.6553974151611328
Batch 7/64 loss: -0.4803133010864258
Batch 8/64 loss: -0.5854573249816895
Batch 9/64 loss: -0.7557826042175293
Batch 10/64 loss: -0.725736141204834
Batch 11/64 loss: -0.6850666999816895
Batch 12/64 loss: -0.5966124534606934
Batch 13/64 loss: -0.5975337028503418
Batch 14/64 loss: -0.7786436080932617
Batch 15/64 loss: 0.07654619216918945
Batch 16/64 loss: -0.39440345764160156
Batch 17/64 loss: 0.760439395904541
Batch 18/64 loss: -0.43801021575927734
Batch 19/64 loss: -0.47783613204956055
Batch 20/64 loss: -0.48183107376098633
Batch 21/64 loss: -0.6111054420471191
Batch 22/64 loss: -0.5825214385986328
Batch 23/64 loss: -0.46330881118774414
Batch 24/64 loss: -0.5783276557922363
Batch 25/64 loss: -0.5808815956115723
Batch 26/64 loss: -0.5153594017028809
Batch 27/64 loss: -0.6604752540588379
Batch 28/64 loss: -0.6876673698425293
Batch 29/64 loss: 1.0025606155395508
Batch 30/64 loss: -0.5754313468933105
Batch 31/64 loss: -0.1321558952331543
Batch 32/64 loss: -0.48198413848876953
Batch 33/64 loss: -0.6505999565124512
Batch 34/64 loss: -0.3901543617248535
Batch 35/64 loss: 0.006037235260009766
Batch 36/64 loss: -0.5877342224121094
Batch 37/64 loss: -0.5313944816589355
Batch 38/64 loss: -0.6558494567871094
Batch 39/64 loss: -0.2902259826660156
Batch 40/64 loss: -0.20412874221801758
Batch 41/64 loss: -0.3339242935180664
Batch 42/64 loss: -0.5548844337463379
Batch 43/64 loss: -0.7052083015441895
Batch 44/64 loss: -0.08545351028442383
Batch 45/64 loss: -0.5098361968994141
Batch 46/64 loss: -0.6448583602905273
Batch 47/64 loss: -0.6750850677490234
Batch 48/64 loss: -0.6644630432128906
Batch 49/64 loss: -0.48453664779663086
Batch 50/64 loss: -0.4417123794555664
Batch 51/64 loss: -0.5408205986022949
Batch 52/64 loss: -0.5859060287475586
Batch 53/64 loss: -0.5535645484924316
Batch 54/64 loss: -0.4311556816101074
Batch 55/64 loss: -0.5434718132019043
Batch 56/64 loss: -0.6121001243591309
Batch 57/64 loss: -0.14265966415405273
Batch 58/64 loss: -0.447967529296875
Batch 59/64 loss: -0.3909029960632324
Batch 60/64 loss: -0.7004671096801758
Batch 61/64 loss: -0.30681324005126953
Batch 62/64 loss: -0.14605331420898438
Batch 63/64 loss: 0.37868309020996094
Batch 64/64 loss: -4.1819257736206055
Epoch 296  Train loss: -0.4987038818060183  Val loss: -0.76556712409475
Epoch 297
-------------------------------
Batch 1/64 loss: -0.5629773139953613
Batch 2/64 loss: -0.5083274841308594
Batch 3/64 loss: -0.5333743095397949
Batch 4/64 loss: -0.68701171875
Batch 5/64 loss: -0.36290931701660156
Batch 6/64 loss: -0.7090201377868652
Batch 7/64 loss: -0.3314704895019531
Batch 8/64 loss: -0.5195479393005371
Batch 9/64 loss: -0.5495462417602539
Batch 10/64 loss: -0.6159019470214844
Batch 11/64 loss: -0.5353188514709473
Batch 12/64 loss: -0.5653157234191895
Batch 13/64 loss: -0.4311709403991699
Batch 14/64 loss: -0.7098755836486816
Batch 15/64 loss: -0.5415072441101074
Batch 16/64 loss: -0.4995384216308594
Batch 17/64 loss: -0.6107568740844727
Batch 18/64 loss: -0.5479068756103516
Batch 19/64 loss: -0.23408126831054688
Batch 20/64 loss: -0.720832347869873
Batch 21/64 loss: 0.2631344795227051
Batch 22/64 loss: -0.5429806709289551
Batch 23/64 loss: -0.6454429626464844
Batch 24/64 loss: -0.6514358520507812
Batch 25/64 loss: 0.29833173751831055
Batch 26/64 loss: -0.12604475021362305
Batch 27/64 loss: -0.47136545181274414
Batch 28/64 loss: -0.45769166946411133
Batch 29/64 loss: -0.3478832244873047
Batch 30/64 loss: -0.5059013366699219
Batch 31/64 loss: -0.561500072479248
Batch 32/64 loss: -0.5893454551696777
Batch 33/64 loss: -0.47129106521606445
Batch 34/64 loss: -0.28049278259277344
Batch 35/64 loss: -0.6261582374572754
Batch 36/64 loss: 1.531944751739502
Batch 37/64 loss: -0.5485577583312988
Batch 38/64 loss: -0.3896450996398926
Batch 39/64 loss: -0.6018514633178711
Batch 40/64 loss: -0.23335838317871094
Batch 41/64 loss: -0.526033878326416
Batch 42/64 loss: -0.623784065246582
Batch 43/64 loss: -0.5833420753479004
Batch 44/64 loss: -0.3576946258544922
Batch 45/64 loss: -0.5602684020996094
Batch 46/64 loss: -0.480226993560791
Batch 47/64 loss: -0.5137176513671875
Batch 48/64 loss: -0.36322641372680664
Batch 49/64 loss: -0.606654167175293
Batch 50/64 loss: -0.6565799713134766
Batch 51/64 loss: -0.5216269493103027
Batch 52/64 loss: 0.08531999588012695
Batch 53/64 loss: -0.49643516540527344
Batch 54/64 loss: -0.7045273780822754
Batch 55/64 loss: -0.6487703323364258
Batch 56/64 loss: -0.578188419342041
Batch 57/64 loss: -0.5795817375183105
Batch 58/64 loss: -0.5777802467346191
Batch 59/64 loss: 0.61724853515625
Batch 60/64 loss: -0.594780445098877
Batch 61/64 loss: -0.3822016716003418
Batch 62/64 loss: -0.4365811347961426
Batch 63/64 loss: -0.4190812110900879
Batch 64/64 loss: -4.151560306549072
Epoch 297  Train loss: -0.47617428910498527  Val loss: -0.7871568686364033
Epoch 298
-------------------------------
Batch 1/64 loss: -0.417116641998291
Batch 2/64 loss: -0.572779655456543
Batch 3/64 loss: -0.3850421905517578
Batch 4/64 loss: -0.6128215789794922
Batch 5/64 loss: -0.4973320960998535
Batch 6/64 loss: -0.5379204750061035
Batch 7/64 loss: -0.6821308135986328
Batch 8/64 loss: -0.47364187240600586
Batch 9/64 loss: -0.5139546394348145
Batch 10/64 loss: -0.48505592346191406
Batch 11/64 loss: -0.5152368545532227
Batch 12/64 loss: -0.49849796295166016
Batch 13/64 loss: -0.5091719627380371
Batch 14/64 loss: -0.42716455459594727
Batch 15/64 loss: -0.6583113670349121
Batch 16/64 loss: -0.6659736633300781
Batch 17/64 loss: -0.44562530517578125
Batch 18/64 loss: -0.6614542007446289
Batch 19/64 loss: -0.6367201805114746
Batch 20/64 loss: -0.5578184127807617
Batch 21/64 loss: 0.5625977516174316
Batch 22/64 loss: -0.5455269813537598
Batch 23/64 loss: -0.6994333267211914
Batch 24/64 loss: -0.687690258026123
Batch 25/64 loss: -0.7297601699829102
Batch 26/64 loss: -0.6101675033569336
Batch 27/64 loss: -0.7478137016296387
Batch 28/64 loss: -0.6853270530700684
Batch 29/64 loss: -0.7221894264221191
Batch 30/64 loss: -0.5301632881164551
Batch 31/64 loss: -0.6502537727355957
Batch 32/64 loss: -0.30736351013183594
Batch 33/64 loss: -0.5552568435668945
Batch 34/64 loss: -0.6164116859436035
Batch 35/64 loss: -0.6170163154602051
Batch 36/64 loss: -0.6228079795837402
Batch 37/64 loss: -0.7474699020385742
Batch 38/64 loss: -0.646202564239502
Batch 39/64 loss: -0.6980066299438477
Batch 40/64 loss: -0.5818653106689453
Batch 41/64 loss: -0.7195844650268555
Batch 42/64 loss: -0.6074023246765137
Batch 43/64 loss: -0.600642204284668
Batch 44/64 loss: 0.5764126777648926
Batch 45/64 loss: -0.7218499183654785
Batch 46/64 loss: -0.7514872550964355
Batch 47/64 loss: -0.6223692893981934
Batch 48/64 loss: -0.5177841186523438
Batch 49/64 loss: -0.7320871353149414
Batch 50/64 loss: -0.43224668502807617
Batch 51/64 loss: -0.7283663749694824
Batch 52/64 loss: -0.2952556610107422
Batch 53/64 loss: -0.566584587097168
Batch 54/64 loss: -0.012705326080322266
Batch 55/64 loss: -0.7902464866638184
Batch 56/64 loss: 0.8258318901062012
Batch 57/64 loss: -0.5945949554443359
Batch 58/64 loss: -0.6587958335876465
Batch 59/64 loss: 0.1280994415283203
Batch 60/64 loss: -0.7794919013977051
Batch 61/64 loss: -0.5882811546325684
Batch 62/64 loss: -0.705653190612793
Batch 63/64 loss: -0.7634906768798828
Batch 64/64 loss: -4.05902624130249
Epoch 298  Train loss: -0.5630548084483428  Val loss: -0.9872776175692319
Epoch 299
-------------------------------
Batch 1/64 loss: -0.7224135398864746
Batch 2/64 loss: -0.6747589111328125
Batch 3/64 loss: -0.708348274230957
Batch 4/64 loss: -0.6919636726379395
Batch 5/64 loss: -0.7180933952331543
Batch 6/64 loss: -0.764641284942627
Batch 7/64 loss: -0.7880725860595703
Batch 8/64 loss: -0.4148068428039551
Batch 9/64 loss: -0.6821036338806152
Batch 10/64 loss: -0.5251126289367676
Batch 11/64 loss: -0.4774932861328125
Batch 12/64 loss: -0.7717914581298828
Batch 13/64 loss: -0.2892618179321289
Batch 14/64 loss: -0.6434988975524902
Batch 15/64 loss: -0.6416487693786621
Batch 16/64 loss: -0.7090644836425781
Batch 17/64 loss: -0.7383508682250977
Batch 18/64 loss: -0.7193536758422852
Batch 19/64 loss: -0.5463271141052246
Batch 20/64 loss: -0.6358528137207031
Batch 21/64 loss: -0.5811958312988281
Batch 22/64 loss: -0.6219773292541504
Batch 23/64 loss: -0.7174825668334961
Batch 24/64 loss: -0.6362314224243164
Batch 25/64 loss: -0.73394775390625
Batch 26/64 loss: -0.6207265853881836
Batch 27/64 loss: -0.7531089782714844
Batch 28/64 loss: -0.6332559585571289
Batch 29/64 loss: -0.6880135536193848
Batch 30/64 loss: -0.6028585433959961
Batch 31/64 loss: -0.5135369300842285
Batch 32/64 loss: -0.6495485305786133
Batch 33/64 loss: -0.6291446685791016
Batch 34/64 loss: -0.7712159156799316
Batch 35/64 loss: -0.6615304946899414
Batch 36/64 loss: -0.6212348937988281
Batch 37/64 loss: -0.36887216567993164
Batch 38/64 loss: -0.05605792999267578
Batch 39/64 loss: -0.668053150177002
Batch 40/64 loss: -0.6910214424133301
Batch 41/64 loss: -0.20558547973632812
Batch 42/64 loss: -0.5856714248657227
Batch 43/64 loss: -0.717249870300293
Batch 44/64 loss: -0.2856431007385254
Batch 45/64 loss: -0.5397281646728516
Batch 46/64 loss: 1.0620989799499512
Batch 47/64 loss: -0.6096954345703125
Batch 48/64 loss: -0.6801972389221191
Batch 49/64 loss: 0.4378371238708496
Batch 50/64 loss: -0.6241779327392578
Batch 51/64 loss: -0.701812744140625
Batch 52/64 loss: -0.6820917129516602
Batch 53/64 loss: -0.6228427886962891
Batch 54/64 loss: -0.3329024314880371
Batch 55/64 loss: -0.6143145561218262
Batch 56/64 loss: -0.8010654449462891
Batch 57/64 loss: 0.4343109130859375
Batch 58/64 loss: 1.6481380462646484
Batch 59/64 loss: -0.4338197708129883
Batch 60/64 loss: -0.7639350891113281
Batch 61/64 loss: -0.7280540466308594
Batch 62/64 loss: -0.5531892776489258
Batch 63/64 loss: -0.6382761001586914
Batch 64/64 loss: -4.149839401245117
Epoch 299  Train loss: -0.5605054069967831  Val loss: -0.9519735775452709
Epoch 300
-------------------------------
Batch 1/64 loss: -0.6523475646972656
Batch 2/64 loss: -0.6775031089782715
Batch 3/64 loss: -0.6230249404907227
Batch 4/64 loss: -0.6667699813842773
Batch 5/64 loss: -0.6093401908874512
Batch 6/64 loss: -0.7769217491149902
Batch 7/64 loss: -0.5531320571899414
Batch 8/64 loss: -0.3177309036254883
Batch 9/64 loss: -0.24130725860595703
Batch 10/64 loss: 0.2547140121459961
Batch 11/64 loss: -0.7400054931640625
Batch 12/64 loss: -0.8205370903015137
Batch 13/64 loss: -0.6946148872375488
Batch 14/64 loss: -0.6865835189819336
Batch 15/64 loss: 0.9027962684631348
Batch 16/64 loss: -0.7234625816345215
Batch 17/64 loss: -0.6692557334899902
Batch 18/64 loss: -0.7871561050415039
Batch 19/64 loss: -0.7228798866271973
Batch 20/64 loss: -0.8000349998474121
Batch 21/64 loss: -0.7296442985534668
Batch 22/64 loss: -0.6444754600524902
Batch 23/64 loss: -0.6267571449279785
Batch 24/64 loss: -0.3945145606994629
Batch 25/64 loss: -0.7480807304382324
Batch 26/64 loss: -0.8471293449401855
Batch 27/64 loss: -0.7638278007507324
Batch 28/64 loss: 1.5439567565917969
Batch 29/64 loss: -0.6287245750427246
Batch 30/64 loss: -0.6904416084289551
Batch 31/64 loss: -0.6926522254943848
Batch 32/64 loss: 0.623652458190918
Batch 33/64 loss: -0.7433066368103027
Batch 34/64 loss: -0.6561355590820312
Batch 35/64 loss: -0.7523856163024902
Batch 36/64 loss: -0.5245237350463867
Batch 37/64 loss: -0.4245610237121582
Batch 38/64 loss: 0.01305389404296875
Batch 39/64 loss: -0.679572582244873
Batch 40/64 loss: 0.3803677558898926
Batch 41/64 loss: -0.6857314109802246
Batch 42/64 loss: -0.6828446388244629
Batch 43/64 loss: -0.5333976745605469
Batch 44/64 loss: -0.6434593200683594
Batch 45/64 loss: -0.6295256614685059
Batch 46/64 loss: -0.5141081809997559
Batch 47/64 loss: -0.6737723350524902
Batch 48/64 loss: -0.6967239379882812
Batch 49/64 loss: -0.6456775665283203
Batch 50/64 loss: -0.7306704521179199
Batch 51/64 loss: -0.6803407669067383
Batch 52/64 loss: -0.7429566383361816
Batch 53/64 loss: -0.6970529556274414
Batch 54/64 loss: -0.6780061721801758
Batch 55/64 loss: -0.5752010345458984
Batch 56/64 loss: -0.563896656036377
Batch 57/64 loss: -0.49280405044555664
Batch 58/64 loss: -0.7417936325073242
Batch 59/64 loss: -0.6287417411804199
Batch 60/64 loss: -0.6686549186706543
Batch 61/64 loss: -0.32740020751953125
Batch 62/64 loss: -0.11931467056274414
Batch 63/64 loss: -0.7186064720153809
Batch 64/64 loss: -4.234974384307861
Epoch 300  Train loss: -0.5621601834016687  Val loss: -0.9446929066451555
Epoch 301
-------------------------------
Batch 1/64 loss: -0.7648649215698242
Batch 2/64 loss: -0.7407636642456055
Batch 3/64 loss: -0.6477479934692383
Batch 4/64 loss: -0.7717175483703613
Batch 5/64 loss: -0.4894218444824219
Batch 6/64 loss: -0.3914370536804199
Batch 7/64 loss: -0.8017525672912598
Batch 8/64 loss: -0.7289705276489258
Batch 9/64 loss: 0.2628746032714844
Batch 10/64 loss: 0.4630141258239746
Batch 11/64 loss: -0.6395907402038574
Batch 12/64 loss: -0.5249037742614746
Batch 13/64 loss: -0.7197413444519043
Batch 14/64 loss: -0.7393460273742676
Batch 15/64 loss: -0.6856937408447266
Batch 16/64 loss: -0.6446013450622559
Batch 17/64 loss: -0.7465944290161133
Batch 18/64 loss: -0.7064809799194336
Batch 19/64 loss: -0.7346010208129883
Batch 20/64 loss: -0.48993635177612305
Batch 21/64 loss: -0.6513805389404297
Batch 22/64 loss: -0.6324148178100586
Batch 23/64 loss: -0.14018487930297852
Batch 24/64 loss: -0.7032017707824707
Batch 25/64 loss: -0.6915640830993652
Batch 26/64 loss: -0.4972095489501953
Batch 27/64 loss: -0.5217709541320801
Batch 28/64 loss: -0.6574068069458008
Batch 29/64 loss: -0.726527214050293
Batch 30/64 loss: -0.7524290084838867
Batch 31/64 loss: -0.7515726089477539
Batch 32/64 loss: -0.6259846687316895
Batch 33/64 loss: -0.675018310546875
Batch 34/64 loss: -0.7326569557189941
Batch 35/64 loss: 0.34673595428466797
Batch 36/64 loss: -0.7178258895874023
Batch 37/64 loss: -0.585686206817627
Batch 38/64 loss: -0.6178231239318848
Batch 39/64 loss: -0.5635356903076172
Batch 40/64 loss: -0.5500812530517578
Batch 41/64 loss: -0.6483383178710938
Batch 42/64 loss: -0.5965461730957031
Batch 43/64 loss: -0.7258610725402832
Batch 44/64 loss: -0.6733064651489258
Batch 45/64 loss: -0.8125448226928711
Batch 46/64 loss: 0.492398738861084
Batch 47/64 loss: 0.28905725479125977
Batch 48/64 loss: -0.569587230682373
Batch 49/64 loss: -0.4336967468261719
Batch 50/64 loss: -0.34607744216918945
Batch 51/64 loss: -0.38317108154296875
Batch 52/64 loss: -0.156402587890625
Batch 53/64 loss: -0.07821130752563477
Batch 54/64 loss: -0.3024330139160156
Batch 55/64 loss: -0.24210262298583984
Batch 56/64 loss: -0.13363027572631836
Batch 57/64 loss: -0.120452880859375
Batch 58/64 loss: -0.16900873184204102
Batch 59/64 loss: -0.1880331039428711
Batch 60/64 loss: -0.4450850486755371
Batch 61/64 loss: -0.24458646774291992
Batch 62/64 loss: -0.34924983978271484
Batch 63/64 loss: -0.42503929138183594
Batch 64/64 loss: -3.785327434539795
Epoch 301  Train loss: -0.5143642294640635  Val loss: -0.4828181315943138
Epoch 302
-------------------------------
Batch 1/64 loss: -0.2969484329223633
Batch 2/64 loss: -0.3826766014099121
Batch 3/64 loss: -0.43451547622680664
Batch 4/64 loss: -0.44730615615844727
Batch 5/64 loss: 0.6072244644165039
Batch 6/64 loss: -0.34549379348754883
Batch 7/64 loss: -0.3456997871398926
Batch 8/64 loss: 0.10001182556152344
Batch 9/64 loss: -0.37041139602661133
Batch 10/64 loss: -0.3958001136779785
Batch 11/64 loss: -0.1929001808166504
Batch 12/64 loss: -0.4219212532043457
Batch 13/64 loss: -0.5350437164306641
Batch 14/64 loss: -0.5089502334594727
Batch 15/64 loss: -0.4982419013977051
Batch 16/64 loss: -0.4579281806945801
Batch 17/64 loss: -0.611945629119873
Batch 18/64 loss: -0.6085000038146973
Batch 19/64 loss: -0.4886813163757324
Batch 20/64 loss: -0.509404182434082
Batch 21/64 loss: -0.673762321472168
Batch 22/64 loss: -0.4008903503417969
Batch 23/64 loss: -0.5149273872375488
Batch 24/64 loss: -0.4628133773803711
Batch 25/64 loss: 1.278470516204834
Batch 26/64 loss: -0.5776829719543457
Batch 27/64 loss: -0.604423999786377
Batch 28/64 loss: -0.6208510398864746
Batch 29/64 loss: -0.5639438629150391
Batch 30/64 loss: -0.6648273468017578
Batch 31/64 loss: -0.5877833366394043
Batch 32/64 loss: -0.5780801773071289
Batch 33/64 loss: 0.6949567794799805
Batch 34/64 loss: -0.6980109214782715
Batch 35/64 loss: -0.4904975891113281
Batch 36/64 loss: -0.6271462440490723
Batch 37/64 loss: -0.6197786331176758
Batch 38/64 loss: -0.5289592742919922
Batch 39/64 loss: -0.42597150802612305
Batch 40/64 loss: -0.6540274620056152
Batch 41/64 loss: -0.5524001121520996
Batch 42/64 loss: -0.32323551177978516
Batch 43/64 loss: -0.6470546722412109
Batch 44/64 loss: -0.623598575592041
Batch 45/64 loss: -0.5546236038208008
Batch 46/64 loss: -0.4795975685119629
Batch 47/64 loss: -0.6226954460144043
Batch 48/64 loss: -0.7140717506408691
Batch 49/64 loss: -0.7034788131713867
Batch 50/64 loss: -0.649543285369873
Batch 51/64 loss: -0.4093050956726074
Batch 52/64 loss: -0.4521470069885254
Batch 53/64 loss: -0.7389588356018066
Batch 54/64 loss: -0.49828338623046875
Batch 55/64 loss: 0.15352106094360352
Batch 56/64 loss: -0.5632271766662598
Batch 57/64 loss: -0.7533106803894043
Batch 58/64 loss: -0.6695094108581543
Batch 59/64 loss: -0.6227402687072754
Batch 60/64 loss: -0.7360305786132812
Batch 61/64 loss: -0.7241325378417969
Batch 62/64 loss: -0.5718817710876465
Batch 63/64 loss: -0.6965374946594238
Batch 64/64 loss: -3.7608165740966797
Epoch 302  Train loss: -0.4931692160812079  Val loss: -0.9323637657558795
Epoch 303
-------------------------------
Batch 1/64 loss: -0.6551294326782227
Batch 2/64 loss: -0.7245087623596191
Batch 3/64 loss: -0.6770615577697754
Batch 4/64 loss: -0.7043642997741699
Batch 5/64 loss: -0.4630308151245117
Batch 6/64 loss: -0.5387301445007324
Batch 7/64 loss: 0.3159923553466797
Batch 8/64 loss: -0.0703134536743164
Batch 9/64 loss: -0.611574649810791
Batch 10/64 loss: -0.5877699851989746
Batch 11/64 loss: -0.6364998817443848
Batch 12/64 loss: -0.6926827430725098
Batch 13/64 loss: -0.5712146759033203
Batch 14/64 loss: -0.6460261344909668
Batch 15/64 loss: -0.1998739242553711
Batch 16/64 loss: -0.7404556274414062
Batch 17/64 loss: -0.7300076484680176
Batch 18/64 loss: -0.6764111518859863
Batch 19/64 loss: -0.6578526496887207
Batch 20/64 loss: -0.6808223724365234
Batch 21/64 loss: -0.6565213203430176
Batch 22/64 loss: -0.7139430046081543
Batch 23/64 loss: -0.6240625381469727
Batch 24/64 loss: -0.6882195472717285
Batch 25/64 loss: -0.14678430557250977
Batch 26/64 loss: -0.6497902870178223
Batch 27/64 loss: -0.6219706535339355
Batch 28/64 loss: -0.6936244964599609
Batch 29/64 loss: -0.6975741386413574
Batch 30/64 loss: -0.5131478309631348
Batch 31/64 loss: -0.5205893516540527
Batch 32/64 loss: 0.8523154258728027
Batch 33/64 loss: -0.6639299392700195
Batch 34/64 loss: -0.4165925979614258
Batch 35/64 loss: 0.9617214202880859
Batch 36/64 loss: -0.5018196105957031
Batch 37/64 loss: -0.6432628631591797
Batch 38/64 loss: -0.5532054901123047
Batch 39/64 loss: -0.6450257301330566
Batch 40/64 loss: -0.616267204284668
Batch 41/64 loss: -0.5718240737915039
Batch 42/64 loss: -0.624110221862793
Batch 43/64 loss: -0.5836000442504883
Batch 44/64 loss: -0.6047325134277344
Batch 45/64 loss: -0.4711647033691406
Batch 46/64 loss: -0.6327199935913086
Batch 47/64 loss: -0.6869597434997559
Batch 48/64 loss: -0.7086057662963867
Batch 49/64 loss: -0.743199348449707
Batch 50/64 loss: -0.640864372253418
Batch 51/64 loss: -0.6977791786193848
Batch 52/64 loss: -0.6235690116882324
Batch 53/64 loss: -0.6567139625549316
Batch 54/64 loss: -0.6141424179077148
Batch 55/64 loss: -0.601710319519043
Batch 56/64 loss: -0.5129337310791016
Batch 57/64 loss: -0.6555376052856445
Batch 58/64 loss: -0.10719585418701172
Batch 59/64 loss: -0.6388216018676758
Batch 60/64 loss: -0.5488762855529785
Batch 61/64 loss: -0.5447916984558105
Batch 62/64 loss: -0.4236469268798828
Batch 63/64 loss: -0.6065630912780762
Batch 64/64 loss: -4.170164585113525
Epoch 303  Train loss: -0.5698559611451393  Val loss: -0.7501209298359979
Epoch 304
-------------------------------
Batch 1/64 loss: -0.3574047088623047
Batch 2/64 loss: -0.1392207145690918
Batch 3/64 loss: -0.5817093849182129
Batch 4/64 loss: -0.41533851623535156
Batch 5/64 loss: -0.6110343933105469
Batch 6/64 loss: -0.6657657623291016
Batch 7/64 loss: -0.6508917808532715
Batch 8/64 loss: -0.7196159362792969
Batch 9/64 loss: -0.6192083358764648
Batch 10/64 loss: -0.43865251541137695
Batch 11/64 loss: -0.7765021324157715
Batch 12/64 loss: -0.654545783996582
Batch 13/64 loss: -0.6417694091796875
Batch 14/64 loss: -0.5728707313537598
Batch 15/64 loss: -0.5745220184326172
Batch 16/64 loss: -0.48362159729003906
Batch 17/64 loss: -0.6612062454223633
Batch 18/64 loss: 1.0902423858642578
Batch 19/64 loss: -0.5159063339233398
Batch 20/64 loss: -0.3349008560180664
Batch 21/64 loss: -0.4641280174255371
Batch 22/64 loss: -0.2455449104309082
Batch 23/64 loss: -0.5241727828979492
Batch 24/64 loss: -0.6353898048400879
Batch 25/64 loss: -0.5729718208312988
Batch 26/64 loss: 0.48128700256347656
Batch 27/64 loss: -0.6035261154174805
Batch 28/64 loss: -0.037514686584472656
Batch 29/64 loss: -0.7208690643310547
Batch 30/64 loss: -0.6441783905029297
Batch 31/64 loss: -0.6756911277770996
Batch 32/64 loss: -0.5846157073974609
Batch 33/64 loss: -0.7860903739929199
Batch 34/64 loss: -0.8368339538574219
Batch 35/64 loss: -0.4999504089355469
Batch 36/64 loss: -0.6965270042419434
Batch 37/64 loss: -0.7320241928100586
Batch 38/64 loss: -0.6877069473266602
Batch 39/64 loss: -0.6348104476928711
Batch 40/64 loss: -0.6991853713989258
Batch 41/64 loss: -0.6357803344726562
Batch 42/64 loss: -0.4907827377319336
Batch 43/64 loss: -0.5606546401977539
Batch 44/64 loss: -0.18510103225708008
Batch 45/64 loss: -0.6719765663146973
Batch 46/64 loss: -0.6593289375305176
Batch 47/64 loss: -0.7448263168334961
Batch 48/64 loss: -0.6894311904907227
Batch 49/64 loss: -0.674896240234375
Batch 50/64 loss: -0.7070140838623047
Batch 51/64 loss: -0.49459314346313477
Batch 52/64 loss: -0.7011327743530273
Batch 53/64 loss: -0.6306424140930176
Batch 54/64 loss: -0.6052274703979492
Batch 55/64 loss: -0.20406103134155273
Batch 56/64 loss: -0.4173908233642578
Batch 57/64 loss: 0.7759575843811035
Batch 58/64 loss: -0.636286735534668
Batch 59/64 loss: -0.43938541412353516
Batch 60/64 loss: -0.7347803115844727
Batch 61/64 loss: -0.6950182914733887
Batch 62/64 loss: -0.7614364624023438
Batch 63/64 loss: -0.6400256156921387
Batch 64/64 loss: -4.05157995223999
Epoch 304  Train loss: -0.5547825701096478  Val loss: -0.8339694963697716
Epoch 305
-------------------------------
Batch 1/64 loss: -0.731743335723877
Batch 2/64 loss: -0.611976146697998
Batch 3/64 loss: -0.7145333290100098
Batch 4/64 loss: -0.6078166961669922
Batch 5/64 loss: -0.5591139793395996
Batch 6/64 loss: -0.6804704666137695
Batch 7/64 loss: -0.6014323234558105
Batch 8/64 loss: -0.7436141967773438
Batch 9/64 loss: -0.6103453636169434
Batch 10/64 loss: -0.6340312957763672
Batch 11/64 loss: -0.6773509979248047
Batch 12/64 loss: -0.650566577911377
Batch 13/64 loss: -0.5286269187927246
Batch 14/64 loss: -0.7287087440490723
Batch 15/64 loss: -0.7247066497802734
Batch 16/64 loss: -0.647148609161377
Batch 17/64 loss: -0.5899176597595215
Batch 18/64 loss: -0.7288532257080078
Batch 19/64 loss: -0.5201916694641113
Batch 20/64 loss: -0.723846435546875
Batch 21/64 loss: -0.4591655731201172
Batch 22/64 loss: -0.7135181427001953
Batch 23/64 loss: -0.7657046318054199
Batch 24/64 loss: -0.6523885726928711
Batch 25/64 loss: -0.5897808074951172
Batch 26/64 loss: -0.458188533782959
Batch 27/64 loss: -0.6667881011962891
Batch 28/64 loss: -0.1852555274963379
Batch 29/64 loss: -0.6196842193603516
Batch 30/64 loss: -0.7033252716064453
Batch 31/64 loss: -0.4753727912902832
Batch 32/64 loss: -0.6322531700134277
Batch 33/64 loss: -0.5382070541381836
Batch 34/64 loss: -0.24814128875732422
Batch 35/64 loss: -0.544278621673584
Batch 36/64 loss: -0.5410690307617188
Batch 37/64 loss: -0.0003719329833984375
Batch 38/64 loss: 0.09048938751220703
Batch 39/64 loss: -0.7569131851196289
Batch 40/64 loss: -0.5833697319030762
Batch 41/64 loss: -0.7111191749572754
Batch 42/64 loss: -0.44237375259399414
Batch 43/64 loss: -0.5039238929748535
Batch 44/64 loss: -0.5406126976013184
Batch 45/64 loss: -0.5762486457824707
Batch 46/64 loss: -0.6719074249267578
Batch 47/64 loss: -0.43488121032714844
Batch 48/64 loss: -0.5647554397583008
Batch 49/64 loss: -0.732661247253418
Batch 50/64 loss: -0.6502251625061035
Batch 51/64 loss: -0.5977745056152344
Batch 52/64 loss: -0.6265950202941895
Batch 53/64 loss: 1.0548763275146484
Batch 54/64 loss: -0.4299654960632324
Batch 55/64 loss: 1.5795187950134277
Batch 56/64 loss: 0.5378780364990234
Batch 57/64 loss: -0.5131096839904785
Batch 58/64 loss: -0.5240507125854492
Batch 59/64 loss: 0.056003570556640625
Batch 60/64 loss: -0.6092891693115234
Batch 61/64 loss: -0.5551314353942871
Batch 62/64 loss: -0.3701930046081543
Batch 63/64 loss: -0.5926365852355957
Batch 64/64 loss: -4.2335944175720215
Epoch 305  Train loss: -0.5278847806593951  Val loss: -0.7390144846283693
Epoch 306
-------------------------------
Batch 1/64 loss: -0.6333932876586914
Batch 2/64 loss: -0.6240558624267578
Batch 3/64 loss: -0.21755313873291016
Batch 4/64 loss: -0.5783686637878418
Batch 5/64 loss: -0.6155638694763184
Batch 6/64 loss: -0.37879037857055664
Batch 7/64 loss: -0.5951776504516602
Batch 8/64 loss: -0.6922874450683594
Batch 9/64 loss: -0.31165075302124023
Batch 10/64 loss: -0.1390666961669922
Batch 11/64 loss: -0.5315079689025879
Batch 12/64 loss: -0.6499748229980469
Batch 13/64 loss: -0.46142053604125977
Batch 14/64 loss: -0.5767412185668945
Batch 15/64 loss: -0.6480331420898438
Batch 16/64 loss: -0.6736927032470703
Batch 17/64 loss: -0.7772336006164551
Batch 18/64 loss: -0.4450392723083496
Batch 19/64 loss: -0.5306291580200195
Batch 20/64 loss: -0.5442585945129395
Batch 21/64 loss: -0.6352977752685547
Batch 22/64 loss: -0.13059377670288086
Batch 23/64 loss: -0.7508063316345215
Batch 24/64 loss: -0.4877200126647949
Batch 25/64 loss: -0.7452659606933594
Batch 26/64 loss: -0.7636609077453613
Batch 27/64 loss: -0.10269594192504883
Batch 28/64 loss: -0.5724382400512695
Batch 29/64 loss: 0.27823781967163086
Batch 30/64 loss: -0.27097511291503906
Batch 31/64 loss: -0.6842875480651855
Batch 32/64 loss: -0.12296295166015625
Batch 33/64 loss: -0.448300838470459
Batch 34/64 loss: -0.5837693214416504
Batch 35/64 loss: -0.6541976928710938
Batch 36/64 loss: -0.649712085723877
Batch 37/64 loss: -0.45085620880126953
Batch 38/64 loss: -0.6258220672607422
Batch 39/64 loss: -0.43988561630249023
Batch 40/64 loss: -0.7204513549804688
Batch 41/64 loss: -0.5329208374023438
Batch 42/64 loss: -0.7366676330566406
Batch 43/64 loss: -0.5019173622131348
Batch 44/64 loss: -0.6348252296447754
Batch 45/64 loss: -0.5741672515869141
Batch 46/64 loss: -0.6435971260070801
Batch 47/64 loss: -0.5292482376098633
Batch 48/64 loss: -0.3971748352050781
Batch 49/64 loss: -0.650331974029541
Batch 50/64 loss: -0.5435299873352051
Batch 51/64 loss: -0.33722925186157227
Batch 52/64 loss: -0.5975289344787598
Batch 53/64 loss: -0.5548415184020996
Batch 54/64 loss: -0.6116094589233398
Batch 55/64 loss: 0.570523738861084
Batch 56/64 loss: -0.43674755096435547
Batch 57/64 loss: -0.7012066841125488
Batch 58/64 loss: -0.43646907806396484
Batch 59/64 loss: 0.8416748046875
Batch 60/64 loss: -0.6369032859802246
Batch 61/64 loss: -0.6831331253051758
Batch 62/64 loss: -0.7353553771972656
Batch 63/64 loss: -0.5645542144775391
Batch 64/64 loss: -3.3266282081604004
Epoch 306  Train loss: -0.5224883490917729  Val loss: -0.8641087902370597
Epoch 307
-------------------------------
Batch 1/64 loss: -0.6923456192016602
Batch 2/64 loss: -0.713376522064209
Batch 3/64 loss: -0.5336093902587891
Batch 4/64 loss: -0.5758419036865234
Batch 5/64 loss: -0.2516441345214844
Batch 6/64 loss: -0.6588034629821777
Batch 7/64 loss: -0.6416130065917969
Batch 8/64 loss: -0.7641468048095703
Batch 9/64 loss: -0.6218357086181641
Batch 10/64 loss: -0.37191295623779297
Batch 11/64 loss: -0.4480400085449219
Batch 12/64 loss: -0.5909571647644043
Batch 13/64 loss: -0.601386547088623
Batch 14/64 loss: -0.6685400009155273
Batch 15/64 loss: -0.794802188873291
Batch 16/64 loss: -0.5955362319946289
Batch 17/64 loss: -0.6522688865661621
Batch 18/64 loss: -0.0832376480102539
Batch 19/64 loss: -0.5845952033996582
Batch 20/64 loss: -0.46600866317749023
Batch 21/64 loss: -0.44783878326416016
Batch 22/64 loss: -0.42008066177368164
Batch 23/64 loss: -0.49698638916015625
Batch 24/64 loss: -0.5082855224609375
Batch 25/64 loss: -0.7098522186279297
Batch 26/64 loss: -0.524803638458252
Batch 27/64 loss: -0.2681612968444824
Batch 28/64 loss: -0.7791900634765625
Batch 29/64 loss: -0.8202095031738281
Batch 30/64 loss: -0.0032606124877929688
Batch 31/64 loss: -0.5024623870849609
Batch 32/64 loss: -0.5815019607543945
Batch 33/64 loss: -0.7331266403198242
Batch 34/64 loss: 0.9928059577941895
Batch 35/64 loss: -0.5368137359619141
Batch 36/64 loss: -0.13297224044799805
Batch 37/64 loss: -0.6534261703491211
Batch 38/64 loss: -0.20721864700317383
Batch 39/64 loss: -0.6033177375793457
Batch 40/64 loss: 0.3394918441772461
Batch 41/64 loss: -0.0022363662719726562
Batch 42/64 loss: -0.4948406219482422
Batch 43/64 loss: -0.698493480682373
Batch 44/64 loss: -0.2861747741699219
Batch 45/64 loss: -0.6319832801818848
Batch 46/64 loss: -0.7047810554504395
Batch 47/64 loss: 0.5780715942382812
Batch 48/64 loss: -0.5869464874267578
Batch 49/64 loss: -0.3679323196411133
Batch 50/64 loss: -0.6776790618896484
Batch 51/64 loss: -0.7044858932495117
Batch 52/64 loss: 0.09197139739990234
Batch 53/64 loss: -0.5608043670654297
Batch 54/64 loss: -0.7638354301452637
Batch 55/64 loss: -0.38910627365112305
Batch 56/64 loss: -0.6480140686035156
Batch 57/64 loss: -0.47037553787231445
Batch 58/64 loss: -0.6133866310119629
Batch 59/64 loss: -0.3241844177246094
Batch 60/64 loss: -0.45360755920410156
Batch 61/64 loss: -0.5611066818237305
Batch 62/64 loss: -0.5072016716003418
Batch 63/64 loss: -0.5836453437805176
Batch 64/64 loss: -4.196507453918457
Epoch 307  Train loss: -0.5084842569687787  Val loss: -0.7653502500343978
Epoch 308
-------------------------------
Batch 1/64 loss: -0.4905376434326172
Batch 2/64 loss: -0.48515748977661133
Batch 3/64 loss: -0.4841618537902832
Batch 4/64 loss: -0.6921024322509766
Batch 5/64 loss: -0.3506288528442383
Batch 6/64 loss: 0.17590904235839844
Batch 7/64 loss: 0.5512790679931641
Batch 8/64 loss: -0.5126714706420898
Batch 9/64 loss: -0.04610443115234375
Batch 10/64 loss: -0.39969348907470703
Batch 11/64 loss: 0.2785305976867676
Batch 12/64 loss: -0.49041175842285156
Batch 13/64 loss: -0.18519926071166992
Batch 14/64 loss: -0.6734738349914551
Batch 15/64 loss: -0.6161594390869141
Batch 16/64 loss: -0.708836555480957
Batch 17/64 loss: -0.6715240478515625
Batch 18/64 loss: -0.5138278007507324
Batch 19/64 loss: -0.47225141525268555
Batch 20/64 loss: -0.710362434387207
Batch 21/64 loss: -0.598757266998291
Batch 22/64 loss: -0.606602668762207
Batch 23/64 loss: -0.5999064445495605
Batch 24/64 loss: -0.6055021286010742
Batch 25/64 loss: -0.5498647689819336
Batch 26/64 loss: -0.3740348815917969
Batch 27/64 loss: -0.6191883087158203
Batch 28/64 loss: -0.4033627510070801
Batch 29/64 loss: -0.6320405006408691
Batch 30/64 loss: -0.6412792205810547
Batch 31/64 loss: -0.45821237564086914
Batch 32/64 loss: -0.4417142868041992
Batch 33/64 loss: -0.7441987991333008
Batch 34/64 loss: -0.4266214370727539
Batch 35/64 loss: -0.4787168502807617
Batch 36/64 loss: -0.6522235870361328
Batch 37/64 loss: -0.5868048667907715
Batch 38/64 loss: -0.3159370422363281
Batch 39/64 loss: -0.6088423728942871
Batch 40/64 loss: -0.41597986221313477
Batch 41/64 loss: -0.4148283004760742
Batch 42/64 loss: -0.5887928009033203
Batch 43/64 loss: -0.49593591690063477
Batch 44/64 loss: 1.572066307067871
Batch 45/64 loss: -0.0740814208984375
Batch 46/64 loss: -0.722144603729248
Batch 47/64 loss: -0.7082700729370117
Batch 48/64 loss: -0.5186352729797363
Batch 49/64 loss: -0.2876863479614258
Batch 50/64 loss: -0.6263036727905273
Batch 51/64 loss: -0.5291471481323242
Batch 52/64 loss: -0.240966796875
Batch 53/64 loss: -0.17680597305297852
Batch 54/64 loss: -0.4725832939147949
Batch 55/64 loss: 0.09369087219238281
Batch 56/64 loss: -0.2406773567199707
Batch 57/64 loss: -0.53082275390625
Batch 58/64 loss: 0.22313451766967773
Batch 59/64 loss: 0.0421452522277832
Batch 60/64 loss: -0.4909849166870117
Batch 61/64 loss: -0.4360470771789551
Batch 62/64 loss: -0.5061855316162109
Batch 63/64 loss: -0.5172553062438965
Batch 64/64 loss: -4.02329158782959
Epoch 308  Train loss: -0.43798842710607194  Val loss: -0.6871522857561144
Epoch 309
-------------------------------
Batch 1/64 loss: -0.5677733421325684
Batch 2/64 loss: -0.403170108795166
Batch 3/64 loss: -0.466583251953125
Batch 4/64 loss: -0.33962106704711914
Batch 5/64 loss: -0.5290484428405762
Batch 6/64 loss: -0.6242251396179199
Batch 7/64 loss: -0.3778815269470215
Batch 8/64 loss: -0.35906076431274414
Batch 9/64 loss: 0.5706987380981445
Batch 10/64 loss: -0.46294641494750977
Batch 11/64 loss: -0.4069938659667969
Batch 12/64 loss: -0.4091825485229492
Batch 13/64 loss: -0.3182806968688965
Batch 14/64 loss: -0.5079026222229004
Batch 15/64 loss: -0.6398701667785645
Batch 16/64 loss: -0.4194526672363281
Batch 17/64 loss: -0.7124667167663574
Batch 18/64 loss: -0.33597517013549805
Batch 19/64 loss: -0.4544515609741211
Batch 20/64 loss: -0.5862598419189453
Batch 21/64 loss: -0.41631174087524414
Batch 22/64 loss: -0.737673282623291
Batch 23/64 loss: 0.7338590621948242
Batch 24/64 loss: -0.2611722946166992
Batch 25/64 loss: -0.6323361396789551
Batch 26/64 loss: -0.5335350036621094
Batch 27/64 loss: -0.5828084945678711
Batch 28/64 loss: -0.5922403335571289
Batch 29/64 loss: -0.15973615646362305
Batch 30/64 loss: -0.5390572547912598
Batch 31/64 loss: -0.4416232109069824
Batch 32/64 loss: -0.42587900161743164
Batch 33/64 loss: -0.13092660903930664
Batch 34/64 loss: -0.4181690216064453
Batch 35/64 loss: 1.0764193534851074
Batch 36/64 loss: -0.581514835357666
Batch 37/64 loss: -0.6371250152587891
Batch 38/64 loss: -0.4026355743408203
Batch 39/64 loss: -0.27280187606811523
Batch 40/64 loss: -0.18608570098876953
Batch 41/64 loss: -0.4809913635253906
Batch 42/64 loss: -0.4804267883300781
Batch 43/64 loss: -0.3728346824645996
Batch 44/64 loss: 0.5143141746520996
Batch 45/64 loss: -0.5019674301147461
Batch 46/64 loss: 0.25706005096435547
Batch 47/64 loss: -0.5253243446350098
Batch 48/64 loss: -0.5984234809875488
Batch 49/64 loss: -0.47143077850341797
Batch 50/64 loss: -0.42395782470703125
Batch 51/64 loss: -0.6039628982543945
Batch 52/64 loss: 0.22788667678833008
Batch 53/64 loss: -0.3168826103210449
Batch 54/64 loss: -0.5867557525634766
Batch 55/64 loss: -0.683690071105957
Batch 56/64 loss: -0.6182651519775391
Batch 57/64 loss: -0.6363024711608887
Batch 58/64 loss: -0.6057238578796387
Batch 59/64 loss: -0.43483829498291016
Batch 60/64 loss: -0.5589065551757812
Batch 61/64 loss: 0.07468414306640625
Batch 62/64 loss: -0.2947101593017578
Batch 63/64 loss: -0.6166834831237793
Batch 64/64 loss: -3.799133777618408
Epoch 309  Train loss: -0.40908680149153165  Val loss: -0.7856963377228308
Epoch 310
-------------------------------
Batch 1/64 loss: -0.6322021484375
Batch 2/64 loss: -0.5508756637573242
Batch 3/64 loss: -0.38074398040771484
Batch 4/64 loss: -0.4599881172180176
Batch 5/64 loss: -0.5759305953979492
Batch 6/64 loss: -0.7023711204528809
Batch 7/64 loss: -0.5934176445007324
Batch 8/64 loss: -0.661036491394043
Batch 9/64 loss: -0.24480247497558594
Batch 10/64 loss: -0.6645851135253906
Batch 11/64 loss: -0.6903185844421387
Batch 12/64 loss: -0.605748176574707
Batch 13/64 loss: -0.11025571823120117
Batch 14/64 loss: -0.5753889083862305
Batch 15/64 loss: -0.6450657844543457
Batch 16/64 loss: -0.34719133377075195
Batch 17/64 loss: -0.39644813537597656
Batch 18/64 loss: -0.5569605827331543
Batch 19/64 loss: -0.5677170753479004
Batch 20/64 loss: -0.6217818260192871
Batch 21/64 loss: -0.7752432823181152
Batch 22/64 loss: 0.13101816177368164
Batch 23/64 loss: -0.605292797088623
Batch 24/64 loss: -0.47704172134399414
Batch 25/64 loss: -0.6721148490905762
Batch 26/64 loss: -0.7199978828430176
Batch 27/64 loss: -0.5835781097412109
Batch 28/64 loss: 0.5384774208068848
Batch 29/64 loss: -0.7116508483886719
Batch 30/64 loss: -0.4569854736328125
Batch 31/64 loss: -0.4460301399230957
Batch 32/64 loss: -0.5143804550170898
Batch 33/64 loss: 0.04985952377319336
Batch 34/64 loss: -0.5244674682617188
Batch 35/64 loss: -0.7056527137756348
Batch 36/64 loss: -0.5549869537353516
Batch 37/64 loss: -0.19022512435913086
Batch 38/64 loss: -0.6715054512023926
Batch 39/64 loss: -0.5564126968383789
Batch 40/64 loss: -0.6424493789672852
Batch 41/64 loss: -0.3764925003051758
Batch 42/64 loss: -0.5608649253845215
Batch 43/64 loss: -0.0929098129272461
Batch 44/64 loss: -0.5334930419921875
Batch 45/64 loss: -0.485198974609375
Batch 46/64 loss: -0.5966348648071289
Batch 47/64 loss: -0.6227812767028809
Batch 48/64 loss: -0.5924539566040039
Batch 49/64 loss: -0.6018204689025879
Batch 50/64 loss: -0.6481027603149414
Batch 51/64 loss: -0.4055781364440918
Batch 52/64 loss: -0.4708366394042969
Batch 53/64 loss: -0.6554865837097168
Batch 54/64 loss: -0.5628466606140137
Batch 55/64 loss: -0.7789278030395508
Batch 56/64 loss: -0.4984760284423828
Batch 57/64 loss: -0.0878596305847168
Batch 58/64 loss: -0.6381120681762695
Batch 59/64 loss: -0.642122745513916
Batch 60/64 loss: 0.6157045364379883
Batch 61/64 loss: 0.8249235153198242
Batch 62/64 loss: -0.4583268165588379
Batch 63/64 loss: -0.7115969657897949
Batch 64/64 loss: -4.305381774902344
Epoch 310  Train loss: -0.5095030691109451  Val loss: -0.7863409429071695
Epoch 311
-------------------------------
Batch 1/64 loss: -0.5805749893188477
Batch 2/64 loss: -0.4287118911743164
Batch 3/64 loss: -0.36275196075439453
Batch 4/64 loss: -0.7237548828125
Batch 5/64 loss: -0.42692136764526367
Batch 6/64 loss: -0.4244651794433594
Batch 7/64 loss: 0.07359933853149414
Batch 8/64 loss: -0.14773273468017578
Batch 9/64 loss: 1.341285228729248
Batch 10/64 loss: -0.20450973510742188
Batch 11/64 loss: -0.47335100173950195
Batch 12/64 loss: 0.3450927734375
Batch 13/64 loss: -0.1238565444946289
Batch 14/64 loss: -0.2219223976135254
Batch 15/64 loss: -0.5763349533081055
Batch 16/64 loss: -0.6650691032409668
Batch 17/64 loss: -0.49356651306152344
Batch 18/64 loss: -0.5502285957336426
Batch 19/64 loss: -0.35158824920654297
Batch 20/64 loss: -0.4823169708251953
Batch 21/64 loss: -0.35892534255981445
Batch 22/64 loss: -0.5228714942932129
Batch 23/64 loss: -0.30431222915649414
Batch 24/64 loss: -0.6599297523498535
Batch 25/64 loss: -0.2808208465576172
Batch 26/64 loss: -0.5723609924316406
Batch 27/64 loss: -0.5517158508300781
Batch 28/64 loss: -0.4679274559020996
Batch 29/64 loss: -0.3341708183288574
Batch 30/64 loss: -0.24583673477172852
Batch 31/64 loss: -0.4346957206726074
Batch 32/64 loss: -0.585662841796875
Batch 33/64 loss: -0.6183233261108398
Batch 34/64 loss: -0.4793386459350586
Batch 35/64 loss: -0.48288583755493164
Batch 36/64 loss: -0.5043301582336426
Batch 37/64 loss: 0.6289248466491699
Batch 38/64 loss: -0.3627767562866211
Batch 39/64 loss: -0.6235122680664062
Batch 40/64 loss: -0.04437971115112305
Batch 41/64 loss: -0.5600776672363281
Batch 42/64 loss: -0.5902261734008789
Batch 43/64 loss: -0.2531442642211914
Batch 44/64 loss: -0.4353170394897461
Batch 45/64 loss: -0.2645912170410156
Batch 46/64 loss: -0.4922046661376953
Batch 47/64 loss: -0.4890437126159668
Batch 48/64 loss: -0.544126033782959
Batch 49/64 loss: -0.49009275436401367
Batch 50/64 loss: 0.23434734344482422
Batch 51/64 loss: -0.33629703521728516
Batch 52/64 loss: -0.4719963073730469
Batch 53/64 loss: -0.4836616516113281
Batch 54/64 loss: -0.42616796493530273
Batch 55/64 loss: -0.45804643630981445
Batch 56/64 loss: -0.21234464645385742
Batch 57/64 loss: -0.47397708892822266
Batch 58/64 loss: -0.2172408103942871
Batch 59/64 loss: 0.07428932189941406
Batch 60/64 loss: -0.3607029914855957
Batch 61/64 loss: -0.5482907295227051
Batch 62/64 loss: -0.18239736557006836
Batch 63/64 loss: -0.5409088134765625
Batch 64/64 loss: -4.105587959289551
Epoch 311  Train loss: -0.39035202101165173  Val loss: -0.701956116456756
Epoch 312
-------------------------------
Batch 1/64 loss: -0.317990779876709
Batch 2/64 loss: -0.4220104217529297
Batch 3/64 loss: -0.3481125831604004
Batch 4/64 loss: -0.4181480407714844
Batch 5/64 loss: -0.5520663261413574
Batch 6/64 loss: -0.24421215057373047
Batch 7/64 loss: -0.36844539642333984
Batch 8/64 loss: -0.5699024200439453
Batch 9/64 loss: -0.43308544158935547
Batch 10/64 loss: -0.5267934799194336
Batch 11/64 loss: -0.30914783477783203
Batch 12/64 loss: -0.4229092597961426
Batch 13/64 loss: 0.7508749961853027
Batch 14/64 loss: -0.49967288970947266
Batch 15/64 loss: -0.3950467109680176
Batch 16/64 loss: 0.06787347793579102
Batch 17/64 loss: -0.48480987548828125
Batch 18/64 loss: -0.6059169769287109
Batch 19/64 loss: -0.4205455780029297
Batch 20/64 loss: -0.5593128204345703
Batch 21/64 loss: -0.5409908294677734
Batch 22/64 loss: -0.4619021415710449
Batch 23/64 loss: -0.5234847068786621
Batch 24/64 loss: 0.27973175048828125
Batch 25/64 loss: -0.5886998176574707
Batch 26/64 loss: -0.43975019454956055
Batch 27/64 loss: -0.5147957801818848
Batch 28/64 loss: -0.5400552749633789
Batch 29/64 loss: -0.5885014533996582
Batch 30/64 loss: -0.6091008186340332
Batch 31/64 loss: -0.5911397933959961
Batch 32/64 loss: -0.4580421447753906
Batch 33/64 loss: -0.5636873245239258
Batch 34/64 loss: -0.10322093963623047
Batch 35/64 loss: -0.510892391204834
Batch 36/64 loss: -0.46375131607055664
Batch 37/64 loss: -0.6428861618041992
Batch 38/64 loss: -0.6049251556396484
Batch 39/64 loss: -0.561030387878418
Batch 40/64 loss: 0.06647300720214844
Batch 41/64 loss: -0.5165109634399414
Batch 42/64 loss: -0.6779084205627441
Batch 43/64 loss: -0.34819459915161133
Batch 44/64 loss: -0.5496492385864258
Batch 45/64 loss: -0.6006646156311035
Batch 46/64 loss: -0.46672487258911133
Batch 47/64 loss: -0.6200742721557617
Batch 48/64 loss: -0.4427299499511719
Batch 49/64 loss: -0.6353297233581543
Batch 50/64 loss: -0.5087976455688477
Batch 51/64 loss: -0.7862339019775391
Batch 52/64 loss: -0.43798160552978516
Batch 53/64 loss: -0.047589778900146484
Batch 54/64 loss: -0.5990505218505859
Batch 55/64 loss: 0.05787849426269531
Batch 56/64 loss: 1.5865211486816406
Batch 57/64 loss: -0.19105005264282227
Batch 58/64 loss: -0.4183783531188965
Batch 59/64 loss: -0.6132183074951172
Batch 60/64 loss: 0.6260952949523926
Batch 61/64 loss: -0.7118611335754395
Batch 62/64 loss: -0.35101747512817383
Batch 63/64 loss: -0.4087400436401367
Batch 64/64 loss: -4.164790630340576
Epoch 312  Train loss: -0.4207817395528158  Val loss: -0.5919819599164721
Epoch 313
-------------------------------
Batch 1/64 loss: -0.5957260131835938
Batch 2/64 loss: -0.6966581344604492
Batch 3/64 loss: -0.4802088737487793
Batch 4/64 loss: 0.4628329277038574
Batch 5/64 loss: -0.5549511909484863
Batch 6/64 loss: -0.5999412536621094
Batch 7/64 loss: -0.35623931884765625
Batch 8/64 loss: -0.6624064445495605
Batch 9/64 loss: -0.46864938735961914
Batch 10/64 loss: -0.2652778625488281
Batch 11/64 loss: -0.5625810623168945
Batch 12/64 loss: -0.7392578125
Batch 13/64 loss: -0.4684410095214844
Batch 14/64 loss: -0.5494613647460938
Batch 15/64 loss: -0.5363969802856445
Batch 16/64 loss: -0.5733418464660645
Batch 17/64 loss: -0.4706001281738281
Batch 18/64 loss: -0.49346494674682617
Batch 19/64 loss: 0.06620454788208008
Batch 20/64 loss: -0.6791849136352539
Batch 21/64 loss: -0.41800785064697266
Batch 22/64 loss: -0.6069540977478027
Batch 23/64 loss: -0.4034571647644043
Batch 24/64 loss: 0.09626340866088867
Batch 25/64 loss: -0.519683837890625
Batch 26/64 loss: -0.6217470169067383
Batch 27/64 loss: -0.41080617904663086
Batch 28/64 loss: -0.6946620941162109
Batch 29/64 loss: -0.5657730102539062
Batch 30/64 loss: -0.6118326187133789
Batch 31/64 loss: -0.26856327056884766
Batch 32/64 loss: -0.2912302017211914
Batch 33/64 loss: -0.38593196868896484
Batch 34/64 loss: -0.6454057693481445
Batch 35/64 loss: -0.20081663131713867
Batch 36/64 loss: -0.45464611053466797
Batch 37/64 loss: -0.4808945655822754
Batch 38/64 loss: -0.4596576690673828
Batch 39/64 loss: -0.5490193367004395
Batch 40/64 loss: -0.42128658294677734
Batch 41/64 loss: -0.5962762832641602
Batch 42/64 loss: -0.5700302124023438
Batch 43/64 loss: -0.5906291007995605
Batch 44/64 loss: -0.3725252151489258
Batch 45/64 loss: -0.5043683052062988
Batch 46/64 loss: -0.3865523338317871
Batch 47/64 loss: -0.5180525779724121
Batch 48/64 loss: 2.0908045768737793
Batch 49/64 loss: -0.039739131927490234
Batch 50/64 loss: -0.08655166625976562
Batch 51/64 loss: -0.39131879806518555
Batch 52/64 loss: -0.4089345932006836
Batch 53/64 loss: -0.5337481498718262
Batch 54/64 loss: -0.5708718299865723
Batch 55/64 loss: -0.6193079948425293
Batch 56/64 loss: -0.5342435836791992
Batch 57/64 loss: -0.5801410675048828
Batch 58/64 loss: -0.1120138168334961
Batch 59/64 loss: -0.4495248794555664
Batch 60/64 loss: -0.6441679000854492
Batch 61/64 loss: -0.7191767692565918
Batch 62/64 loss: -0.47909975051879883
Batch 63/64 loss: 0.4206681251525879
Batch 64/64 loss: -4.005352973937988
Epoch 313  Train loss: -0.4445126215616862  Val loss: -0.693813940094099
Epoch 314
-------------------------------
Batch 1/64 loss: -0.5933032035827637
Batch 2/64 loss: 0.5513081550598145
Batch 3/64 loss: -0.5937027931213379
Batch 4/64 loss: -0.48980283737182617
Batch 5/64 loss: -0.5480823516845703
Batch 6/64 loss: 0.014213085174560547
Batch 7/64 loss: -0.3734164237976074
Batch 8/64 loss: -0.5662860870361328
Batch 9/64 loss: -0.07903003692626953
Batch 10/64 loss: -0.4297642707824707
Batch 11/64 loss: -0.5899534225463867
Batch 12/64 loss: -0.735593318939209
Batch 13/64 loss: -0.647770881652832
Batch 14/64 loss: -0.3162193298339844
Batch 15/64 loss: -0.46817731857299805
Batch 16/64 loss: -0.38829708099365234
Batch 17/64 loss: -0.575829029083252
Batch 18/64 loss: -0.6577038764953613
Batch 19/64 loss: -0.3387765884399414
Batch 20/64 loss: -0.4665870666503906
Batch 21/64 loss: -0.6438274383544922
Batch 22/64 loss: 0.587897777557373
Batch 23/64 loss: -0.32720947265625
Batch 24/64 loss: -0.6743502616882324
Batch 25/64 loss: -0.15383291244506836
Batch 26/64 loss: -0.580986499786377
Batch 27/64 loss: -0.6776981353759766
Batch 28/64 loss: -0.3830742835998535
Batch 29/64 loss: -0.5533599853515625
Batch 30/64 loss: -0.40779781341552734
Batch 31/64 loss: -0.6848621368408203
Batch 32/64 loss: -0.3387489318847656
Batch 33/64 loss: -0.5588240623474121
Batch 34/64 loss: -0.5455031394958496
Batch 35/64 loss: -0.6120281219482422
Batch 36/64 loss: -0.6271443367004395
Batch 37/64 loss: 1.0120735168457031
Batch 38/64 loss: -0.33136796951293945
Batch 39/64 loss: -0.5708379745483398
Batch 40/64 loss: -0.1699204444885254
Batch 41/64 loss: 0.0869135856628418
Batch 42/64 loss: -0.5862560272216797
Batch 43/64 loss: -0.5932774543762207
Batch 44/64 loss: -0.6785540580749512
Batch 45/64 loss: -0.5315074920654297
Batch 46/64 loss: -0.6643290519714355
Batch 47/64 loss: -0.713101863861084
Batch 48/64 loss: -0.5217151641845703
Batch 49/64 loss: -0.005855560302734375
Batch 50/64 loss: -0.6031017303466797
Batch 51/64 loss: -0.7093524932861328
Batch 52/64 loss: -0.6269750595092773
Batch 53/64 loss: -0.7809443473815918
Batch 54/64 loss: -0.4885435104370117
Batch 55/64 loss: -0.41374921798706055
Batch 56/64 loss: -0.551661491394043
Batch 57/64 loss: -0.5720629692077637
Batch 58/64 loss: -0.615577220916748
Batch 59/64 loss: -0.1995258331298828
Batch 60/64 loss: -0.5591788291931152
Batch 61/64 loss: -0.6159710884094238
Batch 62/64 loss: -0.11478042602539062
Batch 63/64 loss: -0.45602846145629883
Batch 64/64 loss: -4.1965813636779785
Epoch 314  Train loss: -0.4736744955474255  Val loss: -0.8272602435239812
Epoch 315
-------------------------------
Batch 1/64 loss: -0.709808349609375
Batch 2/64 loss: -0.4440288543701172
Batch 3/64 loss: -0.6221451759338379
Batch 4/64 loss: -0.6205596923828125
Batch 5/64 loss: -0.48502254486083984
Batch 6/64 loss: -0.7464613914489746
Batch 7/64 loss: -0.34850597381591797
Batch 8/64 loss: -0.6825480461120605
Batch 9/64 loss: -0.4405221939086914
Batch 10/64 loss: -0.45737314224243164
Batch 11/64 loss: -0.5553369522094727
Batch 12/64 loss: -0.5352134704589844
Batch 13/64 loss: -0.3451504707336426
Batch 14/64 loss: -0.6929383277893066
Batch 15/64 loss: -0.4345698356628418
Batch 16/64 loss: -0.33851003646850586
Batch 17/64 loss: -0.40875244140625
Batch 18/64 loss: -0.254518985748291
Batch 19/64 loss: -0.45394086837768555
Batch 20/64 loss: -0.5946745872497559
Batch 21/64 loss: -0.43083667755126953
Batch 22/64 loss: -0.39780569076538086
Batch 23/64 loss: 1.542281150817871
Batch 24/64 loss: -0.36340761184692383
Batch 25/64 loss: -0.5431914329528809
Batch 26/64 loss: -0.5163388252258301
Batch 27/64 loss: -0.532172679901123
Batch 28/64 loss: -0.09086465835571289
Batch 29/64 loss: -0.584381103515625
Batch 30/64 loss: -0.3846921920776367
Batch 31/64 loss: -0.15623140335083008
Batch 32/64 loss: -0.6014070510864258
Batch 33/64 loss: -0.5940937995910645
Batch 34/64 loss: -0.6490731239318848
Batch 35/64 loss: -0.539557933807373
Batch 36/64 loss: -0.4771699905395508
Batch 37/64 loss: -0.6025352478027344
Batch 38/64 loss: 0.4118776321411133
Batch 39/64 loss: -0.48883628845214844
Batch 40/64 loss: -0.5879430770874023
Batch 41/64 loss: -0.6094112396240234
Batch 42/64 loss: -0.5705142021179199
Batch 43/64 loss: -0.6070880889892578
Batch 44/64 loss: -0.47647762298583984
Batch 45/64 loss: -0.336942195892334
Batch 46/64 loss: -0.4674696922302246
Batch 47/64 loss: -0.4752793312072754
Batch 48/64 loss: -0.412198543548584
Batch 49/64 loss: -0.5214571952819824
Batch 50/64 loss: -0.7082066535949707
Batch 51/64 loss: -0.6354985237121582
Batch 52/64 loss: -0.4761648178100586
Batch 53/64 loss: -0.648231029510498
Batch 54/64 loss: -0.2602686882019043
Batch 55/64 loss: -0.6599774360656738
Batch 56/64 loss: -0.40072202682495117
Batch 57/64 loss: -0.6823611259460449
Batch 58/64 loss: -0.32606983184814453
Batch 59/64 loss: 0.6272854804992676
Batch 60/64 loss: -0.7058820724487305
Batch 61/64 loss: -0.5394673347473145
Batch 62/64 loss: 0.23074626922607422
Batch 63/64 loss: -0.3217940330505371
Batch 64/64 loss: -4.219810962677002
Epoch 315  Train loss: -0.46910226672303446  Val loss: -0.7935447430692587
Epoch 316
-------------------------------
Batch 1/64 loss: -0.4544711112976074
Batch 2/64 loss: -0.09548187255859375
Batch 3/64 loss: -0.662078857421875
Batch 4/64 loss: -0.6416783332824707
Batch 5/64 loss: -0.7325363159179688
Batch 6/64 loss: -0.5161342620849609
Batch 7/64 loss: -0.6472992897033691
Batch 8/64 loss: -0.3429226875305176
Batch 9/64 loss: -0.6784682273864746
Batch 10/64 loss: -0.395965576171875
Batch 11/64 loss: -0.2888665199279785
Batch 12/64 loss: -0.6588053703308105
Batch 13/64 loss: -0.6266369819641113
Batch 14/64 loss: -0.6767244338989258
Batch 15/64 loss: 0.432619571685791
Batch 16/64 loss: -0.449094295501709
Batch 17/64 loss: 0.0866389274597168
Batch 18/64 loss: -0.6439208984375
Batch 19/64 loss: -0.7690253257751465
Batch 20/64 loss: 1.9067106246948242
Batch 21/64 loss: -0.6504569053649902
Batch 22/64 loss: -0.5606875419616699
Batch 23/64 loss: -0.5943026542663574
Batch 24/64 loss: -0.6291146278381348
Batch 25/64 loss: -0.45905065536499023
Batch 26/64 loss: -0.5175247192382812
Batch 27/64 loss: -0.44637203216552734
Batch 28/64 loss: -0.15286779403686523
Batch 29/64 loss: -0.5642037391662598
Batch 30/64 loss: -0.2430577278137207
Batch 31/64 loss: -0.6394758224487305
Batch 32/64 loss: -0.5208463668823242
Batch 33/64 loss: -0.18280553817749023
Batch 34/64 loss: -0.3413510322570801
Batch 35/64 loss: -0.5592508316040039
Batch 36/64 loss: -0.5254764556884766
Batch 37/64 loss: -0.652799129486084
Batch 38/64 loss: -0.4418954849243164
Batch 39/64 loss: -0.23936176300048828
Batch 40/64 loss: 0.05567455291748047
Batch 41/64 loss: 0.6052680015563965
Batch 42/64 loss: -0.47553062438964844
Batch 43/64 loss: -0.3886547088623047
Batch 44/64 loss: -0.44966554641723633
Batch 45/64 loss: -0.2221698760986328
Batch 46/64 loss: -0.34201478958129883
Batch 47/64 loss: -0.4219808578491211
Batch 48/64 loss: -0.34176111221313477
Batch 49/64 loss: -0.5121970176696777
Batch 50/64 loss: -0.2981891632080078
Batch 51/64 loss: -0.11869049072265625
Batch 52/64 loss: -0.6368112564086914
Batch 53/64 loss: -0.33879709243774414
Batch 54/64 loss: -0.528167724609375
Batch 55/64 loss: -0.3950047492980957
Batch 56/64 loss: -0.4380979537963867
Batch 57/64 loss: -0.5170869827270508
Batch 58/64 loss: -0.3697943687438965
Batch 59/64 loss: -0.4995460510253906
Batch 60/64 loss: -0.6127614974975586
Batch 61/64 loss: -0.6299433708190918
Batch 62/64 loss: -0.394744873046875
Batch 63/64 loss: -0.42909669876098633
Batch 64/64 loss: -4.1538472175598145
Epoch 316  Train loss: -0.43278732112809726  Val loss: -0.8128398226708481
Epoch 317
-------------------------------
Batch 1/64 loss: -0.5214071273803711
Batch 2/64 loss: -0.5608601570129395
Batch 3/64 loss: -0.48877620697021484
Batch 4/64 loss: -0.3741297721862793
Batch 5/64 loss: -0.44019556045532227
Batch 6/64 loss: -0.43938207626342773
Batch 7/64 loss: -0.6177535057067871
Batch 8/64 loss: -0.32433271408081055
Batch 9/64 loss: -0.46352243423461914
Batch 10/64 loss: -0.6204433441162109
Batch 11/64 loss: -0.6130642890930176
Batch 12/64 loss: -0.6332101821899414
Batch 13/64 loss: -0.23927783966064453
Batch 14/64 loss: -0.4438056945800781
Batch 15/64 loss: -0.13283538818359375
Batch 16/64 loss: -0.539940357208252
Batch 17/64 loss: -0.11092853546142578
Batch 18/64 loss: -0.3214240074157715
Batch 19/64 loss: -0.37581825256347656
Batch 20/64 loss: -0.3694272041320801
Batch 21/64 loss: -0.42562389373779297
Batch 22/64 loss: -0.5930380821228027
Batch 23/64 loss: -0.3756380081176758
Batch 24/64 loss: -0.6113910675048828
Batch 25/64 loss: -0.5637664794921875
Batch 26/64 loss: -0.16173076629638672
Batch 27/64 loss: -0.5292878150939941
Batch 28/64 loss: -0.6450352668762207
Batch 29/64 loss: -0.5563335418701172
Batch 30/64 loss: 0.3326125144958496
Batch 31/64 loss: -0.776641845703125
Batch 32/64 loss: 1.5142760276794434
Batch 33/64 loss: -0.6386942863464355
Batch 34/64 loss: -0.7528367042541504
Batch 35/64 loss: -0.7675161361694336
Batch 36/64 loss: -0.6648454666137695
Batch 37/64 loss: 0.6464295387268066
Batch 38/64 loss: -0.5083436965942383
Batch 39/64 loss: -0.5819015502929688
Batch 40/64 loss: -0.4120912551879883
Batch 41/64 loss: -0.26227664947509766
Batch 42/64 loss: -0.687962532043457
Batch 43/64 loss: -0.7310295104980469
Batch 44/64 loss: -0.7569112777709961
Batch 45/64 loss: -0.18478918075561523
Batch 46/64 loss: -0.45074939727783203
Batch 47/64 loss: -0.7074236869812012
Batch 48/64 loss: -0.2477869987487793
Batch 49/64 loss: -0.6435136795043945
Batch 50/64 loss: -0.7958173751831055
Batch 51/64 loss: -0.5923089981079102
Batch 52/64 loss: -0.6587791442871094
Batch 53/64 loss: -0.5692095756530762
Batch 54/64 loss: -0.4288907051086426
Batch 55/64 loss: -0.5587072372436523
Batch 56/64 loss: -0.7446346282958984
Batch 57/64 loss: -0.7692351341247559
Batch 58/64 loss: -0.6831064224243164
Batch 59/64 loss: -0.777040958404541
Batch 60/64 loss: -0.6277680397033691
Batch 61/64 loss: -0.4715576171875
Batch 62/64 loss: -0.7546849250793457
Batch 63/64 loss: -0.7012391090393066
Batch 64/64 loss: -4.188370704650879
Epoch 317  Train loss: -0.5121354233984854  Val loss: -0.8746747282362476
Epoch 318
-------------------------------
Batch 1/64 loss: -0.8232588768005371
Batch 2/64 loss: -0.6906027793884277
Batch 3/64 loss: -0.4359426498413086
Batch 4/64 loss: -0.5951042175292969
Batch 5/64 loss: -0.7936549186706543
Batch 6/64 loss: -0.7294058799743652
Batch 7/64 loss: -0.3070950508117676
Batch 8/64 loss: -0.7334017753601074
Batch 9/64 loss: -0.5754990577697754
Batch 10/64 loss: -0.6160073280334473
Batch 11/64 loss: -0.5572285652160645
Batch 12/64 loss: -0.6493287086486816
Batch 13/64 loss: -0.7160687446594238
Batch 14/64 loss: -0.7186965942382812
Batch 15/64 loss: -0.6370739936828613
Batch 16/64 loss: -0.6370320320129395
Batch 17/64 loss: -0.6286153793334961
Batch 18/64 loss: -0.668464183807373
Batch 19/64 loss: -0.7985329627990723
Batch 20/64 loss: -0.1842336654663086
Batch 21/64 loss: -0.4741950035095215
Batch 22/64 loss: -0.7775893211364746
Batch 23/64 loss: -0.40407752990722656
Batch 24/64 loss: -0.6509013175964355
Batch 25/64 loss: -0.5284252166748047
Batch 26/64 loss: -0.5823068618774414
Batch 27/64 loss: -0.6430706977844238
Batch 28/64 loss: -0.6215004920959473
Batch 29/64 loss: -0.6421213150024414
Batch 30/64 loss: -0.7561631202697754
Batch 31/64 loss: -0.023691177368164062
Batch 32/64 loss: -0.5924005508422852
Batch 33/64 loss: -0.29535818099975586
Batch 34/64 loss: -0.5633664131164551
Batch 35/64 loss: -0.6247682571411133
Batch 36/64 loss: -0.7025260925292969
Batch 37/64 loss: -0.5525498390197754
Batch 38/64 loss: 0.8765783309936523
Batch 39/64 loss: -0.45902156829833984
Batch 40/64 loss: -0.7268953323364258
Batch 41/64 loss: -0.7628989219665527
Batch 42/64 loss: -0.5840282440185547
Batch 43/64 loss: -0.6207928657531738
Batch 44/64 loss: 0.2985849380493164
Batch 45/64 loss: -0.6044535636901855
Batch 46/64 loss: -0.3424854278564453
Batch 47/64 loss: -0.8365497589111328
Batch 48/64 loss: -0.5342674255371094
Batch 49/64 loss: -0.41323184967041016
Batch 50/64 loss: -0.8383045196533203
Batch 51/64 loss: -0.7210965156555176
Batch 52/64 loss: -0.7849626541137695
Batch 53/64 loss: -0.7076530456542969
Batch 54/64 loss: -0.6340146064758301
Batch 55/64 loss: 0.4690084457397461
Batch 56/64 loss: -0.6039657592773438
Batch 57/64 loss: -0.7357997894287109
Batch 58/64 loss: -0.5885677337646484
Batch 59/64 loss: -0.6580519676208496
Batch 60/64 loss: -0.6303653717041016
Batch 61/64 loss: -0.5797233581542969
Batch 62/64 loss: -0.7728958129882812
Batch 63/64 loss: -0.7149896621704102
Batch 64/64 loss: -3.652866840362549
Epoch 318  Train loss: -0.5942078889585009  Val loss: -0.9594927260146517
Epoch 319
-------------------------------
Batch 1/64 loss: 0.9928221702575684
Batch 2/64 loss: -0.6344447135925293
Batch 3/64 loss: -0.31627511978149414
Batch 4/64 loss: -0.6816902160644531
Batch 5/64 loss: 0.7462120056152344
Batch 6/64 loss: -0.6888003349304199
Batch 7/64 loss: -0.6595077514648438
Batch 8/64 loss: -0.7596697807312012
Batch 9/64 loss: -0.5491290092468262
Batch 10/64 loss: -0.72369384765625
Batch 11/64 loss: -0.6952629089355469
Batch 12/64 loss: -0.5405106544494629
Batch 13/64 loss: -0.5734720230102539
Batch 14/64 loss: -0.6055765151977539
Batch 15/64 loss: -0.7274928092956543
Batch 16/64 loss: -0.6332249641418457
Batch 17/64 loss: -0.35262441635131836
Batch 18/64 loss: -0.6712327003479004
Batch 19/64 loss: -0.5090179443359375
Batch 20/64 loss: -0.7723960876464844
Batch 21/64 loss: -0.6549324989318848
Batch 22/64 loss: -0.542393684387207
Batch 23/64 loss: -0.5968904495239258
Batch 24/64 loss: 1.1299195289611816
Batch 25/64 loss: -0.6230020523071289
Batch 26/64 loss: -0.6647639274597168
Batch 27/64 loss: -0.5081362724304199
Batch 28/64 loss: -0.7298626899719238
Batch 29/64 loss: -0.6849522590637207
Batch 30/64 loss: -0.7029004096984863
Batch 31/64 loss: -0.7622256278991699
Batch 32/64 loss: -0.21416139602661133
Batch 33/64 loss: -0.6901159286499023
Batch 34/64 loss: -0.7901492118835449
Batch 35/64 loss: -0.8301668167114258
Batch 36/64 loss: -0.6920661926269531
Batch 37/64 loss: -0.7387685775756836
Batch 38/64 loss: -0.8320903778076172
Batch 39/64 loss: -0.549781322479248
Batch 40/64 loss: -0.4558730125427246
Batch 41/64 loss: -0.8265380859375
Batch 42/64 loss: -0.8052425384521484
Batch 43/64 loss: -0.6754093170166016
Batch 44/64 loss: -0.6458549499511719
Batch 45/64 loss: -0.3185997009277344
Batch 46/64 loss: -0.6996350288391113
Batch 47/64 loss: -0.5633974075317383
Batch 48/64 loss: -0.7203145027160645
Batch 49/64 loss: -0.7675418853759766
Batch 50/64 loss: -0.8137001991271973
Batch 51/64 loss: -0.42130184173583984
Batch 52/64 loss: -0.5626626014709473
Batch 53/64 loss: -0.769930362701416
Batch 54/64 loss: -0.5754776000976562
Batch 55/64 loss: -0.7406973838806152
Batch 56/64 loss: -0.6720952987670898
Batch 57/64 loss: -0.7441554069519043
Batch 58/64 loss: -0.5648198127746582
Batch 59/64 loss: -0.7144460678100586
Batch 60/64 loss: -0.8298168182373047
Batch 61/64 loss: -0.7367634773254395
Batch 62/64 loss: -0.7882494926452637
Batch 63/64 loss: -0.7979846000671387
Batch 64/64 loss: -4.291711330413818
Epoch 319  Train loss: -0.6190073518192067  Val loss: -0.952217429773914
Epoch 320
-------------------------------
Batch 1/64 loss: -0.6601762771606445
Batch 2/64 loss: -0.6945557594299316
Batch 3/64 loss: -0.6882505416870117
Batch 4/64 loss: -0.819643497467041
Batch 5/64 loss: -0.6576275825500488
Batch 6/64 loss: -0.5708017349243164
Batch 7/64 loss: -0.4635586738586426
Batch 8/64 loss: -0.5138020515441895
Batch 9/64 loss: -0.6745710372924805
Batch 10/64 loss: -0.8062372207641602
Batch 11/64 loss: -0.6808223724365234
Batch 12/64 loss: -0.6723260879516602
Batch 13/64 loss: -0.7734236717224121
Batch 14/64 loss: -0.5611567497253418
Batch 15/64 loss: -0.6281895637512207
Batch 16/64 loss: -0.6304874420166016
Batch 17/64 loss: -0.6922769546508789
Batch 18/64 loss: -0.4178295135498047
Batch 19/64 loss: -0.5988426208496094
Batch 20/64 loss: -0.612459659576416
Batch 21/64 loss: -0.7348299026489258
Batch 22/64 loss: -0.44455480575561523
Batch 23/64 loss: -0.6894187927246094
Batch 24/64 loss: -0.6084160804748535
Batch 25/64 loss: -0.6197395324707031
Batch 26/64 loss: -0.3574485778808594
Batch 27/64 loss: -0.566861629486084
Batch 28/64 loss: -0.7290802001953125
Batch 29/64 loss: -0.6459174156188965
Batch 30/64 loss: -0.6093478202819824
Batch 31/64 loss: 0.09367561340332031
Batch 32/64 loss: -0.44681644439697266
Batch 33/64 loss: -0.6635584831237793
Batch 34/64 loss: 0.613131046295166
Batch 35/64 loss: -0.5923323631286621
Batch 36/64 loss: -0.5400395393371582
Batch 37/64 loss: -0.7292752265930176
Batch 38/64 loss: 0.3127408027648926
Batch 39/64 loss: -0.5293097496032715
Batch 40/64 loss: -0.5451173782348633
Batch 41/64 loss: -0.16407203674316406
Batch 42/64 loss: -0.5812325477600098
Batch 43/64 loss: -0.13193893432617188
Batch 44/64 loss: -0.09987115859985352
Batch 45/64 loss: -0.6211071014404297
Batch 46/64 loss: -0.5595216751098633
Batch 47/64 loss: -0.5043349266052246
Batch 48/64 loss: -0.5051355361938477
Batch 49/64 loss: -0.5586662292480469
Batch 50/64 loss: -0.6710371971130371
Batch 51/64 loss: -0.643150806427002
Batch 52/64 loss: -0.5579795837402344
Batch 53/64 loss: -0.6534304618835449
Batch 54/64 loss: -0.7860321998596191
Batch 55/64 loss: -0.7366576194763184
Batch 56/64 loss: -0.40935230255126953
Batch 57/64 loss: -0.5548973083496094
Batch 58/64 loss: -0.6390509605407715
Batch 59/64 loss: -0.6882820129394531
Batch 60/64 loss: -0.5434250831604004
Batch 61/64 loss: -0.15261268615722656
Batch 62/64 loss: 1.1858901977539062
Batch 63/64 loss: -0.6032562255859375
Batch 64/64 loss: -4.3763933181762695
Epoch 320  Train loss: -0.553898115719066  Val loss: -0.8153128607576245
Epoch 321
-------------------------------
Batch 1/64 loss: -0.7674078941345215
Batch 2/64 loss: -0.714745044708252
Batch 3/64 loss: -0.43152809143066406
Batch 4/64 loss: -0.4741401672363281
Batch 5/64 loss: -0.32262706756591797
Batch 6/64 loss: -0.1387310028076172
Batch 7/64 loss: -0.6393084526062012
Batch 8/64 loss: -0.5730538368225098
Batch 9/64 loss: -0.4658012390136719
Batch 10/64 loss: -0.33244943618774414
Batch 11/64 loss: -0.4313335418701172
Batch 12/64 loss: -0.5776677131652832
Batch 13/64 loss: -0.5190844535827637
Batch 14/64 loss: -0.35462331771850586
Batch 15/64 loss: -0.693915843963623
Batch 16/64 loss: -0.6342496871948242
Batch 17/64 loss: -0.5120854377746582
Batch 18/64 loss: -0.5694127082824707
Batch 19/64 loss: 1.8554139137268066
Batch 20/64 loss: -0.6816158294677734
Batch 21/64 loss: -0.1934218406677246
Batch 22/64 loss: -0.7752914428710938
Batch 23/64 loss: -0.5823187828063965
Batch 24/64 loss: -0.6671657562255859
Batch 25/64 loss: -0.3986029624938965
Batch 26/64 loss: -0.4425816535949707
Batch 27/64 loss: -0.44179582595825195
Batch 28/64 loss: -0.4223461151123047
Batch 29/64 loss: -0.4092888832092285
Batch 30/64 loss: -0.7377009391784668
Batch 31/64 loss: -0.5085620880126953
Batch 32/64 loss: -0.3991279602050781
Batch 33/64 loss: -0.19211959838867188
Batch 34/64 loss: -0.479891300201416
Batch 35/64 loss: -0.3537435531616211
Batch 36/64 loss: -0.4994807243347168
Batch 37/64 loss: -0.4526643753051758
Batch 38/64 loss: -0.48494958877563477
Batch 39/64 loss: -0.4199404716491699
Batch 40/64 loss: -0.43417787551879883
Batch 41/64 loss: -0.4889383316040039
Batch 42/64 loss: -0.48135852813720703
Batch 43/64 loss: 0.015049934387207031
Batch 44/64 loss: -0.628997802734375
Batch 45/64 loss: -0.47071313858032227
Batch 46/64 loss: -0.582697868347168
Batch 47/64 loss: 0.27442407608032227
Batch 48/64 loss: -0.6428384780883789
Batch 49/64 loss: -0.4671511650085449
Batch 50/64 loss: -0.3119325637817383
Batch 51/64 loss: 0.5286145210266113
Batch 52/64 loss: -0.1734766960144043
Batch 53/64 loss: -0.6472225189208984
Batch 54/64 loss: -0.5621161460876465
Batch 55/64 loss: -0.5390076637268066
Batch 56/64 loss: -0.5578608512878418
Batch 57/64 loss: -0.47092390060424805
Batch 58/64 loss: -0.584681510925293
Batch 59/64 loss: -0.5642657279968262
Batch 60/64 loss: -0.5402112007141113
Batch 61/64 loss: -0.5413942337036133
Batch 62/64 loss: -0.6002840995788574
Batch 63/64 loss: -0.34403276443481445
Batch 64/64 loss: -3.506584644317627
Epoch 321  Train loss: -0.4593802938274309  Val loss: -0.8235432995144034
Epoch 322
-------------------------------
Batch 1/64 loss: -0.35387611389160156
Batch 2/64 loss: -0.6228938102722168
Batch 3/64 loss: -0.23434686660766602
Batch 4/64 loss: -0.6409320831298828
Batch 5/64 loss: -0.5302591323852539
Batch 6/64 loss: -0.6352934837341309
Batch 7/64 loss: -0.6316056251525879
Batch 8/64 loss: -0.5204381942749023
Batch 9/64 loss: -0.2417469024658203
Batch 10/64 loss: -0.569493293762207
Batch 11/64 loss: -0.5469837188720703
Batch 12/64 loss: -0.49973440170288086
Batch 13/64 loss: -0.47255802154541016
Batch 14/64 loss: -0.48613786697387695
Batch 15/64 loss: -0.5043954849243164
Batch 16/64 loss: -0.29327392578125
Batch 17/64 loss: -0.5195388793945312
Batch 18/64 loss: -0.41071224212646484
Batch 19/64 loss: -0.4781785011291504
Batch 20/64 loss: -0.6260232925415039
Batch 21/64 loss: -0.5671892166137695
Batch 22/64 loss: -0.3266139030456543
Batch 23/64 loss: -0.4691286087036133
Batch 24/64 loss: -0.16401958465576172
Batch 25/64 loss: 0.2937164306640625
Batch 26/64 loss: -0.5057296752929688
Batch 27/64 loss: -0.7156791687011719
Batch 28/64 loss: -0.5300807952880859
Batch 29/64 loss: -0.5920681953430176
Batch 30/64 loss: -0.5676999092102051
Batch 31/64 loss: -0.6361045837402344
Batch 32/64 loss: -0.5288882255554199
Batch 33/64 loss: 0.6240253448486328
Batch 34/64 loss: -0.558067798614502
Batch 35/64 loss: -0.29626035690307617
Batch 36/64 loss: -0.5944843292236328
Batch 37/64 loss: -0.6132197380065918
Batch 38/64 loss: -0.5548806190490723
Batch 39/64 loss: 0.44658565521240234
Batch 40/64 loss: -0.3167233467102051
Batch 41/64 loss: -0.5589885711669922
Batch 42/64 loss: -0.5759491920471191
Batch 43/64 loss: -0.5130796432495117
Batch 44/64 loss: -0.5297489166259766
Batch 45/64 loss: -0.5981807708740234
Batch 46/64 loss: -0.5246415138244629
Batch 47/64 loss: -0.5569829940795898
Batch 48/64 loss: -0.48447704315185547
Batch 49/64 loss: 1.429922103881836
Batch 50/64 loss: -0.08020210266113281
Batch 51/64 loss: -0.6229338645935059
Batch 52/64 loss: -0.4751138687133789
Batch 53/64 loss: -0.7050457000732422
Batch 54/64 loss: -0.4992227554321289
Batch 55/64 loss: -0.5607943534851074
Batch 56/64 loss: -0.45998191833496094
Batch 57/64 loss: -0.5407896041870117
Batch 58/64 loss: -0.5089836120605469
Batch 59/64 loss: -0.6679601669311523
Batch 60/64 loss: -0.616300106048584
Batch 61/64 loss: -0.5391755104064941
Batch 62/64 loss: -0.6229395866394043
Batch 63/64 loss: -0.5794072151184082
Batch 64/64 loss: -4.216064453125
Epoch 322  Train loss: -0.4791209052590763  Val loss: -0.8321471328997531
Epoch 323
-------------------------------
Batch 1/64 loss: -0.5080676078796387
Batch 2/64 loss: -0.5427126884460449
Batch 3/64 loss: -0.6514058113098145
Batch 4/64 loss: -0.7331838607788086
Batch 5/64 loss: -0.7832779884338379
Batch 6/64 loss: -0.6820573806762695
Batch 7/64 loss: -0.584681510925293
Batch 8/64 loss: -0.645143985748291
Batch 9/64 loss: -0.35724830627441406
Batch 10/64 loss: -0.48532962799072266
Batch 11/64 loss: -0.454160213470459
Batch 12/64 loss: -0.6216230392456055
Batch 13/64 loss: -0.6132192611694336
Batch 14/64 loss: -0.6518468856811523
Batch 15/64 loss: -0.5799942016601562
Batch 16/64 loss: -0.5571436882019043
Batch 17/64 loss: -0.32532787322998047
Batch 18/64 loss: -0.5398139953613281
Batch 19/64 loss: -0.41664838790893555
Batch 20/64 loss: 0.1705331802368164
Batch 21/64 loss: -0.5059413909912109
Batch 22/64 loss: -0.5356616973876953
Batch 23/64 loss: -0.594942569732666
Batch 24/64 loss: -0.8042168617248535
Batch 25/64 loss: 0.17253637313842773
Batch 26/64 loss: -0.09398984909057617
Batch 27/64 loss: -0.6650185585021973
Batch 28/64 loss: -0.4805564880371094
Batch 29/64 loss: -0.6042509078979492
Batch 30/64 loss: -0.6442904472351074
Batch 31/64 loss: -0.7124099731445312
Batch 32/64 loss: 0.5244054794311523
Batch 33/64 loss: -0.3396625518798828
Batch 34/64 loss: -0.5044069290161133
Batch 35/64 loss: -0.40191030502319336
Batch 36/64 loss: -0.6332650184631348
Batch 37/64 loss: -0.7201757431030273
Batch 38/64 loss: -0.22636032104492188
Batch 39/64 loss: -0.5603208541870117
Batch 40/64 loss: -0.3573017120361328
Batch 41/64 loss: 0.34046030044555664
Batch 42/64 loss: 0.9127840995788574
Batch 43/64 loss: -0.6003623008728027
Batch 44/64 loss: -0.5979986190795898
Batch 45/64 loss: -0.48813629150390625
Batch 46/64 loss: -0.24406099319458008
Batch 47/64 loss: -0.5074276924133301
Batch 48/64 loss: -0.607452392578125
Batch 49/64 loss: -0.3802938461303711
Batch 50/64 loss: -0.5815820693969727
Batch 51/64 loss: -0.6432442665100098
Batch 52/64 loss: -0.5725555419921875
Batch 53/64 loss: -0.6414971351623535
Batch 54/64 loss: -0.5332536697387695
Batch 55/64 loss: -0.5126309394836426
Batch 56/64 loss: 0.004668712615966797
Batch 57/64 loss: -0.6492557525634766
Batch 58/64 loss: -0.7632651329040527
Batch 59/64 loss: -0.7029728889465332
Batch 60/64 loss: -0.6366534233093262
Batch 61/64 loss: -0.6443877220153809
Batch 62/64 loss: -0.6130990982055664
Batch 63/64 loss: -0.5699400901794434
Batch 64/64 loss: -4.195857048034668
Epoch 323  Train loss: -0.511829694112142  Val loss: -0.859857303580058
Epoch 324
-------------------------------
Batch 1/64 loss: -0.48947668075561523
Batch 2/64 loss: -0.057921409606933594
Batch 3/64 loss: -0.5238690376281738
Batch 4/64 loss: -0.5774588584899902
Batch 5/64 loss: -0.6998729705810547
Batch 6/64 loss: -0.5415267944335938
Batch 7/64 loss: -0.7210860252380371
Batch 8/64 loss: 0.9103202819824219
Batch 9/64 loss: -0.5461058616638184
Batch 10/64 loss: -0.5910582542419434
Batch 11/64 loss: -0.7111797332763672
Batch 12/64 loss: -0.47812891006469727
Batch 13/64 loss: -0.5646824836730957
Batch 14/64 loss: -0.7400403022766113
Batch 15/64 loss: -0.5662574768066406
Batch 16/64 loss: -0.308046817779541
Batch 17/64 loss: -0.5934586524963379
Batch 18/64 loss: -0.5087766647338867
Batch 19/64 loss: -0.48093748092651367
Batch 20/64 loss: -0.4159846305847168
Batch 21/64 loss: -0.5539107322692871
Batch 22/64 loss: -0.6501331329345703
Batch 23/64 loss: -0.571317195892334
Batch 24/64 loss: -0.5604677200317383
Batch 25/64 loss: -0.7608704566955566
Batch 26/64 loss: -0.516026496887207
Batch 27/64 loss: -0.6599531173706055
Batch 28/64 loss: -0.5583858489990234
Batch 29/64 loss: -0.3890109062194824
Batch 30/64 loss: -0.6329965591430664
Batch 31/64 loss: -0.17376708984375
Batch 32/64 loss: -0.2733430862426758
Batch 33/64 loss: -0.44851016998291016
Batch 34/64 loss: -0.5750255584716797
Batch 35/64 loss: -0.5206894874572754
Batch 36/64 loss: -0.5357732772827148
Batch 37/64 loss: -0.719933032989502
Batch 38/64 loss: -0.8362116813659668
Batch 39/64 loss: -0.7117271423339844
Batch 40/64 loss: -0.2684135437011719
Batch 41/64 loss: 1.0442399978637695
Batch 42/64 loss: -0.45451879501342773
Batch 43/64 loss: -0.631960391998291
Batch 44/64 loss: -0.6932063102722168
Batch 45/64 loss: -0.4566531181335449
Batch 46/64 loss: -0.7243013381958008
Batch 47/64 loss: -0.7093958854675293
Batch 48/64 loss: -0.6086015701293945
Batch 49/64 loss: -0.6818108558654785
Batch 50/64 loss: -0.4230613708496094
Batch 51/64 loss: -0.5817899703979492
Batch 52/64 loss: 0.9440531730651855
Batch 53/64 loss: -0.4279494285583496
Batch 54/64 loss: -0.5685954093933105
Batch 55/64 loss: -0.541224479675293
Batch 56/64 loss: -0.7309179306030273
Batch 57/64 loss: -0.46004343032836914
Batch 58/64 loss: -0.16292142868041992
Batch 59/64 loss: -0.13839006423950195
Batch 60/64 loss: -0.4037332534790039
Batch 61/64 loss: -0.6092038154602051
Batch 62/64 loss: -0.6674065589904785
Batch 63/64 loss: -0.23959827423095703
Batch 64/64 loss: -3.137984275817871
Epoch 324  Train loss: -0.4925881367103726  Val loss: -0.714606176946581
Epoch 325
-------------------------------
Batch 1/64 loss: -0.5340957641601562
Batch 2/64 loss: -0.5489950180053711
Batch 3/64 loss: -0.5081086158752441
Batch 4/64 loss: -0.49941062927246094
Batch 5/64 loss: -0.6780352592468262
Batch 6/64 loss: -0.667717456817627
Batch 7/64 loss: -0.659212589263916
Batch 8/64 loss: -0.04462766647338867
Batch 9/64 loss: -0.5083050727844238
Batch 10/64 loss: 0.0464930534362793
Batch 11/64 loss: -0.7499856948852539
Batch 12/64 loss: -0.7200274467468262
Batch 13/64 loss: -0.609349250793457
Batch 14/64 loss: -0.42126989364624023
Batch 15/64 loss: -0.4630303382873535
Batch 16/64 loss: -0.6194310188293457
Batch 17/64 loss: -0.5254912376403809
Batch 18/64 loss: -0.6118535995483398
Batch 19/64 loss: -0.4402604103088379
Batch 20/64 loss: -0.46086931228637695
Batch 21/64 loss: -0.5063767433166504
Batch 22/64 loss: -0.4590873718261719
Batch 23/64 loss: 0.8120765686035156
Batch 24/64 loss: -0.7242040634155273
Batch 25/64 loss: -0.6692209243774414
Batch 26/64 loss: -0.18120718002319336
Batch 27/64 loss: -0.6703281402587891
Batch 28/64 loss: -0.5658926963806152
Batch 29/64 loss: -0.4631505012512207
Batch 30/64 loss: -0.6871771812438965
Batch 31/64 loss: -0.654050350189209
Batch 32/64 loss: -0.7052206993103027
Batch 33/64 loss: -0.6250853538513184
Batch 34/64 loss: -0.7207674980163574
Batch 35/64 loss: -0.33263301849365234
Batch 36/64 loss: -0.6023964881896973
Batch 37/64 loss: -0.510591983795166
Batch 38/64 loss: -0.4755725860595703
Batch 39/64 loss: 0.4695558547973633
Batch 40/64 loss: -0.6220970153808594
Batch 41/64 loss: -0.33309125900268555
Batch 42/64 loss: -0.5732474327087402
Batch 43/64 loss: -0.5291657447814941
Batch 44/64 loss: -0.29328012466430664
Batch 45/64 loss: -0.5858321189880371
Batch 46/64 loss: -0.6129331588745117
Batch 47/64 loss: -0.5795807838439941
Batch 48/64 loss: -0.6236767768859863
Batch 49/64 loss: -0.5035858154296875
Batch 50/64 loss: 0.3634810447692871
Batch 51/64 loss: -0.4257025718688965
Batch 52/64 loss: -0.5037527084350586
Batch 53/64 loss: -0.7851119041442871
Batch 54/64 loss: -0.876591682434082
Batch 55/64 loss: -0.7018380165100098
Batch 56/64 loss: 0.615570068359375
Batch 57/64 loss: -0.7547802925109863
Batch 58/64 loss: -0.6518115997314453
Batch 59/64 loss: -0.7190442085266113
Batch 60/64 loss: -0.7941875457763672
Batch 61/64 loss: -0.6373758316040039
Batch 62/64 loss: -0.6706085205078125
Batch 63/64 loss: -0.5454015731811523
Batch 64/64 loss: -4.333520889282227
Epoch 325  Train loss: -0.534724703022078  Val loss: -0.9303485831034553
Epoch 326
-------------------------------
Batch 1/64 loss: -0.4073505401611328
Batch 2/64 loss: -0.6584005355834961
Batch 3/64 loss: -0.2750706672668457
Batch 4/64 loss: -0.4416680335998535
Batch 5/64 loss: -0.6228694915771484
Batch 6/64 loss: -0.6736855506896973
Batch 7/64 loss: -0.5324873924255371
Batch 8/64 loss: -0.5473270416259766
Batch 9/64 loss: -0.7036943435668945
Batch 10/64 loss: -0.6595377922058105
Batch 11/64 loss: -0.6797652244567871
Batch 12/64 loss: -0.7066612243652344
Batch 13/64 loss: -0.4296450614929199
Batch 14/64 loss: -0.5457544326782227
Batch 15/64 loss: -0.7936763763427734
Batch 16/64 loss: -0.5127649307250977
Batch 17/64 loss: -0.4640345573425293
Batch 18/64 loss: -0.7426743507385254
Batch 19/64 loss: -0.6884875297546387
Batch 20/64 loss: -0.7310729026794434
Batch 21/64 loss: -0.6060714721679688
Batch 22/64 loss: -0.6705751419067383
Batch 23/64 loss: -0.49654626846313477
Batch 24/64 loss: -0.5224332809448242
Batch 25/64 loss: -0.5983662605285645
Batch 26/64 loss: -0.5301332473754883
Batch 27/64 loss: -0.6774706840515137
Batch 28/64 loss: -0.4984159469604492
Batch 29/64 loss: -0.605224609375
Batch 30/64 loss: 0.47231245040893555
Batch 31/64 loss: -0.6397686004638672
Batch 32/64 loss: -0.1014699935913086
Batch 33/64 loss: -0.6114697456359863
Batch 34/64 loss: -0.5183515548706055
Batch 35/64 loss: -0.4920024871826172
Batch 36/64 loss: -0.6773629188537598
Batch 37/64 loss: -0.3712162971496582
Batch 38/64 loss: -0.5392146110534668
Batch 39/64 loss: -0.6411223411560059
Batch 40/64 loss: -0.4441657066345215
Batch 41/64 loss: -0.65435791015625
Batch 42/64 loss: -0.6788539886474609
Batch 43/64 loss: -0.642486572265625
Batch 44/64 loss: 0.51214599609375
Batch 45/64 loss: -0.5807647705078125
Batch 46/64 loss: -0.6838927268981934
Batch 47/64 loss: -0.6547794342041016
Batch 48/64 loss: -0.6923069953918457
Batch 49/64 loss: -0.710421085357666
Batch 50/64 loss: -0.6755814552307129
Batch 51/64 loss: -0.4658212661743164
Batch 52/64 loss: -0.7366986274719238
Batch 53/64 loss: -0.5997323989868164
Batch 54/64 loss: -0.6301746368408203
Batch 55/64 loss: -0.05750274658203125
Batch 56/64 loss: -0.7953577041625977
Batch 57/64 loss: -0.6650815010070801
Batch 58/64 loss: -0.7338976860046387
Batch 59/64 loss: -0.1796250343322754
Batch 60/64 loss: 0.8990678787231445
Batch 61/64 loss: -0.6775431632995605
Batch 62/64 loss: -0.4962635040283203
Batch 63/64 loss: -0.5262885093688965
Batch 64/64 loss: -3.6227169036865234
Epoch 326  Train loss: -0.5557482700721891  Val loss: -0.944967591066131
Epoch 327
-------------------------------
Batch 1/64 loss: -0.7495570182800293
Batch 2/64 loss: -0.7371931076049805
Batch 3/64 loss: -0.2295370101928711
Batch 4/64 loss: -0.6617403030395508
Batch 5/64 loss: -0.6416115760803223
Batch 6/64 loss: -0.5553750991821289
Batch 7/64 loss: 0.808809757232666
Batch 8/64 loss: -0.6216764450073242
Batch 9/64 loss: -0.5903401374816895
Batch 10/64 loss: -0.592806339263916
Batch 11/64 loss: -0.30704689025878906
Batch 12/64 loss: -0.5716428756713867
Batch 13/64 loss: -0.5939493179321289
Batch 14/64 loss: -0.6382312774658203
Batch 15/64 loss: -0.3346438407897949
Batch 16/64 loss: -0.5184106826782227
Batch 17/64 loss: -0.5288758277893066
Batch 18/64 loss: -0.799685001373291
Batch 19/64 loss: -0.5484223365783691
Batch 20/64 loss: 0.5625848770141602
Batch 21/64 loss: -0.5640950202941895
Batch 22/64 loss: -0.5960402488708496
Batch 23/64 loss: -0.7196187973022461
Batch 24/64 loss: -0.5820693969726562
Batch 25/64 loss: -0.5704512596130371
Batch 26/64 loss: -0.6907706260681152
Batch 27/64 loss: -0.4250063896179199
Batch 28/64 loss: -0.760796070098877
Batch 29/64 loss: -0.3376030921936035
Batch 30/64 loss: -0.5351190567016602
Batch 31/64 loss: -0.6013383865356445
Batch 32/64 loss: -0.49630117416381836
Batch 33/64 loss: -0.33498096466064453
Batch 34/64 loss: -0.6684961318969727
Batch 35/64 loss: -0.5915255546569824
Batch 36/64 loss: -0.684455394744873
Batch 37/64 loss: -0.4961576461791992
Batch 38/64 loss: -0.6050033569335938
Batch 39/64 loss: -0.5844254493713379
Batch 40/64 loss: -0.38843297958374023
Batch 41/64 loss: -0.5326991081237793
Batch 42/64 loss: -0.5982952117919922
Batch 43/64 loss: 0.07704782485961914
Batch 44/64 loss: -0.5291166305541992
Batch 45/64 loss: -0.31096553802490234
Batch 46/64 loss: -0.28058767318725586
Batch 47/64 loss: -0.012337684631347656
Batch 48/64 loss: -0.7164196968078613
Batch 49/64 loss: -0.7507357597351074
Batch 50/64 loss: -0.6068544387817383
Batch 51/64 loss: -0.5940084457397461
Batch 52/64 loss: -0.6080098152160645
Batch 53/64 loss: -0.280977725982666
Batch 54/64 loss: -0.6571521759033203
Batch 55/64 loss: 0.5372028350830078
Batch 56/64 loss: -0.4442133903503418
Batch 57/64 loss: -0.5281534194946289
Batch 58/64 loss: -0.6573643684387207
Batch 59/64 loss: -0.6263947486877441
Batch 60/64 loss: -0.6351709365844727
Batch 61/64 loss: -0.7044615745544434
Batch 62/64 loss: -0.5249929428100586
Batch 63/64 loss: -0.5283064842224121
Batch 64/64 loss: -4.2011003494262695
Epoch 327  Train loss: -0.5293463501275755  Val loss: -0.8270408656588945
Epoch 328
-------------------------------
Batch 1/64 loss: -0.5614004135131836
Batch 2/64 loss: -0.6234054565429688
Batch 3/64 loss: -0.6286067962646484
Batch 4/64 loss: -0.6243329048156738
Batch 5/64 loss: -0.33034229278564453
Batch 6/64 loss: -0.6277880668640137
Batch 7/64 loss: 0.514155387878418
Batch 8/64 loss: -0.46427297592163086
Batch 9/64 loss: -0.5039429664611816
Batch 10/64 loss: -0.47846317291259766
Batch 11/64 loss: -0.7681350708007812
Batch 12/64 loss: -0.5650582313537598
Batch 13/64 loss: -0.6921734809875488
Batch 14/64 loss: 0.06954813003540039
Batch 15/64 loss: -0.588986873626709
Batch 16/64 loss: -0.524144172668457
Batch 17/64 loss: 0.4476933479309082
Batch 18/64 loss: -0.14521265029907227
Batch 19/64 loss: -0.6838092803955078
Batch 20/64 loss: -0.20435237884521484
Batch 21/64 loss: -0.7364063262939453
Batch 22/64 loss: -0.4361405372619629
Batch 23/64 loss: -0.20261049270629883
Batch 24/64 loss: -0.26549291610717773
Batch 25/64 loss: -0.7317399978637695
Batch 26/64 loss: -0.6275773048400879
Batch 27/64 loss: -0.3746938705444336
Batch 28/64 loss: -0.5649313926696777
Batch 29/64 loss: -0.4423694610595703
Batch 30/64 loss: -0.3997526168823242
Batch 31/64 loss: -0.5171389579772949
Batch 32/64 loss: -0.6658568382263184
Batch 33/64 loss: -0.3005051612854004
Batch 34/64 loss: -0.5588488578796387
Batch 35/64 loss: -0.3346524238586426
Batch 36/64 loss: -0.47884321212768555
Batch 37/64 loss: -0.261171817779541
Batch 38/64 loss: -0.35181283950805664
Batch 39/64 loss: -0.5317482948303223
Batch 40/64 loss: -0.743802547454834
Batch 41/64 loss: -0.4625415802001953
Batch 42/64 loss: -0.5153646469116211
Batch 43/64 loss: -0.6640830039978027
Batch 44/64 loss: 0.43314170837402344
Batch 45/64 loss: -0.5635170936584473
Batch 46/64 loss: -0.3928852081298828
Batch 47/64 loss: -0.1422739028930664
Batch 48/64 loss: -0.5777144432067871
Batch 49/64 loss: -0.49231624603271484
Batch 50/64 loss: -0.052277565002441406
Batch 51/64 loss: -0.054212093353271484
Batch 52/64 loss: -0.35181665420532227
Batch 53/64 loss: -0.49227046966552734
Batch 54/64 loss: -0.10231733322143555
Batch 55/64 loss: -0.6453156471252441
Batch 56/64 loss: -0.37389135360717773
Batch 57/64 loss: -0.5100598335266113
Batch 58/64 loss: -0.5414004325866699
Batch 59/64 loss: -0.2708735466003418
Batch 60/64 loss: -0.47336339950561523
Batch 61/64 loss: -0.3545260429382324
Batch 62/64 loss: 0.6546497344970703
Batch 63/64 loss: -0.23479366302490234
Batch 64/64 loss: -1.969740867614746
Epoch 328  Train loss: -0.41045418907614317  Val loss: -0.6625121467301935
Epoch 329
-------------------------------
Batch 1/64 loss: 0.9159636497497559
Batch 2/64 loss: -0.597379207611084
Batch 3/64 loss: -0.32702064514160156
Batch 4/64 loss: -0.45426130294799805
Batch 5/64 loss: -0.60400390625
Batch 6/64 loss: -0.5587596893310547
Batch 7/64 loss: -0.5421614646911621
Batch 8/64 loss: -0.47077417373657227
Batch 9/64 loss: -0.31009817123413086
Batch 10/64 loss: -0.32016754150390625
Batch 11/64 loss: -0.43650150299072266
Batch 12/64 loss: -0.6124801635742188
Batch 13/64 loss: -0.3078737258911133
Batch 14/64 loss: -0.38543176651000977
Batch 15/64 loss: -0.4724235534667969
Batch 16/64 loss: -0.3079242706298828
Batch 17/64 loss: -0.3859367370605469
Batch 18/64 loss: 1.1419744491577148
Batch 19/64 loss: -0.5568442344665527
Batch 20/64 loss: 0.07494926452636719
Batch 21/64 loss: -0.45489931106567383
Batch 22/64 loss: -0.5616641044616699
Batch 23/64 loss: -0.33450841903686523
Batch 24/64 loss: -0.02038860321044922
Batch 25/64 loss: -0.09923982620239258
Batch 26/64 loss: -0.6436467170715332
Batch 27/64 loss: -0.2867922782897949
Batch 28/64 loss: -0.2797260284423828
Batch 29/64 loss: -0.3389601707458496
Batch 30/64 loss: -0.40370845794677734
Batch 31/64 loss: -0.3357563018798828
Batch 32/64 loss: -0.022895336151123047
Batch 33/64 loss: -0.5685520172119141
Batch 34/64 loss: -0.5858535766601562
Batch 35/64 loss: -0.5316452980041504
Batch 36/64 loss: 0.11089801788330078
Batch 37/64 loss: -0.09551429748535156
Batch 38/64 loss: -0.49864864349365234
Batch 39/64 loss: -0.5937213897705078
Batch 40/64 loss: -0.4746079444885254
Batch 41/64 loss: -0.4816584587097168
Batch 42/64 loss: -0.30115413665771484
Batch 43/64 loss: -0.593970775604248
Batch 44/64 loss: -0.45296525955200195
Batch 45/64 loss: -0.41643428802490234
Batch 46/64 loss: -0.3643918037414551
Batch 47/64 loss: 0.015937328338623047
Batch 48/64 loss: -0.5319790840148926
Batch 49/64 loss: -0.48201704025268555
Batch 50/64 loss: -0.38213014602661133
Batch 51/64 loss: -0.28926801681518555
Batch 52/64 loss: -0.4792203903198242
Batch 53/64 loss: -0.513512134552002
Batch 54/64 loss: -0.3136410713195801
Batch 55/64 loss: -0.4298677444458008
Batch 56/64 loss: 0.09061670303344727
Batch 57/64 loss: -0.24640226364135742
Batch 58/64 loss: -0.4829225540161133
Batch 59/64 loss: -0.5433516502380371
Batch 60/64 loss: -0.5421586036682129
Batch 61/64 loss: 0.4182405471801758
Batch 62/64 loss: -0.4226975440979004
Batch 63/64 loss: -0.5496258735656738
Batch 64/64 loss: -3.993171215057373
Epoch 329  Train loss: -0.37374804814656576  Val loss: -0.6355843167124745
Epoch 330
-------------------------------
Batch 1/64 loss: -0.32794952392578125
Batch 2/64 loss: -0.05724811553955078
Batch 3/64 loss: -0.5155806541442871
Batch 4/64 loss: -0.596036434173584
Batch 5/64 loss: -0.47499799728393555
Batch 6/64 loss: -0.16039752960205078
Batch 7/64 loss: -0.4683103561401367
Batch 8/64 loss: -0.42310380935668945
Batch 9/64 loss: -0.36496496200561523
Batch 10/64 loss: -0.4264082908630371
Batch 11/64 loss: -0.2621440887451172
Batch 12/64 loss: -0.43644142150878906
Batch 13/64 loss: -0.4034390449523926
Batch 14/64 loss: -0.4562821388244629
Batch 15/64 loss: 0.1588139533996582
Batch 16/64 loss: 0.978278636932373
Batch 17/64 loss: -0.4949159622192383
Batch 18/64 loss: -0.2709941864013672
Batch 19/64 loss: 0.9542546272277832
Batch 20/64 loss: -0.4877510070800781
Batch 21/64 loss: -0.5175213813781738
Batch 22/64 loss: -0.6528406143188477
Batch 23/64 loss: -0.3364114761352539
Batch 24/64 loss: -0.5299129486083984
Batch 25/64 loss: -0.39960193634033203
Batch 26/64 loss: -0.39884233474731445
Batch 27/64 loss: -0.6033191680908203
Batch 28/64 loss: -0.37308692932128906
Batch 29/64 loss: -0.38776683807373047
Batch 30/64 loss: -0.5476260185241699
Batch 31/64 loss: 0.055338382720947266
Batch 32/64 loss: -0.1729135513305664
Batch 33/64 loss: -0.35811281204223633
Batch 34/64 loss: -0.271486759185791
Batch 35/64 loss: -0.5674562454223633
Batch 36/64 loss: -0.5150775909423828
Batch 37/64 loss: -0.511436939239502
Batch 38/64 loss: -0.5927162170410156
Batch 39/64 loss: -0.6697268486022949
Batch 40/64 loss: -0.507265567779541
Batch 41/64 loss: -0.0023632049560546875
Batch 42/64 loss: -0.47059011459350586
Batch 43/64 loss: -0.5787129402160645
Batch 44/64 loss: -0.40638065338134766
Batch 45/64 loss: -0.45124053955078125
Batch 46/64 loss: -0.40082216262817383
Batch 47/64 loss: 1.2458720207214355
Batch 48/64 loss: -0.5092310905456543
Batch 49/64 loss: -0.4738302230834961
Batch 50/64 loss: -0.6370844841003418
Batch 51/64 loss: -0.5498862266540527
Batch 52/64 loss: -0.6590557098388672
Batch 53/64 loss: -0.008371829986572266
Batch 54/64 loss: 0.06453418731689453
Batch 55/64 loss: -0.6845183372497559
Batch 56/64 loss: -0.4539761543273926
Batch 57/64 loss: -0.5978498458862305
Batch 58/64 loss: -0.4513392448425293
Batch 59/64 loss: -0.38843297958374023
Batch 60/64 loss: -0.6109681129455566
Batch 61/64 loss: -0.6081018447875977
Batch 62/64 loss: -0.43183422088623047
Batch 63/64 loss: -0.6365232467651367
Batch 64/64 loss: -4.087850093841553
Epoch 330  Train loss: -0.39466661378449086  Val loss: -0.7399286748617375
Epoch 331
-------------------------------
Batch 1/64 loss: -0.6078815460205078
Batch 2/64 loss: -0.2105541229248047
Batch 3/64 loss: -0.15158319473266602
Batch 4/64 loss: -0.39638423919677734
Batch 5/64 loss: -0.43747949600219727
Batch 6/64 loss: -0.45441198348999023
Batch 7/64 loss: -0.608039379119873
Batch 8/64 loss: -0.26348114013671875
Batch 9/64 loss: -0.39264631271362305
Batch 10/64 loss: -0.10839033126831055
Batch 11/64 loss: -0.4550142288208008
Batch 12/64 loss: 0.23767375946044922
Batch 13/64 loss: -0.5809535980224609
Batch 14/64 loss: -0.629326343536377
Batch 15/64 loss: -0.5735845565795898
Batch 16/64 loss: -0.4545125961303711
Batch 17/64 loss: -0.4419546127319336
Batch 18/64 loss: -0.4866204261779785
Batch 19/64 loss: -0.5005836486816406
Batch 20/64 loss: -0.6685614585876465
Batch 21/64 loss: -0.4716300964355469
Batch 22/64 loss: -0.4284391403198242
Batch 23/64 loss: -0.6286582946777344
Batch 24/64 loss: -0.46284961700439453
Batch 25/64 loss: -0.5328011512756348
Batch 26/64 loss: 1.065873146057129
Batch 27/64 loss: -0.624544620513916
Batch 28/64 loss: -0.37782955169677734
Batch 29/64 loss: 0.9413924217224121
Batch 30/64 loss: -0.41997766494750977
Batch 31/64 loss: -0.6204872131347656
Batch 32/64 loss: -0.3535304069519043
Batch 33/64 loss: -0.44771337509155273
Batch 34/64 loss: -0.5991497039794922
Batch 35/64 loss: -0.5277538299560547
Batch 36/64 loss: -0.35184717178344727
Batch 37/64 loss: -0.047875404357910156
Batch 38/64 loss: 0.03646707534790039
Batch 39/64 loss: -0.4940214157104492
Batch 40/64 loss: -0.6461162567138672
Batch 41/64 loss: -0.2115311622619629
Batch 42/64 loss: -0.5156583786010742
Batch 43/64 loss: -0.4365100860595703
Batch 44/64 loss: -0.2553572654724121
Batch 45/64 loss: -0.1515369415283203
Batch 46/64 loss: -0.08350086212158203
Batch 47/64 loss: 0.226715087890625
Batch 48/64 loss: -0.4719395637512207
Batch 49/64 loss: -0.3206291198730469
Batch 50/64 loss: 0.0731210708618164
Batch 51/64 loss: -0.4497842788696289
Batch 52/64 loss: -0.3647470474243164
Batch 53/64 loss: -0.14561939239501953
Batch 54/64 loss: -0.44034528732299805
Batch 55/64 loss: -0.21956491470336914
Batch 56/64 loss: 0.6512188911437988
Batch 57/64 loss: 0.15876245498657227
Batch 58/64 loss: -0.4161801338195801
Batch 59/64 loss: -0.42248106002807617
Batch 60/64 loss: -0.364166259765625
Batch 61/64 loss: -0.5324640274047852
Batch 62/64 loss: -0.49561452865600586
Batch 63/64 loss: -0.3220834732055664
Batch 64/64 loss: -4.188342571258545
Epoch 331  Train loss: -0.3580695675868614  Val loss: -0.7579978077682024
Epoch 332
-------------------------------
Batch 1/64 loss: 0.34061384201049805
Batch 2/64 loss: -0.2126612663269043
Batch 3/64 loss: -0.3704643249511719
Batch 4/64 loss: -0.3697991371154785
Batch 5/64 loss: 0.1405620574951172
Batch 6/64 loss: -0.5201764106750488
Batch 7/64 loss: -0.04854106903076172
Batch 8/64 loss: -0.2765231132507324
Batch 9/64 loss: -0.3470792770385742
Batch 10/64 loss: -0.17853736877441406
Batch 11/64 loss: -0.3489985466003418
Batch 12/64 loss: -0.6085548400878906
Batch 13/64 loss: -0.4265308380126953
Batch 14/64 loss: -0.16809463500976562
Batch 15/64 loss: -0.37454843521118164
Batch 16/64 loss: -0.43936634063720703
Batch 17/64 loss: 0.6460003852844238
Batch 18/64 loss: -0.5168380737304688
Batch 19/64 loss: -0.5566048622131348
Batch 20/64 loss: -0.28816652297973633
Batch 21/64 loss: -0.5429911613464355
Batch 22/64 loss: -0.4233512878417969
Batch 23/64 loss: -0.20822477340698242
Batch 24/64 loss: -0.4035019874572754
Batch 25/64 loss: -0.4493236541748047
Batch 26/64 loss: -0.40402841567993164
Batch 27/64 loss: -0.47169971466064453
Batch 28/64 loss: -0.4500412940979004
Batch 29/64 loss: 0.07654285430908203
Batch 30/64 loss: -0.25280284881591797
Batch 31/64 loss: -0.4063839912414551
Batch 32/64 loss: 0.39789915084838867
Batch 33/64 loss: -0.40040063858032227
Batch 34/64 loss: -0.4177889823913574
Batch 35/64 loss: -0.4853024482727051
Batch 36/64 loss: -0.3610243797302246
Batch 37/64 loss: -0.6907839775085449
Batch 38/64 loss: -0.3338179588317871
Batch 39/64 loss: -0.6222629547119141
Batch 40/64 loss: -0.556206226348877
Batch 41/64 loss: -0.535858154296875
Batch 42/64 loss: 0.09687948226928711
Batch 43/64 loss: -0.5061020851135254
Batch 44/64 loss: -0.5700993537902832
Batch 45/64 loss: -0.42312145233154297
Batch 46/64 loss: -0.5500078201293945
Batch 47/64 loss: -0.14880609512329102
Batch 48/64 loss: -0.5157675743103027
Batch 49/64 loss: -0.5998115539550781
Batch 50/64 loss: -0.6668410301208496
Batch 51/64 loss: -0.6588311195373535
Batch 52/64 loss: -0.6370506286621094
Batch 53/64 loss: -0.4505629539489746
Batch 54/64 loss: -0.6173176765441895
Batch 55/64 loss: -0.5708479881286621
Batch 56/64 loss: -0.37607622146606445
Batch 57/64 loss: -0.5152192115783691
Batch 58/64 loss: -0.5834846496582031
Batch 59/64 loss: -0.2917203903198242
Batch 60/64 loss: -0.5518879890441895
Batch 61/64 loss: -0.32839488983154297
Batch 62/64 loss: 0.885159969329834
Batch 63/64 loss: -0.6724262237548828
Batch 64/64 loss: -4.147439956665039
Epoch 332  Train loss: -0.39574241638183594  Val loss: -0.8508870364054781
Epoch 333
-------------------------------
Batch 1/64 loss: -0.7047514915466309
Batch 2/64 loss: -0.6237688064575195
Batch 3/64 loss: 0.6251792907714844
Batch 4/64 loss: -0.6765604019165039
Batch 5/64 loss: -0.4596538543701172
Batch 6/64 loss: -0.4707164764404297
Batch 7/64 loss: -0.3102293014526367
Batch 8/64 loss: -0.3432645797729492
Batch 9/64 loss: -0.4729752540588379
Batch 10/64 loss: -0.6002144813537598
Batch 11/64 loss: -0.45357179641723633
Batch 12/64 loss: -0.5473208427429199
Batch 13/64 loss: -0.664365291595459
Batch 14/64 loss: -0.35808801651000977
Batch 15/64 loss: -0.522181510925293
Batch 16/64 loss: -0.5088357925415039
Batch 17/64 loss: -0.5346531867980957
Batch 18/64 loss: -0.30681467056274414
Batch 19/64 loss: -0.49836111068725586
Batch 20/64 loss: -0.592984676361084
Batch 21/64 loss: -0.5489797592163086
Batch 22/64 loss: -0.7105655670166016
Batch 23/64 loss: -0.5734615325927734
Batch 24/64 loss: -0.5053815841674805
Batch 25/64 loss: -0.48325061798095703
Batch 26/64 loss: -0.6774477958679199
Batch 27/64 loss: -0.4103226661682129
Batch 28/64 loss: -0.6524896621704102
Batch 29/64 loss: -0.44878101348876953
Batch 30/64 loss: -0.6458125114440918
Batch 31/64 loss: -0.7360315322875977
Batch 32/64 loss: 1.4714288711547852
Batch 33/64 loss: -0.7114620208740234
Batch 34/64 loss: -0.4646186828613281
Batch 35/64 loss: -0.5235958099365234
Batch 36/64 loss: -0.34482240676879883
Batch 37/64 loss: -0.27831506729125977
Batch 38/64 loss: -0.568021297454834
Batch 39/64 loss: -0.8078055381774902
Batch 40/64 loss: -0.29210758209228516
Batch 41/64 loss: -0.6051616668701172
Batch 42/64 loss: -0.6384296417236328
Batch 43/64 loss: -0.09697961807250977
Batch 44/64 loss: -0.7108664512634277
Batch 45/64 loss: 0.39092111587524414
Batch 46/64 loss: -0.6700234413146973
Batch 47/64 loss: -0.7349758148193359
Batch 48/64 loss: 0.6530900001525879
Batch 49/64 loss: -0.6041760444641113
Batch 50/64 loss: -0.5622363090515137
Batch 51/64 loss: -0.5772228240966797
Batch 52/64 loss: -0.43465662002563477
Batch 53/64 loss: -0.6278033256530762
Batch 54/64 loss: -0.34475231170654297
Batch 55/64 loss: -0.6693649291992188
Batch 56/64 loss: -0.554903507232666
Batch 57/64 loss: -0.640871524810791
Batch 58/64 loss: -0.4976925849914551
Batch 59/64 loss: -0.5822052955627441
Batch 60/64 loss: -0.5474247932434082
Batch 61/64 loss: -0.7386064529418945
Batch 62/64 loss: -0.16252470016479492
Batch 63/64 loss: -0.4443235397338867
Batch 64/64 loss: -4.1268630027771
Epoch 333  Train loss: -0.49305589900297275  Val loss: -0.8572064953571332
Epoch 334
-------------------------------
Batch 1/64 loss: 0.9812932014465332
Batch 2/64 loss: -0.23339223861694336
Batch 3/64 loss: -0.48944711685180664
Batch 4/64 loss: -0.6725749969482422
Batch 5/64 loss: -0.6038236618041992
Batch 6/64 loss: -0.6279935836791992
Batch 7/64 loss: -0.5617494583129883
Batch 8/64 loss: -0.663020133972168
Batch 9/64 loss: -0.40993642807006836
Batch 10/64 loss: -0.6778740882873535
Batch 11/64 loss: -0.23725080490112305
Batch 12/64 loss: 0.8140668869018555
Batch 13/64 loss: -0.3997945785522461
Batch 14/64 loss: -0.2047438621520996
Batch 15/64 loss: -0.03774547576904297
Batch 16/64 loss: -0.5297136306762695
Batch 17/64 loss: -0.5146188735961914
Batch 18/64 loss: -0.5347952842712402
Batch 19/64 loss: -0.6982216835021973
Batch 20/64 loss: -0.40994977951049805
Batch 21/64 loss: -0.30665016174316406
Batch 22/64 loss: -0.5242905616760254
Batch 23/64 loss: -0.6172337532043457
Batch 24/64 loss: -0.5130376815795898
Batch 25/64 loss: -0.5089192390441895
Batch 26/64 loss: 0.4231405258178711
Batch 27/64 loss: -0.44365739822387695
Batch 28/64 loss: -0.4720950126647949
Batch 29/64 loss: -0.4490785598754883
Batch 30/64 loss: -0.475649356842041
Batch 31/64 loss: -0.48980283737182617
Batch 32/64 loss: -0.5497298240661621
Batch 33/64 loss: -0.6976633071899414
Batch 34/64 loss: -0.3368082046508789
Batch 35/64 loss: -0.41772937774658203
Batch 36/64 loss: -0.35646677017211914
Batch 37/64 loss: -0.7188515663146973
Batch 38/64 loss: -0.4909238815307617
Batch 39/64 loss: -0.50579833984375
Batch 40/64 loss: -0.7037444114685059
Batch 41/64 loss: -0.6912961006164551
Batch 42/64 loss: -0.695101261138916
Batch 43/64 loss: -0.5907831192016602
Batch 44/64 loss: -0.5401544570922852
Batch 45/64 loss: -0.5356359481811523
Batch 46/64 loss: -0.4153270721435547
Batch 47/64 loss: -0.5525069236755371
Batch 48/64 loss: -0.37298583984375
Batch 49/64 loss: -0.6954612731933594
Batch 50/64 loss: -0.6815128326416016
Batch 51/64 loss: -0.5944967269897461
Batch 52/64 loss: -0.1693882942199707
Batch 53/64 loss: -0.5127592086791992
Batch 54/64 loss: -0.10088348388671875
Batch 55/64 loss: 0.5898447036743164
Batch 56/64 loss: -0.548037052154541
Batch 57/64 loss: -0.4984707832336426
Batch 58/64 loss: -0.7479228973388672
Batch 59/64 loss: -0.26963329315185547
Batch 60/64 loss: -0.6837983131408691
Batch 61/64 loss: -0.7760624885559082
Batch 62/64 loss: -0.6617884635925293
Batch 63/64 loss: -0.6228199005126953
Batch 64/64 loss: -4.289302349090576
Epoch 334  Train loss: -0.4776507153230555  Val loss: -0.8051453685432775
Epoch 335
-------------------------------
Batch 1/64 loss: -0.6830744743347168
Batch 2/64 loss: -0.3201942443847656
Batch 3/64 loss: -0.6271977424621582
Batch 4/64 loss: -0.47742366790771484
Batch 5/64 loss: -0.3376498222351074
Batch 6/64 loss: -0.677269458770752
Batch 7/64 loss: -0.46158266067504883
Batch 8/64 loss: -0.3591752052307129
Batch 9/64 loss: -0.46739768981933594
Batch 10/64 loss: -0.7622265815734863
Batch 11/64 loss: -0.2344217300415039
Batch 12/64 loss: -0.7146182060241699
Batch 13/64 loss: -0.6193671226501465
Batch 14/64 loss: -0.451779842376709
Batch 15/64 loss: -0.5770525932312012
Batch 16/64 loss: -0.3116016387939453
Batch 17/64 loss: -0.7170743942260742
Batch 18/64 loss: -0.2898378372192383
Batch 19/64 loss: 0.5223207473754883
Batch 20/64 loss: -0.7046875953674316
Batch 21/64 loss: -0.4639925956726074
Batch 22/64 loss: -0.579078197479248
Batch 23/64 loss: -0.42299509048461914
Batch 24/64 loss: -0.5405611991882324
Batch 25/64 loss: -0.5389089584350586
Batch 26/64 loss: -0.8195834159851074
Batch 27/64 loss: -0.45151519775390625
Batch 28/64 loss: -0.6046366691589355
Batch 29/64 loss: -0.6173233985900879
Batch 30/64 loss: -0.4706735610961914
Batch 31/64 loss: -0.5282020568847656
Batch 32/64 loss: -0.5587739944458008
Batch 33/64 loss: -0.5489797592163086
Batch 34/64 loss: -0.4832158088684082
Batch 35/64 loss: -0.03843259811401367
Batch 36/64 loss: -0.7023444175720215
Batch 37/64 loss: -0.5694198608398438
Batch 38/64 loss: -0.6818027496337891
Batch 39/64 loss: -0.7700085639953613
Batch 40/64 loss: -0.5499649047851562
Batch 41/64 loss: -0.5451459884643555
Batch 42/64 loss: -0.24044466018676758
Batch 43/64 loss: -0.6986932754516602
Batch 44/64 loss: -0.5700359344482422
Batch 45/64 loss: -0.5452818870544434
Batch 46/64 loss: -0.6552534103393555
Batch 47/64 loss: -0.12364864349365234
Batch 48/64 loss: -0.5626010894775391
Batch 49/64 loss: -0.23097944259643555
Batch 50/64 loss: 0.015326499938964844
Batch 51/64 loss: -0.6930828094482422
Batch 52/64 loss: 0.06387519836425781
Batch 53/64 loss: 0.9064970016479492
Batch 54/64 loss: -0.5425052642822266
Batch 55/64 loss: -0.5432047843933105
Batch 56/64 loss: 0.26648759841918945
Batch 57/64 loss: -0.5195794105529785
Batch 58/64 loss: -0.5044026374816895
Batch 59/64 loss: -0.5169095993041992
Batch 60/64 loss: -0.5591044425964355
Batch 61/64 loss: -0.7810540199279785
Batch 62/64 loss: -0.8061108589172363
Batch 63/64 loss: -0.5277490615844727
Batch 64/64 loss: -4.182667255401611
Epoch 335  Train loss: -0.5060757038640041  Val loss: -0.8463171994972885
Epoch 336
-------------------------------
Batch 1/64 loss: -0.6203098297119141
Batch 2/64 loss: -0.6944718360900879
Batch 3/64 loss: -0.33699941635131836
Batch 4/64 loss: -0.45383214950561523
Batch 5/64 loss: -0.5514020919799805
Batch 6/64 loss: -0.7139377593994141
Batch 7/64 loss: -0.6401662826538086
Batch 8/64 loss: -0.7676644325256348
Batch 9/64 loss: -0.5461020469665527
Batch 10/64 loss: -0.466763973236084
Batch 11/64 loss: -0.721834659576416
Batch 12/64 loss: -0.6944975852966309
Batch 13/64 loss: -0.558135986328125
Batch 14/64 loss: -0.49307966232299805
Batch 15/64 loss: -0.6035356521606445
Batch 16/64 loss: -0.7240972518920898
Batch 17/64 loss: -0.4703097343444824
Batch 18/64 loss: -0.6395020484924316
Batch 19/64 loss: -0.8074584007263184
Batch 20/64 loss: -0.47144031524658203
Batch 21/64 loss: -0.8170924186706543
Batch 22/64 loss: -0.4329032897949219
Batch 23/64 loss: -0.28076171875
Batch 24/64 loss: -0.7118358612060547
Batch 25/64 loss: -0.3884310722351074
Batch 26/64 loss: -0.5593852996826172
Batch 27/64 loss: 0.01355886459350586
Batch 28/64 loss: 0.37838029861450195
Batch 29/64 loss: -0.783294677734375
Batch 30/64 loss: -0.7440915107727051
Batch 31/64 loss: -0.6144323348999023
Batch 32/64 loss: -0.39192819595336914
Batch 33/64 loss: -0.4200019836425781
Batch 34/64 loss: -0.5526189804077148
Batch 35/64 loss: -0.6327571868896484
Batch 36/64 loss: -0.6556525230407715
Batch 37/64 loss: -0.6196651458740234
Batch 38/64 loss: -0.6746311187744141
Batch 39/64 loss: -0.7052125930786133
Batch 40/64 loss: -0.6438026428222656
Batch 41/64 loss: 0.7275829315185547
Batch 42/64 loss: -0.28670263290405273
Batch 43/64 loss: -0.03199338912963867
Batch 44/64 loss: -0.7123270034790039
Batch 45/64 loss: 0.6634693145751953
Batch 46/64 loss: -0.8179240226745605
Batch 47/64 loss: -0.6816997528076172
Batch 48/64 loss: -0.48815393447875977
Batch 49/64 loss: -0.17098045349121094
Batch 50/64 loss: -0.3531060218811035
Batch 51/64 loss: -0.660670280456543
Batch 52/64 loss: -0.5961685180664062
Batch 53/64 loss: -0.7945189476013184
Batch 54/64 loss: -0.48039674758911133
Batch 55/64 loss: -0.4481692314147949
Batch 56/64 loss: -0.40086793899536133
Batch 57/64 loss: -0.3621225357055664
Batch 58/64 loss: -0.6636066436767578
Batch 59/64 loss: -0.5981435775756836
Batch 60/64 loss: -0.7585835456848145
Batch 61/64 loss: 0.0162353515625
Batch 62/64 loss: -0.5731425285339355
Batch 63/64 loss: -0.5485358238220215
Batch 64/64 loss: -4.282270908355713
Epoch 336  Train loss: -0.5403032097161985  Val loss: -0.8567656684167606
Epoch 337
-------------------------------
Batch 1/64 loss: -0.4153175354003906
Batch 2/64 loss: -0.5155081748962402
Batch 3/64 loss: -0.6456747055053711
Batch 4/64 loss: -0.6611032485961914
Batch 5/64 loss: -0.8061470985412598
Batch 6/64 loss: -0.7144289016723633
Batch 7/64 loss: -0.750145435333252
Batch 8/64 loss: 0.7380485534667969
Batch 9/64 loss: 1.0345382690429688
Batch 10/64 loss: -0.7285194396972656
Batch 11/64 loss: -0.7239990234375
Batch 12/64 loss: -0.45373010635375977
Batch 13/64 loss: -0.5639457702636719
Batch 14/64 loss: -0.5351758003234863
Batch 15/64 loss: -0.6836657524108887
Batch 16/64 loss: -0.6532187461853027
Batch 17/64 loss: -0.5297751426696777
Batch 18/64 loss: -0.676727294921875
Batch 19/64 loss: -0.5815315246582031
Batch 20/64 loss: -0.338348388671875
Batch 21/64 loss: -0.7639064788818359
Batch 22/64 loss: -0.545966625213623
Batch 23/64 loss: -0.7860803604125977
Batch 24/64 loss: -0.21700620651245117
Batch 25/64 loss: -0.30964183807373047
Batch 26/64 loss: -0.4341259002685547
Batch 27/64 loss: -0.7031521797180176
Batch 28/64 loss: -0.4603266716003418
Batch 29/64 loss: -0.45673513412475586
Batch 30/64 loss: 0.0034880638122558594
Batch 31/64 loss: -0.7099146842956543
Batch 32/64 loss: -0.4667477607727051
Batch 33/64 loss: -0.4503507614135742
Batch 34/64 loss: -0.49153804779052734
Batch 35/64 loss: -0.25855064392089844
Batch 36/64 loss: -0.4231910705566406
Batch 37/64 loss: -0.292478084564209
Batch 38/64 loss: -0.3479471206665039
Batch 39/64 loss: -0.6308107376098633
Batch 40/64 loss: -0.3649110794067383
Batch 41/64 loss: -0.555717945098877
Batch 42/64 loss: -0.6780233383178711
Batch 43/64 loss: -0.6615824699401855
Batch 44/64 loss: 0.5439372062683105
Batch 45/64 loss: -0.5114588737487793
Batch 46/64 loss: -0.6186380386352539
Batch 47/64 loss: -0.7618312835693359
Batch 48/64 loss: -0.6545605659484863
Batch 49/64 loss: -0.6936211585998535
Batch 50/64 loss: -0.5990405082702637
Batch 51/64 loss: -0.48378515243530273
Batch 52/64 loss: -0.5933966636657715
Batch 53/64 loss: -0.4412069320678711
Batch 54/64 loss: 0.1703181266784668
Batch 55/64 loss: -0.3058433532714844
Batch 56/64 loss: -0.5187993049621582
Batch 57/64 loss: -0.4232358932495117
Batch 58/64 loss: -0.5502309799194336
Batch 59/64 loss: -0.504737377166748
Batch 60/64 loss: 0.02307605743408203
Batch 61/64 loss: -0.6955180168151855
Batch 62/64 loss: -0.6064810752868652
Batch 63/64 loss: -0.4633474349975586
Batch 64/64 loss: -3.986215114593506
Epoch 337  Train loss: -0.5001985082439347  Val loss: -0.9136122870691044
Epoch 338
-------------------------------
Batch 1/64 loss: -0.6289477348327637
Batch 2/64 loss: -0.7409887313842773
Batch 3/64 loss: 1.0409116744995117
Batch 4/64 loss: -0.6705927848815918
Batch 5/64 loss: -0.5422430038452148
Batch 6/64 loss: -0.715456485748291
Batch 7/64 loss: -0.7962193489074707
Batch 8/64 loss: -0.7988033294677734
Batch 9/64 loss: -0.6503424644470215
Batch 10/64 loss: -0.5461792945861816
Batch 11/64 loss: -0.5057172775268555
Batch 12/64 loss: -0.6927981376647949
Batch 13/64 loss: -0.5516476631164551
Batch 14/64 loss: -0.45816469192504883
Batch 15/64 loss: -0.6282415390014648
Batch 16/64 loss: -0.48874998092651367
Batch 17/64 loss: -0.3895430564880371
Batch 18/64 loss: -0.4842095375061035
Batch 19/64 loss: -0.45531272888183594
Batch 20/64 loss: -0.7204170227050781
Batch 21/64 loss: -0.7022256851196289
Batch 22/64 loss: 0.8576698303222656
Batch 23/64 loss: -0.5057563781738281
Batch 24/64 loss: -0.6172046661376953
Batch 25/64 loss: -0.5969810485839844
Batch 26/64 loss: -0.7047443389892578
Batch 27/64 loss: -0.7211971282958984
Batch 28/64 loss: -0.3455514907836914
Batch 29/64 loss: -0.47365331649780273
Batch 30/64 loss: -0.5440225601196289
Batch 31/64 loss: -0.6819629669189453
Batch 32/64 loss: -0.7499661445617676
Batch 33/64 loss: -0.5745506286621094
Batch 34/64 loss: -0.7203669548034668
Batch 35/64 loss: -0.602745532989502
Batch 36/64 loss: -0.6252799034118652
Batch 37/64 loss: -0.6738090515136719
Batch 38/64 loss: -0.7383089065551758
Batch 39/64 loss: -0.7371878623962402
Batch 40/64 loss: -0.5238041877746582
Batch 41/64 loss: -0.7818641662597656
Batch 42/64 loss: -0.4024662971496582
Batch 43/64 loss: -0.4594860076904297
Batch 44/64 loss: -0.37711381912231445
Batch 45/64 loss: -0.50982666015625
Batch 46/64 loss: -0.6572127342224121
Batch 47/64 loss: -0.6362338066101074
Batch 48/64 loss: -0.5356254577636719
Batch 49/64 loss: 0.46866703033447266
Batch 50/64 loss: -0.7902188301086426
Batch 51/64 loss: -0.5147323608398438
Batch 52/64 loss: -0.05791330337524414
Batch 53/64 loss: -0.6639294624328613
Batch 54/64 loss: -0.20209741592407227
Batch 55/64 loss: -0.6300878524780273
Batch 56/64 loss: -0.7760882377624512
Batch 57/64 loss: -0.7163820266723633
Batch 58/64 loss: -0.7556343078613281
Batch 59/64 loss: -0.514519214630127
Batch 60/64 loss: -0.5833334922790527
Batch 61/64 loss: 0.15732097625732422
Batch 62/64 loss: -0.5584664344787598
Batch 63/64 loss: -0.6597590446472168
Batch 64/64 loss: -4.053009986877441
Epoch 338  Train loss: -0.5584638819975012  Val loss: -0.8037649987079843
Epoch 339
-------------------------------
Batch 1/64 loss: -0.7219133377075195
Batch 2/64 loss: 0.7083501815795898
Batch 3/64 loss: 0.7126564979553223
Batch 4/64 loss: -0.12007761001586914
Batch 5/64 loss: -0.318082332611084
Batch 6/64 loss: -0.41448354721069336
Batch 7/64 loss: -0.7005720138549805
Batch 8/64 loss: -0.47332000732421875
Batch 9/64 loss: -0.7064309120178223
Batch 10/64 loss: -0.7106914520263672
Batch 11/64 loss: -0.685889720916748
Batch 12/64 loss: -0.5837368965148926
Batch 13/64 loss: -0.5432281494140625
Batch 14/64 loss: -0.2171154022216797
Batch 15/64 loss: -0.6352524757385254
Batch 16/64 loss: -0.4241046905517578
Batch 17/64 loss: -0.5947623252868652
Batch 18/64 loss: -0.31534337997436523
Batch 19/64 loss: -0.5333409309387207
Batch 20/64 loss: -0.5803728103637695
Batch 21/64 loss: -0.7180562019348145
Batch 22/64 loss: -0.5355677604675293
Batch 23/64 loss: -0.574620246887207
Batch 24/64 loss: -0.44054651260375977
Batch 25/64 loss: -0.7252407073974609
Batch 26/64 loss: -0.4184408187866211
Batch 27/64 loss: -0.10471105575561523
Batch 28/64 loss: -0.23450374603271484
Batch 29/64 loss: -0.5660243034362793
Batch 30/64 loss: -0.6406264305114746
Batch 31/64 loss: -0.6602077484130859
Batch 32/64 loss: -0.33312416076660156
Batch 33/64 loss: -0.3801565170288086
Batch 34/64 loss: -0.703606128692627
Batch 35/64 loss: -0.570641040802002
Batch 36/64 loss: -0.5794868469238281
Batch 37/64 loss: -0.3337364196777344
Batch 38/64 loss: -0.49695682525634766
Batch 39/64 loss: -0.6451525688171387
Batch 40/64 loss: -0.4356398582458496
Batch 41/64 loss: -0.7125606536865234
Batch 42/64 loss: -0.7447419166564941
Batch 43/64 loss: -0.6539921760559082
Batch 44/64 loss: -0.6139621734619141
Batch 45/64 loss: -0.5150260925292969
Batch 46/64 loss: 0.29296445846557617
Batch 47/64 loss: -0.6411828994750977
Batch 48/64 loss: -0.7501707077026367
Batch 49/64 loss: -0.6070375442504883
Batch 50/64 loss: -0.6257433891296387
Batch 51/64 loss: -0.5770797729492188
Batch 52/64 loss: -0.7043604850769043
Batch 53/64 loss: -0.6114330291748047
Batch 54/64 loss: -0.6681571006774902
Batch 55/64 loss: -0.30069541931152344
Batch 56/64 loss: -0.7403316497802734
Batch 57/64 loss: -0.4082326889038086
Batch 58/64 loss: -0.6474409103393555
Batch 59/64 loss: -0.4746546745300293
Batch 60/64 loss: -0.6946821212768555
Batch 61/64 loss: -0.6632213592529297
Batch 62/64 loss: 0.008297443389892578
Batch 63/64 loss: -0.6645703315734863
Batch 64/64 loss: -3.896439552307129
Epoch 339  Train loss: -0.5269819932825425  Val loss: -0.8633890184749853
Epoch 340
-------------------------------
Batch 1/64 loss: -0.46667051315307617
Batch 2/64 loss: -0.40045976638793945
Batch 3/64 loss: -0.513545036315918
Batch 4/64 loss: 0.43358516693115234
Batch 5/64 loss: 0.4795660972595215
Batch 6/64 loss: -0.4283928871154785
Batch 7/64 loss: -0.6023430824279785
Batch 8/64 loss: -0.7277421951293945
Batch 9/64 loss: -0.05932426452636719
Batch 10/64 loss: -0.6396036148071289
Batch 11/64 loss: -0.7304081916809082
Batch 12/64 loss: -0.7070279121398926
Batch 13/64 loss: -0.5754666328430176
Batch 14/64 loss: -0.37917470932006836
Batch 15/64 loss: -0.5475368499755859
Batch 16/64 loss: -0.06378984451293945
Batch 17/64 loss: -0.1555042266845703
Batch 18/64 loss: -0.6425952911376953
Batch 19/64 loss: -0.5210914611816406
Batch 20/64 loss: -0.4634437561035156
Batch 21/64 loss: -0.6403894424438477
Batch 22/64 loss: -0.6332159042358398
Batch 23/64 loss: -0.6267557144165039
Batch 24/64 loss: -0.6538519859313965
Batch 25/64 loss: -0.38701486587524414
Batch 26/64 loss: -0.663996696472168
Batch 27/64 loss: -0.5323095321655273
Batch 28/64 loss: -0.6980209350585938
Batch 29/64 loss: -0.19077253341674805
Batch 30/64 loss: -0.71356201171875
Batch 31/64 loss: -0.7298822402954102
Batch 32/64 loss: -0.6711115837097168
Batch 33/64 loss: -0.7540726661682129
Batch 34/64 loss: -0.5845093727111816
Batch 35/64 loss: -0.5297636985778809
Batch 36/64 loss: -0.6783456802368164
Batch 37/64 loss: -0.4985527992248535
Batch 38/64 loss: -0.5914621353149414
Batch 39/64 loss: -0.450223445892334
Batch 40/64 loss: -0.4366025924682617
Batch 41/64 loss: -0.44014739990234375
Batch 42/64 loss: -0.7076163291931152
Batch 43/64 loss: 0.8810029029846191
Batch 44/64 loss: -0.730628490447998
Batch 45/64 loss: -0.665794849395752
Batch 46/64 loss: -0.6806750297546387
Batch 47/64 loss: -0.6624460220336914
Batch 48/64 loss: -0.5840449333190918
Batch 49/64 loss: -0.6066098213195801
Batch 50/64 loss: -0.31318187713623047
Batch 51/64 loss: -0.27330780029296875
Batch 52/64 loss: -0.6309199333190918
Batch 53/64 loss: -0.5191874504089355
Batch 54/64 loss: -0.31400442123413086
Batch 55/64 loss: -0.41823482513427734
Batch 56/64 loss: -0.6736226081848145
Batch 57/64 loss: -0.36172914505004883
Batch 58/64 loss: -0.25182056427001953
Batch 59/64 loss: -0.5439128875732422
Batch 60/64 loss: -0.5863037109375
Batch 61/64 loss: -0.6229028701782227
Batch 62/64 loss: -0.31329822540283203
Batch 63/64 loss: -0.4841461181640625
Batch 64/64 loss: -4.183446407318115
Epoch 340  Train loss: -0.5179059140822466  Val loss: -0.9168344084749517
Epoch 341
-------------------------------
Batch 1/64 loss: -0.08409309387207031
Batch 2/64 loss: -0.5909295082092285
Batch 3/64 loss: -0.509190559387207
Batch 4/64 loss: -0.5284814834594727
Batch 5/64 loss: -0.5376191139221191
Batch 6/64 loss: -0.6701970100402832
Batch 7/64 loss: -0.6176071166992188
Batch 8/64 loss: -0.6685943603515625
Batch 9/64 loss: -0.7116713523864746
Batch 10/64 loss: -0.7119264602661133
Batch 11/64 loss: 0.10790109634399414
Batch 12/64 loss: -0.7440633773803711
Batch 13/64 loss: -0.5325927734375
Batch 14/64 loss: -0.5082926750183105
Batch 15/64 loss: 0.11241817474365234
Batch 16/64 loss: -0.10696697235107422
Batch 17/64 loss: -0.5592875480651855
Batch 18/64 loss: -0.39588165283203125
Batch 19/64 loss: -0.3979177474975586
Batch 20/64 loss: -0.3198542594909668
Batch 21/64 loss: -0.4001951217651367
Batch 22/64 loss: -0.24953222274780273
Batch 23/64 loss: 0.14197635650634766
Batch 24/64 loss: -0.2158045768737793
Batch 25/64 loss: -0.29118824005126953
Batch 26/64 loss: -0.16200542449951172
Batch 27/64 loss: -0.4348921775817871
Batch 28/64 loss: -0.4137592315673828
Batch 29/64 loss: -0.04636335372924805
Batch 30/64 loss: 0.777712345123291
Batch 31/64 loss: -0.31798267364501953
Batch 32/64 loss: -0.4925403594970703
Batch 33/64 loss: -0.5417532920837402
Batch 34/64 loss: -0.4692201614379883
Batch 35/64 loss: -0.3005185127258301
Batch 36/64 loss: 0.07601308822631836
Batch 37/64 loss: -0.3264789581298828
Batch 38/64 loss: -0.37880420684814453
Batch 39/64 loss: -0.3585014343261719
Batch 40/64 loss: -0.5449953079223633
Batch 41/64 loss: -0.4542713165283203
Batch 42/64 loss: -0.34349632263183594
Batch 43/64 loss: -0.21795272827148438
Batch 44/64 loss: -0.1316671371459961
Batch 45/64 loss: -0.5739598274230957
Batch 46/64 loss: -0.47742795944213867
Batch 47/64 loss: -0.37958383560180664
Batch 48/64 loss: -0.651771068572998
Batch 49/64 loss: -0.5708904266357422
Batch 50/64 loss: -0.6028165817260742
Batch 51/64 loss: -0.3465747833251953
Batch 52/64 loss: -0.5199623107910156
Batch 53/64 loss: -0.5601568222045898
Batch 54/64 loss: -0.5258569717407227
Batch 55/64 loss: -0.7298493385314941
Batch 56/64 loss: -0.36232995986938477
Batch 57/64 loss: 0.3134446144104004
Batch 58/64 loss: -0.5437426567077637
Batch 59/64 loss: -0.5946965217590332
Batch 60/64 loss: 0.8694033622741699
Batch 61/64 loss: -0.633995532989502
Batch 62/64 loss: -0.4727029800415039
Batch 63/64 loss: -0.39020299911499023
Batch 64/64 loss: -4.259514331817627
Epoch 341  Train loss: -0.40814709195903703  Val loss: -0.793919592788539
Epoch 342
-------------------------------
Batch 1/64 loss: -0.6739907264709473
Batch 2/64 loss: -0.4637589454650879
Batch 3/64 loss: -0.43561267852783203
Batch 4/64 loss: -0.472381591796875
Batch 5/64 loss: -0.6439952850341797
Batch 6/64 loss: -0.6097760200500488
Batch 7/64 loss: -0.4665513038635254
Batch 8/64 loss: -0.5054306983947754
Batch 9/64 loss: -0.5516200065612793
Batch 10/64 loss: -0.5677895545959473
Batch 11/64 loss: -0.5475444793701172
Batch 12/64 loss: -0.19177722930908203
Batch 13/64 loss: -0.6123099327087402
Batch 14/64 loss: -0.39580774307250977
Batch 15/64 loss: -0.5661115646362305
Batch 16/64 loss: -0.21679925918579102
Batch 17/64 loss: -0.46532773971557617
Batch 18/64 loss: -0.643406867980957
Batch 19/64 loss: -0.18909502029418945
Batch 20/64 loss: -0.5208158493041992
Batch 21/64 loss: -0.6413750648498535
Batch 22/64 loss: -0.4802737236022949
Batch 23/64 loss: -0.6013088226318359
Batch 24/64 loss: -0.7332649230957031
Batch 25/64 loss: -0.5972752571105957
Batch 26/64 loss: -0.2780747413635254
Batch 27/64 loss: -0.5877685546875
Batch 28/64 loss: -0.7393250465393066
Batch 29/64 loss: -0.6089568138122559
Batch 30/64 loss: -0.6212782859802246
Batch 31/64 loss: -0.49694204330444336
Batch 32/64 loss: -0.6792111396789551
Batch 33/64 loss: 0.6591649055480957
Batch 34/64 loss: -0.5916438102722168
Batch 35/64 loss: -0.07360076904296875
Batch 36/64 loss: -0.6813793182373047
Batch 37/64 loss: -0.7823677062988281
Batch 38/64 loss: -0.5177230834960938
Batch 39/64 loss: -0.5602202415466309
Batch 40/64 loss: -0.28311634063720703
Batch 41/64 loss: -0.10141515731811523
Batch 42/64 loss: -0.7280588150024414
Batch 43/64 loss: -0.725156307220459
Batch 44/64 loss: -0.6028695106506348
Batch 45/64 loss: -0.4453854560852051
Batch 46/64 loss: -0.2714676856994629
Batch 47/64 loss: -0.5759458541870117
Batch 48/64 loss: -0.6442151069641113
Batch 49/64 loss: -0.5505709648132324
Batch 50/64 loss: -0.7071323394775391
Batch 51/64 loss: -0.6164088249206543
Batch 52/64 loss: -0.7755527496337891
Batch 53/64 loss: -0.5796689987182617
Batch 54/64 loss: 0.3956165313720703
Batch 55/64 loss: 0.806516170501709
Batch 56/64 loss: -0.6868486404418945
Batch 57/64 loss: -0.13417863845825195
Batch 58/64 loss: -0.625389575958252
Batch 59/64 loss: -0.6911501884460449
Batch 60/64 loss: -0.7125954627990723
Batch 61/64 loss: -0.49982261657714844
Batch 62/64 loss: -0.5217342376708984
Batch 63/64 loss: -0.4701523780822754
Batch 64/64 loss: -4.322079658508301
Epoch 342  Train loss: -0.5229959188723097  Val loss: -0.889778884415774
Epoch 343
-------------------------------
Batch 1/64 loss: -0.6638374328613281
Batch 2/64 loss: -0.5715193748474121
Batch 3/64 loss: -0.6005406379699707
Batch 4/64 loss: -0.32484006881713867
Batch 5/64 loss: -0.6056032180786133
Batch 6/64 loss: -0.7056312561035156
Batch 7/64 loss: -0.7224025726318359
Batch 8/64 loss: -0.5485949516296387
Batch 9/64 loss: -0.43085241317749023
Batch 10/64 loss: 0.39940452575683594
Batch 11/64 loss: -0.5464544296264648
Batch 12/64 loss: -0.6887969970703125
Batch 13/64 loss: -0.6129574775695801
Batch 14/64 loss: -0.6024699211120605
Batch 15/64 loss: -0.698519229888916
Batch 16/64 loss: -0.8133411407470703
Batch 17/64 loss: -0.6657776832580566
Batch 18/64 loss: -0.8777713775634766
Batch 19/64 loss: -0.7298946380615234
Batch 20/64 loss: -0.6052646636962891
Batch 21/64 loss: -0.6348886489868164
Batch 22/64 loss: -0.5040683746337891
Batch 23/64 loss: -0.42368507385253906
Batch 24/64 loss: -0.6768088340759277
Batch 25/64 loss: -0.6834316253662109
Batch 26/64 loss: -0.26090145111083984
Batch 27/64 loss: -0.6748762130737305
Batch 28/64 loss: -0.8067741394042969
Batch 29/64 loss: -0.6690235137939453
Batch 30/64 loss: 0.8321719169616699
Batch 31/64 loss: -0.657325267791748
Batch 32/64 loss: -0.7881126403808594
Batch 33/64 loss: -0.6688709259033203
Batch 34/64 loss: -0.7343683242797852
Batch 35/64 loss: -0.6067705154418945
Batch 36/64 loss: -0.3326292037963867
Batch 37/64 loss: -0.6951980590820312
Batch 38/64 loss: -0.5554089546203613
Batch 39/64 loss: -0.6027240753173828
Batch 40/64 loss: -0.7343916893005371
Batch 41/64 loss: -0.44570207595825195
Batch 42/64 loss: -0.7461676597595215
Batch 43/64 loss: -0.6663475036621094
Batch 44/64 loss: -0.7326116561889648
Batch 45/64 loss: -0.6876316070556641
Batch 46/64 loss: -0.05449628829956055
Batch 47/64 loss: -0.5027222633361816
Batch 48/64 loss: -0.5717926025390625
Batch 49/64 loss: -0.7166628837585449
Batch 50/64 loss: 0.41419029235839844
Batch 51/64 loss: -0.6847357749938965
Batch 52/64 loss: -0.29366064071655273
Batch 53/64 loss: -0.32515716552734375
Batch 54/64 loss: -0.7371273040771484
Batch 55/64 loss: -0.7188334465026855
Batch 56/64 loss: -0.7497844696044922
Batch 57/64 loss: -0.5067839622497559
Batch 58/64 loss: -0.608607292175293
Batch 59/64 loss: -0.541168212890625
Batch 60/64 loss: -0.49794816970825195
Batch 61/64 loss: -0.5987367630004883
Batch 62/64 loss: -0.7260451316833496
Batch 63/64 loss: -0.7874412536621094
Batch 64/64 loss: -4.346874237060547
Epoch 343  Train loss: -0.59984125623516  Val loss: -0.9535738889294392
Epoch 344
-------------------------------
Batch 1/64 loss: -0.7791008949279785
Batch 2/64 loss: -0.725163459777832
Batch 3/64 loss: -0.6264786720275879
Batch 4/64 loss: -0.8417587280273438
Batch 5/64 loss: -0.6824827194213867
Batch 6/64 loss: -0.6477694511413574
Batch 7/64 loss: -0.3679933547973633
Batch 8/64 loss: -0.5904793739318848
Batch 9/64 loss: -0.5023040771484375
Batch 10/64 loss: -0.6432733535766602
Batch 11/64 loss: -0.5925135612487793
Batch 12/64 loss: -0.5344595909118652
Batch 13/64 loss: -0.5310440063476562
Batch 14/64 loss: -0.5015339851379395
Batch 15/64 loss: -0.5124635696411133
Batch 16/64 loss: -0.49573850631713867
Batch 17/64 loss: -0.5207371711730957
Batch 18/64 loss: -0.2493596076965332
Batch 19/64 loss: -0.6284332275390625
Batch 20/64 loss: -0.18240928649902344
Batch 21/64 loss: -0.6264820098876953
Batch 22/64 loss: -0.6075577735900879
Batch 23/64 loss: -0.7317509651184082
Batch 24/64 loss: -0.4312601089477539
Batch 25/64 loss: -0.5949153900146484
Batch 26/64 loss: -0.49797582626342773
Batch 27/64 loss: -0.5024781227111816
Batch 28/64 loss: -0.6642313003540039
Batch 29/64 loss: -0.6396698951721191
Batch 30/64 loss: -0.8370399475097656
Batch 31/64 loss: -0.3692011833190918
Batch 32/64 loss: -0.3450803756713867
Batch 33/64 loss: -0.8033490180969238
Batch 34/64 loss: -0.6452412605285645
Batch 35/64 loss: -0.647796630859375
Batch 36/64 loss: -0.6098628044128418
Batch 37/64 loss: -0.7267723083496094
Batch 38/64 loss: -0.4426302909851074
Batch 39/64 loss: -0.0704493522644043
Batch 40/64 loss: -0.5293941497802734
Batch 41/64 loss: 0.2769584655761719
Batch 42/64 loss: -0.7660965919494629
Batch 43/64 loss: -0.7432198524475098
Batch 44/64 loss: -0.5495409965515137
Batch 45/64 loss: -0.06455850601196289
Batch 46/64 loss: -0.537071704864502
Batch 47/64 loss: 1.0697064399719238
Batch 48/64 loss: -0.5387425422668457
Batch 49/64 loss: -0.3255338668823242
Batch 50/64 loss: 0.9359569549560547
Batch 51/64 loss: -0.597447395324707
Batch 52/64 loss: -0.453702449798584
Batch 53/64 loss: -0.6774530410766602
Batch 54/64 loss: -0.6077733039855957
Batch 55/64 loss: -0.6752510070800781
Batch 56/64 loss: -0.5554623603820801
Batch 57/64 loss: -0.6714768409729004
Batch 58/64 loss: -0.7710204124450684
Batch 59/64 loss: -0.3712606430053711
Batch 60/64 loss: -0.7550559043884277
Batch 61/64 loss: -0.6476612091064453
Batch 62/64 loss: -0.6383895874023438
Batch 63/64 loss: -0.7668371200561523
Batch 64/64 loss: -4.252438545227051
Epoch 344  Train loss: -0.5505709442437864  Val loss: -0.9253000344607428
Epoch 345
-------------------------------
Batch 1/64 loss: -0.021318435668945312
Batch 2/64 loss: -0.6105232238769531
Batch 3/64 loss: 0.8009624481201172
Batch 4/64 loss: -0.5368895530700684
Batch 5/64 loss: -0.7857904434204102
Batch 6/64 loss: -0.45721435546875
Batch 7/64 loss: -0.5538291931152344
Batch 8/64 loss: -0.6250872611999512
Batch 9/64 loss: -0.8150730133056641
Batch 10/64 loss: -0.6567564010620117
Batch 11/64 loss: -0.6027054786682129
Batch 12/64 loss: -0.7577023506164551
Batch 13/64 loss: -0.7637495994567871
Batch 14/64 loss: -0.6942462921142578
Batch 15/64 loss: -0.08492517471313477
Batch 16/64 loss: -0.7249245643615723
Batch 17/64 loss: -0.5562734603881836
Batch 18/64 loss: -0.5560073852539062
Batch 19/64 loss: -0.2557377815246582
Batch 20/64 loss: 0.6004157066345215
Batch 21/64 loss: -0.14106369018554688
Batch 22/64 loss: -0.7425761222839355
Batch 23/64 loss: -0.5798177719116211
Batch 24/64 loss: -0.8485560417175293
Batch 25/64 loss: -0.6433372497558594
Batch 26/64 loss: -0.7139029502868652
Batch 27/64 loss: -0.5655465126037598
Batch 28/64 loss: -0.44239091873168945
Batch 29/64 loss: -0.6803889274597168
Batch 30/64 loss: -0.6891837120056152
Batch 31/64 loss: -0.6385226249694824
Batch 32/64 loss: -0.1697230339050293
Batch 33/64 loss: -0.6923122406005859
Batch 34/64 loss: -0.41411304473876953
Batch 35/64 loss: -0.5871262550354004
Batch 36/64 loss: -0.6200900077819824
Batch 37/64 loss: -0.6566290855407715
Batch 38/64 loss: -0.6485400199890137
Batch 39/64 loss: -0.6234655380249023
Batch 40/64 loss: -0.6776862144470215
Batch 41/64 loss: -0.7412638664245605
Batch 42/64 loss: -0.7199292182922363
Batch 43/64 loss: -0.7650966644287109
Batch 44/64 loss: -0.6012477874755859
Batch 45/64 loss: -0.8293495178222656
Batch 46/64 loss: -0.6312785148620605
Batch 47/64 loss: -0.5520133972167969
Batch 48/64 loss: -0.6141276359558105
Batch 49/64 loss: -0.5364093780517578
Batch 50/64 loss: -0.6756100654602051
Batch 51/64 loss: -0.6404943466186523
Batch 52/64 loss: -0.7113490104675293
Batch 53/64 loss: -0.5628542900085449
Batch 54/64 loss: -0.7571530342102051
Batch 55/64 loss: -0.7770419120788574
Batch 56/64 loss: -0.6082310676574707
Batch 57/64 loss: -0.5294985771179199
Batch 58/64 loss: 0.14584875106811523
Batch 59/64 loss: -0.5947489738464355
Batch 60/64 loss: -0.6851749420166016
Batch 61/64 loss: -0.6606001853942871
Batch 62/64 loss: -0.35048913955688477
Batch 63/64 loss: -0.7054104804992676
Batch 64/64 loss: -4.375123023986816
Epoch 345  Train loss: -0.5932111889708276  Val loss: -1.0048477592337173
Saving best model, epoch: 345
Epoch 346
-------------------------------
Batch 1/64 loss: -0.6835513114929199
Batch 2/64 loss: -0.6904129981994629
Batch 3/64 loss: -0.6543869972229004
Batch 4/64 loss: -0.7577638626098633
Batch 5/64 loss: -0.2701416015625
Batch 6/64 loss: -0.6817173957824707
Batch 7/64 loss: -0.7576932907104492
Batch 8/64 loss: -0.6718378067016602
Batch 9/64 loss: -0.6760134696960449
Batch 10/64 loss: -0.5803570747375488
Batch 11/64 loss: -0.5700736045837402
Batch 12/64 loss: -0.7190055847167969
Batch 13/64 loss: -0.4984002113342285
Batch 14/64 loss: -0.4825453758239746
Batch 15/64 loss: -0.3944272994995117
Batch 16/64 loss: -0.2238454818725586
Batch 17/64 loss: -0.651484489440918
Batch 18/64 loss: 0.6049041748046875
Batch 19/64 loss: -0.5611405372619629
Batch 20/64 loss: -0.6909208297729492
Batch 21/64 loss: -0.5086402893066406
Batch 22/64 loss: -0.6975541114807129
Batch 23/64 loss: -0.7846312522888184
Batch 24/64 loss: -0.5468387603759766
Batch 25/64 loss: -0.550389289855957
Batch 26/64 loss: -0.5130424499511719
Batch 27/64 loss: -0.7230300903320312
Batch 28/64 loss: -0.702517032623291
Batch 29/64 loss: -0.7197155952453613
Batch 30/64 loss: -0.7084550857543945
Batch 31/64 loss: -0.5390348434448242
Batch 32/64 loss: -0.36846256256103516
Batch 33/64 loss: 0.6062045097351074
Batch 34/64 loss: -0.37825822830200195
Batch 35/64 loss: 0.9153895378112793
Batch 36/64 loss: -0.4636096954345703
Batch 37/64 loss: -0.5428361892700195
Batch 38/64 loss: -0.015355110168457031
Batch 39/64 loss: -0.2559971809387207
Batch 40/64 loss: -0.4927248954772949
Batch 41/64 loss: -0.530421257019043
Batch 42/64 loss: -0.3931083679199219
Batch 43/64 loss: -0.3266181945800781
Batch 44/64 loss: -0.39083337783813477
Batch 45/64 loss: -0.4831523895263672
Batch 46/64 loss: -0.5161714553833008
Batch 47/64 loss: -0.4306797981262207
Batch 48/64 loss: -0.2752957344055176
Batch 49/64 loss: -0.3766746520996094
Batch 50/64 loss: -0.5107617378234863
Batch 51/64 loss: -0.5997076034545898
Batch 52/64 loss: -0.21412038803100586
Batch 53/64 loss: -0.6259984970092773
Batch 54/64 loss: -0.6157116889953613
Batch 55/64 loss: -0.6166353225708008
Batch 56/64 loss: -0.7220668792724609
Batch 57/64 loss: -0.6330528259277344
Batch 58/64 loss: -0.6740646362304688
Batch 59/64 loss: -0.5912690162658691
Batch 60/64 loss: -0.6539902687072754
Batch 61/64 loss: 0.4449424743652344
Batch 62/64 loss: -0.6852374076843262
Batch 63/64 loss: -0.2995467185974121
Batch 64/64 loss: -4.286459445953369
Epoch 346  Train loss: -0.510358180251776  Val loss: -0.8261591069067467
Epoch 347
-------------------------------
Batch 1/64 loss: -0.7348117828369141
Batch 2/64 loss: -0.6142354011535645
Batch 3/64 loss: -0.4966917037963867
Batch 4/64 loss: -0.5759272575378418
Batch 5/64 loss: -0.5698370933532715
Batch 6/64 loss: -0.759852409362793
Batch 7/64 loss: -0.5515952110290527
Batch 8/64 loss: -0.6298761367797852
Batch 9/64 loss: -0.6872458457946777
Batch 10/64 loss: -0.7256011962890625
Batch 11/64 loss: -0.6006584167480469
Batch 12/64 loss: -0.5391416549682617
Batch 13/64 loss: -0.6382884979248047
Batch 14/64 loss: -0.7335038185119629
Batch 15/64 loss: -0.700258731842041
Batch 16/64 loss: -0.45920419692993164
Batch 17/64 loss: -0.5453147888183594
Batch 18/64 loss: -0.6247434616088867
Batch 19/64 loss: -0.4131045341491699
Batch 20/64 loss: -0.7507572174072266
Batch 21/64 loss: -0.6410398483276367
Batch 22/64 loss: -0.1593623161315918
Batch 23/64 loss: -0.47115468978881836
Batch 24/64 loss: -0.6288533210754395
Batch 25/64 loss: -0.700869083404541
Batch 26/64 loss: -0.2997746467590332
Batch 27/64 loss: 0.4172053337097168
Batch 28/64 loss: -0.6227011680603027
Batch 29/64 loss: -0.5299105644226074
Batch 30/64 loss: -0.768608570098877
Batch 31/64 loss: -0.4458765983581543
Batch 32/64 loss: -0.5271472930908203
Batch 33/64 loss: -0.5569195747375488
Batch 34/64 loss: -0.5758528709411621
Batch 35/64 loss: -0.5945267677307129
Batch 36/64 loss: -0.5395193099975586
Batch 37/64 loss: -0.6525945663452148
Batch 38/64 loss: 0.007573127746582031
Batch 39/64 loss: -0.4522242546081543
Batch 40/64 loss: -0.4452953338623047
Batch 41/64 loss: -0.31742429733276367
Batch 42/64 loss: -0.6883883476257324
Batch 43/64 loss: -0.633446216583252
Batch 44/64 loss: -0.8316311836242676
Batch 45/64 loss: -0.7295217514038086
Batch 46/64 loss: -0.6720900535583496
Batch 47/64 loss: -0.7875175476074219
Batch 48/64 loss: -0.7901191711425781
Batch 49/64 loss: -0.6021537780761719
Batch 50/64 loss: -0.5778679847717285
Batch 51/64 loss: -0.5747342109680176
Batch 52/64 loss: -0.31410789489746094
Batch 53/64 loss: -0.8050003051757812
Batch 54/64 loss: -0.6403989791870117
Batch 55/64 loss: -0.6851882934570312
Batch 56/64 loss: -0.2559237480163574
Batch 57/64 loss: -0.26802921295166016
Batch 58/64 loss: -0.6814174652099609
Batch 59/64 loss: -0.6843886375427246
Batch 60/64 loss: -0.661799430847168
Batch 61/64 loss: 0.5986285209655762
Batch 62/64 loss: 0.006562709808349609
Batch 63/64 loss: -0.512669563293457
Batch 64/64 loss: -2.466257095336914
Epoch 347  Train loss: -0.556806609209846  Val loss: -0.8403110438605764
Epoch 348
-------------------------------
Batch 1/64 loss: -0.3555569648742676
Batch 2/64 loss: 0.2925724983215332
Batch 3/64 loss: 0.09636306762695312
Batch 4/64 loss: -0.5410594940185547
Batch 5/64 loss: -0.7062864303588867
Batch 6/64 loss: -0.635200023651123
Batch 7/64 loss: -0.21188783645629883
Batch 8/64 loss: -0.7618122100830078
Batch 9/64 loss: -0.022058963775634766
Batch 10/64 loss: -0.5238857269287109
Batch 11/64 loss: -0.6300621032714844
Batch 12/64 loss: -0.4308810234069824
Batch 13/64 loss: -0.4665656089782715
Batch 14/64 loss: -0.6982078552246094
Batch 15/64 loss: -0.6949992179870605
Batch 16/64 loss: -0.7462153434753418
Batch 17/64 loss: -0.4157075881958008
Batch 18/64 loss: -0.6431417465209961
Batch 19/64 loss: -0.5599722862243652
Batch 20/64 loss: -0.7284021377563477
Batch 21/64 loss: -0.5658445358276367
Batch 22/64 loss: -0.579230785369873
Batch 23/64 loss: -0.6564497947692871
Batch 24/64 loss: -0.5855112075805664
Batch 25/64 loss: -0.5997657775878906
Batch 26/64 loss: 0.9971985816955566
Batch 27/64 loss: -0.5089654922485352
Batch 28/64 loss: -0.44753408432006836
Batch 29/64 loss: -0.7388381958007812
Batch 30/64 loss: -0.7445855140686035
Batch 31/64 loss: -0.45691680908203125
Batch 32/64 loss: -0.5385622978210449
Batch 33/64 loss: -0.523045539855957
Batch 34/64 loss: -0.5302014350891113
Batch 35/64 loss: -0.5833020210266113
Batch 36/64 loss: -0.5334715843200684
Batch 37/64 loss: 0.5201282501220703
Batch 38/64 loss: -0.588444709777832
Batch 39/64 loss: -0.7299509048461914
Batch 40/64 loss: -0.4284219741821289
Batch 41/64 loss: -0.5191807746887207
Batch 42/64 loss: -0.5559372901916504
Batch 43/64 loss: -0.7045984268188477
Batch 44/64 loss: -0.7325644493103027
Batch 45/64 loss: -0.611638069152832
Batch 46/64 loss: -0.6852235794067383
Batch 47/64 loss: -0.7026352882385254
Batch 48/64 loss: -0.5345931053161621
Batch 49/64 loss: -0.6435327529907227
Batch 50/64 loss: -0.7626757621765137
Batch 51/64 loss: -0.642920970916748
Batch 52/64 loss: -0.629939079284668
Batch 53/64 loss: -0.6458001136779785
Batch 54/64 loss: -0.7858591079711914
Batch 55/64 loss: -0.6430130004882812
Batch 56/64 loss: -0.6890921592712402
Batch 57/64 loss: -0.6895308494567871
Batch 58/64 loss: -0.48277950286865234
Batch 59/64 loss: -0.6911811828613281
Batch 60/64 loss: -0.6879024505615234
Batch 61/64 loss: -0.4245643615722656
Batch 62/64 loss: -0.37593889236450195
Batch 63/64 loss: -0.5019278526306152
Batch 64/64 loss: -3.7329025268554688
Epoch 348  Train loss: -0.554468843048694  Val loss: -0.9092687102117899
Epoch 349
-------------------------------
Batch 1/64 loss: -0.5556583404541016
Batch 2/64 loss: -0.6981840133666992
Batch 3/64 loss: -0.6529645919799805
Batch 4/64 loss: -0.4310479164123535
Batch 5/64 loss: -0.4666314125061035
Batch 6/64 loss: -0.6169881820678711
Batch 7/64 loss: -0.5445613861083984
Batch 8/64 loss: -0.4741683006286621
Batch 9/64 loss: -0.7611474990844727
Batch 10/64 loss: -0.16393184661865234
Batch 11/64 loss: -0.39766645431518555
Batch 12/64 loss: -0.7587213516235352
Batch 13/64 loss: -0.5039443969726562
Batch 14/64 loss: -0.1654505729675293
Batch 15/64 loss: -0.5867629051208496
Batch 16/64 loss: -0.6549515724182129
Batch 17/64 loss: -0.49158239364624023
Batch 18/64 loss: -0.12537384033203125
Batch 19/64 loss: -0.49649477005004883
Batch 20/64 loss: -0.6237044334411621
Batch 21/64 loss: -0.529512882232666
Batch 22/64 loss: -0.7501378059387207
Batch 23/64 loss: -0.5145387649536133
Batch 24/64 loss: 1.129563808441162
Batch 25/64 loss: -0.5741443634033203
Batch 26/64 loss: -0.5586490631103516
Batch 27/64 loss: -0.4052424430847168
Batch 28/64 loss: -0.40906190872192383
Batch 29/64 loss: -0.6938457489013672
Batch 30/64 loss: -0.8116750717163086
Batch 31/64 loss: -0.6578545570373535
Batch 32/64 loss: -0.6379485130310059
Batch 33/64 loss: -0.6122398376464844
Batch 34/64 loss: 0.5794191360473633
Batch 35/64 loss: -0.6168203353881836
Batch 36/64 loss: -0.6906208992004395
Batch 37/64 loss: -0.4711627960205078
Batch 38/64 loss: -0.6370720863342285
Batch 39/64 loss: -0.7196578979492188
Batch 40/64 loss: -0.6859540939331055
Batch 41/64 loss: -0.6065249443054199
Batch 42/64 loss: -0.5487699508666992
Batch 43/64 loss: 0.30707406997680664
Batch 44/64 loss: -0.6125211715698242
Batch 45/64 loss: -0.33699798583984375
Batch 46/64 loss: -0.5508322715759277
Batch 47/64 loss: -0.714566707611084
Batch 48/64 loss: -0.728093147277832
Batch 49/64 loss: -0.665346622467041
Batch 50/64 loss: -0.6207308769226074
Batch 51/64 loss: -0.6797962188720703
Batch 52/64 loss: -0.4833106994628906
Batch 53/64 loss: -0.7312383651733398
Batch 54/64 loss: -0.6572861671447754
Batch 55/64 loss: -0.2247157096862793
Batch 56/64 loss: -0.7104592323303223
Batch 57/64 loss: -0.7363495826721191
Batch 58/64 loss: 0.33480072021484375
Batch 59/64 loss: -0.5111789703369141
Batch 60/64 loss: -0.7841591835021973
Batch 61/64 loss: -0.5518584251403809
Batch 62/64 loss: -0.6465740203857422
Batch 63/64 loss: -0.635857105255127
Batch 64/64 loss: -4.068335056304932
Epoch 349  Train loss: -0.5424884104261212  Val loss: -0.843160150796687
Epoch 350
-------------------------------
Batch 1/64 loss: -0.5897722244262695
Batch 2/64 loss: -0.6204700469970703
Batch 3/64 loss: -0.6681413650512695
Batch 4/64 loss: -0.6493449211120605
Batch 5/64 loss: -0.5069684982299805
Batch 6/64 loss: 0.9045825004577637
Batch 7/64 loss: -0.5652003288269043
Batch 8/64 loss: -0.5204243659973145
Batch 9/64 loss: 0.08901453018188477
Batch 10/64 loss: -0.4078812599182129
Batch 11/64 loss: -0.5715723037719727
Batch 12/64 loss: -0.7007336616516113
Batch 13/64 loss: -0.6775484085083008
Batch 14/64 loss: -0.6152958869934082
Batch 15/64 loss: -0.47971153259277344
Batch 16/64 loss: -0.6173639297485352
Batch 17/64 loss: -0.5825262069702148
Batch 18/64 loss: -0.6307864189147949
Batch 19/64 loss: 1.876162052154541
Batch 20/64 loss: -0.6444077491760254
Batch 21/64 loss: -0.4231710433959961
Batch 22/64 loss: -0.5738339424133301
Batch 23/64 loss: -0.7026820182800293
Batch 24/64 loss: -0.44516563415527344
Batch 25/64 loss: -0.5594968795776367
Batch 26/64 loss: -0.6145229339599609
Batch 27/64 loss: -0.5828375816345215
Batch 28/64 loss: -0.5383367538452148
Batch 29/64 loss: -0.25397157669067383
Batch 30/64 loss: -0.4913358688354492
Batch 31/64 loss: -0.6052780151367188
Batch 32/64 loss: -0.6777381896972656
Batch 33/64 loss: -0.5987844467163086
Batch 34/64 loss: -0.38037872314453125
Batch 35/64 loss: -0.1147909164428711
Batch 36/64 loss: -0.6414532661437988
Batch 37/64 loss: -0.6439361572265625
Batch 38/64 loss: -0.3625168800354004
Batch 39/64 loss: -0.6016840934753418
Batch 40/64 loss: -0.5855517387390137
Batch 41/64 loss: -0.38654136657714844
Batch 42/64 loss: -0.3739509582519531
Batch 43/64 loss: -0.5993847846984863
Batch 44/64 loss: -0.36701154708862305
Batch 45/64 loss: -0.7376399040222168
Batch 46/64 loss: -0.6333389282226562
Batch 47/64 loss: -0.5529947280883789
Batch 48/64 loss: -0.45395421981811523
Batch 49/64 loss: -0.31559324264526367
Batch 50/64 loss: -0.5932192802429199
Batch 51/64 loss: -0.7033596038818359
Batch 52/64 loss: -0.5864229202270508
Batch 53/64 loss: -0.621427059173584
Batch 54/64 loss: -0.5251760482788086
Batch 55/64 loss: -0.6732921600341797
Batch 56/64 loss: -0.43093013763427734
Batch 57/64 loss: -0.6250228881835938
Batch 58/64 loss: -0.4837651252746582
Batch 59/64 loss: -0.5219392776489258
Batch 60/64 loss: -0.47341489791870117
Batch 61/64 loss: -0.6562790870666504
Batch 62/64 loss: -0.4776787757873535
Batch 63/64 loss: -0.34363222122192383
Batch 64/64 loss: -4.001372337341309
Epoch 350  Train loss: -0.5131428251079485  Val loss: -0.8007382658339038
Epoch 351
-------------------------------
Batch 1/64 loss: -0.4931511878967285
Batch 2/64 loss: -0.5890111923217773
Batch 3/64 loss: -0.706606388092041
Batch 4/64 loss: -0.7832965850830078
Batch 5/64 loss: -0.3647646903991699
Batch 6/64 loss: -0.5511417388916016
Batch 7/64 loss: -0.6030135154724121
Batch 8/64 loss: -0.5473499298095703
Batch 9/64 loss: -0.13693523406982422
Batch 10/64 loss: -0.6772818565368652
Batch 11/64 loss: -0.5875768661499023
Batch 12/64 loss: -0.727931022644043
Batch 13/64 loss: -0.29682016372680664
Batch 14/64 loss: -0.48267364501953125
Batch 15/64 loss: -0.08480453491210938
Batch 16/64 loss: -0.07792282104492188
Batch 17/64 loss: 0.9423532485961914
Batch 18/64 loss: -0.44011878967285156
Batch 19/64 loss: -0.18703317642211914
Batch 20/64 loss: -0.5875034332275391
Batch 21/64 loss: -0.4918999671936035
Batch 22/64 loss: -0.5112514495849609
Batch 23/64 loss: -0.25786495208740234
Batch 24/64 loss: -0.5768632888793945
Batch 25/64 loss: -0.24031352996826172
Batch 26/64 loss: -0.6009726524353027
Batch 27/64 loss: -0.4541149139404297
Batch 28/64 loss: -0.3607206344604492
Batch 29/64 loss: -0.4746890068054199
Batch 30/64 loss: -0.3156604766845703
Batch 31/64 loss: -0.3858208656311035
Batch 32/64 loss: -0.6791558265686035
Batch 33/64 loss: -0.5200018882751465
Batch 34/64 loss: -0.4264817237854004
Batch 35/64 loss: -0.5326442718505859
Batch 36/64 loss: -0.6950011253356934
Batch 37/64 loss: -0.7014141082763672
Batch 38/64 loss: -0.5669207572937012
Batch 39/64 loss: 0.3997464179992676
Batch 40/64 loss: -0.46215009689331055
Batch 41/64 loss: -0.31345415115356445
Batch 42/64 loss: -0.601311206817627
Batch 43/64 loss: -0.5243849754333496
Batch 44/64 loss: -0.477752685546875
Batch 45/64 loss: -0.44944095611572266
Batch 46/64 loss: -0.5341382026672363
Batch 47/64 loss: -0.6529645919799805
Batch 48/64 loss: -0.6575446128845215
Batch 49/64 loss: -0.34322118759155273
Batch 50/64 loss: 0.5086455345153809
Batch 51/64 loss: -0.6085724830627441
Batch 52/64 loss: -0.7170906066894531
Batch 53/64 loss: -0.695746898651123
Batch 54/64 loss: -0.32880687713623047
Batch 55/64 loss: -0.5108561515808105
Batch 56/64 loss: -0.7151222229003906
Batch 57/64 loss: -0.29094409942626953
Batch 58/64 loss: -0.5868535041809082
Batch 59/64 loss: -0.6728196144104004
Batch 60/64 loss: -0.6632561683654785
Batch 61/64 loss: -0.6069588661193848
Batch 62/64 loss: -0.6802597045898438
Batch 63/64 loss: -0.4919757843017578
Batch 64/64 loss: -4.140233993530273
Epoch 351  Train loss: -0.495008378870347  Val loss: -0.6996977501308795
Epoch 352
-------------------------------
Batch 1/64 loss: -0.615654468536377
Batch 2/64 loss: -0.7075238227844238
Batch 3/64 loss: -0.6526141166687012
Batch 4/64 loss: -0.5333662033081055
Batch 5/64 loss: -0.0431666374206543
Batch 6/64 loss: -0.49082374572753906
Batch 7/64 loss: -0.4660224914550781
Batch 8/64 loss: -0.4840569496154785
Batch 9/64 loss: -0.45134496688842773
Batch 10/64 loss: -0.6339907646179199
Batch 11/64 loss: -0.5271720886230469
Batch 12/64 loss: -0.4931221008300781
Batch 13/64 loss: -0.3434739112854004
Batch 14/64 loss: -0.4217252731323242
Batch 15/64 loss: -0.5765190124511719
Batch 16/64 loss: -0.43375396728515625
Batch 17/64 loss: -0.573183536529541
Batch 18/64 loss: -0.41594552993774414
Batch 19/64 loss: -0.17711114883422852
Batch 20/64 loss: -0.47056007385253906
Batch 21/64 loss: -0.5684833526611328
Batch 22/64 loss: 0.6361193656921387
Batch 23/64 loss: -0.5926599502563477
Batch 24/64 loss: -0.36538171768188477
Batch 25/64 loss: -0.5806245803833008
Batch 26/64 loss: -0.2600822448730469
Batch 27/64 loss: -0.6421685218811035
Batch 28/64 loss: -0.7027435302734375
Batch 29/64 loss: -0.22055864334106445
Batch 30/64 loss: -0.3372001647949219
Batch 31/64 loss: -0.701479434967041
Batch 32/64 loss: -0.5479083061218262
Batch 33/64 loss: -0.5473713874816895
Batch 34/64 loss: -0.5751953125
Batch 35/64 loss: -0.584923267364502
Batch 36/64 loss: -0.6792826652526855
Batch 37/64 loss: -0.40620946884155273
Batch 38/64 loss: -0.43906259536743164
Batch 39/64 loss: -0.6083846092224121
Batch 40/64 loss: -0.6340498924255371
Batch 41/64 loss: -0.4206733703613281
Batch 42/64 loss: 1.713883399963379
Batch 43/64 loss: -0.7990818023681641
Batch 44/64 loss: 0.30623722076416016
Batch 45/64 loss: -0.4706721305847168
Batch 46/64 loss: -0.23435401916503906
Batch 47/64 loss: 0.7446928024291992
Batch 48/64 loss: -0.20224618911743164
Batch 49/64 loss: -0.6007437705993652
Batch 50/64 loss: -0.6977214813232422
Batch 51/64 loss: -0.5080714225769043
Batch 52/64 loss: -0.7049832344055176
Batch 53/64 loss: -0.41175127029418945
Batch 54/64 loss: -0.6173181533813477
Batch 55/64 loss: -0.488527774810791
Batch 56/64 loss: -0.5776772499084473
Batch 57/64 loss: -0.7418899536132812
Batch 58/64 loss: -0.7327432632446289
Batch 59/64 loss: -0.5624485015869141
Batch 60/64 loss: -0.6827588081359863
Batch 61/64 loss: -0.6307172775268555
Batch 62/64 loss: -0.675713062286377
Batch 63/64 loss: -0.4503135681152344
Batch 64/64 loss: -4.36020565032959
Epoch 352  Train loss: -0.47975730522006166  Val loss: -0.8566011645130276
Epoch 353
-------------------------------
Batch 1/64 loss: -0.6308393478393555
Batch 2/64 loss: -0.6580362319946289
Batch 3/64 loss: -0.49702978134155273
Batch 4/64 loss: -0.33106327056884766
Batch 5/64 loss: -0.5935454368591309
Batch 6/64 loss: -0.5354032516479492
Batch 7/64 loss: -0.6063385009765625
Batch 8/64 loss: 0.4913473129272461
Batch 9/64 loss: -0.6270437240600586
Batch 10/64 loss: -0.34661245346069336
Batch 11/64 loss: -0.42063426971435547
Batch 12/64 loss: -0.42473411560058594
Batch 13/64 loss: -0.5432658195495605
Batch 14/64 loss: -0.5811395645141602
Batch 15/64 loss: -0.716404914855957
Batch 16/64 loss: -0.616539478302002
Batch 17/64 loss: 0.9743385314941406
Batch 18/64 loss: -0.49835729598999023
Batch 19/64 loss: -0.6501221656799316
Batch 20/64 loss: -0.7385640144348145
Batch 21/64 loss: -0.611793041229248
Batch 22/64 loss: 0.5715756416320801
Batch 23/64 loss: -0.6435251235961914
Batch 24/64 loss: -0.47073936462402344
Batch 25/64 loss: -0.4432716369628906
Batch 26/64 loss: -0.665985107421875
Batch 27/64 loss: -0.591733455657959
Batch 28/64 loss: -0.1944289207458496
Batch 29/64 loss: -0.1359100341796875
Batch 30/64 loss: -0.5412969589233398
Batch 31/64 loss: -0.294647216796875
Batch 32/64 loss: -0.5419173240661621
Batch 33/64 loss: -0.8047003746032715
Batch 34/64 loss: -0.13900232315063477
Batch 35/64 loss: -0.6505579948425293
Batch 36/64 loss: -0.6486668586730957
Batch 37/64 loss: -0.5549077987670898
Batch 38/64 loss: -0.5090484619140625
Batch 39/64 loss: -0.6846981048583984
Batch 40/64 loss: -0.3726177215576172
Batch 41/64 loss: -0.5583043098449707
Batch 42/64 loss: -0.7633266448974609
Batch 43/64 loss: -0.589540958404541
Batch 44/64 loss: -0.641045093536377
Batch 45/64 loss: -0.7288975715637207
Batch 46/64 loss: -0.795771598815918
Batch 47/64 loss: -0.5435986518859863
Batch 48/64 loss: -0.22801637649536133
Batch 49/64 loss: -0.5580134391784668
Batch 50/64 loss: -0.6563596725463867
Batch 51/64 loss: -0.6658935546875
Batch 52/64 loss: -0.5188937187194824
Batch 53/64 loss: -0.7068839073181152
Batch 54/64 loss: -0.6964020729064941
Batch 55/64 loss: -0.6576480865478516
Batch 56/64 loss: -0.7622895240783691
Batch 57/64 loss: -0.6777725219726562
Batch 58/64 loss: -0.6061830520629883
Batch 59/64 loss: -0.6928997039794922
Batch 60/64 loss: -0.669736385345459
Batch 61/64 loss: -0.3225746154785156
Batch 62/64 loss: -0.6947989463806152
Batch 63/64 loss: -0.6337475776672363
Batch 64/64 loss: -4.303633689880371
Epoch 353  Train loss: -0.550183266284419  Val loss: -0.8724523328014255
Epoch 354
-------------------------------
Batch 1/64 loss: -0.772728443145752
Batch 2/64 loss: 0.8350620269775391
Batch 3/64 loss: -0.2958540916442871
Batch 4/64 loss: -0.7306118011474609
Batch 5/64 loss: 0.32152414321899414
Batch 6/64 loss: -0.014534473419189453
Batch 7/64 loss: -0.6647372245788574
Batch 8/64 loss: -0.5208005905151367
Batch 9/64 loss: -0.03810405731201172
Batch 10/64 loss: -0.7096590995788574
Batch 11/64 loss: -0.6565093994140625
Batch 12/64 loss: -0.7419304847717285
Batch 13/64 loss: -0.5975885391235352
Batch 14/64 loss: -0.4958610534667969
Batch 15/64 loss: -0.7874131202697754
Batch 16/64 loss: -0.571962833404541
Batch 17/64 loss: -0.5465140342712402
Batch 18/64 loss: -0.6855769157409668
Batch 19/64 loss: -0.5942649841308594
Batch 20/64 loss: -0.6212334632873535
Batch 21/64 loss: -0.7512702941894531
Batch 22/64 loss: 0.658043384552002
Batch 23/64 loss: -0.5364460945129395
Batch 24/64 loss: -0.768247127532959
Batch 25/64 loss: -0.7428731918334961
Batch 26/64 loss: -0.7169508934020996
Batch 27/64 loss: -0.6392788887023926
Batch 28/64 loss: -0.6289491653442383
Batch 29/64 loss: -0.3747382164001465
Batch 30/64 loss: -0.7784113883972168
Batch 31/64 loss: -0.3871188163757324
Batch 32/64 loss: -0.5774335861206055
Batch 33/64 loss: -0.7392587661743164
Batch 34/64 loss: -0.6120634078979492
Batch 35/64 loss: -0.8100266456604004
Batch 36/64 loss: -0.5805912017822266
Batch 37/64 loss: -0.6506333351135254
Batch 38/64 loss: -0.7347607612609863
Batch 39/64 loss: -0.5984606742858887
Batch 40/64 loss: 0.18784475326538086
Batch 41/64 loss: -0.5335183143615723
Batch 42/64 loss: -0.4609694480895996
Batch 43/64 loss: -0.776888370513916
Batch 44/64 loss: -0.6446990966796875
Batch 45/64 loss: -0.14666748046875
Batch 46/64 loss: -0.6208009719848633
Batch 47/64 loss: -0.7010993957519531
Batch 48/64 loss: -0.5162558555603027
Batch 49/64 loss: -0.49280500411987305
Batch 50/64 loss: -0.7769827842712402
Batch 51/64 loss: -0.7698283195495605
Batch 52/64 loss: -0.6670994758605957
Batch 53/64 loss: -0.502690315246582
Batch 54/64 loss: -0.5227832794189453
Batch 55/64 loss: -0.7636675834655762
Batch 56/64 loss: -0.5483555793762207
Batch 57/64 loss: -0.6710333824157715
Batch 58/64 loss: -0.2719764709472656
Batch 59/64 loss: -0.6192789077758789
Batch 60/64 loss: -0.5550546646118164
Batch 61/64 loss: -0.7856240272521973
Batch 62/64 loss: -0.5753698348999023
Batch 63/64 loss: -0.49624013900756836
Batch 64/64 loss: -4.3570075035095215
Epoch 354  Train loss: -0.5703273268306956  Val loss: -0.9141071883263866
Epoch 355
-------------------------------
Batch 1/64 loss: -0.5647702217102051
Batch 2/64 loss: -0.8610744476318359
Batch 3/64 loss: -0.706047534942627
Batch 4/64 loss: -0.8085136413574219
Batch 5/64 loss: -0.7980289459228516
Batch 6/64 loss: -0.49506616592407227
Batch 7/64 loss: -0.5095667839050293
Batch 8/64 loss: -0.5187282562255859
Batch 9/64 loss: -0.7354898452758789
Batch 10/64 loss: -0.761350154876709
Batch 11/64 loss: -0.5124273300170898
Batch 12/64 loss: -0.7294044494628906
Batch 13/64 loss: -0.5653247833251953
Batch 14/64 loss: -0.5900206565856934
Batch 15/64 loss: -0.7937498092651367
Batch 16/64 loss: -0.7253341674804688
Batch 17/64 loss: -0.5831265449523926
Batch 18/64 loss: -0.6968851089477539
Batch 19/64 loss: 0.1302485466003418
Batch 20/64 loss: -0.6557469367980957
Batch 21/64 loss: -0.5259037017822266
Batch 22/64 loss: -0.35448741912841797
Batch 23/64 loss: -0.5972700119018555
Batch 24/64 loss: -0.07237482070922852
Batch 25/64 loss: -0.6910085678100586
Batch 26/64 loss: -0.7084970474243164
Batch 27/64 loss: 0.03012228012084961
Batch 28/64 loss: -0.6386828422546387
Batch 29/64 loss: 0.3785061836242676
Batch 30/64 loss: -0.7896995544433594
Batch 31/64 loss: -0.6264019012451172
Batch 32/64 loss: -0.04360008239746094
Batch 33/64 loss: -0.5942106246948242
Batch 34/64 loss: -0.7719821929931641
Batch 35/64 loss: -0.7598128318786621
Batch 36/64 loss: -0.6427731513977051
Batch 37/64 loss: -0.8631315231323242
Batch 38/64 loss: 0.5386838912963867
Batch 39/64 loss: -0.8101944923400879
Batch 40/64 loss: -0.7423243522644043
Batch 41/64 loss: -0.5749421119689941
Batch 42/64 loss: -0.7691249847412109
Batch 43/64 loss: -0.600837230682373
Batch 44/64 loss: -0.40732622146606445
Batch 45/64 loss: -0.6156454086303711
Batch 46/64 loss: -0.5391802787780762
Batch 47/64 loss: -0.5120482444763184
Batch 48/64 loss: 0.707763671875
Batch 49/64 loss: -0.6844573020935059
Batch 50/64 loss: -0.7506623268127441
Batch 51/64 loss: -0.7603960037231445
Batch 52/64 loss: -0.6533951759338379
Batch 53/64 loss: -0.5861306190490723
Batch 54/64 loss: -0.5920200347900391
Batch 55/64 loss: -0.7119688987731934
Batch 56/64 loss: -0.5227522850036621
Batch 57/64 loss: -0.6815142631530762
Batch 58/64 loss: -0.04209470748901367
Batch 59/64 loss: -0.4497566223144531
Batch 60/64 loss: -0.4680213928222656
Batch 61/64 loss: -0.66534423828125
Batch 62/64 loss: -0.6143689155578613
Batch 63/64 loss: -0.37662363052368164
Batch 64/64 loss: -4.236944675445557
Epoch 355  Train loss: -0.5774745997260599  Val loss: -0.8990526887559399
Epoch 356
-------------------------------
Batch 1/64 loss: 0.7003259658813477
Batch 2/64 loss: -0.5412182807922363
Batch 3/64 loss: -0.6487941741943359
Batch 4/64 loss: -0.6480488777160645
Batch 5/64 loss: -0.6906089782714844
Batch 6/64 loss: -0.5334653854370117
Batch 7/64 loss: -0.5788054466247559
Batch 8/64 loss: -0.7125296592712402
Batch 9/64 loss: -0.3966794013977051
Batch 10/64 loss: -0.5684871673583984
Batch 11/64 loss: -0.6252989768981934
Batch 12/64 loss: -0.5723404884338379
Batch 13/64 loss: -0.09772682189941406
Batch 14/64 loss: -0.5929450988769531
Batch 15/64 loss: -0.30666065216064453
Batch 16/64 loss: -0.5636038780212402
Batch 17/64 loss: -0.6473212242126465
Batch 18/64 loss: -0.35280561447143555
Batch 19/64 loss: -0.17589616775512695
Batch 20/64 loss: -0.564018726348877
Batch 21/64 loss: -0.6806511878967285
Batch 22/64 loss: -0.6711559295654297
Batch 23/64 loss: -0.8046932220458984
Batch 24/64 loss: -0.41001129150390625
Batch 25/64 loss: -0.5564336776733398
Batch 26/64 loss: -0.7379426956176758
Batch 27/64 loss: -0.8047542572021484
Batch 28/64 loss: -0.7474422454833984
Batch 29/64 loss: -0.7050871849060059
Batch 30/64 loss: -0.49797821044921875
Batch 31/64 loss: -0.7520561218261719
Batch 32/64 loss: -0.46033287048339844
Batch 33/64 loss: -0.6506834030151367
Batch 34/64 loss: -0.2222905158996582
Batch 35/64 loss: -0.6853208541870117
Batch 36/64 loss: -0.49284791946411133
Batch 37/64 loss: -0.606320858001709
Batch 38/64 loss: -0.48201894760131836
Batch 39/64 loss: -0.5059070587158203
Batch 40/64 loss: -0.36124467849731445
Batch 41/64 loss: -0.5550665855407715
Batch 42/64 loss: -0.5790615081787109
Batch 43/64 loss: -0.6904058456420898
Batch 44/64 loss: -0.5031843185424805
Batch 45/64 loss: -0.5695490837097168
Batch 46/64 loss: -0.2662081718444824
Batch 47/64 loss: -0.3346128463745117
Batch 48/64 loss: -0.508507251739502
Batch 49/64 loss: -0.01412343978881836
Batch 50/64 loss: 1.4829611778259277
Batch 51/64 loss: -0.43413448333740234
Batch 52/64 loss: -0.6657528877258301
Batch 53/64 loss: -0.5656805038452148
Batch 54/64 loss: -0.5710005760192871
Batch 55/64 loss: -0.7389612197875977
Batch 56/64 loss: -0.7174558639526367
Batch 57/64 loss: -0.5216794013977051
Batch 58/64 loss: -0.6747775077819824
Batch 59/64 loss: -0.7229223251342773
Batch 60/64 loss: -0.6123170852661133
Batch 61/64 loss: -0.5930638313293457
Batch 62/64 loss: -0.790123462677002
Batch 63/64 loss: -0.5400376319885254
Batch 64/64 loss: -3.957859992980957
Epoch 356  Train loss: -0.5428417542401482  Val loss: -0.9406706950918505
Epoch 357
-------------------------------
Batch 1/64 loss: -0.5916895866394043
Batch 2/64 loss: -0.007951736450195312
Batch 3/64 loss: -0.6445760726928711
Batch 4/64 loss: -0.36602258682250977
Batch 5/64 loss: -0.7176952362060547
Batch 6/64 loss: -0.6139907836914062
Batch 7/64 loss: -0.6576485633850098
Batch 8/64 loss: -0.47768735885620117
Batch 9/64 loss: -0.7523155212402344
Batch 10/64 loss: -0.6240396499633789
Batch 11/64 loss: -0.7286810874938965
Batch 12/64 loss: -0.7726969718933105
Batch 13/64 loss: -0.5829005241394043
Batch 14/64 loss: -0.7481775283813477
Batch 15/64 loss: -0.7945685386657715
Batch 16/64 loss: -0.7213783264160156
Batch 17/64 loss: 0.8901386260986328
Batch 18/64 loss: 0.0333704948425293
Batch 19/64 loss: -0.5682106018066406
Batch 20/64 loss: -0.6065940856933594
Batch 21/64 loss: -0.5707364082336426
Batch 22/64 loss: -0.8519330024719238
Batch 23/64 loss: 0.6785058975219727
Batch 24/64 loss: -0.7517595291137695
Batch 25/64 loss: -0.7204570770263672
Batch 26/64 loss: -0.45332908630371094
Batch 27/64 loss: -0.5525822639465332
Batch 28/64 loss: -0.3532891273498535
Batch 29/64 loss: -0.4336867332458496
Batch 30/64 loss: -0.6758227348327637
Batch 31/64 loss: -0.4532032012939453
Batch 32/64 loss: -0.7199621200561523
Batch 33/64 loss: -0.548067569732666
Batch 34/64 loss: 0.2543144226074219
Batch 35/64 loss: -0.7602987289428711
Batch 36/64 loss: -0.7887721061706543
Batch 37/64 loss: -0.46324634552001953
Batch 38/64 loss: -0.6200475692749023
Batch 39/64 loss: -0.5532865524291992
Batch 40/64 loss: -0.4734020233154297
Batch 41/64 loss: -0.780888557434082
Batch 42/64 loss: -0.5974512100219727
Batch 43/64 loss: -0.5139236450195312
Batch 44/64 loss: -0.7067975997924805
Batch 45/64 loss: 0.12592172622680664
Batch 46/64 loss: -0.6328272819519043
Batch 47/64 loss: -0.5393753051757812
Batch 48/64 loss: -0.4933509826660156
Batch 49/64 loss: -0.635810375213623
Batch 50/64 loss: -0.7637271881103516
Batch 51/64 loss: -0.4888777732849121
Batch 52/64 loss: -0.6707429885864258
Batch 53/64 loss: -0.6120090484619141
Batch 54/64 loss: -0.6712021827697754
Batch 55/64 loss: -0.6049032211303711
Batch 56/64 loss: -0.6319546699523926
Batch 57/64 loss: -0.6580533981323242
Batch 58/64 loss: -0.32801008224487305
Batch 59/64 loss: -0.6633915901184082
Batch 60/64 loss: -0.830812931060791
Batch 61/64 loss: -0.8107032775878906
Batch 62/64 loss: -0.5679383277893066
Batch 63/64 loss: -0.6295566558837891
Batch 64/64 loss: -4.355833530426025
Epoch 357  Train loss: -0.5778453209820915  Val loss: -0.9891842845379282
Epoch 358
-------------------------------
Batch 1/64 loss: -0.34516096115112305
Batch 2/64 loss: -0.09187936782836914
Batch 3/64 loss: -0.8095192909240723
Batch 4/64 loss: -0.755953311920166
Batch 5/64 loss: 0.8666496276855469
Batch 6/64 loss: -0.6391730308532715
Batch 7/64 loss: -0.5944113731384277
Batch 8/64 loss: -0.7036309242248535
Batch 9/64 loss: -0.7340693473815918
Batch 10/64 loss: -0.7800750732421875
Batch 11/64 loss: -0.7049102783203125
Batch 12/64 loss: -0.7636113166809082
Batch 13/64 loss: -0.7072076797485352
Batch 14/64 loss: -0.8548569679260254
Batch 15/64 loss: 0.10517406463623047
Batch 16/64 loss: -0.6985397338867188
Batch 17/64 loss: -0.7940926551818848
Batch 18/64 loss: -0.6849756240844727
Batch 19/64 loss: -0.6912012100219727
Batch 20/64 loss: -0.8632264137268066
Batch 21/64 loss: -0.6976809501647949
Batch 22/64 loss: -0.6733059883117676
Batch 23/64 loss: -0.5795726776123047
Batch 24/64 loss: -0.18222522735595703
Batch 25/64 loss: -0.6621451377868652
Batch 26/64 loss: -0.47925615310668945
Batch 27/64 loss: -0.6272430419921875
Batch 28/64 loss: -0.7155399322509766
Batch 29/64 loss: -0.8287701606750488
Batch 30/64 loss: -0.6253523826599121
Batch 31/64 loss: -0.5994338989257812
Batch 32/64 loss: -0.7378296852111816
Batch 33/64 loss: -0.7689309120178223
Batch 34/64 loss: -0.6892514228820801
Batch 35/64 loss: -0.7285432815551758
Batch 36/64 loss: -0.49660587310791016
Batch 37/64 loss: -0.7683429718017578
Batch 38/64 loss: -0.5839838981628418
Batch 39/64 loss: -0.5191235542297363
Batch 40/64 loss: -0.5113558769226074
Batch 41/64 loss: -0.7657785415649414
Batch 42/64 loss: -0.6274828910827637
Batch 43/64 loss: -0.6270360946655273
Batch 44/64 loss: -0.6601338386535645
Batch 45/64 loss: -0.697392463684082
Batch 46/64 loss: -0.7195339202880859
Batch 47/64 loss: -0.7333784103393555
Batch 48/64 loss: -0.653968334197998
Batch 49/64 loss: -0.3839449882507324
Batch 50/64 loss: -0.7345118522644043
Batch 51/64 loss: -0.6502437591552734
Batch 52/64 loss: -0.7222414016723633
Batch 53/64 loss: -0.6903347969055176
Batch 54/64 loss: -0.5118894577026367
Batch 55/64 loss: 0.23499107360839844
Batch 56/64 loss: -0.5226263999938965
Batch 57/64 loss: -0.7370004653930664
Batch 58/64 loss: -0.6392498016357422
Batch 59/64 loss: 0.0005908012390136719
Batch 60/64 loss: -0.6324973106384277
Batch 61/64 loss: -0.026256084442138672
Batch 62/64 loss: -0.6442470550537109
Batch 63/64 loss: 0.3881802558898926
Batch 64/64 loss: -4.227930068969727
Epoch 358  Train loss: -0.6062132891486672  Val loss: -0.9808623717003262
Epoch 359
-------------------------------
Batch 1/64 loss: -0.7969250679016113
Batch 2/64 loss: -0.6793379783630371
Batch 3/64 loss: -0.7847070693969727
Batch 4/64 loss: -0.8604273796081543
Batch 5/64 loss: 0.32567882537841797
Batch 6/64 loss: -0.6466951370239258
Batch 7/64 loss: -0.5367670059204102
Batch 8/64 loss: -0.4008960723876953
Batch 9/64 loss: -0.7955183982849121
Batch 10/64 loss: -0.6870384216308594
Batch 11/64 loss: -0.5377135276794434
Batch 12/64 loss: -0.6313323974609375
Batch 13/64 loss: -0.6473469734191895
Batch 14/64 loss: -0.6594018936157227
Batch 15/64 loss: -0.7093768119812012
Batch 16/64 loss: -0.6811389923095703
Batch 17/64 loss: -0.7619705200195312
Batch 18/64 loss: -0.7191543579101562
Batch 19/64 loss: -0.8779730796813965
Batch 20/64 loss: -0.3809542655944824
Batch 21/64 loss: -0.7355351448059082
Batch 22/64 loss: -0.7683601379394531
Batch 23/64 loss: -0.4333796501159668
Batch 24/64 loss: -0.6251997947692871
Batch 25/64 loss: -0.6216697692871094
Batch 26/64 loss: -0.6962990760803223
Batch 27/64 loss: -0.6056699752807617
Batch 28/64 loss: -0.6664543151855469
Batch 29/64 loss: -0.6543631553649902
Batch 30/64 loss: -0.6838679313659668
Batch 31/64 loss: -0.4494309425354004
Batch 32/64 loss: -0.5741419792175293
Batch 33/64 loss: -0.45729494094848633
Batch 34/64 loss: 0.17047643661499023
Batch 35/64 loss: -0.8075242042541504
Batch 36/64 loss: -0.651512622833252
Batch 37/64 loss: -0.6219930648803711
Batch 38/64 loss: -0.6987504959106445
Batch 39/64 loss: -0.20243072509765625
Batch 40/64 loss: -0.6706366539001465
Batch 41/64 loss: -0.538355827331543
Batch 42/64 loss: -0.6694126129150391
Batch 43/64 loss: -0.6914405822753906
Batch 44/64 loss: -0.7254428863525391
Batch 45/64 loss: -0.4667530059814453
Batch 46/64 loss: -0.8088688850402832
Batch 47/64 loss: -0.6739673614501953
Batch 48/64 loss: -0.7647762298583984
Batch 49/64 loss: -0.7003598213195801
Batch 50/64 loss: -0.5885519981384277
Batch 51/64 loss: -0.7648720741271973
Batch 52/64 loss: 0.902778148651123
Batch 53/64 loss: -0.6031703948974609
Batch 54/64 loss: -0.711674690246582
Batch 55/64 loss: -0.46174001693725586
Batch 56/64 loss: -0.5991835594177246
Batch 57/64 loss: -0.43259620666503906
Batch 58/64 loss: -0.7268514633178711
Batch 59/64 loss: -0.6480875015258789
Batch 60/64 loss: 0.6274814605712891
Batch 61/64 loss: -0.5023236274719238
Batch 62/64 loss: -0.7745356559753418
Batch 63/64 loss: -0.552736759185791
Batch 64/64 loss: -4.287593841552734
Epoch 359  Train loss: -0.6119870054955576  Val loss: -0.7254693860450561
Epoch 360
-------------------------------
Batch 1/64 loss: -0.6292996406555176
Batch 2/64 loss: -0.007175445556640625
Batch 3/64 loss: -0.6994085311889648
Batch 4/64 loss: -0.5282039642333984
Batch 5/64 loss: -0.6948580741882324
Batch 6/64 loss: -0.6955809593200684
Batch 7/64 loss: -0.7514190673828125
Batch 8/64 loss: -0.6209011077880859
Batch 9/64 loss: -0.7263469696044922
Batch 10/64 loss: -0.48167943954467773
Batch 11/64 loss: -0.5676994323730469
Batch 12/64 loss: -0.2299966812133789
Batch 13/64 loss: -0.6923003196716309
Batch 14/64 loss: -0.6973743438720703
Batch 15/64 loss: 0.007328987121582031
Batch 16/64 loss: -0.5505905151367188
Batch 17/64 loss: -0.31322240829467773
Batch 18/64 loss: -0.6702370643615723
Batch 19/64 loss: -0.6449522972106934
Batch 20/64 loss: -0.6779780387878418
Batch 21/64 loss: -0.7005572319030762
Batch 22/64 loss: -0.7035336494445801
Batch 23/64 loss: -0.7106404304504395
Batch 24/64 loss: -0.6341552734375
Batch 25/64 loss: -0.5716953277587891
Batch 26/64 loss: -0.6301069259643555
Batch 27/64 loss: -0.621826171875
Batch 28/64 loss: -0.6301102638244629
Batch 29/64 loss: -0.6465892791748047
Batch 30/64 loss: -0.6119675636291504
Batch 31/64 loss: -0.590670108795166
Batch 32/64 loss: -0.6184797286987305
Batch 33/64 loss: -0.583467960357666
Batch 34/64 loss: -0.6007418632507324
Batch 35/64 loss: -0.5086159706115723
Batch 36/64 loss: -0.889704704284668
Batch 37/64 loss: -0.5984134674072266
Batch 38/64 loss: 0.31044483184814453
Batch 39/64 loss: -0.7373099327087402
Batch 40/64 loss: -0.7602267265319824
Batch 41/64 loss: -0.7534809112548828
Batch 42/64 loss: -0.632448673248291
Batch 43/64 loss: -0.760401725769043
Batch 44/64 loss: -0.7708535194396973
Batch 45/64 loss: -0.28606653213500977
Batch 46/64 loss: -0.7703886032104492
Batch 47/64 loss: -0.8229494094848633
Batch 48/64 loss: -0.5709519386291504
Batch 49/64 loss: -0.7557291984558105
Batch 50/64 loss: -0.6941623687744141
Batch 51/64 loss: -0.5982165336608887
Batch 52/64 loss: -0.6464991569519043
Batch 53/64 loss: -0.5808086395263672
Batch 54/64 loss: -0.6365346908569336
Batch 55/64 loss: -0.7277631759643555
Batch 56/64 loss: -0.7415981292724609
Batch 57/64 loss: -0.7063326835632324
Batch 58/64 loss: -0.6420001983642578
Batch 59/64 loss: -0.7734346389770508
Batch 60/64 loss: 1.038576602935791
Batch 61/64 loss: -0.7464423179626465
Batch 62/64 loss: -0.6169848442077637
Batch 63/64 loss: -0.5469799041748047
Batch 64/64 loss: -2.463200569152832
Epoch 360  Train loss: -0.6039233675190047  Val loss: -0.8990757735734133
Epoch 361
-------------------------------
Batch 1/64 loss: -0.7447314262390137
Batch 2/64 loss: -0.8371987342834473
Batch 3/64 loss: 0.4205608367919922
Batch 4/64 loss: -0.46953678131103516
Batch 5/64 loss: -0.2848224639892578
Batch 6/64 loss: -0.7192831039428711
Batch 7/64 loss: -0.6796674728393555
Batch 8/64 loss: -0.6255812644958496
Batch 9/64 loss: -0.5841541290283203
Batch 10/64 loss: -0.5738105773925781
Batch 11/64 loss: -0.6567902565002441
Batch 12/64 loss: 0.24702930450439453
Batch 13/64 loss: -0.6811795234680176
Batch 14/64 loss: -0.6049594879150391
Batch 15/64 loss: -0.5561227798461914
Batch 16/64 loss: -0.12503671646118164
Batch 17/64 loss: -0.5678439140319824
Batch 18/64 loss: -0.2639651298522949
Batch 19/64 loss: -0.03406858444213867
Batch 20/64 loss: -0.42210912704467773
Batch 21/64 loss: -0.39513683319091797
Batch 22/64 loss: -0.5065922737121582
Batch 23/64 loss: -0.6804323196411133
Batch 24/64 loss: -0.5767059326171875
Batch 25/64 loss: -0.7622900009155273
Batch 26/64 loss: -0.6727089881896973
Batch 27/64 loss: -0.7729215621948242
Batch 28/64 loss: -0.6579351425170898
Batch 29/64 loss: -0.7470636367797852
Batch 30/64 loss: -0.7322998046875
Batch 31/64 loss: -0.6457481384277344
Batch 32/64 loss: -0.7668685913085938
Batch 33/64 loss: -0.4815526008605957
Batch 34/64 loss: 0.8331952095031738
Batch 35/64 loss: -0.7245078086853027
Batch 36/64 loss: -0.6939635276794434
Batch 37/64 loss: -0.7206859588623047
Batch 38/64 loss: -0.6017212867736816
Batch 39/64 loss: -0.8841209411621094
Batch 40/64 loss: -0.7952446937561035
Batch 41/64 loss: -0.7920923233032227
Batch 42/64 loss: -0.7771024703979492
Batch 43/64 loss: -0.4988284111022949
Batch 44/64 loss: -0.7515802383422852
Batch 45/64 loss: -0.6999220848083496
Batch 46/64 loss: -0.7124195098876953
Batch 47/64 loss: -0.7164101600646973
Batch 48/64 loss: -0.6652026176452637
Batch 49/64 loss: -0.5152831077575684
Batch 50/64 loss: -0.385526180267334
Batch 51/64 loss: -0.8333978652954102
Batch 52/64 loss: -0.7149157524108887
Batch 53/64 loss: -0.23579835891723633
Batch 54/64 loss: -0.5288505554199219
Batch 55/64 loss: -0.7991700172424316
Batch 56/64 loss: -0.5766372680664062
Batch 57/64 loss: -0.7343649864196777
Batch 58/64 loss: -0.6985769271850586
Batch 59/64 loss: -0.7268285751342773
Batch 60/64 loss: -0.6745848655700684
Batch 61/64 loss: -0.26538991928100586
Batch 62/64 loss: -0.7491016387939453
Batch 63/64 loss: -0.8178882598876953
Batch 64/64 loss: -4.559316635131836
Epoch 361  Train loss: -0.6123597762163948  Val loss: -1.015551458929003
Saving best model, epoch: 361
Epoch 362
-------------------------------
Batch 1/64 loss: -0.7725768089294434
Batch 2/64 loss: -0.7992949485778809
Batch 3/64 loss: -0.7261271476745605
Batch 4/64 loss: -0.8199853897094727
Batch 5/64 loss: -0.715606689453125
Batch 6/64 loss: -0.685274600982666
Batch 7/64 loss: -0.4034557342529297
Batch 8/64 loss: -0.7179403305053711
Batch 9/64 loss: -0.8913993835449219
Batch 10/64 loss: -0.6760048866271973
Batch 11/64 loss: -0.6730337142944336
Batch 12/64 loss: -0.8075528144836426
Batch 13/64 loss: -0.7312326431274414
Batch 14/64 loss: -0.685420036315918
Batch 15/64 loss: -0.8147063255310059
Batch 16/64 loss: -0.6656594276428223
Batch 17/64 loss: -0.7570233345031738
Batch 18/64 loss: -0.6264843940734863
Batch 19/64 loss: -0.6303920745849609
Batch 20/64 loss: -0.46827268600463867
Batch 21/64 loss: -0.6210618019104004
Batch 22/64 loss: -0.5925421714782715
Batch 23/64 loss: -0.7840480804443359
Batch 24/64 loss: -0.581751823425293
Batch 25/64 loss: -0.7014884948730469
Batch 26/64 loss: -0.6610784530639648
Batch 27/64 loss: -0.7241687774658203
Batch 28/64 loss: -0.5366334915161133
Batch 29/64 loss: -0.8234238624572754
Batch 30/64 loss: -0.7164301872253418
Batch 31/64 loss: -0.6274247169494629
Batch 32/64 loss: -0.6275372505187988
Batch 33/64 loss: -0.2039175033569336
Batch 34/64 loss: -0.8783941268920898
Batch 35/64 loss: -0.5678753852844238
Batch 36/64 loss: -0.6916294097900391
Batch 37/64 loss: -0.6919689178466797
Batch 38/64 loss: -0.6202249526977539
Batch 39/64 loss: 0.4277825355529785
Batch 40/64 loss: -0.630424976348877
Batch 41/64 loss: -0.7660479545593262
Batch 42/64 loss: -0.5796394348144531
Batch 43/64 loss: 0.4045543670654297
Batch 44/64 loss: -0.7779321670532227
Batch 45/64 loss: -0.6844577789306641
Batch 46/64 loss: -0.5089211463928223
Batch 47/64 loss: -0.5361099243164062
Batch 48/64 loss: -0.6976561546325684
Batch 49/64 loss: -0.8400487899780273
Batch 50/64 loss: -0.7763886451721191
Batch 51/64 loss: -0.7057104110717773
Batch 52/64 loss: -0.6321887969970703
Batch 53/64 loss: -0.7351865768432617
Batch 54/64 loss: -0.6692075729370117
Batch 55/64 loss: -0.5637502670288086
Batch 56/64 loss: -0.8096909523010254
Batch 57/64 loss: -0.6981825828552246
Batch 58/64 loss: 1.1149024963378906
Batch 59/64 loss: -0.0829019546508789
Batch 60/64 loss: -0.3550901412963867
Batch 61/64 loss: -0.7213196754455566
Batch 62/64 loss: -0.4695720672607422
Batch 63/64 loss: 0.16636037826538086
Batch 64/64 loss: -4.379262447357178
Epoch 362  Train loss: -0.6294951812893737  Val loss: -0.8809777223777115
Epoch 363
-------------------------------
Batch 1/64 loss: -0.6084136962890625
Batch 2/64 loss: -0.5430264472961426
Batch 3/64 loss: -0.6900296211242676
Batch 4/64 loss: -0.7433176040649414
Batch 5/64 loss: -0.6515660285949707
Batch 6/64 loss: -0.6402802467346191
Batch 7/64 loss: -0.318845272064209
Batch 8/64 loss: -0.5456390380859375
Batch 9/64 loss: -0.6190872192382812
Batch 10/64 loss: -0.5770010948181152
Batch 11/64 loss: -0.674532413482666
Batch 12/64 loss: -0.5054216384887695
Batch 13/64 loss: -0.4569864273071289
Batch 14/64 loss: -0.6664857864379883
Batch 15/64 loss: -0.08737659454345703
Batch 16/64 loss: -0.4915890693664551
Batch 17/64 loss: -0.5091886520385742
Batch 18/64 loss: -0.5691776275634766
Batch 19/64 loss: -0.7000875473022461
Batch 20/64 loss: 0.43262767791748047
Batch 21/64 loss: -0.5010995864868164
Batch 22/64 loss: -0.6826400756835938
Batch 23/64 loss: -0.7707662582397461
Batch 24/64 loss: -0.6592111587524414
Batch 25/64 loss: -0.49228811264038086
Batch 26/64 loss: -0.06665420532226562
Batch 27/64 loss: -0.7041363716125488
Batch 28/64 loss: -0.3804445266723633
Batch 29/64 loss: -0.31767702102661133
Batch 30/64 loss: -0.6116943359375
Batch 31/64 loss: -0.6829800605773926
Batch 32/64 loss: -0.6315655708312988
Batch 33/64 loss: -0.5603427886962891
Batch 34/64 loss: -0.25329160690307617
Batch 35/64 loss: -0.6294503211975098
Batch 36/64 loss: -0.6272549629211426
Batch 37/64 loss: -0.33379411697387695
Batch 38/64 loss: 0.03169059753417969
Batch 39/64 loss: -0.6895718574523926
Batch 40/64 loss: -0.457150936126709
Batch 41/64 loss: -0.4965534210205078
Batch 42/64 loss: 0.6140294075012207
Batch 43/64 loss: -0.6545066833496094
Batch 44/64 loss: 0.09000062942504883
Batch 45/64 loss: -0.44317102432250977
Batch 46/64 loss: -0.5487465858459473
Batch 47/64 loss: -0.663996696472168
Batch 48/64 loss: -0.6082253456115723
Batch 49/64 loss: -0.5761127471923828
Batch 50/64 loss: -0.5282120704650879
Batch 51/64 loss: -0.610684871673584
Batch 52/64 loss: -0.46549272537231445
Batch 53/64 loss: -0.5423188209533691
Batch 54/64 loss: -0.7280139923095703
Batch 55/64 loss: -0.2101602554321289
Batch 56/64 loss: 0.8350834846496582
Batch 57/64 loss: -0.42635440826416016
Batch 58/64 loss: -0.5206394195556641
Batch 59/64 loss: -0.5345401763916016
Batch 60/64 loss: -0.6445579528808594
Batch 61/64 loss: -0.7069458961486816
Batch 62/64 loss: -0.39595508575439453
Batch 63/64 loss: -0.6901021003723145
Batch 64/64 loss: -4.417193412780762
Epoch 363  Train loss: -0.5169383441700655  Val loss: -0.8991460505220079
Epoch 364
-------------------------------
Batch 1/64 loss: -0.44730567932128906
Batch 2/64 loss: -0.4542107582092285
Batch 3/64 loss: -0.10819196701049805
Batch 4/64 loss: -0.510251522064209
Batch 5/64 loss: -0.6397724151611328
Batch 6/64 loss: -0.7148561477661133
Batch 7/64 loss: -0.6093349456787109
Batch 8/64 loss: -0.7169609069824219
Batch 9/64 loss: -0.5789680480957031
Batch 10/64 loss: -0.5870108604431152
Batch 11/64 loss: -0.6651058197021484
Batch 12/64 loss: -0.6161336898803711
Batch 13/64 loss: -0.5743942260742188
Batch 14/64 loss: -0.6604461669921875
Batch 15/64 loss: -0.6477704048156738
Batch 16/64 loss: -0.7536373138427734
Batch 17/64 loss: -0.6585426330566406
Batch 18/64 loss: -0.55401611328125
Batch 19/64 loss: -0.7842178344726562
Batch 20/64 loss: -0.7147006988525391
Batch 21/64 loss: -0.654076099395752
Batch 22/64 loss: -0.4945192337036133
Batch 23/64 loss: -0.526275634765625
Batch 24/64 loss: -0.664283275604248
Batch 25/64 loss: -0.6565732955932617
Batch 26/64 loss: -0.5825896263122559
Batch 27/64 loss: -0.5913372039794922
Batch 28/64 loss: -0.6113653182983398
Batch 29/64 loss: -0.6980471611022949
Batch 30/64 loss: -0.5460166931152344
Batch 31/64 loss: -0.2977261543273926
Batch 32/64 loss: -0.5772929191589355
Batch 33/64 loss: -0.891268253326416
Batch 34/64 loss: 0.5187101364135742
Batch 35/64 loss: -0.6481623649597168
Batch 36/64 loss: -0.6380982398986816
Batch 37/64 loss: -0.7420668601989746
Batch 38/64 loss: -0.6489057540893555
Batch 39/64 loss: -0.4693136215209961
Batch 40/64 loss: -0.6811914443969727
Batch 41/64 loss: -0.6022372245788574
Batch 42/64 loss: -0.590604305267334
Batch 43/64 loss: -0.7342214584350586
Batch 44/64 loss: -0.7295994758605957
Batch 45/64 loss: -0.7750754356384277
Batch 46/64 loss: -0.6827454566955566
Batch 47/64 loss: -0.2201533317565918
Batch 48/64 loss: 0.04586315155029297
Batch 49/64 loss: -0.7351188659667969
Batch 50/64 loss: -0.5714173316955566
Batch 51/64 loss: -0.532069206237793
Batch 52/64 loss: -0.3533000946044922
Batch 53/64 loss: 0.9226808547973633
Batch 54/64 loss: -0.6139287948608398
Batch 55/64 loss: -0.5732998847961426
Batch 56/64 loss: 0.22515392303466797
Batch 57/64 loss: -0.4203944206237793
Batch 58/64 loss: -0.7412910461425781
Batch 59/64 loss: 0.13257837295532227
Batch 60/64 loss: -0.6790509223937988
Batch 61/64 loss: -0.8607892990112305
Batch 62/64 loss: -0.6268725395202637
Batch 63/64 loss: -0.6493978500366211
Batch 64/64 loss: -4.26861047744751
Epoch 364  Train loss: -0.5751055006887399  Val loss: -0.9209322192005276
Epoch 365
-------------------------------
Batch 1/64 loss: -0.6443252563476562
Batch 2/64 loss: -0.7910833358764648
Batch 3/64 loss: -0.480621337890625
Batch 4/64 loss: -0.7034063339233398
Batch 5/64 loss: -0.7624301910400391
Batch 6/64 loss: -0.6136488914489746
Batch 7/64 loss: -0.722130298614502
Batch 8/64 loss: -0.31228017807006836
Batch 9/64 loss: -0.6266360282897949
Batch 10/64 loss: -0.643425464630127
Batch 11/64 loss: -0.6582698822021484
Batch 12/64 loss: -0.5834503173828125
Batch 13/64 loss: -0.6962347030639648
Batch 14/64 loss: 1.678164005279541
Batch 15/64 loss: -0.7620501518249512
Batch 16/64 loss: -0.6096549034118652
Batch 17/64 loss: -0.5780820846557617
Batch 18/64 loss: -0.5056595802307129
Batch 19/64 loss: -0.4539480209350586
Batch 20/64 loss: -0.5170445442199707
Batch 21/64 loss: -0.5340609550476074
Batch 22/64 loss: -0.6821494102478027
Batch 23/64 loss: -0.7427310943603516
Batch 24/64 loss: -0.6724753379821777
Batch 25/64 loss: -0.08598709106445312
Batch 26/64 loss: -0.7965974807739258
Batch 27/64 loss: -0.6013803482055664
Batch 28/64 loss: -0.7148356437683105
Batch 29/64 loss: -0.6299600601196289
Batch 30/64 loss: -0.6019096374511719
Batch 31/64 loss: -0.5627779960632324
Batch 32/64 loss: -0.6390843391418457
Batch 33/64 loss: -0.7203679084777832
Batch 34/64 loss: -0.7540998458862305
Batch 35/64 loss: -0.639986515045166
Batch 36/64 loss: -0.7594494819641113
Batch 37/64 loss: -0.7157187461853027
Batch 38/64 loss: -0.713719367980957
Batch 39/64 loss: -0.3288393020629883
Batch 40/64 loss: -0.7783536911010742
Batch 41/64 loss: -0.6553244590759277
Batch 42/64 loss: -0.7509613037109375
Batch 43/64 loss: 0.7241740226745605
Batch 44/64 loss: -0.7887306213378906
Batch 45/64 loss: -0.8355145454406738
Batch 46/64 loss: -0.47008752822875977
Batch 47/64 loss: -0.6375293731689453
Batch 48/64 loss: -0.752906322479248
Batch 49/64 loss: -0.35124635696411133
Batch 50/64 loss: -0.8027477264404297
Batch 51/64 loss: -0.8108339309692383
Batch 52/64 loss: -0.8003602027893066
Batch 53/64 loss: -0.8175954818725586
Batch 54/64 loss: 0.2098097801208496
Batch 55/64 loss: -0.7712564468383789
Batch 56/64 loss: -0.7186174392700195
Batch 57/64 loss: -0.8367033004760742
Batch 58/64 loss: -0.6696653366088867
Batch 59/64 loss: -0.7487106323242188
Batch 60/64 loss: -0.82086181640625
Batch 61/64 loss: -0.6962852478027344
Batch 62/64 loss: -0.599311351776123
Batch 63/64 loss: -0.7883830070495605
Batch 64/64 loss: -4.074770927429199
Epoch 365  Train loss: -0.6259831933414235  Val loss: -1.0454140102740415
Saving best model, epoch: 365
Epoch 366
-------------------------------
Batch 1/64 loss: -0.6733536720275879
Batch 2/64 loss: -0.6494755744934082
Batch 3/64 loss: -0.5836267471313477
Batch 4/64 loss: -0.4666423797607422
Batch 5/64 loss: -0.7198529243469238
Batch 6/64 loss: -0.790398120880127
Batch 7/64 loss: -0.5935807228088379
Batch 8/64 loss: -0.5439176559448242
Batch 9/64 loss: -0.499082088470459
Batch 10/64 loss: -0.3940110206604004
Batch 11/64 loss: -0.7977242469787598
Batch 12/64 loss: -0.7768678665161133
Batch 13/64 loss: -0.7305669784545898
Batch 14/64 loss: -0.5850691795349121
Batch 15/64 loss: -0.847928524017334
Batch 16/64 loss: 0.34234189987182617
Batch 17/64 loss: 0.06516695022583008
Batch 18/64 loss: -0.5302233695983887
Batch 19/64 loss: -0.7865824699401855
Batch 20/64 loss: -0.5233645439147949
Batch 21/64 loss: -0.7599101066589355
Batch 22/64 loss: -0.7485113143920898
Batch 23/64 loss: -0.5207538604736328
Batch 24/64 loss: -0.729428768157959
Batch 25/64 loss: -0.6169295310974121
Batch 26/64 loss: -0.7178187370300293
Batch 27/64 loss: -0.7277135848999023
Batch 28/64 loss: -0.7741594314575195
Batch 29/64 loss: -0.30579710006713867
Batch 30/64 loss: -0.32393550872802734
Batch 31/64 loss: -0.8527793884277344
Batch 32/64 loss: -0.8141560554504395
Batch 33/64 loss: -0.7383880615234375
Batch 34/64 loss: -0.689847469329834
Batch 35/64 loss: -0.5571317672729492
Batch 36/64 loss: -0.7281432151794434
Batch 37/64 loss: -0.7592625617980957
Batch 38/64 loss: -0.6842198371887207
Batch 39/64 loss: -0.5732336044311523
Batch 40/64 loss: -0.7196474075317383
Batch 41/64 loss: -0.7049002647399902
Batch 42/64 loss: -0.7073640823364258
Batch 43/64 loss: -0.8291893005371094
Batch 44/64 loss: -0.7982063293457031
Batch 45/64 loss: -0.5954751968383789
Batch 46/64 loss: 0.3037867546081543
Batch 47/64 loss: -0.5940842628479004
Batch 48/64 loss: -0.6735525131225586
Batch 49/64 loss: -0.6760053634643555
Batch 50/64 loss: -0.6862788200378418
Batch 51/64 loss: -0.7520046234130859
Batch 52/64 loss: -0.5782942771911621
Batch 53/64 loss: -0.5434188842773438
Batch 54/64 loss: -0.5215492248535156
Batch 55/64 loss: -0.6529684066772461
Batch 56/64 loss: -0.32794952392578125
Batch 57/64 loss: -0.5645785331726074
Batch 58/64 loss: -0.6730833053588867
Batch 59/64 loss: -0.11387348175048828
Batch 60/64 loss: -0.5251779556274414
Batch 61/64 loss: -0.4266490936279297
Batch 62/64 loss: -0.7279534339904785
Batch 63/64 loss: -0.5037593841552734
Batch 64/64 loss: -2.144537925720215
Epoch 366  Train loss: -0.6103130901561064  Val loss: -0.9388275933019894
Epoch 367
-------------------------------
Batch 1/64 loss: -0.793642520904541
Batch 2/64 loss: -0.7752017974853516
Batch 3/64 loss: -0.845611572265625
Batch 4/64 loss: -0.822303295135498
Batch 5/64 loss: -0.7456049919128418
Batch 6/64 loss: -0.6911959648132324
Batch 7/64 loss: -0.6495981216430664
Batch 8/64 loss: -0.48008012771606445
Batch 9/64 loss: -0.6518349647521973
Batch 10/64 loss: 0.5470132827758789
Batch 11/64 loss: -0.23135757446289062
Batch 12/64 loss: -0.7362194061279297
Batch 13/64 loss: -0.8176445960998535
Batch 14/64 loss: -0.5915236473083496
Batch 15/64 loss: -0.639225959777832
Batch 16/64 loss: -0.8181099891662598
Batch 17/64 loss: -0.5567936897277832
Batch 18/64 loss: -0.4907407760620117
Batch 19/64 loss: -0.6343698501586914
Batch 20/64 loss: -0.2422347068786621
Batch 21/64 loss: -0.7309446334838867
Batch 22/64 loss: -0.7080159187316895
Batch 23/64 loss: -0.6792540550231934
Batch 24/64 loss: -0.5092744827270508
Batch 25/64 loss: -0.6963891983032227
Batch 26/64 loss: -0.7148528099060059
Batch 27/64 loss: -0.5510005950927734
Batch 28/64 loss: -0.16209983825683594
Batch 29/64 loss: -0.7017636299133301
Batch 30/64 loss: -0.7369122505187988
Batch 31/64 loss: -0.6659150123596191
Batch 32/64 loss: -0.7350492477416992
Batch 33/64 loss: -0.3896164894104004
Batch 34/64 loss: -0.6910495758056641
Batch 35/64 loss: -0.6641097068786621
Batch 36/64 loss: 0.544921875
Batch 37/64 loss: -0.6665587425231934
Batch 38/64 loss: -0.7704672813415527
Batch 39/64 loss: -0.6889734268188477
Batch 40/64 loss: -0.7179346084594727
Batch 41/64 loss: -0.1848764419555664
Batch 42/64 loss: -0.47654294967651367
Batch 43/64 loss: -0.27446842193603516
Batch 44/64 loss: -0.750239372253418
Batch 45/64 loss: -0.792722225189209
Batch 46/64 loss: -0.6049633026123047
Batch 47/64 loss: -0.7178440093994141
Batch 48/64 loss: -0.42969369888305664
Batch 49/64 loss: -0.7260665893554688
Batch 50/64 loss: -0.5072293281555176
Batch 51/64 loss: -0.6040945053100586
Batch 52/64 loss: 0.7173252105712891
Batch 53/64 loss: -0.7599177360534668
Batch 54/64 loss: -0.38420772552490234
Batch 55/64 loss: -0.6331343650817871
Batch 56/64 loss: -0.7675247192382812
Batch 57/64 loss: -0.7608704566955566
Batch 58/64 loss: -0.7050466537475586
Batch 59/64 loss: -0.6524372100830078
Batch 60/64 loss: -0.5751285552978516
Batch 61/64 loss: -0.5946846008300781
Batch 62/64 loss: -0.8120360374450684
Batch 63/64 loss: -0.8911628723144531
Batch 64/64 loss: -4.448145389556885
Epoch 367  Train loss: -0.6200033804949592  Val loss: -0.951272603982093
Epoch 368
-------------------------------
Batch 1/64 loss: -0.6726489067077637
Batch 2/64 loss: -0.7446932792663574
Batch 3/64 loss: -0.706850528717041
Batch 4/64 loss: -0.8731222152709961
Batch 5/64 loss: -0.7761349678039551
Batch 6/64 loss: -0.5332527160644531
Batch 7/64 loss: -0.7391729354858398
Batch 8/64 loss: -0.4801912307739258
Batch 9/64 loss: -0.6379055976867676
Batch 10/64 loss: -0.6236858367919922
Batch 11/64 loss: -0.7106943130493164
Batch 12/64 loss: -0.6925325393676758
Batch 13/64 loss: -0.7343668937683105
Batch 14/64 loss: -0.4925355911254883
Batch 15/64 loss: -0.7405848503112793
Batch 16/64 loss: -0.6766819953918457
Batch 17/64 loss: -0.6635246276855469
Batch 18/64 loss: -0.7313337326049805
Batch 19/64 loss: -0.6166925430297852
Batch 20/64 loss: 1.4593992233276367
Batch 21/64 loss: -0.7539277076721191
Batch 22/64 loss: -0.3954448699951172
Batch 23/64 loss: -0.38333797454833984
Batch 24/64 loss: -0.24878501892089844
Batch 25/64 loss: -0.1883072853088379
Batch 26/64 loss: -0.7768454551696777
Batch 27/64 loss: -0.5289607048034668
Batch 28/64 loss: -0.4035639762878418
Batch 29/64 loss: -0.1988377571105957
Batch 30/64 loss: -0.42431116104125977
Batch 31/64 loss: -0.5663275718688965
Batch 32/64 loss: 0.5281796455383301
Batch 33/64 loss: -0.31975793838500977
Batch 34/64 loss: -0.667931079864502
Batch 35/64 loss: -0.4657588005065918
Batch 36/64 loss: -0.33969926834106445
Batch 37/64 loss: -0.5419220924377441
Batch 38/64 loss: -0.3397688865661621
Batch 39/64 loss: -0.42166709899902344
Batch 40/64 loss: -0.09288549423217773
Batch 41/64 loss: -0.43409061431884766
Batch 42/64 loss: -0.5881133079528809
Batch 43/64 loss: -0.5181965827941895
Batch 44/64 loss: -0.4851856231689453
Batch 45/64 loss: -0.5087862014770508
Batch 46/64 loss: -0.533658504486084
Batch 47/64 loss: -0.6943154335021973
Batch 48/64 loss: -0.5489301681518555
Batch 49/64 loss: -0.5447239875793457
Batch 50/64 loss: -0.5032544136047363
Batch 51/64 loss: -0.6230907440185547
Batch 52/64 loss: -0.7134809494018555
Batch 53/64 loss: -0.6977400779724121
Batch 54/64 loss: -0.42821598052978516
Batch 55/64 loss: -0.7004384994506836
Batch 56/64 loss: -0.5902585983276367
Batch 57/64 loss: 0.5252256393432617
Batch 58/64 loss: -0.5333976745605469
Batch 59/64 loss: 0.022705078125
Batch 60/64 loss: -0.44225025177001953
Batch 61/64 loss: -0.054534912109375
Batch 62/64 loss: -0.5766658782958984
Batch 63/64 loss: -0.11118936538696289
Batch 64/64 loss: -4.142387866973877
Epoch 368  Train loss: -0.5067677572661755  Val loss: -0.8080593515507544
Epoch 369
-------------------------------
Batch 1/64 loss: -0.5579500198364258
Batch 2/64 loss: -0.6320929527282715
Batch 3/64 loss: -0.5181422233581543
Batch 4/64 loss: -0.6091341972351074
Batch 5/64 loss: -0.7004842758178711
Batch 6/64 loss: -0.5718560218811035
Batch 7/64 loss: -0.7384705543518066
Batch 8/64 loss: -0.31772661209106445
Batch 9/64 loss: -0.7736663818359375
Batch 10/64 loss: -0.35083627700805664
Batch 11/64 loss: -0.6799468994140625
Batch 12/64 loss: -0.4631342887878418
Batch 13/64 loss: -0.656517505645752
Batch 14/64 loss: -0.6889715194702148
Batch 15/64 loss: -0.46027135848999023
Batch 16/64 loss: -0.6863527297973633
Batch 17/64 loss: -0.7841506004333496
Batch 18/64 loss: -0.3872346878051758
Batch 19/64 loss: -0.7945570945739746
Batch 20/64 loss: -0.666254997253418
Batch 21/64 loss: -0.5052618980407715
Batch 22/64 loss: -0.7449374198913574
Batch 23/64 loss: -0.22066736221313477
Batch 24/64 loss: -0.8536882400512695
Batch 25/64 loss: -0.7608451843261719
Batch 26/64 loss: -0.5772008895874023
Batch 27/64 loss: -0.6997628211975098
Batch 28/64 loss: -0.5538797378540039
Batch 29/64 loss: 0.5466408729553223
Batch 30/64 loss: -0.6341180801391602
Batch 31/64 loss: -0.7875885963439941
Batch 32/64 loss: -0.7660012245178223
Batch 33/64 loss: -0.7454848289489746
Batch 34/64 loss: -0.566565990447998
Batch 35/64 loss: -0.7974433898925781
Batch 36/64 loss: -0.5907115936279297
Batch 37/64 loss: -0.4536590576171875
Batch 38/64 loss: -0.6982207298278809
Batch 39/64 loss: -0.8247799873352051
Batch 40/64 loss: -0.6675348281860352
Batch 41/64 loss: -0.09061050415039062
Batch 42/64 loss: -0.7553000450134277
Batch 43/64 loss: -0.6902194023132324
Batch 44/64 loss: -0.7231216430664062
Batch 45/64 loss: -0.7044467926025391
Batch 46/64 loss: -0.1579909324645996
Batch 47/64 loss: 0.3039069175720215
Batch 48/64 loss: -0.8547940254211426
Batch 49/64 loss: -0.6728639602661133
Batch 50/64 loss: -0.6921544075012207
Batch 51/64 loss: -0.11851978302001953
Batch 52/64 loss: -0.7708621025085449
Batch 53/64 loss: -0.6927862167358398
Batch 54/64 loss: -0.6457638740539551
Batch 55/64 loss: -0.5499758720397949
Batch 56/64 loss: -0.7576799392700195
Batch 57/64 loss: -0.6995372772216797
Batch 58/64 loss: -0.6819801330566406
Batch 59/64 loss: -0.6068038940429688
Batch 60/64 loss: -0.6785941123962402
Batch 61/64 loss: -0.7490215301513672
Batch 62/64 loss: -0.8388819694519043
Batch 63/64 loss: 0.9021282196044922
Batch 64/64 loss: -4.410173416137695
Epoch 369  Train loss: -0.6144778906130324  Val loss: -1.0098024872979758
Epoch 370
-------------------------------
Batch 1/64 loss: -0.8260760307312012
Batch 2/64 loss: 0.3779716491699219
Batch 3/64 loss: -0.6830935478210449
Batch 4/64 loss: -0.5374603271484375
Batch 5/64 loss: -0.7118749618530273
Batch 6/64 loss: -0.6223735809326172
Batch 7/64 loss: -0.5353503227233887
Batch 8/64 loss: -0.6741619110107422
Batch 9/64 loss: -0.8045797348022461
Batch 10/64 loss: -0.3715987205505371
Batch 11/64 loss: -0.7063865661621094
Batch 12/64 loss: -0.7721796035766602
Batch 13/64 loss: -0.7455644607543945
Batch 14/64 loss: -0.8850922584533691
Batch 15/64 loss: -0.6929426193237305
Batch 16/64 loss: -0.6298046112060547
Batch 17/64 loss: -0.7904438972473145
Batch 18/64 loss: -0.5912070274353027
Batch 19/64 loss: -0.7017002105712891
Batch 20/64 loss: -0.3715648651123047
Batch 21/64 loss: -0.6241154670715332
Batch 22/64 loss: -0.7619671821594238
Batch 23/64 loss: -0.4230952262878418
Batch 24/64 loss: -0.804466724395752
Batch 25/64 loss: -0.6557159423828125
Batch 26/64 loss: -0.6102471351623535
Batch 27/64 loss: -0.7143182754516602
Batch 28/64 loss: -0.6369028091430664
Batch 29/64 loss: -0.6563224792480469
Batch 30/64 loss: -0.5251913070678711
Batch 31/64 loss: -0.1946702003479004
Batch 32/64 loss: -0.4100313186645508
Batch 33/64 loss: 0.49236202239990234
Batch 34/64 loss: -0.4723515510559082
Batch 35/64 loss: -0.7219810485839844
Batch 36/64 loss: -0.6826419830322266
Batch 37/64 loss: -0.554408073425293
Batch 38/64 loss: -0.5941462516784668
Batch 39/64 loss: -0.5237808227539062
Batch 40/64 loss: -0.43623971939086914
Batch 41/64 loss: -0.6243772506713867
Batch 42/64 loss: -0.15270233154296875
Batch 43/64 loss: -0.549220085144043
Batch 44/64 loss: -0.5106210708618164
Batch 45/64 loss: -0.6380352973937988
Batch 46/64 loss: -0.5485086441040039
Batch 47/64 loss: -0.5055336952209473
Batch 48/64 loss: -0.1672649383544922
Batch 49/64 loss: -0.14864778518676758
Batch 50/64 loss: -0.4546804428100586
Batch 51/64 loss: -0.6069841384887695
Batch 52/64 loss: -0.38054609298706055
Batch 53/64 loss: -0.6361889839172363
Batch 54/64 loss: -0.07214498519897461
Batch 55/64 loss: -0.6462979316711426
Batch 56/64 loss: 0.3326530456542969
Batch 57/64 loss: -0.659276008605957
Batch 58/64 loss: -0.7187790870666504
Batch 59/64 loss: -0.5718774795532227
Batch 60/64 loss: 0.8725886344909668
Batch 61/64 loss: -0.5194687843322754
Batch 62/64 loss: -0.7468295097351074
Batch 63/64 loss: -0.7054386138916016
Batch 64/64 loss: -4.147889614105225
Epoch 370  Train loss: -0.5530166869070016  Val loss: -0.9007910633414882
Epoch 371
-------------------------------
Batch 1/64 loss: -0.4938802719116211
Batch 2/64 loss: 0.0018587112426757812
Batch 3/64 loss: -0.6867532730102539
Batch 4/64 loss: -0.515470027923584
Batch 5/64 loss: -0.623936653137207
Batch 6/64 loss: -0.6522350311279297
Batch 7/64 loss: -0.6660685539245605
Batch 8/64 loss: 0.7929172515869141
Batch 9/64 loss: -0.5670514106750488
Batch 10/64 loss: -0.6260519027709961
Batch 11/64 loss: -0.5156865119934082
Batch 12/64 loss: -0.2714195251464844
Batch 13/64 loss: 0.04517841339111328
Batch 14/64 loss: -0.7002406120300293
Batch 15/64 loss: -0.7142243385314941
Batch 16/64 loss: -0.5476446151733398
Batch 17/64 loss: -0.7097001075744629
Batch 18/64 loss: -0.6490035057067871
Batch 19/64 loss: -0.6210970878601074
Batch 20/64 loss: -0.6843380928039551
Batch 21/64 loss: -0.6508526802062988
Batch 22/64 loss: -0.5234456062316895
Batch 23/64 loss: -0.7516536712646484
Batch 24/64 loss: -0.7713241577148438
Batch 25/64 loss: -0.6951990127563477
Batch 26/64 loss: -0.5930724143981934
Batch 27/64 loss: -0.8199844360351562
Batch 28/64 loss: -0.740450382232666
Batch 29/64 loss: -0.7571330070495605
Batch 30/64 loss: -0.525907039642334
Batch 31/64 loss: -0.5795197486877441
Batch 32/64 loss: -0.7908434867858887
Batch 33/64 loss: -0.5822362899780273
Batch 34/64 loss: -0.7356672286987305
Batch 35/64 loss: -0.46587467193603516
Batch 36/64 loss: -0.8196601867675781
Batch 37/64 loss: -0.5389480590820312
Batch 38/64 loss: -0.41646480560302734
Batch 39/64 loss: -0.8046145439147949
Batch 40/64 loss: -0.8124914169311523
Batch 41/64 loss: -0.6388154029846191
Batch 42/64 loss: -0.19818735122680664
Batch 43/64 loss: -0.6375746726989746
Batch 44/64 loss: 0.48974609375
Batch 45/64 loss: -0.5943121910095215
Batch 46/64 loss: -0.8003592491149902
Batch 47/64 loss: -0.6669402122497559
Batch 48/64 loss: -0.7425236701965332
Batch 49/64 loss: 0.4340667724609375
Batch 50/64 loss: -0.4688849449157715
Batch 51/64 loss: -0.5305895805358887
Batch 52/64 loss: -0.6515202522277832
Batch 53/64 loss: -0.3603081703186035
Batch 54/64 loss: -0.7087588310241699
Batch 55/64 loss: -0.6433062553405762
Batch 56/64 loss: -0.6641931533813477
Batch 57/64 loss: -0.729252815246582
Batch 58/64 loss: -0.7200837135314941
Batch 59/64 loss: -0.5004854202270508
Batch 60/64 loss: -0.7981700897216797
Batch 61/64 loss: -0.7467174530029297
Batch 62/64 loss: -0.5997881889343262
Batch 63/64 loss: -0.8269929885864258
Batch 64/64 loss: -4.334806442260742
Epoch 371  Train loss: -0.6013372009875728  Val loss: -0.956351906163586
Epoch 372
-------------------------------
Batch 1/64 loss: -0.6855392456054688
Batch 2/64 loss: -0.6852126121520996
Batch 3/64 loss: -0.7622165679931641
Batch 4/64 loss: -0.7420310974121094
Batch 5/64 loss: -0.7211089134216309
Batch 6/64 loss: -0.8284568786621094
Batch 7/64 loss: -0.5892457962036133
Batch 8/64 loss: -0.8201541900634766
Batch 9/64 loss: -0.29375791549682617
Batch 10/64 loss: -0.7318887710571289
Batch 11/64 loss: -0.7668309211730957
Batch 12/64 loss: -0.4057807922363281
Batch 13/64 loss: 0.3389253616333008
Batch 14/64 loss: -0.5142192840576172
Batch 15/64 loss: -0.5844101905822754
Batch 16/64 loss: 0.11453485488891602
Batch 17/64 loss: -0.5836248397827148
Batch 18/64 loss: -0.5080828666687012
Batch 19/64 loss: -0.843024730682373
Batch 20/64 loss: -0.7176547050476074
Batch 21/64 loss: -0.7488484382629395
Batch 22/64 loss: -0.3406686782836914
Batch 23/64 loss: 0.17382478713989258
Batch 24/64 loss: -0.7446455955505371
Batch 25/64 loss: -0.6170334815979004
Batch 26/64 loss: -0.7757201194763184
Batch 27/64 loss: -0.6747279167175293
Batch 28/64 loss: -0.7855448722839355
Batch 29/64 loss: -0.6850848197937012
Batch 30/64 loss: -0.47751665115356445
Batch 31/64 loss: -0.8629841804504395
Batch 32/64 loss: -0.8068547248840332
Batch 33/64 loss: -0.5664339065551758
Batch 34/64 loss: -0.6657905578613281
Batch 35/64 loss: -0.6439414024353027
Batch 36/64 loss: -0.873509407043457
Batch 37/64 loss: -0.4882693290710449
Batch 38/64 loss: -0.33106422424316406
Batch 39/64 loss: -0.6194424629211426
Batch 40/64 loss: -0.7276902198791504
Batch 41/64 loss: -0.7547330856323242
Batch 42/64 loss: -0.6072163581848145
Batch 43/64 loss: -0.19085121154785156
Batch 44/64 loss: -0.5646567344665527
Batch 45/64 loss: 0.6306920051574707
Batch 46/64 loss: -0.6379060745239258
Batch 47/64 loss: -0.6816649436950684
Batch 48/64 loss: -0.7047858238220215
Batch 49/64 loss: -0.33336544036865234
Batch 50/64 loss: -0.5438499450683594
Batch 51/64 loss: -0.7424750328063965
Batch 52/64 loss: -0.711402416229248
Batch 53/64 loss: -0.5247364044189453
Batch 54/64 loss: -0.7296023368835449
Batch 55/64 loss: -0.47584056854248047
Batch 56/64 loss: -0.33208322525024414
Batch 57/64 loss: -0.3245248794555664
Batch 58/64 loss: -0.5607976913452148
Batch 59/64 loss: -0.7815895080566406
Batch 60/64 loss: -0.6774444580078125
Batch 61/64 loss: -0.5684432983398438
Batch 62/64 loss: 0.8194141387939453
Batch 63/64 loss: -0.7686285972595215
Batch 64/64 loss: -4.404752254486084
Epoch 372  Train loss: -0.5907730570026473  Val loss: -0.9424451847666317
Epoch 373
-------------------------------
Batch 1/64 loss: -0.5273785591125488
Batch 2/64 loss: -0.5698814392089844
Batch 3/64 loss: -0.6703405380249023
Batch 4/64 loss: -0.4193077087402344
Batch 5/64 loss: -0.8241314888000488
Batch 6/64 loss: -0.5689001083374023
Batch 7/64 loss: -0.6833229064941406
Batch 8/64 loss: -0.46454668045043945
Batch 9/64 loss: -0.7983741760253906
Batch 10/64 loss: -0.4078068733215332
Batch 11/64 loss: -0.7601699829101562
Batch 12/64 loss: -0.7431879043579102
Batch 13/64 loss: -0.7873210906982422
Batch 14/64 loss: -0.4466977119445801
Batch 15/64 loss: -0.8960871696472168
Batch 16/64 loss: -0.8215765953063965
Batch 17/64 loss: -0.7503604888916016
Batch 18/64 loss: -0.7374787330627441
Batch 19/64 loss: -0.8575968742370605
Batch 20/64 loss: -0.8794879913330078
Batch 21/64 loss: -0.7022228240966797
Batch 22/64 loss: -0.5721249580383301
Batch 23/64 loss: -0.7714858055114746
Batch 24/64 loss: -0.4960746765136719
Batch 25/64 loss: -0.7340278625488281
Batch 26/64 loss: -0.35877323150634766
Batch 27/64 loss: -0.7686104774475098
Batch 28/64 loss: -0.6103024482727051
Batch 29/64 loss: -0.5751752853393555
Batch 30/64 loss: 0.07429647445678711
Batch 31/64 loss: 0.4595785140991211
Batch 32/64 loss: 0.1663203239440918
Batch 33/64 loss: -0.61383056640625
Batch 34/64 loss: -0.5531253814697266
Batch 35/64 loss: -0.458953857421875
Batch 36/64 loss: -0.4051370620727539
Batch 37/64 loss: -0.22320556640625
Batch 38/64 loss: -0.4813413619995117
Batch 39/64 loss: -0.5985941886901855
Batch 40/64 loss: -0.6591639518737793
Batch 41/64 loss: -0.6535029411315918
Batch 42/64 loss: -0.6867055892944336
Batch 43/64 loss: -0.6047616004943848
Batch 44/64 loss: -0.6000556945800781
Batch 45/64 loss: 0.5848608016967773
Batch 46/64 loss: -0.16843366622924805
Batch 47/64 loss: -0.6608777046203613
Batch 48/64 loss: -0.6925058364868164
Batch 49/64 loss: -0.7315702438354492
Batch 50/64 loss: -0.6642217636108398
Batch 51/64 loss: -0.5656213760375977
Batch 52/64 loss: 0.8962359428405762
Batch 53/64 loss: -0.7038812637329102
Batch 54/64 loss: -0.6532416343688965
Batch 55/64 loss: -0.37496185302734375
Batch 56/64 loss: -0.600287914276123
Batch 57/64 loss: -0.5557694435119629
Batch 58/64 loss: -0.6135954856872559
Batch 59/64 loss: -0.7941102981567383
Batch 60/64 loss: -0.4982881546020508
Batch 61/64 loss: -0.6877574920654297
Batch 62/64 loss: -0.591242790222168
Batch 63/64 loss: -0.1106882095336914
Batch 64/64 loss: -3.7749104499816895
Epoch 373  Train loss: -0.5656168825486128  Val loss: -0.8233510440157861
Epoch 374
-------------------------------
Batch 1/64 loss: -0.4644608497619629
Batch 2/64 loss: -0.5492010116577148
Batch 3/64 loss: -0.7144632339477539
Batch 4/64 loss: -0.722224235534668
Batch 5/64 loss: -0.5334482192993164
Batch 6/64 loss: -0.7414464950561523
Batch 7/64 loss: -0.4003019332885742
Batch 8/64 loss: -0.7178635597229004
Batch 9/64 loss: -0.3507084846496582
Batch 10/64 loss: -0.682703971862793
Batch 11/64 loss: -0.5482892990112305
Batch 12/64 loss: -0.7275352478027344
Batch 13/64 loss: -0.670865535736084
Batch 14/64 loss: -0.3478703498840332
Batch 15/64 loss: -0.18509578704833984
Batch 16/64 loss: -0.5245919227600098
Batch 17/64 loss: -0.490325927734375
Batch 18/64 loss: -0.43379688262939453
Batch 19/64 loss: -0.7878193855285645
Batch 20/64 loss: -0.6875739097595215
Batch 21/64 loss: -0.2598428726196289
Batch 22/64 loss: -0.695167064666748
Batch 23/64 loss: -0.673743724822998
Batch 24/64 loss: -0.7278575897216797
Batch 25/64 loss: -0.6953945159912109
Batch 26/64 loss: -0.6625990867614746
Batch 27/64 loss: -0.7283482551574707
Batch 28/64 loss: -0.7520995140075684
Batch 29/64 loss: -0.45111942291259766
Batch 30/64 loss: -0.6513557434082031
Batch 31/64 loss: 0.31847476959228516
Batch 32/64 loss: -0.13646602630615234
Batch 33/64 loss: -0.5222854614257812
Batch 34/64 loss: -0.5811424255371094
Batch 35/64 loss: -0.7239174842834473
Batch 36/64 loss: -0.8160591125488281
Batch 37/64 loss: -0.14031219482421875
Batch 38/64 loss: -0.6644940376281738
Batch 39/64 loss: -0.5983071327209473
Batch 40/64 loss: -0.07805871963500977
Batch 41/64 loss: -0.6702919006347656
Batch 42/64 loss: -0.7149801254272461
Batch 43/64 loss: -0.5119624137878418
Batch 44/64 loss: -0.34320783615112305
Batch 45/64 loss: -0.5959429740905762
Batch 46/64 loss: -0.40264892578125
Batch 47/64 loss: -0.730125904083252
Batch 48/64 loss: -0.5741643905639648
Batch 49/64 loss: -0.7253427505493164
Batch 50/64 loss: -0.206298828125
Batch 51/64 loss: -0.7585310935974121
Batch 52/64 loss: -0.793306827545166
Batch 53/64 loss: 1.4648494720458984
Batch 54/64 loss: -0.5886902809143066
Batch 55/64 loss: 0.5848126411437988
Batch 56/64 loss: -0.04862499237060547
Batch 57/64 loss: -0.5339832305908203
Batch 58/64 loss: -0.7301268577575684
Batch 59/64 loss: -0.7150282859802246
Batch 60/64 loss: -0.417050838470459
Batch 61/64 loss: -0.758150577545166
Batch 62/64 loss: 0.25873422622680664
Batch 63/64 loss: -0.7052721977233887
Batch 64/64 loss: -4.332687854766846
Epoch 374  Train loss: -0.5331063943750718  Val loss: -0.86782941785465
Epoch 375
-------------------------------
Batch 1/64 loss: -0.7077374458312988
Batch 2/64 loss: -0.6942648887634277
Batch 3/64 loss: -0.43042993545532227
Batch 4/64 loss: -0.48389101028442383
Batch 5/64 loss: -0.47989749908447266
Batch 6/64 loss: 0.8267927169799805
Batch 7/64 loss: -0.5957880020141602
Batch 8/64 loss: -0.5232348442077637
Batch 9/64 loss: -0.6966094970703125
Batch 10/64 loss: -0.7379117012023926
Batch 11/64 loss: -0.6537127494812012
Batch 12/64 loss: -0.4810981750488281
Batch 13/64 loss: -0.7451171875
Batch 14/64 loss: -0.8596253395080566
Batch 15/64 loss: -0.7487196922302246
Batch 16/64 loss: -0.8098740577697754
Batch 17/64 loss: -0.7672595977783203
Batch 18/64 loss: 0.5806398391723633
Batch 19/64 loss: -0.6879515647888184
Batch 20/64 loss: -0.8148078918457031
Batch 21/64 loss: -0.44143152236938477
Batch 22/64 loss: -0.675135612487793
Batch 23/64 loss: -0.7386941909790039
Batch 24/64 loss: -0.6573076248168945
Batch 25/64 loss: -0.7176580429077148
Batch 26/64 loss: -0.6411609649658203
Batch 27/64 loss: -0.5640726089477539
Batch 28/64 loss: -0.6788325309753418
Batch 29/64 loss: -0.38353538513183594
Batch 30/64 loss: -0.7825803756713867
Batch 31/64 loss: -0.7250161170959473
Batch 32/64 loss: -0.5444478988647461
Batch 33/64 loss: -0.7467012405395508
Batch 34/64 loss: -0.7043781280517578
Batch 35/64 loss: -0.5383968353271484
Batch 36/64 loss: -0.5302567481994629
Batch 37/64 loss: 0.5093355178833008
Batch 38/64 loss: -0.5800414085388184
Batch 39/64 loss: -0.6685242652893066
Batch 40/64 loss: -0.708287239074707
Batch 41/64 loss: -0.8270998001098633
Batch 42/64 loss: -0.6448392868041992
Batch 43/64 loss: -0.4781031608581543
Batch 44/64 loss: -0.7177534103393555
Batch 45/64 loss: -0.7718691825866699
Batch 46/64 loss: -0.15551996231079102
Batch 47/64 loss: -0.8394908905029297
Batch 48/64 loss: -0.6659297943115234
Batch 49/64 loss: -0.2070631980895996
Batch 50/64 loss: -0.7638592720031738
Batch 51/64 loss: -0.7612786293029785
Batch 52/64 loss: -0.5427050590515137
Batch 53/64 loss: -0.8024635314941406
Batch 54/64 loss: -0.3848996162414551
Batch 55/64 loss: -0.5120525360107422
Batch 56/64 loss: -0.6113595962524414
Batch 57/64 loss: -0.7131171226501465
Batch 58/64 loss: -0.85467529296875
Batch 59/64 loss: -0.3302006721496582
Batch 60/64 loss: -0.7355461120605469
Batch 61/64 loss: -0.42583370208740234
Batch 62/64 loss: -0.6411285400390625
Batch 63/64 loss: 0.17759466171264648
Batch 64/64 loss: -4.472984790802002
Epoch 375  Train loss: -0.6053577105204264  Val loss: -0.985287328765974
Epoch 376
-------------------------------
Batch 1/64 loss: -0.6924924850463867
Batch 2/64 loss: -0.8744401931762695
Batch 3/64 loss: -0.7711091041564941
Batch 4/64 loss: -0.4973478317260742
Batch 5/64 loss: -0.6142411231994629
Batch 6/64 loss: -0.4155764579772949
Batch 7/64 loss: -0.5154643058776855
Batch 8/64 loss: -0.6986241340637207
Batch 9/64 loss: -0.5668735504150391
Batch 10/64 loss: -0.30483198165893555
Batch 11/64 loss: -0.7188677787780762
Batch 12/64 loss: -0.5906429290771484
Batch 13/64 loss: -0.6453280448913574
Batch 14/64 loss: -0.7855925559997559
Batch 15/64 loss: -0.7068338394165039
Batch 16/64 loss: -0.8492374420166016
Batch 17/64 loss: -0.6819205284118652
Batch 18/64 loss: -0.5699601173400879
Batch 19/64 loss: -0.6784238815307617
Batch 20/64 loss: -0.48188352584838867
Batch 21/64 loss: -0.3931150436401367
Batch 22/64 loss: -0.6355686187744141
Batch 23/64 loss: -0.7507085800170898
Batch 24/64 loss: -0.7487611770629883
Batch 25/64 loss: -0.6930990219116211
Batch 26/64 loss: -0.30085182189941406
Batch 27/64 loss: -0.7319130897521973
Batch 28/64 loss: -0.6297411918640137
Batch 29/64 loss: -0.18209075927734375
Batch 30/64 loss: -0.5611329078674316
Batch 31/64 loss: -0.8098225593566895
Batch 32/64 loss: -0.705665111541748
Batch 33/64 loss: -0.8201823234558105
Batch 34/64 loss: 0.7735238075256348
Batch 35/64 loss: -0.8873810768127441
Batch 36/64 loss: -0.7436671257019043
Batch 37/64 loss: -0.7898883819580078
Batch 38/64 loss: -0.7106490135192871
Batch 39/64 loss: -0.6093120574951172
Batch 40/64 loss: -0.7054014205932617
Batch 41/64 loss: -0.24079465866088867
Batch 42/64 loss: -0.7765097618103027
Batch 43/64 loss: -0.8433933258056641
Batch 44/64 loss: -0.8004603385925293
Batch 45/64 loss: -0.6792740821838379
Batch 46/64 loss: -0.655738353729248
Batch 47/64 loss: -0.8080825805664062
Batch 48/64 loss: -0.7022891044616699
Batch 49/64 loss: -0.7928080558776855
Batch 50/64 loss: -0.6163225173950195
Batch 51/64 loss: -0.7270398139953613
Batch 52/64 loss: 0.6242661476135254
Batch 53/64 loss: -0.787086009979248
Batch 54/64 loss: -0.8193783760070801
Batch 55/64 loss: -0.8099398612976074
Batch 56/64 loss: -0.6290946006774902
Batch 57/64 loss: -0.6894512176513672
Batch 58/64 loss: -0.6818466186523438
Batch 59/64 loss: -0.8548674583435059
Batch 60/64 loss: 0.389646053314209
Batch 61/64 loss: -0.5857672691345215
Batch 62/64 loss: -0.8430414199829102
Batch 63/64 loss: -0.5278739929199219
Batch 64/64 loss: -4.099375247955322
Epoch 376  Train loss: -0.6466948696211272  Val loss: -0.9423361185080407
Epoch 377
-------------------------------
Batch 1/64 loss: -0.8170990943908691
Batch 2/64 loss: -0.4798283576965332
Batch 3/64 loss: -0.6772623062133789
Batch 4/64 loss: -0.6397647857666016
Batch 5/64 loss: -0.8202934265136719
Batch 6/64 loss: -0.7769026756286621
Batch 7/64 loss: -0.5417971611022949
Batch 8/64 loss: -0.8279328346252441
Batch 9/64 loss: -0.7239775657653809
Batch 10/64 loss: -0.6474018096923828
Batch 11/64 loss: -0.5052957534790039
Batch 12/64 loss: -0.612309455871582
Batch 13/64 loss: -0.6297850608825684
Batch 14/64 loss: -0.8003802299499512
Batch 15/64 loss: -0.7264313697814941
Batch 16/64 loss: 0.8021688461303711
Batch 17/64 loss: -0.30533313751220703
Batch 18/64 loss: -0.714134693145752
Batch 19/64 loss: -0.8724517822265625
Batch 20/64 loss: -0.5673456192016602
Batch 21/64 loss: -0.6327433586120605
Batch 22/64 loss: -0.5999922752380371
Batch 23/64 loss: -0.7091255187988281
Batch 24/64 loss: -0.6983776092529297
Batch 25/64 loss: -0.7537665367126465
Batch 26/64 loss: -0.7266373634338379
Batch 27/64 loss: -0.7826347351074219
Batch 28/64 loss: -0.7055931091308594
Batch 29/64 loss: -0.639533519744873
Batch 30/64 loss: -0.5973434448242188
Batch 31/64 loss: -0.7573328018188477
Batch 32/64 loss: -0.5901684761047363
Batch 33/64 loss: -0.5401463508605957
Batch 34/64 loss: -0.1177682876586914
Batch 35/64 loss: -0.7285394668579102
Batch 36/64 loss: -0.758702278137207
Batch 37/64 loss: -0.8248734474182129
Batch 38/64 loss: -0.7357912063598633
Batch 39/64 loss: -0.6895508766174316
Batch 40/64 loss: -0.8475627899169922
Batch 41/64 loss: -0.5706992149353027
Batch 42/64 loss: -0.5512948036193848
Batch 43/64 loss: -0.7904844284057617
Batch 44/64 loss: -0.9087648391723633
Batch 45/64 loss: -0.6868829727172852
Batch 46/64 loss: -0.7018938064575195
Batch 47/64 loss: -0.70550537109375
Batch 48/64 loss: -0.7524847984313965
Batch 49/64 loss: -0.7947812080383301
Batch 50/64 loss: -0.6791033744812012
Batch 51/64 loss: -0.8504695892333984
Batch 52/64 loss: -0.8220653533935547
Batch 53/64 loss: -0.8209567070007324
Batch 54/64 loss: 0.605402946472168
Batch 55/64 loss: -0.6923966407775879
Batch 56/64 loss: -0.8609733581542969
Batch 57/64 loss: -0.3214240074157715
Batch 58/64 loss: -0.7072854042053223
Batch 59/64 loss: -0.8577814102172852
Batch 60/64 loss: -0.4950261116027832
Batch 61/64 loss: -0.08716964721679688
Batch 62/64 loss: -0.713904857635498
Batch 63/64 loss: 0.4012308120727539
Batch 64/64 loss: -4.2590861320495605
Epoch 377  Train loss: -0.6569218897352032  Val loss: -1.0509810824574475
Saving best model, epoch: 377
Epoch 378
-------------------------------
Batch 1/64 loss: -0.8083958625793457
Batch 2/64 loss: -0.6089267730712891
Batch 3/64 loss: -0.5102458000183105
Batch 4/64 loss: -0.6476559638977051
Batch 5/64 loss: -0.8027558326721191
Batch 6/64 loss: -0.7594518661499023
Batch 7/64 loss: -0.8637137413024902
Batch 8/64 loss: -0.5534777641296387
Batch 9/64 loss: -0.8441228866577148
Batch 10/64 loss: -0.9031925201416016
Batch 11/64 loss: -0.9038753509521484
Batch 12/64 loss: -0.7568597793579102
Batch 13/64 loss: -0.7946367263793945
Batch 14/64 loss: -0.7799310684204102
Batch 15/64 loss: 0.768805980682373
Batch 16/64 loss: -0.7339472770690918
Batch 17/64 loss: -0.7688827514648438
Batch 18/64 loss: -0.7274703979492188
Batch 19/64 loss: -0.7966365814208984
Batch 20/64 loss: -0.7172918319702148
Batch 21/64 loss: -0.24370861053466797
Batch 22/64 loss: -0.8352980613708496
Batch 23/64 loss: -0.9266843795776367
Batch 24/64 loss: 0.7928533554077148
Batch 25/64 loss: -0.7782578468322754
Batch 26/64 loss: -0.9418196678161621
Batch 27/64 loss: -0.6730103492736816
Batch 28/64 loss: -0.7188892364501953
Batch 29/64 loss: -0.7910380363464355
Batch 30/64 loss: -0.8496065139770508
Batch 31/64 loss: -0.7550220489501953
Batch 32/64 loss: -0.2489032745361328
Batch 33/64 loss: -0.730924129486084
Batch 34/64 loss: -0.7096943855285645
Batch 35/64 loss: -0.7213068008422852
Batch 36/64 loss: -0.8854684829711914
Batch 37/64 loss: -0.5096592903137207
Batch 38/64 loss: -0.5412278175354004
Batch 39/64 loss: -0.7572250366210938
Batch 40/64 loss: -0.8692054748535156
Batch 41/64 loss: -0.77935791015625
Batch 42/64 loss: -0.8269696235656738
Batch 43/64 loss: -0.794684886932373
Batch 44/64 loss: -0.6458663940429688
Batch 45/64 loss: -0.7174267768859863
Batch 46/64 loss: -0.8980622291564941
Batch 47/64 loss: -0.46882057189941406
Batch 48/64 loss: -0.6147212982177734
Batch 49/64 loss: -0.6666512489318848
Batch 50/64 loss: -0.7152566909790039
Batch 51/64 loss: -0.7261972427368164
Batch 52/64 loss: -0.7246336936950684
Batch 53/64 loss: -0.46677541732788086
Batch 54/64 loss: -0.656623363494873
Batch 55/64 loss: -0.7792415618896484
Batch 56/64 loss: -0.6239128112792969
Batch 57/64 loss: -0.5156574249267578
Batch 58/64 loss: -0.7955684661865234
Batch 59/64 loss: -0.6591310501098633
Batch 60/64 loss: -0.7509183883666992
Batch 61/64 loss: 0.2443065643310547
Batch 62/64 loss: 0.2635626792907715
Batch 63/64 loss: -0.7554221153259277
Batch 64/64 loss: -4.474483966827393
Epoch 378  Train loss: -0.684496529897054  Val loss: -0.9723842529087132
Epoch 379
-------------------------------
Batch 1/64 loss: -0.48056650161743164
Batch 2/64 loss: -0.7530808448791504
Batch 3/64 loss: -0.5669393539428711
Batch 4/64 loss: -0.5369267463684082
Batch 5/64 loss: -0.7157068252563477
Batch 6/64 loss: -0.41525840759277344
Batch 7/64 loss: -0.5652370452880859
Batch 8/64 loss: -0.6964120864868164
Batch 9/64 loss: -0.5617671012878418
Batch 10/64 loss: -0.6316914558410645
Batch 11/64 loss: -0.5403547286987305
Batch 12/64 loss: -0.6274075508117676
Batch 13/64 loss: -0.7642254829406738
Batch 14/64 loss: -0.4629635810852051
Batch 15/64 loss: -0.6532959938049316
Batch 16/64 loss: -0.6586623191833496
Batch 17/64 loss: -0.8249669075012207
Batch 18/64 loss: -0.7183599472045898
Batch 19/64 loss: -0.68768310546875
Batch 20/64 loss: 0.4947500228881836
Batch 21/64 loss: -0.6470851898193359
Batch 22/64 loss: -0.535651683807373
Batch 23/64 loss: -0.5947904586791992
Batch 24/64 loss: -0.8072199821472168
Batch 25/64 loss: -0.4611644744873047
Batch 26/64 loss: -0.7141427993774414
Batch 27/64 loss: -0.714515209197998
Batch 28/64 loss: -0.7637262344360352
Batch 29/64 loss: -0.7464199066162109
Batch 30/64 loss: -0.2642359733581543
Batch 31/64 loss: -0.7469344139099121
Batch 32/64 loss: -0.46220874786376953
Batch 33/64 loss: 0.8488740921020508
Batch 34/64 loss: -0.5107026100158691
Batch 35/64 loss: -0.5783872604370117
Batch 36/64 loss: -0.23453569412231445
Batch 37/64 loss: -0.6457066535949707
Batch 38/64 loss: -0.7034201622009277
Batch 39/64 loss: -0.5634770393371582
Batch 40/64 loss: -0.5756626129150391
Batch 41/64 loss: -0.8421192169189453
Batch 42/64 loss: -0.7056593894958496
Batch 43/64 loss: -0.6650614738464355
Batch 44/64 loss: -0.7681150436401367
Batch 45/64 loss: -0.2651662826538086
Batch 46/64 loss: -0.7542171478271484
Batch 47/64 loss: 0.2983231544494629
Batch 48/64 loss: -0.6159615516662598
Batch 49/64 loss: -0.39824390411376953
Batch 50/64 loss: -0.6739282608032227
Batch 51/64 loss: -0.6601653099060059
Batch 52/64 loss: -0.8430728912353516
Batch 53/64 loss: -0.5707721710205078
Batch 54/64 loss: -0.9710140228271484
Batch 55/64 loss: -0.5466656684875488
Batch 56/64 loss: -0.8735227584838867
Batch 57/64 loss: -0.7597990036010742
Batch 58/64 loss: -0.6922636032104492
Batch 59/64 loss: -0.5053286552429199
Batch 60/64 loss: -0.9032678604125977
Batch 61/64 loss: -0.7703113555908203
Batch 62/64 loss: -0.8433141708374023
Batch 63/64 loss: -0.6560115814208984
Batch 64/64 loss: -4.425203323364258
Epoch 379  Train loss: -0.6289008570652382  Val loss: -1.0047201438458104
Epoch 380
-------------------------------
Batch 1/64 loss: -0.698695182800293
Batch 2/64 loss: -0.5020151138305664
Batch 3/64 loss: -0.7798123359680176
Batch 4/64 loss: -0.8434314727783203
Batch 5/64 loss: -0.7829060554504395
Batch 6/64 loss: -0.5927677154541016
Batch 7/64 loss: -0.7931389808654785
Batch 8/64 loss: -0.7673606872558594
Batch 9/64 loss: -0.749112606048584
Batch 10/64 loss: -0.6704549789428711
Batch 11/64 loss: -0.3272552490234375
Batch 12/64 loss: -0.707303524017334
Batch 13/64 loss: -0.7140793800354004
Batch 14/64 loss: -0.749600887298584
Batch 15/64 loss: -0.4908266067504883
Batch 16/64 loss: -0.8871893882751465
Batch 17/64 loss: -0.7329497337341309
Batch 18/64 loss: -0.48781538009643555
Batch 19/64 loss: -0.10181045532226562
Batch 20/64 loss: -0.7546429634094238
Batch 21/64 loss: 0.44875574111938477
Batch 22/64 loss: -0.8745083808898926
Batch 23/64 loss: -0.7979226112365723
Batch 24/64 loss: -0.7317366600036621
Batch 25/64 loss: -0.8173208236694336
Batch 26/64 loss: -0.6946878433227539
Batch 27/64 loss: -0.21248197555541992
Batch 28/64 loss: -0.5613422393798828
Batch 29/64 loss: -0.7653865814208984
Batch 30/64 loss: -0.835026741027832
Batch 31/64 loss: -0.7918887138366699
Batch 32/64 loss: -0.1893172264099121
Batch 33/64 loss: -0.7474212646484375
Batch 34/64 loss: 0.48813486099243164
Batch 35/64 loss: -0.46575355529785156
Batch 36/64 loss: -0.5150861740112305
Batch 37/64 loss: -0.8193702697753906
Batch 38/64 loss: -0.8297524452209473
Batch 39/64 loss: -0.44199705123901367
Batch 40/64 loss: -0.8545093536376953
Batch 41/64 loss: -0.7428274154663086
Batch 42/64 loss: -0.7046513557434082
Batch 43/64 loss: -0.8482537269592285
Batch 44/64 loss: -0.7072796821594238
Batch 45/64 loss: -0.5737242698669434
Batch 46/64 loss: -0.5907425880432129
Batch 47/64 loss: -0.8860187530517578
Batch 48/64 loss: -0.7236495018005371
Batch 49/64 loss: -0.765099048614502
Batch 50/64 loss: -0.730034351348877
Batch 51/64 loss: -0.7546467781066895
Batch 52/64 loss: 0.7045989036560059
Batch 53/64 loss: -0.7827281951904297
Batch 54/64 loss: -0.947688102722168
Batch 55/64 loss: -0.8420586585998535
Batch 56/64 loss: -0.7833189964294434
Batch 57/64 loss: -0.8495731353759766
Batch 58/64 loss: -0.7492876052856445
Batch 59/64 loss: -0.8077406883239746
Batch 60/64 loss: -0.6168980598449707
Batch 61/64 loss: -0.616762638092041
Batch 62/64 loss: -0.8455820083618164
Batch 63/64 loss: -0.6564040184020996
Batch 64/64 loss: -4.309269905090332
Epoch 380  Train loss: -0.6775546840592926  Val loss: -1.0570319526383967
Saving best model, epoch: 380
Epoch 381
-------------------------------
Batch 1/64 loss: -0.7757344245910645
Batch 2/64 loss: -0.7676315307617188
Batch 3/64 loss: -0.7132282257080078
Batch 4/64 loss: -0.7846865653991699
Batch 5/64 loss: -0.9259133338928223
Batch 6/64 loss: -0.7353520393371582
Batch 7/64 loss: -0.530247688293457
Batch 8/64 loss: -0.7492265701293945
Batch 9/64 loss: 0.6675658226013184
Batch 10/64 loss: -0.6279735565185547
Batch 11/64 loss: -0.5556459426879883
Batch 12/64 loss: -0.7453832626342773
Batch 13/64 loss: -0.7620701789855957
Batch 14/64 loss: -0.6824774742126465
Batch 15/64 loss: -0.08951139450073242
Batch 16/64 loss: -0.5555343627929688
Batch 17/64 loss: -0.798243522644043
Batch 18/64 loss: -0.65411376953125
Batch 19/64 loss: 0.5447707176208496
Batch 20/64 loss: -0.7005109786987305
Batch 21/64 loss: -0.7384548187255859
Batch 22/64 loss: -0.7171907424926758
Batch 23/64 loss: -0.8869442939758301
Batch 24/64 loss: -0.9311361312866211
Batch 25/64 loss: -0.8052892684936523
Batch 26/64 loss: 0.17460203170776367
Batch 27/64 loss: -0.8994779586791992
Batch 28/64 loss: -0.8171191215515137
Batch 29/64 loss: -0.5145530700683594
Batch 30/64 loss: -0.8630237579345703
Batch 31/64 loss: -0.8137249946594238
Batch 32/64 loss: -0.8218646049499512
Batch 33/64 loss: -0.7700157165527344
Batch 34/64 loss: -0.767298698425293
Batch 35/64 loss: -0.830864429473877
Batch 36/64 loss: -0.5369954109191895
Batch 37/64 loss: -0.7383551597595215
Batch 38/64 loss: -0.4726743698120117
Batch 39/64 loss: -0.4104628562927246
Batch 40/64 loss: -0.7868318557739258
Batch 41/64 loss: -0.25846099853515625
Batch 42/64 loss: -0.8105430603027344
Batch 43/64 loss: -0.5179018974304199
Batch 44/64 loss: -0.679753303527832
Batch 45/64 loss: -0.5106053352355957
Batch 46/64 loss: -0.6751556396484375
Batch 47/64 loss: -0.4034738540649414
Batch 48/64 loss: -0.2843508720397949
Batch 49/64 loss: -0.7686080932617188
Batch 50/64 loss: -0.6317543983459473
Batch 51/64 loss: -0.7313542366027832
Batch 52/64 loss: -0.6098785400390625
Batch 53/64 loss: -0.6665115356445312
Batch 54/64 loss: -0.21204757690429688
Batch 55/64 loss: -0.8222131729125977
Batch 56/64 loss: -0.7428398132324219
Batch 57/64 loss: -0.4844484329223633
Batch 58/64 loss: -0.7139415740966797
Batch 59/64 loss: -0.5219120979309082
Batch 60/64 loss: -0.4961433410644531
Batch 61/64 loss: -0.7107076644897461
Batch 62/64 loss: -0.7921228408813477
Batch 63/64 loss: -0.6671490669250488
Batch 64/64 loss: -3.6143178939819336
Epoch 381  Train loss: -0.6480225768743777  Val loss: -0.9861082883225274
Epoch 382
-------------------------------
Batch 1/64 loss: -0.609440803527832
Batch 2/64 loss: -0.6403536796569824
Batch 3/64 loss: -0.5711450576782227
Batch 4/64 loss: -0.2899188995361328
Batch 5/64 loss: -0.7359542846679688
Batch 6/64 loss: 0.8630337715148926
Batch 7/64 loss: -0.9028925895690918
Batch 8/64 loss: -0.7739691734313965
Batch 9/64 loss: -0.8152041435241699
Batch 10/64 loss: -0.7791748046875
Batch 11/64 loss: -0.806307315826416
Batch 12/64 loss: -0.8200430870056152
Batch 13/64 loss: -0.8120174407958984
Batch 14/64 loss: -0.8271360397338867
Batch 15/64 loss: -0.8001856803894043
Batch 16/64 loss: -0.7583670616149902
Batch 17/64 loss: -0.4534015655517578
Batch 18/64 loss: -0.7758407592773438
Batch 19/64 loss: -0.7523193359375
Batch 20/64 loss: -0.4660201072692871
Batch 21/64 loss: -0.6832232475280762
Batch 22/64 loss: -0.5365018844604492
Batch 23/64 loss: -0.8394660949707031
Batch 24/64 loss: -0.7107248306274414
Batch 25/64 loss: 0.8056192398071289
Batch 26/64 loss: -0.7878155708312988
Batch 27/64 loss: -0.5037789344787598
Batch 28/64 loss: -0.792508602142334
Batch 29/64 loss: -0.34893274307250977
Batch 30/64 loss: -0.47566890716552734
Batch 31/64 loss: -0.8544468879699707
Batch 32/64 loss: -0.6378173828125
Batch 33/64 loss: -0.6805672645568848
Batch 34/64 loss: -0.6554789543151855
Batch 35/64 loss: -0.4442739486694336
Batch 36/64 loss: -0.587369441986084
Batch 37/64 loss: -0.6449880599975586
Batch 38/64 loss: -0.6915555000305176
Batch 39/64 loss: -0.7731294631958008
Batch 40/64 loss: -0.42113494873046875
Batch 41/64 loss: -0.6979689598083496
Batch 42/64 loss: -0.36276960372924805
Batch 43/64 loss: -0.6880636215209961
Batch 44/64 loss: -0.6557803153991699
Batch 45/64 loss: -0.36652565002441406
Batch 46/64 loss: -0.6931037902832031
Batch 47/64 loss: -0.7689061164855957
Batch 48/64 loss: -0.6587777137756348
Batch 49/64 loss: -0.48795652389526367
Batch 50/64 loss: 0.9103965759277344
Batch 51/64 loss: -0.45725488662719727
Batch 52/64 loss: -0.760101318359375
Batch 53/64 loss: -0.3084406852722168
Batch 54/64 loss: -0.03943681716918945
Batch 55/64 loss: -0.7186527252197266
Batch 56/64 loss: -0.6012544631958008
Batch 57/64 loss: -0.7475218772888184
Batch 58/64 loss: -0.3240776062011719
Batch 59/64 loss: -0.5896916389465332
Batch 60/64 loss: -0.7854423522949219
Batch 61/64 loss: -0.5981369018554688
Batch 62/64 loss: -0.8131580352783203
Batch 63/64 loss: -0.30213165283203125
Batch 64/64 loss: -4.210976600646973
Epoch 382  Train loss: -0.6033476175046435  Val loss: -0.9200464097904586
Epoch 383
-------------------------------
Batch 1/64 loss: -0.6202349662780762
Batch 2/64 loss: 0.021857261657714844
Batch 3/64 loss: -0.5983457565307617
Batch 4/64 loss: -0.5190420150756836
Batch 5/64 loss: -0.7519879341125488
Batch 6/64 loss: -0.7689380645751953
Batch 7/64 loss: -0.16271018981933594
Batch 8/64 loss: -0.633702278137207
Batch 9/64 loss: -0.676567554473877
Batch 10/64 loss: -0.698483943939209
Batch 11/64 loss: -0.5908417701721191
Batch 12/64 loss: -0.8271403312683105
Batch 13/64 loss: -0.8009748458862305
Batch 14/64 loss: -0.6986689567565918
Batch 15/64 loss: -0.7212753295898438
Batch 16/64 loss: 0.5265522003173828
Batch 17/64 loss: -0.6506109237670898
Batch 18/64 loss: -0.6905097961425781
Batch 19/64 loss: -0.8284993171691895
Batch 20/64 loss: -0.5922260284423828
Batch 21/64 loss: -0.6218729019165039
Batch 22/64 loss: -0.7060642242431641
Batch 23/64 loss: -0.6550111770629883
Batch 24/64 loss: 0.24956083297729492
Batch 25/64 loss: 0.8656573295593262
Batch 26/64 loss: -0.4307284355163574
Batch 27/64 loss: -0.6161627769470215
Batch 28/64 loss: -0.34792041778564453
Batch 29/64 loss: -0.8971495628356934
Batch 30/64 loss: -0.5679483413696289
Batch 31/64 loss: -0.7809267044067383
Batch 32/64 loss: -0.3444638252258301
Batch 33/64 loss: -0.7870659828186035
Batch 34/64 loss: -0.7892179489135742
Batch 35/64 loss: -0.6278848648071289
Batch 36/64 loss: -0.7072567939758301
Batch 37/64 loss: -0.6705546379089355
Batch 38/64 loss: -0.8143234252929688
Batch 39/64 loss: -0.6085071563720703
Batch 40/64 loss: -0.680079460144043
Batch 41/64 loss: -0.6893577575683594
Batch 42/64 loss: -0.7461137771606445
Batch 43/64 loss: -0.5816912651062012
Batch 44/64 loss: -0.6670231819152832
Batch 45/64 loss: -0.697657585144043
Batch 46/64 loss: -0.8660516738891602
Batch 47/64 loss: -0.7426252365112305
Batch 48/64 loss: -0.6274509429931641
Batch 49/64 loss: -0.36691713333129883
Batch 50/64 loss: -0.8047361373901367
Batch 51/64 loss: -0.7897377014160156
Batch 52/64 loss: -0.6051392555236816
Batch 53/64 loss: -0.7741756439208984
Batch 54/64 loss: -0.6709637641906738
Batch 55/64 loss: -0.6834702491760254
Batch 56/64 loss: -0.5200228691101074
Batch 57/64 loss: -0.770317554473877
Batch 58/64 loss: -0.19376230239868164
Batch 59/64 loss: -0.8502483367919922
Batch 60/64 loss: -0.6292977333068848
Batch 61/64 loss: -0.5068073272705078
Batch 62/64 loss: -0.6899123191833496
Batch 63/64 loss: -0.3783860206604004
Batch 64/64 loss: -4.298387050628662
Epoch 383  Train loss: -0.6258184638677858  Val loss: -0.9920730328641806
Epoch 384
-------------------------------
Batch 1/64 loss: -0.6576700210571289
Batch 2/64 loss: -0.6201486587524414
Batch 3/64 loss: -0.643714427947998
Batch 4/64 loss: -0.649899959564209
Batch 5/64 loss: -0.6351909637451172
Batch 6/64 loss: -0.7364010810852051
Batch 7/64 loss: -0.8158626556396484
Batch 8/64 loss: -0.15427827835083008
Batch 9/64 loss: -0.38634252548217773
Batch 10/64 loss: 0.18622827529907227
Batch 11/64 loss: -0.723691463470459
Batch 12/64 loss: -0.597287654876709
Batch 13/64 loss: -0.08570384979248047
Batch 14/64 loss: -0.5395317077636719
Batch 15/64 loss: -0.5771470069885254
Batch 16/64 loss: -0.8296222686767578
Batch 17/64 loss: -0.6782341003417969
Batch 18/64 loss: -0.7900447845458984
Batch 19/64 loss: -0.6648440361022949
Batch 20/64 loss: -0.5968294143676758
Batch 21/64 loss: -0.6600017547607422
Batch 22/64 loss: -0.7784252166748047
Batch 23/64 loss: -0.6385865211486816
Batch 24/64 loss: -0.8048620223999023
Batch 25/64 loss: 0.4759969711303711
Batch 26/64 loss: -0.6899127960205078
Batch 27/64 loss: -0.7546253204345703
Batch 28/64 loss: -0.7554974555969238
Batch 29/64 loss: -0.6565260887145996
Batch 30/64 loss: -0.6323962211608887
Batch 31/64 loss: -0.8124027252197266
Batch 32/64 loss: -0.8103184700012207
Batch 33/64 loss: -0.7244548797607422
Batch 34/64 loss: -0.7298088073730469
Batch 35/64 loss: -0.30278873443603516
Batch 36/64 loss: -0.5604586601257324
Batch 37/64 loss: -0.7751812934875488
Batch 38/64 loss: -0.897646427154541
Batch 39/64 loss: -0.7147579193115234
Batch 40/64 loss: -0.6820507049560547
Batch 41/64 loss: -0.057116031646728516
Batch 42/64 loss: -0.4999818801879883
Batch 43/64 loss: -0.5310444831848145
Batch 44/64 loss: -0.8114652633666992
Batch 45/64 loss: -0.7168269157409668
Batch 46/64 loss: 0.4781951904296875
Batch 47/64 loss: -0.7979245185852051
Batch 48/64 loss: -0.760566234588623
Batch 49/64 loss: -0.6446094512939453
Batch 50/64 loss: -0.7442517280578613
Batch 51/64 loss: -0.6638879776000977
Batch 52/64 loss: -0.7021903991699219
Batch 53/64 loss: 0.9981169700622559
Batch 54/64 loss: -0.6300630569458008
Batch 55/64 loss: -0.7972526550292969
Batch 56/64 loss: -0.5465221405029297
Batch 57/64 loss: -0.39606809616088867
Batch 58/64 loss: -0.6772346496582031
Batch 59/64 loss: -0.2876725196838379
Batch 60/64 loss: -0.5911746025085449
Batch 61/64 loss: -0.6935715675354004
Batch 62/64 loss: -0.7900309562683105
Batch 63/64 loss: -0.6299242973327637
Batch 64/64 loss: -4.109869480133057
Epoch 384  Train loss: -0.6066884863610361  Val loss: -0.9594446162587589
Epoch 385
-------------------------------
Batch 1/64 loss: -0.49199533462524414
Batch 2/64 loss: -0.46462535858154297
Batch 3/64 loss: -0.528531551361084
Batch 4/64 loss: -0.6810712814331055
Batch 5/64 loss: 1.7091846466064453
Batch 6/64 loss: -0.8576087951660156
Batch 7/64 loss: -0.5105414390563965
Batch 8/64 loss: -0.6231932640075684
Batch 9/64 loss: -0.6765842437744141
Batch 10/64 loss: -0.7848343849182129
Batch 11/64 loss: -0.6165966987609863
Batch 12/64 loss: -0.6805534362792969
Batch 13/64 loss: -0.7386116981506348
Batch 14/64 loss: -0.8216304779052734
Batch 15/64 loss: -0.7503080368041992
Batch 16/64 loss: -0.7727265357971191
Batch 17/64 loss: -0.6667633056640625
Batch 18/64 loss: -0.5657281875610352
Batch 19/64 loss: -0.5711212158203125
Batch 20/64 loss: -0.7026667594909668
Batch 21/64 loss: -0.6017065048217773
Batch 22/64 loss: -0.6354632377624512
Batch 23/64 loss: 0.7597522735595703
Batch 24/64 loss: -0.2897219657897949
Batch 25/64 loss: -0.6987972259521484
Batch 26/64 loss: -0.6074542999267578
Batch 27/64 loss: -0.8578677177429199
Batch 28/64 loss: -0.47306299209594727
Batch 29/64 loss: -0.7035002708435059
Batch 30/64 loss: -0.7435693740844727
Batch 31/64 loss: -0.7317562103271484
Batch 32/64 loss: -0.5423355102539062
Batch 33/64 loss: -0.6921272277832031
Batch 34/64 loss: -0.6938338279724121
Batch 35/64 loss: -0.5160303115844727
Batch 36/64 loss: -0.4404764175415039
Batch 37/64 loss: -0.2656736373901367
Batch 38/64 loss: -0.6307854652404785
Batch 39/64 loss: -0.7424392700195312
Batch 40/64 loss: -0.4713926315307617
Batch 41/64 loss: -0.6650781631469727
Batch 42/64 loss: -0.3692636489868164
Batch 43/64 loss: -0.6894669532775879
Batch 44/64 loss: -0.5948696136474609
Batch 45/64 loss: -0.7983107566833496
Batch 46/64 loss: -0.6760420799255371
Batch 47/64 loss: -0.5946917533874512
Batch 48/64 loss: -0.6828532218933105
Batch 49/64 loss: -0.41100502014160156
Batch 50/64 loss: -0.8096661567687988
Batch 51/64 loss: -0.7425441741943359
Batch 52/64 loss: -0.7108993530273438
Batch 53/64 loss: -0.7534074783325195
Batch 54/64 loss: -0.7082481384277344
Batch 55/64 loss: -0.42279815673828125
Batch 56/64 loss: -0.6600069999694824
Batch 57/64 loss: 0.26114797592163086
Batch 58/64 loss: -0.06966733932495117
Batch 59/64 loss: -0.8623127937316895
Batch 60/64 loss: -0.6058783531188965
Batch 61/64 loss: -0.696204662322998
Batch 62/64 loss: -0.6235141754150391
Batch 63/64 loss: -0.6852011680603027
Batch 64/64 loss: -4.356095314025879
Epoch 385  Train loss: -0.5994133780984318  Val loss: -0.9305010792316031
Epoch 386
-------------------------------
Batch 1/64 loss: -0.5186572074890137
Batch 2/64 loss: -0.5705504417419434
Batch 3/64 loss: -0.6432971954345703
Batch 4/64 loss: 0.13036775588989258
Batch 5/64 loss: -0.6443867683410645
Batch 6/64 loss: -0.5492968559265137
Batch 7/64 loss: -0.759613037109375
Batch 8/64 loss: -0.5908527374267578
Batch 9/64 loss: -0.42585086822509766
Batch 10/64 loss: -0.31923627853393555
Batch 11/64 loss: -0.6769609451293945
Batch 12/64 loss: -0.7218437194824219
Batch 13/64 loss: -0.7384476661682129
Batch 14/64 loss: -0.5595974922180176
Batch 15/64 loss: -0.4949016571044922
Batch 16/64 loss: -0.7528595924377441
Batch 17/64 loss: -0.648505687713623
Batch 18/64 loss: -0.7172908782958984
Batch 19/64 loss: 0.11826658248901367
Batch 20/64 loss: -0.665947437286377
Batch 21/64 loss: -0.3427877426147461
Batch 22/64 loss: -0.6909894943237305
Batch 23/64 loss: -0.7644762992858887
Batch 24/64 loss: -0.770289421081543
Batch 25/64 loss: 0.6599555015563965
Batch 26/64 loss: -0.5797619819641113
Batch 27/64 loss: -0.4970431327819824
Batch 28/64 loss: -0.4752616882324219
Batch 29/64 loss: -0.7397212982177734
Batch 30/64 loss: -0.6382427215576172
Batch 31/64 loss: -0.7283096313476562
Batch 32/64 loss: -0.7021474838256836
Batch 33/64 loss: -0.4600100517272949
Batch 34/64 loss: -0.6188888549804688
Batch 35/64 loss: -0.4667091369628906
Batch 36/64 loss: -0.5893406867980957
Batch 37/64 loss: -0.714561939239502
Batch 38/64 loss: -0.7987618446350098
Batch 39/64 loss: -0.7913589477539062
Batch 40/64 loss: -0.7909755706787109
Batch 41/64 loss: -0.8025350570678711
Batch 42/64 loss: -0.2244253158569336
Batch 43/64 loss: -0.7684431076049805
Batch 44/64 loss: -0.6665153503417969
Batch 45/64 loss: -0.6903347969055176
Batch 46/64 loss: -0.8004302978515625
Batch 47/64 loss: -0.6162576675415039
Batch 48/64 loss: -0.6320333480834961
Batch 49/64 loss: -0.7351078987121582
Batch 50/64 loss: -0.6563043594360352
Batch 51/64 loss: -0.6386575698852539
Batch 52/64 loss: -0.7249059677124023
Batch 53/64 loss: -0.7286992073059082
Batch 54/64 loss: -0.740788459777832
Batch 55/64 loss: -0.6151638031005859
Batch 56/64 loss: -0.6903104782104492
Batch 57/64 loss: -0.6490345001220703
Batch 58/64 loss: -0.5763974189758301
Batch 59/64 loss: -0.6303720474243164
Batch 60/64 loss: -0.5567073822021484
Batch 61/64 loss: -0.6429324150085449
Batch 62/64 loss: 0.8221969604492188
Batch 63/64 loss: -0.7900786399841309
Batch 64/64 loss: -4.266883850097656
Epoch 386  Train loss: -0.6149575775744869  Val loss: -0.9192414955584863
Epoch 387
-------------------------------
Batch 1/64 loss: -0.7645626068115234
Batch 2/64 loss: -0.48844099044799805
Batch 3/64 loss: -0.4616403579711914
Batch 4/64 loss: -0.5493984222412109
Batch 5/64 loss: 0.2597160339355469
Batch 6/64 loss: -0.5843720436096191
Batch 7/64 loss: -0.5199275016784668
Batch 8/64 loss: -0.6869854927062988
Batch 9/64 loss: -0.5757842063903809
Batch 10/64 loss: -0.7101850509643555
Batch 11/64 loss: -0.7298579216003418
Batch 12/64 loss: -0.831733226776123
Batch 13/64 loss: -0.8472838401794434
Batch 14/64 loss: -0.4991340637207031
Batch 15/64 loss: -0.6657052040100098
Batch 16/64 loss: -0.6218423843383789
Batch 17/64 loss: -0.617091178894043
Batch 18/64 loss: -0.6356935501098633
Batch 19/64 loss: 0.2445063591003418
Batch 20/64 loss: -0.736238956451416
Batch 21/64 loss: -0.6915273666381836
Batch 22/64 loss: -0.7176742553710938
Batch 23/64 loss: -0.2770843505859375
Batch 24/64 loss: -0.6327695846557617
Batch 25/64 loss: -0.47704362869262695
Batch 26/64 loss: -0.47088193893432617
Batch 27/64 loss: -0.22201251983642578
Batch 28/64 loss: -0.684565544128418
Batch 29/64 loss: -0.7879328727722168
Batch 30/64 loss: -0.8920254707336426
Batch 31/64 loss: -0.23861122131347656
Batch 32/64 loss: -0.7064175605773926
Batch 33/64 loss: -0.17336606979370117
Batch 34/64 loss: -0.7442660331726074
Batch 35/64 loss: -0.6074399948120117
Batch 36/64 loss: -0.806464672088623
Batch 37/64 loss: -0.73480224609375
Batch 38/64 loss: -0.7721333503723145
Batch 39/64 loss: -0.6885666847229004
Batch 40/64 loss: -0.6303386688232422
Batch 41/64 loss: -0.6820330619812012
Batch 42/64 loss: -0.7608513832092285
Batch 43/64 loss: -0.8252620697021484
Batch 44/64 loss: -0.6981134414672852
Batch 45/64 loss: -0.5001306533813477
Batch 46/64 loss: -0.6040616035461426
Batch 47/64 loss: -0.4889836311340332
Batch 48/64 loss: -0.7259678840637207
Batch 49/64 loss: -0.3777608871459961
Batch 50/64 loss: -0.6391668319702148
Batch 51/64 loss: 0.6875262260437012
Batch 52/64 loss: -0.7188596725463867
Batch 53/64 loss: -0.15539073944091797
Batch 54/64 loss: -0.6770777702331543
Batch 55/64 loss: -0.7029728889465332
Batch 56/64 loss: -0.6569361686706543
Batch 57/64 loss: -0.7227072715759277
Batch 58/64 loss: -0.7180361747741699
Batch 59/64 loss: -0.44746971130371094
Batch 60/64 loss: -0.6502718925476074
Batch 61/64 loss: -0.6828970909118652
Batch 62/64 loss: -0.6588211059570312
Batch 63/64 loss: -0.37408924102783203
Batch 64/64 loss: -4.33080530166626
Epoch 387  Train loss: -0.6118590990702312  Val loss: -0.8001845448287492
Epoch 388
-------------------------------
Batch 1/64 loss: -0.5339927673339844
Batch 2/64 loss: -0.5063085556030273
Batch 3/64 loss: -0.5704903602600098
Batch 4/64 loss: -0.6047444343566895
Batch 5/64 loss: -0.6466598510742188
Batch 6/64 loss: -0.7442412376403809
Batch 7/64 loss: -0.5064659118652344
Batch 8/64 loss: -0.5639553070068359
Batch 9/64 loss: -0.5460772514343262
Batch 10/64 loss: -0.6388182640075684
Batch 11/64 loss: -0.6606230735778809
Batch 12/64 loss: -0.7341136932373047
Batch 13/64 loss: -0.6374483108520508
Batch 14/64 loss: -0.671149730682373
Batch 15/64 loss: -0.1871204376220703
Batch 16/64 loss: 0.4782109260559082
Batch 17/64 loss: -0.709352970123291
Batch 18/64 loss: -0.7315835952758789
Batch 19/64 loss: -0.5252680778503418
Batch 20/64 loss: -0.547797679901123
Batch 21/64 loss: -0.6542110443115234
Batch 22/64 loss: -0.6808180809020996
Batch 23/64 loss: -0.8218855857849121
Batch 24/64 loss: 0.8704185485839844
Batch 25/64 loss: -0.6555385589599609
Batch 26/64 loss: -0.7509794235229492
Batch 27/64 loss: -0.8047075271606445
Batch 28/64 loss: -0.4797854423522949
Batch 29/64 loss: -0.32533836364746094
Batch 30/64 loss: -0.6734380722045898
Batch 31/64 loss: -0.6752099990844727
Batch 32/64 loss: -0.5395855903625488
Batch 33/64 loss: -0.7707996368408203
Batch 34/64 loss: -0.7373771667480469
Batch 35/64 loss: -0.5467705726623535
Batch 36/64 loss: -0.5162582397460938
Batch 37/64 loss: -0.8409438133239746
Batch 38/64 loss: 0.25142526626586914
Batch 39/64 loss: -0.6788020133972168
Batch 40/64 loss: -0.31932878494262695
Batch 41/64 loss: -0.48984193801879883
Batch 42/64 loss: 0.045400142669677734
Batch 43/64 loss: -0.0003223419189453125
Batch 44/64 loss: 0.24587440490722656
Batch 45/64 loss: 0.12529706954956055
Batch 46/64 loss: 0.9047703742980957
Batch 47/64 loss: 0.2807803153991699
Batch 48/64 loss: 0.6640458106994629
Batch 49/64 loss: 0.2981867790222168
Batch 50/64 loss: 1.5064101219177246
Batch 51/64 loss: 0.0585784912109375
Batch 52/64 loss: -0.06245088577270508
Batch 53/64 loss: -0.011474609375
Batch 54/64 loss: 0.07926082611083984
Batch 55/64 loss: -0.023809432983398438
Batch 56/64 loss: -0.14144325256347656
Batch 57/64 loss: -0.16858530044555664
Batch 58/64 loss: 0.07455682754516602
Batch 59/64 loss: 0.06736516952514648
Batch 60/64 loss: 0.30279970169067383
Batch 61/64 loss: 0.1094965934753418
Batch 62/64 loss: 0.14092683792114258
Batch 63/64 loss: -0.3679537773132324
Batch 64/64 loss: -3.180051803588867
Epoch 388  Train loss: -0.31192322525323607  Val loss: -0.527356085498718
Epoch 389
-------------------------------
Batch 1/64 loss: -0.27030515670776367
Batch 2/64 loss: -0.3998713493347168
Batch 3/64 loss: -0.3791990280151367
Batch 4/64 loss: 0.20986032485961914
Batch 5/64 loss: -0.19437026977539062
Batch 6/64 loss: -0.3820362091064453
Batch 7/64 loss: -0.2073650360107422
Batch 8/64 loss: 0.10293102264404297
Batch 9/64 loss: -0.36774539947509766
Batch 10/64 loss: -0.42619800567626953
Batch 11/64 loss: -0.37194108963012695
Batch 12/64 loss: -0.5014352798461914
Batch 13/64 loss: -0.2908797264099121
Batch 14/64 loss: -0.3204493522644043
Batch 15/64 loss: -0.39758777618408203
Batch 16/64 loss: -0.5098390579223633
Batch 17/64 loss: -0.46924734115600586
Batch 18/64 loss: -0.41304445266723633
Batch 19/64 loss: -0.406588077545166
Batch 20/64 loss: -0.5553765296936035
Batch 21/64 loss: -0.11174774169921875
Batch 22/64 loss: -0.5545496940612793
Batch 23/64 loss: -0.39670228958129883
Batch 24/64 loss: -0.6679496765136719
Batch 25/64 loss: -0.540898323059082
Batch 26/64 loss: 0.45798492431640625
Batch 27/64 loss: -0.6806869506835938
Batch 28/64 loss: -0.3728303909301758
Batch 29/64 loss: 0.6263003349304199
Batch 30/64 loss: -0.5377097129821777
Batch 31/64 loss: -0.41326045989990234
Batch 32/64 loss: -0.38701725006103516
Batch 33/64 loss: -0.6588048934936523
Batch 34/64 loss: -0.3243274688720703
Batch 35/64 loss: -0.5113191604614258
Batch 36/64 loss: -0.4669981002807617
Batch 37/64 loss: 1.2905712127685547
Batch 38/64 loss: -0.08500862121582031
Batch 39/64 loss: -0.41214609146118164
Batch 40/64 loss: -0.4210782051086426
Batch 41/64 loss: -0.4611835479736328
Batch 42/64 loss: -0.45852136611938477
Batch 43/64 loss: -0.4166121482849121
Batch 44/64 loss: -0.4748687744140625
Batch 45/64 loss: -0.5936369895935059
Batch 46/64 loss: -0.26339006423950195
Batch 47/64 loss: -0.4572000503540039
Batch 48/64 loss: -0.4171934127807617
Batch 49/64 loss: -0.34398460388183594
Batch 50/64 loss: -0.5511269569396973
Batch 51/64 loss: -0.5315937995910645
Batch 52/64 loss: -0.49242401123046875
Batch 53/64 loss: -0.589353084564209
Batch 54/64 loss: 0.3816514015197754
Batch 55/64 loss: -0.708493709564209
Batch 56/64 loss: -0.5559277534484863
Batch 57/64 loss: -0.6286377906799316
Batch 58/64 loss: -0.4779500961303711
Batch 59/64 loss: -0.6699142456054688
Batch 60/64 loss: 0.032631874084472656
Batch 61/64 loss: -0.4786548614501953
Batch 62/64 loss: -0.607264518737793
Batch 63/64 loss: -0.6703290939331055
Batch 64/64 loss: -4.3739728927612305
Epoch 389  Train loss: -0.3989540960274491  Val loss: -0.9301999934350502
Epoch 390
-------------------------------
Batch 1/64 loss: 0.7878174781799316
Batch 2/64 loss: -0.5489568710327148
Batch 3/64 loss: -0.6207666397094727
Batch 4/64 loss: -0.6494135856628418
Batch 5/64 loss: -0.6819238662719727
Batch 6/64 loss: -0.6682648658752441
Batch 7/64 loss: -0.7079529762268066
Batch 8/64 loss: -0.4086785316467285
Batch 9/64 loss: -0.7107992172241211
Batch 10/64 loss: -0.397432804107666
Batch 11/64 loss: 0.5897846221923828
Batch 12/64 loss: -0.7730345726013184
Batch 13/64 loss: -0.35612058639526367
Batch 14/64 loss: -0.5682563781738281
Batch 15/64 loss: 0.7587141990661621
Batch 16/64 loss: -0.814570426940918
Batch 17/64 loss: -0.6242232322692871
Batch 18/64 loss: -0.560905933380127
Batch 19/64 loss: -0.6552319526672363
Batch 20/64 loss: -0.726872444152832
Batch 21/64 loss: -0.7620120048522949
Batch 22/64 loss: -0.7544546127319336
Batch 23/64 loss: -0.7257270812988281
Batch 24/64 loss: -0.5435652732849121
Batch 25/64 loss: -0.7343306541442871
Batch 26/64 loss: -0.7299437522888184
Batch 27/64 loss: -0.7635154724121094
Batch 28/64 loss: -0.5527663230895996
Batch 29/64 loss: -0.5836949348449707
Batch 30/64 loss: -0.7181572914123535
Batch 31/64 loss: -0.28728199005126953
Batch 32/64 loss: -0.6757135391235352
Batch 33/64 loss: -0.6278877258300781
Batch 34/64 loss: -0.7910852432250977
Batch 35/64 loss: -0.7198543548583984
Batch 36/64 loss: -0.5903472900390625
Batch 37/64 loss: -0.6426849365234375
Batch 38/64 loss: -0.47094058990478516
Batch 39/64 loss: -0.6584601402282715
Batch 40/64 loss: -0.3772287368774414
Batch 41/64 loss: -0.40595436096191406
Batch 42/64 loss: -0.7044353485107422
Batch 43/64 loss: -0.3962712287902832
Batch 44/64 loss: -0.6422863006591797
Batch 45/64 loss: -0.6051197052001953
Batch 46/64 loss: -0.5808906555175781
Batch 47/64 loss: 0.6428203582763672
Batch 48/64 loss: -0.6446380615234375
Batch 49/64 loss: -0.10953807830810547
Batch 50/64 loss: -0.40516138076782227
Batch 51/64 loss: -0.583524227142334
Batch 52/64 loss: -0.4810061454772949
Batch 53/64 loss: -0.6754112243652344
Batch 54/64 loss: -0.47455930709838867
Batch 55/64 loss: -0.27295446395874023
Batch 56/64 loss: -0.6895546913146973
Batch 57/64 loss: -0.4573554992675781
Batch 58/64 loss: -0.6133184432983398
Batch 59/64 loss: -0.7151741981506348
Batch 60/64 loss: -0.6520347595214844
Batch 61/64 loss: -0.6176266670227051
Batch 62/64 loss: -0.6218500137329102
Batch 63/64 loss: -0.5668988227844238
Batch 64/64 loss: -4.325046062469482
Epoch 390  Train loss: -0.5578551890803318  Val loss: -0.8922562877746791
Epoch 391
-------------------------------
Batch 1/64 loss: -0.6847257614135742
Batch 2/64 loss: -0.6407556533813477
Batch 3/64 loss: -0.8382077217102051
Batch 4/64 loss: -0.7853760719299316
Batch 5/64 loss: -0.775871753692627
Batch 6/64 loss: -0.780148983001709
Batch 7/64 loss: -0.5868000984191895
Batch 8/64 loss: -0.5878419876098633
Batch 9/64 loss: -0.5625090599060059
Batch 10/64 loss: -0.6928777694702148
Batch 11/64 loss: -0.8288331031799316
Batch 12/64 loss: -0.3349494934082031
Batch 13/64 loss: -0.48062801361083984
Batch 14/64 loss: -0.12046623229980469
Batch 15/64 loss: -0.7307133674621582
Batch 16/64 loss: -0.4731173515319824
Batch 17/64 loss: -0.6220622062683105
Batch 18/64 loss: -0.736320972442627
Batch 19/64 loss: -0.6340327262878418
Batch 20/64 loss: -0.6695804595947266
Batch 21/64 loss: -0.6556730270385742
Batch 22/64 loss: -0.6625261306762695
Batch 23/64 loss: -0.08149337768554688
Batch 24/64 loss: -0.6717090606689453
Batch 25/64 loss: -0.7876100540161133
Batch 26/64 loss: 0.7806229591369629
Batch 27/64 loss: -0.7256278991699219
Batch 28/64 loss: -0.6789770126342773
Batch 29/64 loss: -0.6710453033447266
Batch 30/64 loss: -0.5004978179931641
Batch 31/64 loss: -0.5360727310180664
Batch 32/64 loss: -0.7087059020996094
Batch 33/64 loss: -0.6136307716369629
Batch 34/64 loss: -0.6478962898254395
Batch 35/64 loss: -0.6592035293579102
Batch 36/64 loss: -0.7234110832214355
Batch 37/64 loss: 0.42165374755859375
Batch 38/64 loss: -0.5867290496826172
Batch 39/64 loss: -0.39572811126708984
Batch 40/64 loss: -0.4924783706665039
Batch 41/64 loss: -0.3717827796936035
Batch 42/64 loss: -0.6843299865722656
Batch 43/64 loss: -0.6945962905883789
Batch 44/64 loss: -0.770723819732666
Batch 45/64 loss: -0.6864833831787109
Batch 46/64 loss: -0.5146737098693848
Batch 47/64 loss: 0.518653392791748
Batch 48/64 loss: -0.7156081199645996
Batch 49/64 loss: -0.6665687561035156
Batch 50/64 loss: -0.6067256927490234
Batch 51/64 loss: -0.7972331047058105
Batch 52/64 loss: -0.7028851509094238
Batch 53/64 loss: -0.7506461143493652
Batch 54/64 loss: -0.7667551040649414
Batch 55/64 loss: -0.5989871025085449
Batch 56/64 loss: -0.6045689582824707
Batch 57/64 loss: -0.7288928031921387
Batch 58/64 loss: -0.17871332168579102
Batch 59/64 loss: -0.7910456657409668
Batch 60/64 loss: -0.6556220054626465
Batch 61/64 loss: -0.6966614723205566
Batch 62/64 loss: -0.8214550018310547
Batch 63/64 loss: -0.4666604995727539
Batch 64/64 loss: -4.345909118652344
Epoch 391  Train loss: -0.6145090963326248  Val loss: -0.9269543546171942
Epoch 392
-------------------------------
Batch 1/64 loss: -0.5868525505065918
Batch 2/64 loss: 0.8313083648681641
Batch 3/64 loss: -0.5571160316467285
Batch 4/64 loss: -0.690544605255127
Batch 5/64 loss: -0.44593334197998047
Batch 6/64 loss: -0.23828125
Batch 7/64 loss: -0.7153182029724121
Batch 8/64 loss: -0.7907085418701172
Batch 9/64 loss: -0.7408623695373535
Batch 10/64 loss: -0.567476749420166
Batch 11/64 loss: -0.6459512710571289
Batch 12/64 loss: -0.5735182762145996
Batch 13/64 loss: 0.4405374526977539
Batch 14/64 loss: -0.3811936378479004
Batch 15/64 loss: -0.6271328926086426
Batch 16/64 loss: -0.5270910263061523
Batch 17/64 loss: 0.3283863067626953
Batch 18/64 loss: -0.5204195976257324
Batch 19/64 loss: -0.5399351119995117
Batch 20/64 loss: -0.4244246482849121
Batch 21/64 loss: -0.7163166999816895
Batch 22/64 loss: -0.6978759765625
Batch 23/64 loss: -0.7167911529541016
Batch 24/64 loss: -0.6526823043823242
Batch 25/64 loss: -0.8168964385986328
Batch 26/64 loss: -0.7404980659484863
Batch 27/64 loss: -0.5171804428100586
Batch 28/64 loss: -0.6196799278259277
Batch 29/64 loss: -0.5301647186279297
Batch 30/64 loss: -0.680910587310791
Batch 31/64 loss: -0.6320271492004395
Batch 32/64 loss: -0.29575681686401367
Batch 33/64 loss: -0.44020891189575195
Batch 34/64 loss: -0.6329574584960938
Batch 35/64 loss: -0.545590877532959
Batch 36/64 loss: -0.7056775093078613
Batch 37/64 loss: -0.4579143524169922
Batch 38/64 loss: -0.7153921127319336
Batch 39/64 loss: -0.6697254180908203
Batch 40/64 loss: -0.6203370094299316
Batch 41/64 loss: -0.6523361206054688
Batch 42/64 loss: -0.6761422157287598
Batch 43/64 loss: -0.6864724159240723
Batch 44/64 loss: -0.6642932891845703
Batch 45/64 loss: -0.545438289642334
Batch 46/64 loss: -0.44551515579223633
Batch 47/64 loss: -0.616142749786377
Batch 48/64 loss: -0.4643392562866211
Batch 49/64 loss: -0.7407708168029785
Batch 50/64 loss: -0.6543183326721191
Batch 51/64 loss: -0.4236159324645996
Batch 52/64 loss: -0.5872359275817871
Batch 53/64 loss: -0.5923199653625488
Batch 54/64 loss: -0.6216850280761719
Batch 55/64 loss: -0.7422971725463867
Batch 56/64 loss: -0.566535472869873
Batch 57/64 loss: -0.641512393951416
Batch 58/64 loss: -0.29902029037475586
Batch 59/64 loss: -0.638336181640625
Batch 60/64 loss: -0.4958009719848633
Batch 61/64 loss: 0.8175554275512695
Batch 62/64 loss: -0.019690990447998047
Batch 63/64 loss: 0.30151987075805664
Batch 64/64 loss: -4.271638870239258
Epoch 392  Train loss: -0.5370287801705155  Val loss: -0.8313246789257142
Epoch 393
-------------------------------
Batch 1/64 loss: -0.5416731834411621
Batch 2/64 loss: -0.47643136978149414
Batch 3/64 loss: -0.7024316787719727
Batch 4/64 loss: -0.5162086486816406
Batch 5/64 loss: -0.40012550354003906
Batch 6/64 loss: 0.07489824295043945
Batch 7/64 loss: -0.6693763732910156
Batch 8/64 loss: -0.3323063850402832
Batch 9/64 loss: -0.57073974609375
Batch 10/64 loss: -0.6131730079650879
Batch 11/64 loss: -0.2951469421386719
Batch 12/64 loss: -0.41738367080688477
Batch 13/64 loss: -0.6797623634338379
Batch 14/64 loss: -0.5650091171264648
Batch 15/64 loss: -0.5902352333068848
Batch 16/64 loss: -0.30278873443603516
Batch 17/64 loss: -0.601987361907959
Batch 18/64 loss: 0.7451009750366211
Batch 19/64 loss: -0.6513409614562988
Batch 20/64 loss: -0.5782051086425781
Batch 21/64 loss: -0.7052268981933594
Batch 22/64 loss: -0.35422801971435547
Batch 23/64 loss: -0.6082701683044434
Batch 24/64 loss: -0.46227312088012695
Batch 25/64 loss: -0.5049839019775391
Batch 26/64 loss: -0.37984418869018555
Batch 27/64 loss: -0.5379953384399414
Batch 28/64 loss: -0.412628173828125
Batch 29/64 loss: -0.34776782989501953
Batch 30/64 loss: -0.4410991668701172
Batch 31/64 loss: -0.3004722595214844
Batch 32/64 loss: -0.26207971572875977
Batch 33/64 loss: -0.5485630035400391
Batch 34/64 loss: -0.4922308921813965
Batch 35/64 loss: -0.18686532974243164
Batch 36/64 loss: -0.6298341751098633
Batch 37/64 loss: -0.019104957580566406
Batch 38/64 loss: -0.1312093734741211
Batch 39/64 loss: -0.06591987609863281
Batch 40/64 loss: -0.5942134857177734
Batch 41/64 loss: -0.5862517356872559
Batch 42/64 loss: -0.5192661285400391
Batch 43/64 loss: -0.47575807571411133
Batch 44/64 loss: 0.20825910568237305
Batch 45/64 loss: -0.4815025329589844
Batch 46/64 loss: -0.346494197845459
Batch 47/64 loss: -0.3656034469604492
Batch 48/64 loss: -0.30210447311401367
Batch 49/64 loss: -0.4367513656616211
Batch 50/64 loss: 1.0323567390441895
Batch 51/64 loss: -0.47136497497558594
Batch 52/64 loss: -0.47673511505126953
Batch 53/64 loss: -0.44956398010253906
Batch 54/64 loss: -0.5439453125
Batch 55/64 loss: -0.7128238677978516
Batch 56/64 loss: -0.6328082084655762
Batch 57/64 loss: 1.0681018829345703
Batch 58/64 loss: -0.663996696472168
Batch 59/64 loss: -0.6231169700622559
Batch 60/64 loss: -0.5100984573364258
Batch 61/64 loss: -0.6220932006835938
Batch 62/64 loss: -0.7404189109802246
Batch 63/64 loss: -0.6122612953186035
Batch 64/64 loss: -4.3170599937438965
Epoch 393  Train loss: -0.4419007413527545  Val loss: -0.8409417666930104
Epoch 394
-------------------------------
Batch 1/64 loss: -0.7779579162597656
Batch 2/64 loss: -0.5590519905090332
Batch 3/64 loss: -0.5654006004333496
Batch 4/64 loss: -0.4533381462097168
Batch 5/64 loss: -0.34255027770996094
Batch 6/64 loss: -0.7128629684448242
Batch 7/64 loss: -0.5754280090332031
Batch 8/64 loss: -0.23149442672729492
Batch 9/64 loss: -0.5623235702514648
Batch 10/64 loss: -0.6157875061035156
Batch 11/64 loss: -0.6520075798034668
Batch 12/64 loss: -0.5812530517578125
Batch 13/64 loss: 0.008287906646728516
Batch 14/64 loss: 0.1865839958190918
Batch 15/64 loss: -0.46622419357299805
Batch 16/64 loss: -0.3079829216003418
Batch 17/64 loss: -0.6049976348876953
Batch 18/64 loss: -0.769320011138916
Batch 19/64 loss: -0.6672554016113281
Batch 20/64 loss: -0.5107836723327637
Batch 21/64 loss: -0.6727023124694824
Batch 22/64 loss: 0.3820638656616211
Batch 23/64 loss: -0.6920976638793945
Batch 24/64 loss: -0.5876216888427734
Batch 25/64 loss: -0.8556933403015137
Batch 26/64 loss: -0.6477899551391602
Batch 27/64 loss: -0.5697832107543945
Batch 28/64 loss: -0.6461572647094727
Batch 29/64 loss: -0.4646930694580078
Batch 30/64 loss: -0.6520223617553711
Batch 31/64 loss: -0.7547392845153809
Batch 32/64 loss: -0.4554572105407715
Batch 33/64 loss: -0.356412410736084
Batch 34/64 loss: -0.7336635589599609
Batch 35/64 loss: -0.561704158782959
Batch 36/64 loss: 0.8916559219360352
Batch 37/64 loss: -0.5112757682800293
Batch 38/64 loss: -0.49194955825805664
Batch 39/64 loss: -0.6578688621520996
Batch 40/64 loss: -0.6474967002868652
Batch 41/64 loss: -0.6643152236938477
Batch 42/64 loss: -0.7066116333007812
Batch 43/64 loss: 0.5634851455688477
Batch 44/64 loss: -0.16120576858520508
Batch 45/64 loss: -0.6778926849365234
Batch 46/64 loss: -0.6180839538574219
Batch 47/64 loss: -0.6895246505737305
Batch 48/64 loss: -0.667201042175293
Batch 49/64 loss: -0.6875481605529785
Batch 50/64 loss: -0.8089308738708496
Batch 51/64 loss: -0.7037615776062012
Batch 52/64 loss: -0.30254459381103516
Batch 53/64 loss: -0.7223615646362305
Batch 54/64 loss: -0.4504661560058594
Batch 55/64 loss: -0.5316085815429688
Batch 56/64 loss: -0.6321849822998047
Batch 57/64 loss: -0.6353564262390137
Batch 58/64 loss: -0.7343058586120605
Batch 59/64 loss: -0.4590272903442383
Batch 60/64 loss: -0.668053150177002
Batch 61/64 loss: -0.5288400650024414
Batch 62/64 loss: -0.545896053314209
Batch 63/64 loss: -0.8188753128051758
Batch 64/64 loss: -4.168079853057861
Epoch 394  Train loss: -0.5556663045696184  Val loss: -0.9086426803746175
Epoch 395
-------------------------------
Batch 1/64 loss: -0.0016140937805175781
Batch 2/64 loss: -0.703157901763916
Batch 3/64 loss: -0.573829174041748
Batch 4/64 loss: -0.6224093437194824
Batch 5/64 loss: -0.5779843330383301
Batch 6/64 loss: -0.5239801406860352
Batch 7/64 loss: -0.7054424285888672
Batch 8/64 loss: -0.6768016815185547
Batch 9/64 loss: -0.6118478775024414
Batch 10/64 loss: -0.8134026527404785
Batch 11/64 loss: -0.5882840156555176
Batch 12/64 loss: -0.4661269187927246
Batch 13/64 loss: -0.6196041107177734
Batch 14/64 loss: -0.7835469245910645
Batch 15/64 loss: -0.5948429107666016
Batch 16/64 loss: -0.5802779197692871
Batch 17/64 loss: 0.5742645263671875
Batch 18/64 loss: -0.7547283172607422
Batch 19/64 loss: -0.34778785705566406
Batch 20/64 loss: -0.6312751770019531
Batch 21/64 loss: -0.3434295654296875
Batch 22/64 loss: -0.5582895278930664
Batch 23/64 loss: -0.8730196952819824
Batch 24/64 loss: -0.5004153251647949
Batch 25/64 loss: -0.43519115447998047
Batch 26/64 loss: -0.4377121925354004
Batch 27/64 loss: -0.792457103729248
Batch 28/64 loss: 0.5680551528930664
Batch 29/64 loss: -0.5355143547058105
Batch 30/64 loss: -0.44023847579956055
Batch 31/64 loss: -0.5627841949462891
Batch 32/64 loss: -0.6849417686462402
Batch 33/64 loss: -0.30377626419067383
Batch 34/64 loss: -0.5597586631774902
Batch 35/64 loss: -0.41300439834594727
Batch 36/64 loss: -0.6929483413696289
Batch 37/64 loss: -0.5952415466308594
Batch 38/64 loss: -0.7953863143920898
Batch 39/64 loss: -0.5345726013183594
Batch 40/64 loss: -0.5928425788879395
Batch 41/64 loss: -0.6764369010925293
Batch 42/64 loss: -0.6915626525878906
Batch 43/64 loss: -0.3410196304321289
Batch 44/64 loss: -0.6399469375610352
Batch 45/64 loss: -0.608954906463623
Batch 46/64 loss: -0.11294841766357422
Batch 47/64 loss: -0.6928210258483887
Batch 48/64 loss: -0.5129132270812988
Batch 49/64 loss: -0.639366626739502
Batch 50/64 loss: -0.6853985786437988
Batch 51/64 loss: -0.4681706428527832
Batch 52/64 loss: -0.7213425636291504
Batch 53/64 loss: -0.7591767311096191
Batch 54/64 loss: -0.6627354621887207
Batch 55/64 loss: -0.6554107666015625
Batch 56/64 loss: -0.7706117630004883
Batch 57/64 loss: -0.8276100158691406
Batch 58/64 loss: 1.263270378112793
Batch 59/64 loss: 0.1881084442138672
Batch 60/64 loss: -0.8175158500671387
Batch 61/64 loss: -0.6459970474243164
Batch 62/64 loss: -0.6978373527526855
Batch 63/64 loss: -0.43364429473876953
Batch 64/64 loss: -4.180106163024902
Epoch 395  Train loss: -0.5557846331128887  Val loss: -0.9393595666000524
Epoch 396
-------------------------------
Batch 1/64 loss: -0.6528630256652832
Batch 2/64 loss: -0.4485931396484375
Batch 3/64 loss: -0.6211791038513184
Batch 4/64 loss: -0.8269786834716797
Batch 5/64 loss: -0.8248543739318848
Batch 6/64 loss: -0.870063304901123
Batch 7/64 loss: -0.7978048324584961
Batch 8/64 loss: 0.5249495506286621
Batch 9/64 loss: -0.6677422523498535
Batch 10/64 loss: -0.7076592445373535
Batch 11/64 loss: -0.7296810150146484
Batch 12/64 loss: -0.6105055809020996
Batch 13/64 loss: -0.6524333953857422
Batch 14/64 loss: -0.7277793884277344
Batch 15/64 loss: -0.5348677635192871
Batch 16/64 loss: -0.8762803077697754
Batch 17/64 loss: -0.6020870208740234
Batch 18/64 loss: -0.5108222961425781
Batch 19/64 loss: -0.6832666397094727
Batch 20/64 loss: -0.5989766120910645
Batch 21/64 loss: -0.525395393371582
Batch 22/64 loss: -0.3098292350769043
Batch 23/64 loss: -0.824310302734375
Batch 24/64 loss: -0.7017984390258789
Batch 25/64 loss: -0.7530274391174316
Batch 26/64 loss: -0.4219851493835449
Batch 27/64 loss: -0.8085489273071289
Batch 28/64 loss: -0.45478248596191406
Batch 29/64 loss: -0.7895703315734863
Batch 30/64 loss: -0.6802291870117188
Batch 31/64 loss: -0.7444500923156738
Batch 32/64 loss: -0.5708808898925781
Batch 33/64 loss: -0.7418241500854492
Batch 34/64 loss: -0.4960641860961914
Batch 35/64 loss: 0.8719511032104492
Batch 36/64 loss: -0.6987514495849609
Batch 37/64 loss: -0.7361130714416504
Batch 38/64 loss: -0.15028762817382812
Batch 39/64 loss: -0.7298698425292969
Batch 40/64 loss: -0.6926555633544922
Batch 41/64 loss: -0.606081485748291
Batch 42/64 loss: -0.709144115447998
Batch 43/64 loss: -0.7305059432983398
Batch 44/64 loss: -0.49944019317626953
Batch 45/64 loss: -0.5828509330749512
Batch 46/64 loss: -0.28849029541015625
Batch 47/64 loss: -0.5356349945068359
Batch 48/64 loss: -0.7391510009765625
Batch 49/64 loss: -0.6685166358947754
Batch 50/64 loss: -0.7263669967651367
Batch 51/64 loss: -0.7164511680603027
Batch 52/64 loss: -0.5006647109985352
Batch 53/64 loss: -0.7004084587097168
Batch 54/64 loss: -0.6166510581970215
Batch 55/64 loss: -0.34194374084472656
Batch 56/64 loss: 0.30069828033447266
Batch 57/64 loss: -0.39181089401245117
Batch 58/64 loss: -0.5477991104125977
Batch 59/64 loss: -0.8457798957824707
Batch 60/64 loss: -0.3576955795288086
Batch 61/64 loss: -0.7171783447265625
Batch 62/64 loss: 0.031029701232910156
Batch 63/64 loss: -0.7392749786376953
Batch 64/64 loss: -4.330380439758301
Epoch 396  Train loss: -0.6095028858558804  Val loss: -0.7249520160897901
Epoch 397
-------------------------------
Batch 1/64 loss: -0.3957810401916504
Batch 2/64 loss: -0.7192697525024414
Batch 3/64 loss: -0.648470401763916
Batch 4/64 loss: -0.17386627197265625
Batch 5/64 loss: -0.19976377487182617
Batch 6/64 loss: -0.5679922103881836
Batch 7/64 loss: -0.7776851654052734
Batch 8/64 loss: -0.6098928451538086
Batch 9/64 loss: -0.3462495803833008
Batch 10/64 loss: -0.6048636436462402
Batch 11/64 loss: -0.6615309715270996
Batch 12/64 loss: -0.4525771141052246
Batch 13/64 loss: -0.4787278175354004
Batch 14/64 loss: -0.7253947257995605
Batch 15/64 loss: -0.43482398986816406
Batch 16/64 loss: -0.6380257606506348
Batch 17/64 loss: -0.7805685997009277
Batch 18/64 loss: 0.8285913467407227
Batch 19/64 loss: -0.524169921875
Batch 20/64 loss: -0.19957733154296875
Batch 21/64 loss: -0.3893747329711914
Batch 22/64 loss: -0.4762840270996094
Batch 23/64 loss: -0.5500264167785645
Batch 24/64 loss: -0.6606936454772949
Batch 25/64 loss: -0.5721492767333984
Batch 26/64 loss: -0.686187744140625
Batch 27/64 loss: -0.5707426071166992
Batch 28/64 loss: -0.601996898651123
Batch 29/64 loss: -0.6083807945251465
Batch 30/64 loss: -0.6304354667663574
Batch 31/64 loss: -0.6860613822937012
Batch 32/64 loss: -0.7039661407470703
Batch 33/64 loss: -0.4490065574645996
Batch 34/64 loss: -0.4972677230834961
Batch 35/64 loss: -0.5694208145141602
Batch 36/64 loss: -0.6570048332214355
Batch 37/64 loss: -0.672217845916748
Batch 38/64 loss: -0.46799802780151367
Batch 39/64 loss: -0.6717815399169922
Batch 40/64 loss: -0.5726780891418457
Batch 41/64 loss: -0.08982133865356445
Batch 42/64 loss: 0.5890974998474121
Batch 43/64 loss: -0.4507875442504883
Batch 44/64 loss: -0.5823040008544922
Batch 45/64 loss: -0.34099817276000977
Batch 46/64 loss: -0.15088510513305664
Batch 47/64 loss: -0.44934844970703125
Batch 48/64 loss: 0.08923482894897461
Batch 49/64 loss: -0.4771251678466797
Batch 50/64 loss: -0.3330097198486328
Batch 51/64 loss: -0.34975290298461914
Batch 52/64 loss: -0.038727760314941406
Batch 53/64 loss: -0.6693367958068848
Batch 54/64 loss: 1.1113605499267578
Batch 55/64 loss: -0.5754742622375488
Batch 56/64 loss: -0.5242877006530762
Batch 57/64 loss: -0.5570888519287109
Batch 58/64 loss: -0.5525956153869629
Batch 59/64 loss: -0.4147353172302246
Batch 60/64 loss: -0.047740936279296875
Batch 61/64 loss: -0.41319847106933594
Batch 62/64 loss: -0.10619258880615234
Batch 63/64 loss: -0.4596676826477051
Batch 64/64 loss: -4.188171863555908
Epoch 397  Train loss: -0.4664914280760522  Val loss: -0.6789893317468387
Epoch 398
-------------------------------
Batch 1/64 loss: -0.3065924644470215
Batch 2/64 loss: -0.24505090713500977
Batch 3/64 loss: -0.4280686378479004
Batch 4/64 loss: -0.4996943473815918
Batch 5/64 loss: -0.43186521530151367
Batch 6/64 loss: -0.5654654502868652
Batch 7/64 loss: -0.10166311264038086
Batch 8/64 loss: -0.2626500129699707
Batch 9/64 loss: -0.5592279434204102
Batch 10/64 loss: -0.5457844734191895
Batch 11/64 loss: -0.5279202461242676
Batch 12/64 loss: -0.4890270233154297
Batch 13/64 loss: -0.5854396820068359
Batch 14/64 loss: -0.5725278854370117
Batch 15/64 loss: -0.4724907875061035
Batch 16/64 loss: -0.3809623718261719
Batch 17/64 loss: -0.4840545654296875
Batch 18/64 loss: -0.5772190093994141
Batch 19/64 loss: -0.496091365814209
Batch 20/64 loss: -0.5477843284606934
Batch 21/64 loss: 1.2346491813659668
Batch 22/64 loss: -0.14180374145507812
Batch 23/64 loss: -0.38892602920532227
Batch 24/64 loss: -0.6953568458557129
Batch 25/64 loss: -0.600944995880127
Batch 26/64 loss: -0.7357234954833984
Batch 27/64 loss: -0.5508213043212891
Batch 28/64 loss: -0.5720839500427246
Batch 29/64 loss: -0.5274248123168945
Batch 30/64 loss: -0.5193161964416504
Batch 31/64 loss: 0.473447322845459
Batch 32/64 loss: -0.3838973045349121
Batch 33/64 loss: -0.48027849197387695
Batch 34/64 loss: -0.46739673614501953
Batch 35/64 loss: -0.7021803855895996
Batch 36/64 loss: -0.21480798721313477
Batch 37/64 loss: -0.680905818939209
Batch 38/64 loss: -0.7366423606872559
Batch 39/64 loss: 0.10941600799560547
Batch 40/64 loss: -0.5696015357971191
Batch 41/64 loss: -0.6505465507507324
Batch 42/64 loss: -0.7401585578918457
Batch 43/64 loss: -0.6971001625061035
Batch 44/64 loss: -0.4748821258544922
Batch 45/64 loss: -0.5955114364624023
Batch 46/64 loss: -0.601142406463623
Batch 47/64 loss: -0.6869559288024902
Batch 48/64 loss: -0.6341185569763184
Batch 49/64 loss: -0.5595812797546387
Batch 50/64 loss: -0.5444040298461914
Batch 51/64 loss: -0.5978455543518066
Batch 52/64 loss: -0.7002444267272949
Batch 53/64 loss: -0.5853314399719238
Batch 54/64 loss: -0.6510500907897949
Batch 55/64 loss: -0.581021785736084
Batch 56/64 loss: 0.8433218002319336
Batch 57/64 loss: -0.5306525230407715
Batch 58/64 loss: -0.4535989761352539
Batch 59/64 loss: -0.3939394950866699
Batch 60/64 loss: -0.6942353248596191
Batch 61/64 loss: -0.5008206367492676
Batch 62/64 loss: -0.6895351409912109
Batch 63/64 loss: -0.528353214263916
Batch 64/64 loss: -4.194695472717285
Epoch 398  Train loss: -0.49606131385354435  Val loss: -0.8327867304746228
Epoch 399
-------------------------------
Batch 1/64 loss: -0.40015506744384766
Batch 2/64 loss: -0.6429347991943359
Batch 3/64 loss: -0.40814685821533203
Batch 4/64 loss: -0.6547999382019043
Batch 5/64 loss: -0.11056375503540039
Batch 6/64 loss: -0.3274688720703125
Batch 7/64 loss: 0.45769500732421875
Batch 8/64 loss: -0.5265178680419922
Batch 9/64 loss: -0.5366959571838379
Batch 10/64 loss: 0.9830574989318848
Batch 11/64 loss: -0.6339993476867676
Batch 12/64 loss: -0.6496696472167969
Batch 13/64 loss: -0.5717306137084961
Batch 14/64 loss: -0.5481534004211426
Batch 15/64 loss: -0.18926715850830078
Batch 16/64 loss: -0.6765990257263184
Batch 17/64 loss: -0.5202536582946777
Batch 18/64 loss: -0.7061729431152344
Batch 19/64 loss: -0.15523195266723633
Batch 20/64 loss: -0.48225975036621094
Batch 21/64 loss: -0.5844020843505859
Batch 22/64 loss: -0.5376663208007812
Batch 23/64 loss: -0.654818058013916
Batch 24/64 loss: -0.6869072914123535
Batch 25/64 loss: -0.4380321502685547
Batch 26/64 loss: -0.7286949157714844
Batch 27/64 loss: -0.7581615447998047
Batch 28/64 loss: -0.3744783401489258
Batch 29/64 loss: -0.5636940002441406
Batch 30/64 loss: -0.49364328384399414
Batch 31/64 loss: 0.45218753814697266
Batch 32/64 loss: -0.5690765380859375
Batch 33/64 loss: -0.5828104019165039
Batch 34/64 loss: -0.6370739936828613
Batch 35/64 loss: -0.7422800064086914
Batch 36/64 loss: -0.6538701057434082
Batch 37/64 loss: -0.5622134208679199
Batch 38/64 loss: -0.3374471664428711
Batch 39/64 loss: -0.49611759185791016
Batch 40/64 loss: -0.6757850646972656
Batch 41/64 loss: -0.5290737152099609
Batch 42/64 loss: -0.584810733795166
Batch 43/64 loss: -0.692284107208252
Batch 44/64 loss: -0.4646339416503906
Batch 45/64 loss: -0.7840604782104492
Batch 46/64 loss: -0.5380864143371582
Batch 47/64 loss: -0.46853208541870117
Batch 48/64 loss: -0.44887828826904297
Batch 49/64 loss: -0.8857517242431641
Batch 50/64 loss: -0.8010468482971191
Batch 51/64 loss: -0.7691435813903809
Batch 52/64 loss: -0.44849395751953125
Batch 53/64 loss: -0.6908383369445801
Batch 54/64 loss: -0.6327996253967285
Batch 55/64 loss: -0.6348137855529785
Batch 56/64 loss: -0.7050142288208008
Batch 57/64 loss: -0.7998089790344238
Batch 58/64 loss: -0.6989011764526367
Batch 59/64 loss: -0.7470602989196777
Batch 60/64 loss: -0.677116870880127
Batch 61/64 loss: -0.702695369720459
Batch 62/64 loss: -0.8302388191223145
Batch 63/64 loss: -0.15385103225708008
Batch 64/64 loss: -4.158259868621826
Epoch 399  Train loss: -0.5604938376183604  Val loss: -1.0110795063661135
Epoch 400
-------------------------------
Batch 1/64 loss: -0.6691694259643555
Batch 2/64 loss: -0.4881577491760254
Batch 3/64 loss: -0.7091374397277832
Batch 4/64 loss: 0.23047208786010742
Batch 5/64 loss: -0.4114408493041992
Batch 6/64 loss: -0.7121725082397461
Batch 7/64 loss: -0.8370509147644043
Batch 8/64 loss: -0.8588919639587402
Batch 9/64 loss: -0.6433615684509277
Batch 10/64 loss: -0.7581286430358887
Batch 11/64 loss: -0.6390972137451172
Batch 12/64 loss: -0.8200035095214844
Batch 13/64 loss: -0.5394601821899414
Batch 14/64 loss: -0.7277054786682129
Batch 15/64 loss: -0.5562620162963867
Batch 16/64 loss: -0.725611686706543
Batch 17/64 loss: -0.722893238067627
Batch 18/64 loss: -0.8336248397827148
Batch 19/64 loss: -0.6201233863830566
Batch 20/64 loss: -0.8421497344970703
Batch 21/64 loss: -0.0675358772277832
Batch 22/64 loss: -0.2104201316833496
Batch 23/64 loss: -0.5343947410583496
Batch 24/64 loss: -0.6861691474914551
Batch 25/64 loss: -0.5143642425537109
Batch 26/64 loss: -0.5177559852600098
Batch 27/64 loss: -0.5742197036743164
Batch 28/64 loss: -0.5522050857543945
Batch 29/64 loss: -0.5723042488098145
Batch 30/64 loss: -0.6974959373474121
Batch 31/64 loss: -0.55487060546875
Batch 32/64 loss: -0.6255574226379395
Batch 33/64 loss: -0.3081808090209961
Batch 34/64 loss: -0.3783731460571289
Batch 35/64 loss: -0.6173686981201172
Batch 36/64 loss: -0.25594139099121094
Batch 37/64 loss: -0.5667414665222168
Batch 38/64 loss: -0.611628532409668
Batch 39/64 loss: -0.03534412384033203
Batch 40/64 loss: -0.6544041633605957
Batch 41/64 loss: 0.7366495132446289
Batch 42/64 loss: -0.571753978729248
Batch 43/64 loss: 0.6584649085998535
Batch 44/64 loss: -0.6323704719543457
Batch 45/64 loss: -0.523378849029541
Batch 46/64 loss: -0.5732212066650391
Batch 47/64 loss: -0.541987419128418
Batch 48/64 loss: -0.5373082160949707
Batch 49/64 loss: -0.6377453804016113
Batch 50/64 loss: -0.6136674880981445
Batch 51/64 loss: -0.5083889961242676
Batch 52/64 loss: -0.5909595489501953
Batch 53/64 loss: -0.22879791259765625
Batch 54/64 loss: -0.5589160919189453
Batch 55/64 loss: -0.6007518768310547
Batch 56/64 loss: -0.2943429946899414
Batch 57/64 loss: -0.3519444465637207
Batch 58/64 loss: -0.48346757888793945
Batch 59/64 loss: -0.6303315162658691
Batch 60/64 loss: -0.545647144317627
Batch 61/64 loss: -0.48285579681396484
Batch 62/64 loss: -0.30916357040405273
Batch 63/64 loss: -0.3590841293334961
Batch 64/64 loss: -4.2763991355896
Epoch 400  Train loss: -0.5460002394283519  Val loss: -0.7279972325485596
Epoch 401
-------------------------------
Batch 1/64 loss: -0.4774918556213379
Batch 2/64 loss: -0.201690673828125
Batch 3/64 loss: 0.1897602081298828
Batch 4/64 loss: -0.4849066734313965
Batch 5/64 loss: -0.3814711570739746
Batch 6/64 loss: -0.2373666763305664
Batch 7/64 loss: -0.542625904083252
Batch 8/64 loss: -0.6237006187438965
Batch 9/64 loss: -0.5770201683044434
Batch 10/64 loss: -0.03598308563232422
Batch 11/64 loss: -0.691288948059082
Batch 12/64 loss: -0.17149782180786133
Batch 13/64 loss: -0.5707802772521973
Batch 14/64 loss: -0.5442395210266113
Batch 15/64 loss: -0.5623593330383301
Batch 16/64 loss: -0.6027164459228516
Batch 17/64 loss: -0.49389171600341797
Batch 18/64 loss: -0.40694713592529297
Batch 19/64 loss: -0.6310882568359375
Batch 20/64 loss: -0.5382142066955566
Batch 21/64 loss: -0.22600173950195312
Batch 22/64 loss: -0.634181022644043
Batch 23/64 loss: -0.38792943954467773
Batch 24/64 loss: -0.6559410095214844
Batch 25/64 loss: -0.5106296539306641
Batch 26/64 loss: -0.4591865539550781
Batch 27/64 loss: -0.5358409881591797
Batch 28/64 loss: -0.7375378608703613
Batch 29/64 loss: -0.7380094528198242
Batch 30/64 loss: -0.6014370918273926
Batch 31/64 loss: -0.6679677963256836
Batch 32/64 loss: -0.6721439361572266
Batch 33/64 loss: 0.7588281631469727
Batch 34/64 loss: -0.5584940910339355
Batch 35/64 loss: -0.23312664031982422
Batch 36/64 loss: -0.485379695892334
Batch 37/64 loss: -0.5445866584777832
Batch 38/64 loss: -0.6443190574645996
Batch 39/64 loss: -0.41910314559936523
Batch 40/64 loss: -0.472991943359375
Batch 41/64 loss: 0.7534632682800293
Batch 42/64 loss: -0.5676817893981934
Batch 43/64 loss: -0.7186269760131836
Batch 44/64 loss: -0.5469255447387695
Batch 45/64 loss: -0.5597906112670898
Batch 46/64 loss: -0.6273994445800781
Batch 47/64 loss: -0.5977163314819336
Batch 48/64 loss: -0.38785266876220703
Batch 49/64 loss: -0.5642390251159668
Batch 50/64 loss: -0.26421308517456055
Batch 51/64 loss: 0.7495551109313965
Batch 52/64 loss: -0.6577877998352051
Batch 53/64 loss: -0.29359960556030273
Batch 54/64 loss: -0.5047335624694824
Batch 55/64 loss: -0.17226314544677734
Batch 56/64 loss: -0.7328891754150391
Batch 57/64 loss: -0.7478890419006348
Batch 58/64 loss: 0.9342546463012695
Batch 59/64 loss: -0.3209977149963379
Batch 60/64 loss: -0.5697031021118164
Batch 61/64 loss: -0.36643362045288086
Batch 62/64 loss: -0.38623571395874023
Batch 63/64 loss: -0.38456106185913086
Batch 64/64 loss: -4.348219871520996
Epoch 401  Train loss: -0.4518734240064434  Val loss: -0.7332440143598314
Epoch 402
-------------------------------
Batch 1/64 loss: -0.30254077911376953
Batch 2/64 loss: -0.2159891128540039
Batch 3/64 loss: -0.5385513305664062
Batch 4/64 loss: 1.848599910736084
Batch 5/64 loss: -0.5376095771789551
Batch 6/64 loss: -0.43879175186157227
Batch 7/64 loss: -0.56805419921875
Batch 8/64 loss: -0.3542466163635254
Batch 9/64 loss: -0.5800423622131348
Batch 10/64 loss: -0.46387720108032227
Batch 11/64 loss: -0.5465326309204102
Batch 12/64 loss: -0.6718673706054688
Batch 13/64 loss: -0.6846561431884766
Batch 14/64 loss: -0.5849080085754395
Batch 15/64 loss: -0.03540372848510742
Batch 16/64 loss: -0.17837905883789062
Batch 17/64 loss: -0.5238585472106934
Batch 18/64 loss: -0.4793562889099121
Batch 19/64 loss: -0.6068382263183594
Batch 20/64 loss: -0.4573020935058594
Batch 21/64 loss: -0.6267228126525879
Batch 22/64 loss: -0.6733536720275879
Batch 23/64 loss: -0.6285915374755859
Batch 24/64 loss: -0.7443900108337402
Batch 25/64 loss: -0.36806678771972656
Batch 26/64 loss: -0.5427627563476562
Batch 27/64 loss: -0.5330300331115723
Batch 28/64 loss: -0.701909065246582
Batch 29/64 loss: -0.7320804595947266
Batch 30/64 loss: -0.6134867668151855
Batch 31/64 loss: -0.6089224815368652
Batch 32/64 loss: -0.6218442916870117
Batch 33/64 loss: -0.6270980834960938
Batch 34/64 loss: -0.5897431373596191
Batch 35/64 loss: -0.7620363235473633
Batch 36/64 loss: -0.35309600830078125
Batch 37/64 loss: -0.48558664321899414
Batch 38/64 loss: -0.6376080513000488
Batch 39/64 loss: -0.4951157569885254
Batch 40/64 loss: -0.5942120552062988
Batch 41/64 loss: -0.40809202194213867
Batch 42/64 loss: -0.4827704429626465
Batch 43/64 loss: -0.4990506172180176
Batch 44/64 loss: -0.5545315742492676
Batch 45/64 loss: -0.5557670593261719
Batch 46/64 loss: -0.47017335891723633
Batch 47/64 loss: -0.6649341583251953
Batch 48/64 loss: 0.006961345672607422
Batch 49/64 loss: 0.4205479621887207
Batch 50/64 loss: -0.421903133392334
Batch 51/64 loss: -0.5284156799316406
Batch 52/64 loss: -0.6191482543945312
Batch 53/64 loss: -0.6575260162353516
Batch 54/64 loss: -0.6415433883666992
Batch 55/64 loss: -0.5708165168762207
Batch 56/64 loss: -0.5696768760681152
Batch 57/64 loss: -0.5917620658874512
Batch 58/64 loss: -0.5284333229064941
Batch 59/64 loss: -0.6537909507751465
Batch 60/64 loss: -0.669426441192627
Batch 61/64 loss: -0.7699551582336426
Batch 62/64 loss: -0.6081762313842773
Batch 63/64 loss: -0.6612100601196289
Batch 64/64 loss: -4.008577346801758
Epoch 402  Train loss: -0.5265237471636603  Val loss: -0.8889066951791036
Epoch 403
-------------------------------
Batch 1/64 loss: -0.6316013336181641
Batch 2/64 loss: -0.6789684295654297
Batch 3/64 loss: -0.4792971611022949
Batch 4/64 loss: -0.7377619743347168
Batch 5/64 loss: -0.6620597839355469
Batch 6/64 loss: -0.5976881980895996
Batch 7/64 loss: -0.6864619255065918
Batch 8/64 loss: 0.5803771018981934
Batch 9/64 loss: -0.5279960632324219
Batch 10/64 loss: -0.8115038871765137
Batch 11/64 loss: -0.27393245697021484
Batch 12/64 loss: -0.608119010925293
Batch 13/64 loss: -0.5646743774414062
Batch 14/64 loss: -0.5299038887023926
Batch 15/64 loss: -0.5334844589233398
Batch 16/64 loss: -0.702667236328125
Batch 17/64 loss: -0.45479726791381836
Batch 18/64 loss: -0.6408438682556152
Batch 19/64 loss: -0.39739322662353516
Batch 20/64 loss: -0.7337450981140137
Batch 21/64 loss: -0.7309737205505371
Batch 22/64 loss: -0.657923698425293
Batch 23/64 loss: -0.7627687454223633
Batch 24/64 loss: -0.5932407379150391
Batch 25/64 loss: 0.7963542938232422
Batch 26/64 loss: -0.7651157379150391
Batch 27/64 loss: -0.6277852058410645
Batch 28/64 loss: -0.6667623519897461
Batch 29/64 loss: -0.6868801116943359
Batch 30/64 loss: -0.6898927688598633
Batch 31/64 loss: -0.5771098136901855
Batch 32/64 loss: -0.7753129005432129
Batch 33/64 loss: -0.8147115707397461
Batch 34/64 loss: -0.5506629943847656
Batch 35/64 loss: -0.6864895820617676
Batch 36/64 loss: -0.7418742179870605
Batch 37/64 loss: -0.6252470016479492
Batch 38/64 loss: -0.8282351493835449
Batch 39/64 loss: -0.808197021484375
Batch 40/64 loss: -0.6065444946289062
Batch 41/64 loss: -0.5401043891906738
Batch 42/64 loss: -0.29739809036254883
Batch 43/64 loss: -0.7014632225036621
Batch 44/64 loss: -0.5681414604187012
Batch 45/64 loss: -0.7098584175109863
Batch 46/64 loss: -0.09049844741821289
Batch 47/64 loss: -0.7291107177734375
Batch 48/64 loss: -0.5643196105957031
Batch 49/64 loss: 0.3374314308166504
Batch 50/64 loss: -0.4340996742248535
Batch 51/64 loss: -0.04984712600708008
Batch 52/64 loss: -0.2776808738708496
Batch 53/64 loss: -0.6187343597412109
Batch 54/64 loss: -0.7338976860046387
Batch 55/64 loss: -0.47231149673461914
Batch 56/64 loss: -0.7015542984008789
Batch 57/64 loss: -0.6205854415893555
Batch 58/64 loss: -0.6665148735046387
Batch 59/64 loss: 0.08571767807006836
Batch 60/64 loss: -0.8008179664611816
Batch 61/64 loss: -0.833958625793457
Batch 62/64 loss: -0.38928651809692383
Batch 63/64 loss: -0.5105447769165039
Batch 64/64 loss: -4.285790920257568
Epoch 403  Train loss: -0.5831187173431995  Val loss: -0.9422743099251973
Epoch 404
-------------------------------
Batch 1/64 loss: -0.7674455642700195
Batch 2/64 loss: -0.5414938926696777
Batch 3/64 loss: -0.3806147575378418
Batch 4/64 loss: -0.8356280326843262
Batch 5/64 loss: 0.5987973213195801
Batch 6/64 loss: -0.7906084060668945
Batch 7/64 loss: -0.6854233741760254
Batch 8/64 loss: -0.7231135368347168
Batch 9/64 loss: -0.7933683395385742
Batch 10/64 loss: -0.7354354858398438
Batch 11/64 loss: -0.6980099678039551
Batch 12/64 loss: -0.459932804107666
Batch 13/64 loss: -0.4786076545715332
Batch 14/64 loss: -0.6755290031433105
Batch 15/64 loss: -0.8922386169433594
Batch 16/64 loss: -0.7058262825012207
Batch 17/64 loss: -0.6368017196655273
Batch 18/64 loss: -0.5062751770019531
Batch 19/64 loss: -0.5911831855773926
Batch 20/64 loss: -0.2785334587097168
Batch 21/64 loss: -0.8661994934082031
Batch 22/64 loss: -0.7661910057067871
Batch 23/64 loss: -0.843447208404541
Batch 24/64 loss: -0.7007904052734375
Batch 25/64 loss: -0.6840686798095703
Batch 26/64 loss: -0.6728262901306152
Batch 27/64 loss: 0.29558610916137695
Batch 28/64 loss: -0.6152448654174805
Batch 29/64 loss: -0.6838960647583008
Batch 30/64 loss: -0.5821294784545898
Batch 31/64 loss: -0.7045516967773438
Batch 32/64 loss: 0.943934440612793
Batch 33/64 loss: -0.030995845794677734
Batch 34/64 loss: -0.7572855949401855
Batch 35/64 loss: -0.7683372497558594
Batch 36/64 loss: -0.6522665023803711
Batch 37/64 loss: -0.6361761093139648
Batch 38/64 loss: -0.2884812355041504
Batch 39/64 loss: -0.8045926094055176
Batch 40/64 loss: -0.5810136795043945
Batch 41/64 loss: -0.7480731010437012
Batch 42/64 loss: -0.46656274795532227
Batch 43/64 loss: -0.4174356460571289
Batch 44/64 loss: -0.6154494285583496
Batch 45/64 loss: -0.2770876884460449
Batch 46/64 loss: -0.7435126304626465
Batch 47/64 loss: -0.43013429641723633
Batch 48/64 loss: -0.40323972702026367
Batch 49/64 loss: -0.5829787254333496
Batch 50/64 loss: -0.4726834297180176
Batch 51/64 loss: -0.633882999420166
Batch 52/64 loss: -0.5991992950439453
Batch 53/64 loss: -0.6393289566040039
Batch 54/64 loss: -0.3979015350341797
Batch 55/64 loss: -0.7177424430847168
Batch 56/64 loss: -0.7289505004882812
Batch 57/64 loss: -0.6806001663208008
Batch 58/64 loss: -0.6859889030456543
Batch 59/64 loss: -0.4341001510620117
Batch 60/64 loss: -0.05898427963256836
Batch 61/64 loss: -0.7508087158203125
Batch 62/64 loss: -0.6653122901916504
Batch 63/64 loss: -0.7378168106079102
Batch 64/64 loss: -4.402581214904785
Epoch 404  Train loss: -0.5986816593244964  Val loss: -0.9784553763792687
Epoch 405
-------------------------------
Batch 1/64 loss: -0.727696418762207
Batch 2/64 loss: -0.7375292778015137
Batch 3/64 loss: -0.754946231842041
Batch 4/64 loss: -0.6166138648986816
Batch 5/64 loss: -0.7135176658630371
Batch 6/64 loss: -0.743934154510498
Batch 7/64 loss: -0.5719141960144043
Batch 8/64 loss: -0.008887290954589844
Batch 9/64 loss: -0.4899024963378906
Batch 10/64 loss: -0.7700800895690918
Batch 11/64 loss: -0.5731110572814941
Batch 12/64 loss: -0.41481971740722656
Batch 13/64 loss: -0.5918388366699219
Batch 14/64 loss: -0.2776174545288086
Batch 15/64 loss: 0.8403277397155762
Batch 16/64 loss: -0.5523309707641602
Batch 17/64 loss: -0.42887258529663086
Batch 18/64 loss: 0.6059627532958984
Batch 19/64 loss: -0.8222136497497559
Batch 20/64 loss: -0.4118919372558594
Batch 21/64 loss: -0.35393810272216797
Batch 22/64 loss: -0.7921876907348633
Batch 23/64 loss: -0.45847177505493164
Batch 24/64 loss: -0.7632336616516113
Batch 25/64 loss: -0.6196279525756836
Batch 26/64 loss: -0.40601587295532227
Batch 27/64 loss: -0.47989845275878906
Batch 28/64 loss: -0.6315393447875977
Batch 29/64 loss: -0.7171769142150879
Batch 30/64 loss: -0.7308087348937988
Batch 31/64 loss: -0.699587345123291
Batch 32/64 loss: -0.5501365661621094
Batch 33/64 loss: -0.7477564811706543
Batch 34/64 loss: -0.19396495819091797
Batch 35/64 loss: -0.5242819786071777
Batch 36/64 loss: -0.7072367668151855
Batch 37/64 loss: -0.6232485771179199
Batch 38/64 loss: -0.01127767562866211
Batch 39/64 loss: -0.9187154769897461
Batch 40/64 loss: -0.5619258880615234
Batch 41/64 loss: -0.642179012298584
Batch 42/64 loss: -0.6315145492553711
Batch 43/64 loss: -0.45257043838500977
Batch 44/64 loss: 0.4026312828063965
Batch 45/64 loss: -0.5901250839233398
Batch 46/64 loss: -0.5668535232543945
Batch 47/64 loss: -0.2508268356323242
Batch 48/64 loss: -0.7361326217651367
Batch 49/64 loss: -0.7277693748474121
Batch 50/64 loss: -0.4120621681213379
Batch 51/64 loss: -0.7183742523193359
Batch 52/64 loss: -0.6209745407104492
Batch 53/64 loss: -0.5181112289428711
Batch 54/64 loss: -0.6255931854248047
Batch 55/64 loss: -0.7020564079284668
Batch 56/64 loss: -0.6858291625976562
Batch 57/64 loss: -0.6666245460510254
Batch 58/64 loss: -0.43559932708740234
Batch 59/64 loss: -0.7973837852478027
Batch 60/64 loss: -0.6237869262695312
Batch 61/64 loss: -0.8264994621276855
Batch 62/64 loss: -0.19830083847045898
Batch 63/64 loss: -0.7384142875671387
Batch 64/64 loss: -4.394107818603516
Epoch 405  Train loss: -0.5696155099307789  Val loss: -0.9449221030953004
Epoch 406
-------------------------------
Batch 1/64 loss: -0.6115107536315918
Batch 2/64 loss: -0.5736727714538574
Batch 3/64 loss: -0.650667667388916
Batch 4/64 loss: -0.7021036148071289
Batch 5/64 loss: -0.8719167709350586
Batch 6/64 loss: -0.7385268211364746
Batch 7/64 loss: -0.44635772705078125
Batch 8/64 loss: -0.3905954360961914
Batch 9/64 loss: -0.07825708389282227
Batch 10/64 loss: -0.6863627433776855
Batch 11/64 loss: -0.47144269943237305
Batch 12/64 loss: -0.719367504119873
Batch 13/64 loss: -0.708125114440918
Batch 14/64 loss: 1.5567212104797363
Batch 15/64 loss: -0.8202638626098633
Batch 16/64 loss: -0.7702155113220215
Batch 17/64 loss: -0.553901195526123
Batch 18/64 loss: -0.739173412322998
Batch 19/64 loss: -0.38069915771484375
Batch 20/64 loss: -0.3911623954772949
Batch 21/64 loss: -0.6332826614379883
Batch 22/64 loss: -0.706512451171875
Batch 23/64 loss: -0.6415095329284668
Batch 24/64 loss: -0.7842497825622559
Batch 25/64 loss: -0.5279755592346191
Batch 26/64 loss: -0.8398351669311523
Batch 27/64 loss: -0.7146930694580078
Batch 28/64 loss: -0.6281814575195312
Batch 29/64 loss: -0.7868361473083496
Batch 30/64 loss: -0.8089785575866699
Batch 31/64 loss: -0.8296799659729004
Batch 32/64 loss: -0.6124711036682129
Batch 33/64 loss: -0.6980781555175781
Batch 34/64 loss: 0.33936548233032227
Batch 35/64 loss: -0.5702152252197266
Batch 36/64 loss: -0.7356486320495605
Batch 37/64 loss: -0.7865943908691406
Batch 38/64 loss: -0.7817788124084473
Batch 39/64 loss: -0.6375470161437988
Batch 40/64 loss: -0.7406458854675293
Batch 41/64 loss: -0.5559887886047363
Batch 42/64 loss: -0.44808340072631836
Batch 43/64 loss: -0.6990451812744141
Batch 44/64 loss: -0.2332448959350586
Batch 45/64 loss: -0.44466209411621094
Batch 46/64 loss: -0.5981383323669434
Batch 47/64 loss: -0.3213505744934082
Batch 48/64 loss: -0.11241006851196289
Batch 49/64 loss: -0.8112072944641113
Batch 50/64 loss: -0.8115983009338379
Batch 51/64 loss: -0.6774411201477051
Batch 52/64 loss: -0.2781863212585449
Batch 53/64 loss: -0.7492251396179199
Batch 54/64 loss: -0.7821440696716309
Batch 55/64 loss: -0.5180840492248535
Batch 56/64 loss: -0.43070220947265625
Batch 57/64 loss: -0.6341619491577148
Batch 58/64 loss: -0.6707453727722168
Batch 59/64 loss: -0.06353330612182617
Batch 60/64 loss: -0.1364293098449707
Batch 61/64 loss: -0.7497982978820801
Batch 62/64 loss: -0.6145501136779785
Batch 63/64 loss: 0.5470542907714844
Batch 64/64 loss: -4.315996170043945
Epoch 406  Train loss: -0.5788803474575865  Val loss: -0.9517499654973086
Epoch 407
-------------------------------
Batch 1/64 loss: -0.8656268119812012
Batch 2/64 loss: -0.6584901809692383
Batch 3/64 loss: -0.7760157585144043
Batch 4/64 loss: -0.4052553176879883
Batch 5/64 loss: -0.5652556419372559
Batch 6/64 loss: -0.28568124771118164
Batch 7/64 loss: -0.6780104637145996
Batch 8/64 loss: -0.7619948387145996
Batch 9/64 loss: -0.801398754119873
Batch 10/64 loss: -0.5385375022888184
Batch 11/64 loss: 0.559262752532959
Batch 12/64 loss: -0.5911798477172852
Batch 13/64 loss: -0.4969334602355957
Batch 14/64 loss: -0.5650696754455566
Batch 15/64 loss: -0.7174263000488281
Batch 16/64 loss: -0.641016960144043
Batch 17/64 loss: -0.7183194160461426
Batch 18/64 loss: -0.707603931427002
Batch 19/64 loss: -0.7407732009887695
Batch 20/64 loss: -0.7217884063720703
Batch 21/64 loss: -0.5874457359313965
Batch 22/64 loss: -0.6694817543029785
Batch 23/64 loss: -0.5651803016662598
Batch 24/64 loss: -0.6763558387756348
Batch 25/64 loss: -0.5740046501159668
Batch 26/64 loss: -0.6736836433410645
Batch 27/64 loss: -0.7933754920959473
Batch 28/64 loss: -0.7741670608520508
Batch 29/64 loss: -0.44433164596557617
Batch 30/64 loss: -0.5818424224853516
Batch 31/64 loss: -0.3575935363769531
Batch 32/64 loss: -0.7635297775268555
Batch 33/64 loss: -0.5978198051452637
Batch 34/64 loss: -0.7027649879455566
Batch 35/64 loss: -0.8131818771362305
Batch 36/64 loss: 0.9356565475463867
Batch 37/64 loss: 0.1267991065979004
Batch 38/64 loss: -0.6628684997558594
Batch 39/64 loss: -0.6021499633789062
Batch 40/64 loss: -0.6865134239196777
Batch 41/64 loss: 0.1040034294128418
Batch 42/64 loss: -0.678107738494873
Batch 43/64 loss: -0.5887022018432617
Batch 44/64 loss: -0.7261233329772949
Batch 45/64 loss: -0.7569408416748047
Batch 46/64 loss: -0.802241325378418
Batch 47/64 loss: -0.7450275421142578
Batch 48/64 loss: -0.559720516204834
Batch 49/64 loss: -0.7053494453430176
Batch 50/64 loss: -0.4738788604736328
Batch 51/64 loss: -0.44979095458984375
Batch 52/64 loss: -0.7502808570861816
Batch 53/64 loss: -0.6244878768920898
Batch 54/64 loss: 0.8132991790771484
Batch 55/64 loss: -0.6076464653015137
Batch 56/64 loss: -0.6277847290039062
Batch 57/64 loss: -0.7215642929077148
Batch 58/64 loss: -0.5118017196655273
Batch 59/64 loss: -0.4884772300720215
Batch 60/64 loss: -0.6496996879577637
Batch 61/64 loss: -0.7664027214050293
Batch 62/64 loss: 0.2693190574645996
Batch 63/64 loss: -0.5553832054138184
Batch 64/64 loss: -4.467157363891602
Epoch 407  Train loss: -0.5818683549469592  Val loss: -0.8150817897311601
Epoch 408
-------------------------------
Batch 1/64 loss: -0.6233978271484375
Batch 2/64 loss: 0.409759521484375
Batch 3/64 loss: -0.6343722343444824
Batch 4/64 loss: 0.05942058563232422
Batch 5/64 loss: -0.6706323623657227
Batch 6/64 loss: -0.8008532524108887
Batch 7/64 loss: -0.49201154708862305
Batch 8/64 loss: -0.5789813995361328
Batch 9/64 loss: -0.2046799659729004
Batch 10/64 loss: -0.5416994094848633
Batch 11/64 loss: -0.6854181289672852
Batch 12/64 loss: 0.9752459526062012
Batch 13/64 loss: -0.4248356819152832
Batch 14/64 loss: -0.7517452239990234
Batch 15/64 loss: -0.17923450469970703
Batch 16/64 loss: 1.4147233963012695
Batch 17/64 loss: -0.6810956001281738
Batch 18/64 loss: 0.7157077789306641
Batch 19/64 loss: -0.46140480041503906
Batch 20/64 loss: 0.13115549087524414
Batch 21/64 loss: 0.525846004486084
Batch 22/64 loss: -0.34340476989746094
Batch 23/64 loss: 0.06770896911621094
Batch 24/64 loss: -0.0928349494934082
Batch 25/64 loss: -0.5213479995727539
Batch 26/64 loss: -0.14100170135498047
Batch 27/64 loss: -0.7221083641052246
Batch 28/64 loss: -0.41437578201293945
Batch 29/64 loss: -0.5573983192443848
Batch 30/64 loss: 0.04332542419433594
Batch 31/64 loss: -0.617121696472168
Batch 32/64 loss: -0.37793731689453125
Batch 33/64 loss: -0.5177087783813477
Batch 34/64 loss: 0.48106861114501953
Batch 35/64 loss: -0.5259499549865723
Batch 36/64 loss: -0.25136613845825195
Batch 37/64 loss: -0.5923347473144531
Batch 38/64 loss: -0.41849613189697266
Batch 39/64 loss: -0.7221293449401855
Batch 40/64 loss: -0.6374049186706543
Batch 41/64 loss: -0.7483797073364258
Batch 42/64 loss: -0.41060781478881836
Batch 43/64 loss: -0.43634510040283203
Batch 44/64 loss: -0.6352615356445312
Batch 45/64 loss: -0.5586252212524414
Batch 46/64 loss: -0.7534828186035156
Batch 47/64 loss: -0.46065521240234375
Batch 48/64 loss: -0.46526098251342773
Batch 49/64 loss: -0.5570998191833496
Batch 50/64 loss: -0.7039132118225098
Batch 51/64 loss: -0.6288480758666992
Batch 52/64 loss: -0.43048524856567383
Batch 53/64 loss: -0.31648874282836914
Batch 54/64 loss: 0.39818239212036133
Batch 55/64 loss: -0.62481689453125
Batch 56/64 loss: -0.7227005958557129
Batch 57/64 loss: 0.9130282402038574
Batch 58/64 loss: -0.6717863082885742
Batch 59/64 loss: -0.6283974647521973
Batch 60/64 loss: -0.3012247085571289
Batch 61/64 loss: -0.6904606819152832
Batch 62/64 loss: -0.7479104995727539
Batch 63/64 loss: -0.4294929504394531
Batch 64/64 loss: -4.3241658210754395
Epoch 408  Train loss: -0.3798192697412827  Val loss: -0.9171334754970065
Epoch 409
-------------------------------
Batch 1/64 loss: -0.636293888092041
Batch 2/64 loss: -0.7211575508117676
Batch 3/64 loss: -0.16623878479003906
Batch 4/64 loss: 0.47638511657714844
Batch 5/64 loss: -0.6918702125549316
Batch 6/64 loss: -0.542015552520752
Batch 7/64 loss: -0.49971818923950195
Batch 8/64 loss: -0.8071074485778809
Batch 9/64 loss: -0.6450071334838867
Batch 10/64 loss: -0.5276789665222168
Batch 11/64 loss: -0.5667433738708496
Batch 12/64 loss: -0.5985822677612305
Batch 13/64 loss: -0.6875262260437012
Batch 14/64 loss: -0.6302971839904785
Batch 15/64 loss: -0.6452522277832031
Batch 16/64 loss: -0.41352081298828125
Batch 17/64 loss: -0.5993819236755371
Batch 18/64 loss: -0.6228342056274414
Batch 19/64 loss: -0.057179927825927734
Batch 20/64 loss: -0.7100415229797363
Batch 21/64 loss: -0.5328831672668457
Batch 22/64 loss: -0.38192224502563477
Batch 23/64 loss: -0.6391949653625488
Batch 24/64 loss: -0.6827244758605957
Batch 25/64 loss: -0.7684669494628906
Batch 26/64 loss: -0.4507918357849121
Batch 27/64 loss: -0.23958587646484375
Batch 28/64 loss: -0.7599844932556152
Batch 29/64 loss: -0.5365324020385742
Batch 30/64 loss: -0.7730298042297363
Batch 31/64 loss: -0.6881961822509766
Batch 32/64 loss: -0.649146556854248
Batch 33/64 loss: -0.5572400093078613
Batch 34/64 loss: -0.6915278434753418
Batch 35/64 loss: -0.7702121734619141
Batch 36/64 loss: -0.6302266120910645
Batch 37/64 loss: -0.5807361602783203
Batch 38/64 loss: -0.6144852638244629
Batch 39/64 loss: -0.5527901649475098
Batch 40/64 loss: -0.44322729110717773
Batch 41/64 loss: -0.6301374435424805
Batch 42/64 loss: -0.4439239501953125
Batch 43/64 loss: -0.5189676284790039
Batch 44/64 loss: -0.8027749061584473
Batch 45/64 loss: -0.5991778373718262
Batch 46/64 loss: -0.5755600929260254
Batch 47/64 loss: -0.7111663818359375
Batch 48/64 loss: -0.7823219299316406
Batch 49/64 loss: 0.326751708984375
Batch 50/64 loss: -0.329864501953125
Batch 51/64 loss: -0.5484805107116699
Batch 52/64 loss: 0.9517083168029785
Batch 53/64 loss: -0.7211771011352539
Batch 54/64 loss: -0.6154732704162598
Batch 55/64 loss: -0.7217645645141602
Batch 56/64 loss: -0.6456398963928223
Batch 57/64 loss: -0.22603750228881836
Batch 58/64 loss: -0.09917736053466797
Batch 59/64 loss: -0.4534897804260254
Batch 60/64 loss: -0.5188732147216797
Batch 61/64 loss: -0.6130833625793457
Batch 62/64 loss: -0.6753096580505371
Batch 63/64 loss: -0.6498904228210449
Batch 64/64 loss: -4.318024635314941
Epoch 409  Train loss: -0.5659186905505611  Val loss: -0.8641231051834998
Epoch 410
-------------------------------
Batch 1/64 loss: -0.7113437652587891
Batch 2/64 loss: -0.7937831878662109
Batch 3/64 loss: 0.6482062339782715
Batch 4/64 loss: -0.6161565780639648
Batch 5/64 loss: -0.627223014831543
Batch 6/64 loss: -0.4166145324707031
Batch 7/64 loss: -0.5259275436401367
Batch 8/64 loss: -0.7202668190002441
Batch 9/64 loss: -0.5349273681640625
Batch 10/64 loss: -0.5257368087768555
Batch 11/64 loss: -0.6615805625915527
Batch 12/64 loss: -0.6033225059509277
Batch 13/64 loss: -0.6304869651794434
Batch 14/64 loss: -0.5324921607971191
Batch 15/64 loss: -0.6198654174804688
Batch 16/64 loss: -0.7289280891418457
Batch 17/64 loss: -0.595334529876709
Batch 18/64 loss: -0.22592830657958984
Batch 19/64 loss: -0.4876737594604492
Batch 20/64 loss: -0.7332472801208496
Batch 21/64 loss: -0.6118278503417969
Batch 22/64 loss: -0.6751899719238281
Batch 23/64 loss: 0.3027048110961914
Batch 24/64 loss: -0.6773347854614258
Batch 25/64 loss: -0.8142952919006348
Batch 26/64 loss: -0.6782341003417969
Batch 27/64 loss: -0.806889533996582
Batch 28/64 loss: -0.6757307052612305
Batch 29/64 loss: -0.5907940864562988
Batch 30/64 loss: -0.6489114761352539
Batch 31/64 loss: -0.7307844161987305
Batch 32/64 loss: -0.7314128875732422
Batch 33/64 loss: -0.4796719551086426
Batch 34/64 loss: -0.7677106857299805
Batch 35/64 loss: -0.6483669281005859
Batch 36/64 loss: -0.6633396148681641
Batch 37/64 loss: -0.5622892379760742
Batch 38/64 loss: -0.4807586669921875
Batch 39/64 loss: -0.6095499992370605
Batch 40/64 loss: -0.6727972030639648
Batch 41/64 loss: -0.8017807006835938
Batch 42/64 loss: -0.5696344375610352
Batch 43/64 loss: -0.5078396797180176
Batch 44/64 loss: -0.5884203910827637
Batch 45/64 loss: -0.8038711547851562
Batch 46/64 loss: -0.3311924934387207
Batch 47/64 loss: -0.6888599395751953
Batch 48/64 loss: -0.8405227661132812
Batch 49/64 loss: -0.7416954040527344
Batch 50/64 loss: -0.5803232192993164
Batch 51/64 loss: -0.28489160537719727
Batch 52/64 loss: -0.7568268775939941
Batch 53/64 loss: -0.7493529319763184
Batch 54/64 loss: -0.04754495620727539
Batch 55/64 loss: -0.6662511825561523
Batch 56/64 loss: -0.7694540023803711
Batch 57/64 loss: -0.5381946563720703
Batch 58/64 loss: -0.23081588745117188
Batch 59/64 loss: -0.44308996200561523
Batch 60/64 loss: -0.5998430252075195
Batch 61/64 loss: -0.7700109481811523
Batch 62/64 loss: -0.5459680557250977
Batch 63/64 loss: -0.7174038887023926
Batch 64/64 loss: -2.291025161743164
Epoch 410  Train loss: -0.5985548954384  Val loss: -1.008074652288378
Epoch 411
-------------------------------
Batch 1/64 loss: -0.8304281234741211
Batch 2/64 loss: -0.6023831367492676
Batch 3/64 loss: -0.6362829208374023
Batch 4/64 loss: -0.6873893737792969
Batch 5/64 loss: 0.8915696144104004
Batch 6/64 loss: -0.7925925254821777
Batch 7/64 loss: -0.7818918228149414
Batch 8/64 loss: -0.6159968376159668
Batch 9/64 loss: -0.582944393157959
Batch 10/64 loss: -0.22855663299560547
Batch 11/64 loss: -0.6954398155212402
Batch 12/64 loss: -0.6318612098693848
Batch 13/64 loss: -0.7636032104492188
Batch 14/64 loss: -0.3755216598510742
Batch 15/64 loss: -0.4588046073913574
Batch 16/64 loss: -0.5373697280883789
Batch 17/64 loss: -0.7002358436584473
Batch 18/64 loss: -0.760871410369873
Batch 19/64 loss: -0.6854023933410645
Batch 20/64 loss: -0.7411074638366699
Batch 21/64 loss: -0.7805285453796387
Batch 22/64 loss: -0.43659019470214844
Batch 23/64 loss: -0.4892873764038086
Batch 24/64 loss: -0.8245973587036133
Batch 25/64 loss: -0.6617183685302734
Batch 26/64 loss: -0.7358493804931641
Batch 27/64 loss: -0.4486074447631836
Batch 28/64 loss: -0.31203651428222656
Batch 29/64 loss: -0.643214225769043
Batch 30/64 loss: -0.5722460746765137
Batch 31/64 loss: -0.6062784194946289
Batch 32/64 loss: -0.6370754241943359
Batch 33/64 loss: -0.6257872581481934
Batch 34/64 loss: -0.715540885925293
Batch 35/64 loss: -0.583378791809082
Batch 36/64 loss: -0.6970839500427246
Batch 37/64 loss: -0.7842545509338379
Batch 38/64 loss: -0.4421553611755371
Batch 39/64 loss: -0.6662797927856445
Batch 40/64 loss: -0.6583170890808105
Batch 41/64 loss: -0.7016801834106445
Batch 42/64 loss: -0.7566156387329102
Batch 43/64 loss: -0.7085428237915039
Batch 44/64 loss: -0.6944026947021484
Batch 45/64 loss: -0.7075657844543457
Batch 46/64 loss: 0.23344850540161133
Batch 47/64 loss: -0.7432074546813965
Batch 48/64 loss: 0.8448729515075684
Batch 49/64 loss: -0.6697921752929688
Batch 50/64 loss: -0.739778995513916
Batch 51/64 loss: -0.5846471786499023
Batch 52/64 loss: -0.6902518272399902
Batch 53/64 loss: -0.7885351181030273
Batch 54/64 loss: -0.44539833068847656
Batch 55/64 loss: -0.44114255905151367
Batch 56/64 loss: 0.47757530212402344
Batch 57/64 loss: -0.7402219772338867
Batch 58/64 loss: -0.7437305450439453
Batch 59/64 loss: -0.7316856384277344
Batch 60/64 loss: -0.7744555473327637
Batch 61/64 loss: -0.5737214088439941
Batch 62/64 loss: -0.7198071479797363
Batch 63/64 loss: -0.6392693519592285
Batch 64/64 loss: -4.271170139312744
Epoch 411  Train loss: -0.6083117447647394  Val loss: -0.9563183342058634
Epoch 412
-------------------------------
Batch 1/64 loss: -0.6543359756469727
Batch 2/64 loss: -0.7826976776123047
Batch 3/64 loss: -0.7742395401000977
Batch 4/64 loss: -0.7726683616638184
Batch 5/64 loss: -0.7777180671691895
Batch 6/64 loss: -0.5155344009399414
Batch 7/64 loss: -0.49958324432373047
Batch 8/64 loss: -0.6768074035644531
Batch 9/64 loss: -0.675412654876709
Batch 10/64 loss: -0.6085891723632812
Batch 11/64 loss: -0.7801351547241211
Batch 12/64 loss: -0.7607536315917969
Batch 13/64 loss: -0.7510075569152832
Batch 14/64 loss: -0.7958927154541016
Batch 15/64 loss: -0.32291078567504883
Batch 16/64 loss: -0.6910309791564941
Batch 17/64 loss: -0.1783761978149414
Batch 18/64 loss: -0.41033267974853516
Batch 19/64 loss: -0.6271858215332031
Batch 20/64 loss: 0.5338912010192871
Batch 21/64 loss: -0.4208240509033203
Batch 22/64 loss: -0.8622908592224121
Batch 23/64 loss: -0.6124887466430664
Batch 24/64 loss: -0.7833733558654785
Batch 25/64 loss: -0.7163052558898926
Batch 26/64 loss: -0.8016533851623535
Batch 27/64 loss: -0.5830297470092773
Batch 28/64 loss: -0.17967748641967773
Batch 29/64 loss: -0.36907291412353516
Batch 30/64 loss: -0.5741095542907715
Batch 31/64 loss: -0.4776124954223633
Batch 32/64 loss: -0.5254330635070801
Batch 33/64 loss: 1.7014389038085938
Batch 34/64 loss: 0.10155630111694336
Batch 35/64 loss: -0.41027116775512695
Batch 36/64 loss: -0.2706780433654785
Batch 37/64 loss: -0.5426154136657715
Batch 38/64 loss: -0.448153018951416
Batch 39/64 loss: -0.4886627197265625
Batch 40/64 loss: -0.542536735534668
Batch 41/64 loss: -0.3988785743713379
Batch 42/64 loss: -0.6440310478210449
Batch 43/64 loss: -0.6253681182861328
Batch 44/64 loss: -0.5681643486022949
Batch 45/64 loss: -0.42643117904663086
Batch 46/64 loss: -0.2534651756286621
Batch 47/64 loss: -0.3543119430541992
Batch 48/64 loss: -0.5643243789672852
Batch 49/64 loss: -0.4353780746459961
Batch 50/64 loss: -0.4288797378540039
Batch 51/64 loss: 1.1261496543884277
Batch 52/64 loss: -0.51220703125
Batch 53/64 loss: -0.5872254371643066
Batch 54/64 loss: -0.6394877433776855
Batch 55/64 loss: -0.6946907043457031
Batch 56/64 loss: -0.6661529541015625
Batch 57/64 loss: -0.7021112442016602
Batch 58/64 loss: -0.5086731910705566
Batch 59/64 loss: -0.5078949928283691
Batch 60/64 loss: -0.8420538902282715
Batch 61/64 loss: -0.6464114189147949
Batch 62/64 loss: -0.6396689414978027
Batch 63/64 loss: -0.5093488693237305
Batch 64/64 loss: -4.4378509521484375
Epoch 412  Train loss: -0.5283844891716453  Val loss: -0.8551419051652102
Epoch 413
-------------------------------
Batch 1/64 loss: -0.6570816040039062
Batch 2/64 loss: -0.8272976875305176
Batch 3/64 loss: -0.4919319152832031
Batch 4/64 loss: -0.6799707412719727
Batch 5/64 loss: -0.711021900177002
Batch 6/64 loss: -0.8419094085693359
Batch 7/64 loss: -0.5649323463439941
Batch 8/64 loss: -0.8326740264892578
Batch 9/64 loss: -0.7177972793579102
Batch 10/64 loss: -0.586669921875
Batch 11/64 loss: 1.717921257019043
Batch 12/64 loss: -0.4213724136352539
Batch 13/64 loss: -0.821742057800293
Batch 14/64 loss: -0.5988073348999023
Batch 15/64 loss: -0.6190824508666992
Batch 16/64 loss: -0.7334232330322266
Batch 17/64 loss: -0.38184642791748047
Batch 18/64 loss: -0.6544508934020996
Batch 19/64 loss: -0.6381988525390625
Batch 20/64 loss: -0.6753511428833008
Batch 21/64 loss: -0.5099210739135742
Batch 22/64 loss: -0.7199859619140625
Batch 23/64 loss: -0.7507233619689941
Batch 24/64 loss: -0.7754068374633789
Batch 25/64 loss: -0.15598392486572266
Batch 26/64 loss: 0.10848140716552734
Batch 27/64 loss: 0.6564497947692871
Batch 28/64 loss: -0.73284912109375
Batch 29/64 loss: -0.6928081512451172
Batch 30/64 loss: -0.6790952682495117
Batch 31/64 loss: -0.14721393585205078
Batch 32/64 loss: -0.7715749740600586
Batch 33/64 loss: -0.5257635116577148
Batch 34/64 loss: -0.6771335601806641
Batch 35/64 loss: -0.7918353080749512
Batch 36/64 loss: -0.5227112770080566
Batch 37/64 loss: -0.11176300048828125
Batch 38/64 loss: -0.8748540878295898
Batch 39/64 loss: -0.8343977928161621
Batch 40/64 loss: -0.6464810371398926
Batch 41/64 loss: -0.722130298614502
Batch 42/64 loss: -0.7172088623046875
Batch 43/64 loss: -0.8076586723327637
Batch 44/64 loss: -0.742215633392334
Batch 45/64 loss: -0.5003485679626465
Batch 46/64 loss: -0.7945137023925781
Batch 47/64 loss: -0.788938045501709
Batch 48/64 loss: -0.77197265625
Batch 49/64 loss: -0.5912690162658691
Batch 50/64 loss: -0.7521853446960449
Batch 51/64 loss: -0.18043136596679688
Batch 52/64 loss: -0.8065204620361328
Batch 53/64 loss: 0.29306507110595703
Batch 54/64 loss: -0.7348213195800781
Batch 55/64 loss: -0.7191338539123535
Batch 56/64 loss: -0.009968757629394531
Batch 57/64 loss: -0.6582579612731934
Batch 58/64 loss: -0.6412296295166016
Batch 59/64 loss: -0.8890719413757324
Batch 60/64 loss: 0.9504380226135254
Batch 61/64 loss: 0.2104191780090332
Batch 62/64 loss: -0.882880687713623
Batch 63/64 loss: -0.8794660568237305
Batch 64/64 loss: -4.486367702484131
Epoch 413  Train loss: -0.5708907875360227  Val loss: -1.0274967311583842
Epoch 414
-------------------------------
Batch 1/64 loss: -0.746035099029541
Batch 2/64 loss: -0.6418852806091309
Batch 3/64 loss: 1.4392104148864746
Batch 4/64 loss: 0.17350339889526367
Batch 5/64 loss: -0.6589241027832031
Batch 6/64 loss: -0.7860927581787109
Batch 7/64 loss: -0.9179301261901855
Batch 8/64 loss: -0.6984553337097168
Batch 9/64 loss: -0.7306852340698242
Batch 10/64 loss: 0.027037620544433594
Batch 11/64 loss: -0.6784372329711914
Batch 12/64 loss: -0.7102599143981934
Batch 13/64 loss: -0.7543425559997559
Batch 14/64 loss: -0.6120438575744629
Batch 15/64 loss: 0.07884836196899414
Batch 16/64 loss: -0.7721743583679199
Batch 17/64 loss: -0.7940645217895508
Batch 18/64 loss: -0.7082533836364746
Batch 19/64 loss: -0.44161462783813477
Batch 20/64 loss: -0.865056037902832
Batch 21/64 loss: -0.778223991394043
Batch 22/64 loss: -0.36965370178222656
Batch 23/64 loss: -0.7494010925292969
Batch 24/64 loss: -0.6286406517028809
Batch 25/64 loss: -0.7874093055725098
Batch 26/64 loss: -0.551002025604248
Batch 27/64 loss: -0.7492656707763672
Batch 28/64 loss: -0.48816347122192383
Batch 29/64 loss: -0.7892980575561523
Batch 30/64 loss: 0.30625486373901367
Batch 31/64 loss: -0.7573542594909668
Batch 32/64 loss: -0.5258817672729492
Batch 33/64 loss: -0.7858290672302246
Batch 34/64 loss: -0.7778196334838867
Batch 35/64 loss: -0.813629150390625
Batch 36/64 loss: -0.47101640701293945
Batch 37/64 loss: -0.9065394401550293
Batch 38/64 loss: -0.7693886756896973
Batch 39/64 loss: -0.8650979995727539
Batch 40/64 loss: -0.8310532569885254
Batch 41/64 loss: -0.7448582649230957
Batch 42/64 loss: -0.8030099868774414
Batch 43/64 loss: -0.561769962310791
Batch 44/64 loss: -0.4746212959289551
Batch 45/64 loss: -0.8529057502746582
Batch 46/64 loss: -0.8943886756896973
Batch 47/64 loss: 0.7996644973754883
Batch 48/64 loss: -0.8074026107788086
Batch 49/64 loss: -0.897667407989502
Batch 50/64 loss: -0.7342085838317871
Batch 51/64 loss: -0.5477361679077148
Batch 52/64 loss: -0.904881477355957
Batch 53/64 loss: -0.9311752319335938
Batch 54/64 loss: 0.5956377983093262
Batch 55/64 loss: -0.8520879745483398
Batch 56/64 loss: -0.7240109443664551
Batch 57/64 loss: -0.8189725875854492
Batch 58/64 loss: -0.7995448112487793
Batch 59/64 loss: -0.7465367317199707
Batch 60/64 loss: 0.019417762756347656
Batch 61/64 loss: -0.6076021194458008
Batch 62/64 loss: -0.6168231964111328
Batch 63/64 loss: -0.46100711822509766
Batch 64/64 loss: -4.409880638122559
Epoch 414  Train loss: -0.6205485287834617  Val loss: -1.0475412218021773
Epoch 415
-------------------------------
Batch 1/64 loss: 0.37343597412109375
Batch 2/64 loss: -0.4580554962158203
Batch 3/64 loss: -0.7347593307495117
Batch 4/64 loss: 0.5136604309082031
Batch 5/64 loss: -0.6166138648986816
Batch 6/64 loss: -0.7473249435424805
Batch 7/64 loss: -0.06801652908325195
Batch 8/64 loss: -0.7978372573852539
Batch 9/64 loss: 1.6593494415283203
Batch 10/64 loss: -0.727907657623291
Batch 11/64 loss: -0.8237967491149902
Batch 12/64 loss: -0.7800755500793457
Batch 13/64 loss: -0.8278799057006836
Batch 14/64 loss: -0.6731481552124023
Batch 15/64 loss: -0.8114843368530273
Batch 16/64 loss: -0.8528976440429688
Batch 17/64 loss: -0.8695321083068848
Batch 18/64 loss: -0.7570571899414062
Batch 19/64 loss: -0.6700863838195801
Batch 20/64 loss: -0.7227544784545898
Batch 21/64 loss: -0.6770844459533691
Batch 22/64 loss: -0.8098397254943848
Batch 23/64 loss: -0.648468017578125
Batch 24/64 loss: -0.0031366348266601562
Batch 25/64 loss: -0.7969660758972168
Batch 26/64 loss: -0.7483196258544922
Batch 27/64 loss: -0.8179898262023926
Batch 28/64 loss: -0.8505392074584961
Batch 29/64 loss: -0.7384891510009766
Batch 30/64 loss: -0.5459818840026855
Batch 31/64 loss: -0.7903294563293457
Batch 32/64 loss: -0.3345775604248047
Batch 33/64 loss: -0.6737785339355469
Batch 34/64 loss: -0.4185504913330078
Batch 35/64 loss: -0.7367935180664062
Batch 36/64 loss: -0.8233523368835449
Batch 37/64 loss: -0.8306155204772949
Batch 38/64 loss: -0.7513189315795898
Batch 39/64 loss: -0.769618034362793
Batch 40/64 loss: -0.6553215980529785
Batch 41/64 loss: -0.3817253112792969
Batch 42/64 loss: -0.5068602561950684
Batch 43/64 loss: -0.6709737777709961
Batch 44/64 loss: -0.613896369934082
Batch 45/64 loss: -0.35254335403442383
Batch 46/64 loss: -0.6632757186889648
Batch 47/64 loss: -0.755800724029541
Batch 48/64 loss: -0.7453241348266602
Batch 49/64 loss: 0.42175865173339844
Batch 50/64 loss: -0.6704325675964355
Batch 51/64 loss: -0.628777027130127
Batch 52/64 loss: 0.4916086196899414
Batch 53/64 loss: -0.5256428718566895
Batch 54/64 loss: -0.7971234321594238
Batch 55/64 loss: -0.6739597320556641
Batch 56/64 loss: -0.7084131240844727
Batch 57/64 loss: -0.6085672378540039
Batch 58/64 loss: -0.7465023994445801
Batch 59/64 loss: -0.4276738166809082
Batch 60/64 loss: -0.5518498420715332
Batch 61/64 loss: -0.388033390045166
Batch 62/64 loss: -0.6950564384460449
Batch 63/64 loss: -0.8163471221923828
Batch 64/64 loss: -4.2581987380981445
Epoch 415  Train loss: -0.5964378469130572  Val loss: -0.9511432188892692
Epoch 416
-------------------------------
Batch 1/64 loss: -0.5441656112670898
Batch 2/64 loss: -0.6108441352844238
Batch 3/64 loss: -0.7112994194030762
Batch 4/64 loss: -0.4958772659301758
Batch 5/64 loss: -0.48537111282348633
Batch 6/64 loss: -0.3438253402709961
Batch 7/64 loss: -0.09771108627319336
Batch 8/64 loss: -0.4510207176208496
Batch 9/64 loss: 1.3870477676391602
Batch 10/64 loss: -0.7785964012145996
Batch 11/64 loss: -0.6990189552307129
Batch 12/64 loss: -0.7784457206726074
Batch 13/64 loss: -0.6486554145812988
Batch 14/64 loss: -0.8862800598144531
Batch 15/64 loss: -0.7311902046203613
Batch 16/64 loss: -0.7803587913513184
Batch 17/64 loss: -0.046523094177246094
Batch 18/64 loss: 0.21795177459716797
Batch 19/64 loss: -0.6022186279296875
Batch 20/64 loss: -0.7162470817565918
Batch 21/64 loss: -0.7192535400390625
Batch 22/64 loss: -0.7389941215515137
Batch 23/64 loss: -0.7418217658996582
Batch 24/64 loss: -0.8497610092163086
Batch 25/64 loss: -0.7727241516113281
Batch 26/64 loss: -0.9022073745727539
Batch 27/64 loss: -0.6128878593444824
Batch 28/64 loss: 0.4476461410522461
Batch 29/64 loss: -0.789609432220459
Batch 30/64 loss: -0.673393726348877
Batch 31/64 loss: -0.8942809104919434
Batch 32/64 loss: -0.8172540664672852
Batch 33/64 loss: -0.7947607040405273
Batch 34/64 loss: -0.8488526344299316
Batch 35/64 loss: -0.8107566833496094
Batch 36/64 loss: -0.8435707092285156
Batch 37/64 loss: 0.01938915252685547
Batch 38/64 loss: -0.8372097015380859
Batch 39/64 loss: -0.6815199851989746
Batch 40/64 loss: -0.8255405426025391
Batch 41/64 loss: -0.6019473075866699
Batch 42/64 loss: -0.8300251960754395
Batch 43/64 loss: -0.5590200424194336
Batch 44/64 loss: -0.7599148750305176
Batch 45/64 loss: -0.8263335227966309
Batch 46/64 loss: -0.5765514373779297
Batch 47/64 loss: 1.5898332595825195
Batch 48/64 loss: -0.711857795715332
Batch 49/64 loss: -0.8588614463806152
Batch 50/64 loss: -0.953798770904541
Batch 51/64 loss: -0.3705911636352539
Batch 52/64 loss: -0.7264571189880371
Batch 53/64 loss: 0.6625809669494629
Batch 54/64 loss: -0.8965311050415039
Batch 55/64 loss: -0.7830972671508789
Batch 56/64 loss: -0.697838306427002
Batch 57/64 loss: -0.6835951805114746
Batch 58/64 loss: -0.8391637802124023
Batch 59/64 loss: -0.5600390434265137
Batch 60/64 loss: -0.7461328506469727
Batch 61/64 loss: -0.7968716621398926
Batch 62/64 loss: -0.39405393600463867
Batch 63/64 loss: -0.7208447456359863
Batch 64/64 loss: -4.302712440490723
Epoch 416  Train loss: -0.6016966240078795  Val loss: -0.9954009695151418
Epoch 417
-------------------------------
Batch 1/64 loss: -0.7876920700073242
Batch 2/64 loss: -0.7906336784362793
Batch 3/64 loss: -0.4420337677001953
Batch 4/64 loss: -0.6601719856262207
Batch 5/64 loss: -0.25094175338745117
Batch 6/64 loss: -0.22070693969726562
Batch 7/64 loss: -0.6633505821228027
Batch 8/64 loss: -0.874326229095459
Batch 9/64 loss: -0.7083005905151367
Batch 10/64 loss: -0.6408901214599609
Batch 11/64 loss: 1.0894746780395508
Batch 12/64 loss: 0.5742478370666504
Batch 13/64 loss: -0.8201828002929688
Batch 14/64 loss: -0.753608226776123
Batch 15/64 loss: -0.779512882232666
Batch 16/64 loss: -0.11705970764160156
Batch 17/64 loss: -0.8286738395690918
Batch 18/64 loss: -0.8049149513244629
Batch 19/64 loss: -0.7082810401916504
Batch 20/64 loss: -0.5753679275512695
Batch 21/64 loss: -0.7339086532592773
Batch 22/64 loss: -0.7465553283691406
Batch 23/64 loss: -0.8765106201171875
Batch 24/64 loss: -0.7162265777587891
Batch 25/64 loss: -0.7762236595153809
Batch 26/64 loss: -0.7804884910583496
Batch 27/64 loss: -0.5900077819824219
Batch 28/64 loss: -0.6380424499511719
Batch 29/64 loss: 0.13187265396118164
Batch 30/64 loss: -0.611081600189209
Batch 31/64 loss: -0.6417155265808105
Batch 32/64 loss: -0.7837023735046387
Batch 33/64 loss: -0.7470316886901855
Batch 34/64 loss: -0.1603684425354004
Batch 35/64 loss: -0.8503823280334473
Batch 36/64 loss: -0.6943302154541016
Batch 37/64 loss: -0.7454352378845215
Batch 38/64 loss: -0.6448869705200195
Batch 39/64 loss: -0.8324851989746094
Batch 40/64 loss: -0.7280755043029785
Batch 41/64 loss: -0.7859811782836914
Batch 42/64 loss: -0.2055344581604004
Batch 43/64 loss: -0.8291764259338379
Batch 44/64 loss: -0.5050582885742188
Batch 45/64 loss: -0.647737979888916
Batch 46/64 loss: -0.715278148651123
Batch 47/64 loss: -0.7151393890380859
Batch 48/64 loss: -0.8679103851318359
Batch 49/64 loss: 0.9608249664306641
Batch 50/64 loss: -0.8799476623535156
Batch 51/64 loss: -0.8178534507751465
Batch 52/64 loss: -0.7534255981445312
Batch 53/64 loss: -0.4001321792602539
Batch 54/64 loss: -0.8192620277404785
Batch 55/64 loss: -0.4387798309326172
Batch 56/64 loss: -0.692817211151123
Batch 57/64 loss: -0.7457551956176758
Batch 58/64 loss: -0.6664519309997559
Batch 59/64 loss: -0.6154966354370117
Batch 60/64 loss: -0.6532139778137207
Batch 61/64 loss: -0.6981744766235352
Batch 62/64 loss: -0.7049078941345215
Batch 63/64 loss: -0.4042205810546875
Batch 64/64 loss: -3.9727935791015625
Epoch 417  Train loss: -0.619757454068053  Val loss: -1.01780395245634
Epoch 418
-------------------------------
Batch 1/64 loss: 0.1864156723022461
Batch 2/64 loss: -0.3314647674560547
Batch 3/64 loss: -0.6907773017883301
Batch 4/64 loss: -0.7409391403198242
Batch 5/64 loss: -0.7648172378540039
Batch 6/64 loss: -0.5391311645507812
Batch 7/64 loss: -0.7121191024780273
Batch 8/64 loss: -0.7810726165771484
Batch 9/64 loss: -0.7587752342224121
Batch 10/64 loss: -0.7178511619567871
Batch 11/64 loss: -0.6775155067443848
Batch 12/64 loss: -0.7632021903991699
Batch 13/64 loss: -0.6306829452514648
Batch 14/64 loss: -0.6356563568115234
Batch 15/64 loss: -0.7995476722717285
Batch 16/64 loss: -0.6802377700805664
Batch 17/64 loss: -0.652320384979248
Batch 18/64 loss: -0.5640501976013184
Batch 19/64 loss: -0.7391753196716309
Batch 20/64 loss: -0.7790317535400391
Batch 21/64 loss: -0.7624993324279785
Batch 22/64 loss: -0.5872507095336914
Batch 23/64 loss: -0.5018234252929688
Batch 24/64 loss: -0.6083879470825195
Batch 25/64 loss: -0.7082886695861816
Batch 26/64 loss: -0.6840677261352539
Batch 27/64 loss: -0.434906005859375
Batch 28/64 loss: -0.6024880409240723
Batch 29/64 loss: -0.5956587791442871
Batch 30/64 loss: -0.7291483879089355
Batch 31/64 loss: -0.8156909942626953
Batch 32/64 loss: -0.6782140731811523
Batch 33/64 loss: 0.7691783905029297
Batch 34/64 loss: -0.5377826690673828
Batch 35/64 loss: 0.5059394836425781
Batch 36/64 loss: -0.8172240257263184
Batch 37/64 loss: -0.6105351448059082
Batch 38/64 loss: -0.7126979827880859
Batch 39/64 loss: -0.7242107391357422
Batch 40/64 loss: -0.49496030807495117
Batch 41/64 loss: -0.7638635635375977
Batch 42/64 loss: -0.5254640579223633
Batch 43/64 loss: -0.08518075942993164
Batch 44/64 loss: -0.6234531402587891
Batch 45/64 loss: -0.6413192749023438
Batch 46/64 loss: 0.0903778076171875
Batch 47/64 loss: -0.6973538398742676
Batch 48/64 loss: -0.6518650054931641
Batch 49/64 loss: -0.656768798828125
Batch 50/64 loss: -0.7220921516418457
Batch 51/64 loss: -0.8300008773803711
Batch 52/64 loss: -0.6881179809570312
Batch 53/64 loss: -0.8815999031066895
Batch 54/64 loss: -0.42513370513916016
Batch 55/64 loss: -0.6423358917236328
Batch 56/64 loss: -0.6761417388916016
Batch 57/64 loss: -0.5712432861328125
Batch 58/64 loss: -0.7090029716491699
Batch 59/64 loss: -0.7128496170043945
Batch 60/64 loss: -0.6401715278625488
Batch 61/64 loss: -0.7415027618408203
Batch 62/64 loss: -0.5696334838867188
Batch 63/64 loss: -0.5880708694458008
Batch 64/64 loss: -3.817624092102051
Epoch 418  Train loss: -0.6261752895280427  Val loss: -1.005364526178419
Epoch 419
-------------------------------
Batch 1/64 loss: -0.42849016189575195
Batch 2/64 loss: -0.6602048873901367
Batch 3/64 loss: -0.7480182647705078
Batch 4/64 loss: -0.5819993019104004
Batch 5/64 loss: -0.8300065994262695
Batch 6/64 loss: 0.2675471305847168
Batch 7/64 loss: -0.7996621131896973
Batch 8/64 loss: -0.6860232353210449
Batch 9/64 loss: -0.72564697265625
Batch 10/64 loss: -0.6513471603393555
Batch 11/64 loss: -0.6153998374938965
Batch 12/64 loss: -0.40773487091064453
Batch 13/64 loss: -0.7648992538452148
Batch 14/64 loss: 0.4573078155517578
Batch 15/64 loss: -0.7765345573425293
Batch 16/64 loss: -0.8610081672668457
Batch 17/64 loss: -0.8117899894714355
Batch 18/64 loss: -0.7760396003723145
Batch 19/64 loss: -0.8893189430236816
Batch 20/64 loss: -0.5665140151977539
Batch 21/64 loss: -0.706153392791748
Batch 22/64 loss: -0.7568960189819336
Batch 23/64 loss: -0.9196577072143555
Batch 24/64 loss: -0.7879061698913574
Batch 25/64 loss: -0.6837763786315918
Batch 26/64 loss: -0.6704845428466797
Batch 27/64 loss: -0.6228604316711426
Batch 28/64 loss: -0.7491917610168457
Batch 29/64 loss: -0.7200264930725098
Batch 30/64 loss: -0.17441368103027344
Batch 31/64 loss: -0.8281655311584473
Batch 32/64 loss: -0.6836333274841309
Batch 33/64 loss: -0.7110753059387207
Batch 34/64 loss: -0.7087669372558594
Batch 35/64 loss: -0.7858753204345703
Batch 36/64 loss: -0.817418098449707
Batch 37/64 loss: -0.8001971244812012
Batch 38/64 loss: -0.6425762176513672
Batch 39/64 loss: -0.6052069664001465
Batch 40/64 loss: -0.7706122398376465
Batch 41/64 loss: -0.7754831314086914
Batch 42/64 loss: -0.3972463607788086
Batch 43/64 loss: -0.7583723068237305
Batch 44/64 loss: -0.6505131721496582
Batch 45/64 loss: -0.7208304405212402
Batch 46/64 loss: -0.6581048965454102
Batch 47/64 loss: -0.8447537422180176
Batch 48/64 loss: -0.6658635139465332
Batch 49/64 loss: -0.673370361328125
Batch 50/64 loss: -0.7970318794250488
Batch 51/64 loss: -0.4267559051513672
Batch 52/64 loss: -0.7639865875244141
Batch 53/64 loss: 0.898890495300293
Batch 54/64 loss: 0.06993913650512695
Batch 55/64 loss: -0.6866450309753418
Batch 56/64 loss: -0.6793351173400879
Batch 57/64 loss: -0.8857526779174805
Batch 58/64 loss: -0.7452487945556641
Batch 59/64 loss: -0.8919463157653809
Batch 60/64 loss: -0.6588821411132812
Batch 61/64 loss: -0.33563947677612305
Batch 62/64 loss: -0.5314412117004395
Batch 63/64 loss: -0.7367916107177734
Batch 64/64 loss: -4.324893474578857
Epoch 419  Train loss: -0.6676001847959032  Val loss: -0.9609515920947098
Epoch 420
-------------------------------
Batch 1/64 loss: -0.8143110275268555
Batch 2/64 loss: 0.6370491981506348
Batch 3/64 loss: -0.7509498596191406
Batch 4/64 loss: -0.8041586875915527
Batch 5/64 loss: -0.5520997047424316
Batch 6/64 loss: -0.719200611114502
Batch 7/64 loss: -0.598940372467041
Batch 8/64 loss: -0.18800115585327148
Batch 9/64 loss: -0.7219486236572266
Batch 10/64 loss: -0.4389629364013672
Batch 11/64 loss: -0.09315919876098633
Batch 12/64 loss: -0.7269473075866699
Batch 13/64 loss: -0.5836915969848633
Batch 14/64 loss: -0.5937161445617676
Batch 15/64 loss: -0.4432344436645508
Batch 16/64 loss: -0.6613168716430664
Batch 17/64 loss: -0.8038077354431152
Batch 18/64 loss: -0.5659017562866211
Batch 19/64 loss: -0.6300387382507324
Batch 20/64 loss: -0.7915472984313965
Batch 21/64 loss: -0.546389102935791
Batch 22/64 loss: -0.7238588333129883
Batch 23/64 loss: 0.5320405960083008
Batch 24/64 loss: -0.6079654693603516
Batch 25/64 loss: -0.6741361618041992
Batch 26/64 loss: -0.6932964324951172
Batch 27/64 loss: -0.5513615608215332
Batch 28/64 loss: -0.7637662887573242
Batch 29/64 loss: -0.6912789344787598
Batch 30/64 loss: -0.5431461334228516
Batch 31/64 loss: -0.6720352172851562
Batch 32/64 loss: -0.10921001434326172
Batch 33/64 loss: -0.7003440856933594
Batch 34/64 loss: -0.7873811721801758
Batch 35/64 loss: -0.6327266693115234
Batch 36/64 loss: -0.6700935363769531
Batch 37/64 loss: 0.9038114547729492
Batch 38/64 loss: -0.4870882034301758
Batch 39/64 loss: -0.6789302825927734
Batch 40/64 loss: -0.8422842025756836
Batch 41/64 loss: -0.813873291015625
Batch 42/64 loss: -0.7003483772277832
Batch 43/64 loss: -0.6089158058166504
Batch 44/64 loss: -0.7748756408691406
Batch 45/64 loss: -0.7578282356262207
Batch 46/64 loss: -0.30971336364746094
Batch 47/64 loss: -0.8611083030700684
Batch 48/64 loss: -0.6641125679016113
Batch 49/64 loss: -0.8338885307312012
Batch 50/64 loss: -0.6746530532836914
Batch 51/64 loss: -0.6617517471313477
Batch 52/64 loss: -0.7892088890075684
Batch 53/64 loss: -0.6300954818725586
Batch 54/64 loss: -0.7773633003234863
Batch 55/64 loss: -0.7399783134460449
Batch 56/64 loss: -0.684659481048584
Batch 57/64 loss: -0.7429156303405762
Batch 58/64 loss: -0.649714469909668
Batch 59/64 loss: -0.6915702819824219
Batch 60/64 loss: -0.7720494270324707
Batch 61/64 loss: -0.7960891723632812
Batch 62/64 loss: -0.5086531639099121
Batch 63/64 loss: -0.6800441741943359
Batch 64/64 loss: -4.505005359649658
Epoch 420  Train loss: -0.631944938734466  Val loss: -1.0426882321072608
Epoch 421
-------------------------------
Batch 1/64 loss: -0.7544713020324707
Batch 2/64 loss: -0.7822489738464355
Batch 3/64 loss: 0.29746341705322266
Batch 4/64 loss: -0.5195956230163574
Batch 5/64 loss: -0.7060861587524414
Batch 6/64 loss: -0.9266839027404785
Batch 7/64 loss: -0.7699985504150391
Batch 8/64 loss: -0.7347803115844727
Batch 9/64 loss: -0.8197875022888184
Batch 10/64 loss: -0.6613388061523438
Batch 11/64 loss: -0.7245688438415527
Batch 12/64 loss: -0.7040953636169434
Batch 13/64 loss: -0.38999176025390625
Batch 14/64 loss: -0.3643627166748047
Batch 15/64 loss: -0.727508544921875
Batch 16/64 loss: -0.7669310569763184
Batch 17/64 loss: -0.7557215690612793
Batch 18/64 loss: -0.8701434135437012
Batch 19/64 loss: -0.8672547340393066
Batch 20/64 loss: -0.39563846588134766
Batch 21/64 loss: -0.7791895866394043
Batch 22/64 loss: -0.6577882766723633
Batch 23/64 loss: -0.4890866279602051
Batch 24/64 loss: -0.8912053108215332
Batch 25/64 loss: -0.8868374824523926
Batch 26/64 loss: -0.7337980270385742
Batch 27/64 loss: -0.8418488502502441
Batch 28/64 loss: -0.639765739440918
Batch 29/64 loss: -0.6703739166259766
Batch 30/64 loss: -0.2686038017272949
Batch 31/64 loss: 0.4092693328857422
Batch 32/64 loss: -0.8422703742980957
Batch 33/64 loss: -0.7518057823181152
Batch 34/64 loss: -0.7220921516418457
Batch 35/64 loss: -0.7294354438781738
Batch 36/64 loss: -0.8979063034057617
Batch 37/64 loss: -0.7786068916320801
Batch 38/64 loss: -0.7199835777282715
Batch 39/64 loss: -0.8073487281799316
Batch 40/64 loss: 0.6367812156677246
Batch 41/64 loss: -0.5340805053710938
Batch 42/64 loss: -0.4524683952331543
Batch 43/64 loss: -0.6396913528442383
Batch 44/64 loss: -0.5898351669311523
Batch 45/64 loss: -0.8021683692932129
Batch 46/64 loss: -0.8136920928955078
Batch 47/64 loss: -0.8429880142211914
Batch 48/64 loss: -0.5951781272888184
Batch 49/64 loss: -0.7392749786376953
Batch 50/64 loss: -0.758732795715332
Batch 51/64 loss: -0.7711849212646484
Batch 52/64 loss: -0.5911650657653809
Batch 53/64 loss: -0.8326330184936523
Batch 54/64 loss: -0.4824857711791992
Batch 55/64 loss: -0.7107925415039062
Batch 56/64 loss: -0.7282767295837402
Batch 57/64 loss: -0.804985523223877
Batch 58/64 loss: -0.2839984893798828
Batch 59/64 loss: -0.6093916893005371
Batch 60/64 loss: -0.8388690948486328
Batch 61/64 loss: -0.7860159873962402
Batch 62/64 loss: -0.6143989562988281
Batch 63/64 loss: -0.6853623390197754
Batch 64/64 loss: -4.17429780960083
Epoch 421  Train loss: -0.6846122947393679  Val loss: -0.9312416155313709
Epoch 422
-------------------------------
Batch 1/64 loss: -0.8179445266723633
Batch 2/64 loss: -0.8488717079162598
Batch 3/64 loss: -0.6070685386657715
Batch 4/64 loss: -0.2773709297180176
Batch 5/64 loss: -0.7067728042602539
Batch 6/64 loss: -0.7190799713134766
Batch 7/64 loss: -0.5853466987609863
Batch 8/64 loss: -0.7315969467163086
Batch 9/64 loss: 0.8171801567077637
Batch 10/64 loss: -0.6619720458984375
Batch 11/64 loss: -0.6520743370056152
Batch 12/64 loss: -0.730738639831543
Batch 13/64 loss: -0.4776420593261719
Batch 14/64 loss: -0.45797109603881836
Batch 15/64 loss: -0.48381567001342773
Batch 16/64 loss: 0.9397239685058594
Batch 17/64 loss: -0.7440934181213379
Batch 18/64 loss: -0.09492921829223633
Batch 19/64 loss: -0.3529696464538574
Batch 20/64 loss: -0.22344541549682617
Batch 21/64 loss: -0.517845630645752
Batch 22/64 loss: -0.5058832168579102
Batch 23/64 loss: -0.45545434951782227
Batch 24/64 loss: -0.5030627250671387
Batch 25/64 loss: -0.20320844650268555
Batch 26/64 loss: -0.3478388786315918
Batch 27/64 loss: 0.08394241333007812
Batch 28/64 loss: -0.2732219696044922
Batch 29/64 loss: -0.40984296798706055
Batch 30/64 loss: -0.4214296340942383
Batch 31/64 loss: -0.5308947563171387
Batch 32/64 loss: -0.4332551956176758
Batch 33/64 loss: -0.6193480491638184
Batch 34/64 loss: -0.4483222961425781
Batch 35/64 loss: -0.5048112869262695
Batch 36/64 loss: -0.3918471336364746
Batch 37/64 loss: -0.30869436264038086
Batch 38/64 loss: -0.5561375617980957
Batch 39/64 loss: -0.4183969497680664
Batch 40/64 loss: -0.3192324638366699
Batch 41/64 loss: 0.4424715042114258
Batch 42/64 loss: -0.526360034942627
Batch 43/64 loss: -0.289003849029541
Batch 44/64 loss: -0.4887981414794922
Batch 45/64 loss: -0.11742639541625977
Batch 46/64 loss: -0.5360960960388184
Batch 47/64 loss: -0.050762176513671875
Batch 48/64 loss: -0.1621837615966797
Batch 49/64 loss: -0.3595004081726074
Batch 50/64 loss: -0.6782517433166504
Batch 51/64 loss: -0.5813941955566406
Batch 52/64 loss: 0.06808710098266602
Batch 53/64 loss: -0.4254770278930664
Batch 54/64 loss: -0.21811532974243164
Batch 55/64 loss: -0.4589390754699707
Batch 56/64 loss: -0.4390420913696289
Batch 57/64 loss: -0.5227746963500977
Batch 58/64 loss: 0.16140174865722656
Batch 59/64 loss: -0.5491223335266113
Batch 60/64 loss: -0.7327718734741211
Batch 61/64 loss: -0.39997196197509766
Batch 62/64 loss: 0.7203993797302246
Batch 63/64 loss: -0.5462260246276855
Batch 64/64 loss: -4.138727188110352
Epoch 422  Train loss: -0.4124782412659888  Val loss: -0.6823985175168801
Epoch 423
-------------------------------
Batch 1/64 loss: -0.5436549186706543
Batch 2/64 loss: -0.46964263916015625
Batch 3/64 loss: -0.6393189430236816
Batch 4/64 loss: -0.6844005584716797
Batch 5/64 loss: -0.6573886871337891
Batch 6/64 loss: -0.38635730743408203
Batch 7/64 loss: -0.5816407203674316
Batch 8/64 loss: -0.5564565658569336
Batch 9/64 loss: -0.48365306854248047
Batch 10/64 loss: -0.6531157493591309
Batch 11/64 loss: -0.21039152145385742
Batch 12/64 loss: -0.4604072570800781
Batch 13/64 loss: -0.6639127731323242
Batch 14/64 loss: -0.0128021240234375
Batch 15/64 loss: -0.5306515693664551
Batch 16/64 loss: -0.5516538619995117
Batch 17/64 loss: -0.3800539970397949
Batch 18/64 loss: -0.824800968170166
Batch 19/64 loss: -0.6277899742126465
Batch 20/64 loss: -0.645045280456543
Batch 21/64 loss: -0.484926700592041
Batch 22/64 loss: -0.7053823471069336
Batch 23/64 loss: -0.6126828193664551
Batch 24/64 loss: -0.7593650817871094
Batch 25/64 loss: -0.7011117935180664
Batch 26/64 loss: -0.49851322174072266
Batch 27/64 loss: -0.6500577926635742
Batch 28/64 loss: -0.6033024787902832
Batch 29/64 loss: -0.7531027793884277
Batch 30/64 loss: -0.5308880805969238
Batch 31/64 loss: -0.7271838188171387
Batch 32/64 loss: -0.8513798713684082
Batch 33/64 loss: -0.3026313781738281
Batch 34/64 loss: -0.4845700263977051
Batch 35/64 loss: -0.4277033805847168
Batch 36/64 loss: -0.4860420227050781
Batch 37/64 loss: -0.7639350891113281
Batch 38/64 loss: -0.7694244384765625
Batch 39/64 loss: -0.5292258262634277
Batch 40/64 loss: 0.012640953063964844
Batch 41/64 loss: -0.18921518325805664
Batch 42/64 loss: -0.4575810432434082
Batch 43/64 loss: -0.6196107864379883
Batch 44/64 loss: -0.5844249725341797
Batch 45/64 loss: -0.5347990989685059
Batch 46/64 loss: -0.2686171531677246
Batch 47/64 loss: -0.7315258979797363
Batch 48/64 loss: -0.5865139961242676
Batch 49/64 loss: -0.18981313705444336
Batch 50/64 loss: -0.6533041000366211
Batch 51/64 loss: -0.7192444801330566
Batch 52/64 loss: -0.6754570007324219
Batch 53/64 loss: -0.6606884002685547
Batch 54/64 loss: 0.32586002349853516
Batch 55/64 loss: -0.6948094367980957
Batch 56/64 loss: -0.6969590187072754
Batch 57/64 loss: -0.7638816833496094
Batch 58/64 loss: 2.2478461265563965
Batch 59/64 loss: -0.785700798034668
Batch 60/64 loss: -0.6215610504150391
Batch 61/64 loss: -0.8138599395751953
Batch 62/64 loss: -0.6672301292419434
Batch 63/64 loss: -0.5524168014526367
Batch 64/64 loss: -4.368358612060547
Epoch 423  Train loss: -0.5546933866014667  Val loss: -0.7337933832017827
Epoch 424
-------------------------------
Batch 1/64 loss: 0.5768780708312988
Batch 2/64 loss: -0.742218017578125
Batch 3/64 loss: -0.4103703498840332
Batch 4/64 loss: -0.20812034606933594
Batch 5/64 loss: -0.6726150512695312
Batch 6/64 loss: -0.12704181671142578
Batch 7/64 loss: -0.5355849266052246
Batch 8/64 loss: -0.774808406829834
Batch 9/64 loss: -0.5665583610534668
Batch 10/64 loss: -0.6086316108703613
Batch 11/64 loss: -0.46033811569213867
Batch 12/64 loss: -0.6594915390014648
Batch 13/64 loss: -0.6146836280822754
Batch 14/64 loss: -0.736443042755127
Batch 15/64 loss: -0.7353816032409668
Batch 16/64 loss: -0.48665380477905273
Batch 17/64 loss: -0.5040340423583984
Batch 18/64 loss: -0.2844696044921875
Batch 19/64 loss: -0.5495572090148926
Batch 20/64 loss: -0.5035362243652344
Batch 21/64 loss: -0.49331092834472656
Batch 22/64 loss: -0.6402459144592285
Batch 23/64 loss: -0.5875186920166016
Batch 24/64 loss: -0.40447092056274414
Batch 25/64 loss: -0.6740994453430176
Batch 26/64 loss: -0.6943354606628418
Batch 27/64 loss: -0.6178393363952637
Batch 28/64 loss: -0.5778217315673828
Batch 29/64 loss: -0.7585544586181641
Batch 30/64 loss: -0.6439595222473145
Batch 31/64 loss: -0.818695068359375
Batch 32/64 loss: -0.7418127059936523
Batch 33/64 loss: -0.5459122657775879
Batch 34/64 loss: -0.6000032424926758
Batch 35/64 loss: -0.7507538795471191
Batch 36/64 loss: -0.8062467575073242
Batch 37/64 loss: -0.7710895538330078
Batch 38/64 loss: -0.17027997970581055
Batch 39/64 loss: -0.6594452857971191
Batch 40/64 loss: -0.7179470062255859
Batch 41/64 loss: -0.864680290222168
Batch 42/64 loss: 0.09883308410644531
Batch 43/64 loss: -0.7240324020385742
Batch 44/64 loss: -0.5790324211120605
Batch 45/64 loss: -0.4471397399902344
Batch 46/64 loss: -0.765082836151123
Batch 47/64 loss: -0.3543243408203125
Batch 48/64 loss: -0.5420827865600586
Batch 49/64 loss: 0.7637953758239746
Batch 50/64 loss: -0.6573143005371094
Batch 51/64 loss: -0.6704893112182617
Batch 52/64 loss: -0.5723042488098145
Batch 53/64 loss: -0.6591835021972656
Batch 54/64 loss: -0.3823556900024414
Batch 55/64 loss: -0.7364063262939453
Batch 56/64 loss: -0.596550464630127
Batch 57/64 loss: 0.27416086196899414
Batch 58/64 loss: -0.5286130905151367
Batch 59/64 loss: -0.6943173408508301
Batch 60/64 loss: -0.5296630859375
Batch 61/64 loss: -0.7437763214111328
Batch 62/64 loss: -0.6919412612915039
Batch 63/64 loss: -0.8293037414550781
Batch 64/64 loss: -4.281499862670898
Epoch 424  Train loss: -0.5791518566655177  Val loss: -0.9531997864189017
Epoch 425
-------------------------------
Batch 1/64 loss: -0.7928004264831543
Batch 2/64 loss: -0.6935262680053711
Batch 3/64 loss: -0.6247591972351074
Batch 4/64 loss: -0.4320197105407715
Batch 5/64 loss: -0.598325252532959
Batch 6/64 loss: -0.5703883171081543
Batch 7/64 loss: -0.5808424949645996
Batch 8/64 loss: -0.718268871307373
Batch 9/64 loss: -0.7925496101379395
Batch 10/64 loss: -0.7372674942016602
Batch 11/64 loss: -0.6633305549621582
Batch 12/64 loss: -0.6338496208190918
Batch 13/64 loss: -0.6286258697509766
Batch 14/64 loss: -0.7928671836853027
Batch 15/64 loss: -0.7156052589416504
Batch 16/64 loss: -0.5879216194152832
Batch 17/64 loss: -0.8386092185974121
Batch 18/64 loss: -0.6712298393249512
Batch 19/64 loss: -0.722475528717041
Batch 20/64 loss: -0.6508874893188477
Batch 21/64 loss: -0.6412758827209473
Batch 22/64 loss: -0.3711204528808594
Batch 23/64 loss: -0.6876897811889648
Batch 24/64 loss: -0.9159064292907715
Batch 25/64 loss: -0.6270260810852051
Batch 26/64 loss: -0.6871027946472168
Batch 27/64 loss: -0.5285000801086426
Batch 28/64 loss: -0.11028051376342773
Batch 29/64 loss: -0.7844724655151367
Batch 30/64 loss: -0.19999217987060547
Batch 31/64 loss: -0.8714327812194824
Batch 32/64 loss: 0.36805295944213867
Batch 33/64 loss: 0.31695985794067383
Batch 34/64 loss: -0.6499242782592773
Batch 35/64 loss: -0.17037725448608398
Batch 36/64 loss: -0.5403218269348145
Batch 37/64 loss: -0.6659889221191406
Batch 38/64 loss: 0.7151150703430176
Batch 39/64 loss: -0.6230101585388184
Batch 40/64 loss: -0.7352213859558105
Batch 41/64 loss: -0.5454182624816895
Batch 42/64 loss: -0.6892485618591309
Batch 43/64 loss: -0.43991947174072266
Batch 44/64 loss: -0.7657051086425781
Batch 45/64 loss: -0.4047846794128418
Batch 46/64 loss: -0.7133479118347168
Batch 47/64 loss: -0.8283166885375977
Batch 48/64 loss: -0.3869743347167969
Batch 49/64 loss: -0.1521625518798828
Batch 50/64 loss: -0.7032489776611328
Batch 51/64 loss: -0.5824933052062988
Batch 52/64 loss: -0.4553041458129883
Batch 53/64 loss: -0.6647396087646484
Batch 54/64 loss: -0.3925971984863281
Batch 55/64 loss: -0.8761935234069824
Batch 56/64 loss: -0.5773406028747559
Batch 57/64 loss: -0.5826201438903809
Batch 58/64 loss: -0.7163043022155762
Batch 59/64 loss: -0.8364286422729492
Batch 60/64 loss: -0.057691097259521484
Batch 61/64 loss: -0.7673020362854004
Batch 62/64 loss: -0.6921544075012207
Batch 63/64 loss: -0.6923360824584961
Batch 64/64 loss: -4.265343189239502
Epoch 425  Train loss: -0.6051341842202579  Val loss: -1.0230238740796487
Epoch 426
-------------------------------
Batch 1/64 loss: -0.7151775360107422
Batch 2/64 loss: -0.7424278259277344
Batch 3/64 loss: -0.4446439743041992
Batch 4/64 loss: -0.6720266342163086
Batch 5/64 loss: -0.6171183586120605
Batch 6/64 loss: -0.7485547065734863
Batch 7/64 loss: -0.6608762741088867
Batch 8/64 loss: -0.4254188537597656
Batch 9/64 loss: -0.8127140998840332
Batch 10/64 loss: -0.7019162178039551
Batch 11/64 loss: -0.7470688819885254
Batch 12/64 loss: -0.7662754058837891
Batch 13/64 loss: -0.757695198059082
Batch 14/64 loss: -0.4966707229614258
Batch 15/64 loss: -0.5933876037597656
Batch 16/64 loss: -0.12462806701660156
Batch 17/64 loss: -0.8150725364685059
Batch 18/64 loss: -0.9195513725280762
Batch 19/64 loss: -0.40084171295166016
Batch 20/64 loss: -0.4470853805541992
Batch 21/64 loss: -0.6001758575439453
Batch 22/64 loss: -0.8325138092041016
Batch 23/64 loss: -0.7149562835693359
Batch 24/64 loss: 0.3251686096191406
Batch 25/64 loss: -0.5789380073547363
Batch 26/64 loss: -0.5867223739624023
Batch 27/64 loss: -0.8122053146362305
Batch 28/64 loss: -0.7519607543945312
Batch 29/64 loss: -0.7786078453063965
Batch 30/64 loss: -0.47368717193603516
Batch 31/64 loss: -0.512723445892334
Batch 32/64 loss: -0.8058881759643555
Batch 33/64 loss: -0.8326539993286133
Batch 34/64 loss: -0.7559804916381836
Batch 35/64 loss: -0.7692461013793945
Batch 36/64 loss: 0.39728212356567383
Batch 37/64 loss: -0.2355356216430664
Batch 38/64 loss: -0.7624502182006836
Batch 39/64 loss: 0.430023193359375
Batch 40/64 loss: -0.6310892105102539
Batch 41/64 loss: 0.8573198318481445
Batch 42/64 loss: -0.6372466087341309
Batch 43/64 loss: -0.65875244140625
Batch 44/64 loss: -0.6344075202941895
Batch 45/64 loss: -0.19924545288085938
Batch 46/64 loss: -0.48138952255249023
Batch 47/64 loss: -0.608466625213623
Batch 48/64 loss: -0.4629817008972168
Batch 49/64 loss: -0.8134951591491699
Batch 50/64 loss: -0.798372745513916
Batch 51/64 loss: -0.7499938011169434
Batch 52/64 loss: -0.742100715637207
Batch 53/64 loss: -0.7867026329040527
Batch 54/64 loss: -0.694878101348877
Batch 55/64 loss: -0.6219806671142578
Batch 56/64 loss: -0.5895218849182129
Batch 57/64 loss: -0.559962272644043
Batch 58/64 loss: -0.8530120849609375
Batch 59/64 loss: -0.8214931488037109
Batch 60/64 loss: -0.7055244445800781
Batch 61/64 loss: -0.5543746948242188
Batch 62/64 loss: -0.7120046615600586
Batch 63/64 loss: -0.4427967071533203
Batch 64/64 loss: -4.476396083831787
Epoch 426  Train loss: -0.6199324383455165  Val loss: -0.8900273378772015
Epoch 427
-------------------------------
Batch 1/64 loss: -0.5382256507873535
Batch 2/64 loss: -0.6272802352905273
Batch 3/64 loss: 0.5360431671142578
Batch 4/64 loss: -0.4417753219604492
Batch 5/64 loss: -0.5813589096069336
Batch 6/64 loss: -0.6176772117614746
Batch 7/64 loss: -0.7168517112731934
Batch 8/64 loss: -0.5308184623718262
Batch 9/64 loss: -0.45905160903930664
Batch 10/64 loss: -0.5329709053039551
Batch 11/64 loss: -0.308103084564209
Batch 12/64 loss: -0.7610249519348145
Batch 13/64 loss: -0.2892136573791504
Batch 14/64 loss: -0.7591571807861328
Batch 15/64 loss: -0.5624790191650391
Batch 16/64 loss: -0.7273344993591309
Batch 17/64 loss: -0.6155319213867188
Batch 18/64 loss: -0.5816478729248047
Batch 19/64 loss: -0.6117081642150879
Batch 20/64 loss: 0.2640824317932129
Batch 21/64 loss: -0.7938008308410645
Batch 22/64 loss: -0.4552931785583496
Batch 23/64 loss: 0.4661850929260254
Batch 24/64 loss: -0.33020734786987305
Batch 25/64 loss: 1.522740364074707
Batch 26/64 loss: -0.7720799446105957
Batch 27/64 loss: -0.5243330001831055
Batch 28/64 loss: -0.44517040252685547
Batch 29/64 loss: -0.684699535369873
Batch 30/64 loss: -0.3502793312072754
Batch 31/64 loss: -0.6199479103088379
Batch 32/64 loss: -0.6057801246643066
Batch 33/64 loss: -0.6184206008911133
Batch 34/64 loss: -0.5540995597839355
Batch 35/64 loss: -0.6795587539672852
Batch 36/64 loss: -0.6380853652954102
Batch 37/64 loss: -0.6434793472290039
Batch 38/64 loss: 0.5546479225158691
Batch 39/64 loss: -0.6774444580078125
Batch 40/64 loss: 0.2754502296447754
Batch 41/64 loss: -0.5746426582336426
Batch 42/64 loss: -0.7832503318786621
Batch 43/64 loss: -0.6876072883605957
Batch 44/64 loss: -0.6942038536071777
Batch 45/64 loss: -0.4658317565917969
Batch 46/64 loss: -0.5537528991699219
Batch 47/64 loss: 0.5854964256286621
Batch 48/64 loss: -0.1824808120727539
Batch 49/64 loss: -0.7458200454711914
Batch 50/64 loss: 0.17886829376220703
Batch 51/64 loss: -0.7380595207214355
Batch 52/64 loss: -0.7615876197814941
Batch 53/64 loss: -0.6843886375427246
Batch 54/64 loss: -0.7137856483459473
Batch 55/64 loss: -0.9012880325317383
Batch 56/64 loss: -0.6943836212158203
Batch 57/64 loss: -0.47402524948120117
Batch 58/64 loss: -0.6844754219055176
Batch 59/64 loss: -0.6579675674438477
Batch 60/64 loss: -0.4647221565246582
Batch 61/64 loss: -0.6525874137878418
Batch 62/64 loss: -0.7665867805480957
Batch 63/64 loss: -0.35378551483154297
Batch 64/64 loss: -4.321173191070557
Epoch 427  Train loss: -0.49799982519710767  Val loss: -0.9250414346911243
Epoch 428
-------------------------------
Batch 1/64 loss: -0.6552724838256836
Batch 2/64 loss: -0.6162586212158203
Batch 3/64 loss: -0.7853808403015137
Batch 4/64 loss: -0.8281512260437012
Batch 5/64 loss: -0.6103672981262207
Batch 6/64 loss: -0.47756385803222656
Batch 7/64 loss: -0.6213703155517578
Batch 8/64 loss: -0.5586919784545898
Batch 9/64 loss: -0.10265445709228516
Batch 10/64 loss: -0.5326132774353027
Batch 11/64 loss: 0.6713857650756836
Batch 12/64 loss: -0.5968494415283203
Batch 13/64 loss: -0.7910704612731934
Batch 14/64 loss: -0.771998405456543
Batch 15/64 loss: -0.7820930480957031
Batch 16/64 loss: -0.654207706451416
Batch 17/64 loss: -0.8831915855407715
Batch 18/64 loss: 1.22902250289917
Batch 19/64 loss: -0.6737885475158691
Batch 20/64 loss: -0.6412200927734375
Batch 21/64 loss: -0.6656327247619629
Batch 22/64 loss: -0.4773445129394531
Batch 23/64 loss: -0.5198917388916016
Batch 24/64 loss: -0.7745242118835449
Batch 25/64 loss: -0.4877147674560547
Batch 26/64 loss: -0.6298470497131348
Batch 27/64 loss: -0.644859790802002
Batch 28/64 loss: -0.7057900428771973
Batch 29/64 loss: -0.5578961372375488
Batch 30/64 loss: -0.48341798782348633
Batch 31/64 loss: -0.38958740234375
Batch 32/64 loss: 0.6820783615112305
Batch 33/64 loss: -0.4991416931152344
Batch 34/64 loss: -0.7294306755065918
Batch 35/64 loss: -0.44400548934936523
Batch 36/64 loss: -0.5155696868896484
Batch 37/64 loss: -0.5589113235473633
Batch 38/64 loss: -0.5365967750549316
Batch 39/64 loss: -0.2823958396911621
Batch 40/64 loss: -0.6549625396728516
Batch 41/64 loss: -0.6187267303466797
Batch 42/64 loss: -0.06890869140625
Batch 43/64 loss: -0.29163169860839844
Batch 44/64 loss: -0.35923099517822266
Batch 45/64 loss: -0.5270276069641113
Batch 46/64 loss: -0.3479347229003906
Batch 47/64 loss: -0.4554281234741211
Batch 48/64 loss: -0.5576653480529785
Batch 49/64 loss: -0.3676328659057617
Batch 50/64 loss: -0.5283389091491699
Batch 51/64 loss: -0.2497234344482422
Batch 52/64 loss: -0.6580548286437988
Batch 53/64 loss: -0.38501739501953125
Batch 54/64 loss: -0.6380391120910645
Batch 55/64 loss: -0.38710546493530273
Batch 56/64 loss: -0.485140323638916
Batch 57/64 loss: 0.020522117614746094
Batch 58/64 loss: -0.6268734931945801
Batch 59/64 loss: -0.5129294395446777
Batch 60/64 loss: -0.5614180564880371
Batch 61/64 loss: -0.10772275924682617
Batch 62/64 loss: -0.3753633499145508
Batch 63/64 loss: 0.543452262878418
Batch 64/64 loss: -4.223748207092285
Epoch 428  Train loss: -0.4905337651570638  Val loss: -0.8632897970193031
Epoch 429
-------------------------------
Batch 1/64 loss: -0.5031137466430664
Batch 2/64 loss: -0.4855475425720215
Batch 3/64 loss: -0.5564823150634766
Batch 4/64 loss: -0.5776772499084473
Batch 5/64 loss: -0.6872854232788086
Batch 6/64 loss: -0.6204380989074707
Batch 7/64 loss: -0.3041825294494629
Batch 8/64 loss: -0.2090625762939453
Batch 9/64 loss: -0.572685718536377
Batch 10/64 loss: -0.5759787559509277
Batch 11/64 loss: -0.3122110366821289
Batch 12/64 loss: -0.5886449813842773
Batch 13/64 loss: -0.7100157737731934
Batch 14/64 loss: -0.2534365653991699
Batch 15/64 loss: -0.2692580223083496
Batch 16/64 loss: -0.7070698738098145
Batch 17/64 loss: -0.5820755958557129
Batch 18/64 loss: -0.4650750160217285
Batch 19/64 loss: -0.4682598114013672
Batch 20/64 loss: -0.7320270538330078
Batch 21/64 loss: -0.5336050987243652
Batch 22/64 loss: -0.5295367240905762
Batch 23/64 loss: -0.3057436943054199
Batch 24/64 loss: -0.5628395080566406
Batch 25/64 loss: -0.565093994140625
Batch 26/64 loss: -0.3817915916442871
Batch 27/64 loss: -0.21941661834716797
Batch 28/64 loss: -0.5592913627624512
Batch 29/64 loss: -0.4286613464355469
Batch 30/64 loss: 0.04860687255859375
Batch 31/64 loss: -0.3202085494995117
Batch 32/64 loss: 0.6934256553649902
Batch 33/64 loss: -0.007922172546386719
Batch 34/64 loss: -0.7116661071777344
Batch 35/64 loss: -0.3822212219238281
Batch 36/64 loss: -0.4600830078125
Batch 37/64 loss: -0.4649810791015625
Batch 38/64 loss: -0.5678634643554688
Batch 39/64 loss: -0.3502683639526367
Batch 40/64 loss: -0.5669946670532227
Batch 41/64 loss: -0.5001115798950195
Batch 42/64 loss: -0.696373462677002
Batch 43/64 loss: -0.5834665298461914
Batch 44/64 loss: -0.6734066009521484
Batch 45/64 loss: -0.5354843139648438
Batch 46/64 loss: -0.5349597930908203
Batch 47/64 loss: -0.7064332962036133
Batch 48/64 loss: -0.6256389617919922
Batch 49/64 loss: -0.7225627899169922
Batch 50/64 loss: -0.6612296104431152
Batch 51/64 loss: -0.5052309036254883
Batch 52/64 loss: 0.3651309013366699
Batch 53/64 loss: -0.52178955078125
Batch 54/64 loss: -0.6299357414245605
Batch 55/64 loss: -0.3537564277648926
Batch 56/64 loss: -0.22623920440673828
Batch 57/64 loss: -0.6705856323242188
Batch 58/64 loss: 1.333465576171875
Batch 59/64 loss: -0.78778076171875
Batch 60/64 loss: -0.7277913093566895
Batch 61/64 loss: -0.6888418197631836
Batch 62/64 loss: -0.5214214324951172
Batch 63/64 loss: -0.5354423522949219
Batch 64/64 loss: -4.232893466949463
Epoch 429  Train loss: -0.4900586577022777  Val loss: -0.7982336352371269
Epoch 430
-------------------------------
Batch 1/64 loss: -0.6500349044799805
Batch 2/64 loss: -0.668917179107666
Batch 3/64 loss: -0.6109681129455566
Batch 4/64 loss: -0.49251651763916016
Batch 5/64 loss: -0.5920090675354004
Batch 6/64 loss: -0.8142194747924805
Batch 7/64 loss: -0.6812496185302734
Batch 8/64 loss: -0.7259430885314941
Batch 9/64 loss: -0.1889324188232422
Batch 10/64 loss: -0.4874138832092285
Batch 11/64 loss: -0.0026645660400390625
Batch 12/64 loss: -0.13608932495117188
Batch 13/64 loss: -0.5538749694824219
Batch 14/64 loss: -0.40653038024902344
Batch 15/64 loss: -0.4417548179626465
Batch 16/64 loss: -0.6661267280578613
Batch 17/64 loss: -0.704681396484375
Batch 18/64 loss: -0.574152946472168
Batch 19/64 loss: -0.07809162139892578
Batch 20/64 loss: -0.2920989990234375
Batch 21/64 loss: -0.46428489685058594
Batch 22/64 loss: -0.7192306518554688
Batch 23/64 loss: -0.7502408027648926
Batch 24/64 loss: -0.5500521659851074
Batch 25/64 loss: -0.06036996841430664
Batch 26/64 loss: -0.29656076431274414
Batch 27/64 loss: -0.721773624420166
Batch 28/64 loss: -0.13694000244140625
Batch 29/64 loss: -0.582974910736084
Batch 30/64 loss: -0.6453700065612793
Batch 31/64 loss: 0.4552750587463379
Batch 32/64 loss: -0.5831632614135742
Batch 33/64 loss: -0.546302318572998
Batch 34/64 loss: -0.33112096786499023
Batch 35/64 loss: -0.44009876251220703
Batch 36/64 loss: -0.509160041809082
Batch 37/64 loss: -0.5061759948730469
Batch 38/64 loss: 2.1544084548950195
Batch 39/64 loss: -0.6016254425048828
Batch 40/64 loss: -0.4483656883239746
Batch 41/64 loss: -0.3515033721923828
Batch 42/64 loss: -0.6297225952148438
Batch 43/64 loss: -0.6112017631530762
Batch 44/64 loss: -0.6503100395202637
Batch 45/64 loss: -0.6393589973449707
Batch 46/64 loss: -0.660240650177002
Batch 47/64 loss: -0.5982418060302734
Batch 48/64 loss: -0.5490455627441406
Batch 49/64 loss: -0.42147302627563477
Batch 50/64 loss: -0.6028327941894531
Batch 51/64 loss: -0.5873384475708008
Batch 52/64 loss: -0.3942289352416992
Batch 53/64 loss: -0.6597695350646973
Batch 54/64 loss: -0.6629161834716797
Batch 55/64 loss: 0.04431486129760742
Batch 56/64 loss: -0.6458516120910645
Batch 57/64 loss: -0.17946434020996094
Batch 58/64 loss: -0.6885251998901367
Batch 59/64 loss: -0.5819625854492188
Batch 60/64 loss: -0.7479653358459473
Batch 61/64 loss: -0.727898120880127
Batch 62/64 loss: -0.5373597145080566
Batch 63/64 loss: -0.5815844535827637
Batch 64/64 loss: -4.224617004394531
Epoch 430  Train loss: -0.5001621919519761  Val loss: -0.9661729556998027
Epoch 431
-------------------------------
Batch 1/64 loss: -0.3048877716064453
Batch 2/64 loss: -0.7470698356628418
Batch 3/64 loss: -0.6764883995056152
Batch 4/64 loss: -0.6226716041564941
Batch 5/64 loss: -0.4778003692626953
Batch 6/64 loss: 0.0552825927734375
Batch 7/64 loss: -0.5658473968505859
Batch 8/64 loss: -0.3610954284667969
Batch 9/64 loss: -0.6337094306945801
Batch 10/64 loss: -0.6272602081298828
Batch 11/64 loss: -0.6834545135498047
Batch 12/64 loss: -0.626673698425293
Batch 13/64 loss: -0.6530251502990723
Batch 14/64 loss: -0.7520503997802734
Batch 15/64 loss: -0.7062778472900391
Batch 16/64 loss: -0.5034341812133789
Batch 17/64 loss: -0.8082613945007324
Batch 18/64 loss: -0.7326478958129883
Batch 19/64 loss: -0.7338995933532715
Batch 20/64 loss: -0.7228994369506836
Batch 21/64 loss: -0.6840829849243164
Batch 22/64 loss: -0.6635513305664062
Batch 23/64 loss: -0.5421948432922363
Batch 24/64 loss: -0.8159604072570801
Batch 25/64 loss: -0.5786547660827637
Batch 26/64 loss: -0.7202663421630859
Batch 27/64 loss: -0.72613525390625
Batch 28/64 loss: -0.39819955825805664
Batch 29/64 loss: -0.5154261589050293
Batch 30/64 loss: -0.6144590377807617
Batch 31/64 loss: -0.773167610168457
Batch 32/64 loss: -0.6069092750549316
Batch 33/64 loss: -0.5793905258178711
Batch 34/64 loss: -0.6810483932495117
Batch 35/64 loss: -0.6968040466308594
Batch 36/64 loss: -0.25533390045166016
Batch 37/64 loss: -0.5890607833862305
Batch 38/64 loss: 0.8371438980102539
Batch 39/64 loss: -0.6832928657531738
Batch 40/64 loss: -0.6956315040588379
Batch 41/64 loss: -0.6413707733154297
Batch 42/64 loss: -0.6582398414611816
Batch 43/64 loss: 0.007652759552001953
Batch 44/64 loss: -0.7182612419128418
Batch 45/64 loss: -0.6577520370483398
Batch 46/64 loss: -0.517859935760498
Batch 47/64 loss: -0.6327166557312012
Batch 48/64 loss: 0.47214841842651367
Batch 49/64 loss: -0.39450740814208984
Batch 50/64 loss: -0.5927057266235352
Batch 51/64 loss: 0.04481935501098633
Batch 52/64 loss: 0.34611940383911133
Batch 53/64 loss: -0.5311546325683594
Batch 54/64 loss: -0.6372308731079102
Batch 55/64 loss: -0.287045955657959
Batch 56/64 loss: -0.7029733657836914
Batch 57/64 loss: -0.42355775833129883
Batch 58/64 loss: -0.6098394393920898
Batch 59/64 loss: -0.6049652099609375
Batch 60/64 loss: -0.5730838775634766
Batch 61/64 loss: -0.6340994834899902
Batch 62/64 loss: -0.6157422065734863
Batch 63/64 loss: -0.5003242492675781
Batch 64/64 loss: -4.422192573547363
Epoch 431  Train loss: -0.5685636520385742  Val loss: -0.9328957783807185
Epoch 432
-------------------------------
Batch 1/64 loss: -0.4041581153869629
Batch 2/64 loss: -0.6805238723754883
Batch 3/64 loss: -0.5594282150268555
Batch 4/64 loss: -0.7619514465332031
Batch 5/64 loss: -0.7615847587585449
Batch 6/64 loss: 0.7818832397460938
Batch 7/64 loss: 0.2546825408935547
Batch 8/64 loss: -0.7174892425537109
Batch 9/64 loss: -0.530301570892334
Batch 10/64 loss: -0.659487247467041
Batch 11/64 loss: -0.6301379203796387
Batch 12/64 loss: -0.7180385589599609
Batch 13/64 loss: -0.7082934379577637
Batch 14/64 loss: -0.5591397285461426
Batch 15/64 loss: -0.6784720420837402
Batch 16/64 loss: -0.6979308128356934
Batch 17/64 loss: -0.19357681274414062
Batch 18/64 loss: -0.6986918449401855
Batch 19/64 loss: -0.6275119781494141
Batch 20/64 loss: -0.7573113441467285
Batch 21/64 loss: -0.08848762512207031
Batch 22/64 loss: -0.3718743324279785
Batch 23/64 loss: -0.7119531631469727
Batch 24/64 loss: -0.6744790077209473
Batch 25/64 loss: -0.6133871078491211
Batch 26/64 loss: -0.47737741470336914
Batch 27/64 loss: -0.6038675308227539
Batch 28/64 loss: -0.6509647369384766
Batch 29/64 loss: -0.7577228546142578
Batch 30/64 loss: -0.642916202545166
Batch 31/64 loss: -0.45772886276245117
Batch 32/64 loss: -0.6910958290100098
Batch 33/64 loss: -0.691281795501709
Batch 34/64 loss: -0.7484426498413086
Batch 35/64 loss: -0.6556057929992676
Batch 36/64 loss: -0.3440666198730469
Batch 37/64 loss: -0.5970001220703125
Batch 38/64 loss: -0.5216312408447266
Batch 39/64 loss: -0.5976152420043945
Batch 40/64 loss: -0.17010831832885742
Batch 41/64 loss: -0.5889911651611328
Batch 42/64 loss: -0.5522627830505371
Batch 43/64 loss: -0.5259270668029785
Batch 44/64 loss: 0.4278998374938965
Batch 45/64 loss: -0.5231747627258301
Batch 46/64 loss: -0.5722665786743164
Batch 47/64 loss: -0.6740517616271973
Batch 48/64 loss: -0.505469799041748
Batch 49/64 loss: -0.5216412544250488
Batch 50/64 loss: -0.6295475959777832
Batch 51/64 loss: -0.6296687126159668
Batch 52/64 loss: -0.11303138732910156
Batch 53/64 loss: -0.6119809150695801
Batch 54/64 loss: -0.5679035186767578
Batch 55/64 loss: -0.5346236228942871
Batch 56/64 loss: -0.46352529525756836
Batch 57/64 loss: -0.32837915420532227
Batch 58/64 loss: -0.22629165649414062
Batch 59/64 loss: -0.7114853858947754
Batch 60/64 loss: -0.6359915733337402
Batch 61/64 loss: 0.4486966133117676
Batch 62/64 loss: -0.5804057121276855
Batch 63/64 loss: -0.6019349098205566
Batch 64/64 loss: -4.386692047119141
Epoch 432  Train loss: -0.5472477931602329  Val loss: -0.939765287838441
Epoch 433
-------------------------------
Batch 1/64 loss: -0.7661242485046387
Batch 2/64 loss: -0.7193069458007812
Batch 3/64 loss: -0.5130863189697266
Batch 4/64 loss: -0.4268832206726074
Batch 5/64 loss: -0.6003804206848145
Batch 6/64 loss: -0.5854678153991699
Batch 7/64 loss: -0.34752559661865234
Batch 8/64 loss: -0.6479778289794922
Batch 9/64 loss: -0.7995367050170898
Batch 10/64 loss: -0.7385621070861816
Batch 11/64 loss: -0.29266929626464844
Batch 12/64 loss: -0.5406842231750488
Batch 13/64 loss: -0.6048469543457031
Batch 14/64 loss: -0.5710902214050293
Batch 15/64 loss: -0.6018228530883789
Batch 16/64 loss: -0.7524547576904297
Batch 17/64 loss: -0.662745475769043
Batch 18/64 loss: -0.5674638748168945
Batch 19/64 loss: 0.4037008285522461
Batch 20/64 loss: -0.575096607208252
Batch 21/64 loss: -0.6248750686645508
Batch 22/64 loss: -0.3299283981323242
Batch 23/64 loss: -0.6657423973083496
Batch 24/64 loss: -0.6157236099243164
Batch 25/64 loss: -0.6042652130126953
Batch 26/64 loss: -0.474271297454834
Batch 27/64 loss: -0.749718189239502
Batch 28/64 loss: -0.679135799407959
Batch 29/64 loss: -0.6884493827819824
Batch 30/64 loss: -0.733881950378418
Batch 31/64 loss: -0.5677213668823242
Batch 32/64 loss: -0.5405974388122559
Batch 33/64 loss: -0.6187310218811035
Batch 34/64 loss: -0.5356335639953613
Batch 35/64 loss: -0.11151313781738281
Batch 36/64 loss: -0.7763915061950684
Batch 37/64 loss: -0.2174243927001953
Batch 38/64 loss: 0.6111412048339844
Batch 39/64 loss: -0.7275352478027344
Batch 40/64 loss: -0.7351384162902832
Batch 41/64 loss: -0.6006984710693359
Batch 42/64 loss: -0.6757984161376953
Batch 43/64 loss: -0.6771039962768555
Batch 44/64 loss: -0.7587275505065918
Batch 45/64 loss: -0.587153434753418
Batch 46/64 loss: -0.8709545135498047
Batch 47/64 loss: -0.6869597434997559
Batch 48/64 loss: -0.6832256317138672
Batch 49/64 loss: -0.26066017150878906
Batch 50/64 loss: -0.774681568145752
Batch 51/64 loss: -0.521510124206543
Batch 52/64 loss: -0.7682018280029297
Batch 53/64 loss: -0.7599682807922363
Batch 54/64 loss: -0.7567753791809082
Batch 55/64 loss: 0.6995811462402344
Batch 56/64 loss: -0.5982604026794434
Batch 57/64 loss: -0.31682729721069336
Batch 58/64 loss: -0.7689404487609863
Batch 59/64 loss: -0.723419189453125
Batch 60/64 loss: -0.6833372116088867
Batch 61/64 loss: -0.7989706993103027
Batch 62/64 loss: -0.7156901359558105
Batch 63/64 loss: -0.39330291748046875
Batch 64/64 loss: -4.40484094619751
Epoch 433  Train loss: -0.6004827892079073  Val loss: -1.001330228195977
Epoch 434
-------------------------------
Batch 1/64 loss: -0.9079065322875977
Batch 2/64 loss: -0.7914600372314453
Batch 3/64 loss: -0.6256723403930664
Batch 4/64 loss: -0.8239893913269043
Batch 5/64 loss: -0.7209711074829102
Batch 6/64 loss: -0.815101146697998
Batch 7/64 loss: 0.29279661178588867
Batch 8/64 loss: -0.592442512512207
Batch 9/64 loss: -0.8513398170471191
Batch 10/64 loss: -0.7438516616821289
Batch 11/64 loss: -0.8177170753479004
Batch 12/64 loss: -0.5912423133850098
Batch 13/64 loss: -0.6272048950195312
Batch 14/64 loss: -0.6863861083984375
Batch 15/64 loss: -0.8368134498596191
Batch 16/64 loss: -0.8328409194946289
Batch 17/64 loss: -0.7345619201660156
Batch 18/64 loss: -0.7765660285949707
Batch 19/64 loss: -0.6687130928039551
Batch 20/64 loss: -0.07324504852294922
Batch 21/64 loss: -0.6321983337402344
Batch 22/64 loss: -0.8739547729492188
Batch 23/64 loss: -0.8002681732177734
Batch 24/64 loss: -0.8274917602539062
Batch 25/64 loss: -0.6688551902770996
Batch 26/64 loss: -0.6551947593688965
Batch 27/64 loss: -0.8014678955078125
Batch 28/64 loss: -0.43918800354003906
Batch 29/64 loss: 0.631525993347168
Batch 30/64 loss: -0.529477596282959
Batch 31/64 loss: -0.5434660911560059
Batch 32/64 loss: -0.606414794921875
Batch 33/64 loss: -0.6879968643188477
Batch 34/64 loss: -0.37971925735473633
Batch 35/64 loss: 1.1604132652282715
Batch 36/64 loss: -0.6542987823486328
Batch 37/64 loss: -0.5625185966491699
Batch 38/64 loss: -0.5965666770935059
Batch 39/64 loss: -0.6164507865905762
Batch 40/64 loss: -0.5172390937805176
Batch 41/64 loss: -0.7603549957275391
Batch 42/64 loss: -0.5845756530761719
Batch 43/64 loss: -0.7681155204772949
Batch 44/64 loss: -0.7389302253723145
Batch 45/64 loss: -0.7521448135375977
Batch 46/64 loss: -0.6990833282470703
Batch 47/64 loss: -0.7417068481445312
Batch 48/64 loss: -0.7834129333496094
Batch 49/64 loss: -0.2312607765197754
Batch 50/64 loss: -0.7873239517211914
Batch 51/64 loss: -0.799379825592041
Batch 52/64 loss: -0.7646050453186035
Batch 53/64 loss: -0.8185176849365234
Batch 54/64 loss: -0.8616828918457031
Batch 55/64 loss: -0.8563618659973145
Batch 56/64 loss: -0.7017745971679688
Batch 57/64 loss: -0.8986411094665527
Batch 58/64 loss: -0.7835726737976074
Batch 59/64 loss: -0.7469964027404785
Batch 60/64 loss: -0.8011088371276855
Batch 61/64 loss: -0.8182377815246582
Batch 62/64 loss: -0.8596911430358887
Batch 63/64 loss: -0.9149384498596191
Batch 64/64 loss: -4.010592460632324
Epoch 434  Train loss: -0.6793163710949468  Val loss: -1.0225820508609522
Epoch 435
-------------------------------
Batch 1/64 loss: -0.7806563377380371
Batch 2/64 loss: -0.9143157005310059
Batch 3/64 loss: -0.7121882438659668
Batch 4/64 loss: -0.8766956329345703
Batch 5/64 loss: -0.7928190231323242
Batch 6/64 loss: -0.12798261642456055
Batch 7/64 loss: -0.7233262062072754
Batch 8/64 loss: 0.7218437194824219
Batch 9/64 loss: -0.7695660591125488
Batch 10/64 loss: -0.7908720970153809
Batch 11/64 loss: -0.7388315200805664
Batch 12/64 loss: -0.7986278533935547
Batch 13/64 loss: -0.7506961822509766
Batch 14/64 loss: -0.28452253341674805
Batch 15/64 loss: -0.8126635551452637
Batch 16/64 loss: -0.44553661346435547
Batch 17/64 loss: -0.6796040534973145
Batch 18/64 loss: -0.6508417129516602
Batch 19/64 loss: -0.8732509613037109
Batch 20/64 loss: -0.8370599746704102
Batch 21/64 loss: -0.7014861106872559
Batch 22/64 loss: -0.8792724609375
Batch 23/64 loss: -0.863396167755127
Batch 24/64 loss: -0.7618808746337891
Batch 25/64 loss: -0.47504520416259766
Batch 26/64 loss: 0.8895397186279297
Batch 27/64 loss: 0.08936071395874023
Batch 28/64 loss: -0.7213530540466309
Batch 29/64 loss: -0.6684308052062988
Batch 30/64 loss: -0.04162931442260742
Batch 31/64 loss: -0.7321915626525879
Batch 32/64 loss: -0.6000313758850098
Batch 33/64 loss: -0.6546440124511719
Batch 34/64 loss: -0.6208677291870117
Batch 35/64 loss: -0.46686744689941406
Batch 36/64 loss: -0.17144775390625
Batch 37/64 loss: -0.515632152557373
Batch 38/64 loss: -0.6191792488098145
Batch 39/64 loss: -0.42015743255615234
Batch 40/64 loss: 0.5441346168518066
Batch 41/64 loss: -0.5564122200012207
Batch 42/64 loss: -0.4031991958618164
Batch 43/64 loss: -0.003253459930419922
Batch 44/64 loss: -0.5630536079406738
Batch 45/64 loss: -0.5526866912841797
Batch 46/64 loss: -0.5812501907348633
Batch 47/64 loss: -0.6354236602783203
Batch 48/64 loss: -0.5355520248413086
Batch 49/64 loss: -0.7592077255249023
Batch 50/64 loss: -0.4479236602783203
Batch 51/64 loss: -0.7715444564819336
Batch 52/64 loss: -0.7720541954040527
Batch 53/64 loss: -0.3940119743347168
Batch 54/64 loss: -0.41466283798217773
Batch 55/64 loss: -0.7140712738037109
Batch 56/64 loss: -0.8100261688232422
Batch 57/64 loss: -0.40665102005004883
Batch 58/64 loss: -0.6677041053771973
Batch 59/64 loss: -0.6632199287414551
Batch 60/64 loss: -0.7289280891418457
Batch 61/64 loss: -0.6209654808044434
Batch 62/64 loss: -0.6991686820983887
Batch 63/64 loss: -0.7623300552368164
Batch 64/64 loss: -4.30000114440918
Epoch 435  Train loss: -0.5916390961291743  Val loss: -0.974652018334038
Epoch 436
-------------------------------
Batch 1/64 loss: 0.02226400375366211
Batch 2/64 loss: -0.6737761497497559
Batch 3/64 loss: -0.8426365852355957
Batch 4/64 loss: -0.7681708335876465
Batch 5/64 loss: -0.6951231956481934
Batch 6/64 loss: -0.7581591606140137
Batch 7/64 loss: -0.4240412712097168
Batch 8/64 loss: -0.7349481582641602
Batch 9/64 loss: -0.7861390113830566
Batch 10/64 loss: -0.8275947570800781
Batch 11/64 loss: -0.7227091789245605
Batch 12/64 loss: -0.7102909088134766
Batch 13/64 loss: -0.5743536949157715
Batch 14/64 loss: -0.6646456718444824
Batch 15/64 loss: -0.8178400993347168
Batch 16/64 loss: -0.7606205940246582
Batch 17/64 loss: 0.2474994659423828
Batch 18/64 loss: 0.8935470581054688
Batch 19/64 loss: -0.5109615325927734
Batch 20/64 loss: -0.6730685234069824
Batch 21/64 loss: -0.5949559211730957
Batch 22/64 loss: -0.3068971633911133
Batch 23/64 loss: -0.8430404663085938
Batch 24/64 loss: -0.4259481430053711
Batch 25/64 loss: -0.7836399078369141
Batch 26/64 loss: -0.5164151191711426
Batch 27/64 loss: -0.5365138053894043
Batch 28/64 loss: -0.5484247207641602
Batch 29/64 loss: -0.4645862579345703
Batch 30/64 loss: -0.7697563171386719
Batch 31/64 loss: -0.7773728370666504
Batch 32/64 loss: -0.6527495384216309
Batch 33/64 loss: 1.397658348083496
Batch 34/64 loss: -0.6872282028198242
Batch 35/64 loss: -0.520176887512207
Batch 36/64 loss: -0.5963125228881836
Batch 37/64 loss: 0.9055242538452148
Batch 38/64 loss: -0.7349452972412109
Batch 39/64 loss: -0.8676104545593262
Batch 40/64 loss: -0.6696105003356934
Batch 41/64 loss: -0.7899413108825684
Batch 42/64 loss: -0.5538296699523926
Batch 43/64 loss: -0.7637286186218262
Batch 44/64 loss: -0.6973624229431152
Batch 45/64 loss: -0.777275562286377
Batch 46/64 loss: -0.8101639747619629
Batch 47/64 loss: 0.8634490966796875
Batch 48/64 loss: 0.07132863998413086
Batch 49/64 loss: -0.6475715637207031
Batch 50/64 loss: -0.8140220642089844
Batch 51/64 loss: -0.6570525169372559
Batch 52/64 loss: -0.6759195327758789
Batch 53/64 loss: -0.6677160263061523
Batch 54/64 loss: -0.6835451126098633
Batch 55/64 loss: 0.2908496856689453
Batch 56/64 loss: -0.6920032501220703
Batch 57/64 loss: -0.6977887153625488
Batch 58/64 loss: -0.7367520332336426
Batch 59/64 loss: -0.46668577194213867
Batch 60/64 loss: -0.6836028099060059
Batch 61/64 loss: -0.6814970970153809
Batch 62/64 loss: -0.7012810707092285
Batch 63/64 loss: -0.6843318939208984
Batch 64/64 loss: -4.3118414878845215
Epoch 436  Train loss: -0.5594524701436361  Val loss: -1.0569759775273169
Epoch 437
-------------------------------
Batch 1/64 loss: -0.6998472213745117
Batch 2/64 loss: -0.6208343505859375
Batch 3/64 loss: -0.7182655334472656
Batch 4/64 loss: -0.710606575012207
Batch 5/64 loss: -0.7817869186401367
Batch 6/64 loss: -0.6720490455627441
Batch 7/64 loss: -0.034819602966308594
Batch 8/64 loss: -0.28146791458129883
Batch 9/64 loss: -0.529205322265625
Batch 10/64 loss: -0.594510555267334
Batch 11/64 loss: -0.7648773193359375
Batch 12/64 loss: -0.5863356590270996
Batch 13/64 loss: -0.709601879119873
Batch 14/64 loss: -0.6832895278930664
Batch 15/64 loss: -0.6479558944702148
Batch 16/64 loss: -0.5388412475585938
Batch 17/64 loss: -0.6833243370056152
Batch 18/64 loss: -0.6895289421081543
Batch 19/64 loss: -0.5448131561279297
Batch 20/64 loss: -0.6765251159667969
Batch 21/64 loss: -0.7822022438049316
Batch 22/64 loss: -0.6601476669311523
Batch 23/64 loss: -0.7009763717651367
Batch 24/64 loss: -0.709228515625
Batch 25/64 loss: -0.7598037719726562
Batch 26/64 loss: -0.6249122619628906
Batch 27/64 loss: -0.4585256576538086
Batch 28/64 loss: -0.7422609329223633
Batch 29/64 loss: -0.7481884956359863
Batch 30/64 loss: -0.28541040420532227
Batch 31/64 loss: -0.16451358795166016
Batch 32/64 loss: -0.46510791778564453
Batch 33/64 loss: -0.6067991256713867
Batch 34/64 loss: -0.48906898498535156
Batch 35/64 loss: -0.8525443077087402
Batch 36/64 loss: -0.603935718536377
Batch 37/64 loss: -0.3523397445678711
Batch 38/64 loss: -0.692072868347168
Batch 39/64 loss: -0.6940879821777344
Batch 40/64 loss: -0.5642085075378418
Batch 41/64 loss: -0.7244768142700195
Batch 42/64 loss: -0.6065549850463867
Batch 43/64 loss: -0.9547362327575684
Batch 44/64 loss: -0.7153644561767578
Batch 45/64 loss: -0.5613274574279785
Batch 46/64 loss: -0.6805496215820312
Batch 47/64 loss: -0.888939380645752
Batch 48/64 loss: -0.37746763229370117
Batch 49/64 loss: -0.6362400054931641
Batch 50/64 loss: 1.7050018310546875
Batch 51/64 loss: -0.7178492546081543
Batch 52/64 loss: 0.6930637359619141
Batch 53/64 loss: -0.8206515312194824
Batch 54/64 loss: -0.7771587371826172
Batch 55/64 loss: -0.6525740623474121
Batch 56/64 loss: -0.7414340972900391
Batch 57/64 loss: -0.5333442687988281
Batch 58/64 loss: -0.6311407089233398
Batch 59/64 loss: -0.8351106643676758
Batch 60/64 loss: -0.7258749008178711
Batch 61/64 loss: -0.5758171081542969
Batch 62/64 loss: -0.7500548362731934
Batch 63/64 loss: -0.8680891990661621
Batch 64/64 loss: -4.5539374351501465
Epoch 437  Train loss: -0.6261484651004567  Val loss: -1.1204421184316944
Saving best model, epoch: 437
Epoch 438
-------------------------------
Batch 1/64 loss: -0.8134174346923828
Batch 2/64 loss: -0.8771705627441406
Batch 3/64 loss: -0.8226966857910156
Batch 4/64 loss: 0.37966394424438477
Batch 5/64 loss: -0.754631519317627
Batch 6/64 loss: -0.7497549057006836
Batch 7/64 loss: -0.7614665031433105
Batch 8/64 loss: -0.8522963523864746
Batch 9/64 loss: -0.5906891822814941
Batch 10/64 loss: -0.8140783309936523
Batch 11/64 loss: -0.682830810546875
Batch 12/64 loss: -0.8826971054077148
Batch 13/64 loss: -0.8762402534484863
Batch 14/64 loss: -0.6490159034729004
Batch 15/64 loss: -0.7178487777709961
Batch 16/64 loss: -0.853184700012207
Batch 17/64 loss: -0.7765092849731445
Batch 18/64 loss: -0.8297181129455566
Batch 19/64 loss: -0.5335383415222168
Batch 20/64 loss: -0.6807861328125
Batch 21/64 loss: -0.8836455345153809
Batch 22/64 loss: -0.8054103851318359
Batch 23/64 loss: -0.5883169174194336
Batch 24/64 loss: -0.651850700378418
Batch 25/64 loss: -0.5667548179626465
Batch 26/64 loss: -0.9637594223022461
Batch 27/64 loss: -0.7399473190307617
Batch 28/64 loss: -0.8090057373046875
Batch 29/64 loss: -0.7754335403442383
Batch 30/64 loss: -0.5784311294555664
Batch 31/64 loss: -0.9128322601318359
Batch 32/64 loss: -0.7952876091003418
Batch 33/64 loss: -0.9260373115539551
Batch 34/64 loss: -0.8583364486694336
Batch 35/64 loss: -0.1793375015258789
Batch 36/64 loss: -0.867103099822998
Batch 37/64 loss: -0.8272242546081543
Batch 38/64 loss: -0.7893614768981934
Batch 39/64 loss: -0.6474781036376953
Batch 40/64 loss: -0.7341012954711914
Batch 41/64 loss: -0.8553476333618164
Batch 42/64 loss: -0.8565845489501953
Batch 43/64 loss: -0.7242355346679688
Batch 44/64 loss: -0.7737188339233398
Batch 45/64 loss: -0.813270092010498
Batch 46/64 loss: -0.8227667808532715
Batch 47/64 loss: -0.5773124694824219
Batch 48/64 loss: -0.8342041969299316
Batch 49/64 loss: -0.7442402839660645
Batch 50/64 loss: -0.7477178573608398
Batch 51/64 loss: -0.22470426559448242
Batch 52/64 loss: -0.6690115928649902
Batch 53/64 loss: -0.686089038848877
Batch 54/64 loss: -0.844482421875
Batch 55/64 loss: -0.9348969459533691
Batch 56/64 loss: -0.8257875442504883
Batch 57/64 loss: -0.163909912109375
Batch 58/64 loss: 0.2753000259399414
Batch 59/64 loss: 0.668973445892334
Batch 60/64 loss: -0.7199559211730957
Batch 61/64 loss: -0.8143377304077148
Batch 62/64 loss: -0.6817073822021484
Batch 63/64 loss: -0.757084846496582
Batch 64/64 loss: -4.426734447479248
Epoch 438  Train loss: -0.7291875296948003  Val loss: -1.1160827846461554
Epoch 439
-------------------------------
Batch 1/64 loss: -0.8475184440612793
Batch 2/64 loss: -0.8594150543212891
Batch 3/64 loss: -0.8847794532775879
Batch 4/64 loss: -0.41092824935913086
Batch 5/64 loss: -0.8075213432312012
Batch 6/64 loss: -0.7267346382141113
Batch 7/64 loss: -0.8697257041931152
Batch 8/64 loss: -0.6764926910400391
Batch 9/64 loss: -0.8526105880737305
Batch 10/64 loss: -0.8614788055419922
Batch 11/64 loss: -0.774299144744873
Batch 12/64 loss: -0.6482682228088379
Batch 13/64 loss: -0.7970614433288574
Batch 14/64 loss: 0.9178042411804199
Batch 15/64 loss: -0.8279199600219727
Batch 16/64 loss: -0.6404862403869629
Batch 17/64 loss: 1.5636320114135742
Batch 18/64 loss: -0.8618292808532715
Batch 19/64 loss: -0.08954477310180664
Batch 20/64 loss: -0.821406364440918
Batch 21/64 loss: -0.787381649017334
Batch 22/64 loss: -0.8889317512512207
Batch 23/64 loss: -0.7656803131103516
Batch 24/64 loss: 0.3898477554321289
Batch 25/64 loss: -0.6864185333251953
Batch 26/64 loss: -0.8323988914489746
Batch 27/64 loss: -0.7911725044250488
Batch 28/64 loss: -0.7761068344116211
Batch 29/64 loss: -0.838991641998291
Batch 30/64 loss: 0.11059999465942383
Batch 31/64 loss: -0.8899660110473633
Batch 32/64 loss: -0.6907773017883301
Batch 33/64 loss: -0.5141434669494629
Batch 34/64 loss: -0.9161953926086426
Batch 35/64 loss: -0.7782487869262695
Batch 36/64 loss: -0.8070335388183594
Batch 37/64 loss: -0.7008471488952637
Batch 38/64 loss: -0.7060656547546387
Batch 39/64 loss: -0.6511678695678711
Batch 40/64 loss: -0.8807277679443359
Batch 41/64 loss: -0.861626148223877
Batch 42/64 loss: -0.531740665435791
Batch 43/64 loss: -0.3379817008972168
Batch 44/64 loss: -0.6863012313842773
Batch 45/64 loss: -0.7727851867675781
Batch 46/64 loss: -0.8917913436889648
Batch 47/64 loss: 0.48285436630249023
Batch 48/64 loss: -0.037154197692871094
Batch 49/64 loss: -0.8521947860717773
Batch 50/64 loss: -0.4063897132873535
Batch 51/64 loss: -0.7941598892211914
Batch 52/64 loss: -0.738861083984375
Batch 53/64 loss: -0.35765743255615234
Batch 54/64 loss: -0.8294939994812012
Batch 55/64 loss: -0.7512211799621582
Batch 56/64 loss: -0.7846264839172363
Batch 57/64 loss: -0.7421045303344727
Batch 58/64 loss: -0.7438807487487793
Batch 59/64 loss: -0.6987829208374023
Batch 60/64 loss: -0.6620392799377441
Batch 61/64 loss: -0.7668190002441406
Batch 62/64 loss: -0.7785944938659668
Batch 63/64 loss: -0.8260254859924316
Batch 64/64 loss: -4.637260437011719
Epoch 439  Train loss: -0.6592268850289139  Val loss: -1.0900340653776712
Epoch 440
-------------------------------
Batch 1/64 loss: -0.5215358734130859
Batch 2/64 loss: -0.7229866981506348
Batch 3/64 loss: -0.7826828956604004
Batch 4/64 loss: -0.8316926956176758
Batch 5/64 loss: -0.8247570991516113
Batch 6/64 loss: -0.8974623680114746
Batch 7/64 loss: -0.817807674407959
Batch 8/64 loss: 0.28946447372436523
Batch 9/64 loss: -0.9139461517333984
Batch 10/64 loss: 0.36704254150390625
Batch 11/64 loss: 0.08777999877929688
Batch 12/64 loss: -0.4579901695251465
Batch 13/64 loss: -0.49746274948120117
Batch 14/64 loss: -0.8207879066467285
Batch 15/64 loss: -0.8120303153991699
Batch 16/64 loss: -0.7029032707214355
Batch 17/64 loss: -0.5781354904174805
Batch 18/64 loss: -0.7940716743469238
Batch 19/64 loss: 0.800318717956543
Batch 20/64 loss: -0.8049216270446777
Batch 21/64 loss: -0.8332791328430176
Batch 22/64 loss: -0.8281855583190918
Batch 23/64 loss: -0.868441104888916
Batch 24/64 loss: -0.8197450637817383
Batch 25/64 loss: -0.8347148895263672
Batch 26/64 loss: -0.6788616180419922
Batch 27/64 loss: -0.7606277465820312
Batch 28/64 loss: -0.8972201347351074
Batch 29/64 loss: -0.8082385063171387
Batch 30/64 loss: -0.7895321846008301
Batch 31/64 loss: -0.49184083938598633
Batch 32/64 loss: -0.8924679756164551
Batch 33/64 loss: -0.8447909355163574
Batch 34/64 loss: -0.8602504730224609
Batch 35/64 loss: -0.8366222381591797
Batch 36/64 loss: -0.768333911895752
Batch 37/64 loss: -0.7898406982421875
Batch 38/64 loss: -0.8296384811401367
Batch 39/64 loss: -0.8963723182678223
Batch 40/64 loss: -0.7394800186157227
Batch 41/64 loss: -0.7304143905639648
Batch 42/64 loss: -0.9167637825012207
Batch 43/64 loss: -0.9289054870605469
Batch 44/64 loss: -0.7355499267578125
Batch 45/64 loss: -0.1623396873474121
Batch 46/64 loss: -0.7364592552185059
Batch 47/64 loss: -0.8525958061218262
Batch 48/64 loss: -0.8392081260681152
Batch 49/64 loss: -0.7982277870178223
Batch 50/64 loss: -0.8376851081848145
Batch 51/64 loss: -0.8495578765869141
Batch 52/64 loss: -0.9790635108947754
Batch 53/64 loss: -0.882575511932373
Batch 54/64 loss: -0.3525357246398926
Batch 55/64 loss: -1.0099596977233887
Batch 56/64 loss: -0.9014124870300293
Batch 57/64 loss: -0.8639206886291504
Batch 58/64 loss: -0.9439020156860352
Batch 59/64 loss: -0.8319106101989746
Batch 60/64 loss: -0.8436751365661621
Batch 61/64 loss: -0.9359135627746582
Batch 62/64 loss: -0.9331393241882324
Batch 63/64 loss: -0.9621105194091797
Batch 64/64 loss: -3.313974380493164
Epoch 440  Train loss: -0.7469546598546645  Val loss: -1.2091970542042525
Saving best model, epoch: 440
Epoch 441
-------------------------------
Batch 1/64 loss: -0.8072032928466797
Batch 2/64 loss: -0.8502445220947266
Batch 3/64 loss: -0.8191657066345215
Batch 4/64 loss: -0.7992582321166992
Batch 5/64 loss: -0.7864899635314941
Batch 6/64 loss: -0.7705464363098145
Batch 7/64 loss: -0.9714927673339844
Batch 8/64 loss: -0.9175314903259277
Batch 9/64 loss: -0.9326043128967285
Batch 10/64 loss: -0.7287797927856445
Batch 11/64 loss: -0.8584227561950684
Batch 12/64 loss: -0.7130522727966309
Batch 13/64 loss: -0.8211650848388672
Batch 14/64 loss: -0.4624629020690918
Batch 15/64 loss: -0.6354050636291504
Batch 16/64 loss: -0.7653646469116211
Batch 17/64 loss: -0.8982386589050293
Batch 18/64 loss: -0.8816361427307129
Batch 19/64 loss: -0.7917075157165527
Batch 20/64 loss: -0.8226518630981445
Batch 21/64 loss: -0.7089414596557617
Batch 22/64 loss: -0.868891716003418
Batch 23/64 loss: -0.9268383979797363
Batch 24/64 loss: -0.9305825233459473
Batch 25/64 loss: -1.0629067420959473
Batch 26/64 loss: -0.4140286445617676
Batch 27/64 loss: -0.9006433486938477
Batch 28/64 loss: -0.9765057563781738
Batch 29/64 loss: -0.8918595314025879
Batch 30/64 loss: -0.8346719741821289
Batch 31/64 loss: -0.9379329681396484
Batch 32/64 loss: -0.9707145690917969
Batch 33/64 loss: -0.8853845596313477
Batch 34/64 loss: -0.6645050048828125
Batch 35/64 loss: -0.9025444984436035
Batch 36/64 loss: -0.7823901176452637
Batch 37/64 loss: -0.9417634010314941
Batch 38/64 loss: -0.8980603218078613
Batch 39/64 loss: -0.9045567512512207
Batch 40/64 loss: -0.3124256134033203
Batch 41/64 loss: -0.8692564964294434
Batch 42/64 loss: -0.9287171363830566
Batch 43/64 loss: -0.9539718627929688
Batch 44/64 loss: -0.9564023017883301
Batch 45/64 loss: -0.9457807540893555
Batch 46/64 loss: -0.627286434173584
Batch 47/64 loss: -0.9723639488220215
Batch 48/64 loss: 0.5823211669921875
Batch 49/64 loss: -0.7433757781982422
Batch 50/64 loss: -0.7365846633911133
Batch 51/64 loss: -0.8744702339172363
Batch 52/64 loss: -0.8669905662536621
Batch 53/64 loss: -0.9213271141052246
Batch 54/64 loss: -0.937443733215332
Batch 55/64 loss: -0.8313617706298828
Batch 56/64 loss: -0.9381532669067383
Batch 57/64 loss: -0.8786745071411133
Batch 58/64 loss: -0.8494691848754883
Batch 59/64 loss: -0.9771804809570312
Batch 60/64 loss: -0.8827481269836426
Batch 61/64 loss: -0.8227701187133789
Batch 62/64 loss: -0.7751498222351074
Batch 63/64 loss: 1.0600948333740234
Batch 64/64 loss: -3.495361328125
Epoch 441  Train loss: -0.8159710528803806  Val loss: -1.1813481910941528
Epoch 442
-------------------------------
Batch 1/64 loss: -1.0989384651184082
Batch 2/64 loss: 1.1366000175476074
Batch 3/64 loss: -0.9408688545227051
Batch 4/64 loss: -0.8914780616760254
Batch 5/64 loss: -0.9760704040527344
Batch 6/64 loss: -0.9256472587585449
Batch 7/64 loss: -0.8008303642272949
Batch 8/64 loss: -0.9161605834960938
Batch 9/64 loss: -0.3977508544921875
Batch 10/64 loss: 0.31648874282836914
Batch 11/64 loss: -0.5526137351989746
Batch 12/64 loss: -0.8433065414428711
Batch 13/64 loss: -1.037217617034912
Batch 14/64 loss: -1.0373215675354004
Batch 15/64 loss: -0.8023252487182617
Batch 16/64 loss: -0.5624136924743652
Batch 17/64 loss: -0.8583774566650391
Batch 18/64 loss: -0.9664273262023926
Batch 19/64 loss: -0.8168349266052246
Batch 20/64 loss: -0.8015851974487305
Batch 21/64 loss: -0.7211694717407227
Batch 22/64 loss: -0.7307004928588867
Batch 23/64 loss: -0.7309727668762207
Batch 24/64 loss: -0.9138121604919434
Batch 25/64 loss: -0.87091064453125
Batch 26/64 loss: -0.8226242065429688
Batch 27/64 loss: -0.8615279197692871
Batch 28/64 loss: -0.9020247459411621
Batch 29/64 loss: -0.8165431022644043
Batch 30/64 loss: 0.02348613739013672
Batch 31/64 loss: -0.819176197052002
Batch 32/64 loss: -0.7974100112915039
Batch 33/64 loss: -0.885465145111084
Batch 34/64 loss: -0.8426475524902344
Batch 35/64 loss: -0.9297289848327637
Batch 36/64 loss: -0.8454241752624512
Batch 37/64 loss: -0.4039487838745117
Batch 38/64 loss: -0.7824568748474121
Batch 39/64 loss: -0.20297622680664062
Batch 40/64 loss: -0.9821462631225586
Batch 41/64 loss: -0.841336727142334
Batch 42/64 loss: -0.7950296401977539
Batch 43/64 loss: -0.8707265853881836
Batch 44/64 loss: -0.8838281631469727
Batch 45/64 loss: -0.8992400169372559
Batch 46/64 loss: -0.8355326652526855
Batch 47/64 loss: -0.8225150108337402
Batch 48/64 loss: -0.6722722053527832
Batch 49/64 loss: -0.8276524543762207
Batch 50/64 loss: -0.6623263359069824
Batch 51/64 loss: -0.8849039077758789
Batch 52/64 loss: -0.6727290153503418
Batch 53/64 loss: -0.8186078071594238
Batch 54/64 loss: -0.8471989631652832
Batch 55/64 loss: -0.7487273216247559
Batch 56/64 loss: -0.7602314949035645
Batch 57/64 loss: -0.6720705032348633
Batch 58/64 loss: -0.8445296287536621
Batch 59/64 loss: -0.7874107360839844
Batch 60/64 loss: -0.7710065841674805
Batch 61/64 loss: -0.7869424819946289
Batch 62/64 loss: -0.6928448677062988
Batch 63/64 loss: -0.926720142364502
Batch 64/64 loss: -4.511312484741211
Epoch 442  Train loss: -0.7893196330350988  Val loss: -1.0160830782860826
Epoch 443
-------------------------------
Batch 1/64 loss: -0.8498649597167969
Batch 2/64 loss: -0.8769683837890625
Batch 3/64 loss: -0.8457522392272949
Batch 4/64 loss: -0.8895187377929688
Batch 5/64 loss: -0.6884284019470215
Batch 6/64 loss: -0.7275276184082031
Batch 7/64 loss: -0.8584794998168945
Batch 8/64 loss: -0.33586978912353516
Batch 9/64 loss: -0.8729500770568848
Batch 10/64 loss: -0.8591289520263672
Batch 11/64 loss: -0.6724147796630859
Batch 12/64 loss: -0.8851828575134277
Batch 13/64 loss: -0.7991714477539062
Batch 14/64 loss: -0.7973461151123047
Batch 15/64 loss: -0.7591867446899414
Batch 16/64 loss: -0.8191952705383301
Batch 17/64 loss: -0.647216796875
Batch 18/64 loss: -0.8699283599853516
Batch 19/64 loss: -0.5778899192810059
Batch 20/64 loss: -0.1445326805114746
Batch 21/64 loss: -0.6589083671569824
Batch 22/64 loss: -0.8765044212341309
Batch 23/64 loss: -0.8283953666687012
Batch 24/64 loss: -0.3648257255554199
Batch 25/64 loss: -0.8357782363891602
Batch 26/64 loss: -0.7427968978881836
Batch 27/64 loss: -0.8523836135864258
Batch 28/64 loss: -0.8283724784851074
Batch 29/64 loss: -0.6959195137023926
Batch 30/64 loss: 0.4156928062438965
Batch 31/64 loss: -0.6928119659423828
Batch 32/64 loss: -0.661949634552002
Batch 33/64 loss: -0.6250696182250977
Batch 34/64 loss: -0.8398566246032715
Batch 35/64 loss: -0.915463924407959
Batch 36/64 loss: -0.2786283493041992
Batch 37/64 loss: -0.5399336814880371
Batch 38/64 loss: -0.7974987030029297
Batch 39/64 loss: -0.8677277565002441
Batch 40/64 loss: -0.7182626724243164
Batch 41/64 loss: -0.42972612380981445
Batch 42/64 loss: -0.6775016784667969
Batch 43/64 loss: -0.8434772491455078
Batch 44/64 loss: -0.985133171081543
Batch 45/64 loss: -0.9189724922180176
Batch 46/64 loss: -0.8785176277160645
Batch 47/64 loss: -0.6560788154602051
Batch 48/64 loss: -0.5983924865722656
Batch 49/64 loss: -0.7313261032104492
Batch 50/64 loss: -0.7590975761413574
Batch 51/64 loss: -0.734468936920166
Batch 52/64 loss: -0.5974349975585938
Batch 53/64 loss: -0.7065548896789551
Batch 54/64 loss: -0.3976116180419922
Batch 55/64 loss: -0.43404054641723633
Batch 56/64 loss: -0.7990274429321289
Batch 57/64 loss: -0.7690291404724121
Batch 58/64 loss: -0.9753789901733398
Batch 59/64 loss: -0.8992519378662109
Batch 60/64 loss: -0.7061657905578613
Batch 61/64 loss: -0.8364777565002441
Batch 62/64 loss: 0.14218521118164062
Batch 63/64 loss: -0.9489350318908691
Batch 64/64 loss: -3.015726089477539
Epoch 443  Train loss: -0.7275946374033012  Val loss: -1.0795263965514927
Epoch 444
-------------------------------
Batch 1/64 loss: -0.7683863639831543
Batch 2/64 loss: -0.7680716514587402
Batch 3/64 loss: -0.6599435806274414
Batch 4/64 loss: 0.4078240394592285
Batch 5/64 loss: -0.6926918029785156
Batch 6/64 loss: -0.6931495666503906
Batch 7/64 loss: -0.7388014793395996
Batch 8/64 loss: -0.7716770172119141
Batch 9/64 loss: -0.8829584121704102
Batch 10/64 loss: -0.7563409805297852
Batch 11/64 loss: -0.8336911201477051
Batch 12/64 loss: -0.613222599029541
Batch 13/64 loss: -0.8474335670471191
Batch 14/64 loss: -0.7635321617126465
Batch 15/64 loss: -0.7658348083496094
Batch 16/64 loss: -0.585169792175293
Batch 17/64 loss: -0.6998181343078613
Batch 18/64 loss: -0.32716894149780273
Batch 19/64 loss: -0.8121471405029297
Batch 20/64 loss: -0.7266936302185059
Batch 21/64 loss: -0.8095240592956543
Batch 22/64 loss: -0.9114212989807129
Batch 23/64 loss: -0.7629742622375488
Batch 24/64 loss: -0.7821664810180664
Batch 25/64 loss: -0.6339659690856934
Batch 26/64 loss: -0.7730674743652344
Batch 27/64 loss: -0.7393403053283691
Batch 28/64 loss: -0.388094425201416
Batch 29/64 loss: -0.7661566734313965
Batch 30/64 loss: -0.7104568481445312
Batch 31/64 loss: -0.8123459815979004
Batch 32/64 loss: -0.7535319328308105
Batch 33/64 loss: -0.6383881568908691
Batch 34/64 loss: -0.7046670913696289
Batch 35/64 loss: -0.7971458435058594
Batch 36/64 loss: -0.8510594367980957
Batch 37/64 loss: -0.7370905876159668
Batch 38/64 loss: -0.9039115905761719
Batch 39/64 loss: -0.9074559211730957
Batch 40/64 loss: -0.7126221656799316
Batch 41/64 loss: -0.7358579635620117
Batch 42/64 loss: -0.8204507827758789
Batch 43/64 loss: -0.44558238983154297
Batch 44/64 loss: -0.7789525985717773
Batch 45/64 loss: -0.7862706184387207
Batch 46/64 loss: -0.9108872413635254
Batch 47/64 loss: -0.8892416954040527
Batch 48/64 loss: -0.35062408447265625
Batch 49/64 loss: 0.8127570152282715
Batch 50/64 loss: -0.8130669593811035
Batch 51/64 loss: -0.867945671081543
Batch 52/64 loss: -0.7522792816162109
Batch 53/64 loss: -0.9199132919311523
Batch 54/64 loss: -0.7665600776672363
Batch 55/64 loss: -0.7699317932128906
Batch 56/64 loss: -0.6386747360229492
Batch 57/64 loss: -0.6679062843322754
Batch 58/64 loss: -0.8781871795654297
Batch 59/64 loss: -0.3027191162109375
Batch 60/64 loss: -0.7827777862548828
Batch 61/64 loss: -0.9040474891662598
Batch 62/64 loss: 0.3799290657043457
Batch 63/64 loss: -0.7279291152954102
Batch 64/64 loss: -4.423344612121582
Epoch 444  Train loss: -0.7220223333321366  Val loss: -1.0405540859576352
Epoch 445
-------------------------------
Batch 1/64 loss: -0.8696651458740234
Batch 2/64 loss: -0.7709717750549316
Batch 3/64 loss: -0.7498126029968262
Batch 4/64 loss: -0.6070718765258789
Batch 5/64 loss: -0.04325294494628906
Batch 6/64 loss: -0.7432074546813965
Batch 7/64 loss: -0.6669535636901855
Batch 8/64 loss: -0.5220026969909668
Batch 9/64 loss: -0.7989358901977539
Batch 10/64 loss: -0.8101167678833008
Batch 11/64 loss: -0.7564334869384766
Batch 12/64 loss: -0.8807201385498047
Batch 13/64 loss: -0.6681623458862305
Batch 14/64 loss: -0.9035019874572754
Batch 15/64 loss: -0.5355253219604492
Batch 16/64 loss: -0.4893159866333008
Batch 17/64 loss: 0.2579479217529297
Batch 18/64 loss: -0.8649425506591797
Batch 19/64 loss: -0.623051643371582
Batch 20/64 loss: -0.7316775321960449
Batch 21/64 loss: -0.8651361465454102
Batch 22/64 loss: -0.8115673065185547
Batch 23/64 loss: -0.7880945205688477
Batch 24/64 loss: -0.617652416229248
Batch 25/64 loss: -0.8565158843994141
Batch 26/64 loss: -0.8528294563293457
Batch 27/64 loss: 0.9658284187316895
Batch 28/64 loss: -0.7420434951782227
Batch 29/64 loss: -0.6868376731872559
Batch 30/64 loss: -0.8323359489440918
Batch 31/64 loss: -0.7658376693725586
Batch 32/64 loss: -0.748389720916748
Batch 33/64 loss: -0.8438720703125
Batch 34/64 loss: -0.7803263664245605
Batch 35/64 loss: -0.5293827056884766
Batch 36/64 loss: -0.832789421081543
Batch 37/64 loss: -0.8880271911621094
Batch 38/64 loss: -0.8669219017028809
Batch 39/64 loss: -0.6104588508605957
Batch 40/64 loss: -0.5136079788208008
Batch 41/64 loss: 0.13728857040405273
Batch 42/64 loss: -0.8653273582458496
Batch 43/64 loss: -0.811469554901123
Batch 44/64 loss: -0.7660403251647949
Batch 45/64 loss: -0.7634825706481934
Batch 46/64 loss: -0.29828834533691406
Batch 47/64 loss: -0.8428988456726074
Batch 48/64 loss: -0.5575685501098633
Batch 49/64 loss: -0.7437982559204102
Batch 50/64 loss: -0.508544921875
Batch 51/64 loss: -0.7829265594482422
Batch 52/64 loss: -0.8010320663452148
Batch 53/64 loss: -0.7302823066711426
Batch 54/64 loss: -0.659578800201416
Batch 55/64 loss: -0.7971382141113281
Batch 56/64 loss: -0.8745207786560059
Batch 57/64 loss: 0.5707969665527344
Batch 58/64 loss: -0.8099422454833984
Batch 59/64 loss: -0.44524240493774414
Batch 60/64 loss: -0.8543310165405273
Batch 61/64 loss: -0.5971112251281738
Batch 62/64 loss: -0.787569522857666
Batch 63/64 loss: -0.9637517929077148
Batch 64/64 loss: -4.5321784019470215
Epoch 445  Train loss: -0.6932716238732431  Val loss: -1.1362208861255974
Epoch 446
-------------------------------
Batch 1/64 loss: -0.7018060684204102
Batch 2/64 loss: -0.9103660583496094
Batch 3/64 loss: -0.7685489654541016
Batch 4/64 loss: 0.1577291488647461
Batch 5/64 loss: -0.880378246307373
Batch 6/64 loss: -0.7657532691955566
Batch 7/64 loss: -0.7577986717224121
Batch 8/64 loss: -0.7847132682800293
Batch 9/64 loss: -0.6505203247070312
Batch 10/64 loss: -0.932253360748291
Batch 11/64 loss: -0.8439512252807617
Batch 12/64 loss: -0.7298769950866699
Batch 13/64 loss: -0.5847477912902832
Batch 14/64 loss: -0.6189866065979004
Batch 15/64 loss: -0.6822471618652344
Batch 16/64 loss: 0.1817464828491211
Batch 17/64 loss: 0.4509139060974121
Batch 18/64 loss: -0.897343635559082
Batch 19/64 loss: -0.9313774108886719
Batch 20/64 loss: -0.9157562255859375
Batch 21/64 loss: -0.696681022644043
Batch 22/64 loss: -0.6660876274108887
Batch 23/64 loss: -0.8790717124938965
Batch 24/64 loss: -0.45182323455810547
Batch 25/64 loss: 0.654228687286377
Batch 26/64 loss: -0.7640700340270996
Batch 27/64 loss: -0.7800073623657227
Batch 28/64 loss: -0.8222160339355469
Batch 29/64 loss: -0.7925071716308594
Batch 30/64 loss: -0.9443130493164062
Batch 31/64 loss: -0.8668556213378906
Batch 32/64 loss: -0.7892365455627441
Batch 33/64 loss: -0.5337977409362793
Batch 34/64 loss: -0.339505672454834
Batch 35/64 loss: -0.726104736328125
Batch 36/64 loss: -0.7032837867736816
Batch 37/64 loss: -0.6486091613769531
Batch 38/64 loss: -0.180145263671875
Batch 39/64 loss: -0.7910113334655762
Batch 40/64 loss: -0.8134975433349609
Batch 41/64 loss: -0.6813092231750488
Batch 42/64 loss: -0.7428402900695801
Batch 43/64 loss: -0.9104018211364746
Batch 44/64 loss: -0.5476455688476562
Batch 45/64 loss: -0.9588041305541992
Batch 46/64 loss: -0.7709612846374512
Batch 47/64 loss: -0.7945036888122559
Batch 48/64 loss: -0.8112654685974121
Batch 49/64 loss: -0.8626608848571777
Batch 50/64 loss: -0.2671208381652832
Batch 51/64 loss: -0.9225649833679199
Batch 52/64 loss: -0.9365458488464355
Batch 53/64 loss: -0.5665779113769531
Batch 54/64 loss: -0.9920530319213867
Batch 55/64 loss: -0.902951717376709
Batch 56/64 loss: -0.6664900779724121
Batch 57/64 loss: -0.8300223350524902
Batch 58/64 loss: -0.7794761657714844
Batch 59/64 loss: -0.8198118209838867
Batch 60/64 loss: -0.9177336692810059
Batch 61/64 loss: -0.803368091583252
Batch 62/64 loss: -0.40720605850219727
Batch 63/64 loss: -0.7144899368286133
Batch 64/64 loss: -4.469502925872803
Epoch 446  Train loss: -0.72250296087826  Val loss: -1.0568389761488872
Epoch 447
-------------------------------
Batch 1/64 loss: -0.8438968658447266
Batch 2/64 loss: -0.5460710525512695
Batch 3/64 loss: 0.35796546936035156
Batch 4/64 loss: -0.7722921371459961
Batch 5/64 loss: -0.3765902519226074
Batch 6/64 loss: -0.8423757553100586
Batch 7/64 loss: -0.7299456596374512
Batch 8/64 loss: -0.5550479888916016
Batch 9/64 loss: -0.19559288024902344
Batch 10/64 loss: -0.8811173439025879
Batch 11/64 loss: -0.6853833198547363
Batch 12/64 loss: -0.27460479736328125
Batch 13/64 loss: 0.6986284255981445
Batch 14/64 loss: -0.6852216720581055
Batch 15/64 loss: -0.6305994987487793
Batch 16/64 loss: -0.6889982223510742
Batch 17/64 loss: -0.5489716529846191
Batch 18/64 loss: -0.7838926315307617
Batch 19/64 loss: -0.6119289398193359
Batch 20/64 loss: -0.7436332702636719
Batch 21/64 loss: -0.8349127769470215
Batch 22/64 loss: -0.0107269287109375
Batch 23/64 loss: -0.7630510330200195
Batch 24/64 loss: -0.8857994079589844
Batch 25/64 loss: -0.8380136489868164
Batch 26/64 loss: -0.7027440071105957
Batch 27/64 loss: -0.7458162307739258
Batch 28/64 loss: -0.7472147941589355
Batch 29/64 loss: -0.874168872833252
Batch 30/64 loss: 0.18936586380004883
Batch 31/64 loss: -0.754946231842041
Batch 32/64 loss: -0.7595181465148926
Batch 33/64 loss: -0.8193387985229492
Batch 34/64 loss: -0.8599185943603516
Batch 35/64 loss: -0.7077417373657227
Batch 36/64 loss: -0.9758052825927734
Batch 37/64 loss: -0.7854104042053223
Batch 38/64 loss: -0.8445696830749512
Batch 39/64 loss: -0.800560474395752
Batch 40/64 loss: -0.9202651977539062
Batch 41/64 loss: -0.6818976402282715
Batch 42/64 loss: -0.7195630073547363
Batch 43/64 loss: -0.7787542343139648
Batch 44/64 loss: -0.8517618179321289
Batch 45/64 loss: -0.5468144416809082
Batch 46/64 loss: -0.7969551086425781
Batch 47/64 loss: -0.8090486526489258
Batch 48/64 loss: -0.8941359519958496
Batch 49/64 loss: -0.8643851280212402
Batch 50/64 loss: -0.7799615859985352
Batch 51/64 loss: -0.9161438941955566
Batch 52/64 loss: -0.8989434242248535
Batch 53/64 loss: -0.6906828880310059
Batch 54/64 loss: -0.7581996917724609
Batch 55/64 loss: -0.5804104804992676
Batch 56/64 loss: -0.8207759857177734
Batch 57/64 loss: -0.7358226776123047
Batch 58/64 loss: -0.9056782722473145
Batch 59/64 loss: -0.7021870613098145
Batch 60/64 loss: -0.4746427536010742
Batch 61/64 loss: -0.8229541778564453
Batch 62/64 loss: -0.7983474731445312
Batch 63/64 loss: -0.5014381408691406
Batch 64/64 loss: -4.347604751586914
Epoch 447  Train loss: -0.71170093311983  Val loss: -1.0384516371894128
Epoch 448
-------------------------------
Batch 1/64 loss: -0.6517024040222168
Batch 2/64 loss: -0.7310051918029785
Batch 3/64 loss: 0.7035059928894043
Batch 4/64 loss: -0.7959823608398438
Batch 5/64 loss: -0.7690825462341309
Batch 6/64 loss: -0.7034525871276855
Batch 7/64 loss: -0.509772777557373
Batch 8/64 loss: -0.676142692565918
Batch 9/64 loss: -0.42468786239624023
Batch 10/64 loss: -0.5802583694458008
Batch 11/64 loss: -0.8706021308898926
Batch 12/64 loss: -0.6701316833496094
Batch 13/64 loss: -0.8040413856506348
Batch 14/64 loss: -0.783348560333252
Batch 15/64 loss: -0.8028979301452637
Batch 16/64 loss: -0.8147649765014648
Batch 17/64 loss: -0.7985224723815918
Batch 18/64 loss: -0.738527774810791
Batch 19/64 loss: -0.6368007659912109
Batch 20/64 loss: -0.698453426361084
Batch 21/64 loss: -0.8799529075622559
Batch 22/64 loss: -0.8131885528564453
Batch 23/64 loss: -0.7706432342529297
Batch 24/64 loss: -0.5646829605102539
Batch 25/64 loss: -0.46550607681274414
Batch 26/64 loss: -0.7710614204406738
Batch 27/64 loss: -0.7748622894287109
Batch 28/64 loss: 0.0032129287719726562
Batch 29/64 loss: -0.6912846565246582
Batch 30/64 loss: 0.4541497230529785
Batch 31/64 loss: -0.6265177726745605
Batch 32/64 loss: -0.6827068328857422
Batch 33/64 loss: -0.5812535285949707
Batch 34/64 loss: -0.4646759033203125
Batch 35/64 loss: -1.0136332511901855
Batch 36/64 loss: -0.5373754501342773
Batch 37/64 loss: -0.6156344413757324
Batch 38/64 loss: -0.4574441909790039
Batch 39/64 loss: -0.5434741973876953
Batch 40/64 loss: -0.7949123382568359
Batch 41/64 loss: -0.10464286804199219
Batch 42/64 loss: -0.8196134567260742
Batch 43/64 loss: 0.1810140609741211
Batch 44/64 loss: -0.8063573837280273
Batch 45/64 loss: -0.6592960357666016
Batch 46/64 loss: 0.35544395446777344
Batch 47/64 loss: -0.8697028160095215
Batch 48/64 loss: -0.7601799964904785
Batch 49/64 loss: -0.4987039566040039
Batch 50/64 loss: -0.3326072692871094
Batch 51/64 loss: -0.6227726936340332
Batch 52/64 loss: -0.7777366638183594
Batch 53/64 loss: -0.8241519927978516
Batch 54/64 loss: -0.6491360664367676
Batch 55/64 loss: -0.8105020523071289
Batch 56/64 loss: -0.6690592765808105
Batch 57/64 loss: -0.6831483840942383
Batch 58/64 loss: -0.6913619041442871
Batch 59/64 loss: -0.7118649482727051
Batch 60/64 loss: -0.993067741394043
Batch 61/64 loss: -0.9149971008300781
Batch 62/64 loss: -0.7595338821411133
Batch 63/64 loss: -0.6955809593200684
Batch 64/64 loss: -4.50916862487793
Epoch 448  Train loss: -0.656432237812117  Val loss: -1.0753839499352313
Epoch 449
-------------------------------
Batch 1/64 loss: -0.7638077735900879
Batch 2/64 loss: -0.8758773803710938
Batch 3/64 loss: -0.8555669784545898
Batch 4/64 loss: -0.8306007385253906
Batch 5/64 loss: -0.5546970367431641
Batch 6/64 loss: -0.7948493957519531
Batch 7/64 loss: -0.7909455299377441
Batch 8/64 loss: -0.7332210540771484
Batch 9/64 loss: -0.7011356353759766
Batch 10/64 loss: -0.7014641761779785
Batch 11/64 loss: -0.8439736366271973
Batch 12/64 loss: -0.7711105346679688
Batch 13/64 loss: 0.33874082565307617
Batch 14/64 loss: -0.701195240020752
Batch 15/64 loss: -0.7537093162536621
Batch 16/64 loss: -0.6904115676879883
Batch 17/64 loss: -0.8758530616760254
Batch 18/64 loss: -0.8206229209899902
Batch 19/64 loss: -0.7509045600891113
Batch 20/64 loss: -0.9778661727905273
Batch 21/64 loss: -0.8131284713745117
Batch 22/64 loss: -0.38762855529785156
Batch 23/64 loss: 0.2855043411254883
Batch 24/64 loss: -0.7748608589172363
Batch 25/64 loss: -0.49746274948120117
Batch 26/64 loss: -0.7691855430603027
Batch 27/64 loss: -0.7509236335754395
Batch 28/64 loss: -0.14700078964233398
Batch 29/64 loss: -0.5708651542663574
Batch 30/64 loss: -0.7113842964172363
Batch 31/64 loss: -0.8213672637939453
Batch 32/64 loss: -0.6032195091247559
Batch 33/64 loss: -0.7011685371398926
Batch 34/64 loss: -0.7710666656494141
Batch 35/64 loss: -0.6781373023986816
Batch 36/64 loss: -0.8435192108154297
Batch 37/64 loss: -0.7526001930236816
Batch 38/64 loss: -0.6448979377746582
Batch 39/64 loss: -0.8010258674621582
Batch 40/64 loss: -0.8246769905090332
Batch 41/64 loss: -0.7568416595458984
Batch 42/64 loss: -0.8553895950317383
Batch 43/64 loss: -0.6473584175109863
Batch 44/64 loss: -0.7613368034362793
Batch 45/64 loss: -0.6774406433105469
Batch 46/64 loss: -0.791414737701416
Batch 47/64 loss: -0.8530106544494629
Batch 48/64 loss: -0.7261433601379395
Batch 49/64 loss: -0.6282320022583008
Batch 50/64 loss: -0.8192906379699707
Batch 51/64 loss: -0.8667635917663574
Batch 52/64 loss: -0.900665283203125
Batch 53/64 loss: -0.5927858352661133
Batch 54/64 loss: -0.3261079788208008
Batch 55/64 loss: -0.5412182807922363
Batch 56/64 loss: -0.3488173484802246
Batch 57/64 loss: 0.7992372512817383
Batch 58/64 loss: -0.8846879005432129
Batch 59/64 loss: -0.7887544631958008
Batch 60/64 loss: -0.7294764518737793
Batch 61/64 loss: -0.7868051528930664
Batch 62/64 loss: -0.7981786727905273
Batch 63/64 loss: -0.37894201278686523
Batch 64/64 loss: -4.404307842254639
Epoch 449  Train loss: -0.7057465441086713  Val loss: -1.088021452074608
Epoch 450
-------------------------------
Batch 1/64 loss: -0.7610001564025879
Batch 2/64 loss: -0.7333307266235352
Batch 3/64 loss: -0.7703194618225098
Batch 4/64 loss: -0.7087626457214355
Batch 5/64 loss: -0.7652487754821777
Batch 6/64 loss: -0.8949651718139648
Batch 7/64 loss: -0.7162280082702637
Batch 8/64 loss: 0.8413591384887695
Batch 9/64 loss: -0.5898385047912598
Batch 10/64 loss: -0.9052114486694336
Batch 11/64 loss: -0.40796613693237305
Batch 12/64 loss: -0.773582935333252
Batch 13/64 loss: -0.757899284362793
Batch 14/64 loss: -0.7778224945068359
Batch 15/64 loss: -0.7028141021728516
Batch 16/64 loss: -0.7017688751220703
Batch 17/64 loss: -0.6512932777404785
Batch 18/64 loss: -0.7094345092773438
Batch 19/64 loss: -0.609954833984375
Batch 20/64 loss: -0.7918529510498047
Batch 21/64 loss: -0.8358278274536133
Batch 22/64 loss: -0.7764010429382324
Batch 23/64 loss: -0.7958254814147949
Batch 24/64 loss: -0.8326702117919922
Batch 25/64 loss: -0.8484163284301758
Batch 26/64 loss: -0.6397390365600586
Batch 27/64 loss: -0.21975135803222656
Batch 28/64 loss: -0.7251195907592773
Batch 29/64 loss: -0.8300600051879883
Batch 30/64 loss: -0.7236604690551758
Batch 31/64 loss: -0.5886363983154297
Batch 32/64 loss: -0.8499670028686523
Batch 33/64 loss: -0.6859450340270996
Batch 34/64 loss: -0.8774971961975098
Batch 35/64 loss: -0.7027101516723633
Batch 36/64 loss: -0.6387152671813965
Batch 37/64 loss: -0.2732377052307129
Batch 38/64 loss: 0.35674238204956055
Batch 39/64 loss: -0.21452665328979492
Batch 40/64 loss: -0.9801344871520996
Batch 41/64 loss: -0.6631665229797363
Batch 42/64 loss: -0.9280247688293457
Batch 43/64 loss: -0.8064661026000977
Batch 44/64 loss: -0.904451847076416
Batch 45/64 loss: -0.32308435440063477
Batch 46/64 loss: -0.8698883056640625
Batch 47/64 loss: -0.7407417297363281
Batch 48/64 loss: -0.6887674331665039
Batch 49/64 loss: -0.5762357711791992
Batch 50/64 loss: -0.7773995399475098
Batch 51/64 loss: -0.9238386154174805
Batch 52/64 loss: 0.12012290954589844
Batch 53/64 loss: -0.7667059898376465
Batch 54/64 loss: -0.8178963661193848
Batch 55/64 loss: -0.8553643226623535
Batch 56/64 loss: -0.776728630065918
Batch 57/64 loss: -0.976811408996582
Batch 58/64 loss: -0.969088077545166
Batch 59/64 loss: -0.6032676696777344
Batch 60/64 loss: -0.8201389312744141
Batch 61/64 loss: -0.8824472427368164
Batch 62/64 loss: -0.9590191841125488
Batch 63/64 loss: -0.933804988861084
Batch 64/64 loss: -4.600203990936279
Epoch 450  Train loss: -0.7288376770767511  Val loss: -1.13028648874604
Epoch 451
-------------------------------
Batch 1/64 loss: -0.9754676818847656
Batch 2/64 loss: -0.8801436424255371
Batch 3/64 loss: 0.4010953903198242
Batch 4/64 loss: -0.4523587226867676
Batch 5/64 loss: -0.849858283996582
Batch 6/64 loss: -0.8001294136047363
Batch 7/64 loss: -0.917694091796875
Batch 8/64 loss: 0.12108135223388672
Batch 9/64 loss: -0.25007057189941406
Batch 10/64 loss: -0.7683277130126953
Batch 11/64 loss: -0.8882918357849121
Batch 12/64 loss: -0.909855842590332
Batch 13/64 loss: -0.13542556762695312
Batch 14/64 loss: -0.9852705001831055
Batch 15/64 loss: -0.7426233291625977
Batch 16/64 loss: -0.42267465591430664
Batch 17/64 loss: -0.8742609024047852
Batch 18/64 loss: -0.8240351676940918
Batch 19/64 loss: -0.6231422424316406
Batch 20/64 loss: -0.8225669860839844
Batch 21/64 loss: -0.8189253807067871
Batch 22/64 loss: -0.746516227722168
Batch 23/64 loss: -0.5362086296081543
Batch 24/64 loss: -0.8327236175537109
Batch 25/64 loss: -0.8713474273681641
Batch 26/64 loss: -0.7645196914672852
Batch 27/64 loss: -0.8867926597595215
Batch 28/64 loss: -0.7822322845458984
Batch 29/64 loss: -0.8666243553161621
Batch 30/64 loss: -0.686011791229248
Batch 31/64 loss: -0.604121208190918
Batch 32/64 loss: -0.9053826332092285
Batch 33/64 loss: -0.5098938941955566
Batch 34/64 loss: -0.8076200485229492
Batch 35/64 loss: -0.6713161468505859
Batch 36/64 loss: -0.4349174499511719
Batch 37/64 loss: -0.7398438453674316
Batch 38/64 loss: -0.8069238662719727
Batch 39/64 loss: -0.6819443702697754
Batch 40/64 loss: -0.7571954727172852
Batch 41/64 loss: -0.9672565460205078
Batch 42/64 loss: -0.7734527587890625
Batch 43/64 loss: -0.9334449768066406
Batch 44/64 loss: -0.6646194458007812
Batch 45/64 loss: -0.8215198516845703
Batch 46/64 loss: 0.6712346076965332
Batch 47/64 loss: -0.8157057762145996
Batch 48/64 loss: -0.8575520515441895
Batch 49/64 loss: -0.7737507820129395
Batch 50/64 loss: -0.693967342376709
Batch 51/64 loss: -0.8350567817687988
Batch 52/64 loss: -0.8182005882263184
Batch 53/64 loss: -0.8486099243164062
Batch 54/64 loss: -0.9060606956481934
Batch 55/64 loss: -0.5863537788391113
Batch 56/64 loss: -0.7044029235839844
Batch 57/64 loss: -0.7265477180480957
Batch 58/64 loss: -0.893700122833252
Batch 59/64 loss: -0.824127197265625
Batch 60/64 loss: -0.7611908912658691
Batch 61/64 loss: -0.7788782119750977
Batch 62/64 loss: -0.9806389808654785
Batch 63/64 loss: -0.7688174247741699
Batch 64/64 loss: -4.219111919403076
Epoch 451  Train loss: -0.7456946746975768  Val loss: -1.1540578861826474
Epoch 452
-------------------------------
Batch 1/64 loss: -0.6430764198303223
Batch 2/64 loss: -0.8761477470397949
Batch 3/64 loss: -0.2980952262878418
Batch 4/64 loss: -0.8099789619445801
Batch 5/64 loss: -0.8284912109375
Batch 6/64 loss: -0.7996459007263184
Batch 7/64 loss: -0.8732280731201172
Batch 8/64 loss: 0.21280670166015625
Batch 9/64 loss: -0.8293333053588867
Batch 10/64 loss: -0.6932191848754883
Batch 11/64 loss: -0.8670563697814941
Batch 12/64 loss: -0.7875156402587891
Batch 13/64 loss: -0.6020979881286621
Batch 14/64 loss: 0.38323163986206055
Batch 15/64 loss: -0.7700386047363281
Batch 16/64 loss: -0.8686699867248535
Batch 17/64 loss: -0.8107151985168457
Batch 18/64 loss: -0.8535523414611816
Batch 19/64 loss: -0.8557476997375488
Batch 20/64 loss: -0.8073039054870605
Batch 21/64 loss: -0.8963198661804199
Batch 22/64 loss: -0.9060878753662109
Batch 23/64 loss: -0.3321385383605957
Batch 24/64 loss: -0.9668059349060059
Batch 25/64 loss: -0.9071722030639648
Batch 26/64 loss: -0.8537359237670898
Batch 27/64 loss: -0.9937505722045898
Batch 28/64 loss: -0.8075394630432129
Batch 29/64 loss: -0.8763341903686523
Batch 30/64 loss: 0.6748476028442383
Batch 31/64 loss: -0.758629322052002
Batch 32/64 loss: -0.7714705467224121
Batch 33/64 loss: -0.841618537902832
Batch 34/64 loss: -0.8351049423217773
Batch 35/64 loss: -0.7388439178466797
Batch 36/64 loss: -0.8180885314941406
Batch 37/64 loss: -0.8896245956420898
Batch 38/64 loss: -0.8255462646484375
Batch 39/64 loss: -0.7537875175476074
Batch 40/64 loss: -0.918243408203125
Batch 41/64 loss: -0.27673864364624023
Batch 42/64 loss: -0.9605083465576172
Batch 43/64 loss: -0.7076845169067383
Batch 44/64 loss: -0.7980141639709473
Batch 45/64 loss: -0.8555622100830078
Batch 46/64 loss: -0.8773913383483887
Batch 47/64 loss: -0.8244071006774902
Batch 48/64 loss: -0.8209762573242188
Batch 49/64 loss: -0.634824275970459
Batch 50/64 loss: -0.6145062446594238
Batch 51/64 loss: -0.7624244689941406
Batch 52/64 loss: -0.7673697471618652
Batch 53/64 loss: -0.38787841796875
Batch 54/64 loss: -0.8554081916809082
Batch 55/64 loss: -0.7888083457946777
Batch 56/64 loss: -0.44193315505981445
Batch 57/64 loss: -0.8085746765136719
Batch 58/64 loss: -0.7766966819763184
Batch 59/64 loss: -0.7770757675170898
Batch 60/64 loss: -0.7326021194458008
Batch 61/64 loss: -0.9388837814331055
Batch 62/64 loss: -0.6361017227172852
Batch 63/64 loss: -0.580540657043457
Batch 64/64 loss: -4.492150783538818
Epoch 452  Train loss: -0.7574571590797574  Val loss: -1.121600042913378
Epoch 453
-------------------------------
Batch 1/64 loss: -0.7554035186767578
Batch 2/64 loss: -0.9176502227783203
Batch 3/64 loss: -0.8368539810180664
Batch 4/64 loss: -0.7867960929870605
Batch 5/64 loss: -0.7768468856811523
Batch 6/64 loss: 0.18028926849365234
Batch 7/64 loss: -0.5332255363464355
Batch 8/64 loss: -0.7551889419555664
Batch 9/64 loss: 0.8665757179260254
Batch 10/64 loss: -0.8696126937866211
Batch 11/64 loss: -0.7208118438720703
Batch 12/64 loss: -0.7697896957397461
Batch 13/64 loss: -0.7152256965637207
Batch 14/64 loss: -0.7334833145141602
Batch 15/64 loss: -0.857154369354248
Batch 16/64 loss: -0.9412941932678223
Batch 17/64 loss: -0.7751736640930176
Batch 18/64 loss: -0.7922401428222656
Batch 19/64 loss: -0.8840513229370117
Batch 20/64 loss: -0.8244991302490234
Batch 21/64 loss: -0.9255619049072266
Batch 22/64 loss: -0.5031399726867676
Batch 23/64 loss: -0.816131591796875
Batch 24/64 loss: -0.903745174407959
Batch 25/64 loss: -0.8449974060058594
Batch 26/64 loss: -0.8041234016418457
Batch 27/64 loss: -0.8564538955688477
Batch 28/64 loss: -0.8590078353881836
Batch 29/64 loss: -0.7885708808898926
Batch 30/64 loss: -0.8803434371948242
Batch 31/64 loss: -0.9504609107971191
Batch 32/64 loss: -0.9646940231323242
Batch 33/64 loss: -0.9005732536315918
Batch 34/64 loss: -0.5689101219177246
Batch 35/64 loss: -0.773033618927002
Batch 36/64 loss: -0.7130084037780762
Batch 37/64 loss: -0.7828025817871094
Batch 38/64 loss: -0.8698382377624512
Batch 39/64 loss: -0.08364439010620117
Batch 40/64 loss: -0.28127479553222656
Batch 41/64 loss: -0.6759829521179199
Batch 42/64 loss: -0.7419638633728027
Batch 43/64 loss: -0.9154129028320312
Batch 44/64 loss: -0.8021888732910156
Batch 45/64 loss: -0.6474933624267578
Batch 46/64 loss: -0.6754131317138672
Batch 47/64 loss: -0.7789778709411621
Batch 48/64 loss: -0.7757072448730469
Batch 49/64 loss: -0.7217822074890137
Batch 50/64 loss: -0.6602215766906738
Batch 51/64 loss: 0.07020854949951172
Batch 52/64 loss: -0.8084354400634766
Batch 53/64 loss: -0.5522527694702148
Batch 54/64 loss: -0.8096156120300293
Batch 55/64 loss: -0.7162704467773438
Batch 56/64 loss: -0.7724018096923828
Batch 57/64 loss: -0.552800178527832
Batch 58/64 loss: -0.6409516334533691
Batch 59/64 loss: -0.691831111907959
Batch 60/64 loss: 0.38314390182495117
Batch 61/64 loss: -0.8103961944580078
Batch 62/64 loss: -0.6499443054199219
Batch 63/64 loss: -0.6287884712219238
Batch 64/64 loss: -4.221274375915527
Epoch 453  Train loss: -0.7217284296073165  Val loss: -1.1098442601993732
Epoch 454
-------------------------------
Batch 1/64 loss: -0.7270779609680176
Batch 2/64 loss: -0.768702507019043
Batch 3/64 loss: -0.7628393173217773
Batch 4/64 loss: -0.6396064758300781
Batch 5/64 loss: -0.585113525390625
Batch 6/64 loss: -0.8182239532470703
Batch 7/64 loss: -0.5573744773864746
Batch 8/64 loss: -0.8296413421630859
Batch 9/64 loss: -0.6893353462219238
Batch 10/64 loss: -0.7278890609741211
Batch 11/64 loss: -0.662592887878418
Batch 12/64 loss: -0.731419563293457
Batch 13/64 loss: 0.2318592071533203
Batch 14/64 loss: -0.710747241973877
Batch 15/64 loss: -0.8362030982971191
Batch 16/64 loss: -0.597139835357666
Batch 17/64 loss: -0.7441525459289551
Batch 18/64 loss: -0.8696746826171875
Batch 19/64 loss: -0.45156431198120117
Batch 20/64 loss: -0.6932063102722168
Batch 21/64 loss: -0.7158327102661133
Batch 22/64 loss: -0.7314739227294922
Batch 23/64 loss: -0.7553906440734863
Batch 24/64 loss: 0.02190542221069336
Batch 25/64 loss: -0.15001392364501953
Batch 26/64 loss: -0.7483491897583008
Batch 27/64 loss: -0.8004064559936523
Batch 28/64 loss: -0.7269430160522461
Batch 29/64 loss: -0.3553495407104492
Batch 30/64 loss: -0.8024835586547852
Batch 31/64 loss: -0.8781766891479492
Batch 32/64 loss: -0.7350926399230957
Batch 33/64 loss: -0.649390697479248
Batch 34/64 loss: -0.7193436622619629
Batch 35/64 loss: -0.6623172760009766
Batch 36/64 loss: -0.7942037582397461
Batch 37/64 loss: -0.7785234451293945
Batch 38/64 loss: -0.6247272491455078
Batch 39/64 loss: -0.5104165077209473
Batch 40/64 loss: -0.8545589447021484
Batch 41/64 loss: -0.7866430282592773
Batch 42/64 loss: -0.8226876258850098
Batch 43/64 loss: -0.8382411003112793
Batch 44/64 loss: -0.8876285552978516
Batch 45/64 loss: -0.8642082214355469
Batch 46/64 loss: -0.5932846069335938
Batch 47/64 loss: -0.8000059127807617
Batch 48/64 loss: -0.7288589477539062
Batch 49/64 loss: -0.6445736885070801
Batch 50/64 loss: -0.8468098640441895
Batch 51/64 loss: -0.8831090927124023
Batch 52/64 loss: 0.7348499298095703
Batch 53/64 loss: -0.948554515838623
Batch 54/64 loss: -0.8302798271179199
Batch 55/64 loss: -0.6899595260620117
Batch 56/64 loss: -0.8840045928955078
Batch 57/64 loss: -0.8818635940551758
Batch 58/64 loss: -0.6113519668579102
Batch 59/64 loss: 1.092355728149414
Batch 60/64 loss: -0.7323627471923828
Batch 61/64 loss: -0.7077836990356445
Batch 62/64 loss: -0.7006549835205078
Batch 63/64 loss: -0.8405604362487793
Batch 64/64 loss: -4.265964031219482
Epoch 454  Train loss: -0.690783177170099  Val loss: -0.9880178785815681
Epoch 455
-------------------------------
Batch 1/64 loss: -0.6618471145629883
Batch 2/64 loss: -0.7422456741333008
Batch 3/64 loss: -0.7394847869873047
Batch 4/64 loss: -0.8610372543334961
Batch 5/64 loss: -0.8081212043762207
Batch 6/64 loss: -0.4860525131225586
Batch 7/64 loss: -0.7130422592163086
Batch 8/64 loss: -0.8281044960021973
Batch 9/64 loss: -0.7588300704956055
Batch 10/64 loss: -0.7182722091674805
Batch 11/64 loss: -0.38202953338623047
Batch 12/64 loss: -0.7068185806274414
Batch 13/64 loss: -0.6893348693847656
Batch 14/64 loss: 0.5269303321838379
Batch 15/64 loss: -0.7766737937927246
Batch 16/64 loss: -0.4684467315673828
Batch 17/64 loss: 0.3713526725769043
Batch 18/64 loss: -0.31735801696777344
Batch 19/64 loss: -0.6878376007080078
Batch 20/64 loss: -0.6904091835021973
Batch 21/64 loss: -0.5999832153320312
Batch 22/64 loss: -0.698300838470459
Batch 23/64 loss: -0.6658625602722168
Batch 24/64 loss: -0.6033620834350586
Batch 25/64 loss: -0.5589475631713867
Batch 26/64 loss: -0.7598543167114258
Batch 27/64 loss: -0.6649360656738281
Batch 28/64 loss: -0.8328900337219238
Batch 29/64 loss: -0.650115966796875
Batch 30/64 loss: -0.6700501441955566
Batch 31/64 loss: -0.6636338233947754
Batch 32/64 loss: -0.7386050224304199
Batch 33/64 loss: -0.6652083396911621
Batch 34/64 loss: -0.7247872352600098
Batch 35/64 loss: -0.6214971542358398
Batch 36/64 loss: -0.3799710273742676
Batch 37/64 loss: -0.7092313766479492
Batch 38/64 loss: -0.6851143836975098
Batch 39/64 loss: -0.7709980010986328
Batch 40/64 loss: -0.8622956275939941
Batch 41/64 loss: -0.7003393173217773
Batch 42/64 loss: -0.8040571212768555
Batch 43/64 loss: -0.642179012298584
Batch 44/64 loss: -0.6893501281738281
Batch 45/64 loss: -0.771695613861084
Batch 46/64 loss: -0.7742018699645996
Batch 47/64 loss: -0.614494800567627
Batch 48/64 loss: -0.7246661186218262
Batch 49/64 loss: -0.4951338768005371
Batch 50/64 loss: -0.34912633895874023
Batch 51/64 loss: -0.7771797180175781
Batch 52/64 loss: 0.6398797035217285
Batch 53/64 loss: -0.8026113510131836
Batch 54/64 loss: -0.666172981262207
Batch 55/64 loss: -0.8360657691955566
Batch 56/64 loss: -0.890683650970459
Batch 57/64 loss: -0.7743344306945801
Batch 58/64 loss: -0.2832150459289551
Batch 59/64 loss: -0.8852276802062988
Batch 60/64 loss: 0.3292102813720703
Batch 61/64 loss: -0.6085004806518555
Batch 62/64 loss: -0.8572998046875
Batch 63/64 loss: -0.7603116035461426
Batch 64/64 loss: -4.450504779815674
Epoch 455  Train loss: -0.6547285173453536  Val loss: -1.1077255432548392
Epoch 456
-------------------------------
Batch 1/64 loss: -0.7432241439819336
Batch 2/64 loss: -0.2414412498474121
Batch 3/64 loss: -0.45262765884399414
Batch 4/64 loss: -0.8478946685791016
Batch 5/64 loss: -0.688840389251709
Batch 6/64 loss: -0.6898231506347656
Batch 7/64 loss: -0.7887058258056641
Batch 8/64 loss: -0.5925188064575195
Batch 9/64 loss: -0.5248785018920898
Batch 10/64 loss: -0.37924623489379883
Batch 11/64 loss: -0.4888272285461426
Batch 12/64 loss: -0.618647575378418
Batch 13/64 loss: -0.7839059829711914
Batch 14/64 loss: -0.6137938499450684
Batch 15/64 loss: -0.8011445999145508
Batch 16/64 loss: -0.7459311485290527
Batch 17/64 loss: -0.8430747985839844
Batch 18/64 loss: -0.275482177734375
Batch 19/64 loss: -0.7040586471557617
Batch 20/64 loss: -0.7826581001281738
Batch 21/64 loss: -0.6740431785583496
Batch 22/64 loss: -0.756415843963623
Batch 23/64 loss: -0.8021411895751953
Batch 24/64 loss: -0.8456439971923828
Batch 25/64 loss: -0.8453927040100098
Batch 26/64 loss: -0.7452807426452637
Batch 27/64 loss: -0.5877766609191895
Batch 28/64 loss: -0.6940250396728516
Batch 29/64 loss: -0.788907527923584
Batch 30/64 loss: -0.7224979400634766
Batch 31/64 loss: -0.8150515556335449
Batch 32/64 loss: -0.6058449745178223
Batch 33/64 loss: -0.6976404190063477
Batch 34/64 loss: -0.6063637733459473
Batch 35/64 loss: 0.42759084701538086
Batch 36/64 loss: -0.7680444717407227
Batch 37/64 loss: -0.794644832611084
Batch 38/64 loss: -0.6899204254150391
Batch 39/64 loss: -0.5383257865905762
Batch 40/64 loss: -0.7112007141113281
Batch 41/64 loss: -0.8082919120788574
Batch 42/64 loss: -0.6917462348937988
Batch 43/64 loss: -0.8634195327758789
Batch 44/64 loss: -0.8659095764160156
Batch 45/64 loss: -0.5997004508972168
Batch 46/64 loss: -0.8609580993652344
Batch 47/64 loss: -0.6630558967590332
Batch 48/64 loss: -0.5372180938720703
Batch 49/64 loss: -0.7789206504821777
Batch 50/64 loss: 0.7704744338989258
Batch 51/64 loss: -0.8296184539794922
Batch 52/64 loss: -0.6723804473876953
Batch 53/64 loss: -0.7542862892150879
Batch 54/64 loss: 0.41183042526245117
Batch 55/64 loss: -0.7141985893249512
Batch 56/64 loss: -0.7458181381225586
Batch 57/64 loss: -0.7109107971191406
Batch 58/64 loss: -0.6935091018676758
Batch 59/64 loss: -0.6153936386108398
Batch 60/64 loss: -0.26706790924072266
Batch 61/64 loss: -0.7744383811950684
Batch 62/64 loss: -0.8257298469543457
Batch 63/64 loss: -0.8286938667297363
Batch 64/64 loss: -4.3909711837768555
Epoch 456  Train loss: -0.67577231538062  Val loss: -1.056511108817923
Epoch 457
-------------------------------
Batch 1/64 loss: -0.7020387649536133
Batch 2/64 loss: -0.1264204978942871
Batch 3/64 loss: -0.8046059608459473
Batch 4/64 loss: -0.7730817794799805
Batch 5/64 loss: -0.8617153167724609
Batch 6/64 loss: -0.7410140037536621
Batch 7/64 loss: -0.7224187850952148
Batch 8/64 loss: -0.566009521484375
Batch 9/64 loss: -0.8912243843078613
Batch 10/64 loss: -0.7936697006225586
Batch 11/64 loss: -0.7908782958984375
Batch 12/64 loss: -0.8006405830383301
Batch 13/64 loss: -0.35273122787475586
Batch 14/64 loss: -0.6892356872558594
Batch 15/64 loss: -0.6564035415649414
Batch 16/64 loss: -0.9338150024414062
Batch 17/64 loss: -0.8783740997314453
Batch 18/64 loss: -0.6707158088684082
Batch 19/64 loss: -0.8996834754943848
Batch 20/64 loss: -0.8426804542541504
Batch 21/64 loss: 0.38763427734375
Batch 22/64 loss: 0.24498558044433594
Batch 23/64 loss: -0.9901175498962402
Batch 24/64 loss: -0.7700505256652832
Batch 25/64 loss: -0.6246037483215332
Batch 26/64 loss: -0.7340254783630371
Batch 27/64 loss: -0.5054616928100586
Batch 28/64 loss: -0.5334410667419434
Batch 29/64 loss: -0.24923133850097656
Batch 30/64 loss: -0.7685952186584473
Batch 31/64 loss: -0.7980046272277832
Batch 32/64 loss: -0.7500166893005371
Batch 33/64 loss: -0.6493349075317383
Batch 34/64 loss: -0.8995256423950195
Batch 35/64 loss: -0.7232241630554199
Batch 36/64 loss: -0.584683895111084
Batch 37/64 loss: -0.8161892890930176
Batch 38/64 loss: 0.7200379371643066
Batch 39/64 loss: -0.8381752967834473
Batch 40/64 loss: -0.6298699378967285
Batch 41/64 loss: -0.5299091339111328
Batch 42/64 loss: -0.8928112983703613
Batch 43/64 loss: -0.818793773651123
Batch 44/64 loss: 0.1619405746459961
Batch 45/64 loss: -0.6822800636291504
Batch 46/64 loss: -0.8112850189208984
Batch 47/64 loss: -0.9055547714233398
Batch 48/64 loss: -0.7190389633178711
Batch 49/64 loss: -0.3361239433288574
Batch 50/64 loss: -0.8145675659179688
Batch 51/64 loss: -0.5779147148132324
Batch 52/64 loss: -0.6937150955200195
Batch 53/64 loss: -0.8372554779052734
Batch 54/64 loss: -0.8154268264770508
Batch 55/64 loss: -0.8085131645202637
Batch 56/64 loss: -0.8033542633056641
Batch 57/64 loss: -0.8509998321533203
Batch 58/64 loss: -0.6162052154541016
Batch 59/64 loss: -0.7026848793029785
Batch 60/64 loss: -0.6735310554504395
Batch 61/64 loss: -0.8211236000061035
Batch 62/64 loss: -0.7550482749938965
Batch 63/64 loss: -0.806941032409668
Batch 64/64 loss: -4.451661586761475
Epoch 457  Train loss: -0.6973981763802323  Val loss: -0.9820306915597817
Epoch 458
-------------------------------
Batch 1/64 loss: -0.9171948432922363
Batch 2/64 loss: -0.7219944000244141
Batch 3/64 loss: -0.891629695892334
Batch 4/64 loss: -0.7097434997558594
Batch 5/64 loss: -0.6833710670471191
Batch 6/64 loss: -0.7154264450073242
Batch 7/64 loss: -0.5145249366760254
Batch 8/64 loss: -0.6800436973571777
Batch 9/64 loss: -0.6161055564880371
Batch 10/64 loss: -0.5016899108886719
Batch 11/64 loss: -0.7321987152099609
Batch 12/64 loss: -0.8810868263244629
Batch 13/64 loss: -0.7785372734069824
Batch 14/64 loss: -0.34622812271118164
Batch 15/64 loss: -0.8998756408691406
Batch 16/64 loss: 0.7530965805053711
Batch 17/64 loss: -0.7115688323974609
Batch 18/64 loss: 0.34238195419311523
Batch 19/64 loss: -0.7467207908630371
Batch 20/64 loss: -0.6623344421386719
Batch 21/64 loss: -0.728238582611084
Batch 22/64 loss: 0.4376959800720215
Batch 23/64 loss: -0.7827882766723633
Batch 24/64 loss: -0.8873395919799805
Batch 25/64 loss: -0.38242197036743164
Batch 26/64 loss: -0.6352381706237793
Batch 27/64 loss: -0.7758984565734863
Batch 28/64 loss: -0.2926187515258789
Batch 29/64 loss: -0.8126025199890137
Batch 30/64 loss: -0.8104991912841797
Batch 31/64 loss: -0.6412811279296875
Batch 32/64 loss: -0.880640983581543
Batch 33/64 loss: -0.4953036308288574
Batch 34/64 loss: -0.6572299003601074
Batch 35/64 loss: -0.677788257598877
Batch 36/64 loss: -0.1803455352783203
Batch 37/64 loss: -0.8131856918334961
Batch 38/64 loss: -0.6805768013000488
Batch 39/64 loss: -0.600095272064209
Batch 40/64 loss: -0.5164642333984375
Batch 41/64 loss: -0.7807583808898926
Batch 42/64 loss: -0.7983417510986328
Batch 43/64 loss: -0.7668266296386719
Batch 44/64 loss: -0.7978596687316895
Batch 45/64 loss: -0.7239108085632324
Batch 46/64 loss: -0.6886224746704102
Batch 47/64 loss: -0.8096709251403809
Batch 48/64 loss: -0.707366943359375
Batch 49/64 loss: -0.8176989555358887
Batch 50/64 loss: -0.8395256996154785
Batch 51/64 loss: -0.7509541511535645
Batch 52/64 loss: -0.35564756393432617
Batch 53/64 loss: -0.8401093482971191
Batch 54/64 loss: -0.7793612480163574
Batch 55/64 loss: -0.8687119483947754
Batch 56/64 loss: -0.7554326057434082
Batch 57/64 loss: -0.6451992988586426
Batch 58/64 loss: -0.7923073768615723
Batch 59/64 loss: -0.7929191589355469
Batch 60/64 loss: -0.5404524803161621
Batch 61/64 loss: -0.3180985450744629
Batch 62/64 loss: -0.38968324661254883
Batch 63/64 loss: -0.8722548484802246
Batch 64/64 loss: -4.410597801208496
Epoch 458  Train loss: -0.6771344240973978  Val loss: -1.100972598360986
Epoch 459
-------------------------------
Batch 1/64 loss: -0.7943286895751953
Batch 2/64 loss: -0.6796302795410156
Batch 3/64 loss: -0.8048162460327148
Batch 4/64 loss: -0.7770648002624512
Batch 5/64 loss: -0.8766231536865234
Batch 6/64 loss: -0.674889087677002
Batch 7/64 loss: -0.6804494857788086
Batch 8/64 loss: -0.5062808990478516
Batch 9/64 loss: -0.4933485984802246
Batch 10/64 loss: -0.842402458190918
Batch 11/64 loss: -0.7985944747924805
Batch 12/64 loss: -0.7671113014221191
Batch 13/64 loss: -0.7859334945678711
Batch 14/64 loss: -0.7230854034423828
Batch 15/64 loss: -0.8327279090881348
Batch 16/64 loss: -0.6720190048217773
Batch 17/64 loss: -0.7447538375854492
Batch 18/64 loss: -0.7423281669616699
Batch 19/64 loss: -0.8281078338623047
Batch 20/64 loss: -0.7680034637451172
Batch 21/64 loss: -0.6616473197937012
Batch 22/64 loss: -0.7585630416870117
Batch 23/64 loss: -0.5937848091125488
Batch 24/64 loss: -0.7202415466308594
Batch 25/64 loss: -0.01661539077758789
Batch 26/64 loss: -0.7704391479492188
Batch 27/64 loss: -0.5099725723266602
Batch 28/64 loss: -0.7766013145446777
Batch 29/64 loss: -0.8016490936279297
Batch 30/64 loss: -0.9296283721923828
Batch 31/64 loss: -0.34484434127807617
Batch 32/64 loss: -0.7678074836730957
Batch 33/64 loss: -0.7343864440917969
Batch 34/64 loss: -0.9075846672058105
Batch 35/64 loss: -0.6029448509216309
Batch 36/64 loss: -0.7183418273925781
Batch 37/64 loss: -0.8547701835632324
Batch 38/64 loss: -0.6157927513122559
Batch 39/64 loss: -0.830233097076416
Batch 40/64 loss: -0.6832318305969238
Batch 41/64 loss: 0.39400291442871094
Batch 42/64 loss: -0.8047275543212891
Batch 43/64 loss: -0.8708682060241699
Batch 44/64 loss: -0.7400445938110352
Batch 45/64 loss: -0.6404423713684082
Batch 46/64 loss: -0.6092758178710938
Batch 47/64 loss: 0.18097734451293945
Batch 48/64 loss: -0.7733259201049805
Batch 49/64 loss: -0.6710171699523926
Batch 50/64 loss: -0.5068984031677246
Batch 51/64 loss: -0.7975893020629883
Batch 52/64 loss: -0.8769464492797852
Batch 53/64 loss: 0.7393536567687988
Batch 54/64 loss: -0.38260364532470703
Batch 55/64 loss: -0.7936124801635742
Batch 56/64 loss: -0.8232932090759277
Batch 57/64 loss: -0.7602109909057617
Batch 58/64 loss: -0.6504650115966797
Batch 59/64 loss: -0.5722708702087402
Batch 60/64 loss: -0.7150402069091797
Batch 61/64 loss: -0.8519988059997559
Batch 62/64 loss: -0.8289523124694824
Batch 63/64 loss: -0.20028924942016602
Batch 64/64 loss: -4.4036335945129395
Epoch 459  Train loss: -0.6941151768553491  Val loss: -0.9407312321089387
Epoch 460
-------------------------------
Batch 1/64 loss: -0.713134765625
Batch 2/64 loss: -0.6493916511535645
Batch 3/64 loss: -0.811427116394043
Batch 4/64 loss: 0.9326481819152832
Batch 5/64 loss: -0.7514858245849609
Batch 6/64 loss: -0.7214760780334473
Batch 7/64 loss: -0.7609853744506836
Batch 8/64 loss: -0.9214558601379395
Batch 9/64 loss: -0.6774168014526367
Batch 10/64 loss: -0.21854686737060547
Batch 11/64 loss: -0.5038928985595703
Batch 12/64 loss: -0.8082427978515625
Batch 13/64 loss: 0.5076055526733398
Batch 14/64 loss: -0.6571946144104004
Batch 15/64 loss: -0.3824343681335449
Batch 16/64 loss: -0.7330360412597656
Batch 17/64 loss: -0.7148518562316895
Batch 18/64 loss: -0.8158321380615234
Batch 19/64 loss: -0.8081521987915039
Batch 20/64 loss: -0.5392827987670898
Batch 21/64 loss: -0.0782175064086914
Batch 22/64 loss: -0.520714282989502
Batch 23/64 loss: -0.7445063591003418
Batch 24/64 loss: -0.7589888572692871
Batch 25/64 loss: -0.78521728515625
Batch 26/64 loss: -0.7938261032104492
Batch 27/64 loss: -0.5543045997619629
Batch 28/64 loss: -0.6294722557067871
Batch 29/64 loss: -0.6808552742004395
Batch 30/64 loss: -0.749992847442627
Batch 31/64 loss: -0.7712750434875488
Batch 32/64 loss: -0.6565508842468262
Batch 33/64 loss: -0.8257360458374023
Batch 34/64 loss: -0.8033747673034668
Batch 35/64 loss: -0.8223657608032227
Batch 36/64 loss: -0.6647152900695801
Batch 37/64 loss: -0.6696386337280273
Batch 38/64 loss: -0.7711858749389648
Batch 39/64 loss: -0.8086423873901367
Batch 40/64 loss: -0.664588451385498
Batch 41/64 loss: -0.6619071960449219
Batch 42/64 loss: -0.5264978408813477
Batch 43/64 loss: -0.8496174812316895
Batch 44/64 loss: -0.07743167877197266
Batch 45/64 loss: -0.614774227142334
Batch 46/64 loss: -0.6815519332885742
Batch 47/64 loss: -0.36272239685058594
Batch 48/64 loss: -0.7655525207519531
Batch 49/64 loss: -0.71209716796875
Batch 50/64 loss: -0.6928596496582031
Batch 51/64 loss: -0.6540260314941406
Batch 52/64 loss: 0.1745762825012207
Batch 53/64 loss: -0.7591276168823242
Batch 54/64 loss: -0.7613530158996582
Batch 55/64 loss: -0.6656689643859863
Batch 56/64 loss: -0.664466381072998
Batch 57/64 loss: -0.4112997055053711
Batch 58/64 loss: -0.8989605903625488
Batch 59/64 loss: -0.7396602630615234
Batch 60/64 loss: -0.7615547180175781
Batch 61/64 loss: -0.7161316871643066
Batch 62/64 loss: -0.4750704765319824
Batch 63/64 loss: -0.8511323928833008
Batch 64/64 loss: -4.449350833892822
Epoch 460  Train loss: -0.6583224408766802  Val loss: -0.9744769093097281
Epoch 461
-------------------------------
Batch 1/64 loss: -0.7765264511108398
Batch 2/64 loss: -0.7041301727294922
Batch 3/64 loss: -0.774693489074707
Batch 4/64 loss: -0.6976680755615234
Batch 5/64 loss: -0.5403180122375488
Batch 6/64 loss: -0.7446422576904297
Batch 7/64 loss: -0.6006512641906738
Batch 8/64 loss: -0.6828732490539551
Batch 9/64 loss: -0.7651028633117676
Batch 10/64 loss: -0.7021865844726562
Batch 11/64 loss: -0.6488676071166992
Batch 12/64 loss: -0.6717605590820312
Batch 13/64 loss: -0.8151841163635254
Batch 14/64 loss: -0.6125178337097168
Batch 15/64 loss: -0.7742118835449219
Batch 16/64 loss: -0.5644488334655762
Batch 17/64 loss: -0.7559680938720703
Batch 18/64 loss: -0.7804813385009766
Batch 19/64 loss: -0.6677899360656738
Batch 20/64 loss: -0.6433224678039551
Batch 21/64 loss: -0.45357704162597656
Batch 22/64 loss: -0.7442092895507812
Batch 23/64 loss: -0.7033510208129883
Batch 24/64 loss: -0.6704001426696777
Batch 25/64 loss: -0.8166351318359375
Batch 26/64 loss: -0.8625564575195312
Batch 27/64 loss: -0.729729175567627
Batch 28/64 loss: -0.8583998680114746
Batch 29/64 loss: -0.5951199531555176
Batch 30/64 loss: -0.8074631690979004
Batch 31/64 loss: -0.4929013252258301
Batch 32/64 loss: -0.7885193824768066
Batch 33/64 loss: -0.8673291206359863
Batch 34/64 loss: -0.4187326431274414
Batch 35/64 loss: -0.6946020126342773
Batch 36/64 loss: -0.6969890594482422
Batch 37/64 loss: -0.6912989616394043
Batch 38/64 loss: 0.6900744438171387
Batch 39/64 loss: -0.6648654937744141
Batch 40/64 loss: -0.4269118309020996
Batch 41/64 loss: -0.7266626358032227
Batch 42/64 loss: -0.7884297370910645
Batch 43/64 loss: -0.6790452003479004
Batch 44/64 loss: -0.2225341796875
Batch 45/64 loss: -0.63934326171875
Batch 46/64 loss: -0.837122917175293
Batch 47/64 loss: -0.7836284637451172
Batch 48/64 loss: 0.2778205871582031
Batch 49/64 loss: -0.884066104888916
Batch 50/64 loss: -0.8024182319641113
Batch 51/64 loss: -0.7353930473327637
Batch 52/64 loss: -0.8548445701599121
Batch 53/64 loss: -0.7955946922302246
Batch 54/64 loss: -0.6459126472473145
Batch 55/64 loss: -0.8024201393127441
Batch 56/64 loss: -0.6994218826293945
Batch 57/64 loss: -0.7838401794433594
Batch 58/64 loss: -0.797605037689209
Batch 59/64 loss: -0.8464102745056152
Batch 60/64 loss: -0.6301660537719727
Batch 61/64 loss: -0.26805686950683594
Batch 62/64 loss: -0.8576340675354004
Batch 63/64 loss: -0.7664389610290527
Batch 64/64 loss: -2.7163829803466797
Epoch 461  Train loss: -0.6874245886709176  Val loss: -1.0663007362601684
Epoch 462
-------------------------------
Batch 1/64 loss: -0.8156542778015137
Batch 2/64 loss: -0.7657327651977539
Batch 3/64 loss: -0.8089017868041992
Batch 4/64 loss: -0.8022942543029785
Batch 5/64 loss: -0.8467040061950684
Batch 6/64 loss: -0.8292732238769531
Batch 7/64 loss: -0.8372049331665039
Batch 8/64 loss: -0.8920927047729492
Batch 9/64 loss: -0.6159229278564453
Batch 10/64 loss: -0.8313660621643066
Batch 11/64 loss: -0.869624137878418
Batch 12/64 loss: -0.8338618278503418
Batch 13/64 loss: -0.5134410858154297
Batch 14/64 loss: -0.8141098022460938
Batch 15/64 loss: -0.9045414924621582
Batch 16/64 loss: -0.435910701751709
Batch 17/64 loss: -0.8070564270019531
Batch 18/64 loss: -0.7451772689819336
Batch 19/64 loss: -0.7832612991333008
Batch 20/64 loss: -0.7206077575683594
Batch 21/64 loss: -0.4700322151184082
Batch 22/64 loss: -0.8697967529296875
Batch 23/64 loss: -0.6781115531921387
Batch 24/64 loss: -0.8314375877380371
Batch 25/64 loss: -0.819915771484375
Batch 26/64 loss: -0.5939373970031738
Batch 27/64 loss: -0.834470272064209
Batch 28/64 loss: -0.8012375831604004
Batch 29/64 loss: -0.7139801979064941
Batch 30/64 loss: -0.6626472473144531
Batch 31/64 loss: 0.0055179595947265625
Batch 32/64 loss: -0.6074013710021973
Batch 33/64 loss: -0.7187919616699219
Batch 34/64 loss: 0.7569332122802734
Batch 35/64 loss: -0.9037909507751465
Batch 36/64 loss: -0.7082505226135254
Batch 37/64 loss: -0.5423688888549805
Batch 38/64 loss: -0.6263303756713867
Batch 39/64 loss: -0.15144634246826172
Batch 40/64 loss: 0.24239587783813477
Batch 41/64 loss: -0.7056312561035156
Batch 42/64 loss: -0.8580412864685059
Batch 43/64 loss: -0.5987401008605957
Batch 44/64 loss: -0.5160598754882812
Batch 45/64 loss: -0.7185173034667969
Batch 46/64 loss: -0.7091717720031738
Batch 47/64 loss: -0.6834378242492676
Batch 48/64 loss: -0.5880289077758789
Batch 49/64 loss: 0.38324451446533203
Batch 50/64 loss: -0.5433907508850098
Batch 51/64 loss: -0.45509910583496094
Batch 52/64 loss: -0.8184499740600586
Batch 53/64 loss: -0.6719603538513184
Batch 54/64 loss: -0.6812372207641602
Batch 55/64 loss: -0.3802165985107422
Batch 56/64 loss: -0.8533735275268555
Batch 57/64 loss: -0.8158888816833496
Batch 58/64 loss: -0.8663992881774902
Batch 59/64 loss: -0.6550211906433105
Batch 60/64 loss: -0.8783459663391113
Batch 61/64 loss: -0.6171207427978516
Batch 62/64 loss: -0.6929707527160645
Batch 63/64 loss: -0.8815903663635254
Batch 64/64 loss: -4.2711100578308105
Epoch 462  Train loss: -0.690362662895053  Val loss: -1.0385286521256174
Epoch 463
-------------------------------
Batch 1/64 loss: -0.7022032737731934
Batch 2/64 loss: -0.9221787452697754
Batch 3/64 loss: -0.8877110481262207
Batch 4/64 loss: -0.6860685348510742
Batch 5/64 loss: -0.9038233757019043
Batch 6/64 loss: -0.25992536544799805
Batch 7/64 loss: -0.9343228340148926
Batch 8/64 loss: -0.8627533912658691
Batch 9/64 loss: -0.7209019660949707
Batch 10/64 loss: -0.8342766761779785
Batch 11/64 loss: -0.5749688148498535
Batch 12/64 loss: -0.8271665573120117
Batch 13/64 loss: -0.16650009155273438
Batch 14/64 loss: -0.5571932792663574
Batch 15/64 loss: -0.974189281463623
Batch 16/64 loss: -0.8164167404174805
Batch 17/64 loss: -0.6660966873168945
Batch 18/64 loss: -0.8930368423461914
Batch 19/64 loss: -0.7064728736877441
Batch 20/64 loss: -0.7699418067932129
Batch 21/64 loss: -0.8184256553649902
Batch 22/64 loss: -0.9185080528259277
Batch 23/64 loss: -0.7202520370483398
Batch 24/64 loss: 0.3439655303955078
Batch 25/64 loss: -0.37685632705688477
Batch 26/64 loss: -0.7219343185424805
Batch 27/64 loss: -0.875586986541748
Batch 28/64 loss: -0.6628627777099609
Batch 29/64 loss: -0.8283500671386719
Batch 30/64 loss: -0.7953052520751953
Batch 31/64 loss: -0.8630867004394531
Batch 32/64 loss: -0.7114205360412598
Batch 33/64 loss: -0.9195671081542969
Batch 34/64 loss: -0.9637055397033691
Batch 35/64 loss: -0.8138952255249023
Batch 36/64 loss: -0.8920588493347168
Batch 37/64 loss: -0.9382548332214355
Batch 38/64 loss: -0.851773738861084
Batch 39/64 loss: -0.8707213401794434
Batch 40/64 loss: -0.8889245986938477
Batch 41/64 loss: -0.6529507637023926
Batch 42/64 loss: -0.8518919944763184
Batch 43/64 loss: -0.7184739112854004
Batch 44/64 loss: -0.358705997467041
Batch 45/64 loss: -0.7176651954650879
Batch 46/64 loss: -0.8765239715576172
Batch 47/64 loss: -0.8942155838012695
Batch 48/64 loss: -0.8018302917480469
Batch 49/64 loss: 0.6219110488891602
Batch 50/64 loss: -0.806281566619873
Batch 51/64 loss: -0.8928699493408203
Batch 52/64 loss: -0.8142156600952148
Batch 53/64 loss: -0.857182502746582
Batch 54/64 loss: -0.966616153717041
Batch 55/64 loss: -0.8446831703186035
Batch 56/64 loss: -0.9463157653808594
Batch 57/64 loss: 0.27594566345214844
Batch 58/64 loss: -0.8466720581054688
Batch 59/64 loss: -1.0320725440979004
Batch 60/64 loss: -0.5972757339477539
Batch 61/64 loss: -0.6858205795288086
Batch 62/64 loss: -0.9849295616149902
Batch 63/64 loss: -0.9787707328796387
Batch 64/64 loss: -4.365142345428467
Epoch 463  Train loss: -0.7726374326967725  Val loss: -1.1320075595501773
Epoch 464
-------------------------------
Batch 1/64 loss: -0.7913656234741211
Batch 2/64 loss: -0.9412798881530762
Batch 3/64 loss: -0.9080729484558105
Batch 4/64 loss: -0.749781608581543
Batch 5/64 loss: -0.8129353523254395
Batch 6/64 loss: -0.841942310333252
Batch 7/64 loss: -0.9370393753051758
Batch 8/64 loss: -1.0268869400024414
Batch 9/64 loss: -0.715202808380127
Batch 10/64 loss: -0.9236230850219727
Batch 11/64 loss: -0.7383298873901367
Batch 12/64 loss: -0.8273634910583496
Batch 13/64 loss: -0.9432926177978516
Batch 14/64 loss: -0.812047004699707
Batch 15/64 loss: -1.0435032844543457
Batch 16/64 loss: -0.45975351333618164
Batch 17/64 loss: -0.707085132598877
Batch 18/64 loss: -0.9140973091125488
Batch 19/64 loss: -0.19408798217773438
Batch 20/64 loss: -0.8726177215576172
Batch 21/64 loss: -0.8522143363952637
Batch 22/64 loss: -0.8450775146484375
Batch 23/64 loss: -0.6046328544616699
Batch 24/64 loss: -0.8751182556152344
Batch 25/64 loss: -0.7615523338317871
Batch 26/64 loss: -0.5482025146484375
Batch 27/64 loss: -0.8931632041931152
Batch 28/64 loss: -0.8280839920043945
Batch 29/64 loss: -0.8933663368225098
Batch 30/64 loss: -0.7956128120422363
Batch 31/64 loss: -0.9824051856994629
Batch 32/64 loss: -0.8863716125488281
Batch 33/64 loss: -0.5297541618347168
Batch 34/64 loss: 0.43576908111572266
Batch 35/64 loss: -0.839545726776123
Batch 36/64 loss: -0.8131036758422852
Batch 37/64 loss: -0.8780989646911621
Batch 38/64 loss: -0.7228989601135254
Batch 39/64 loss: -0.6648411750793457
Batch 40/64 loss: -0.987152099609375
Batch 41/64 loss: -0.760462760925293
Batch 42/64 loss: 0.6668901443481445
Batch 43/64 loss: -0.9951505661010742
Batch 44/64 loss: -0.8811430931091309
Batch 45/64 loss: -0.8815879821777344
Batch 46/64 loss: -0.8721938133239746
Batch 47/64 loss: -0.9173369407653809
Batch 48/64 loss: -0.9642353057861328
Batch 49/64 loss: -0.9356489181518555
Batch 50/64 loss: -0.9047908782958984
Batch 51/64 loss: -0.41289377212524414
Batch 52/64 loss: -0.6782393455505371
Batch 53/64 loss: -0.7538537979125977
Batch 54/64 loss: -0.9174165725708008
Batch 55/64 loss: -0.7735114097595215
Batch 56/64 loss: -0.003941059112548828
Batch 57/64 loss: -0.9256691932678223
Batch 58/64 loss: -0.9887690544128418
Batch 59/64 loss: -0.9804902076721191
Batch 60/64 loss: -0.8539900779724121
Batch 61/64 loss: -0.7768678665161133
Batch 62/64 loss: -0.8637046813964844
Batch 63/64 loss: -0.6070280075073242
Batch 64/64 loss: -4.652041912078857
Epoch 464  Train loss: -0.8062243312012916  Val loss: -1.2007696145178937
Epoch 465
-------------------------------
Batch 1/64 loss: -0.9280099868774414
Batch 2/64 loss: -0.8407430648803711
Batch 3/64 loss: -0.8606290817260742
Batch 4/64 loss: -0.6981687545776367
Batch 5/64 loss: -0.9613227844238281
Batch 6/64 loss: -0.9639239311218262
Batch 7/64 loss: -0.8507719039916992
Batch 8/64 loss: -0.878260612487793
Batch 9/64 loss: -0.8186593055725098
Batch 10/64 loss: -0.7641868591308594
Batch 11/64 loss: -0.7393836975097656
Batch 12/64 loss: -0.8711323738098145
Batch 13/64 loss: -0.8129634857177734
Batch 14/64 loss: -0.6717362403869629
Batch 15/64 loss: 0.9963784217834473
Batch 16/64 loss: -0.7820768356323242
Batch 17/64 loss: -0.7415432929992676
Batch 18/64 loss: -0.8950963020324707
Batch 19/64 loss: -0.10516595840454102
Batch 20/64 loss: -0.7641429901123047
Batch 21/64 loss: -0.9438457489013672
Batch 22/64 loss: 0.1598219871520996
Batch 23/64 loss: -0.6100997924804688
Batch 24/64 loss: -0.8824996948242188
Batch 25/64 loss: -0.7920489311218262
Batch 26/64 loss: -0.7434101104736328
Batch 27/64 loss: -0.8454098701477051
Batch 28/64 loss: 0.344419002532959
Batch 29/64 loss: -0.9045782089233398
Batch 30/64 loss: -0.8585429191589355
Batch 31/64 loss: -0.2502756118774414
Batch 32/64 loss: -0.7926607131958008
Batch 33/64 loss: -0.7931890487670898
Batch 34/64 loss: -0.9305520057678223
Batch 35/64 loss: -0.8085465431213379
Batch 36/64 loss: -0.8534693717956543
Batch 37/64 loss: -0.8381447792053223
Batch 38/64 loss: -0.7140464782714844
Batch 39/64 loss: -0.8634781837463379
Batch 40/64 loss: -0.7485451698303223
Batch 41/64 loss: -0.8965530395507812
Batch 42/64 loss: -0.7761454582214355
Batch 43/64 loss: -0.8885431289672852
Batch 44/64 loss: -0.2845888137817383
Batch 45/64 loss: -0.7843999862670898
Batch 46/64 loss: -0.798614501953125
Batch 47/64 loss: -0.6956467628479004
Batch 48/64 loss: -0.49868011474609375
Batch 49/64 loss: -0.7077856063842773
Batch 50/64 loss: -0.7382841110229492
Batch 51/64 loss: -0.8344235420227051
Batch 52/64 loss: -0.861504077911377
Batch 53/64 loss: -0.48209238052368164
Batch 54/64 loss: -0.755037784576416
Batch 55/64 loss: -0.7362875938415527
Batch 56/64 loss: -0.7072439193725586
Batch 57/64 loss: -0.9112725257873535
Batch 58/64 loss: -0.8182563781738281
Batch 59/64 loss: -0.8701491355895996
Batch 60/64 loss: -0.8869638442993164
Batch 61/64 loss: -0.8295507431030273
Batch 62/64 loss: -0.8019776344299316
Batch 63/64 loss: -0.7182407379150391
Batch 64/64 loss: -4.447248935699463
Epoch 465  Train loss: -0.7566795255623612  Val loss: -1.13169239476784
Epoch 466
-------------------------------
Batch 1/64 loss: -0.7367639541625977
Batch 2/64 loss: -0.9214797019958496
Batch 3/64 loss: -0.8836579322814941
Batch 4/64 loss: -0.8212742805480957
Batch 5/64 loss: -0.8533420562744141
Batch 6/64 loss: -0.7813715934753418
Batch 7/64 loss: -0.8216228485107422
Batch 8/64 loss: -0.8136262893676758
Batch 9/64 loss: -0.7970833778381348
Batch 10/64 loss: 0.15298795700073242
Batch 11/64 loss: -0.8732051849365234
Batch 12/64 loss: -0.506589412689209
Batch 13/64 loss: -0.6743402481079102
Batch 14/64 loss: -0.9399094581604004
Batch 15/64 loss: -0.772852897644043
Batch 16/64 loss: -0.7077569961547852
Batch 17/64 loss: -0.6611895561218262
Batch 18/64 loss: -0.7112512588500977
Batch 19/64 loss: -0.5144748687744141
Batch 20/64 loss: -0.7296161651611328
Batch 21/64 loss: -0.7954230308532715
Batch 22/64 loss: -0.6485424041748047
Batch 23/64 loss: -0.8783984184265137
Batch 24/64 loss: -0.8502388000488281
Batch 25/64 loss: -0.8440446853637695
Batch 26/64 loss: -0.7725143432617188
Batch 27/64 loss: -0.3486924171447754
Batch 28/64 loss: -0.774925708770752
Batch 29/64 loss: -0.6527628898620605
Batch 30/64 loss: -0.21244382858276367
Batch 31/64 loss: -0.4939441680908203
Batch 32/64 loss: -0.8441357612609863
Batch 33/64 loss: -0.6977658271789551
Batch 34/64 loss: -0.6815376281738281
Batch 35/64 loss: -0.8368058204650879
Batch 36/64 loss: -0.8405790328979492
Batch 37/64 loss: -0.5735726356506348
Batch 38/64 loss: -0.7815818786621094
Batch 39/64 loss: -0.2724189758300781
Batch 40/64 loss: 0.5277061462402344
Batch 41/64 loss: -0.7693796157836914
Batch 42/64 loss: -0.44124603271484375
Batch 43/64 loss: -0.772315502166748
Batch 44/64 loss: -0.6915826797485352
Batch 45/64 loss: -0.6715188026428223
Batch 46/64 loss: -0.7917814254760742
Batch 47/64 loss: -0.6813087463378906
Batch 48/64 loss: -0.833045482635498
Batch 49/64 loss: -0.9706406593322754
Batch 50/64 loss: -0.7267165184020996
Batch 51/64 loss: -0.21823930740356445
Batch 52/64 loss: -0.7591814994812012
Batch 53/64 loss: -0.888282299041748
Batch 54/64 loss: -0.7663812637329102
Batch 55/64 loss: -0.7025394439697266
Batch 56/64 loss: -0.7886910438537598
Batch 57/64 loss: -0.8896098136901855
Batch 58/64 loss: 0.8112306594848633
Batch 59/64 loss: -0.9300942420959473
Batch 60/64 loss: -0.9018974304199219
Batch 61/64 loss: -0.7720813751220703
Batch 62/64 loss: -0.888190746307373
Batch 63/64 loss: -0.884333610534668
Batch 64/64 loss: -4.418794631958008
Epoch 466  Train loss: -0.719732785692402  Val loss: -1.0144654369026525
Epoch 467
-------------------------------
Batch 1/64 loss: -0.583498477935791
Batch 2/64 loss: -0.9215507507324219
Batch 3/64 loss: -0.7780189514160156
Batch 4/64 loss: -0.737945556640625
Batch 5/64 loss: -0.28655195236206055
Batch 6/64 loss: 0.6140556335449219
Batch 7/64 loss: -0.7346444129943848
Batch 8/64 loss: -0.16349029541015625
Batch 9/64 loss: -0.7318367958068848
Batch 10/64 loss: -0.6158480644226074
Batch 11/64 loss: -0.803074836730957
Batch 12/64 loss: -0.2099452018737793
Batch 13/64 loss: -0.8504519462585449
Batch 14/64 loss: -0.8498845100402832
Batch 15/64 loss: -0.8020071983337402
Batch 16/64 loss: -0.8819093704223633
Batch 17/64 loss: -0.7439870834350586
Batch 18/64 loss: -0.6938910484313965
Batch 19/64 loss: -0.7707743644714355
Batch 20/64 loss: -0.9225287437438965
Batch 21/64 loss: -0.6401958465576172
Batch 22/64 loss: -0.9149670600891113
Batch 23/64 loss: -0.8837757110595703
Batch 24/64 loss: -0.4173893928527832
Batch 25/64 loss: -0.5129661560058594
Batch 26/64 loss: -0.5787863731384277
Batch 27/64 loss: -0.8947224617004395
Batch 28/64 loss: -0.9232807159423828
Batch 29/64 loss: -0.7373285293579102
Batch 30/64 loss: -0.742283821105957
Batch 31/64 loss: -0.743321418762207
Batch 32/64 loss: -0.8955645561218262
Batch 33/64 loss: -0.8591561317443848
Batch 34/64 loss: -0.7870383262634277
Batch 35/64 loss: -0.8244400024414062
Batch 36/64 loss: -0.845707893371582
Batch 37/64 loss: -0.8859663009643555
Batch 38/64 loss: -0.9434938430786133
Batch 39/64 loss: -0.8697504997253418
Batch 40/64 loss: -0.9242744445800781
Batch 41/64 loss: -0.8846368789672852
Batch 42/64 loss: -0.8551554679870605
Batch 43/64 loss: -0.907374382019043
Batch 44/64 loss: -0.8728394508361816
Batch 45/64 loss: -0.7576594352722168
Batch 46/64 loss: -0.9775099754333496
Batch 47/64 loss: -0.925652027130127
Batch 48/64 loss: -0.48450660705566406
Batch 49/64 loss: -0.7465882301330566
Batch 50/64 loss: -0.8852686882019043
Batch 51/64 loss: -0.7343635559082031
Batch 52/64 loss: -0.7044334411621094
Batch 53/64 loss: -0.683927059173584
Batch 54/64 loss: -0.6619405746459961
Batch 55/64 loss: 0.40500783920288086
Batch 56/64 loss: -0.8660593032836914
Batch 57/64 loss: -0.961860179901123
Batch 58/64 loss: -0.991600513458252
Batch 59/64 loss: -0.8587274551391602
Batch 60/64 loss: -0.8936467170715332
Batch 61/64 loss: -0.7450952529907227
Batch 62/64 loss: -0.694005012512207
Batch 63/64 loss: 0.3434734344482422
Batch 64/64 loss: -4.534389972686768
Epoch 467  Train loss: -0.7535271345400343  Val loss: -1.1579252420012485
Epoch 468
-------------------------------
Batch 1/64 loss: -1.0035805702209473
Batch 2/64 loss: -0.6046452522277832
Batch 3/64 loss: -0.976405143737793
Batch 4/64 loss: -0.6668539047241211
Batch 5/64 loss: -0.5942692756652832
Batch 6/64 loss: -0.7014913558959961
Batch 7/64 loss: -0.8454699516296387
Batch 8/64 loss: -0.7427611351013184
Batch 9/64 loss: -0.9018797874450684
Batch 10/64 loss: -0.7855887413024902
Batch 11/64 loss: -0.8530349731445312
Batch 12/64 loss: -0.7906856536865234
Batch 13/64 loss: -0.23939895629882812
Batch 14/64 loss: -0.7061767578125
Batch 15/64 loss: -0.9652886390686035
Batch 16/64 loss: -0.8558168411254883
Batch 17/64 loss: -0.6712417602539062
Batch 18/64 loss: -0.8060207366943359
Batch 19/64 loss: -0.8443489074707031
Batch 20/64 loss: -0.7910661697387695
Batch 21/64 loss: -0.754976749420166
Batch 22/64 loss: -0.832313060760498
Batch 23/64 loss: -0.7731122970581055
Batch 24/64 loss: 0.4138789176940918
Batch 25/64 loss: -0.8159618377685547
Batch 26/64 loss: -1.009943962097168
Batch 27/64 loss: -0.8820381164550781
Batch 28/64 loss: -0.874962329864502
Batch 29/64 loss: -0.8528041839599609
Batch 30/64 loss: -0.7811121940612793
Batch 31/64 loss: -0.7120389938354492
Batch 32/64 loss: -0.6733479499816895
Batch 33/64 loss: -0.8498282432556152
Batch 34/64 loss: -0.7832779884338379
Batch 35/64 loss: -0.7945919036865234
Batch 36/64 loss: -0.7098684310913086
Batch 37/64 loss: -0.8484978675842285
Batch 38/64 loss: -0.7120528221130371
Batch 39/64 loss: -0.907233715057373
Batch 40/64 loss: -0.9070777893066406
Batch 41/64 loss: -0.7533712387084961
Batch 42/64 loss: -0.7430019378662109
Batch 43/64 loss: -0.8039731979370117
Batch 44/64 loss: -0.8339390754699707
Batch 45/64 loss: -0.7083759307861328
Batch 46/64 loss: -0.8354549407958984
Batch 47/64 loss: -0.8182811737060547
Batch 48/64 loss: 0.5276331901550293
Batch 49/64 loss: -0.26929235458374023
Batch 50/64 loss: -0.779665470123291
Batch 51/64 loss: -0.5195741653442383
Batch 52/64 loss: -0.817044734954834
Batch 53/64 loss: -0.7967934608459473
Batch 54/64 loss: -0.8867478370666504
Batch 55/64 loss: -0.9824824333190918
Batch 56/64 loss: -0.8680925369262695
Batch 57/64 loss: -0.20174217224121094
Batch 58/64 loss: -0.803863525390625
Batch 59/64 loss: -0.9324798583984375
Batch 60/64 loss: -0.5701570510864258
Batch 61/64 loss: 0.8118009567260742
Batch 62/64 loss: -1.0040392875671387
Batch 63/64 loss: -0.7871909141540527
Batch 64/64 loss: -4.636209964752197
Epoch 468  Train loss: -0.757027022043864  Val loss: -1.1030182723736845
Epoch 469
-------------------------------
Batch 1/64 loss: -0.8673081398010254
Batch 2/64 loss: -0.4584007263183594
Batch 3/64 loss: -0.7825112342834473
Batch 4/64 loss: -0.8498110771179199
Batch 5/64 loss: -0.8570971488952637
Batch 6/64 loss: -0.7784762382507324
Batch 7/64 loss: -0.8460345268249512
Batch 8/64 loss: -0.6915688514709473
Batch 9/64 loss: -0.9724164009094238
Batch 10/64 loss: -0.9433612823486328
Batch 11/64 loss: -0.2729649543762207
Batch 12/64 loss: 0.38297319412231445
Batch 13/64 loss: -0.944000244140625
Batch 14/64 loss: -0.7688732147216797
Batch 15/64 loss: -0.6534843444824219
Batch 16/64 loss: -0.7575454711914062
Batch 17/64 loss: -0.6052618026733398
Batch 18/64 loss: -0.6561903953552246
Batch 19/64 loss: -0.6690521240234375
Batch 20/64 loss: -0.7445836067199707
Batch 21/64 loss: -0.8550477027893066
Batch 22/64 loss: 0.5903863906860352
Batch 23/64 loss: -0.6661362648010254
Batch 24/64 loss: -0.8153409957885742
Batch 25/64 loss: -0.9677858352661133
Batch 26/64 loss: -0.9130349159240723
Batch 27/64 loss: -0.9046568870544434
Batch 28/64 loss: -0.6184778213500977
Batch 29/64 loss: -0.5982308387756348
Batch 30/64 loss: -0.7585887908935547
Batch 31/64 loss: -0.40232086181640625
Batch 32/64 loss: -0.8365063667297363
Batch 33/64 loss: -0.7846403121948242
Batch 34/64 loss: -0.8535723686218262
Batch 35/64 loss: -0.1123361587524414
Batch 36/64 loss: -0.8996405601501465
Batch 37/64 loss: -0.17268037796020508
Batch 38/64 loss: 0.3544654846191406
Batch 39/64 loss: -0.8772883415222168
Batch 40/64 loss: -0.7071089744567871
Batch 41/64 loss: -0.95782470703125
Batch 42/64 loss: -0.6866259574890137
Batch 43/64 loss: -0.8651337623596191
Batch 44/64 loss: -0.7105679512023926
Batch 45/64 loss: -0.8454904556274414
Batch 46/64 loss: -0.9320049285888672
Batch 47/64 loss: -0.8566021919250488
Batch 48/64 loss: -0.7450380325317383
Batch 49/64 loss: -0.6054244041442871
Batch 50/64 loss: -0.9712090492248535
Batch 51/64 loss: -0.8677597045898438
Batch 52/64 loss: -0.9748988151550293
Batch 53/64 loss: -0.6165390014648438
Batch 54/64 loss: -0.950465202331543
Batch 55/64 loss: -0.85699462890625
Batch 56/64 loss: -0.6065630912780762
Batch 57/64 loss: -0.8566470146179199
Batch 58/64 loss: -0.8113985061645508
Batch 59/64 loss: -0.7832818031311035
Batch 60/64 loss: -0.9239740371704102
Batch 61/64 loss: -0.9606032371520996
Batch 62/64 loss: -0.9093785285949707
Batch 63/64 loss: -0.9818258285522461
Batch 64/64 loss: -4.585793495178223
Epoch 469  Train loss: -0.756864424312816  Val loss: -1.1975479781422829
Epoch 470
-------------------------------
Batch 1/64 loss: 0.15384578704833984
Batch 2/64 loss: -0.8829817771911621
Batch 3/64 loss: -0.9163966178894043
Batch 4/64 loss: -0.7318730354309082
Batch 5/64 loss: -0.8902778625488281
Batch 6/64 loss: -0.8234472274780273
Batch 7/64 loss: -0.8274836540222168
Batch 8/64 loss: -0.6480612754821777
Batch 9/64 loss: -0.5213122367858887
Batch 10/64 loss: -0.7811822891235352
Batch 11/64 loss: -0.5286250114440918
Batch 12/64 loss: -0.8024082183837891
Batch 13/64 loss: -0.6311173439025879
Batch 14/64 loss: -0.9143352508544922
Batch 15/64 loss: -0.9233698844909668
Batch 16/64 loss: -0.7400989532470703
Batch 17/64 loss: -0.9618105888366699
Batch 18/64 loss: -0.8967390060424805
Batch 19/64 loss: -0.9449539184570312
Batch 20/64 loss: -0.8966808319091797
Batch 21/64 loss: -0.836702823638916
Batch 22/64 loss: -0.7885193824768066
Batch 23/64 loss: -0.820253849029541
Batch 24/64 loss: 0.35161590576171875
Batch 25/64 loss: -0.9521617889404297
Batch 26/64 loss: -0.9533481597900391
Batch 27/64 loss: -0.9454970359802246
Batch 28/64 loss: -0.7731733322143555
Batch 29/64 loss: -0.9185347557067871
Batch 30/64 loss: -0.9347286224365234
Batch 31/64 loss: -0.9552016258239746
Batch 32/64 loss: -0.9653615951538086
Batch 33/64 loss: -0.7354774475097656
Batch 34/64 loss: 0.6130247116088867
Batch 35/64 loss: -0.6283025741577148
Batch 36/64 loss: -0.7430381774902344
Batch 37/64 loss: -0.8097820281982422
Batch 38/64 loss: -0.8847074508666992
Batch 39/64 loss: -0.6835103034973145
Batch 40/64 loss: -0.6436877250671387
Batch 41/64 loss: -0.7237863540649414
Batch 42/64 loss: -0.9014482498168945
Batch 43/64 loss: -0.6956205368041992
Batch 44/64 loss: -0.7044897079467773
Batch 45/64 loss: -0.9047694206237793
Batch 46/64 loss: -0.838536262512207
Batch 47/64 loss: -0.9333953857421875
Batch 48/64 loss: 0.40687990188598633
Batch 49/64 loss: -0.9394059181213379
Batch 50/64 loss: -0.18929529190063477
Batch 51/64 loss: -0.7003960609436035
Batch 52/64 loss: -1.0201530456542969
Batch 53/64 loss: -0.908271312713623
Batch 54/64 loss: -0.8392834663391113
Batch 55/64 loss: -0.7445979118347168
Batch 56/64 loss: -0.8513288497924805
Batch 57/64 loss: -0.9974126815795898
Batch 58/64 loss: -0.627103328704834
Batch 59/64 loss: -0.8473939895629883
Batch 60/64 loss: -0.8003506660461426
Batch 61/64 loss: -0.7606344223022461
Batch 62/64 loss: -0.8171863555908203
Batch 63/64 loss: -0.9512252807617188
Batch 64/64 loss: -4.622760772705078
Epoch 470  Train loss: -0.7823205087699142  Val loss: -1.1722806556937622
Epoch 471
-------------------------------
Batch 1/64 loss: -0.8903021812438965
Batch 2/64 loss: -0.7664680480957031
Batch 3/64 loss: -0.7072582244873047
Batch 4/64 loss: -0.8933801651000977
Batch 5/64 loss: -0.2891058921813965
Batch 6/64 loss: -0.7468876838684082
Batch 7/64 loss: -0.6157035827636719
Batch 8/64 loss: -0.8558239936828613
Batch 9/64 loss: -0.9871182441711426
Batch 10/64 loss: -0.4379744529724121
Batch 11/64 loss: -0.737278938293457
Batch 12/64 loss: -0.997779369354248
Batch 13/64 loss: -0.807894229888916
Batch 14/64 loss: -0.7664122581481934
Batch 15/64 loss: -0.47165775299072266
Batch 16/64 loss: -0.933586597442627
Batch 17/64 loss: -0.8426895141601562
Batch 18/64 loss: -0.9220304489135742
Batch 19/64 loss: -0.9511022567749023
Batch 20/64 loss: -0.8578433990478516
Batch 21/64 loss: -0.8831863403320312
Batch 22/64 loss: -0.8136653900146484
Batch 23/64 loss: -1.0335931777954102
Batch 24/64 loss: -0.831784725189209
Batch 25/64 loss: -0.6353106498718262
Batch 26/64 loss: -0.8172445297241211
Batch 27/64 loss: -0.8038935661315918
Batch 28/64 loss: 0.505396842956543
Batch 29/64 loss: -0.9726481437683105
Batch 30/64 loss: -0.8576664924621582
Batch 31/64 loss: -0.8122830390930176
Batch 32/64 loss: -0.8136448860168457
Batch 33/64 loss: -0.8169388771057129
Batch 34/64 loss: -0.38553619384765625
Batch 35/64 loss: -0.9504261016845703
Batch 36/64 loss: -0.7878232002258301
Batch 37/64 loss: -0.8412942886352539
Batch 38/64 loss: -0.927734375
Batch 39/64 loss: -0.8368501663208008
Batch 40/64 loss: -0.9300308227539062
Batch 41/64 loss: -0.9249157905578613
Batch 42/64 loss: -0.7283768653869629
Batch 43/64 loss: -0.8675088882446289
Batch 44/64 loss: -0.8581523895263672
Batch 45/64 loss: -1.0361042022705078
Batch 46/64 loss: 0.29616737365722656
Batch 47/64 loss: -0.8346309661865234
Batch 48/64 loss: -0.7182574272155762
Batch 49/64 loss: -0.9126167297363281
Batch 50/64 loss: -0.885164737701416
Batch 51/64 loss: -0.9433636665344238
Batch 52/64 loss: -1.0342774391174316
Batch 53/64 loss: -1.0246829986572266
Batch 54/64 loss: -0.7357082366943359
Batch 55/64 loss: -0.9020447731018066
Batch 56/64 loss: -0.9591274261474609
Batch 57/64 loss: -0.8991508483886719
Batch 58/64 loss: -0.9068059921264648
Batch 59/64 loss: 0.40157556533813477
Batch 60/64 loss: -0.8984165191650391
Batch 61/64 loss: -0.7988123893737793
Batch 62/64 loss: -0.6931972503662109
Batch 63/64 loss: -0.838587760925293
Batch 64/64 loss: -4.641880989074707
Epoch 471  Train loss: -0.8142435522640452  Val loss: -1.1471978020422238
Epoch 472
-------------------------------
Batch 1/64 loss: -0.7839450836181641
Batch 2/64 loss: -0.6699862480163574
Batch 3/64 loss: -0.8149595260620117
Batch 4/64 loss: -0.8571343421936035
Batch 5/64 loss: -0.7234053611755371
Batch 6/64 loss: -0.6913237571716309
Batch 7/64 loss: -0.9034461975097656
Batch 8/64 loss: -0.45006227493286133
Batch 9/64 loss: -0.6317625045776367
Batch 10/64 loss: -0.7546520233154297
Batch 11/64 loss: -0.8949055671691895
Batch 12/64 loss: -0.8481082916259766
Batch 13/64 loss: -0.8179478645324707
Batch 14/64 loss: -0.9166088104248047
Batch 15/64 loss: -0.7027268409729004
Batch 16/64 loss: -0.26557159423828125
Batch 17/64 loss: -0.8416471481323242
Batch 18/64 loss: -0.8345832824707031
Batch 19/64 loss: -0.8597302436828613
Batch 20/64 loss: -0.48386383056640625
Batch 21/64 loss: -0.8585968017578125
Batch 22/64 loss: -0.7003583908081055
Batch 23/64 loss: -0.909637451171875
Batch 24/64 loss: -0.9758415222167969
Batch 25/64 loss: -0.8820033073425293
Batch 26/64 loss: -0.8481554985046387
Batch 27/64 loss: 0.7389764785766602
Batch 28/64 loss: -0.4803495407104492
Batch 29/64 loss: -0.988306999206543
Batch 30/64 loss: -0.7685308456420898
Batch 31/64 loss: -0.6538710594177246
Batch 32/64 loss: -0.6940274238586426
Batch 33/64 loss: -0.8926472663879395
Batch 34/64 loss: -0.9136476516723633
Batch 35/64 loss: -0.7312946319580078
Batch 36/64 loss: -0.8287367820739746
Batch 37/64 loss: 0.11236429214477539
Batch 38/64 loss: -0.7851972579956055
Batch 39/64 loss: -0.9402980804443359
Batch 40/64 loss: 0.27341365814208984
Batch 41/64 loss: -0.833925724029541
Batch 42/64 loss: -0.9312458038330078
Batch 43/64 loss: -0.33414268493652344
Batch 44/64 loss: -0.9016728401184082
Batch 45/64 loss: -0.875096321105957
Batch 46/64 loss: -0.7239599227905273
Batch 47/64 loss: -0.8829460144042969
Batch 48/64 loss: -0.9329633712768555
Batch 49/64 loss: -0.7652583122253418
Batch 50/64 loss: -0.8651323318481445
Batch 51/64 loss: -0.9713077545166016
Batch 52/64 loss: -0.984992504119873
Batch 53/64 loss: -0.9435725212097168
Batch 54/64 loss: -0.18520545959472656
Batch 55/64 loss: -0.9255247116088867
Batch 56/64 loss: -0.9624948501586914
Batch 57/64 loss: -0.8628330230712891
Batch 58/64 loss: -0.6966166496276855
Batch 59/64 loss: -0.902277946472168
Batch 60/64 loss: -0.6732845306396484
Batch 61/64 loss: -0.8251214027404785
Batch 62/64 loss: -0.880713939666748
Batch 63/64 loss: -0.6951007843017578
Batch 64/64 loss: -4.74825382232666
Epoch 472  Train loss: -0.7778775944429286  Val loss: -1.0773199546787746
Epoch 473
-------------------------------
Batch 1/64 loss: -0.7470006942749023
Batch 2/64 loss: -0.5858011245727539
Batch 3/64 loss: -0.777336597442627
Batch 4/64 loss: -0.6540350914001465
Batch 5/64 loss: -0.3061809539794922
Batch 6/64 loss: -0.8184537887573242
Batch 7/64 loss: -0.7662763595581055
Batch 8/64 loss: -0.8167638778686523
Batch 9/64 loss: -0.8127670288085938
Batch 10/64 loss: -0.822868824005127
Batch 11/64 loss: -0.1682596206665039
Batch 12/64 loss: -0.7267608642578125
Batch 13/64 loss: -0.7842860221862793
Batch 14/64 loss: -0.7709779739379883
Batch 15/64 loss: -0.8384232521057129
Batch 16/64 loss: -0.7656259536743164
Batch 17/64 loss: -0.7892322540283203
Batch 18/64 loss: -0.8857698440551758
Batch 19/64 loss: -0.7296323776245117
Batch 20/64 loss: -0.6991410255432129
Batch 21/64 loss: -0.6028122901916504
Batch 22/64 loss: -0.8026957511901855
Batch 23/64 loss: -0.7161407470703125
Batch 24/64 loss: 0.8840813636779785
Batch 25/64 loss: -0.8052253723144531
Batch 26/64 loss: -0.7997846603393555
Batch 27/64 loss: -0.66302490234375
Batch 28/64 loss: -0.782127857208252
Batch 29/64 loss: -0.8884954452514648
Batch 30/64 loss: -0.8053808212280273
Batch 31/64 loss: -0.9462294578552246
Batch 32/64 loss: -0.8404397964477539
Batch 33/64 loss: -0.587432861328125
Batch 34/64 loss: -0.8611149787902832
Batch 35/64 loss: -0.46609973907470703
Batch 36/64 loss: -0.7764806747436523
Batch 37/64 loss: -0.6444797515869141
Batch 38/64 loss: -0.8484301567077637
Batch 39/64 loss: -0.81976318359375
Batch 40/64 loss: -0.9022774696350098
Batch 41/64 loss: -0.8556814193725586
Batch 42/64 loss: -0.5692510604858398
Batch 43/64 loss: -0.9979047775268555
Batch 44/64 loss: -0.8198699951171875
Batch 45/64 loss: -0.7352242469787598
Batch 46/64 loss: -0.7633438110351562
Batch 47/64 loss: -0.7716412544250488
Batch 48/64 loss: -0.7251133918762207
Batch 49/64 loss: -0.7148065567016602
Batch 50/64 loss: -0.749755859375
Batch 51/64 loss: -0.5886678695678711
Batch 52/64 loss: -0.701270580291748
Batch 53/64 loss: 0.47139501571655273
Batch 54/64 loss: -0.8887081146240234
Batch 55/64 loss: -0.6912503242492676
Batch 56/64 loss: -0.7853431701660156
Batch 57/64 loss: -0.9129843711853027
Batch 58/64 loss: -0.4693765640258789
Batch 59/64 loss: -0.9477734565734863
Batch 60/64 loss: -0.9145421981811523
Batch 61/64 loss: 0.3551616668701172
Batch 62/64 loss: -0.8041653633117676
Batch 63/64 loss: -0.5793967247009277
Batch 64/64 loss: -4.472102165222168
Epoch 473  Train loss: -0.7286829667932847  Val loss: -1.1398594518707381
Epoch 474
-------------------------------
Batch 1/64 loss: -0.7028141021728516
Batch 2/64 loss: 0.3488764762878418
Batch 3/64 loss: -0.9203376770019531
Batch 4/64 loss: -0.8699092864990234
Batch 5/64 loss: -0.7480015754699707
Batch 6/64 loss: -0.8813338279724121
Batch 7/64 loss: -0.9273533821105957
Batch 8/64 loss: -0.8646025657653809
Batch 9/64 loss: -0.8154544830322266
Batch 10/64 loss: -0.7302742004394531
Batch 11/64 loss: -0.7994203567504883
Batch 12/64 loss: -0.8584556579589844
Batch 13/64 loss: -0.794886589050293
Batch 14/64 loss: -0.8962001800537109
Batch 15/64 loss: -0.7479610443115234
Batch 16/64 loss: -0.8686728477478027
Batch 17/64 loss: -0.908689022064209
Batch 18/64 loss: -0.9777984619140625
Batch 19/64 loss: -0.7990317344665527
Batch 20/64 loss: -0.8905887603759766
Batch 21/64 loss: -0.4928469657897949
Batch 22/64 loss: 0.15148687362670898
Batch 23/64 loss: -1.0552430152893066
Batch 24/64 loss: -0.8922820091247559
Batch 25/64 loss: -0.5568432807922363
Batch 26/64 loss: -0.6204290390014648
Batch 27/64 loss: -0.9303007125854492
Batch 28/64 loss: -0.6654758453369141
Batch 29/64 loss: -0.8731389045715332
Batch 30/64 loss: -0.8049674034118652
Batch 31/64 loss: -0.8236589431762695
Batch 32/64 loss: -0.8017067909240723
Batch 33/64 loss: -0.8564443588256836
Batch 34/64 loss: -0.9174628257751465
Batch 35/64 loss: -0.7747530937194824
Batch 36/64 loss: -0.8776545524597168
Batch 37/64 loss: -0.3606863021850586
Batch 38/64 loss: -0.18712091445922852
Batch 39/64 loss: -0.8525381088256836
Batch 40/64 loss: -0.7543902397155762
Batch 41/64 loss: -0.8783879280090332
Batch 42/64 loss: -0.7610421180725098
Batch 43/64 loss: -0.7406339645385742
Batch 44/64 loss: -0.9305086135864258
Batch 45/64 loss: -0.9074139595031738
Batch 46/64 loss: -0.9032735824584961
Batch 47/64 loss: -0.9055399894714355
Batch 48/64 loss: -0.7330498695373535
Batch 49/64 loss: -0.9456324577331543
Batch 50/64 loss: -0.6417622566223145
Batch 51/64 loss: -0.9046111106872559
Batch 52/64 loss: -0.3125877380371094
Batch 53/64 loss: -0.8903307914733887
Batch 54/64 loss: -1.0232810974121094
Batch 55/64 loss: 0.829289436340332
Batch 56/64 loss: -0.7766070365905762
Batch 57/64 loss: -0.9378166198730469
Batch 58/64 loss: -0.8924903869628906
Batch 59/64 loss: -0.9666438102722168
Batch 60/64 loss: -0.8186326026916504
Batch 61/64 loss: -0.962407112121582
Batch 62/64 loss: -0.8823866844177246
Batch 63/64 loss: -0.8090143203735352
Batch 64/64 loss: -4.645730972290039
Epoch 474  Train loss: -0.7965243694829006  Val loss: -1.1975589503127684
Epoch 475
-------------------------------
Batch 1/64 loss: -0.9897451400756836
Batch 2/64 loss: -1.0009393692016602
Batch 3/64 loss: -0.8183660507202148
Batch 4/64 loss: -0.7891044616699219
Batch 5/64 loss: -0.9695730209350586
Batch 6/64 loss: -0.8543214797973633
Batch 7/64 loss: 0.10681438446044922
Batch 8/64 loss: -0.6597685813903809
Batch 9/64 loss: -0.93463134765625
Batch 10/64 loss: -0.9861879348754883
Batch 11/64 loss: 0.23993444442749023
Batch 12/64 loss: -0.8996090888977051
Batch 13/64 loss: -0.992455005645752
Batch 14/64 loss: -0.9509291648864746
Batch 15/64 loss: -0.9598903656005859
Batch 16/64 loss: -0.852806568145752
Batch 17/64 loss: -0.8536372184753418
Batch 18/64 loss: -0.8704466819763184
Batch 19/64 loss: -0.7801618576049805
Batch 20/64 loss: -0.9690380096435547
Batch 21/64 loss: 0.5579051971435547
Batch 22/64 loss: -0.9909367561340332
Batch 23/64 loss: -0.9752197265625
Batch 24/64 loss: -0.9093141555786133
Batch 25/64 loss: -0.9193830490112305
Batch 26/64 loss: -1.0306992530822754
Batch 27/64 loss: -0.9474015235900879
Batch 28/64 loss: -0.8140020370483398
Batch 29/64 loss: -0.9482660293579102
Batch 30/64 loss: -1.0328450202941895
Batch 31/64 loss: -0.9895644187927246
Batch 32/64 loss: -0.8296127319335938
Batch 33/64 loss: -0.9551053047180176
Batch 34/64 loss: -0.9037094116210938
Batch 35/64 loss: -0.8911237716674805
Batch 36/64 loss: -0.9034790992736816
Batch 37/64 loss: -0.7463240623474121
Batch 38/64 loss: -0.9158220291137695
Batch 39/64 loss: -0.8963112831115723
Batch 40/64 loss: -0.4580979347229004
Batch 41/64 loss: -0.8253283500671387
Batch 42/64 loss: -0.8105478286743164
Batch 43/64 loss: -0.9593105316162109
Batch 44/64 loss: -0.944084644317627
Batch 45/64 loss: -0.8696660995483398
Batch 46/64 loss: -0.7399311065673828
Batch 47/64 loss: -0.728546142578125
Batch 48/64 loss: 0.2442646026611328
Batch 49/64 loss: -0.9428305625915527
Batch 50/64 loss: -0.8145012855529785
Batch 51/64 loss: -0.9260015487670898
Batch 52/64 loss: -0.4773445129394531
Batch 53/64 loss: -0.8694725036621094
Batch 54/64 loss: -0.7670884132385254
Batch 55/64 loss: -0.8752017021179199
Batch 56/64 loss: -0.6470050811767578
Batch 57/64 loss: -0.919499397277832
Batch 58/64 loss: -0.9036941528320312
Batch 59/64 loss: -0.8380379676818848
Batch 60/64 loss: -0.8875255584716797
Batch 61/64 loss: -0.9099082946777344
Batch 62/64 loss: -0.8872175216674805
Batch 63/64 loss: -0.9743633270263672
Batch 64/64 loss: -4.172171592712402
Epoch 475  Train loss: -0.8421356163772882  Val loss: -1.2233630963617175
Saving best model, epoch: 475
Epoch 476
-------------------------------
Batch 1/64 loss: -0.8947057723999023
Batch 2/64 loss: -0.781794548034668
Batch 3/64 loss: -0.692481517791748
Batch 4/64 loss: -0.9948997497558594
Batch 5/64 loss: -1.0233650207519531
Batch 6/64 loss: -0.7233090400695801
Batch 7/64 loss: -0.8411808013916016
Batch 8/64 loss: -0.935798168182373
Batch 9/64 loss: -0.9368696212768555
Batch 10/64 loss: -0.9160146713256836
Batch 11/64 loss: -0.8204793930053711
Batch 12/64 loss: -0.8865714073181152
Batch 13/64 loss: 0.40928077697753906
Batch 14/64 loss: -1.0100083351135254
Batch 15/64 loss: -0.941037654876709
Batch 16/64 loss: -0.9075632095336914
Batch 17/64 loss: -0.5823249816894531
Batch 18/64 loss: -0.7616968154907227
Batch 19/64 loss: -0.7321915626525879
Batch 20/64 loss: -0.6356511116027832
Batch 21/64 loss: -0.724766731262207
Batch 22/64 loss: -0.9911012649536133
Batch 23/64 loss: -0.9323315620422363
Batch 24/64 loss: -0.9552531242370605
Batch 25/64 loss: -0.9458155632019043
Batch 26/64 loss: -0.9594674110412598
Batch 27/64 loss: -0.6604266166687012
Batch 28/64 loss: -0.43474388122558594
Batch 29/64 loss: -0.9263668060302734
Batch 30/64 loss: -0.9455456733703613
Batch 31/64 loss: 0.29375457763671875
Batch 32/64 loss: -0.8801932334899902
Batch 33/64 loss: -0.8048229217529297
Batch 34/64 loss: -0.7982940673828125
Batch 35/64 loss: -0.9356451034545898
Batch 36/64 loss: -0.4523801803588867
Batch 37/64 loss: -0.8406510353088379
Batch 38/64 loss: -0.944328784942627
Batch 39/64 loss: -0.9647488594055176
Batch 40/64 loss: -0.864560604095459
Batch 41/64 loss: -0.8719744682312012
Batch 42/64 loss: -0.9597897529602051
Batch 43/64 loss: -0.6346440315246582
Batch 44/64 loss: -0.7893519401550293
Batch 45/64 loss: -0.8090057373046875
Batch 46/64 loss: -0.8358755111694336
Batch 47/64 loss: -0.8034558296203613
Batch 48/64 loss: -0.842864990234375
Batch 49/64 loss: -0.36670970916748047
Batch 50/64 loss: -0.8634233474731445
Batch 51/64 loss: -0.9258289337158203
Batch 52/64 loss: -0.8874473571777344
Batch 53/64 loss: -0.6311025619506836
Batch 54/64 loss: -0.7162513732910156
Batch 55/64 loss: -0.7341623306274414
Batch 56/64 loss: -0.5653076171875
Batch 57/64 loss: -0.7216854095458984
Batch 58/64 loss: -0.6337223052978516
Batch 59/64 loss: -0.6965909004211426
Batch 60/64 loss: -0.8007001876831055
Batch 61/64 loss: -0.7804555892944336
Batch 62/64 loss: -0.9932231903076172
Batch 63/64 loss: 1.541895866394043
Batch 64/64 loss: -4.429520606994629
Epoch 476  Train loss: -0.7830614763147691  Val loss: -1.0402526462200992
Epoch 477
-------------------------------
Batch 1/64 loss: -0.8150601387023926
Batch 2/64 loss: -0.7853784561157227
Batch 3/64 loss: -0.9557328224182129
Batch 4/64 loss: -0.7502551078796387
Batch 5/64 loss: -0.6877517700195312
Batch 6/64 loss: -0.9966769218444824
Batch 7/64 loss: -0.7692365646362305
Batch 8/64 loss: -0.912813663482666
Batch 9/64 loss: -0.34929418563842773
Batch 10/64 loss: -0.7936034202575684
Batch 11/64 loss: 0.40640783309936523
Batch 12/64 loss: -0.8995070457458496
Batch 13/64 loss: -0.7863373756408691
Batch 14/64 loss: -0.8319849967956543
Batch 15/64 loss: 0.028514862060546875
Batch 16/64 loss: -0.804710865020752
Batch 17/64 loss: -0.8638148307800293
Batch 18/64 loss: -0.7942848205566406
Batch 19/64 loss: -0.5002055168151855
Batch 20/64 loss: -0.8016400337219238
Batch 21/64 loss: -0.7790684700012207
Batch 22/64 loss: -0.5791568756103516
Batch 23/64 loss: -0.59912109375
Batch 24/64 loss: -0.9482722282409668
Batch 25/64 loss: -0.7923731803894043
Batch 26/64 loss: -0.8240523338317871
Batch 27/64 loss: -0.8163437843322754
Batch 28/64 loss: -0.7234997749328613
Batch 29/64 loss: -0.8797922134399414
Batch 30/64 loss: -0.2536749839782715
Batch 31/64 loss: -0.9087252616882324
Batch 32/64 loss: -0.7470235824584961
Batch 33/64 loss: -0.8006129264831543
Batch 34/64 loss: -0.5838899612426758
Batch 35/64 loss: -0.6899504661560059
Batch 36/64 loss: -0.8339390754699707
Batch 37/64 loss: -0.678706169128418
Batch 38/64 loss: -0.7564492225646973
Batch 39/64 loss: -0.7958908081054688
Batch 40/64 loss: -0.45293712615966797
Batch 41/64 loss: -0.7789735794067383
Batch 42/64 loss: -0.729987621307373
Batch 43/64 loss: -0.7385969161987305
Batch 44/64 loss: -0.9416036605834961
Batch 45/64 loss: -0.9517111778259277
Batch 46/64 loss: -0.8670029640197754
Batch 47/64 loss: -0.348299503326416
Batch 48/64 loss: -0.9758920669555664
Batch 49/64 loss: -0.852482795715332
Batch 50/64 loss: -0.8672418594360352
Batch 51/64 loss: -0.7101321220397949
Batch 52/64 loss: -0.7649097442626953
Batch 53/64 loss: -0.8645529747009277
Batch 54/64 loss: 1.5456228256225586
Batch 55/64 loss: -0.9329371452331543
Batch 56/64 loss: -0.8406543731689453
Batch 57/64 loss: -0.9039931297302246
Batch 58/64 loss: -0.9660649299621582
Batch 59/64 loss: -0.8436117172241211
Batch 60/64 loss: -0.8855195045471191
Batch 61/64 loss: -0.8449397087097168
Batch 62/64 loss: -0.6672959327697754
Batch 63/64 loss: -0.6668200492858887
Batch 64/64 loss: -4.679478645324707
Epoch 477  Train loss: -0.7531616472730449  Val loss: -1.1091881191607602
Epoch 478
-------------------------------
Batch 1/64 loss: -0.6795635223388672
Batch 2/64 loss: -0.7488369941711426
Batch 3/64 loss: -0.9285893440246582
Batch 4/64 loss: 0.39664363861083984
Batch 5/64 loss: -0.8629765510559082
Batch 6/64 loss: -0.3351759910583496
Batch 7/64 loss: -0.7068281173706055
Batch 8/64 loss: -0.669762134552002
Batch 9/64 loss: -0.9441938400268555
Batch 10/64 loss: -0.8674812316894531
Batch 11/64 loss: -0.8415265083312988
Batch 12/64 loss: -0.6570339202880859
Batch 13/64 loss: -0.7731385231018066
Batch 14/64 loss: -0.7225513458251953
Batch 15/64 loss: -0.7596940994262695
Batch 16/64 loss: -0.8709831237792969
Batch 17/64 loss: -0.7845897674560547
Batch 18/64 loss: -0.9259963035583496
Batch 19/64 loss: -0.49469566345214844
Batch 20/64 loss: -0.47362756729125977
Batch 21/64 loss: -0.7687711715698242
Batch 22/64 loss: -0.44187116622924805
Batch 23/64 loss: -0.7912921905517578
Batch 24/64 loss: -0.9015898704528809
Batch 25/64 loss: -0.7770824432373047
Batch 26/64 loss: -0.8363361358642578
Batch 27/64 loss: -0.8540043830871582
Batch 28/64 loss: -0.7673392295837402
Batch 29/64 loss: -0.7183294296264648
Batch 30/64 loss: -0.6997604370117188
Batch 31/64 loss: -0.7615528106689453
Batch 32/64 loss: -0.8453402519226074
Batch 33/64 loss: -0.8312420845031738
Batch 34/64 loss: -0.8307323455810547
Batch 35/64 loss: -0.773921012878418
Batch 36/64 loss: -0.5732607841491699
Batch 37/64 loss: -0.8577699661254883
Batch 38/64 loss: -0.8447823524475098
Batch 39/64 loss: -0.6530122756958008
Batch 40/64 loss: -0.8926472663879395
Batch 41/64 loss: -0.6293916702270508
Batch 42/64 loss: -0.8708109855651855
Batch 43/64 loss: -0.7855625152587891
Batch 44/64 loss: -0.8926401138305664
Batch 45/64 loss: -0.21995019912719727
Batch 46/64 loss: -0.7740654945373535
Batch 47/64 loss: -0.7523841857910156
Batch 48/64 loss: -0.8420047760009766
Batch 49/64 loss: -0.557579517364502
Batch 50/64 loss: -0.8341388702392578
Batch 51/64 loss: -0.6862034797668457
Batch 52/64 loss: 0.666846752166748
Batch 53/64 loss: -0.7973356246948242
Batch 54/64 loss: -0.7845821380615234
Batch 55/64 loss: -0.9214754104614258
Batch 56/64 loss: -0.8178000450134277
Batch 57/64 loss: -0.37278032302856445
Batch 58/64 loss: 0.2870311737060547
Batch 59/64 loss: -0.43475818634033203
Batch 60/64 loss: -0.7917828559875488
Batch 61/64 loss: -0.5528273582458496
Batch 62/64 loss: -0.7287912368774414
Batch 63/64 loss: -0.8843569755554199
Batch 64/64 loss: -4.421629905700684
Epoch 478  Train loss: -0.7272910585590437  Val loss: -0.9738562672408586
Epoch 479
-------------------------------
Batch 1/64 loss: -0.7557239532470703
Batch 2/64 loss: -0.4790358543395996
Batch 3/64 loss: -0.8303027153015137
Batch 4/64 loss: -0.8130378723144531
Batch 5/64 loss: -0.7036700248718262
Batch 6/64 loss: 0.7133488655090332
Batch 7/64 loss: -0.3422980308532715
Batch 8/64 loss: -0.5730009078979492
Batch 9/64 loss: -0.7076873779296875
Batch 10/64 loss: -0.6467909812927246
Batch 11/64 loss: -0.4618082046508789
Batch 12/64 loss: -0.8007392883300781
Batch 13/64 loss: -0.7435183525085449
Batch 14/64 loss: -0.4903249740600586
Batch 15/64 loss: -0.3338804244995117
Batch 16/64 loss: -0.8741931915283203
Batch 17/64 loss: -0.8384084701538086
Batch 18/64 loss: -0.2398538589477539
Batch 19/64 loss: -0.7046971321105957
Batch 20/64 loss: -0.6668777465820312
Batch 21/64 loss: -0.7550196647644043
Batch 22/64 loss: -0.5493507385253906
Batch 23/64 loss: -0.8472962379455566
Batch 24/64 loss: -0.8144893646240234
Batch 25/64 loss: -0.9030084609985352
Batch 26/64 loss: -0.8651695251464844
Batch 27/64 loss: -0.8336949348449707
Batch 28/64 loss: -0.8390979766845703
Batch 29/64 loss: -0.7579469680786133
Batch 30/64 loss: -0.6989755630493164
Batch 31/64 loss: -0.6726503372192383
Batch 32/64 loss: -0.5442795753479004
Batch 33/64 loss: 0.37101316452026367
Batch 34/64 loss: -0.6321115493774414
Batch 35/64 loss: -0.8371047973632812
Batch 36/64 loss: -0.7310476303100586
Batch 37/64 loss: -0.6505656242370605
Batch 38/64 loss: -0.8362441062927246
Batch 39/64 loss: -0.7249841690063477
Batch 40/64 loss: -0.7659978866577148
Batch 41/64 loss: -0.7646584510803223
Batch 42/64 loss: -0.7916674613952637
Batch 43/64 loss: -0.6990957260131836
Batch 44/64 loss: -0.7361040115356445
Batch 45/64 loss: -0.8103013038635254
Batch 46/64 loss: -0.9218873977661133
Batch 47/64 loss: -0.7170076370239258
Batch 48/64 loss: -0.6932268142700195
Batch 49/64 loss: -0.7153244018554688
Batch 50/64 loss: -0.6220874786376953
Batch 51/64 loss: -0.7352209091186523
Batch 52/64 loss: -0.7024145126342773
Batch 53/64 loss: 0.3676319122314453
Batch 54/64 loss: -0.8582668304443359
Batch 55/64 loss: -0.7437663078308105
Batch 56/64 loss: -0.8523859977722168
Batch 57/64 loss: -0.7494230270385742
Batch 58/64 loss: -0.7358717918395996
Batch 59/64 loss: -0.8728313446044922
Batch 60/64 loss: -0.8909835815429688
Batch 61/64 loss: -0.8921465873718262
Batch 62/64 loss: -0.8173274993896484
Batch 63/64 loss: -0.2969236373901367
Batch 64/64 loss: -4.55292272567749
Epoch 479  Train loss: -0.7034745926950492  Val loss: -1.110838375550365
Epoch 480
-------------------------------
Batch 1/64 loss: -0.8070735931396484
Batch 2/64 loss: -0.8509006500244141
Batch 3/64 loss: -0.8887157440185547
Batch 4/64 loss: -0.45684814453125
Batch 5/64 loss: -0.8305573463439941
Batch 6/64 loss: -0.7554078102111816
Batch 7/64 loss: -0.3349776268005371
Batch 8/64 loss: -0.8857202529907227
Batch 9/64 loss: -0.7476801872253418
Batch 10/64 loss: -0.8411741256713867
Batch 11/64 loss: -0.6957674026489258
Batch 12/64 loss: -0.7029991149902344
Batch 13/64 loss: -0.8151769638061523
Batch 14/64 loss: -0.8519911766052246
Batch 15/64 loss: -0.786311149597168
Batch 16/64 loss: -0.9192614555358887
Batch 17/64 loss: -0.8477826118469238
Batch 18/64 loss: -0.8877806663513184
Batch 19/64 loss: -0.7021579742431641
Batch 20/64 loss: -0.9355173110961914
Batch 21/64 loss: -0.9243683815002441
Batch 22/64 loss: -0.8415703773498535
Batch 23/64 loss: -0.2680230140686035
Batch 24/64 loss: 0.40041017532348633
Batch 25/64 loss: -0.9621419906616211
Batch 26/64 loss: -0.8962044715881348
Batch 27/64 loss: -0.9581055641174316
Batch 28/64 loss: -0.9294652938842773
Batch 29/64 loss: -0.771385669708252
Batch 30/64 loss: -0.8951201438903809
Batch 31/64 loss: -0.42811012268066406
Batch 32/64 loss: -0.8069329261779785
Batch 33/64 loss: -0.8666186332702637
Batch 34/64 loss: 0.2978477478027344
Batch 35/64 loss: -0.6798520088195801
Batch 36/64 loss: -0.8472533226013184
Batch 37/64 loss: -0.9920578002929688
Batch 38/64 loss: -0.7371163368225098
Batch 39/64 loss: -0.8675265312194824
Batch 40/64 loss: -0.8377375602722168
Batch 41/64 loss: -0.7043232917785645
Batch 42/64 loss: -0.8054823875427246
Batch 43/64 loss: -0.7256531715393066
Batch 44/64 loss: -0.7254252433776855
Batch 45/64 loss: -0.796959400177002
Batch 46/64 loss: -0.9319033622741699
Batch 47/64 loss: -0.8618011474609375
Batch 48/64 loss: -0.8839840888977051
Batch 49/64 loss: -0.7890944480895996
Batch 50/64 loss: -0.501924991607666
Batch 51/64 loss: -0.8146238327026367
Batch 52/64 loss: -0.8069024085998535
Batch 53/64 loss: -0.858299732208252
Batch 54/64 loss: -0.9187078475952148
Batch 55/64 loss: -0.936370849609375
Batch 56/64 loss: -0.9594845771789551
Batch 57/64 loss: 0.8486843109130859
Batch 58/64 loss: -0.6649360656738281
Batch 59/64 loss: -0.8970069885253906
Batch 60/64 loss: -0.7384395599365234
Batch 61/64 loss: -0.7537088394165039
Batch 62/64 loss: -0.9511771202087402
Batch 63/64 loss: -0.9134912490844727
Batch 64/64 loss: -4.586973667144775
Epoch 480  Train loss: -0.782531452178955  Val loss: -1.1926421201515853
Epoch 481
-------------------------------
Batch 1/64 loss: -0.6894164085388184
Batch 2/64 loss: -0.8090672492980957
Batch 3/64 loss: -0.7877840995788574
Batch 4/64 loss: -0.9073300361633301
Batch 5/64 loss: -0.700770378112793
Batch 6/64 loss: -0.7988500595092773
Batch 7/64 loss: -0.8654704093933105
Batch 8/64 loss: -0.38385486602783203
Batch 9/64 loss: -0.7922377586364746
Batch 10/64 loss: -0.9435558319091797
Batch 11/64 loss: -0.8593082427978516
Batch 12/64 loss: -0.4141216278076172
Batch 13/64 loss: -0.8346571922302246
Batch 14/64 loss: -0.950714111328125
Batch 15/64 loss: -0.9103059768676758
Batch 16/64 loss: -0.43328285217285156
Batch 17/64 loss: -0.8662075996398926
Batch 18/64 loss: -0.934321403503418
Batch 19/64 loss: -0.8975381851196289
Batch 20/64 loss: 0.2836604118347168
Batch 21/64 loss: -0.9821333885192871
Batch 22/64 loss: -0.33710718154907227
Batch 23/64 loss: -0.9254446029663086
Batch 24/64 loss: -0.8055672645568848
Batch 25/64 loss: -0.709625244140625
Batch 26/64 loss: -0.7888298034667969
Batch 27/64 loss: -0.6864504814147949
Batch 28/64 loss: -0.5125961303710938
Batch 29/64 loss: -0.7560606002807617
Batch 30/64 loss: -0.7852683067321777
Batch 31/64 loss: 0.07352876663208008
Batch 32/64 loss: -0.9244942665100098
Batch 33/64 loss: -0.9393105506896973
Batch 34/64 loss: -0.7873744964599609
Batch 35/64 loss: -0.845832347869873
Batch 36/64 loss: -0.9132766723632812
Batch 37/64 loss: -0.7824301719665527
Batch 38/64 loss: -0.7296586036682129
Batch 39/64 loss: -0.734525203704834
Batch 40/64 loss: -0.8716878890991211
Batch 41/64 loss: -0.7978715896606445
Batch 42/64 loss: -0.3274517059326172
Batch 43/64 loss: -0.6592473983764648
Batch 44/64 loss: -0.8387775421142578
Batch 45/64 loss: -0.9164233207702637
Batch 46/64 loss: -0.4363417625427246
Batch 47/64 loss: -0.6255912780761719
Batch 48/64 loss: -0.8090863227844238
Batch 49/64 loss: -0.8783779144287109
Batch 50/64 loss: -0.7501921653747559
Batch 51/64 loss: -0.9394025802612305
Batch 52/64 loss: -0.798095703125
Batch 53/64 loss: -0.8561897277832031
Batch 54/64 loss: -0.6511754989624023
Batch 55/64 loss: -0.8833227157592773
Batch 56/64 loss: -0.88568115234375
Batch 57/64 loss: -0.8235616683959961
Batch 58/64 loss: -0.7820162773132324
Batch 59/64 loss: -0.9330029487609863
Batch 60/64 loss: -1.0256896018981934
Batch 61/64 loss: 0.6750712394714355
Batch 62/64 loss: -0.8555970191955566
Batch 63/64 loss: -0.8476371765136719
Batch 64/64 loss: -4.374178409576416
Epoch 481  Train loss: -0.7712246894836425  Val loss: -1.093940721754356
Epoch 482
-------------------------------
Batch 1/64 loss: -0.8969593048095703
Batch 2/64 loss: -0.7119097709655762
Batch 3/64 loss: -0.7822999954223633
Batch 4/64 loss: 0.7522974014282227
Batch 5/64 loss: -0.7066664695739746
Batch 6/64 loss: -0.8551020622253418
Batch 7/64 loss: -0.38826656341552734
Batch 8/64 loss: -0.9408783912658691
Batch 9/64 loss: -0.799720287322998
Batch 10/64 loss: -0.8413844108581543
Batch 11/64 loss: -0.9028534889221191
Batch 12/64 loss: -0.8236446380615234
Batch 13/64 loss: -0.8190360069274902
Batch 14/64 loss: -0.7373089790344238
Batch 15/64 loss: -0.923464298248291
Batch 16/64 loss: -0.7650799751281738
Batch 17/64 loss: -0.8550372123718262
Batch 18/64 loss: -0.7294549942016602
Batch 19/64 loss: -0.8634090423583984
Batch 20/64 loss: -0.7965569496154785
Batch 21/64 loss: -0.7338676452636719
Batch 22/64 loss: -0.4741530418395996
Batch 23/64 loss: -0.7783699035644531
Batch 24/64 loss: -0.8368082046508789
Batch 25/64 loss: -0.9234027862548828
Batch 26/64 loss: -0.8286943435668945
Batch 27/64 loss: -1.0120863914489746
Batch 28/64 loss: -0.7751569747924805
Batch 29/64 loss: -0.7077531814575195
Batch 30/64 loss: -0.7518939971923828
Batch 31/64 loss: -0.7728018760681152
Batch 32/64 loss: 0.2425532341003418
Batch 33/64 loss: -0.33077573776245117
Batch 34/64 loss: -0.7604918479919434
Batch 35/64 loss: -0.5841779708862305
Batch 36/64 loss: -0.9355788230895996
Batch 37/64 loss: -0.8919029235839844
Batch 38/64 loss: -0.857853889465332
Batch 39/64 loss: -0.7300381660461426
Batch 40/64 loss: -0.8985500335693359
Batch 41/64 loss: -0.8005781173706055
Batch 42/64 loss: -0.8909120559692383
Batch 43/64 loss: -0.8719582557678223
Batch 44/64 loss: -0.7793145179748535
Batch 45/64 loss: -0.8708720207214355
Batch 46/64 loss: -0.43043947219848633
Batch 47/64 loss: -0.7626180648803711
Batch 48/64 loss: -0.7669239044189453
Batch 49/64 loss: -0.11251068115234375
Batch 50/64 loss: -0.760871410369873
Batch 51/64 loss: -0.5384583473205566
Batch 52/64 loss: -0.8111171722412109
Batch 53/64 loss: -0.8378944396972656
Batch 54/64 loss: 0.4511690139770508
Batch 55/64 loss: -0.39252710342407227
Batch 56/64 loss: -0.6805400848388672
Batch 57/64 loss: -0.8019399642944336
Batch 58/64 loss: -0.6139779090881348
Batch 59/64 loss: -0.6865653991699219
Batch 60/64 loss: -0.7839431762695312
Batch 61/64 loss: -0.46358346939086914
Batch 62/64 loss: -0.6537494659423828
Batch 63/64 loss: -0.862152099609375
Batch 64/64 loss: -4.294936656951904
Epoch 482  Train loss: -0.7321101263457653  Val loss: -1.0036118366464306
Epoch 483
-------------------------------
Batch 1/64 loss: -0.856635570526123
Batch 2/64 loss: -0.7702207565307617
Batch 3/64 loss: -0.7532305717468262
Batch 4/64 loss: -0.8702893257141113
Batch 5/64 loss: -0.6533279418945312
Batch 6/64 loss: -0.5579514503479004
Batch 7/64 loss: -0.6402072906494141
Batch 8/64 loss: -0.7745790481567383
Batch 9/64 loss: -0.04637956619262695
Batch 10/64 loss: -0.7262997627258301
Batch 11/64 loss: -0.7947664260864258
Batch 12/64 loss: -0.7246890068054199
Batch 13/64 loss: -0.7884116172790527
Batch 14/64 loss: -0.49256134033203125
Batch 15/64 loss: -0.9578890800476074
Batch 16/64 loss: -0.9541769027709961
Batch 17/64 loss: -0.6338686943054199
Batch 18/64 loss: -0.9237875938415527
Batch 19/64 loss: -0.8858799934387207
Batch 20/64 loss: -0.8966879844665527
Batch 21/64 loss: -0.711524486541748
Batch 22/64 loss: -0.8266391754150391
Batch 23/64 loss: -0.9369215965270996
Batch 24/64 loss: -0.7843995094299316
Batch 25/64 loss: -0.8100113868713379
Batch 26/64 loss: -0.7539157867431641
Batch 27/64 loss: -0.4942140579223633
Batch 28/64 loss: -0.8149518966674805
Batch 29/64 loss: -0.778315544128418
Batch 30/64 loss: -0.8976116180419922
Batch 31/64 loss: -0.8507204055786133
Batch 32/64 loss: -0.884922981262207
Batch 33/64 loss: -0.9084944725036621
Batch 34/64 loss: -0.528834342956543
Batch 35/64 loss: 0.6710982322692871
Batch 36/64 loss: -0.43604373931884766
Batch 37/64 loss: -0.8365921974182129
Batch 38/64 loss: -0.8499107360839844
Batch 39/64 loss: -0.9700980186462402
Batch 40/64 loss: -0.7790398597717285
Batch 41/64 loss: -0.25217485427856445
Batch 42/64 loss: -0.7165255546569824
Batch 43/64 loss: -0.9216794967651367
Batch 44/64 loss: -0.6928362846374512
Batch 45/64 loss: -0.8844976425170898
Batch 46/64 loss: -0.8164048194885254
Batch 47/64 loss: -0.848121166229248
Batch 48/64 loss: -0.7440133094787598
Batch 49/64 loss: -0.2942819595336914
Batch 50/64 loss: -0.8423476219177246
Batch 51/64 loss: -0.767982006072998
Batch 52/64 loss: -0.8831281661987305
Batch 53/64 loss: -0.7875175476074219
Batch 54/64 loss: -0.7946782112121582
Batch 55/64 loss: -0.8897185325622559
Batch 56/64 loss: -0.955355167388916
Batch 57/64 loss: -0.7063617706298828
Batch 58/64 loss: -0.686863899230957
Batch 59/64 loss: 0.2752685546875
Batch 60/64 loss: 0.34008216857910156
Batch 61/64 loss: -0.8792171478271484
Batch 62/64 loss: -0.7718305587768555
Batch 63/64 loss: -0.7014493942260742
Batch 64/64 loss: -4.543134689331055
Epoch 483  Train loss: -0.7453002181707644  Val loss: -1.0826548809038405
Epoch 484
-------------------------------
Batch 1/64 loss: -0.737281322479248
Batch 2/64 loss: -0.7499775886535645
Batch 3/64 loss: -0.8306050300598145
Batch 4/64 loss: -0.8263745307922363
Batch 5/64 loss: -0.14087820053100586
Batch 6/64 loss: -0.7416658401489258
Batch 7/64 loss: -0.7689785957336426
Batch 8/64 loss: -0.8263034820556641
Batch 9/64 loss: -0.82037353515625
Batch 10/64 loss: -0.8793601989746094
Batch 11/64 loss: -0.685889720916748
Batch 12/64 loss: -0.7435803413391113
Batch 13/64 loss: -0.7076621055603027
Batch 14/64 loss: -0.7997260093688965
Batch 15/64 loss: -0.9639797210693359
Batch 16/64 loss: -0.7879586219787598
Batch 17/64 loss: -0.7471628189086914
Batch 18/64 loss: -0.602963924407959
Batch 19/64 loss: -0.7820472717285156
Batch 20/64 loss: -0.9173460006713867
Batch 21/64 loss: -0.6901469230651855
Batch 22/64 loss: -0.6849608421325684
Batch 23/64 loss: -0.9275636672973633
Batch 24/64 loss: -0.6612496376037598
Batch 25/64 loss: -0.8594255447387695
Batch 26/64 loss: -0.6854643821716309
Batch 27/64 loss: -0.8602442741394043
Batch 28/64 loss: 0.34975147247314453
Batch 29/64 loss: -0.5519771575927734
Batch 30/64 loss: -0.23593521118164062
Batch 31/64 loss: -0.7216315269470215
Batch 32/64 loss: -0.47754669189453125
Batch 33/64 loss: -0.9525961875915527
Batch 34/64 loss: -0.31522321701049805
Batch 35/64 loss: -0.9030632972717285
Batch 36/64 loss: -0.8752646446228027
Batch 37/64 loss: -0.7982993125915527
Batch 38/64 loss: -0.9708447456359863
Batch 39/64 loss: -0.9204549789428711
Batch 40/64 loss: -0.8332619667053223
Batch 41/64 loss: 0.665642261505127
Batch 42/64 loss: -0.8661065101623535
Batch 43/64 loss: -0.8449783325195312
Batch 44/64 loss: 0.4467658996582031
Batch 45/64 loss: -0.8481755256652832
Batch 46/64 loss: -0.8729515075683594
Batch 47/64 loss: -0.8551836013793945
Batch 48/64 loss: -0.8730812072753906
Batch 49/64 loss: -0.7861828804016113
Batch 50/64 loss: -0.731201171875
Batch 51/64 loss: -0.9492292404174805
Batch 52/64 loss: -0.8191289901733398
Batch 53/64 loss: -0.8308992385864258
Batch 54/64 loss: -0.6675524711608887
Batch 55/64 loss: -0.7899737358093262
Batch 56/64 loss: -0.8602046966552734
Batch 57/64 loss: -0.7156429290771484
Batch 58/64 loss: -0.6807651519775391
Batch 59/64 loss: -0.7754392623901367
Batch 60/64 loss: -0.8157591819763184
Batch 61/64 loss: -0.8504061698913574
Batch 62/64 loss: -0.6440682411193848
Batch 63/64 loss: -0.7335829734802246
Batch 64/64 loss: -4.433191776275635
Epoch 484  Train loss: -0.7475845692204494  Val loss: -1.0450162854800928
Epoch 485
-------------------------------
Batch 1/64 loss: -0.7726116180419922
Batch 2/64 loss: -0.8235645294189453
Batch 3/64 loss: -0.7923035621643066
Batch 4/64 loss: -0.8840866088867188
Batch 5/64 loss: -0.7332935333251953
Batch 6/64 loss: -0.7526683807373047
Batch 7/64 loss: -0.8179159164428711
Batch 8/64 loss: 0.3929939270019531
Batch 9/64 loss: -0.759666919708252
Batch 10/64 loss: -0.8668680191040039
Batch 11/64 loss: -0.8752536773681641
Batch 12/64 loss: -0.7560520172119141
Batch 13/64 loss: -0.8242130279541016
Batch 14/64 loss: -0.2617759704589844
Batch 15/64 loss: -0.8910589218139648
Batch 16/64 loss: -0.6519312858581543
Batch 17/64 loss: -0.5519165992736816
Batch 18/64 loss: -0.817108154296875
Batch 19/64 loss: -0.8252429962158203
Batch 20/64 loss: -0.6798281669616699
Batch 21/64 loss: 0.4104790687561035
Batch 22/64 loss: -0.5587983131408691
Batch 23/64 loss: -0.7839608192443848
Batch 24/64 loss: -0.8806891441345215
Batch 25/64 loss: -0.8734045028686523
Batch 26/64 loss: -0.8765568733215332
Batch 27/64 loss: -0.9055490493774414
Batch 28/64 loss: -0.8649773597717285
Batch 29/64 loss: -0.8281068801879883
Batch 30/64 loss: -0.8949527740478516
Batch 31/64 loss: -0.7953095436096191
Batch 32/64 loss: -0.7744846343994141
Batch 33/64 loss: -0.7934212684631348
Batch 34/64 loss: -0.5489168167114258
Batch 35/64 loss: -0.723139762878418
Batch 36/64 loss: -0.8760695457458496
Batch 37/64 loss: -0.6512837409973145
Batch 38/64 loss: -0.6281485557556152
Batch 39/64 loss: 0.695460319519043
Batch 40/64 loss: -0.2006368637084961
Batch 41/64 loss: -0.6390500068664551
Batch 42/64 loss: -0.8748283386230469
Batch 43/64 loss: -0.7687492370605469
Batch 44/64 loss: -0.7153072357177734
Batch 45/64 loss: -0.8863000869750977
Batch 46/64 loss: -0.82177734375
Batch 47/64 loss: -0.6914429664611816
Batch 48/64 loss: -0.8671746253967285
Batch 49/64 loss: -0.8292818069458008
Batch 50/64 loss: -0.7740154266357422
Batch 51/64 loss: -0.8166990280151367
Batch 52/64 loss: -0.8895950317382812
Batch 53/64 loss: -0.7042841911315918
Batch 54/64 loss: -0.6692461967468262
Batch 55/64 loss: -0.7609777450561523
Batch 56/64 loss: -0.8900871276855469
Batch 57/64 loss: -0.7448511123657227
Batch 58/64 loss: -0.8403549194335938
Batch 59/64 loss: -0.2701749801635742
Batch 60/64 loss: -0.7614231109619141
Batch 61/64 loss: -0.5919947624206543
Batch 62/64 loss: -0.7991981506347656
Batch 63/64 loss: -0.5557284355163574
Batch 64/64 loss: -4.4408369064331055
Epoch 485  Train loss: -0.7339608248542336  Val loss: -1.036832868438406
Epoch 486
-------------------------------
Batch 1/64 loss: -0.7494478225708008
Batch 2/64 loss: -0.7553596496582031
Batch 3/64 loss: -0.7795696258544922
Batch 4/64 loss: -0.7839980125427246
Batch 5/64 loss: -0.4615797996520996
Batch 6/64 loss: -0.706916332244873
Batch 7/64 loss: -0.8107414245605469
Batch 8/64 loss: -0.3290138244628906
Batch 9/64 loss: -0.7486639022827148
Batch 10/64 loss: -0.7195544242858887
Batch 11/64 loss: -0.8244256973266602
Batch 12/64 loss: -0.5698862075805664
Batch 13/64 loss: -0.7304444313049316
Batch 14/64 loss: -0.7447381019592285
Batch 15/64 loss: -0.5772886276245117
Batch 16/64 loss: -0.7464232444763184
Batch 17/64 loss: -0.7703852653503418
Batch 18/64 loss: -0.7106895446777344
Batch 19/64 loss: -0.8711104393005371
Batch 20/64 loss: -0.7290749549865723
Batch 21/64 loss: -0.7605628967285156
Batch 22/64 loss: -0.7052927017211914
Batch 23/64 loss: -0.7531261444091797
Batch 24/64 loss: -0.2066655158996582
Batch 25/64 loss: -0.6601214408874512
Batch 26/64 loss: -0.7854413986206055
Batch 27/64 loss: -0.3435826301574707
Batch 28/64 loss: -0.7135725021362305
Batch 29/64 loss: -0.7777204513549805
Batch 30/64 loss: -0.8454470634460449
Batch 31/64 loss: -0.4428553581237793
Batch 32/64 loss: -0.7455801963806152
Batch 33/64 loss: -0.8361854553222656
Batch 34/64 loss: -0.6588468551635742
Batch 35/64 loss: -0.5125150680541992
Batch 36/64 loss: -0.8259110450744629
Batch 37/64 loss: -0.7092843055725098
Batch 38/64 loss: 0.1359696388244629
Batch 39/64 loss: 0.6505088806152344
Batch 40/64 loss: -0.7654299736022949
Batch 41/64 loss: -0.8061923980712891
Batch 42/64 loss: -0.668123722076416
Batch 43/64 loss: -0.8941097259521484
Batch 44/64 loss: 0.3104543685913086
Batch 45/64 loss: -0.8968348503112793
Batch 46/64 loss: -0.9121098518371582
Batch 47/64 loss: -0.7265076637268066
Batch 48/64 loss: -0.8831748962402344
Batch 49/64 loss: -0.7647886276245117
Batch 50/64 loss: -0.5254864692687988
Batch 51/64 loss: -0.9248080253601074
Batch 52/64 loss: -0.7567958831787109
Batch 53/64 loss: -0.8111896514892578
Batch 54/64 loss: -0.7094326019287109
Batch 55/64 loss: -0.6394190788269043
Batch 56/64 loss: -0.6139183044433594
Batch 57/64 loss: -0.7052426338195801
Batch 58/64 loss: -0.7653899192810059
Batch 59/64 loss: -0.5969185829162598
Batch 60/64 loss: -0.7993965148925781
Batch 61/64 loss: -0.7440710067749023
Batch 62/64 loss: -0.8724455833435059
Batch 63/64 loss: -0.6371493339538574
Batch 64/64 loss: -4.348025798797607
Epoch 486  Train loss: -0.7056477509292902  Val loss: -1.0154440575039263
Epoch 487
-------------------------------
Batch 1/64 loss: -0.6198019981384277
Batch 2/64 loss: -0.7967634201049805
Batch 3/64 loss: -0.5289983749389648
Batch 4/64 loss: -0.2068643569946289
Batch 5/64 loss: -0.6202974319458008
Batch 6/64 loss: -0.6464104652404785
Batch 7/64 loss: -0.7413949966430664
Batch 8/64 loss: -0.7425460815429688
Batch 9/64 loss: -0.6424760818481445
Batch 10/64 loss: -0.6208868026733398
Batch 11/64 loss: -0.8596005439758301
Batch 12/64 loss: -0.7102828025817871
Batch 13/64 loss: -0.7153325080871582
Batch 14/64 loss: -0.6257662773132324
Batch 15/64 loss: -0.6455001831054688
Batch 16/64 loss: -0.7484316825866699
Batch 17/64 loss: -0.6064548492431641
Batch 18/64 loss: -0.7855939865112305
Batch 19/64 loss: -0.7196950912475586
Batch 20/64 loss: -0.7519707679748535
Batch 21/64 loss: -0.607017993927002
Batch 22/64 loss: -0.7133097648620605
Batch 23/64 loss: -0.8127627372741699
Batch 24/64 loss: -0.7526607513427734
Batch 25/64 loss: -0.7474560737609863
Batch 26/64 loss: -0.6162524223327637
Batch 27/64 loss: -0.6403484344482422
Batch 28/64 loss: -0.675135612487793
Batch 29/64 loss: -0.8998208045959473
Batch 30/64 loss: -0.6360583305358887
Batch 31/64 loss: -0.5667729377746582
Batch 32/64 loss: -0.8451390266418457
Batch 33/64 loss: -0.8323087692260742
Batch 34/64 loss: -0.704129695892334
Batch 35/64 loss: -0.7260799407958984
Batch 36/64 loss: -0.7731060981750488
Batch 37/64 loss: -0.51580810546875
Batch 38/64 loss: -0.47402429580688477
Batch 39/64 loss: 0.5357222557067871
Batch 40/64 loss: -0.804175853729248
Batch 41/64 loss: -0.8321132659912109
Batch 42/64 loss: -0.7672085762023926
Batch 43/64 loss: -0.6837754249572754
Batch 44/64 loss: -0.8397679328918457
Batch 45/64 loss: -0.6897835731506348
Batch 46/64 loss: -0.9057087898254395
Batch 47/64 loss: 0.9684419631958008
Batch 48/64 loss: -0.6931414604187012
Batch 49/64 loss: -0.15848875045776367
Batch 50/64 loss: -0.6428442001342773
Batch 51/64 loss: -0.8849663734436035
Batch 52/64 loss: -0.7653260231018066
Batch 53/64 loss: -0.24816083908081055
Batch 54/64 loss: -0.5648703575134277
Batch 55/64 loss: -0.6328606605529785
Batch 56/64 loss: -0.42726612091064453
Batch 57/64 loss: -0.7239170074462891
Batch 58/64 loss: -0.5513486862182617
Batch 59/64 loss: -0.17941856384277344
Batch 60/64 loss: -0.5293445587158203
Batch 61/64 loss: 0.31673431396484375
Batch 62/64 loss: -0.8451542854309082
Batch 63/64 loss: -0.339296817779541
Batch 64/64 loss: -4.150219440460205
Epoch 487  Train loss: -0.6411602300756117  Val loss: -0.9565349985234106
Epoch 488
-------------------------------
Batch 1/64 loss: -0.8103170394897461
Batch 2/64 loss: -0.7415776252746582
Batch 3/64 loss: -0.3662834167480469
Batch 4/64 loss: -0.2848539352416992
Batch 5/64 loss: -0.5404181480407715
Batch 6/64 loss: -0.694094181060791
Batch 7/64 loss: -0.7800846099853516
Batch 8/64 loss: -0.8176097869873047
Batch 9/64 loss: -0.7760553359985352
Batch 10/64 loss: -0.7693519592285156
Batch 11/64 loss: -0.6810698509216309
Batch 12/64 loss: -0.2181687355041504
Batch 13/64 loss: -0.7203927040100098
Batch 14/64 loss: -0.6875629425048828
Batch 15/64 loss: -0.6506342887878418
Batch 16/64 loss: 0.8955192565917969
Batch 17/64 loss: -0.5191159248352051
Batch 18/64 loss: -0.7864928245544434
Batch 19/64 loss: -0.7083249092102051
Batch 20/64 loss: -0.49579620361328125
Batch 21/64 loss: -0.8372364044189453
Batch 22/64 loss: -0.6195063591003418
Batch 23/64 loss: -0.7511825561523438
Batch 24/64 loss: -0.8904800415039062
Batch 25/64 loss: -0.8883662223815918
Batch 26/64 loss: -0.8209543228149414
Batch 27/64 loss: -0.7609853744506836
Batch 28/64 loss: -0.7067165374755859
Batch 29/64 loss: -0.8122572898864746
Batch 30/64 loss: -0.8792867660522461
Batch 31/64 loss: -0.749577522277832
Batch 32/64 loss: -0.7187843322753906
Batch 33/64 loss: -0.6951436996459961
Batch 34/64 loss: -0.8717689514160156
Batch 35/64 loss: -0.8192358016967773
Batch 36/64 loss: -0.8934993743896484
Batch 37/64 loss: -0.7589964866638184
Batch 38/64 loss: -0.8581204414367676
Batch 39/64 loss: -0.8451385498046875
Batch 40/64 loss: -0.9388508796691895
Batch 41/64 loss: 0.16590023040771484
Batch 42/64 loss: -0.717522144317627
Batch 43/64 loss: -0.039974212646484375
Batch 44/64 loss: -0.6735134124755859
Batch 45/64 loss: -0.3723883628845215
Batch 46/64 loss: -0.7407526969909668
Batch 47/64 loss: -0.7828640937805176
Batch 48/64 loss: -0.8062314987182617
Batch 49/64 loss: -0.6901993751525879
Batch 50/64 loss: -0.7912302017211914
Batch 51/64 loss: -0.7656741142272949
Batch 52/64 loss: -0.7345309257507324
Batch 53/64 loss: -0.7252821922302246
Batch 54/64 loss: -0.5436587333679199
Batch 55/64 loss: -0.7334384918212891
Batch 56/64 loss: -0.7913846969604492
Batch 57/64 loss: -0.8556890487670898
Batch 58/64 loss: -0.6352033615112305
Batch 59/64 loss: -0.737823486328125
Batch 60/64 loss: 0.36922311782836914
Batch 61/64 loss: -0.606440544128418
Batch 62/64 loss: -0.5064859390258789
Batch 63/64 loss: -0.7154450416564941
Batch 64/64 loss: -4.400603294372559
Epoch 488  Train loss: -0.6901934866811715  Val loss: -0.9854988150580233
Epoch 489
-------------------------------
Batch 1/64 loss: -0.8490371704101562
Batch 2/64 loss: -0.8634243011474609
Batch 3/64 loss: -0.8419632911682129
Batch 4/64 loss: 0.2442021369934082
Batch 5/64 loss: -0.699864387512207
Batch 6/64 loss: -0.7418012619018555
Batch 7/64 loss: -0.44718503952026367
Batch 8/64 loss: -0.7440237998962402
Batch 9/64 loss: -0.8332381248474121
Batch 10/64 loss: -0.7603793144226074
Batch 11/64 loss: -0.759920597076416
Batch 12/64 loss: -0.6950278282165527
Batch 13/64 loss: -0.6173524856567383
Batch 14/64 loss: -0.6488919258117676
Batch 15/64 loss: -0.36821460723876953
Batch 16/64 loss: -0.5078253746032715
Batch 17/64 loss: 0.7198739051818848
Batch 18/64 loss: -0.7350835800170898
Batch 19/64 loss: -0.819849967956543
Batch 20/64 loss: -0.8102569580078125
Batch 21/64 loss: -0.8848996162414551
Batch 22/64 loss: -0.8509101867675781
Batch 23/64 loss: -0.8266696929931641
Batch 24/64 loss: -0.7050743103027344
Batch 25/64 loss: -0.902921199798584
Batch 26/64 loss: 0.48136281967163086
Batch 27/64 loss: -0.6637053489685059
Batch 28/64 loss: -0.6194896697998047
Batch 29/64 loss: -0.8324241638183594
Batch 30/64 loss: -0.6711335182189941
Batch 31/64 loss: -0.4496140480041504
Batch 32/64 loss: -0.8120088577270508
Batch 33/64 loss: -0.6253371238708496
Batch 34/64 loss: -0.31062793731689453
Batch 35/64 loss: -0.8651266098022461
Batch 36/64 loss: -0.8541059494018555
Batch 37/64 loss: -0.7403182983398438
Batch 38/64 loss: -0.626643180847168
Batch 39/64 loss: -0.21475601196289062
Batch 40/64 loss: -0.3298916816711426
Batch 41/64 loss: -0.7113370895385742
Batch 42/64 loss: -0.7014880180358887
Batch 43/64 loss: -0.7800383567810059
Batch 44/64 loss: -0.7129201889038086
Batch 45/64 loss: -0.7849469184875488
Batch 46/64 loss: -0.5935602188110352
Batch 47/64 loss: -0.8161287307739258
Batch 48/64 loss: -0.7070388793945312
Batch 49/64 loss: -0.27685022354125977
Batch 50/64 loss: -0.8043913841247559
Batch 51/64 loss: -0.7223634719848633
Batch 52/64 loss: -0.7808938026428223
Batch 53/64 loss: -0.9151639938354492
Batch 54/64 loss: -0.9449844360351562
Batch 55/64 loss: -0.6277050971984863
Batch 56/64 loss: -0.8360800743103027
Batch 57/64 loss: -0.8729066848754883
Batch 58/64 loss: -0.768549919128418
Batch 59/64 loss: -0.7410111427307129
Batch 60/64 loss: -0.7884383201599121
Batch 61/64 loss: -0.6636567115783691
Batch 62/64 loss: -0.8399429321289062
Batch 63/64 loss: -0.7868924140930176
Batch 64/64 loss: -4.5710930824279785
Epoch 489  Train loss: -0.7010065471424776  Val loss: -1.0533253122441137
Epoch 490
-------------------------------
Batch 1/64 loss: -0.8168830871582031
Batch 2/64 loss: -0.925358772277832
Batch 3/64 loss: -0.851900577545166
Batch 4/64 loss: -0.8704004287719727
Batch 5/64 loss: -0.8976092338562012
Batch 6/64 loss: -0.6930346488952637
Batch 7/64 loss: 0.4381570816040039
Batch 8/64 loss: 0.18950462341308594
Batch 9/64 loss: -0.7302932739257812
Batch 10/64 loss: -0.8817849159240723
Batch 11/64 loss: -0.3059687614440918
Batch 12/64 loss: -0.7498273849487305
Batch 13/64 loss: -0.7441935539245605
Batch 14/64 loss: -0.8411374092102051
Batch 15/64 loss: -0.830174446105957
Batch 16/64 loss: -0.8674778938293457
Batch 17/64 loss: -0.9085726737976074
Batch 18/64 loss: -0.7571024894714355
Batch 19/64 loss: -0.45535993576049805
Batch 20/64 loss: -0.8579139709472656
Batch 21/64 loss: -0.7094488143920898
Batch 22/64 loss: -0.731534481048584
Batch 23/64 loss: -0.7844562530517578
Batch 24/64 loss: -0.8146500587463379
Batch 25/64 loss: -0.8198099136352539
Batch 26/64 loss: -0.6174335479736328
Batch 27/64 loss: -0.16838598251342773
Batch 28/64 loss: -0.8461675643920898
Batch 29/64 loss: -0.6669812202453613
Batch 30/64 loss: -0.7079911231994629
Batch 31/64 loss: -0.8655409812927246
Batch 32/64 loss: -0.8135790824890137
Batch 33/64 loss: -0.7942104339599609
Batch 34/64 loss: -0.5378971099853516
Batch 35/64 loss: -0.8023085594177246
Batch 36/64 loss: -0.9674034118652344
Batch 37/64 loss: -0.8974337577819824
Batch 38/64 loss: -0.802800178527832
Batch 39/64 loss: -0.7151060104370117
Batch 40/64 loss: -0.8319592475891113
Batch 41/64 loss: -0.7093648910522461
Batch 42/64 loss: -0.9087915420532227
Batch 43/64 loss: -0.8661942481994629
Batch 44/64 loss: -0.7528939247131348
Batch 45/64 loss: -0.6250252723693848
Batch 46/64 loss: 0.6106429100036621
Batch 47/64 loss: -0.8279423713684082
Batch 48/64 loss: -0.41651105880737305
Batch 49/64 loss: -0.8382253646850586
Batch 50/64 loss: -0.8852982521057129
Batch 51/64 loss: -0.832618236541748
Batch 52/64 loss: -0.9640650749206543
Batch 53/64 loss: -0.790687084197998
Batch 54/64 loss: -0.9550213813781738
Batch 55/64 loss: -0.4447135925292969
Batch 56/64 loss: -0.9549708366394043
Batch 57/64 loss: -0.7327561378479004
Batch 58/64 loss: -0.8019566535949707
Batch 59/64 loss: -0.8535256385803223
Batch 60/64 loss: -0.7812590599060059
Batch 61/64 loss: -0.8111977577209473
Batch 62/64 loss: -0.9818997383117676
Batch 63/64 loss: -0.8728351593017578
Batch 64/64 loss: -4.615606307983398
Epoch 490  Train loss: -0.7640979542451747  Val loss: -1.1492970260148196
Epoch 491
-------------------------------
Batch 1/64 loss: -0.6798853874206543
Batch 2/64 loss: -0.8915667533874512
Batch 3/64 loss: -0.7241191864013672
Batch 4/64 loss: -0.9704718589782715
Batch 5/64 loss: -0.8719797134399414
Batch 6/64 loss: -0.23744964599609375
Batch 7/64 loss: -0.9182929992675781
Batch 8/64 loss: -0.8774018287658691
Batch 9/64 loss: -0.8806591033935547
Batch 10/64 loss: -0.864995002746582
Batch 11/64 loss: -0.8725252151489258
Batch 12/64 loss: -0.7025508880615234
Batch 13/64 loss: -0.5867376327514648
Batch 14/64 loss: -0.8874797821044922
Batch 15/64 loss: -0.7930011749267578
Batch 16/64 loss: -0.4547762870788574
Batch 17/64 loss: -0.7354378700256348
Batch 18/64 loss: -0.8747978210449219
Batch 19/64 loss: -0.6566448211669922
Batch 20/64 loss: -0.67755126953125
Batch 21/64 loss: -0.8293004035949707
Batch 22/64 loss: -0.7618374824523926
Batch 23/64 loss: -0.8660898208618164
Batch 24/64 loss: -0.4235100746154785
Batch 25/64 loss: -0.9666633605957031
Batch 26/64 loss: -0.6078548431396484
Batch 27/64 loss: -0.9846506118774414
Batch 28/64 loss: -0.8948230743408203
Batch 29/64 loss: -0.6989884376525879
Batch 30/64 loss: -0.8437471389770508
Batch 31/64 loss: -0.8569149971008301
Batch 32/64 loss: -0.6873383522033691
Batch 33/64 loss: -0.560704231262207
Batch 34/64 loss: -0.7102046012878418
Batch 35/64 loss: -0.9544839859008789
Batch 36/64 loss: -0.6618318557739258
Batch 37/64 loss: -0.8868594169616699
Batch 38/64 loss: -0.7112326622009277
Batch 39/64 loss: -0.7845425605773926
Batch 40/64 loss: -0.8191699981689453
Batch 41/64 loss: -0.8854413032531738
Batch 42/64 loss: -0.7572746276855469
Batch 43/64 loss: -0.7845144271850586
Batch 44/64 loss: -0.7884230613708496
Batch 45/64 loss: -0.904320240020752
Batch 46/64 loss: -0.9441647529602051
Batch 47/64 loss: -0.8423829078674316
Batch 48/64 loss: 0.029196739196777344
Batch 49/64 loss: -0.8853926658630371
Batch 50/64 loss: -0.6945643424987793
Batch 51/64 loss: -0.7943539619445801
Batch 52/64 loss: -0.8434200286865234
Batch 53/64 loss: -0.872429370880127
Batch 54/64 loss: -0.28034496307373047
Batch 55/64 loss: -0.9253482818603516
Batch 56/64 loss: -0.9232745170593262
Batch 57/64 loss: -0.7828826904296875
Batch 58/64 loss: -0.9804553985595703
Batch 59/64 loss: 0.7117900848388672
Batch 60/64 loss: -0.7677569389343262
Batch 61/64 loss: -0.8482708930969238
Batch 62/64 loss: -0.827308177947998
Batch 63/64 loss: 0.6158280372619629
Batch 64/64 loss: -4.527019500732422
Epoch 491  Train loss: -0.7692524779076669  Val loss: -1.11157668333283
Epoch 492
-------------------------------
Batch 1/64 loss: -0.8468437194824219
Batch 2/64 loss: -0.3586001396179199
Batch 3/64 loss: -0.6215567588806152
Batch 4/64 loss: -0.7986702919006348
Batch 5/64 loss: -0.5502419471740723
Batch 6/64 loss: -0.5712308883666992
Batch 7/64 loss: -0.6408824920654297
Batch 8/64 loss: -0.8398017883300781
Batch 9/64 loss: -0.8879051208496094
Batch 10/64 loss: -0.8699159622192383
Batch 11/64 loss: -0.9823751449584961
Batch 12/64 loss: -0.9022088050842285
Batch 13/64 loss: -0.7518668174743652
Batch 14/64 loss: -0.7457876205444336
Batch 15/64 loss: -0.7938199043273926
Batch 16/64 loss: -0.771583080291748
Batch 17/64 loss: -0.7063851356506348
Batch 18/64 loss: -0.7366704940795898
Batch 19/64 loss: -0.8546290397644043
Batch 20/64 loss: -0.7740488052368164
Batch 21/64 loss: -0.8627171516418457
Batch 22/64 loss: -0.8442597389221191
Batch 23/64 loss: -0.9549288749694824
Batch 24/64 loss: -0.8177833557128906
Batch 25/64 loss: -0.45345592498779297
Batch 26/64 loss: -0.5269002914428711
Batch 27/64 loss: 0.1679368019104004
Batch 28/64 loss: -0.8159122467041016
Batch 29/64 loss: -0.9093813896179199
Batch 30/64 loss: 0.8064484596252441
Batch 31/64 loss: -0.8607392311096191
Batch 32/64 loss: -0.8694658279418945
Batch 33/64 loss: -0.8178520202636719
Batch 34/64 loss: -0.5085673332214355
Batch 35/64 loss: -0.8492374420166016
Batch 36/64 loss: -0.5952887535095215
Batch 37/64 loss: -0.6490445137023926
Batch 38/64 loss: -0.8746466636657715
Batch 39/64 loss: -0.8520994186401367
Batch 40/64 loss: -0.3645505905151367
Batch 41/64 loss: -0.8112263679504395
Batch 42/64 loss: -0.7977771759033203
Batch 43/64 loss: -0.7982468605041504
Batch 44/64 loss: -0.7195229530334473
Batch 45/64 loss: 0.3637371063232422
Batch 46/64 loss: -0.7911605834960938
Batch 47/64 loss: -0.8172860145568848
Batch 48/64 loss: -0.8077864646911621
Batch 49/64 loss: -0.8250999450683594
Batch 50/64 loss: -0.9877729415893555
Batch 51/64 loss: -0.7981610298156738
Batch 52/64 loss: -0.7273135185241699
Batch 53/64 loss: -0.9030179977416992
Batch 54/64 loss: -0.6215085983276367
Batch 55/64 loss: -0.7880406379699707
Batch 56/64 loss: -0.9307007789611816
Batch 57/64 loss: -0.7890243530273438
Batch 58/64 loss: -0.6621675491333008
Batch 59/64 loss: -0.8220901489257812
Batch 60/64 loss: -0.8156476020812988
Batch 61/64 loss: -0.7893118858337402
Batch 62/64 loss: -0.8169174194335938
Batch 63/64 loss: -0.8891472816467285
Batch 64/64 loss: -3.7050442695617676
Epoch 492  Train loss: -0.746375586939793  Val loss: -1.1399488416324366
Epoch 493
-------------------------------
Batch 1/64 loss: -0.8417692184448242
Batch 2/64 loss: -0.8610405921936035
Batch 3/64 loss: -0.6878180503845215
Batch 4/64 loss: -0.9324650764465332
Batch 5/64 loss: -0.8940601348876953
Batch 6/64 loss: -0.922907829284668
Batch 7/64 loss: -0.8787884712219238
Batch 8/64 loss: -0.8636212348937988
Batch 9/64 loss: -0.8004794120788574
Batch 10/64 loss: -0.7672629356384277
Batch 11/64 loss: 0.41895198822021484
Batch 12/64 loss: -0.7971463203430176
Batch 13/64 loss: -0.7940950393676758
Batch 14/64 loss: -0.9371209144592285
Batch 15/64 loss: -0.9961109161376953
Batch 16/64 loss: -0.2544236183166504
Batch 17/64 loss: -0.9672532081604004
Batch 18/64 loss: -0.9977984428405762
Batch 19/64 loss: -0.8923792839050293
Batch 20/64 loss: -0.9628758430480957
Batch 21/64 loss: -0.876060962677002
Batch 22/64 loss: -0.8133087158203125
Batch 23/64 loss: -0.8749651908874512
Batch 24/64 loss: -0.9343347549438477
Batch 25/64 loss: -0.8188109397888184
Batch 26/64 loss: -0.5199294090270996
Batch 27/64 loss: -0.9210882186889648
Batch 28/64 loss: -0.9105672836303711
Batch 29/64 loss: -0.8932466506958008
Batch 30/64 loss: -0.7627148628234863
Batch 31/64 loss: -0.7187747955322266
Batch 32/64 loss: -0.9230399131774902
Batch 33/64 loss: -0.8643026351928711
Batch 34/64 loss: -0.7023811340332031
Batch 35/64 loss: -0.9376826286315918
Batch 36/64 loss: -0.8864555358886719
Batch 37/64 loss: -0.8216700553894043
Batch 38/64 loss: 0.14586877822875977
Batch 39/64 loss: -0.8932995796203613
Batch 40/64 loss: -0.8936958312988281
Batch 41/64 loss: 1.1375970840454102
Batch 42/64 loss: -0.9218907356262207
Batch 43/64 loss: -0.6965818405151367
Batch 44/64 loss: -0.8412723541259766
Batch 45/64 loss: 0.10506153106689453
Batch 46/64 loss: -0.7974376678466797
Batch 47/64 loss: -0.6396946907043457
Batch 48/64 loss: -0.8899598121643066
Batch 49/64 loss: -0.9802565574645996
Batch 50/64 loss: -0.5670428276062012
Batch 51/64 loss: -0.9136219024658203
Batch 52/64 loss: -0.7537918090820312
Batch 53/64 loss: -0.823272705078125
Batch 54/64 loss: -0.4947934150695801
Batch 55/64 loss: -0.8151555061340332
Batch 56/64 loss: -0.7244725227355957
Batch 57/64 loss: -0.992377758026123
Batch 58/64 loss: -0.8062930107116699
Batch 59/64 loss: -0.8932232856750488
Batch 60/64 loss: -0.8880791664123535
Batch 61/64 loss: -0.9118084907531738
Batch 62/64 loss: -0.8693437576293945
Batch 63/64 loss: -0.7955546379089355
Batch 64/64 loss: -4.558838367462158
Epoch 493  Train loss: -0.7944050115697524  Val loss: -1.1682949131706737
Epoch 494
-------------------------------
Batch 1/64 loss: -0.9711761474609375
Batch 2/64 loss: -0.9087491035461426
Batch 3/64 loss: -0.9010100364685059
Batch 4/64 loss: -0.907188892364502
Batch 5/64 loss: -0.8018603324890137
Batch 6/64 loss: -0.6341042518615723
Batch 7/64 loss: -0.7906079292297363
Batch 8/64 loss: -0.8218874931335449
Batch 9/64 loss: -0.9555563926696777
Batch 10/64 loss: -0.5067358016967773
Batch 11/64 loss: -0.8845949172973633
Batch 12/64 loss: 0.6375861167907715
Batch 13/64 loss: -0.8383259773254395
Batch 14/64 loss: -0.7994904518127441
Batch 15/64 loss: -0.7728095054626465
Batch 16/64 loss: 1.0969080924987793
Batch 17/64 loss: -0.6958818435668945
Batch 18/64 loss: -1.0125751495361328
Batch 19/64 loss: -0.8062286376953125
Batch 20/64 loss: -0.8685355186462402
Batch 21/64 loss: -0.29377174377441406
Batch 22/64 loss: -0.8562779426574707
Batch 23/64 loss: -0.9202303886413574
Batch 24/64 loss: -0.5973014831542969
Batch 25/64 loss: -0.9997897148132324
Batch 26/64 loss: -0.7896718978881836
Batch 27/64 loss: -0.6326713562011719
Batch 28/64 loss: -0.9381675720214844
Batch 29/64 loss: -0.5641946792602539
Batch 30/64 loss: -0.7476677894592285
Batch 31/64 loss: -0.7489709854125977
Batch 32/64 loss: -0.7832350730895996
Batch 33/64 loss: -0.8607511520385742
Batch 34/64 loss: -0.8127193450927734
Batch 35/64 loss: -0.8540816307067871
Batch 36/64 loss: -0.7516250610351562
Batch 37/64 loss: -0.8464694023132324
Batch 38/64 loss: -0.8796749114990234
Batch 39/64 loss: -0.5283188819885254
Batch 40/64 loss: -0.8848557472229004
Batch 41/64 loss: -0.9438505172729492
Batch 42/64 loss: -0.8726296424865723
Batch 43/64 loss: -0.7871942520141602
Batch 44/64 loss: -0.311126708984375
Batch 45/64 loss: -0.5176377296447754
Batch 46/64 loss: -0.7992110252380371
Batch 47/64 loss: -0.9152312278747559
Batch 48/64 loss: -0.6156167984008789
Batch 49/64 loss: -0.7349810600280762
Batch 50/64 loss: -0.7206716537475586
Batch 51/64 loss: -0.4138469696044922
Batch 52/64 loss: -0.6905837059020996
Batch 53/64 loss: -0.7590065002441406
Batch 54/64 loss: -0.8635191917419434
Batch 55/64 loss: -0.7015886306762695
Batch 56/64 loss: -0.851891040802002
Batch 57/64 loss: -0.9374518394470215
Batch 58/64 loss: -0.809382438659668
Batch 59/64 loss: -0.7763352394104004
Batch 60/64 loss: -0.7978224754333496
Batch 61/64 loss: -0.8529167175292969
Batch 62/64 loss: -0.836209774017334
Batch 63/64 loss: -1.0390114784240723
Batch 64/64 loss: -4.5190582275390625
Epoch 494  Train loss: -0.7744357838350184  Val loss: -1.117442416161606
Epoch 495
-------------------------------
Batch 1/64 loss: -0.799494743347168
Batch 2/64 loss: -0.7845921516418457
Batch 3/64 loss: -0.8623485565185547
Batch 4/64 loss: -0.9415187835693359
Batch 5/64 loss: -0.8308429718017578
Batch 6/64 loss: -0.9791879653930664
Batch 7/64 loss: -0.8481850624084473
Batch 8/64 loss: -0.8256540298461914
Batch 9/64 loss: -0.7631611824035645
Batch 10/64 loss: -0.798975944519043
Batch 11/64 loss: -0.7820310592651367
Batch 12/64 loss: -0.7409100532531738
Batch 13/64 loss: -0.8336515426635742
Batch 14/64 loss: -0.9281773567199707
Batch 15/64 loss: -0.8582382202148438
Batch 16/64 loss: -0.9493908882141113
Batch 17/64 loss: -0.8738546371459961
Batch 18/64 loss: -0.7493448257446289
Batch 19/64 loss: 0.28680896759033203
Batch 20/64 loss: -0.9229540824890137
Batch 21/64 loss: -0.9615621566772461
Batch 22/64 loss: -0.8041439056396484
Batch 23/64 loss: -0.9021553993225098
Batch 24/64 loss: -0.9725346565246582
Batch 25/64 loss: -0.8504061698913574
Batch 26/64 loss: -0.8817825317382812
Batch 27/64 loss: -0.8098936080932617
Batch 28/64 loss: -0.9006967544555664
Batch 29/64 loss: -0.9385638236999512
Batch 30/64 loss: -0.8905820846557617
Batch 31/64 loss: -0.40958404541015625
Batch 32/64 loss: -0.9026703834533691
Batch 33/64 loss: -0.32871150970458984
Batch 34/64 loss: -0.08132553100585938
Batch 35/64 loss: -1.0089731216430664
Batch 36/64 loss: -0.8695273399353027
Batch 37/64 loss: -0.6932401657104492
Batch 38/64 loss: -0.7611174583435059
Batch 39/64 loss: -0.9097080230712891
Batch 40/64 loss: -0.796198844909668
Batch 41/64 loss: -0.5020313262939453
Batch 42/64 loss: -0.7089419364929199
Batch 43/64 loss: -0.8529720306396484
Batch 44/64 loss: -0.7202296257019043
Batch 45/64 loss: -0.9018716812133789
Batch 46/64 loss: -0.7029800415039062
Batch 47/64 loss: -0.8267641067504883
Batch 48/64 loss: -0.7078919410705566
Batch 49/64 loss: -1.0170588493347168
Batch 50/64 loss: -0.8662910461425781
Batch 51/64 loss: -0.712580680847168
Batch 52/64 loss: -0.7364358901977539
Batch 53/64 loss: -0.785895824432373
Batch 54/64 loss: -0.8642916679382324
Batch 55/64 loss: 0.4600968360900879
Batch 56/64 loss: -0.8195924758911133
Batch 57/64 loss: -0.6845955848693848
Batch 58/64 loss: -0.7301297187805176
Batch 59/64 loss: 0.6759676933288574
Batch 60/64 loss: -0.9816617965698242
Batch 61/64 loss: -0.9984254837036133
Batch 62/64 loss: -0.43915700912475586
Batch 63/64 loss: -0.9209761619567871
Batch 64/64 loss: -4.560760974884033
Epoch 495  Train loss: -0.7878331558377135  Val loss: -1.1506769763645028
Epoch 496
-------------------------------
Batch 1/64 loss: -0.7290534973144531
Batch 2/64 loss: -0.731377124786377
Batch 3/64 loss: -0.9144549369812012
Batch 4/64 loss: -0.8743071556091309
Batch 5/64 loss: -0.8861217498779297
Batch 6/64 loss: -0.7992115020751953
Batch 7/64 loss: -0.8996520042419434
Batch 8/64 loss: -1.024012565612793
Batch 9/64 loss: -0.8316097259521484
Batch 10/64 loss: -0.8936309814453125
Batch 11/64 loss: -0.9413518905639648
Batch 12/64 loss: -0.9727835655212402
Batch 13/64 loss: -0.9837121963500977
Batch 14/64 loss: -0.841550350189209
Batch 15/64 loss: -0.994534969329834
Batch 16/64 loss: -0.8535208702087402
Batch 17/64 loss: -0.8325581550598145
Batch 18/64 loss: -0.9115018844604492
Batch 19/64 loss: -0.5595097541809082
Batch 20/64 loss: -0.9216747283935547
Batch 21/64 loss: -0.7234721183776855
Batch 22/64 loss: -0.790320873260498
Batch 23/64 loss: -0.9392728805541992
Batch 24/64 loss: -0.9783186912536621
Batch 25/64 loss: -0.7500729560852051
Batch 26/64 loss: -0.8788652420043945
Batch 27/64 loss: -0.7859234809875488
Batch 28/64 loss: 0.7293057441711426
Batch 29/64 loss: -0.4624967575073242
Batch 30/64 loss: -0.9053769111633301
Batch 31/64 loss: -0.8925762176513672
Batch 32/64 loss: -0.9161214828491211
Batch 33/64 loss: -0.8544831275939941
Batch 34/64 loss: -0.26580381393432617
Batch 35/64 loss: -0.8435459136962891
Batch 36/64 loss: 0.6130213737487793
Batch 37/64 loss: -0.12445497512817383
Batch 38/64 loss: -0.9842419624328613
Batch 39/64 loss: -0.73382568359375
Batch 40/64 loss: -0.5281004905700684
Batch 41/64 loss: -0.7172942161560059
Batch 42/64 loss: -0.6880831718444824
Batch 43/64 loss: -0.9273257255554199
Batch 44/64 loss: -0.7838053703308105
Batch 45/64 loss: -0.8971819877624512
Batch 46/64 loss: -0.27642107009887695
Batch 47/64 loss: -0.6712336540222168
Batch 48/64 loss: -0.6815295219421387
Batch 49/64 loss: 0.2927713394165039
Batch 50/64 loss: -0.5259971618652344
Batch 51/64 loss: -0.8370418548583984
Batch 52/64 loss: -0.5601167678833008
Batch 53/64 loss: -0.5711846351623535
Batch 54/64 loss: -0.5962681770324707
Batch 55/64 loss: -0.7109456062316895
Batch 56/64 loss: -0.9264125823974609
Batch 57/64 loss: -0.7208633422851562
Batch 58/64 loss: -0.4481511116027832
Batch 59/64 loss: -0.6245861053466797
Batch 60/64 loss: -0.4235506057739258
Batch 61/64 loss: -0.8762664794921875
Batch 62/64 loss: -0.7790584564208984
Batch 63/64 loss: -0.7609090805053711
Batch 64/64 loss: -4.492944240570068
Epoch 496  Train loss: -0.7449763073640712  Val loss: -1.0436186905169405
Epoch 497
-------------------------------
Batch 1/64 loss: -0.6872735023498535
Batch 2/64 loss: -0.7871856689453125
Batch 3/64 loss: -0.7665324211120605
Batch 4/64 loss: 0.4817013740539551
Batch 5/64 loss: -0.5766582489013672
Batch 6/64 loss: -0.6600475311279297
Batch 7/64 loss: -0.7936224937438965
Batch 8/64 loss: -0.7377772331237793
Batch 9/64 loss: -0.9121766090393066
Batch 10/64 loss: -0.9573149681091309
Batch 11/64 loss: -0.6710267066955566
Batch 12/64 loss: -0.7563066482543945
Batch 13/64 loss: -0.6736598014831543
Batch 14/64 loss: -0.3913865089416504
Batch 15/64 loss: -0.8340492248535156
Batch 16/64 loss: -0.6869025230407715
Batch 17/64 loss: -0.7176241874694824
Batch 18/64 loss: 0.7267632484436035
Batch 19/64 loss: -0.6610374450683594
Batch 20/64 loss: -0.8724784851074219
Batch 21/64 loss: 0.08653974533081055
Batch 22/64 loss: -0.8179535865783691
Batch 23/64 loss: -0.844545841217041
Batch 24/64 loss: -0.8532862663269043
Batch 25/64 loss: -0.8668875694274902
Batch 26/64 loss: -0.7236132621765137
Batch 27/64 loss: -0.9384450912475586
Batch 28/64 loss: -0.911595344543457
Batch 29/64 loss: -0.6389594078063965
Batch 30/64 loss: -0.885622501373291
Batch 31/64 loss: -0.756220817565918
Batch 32/64 loss: -0.5763487815856934
Batch 33/64 loss: -0.8532958030700684
Batch 34/64 loss: -0.3899688720703125
Batch 35/64 loss: -0.35894250869750977
Batch 36/64 loss: -0.7889738082885742
Batch 37/64 loss: -0.10682439804077148
Batch 38/64 loss: -0.9246377944946289
Batch 39/64 loss: -0.5864291191101074
Batch 40/64 loss: -0.9892888069152832
Batch 41/64 loss: -0.8979067802429199
Batch 42/64 loss: -0.7982568740844727
Batch 43/64 loss: -0.8102841377258301
Batch 44/64 loss: -0.7682723999023438
Batch 45/64 loss: -0.12658405303955078
Batch 46/64 loss: -0.800788402557373
Batch 47/64 loss: -0.9064469337463379
Batch 48/64 loss: -0.4440932273864746
Batch 49/64 loss: -0.8770923614501953
Batch 50/64 loss: -0.6797776222229004
Batch 51/64 loss: -0.9336233139038086
Batch 52/64 loss: -0.8122391700744629
Batch 53/64 loss: -0.6518635749816895
Batch 54/64 loss: -0.9412350654602051
Batch 55/64 loss: -0.9424147605895996
Batch 56/64 loss: -0.7856731414794922
Batch 57/64 loss: -0.8566761016845703
Batch 58/64 loss: -0.9413719177246094
Batch 59/64 loss: -0.8552851676940918
Batch 60/64 loss: -0.8738589286804199
Batch 61/64 loss: -0.8176846504211426
Batch 62/64 loss: -0.8604955673217773
Batch 63/64 loss: -0.9611783027648926
Batch 64/64 loss: -4.470160961151123
Epoch 497  Train loss: -0.7428332328796386  Val loss: -1.1527945003968334
Epoch 498
-------------------------------
Batch 1/64 loss: -0.9079036712646484
Batch 2/64 loss: -1.0185389518737793
Batch 3/64 loss: -0.16312026977539062
Batch 4/64 loss: -0.9439501762390137
Batch 5/64 loss: -0.9217367172241211
Batch 6/64 loss: -0.901710033416748
Batch 7/64 loss: -0.7126684188842773
Batch 8/64 loss: -0.8036847114562988
Batch 9/64 loss: -0.8398680686950684
Batch 10/64 loss: -0.8644914627075195
Batch 11/64 loss: -0.9137845039367676
Batch 12/64 loss: -0.8122916221618652
Batch 13/64 loss: -0.677283763885498
Batch 14/64 loss: -0.9277181625366211
Batch 15/64 loss: -0.7031683921813965
Batch 16/64 loss: -0.15251398086547852
Batch 17/64 loss: -1.0221710205078125
Batch 18/64 loss: -0.8925604820251465
Batch 19/64 loss: -1.0130743980407715
Batch 20/64 loss: -0.8634753227233887
Batch 21/64 loss: 0.653770923614502
Batch 22/64 loss: -0.6859908103942871
Batch 23/64 loss: -1.0194597244262695
Batch 24/64 loss: -0.8077645301818848
Batch 25/64 loss: -0.9265270233154297
Batch 26/64 loss: -0.9359183311462402
Batch 27/64 loss: -0.7789506912231445
Batch 28/64 loss: -0.8934078216552734
Batch 29/64 loss: -0.8921847343444824
Batch 30/64 loss: -0.8296804428100586
Batch 31/64 loss: -0.7978878021240234
Batch 32/64 loss: -0.8514175415039062
Batch 33/64 loss: -0.9638781547546387
Batch 34/64 loss: -0.6943435668945312
Batch 35/64 loss: -0.8557147979736328
Batch 36/64 loss: -0.884188175201416
Batch 37/64 loss: -0.6756601333618164
Batch 38/64 loss: -0.8204598426818848
Batch 39/64 loss: -0.8591585159301758
Batch 40/64 loss: -0.5820283889770508
Batch 41/64 loss: -0.6444792747497559
Batch 42/64 loss: -0.4679403305053711
Batch 43/64 loss: -0.6619391441345215
Batch 44/64 loss: -0.7595052719116211
Batch 45/64 loss: -0.8075747489929199
Batch 46/64 loss: -0.6732172966003418
Batch 47/64 loss: -0.6679983139038086
Batch 48/64 loss: -0.45613765716552734
Batch 49/64 loss: -0.8502488136291504
Batch 50/64 loss: -0.8991837501525879
Batch 51/64 loss: -0.9569501876831055
Batch 52/64 loss: -0.887260913848877
Batch 53/64 loss: -0.7785663604736328
Batch 54/64 loss: -0.7178592681884766
Batch 55/64 loss: 0.6690831184387207
Batch 56/64 loss: -0.747650146484375
Batch 57/64 loss: -0.7869067192077637
Batch 58/64 loss: 0.10620403289794922
Batch 59/64 loss: -0.9238243103027344
Batch 60/64 loss: -0.6620397567749023
Batch 61/64 loss: -0.9295144081115723
Batch 62/64 loss: -0.8586053848266602
Batch 63/64 loss: -0.8230047225952148
Batch 64/64 loss: -4.453989028930664
Epoch 498  Train loss: -0.7793282452751609  Val loss: -1.103524630831689
Epoch 499
-------------------------------
Batch 1/64 loss: -0.8619551658630371
Batch 2/64 loss: -0.893394947052002
Batch 3/64 loss: -0.7595658302307129
Batch 4/64 loss: -0.8206424713134766
Batch 5/64 loss: -0.7252182960510254
Batch 6/64 loss: -0.7800736427307129
Batch 7/64 loss: -0.7390279769897461
Batch 8/64 loss: -0.8133401870727539
Batch 9/64 loss: -0.9236927032470703
Batch 10/64 loss: -0.9287252426147461
Batch 11/64 loss: -0.7815999984741211
Batch 12/64 loss: 0.10239219665527344
Batch 13/64 loss: -0.549713134765625
Batch 14/64 loss: -0.8515567779541016
Batch 15/64 loss: -0.9506020545959473
Batch 16/64 loss: -0.9020242691040039
Batch 17/64 loss: -0.10976600646972656
Batch 18/64 loss: -0.8898448944091797
Batch 19/64 loss: -0.9340729713439941
Batch 20/64 loss: -0.9086012840270996
Batch 21/64 loss: -0.9673013687133789
Batch 22/64 loss: -0.3258066177368164
Batch 23/64 loss: -0.7927660942077637
Batch 24/64 loss: -0.36244821548461914
Batch 25/64 loss: -0.8328046798706055
Batch 26/64 loss: 0.6363015174865723
Batch 27/64 loss: -0.636265754699707
Batch 28/64 loss: -0.8767485618591309
Batch 29/64 loss: -0.7556242942810059
Batch 30/64 loss: -0.7292890548706055
Batch 31/64 loss: -0.7078065872192383
Batch 32/64 loss: -0.729496955871582
Batch 33/64 loss: -0.85247802734375
Batch 34/64 loss: -0.8884525299072266
Batch 35/64 loss: -0.8468227386474609
Batch 36/64 loss: -0.7375802993774414
Batch 37/64 loss: -0.8963813781738281
Batch 38/64 loss: -0.7798099517822266
Batch 39/64 loss: -0.7947726249694824
Batch 40/64 loss: -0.7748456001281738
Batch 41/64 loss: -0.8364109992980957
Batch 42/64 loss: -0.7669219970703125
Batch 43/64 loss: -0.9376082420349121
Batch 44/64 loss: -0.8818950653076172
Batch 45/64 loss: -0.8801393508911133
Batch 46/64 loss: -0.7354936599731445
Batch 47/64 loss: -0.9369239807128906
Batch 48/64 loss: -0.9951963424682617
Batch 49/64 loss: -0.8950066566467285
Batch 50/64 loss: -0.9901857376098633
Batch 51/64 loss: -0.9143843650817871
Batch 52/64 loss: -0.9740152359008789
Batch 53/64 loss: 0.3468918800354004
Batch 54/64 loss: -0.970026969909668
Batch 55/64 loss: -1.003981590270996
Batch 56/64 loss: -0.9830055236816406
Batch 57/64 loss: -0.589789867401123
Batch 58/64 loss: -0.7900795936584473
Batch 59/64 loss: -0.9031200408935547
Batch 60/64 loss: -0.8655276298522949
Batch 61/64 loss: -0.7674746513366699
Batch 62/64 loss: -0.7425765991210938
Batch 63/64 loss: -0.9512724876403809
Batch 64/64 loss: -4.741837978363037
Epoch 499  Train loss: -0.8030235084832883  Val loss: -1.1502337111640222
Epoch 500
-------------------------------
Batch 1/64 loss: -0.8186888694763184
Batch 2/64 loss: -0.7450981140136719
Batch 3/64 loss: -0.9543180465698242
Batch 4/64 loss: -0.8445658683776855
Batch 5/64 loss: -0.9718270301818848
Batch 6/64 loss: -0.8308582305908203
Batch 7/64 loss: -0.8974437713623047
Batch 8/64 loss: -0.8592467308044434
Batch 9/64 loss: -0.9481406211853027
Batch 10/64 loss: -0.9962625503540039
Batch 11/64 loss: -0.2230238914489746
Batch 12/64 loss: -0.8221263885498047
Batch 13/64 loss: -0.8849086761474609
Batch 14/64 loss: -0.7237348556518555
Batch 15/64 loss: -1.0691032409667969
Batch 16/64 loss: -0.6545286178588867
Batch 17/64 loss: 0.35367918014526367
Batch 18/64 loss: -1.1257834434509277
Batch 19/64 loss: -0.7971105575561523
Batch 20/64 loss: -0.76544189453125
Batch 21/64 loss: -0.8425836563110352
Batch 22/64 loss: 0.5461363792419434
Batch 23/64 loss: -0.9403719902038574
Batch 24/64 loss: -0.9168691635131836
Batch 25/64 loss: -0.8188657760620117
Batch 26/64 loss: -0.790740966796875
Batch 27/64 loss: -0.38589000701904297
Batch 28/64 loss: -0.8824372291564941
Batch 29/64 loss: -0.843592643737793
Batch 30/64 loss: -0.8316307067871094
Batch 31/64 loss: -0.9370875358581543
Batch 32/64 loss: -1.0108165740966797
Batch 33/64 loss: -0.9908280372619629
Batch 34/64 loss: -1.0236144065856934
Batch 35/64 loss: -0.9041604995727539
Batch 36/64 loss: -0.9317092895507812
Batch 37/64 loss: 0.12664508819580078
Batch 38/64 loss: -0.9897317886352539
Batch 39/64 loss: -0.9669694900512695
Batch 40/64 loss: -0.8438973426818848
Batch 41/64 loss: -0.9452838897705078
Batch 42/64 loss: -1.039721965789795
Batch 43/64 loss: -0.9116878509521484
Batch 44/64 loss: -0.8824844360351562
Batch 45/64 loss: -0.9476766586303711
Batch 46/64 loss: -1.0308756828308105
Batch 47/64 loss: -0.7826147079467773
Batch 48/64 loss: -0.6422405242919922
Batch 49/64 loss: -0.9672245979309082
Batch 50/64 loss: -0.9171781539916992
Batch 51/64 loss: -1.0595426559448242
Batch 52/64 loss: -0.33239078521728516
Batch 53/64 loss: -0.8477988243103027
Batch 54/64 loss: -1.0314898490905762
Batch 55/64 loss: -0.5363712310791016
Batch 56/64 loss: -0.9318442344665527
Batch 57/64 loss: -1.024369239807129
Batch 58/64 loss: -0.8161959648132324
Batch 59/64 loss: -0.932136058807373
Batch 60/64 loss: -0.9465022087097168
Batch 61/64 loss: -0.9788150787353516
Batch 62/64 loss: -0.7070460319519043
Batch 63/64 loss: -0.8426899909973145
Batch 64/64 loss: -4.449804306030273
Epoch 500  Train loss: -0.8493973601098154  Val loss: -1.2340851616613644
Saving best model, epoch: 500
SLIC undersegmentation error: 0.09655395189003434
SLIC inter-cluster variation: 0.08782060068082356
SLIC number of superpixels: 38426
SLIC superpixels per image: 132.04810996563575
Model loaded
Test metrics:
-1.7810372421421956 0.28718762886597937 12.272581389317923 tensor(0.1689, dtype=torch.float64) 0.644928990257062 2.508327202193833 44853
Inference time: 0.003996252603956924 seconds
Relabeled undersegmentation error: 0.08920687285223369
Relabeled inter-cluster variation: 0.04466029941992921
Relabeled mean superpixels count: 386.6151202749141
Original mean superpixels count: 154.15463917525773
Done!
Job id: 488560
Job id: 492268
