Started preprocessing dataset
Number of training samples: 2040
Number of validation samples: 582
Number of testing samples: 291
Using cuda device
Epoch 1
-------------------------------
Batch 1/64 loss: 0.4366331100463867
Batch 2/64 loss: 0.36095476150512695
Batch 3/64 loss: 0.3419675827026367
Batch 4/64 loss: 0.33046144247055054
Batch 5/64 loss: 0.32818710803985596
Batch 6/64 loss: 0.32456910610198975
Batch 7/64 loss: 0.32181787490844727
Batch 8/64 loss: 0.3200256824493408
Batch 9/64 loss: 0.3222705125808716
Batch 10/64 loss: 0.3221226930618286
Batch 11/64 loss: 0.32324206829071045
Batch 12/64 loss: 0.3141440749168396
Batch 13/64 loss: 0.3134436011314392
Batch 14/64 loss: 0.3160663843154907
Batch 15/64 loss: 0.3158130645751953
Batch 16/64 loss: 0.31449806690216064
Batch 17/64 loss: 0.31219935417175293
Batch 18/64 loss: 0.3063175082206726
Batch 19/64 loss: 0.306884229183197
Batch 20/64 loss: 0.305782675743103
Batch 21/64 loss: 0.308566689491272
Batch 22/64 loss: 0.3068404197692871
Batch 23/64 loss: 0.29674357175827026
Batch 24/64 loss: 0.3040311336517334
Batch 25/64 loss: 0.2993294596672058
Batch 26/64 loss: 0.3050050735473633
Batch 27/64 loss: 0.30049699544906616
Batch 28/64 loss: 0.2966368794441223
Batch 29/64 loss: 0.30174559354782104
Batch 30/64 loss: 0.3005755543708801
Batch 31/64 loss: 0.2944645881652832
Batch 32/64 loss: 0.2958805561065674
Batch 33/64 loss: 0.2858104705810547
Batch 34/64 loss: 0.2921527028083801
Batch 35/64 loss: 0.2964935302734375
Batch 36/64 loss: 0.2892591953277588
Batch 37/64 loss: 0.29150450229644775
Batch 38/64 loss: 0.2962958812713623
Batch 39/64 loss: 0.2899669408798218
Batch 40/64 loss: 0.2947530150413513
Batch 41/64 loss: 0.2852884531021118
Batch 42/64 loss: 0.2816030979156494
Batch 43/64 loss: 0.2775647044181824
Batch 44/64 loss: 0.2801929712295532
Batch 45/64 loss: 0.2807077169418335
Batch 46/64 loss: 0.28119730949401855
Batch 47/64 loss: 0.28547030687332153
Batch 48/64 loss: 0.28269684314727783
Batch 49/64 loss: 0.28213322162628174
Batch 50/64 loss: 0.28509700298309326
Batch 51/64 loss: 0.27935516834259033
Batch 52/64 loss: 0.2751878499984741
Batch 53/64 loss: 0.28612709045410156
Batch 54/64 loss: 0.28642159700393677
Batch 55/64 loss: 0.27828311920166016
Batch 56/64 loss: 0.27828317880630493
Batch 57/64 loss: 0.2826763391494751
Batch 58/64 loss: 0.27852314710617065
Batch 59/64 loss: 0.2833341360092163
Batch 60/64 loss: 0.27326422929763794
Batch 61/64 loss: 0.2907438278198242
Batch 62/64 loss: 0.28371143341064453
Batch 63/64 loss: 0.28397512435913086
Batch 64/64 loss: 0.28134745359420776
Epoch 1  Train loss: 0.30081258217493695  Val loss: 0.2908709729250354
Saving best model, epoch: 1
Epoch 2
-------------------------------
Batch 1/64 loss: 0.27761316299438477
Batch 2/64 loss: 0.27276283502578735
Batch 3/64 loss: 0.27728527784347534
Batch 4/64 loss: 0.2686249017715454
Batch 5/64 loss: 0.2702004909515381
Batch 6/64 loss: 0.270407497882843
Batch 7/64 loss: 0.27813732624053955
Batch 8/64 loss: 0.2773687243461609
Batch 9/64 loss: 0.2785945534706116
Batch 10/64 loss: 0.26140308380126953
Batch 11/64 loss: 0.28129684925079346
Batch 12/64 loss: 0.27786093950271606
Batch 13/64 loss: 0.26656031608581543
Batch 14/64 loss: 0.2719264030456543
Batch 15/64 loss: 0.2660250663757324
Batch 16/64 loss: 0.27383047342300415
Batch 17/64 loss: 0.2737959027290344
Batch 18/64 loss: 0.2617796063423157
Batch 19/64 loss: 0.26082825660705566
Batch 20/64 loss: 0.2720174789428711
Batch 21/64 loss: 0.2667912244796753
Batch 22/64 loss: 0.27314460277557373
Batch 23/64 loss: 0.26137590408325195
Batch 24/64 loss: 0.27306222915649414
Batch 25/64 loss: 0.27193892002105713
Batch 26/64 loss: 0.25828444957733154
Batch 27/64 loss: 0.25785815715789795
Batch 28/64 loss: 0.25515687465667725
Batch 29/64 loss: 0.26058852672576904
Batch 30/64 loss: 0.2570940852165222
Batch 31/64 loss: 0.2585017681121826
Batch 32/64 loss: 0.255171537399292
Batch 33/64 loss: 0.2663196325302124
Batch 34/64 loss: 0.2710050940513611
Batch 35/64 loss: 0.2515857219696045
Batch 36/64 loss: 0.26157212257385254
Batch 37/64 loss: 0.2705689072608948
Batch 38/64 loss: 0.2564011812210083
Batch 39/64 loss: 0.26290708780288696
Batch 40/64 loss: 0.2523936629295349
Batch 41/64 loss: 0.2631007432937622
Batch 42/64 loss: 0.25949835777282715
Batch 43/64 loss: 0.2522132992744446
Batch 44/64 loss: 0.26061397790908813
Batch 45/64 loss: 0.2590280771255493
Batch 46/64 loss: 0.25646311044692993
Batch 47/64 loss: 0.2613787055015564
Batch 48/64 loss: 0.25744199752807617
Batch 49/64 loss: 0.2549196481704712
Batch 50/64 loss: 0.24943208694458008
Batch 51/64 loss: 0.2595219612121582
Batch 52/64 loss: 0.26065731048583984
Batch 53/64 loss: 0.25900888442993164
Batch 54/64 loss: 0.2514995336532593
Batch 55/64 loss: 0.24555730819702148
Batch 56/64 loss: 0.2575639486312866
Batch 57/64 loss: 0.25274914503097534
Batch 58/64 loss: 0.24382781982421875
Batch 59/64 loss: 0.256516695022583
Batch 60/64 loss: 0.2600446939468384
Batch 61/64 loss: 0.24362200498580933
Batch 62/64 loss: 0.25222641229629517
Batch 63/64 loss: 0.2543659210205078
Batch 64/64 loss: 0.24810868501663208
Epoch 2  Train loss: 0.2625470430243249  Val loss: 0.262903396616277
Saving best model, epoch: 2
Epoch 3
-------------------------------
Batch 1/64 loss: 0.24402570724487305
Batch 2/64 loss: 0.24570125341415405
Batch 3/64 loss: 0.24446392059326172
Batch 4/64 loss: 0.254477858543396
Batch 5/64 loss: 0.24202895164489746
Batch 6/64 loss: 0.23913687467575073
Batch 7/64 loss: 0.2524442672729492
Batch 8/64 loss: 0.25399619340896606
Batch 9/64 loss: 0.24441570043563843
Batch 10/64 loss: 0.24783575534820557
Batch 11/64 loss: 0.24240422248840332
Batch 12/64 loss: 0.2540331482887268
Batch 13/64 loss: 0.23679041862487793
Batch 14/64 loss: 0.25155574083328247
Batch 15/64 loss: 0.24977439641952515
Batch 16/64 loss: 0.24363934993743896
Batch 17/64 loss: 0.24458885192871094
Batch 18/64 loss: 0.2323896884918213
Batch 19/64 loss: 0.25661176443099976
Batch 20/64 loss: 0.24700641632080078
Batch 21/64 loss: 0.23793840408325195
Batch 22/64 loss: 0.23177307844161987
Batch 23/64 loss: 0.24250918626785278
Batch 24/64 loss: 0.23761826753616333
Batch 25/64 loss: 0.23658603429794312
Batch 26/64 loss: 0.26186490058898926
Batch 27/64 loss: 0.25049906969070435
Batch 28/64 loss: 0.2422724962234497
Batch 29/64 loss: 0.2457904815673828
Batch 30/64 loss: 0.2394741177558899
Batch 31/64 loss: 0.24183636903762817
Batch 32/64 loss: 0.23686200380325317
Batch 33/64 loss: 0.24664562940597534
Batch 34/64 loss: 0.24312448501586914
Batch 35/64 loss: 0.2444833517074585
Batch 36/64 loss: 0.24754667282104492
Batch 37/64 loss: 0.23934835195541382
Batch 38/64 loss: 0.24706685543060303
Batch 39/64 loss: 0.24889016151428223
Batch 40/64 loss: 0.2418529987335205
Batch 41/64 loss: 0.24108600616455078
Batch 42/64 loss: 0.2501612901687622
Batch 43/64 loss: 0.23481225967407227
Batch 44/64 loss: 0.23442953824996948
Batch 45/64 loss: 0.22681677341461182
Batch 46/64 loss: 0.24508249759674072
Batch 47/64 loss: 0.2444087266921997
Batch 48/64 loss: 0.24429500102996826
Batch 49/64 loss: 0.23695862293243408
Batch 50/64 loss: 0.23601776361465454
Batch 51/64 loss: 0.23478972911834717
Batch 52/64 loss: 0.24155563116073608
Batch 53/64 loss: 0.23620831966400146
Batch 54/64 loss: 0.24374616146087646
Batch 55/64 loss: 0.2455683946609497
Batch 56/64 loss: 0.23310160636901855
Batch 57/64 loss: 0.23613697290420532
Batch 58/64 loss: 0.2467806339263916
Batch 59/64 loss: 0.2397218942642212
Batch 60/64 loss: 0.24818682670593262
Batch 61/64 loss: 0.2481689453125
Batch 62/64 loss: 0.23755478858947754
Batch 63/64 loss: 0.24143075942993164
Batch 64/64 loss: 0.2349417805671692
Epoch 3  Train loss: 0.24305143332948873  Val loss: 0.24604630593172053
Saving best model, epoch: 3
Epoch 4
-------------------------------
Batch 1/64 loss: 0.2251148819923401
Batch 2/64 loss: 0.25053977966308594
Batch 3/64 loss: 0.23672783374786377
Batch 4/64 loss: 0.2364693284034729
Batch 5/64 loss: 0.23329460620880127
Batch 6/64 loss: 0.2256453037261963
Batch 7/64 loss: 0.23441314697265625
Batch 8/64 loss: 0.24125194549560547
Batch 9/64 loss: 0.23435240983963013
Batch 10/64 loss: 0.23618298768997192
Batch 11/64 loss: 0.23637020587921143
Batch 12/64 loss: 0.24357515573501587
Batch 13/64 loss: 0.25149106979370117
Batch 14/64 loss: 0.23565995693206787
Batch 15/64 loss: 0.24424582719802856
Batch 16/64 loss: 0.2262275218963623
Batch 17/64 loss: 0.236497163772583
Batch 18/64 loss: 0.23023074865341187
Batch 19/64 loss: 0.231747567653656
Batch 20/64 loss: 0.23697227239608765
Batch 21/64 loss: 0.23201793432235718
Batch 22/64 loss: 0.22946643829345703
Batch 23/64 loss: 0.23083561658859253
Batch 24/64 loss: 0.2237759828567505
Batch 25/64 loss: 0.23249757289886475
Batch 26/64 loss: 0.24062442779541016
Batch 27/64 loss: 0.22963428497314453
Batch 28/64 loss: 0.23775428533554077
Batch 29/64 loss: 0.23292303085327148
Batch 30/64 loss: 0.22085392475128174
Batch 31/64 loss: 0.2278972864151001
Batch 32/64 loss: 0.22971093654632568
Batch 33/64 loss: 0.23825430870056152
Batch 34/64 loss: 0.23757171630859375
Batch 35/64 loss: 0.23344743251800537
Batch 36/64 loss: 0.21621710062026978
Batch 37/64 loss: 0.22833800315856934
Batch 38/64 loss: 0.22010648250579834
Batch 39/64 loss: 0.2373563051223755
Batch 40/64 loss: 0.23774218559265137
Batch 41/64 loss: 0.2304013967514038
Batch 42/64 loss: 0.21669507026672363
Batch 43/64 loss: 0.2401566505432129
Batch 44/64 loss: 0.23413288593292236
Batch 45/64 loss: 0.23055493831634521
Batch 46/64 loss: 0.22687125205993652
Batch 47/64 loss: 0.22723424434661865
Batch 48/64 loss: 0.23501932621002197
Batch 49/64 loss: 0.239557147026062
Batch 50/64 loss: 0.2345367670059204
Batch 51/64 loss: 0.23174512386322021
Batch 52/64 loss: 0.240062415599823
Batch 53/64 loss: 0.2369047999382019
Batch 54/64 loss: 0.23144567012786865
Batch 55/64 loss: 0.23142218589782715
Batch 56/64 loss: 0.23136258125305176
Batch 57/64 loss: 0.23003661632537842
Batch 58/64 loss: 0.23842531442642212
Batch 59/64 loss: 0.21097040176391602
Batch 60/64 loss: 0.2190811038017273
Batch 61/64 loss: 0.22346043586730957
Batch 62/64 loss: 0.22944337129592896
Batch 63/64 loss: 0.2539989948272705
Batch 64/64 loss: 0.23256957530975342
Epoch 4  Train loss: 0.23281538579978195  Val loss: 0.24485994286553556
Saving best model, epoch: 4
Epoch 5
-------------------------------
Batch 1/64 loss: 0.2337881326675415
Batch 2/64 loss: 0.23165106773376465
Batch 3/64 loss: 0.2200738787651062
Batch 4/64 loss: 0.23346710205078125
Batch 5/64 loss: 0.22291350364685059
Batch 6/64 loss: 0.21962976455688477
Batch 7/64 loss: 0.2176939845085144
Batch 8/64 loss: 0.225935161113739
Batch 9/64 loss: 0.22816413640975952
Batch 10/64 loss: 0.22822868824005127
Batch 11/64 loss: 0.22625041007995605
Batch 12/64 loss: 0.22572308778762817
Batch 13/64 loss: 0.22171318531036377
Batch 14/64 loss: 0.22479009628295898
Batch 15/64 loss: 0.23197436332702637
Batch 16/64 loss: 0.2194843292236328
Batch 17/64 loss: 0.21519207954406738
Batch 18/64 loss: 0.22235524654388428
Batch 19/64 loss: 0.2180384397506714
Batch 20/64 loss: 0.24385762214660645
Batch 21/64 loss: 0.2262864112854004
Batch 22/64 loss: 0.22079885005950928
Batch 23/64 loss: 0.22440630197525024
Batch 24/64 loss: 0.22080570459365845
Batch 25/64 loss: 0.22425979375839233
Batch 26/64 loss: 0.20793664455413818
Batch 27/64 loss: 0.22114062309265137
Batch 28/64 loss: 0.2228296995162964
Batch 29/64 loss: 0.2229936122894287
Batch 30/64 loss: 0.22739732265472412
Batch 31/64 loss: 0.2335911989212036
Batch 32/64 loss: 0.2206067442893982
Batch 33/64 loss: 0.21537190675735474
Batch 34/64 loss: 0.2229265570640564
Batch 35/64 loss: 0.23911648988723755
Batch 36/64 loss: 0.2222609519958496
Batch 37/64 loss: 0.21031248569488525
Batch 38/64 loss: 0.22658348083496094
Batch 39/64 loss: 0.2247227430343628
Batch 40/64 loss: 0.22919297218322754
Batch 41/64 loss: 0.22941726446151733
Batch 42/64 loss: 0.22149658203125
Batch 43/64 loss: 0.231711745262146
Batch 44/64 loss: 0.21395903825759888
Batch 45/64 loss: 0.22314918041229248
Batch 46/64 loss: 0.21156418323516846
Batch 47/64 loss: 0.21816551685333252
Batch 48/64 loss: 0.24049097299575806
Batch 49/64 loss: 0.2128899097442627
Batch 50/64 loss: 0.23013520240783691
Batch 51/64 loss: 0.21678674221038818
Batch 52/64 loss: 0.22106802463531494
Batch 53/64 loss: 0.22000575065612793
Batch 54/64 loss: 0.21964967250823975
Batch 55/64 loss: 0.2268596887588501
Batch 56/64 loss: 0.2269466519355774
Batch 57/64 loss: 0.2247697114944458
Batch 58/64 loss: 0.24561047554016113
Batch 59/64 loss: 0.22547322511672974
Batch 60/64 loss: 0.2222057580947876
Batch 61/64 loss: 0.23910361528396606
Batch 62/64 loss: 0.21583670377731323
Batch 63/64 loss: 0.22831439971923828
Batch 64/64 loss: 0.2075178623199463
Epoch 5  Train loss: 0.2242464813531614  Val loss: 0.23060428891394966
Saving best model, epoch: 5
Epoch 6
-------------------------------
Batch 1/64 loss: 0.21919381618499756
Batch 2/64 loss: 0.21730148792266846
Batch 3/64 loss: 0.2312474250793457
Batch 4/64 loss: 0.21137797832489014
Batch 5/64 loss: 0.22499287128448486
Batch 6/64 loss: 0.2052798867225647
Batch 7/64 loss: 0.2385038137435913
Batch 8/64 loss: 0.22441768646240234
Batch 9/64 loss: 0.22491717338562012
Batch 10/64 loss: 0.21857577562332153
Batch 11/64 loss: 0.2129671573638916
Batch 12/64 loss: 0.22007614374160767
Batch 13/64 loss: 0.21691536903381348
Batch 14/64 loss: 0.21305358409881592
Batch 15/64 loss: 0.21035605669021606
Batch 16/64 loss: 0.22096389532089233
Batch 17/64 loss: 0.21194541454315186
Batch 18/64 loss: 0.205189049243927
Batch 19/64 loss: 0.21077251434326172
Batch 20/64 loss: 0.20672303438186646
Batch 21/64 loss: 0.21016955375671387
Batch 22/64 loss: 0.22935748100280762
Batch 23/64 loss: 0.2316145896911621
Batch 24/64 loss: 0.2067546844482422
Batch 25/64 loss: 0.21090751886367798
Batch 26/64 loss: 0.20735353231430054
Batch 27/64 loss: 0.21576648950576782
Batch 28/64 loss: 0.21868950128555298
Batch 29/64 loss: 0.21915245056152344
Batch 30/64 loss: 0.21284997463226318
Batch 31/64 loss: 0.2154691219329834
Batch 32/64 loss: 0.23423588275909424
Batch 33/64 loss: 0.22092247009277344
Batch 34/64 loss: 0.21708309650421143
Batch 35/64 loss: 0.21659541130065918
Batch 36/64 loss: 0.22195124626159668
Batch 37/64 loss: 0.22298169136047363
Batch 38/64 loss: 0.21198952198028564
Batch 39/64 loss: 0.21129977703094482
Batch 40/64 loss: 0.20777863264083862
Batch 41/64 loss: 0.21851879358291626
Batch 42/64 loss: 0.209875226020813
Batch 43/64 loss: 0.22710055112838745
Batch 44/64 loss: 0.22269392013549805
Batch 45/64 loss: 0.21898376941680908
Batch 46/64 loss: 0.22129786014556885
Batch 47/64 loss: 0.22115778923034668
Batch 48/64 loss: 0.2061479091644287
Batch 49/64 loss: 0.2033516764640808
Batch 50/64 loss: 0.2089238166809082
Batch 51/64 loss: 0.2053855061531067
Batch 52/64 loss: 0.21583139896392822
Batch 53/64 loss: 0.21195363998413086
Batch 54/64 loss: 0.21286743879318237
Batch 55/64 loss: 0.21126240491867065
Batch 56/64 loss: 0.2176571488380432
Batch 57/64 loss: 0.20801043510437012
Batch 58/64 loss: 0.21232616901397705
Batch 59/64 loss: 0.22462713718414307
Batch 60/64 loss: 0.23529541492462158
Batch 61/64 loss: 0.21099627017974854
Batch 62/64 loss: 0.20695757865905762
Batch 63/64 loss: 0.20671898126602173
Batch 64/64 loss: 0.2148098349571228
Epoch 6  Train loss: 0.21626260350732243  Val loss: 0.22407428628390597
Saving best model, epoch: 6
Epoch 7
-------------------------------
Batch 1/64 loss: 0.19607830047607422
Batch 2/64 loss: 0.20591813325881958
Batch 3/64 loss: 0.20512515306472778
Batch 4/64 loss: 0.21268093585968018
Batch 5/64 loss: 0.20703595876693726
Batch 6/64 loss: 0.21598637104034424
Batch 7/64 loss: 0.22052574157714844
Batch 8/64 loss: 0.2250194549560547
Batch 9/64 loss: 0.2092219591140747
Batch 10/64 loss: 0.21294760704040527
Batch 11/64 loss: 0.20828545093536377
Batch 12/64 loss: 0.20556539297103882
Batch 13/64 loss: 0.22010111808776855
Batch 14/64 loss: 0.21865630149841309
Batch 15/64 loss: 0.2203858494758606
Batch 16/64 loss: 0.21404200792312622
Batch 17/64 loss: 0.20848804712295532
Batch 18/64 loss: 0.2015514373779297
Batch 19/64 loss: 0.21675288677215576
Batch 20/64 loss: 0.20787298679351807
Batch 21/64 loss: 0.20743286609649658
Batch 22/64 loss: 0.21285176277160645
Batch 23/64 loss: 0.21306800842285156
Batch 24/64 loss: 0.22861194610595703
Batch 25/64 loss: 0.21188616752624512
Batch 26/64 loss: 0.21096700429916382
Batch 27/64 loss: 0.21447503566741943
Batch 28/64 loss: 0.19865137338638306
Batch 29/64 loss: 0.2180492877960205
Batch 30/64 loss: 0.22421938180923462
Batch 31/64 loss: 0.21036607027053833
Batch 32/64 loss: 0.20344209671020508
Batch 33/64 loss: 0.21878564357757568
Batch 34/64 loss: 0.20494627952575684
Batch 35/64 loss: 0.19823122024536133
Batch 36/64 loss: 0.21739166975021362
Batch 37/64 loss: 0.1900305151939392
Batch 38/64 loss: 0.20728135108947754
Batch 39/64 loss: 0.20739102363586426
Batch 40/64 loss: 0.2050979733467102
Batch 41/64 loss: 0.21120142936706543
Batch 42/64 loss: 0.2038872241973877
Batch 43/64 loss: 0.21011143922805786
Batch 44/64 loss: 0.20546340942382812
Batch 45/64 loss: 0.20763278007507324
Batch 46/64 loss: 0.20958483219146729
Batch 47/64 loss: 0.21097350120544434
Batch 48/64 loss: 0.2085946798324585
Batch 49/64 loss: 0.21279388666152954
Batch 50/64 loss: 0.21862506866455078
Batch 51/64 loss: 0.2021973729133606
Batch 52/64 loss: 0.20643413066864014
Batch 53/64 loss: 0.21258127689361572
Batch 54/64 loss: 0.1998317837715149
Batch 55/64 loss: 0.21341651678085327
Batch 56/64 loss: 0.21914279460906982
Batch 57/64 loss: 0.20142972469329834
Batch 58/64 loss: 0.22479212284088135
Batch 59/64 loss: 0.1914994716644287
Batch 60/64 loss: 0.20039969682693481
Batch 61/64 loss: 0.2222745418548584
Batch 62/64 loss: 0.2167530059814453
Batch 63/64 loss: 0.22533166408538818
Batch 64/64 loss: 0.2087823748588562
Epoch 7  Train loss: 0.21061893187317193  Val loss: 0.22035859048981027
Saving best model, epoch: 7
Epoch 8
-------------------------------
Batch 1/64 loss: 0.21205610036849976
Batch 2/64 loss: 0.20399343967437744
Batch 3/64 loss: 0.20945823192596436
Batch 4/64 loss: 0.2013310194015503
Batch 5/64 loss: 0.1934911012649536
Batch 6/64 loss: 0.20498251914978027
Batch 7/64 loss: 0.21716755628585815
Batch 8/64 loss: 0.2092043161392212
Batch 9/64 loss: 0.2157740592956543
Batch 10/64 loss: 0.1940450668334961
Batch 11/64 loss: 0.21286296844482422
Batch 12/64 loss: 0.20040661096572876
Batch 13/64 loss: 0.2037193775177002
Batch 14/64 loss: 0.1961570382118225
Batch 15/64 loss: 0.21045446395874023
Batch 16/64 loss: 0.2128283977508545
Batch 17/64 loss: 0.2071695327758789
Batch 18/64 loss: 0.19975942373275757
Batch 19/64 loss: 0.1949753761291504
Batch 20/64 loss: 0.19419652223587036
Batch 21/64 loss: 0.20716798305511475
Batch 22/64 loss: 0.1987534761428833
Batch 23/64 loss: 0.1915426254272461
Batch 24/64 loss: 0.19197845458984375
Batch 25/64 loss: 0.20529919862747192
Batch 26/64 loss: 0.1940556764602661
Batch 27/64 loss: 0.2026156783103943
Batch 28/64 loss: 0.2108372449874878
Batch 29/64 loss: 0.20557737350463867
Batch 30/64 loss: 0.19613349437713623
Batch 31/64 loss: 0.2124398946762085
Batch 32/64 loss: 0.2009761929512024
Batch 33/64 loss: 0.20801615715026855
Batch 34/64 loss: 0.20227879285812378
Batch 35/64 loss: 0.20565521717071533
Batch 36/64 loss: 0.19078809022903442
Batch 37/64 loss: 0.20024335384368896
Batch 38/64 loss: 0.20425879955291748
Batch 39/64 loss: 0.2147449254989624
Batch 40/64 loss: 0.20660871267318726
Batch 41/64 loss: 0.19687896966934204
Batch 42/64 loss: 0.20357555150985718
Batch 43/64 loss: 0.19281333684921265
Batch 44/64 loss: 0.19625675678253174
Batch 45/64 loss: 0.18741482496261597
Batch 46/64 loss: 0.1962164044380188
Batch 47/64 loss: 0.19675040245056152
Batch 48/64 loss: 0.1861499547958374
Batch 49/64 loss: 0.1955258846282959
Batch 50/64 loss: 0.19704663753509521
Batch 51/64 loss: 0.19701355695724487
Batch 52/64 loss: 0.1978597640991211
Batch 53/64 loss: 0.19702959060668945
Batch 54/64 loss: 0.2095479965209961
Batch 55/64 loss: 0.19919824600219727
Batch 56/64 loss: 0.20856642723083496
Batch 57/64 loss: 0.19236314296722412
Batch 58/64 loss: 0.19216030836105347
Batch 59/64 loss: 0.18809425830841064
Batch 60/64 loss: 0.19936680793762207
Batch 61/64 loss: 0.19526022672653198
Batch 62/64 loss: 0.20265012979507446
Batch 63/64 loss: 0.22074973583221436
Batch 64/64 loss: 0.1988961100578308
Epoch 8  Train loss: 0.20143788959465775  Val loss: 0.21038159512981927
Saving best model, epoch: 8
Epoch 9
-------------------------------
Batch 1/64 loss: 0.1897345781326294
Batch 2/64 loss: 0.20012742280960083
Batch 3/64 loss: 0.22429007291793823
Batch 4/64 loss: 0.2047109603881836
Batch 5/64 loss: 0.20719993114471436
Batch 6/64 loss: 0.18829196691513062
Batch 7/64 loss: 0.20726311206817627
Batch 8/64 loss: 0.19527089595794678
Batch 9/64 loss: 0.19086217880249023
Batch 10/64 loss: 0.20319247245788574
Batch 11/64 loss: 0.19143807888031006
Batch 12/64 loss: 0.19806408882141113
Batch 13/64 loss: 0.20196610689163208
Batch 14/64 loss: 0.19897711277008057
Batch 15/64 loss: 0.19687998294830322
Batch 16/64 loss: 0.1983957290649414
Batch 17/64 loss: 0.20102906227111816
Batch 18/64 loss: 0.19869685173034668
Batch 19/64 loss: 0.1924799680709839
Batch 20/64 loss: 0.18840956687927246
Batch 21/64 loss: 0.17892706394195557
Batch 22/64 loss: 0.19539260864257812
Batch 23/64 loss: 0.1845637559890747
Batch 24/64 loss: 0.18191087245941162
Batch 25/64 loss: 0.19621586799621582
Batch 26/64 loss: 0.18696743249893188
Batch 27/64 loss: 0.20393896102905273
Batch 28/64 loss: 0.2155705690383911
Batch 29/64 loss: 0.18854546546936035
Batch 30/64 loss: 0.2049892544746399
Batch 31/64 loss: 0.19256675243377686
Batch 32/64 loss: 0.18518352508544922
Batch 33/64 loss: 0.1828247308731079
Batch 34/64 loss: 0.19058167934417725
Batch 35/64 loss: 0.19853723049163818
Batch 36/64 loss: 0.1775592565536499
Batch 37/64 loss: 0.20055580139160156
Batch 38/64 loss: 0.18501293659210205
Batch 39/64 loss: 0.20184826850891113
Batch 40/64 loss: 0.19183653593063354
Batch 41/64 loss: 0.1851082444190979
Batch 42/64 loss: 0.19545578956604004
Batch 43/64 loss: 0.18818867206573486
Batch 44/64 loss: 0.1990153193473816
Batch 45/64 loss: 0.205352783203125
Batch 46/64 loss: 0.19238829612731934
Batch 47/64 loss: 0.18816041946411133
Batch 48/64 loss: 0.20334237813949585
Batch 49/64 loss: 0.19899117946624756
Batch 50/64 loss: 0.18842875957489014
Batch 51/64 loss: 0.17335748672485352
Batch 52/64 loss: 0.1919475793838501
Batch 53/64 loss: 0.1808375120162964
Batch 54/64 loss: 0.19302165508270264
Batch 55/64 loss: 0.19047188758850098
Batch 56/64 loss: 0.19528591632843018
Batch 57/64 loss: 0.19121670722961426
Batch 58/64 loss: 0.18313312530517578
Batch 59/64 loss: 0.20243167877197266
Batch 60/64 loss: 0.17880761623382568
Batch 61/64 loss: 0.20103871822357178
Batch 62/64 loss: 0.19738531112670898
Batch 63/64 loss: 0.1974593997001648
Batch 64/64 loss: 0.19006383419036865
Epoch 9  Train loss: 0.19426169442195518  Val loss: 0.2023954133397525
Saving best model, epoch: 9
Epoch 10
-------------------------------
Batch 1/64 loss: 0.18313705921173096
Batch 2/64 loss: 0.18517529964447021
Batch 3/64 loss: 0.18083691596984863
Batch 4/64 loss: 0.1773592233657837
Batch 5/64 loss: 0.18844515085220337
Batch 6/64 loss: 0.19046860933303833
Batch 7/64 loss: 0.18844878673553467
Batch 8/64 loss: 0.20392554998397827
Batch 9/64 loss: 0.20022666454315186
Batch 10/64 loss: 0.18569177389144897
Batch 11/64 loss: 0.18309199810028076
Batch 12/64 loss: 0.19170749187469482
Batch 13/64 loss: 0.18062323331832886
Batch 14/64 loss: 0.17709386348724365
Batch 15/64 loss: 0.1799936294555664
Batch 16/64 loss: 0.19638240337371826
Batch 17/64 loss: 0.20400947332382202
Batch 18/64 loss: 0.18815171718597412
Batch 19/64 loss: 0.17867344617843628
Batch 20/64 loss: 0.18544846773147583
Batch 21/64 loss: 0.19360780715942383
Batch 22/64 loss: 0.1896631121635437
Batch 23/64 loss: 0.19215869903564453
Batch 24/64 loss: 0.17402362823486328
Batch 25/64 loss: 0.19096839427947998
Batch 26/64 loss: 0.1620173454284668
Batch 27/64 loss: 0.20978516340255737
Batch 28/64 loss: 0.1624208688735962
Batch 29/64 loss: 0.18689918518066406
Batch 30/64 loss: 0.18506163358688354
Batch 31/64 loss: 0.18265432119369507
Batch 32/64 loss: 0.17921346426010132
Batch 33/64 loss: 0.17510712146759033
Batch 34/64 loss: 0.1959768533706665
Batch 35/64 loss: 0.18561673164367676
Batch 36/64 loss: 0.16914623975753784
Batch 37/64 loss: 0.1762164831161499
Batch 38/64 loss: 0.19574570655822754
Batch 39/64 loss: 0.18376785516738892
Batch 40/64 loss: 0.17734956741333008
Batch 41/64 loss: 0.1879897117614746
Batch 42/64 loss: 0.18707537651062012
Batch 43/64 loss: 0.17289435863494873
Batch 44/64 loss: 0.20252376794815063
Batch 45/64 loss: 0.1909182071685791
Batch 46/64 loss: 0.17731702327728271
Batch 47/64 loss: 0.17962634563446045
Batch 48/64 loss: 0.1835085153579712
Batch 49/64 loss: 0.18762648105621338
Batch 50/64 loss: 0.1876823902130127
Batch 51/64 loss: 0.1749098300933838
Batch 52/64 loss: 0.18445426225662231
Batch 53/64 loss: 0.19041955471038818
Batch 54/64 loss: 0.19647902250289917
Batch 55/64 loss: 0.1717328429222107
Batch 56/64 loss: 0.17645180225372314
Batch 57/64 loss: 0.17962360382080078
Batch 58/64 loss: 0.18989157676696777
Batch 59/64 loss: 0.19422727823257446
Batch 60/64 loss: 0.18038594722747803
Batch 61/64 loss: 0.172163724899292
Batch 62/64 loss: 0.1951906681060791
Batch 63/64 loss: 0.17902839183807373
Batch 64/64 loss: 0.16963303089141846
Epoch 10  Train loss: 0.18487272776809394  Val loss: 0.20381977468012125
Epoch 11
-------------------------------
Batch 1/64 loss: 0.1742875576019287
Batch 2/64 loss: 0.18540644645690918
Batch 3/64 loss: 0.20132267475128174
Batch 4/64 loss: 0.16666251420974731
Batch 5/64 loss: 0.16547852754592896
Batch 6/64 loss: 0.18565869331359863
Batch 7/64 loss: 0.18472570180892944
Batch 8/64 loss: 0.1661766767501831
Batch 9/64 loss: 0.1881369948387146
Batch 10/64 loss: 0.18369388580322266
Batch 11/64 loss: 0.18918001651763916
Batch 12/64 loss: 0.16714489459991455
Batch 13/64 loss: 0.16993260383605957
Batch 14/64 loss: 0.16418743133544922
Batch 15/64 loss: 0.1729230284690857
Batch 16/64 loss: 0.1827986240386963
Batch 17/64 loss: 0.18392914533615112
Batch 18/64 loss: 0.18782293796539307
Batch 19/64 loss: 0.1540050506591797
Batch 20/64 loss: 0.1650238037109375
Batch 21/64 loss: 0.18662500381469727
Batch 22/64 loss: 0.1619829535484314
Batch 23/64 loss: 0.1706486940383911
Batch 24/64 loss: 0.17864888906478882
Batch 25/64 loss: 0.18458569049835205
Batch 26/64 loss: 0.18188583850860596
Batch 27/64 loss: 0.17476511001586914
Batch 28/64 loss: 0.16054600477218628
Batch 29/64 loss: 0.1732986569404602
Batch 30/64 loss: 0.17297327518463135
Batch 31/64 loss: 0.18718290328979492
Batch 32/64 loss: 0.16894030570983887
Batch 33/64 loss: 0.1851745843887329
Batch 34/64 loss: 0.18683600425720215
Batch 35/64 loss: 0.18529653549194336
Batch 36/64 loss: 0.16121304035186768
Batch 37/64 loss: 0.16812849044799805
Batch 38/64 loss: 0.18560469150543213
Batch 39/64 loss: 0.1741734743118286
Batch 40/64 loss: 0.18178236484527588
Batch 41/64 loss: 0.16915231943130493
Batch 42/64 loss: 0.1867559552192688
Batch 43/64 loss: 0.1695544719696045
Batch 44/64 loss: 0.17650645971298218
Batch 45/64 loss: 0.16881197690963745
Batch 46/64 loss: 0.1790299415588379
Batch 47/64 loss: 0.1862565279006958
Batch 48/64 loss: 0.18853527307510376
Batch 49/64 loss: 0.15506887435913086
Batch 50/64 loss: 0.17429816722869873
Batch 51/64 loss: 0.17275595664978027
Batch 52/64 loss: 0.1781853437423706
Batch 53/64 loss: 0.18186098337173462
Batch 54/64 loss: 0.1974799633026123
Batch 55/64 loss: 0.16225606203079224
Batch 56/64 loss: 0.1894897222518921
Batch 57/64 loss: 0.17458021640777588
Batch 58/64 loss: 0.1692134141921997
Batch 59/64 loss: 0.1581750512123108
Batch 60/64 loss: 0.16504472494125366
Batch 61/64 loss: 0.17155051231384277
Batch 62/64 loss: 0.16120445728302002
Batch 63/64 loss: 0.17248529195785522
Batch 64/64 loss: 0.16399812698364258
Epoch 11  Train loss: 0.1756869016909132  Val loss: 0.19163256801690431
Saving best model, epoch: 11
Epoch 12
-------------------------------
Batch 1/64 loss: 0.17414361238479614
Batch 2/64 loss: 0.1745281219482422
Batch 3/64 loss: 0.15959829092025757
Batch 4/64 loss: 0.14489728212356567
Batch 5/64 loss: 0.17396080493927002
Batch 6/64 loss: 0.17989379167556763
Batch 7/64 loss: 0.16978126764297485
Batch 8/64 loss: 0.1698557734489441
Batch 9/64 loss: 0.19729888439178467
Batch 10/64 loss: 0.16523927450180054
Batch 11/64 loss: 0.1662588119506836
Batch 12/64 loss: 0.169663667678833
Batch 13/64 loss: 0.17108988761901855
Batch 14/64 loss: 0.15867602825164795
Batch 15/64 loss: 0.18120884895324707
Batch 16/64 loss: 0.16511964797973633
Batch 17/64 loss: 0.18540769815444946
Batch 18/64 loss: 0.1702907681465149
Batch 19/64 loss: 0.1654619574546814
Batch 20/64 loss: 0.17157840728759766
Batch 21/64 loss: 0.17858195304870605
Batch 22/64 loss: 0.17339521646499634
Batch 23/64 loss: 0.18460243940353394
Batch 24/64 loss: 0.166068434715271
Batch 25/64 loss: 0.15648305416107178
Batch 26/64 loss: 0.16711753606796265
Batch 27/64 loss: 0.1553727388381958
Batch 28/64 loss: 0.18947499990463257
Batch 29/64 loss: 0.1781802773475647
Batch 30/64 loss: 0.18512743711471558
Batch 31/64 loss: 0.16106247901916504
Batch 32/64 loss: 0.1813122034072876
Batch 33/64 loss: 0.16515237092971802
Batch 34/64 loss: 0.16422158479690552
Batch 35/64 loss: 0.20220351219177246
Batch 36/64 loss: 0.1502363681793213
Batch 37/64 loss: 0.1672670841217041
Batch 38/64 loss: 0.14936989545822144
Batch 39/64 loss: 0.1716470718383789
Batch 40/64 loss: 0.171983540058136
Batch 41/64 loss: 0.18582004308700562
Batch 42/64 loss: 0.17159438133239746
Batch 43/64 loss: 0.1745084524154663
Batch 44/64 loss: 0.1446932554244995
Batch 45/64 loss: 0.1715608835220337
Batch 46/64 loss: 0.18514055013656616
Batch 47/64 loss: 0.17242133617401123
Batch 48/64 loss: 0.17111945152282715
Batch 49/64 loss: 0.17531037330627441
Batch 50/64 loss: 0.1663540005683899
Batch 51/64 loss: 0.1616763472557068
Batch 52/64 loss: 0.1648697853088379
Batch 53/64 loss: 0.17031872272491455
Batch 54/64 loss: 0.15964347124099731
Batch 55/64 loss: 0.16002708673477173
Batch 56/64 loss: 0.16388869285583496
Batch 57/64 loss: 0.17589259147644043
Batch 58/64 loss: 0.16876447200775146
Batch 59/64 loss: 0.16363602876663208
Batch 60/64 loss: 0.14275920391082764
Batch 61/64 loss: 0.16402000188827515
Batch 62/64 loss: 0.15467262268066406
Batch 63/64 loss: 0.16330254077911377
Batch 64/64 loss: 0.17838943004608154
Epoch 12  Train loss: 0.16938979438706941  Val loss: 0.18648618029564926
Saving best model, epoch: 12
Epoch 13
-------------------------------
Batch 1/64 loss: 0.17093169689178467
Batch 2/64 loss: 0.18650150299072266
Batch 3/64 loss: 0.16067814826965332
Batch 4/64 loss: 0.16633713245391846
Batch 5/64 loss: 0.16719979047775269
Batch 6/64 loss: 0.16096168756484985
Batch 7/64 loss: 0.17490136623382568
Batch 8/64 loss: 0.15365058183670044
Batch 9/64 loss: 0.17362403869628906
Batch 10/64 loss: 0.16556894779205322
Batch 11/64 loss: 0.18235808610916138
Batch 12/64 loss: 0.1583179235458374
Batch 13/64 loss: 0.17130041122436523
Batch 14/64 loss: 0.15117526054382324
Batch 15/64 loss: 0.160813570022583
Batch 16/64 loss: 0.17077672481536865
Batch 17/64 loss: 0.16896426677703857
Batch 18/64 loss: 0.15308892726898193
Batch 19/64 loss: 0.17412984371185303
Batch 20/64 loss: 0.17119896411895752
Batch 21/64 loss: 0.16085267066955566
Batch 22/64 loss: 0.14641600847244263
Batch 23/64 loss: 0.16809183359146118
Batch 24/64 loss: 0.16035211086273193
Batch 25/64 loss: 0.16241955757141113
Batch 26/64 loss: 0.15180152654647827
Batch 27/64 loss: 0.15567535161972046
Batch 28/64 loss: 0.15358495712280273
Batch 29/64 loss: 0.1641523838043213
Batch 30/64 loss: 0.14919883012771606
Batch 31/64 loss: 0.1636652946472168
Batch 32/64 loss: 0.17005276679992676
Batch 33/64 loss: 0.1560969352722168
Batch 34/64 loss: 0.16386890411376953
Batch 35/64 loss: 0.14922714233398438
Batch 36/64 loss: 0.15376925468444824
Batch 37/64 loss: 0.15284723043441772
Batch 38/64 loss: 0.15511947870254517
Batch 39/64 loss: 0.1620093584060669
Batch 40/64 loss: 0.15744048357009888
Batch 41/64 loss: 0.17407262325286865
Batch 42/64 loss: 0.1600104570388794
Batch 43/64 loss: 0.15628975629806519
Batch 44/64 loss: 0.15214717388153076
Batch 45/64 loss: 0.16224312782287598
Batch 46/64 loss: 0.1487870216369629
Batch 47/64 loss: 0.1514233946800232
Batch 48/64 loss: 0.14264905452728271
Batch 49/64 loss: 0.15784764289855957
Batch 50/64 loss: 0.1585124135017395
Batch 51/64 loss: 0.14955776929855347
Batch 52/64 loss: 0.15216350555419922
Batch 53/64 loss: 0.1583409309387207
Batch 54/64 loss: 0.17860925197601318
Batch 55/64 loss: 0.1687023639678955
Batch 56/64 loss: 0.16830092668533325
Batch 57/64 loss: 0.17272865772247314
Batch 58/64 loss: 0.15539324283599854
Batch 59/64 loss: 0.15249639749526978
Batch 60/64 loss: 0.14559006690979004
Batch 61/64 loss: 0.1387927532196045
Batch 62/64 loss: 0.13936173915863037
Batch 63/64 loss: 0.15509843826293945
Batch 64/64 loss: 0.16245371103286743
Epoch 13  Train loss: 0.16031497950647391  Val loss: 0.17724062855710687
Saving best model, epoch: 13
Epoch 14
-------------------------------
Batch 1/64 loss: 0.1576906442642212
Batch 2/64 loss: 0.1567831039428711
Batch 3/64 loss: 0.14373928308486938
Batch 4/64 loss: 0.16599535942077637
Batch 5/64 loss: 0.1550503969192505
Batch 6/64 loss: 0.145471453666687
Batch 7/64 loss: 0.15437006950378418
Batch 8/64 loss: 0.13191264867782593
Batch 9/64 loss: 0.15422582626342773
Batch 10/64 loss: 0.14061367511749268
Batch 11/64 loss: 0.16056668758392334
Batch 12/64 loss: 0.16072499752044678
Batch 13/64 loss: 0.15853619575500488
Batch 14/64 loss: 0.1405147910118103
Batch 15/64 loss: 0.16326457262039185
Batch 16/64 loss: 0.1540008783340454
Batch 17/64 loss: 0.1483827829360962
Batch 18/64 loss: 0.1638820767402649
Batch 19/64 loss: 0.14890378713607788
Batch 20/64 loss: 0.14138126373291016
Batch 21/64 loss: 0.1513594388961792
Batch 22/64 loss: 0.13769769668579102
Batch 23/64 loss: 0.15511834621429443
Batch 24/64 loss: 0.14070427417755127
Batch 25/64 loss: 0.17177003622055054
Batch 26/64 loss: 0.14559495449066162
Batch 27/64 loss: 0.15259790420532227
Batch 28/64 loss: 0.13002043962478638
Batch 29/64 loss: 0.15452146530151367
Batch 30/64 loss: 0.14085537195205688
Batch 31/64 loss: 0.1605229377746582
Batch 32/64 loss: 0.1753983497619629
Batch 33/64 loss: 0.15920734405517578
Batch 34/64 loss: 0.16959643363952637
Batch 35/64 loss: 0.15767186880111694
Batch 36/64 loss: 0.15323370695114136
Batch 37/64 loss: 0.14181512594223022
Batch 38/64 loss: 0.15371108055114746
Batch 39/64 loss: 0.14143294095993042
Batch 40/64 loss: 0.15689635276794434
Batch 41/64 loss: 0.14876258373260498
Batch 42/64 loss: 0.1256622076034546
Batch 43/64 loss: 0.1601325273513794
Batch 44/64 loss: 0.15102654695510864
Batch 45/64 loss: 0.14686048030853271
Batch 46/64 loss: 0.14215779304504395
Batch 47/64 loss: 0.13918161392211914
Batch 48/64 loss: 0.13053840398788452
Batch 49/64 loss: 0.15103638172149658
Batch 50/64 loss: 0.14938533306121826
Batch 51/64 loss: 0.1568446159362793
Batch 52/64 loss: 0.1720770001411438
Batch 53/64 loss: 0.1444644331932068
Batch 54/64 loss: 0.12393611669540405
Batch 55/64 loss: 0.13969296216964722
Batch 56/64 loss: 0.16364145278930664
Batch 57/64 loss: 0.1370251178741455
Batch 58/64 loss: 0.1329408884048462
Batch 59/64 loss: 0.1414865255355835
Batch 60/64 loss: 0.13562840223312378
Batch 61/64 loss: 0.14444243907928467
Batch 62/64 loss: 0.15827316045761108
Batch 63/64 loss: 0.15949690341949463
Batch 64/64 loss: 0.12736546993255615
Epoch 14  Train loss: 0.14974046361212637  Val loss: 0.17542553972132838
Saving best model, epoch: 14
Epoch 15
-------------------------------
Batch 1/64 loss: 0.1355491280555725
Batch 2/64 loss: 0.13318347930908203
Batch 3/64 loss: 0.1502913236618042
Batch 4/64 loss: 0.15784597396850586
Batch 5/64 loss: 0.1443888545036316
Batch 6/64 loss: 0.13270872831344604
Batch 7/64 loss: 0.13399291038513184
Batch 8/64 loss: 0.13515716791152954
Batch 9/64 loss: 0.176055908203125
Batch 10/64 loss: 0.1253107190132141
Batch 11/64 loss: 0.13386797904968262
Batch 12/64 loss: 0.1685674786567688
Batch 13/64 loss: 0.12884628772735596
Batch 14/64 loss: 0.16001492738723755
Batch 15/64 loss: 0.15384578704833984
Batch 16/64 loss: 0.1350056529045105
Batch 17/64 loss: 0.15568846464157104
Batch 18/64 loss: 0.1578429937362671
Batch 19/64 loss: 0.15362751483917236
Batch 20/64 loss: 0.142287015914917
Batch 21/64 loss: 0.14957928657531738
Batch 22/64 loss: 0.15691602230072021
Batch 23/64 loss: 0.1387653350830078
Batch 24/64 loss: 0.1659807562828064
Batch 25/64 loss: 0.14220410585403442
Batch 26/64 loss: 0.14028024673461914
Batch 27/64 loss: 0.14194059371948242
Batch 28/64 loss: 0.1296325922012329
Batch 29/64 loss: 0.15375638008117676
Batch 30/64 loss: 0.14265984296798706
Batch 31/64 loss: 0.1381480097770691
Batch 32/64 loss: 0.13325881958007812
Batch 33/64 loss: 0.12846273183822632
Batch 34/64 loss: 0.13370317220687866
Batch 35/64 loss: 0.15057480335235596
Batch 36/64 loss: 0.1262086033821106
Batch 37/64 loss: 0.15446388721466064
Batch 38/64 loss: 0.15282613039016724
Batch 39/64 loss: 0.13578641414642334
Batch 40/64 loss: 0.1478978395462036
Batch 41/64 loss: 0.15126711130142212
Batch 42/64 loss: 0.1260853409767151
Batch 43/64 loss: 0.1281566619873047
Batch 44/64 loss: 0.14048528671264648
Batch 45/64 loss: 0.1341172456741333
Batch 46/64 loss: 0.14029628038406372
Batch 47/64 loss: 0.14708346128463745
Batch 48/64 loss: 0.13000446557998657
Batch 49/64 loss: 0.12734061479568481
Batch 50/64 loss: 0.1315321922302246
Batch 51/64 loss: 0.14885777235031128
Batch 52/64 loss: 0.13581746816635132
Batch 53/64 loss: 0.13504528999328613
Batch 54/64 loss: 0.15112149715423584
Batch 55/64 loss: 0.13923859596252441
Batch 56/64 loss: 0.13549524545669556
Batch 57/64 loss: 0.1302638053894043
Batch 58/64 loss: 0.1542993187904358
Batch 59/64 loss: 0.14393943548202515
Batch 60/64 loss: 0.13172310590744019
Batch 61/64 loss: 0.1530437469482422
Batch 62/64 loss: 0.12948864698410034
Batch 63/64 loss: 0.13550841808319092
Batch 64/64 loss: 0.13937455415725708
Epoch 15  Train loss: 0.14214691437926946  Val loss: 0.15579413855608387
Saving best model, epoch: 15
Epoch 16
-------------------------------
Batch 1/64 loss: 0.1267915964126587
Batch 2/64 loss: 0.12546414136886597
Batch 3/64 loss: 0.12564623355865479
Batch 4/64 loss: 0.14065217971801758
Batch 5/64 loss: 0.13185584545135498
Batch 6/64 loss: 0.13281714916229248
Batch 7/64 loss: 0.12990885972976685
Batch 8/64 loss: 0.11668860912322998
Batch 9/64 loss: 0.11261707544326782
Batch 10/64 loss: 0.13884294033050537
Batch 11/64 loss: 0.1391890048980713
Batch 12/64 loss: 0.12025129795074463
Batch 13/64 loss: 0.14986592531204224
Batch 14/64 loss: 0.12475264072418213
Batch 15/64 loss: 0.14589864015579224
Batch 16/64 loss: 0.12213623523712158
Batch 17/64 loss: 0.1414567232131958
Batch 18/64 loss: 0.13185334205627441
Batch 19/64 loss: 0.1465843915939331
Batch 20/64 loss: 0.1291854977607727
Batch 21/64 loss: 0.14159941673278809
Batch 22/64 loss: 0.1512516736984253
Batch 23/64 loss: 0.11299353837966919
Batch 24/64 loss: 0.12192988395690918
Batch 25/64 loss: 0.14227628707885742
Batch 26/64 loss: 0.1292283535003662
Batch 27/64 loss: 0.12108814716339111
Batch 28/64 loss: 0.14186495542526245
Batch 29/64 loss: 0.13926970958709717
Batch 30/64 loss: 0.13804298639297485
Batch 31/64 loss: 0.13547587394714355
Batch 32/64 loss: 0.1499701738357544
Batch 33/64 loss: 0.13256025314331055
Batch 34/64 loss: 0.1248626708984375
Batch 35/64 loss: 0.12582528591156006
Batch 36/64 loss: 0.1416860818862915
Batch 37/64 loss: 0.14034920930862427
Batch 38/64 loss: 0.13058775663375854
Batch 39/64 loss: 0.1310502290725708
Batch 40/64 loss: 0.13257944583892822
Batch 41/64 loss: 0.15011101961135864
Batch 42/64 loss: 0.13149523735046387
Batch 43/64 loss: 0.13561153411865234
Batch 44/64 loss: 0.1334228515625
Batch 45/64 loss: 0.12505191564559937
Batch 46/64 loss: 0.15025627613067627
Batch 47/64 loss: 0.11543488502502441
Batch 48/64 loss: 0.14268702268600464
Batch 49/64 loss: 0.12225663661956787
Batch 50/64 loss: 0.1709815263748169
Batch 51/64 loss: 0.11662721633911133
Batch 52/64 loss: 0.13817644119262695
Batch 53/64 loss: 0.13654166460037231
Batch 54/64 loss: 0.1210625171661377
Batch 55/64 loss: 0.12929940223693848
Batch 56/64 loss: 0.1283888816833496
Batch 57/64 loss: 0.12829798460006714
Batch 58/64 loss: 0.13440454006195068
Batch 59/64 loss: 0.12639069557189941
Batch 60/64 loss: 0.12762057781219482
Batch 61/64 loss: 0.09832996129989624
Batch 62/64 loss: 0.16126275062561035
Batch 63/64 loss: 0.12093997001647949
Batch 64/64 loss: 0.1215740442276001
Epoch 16  Train loss: 0.13259266357795865  Val loss: 0.16190024667589115
Epoch 17
-------------------------------
Batch 1/64 loss: 0.12073391675949097
Batch 2/64 loss: 0.12885254621505737
Batch 3/64 loss: 0.13369029760360718
Batch 4/64 loss: 0.10743463039398193
Batch 5/64 loss: 0.1405503749847412
Batch 6/64 loss: 0.1470286250114441
Batch 7/64 loss: 0.10708290338516235
Batch 8/64 loss: 0.12668651342391968
Batch 9/64 loss: 0.12350934743881226
Batch 10/64 loss: 0.12101930379867554
Batch 11/64 loss: 0.11722129583358765
Batch 12/64 loss: 0.11931502819061279
Batch 13/64 loss: 0.1217811107635498
Batch 14/64 loss: 0.11434316635131836
Batch 15/64 loss: 0.13568753004074097
Batch 16/64 loss: 0.11127036809921265
Batch 17/64 loss: 0.1208733320236206
Batch 18/64 loss: 0.13042891025543213
Batch 19/64 loss: 0.09891974925994873
Batch 20/64 loss: 0.13245630264282227
Batch 21/64 loss: 0.12817567586898804
Batch 22/64 loss: 0.14784318208694458
Batch 23/64 loss: 0.10597199201583862
Batch 24/64 loss: 0.1200559139251709
Batch 25/64 loss: 0.11051976680755615
Batch 26/64 loss: 0.123130202293396
Batch 27/64 loss: 0.12204939126968384
Batch 28/64 loss: 0.1269550323486328
Batch 29/64 loss: 0.1321333646774292
Batch 30/64 loss: 0.15192830562591553
Batch 31/64 loss: 0.11848896741867065
Batch 32/64 loss: 0.12991726398468018
Batch 33/64 loss: 0.10680818557739258
Batch 34/64 loss: 0.12013828754425049
Batch 35/64 loss: 0.11930769681930542
Batch 36/64 loss: 0.11883652210235596
Batch 37/64 loss: 0.11658215522766113
Batch 38/64 loss: 0.10773718357086182
Batch 39/64 loss: 0.14003902673721313
Batch 40/64 loss: 0.11909234523773193
Batch 41/64 loss: 0.15127038955688477
Batch 42/64 loss: 0.1223757266998291
Batch 43/64 loss: 0.14177221059799194
Batch 44/64 loss: 0.1272200345993042
Batch 45/64 loss: 0.13007307052612305
Batch 46/64 loss: 0.11144697666168213
Batch 47/64 loss: 0.1244208812713623
Batch 48/64 loss: 0.1352914571762085
Batch 49/64 loss: 0.134759783744812
Batch 50/64 loss: 0.1272106170654297
Batch 51/64 loss: 0.10806560516357422
Batch 52/64 loss: 0.13213205337524414
Batch 53/64 loss: 0.12562942504882812
Batch 54/64 loss: 0.12597006559371948
Batch 55/64 loss: 0.10481142997741699
Batch 56/64 loss: 0.12991708517074585
Batch 57/64 loss: 0.11324548721313477
Batch 58/64 loss: 0.12780511379241943
Batch 59/64 loss: 0.12144964933395386
Batch 60/64 loss: 0.10622799396514893
Batch 61/64 loss: 0.14675378799438477
Batch 62/64 loss: 0.12036031484603882
Batch 63/64 loss: 0.12546151876449585
Batch 64/64 loss: 0.15256989002227783
Epoch 17  Train loss: 0.12443441269444484  Val loss: 0.14217069693857043
Saving best model, epoch: 17
Epoch 18
-------------------------------
Batch 1/64 loss: 0.1290818452835083
Batch 2/64 loss: 0.11523658037185669
Batch 3/64 loss: 0.14038574695587158
Batch 4/64 loss: 0.12285584211349487
Batch 5/64 loss: 0.1285424828529358
Batch 6/64 loss: 0.1273963451385498
Batch 7/64 loss: 0.11139905452728271
Batch 8/64 loss: 0.1264159083366394
Batch 9/64 loss: 0.1344296932220459
Batch 10/64 loss: 0.12055683135986328
Batch 11/64 loss: 0.14032131433486938
Batch 12/64 loss: 0.1399109959602356
Batch 13/64 loss: 0.1356520652770996
Batch 14/64 loss: 0.11906313896179199
Batch 15/64 loss: 0.11735254526138306
Batch 16/64 loss: 0.11617326736450195
Batch 17/64 loss: 0.1372120976448059
Batch 18/64 loss: 0.09531581401824951
Batch 19/64 loss: 0.12386864423751831
Batch 20/64 loss: 0.11585330963134766
Batch 21/64 loss: 0.12412291765213013
Batch 22/64 loss: 0.11197006702423096
Batch 23/64 loss: 0.11229515075683594
Batch 24/64 loss: 0.13295269012451172
Batch 25/64 loss: 0.11833077669143677
Batch 26/64 loss: 0.10103702545166016
Batch 27/64 loss: 0.10669630765914917
Batch 28/64 loss: 0.12334585189819336
Batch 29/64 loss: 0.10744071006774902
Batch 30/64 loss: 0.12847846746444702
Batch 31/64 loss: 0.13243341445922852
Batch 32/64 loss: 0.1332850456237793
Batch 33/64 loss: 0.12728279829025269
Batch 34/64 loss: 0.12625396251678467
Batch 35/64 loss: 0.11308944225311279
Batch 36/64 loss: 0.12630760669708252
Batch 37/64 loss: 0.1317429542541504
Batch 38/64 loss: 0.12932473421096802
Batch 39/64 loss: 0.11592328548431396
Batch 40/64 loss: 0.12224608659744263
Batch 41/64 loss: 0.12627673149108887
Batch 42/64 loss: 0.12133318185806274
Batch 43/64 loss: 0.09623080492019653
Batch 44/64 loss: 0.10652387142181396
Batch 45/64 loss: 0.14456194639205933
Batch 46/64 loss: 0.12322878837585449
Batch 47/64 loss: 0.13045305013656616
Batch 48/64 loss: 0.11748045682907104
Batch 49/64 loss: 0.13481634855270386
Batch 50/64 loss: 0.114368736743927
Batch 51/64 loss: 0.12162470817565918
Batch 52/64 loss: 0.1260325312614441
Batch 53/64 loss: 0.11650729179382324
Batch 54/64 loss: 0.12719577550888062
Batch 55/64 loss: 0.12532782554626465
Batch 56/64 loss: 0.12235891819000244
Batch 57/64 loss: 0.09777325391769409
Batch 58/64 loss: 0.11791491508483887
Batch 59/64 loss: 0.12425947189331055
Batch 60/64 loss: 0.10282814502716064
Batch 61/64 loss: 0.1063728928565979
Batch 62/64 loss: 0.1051931381225586
Batch 63/64 loss: 0.09567630290985107
Batch 64/64 loss: 0.1126282811164856
Epoch 18  Train loss: 0.12094734299416636  Val loss: 0.15169717704307584
Epoch 19
-------------------------------
Batch 1/64 loss: 0.11346656084060669
Batch 2/64 loss: 0.13256847858428955
Batch 3/64 loss: 0.11110550165176392
Batch 4/64 loss: 0.10998880863189697
Batch 5/64 loss: 0.1297391653060913
Batch 6/64 loss: 0.13398218154907227
Batch 7/64 loss: 0.10761183500289917
Batch 8/64 loss: 0.1255626678466797
Batch 9/64 loss: 0.11826050281524658
Batch 10/64 loss: 0.0911068320274353
Batch 11/64 loss: 0.11918026208877563
Batch 12/64 loss: 0.12663739919662476
Batch 13/64 loss: 0.09456449747085571
Batch 14/64 loss: 0.11487948894500732
Batch 15/64 loss: 0.09133517742156982
Batch 16/64 loss: 0.11236870288848877
Batch 17/64 loss: 0.11528635025024414
Batch 18/64 loss: 0.1071203351020813
Batch 19/64 loss: 0.11108779907226562
Batch 20/64 loss: 0.11270856857299805
Batch 21/64 loss: 0.10430079698562622
Batch 22/64 loss: 0.13737094402313232
Batch 23/64 loss: 0.11108708381652832
Batch 24/64 loss: 0.09917473793029785
Batch 25/64 loss: 0.132470965385437
Batch 26/64 loss: 0.12073004245758057
Batch 27/64 loss: 0.11450564861297607
Batch 28/64 loss: 0.08374810218811035
Batch 29/64 loss: 0.12518024444580078
Batch 30/64 loss: 0.10842239856719971
Batch 31/64 loss: 0.1083521842956543
Batch 32/64 loss: 0.10712534189224243
Batch 33/64 loss: 0.09683072566986084
Batch 34/64 loss: 0.09577822685241699
Batch 35/64 loss: 0.10915493965148926
Batch 36/64 loss: 0.1016998291015625
Batch 37/64 loss: 0.09433305263519287
Batch 38/64 loss: 0.13411635160446167
Batch 39/64 loss: 0.10809159278869629
Batch 40/64 loss: 0.10693705081939697
Batch 41/64 loss: 0.12725794315338135
Batch 42/64 loss: 0.10659569501876831
Batch 43/64 loss: 0.11983954906463623
Batch 44/64 loss: 0.09674394130706787
Batch 45/64 loss: 0.1122732162475586
Batch 46/64 loss: 0.12319117784500122
Batch 47/64 loss: 0.07886713743209839
Batch 48/64 loss: 0.10807806253433228
Batch 49/64 loss: 0.08856087923049927
Batch 50/64 loss: 0.11072826385498047
Batch 51/64 loss: 0.11688452959060669
Batch 52/64 loss: 0.09130656719207764
Batch 53/64 loss: 0.10720223188400269
Batch 54/64 loss: 0.09514224529266357
Batch 55/64 loss: 0.11331427097320557
Batch 56/64 loss: 0.1227790117263794
Batch 57/64 loss: 0.1341957449913025
Batch 58/64 loss: 0.11930328607559204
Batch 59/64 loss: 0.09483718872070312
Batch 60/64 loss: 0.10240697860717773
Batch 61/64 loss: 0.11116617918014526
Batch 62/64 loss: 0.11257338523864746
Batch 63/64 loss: 0.1076996922492981
Batch 64/64 loss: 0.09758758544921875
Epoch 19  Train loss: 0.11062132143506817  Val loss: 0.13637716028698532
Saving best model, epoch: 19
Epoch 20
-------------------------------
Batch 1/64 loss: 0.11081880331039429
Batch 2/64 loss: 0.11315792798995972
Batch 3/64 loss: 0.11032998561859131
Batch 4/64 loss: 0.10736632347106934
Batch 5/64 loss: 0.11356031894683838
Batch 6/64 loss: 0.09646713733673096
Batch 7/64 loss: 0.11368900537490845
Batch 8/64 loss: 0.1098254919052124
Batch 9/64 loss: 0.10669893026351929
Batch 10/64 loss: 0.10863405466079712
Batch 11/64 loss: 0.09242987632751465
Batch 12/64 loss: 0.12495607137680054
Batch 13/64 loss: 0.09900546073913574
Batch 14/64 loss: 0.1070244312286377
Batch 15/64 loss: 0.09790956974029541
Batch 16/64 loss: 0.09222942590713501
Batch 17/64 loss: 0.09601342678070068
Batch 18/64 loss: 0.0984874963760376
Batch 19/64 loss: 0.0989801287651062
Batch 20/64 loss: 0.09408581256866455
Batch 21/64 loss: 0.1276530623435974
Batch 22/64 loss: 0.08347582817077637
Batch 23/64 loss: 0.09459108114242554
Batch 24/64 loss: 0.0970618724822998
Batch 25/64 loss: 0.11366313695907593
Batch 26/64 loss: 0.11841917037963867
Batch 27/64 loss: 0.10664641857147217
Batch 28/64 loss: 0.10628223419189453
Batch 29/64 loss: 0.09984898567199707
Batch 30/64 loss: 0.09533286094665527
Batch 31/64 loss: 0.11808288097381592
Batch 32/64 loss: 0.10576647520065308
Batch 33/64 loss: 0.11319267749786377
Batch 34/64 loss: 0.11449813842773438
Batch 35/64 loss: 0.1184760332107544
Batch 36/64 loss: 0.10674738883972168
Batch 37/64 loss: 0.09779179096221924
Batch 38/64 loss: 0.0787273645401001
Batch 39/64 loss: 0.11063724756240845
Batch 40/64 loss: 0.08399856090545654
Batch 41/64 loss: 0.11486995220184326
Batch 42/64 loss: 0.11538445949554443
Batch 43/64 loss: 0.096443772315979
Batch 44/64 loss: 0.10919409990310669
Batch 45/64 loss: 0.10515594482421875
Batch 46/64 loss: 0.09417992830276489
Batch 47/64 loss: 0.10546952486038208
Batch 48/64 loss: 0.13951611518859863
Batch 49/64 loss: 0.12011384963989258
Batch 50/64 loss: 0.09579086303710938
Batch 51/64 loss: 0.08714979887008667
Batch 52/64 loss: 0.10455501079559326
Batch 53/64 loss: 0.1108277440071106
Batch 54/64 loss: 0.12148892879486084
Batch 55/64 loss: 0.09891265630722046
Batch 56/64 loss: 0.0984148383140564
Batch 57/64 loss: 0.0929340124130249
Batch 58/64 loss: 0.11012709140777588
Batch 59/64 loss: 0.11917400360107422
Batch 60/64 loss: 0.10314130783081055
Batch 61/64 loss: 0.1244707703590393
Batch 62/64 loss: 0.0860280990600586
Batch 63/64 loss: 0.1010892391204834
Batch 64/64 loss: 0.08962470293045044
Epoch 20  Train loss: 0.10516413216497383  Val loss: 0.13346500740837805
Saving best model, epoch: 20
Epoch 21
-------------------------------
Batch 1/64 loss: 0.10545623302459717
Batch 2/64 loss: 0.08841252326965332
Batch 3/64 loss: 0.09277075529098511
Batch 4/64 loss: 0.13095074892044067
Batch 5/64 loss: 0.11572587490081787
Batch 6/64 loss: 0.095348060131073
Batch 7/64 loss: 0.11115574836730957
Batch 8/64 loss: 0.09851330518722534
Batch 9/64 loss: 0.10327637195587158
Batch 10/64 loss: 0.10418593883514404
Batch 11/64 loss: 0.09288734197616577
Batch 12/64 loss: 0.08513402938842773
Batch 13/64 loss: 0.10062748193740845
Batch 14/64 loss: 0.10131734609603882
Batch 15/64 loss: 0.08417314291000366
Batch 16/64 loss: 0.0971900224685669
Batch 17/64 loss: 0.07311832904815674
Batch 18/64 loss: 0.09666991233825684
Batch 19/64 loss: 0.09276753664016724
Batch 20/64 loss: 0.08155786991119385
Batch 21/64 loss: 0.10083305835723877
Batch 22/64 loss: 0.08471894264221191
Batch 23/64 loss: 0.10281020402908325
Batch 24/64 loss: 0.09679985046386719
Batch 25/64 loss: 0.11167562007904053
Batch 26/64 loss: 0.10605859756469727
Batch 27/64 loss: 0.11615276336669922
Batch 28/64 loss: 0.08960908651351929
Batch 29/64 loss: 0.08326172828674316
Batch 30/64 loss: 0.09776854515075684
Batch 31/64 loss: 0.10753238201141357
Batch 32/64 loss: 0.11599916219711304
Batch 33/64 loss: 0.11244916915893555
Batch 34/64 loss: 0.10438060760498047
Batch 35/64 loss: 0.08327305316925049
Batch 36/64 loss: 0.10204768180847168
Batch 37/64 loss: 0.10718506574630737
Batch 38/64 loss: 0.1121017336845398
Batch 39/64 loss: 0.11696070432662964
Batch 40/64 loss: 0.08770203590393066
Batch 41/64 loss: 0.08512890338897705
Batch 42/64 loss: 0.10020816326141357
Batch 43/64 loss: 0.0813828706741333
Batch 44/64 loss: 0.0822896957397461
Batch 45/64 loss: 0.1179732084274292
Batch 46/64 loss: 0.10051155090332031
Batch 47/64 loss: 0.12111657857894897
Batch 48/64 loss: 0.09952270984649658
Batch 49/64 loss: 0.10899627208709717
Batch 50/64 loss: 0.08837491273880005
Batch 51/64 loss: 0.11359035968780518
Batch 52/64 loss: 0.10680198669433594
Batch 53/64 loss: 0.11625134944915771
Batch 54/64 loss: 0.1103476881980896
Batch 55/64 loss: 0.09676915407180786
Batch 56/64 loss: 0.0863637924194336
Batch 57/64 loss: 0.08947169780731201
Batch 58/64 loss: 0.10485988855361938
Batch 59/64 loss: 0.10769456624984741
Batch 60/64 loss: 0.10304003953933716
Batch 61/64 loss: 0.09592294692993164
Batch 62/64 loss: 0.09108579158782959
Batch 63/64 loss: 0.09366339445114136
Batch 64/64 loss: 0.10234099626541138
Epoch 21  Train loss: 0.09990089150036083  Val loss: 0.12604879575087033
Saving best model, epoch: 21
Epoch 22
-------------------------------
Batch 1/64 loss: 0.1106863021850586
Batch 2/64 loss: 0.0916634202003479
Batch 3/64 loss: 0.09263050556182861
Batch 4/64 loss: 0.09218025207519531
Batch 5/64 loss: 0.10392707586288452
Batch 6/64 loss: 0.08530592918395996
Batch 7/64 loss: 0.08063352108001709
Batch 8/64 loss: 0.1167173981666565
Batch 9/64 loss: 0.07909613847732544
Batch 10/64 loss: 0.08346450328826904
Batch 11/64 loss: 0.10043030977249146
Batch 12/64 loss: 0.10445278882980347
Batch 13/64 loss: 0.09374165534973145
Batch 14/64 loss: 0.10823416709899902
Batch 15/64 loss: 0.11432808637619019
Batch 16/64 loss: 0.08491867780685425
Batch 17/64 loss: 0.07831478118896484
Batch 18/64 loss: 0.08566510677337646
Batch 19/64 loss: 0.11139929294586182
Batch 20/64 loss: 0.09309577941894531
Batch 21/64 loss: 0.10026389360427856
Batch 22/64 loss: 0.08487272262573242
Batch 23/64 loss: 0.086758553981781
Batch 24/64 loss: 0.12078064680099487
Batch 25/64 loss: 0.07380020618438721
Batch 26/64 loss: 0.07291626930236816
Batch 27/64 loss: 0.09070390462875366
Batch 28/64 loss: 0.07826918363571167
Batch 29/64 loss: 0.08338290452957153
Batch 30/64 loss: 0.10914361476898193
Batch 31/64 loss: 0.08525335788726807
Batch 32/64 loss: 0.06867420673370361
Batch 33/64 loss: 0.09470963478088379
Batch 34/64 loss: 0.11509871482849121
Batch 35/64 loss: 0.1003219485282898
Batch 36/64 loss: 0.09770369529724121
Batch 37/64 loss: 0.10692143440246582
Batch 38/64 loss: 0.09739470481872559
Batch 39/64 loss: 0.08920353651046753
Batch 40/64 loss: 0.08316349983215332
Batch 41/64 loss: 0.09717369079589844
Batch 42/64 loss: 0.08775115013122559
Batch 43/64 loss: 0.08843725919723511
Batch 44/64 loss: 0.10074079036712646
Batch 45/64 loss: 0.10432952642440796
Batch 46/64 loss: 0.09414929151535034
Batch 47/64 loss: 0.07902902364730835
Batch 48/64 loss: 0.11047065258026123
Batch 49/64 loss: 0.09206390380859375
Batch 50/64 loss: 0.10150617361068726
Batch 51/64 loss: 0.08557313680648804
Batch 52/64 loss: 0.08952558040618896
Batch 53/64 loss: 0.08752971887588501
Batch 54/64 loss: 0.08653295040130615
Batch 55/64 loss: 0.09979110956192017
Batch 56/64 loss: 0.08091378211975098
Batch 57/64 loss: 0.0924767255783081
Batch 58/64 loss: 0.09783357381820679
Batch 59/64 loss: 0.08067363500595093
Batch 60/64 loss: 0.10236144065856934
Batch 61/64 loss: 0.08571171760559082
Batch 62/64 loss: 0.12084716558456421
Batch 63/64 loss: 0.07352644205093384
Batch 64/64 loss: 0.11489319801330566
Epoch 22  Train loss: 0.09373083395116469  Val loss: 0.1358431675999435
Epoch 23
-------------------------------
Batch 1/64 loss: 0.09361374378204346
Batch 2/64 loss: 0.0842774510383606
Batch 3/64 loss: 0.07862168550491333
Batch 4/64 loss: 0.10334235429763794
Batch 5/64 loss: 0.1162061095237732
Batch 6/64 loss: 0.1019129753112793
Batch 7/64 loss: 0.10208994150161743
Batch 8/64 loss: 0.10729461908340454
Batch 9/64 loss: 0.09473353624343872
Batch 10/64 loss: 0.11911511421203613
Batch 11/64 loss: 0.0969190001487732
Batch 12/64 loss: 0.07465773820877075
Batch 13/64 loss: 0.08384329080581665
Batch 14/64 loss: 0.10023009777069092
Batch 15/64 loss: 0.09519344568252563
Batch 16/64 loss: 0.08085155487060547
Batch 17/64 loss: 0.10774219036102295
Batch 18/64 loss: 0.10129600763320923
Batch 19/64 loss: 0.09427887201309204
Batch 20/64 loss: 0.08684796094894409
Batch 21/64 loss: 0.07325410842895508
Batch 22/64 loss: 0.09672355651855469
Batch 23/64 loss: 0.10138839483261108
Batch 24/64 loss: 0.1105654239654541
Batch 25/64 loss: 0.09251236915588379
Batch 26/64 loss: 0.09906959533691406
Batch 27/64 loss: 0.08972615003585815
Batch 28/64 loss: 0.07943928241729736
Batch 29/64 loss: 0.08834612369537354
Batch 30/64 loss: 0.08758962154388428
Batch 31/64 loss: 0.07493448257446289
Batch 32/64 loss: 0.08442449569702148
Batch 33/64 loss: 0.08200103044509888
Batch 34/64 loss: 0.07960069179534912
Batch 35/64 loss: 0.10620683431625366
Batch 36/64 loss: 0.07033973932266235
Batch 37/64 loss: 0.0934685468673706
Batch 38/64 loss: 0.08890461921691895
Batch 39/64 loss: 0.07912296056747437
Batch 40/64 loss: 0.08814215660095215
Batch 41/64 loss: 0.09639310836791992
Batch 42/64 loss: 0.08225864171981812
Batch 43/64 loss: 0.08764052391052246
Batch 44/64 loss: 0.07183724641799927
Batch 45/64 loss: 0.08476078510284424
Batch 46/64 loss: 0.06973004341125488
Batch 47/64 loss: 0.09267991781234741
Batch 48/64 loss: 0.09820175170898438
Batch 49/64 loss: 0.0745895504951477
Batch 50/64 loss: 0.09904682636260986
Batch 51/64 loss: 0.0847177505493164
Batch 52/64 loss: 0.09231388568878174
Batch 53/64 loss: 0.08297491073608398
Batch 54/64 loss: 0.07586264610290527
Batch 55/64 loss: 0.09388673305511475
Batch 56/64 loss: 0.0780555009841919
Batch 57/64 loss: 0.09586036205291748
Batch 58/64 loss: 0.0780135989189148
Batch 59/64 loss: 0.07949930429458618
Batch 60/64 loss: 0.10920017957687378
Batch 61/64 loss: 0.06258988380432129
Batch 62/64 loss: 0.08274829387664795
Batch 63/64 loss: 0.07414108514785767
Batch 64/64 loss: 0.08162868022918701
Epoch 23  Train loss: 0.08936552019680248  Val loss: 0.10709224533788937
Saving best model, epoch: 23
Epoch 24
-------------------------------
Batch 1/64 loss: 0.07167333364486694
Batch 2/64 loss: 0.09976851940155029
Batch 3/64 loss: 0.08781248331069946
Batch 4/64 loss: 0.07930094003677368
Batch 5/64 loss: 0.08802837133407593
Batch 6/64 loss: 0.07904857397079468
Batch 7/64 loss: 0.06871938705444336
Batch 8/64 loss: 0.08194613456726074
Batch 9/64 loss: 0.0700981616973877
Batch 10/64 loss: 0.08095312118530273
Batch 11/64 loss: 0.08825713396072388
Batch 12/64 loss: 0.08168834447860718
Batch 13/64 loss: 0.07313573360443115
Batch 14/64 loss: 0.09698474407196045
Batch 15/64 loss: 0.07585746049880981
Batch 16/64 loss: 0.09259486198425293
Batch 17/64 loss: 0.06974047422409058
Batch 18/64 loss: 0.07343173027038574
Batch 19/64 loss: 0.06598931550979614
Batch 20/64 loss: 0.08211976289749146
Batch 21/64 loss: 0.09528863430023193
Batch 22/64 loss: 0.08589184284210205
Batch 23/64 loss: 0.08019822835922241
Batch 24/64 loss: 0.0856521725654602
Batch 25/64 loss: 0.09186404943466187
Batch 26/64 loss: 0.07312572002410889
Batch 27/64 loss: 0.07443439960479736
Batch 28/64 loss: 0.08690088987350464
Batch 29/64 loss: 0.0811154842376709
Batch 30/64 loss: 0.09990251064300537
Batch 31/64 loss: 0.10197561979293823
Batch 32/64 loss: 0.09369766712188721
Batch 33/64 loss: 0.08179962635040283
Batch 34/64 loss: 0.09026944637298584
Batch 35/64 loss: 0.06045842170715332
Batch 36/64 loss: 0.07517415285110474
Batch 37/64 loss: 0.07117700576782227
Batch 38/64 loss: 0.1015326976776123
Batch 39/64 loss: 0.06000983715057373
Batch 40/64 loss: 0.10341793298721313
Batch 41/64 loss: 0.08116906881332397
Batch 42/64 loss: 0.09892815351486206
Batch 43/64 loss: 0.06123560667037964
Batch 44/64 loss: 0.08137190341949463
Batch 45/64 loss: 0.09340780973434448
Batch 46/64 loss: 0.09248161315917969
Batch 47/64 loss: 0.08971524238586426
Batch 48/64 loss: 0.09403693675994873
Batch 49/64 loss: 0.07383263111114502
Batch 50/64 loss: 0.07976430654525757
Batch 51/64 loss: 0.0859338641166687
Batch 52/64 loss: 0.07895511388778687
Batch 53/64 loss: 0.10343652963638306
Batch 54/64 loss: 0.08627402782440186
Batch 55/64 loss: 0.08622616529464722
Batch 56/64 loss: 0.08307790756225586
Batch 57/64 loss: 0.10120570659637451
Batch 58/64 loss: 0.05990481376647949
Batch 59/64 loss: 0.07392722368240356
Batch 60/64 loss: 0.059230148792266846
Batch 61/64 loss: 0.07398742437362671
Batch 62/64 loss: 0.07930934429168701
Batch 63/64 loss: 0.09427672624588013
Batch 64/64 loss: 0.06553888320922852
Epoch 24  Train loss: 0.08253925267387839  Val loss: 0.11229446499618058
Epoch 25
-------------------------------
Batch 1/64 loss: 0.09461528062820435
Batch 2/64 loss: 0.06459343433380127
Batch 3/64 loss: 0.06331497430801392
Batch 4/64 loss: 0.08249711990356445
Batch 5/64 loss: 0.08699953556060791
Batch 6/64 loss: 0.057720184326171875
Batch 7/64 loss: 0.08276784420013428
Batch 8/64 loss: 0.07289761304855347
Batch 9/64 loss: 0.06509864330291748
Batch 10/64 loss: 0.07684719562530518
Batch 11/64 loss: 0.08482295274734497
Batch 12/64 loss: 0.08550477027893066
Batch 13/64 loss: 0.06985914707183838
Batch 14/64 loss: 0.08160567283630371
Batch 15/64 loss: 0.09282994270324707
Batch 16/64 loss: 0.08425015211105347
Batch 17/64 loss: 0.08021736145019531
Batch 18/64 loss: 0.0705711841583252
Batch 19/64 loss: 0.0804983377456665
Batch 20/64 loss: 0.06802868843078613
Batch 21/64 loss: 0.10444682836532593
Batch 22/64 loss: 0.09408366680145264
Batch 23/64 loss: 0.08416867256164551
Batch 24/64 loss: 0.08079791069030762
Batch 25/64 loss: 0.08187514543533325
Batch 26/64 loss: 0.09671640396118164
Batch 27/64 loss: 0.08100837469100952
Batch 28/64 loss: 0.081187903881073
Batch 29/64 loss: 0.09306424856185913
Batch 30/64 loss: 0.05983912944793701
Batch 31/64 loss: 0.07226341962814331
Batch 32/64 loss: 0.10127878189086914
Batch 33/64 loss: 0.06819802522659302
Batch 34/64 loss: 0.09325599670410156
Batch 35/64 loss: 0.07166910171508789
Batch 36/64 loss: 0.06429916620254517
Batch 37/64 loss: 0.06425148248672485
Batch 38/64 loss: 0.07859838008880615
Batch 39/64 loss: 0.07063418626785278
Batch 40/64 loss: 0.0704919695854187
Batch 41/64 loss: 0.08266586065292358
Batch 42/64 loss: 0.07239758968353271
Batch 43/64 loss: 0.07787454128265381
Batch 44/64 loss: 0.07766985893249512
Batch 45/64 loss: 0.0982561707496643
Batch 46/64 loss: 0.09549903869628906
Batch 47/64 loss: 0.10786843299865723
Batch 48/64 loss: 0.07454550266265869
Batch 49/64 loss: 0.07969623804092407
Batch 50/64 loss: 0.10239320993423462
Batch 51/64 loss: 0.09753894805908203
Batch 52/64 loss: 0.07031989097595215
Batch 53/64 loss: 0.0688444972038269
Batch 54/64 loss: 0.08246266841888428
Batch 55/64 loss: 0.06484156847000122
Batch 56/64 loss: 0.07616645097732544
Batch 57/64 loss: 0.05804377794265747
Batch 58/64 loss: 0.07625538110733032
Batch 59/64 loss: 0.06197357177734375
Batch 60/64 loss: 0.05969846248626709
Batch 61/64 loss: 0.06285804510116577
Batch 62/64 loss: 0.07703989744186401
Batch 63/64 loss: 0.08079534769058228
Batch 64/64 loss: 0.09195441007614136
Epoch 25  Train loss: 0.0789386602009044  Val loss: 0.1135747297522948
Epoch 26
-------------------------------
Batch 1/64 loss: 0.060364365577697754
Batch 2/64 loss: 0.08255928754806519
Batch 3/64 loss: 0.0932278037071228
Batch 4/64 loss: 0.07087951898574829
Batch 5/64 loss: 0.08094757795333862
Batch 6/64 loss: 0.08618664741516113
Batch 7/64 loss: 0.08291089534759521
Batch 8/64 loss: 0.07140493392944336
Batch 9/64 loss: 0.06296408176422119
Batch 10/64 loss: 0.04742377996444702
Batch 11/64 loss: 0.09419578313827515
Batch 12/64 loss: 0.05523395538330078
Batch 13/64 loss: 0.048211753368377686
Batch 14/64 loss: 0.08291751146316528
Batch 15/64 loss: 0.08362996578216553
Batch 16/64 loss: 0.08648300170898438
Batch 17/64 loss: 0.07173007726669312
Batch 18/64 loss: 0.079257071018219
Batch 19/64 loss: 0.09340626001358032
Batch 20/64 loss: 0.04758715629577637
Batch 21/64 loss: 0.07373136281967163
Batch 22/64 loss: 0.06554841995239258
Batch 23/64 loss: 0.07782727479934692
Batch 24/64 loss: 0.07618540525436401
Batch 25/64 loss: 0.0852433443069458
Batch 26/64 loss: 0.08539485931396484
Batch 27/64 loss: 0.07654798030853271
Batch 28/64 loss: 0.07578939199447632
Batch 29/64 loss: 0.09227287769317627
Batch 30/64 loss: 0.07592964172363281
Batch 31/64 loss: 0.0634758472442627
Batch 32/64 loss: 0.10683393478393555
Batch 33/64 loss: 0.05282050371170044
Batch 34/64 loss: 0.08252805471420288
Batch 35/64 loss: 0.06979107856750488
Batch 36/64 loss: 0.07510721683502197
Batch 37/64 loss: 0.06015515327453613
Batch 38/64 loss: 0.06933099031448364
Batch 39/64 loss: 0.10151791572570801
Batch 40/64 loss: 0.07857763767242432
Batch 41/64 loss: 0.0954102873802185
Batch 42/64 loss: 0.08954846858978271
Batch 43/64 loss: 0.079109787940979
Batch 44/64 loss: 0.08027046918869019
Batch 45/64 loss: 0.07655292749404907
Batch 46/64 loss: 0.0796663761138916
Batch 47/64 loss: 0.08706897497177124
Batch 48/64 loss: 0.07294881343841553
Batch 49/64 loss: 0.08562588691711426
Batch 50/64 loss: 0.07909464836120605
Batch 51/64 loss: 0.10457020998001099
Batch 52/64 loss: 0.08361005783081055
Batch 53/64 loss: 0.08242166042327881
Batch 54/64 loss: 0.0744704008102417
Batch 55/64 loss: 0.0724327564239502
Batch 56/64 loss: 0.08433747291564941
Batch 57/64 loss: 0.10412603616714478
Batch 58/64 loss: 0.074504554271698
Batch 59/64 loss: 0.07615602016448975
Batch 60/64 loss: 0.060601115226745605
Batch 61/64 loss: 0.056919217109680176
Batch 62/64 loss: 0.06378591060638428
Batch 63/64 loss: 0.07052087783813477
Batch 64/64 loss: 0.07022327184677124
Epoch 26  Train loss: 0.0770596188657424  Val loss: 0.11909503891705647
Epoch 27
-------------------------------
Batch 1/64 loss: 0.07137036323547363
Batch 2/64 loss: 0.08265256881713867
Batch 3/64 loss: 0.06381642818450928
Batch 4/64 loss: 0.06411123275756836
Batch 5/64 loss: 0.05771589279174805
Batch 6/64 loss: 0.08081328868865967
Batch 7/64 loss: 0.07186293601989746
Batch 8/64 loss: 0.06776678562164307
Batch 9/64 loss: 0.08003753423690796
Batch 10/64 loss: 0.10102665424346924
Batch 11/64 loss: 0.06762862205505371
Batch 12/64 loss: 0.0892643928527832
Batch 13/64 loss: 0.07017356157302856
Batch 14/64 loss: 0.08583199977874756
Batch 15/64 loss: 0.07206839323043823
Batch 16/64 loss: 0.06738221645355225
Batch 17/64 loss: 0.05633801221847534
Batch 18/64 loss: 0.07094687223434448
Batch 19/64 loss: 0.07151281833648682
Batch 20/64 loss: 0.07678204774856567
Batch 21/64 loss: 0.08513891696929932
Batch 22/64 loss: 0.08522838354110718
Batch 23/64 loss: 0.04755884408950806
Batch 24/64 loss: 0.08488011360168457
Batch 25/64 loss: 0.06441003084182739
Batch 26/64 loss: 0.06256186962127686
Batch 27/64 loss: 0.06899374723434448
Batch 28/64 loss: 0.06930935382843018
Batch 29/64 loss: 0.10007774829864502
Batch 30/64 loss: 0.07368814945220947
Batch 31/64 loss: 0.06560271978378296
Batch 32/64 loss: 0.0721125602722168
Batch 33/64 loss: 0.0785178542137146
Batch 34/64 loss: 0.0575329065322876
Batch 35/64 loss: 0.07806545495986938
Batch 36/64 loss: 0.07551032304763794
Batch 37/64 loss: 0.07471209764480591
Batch 38/64 loss: 0.0516471266746521
Batch 39/64 loss: 0.06519556045532227
Batch 40/64 loss: 0.08416217565536499
Batch 41/64 loss: 0.07547032833099365
Batch 42/64 loss: 0.08969706296920776
Batch 43/64 loss: 0.08464312553405762
Batch 44/64 loss: 0.04733431339263916
Batch 45/64 loss: 0.06243550777435303
Batch 46/64 loss: 0.07355082035064697
Batch 47/64 loss: 0.06035161018371582
Batch 48/64 loss: 0.06316065788269043
Batch 49/64 loss: 0.08524829149246216
Batch 50/64 loss: 0.0763556957244873
Batch 51/64 loss: 0.07806295156478882
Batch 52/64 loss: 0.08373141288757324
Batch 53/64 loss: 0.0559733510017395
Batch 54/64 loss: 0.08169138431549072
Batch 55/64 loss: 0.07535362243652344
Batch 56/64 loss: 0.07091832160949707
Batch 57/64 loss: 0.05498284101486206
Batch 58/64 loss: 0.06825006008148193
Batch 59/64 loss: 0.059467196464538574
Batch 60/64 loss: 0.07714557647705078
Batch 61/64 loss: 0.057755231857299805
Batch 62/64 loss: 0.07556229829788208
Batch 63/64 loss: 0.09366869926452637
Batch 64/64 loss: 0.08670759201049805
Epoch 27  Train loss: 0.0726250919641233  Val loss: 0.08859561870188237
Saving best model, epoch: 27
Epoch 28
-------------------------------
Batch 1/64 loss: 0.07666993141174316
Batch 2/64 loss: 0.0706673264503479
Batch 3/64 loss: 0.0769205093383789
Batch 4/64 loss: 0.07055056095123291
Batch 5/64 loss: 0.07064962387084961
Batch 6/64 loss: 0.08350586891174316
Batch 7/64 loss: 0.07615065574645996
Batch 8/64 loss: 0.07638269662857056
Batch 9/64 loss: 0.0780109167098999
Batch 10/64 loss: 0.10088151693344116
Batch 11/64 loss: 0.07973676919937134
Batch 12/64 loss: 0.11015266180038452
Batch 13/64 loss: 0.06859540939331055
Batch 14/64 loss: 0.05888646841049194
Batch 15/64 loss: 0.03914618492126465
Batch 16/64 loss: 0.058663368225097656
Batch 17/64 loss: 0.03962385654449463
Batch 18/64 loss: 0.04779648780822754
Batch 19/64 loss: 0.08457863330841064
Batch 20/64 loss: 0.07667773962020874
Batch 21/64 loss: 0.06892931461334229
Batch 22/64 loss: 0.050998032093048096
Batch 23/64 loss: 0.07879757881164551
Batch 24/64 loss: 0.052582383155822754
Batch 25/64 loss: 0.07114952802658081
Batch 26/64 loss: 0.06556802988052368
Batch 27/64 loss: 0.08449876308441162
Batch 28/64 loss: 0.0836135745048523
Batch 29/64 loss: 0.09275156259536743
Batch 30/64 loss: 0.07207351922988892
Batch 31/64 loss: 0.0542868971824646
Batch 32/64 loss: 0.0943155288696289
Batch 33/64 loss: 0.055181920528411865
Batch 34/64 loss: 0.0649106502532959
Batch 35/64 loss: 0.06397414207458496
Batch 36/64 loss: 0.07324403524398804
Batch 37/64 loss: 0.06639862060546875
Batch 38/64 loss: 0.05607271194458008
Batch 39/64 loss: 0.05758833885192871
Batch 40/64 loss: 0.07625162601470947
Batch 41/64 loss: 0.07847088575363159
Batch 42/64 loss: 0.056390345096588135
Batch 43/64 loss: 0.056234538555145264
Batch 44/64 loss: 0.042356789112091064
Batch 45/64 loss: 0.07079076766967773
Batch 46/64 loss: 0.08123832941055298
Batch 47/64 loss: 0.0750611424446106
Batch 48/64 loss: 0.04870706796646118
Batch 49/64 loss: 0.0675276517868042
Batch 50/64 loss: 0.045645296573638916
Batch 51/64 loss: 0.07877731323242188
Batch 52/64 loss: 0.057907700538635254
Batch 53/64 loss: 0.061419904232025146
Batch 54/64 loss: 0.05658537149429321
Batch 55/64 loss: 0.0422368049621582
Batch 56/64 loss: 0.0676652193069458
Batch 57/64 loss: 0.061557769775390625
Batch 58/64 loss: 0.052842020988464355
Batch 59/64 loss: 0.06953579187393188
Batch 60/64 loss: 0.06382960081100464
Batch 61/64 loss: 0.04942202568054199
Batch 62/64 loss: 0.06788754463195801
Batch 63/64 loss: 0.04782390594482422
Batch 64/64 loss: 0.0596846342086792
Epoch 28  Train loss: 0.06701350913328283  Val loss: 0.0891998178770452
Epoch 29
-------------------------------
Batch 1/64 loss: 0.03873211145401001
Batch 2/64 loss: 0.0694509744644165
Batch 3/64 loss: 0.06580054759979248
Batch 4/64 loss: 0.06596142053604126
Batch 5/64 loss: 0.09152346849441528
Batch 6/64 loss: 0.05329412221908569
Batch 7/64 loss: 0.056388139724731445
Batch 8/64 loss: 0.06853187084197998
Batch 9/64 loss: 0.0503849983215332
Batch 10/64 loss: 0.07514059543609619
Batch 11/64 loss: 0.049524784088134766
Batch 12/64 loss: 0.0572514533996582
Batch 13/64 loss: 0.0533519983291626
Batch 14/64 loss: 0.08925306797027588
Batch 15/64 loss: 0.05816930532455444
Batch 16/64 loss: 0.04184037446975708
Batch 17/64 loss: 0.07654649019241333
Batch 18/64 loss: 0.07271850109100342
Batch 19/64 loss: 0.08527523279190063
Batch 20/64 loss: 0.07017755508422852
Batch 21/64 loss: 0.05829942226409912
Batch 22/64 loss: 0.0632394552230835
Batch 23/64 loss: 0.05511671304702759
Batch 24/64 loss: 0.06417220830917358
Batch 25/64 loss: 0.0810537338256836
Batch 26/64 loss: 0.07328712940216064
Batch 27/64 loss: 0.06591665744781494
Batch 28/64 loss: 0.07502222061157227
Batch 29/64 loss: 0.06769800186157227
Batch 30/64 loss: 0.06940531730651855
Batch 31/64 loss: 0.06978458166122437
Batch 32/64 loss: 0.07318896055221558
Batch 33/64 loss: 0.04802221059799194
Batch 34/64 loss: 0.0824810266494751
Batch 35/64 loss: 0.06301212310791016
Batch 36/64 loss: 0.07057845592498779
Batch 37/64 loss: 0.05660557746887207
Batch 38/64 loss: 0.061461687088012695
Batch 39/64 loss: 0.0559995174407959
Batch 40/64 loss: 0.05653727054595947
Batch 41/64 loss: 0.08292776346206665
Batch 42/64 loss: 0.0803566575050354
Batch 43/64 loss: 0.050865232944488525
Batch 44/64 loss: 0.06597602367401123
Batch 45/64 loss: 0.059312283992767334
Batch 46/64 loss: 0.08008235692977905
Batch 47/64 loss: 0.06873995065689087
Batch 48/64 loss: 0.09498864412307739
Batch 49/64 loss: 0.07498127222061157
Batch 50/64 loss: 0.05994284152984619
Batch 51/64 loss: 0.06164783239364624
Batch 52/64 loss: 0.04092305898666382
Batch 53/64 loss: 0.07555592060089111
Batch 54/64 loss: 0.06088221073150635
Batch 55/64 loss: 0.07404196262359619
Batch 56/64 loss: 0.05478715896606445
Batch 57/64 loss: 0.05128830671310425
Batch 58/64 loss: 0.056087613105773926
Batch 59/64 loss: 0.07075554132461548
Batch 60/64 loss: 0.06497001647949219
Batch 61/64 loss: 0.06470143795013428
Batch 62/64 loss: 0.03715604543685913
Batch 63/64 loss: 0.05478942394256592
Batch 64/64 loss: 0.05167955160140991
Epoch 29  Train loss: 0.06470146670060999  Val loss: 0.08333628427531711
Saving best model, epoch: 29
Epoch 30
-------------------------------
Batch 1/64 loss: 0.057088494300842285
Batch 2/64 loss: 0.041551411151885986
Batch 3/64 loss: 0.03640776872634888
Batch 4/64 loss: 0.04043757915496826
Batch 5/64 loss: 0.06454181671142578
Batch 6/64 loss: 0.03507184982299805
Batch 7/64 loss: 0.057033538818359375
Batch 8/64 loss: 0.05307716131210327
Batch 9/64 loss: 0.0845230221748352
Batch 10/64 loss: 0.06124669313430786
Batch 11/64 loss: 0.040398478507995605
Batch 12/64 loss: 0.07312202453613281
Batch 13/64 loss: 0.05406546592712402
Batch 14/64 loss: 0.049946367740631104
Batch 15/64 loss: 0.08684647083282471
Batch 16/64 loss: 0.06995552778244019
Batch 17/64 loss: 0.062451064586639404
Batch 18/64 loss: 0.07951611280441284
Batch 19/64 loss: 0.06284928321838379
Batch 20/64 loss: 0.08032727241516113
Batch 21/64 loss: 0.06556349992752075
Batch 22/64 loss: 0.04454076290130615
Batch 23/64 loss: 0.09206211566925049
Batch 24/64 loss: 0.0654594898223877
Batch 25/64 loss: 0.046290040016174316
Batch 26/64 loss: 0.05537694692611694
Batch 27/64 loss: 0.03304934501647949
Batch 28/64 loss: 0.04993486404418945
Batch 29/64 loss: 0.040667951107025146
Batch 30/64 loss: 0.04919523000717163
Batch 31/64 loss: 0.055821359157562256
Batch 32/64 loss: 0.07587915658950806
Batch 33/64 loss: 0.05793607234954834
Batch 34/64 loss: 0.052755653858184814
Batch 35/64 loss: 0.055063843727111816
Batch 36/64 loss: 0.08130824565887451
Batch 37/64 loss: 0.04365873336791992
Batch 38/64 loss: 0.04618293046951294
Batch 39/64 loss: 0.07528883218765259
Batch 40/64 loss: 0.07583212852478027
Batch 41/64 loss: 0.07528603076934814
Batch 42/64 loss: 0.06158202886581421
Batch 43/64 loss: 0.07091915607452393
Batch 44/64 loss: 0.044159650802612305
Batch 45/64 loss: 0.05307358503341675
Batch 46/64 loss: 0.04509115219116211
Batch 47/64 loss: 0.08312219381332397
Batch 48/64 loss: 0.0706181526184082
Batch 49/64 loss: 0.055386364459991455
Batch 50/64 loss: 0.0577160120010376
Batch 51/64 loss: 0.04090869426727295
Batch 52/64 loss: 0.07048916816711426
Batch 53/64 loss: 0.048009634017944336
Batch 54/64 loss: 0.049963951110839844
Batch 55/64 loss: 0.06527984142303467
Batch 56/64 loss: 0.07804781198501587
Batch 57/64 loss: 0.10201078653335571
Batch 58/64 loss: 0.026834726333618164
Batch 59/64 loss: 0.05025005340576172
Batch 60/64 loss: 0.04528820514678955
Batch 61/64 loss: 0.0707818865776062
Batch 62/64 loss: 0.08067387342453003
Batch 63/64 loss: 0.05296969413757324
Batch 64/64 loss: 0.06217843294143677
Epoch 30  Train loss: 0.059567389534968956  Val loss: 0.10370473538067743
Epoch 31
-------------------------------
Batch 1/64 loss: 0.05439627170562744
Batch 2/64 loss: 0.07873421907424927
Batch 3/64 loss: 0.05548596382141113
Batch 4/64 loss: 0.05971384048461914
Batch 5/64 loss: 0.052829623222351074
Batch 6/64 loss: 0.06461524963378906
Batch 7/64 loss: 0.034158945083618164
Batch 8/64 loss: 0.06705683469772339
Batch 9/64 loss: 0.046209514141082764
Batch 10/64 loss: 0.061571598052978516
Batch 11/64 loss: 0.056261420249938965
Batch 12/64 loss: 0.05566442012786865
Batch 13/64 loss: 0.045446038246154785
Batch 14/64 loss: 0.06582057476043701
Batch 15/64 loss: 0.043174684047698975
Batch 16/64 loss: 0.07936912775039673
Batch 17/64 loss: 0.05239105224609375
Batch 18/64 loss: 0.04190486669540405
Batch 19/64 loss: 0.07132697105407715
Batch 20/64 loss: 0.02756059169769287
Batch 21/64 loss: 0.05426943302154541
Batch 22/64 loss: 0.05200153589248657
Batch 23/64 loss: 0.052466630935668945
Batch 24/64 loss: 0.05521571636199951
Batch 25/64 loss: 0.05000275373458862
Batch 26/64 loss: 0.05469632148742676
Batch 27/64 loss: 0.04334461688995361
Batch 28/64 loss: 0.04415494203567505
Batch 29/64 loss: 0.05829489231109619
Batch 30/64 loss: 0.06159937381744385
Batch 31/64 loss: 0.03108835220336914
Batch 32/64 loss: 0.06344330310821533
Batch 33/64 loss: 0.061447203159332275
Batch 34/64 loss: 0.047094881534576416
Batch 35/64 loss: 0.04481816291809082
Batch 36/64 loss: 0.07263904809951782
Batch 37/64 loss: 0.033652663230895996
Batch 38/64 loss: 0.06651675701141357
Batch 39/64 loss: 0.032570838928222656
Batch 40/64 loss: 0.046803057193756104
Batch 41/64 loss: 0.07751429080963135
Batch 42/64 loss: 0.04966694116592407
Batch 43/64 loss: 0.07794177532196045
Batch 44/64 loss: 0.03361046314239502
Batch 45/64 loss: 0.06027984619140625
Batch 46/64 loss: 0.08154910802841187
Batch 47/64 loss: 0.0683477520942688
Batch 48/64 loss: 0.04718327522277832
Batch 49/64 loss: 0.04449260234832764
Batch 50/64 loss: 0.058618366718292236
Batch 51/64 loss: 0.04770082235336304
Batch 52/64 loss: 0.0701744556427002
Batch 53/64 loss: 0.052400171756744385
Batch 54/64 loss: 0.06030827760696411
Batch 55/64 loss: 0.05393481254577637
Batch 56/64 loss: 0.04747200012207031
Batch 57/64 loss: 0.06444168090820312
Batch 58/64 loss: 0.06930655241012573
Batch 59/64 loss: 0.06053107976913452
Batch 60/64 loss: 0.06273496150970459
Batch 61/64 loss: 0.059788405895233154
Batch 62/64 loss: 0.07167434692382812
Batch 63/64 loss: 0.04867851734161377
Batch 64/64 loss: 0.08883464336395264
Epoch 31  Train loss: 0.056075118102279366  Val loss: 0.084262251239462
Epoch 32
-------------------------------
Batch 1/64 loss: 0.06963217258453369
Batch 2/64 loss: 0.0906674861907959
Batch 3/64 loss: 0.05105555057525635
Batch 4/64 loss: 0.06728708744049072
Batch 5/64 loss: 0.07308685779571533
Batch 6/64 loss: 0.044063687324523926
Batch 7/64 loss: 0.0759010910987854
Batch 8/64 loss: 0.0710299015045166
Batch 9/64 loss: 0.05649268627166748
Batch 10/64 loss: 0.060068726539611816
Batch 11/64 loss: 0.06321227550506592
Batch 12/64 loss: 0.05259084701538086
Batch 13/64 loss: 0.05495697259902954
Batch 14/64 loss: 0.062262773513793945
Batch 15/64 loss: 0.05382943153381348
Batch 16/64 loss: 0.04245352745056152
Batch 17/64 loss: 0.04021930694580078
Batch 18/64 loss: 0.05705028772354126
Batch 19/64 loss: 0.04154491424560547
Batch 20/64 loss: 0.06792283058166504
Batch 21/64 loss: 0.06116265058517456
Batch 22/64 loss: 0.06036311388015747
Batch 23/64 loss: 0.04206681251525879
Batch 24/64 loss: 0.05277550220489502
Batch 25/64 loss: 0.03347510099411011
Batch 26/64 loss: 0.05739772319793701
Batch 27/64 loss: 0.0545351505279541
Batch 28/64 loss: 0.039868295192718506
Batch 29/64 loss: 0.05479586124420166
Batch 30/64 loss: 0.05259573459625244
Batch 31/64 loss: 0.03956407308578491
Batch 32/64 loss: 0.052828848361968994
Batch 33/64 loss: 0.06984484195709229
Batch 34/64 loss: 0.05099797248840332
Batch 35/64 loss: 0.04862135648727417
Batch 36/64 loss: 0.055169641971588135
Batch 37/64 loss: 0.04830735921859741
Batch 38/64 loss: 0.033525943756103516
Batch 39/64 loss: 0.04539209604263306
Batch 40/64 loss: 0.023554205894470215
Batch 41/64 loss: 0.052500128746032715
Batch 42/64 loss: 0.07567703723907471
Batch 43/64 loss: 0.04914522171020508
Batch 44/64 loss: 0.033761024475097656
Batch 45/64 loss: 0.06246805191040039
Batch 46/64 loss: 0.039816439151763916
Batch 47/64 loss: 0.04495590925216675
Batch 48/64 loss: 0.062686026096344
Batch 49/64 loss: 0.04082965850830078
Batch 50/64 loss: 0.04399299621582031
Batch 51/64 loss: 0.08654308319091797
Batch 52/64 loss: 0.03699493408203125
Batch 53/64 loss: 0.05600261688232422
Batch 54/64 loss: 0.06746405363082886
Batch 55/64 loss: 0.06295669078826904
Batch 56/64 loss: 0.04740297794342041
Batch 57/64 loss: 0.07005584239959717
Batch 58/64 loss: 0.048534512519836426
Batch 59/64 loss: 0.06782227754592896
Batch 60/64 loss: 0.029271423816680908
Batch 61/64 loss: 0.05837291479110718
Batch 62/64 loss: 0.04056835174560547
Batch 63/64 loss: 0.05057108402252197
Batch 64/64 loss: 0.08468782901763916
Epoch 32  Train loss: 0.054338507091297825  Val loss: 0.08310382042553827
Saving best model, epoch: 32
Epoch 33
-------------------------------
Batch 1/64 loss: 0.06289762258529663
Batch 2/64 loss: 0.059629201889038086
Batch 3/64 loss: 0.05513375997543335
Batch 4/64 loss: 0.059291183948516846
Batch 5/64 loss: 0.06770676374435425
Batch 6/64 loss: 0.0648660659790039
Batch 7/64 loss: 0.07146233320236206
Batch 8/64 loss: 0.04373115301132202
Batch 9/64 loss: 0.06826537847518921
Batch 10/64 loss: 0.07651853561401367
Batch 11/64 loss: 0.05350518226623535
Batch 12/64 loss: 0.043299078941345215
Batch 13/64 loss: 0.05659610033035278
Batch 14/64 loss: 0.055037856101989746
Batch 15/64 loss: 0.055430054664611816
Batch 16/64 loss: 0.05763214826583862
Batch 17/64 loss: 0.05472266674041748
Batch 18/64 loss: 0.07372933626174927
Batch 19/64 loss: 0.07564979791641235
Batch 20/64 loss: 0.07981562614440918
Batch 21/64 loss: 0.047591447830200195
Batch 22/64 loss: 0.049576759338378906
Batch 23/64 loss: 0.059825241565704346
Batch 24/64 loss: 0.04933220148086548
Batch 25/64 loss: 0.06360197067260742
Batch 26/64 loss: 0.0595209002494812
Batch 27/64 loss: 0.06827908754348755
Batch 28/64 loss: 0.0511816143989563
Batch 29/64 loss: 0.04639887809753418
Batch 30/64 loss: 0.04176288843154907
Batch 31/64 loss: 0.06256836652755737
Batch 32/64 loss: 0.04122519493103027
Batch 33/64 loss: 0.05825471878051758
Batch 34/64 loss: 0.06319868564605713
Batch 35/64 loss: 0.028223812580108643
Batch 36/64 loss: 0.05484062433242798
Batch 37/64 loss: 0.06634718179702759
Batch 38/64 loss: 0.04383748769760132
Batch 39/64 loss: 0.03537452220916748
Batch 40/64 loss: 0.03811722993850708
Batch 41/64 loss: 0.025076627731323242
Batch 42/64 loss: 0.0702972412109375
Batch 43/64 loss: 0.05159372091293335
Batch 44/64 loss: 0.040887534618377686
Batch 45/64 loss: 0.037667274475097656
Batch 46/64 loss: 0.052690863609313965
Batch 47/64 loss: 0.08009779453277588
Batch 48/64 loss: 0.03898876905441284
Batch 49/64 loss: 0.06558185815811157
Batch 50/64 loss: 0.04253596067428589
Batch 51/64 loss: 0.037196218967437744
Batch 52/64 loss: 0.059259653091430664
Batch 53/64 loss: 0.049959421157836914
Batch 54/64 loss: 0.05983489751815796
Batch 55/64 loss: 0.061064839363098145
Batch 56/64 loss: 0.056110501289367676
Batch 57/64 loss: 0.06609869003295898
Batch 58/64 loss: 0.04027599096298218
Batch 59/64 loss: 0.048171281814575195
Batch 60/64 loss: 0.05638599395751953
Batch 61/64 loss: 0.02798992395401001
Batch 62/64 loss: 0.051218390464782715
Batch 63/64 loss: 0.05490344762802124
Batch 64/64 loss: 0.08318012952804565
Epoch 33  Train loss: 0.054905892587175556  Val loss: 0.0725966059055525
Saving best model, epoch: 33
Epoch 34
-------------------------------
Batch 1/64 loss: 0.05668407678604126
Batch 2/64 loss: 0.018619835376739502
Batch 3/64 loss: 0.041408002376556396
Batch 4/64 loss: 0.05255424976348877
Batch 5/64 loss: 0.08299386501312256
Batch 6/64 loss: 0.07078933715820312
Batch 7/64 loss: 0.05241870880126953
Batch 8/64 loss: 0.06769996881484985
Batch 9/64 loss: 0.0268365740776062
Batch 10/64 loss: 0.05731081962585449
Batch 11/64 loss: 0.04135704040527344
Batch 12/64 loss: 0.08083856105804443
Batch 13/64 loss: 0.055721819400787354
Batch 14/64 loss: 0.04856753349304199
Batch 15/64 loss: 0.061300575733184814
Batch 16/64 loss: 0.037856340408325195
Batch 17/64 loss: 0.03979301452636719
Batch 18/64 loss: 0.05070459842681885
Batch 19/64 loss: 0.06719380617141724
Batch 20/64 loss: 0.04710578918457031
Batch 21/64 loss: 0.05856442451477051
Batch 22/64 loss: 0.043475210666656494
Batch 23/64 loss: 0.055252671241760254
Batch 24/64 loss: 0.04059183597564697
Batch 25/64 loss: 0.036284804344177246
Batch 26/64 loss: 0.03989553451538086
Batch 27/64 loss: 0.03672981262207031
Batch 28/64 loss: 0.06174725294113159
Batch 29/64 loss: 0.06714659929275513
Batch 30/64 loss: 0.04387974739074707
Batch 31/64 loss: 0.032699763774871826
Batch 32/64 loss: 0.05469107627868652
Batch 33/64 loss: 0.04571878910064697
Batch 34/64 loss: 0.052333831787109375
Batch 35/64 loss: 0.048289954662323
Batch 36/64 loss: 0.052944302558898926
Batch 37/64 loss: 0.053943932056427
Batch 38/64 loss: 0.043646931648254395
Batch 39/64 loss: 0.06112563610076904
Batch 40/64 loss: 0.05317866802215576
Batch 41/64 loss: 0.04801064729690552
Batch 42/64 loss: 0.034223735332489014
Batch 43/64 loss: 0.06421434879302979
Batch 44/64 loss: 0.038178205490112305
Batch 45/64 loss: 0.026489734649658203
Batch 46/64 loss: 0.030933380126953125
Batch 47/64 loss: 0.06211668252944946
Batch 48/64 loss: 0.04947817325592041
Batch 49/64 loss: 0.05892151594161987
Batch 50/64 loss: 0.05688035488128662
Batch 51/64 loss: 0.05878192186355591
Batch 52/64 loss: 0.03898042440414429
Batch 53/64 loss: 0.06924742460250854
Batch 54/64 loss: 0.05361539125442505
Batch 55/64 loss: 0.05582922697067261
Batch 56/64 loss: 0.050643205642700195
Batch 57/64 loss: 0.039483726024627686
Batch 58/64 loss: 0.02952498197555542
Batch 59/64 loss: 0.04635590314865112
Batch 60/64 loss: 0.05728942155838013
Batch 61/64 loss: 0.03834491968154907
Batch 62/64 loss: 0.031937599182128906
Batch 63/64 loss: 0.07247447967529297
Batch 64/64 loss: 0.07379782199859619
Epoch 34  Train loss: 0.050309004970625334  Val loss: 0.07673995806179505
Epoch 35
-------------------------------
Batch 1/64 loss: 0.03399801254272461
Batch 2/64 loss: 0.06273210048675537
Batch 3/64 loss: 0.07716751098632812
Batch 4/64 loss: 0.06480187177658081
Batch 5/64 loss: 0.03727138042449951
Batch 6/64 loss: 0.060544371604919434
Batch 7/64 loss: 0.0666542649269104
Batch 8/64 loss: 0.05464881658554077
Batch 9/64 loss: 0.020588397979736328
Batch 10/64 loss: 0.035176098346710205
Batch 11/64 loss: 0.04823035001754761
Batch 12/64 loss: 0.04882615804672241
Batch 13/64 loss: 0.03201448917388916
Batch 14/64 loss: 0.025123953819274902
Batch 15/64 loss: 0.040387630462646484
Batch 16/64 loss: 0.030695855617523193
Batch 17/64 loss: 0.05958276987075806
Batch 18/64 loss: 0.039994239807128906
Batch 19/64 loss: 0.043607890605926514
Batch 20/64 loss: 0.05421280860900879
Batch 21/64 loss: 0.06267541646957397
Batch 22/64 loss: 0.0439797043800354
Batch 23/64 loss: 0.05654096603393555
Batch 24/64 loss: 0.04272055625915527
Batch 25/64 loss: 0.07423341274261475
Batch 26/64 loss: 0.06269341707229614
Batch 27/64 loss: 0.04607272148132324
Batch 28/64 loss: 0.050759077072143555
Batch 29/64 loss: 0.051506757736206055
Batch 30/64 loss: 0.05552619695663452
Batch 31/64 loss: 0.03681457042694092
Batch 32/64 loss: 0.050255000591278076
Batch 33/64 loss: 0.051823437213897705
Batch 34/64 loss: 0.030243754386901855
Batch 35/64 loss: 0.05963975191116333
Batch 36/64 loss: 0.05889856815338135
Batch 37/64 loss: 0.06363600492477417
Batch 38/64 loss: 0.039777398109436035
Batch 39/64 loss: 0.06175512075424194
Batch 40/64 loss: 0.03782081604003906
Batch 41/64 loss: 0.05614185333251953
Batch 42/64 loss: 0.03730720281600952
Batch 43/64 loss: 0.026882410049438477
Batch 44/64 loss: 0.02725452184677124
Batch 45/64 loss: 0.04726022481918335
Batch 46/64 loss: 0.06617653369903564
Batch 47/64 loss: 0.029905378818511963
Batch 48/64 loss: 0.034015655517578125
Batch 49/64 loss: 0.03617048263549805
Batch 50/64 loss: 0.059664905071258545
Batch 51/64 loss: 0.03567993640899658
Batch 52/64 loss: 0.04448378086090088
Batch 53/64 loss: 0.04877251386642456
Batch 54/64 loss: 0.04244422912597656
Batch 55/64 loss: 0.05294770002365112
Batch 56/64 loss: 0.04240310192108154
Batch 57/64 loss: 0.04748642444610596
Batch 58/64 loss: 0.0834469199180603
Batch 59/64 loss: 0.03700941801071167
Batch 60/64 loss: 0.05195474624633789
Batch 61/64 loss: 0.04323667287826538
Batch 62/64 loss: 0.04917430877685547
Batch 63/64 loss: 0.027742981910705566
Batch 64/64 loss: 0.07312965393066406
Epoch 35  Train loss: 0.04793789433498009  Val loss: 0.06427979121093488
Saving best model, epoch: 35
Epoch 36
-------------------------------
Batch 1/64 loss: 0.046832919120788574
Batch 2/64 loss: 0.04453694820404053
Batch 3/64 loss: 0.0631757378578186
Batch 4/64 loss: 0.03755974769592285
Batch 5/64 loss: 0.03446316719055176
Batch 6/64 loss: 0.042743027210235596
Batch 7/64 loss: 0.06424438953399658
Batch 8/64 loss: 0.053019940853118896
Batch 9/64 loss: 0.05973684787750244
Batch 10/64 loss: 0.04309093952178955
Batch 11/64 loss: 0.025797665119171143
Batch 12/64 loss: 0.0438387393951416
Batch 13/64 loss: 0.040360867977142334
Batch 14/64 loss: 0.01775747537612915
Batch 15/64 loss: 0.04676926136016846
Batch 16/64 loss: 0.038860201835632324
Batch 17/64 loss: 0.04945516586303711
Batch 18/64 loss: 0.05001574754714966
Batch 19/64 loss: 0.03217661380767822
Batch 20/64 loss: 0.04341721534729004
Batch 21/64 loss: 0.045981645584106445
Batch 22/64 loss: 0.03653395175933838
Batch 23/64 loss: 0.06147092580795288
Batch 24/64 loss: 0.052645206451416016
Batch 25/64 loss: 0.07610034942626953
Batch 26/64 loss: 0.05833613872528076
Batch 27/64 loss: 0.044402241706848145
Batch 28/64 loss: 0.039099037647247314
Batch 29/64 loss: 0.042669832706451416
Batch 30/64 loss: 0.05904930830001831
Batch 31/64 loss: 0.037790536880493164
Batch 32/64 loss: 0.03582507371902466
Batch 33/64 loss: 0.029308676719665527
Batch 34/64 loss: 0.04224175214767456
Batch 35/64 loss: 0.04610651731491089
Batch 36/64 loss: 0.04103374481201172
Batch 37/64 loss: 0.03059566020965576
Batch 38/64 loss: 0.042785465717315674
Batch 39/64 loss: 0.046662211418151855
Batch 40/64 loss: 0.07007622718811035
Batch 41/64 loss: 0.07116478681564331
Batch 42/64 loss: 0.02778637409210205
Batch 43/64 loss: 0.042235612869262695
Batch 44/64 loss: 0.030147552490234375
Batch 45/64 loss: 0.05320298671722412
Batch 46/64 loss: 0.029853403568267822
Batch 47/64 loss: 0.04943209886550903
Batch 48/64 loss: 0.03220778703689575
Batch 49/64 loss: 0.04161369800567627
Batch 50/64 loss: 0.07601946592330933
Batch 51/64 loss: 0.05452871322631836
Batch 52/64 loss: 0.04316025972366333
Batch 53/64 loss: 0.03671771287918091
Batch 54/64 loss: 0.04845458269119263
Batch 55/64 loss: 0.055998802185058594
Batch 56/64 loss: 0.04271388053894043
Batch 57/64 loss: 0.054161787033081055
Batch 58/64 loss: 0.04861396551132202
Batch 59/64 loss: 0.061864614486694336
Batch 60/64 loss: 0.034849464893341064
Batch 61/64 loss: 0.023183107376098633
Batch 62/64 loss: 0.058867037296295166
Batch 63/64 loss: 0.030543744564056396
Batch 64/64 loss: 0.04569089412689209
Epoch 36  Train loss: 0.04546128207562016  Val loss: 0.08078265210607208
Epoch 37
-------------------------------
Batch 1/64 loss: 0.04412585496902466
Batch 2/64 loss: 0.05973321199417114
Batch 3/64 loss: 0.04111373424530029
Batch 4/64 loss: 0.05115997791290283
Batch 5/64 loss: 0.0512920618057251
Batch 6/64 loss: 0.05941975116729736
Batch 7/64 loss: 0.05898863077163696
Batch 8/64 loss: 0.05605816841125488
Batch 9/64 loss: 0.0657358169555664
Batch 10/64 loss: 0.038751959800720215
Batch 11/64 loss: 0.0469585657119751
Batch 12/64 loss: 0.05100655555725098
Batch 13/64 loss: 0.03566551208496094
Batch 14/64 loss: 0.034770429134368896
Batch 15/64 loss: 0.054747164249420166
Batch 16/64 loss: 0.03221619129180908
Batch 17/64 loss: 0.04174691438674927
Batch 18/64 loss: 0.026033222675323486
Batch 19/64 loss: 0.049003422260284424
Batch 20/64 loss: 0.06312769651412964
Batch 21/64 loss: 0.035900890827178955
Batch 22/64 loss: 0.040498197078704834
Batch 23/64 loss: 0.047028958797454834
Batch 24/64 loss: 0.03840184211730957
Batch 25/64 loss: 0.028160035610198975
Batch 26/64 loss: 0.037592172622680664
Batch 27/64 loss: 0.04620939493179321
Batch 28/64 loss: 0.050924837589263916
Batch 29/64 loss: 0.051843106746673584
Batch 30/64 loss: 0.030115127563476562
Batch 31/64 loss: 0.06016945838928223
Batch 32/64 loss: 0.03445917367935181
Batch 33/64 loss: 0.03345358371734619
Batch 34/64 loss: 0.06464958190917969
Batch 35/64 loss: 0.04186141490936279
Batch 36/64 loss: 0.04289817810058594
Batch 37/64 loss: 0.041168212890625
Batch 38/64 loss: 0.043541014194488525
Batch 39/64 loss: 0.05273193120956421
Batch 40/64 loss: 0.04543966054916382
Batch 41/64 loss: 0.04567652940750122
Batch 42/64 loss: 0.07670742273330688
Batch 43/64 loss: 0.0522729754447937
Batch 44/64 loss: 0.044876277446746826
Batch 45/64 loss: 0.032531023025512695
Batch 46/64 loss: 0.06599277257919312
Batch 47/64 loss: 0.056718289852142334
Batch 48/64 loss: 0.022430121898651123
Batch 49/64 loss: 0.02727562189102173
Batch 50/64 loss: 0.043110787868499756
Batch 51/64 loss: 0.02457946538925171
Batch 52/64 loss: 0.03706425428390503
Batch 53/64 loss: 0.0248071551322937
Batch 54/64 loss: 0.054684221744537354
Batch 55/64 loss: 0.029064953327178955
Batch 56/64 loss: 0.03579413890838623
Batch 57/64 loss: 0.01986253261566162
Batch 58/64 loss: 0.0446019172668457
Batch 59/64 loss: 0.04723411798477173
Batch 60/64 loss: 0.055436134338378906
Batch 61/64 loss: 0.04183387756347656
Batch 62/64 loss: 0.03783011436462402
Batch 63/64 loss: 0.03486335277557373
Batch 64/64 loss: 0.039013564586639404
Epoch 37  Train loss: 0.044128781907698686  Val loss: 0.07376542636209338
Epoch 38
-------------------------------
Batch 1/64 loss: 0.02637350559234619
Batch 2/64 loss: 0.046343326568603516
Batch 3/64 loss: 0.04931151866912842
Batch 4/64 loss: 0.05710935592651367
Batch 5/64 loss: 0.04012781381607056
Batch 6/64 loss: 0.03028768301010132
Batch 7/64 loss: 0.03968322277069092
Batch 8/64 loss: 0.0510406494140625
Batch 9/64 loss: 0.03251570463180542
Batch 10/64 loss: 0.03450942039489746
Batch 11/64 loss: 0.04874521493911743
Batch 12/64 loss: 0.0371478796005249
Batch 13/64 loss: 0.03949081897735596
Batch 14/64 loss: 0.024783730506896973
Batch 15/64 loss: 0.03785836696624756
Batch 16/64 loss: 0.01797008514404297
Batch 17/64 loss: 0.04819166660308838
Batch 18/64 loss: 0.015047788619995117
Batch 19/64 loss: 0.04482799768447876
Batch 20/64 loss: 0.06305938959121704
Batch 21/64 loss: 0.023684978485107422
Batch 22/64 loss: 0.04696166515350342
Batch 23/64 loss: 0.036104023456573486
Batch 24/64 loss: 0.030783891677856445
Batch 25/64 loss: 0.05884057283401489
Batch 26/64 loss: 0.038196563720703125
Batch 27/64 loss: 0.032061636447906494
Batch 28/64 loss: 0.0438077449798584
Batch 29/64 loss: 0.05217313766479492
Batch 30/64 loss: 0.048853278160095215
Batch 31/64 loss: 0.036414265632629395
Batch 32/64 loss: 0.0448533296585083
Batch 33/64 loss: 0.04372727870941162
Batch 34/64 loss: 0.031547605991363525
Batch 35/64 loss: 0.02512812614440918
Batch 36/64 loss: 0.04737430810928345
Batch 37/64 loss: 0.025826692581176758
Batch 38/64 loss: 0.04810267686843872
Batch 39/64 loss: 0.046708106994628906
Batch 40/64 loss: 0.04816305637359619
Batch 41/64 loss: 0.05317533016204834
Batch 42/64 loss: 0.03683829307556152
Batch 43/64 loss: 0.035321056842803955
Batch 44/64 loss: 0.04491317272186279
Batch 45/64 loss: 0.052322447299957275
Batch 46/64 loss: 0.05364161729812622
Batch 47/64 loss: 0.010067641735076904
Batch 48/64 loss: 0.05864518880844116
Batch 49/64 loss: 0.06275564432144165
Batch 50/64 loss: 0.05100739002227783
Batch 51/64 loss: 0.03227013349533081
Batch 52/64 loss: 0.04811638593673706
Batch 53/64 loss: 0.024676024913787842
Batch 54/64 loss: 0.049944162368774414
Batch 55/64 loss: 0.04635906219482422
Batch 56/64 loss: 0.03947937488555908
Batch 57/64 loss: 0.04773974418640137
Batch 58/64 loss: 0.025366902351379395
Batch 59/64 loss: 0.04861217737197876
Batch 60/64 loss: 0.035321056842803955
Batch 61/64 loss: 0.027402162551879883
Batch 62/64 loss: 0.04817652702331543
Batch 63/64 loss: 0.03644222021102905
Batch 64/64 loss: 0.04378175735473633
Epoch 38  Train loss: 0.040708519430721506  Val loss: 0.06886160271274265
Epoch 39
-------------------------------
Batch 1/64 loss: 0.03857874870300293
Batch 2/64 loss: 0.038950324058532715
Batch 3/64 loss: 0.03733104467391968
Batch 4/64 loss: 0.03854358196258545
Batch 5/64 loss: 0.01995021104812622
Batch 6/64 loss: 0.06715887784957886
Batch 7/64 loss: 0.04515808820724487
Batch 8/64 loss: 0.049591779708862305
Batch 9/64 loss: 0.02746272087097168
Batch 10/64 loss: 0.03938215970993042
Batch 11/64 loss: 0.047380924224853516
Batch 12/64 loss: 0.043289124965667725
Batch 13/64 loss: 0.02588266134262085
Batch 14/64 loss: 0.05118107795715332
Batch 15/64 loss: 0.04876124858856201
Batch 16/64 loss: 0.03492391109466553
Batch 17/64 loss: 0.04014742374420166
Batch 18/64 loss: 0.0408363938331604
Batch 19/64 loss: 0.01907670497894287
Batch 20/64 loss: 0.03671455383300781
Batch 21/64 loss: 0.04474222660064697
Batch 22/64 loss: 0.028750121593475342
Batch 23/64 loss: 0.03776252269744873
Batch 24/64 loss: 0.045973241329193115
Batch 25/64 loss: 0.01807457208633423
Batch 26/64 loss: 0.02951115369796753
Batch 27/64 loss: 0.034365952014923096
Batch 28/64 loss: 0.03648465871810913
Batch 29/64 loss: 0.055593252182006836
Batch 30/64 loss: 0.039054274559020996
Batch 31/64 loss: 0.0523945689201355
Batch 32/64 loss: 0.04225873947143555
Batch 33/64 loss: 0.05620819330215454
Batch 34/64 loss: 0.050576984882354736
Batch 35/64 loss: 0.07608044147491455
Batch 36/64 loss: 0.019217967987060547
Batch 37/64 loss: 0.03117227554321289
Batch 38/64 loss: 0.04293292760848999
Batch 39/64 loss: 0.05312079191207886
Batch 40/64 loss: 0.03032684326171875
Batch 41/64 loss: 0.02987128496170044
Batch 42/64 loss: 0.03811514377593994
Batch 43/64 loss: 0.04519784450531006
Batch 44/64 loss: 0.04185605049133301
Batch 45/64 loss: 0.03157776594161987
Batch 46/64 loss: 0.030405402183532715
Batch 47/64 loss: 0.04837965965270996
Batch 48/64 loss: 0.02937471866607666
Batch 49/64 loss: 0.04898405075073242
Batch 50/64 loss: 0.03741168975830078
Batch 51/64 loss: 0.051074087619781494
Batch 52/64 loss: 0.016546189785003662
Batch 53/64 loss: 0.0280076265335083
Batch 54/64 loss: 0.043450355529785156
Batch 55/64 loss: 0.0675010085105896
Batch 56/64 loss: 0.03314566612243652
Batch 57/64 loss: 0.04864603281021118
Batch 58/64 loss: 0.011888086795806885
Batch 59/64 loss: 0.0384676456451416
Batch 60/64 loss: 0.015437960624694824
Batch 61/64 loss: 0.02916431427001953
Batch 62/64 loss: 0.021783113479614258
Batch 63/64 loss: 0.03945660591125488
Batch 64/64 loss: 0.02724289894104004
Epoch 39  Train loss: 0.038605141172222066  Val loss: 0.07962516042375073
Epoch 40
-------------------------------
Batch 1/64 loss: 0.06962692737579346
Batch 2/64 loss: 0.046127915382385254
Batch 3/64 loss: 0.051840364933013916
Batch 4/64 loss: 0.07209372520446777
Batch 5/64 loss: 0.052058637142181396
Batch 6/64 loss: 0.05452078580856323
Batch 7/64 loss: 0.03846728801727295
Batch 8/64 loss: 0.03013300895690918
Batch 9/64 loss: 0.03587198257446289
Batch 10/64 loss: 0.040491342544555664
Batch 11/64 loss: 0.04393184185028076
Batch 12/64 loss: 0.058625757694244385
Batch 13/64 loss: 0.02664715051651001
Batch 14/64 loss: 0.02810800075531006
Batch 15/64 loss: 0.03158754110336304
Batch 16/64 loss: 0.06230694055557251
Batch 17/64 loss: 0.048319876194000244
Batch 18/64 loss: 0.04142850637435913
Batch 19/64 loss: 0.03399956226348877
Batch 20/64 loss: 0.04809713363647461
Batch 21/64 loss: 0.031073689460754395
Batch 22/64 loss: 0.034736454486846924
Batch 23/64 loss: 0.014735639095306396
Batch 24/64 loss: 0.050133705139160156
Batch 25/64 loss: 0.05082172155380249
Batch 26/64 loss: 0.03565293550491333
Batch 27/64 loss: 0.04466855525970459
Batch 28/64 loss: 0.05231696367263794
Batch 29/64 loss: 0.04670572280883789
Batch 30/64 loss: 0.05555152893066406
Batch 31/64 loss: 0.04504835605621338
Batch 32/64 loss: 0.02655857801437378
Batch 33/64 loss: 0.05315506458282471
Batch 34/64 loss: 0.031962573528289795
Batch 35/64 loss: 0.05467188358306885
Batch 36/64 loss: 0.04462850093841553
Batch 37/64 loss: 0.03464019298553467
Batch 38/64 loss: 0.03524523973464966
Batch 39/64 loss: 0.04731631278991699
Batch 40/64 loss: 0.033089518547058105
Batch 41/64 loss: 0.06437951326370239
Batch 42/64 loss: 0.031941771507263184
Batch 43/64 loss: 0.029591500759124756
Batch 44/64 loss: 0.024590015411376953
Batch 45/64 loss: 0.036839842796325684
Batch 46/64 loss: 0.023857474327087402
Batch 47/64 loss: 0.01820540428161621
Batch 48/64 loss: 0.06332296133041382
Batch 49/64 loss: 0.0185510516166687
Batch 50/64 loss: 0.03791850805282593
Batch 51/64 loss: 0.041336774826049805
Batch 52/64 loss: 0.02335047721862793
Batch 53/64 loss: 0.03540545701980591
Batch 54/64 loss: 0.03923708200454712
Batch 55/64 loss: 0.06631803512573242
Batch 56/64 loss: 0.04874664545059204
Batch 57/64 loss: 0.019965171813964844
Batch 58/64 loss: 0.03293484449386597
Batch 59/64 loss: 0.02248281240463257
Batch 60/64 loss: 0.040377676486968994
Batch 61/64 loss: 0.03682821989059448
Batch 62/64 loss: 0.037631094455718994
Batch 63/64 loss: 0.042031049728393555
Batch 64/64 loss: 0.03289574384689331
Epoch 40  Train loss: 0.04074529596403533  Val loss: 0.055103882276725115
Saving best model, epoch: 40
Epoch 41
-------------------------------
Batch 1/64 loss: 0.043830037117004395
Batch 2/64 loss: 0.0276261568069458
Batch 3/64 loss: 0.01850605010986328
Batch 4/64 loss: 0.05319499969482422
Batch 5/64 loss: 0.04130363464355469
Batch 6/64 loss: 0.025951027870178223
Batch 7/64 loss: 0.03891623020172119
Batch 8/64 loss: 0.02960062026977539
Batch 9/64 loss: 0.05027419328689575
Batch 10/64 loss: 0.05047714710235596
Batch 11/64 loss: 0.039279282093048096
Batch 12/64 loss: 0.035414159297943115
Batch 13/64 loss: 0.016914963722229004
Batch 14/64 loss: 0.03793299198150635
Batch 15/64 loss: 0.030911386013031006
Batch 16/64 loss: 0.043550074100494385
Batch 17/64 loss: 0.03402584791183472
Batch 18/64 loss: 0.055030226707458496
Batch 19/64 loss: 0.022468745708465576
Batch 20/64 loss: 0.04229104518890381
Batch 21/64 loss: 0.03058081865310669
Batch 22/64 loss: 0.05627179145812988
Batch 23/64 loss: 0.04241943359375
Batch 24/64 loss: 0.03995877504348755
Batch 25/64 loss: 0.06314235925674438
Batch 26/64 loss: 0.03508555889129639
Batch 27/64 loss: 0.03673744201660156
Batch 28/64 loss: 0.028690338134765625
Batch 29/64 loss: 0.026551663875579834
Batch 30/64 loss: 0.06937426328659058
Batch 31/64 loss: 0.030041813850402832
Batch 32/64 loss: 0.03551781177520752
Batch 33/64 loss: 0.04514044523239136
Batch 34/64 loss: 0.018207430839538574
Batch 35/64 loss: 0.017093360424041748
Batch 36/64 loss: 0.05676567554473877
Batch 37/64 loss: 0.04780775308609009
Batch 38/64 loss: 0.03658264875411987
Batch 39/64 loss: 0.04504382610321045
Batch 40/64 loss: 0.03421664237976074
Batch 41/64 loss: 0.015331149101257324
Batch 42/64 loss: 0.025939524173736572
Batch 43/64 loss: 0.048635661602020264
Batch 44/64 loss: 0.04049324989318848
Batch 45/64 loss: 0.01985412836074829
Batch 46/64 loss: 0.038851380348205566
Batch 47/64 loss: 0.01419210433959961
Batch 48/64 loss: 0.030850350856781006
Batch 49/64 loss: 0.03469491004943848
Batch 50/64 loss: 0.016762912273406982
Batch 51/64 loss: 0.04037022590637207
Batch 52/64 loss: 0.041181206703186035
Batch 53/64 loss: 0.05628645420074463
Batch 54/64 loss: 0.03326225280761719
Batch 55/64 loss: 0.031177222728729248
Batch 56/64 loss: 0.04871022701263428
Batch 57/64 loss: 0.04509609937667847
Batch 58/64 loss: 0.04071855545043945
Batch 59/64 loss: 0.01639765501022339
Batch 60/64 loss: 0.022917747497558594
Batch 61/64 loss: 0.017169833183288574
Batch 62/64 loss: 0.018739283084869385
Batch 63/64 loss: 0.03588080406188965
Batch 64/64 loss: 0.01705688238143921
Epoch 41  Train loss: 0.03574955720527499  Val loss: 0.06302031045107498
Epoch 42
-------------------------------
Batch 1/64 loss: 0.03552055358886719
Batch 2/64 loss: 0.021499454975128174
Batch 3/64 loss: 0.04902923107147217
Batch 4/64 loss: 0.016033709049224854
Batch 5/64 loss: 0.025423169136047363
Batch 6/64 loss: 0.028998851776123047
Batch 7/64 loss: 0.022347688674926758
Batch 8/64 loss: 0.0292055606842041
Batch 9/64 loss: 0.04682135581970215
Batch 10/64 loss: 0.022668182849884033
Batch 11/64 loss: 0.036540865898132324
Batch 12/64 loss: 0.02149432897567749
Batch 13/64 loss: 0.027766942977905273
Batch 14/64 loss: 0.03603935241699219
Batch 15/64 loss: 0.0433083176612854
Batch 16/64 loss: 0.03650164604187012
Batch 17/64 loss: 0.048513591289520264
Batch 18/64 loss: 0.042222678661346436
Batch 19/64 loss: 0.02131551504135132
Batch 20/64 loss: 0.033403635025024414
Batch 21/64 loss: 0.03871297836303711
Batch 22/64 loss: 0.04044926166534424
Batch 23/64 loss: 0.040684282779693604
Batch 24/64 loss: 0.04665815830230713
Batch 25/64 loss: 0.04566359519958496
Batch 26/64 loss: 0.037384033203125
Batch 27/64 loss: 0.03811311721801758
Batch 28/64 loss: 0.036794066429138184
Batch 29/64 loss: 0.029141604900360107
Batch 30/64 loss: 0.0462801456451416
Batch 31/64 loss: 0.020825982093811035
Batch 32/64 loss: 0.02317345142364502
Batch 33/64 loss: 0.03339773416519165
Batch 34/64 loss: 0.047686636447906494
Batch 35/64 loss: 0.0366784930229187
Batch 36/64 loss: 0.04712611436843872
Batch 37/64 loss: 0.030512869358062744
Batch 38/64 loss: 0.0525968074798584
Batch 39/64 loss: 0.02467399835586548
Batch 40/64 loss: 0.02262014150619507
Batch 41/64 loss: 0.0438692569732666
Batch 42/64 loss: 0.03605639934539795
Batch 43/64 loss: 0.03765523433685303
Batch 44/64 loss: 0.0417560338973999
Batch 45/64 loss: 0.05364161729812622
Batch 46/64 loss: 0.0364377498626709
Batch 47/64 loss: 0.044252216815948486
Batch 48/64 loss: 0.03604090213775635
Batch 49/64 loss: 0.03690934181213379
Batch 50/64 loss: 0.005912482738494873
Batch 51/64 loss: 0.051909565925598145
Batch 52/64 loss: 0.037935733795166016
Batch 53/64 loss: 0.0527268648147583
Batch 54/64 loss: 0.02601933479309082
Batch 55/64 loss: 0.024649441242218018
Batch 56/64 loss: 0.026112020015716553
Batch 57/64 loss: 0.03266119956970215
Batch 58/64 loss: 0.03419262170791626
Batch 59/64 loss: 0.022868216037750244
Batch 60/64 loss: 0.04720354080200195
Batch 61/64 loss: 0.02071601152420044
Batch 62/64 loss: 0.036254823207855225
Batch 63/64 loss: 0.049757957458496094
Batch 64/64 loss: 0.03439825773239136
Epoch 42  Train loss: 0.03521828020320219  Val loss: 0.05163806246728012
Saving best model, epoch: 42
Epoch 43
-------------------------------
Batch 1/64 loss: 0.03444206714630127
Batch 2/64 loss: 0.020909905433654785
Batch 3/64 loss: 0.030270695686340332
Batch 4/64 loss: 0.012517154216766357
Batch 5/64 loss: 0.015157997608184814
Batch 6/64 loss: 0.037189781665802
Batch 7/64 loss: 0.033470869064331055
Batch 8/64 loss: 0.022960007190704346
Batch 9/64 loss: 0.04412662982940674
Batch 10/64 loss: 0.047106027603149414
Batch 11/64 loss: 0.0423927903175354
Batch 12/64 loss: 0.03369414806365967
Batch 13/64 loss: 0.04050636291503906
Batch 14/64 loss: 0.033749163150787354
Batch 15/64 loss: 0.07580667734146118
Batch 16/64 loss: 0.04218393564224243
Batch 17/64 loss: 0.004791855812072754
Batch 18/64 loss: 0.01890552043914795
Batch 19/64 loss: 0.03494596481323242
Batch 20/64 loss: 0.035181641578674316
Batch 21/64 loss: 0.030801057815551758
Batch 22/64 loss: 0.02094888687133789
Batch 23/64 loss: 0.043972134590148926
Batch 24/64 loss: 0.021065115928649902
Batch 25/64 loss: 0.026030540466308594
Batch 26/64 loss: 0.014141678810119629
Batch 27/64 loss: 0.035939812660217285
Batch 28/64 loss: 0.0254402756690979
Batch 29/64 loss: 0.04606121778488159
Batch 30/64 loss: 0.048109233379364014
Batch 31/64 loss: 0.008673548698425293
Batch 32/64 loss: 0.04182910919189453
Batch 33/64 loss: 0.047209978103637695
Batch 34/64 loss: 0.01718461513519287
Batch 35/64 loss: 0.05625259876251221
Batch 36/64 loss: 0.030091702938079834
Batch 37/64 loss: 0.042287468910217285
Batch 38/64 loss: 0.018248438835144043
Batch 39/64 loss: 0.004034280776977539
Batch 40/64 loss: 0.05450773239135742
Batch 41/64 loss: 0.02931874990463257
Batch 42/64 loss: 0.029490113258361816
Batch 43/64 loss: 0.02619856595993042
Batch 44/64 loss: 0.03125542402267456
Batch 45/64 loss: 0.029714345932006836
Batch 46/64 loss: 0.060030996799468994
Batch 47/64 loss: 0.037239789962768555
Batch 48/64 loss: 0.018321454524993896
Batch 49/64 loss: 0.04071807861328125
Batch 50/64 loss: 0.054822564125061035
Batch 51/64 loss: 0.04120272397994995
Batch 52/64 loss: 0.032564759254455566
Batch 53/64 loss: 0.037624239921569824
Batch 54/64 loss: 0.023148298263549805
Batch 55/64 loss: 0.038040339946746826
Batch 56/64 loss: 0.021762371063232422
Batch 57/64 loss: 0.020706653594970703
Batch 58/64 loss: 0.012708961963653564
Batch 59/64 loss: 0.03665518760681152
Batch 60/64 loss: 0.019003868103027344
Batch 61/64 loss: 0.07998192310333252
Batch 62/64 loss: 0.04457366466522217
Batch 63/64 loss: 0.03883159160614014
Batch 64/64 loss: 0.02401977777481079
Epoch 43  Train loss: 0.033177539180306824  Val loss: 0.06278116391696471
Epoch 44
-------------------------------
Batch 1/64 loss: 0.035791218280792236
Batch 2/64 loss: 0.03413909673690796
Batch 3/64 loss: 0.032823920249938965
Batch 4/64 loss: 0.03509557247161865
Batch 5/64 loss: 0.05686366558074951
Batch 6/64 loss: 0.038834571838378906
Batch 7/64 loss: 0.039455533027648926
Batch 8/64 loss: 0.013992488384246826
Batch 9/64 loss: 0.024774551391601562
Batch 10/64 loss: 0.024267733097076416
Batch 11/64 loss: 0.052767932415008545
Batch 12/64 loss: 0.03109920024871826
Batch 13/64 loss: 0.04488086700439453
Batch 14/64 loss: 0.03026270866394043
Batch 15/64 loss: 0.020605623722076416
Batch 16/64 loss: 0.020759165287017822
Batch 17/64 loss: 0.06410008668899536
Batch 18/64 loss: 0.01002734899520874
Batch 19/64 loss: 0.03444242477416992
Batch 20/64 loss: 0.012898027896881104
Batch 21/64 loss: 0.015707790851593018
Batch 22/64 loss: 0.026491820812225342
Batch 23/64 loss: 0.03824770450592041
Batch 24/64 loss: 0.024246513843536377
Batch 25/64 loss: 0.016727864742279053
Batch 26/64 loss: 0.02895069122314453
Batch 27/64 loss: 0.03341156244277954
Batch 28/64 loss: 0.02254343032836914
Batch 29/64 loss: 0.0416867733001709
Batch 30/64 loss: 0.033357858657836914
Batch 31/64 loss: 0.02987116575241089
Batch 32/64 loss: 0.040313541889190674
Batch 33/64 loss: 0.02490013837814331
Batch 34/64 loss: 0.036234498023986816
Batch 35/64 loss: 0.036580681800842285
Batch 36/64 loss: 0.03055429458618164
Batch 37/64 loss: 0.028436481952667236
Batch 38/64 loss: 0.028678715229034424
Batch 39/64 loss: 0.03119349479675293
Batch 40/64 loss: 0.04644852876663208
Batch 41/64 loss: 0.022638380527496338
Batch 42/64 loss: 0.03787475824356079
Batch 43/64 loss: 0.05100250244140625
Batch 44/64 loss: 0.02602165937423706
Batch 45/64 loss: 0.02699202299118042
Batch 46/64 loss: 0.0400395393371582
Batch 47/64 loss: 0.035726308822631836
Batch 48/64 loss: 0.03326290845870972
Batch 49/64 loss: 0.04649549722671509
Batch 50/64 loss: 0.03121769428253174
Batch 51/64 loss: 0.020524024963378906
Batch 52/64 loss: 0.03808403015136719
Batch 53/64 loss: 0.030862927436828613
Batch 54/64 loss: 0.02863973379135132
Batch 55/64 loss: 0.030121564865112305
Batch 56/64 loss: 0.034160494804382324
Batch 57/64 loss: 0.04510343074798584
Batch 58/64 loss: 0.02551335096359253
Batch 59/64 loss: 0.05369889736175537
Batch 60/64 loss: 0.03498774766921997
Batch 61/64 loss: 0.028140664100646973
Batch 62/64 loss: 0.036877989768981934
Batch 63/64 loss: 0.02639073133468628
Batch 64/64 loss: 0.04706341028213501
Epoch 44  Train loss: 0.03281787774142097  Val loss: 0.06529099179297379
Epoch 45
-------------------------------
Batch 1/64 loss: 0.029490113258361816
Batch 2/64 loss: 0.05722343921661377
Batch 3/64 loss: 0.03697848320007324
Batch 4/64 loss: 0.04660534858703613
Batch 5/64 loss: 0.04113191366195679
Batch 6/64 loss: 0.03361928462982178
Batch 7/64 loss: 0.044682979583740234
Batch 8/64 loss: 0.02176952362060547
Batch 9/64 loss: 0.036855459213256836
Batch 10/64 loss: 0.020123422145843506
Batch 11/64 loss: 0.04441344738006592
Batch 12/64 loss: 0.04317194223403931
Batch 13/64 loss: 0.03237825632095337
Batch 14/64 loss: 0.03664231300354004
Batch 15/64 loss: 0.037305355072021484
Batch 16/64 loss: 0.023119330406188965
Batch 17/64 loss: 0.03794056177139282
Batch 18/64 loss: 0.03738456964492798
Batch 19/64 loss: 0.04155093431472778
Batch 20/64 loss: 0.004808664321899414
Batch 21/64 loss: 0.02355247735977173
Batch 22/64 loss: 0.017322003841400146
Batch 23/64 loss: 0.01248013973236084
Batch 24/64 loss: 0.06295055150985718
Batch 25/64 loss: 0.03907465934753418
Batch 26/64 loss: 0.07415890693664551
Batch 27/64 loss: 0.013781249523162842
Batch 28/64 loss: 0.025673270225524902
Batch 29/64 loss: 0.019094467163085938
Batch 30/64 loss: 0.01860111951828003
Batch 31/64 loss: 0.03958785533905029
Batch 32/64 loss: 0.027041316032409668
Batch 33/64 loss: 0.04031217098236084
Batch 34/64 loss: 0.022223353385925293
Batch 35/64 loss: 0.013093352317810059
Batch 36/64 loss: 0.055781543254852295
Batch 37/64 loss: 0.026622653007507324
Batch 38/64 loss: 0.04296201467514038
Batch 39/64 loss: 0.020790934562683105
Batch 40/64 loss: 0.027449607849121094
Batch 41/64 loss: 0.0485607385635376
Batch 42/64 loss: 0.014295339584350586
Batch 43/64 loss: 0.012198090553283691
Batch 44/64 loss: 0.039904236793518066
Batch 45/64 loss: 0.02069878578186035
Batch 46/64 loss: 0.02601170539855957
Batch 47/64 loss: 0.017422735691070557
Batch 48/64 loss: 0.05093646049499512
Batch 49/64 loss: 0.03274428844451904
Batch 50/64 loss: 0.019484400749206543
Batch 51/64 loss: 0.03481018543243408
Batch 52/64 loss: 0.05588579177856445
Batch 53/64 loss: 0.045476555824279785
Batch 54/64 loss: 0.031449198722839355
Batch 55/64 loss: 0.02184051275253296
Batch 56/64 loss: 0.03256726264953613
Batch 57/64 loss: 0.04107201099395752
Batch 58/64 loss: 0.022440671920776367
Batch 59/64 loss: 0.03722339868545532
Batch 60/64 loss: 0.013915657997131348
Batch 61/64 loss: 0.017757415771484375
Batch 62/64 loss: 0.019628524780273438
Batch 63/64 loss: 0.04202777147293091
Batch 64/64 loss: 0.0028250813484191895
Epoch 45  Train loss: 0.031846581019607245  Val loss: 0.05935096884101527
Epoch 46
-------------------------------
Batch 1/64 loss: 0.021768569946289062
Batch 2/64 loss: 0.02507394552230835
Batch 3/64 loss: 0.05327051877975464
Batch 4/64 loss: 0.02817636728286743
Batch 5/64 loss: 0.018736660480499268
Batch 6/64 loss: 0.014357924461364746
Batch 7/64 loss: 0.028484106063842773
Batch 8/64 loss: 0.04416400194168091
Batch 9/64 loss: 0.016690552234649658
Batch 10/64 loss: 0.017424702644348145
Batch 11/64 loss: 0.04400414228439331
Batch 12/64 loss: 0.026808857917785645
Batch 13/64 loss: 0.0556333065032959
Batch 14/64 loss: 0.038246989250183105
Batch 15/64 loss: 0.03455781936645508
Batch 16/64 loss: 0.01746845245361328
Batch 17/64 loss: 0.036213815212249756
Batch 18/64 loss: 0.025160253047943115
Batch 19/64 loss: 0.030611157417297363
Batch 20/64 loss: 0.028881609439849854
Batch 21/64 loss: 0.024758100509643555
Batch 22/64 loss: 0.03314542770385742
Batch 23/64 loss: 0.011860966682434082
Batch 24/64 loss: -0.0005099773406982422
Batch 25/64 loss: 0.03578668832778931
Batch 26/64 loss: 0.044508516788482666
Batch 27/64 loss: 0.015964150428771973
Batch 28/64 loss: 0.03929591178894043
Batch 29/64 loss: 0.023224949836730957
Batch 30/64 loss: 0.01130056381225586
Batch 31/64 loss: 0.02784430980682373
Batch 32/64 loss: 0.02894890308380127
Batch 33/64 loss: 0.02631700038909912
Batch 34/64 loss: 0.014852344989776611
Batch 35/64 loss: 0.04126381874084473
Batch 36/64 loss: 0.03240633010864258
Batch 37/64 loss: -0.002260446548461914
Batch 38/64 loss: 0.01221919059753418
Batch 39/64 loss: 0.01612257957458496
Batch 40/64 loss: 0.01902186870574951
Batch 41/64 loss: 0.03189891576766968
Batch 42/64 loss: 0.07039022445678711
Batch 43/64 loss: 0.0008328557014465332
Batch 44/64 loss: 0.04353064298629761
Batch 45/64 loss: 0.02266472578048706
Batch 46/64 loss: 0.01261293888092041
Batch 47/64 loss: 0.03465569019317627
Batch 48/64 loss: 0.04194360971450806
Batch 49/64 loss: 0.03207516670227051
Batch 50/64 loss: 0.023239195346832275
Batch 51/64 loss: 0.025151729583740234
Batch 52/64 loss: 0.03204333782196045
Batch 53/64 loss: 0.0323481559753418
Batch 54/64 loss: 0.025304853916168213
Batch 55/64 loss: 0.02526336908340454
Batch 56/64 loss: 0.02211827039718628
Batch 57/64 loss: 0.02396625280380249
Batch 58/64 loss: 0.02123051881790161
Batch 59/64 loss: 0.04451107978820801
Batch 60/64 loss: 0.018033146858215332
Batch 61/64 loss: 0.017666339874267578
Batch 62/64 loss: 0.021247506141662598
Batch 63/64 loss: 0.012963414192199707
Batch 64/64 loss: 0.034598588943481445
Epoch 46  Train loss: 0.027034444434970033  Val loss: 0.06005695923087523
Epoch 47
-------------------------------
Batch 1/64 loss: 0.024579763412475586
Batch 2/64 loss: 0.02451193332672119
Batch 3/64 loss: 0.039228200912475586
Batch 4/64 loss: 0.018419325351715088
Batch 5/64 loss: 0.03408849239349365
Batch 6/64 loss: 0.055254995822906494
Batch 7/64 loss: 0.03200852870941162
Batch 8/64 loss: 0.03297907114028931
Batch 9/64 loss: 0.031740665435791016
Batch 10/64 loss: 0.015051782131195068
Batch 11/64 loss: 0.01530003547668457
Batch 12/64 loss: 0.024684429168701172
Batch 13/64 loss: 0.031576454639434814
Batch 14/64 loss: 0.017243266105651855
Batch 15/64 loss: 0.028083384037017822
Batch 16/64 loss: 0.024832844734191895
Batch 17/64 loss: 0.02617424726486206
Batch 18/64 loss: 0.0059770941734313965
Batch 19/64 loss: 0.01281440258026123
Batch 20/64 loss: 0.0003299713134765625
Batch 21/64 loss: 0.04156559705734253
Batch 22/64 loss: 0.016026854515075684
Batch 23/64 loss: 0.009561359882354736
Batch 24/64 loss: 0.031060874462127686
Batch 25/64 loss: 0.020782291889190674
Batch 26/64 loss: 0.037013351917266846
Batch 27/64 loss: 0.01915842294692993
Batch 28/64 loss: 0.02143990993499756
Batch 29/64 loss: 0.03866857290267944
Batch 30/64 loss: 0.037285566329956055
Batch 31/64 loss: 0.0317608118057251
Batch 32/64 loss: 0.02266871929168701
Batch 33/64 loss: 0.029884934425354004
Batch 34/64 loss: 0.009825825691223145
Batch 35/64 loss: 0.015837550163269043
Batch 36/64 loss: 0.030910134315490723
Batch 37/64 loss: 0.029774367809295654
Batch 38/64 loss: 0.012236833572387695
Batch 39/64 loss: 0.013821303844451904
Batch 40/64 loss: 0.027572929859161377
Batch 41/64 loss: 0.030419111251831055
Batch 42/64 loss: 0.027488648891448975
Batch 43/64 loss: 0.012667834758758545
Batch 44/64 loss: 0.031781017780303955
Batch 45/64 loss: 0.006567120552062988
Batch 46/64 loss: 0.028797268867492676
Batch 47/64 loss: 0.0456807017326355
Batch 48/64 loss: 0.018737614154815674
Batch 49/64 loss: 0.03641480207443237
Batch 50/64 loss: 0.027830839157104492
Batch 51/64 loss: 0.03628116846084595
Batch 52/64 loss: 0.012211084365844727
Batch 53/64 loss: 0.026945829391479492
Batch 54/64 loss: 0.025976240634918213
Batch 55/64 loss: 0.047672390937805176
Batch 56/64 loss: 0.01628267765045166
Batch 57/64 loss: 0.05915409326553345
Batch 58/64 loss: 0.011223793029785156
Batch 59/64 loss: 0.033505260944366455
Batch 60/64 loss: 0.007418394088745117
Batch 61/64 loss: 0.04976290464401245
Batch 62/64 loss: 0.019883930683135986
Batch 63/64 loss: 0.03254634141921997
Batch 64/64 loss: 0.03155696392059326
Epoch 47  Train loss: 0.026049441449782426  Val loss: 0.057412204873520896
Epoch 48
-------------------------------
Batch 1/64 loss: 0.017333567142486572
Batch 2/64 loss: 0.029769837856292725
Batch 3/64 loss: 0.01563817262649536
Batch 4/64 loss: 0.051258623600006104
Batch 5/64 loss: 0.016345560550689697
Batch 6/64 loss: 0.03310674428939819
Batch 7/64 loss: 0.020154714584350586
Batch 8/64 loss: 0.02553713321685791
Batch 9/64 loss: 0.016224384307861328
Batch 10/64 loss: 0.03128659725189209
Batch 11/64 loss: 0.038117289543151855
Batch 12/64 loss: 0.03523862361907959
Batch 13/64 loss: 0.03328287601470947
Batch 14/64 loss: 0.0467187762260437
Batch 15/64 loss: 0.03453195095062256
Batch 16/64 loss: 0.03159952163696289
Batch 17/64 loss: 0.04714202880859375
Batch 18/64 loss: 0.035987913608551025
Batch 19/64 loss: 0.052498817443847656
Batch 20/64 loss: 0.014101028442382812
Batch 21/64 loss: 0.03619182109832764
Batch 22/64 loss: 0.039198994636535645
Batch 23/64 loss: 0.053496479988098145
Batch 24/64 loss: 0.04561722278594971
Batch 25/64 loss: 0.009459435939788818
Batch 26/64 loss: 0.00726008415222168
Batch 27/64 loss: 0.020099163055419922
Batch 28/64 loss: 0.015167295932769775
Batch 29/64 loss: 0.011814594268798828
Batch 30/64 loss: 0.019385814666748047
Batch 31/64 loss: 0.040491461753845215
Batch 32/64 loss: 0.021553456783294678
Batch 33/64 loss: 0.011417150497436523
Batch 34/64 loss: 0.024208664894104004
Batch 35/64 loss: 0.009739279747009277
Batch 36/64 loss: 0.022576570510864258
Batch 37/64 loss: 0.021747708320617676
Batch 38/64 loss: 0.031497180461883545
Batch 39/64 loss: 0.027001380920410156
Batch 40/64 loss: 0.031471073627471924
Batch 41/64 loss: 0.03910309076309204
Batch 42/64 loss: 0.01808309555053711
Batch 43/64 loss: 0.014993429183959961
Batch 44/64 loss: 0.01124727725982666
Batch 45/64 loss: 0.031110167503356934
Batch 46/64 loss: 0.02107471227645874
Batch 47/64 loss: 0.03608131408691406
Batch 48/64 loss: 0.0166398286819458
Batch 49/64 loss: 0.03530311584472656
Batch 50/64 loss: 0.0069730281829833984
Batch 51/64 loss: 0.03180551528930664
Batch 52/64 loss: 0.017084360122680664
Batch 53/64 loss: 0.024885177612304688
Batch 54/64 loss: 0.03375959396362305
Batch 55/64 loss: 0.023471057415008545
Batch 56/64 loss: 0.03554898500442505
Batch 57/64 loss: 0.004220724105834961
Batch 58/64 loss: 0.028659582138061523
Batch 59/64 loss: 0.021464884281158447
Batch 60/64 loss: 0.021669983863830566
Batch 61/64 loss: 0.007784128189086914
Batch 62/64 loss: 0.02134251594543457
Batch 63/64 loss: 0.02566671371459961
Batch 64/64 loss: 0.031938374042510986
Epoch 48  Train loss: 0.026308942074869193  Val loss: 0.052942769019464445
Epoch 49
-------------------------------
Batch 1/64 loss: 0.0282934308052063
Batch 2/64 loss: 0.011236846446990967
Batch 3/64 loss: 0.01883453130722046
Batch 4/64 loss: 0.009522020816802979
Batch 5/64 loss: 0.04443484544754028
Batch 6/64 loss: 0.028282582759857178
Batch 7/64 loss: 0.005798399448394775
Batch 8/64 loss: 0.019637107849121094
Batch 9/64 loss: 0.017936229705810547
Batch 10/64 loss: 0.02973949909210205
Batch 11/64 loss: 0.05138707160949707
Batch 12/64 loss: 0.04006695747375488
Batch 13/64 loss: -0.002659142017364502
Batch 14/64 loss: 0.020448505878448486
Batch 15/64 loss: 0.027481555938720703
Batch 16/64 loss: 0.0515516996383667
Batch 17/64 loss: 0.03010958433151245
Batch 18/64 loss: 0.02883601188659668
Batch 19/64 loss: 0.03067547082901001
Batch 20/64 loss: 0.025112390518188477
Batch 21/64 loss: 0.017029941082000732
Batch 22/64 loss: 0.016380667686462402
Batch 23/64 loss: 0.03015810251235962
Batch 24/64 loss: 0.04813730716705322
Batch 25/64 loss: 0.02806079387664795
Batch 26/64 loss: 0.01573801040649414
Batch 27/64 loss: 0.03806471824645996
Batch 28/64 loss: 0.0002714395523071289
Batch 29/64 loss: 0.009611427783966064
Batch 30/64 loss: 0.008505403995513916
Batch 31/64 loss: 0.02374368906021118
Batch 32/64 loss: 0.028300046920776367
Batch 33/64 loss: 0.0265578031539917
Batch 34/64 loss: 0.023129940032958984
Batch 35/64 loss: 0.018117666244506836
Batch 36/64 loss: 0.013730466365814209
Batch 37/64 loss: 0.018936872482299805
Batch 38/64 loss: 0.042692720890045166
Batch 39/64 loss: 0.01240837574005127
Batch 40/64 loss: 0.016784369945526123
Batch 41/64 loss: 0.017252206802368164
Batch 42/64 loss: 0.030155301094055176
Batch 43/64 loss: 0.022342383861541748
Batch 44/64 loss: 0.0414159893989563
Batch 45/64 loss: 0.022153139114379883
Batch 46/64 loss: 0.008151710033416748
Batch 47/64 loss: 0.03218120336532593
Batch 48/64 loss: 0.0391162633895874
Batch 49/64 loss: 0.02196139097213745
Batch 50/64 loss: 0.041458964347839355
Batch 51/64 loss: 0.031060338020324707
Batch 52/64 loss: 0.03436684608459473
Batch 53/64 loss: 0.034870922565460205
Batch 54/64 loss: 0.03310883045196533
Batch 55/64 loss: 0.03619283437728882
Batch 56/64 loss: 0.016921520233154297
Batch 57/64 loss: 0.02132660150527954
Batch 58/64 loss: 0.01398402452468872
Batch 59/64 loss: 0.05043822526931763
Batch 60/64 loss: 0.0033583641052246094
Batch 61/64 loss: 0.009238004684448242
Batch 62/64 loss: 0.028055012226104736
Batch 63/64 loss: 0.024991154670715332
Batch 64/64 loss: 0.015669286251068115
Epoch 49  Train loss: 0.024767663665846283  Val loss: 0.0464243325580846
Saving best model, epoch: 49
Epoch 50
-------------------------------
Batch 1/64 loss: 0.04785418510437012
Batch 2/64 loss: 0.02370131015777588
Batch 3/64 loss: 0.002096831798553467
Batch 4/64 loss: 0.07435721158981323
Batch 5/64 loss: 0.0025461316108703613
Batch 6/64 loss: 0.051786601543426514
Batch 7/64 loss: 0.037937283515930176
Batch 8/64 loss: 0.021121859550476074
Batch 9/64 loss: 0.019562721252441406
Batch 10/64 loss: 0.00650864839553833
Batch 11/64 loss: 0.016112089157104492
Batch 12/64 loss: 0.02023214101791382
Batch 13/64 loss: -0.005001246929168701
Batch 14/64 loss: 0.002277374267578125
Batch 15/64 loss: 0.01833033561706543
Batch 16/64 loss: -0.0003638267517089844
Batch 17/64 loss: 0.0236930251121521
Batch 18/64 loss: 0.022293150424957275
Batch 19/64 loss: 0.02030736207962036
Batch 20/64 loss: 0.04062086343765259
Batch 21/64 loss: 0.026135742664337158
Batch 22/64 loss: 0.04468047618865967
Batch 23/64 loss: 0.007990658283233643
Batch 24/64 loss: 0.006470203399658203
Batch 25/64 loss: 0.008016884326934814
Batch 26/64 loss: 0.030652940273284912
Batch 27/64 loss: 0.023979365825653076
Batch 28/64 loss: 0.04032719135284424
Batch 29/64 loss: 0.010043144226074219
Batch 30/64 loss: 0.005172610282897949
Batch 31/64 loss: 0.03671276569366455
Batch 32/64 loss: 0.02832728624343872
Batch 33/64 loss: 0.0019077062606811523
Batch 34/64 loss: 0.00036466121673583984
Batch 35/64 loss: 0.021138429641723633
Batch 36/64 loss: -0.0036698579788208008
Batch 37/64 loss: 0.025588393211364746
Batch 38/64 loss: 0.010706841945648193
Batch 39/64 loss: 0.019711852073669434
Batch 40/64 loss: 0.021701455116271973
Batch 41/64 loss: 0.010515749454498291
Batch 42/64 loss: 0.011996865272521973
Batch 43/64 loss: 0.047669053077697754
Batch 44/64 loss: 0.005666136741638184
Batch 45/64 loss: 0.039159417152404785
Batch 46/64 loss: 0.02562791109085083
Batch 47/64 loss: 0.020940184593200684
Batch 48/64 loss: 0.034421682357788086
Batch 49/64 loss: 0.03687530755996704
Batch 50/64 loss: 0.027435302734375
Batch 51/64 loss: 0.019024789333343506
Batch 52/64 loss: 0.035588741302490234
Batch 53/64 loss: 0.017558932304382324
Batch 54/64 loss: 0.05247145891189575
Batch 55/64 loss: 0.018085360527038574
Batch 56/64 loss: 0.019038796424865723
Batch 57/64 loss: 0.013638734817504883
Batch 58/64 loss: 0.02003800868988037
Batch 59/64 loss: 0.032529592514038086
Batch 60/64 loss: 0.03806251287460327
Batch 61/64 loss: 0.021525979042053223
Batch 62/64 loss: 0.019897937774658203
Batch 63/64 loss: 0.031732141971588135
Batch 64/64 loss: 0.01006382703781128
Epoch 50  Train loss: 0.02225852970983468  Val loss: 0.047147069395202953
Epoch 51
-------------------------------
Batch 1/64 loss: 0.009411811828613281
Batch 2/64 loss: 0.0300714373588562
Batch 3/64 loss: 0.03273653984069824
Batch 4/64 loss: 0.027008652687072754
Batch 5/64 loss: 0.017124831676483154
Batch 6/64 loss: 0.0436481237411499
Batch 7/64 loss: 0.022597849369049072
Batch 8/64 loss: 0.03748363256454468
Batch 9/64 loss: 0.013869404792785645
Batch 10/64 loss: 0.012764155864715576
Batch 11/64 loss: 0.018565773963928223
Batch 12/64 loss: 0.03524982929229736
Batch 13/64 loss: 0.020486116409301758
Batch 14/64 loss: 0.05248737335205078
Batch 15/64 loss: 0.015586435794830322
Batch 16/64 loss: 0.01618218421936035
Batch 17/64 loss: 0.012490987777709961
Batch 18/64 loss: 0.005955874919891357
Batch 19/64 loss: 0.04936772584915161
Batch 20/64 loss: 0.025072097778320312
Batch 21/64 loss: 0.0348050594329834
Batch 22/64 loss: 0.04228454828262329
Batch 23/64 loss: 0.03023308515548706
Batch 24/64 loss: 0.032515645027160645
Batch 25/64 loss: 0.029775798320770264
Batch 26/64 loss: 0.008170127868652344
Batch 27/64 loss: -0.004713654518127441
Batch 28/64 loss: 0.016633152961730957
Batch 29/64 loss: 0.025175869464874268
Batch 30/64 loss: 0.04842376708984375
Batch 31/64 loss: 0.035375237464904785
Batch 32/64 loss: 0.04211658239364624
Batch 33/64 loss: 0.034771859645843506
Batch 34/64 loss: 0.014447629451751709
Batch 35/64 loss: 0.020045995712280273
Batch 36/64 loss: 0.036367595195770264
Batch 37/64 loss: 0.01841568946838379
Batch 38/64 loss: -0.00864940881729126
Batch 39/64 loss: 0.01623678207397461
Batch 40/64 loss: 0.019251704216003418
Batch 41/64 loss: 0.04814732074737549
Batch 42/64 loss: 0.03933018445968628
Batch 43/64 loss: 0.00896143913269043
Batch 44/64 loss: 0.03632932901382446
Batch 45/64 loss: 0.023166120052337646
Batch 46/64 loss: 0.015369236469268799
Batch 47/64 loss: 0.024991512298583984
Batch 48/64 loss: 0.00856029987335205
Batch 49/64 loss: 0.03825479745864868
Batch 50/64 loss: 0.012802600860595703
Batch 51/64 loss: 0.021315455436706543
Batch 52/64 loss: 0.010670781135559082
Batch 53/64 loss: 0.015996932983398438
Batch 54/64 loss: 0.041167259216308594
Batch 55/64 loss: 0.010042011737823486
Batch 56/64 loss: 0.018053174018859863
Batch 57/64 loss: 0.041009485721588135
Batch 58/64 loss: 0.016574561595916748
Batch 59/64 loss: 0.0242081880569458
Batch 60/64 loss: 0.00362396240234375
Batch 61/64 loss: -0.0001881122589111328
Batch 62/64 loss: 0.022305548191070557
Batch 63/64 loss: 0.01787632703781128
Batch 64/64 loss: 0.019529223442077637
Epoch 51  Train loss: 0.02357739981483011  Val loss: 0.054265888286210415
Epoch 52
-------------------------------
Batch 1/64 loss: 0.03812086582183838
Batch 2/64 loss: 0.0016186833381652832
Batch 3/64 loss: 0.01860809326171875
Batch 4/64 loss: 0.01764655113220215
Batch 5/64 loss: 0.030566096305847168
Batch 6/64 loss: 0.00901108980178833
Batch 7/64 loss: 0.015476644039154053
Batch 8/64 loss: 0.006536960601806641
Batch 9/64 loss: 0.02372574806213379
Batch 10/64 loss: -0.0017027854919433594
Batch 11/64 loss: 0.04256618022918701
Batch 12/64 loss: 0.0036624670028686523
Batch 13/64 loss: 0.016015708446502686
Batch 14/64 loss: 0.00825732946395874
Batch 15/64 loss: 0.032842278480529785
Batch 16/64 loss: 0.0025107860565185547
Batch 17/64 loss: 0.03312480449676514
Batch 18/64 loss: 0.04000246524810791
Batch 19/64 loss: 0.0081215500831604
Batch 20/64 loss: 0.018324077129364014
Batch 21/64 loss: 0.038567185401916504
Batch 22/64 loss: 0.024652302265167236
Batch 23/64 loss: 0.037335872650146484
Batch 24/64 loss: 0.027313530445098877
Batch 25/64 loss: 0.018912911415100098
Batch 26/64 loss: 0.05778062343597412
Batch 27/64 loss: 0.0251157283782959
Batch 28/64 loss: 0.03326570987701416
Batch 29/64 loss: 0.023462772369384766
Batch 30/64 loss: 0.020457684993743896
Batch 31/64 loss: 0.041240394115448
Batch 32/64 loss: 0.009491920471191406
Batch 33/64 loss: 0.02151346206665039
Batch 34/64 loss: 0.01994866132736206
Batch 35/64 loss: 0.02750992774963379
Batch 36/64 loss: 0.022475004196166992
Batch 37/64 loss: 0.024587035179138184
Batch 38/64 loss: 0.016046106815338135
Batch 39/64 loss: 0.012582719326019287
Batch 40/64 loss: 0.008128941059112549
Batch 41/64 loss: 0.0033962130546569824
Batch 42/64 loss: 0.02921229600906372
Batch 43/64 loss: 0.013451337814331055
Batch 44/64 loss: 0.0177687406539917
Batch 45/64 loss: 0.030561983585357666
Batch 46/64 loss: 0.025144219398498535
Batch 47/64 loss: 0.023026704788208008
Batch 48/64 loss: 0.004136979579925537
Batch 49/64 loss: 0.031013548374176025
Batch 50/64 loss: 0.06357985734939575
Batch 51/64 loss: 0.01774144172668457
Batch 52/64 loss: 0.04913884401321411
Batch 53/64 loss: 0.02100503444671631
Batch 54/64 loss: 0.02850586175918579
Batch 55/64 loss: 0.010925889015197754
Batch 56/64 loss: 0.015229940414428711
Batch 57/64 loss: 0.03360939025878906
Batch 58/64 loss: 0.02895665168762207
Batch 59/64 loss: 0.023234546184539795
Batch 60/64 loss: 0.014284789562225342
Batch 61/64 loss: 0.032796621322631836
Batch 62/64 loss: 0.01940286159515381
Batch 63/64 loss: 0.0239102840423584
Batch 64/64 loss: 0.0010674595832824707
Epoch 52  Train loss: 0.022529548523472803  Val loss: 0.05624461583665146
Epoch 53
-------------------------------
Batch 1/64 loss: 0.03250753879547119
Batch 2/64 loss: 0.0049724578857421875
Batch 3/64 loss: 0.02663511037826538
Batch 4/64 loss: 0.020375490188598633
Batch 5/64 loss: 0.01739346981048584
Batch 6/64 loss: 0.012920975685119629
Batch 7/64 loss: 0.026554584503173828
Batch 8/64 loss: -0.0014758110046386719
Batch 9/64 loss: 0.04051405191421509
Batch 10/64 loss: 0.011808037757873535
Batch 11/64 loss: 0.019599080085754395
Batch 12/64 loss: 0.03905808925628662
Batch 13/64 loss: 0.020441174507141113
Batch 14/64 loss: 0.0016997456550598145
Batch 15/64 loss: 0.011061370372772217
Batch 16/64 loss: 0.017452597618103027
Batch 17/64 loss: 0.026758015155792236
Batch 18/64 loss: 0.0013110637664794922
Batch 19/64 loss: 0.04662024974822998
Batch 20/64 loss: 0.014954566955566406
Batch 21/64 loss: 0.028641879558563232
Batch 22/64 loss: 0.010178208351135254
Batch 23/64 loss: 0.028876960277557373
Batch 24/64 loss: 0.0465853214263916
Batch 25/64 loss: 0.013918280601501465
Batch 26/64 loss: -0.014113068580627441
Batch 27/64 loss: 0.024451136589050293
Batch 28/64 loss: 0.016197264194488525
Batch 29/64 loss: 0.00854557752609253
Batch 30/64 loss: 0.027771174907684326
Batch 31/64 loss: 0.01965874433517456
Batch 32/64 loss: 0.00850224494934082
Batch 33/64 loss: 0.009101152420043945
Batch 34/64 loss: 0.004351258277893066
Batch 35/64 loss: 0.011725425720214844
Batch 36/64 loss: 0.0053424835205078125
Batch 37/64 loss: 0.018645286560058594
Batch 38/64 loss: 0.03652191162109375
Batch 39/64 loss: 0.0324099063873291
Batch 40/64 loss: 0.007821142673492432
Batch 41/64 loss: 0.019937217235565186
Batch 42/64 loss: 0.03263533115386963
Batch 43/64 loss: 0.013839960098266602
Batch 44/64 loss: 0.01089167594909668
Batch 45/64 loss: 0.0013015270233154297
Batch 46/64 loss: 0.02933204174041748
Batch 47/64 loss: 0.01704573631286621
Batch 48/64 loss: 0.022950053215026855
Batch 49/64 loss: 0.03441351652145386
Batch 50/64 loss: -0.005555450916290283
Batch 51/64 loss: 0.014522910118103027
Batch 52/64 loss: 0.03786063194274902
Batch 53/64 loss: 0.0042081475257873535
Batch 54/64 loss: 0.021010160446166992
Batch 55/64 loss: 0.03311610221862793
Batch 56/64 loss: 0.014720797538757324
Batch 57/64 loss: 0.00403141975402832
Batch 58/64 loss: 0.057544708251953125
Batch 59/64 loss: 0.017964184284210205
Batch 60/64 loss: 0.012189269065856934
Batch 61/64 loss: 0.045087337493896484
Batch 62/64 loss: 0.023109853267669678
Batch 63/64 loss: 0.027715742588043213
Batch 64/64 loss: 0.026015520095825195
Epoch 53  Train loss: 0.019540057462804458  Val loss: 0.05311323646007944
Epoch 54
-------------------------------
Batch 1/64 loss: 0.015370607376098633
Batch 2/64 loss: 0.009193718433380127
Batch 3/64 loss: 0.04592496156692505
Batch 4/64 loss: 0.018678128719329834
Batch 5/64 loss: 0.020163416862487793
Batch 6/64 loss: 0.045192718505859375
Batch 7/64 loss: 0.03800708055496216
Batch 8/64 loss: 0.013951659202575684
Batch 9/64 loss: 0.014200925827026367
Batch 10/64 loss: 0.006942331790924072
Batch 11/64 loss: 0.01872563362121582
Batch 12/64 loss: -0.010687828063964844
Batch 13/64 loss: 0.006775856018066406
Batch 14/64 loss: 0.03347963094711304
Batch 15/64 loss: 0.026328325271606445
Batch 16/64 loss: 0.007731735706329346
Batch 17/64 loss: 0.03881758451461792
Batch 18/64 loss: 0.025289595127105713
Batch 19/64 loss: 0.044819653034210205
Batch 20/64 loss: 0.010476946830749512
Batch 21/64 loss: 0.017156481742858887
Batch 22/64 loss: 0.011194407939910889
Batch 23/64 loss: 0.008925080299377441
Batch 24/64 loss: -0.0046749114990234375
Batch 25/64 loss: 0.0187070369720459
Batch 26/64 loss: -0.00265425443649292
Batch 27/64 loss: 0.008849620819091797
Batch 28/64 loss: 0.01766383647918701
Batch 29/64 loss: 0.023086249828338623
Batch 30/64 loss: 0.00855952501296997
Batch 31/64 loss: 0.006662964820861816
Batch 32/64 loss: 0.021298527717590332
Batch 33/64 loss: 0.04308974742889404
Batch 34/64 loss: 0.0064865946769714355
Batch 35/64 loss: 0.030691921710968018
Batch 36/64 loss: 0.016284048557281494
Batch 37/64 loss: 0.009500384330749512
Batch 38/64 loss: 0.02929234504699707
Batch 39/64 loss: 0.009132087230682373
Batch 40/64 loss: 0.019928038120269775
Batch 41/64 loss: 0.014054298400878906
Batch 42/64 loss: 0.012195050716400146
Batch 43/64 loss: -3.314018249511719e-05
Batch 44/64 loss: 0.015335679054260254
Batch 45/64 loss: 0.034687578678131104
Batch 46/64 loss: 0.034205079078674316
Batch 47/64 loss: 0.028976857662200928
Batch 48/64 loss: 0.03248798847198486
Batch 49/64 loss: 0.0639311671257019
Batch 50/64 loss: 0.024472534656524658
Batch 51/64 loss: 0.017692148685455322
Batch 52/64 loss: 0.009283959865570068
Batch 53/64 loss: 0.03223872184753418
Batch 54/64 loss: 0.01595216989517212
Batch 55/64 loss: 0.03403472900390625
Batch 56/64 loss: 0.01837313175201416
Batch 57/64 loss: 0.023113131523132324
Batch 58/64 loss: 0.027523040771484375
Batch 59/64 loss: 0.03287011384963989
Batch 60/64 loss: 0.029873430728912354
Batch 61/64 loss: 0.01583915948867798
Batch 62/64 loss: 0.006507396697998047
Batch 63/64 loss: 0.019954800605773926
Batch 64/64 loss: 0.004519224166870117
Epoch 54  Train loss: 0.020008170370962107  Val loss: 0.046644406015520654
Epoch 55
-------------------------------
Batch 1/64 loss: 0.023616552352905273
Batch 2/64 loss: 0.008027970790863037
Batch 3/64 loss: 0.020357131958007812
Batch 4/64 loss: 0.020743191242218018
Batch 5/64 loss: 0.01839625835418701
Batch 6/64 loss: 0.06184697151184082
Batch 7/64 loss: 0.012894749641418457
Batch 8/64 loss: 0.01746368408203125
Batch 9/64 loss: 0.001547694206237793
Batch 10/64 loss: 0.0009096264839172363
Batch 11/64 loss: 0.020984649658203125
Batch 12/64 loss: 0.04725879430770874
Batch 13/64 loss: 0.05162268877029419
Batch 14/64 loss: 0.016746938228607178
Batch 15/64 loss: 0.022140800952911377
Batch 16/64 loss: 0.012163639068603516
Batch 17/64 loss: 0.0028544068336486816
Batch 18/64 loss: 0.009439527988433838
Batch 19/64 loss: 0.013643443584442139
Batch 20/64 loss: 0.022733449935913086
Batch 21/64 loss: 0.017588019371032715
Batch 22/64 loss: 0.002925395965576172
Batch 23/64 loss: 0.0018953680992126465
Batch 24/64 loss: 0.033540964126586914
Batch 25/64 loss: 0.01301264762878418
Batch 26/64 loss: 0.013024508953094482
Batch 27/64 loss: 0.012007951736450195
Batch 28/64 loss: 0.04463261365890503
Batch 29/64 loss: 0.0020052194595336914
Batch 30/64 loss: 0.022371113300323486
Batch 31/64 loss: 0.014381885528564453
Batch 32/64 loss: 0.036205291748046875
Batch 33/64 loss: 0.003962278366088867
Batch 34/64 loss: -0.001325845718383789
Batch 35/64 loss: 0.011702299118041992
Batch 36/64 loss: -0.006103217601776123
Batch 37/64 loss: 0.0024864673614501953
Batch 38/64 loss: 0.006416201591491699
Batch 39/64 loss: 0.017526865005493164
Batch 40/64 loss: 0.0006258487701416016
Batch 41/64 loss: 0.019284427165985107
Batch 42/64 loss: 0.014866173267364502
Batch 43/64 loss: 0.00716930627822876
Batch 44/64 loss: 0.01373434066772461
Batch 45/64 loss: 0.005120694637298584
Batch 46/64 loss: 0.01817864179611206
Batch 47/64 loss: 0.03316754102706909
Batch 48/64 loss: 0.0033313632011413574
Batch 49/64 loss: 0.00965958833694458
Batch 50/64 loss: 0.029732465744018555
Batch 51/64 loss: 0.00849771499633789
Batch 52/64 loss: -0.02240276336669922
Batch 53/64 loss: 0.005668044090270996
Batch 54/64 loss: 0.005108475685119629
Batch 55/64 loss: 0.015542089939117432
Batch 56/64 loss: 0.030942082405090332
Batch 57/64 loss: 0.013253092765808105
Batch 58/64 loss: 0.02758711576461792
Batch 59/64 loss: 0.008061349391937256
Batch 60/64 loss: 0.008223772048950195
Batch 61/64 loss: 0.011168897151947021
Batch 62/64 loss: 0.01182103157043457
Batch 63/64 loss: 0.0377083420753479
Batch 64/64 loss: 0.04230380058288574
Epoch 55  Train loss: 0.01570863817252365  Val loss: 0.044287042314653954
Saving best model, epoch: 55
Epoch 56
-------------------------------
Batch 1/64 loss: 0.01783931255340576
Batch 2/64 loss: 0.01618194580078125
Batch 3/64 loss: 0.019020020961761475
Batch 4/64 loss: 0.02851492166519165
Batch 5/64 loss: 0.00920867919921875
Batch 6/64 loss: 0.01177990436553955
Batch 7/64 loss: 0.008371114730834961
Batch 8/64 loss: 0.017405569553375244
Batch 9/64 loss: 0.02514469623565674
Batch 10/64 loss: 0.020709335803985596
Batch 11/64 loss: 0.00664830207824707
Batch 12/64 loss: 0.024293959140777588
Batch 13/64 loss: -0.006722569465637207
Batch 14/64 loss: -0.003014087677001953
Batch 15/64 loss: 0.03528010845184326
Batch 16/64 loss: 0.0049582719802856445
Batch 17/64 loss: 0.008580684661865234
Batch 18/64 loss: -0.013502001762390137
Batch 19/64 loss: 0.02493804693222046
Batch 20/64 loss: 0.0023134946823120117
Batch 21/64 loss: 0.017020463943481445
Batch 22/64 loss: 0.010650277137756348
Batch 23/64 loss: 0.013445913791656494
Batch 24/64 loss: 0.02028268575668335
Batch 25/64 loss: 0.0019136667251586914
Batch 26/64 loss: 0.014788031578063965
Batch 27/64 loss: 0.007031440734863281
Batch 28/64 loss: -0.019549846649169922
Batch 29/64 loss: 0.02167356014251709
Batch 30/64 loss: 0.024428725242614746
Batch 31/64 loss: 0.013036549091339111
Batch 32/64 loss: 0.03468555212020874
Batch 33/64 loss: 0.020122647285461426
Batch 34/64 loss: 0.004710197448730469
Batch 35/64 loss: 0.004250168800354004
Batch 36/64 loss: 0.039283812046051025
Batch 37/64 loss: 0.008127093315124512
Batch 38/64 loss: 0.027620315551757812
Batch 39/64 loss: 0.009276390075683594
Batch 40/64 loss: 0.03947097063064575
Batch 41/64 loss: 0.02954566478729248
Batch 42/64 loss: 0.028554558753967285
Batch 43/64 loss: 0.02119976282119751
Batch 44/64 loss: 0.03725379705429077
Batch 45/64 loss: -0.000676572322845459
Batch 46/64 loss: 0.014417052268981934
Batch 47/64 loss: 0.02552175521850586
Batch 48/64 loss: -0.0033395886421203613
Batch 49/64 loss: 0.024614453315734863
Batch 50/64 loss: 0.013463497161865234
Batch 51/64 loss: 0.011825025081634521
Batch 52/64 loss: 0.020944058895111084
Batch 53/64 loss: -0.006476163864135742
Batch 54/64 loss: 0.013648629188537598
Batch 55/64 loss: 0.023761630058288574
Batch 56/64 loss: -0.005115807056427002
Batch 57/64 loss: 0.02976393699645996
Batch 58/64 loss: 0.016031384468078613
Batch 59/64 loss: -0.0011462569236755371
Batch 60/64 loss: 0.028373122215270996
Batch 61/64 loss: 0.01367199420928955
Batch 62/64 loss: 0.02324211597442627
Batch 63/64 loss: 0.007317900657653809
Batch 64/64 loss: 0.011255979537963867
Epoch 56  Train loss: 0.014824411915797813  Val loss: 0.04401983182454847
Saving best model, epoch: 56
Epoch 57
-------------------------------
Batch 1/64 loss: 0.023712992668151855
Batch 2/64 loss: 0.01881539821624756
Batch 3/64 loss: 0.0013039112091064453
Batch 4/64 loss: 0.01762688159942627
Batch 5/64 loss: -0.0013802051544189453
Batch 6/64 loss: 0.02865654230117798
Batch 7/64 loss: -0.02093660831451416
Batch 8/64 loss: 0.06281346082687378
Batch 9/64 loss: 0.0007104277610778809
Batch 10/64 loss: 0.02299445867538452
Batch 11/64 loss: 0.013817131519317627
Batch 12/64 loss: 0.01794600486755371
Batch 13/64 loss: 0.023434817790985107
Batch 14/64 loss: 0.006602644920349121
Batch 15/64 loss: 0.007002353668212891
Batch 16/64 loss: 0.001603841781616211
Batch 17/64 loss: 0.049004971981048584
Batch 18/64 loss: 0.0179024338722229
Batch 19/64 loss: 0.007052302360534668
Batch 20/64 loss: 0.03681093454360962
Batch 21/64 loss: 0.003434479236602783
Batch 22/64 loss: 0.028712749481201172
Batch 23/64 loss: 0.014842867851257324
Batch 24/64 loss: 0.00638580322265625
Batch 25/64 loss: -0.0014026165008544922
Batch 26/64 loss: 0.0274849534034729
Batch 27/64 loss: 0.0129738450050354
Batch 28/64 loss: 0.03511202335357666
Batch 29/64 loss: 0.014001607894897461
Batch 30/64 loss: 0.009170114994049072
Batch 31/64 loss: 0.0038028955459594727
Batch 32/64 loss: 0.020402491092681885
Batch 33/64 loss: 0.005547463893890381
Batch 34/64 loss: 0.008274555206298828
Batch 35/64 loss: 0.03782021999359131
Batch 36/64 loss: -0.015441536903381348
Batch 37/64 loss: 0.01493161916732788
Batch 38/64 loss: 0.02754807472229004
Batch 39/64 loss: -0.01806795597076416
Batch 40/64 loss: 0.013501107692718506
Batch 41/64 loss: 0.023329198360443115
Batch 42/64 loss: 0.02001047134399414
Batch 43/64 loss: 0.0015797615051269531
Batch 44/64 loss: 0.018854260444641113
Batch 45/64 loss: 0.01315385103225708
Batch 46/64 loss: 0.017131507396697998
Batch 47/64 loss: 0.004854798316955566
Batch 48/64 loss: 0.011582374572753906
Batch 49/64 loss: -0.01045238971710205
Batch 50/64 loss: 0.008501768112182617
Batch 51/64 loss: 0.027209699153900146
Batch 52/64 loss: 0.016901612281799316
Batch 53/64 loss: 0.0012661218643188477
Batch 54/64 loss: 0.013422369956970215
Batch 55/64 loss: 0.034517765045166016
Batch 56/64 loss: 0.007774174213409424
Batch 57/64 loss: 0.030443906784057617
Batch 58/64 loss: 0.028266727924346924
Batch 59/64 loss: -0.0023776888847351074
Batch 60/64 loss: 0.024826347827911377
Batch 61/64 loss: -0.0017780661582946777
Batch 62/64 loss: 0.014082193374633789
Batch 63/64 loss: 0.007967948913574219
Batch 64/64 loss: 0.01675260066986084
Epoch 57  Train loss: 0.01424568821402157  Val loss: 0.04020693212030679
Saving best model, epoch: 57
Epoch 58
-------------------------------
Batch 1/64 loss: 0.04234731197357178
Batch 2/64 loss: 0.014864504337310791
Batch 3/64 loss: 0.01608288288116455
Batch 4/64 loss: -0.006426870822906494
Batch 5/64 loss: 0.01077568531036377
Batch 6/64 loss: 0.010690391063690186
Batch 7/64 loss: 0.0007554888725280762
Batch 8/64 loss: -0.00039136409759521484
Batch 9/64 loss: 0.017267823219299316
Batch 10/64 loss: 0.007337749004364014
Batch 11/64 loss: -0.005062222480773926
Batch 12/64 loss: 0.02340376377105713
Batch 13/64 loss: -0.004657089710235596
Batch 14/64 loss: 0.010814785957336426
Batch 15/64 loss: 0.0003196597099304199
Batch 16/64 loss: 0.021748602390289307
Batch 17/64 loss: 0.013296842575073242
Batch 18/64 loss: -0.0028603672981262207
Batch 19/64 loss: 0.00654369592666626
Batch 20/64 loss: 0.013643383979797363
Batch 21/64 loss: 0.03202444314956665
Batch 22/64 loss: 0.02180492877960205
Batch 23/64 loss: -0.006522774696350098
Batch 24/64 loss: 0.019055962562561035
Batch 25/64 loss: 0.038610756397247314
Batch 26/64 loss: 0.037087202072143555
Batch 27/64 loss: 0.03443896770477295
Batch 28/64 loss: 0.0009070634841918945
Batch 29/64 loss: 0.016793489456176758
Batch 30/64 loss: 0.023884594440460205
Batch 31/64 loss: -0.000316619873046875
Batch 32/64 loss: -0.0007976889610290527
Batch 33/64 loss: 0.006453216075897217
Batch 34/64 loss: 0.0125349760055542
Batch 35/64 loss: 0.009669303894042969
Batch 36/64 loss: -0.009896516799926758
Batch 37/64 loss: 0.00976407527923584
Batch 38/64 loss: 0.029654979705810547
Batch 39/64 loss: 0.018091142177581787
Batch 40/64 loss: 0.031296491622924805
Batch 41/64 loss: 0.0008205175399780273
Batch 42/64 loss: 0.01424109935760498
Batch 43/64 loss: 0.02034139633178711
Batch 44/64 loss: 0.016177475452423096
Batch 45/64 loss: 0.027270734310150146
Batch 46/64 loss: 0.01861274242401123
Batch 47/64 loss: 0.010218024253845215
Batch 48/64 loss: 0.019236087799072266
Batch 49/64 loss: 0.0008351802825927734
Batch 50/64 loss: 0.02058720588684082
Batch 51/64 loss: 0.013271927833557129
Batch 52/64 loss: 0.01759558916091919
Batch 53/64 loss: -0.01851046085357666
Batch 54/64 loss: 0.017469167709350586
Batch 55/64 loss: -0.0030823945999145508
Batch 56/64 loss: 0.028542637825012207
Batch 57/64 loss: 0.023617327213287354
Batch 58/64 loss: 0.016620874404907227
Batch 59/64 loss: 0.013875603675842285
Batch 60/64 loss: 0.01979362964630127
Batch 61/64 loss: 0.016364216804504395
Batch 62/64 loss: 0.013324975967407227
Batch 63/64 loss: 0.005171656608581543
Batch 64/64 loss: -0.0015780925750732422
Epoch 58  Train loss: 0.01296069481793572  Val loss: 0.03492787593009136
Saving best model, epoch: 58
Epoch 59
-------------------------------
Batch 1/64 loss: 0.029579639434814453
Batch 2/64 loss: 0.04007565975189209
Batch 3/64 loss: 0.021241307258605957
Batch 4/64 loss: -0.0009450316429138184
Batch 5/64 loss: 0.01664644479751587
Batch 6/64 loss: 0.016064941883087158
Batch 7/64 loss: 0.01756608486175537
Batch 8/64 loss: 0.011137127876281738
Batch 9/64 loss: 0.0067375898361206055
Batch 10/64 loss: 0.004580080509185791
Batch 11/64 loss: 0.006708800792694092
Batch 12/64 loss: 0.010822892189025879
Batch 13/64 loss: 0.014991402626037598
Batch 14/64 loss: 0.010553836822509766
Batch 15/64 loss: 0.002407670021057129
Batch 16/64 loss: -0.0016771554946899414
Batch 17/64 loss: 0.01181095838546753
Batch 18/64 loss: 0.021126985549926758
Batch 19/64 loss: 0.00830090045928955
Batch 20/64 loss: 0.01406240463256836
Batch 21/64 loss: 0.04074728488922119
Batch 22/64 loss: 0.016098618507385254
Batch 23/64 loss: -0.003267943859100342
Batch 24/64 loss: 0.0038126111030578613
Batch 25/64 loss: 0.047524988651275635
Batch 26/64 loss: 0.019592225551605225
Batch 27/64 loss: 0.006534695625305176
Batch 28/64 loss: 0.01155310869216919
Batch 29/64 loss: -0.016646265983581543
Batch 30/64 loss: 0.010675549507141113
Batch 31/64 loss: -0.008938014507293701
Batch 32/64 loss: 0.025999605655670166
Batch 33/64 loss: 0.026420652866363525
Batch 34/64 loss: 0.008319437503814697
Batch 35/64 loss: 0.005534231662750244
Batch 36/64 loss: 0.013375461101531982
Batch 37/64 loss: -0.0069266557693481445
Batch 38/64 loss: 0.009537041187286377
Batch 39/64 loss: -0.005188882350921631
Batch 40/64 loss: 0.03140610456466675
Batch 41/64 loss: 0.025560319423675537
Batch 42/64 loss: 0.009759664535522461
Batch 43/64 loss: 0.00862419605255127
Batch 44/64 loss: -0.00793081521987915
Batch 45/64 loss: 0.007854938507080078
Batch 46/64 loss: 0.0008397102355957031
Batch 47/64 loss: 0.02096647024154663
Batch 48/64 loss: 0.007587611675262451
Batch 49/64 loss: 0.012780189514160156
Batch 50/64 loss: 0.024629175662994385
Batch 51/64 loss: 0.014353156089782715
Batch 52/64 loss: 0.003280341625213623
Batch 53/64 loss: 0.029667973518371582
Batch 54/64 loss: 0.017926573753356934
Batch 55/64 loss: 0.009272754192352295
Batch 56/64 loss: 0.021772921085357666
Batch 57/64 loss: 0.0014708638191223145
Batch 58/64 loss: 0.04240208864212036
Batch 59/64 loss: 0.031166553497314453
Batch 60/64 loss: 0.0156019926071167
Batch 61/64 loss: 0.014333903789520264
Batch 62/64 loss: 0.011888682842254639
Batch 63/64 loss: 0.004278421401977539
Batch 64/64 loss: 0.012588918209075928
Epoch 59  Train loss: 0.013105659157622095  Val loss: 0.038527869072157085
Epoch 60
-------------------------------
Batch 1/64 loss: 0.014139235019683838
Batch 2/64 loss: 0.012970328330993652
Batch 3/64 loss: 0.006193280220031738
Batch 4/64 loss: 0.022085487842559814
Batch 5/64 loss: 0.003905773162841797
Batch 6/64 loss: 0.009842872619628906
Batch 7/64 loss: 0.027233660221099854
Batch 8/64 loss: 0.02282470464706421
Batch 9/64 loss: -0.011712729930877686
Batch 10/64 loss: -0.001600027084350586
Batch 11/64 loss: 0.009573221206665039
Batch 12/64 loss: -0.002400219440460205
Batch 13/64 loss: 0.010294854640960693
Batch 14/64 loss: 0.014290153980255127
Batch 15/64 loss: -0.01211172342300415
Batch 16/64 loss: 0.007452130317687988
Batch 17/64 loss: 0.003562450408935547
Batch 18/64 loss: 0.029082179069519043
Batch 19/64 loss: 0.022187232971191406
Batch 20/64 loss: 0.05221223831176758
Batch 21/64 loss: 0.017963111400604248
Batch 22/64 loss: 0.014880955219268799
Batch 23/64 loss: 0.0015864372253417969
Batch 24/64 loss: 0.008254051208496094
Batch 25/64 loss: 0.0023062825202941895
Batch 26/64 loss: 0.017361819744110107
Batch 27/64 loss: 0.03188389539718628
Batch 28/64 loss: -0.003455638885498047
Batch 29/64 loss: 0.00780487060546875
Batch 30/64 loss: 0.025197744369506836
Batch 31/64 loss: 0.007747173309326172
Batch 32/64 loss: 0.011507511138916016
Batch 33/64 loss: 0.029751896858215332
Batch 34/64 loss: 0.0070931315422058105
Batch 35/64 loss: 0.0032626986503601074
Batch 36/64 loss: -0.00842970609664917
Batch 37/64 loss: 0.011669576168060303
Batch 38/64 loss: 0.006444454193115234
Batch 39/64 loss: 0.004047036170959473
Batch 40/64 loss: 0.02167147397994995
Batch 41/64 loss: 0.01243293285369873
Batch 42/64 loss: 0.009317755699157715
Batch 43/64 loss: 0.0058479905128479
Batch 44/64 loss: 0.0335308313369751
Batch 45/64 loss: 0.0015898942947387695
Batch 46/64 loss: 0.003968238830566406
Batch 47/64 loss: 0.01983642578125
Batch 48/64 loss: 0.010100066661834717
Batch 49/64 loss: 0.005947411060333252
Batch 50/64 loss: 0.0004845857620239258
Batch 51/64 loss: 0.016040563583374023
Batch 52/64 loss: 0.012554287910461426
Batch 53/64 loss: 0.026284396648406982
Batch 54/64 loss: 0.004501163959503174
Batch 55/64 loss: -0.0019942522048950195
Batch 56/64 loss: 0.00889897346496582
Batch 57/64 loss: 0.005536317825317383
Batch 58/64 loss: 0.007434368133544922
Batch 59/64 loss: 0.013301193714141846
Batch 60/64 loss: 0.00754547119140625
Batch 61/64 loss: 0.024724125862121582
Batch 62/64 loss: 0.03670239448547363
Batch 63/64 loss: 0.013106048107147217
Batch 64/64 loss: -0.0002148151397705078
Epoch 60  Train loss: 0.011546728657741173  Val loss: 0.03814911985725062
Epoch 61
-------------------------------
Batch 1/64 loss: -0.005849003791809082
Batch 2/64 loss: 0.0027719736099243164
Batch 3/64 loss: 0.008304059505462646
Batch 4/64 loss: 0.005647897720336914
Batch 5/64 loss: 0.02840954065322876
Batch 6/64 loss: 0.028765499591827393
Batch 7/64 loss: -0.004336237907409668
Batch 8/64 loss: -0.012460529804229736
Batch 9/64 loss: 0.011246800422668457
Batch 10/64 loss: 0.009018361568450928
Batch 11/64 loss: -0.00565105676651001
Batch 12/64 loss: 0.06211400032043457
Batch 13/64 loss: 0.018085122108459473
Batch 14/64 loss: 0.003443479537963867
Batch 15/64 loss: -0.007131457328796387
Batch 16/64 loss: 0.013119697570800781
Batch 17/64 loss: 0.021307945251464844
Batch 18/64 loss: 0.018307209014892578
Batch 19/64 loss: 0.01210695505142212
Batch 20/64 loss: 0.010084688663482666
Batch 21/64 loss: 0.017815232276916504
Batch 22/64 loss: 0.014393389225006104
Batch 23/64 loss: 0.009784340858459473
Batch 24/64 loss: 0.021890640258789062
Batch 25/64 loss: 0.01926124095916748
Batch 26/64 loss: 0.003827214241027832
Batch 27/64 loss: 0.00751185417175293
Batch 28/64 loss: 0.004646599292755127
Batch 29/64 loss: 0.0001500248908996582
Batch 30/64 loss: 0.013040781021118164
Batch 31/64 loss: -0.0006207823753356934
Batch 32/64 loss: 0.01500922441482544
Batch 33/64 loss: 0.01596587896347046
Batch 34/64 loss: 0.013712584972381592
Batch 35/64 loss: 0.0040433406829833984
Batch 36/64 loss: 0.025379061698913574
Batch 37/64 loss: 0.011070966720581055
Batch 38/64 loss: -0.0010055303573608398
Batch 39/64 loss: 0.02359527349472046
Batch 40/64 loss: 0.000995635986328125
Batch 41/64 loss: 0.03403174877166748
Batch 42/64 loss: 0.01682913303375244
Batch 43/64 loss: 0.030382990837097168
Batch 44/64 loss: 0.014392733573913574
Batch 45/64 loss: 0.01245129108428955
Batch 46/64 loss: 0.021408557891845703
Batch 47/64 loss: 0.023135662078857422
Batch 48/64 loss: 0.022282123565673828
Batch 49/64 loss: 0.020533621311187744
Batch 50/64 loss: 3.4868717193603516e-05
Batch 51/64 loss: 0.01088029146194458
Batch 52/64 loss: 0.007943511009216309
Batch 53/64 loss: 0.021445393562316895
Batch 54/64 loss: -0.010876893997192383
Batch 55/64 loss: 0.0051947832107543945
Batch 56/64 loss: -0.0011743903160095215
Batch 57/64 loss: -0.0010794997215270996
Batch 58/64 loss: 0.004867076873779297
Batch 59/64 loss: -0.0025753378868103027
Batch 60/64 loss: 0.04179978370666504
Batch 61/64 loss: 0.011006832122802734
Batch 62/64 loss: 0.015246391296386719
Batch 63/64 loss: 0.01699542999267578
Batch 64/64 loss: -0.000951230525970459
Epoch 61  Train loss: 0.011799444638046564  Val loss: 0.032938477509619854
Saving best model, epoch: 61
Epoch 62
-------------------------------
Batch 1/64 loss: 0.019185304641723633
Batch 2/64 loss: 0.009211540222167969
Batch 3/64 loss: -0.008866488933563232
Batch 4/64 loss: -0.0062988996505737305
Batch 5/64 loss: 0.010366261005401611
Batch 6/64 loss: 0.0065773725509643555
Batch 7/64 loss: 0.024376213550567627
Batch 8/64 loss: -0.006570935249328613
Batch 9/64 loss: 0.017182648181915283
Batch 10/64 loss: 0.016180992126464844
Batch 11/64 loss: 0.013109326362609863
Batch 12/64 loss: 0.029075562953948975
Batch 13/64 loss: 0.009241163730621338
Batch 14/64 loss: 0.030268192291259766
Batch 15/64 loss: 0.02192509174346924
Batch 16/64 loss: 0.011695504188537598
Batch 17/64 loss: -0.000570833683013916
Batch 18/64 loss: -0.008378744125366211
Batch 19/64 loss: -0.011505663394927979
Batch 20/64 loss: 0.008490204811096191
Batch 21/64 loss: -0.006722807884216309
Batch 22/64 loss: 0.02120530605316162
Batch 23/64 loss: -0.000695347785949707
Batch 24/64 loss: -0.01187354326248169
Batch 25/64 loss: -0.0006187558174133301
Batch 26/64 loss: 0.011007905006408691
Batch 27/64 loss: 0.01859182119369507
Batch 28/64 loss: 0.023382365703582764
Batch 29/64 loss: 0.024857044219970703
Batch 30/64 loss: 0.03234362602233887
Batch 31/64 loss: -0.0029451847076416016
Batch 32/64 loss: -0.008505702018737793
Batch 33/64 loss: 0.029330670833587646
Batch 34/64 loss: -0.005984246730804443
Batch 35/64 loss: 0.03108876943588257
Batch 36/64 loss: 0.00578153133392334
Batch 37/64 loss: 0.016505420207977295
Batch 38/64 loss: -0.0036962032318115234
Batch 39/64 loss: 0.00923764705657959
Batch 40/64 loss: 0.022966086864471436
Batch 41/64 loss: 0.008773505687713623
Batch 42/64 loss: 0.014436423778533936
Batch 43/64 loss: 0.013869643211364746
Batch 44/64 loss: 0.02106630802154541
Batch 45/64 loss: 0.005544066429138184
Batch 46/64 loss: 0.006500363349914551
Batch 47/64 loss: 0.01914989948272705
Batch 48/64 loss: 0.003265559673309326
Batch 49/64 loss: 0.01703590154647827
Batch 50/64 loss: 0.014163494110107422
Batch 51/64 loss: 0.018446922302246094
Batch 52/64 loss: 0.014904677867889404
Batch 53/64 loss: -0.006253838539123535
Batch 54/64 loss: -0.006478786468505859
Batch 55/64 loss: -0.0022536516189575195
Batch 56/64 loss: 0.009077668190002441
Batch 57/64 loss: -0.00047969818115234375
Batch 58/64 loss: 0.021021246910095215
Batch 59/64 loss: 0.006553292274475098
Batch 60/64 loss: 0.011327981948852539
Batch 61/64 loss: 0.012702465057373047
Batch 62/64 loss: 0.014050722122192383
Batch 63/64 loss: 0.013708293437957764
Batch 64/64 loss: -0.009505987167358398
Epoch 62  Train loss: 0.009614951937806373  Val loss: 0.03392219543457031
Epoch 63
-------------------------------
Batch 1/64 loss: 0.04724222421646118
Batch 2/64 loss: 0.017972707748413086
Batch 3/64 loss: 0.033040404319763184
Batch 4/64 loss: 0.0017440319061279297
Batch 5/64 loss: -0.01348203420639038
Batch 6/64 loss: 0.008331656455993652
Batch 7/64 loss: -0.004595756530761719
Batch 8/64 loss: 0.03764688968658447
Batch 9/64 loss: -0.0001646280288696289
Batch 10/64 loss: 0.0051038265228271484
Batch 11/64 loss: 0.026505768299102783
Batch 12/64 loss: 0.00593721866607666
Batch 13/64 loss: 0.006694197654724121
Batch 14/64 loss: 0.0177728533744812
Batch 15/64 loss: -0.0022699832916259766
Batch 16/64 loss: 0.008530080318450928
Batch 17/64 loss: 0.015066266059875488
Batch 18/64 loss: 0.012081146240234375
Batch 19/64 loss: -0.01638108491897583
Batch 20/64 loss: 0.018635034561157227
Batch 21/64 loss: -0.007959604263305664
Batch 22/64 loss: 0.01696866750717163
Batch 23/64 loss: 0.026895344257354736
Batch 24/64 loss: 0.0020761489868164062
Batch 25/64 loss: -0.004498898983001709
Batch 26/64 loss: 0.011618614196777344
Batch 27/64 loss: -0.0031518936157226562
Batch 28/64 loss: 0.009844362735748291
Batch 29/64 loss: 0.018107593059539795
Batch 30/64 loss: 0.007456064224243164
Batch 31/64 loss: -0.015857577323913574
Batch 32/64 loss: -0.002095937728881836
Batch 33/64 loss: 0.01656132936477661
Batch 34/64 loss: 0.013392865657806396
Batch 35/64 loss: -0.002310037612915039
Batch 36/64 loss: 0.008213043212890625
Batch 37/64 loss: 0.01279914379119873
Batch 38/64 loss: 0.021453678607940674
Batch 39/64 loss: 0.01085054874420166
Batch 40/64 loss: -0.006687939167022705
Batch 41/64 loss: 0.004382669925689697
Batch 42/64 loss: -0.005819797515869141
Batch 43/64 loss: 0.012641310691833496
Batch 44/64 loss: 0.01168966293334961
Batch 45/64 loss: 0.0193328857421875
Batch 46/64 loss: -0.020992815494537354
Batch 47/64 loss: -0.004769027233123779
Batch 48/64 loss: 0.01501387357711792
Batch 49/64 loss: 0.008757412433624268
Batch 50/64 loss: 0.008482098579406738
Batch 51/64 loss: -0.0019047856330871582
Batch 52/64 loss: 0.003876328468322754
Batch 53/64 loss: -0.006996452808380127
Batch 54/64 loss: -0.011377573013305664
Batch 55/64 loss: -0.005707383155822754
Batch 56/64 loss: 0.005039632320404053
Batch 57/64 loss: 0.0056945085525512695
Batch 58/64 loss: 0.014163196086883545
Batch 59/64 loss: 0.002472400665283203
Batch 60/64 loss: 0.004050612449645996
Batch 61/64 loss: 0.02145206928253174
Batch 62/64 loss: 0.017178058624267578
Batch 63/64 loss: 0.011370241641998291
Batch 64/64 loss: 0.008016407489776611
Epoch 63  Train loss: 0.0074216120383318734  Val loss: 0.033803027724892
Epoch 64
-------------------------------
Batch 1/64 loss: -0.002948939800262451
Batch 2/64 loss: 0.008036136627197266
Batch 3/64 loss: 0.0005609393119812012
Batch 4/64 loss: 0.01027768850326538
Batch 5/64 loss: 0.0033332109451293945
Batch 6/64 loss: 0.011780142784118652
Batch 7/64 loss: -0.0007981657981872559
Batch 8/64 loss: 0.013054728507995605
Batch 9/64 loss: 0.012079775333404541
Batch 10/64 loss: 0.0011751055717468262
Batch 11/64 loss: -0.0010906457901000977
Batch 12/64 loss: 0.0012753605842590332
Batch 13/64 loss: -0.012023985385894775
Batch 14/64 loss: 0.004507184028625488
Batch 15/64 loss: 0.008413374423980713
Batch 16/64 loss: 0.010017037391662598
Batch 17/64 loss: 0.009128332138061523
Batch 18/64 loss: 0.01934593915939331
Batch 19/64 loss: 0.014589190483093262
Batch 20/64 loss: 0.017569899559020996
Batch 21/64 loss: 0.0051686763763427734
Batch 22/64 loss: -0.004291355609893799
Batch 23/64 loss: 0.002582252025604248
Batch 24/64 loss: 0.009990334510803223
Batch 25/64 loss: 0.016334176063537598
Batch 26/64 loss: 0.022870182991027832
Batch 27/64 loss: 0.010190904140472412
Batch 28/64 loss: -0.0001996755599975586
Batch 29/64 loss: -0.012578427791595459
Batch 30/64 loss: -0.008633434772491455
Batch 31/64 loss: 0.0070670247077941895
Batch 32/64 loss: 0.011336445808410645
Batch 33/64 loss: 0.010468602180480957
Batch 34/64 loss: 0.011841237545013428
Batch 35/64 loss: -0.005058467388153076
Batch 36/64 loss: 0.038893818855285645
Batch 37/64 loss: 0.017702877521514893
Batch 38/64 loss: 0.034234046936035156
Batch 39/64 loss: -0.004585385322570801
Batch 40/64 loss: 0.006312251091003418
Batch 41/64 loss: 0.016142845153808594
Batch 42/64 loss: -0.0054413676261901855
Batch 43/64 loss: 0.033171772956848145
Batch 44/64 loss: -0.010073065757751465
Batch 45/64 loss: 0.02668285369873047
Batch 46/64 loss: 0.025931239128112793
Batch 47/64 loss: -0.00885152816772461
Batch 48/64 loss: 0.007356703281402588
Batch 49/64 loss: 0.010368645191192627
Batch 50/64 loss: -0.009312331676483154
Batch 51/64 loss: 0.011864840984344482
Batch 52/64 loss: 0.004723072052001953
Batch 53/64 loss: -0.007659196853637695
Batch 54/64 loss: 0.003558933734893799
Batch 55/64 loss: -0.00417780876159668
Batch 56/64 loss: 0.01378631591796875
Batch 57/64 loss: 0.0046083927154541016
Batch 58/64 loss: 0.03304702043533325
Batch 59/64 loss: 0.014516770839691162
Batch 60/64 loss: 0.015171051025390625
Batch 61/64 loss: -0.004213809967041016
Batch 62/64 loss: -0.0022855401039123535
Batch 63/64 loss: -0.0006256699562072754
Batch 64/64 loss: 0.02380549907684326
Epoch 64  Train loss: 0.007593296555911793  Val loss: 0.03247457845104519
Saving best model, epoch: 64
Epoch 65
-------------------------------
Batch 1/64 loss: 0.008970677852630615
Batch 2/64 loss: 0.0031801462173461914
Batch 3/64 loss: 0.0068089962005615234
Batch 4/64 loss: 0.017229080200195312
Batch 5/64 loss: 0.00624394416809082
Batch 6/64 loss: 0.012945950031280518
Batch 7/64 loss: -0.02480781078338623
Batch 8/64 loss: 0.0020722150802612305
Batch 9/64 loss: -0.006394505500793457
Batch 10/64 loss: 0.019278645515441895
Batch 11/64 loss: -0.0066533684730529785
Batch 12/64 loss: 0.011357247829437256
Batch 13/64 loss: 0.012457072734832764
Batch 14/64 loss: 0.004887759685516357
Batch 15/64 loss: 0.017106473445892334
Batch 16/64 loss: -0.010932862758636475
Batch 17/64 loss: 0.01963341236114502
Batch 18/64 loss: 0.0068013668060302734
Batch 19/64 loss: -0.014184713363647461
Batch 20/64 loss: 0.00841522216796875
Batch 21/64 loss: 0.013395309448242188
Batch 22/64 loss: -0.003731966018676758
Batch 23/64 loss: 0.008033692836761475
Batch 24/64 loss: 0.02222353219985962
Batch 25/64 loss: 0.0001925826072692871
Batch 26/64 loss: 0.0053629279136657715
Batch 27/64 loss: 0.006450831890106201
Batch 28/64 loss: -0.004111289978027344
Batch 29/64 loss: 0.011145710945129395
Batch 30/64 loss: 0.0038919448852539062
Batch 31/64 loss: 0.04468327760696411
Batch 32/64 loss: 0.020476460456848145
Batch 33/64 loss: 0.009431242942810059
Batch 34/64 loss: 0.0012998580932617188
Batch 35/64 loss: 0.01233983039855957
Batch 36/64 loss: 0.009034514427185059
Batch 37/64 loss: 0.013710737228393555
Batch 38/64 loss: 0.010319828987121582
Batch 39/64 loss: 0.009633839130401611
Batch 40/64 loss: 0.0045926570892333984
Batch 41/64 loss: 0.007872402667999268
Batch 42/64 loss: 0.0004356503486633301
Batch 43/64 loss: -0.006044924259185791
Batch 44/64 loss: 0.01520693302154541
Batch 45/64 loss: 0.0069748759269714355
Batch 46/64 loss: 0.014737069606781006
Batch 47/64 loss: 0.008393704891204834
Batch 48/64 loss: 0.023414790630340576
Batch 49/64 loss: 0.0144881010055542
Batch 50/64 loss: 0.0077675580978393555
Batch 51/64 loss: 0.003516674041748047
Batch 52/64 loss: 0.011992335319519043
Batch 53/64 loss: 0.010933637619018555
Batch 54/64 loss: -0.0008658170700073242
Batch 55/64 loss: 0.029939770698547363
Batch 56/64 loss: -0.0048621296882629395
Batch 57/64 loss: -0.024802088737487793
Batch 58/64 loss: 0.0161973237991333
Batch 59/64 loss: 0.02170795202255249
Batch 60/64 loss: 0.00985628366470337
Batch 61/64 loss: -0.002652585506439209
Batch 62/64 loss: -0.011552691459655762
Batch 63/64 loss: -0.018410921096801758
Batch 64/64 loss: -0.006273329257965088
Epoch 65  Train loss: 0.006624774605620141  Val loss: 0.02959148805985336
Saving best model, epoch: 65
Epoch 66
-------------------------------
Batch 1/64 loss: 0.023549973964691162
Batch 2/64 loss: 0.00995868444442749
Batch 3/64 loss: 0.00404667854309082
Batch 4/64 loss: -0.002097010612487793
Batch 5/64 loss: 0.023629367351531982
Batch 6/64 loss: 0.003284633159637451
Batch 7/64 loss: -0.013474762439727783
Batch 8/64 loss: 0.0006802678108215332
Batch 9/64 loss: 0.007137715816497803
Batch 10/64 loss: -0.003366827964782715
Batch 11/64 loss: -0.0002294778823852539
Batch 12/64 loss: -0.003012239933013916
Batch 13/64 loss: 0.016042709350585938
Batch 14/64 loss: -0.01302647590637207
Batch 15/64 loss: 0.004614055156707764
Batch 16/64 loss: -0.008140742778778076
Batch 17/64 loss: -0.026331305503845215
Batch 18/64 loss: 0.0003198981285095215
Batch 19/64 loss: 0.018516957759857178
Batch 20/64 loss: -0.02010554075241089
Batch 21/64 loss: 0.006516993045806885
Batch 22/64 loss: 0.0008894205093383789
Batch 23/64 loss: 0.02860623598098755
Batch 24/64 loss: 0.011876046657562256
Batch 25/64 loss: 0.00045812129974365234
Batch 26/64 loss: 0.0088120698928833
Batch 27/64 loss: 0.015103459358215332
Batch 28/64 loss: 0.006905555725097656
Batch 29/64 loss: 0.021394073963165283
Batch 30/64 loss: -0.016461491584777832
Batch 31/64 loss: -0.0012298226356506348
Batch 32/64 loss: -0.0042046308517456055
Batch 33/64 loss: -0.0027617812156677246
Batch 34/64 loss: 0.0012996196746826172
Batch 35/64 loss: 0.011429011821746826
Batch 36/64 loss: 0.0006725788116455078
Batch 37/64 loss: 0.004043877124786377
Batch 38/64 loss: 0.0074416399002075195
Batch 39/64 loss: 0.012864947319030762
Batch 40/64 loss: 0.012065768241882324
Batch 41/64 loss: 0.017000675201416016
Batch 42/64 loss: -0.0008385181427001953
Batch 43/64 loss: 0.01627480983734131
Batch 44/64 loss: -0.025887131690979004
Batch 45/64 loss: 0.00020933151245117188
Batch 46/64 loss: 0.004358112812042236
Batch 47/64 loss: 0.005326390266418457
Batch 48/64 loss: 0.014462709426879883
Batch 49/64 loss: 0.0020890235900878906
Batch 50/64 loss: -0.0017820000648498535
Batch 51/64 loss: 0.02217710018157959
Batch 52/64 loss: 0.011793732643127441
Batch 53/64 loss: -0.00026810169219970703
Batch 54/64 loss: 0.009540975093841553
Batch 55/64 loss: 0.0033333897590637207
Batch 56/64 loss: 0.012140870094299316
Batch 57/64 loss: 0.003415524959564209
Batch 58/64 loss: 0.008376836776733398
Batch 59/64 loss: -0.00013381242752075195
Batch 60/64 loss: -0.016630113124847412
Batch 61/64 loss: 0.040812134742736816
Batch 62/64 loss: -0.002959728240966797
Batch 63/64 loss: 0.0007804632186889648
Batch 64/64 loss: 0.008587419986724854
Epoch 66  Train loss: 0.0043568861250783885  Val loss: 0.02547053352664017
Saving best model, epoch: 66
Epoch 67
-------------------------------
Batch 1/64 loss: 0.004162490367889404
Batch 2/64 loss: 0.0008493661880493164
Batch 3/64 loss: 0.01950836181640625
Batch 4/64 loss: -0.003217458724975586
Batch 5/64 loss: -0.0022089481353759766
Batch 6/64 loss: -0.009074628353118896
Batch 7/64 loss: -0.009076535701751709
Batch 8/64 loss: -0.0013520121574401855
Batch 9/64 loss: -0.013914048671722412
Batch 10/64 loss: 0.0030201077461242676
Batch 11/64 loss: -0.009309172630310059
Batch 12/64 loss: 0.004725456237792969
Batch 13/64 loss: 0.025482177734375
Batch 14/64 loss: -0.0020508766174316406
Batch 15/64 loss: 0.0034232735633850098
Batch 16/64 loss: 0.002108335494995117
Batch 17/64 loss: 0.009645819664001465
Batch 18/64 loss: 0.00738602876663208
Batch 19/64 loss: 0.0040403008460998535
Batch 20/64 loss: -0.00102996826171875
Batch 21/64 loss: -0.010744810104370117
Batch 22/64 loss: 0.018100202083587646
Batch 23/64 loss: -0.0067809224128723145
Batch 24/64 loss: 0.016831040382385254
Batch 25/64 loss: -0.001170337200164795
Batch 26/64 loss: -0.0018048286437988281
Batch 27/64 loss: 0.009535729885101318
Batch 28/64 loss: 0.02477419376373291
Batch 29/64 loss: 0.02867865562438965
Batch 30/64 loss: 0.008054673671722412
Batch 31/64 loss: 0.008735597133636475
Batch 32/64 loss: 0.02003610134124756
Batch 33/64 loss: 0.01311558485031128
Batch 34/64 loss: 0.011442184448242188
Batch 35/64 loss: -0.005193114280700684
Batch 36/64 loss: 0.005370795726776123
Batch 37/64 loss: 0.02507913112640381
Batch 38/64 loss: -0.0016126036643981934
Batch 39/64 loss: 0.02566540241241455
Batch 40/64 loss: 0.014415144920349121
Batch 41/64 loss: -0.014805793762207031
Batch 42/64 loss: -0.0013495683670043945
Batch 43/64 loss: -0.005369842052459717
Batch 44/64 loss: 0.01798689365386963
Batch 45/64 loss: 0.01938915252685547
Batch 46/64 loss: 0.014934062957763672
Batch 47/64 loss: 0.025356531143188477
Batch 48/64 loss: 0.024356424808502197
Batch 49/64 loss: 0.014900684356689453
Batch 50/64 loss: 0.022259235382080078
Batch 51/64 loss: -0.004358649253845215
Batch 52/64 loss: 0.003377676010131836
Batch 53/64 loss: 0.0010221600532531738
Batch 54/64 loss: 0.01503753662109375
Batch 55/64 loss: 0.02480792999267578
Batch 56/64 loss: -0.008487999439239502
Batch 57/64 loss: 0.005241692066192627
Batch 58/64 loss: 0.010364353656768799
Batch 59/64 loss: -0.0052915215492248535
Batch 60/64 loss: -0.009289681911468506
Batch 61/64 loss: 0.015558302402496338
Batch 62/64 loss: 0.009082555770874023
Batch 63/64 loss: 0.02336132526397705
Batch 64/64 loss: 0.0034140348434448242
Epoch 67  Train loss: 0.006843762771756042  Val loss: 0.03714194330562841
Epoch 68
-------------------------------
Batch 1/64 loss: 0.014109194278717041
Batch 2/64 loss: -0.0016756057739257812
Batch 3/64 loss: -0.02062249183654785
Batch 4/64 loss: 0.0435948371887207
Batch 5/64 loss: 0.0007804632186889648
Batch 6/64 loss: 0.0012221336364746094
Batch 7/64 loss: 0.007514595985412598
Batch 8/64 loss: -0.009848833084106445
Batch 9/64 loss: 0.008941471576690674
Batch 10/64 loss: 0.010436177253723145
Batch 11/64 loss: -0.013997793197631836
Batch 12/64 loss: -0.006766855716705322
Batch 13/64 loss: 0.019444286823272705
Batch 14/64 loss: -0.00498431921005249
Batch 15/64 loss: 0.0003083348274230957
Batch 16/64 loss: 0.020483970642089844
Batch 17/64 loss: 0.030244827270507812
Batch 18/64 loss: 0.01924896240234375
Batch 19/64 loss: 0.004513144493103027
Batch 20/64 loss: 0.006100654602050781
Batch 21/64 loss: -0.005401670932769775
Batch 22/64 loss: 0.01147228479385376
Batch 23/64 loss: 0.011008024215698242
Batch 24/64 loss: -0.008028268814086914
Batch 25/64 loss: 0.006664693355560303
Batch 26/64 loss: -9.530782699584961e-05
Batch 27/64 loss: 0.006523489952087402
Batch 28/64 loss: 0.00027173757553100586
Batch 29/64 loss: 0.003560304641723633
Batch 30/64 loss: -0.012358546257019043
Batch 31/64 loss: 0.004436373710632324
Batch 32/64 loss: 0.0019503235816955566
Batch 33/64 loss: -0.00038695335388183594
Batch 34/64 loss: 0.0010546445846557617
Batch 35/64 loss: 0.014797747135162354
Batch 36/64 loss: 0.017795860767364502
Batch 37/64 loss: 0.0015142560005187988
Batch 38/64 loss: -0.006632626056671143
Batch 39/64 loss: -0.0038080811500549316
Batch 40/64 loss: -0.007454872131347656
Batch 41/64 loss: 0.004417479038238525
Batch 42/64 loss: 0.011302292346954346
Batch 43/64 loss: -0.00676196813583374
Batch 44/64 loss: 0.026610732078552246
Batch 45/64 loss: -0.00586855411529541
Batch 46/64 loss: -0.00152587890625
Batch 47/64 loss: 0.019254088401794434
Batch 48/64 loss: 0.005070865154266357
Batch 49/64 loss: -0.014730453491210938
Batch 50/64 loss: -0.005411684513092041
Batch 51/64 loss: -0.001727759838104248
Batch 52/64 loss: 0.01926243305206299
Batch 53/64 loss: -0.0020301342010498047
Batch 54/64 loss: 0.018387258052825928
Batch 55/64 loss: -0.005400717258453369
Batch 56/64 loss: -0.0044814348220825195
Batch 57/64 loss: -0.002147376537322998
Batch 58/64 loss: -0.0034862756729125977
Batch 59/64 loss: -0.008992493152618408
Batch 60/64 loss: -0.013535797595977783
Batch 61/64 loss: -0.01842719316482544
Batch 62/64 loss: 0.02826547622680664
Batch 63/64 loss: -0.0003210306167602539
Batch 64/64 loss: 0.001059114933013916
Epoch 68  Train loss: 0.0032070082776686723  Val loss: 0.03391428133056745
Epoch 69
-------------------------------
Batch 1/64 loss: 0.0006757974624633789
Batch 2/64 loss: -0.011769652366638184
Batch 3/64 loss: -0.004096508026123047
Batch 4/64 loss: -0.00979834794998169
Batch 5/64 loss: 0.016864001750946045
Batch 6/64 loss: 0.009488463401794434
Batch 7/64 loss: 0.007736086845397949
Batch 8/64 loss: 0.013577878475189209
Batch 9/64 loss: 0.03159576654434204
Batch 10/64 loss: 0.006676077842712402
Batch 11/64 loss: 0.031064271926879883
Batch 12/64 loss: 0.005814909934997559
Batch 13/64 loss: -0.006016850471496582
Batch 14/64 loss: -0.003058910369873047
Batch 15/64 loss: -0.0070882439613342285
Batch 16/64 loss: 0.029523491859436035
Batch 17/64 loss: -0.0013304948806762695
Batch 18/64 loss: 0.028856217861175537
Batch 19/64 loss: -0.004058957099914551
Batch 20/64 loss: -0.008839547634124756
Batch 21/64 loss: 0.004919767379760742
Batch 22/64 loss: -0.010696053504943848
Batch 23/64 loss: 0.020298004150390625
Batch 24/64 loss: -0.0013735294342041016
Batch 25/64 loss: -0.006640493869781494
Batch 26/64 loss: 0.009562551975250244
Batch 27/64 loss: -0.0064983367919921875
Batch 28/64 loss: -0.016516566276550293
Batch 29/64 loss: 0.007616698741912842
Batch 30/64 loss: -0.011337637901306152
Batch 31/64 loss: 0.011690199375152588
Batch 32/64 loss: 0.02327483892440796
Batch 33/64 loss: -0.006183803081512451
Batch 34/64 loss: 0.0009515285491943359
Batch 35/64 loss: -0.009306490421295166
Batch 36/64 loss: -0.001575469970703125
Batch 37/64 loss: 0.005646765232086182
Batch 38/64 loss: 0.032032012939453125
Batch 39/64 loss: -0.017024576663970947
Batch 40/64 loss: -0.008946120738983154
Batch 41/64 loss: -0.007848858833312988
Batch 42/64 loss: -0.014696300029754639
Batch 43/64 loss: -0.003230154514312744
Batch 44/64 loss: -0.003100454807281494
Batch 45/64 loss: -0.005219578742980957
Batch 46/64 loss: -0.012380063533782959
Batch 47/64 loss: -0.0007926225662231445
Batch 48/64 loss: -0.0012383460998535156
Batch 49/64 loss: -0.0004425048828125
Batch 50/64 loss: -0.00851893424987793
Batch 51/64 loss: 0.0005854368209838867
Batch 52/64 loss: -0.0033596158027648926
Batch 53/64 loss: 0.04172706604003906
Batch 54/64 loss: -0.010747313499450684
Batch 55/64 loss: 0.023184657096862793
Batch 56/64 loss: -0.00831747055053711
Batch 57/64 loss: 0.04669684171676636
Batch 58/64 loss: -0.011491239070892334
Batch 59/64 loss: 0.0171775221824646
Batch 60/64 loss: -0.00457388162612915
Batch 61/64 loss: -0.015916824340820312
Batch 62/64 loss: 0.011408805847167969
Batch 63/64 loss: 0.028146862983703613
Batch 64/64 loss: 0.011610567569732666
Epoch 69  Train loss: 0.0033171716858358944  Val loss: 0.03395951060495016
Epoch 70
-------------------------------
Batch 1/64 loss: 0.012573838233947754
Batch 2/64 loss: -0.006151556968688965
Batch 3/64 loss: -0.0017458200454711914
Batch 4/64 loss: -0.00710827112197876
Batch 5/64 loss: 0.006435036659240723
Batch 6/64 loss: -0.005079030990600586
Batch 7/64 loss: 0.0042133331298828125
Batch 8/64 loss: -0.012931108474731445
Batch 9/64 loss: -0.011865496635437012
Batch 10/64 loss: -0.0040509700775146484
Batch 11/64 loss: 0.004480540752410889
Batch 12/64 loss: -0.002607583999633789
Batch 13/64 loss: -0.0012193918228149414
Batch 14/64 loss: -0.0014746785163879395
Batch 15/64 loss: -0.009375512599945068
Batch 16/64 loss: 0.0004845857620239258
Batch 17/64 loss: 0.006016433238983154
Batch 18/64 loss: -0.0016887187957763672
Batch 19/64 loss: -0.0012345314025878906
Batch 20/64 loss: -0.005306720733642578
Batch 21/64 loss: -0.021233320236206055
Batch 22/64 loss: 0.03545975685119629
Batch 23/64 loss: -0.0125807523727417
Batch 24/64 loss: 0.0010411739349365234
Batch 25/64 loss: 0.029691100120544434
Batch 26/64 loss: 0.0042160749435424805
Batch 27/64 loss: 0.01567554473876953
Batch 28/64 loss: 0.018410921096801758
Batch 29/64 loss: 0.0012108087539672852
Batch 30/64 loss: 0.009105086326599121
Batch 31/64 loss: -0.005793154239654541
Batch 32/64 loss: 0.004486382007598877
Batch 33/64 loss: 0.0036055445671081543
Batch 34/64 loss: 0.010969281196594238
Batch 35/64 loss: -0.008818864822387695
Batch 36/64 loss: 0.030287742614746094
Batch 37/64 loss: 0.011271357536315918
Batch 38/64 loss: -0.006438255310058594
Batch 39/64 loss: 0.011190235614776611
Batch 40/64 loss: -0.0015540122985839844
Batch 41/64 loss: 0.019662916660308838
Batch 42/64 loss: -0.009091377258300781
Batch 43/64 loss: 0.005782485008239746
Batch 44/64 loss: 0.029333293437957764
Batch 45/64 loss: 0.005031943321228027
Batch 46/64 loss: -0.004377901554107666
Batch 47/64 loss: 0.0005145668983459473
Batch 48/64 loss: -0.01315087080001831
Batch 49/64 loss: -0.0006149411201477051
Batch 50/64 loss: 0.0053032636642456055
Batch 51/64 loss: 0.008833348751068115
Batch 52/64 loss: 0.017552495002746582
Batch 53/64 loss: 0.001448512077331543
Batch 54/64 loss: -0.012263894081115723
Batch 55/64 loss: 0.011063158512115479
Batch 56/64 loss: 0.012761235237121582
Batch 57/64 loss: 0.01795220375061035
Batch 58/64 loss: 0.01651322841644287
Batch 59/64 loss: -0.017088651657104492
Batch 60/64 loss: 0.016505956649780273
Batch 61/64 loss: -0.01274573802947998
Batch 62/64 loss: -0.010225176811218262
Batch 63/64 loss: -0.005765795707702637
Batch 64/64 loss: 0.006585597991943359
Epoch 70  Train loss: 0.0028304389878815294  Val loss: 0.03899563526369862
Epoch 71
-------------------------------
Batch 1/64 loss: -0.012979507446289062
Batch 2/64 loss: -0.0028524398803710938
Batch 3/64 loss: -0.002055227756500244
Batch 4/64 loss: 0.02118217945098877
Batch 5/64 loss: 0.006645023822784424
Batch 6/64 loss: 0.026501774787902832
Batch 7/64 loss: 0.009131669998168945
Batch 8/64 loss: 0.006144404411315918
Batch 9/64 loss: 0.009713590145111084
Batch 10/64 loss: -0.010802924633026123
Batch 11/64 loss: -0.00470346212387085
Batch 12/64 loss: -0.00444638729095459
Batch 13/64 loss: -0.011697530746459961
Batch 14/64 loss: -0.015659689903259277
Batch 15/64 loss: 0.00896066427230835
Batch 16/64 loss: 0.0410042405128479
Batch 17/64 loss: 0.001475989818572998
Batch 18/64 loss: 0.008339464664459229
Batch 19/64 loss: 0.008676707744598389
Batch 20/64 loss: -0.011219263076782227
Batch 21/64 loss: -0.002389252185821533
Batch 22/64 loss: -0.004243135452270508
Batch 23/64 loss: -0.013984203338623047
Batch 24/64 loss: -0.017071008682250977
Batch 25/64 loss: -0.025029361248016357
Batch 26/64 loss: 0.017850637435913086
Batch 27/64 loss: 0.007603704929351807
Batch 28/64 loss: -0.004155635833740234
Batch 29/64 loss: -0.028278470039367676
Batch 30/64 loss: -0.001926422119140625
Batch 31/64 loss: -0.016808390617370605
Batch 32/64 loss: 0.00018167495727539062
Batch 33/64 loss: -0.009836435317993164
Batch 34/64 loss: -0.010149836540222168
Batch 35/64 loss: 0.010695099830627441
Batch 36/64 loss: -0.00016736984252929688
Batch 37/64 loss: -0.0016512870788574219
Batch 38/64 loss: 0.013296902179718018
Batch 39/64 loss: 0.011902689933776855
Batch 40/64 loss: -0.0022622346878051758
Batch 41/64 loss: -0.007938563823699951
Batch 42/64 loss: 0.02473658323287964
Batch 43/64 loss: -0.001686692237854004
Batch 44/64 loss: 0.022006630897521973
Batch 45/64 loss: -0.011798858642578125
Batch 46/64 loss: -0.011900722980499268
Batch 47/64 loss: -0.010904312133789062
Batch 48/64 loss: -0.01469200849533081
Batch 49/64 loss: -0.011892318725585938
Batch 50/64 loss: 0.010939300060272217
Batch 51/64 loss: 0.002076089382171631
Batch 52/64 loss: -0.012244343757629395
Batch 53/64 loss: -0.0036095380783081055
Batch 54/64 loss: -0.0001423358917236328
Batch 55/64 loss: 0.021709442138671875
Batch 56/64 loss: -0.009551763534545898
Batch 57/64 loss: -0.002281486988067627
Batch 58/64 loss: -0.0023040175437927246
Batch 59/64 loss: 0.008169412612915039
Batch 60/64 loss: -0.009131431579589844
Batch 61/64 loss: -0.007697165012359619
Batch 62/64 loss: -0.014756917953491211
Batch 63/64 loss: 0.024645507335662842
Batch 64/64 loss: -0.004871845245361328
Epoch 71  Train loss: -0.0004230031780168122  Val loss: 0.04148920151785886
Epoch 72
-------------------------------
Batch 1/64 loss: 0.005625724792480469
Batch 2/64 loss: 0.001216292381286621
Batch 3/64 loss: 0.0004757046699523926
Batch 4/64 loss: -0.0020865797996520996
Batch 5/64 loss: 0.008269011974334717
Batch 6/64 loss: -0.001216113567352295
Batch 7/64 loss: -0.0033765435218811035
Batch 8/64 loss: -0.015054702758789062
Batch 9/64 loss: 0.004327356815338135
Batch 10/64 loss: 0.009736955165863037
Batch 11/64 loss: -0.01304316520690918
Batch 12/64 loss: 0.019231081008911133
Batch 13/64 loss: -0.011482477188110352
Batch 14/64 loss: 0.011474907398223877
Batch 15/64 loss: 0.01705002784729004
Batch 16/64 loss: 0.024515151977539062
Batch 17/64 loss: -0.004448533058166504
Batch 18/64 loss: -0.002918839454650879
Batch 19/64 loss: 0.0025166869163513184
Batch 20/64 loss: 0.007570803165435791
Batch 21/64 loss: -0.0012968182563781738
Batch 22/64 loss: -0.0055817365646362305
Batch 23/64 loss: -0.011833012104034424
Batch 24/64 loss: 0.0161592960357666
Batch 25/64 loss: -0.010912477970123291
Batch 26/64 loss: 0.018843472003936768
Batch 27/64 loss: -0.003764033317565918
Batch 28/64 loss: -0.009204208850860596
Batch 29/64 loss: 0.0027837753295898438
Batch 30/64 loss: 0.017886757850646973
Batch 31/64 loss: -0.01459115743637085
Batch 32/64 loss: -0.004697561264038086
Batch 33/64 loss: -0.011429250240325928
Batch 34/64 loss: 0.004231870174407959
Batch 35/64 loss: -0.007391870021820068
Batch 36/64 loss: -0.008184850215911865
Batch 37/64 loss: -0.012844204902648926
Batch 38/64 loss: 0.013143956661224365
Batch 39/64 loss: 0.005554556846618652
Batch 40/64 loss: -0.006774604320526123
Batch 41/64 loss: 0.011400282382965088
Batch 42/64 loss: -0.0014490485191345215
Batch 43/64 loss: 0.008048832416534424
Batch 44/64 loss: 0.006644010543823242
Batch 45/64 loss: 0.00906383991241455
Batch 46/64 loss: -0.010139107704162598
Batch 47/64 loss: 0.023854374885559082
Batch 48/64 loss: -0.00691908597946167
Batch 49/64 loss: -0.0009402036666870117
Batch 50/64 loss: 0.004500627517700195
Batch 51/64 loss: -0.007841110229492188
Batch 52/64 loss: -0.0024913549423217773
Batch 53/64 loss: -0.015892505645751953
Batch 54/64 loss: 0.026495695114135742
Batch 55/64 loss: -0.005235910415649414
Batch 56/64 loss: -0.029976606369018555
Batch 57/64 loss: 0.0024497509002685547
Batch 58/64 loss: -0.005596816539764404
Batch 59/64 loss: -0.0050119757652282715
Batch 60/64 loss: -0.01163560152053833
Batch 61/64 loss: -0.003133535385131836
Batch 62/64 loss: 0.010673224925994873
Batch 63/64 loss: 0.020990967750549316
Batch 64/64 loss: 0.0005150437355041504
Epoch 72  Train loss: 0.0007329517719792385  Val loss: 0.02703551041711237
Epoch 73
-------------------------------
Batch 1/64 loss: -0.016937613487243652
Batch 2/64 loss: -0.00610196590423584
Batch 3/64 loss: 0.012504398822784424
Batch 4/64 loss: -0.013886868953704834
Batch 5/64 loss: -0.011332511901855469
Batch 6/64 loss: -0.008953392505645752
Batch 7/64 loss: -0.00689464807510376
Batch 8/64 loss: -0.008186161518096924
Batch 9/64 loss: 0.012031197547912598
Batch 10/64 loss: -0.004675865173339844
Batch 11/64 loss: 0.011873245239257812
Batch 12/64 loss: -0.015410840511322021
Batch 13/64 loss: -0.00722426176071167
Batch 14/64 loss: 0.0016840696334838867
Batch 15/64 loss: -0.015693247318267822
Batch 16/64 loss: 0.00548022985458374
Batch 17/64 loss: -0.02329176664352417
Batch 18/64 loss: 0.009383559226989746
Batch 19/64 loss: -0.0002573728561401367
Batch 20/64 loss: -0.014087915420532227
Batch 21/64 loss: -0.011920571327209473
Batch 22/64 loss: -0.002462923526763916
Batch 23/64 loss: -0.0011500716209411621
Batch 24/64 loss: 0.0014026761054992676
Batch 25/64 loss: -0.01289147138595581
Batch 26/64 loss: 0.01421290636062622
Batch 27/64 loss: -0.0010077357292175293
Batch 28/64 loss: -0.008251667022705078
Batch 29/64 loss: 0.00919884443283081
Batch 30/64 loss: 0.028509438037872314
Batch 31/64 loss: 0.014680922031402588
Batch 32/64 loss: -0.01616203784942627
Batch 33/64 loss: 0.01822417974472046
Batch 34/64 loss: 0.014361441135406494
Batch 35/64 loss: 0.02008676528930664
Batch 36/64 loss: -0.0084913969039917
Batch 37/64 loss: -0.02134650945663452
Batch 38/64 loss: 0.0028507113456726074
Batch 39/64 loss: 0.008558452129364014
Batch 40/64 loss: -0.011007428169250488
Batch 41/64 loss: -0.022869646549224854
Batch 42/64 loss: 0.006549477577209473
Batch 43/64 loss: 0.015101730823516846
Batch 44/64 loss: -0.0017820000648498535
Batch 45/64 loss: -0.006923019886016846
Batch 46/64 loss: -0.0056149959564208984
Batch 47/64 loss: -0.005672931671142578
Batch 48/64 loss: 0.00500720739364624
Batch 49/64 loss: -0.0011681318283081055
Batch 50/64 loss: 0.008815526962280273
Batch 51/64 loss: 0.0010195374488830566
Batch 52/64 loss: 0.01925492286682129
Batch 53/64 loss: 0.01609981060028076
Batch 54/64 loss: -0.008828043937683105
Batch 55/64 loss: -0.004034519195556641
Batch 56/64 loss: 0.005325436592102051
Batch 57/64 loss: 0.03846365213394165
Batch 58/64 loss: -0.013931870460510254
Batch 59/64 loss: 0.00565570592880249
Batch 60/64 loss: 0.0005533695220947266
Batch 61/64 loss: 0.00425952672958374
Batch 62/64 loss: -0.018382728099822998
Batch 63/64 loss: -0.0002886056900024414
Batch 64/64 loss: 0.00385892391204834
Epoch 73  Train loss: -0.0003620330025168026  Val loss: 0.04546819212510414
Epoch 74
-------------------------------
Batch 1/64 loss: -0.012777507305145264
Batch 2/64 loss: -0.0016608238220214844
Batch 3/64 loss: 0.015791654586791992
Batch 4/64 loss: 0.003963947296142578
Batch 5/64 loss: 0.02246248722076416
Batch 6/64 loss: 0.014966189861297607
Batch 7/64 loss: 0.00806570053100586
Batch 8/64 loss: 0.007800042629241943
Batch 9/64 loss: -0.011329293251037598
Batch 10/64 loss: 0.0012372136116027832
Batch 11/64 loss: -0.006263136863708496
Batch 12/64 loss: -0.01773303747177124
Batch 13/64 loss: 0.026932597160339355
Batch 14/64 loss: -0.0032051801681518555
Batch 15/64 loss: -0.015582263469696045
Batch 16/64 loss: 0.010803580284118652
Batch 17/64 loss: 0.01540607213973999
Batch 18/64 loss: 0.018526136875152588
Batch 19/64 loss: 0.0004832744598388672
Batch 20/64 loss: -0.0018172860145568848
Batch 21/64 loss: -0.005793631076812744
Batch 22/64 loss: 0.013322412967681885
Batch 23/64 loss: 0.01670050621032715
Batch 24/64 loss: 0.013560593128204346
Batch 25/64 loss: 0.01687633991241455
Batch 26/64 loss: 0.012293994426727295
Batch 27/64 loss: 0.0011686086654663086
Batch 28/64 loss: 0.01136857271194458
Batch 29/64 loss: -0.013564646244049072
Batch 30/64 loss: 0.005570530891418457
Batch 31/64 loss: -0.013548851013183594
Batch 32/64 loss: -0.0032094717025756836
Batch 33/64 loss: -0.01709115505218506
Batch 34/64 loss: 0.005913197994232178
Batch 35/64 loss: -0.012119531631469727
Batch 36/64 loss: 0.01280820369720459
Batch 37/64 loss: -0.002330958843231201
Batch 38/64 loss: 0.02570509910583496
Batch 39/64 loss: 0.032710492610931396
Batch 40/64 loss: 0.00015175342559814453
Batch 41/64 loss: 0.006522417068481445
Batch 42/64 loss: -0.013689637184143066
Batch 43/64 loss: 0.009152770042419434
Batch 44/64 loss: -0.004881560802459717
Batch 45/64 loss: 0.010592103004455566
Batch 46/64 loss: -0.009105086326599121
Batch 47/64 loss: -0.01401740312576294
Batch 48/64 loss: 0.0009950995445251465
Batch 49/64 loss: -0.0013919472694396973
Batch 50/64 loss: -0.010041415691375732
Batch 51/64 loss: -0.007383108139038086
Batch 52/64 loss: -0.005071282386779785
Batch 53/64 loss: 0.010188579559326172
Batch 54/64 loss: 0.008011281490325928
Batch 55/64 loss: -0.01911604404449463
Batch 56/64 loss: 0.011279463768005371
Batch 57/64 loss: 0.012603342533111572
Batch 58/64 loss: 0.004518449306488037
Batch 59/64 loss: 0.009010016918182373
Batch 60/64 loss: -0.008751392364501953
Batch 61/64 loss: -0.01397562026977539
Batch 62/64 loss: -0.0015224814414978027
Batch 63/64 loss: -0.006295144557952881
Batch 64/64 loss: -0.005957603454589844
Epoch 74  Train loss: 0.002191774517882104  Val loss: 0.022739221959589273
Saving best model, epoch: 74
Epoch 75
-------------------------------
Batch 1/64 loss: 0.019792795181274414
Batch 2/64 loss: -0.015481770038604736
Batch 3/64 loss: 0.01461338996887207
Batch 4/64 loss: 0.009983360767364502
Batch 5/64 loss: 0.007125675678253174
Batch 6/64 loss: 0.001797318458557129
Batch 7/64 loss: -0.004598736763000488
Batch 8/64 loss: -0.01220083236694336
Batch 9/64 loss: 0.008127689361572266
Batch 10/64 loss: 0.009170114994049072
Batch 11/64 loss: -0.01049274206161499
Batch 12/64 loss: 0.03548252582550049
Batch 13/64 loss: 0.020124614238739014
Batch 14/64 loss: -0.005659162998199463
Batch 15/64 loss: 0.00638502836227417
Batch 16/64 loss: 0.0061185359954833984
Batch 17/64 loss: 0.017445921897888184
Batch 18/64 loss: 0.01678234338760376
Batch 19/64 loss: -0.00872105360031128
Batch 20/64 loss: 0.001733541488647461
Batch 21/64 loss: 0.0032142996788024902
Batch 22/64 loss: 0.002377808094024658
Batch 23/64 loss: 0.007660567760467529
Batch 24/64 loss: -0.0041201114654541016
Batch 25/64 loss: -7.975101470947266e-05
Batch 26/64 loss: -0.0028157830238342285
Batch 27/64 loss: 0.00670313835144043
Batch 28/64 loss: 0.0007842183113098145
Batch 29/64 loss: 0.011540114879608154
Batch 30/64 loss: 0.008979856967926025
Batch 31/64 loss: 0.0013177990913391113
Batch 32/64 loss: -0.012046217918395996
Batch 33/64 loss: 0.004620015621185303
Batch 34/64 loss: 0.002140522003173828
Batch 35/64 loss: -0.0075342655181884766
Batch 36/64 loss: -0.026282668113708496
Batch 37/64 loss: -0.0011351704597473145
Batch 38/64 loss: -0.008097171783447266
Batch 39/64 loss: -0.004366874694824219
Batch 40/64 loss: -0.010673105716705322
Batch 41/64 loss: -0.021900713443756104
Batch 42/64 loss: -0.004200339317321777
Batch 43/64 loss: 0.011511027812957764
Batch 44/64 loss: 0.0024018287658691406
Batch 45/64 loss: -0.013177156448364258
Batch 46/64 loss: 0.005348384380340576
Batch 47/64 loss: -0.008941113948822021
Batch 48/64 loss: 0.004291534423828125
Batch 49/64 loss: -0.010468840599060059
Batch 50/64 loss: -0.016671419143676758
Batch 51/64 loss: 0.0027810335159301758
Batch 52/64 loss: 0.010798215866088867
Batch 53/64 loss: 0.001013636589050293
Batch 54/64 loss: -0.01010739803314209
Batch 55/64 loss: -0.00904613733291626
Batch 56/64 loss: 0.00048601627349853516
Batch 57/64 loss: 0.0023841261863708496
Batch 58/64 loss: -0.027954697608947754
Batch 59/64 loss: -0.017561137676239014
Batch 60/64 loss: 0.01203697919845581
Batch 61/64 loss: 0.0053192973136901855
Batch 62/64 loss: -0.015609681606292725
Batch 63/64 loss: -0.019039392471313477
Batch 64/64 loss: -0.02184128761291504
Epoch 75  Train loss: -0.0006740570068359375  Val loss: 0.03060033722841453
Epoch 76
-------------------------------
Batch 1/64 loss: -0.008781015872955322
Batch 2/64 loss: -0.006208658218383789
Batch 3/64 loss: -0.010389149188995361
Batch 4/64 loss: -0.004106581211090088
Batch 5/64 loss: -0.02574235200881958
Batch 6/64 loss: -0.0059642791748046875
Batch 7/64 loss: -0.012701272964477539
Batch 8/64 loss: -0.0129164457321167
Batch 9/64 loss: 0.0183144211769104
Batch 10/64 loss: -0.016322553157806396
Batch 11/64 loss: 0.009304285049438477
Batch 12/64 loss: 0.017961442470550537
Batch 13/64 loss: 0.007376909255981445
Batch 14/64 loss: -0.02137845754623413
Batch 15/64 loss: 0.0006859898567199707
Batch 16/64 loss: 0.005719900131225586
Batch 17/64 loss: -0.0058065056800842285
Batch 18/64 loss: -0.002902686595916748
Batch 19/64 loss: -0.008910596370697021
Batch 20/64 loss: -0.0062329769134521484
Batch 21/64 loss: -0.01354891061782837
Batch 22/64 loss: -0.0036083459854125977
Batch 23/64 loss: 0.007832825183868408
Batch 24/64 loss: -0.0005750656127929688
Batch 25/64 loss: -0.0050664544105529785
Batch 26/64 loss: -0.007259845733642578
Batch 27/64 loss: 0.028415977954864502
Batch 28/64 loss: -0.02076774835586548
Batch 29/64 loss: -0.01149284839630127
Batch 30/64 loss: -0.015772700309753418
Batch 31/64 loss: -0.0009611845016479492
Batch 32/64 loss: -0.004100799560546875
Batch 33/64 loss: -0.004979252815246582
Batch 34/64 loss: -0.0077342987060546875
Batch 35/64 loss: -0.004686295986175537
Batch 36/64 loss: -0.010886788368225098
Batch 37/64 loss: -0.0009534358978271484
Batch 38/64 loss: -0.0033549070358276367
Batch 39/64 loss: 0.0008352994918823242
Batch 40/64 loss: -0.005760073661804199
Batch 41/64 loss: 0.021992266178131104
Batch 42/64 loss: -0.009926080703735352
Batch 43/64 loss: 0.018004179000854492
Batch 44/64 loss: 0.00162506103515625
Batch 45/64 loss: -0.009005188941955566
Batch 46/64 loss: -0.007265925407409668
Batch 47/64 loss: -0.012769639492034912
Batch 48/64 loss: -0.007593333721160889
Batch 49/64 loss: 0.0022642016410827637
Batch 50/64 loss: 0.013227343559265137
Batch 51/64 loss: 0.001286923885345459
Batch 52/64 loss: -0.004858851432800293
Batch 53/64 loss: -0.0034207701683044434
Batch 54/64 loss: -0.01226663589477539
Batch 55/64 loss: -0.004270970821380615
Batch 56/64 loss: -0.012500762939453125
Batch 57/64 loss: -0.002343297004699707
Batch 58/64 loss: -0.00023746490478515625
Batch 59/64 loss: -0.020188212394714355
Batch 60/64 loss: 0.003158748149871826
Batch 61/64 loss: -0.008568406105041504
Batch 62/64 loss: -0.009627044200897217
Batch 63/64 loss: 0.002224147319793701
Batch 64/64 loss: -0.006782650947570801
Epoch 76  Train loss: -0.0037579943152034985  Val loss: 0.02439374776230645
Epoch 77
-------------------------------
Batch 1/64 loss: 0.0047563910484313965
Batch 2/64 loss: -0.014620184898376465
Batch 3/64 loss: -0.001140594482421875
Batch 4/64 loss: -0.03443098068237305
Batch 5/64 loss: -0.008388221263885498
Batch 6/64 loss: -0.009110033512115479
Batch 7/64 loss: -0.013101577758789062
Batch 8/64 loss: -0.013359785079956055
Batch 9/64 loss: 0.000882267951965332
Batch 10/64 loss: -0.013080894947052002
Batch 11/64 loss: -0.005156219005584717
Batch 12/64 loss: -0.01756584644317627
Batch 13/64 loss: -0.0212782621383667
Batch 14/64 loss: -0.0006295442581176758
Batch 15/64 loss: 0.007904767990112305
Batch 16/64 loss: -0.009151279926300049
Batch 17/64 loss: -0.00989001989364624
Batch 18/64 loss: -0.003323495388031006
Batch 19/64 loss: -0.018769681453704834
Batch 20/64 loss: 0.013878226280212402
Batch 21/64 loss: -0.009046792984008789
Batch 22/64 loss: -0.02911931276321411
Batch 23/64 loss: -0.0037303566932678223
Batch 24/64 loss: 0.012160420417785645
Batch 25/64 loss: 0.00028955936431884766
Batch 26/64 loss: -0.02327132225036621
Batch 27/64 loss: -0.021245121955871582
Batch 28/64 loss: 0.019208967685699463
Batch 29/64 loss: 0.008021414279937744
Batch 30/64 loss: 0.012974381446838379
Batch 31/64 loss: 0.008351922035217285
Batch 32/64 loss: -0.003382563591003418
Batch 33/64 loss: 0.020380616188049316
Batch 34/64 loss: 0.004457950592041016
Batch 35/64 loss: 0.011762619018554688
Batch 36/64 loss: -0.010243713855743408
Batch 37/64 loss: 0.0013289451599121094
Batch 38/64 loss: -0.0023630857467651367
Batch 39/64 loss: -0.0015814900398254395
Batch 40/64 loss: -0.009363412857055664
Batch 41/64 loss: -0.020039141178131104
Batch 42/64 loss: -0.025039255619049072
Batch 43/64 loss: 0.004200875759124756
Batch 44/64 loss: -0.0023018717765808105
Batch 45/64 loss: 0.017183542251586914
Batch 46/64 loss: 0.028249025344848633
Batch 47/64 loss: -0.011281728744506836
Batch 48/64 loss: 0.003443300724029541
Batch 49/64 loss: -0.005192160606384277
Batch 50/64 loss: 0.005107522010803223
Batch 51/64 loss: -0.00790172815322876
Batch 52/64 loss: 0.0011181235313415527
Batch 53/64 loss: -0.02429676055908203
Batch 54/64 loss: -0.0041217803955078125
Batch 55/64 loss: -0.0248105525970459
Batch 56/64 loss: 0.01784670352935791
Batch 57/64 loss: -0.0077950358390808105
Batch 58/64 loss: 0.0052449703216552734
Batch 59/64 loss: -0.019704222679138184
Batch 60/64 loss: -0.004596590995788574
Batch 61/64 loss: 0.0021859407424926758
Batch 62/64 loss: -0.00044214725494384766
Batch 63/64 loss: -0.008652687072753906
Batch 64/64 loss: -0.012949764728546143
Epoch 77  Train loss: -0.004255581603330724  Val loss: 0.021882482001052278
Saving best model, epoch: 77
Epoch 78
-------------------------------
Batch 1/64 loss: -0.0290260910987854
Batch 2/64 loss: -0.02131044864654541
Batch 3/64 loss: -0.03920990228652954
Batch 4/64 loss: -0.007609844207763672
Batch 5/64 loss: -0.01473534107208252
Batch 6/64 loss: 0.012883305549621582
Batch 7/64 loss: -0.015810728073120117
Batch 8/64 loss: 0.010877430438995361
Batch 9/64 loss: -0.013537883758544922
Batch 10/64 loss: -0.021619796752929688
Batch 11/64 loss: -0.01554948091506958
Batch 12/64 loss: -0.0016179084777832031
Batch 13/64 loss: -0.005212068557739258
Batch 14/64 loss: -0.020986497402191162
Batch 15/64 loss: -0.015604615211486816
Batch 16/64 loss: 0.002339959144592285
Batch 17/64 loss: 0.004779934883117676
Batch 18/64 loss: 0.012511909008026123
Batch 19/64 loss: 0.026904821395874023
Batch 20/64 loss: 0.01641303300857544
Batch 21/64 loss: 0.003533005714416504
Batch 22/64 loss: 0.018264710903167725
Batch 23/64 loss: 0.010556936264038086
Batch 24/64 loss: 0.0020020008087158203
Batch 25/64 loss: -0.0048449039459228516
Batch 26/64 loss: 0.0032626986503601074
Batch 27/64 loss: 0.016755759716033936
Batch 28/64 loss: 0.0003472566604614258
Batch 29/64 loss: 0.0025211572647094727
Batch 30/64 loss: -0.006867587566375732
Batch 31/64 loss: -0.01104283332824707
Batch 32/64 loss: 0.013781070709228516
Batch 33/64 loss: -0.0018180608749389648
Batch 34/64 loss: -0.0019762516021728516
Batch 35/64 loss: 0.0053653717041015625
Batch 36/64 loss: 0.004894912242889404
Batch 37/64 loss: -0.005476832389831543
Batch 38/64 loss: -0.004770934581756592
Batch 39/64 loss: 0.01031959056854248
Batch 40/64 loss: 0.017233848571777344
Batch 41/64 loss: -0.002396225929260254
Batch 42/64 loss: -0.00025177001953125
Batch 43/64 loss: -0.004071652889251709
Batch 44/64 loss: -0.009832262992858887
Batch 45/64 loss: -0.011487066745758057
Batch 46/64 loss: -0.01816803216934204
Batch 47/64 loss: -0.005280911922454834
Batch 48/64 loss: -0.006727874279022217
Batch 49/64 loss: -0.004658043384552002
Batch 50/64 loss: -0.002171754837036133
Batch 51/64 loss: -0.01391589641571045
Batch 52/64 loss: -0.006256580352783203
Batch 53/64 loss: -0.010162889957427979
Batch 54/64 loss: 0.006011962890625
Batch 55/64 loss: -0.012817084789276123
Batch 56/64 loss: -0.003365457057952881
Batch 57/64 loss: -0.015331149101257324
Batch 58/64 loss: -0.018348872661590576
Batch 59/64 loss: 0.00022560358047485352
Batch 60/64 loss: -0.015094399452209473
Batch 61/64 loss: -0.0001283884048461914
Batch 62/64 loss: -0.012701749801635742
Batch 63/64 loss: -0.02316868305206299
Batch 64/64 loss: -0.0015017390251159668
Epoch 78  Train loss: -0.0039890946126451676  Val loss: 0.02076338164994807
Saving best model, epoch: 78
Epoch 79
-------------------------------
Batch 1/64 loss: -0.008906066417694092
Batch 2/64 loss: -0.005135476589202881
Batch 3/64 loss: -0.0007230639457702637
Batch 4/64 loss: 0.003292560577392578
Batch 5/64 loss: -0.0047245025634765625
Batch 6/64 loss: -0.013086915016174316
Batch 7/64 loss: -0.019530534744262695
Batch 8/64 loss: -0.022870123386383057
Batch 9/64 loss: 0.0019975900650024414
Batch 10/64 loss: -0.02350074052810669
Batch 11/64 loss: 0.005991816520690918
Batch 12/64 loss: -0.00320279598236084
Batch 13/64 loss: -0.016917526721954346
Batch 14/64 loss: -0.002514362335205078
Batch 15/64 loss: -0.02196592092514038
Batch 16/64 loss: -0.013189852237701416
Batch 17/64 loss: -0.01539158821105957
Batch 18/64 loss: -0.002115786075592041
Batch 19/64 loss: -0.0038518309593200684
Batch 20/64 loss: 0.010342538356781006
Batch 21/64 loss: -0.006910085678100586
Batch 22/64 loss: 0.009296655654907227
Batch 23/64 loss: -0.014467418193817139
Batch 24/64 loss: 0.017216622829437256
Batch 25/64 loss: 0.0010681748390197754
Batch 26/64 loss: 2.2411346435546875e-05
Batch 27/64 loss: -0.013570189476013184
Batch 28/64 loss: -0.02387481927871704
Batch 29/64 loss: -0.008215904235839844
Batch 30/64 loss: -0.0064476728439331055
Batch 31/64 loss: -0.006238341331481934
Batch 32/64 loss: -0.008584856986999512
Batch 33/64 loss: -0.00799793004989624
Batch 34/64 loss: -0.011662900447845459
Batch 35/64 loss: 0.004882872104644775
Batch 36/64 loss: -0.023841559886932373
Batch 37/64 loss: 0.011838972568511963
Batch 38/64 loss: -0.011204063892364502
Batch 39/64 loss: -0.004652261734008789
Batch 40/64 loss: 0.00608217716217041
Batch 41/64 loss: 0.015949904918670654
Batch 42/64 loss: -0.020701348781585693
Batch 43/64 loss: 0.006210505962371826
Batch 44/64 loss: 0.0070983171463012695
Batch 45/64 loss: -0.007954418659210205
Batch 46/64 loss: 0.015555977821350098
Batch 47/64 loss: -0.016609489917755127
Batch 48/64 loss: 0.01612609624862671
Batch 49/64 loss: -0.009789526462554932
Batch 50/64 loss: -0.026906132698059082
Batch 51/64 loss: -0.008264422416687012
Batch 52/64 loss: 0.009304046630859375
Batch 53/64 loss: -0.021433591842651367
Batch 54/64 loss: -0.01520693302154541
Batch 55/64 loss: -0.013015151023864746
Batch 56/64 loss: -0.027324974536895752
Batch 57/64 loss: -0.00893700122833252
Batch 58/64 loss: -0.012458860874176025
Batch 59/64 loss: -0.013939321041107178
Batch 60/64 loss: -0.00654911994934082
Batch 61/64 loss: -0.015593886375427246
Batch 62/64 loss: -0.02584230899810791
Batch 63/64 loss: 0.020599663257598877
Batch 64/64 loss: 0.0002676248550415039
Epoch 79  Train loss: -0.006474414993734921  Val loss: 0.021716582201600484
Epoch 80
-------------------------------
Batch 1/64 loss: -0.017614662647247314
Batch 2/64 loss: 0.0003838539123535156
Batch 3/64 loss: 0.009205281734466553
Batch 4/64 loss: -0.009327352046966553
Batch 5/64 loss: 0.0027587413787841797
Batch 6/64 loss: -0.00948488712310791
Batch 7/64 loss: 0.015426158905029297
Batch 8/64 loss: -0.01499330997467041
Batch 9/64 loss: -0.0014309883117675781
Batch 10/64 loss: -0.020450949668884277
Batch 11/64 loss: 0.00013184547424316406
Batch 12/64 loss: 0.008551299571990967
Batch 13/64 loss: -0.020352303981781006
Batch 14/64 loss: 0.004031956195831299
Batch 15/64 loss: -0.029237031936645508
Batch 16/64 loss: -0.012228667736053467
Batch 17/64 loss: -0.012400627136230469
Batch 18/64 loss: -0.011421740055084229
Batch 19/64 loss: -0.0004881620407104492
Batch 20/64 loss: -0.0038187503814697266
Batch 21/64 loss: -0.016842305660247803
Batch 22/64 loss: 0.020537912845611572
Batch 23/64 loss: -0.03439825773239136
Batch 24/64 loss: -0.010634303092956543
Batch 25/64 loss: -0.00046193599700927734
Batch 26/64 loss: -0.004872798919677734
Batch 27/64 loss: -0.01794910430908203
Batch 28/64 loss: -0.018352746963500977
Batch 29/64 loss: -0.013320446014404297
Batch 30/64 loss: -0.011435925960540771
Batch 31/64 loss: -0.015867948532104492
Batch 32/64 loss: -0.01879870891571045
Batch 33/64 loss: 0.0017824769020080566
Batch 34/64 loss: 0.006888329982757568
Batch 35/64 loss: 0.015900254249572754
Batch 36/64 loss: 0.007921934127807617
Batch 37/64 loss: -0.0008446574211120605
Batch 38/64 loss: -0.01729494333267212
Batch 39/64 loss: -0.017551124095916748
Batch 40/64 loss: -0.005485713481903076
Batch 41/64 loss: -0.018844664096832275
Batch 42/64 loss: 0.0018432140350341797
Batch 43/64 loss: 0.009925127029418945
Batch 44/64 loss: -0.02205336093902588
Batch 45/64 loss: -0.007507979869842529
Batch 46/64 loss: 0.009266793727874756
Batch 47/64 loss: -0.010922253131866455
Batch 48/64 loss: -0.018283963203430176
Batch 49/64 loss: -0.021351754665374756
Batch 50/64 loss: 0.018736720085144043
Batch 51/64 loss: 0.016240060329437256
Batch 52/64 loss: -0.012163758277893066
Batch 53/64 loss: -0.015911757946014404
Batch 54/64 loss: -0.01754283905029297
Batch 55/64 loss: -0.005014538764953613
Batch 56/64 loss: 0.0006372332572937012
Batch 57/64 loss: -0.01237398386001587
Batch 58/64 loss: 0.002031862735748291
Batch 59/64 loss: -0.0026624202728271484
Batch 60/64 loss: -0.020872116088867188
Batch 61/64 loss: -0.023192882537841797
Batch 62/64 loss: -0.0066484808921813965
Batch 63/64 loss: -0.013020515441894531
Batch 64/64 loss: -0.02266073226928711
Epoch 80  Train loss: -0.007223876317342123  Val loss: 0.029282474435891482
Epoch 81
-------------------------------
Batch 1/64 loss: 0.004577517509460449
Batch 2/64 loss: -0.01693040132522583
Batch 3/64 loss: -0.01989114284515381
Batch 4/64 loss: -0.028674542903900146
Batch 5/64 loss: -0.01366490125656128
Batch 6/64 loss: -0.011558473110198975
Batch 7/64 loss: -0.02567589282989502
Batch 8/64 loss: 0.009848952293395996
Batch 9/64 loss: -0.005490422248840332
Batch 10/64 loss: -0.011771321296691895
Batch 11/64 loss: 0.004299163818359375
Batch 12/64 loss: -0.004899024963378906
Batch 13/64 loss: -0.011312603950500488
Batch 14/64 loss: -0.008908629417419434
Batch 15/64 loss: -0.03244984149932861
Batch 16/64 loss: -0.0072838664054870605
Batch 17/64 loss: 0.0028339624404907227
Batch 18/64 loss: -0.015061259269714355
Batch 19/64 loss: -0.011623561382293701
Batch 20/64 loss: -0.020103394985198975
Batch 21/64 loss: 0.0010829567909240723
Batch 22/64 loss: -0.0010316967964172363
Batch 23/64 loss: -0.01737511157989502
Batch 24/64 loss: -0.0029681921005249023
Batch 25/64 loss: 0.0023981332778930664
Batch 26/64 loss: 0.009564340114593506
Batch 27/64 loss: -0.011018693447113037
Batch 28/64 loss: -0.00016385316848754883
Batch 29/64 loss: 0.0034432411193847656
Batch 30/64 loss: -0.0008847713470458984
Batch 31/64 loss: -0.002603769302368164
Batch 32/64 loss: 0.0191916823387146
Batch 33/64 loss: -0.006177425384521484
Batch 34/64 loss: -0.018777906894683838
Batch 35/64 loss: -0.0027732253074645996
Batch 36/64 loss: -0.012224853038787842
Batch 37/64 loss: -0.022100448608398438
Batch 38/64 loss: 0.005193471908569336
Batch 39/64 loss: -0.017398416996002197
Batch 40/64 loss: -0.01770883798599243
Batch 41/64 loss: 0.011131823062896729
Batch 42/64 loss: -0.02124994993209839
Batch 43/64 loss: -0.011410951614379883
Batch 44/64 loss: -0.00974428653717041
Batch 45/64 loss: -0.0006492733955383301
Batch 46/64 loss: -0.0018031001091003418
Batch 47/64 loss: -0.0212554931640625
Batch 48/64 loss: -0.0010074973106384277
Batch 49/64 loss: 0.00033020973205566406
Batch 50/64 loss: -0.0004686713218688965
Batch 51/64 loss: -0.003932356834411621
Batch 52/64 loss: -0.014289796352386475
Batch 53/64 loss: 0.005510449409484863
Batch 54/64 loss: -0.008983314037322998
Batch 55/64 loss: 0.01022714376449585
Batch 56/64 loss: -0.026153147220611572
Batch 57/64 loss: -0.005646824836730957
Batch 58/64 loss: -0.001759648323059082
Batch 59/64 loss: 0.0007497072219848633
Batch 60/64 loss: 0.003473043441772461
Batch 61/64 loss: -0.006388843059539795
Batch 62/64 loss: -0.007509708404541016
Batch 63/64 loss: 0.0018513202667236328
Batch 64/64 loss: -0.018127143383026123
Epoch 81  Train loss: -0.006880746401992498  Val loss: 0.034264611624360494
Epoch 82
-------------------------------
Batch 1/64 loss: -0.01046079397201538
Batch 2/64 loss: 0.003967583179473877
Batch 3/64 loss: -0.013074994087219238
Batch 4/64 loss: -0.0095444917678833
Batch 5/64 loss: 0.0033326148986816406
Batch 6/64 loss: -0.013320446014404297
Batch 7/64 loss: -0.006824493408203125
Batch 8/64 loss: -0.015361428260803223
Batch 9/64 loss: -0.007929682731628418
Batch 10/64 loss: 0.0049359798431396484
Batch 11/64 loss: 0.03740638494491577
Batch 12/64 loss: -0.0018009543418884277
Batch 13/64 loss: -0.00902777910232544
Batch 14/64 loss: -0.027028024196624756
Batch 15/64 loss: -0.01420283317565918
Batch 16/64 loss: -0.02379363775253296
Batch 17/64 loss: -0.007353603839874268
Batch 18/64 loss: -0.001725614070892334
Batch 19/64 loss: -0.0018308162689208984
Batch 20/64 loss: -0.013671815395355225
Batch 21/64 loss: -0.015183329582214355
Batch 22/64 loss: 0.007415115833282471
Batch 23/64 loss: -0.02589517831802368
Batch 24/64 loss: -0.020167112350463867
Batch 25/64 loss: -0.024128258228302002
Batch 26/64 loss: -0.006048381328582764
Batch 27/64 loss: -0.026465654373168945
Batch 28/64 loss: 0.0038380026817321777
Batch 29/64 loss: -0.018622875213623047
Batch 30/64 loss: -0.0029715299606323242
Batch 31/64 loss: -0.016278624534606934
Batch 32/64 loss: 0.01103353500366211
Batch 33/64 loss: 0.001658022403717041
Batch 34/64 loss: -0.011325657367706299
Batch 35/64 loss: -0.01320350170135498
Batch 36/64 loss: -0.010802686214447021
Batch 37/64 loss: -0.008369922637939453
Batch 38/64 loss: 0.012656450271606445
Batch 39/64 loss: -0.03093010187149048
Batch 40/64 loss: -0.0018374323844909668
Batch 41/64 loss: -0.000997304916381836
Batch 42/64 loss: 0.013919472694396973
Batch 43/64 loss: 0.006015598773956299
Batch 44/64 loss: -0.005388796329498291
Batch 45/64 loss: -0.009715676307678223
Batch 46/64 loss: 0.0013474225997924805
Batch 47/64 loss: -0.004076957702636719
Batch 48/64 loss: -0.004189133644104004
Batch 49/64 loss: 0.0061588287353515625
Batch 50/64 loss: 0.007176220417022705
Batch 51/64 loss: -0.019474387168884277
Batch 52/64 loss: -0.006589055061340332
Batch 53/64 loss: 0.01503080129623413
Batch 54/64 loss: -0.0053147077560424805
Batch 55/64 loss: -0.006532251834869385
Batch 56/64 loss: -0.009082138538360596
Batch 57/64 loss: -0.015832602977752686
Batch 58/64 loss: -0.016274094581604004
Batch 59/64 loss: 0.004315197467803955
Batch 60/64 loss: -0.011163115501403809
Batch 61/64 loss: -0.005420684814453125
Batch 62/64 loss: -0.006095707416534424
Batch 63/64 loss: -0.028028249740600586
Batch 64/64 loss: -0.006627559661865234
Epoch 82  Train loss: -0.006715607175640031  Val loss: 0.02798468857696376
Epoch 83
-------------------------------
Batch 1/64 loss: -0.024927735328674316
Batch 2/64 loss: 0.00033485889434814453
Batch 3/64 loss: -0.031698405742645264
Batch 4/64 loss: -0.011876940727233887
Batch 5/64 loss: -0.019786834716796875
Batch 6/64 loss: -0.04463130235671997
Batch 7/64 loss: -0.013968706130981445
Batch 8/64 loss: 0.02945244312286377
Batch 9/64 loss: 0.0029187798500061035
Batch 10/64 loss: -0.019630253314971924
Batch 11/64 loss: 0.006473422050476074
Batch 12/64 loss: -0.014022886753082275
Batch 13/64 loss: -0.02264183759689331
Batch 14/64 loss: -0.019510090351104736
Batch 15/64 loss: -0.02047896385192871
Batch 16/64 loss: -0.012732148170471191
Batch 17/64 loss: -0.012418031692504883
Batch 18/64 loss: -0.024514377117156982
Batch 19/64 loss: -0.013292014598846436
Batch 20/64 loss: -0.022413969039916992
Batch 21/64 loss: -0.003954946994781494
Batch 22/64 loss: -0.026088356971740723
Batch 23/64 loss: -0.006099700927734375
Batch 24/64 loss: -0.011626362800598145
Batch 25/64 loss: -0.0034008026123046875
Batch 26/64 loss: -0.022755086421966553
Batch 27/64 loss: 0.0065888166427612305
Batch 28/64 loss: 0.013814389705657959
Batch 29/64 loss: 0.002778291702270508
Batch 30/64 loss: -0.0014463067054748535
Batch 31/64 loss: -0.015196681022644043
Batch 32/64 loss: -0.006799280643463135
Batch 33/64 loss: -0.030655860900878906
Batch 34/64 loss: -0.015393495559692383
Batch 35/64 loss: -0.013189315795898438
Batch 36/64 loss: 0.007337212562561035
Batch 37/64 loss: -0.010752558708190918
Batch 38/64 loss: -0.008120477199554443
Batch 39/64 loss: 0.0034524202346801758
Batch 40/64 loss: -0.0100669264793396
Batch 41/64 loss: -0.027413129806518555
Batch 42/64 loss: -0.020384669303894043
Batch 43/64 loss: -0.014499783515930176
Batch 44/64 loss: -0.028711378574371338
Batch 45/64 loss: 0.015090346336364746
Batch 46/64 loss: -0.03645658493041992
Batch 47/64 loss: -0.008208632469177246
Batch 48/64 loss: -0.00210493803024292
Batch 49/64 loss: 0.013095974922180176
Batch 50/64 loss: -0.010635018348693848
Batch 51/64 loss: -0.02241683006286621
Batch 52/64 loss: -0.007308244705200195
Batch 53/64 loss: 0.00722116231918335
Batch 54/64 loss: 0.0061522722244262695
Batch 55/64 loss: 0.009386301040649414
Batch 56/64 loss: -0.019451916217803955
Batch 57/64 loss: -0.00598520040512085
Batch 58/64 loss: 0.0009119510650634766
Batch 59/64 loss: -0.004463553428649902
Batch 60/64 loss: -0.021590769290924072
Batch 61/64 loss: -0.009220302104949951
Batch 62/64 loss: 0.008105337619781494
Batch 63/64 loss: -0.023244261741638184
Batch 64/64 loss: -0.0068032145500183105
Epoch 83  Train loss: -0.010167440012389539  Val loss: 0.04092505420606161
Epoch 84
-------------------------------
Batch 1/64 loss: -0.02290433645248413
Batch 2/64 loss: -0.017467021942138672
Batch 3/64 loss: 0.003991067409515381
Batch 4/64 loss: -0.002491772174835205
Batch 5/64 loss: -0.019307315349578857
Batch 6/64 loss: 0.004579067230224609
Batch 7/64 loss: -0.030077338218688965
Batch 8/64 loss: -0.0045815110206604
Batch 9/64 loss: -0.019057512283325195
Batch 10/64 loss: -0.013031244277954102
Batch 11/64 loss: -0.017594337463378906
Batch 12/64 loss: -0.01622539758682251
Batch 13/64 loss: -0.0067708492279052734
Batch 14/64 loss: -0.00858086347579956
Batch 15/64 loss: -0.012839257717132568
Batch 16/64 loss: -0.003143012523651123
Batch 17/64 loss: -0.012187719345092773
Batch 18/64 loss: -0.030004024505615234
Batch 19/64 loss: -0.006118953227996826
Batch 20/64 loss: -0.0191270112991333
Batch 21/64 loss: -0.001041710376739502
Batch 22/64 loss: -0.009286999702453613
Batch 23/64 loss: -0.021677911281585693
Batch 24/64 loss: -0.007439851760864258
Batch 25/64 loss: -0.013918876647949219
Batch 26/64 loss: -0.016121208667755127
Batch 27/64 loss: -0.001788318157196045
Batch 28/64 loss: -0.009452879428863525
Batch 29/64 loss: -0.032364845275878906
Batch 30/64 loss: 0.00593876838684082
Batch 31/64 loss: -0.023319482803344727
Batch 32/64 loss: -0.010774016380310059
Batch 33/64 loss: -0.0036855340003967285
Batch 34/64 loss: -0.007977545261383057
Batch 35/64 loss: -0.007504642009735107
Batch 36/64 loss: -0.0063353776931762695
Batch 37/64 loss: -0.015283584594726562
Batch 38/64 loss: -0.0252840518951416
Batch 39/64 loss: -0.015539765357971191
Batch 40/64 loss: -0.008638620376586914
Batch 41/64 loss: -0.022275567054748535
Batch 42/64 loss: -0.0173148512840271
Batch 43/64 loss: -0.006352782249450684
Batch 44/64 loss: 0.021231412887573242
Batch 45/64 loss: 0.009452223777770996
Batch 46/64 loss: -0.017300784587860107
Batch 47/64 loss: -0.008270978927612305
Batch 48/64 loss: -0.017621934413909912
Batch 49/64 loss: -0.004171490669250488
Batch 50/64 loss: 0.009578704833984375
Batch 51/64 loss: -0.019956767559051514
Batch 52/64 loss: -0.002695322036743164
Batch 53/64 loss: -0.019258975982666016
Batch 54/64 loss: -0.007875025272369385
Batch 55/64 loss: -0.011216163635253906
Batch 56/64 loss: -0.014761269092559814
Batch 57/64 loss: -0.0199429988861084
Batch 58/64 loss: -0.0027785301208496094
Batch 59/64 loss: 0.01555168628692627
Batch 60/64 loss: -0.008217036724090576
Batch 61/64 loss: 0.0020977258682250977
Batch 62/64 loss: -0.0057236552238464355
Batch 63/64 loss: 0.007303357124328613
Batch 64/64 loss: 0.016103804111480713
Epoch 84  Train loss: -0.00961375633875529  Val loss: 0.02522021204335583
Epoch 85
-------------------------------
Batch 1/64 loss: -0.004365503787994385
Batch 2/64 loss: -0.010310232639312744
Batch 3/64 loss: -0.014271020889282227
Batch 4/64 loss: 0.009169042110443115
Batch 5/64 loss: -0.0169217586517334
Batch 6/64 loss: -0.02171987295150757
Batch 7/64 loss: -0.0014513731002807617
Batch 8/64 loss: -0.00022876262664794922
Batch 9/64 loss: -0.013573229312896729
Batch 10/64 loss: -0.0063089728355407715
Batch 11/64 loss: -0.00548863410949707
Batch 12/64 loss: -0.023351550102233887
Batch 13/64 loss: -0.013324081897735596
Batch 14/64 loss: 0.019805967807769775
Batch 15/64 loss: 0.0030676722526550293
Batch 16/64 loss: -0.002788841724395752
Batch 17/64 loss: -0.01084369421005249
Batch 18/64 loss: -0.004312872886657715
Batch 19/64 loss: -0.011392414569854736
Batch 20/64 loss: 0.005820274353027344
Batch 21/64 loss: -0.017674386501312256
Batch 22/64 loss: -0.023994505405426025
Batch 23/64 loss: 0.007095515727996826
Batch 24/64 loss: -0.014926314353942871
Batch 25/64 loss: -6.401538848876953e-05
Batch 26/64 loss: 0.005247592926025391
Batch 27/64 loss: 0.0007548928260803223
Batch 28/64 loss: -0.0048021674156188965
Batch 29/64 loss: -0.006333649158477783
Batch 30/64 loss: -0.020247697830200195
Batch 31/64 loss: -0.009936273097991943
Batch 32/64 loss: 0.00012564659118652344
Batch 33/64 loss: -0.00653153657913208
Batch 34/64 loss: -0.010999798774719238
Batch 35/64 loss: -0.003224313259124756
Batch 36/64 loss: -0.01455456018447876
Batch 37/64 loss: -0.011889815330505371
Batch 38/64 loss: -0.00968170166015625
Batch 39/64 loss: -0.016400694847106934
Batch 40/64 loss: 0.0031505227088928223
Batch 41/64 loss: -0.0018104314804077148
Batch 42/64 loss: -0.023940205574035645
Batch 43/64 loss: -0.008691132068634033
Batch 44/64 loss: -0.012233912944793701
Batch 45/64 loss: -0.015790343284606934
Batch 46/64 loss: -0.034885525703430176
Batch 47/64 loss: 0.010647833347320557
Batch 48/64 loss: -0.02959609031677246
Batch 49/64 loss: -0.011935532093048096
Batch 50/64 loss: -0.02579653263092041
Batch 51/64 loss: -0.012524068355560303
Batch 52/64 loss: -0.00582653284072876
Batch 53/64 loss: -0.0071299076080322266
Batch 54/64 loss: -0.028814971446990967
Batch 55/64 loss: -0.003331601619720459
Batch 56/64 loss: 0.002261042594909668
Batch 57/64 loss: -0.021022021770477295
Batch 58/64 loss: -0.021606802940368652
Batch 59/64 loss: -0.008339881896972656
Batch 60/64 loss: 0.0246770977973938
Batch 61/64 loss: -0.016710937023162842
Batch 62/64 loss: -0.019158661365509033
Batch 63/64 loss: -0.010187149047851562
Batch 64/64 loss: -0.022252261638641357
Epoch 85  Train loss: -0.00903706012987623  Val loss: 0.016030742950046185
Saving best model, epoch: 85
Epoch 86
-------------------------------
Batch 1/64 loss: -0.02420949935913086
Batch 2/64 loss: -0.020861268043518066
Batch 3/64 loss: -0.02903592586517334
Batch 4/64 loss: -0.013586163520812988
Batch 5/64 loss: 0.004914402961730957
Batch 6/64 loss: -0.017508983612060547
Batch 7/64 loss: -0.023961246013641357
Batch 8/64 loss: -0.02079981565475464
Batch 9/64 loss: -0.011164188385009766
Batch 10/64 loss: -0.01616990566253662
Batch 11/64 loss: 0.01848667860031128
Batch 12/64 loss: -0.011457502841949463
Batch 13/64 loss: -0.017583250999450684
Batch 14/64 loss: -0.026407361030578613
Batch 15/64 loss: -0.01357114315032959
Batch 16/64 loss: 0.012041091918945312
Batch 17/64 loss: -0.005146324634552002
Batch 18/64 loss: -0.010785281658172607
Batch 19/64 loss: 0.005898535251617432
Batch 20/64 loss: -0.0018118619918823242
Batch 21/64 loss: -0.004768073558807373
Batch 22/64 loss: 0.012081503868103027
Batch 23/64 loss: -0.011682868003845215
Batch 24/64 loss: -0.012009024620056152
Batch 25/64 loss: -0.009344816207885742
Batch 26/64 loss: -0.013802528381347656
Batch 27/64 loss: -0.0038468241691589355
Batch 28/64 loss: 0.0016043782234191895
Batch 29/64 loss: -0.023188889026641846
Batch 30/64 loss: -0.021922647953033447
Batch 31/64 loss: -0.0073740482330322266
Batch 32/64 loss: -0.018947124481201172
Batch 33/64 loss: -0.027569353580474854
Batch 34/64 loss: -0.020573437213897705
Batch 35/64 loss: -0.02082967758178711
Batch 36/64 loss: -0.024833500385284424
Batch 37/64 loss: -0.009872138500213623
Batch 38/64 loss: -0.022600948810577393
Batch 39/64 loss: -0.025050103664398193
Batch 40/64 loss: -0.009745001792907715
Batch 41/64 loss: -0.015864312648773193
Batch 42/64 loss: -0.006926178932189941
Batch 43/64 loss: -0.028092026710510254
Batch 44/64 loss: -0.03369319438934326
Batch 45/64 loss: -0.00767362117767334
Batch 46/64 loss: 0.009390532970428467
Batch 47/64 loss: -0.0006958246231079102
Batch 48/64 loss: 0.004061102867126465
Batch 49/64 loss: -0.016531705856323242
Batch 50/64 loss: -0.021666765213012695
Batch 51/64 loss: -0.014027655124664307
Batch 52/64 loss: -0.008399486541748047
Batch 53/64 loss: -0.026819288730621338
Batch 54/64 loss: -0.018397808074951172
Batch 55/64 loss: -0.01311802864074707
Batch 56/64 loss: 0.00773167610168457
Batch 57/64 loss: -0.01843118667602539
Batch 58/64 loss: -0.006801009178161621
Batch 59/64 loss: 0.018502235412597656
Batch 60/64 loss: -0.002890467643737793
Batch 61/64 loss: -0.011197924613952637
Batch 62/64 loss: 0.014958441257476807
Batch 63/64 loss: -0.01451045274734497
Batch 64/64 loss: -0.04033321142196655
Epoch 86  Train loss: -0.011581756788141587  Val loss: 0.01781771027345428
Epoch 87
-------------------------------
Batch 1/64 loss: -0.0033113956451416016
Batch 2/64 loss: -0.018196940422058105
Batch 3/64 loss: -0.02305757999420166
Batch 4/64 loss: -0.012250721454620361
Batch 5/64 loss: -0.005664825439453125
Batch 6/64 loss: -0.02411741018295288
Batch 7/64 loss: -0.013776421546936035
Batch 8/64 loss: -0.025110721588134766
Batch 9/64 loss: -0.0298994779586792
Batch 10/64 loss: -0.024471580982208252
Batch 11/64 loss: -0.004299342632293701
Batch 12/64 loss: -0.01860976219177246
Batch 13/64 loss: 0.01298457384109497
Batch 14/64 loss: -0.022037804126739502
Batch 15/64 loss: -0.00807112455368042
Batch 16/64 loss: -0.010601282119750977
Batch 17/64 loss: -0.01109844446182251
Batch 18/64 loss: -0.025497257709503174
Batch 19/64 loss: -0.0356670618057251
Batch 20/64 loss: -0.002021193504333496
Batch 21/64 loss: -0.013928771018981934
Batch 22/64 loss: -0.014144837856292725
Batch 23/64 loss: -0.01605820655822754
Batch 24/64 loss: -0.02366572618484497
Batch 25/64 loss: -0.023047804832458496
Batch 26/64 loss: -0.0009790658950805664
Batch 27/64 loss: -0.006610274314880371
Batch 28/64 loss: -0.01722848415374756
Batch 29/64 loss: -0.0245821475982666
Batch 30/64 loss: -0.009617090225219727
Batch 31/64 loss: 0.012534499168395996
Batch 32/64 loss: -0.02266538143157959
Batch 33/64 loss: -0.0192868709564209
Batch 34/64 loss: -0.005638480186462402
Batch 35/64 loss: -0.03845655918121338
Batch 36/64 loss: -0.020984292030334473
Batch 37/64 loss: -0.009908795356750488
Batch 38/64 loss: 0.004084169864654541
Batch 39/64 loss: -0.005950629711151123
Batch 40/64 loss: -0.0006680488586425781
Batch 41/64 loss: -0.01662391424179077
Batch 42/64 loss: 0.010299980640411377
Batch 43/64 loss: 0.027483224868774414
Batch 44/64 loss: 0.009432196617126465
Batch 45/64 loss: 0.006120741367340088
Batch 46/64 loss: -0.007870197296142578
Batch 47/64 loss: -0.03081613779067993
Batch 48/64 loss: -0.012915432453155518
Batch 49/64 loss: -0.015703022480010986
Batch 50/64 loss: 0.00032263994216918945
Batch 51/64 loss: -0.015243470668792725
Batch 52/64 loss: -0.019818246364593506
Batch 53/64 loss: -0.014110743999481201
Batch 54/64 loss: -0.004374980926513672
Batch 55/64 loss: -0.01435476541519165
Batch 56/64 loss: -0.016679227352142334
Batch 57/64 loss: -0.006977975368499756
Batch 58/64 loss: -0.03323483467102051
Batch 59/64 loss: -0.021508991718292236
Batch 60/64 loss: 0.023436427116394043
Batch 61/64 loss: -0.02486187219619751
Batch 62/64 loss: -0.013967037200927734
Batch 63/64 loss: -0.004761159420013428
Batch 64/64 loss: -0.007089734077453613
Epoch 87  Train loss: -0.011978394844952752  Val loss: 0.012596068923006352
Saving best model, epoch: 87
Epoch 88
-------------------------------
Batch 1/64 loss: -0.01472705602645874
Batch 2/64 loss: -0.01282578706741333
Batch 3/64 loss: -0.009739518165588379
Batch 4/64 loss: -0.029269099235534668
Batch 5/64 loss: -0.0011395812034606934
Batch 6/64 loss: -0.018952667713165283
Batch 7/64 loss: -0.028433263301849365
Batch 8/64 loss: -0.027697980403900146
Batch 9/64 loss: -0.032312631607055664
Batch 10/64 loss: -0.028484761714935303
Batch 11/64 loss: 0.002566039562225342
Batch 12/64 loss: -0.026152491569519043
Batch 13/64 loss: -0.0003045201301574707
Batch 14/64 loss: -0.010384559631347656
Batch 15/64 loss: -0.012769997119903564
Batch 16/64 loss: -0.019751131534576416
Batch 17/64 loss: -0.014313280582427979
Batch 18/64 loss: -0.019367456436157227
Batch 19/64 loss: 0.016194701194763184
Batch 20/64 loss: -0.0009875297546386719
Batch 21/64 loss: -0.01460421085357666
Batch 22/64 loss: -0.0331273078918457
Batch 23/64 loss: -0.01945716142654419
Batch 24/64 loss: -0.02018505334854126
Batch 25/64 loss: -0.004297971725463867
Batch 26/64 loss: -0.023304641246795654
Batch 27/64 loss: -0.03709673881530762
Batch 28/64 loss: -0.006516516208648682
Batch 29/64 loss: -0.03211808204650879
Batch 30/64 loss: -0.004340648651123047
Batch 31/64 loss: -0.017091691493988037
Batch 32/64 loss: -0.015915632247924805
Batch 33/64 loss: -0.016486048698425293
Batch 34/64 loss: -0.031429827213287354
Batch 35/64 loss: 0.0007529258728027344
Batch 36/64 loss: -0.029583871364593506
Batch 37/64 loss: -0.005630135536193848
Batch 38/64 loss: -0.022422850131988525
Batch 39/64 loss: 0.0033090710639953613
Batch 40/64 loss: -0.018662691116333008
Batch 41/64 loss: -0.031071364879608154
Batch 42/64 loss: -0.02409231662750244
Batch 43/64 loss: -0.014575004577636719
Batch 44/64 loss: -0.018541991710662842
Batch 45/64 loss: -0.008737146854400635
Batch 46/64 loss: 0.008096754550933838
Batch 47/64 loss: -0.010312914848327637
Batch 48/64 loss: -0.011689484119415283
Batch 49/64 loss: -0.01986163854598999
Batch 50/64 loss: -0.020059823989868164
Batch 51/64 loss: -0.01659458875656128
Batch 52/64 loss: -0.03002774715423584
Batch 53/64 loss: -0.025804638862609863
Batch 54/64 loss: -0.0011724233627319336
Batch 55/64 loss: 0.00712895393371582
Batch 56/64 loss: -0.009514093399047852
Batch 57/64 loss: 0.012552976608276367
Batch 58/64 loss: -0.001480400562286377
Batch 59/64 loss: -0.002637028694152832
Batch 60/64 loss: 0.02265077829360962
Batch 61/64 loss: -0.022356152534484863
Batch 62/64 loss: 0.0033843517303466797
Batch 63/64 loss: 0.00341188907623291
Batch 64/64 loss: -0.013004302978515625
Epoch 88  Train loss: -0.013460673537908816  Val loss: 0.016915008374505845
Epoch 89
-------------------------------
Batch 1/64 loss: -0.017382562160491943
Batch 2/64 loss: -0.009965121746063232
Batch 3/64 loss: 0.0043790340423583984
Batch 4/64 loss: -0.009462952613830566
Batch 5/64 loss: -0.03405362367630005
Batch 6/64 loss: -0.004470169544219971
Batch 7/64 loss: -0.002548038959503174
Batch 8/64 loss: -0.023604512214660645
Batch 9/64 loss: -0.01690673828125
Batch 10/64 loss: -0.006596207618713379
Batch 11/64 loss: -0.050302863121032715
Batch 12/64 loss: -0.021044909954071045
Batch 13/64 loss: -0.029648542404174805
Batch 14/64 loss: -0.015003681182861328
Batch 15/64 loss: -0.018669307231903076
Batch 16/64 loss: -0.02461034059524536
Batch 17/64 loss: -0.020843029022216797
Batch 18/64 loss: -0.01264655590057373
Batch 19/64 loss: -0.02266538143157959
Batch 20/64 loss: -0.012352705001831055
Batch 21/64 loss: -0.020791590213775635
Batch 22/64 loss: -0.03594398498535156
Batch 23/64 loss: -0.00745546817779541
Batch 24/64 loss: -0.010142385959625244
Batch 25/64 loss: -0.029156506061553955
Batch 26/64 loss: 0.007070004940032959
Batch 27/64 loss: -0.018522202968597412
Batch 28/64 loss: -0.025190234184265137
Batch 29/64 loss: -0.011800765991210938
Batch 30/64 loss: 0.002383708953857422
Batch 31/64 loss: -0.011194765567779541
Batch 32/64 loss: -0.03081655502319336
Batch 33/64 loss: -0.02848684787750244
Batch 34/64 loss: 0.008374154567718506
Batch 35/64 loss: -0.01908397674560547
Batch 36/64 loss: -0.02664649486541748
Batch 37/64 loss: -0.01665860414505005
Batch 38/64 loss: -0.018430113792419434
Batch 39/64 loss: -0.001196742057800293
Batch 40/64 loss: -0.002756059169769287
Batch 41/64 loss: -0.023882031440734863
Batch 42/64 loss: -0.0017656683921813965
Batch 43/64 loss: 0.00019121170043945312
Batch 44/64 loss: -0.01124197244644165
Batch 45/64 loss: -0.007730066776275635
Batch 46/64 loss: -0.016007721424102783
Batch 47/64 loss: -0.003382384777069092
Batch 48/64 loss: -0.008618354797363281
Batch 49/64 loss: -0.0062760114669799805
Batch 50/64 loss: 0.0033324360847473145
Batch 51/64 loss: -0.03437894582748413
Batch 52/64 loss: -0.02122265100479126
Batch 53/64 loss: -0.013621330261230469
Batch 54/64 loss: -0.002126455307006836
Batch 55/64 loss: -0.02496117353439331
Batch 56/64 loss: 0.004213571548461914
Batch 57/64 loss: 0.01707768440246582
Batch 58/64 loss: 0.0076386332511901855
Batch 59/64 loss: -0.005069077014923096
Batch 60/64 loss: 0.005813181400299072
Batch 61/64 loss: -0.020591437816619873
Batch 62/64 loss: -0.017597317695617676
Batch 63/64 loss: 0.030045509338378906
Batch 64/64 loss: 0.01589268445968628
Epoch 89  Train loss: -0.012283678382050757  Val loss: 0.02000774837441461
Epoch 90
-------------------------------
Batch 1/64 loss: -0.035096168518066406
Batch 2/64 loss: -0.017437517642974854
Batch 3/64 loss: 0.007516205310821533
Batch 4/64 loss: -0.010586261749267578
Batch 5/64 loss: 0.01549685001373291
Batch 6/64 loss: -0.015534043312072754
Batch 7/64 loss: -0.01835155487060547
Batch 8/64 loss: -0.02836364507675171
Batch 9/64 loss: -0.022389113903045654
Batch 10/64 loss: 0.008326590061187744
Batch 11/64 loss: -0.01692885160446167
Batch 12/64 loss: -0.03285866975784302
Batch 13/64 loss: -0.02313178777694702
Batch 14/64 loss: -0.021305859088897705
Batch 15/64 loss: -0.0149955153465271
Batch 16/64 loss: -0.007566690444946289
Batch 17/64 loss: 0.003557264804840088
Batch 18/64 loss: -0.004157960414886475
Batch 19/64 loss: -0.01455456018447876
Batch 20/64 loss: -0.019403815269470215
Batch 21/64 loss: -0.02743828296661377
Batch 22/64 loss: -0.006731212139129639
Batch 23/64 loss: 0.0014725327491760254
Batch 24/64 loss: -0.010279297828674316
Batch 25/64 loss: -0.026440024375915527
Batch 26/64 loss: -0.011369824409484863
Batch 27/64 loss: -0.012058019638061523
Batch 28/64 loss: -0.007955670356750488
Batch 29/64 loss: -0.004715561866760254
Batch 30/64 loss: -0.01237022876739502
Batch 31/64 loss: -0.01379770040512085
Batch 32/64 loss: -0.006999373435974121
Batch 33/64 loss: -0.029206395149230957
Batch 34/64 loss: -0.015127241611480713
Batch 35/64 loss: -0.03984940052032471
Batch 36/64 loss: -0.008336007595062256
Batch 37/64 loss: -0.02106785774230957
Batch 38/64 loss: -0.02847456932067871
Batch 39/64 loss: -0.030132174491882324
Batch 40/64 loss: 0.0012537240982055664
Batch 41/64 loss: -0.01586925983428955
Batch 42/64 loss: -0.02815377712249756
Batch 43/64 loss: -0.025081515312194824
Batch 44/64 loss: -0.02493417263031006
Batch 45/64 loss: -0.00704038143157959
Batch 46/64 loss: -0.005742669105529785
Batch 47/64 loss: -0.004775881767272949
Batch 48/64 loss: -0.019752979278564453
Batch 49/64 loss: 0.019112586975097656
Batch 50/64 loss: -0.016555190086364746
Batch 51/64 loss: -0.0030468106269836426
Batch 52/64 loss: -0.012036442756652832
Batch 53/64 loss: -0.020020127296447754
Batch 54/64 loss: -0.007562816143035889
Batch 55/64 loss: -0.01937788724899292
Batch 56/64 loss: -0.009111523628234863
Batch 57/64 loss: -0.017031192779541016
Batch 58/64 loss: -0.008227169513702393
Batch 59/64 loss: -0.022396624088287354
Batch 60/64 loss: -0.005174219608306885
Batch 61/64 loss: -0.006490349769592285
Batch 62/64 loss: -0.021309316158294678
Batch 63/64 loss: -0.01612323522567749
Batch 64/64 loss: -0.030323028564453125
Epoch 90  Train loss: -0.014067935943603515  Val loss: 0.023330932425469467
Epoch 91
-------------------------------
Batch 1/64 loss: -0.0458793044090271
Batch 2/64 loss: -0.006364941596984863
Batch 3/64 loss: 0.012397944927215576
Batch 4/64 loss: -0.034310996532440186
Batch 5/64 loss: -0.009803831577301025
Batch 6/64 loss: -0.025057613849639893
Batch 7/64 loss: 0.002051830291748047
Batch 8/64 loss: -0.010665059089660645
Batch 9/64 loss: -0.017751574516296387
Batch 10/64 loss: -0.012458205223083496
Batch 11/64 loss: -0.029580116271972656
Batch 12/64 loss: -0.014254450798034668
Batch 13/64 loss: -0.01742631196975708
Batch 14/64 loss: -0.00041854381561279297
Batch 15/64 loss: -0.025752663612365723
Batch 16/64 loss: -0.025448918342590332
Batch 17/64 loss: -0.03178006410598755
Batch 18/64 loss: -0.0026270151138305664
Batch 19/64 loss: 0.0012271404266357422
Batch 20/64 loss: -0.017589092254638672
Batch 21/64 loss: -0.022101163864135742
Batch 22/64 loss: -0.02136605978012085
Batch 23/64 loss: -0.025267422199249268
Batch 24/64 loss: -0.03785890340805054
Batch 25/64 loss: -0.015436530113220215
Batch 26/64 loss: -0.02727663516998291
Batch 27/64 loss: -0.035022199153900146
Batch 28/64 loss: -0.021863698959350586
Batch 29/64 loss: 0.00825268030166626
Batch 30/64 loss: -0.0157928466796875
Batch 31/64 loss: 0.011779606342315674
Batch 32/64 loss: -0.01222217082977295
Batch 33/64 loss: -0.023653030395507812
Batch 34/64 loss: -0.012883126735687256
Batch 35/64 loss: -0.015558183193206787
Batch 36/64 loss: 0.008118391036987305
Batch 37/64 loss: -0.0030527114868164062
Batch 38/64 loss: -0.00716322660446167
Batch 39/64 loss: -0.015111684799194336
Batch 40/64 loss: -0.018072068691253662
Batch 41/64 loss: -0.00824129581451416
Batch 42/64 loss: 0.008322179317474365
Batch 43/64 loss: -0.006798386573791504
Batch 44/64 loss: 0.014144301414489746
Batch 45/64 loss: -0.022504091262817383
Batch 46/64 loss: -0.01542973518371582
Batch 47/64 loss: -0.024875879287719727
Batch 48/64 loss: -0.011666953563690186
Batch 49/64 loss: -0.009863078594207764
Batch 50/64 loss: -0.011870265007019043
Batch 51/64 loss: -0.0020876526832580566
Batch 52/64 loss: -0.00704646110534668
Batch 53/64 loss: -0.02191084623336792
Batch 54/64 loss: -0.0326838493347168
Batch 55/64 loss: -0.006436467170715332
Batch 56/64 loss: -0.006579399108886719
Batch 57/64 loss: -0.02201676368713379
Batch 58/64 loss: -0.013170301914215088
Batch 59/64 loss: 0.00024074316024780273
Batch 60/64 loss: -0.02255958318710327
Batch 61/64 loss: -0.018855631351470947
Batch 62/64 loss: -0.005839228630065918
Batch 63/64 loss: -0.04429161548614502
Batch 64/64 loss: -0.019899964332580566
Epoch 91  Train loss: -0.014431184413386327  Val loss: 0.01466215885791582
Epoch 92
-------------------------------
Batch 1/64 loss: -0.007940173149108887
Batch 2/64 loss: -0.014762341976165771
Batch 3/64 loss: -0.024348974227905273
Batch 4/64 loss: -0.013298213481903076
Batch 5/64 loss: -0.022554397583007812
Batch 6/64 loss: -0.012585997581481934
Batch 7/64 loss: -0.02670520544052124
Batch 8/64 loss: -0.026772141456604004
Batch 9/64 loss: -0.020585060119628906
Batch 10/64 loss: -0.0331154465675354
Batch 11/64 loss: -0.016657531261444092
Batch 12/64 loss: -0.019408047199249268
Batch 13/64 loss: -0.009553134441375732
Batch 14/64 loss: -0.02003657817840576
Batch 15/64 loss: -0.02191859483718872
Batch 16/64 loss: -0.017124831676483154
Batch 17/64 loss: -0.016123294830322266
Batch 18/64 loss: -0.026739656925201416
Batch 19/64 loss: -0.03071308135986328
Batch 20/64 loss: -0.028257131576538086
Batch 21/64 loss: -0.017127811908721924
Batch 22/64 loss: -0.006946146488189697
Batch 23/64 loss: -0.026717662811279297
Batch 24/64 loss: -0.026141047477722168
Batch 25/64 loss: -0.024723529815673828
Batch 26/64 loss: -0.02008819580078125
Batch 27/64 loss: -0.0471491813659668
Batch 28/64 loss: -0.022714734077453613
Batch 29/64 loss: -0.022900938987731934
Batch 30/64 loss: 0.00039893388748168945
Batch 31/64 loss: -0.02033776044845581
Batch 32/64 loss: -0.017646074295043945
Batch 33/64 loss: 0.00118178129196167
Batch 34/64 loss: -0.009870588779449463
Batch 35/64 loss: -0.015504121780395508
Batch 36/64 loss: -0.0008568763732910156
Batch 37/64 loss: -0.02169942855834961
Batch 38/64 loss: -0.0016099810600280762
Batch 39/64 loss: -0.028874516487121582
Batch 40/64 loss: 0.021377980709075928
Batch 41/64 loss: -0.024761199951171875
Batch 42/64 loss: -0.019626080989837646
Batch 43/64 loss: -0.011542558670043945
Batch 44/64 loss: -0.03745269775390625
Batch 45/64 loss: -0.007584571838378906
Batch 46/64 loss: 0.004647731781005859
Batch 47/64 loss: -0.0016286373138427734
Batch 48/64 loss: -0.036026954650878906
Batch 49/64 loss: -0.023938894271850586
Batch 50/64 loss: -0.008758246898651123
Batch 51/64 loss: -0.026268601417541504
Batch 52/64 loss: -0.011446356773376465
Batch 53/64 loss: -0.009763717651367188
Batch 54/64 loss: -0.01913011074066162
Batch 55/64 loss: -0.003878951072692871
Batch 56/64 loss: -0.014285683631896973
Batch 57/64 loss: -0.008561253547668457
Batch 58/64 loss: -0.004760861396789551
Batch 59/64 loss: -0.007307648658752441
Batch 60/64 loss: -0.01697397232055664
Batch 61/64 loss: -0.0017437338829040527
Batch 62/64 loss: -0.014949023723602295
Batch 63/64 loss: -0.037898123264312744
Batch 64/64 loss: -0.005826473236083984
Epoch 92  Train loss: -0.0167083880480598  Val loss: 0.008151312669118246
Saving best model, epoch: 92
Epoch 93
-------------------------------
Batch 1/64 loss: -0.004521608352661133
Batch 2/64 loss: -0.029563844203948975
Batch 3/64 loss: -0.02760368585586548
Batch 4/64 loss: -0.019073843955993652
Batch 5/64 loss: -0.020084917545318604
Batch 6/64 loss: 0.005784153938293457
Batch 7/64 loss: -0.026936113834381104
Batch 8/64 loss: -0.029340803623199463
Batch 9/64 loss: -0.011153042316436768
Batch 10/64 loss: -0.016167402267456055
Batch 11/64 loss: -0.022415995597839355
Batch 12/64 loss: -0.015193343162536621
Batch 13/64 loss: -0.0038594603538513184
Batch 14/64 loss: -0.02265387773513794
Batch 15/64 loss: -0.015683889389038086
Batch 16/64 loss: 0.0016330480575561523
Batch 17/64 loss: -0.011489927768707275
Batch 18/64 loss: -0.0035821199417114258
Batch 19/64 loss: -0.02585083246231079
Batch 20/64 loss: 0.005137979984283447
Batch 21/64 loss: -0.0014854073524475098
Batch 22/64 loss: -0.030838489532470703
Batch 23/64 loss: -0.01772254705429077
Batch 24/64 loss: -0.020693182945251465
Batch 25/64 loss: -0.02611011266708374
Batch 26/64 loss: -0.006619453430175781
Batch 27/64 loss: -0.028813064098358154
Batch 28/64 loss: -0.01291501522064209
Batch 29/64 loss: -0.022391855716705322
Batch 30/64 loss: -0.014418244361877441
Batch 31/64 loss: 6.884336471557617e-05
Batch 32/64 loss: -0.013367831707000732
Batch 33/64 loss: -0.031718552112579346
Batch 34/64 loss: -0.02837824821472168
Batch 35/64 loss: -0.01550668478012085
Batch 36/64 loss: -0.024394094944000244
Batch 37/64 loss: -0.01057279109954834
Batch 38/64 loss: -0.022867798805236816
Batch 39/64 loss: -0.026595592498779297
Batch 40/64 loss: 0.019474565982818604
Batch 41/64 loss: -0.000604093074798584
Batch 42/64 loss: -0.022513389587402344
Batch 43/64 loss: -0.036601901054382324
Batch 44/64 loss: -0.019872725009918213
Batch 45/64 loss: -0.03356337547302246
Batch 46/64 loss: -0.012382924556732178
Batch 47/64 loss: -0.016293764114379883
Batch 48/64 loss: 0.011995553970336914
Batch 49/64 loss: -0.0042209625244140625
Batch 50/64 loss: -0.02510964870452881
Batch 51/64 loss: -0.024649977684020996
Batch 52/64 loss: -0.012873530387878418
Batch 53/64 loss: -0.024277925491333008
Batch 54/64 loss: 0.01122581958770752
Batch 55/64 loss: -0.020258665084838867
Batch 56/64 loss: -0.017749488353729248
Batch 57/64 loss: -0.022277653217315674
Batch 58/64 loss: -0.01926255226135254
Batch 59/64 loss: -0.03304004669189453
Batch 60/64 loss: -0.012965381145477295
Batch 61/64 loss: -0.014743685722351074
Batch 62/64 loss: -0.002260923385620117
Batch 63/64 loss: -0.036327481269836426
Batch 64/64 loss: -0.021329283714294434
Epoch 93  Train loss: -0.016205659099653654  Val loss: 0.02041760035806505
Epoch 94
-------------------------------
Batch 1/64 loss: 0.005196034908294678
Batch 2/64 loss: -0.02308720350265503
Batch 3/64 loss: -0.012658834457397461
Batch 4/64 loss: -0.03311467170715332
Batch 5/64 loss: -0.012562870979309082
Batch 6/64 loss: -0.020567595958709717
Batch 7/64 loss: -0.025155723094940186
Batch 8/64 loss: -0.029473304748535156
Batch 9/64 loss: -0.021259546279907227
Batch 10/64 loss: -0.023563742637634277
Batch 11/64 loss: -0.033456265926361084
Batch 12/64 loss: -0.02154707908630371
Batch 13/64 loss: 0.0039168596267700195
Batch 14/64 loss: -0.03389078378677368
Batch 15/64 loss: -0.020410537719726562
Batch 16/64 loss: -0.005849182605743408
Batch 17/64 loss: -0.014962434768676758
Batch 18/64 loss: 0.010947346687316895
Batch 19/64 loss: -0.03158611059188843
Batch 20/64 loss: -0.021917879581451416
Batch 21/64 loss: 0.007327079772949219
Batch 22/64 loss: -0.026364624500274658
Batch 23/64 loss: -0.028349220752716064
Batch 24/64 loss: -0.04023057222366333
Batch 25/64 loss: -0.005580902099609375
Batch 26/64 loss: -0.005509316921234131
Batch 27/64 loss: 0.009490013122558594
Batch 28/64 loss: -0.019444167613983154
Batch 29/64 loss: -0.021268844604492188
Batch 30/64 loss: -0.018660306930541992
Batch 31/64 loss: -0.021055161952972412
Batch 32/64 loss: -0.02062392234802246
Batch 33/64 loss: -0.026113033294677734
Batch 34/64 loss: -0.04473906755447388
Batch 35/64 loss: -0.02510511875152588
Batch 36/64 loss: -0.03847867250442505
Batch 37/64 loss: -0.01885145902633667
Batch 38/64 loss: -0.01913774013519287
Batch 39/64 loss: 0.004169106483459473
Batch 40/64 loss: -0.029523253440856934
Batch 41/64 loss: 0.003666818141937256
Batch 42/64 loss: -0.007737398147583008
Batch 43/64 loss: -0.028227627277374268
Batch 44/64 loss: -0.032525479793548584
Batch 45/64 loss: -0.010624051094055176
Batch 46/64 loss: -0.008507907390594482
Batch 47/64 loss: -0.03583669662475586
Batch 48/64 loss: 0.001056969165802002
Batch 49/64 loss: -0.02123093605041504
Batch 50/64 loss: -0.013902842998504639
Batch 51/64 loss: -0.019921302795410156
Batch 52/64 loss: -0.01633089780807495
Batch 53/64 loss: -0.02231067419052124
Batch 54/64 loss: -0.022111475467681885
Batch 55/64 loss: -0.02236497402191162
Batch 56/64 loss: -0.005295097827911377
Batch 57/64 loss: -0.009210824966430664
Batch 58/64 loss: 0.0015164017677307129
Batch 59/64 loss: -0.00883638858795166
Batch 60/64 loss: -0.007952511310577393
Batch 61/64 loss: 0.0008018016815185547
Batch 62/64 loss: -0.0231170654296875
Batch 63/64 loss: -0.03188401460647583
Batch 64/64 loss: -0.013903439044952393
Epoch 94  Train loss: -0.017323395551419724  Val loss: 0.018705220156928517
Epoch 95
-------------------------------
Batch 1/64 loss: -0.014488518238067627
Batch 2/64 loss: -0.015402436256408691
Batch 3/64 loss: -0.02943551540374756
Batch 4/64 loss: -0.020298659801483154
Batch 5/64 loss: -0.010868191719055176
Batch 6/64 loss: -0.01027148962020874
Batch 7/64 loss: -0.0009506344795227051
Batch 8/64 loss: -0.007326006889343262
Batch 9/64 loss: -0.02789020538330078
Batch 10/64 loss: -0.01294708251953125
Batch 11/64 loss: -0.020659327507019043
Batch 12/64 loss: -0.01618492603302002
Batch 13/64 loss: -0.007677912712097168
Batch 14/64 loss: -0.01482623815536499
Batch 15/64 loss: -0.023482799530029297
Batch 16/64 loss: -0.018507659435272217
Batch 17/64 loss: -0.01699376106262207
Batch 18/64 loss: -0.025867104530334473
Batch 19/64 loss: -0.023067057132720947
Batch 20/64 loss: -0.024795949459075928
Batch 21/64 loss: -0.03606557846069336
Batch 22/64 loss: -0.004932522773742676
Batch 23/64 loss: -0.0043985843658447266
Batch 24/64 loss: -0.028427064418792725
Batch 25/64 loss: -0.029679477214813232
Batch 26/64 loss: -0.03499406576156616
Batch 27/64 loss: -0.002463102340698242
Batch 28/64 loss: -0.02047431468963623
Batch 29/64 loss: -0.01446378231048584
Batch 30/64 loss: -0.029975056648254395
Batch 31/64 loss: -0.010868370532989502
Batch 32/64 loss: -0.016269803047180176
Batch 33/64 loss: -0.035901546478271484
Batch 34/64 loss: -0.016839981079101562
Batch 35/64 loss: -0.007526040077209473
Batch 36/64 loss: -0.027694880962371826
Batch 37/64 loss: -0.032872557640075684
Batch 38/64 loss: -0.020470619201660156
Batch 39/64 loss: -0.01969277858734131
Batch 40/64 loss: -0.029410183429718018
Batch 41/64 loss: -0.013606250286102295
Batch 42/64 loss: -0.02214181423187256
Batch 43/64 loss: 0.011777162551879883
Batch 44/64 loss: -0.023647069931030273
Batch 45/64 loss: -0.031980276107788086
Batch 46/64 loss: -0.030657708644866943
Batch 47/64 loss: -0.02882939577102661
Batch 48/64 loss: 0.001335442066192627
Batch 49/64 loss: -0.0345495343208313
Batch 50/64 loss: -0.005254983901977539
Batch 51/64 loss: 0.0016678571701049805
Batch 52/64 loss: -0.02552497386932373
Batch 53/64 loss: -0.014401793479919434
Batch 54/64 loss: -0.01681375503540039
Batch 55/64 loss: -0.02077484130859375
Batch 56/64 loss: 0.004765033721923828
Batch 57/64 loss: -0.026188015937805176
Batch 58/64 loss: -0.009590864181518555
Batch 59/64 loss: -0.01317739486694336
Batch 60/64 loss: -0.021081209182739258
Batch 61/64 loss: -0.01815176010131836
Batch 62/64 loss: 0.01873117685317993
Batch 63/64 loss: -0.013867318630218506
Batch 64/64 loss: -0.00943690538406372
Epoch 95  Train loss: -0.01732394905651317  Val loss: 0.01729139019943185
Epoch 96
-------------------------------
Batch 1/64 loss: -0.019479751586914062
Batch 2/64 loss: -0.019299983978271484
Batch 3/64 loss: -0.020086467266082764
Batch 4/64 loss: -0.015924572944641113
Batch 5/64 loss: -0.00039118528366088867
Batch 6/64 loss: -0.00687861442565918
Batch 7/64 loss: -0.02297365665435791
Batch 8/64 loss: -0.023583948612213135
Batch 9/64 loss: -0.02046644687652588
Batch 10/64 loss: -0.02139216661453247
Batch 11/64 loss: -0.011507689952850342
Batch 12/64 loss: -0.002129197120666504
Batch 13/64 loss: -0.033835411071777344
Batch 14/64 loss: -0.04155004024505615
Batch 15/64 loss: -0.006224393844604492
Batch 16/64 loss: -0.0008511543273925781
Batch 17/64 loss: -0.023595988750457764
Batch 18/64 loss: -0.00937563180923462
Batch 19/64 loss: -0.026420533657073975
Batch 20/64 loss: -0.023078322410583496
Batch 21/64 loss: -0.01790827512741089
Batch 22/64 loss: -0.030473172664642334
Batch 23/64 loss: -0.03218519687652588
Batch 24/64 loss: -0.0012508034706115723
Batch 25/64 loss: -0.03412365913391113
Batch 26/64 loss: -0.03813070058822632
Batch 27/64 loss: -0.01056981086730957
Batch 28/64 loss: -0.019919395446777344
Batch 29/64 loss: -0.02574169635772705
Batch 30/64 loss: -0.01250547170639038
Batch 31/64 loss: -0.02795642614364624
Batch 32/64 loss: -0.027774512767791748
Batch 33/64 loss: -0.009748637676239014
Batch 34/64 loss: -0.007086455821990967
Batch 35/64 loss: -0.01741623878479004
Batch 36/64 loss: -0.019250154495239258
Batch 37/64 loss: -0.027996420860290527
Batch 38/64 loss: -0.02689725160598755
Batch 39/64 loss: -0.01854705810546875
Batch 40/64 loss: -0.021290719509124756
Batch 41/64 loss: 0.007465064525604248
Batch 42/64 loss: -0.014582514762878418
Batch 43/64 loss: -0.036767005920410156
Batch 44/64 loss: -0.02781379222869873
Batch 45/64 loss: -0.012788593769073486
Batch 46/64 loss: -0.010581374168395996
Batch 47/64 loss: -0.018487870693206787
Batch 48/64 loss: 0.010410547256469727
Batch 49/64 loss: -0.0342441201210022
Batch 50/64 loss: -0.006126761436462402
Batch 51/64 loss: -0.00578916072845459
Batch 52/64 loss: -0.029098868370056152
Batch 53/64 loss: 0.004593491554260254
Batch 54/64 loss: -0.039258718490600586
Batch 55/64 loss: -0.013255834579467773
Batch 56/64 loss: -0.033123135566711426
Batch 57/64 loss: -0.01363992691040039
Batch 58/64 loss: -0.02739650011062622
Batch 59/64 loss: -0.021816372871398926
Batch 60/64 loss: -0.015456914901733398
Batch 61/64 loss: -0.021577537059783936
Batch 62/64 loss: -0.007677555084228516
Batch 63/64 loss: 0.0027208328247070312
Batch 64/64 loss: 0.020071983337402344
Epoch 96  Train loss: -0.017647934894935758  Val loss: 0.016621853068112508
Epoch 97
-------------------------------
Batch 1/64 loss: -0.008254945278167725
Batch 2/64 loss: -0.030039548873901367
Batch 3/64 loss: -0.021208345890045166
Batch 4/64 loss: -0.02652031183242798
Batch 5/64 loss: -0.025050222873687744
Batch 6/64 loss: -0.0261419415473938
Batch 7/64 loss: -0.04161638021469116
Batch 8/64 loss: -0.025709331035614014
Batch 9/64 loss: -0.016697585582733154
Batch 10/64 loss: -0.0038114190101623535
Batch 11/64 loss: -0.024411380290985107
Batch 12/64 loss: -0.009035587310791016
Batch 13/64 loss: -0.018050730228424072
Batch 14/64 loss: -0.03662383556365967
Batch 15/64 loss: -0.015781402587890625
Batch 16/64 loss: -0.03735649585723877
Batch 17/64 loss: -0.025589942932128906
Batch 18/64 loss: -0.014785170555114746
Batch 19/64 loss: -0.027095913887023926
Batch 20/64 loss: -0.04460674524307251
Batch 21/64 loss: -0.0300445556640625
Batch 22/64 loss: -0.027296841144561768
Batch 23/64 loss: -0.01710110902786255
Batch 24/64 loss: -0.019351601600646973
Batch 25/64 loss: -0.0025850534439086914
Batch 26/64 loss: 0.024634718894958496
Batch 27/64 loss: 0.0038213133811950684
Batch 28/64 loss: -0.0524907112121582
Batch 29/64 loss: -0.00028526782989501953
Batch 30/64 loss: -0.003917574882507324
Batch 31/64 loss: -0.029791295528411865
Batch 32/64 loss: -0.025979220867156982
Batch 33/64 loss: -0.02294135093688965
Batch 34/64 loss: -0.04261922836303711
Batch 35/64 loss: -0.023123860359191895
Batch 36/64 loss: -0.02348482608795166
Batch 37/64 loss: -0.036649107933044434
Batch 38/64 loss: -0.009549498558044434
Batch 39/64 loss: -0.024692177772521973
Batch 40/64 loss: -0.03059983253479004
Batch 41/64 loss: -0.027496814727783203
Batch 42/64 loss: -0.01714986562728882
Batch 43/64 loss: 0.0011221766471862793
Batch 44/64 loss: -0.00640261173248291
Batch 45/64 loss: -0.017436683177947998
Batch 46/64 loss: 0.002351522445678711
Batch 47/64 loss: -0.00863039493560791
Batch 48/64 loss: -0.008606910705566406
Batch 49/64 loss: -0.017785310745239258
Batch 50/64 loss: -0.02123159170150757
Batch 51/64 loss: -0.03311961889266968
Batch 52/64 loss: -0.015494167804718018
Batch 53/64 loss: -0.021699249744415283
Batch 54/64 loss: -0.022231757640838623
Batch 55/64 loss: -0.01146465539932251
Batch 56/64 loss: -0.025580525398254395
Batch 57/64 loss: -0.012453019618988037
Batch 58/64 loss: -0.030887842178344727
Batch 59/64 loss: -0.023546457290649414
Batch 60/64 loss: -0.018340349197387695
Batch 61/64 loss: -0.017238616943359375
Batch 62/64 loss: -0.031080663204193115
Batch 63/64 loss: -0.017014801502227783
Batch 64/64 loss: -0.024683475494384766
Epoch 97  Train loss: -0.020272394255095836  Val loss: 0.017621146444602522
Epoch 98
-------------------------------
Batch 1/64 loss: -0.030331671237945557
Batch 2/64 loss: -0.014230012893676758
Batch 3/64 loss: -0.001241922378540039
Batch 4/64 loss: -0.030706405639648438
Batch 5/64 loss: -0.029310882091522217
Batch 6/64 loss: -0.016872286796569824
Batch 7/64 loss: -0.05361330509185791
Batch 8/64 loss: -0.039240360260009766
Batch 9/64 loss: -0.007848381996154785
Batch 10/64 loss: -0.01856905221939087
Batch 11/64 loss: -0.0008817911148071289
Batch 12/64 loss: -0.0031669139862060547
Batch 13/64 loss: -0.00376206636428833
Batch 14/64 loss: 0.0037609338760375977
Batch 15/64 loss: -0.027589142322540283
Batch 16/64 loss: -0.038221776485443115
Batch 17/64 loss: -0.011988997459411621
Batch 18/64 loss: -0.014961004257202148
Batch 19/64 loss: -0.03904038667678833
Batch 20/64 loss: -0.02832496166229248
Batch 21/64 loss: -0.02086782455444336
Batch 22/64 loss: 0.0027573108673095703
Batch 23/64 loss: -0.029756128787994385
Batch 24/64 loss: -0.03817540407180786
Batch 25/64 loss: -0.014561176300048828
Batch 26/64 loss: -0.02603358030319214
Batch 27/64 loss: -0.009874522686004639
Batch 28/64 loss: -0.02953469753265381
Batch 29/64 loss: -0.03132295608520508
Batch 30/64 loss: -0.026861965656280518
Batch 31/64 loss: -0.022698581218719482
Batch 32/64 loss: -0.027617216110229492
Batch 33/64 loss: -0.017698585987091064
Batch 34/64 loss: -0.022339046001434326
Batch 35/64 loss: -0.008607089519500732
Batch 36/64 loss: -0.034662723541259766
Batch 37/64 loss: -0.032063186168670654
Batch 38/64 loss: -0.006787419319152832
Batch 39/64 loss: -0.010251939296722412
Batch 40/64 loss: -0.035468339920043945
Batch 41/64 loss: -0.019081711769104004
Batch 42/64 loss: 0.0023056864738464355
Batch 43/64 loss: -0.03182506561279297
Batch 44/64 loss: -0.035494446754455566
Batch 45/64 loss: -0.02907729148864746
Batch 46/64 loss: -0.018746376037597656
Batch 47/64 loss: -0.011055350303649902
Batch 48/64 loss: -0.0032079219818115234
Batch 49/64 loss: -0.03125542402267456
Batch 50/64 loss: -0.01685023307800293
Batch 51/64 loss: -0.012873828411102295
Batch 52/64 loss: -0.022764623165130615
Batch 53/64 loss: -0.028548717498779297
Batch 54/64 loss: -0.006088614463806152
Batch 55/64 loss: -0.014636874198913574
Batch 56/64 loss: -0.017448604106903076
Batch 57/64 loss: -0.018341064453125
Batch 58/64 loss: -0.02996683120727539
Batch 59/64 loss: -0.032493650913238525
Batch 60/64 loss: -0.018740177154541016
Batch 61/64 loss: -0.026175498962402344
Batch 62/64 loss: -0.028193235397338867
Batch 63/64 loss: -0.02954190969467163
Batch 64/64 loss: -0.009426355361938477
Epoch 98  Train loss: -0.020952737097646675  Val loss: 0.017226359688539274
Epoch 99
-------------------------------
Batch 1/64 loss: -0.04207944869995117
Batch 2/64 loss: 0.001394808292388916
Batch 3/64 loss: -0.021293163299560547
Batch 4/64 loss: -0.016144275665283203
Batch 5/64 loss: 0.02346956729888916
Batch 6/64 loss: -0.0362473726272583
Batch 7/64 loss: -0.030687332153320312
Batch 8/64 loss: -0.03489840030670166
Batch 9/64 loss: -0.009476780891418457
Batch 10/64 loss: -0.031242191791534424
Batch 11/64 loss: -0.02360367774963379
Batch 12/64 loss: -0.027831077575683594
Batch 13/64 loss: -0.0026700496673583984
Batch 14/64 loss: -0.032612383365631104
Batch 15/64 loss: -0.02126610279083252
Batch 16/64 loss: -0.010631322860717773
Batch 17/64 loss: -0.018235385417938232
Batch 18/64 loss: 0.0031965970993041992
Batch 19/64 loss: -0.02209383249282837
Batch 20/64 loss: -0.012761056423187256
Batch 21/64 loss: -0.006703853607177734
Batch 22/64 loss: -0.011209726333618164
Batch 23/64 loss: -0.026211857795715332
Batch 24/64 loss: -0.0071395039558410645
Batch 25/64 loss: -0.02593296766281128
Batch 26/64 loss: -0.02041780948638916
Batch 27/64 loss: -0.011639893054962158
Batch 28/64 loss: -0.005227565765380859
Batch 29/64 loss: -0.001411139965057373
Batch 30/64 loss: -0.027689456939697266
Batch 31/64 loss: -0.0114668607711792
Batch 32/64 loss: -0.02301311492919922
Batch 33/64 loss: -0.022136270999908447
Batch 34/64 loss: -0.02210676670074463
Batch 35/64 loss: -0.018030285835266113
Batch 36/64 loss: -0.024668335914611816
Batch 37/64 loss: -0.024180948734283447
Batch 38/64 loss: -0.023326396942138672
Batch 39/64 loss: -0.0138435959815979
Batch 40/64 loss: -0.02045583724975586
Batch 41/64 loss: -0.01079636812210083
Batch 42/64 loss: -0.01666778326034546
Batch 43/64 loss: -0.02062821388244629
Batch 44/64 loss: -0.024343490600585938
Batch 45/64 loss: -0.028924107551574707
Batch 46/64 loss: -0.03228354454040527
Batch 47/64 loss: -0.03216463327407837
Batch 48/64 loss: -0.021971821784973145
Batch 49/64 loss: -0.033585309982299805
Batch 50/64 loss: -0.03906947374343872
Batch 51/64 loss: -0.02047419548034668
Batch 52/64 loss: -0.020620346069335938
Batch 53/64 loss: -0.004421532154083252
Batch 54/64 loss: -0.01941502094268799
Batch 55/64 loss: -0.03761225938796997
Batch 56/64 loss: -0.014171838760375977
Batch 57/64 loss: -0.03782767057418823
Batch 58/64 loss: -0.007399022579193115
Batch 59/64 loss: -0.015906155109405518
Batch 60/64 loss: -0.0235445499420166
Batch 61/64 loss: -0.02350229024887085
Batch 62/64 loss: -0.01771777868270874
Batch 63/64 loss: -0.013140738010406494
Batch 64/64 loss: -0.008326053619384766
Epoch 99  Train loss: -0.019371886346854415  Val loss: 0.01330041229929711
Epoch 100
-------------------------------
Batch 1/64 loss: -0.03340601921081543
Batch 2/64 loss: -0.004638493061065674
Batch 3/64 loss: -0.02989858388900757
Batch 4/64 loss: -0.02306544780731201
Batch 5/64 loss: 0.005060970783233643
Batch 6/64 loss: -0.005405902862548828
Batch 7/64 loss: -0.018860995769500732
Batch 8/64 loss: -0.03785550594329834
Batch 9/64 loss: -0.0237463116645813
Batch 10/64 loss: -0.02054917812347412
Batch 11/64 loss: -0.03235238790512085
Batch 12/64 loss: -0.035472989082336426
Batch 13/64 loss: -0.010619044303894043
Batch 14/64 loss: -0.027196645736694336
Batch 15/64 loss: -0.027798950672149658
Batch 16/64 loss: -0.017084717750549316
Batch 17/64 loss: -0.036689043045043945
Batch 18/64 loss: -0.03433936834335327
Batch 19/64 loss: -0.04630845785140991
Batch 20/64 loss: -0.002030014991760254
Batch 21/64 loss: -0.025272846221923828
Batch 22/64 loss: -0.00829094648361206
Batch 23/64 loss: -0.03851211071014404
Batch 24/64 loss: -0.015777409076690674
Batch 25/64 loss: -0.03155684471130371
Batch 26/64 loss: -0.0007714629173278809
Batch 27/64 loss: -0.03344756364822388
Batch 28/64 loss: -0.003316164016723633
Batch 29/64 loss: -0.0340576171875
Batch 30/64 loss: 0.01127016544342041
Batch 31/64 loss: -0.03682440519332886
Batch 32/64 loss: -0.03220808506011963
Batch 33/64 loss: -0.004122734069824219
Batch 34/64 loss: -0.03084254264831543
Batch 35/64 loss: -0.01635962724685669
Batch 36/64 loss: -0.016112983226776123
Batch 37/64 loss: -0.01427912712097168
Batch 38/64 loss: -0.03232753276824951
Batch 39/64 loss: -0.03467440605163574
Batch 40/64 loss: -0.002221405506134033
Batch 41/64 loss: -0.006021976470947266
Batch 42/64 loss: -0.026014268398284912
Batch 43/64 loss: -0.03807312250137329
Batch 44/64 loss: -0.01318502426147461
Batch 45/64 loss: -0.0243799090385437
Batch 46/64 loss: -0.021857500076293945
Batch 47/64 loss: -0.014674186706542969
Batch 48/64 loss: -0.015010654926300049
Batch 49/64 loss: -0.03259396553039551
Batch 50/64 loss: -0.02500206232070923
Batch 51/64 loss: -0.02028357982635498
Batch 52/64 loss: -0.018041133880615234
Batch 53/64 loss: -0.021477878093719482
Batch 54/64 loss: -0.01784384250640869
Batch 55/64 loss: -0.029342174530029297
Batch 56/64 loss: -0.0019568800926208496
Batch 57/64 loss: -0.030620098114013672
Batch 58/64 loss: -0.041105568408966064
Batch 59/64 loss: -0.02267402410507202
Batch 60/64 loss: -0.014509379863739014
Batch 61/64 loss: -0.019109725952148438
Batch 62/64 loss: -0.029365360736846924
Batch 63/64 loss: -0.018061578273773193
Batch 64/64 loss: -0.02312248945236206
Epoch 100  Train loss: -0.021655035252664604  Val loss: 0.013245004968544872
Epoch 101
-------------------------------
Batch 1/64 loss: -0.022433996200561523
Batch 2/64 loss: -0.023800253868103027
Batch 3/64 loss: -0.011255860328674316
Batch 4/64 loss: -0.010374605655670166
Batch 5/64 loss: -0.02452409267425537
Batch 6/64 loss: -0.00020253658294677734
Batch 7/64 loss: -0.01142120361328125
Batch 8/64 loss: -0.03518819808959961
Batch 9/64 loss: -0.038474321365356445
Batch 10/64 loss: -0.023606538772583008
Batch 11/64 loss: -0.022199690341949463
Batch 12/64 loss: -0.009287357330322266
Batch 13/64 loss: -0.02841132879257202
Batch 14/64 loss: -0.01760774850845337
Batch 15/64 loss: -0.03433734178543091
Batch 16/64 loss: -0.02883124351501465
Batch 17/64 loss: -0.006585657596588135
Batch 18/64 loss: -0.006852686405181885
Batch 19/64 loss: -0.03312826156616211
Batch 20/64 loss: -0.025400876998901367
Batch 21/64 loss: -0.005866348743438721
Batch 22/64 loss: -0.006136536598205566
Batch 23/64 loss: -0.03752630949020386
Batch 24/64 loss: -0.008593559265136719
Batch 25/64 loss: -0.02782219648361206
Batch 26/64 loss: -0.02534085512161255
Batch 27/64 loss: -0.037739455699920654
Batch 28/64 loss: -0.02286672592163086
Batch 29/64 loss: -0.04177224636077881
Batch 30/64 loss: 0.0006691813468933105
Batch 31/64 loss: -0.02827131748199463
Batch 32/64 loss: -0.024829626083374023
Batch 33/64 loss: -0.02702641487121582
Batch 34/64 loss: -0.028323352336883545
Batch 35/64 loss: -0.025825023651123047
Batch 36/64 loss: -0.021664142608642578
Batch 37/64 loss: -0.026313602924346924
Batch 38/64 loss: -0.03615695238113403
Batch 39/64 loss: -0.021749675273895264
Batch 40/64 loss: -0.03905355930328369
Batch 41/64 loss: 0.0016531944274902344
Batch 42/64 loss: -0.017294108867645264
Batch 43/64 loss: -0.020323634147644043
Batch 44/64 loss: -0.014177083969116211
Batch 45/64 loss: -0.010311722755432129
Batch 46/64 loss: -0.02244502305984497
Batch 47/64 loss: -0.034076809883117676
Batch 48/64 loss: -0.010457813739776611
Batch 49/64 loss: -0.017797231674194336
Batch 50/64 loss: -0.04329836368560791
Batch 51/64 loss: -0.01849067211151123
Batch 52/64 loss: -0.0318790078163147
Batch 53/64 loss: -0.03658413887023926
Batch 54/64 loss: -0.0004031658172607422
Batch 55/64 loss: -0.02009737491607666
Batch 56/64 loss: -0.030814051628112793
Batch 57/64 loss: -0.019086122512817383
Batch 58/64 loss: -0.029869556427001953
Batch 59/64 loss: -0.022422492504119873
Batch 60/64 loss: -0.031656086444854736
Batch 61/64 loss: -0.02912449836730957
Batch 62/64 loss: -0.02717047929763794
Batch 63/64 loss: 0.007183969020843506
Batch 64/64 loss: -0.0032846927642822266
Epoch 101  Train loss: -0.02176530688416724  Val loss: 0.013263998367532422
Epoch 102
-------------------------------
Batch 1/64 loss: -0.03903907537460327
Batch 2/64 loss: -0.027209818363189697
Batch 3/64 loss: -0.018224358558654785
Batch 4/64 loss: -0.02470463514328003
Batch 5/64 loss: -0.03281921148300171
Batch 6/64 loss: -0.023403525352478027
Batch 7/64 loss: -0.042853713035583496
Batch 8/64 loss: -0.008747518062591553
Batch 9/64 loss: -0.024909913539886475
Batch 10/64 loss: -0.04775351285934448
Batch 11/64 loss: -0.020646870136260986
Batch 12/64 loss: -0.010291218757629395
Batch 13/64 loss: -0.035161733627319336
Batch 14/64 loss: -0.028001785278320312
Batch 15/64 loss: -0.033414483070373535
Batch 16/64 loss: -0.0049561262130737305
Batch 17/64 loss: -0.014069855213165283
Batch 18/64 loss: -0.029354572296142578
Batch 19/64 loss: -0.03484731912612915
Batch 20/64 loss: -0.03593921661376953
Batch 21/64 loss: -0.0014894604682922363
Batch 22/64 loss: -0.023687124252319336
Batch 23/64 loss: -0.03833973407745361
Batch 24/64 loss: -0.019837260246276855
Batch 25/64 loss: -0.022410809993743896
Batch 26/64 loss: -0.02047264575958252
Batch 27/64 loss: -0.024854540824890137
Batch 28/64 loss: -0.03208959102630615
Batch 29/64 loss: -0.022206902503967285
Batch 30/64 loss: -0.024339616298675537
Batch 31/64 loss: -0.01431041955947876
Batch 32/64 loss: -0.036010026931762695
Batch 33/64 loss: -0.010921955108642578
Batch 34/64 loss: -0.045518457889556885
Batch 35/64 loss: -0.03276371955871582
Batch 36/64 loss: -0.02200186252593994
Batch 37/64 loss: -0.006453514099121094
Batch 38/64 loss: -0.00624924898147583
Batch 39/64 loss: -0.0073618292808532715
Batch 40/64 loss: -0.023894548416137695
Batch 41/64 loss: -0.012647151947021484
Batch 42/64 loss: -0.042850494384765625
Batch 43/64 loss: -0.006948590278625488
Batch 44/64 loss: 0.004372537136077881
Batch 45/64 loss: -0.028976380825042725
Batch 46/64 loss: -0.028356194496154785
Batch 47/64 loss: -0.030230283737182617
Batch 48/64 loss: -0.022083580493927002
Batch 49/64 loss: -0.015579938888549805
Batch 50/64 loss: -0.011571049690246582
Batch 51/64 loss: -0.001573801040649414
Batch 52/64 loss: -0.013593554496765137
Batch 53/64 loss: -0.012889623641967773
Batch 54/64 loss: -0.025476515293121338
Batch 55/64 loss: -0.02872157096862793
Batch 56/64 loss: -0.022789716720581055
Batch 57/64 loss: -0.01041501760482788
Batch 58/64 loss: -0.032628536224365234
Batch 59/64 loss: -0.02643406391143799
Batch 60/64 loss: -0.0018346905708312988
Batch 61/64 loss: -0.04235804080963135
Batch 62/64 loss: -0.024956583976745605
Batch 63/64 loss: -0.03405928611755371
Batch 64/64 loss: -0.023703336715698242
Epoch 102  Train loss: -0.02293241351258521  Val loss: 0.003178054729278145
Saving best model, epoch: 102
Epoch 103
-------------------------------
Batch 1/64 loss: -0.03131067752838135
Batch 2/64 loss: -0.02547752857208252
Batch 3/64 loss: -0.03562206029891968
Batch 4/64 loss: -0.04922616481781006
Batch 5/64 loss: -0.0230557918548584
Batch 6/64 loss: -0.02317863702774048
Batch 7/64 loss: -0.026761174201965332
Batch 8/64 loss: -0.019145965576171875
Batch 9/64 loss: -0.03866690397262573
Batch 10/64 loss: -0.021333277225494385
Batch 11/64 loss: -0.037778377532958984
Batch 12/64 loss: -0.02720552682876587
Batch 13/64 loss: -0.03048419952392578
Batch 14/64 loss: -0.01588338613510132
Batch 15/64 loss: -0.02820199728012085
Batch 16/64 loss: -0.010278582572937012
Batch 17/64 loss: -0.03401517868041992
Batch 18/64 loss: -0.02672135829925537
Batch 19/64 loss: -0.024613022804260254
Batch 20/64 loss: -0.02315545082092285
Batch 21/64 loss: -0.004722118377685547
Batch 22/64 loss: -0.051495134830474854
Batch 23/64 loss: -0.038149356842041016
Batch 24/64 loss: -0.018194615840911865
Batch 25/64 loss: -0.02561366558074951
Batch 26/64 loss: -0.02972811460494995
Batch 27/64 loss: -0.017001867294311523
Batch 28/64 loss: -0.009694337844848633
Batch 29/64 loss: -0.028527438640594482
Batch 30/64 loss: -0.03633522987365723
Batch 31/64 loss: -0.03403735160827637
Batch 32/64 loss: -0.036516666412353516
Batch 33/64 loss: -0.039711952209472656
Batch 34/64 loss: -0.017133355140686035
Batch 35/64 loss: -0.03766375780105591
Batch 36/64 loss: -0.019063353538513184
Batch 37/64 loss: -0.03801184892654419
Batch 38/64 loss: -0.015170633792877197
Batch 39/64 loss: -0.02951192855834961
Batch 40/64 loss: -0.0027506351470947266
Batch 41/64 loss: -0.02244013547897339
Batch 42/64 loss: -0.028636515140533447
Batch 43/64 loss: -0.02660757303237915
Batch 44/64 loss: -0.029651343822479248
Batch 45/64 loss: -0.007529139518737793
Batch 46/64 loss: -0.028310060501098633
Batch 47/64 loss: -0.03068464994430542
Batch 48/64 loss: -0.03625965118408203
Batch 49/64 loss: -0.018173515796661377
Batch 50/64 loss: -0.029190540313720703
Batch 51/64 loss: -0.021393656730651855
Batch 52/64 loss: -0.01238340139389038
Batch 53/64 loss: -0.01441347599029541
Batch 54/64 loss: 0.001423180103302002
Batch 55/64 loss: -0.019048631191253662
Batch 56/64 loss: -0.024103760719299316
Batch 57/64 loss: -0.023245573043823242
Batch 58/64 loss: -0.037653565406799316
Batch 59/64 loss: -0.008964598178863525
Batch 60/64 loss: -0.026040971279144287
Batch 61/64 loss: -0.03120601177215576
Batch 62/64 loss: -0.028381526470184326
Batch 63/64 loss: -0.03082519769668579
Batch 64/64 loss: -0.026356637477874756
Epoch 103  Train loss: -0.025641355561275107  Val loss: 0.012633227400763338
Epoch 104
-------------------------------
Batch 1/64 loss: -0.03295665979385376
Batch 2/64 loss: -0.014400482177734375
Batch 3/64 loss: -0.020402729511260986
Batch 4/64 loss: -0.011464118957519531
Batch 5/64 loss: -0.03138267993927002
Batch 6/64 loss: -0.014558196067810059
Batch 7/64 loss: -0.020658910274505615
Batch 8/64 loss: -0.022253215312957764
Batch 9/64 loss: -0.0035242438316345215
Batch 10/64 loss: -0.01473921537399292
Batch 11/64 loss: -0.027064025402069092
Batch 12/64 loss: -0.021494507789611816
Batch 13/64 loss: -0.03155165910720825
Batch 14/64 loss: -0.007130622863769531
Batch 15/64 loss: 0.00580906867980957
Batch 16/64 loss: -0.02921760082244873
Batch 17/64 loss: -0.03654831647872925
Batch 18/64 loss: -0.02036505937576294
Batch 19/64 loss: -0.041800618171691895
Batch 20/64 loss: -0.015830636024475098
Batch 21/64 loss: -0.03941047191619873
Batch 22/64 loss: -0.01845109462738037
Batch 23/64 loss: -0.022430777549743652
Batch 24/64 loss: 0.001097261905670166
Batch 25/64 loss: -0.026633262634277344
Batch 26/64 loss: -0.033372461795806885
Batch 27/64 loss: -0.024948716163635254
Batch 28/64 loss: -0.022499382495880127
Batch 29/64 loss: -0.03836870193481445
Batch 30/64 loss: -0.015222430229187012
Batch 31/64 loss: -0.015770316123962402
Batch 32/64 loss: -0.020954012870788574
Batch 33/64 loss: -0.024916470050811768
Batch 34/64 loss: -0.033081650733947754
Batch 35/64 loss: -0.01415783166885376
Batch 36/64 loss: 0.005638599395751953
Batch 37/64 loss: -0.019019603729248047
Batch 38/64 loss: -0.019008517265319824
Batch 39/64 loss: -0.02428269386291504
Batch 40/64 loss: -0.004198431968688965
Batch 41/64 loss: -0.018864989280700684
Batch 42/64 loss: -0.03009665012359619
Batch 43/64 loss: -0.013781309127807617
Batch 44/64 loss: -0.005963802337646484
Batch 45/64 loss: -0.028253257274627686
Batch 46/64 loss: -0.020937979221343994
Batch 47/64 loss: -0.02935326099395752
Batch 48/64 loss: -0.006857097148895264
Batch 49/64 loss: -0.018131136894226074
Batch 50/64 loss: -0.02147650718688965
Batch 51/64 loss: -0.023808836936950684
Batch 52/64 loss: -0.027239680290222168
Batch 53/64 loss: -0.029261350631713867
Batch 54/64 loss: -0.03655135631561279
Batch 55/64 loss: -0.027291715145111084
Batch 56/64 loss: -0.008396565914154053
Batch 57/64 loss: -0.022915005683898926
Batch 58/64 loss: -0.01092904806137085
Batch 59/64 loss: -0.019253015518188477
Batch 60/64 loss: -0.017036795616149902
Batch 61/64 loss: -0.03821533918380737
Batch 62/64 loss: -0.031885385513305664
Batch 63/64 loss: -0.02994626760482788
Batch 64/64 loss: -0.011088907718658447
Epoch 104  Train loss: -0.021118249846439736  Val loss: 0.019799941184184804
Epoch 105
-------------------------------
Batch 1/64 loss: -0.02445840835571289
Batch 2/64 loss: -0.036568403244018555
Batch 3/64 loss: -0.017649531364440918
Batch 4/64 loss: -0.025708436965942383
Batch 5/64 loss: -0.0355144739151001
Batch 6/64 loss: 0.002721726894378662
Batch 7/64 loss: -0.022194385528564453
Batch 8/64 loss: -0.034412264823913574
Batch 9/64 loss: -0.025739610195159912
Batch 10/64 loss: -0.015899062156677246
Batch 11/64 loss: -0.011609315872192383
Batch 12/64 loss: -0.028396785259246826
Batch 13/64 loss: -0.017271697521209717
Batch 14/64 loss: -0.028526484966278076
Batch 15/64 loss: -0.028340578079223633
Batch 16/64 loss: -0.020669996738433838
Batch 17/64 loss: 0.015998423099517822
Batch 18/64 loss: -0.01239699125289917
Batch 19/64 loss: -0.044296979904174805
Batch 20/64 loss: -0.027272462844848633
Batch 21/64 loss: -0.0019438862800598145
Batch 22/64 loss: -0.014474749565124512
Batch 23/64 loss: -0.009103775024414062
Batch 24/64 loss: -0.02631056308746338
Batch 25/64 loss: -0.008413910865783691
Batch 26/64 loss: -0.010123074054718018
Batch 27/64 loss: -0.033271610736846924
Batch 28/64 loss: -0.01802349090576172
Batch 29/64 loss: -0.020017921924591064
Batch 30/64 loss: -0.030852675437927246
Batch 31/64 loss: -0.04833334684371948
Batch 32/64 loss: -0.027142703533172607
Batch 33/64 loss: -0.02120339870452881
Batch 34/64 loss: -0.00404047966003418
Batch 35/64 loss: -0.026370584964752197
Batch 36/64 loss: -0.022921085357666016
Batch 37/64 loss: -0.020386219024658203
Batch 38/64 loss: -0.027234017848968506
Batch 39/64 loss: -0.030024826526641846
Batch 40/64 loss: -0.018118619918823242
Batch 41/64 loss: -0.005003929138183594
Batch 42/64 loss: -0.019475936889648438
Batch 43/64 loss: -0.03867274522781372
Batch 44/64 loss: -0.034926533699035645
Batch 45/64 loss: -0.02266615629196167
Batch 46/64 loss: -0.02020198106765747
Batch 47/64 loss: -0.031171083450317383
Batch 48/64 loss: -0.02189415693283081
Batch 49/64 loss: -0.019070148468017578
Batch 50/64 loss: -0.02955251932144165
Batch 51/64 loss: -0.028867125511169434
Batch 52/64 loss: -0.013325273990631104
Batch 53/64 loss: -0.036415934562683105
Batch 54/64 loss: -0.016833603382110596
Batch 55/64 loss: -0.01034313440322876
Batch 56/64 loss: -0.04064279794692993
Batch 57/64 loss: -0.034920692443847656
Batch 58/64 loss: -0.010598540306091309
Batch 59/64 loss: -0.041325926780700684
Batch 60/64 loss: -0.03776663541793823
Batch 61/64 loss: -0.011980593204498291
Batch 62/64 loss: -0.01386415958404541
Batch 63/64 loss: -0.01787853240966797
Batch 64/64 loss: -0.035472095012664795
Epoch 105  Train loss: -0.022596374446270513  Val loss: 0.011113151447060182
Epoch 106
-------------------------------
Batch 1/64 loss: -0.020606279373168945
Batch 2/64 loss: -0.028610408306121826
Batch 3/64 loss: -0.02050042152404785
Batch 4/64 loss: -0.03653782606124878
Batch 5/64 loss: -0.023883581161499023
Batch 6/64 loss: -0.04349273443222046
Batch 7/64 loss: -0.03289562463760376
Batch 8/64 loss: -0.03000771999359131
Batch 9/64 loss: 0.005813539028167725
Batch 10/64 loss: -0.019591867923736572
Batch 11/64 loss: -0.0405658483505249
Batch 12/64 loss: -0.04635798931121826
Batch 13/64 loss: -0.03669238090515137
Batch 14/64 loss: -0.041815876960754395
Batch 15/64 loss: 0.0116768479347229
Batch 16/64 loss: -0.024399876594543457
Batch 17/64 loss: 0.004130661487579346
Batch 18/64 loss: -0.018662452697753906
Batch 19/64 loss: -0.01629573106765747
Batch 20/64 loss: -0.02898383140563965
Batch 21/64 loss: -0.014937400817871094
Batch 22/64 loss: -0.011044025421142578
Batch 23/64 loss: -0.03781270980834961
Batch 24/64 loss: -0.035424768924713135
Batch 25/64 loss: -0.03270840644836426
Batch 26/64 loss: -0.030430316925048828
Batch 27/64 loss: -0.03230804204940796
Batch 28/64 loss: -0.022037982940673828
Batch 29/64 loss: -0.023502469062805176
Batch 30/64 loss: -0.023655354976654053
Batch 31/64 loss: -0.03773057460784912
Batch 32/64 loss: -0.03233373165130615
Batch 33/64 loss: -0.03217118978500366
Batch 34/64 loss: -0.027796804904937744
Batch 35/64 loss: -0.013378679752349854
Batch 36/64 loss: -0.024182677268981934
Batch 37/64 loss: -0.019237160682678223
Batch 38/64 loss: -0.03417706489562988
Batch 39/64 loss: -0.0024209022521972656
Batch 40/64 loss: -0.02287006378173828
Batch 41/64 loss: -0.00494384765625
Batch 42/64 loss: -0.011173784732818604
Batch 43/64 loss: -0.038405656814575195
Batch 44/64 loss: -0.027312040328979492
Batch 45/64 loss: -0.0273820161819458
Batch 46/64 loss: -0.060937583446502686
Batch 47/64 loss: -0.03590357303619385
Batch 48/64 loss: -0.012528181076049805
Batch 49/64 loss: -0.032661497592926025
Batch 50/64 loss: -0.022345662117004395
Batch 51/64 loss: -0.03196758031845093
Batch 52/64 loss: -0.03720712661743164
Batch 53/64 loss: -0.022438883781433105
Batch 54/64 loss: -0.039838433265686035
Batch 55/64 loss: -0.01582103967666626
Batch 56/64 loss: -0.016699016094207764
Batch 57/64 loss: -0.025149166584014893
Batch 58/64 loss: -0.0212554931640625
Batch 59/64 loss: -0.03360950946807861
Batch 60/64 loss: -0.020908832550048828
Batch 61/64 loss: 0.006592512130737305
Batch 62/64 loss: -0.009692907333374023
Batch 63/64 loss: -0.017804980278015137
Batch 64/64 loss: -0.02442830801010132
Epoch 106  Train loss: -0.024724004081651277  Val loss: 0.010840567731365715
Epoch 107
-------------------------------
Batch 1/64 loss: -0.03186655044555664
Batch 2/64 loss: -0.04455369710922241
Batch 3/64 loss: -0.020157039165496826
Batch 4/64 loss: -0.01804351806640625
Batch 5/64 loss: -0.013245761394500732
Batch 6/64 loss: -0.04284745454788208
Batch 7/64 loss: -0.011972129344940186
Batch 8/64 loss: -0.02193397283554077
Batch 9/64 loss: -0.04495823383331299
Batch 10/64 loss: -0.03308504819869995
Batch 11/64 loss: -0.038938164710998535
Batch 12/64 loss: -0.0350872278213501
Batch 13/64 loss: -0.03851896524429321
Batch 14/64 loss: -0.012587666511535645
Batch 15/64 loss: -0.02130359411239624
Batch 16/64 loss: -0.03710728883743286
Batch 17/64 loss: -0.03147530555725098
Batch 18/64 loss: -0.031102001667022705
Batch 19/64 loss: -0.023149430751800537
Batch 20/64 loss: -0.024998188018798828
Batch 21/64 loss: -0.036968350410461426
Batch 22/64 loss: -0.0415879487991333
Batch 23/64 loss: -0.015512049198150635
Batch 24/64 loss: -0.02750563621520996
Batch 25/64 loss: -0.03589522838592529
Batch 26/64 loss: -0.033166587352752686
Batch 27/64 loss: -0.0078057050704956055
Batch 28/64 loss: -0.029977798461914062
Batch 29/64 loss: -0.02300494909286499
Batch 30/64 loss: -0.01831960678100586
Batch 31/64 loss: -0.026967525482177734
Batch 32/64 loss: -0.031275272369384766
Batch 33/64 loss: -0.022264599800109863
Batch 34/64 loss: -0.029989182949066162
Batch 35/64 loss: -0.02065873146057129
Batch 36/64 loss: -0.028943121433258057
Batch 37/64 loss: -0.03378486633300781
Batch 38/64 loss: -0.026145339012145996
Batch 39/64 loss: -0.01514577865600586
Batch 40/64 loss: -0.028602302074432373
Batch 41/64 loss: -0.035646915435791016
Batch 42/64 loss: -0.0028058290481567383
Batch 43/64 loss: -0.04536181688308716
Batch 44/64 loss: -0.04275292158126831
Batch 45/64 loss: -0.0001291036605834961
Batch 46/64 loss: -0.026616930961608887
Batch 47/64 loss: -0.036215782165527344
Batch 48/64 loss: -0.03583025932312012
Batch 49/64 loss: -0.041038453578948975
Batch 50/64 loss: -0.017899513244628906
Batch 51/64 loss: -0.02963113784790039
Batch 52/64 loss: -0.03576481342315674
Batch 53/64 loss: -0.023829519748687744
Batch 54/64 loss: -0.036622047424316406
Batch 55/64 loss: 0.014774322509765625
Batch 56/64 loss: -0.019183754920959473
Batch 57/64 loss: -0.0369105339050293
Batch 58/64 loss: -0.0326533317565918
Batch 59/64 loss: -0.021607697010040283
Batch 60/64 loss: -0.03642582893371582
Batch 61/64 loss: -0.012993037700653076
Batch 62/64 loss: -0.02361851930618286
Batch 63/64 loss: -0.00023114681243896484
Batch 64/64 loss: -0.047193825244903564
Epoch 107  Train loss: -0.02705634131151087  Val loss: 0.002659620697965327
Saving best model, epoch: 107
Epoch 108
-------------------------------
Batch 1/64 loss: -0.02620309591293335
Batch 2/64 loss: -0.03685963153839111
Batch 3/64 loss: -0.0402643084526062
Batch 4/64 loss: -0.023562967777252197
Batch 5/64 loss: -0.03393745422363281
Batch 6/64 loss: -0.03666341304779053
Batch 7/64 loss: -0.0252649188041687
Batch 8/64 loss: -0.020782470703125
Batch 9/64 loss: -0.032247304916381836
Batch 10/64 loss: -0.020170211791992188
Batch 11/64 loss: -0.011967599391937256
Batch 12/64 loss: -0.037040889263153076
Batch 13/64 loss: -0.040165066719055176
Batch 14/64 loss: -0.03478693962097168
Batch 15/64 loss: -0.007637619972229004
Batch 16/64 loss: -0.033548593521118164
Batch 17/64 loss: -0.031499385833740234
Batch 18/64 loss: -0.03420311212539673
Batch 19/64 loss: -0.01989567279815674
Batch 20/64 loss: -0.02965843677520752
Batch 21/64 loss: -0.03006303310394287
Batch 22/64 loss: -0.028322815895080566
Batch 23/64 loss: -0.01849985122680664
Batch 24/64 loss: -0.037119925022125244
Batch 25/64 loss: -0.03515422344207764
Batch 26/64 loss: -0.018706858158111572
Batch 27/64 loss: -0.0355493426322937
Batch 28/64 loss: -0.04653429985046387
Batch 29/64 loss: -0.03676110506057739
Batch 30/64 loss: -0.03976142406463623
Batch 31/64 loss: -0.02821934223175049
Batch 32/64 loss: -0.012641847133636475
Batch 33/64 loss: 0.004922807216644287
Batch 34/64 loss: -0.017803609371185303
Batch 35/64 loss: -0.0055373311042785645
Batch 36/64 loss: -0.021637797355651855
Batch 37/64 loss: -0.005339443683624268
Batch 38/64 loss: -0.04438185691833496
Batch 39/64 loss: -0.02247774600982666
Batch 40/64 loss: -0.05762672424316406
Batch 41/64 loss: -0.029987335205078125
Batch 42/64 loss: -0.014841079711914062
Batch 43/64 loss: -0.013599991798400879
Batch 44/64 loss: -0.028608202934265137
Batch 45/64 loss: -0.00941556692123413
Batch 46/64 loss: -0.02055126428604126
Batch 47/64 loss: -0.03650254011154175
Batch 48/64 loss: -0.023312389850616455
Batch 49/64 loss: -0.04334026575088501
Batch 50/64 loss: -0.011415600776672363
Batch 51/64 loss: -0.027779459953308105
Batch 52/64 loss: -0.027762770652770996
Batch 53/64 loss: -0.027082622051239014
Batch 54/64 loss: -0.026437699794769287
Batch 55/64 loss: 0.007596492767333984
Batch 56/64 loss: -0.038739562034606934
Batch 57/64 loss: -0.029749155044555664
Batch 58/64 loss: -0.024248838424682617
Batch 59/64 loss: -0.009862303733825684
Batch 60/64 loss: -0.01738053560256958
Batch 61/64 loss: -0.028132736682891846
Batch 62/64 loss: -0.028398454189300537
Batch 63/64 loss: -0.016715168952941895
Batch 64/64 loss: -0.021576881408691406
Epoch 108  Train loss: -0.02597685608209348  Val loss: 0.004477387646219575
Epoch 109
-------------------------------
Batch 1/64 loss: -0.05372416973114014
Batch 2/64 loss: -0.027585864067077637
Batch 3/64 loss: -0.036434948444366455
Batch 4/64 loss: -0.024371862411499023
Batch 5/64 loss: -0.04199087619781494
Batch 6/64 loss: -0.022099733352661133
Batch 7/64 loss: -0.03421187400817871
Batch 8/64 loss: -0.02631908655166626
Batch 9/64 loss: -0.03872567415237427
Batch 10/64 loss: -0.03366494178771973
Batch 11/64 loss: -0.01721501350402832
Batch 12/64 loss: -0.027128398418426514
Batch 13/64 loss: -0.01808720827102661
Batch 14/64 loss: -0.028037667274475098
Batch 15/64 loss: -0.039385318756103516
Batch 16/64 loss: -0.038258373737335205
Batch 17/64 loss: -0.019152820110321045
Batch 18/64 loss: -0.023758649826049805
Batch 19/64 loss: -0.010624527931213379
Batch 20/64 loss: -0.004336357116699219
Batch 21/64 loss: -0.03360271453857422
Batch 22/64 loss: -0.030316710472106934
Batch 23/64 loss: -0.03384298086166382
Batch 24/64 loss: -0.020797789096832275
Batch 25/64 loss: -0.029325246810913086
Batch 26/64 loss: -0.04151397943496704
Batch 27/64 loss: -0.051631808280944824
Batch 28/64 loss: -0.04740709066390991
Batch 29/64 loss: -0.025459706783294678
Batch 30/64 loss: -0.007849693298339844
Batch 31/64 loss: -0.0404735803604126
Batch 32/64 loss: -0.010286092758178711
Batch 33/64 loss: -0.03716623783111572
Batch 34/64 loss: -0.04418915510177612
Batch 35/64 loss: -0.03515052795410156
Batch 36/64 loss: -0.023173153400421143
Batch 37/64 loss: -0.015843212604522705
Batch 38/64 loss: -0.03907221555709839
Batch 39/64 loss: -0.04490506649017334
Batch 40/64 loss: -0.03188425302505493
Batch 41/64 loss: -0.018526315689086914
Batch 42/64 loss: -0.041826069355010986
Batch 43/64 loss: -0.004562497138977051
Batch 44/64 loss: -0.033119022846221924
Batch 45/64 loss: -0.0005282759666442871
Batch 46/64 loss: -0.037377893924713135
Batch 47/64 loss: -0.0360943078994751
Batch 48/64 loss: -0.027415931224822998
Batch 49/64 loss: -0.02920156717300415
Batch 50/64 loss: -0.024292349815368652
Batch 51/64 loss: -0.015915751457214355
Batch 52/64 loss: -0.03712058067321777
Batch 53/64 loss: -0.024989604949951172
Batch 54/64 loss: -0.026425957679748535
Batch 55/64 loss: -0.04837799072265625
Batch 56/64 loss: -0.03664177656173706
Batch 57/64 loss: -0.02804011106491089
Batch 58/64 loss: -0.032761216163635254
Batch 59/64 loss: -0.02593594789505005
Batch 60/64 loss: -0.02145451307296753
Batch 61/64 loss: -0.03086221218109131
Batch 62/64 loss: -0.032889485359191895
Batch 63/64 loss: -0.02619314193725586
Batch 64/64 loss: -0.030676722526550293
Epoch 109  Train loss: -0.02937356303719913  Val loss: 0.007221015867908386
Epoch 110
-------------------------------
Batch 1/64 loss: -0.016696274280548096
Batch 2/64 loss: -0.04016518592834473
Batch 3/64 loss: -0.0234450101852417
Batch 4/64 loss: -0.046626389026641846
Batch 5/64 loss: -0.03733325004577637
Batch 6/64 loss: -0.05067312717437744
Batch 7/64 loss: -0.03191351890563965
Batch 8/64 loss: -0.016817986965179443
Batch 9/64 loss: -0.029933691024780273
Batch 10/64 loss: -0.015876948833465576
Batch 11/64 loss: -0.024222075939178467
Batch 12/64 loss: -0.027686476707458496
Batch 13/64 loss: -0.019786357879638672
Batch 14/64 loss: -0.03635108470916748
Batch 15/64 loss: -0.022335350513458252
Batch 16/64 loss: -0.03500378131866455
Batch 17/64 loss: -0.03523164987564087
Batch 18/64 loss: -0.041504621505737305
Batch 19/64 loss: -0.04124176502227783
Batch 20/64 loss: -0.024660766124725342
Batch 21/64 loss: -0.008750677108764648
Batch 22/64 loss: -0.04153162240982056
Batch 23/64 loss: -0.041234493255615234
Batch 24/64 loss: -0.012856364250183105
Batch 25/64 loss: -0.017884910106658936
Batch 26/64 loss: -0.041164934635162354
Batch 27/64 loss: -0.044075846672058105
Batch 28/64 loss: -0.023483693599700928
Batch 29/64 loss: -0.03129100799560547
Batch 30/64 loss: -0.029346704483032227
Batch 31/64 loss: -0.024341940879821777
Batch 32/64 loss: -0.04707622528076172
Batch 33/64 loss: -0.026570677757263184
Batch 34/64 loss: -0.03221738338470459
Batch 35/64 loss: -0.047314226627349854
Batch 36/64 loss: -0.03036653995513916
Batch 37/64 loss: -0.040468454360961914
Batch 38/64 loss: -0.022134065628051758
Batch 39/64 loss: -0.022184431552886963
Batch 40/64 loss: -0.035734713077545166
Batch 41/64 loss: -0.024539053440093994
Batch 42/64 loss: -0.04313385486602783
Batch 43/64 loss: -0.03542131185531616
Batch 44/64 loss: -0.0266302227973938
Batch 45/64 loss: -0.023415088653564453
Batch 46/64 loss: -0.028986573219299316
Batch 47/64 loss: -0.057320594787597656
Batch 48/64 loss: -0.010628163814544678
Batch 49/64 loss: -0.0013332366943359375
Batch 50/64 loss: -0.015394985675811768
Batch 51/64 loss: -0.022509753704071045
Batch 52/64 loss: -0.010068535804748535
Batch 53/64 loss: -0.03383743762969971
Batch 54/64 loss: -0.023016393184661865
Batch 55/64 loss: -0.012243509292602539
Batch 56/64 loss: -0.027691900730133057
Batch 57/64 loss: -0.004418551921844482
Batch 58/64 loss: -0.015746653079986572
Batch 59/64 loss: 0.019504427909851074
Batch 60/64 loss: -0.019321680068969727
Batch 61/64 loss: -0.031085312366485596
Batch 62/64 loss: -0.02314436435699463
Batch 63/64 loss: -0.017081022262573242
Batch 64/64 loss: -0.0348658561706543
Epoch 110  Train loss: -0.02746897865744198  Val loss: 0.007263924452857054
Epoch 111
-------------------------------
Batch 1/64 loss: -0.016332626342773438
Batch 2/64 loss: -0.02298903465270996
Batch 3/64 loss: -0.03418135643005371
Batch 4/64 loss: -0.02999401092529297
Batch 5/64 loss: -0.050021350383758545
Batch 6/64 loss: -0.018336057662963867
Batch 7/64 loss: -0.025874555110931396
Batch 8/64 loss: -0.026736199855804443
Batch 9/64 loss: -0.03119295835494995
Batch 10/64 loss: -0.04315531253814697
Batch 11/64 loss: -0.027255654335021973
Batch 12/64 loss: -0.027665793895721436
Batch 13/64 loss: -0.023148655891418457
Batch 14/64 loss: -0.018101811408996582
Batch 15/64 loss: -0.041380107402801514
Batch 16/64 loss: -0.03209328651428223
Batch 17/64 loss: -0.030049383640289307
Batch 18/64 loss: -0.024572432041168213
Batch 19/64 loss: -0.026686787605285645
Batch 20/64 loss: -0.02786308526992798
Batch 21/64 loss: -0.01834273338317871
Batch 22/64 loss: -0.015331923961639404
Batch 23/64 loss: -0.03052741289138794
Batch 24/64 loss: -0.02468949556350708
Batch 25/64 loss: -0.03352314233779907
Batch 26/64 loss: -0.04101920127868652
Batch 27/64 loss: -0.012209415435791016
Batch 28/64 loss: -0.028455615043640137
Batch 29/64 loss: -0.03414648771286011
Batch 30/64 loss: -0.03229880332946777
Batch 31/64 loss: -0.02884232997894287
Batch 32/64 loss: -0.020216703414916992
Batch 33/64 loss: -0.014627933502197266
Batch 34/64 loss: -0.04257780313491821
Batch 35/64 loss: -0.03446030616760254
Batch 36/64 loss: -0.02358865737915039
Batch 37/64 loss: -0.037276387214660645
Batch 38/64 loss: -0.02270132303237915
Batch 39/64 loss: -0.015553891658782959
Batch 40/64 loss: 0.0026543736457824707
Batch 41/64 loss: -0.028571248054504395
Batch 42/64 loss: -0.022256076335906982
Batch 43/64 loss: -0.025758564472198486
Batch 44/64 loss: -0.025333404541015625
Batch 45/64 loss: -0.020826518535614014
Batch 46/64 loss: -0.028857529163360596
Batch 47/64 loss: -0.028048336505889893
Batch 48/64 loss: -0.024335265159606934
Batch 49/64 loss: -0.02746272087097168
Batch 50/64 loss: -0.024563312530517578
Batch 51/64 loss: -0.02869480848312378
Batch 52/64 loss: -0.017307817935943604
Batch 53/64 loss: -0.021637260913848877
Batch 54/64 loss: -0.03471189737319946
Batch 55/64 loss: -0.025467753410339355
Batch 56/64 loss: -0.01920771598815918
Batch 57/64 loss: -0.035480380058288574
Batch 58/64 loss: -0.046620070934295654
Batch 59/64 loss: -0.033326804637908936
Batch 60/64 loss: -0.035301804542541504
Batch 61/64 loss: -0.045000433921813965
Batch 62/64 loss: -0.0366441011428833
Batch 63/64 loss: -0.022899270057678223
Batch 64/64 loss: -0.02453291416168213
Epoch 111  Train loss: -0.027640446027119955  Val loss: 0.005641081693655846
Epoch 112
-------------------------------
Batch 1/64 loss: -0.03128474950790405
Batch 2/64 loss: -0.026365995407104492
Batch 3/64 loss: -0.042594075202941895
Batch 4/64 loss: -0.025585532188415527
Batch 5/64 loss: -0.027185380458831787
Batch 6/64 loss: -0.01850128173828125
Batch 7/64 loss: -0.020445525646209717
Batch 8/64 loss: -0.029518544673919678
Batch 9/64 loss: -0.03565359115600586
Batch 10/64 loss: -0.03323084115982056
Batch 11/64 loss: -0.028828799724578857
Batch 12/64 loss: -0.007974386215209961
Batch 13/64 loss: -0.0352749228477478
Batch 14/64 loss: -0.043679237365722656
Batch 15/64 loss: -0.03003978729248047
Batch 16/64 loss: -0.03000873327255249
Batch 17/64 loss: -0.035850703716278076
Batch 18/64 loss: -0.021095633506774902
Batch 19/64 loss: -0.03673285245895386
Batch 20/64 loss: -0.051557302474975586
Batch 21/64 loss: -0.02608644962310791
Batch 22/64 loss: -0.027676701545715332
Batch 23/64 loss: -0.027653872966766357
Batch 24/64 loss: -0.01986563205718994
Batch 25/64 loss: -0.007481575012207031
Batch 26/64 loss: -0.035835087299346924
Batch 27/64 loss: -0.02980947494506836
Batch 28/64 loss: -0.035629451274871826
Batch 29/64 loss: -0.03486275672912598
Batch 30/64 loss: -0.029080212116241455
Batch 31/64 loss: -0.04322969913482666
Batch 32/64 loss: -0.02507472038269043
Batch 33/64 loss: -0.030687808990478516
Batch 34/64 loss: -0.032640695571899414
Batch 35/64 loss: -0.030292093753814697
Batch 36/64 loss: -0.016654253005981445
Batch 37/64 loss: -0.04066282510757446
Batch 38/64 loss: -0.0113983154296875
Batch 39/64 loss: -0.016903400421142578
Batch 40/64 loss: -0.031356215476989746
Batch 41/64 loss: -0.055347561836242676
Batch 42/64 loss: -0.02672255039215088
Batch 43/64 loss: -0.036730408668518066
Batch 44/64 loss: -0.03138160705566406
Batch 45/64 loss: -0.044983625411987305
Batch 46/64 loss: -0.03349119424819946
Batch 47/64 loss: -0.033923566341400146
Batch 48/64 loss: -0.018418073654174805
Batch 49/64 loss: -0.03881305456161499
Batch 50/64 loss: -0.03332400321960449
Batch 51/64 loss: -0.03372907638549805
Batch 52/64 loss: -0.03996497392654419
Batch 53/64 loss: -0.026427507400512695
Batch 54/64 loss: -0.0324823260307312
Batch 55/64 loss: -0.03594815731048584
Batch 56/64 loss: -0.02693808078765869
Batch 57/64 loss: 0.003903329372406006
Batch 58/64 loss: -0.039703190326690674
Batch 59/64 loss: 0.0062937140464782715
Batch 60/64 loss: -0.035119593143463135
Batch 61/64 loss: -0.02682971954345703
Batch 62/64 loss: -0.035416364669799805
Batch 63/64 loss: -0.0320279598236084
Batch 64/64 loss: -0.027141332626342773
Epoch 112  Train loss: -0.029681108512130436  Val loss: 0.005565486413097054
Epoch 113
-------------------------------
Batch 1/64 loss: -0.03211164474487305
Batch 2/64 loss: -0.01421356201171875
Batch 3/64 loss: -0.022177696228027344
Batch 4/64 loss: -0.05037868022918701
Batch 5/64 loss: -0.028968751430511475
Batch 6/64 loss: -0.03739285469055176
Batch 7/64 loss: -0.030962049961090088
Batch 8/64 loss: -0.05046814680099487
Batch 9/64 loss: -0.04595917463302612
Batch 10/64 loss: -0.04357272386550903
Batch 11/64 loss: -0.02865445613861084
Batch 12/64 loss: -0.029236137866973877
Batch 13/64 loss: -0.021988630294799805
Batch 14/64 loss: -0.03931671380996704
Batch 15/64 loss: -0.02402341365814209
Batch 16/64 loss: -0.021083354949951172
Batch 17/64 loss: 0.007335245609283447
Batch 18/64 loss: -0.037687063217163086
Batch 19/64 loss: -0.03751993179321289
Batch 20/64 loss: -0.021972298622131348
Batch 21/64 loss: -0.029541373252868652
Batch 22/64 loss: -0.021132349967956543
Batch 23/64 loss: -0.014068305492401123
Batch 24/64 loss: -0.04092592000961304
Batch 25/64 loss: -0.009994864463806152
Batch 26/64 loss: -0.04783672094345093
Batch 27/64 loss: -0.013422131538391113
Batch 28/64 loss: -0.036061882972717285
Batch 29/64 loss: -0.03225719928741455
Batch 30/64 loss: -0.026636064052581787
Batch 31/64 loss: -0.04608410596847534
Batch 32/64 loss: -0.038749516010284424
Batch 33/64 loss: -0.037132084369659424
Batch 34/64 loss: -0.03401285409927368
Batch 35/64 loss: -0.03562527894973755
Batch 36/64 loss: -0.030996263027191162
Batch 37/64 loss: -0.037190914154052734
Batch 38/64 loss: -0.03664290904998779
Batch 39/64 loss: -0.038143813610076904
Batch 40/64 loss: -0.022092103958129883
Batch 41/64 loss: -0.014727234840393066
Batch 42/64 loss: -0.008575439453125
Batch 43/64 loss: -0.020476579666137695
Batch 44/64 loss: -0.027571678161621094
Batch 45/64 loss: -0.025211095809936523
Batch 46/64 loss: -0.03150010108947754
Batch 47/64 loss: -0.0281640887260437
Batch 48/64 loss: -0.02981579303741455
Batch 49/64 loss: -0.034746527671813965
Batch 50/64 loss: -0.04493081569671631
Batch 51/64 loss: -0.026249289512634277
Batch 52/64 loss: -0.042041003704071045
Batch 53/64 loss: -0.04505348205566406
Batch 54/64 loss: -0.04290175437927246
Batch 55/64 loss: -0.019804418087005615
Batch 56/64 loss: -0.04121297597885132
Batch 57/64 loss: -0.021590888500213623
Batch 58/64 loss: -0.050982654094696045
Batch 59/64 loss: -0.01856815814971924
Batch 60/64 loss: -0.04654586315155029
Batch 61/64 loss: -0.038323163986206055
Batch 62/64 loss: -0.03865021467208862
Batch 63/64 loss: -0.021869182586669922
Batch 64/64 loss: -0.04596596956253052
Epoch 113  Train loss: -0.031260950191348204  Val loss: 0.0032134064284386915
Epoch 114
-------------------------------
Batch 1/64 loss: -0.03662222623825073
Batch 2/64 loss: -0.019750773906707764
Batch 3/64 loss: -0.028712809085845947
Batch 4/64 loss: -0.040644168853759766
Batch 5/64 loss: -0.028981268405914307
Batch 6/64 loss: -0.024221718311309814
Batch 7/64 loss: -0.030366182327270508
Batch 8/64 loss: -0.02658247947692871
Batch 9/64 loss: -0.049290478229522705
Batch 10/64 loss: -0.03808635473251343
Batch 11/64 loss: -0.0328482985496521
Batch 12/64 loss: -0.03614151477813721
Batch 13/64 loss: -0.03983956575393677
Batch 14/64 loss: -0.03959852457046509
Batch 15/64 loss: -0.034358084201812744
Batch 16/64 loss: -0.036915481090545654
Batch 17/64 loss: -0.05802357196807861
Batch 18/64 loss: -0.02967900037765503
Batch 19/64 loss: -0.04235243797302246
Batch 20/64 loss: -0.020253300666809082
Batch 21/64 loss: -0.03453195095062256
Batch 22/64 loss: -0.02814793586730957
Batch 23/64 loss: 0.0027232766151428223
Batch 24/64 loss: -0.0343623161315918
Batch 25/64 loss: -0.0329134464263916
Batch 26/64 loss: -0.050534844398498535
Batch 27/64 loss: -0.0105820894241333
Batch 28/64 loss: -0.029567480087280273
Batch 29/64 loss: -0.03223884105682373
Batch 30/64 loss: -0.027571439743041992
Batch 31/64 loss: -0.03390169143676758
Batch 32/64 loss: -0.022978007793426514
Batch 33/64 loss: -0.037813782691955566
Batch 34/64 loss: -0.04599320888519287
Batch 35/64 loss: -0.02080512046813965
Batch 36/64 loss: -0.05243402719497681
Batch 37/64 loss: -0.049906134605407715
Batch 38/64 loss: -0.018686532974243164
Batch 39/64 loss: -0.0464596152305603
Batch 40/64 loss: -0.02453148365020752
Batch 41/64 loss: -0.015616536140441895
Batch 42/64 loss: -0.04453611373901367
Batch 43/64 loss: -0.012917816638946533
Batch 44/64 loss: -0.01904881000518799
Batch 45/64 loss: -0.0226747989654541
Batch 46/64 loss: -0.04287594556808472
Batch 47/64 loss: -0.05047541856765747
Batch 48/64 loss: -0.00973290205001831
Batch 49/64 loss: -0.01080864667892456
Batch 50/64 loss: -0.027782559394836426
Batch 51/64 loss: -0.03448164463043213
Batch 52/64 loss: -0.019161224365234375
Batch 53/64 loss: -0.025554239749908447
Batch 54/64 loss: -0.031944990158081055
Batch 55/64 loss: -0.03035914897918701
Batch 56/64 loss: -0.02707958221435547
Batch 57/64 loss: -0.021429777145385742
Batch 58/64 loss: -0.032841384410858154
Batch 59/64 loss: -0.016581714153289795
Batch 60/64 loss: -0.03524196147918701
Batch 61/64 loss: -0.04261058568954468
Batch 62/64 loss: -0.04051166772842407
Batch 63/64 loss: -0.037188589572906494
Batch 64/64 loss: -0.05174970626831055
Epoch 114  Train loss: -0.03160428626864564  Val loss: -0.002826554463901061
Saving best model, epoch: 114
Epoch 115
-------------------------------
Batch 1/64 loss: -0.025069832801818848
Batch 2/64 loss: -0.04240751266479492
Batch 3/64 loss: -0.03394252061843872
Batch 4/64 loss: -0.052492618560791016
Batch 5/64 loss: -0.03113532066345215
Batch 6/64 loss: -0.04360520839691162
Batch 7/64 loss: -0.021392345428466797
Batch 8/64 loss: -0.03193598985671997
Batch 9/64 loss: -0.04402613639831543
Batch 10/64 loss: -0.04847520589828491
Batch 11/64 loss: -0.03597456216812134
Batch 12/64 loss: -0.027760982513427734
Batch 13/64 loss: -0.053156137466430664
Batch 14/64 loss: -0.041082024574279785
Batch 15/64 loss: -0.030554115772247314
Batch 16/64 loss: -0.030896306037902832
Batch 17/64 loss: -0.037787437438964844
Batch 18/64 loss: -0.04768848419189453
Batch 19/64 loss: -0.026634931564331055
Batch 20/64 loss: -0.027629733085632324
Batch 21/64 loss: -0.02257370948791504
Batch 22/64 loss: -0.03412550687789917
Batch 23/64 loss: -0.029539823532104492
Batch 24/64 loss: -0.04264408349990845
Batch 25/64 loss: -0.039555370807647705
Batch 26/64 loss: -0.03880411386489868
Batch 27/64 loss: -0.03038609027862549
Batch 28/64 loss: -0.031949520111083984
Batch 29/64 loss: -0.02149796485900879
Batch 30/64 loss: -0.04632723331451416
Batch 31/64 loss: -0.02033323049545288
Batch 32/64 loss: -0.02582991123199463
Batch 33/64 loss: -0.04122567176818848
Batch 34/64 loss: -0.03280353546142578
Batch 35/64 loss: -0.04451406002044678
Batch 36/64 loss: -0.019136786460876465
Batch 37/64 loss: -0.041510701179504395
Batch 38/64 loss: -0.03187680244445801
Batch 39/64 loss: -0.01584458351135254
Batch 40/64 loss: -0.01748567819595337
Batch 41/64 loss: -0.02702087163925171
Batch 42/64 loss: -0.036994338035583496
Batch 43/64 loss: -0.02631092071533203
Batch 44/64 loss: 0.0032372474670410156
Batch 45/64 loss: -0.03017568588256836
Batch 46/64 loss: -0.03350299596786499
Batch 47/64 loss: -0.03831326961517334
Batch 48/64 loss: -0.03580749034881592
Batch 49/64 loss: -0.0391269326210022
Batch 50/64 loss: -0.045371055603027344
Batch 51/64 loss: -0.02301323413848877
Batch 52/64 loss: -0.02052485942840576
Batch 53/64 loss: -0.06128203868865967
Batch 54/64 loss: -0.03502148389816284
Batch 55/64 loss: -0.03516542911529541
Batch 56/64 loss: -0.04205036163330078
Batch 57/64 loss: -0.03380084037780762
Batch 58/64 loss: -0.03452765941619873
Batch 59/64 loss: -0.025839567184448242
Batch 60/64 loss: 0.001338362693786621
Batch 61/64 loss: -0.015406489372253418
Batch 62/64 loss: -0.030652642250061035
Batch 63/64 loss: -0.020355701446533203
Batch 64/64 loss: -0.026378870010375977
Epoch 115  Train loss: -0.03242485008987726  Val loss: 0.005726335384591748
Epoch 116
-------------------------------
Batch 1/64 loss: -0.038059353828430176
Batch 2/64 loss: -0.013703405857086182
Batch 3/64 loss: -0.03331953287124634
Batch 4/64 loss: -0.023756861686706543
Batch 5/64 loss: -0.044081687927246094
Batch 6/64 loss: -0.04346799850463867
Batch 7/64 loss: -0.04355841875076294
Batch 8/64 loss: -0.012534797191619873
Batch 9/64 loss: -0.031046748161315918
Batch 10/64 loss: -0.019016683101654053
Batch 11/64 loss: -0.037289321422576904
Batch 12/64 loss: -0.02661716938018799
Batch 13/64 loss: -0.03605329990386963
Batch 14/64 loss: -0.038344621658325195
Batch 15/64 loss: -0.01725900173187256
Batch 16/64 loss: -0.010081231594085693
Batch 17/64 loss: -0.018514156341552734
Batch 18/64 loss: -0.026778340339660645
Batch 19/64 loss: -0.03026360273361206
Batch 20/64 loss: -0.014997780323028564
Batch 21/64 loss: -0.030755996704101562
Batch 22/64 loss: 0.0024451017379760742
Batch 23/64 loss: -0.030260920524597168
Batch 24/64 loss: -0.03624832630157471
Batch 25/64 loss: -0.04433238506317139
Batch 26/64 loss: -0.02247178554534912
Batch 27/64 loss: -0.022208094596862793
Batch 28/64 loss: -0.025566041469573975
Batch 29/64 loss: -0.03480464220046997
Batch 30/64 loss: -0.02233189344406128
Batch 31/64 loss: -0.025364816188812256
Batch 32/64 loss: -0.00884014368057251
Batch 33/64 loss: -0.03435194492340088
Batch 34/64 loss: -0.04686671495437622
Batch 35/64 loss: -0.032822370529174805
Batch 36/64 loss: -0.03723019361495972
Batch 37/64 loss: -0.04872322082519531
Batch 38/64 loss: -0.040008723735809326
Batch 39/64 loss: -0.03222864866256714
Batch 40/64 loss: -0.03311365842819214
Batch 41/64 loss: -0.027387797832489014
Batch 42/64 loss: -0.041604459285736084
Batch 43/64 loss: -0.04734909534454346
Batch 44/64 loss: -0.013894498348236084
Batch 45/64 loss: -0.041559040546417236
Batch 46/64 loss: -0.030210375785827637
Batch 47/64 loss: -0.033312201499938965
Batch 48/64 loss: -0.034884870052337646
Batch 49/64 loss: -0.02504754066467285
Batch 50/64 loss: -0.045365333557128906
Batch 51/64 loss: -0.032622575759887695
Batch 52/64 loss: -0.024283289909362793
Batch 53/64 loss: -0.010861217975616455
Batch 54/64 loss: -0.050733864307403564
Batch 55/64 loss: -0.026426732540130615
Batch 56/64 loss: -0.036944031715393066
Batch 57/64 loss: -0.03988605737686157
Batch 58/64 loss: -0.02296137809753418
Batch 59/64 loss: -0.03475081920623779
Batch 60/64 loss: -0.02814406156539917
Batch 61/64 loss: -0.038170576095581055
Batch 62/64 loss: -0.03755462169647217
Batch 63/64 loss: -0.043274641036987305
Batch 64/64 loss: -0.032846689224243164
Epoch 116  Train loss: -0.03069323184443455  Val loss: 0.003278417890424171
Epoch 117
-------------------------------
Batch 1/64 loss: -0.04608029127120972
Batch 2/64 loss: -0.005626022815704346
Batch 3/64 loss: -0.01590198278427124
Batch 4/64 loss: -0.04131436347961426
Batch 5/64 loss: -0.04733556509017944
Batch 6/64 loss: -0.011631309986114502
Batch 7/64 loss: -0.030344009399414062
Batch 8/64 loss: -0.007958769798278809
Batch 9/64 loss: 0.00019180774688720703
Batch 10/64 loss: -0.03090810775756836
Batch 11/64 loss: -0.04132986068725586
Batch 12/64 loss: -0.030902504920959473
Batch 13/64 loss: -0.0337335467338562
Batch 14/64 loss: -0.03356492519378662
Batch 15/64 loss: -0.03216075897216797
Batch 16/64 loss: -0.04363858699798584
Batch 17/64 loss: -0.038111209869384766
Batch 18/64 loss: -0.045731425285339355
Batch 19/64 loss: -0.02720320224761963
Batch 20/64 loss: -0.028941452503204346
Batch 21/64 loss: -0.043373703956604004
Batch 22/64 loss: -0.041638851165771484
Batch 23/64 loss: -0.044196367263793945
Batch 24/64 loss: -0.05263632535934448
Batch 25/64 loss: -0.03580242395401001
Batch 26/64 loss: -0.01414424180984497
Batch 27/64 loss: -0.056379497051239014
Batch 28/64 loss: -0.018447160720825195
Batch 29/64 loss: -0.034687161445617676
Batch 30/64 loss: -0.03396493196487427
Batch 31/64 loss: -0.021913588047027588
Batch 32/64 loss: -0.051110684871673584
Batch 33/64 loss: -0.026363730430603027
Batch 34/64 loss: -0.013514399528503418
Batch 35/64 loss: -0.036669015884399414
Batch 36/64 loss: -0.0371021032333374
Batch 37/64 loss: -0.01303333044052124
Batch 38/64 loss: -0.021283328533172607
Batch 39/64 loss: -0.03356754779815674
Batch 40/64 loss: -0.03142058849334717
Batch 41/64 loss: -0.05371040105819702
Batch 42/64 loss: -0.03723376989364624
Batch 43/64 loss: -0.03872275352478027
Batch 44/64 loss: -0.040165066719055176
Batch 45/64 loss: -0.031836748123168945
Batch 46/64 loss: -0.03757268190383911
Batch 47/64 loss: -0.0344233512878418
Batch 48/64 loss: -0.014962613582611084
Batch 49/64 loss: -0.028866350650787354
Batch 50/64 loss: -0.035902440547943115
Batch 51/64 loss: -0.023339033126831055
Batch 52/64 loss: -0.0352402925491333
Batch 53/64 loss: -0.028632283210754395
Batch 54/64 loss: -0.04287821054458618
Batch 55/64 loss: -0.03743422031402588
Batch 56/64 loss: -0.03400373458862305
Batch 57/64 loss: -0.041051268577575684
Batch 58/64 loss: -0.03475463390350342
Batch 59/64 loss: -0.024105608463287354
Batch 60/64 loss: -0.042552411556243896
Batch 61/64 loss: -0.03949552774429321
Batch 62/64 loss: -0.04791015386581421
Batch 63/64 loss: -0.03158283233642578
Batch 64/64 loss: -0.019406914710998535
Epoch 117  Train loss: -0.03269657480950449  Val loss: -0.0005967528959320174
Epoch 118
-------------------------------
Batch 1/64 loss: -0.047087788581848145
Batch 2/64 loss: -0.051963984966278076
Batch 3/64 loss: -0.030594170093536377
Batch 4/64 loss: -0.02551978826522827
Batch 5/64 loss: -0.04238015413284302
Batch 6/64 loss: -0.0320093035697937
Batch 7/64 loss: -0.04701530933380127
Batch 8/64 loss: -0.037465572357177734
Batch 9/64 loss: -0.034418344497680664
Batch 10/64 loss: -0.0136374831199646
Batch 11/64 loss: -0.029984772205352783
Batch 12/64 loss: -0.03422433137893677
Batch 13/64 loss: -0.01986825466156006
Batch 14/64 loss: -0.023567497730255127
Batch 15/64 loss: -0.03185272216796875
Batch 16/64 loss: -0.029428482055664062
Batch 17/64 loss: -0.039252281188964844
Batch 18/64 loss: 0.0018335580825805664
Batch 19/64 loss: -0.019865453243255615
Batch 20/64 loss: -0.02272188663482666
Batch 21/64 loss: -0.03331488370895386
Batch 22/64 loss: -0.040507733821868896
Batch 23/64 loss: -0.03324693441390991
Batch 24/64 loss: -0.04071605205535889
Batch 25/64 loss: -0.031288862228393555
Batch 26/64 loss: -0.029153168201446533
Batch 27/64 loss: -0.026566684246063232
Batch 28/64 loss: -0.02869892120361328
Batch 29/64 loss: -0.04541885852813721
Batch 30/64 loss: -0.03795182704925537
Batch 31/64 loss: -0.012220561504364014
Batch 32/64 loss: -0.02383720874786377
Batch 33/64 loss: -0.03218764066696167
Batch 34/64 loss: -0.03273981809616089
Batch 35/64 loss: -0.03930795192718506
Batch 36/64 loss: -0.04026377201080322
Batch 37/64 loss: -0.024950504302978516
Batch 38/64 loss: -0.03680628538131714
Batch 39/64 loss: -0.04844033718109131
Batch 40/64 loss: -0.02787989377975464
Batch 41/64 loss: -0.015745878219604492
Batch 42/64 loss: -0.04746919870376587
Batch 43/64 loss: -0.041222453117370605
Batch 44/64 loss: -0.038537025451660156
Batch 45/64 loss: -0.02852654457092285
Batch 46/64 loss: -0.03690391778945923
Batch 47/64 loss: -0.01598513126373291
Batch 48/64 loss: -0.03159576654434204
Batch 49/64 loss: -0.021836042404174805
Batch 50/64 loss: -0.040762364864349365
Batch 51/64 loss: -0.030187487602233887
Batch 52/64 loss: -0.02749931812286377
Batch 53/64 loss: -0.032088398933410645
Batch 54/64 loss: -0.04781317710876465
Batch 55/64 loss: -0.050825536251068115
Batch 56/64 loss: -0.042737722396850586
Batch 57/64 loss: -0.03624391555786133
Batch 58/64 loss: -0.03557819128036499
Batch 59/64 loss: -0.03949737548828125
Batch 60/64 loss: -0.03906315565109253
Batch 61/64 loss: -0.01994025707244873
Batch 62/64 loss: -0.040503859519958496
Batch 63/64 loss: -0.03546863794326782
Batch 64/64 loss: -0.04591381549835205
Epoch 118  Train loss: -0.03305080217473647  Val loss: 0.0016513380807699617
Epoch 119
-------------------------------
Batch 1/64 loss: -0.046017229557037354
Batch 2/64 loss: -0.005462825298309326
Batch 3/64 loss: -0.05121171474456787
Batch 4/64 loss: 0.00731348991394043
Batch 5/64 loss: -0.06250268220901489
Batch 6/64 loss: -0.024457216262817383
Batch 7/64 loss: -0.016655564308166504
Batch 8/64 loss: -0.03916710615158081
Batch 9/64 loss: -0.005883336067199707
Batch 10/64 loss: -0.04696393013000488
Batch 11/64 loss: -0.04550141096115112
Batch 12/64 loss: -0.037931740283966064
Batch 13/64 loss: -0.029668092727661133
Batch 14/64 loss: -0.03822171688079834
Batch 15/64 loss: -0.04637181758880615
Batch 16/64 loss: -0.033531367778778076
Batch 17/64 loss: -0.041706860065460205
Batch 18/64 loss: -0.04199564456939697
Batch 19/64 loss: -0.06754028797149658
Batch 20/64 loss: -0.04795259237289429
Batch 21/64 loss: -0.043273985385894775
Batch 22/64 loss: -0.03634488582611084
Batch 23/64 loss: -0.04870861768722534
Batch 24/64 loss: -0.026731371879577637
Batch 25/64 loss: -0.030014872550964355
Batch 26/64 loss: -0.032325148582458496
Batch 27/64 loss: -0.04101485013961792
Batch 28/64 loss: -0.025438129901885986
Batch 29/64 loss: -0.03236889839172363
Batch 30/64 loss: -0.04485088586807251
Batch 31/64 loss: -0.041744232177734375
Batch 32/64 loss: -0.01990807056427002
Batch 33/64 loss: -0.03643321990966797
Batch 34/64 loss: -0.05219393968582153
Batch 35/64 loss: -0.04816889762878418
Batch 36/64 loss: -0.04054456949234009
Batch 37/64 loss: -0.04787099361419678
Batch 38/64 loss: -0.04792451858520508
Batch 39/64 loss: -0.02676093578338623
Batch 40/64 loss: -0.04393351078033447
Batch 41/64 loss: -0.015886664390563965
Batch 42/64 loss: -0.05704575777053833
Batch 43/64 loss: -0.016354739665985107
Batch 44/64 loss: -0.04625946283340454
Batch 45/64 loss: -0.03390532732009888
Batch 46/64 loss: -0.04296219348907471
Batch 47/64 loss: -0.0317612886428833
Batch 48/64 loss: -0.03723299503326416
Batch 49/64 loss: -0.05326145887374878
Batch 50/64 loss: -0.049758195877075195
Batch 51/64 loss: -0.028963029384613037
Batch 52/64 loss: -0.02565091848373413
Batch 53/64 loss: -0.016962170600891113
Batch 54/64 loss: -0.04300349950790405
Batch 55/64 loss: -0.039098381996154785
Batch 56/64 loss: -0.047218501567840576
Batch 57/64 loss: -0.02904874086380005
Batch 58/64 loss: -0.03768002986907959
Batch 59/64 loss: -0.02215707302093506
Batch 60/64 loss: -0.033069729804992676
Batch 61/64 loss: -0.014021694660186768
Batch 62/64 loss: -0.040883004665374756
Batch 63/64 loss: -0.017735064029693604
Batch 64/64 loss: -0.04903256893157959
Epoch 119  Train loss: -0.036151820070603316  Val loss: 0.0035712731253240526
Epoch 120
-------------------------------
Batch 1/64 loss: -0.042029500007629395
Batch 2/64 loss: -0.01954066753387451
Batch 3/64 loss: -0.03263521194458008
Batch 4/64 loss: -0.023453593254089355
Batch 5/64 loss: -0.05430799722671509
Batch 6/64 loss: -0.021963298320770264
Batch 7/64 loss: -0.051837146282196045
Batch 8/64 loss: -0.048019349575042725
Batch 9/64 loss: -0.0458797812461853
Batch 10/64 loss: -0.022942304611206055
Batch 11/64 loss: -0.03234744071960449
Batch 12/64 loss: -0.03447425365447998
Batch 13/64 loss: -0.04723072052001953
Batch 14/64 loss: -0.022917985916137695
Batch 15/64 loss: -0.03417688608169556
Batch 16/64 loss: -0.04117989540100098
Batch 17/64 loss: -0.030468344688415527
Batch 18/64 loss: -0.03204554319381714
Batch 19/64 loss: 0.002950131893157959
Batch 20/64 loss: -0.010080158710479736
Batch 21/64 loss: -0.04824328422546387
Batch 22/64 loss: -0.03639626502990723
Batch 23/64 loss: -0.041484832763671875
Batch 24/64 loss: -0.03885388374328613
Batch 25/64 loss: -0.031014561653137207
Batch 26/64 loss: -0.045777082443237305
Batch 27/64 loss: -0.044352054595947266
Batch 28/64 loss: -0.04468345642089844
Batch 29/64 loss: -0.04109954833984375
Batch 30/64 loss: -0.017681479454040527
Batch 31/64 loss: -0.03148287534713745
Batch 32/64 loss: -0.04217171669006348
Batch 33/64 loss: -0.027035653591156006
Batch 34/64 loss: -0.0417780876159668
Batch 35/64 loss: -0.046866416931152344
Batch 36/64 loss: -0.02636432647705078
Batch 37/64 loss: -0.026244819164276123
Batch 38/64 loss: -0.04004096984863281
Batch 39/64 loss: -0.04072976112365723
Batch 40/64 loss: -0.04734683036804199
Batch 41/64 loss: -0.022837817668914795
Batch 42/64 loss: -0.01892077922821045
Batch 43/64 loss: -0.029969334602355957
Batch 44/64 loss: -0.046977996826171875
Batch 45/64 loss: -0.01987898349761963
Batch 46/64 loss: -0.008207976818084717
Batch 47/64 loss: -0.011484146118164062
Batch 48/64 loss: -0.022409021854400635
Batch 49/64 loss: -0.027056455612182617
Batch 50/64 loss: -0.03792309761047363
Batch 51/64 loss: -0.023581743240356445
Batch 52/64 loss: -0.03261953592300415
Batch 53/64 loss: -0.02249300479888916
Batch 54/64 loss: -0.011005640029907227
Batch 55/64 loss: -0.044191956520080566
Batch 56/64 loss: -0.04445463418960571
Batch 57/64 loss: -0.03576207160949707
Batch 58/64 loss: -0.026593327522277832
Batch 59/64 loss: -0.03691047430038452
Batch 60/64 loss: -0.04628652334213257
Batch 61/64 loss: -0.03836631774902344
Batch 62/64 loss: -0.036939382553100586
Batch 63/64 loss: -0.015924274921417236
Batch 64/64 loss: -0.029322028160095215
Epoch 120  Train loss: -0.032737472478081196  Val loss: 0.003980636391852729
Epoch 121
-------------------------------
Batch 1/64 loss: -0.034108877182006836
Batch 2/64 loss: -0.05781525373458862
Batch 3/64 loss: -0.03480124473571777
Batch 4/64 loss: -0.024863600730895996
Batch 5/64 loss: -0.044764697551727295
Batch 6/64 loss: -0.034779489040374756
Batch 7/64 loss: -0.02626478672027588
Batch 8/64 loss: -0.04345494508743286
Batch 9/64 loss: -0.045255064964294434
Batch 10/64 loss: -0.05367499589920044
Batch 11/64 loss: -0.034479498863220215
Batch 12/64 loss: -0.03514355421066284
Batch 13/64 loss: -0.00903022289276123
Batch 14/64 loss: -0.022392988204956055
Batch 15/64 loss: -0.030112385749816895
Batch 16/64 loss: -0.030699312686920166
Batch 17/64 loss: -0.0387040376663208
Batch 18/64 loss: -0.04038643836975098
Batch 19/64 loss: -0.015717267990112305
Batch 20/64 loss: -0.039376139640808105
Batch 21/64 loss: -0.04370427131652832
Batch 22/64 loss: -0.04136800765991211
Batch 23/64 loss: -0.015008091926574707
Batch 24/64 loss: -0.04802083969116211
Batch 25/64 loss: -0.04582643508911133
Batch 26/64 loss: -0.047478318214416504
Batch 27/64 loss: -0.04814702272415161
Batch 28/64 loss: -0.018977761268615723
Batch 29/64 loss: -0.031922340393066406
Batch 30/64 loss: -0.02611398696899414
Batch 31/64 loss: -0.02752828598022461
Batch 32/64 loss: -0.05017489194869995
Batch 33/64 loss: -0.035414040088653564
Batch 34/64 loss: -0.03142428398132324
Batch 35/64 loss: -0.031988441944122314
Batch 36/64 loss: -0.04054415225982666
Batch 37/64 loss: -0.03401827812194824
Batch 38/64 loss: -0.032364606857299805
Batch 39/64 loss: -0.03971952199935913
Batch 40/64 loss: -0.02055734395980835
Batch 41/64 loss: -0.02819192409515381
Batch 42/64 loss: -0.03136038780212402
Batch 43/64 loss: -0.04578334093093872
Batch 44/64 loss: -0.02695620059967041
Batch 45/64 loss: -0.01202172040939331
Batch 46/64 loss: -0.03727775812149048
Batch 47/64 loss: -0.04076272249221802
Batch 48/64 loss: -0.030890047550201416
Batch 49/64 loss: -0.041503190994262695
Batch 50/64 loss: -0.057645440101623535
Batch 51/64 loss: 0.00035190582275390625
Batch 52/64 loss: -0.03454017639160156
Batch 53/64 loss: -0.03654468059539795
Batch 54/64 loss: -0.039469361305236816
Batch 55/64 loss: -0.016963660717010498
Batch 56/64 loss: -0.04033350944519043
Batch 57/64 loss: -0.04004549980163574
Batch 58/64 loss: 0.0036500096321105957
Batch 59/64 loss: -0.028343141078948975
Batch 60/64 loss: -0.038942813873291016
Batch 61/64 loss: -0.03009819984436035
Batch 62/64 loss: -0.010222136974334717
Batch 63/64 loss: -0.022404134273529053
Batch 64/64 loss: -0.028738439083099365
Epoch 121  Train loss: -0.03316043428346223  Val loss: 0.006656391514125969
Epoch 122
-------------------------------
Batch 1/64 loss: -0.04863852262496948
Batch 2/64 loss: -0.040111541748046875
Batch 3/64 loss: -0.041500091552734375
Batch 4/64 loss: -0.05455440282821655
Batch 5/64 loss: -0.03206634521484375
Batch 6/64 loss: -0.04929661750793457
Batch 7/64 loss: -0.041243016719818115
Batch 8/64 loss: -0.03641676902770996
Batch 9/64 loss: -0.045035600662231445
Batch 10/64 loss: -0.04327958822250366
Batch 11/64 loss: -0.04068666696548462
Batch 12/64 loss: -0.03506934642791748
Batch 13/64 loss: -0.02922976016998291
Batch 14/64 loss: -0.018179714679718018
Batch 15/64 loss: -0.03272116184234619
Batch 16/64 loss: -0.050023555755615234
Batch 17/64 loss: -0.04359900951385498
Batch 18/64 loss: -0.04365772008895874
Batch 19/64 loss: -0.04337263107299805
Batch 20/64 loss: -0.04081761837005615
Batch 21/64 loss: -0.03279751539230347
Batch 22/64 loss: -0.030752599239349365
Batch 23/64 loss: -0.033821821212768555
Batch 24/64 loss: -0.03117448091506958
Batch 25/64 loss: -0.03364652395248413
Batch 26/64 loss: -0.032460808753967285
Batch 27/64 loss: -0.037091732025146484
Batch 28/64 loss: -0.025017619132995605
Batch 29/64 loss: -0.04162520170211792
Batch 30/64 loss: -0.02963995933532715
Batch 31/64 loss: -0.0304335355758667
Batch 32/64 loss: -0.024806559085845947
Batch 33/64 loss: -0.03682887554168701
Batch 34/64 loss: -0.03049778938293457
Batch 35/64 loss: -0.031957924365997314
Batch 36/64 loss: -0.05014050006866455
Batch 37/64 loss: -0.026843369007110596
Batch 38/64 loss: -0.0428088903427124
Batch 39/64 loss: -0.020227372646331787
Batch 40/64 loss: -0.04047548770904541
Batch 41/64 loss: -0.029982328414916992
Batch 42/64 loss: -0.037837326526641846
Batch 43/64 loss: -0.028378963470458984
Batch 44/64 loss: -0.04901325702667236
Batch 45/64 loss: -0.04016530513763428
Batch 46/64 loss: -0.02415788173675537
Batch 47/64 loss: -0.019639670848846436
Batch 48/64 loss: -0.048889100551605225
Batch 49/64 loss: -0.04897099733352661
Batch 50/64 loss: -0.03130316734313965
Batch 51/64 loss: -0.056721270084381104
Batch 52/64 loss: -0.03567081689834595
Batch 53/64 loss: -0.05022549629211426
Batch 54/64 loss: -0.030025184154510498
Batch 55/64 loss: -0.029751896858215332
Batch 56/64 loss: -0.04827672243118286
Batch 57/64 loss: -0.018880009651184082
Batch 58/64 loss: -0.020167887210845947
Batch 59/64 loss: -0.03676271438598633
Batch 60/64 loss: -0.030779480934143066
Batch 61/64 loss: -0.04358375072479248
Batch 62/64 loss: -0.021271705627441406
Batch 63/64 loss: -0.028404414653778076
Batch 64/64 loss: -0.037064433097839355
Epoch 122  Train loss: -0.03622283795300652  Val loss: 0.004631632996588638
Epoch 123
-------------------------------
Batch 1/64 loss: -0.0488581657409668
Batch 2/64 loss: -0.029817819595336914
Batch 3/64 loss: -0.047624170780181885
Batch 4/64 loss: -0.031730592250823975
Batch 5/64 loss: -0.03627181053161621
Batch 6/64 loss: -0.06206989288330078
Batch 7/64 loss: -0.0371246337890625
Batch 8/64 loss: -0.05387657880783081
Batch 9/64 loss: -0.018902063369750977
Batch 10/64 loss: -0.050014376640319824
Batch 11/64 loss: -0.04319918155670166
Batch 12/64 loss: -0.04916590452194214
Batch 13/64 loss: -0.031067609786987305
Batch 14/64 loss: -0.026951193809509277
Batch 15/64 loss: -0.03737872838973999
Batch 16/64 loss: -0.05188107490539551
Batch 17/64 loss: -0.05012011528015137
Batch 18/64 loss: -0.040133774280548096
Batch 19/64 loss: -0.05552339553833008
Batch 20/64 loss: -0.02674466371536255
Batch 21/64 loss: -0.047715961933135986
Batch 22/64 loss: -0.04483366012573242
Batch 23/64 loss: -0.03893864154815674
Batch 24/64 loss: -0.03428679704666138
Batch 25/64 loss: -0.005608797073364258
Batch 26/64 loss: -0.05482637882232666
Batch 27/64 loss: -0.022316396236419678
Batch 28/64 loss: 0.0026816129684448242
Batch 29/64 loss: -0.06014728546142578
Batch 30/64 loss: -0.027306854724884033
Batch 31/64 loss: -0.03078317642211914
Batch 32/64 loss: -0.05337345600128174
Batch 33/64 loss: -0.04174584150314331
Batch 34/64 loss: -0.04876255989074707
Batch 35/64 loss: -0.02828669548034668
Batch 36/64 loss: -0.05104774236679077
Batch 37/64 loss: -0.021367311477661133
Batch 38/64 loss: -0.02458202838897705
Batch 39/64 loss: -0.01443469524383545
Batch 40/64 loss: -0.03065890073776245
Batch 41/64 loss: -0.03592991828918457
Batch 42/64 loss: -0.0459212064743042
Batch 43/64 loss: -0.028341948986053467
Batch 44/64 loss: -0.03773009777069092
Batch 45/64 loss: -0.034301042556762695
Batch 46/64 loss: -0.06360596418380737
Batch 47/64 loss: -0.04654085636138916
Batch 48/64 loss: -0.04940837621688843
Batch 49/64 loss: -0.04098302125930786
Batch 50/64 loss: -0.017852187156677246
Batch 51/64 loss: -0.026186764240264893
Batch 52/64 loss: -0.048603177070617676
Batch 53/64 loss: -0.025736510753631592
Batch 54/64 loss: -0.03705406188964844
Batch 55/64 loss: -0.04209339618682861
Batch 56/64 loss: -0.05689519643783569
Batch 57/64 loss: -0.03824019432067871
Batch 58/64 loss: -0.05162310600280762
Batch 59/64 loss: -0.04273420572280884
Batch 60/64 loss: -0.029088854789733887
Batch 61/64 loss: -0.0102766752243042
Batch 62/64 loss: -0.02955549955368042
Batch 63/64 loss: -0.0098494291305542
Batch 64/64 loss: -0.05155092477798462
Epoch 123  Train loss: -0.0375531325153276  Val loss: 0.002071397615871888
Epoch 124
-------------------------------
Batch 1/64 loss: -0.046686530113220215
Batch 2/64 loss: -0.06392282247543335
Batch 3/64 loss: -0.018702208995819092
Batch 4/64 loss: -0.03600233793258667
Batch 5/64 loss: -0.022389113903045654
Batch 6/64 loss: -0.04282665252685547
Batch 7/64 loss: -0.01203089952468872
Batch 8/64 loss: -0.019241631031036377
Batch 9/64 loss: -0.0352703332901001
Batch 10/64 loss: -0.03140151500701904
Batch 11/64 loss: -0.038070857524871826
Batch 12/64 loss: -0.02674686908721924
Batch 13/64 loss: -0.05256545543670654
Batch 14/64 loss: -0.05705451965332031
Batch 15/64 loss: -0.0487288236618042
Batch 16/64 loss: -0.04976445436477661
Batch 17/64 loss: -0.04055309295654297
Batch 18/64 loss: -0.03896820545196533
Batch 19/64 loss: -0.02321237325668335
Batch 20/64 loss: -0.05498397350311279
Batch 21/64 loss: -0.020833373069763184
Batch 22/64 loss: -0.03805279731750488
Batch 23/64 loss: -0.04150867462158203
Batch 24/64 loss: -0.053350210189819336
Batch 25/64 loss: -0.031956374645233154
Batch 26/64 loss: -0.040853142738342285
Batch 27/64 loss: -0.027471423149108887
Batch 28/64 loss: -0.03555715084075928
Batch 29/64 loss: -0.03729724884033203
Batch 30/64 loss: -0.020431578159332275
Batch 31/64 loss: -0.03151750564575195
Batch 32/64 loss: -0.039141178131103516
Batch 33/64 loss: -0.035529494285583496
Batch 34/64 loss: -0.0442807674407959
Batch 35/64 loss: -0.037224411964416504
Batch 36/64 loss: -0.04933524131774902
Batch 37/64 loss: -0.04236394166946411
Batch 38/64 loss: -0.03706371784210205
Batch 39/64 loss: -0.03419351577758789
Batch 40/64 loss: -0.04398268461227417
Batch 41/64 loss: -0.028724372386932373
Batch 42/64 loss: -0.026330113410949707
Batch 43/64 loss: -0.03827261924743652
Batch 44/64 loss: -0.04100966453552246
Batch 45/64 loss: -0.043970465660095215
Batch 46/64 loss: -0.03034806251525879
Batch 47/64 loss: -0.02477121353149414
Batch 48/64 loss: -0.038818955421447754
Batch 49/64 loss: -0.015462756156921387
Batch 50/64 loss: -0.03653538227081299
Batch 51/64 loss: -0.009353816509246826
Batch 52/64 loss: -0.034024596214294434
Batch 53/64 loss: -0.043840885162353516
Batch 54/64 loss: -0.031084120273590088
Batch 55/64 loss: -0.03006535768508911
Batch 56/64 loss: -0.02330195903778076
Batch 57/64 loss: -0.01755833625793457
Batch 58/64 loss: -0.02401655912399292
Batch 59/64 loss: -0.056330204010009766
Batch 60/64 loss: -0.04400056600570679
Batch 61/64 loss: -0.025254547595977783
Batch 62/64 loss: -0.036933302879333496
Batch 63/64 loss: -0.02714216709136963
Batch 64/64 loss: -0.05223417282104492
Epoch 124  Train loss: -0.035566945169486254  Val loss: 0.004118584890136195
Epoch 125
-------------------------------
Batch 1/64 loss: -0.03745102882385254
Batch 2/64 loss: -0.043252527713775635
Batch 3/64 loss: -0.03963136672973633
Batch 4/64 loss: -0.038886189460754395
Batch 5/64 loss: -0.03573620319366455
Batch 6/64 loss: -0.03726518154144287
Batch 7/64 loss: -0.04710191488265991
Batch 8/64 loss: -0.04282200336456299
Batch 9/64 loss: -0.04485958814620972
Batch 10/64 loss: -0.04679059982299805
Batch 11/64 loss: -0.05596041679382324
Batch 12/64 loss: -0.05788838863372803
Batch 13/64 loss: -0.04862934350967407
Batch 14/64 loss: -0.03734773397445679
Batch 15/64 loss: -0.032924771308898926
Batch 16/64 loss: -0.0284578800201416
Batch 17/64 loss: -0.040990352630615234
Batch 18/64 loss: -0.0440865159034729
Batch 19/64 loss: -0.06181448698043823
Batch 20/64 loss: -0.02692544460296631
Batch 21/64 loss: -0.02361398935317993
Batch 22/64 loss: -0.04892134666442871
Batch 23/64 loss: -0.04069817066192627
Batch 24/64 loss: -0.04845750331878662
Batch 25/64 loss: -0.04060351848602295
Batch 26/64 loss: -0.027294397354125977
Batch 27/64 loss: -0.051021575927734375
Batch 28/64 loss: -0.033369362354278564
Batch 29/64 loss: -0.053384244441986084
Batch 30/64 loss: -0.019034147262573242
Batch 31/64 loss: -0.02972358465194702
Batch 32/64 loss: -0.04836231470108032
Batch 33/64 loss: -0.04608356952667236
Batch 34/64 loss: -0.051357805728912354
Batch 35/64 loss: -0.06096374988555908
Batch 36/64 loss: -0.029481589794158936
Batch 37/64 loss: -0.037783801555633545
Batch 38/64 loss: -0.041386187076568604
Batch 39/64 loss: -0.052276611328125
Batch 40/64 loss: -0.0418584942817688
Batch 41/64 loss: -0.04197275638580322
Batch 42/64 loss: -0.05307042598724365
Batch 43/64 loss: -0.04367917776107788
Batch 44/64 loss: -0.033282577991485596
Batch 45/64 loss: -0.011241137981414795
Batch 46/64 loss: -0.03791779279708862
Batch 47/64 loss: -0.05063629150390625
Batch 48/64 loss: -0.030058443546295166
Batch 49/64 loss: -0.045008301734924316
Batch 50/64 loss: -0.029474198818206787
Batch 51/64 loss: -0.037684619426727295
Batch 52/64 loss: -0.038360536098480225
Batch 53/64 loss: -0.03302347660064697
Batch 54/64 loss: -0.021987557411193848
Batch 55/64 loss: -0.03235083818435669
Batch 56/64 loss: -0.03005814552307129
Batch 57/64 loss: -0.040295302867889404
Batch 58/64 loss: -0.033537983894348145
Batch 59/64 loss: -0.039996325969696045
Batch 60/64 loss: -0.041471123695373535
Batch 61/64 loss: -0.041463255882263184
Batch 62/64 loss: -0.05234944820404053
Batch 63/64 loss: -0.03794831037521362
Batch 64/64 loss: -0.03481167554855347
Epoch 125  Train loss: -0.04011725000306672  Val loss: -0.005691987542352316
Saving best model, epoch: 125
Epoch 126
-------------------------------
Batch 1/64 loss: -0.04092437028884888
Batch 2/64 loss: -0.05673050880432129
Batch 3/64 loss: -0.053305625915527344
Batch 4/64 loss: -0.04264962673187256
Batch 5/64 loss: -0.01770693063735962
Batch 6/64 loss: -0.04417222738265991
Batch 7/64 loss: -0.02949845790863037
Batch 8/64 loss: -0.03379565477371216
Batch 9/64 loss: -0.05485987663269043
Batch 10/64 loss: -0.04695093631744385
Batch 11/64 loss: -0.055646538734436035
Batch 12/64 loss: -0.04099011421203613
Batch 13/64 loss: -0.032097697257995605
Batch 14/64 loss: -0.049202024936676025
Batch 15/64 loss: -0.00377810001373291
Batch 16/64 loss: -0.04776811599731445
Batch 17/64 loss: -0.05325770378112793
Batch 18/64 loss: -0.05419719219207764
Batch 19/64 loss: -0.04392582178115845
Batch 20/64 loss: -0.04128783941268921
Batch 21/64 loss: -0.03194570541381836
Batch 22/64 loss: -0.0567207932472229
Batch 23/64 loss: -0.020872116088867188
Batch 24/64 loss: -0.05700594186782837
Batch 25/64 loss: -0.040935635566711426
Batch 26/64 loss: -0.04258930683135986
Batch 27/64 loss: -0.03193378448486328
Batch 28/64 loss: -0.05300968885421753
Batch 29/64 loss: -0.05022871494293213
Batch 30/64 loss: -0.04111933708190918
Batch 31/64 loss: -0.05631977319717407
Batch 32/64 loss: -0.030536770820617676
Batch 33/64 loss: -0.03942394256591797
Batch 34/64 loss: -0.062438011169433594
Batch 35/64 loss: -0.04993772506713867
Batch 36/64 loss: -0.026356935501098633
Batch 37/64 loss: -0.03934049606323242
Batch 38/64 loss: -0.047880470752716064
Batch 39/64 loss: -0.0377468466758728
Batch 40/64 loss: -0.014345943927764893
Batch 41/64 loss: -0.04636770486831665
Batch 42/64 loss: -0.045730769634246826
Batch 43/64 loss: -0.03966951370239258
Batch 44/64 loss: -0.024699628353118896
Batch 45/64 loss: -0.038014233112335205
Batch 46/64 loss: -0.04426091909408569
Batch 47/64 loss: -0.04986572265625
Batch 48/64 loss: -0.028995394706726074
Batch 49/64 loss: -0.037908732891082764
Batch 50/64 loss: -0.04982948303222656
Batch 51/64 loss: -0.0421636700630188
Batch 52/64 loss: -0.03528451919555664
Batch 53/64 loss: -0.011524796485900879
Batch 54/64 loss: -0.02924579381942749
Batch 55/64 loss: -0.0319254994392395
Batch 56/64 loss: -0.04390263557434082
Batch 57/64 loss: -0.041374802589416504
Batch 58/64 loss: -0.03985273838043213
Batch 59/64 loss: -0.03637194633483887
Batch 60/64 loss: -0.059819579124450684
Batch 61/64 loss: -0.02628016471862793
Batch 62/64 loss: -0.0388866662979126
Batch 63/64 loss: -0.03366440534591675
Batch 64/64 loss: -0.03745114803314209
Epoch 126  Train loss: -0.040426054655336864  Val loss: 0.0048805200766861645
Epoch 127
-------------------------------
Batch 1/64 loss: -0.03838437795639038
Batch 2/64 loss: -0.04871302843093872
Batch 3/64 loss: -0.0423700213432312
Batch 4/64 loss: -0.047681331634521484
Batch 5/64 loss: -0.03882211446762085
Batch 6/64 loss: -0.03459352254867554
Batch 7/64 loss: -0.054744601249694824
Batch 8/64 loss: -0.044719040393829346
Batch 9/64 loss: -0.04795563220977783
Batch 10/64 loss: -0.048851609230041504
Batch 11/64 loss: -0.03996157646179199
Batch 12/64 loss: -0.04982435703277588
Batch 13/64 loss: -0.053580284118652344
Batch 14/64 loss: -0.061064720153808594
Batch 15/64 loss: -0.03254181146621704
Batch 16/64 loss: -0.04986274242401123
Batch 17/64 loss: -0.035523831844329834
Batch 18/64 loss: -0.0479351282119751
Batch 19/64 loss: -0.04701358079910278
Batch 20/64 loss: -0.04273223876953125
Batch 21/64 loss: -0.03460890054702759
Batch 22/64 loss: -0.03555023670196533
Batch 23/64 loss: -0.046822965145111084
Batch 24/64 loss: -0.025245606899261475
Batch 25/64 loss: -0.049092113971710205
Batch 26/64 loss: -0.029918909072875977
Batch 27/64 loss: -0.017943918704986572
Batch 28/64 loss: -0.037207722663879395
Batch 29/64 loss: -0.03201037645339966
Batch 30/64 loss: -0.04078322649002075
Batch 31/64 loss: -0.013568460941314697
Batch 32/64 loss: -0.052696406841278076
Batch 33/64 loss: -0.03539317846298218
Batch 34/64 loss: -0.055094361305236816
Batch 35/64 loss: -0.041709303855895996
Batch 36/64 loss: -0.044096171855926514
Batch 37/64 loss: -0.037734150886535645
Batch 38/64 loss: -0.032647132873535156
Batch 39/64 loss: -0.03221607208251953
Batch 40/64 loss: -0.03310227394104004
Batch 41/64 loss: -0.031319379806518555
Batch 42/64 loss: -0.04706764221191406
Batch 43/64 loss: -0.054105401039123535
Batch 44/64 loss: -0.043853044509887695
Batch 45/64 loss: -0.047698140144348145
Batch 46/64 loss: -0.04334259033203125
Batch 47/64 loss: -0.03737330436706543
Batch 48/64 loss: -0.00025898218154907227
Batch 49/64 loss: -0.03463125228881836
Batch 50/64 loss: -0.03629499673843384
Batch 51/64 loss: -0.03854870796203613
Batch 52/64 loss: -0.03724640607833862
Batch 53/64 loss: -0.04689514636993408
Batch 54/64 loss: -0.05084294080734253
Batch 55/64 loss: -0.05510193109512329
Batch 56/64 loss: -0.037391066551208496
Batch 57/64 loss: -0.05665242671966553
Batch 58/64 loss: -0.047643065452575684
Batch 59/64 loss: -0.033621370792388916
Batch 60/64 loss: -0.02065378427505493
Batch 61/64 loss: -0.014358878135681152
Batch 62/64 loss: -0.01799672842025757
Batch 63/64 loss: -0.042212724685668945
Batch 64/64 loss: -0.04664444923400879
Epoch 127  Train loss: -0.03991231824837479  Val loss: -0.0015116440471504972
Epoch 128
-------------------------------
Batch 1/64 loss: -0.04429131746292114
Batch 2/64 loss: -0.05768245458602905
Batch 3/64 loss: -0.022518277168273926
Batch 4/64 loss: -0.0474320650100708
Batch 5/64 loss: -0.05721580982208252
Batch 6/64 loss: -0.04494333267211914
Batch 7/64 loss: -0.020509719848632812
Batch 8/64 loss: -0.0308682918548584
Batch 9/64 loss: -0.042818546295166016
Batch 10/64 loss: -0.05250203609466553
Batch 11/64 loss: -0.05474752187728882
Batch 12/64 loss: -0.03135383129119873
Batch 13/64 loss: -0.046620965003967285
Batch 14/64 loss: -0.03392678499221802
Batch 15/64 loss: -0.05125868320465088
Batch 16/64 loss: -0.04318690299987793
Batch 17/64 loss: -0.04471653699874878
Batch 18/64 loss: -0.04247570037841797
Batch 19/64 loss: -0.04155302047729492
Batch 20/64 loss: -0.054834723472595215
Batch 21/64 loss: -0.05572068691253662
Batch 22/64 loss: -0.036414265632629395
Batch 23/64 loss: -0.0395430326461792
Batch 24/64 loss: -0.050805747509002686
Batch 25/64 loss: -0.04088246822357178
Batch 26/64 loss: -0.045278728008270264
Batch 27/64 loss: -0.04813051223754883
Batch 28/64 loss: -0.035418808460235596
Batch 29/64 loss: -0.06126898527145386
Batch 30/64 loss: -0.039736807346343994
Batch 31/64 loss: -0.04752826690673828
Batch 32/64 loss: -0.038810014724731445
Batch 33/64 loss: -0.04739236831665039
Batch 34/64 loss: -0.058653295040130615
Batch 35/64 loss: -0.044062912464141846
Batch 36/64 loss: -0.02753150463104248
Batch 37/64 loss: -0.031032860279083252
Batch 38/64 loss: -0.03257089853286743
Batch 39/64 loss: -0.05645108222961426
Batch 40/64 loss: -0.024470865726470947
Batch 41/64 loss: -0.0506671667098999
Batch 42/64 loss: -0.05091404914855957
Batch 43/64 loss: -0.0147552490234375
Batch 44/64 loss: -0.04674273729324341
Batch 45/64 loss: -0.02951735258102417
Batch 46/64 loss: -0.06025797128677368
Batch 47/64 loss: -0.018427252769470215
Batch 48/64 loss: -0.03759205341339111
Batch 49/64 loss: -0.04880934953689575
Batch 50/64 loss: -0.04616957902908325
Batch 51/64 loss: -0.006499171257019043
Batch 52/64 loss: -0.04555773735046387
Batch 53/64 loss: -0.054694175720214844
Batch 54/64 loss: -0.03136920928955078
Batch 55/64 loss: -0.04730021953582764
Batch 56/64 loss: -0.05426788330078125
Batch 57/64 loss: -0.024030089378356934
Batch 58/64 loss: -0.034059107303619385
Batch 59/64 loss: -0.03989022970199585
Batch 60/64 loss: -0.0377202033996582
Batch 61/64 loss: -0.013911068439483643
Batch 62/64 loss: -0.04281646013259888
Batch 63/64 loss: -0.05647885799407959
Batch 64/64 loss: -0.05156373977661133
Epoch 128  Train loss: -0.041698519388834634  Val loss: -0.003253594501731322
Epoch 129
-------------------------------
Batch 1/64 loss: -0.04178059101104736
Batch 2/64 loss: -0.03167080879211426
Batch 3/64 loss: -0.030150294303894043
Batch 4/64 loss: -0.034094929695129395
Batch 5/64 loss: -0.06102818250656128
Batch 6/64 loss: -0.03558772802352905
Batch 7/64 loss: -0.053858280181884766
Batch 8/64 loss: -0.03269010782241821
Batch 9/64 loss: -0.03668355941772461
Batch 10/64 loss: -0.055953025817871094
Batch 11/64 loss: -0.04558742046356201
Batch 12/64 loss: -0.03113687038421631
Batch 13/64 loss: -0.04346585273742676
Batch 14/64 loss: -0.04929625988006592
Batch 15/64 loss: -0.033368706703186035
Batch 16/64 loss: -0.04715919494628906
Batch 17/64 loss: -0.050023674964904785
Batch 18/64 loss: -0.03242921829223633
Batch 19/64 loss: -0.033585190773010254
Batch 20/64 loss: -0.034552574157714844
Batch 21/64 loss: -0.03348708152770996
Batch 22/64 loss: -0.027459383010864258
Batch 23/64 loss: -0.03917032480239868
Batch 24/64 loss: -0.04403674602508545
Batch 25/64 loss: -0.05399787425994873
Batch 26/64 loss: -0.0413820743560791
Batch 27/64 loss: -0.05782419443130493
Batch 28/64 loss: -0.0528254508972168
Batch 29/64 loss: -0.04500764608383179
Batch 30/64 loss: -0.04078495502471924
Batch 31/64 loss: -0.03942692279815674
Batch 32/64 loss: -0.05276304483413696
Batch 33/64 loss: -0.032535016536712646
Batch 34/64 loss: -0.05919080972671509
Batch 35/64 loss: -0.02572488784790039
Batch 36/64 loss: -0.04949021339416504
Batch 37/64 loss: -0.039582133293151855
Batch 38/64 loss: -0.035361528396606445
Batch 39/64 loss: -0.014229953289031982
Batch 40/64 loss: -0.04376298189163208
Batch 41/64 loss: -0.007840156555175781
Batch 42/64 loss: -0.04384040832519531
Batch 43/64 loss: -0.049461543560028076
Batch 44/64 loss: -0.05445730686187744
Batch 45/64 loss: -0.025616168975830078
Batch 46/64 loss: -0.014104843139648438
Batch 47/64 loss: -0.04597419500350952
Batch 48/64 loss: -0.04046148061752319
Batch 49/64 loss: -0.037641167640686035
Batch 50/64 loss: -0.05296200513839722
Batch 51/64 loss: -0.06088221073150635
Batch 52/64 loss: -0.03131282329559326
Batch 53/64 loss: -0.04148292541503906
Batch 54/64 loss: -0.051245808601379395
Batch 55/64 loss: -0.0532723069190979
Batch 56/64 loss: -0.03188657760620117
Batch 57/64 loss: -0.042133212089538574
Batch 58/64 loss: -0.04297977685928345
Batch 59/64 loss: -0.036212921142578125
Batch 60/64 loss: -0.03917539119720459
Batch 61/64 loss: -0.04375624656677246
Batch 62/64 loss: -0.04485344886779785
Batch 63/64 loss: -0.05396038293838501
Batch 64/64 loss: -0.04579567909240723
Epoch 129  Train loss: -0.041129502128152286  Val loss: -0.0028956839718769505
Epoch 130
-------------------------------
Batch 1/64 loss: -0.03599494695663452
Batch 2/64 loss: -0.038133859634399414
Batch 3/64 loss: -0.05487406253814697
Batch 4/64 loss: -0.029555976390838623
Batch 5/64 loss: -0.03666752576828003
Batch 6/64 loss: -0.04731309413909912
Batch 7/64 loss: -0.032033562660217285
Batch 8/64 loss: -0.029572665691375732
Batch 9/64 loss: -0.02431386709213257
Batch 10/64 loss: -0.03038930892944336
Batch 11/64 loss: -0.032297611236572266
Batch 12/64 loss: -0.06284236907958984
Batch 13/64 loss: -0.04865783452987671
Batch 14/64 loss: -0.060089051723480225
Batch 15/64 loss: -0.04636502265930176
Batch 16/64 loss: -0.06406563520431519
Batch 17/64 loss: -0.05184507369995117
Batch 18/64 loss: -0.040331363677978516
Batch 19/64 loss: -0.0541648268699646
Batch 20/64 loss: -0.03253757953643799
Batch 21/64 loss: -0.03259718418121338
Batch 22/64 loss: -0.051177382469177246
Batch 23/64 loss: -0.03572356700897217
Batch 24/64 loss: -0.04234415292739868
Batch 25/64 loss: -0.04100632667541504
Batch 26/64 loss: -0.04210156202316284
Batch 27/64 loss: -0.06526768207550049
Batch 28/64 loss: -0.056946516036987305
Batch 29/64 loss: -0.03519386053085327
Batch 30/64 loss: -0.03518640995025635
Batch 31/64 loss: -0.04568082094192505
Batch 32/64 loss: -0.0504457950592041
Batch 33/64 loss: -0.03652358055114746
Batch 34/64 loss: -0.056170880794525146
Batch 35/64 loss: -0.037795841693878174
Batch 36/64 loss: -0.036009252071380615
Batch 37/64 loss: -0.033197879791259766
Batch 38/64 loss: -0.037351369857788086
Batch 39/64 loss: -0.042957186698913574
Batch 40/64 loss: -0.058320701122283936
Batch 41/64 loss: -0.023384690284729004
Batch 42/64 loss: -0.04265451431274414
Batch 43/64 loss: -0.0346604585647583
Batch 44/64 loss: -0.015420258045196533
Batch 45/64 loss: -0.03360718488693237
Batch 46/64 loss: -0.0687260627746582
Batch 47/64 loss: -0.0264776349067688
Batch 48/64 loss: -0.02664172649383545
Batch 49/64 loss: -0.034268736839294434
Batch 50/64 loss: -0.03805804252624512
Batch 51/64 loss: -0.04629760980606079
Batch 52/64 loss: -0.03938770294189453
Batch 53/64 loss: -0.03442806005477905
Batch 54/64 loss: -0.04731178283691406
Batch 55/64 loss: -0.05749601125717163
Batch 56/64 loss: -0.038488924503326416
Batch 57/64 loss: -0.037218451499938965
Batch 58/64 loss: -0.0454634428024292
Batch 59/64 loss: -0.043599069118499756
Batch 60/64 loss: -0.028354287147521973
Batch 61/64 loss: -0.044261932373046875
Batch 62/64 loss: -0.03936362266540527
Batch 63/64 loss: -0.04069185256958008
Batch 64/64 loss: -0.066109299659729
Epoch 130  Train loss: -0.04172372116762049  Val loss: 0.0024824509096309493
Epoch 131
-------------------------------
Batch 1/64 loss: -0.03982996940612793
Batch 2/64 loss: -0.03655529022216797
Batch 3/64 loss: -0.036294519901275635
Batch 4/64 loss: -0.04465538263320923
Batch 5/64 loss: -0.0634164810180664
Batch 6/64 loss: -0.03379899263381958
Batch 7/64 loss: -0.043942153453826904
Batch 8/64 loss: -0.03374350070953369
Batch 9/64 loss: -0.03230893611907959
Batch 10/64 loss: -0.04928940534591675
Batch 11/64 loss: -0.048909664154052734
Batch 12/64 loss: -0.05602121353149414
Batch 13/64 loss: -0.029529452323913574
Batch 14/64 loss: -0.034414470195770264
Batch 15/64 loss: -0.05723762512207031
Batch 16/64 loss: -0.04454904794692993
Batch 17/64 loss: -0.059945881366729736
Batch 18/64 loss: -0.05371826887130737
Batch 19/64 loss: -0.0431901216506958
Batch 20/64 loss: -0.04231959581375122
Batch 21/64 loss: -0.047092318534851074
Batch 22/64 loss: -0.033304452896118164
Batch 23/64 loss: -0.041902780532836914
Batch 24/64 loss: -0.037155330181121826
Batch 25/64 loss: -0.052703917026519775
Batch 26/64 loss: -0.03074467182159424
Batch 27/64 loss: -0.05261659622192383
Batch 28/64 loss: -0.03381472826004028
Batch 29/64 loss: -0.037101924419403076
Batch 30/64 loss: -0.025852322578430176
Batch 31/64 loss: -0.048797547817230225
Batch 32/64 loss: -0.043225765228271484
Batch 33/64 loss: -0.05419909954071045
Batch 34/64 loss: -0.04296290874481201
Batch 35/64 loss: -0.03773754835128784
Batch 36/64 loss: -0.044403791427612305
Batch 37/64 loss: -0.04060441255569458
Batch 38/64 loss: -0.044628918170928955
Batch 39/64 loss: -0.04688155651092529
Batch 40/64 loss: -0.04147946834564209
Batch 41/64 loss: -0.04510772228240967
Batch 42/64 loss: -0.029703140258789062
Batch 43/64 loss: -0.044745445251464844
Batch 44/64 loss: -0.05119800567626953
Batch 45/64 loss: -0.03905832767486572
Batch 46/64 loss: -0.03315091133117676
Batch 47/64 loss: -0.03023552894592285
Batch 48/64 loss: -0.03518474102020264
Batch 49/64 loss: -0.024587631225585938
Batch 50/64 loss: -0.02168220281600952
Batch 51/64 loss: -0.029312968254089355
Batch 52/64 loss: -0.06302517652511597
Batch 53/64 loss: -0.04042792320251465
Batch 54/64 loss: -0.05051255226135254
Batch 55/64 loss: -0.04291492700576782
Batch 56/64 loss: -0.05805671215057373
Batch 57/64 loss: -0.04079937934875488
Batch 58/64 loss: -0.053177475929260254
Batch 59/64 loss: -0.0450630784034729
Batch 60/64 loss: -0.04254084825515747
Batch 61/64 loss: -0.04934650659561157
Batch 62/64 loss: -0.03429973125457764
Batch 63/64 loss: -0.054354846477508545
Batch 64/64 loss: -0.043421030044555664
Epoch 131  Train loss: -0.04254010331396963  Val loss: -0.008547222696219114
Saving best model, epoch: 131
Epoch 132
-------------------------------
Batch 1/64 loss: -0.04925096035003662
Batch 2/64 loss: -0.0584639310836792
Batch 3/64 loss: -0.04504585266113281
Batch 4/64 loss: -0.049201011657714844
Batch 5/64 loss: -0.016086220741271973
Batch 6/64 loss: -0.045834898948669434
Batch 7/64 loss: -0.06581735610961914
Batch 8/64 loss: -0.06143331527709961
Batch 9/64 loss: -0.04723620414733887
Batch 10/64 loss: -0.02437281608581543
Batch 11/64 loss: -0.05582982301712036
Batch 12/64 loss: -0.03381627798080444
Batch 13/64 loss: -0.0445370078086853
Batch 14/64 loss: -0.04001009464263916
Batch 15/64 loss: -0.045024991035461426
Batch 16/64 loss: -0.03459811210632324
Batch 17/64 loss: -0.03865140676498413
Batch 18/64 loss: -0.03973132371902466
Batch 19/64 loss: -0.053120970726013184
Batch 20/64 loss: -0.04866868257522583
Batch 21/64 loss: -0.0514674186706543
Batch 22/64 loss: -0.06157410144805908
Batch 23/64 loss: -0.054650187492370605
Batch 24/64 loss: -0.04773271083831787
Batch 25/64 loss: -0.05842643976211548
Batch 26/64 loss: -0.032854437828063965
Batch 27/64 loss: -0.03662896156311035
Batch 28/64 loss: -0.05803251266479492
Batch 29/64 loss: -0.04926949739456177
Batch 30/64 loss: -0.04370969533920288
Batch 31/64 loss: -0.04146397113800049
Batch 32/64 loss: -0.049286067485809326
Batch 33/64 loss: -0.03101569414138794
Batch 34/64 loss: -0.038011789321899414
Batch 35/64 loss: -0.03785407543182373
Batch 36/64 loss: -0.05507850646972656
Batch 37/64 loss: -0.04740011692047119
Batch 38/64 loss: -0.05698120594024658
Batch 39/64 loss: -0.046235859394073486
Batch 40/64 loss: -0.031082510948181152
Batch 41/64 loss: -0.04480558633804321
Batch 42/64 loss: -0.04159414768218994
Batch 43/64 loss: -0.04378068447113037
Batch 44/64 loss: -0.040788233280181885
Batch 45/64 loss: -0.06660854816436768
Batch 46/64 loss: -0.052051007747650146
Batch 47/64 loss: -0.0716707706451416
Batch 48/64 loss: -0.011787354946136475
Batch 49/64 loss: -0.01810091733932495
Batch 50/64 loss: -0.041951656341552734
Batch 51/64 loss: -0.06364965438842773
Batch 52/64 loss: -0.04968726634979248
Batch 53/64 loss: -0.049175262451171875
Batch 54/64 loss: -0.05093705654144287
Batch 55/64 loss: -0.03627818822860718
Batch 56/64 loss: -0.05088019371032715
Batch 57/64 loss: -0.056166768074035645
Batch 58/64 loss: -0.06263071298599243
Batch 59/64 loss: -0.05746537446975708
Batch 60/64 loss: -0.04549223184585571
Batch 61/64 loss: -0.04648470878601074
Batch 62/64 loss: -0.04356813430786133
Batch 63/64 loss: -0.012008309364318848
Batch 64/64 loss: -0.05450940132141113
Epoch 132  Train loss: -0.045865597444422104  Val loss: -0.001310339908009952
Epoch 133
-------------------------------
Batch 1/64 loss: -0.05389523506164551
Batch 2/64 loss: -0.06277155876159668
Batch 3/64 loss: -0.05798763036727905
Batch 4/64 loss: -0.01929110288619995
Batch 5/64 loss: -0.012304723262786865
Batch 6/64 loss: -0.059478819370269775
Batch 7/64 loss: -0.039472341537475586
Batch 8/64 loss: -0.04292011260986328
Batch 9/64 loss: -0.043820083141326904
Batch 10/64 loss: -0.029699325561523438
Batch 11/64 loss: -0.04164916276931763
Batch 12/64 loss: -0.02994006872177124
Batch 13/64 loss: -0.0645298957824707
Batch 14/64 loss: -0.04254412651062012
Batch 15/64 loss: -0.06870889663696289
Batch 16/64 loss: -0.03997361660003662
Batch 17/64 loss: -0.04705327749252319
Batch 18/64 loss: -0.028336822986602783
Batch 19/64 loss: -0.05764120817184448
Batch 20/64 loss: -0.03453105688095093
Batch 21/64 loss: -0.035739004611968994
Batch 22/64 loss: -0.04891800880432129
Batch 23/64 loss: -0.044453978538513184
Batch 24/64 loss: -0.04959452152252197
Batch 25/64 loss: -0.04602783918380737
Batch 26/64 loss: -0.0629839301109314
Batch 27/64 loss: -0.06832271814346313
Batch 28/64 loss: -0.0601656436920166
Batch 29/64 loss: -0.060447096824645996
Batch 30/64 loss: -0.022959411144256592
Batch 31/64 loss: -0.04511594772338867
Batch 32/64 loss: -0.05803811550140381
Batch 33/64 loss: -0.0511283278465271
Batch 34/64 loss: -0.038538992404937744
Batch 35/64 loss: -0.027795076370239258
Batch 36/64 loss: -0.024647951126098633
Batch 37/64 loss: -0.04933983087539673
Batch 38/64 loss: -0.04765486717224121
Batch 39/64 loss: -0.04491066932678223
Batch 40/64 loss: -0.030309200286865234
Batch 41/64 loss: -0.04773128032684326
Batch 42/64 loss: -0.040660560131073
Batch 43/64 loss: -0.024895191192626953
Batch 44/64 loss: -0.0548321008682251
Batch 45/64 loss: -0.0524975061416626
Batch 46/64 loss: -0.03714340925216675
Batch 47/64 loss: -0.04332989454269409
Batch 48/64 loss: -0.057534754276275635
Batch 49/64 loss: -0.010258615016937256
Batch 50/64 loss: -0.03196227550506592
Batch 51/64 loss: -0.041347503662109375
Batch 52/64 loss: -0.03688007593154907
Batch 53/64 loss: -0.06165182590484619
Batch 54/64 loss: -0.04890120029449463
Batch 55/64 loss: -0.05296820402145386
Batch 56/64 loss: -0.05542033910751343
Batch 57/64 loss: -0.039791107177734375
Batch 58/64 loss: -0.056929051876068115
Batch 59/64 loss: -0.05220240354537964
Batch 60/64 loss: -0.019311726093292236
Batch 61/64 loss: -0.0482904314994812
Batch 62/64 loss: -0.037524521350860596
Batch 63/64 loss: -0.026851177215576172
Batch 64/64 loss: -0.04213571548461914
Epoch 133  Train loss: -0.04398677863326727  Val loss: -0.006063677600978576
Epoch 134
-------------------------------
Batch 1/64 loss: -0.04543405771255493
Batch 2/64 loss: -0.03865063190460205
Batch 3/64 loss: -0.015292346477508545
Batch 4/64 loss: -0.039363861083984375
Batch 5/64 loss: -0.049260735511779785
Batch 6/64 loss: -0.036465346813201904
Batch 7/64 loss: -0.04270964860916138
Batch 8/64 loss: -0.03540360927581787
Batch 9/64 loss: -0.04614102840423584
Batch 10/64 loss: -0.036216139793395996
Batch 11/64 loss: -0.05917525291442871
Batch 12/64 loss: -0.04185688495635986
Batch 13/64 loss: -0.0319979190826416
Batch 14/64 loss: -0.03938472270965576
Batch 15/64 loss: -0.03164255619049072
Batch 16/64 loss: -0.0203210711479187
Batch 17/64 loss: -0.04599207639694214
Batch 18/64 loss: -0.045097529888153076
Batch 19/64 loss: -0.048722267150878906
Batch 20/64 loss: -0.029906749725341797
Batch 21/64 loss: -0.03011876344680786
Batch 22/64 loss: -0.04255253076553345
Batch 23/64 loss: -0.029443740844726562
Batch 24/64 loss: -0.058051109313964844
Batch 25/64 loss: -0.04147064685821533
Batch 26/64 loss: -0.05265164375305176
Batch 27/64 loss: -0.04695862531661987
Batch 28/64 loss: -0.013334453105926514
Batch 29/64 loss: -0.06273293495178223
Batch 30/64 loss: -0.038996338844299316
Batch 31/64 loss: -0.06481575965881348
Batch 32/64 loss: -0.047542572021484375
Batch 33/64 loss: -0.04481691122055054
Batch 34/64 loss: -0.045816898345947266
Batch 35/64 loss: -0.05113571882247925
Batch 36/64 loss: -0.06141042709350586
Batch 37/64 loss: -0.0465359091758728
Batch 38/64 loss: -0.043466806411743164
Batch 39/64 loss: -0.052233755588531494
Batch 40/64 loss: -0.03801858425140381
Batch 41/64 loss: -0.056687235832214355
Batch 42/64 loss: -0.03814345598220825
Batch 43/64 loss: -0.04468655586242676
Batch 44/64 loss: -0.04474496841430664
Batch 45/64 loss: -0.04400140047073364
Batch 46/64 loss: -0.0621798038482666
Batch 47/64 loss: -0.03655362129211426
Batch 48/64 loss: -0.04381120204925537
Batch 49/64 loss: -0.04989522695541382
Batch 50/64 loss: -0.04990339279174805
Batch 51/64 loss: -0.06893408298492432
Batch 52/64 loss: -0.03286087512969971
Batch 53/64 loss: -0.05748993158340454
Batch 54/64 loss: -0.049388587474823
Batch 55/64 loss: -0.050512611865997314
Batch 56/64 loss: -0.04841947555541992
Batch 57/64 loss: -0.057128965854644775
Batch 58/64 loss: -0.04847055673599243
Batch 59/64 loss: -0.05252045392990112
Batch 60/64 loss: -0.048589348793029785
Batch 61/64 loss: -0.05488699674606323
Batch 62/64 loss: -0.04889434576034546
Batch 63/64 loss: -0.029228031635284424
Batch 64/64 loss: -0.04879969358444214
Epoch 134  Train loss: -0.04463795236512726  Val loss: -0.000933777425707001
Epoch 135
-------------------------------
Batch 1/64 loss: -0.04317593574523926
Batch 2/64 loss: -0.06373625993728638
Batch 3/64 loss: -0.06371986865997314
Batch 4/64 loss: -0.05604898929595947
Batch 5/64 loss: -0.04916548728942871
Batch 6/64 loss: -0.03284221887588501
Batch 7/64 loss: -0.04717874526977539
Batch 8/64 loss: -0.06039726734161377
Batch 9/64 loss: -0.03951585292816162
Batch 10/64 loss: -0.04718512296676636
Batch 11/64 loss: -0.047995686531066895
Batch 12/64 loss: -0.04301559925079346
Batch 13/64 loss: -0.04189491271972656
Batch 14/64 loss: -0.06162673234939575
Batch 15/64 loss: -0.04209411144256592
Batch 16/64 loss: -0.04994392395019531
Batch 17/64 loss: -0.050794899463653564
Batch 18/64 loss: -0.04073411226272583
Batch 19/64 loss: -0.046910643577575684
Batch 20/64 loss: -0.02575099468231201
Batch 21/64 loss: -0.046862244606018066
Batch 22/64 loss: -0.012243032455444336
Batch 23/64 loss: -0.05892610549926758
Batch 24/64 loss: -0.05819946527481079
Batch 25/64 loss: -0.041626691818237305
Batch 26/64 loss: -0.05024278163909912
Batch 27/64 loss: -0.0458415150642395
Batch 28/64 loss: -0.05884164571762085
Batch 29/64 loss: -0.04869186878204346
Batch 30/64 loss: -0.04783135652542114
Batch 31/64 loss: -0.047995030879974365
Batch 32/64 loss: -0.03310638666152954
Batch 33/64 loss: -0.045679688453674316
Batch 34/64 loss: -0.05999600887298584
Batch 35/64 loss: -0.0649612545967102
Batch 36/64 loss: -0.044414520263671875
Batch 37/64 loss: -0.053773701190948486
Batch 38/64 loss: -0.06585067510604858
Batch 39/64 loss: -0.04171854257583618
Batch 40/64 loss: -0.058357298374176025
Batch 41/64 loss: -0.03770571947097778
Batch 42/64 loss: -0.0269775390625
Batch 43/64 loss: -0.0056059956550598145
Batch 44/64 loss: -0.04439955949783325
Batch 45/64 loss: -0.052501797676086426
Batch 46/64 loss: -0.061758577823638916
Batch 47/64 loss: -0.04965639114379883
Batch 48/64 loss: -0.056347012519836426
Batch 49/64 loss: -0.04871320724487305
Batch 50/64 loss: -0.03909122943878174
Batch 51/64 loss: -0.03360193967819214
Batch 52/64 loss: -0.05254566669464111
Batch 53/64 loss: -0.04399150609970093
Batch 54/64 loss: -0.05463689565658569
Batch 55/64 loss: -0.05628395080566406
Batch 56/64 loss: -0.04507702589035034
Batch 57/64 loss: -0.029125571250915527
Batch 58/64 loss: -0.047838568687438965
Batch 59/64 loss: -0.0393943190574646
Batch 60/64 loss: -0.03851580619812012
Batch 61/64 loss: -0.021722137928009033
Batch 62/64 loss: -0.04293590784072876
Batch 63/64 loss: -0.05510878562927246
Batch 64/64 loss: -0.057761549949645996
Epoch 135  Train loss: -0.04652146591859705  Val loss: -0.003950237203709448
Epoch 136
-------------------------------
Batch 1/64 loss: -0.03806865215301514
Batch 2/64 loss: -0.049746572971343994
Batch 3/64 loss: -0.047985613346099854
Batch 4/64 loss: -0.04500991106033325
Batch 5/64 loss: -0.05834650993347168
Batch 6/64 loss: -0.056347668170928955
Batch 7/64 loss: -0.02378237247467041
Batch 8/64 loss: -0.061352431774139404
Batch 9/64 loss: -0.050369083881378174
Batch 10/64 loss: -0.05344003438949585
Batch 11/64 loss: -0.04212164878845215
Batch 12/64 loss: -0.058968961238861084
Batch 13/64 loss: -0.046397507190704346
Batch 14/64 loss: -0.04785716533660889
Batch 15/64 loss: -0.04712510108947754
Batch 16/64 loss: -0.02346181869506836
Batch 17/64 loss: -0.05672276020050049
Batch 18/64 loss: -0.03486061096191406
Batch 19/64 loss: -0.03512674570083618
Batch 20/64 loss: -0.047974348068237305
Batch 21/64 loss: -0.05672132968902588
Batch 22/64 loss: -0.011518657207489014
Batch 23/64 loss: -0.05075252056121826
Batch 24/64 loss: -0.0516471266746521
Batch 25/64 loss: -0.05365276336669922
Batch 26/64 loss: -0.03501415252685547
Batch 27/64 loss: -0.051654815673828125
Batch 28/64 loss: -0.05515885353088379
Batch 29/64 loss: -0.04029130935668945
Batch 30/64 loss: -0.049189746379852295
Batch 31/64 loss: -0.06048130989074707
Batch 32/64 loss: -0.044993698596954346
Batch 33/64 loss: -0.04210984706878662
Batch 34/64 loss: -0.055123209953308105
Batch 35/64 loss: -0.054611027240753174
Batch 36/64 loss: -0.0253676176071167
Batch 37/64 loss: -0.06072890758514404
Batch 38/64 loss: -0.046242713928222656
Batch 39/64 loss: -0.05336761474609375
Batch 40/64 loss: -0.06046271324157715
Batch 41/64 loss: -0.050848424434661865
Batch 42/64 loss: -0.06778669357299805
Batch 43/64 loss: -0.035492658615112305
Batch 44/64 loss: -0.02716517448425293
Batch 45/64 loss: -0.04364496469497681
Batch 46/64 loss: -0.05355489253997803
Batch 47/64 loss: -0.06286287307739258
Batch 48/64 loss: -0.0514521598815918
Batch 49/64 loss: -0.029333412647247314
Batch 50/64 loss: -0.03204762935638428
Batch 51/64 loss: -0.041934847831726074
Batch 52/64 loss: -0.05089390277862549
Batch 53/64 loss: -0.06232190132141113
Batch 54/64 loss: -0.04867821931838989
Batch 55/64 loss: -0.04910707473754883
Batch 56/64 loss: -0.043881237506866455
Batch 57/64 loss: -0.039158761501312256
Batch 58/64 loss: -0.03227972984313965
Batch 59/64 loss: -0.03916740417480469
Batch 60/64 loss: -0.04293179512023926
Batch 61/64 loss: -0.06518399715423584
Batch 62/64 loss: -0.042832136154174805
Batch 63/64 loss: -0.0379449725151062
Batch 64/64 loss: -0.04694485664367676
Epoch 136  Train loss: -0.04658617973327637  Val loss: -0.0036703063450318433
Epoch 137
-------------------------------
Batch 1/64 loss: -0.03930354118347168
Batch 2/64 loss: -0.029497742652893066
Batch 3/64 loss: -0.04856473207473755
Batch 4/64 loss: -0.03780698776245117
Batch 5/64 loss: -0.04514610767364502
Batch 6/64 loss: -0.05083012580871582
Batch 7/64 loss: -0.05450671911239624
Batch 8/64 loss: -0.04556077718734741
Batch 9/64 loss: -0.03794980049133301
Batch 10/64 loss: -0.04290163516998291
Batch 11/64 loss: -0.03631734848022461
Batch 12/64 loss: -0.034057021141052246
Batch 13/64 loss: -0.06196862459182739
Batch 14/64 loss: -0.03459763526916504
Batch 15/64 loss: -0.027149498462677002
Batch 16/64 loss: -0.05510532855987549
Batch 17/64 loss: -0.039540886878967285
Batch 18/64 loss: -0.043599724769592285
Batch 19/64 loss: -0.052509188652038574
Batch 20/64 loss: -0.058965086936950684
Batch 21/64 loss: -0.059617698192596436
Batch 22/64 loss: -0.05323392152786255
Batch 23/64 loss: -0.06793606281280518
Batch 24/64 loss: -0.05318242311477661
Batch 25/64 loss: -0.04585236310958862
Batch 26/64 loss: -0.050412654876708984
Batch 27/64 loss: -0.04528403282165527
Batch 28/64 loss: -0.03440362215042114
Batch 29/64 loss: -0.0599442720413208
Batch 30/64 loss: -0.06549692153930664
Batch 31/64 loss: -0.0441930890083313
Batch 32/64 loss: -0.04262411594390869
Batch 33/64 loss: -0.04113966226577759
Batch 34/64 loss: -0.04182851314544678
Batch 35/64 loss: -0.02772390842437744
Batch 36/64 loss: -0.0361819863319397
Batch 37/64 loss: -0.043719708919525146
Batch 38/64 loss: -0.04746013879776001
Batch 39/64 loss: -0.03182333707809448
Batch 40/64 loss: -0.03988361358642578
Batch 41/64 loss: -0.033431410789489746
Batch 42/64 loss: -0.05176198482513428
Batch 43/64 loss: -0.04855978488922119
Batch 44/64 loss: -0.043443262577056885
Batch 45/64 loss: -0.033498525619506836
Batch 46/64 loss: -0.04181760549545288
Batch 47/64 loss: 0.00104522705078125
Batch 48/64 loss: -0.05129200220108032
Batch 49/64 loss: -0.053015708923339844
Batch 50/64 loss: -0.029543757438659668
Batch 51/64 loss: -0.005785584449768066
Batch 52/64 loss: -0.05349820852279663
Batch 53/64 loss: -0.038094937801361084
Batch 54/64 loss: -0.04726588726043701
Batch 55/64 loss: -0.046632468700408936
Batch 56/64 loss: -0.03316384553909302
Batch 57/64 loss: -0.024076521396636963
Batch 58/64 loss: -0.05929410457611084
Batch 59/64 loss: -0.04623126983642578
Batch 60/64 loss: -0.04395937919616699
Batch 61/64 loss: -0.023487985134124756
Batch 62/64 loss: -0.049908578395843506
Batch 63/64 loss: -0.05664545297622681
Batch 64/64 loss: -0.04711568355560303
Epoch 137  Train loss: -0.043239535069933124  Val loss: -0.00245625313204998
Epoch 138
-------------------------------
Batch 1/64 loss: -0.05928659439086914
Batch 2/64 loss: -0.03505587577819824
Batch 3/64 loss: -0.03195023536682129
Batch 4/64 loss: -0.030991971492767334
Batch 5/64 loss: -0.03475922346115112
Batch 6/64 loss: -0.040344178676605225
Batch 7/64 loss: -0.04853188991546631
Batch 8/64 loss: -0.032416701316833496
Batch 9/64 loss: -0.04587024450302124
Batch 10/64 loss: -0.03229326009750366
Batch 11/64 loss: -0.025731682777404785
Batch 12/64 loss: -0.03283172845840454
Batch 13/64 loss: -0.02747368812561035
Batch 14/64 loss: -0.03339904546737671
Batch 15/64 loss: -0.042753517627716064
Batch 16/64 loss: -0.05365806818008423
Batch 17/64 loss: -0.0517696738243103
Batch 18/64 loss: -0.044605791568756104
Batch 19/64 loss: -0.03930938243865967
Batch 20/64 loss: -0.05337703227996826
Batch 21/64 loss: -0.06087714433670044
Batch 22/64 loss: -0.04350823163986206
Batch 23/64 loss: -0.06263995170593262
Batch 24/64 loss: -0.055985867977142334
Batch 25/64 loss: -0.041367411613464355
Batch 26/64 loss: -0.06410181522369385
Batch 27/64 loss: -0.04834091663360596
Batch 28/64 loss: -0.03452157974243164
Batch 29/64 loss: -0.06328403949737549
Batch 30/64 loss: -0.04360097646713257
Batch 31/64 loss: -0.054349303245544434
Batch 32/64 loss: -0.059466004371643066
Batch 33/64 loss: -0.04174894094467163
Batch 34/64 loss: -0.044643282890319824
Batch 35/64 loss: -0.03535032272338867
Batch 36/64 loss: -0.061772823333740234
Batch 37/64 loss: -0.03857409954071045
Batch 38/64 loss: -0.03759568929672241
Batch 39/64 loss: -0.03679358959197998
Batch 40/64 loss: -0.04501885175704956
Batch 41/64 loss: -0.06896668672561646
Batch 42/64 loss: -0.02998054027557373
Batch 43/64 loss: -0.07189279794692993
Batch 44/64 loss: -0.05662870407104492
Batch 45/64 loss: -0.06820440292358398
Batch 46/64 loss: -0.05339372158050537
Batch 47/64 loss: -0.024996817111968994
Batch 48/64 loss: -0.03845226764678955
Batch 49/64 loss: -0.05663192272186279
Batch 50/64 loss: -0.026638269424438477
Batch 51/64 loss: -0.05987578630447388
Batch 52/64 loss: -0.06446772813796997
Batch 53/64 loss: -0.05648207664489746
Batch 54/64 loss: -0.027390122413635254
Batch 55/64 loss: -0.020882248878479004
Batch 56/64 loss: -0.041161179542541504
Batch 57/64 loss: -0.042002320289611816
Batch 58/64 loss: -0.03521806001663208
Batch 59/64 loss: -0.02260565757751465
Batch 60/64 loss: -0.05050241947174072
Batch 61/64 loss: -0.05347442626953125
Batch 62/64 loss: -0.059919774532318115
Batch 63/64 loss: -0.045877933502197266
Batch 64/64 loss: -0.04859793186187744
Epoch 138  Train loss: -0.04520854809704949  Val loss: -0.005819755116688837
Epoch 139
-------------------------------
Batch 1/64 loss: -0.046845197677612305
Batch 2/64 loss: -0.0323980450630188
Batch 3/64 loss: -0.059324681758880615
Batch 4/64 loss: -0.044044315814971924
Batch 5/64 loss: -0.03787791728973389
Batch 6/64 loss: -0.04654663801193237
Batch 7/64 loss: -0.0570758581161499
Batch 8/64 loss: -0.05187422037124634
Batch 9/64 loss: -0.05932879447937012
Batch 10/64 loss: -0.060565173625946045
Batch 11/64 loss: -0.037353515625
Batch 12/64 loss: -0.04893457889556885
Batch 13/64 loss: -0.055912792682647705
Batch 14/64 loss: -0.05237400531768799
Batch 15/64 loss: -0.04902607202529907
Batch 16/64 loss: -0.042348265647888184
Batch 17/64 loss: -0.04737907648086548
Batch 18/64 loss: -0.04698997735977173
Batch 19/64 loss: -0.05133402347564697
Batch 20/64 loss: -0.048414766788482666
Batch 21/64 loss: -0.04891473054885864
Batch 22/64 loss: -0.04694396257400513
Batch 23/64 loss: -0.0440516471862793
Batch 24/64 loss: -0.026152610778808594
Batch 25/64 loss: -0.045004427433013916
Batch 26/64 loss: -0.044570207595825195
Batch 27/64 loss: -0.042599260807037354
Batch 28/64 loss: -0.056278228759765625
Batch 29/64 loss: -0.06176477670669556
Batch 30/64 loss: -0.05316966772079468
Batch 31/64 loss: -0.03842836618423462
Batch 32/64 loss: -0.030738413333892822
Batch 33/64 loss: -0.03908056020736694
Batch 34/64 loss: -0.04213225841522217
Batch 35/64 loss: -0.059300780296325684
Batch 36/64 loss: -0.05362057685852051
Batch 37/64 loss: -0.0420989990234375
Batch 38/64 loss: -0.044591307640075684
Batch 39/64 loss: -0.029973864555358887
Batch 40/64 loss: -0.041669487953186035
Batch 41/64 loss: -0.04017120599746704
Batch 42/64 loss: -0.05578923225402832
Batch 43/64 loss: -0.025389432907104492
Batch 44/64 loss: -0.059223711490631104
Batch 45/64 loss: -0.0621485710144043
Batch 46/64 loss: -0.04636585712432861
Batch 47/64 loss: -0.0638892650604248
Batch 48/64 loss: -0.059340476989746094
Batch 49/64 loss: -0.054998815059661865
Batch 50/64 loss: -0.0568009614944458
Batch 51/64 loss: -0.032148540019989014
Batch 52/64 loss: -0.05089682340621948
Batch 53/64 loss: -0.04220426082611084
Batch 54/64 loss: -0.04297304153442383
Batch 55/64 loss: -0.03450542688369751
Batch 56/64 loss: -0.056015849113464355
Batch 57/64 loss: -0.04037600755691528
Batch 58/64 loss: -0.015368998050689697
Batch 59/64 loss: -0.021663129329681396
Batch 60/64 loss: -0.06211650371551514
Batch 61/64 loss: -0.027731895446777344
Batch 62/64 loss: -0.06149756908416748
Batch 63/64 loss: -0.04139822721481323
Batch 64/64 loss: -0.039759039878845215
Epoch 139  Train loss: -0.04627239423639634  Val loss: -0.004247035562377615
Epoch 140
-------------------------------
Batch 1/64 loss: -0.07036715745925903
Batch 2/64 loss: -0.02091085910797119
Batch 3/64 loss: -0.04687374830245972
Batch 4/64 loss: -0.037069082260131836
Batch 5/64 loss: -0.05205267667770386
Batch 6/64 loss: -0.05567800998687744
Batch 7/64 loss: -0.03015667200088501
Batch 8/64 loss: -0.04041016101837158
Batch 9/64 loss: -0.0349268913269043
Batch 10/64 loss: -0.04424095153808594
Batch 11/64 loss: -0.04605364799499512
Batch 12/64 loss: -0.05491983890533447
Batch 13/64 loss: -0.05521726608276367
Batch 14/64 loss: -0.034601032733917236
Batch 15/64 loss: -0.038703322410583496
Batch 16/64 loss: -0.04744279384613037
Batch 17/64 loss: -0.04878354072570801
Batch 18/64 loss: -0.04112958908081055
Batch 19/64 loss: -0.04312562942504883
Batch 20/64 loss: -0.0481758713722229
Batch 21/64 loss: -0.040910959243774414
Batch 22/64 loss: -0.031182408332824707
Batch 23/64 loss: -0.024712681770324707
Batch 24/64 loss: -0.04305624961853027
Batch 25/64 loss: -0.05047529935836792
Batch 26/64 loss: -0.041539907455444336
Batch 27/64 loss: -0.052407145500183105
Batch 28/64 loss: -0.049665212631225586
Batch 29/64 loss: -0.06157439947128296
Batch 30/64 loss: -0.06398200988769531
Batch 31/64 loss: -0.04279083013534546
Batch 32/64 loss: -0.044998884201049805
Batch 33/64 loss: -0.052398741245269775
Batch 34/64 loss: -0.06310182809829712
Batch 35/64 loss: -0.0655023455619812
Batch 36/64 loss: -0.04805421829223633
Batch 37/64 loss: -0.03864258527755737
Batch 38/64 loss: -0.05373799800872803
Batch 39/64 loss: -0.051353633403778076
Batch 40/64 loss: -0.05253732204437256
Batch 41/64 loss: -0.0655442476272583
Batch 42/64 loss: -0.03477799892425537
Batch 43/64 loss: -0.053197383880615234
Batch 44/64 loss: -0.054142773151397705
Batch 45/64 loss: -0.06064552068710327
Batch 46/64 loss: -0.03691750764846802
Batch 47/64 loss: -0.033741652965545654
Batch 48/64 loss: -0.04745739698410034
Batch 49/64 loss: -0.04707896709442139
Batch 50/64 loss: -0.02363407611846924
Batch 51/64 loss: -0.05048602819442749
Batch 52/64 loss: -0.06025087833404541
Batch 53/64 loss: -0.045293986797332764
Batch 54/64 loss: -0.05110400915145874
Batch 55/64 loss: -0.030687391757965088
Batch 56/64 loss: -0.05279916524887085
Batch 57/64 loss: -0.051133036613464355
Batch 58/64 loss: -0.03139686584472656
Batch 59/64 loss: -0.04716557264328003
Batch 60/64 loss: -0.05499356985092163
Batch 61/64 loss: -0.04558473825454712
Batch 62/64 loss: -0.03344947099685669
Batch 63/64 loss: -0.054445624351501465
Batch 64/64 loss: -0.05538874864578247
Epoch 140  Train loss: -0.04660286786509495  Val loss: -0.008765345791361177
Saving best model, epoch: 140
Epoch 141
-------------------------------
Batch 1/64 loss: -0.04498666524887085
Batch 2/64 loss: -0.06794893741607666
Batch 3/64 loss: -0.05318790674209595
Batch 4/64 loss: -0.04763996601104736
Batch 5/64 loss: -0.04418152570724487
Batch 6/64 loss: -0.038349032402038574
Batch 7/64 loss: -0.04241538047790527
Batch 8/64 loss: -0.04352611303329468
Batch 9/64 loss: -0.05552399158477783
Batch 10/64 loss: -0.06197077035903931
Batch 11/64 loss: -0.0461963415145874
Batch 12/64 loss: -0.06177711486816406
Batch 13/64 loss: -0.06135660409927368
Batch 14/64 loss: -0.05882173776626587
Batch 15/64 loss: -0.06395936012268066
Batch 16/64 loss: -0.07048749923706055
Batch 17/64 loss: -0.0238649845123291
Batch 18/64 loss: -0.045303285121917725
Batch 19/64 loss: -0.0343746542930603
Batch 20/64 loss: -0.03598755598068237
Batch 21/64 loss: -0.054449260234832764
Batch 22/64 loss: -0.06503528356552124
Batch 23/64 loss: -0.04233831167221069
Batch 24/64 loss: -0.0685802698135376
Batch 25/64 loss: -0.06118130683898926
Batch 26/64 loss: -0.058828651905059814
Batch 27/64 loss: -0.0422898530960083
Batch 28/64 loss: -0.061799705028533936
Batch 29/64 loss: -0.054630398750305176
Batch 30/64 loss: -0.06500566005706787
Batch 31/64 loss: -0.04738783836364746
Batch 32/64 loss: -0.0563807487487793
Batch 33/64 loss: -0.04761326313018799
Batch 34/64 loss: -0.04308056831359863
Batch 35/64 loss: -0.05889827013015747
Batch 36/64 loss: -0.029215991497039795
Batch 37/64 loss: -0.037746548652648926
Batch 38/64 loss: -0.030433177947998047
Batch 39/64 loss: -0.04370695352554321
Batch 40/64 loss: -0.03475385904312134
Batch 41/64 loss: -0.033824384212493896
Batch 42/64 loss: -0.05542987585067749
Batch 43/64 loss: -0.04759049415588379
Batch 44/64 loss: -0.06082713603973389
Batch 45/64 loss: -0.06494849920272827
Batch 46/64 loss: -0.04904055595397949
Batch 47/64 loss: -0.048056960105895996
Batch 48/64 loss: -0.04698532819747925
Batch 49/64 loss: -0.0526731014251709
Batch 50/64 loss: -0.036464571952819824
Batch 51/64 loss: -0.04531705379486084
Batch 52/64 loss: -0.050428926944732666
Batch 53/64 loss: -0.048755645751953125
Batch 54/64 loss: -0.06057906150817871
Batch 55/64 loss: -0.040483713150024414
Batch 56/64 loss: -0.06174898147583008
Batch 57/64 loss: -0.0668841004371643
Batch 58/64 loss: -0.043190181255340576
Batch 59/64 loss: -0.05271631479263306
Batch 60/64 loss: -0.0581015944480896
Batch 61/64 loss: -0.06610369682312012
Batch 62/64 loss: -0.04735004901885986
Batch 63/64 loss: -0.028944790363311768
Batch 64/64 loss: -0.04537391662597656
Epoch 141  Train loss: -0.05028534590029249  Val loss: -0.00362208536810072
Epoch 142
-------------------------------
Batch 1/64 loss: -0.06137263774871826
Batch 2/64 loss: -0.04012936353683472
Batch 3/64 loss: -0.0409388542175293
Batch 4/64 loss: -0.04444009065628052
Batch 5/64 loss: -0.0559844970703125
Batch 6/64 loss: -0.05512285232543945
Batch 7/64 loss: -0.05508226156234741
Batch 8/64 loss: -0.054427385330200195
Batch 9/64 loss: -0.04522138833999634
Batch 10/64 loss: -0.06357336044311523
Batch 11/64 loss: -0.0485994815826416
Batch 12/64 loss: -0.036382436752319336
Batch 13/64 loss: -0.06034517288208008
Batch 14/64 loss: -0.06895548105239868
Batch 15/64 loss: -0.07494175434112549
Batch 16/64 loss: -0.040947556495666504
Batch 17/64 loss: -0.047115445137023926
Batch 18/64 loss: -0.05180466175079346
Batch 19/64 loss: -0.0655057430267334
Batch 20/64 loss: -0.06114494800567627
Batch 21/64 loss: -0.05346691608428955
Batch 22/64 loss: -0.03228718042373657
Batch 23/64 loss: -0.05408322811126709
Batch 24/64 loss: -0.022751152515411377
Batch 25/64 loss: -0.04748266935348511
Batch 26/64 loss: -0.05866813659667969
Batch 27/64 loss: -0.05278420448303223
Batch 28/64 loss: -0.06669884920120239
Batch 29/64 loss: -0.02338230609893799
Batch 30/64 loss: -0.061600327491760254
Batch 31/64 loss: -0.06331110000610352
Batch 32/64 loss: -0.04583930969238281
Batch 33/64 loss: -0.04496133327484131
Batch 34/64 loss: -0.03777033090591431
Batch 35/64 loss: -0.04160916805267334
Batch 36/64 loss: -0.04463428258895874
Batch 37/64 loss: -0.052071988582611084
Batch 38/64 loss: -0.050107598304748535
Batch 39/64 loss: -0.04218614101409912
Batch 40/64 loss: -0.05047672986984253
Batch 41/64 loss: -0.0627584457397461
Batch 42/64 loss: -0.028440773487091064
Batch 43/64 loss: -0.04689526557922363
Batch 44/64 loss: -0.047837018966674805
Batch 45/64 loss: -0.04494518041610718
Batch 46/64 loss: -0.02789616584777832
Batch 47/64 loss: -0.06764060258865356
Batch 48/64 loss: -0.05485224723815918
Batch 49/64 loss: -0.07172256708145142
Batch 50/64 loss: -0.05753833055496216
Batch 51/64 loss: -0.041065216064453125
Batch 52/64 loss: -0.028743743896484375
Batch 53/64 loss: -0.05159026384353638
Batch 54/64 loss: -0.051167845726013184
Batch 55/64 loss: -0.06354767084121704
Batch 56/64 loss: -0.055435240268707275
Batch 57/64 loss: -0.03387904167175293
Batch 58/64 loss: -0.03135901689529419
Batch 59/64 loss: -0.04407155513763428
Batch 60/64 loss: -0.04338186979293823
Batch 61/64 loss: -0.060075342655181885
Batch 62/64 loss: -0.0457308292388916
Batch 63/64 loss: -0.05324828624725342
Batch 64/64 loss: -0.02947920560836792
Epoch 142  Train loss: -0.049476709786583396  Val loss: 0.006249514846867302
Epoch 143
-------------------------------
Batch 1/64 loss: -0.03358113765716553
Batch 2/64 loss: -0.06943529844284058
Batch 3/64 loss: -0.04312509298324585
Batch 4/64 loss: -0.06322729587554932
Batch 5/64 loss: -0.03629106283187866
Batch 6/64 loss: -0.04531657695770264
Batch 7/64 loss: -0.06137192249298096
Batch 8/64 loss: -0.054130733013153076
Batch 9/64 loss: -0.025520920753479004
Batch 10/64 loss: -0.05723065137863159
Batch 11/64 loss: -0.05793321132659912
Batch 12/64 loss: -0.05171555280685425
Batch 13/64 loss: -0.04115086793899536
Batch 14/64 loss: -0.04320359230041504
Batch 15/64 loss: -0.05506175756454468
Batch 16/64 loss: -0.06270521879196167
Batch 17/64 loss: -0.049034833908081055
Batch 18/64 loss: -0.05701416730880737
Batch 19/64 loss: -0.05346107482910156
Batch 20/64 loss: -0.03634822368621826
Batch 21/64 loss: -0.06910848617553711
Batch 22/64 loss: -0.03594166040420532
Batch 23/64 loss: -0.048225462436676025
Batch 24/64 loss: -0.05924797058105469
Batch 25/64 loss: -0.0668250322341919
Batch 26/64 loss: -0.0501173734664917
Batch 27/64 loss: -0.02653646469116211
Batch 28/64 loss: -0.032736897468566895
Batch 29/64 loss: -0.05424219369888306
Batch 30/64 loss: -0.04697537422180176
Batch 31/64 loss: -0.06639611721038818
Batch 32/64 loss: -0.04821658134460449
Batch 33/64 loss: -0.06763350963592529
Batch 34/64 loss: -0.05411142110824585
Batch 35/64 loss: -0.047006070613861084
Batch 36/64 loss: -0.057711243629455566
Batch 37/64 loss: -0.05795133113861084
Batch 38/64 loss: -0.038925349712371826
Batch 39/64 loss: -0.029257118701934814
Batch 40/64 loss: -0.043652892112731934
Batch 41/64 loss: -0.04371386766433716
Batch 42/64 loss: -0.04954946041107178
Batch 43/64 loss: -0.031430721282958984
Batch 44/64 loss: -0.04669839143753052
Batch 45/64 loss: -0.026375293731689453
Batch 46/64 loss: -0.05859309434890747
Batch 47/64 loss: -0.050696611404418945
Batch 48/64 loss: -0.04497110843658447
Batch 49/64 loss: -0.02631276845932007
Batch 50/64 loss: -0.05316317081451416
Batch 51/64 loss: -0.04544198513031006
Batch 52/64 loss: -0.04042690992355347
Batch 53/64 loss: -0.04569554328918457
Batch 54/64 loss: -0.05381578207015991
Batch 55/64 loss: -0.04178684949874878
Batch 56/64 loss: -0.05940043926239014
Batch 57/64 loss: -0.05228996276855469
Batch 58/64 loss: -0.05713152885437012
Batch 59/64 loss: -0.03535729646682739
Batch 60/64 loss: -0.047411203384399414
Batch 61/64 loss: -0.03161805868148804
Batch 62/64 loss: -0.06612515449523926
Batch 63/64 loss: -0.053782761096954346
Batch 64/64 loss: -0.05353903770446777
Epoch 143  Train loss: -0.04862148995492972  Val loss: -0.008333359182495432
Epoch 144
-------------------------------
Batch 1/64 loss: -0.05481123924255371
Batch 2/64 loss: -0.06083416938781738
Batch 3/64 loss: -0.0487787127494812
Batch 4/64 loss: -0.05338543653488159
Batch 5/64 loss: -0.014110743999481201
Batch 6/64 loss: -0.05396533012390137
Batch 7/64 loss: -0.06209975481033325
Batch 8/64 loss: -0.04456651210784912
Batch 9/64 loss: -0.052548766136169434
Batch 10/64 loss: -0.055300354957580566
Batch 11/64 loss: -0.059198975563049316
Batch 12/64 loss: -0.061720848083496094
Batch 13/64 loss: -0.059691667556762695
Batch 14/64 loss: -0.02723485231399536
Batch 15/64 loss: -0.04385721683502197
Batch 16/64 loss: -0.03919529914855957
Batch 17/64 loss: -0.037182629108428955
Batch 18/64 loss: -0.07407212257385254
Batch 19/64 loss: -0.041674017906188965
Batch 20/64 loss: -0.05008798837661743
Batch 21/64 loss: -0.05449330806732178
Batch 22/64 loss: -0.05435532331466675
Batch 23/64 loss: -0.06782305240631104
Batch 24/64 loss: -0.04460722208023071
Batch 25/64 loss: -0.05295521020889282
Batch 26/64 loss: -0.04611259698867798
Batch 27/64 loss: -0.04532778263092041
Batch 28/64 loss: -0.043390631675720215
Batch 29/64 loss: -0.06678533554077148
Batch 30/64 loss: -0.05648183822631836
Batch 31/64 loss: -0.05227261781692505
Batch 32/64 loss: -0.02834486961364746
Batch 33/64 loss: -0.04750418663024902
Batch 34/64 loss: -0.043612003326416016
Batch 35/64 loss: -0.04973578453063965
Batch 36/64 loss: -0.05562901496887207
Batch 37/64 loss: -0.06601309776306152
Batch 38/64 loss: -0.06354272365570068
Batch 39/64 loss: -0.05307304859161377
Batch 40/64 loss: -0.040529727935791016
Batch 41/64 loss: -0.0453953742980957
Batch 42/64 loss: -0.0457497239112854
Batch 43/64 loss: -0.046195268630981445
Batch 44/64 loss: -0.05099993944168091
Batch 45/64 loss: -0.04623693227767944
Batch 46/64 loss: -0.06368935108184814
Batch 47/64 loss: -0.05769813060760498
Batch 48/64 loss: -0.03329432010650635
Batch 49/64 loss: -0.04025924205780029
Batch 50/64 loss: -0.038049280643463135
Batch 51/64 loss: -0.036085426807403564
Batch 52/64 loss: -0.0632028579711914
Batch 53/64 loss: -0.04619109630584717
Batch 54/64 loss: -0.060553669929504395
Batch 55/64 loss: -0.03609281778335571
Batch 56/64 loss: -0.04295933246612549
Batch 57/64 loss: -0.03744274377822876
Batch 58/64 loss: -0.04535233974456787
Batch 59/64 loss: -0.04658055305480957
Batch 60/64 loss: -0.03844732046127319
Batch 61/64 loss: -0.05102574825286865
Batch 62/64 loss: -0.05389082431793213
Batch 63/64 loss: -0.0499381422996521
Batch 64/64 loss: -0.0452461838722229
Epoch 144  Train loss: -0.0491948405901591  Val loss: -0.009546045380359663
Saving best model, epoch: 144
Epoch 145
-------------------------------
Batch 1/64 loss: -0.066489577293396
Batch 2/64 loss: -0.05876559019088745
Batch 3/64 loss: -0.08263146877288818
Batch 4/64 loss: -0.016578376293182373
Batch 5/64 loss: -0.022609412670135498
Batch 6/64 loss: -0.044501662254333496
Batch 7/64 loss: -0.032598018646240234
Batch 8/64 loss: -0.05456531047821045
Batch 9/64 loss: -0.042677462100982666
Batch 10/64 loss: -0.057118237018585205
Batch 11/64 loss: -0.06244868040084839
Batch 12/64 loss: -0.055763304233551025
Batch 13/64 loss: -0.023953557014465332
Batch 14/64 loss: -0.04227513074874878
Batch 15/64 loss: -0.04991370439529419
Batch 16/64 loss: -0.043232262134552
Batch 17/64 loss: -0.06240701675415039
Batch 18/64 loss: -0.04409348964691162
Batch 19/64 loss: -0.0585712194442749
Batch 20/64 loss: -0.05658841133117676
Batch 21/64 loss: -0.06850647926330566
Batch 22/64 loss: -0.0320509672164917
Batch 23/64 loss: -0.047394752502441406
Batch 24/64 loss: -0.045152127742767334
Batch 25/64 loss: -0.05293571949005127
Batch 26/64 loss: -0.06295537948608398
Batch 27/64 loss: -0.061288654804229736
Batch 28/64 loss: -0.04934936761856079
Batch 29/64 loss: -0.044714927673339844
Batch 30/64 loss: -0.05470091104507446
Batch 31/64 loss: -0.0711100697517395
Batch 32/64 loss: -0.053701579570770264
Batch 33/64 loss: -0.05491751432418823
Batch 34/64 loss: -0.05740964412689209
Batch 35/64 loss: -0.0521126389503479
Batch 36/64 loss: -0.062157511711120605
Batch 37/64 loss: -0.05958151817321777
Batch 38/64 loss: -0.050991833209991455
Batch 39/64 loss: -0.04330998659133911
Batch 40/64 loss: -0.05645555257797241
Batch 41/64 loss: -0.04244232177734375
Batch 42/64 loss: -0.05458348989486694
Batch 43/64 loss: -0.060604095458984375
Batch 44/64 loss: -0.05421179533004761
Batch 45/64 loss: -0.038773298263549805
Batch 46/64 loss: -0.054114580154418945
Batch 47/64 loss: -0.0505375862121582
Batch 48/64 loss: -0.06252741813659668
Batch 49/64 loss: -0.04270273447036743
Batch 50/64 loss: -0.04967975616455078
Batch 51/64 loss: -0.05850958824157715
Batch 52/64 loss: -0.06049549579620361
Batch 53/64 loss: -0.06293749809265137
Batch 54/64 loss: -0.045755863189697266
Batch 55/64 loss: -0.06023591756820679
Batch 56/64 loss: -0.039695143699645996
Batch 57/64 loss: -0.04859113693237305
Batch 58/64 loss: -0.04347306489944458
Batch 59/64 loss: -0.04922592639923096
Batch 60/64 loss: -0.03464227914810181
Batch 61/64 loss: -0.058357179164886475
Batch 62/64 loss: -0.062211811542510986
Batch 63/64 loss: -0.03814840316772461
Batch 64/64 loss: -0.05546349287033081
Epoch 145  Train loss: -0.05133542009428436  Val loss: -0.005035433982246111
Epoch 146
-------------------------------
Batch 1/64 loss: -0.041124165058135986
Batch 2/64 loss: -0.044046223163604736
Batch 3/64 loss: -0.04626452922821045
Batch 4/64 loss: -0.06001722812652588
Batch 5/64 loss: -0.05783116817474365
Batch 6/64 loss: -0.045961737632751465
Batch 7/64 loss: -0.05037391185760498
Batch 8/64 loss: -0.062396347522735596
Batch 9/64 loss: -0.0721164345741272
Batch 10/64 loss: -0.05530339479446411
Batch 11/64 loss: -0.06921011209487915
Batch 12/64 loss: -0.060206830501556396
Batch 13/64 loss: -0.06550318002700806
Batch 14/64 loss: -0.04716289043426514
Batch 15/64 loss: -0.060975492000579834
Batch 16/64 loss: -0.05632627010345459
Batch 17/64 loss: -0.06997889280319214
Batch 18/64 loss: -0.06013131141662598
Batch 19/64 loss: -0.020605266094207764
Batch 20/64 loss: -0.0474279522895813
Batch 21/64 loss: -0.049846649169921875
Batch 22/64 loss: -0.030313551425933838
Batch 23/64 loss: -0.04323852062225342
Batch 24/64 loss: -0.05489403009414673
Batch 25/64 loss: -0.04644203186035156
Batch 26/64 loss: -0.06269633769989014
Batch 27/64 loss: -0.04795074462890625
Batch 28/64 loss: -0.07071995735168457
Batch 29/64 loss: -0.05564534664154053
Batch 30/64 loss: -0.059115052223205566
Batch 31/64 loss: -0.07213407754898071
Batch 32/64 loss: -0.048914551734924316
Batch 33/64 loss: -0.055883705615997314
Batch 34/64 loss: -0.059311509132385254
Batch 35/64 loss: -0.04865473508834839
Batch 36/64 loss: -0.06508749723434448
Batch 37/64 loss: -0.041909098625183105
Batch 38/64 loss: -0.021854400634765625
Batch 39/64 loss: -0.03956127166748047
Batch 40/64 loss: -0.036610305309295654
Batch 41/64 loss: -0.050165534019470215
Batch 42/64 loss: -0.04659658670425415
Batch 43/64 loss: -0.033828139305114746
Batch 44/64 loss: -0.02764451503753662
Batch 45/64 loss: -0.05570894479751587
Batch 46/64 loss: -0.05633270740509033
Batch 47/64 loss: -0.05432707071304321
Batch 48/64 loss: -0.02261948585510254
Batch 49/64 loss: -0.03256881237030029
Batch 50/64 loss: -0.04286062717437744
Batch 51/64 loss: -0.057933926582336426
Batch 52/64 loss: -0.04270315170288086
Batch 53/64 loss: -0.0551186203956604
Batch 54/64 loss: -0.06047928333282471
Batch 55/64 loss: -0.0394817590713501
Batch 56/64 loss: -0.06325149536132812
Batch 57/64 loss: -0.05556678771972656
Batch 58/64 loss: -0.04414498805999756
Batch 59/64 loss: -0.04910761117935181
Batch 60/64 loss: -0.05302560329437256
Batch 61/64 loss: -0.054515540599823
Batch 62/64 loss: -0.06442689895629883
Batch 63/64 loss: -0.0524982213973999
Batch 64/64 loss: -0.05491340160369873
Epoch 146  Train loss: -0.051134714893266264  Val loss: -0.008453540990442755
Epoch 147
-------------------------------
Batch 1/64 loss: -0.05419015884399414
Batch 2/64 loss: -0.04887491464614868
Batch 3/64 loss: -0.041655778884887695
Batch 4/64 loss: -0.0474429726600647
Batch 5/64 loss: -0.06367427110671997
Batch 6/64 loss: -0.06630086898803711
Batch 7/64 loss: -0.03937232494354248
Batch 8/64 loss: -0.048096656799316406
Batch 9/64 loss: -0.047327518463134766
Batch 10/64 loss: -0.056513965129852295
Batch 11/64 loss: -0.06268548965454102
Batch 12/64 loss: -0.04574459791183472
Batch 13/64 loss: -0.04999744892120361
Batch 14/64 loss: -0.05028790235519409
Batch 15/64 loss: -0.06295859813690186
Batch 16/64 loss: -0.05293619632720947
Batch 17/64 loss: -0.04023551940917969
Batch 18/64 loss: -0.04299807548522949
Batch 19/64 loss: -0.06139284372329712
Batch 20/64 loss: -0.06184089183807373
Batch 21/64 loss: -0.05609297752380371
Batch 22/64 loss: -0.051968395709991455
Batch 23/64 loss: -0.06016552448272705
Batch 24/64 loss: -0.05356144905090332
Batch 25/64 loss: -0.05259501934051514
Batch 26/64 loss: -0.04658252000808716
Batch 27/64 loss: -0.03256428241729736
Batch 28/64 loss: -0.05662590265274048
Batch 29/64 loss: -0.0535351037979126
Batch 30/64 loss: -0.06046015024185181
Batch 31/64 loss: -0.05251908302307129
Batch 32/64 loss: -0.05354046821594238
Batch 33/64 loss: -0.03394877910614014
Batch 34/64 loss: -0.06577032804489136
Batch 35/64 loss: -0.04820960760116577
Batch 36/64 loss: -0.05446267127990723
Batch 37/64 loss: -0.06818825006484985
Batch 38/64 loss: -0.06985098123550415
Batch 39/64 loss: -0.06967341899871826
Batch 40/64 loss: -0.05623871088027954
Batch 41/64 loss: -0.047436654567718506
Batch 42/64 loss: -0.04932844638824463
Batch 43/64 loss: -0.057733356952667236
Batch 44/64 loss: -0.05452805757522583
Batch 45/64 loss: -0.0676494836807251
Batch 46/64 loss: -0.04163771867752075
Batch 47/64 loss: -0.05463099479675293
Batch 48/64 loss: -0.062293291091918945
Batch 49/64 loss: -0.05148470401763916
Batch 50/64 loss: -0.051889002323150635
Batch 51/64 loss: -0.05727130174636841
Batch 52/64 loss: -0.05122572183609009
Batch 53/64 loss: -0.05436420440673828
Batch 54/64 loss: -0.05528903007507324
Batch 55/64 loss: -0.047583937644958496
Batch 56/64 loss: -0.0531654953956604
Batch 57/64 loss: -0.0671687126159668
Batch 58/64 loss: -0.03749758005142212
Batch 59/64 loss: -0.05406695604324341
Batch 60/64 loss: -0.06344163417816162
Batch 61/64 loss: -0.05977505445480347
Batch 62/64 loss: -0.07641160488128662
Batch 63/64 loss: -0.04997551441192627
Batch 64/64 loss: -0.04066324234008789
Epoch 147  Train loss: -0.05392041580349791  Val loss: -0.009986092749330187
Saving best model, epoch: 147
Epoch 148
-------------------------------
Batch 1/64 loss: -0.04293489456176758
Batch 2/64 loss: -0.051042020320892334
Batch 3/64 loss: -0.05888330936431885
Batch 4/64 loss: -0.03282588720321655
Batch 5/64 loss: -0.06789827346801758
Batch 6/64 loss: -0.06774604320526123
Batch 7/64 loss: -0.06020355224609375
Batch 8/64 loss: -0.0549888014793396
Batch 9/64 loss: -0.04720962047576904
Batch 10/64 loss: -0.049064695835113525
Batch 11/64 loss: -0.0489007830619812
Batch 12/64 loss: -0.05169475078582764
Batch 13/64 loss: -0.06023603677749634
Batch 14/64 loss: -0.05217468738555908
Batch 15/64 loss: -0.06768923997879028
Batch 16/64 loss: -0.0508800745010376
Batch 17/64 loss: -0.048588335514068604
Batch 18/64 loss: -0.06399452686309814
Batch 19/64 loss: -0.04956698417663574
Batch 20/64 loss: -0.056050002574920654
Batch 21/64 loss: -0.05515420436859131
Batch 22/64 loss: -0.051381826400756836
Batch 23/64 loss: -0.04620081186294556
Batch 24/64 loss: -0.06703406572341919
Batch 25/64 loss: -0.05805307626724243
Batch 26/64 loss: -0.05955159664154053
Batch 27/64 loss: -0.06790024042129517
Batch 28/64 loss: -0.054699718952178955
Batch 29/64 loss: -0.056532204151153564
Batch 30/64 loss: -0.06496632099151611
Batch 31/64 loss: -0.06402385234832764
Batch 32/64 loss: -0.060982584953308105
Batch 33/64 loss: -0.054739177227020264
Batch 34/64 loss: -0.04248940944671631
Batch 35/64 loss: -0.05036187171936035
Batch 36/64 loss: -0.06696075201034546
Batch 37/64 loss: -0.06351089477539062
Batch 38/64 loss: -0.06918096542358398
Batch 39/64 loss: -0.06445503234863281
Batch 40/64 loss: -0.04777592420578003
Batch 41/64 loss: -0.05665397644042969
Batch 42/64 loss: -0.025506794452667236
Batch 43/64 loss: -0.05937439203262329
Batch 44/64 loss: -0.033342182636260986
Batch 45/64 loss: -0.05817288160324097
Batch 46/64 loss: -0.06699591875076294
Batch 47/64 loss: -0.06467056274414062
Batch 48/64 loss: -0.03861892223358154
Batch 49/64 loss: -0.06021082401275635
Batch 50/64 loss: -0.06360983848571777
Batch 51/64 loss: -0.05423778295516968
Batch 52/64 loss: -0.06578779220581055
Batch 53/64 loss: -0.0694311261177063
Batch 54/64 loss: -0.0373653769493103
Batch 55/64 loss: -0.046535491943359375
Batch 56/64 loss: -0.019125699996948242
Batch 57/64 loss: -0.01643657684326172
Batch 58/64 loss: -0.04987984895706177
Batch 59/64 loss: -0.06214362382888794
Batch 60/64 loss: -0.04398822784423828
Batch 61/64 loss: -0.036407411098480225
Batch 62/64 loss: -0.054475605487823486
Batch 63/64 loss: -0.05849844217300415
Batch 64/64 loss: -0.03798884153366089
Epoch 148  Train loss: -0.05362334087783215  Val loss: -0.0092366143190574
Epoch 149
-------------------------------
Batch 1/64 loss: -0.046779632568359375
Batch 2/64 loss: -0.05112659931182861
Batch 3/64 loss: -0.06340330839157104
Batch 4/64 loss: -0.05848950147628784
Batch 5/64 loss: -0.04877811670303345
Batch 6/64 loss: -0.05796611309051514
Batch 7/64 loss: -0.05329829454421997
Batch 8/64 loss: -0.08056241273880005
Batch 9/64 loss: -0.06564158201217651
Batch 10/64 loss: -0.043460845947265625
Batch 11/64 loss: -0.06977170705795288
Batch 12/64 loss: -0.06750631332397461
Batch 13/64 loss: -0.05623871088027954
Batch 14/64 loss: -0.04379916191101074
Batch 15/64 loss: -0.066062331199646
Batch 16/64 loss: -0.0511661171913147
Batch 17/64 loss: -0.047006964683532715
Batch 18/64 loss: -0.05351817607879639
Batch 19/64 loss: -0.04503518342971802
Batch 20/64 loss: -0.04746514558792114
Batch 21/64 loss: -0.06830644607543945
Batch 22/64 loss: -0.06788450479507446
Batch 23/64 loss: -0.05004453659057617
Batch 24/64 loss: -0.05531841516494751
Batch 25/64 loss: -0.06639236211776733
Batch 26/64 loss: -0.05363929271697998
Batch 27/64 loss: -0.052989959716796875
Batch 28/64 loss: -0.05555164813995361
Batch 29/64 loss: -0.05877220630645752
Batch 30/64 loss: -0.0834507942199707
Batch 31/64 loss: -0.056803345680236816
Batch 32/64 loss: -0.06802922487258911
Batch 33/64 loss: -0.04976928234100342
Batch 34/64 loss: -0.06521821022033691
Batch 35/64 loss: -0.05997282266616821
Batch 36/64 loss: -0.04205423593521118
Batch 37/64 loss: -0.07407248020172119
Batch 38/64 loss: -0.05639517307281494
Batch 39/64 loss: -0.04644662141799927
Batch 40/64 loss: -0.028348684310913086
Batch 41/64 loss: -0.06044286489486694
Batch 42/64 loss: -0.05302006006240845
Batch 43/64 loss: -0.04280674457550049
Batch 44/64 loss: -0.042990028858184814
Batch 45/64 loss: -0.036083757877349854
Batch 46/64 loss: -0.06127572059631348
Batch 47/64 loss: -0.06181454658508301
Batch 48/64 loss: -0.0540347695350647
Batch 49/64 loss: -0.05812019109725952
Batch 50/64 loss: -0.0242156982421875
Batch 51/64 loss: -0.05773031711578369
Batch 52/64 loss: -0.056828856468200684
Batch 53/64 loss: -0.05418109893798828
Batch 54/64 loss: -0.05405890941619873
Batch 55/64 loss: -0.012605130672454834
Batch 56/64 loss: -0.058704912662506104
Batch 57/64 loss: -0.04053717851638794
Batch 58/64 loss: -0.05491536855697632
Batch 59/64 loss: -0.04465204477310181
Batch 60/64 loss: -0.07299339771270752
Batch 61/64 loss: -0.040618717670440674
Batch 62/64 loss: -0.0466880202293396
Batch 63/64 loss: -0.06506240367889404
Batch 64/64 loss: -0.048734426498413086
Epoch 149  Train loss: -0.05439165526745366  Val loss: -0.00973436512897924
Epoch 150
-------------------------------
Batch 1/64 loss: -0.06585979461669922
Batch 2/64 loss: -0.07260119915008545
Batch 3/64 loss: -0.06944072246551514
Batch 4/64 loss: -0.026156842708587646
Batch 5/64 loss: -0.06573182344436646
Batch 6/64 loss: -0.06438630819320679
Batch 7/64 loss: -0.05002957582473755
Batch 8/64 loss: -0.07564246654510498
Batch 9/64 loss: -0.051761746406555176
Batch 10/64 loss: -0.04346036911010742
Batch 11/64 loss: -0.06944513320922852
Batch 12/64 loss: -0.0582273006439209
Batch 13/64 loss: -0.05504655838012695
Batch 14/64 loss: -0.04218149185180664
Batch 15/64 loss: -0.056053996086120605
Batch 16/64 loss: -0.05140775442123413
Batch 17/64 loss: -0.05037778615951538
Batch 18/64 loss: -0.012102365493774414
Batch 19/64 loss: -0.03704357147216797
Batch 20/64 loss: -0.05643653869628906
Batch 21/64 loss: -0.04257667064666748
Batch 22/64 loss: -0.06054562330245972
Batch 23/64 loss: -0.05200958251953125
Batch 24/64 loss: -0.05334019660949707
Batch 25/64 loss: -0.05765366554260254
Batch 26/64 loss: -0.06486564874649048
Batch 27/64 loss: -0.05426514148712158
Batch 28/64 loss: -0.053710997104644775
Batch 29/64 loss: -0.04012054204940796
Batch 30/64 loss: -0.05024862289428711
Batch 31/64 loss: -0.05469542741775513
Batch 32/64 loss: -0.04892390966415405
Batch 33/64 loss: -0.04950547218322754
Batch 34/64 loss: -0.039222002029418945
Batch 35/64 loss: -0.030296921730041504
Batch 36/64 loss: -0.033971309661865234
Batch 37/64 loss: -0.036816537380218506
Batch 38/64 loss: -0.05568218231201172
Batch 39/64 loss: -0.05074030160903931
Batch 40/64 loss: -0.05683112144470215
Batch 41/64 loss: -0.0293007493019104
Batch 42/64 loss: -0.05200552940368652
Batch 43/64 loss: -0.06564658880233765
Batch 44/64 loss: -0.06489789485931396
Batch 45/64 loss: -0.06602638959884644
Batch 46/64 loss: -0.05474865436553955
Batch 47/64 loss: -0.028787612915039062
Batch 48/64 loss: -0.04472970962524414
Batch 49/64 loss: -0.05775642395019531
Batch 50/64 loss: -0.061207592487335205
Batch 51/64 loss: -0.06420087814331055
Batch 52/64 loss: -0.04388415813446045
Batch 53/64 loss: -0.04997897148132324
Batch 54/64 loss: -0.055449604988098145
Batch 55/64 loss: -0.05206489562988281
Batch 56/64 loss: -0.043091535568237305
Batch 57/64 loss: -0.03818714618682861
Batch 58/64 loss: -0.07249677181243896
Batch 59/64 loss: -0.04127311706542969
Batch 60/64 loss: -0.0180395245552063
Batch 61/64 loss: -0.056696951389312744
Batch 62/64 loss: -0.049521446228027344
Batch 63/64 loss: -0.058901429176330566
Batch 64/64 loss: -0.06395363807678223
Epoch 150  Train loss: -0.051392533732395544  Val loss: -0.005188730778972718
Epoch 151
-------------------------------
Batch 1/64 loss: -0.05156928300857544
Batch 2/64 loss: -0.05663555860519409
Batch 3/64 loss: -0.04339355230331421
Batch 4/64 loss: -0.05669403076171875
Batch 5/64 loss: -0.06382453441619873
Batch 6/64 loss: -0.07683366537094116
Batch 7/64 loss: -0.04360872507095337
Batch 8/64 loss: -0.06193101406097412
Batch 9/64 loss: -0.04839301109313965
Batch 10/64 loss: -0.0466037392616272
Batch 11/64 loss: -0.04938197135925293
Batch 12/64 loss: -0.055558621883392334
Batch 13/64 loss: -0.04572618007659912
Batch 14/64 loss: -0.03738611936569214
Batch 15/64 loss: -0.06050598621368408
Batch 16/64 loss: -0.07278567552566528
Batch 17/64 loss: -0.0631033182144165
Batch 18/64 loss: -0.06931507587432861
Batch 19/64 loss: -0.05326181650161743
Batch 20/64 loss: -0.05918055772781372
Batch 21/64 loss: -0.07398313283920288
Batch 22/64 loss: -0.05499356985092163
Batch 23/64 loss: -0.06159573793411255
Batch 24/64 loss: -0.06602954864501953
Batch 25/64 loss: -0.05397003889083862
Batch 26/64 loss: -0.06893932819366455
Batch 27/64 loss: -0.07615113258361816
Batch 28/64 loss: -0.07121884822845459
Batch 29/64 loss: -0.07566046714782715
Batch 30/64 loss: -0.06340599060058594
Batch 31/64 loss: -0.06011563539505005
Batch 32/64 loss: -0.03990435600280762
Batch 33/64 loss: -0.043180227279663086
Batch 34/64 loss: -0.04271399974822998
Batch 35/64 loss: -0.040624022483825684
Batch 36/64 loss: -0.06964612007141113
Batch 37/64 loss: -0.04863828420639038
Batch 38/64 loss: -0.08169019222259521
Batch 39/64 loss: -0.05353426933288574
Batch 40/64 loss: -0.08442080020904541
Batch 41/64 loss: -0.046920180320739746
Batch 42/64 loss: -0.06684577465057373
Batch 43/64 loss: -0.04951286315917969
Batch 44/64 loss: -0.051443636417388916
Batch 45/64 loss: -0.06147795915603638
Batch 46/64 loss: -0.02591860294342041
Batch 47/64 loss: -0.06603574752807617
Batch 48/64 loss: -0.03246331214904785
Batch 49/64 loss: -0.03340023756027222
Batch 50/64 loss: -0.05928504467010498
Batch 51/64 loss: -0.04558295011520386
Batch 52/64 loss: -0.06518089771270752
Batch 53/64 loss: -0.06229609251022339
Batch 54/64 loss: -0.052084147930145264
Batch 55/64 loss: -0.05716979503631592
Batch 56/64 loss: -0.04596292972564697
Batch 57/64 loss: -0.05826205015182495
Batch 58/64 loss: -0.045694708824157715
Batch 59/64 loss: -0.047850728034973145
Batch 60/64 loss: -0.058902621269226074
Batch 61/64 loss: -0.03175210952758789
Batch 62/64 loss: -0.04730576276779175
Batch 63/64 loss: -0.05479288101196289
Batch 64/64 loss: -0.05123698711395264
Epoch 151  Train loss: -0.05569689273834229  Val loss: -0.008351050701337992
Epoch 152
-------------------------------
Batch 1/64 loss: -0.06824302673339844
Batch 2/64 loss: -0.038590192794799805
Batch 3/64 loss: -0.06459558010101318
Batch 4/64 loss: -0.05154681205749512
Batch 5/64 loss: -0.05836755037307739
Batch 6/64 loss: -0.06710749864578247
Batch 7/64 loss: -0.06849974393844604
Batch 8/64 loss: -0.06707882881164551
Batch 9/64 loss: -0.056215643882751465
Batch 10/64 loss: -0.06165611743927002
Batch 11/64 loss: -0.06709492206573486
Batch 12/64 loss: -0.05105853080749512
Batch 13/64 loss: -0.06628024578094482
Batch 14/64 loss: -0.04407232999801636
Batch 15/64 loss: -0.05861926078796387
Batch 16/64 loss: -0.04936790466308594
Batch 17/64 loss: -0.0647733211517334
Batch 18/64 loss: -0.05137801170349121
Batch 19/64 loss: -0.0328754186630249
Batch 20/64 loss: -0.05291306972503662
Batch 21/64 loss: -0.07141518592834473
Batch 22/64 loss: -0.048335492610931396
Batch 23/64 loss: -0.0458185076713562
Batch 24/64 loss: -0.04740673303604126
Batch 25/64 loss: -0.06554555892944336
Batch 26/64 loss: -0.06744974851608276
Batch 27/64 loss: -0.05892258882522583
Batch 28/64 loss: -0.06240898370742798
Batch 29/64 loss: -0.08345180749893188
Batch 30/64 loss: -0.0680658221244812
Batch 31/64 loss: -0.04182249307632446
Batch 32/64 loss: -0.0383448600769043
Batch 33/64 loss: -0.050692975521087646
Batch 34/64 loss: -0.06977218389511108
Batch 35/64 loss: -0.042958080768585205
Batch 36/64 loss: -0.0639771819114685
Batch 37/64 loss: -0.046006202697753906
Batch 38/64 loss: -0.07342863082885742
Batch 39/64 loss: -0.04539996385574341
Batch 40/64 loss: -0.050142526626586914
Batch 41/64 loss: -0.05511653423309326
Batch 42/64 loss: -0.04763740301132202
Batch 43/64 loss: -0.049138009548187256
Batch 44/64 loss: -0.04566830396652222
Batch 45/64 loss: -0.05448639392852783
Batch 46/64 loss: -0.06760942935943604
Batch 47/64 loss: -0.033991336822509766
Batch 48/64 loss: -0.029052555561065674
Batch 49/64 loss: -0.057468295097351074
Batch 50/64 loss: -0.04338383674621582
Batch 51/64 loss: -0.06074988842010498
Batch 52/64 loss: -0.061548709869384766
Batch 53/64 loss: -0.04268515110015869
Batch 54/64 loss: -0.06856685876846313
Batch 55/64 loss: -0.04693037271499634
Batch 56/64 loss: -0.05770748853683472
Batch 57/64 loss: -0.0591316819190979
Batch 58/64 loss: -0.028768539428710938
Batch 59/64 loss: -0.055812299251556396
Batch 60/64 loss: -0.05003875494003296
Batch 61/64 loss: -0.050477802753448486
Batch 62/64 loss: -0.07963341474533081
Batch 63/64 loss: -0.04911613464355469
Batch 64/64 loss: -0.04859423637390137
Epoch 152  Train loss: -0.055103755464740826  Val loss: -0.00851535960980707
Epoch 153
-------------------------------
Batch 1/64 loss: -0.0644005537033081
Batch 2/64 loss: -0.06547218561172485
Batch 3/64 loss: -0.06156325340270996
Batch 4/64 loss: -0.029299378395080566
Batch 5/64 loss: -0.0547749400138855
Batch 6/64 loss: -0.06842577457427979
Batch 7/64 loss: -0.04962873458862305
Batch 8/64 loss: -0.052880942821502686
Batch 9/64 loss: -0.05291426181793213
Batch 10/64 loss: -0.056944310665130615
Batch 11/64 loss: -0.0509641170501709
Batch 12/64 loss: -0.06916189193725586
Batch 13/64 loss: -0.0690995454788208
Batch 14/64 loss: -0.046957194805145264
Batch 15/64 loss: -0.051145315170288086
Batch 16/64 loss: -0.04341447353363037
Batch 17/64 loss: -0.04673081636428833
Batch 18/64 loss: -0.07007396221160889
Batch 19/64 loss: -0.04401111602783203
Batch 20/64 loss: -0.03157228231430054
Batch 21/64 loss: -0.05730050802230835
Batch 22/64 loss: -0.056538164615631104
Batch 23/64 loss: -0.03725302219390869
Batch 24/64 loss: -0.05237346887588501
Batch 25/64 loss: -0.0460432767868042
Batch 26/64 loss: -0.06194424629211426
Batch 27/64 loss: -0.05357003211975098
Batch 28/64 loss: -0.06944888830184937
Batch 29/64 loss: -0.06347370147705078
Batch 30/64 loss: -0.05883228778839111
Batch 31/64 loss: -0.03753352165222168
Batch 32/64 loss: -0.06097453832626343
Batch 33/64 loss: -0.04864233732223511
Batch 34/64 loss: -0.05522489547729492
Batch 35/64 loss: -0.061144351959228516
Batch 36/64 loss: -0.05531340837478638
Batch 37/64 loss: -0.06401324272155762
Batch 38/64 loss: -0.04935657978057861
Batch 39/64 loss: -0.04681730270385742
Batch 40/64 loss: -0.0415421724319458
Batch 41/64 loss: -0.051459550857543945
Batch 42/64 loss: -0.06195646524429321
Batch 43/64 loss: -0.03357696533203125
Batch 44/64 loss: -0.06577193737030029
Batch 45/64 loss: -0.05452042818069458
Batch 46/64 loss: -0.06440317630767822
Batch 47/64 loss: -0.04354429244995117
Batch 48/64 loss: -0.03960895538330078
Batch 49/64 loss: -0.042862772941589355
Batch 50/64 loss: -0.05981791019439697
Batch 51/64 loss: -0.04497802257537842
Batch 52/64 loss: -0.05499839782714844
Batch 53/64 loss: -0.06183558702468872
Batch 54/64 loss: -0.06452769041061401
Batch 55/64 loss: -0.04766571521759033
Batch 56/64 loss: -0.05624645948410034
Batch 57/64 loss: -0.04806029796600342
Batch 58/64 loss: -0.07515347003936768
Batch 59/64 loss: -0.040516674518585205
Batch 60/64 loss: -0.048155903816223145
Batch 61/64 loss: -0.05538487434387207
Batch 62/64 loss: -0.03289669752120972
Batch 63/64 loss: -0.03947281837463379
Batch 64/64 loss: -0.06967902183532715
Epoch 153  Train loss: -0.05327763650931564  Val loss: -0.00900205037847827
Epoch 154
-------------------------------
Batch 1/64 loss: -0.05406022071838379
Batch 2/64 loss: -0.03880500793457031
Batch 3/64 loss: -0.05502587556838989
Batch 4/64 loss: -0.07094663381576538
Batch 5/64 loss: -0.06776297092437744
Batch 6/64 loss: -0.06404978036880493
Batch 7/64 loss: -0.04997611045837402
Batch 8/64 loss: -0.06854546070098877
Batch 9/64 loss: -0.058816492557525635
Batch 10/64 loss: -0.06459194421768188
Batch 11/64 loss: -0.05942779779434204
Batch 12/64 loss: -0.057884156703948975
Batch 13/64 loss: -0.05112367868423462
Batch 14/64 loss: -0.06590956449508667
Batch 15/64 loss: -0.056987226009368896
Batch 16/64 loss: -0.06408238410949707
Batch 17/64 loss: -0.07555371522903442
Batch 18/64 loss: -0.023293614387512207
Batch 19/64 loss: -0.04198932647705078
Batch 20/64 loss: -0.04997020959854126
Batch 21/64 loss: -0.047332584857940674
Batch 22/64 loss: -0.056338369846343994
Batch 23/64 loss: -0.0675235390663147
Batch 24/64 loss: -0.04681968688964844
Batch 25/64 loss: -0.04548752307891846
Batch 26/64 loss: -0.056875407695770264
Batch 27/64 loss: -0.07537692785263062
Batch 28/64 loss: -0.048577964305877686
Batch 29/64 loss: -0.06343555450439453
Batch 30/64 loss: -0.05871051549911499
Batch 31/64 loss: -0.04095447063446045
Batch 32/64 loss: -0.04645407199859619
Batch 33/64 loss: -0.05830568075180054
Batch 34/64 loss: -0.06897097826004028
Batch 35/64 loss: -0.04689663648605347
Batch 36/64 loss: -0.07926714420318604
Batch 37/64 loss: -0.07240289449691772
Batch 38/64 loss: -0.0649268627166748
Batch 39/64 loss: -0.07672369480133057
Batch 40/64 loss: -0.0640639066696167
Batch 41/64 loss: -0.04472947120666504
Batch 42/64 loss: -0.051007747650146484
Batch 43/64 loss: -0.050607383251190186
Batch 44/64 loss: -0.04998147487640381
Batch 45/64 loss: -0.06783038377761841
Batch 46/64 loss: -0.050800204277038574
Batch 47/64 loss: -0.060688674449920654
Batch 48/64 loss: -0.04731029272079468
Batch 49/64 loss: -0.04529094696044922
Batch 50/64 loss: -0.05783200263977051
Batch 51/64 loss: -0.060974836349487305
Batch 52/64 loss: -0.07040512561798096
Batch 53/64 loss: -0.0598641037940979
Batch 54/64 loss: -0.06512558460235596
Batch 55/64 loss: -0.06392723321914673
Batch 56/64 loss: -0.06769508123397827
Batch 57/64 loss: -0.03503727912902832
Batch 58/64 loss: -0.053746700286865234
Batch 59/64 loss: -0.062076449394226074
Batch 60/64 loss: -0.056343138217926025
Batch 61/64 loss: -0.051162123680114746
Batch 62/64 loss: -0.05578547716140747
Batch 63/64 loss: -0.047028958797454834
Batch 64/64 loss: -0.06667089462280273
Epoch 154  Train loss: -0.057247065562827916  Val loss: -0.010151375200330597
Saving best model, epoch: 154
Epoch 155
-------------------------------
Batch 1/64 loss: -0.06654006242752075
Batch 2/64 loss: -0.058825910091400146
Batch 3/64 loss: -0.06193399429321289
Batch 4/64 loss: -0.04749101400375366
Batch 5/64 loss: -0.057575106620788574
Batch 6/64 loss: -0.07057994604110718
Batch 7/64 loss: -0.05541414022445679
Batch 8/64 loss: -0.07615852355957031
Batch 9/64 loss: -0.057643890380859375
Batch 10/64 loss: -0.0703076720237732
Batch 11/64 loss: -0.041468143463134766
Batch 12/64 loss: -0.07627344131469727
Batch 13/64 loss: -0.036884963512420654
Batch 14/64 loss: -0.028391003608703613
Batch 15/64 loss: -0.06127738952636719
Batch 16/64 loss: -0.06174898147583008
Batch 17/64 loss: -0.05965983867645264
Batch 18/64 loss: -0.05242282152175903
Batch 19/64 loss: -0.045087575912475586
Batch 20/64 loss: -0.06937563419342041
Batch 21/64 loss: -0.0695236325263977
Batch 22/64 loss: -0.07684975862503052
Batch 23/64 loss: -0.06480026245117188
Batch 24/64 loss: -0.05451244115829468
Batch 25/64 loss: -0.04937106370925903
Batch 26/64 loss: -0.04704958200454712
Batch 27/64 loss: -0.07478249073028564
Batch 28/64 loss: -0.058574140071868896
Batch 29/64 loss: -0.06575500965118408
Batch 30/64 loss: -0.056506454944610596
Batch 31/64 loss: -0.07807493209838867
Batch 32/64 loss: -0.06107962131500244
Batch 33/64 loss: -0.054836273193359375
Batch 34/64 loss: -0.054022789001464844
Batch 35/64 loss: -0.040529847145080566
Batch 36/64 loss: -0.05980557203292847
Batch 37/64 loss: -0.05100029706954956
Batch 38/64 loss: -0.0693584680557251
Batch 39/64 loss: -0.049630045890808105
Batch 40/64 loss: -0.04620373249053955
Batch 41/64 loss: -0.062541663646698
Batch 42/64 loss: -0.0530737042427063
Batch 43/64 loss: -0.06255638599395752
Batch 44/64 loss: -0.04532325267791748
Batch 45/64 loss: -0.06990057229995728
Batch 46/64 loss: -0.060555994510650635
Batch 47/64 loss: -0.055105388164520264
Batch 48/64 loss: -0.062077999114990234
Batch 49/64 loss: -0.08303588628768921
Batch 50/64 loss: -0.047213613986968994
Batch 51/64 loss: -0.04517930746078491
Batch 52/64 loss: -0.06766313314437866
Batch 53/64 loss: -0.04584145545959473
Batch 54/64 loss: -0.056519389152526855
Batch 55/64 loss: -0.05791115760803223
Batch 56/64 loss: -0.05247938632965088
Batch 57/64 loss: -0.05900752544403076
Batch 58/64 loss: -0.05324000120162964
Batch 59/64 loss: -0.06369125843048096
Batch 60/64 loss: -0.0703158974647522
Batch 61/64 loss: -0.06821537017822266
Batch 62/64 loss: -0.06176406145095825
Batch 63/64 loss: -0.06262451410293579
Batch 64/64 loss: -0.06038057804107666
Epoch 155  Train loss: -0.05879919715956146  Val loss: -0.015317595086966184
Saving best model, epoch: 155
Epoch 156
-------------------------------
Batch 1/64 loss: -0.05167436599731445
Batch 2/64 loss: -0.05466127395629883
Batch 3/64 loss: -0.07648175954818726
Batch 4/64 loss: -0.04830670356750488
Batch 5/64 loss: -0.03600478172302246
Batch 6/64 loss: -0.06151944398880005
Batch 7/64 loss: -0.04418385028839111
Batch 8/64 loss: -0.06528884172439575
Batch 9/64 loss: -0.053557753562927246
Batch 10/64 loss: -0.07380437850952148
Batch 11/64 loss: -0.058494746685028076
Batch 12/64 loss: -0.06541383266448975
Batch 13/64 loss: -0.07163608074188232
Batch 14/64 loss: -0.07285618782043457
Batch 15/64 loss: -0.054982781410217285
Batch 16/64 loss: -0.05618637800216675
Batch 17/64 loss: -0.06814122200012207
Batch 18/64 loss: -0.07524478435516357
Batch 19/64 loss: -0.07667851448059082
Batch 20/64 loss: -0.05427289009094238
Batch 21/64 loss: -0.07089519500732422
Batch 22/64 loss: -0.06964665651321411
Batch 23/64 loss: -0.03145909309387207
Batch 24/64 loss: -0.06660783290863037
Batch 25/64 loss: -0.06428831815719604
Batch 26/64 loss: -0.06596159934997559
Batch 27/64 loss: -0.06489241123199463
Batch 28/64 loss: -0.05114644765853882
Batch 29/64 loss: -0.04925769567489624
Batch 30/64 loss: -0.06733119487762451
Batch 31/64 loss: -0.05704694986343384
Batch 32/64 loss: -0.06067168712615967
Batch 33/64 loss: -0.055289626121520996
Batch 34/64 loss: -0.0535280704498291
Batch 35/64 loss: -0.04936552047729492
Batch 36/64 loss: -0.0638355016708374
Batch 37/64 loss: -0.04383265972137451
Batch 38/64 loss: -0.027248620986938477
Batch 39/64 loss: -0.03366434574127197
Batch 40/64 loss: -0.0627899169921875
Batch 41/64 loss: -0.04696190357208252
Batch 42/64 loss: -0.07022768259048462
Batch 43/64 loss: -0.0718156099319458
Batch 44/64 loss: -0.03113698959350586
Batch 45/64 loss: -0.061416566371917725
Batch 46/64 loss: -0.04998779296875
Batch 47/64 loss: -0.0658273696899414
Batch 48/64 loss: -0.05425375699996948
Batch 49/64 loss: -0.06499701738357544
Batch 50/64 loss: -0.05470395088195801
Batch 51/64 loss: -0.0655927062034607
Batch 52/64 loss: -0.06101793050765991
Batch 53/64 loss: -0.06574743986129761
Batch 54/64 loss: -0.045256972312927246
Batch 55/64 loss: -0.05315101146697998
Batch 56/64 loss: -0.06017798185348511
Batch 57/64 loss: -0.057547032833099365
Batch 58/64 loss: -0.05359750986099243
Batch 59/64 loss: -0.04799449443817139
Batch 60/64 loss: -0.05680876970291138
Batch 61/64 loss: -0.07344168424606323
Batch 62/64 loss: -0.04718315601348877
Batch 63/64 loss: -0.04590320587158203
Batch 64/64 loss: -0.0542144775390625
Epoch 156  Train loss: -0.05762445973415001  Val loss: -0.008377177813618453
Epoch 157
-------------------------------
Batch 1/64 loss: -0.05522572994232178
Batch 2/64 loss: -0.0648999810218811
Batch 3/64 loss: -0.03535008430480957
Batch 4/64 loss: -0.06010127067565918
Batch 5/64 loss: -0.039830565452575684
Batch 6/64 loss: -0.058536648750305176
Batch 7/64 loss: -0.04766273498535156
Batch 8/64 loss: -0.06037008762359619
Batch 9/64 loss: -0.0605049729347229
Batch 10/64 loss: -0.056835055351257324
Batch 11/64 loss: -0.06108200550079346
Batch 12/64 loss: -0.0662926435470581
Batch 13/64 loss: -0.038674116134643555
Batch 14/64 loss: -0.03933131694793701
Batch 15/64 loss: -0.05244988203048706
Batch 16/64 loss: -0.062242865562438965
Batch 17/64 loss: -0.05484253168106079
Batch 18/64 loss: -0.051691532135009766
Batch 19/64 loss: -0.052980661392211914
Batch 20/64 loss: -0.06513476371765137
Batch 21/64 loss: -0.0589144229888916
Batch 22/64 loss: -0.05835670232772827
Batch 23/64 loss: -0.043620169162750244
Batch 24/64 loss: -0.054482460021972656
Batch 25/64 loss: -0.033171236515045166
Batch 26/64 loss: -0.04422259330749512
Batch 27/64 loss: -0.05285513401031494
Batch 28/64 loss: -0.06675839424133301
Batch 29/64 loss: -0.05758625268936157
Batch 30/64 loss: -0.041895270347595215
Batch 31/64 loss: -0.04078662395477295
Batch 32/64 loss: -0.06713426113128662
Batch 33/64 loss: -0.07403367757797241
Batch 34/64 loss: -0.06570899486541748
Batch 35/64 loss: -0.0321391224861145
Batch 36/64 loss: -0.0617789626121521
Batch 37/64 loss: -0.05204349756240845
Batch 38/64 loss: -0.050369977951049805
Batch 39/64 loss: -0.053756117820739746
Batch 40/64 loss: -0.04593479633331299
Batch 41/64 loss: -0.041924476623535156
Batch 42/64 loss: -0.04985541105270386
Batch 43/64 loss: -0.020912587642669678
Batch 44/64 loss: -0.06273090839385986
Batch 45/64 loss: -0.05665391683578491
Batch 46/64 loss: -0.04954886436462402
Batch 47/64 loss: -0.062367260456085205
Batch 48/64 loss: -0.06097996234893799
Batch 49/64 loss: -0.0554046630859375
Batch 50/64 loss: -0.05999755859375
Batch 51/64 loss: -0.062208712100982666
Batch 52/64 loss: -0.06836497783660889
Batch 53/64 loss: -0.052880048751831055
Batch 54/64 loss: -0.06767332553863525
Batch 55/64 loss: -0.038669705390930176
Batch 56/64 loss: -0.055230796337127686
Batch 57/64 loss: -0.053473830223083496
Batch 58/64 loss: -0.039411067962646484
Batch 59/64 loss: -0.07051670551300049
Batch 60/64 loss: -0.0632619857788086
Batch 61/64 loss: -0.07665085792541504
Batch 62/64 loss: -0.06235098838806152
Batch 63/64 loss: -0.06595736742019653
Batch 64/64 loss: -0.0784493088722229
Epoch 157  Train loss: -0.054830605142256794  Val loss: -0.008547650989388273
Epoch 158
-------------------------------
Batch 1/64 loss: -0.05460476875305176
Batch 2/64 loss: -0.059432923793792725
Batch 3/64 loss: -0.0841977596282959
Batch 4/64 loss: -0.0343974232673645
Batch 5/64 loss: -0.03132063150405884
Batch 6/64 loss: -0.07093077898025513
Batch 7/64 loss: -0.0674663782119751
Batch 8/64 loss: -0.07797527313232422
Batch 9/64 loss: -0.051757097244262695
Batch 10/64 loss: -0.056944847106933594
Batch 11/64 loss: -0.04836142063140869
Batch 12/64 loss: -0.058424293994903564
Batch 13/64 loss: -0.04184442758560181
Batch 14/64 loss: -0.06778985261917114
Batch 15/64 loss: -0.05614501237869263
Batch 16/64 loss: -0.05814051628112793
Batch 17/64 loss: -0.05841761827468872
Batch 18/64 loss: -0.0699765682220459
Batch 19/64 loss: -0.03867429494857788
Batch 20/64 loss: -0.07310283184051514
Batch 21/64 loss: -0.06149083375930786
Batch 22/64 loss: -0.053331613540649414
Batch 23/64 loss: -0.06587767601013184
Batch 24/64 loss: -0.07310318946838379
Batch 25/64 loss: -0.054471373558044434
Batch 26/64 loss: -0.0625464916229248
Batch 27/64 loss: -0.07518088817596436
Batch 28/64 loss: -0.052438199520111084
Batch 29/64 loss: -0.04632776975631714
Batch 30/64 loss: -0.055507123470306396
Batch 31/64 loss: -0.047471702098846436
Batch 32/64 loss: -0.05148136615753174
Batch 33/64 loss: -0.059219419956207275
Batch 34/64 loss: -0.0572054386138916
Batch 35/64 loss: -0.04753923416137695
Batch 36/64 loss: -0.05099189281463623
Batch 37/64 loss: -0.045498013496398926
Batch 38/64 loss: -0.05068540573120117
Batch 39/64 loss: -0.061527907848358154
Batch 40/64 loss: -0.06639224290847778
Batch 41/64 loss: -0.05086100101470947
Batch 42/64 loss: -0.051027119159698486
Batch 43/64 loss: -0.04930603504180908
Batch 44/64 loss: -0.04486727714538574
Batch 45/64 loss: -0.054894983768463135
Batch 46/64 loss: -0.054842352867126465
Batch 47/64 loss: -0.05212235450744629
Batch 48/64 loss: -0.04303056001663208
Batch 49/64 loss: -0.06685405969619751
Batch 50/64 loss: -0.06309449672698975
Batch 51/64 loss: -0.06287306547164917
Batch 52/64 loss: -0.05651247501373291
Batch 53/64 loss: -0.06182461977005005
Batch 54/64 loss: -0.06596016883850098
Batch 55/64 loss: -0.05785268545150757
Batch 56/64 loss: -0.04846233129501343
Batch 57/64 loss: -0.06760883331298828
Batch 58/64 loss: -0.06421917676925659
Batch 59/64 loss: -0.044681668281555176
Batch 60/64 loss: -0.0669710636138916
Batch 61/64 loss: -0.05754965543746948
Batch 62/64 loss: -0.0419122576713562
Batch 63/64 loss: -0.053319334983825684
Batch 64/64 loss: -0.06601917743682861
Epoch 158  Train loss: -0.05691536407844693  Val loss: -0.004767448426931584
Epoch 159
-------------------------------
Batch 1/64 loss: -0.050428926944732666
Batch 2/64 loss: -0.05528932809829712
Batch 3/64 loss: -0.06337308883666992
Batch 4/64 loss: -0.06862592697143555
Batch 5/64 loss: -0.07154643535614014
Batch 6/64 loss: -0.06516391038894653
Batch 7/64 loss: -0.06185555458068848
Batch 8/64 loss: -0.05064034461975098
Batch 9/64 loss: -0.07106286287307739
Batch 10/64 loss: -0.037153422832489014
Batch 11/64 loss: -0.0723581314086914
Batch 12/64 loss: -0.06956994533538818
Batch 13/64 loss: -0.06272625923156738
Batch 14/64 loss: -0.046960651874542236
Batch 15/64 loss: -0.06290799379348755
Batch 16/64 loss: -0.06520688533782959
Batch 17/64 loss: -0.07985556125640869
Batch 18/64 loss: -0.0766679048538208
Batch 19/64 loss: -0.06428146362304688
Batch 20/64 loss: -0.06057846546173096
Batch 21/64 loss: -0.0777735710144043
Batch 22/64 loss: -0.03873562812805176
Batch 23/64 loss: -0.03652215003967285
Batch 24/64 loss: -0.05179709196090698
Batch 25/64 loss: -0.07199162244796753
Batch 26/64 loss: -0.06569850444793701
Batch 27/64 loss: -0.05751079320907593
Batch 28/64 loss: -0.04082155227661133
Batch 29/64 loss: -0.04731881618499756
Batch 30/64 loss: -0.054005563259124756
Batch 31/64 loss: -0.03164321184158325
Batch 32/64 loss: -0.06421875953674316
Batch 33/64 loss: -0.06257259845733643
Batch 34/64 loss: -0.043823301792144775
Batch 35/64 loss: -0.06465005874633789
Batch 36/64 loss: -0.04393637180328369
Batch 37/64 loss: -0.05210846662521362
Batch 38/64 loss: -0.041168808937072754
Batch 39/64 loss: -0.05619621276855469
Batch 40/64 loss: -0.057978153228759766
Batch 41/64 loss: -0.06637442111968994
Batch 42/64 loss: -0.05432868003845215
Batch 43/64 loss: -0.05900043249130249
Batch 44/64 loss: -0.054912447929382324
Batch 45/64 loss: -0.05525022745132446
Batch 46/64 loss: -0.06521826982498169
Batch 47/64 loss: -0.03977411985397339
Batch 48/64 loss: -0.06652259826660156
Batch 49/64 loss: -0.06193661689758301
Batch 50/64 loss: -0.06742274761199951
Batch 51/64 loss: -0.06017237901687622
Batch 52/64 loss: -0.06871527433395386
Batch 53/64 loss: -0.05190145969390869
Batch 54/64 loss: -0.05968356132507324
Batch 55/64 loss: -0.07191944122314453
Batch 56/64 loss: -0.05459606647491455
Batch 57/64 loss: -0.07839465141296387
Batch 58/64 loss: -0.05105948448181152
Batch 59/64 loss: -0.05464768409729004
Batch 60/64 loss: -0.057219505310058594
Batch 61/64 loss: -0.04520684480667114
Batch 62/64 loss: -0.02745342254638672
Batch 63/64 loss: -0.0645986795425415
Batch 64/64 loss: -0.0578615665435791
Epoch 159  Train loss: -0.05798320770263672  Val loss: -0.012673034291087147
Epoch 160
-------------------------------
Batch 1/64 loss: -0.06030166149139404
Batch 2/64 loss: -0.06833648681640625
Batch 3/64 loss: -0.06157875061035156
Batch 4/64 loss: -0.06550145149230957
Batch 5/64 loss: -0.06442391872406006
Batch 6/64 loss: -0.06883740425109863
Batch 7/64 loss: -0.05462127923965454
Batch 8/64 loss: -0.07217085361480713
Batch 9/64 loss: -0.06391549110412598
Batch 10/64 loss: -0.075686514377594
Batch 11/64 loss: -0.03519785404205322
Batch 12/64 loss: -0.06580591201782227
Batch 13/64 loss: -0.03583580255508423
Batch 14/64 loss: -0.05359065532684326
Batch 15/64 loss: -0.06379735469818115
Batch 16/64 loss: -0.07287156581878662
Batch 17/64 loss: -0.044198811054229736
Batch 18/64 loss: -0.06965351104736328
Batch 19/64 loss: -0.06137526035308838
Batch 20/64 loss: -0.05514228343963623
Batch 21/64 loss: -0.05496346950531006
Batch 22/64 loss: -0.05523359775543213
Batch 23/64 loss: -0.07876640558242798
Batch 24/64 loss: -0.05483180284500122
Batch 25/64 loss: -0.0728219747543335
Batch 26/64 loss: -0.0842432975769043
Batch 27/64 loss: -0.06304824352264404
Batch 28/64 loss: -0.07118290662765503
Batch 29/64 loss: -0.07601553201675415
Batch 30/64 loss: -0.04963481426239014
Batch 31/64 loss: -0.05637335777282715
Batch 32/64 loss: -0.06510114669799805
Batch 33/64 loss: -0.03380376100540161
Batch 34/64 loss: -0.06408858299255371
Batch 35/64 loss: -0.04447263479232788
Batch 36/64 loss: -0.061344146728515625
Batch 37/64 loss: -0.0589674711227417
Batch 38/64 loss: -0.057540297508239746
Batch 39/64 loss: -0.058891117572784424
Batch 40/64 loss: -0.05937957763671875
Batch 41/64 loss: -0.05314815044403076
Batch 42/64 loss: -0.04945874214172363
Batch 43/64 loss: -0.07020699977874756
Batch 44/64 loss: -0.0740918517112732
Batch 45/64 loss: -0.06471383571624756
Batch 46/64 loss: -0.03592735528945923
Batch 47/64 loss: -0.06211197376251221
Batch 48/64 loss: -0.06661480665206909
Batch 49/64 loss: -0.05006098747253418
Batch 50/64 loss: -0.06626760959625244
Batch 51/64 loss: -0.06758791208267212
Batch 52/64 loss: -0.066902756690979
Batch 53/64 loss: -0.045343875885009766
Batch 54/64 loss: -0.05485856533050537
Batch 55/64 loss: -0.06260818243026733
Batch 56/64 loss: -0.06781721115112305
Batch 57/64 loss: -0.05401873588562012
Batch 58/64 loss: -0.05409979820251465
Batch 59/64 loss: -0.04465550184249878
Batch 60/64 loss: -0.05690997838973999
Batch 61/64 loss: -0.06598496437072754
Batch 62/64 loss: -0.08001697063446045
Batch 63/64 loss: -0.0751228928565979
Batch 64/64 loss: -0.08071422576904297
Epoch 160  Train loss: -0.06090372216467764  Val loss: 0.006228670631487345
Epoch 161
-------------------------------
Batch 1/64 loss: -0.04916602373123169
Batch 2/64 loss: -0.02702409029006958
Batch 3/64 loss: -0.04654008150100708
Batch 4/64 loss: -0.062387168407440186
Batch 5/64 loss: -0.03779631853103638
Batch 6/64 loss: -0.06530946493148804
Batch 7/64 loss: -0.045799970626831055
Batch 8/64 loss: -0.05643045902252197
Batch 9/64 loss: -0.05078327655792236
Batch 10/64 loss: -0.05523526668548584
Batch 11/64 loss: -0.053180813789367676
Batch 12/64 loss: -0.05788993835449219
Batch 13/64 loss: -0.05870985984802246
Batch 14/64 loss: -0.03961944580078125
Batch 15/64 loss: -0.0737220048904419
Batch 16/64 loss: -0.0491144061088562
Batch 17/64 loss: -0.03959012031555176
Batch 18/64 loss: -0.0692242980003357
Batch 19/64 loss: -0.05502450466156006
Batch 20/64 loss: -0.06095510721206665
Batch 21/64 loss: -0.0337100625038147
Batch 22/64 loss: -0.06259441375732422
Batch 23/64 loss: -0.07422524690628052
Batch 24/64 loss: -0.0650794506072998
Batch 25/64 loss: -0.05950087308883667
Batch 26/64 loss: -0.0680999755859375
Batch 27/64 loss: -0.06545472145080566
Batch 28/64 loss: -0.05062997341156006
Batch 29/64 loss: -0.07068431377410889
Batch 30/64 loss: -0.046564698219299316
Batch 31/64 loss: -0.05427676439285278
Batch 32/64 loss: -0.048568546772003174
Batch 33/64 loss: -0.03478819131851196
Batch 34/64 loss: -0.07449787855148315
Batch 35/64 loss: -0.038752853870391846
Batch 36/64 loss: -0.038358211517333984
Batch 37/64 loss: -0.05434727668762207
Batch 38/64 loss: -0.05550539493560791
Batch 39/64 loss: -0.066764235496521
Batch 40/64 loss: -0.07990115880966187
Batch 41/64 loss: -0.05465829372406006
Batch 42/64 loss: -0.050400614738464355
Batch 43/64 loss: -0.05357539653778076
Batch 44/64 loss: -0.05297279357910156
Batch 45/64 loss: -0.07520973682403564
Batch 46/64 loss: -0.08215510845184326
Batch 47/64 loss: -0.05381089448928833
Batch 48/64 loss: -0.05543619394302368
Batch 49/64 loss: -0.05394184589385986
Batch 50/64 loss: -0.07354074716567993
Batch 51/64 loss: -0.06659471988677979
Batch 52/64 loss: -0.05531579256057739
Batch 53/64 loss: -0.04934275150299072
Batch 54/64 loss: -0.06629586219787598
Batch 55/64 loss: -0.06117069721221924
Batch 56/64 loss: -0.0712958574295044
Batch 57/64 loss: -0.043440937995910645
Batch 58/64 loss: -0.04825180768966675
Batch 59/64 loss: -0.05359429121017456
Batch 60/64 loss: -0.05739051103591919
Batch 61/64 loss: -0.05902540683746338
Batch 62/64 loss: -0.045037150382995605
Batch 63/64 loss: -0.049982428550720215
Batch 64/64 loss: -0.06179016828536987
Epoch 161  Train loss: -0.05607198944278792  Val loss: -0.009044281191022945
Epoch 162
-------------------------------
Batch 1/64 loss: -0.07297414541244507
Batch 2/64 loss: -0.06261837482452393
Batch 3/64 loss: -0.06895095109939575
Batch 4/64 loss: -0.0658789873123169
Batch 5/64 loss: -0.02541428804397583
Batch 6/64 loss: -0.06715470552444458
Batch 7/64 loss: -0.04535466432571411
Batch 8/64 loss: -0.06010407209396362
Batch 9/64 loss: -0.049422502517700195
Batch 10/64 loss: -0.06131774187088013
Batch 11/64 loss: -0.052209436893463135
Batch 12/64 loss: -0.060923099517822266
Batch 13/64 loss: -0.07020491361618042
Batch 14/64 loss: -0.05472135543823242
Batch 15/64 loss: -0.04158300161361694
Batch 16/64 loss: -0.07015228271484375
Batch 17/64 loss: -0.04761862754821777
Batch 18/64 loss: -0.058069467544555664
Batch 19/64 loss: -0.05820268392562866
Batch 20/64 loss: -0.07243931293487549
Batch 21/64 loss: -0.05377405881881714
Batch 22/64 loss: -0.055595993995666504
Batch 23/64 loss: -0.05464005470275879
Batch 24/64 loss: -0.0657920241355896
Batch 25/64 loss: -0.0677875280380249
Batch 26/64 loss: -0.05805313587188721
Batch 27/64 loss: -0.05937308073043823
Batch 28/64 loss: -0.0742042064666748
Batch 29/64 loss: -0.047551631927490234
Batch 30/64 loss: -0.058478474617004395
Batch 31/64 loss: -0.07242685556411743
Batch 32/64 loss: -0.059404850006103516
Batch 33/64 loss: -0.07578796148300171
Batch 34/64 loss: -0.05563509464263916
Batch 35/64 loss: -0.0794745683670044
Batch 36/64 loss: -0.06703555583953857
Batch 37/64 loss: -0.06643557548522949
Batch 38/64 loss: -0.0460127592086792
Batch 39/64 loss: -0.07196307182312012
Batch 40/64 loss: -0.07964581251144409
Batch 41/64 loss: -0.07246595621109009
Batch 42/64 loss: -0.06957614421844482
Batch 43/64 loss: -0.07129520177841187
Batch 44/64 loss: -0.06062054634094238
Batch 45/64 loss: -0.06104707717895508
Batch 46/64 loss: -0.031410038471221924
Batch 47/64 loss: -0.06241852045059204
Batch 48/64 loss: -0.048270583152770996
Batch 49/64 loss: -0.07114535570144653
Batch 50/64 loss: -0.06111907958984375
Batch 51/64 loss: -0.05077701807022095
Batch 52/64 loss: -0.0882067084312439
Batch 53/64 loss: -0.055181145668029785
Batch 54/64 loss: -0.04913532733917236
Batch 55/64 loss: -0.07528579235076904
Batch 56/64 loss: -0.053254127502441406
Batch 57/64 loss: -0.05306649208068848
Batch 58/64 loss: -0.048108458518981934
Batch 59/64 loss: -0.05276906490325928
Batch 60/64 loss: -0.05691128969192505
Batch 61/64 loss: -0.043725788593292236
Batch 62/64 loss: -0.052265286445617676
Batch 63/64 loss: -0.06788605451583862
Batch 64/64 loss: -0.06375068426132202
Epoch 162  Train loss: -0.060206133010340675  Val loss: -0.005604824864167937
Epoch 163
-------------------------------
Batch 1/64 loss: -0.05137228965759277
Batch 2/64 loss: -0.05478310585021973
Batch 3/64 loss: -0.05374765396118164
Batch 4/64 loss: -0.06279420852661133
Batch 5/64 loss: -0.05769449472427368
Batch 6/64 loss: -0.059150636196136475
Batch 7/64 loss: -0.05397695302963257
Batch 8/64 loss: -0.06208813190460205
Batch 9/64 loss: -0.07434487342834473
Batch 10/64 loss: -0.05765116214752197
Batch 11/64 loss: -0.07194137573242188
Batch 12/64 loss: -0.08095669746398926
Batch 13/64 loss: -0.08059829473495483
Batch 14/64 loss: -0.05839502811431885
Batch 15/64 loss: -0.07053983211517334
Batch 16/64 loss: -0.054162561893463135
Batch 17/64 loss: -0.06470131874084473
Batch 18/64 loss: -0.06823700666427612
Batch 19/64 loss: -0.05836474895477295
Batch 20/64 loss: -0.07346338033676147
Batch 21/64 loss: -0.07832252979278564
Batch 22/64 loss: -0.07502686977386475
Batch 23/64 loss: -0.05910366773605347
Batch 24/64 loss: -0.048684775829315186
Batch 25/64 loss: -0.056064724922180176
Batch 26/64 loss: -0.06051403284072876
Batch 27/64 loss: -0.06678462028503418
Batch 28/64 loss: -0.02927464246749878
Batch 29/64 loss: -0.06903398036956787
Batch 30/64 loss: -0.048156678676605225
Batch 31/64 loss: -0.0630960464477539
Batch 32/64 loss: -0.05496346950531006
Batch 33/64 loss: -0.071017324924469
Batch 34/64 loss: -0.0600016713142395
Batch 35/64 loss: -0.06159663200378418
Batch 36/64 loss: -0.045012831687927246
Batch 37/64 loss: -0.05467057228088379
Batch 38/64 loss: -0.07400739192962646
Batch 39/64 loss: -0.06368893384933472
Batch 40/64 loss: -0.05146670341491699
Batch 41/64 loss: -0.030218660831451416
Batch 42/64 loss: -0.05442994832992554
Batch 43/64 loss: -0.0612940788269043
Batch 44/64 loss: -0.057606637477874756
Batch 45/64 loss: -0.058942317962646484
Batch 46/64 loss: -0.062390923500061035
Batch 47/64 loss: -0.06298375129699707
Batch 48/64 loss: -0.06487059593200684
Batch 49/64 loss: -0.05904412269592285
Batch 50/64 loss: -0.07519352436065674
Batch 51/64 loss: -0.05449843406677246
Batch 52/64 loss: -0.050538480281829834
Batch 53/64 loss: -0.05583375692367554
Batch 54/64 loss: -0.02690058946609497
Batch 55/64 loss: -0.04524576663970947
Batch 56/64 loss: -0.05067181587219238
Batch 57/64 loss: -0.07411402463912964
Batch 58/64 loss: -0.05501878261566162
Batch 59/64 loss: -0.056934356689453125
Batch 60/64 loss: -0.06680536270141602
Batch 61/64 loss: -0.07146120071411133
Batch 62/64 loss: -0.034232497215270996
Batch 63/64 loss: -0.07372874021530151
Batch 64/64 loss: -0.08444619178771973
Epoch 163  Train loss: -0.06001168419333065  Val loss: -0.01574462304000592
Saving best model, epoch: 163
Epoch 164
-------------------------------
Batch 1/64 loss: -0.07446646690368652
Batch 2/64 loss: -0.0699647068977356
Batch 3/64 loss: -0.07824259996414185
Batch 4/64 loss: -0.05048257112503052
Batch 5/64 loss: -0.0657355785369873
Batch 6/64 loss: -0.07381033897399902
Batch 7/64 loss: -0.0752028226852417
Batch 8/64 loss: -0.06075632572174072
Batch 9/64 loss: -0.05958765745162964
Batch 10/64 loss: -0.07596969604492188
Batch 11/64 loss: -0.06664401292800903
Batch 12/64 loss: -0.04246187210083008
Batch 13/64 loss: -0.048724591732025146
Batch 14/64 loss: -0.043138980865478516
Batch 15/64 loss: -0.05104982852935791
Batch 16/64 loss: -0.05480325222015381
Batch 17/64 loss: -0.041844844818115234
Batch 18/64 loss: -0.07181727886199951
Batch 19/64 loss: -0.06825727224349976
Batch 20/64 loss: -0.06122159957885742
Batch 21/64 loss: -0.059314846992492676
Batch 22/64 loss: -0.05025762319564819
Batch 23/64 loss: -0.07424384355545044
Batch 24/64 loss: -0.0679357647895813
Batch 25/64 loss: -0.04626721143722534
Batch 26/64 loss: -0.06295925378799438
Batch 27/64 loss: -0.07748836278915405
Batch 28/64 loss: -0.0643378496170044
Batch 29/64 loss: -0.044888854026794434
Batch 30/64 loss: -0.07203686237335205
Batch 31/64 loss: -0.0701068639755249
Batch 32/64 loss: -0.04795718193054199
Batch 33/64 loss: -0.07250487804412842
Batch 34/64 loss: -0.07600986957550049
Batch 35/64 loss: -0.03726762533187866
Batch 36/64 loss: -0.06423938274383545
Batch 37/64 loss: -0.06941711902618408
Batch 38/64 loss: -0.056727588176727295
Batch 39/64 loss: -0.05939602851867676
Batch 40/64 loss: -0.07542538642883301
Batch 41/64 loss: -0.07813394069671631
Batch 42/64 loss: -0.061582088470458984
Batch 43/64 loss: -0.06895649433135986
Batch 44/64 loss: -0.04413086175918579
Batch 45/64 loss: -0.06645184755325317
Batch 46/64 loss: -0.059307098388671875
Batch 47/64 loss: -0.06077432632446289
Batch 48/64 loss: -0.06118428707122803
Batch 49/64 loss: -0.04892045259475708
Batch 50/64 loss: -0.04172259569168091
Batch 51/64 loss: -0.06390583515167236
Batch 52/64 loss: -0.04450643062591553
Batch 53/64 loss: -0.04985833168029785
Batch 54/64 loss: -0.049845993518829346
Batch 55/64 loss: -0.06665599346160889
Batch 56/64 loss: -0.05312007665634155
Batch 57/64 loss: -0.05532729625701904
Batch 58/64 loss: -0.052184343338012695
Batch 59/64 loss: -0.07348239421844482
Batch 60/64 loss: -0.05972635746002197
Batch 61/64 loss: -0.041079163551330566
Batch 62/64 loss: -0.07048934698104858
Batch 63/64 loss: -0.05982232093811035
Batch 64/64 loss: -0.061355531215667725
Epoch 164  Train loss: -0.06055136030795528  Val loss: -0.013959036659948604
Epoch 165
-------------------------------
Batch 1/64 loss: -0.06547963619232178
Batch 2/64 loss: -0.061135947704315186
Batch 3/64 loss: -0.05971837043762207
Batch 4/64 loss: -0.06193429231643677
Batch 5/64 loss: -0.06762754917144775
Batch 6/64 loss: -0.0541117787361145
Batch 7/64 loss: -0.05374401807785034
Batch 8/64 loss: -0.06738173961639404
Batch 9/64 loss: -0.03843545913696289
Batch 10/64 loss: -0.06493782997131348
Batch 11/64 loss: -0.058839619159698486
Batch 12/64 loss: -0.06829833984375
Batch 13/64 loss: -0.06965714693069458
Batch 14/64 loss: -0.05936849117279053
Batch 15/64 loss: -0.06023210287094116
Batch 16/64 loss: -0.0765802264213562
Batch 17/64 loss: -0.06846141815185547
Batch 18/64 loss: -0.0732085108757019
Batch 19/64 loss: -0.0735321044921875
Batch 20/64 loss: -0.07997345924377441
Batch 21/64 loss: -0.07598662376403809
Batch 22/64 loss: -0.05946922302246094
Batch 23/64 loss: -0.06408542394638062
Batch 24/64 loss: -0.08334875106811523
Batch 25/64 loss: -0.06726443767547607
Batch 26/64 loss: -0.06487017869949341
Batch 27/64 loss: -0.04395085573196411
Batch 28/64 loss: -0.07479798793792725
Batch 29/64 loss: -0.04101914167404175
Batch 30/64 loss: -0.06475305557250977
Batch 31/64 loss: -0.07319992780685425
Batch 32/64 loss: -0.08037835359573364
Batch 33/64 loss: -0.0605282187461853
Batch 34/64 loss: -0.044522106647491455
Batch 35/64 loss: -0.061155855655670166
Batch 36/64 loss: -0.05737483501434326
Batch 37/64 loss: -0.06132960319519043
Batch 38/64 loss: -0.06616604328155518
Batch 39/64 loss: -0.07448005676269531
Batch 40/64 loss: -0.06197917461395264
Batch 41/64 loss: -0.0641021728515625
Batch 42/64 loss: -0.05769824981689453
Batch 43/64 loss: -0.07372623682022095
Batch 44/64 loss: -0.08030515909194946
Batch 45/64 loss: -0.0705726146697998
Batch 46/64 loss: -0.05613827705383301
Batch 47/64 loss: -0.06392103433609009
Batch 48/64 loss: -0.06380754709243774
Batch 49/64 loss: -0.06802666187286377
Batch 50/64 loss: -0.06954234838485718
Batch 51/64 loss: -0.07153689861297607
Batch 52/64 loss: -0.06664794683456421
Batch 53/64 loss: -0.07276546955108643
Batch 54/64 loss: -0.06844961643218994
Batch 55/64 loss: -0.06586581468582153
Batch 56/64 loss: -0.06791895627975464
Batch 57/64 loss: -0.07369458675384521
Batch 58/64 loss: -0.05870240926742554
Batch 59/64 loss: -0.047478437423706055
Batch 60/64 loss: -0.06184738874435425
Batch 61/64 loss: -0.062450945377349854
Batch 62/64 loss: -0.04963421821594238
Batch 63/64 loss: -0.04742395877838135
Batch 64/64 loss: -0.05608087778091431
Epoch 165  Train loss: -0.06411977258383059  Val loss: -0.009664846654610126
Epoch 166
-------------------------------
Batch 1/64 loss: -0.07384687662124634
Batch 2/64 loss: -0.07057446241378784
Batch 3/64 loss: -0.06117016077041626
Batch 4/64 loss: -0.05119091272354126
Batch 5/64 loss: -0.0735623836517334
Batch 6/64 loss: -0.07117843627929688
Batch 7/64 loss: -0.045302510261535645
Batch 8/64 loss: -0.060860514640808105
Batch 9/64 loss: -0.06932246685028076
Batch 10/64 loss: -0.07674574851989746
Batch 11/64 loss: -0.04456198215484619
Batch 12/64 loss: -0.07524538040161133
Batch 13/64 loss: -0.06103712320327759
Batch 14/64 loss: -0.06515544652938843
Batch 15/64 loss: -0.05694621801376343
Batch 16/64 loss: -0.06440401077270508
Batch 17/64 loss: -0.0498618483543396
Batch 18/64 loss: -0.08386170864105225
Batch 19/64 loss: -0.059002578258514404
Batch 20/64 loss: -0.05110734701156616
Batch 21/64 loss: -0.07217276096343994
Batch 22/64 loss: -0.06888175010681152
Batch 23/64 loss: -0.055694580078125
Batch 24/64 loss: -0.04452621936798096
Batch 25/64 loss: -0.07634603977203369
Batch 26/64 loss: -0.04387706518173218
Batch 27/64 loss: -0.06695938110351562
Batch 28/64 loss: -0.0752331018447876
Batch 29/64 loss: -0.043538451194763184
Batch 30/64 loss: -0.04965686798095703
Batch 31/64 loss: -0.07266110181808472
Batch 32/64 loss: -0.07521474361419678
Batch 33/64 loss: -0.059713661670684814
Batch 34/64 loss: -0.07083070278167725
Batch 35/64 loss: -0.04203605651855469
Batch 36/64 loss: -0.06927132606506348
Batch 37/64 loss: -0.035012006759643555
Batch 38/64 loss: -0.037001729011535645
Batch 39/64 loss: -0.03612256050109863
Batch 40/64 loss: -0.06530600786209106
Batch 41/64 loss: -0.08024191856384277
Batch 42/64 loss: -0.0581168532371521
Batch 43/64 loss: -0.06856930255889893
Batch 44/64 loss: -0.0608670711517334
Batch 45/64 loss: -0.06303280591964722
Batch 46/64 loss: -0.03836619853973389
Batch 47/64 loss: -0.058186888694763184
Batch 48/64 loss: -0.043494343757629395
Batch 49/64 loss: -0.06110745668411255
Batch 50/64 loss: -0.05455124378204346
Batch 51/64 loss: -0.05229449272155762
Batch 52/64 loss: -0.03956323862075806
Batch 53/64 loss: -0.06582486629486084
Batch 54/64 loss: -0.06588119268417358
Batch 55/64 loss: -0.06330621242523193
Batch 56/64 loss: -0.050984859466552734
Batch 57/64 loss: -0.07203376293182373
Batch 58/64 loss: -0.08174073696136475
Batch 59/64 loss: -0.08022022247314453
Batch 60/64 loss: -0.05309027433395386
Batch 61/64 loss: -0.04590243101119995
Batch 62/64 loss: -0.06327992677688599
Batch 63/64 loss: -0.07825702428817749
Batch 64/64 loss: -0.0554194450378418
Epoch 166  Train loss: -0.060634857065537395  Val loss: -0.01320729960281005
Epoch 167
-------------------------------
Batch 1/64 loss: -0.05837905406951904
Batch 2/64 loss: -0.07368075847625732
Batch 3/64 loss: -0.07262259721755981
Batch 4/64 loss: -0.07013791799545288
Batch 5/64 loss: -0.05177712440490723
Batch 6/64 loss: -0.05453598499298096
Batch 7/64 loss: -0.06106644868850708
Batch 8/64 loss: -0.06965219974517822
Batch 9/64 loss: -0.07120776176452637
Batch 10/64 loss: -0.06166321039199829
Batch 11/64 loss: -0.06977224349975586
Batch 12/64 loss: -0.07862663269042969
Batch 13/64 loss: -0.06927657127380371
Batch 14/64 loss: -0.06803858280181885
Batch 15/64 loss: -0.057886600494384766
Batch 16/64 loss: -0.05472135543823242
Batch 17/64 loss: -0.06295591592788696
Batch 18/64 loss: -0.05047518014907837
Batch 19/64 loss: -0.07100319862365723
Batch 20/64 loss: -0.07439446449279785
Batch 21/64 loss: -0.05713224411010742
Batch 22/64 loss: -0.06681478023529053
Batch 23/64 loss: -0.07086968421936035
Batch 24/64 loss: -0.06355679035186768
Batch 25/64 loss: -0.030711591243743896
Batch 26/64 loss: -0.04813838005065918
Batch 27/64 loss: -0.06580370664596558
Batch 28/64 loss: -0.06642961502075195
Batch 29/64 loss: -0.06383949518203735
Batch 30/64 loss: -0.06911599636077881
Batch 31/64 loss: -0.06578582525253296
Batch 32/64 loss: -0.07753825187683105
Batch 33/64 loss: -0.06483632326126099
Batch 34/64 loss: -0.06879395246505737
Batch 35/64 loss: -0.059059977531433105
Batch 36/64 loss: -0.044699668884277344
Batch 37/64 loss: -0.0650097131729126
Batch 38/64 loss: -0.05830669403076172
Batch 39/64 loss: -0.09040588140487671
Batch 40/64 loss: -0.05675727128982544
Batch 41/64 loss: -0.06204324960708618
Batch 42/64 loss: -0.08968061208724976
Batch 43/64 loss: -0.07865232229232788
Batch 44/64 loss: -0.06010556221008301
Batch 45/64 loss: -0.06279653310775757
Batch 46/64 loss: -0.04956549406051636
Batch 47/64 loss: -0.08113443851470947
Batch 48/64 loss: -0.0833815336227417
Batch 49/64 loss: -0.06356251239776611
Batch 50/64 loss: -0.041003406047821045
Batch 51/64 loss: -0.06342679262161255
Batch 52/64 loss: -0.04007411003112793
Batch 53/64 loss: -0.06532847881317139
Batch 54/64 loss: -0.07422834634780884
Batch 55/64 loss: -0.06294441223144531
Batch 56/64 loss: -0.05942583084106445
Batch 57/64 loss: -0.07812827825546265
Batch 58/64 loss: -0.07031428813934326
Batch 59/64 loss: -0.05797290802001953
Batch 60/64 loss: -0.04907238483428955
Batch 61/64 loss: -0.06013894081115723
Batch 62/64 loss: -0.056695520877838135
Batch 63/64 loss: -0.06205099821090698
Batch 64/64 loss: -0.03424710035324097
Epoch 167  Train loss: -0.06357477529376161  Val loss: -0.01670768887726302
Saving best model, epoch: 167
Epoch 168
-------------------------------
Batch 1/64 loss: -0.03420138359069824
Batch 2/64 loss: -0.07259553670883179
Batch 3/64 loss: -0.059407174587249756
Batch 4/64 loss: -0.0694153904914856
Batch 5/64 loss: -0.052947998046875
Batch 6/64 loss: -0.060556769371032715
Batch 7/64 loss: -0.09011352062225342
Batch 8/64 loss: -0.06758219003677368
Batch 9/64 loss: -0.0649154782295227
Batch 10/64 loss: -0.07339799404144287
Batch 11/64 loss: -0.06835013628005981
Batch 12/64 loss: -0.06531548500061035
Batch 13/64 loss: -0.0933905839920044
Batch 14/64 loss: -0.06921112537384033
Batch 15/64 loss: -0.043403565883636475
Batch 16/64 loss: -0.07846444845199585
Batch 17/64 loss: -0.06625783443450928
Batch 18/64 loss: -0.0472865104675293
Batch 19/64 loss: -0.07957231998443604
Batch 20/64 loss: -0.038440585136413574
Batch 21/64 loss: -0.053255319595336914
Batch 22/64 loss: -0.07344275712966919
Batch 23/64 loss: -0.07378387451171875
Batch 24/64 loss: -0.06868433952331543
Batch 25/64 loss: -0.07786083221435547
Batch 26/64 loss: -0.06925570964813232
Batch 27/64 loss: -0.059700608253479004
Batch 28/64 loss: -0.07274496555328369
Batch 29/64 loss: -0.08417344093322754
Batch 30/64 loss: -0.023472070693969727
Batch 31/64 loss: -0.05965721607208252
Batch 32/64 loss: -0.04775047302246094
Batch 33/64 loss: -0.08045852184295654
Batch 34/64 loss: -0.059422433376312256
Batch 35/64 loss: -0.04591012001037598
Batch 36/64 loss: -0.06577754020690918
Batch 37/64 loss: -0.07474184036254883
Batch 38/64 loss: -0.07796013355255127
Batch 39/64 loss: -0.06644517183303833
Batch 40/64 loss: -0.0587119460105896
Batch 41/64 loss: -0.07101178169250488
Batch 42/64 loss: -0.07649791240692139
Batch 43/64 loss: -0.05269432067871094
Batch 44/64 loss: -0.06106746196746826
Batch 45/64 loss: -0.0780174732208252
Batch 46/64 loss: -0.0684044361114502
Batch 47/64 loss: -0.05458337068557739
Batch 48/64 loss: -0.06826013326644897
Batch 49/64 loss: -0.06385040283203125
Batch 50/64 loss: -0.0659906268119812
Batch 51/64 loss: -0.07753461599349976
Batch 52/64 loss: -0.05887383222579956
Batch 53/64 loss: -0.057758331298828125
Batch 54/64 loss: -0.06263244152069092
Batch 55/64 loss: -0.06214720010757446
Batch 56/64 loss: -0.05198794603347778
Batch 57/64 loss: -0.06370919942855835
Batch 58/64 loss: -0.06915152072906494
Batch 59/64 loss: -0.053173601627349854
Batch 60/64 loss: -0.05831408500671387
Batch 61/64 loss: -0.060835063457489014
Batch 62/64 loss: -0.0730823278427124
Batch 63/64 loss: -0.04005235433578491
Batch 64/64 loss: -0.06689822673797607
Epoch 168  Train loss: -0.06412297183392095  Val loss: -0.010358529197391365
Epoch 169
-------------------------------
Batch 1/64 loss: -0.06308257579803467
Batch 2/64 loss: -0.06234234571456909
Batch 3/64 loss: -0.06682264804840088
Batch 4/64 loss: -0.07062244415283203
Batch 5/64 loss: -0.06868797540664673
Batch 6/64 loss: -0.0582394003868103
Batch 7/64 loss: -0.07960671186447144
Batch 8/64 loss: -0.04679977893829346
Batch 9/64 loss: -0.08231806755065918
Batch 10/64 loss: -0.06599855422973633
Batch 11/64 loss: -0.0583917498588562
Batch 12/64 loss: -0.052045583724975586
Batch 13/64 loss: -0.0739697813987732
Batch 14/64 loss: -0.05688267946243286
Batch 15/64 loss: -0.06844627857208252
Batch 16/64 loss: -0.07405006885528564
Batch 17/64 loss: -0.07144510746002197
Batch 18/64 loss: -0.056649088859558105
Batch 19/64 loss: -0.06776678562164307
Batch 20/64 loss: -0.0607873797416687
Batch 21/64 loss: -0.06209760904312134
Batch 22/64 loss: -0.06316298246383667
Batch 23/64 loss: -0.0482792854309082
Batch 24/64 loss: -0.09223932027816772
Batch 25/64 loss: -0.06915086507797241
Batch 26/64 loss: -0.07620418071746826
Batch 27/64 loss: -0.06621319055557251
Batch 28/64 loss: -0.08980512619018555
Batch 29/64 loss: -0.0819399356842041
Batch 30/64 loss: -0.07622277736663818
Batch 31/64 loss: -0.057286977767944336
Batch 32/64 loss: -0.046399712562561035
Batch 33/64 loss: -0.06765878200531006
Batch 34/64 loss: -0.06461888551712036
Batch 35/64 loss: -0.06488382816314697
Batch 36/64 loss: -0.06226003170013428
Batch 37/64 loss: -0.07004529237747192
Batch 38/64 loss: -0.08519566059112549
Batch 39/64 loss: -0.05418890714645386
Batch 40/64 loss: -0.06630760431289673
Batch 41/64 loss: -0.0652356743812561
Batch 42/64 loss: -0.04823422431945801
Batch 43/64 loss: -0.0652347207069397
Batch 44/64 loss: -0.05183565616607666
Batch 45/64 loss: -0.06519025564193726
Batch 46/64 loss: -0.07401424646377563
Batch 47/64 loss: -0.05679202079772949
Batch 48/64 loss: -0.06646859645843506
Batch 49/64 loss: -0.06681036949157715
Batch 50/64 loss: -0.0684589147567749
Batch 51/64 loss: -0.058358967304229736
Batch 52/64 loss: -0.07702374458312988
Batch 53/64 loss: -0.07532680034637451
Batch 54/64 loss: -0.07319432497024536
Batch 55/64 loss: -0.0776526927947998
Batch 56/64 loss: -0.06048023700714111
Batch 57/64 loss: -0.051522016525268555
Batch 58/64 loss: -0.04795098304748535
Batch 59/64 loss: -0.0737345814704895
Batch 60/64 loss: -0.07460570335388184
Batch 61/64 loss: -0.07022285461425781
Batch 62/64 loss: -0.06875455379486084
Batch 63/64 loss: -0.08064264059066772
Batch 64/64 loss: -0.07344341278076172
Epoch 169  Train loss: -0.06654028705522126  Val loss: -0.013321118870961298
Epoch 170
-------------------------------
Batch 1/64 loss: -0.06364023685455322
Batch 2/64 loss: -0.06438958644866943
Batch 3/64 loss: -0.06510329246520996
Batch 4/64 loss: -0.0690506100654602
Batch 5/64 loss: -0.07314026355743408
Batch 6/64 loss: -0.06511026620864868
Batch 7/64 loss: -0.09182828664779663
Batch 8/64 loss: -0.05981755256652832
Batch 9/64 loss: -0.06866103410720825
Batch 10/64 loss: -0.07095897197723389
Batch 11/64 loss: -0.08056318759918213
Batch 12/64 loss: -0.0676729679107666
Batch 13/64 loss: -0.04723852872848511
Batch 14/64 loss: -0.06983393430709839
Batch 15/64 loss: -0.07232052087783813
Batch 16/64 loss: -0.06452560424804688
Batch 17/64 loss: -0.06710487604141235
Batch 18/64 loss: -0.07010728120803833
Batch 19/64 loss: -0.056288719177246094
Batch 20/64 loss: -0.0746769905090332
Batch 21/64 loss: -0.05306065082550049
Batch 22/64 loss: -0.0752444863319397
Batch 23/64 loss: -0.07184898853302002
Batch 24/64 loss: -0.07507467269897461
Batch 25/64 loss: -0.07832199335098267
Batch 26/64 loss: -0.08252370357513428
Batch 27/64 loss: -0.06573134660720825
Batch 28/64 loss: -0.0560726523399353
Batch 29/64 loss: -0.07844382524490356
Batch 30/64 loss: -0.04897028207778931
Batch 31/64 loss: -0.07731562852859497
Batch 32/64 loss: -0.07835215330123901
Batch 33/64 loss: -0.08113312721252441
Batch 34/64 loss: -0.07439184188842773
Batch 35/64 loss: -0.06886714696884155
Batch 36/64 loss: -0.07021158933639526
Batch 37/64 loss: -0.04162973165512085
Batch 38/64 loss: -0.07360219955444336
Batch 39/64 loss: -0.043528199195861816
Batch 40/64 loss: -0.07683783769607544
Batch 41/64 loss: -0.06147599220275879
Batch 42/64 loss: -0.0699964165687561
Batch 43/64 loss: -0.06903457641601562
Batch 44/64 loss: -0.08049595355987549
Batch 45/64 loss: -0.06900650262832642
Batch 46/64 loss: -0.07772797346115112
Batch 47/64 loss: -0.06882059574127197
Batch 48/64 loss: -0.0807034969329834
Batch 49/64 loss: -0.0478096604347229
Batch 50/64 loss: -0.05262809991836548
Batch 51/64 loss: -0.06009364128112793
Batch 52/64 loss: -0.06549990177154541
Batch 53/64 loss: -0.05342215299606323
Batch 54/64 loss: -0.05704599618911743
Batch 55/64 loss: -0.05548572540283203
Batch 56/64 loss: -0.035783588886260986
Batch 57/64 loss: -0.04987269639968872
Batch 58/64 loss: -0.07190090417861938
Batch 59/64 loss: -0.04915261268615723
Batch 60/64 loss: -0.06270134449005127
Batch 61/64 loss: -0.07406896352767944
Batch 62/64 loss: -0.08097386360168457
Batch 63/64 loss: -0.07108646631240845
Batch 64/64 loss: -0.06737446784973145
Epoch 170  Train loss: -0.06664333717495788  Val loss: -0.014798848899369388
Epoch 171
-------------------------------
Batch 1/64 loss: -0.06752985715866089
Batch 2/64 loss: -0.06335455179214478
Batch 3/64 loss: -0.0734480619430542
Batch 4/64 loss: -0.07156729698181152
Batch 5/64 loss: -0.06349217891693115
Batch 6/64 loss: -0.07903933525085449
Batch 7/64 loss: -0.07445412874221802
Batch 8/64 loss: -0.07262778282165527
Batch 9/64 loss: -0.05695682764053345
Batch 10/64 loss: -0.06518244743347168
Batch 11/64 loss: -0.05973410606384277
Batch 12/64 loss: -0.07054692506790161
Batch 13/64 loss: -0.06354665756225586
Batch 14/64 loss: -0.05783486366271973
Batch 15/64 loss: -0.07361388206481934
Batch 16/64 loss: -0.05946403741836548
Batch 17/64 loss: -0.058561503887176514
Batch 18/64 loss: -0.06929534673690796
Batch 19/64 loss: -0.07851481437683105
Batch 20/64 loss: -0.07615476846694946
Batch 21/64 loss: -0.053763747215270996
Batch 22/64 loss: -0.05228537321090698
Batch 23/64 loss: -0.057048141956329346
Batch 24/64 loss: -0.06630373001098633
Batch 25/64 loss: -0.06410396099090576
Batch 26/64 loss: -0.08351647853851318
Batch 27/64 loss: -0.06114637851715088
Batch 28/64 loss: -0.06525874137878418
Batch 29/64 loss: -0.059693098068237305
Batch 30/64 loss: -0.08052551746368408
Batch 31/64 loss: -0.06285691261291504
Batch 32/64 loss: -0.06501489877700806
Batch 33/64 loss: -0.05719238519668579
Batch 34/64 loss: -0.05155467987060547
Batch 35/64 loss: -0.07715517282485962
Batch 36/64 loss: -0.07546961307525635
Batch 37/64 loss: -0.06552213430404663
Batch 38/64 loss: -0.07832157611846924
Batch 39/64 loss: -0.057855427265167236
Batch 40/64 loss: -0.07153201103210449
Batch 41/64 loss: -0.07413822412490845
Batch 42/64 loss: -0.06965208053588867
Batch 43/64 loss: -0.08270877599716187
Batch 44/64 loss: -0.07235252857208252
Batch 45/64 loss: -0.06347149610519409
Batch 46/64 loss: -0.0851549506187439
Batch 47/64 loss: -0.06361883878707886
Batch 48/64 loss: -0.05682104825973511
Batch 49/64 loss: -0.07458031177520752
Batch 50/64 loss: -0.06103891134262085
Batch 51/64 loss: -0.08365851640701294
Batch 52/64 loss: -0.07597804069519043
Batch 53/64 loss: -0.06648761034011841
Batch 54/64 loss: -0.06606191396713257
Batch 55/64 loss: -0.06173515319824219
Batch 56/64 loss: -0.06272006034851074
Batch 57/64 loss: -0.053286075592041016
Batch 58/64 loss: -0.05525738000869751
Batch 59/64 loss: -0.05462777614593506
Batch 60/64 loss: -0.09000217914581299
Batch 61/64 loss: -0.0663841962814331
Batch 62/64 loss: -0.0793641209602356
Batch 63/64 loss: -0.06801003217697144
Batch 64/64 loss: -0.04841971397399902
Epoch 171  Train loss: -0.06720728406719133  Val loss: -0.012812838111956095
Epoch 172
-------------------------------
Batch 1/64 loss: -0.05616849660873413
Batch 2/64 loss: -0.04287844896316528
Batch 3/64 loss: -0.07312512397766113
Batch 4/64 loss: -0.07433527708053589
Batch 5/64 loss: -0.04527920484542847
Batch 6/64 loss: -0.054061949253082275
Batch 7/64 loss: -0.06749266386032104
Batch 8/64 loss: -0.08297741413116455
Batch 9/64 loss: -0.07629585266113281
Batch 10/64 loss: -0.06973999738693237
Batch 11/64 loss: -0.08358711004257202
Batch 12/64 loss: -0.06442916393280029
Batch 13/64 loss: -0.05906832218170166
Batch 14/64 loss: -0.07292413711547852
Batch 15/64 loss: -0.07271724939346313
Batch 16/64 loss: -0.05240130424499512
Batch 17/64 loss: -0.05144000053405762
Batch 18/64 loss: -0.050847649574279785
Batch 19/64 loss: -0.06254976987838745
Batch 20/64 loss: -0.08174902200698853
Batch 21/64 loss: -0.08094090223312378
Batch 22/64 loss: -0.05494964122772217
Batch 23/64 loss: -0.08182340860366821
Batch 24/64 loss: -0.06497824192047119
Batch 25/64 loss: -0.07355773448944092
Batch 26/64 loss: -0.07925808429718018
Batch 27/64 loss: -0.07207393646240234
Batch 28/64 loss: -0.08573389053344727
Batch 29/64 loss: -0.07283037900924683
Batch 30/64 loss: -0.07209837436676025
Batch 31/64 loss: -0.08390343189239502
Batch 32/64 loss: -0.057031989097595215
Batch 33/64 loss: -0.05846524238586426
Batch 34/64 loss: -0.048031747341156006
Batch 35/64 loss: -0.06970536708831787
Batch 36/64 loss: -0.07287091016769409
Batch 37/64 loss: -0.07495641708374023
Batch 38/64 loss: -0.06874817609786987
Batch 39/64 loss: -0.08976143598556519
Batch 40/64 loss: -0.08006983995437622
Batch 41/64 loss: -0.07961803674697876
Batch 42/64 loss: -0.06991606950759888
Batch 43/64 loss: -0.03835630416870117
Batch 44/64 loss: -0.0670965313911438
Batch 45/64 loss: -0.03873932361602783
Batch 46/64 loss: -0.051387906074523926
Batch 47/64 loss: -0.063404381275177
Batch 48/64 loss: -0.08197695016860962
Batch 49/64 loss: -0.08054137229919434
Batch 50/64 loss: -0.06544065475463867
Batch 51/64 loss: -0.07084763050079346
Batch 52/64 loss: -0.08536481857299805
Batch 53/64 loss: -0.06734699010848999
Batch 54/64 loss: -0.057103872299194336
Batch 55/64 loss: -0.0767936110496521
Batch 56/64 loss: -0.03118807077407837
Batch 57/64 loss: -0.07153993844985962
Batch 58/64 loss: -0.06838464736938477
Batch 59/64 loss: -0.08057159185409546
Batch 60/64 loss: -0.05834585428237915
Batch 61/64 loss: -0.06851989030838013
Batch 62/64 loss: -0.0728757381439209
Batch 63/64 loss: -0.0662006139755249
Batch 64/64 loss: -0.0565648078918457
Epoch 172  Train loss: -0.06729163469052782  Val loss: -0.012308071569069145
Epoch 173
-------------------------------
Batch 1/64 loss: -0.07293850183486938
Batch 2/64 loss: -0.05678260326385498
Batch 3/64 loss: -0.08683300018310547
Batch 4/64 loss: -0.07571011781692505
Batch 5/64 loss: -0.07145214080810547
Batch 6/64 loss: -0.0750841498374939
Batch 7/64 loss: -0.06979960203170776
Batch 8/64 loss: -0.07759582996368408
Batch 9/64 loss: -0.07913804054260254
Batch 10/64 loss: -0.09748655557632446
Batch 11/64 loss: -0.07355117797851562
Batch 12/64 loss: -0.06706947088241577
Batch 13/64 loss: -0.05022072792053223
Batch 14/64 loss: -0.0634915828704834
Batch 15/64 loss: -0.08607465028762817
Batch 16/64 loss: -0.07597345113754272
Batch 17/64 loss: -0.0712481141090393
Batch 18/64 loss: -0.07238572835922241
Batch 19/64 loss: -0.02690023183822632
Batch 20/64 loss: -0.08245998620986938
Batch 21/64 loss: -0.05575448274612427
Batch 22/64 loss: -0.0816502571105957
Batch 23/64 loss: -0.0719151496887207
Batch 24/64 loss: -0.0729406476020813
Batch 25/64 loss: -0.0756140947341919
Batch 26/64 loss: -0.09066975116729736
Batch 27/64 loss: -0.07987231016159058
Batch 28/64 loss: -0.0753445029258728
Batch 29/64 loss: -0.07118898630142212
Batch 30/64 loss: -0.06887352466583252
Batch 31/64 loss: -0.07290124893188477
Batch 32/64 loss: -0.07851791381835938
Batch 33/64 loss: -0.07747435569763184
Batch 34/64 loss: -0.041321516036987305
Batch 35/64 loss: -0.06655466556549072
Batch 36/64 loss: -0.0634688138961792
Batch 37/64 loss: -0.07329726219177246
Batch 38/64 loss: -0.06692975759506226
Batch 39/64 loss: -0.08388221263885498
Batch 40/64 loss: -0.07420283555984497
Batch 41/64 loss: -0.0871269702911377
Batch 42/64 loss: -0.06670409440994263
Batch 43/64 loss: -0.07651287317276001
Batch 44/64 loss: -0.06981039047241211
Batch 45/64 loss: -0.05653786659240723
Batch 46/64 loss: -0.0881386399269104
Batch 47/64 loss: -0.07008945941925049
Batch 48/64 loss: -0.054177284240722656
Batch 49/64 loss: -0.07686614990234375
Batch 50/64 loss: -0.07472312450408936
Batch 51/64 loss: -0.05689901113510132
Batch 52/64 loss: -0.06880313158035278
Batch 53/64 loss: -0.06786781549453735
Batch 54/64 loss: -0.05716836452484131
Batch 55/64 loss: -0.05957287549972534
Batch 56/64 loss: -0.036599576473236084
Batch 57/64 loss: -0.07308220863342285
Batch 58/64 loss: -0.061149001121520996
Batch 59/64 loss: -0.06745314598083496
Batch 60/64 loss: -0.07536417245864868
Batch 61/64 loss: -0.06642031669616699
Batch 62/64 loss: -0.07793951034545898
Batch 63/64 loss: -0.06324946880340576
Batch 64/64 loss: -0.07422322034835815
Epoch 173  Train loss: -0.07031361285377952  Val loss: -0.015785937866394463
Epoch 174
-------------------------------
Batch 1/64 loss: -0.06399679183959961
Batch 2/64 loss: -0.07886278629302979
Batch 3/64 loss: -0.07694429159164429
Batch 4/64 loss: -0.07613539695739746
Batch 5/64 loss: -0.05191397666931152
Batch 6/64 loss: -0.07899153232574463
Batch 7/64 loss: -0.07769215106964111
Batch 8/64 loss: -0.06066614389419556
Batch 9/64 loss: -0.06673580408096313
Batch 10/64 loss: -0.08691602945327759
Batch 11/64 loss: -0.07413262128829956
Batch 12/64 loss: -0.0756799578666687
Batch 13/64 loss: -0.08617758750915527
Batch 14/64 loss: -0.08895242214202881
Batch 15/64 loss: -0.06673115491867065
Batch 16/64 loss: -0.08440238237380981
Batch 17/64 loss: -0.07617735862731934
Batch 18/64 loss: -0.0593949556350708
Batch 19/64 loss: -0.0558621883392334
Batch 20/64 loss: -0.07990902662277222
Batch 21/64 loss: -0.07936030626296997
Batch 22/64 loss: -0.07012927532196045
Batch 23/64 loss: -0.0699230432510376
Batch 24/64 loss: -0.07282543182373047
Batch 25/64 loss: -0.05427265167236328
Batch 26/64 loss: -0.07738780975341797
Batch 27/64 loss: -0.06340253353118896
Batch 28/64 loss: -0.062004685401916504
Batch 29/64 loss: -0.07584929466247559
Batch 30/64 loss: -0.08785521984100342
Batch 31/64 loss: -0.05492997169494629
Batch 32/64 loss: -0.07094079256057739
Batch 33/64 loss: -0.06834399700164795
Batch 34/64 loss: -0.07966184616088867
Batch 35/64 loss: -0.0662374496459961
Batch 36/64 loss: -0.07031369209289551
Batch 37/64 loss: -0.06723666191101074
Batch 38/64 loss: -0.07371127605438232
Batch 39/64 loss: -0.06452327966690063
Batch 40/64 loss: -0.07733672857284546
Batch 41/64 loss: -0.07905411720275879
Batch 42/64 loss: -0.049887776374816895
Batch 43/64 loss: -0.028250157833099365
Batch 44/64 loss: -0.06367850303649902
Batch 45/64 loss: -0.06115269660949707
Batch 46/64 loss: -0.07292556762695312
Batch 47/64 loss: -0.07893967628479004
Batch 48/64 loss: -0.0728752613067627
Batch 49/64 loss: -0.078960120677948
Batch 50/64 loss: -0.0675542950630188
Batch 51/64 loss: -0.06482487916946411
Batch 52/64 loss: -0.07836699485778809
Batch 53/64 loss: -0.07356643676757812
Batch 54/64 loss: -0.07098740339279175
Batch 55/64 loss: -0.07107400894165039
Batch 56/64 loss: -0.07830566167831421
Batch 57/64 loss: -0.0762099027633667
Batch 58/64 loss: -0.05602550506591797
Batch 59/64 loss: -0.06716322898864746
Batch 60/64 loss: -0.07073640823364258
Batch 61/64 loss: -0.07425999641418457
Batch 62/64 loss: -0.07106488943099976
Batch 63/64 loss: -0.08040142059326172
Batch 64/64 loss: -0.06897473335266113
Epoch 174  Train loss: -0.07075319944643507  Val loss: -0.011939132131661746
Epoch 175
-------------------------------
Batch 1/64 loss: -0.09091103076934814
Batch 2/64 loss: -0.044912517070770264
Batch 3/64 loss: -0.0720059871673584
Batch 4/64 loss: -0.07465386390686035
Batch 5/64 loss: -0.07127708196640015
Batch 6/64 loss: -0.0762757658958435
Batch 7/64 loss: -0.06099140644073486
Batch 8/64 loss: -0.05965656042098999
Batch 9/64 loss: -0.062219858169555664
Batch 10/64 loss: -0.07121556997299194
Batch 11/64 loss: -0.06996715068817139
Batch 12/64 loss: -0.0745123028755188
Batch 13/64 loss: -0.06492793560028076
Batch 14/64 loss: -0.06270408630371094
Batch 15/64 loss: -0.07950752973556519
Batch 16/64 loss: -0.08704537153244019
Batch 17/64 loss: -0.0752863883972168
Batch 18/64 loss: -0.07286268472671509
Batch 19/64 loss: -0.086189866065979
Batch 20/64 loss: -0.09061837196350098
Batch 21/64 loss: -0.07089006900787354
Batch 22/64 loss: -0.06022298336029053
Batch 23/64 loss: -0.07006877660751343
Batch 24/64 loss: -0.07493007183074951
Batch 25/64 loss: -0.07832783460617065
Batch 26/64 loss: -0.05605471134185791
Batch 27/64 loss: -0.07918393611907959
Batch 28/64 loss: -0.048483192920684814
Batch 29/64 loss: -0.0516926646232605
Batch 30/64 loss: -0.06903433799743652
Batch 31/64 loss: -0.07893425226211548
Batch 32/64 loss: -0.07164102792739868
Batch 33/64 loss: -0.0513954758644104
Batch 34/64 loss: -0.07444518804550171
Batch 35/64 loss: -0.06727069616317749
Batch 36/64 loss: -0.05707353353500366
Batch 37/64 loss: -0.07743537425994873
Batch 38/64 loss: -0.06714040040969849
Batch 39/64 loss: -0.08395397663116455
Batch 40/64 loss: -0.0802607536315918
Batch 41/64 loss: -0.09462112188339233
Batch 42/64 loss: -0.08347678184509277
Batch 43/64 loss: -0.06998705863952637
Batch 44/64 loss: -0.06465363502502441
Batch 45/64 loss: -0.06684619188308716
Batch 46/64 loss: -0.03337252140045166
Batch 47/64 loss: -0.05564916133880615
Batch 48/64 loss: -0.0760154128074646
Batch 49/64 loss: -0.053920865058898926
Batch 50/64 loss: -0.057384610176086426
Batch 51/64 loss: -0.061791837215423584
Batch 52/64 loss: -0.08863919973373413
Batch 53/64 loss: -0.07801765203475952
Batch 54/64 loss: -0.07634115219116211
Batch 55/64 loss: -0.06439018249511719
Batch 56/64 loss: -0.07641422748565674
Batch 57/64 loss: -0.07163149118423462
Batch 58/64 loss: -0.054295241832733154
Batch 59/64 loss: -0.06380915641784668
Batch 60/64 loss: -0.05486249923706055
Batch 61/64 loss: -0.07250690460205078
Batch 62/64 loss: -0.05413484573364258
Batch 63/64 loss: -0.07806074619293213
Batch 64/64 loss: -0.07361769676208496
Epoch 175  Train loss: -0.0693680996988334  Val loss: -0.016120320742892234
Epoch 176
-------------------------------
Batch 1/64 loss: -0.06434345245361328
Batch 2/64 loss: -0.0573427677154541
Batch 3/64 loss: -0.06105804443359375
Batch 4/64 loss: -0.057602107524871826
Batch 5/64 loss: -0.08768194913864136
Batch 6/64 loss: -0.06362152099609375
Batch 7/64 loss: -0.07013547420501709
Batch 8/64 loss: -0.08120989799499512
Batch 9/64 loss: -0.07098352909088135
Batch 10/64 loss: -0.0752418041229248
Batch 11/64 loss: -0.08546239137649536
Batch 12/64 loss: -0.05511283874511719
Batch 13/64 loss: -0.07836043834686279
Batch 14/64 loss: -0.07857054471969604
Batch 15/64 loss: -0.06336158514022827
Batch 16/64 loss: -0.06914657354354858
Batch 17/64 loss: -0.06129777431488037
Batch 18/64 loss: -0.059018731117248535
Batch 19/64 loss: -0.061732590198516846
Batch 20/64 loss: -0.07975143194198608
Batch 21/64 loss: -0.0676005482673645
Batch 22/64 loss: -0.08573681116104126
Batch 23/64 loss: -0.06968528032302856
Batch 24/64 loss: -0.0748181939125061
Batch 25/64 loss: -0.056466519832611084
Batch 26/64 loss: -0.06114417314529419
Batch 27/64 loss: -0.05072706937789917
Batch 28/64 loss: -0.05469965934753418
Batch 29/64 loss: -0.0784308910369873
Batch 30/64 loss: -0.06638872623443604
Batch 31/64 loss: -0.06838905811309814
Batch 32/64 loss: -0.07095134258270264
Batch 33/64 loss: -0.07136470079421997
Batch 34/64 loss: -0.06448721885681152
Batch 35/64 loss: -0.054003775119781494
Batch 36/64 loss: -0.05848217010498047
Batch 37/64 loss: -0.0795668363571167
Batch 38/64 loss: -0.07064521312713623
Batch 39/64 loss: -0.05411803722381592
Batch 40/64 loss: -0.06613534688949585
Batch 41/64 loss: -0.0716092586517334
Batch 42/64 loss: -0.05822110176086426
Batch 43/64 loss: -0.06025427579879761
Batch 44/64 loss: -0.06677085161209106
Batch 45/64 loss: -0.06849044561386108
Batch 46/64 loss: -0.060966670513153076
Batch 47/64 loss: -0.08275163173675537
Batch 48/64 loss: -0.09383845329284668
Batch 49/64 loss: -0.07499760389328003
Batch 50/64 loss: -0.07102525234222412
Batch 51/64 loss: -0.09535318613052368
Batch 52/64 loss: -0.07642340660095215
Batch 53/64 loss: -0.06117194890975952
Batch 54/64 loss: -0.06098604202270508
Batch 55/64 loss: -0.05166888236999512
Batch 56/64 loss: -0.06551551818847656
Batch 57/64 loss: -0.09348201751708984
Batch 58/64 loss: -0.0831347107887268
Batch 59/64 loss: -0.049367308616638184
Batch 60/64 loss: -0.061025023460388184
Batch 61/64 loss: -0.06291037797927856
Batch 62/64 loss: -0.07423341274261475
Batch 63/64 loss: -0.08364760875701904
Batch 64/64 loss: -0.07076787948608398
Epoch 176  Train loss: -0.06879683008380964  Val loss: -0.009361044033286497
Epoch 177
-------------------------------
Batch 1/64 loss: -0.06255251169204712
Batch 2/64 loss: -0.07234114408493042
Batch 3/64 loss: -0.06699889898300171
Batch 4/64 loss: -0.06288444995880127
Batch 5/64 loss: -0.06831282377243042
Batch 6/64 loss: -0.05598098039627075
Batch 7/64 loss: -0.07203799486160278
Batch 8/64 loss: -0.05909848213195801
Batch 9/64 loss: -0.09097802639007568
Batch 10/64 loss: -0.057643890380859375
Batch 11/64 loss: -0.060812950134277344
Batch 12/64 loss: -0.07154154777526855
Batch 13/64 loss: -0.07133704423904419
Batch 14/64 loss: -0.07229357957839966
Batch 15/64 loss: -0.06490379571914673
Batch 16/64 loss: -0.07322067022323608
Batch 17/64 loss: -0.05541783571243286
Batch 18/64 loss: -0.06865227222442627
Batch 19/64 loss: -0.07062304019927979
Batch 20/64 loss: -0.07004815340042114
Batch 21/64 loss: -0.07539433240890503
Batch 22/64 loss: -0.058892667293548584
Batch 23/64 loss: -0.08249771595001221
Batch 24/64 loss: -0.07138431072235107
Batch 25/64 loss: -0.06439495086669922
Batch 26/64 loss: -0.06848782300949097
Batch 27/64 loss: -0.07237493991851807
Batch 28/64 loss: -0.06833523511886597
Batch 29/64 loss: -0.057758986949920654
Batch 30/64 loss: -0.056593239307403564
Batch 31/64 loss: -0.06783241033554077
Batch 32/64 loss: -0.04059410095214844
Batch 33/64 loss: -0.0670086145401001
Batch 34/64 loss: -0.07413333654403687
Batch 35/64 loss: -0.07920104265213013
Batch 36/64 loss: -0.05927634239196777
Batch 37/64 loss: -0.06530207395553589
Batch 38/64 loss: -0.06124472618103027
Batch 39/64 loss: -0.06482994556427002
Batch 40/64 loss: -0.08606952428817749
Batch 41/64 loss: -0.07041293382644653
Batch 42/64 loss: -0.06448280811309814
Batch 43/64 loss: -0.08516502380371094
Batch 44/64 loss: -0.08199828863143921
Batch 45/64 loss: -0.06426215171813965
Batch 46/64 loss: -0.0605626106262207
Batch 47/64 loss: -0.07613277435302734
Batch 48/64 loss: -0.0687781572341919
Batch 49/64 loss: -0.07372182607650757
Batch 50/64 loss: -0.08466583490371704
Batch 51/64 loss: -0.06872290372848511
Batch 52/64 loss: -0.08532977104187012
Batch 53/64 loss: -0.06213641166687012
Batch 54/64 loss: -0.0199429988861084
Batch 55/64 loss: -0.08774203062057495
Batch 56/64 loss: -0.062121570110321045
Batch 57/64 loss: -0.04208248853683472
Batch 58/64 loss: -0.08890283107757568
Batch 59/64 loss: -0.05917096138000488
Batch 60/64 loss: -0.09421014785766602
Batch 61/64 loss: -0.07114720344543457
Batch 62/64 loss: -0.06837064027786255
Batch 63/64 loss: -0.07675784826278687
Batch 64/64 loss: -0.07607299089431763
Epoch 177  Train loss: -0.06844174698287366  Val loss: -0.0180679070171212
Saving best model, epoch: 177
Epoch 178
-------------------------------
Batch 1/64 loss: -0.06904137134552002
Batch 2/64 loss: -0.07762837409973145
Batch 3/64 loss: -0.06415045261383057
Batch 4/64 loss: -0.08246326446533203
Batch 5/64 loss: -0.06069004535675049
Batch 6/64 loss: -0.0878903865814209
Batch 7/64 loss: -0.07087218761444092
Batch 8/64 loss: -0.06423550844192505
Batch 9/64 loss: -0.06399655342102051
Batch 10/64 loss: -0.08164018392562866
Batch 11/64 loss: -0.08777707815170288
Batch 12/64 loss: -0.04937112331390381
Batch 13/64 loss: -0.04924142360687256
Batch 14/64 loss: -0.0685039758682251
Batch 15/64 loss: -0.08948326110839844
Batch 16/64 loss: -0.07410848140716553
Batch 17/64 loss: -0.0766565203666687
Batch 18/64 loss: -0.06546133756637573
Batch 19/64 loss: -0.0901637077331543
Batch 20/64 loss: -0.06187307834625244
Batch 21/64 loss: -0.0846184492111206
Batch 22/64 loss: -0.055078983306884766
Batch 23/64 loss: -0.07394468784332275
Batch 24/64 loss: -0.06804227828979492
Batch 25/64 loss: -0.07568061351776123
Batch 26/64 loss: -0.07935857772827148
Batch 27/64 loss: -0.07861751317977905
Batch 28/64 loss: -0.08151960372924805
Batch 29/64 loss: -0.061080098152160645
Batch 30/64 loss: -0.07529205083847046
Batch 31/64 loss: -0.07266980409622192
Batch 32/64 loss: -0.05246436595916748
Batch 33/64 loss: -0.07815051078796387
Batch 34/64 loss: -0.0907888412475586
Batch 35/64 loss: -0.06560724973678589
Batch 36/64 loss: -0.08132654428482056
Batch 37/64 loss: -0.0830392837524414
Batch 38/64 loss: -0.07469344139099121
Batch 39/64 loss: -0.06432831287384033
Batch 40/64 loss: -0.06425851583480835
Batch 41/64 loss: -0.061102986335754395
Batch 42/64 loss: -0.07241010665893555
Batch 43/64 loss: -0.07365870475769043
Batch 44/64 loss: -0.06864023208618164
Batch 45/64 loss: -0.05401831865310669
Batch 46/64 loss: -0.0391002893447876
Batch 47/64 loss: -0.04613524675369263
Batch 48/64 loss: -0.04494333267211914
Batch 49/64 loss: -0.03695636987686157
Batch 50/64 loss: -0.06931948661804199
Batch 51/64 loss: -0.0736733078956604
Batch 52/64 loss: -0.061383605003356934
Batch 53/64 loss: -0.06914359331130981
Batch 54/64 loss: -0.06653201580047607
Batch 55/64 loss: -0.06756854057312012
Batch 56/64 loss: -0.07590621709823608
Batch 57/64 loss: -0.07118397951126099
Batch 58/64 loss: -0.07546007633209229
Batch 59/64 loss: -0.06980925798416138
Batch 60/64 loss: -0.07947880029678345
Batch 61/64 loss: -0.05890864133834839
Batch 62/64 loss: -0.07953298091888428
Batch 63/64 loss: -0.06170177459716797
Batch 64/64 loss: -0.0638265609741211
Epoch 178  Train loss: -0.06933718943128399  Val loss: -0.013209432056269696
Epoch 179
-------------------------------
Batch 1/64 loss: -0.07418835163116455
Batch 2/64 loss: -0.07904332876205444
Batch 3/64 loss: -0.07256388664245605
Batch 4/64 loss: -0.08571988344192505
Batch 5/64 loss: -0.07343381643295288
Batch 6/64 loss: -0.07846534252166748
Batch 7/64 loss: -0.09161984920501709
Batch 8/64 loss: -0.05066102743148804
Batch 9/64 loss: -0.08520406484603882
Batch 10/64 loss: -0.07006615400314331
Batch 11/64 loss: -0.06546872854232788
Batch 12/64 loss: -0.07959955930709839
Batch 13/64 loss: -0.08907967805862427
Batch 14/64 loss: -0.06788235902786255
Batch 15/64 loss: -0.07845801115036011
Batch 16/64 loss: -0.0568203330039978
Batch 17/64 loss: -0.09346091747283936
Batch 18/64 loss: -0.06302547454833984
Batch 19/64 loss: -0.08709490299224854
Batch 20/64 loss: -0.07944875955581665
Batch 21/64 loss: -0.07257002592086792
Batch 22/64 loss: -0.09116584062576294
Batch 23/64 loss: -0.0775521993637085
Batch 24/64 loss: -0.06621623039245605
Batch 25/64 loss: -0.09376257658004761
Batch 26/64 loss: -0.0765756368637085
Batch 27/64 loss: -0.060832679271698
Batch 28/64 loss: -0.07454806566238403
Batch 29/64 loss: -0.05731004476547241
Batch 30/64 loss: -0.07347804307937622
Batch 31/64 loss: -0.07760024070739746
Batch 32/64 loss: -0.06396317481994629
Batch 33/64 loss: -0.0802033543586731
Batch 34/64 loss: -0.06512051820755005
Batch 35/64 loss: -0.0775841474533081
Batch 36/64 loss: -0.07866472005844116
Batch 37/64 loss: -0.07557415962219238
Batch 38/64 loss: -0.0870177149772644
Batch 39/64 loss: -0.09018814563751221
Batch 40/64 loss: -0.0645250678062439
Batch 41/64 loss: -0.08013933897018433
Batch 42/64 loss: -0.07338964939117432
Batch 43/64 loss: -0.05902814865112305
Batch 44/64 loss: -0.08034253120422363
Batch 45/64 loss: -0.06749236583709717
Batch 46/64 loss: -0.09594476222991943
Batch 47/64 loss: -0.07881581783294678
Batch 48/64 loss: -0.08363324403762817
Batch 49/64 loss: -0.08514440059661865
Batch 50/64 loss: -0.06935977935791016
Batch 51/64 loss: -0.051704227924346924
Batch 52/64 loss: -0.06742960214614868
Batch 53/64 loss: -0.05743688344955444
Batch 54/64 loss: -0.07477843761444092
Batch 55/64 loss: -0.06663769483566284
Batch 56/64 loss: -0.07031232118606567
Batch 57/64 loss: -0.06633955240249634
Batch 58/64 loss: -0.08084845542907715
Batch 59/64 loss: -0.06301552057266235
Batch 60/64 loss: -0.059887826442718506
Batch 61/64 loss: -0.04173421859741211
Batch 62/64 loss: -0.06138956546783447
Batch 63/64 loss: -0.06460976600646973
Batch 64/64 loss: -0.0807269811630249
Epoch 179  Train loss: -0.0735014331107046  Val loss: -0.016291581068661614
Epoch 180
-------------------------------
Batch 1/64 loss: -0.06877374649047852
Batch 2/64 loss: -0.07833164930343628
Batch 3/64 loss: -0.07876473665237427
Batch 4/64 loss: -0.0706026554107666
Batch 5/64 loss: -0.07160425186157227
Batch 6/64 loss: -0.07669031620025635
Batch 7/64 loss: -0.07017898559570312
Batch 8/64 loss: -0.05744260549545288
Batch 9/64 loss: -0.09025716781616211
Batch 10/64 loss: -0.08089280128479004
Batch 11/64 loss: -0.0809507966041565
Batch 12/64 loss: -0.08296811580657959
Batch 13/64 loss: -0.0948365330696106
Batch 14/64 loss: -0.06459563970565796
Batch 15/64 loss: -0.06378912925720215
Batch 16/64 loss: -0.09576702117919922
Batch 17/64 loss: -0.06997191905975342
Batch 18/64 loss: -0.05956524610519409
Batch 19/64 loss: -0.09082740545272827
Batch 20/64 loss: -0.07818615436553955
Batch 21/64 loss: -0.05858433246612549
Batch 22/64 loss: -0.07815241813659668
Batch 23/64 loss: -0.081642746925354
Batch 24/64 loss: -0.08290231227874756
Batch 25/64 loss: -0.07233840227127075
Batch 26/64 loss: -0.08378809690475464
Batch 27/64 loss: -0.060600996017456055
Batch 28/64 loss: -0.06418150663375854
Batch 29/64 loss: -0.07821035385131836
Batch 30/64 loss: -0.08920705318450928
Batch 31/64 loss: -0.08435642719268799
Batch 32/64 loss: -0.04434788227081299
Batch 33/64 loss: -0.05111229419708252
Batch 34/64 loss: -0.07891309261322021
Batch 35/64 loss: -0.07476019859313965
Batch 36/64 loss: -0.07963192462921143
Batch 37/64 loss: -0.08186554908752441
Batch 38/64 loss: -0.07037323713302612
Batch 39/64 loss: -0.0788082480430603
Batch 40/64 loss: -0.07791966199874878
Batch 41/64 loss: -0.08547508716583252
Batch 42/64 loss: -0.06656980514526367
Batch 43/64 loss: -0.08336454629898071
Batch 44/64 loss: -0.05592203140258789
Batch 45/64 loss: -0.06686842441558838
Batch 46/64 loss: -0.07192003726959229
Batch 47/64 loss: -0.07613778114318848
Batch 48/64 loss: -0.07500088214874268
Batch 49/64 loss: -0.05219101905822754
Batch 50/64 loss: -0.06066662073135376
Batch 51/64 loss: -0.06781512498855591
Batch 52/64 loss: -0.0487445592880249
Batch 53/64 loss: -0.08198493719100952
Batch 54/64 loss: -0.08996719121932983
Batch 55/64 loss: -0.07652807235717773
Batch 56/64 loss: -0.07850468158721924
Batch 57/64 loss: -0.08109098672866821
Batch 58/64 loss: -0.07390958070755005
Batch 59/64 loss: -0.08667141199111938
Batch 60/64 loss: -0.07019931077957153
Batch 61/64 loss: -0.07455962896347046
Batch 62/64 loss: -0.07745790481567383
Batch 63/64 loss: -0.06485307216644287
Batch 64/64 loss: -0.08554583787918091
Epoch 180  Train loss: -0.07415303038615806  Val loss: -0.010006662906240351
Epoch 181
-------------------------------
Batch 1/64 loss: -0.09726178646087646
Batch 2/64 loss: -0.07123422622680664
Batch 3/64 loss: -0.05863410234451294
Batch 4/64 loss: -0.08392077684402466
Batch 5/64 loss: -0.06995904445648193
Batch 6/64 loss: -0.07864099740982056
Batch 7/64 loss: -0.07088100910186768
Batch 8/64 loss: -0.06953954696655273
Batch 9/64 loss: -0.07142484188079834
Batch 10/64 loss: -0.060540854930877686
Batch 11/64 loss: -0.06981223821640015
Batch 12/64 loss: -0.08035928010940552
Batch 13/64 loss: -0.08397775888442993
Batch 14/64 loss: -0.08883392810821533
Batch 15/64 loss: -0.07798045873641968
Batch 16/64 loss: -0.053179919719696045
Batch 17/64 loss: -0.07725942134857178
Batch 18/64 loss: -0.06769853830337524
Batch 19/64 loss: -0.08139902353286743
Batch 20/64 loss: -0.06820088624954224
Batch 21/64 loss: -0.0863196849822998
Batch 22/64 loss: -0.06516993045806885
Batch 23/64 loss: -0.0826883316040039
Batch 24/64 loss: -0.06726217269897461
Batch 25/64 loss: -0.06410181522369385
Batch 26/64 loss: -0.0870363712310791
Batch 27/64 loss: -0.07450896501541138
Batch 28/64 loss: -0.05882197618484497
Batch 29/64 loss: -0.06234431266784668
Batch 30/64 loss: -0.0828586220741272
Batch 31/64 loss: -0.0619884729385376
Batch 32/64 loss: -0.05660200119018555
Batch 33/64 loss: -0.06771612167358398
Batch 34/64 loss: -0.06715917587280273
Batch 35/64 loss: -0.07679295539855957
Batch 36/64 loss: -0.08841145038604736
Batch 37/64 loss: -0.07349079847335815
Batch 38/64 loss: -0.10887181758880615
Batch 39/64 loss: -0.07726997137069702
Batch 40/64 loss: -0.06952673196792603
Batch 41/64 loss: -0.07914316654205322
Batch 42/64 loss: -0.07795321941375732
Batch 43/64 loss: -0.07820749282836914
Batch 44/64 loss: -0.0858159065246582
Batch 45/64 loss: -0.05121564865112305
Batch 46/64 loss: -0.04042160511016846
Batch 47/64 loss: -0.07665258646011353
Batch 48/64 loss: -0.08299583196640015
Batch 49/64 loss: -0.08261775970458984
Batch 50/64 loss: -0.07590234279632568
Batch 51/64 loss: -0.07280480861663818
Batch 52/64 loss: -0.07479381561279297
Batch 53/64 loss: -0.0733291506767273
Batch 54/64 loss: -0.054599106311798096
Batch 55/64 loss: -0.07627755403518677
Batch 56/64 loss: -0.0885457992553711
Batch 57/64 loss: -0.07691240310668945
Batch 58/64 loss: -0.07499831914901733
Batch 59/64 loss: -0.0653458833694458
Batch 60/64 loss: -0.0766761302947998
Batch 61/64 loss: -0.07920175790786743
Batch 62/64 loss: -0.07136917114257812
Batch 63/64 loss: -0.07137715816497803
Batch 64/64 loss: -0.0570870041847229
Epoch 181  Train loss: -0.07356317160176296  Val loss: -0.010657427646859815
Epoch 182
-------------------------------
Batch 1/64 loss: -0.06257963180541992
Batch 2/64 loss: -0.0689663290977478
Batch 3/64 loss: -0.08072161674499512
Batch 4/64 loss: -0.07852107286453247
Batch 5/64 loss: -0.08982479572296143
Batch 6/64 loss: -0.0777478814125061
Batch 7/64 loss: -0.07508057355880737
Batch 8/64 loss: -0.08189517259597778
Batch 9/64 loss: -0.08097189664840698
Batch 10/64 loss: -0.07911360263824463
Batch 11/64 loss: -0.06002277135848999
Batch 12/64 loss: -0.08544909954071045
Batch 13/64 loss: -0.07325375080108643
Batch 14/64 loss: -0.07710099220275879
Batch 15/64 loss: -0.0666189193725586
Batch 16/64 loss: -0.09100019931793213
Batch 17/64 loss: -0.07376039028167725
Batch 18/64 loss: -0.06633752584457397
Batch 19/64 loss: -0.0664529800415039
Batch 20/64 loss: -0.05555671453475952
Batch 21/64 loss: -0.07742047309875488
Batch 22/64 loss: -0.09399449825286865
Batch 23/64 loss: -0.06614291667938232
Batch 24/64 loss: -0.05115509033203125
Batch 25/64 loss: -0.06318330764770508
Batch 26/64 loss: -0.07846581935882568
Batch 27/64 loss: -0.0792914628982544
Batch 28/64 loss: -0.07065081596374512
Batch 29/64 loss: -0.08311885595321655
Batch 30/64 loss: -0.07702964544296265
Batch 31/64 loss: -0.08108526468276978
Batch 32/64 loss: -0.07987338304519653
Batch 33/64 loss: -0.06877636909484863
Batch 34/64 loss: -0.08262020349502563
Batch 35/64 loss: -0.08378881216049194
Batch 36/64 loss: -0.0767943263053894
Batch 37/64 loss: -0.0882493257522583
Batch 38/64 loss: -0.06375765800476074
Batch 39/64 loss: -0.08664166927337646
Batch 40/64 loss: -0.07652997970581055
Batch 41/64 loss: -0.04118216037750244
Batch 42/64 loss: -0.06901532411575317
Batch 43/64 loss: -0.08021032810211182
Batch 44/64 loss: -0.08818697929382324
Batch 45/64 loss: -0.060082435607910156
Batch 46/64 loss: -0.07130634784698486
Batch 47/64 loss: -0.07839310169219971
Batch 48/64 loss: -0.07087719440460205
Batch 49/64 loss: -0.08224034309387207
Batch 50/64 loss: -0.06348401308059692
Batch 51/64 loss: -0.07268112897872925
Batch 52/64 loss: -0.07454043626785278
Batch 53/64 loss: -0.06504309177398682
Batch 54/64 loss: -0.08166009187698364
Batch 55/64 loss: -0.06645649671554565
Batch 56/64 loss: -0.07833337783813477
Batch 57/64 loss: -0.06240350008010864
Batch 58/64 loss: -0.08259260654449463
Batch 59/64 loss: -0.07970178127288818
Batch 60/64 loss: -0.061678171157836914
Batch 61/64 loss: -0.08342564105987549
Batch 62/64 loss: -0.06667578220367432
Batch 63/64 loss: -0.07341992855072021
Batch 64/64 loss: -0.08551102876663208
Epoch 182  Train loss: -0.07431010709089392  Val loss: 1.7561863378151175e-06
Epoch 183
-------------------------------
Batch 1/64 loss: -0.0814356803894043
Batch 2/64 loss: -0.0775023102760315
Batch 3/64 loss: -0.03722238540649414
Batch 4/64 loss: -0.08540564775466919
Batch 5/64 loss: -0.07938194274902344
Batch 6/64 loss: -0.07931506633758545
Batch 7/64 loss: -0.08747339248657227
Batch 8/64 loss: -0.0761404037475586
Batch 9/64 loss: -0.09556162357330322
Batch 10/64 loss: -0.07738590240478516
Batch 11/64 loss: -0.07986152172088623
Batch 12/64 loss: -0.0533258318901062
Batch 13/64 loss: -0.06165283918380737
Batch 14/64 loss: -0.07192885875701904
Batch 15/64 loss: -0.07284700870513916
Batch 16/64 loss: -0.07843971252441406
Batch 17/64 loss: -0.08218061923980713
Batch 18/64 loss: -0.06688213348388672
Batch 19/64 loss: -0.06987965106964111
Batch 20/64 loss: -0.055267393589019775
Batch 21/64 loss: -0.06949138641357422
Batch 22/64 loss: -0.06318145990371704
Batch 23/64 loss: -0.078091561794281
Batch 24/64 loss: -0.0664294958114624
Batch 25/64 loss: -0.10097759962081909
Batch 26/64 loss: -0.06126141548156738
Batch 27/64 loss: -0.07626056671142578
Batch 28/64 loss: -0.08278536796569824
Batch 29/64 loss: -0.0809859037399292
Batch 30/64 loss: -0.07357418537139893
Batch 31/64 loss: -0.07640844583511353
Batch 32/64 loss: -0.04782909154891968
Batch 33/64 loss: -0.08363431692123413
Batch 34/64 loss: -0.09467583894729614
Batch 35/64 loss: -0.06543737649917603
Batch 36/64 loss: -0.0623127818107605
Batch 37/64 loss: -0.08089810609817505
Batch 38/64 loss: -0.07258039712905884
Batch 39/64 loss: -0.07263398170471191
Batch 40/64 loss: -0.06415355205535889
Batch 41/64 loss: -0.08059042692184448
Batch 42/64 loss: -0.05419677495956421
Batch 43/64 loss: -0.07637518644332886
Batch 44/64 loss: -0.09943467378616333
Batch 45/64 loss: -0.08682185411453247
Batch 46/64 loss: -0.08708536624908447
Batch 47/64 loss: -0.05560547113418579
Batch 48/64 loss: -0.07550042867660522
Batch 49/64 loss: -0.08904910087585449
Batch 50/64 loss: -0.07264977693557739
Batch 51/64 loss: -0.05442309379577637
Batch 52/64 loss: -0.07873469591140747
Batch 53/64 loss: -0.07044744491577148
Batch 54/64 loss: -0.06718891859054565
Batch 55/64 loss: -0.0637277364730835
Batch 56/64 loss: -0.07021218538284302
Batch 57/64 loss: -0.05392110347747803
Batch 58/64 loss: -0.08225852251052856
Batch 59/64 loss: -0.07250434160232544
Batch 60/64 loss: -0.07602894306182861
Batch 61/64 loss: -0.08475875854492188
Batch 62/64 loss: -0.05969679355621338
Batch 63/64 loss: -0.07084310054779053
Batch 64/64 loss: -0.0751047134399414
Epoch 183  Train loss: -0.07342864298353008  Val loss: -0.011330165404224723
Epoch 184
-------------------------------
Batch 1/64 loss: -0.0860816240310669
Batch 2/64 loss: -0.09008413553237915
Batch 3/64 loss: -0.08983373641967773
Batch 4/64 loss: -0.05441868305206299
Batch 5/64 loss: -0.07596808671951294
Batch 6/64 loss: -0.09919887781143188
Batch 7/64 loss: -0.08534371852874756
Batch 8/64 loss: -0.09021216630935669
Batch 9/64 loss: -0.08434665203094482
Batch 10/64 loss: -0.08245068788528442
Batch 11/64 loss: -0.05982941389083862
Batch 12/64 loss: -0.06951904296875
Batch 13/64 loss: -0.07349580526351929
Batch 14/64 loss: -0.06809908151626587
Batch 15/64 loss: -0.07545465230941772
Batch 16/64 loss: -0.07486611604690552
Batch 17/64 loss: -0.07337570190429688
Batch 18/64 loss: -0.06806939840316772
Batch 19/64 loss: -0.07294327020645142
Batch 20/64 loss: -0.07834625244140625
Batch 21/64 loss: -0.07795721292495728
Batch 22/64 loss: -0.055306971073150635
Batch 23/64 loss: -0.032532453536987305
Batch 24/64 loss: -0.07425159215927124
Batch 25/64 loss: -0.08763623237609863
Batch 26/64 loss: -0.07590734958648682
Batch 27/64 loss: -0.0670708417892456
Batch 28/64 loss: -0.07589900493621826
Batch 29/64 loss: -0.08461713790893555
Batch 30/64 loss: -0.07863831520080566
Batch 31/64 loss: -0.02669447660446167
Batch 32/64 loss: -0.06398224830627441
Batch 33/64 loss: -0.07874786853790283
Batch 34/64 loss: -0.06555694341659546
Batch 35/64 loss: -0.07935863733291626
Batch 36/64 loss: -0.09958881139755249
Batch 37/64 loss: -0.06022942066192627
Batch 38/64 loss: -0.0727987289428711
Batch 39/64 loss: -0.06821805238723755
Batch 40/64 loss: -0.07498705387115479
Batch 41/64 loss: -0.07325625419616699
Batch 42/64 loss: -0.07116430997848511
Batch 43/64 loss: -0.08403509855270386
Batch 44/64 loss: -0.06709498167037964
Batch 45/64 loss: -0.08545047044754028
Batch 46/64 loss: -0.076987624168396
Batch 47/64 loss: -0.0867847204208374
Batch 48/64 loss: -0.07050150632858276
Batch 49/64 loss: -0.061291277408599854
Batch 50/64 loss: -0.06797271966934204
Batch 51/64 loss: -0.0703851580619812
Batch 52/64 loss: -0.07549649477005005
Batch 53/64 loss: -0.07650977373123169
Batch 54/64 loss: -0.06700736284255981
Batch 55/64 loss: -0.07400375604629517
Batch 56/64 loss: -0.08265805244445801
Batch 57/64 loss: -0.042246997356414795
Batch 58/64 loss: -0.07646709680557251
Batch 59/64 loss: -0.07608234882354736
Batch 60/64 loss: -0.07516348361968994
Batch 61/64 loss: -0.07795780897140503
Batch 62/64 loss: -0.07900434732437134
Batch 63/64 loss: -0.08656775951385498
Batch 64/64 loss: -0.07786428928375244
Epoch 184  Train loss: -0.07395143649157356  Val loss: -0.018911082720019155
Saving best model, epoch: 184
Epoch 185
-------------------------------
Batch 1/64 loss: -0.07477086782455444
Batch 2/64 loss: -0.07321763038635254
Batch 3/64 loss: -0.07124638557434082
Batch 4/64 loss: -0.0819021463394165
Batch 5/64 loss: -0.08047926425933838
Batch 6/64 loss: -0.09458804130554199
Batch 7/64 loss: -0.0743207335472107
Batch 8/64 loss: -0.08543604612350464
Batch 9/64 loss: -0.07869744300842285
Batch 10/64 loss: -0.07762491703033447
Batch 11/64 loss: -0.06457597017288208
Batch 12/64 loss: -0.08271384239196777
Batch 13/64 loss: -0.07594102621078491
Batch 14/64 loss: -0.08132684230804443
Batch 15/64 loss: -0.09157371520996094
Batch 16/64 loss: -0.06547605991363525
Batch 17/64 loss: -0.08124727010726929
Batch 18/64 loss: -0.07656866312026978
Batch 19/64 loss: -0.09325987100601196
Batch 20/64 loss: -0.09973597526550293
Batch 21/64 loss: -0.0698392391204834
Batch 22/64 loss: -0.08148539066314697
Batch 23/64 loss: -0.07312065362930298
Batch 24/64 loss: -0.06365752220153809
Batch 25/64 loss: -0.07133603096008301
Batch 26/64 loss: -0.0740310549736023
Batch 27/64 loss: -0.08308804035186768
Batch 28/64 loss: -0.05849349498748779
Batch 29/64 loss: -0.08843308687210083
Batch 30/64 loss: -0.07675337791442871
Batch 31/64 loss: -0.06489086151123047
Batch 32/64 loss: -0.07709860801696777
Batch 33/64 loss: -0.06545412540435791
Batch 34/64 loss: -0.059884488582611084
Batch 35/64 loss: -0.05654478073120117
Batch 36/64 loss: -0.07085806131362915
Batch 37/64 loss: -0.06550335884094238
Batch 38/64 loss: -0.09586292505264282
Batch 39/64 loss: -0.07208818197250366
Batch 40/64 loss: -0.0702744722366333
Batch 41/64 loss: -0.06896615028381348
Batch 42/64 loss: -0.08956694602966309
Batch 43/64 loss: -0.06714487075805664
Batch 44/64 loss: -0.08052003383636475
Batch 45/64 loss: -0.0600055456161499
Batch 46/64 loss: -0.08094871044158936
Batch 47/64 loss: -0.08878785371780396
Batch 48/64 loss: -0.06808143854141235
Batch 49/64 loss: -0.06630909442901611
Batch 50/64 loss: -0.06930851936340332
Batch 51/64 loss: -0.07859379053115845
Batch 52/64 loss: -0.07599979639053345
Batch 53/64 loss: -0.05516362190246582
Batch 54/64 loss: -0.06294101476669312
Batch 55/64 loss: -0.06280708312988281
Batch 56/64 loss: -0.09607177972793579
Batch 57/64 loss: -0.07837516069412231
Batch 58/64 loss: -0.052426934242248535
Batch 59/64 loss: -0.0771017074584961
Batch 60/64 loss: -0.078646719455719
Batch 61/64 loss: -0.05744755268096924
Batch 62/64 loss: -0.08418363332748413
Batch 63/64 loss: -0.06363552808761597
Batch 64/64 loss: -0.0838577151298523
Epoch 185  Train loss: -0.07481297628552305  Val loss: -0.019427360742772157
Saving best model, epoch: 185
Epoch 186
-------------------------------
Batch 1/64 loss: -0.0743865966796875
Batch 2/64 loss: -0.082039475440979
Batch 3/64 loss: -0.08054405450820923
Batch 4/64 loss: -0.08256042003631592
Batch 5/64 loss: -0.08215552568435669
Batch 6/64 loss: -0.08885937929153442
Batch 7/64 loss: -0.08927065134048462
Batch 8/64 loss: -0.07539325952529907
Batch 9/64 loss: -0.09247839450836182
Batch 10/64 loss: -0.07322114706039429
Batch 11/64 loss: -0.06704789400100708
Batch 12/64 loss: -0.07536816596984863
Batch 13/64 loss: -0.015128016471862793
Batch 14/64 loss: -0.08063995838165283
Batch 15/64 loss: -0.07377219200134277
Batch 16/64 loss: -0.09169608354568481
Batch 17/64 loss: -0.08062398433685303
Batch 18/64 loss: -0.08579075336456299
Batch 19/64 loss: -0.09087097644805908
Batch 20/64 loss: -0.07486212253570557
Batch 21/64 loss: -0.07018768787384033
Batch 22/64 loss: -0.06154388189315796
Batch 23/64 loss: -0.07719016075134277
Batch 24/64 loss: -0.05847489833831787
Batch 25/64 loss: -0.08057326078414917
Batch 26/64 loss: -0.06332248449325562
Batch 27/64 loss: -0.07452183961868286
Batch 28/64 loss: -0.06713736057281494
Batch 29/64 loss: -0.09182184934616089
Batch 30/64 loss: -0.08805739879608154
Batch 31/64 loss: -0.07497739791870117
Batch 32/64 loss: -0.07575350999832153
Batch 33/64 loss: -0.03600192070007324
Batch 34/64 loss: -0.08089947700500488
Batch 35/64 loss: -0.05567294359207153
Batch 36/64 loss: -0.07813328504562378
Batch 37/64 loss: -0.06878435611724854
Batch 38/64 loss: -0.08464986085891724
Batch 39/64 loss: -0.07308226823806763
Batch 40/64 loss: -0.09355306625366211
Batch 41/64 loss: -0.05969059467315674
Batch 42/64 loss: -0.06499409675598145
Batch 43/64 loss: -0.08663493394851685
Batch 44/64 loss: -0.06639349460601807
Batch 45/64 loss: -0.05552828311920166
Batch 46/64 loss: -0.0746312141418457
Batch 47/64 loss: -0.08146905899047852
Batch 48/64 loss: -0.07750940322875977
Batch 49/64 loss: -0.05600690841674805
Batch 50/64 loss: -0.07102525234222412
Batch 51/64 loss: -0.0814218521118164
Batch 52/64 loss: -0.09138858318328857
Batch 53/64 loss: -0.06837111711502075
Batch 54/64 loss: -0.07757800817489624
Batch 55/64 loss: -0.06935960054397583
Batch 56/64 loss: -0.06667166948318481
Batch 57/64 loss: -0.08465367555618286
Batch 58/64 loss: -0.06220424175262451
Batch 59/64 loss: -0.07516086101531982
Batch 60/64 loss: -0.07309073209762573
Batch 61/64 loss: -0.07574582099914551
Batch 62/64 loss: -0.06571853160858154
Batch 63/64 loss: -0.08726620674133301
Batch 64/64 loss: -0.07050776481628418
Epoch 186  Train loss: -0.07429714389875823  Val loss: -0.01109855158632154
Epoch 187
-------------------------------
Batch 1/64 loss: -0.07659757137298584
Batch 2/64 loss: -0.032749712467193604
Batch 3/64 loss: -0.08560389280319214
Batch 4/64 loss: -0.06644713878631592
Batch 5/64 loss: -0.0672655701637268
Batch 6/64 loss: -0.07108855247497559
Batch 7/64 loss: -0.07876312732696533
Batch 8/64 loss: -0.07212990522384644
Batch 9/64 loss: -0.0911484956741333
Batch 10/64 loss: -0.08143258094787598
Batch 11/64 loss: -0.08095592260360718
Batch 12/64 loss: -0.07893174886703491
Batch 13/64 loss: -0.08020550012588501
Batch 14/64 loss: -0.07445400953292847
Batch 15/64 loss: -0.06774675846099854
Batch 16/64 loss: -0.08484750986099243
Batch 17/64 loss: -0.07496798038482666
Batch 18/64 loss: -0.07652348279953003
Batch 19/64 loss: -0.07373666763305664
Batch 20/64 loss: -0.04870396852493286
Batch 21/64 loss: -0.06662619113922119
Batch 22/64 loss: -0.09782004356384277
Batch 23/64 loss: -0.07508736848831177
Batch 24/64 loss: -0.05290716886520386
Batch 25/64 loss: -0.07610726356506348
Batch 26/64 loss: -0.07251524925231934
Batch 27/64 loss: -0.07488155364990234
Batch 28/64 loss: -0.08755052089691162
Batch 29/64 loss: -0.07327872514724731
Batch 30/64 loss: -0.07944905757904053
Batch 31/64 loss: -0.08437049388885498
Batch 32/64 loss: -0.04900240898132324
Batch 33/64 loss: -0.04976612329483032
Batch 34/64 loss: -0.07684701681137085
Batch 35/64 loss: -0.0657954216003418
Batch 36/64 loss: -0.0830162763595581
Batch 37/64 loss: -0.054605305194854736
Batch 38/64 loss: -0.08098649978637695
Batch 39/64 loss: -0.09377408027648926
Batch 40/64 loss: -0.08461415767669678
Batch 41/64 loss: -0.07479941844940186
Batch 42/64 loss: -0.0635688304901123
Batch 43/64 loss: -0.07907736301422119
Batch 44/64 loss: -0.07150024175643921
Batch 45/64 loss: -0.08844518661499023
Batch 46/64 loss: -0.07647156715393066
Batch 47/64 loss: -0.06746482849121094
Batch 48/64 loss: -0.08707070350646973
Batch 49/64 loss: -0.07003355026245117
Batch 50/64 loss: -0.08388024568557739
Batch 51/64 loss: -0.07609999179840088
Batch 52/64 loss: -0.07092994451522827
Batch 53/64 loss: -0.07899010181427002
Batch 54/64 loss: -0.0724143385887146
Batch 55/64 loss: -0.07111471891403198
Batch 56/64 loss: -0.07770216464996338
Batch 57/64 loss: -0.06206822395324707
Batch 58/64 loss: -0.07403314113616943
Batch 59/64 loss: -0.08957982063293457
Batch 60/64 loss: -0.059822678565979004
Batch 61/64 loss: -0.07970249652862549
Batch 62/64 loss: -0.07143282890319824
Batch 63/64 loss: -0.07123458385467529
Batch 64/64 loss: -0.051397085189819336
Epoch 187  Train loss: -0.07371428714079016  Val loss: -0.016196609567530787
Epoch 188
-------------------------------
Batch 1/64 loss: -0.0891348123550415
Batch 2/64 loss: -0.08913993835449219
Batch 3/64 loss: -0.08741140365600586
Batch 4/64 loss: -0.07014721632003784
Batch 5/64 loss: -0.05378377437591553
Batch 6/64 loss: -0.07641863822937012
Batch 7/64 loss: -0.08222329616546631
Batch 8/64 loss: -0.07935023307800293
Batch 9/64 loss: -0.09092378616333008
Batch 10/64 loss: -0.09768372774124146
Batch 11/64 loss: -0.07095855474472046
Batch 12/64 loss: -0.06720149517059326
Batch 13/64 loss: -0.07962608337402344
Batch 14/64 loss: -0.08143281936645508
Batch 15/64 loss: -0.0880582332611084
Batch 16/64 loss: -0.07658052444458008
Batch 17/64 loss: -0.09184181690216064
Batch 18/64 loss: -0.0932387113571167
Batch 19/64 loss: -0.07898920774459839
Batch 20/64 loss: -0.0966787338256836
Batch 21/64 loss: -0.08643138408660889
Batch 22/64 loss: -0.07507920265197754
Batch 23/64 loss: -0.08173167705535889
Batch 24/64 loss: -0.08117085695266724
Batch 25/64 loss: -0.07268106937408447
Batch 26/64 loss: -0.09739291667938232
Batch 27/64 loss: -0.06233173608779907
Batch 28/64 loss: -0.06856322288513184
Batch 29/64 loss: -0.0859607458114624
Batch 30/64 loss: -0.0712575912475586
Batch 31/64 loss: -0.08094578981399536
Batch 32/64 loss: -0.05234473943710327
Batch 33/64 loss: -0.060072481632232666
Batch 34/64 loss: -0.05906122922897339
Batch 35/64 loss: -0.07120358943939209
Batch 36/64 loss: -0.06398844718933105
Batch 37/64 loss: -0.08229941129684448
Batch 38/64 loss: -0.08643966913223267
Batch 39/64 loss: -0.06652647256851196
Batch 40/64 loss: -0.055492401123046875
Batch 41/64 loss: -0.07690143585205078
Batch 42/64 loss: -0.06207764148712158
Batch 43/64 loss: -0.054021239280700684
Batch 44/64 loss: -0.07864141464233398
Batch 45/64 loss: -0.08589565753936768
Batch 46/64 loss: -0.08697497844696045
Batch 47/64 loss: -0.08387500047683716
Batch 48/64 loss: -0.08688342571258545
Batch 49/64 loss: -0.05725210905075073
Batch 50/64 loss: -0.08606827259063721
Batch 51/64 loss: -0.07595694065093994
Batch 52/64 loss: -0.05426585674285889
Batch 53/64 loss: -0.0776815414428711
Batch 54/64 loss: -0.0714917778968811
Batch 55/64 loss: -0.05467122793197632
Batch 56/64 loss: -0.07633721828460693
Batch 57/64 loss: -0.06252557039260864
Batch 58/64 loss: -0.07091408967971802
Batch 59/64 loss: -0.08341997861862183
Batch 60/64 loss: -0.08047586679458618
Batch 61/64 loss: -0.08337891101837158
Batch 62/64 loss: -0.08944672346115112
Batch 63/64 loss: -0.05735909938812256
Batch 64/64 loss: -0.05912727117538452
Epoch 188  Train loss: -0.07596327954647587  Val loss: -0.017409831592717123
Epoch 189
-------------------------------
Batch 1/64 loss: -0.06437766551971436
Batch 2/64 loss: -0.06936442852020264
Batch 3/64 loss: -0.08253484964370728
Batch 4/64 loss: -0.08305561542510986
Batch 5/64 loss: -0.08892035484313965
Batch 6/64 loss: -0.07693958282470703
Batch 7/64 loss: -0.0786128044128418
Batch 8/64 loss: -0.09694015979766846
Batch 9/64 loss: -0.052198171615600586
Batch 10/64 loss: -0.08108043670654297
Batch 11/64 loss: -0.09005683660507202
Batch 12/64 loss: -0.08856809139251709
Batch 13/64 loss: -0.08926939964294434
Batch 14/64 loss: -0.07166332006454468
Batch 15/64 loss: -0.07557135820388794
Batch 16/64 loss: -0.07542753219604492
Batch 17/64 loss: -0.06499731540679932
Batch 18/64 loss: -0.08456873893737793
Batch 19/64 loss: -0.0798637866973877
Batch 20/64 loss: -0.07409834861755371
Batch 21/64 loss: -0.08817720413208008
Batch 22/64 loss: -0.0729759931564331
Batch 23/64 loss: -0.06444203853607178
Batch 24/64 loss: -0.077728271484375
Batch 25/64 loss: -0.08181989192962646
Batch 26/64 loss: -0.06110668182373047
Batch 27/64 loss: -0.05622589588165283
Batch 28/64 loss: -0.06108570098876953
Batch 29/64 loss: -0.07876324653625488
Batch 30/64 loss: -0.08139932155609131
Batch 31/64 loss: -0.06977134943008423
Batch 32/64 loss: -0.06963992118835449
Batch 33/64 loss: -0.05117380619049072
Batch 34/64 loss: -0.08184760808944702
Batch 35/64 loss: -0.07182466983795166
Batch 36/64 loss: -0.07293367385864258
Batch 37/64 loss: -0.08107799291610718
Batch 38/64 loss: -0.0823514461517334
Batch 39/64 loss: -0.07933270931243896
Batch 40/64 loss: -0.06396281719207764
Batch 41/64 loss: -0.07490366697311401
Batch 42/64 loss: -0.07095211744308472
Batch 43/64 loss: -0.0792350172996521
Batch 44/64 loss: -0.08568406105041504
Batch 45/64 loss: -0.10482168197631836
Batch 46/64 loss: -0.08465170860290527
Batch 47/64 loss: -0.07456064224243164
Batch 48/64 loss: -0.07932519912719727
Batch 49/64 loss: -0.07689458131790161
Batch 50/64 loss: -0.06948626041412354
Batch 51/64 loss: -0.09052860736846924
Batch 52/64 loss: -0.08209329843521118
Batch 53/64 loss: -0.08802592754364014
Batch 54/64 loss: -0.08745592832565308
Batch 55/64 loss: -0.08409178256988525
Batch 56/64 loss: -0.062185585498809814
Batch 57/64 loss: -0.09250909090042114
Batch 58/64 loss: -0.10274869203567505
Batch 59/64 loss: -0.05833125114440918
Batch 60/64 loss: -0.08953893184661865
Batch 61/64 loss: -0.05652320384979248
Batch 62/64 loss: -0.08936077356338501
Batch 63/64 loss: -0.062088608741760254
Batch 64/64 loss: -0.08190768957138062
Epoch 189  Train loss: -0.07722629682690489  Val loss: -0.018582225050713188
Epoch 190
-------------------------------
Batch 1/64 loss: -0.07929247617721558
Batch 2/64 loss: -0.08159935474395752
Batch 3/64 loss: -0.09426230192184448
Batch 4/64 loss: -0.07333040237426758
Batch 5/64 loss: -0.06585532426834106
Batch 6/64 loss: -0.08140039443969727
Batch 7/64 loss: -0.10065627098083496
Batch 8/64 loss: -0.076901376247406
Batch 9/64 loss: -0.0777478814125061
Batch 10/64 loss: -0.0907973051071167
Batch 11/64 loss: -0.07885658740997314
Batch 12/64 loss: -0.08560031652450562
Batch 13/64 loss: -0.07863837480545044
Batch 14/64 loss: -0.05864107608795166
Batch 15/64 loss: -0.08760160207748413
Batch 16/64 loss: -0.09652286767959595
Batch 17/64 loss: -0.10083287954330444
Batch 18/64 loss: -0.07243752479553223
Batch 19/64 loss: -0.059013545513153076
Batch 20/64 loss: -0.060861945152282715
Batch 21/64 loss: -0.059337735176086426
Batch 22/64 loss: -0.06854122877120972
Batch 23/64 loss: -0.060530781745910645
Batch 24/64 loss: -0.09549373388290405
Batch 25/64 loss: -0.07657617330551147
Batch 26/64 loss: -0.09489244222640991
Batch 27/64 loss: -0.08719223737716675
Batch 28/64 loss: -0.059200167655944824
Batch 29/64 loss: -0.09512186050415039
Batch 30/64 loss: -0.07335031032562256
Batch 31/64 loss: -0.08587765693664551
Batch 32/64 loss: -0.09857112169265747
Batch 33/64 loss: -0.08696138858795166
Batch 34/64 loss: -0.08482229709625244
Batch 35/64 loss: -0.08929949998855591
Batch 36/64 loss: -0.09206753969192505
Batch 37/64 loss: -0.06137561798095703
Batch 38/64 loss: -0.07113182544708252
Batch 39/64 loss: -0.08513391017913818
Batch 40/64 loss: -0.07187235355377197
Batch 41/64 loss: -0.06719905138015747
Batch 42/64 loss: -0.08742821216583252
Batch 43/64 loss: -0.08640426397323608
Batch 44/64 loss: -0.07227784395217896
Batch 45/64 loss: -0.09039533138275146
Batch 46/64 loss: -0.08529412746429443
Batch 47/64 loss: -0.0848245620727539
Batch 48/64 loss: -0.05800658464431763
Batch 49/64 loss: -0.07659745216369629
Batch 50/64 loss: -0.08309727907180786
Batch 51/64 loss: -0.09040087461471558
Batch 52/64 loss: -0.0803496241569519
Batch 53/64 loss: -0.07422399520874023
Batch 54/64 loss: -0.05611896514892578
Batch 55/64 loss: -0.07994192838668823
Batch 56/64 loss: -0.07709002494812012
Batch 57/64 loss: -0.07438468933105469
Batch 58/64 loss: -0.08238047361373901
Batch 59/64 loss: -0.06883329153060913
Batch 60/64 loss: -0.08789944648742676
Batch 61/64 loss: -0.034873127937316895
Batch 62/64 loss: -0.05462801456451416
Batch 63/64 loss: -0.05070340633392334
Batch 64/64 loss: -0.06622809171676636
Epoch 190  Train loss: -0.07766624829348395  Val loss: -0.014223475636485516
Epoch 191
-------------------------------
Batch 1/64 loss: -0.05856829881668091
Batch 2/64 loss: -0.08498013019561768
Batch 3/64 loss: -0.08829432725906372
Batch 4/64 loss: -0.070168137550354
Batch 5/64 loss: -0.06602537631988525
Batch 6/64 loss: -0.08663338422775269
Batch 7/64 loss: -0.09750652313232422
Batch 8/64 loss: -0.06828564405441284
Batch 9/64 loss: -0.07093572616577148
Batch 10/64 loss: -0.07871055603027344
Batch 11/64 loss: -0.08919030427932739
Batch 12/64 loss: -0.09793037176132202
Batch 13/64 loss: -0.0528566837310791
Batch 14/64 loss: -0.09244859218597412
Batch 15/64 loss: -0.06682091951370239
Batch 16/64 loss: -0.0882805585861206
Batch 17/64 loss: -0.08145713806152344
Batch 18/64 loss: -0.07575422525405884
Batch 19/64 loss: -0.10086899995803833
Batch 20/64 loss: -0.08081591129302979
Batch 21/64 loss: -0.08955752849578857
Batch 22/64 loss: -0.07329589128494263
Batch 23/64 loss: -0.08449500799179077
Batch 24/64 loss: -0.06629675626754761
Batch 25/64 loss: -0.04712319374084473
Batch 26/64 loss: -0.06441473960876465
Batch 27/64 loss: -0.07647311687469482
Batch 28/64 loss: -0.09310495853424072
Batch 29/64 loss: -0.06295192241668701
Batch 30/64 loss: -0.09069472551345825
Batch 31/64 loss: -0.06561535596847534
Batch 32/64 loss: -0.07624965906143188
Batch 33/64 loss: -0.08036404848098755
Batch 34/64 loss: -0.09502911567687988
Batch 35/64 loss: -0.07943302392959595
Batch 36/64 loss: -0.08212244510650635
Batch 37/64 loss: -0.07282555103302002
Batch 38/64 loss: -0.0786055326461792
Batch 39/64 loss: -0.08231371641159058
Batch 40/64 loss: -0.05888521671295166
Batch 41/64 loss: -0.07574951648712158
Batch 42/64 loss: -0.07947123050689697
Batch 43/64 loss: -0.07520341873168945
Batch 44/64 loss: -0.08872538805007935
Batch 45/64 loss: -0.05443704128265381
Batch 46/64 loss: -0.09035634994506836
Batch 47/64 loss: -0.07468575239181519
Batch 48/64 loss: -0.07854253053665161
Batch 49/64 loss: -0.08628445863723755
Batch 50/64 loss: -0.08978414535522461
Batch 51/64 loss: -0.07831329107284546
Batch 52/64 loss: -0.08345234394073486
Batch 53/64 loss: -0.07050180435180664
Batch 54/64 loss: -0.08671557903289795
Batch 55/64 loss: -0.07169270515441895
Batch 56/64 loss: -0.07025355100631714
Batch 57/64 loss: -0.07467532157897949
Batch 58/64 loss: -0.06433171033859253
Batch 59/64 loss: -0.07859516143798828
Batch 60/64 loss: -0.06464314460754395
Batch 61/64 loss: -0.06086772680282593
Batch 62/64 loss: -0.09502679109573364
Batch 63/64 loss: -0.08364486694335938
Batch 64/64 loss: -0.08418393135070801
Epoch 191  Train loss: -0.07773294261857575  Val loss: -0.016536892074899574
Epoch 192
-------------------------------
Batch 1/64 loss: -0.09968382120132446
Batch 2/64 loss: -0.09182006120681763
Batch 3/64 loss: -0.07873636484146118
Batch 4/64 loss: -0.08653533458709717
Batch 5/64 loss: -0.07613867521286011
Batch 6/64 loss: -0.08578157424926758
Batch 7/64 loss: -0.08098232746124268
Batch 8/64 loss: -0.09509092569351196
Batch 9/64 loss: -0.07340800762176514
Batch 10/64 loss: -0.06737542152404785
Batch 11/64 loss: -0.0949791669845581
Batch 12/64 loss: -0.08081579208374023
Batch 13/64 loss: -0.06624090671539307
Batch 14/64 loss: -0.0834386944770813
Batch 15/64 loss: -0.030992627143859863
Batch 16/64 loss: -0.08520692586898804
Batch 17/64 loss: -0.08070838451385498
Batch 18/64 loss: -0.0701403021812439
Batch 19/64 loss: -0.08829104900360107
Batch 20/64 loss: -0.09154391288757324
Batch 21/64 loss: -0.09313565492630005
Batch 22/64 loss: -0.07432353496551514
Batch 23/64 loss: -0.07434552907943726
Batch 24/64 loss: -0.08564579486846924
Batch 25/64 loss: -0.09408724308013916
Batch 26/64 loss: -0.08647096157073975
Batch 27/64 loss: -0.08153259754180908
Batch 28/64 loss: -0.08959102630615234
Batch 29/64 loss: -0.09185576438903809
Batch 30/64 loss: -0.07949584722518921
Batch 31/64 loss: -0.07822197675704956
Batch 32/64 loss: -0.09018003940582275
Batch 33/64 loss: -0.08793109655380249
Batch 34/64 loss: -0.06363117694854736
Batch 35/64 loss: -0.050769805908203125
Batch 36/64 loss: -0.07499504089355469
Batch 37/64 loss: -0.08322417736053467
Batch 38/64 loss: -0.07713615894317627
Batch 39/64 loss: -0.09122371673583984
Batch 40/64 loss: -0.07159852981567383
Batch 41/64 loss: -0.08996862173080444
Batch 42/64 loss: -0.08881151676177979
Batch 43/64 loss: -0.08522194623947144
Batch 44/64 loss: -0.08928787708282471
Batch 45/64 loss: -0.08646857738494873
Batch 46/64 loss: -0.06925630569458008
Batch 47/64 loss: -0.08404844999313354
Batch 48/64 loss: -0.0726480484008789
Batch 49/64 loss: -0.09129297733306885
Batch 50/64 loss: -0.08069860935211182
Batch 51/64 loss: -0.06311208009719849
Batch 52/64 loss: -0.09091305732727051
Batch 53/64 loss: -0.0789417028427124
Batch 54/64 loss: -0.07805752754211426
Batch 55/64 loss: -0.0913916826248169
Batch 56/64 loss: -0.08465576171875
Batch 57/64 loss: -0.07315605878829956
Batch 58/64 loss: -0.08364510536193848
Batch 59/64 loss: -0.07000130414962769
Batch 60/64 loss: -0.06038331985473633
Batch 61/64 loss: -0.05786699056625366
Batch 62/64 loss: -0.07894641160964966
Batch 63/64 loss: -0.07858115434646606
Batch 64/64 loss: -0.08648896217346191
Epoch 192  Train loss: -0.08030631775949515  Val loss: -0.01811555652683953
Epoch 193
-------------------------------
Batch 1/64 loss: -0.08215314149856567
Batch 2/64 loss: -0.08309566974639893
Batch 3/64 loss: -0.09802210330963135
Batch 4/64 loss: -0.06984347105026245
Batch 5/64 loss: -0.08858060836791992
Batch 6/64 loss: -0.06011343002319336
Batch 7/64 loss: -0.067718505859375
Batch 8/64 loss: -0.086550772190094
Batch 9/64 loss: -0.05562502145767212
Batch 10/64 loss: -0.09099471569061279
Batch 11/64 loss: -0.07750725746154785
Batch 12/64 loss: -0.08013558387756348
Batch 13/64 loss: -0.09602755308151245
Batch 14/64 loss: -0.0764356255531311
Batch 15/64 loss: -0.08687949180603027
Batch 16/64 loss: -0.07561945915222168
Batch 17/64 loss: -0.06615668535232544
Batch 18/64 loss: -0.06802940368652344
Batch 19/64 loss: -0.07772278785705566
Batch 20/64 loss: -0.08809095621109009
Batch 21/64 loss: -0.08791595697402954
Batch 22/64 loss: -0.08478760719299316
Batch 23/64 loss: -0.08208960294723511
Batch 24/64 loss: -0.0843268632888794
Batch 25/64 loss: -0.0914275050163269
Batch 26/64 loss: -0.1033705472946167
Batch 27/64 loss: -0.08445084095001221
Batch 28/64 loss: -0.0738341212272644
Batch 29/64 loss: -0.10049837827682495
Batch 30/64 loss: -0.09487438201904297
Batch 31/64 loss: -0.07955580949783325
Batch 32/64 loss: -0.06896913051605225
Batch 33/64 loss: -0.09007769823074341
Batch 34/64 loss: -0.06218719482421875
Batch 35/64 loss: -0.06392627954483032
Batch 36/64 loss: -0.07023447751998901
Batch 37/64 loss: -0.0727616548538208
Batch 38/64 loss: -0.0894845724105835
Batch 39/64 loss: -0.07919710874557495
Batch 40/64 loss: -0.08370351791381836
Batch 41/64 loss: -0.07394152879714966
Batch 42/64 loss: -0.07533454895019531
Batch 43/64 loss: -0.07903110980987549
Batch 44/64 loss: -0.07060706615447998
Batch 45/64 loss: -0.06398344039916992
Batch 46/64 loss: -0.07884728908538818
Batch 47/64 loss: -0.09551781415939331
Batch 48/64 loss: -0.06664097309112549
Batch 49/64 loss: -0.0898667573928833
Batch 50/64 loss: -0.08535820245742798
Batch 51/64 loss: -0.09598958492279053
Batch 52/64 loss: -0.08301788568496704
Batch 53/64 loss: -0.09009748697280884
Batch 54/64 loss: -0.09858214855194092
Batch 55/64 loss: -0.09306681156158447
Batch 56/64 loss: -0.07609659433364868
Batch 57/64 loss: -0.1037093997001648
Batch 58/64 loss: -0.06709796190261841
Batch 59/64 loss: -0.05620133876800537
Batch 60/64 loss: -0.08062446117401123
Batch 61/64 loss: -0.06513398885726929
Batch 62/64 loss: -0.06624388694763184
Batch 63/64 loss: -0.08635902404785156
Batch 64/64 loss: -0.06868284940719604
Epoch 193  Train loss: -0.08024842248243444  Val loss: -0.017371375331354307
Epoch 194
-------------------------------
Batch 1/64 loss: -0.07982683181762695
Batch 2/64 loss: -0.06169629096984863
Batch 3/64 loss: -0.08724325895309448
Batch 4/64 loss: -0.08310079574584961
Batch 5/64 loss: -0.0712425708770752
Batch 6/64 loss: -0.07742202281951904
Batch 7/64 loss: -0.08461010456085205
Batch 8/64 loss: -0.09362244606018066
Batch 9/64 loss: -0.1017618179321289
Batch 10/64 loss: -0.0631561279296875
Batch 11/64 loss: -0.08680391311645508
Batch 12/64 loss: -0.0866999626159668
Batch 13/64 loss: -0.08689415454864502
Batch 14/64 loss: -0.0749427080154419
Batch 15/64 loss: -0.08917003870010376
Batch 16/64 loss: -0.07946670055389404
Batch 17/64 loss: -0.08777260780334473
Batch 18/64 loss: -0.0740167498588562
Batch 19/64 loss: -0.0968819260597229
Batch 20/64 loss: -0.09574663639068604
Batch 21/64 loss: -0.07950019836425781
Batch 22/64 loss: -0.08219975233078003
Batch 23/64 loss: -0.0947914719581604
Batch 24/64 loss: -0.07718527317047119
Batch 25/64 loss: -0.08928036689758301
Batch 26/64 loss: -0.08073192834854126
Batch 27/64 loss: -0.05611371994018555
Batch 28/64 loss: -0.07389140129089355
Batch 29/64 loss: -0.08288681507110596
Batch 30/64 loss: -0.07332521677017212
Batch 31/64 loss: -0.08547055721282959
Batch 32/64 loss: -0.09205490350723267
Batch 33/64 loss: -0.0853303074836731
Batch 34/64 loss: -0.09544181823730469
Batch 35/64 loss: -0.09784162044525146
Batch 36/64 loss: -0.07840275764465332
Batch 37/64 loss: -0.08623689413070679
Batch 38/64 loss: -0.06021618843078613
Batch 39/64 loss: -0.0912739634513855
Batch 40/64 loss: -0.09372454881668091
Batch 41/64 loss: -0.058523476123809814
Batch 42/64 loss: -0.07672244310379028
Batch 43/64 loss: -0.07946527004241943
Batch 44/64 loss: -0.06983721256256104
Batch 45/64 loss: -0.018564939498901367
Batch 46/64 loss: -0.08176863193511963
Batch 47/64 loss: -0.0872265100479126
Batch 48/64 loss: -0.09224998950958252
Batch 49/64 loss: -0.08431476354598999
Batch 50/64 loss: -0.10009121894836426
Batch 51/64 loss: -0.08464163541793823
Batch 52/64 loss: -0.06951993703842163
Batch 53/64 loss: -0.0809028148651123
Batch 54/64 loss: -0.07295352220535278
Batch 55/64 loss: -0.07738316059112549
Batch 56/64 loss: -0.07840234041213989
Batch 57/64 loss: -0.07792437076568604
Batch 58/64 loss: -0.06805253028869629
Batch 59/64 loss: -0.0807042121887207
Batch 60/64 loss: -0.08230209350585938
Batch 61/64 loss: -0.09041339159011841
Batch 62/64 loss: -0.08397448062896729
Batch 63/64 loss: -0.0729302167892456
Batch 64/64 loss: -0.0711512565612793
Epoch 194  Train loss: -0.08063074467228908  Val loss: -0.01638858506769659
Epoch 195
-------------------------------
Batch 1/64 loss: -0.08567094802856445
Batch 2/64 loss: -0.07154208421707153
Batch 3/64 loss: -0.07179683446884155
Batch 4/64 loss: -0.0811769962310791
Batch 5/64 loss: -0.08333587646484375
Batch 6/64 loss: -0.09211283922195435
Batch 7/64 loss: -0.06642204523086548
Batch 8/64 loss: -0.09480750560760498
Batch 9/64 loss: -0.06969261169433594
Batch 10/64 loss: -0.06993430852890015
Batch 11/64 loss: -0.09274452924728394
Batch 12/64 loss: -0.0770825743675232
Batch 13/64 loss: -0.07580584287643433
Batch 14/64 loss: -0.08158957958221436
Batch 15/64 loss: -0.07361578941345215
Batch 16/64 loss: -0.07407397031784058
Batch 17/64 loss: -0.0861365795135498
Batch 18/64 loss: -0.08308809995651245
Batch 19/64 loss: -0.0701289176940918
Batch 20/64 loss: -0.0859406590461731
Batch 21/64 loss: -0.09249818325042725
Batch 22/64 loss: -0.08258134126663208
Batch 23/64 loss: -0.07440543174743652
Batch 24/64 loss: -0.08093523979187012
Batch 25/64 loss: -0.08826732635498047
Batch 26/64 loss: -0.09820336103439331
Batch 27/64 loss: -0.09159010648727417
Batch 28/64 loss: -0.08647501468658447
Batch 29/64 loss: -0.0982738733291626
Batch 30/64 loss: -0.08204859495162964
Batch 31/64 loss: -0.09126156568527222
Batch 32/64 loss: -0.09138911962509155
Batch 33/64 loss: -0.08618354797363281
Batch 34/64 loss: -0.06428837776184082
Batch 35/64 loss: -0.0788583755493164
Batch 36/64 loss: -0.08858811855316162
Batch 37/64 loss: -0.08280336856842041
Batch 38/64 loss: -0.07876384258270264
Batch 39/64 loss: -0.06595081090927124
Batch 40/64 loss: -0.0789792537689209
Batch 41/64 loss: -0.06995218992233276
Batch 42/64 loss: -0.07471781969070435
Batch 43/64 loss: -0.07508713006973267
Batch 44/64 loss: -0.05412322282791138
Batch 45/64 loss: -0.07960861921310425
Batch 46/64 loss: -0.0917556881904602
Batch 47/64 loss: -0.07895159721374512
Batch 48/64 loss: -0.07536303997039795
Batch 49/64 loss: -0.06572532653808594
Batch 50/64 loss: -0.07377845048904419
Batch 51/64 loss: -0.10492902994155884
Batch 52/64 loss: -0.08357042074203491
Batch 53/64 loss: -0.08368897438049316
Batch 54/64 loss: -0.08034956455230713
Batch 55/64 loss: -0.06466543674468994
Batch 56/64 loss: -0.10139220952987671
Batch 57/64 loss: -0.08713322877883911
Batch 58/64 loss: -0.0867847204208374
Batch 59/64 loss: -0.08232605457305908
Batch 60/64 loss: -0.06861549615859985
Batch 61/64 loss: -0.07468867301940918
Batch 62/64 loss: -0.08028900623321533
Batch 63/64 loss: -0.08740627765655518
Batch 64/64 loss: -0.06577104330062866
Epoch 195  Train loss: -0.08067880635168038  Val loss: -0.019230523060277564
Epoch 196
-------------------------------
Batch 1/64 loss: -0.07656216621398926
Batch 2/64 loss: -0.08385515213012695
Batch 3/64 loss: -0.09030652046203613
Batch 4/64 loss: -0.07303297519683838
Batch 5/64 loss: -0.09517669677734375
Batch 6/64 loss: -0.09099292755126953
Batch 7/64 loss: -0.08176153898239136
Batch 8/64 loss: -0.09578204154968262
Batch 9/64 loss: -0.08351600170135498
Batch 10/64 loss: -0.06381738185882568
Batch 11/64 loss: -0.0709843635559082
Batch 12/64 loss: -0.06788367033004761
Batch 13/64 loss: -0.0827866792678833
Batch 14/64 loss: -0.07944899797439575
Batch 15/64 loss: -0.0675581693649292
Batch 16/64 loss: -0.09537911415100098
Batch 17/64 loss: -0.08258116245269775
Batch 18/64 loss: -0.08003759384155273
Batch 19/64 loss: -0.07655614614486694
Batch 20/64 loss: -0.09118795394897461
Batch 21/64 loss: -0.06980454921722412
Batch 22/64 loss: -0.10134488344192505
Batch 23/64 loss: -0.0893627405166626
Batch 24/64 loss: -0.09337818622589111
Batch 25/64 loss: -0.08869874477386475
Batch 26/64 loss: -0.10356718301773071
Batch 27/64 loss: -0.07712054252624512
Batch 28/64 loss: -0.05790513753890991
Batch 29/64 loss: -0.08311867713928223
Batch 30/64 loss: -0.08618396520614624
Batch 31/64 loss: -0.07803988456726074
Batch 32/64 loss: -0.09082561731338501
Batch 33/64 loss: -0.07496988773345947
Batch 34/64 loss: -0.0959596037864685
Batch 35/64 loss: -0.09375262260437012
Batch 36/64 loss: -0.09203803539276123
Batch 37/64 loss: -0.07441240549087524
Batch 38/64 loss: -0.08637231588363647
Batch 39/64 loss: -0.07704508304595947
Batch 40/64 loss: -0.08888334035873413
Batch 41/64 loss: -0.07538235187530518
Batch 42/64 loss: -0.08380776643753052
Batch 43/64 loss: -0.08071541786193848
Batch 44/64 loss: -0.08702480792999268
Batch 45/64 loss: -0.08016127347946167
Batch 46/64 loss: -0.08151787519454956
Batch 47/64 loss: -0.08480417728424072
Batch 48/64 loss: -0.09587788581848145
Batch 49/64 loss: -0.07577162981033325
Batch 50/64 loss: -0.07312244176864624
Batch 51/64 loss: -0.09311258792877197
Batch 52/64 loss: -0.08911848068237305
Batch 53/64 loss: -0.07631510496139526
Batch 54/64 loss: -0.08226358890533447
Batch 55/64 loss: -0.091278076171875
Batch 56/64 loss: -0.08930593729019165
Batch 57/64 loss: -0.06974691152572632
Batch 58/64 loss: -0.0870431661605835
Batch 59/64 loss: -0.05693542957305908
Batch 60/64 loss: -0.08572149276733398
Batch 61/64 loss: -0.08264434337615967
Batch 62/64 loss: -0.08325165510177612
Batch 63/64 loss: -0.08545410633087158
Batch 64/64 loss: -0.06955188512802124
Epoch 196  Train loss: -0.08273774245206048  Val loss: -0.018630291588117985
Epoch 197
-------------------------------
Batch 1/64 loss: -0.09216690063476562
Batch 2/64 loss: -0.08252698183059692
Batch 3/64 loss: -0.08957576751708984
Batch 4/64 loss: -0.08262884616851807
Batch 5/64 loss: -0.10094022750854492
Batch 6/64 loss: -0.08006972074508667
Batch 7/64 loss: -0.08336687088012695
Batch 8/64 loss: -0.05144864320755005
Batch 9/64 loss: -0.08940768241882324
Batch 10/64 loss: -0.08129936456680298
Batch 11/64 loss: -0.10227668285369873
Batch 12/64 loss: -0.09329581260681152
Batch 13/64 loss: -0.069877028465271
Batch 14/64 loss: -0.0916854739189148
Batch 15/64 loss: -0.07705807685852051
Batch 16/64 loss: -0.08300918340682983
Batch 17/64 loss: -0.09683859348297119
Batch 18/64 loss: -0.08977329730987549
Batch 19/64 loss: -0.08449292182922363
Batch 20/64 loss: -0.08056825399398804
Batch 21/64 loss: -0.0895952582359314
Batch 22/64 loss: -0.0935288667678833
Batch 23/64 loss: -0.08880925178527832
Batch 24/64 loss: -0.07968378067016602
Batch 25/64 loss: -0.09241485595703125
Batch 26/64 loss: -0.06965082883834839
Batch 27/64 loss: -0.0678105354309082
Batch 28/64 loss: -0.09239482879638672
Batch 29/64 loss: -0.08956164121627808
Batch 30/64 loss: -0.0624309778213501
Batch 31/64 loss: -0.07755720615386963
Batch 32/64 loss: -0.08165204524993896
Batch 33/64 loss: -0.09777486324310303
Batch 34/64 loss: -0.09671604633331299
Batch 35/64 loss: -0.09424459934234619
Batch 36/64 loss: -0.08607321977615356
Batch 37/64 loss: -0.10033893585205078
Batch 38/64 loss: -0.10111027956008911
Batch 39/64 loss: -0.05894744396209717
Batch 40/64 loss: -0.08490800857543945
Batch 41/64 loss: -0.09574902057647705
Batch 42/64 loss: -0.08360695838928223
Batch 43/64 loss: -0.0882941484451294
Batch 44/64 loss: -0.06844627857208252
Batch 45/64 loss: -0.07388979196548462
Batch 46/64 loss: -0.07429838180541992
Batch 47/64 loss: -0.07291626930236816
Batch 48/64 loss: -0.08931881189346313
Batch 49/64 loss: -0.0483546257019043
Batch 50/64 loss: -0.0817033052444458
Batch 51/64 loss: -0.08326220512390137
Batch 52/64 loss: -0.09638625383377075
Batch 53/64 loss: -0.08952665328979492
Batch 54/64 loss: -0.08667218685150146
Batch 55/64 loss: -0.0986032485961914
Batch 56/64 loss: -0.09520792961120605
Batch 57/64 loss: -0.06165128946304321
Batch 58/64 loss: -0.08335155248641968
Batch 59/64 loss: -0.0905231237411499
Batch 60/64 loss: -0.08848565816879272
Batch 61/64 loss: -0.08784717321395874
Batch 62/64 loss: -0.08604156970977783
Batch 63/64 loss: -0.09535080194473267
Batch 64/64 loss: -0.08608156442642212
Epoch 197  Train loss: -0.08457346219642489  Val loss: -0.016236480568692445
Epoch 198
-------------------------------
Batch 1/64 loss: -0.07344233989715576
Batch 2/64 loss: -0.08313918113708496
Batch 3/64 loss: -0.08012336492538452
Batch 4/64 loss: -0.07965105772018433
Batch 5/64 loss: -0.09250372648239136
Batch 6/64 loss: -0.054776787757873535
Batch 7/64 loss: -0.07605499029159546
Batch 8/64 loss: -0.07642906904220581
Batch 9/64 loss: -0.07990372180938721
Batch 10/64 loss: -0.08499699831008911
Batch 11/64 loss: -0.08623754978179932
Batch 12/64 loss: -0.08634883165359497
Batch 13/64 loss: -0.0839759111404419
Batch 14/64 loss: -0.08366239070892334
Batch 15/64 loss: -0.0720369815826416
Batch 16/64 loss: -0.06084084510803223
Batch 17/64 loss: -0.07595443725585938
Batch 18/64 loss: -0.07191842794418335
Batch 19/64 loss: -0.08181548118591309
Batch 20/64 loss: -0.05573725700378418
Batch 21/64 loss: -0.08708667755126953
Batch 22/64 loss: -0.07936471700668335
Batch 23/64 loss: -0.09152525663375854
Batch 24/64 loss: -0.09534305334091187
Batch 25/64 loss: -0.08669537305831909
Batch 26/64 loss: -0.08292454481124878
Batch 27/64 loss: -0.08107942342758179
Batch 28/64 loss: -0.06612902879714966
Batch 29/64 loss: -0.08255314826965332
Batch 30/64 loss: -0.0833892822265625
Batch 31/64 loss: -0.08852547407150269
Batch 32/64 loss: -0.09009087085723877
Batch 33/64 loss: -0.0906449556350708
Batch 34/64 loss: -0.0887499451637268
Batch 35/64 loss: -0.07181358337402344
Batch 36/64 loss: -0.10273295640945435
Batch 37/64 loss: -0.08190542459487915
Batch 38/64 loss: -0.06467282772064209
Batch 39/64 loss: -0.10350298881530762
Batch 40/64 loss: -0.09312015771865845
Batch 41/64 loss: -0.08093351125717163
Batch 42/64 loss: -0.09586328268051147
Batch 43/64 loss: -0.07233887910842896
Batch 44/64 loss: -0.0801551342010498
Batch 45/64 loss: -0.0816347599029541
Batch 46/64 loss: -0.09517717361450195
Batch 47/64 loss: -0.0754806399345398
Batch 48/64 loss: -0.0860586166381836
Batch 49/64 loss: -0.0894538164138794
Batch 50/64 loss: -0.09322291612625122
Batch 51/64 loss: -0.09035378694534302
Batch 52/64 loss: -0.06539362668991089
Batch 53/64 loss: -0.069862961769104
Batch 54/64 loss: -0.08326065540313721
Batch 55/64 loss: -0.08797764778137207
Batch 56/64 loss: -0.0960533618927002
Batch 57/64 loss: -0.08119058609008789
Batch 58/64 loss: -0.06849610805511475
Batch 59/64 loss: -0.08615148067474365
Batch 60/64 loss: -0.08542478084564209
Batch 61/64 loss: -0.07086467742919922
Batch 62/64 loss: -0.09876954555511475
Batch 63/64 loss: -0.09013235569000244
Batch 64/64 loss: -0.08276116847991943
Epoch 198  Train loss: -0.08216031719656551  Val loss: -0.02078713297434279
Saving best model, epoch: 198
Epoch 199
-------------------------------
Batch 1/64 loss: -0.10424065589904785
Batch 2/64 loss: -0.09203183650970459
Batch 3/64 loss: -0.08296525478363037
Batch 4/64 loss: -0.08874714374542236
Batch 5/64 loss: -0.0748523473739624
Batch 6/64 loss: -0.0795900821685791
Batch 7/64 loss: -0.10609626770019531
Batch 8/64 loss: -0.08992338180541992
Batch 9/64 loss: -0.07835584878921509
Batch 10/64 loss: -0.07828432321548462
Batch 11/64 loss: -0.06443768739700317
Batch 12/64 loss: -0.07983636856079102
Batch 13/64 loss: -0.0880623459815979
Batch 14/64 loss: -0.10694903135299683
Batch 15/64 loss: -0.09333479404449463
Batch 16/64 loss: -0.08448708057403564
Batch 17/64 loss: -0.0703350305557251
Batch 18/64 loss: -0.0861998200416565
Batch 19/64 loss: -0.08567637205123901
Batch 20/64 loss: -0.06360810995101929
Batch 21/64 loss: -0.09333229064941406
Batch 22/64 loss: -0.08981466293334961
Batch 23/64 loss: -0.05178767442703247
Batch 24/64 loss: -0.06583088636398315
Batch 25/64 loss: -0.07323133945465088
Batch 26/64 loss: -0.06481277942657471
Batch 27/64 loss: -0.08766978979110718
Batch 28/64 loss: -0.08193761110305786
Batch 29/64 loss: -0.08932363986968994
Batch 30/64 loss: -0.07444584369659424
Batch 31/64 loss: -0.07575982809066772
Batch 32/64 loss: -0.06246531009674072
Batch 33/64 loss: -0.07920455932617188
Batch 34/64 loss: -0.077933669090271
Batch 35/64 loss: -0.08182483911514282
Batch 36/64 loss: -0.08738934993743896
Batch 37/64 loss: -0.06659543514251709
Batch 38/64 loss: -0.07548803091049194
Batch 39/64 loss: -0.08432239294052124
Batch 40/64 loss: -0.08721190690994263
Batch 41/64 loss: -0.07654154300689697
Batch 42/64 loss: -0.08690762519836426
Batch 43/64 loss: -0.07097721099853516
Batch 44/64 loss: -0.09012365341186523
Batch 45/64 loss: -0.07169526815414429
Batch 46/64 loss: -0.0901191234588623
Batch 47/64 loss: -0.07406860589981079
Batch 48/64 loss: -0.08992451429367065
Batch 49/64 loss: -0.09972769021987915
Batch 50/64 loss: -0.08955812454223633
Batch 51/64 loss: -0.07936882972717285
Batch 52/64 loss: -0.0789405107498169
Batch 53/64 loss: -0.08875441551208496
Batch 54/64 loss: -0.08763271570205688
Batch 55/64 loss: -0.08163261413574219
Batch 56/64 loss: -0.08212387561798096
Batch 57/64 loss: -0.09288889169692993
Batch 58/64 loss: -0.06851822137832642
Batch 59/64 loss: -0.06873834133148193
Batch 60/64 loss: -0.0819467306137085
Batch 61/64 loss: -0.07736444473266602
Batch 62/64 loss: -0.08864551782608032
Batch 63/64 loss: -0.09616374969482422
Batch 64/64 loss: -0.07731229066848755
Epoch 199  Train loss: -0.08186262051264445  Val loss: -0.020160750220321707
Epoch 200
-------------------------------
Batch 1/64 loss: -0.10044324398040771
Batch 2/64 loss: -0.10827940702438354
Batch 3/64 loss: -0.09202533960342407
Batch 4/64 loss: -0.08892613649368286
Batch 5/64 loss: -0.09393984079360962
Batch 6/64 loss: -0.06074035167694092
Batch 7/64 loss: -0.07344740629196167
Batch 8/64 loss: -0.10525375604629517
Batch 9/64 loss: -0.08694183826446533
Batch 10/64 loss: -0.09540361166000366
Batch 11/64 loss: -0.09313923120498657
Batch 12/64 loss: -0.07727354764938354
Batch 13/64 loss: -0.0928717851638794
Batch 14/64 loss: -0.07366704940795898
Batch 15/64 loss: -0.09732383489608765
Batch 16/64 loss: -0.10578352212905884
Batch 17/64 loss: -0.10834693908691406
Batch 18/64 loss: -0.07071584463119507
Batch 19/64 loss: -0.0708245038986206
Batch 20/64 loss: -0.09241199493408203
Batch 21/64 loss: -0.09528028964996338
Batch 22/64 loss: -0.1009870171546936
Batch 23/64 loss: -0.07623487710952759
Batch 24/64 loss: -0.08856749534606934
Batch 25/64 loss: -0.110323965549469
Batch 26/64 loss: -0.09549260139465332
Batch 27/64 loss: -0.09123319387435913
Batch 28/64 loss: -0.0919308066368103
Batch 29/64 loss: -0.045571744441986084
Batch 30/64 loss: -0.09378921985626221
Batch 31/64 loss: -0.11013680696487427
Batch 32/64 loss: -0.08720886707305908
Batch 33/64 loss: -0.0910719633102417
Batch 34/64 loss: -0.08510798215866089
Batch 35/64 loss: -0.10229450464248657
Batch 36/64 loss: -0.0733867883682251
Batch 37/64 loss: -0.08780544996261597
Batch 38/64 loss: -0.0928492546081543
Batch 39/64 loss: -0.08375358581542969
Batch 40/64 loss: -0.0791885256767273
Batch 41/64 loss: -0.06352651119232178
Batch 42/64 loss: -0.059314191341400146
Batch 43/64 loss: -0.10483700037002563
Batch 44/64 loss: -0.07141101360321045
Batch 45/64 loss: -0.08624672889709473
Batch 46/64 loss: -0.10104501247406006
Batch 47/64 loss: -0.08126837015151978
Batch 48/64 loss: -0.0735403299331665
Batch 49/64 loss: -0.08011209964752197
Batch 50/64 loss: -0.09116154909133911
Batch 51/64 loss: -0.07922905683517456
Batch 52/64 loss: -0.07067131996154785
Batch 53/64 loss: -0.06469148397445679
Batch 54/64 loss: -0.08451211452484131
Batch 55/64 loss: -0.09666508436203003
Batch 56/64 loss: -0.07546025514602661
Batch 57/64 loss: -0.1011967658996582
Batch 58/64 loss: -0.08225458860397339
Batch 59/64 loss: -0.06249642372131348
Batch 60/64 loss: -0.08806949853897095
Batch 61/64 loss: -0.06241023540496826
Batch 62/64 loss: -0.06144207715988159
Batch 63/64 loss: -0.08112740516662598
Batch 64/64 loss: -0.0687914490699768
Epoch 200  Train loss: -0.08540010709388583  Val loss: -0.019198568006561383
Epoch 201
-------------------------------
Batch 1/64 loss: -0.07353699207305908
Batch 2/64 loss: -0.09768080711364746
Batch 3/64 loss: -0.09055769443511963
Batch 4/64 loss: -0.08832281827926636
Batch 5/64 loss: -0.0917290449142456
Batch 6/64 loss: -0.10425519943237305
Batch 7/64 loss: -0.09706437587738037
Batch 8/64 loss: -0.0737953782081604
Batch 9/64 loss: -0.0954321026802063
Batch 10/64 loss: -0.09235167503356934
Batch 11/64 loss: -0.1047789454460144
Batch 12/64 loss: -0.09226202964782715
Batch 13/64 loss: -0.08484739065170288
Batch 14/64 loss: -0.08091604709625244
Batch 15/64 loss: -0.08992242813110352
Batch 16/64 loss: -0.0868070125579834
Batch 17/64 loss: -0.09629940986633301
Batch 18/64 loss: -0.10856640338897705
Batch 19/64 loss: -0.0631873607635498
Batch 20/64 loss: -0.1002277135848999
Batch 21/64 loss: -0.08498966693878174
Batch 22/64 loss: -0.10666525363922119
Batch 23/64 loss: -0.10788166522979736
Batch 24/64 loss: -0.07638287544250488
Batch 25/64 loss: -0.10655516386032104
Batch 26/64 loss: -0.09758937358856201
Batch 27/64 loss: -0.09327960014343262
Batch 28/64 loss: -0.0684317946434021
Batch 29/64 loss: -0.08333742618560791
Batch 30/64 loss: -0.08444780111312866
Batch 31/64 loss: -0.08632051944732666
Batch 32/64 loss: -0.09919703006744385
Batch 33/64 loss: -0.0419771671295166
Batch 34/64 loss: -0.09071141481399536
Batch 35/64 loss: -0.06568759679794312
Batch 36/64 loss: -0.08194327354431152
Batch 37/64 loss: -0.10563778877258301
Batch 38/64 loss: -0.10152095556259155
Batch 39/64 loss: -0.08357203006744385
Batch 40/64 loss: -0.08062618970870972
Batch 41/64 loss: -0.07983344793319702
Batch 42/64 loss: -0.09368026256561279
Batch 43/64 loss: -0.057938575744628906
Batch 44/64 loss: -0.0710364580154419
Batch 45/64 loss: -0.08506381511688232
Batch 46/64 loss: -0.08306986093521118
Batch 47/64 loss: -0.07307887077331543
Batch 48/64 loss: -0.08118987083435059
Batch 49/64 loss: -0.09200882911682129
Batch 50/64 loss: -0.07725214958190918
Batch 51/64 loss: -0.09679996967315674
Batch 52/64 loss: -0.08376961946487427
Batch 53/64 loss: -0.07052993774414062
Batch 54/64 loss: -0.07336002588272095
Batch 55/64 loss: -0.08859962224960327
Batch 56/64 loss: -0.07803928852081299
Batch 57/64 loss: -0.08624005317687988
Batch 58/64 loss: -0.08922767639160156
Batch 59/64 loss: -0.08186465501785278
Batch 60/64 loss: -0.07848000526428223
Batch 61/64 loss: -0.07925105094909668
Batch 62/64 loss: -0.09152549505233765
Batch 63/64 loss: -0.07049709558486938
Batch 64/64 loss: -0.06374341249465942
Epoch 201  Train loss: -0.08579513115041396  Val loss: -0.015722063398852792
Epoch 202
-------------------------------
Batch 1/64 loss: -0.08315348625183105
Batch 2/64 loss: -0.08400315046310425
Batch 3/64 loss: -0.07749193906784058
Batch 4/64 loss: -0.047065675258636475
Batch 5/64 loss: -0.09693294763565063
Batch 6/64 loss: -0.08485674858093262
Batch 7/64 loss: -0.05766558647155762
Batch 8/64 loss: -0.10236769914627075
Batch 9/64 loss: -0.08874624967575073
Batch 10/64 loss: -0.0957607626914978
Batch 11/64 loss: -0.06601464748382568
Batch 12/64 loss: -0.08879357576370239
Batch 13/64 loss: -0.10304051637649536
Batch 14/64 loss: -0.08720886707305908
Batch 15/64 loss: -0.09096038341522217
Batch 16/64 loss: -0.08156311511993408
Batch 17/64 loss: -0.0968008041381836
Batch 18/64 loss: -0.07659584283828735
Batch 19/64 loss: -0.06721889972686768
Batch 20/64 loss: -0.09093809127807617
Batch 21/64 loss: -0.08791828155517578
Batch 22/64 loss: -0.08987826108932495
Batch 23/64 loss: -0.08985620737075806
Batch 24/64 loss: -0.06460869312286377
Batch 25/64 loss: -0.10581791400909424
Batch 26/64 loss: -0.0834922194480896
Batch 27/64 loss: -0.08978080749511719
Batch 28/64 loss: -0.09322106838226318
Batch 29/64 loss: -0.10294437408447266
Batch 30/64 loss: -0.0931626558303833
Batch 31/64 loss: -0.10468602180480957
Batch 32/64 loss: -0.08490711450576782
Batch 33/64 loss: -0.09723597764968872
Batch 34/64 loss: -0.08342671394348145
Batch 35/64 loss: -0.09546768665313721
Batch 36/64 loss: -0.0955502986907959
Batch 37/64 loss: -0.0804373025894165
Batch 38/64 loss: -0.08290499448776245
Batch 39/64 loss: -0.08433884382247925
Batch 40/64 loss: -0.09106850624084473
Batch 41/64 loss: -0.0658835768699646
Batch 42/64 loss: -0.07642555236816406
Batch 43/64 loss: -0.10024821758270264
Batch 44/64 loss: -0.08173859119415283
Batch 45/64 loss: -0.08789288997650146
Batch 46/64 loss: -0.09126430749893188
Batch 47/64 loss: -0.07347917556762695
Batch 48/64 loss: -0.08518493175506592
Batch 49/64 loss: -0.08764833211898804
Batch 50/64 loss: -0.08981996774673462
Batch 51/64 loss: -0.09144330024719238
Batch 52/64 loss: -0.056986868381500244
Batch 53/64 loss: -0.08137154579162598
Batch 54/64 loss: -0.07962840795516968
Batch 55/64 loss: -0.09599602222442627
Batch 56/64 loss: -0.07683110237121582
Batch 57/64 loss: -0.0746685266494751
Batch 58/64 loss: -0.07261139154434204
Batch 59/64 loss: -0.07997393608093262
Batch 60/64 loss: -0.0704641342163086
Batch 61/64 loss: -0.0567857027053833
Batch 62/64 loss: -0.09922122955322266
Batch 63/64 loss: -0.09136962890625
Batch 64/64 loss: -0.09122061729431152
Epoch 202  Train loss: -0.08475663895700492  Val loss: -0.02065870503789371
Epoch 203
-------------------------------
Batch 1/64 loss: -0.07667040824890137
Batch 2/64 loss: -0.10249000787734985
Batch 3/64 loss: -0.09774637222290039
Batch 4/64 loss: -0.09030675888061523
Batch 5/64 loss: -0.08748692274093628
Batch 6/64 loss: -0.0838923454284668
Batch 7/64 loss: -0.09975922107696533
Batch 8/64 loss: -0.07452934980392456
Batch 9/64 loss: -0.1003449559211731
Batch 10/64 loss: -0.10036313533782959
Batch 11/64 loss: -0.1019471287727356
Batch 12/64 loss: -0.09332585334777832
Batch 13/64 loss: -0.10559850931167603
Batch 14/64 loss: -0.07971155643463135
Batch 15/64 loss: -0.073905348777771
Batch 16/64 loss: -0.08706921339035034
Batch 17/64 loss: -0.09253138303756714
Batch 18/64 loss: -0.10371983051300049
Batch 19/64 loss: -0.0925980806350708
Batch 20/64 loss: -0.09593057632446289
Batch 21/64 loss: -0.07053625583648682
Batch 22/64 loss: -0.10192739963531494
Batch 23/64 loss: -0.09644412994384766
Batch 24/64 loss: -0.08860719203948975
Batch 25/64 loss: -0.09745174646377563
Batch 26/64 loss: -0.09713321924209595
Batch 27/64 loss: -0.07713693380355835
Batch 28/64 loss: -0.06200718879699707
Batch 29/64 loss: -0.08653819561004639
Batch 30/64 loss: -0.08433270454406738
Batch 31/64 loss: -0.08483284711837769
Batch 32/64 loss: -0.09227263927459717
Batch 33/64 loss: -0.05979281663894653
Batch 34/64 loss: -0.09237909317016602
Batch 35/64 loss: -0.09750336408615112
Batch 36/64 loss: -0.0773017406463623
Batch 37/64 loss: -0.10372453927993774
Batch 38/64 loss: -0.09530997276306152
Batch 39/64 loss: -0.07906806468963623
Batch 40/64 loss: -0.077953040599823
Batch 41/64 loss: -0.05659604072570801
Batch 42/64 loss: -0.050269246101379395
Batch 43/64 loss: -0.0796273946762085
Batch 44/64 loss: -0.08413320779800415
Batch 45/64 loss: -0.09342479705810547
Batch 46/64 loss: -0.08538198471069336
Batch 47/64 loss: -0.09039515256881714
Batch 48/64 loss: -0.09884166717529297
Batch 49/64 loss: -0.050068199634552
Batch 50/64 loss: -0.0883474349975586
Batch 51/64 loss: -0.07807481288909912
Batch 52/64 loss: -0.0814092755317688
Batch 53/64 loss: -0.09366393089294434
Batch 54/64 loss: -0.07942676544189453
Batch 55/64 loss: -0.07361721992492676
Batch 56/64 loss: -0.06552135944366455
Batch 57/64 loss: -0.0774846076965332
Batch 58/64 loss: -0.08437013626098633
Batch 59/64 loss: -0.08480459451675415
Batch 60/64 loss: -0.07131725549697876
Batch 61/64 loss: -0.09453988075256348
Batch 62/64 loss: -0.08248686790466309
Batch 63/64 loss: -0.0832027792930603
Batch 64/64 loss: -0.09524059295654297
Epoch 203  Train loss: -0.08565670742708094  Val loss: -0.019217361699264895
Epoch 204
-------------------------------
Batch 1/64 loss: -0.0773271918296814
Batch 2/64 loss: -0.07705670595169067
Batch 3/64 loss: -0.09122329950332642
Batch 4/64 loss: -0.10464119911193848
Batch 5/64 loss: -0.09276705980300903
Batch 6/64 loss: -0.07495647668838501
Batch 7/64 loss: -0.08561164140701294
Batch 8/64 loss: -0.09456390142440796
Batch 9/64 loss: -0.07860523462295532
Batch 10/64 loss: -0.09221601486206055
Batch 11/64 loss: -0.10531735420227051
Batch 12/64 loss: -0.11086392402648926
Batch 13/64 loss: -0.05268019437789917
Batch 14/64 loss: -0.08743244409561157
Batch 15/64 loss: -0.09402275085449219
Batch 16/64 loss: -0.07197344303131104
Batch 17/64 loss: -0.09233832359313965
Batch 18/64 loss: -0.08306372165679932
Batch 19/64 loss: -0.10073179006576538
Batch 20/64 loss: -0.09030437469482422
Batch 21/64 loss: -0.09940379858016968
Batch 22/64 loss: -0.09006208181381226
Batch 23/64 loss: -0.07359433174133301
Batch 24/64 loss: -0.08570170402526855
Batch 25/64 loss: -0.0908706784248352
Batch 26/64 loss: -0.07275927066802979
Batch 27/64 loss: -0.09536093473434448
Batch 28/64 loss: -0.10045039653778076
Batch 29/64 loss: -0.09458112716674805
Batch 30/64 loss: -0.08235734701156616
Batch 31/64 loss: -0.08719557523727417
Batch 32/64 loss: -0.0783689022064209
Batch 33/64 loss: -0.09041768312454224
Batch 34/64 loss: -0.0900888442993164
Batch 35/64 loss: -0.09735655784606934
Batch 36/64 loss: -0.09021192789077759
Batch 37/64 loss: -0.08922702074050903
Batch 38/64 loss: -0.07799035310745239
Batch 39/64 loss: -0.05192667245864868
Batch 40/64 loss: -0.06377941370010376
Batch 41/64 loss: -0.09080016613006592
Batch 42/64 loss: -0.06888025999069214
Batch 43/64 loss: -0.08191859722137451
Batch 44/64 loss: -0.10286176204681396
Batch 45/64 loss: -0.08405208587646484
Batch 46/64 loss: -0.06691431999206543
Batch 47/64 loss: -0.08352035284042358
Batch 48/64 loss: -0.07887327671051025
Batch 49/64 loss: -0.08922767639160156
Batch 50/64 loss: -0.09761953353881836
Batch 51/64 loss: -0.0696377158164978
Batch 52/64 loss: -0.07514262199401855
Batch 53/64 loss: -0.07330054044723511
Batch 54/64 loss: -0.09097719192504883
Batch 55/64 loss: -0.0785449743270874
Batch 56/64 loss: -0.06542438268661499
Batch 57/64 loss: -0.09039223194122314
Batch 58/64 loss: -0.07119607925415039
Batch 59/64 loss: -0.07033669948577881
Batch 60/64 loss: -0.08474618196487427
Batch 61/64 loss: -0.083335280418396
Batch 62/64 loss: -0.07830685377120972
Batch 63/64 loss: -0.08553451299667358
Batch 64/64 loss: -0.05444210767745972
Epoch 204  Train loss: -0.08410629875519697  Val loss: -0.01473916846861954
Epoch 205
-------------------------------
Batch 1/64 loss: -0.0891653299331665
Batch 2/64 loss: -0.08281075954437256
Batch 3/64 loss: -0.08967876434326172
Batch 4/64 loss: -0.07791531085968018
Batch 5/64 loss: -0.09271734952926636
Batch 6/64 loss: -0.08522379398345947
Batch 7/64 loss: -0.09023594856262207
Batch 8/64 loss: -0.09300351142883301
Batch 9/64 loss: -0.07689440250396729
Batch 10/64 loss: -0.06579279899597168
Batch 11/64 loss: -0.09059488773345947
Batch 12/64 loss: -0.07626092433929443
Batch 13/64 loss: -0.10379719734191895
Batch 14/64 loss: -0.09893953800201416
Batch 15/64 loss: -0.0845983624458313
Batch 16/64 loss: -0.07653427124023438
Batch 17/64 loss: -0.10052478313446045
Batch 18/64 loss: -0.09124070405960083
Batch 19/64 loss: -0.09565699100494385
Batch 20/64 loss: -0.09595167636871338
Batch 21/64 loss: -0.09799391031265259
Batch 22/64 loss: -0.09342563152313232
Batch 23/64 loss: -0.08516991138458252
Batch 24/64 loss: -0.08842045068740845
Batch 25/64 loss: -0.09001541137695312
Batch 26/64 loss: -0.09037721157073975
Batch 27/64 loss: -0.1053321361541748
Batch 28/64 loss: -0.10211539268493652
Batch 29/64 loss: -0.09859603643417358
Batch 30/64 loss: -0.08535021543502808
Batch 31/64 loss: -0.09022969007492065
Batch 32/64 loss: -0.07180577516555786
Batch 33/64 loss: -0.08457028865814209
Batch 34/64 loss: -0.09097915887832642
Batch 35/64 loss: -0.09242963790893555
Batch 36/64 loss: -0.08907490968704224
Batch 37/64 loss: -0.0766216516494751
Batch 38/64 loss: -0.09143543243408203
Batch 39/64 loss: -0.08004564046859741
Batch 40/64 loss: -0.08150368928909302
Batch 41/64 loss: -0.07251185178756714
Batch 42/64 loss: -0.06514263153076172
Batch 43/64 loss: -0.0976637601852417
Batch 44/64 loss: -0.08125251531600952
Batch 45/64 loss: -0.08426123857498169
Batch 46/64 loss: -0.07425463199615479
Batch 47/64 loss: -0.09295761585235596
Batch 48/64 loss: -0.10649025440216064
Batch 49/64 loss: -0.06190800666809082
Batch 50/64 loss: -0.06708306074142456
Batch 51/64 loss: -0.09320789575576782
Batch 52/64 loss: -0.09270972013473511
Batch 53/64 loss: -0.09474802017211914
Batch 54/64 loss: -0.09284371137619019
Batch 55/64 loss: -0.09108579158782959
Batch 56/64 loss: -0.08315306901931763
Batch 57/64 loss: -0.07647150754928589
Batch 58/64 loss: -0.06317883729934692
Batch 59/64 loss: -0.08080542087554932
Batch 60/64 loss: -0.07223987579345703
Batch 61/64 loss: -0.0643463134765625
Batch 62/64 loss: -0.08337545394897461
Batch 63/64 loss: -0.07891136407852173
Batch 64/64 loss: -0.09609705209732056
Epoch 205  Train loss: -0.08605027175417133  Val loss: -0.019592149970457724
Epoch 206
-------------------------------
Batch 1/64 loss: -0.10262733697891235
Batch 2/64 loss: -0.0861659049987793
Batch 3/64 loss: -0.08283162117004395
Batch 4/64 loss: -0.10138064622879028
Batch 5/64 loss: -0.0910576581954956
Batch 6/64 loss: -0.07737189531326294
Batch 7/64 loss: -0.08280700445175171
Batch 8/64 loss: -0.07351803779602051
Batch 9/64 loss: -0.08963125944137573
Batch 10/64 loss: -0.07862496376037598
Batch 11/64 loss: -0.0979732871055603
Batch 12/64 loss: -0.09343761205673218
Batch 13/64 loss: -0.09012264013290405
Batch 14/64 loss: -0.11158376932144165
Batch 15/64 loss: -0.09582996368408203
Batch 16/64 loss: -0.0978008508682251
Batch 17/64 loss: -0.0946354866027832
Batch 18/64 loss: -0.08595836162567139
Batch 19/64 loss: -0.09727746248245239
Batch 20/64 loss: -0.10592126846313477
Batch 21/64 loss: -0.1090591549873352
Batch 22/64 loss: -0.07761579751968384
Batch 23/64 loss: -0.08750587701797485
Batch 24/64 loss: -0.09229099750518799
Batch 25/64 loss: -0.09297347068786621
Batch 26/64 loss: -0.09098905324935913
Batch 27/64 loss: -0.10124433040618896
Batch 28/64 loss: -0.09389281272888184
Batch 29/64 loss: -0.10231137275695801
Batch 30/64 loss: -0.09948122501373291
Batch 31/64 loss: -0.0852699875831604
Batch 32/64 loss: -0.0985790491104126
Batch 33/64 loss: -0.08151429891586304
Batch 34/64 loss: -0.07595038414001465
Batch 35/64 loss: -0.09720182418823242
Batch 36/64 loss: -0.05933576822280884
Batch 37/64 loss: -0.08499407768249512
Batch 38/64 loss: -0.09805405139923096
Batch 39/64 loss: -0.06810665130615234
Batch 40/64 loss: -0.0980067253112793
Batch 41/64 loss: -0.09028482437133789
Batch 42/64 loss: -0.10124772787094116
Batch 43/64 loss: -0.10347449779510498
Batch 44/64 loss: -0.0798797607421875
Batch 45/64 loss: -0.08927607536315918
Batch 46/64 loss: -0.08515322208404541
Batch 47/64 loss: -0.09044045209884644
Batch 48/64 loss: -0.07763278484344482
Batch 49/64 loss: -0.10361284017562866
Batch 50/64 loss: -0.09148186445236206
Batch 51/64 loss: -0.08999556303024292
Batch 52/64 loss: -0.07361483573913574
Batch 53/64 loss: -0.09108215570449829
Batch 54/64 loss: -0.10220754146575928
Batch 55/64 loss: -0.08849626779556274
Batch 56/64 loss: -0.08173781633377075
Batch 57/64 loss: -0.10639846324920654
Batch 58/64 loss: -0.08216536045074463
Batch 59/64 loss: -0.07539117336273193
Batch 60/64 loss: -0.06840550899505615
Batch 61/64 loss: -0.06784319877624512
Batch 62/64 loss: -0.07339620590209961
Batch 63/64 loss: -0.08044058084487915
Batch 64/64 loss: -0.10694742202758789
Epoch 206  Train loss: -0.08948710946475759  Val loss: -0.01845562990588421
Epoch 207
-------------------------------
Batch 1/64 loss: -0.10065990686416626
Batch 2/64 loss: -0.06845277547836304
Batch 3/64 loss: -0.10106158256530762
Batch 4/64 loss: -0.09420466423034668
Batch 5/64 loss: -0.08335411548614502
Batch 6/64 loss: -0.08888214826583862
Batch 7/64 loss: -0.08557736873626709
Batch 8/64 loss: -0.09128284454345703
Batch 9/64 loss: -0.09148812294006348
Batch 10/64 loss: -0.07701712846755981
Batch 11/64 loss: -0.08876609802246094
Batch 12/64 loss: -0.08705627918243408
Batch 13/64 loss: -0.08597773313522339
Batch 14/64 loss: -0.1004440188407898
Batch 15/64 loss: -0.08811259269714355
Batch 16/64 loss: -0.06900644302368164
Batch 17/64 loss: -0.09960818290710449
Batch 18/64 loss: -0.08671420812606812
Batch 19/64 loss: -0.08324766159057617
Batch 20/64 loss: -0.07218402624130249
Batch 21/64 loss: -0.08985865116119385
Batch 22/64 loss: -0.08409929275512695
Batch 23/64 loss: -0.08150249719619751
Batch 24/64 loss: -0.09597408771514893
Batch 25/64 loss: -0.07840144634246826
Batch 26/64 loss: -0.08157044649124146
Batch 27/64 loss: -0.07920604944229126
Batch 28/64 loss: -0.07444965839385986
Batch 29/64 loss: -0.08443605899810791
Batch 30/64 loss: -0.09637176990509033
Batch 31/64 loss: -0.08737671375274658
Batch 32/64 loss: -0.09789979457855225
Batch 33/64 loss: -0.05164295434951782
Batch 34/64 loss: -0.09438979625701904
Batch 35/64 loss: -0.08555006980895996
Batch 36/64 loss: -0.09417730569839478
Batch 37/64 loss: -0.10199505090713501
Batch 38/64 loss: -0.07841682434082031
Batch 39/64 loss: -0.09993505477905273
Batch 40/64 loss: -0.09476524591445923
Batch 41/64 loss: -0.0834013819694519
Batch 42/64 loss: -0.08248591423034668
Batch 43/64 loss: -0.0786629319190979
Batch 44/64 loss: -0.09179902076721191
Batch 45/64 loss: -0.07967591285705566
Batch 46/64 loss: -0.09019827842712402
Batch 47/64 loss: -0.09016764163970947
Batch 48/64 loss: -0.08988684415817261
Batch 49/64 loss: -0.1024966835975647
Batch 50/64 loss: -0.10803532600402832
Batch 51/64 loss: -0.08352959156036377
Batch 52/64 loss: -0.06461411714553833
Batch 53/64 loss: -0.07402992248535156
Batch 54/64 loss: -0.08201771974563599
Batch 55/64 loss: -0.08814215660095215
Batch 56/64 loss: -0.09073734283447266
Batch 57/64 loss: -0.10286599397659302
Batch 58/64 loss: -0.10050606727600098
Batch 59/64 loss: -0.0945974588394165
Batch 60/64 loss: -0.08816581964492798
Batch 61/64 loss: -0.06917577981948853
Batch 62/64 loss: -0.08660435676574707
Batch 63/64 loss: -0.09343963861465454
Batch 64/64 loss: -0.06746327877044678
Epoch 207  Train loss: -0.08691686321707333  Val loss: -0.02019412329106806
Epoch 208
-------------------------------
Batch 1/64 loss: -0.09923410415649414
Batch 2/64 loss: -0.08523792028427124
Batch 3/64 loss: -0.08558058738708496
Batch 4/64 loss: -0.08743715286254883
Batch 5/64 loss: -0.08743685483932495
Batch 6/64 loss: -0.08651489019393921
Batch 7/64 loss: -0.09658616781234741
Batch 8/64 loss: -0.07444852590560913
Batch 9/64 loss: -0.0814778208732605
Batch 10/64 loss: -0.09049361944198608
Batch 11/64 loss: -0.09590262174606323
Batch 12/64 loss: -0.1012042760848999
Batch 13/64 loss: -0.1040576696395874
Batch 14/64 loss: -0.0875808596611023
Batch 15/64 loss: -0.08821910619735718
Batch 16/64 loss: -0.10370922088623047
Batch 17/64 loss: -0.09923702478408813
Batch 18/64 loss: -0.08310985565185547
Batch 19/64 loss: -0.08917248249053955
Batch 20/64 loss: -0.07525873184204102
Batch 21/64 loss: -0.09173989295959473
Batch 22/64 loss: -0.09422695636749268
Batch 23/64 loss: -0.08975684642791748
Batch 24/64 loss: -0.08937835693359375
Batch 25/64 loss: -0.10720038414001465
Batch 26/64 loss: -0.09383320808410645
Batch 27/64 loss: -0.09422516822814941
Batch 28/64 loss: -0.10126608610153198
Batch 29/64 loss: -0.07034718990325928
Batch 30/64 loss: -0.08486253023147583
Batch 31/64 loss: -0.10476052761077881
Batch 32/64 loss: -0.09689652919769287
Batch 33/64 loss: -0.0941573977470398
Batch 34/64 loss: -0.1001853346824646
Batch 35/64 loss: -0.08748120069503784
Batch 36/64 loss: -0.05986553430557251
Batch 37/64 loss: -0.0955578088760376
Batch 38/64 loss: -0.09308761358261108
Batch 39/64 loss: -0.07950174808502197
Batch 40/64 loss: -0.07880133390426636
Batch 41/64 loss: -0.0711219310760498
Batch 42/64 loss: -0.08579802513122559
Batch 43/64 loss: -0.07573413848876953
Batch 44/64 loss: -0.06865739822387695
Batch 45/64 loss: -0.08500421047210693
Batch 46/64 loss: -0.09260571002960205
Batch 47/64 loss: -0.1041712760925293
Batch 48/64 loss: -0.08811789751052856
Batch 49/64 loss: -0.09939706325531006
Batch 50/64 loss: -0.09845256805419922
Batch 51/64 loss: -0.06490880250930786
Batch 52/64 loss: -0.08048248291015625
Batch 53/64 loss: -0.10925674438476562
Batch 54/64 loss: -0.09946399927139282
Batch 55/64 loss: -0.09095686674118042
Batch 56/64 loss: -0.07544982433319092
Batch 57/64 loss: -0.0569767951965332
Batch 58/64 loss: -0.07219809293746948
Batch 59/64 loss: -0.0905182957649231
Batch 60/64 loss: -0.0755276083946228
Batch 61/64 loss: -0.07177746295928955
Batch 62/64 loss: -0.10323941707611084
Batch 63/64 loss: -0.07878667116165161
Batch 64/64 loss: -0.07583391666412354
Epoch 208  Train loss: -0.0879138801612106  Val loss: -0.018533188657662302
Epoch 209
-------------------------------
Batch 1/64 loss: -0.08938676118850708
Batch 2/64 loss: -0.09654456377029419
Batch 3/64 loss: -0.08046209812164307
Batch 4/64 loss: -0.10388672351837158
Batch 5/64 loss: -0.09018850326538086
Batch 6/64 loss: -0.1046794056892395
Batch 7/64 loss: -0.07335096597671509
Batch 8/64 loss: -0.10195022821426392
Batch 9/64 loss: -0.11541485786437988
Batch 10/64 loss: -0.10303276777267456
Batch 11/64 loss: -0.1153264045715332
Batch 12/64 loss: -0.09261512756347656
Batch 13/64 loss: -0.09908920526504517
Batch 14/64 loss: -0.09222769737243652
Batch 15/64 loss: -0.07982075214385986
Batch 16/64 loss: -0.08690857887268066
Batch 17/64 loss: -0.09587365388870239
Batch 18/64 loss: -0.08949512243270874
Batch 19/64 loss: -0.08619391918182373
Batch 20/64 loss: -0.08521157503128052
Batch 21/64 loss: -0.08680832386016846
Batch 22/64 loss: -0.09831440448760986
Batch 23/64 loss: -0.10062432289123535
Batch 24/64 loss: -0.09541666507720947
Batch 25/64 loss: -0.07854759693145752
Batch 26/64 loss: -0.10478895902633667
Batch 27/64 loss: -0.09443140029907227
Batch 28/64 loss: -0.0746011734008789
Batch 29/64 loss: -0.08915001153945923
Batch 30/64 loss: -0.08985549211502075
Batch 31/64 loss: -0.09362953901290894
Batch 32/64 loss: -0.0721132755279541
Batch 33/64 loss: -0.09553027153015137
Batch 34/64 loss: -0.08535975217819214
Batch 35/64 loss: -0.10116803646087646
Batch 36/64 loss: -0.10061979293823242
Batch 37/64 loss: -0.11585593223571777
Batch 38/64 loss: -0.07389438152313232
Batch 39/64 loss: -0.08057552576065063
Batch 40/64 loss: -0.0965200662612915
Batch 41/64 loss: -0.09869569540023804
Batch 42/64 loss: -0.09319674968719482
Batch 43/64 loss: -0.08524549007415771
Batch 44/64 loss: -0.07477915287017822
Batch 45/64 loss: -0.1042097806930542
Batch 46/64 loss: -0.07941967248916626
Batch 47/64 loss: -0.0792573094367981
Batch 48/64 loss: -0.09312540292739868
Batch 49/64 loss: -0.09337282180786133
Batch 50/64 loss: -0.06734049320220947
Batch 51/64 loss: -0.0946584939956665
Batch 52/64 loss: -0.058634042739868164
Batch 53/64 loss: -0.10104548931121826
Batch 54/64 loss: -0.09471595287322998
Batch 55/64 loss: -0.08076721429824829
Batch 56/64 loss: -0.09105587005615234
Batch 57/64 loss: -0.08362996578216553
Batch 58/64 loss: -0.08282756805419922
Batch 59/64 loss: -0.08788782358169556
Batch 60/64 loss: -0.07721364498138428
Batch 61/64 loss: -0.07341885566711426
Batch 62/64 loss: -0.07744204998016357
Batch 63/64 loss: -0.07219856977462769
Batch 64/64 loss: -0.07065582275390625
Epoch 209  Train loss: -0.08951519704332539  Val loss: -0.02457274112504782
Saving best model, epoch: 209
Epoch 210
-------------------------------
Batch 1/64 loss: -0.09534585475921631
Batch 2/64 loss: -0.10715198516845703
Batch 3/64 loss: -0.11472171545028687
Batch 4/64 loss: -0.08268290758132935
Batch 5/64 loss: -0.09564250707626343
Batch 6/64 loss: -0.07964098453521729
Batch 7/64 loss: -0.08411037921905518
Batch 8/64 loss: -0.09274274110794067
Batch 9/64 loss: -0.10606914758682251
Batch 10/64 loss: -0.09748506546020508
Batch 11/64 loss: -0.09964478015899658
Batch 12/64 loss: -0.09889280796051025
Batch 13/64 loss: -0.0932767391204834
Batch 14/64 loss: -0.09275728464126587
Batch 15/64 loss: -0.09137457609176636
Batch 16/64 loss: -0.11245697736740112
Batch 17/64 loss: -0.09442383050918579
Batch 18/64 loss: -0.08976727724075317
Batch 19/64 loss: -0.09549510478973389
Batch 20/64 loss: -0.08755850791931152
Batch 21/64 loss: -0.09077823162078857
Batch 22/64 loss: -0.0963258147239685
Batch 23/64 loss: -0.08864331245422363
Batch 24/64 loss: -0.08933717012405396
Batch 25/64 loss: -0.09318572282791138
Batch 26/64 loss: -0.08915144205093384
Batch 27/64 loss: -0.09013062715530396
Batch 28/64 loss: -0.07743406295776367
Batch 29/64 loss: -0.0779346227645874
Batch 30/64 loss: -0.10180878639221191
Batch 31/64 loss: -0.10618680715560913
Batch 32/64 loss: -0.07924044132232666
Batch 33/64 loss: -0.10110074281692505
Batch 34/64 loss: -0.05914795398712158
Batch 35/64 loss: -0.10441422462463379
Batch 36/64 loss: -0.09793210029602051
Batch 37/64 loss: -0.09718608856201172
Batch 38/64 loss: -0.09188675880432129
Batch 39/64 loss: -0.09454822540283203
Batch 40/64 loss: -0.0968790054321289
Batch 41/64 loss: -0.0735938549041748
Batch 42/64 loss: -0.09163045883178711
Batch 43/64 loss: -0.10606741905212402
Batch 44/64 loss: -0.07880377769470215
Batch 45/64 loss: -0.0681447982788086
Batch 46/64 loss: -0.09006190299987793
Batch 47/64 loss: -0.07971781492233276
Batch 48/64 loss: -0.11153280735015869
Batch 49/64 loss: -0.10665524005889893
Batch 50/64 loss: -0.09746360778808594
Batch 51/64 loss: -0.07240486145019531
Batch 52/64 loss: -0.06710511445999146
Batch 53/64 loss: -0.09221434593200684
Batch 54/64 loss: -0.08672171831130981
Batch 55/64 loss: -0.06861883401870728
Batch 56/64 loss: -0.08820164203643799
Batch 57/64 loss: -0.07074284553527832
Batch 58/64 loss: -0.08746659755706787
Batch 59/64 loss: -0.11174678802490234
Batch 60/64 loss: -0.08076059818267822
Batch 61/64 loss: -0.07144045829772949
Batch 62/64 loss: -0.06994229555130005
Batch 63/64 loss: -0.07124555110931396
Batch 64/64 loss: -0.08824867010116577
Epoch 210  Train loss: -0.09008569647284115  Val loss: -0.02201898237274275
Epoch 211
-------------------------------
Batch 1/64 loss: -0.09547334909439087
Batch 2/64 loss: -0.11114144325256348
Batch 3/64 loss: -0.06683129072189331
Batch 4/64 loss: -0.09674978256225586
Batch 5/64 loss: -0.08275115489959717
Batch 6/64 loss: -0.0952487587928772
Batch 7/64 loss: -0.0679582953453064
Batch 8/64 loss: -0.09606868028640747
Batch 9/64 loss: -0.08926838636398315
Batch 10/64 loss: -0.04667949676513672
Batch 11/64 loss: -0.09601408243179321
Batch 12/64 loss: -0.07678103446960449
Batch 13/64 loss: -0.09439235925674438
Batch 14/64 loss: -0.07151412963867188
Batch 15/64 loss: -0.08168429136276245
Batch 16/64 loss: -0.09928625822067261
Batch 17/64 loss: -0.10521376132965088
Batch 18/64 loss: -0.07539516687393188
Batch 19/64 loss: -0.09187334775924683
Batch 20/64 loss: -0.07357966899871826
Batch 21/64 loss: -0.09749799966812134
Batch 22/64 loss: -0.08801120519638062
Batch 23/64 loss: -0.09637075662612915
Batch 24/64 loss: -0.06634366512298584
Batch 25/64 loss: -0.08465290069580078
Batch 26/64 loss: -0.09498739242553711
Batch 27/64 loss: -0.08525621891021729
Batch 28/64 loss: -0.07637286186218262
Batch 29/64 loss: -0.09782689809799194
Batch 30/64 loss: -0.09060299396514893
Batch 31/64 loss: -0.10612225532531738
Batch 32/64 loss: -0.1077193021774292
Batch 33/64 loss: -0.08207660913467407
Batch 34/64 loss: -0.08545279502868652
Batch 35/64 loss: -0.10583287477493286
Batch 36/64 loss: -0.07947474718093872
Batch 37/64 loss: -0.08809340000152588
Batch 38/64 loss: -0.10347390174865723
Batch 39/64 loss: -0.08034878969192505
Batch 40/64 loss: -0.07828319072723389
Batch 41/64 loss: -0.07489967346191406
Batch 42/64 loss: -0.09710162878036499
Batch 43/64 loss: -0.08586013317108154
Batch 44/64 loss: -0.08723586797714233
Batch 45/64 loss: -0.09370017051696777
Batch 46/64 loss: -0.10366261005401611
Batch 47/64 loss: -0.10673677921295166
Batch 48/64 loss: -0.09170037508010864
Batch 49/64 loss: -0.09144461154937744
Batch 50/64 loss: -0.09550899267196655
Batch 51/64 loss: -0.10080301761627197
Batch 52/64 loss: -0.0946764349937439
Batch 53/64 loss: -0.1121569275856018
Batch 54/64 loss: -0.10066145658493042
Batch 55/64 loss: -0.10266995429992676
Batch 56/64 loss: -0.09406602382659912
Batch 57/64 loss: -0.1029481291770935
Batch 58/64 loss: -0.09228551387786865
Batch 59/64 loss: -0.07087743282318115
Batch 60/64 loss: -0.08042395114898682
Batch 61/64 loss: -0.08132743835449219
Batch 62/64 loss: -0.0926588773727417
Batch 63/64 loss: -0.08995223045349121
Batch 64/64 loss: -0.09249567985534668
Epoch 211  Train loss: -0.08974800764345656  Val loss: -0.022011936530214816
Epoch 212
-------------------------------
Batch 1/64 loss: -0.09541124105453491
Batch 2/64 loss: -0.07204127311706543
Batch 3/64 loss: -0.09479498863220215
Batch 4/64 loss: -0.09144318103790283
Batch 5/64 loss: -0.06208920478820801
Batch 6/64 loss: -0.0833815336227417
Batch 7/64 loss: -0.10053688287734985
Batch 8/64 loss: -0.11106163263320923
Batch 9/64 loss: -0.10280883312225342
Batch 10/64 loss: -0.07228845357894897
Batch 11/64 loss: -0.07107555866241455
Batch 12/64 loss: -0.11024540662765503
Batch 13/64 loss: -0.11549383401870728
Batch 14/64 loss: -0.1034204363822937
Batch 15/64 loss: -0.10738801956176758
Batch 16/64 loss: -0.09113585948944092
Batch 17/64 loss: -0.08356732130050659
Batch 18/64 loss: -0.09316855669021606
Batch 19/64 loss: -0.07099741697311401
Batch 20/64 loss: -0.09422975778579712
Batch 21/64 loss: -0.0944281816482544
Batch 22/64 loss: -0.07772618532180786
Batch 23/64 loss: -0.10791003704071045
Batch 24/64 loss: -0.09219682216644287
Batch 25/64 loss: -0.09064388275146484
Batch 26/64 loss: -0.10443335771560669
Batch 27/64 loss: -0.08605074882507324
Batch 28/64 loss: -0.11712837219238281
Batch 29/64 loss: -0.0817025899887085
Batch 30/64 loss: -0.10977983474731445
Batch 31/64 loss: -0.09610098600387573
Batch 32/64 loss: -0.09945642948150635
Batch 33/64 loss: -0.11077892780303955
Batch 34/64 loss: -0.11302858591079712
Batch 35/64 loss: -0.07397961616516113
Batch 36/64 loss: -0.10207206010818481
Batch 37/64 loss: -0.07436525821685791
Batch 38/64 loss: -0.066489577293396
Batch 39/64 loss: -0.07426238059997559
Batch 40/64 loss: -0.08836579322814941
Batch 41/64 loss: -0.10674357414245605
Batch 42/64 loss: -0.08821380138397217
Batch 43/64 loss: -0.10059404373168945
Batch 44/64 loss: -0.10310781002044678
Batch 45/64 loss: -0.1052255630493164
Batch 46/64 loss: -0.10338389873504639
Batch 47/64 loss: -0.10708355903625488
Batch 48/64 loss: -0.09964978694915771
Batch 49/64 loss: -0.081973135471344
Batch 50/64 loss: -0.09238117933273315
Batch 51/64 loss: -0.08884072303771973
Batch 52/64 loss: -0.06562042236328125
Batch 53/64 loss: -0.07892870903015137
Batch 54/64 loss: -0.10025161504745483
Batch 55/64 loss: -0.06546741724014282
Batch 56/64 loss: -0.08116567134857178
Batch 57/64 loss: -0.08363372087478638
Batch 58/64 loss: -0.08775150775909424
Batch 59/64 loss: -0.07702946662902832
Batch 60/64 loss: -0.08440369367599487
Batch 61/64 loss: -0.09082239866256714
Batch 62/64 loss: -0.09149646759033203
Batch 63/64 loss: -0.10094064474105835
Batch 64/64 loss: -0.07585209608078003
Epoch 212  Train loss: -0.09143650040907018  Val loss: -0.02005189943969045
Epoch 213
-------------------------------
Batch 1/64 loss: -0.101662278175354
Batch 2/64 loss: -0.08511483669281006
Batch 3/64 loss: -0.06580460071563721
Batch 4/64 loss: -0.06793880462646484
Batch 5/64 loss: -0.08460426330566406
Batch 6/64 loss: -0.08428287506103516
Batch 7/64 loss: -0.05029350519180298
Batch 8/64 loss: -0.08371645212173462
Batch 9/64 loss: -0.09645706415176392
Batch 10/64 loss: -0.09373867511749268
Batch 11/64 loss: -0.10624474287033081
Batch 12/64 loss: -0.09245342016220093
Batch 13/64 loss: -0.09513813257217407
Batch 14/64 loss: -0.08125686645507812
Batch 15/64 loss: -0.09885412454605103
Batch 16/64 loss: -0.09149765968322754
Batch 17/64 loss: -0.08763724565505981
Batch 18/64 loss: -0.10066342353820801
Batch 19/64 loss: -0.06373584270477295
Batch 20/64 loss: -0.09682583808898926
Batch 21/64 loss: -0.09720802307128906
Batch 22/64 loss: -0.10357892513275146
Batch 23/64 loss: -0.08923995494842529
Batch 24/64 loss: -0.08116137981414795
Batch 25/64 loss: -0.08084654808044434
Batch 26/64 loss: -0.10267192125320435
Batch 27/64 loss: -0.09911859035491943
Batch 28/64 loss: -0.10287237167358398
Batch 29/64 loss: -0.10906702280044556
Batch 30/64 loss: -0.08128303289413452
Batch 31/64 loss: -0.10540390014648438
Batch 32/64 loss: -0.09184163808822632
Batch 33/64 loss: -0.10726511478424072
Batch 34/64 loss: -0.08169674873352051
Batch 35/64 loss: -0.07612287998199463
Batch 36/64 loss: -0.08816730976104736
Batch 37/64 loss: -0.0964708924293518
Batch 38/64 loss: -0.09960305690765381
Batch 39/64 loss: -0.09812945127487183
Batch 40/64 loss: -0.10364437103271484
Batch 41/64 loss: -0.09989255666732788
Batch 42/64 loss: -0.07451844215393066
Batch 43/64 loss: -0.092171311378479
Batch 44/64 loss: -0.10592377185821533
Batch 45/64 loss: -0.08610415458679199
Batch 46/64 loss: -0.08440929651260376
Batch 47/64 loss: -0.10793125629425049
Batch 48/64 loss: -0.09092497825622559
Batch 49/64 loss: -0.08937782049179077
Batch 50/64 loss: -0.10728049278259277
Batch 51/64 loss: -0.09233278036117554
Batch 52/64 loss: -0.07368665933609009
Batch 53/64 loss: -0.06960827112197876
Batch 54/64 loss: -0.08260595798492432
Batch 55/64 loss: -0.09260201454162598
Batch 56/64 loss: -0.09877908229827881
Batch 57/64 loss: -0.09294307231903076
Batch 58/64 loss: -0.10329872369766235
Batch 59/64 loss: -0.06932127475738525
Batch 60/64 loss: -0.07649374008178711
Batch 61/64 loss: -0.08686542510986328
Batch 62/64 loss: -0.10727834701538086
Batch 63/64 loss: -0.09098917245864868
Batch 64/64 loss: -0.09068500995635986
Epoch 213  Train loss: -0.09045750814325669  Val loss: -0.02285353076417012
Epoch 214
-------------------------------
Batch 1/64 loss: -0.10197114944458008
Batch 2/64 loss: -0.11208873987197876
Batch 3/64 loss: -0.08926981687545776
Batch 4/64 loss: -0.09746849536895752
Batch 5/64 loss: -0.09379971027374268
Batch 6/64 loss: -0.10470902919769287
Batch 7/64 loss: -0.08131921291351318
Batch 8/64 loss: -0.10071825981140137
Batch 9/64 loss: -0.09196734428405762
Batch 10/64 loss: -0.09047365188598633
Batch 11/64 loss: -0.1042330265045166
Batch 12/64 loss: -0.11956948041915894
Batch 13/64 loss: -0.08708053827285767
Batch 14/64 loss: -0.09841102361679077
Batch 15/64 loss: -0.06509852409362793
Batch 16/64 loss: -0.08604753017425537
Batch 17/64 loss: -0.11350357532501221
Batch 18/64 loss: -0.0949409008026123
Batch 19/64 loss: -0.11089187860488892
Batch 20/64 loss: -0.09651792049407959
Batch 21/64 loss: -0.09858834743499756
Batch 22/64 loss: -0.09496009349822998
Batch 23/64 loss: -0.07364839315414429
Batch 24/64 loss: -0.10663259029388428
Batch 25/64 loss: -0.09174871444702148
Batch 26/64 loss: -0.10249871015548706
Batch 27/64 loss: -0.08946150541305542
Batch 28/64 loss: -0.1035308837890625
Batch 29/64 loss: -0.10655027627944946
Batch 30/64 loss: -0.08800756931304932
Batch 31/64 loss: -0.09023392200469971
Batch 32/64 loss: -0.10351181030273438
Batch 33/64 loss: -0.10796505212783813
Batch 34/64 loss: -0.07511228322982788
Batch 35/64 loss: -0.105865478515625
Batch 36/64 loss: -0.07882827520370483
Batch 37/64 loss: -0.09562009572982788
Batch 38/64 loss: -0.08827918767929077
Batch 39/64 loss: -0.10107815265655518
Batch 40/64 loss: -0.09059131145477295
Batch 41/64 loss: -0.07551467418670654
Batch 42/64 loss: -0.10381925106048584
Batch 43/64 loss: -0.09150320291519165
Batch 44/64 loss: -0.0807194709777832
Batch 45/64 loss: -0.09223198890686035
Batch 46/64 loss: -0.09561020135879517
Batch 47/64 loss: -0.09070956707000732
Batch 48/64 loss: -0.06538188457489014
Batch 49/64 loss: -0.09336662292480469
Batch 50/64 loss: -0.09011054039001465
Batch 51/64 loss: -0.08547401428222656
Batch 52/64 loss: -0.08875125646591187
Batch 53/64 loss: -0.08340466022491455
Batch 54/64 loss: -0.0679289698600769
Batch 55/64 loss: -0.09571540355682373
Batch 56/64 loss: -0.09798932075500488
Batch 57/64 loss: -0.09883248805999756
Batch 58/64 loss: -0.08591550588607788
Batch 59/64 loss: -0.09452098608016968
Batch 60/64 loss: -0.09501850605010986
Batch 61/64 loss: -0.09045487642288208
Batch 62/64 loss: -0.07600408792495728
Batch 63/64 loss: -0.08245038986206055
Batch 64/64 loss: -0.09766918420791626
Epoch 214  Train loss: -0.09297995637444889  Val loss: -0.019043062355919804
Epoch 215
-------------------------------
Batch 1/64 loss: -0.10311144590377808
Batch 2/64 loss: -0.07349616289138794
Batch 3/64 loss: -0.08684879541397095
Batch 4/64 loss: -0.09718698263168335
Batch 5/64 loss: -0.08760958909988403
Batch 6/64 loss: -0.09375721216201782
Batch 7/64 loss: -0.06655555963516235
Batch 8/64 loss: -0.10789328813552856
Batch 9/64 loss: -0.09341627359390259
Batch 10/64 loss: -0.11338013410568237
Batch 11/64 loss: -0.10680222511291504
Batch 12/64 loss: -0.08269411325454712
Batch 13/64 loss: -0.08532965183258057
Batch 14/64 loss: -0.09939461946487427
Batch 15/64 loss: -0.10613417625427246
Batch 16/64 loss: -0.10234785079956055
Batch 17/64 loss: -0.10162603855133057
Batch 18/64 loss: -0.10080832242965698
Batch 19/64 loss: -0.09743624925613403
Batch 20/64 loss: -0.07784426212310791
Batch 21/64 loss: -0.08577901124954224
Batch 22/64 loss: -0.08510911464691162
Batch 23/64 loss: -0.09821963310241699
Batch 24/64 loss: -0.10739195346832275
Batch 25/64 loss: -0.09695243835449219
Batch 26/64 loss: -0.0983852744102478
Batch 27/64 loss: -0.1058884859085083
Batch 28/64 loss: -0.10533487796783447
Batch 29/64 loss: -0.11051380634307861
Batch 30/64 loss: -0.08261984586715698
Batch 31/64 loss: -0.0981549620628357
Batch 32/64 loss: -0.09905761480331421
Batch 33/64 loss: -0.04766488075256348
Batch 34/64 loss: -0.06612348556518555
Batch 35/64 loss: -0.09280955791473389
Batch 36/64 loss: -0.09496170282363892
Batch 37/64 loss: -0.11476922035217285
Batch 38/64 loss: -0.08282101154327393
Batch 39/64 loss: -0.1093149185180664
Batch 40/64 loss: -0.07700741291046143
Batch 41/64 loss: -0.10111284255981445
Batch 42/64 loss: -0.09660911560058594
Batch 43/64 loss: -0.09225231409072876
Batch 44/64 loss: -0.07933610677719116
Batch 45/64 loss: -0.08513802289962769
Batch 46/64 loss: -0.09531819820404053
Batch 47/64 loss: -0.1091160774230957
Batch 48/64 loss: -0.06687939167022705
Batch 49/64 loss: -0.1022372841835022
Batch 50/64 loss: -0.09635543823242188
Batch 51/64 loss: -0.0829010009765625
Batch 52/64 loss: -0.09756731986999512
Batch 53/64 loss: -0.11339545249938965
Batch 54/64 loss: -0.09873509407043457
Batch 55/64 loss: -0.08647197484970093
Batch 56/64 loss: -0.08142662048339844
Batch 57/64 loss: -0.08700054883956909
Batch 58/64 loss: -0.07889145612716675
Batch 59/64 loss: -0.08554935455322266
Batch 60/64 loss: -0.08220148086547852
Batch 61/64 loss: -0.10194027423858643
Batch 62/64 loss: -0.0974385142326355
Batch 63/64 loss: -0.10617923736572266
Batch 64/64 loss: -0.09854841232299805
Epoch 215  Train loss: -0.0931845739776013  Val loss: -0.021294667958393947
Epoch 216
-------------------------------
Batch 1/64 loss: -0.08591973781585693
Batch 2/64 loss: -0.11507999897003174
Batch 3/64 loss: -0.11952477693557739
Batch 4/64 loss: -0.10132336616516113
Batch 5/64 loss: -0.11172562837600708
Batch 6/64 loss: -0.09645682573318481
Batch 7/64 loss: -0.10888516902923584
Batch 8/64 loss: -0.10941892862319946
Batch 9/64 loss: -0.09476709365844727
Batch 10/64 loss: -0.10507506132125854
Batch 11/64 loss: -0.10450565814971924
Batch 12/64 loss: -0.10034322738647461
Batch 13/64 loss: -0.07346409559249878
Batch 14/64 loss: -0.10076206922531128
Batch 15/64 loss: -0.08931136131286621
Batch 16/64 loss: -0.06905442476272583
Batch 17/64 loss: -0.10526382923126221
Batch 18/64 loss: -0.09617698192596436
Batch 19/64 loss: -0.07599961757659912
Batch 20/64 loss: -0.070065438747406
Batch 21/64 loss: -0.08409887552261353
Batch 22/64 loss: -0.10964536666870117
Batch 23/64 loss: -0.08583354949951172
Batch 24/64 loss: -0.09427356719970703
Batch 25/64 loss: -0.09712958335876465
Batch 26/64 loss: -0.0891181230545044
Batch 27/64 loss: -0.08862888813018799
Batch 28/64 loss: -0.10345566272735596
Batch 29/64 loss: -0.10328197479248047
Batch 30/64 loss: -0.1057477593421936
Batch 31/64 loss: -0.09980189800262451
Batch 32/64 loss: -0.09816431999206543
Batch 33/64 loss: -0.09044504165649414
Batch 34/64 loss: -0.09080111980438232
Batch 35/64 loss: -0.07090729475021362
Batch 36/64 loss: -0.10771811008453369
Batch 37/64 loss: -0.0837356448173523
Batch 38/64 loss: -0.09540283679962158
Batch 39/64 loss: -0.09967672824859619
Batch 40/64 loss: -0.09399920701980591
Batch 41/64 loss: -0.10009968280792236
Batch 42/64 loss: -0.09559845924377441
Batch 43/64 loss: -0.08599019050598145
Batch 44/64 loss: -0.07921463251113892
Batch 45/64 loss: -0.09960263967514038
Batch 46/64 loss: -0.07269597053527832
Batch 47/64 loss: -0.0763545036315918
Batch 48/64 loss: -0.07208466529846191
Batch 49/64 loss: -0.08291447162628174
Batch 50/64 loss: -0.08910590410232544
Batch 51/64 loss: -0.07869207859039307
Batch 52/64 loss: -0.08574378490447998
Batch 53/64 loss: -0.09437668323516846
Batch 54/64 loss: -0.10559028387069702
Batch 55/64 loss: -0.10609251260757446
Batch 56/64 loss: -0.06405770778656006
Batch 57/64 loss: -0.0609438419342041
Batch 58/64 loss: -0.07677656412124634
Batch 59/64 loss: -0.1003158688545227
Batch 60/64 loss: -0.08878862857818604
Batch 61/64 loss: -0.07976359128952026
Batch 62/64 loss: -0.10992950201034546
Batch 63/64 loss: -0.08985710144042969
Batch 64/64 loss: -0.10680145025253296
Epoch 216  Train loss: -0.09254398743311563  Val loss: -0.020221584031672003
Epoch 217
-------------------------------
Batch 1/64 loss: -0.10202664136886597
Batch 2/64 loss: -0.10962587594985962
Batch 3/64 loss: -0.08156311511993408
Batch 4/64 loss: -0.10289299488067627
Batch 5/64 loss: -0.07770252227783203
Batch 6/64 loss: -0.0844760537147522
Batch 7/64 loss: -0.10785698890686035
Batch 8/64 loss: -0.08805322647094727
Batch 9/64 loss: -0.0808563232421875
Batch 10/64 loss: -0.09017163515090942
Batch 11/64 loss: -0.09102809429168701
Batch 12/64 loss: -0.09087145328521729
Batch 13/64 loss: -0.08146202564239502
Batch 14/64 loss: -0.08393537998199463
Batch 15/64 loss: -0.11201071739196777
Batch 16/64 loss: -0.07661789655685425
Batch 17/64 loss: -0.09124737977981567
Batch 18/64 loss: -0.11596214771270752
Batch 19/64 loss: -0.1121562123298645
Batch 20/64 loss: -0.07692527770996094
Batch 21/64 loss: -0.09858942031860352
Batch 22/64 loss: -0.08695125579833984
Batch 23/64 loss: -0.07798707485198975
Batch 24/64 loss: -0.11070460081100464
Batch 25/64 loss: -0.10648906230926514
Batch 26/64 loss: -0.10490274429321289
Batch 27/64 loss: -0.10780900716781616
Batch 28/64 loss: -0.10037893056869507
Batch 29/64 loss: -0.08275973796844482
Batch 30/64 loss: -0.10173076391220093
Batch 31/64 loss: -0.10226726531982422
Batch 32/64 loss: -0.09705513715744019
Batch 33/64 loss: -0.0969536304473877
Batch 34/64 loss: -0.08645421266555786
Batch 35/64 loss: -0.10926717519760132
Batch 36/64 loss: -0.09807366132736206
Batch 37/64 loss: -0.09274023771286011
Batch 38/64 loss: -0.07474517822265625
Batch 39/64 loss: -0.08359330892562866
Batch 40/64 loss: -0.08617538213729858
Batch 41/64 loss: -0.09190630912780762
Batch 42/64 loss: -0.11530226469039917
Batch 43/64 loss: -0.11122184991836548
Batch 44/64 loss: -0.09617865085601807
Batch 45/64 loss: -0.08429932594299316
Batch 46/64 loss: -0.09866940975189209
Batch 47/64 loss: -0.10809326171875
Batch 48/64 loss: -0.09076273441314697
Batch 49/64 loss: -0.0953947901725769
Batch 50/64 loss: -0.09182858467102051
Batch 51/64 loss: -0.08545929193496704
Batch 52/64 loss: -0.09428989887237549
Batch 53/64 loss: -0.09289544820785522
Batch 54/64 loss: -0.09199857711791992
Batch 55/64 loss: -0.09372460842132568
Batch 56/64 loss: -0.1125863790512085
Batch 57/64 loss: -0.07525843381881714
Batch 58/64 loss: -0.0984451174736023
Batch 59/64 loss: -0.09717857837677002
Batch 60/64 loss: -0.09048449993133545
Batch 61/64 loss: -0.08847790956497192
Batch 62/64 loss: -0.10245895385742188
Batch 63/64 loss: -0.11627781391143799
Batch 64/64 loss: -0.09779137372970581
Epoch 217  Train loss: -0.0950526426820194  Val loss: -0.02458640042039537
Saving best model, epoch: 217
Epoch 218
-------------------------------
Batch 1/64 loss: -0.0776599645614624
Batch 2/64 loss: -0.11438429355621338
Batch 3/64 loss: -0.1084069013595581
Batch 4/64 loss: -0.10434424877166748
Batch 5/64 loss: -0.08914721012115479
Batch 6/64 loss: -0.10468846559524536
Batch 7/64 loss: -0.0948333740234375
Batch 8/64 loss: -0.11573731899261475
Batch 9/64 loss: -0.12018918991088867
Batch 10/64 loss: -0.10553872585296631
Batch 11/64 loss: -0.10141575336456299
Batch 12/64 loss: -0.10502499341964722
Batch 13/64 loss: -0.0719001293182373
Batch 14/64 loss: -0.09207391738891602
Batch 15/64 loss: -0.10268241167068481
Batch 16/64 loss: -0.08035844564437866
Batch 17/64 loss: -0.09847939014434814
Batch 18/64 loss: -0.08031105995178223
Batch 19/64 loss: -0.0811200737953186
Batch 20/64 loss: -0.12166476249694824
Batch 21/64 loss: -0.0838131308555603
Batch 22/64 loss: -0.09551817178726196
Batch 23/64 loss: -0.0836440920829773
Batch 24/64 loss: -0.09095823764801025
Batch 25/64 loss: -0.10329586267471313
Batch 26/64 loss: -0.09233129024505615
Batch 27/64 loss: -0.07613420486450195
Batch 28/64 loss: -0.06926482915878296
Batch 29/64 loss: -0.09373468160629272
Batch 30/64 loss: -0.07551515102386475
Batch 31/64 loss: -0.09397649765014648
Batch 32/64 loss: -0.1027182936668396
Batch 33/64 loss: -0.07526236772537231
Batch 34/64 loss: -0.07740825414657593
Batch 35/64 loss: -0.10273861885070801
Batch 36/64 loss: -0.09196323156356812
Batch 37/64 loss: -0.09660470485687256
Batch 38/64 loss: -0.08943235874176025
Batch 39/64 loss: -0.06328445672988892
Batch 40/64 loss: -0.09709823131561279
Batch 41/64 loss: -0.06112384796142578
Batch 42/64 loss: -0.08149516582489014
Batch 43/64 loss: -0.08017420768737793
Batch 44/64 loss: -0.09213113784790039
Batch 45/64 loss: -0.0872495174407959
Batch 46/64 loss: -0.09061145782470703
Batch 47/64 loss: -0.10031837224960327
Batch 48/64 loss: -0.10217058658599854
Batch 49/64 loss: -0.07776778936386108
Batch 50/64 loss: -0.08922058343887329
Batch 51/64 loss: -0.08379101753234863
Batch 52/64 loss: -0.08364903926849365
Batch 53/64 loss: -0.08513689041137695
Batch 54/64 loss: -0.09481018781661987
Batch 55/64 loss: -0.10748857259750366
Batch 56/64 loss: -0.08801156282424927
Batch 57/64 loss: -0.11037129163742065
Batch 58/64 loss: -0.09850180149078369
Batch 59/64 loss: -0.08685755729675293
Batch 60/64 loss: -0.0874243974685669
Batch 61/64 loss: -0.08417809009552002
Batch 62/64 loss: -0.09393519163131714
Batch 63/64 loss: -0.10593962669372559
Batch 64/64 loss: -0.089641273021698
Epoch 218  Train loss: -0.09198817482181623  Val loss: -0.020427401328004923
Epoch 219
-------------------------------
Batch 1/64 loss: -0.08830124139785767
Batch 2/64 loss: -0.09688621759414673
Batch 3/64 loss: -0.09978735446929932
Batch 4/64 loss: -0.09281492233276367
Batch 5/64 loss: -0.10116326808929443
Batch 6/64 loss: -0.09873783588409424
Batch 7/64 loss: -0.100680410861969
Batch 8/64 loss: -0.09199249744415283
Batch 9/64 loss: -0.0667993426322937
Batch 10/64 loss: -0.09409195184707642
Batch 11/64 loss: -0.11173838376998901
Batch 12/64 loss: -0.09770452976226807
Batch 13/64 loss: -0.08462166786193848
Batch 14/64 loss: -0.09095108509063721
Batch 15/64 loss: -0.10649973154067993
Batch 16/64 loss: -0.09678006172180176
Batch 17/64 loss: -0.08862161636352539
Batch 18/64 loss: -0.0965726375579834
Batch 19/64 loss: -0.09977716207504272
Batch 20/64 loss: -0.08779579401016235
Batch 21/64 loss: -0.06989741325378418
Batch 22/64 loss: -0.08924227952957153
Batch 23/64 loss: -0.06892180442810059
Batch 24/64 loss: -0.0748634934425354
Batch 25/64 loss: -0.09050548076629639
Batch 26/64 loss: -0.05843585729598999
Batch 27/64 loss: -0.08158546686172485
Batch 28/64 loss: -0.09565716981887817
Batch 29/64 loss: -0.08956629037857056
Batch 30/64 loss: -0.10234969854354858
Batch 31/64 loss: -0.08299380540847778
Batch 32/64 loss: -0.10900241136550903
Batch 33/64 loss: -0.08555114269256592
Batch 34/64 loss: -0.07771199941635132
Batch 35/64 loss: -0.09402376413345337
Batch 36/64 loss: -0.08436894416809082
Batch 37/64 loss: -0.10080426931381226
Batch 38/64 loss: -0.09640026092529297
Batch 39/64 loss: -0.09307277202606201
Batch 40/64 loss: -0.09683477878570557
Batch 41/64 loss: -0.08079779148101807
Batch 42/64 loss: -0.08153742551803589
Batch 43/64 loss: -0.11408042907714844
Batch 44/64 loss: -0.06690311431884766
Batch 45/64 loss: -0.08909720182418823
Batch 46/64 loss: -0.0777820348739624
Batch 47/64 loss: -0.09683865308761597
Batch 48/64 loss: -0.08813893795013428
Batch 49/64 loss: -0.1099621057510376
Batch 50/64 loss: -0.09776163101196289
Batch 51/64 loss: -0.09444117546081543
Batch 52/64 loss: -0.08422309160232544
Batch 53/64 loss: -0.0901116132736206
Batch 54/64 loss: -0.07702517509460449
Batch 55/64 loss: -0.10719043016433716
Batch 56/64 loss: -0.10177075862884521
Batch 57/64 loss: -0.10627186298370361
Batch 58/64 loss: -0.09778439998626709
Batch 59/64 loss: -0.09852486848831177
Batch 60/64 loss: -0.09689807891845703
Batch 61/64 loss: -0.08376455307006836
Batch 62/64 loss: -0.09118080139160156
Batch 63/64 loss: -0.08193445205688477
Batch 64/64 loss: -0.10066300630569458
Epoch 219  Train loss: -0.09135097498987235  Val loss: -0.023474990502255887
Epoch 220
-------------------------------
Batch 1/64 loss: -0.10916471481323242
Batch 2/64 loss: -0.10336440801620483
Batch 3/64 loss: -0.10135656595230103
Batch 4/64 loss: -0.09617078304290771
Batch 5/64 loss: -0.0750352144241333
Batch 6/64 loss: -0.09359568357467651
Batch 7/64 loss: -0.06334489583969116
Batch 8/64 loss: -0.11574524641036987
Batch 9/64 loss: -0.10606569051742554
Batch 10/64 loss: -0.09648382663726807
Batch 11/64 loss: -0.09528541564941406
Batch 12/64 loss: -0.09578937292098999
Batch 13/64 loss: -0.11049425601959229
Batch 14/64 loss: -0.10700130462646484
Batch 15/64 loss: -0.0924844741821289
Batch 16/64 loss: -0.08268296718597412
Batch 17/64 loss: -0.10212588310241699
Batch 18/64 loss: -0.08587974309921265
Batch 19/64 loss: -0.09445273876190186
Batch 20/64 loss: -0.1008598804473877
Batch 21/64 loss: -0.10248273611068726
Batch 22/64 loss: -0.08125036954879761
Batch 23/64 loss: -0.10205399990081787
Batch 24/64 loss: -0.08592742681503296
Batch 25/64 loss: -0.10008913278579712
Batch 26/64 loss: -0.0840761661529541
Batch 27/64 loss: -0.07149004936218262
Batch 28/64 loss: -0.10318315029144287
Batch 29/64 loss: -0.09808266162872314
Batch 30/64 loss: -0.10300564765930176
Batch 31/64 loss: -0.07762640714645386
Batch 32/64 loss: -0.08920001983642578
Batch 33/64 loss: -0.09706610441207886
Batch 34/64 loss: -0.09077543020248413
Batch 35/64 loss: -0.10143119096755981
Batch 36/64 loss: -0.1135326623916626
Batch 37/64 loss: -0.10545533895492554
Batch 38/64 loss: -0.07224172353744507
Batch 39/64 loss: -0.10674703121185303
Batch 40/64 loss: -0.09560942649841309
Batch 41/64 loss: -0.1091839075088501
Batch 42/64 loss: -0.10277193784713745
Batch 43/64 loss: -0.07892340421676636
Batch 44/64 loss: -0.10519325733184814
Batch 45/64 loss: -0.10291719436645508
Batch 46/64 loss: -0.10144037008285522
Batch 47/64 loss: -0.0991135835647583
Batch 48/64 loss: -0.09968072175979614
Batch 49/64 loss: -0.09462970495223999
Batch 50/64 loss: -0.08294481039047241
Batch 51/64 loss: -0.10953176021575928
Batch 52/64 loss: -0.09266877174377441
Batch 53/64 loss: -0.09457284212112427
Batch 54/64 loss: -0.09319692850112915
Batch 55/64 loss: -0.08356696367263794
Batch 56/64 loss: -0.0859408974647522
Batch 57/64 loss: -0.0916290283203125
Batch 58/64 loss: -0.10643976926803589
Batch 59/64 loss: -0.09920096397399902
Batch 60/64 loss: -0.10825854539871216
Batch 61/64 loss: -0.09923869371414185
Batch 62/64 loss: -0.08471238613128662
Batch 63/64 loss: -0.08072739839553833
Batch 64/64 loss: -0.08054602146148682
Epoch 220  Train loss: -0.0953035780027801  Val loss: -0.017623164399792647
Epoch 221
-------------------------------
Batch 1/64 loss: -0.08853012323379517
Batch 2/64 loss: -0.09943616390228271
Batch 3/64 loss: -0.10209906101226807
Batch 4/64 loss: -0.10606527328491211
Batch 5/64 loss: -0.11166369915008545
Batch 6/64 loss: -0.09731829166412354
Batch 7/64 loss: -0.10783922672271729
Batch 8/64 loss: -0.08942234516143799
Batch 9/64 loss: -0.10959506034851074
Batch 10/64 loss: -0.09216201305389404
Batch 11/64 loss: -0.10588151216506958
Batch 12/64 loss: -0.10072487592697144
Batch 13/64 loss: -0.09214657545089722
Batch 14/64 loss: -0.10843920707702637
Batch 15/64 loss: -0.09974372386932373
Batch 16/64 loss: -0.08713704347610474
Batch 17/64 loss: -0.09006941318511963
Batch 18/64 loss: -0.08952367305755615
Batch 19/64 loss: -0.10596191883087158
Batch 20/64 loss: -0.08513426780700684
Batch 21/64 loss: -0.10793715715408325
Batch 22/64 loss: -0.09658986330032349
Batch 23/64 loss: -0.10132193565368652
Batch 24/64 loss: -0.1123616099357605
Batch 25/64 loss: -0.060995399951934814
Batch 26/64 loss: -0.1023150086402893
Batch 27/64 loss: -0.11862021684646606
Batch 28/64 loss: -0.1093515157699585
Batch 29/64 loss: -0.11397457122802734
Batch 30/64 loss: -0.09609407186508179
Batch 31/64 loss: -0.1087225079536438
Batch 32/64 loss: -0.10156184434890747
Batch 33/64 loss: -0.09292346239089966
Batch 34/64 loss: -0.07768243551254272
Batch 35/64 loss: -0.09652596712112427
Batch 36/64 loss: -0.0875319242477417
Batch 37/64 loss: -0.08406335115432739
Batch 38/64 loss: -0.08630508184432983
Batch 39/64 loss: -0.10259586572647095
Batch 40/64 loss: -0.09106647968292236
Batch 41/64 loss: -0.09388864040374756
Batch 42/64 loss: -0.09426885843276978
Batch 43/64 loss: -0.0888059139251709
Batch 44/64 loss: -0.10041999816894531
Batch 45/64 loss: -0.08096641302108765
Batch 46/64 loss: -0.09437960386276245
Batch 47/64 loss: -0.10940974950790405
Batch 48/64 loss: -0.10784554481506348
Batch 49/64 loss: -0.09852063655853271
Batch 50/64 loss: -0.10050427913665771
Batch 51/64 loss: -0.07531994581222534
Batch 52/64 loss: -0.0964500904083252
Batch 53/64 loss: -0.09920459985733032
Batch 54/64 loss: -0.10284149646759033
Batch 55/64 loss: -0.09835207462310791
Batch 56/64 loss: -0.09396344423294067
Batch 57/64 loss: -0.09536945819854736
Batch 58/64 loss: -0.09530472755432129
Batch 59/64 loss: -0.097353994846344
Batch 60/64 loss: -0.10727584362030029
Batch 61/64 loss: -0.10885196924209595
Batch 62/64 loss: -0.10177159309387207
Batch 63/64 loss: -0.08548861742019653
Batch 64/64 loss: -0.09318190813064575
Epoch 221  Train loss: -0.09747259079241286  Val loss: -0.020114638346577018
Epoch 222
-------------------------------
Batch 1/64 loss: -0.10187745094299316
Batch 2/64 loss: -0.08057975769042969
Batch 3/64 loss: -0.1039080023765564
Batch 4/64 loss: -0.10096549987792969
Batch 5/64 loss: -0.09507137537002563
Batch 6/64 loss: -0.09014147520065308
Batch 7/64 loss: -0.08010762929916382
Batch 8/64 loss: -0.11874008178710938
Batch 9/64 loss: -0.09764808416366577
Batch 10/64 loss: -0.07577729225158691
Batch 11/64 loss: -0.0978766679763794
Batch 12/64 loss: -0.10799211263656616
Batch 13/64 loss: -0.09637415409088135
Batch 14/64 loss: -0.1096200942993164
Batch 15/64 loss: -0.08224684000015259
Batch 16/64 loss: -0.05489230155944824
Batch 17/64 loss: -0.11483407020568848
Batch 18/64 loss: -0.0919533371925354
Batch 19/64 loss: -0.09789633750915527
Batch 20/64 loss: -0.1121721863746643
Batch 21/64 loss: -0.10722315311431885
Batch 22/64 loss: -0.10172712802886963
Batch 23/64 loss: -0.11432534456253052
Batch 24/64 loss: -0.08241879940032959
Batch 25/64 loss: -0.09418720006942749
Batch 26/64 loss: -0.09511363506317139
Batch 27/64 loss: -0.08756709098815918
Batch 28/64 loss: -0.11169284582138062
Batch 29/64 loss: -0.09879636764526367
Batch 30/64 loss: -0.08693921566009521
Batch 31/64 loss: -0.1032252311706543
Batch 32/64 loss: -0.1084282398223877
Batch 33/64 loss: -0.08768558502197266
Batch 34/64 loss: -0.11682069301605225
Batch 35/64 loss: -0.09658610820770264
Batch 36/64 loss: -0.08955347537994385
Batch 37/64 loss: -0.10970807075500488
Batch 38/64 loss: -0.09901982545852661
Batch 39/64 loss: -0.09014487266540527
Batch 40/64 loss: -0.11738193035125732
Batch 41/64 loss: -0.09819763898849487
Batch 42/64 loss: -0.0885629653930664
Batch 43/64 loss: -0.10213881731033325
Batch 44/64 loss: -0.09023356437683105
Batch 45/64 loss: -0.09899109601974487
Batch 46/64 loss: -0.10861116647720337
Batch 47/64 loss: -0.0880969762802124
Batch 48/64 loss: -0.10560441017150879
Batch 49/64 loss: -0.10793709754943848
Batch 50/64 loss: -0.0631188154220581
Batch 51/64 loss: -0.09026592969894409
Batch 52/64 loss: -0.09835350513458252
Batch 53/64 loss: -0.11113631725311279
Batch 54/64 loss: -0.10075211524963379
Batch 55/64 loss: -0.09490561485290527
Batch 56/64 loss: -0.09287327527999878
Batch 57/64 loss: -0.097636878490448
Batch 58/64 loss: -0.1000261902809143
Batch 59/64 loss: -0.1145099401473999
Batch 60/64 loss: -0.08481425046920776
Batch 61/64 loss: -0.0994066596031189
Batch 62/64 loss: -0.10579800605773926
Batch 63/64 loss: -0.11202597618103027
Batch 64/64 loss: -0.10213494300842285
Epoch 222  Train loss: -0.09787949767767214  Val loss: -0.023988555387123345
Epoch 223
-------------------------------
Batch 1/64 loss: -0.11302065849304199
Batch 2/64 loss: -0.11273491382598877
Batch 3/64 loss: -0.1110985279083252
Batch 4/64 loss: -0.1066049337387085
Batch 5/64 loss: -0.10993075370788574
Batch 6/64 loss: -0.1090589165687561
Batch 7/64 loss: -0.10656601190567017
Batch 8/64 loss: -0.09537005424499512
Batch 9/64 loss: -0.10070890188217163
Batch 10/64 loss: -0.10658901929855347
Batch 11/64 loss: -0.09549742937088013
Batch 12/64 loss: -0.10153406858444214
Batch 13/64 loss: -0.11473846435546875
Batch 14/64 loss: -0.10088032484054565
Batch 15/64 loss: -0.10848647356033325
Batch 16/64 loss: -0.08865392208099365
Batch 17/64 loss: -0.10491448640823364
Batch 18/64 loss: -0.09020745754241943
Batch 19/64 loss: -0.10354453325271606
Batch 20/64 loss: -0.10797280073165894
Batch 21/64 loss: -0.0889286994934082
Batch 22/64 loss: -0.08693444728851318
Batch 23/64 loss: -0.11200255155563354
Batch 24/64 loss: -0.09128355979919434
Batch 25/64 loss: -0.10290330648422241
Batch 26/64 loss: -0.11587828397750854
Batch 27/64 loss: -0.10402554273605347
Batch 28/64 loss: -0.09824413061141968
Batch 29/64 loss: -0.09045994281768799
Batch 30/64 loss: -0.1223185658454895
Batch 31/64 loss: -0.07577520608901978
Batch 32/64 loss: -0.09341168403625488
Batch 33/64 loss: -0.11206662654876709
Batch 34/64 loss: -0.09352457523345947
Batch 35/64 loss: -0.10963916778564453
Batch 36/64 loss: -0.0986984372138977
Batch 37/64 loss: -0.10078656673431396
Batch 38/64 loss: -0.09864181280136108
Batch 39/64 loss: -0.10544770956039429
Batch 40/64 loss: -0.08329790830612183
Batch 41/64 loss: -0.09484195709228516
Batch 42/64 loss: -0.10046756267547607
Batch 43/64 loss: -0.10666978359222412
Batch 44/64 loss: -0.09374892711639404
Batch 45/64 loss: -0.10641968250274658
Batch 46/64 loss: -0.10389554500579834
Batch 47/64 loss: -0.09256565570831299
Batch 48/64 loss: -0.09671872854232788
Batch 49/64 loss: -0.09919899702072144
Batch 50/64 loss: -0.08272868394851685
Batch 51/64 loss: -0.10303086042404175
Batch 52/64 loss: -0.07426720857620239
Batch 53/64 loss: -0.08647561073303223
Batch 54/64 loss: -0.09406554698944092
Batch 55/64 loss: -0.09003442525863647
Batch 56/64 loss: -0.09724050760269165
Batch 57/64 loss: -0.10281550884246826
Batch 58/64 loss: -0.0906711220741272
Batch 59/64 loss: -0.09435921907424927
Batch 60/64 loss: -0.11715126037597656
Batch 61/64 loss: -0.11360818147659302
Batch 62/64 loss: -0.11105668544769287
Batch 63/64 loss: -0.09603357315063477
Batch 64/64 loss: -0.1006157398223877
Epoch 223  Train loss: -0.10032797514223585  Val loss: -0.024018366107416318
Epoch 224
-------------------------------
Batch 1/64 loss: -0.10766339302062988
Batch 2/64 loss: -0.10171341896057129
Batch 3/64 loss: -0.0952521562576294
Batch 4/64 loss: -0.09648525714874268
Batch 5/64 loss: -0.07672375440597534
Batch 6/64 loss: -0.12060397863388062
Batch 7/64 loss: -0.11212265491485596
Batch 8/64 loss: -0.11697685718536377
Batch 9/64 loss: -0.07871705293655396
Batch 10/64 loss: -0.1134113073348999
Batch 11/64 loss: -0.12002420425415039
Batch 12/64 loss: -0.1168777346611023
Batch 13/64 loss: -0.09260374307632446
Batch 14/64 loss: -0.11018991470336914
Batch 15/64 loss: -0.08643341064453125
Batch 16/64 loss: -0.10571318864822388
Batch 17/64 loss: -0.11384910345077515
Batch 18/64 loss: -0.10246795415878296
Batch 19/64 loss: -0.09314084053039551
Batch 20/64 loss: -0.06999564170837402
Batch 21/64 loss: -0.1056528091430664
Batch 22/64 loss: -0.10720670223236084
Batch 23/64 loss: -0.10719192028045654
Batch 24/64 loss: -0.09872657060623169
Batch 25/64 loss: -0.10411977767944336
Batch 26/64 loss: -0.11034929752349854
Batch 27/64 loss: -0.0993272066116333
Batch 28/64 loss: -0.08690476417541504
Batch 29/64 loss: -0.08590179681777954
Batch 30/64 loss: -0.08600354194641113
Batch 31/64 loss: -0.10021317005157471
Batch 32/64 loss: -0.09419775009155273
Batch 33/64 loss: -0.08962160348892212
Batch 34/64 loss: -0.0887407660484314
Batch 35/64 loss: -0.10180515050888062
Batch 36/64 loss: -0.09612971544265747
Batch 37/64 loss: -0.09419381618499756
Batch 38/64 loss: -0.06521916389465332
Batch 39/64 loss: -0.0957486629486084
Batch 40/64 loss: -0.0960918664932251
Batch 41/64 loss: -0.07249993085861206
Batch 42/64 loss: -0.10929703712463379
Batch 43/64 loss: -0.10129916667938232
Batch 44/64 loss: -0.10002464056015015
Batch 45/64 loss: -0.08543634414672852
Batch 46/64 loss: -0.08788025379180908
Batch 47/64 loss: -0.08204591274261475
Batch 48/64 loss: -0.10396981239318848
Batch 49/64 loss: -0.08739650249481201
Batch 50/64 loss: -0.09728491306304932
Batch 51/64 loss: -0.08534365892410278
Batch 52/64 loss: -0.08833295106887817
Batch 53/64 loss: -0.08730095624923706
Batch 54/64 loss: -0.09109252691268921
Batch 55/64 loss: -0.10390937328338623
Batch 56/64 loss: -0.09951204061508179
Batch 57/64 loss: -0.08895093202590942
Batch 58/64 loss: -0.08513689041137695
Batch 59/64 loss: -0.10052037239074707
Batch 60/64 loss: -0.09319978952407837
Batch 61/64 loss: -0.09758555889129639
Batch 62/64 loss: -0.09764206409454346
Batch 63/64 loss: -0.08459824323654175
Batch 64/64 loss: -0.09962743520736694
Epoch 224  Train loss: -0.09645948339911069  Val loss: -0.018571884566565968
Epoch 225
-------------------------------
Batch 1/64 loss: -0.07628720998764038
Batch 2/64 loss: -0.11352747678756714
Batch 3/64 loss: -0.09550619125366211
Batch 4/64 loss: -0.09214359521865845
Batch 5/64 loss: -0.07590115070343018
Batch 6/64 loss: -0.09683728218078613
Batch 7/64 loss: -0.09705352783203125
Batch 8/64 loss: -0.06338393688201904
Batch 9/64 loss: -0.10483306646347046
Batch 10/64 loss: -0.09574419260025024
Batch 11/64 loss: -0.10323089361190796
Batch 12/64 loss: -0.07784152030944824
Batch 13/64 loss: -0.09565651416778564
Batch 14/64 loss: -0.10658258199691772
Batch 15/64 loss: -0.07754045724868774
Batch 16/64 loss: -0.10367774963378906
Batch 17/64 loss: -0.11641693115234375
Batch 18/64 loss: -0.10765296220779419
Batch 19/64 loss: -0.10972988605499268
Batch 20/64 loss: -0.08435016870498657
Batch 21/64 loss: -0.09937435388565063
Batch 22/64 loss: -0.07893228530883789
Batch 23/64 loss: -0.09540790319442749
Batch 24/64 loss: -0.11877697706222534
Batch 25/64 loss: -0.10053896903991699
Batch 26/64 loss: -0.1038970947265625
Batch 27/64 loss: -0.09504091739654541
Batch 28/64 loss: -0.07066786289215088
Batch 29/64 loss: -0.10120886564254761
Batch 30/64 loss: -0.11780732870101929
Batch 31/64 loss: -0.10467225313186646
Batch 32/64 loss: -0.10432380437850952
Batch 33/64 loss: -0.10005760192871094
Batch 34/64 loss: -0.0833289623260498
Batch 35/64 loss: -0.09256047010421753
Batch 36/64 loss: -0.10564064979553223
Batch 37/64 loss: -0.09288012981414795
Batch 38/64 loss: -0.08272010087966919
Batch 39/64 loss: -0.10508525371551514
Batch 40/64 loss: -0.09842151403427124
Batch 41/64 loss: -0.10399764776229858
Batch 42/64 loss: -0.08884763717651367
Batch 43/64 loss: -0.08197635412216187
Batch 44/64 loss: -0.07197785377502441
Batch 45/64 loss: -0.06802409887313843
Batch 46/64 loss: -0.1132156252861023
Batch 47/64 loss: -0.10521519184112549
Batch 48/64 loss: -0.07971030473709106
Batch 49/64 loss: -0.10423201322555542
Batch 50/64 loss: -0.09294474124908447
Batch 51/64 loss: -0.10011398792266846
Batch 52/64 loss: -0.09703868627548218
Batch 53/64 loss: -0.0911250114440918
Batch 54/64 loss: -0.08677738904953003
Batch 55/64 loss: -0.09457266330718994
Batch 56/64 loss: -0.08634674549102783
Batch 57/64 loss: -0.11265212297439575
Batch 58/64 loss: -0.09161782264709473
Batch 59/64 loss: -0.10581976175308228
Batch 60/64 loss: -0.10422289371490479
Batch 61/64 loss: -0.10946345329284668
Batch 62/64 loss: -0.09728294610977173
Batch 63/64 loss: -0.09037977457046509
Batch 64/64 loss: -0.09430718421936035
Epoch 225  Train loss: -0.09558471698386996  Val loss: -0.019419810206619734
Epoch 226
-------------------------------
Batch 1/64 loss: -0.10340380668640137
Batch 2/64 loss: -0.0925520658493042
Batch 3/64 loss: -0.07199519872665405
Batch 4/64 loss: -0.09698939323425293
Batch 5/64 loss: -0.09383833408355713
Batch 6/64 loss: -0.11447978019714355
Batch 7/64 loss: -0.09610384702682495
Batch 8/64 loss: -0.10772240161895752
Batch 9/64 loss: -0.08873063325881958
Batch 10/64 loss: -0.09585869312286377
Batch 11/64 loss: -0.09959542751312256
Batch 12/64 loss: -0.10127973556518555
Batch 13/64 loss: -0.09371316432952881
Batch 14/64 loss: -0.1058429479598999
Batch 15/64 loss: -0.11086094379425049
Batch 16/64 loss: -0.11070370674133301
Batch 17/64 loss: -0.12724661827087402
Batch 18/64 loss: -0.09906452894210815
Batch 19/64 loss: -0.10222327709197998
Batch 20/64 loss: -0.11697828769683838
Batch 21/64 loss: -0.09154659509658813
Batch 22/64 loss: -0.07981997728347778
Batch 23/64 loss: -0.11719584465026855
Batch 24/64 loss: -0.11462193727493286
Batch 25/64 loss: -0.10642135143280029
Batch 26/64 loss: -0.11195540428161621
Batch 27/64 loss: -0.11531609296798706
Batch 28/64 loss: -0.08954370021820068
Batch 29/64 loss: -0.11305063962936401
Batch 30/64 loss: -0.10472232103347778
Batch 31/64 loss: -0.08915311098098755
Batch 32/64 loss: -0.09345430135726929
Batch 33/64 loss: -0.09934848546981812
Batch 34/64 loss: -0.10516232252120972
Batch 35/64 loss: -0.10119777917861938
Batch 36/64 loss: -0.09597110748291016
Batch 37/64 loss: -0.0814247727394104
Batch 38/64 loss: -0.09479027986526489
Batch 39/64 loss: -0.12128233909606934
Batch 40/64 loss: -0.09717720746994019
Batch 41/64 loss: -0.11399656534194946
Batch 42/64 loss: -0.08602797985076904
Batch 43/64 loss: -0.09913808107376099
Batch 44/64 loss: -0.07892715930938721
Batch 45/64 loss: -0.10978114604949951
Batch 46/64 loss: -0.09945058822631836
Batch 47/64 loss: -0.10330194234848022
Batch 48/64 loss: -0.10697972774505615
Batch 49/64 loss: -0.09785497188568115
Batch 50/64 loss: -0.07568925619125366
Batch 51/64 loss: -0.10814905166625977
Batch 52/64 loss: -0.1008719801902771
Batch 53/64 loss: -0.09119552373886108
Batch 54/64 loss: -0.10284453630447388
Batch 55/64 loss: -0.11630749702453613
Batch 56/64 loss: -0.09413164854049683
Batch 57/64 loss: -0.08915215730667114
Batch 58/64 loss: -0.08373653888702393
Batch 59/64 loss: -0.08662331104278564
Batch 60/64 loss: -0.09579330682754517
Batch 61/64 loss: -0.10795164108276367
Batch 62/64 loss: -0.10814481973648071
Batch 63/64 loss: -0.10098803043365479
Batch 64/64 loss: -0.0757375955581665
Epoch 226  Train loss: -0.09986163167392506  Val loss: -0.022719375865975607
Epoch 227
-------------------------------
Batch 1/64 loss: -0.0827871561050415
Batch 2/64 loss: -0.08219337463378906
Batch 3/64 loss: -0.12310481071472168
Batch 4/64 loss: -0.08801865577697754
Batch 5/64 loss: -0.0631292462348938
Batch 6/64 loss: -0.09255784749984741
Batch 7/64 loss: -0.09103512763977051
Batch 8/64 loss: -0.11538457870483398
Batch 9/64 loss: -0.10452002286911011
Batch 10/64 loss: -0.11358493566513062
Batch 11/64 loss: -0.09719938039779663
Batch 12/64 loss: -0.08846896886825562
Batch 13/64 loss: -0.08167177438735962
Batch 14/64 loss: -0.08875995874404907
Batch 15/64 loss: -0.09264850616455078
Batch 16/64 loss: -0.11429208517074585
Batch 17/64 loss: -0.10466474294662476
Batch 18/64 loss: -0.10285323858261108
Batch 19/64 loss: -0.08734524250030518
Batch 20/64 loss: -0.10451197624206543
Batch 21/64 loss: -0.10391455888748169
Batch 22/64 loss: -0.1086122989654541
Batch 23/64 loss: -0.10327804088592529
Batch 24/64 loss: -0.11692583560943604
Batch 25/64 loss: -0.08797216415405273
Batch 26/64 loss: -0.0949050784111023
Batch 27/64 loss: -0.10121667385101318
Batch 28/64 loss: -0.11824607849121094
Batch 29/64 loss: -0.09156149625778198
Batch 30/64 loss: -0.09920960664749146
Batch 31/64 loss: -0.10204070806503296
Batch 32/64 loss: -0.1079787015914917
Batch 33/64 loss: -0.11773133277893066
Batch 34/64 loss: -0.09711593389511108
Batch 35/64 loss: -0.1200559139251709
Batch 36/64 loss: -0.09023970365524292
Batch 37/64 loss: -0.09386187791824341
Batch 38/64 loss: -0.10586988925933838
Batch 39/64 loss: -0.06999200582504272
Batch 40/64 loss: -0.09754019975662231
Batch 41/64 loss: -0.1006859540939331
Batch 42/64 loss: -0.0995866060256958
Batch 43/64 loss: -0.12602537870407104
Batch 44/64 loss: -0.09756481647491455
Batch 45/64 loss: -0.09252583980560303
Batch 46/64 loss: -0.10298311710357666
Batch 47/64 loss: -0.10703617334365845
Batch 48/64 loss: -0.09441417455673218
Batch 49/64 loss: -0.09556138515472412
Batch 50/64 loss: -0.05727434158325195
Batch 51/64 loss: -0.08235418796539307
Batch 52/64 loss: -0.08731311559677124
Batch 53/64 loss: -0.09760230779647827
Batch 54/64 loss: -0.10612624883651733
Batch 55/64 loss: -0.08758890628814697
Batch 56/64 loss: -0.10375463962554932
Batch 57/64 loss: -0.11365324258804321
Batch 58/64 loss: -0.08729356527328491
Batch 59/64 loss: -0.10279190540313721
Batch 60/64 loss: -0.1087847352027893
Batch 61/64 loss: -0.08170032501220703
Batch 62/64 loss: -0.10495787858963013
Batch 63/64 loss: -0.10321128368377686
Batch 64/64 loss: -0.09898018836975098
Epoch 227  Train loss: -0.09822784311631147  Val loss: -0.01378421562234151
Epoch 228
-------------------------------
Batch 1/64 loss: -0.1108044981956482
Batch 2/64 loss: -0.11818468570709229
Batch 3/64 loss: -0.09543532133102417
Batch 4/64 loss: -0.10355156660079956
Batch 5/64 loss: -0.094352126121521
Batch 6/64 loss: -0.11737179756164551
Batch 7/64 loss: -0.11021292209625244
Batch 8/64 loss: -0.11091893911361694
Batch 9/64 loss: -0.07740914821624756
Batch 10/64 loss: -0.11459499597549438
Batch 11/64 loss: -0.07767832279205322
Batch 12/64 loss: -0.11899024248123169
Batch 13/64 loss: -0.11349016427993774
Batch 14/64 loss: -0.10625946521759033
Batch 15/64 loss: -0.10842174291610718
Batch 16/64 loss: -0.1098896861076355
Batch 17/64 loss: -0.1022188663482666
Batch 18/64 loss: -0.09471350908279419
Batch 19/64 loss: -0.09563076496124268
Batch 20/64 loss: -0.10276246070861816
Batch 21/64 loss: -0.1170549988746643
Batch 22/64 loss: -0.06642389297485352
Batch 23/64 loss: -0.08512765169143677
Batch 24/64 loss: -0.08634746074676514
Batch 25/64 loss: -0.10315465927124023
Batch 26/64 loss: -0.12212890386581421
Batch 27/64 loss: -0.11084955930709839
Batch 28/64 loss: -0.09600591659545898
Batch 29/64 loss: -0.1131124496459961
Batch 30/64 loss: -0.08771324157714844
Batch 31/64 loss: -0.091755211353302
Batch 32/64 loss: -0.09967803955078125
Batch 33/64 loss: -0.09930038452148438
Batch 34/64 loss: -0.09400367736816406
Batch 35/64 loss: -0.1076744794845581
Batch 36/64 loss: -0.11330187320709229
Batch 37/64 loss: -0.0961657166481018
Batch 38/64 loss: -0.09510970115661621
Batch 39/64 loss: -0.10046350955963135
Batch 40/64 loss: -0.10841012001037598
Batch 41/64 loss: -0.11022061109542847
Batch 42/64 loss: -0.10213011503219604
Batch 43/64 loss: -0.09931862354278564
Batch 44/64 loss: -0.10970163345336914
Batch 45/64 loss: -0.08473217487335205
Batch 46/64 loss: -0.11774969100952148
Batch 47/64 loss: -0.07425034046173096
Batch 48/64 loss: -0.08943867683410645
Batch 49/64 loss: -0.08439481258392334
Batch 50/64 loss: -0.11532425880432129
Batch 51/64 loss: -0.1085824966430664
Batch 52/64 loss: -0.10957247018814087
Batch 53/64 loss: -0.11025792360305786
Batch 54/64 loss: -0.09951680898666382
Batch 55/64 loss: -0.06364375352859497
Batch 56/64 loss: -0.10911035537719727
Batch 57/64 loss: -0.11967158317565918
Batch 58/64 loss: -0.10965800285339355
Batch 59/64 loss: -0.08128190040588379
Batch 60/64 loss: -0.10725796222686768
Batch 61/64 loss: -0.1062769889831543
Batch 62/64 loss: -0.08807075023651123
Batch 63/64 loss: -0.11414384841918945
Batch 64/64 loss: -0.11121338605880737
Epoch 228  Train loss: -0.10155903521706076  Val loss: -0.022642239877038804
Epoch 229
-------------------------------
Batch 1/64 loss: -0.12290775775909424
Batch 2/64 loss: -0.0780896544456482
Batch 3/64 loss: -0.12173360586166382
Batch 4/64 loss: -0.1067197322845459
Batch 5/64 loss: -0.10558807849884033
Batch 6/64 loss: -0.12606030702590942
Batch 7/64 loss: -0.120688796043396
Batch 8/64 loss: -0.10030794143676758
Batch 9/64 loss: -0.10736864805221558
Batch 10/64 loss: -0.1116684079170227
Batch 11/64 loss: -0.09320569038391113
Batch 12/64 loss: -0.12363296747207642
Batch 13/64 loss: -0.10662150382995605
Batch 14/64 loss: -0.1158248782157898
Batch 15/64 loss: -0.10423505306243896
Batch 16/64 loss: -0.0950162410736084
Batch 17/64 loss: -0.12192904949188232
Batch 18/64 loss: -0.11897176504135132
Batch 19/64 loss: -0.10739588737487793
Batch 20/64 loss: -0.09464794397354126
Batch 21/64 loss: -0.1201632022857666
Batch 22/64 loss: -0.11332184076309204
Batch 23/64 loss: -0.11333870887756348
Batch 24/64 loss: -0.10183632373809814
Batch 25/64 loss: -0.0953911542892456
Batch 26/64 loss: -0.11352801322937012
Batch 27/64 loss: -0.08923143148422241
Batch 28/64 loss: -0.10576790571212769
Batch 29/64 loss: -0.11045396327972412
Batch 30/64 loss: -0.09311556816101074
Batch 31/64 loss: -0.10594278573989868
Batch 32/64 loss: -0.12285679578781128
Batch 33/64 loss: -0.08392441272735596
Batch 34/64 loss: -0.11566698551177979
Batch 35/64 loss: -0.10214757919311523
Batch 36/64 loss: -0.10141748189926147
Batch 37/64 loss: -0.08800935745239258
Batch 38/64 loss: -0.09059065580368042
Batch 39/64 loss: -0.11197781562805176
Batch 40/64 loss: -0.09183639287948608
Batch 41/64 loss: -0.11381733417510986
Batch 42/64 loss: -0.11308705806732178
Batch 43/64 loss: -0.10171598196029663
Batch 44/64 loss: -0.11321264505386353
Batch 45/64 loss: -0.09623241424560547
Batch 46/64 loss: -0.10681533813476562
Batch 47/64 loss: -0.08883470296859741
Batch 48/64 loss: -0.08530795574188232
Batch 49/64 loss: -0.08636355400085449
Batch 50/64 loss: -0.09443902969360352
Batch 51/64 loss: -0.10222601890563965
Batch 52/64 loss: -0.09297651052474976
Batch 53/64 loss: -0.08068066835403442
Batch 54/64 loss: -0.09218478202819824
Batch 55/64 loss: -0.10043424367904663
Batch 56/64 loss: -0.08727836608886719
Batch 57/64 loss: -0.06300497055053711
Batch 58/64 loss: -0.11116814613342285
Batch 59/64 loss: -0.11389875411987305
Batch 60/64 loss: -0.11732196807861328
Batch 61/64 loss: -0.11041474342346191
Batch 62/64 loss: -0.10147362947463989
Batch 63/64 loss: -0.09954303503036499
Batch 64/64 loss: -0.10022473335266113
Epoch 229  Train loss: -0.10354090484918332  Val loss: -0.019800383610414064
Epoch 230
-------------------------------
Batch 1/64 loss: -0.09716188907623291
Batch 2/64 loss: -0.08499103784561157
Batch 3/64 loss: -0.12013381719589233
Batch 4/64 loss: -0.09743887186050415
Batch 5/64 loss: -0.11482107639312744
Batch 6/64 loss: -0.11127793788909912
Batch 7/64 loss: -0.11039924621582031
Batch 8/64 loss: -0.09279769659042358
Batch 9/64 loss: -0.11030679941177368
Batch 10/64 loss: -0.12949049472808838
Batch 11/64 loss: -0.11718231439590454
Batch 12/64 loss: -0.10635793209075928
Batch 13/64 loss: -0.09262436628341675
Batch 14/64 loss: -0.11930477619171143
Batch 15/64 loss: -0.09418332576751709
Batch 16/64 loss: -0.09508776664733887
Batch 17/64 loss: -0.0861964225769043
Batch 18/64 loss: -0.10050201416015625
Batch 19/64 loss: -0.10137611627578735
Batch 20/64 loss: -0.0790795087814331
Batch 21/64 loss: -0.09867644309997559
Batch 22/64 loss: -0.10495448112487793
Batch 23/64 loss: -0.09307551383972168
Batch 24/64 loss: -0.09304255247116089
Batch 25/64 loss: -0.09424352645874023
Batch 26/64 loss: -0.0909125804901123
Batch 27/64 loss: -0.08957946300506592
Batch 28/64 loss: -0.09450554847717285
Batch 29/64 loss: -0.10805690288543701
Batch 30/64 loss: -0.07049548625946045
Batch 31/64 loss: -0.11175012588500977
Batch 32/64 loss: -0.08812165260314941
Batch 33/64 loss: -0.08776134252548218
Batch 34/64 loss: -0.10108208656311035
Batch 35/64 loss: -0.08889937400817871
Batch 36/64 loss: -0.08707308769226074
Batch 37/64 loss: -0.11659878492355347
Batch 38/64 loss: -0.10245859622955322
Batch 39/64 loss: -0.10662758350372314
Batch 40/64 loss: -0.0994575023651123
Batch 41/64 loss: -0.09968072175979614
Batch 42/64 loss: -0.10165524482727051
Batch 43/64 loss: -0.0955236554145813
Batch 44/64 loss: -0.11682236194610596
Batch 45/64 loss: -0.09535384178161621
Batch 46/64 loss: -0.09292840957641602
Batch 47/64 loss: -0.100730299949646
Batch 48/64 loss: -0.09518426656723022
Batch 49/64 loss: -0.09482467174530029
Batch 50/64 loss: -0.10192012786865234
Batch 51/64 loss: -0.10530436038970947
Batch 52/64 loss: -0.1041039228439331
Batch 53/64 loss: -0.09339869022369385
Batch 54/64 loss: -0.10541951656341553
Batch 55/64 loss: -0.090099036693573
Batch 56/64 loss: -0.10477602481842041
Batch 57/64 loss: -0.07358360290527344
Batch 58/64 loss: -0.09982585906982422
Batch 59/64 loss: -0.08898401260375977
Batch 60/64 loss: -0.09539234638214111
Batch 61/64 loss: -0.08810961246490479
Batch 62/64 loss: -0.10424739122390747
Batch 63/64 loss: -0.11465650796890259
Batch 64/64 loss: -0.09618765115737915
Epoch 230  Train loss: -0.09918041205873676  Val loss: -0.023501950850601458
Epoch 231
-------------------------------
Batch 1/64 loss: -0.10946393013000488
Batch 2/64 loss: -0.09307986497879028
Batch 3/64 loss: -0.0911363959312439
Batch 4/64 loss: -0.08937227725982666
Batch 5/64 loss: -0.11001545190811157
Batch 6/64 loss: -0.09454935789108276
Batch 7/64 loss: -0.10861915349960327
Batch 8/64 loss: -0.10164248943328857
Batch 9/64 loss: -0.09127414226531982
Batch 10/64 loss: -0.11906248331069946
Batch 11/64 loss: -0.09868234395980835
Batch 12/64 loss: -0.09391814470291138
Batch 13/64 loss: -0.0913805365562439
Batch 14/64 loss: -0.10632646083831787
Batch 15/64 loss: -0.1081574559211731
Batch 16/64 loss: -0.1021111011505127
Batch 17/64 loss: -0.09075558185577393
Batch 18/64 loss: -0.08608406782150269
Batch 19/64 loss: -0.1070900559425354
Batch 20/64 loss: -0.09722405672073364
Batch 21/64 loss: -0.09869277477264404
Batch 22/64 loss: -0.09658443927764893
Batch 23/64 loss: -0.10435283184051514
Batch 24/64 loss: -0.11602801084518433
Batch 25/64 loss: -0.10209274291992188
Batch 26/64 loss: -0.09013605117797852
Batch 27/64 loss: -0.10064101219177246
Batch 28/64 loss: -0.11772763729095459
Batch 29/64 loss: -0.10851573944091797
Batch 30/64 loss: -0.09196209907531738
Batch 31/64 loss: -0.07969361543655396
Batch 32/64 loss: -0.12010246515274048
Batch 33/64 loss: -0.09955668449401855
Batch 34/64 loss: -0.09324634075164795
Batch 35/64 loss: -0.09578907489776611
Batch 36/64 loss: -0.10710501670837402
Batch 37/64 loss: -0.0816800594329834
Batch 38/64 loss: -0.08830219507217407
Batch 39/64 loss: -0.08916366100311279
Batch 40/64 loss: -0.11312413215637207
Batch 41/64 loss: -0.10954338312149048
Batch 42/64 loss: -0.11046642065048218
Batch 43/64 loss: -0.11319106817245483
Batch 44/64 loss: -0.11755973100662231
Batch 45/64 loss: -0.08970493078231812
Batch 46/64 loss: -0.09408438205718994
Batch 47/64 loss: -0.07535839080810547
Batch 48/64 loss: -0.09907788038253784
Batch 49/64 loss: -0.11597609519958496
Batch 50/64 loss: -0.10677015781402588
Batch 51/64 loss: -0.11395508050918579
Batch 52/64 loss: -0.08612060546875
Batch 53/64 loss: -0.11010998487472534
Batch 54/64 loss: -0.09736514091491699
Batch 55/64 loss: -0.09867978096008301
Batch 56/64 loss: -0.10408645868301392
Batch 57/64 loss: -0.11086869239807129
Batch 58/64 loss: -0.11085432767868042
Batch 59/64 loss: -0.1028323769569397
Batch 60/64 loss: -0.11674493551254272
Batch 61/64 loss: -0.09669584035873413
Batch 62/64 loss: -0.11410623788833618
Batch 63/64 loss: -0.10489892959594727
Batch 64/64 loss: -0.08311623334884644
Epoch 231  Train loss: -0.10111105792662677  Val loss: -0.022460028887614353
Epoch 232
-------------------------------
Batch 1/64 loss: -0.11175715923309326
Batch 2/64 loss: -0.12081176042556763
Batch 3/64 loss: -0.10312944650650024
Batch 4/64 loss: -0.096030592918396
Batch 5/64 loss: -0.10088181495666504
Batch 6/64 loss: -0.1019892692565918
Batch 7/64 loss: -0.09674447774887085
Batch 8/64 loss: -0.0959627628326416
Batch 9/64 loss: -0.08237528800964355
Batch 10/64 loss: -0.11350095272064209
Batch 11/64 loss: -0.1026429533958435
Batch 12/64 loss: -0.09936785697937012
Batch 13/64 loss: -0.11463284492492676
Batch 14/64 loss: -0.1342984437942505
Batch 15/64 loss: -0.0789300799369812
Batch 16/64 loss: -0.11663633584976196
Batch 17/64 loss: -0.10481369495391846
Batch 18/64 loss: -0.11389356851577759
Batch 19/64 loss: -0.09958714246749878
Batch 20/64 loss: -0.11640715599060059
Batch 21/64 loss: -0.11499977111816406
Batch 22/64 loss: -0.12158924341201782
Batch 23/64 loss: -0.10616105794906616
Batch 24/64 loss: -0.09831541776657104
Batch 25/64 loss: -0.11237525939941406
Batch 26/64 loss: -0.0958603024482727
Batch 27/64 loss: -0.10953354835510254
Batch 28/64 loss: -0.10957252979278564
Batch 29/64 loss: -0.1001923680305481
Batch 30/64 loss: -0.09863704442977905
Batch 31/64 loss: -0.10522764921188354
Batch 32/64 loss: -0.08662188053131104
Batch 33/64 loss: -0.10321444272994995
Batch 34/64 loss: -0.09110516309738159
Batch 35/64 loss: -0.09620881080627441
Batch 36/64 loss: -0.10649371147155762
Batch 37/64 loss: -0.11386466026306152
Batch 38/64 loss: -0.09331953525543213
Batch 39/64 loss: -0.09488779306411743
Batch 40/64 loss: -0.11202341318130493
Batch 41/64 loss: -0.09510302543640137
Batch 42/64 loss: -0.1161537766456604
Batch 43/64 loss: -0.08759057521820068
Batch 44/64 loss: -0.1128801703453064
Batch 45/64 loss: -0.1082199215888977
Batch 46/64 loss: -0.10370504856109619
Batch 47/64 loss: -0.10730350017547607
Batch 48/64 loss: -0.10722076892852783
Batch 49/64 loss: -0.11261200904846191
Batch 50/64 loss: -0.09734773635864258
Batch 51/64 loss: -0.08461076021194458
Batch 52/64 loss: -0.10323715209960938
Batch 53/64 loss: -0.10425716638565063
Batch 54/64 loss: -0.0937541127204895
Batch 55/64 loss: -0.09769272804260254
Batch 56/64 loss: -0.08701562881469727
Batch 57/64 loss: -0.04952836036682129
Batch 58/64 loss: -0.09905135631561279
Batch 59/64 loss: -0.09629923105239868
Batch 60/64 loss: -0.10700857639312744
Batch 61/64 loss: -0.1175505518913269
Batch 62/64 loss: -0.11683255434036255
Batch 63/64 loss: -0.0998578667640686
Batch 64/64 loss: -0.11416608095169067
Epoch 232  Train loss: -0.1029812445827559  Val loss: -0.018120618825106277
Epoch 233
-------------------------------
Batch 1/64 loss: -0.1056482195854187
Batch 2/64 loss: -0.10039341449737549
Batch 3/64 loss: -0.09726393222808838
Batch 4/64 loss: -0.11096334457397461
Batch 5/64 loss: -0.11084520816802979
Batch 6/64 loss: -0.10963273048400879
Batch 7/64 loss: -0.1197131872177124
Batch 8/64 loss: -0.07307136058807373
Batch 9/64 loss: -0.09438830614089966
Batch 10/64 loss: -0.09860098361968994
Batch 11/64 loss: -0.08283799886703491
Batch 12/64 loss: -0.12343168258666992
Batch 13/64 loss: -0.1101984977722168
Batch 14/64 loss: -0.11510050296783447
Batch 15/64 loss: -0.11155033111572266
Batch 16/64 loss: -0.10052180290222168
Batch 17/64 loss: -0.10107433795928955
Batch 18/64 loss: -0.1187404990196228
Batch 19/64 loss: -0.09917175769805908
Batch 20/64 loss: -0.12245917320251465
Batch 21/64 loss: -0.08887028694152832
Batch 22/64 loss: -0.11529237031936646
Batch 23/64 loss: -0.09482836723327637
Batch 24/64 loss: -0.08964395523071289
Batch 25/64 loss: -0.099246084690094
Batch 26/64 loss: -0.09421825408935547
Batch 27/64 loss: -0.10133892297744751
Batch 28/64 loss: -0.10882341861724854
Batch 29/64 loss: -0.11162543296813965
Batch 30/64 loss: -0.11572849750518799
Batch 31/64 loss: -0.10681241750717163
Batch 32/64 loss: -0.09137630462646484
Batch 33/64 loss: -0.10954517126083374
Batch 34/64 loss: -0.09797340631484985
Batch 35/64 loss: -0.09256970882415771
Batch 36/64 loss: -0.10451120138168335
Batch 37/64 loss: -0.09211468696594238
Batch 38/64 loss: -0.09997814893722534
Batch 39/64 loss: -0.11963558197021484
Batch 40/64 loss: -0.11529892683029175
Batch 41/64 loss: -0.10234105587005615
Batch 42/64 loss: -0.11218291521072388
Batch 43/64 loss: -0.08186519145965576
Batch 44/64 loss: -0.11499631404876709
Batch 45/64 loss: -0.10848748683929443
Batch 46/64 loss: -0.10775518417358398
Batch 47/64 loss: -0.09703481197357178
Batch 48/64 loss: -0.09690874814987183
Batch 49/64 loss: -0.08933287858963013
Batch 50/64 loss: -0.1144372820854187
Batch 51/64 loss: -0.11511266231536865
Batch 52/64 loss: -0.103049635887146
Batch 53/64 loss: -0.103543221950531
Batch 54/64 loss: -0.126275897026062
Batch 55/64 loss: -0.09977692365646362
Batch 56/64 loss: -0.08992558717727661
Batch 57/64 loss: -0.10312366485595703
Batch 58/64 loss: -0.0700264573097229
Batch 59/64 loss: -0.09639853239059448
Batch 60/64 loss: -0.0970926284790039
Batch 61/64 loss: -0.08632862567901611
Batch 62/64 loss: -0.0865105390548706
Batch 63/64 loss: -0.10407698154449463
Batch 64/64 loss: -0.11022031307220459
Epoch 233  Train loss: -0.10265548042222565  Val loss: -0.01893256333275759
Epoch 234
-------------------------------
Batch 1/64 loss: -0.11901122331619263
Batch 2/64 loss: -0.1208297610282898
Batch 3/64 loss: -0.10679745674133301
Batch 4/64 loss: -0.10741543769836426
Batch 5/64 loss: -0.1247711181640625
Batch 6/64 loss: -0.10347068309783936
Batch 7/64 loss: -0.11562347412109375
Batch 8/64 loss: -0.11665058135986328
Batch 9/64 loss: -0.11707198619842529
Batch 10/64 loss: -0.10255086421966553
Batch 11/64 loss: -0.11723583936691284
Batch 12/64 loss: -0.09555155038833618
Batch 13/64 loss: -0.07313305139541626
Batch 14/64 loss: -0.1185373067855835
Batch 15/64 loss: -0.09298920631408691
Batch 16/64 loss: -0.09856247901916504
Batch 17/64 loss: -0.10589444637298584
Batch 18/64 loss: -0.10175633430480957
Batch 19/64 loss: -0.1103973388671875
Batch 20/64 loss: -0.1062232255935669
Batch 21/64 loss: -0.10056626796722412
Batch 22/64 loss: -0.11262267827987671
Batch 23/64 loss: -0.1140623688697815
Batch 24/64 loss: -0.10972929000854492
Batch 25/64 loss: -0.1154908537864685
Batch 26/64 loss: -0.12211459875106812
Batch 27/64 loss: -0.08887958526611328
Batch 28/64 loss: -0.10840469598770142
Batch 29/64 loss: -0.06172245740890503
Batch 30/64 loss: -0.08182460069656372
Batch 31/64 loss: -0.10247683525085449
Batch 32/64 loss: -0.10135787725448608
Batch 33/64 loss: -0.09548032283782959
Batch 34/64 loss: -0.07521551847457886
Batch 35/64 loss: -0.09551656246185303
Batch 36/64 loss: -0.10188925266265869
Batch 37/64 loss: -0.10118120908737183
Batch 38/64 loss: -0.10386478900909424
Batch 39/64 loss: -0.12356352806091309
Batch 40/64 loss: -0.10185515880584717
Batch 41/64 loss: -0.0878564715385437
Batch 42/64 loss: -0.10643434524536133
Batch 43/64 loss: -0.10435837507247925
Batch 44/64 loss: -0.10282588005065918
Batch 45/64 loss: -0.0927390456199646
Batch 46/64 loss: -0.10882556438446045
Batch 47/64 loss: -0.11686372756958008
Batch 48/64 loss: -0.09664273262023926
Batch 49/64 loss: -0.06700378656387329
Batch 50/64 loss: -0.08800005912780762
Batch 51/64 loss: -0.10421746969223022
Batch 52/64 loss: -0.10510629415512085
Batch 53/64 loss: -0.1123729944229126
Batch 54/64 loss: -0.09549516439437866
Batch 55/64 loss: -0.09579193592071533
Batch 56/64 loss: -0.12123274803161621
Batch 57/64 loss: -0.1253887414932251
Batch 58/64 loss: -0.09224569797515869
Batch 59/64 loss: -0.11978387832641602
Batch 60/64 loss: -0.10600197315216064
Batch 61/64 loss: -0.12003743648529053
Batch 62/64 loss: -0.11168038845062256
Batch 63/64 loss: -0.11496788263320923
Batch 64/64 loss: -0.0970921516418457
Epoch 234  Train loss: -0.104172290540209  Val loss: -0.02594514319167514
Saving best model, epoch: 234
Epoch 235
-------------------------------
Batch 1/64 loss: -0.10262352228164673
Batch 2/64 loss: -0.10684794187545776
Batch 3/64 loss: -0.10034620761871338
Batch 4/64 loss: -0.10182416439056396
Batch 5/64 loss: -0.10749119520187378
Batch 6/64 loss: -0.10433083772659302
Batch 7/64 loss: -0.12140136957168579
Batch 8/64 loss: -0.08112221956253052
Batch 9/64 loss: -0.11089301109313965
Batch 10/64 loss: -0.08953124284744263
Batch 11/64 loss: -0.1192706823348999
Batch 12/64 loss: -0.08361297845840454
Batch 13/64 loss: -0.086697518825531
Batch 14/64 loss: -0.10935944318771362
Batch 15/64 loss: -0.10273921489715576
Batch 16/64 loss: -0.11123442649841309
Batch 17/64 loss: -0.10955023765563965
Batch 18/64 loss: -0.11661922931671143
Batch 19/64 loss: -0.11372697353363037
Batch 20/64 loss: -0.11062890291213989
Batch 21/64 loss: -0.08880096673965454
Batch 22/64 loss: -0.11269915103912354
Batch 23/64 loss: -0.11878275871276855
Batch 24/64 loss: -0.10665887594223022
Batch 25/64 loss: -0.1019219160079956
Batch 26/64 loss: -0.11883312463760376
Batch 27/64 loss: -0.10976141691207886
Batch 28/64 loss: -0.10380297899246216
Batch 29/64 loss: -0.0914616584777832
Batch 30/64 loss: -0.09912765026092529
Batch 31/64 loss: -0.08347344398498535
Batch 32/64 loss: -0.10776573419570923
Batch 33/64 loss: -0.10242456197738647
Batch 34/64 loss: -0.10812056064605713
Batch 35/64 loss: -0.12721115350723267
Batch 36/64 loss: -0.09441787004470825
Batch 37/64 loss: -0.06156885623931885
Batch 38/64 loss: -0.08183050155639648
Batch 39/64 loss: -0.09928679466247559
Batch 40/64 loss: -0.09422814846038818
Batch 41/64 loss: -0.09828662872314453
Batch 42/64 loss: -0.09557855129241943
Batch 43/64 loss: -0.12004047632217407
Batch 44/64 loss: -0.09335941076278687
Batch 45/64 loss: -0.11769717931747437
Batch 46/64 loss: -0.08972716331481934
Batch 47/64 loss: -0.1114739179611206
Batch 48/64 loss: -0.10101580619812012
Batch 49/64 loss: -0.08852338790893555
Batch 50/64 loss: -0.10261845588684082
Batch 51/64 loss: -0.094185471534729
Batch 52/64 loss: -0.09777295589447021
Batch 53/64 loss: -0.0845763087272644
Batch 54/64 loss: -0.09829992055892944
Batch 55/64 loss: -0.12191861867904663
Batch 56/64 loss: -0.09811615943908691
Batch 57/64 loss: -0.09952235221862793
Batch 58/64 loss: -0.12197321653366089
Batch 59/64 loss: -0.10609012842178345
Batch 60/64 loss: -0.08688056468963623
Batch 61/64 loss: -0.12719577550888062
Batch 62/64 loss: -0.11221522092819214
Batch 63/64 loss: -0.10790777206420898
Batch 64/64 loss: -0.07046675682067871
Epoch 235  Train loss: -0.10242909730649462  Val loss: -0.020643137574605515
Epoch 236
-------------------------------
Batch 1/64 loss: -0.10379624366760254
Batch 2/64 loss: -0.11592262983322144
Batch 3/64 loss: -0.10429960489273071
Batch 4/64 loss: -0.09266442060470581
Batch 5/64 loss: -0.10795408487319946
Batch 6/64 loss: -0.11002480983734131
Batch 7/64 loss: -0.10562896728515625
Batch 8/64 loss: -0.09742134809494019
Batch 9/64 loss: -0.0897931456565857
Batch 10/64 loss: -0.11356449127197266
Batch 11/64 loss: -0.11122369766235352
Batch 12/64 loss: -0.10945743322372437
Batch 13/64 loss: -0.09636259078979492
Batch 14/64 loss: -0.10240292549133301
Batch 15/64 loss: -0.09772235155105591
Batch 16/64 loss: -0.09056955575942993
Batch 17/64 loss: -0.07016652822494507
Batch 18/64 loss: -0.08253037929534912
Batch 19/64 loss: -0.09781593084335327
Batch 20/64 loss: -0.09581166505813599
Batch 21/64 loss: -0.11617028713226318
Batch 22/64 loss: -0.09391593933105469
Batch 23/64 loss: -0.11243301630020142
Batch 24/64 loss: -0.0902983546257019
Batch 25/64 loss: -0.10609114170074463
Batch 26/64 loss: -0.10977458953857422
Batch 27/64 loss: -0.07779932022094727
Batch 28/64 loss: -0.1030849814414978
Batch 29/64 loss: -0.11724215745925903
Batch 30/64 loss: -0.11657524108886719
Batch 31/64 loss: -0.10461533069610596
Batch 32/64 loss: -0.09604465961456299
Batch 33/64 loss: -0.08148807287216187
Batch 34/64 loss: -0.08847403526306152
Batch 35/64 loss: -0.09215080738067627
Batch 36/64 loss: -0.1212838888168335
Batch 37/64 loss: -0.0998755693435669
Batch 38/64 loss: -0.11700820922851562
Batch 39/64 loss: -0.10657000541687012
Batch 40/64 loss: -0.08973801136016846
Batch 41/64 loss: -0.10815441608428955
Batch 42/64 loss: -0.10345548391342163
Batch 43/64 loss: -0.1100534200668335
Batch 44/64 loss: -0.09573441743850708
Batch 45/64 loss: -0.12061536312103271
Batch 46/64 loss: -0.08253568410873413
Batch 47/64 loss: -0.10176455974578857
Batch 48/64 loss: -0.1155850887298584
Batch 49/64 loss: -0.10080480575561523
Batch 50/64 loss: -0.10117590427398682
Batch 51/64 loss: -0.12468326091766357
Batch 52/64 loss: -0.11174780130386353
Batch 53/64 loss: -0.10694944858551025
Batch 54/64 loss: -0.11358523368835449
Batch 55/64 loss: -0.09457093477249146
Batch 56/64 loss: -0.10670751333236694
Batch 57/64 loss: -0.11803328990936279
Batch 58/64 loss: -0.0889425277709961
Batch 59/64 loss: -0.11307376623153687
Batch 60/64 loss: -0.12044894695281982
Batch 61/64 loss: -0.09697377681732178
Batch 62/64 loss: -0.10484862327575684
Batch 63/64 loss: -0.1070551872253418
Batch 64/64 loss: -0.10537940263748169
Epoch 236  Train loss: -0.10293804594114715  Val loss: -0.02236569890451595
Epoch 237
-------------------------------
Batch 1/64 loss: -0.11475473642349243
Batch 2/64 loss: -0.11108410358428955
Batch 3/64 loss: -0.11766558885574341
Batch 4/64 loss: -0.12562620639801025
Batch 5/64 loss: -0.12205976247787476
Batch 6/64 loss: -0.11147356033325195
Batch 7/64 loss: -0.09995847940444946
Batch 8/64 loss: -0.10352838039398193
Batch 9/64 loss: -0.09472030401229858
Batch 10/64 loss: -0.113442063331604
Batch 11/64 loss: -0.10612058639526367
Batch 12/64 loss: -0.09054040908813477
Batch 13/64 loss: -0.10739684104919434
Batch 14/64 loss: -0.10246694087982178
Batch 15/64 loss: -0.09541451930999756
Batch 16/64 loss: -0.12588351964950562
Batch 17/64 loss: -0.09395033121109009
Batch 18/64 loss: -0.11009889841079712
Batch 19/64 loss: -0.09519630670547485
Batch 20/64 loss: -0.10954463481903076
Batch 21/64 loss: -0.08035010099411011
Batch 22/64 loss: -0.09971541166305542
Batch 23/64 loss: -0.12659716606140137
Batch 24/64 loss: -0.12220275402069092
Batch 25/64 loss: -0.10409122705459595
Batch 26/64 loss: -0.10028934478759766
Batch 27/64 loss: -0.10974454879760742
Batch 28/64 loss: -0.10002851486206055
Batch 29/64 loss: -0.09196823835372925
Batch 30/64 loss: -0.12057298421859741
Batch 31/64 loss: -0.10827797651290894
Batch 32/64 loss: -0.09925687313079834
Batch 33/64 loss: -0.1206049919128418
Batch 34/64 loss: -0.0834079384803772
Batch 35/64 loss: -0.09426933526992798
Batch 36/64 loss: -0.10565042495727539
Batch 37/64 loss: -0.11776864528656006
Batch 38/64 loss: -0.12224191427230835
Batch 39/64 loss: -0.1000751256942749
Batch 40/64 loss: -0.12943142652511597
Batch 41/64 loss: -0.09655731916427612
Batch 42/64 loss: -0.09971660375595093
Batch 43/64 loss: -0.11208373308181763
Batch 44/64 loss: -0.10713618993759155
Batch 45/64 loss: -0.11671566963195801
Batch 46/64 loss: -0.1193239688873291
Batch 47/64 loss: -0.08388328552246094
Batch 48/64 loss: -0.10256892442703247
Batch 49/64 loss: -0.10974526405334473
Batch 50/64 loss: -0.08931201696395874
Batch 51/64 loss: -0.10777920484542847
Batch 52/64 loss: -0.10368108749389648
Batch 53/64 loss: -0.10125291347503662
Batch 54/64 loss: -0.10141134262084961
Batch 55/64 loss: -0.08259087800979614
Batch 56/64 loss: -0.09037429094314575
Batch 57/64 loss: -0.09733295440673828
Batch 58/64 loss: -0.10075640678405762
Batch 59/64 loss: -0.09943175315856934
Batch 60/64 loss: -0.12038707733154297
Batch 61/64 loss: -0.10478603839874268
Batch 62/64 loss: -0.10214102268218994
Batch 63/64 loss: -0.08898717164993286
Batch 64/64 loss: -0.09676963090896606
Epoch 237  Train loss: -0.10506672087837668  Val loss: -0.02498979482454123
Epoch 238
-------------------------------
Batch 1/64 loss: -0.12524783611297607
Batch 2/64 loss: -0.11030840873718262
Batch 3/64 loss: -0.10288596153259277
Batch 4/64 loss: -0.11507010459899902
Batch 5/64 loss: -0.11764466762542725
Batch 6/64 loss: -0.12111407518386841
Batch 7/64 loss: -0.06224775314331055
Batch 8/64 loss: -0.10473859310150146
Batch 9/64 loss: -0.09255111217498779
Batch 10/64 loss: -0.09172159433364868
Batch 11/64 loss: -0.1166955828666687
Batch 12/64 loss: -0.11307090520858765
Batch 13/64 loss: -0.08469951152801514
Batch 14/64 loss: -0.12024736404418945
Batch 15/64 loss: -0.1027078628540039
Batch 16/64 loss: -0.09426063299179077
Batch 17/64 loss: -0.12015223503112793
Batch 18/64 loss: -0.09971952438354492
Batch 19/64 loss: -0.0970504879951477
Batch 20/64 loss: -0.10358971357345581
Batch 21/64 loss: -0.11522728204727173
Batch 22/64 loss: -0.11805951595306396
Batch 23/64 loss: -0.09242427349090576
Batch 24/64 loss: -0.11930811405181885
Batch 25/64 loss: -0.075980544090271
Batch 26/64 loss: -0.09132248163223267
Batch 27/64 loss: -0.11595320701599121
Batch 28/64 loss: -0.11241096258163452
Batch 29/64 loss: -0.11963337659835815
Batch 30/64 loss: -0.10731858015060425
Batch 31/64 loss: -0.11444681882858276
Batch 32/64 loss: -0.11278998851776123
Batch 33/64 loss: -0.1018030047416687
Batch 34/64 loss: -0.08036774396896362
Batch 35/64 loss: -0.07845568656921387
Batch 36/64 loss: -0.10944128036499023
Batch 37/64 loss: -0.12339389324188232
Batch 38/64 loss: -0.10273224115371704
Batch 39/64 loss: -0.1080058217048645
Batch 40/64 loss: -0.10724663734436035
Batch 41/64 loss: -0.11154985427856445
Batch 42/64 loss: -0.10576844215393066
Batch 43/64 loss: -0.11601114273071289
Batch 44/64 loss: -0.10835772752761841
Batch 45/64 loss: -0.09803938865661621
Batch 46/64 loss: -0.10474753379821777
Batch 47/64 loss: -0.10329532623291016
Batch 48/64 loss: -0.12563329935073853
Batch 49/64 loss: -0.11546981334686279
Batch 50/64 loss: -0.10761141777038574
Batch 51/64 loss: -0.09722673892974854
Batch 52/64 loss: -0.10054224729537964
Batch 53/64 loss: -0.10869616270065308
Batch 54/64 loss: -0.09595775604248047
Batch 55/64 loss: -0.10016608238220215
Batch 56/64 loss: -0.1201775074005127
Batch 57/64 loss: -0.07594043016433716
Batch 58/64 loss: -0.10178780555725098
Batch 59/64 loss: -0.10978388786315918
Batch 60/64 loss: -0.10129064321517944
Batch 61/64 loss: -0.08841854333877563
Batch 62/64 loss: -0.10411667823791504
Batch 63/64 loss: -0.09425580501556396
Batch 64/64 loss: -0.10570025444030762
Epoch 238  Train loss: -0.1047241538178687  Val loss: -0.023636605731400428
Epoch 239
-------------------------------
Batch 1/64 loss: -0.10580313205718994
Batch 2/64 loss: -0.11587011814117432
Batch 3/64 loss: -0.13665145635604858
Batch 4/64 loss: -0.10423123836517334
Batch 5/64 loss: -0.1017349362373352
Batch 6/64 loss: -0.09297120571136475
Batch 7/64 loss: -0.11602109670639038
Batch 8/64 loss: -0.11477965116500854
Batch 9/64 loss: -0.11909991502761841
Batch 10/64 loss: -0.08261370658874512
Batch 11/64 loss: -0.11515682935714722
Batch 12/64 loss: -0.10588258504867554
Batch 13/64 loss: -0.1120373010635376
Batch 14/64 loss: -0.11174464225769043
Batch 15/64 loss: -0.09748423099517822
Batch 16/64 loss: -0.12437808513641357
Batch 17/64 loss: -0.12693804502487183
Batch 18/64 loss: -0.11822056770324707
Batch 19/64 loss: -0.10842365026473999
Batch 20/64 loss: -0.12374454736709595
Batch 21/64 loss: -0.10180491209030151
Batch 22/64 loss: -0.09627401828765869
Batch 23/64 loss: -0.08446800708770752
Batch 24/64 loss: -0.10453265905380249
Batch 25/64 loss: -0.12458598613739014
Batch 26/64 loss: -0.11448818445205688
Batch 27/64 loss: -0.10743236541748047
Batch 28/64 loss: -0.08177030086517334
Batch 29/64 loss: -0.10085946321487427
Batch 30/64 loss: -0.10550928115844727
Batch 31/64 loss: -0.1146928071975708
Batch 32/64 loss: -0.1126973032951355
Batch 33/64 loss: -0.10041284561157227
Batch 34/64 loss: -0.1076514720916748
Batch 35/64 loss: -0.10263282060623169
Batch 36/64 loss: -0.0708545446395874
Batch 37/64 loss: -0.09835004806518555
Batch 38/64 loss: -0.11760622262954712
Batch 39/64 loss: -0.08295780420303345
Batch 40/64 loss: -0.09782618284225464
Batch 41/64 loss: -0.10306131839752197
Batch 42/64 loss: -0.0996694564819336
Batch 43/64 loss: -0.10223865509033203
Batch 44/64 loss: -0.12109410762786865
Batch 45/64 loss: -0.10723340511322021
Batch 46/64 loss: -0.115084707736969
Batch 47/64 loss: -0.1150813102722168
Batch 48/64 loss: -0.12466508150100708
Batch 49/64 loss: -0.08742177486419678
Batch 50/64 loss: -0.08145034313201904
Batch 51/64 loss: -0.11434686183929443
Batch 52/64 loss: -0.10348796844482422
Batch 53/64 loss: -0.10026657581329346
Batch 54/64 loss: -0.10321986675262451
Batch 55/64 loss: -0.0964580774307251
Batch 56/64 loss: -0.0974472165107727
Batch 57/64 loss: -0.10263866186141968
Batch 58/64 loss: -0.11890256404876709
Batch 59/64 loss: -0.10829722881317139
Batch 60/64 loss: -0.09851652383804321
Batch 61/64 loss: -0.10543882846832275
Batch 62/64 loss: -0.12113350629806519
Batch 63/64 loss: -0.11477208137512207
Batch 64/64 loss: -0.11458718776702881
Epoch 239  Train loss: -0.10649506952248368  Val loss: -0.02393948827002876
Epoch 240
-------------------------------
Batch 1/64 loss: -0.12027275562286377
Batch 2/64 loss: -0.13268166780471802
Batch 3/64 loss: -0.11312150955200195
Batch 4/64 loss: -0.1028296947479248
Batch 5/64 loss: -0.11925232410430908
Batch 6/64 loss: -0.11973559856414795
Batch 7/64 loss: -0.12102985382080078
Batch 8/64 loss: -0.12931859493255615
Batch 9/64 loss: -0.11085909605026245
Batch 10/64 loss: -0.08420765399932861
Batch 11/64 loss: -0.1145893931388855
Batch 12/64 loss: -0.11586356163024902
Batch 13/64 loss: -0.11857479810714722
Batch 14/64 loss: -0.09551727771759033
Batch 15/64 loss: -0.11981141567230225
Batch 16/64 loss: -0.09820103645324707
Batch 17/64 loss: -0.11811703443527222
Batch 18/64 loss: -0.0856102705001831
Batch 19/64 loss: -0.11217093467712402
Batch 20/64 loss: -0.10861539840698242
Batch 21/64 loss: -0.12143611907958984
Batch 22/64 loss: -0.10468876361846924
Batch 23/64 loss: -0.11593204736709595
Batch 24/64 loss: -0.09897100925445557
Batch 25/64 loss: -0.09848523139953613
Batch 26/64 loss: -0.11650365591049194
Batch 27/64 loss: -0.08935970067977905
Batch 28/64 loss: -0.11745238304138184
Batch 29/64 loss: -0.11443400382995605
Batch 30/64 loss: -0.09543925523757935
Batch 31/64 loss: -0.11641383171081543
Batch 32/64 loss: -0.10609591007232666
Batch 33/64 loss: -0.09715545177459717
Batch 34/64 loss: -0.12573760747909546
Batch 35/64 loss: -0.11680668592453003
Batch 36/64 loss: -0.10326886177062988
Batch 37/64 loss: -0.11747705936431885
Batch 38/64 loss: -0.12499195337295532
Batch 39/64 loss: -0.10565918684005737
Batch 40/64 loss: -0.0945175290107727
Batch 41/64 loss: -0.10788977146148682
Batch 42/64 loss: -0.11122733354568481
Batch 43/64 loss: -0.11107450723648071
Batch 44/64 loss: -0.09999436140060425
Batch 45/64 loss: -0.09821808338165283
Batch 46/64 loss: -0.10275673866271973
Batch 47/64 loss: -0.11409169435501099
Batch 48/64 loss: -0.11106842756271362
Batch 49/64 loss: -0.11557674407958984
Batch 50/64 loss: -0.09970736503601074
Batch 51/64 loss: -0.09692144393920898
Batch 52/64 loss: -0.11140275001525879
Batch 53/64 loss: -0.10591894388198853
Batch 54/64 loss: -0.09558475017547607
Batch 55/64 loss: -0.11323165893554688
Batch 56/64 loss: -0.08998149633407593
Batch 57/64 loss: -0.11621910333633423
Batch 58/64 loss: -0.10096502304077148
Batch 59/64 loss: -0.11773115396499634
Batch 60/64 loss: -0.10145038366317749
Batch 61/64 loss: -0.10946929454803467
Batch 62/64 loss: -0.11497199535369873
Batch 63/64 loss: -0.1150239109992981
Batch 64/64 loss: -0.08321040868759155
Epoch 240  Train loss: -0.1089269154212054  Val loss: -0.019460816973263455
Epoch 241
-------------------------------
Batch 1/64 loss: -0.1116599440574646
Batch 2/64 loss: -0.11615300178527832
Batch 3/64 loss: -0.08540260791778564
Batch 4/64 loss: -0.1274893879890442
Batch 5/64 loss: -0.11150562763214111
Batch 6/64 loss: -0.12166547775268555
Batch 7/64 loss: -0.11583191156387329
Batch 8/64 loss: -0.11473453044891357
Batch 9/64 loss: -0.08368468284606934
Batch 10/64 loss: -0.10353058576583862
Batch 11/64 loss: -0.08501577377319336
Batch 12/64 loss: -0.09168261289596558
Batch 13/64 loss: -0.08842772245407104
Batch 14/64 loss: -0.08876222372055054
Batch 15/64 loss: -0.11038786172866821
Batch 16/64 loss: -0.09572434425354004
Batch 17/64 loss: -0.10032576322555542
Batch 18/64 loss: -0.09833657741546631
Batch 19/64 loss: -0.11065465211868286
Batch 20/64 loss: -0.1300184726715088
Batch 21/64 loss: -0.09253561496734619
Batch 22/64 loss: -0.11293888092041016
Batch 23/64 loss: -0.1015617847442627
Batch 24/64 loss: -0.10446643829345703
Batch 25/64 loss: -0.0849912166595459
Batch 26/64 loss: -0.08727794885635376
Batch 27/64 loss: -0.11216235160827637
Batch 28/64 loss: -0.09867125749588013
Batch 29/64 loss: -0.10004270076751709
Batch 30/64 loss: -0.11779254674911499
Batch 31/64 loss: -0.11542195081710815
Batch 32/64 loss: -0.08757674694061279
Batch 33/64 loss: -0.08605027198791504
Batch 34/64 loss: -0.09749627113342285
Batch 35/64 loss: -0.104736328125
Batch 36/64 loss: -0.10892200469970703
Batch 37/64 loss: -0.1145278811454773
Batch 38/64 loss: -0.11589592695236206
Batch 39/64 loss: -0.11498218774795532
Batch 40/64 loss: -0.11638492345809937
Batch 41/64 loss: -0.11689603328704834
Batch 42/64 loss: -0.09754061698913574
Batch 43/64 loss: -0.11015492677688599
Batch 44/64 loss: -0.11033296585083008
Batch 45/64 loss: -0.12289696931838989
Batch 46/64 loss: -0.11427199840545654
Batch 47/64 loss: -0.10778343677520752
Batch 48/64 loss: -0.1145319938659668
Batch 49/64 loss: -0.08503937721252441
Batch 50/64 loss: -0.11330544948577881
Batch 51/64 loss: -0.10784482955932617
Batch 52/64 loss: -0.10340100526809692
Batch 53/64 loss: -0.1036112904548645
Batch 54/64 loss: -0.10494714975357056
Batch 55/64 loss: -0.11875063180923462
Batch 56/64 loss: -0.10483014583587646
Batch 57/64 loss: -0.1220099925994873
Batch 58/64 loss: -0.10890525579452515
Batch 59/64 loss: -0.13060587644577026
Batch 60/64 loss: -0.09998685121536255
Batch 61/64 loss: -0.08123809099197388
Batch 62/64 loss: -0.1048845648765564
Batch 63/64 loss: -0.09676587581634521
Batch 64/64 loss: -0.0963207483291626
Epoch 241  Train loss: -0.10538360605052874  Val loss: -0.020913998695583278
Epoch 242
-------------------------------
Batch 1/64 loss: -0.10352963209152222
Batch 2/64 loss: -0.11225509643554688
Batch 3/64 loss: -0.12043869495391846
Batch 4/64 loss: -0.10777223110198975
Batch 5/64 loss: -0.11024683713912964
Batch 6/64 loss: -0.1010594367980957
Batch 7/64 loss: -0.11895805597305298
Batch 8/64 loss: -0.10992032289505005
Batch 9/64 loss: -0.10552150011062622
Batch 10/64 loss: -0.11343169212341309
Batch 11/64 loss: -0.11336296796798706
Batch 12/64 loss: -0.13150310516357422
Batch 13/64 loss: -0.09470444917678833
Batch 14/64 loss: -0.1303548812866211
Batch 15/64 loss: -0.09670794010162354
Batch 16/64 loss: -0.11706322431564331
Batch 17/64 loss: -0.0962103009223938
Batch 18/64 loss: -0.11030387878417969
Batch 19/64 loss: -0.10144937038421631
Batch 20/64 loss: -0.1209058165550232
Batch 21/64 loss: -0.1286325454711914
Batch 22/64 loss: -0.09957176446914673
Batch 23/64 loss: -0.10924053192138672
Batch 24/64 loss: -0.10993003845214844
Batch 25/64 loss: -0.10170912742614746
Batch 26/64 loss: -0.09025663137435913
Batch 27/64 loss: -0.09846746921539307
Batch 28/64 loss: -0.10179746150970459
Batch 29/64 loss: -0.10818314552307129
Batch 30/64 loss: -0.09979349374771118
Batch 31/64 loss: -0.10736054182052612
Batch 32/64 loss: -0.1123153567314148
Batch 33/64 loss: -0.10657888650894165
Batch 34/64 loss: -0.12229806184768677
Batch 35/64 loss: -0.12027424573898315
Batch 36/64 loss: -0.11301136016845703
Batch 37/64 loss: -0.1156846284866333
Batch 38/64 loss: -0.10132259130477905
Batch 39/64 loss: -0.09807312488555908
Batch 40/64 loss: -0.11199253797531128
Batch 41/64 loss: -0.10591316223144531
Batch 42/64 loss: -0.11600536108016968
Batch 43/64 loss: -0.11893081665039062
Batch 44/64 loss: -0.09719043970108032
Batch 45/64 loss: -0.11018598079681396
Batch 46/64 loss: -0.0810350775718689
Batch 47/64 loss: -0.1049954891204834
Batch 48/64 loss: -0.059981465339660645
Batch 49/64 loss: -0.11056220531463623
Batch 50/64 loss: -0.09369587898254395
Batch 51/64 loss: -0.10868597030639648
Batch 52/64 loss: -0.09271526336669922
Batch 53/64 loss: -0.09548896551132202
Batch 54/64 loss: -0.11407184600830078
Batch 55/64 loss: -0.11679500341415405
Batch 56/64 loss: -0.10673058032989502
Batch 57/64 loss: -0.09591293334960938
Batch 58/64 loss: -0.10251063108444214
Batch 59/64 loss: -0.11975240707397461
Batch 60/64 loss: -0.1130753755569458
Batch 61/64 loss: -0.10643792152404785
Batch 62/64 loss: -0.0856316089630127
Batch 63/64 loss: -0.11675441265106201
Batch 64/64 loss: -0.10915666818618774
Epoch 242  Train loss: -0.10709206707337324  Val loss: -0.02085327528596334
Epoch 243
-------------------------------
Batch 1/64 loss: -0.12818396091461182
Batch 2/64 loss: -0.12019491195678711
Batch 3/64 loss: -0.08404779434204102
Batch 4/64 loss: -0.12074333429336548
Batch 5/64 loss: -0.1025463342666626
Batch 6/64 loss: -0.12067902088165283
Batch 7/64 loss: -0.1006845235824585
Batch 8/64 loss: -0.09777277708053589
Batch 9/64 loss: -0.11567080020904541
Batch 10/64 loss: -0.0909661054611206
Batch 11/64 loss: -0.09810179471969604
Batch 12/64 loss: -0.10952520370483398
Batch 13/64 loss: -0.1155051589012146
Batch 14/64 loss: -0.11260902881622314
Batch 15/64 loss: -0.1120452880859375
Batch 16/64 loss: -0.11117368936538696
Batch 17/64 loss: -0.10077488422393799
Batch 18/64 loss: -0.12999385595321655
Batch 19/64 loss: -0.11907625198364258
Batch 20/64 loss: -0.0999029278755188
Batch 21/64 loss: -0.12985748052597046
Batch 22/64 loss: -0.11586350202560425
Batch 23/64 loss: -0.11457663774490356
Batch 24/64 loss: -0.10697966814041138
Batch 25/64 loss: -0.10038304328918457
Batch 26/64 loss: -0.09302705526351929
Batch 27/64 loss: -0.1128007173538208
Batch 28/64 loss: -0.11626994609832764
Batch 29/64 loss: -0.10229295492172241
Batch 30/64 loss: -0.0952913761138916
Batch 31/64 loss: -0.10808801651000977
Batch 32/64 loss: -0.10423529148101807
Batch 33/64 loss: -0.10343056917190552
Batch 34/64 loss: -0.12969350814819336
Batch 35/64 loss: -0.1088457703590393
Batch 36/64 loss: -0.1001695990562439
Batch 37/64 loss: -0.1251964569091797
Batch 38/64 loss: -0.11308705806732178
Batch 39/64 loss: -0.10184037685394287
Batch 40/64 loss: -0.08809220790863037
Batch 41/64 loss: -0.08090358972549438
Batch 42/64 loss: -0.11460202932357788
Batch 43/64 loss: -0.10458612442016602
Batch 44/64 loss: -0.12402617931365967
Batch 45/64 loss: -0.11623382568359375
Batch 46/64 loss: -0.11880600452423096
Batch 47/64 loss: -0.11730027198791504
Batch 48/64 loss: -0.08674591779708862
Batch 49/64 loss: -0.09168797731399536
Batch 50/64 loss: -0.09993702173233032
Batch 51/64 loss: -0.09333217144012451
Batch 52/64 loss: -0.12319737672805786
Batch 53/64 loss: -0.0941055417060852
Batch 54/64 loss: -0.12190145254135132
Batch 55/64 loss: -0.10070502758026123
Batch 56/64 loss: -0.10089725255966187
Batch 57/64 loss: -0.10976541042327881
Batch 58/64 loss: -0.10717427730560303
Batch 59/64 loss: -0.10616588592529297
Batch 60/64 loss: -0.11380863189697266
Batch 61/64 loss: -0.11018472909927368
Batch 62/64 loss: -0.10266441106796265
Batch 63/64 loss: -0.09925532341003418
Batch 64/64 loss: -0.11696445941925049
Epoch 243  Train loss: -0.10801456722558714  Val loss: -0.023496922963263653
Epoch 244
-------------------------------
Batch 1/64 loss: -0.11796581745147705
Batch 2/64 loss: -0.1149834394454956
Batch 3/64 loss: -0.0874931812286377
Batch 4/64 loss: -0.0864744782447815
Batch 5/64 loss: -0.11028718948364258
Batch 6/64 loss: -0.1243818998336792
Batch 7/64 loss: -0.10849946737289429
Batch 8/64 loss: -0.11119085550308228
Batch 9/64 loss: -0.06575274467468262
Batch 10/64 loss: -0.12837713956832886
Batch 11/64 loss: -0.09868842363357544
Batch 12/64 loss: -0.10590428113937378
Batch 13/64 loss: -0.12100547552108765
Batch 14/64 loss: -0.11670619249343872
Batch 15/64 loss: -0.11877888441085815
Batch 16/64 loss: -0.11342853307723999
Batch 17/64 loss: -0.09515804052352905
Batch 18/64 loss: -0.11037564277648926
Batch 19/64 loss: -0.12183403968811035
Batch 20/64 loss: -0.11261975765228271
Batch 21/64 loss: -0.11395841836929321
Batch 22/64 loss: -0.0913240909576416
Batch 23/64 loss: -0.10083615779876709
Batch 24/64 loss: -0.11350429058074951
Batch 25/64 loss: -0.11052858829498291
Batch 26/64 loss: -0.11514431238174438
Batch 27/64 loss: -0.09449821710586548
Batch 28/64 loss: -0.1290416121482849
Batch 29/64 loss: -0.10566544532775879
Batch 30/64 loss: -0.10260027647018433
Batch 31/64 loss: -0.12013095617294312
Batch 32/64 loss: -0.10654151439666748
Batch 33/64 loss: -0.10775220394134521
Batch 34/64 loss: -0.0998915433883667
Batch 35/64 loss: -0.08537304401397705
Batch 36/64 loss: -0.12920475006103516
Batch 37/64 loss: -0.11493158340454102
Batch 38/64 loss: -0.09437084197998047
Batch 39/64 loss: -0.11232364177703857
Batch 40/64 loss: -0.09675359725952148
Batch 41/64 loss: -0.11349886655807495
Batch 42/64 loss: -0.1083562970161438
Batch 43/64 loss: -0.10802417993545532
Batch 44/64 loss: -0.10213989019393921
Batch 45/64 loss: -0.11507248878479004
Batch 46/64 loss: -0.10354578495025635
Batch 47/64 loss: -0.08374512195587158
Batch 48/64 loss: -0.0979965329170227
Batch 49/64 loss: -0.11526119709014893
Batch 50/64 loss: -0.09530222415924072
Batch 51/64 loss: -0.09808140993118286
Batch 52/64 loss: -0.11098796129226685
Batch 53/64 loss: -0.10483664274215698
Batch 54/64 loss: -0.0814087986946106
Batch 55/64 loss: -0.08894854784011841
Batch 56/64 loss: -0.08014553785324097
Batch 57/64 loss: -0.11122429370880127
Batch 58/64 loss: -0.10229712724685669
Batch 59/64 loss: -0.11068922281265259
Batch 60/64 loss: -0.0998498797416687
Batch 61/64 loss: -0.10944771766662598
Batch 62/64 loss: -0.11070990562438965
Batch 63/64 loss: -0.10024058818817139
Batch 64/64 loss: -0.11797010898590088
Epoch 244  Train loss: -0.10595401361876843  Val loss: -0.01583172570389161
Epoch 245
-------------------------------
Batch 1/64 loss: -0.09849268198013306
Batch 2/64 loss: -0.08792316913604736
Batch 3/64 loss: -0.1188843846321106
Batch 4/64 loss: -0.09877759218215942
Batch 5/64 loss: -0.128525972366333
Batch 6/64 loss: -0.12561923265457153
Batch 7/64 loss: -0.1199040412902832
Batch 8/64 loss: -0.11173176765441895
Batch 9/64 loss: -0.09980994462966919
Batch 10/64 loss: -0.12334299087524414
Batch 11/64 loss: -0.12921440601348877
Batch 12/64 loss: -0.12301391363143921
Batch 13/64 loss: -0.11873513460159302
Batch 14/64 loss: -0.09115469455718994
Batch 15/64 loss: -0.12350112199783325
Batch 16/64 loss: -0.12069547176361084
Batch 17/64 loss: -0.08958899974822998
Batch 18/64 loss: -0.10946637392044067
Batch 19/64 loss: -0.10666078329086304
Batch 20/64 loss: -0.10342764854431152
Batch 21/64 loss: -0.09970098733901978
Batch 22/64 loss: -0.10973870754241943
Batch 23/64 loss: -0.11425626277923584
Batch 24/64 loss: -0.1230042576789856
Batch 25/64 loss: -0.1145777702331543
Batch 26/64 loss: -0.09685873985290527
Batch 27/64 loss: -0.12209856510162354
Batch 28/64 loss: -0.09553366899490356
Batch 29/64 loss: -0.10027647018432617
Batch 30/64 loss: -0.1120462417602539
Batch 31/64 loss: -0.11213630437850952
Batch 32/64 loss: -0.10764235258102417
Batch 33/64 loss: -0.11251747608184814
Batch 34/64 loss: -0.10210025310516357
Batch 35/64 loss: -0.11043697595596313
Batch 36/64 loss: -0.11430978775024414
Batch 37/64 loss: -0.1142534613609314
Batch 38/64 loss: -0.08566749095916748
Batch 39/64 loss: -0.10933190584182739
Batch 40/64 loss: -0.10985714197158813
Batch 41/64 loss: -0.10855847597122192
Batch 42/64 loss: -0.11727738380432129
Batch 43/64 loss: -0.11604732275009155
Batch 44/64 loss: -0.12294352054595947
Batch 45/64 loss: -0.09850597381591797
Batch 46/64 loss: -0.0811581015586853
Batch 47/64 loss: -0.1310577392578125
Batch 48/64 loss: -0.11329400539398193
Batch 49/64 loss: -0.11831676959991455
Batch 50/64 loss: -0.09391778707504272
Batch 51/64 loss: -0.12911885976791382
Batch 52/64 loss: -0.09765416383743286
Batch 53/64 loss: -0.10175681114196777
Batch 54/64 loss: -0.11652415990829468
Batch 55/64 loss: -0.11177510023117065
Batch 56/64 loss: -0.10207498073577881
Batch 57/64 loss: -0.08997237682342529
Batch 58/64 loss: -0.09234410524368286
Batch 59/64 loss: -0.11239802837371826
Batch 60/64 loss: -0.11636000871658325
Batch 61/64 loss: -0.11093437671661377
Batch 62/64 loss: -0.11645174026489258
Batch 63/64 loss: -0.09362959861755371
Batch 64/64 loss: -0.08001208305358887
Epoch 245  Train loss: -0.10897091136259192  Val loss: -0.022586969985175377
Epoch 246
-------------------------------
Batch 1/64 loss: -0.12300610542297363
Batch 2/64 loss: -0.1131676435470581
Batch 3/64 loss: -0.12208640575408936
Batch 4/64 loss: -0.11635106801986694
Batch 5/64 loss: -0.10076391696929932
Batch 6/64 loss: -0.0882798433303833
Batch 7/64 loss: -0.10368090867996216
Batch 8/64 loss: -0.11499989032745361
Batch 9/64 loss: -0.0964708924293518
Batch 10/64 loss: -0.12158536911010742
Batch 11/64 loss: -0.1137702465057373
Batch 12/64 loss: -0.08899182081222534
Batch 13/64 loss: -0.10274010896682739
Batch 14/64 loss: -0.11576831340789795
Batch 15/64 loss: -0.1292790174484253
Batch 16/64 loss: -0.11631518602371216
Batch 17/64 loss: -0.09871178865432739
Batch 18/64 loss: -0.1019129753112793
Batch 19/64 loss: -0.12163901329040527
Batch 20/64 loss: -0.12960189580917358
Batch 21/64 loss: -0.09874063730239868
Batch 22/64 loss: -0.09858918190002441
Batch 23/64 loss: -0.10984843969345093
Batch 24/64 loss: -0.10518980026245117
Batch 25/64 loss: -0.12640804052352905
Batch 26/64 loss: -0.11551773548126221
Batch 27/64 loss: -0.1252158284187317
Batch 28/64 loss: -0.12385332584381104
Batch 29/64 loss: -0.1286388635635376
Batch 30/64 loss: -0.10826462507247925
Batch 31/64 loss: -0.11174434423446655
Batch 32/64 loss: -0.0838555097579956
Batch 33/64 loss: -0.1276310682296753
Batch 34/64 loss: -0.08209550380706787
Batch 35/64 loss: -0.09407234191894531
Batch 36/64 loss: -0.10750937461853027
Batch 37/64 loss: -0.11482274532318115
Batch 38/64 loss: -0.09728801250457764
Batch 39/64 loss: -0.11999952793121338
Batch 40/64 loss: -0.11324942111968994
Batch 41/64 loss: -0.11300516128540039
Batch 42/64 loss: -0.08556389808654785
Batch 43/64 loss: -0.1101529598236084
Batch 44/64 loss: -0.11352372169494629
Batch 45/64 loss: -0.10784465074539185
Batch 46/64 loss: -0.12512987852096558
Batch 47/64 loss: -0.10164785385131836
Batch 48/64 loss: -0.11214256286621094
Batch 49/64 loss: -0.10850870609283447
Batch 50/64 loss: -0.11919963359832764
Batch 51/64 loss: -0.10390597581863403
Batch 52/64 loss: -0.10941934585571289
Batch 53/64 loss: -0.09155845642089844
Batch 54/64 loss: -0.1035194993019104
Batch 55/64 loss: -0.12822657823562622
Batch 56/64 loss: -0.12064182758331299
Batch 57/64 loss: -0.1162487268447876
Batch 58/64 loss: -0.10477292537689209
Batch 59/64 loss: -0.10322469472885132
Batch 60/64 loss: -0.08849793672561646
Batch 61/64 loss: -0.07593727111816406
Batch 62/64 loss: -0.10815602540969849
Batch 63/64 loss: -0.11917436122894287
Batch 64/64 loss: -0.10944575071334839
Epoch 246  Train loss: -0.10923519531885782  Val loss: -0.019638496371069316
Epoch 247
-------------------------------
Batch 1/64 loss: -0.12321823835372925
Batch 2/64 loss: -0.1335446834564209
Batch 3/64 loss: -0.12537848949432373
Batch 4/64 loss: -0.10240888595581055
Batch 5/64 loss: -0.12635648250579834
Batch 6/64 loss: -0.11805468797683716
Batch 7/64 loss: -0.12366640567779541
Batch 8/64 loss: -0.11556923389434814
Batch 9/64 loss: -0.11450225114822388
Batch 10/64 loss: -0.09081858396530151
Batch 11/64 loss: -0.11006605625152588
Batch 12/64 loss: -0.12102413177490234
Batch 13/64 loss: -0.11438381671905518
Batch 14/64 loss: -0.11634784936904907
Batch 15/64 loss: -0.1169663667678833
Batch 16/64 loss: -0.11094659566879272
Batch 17/64 loss: -0.10389691591262817
Batch 18/64 loss: -0.11863553524017334
Batch 19/64 loss: -0.10812205076217651
Batch 20/64 loss: -0.10801112651824951
Batch 21/64 loss: -0.11883664131164551
Batch 22/64 loss: -0.11187237501144409
Batch 23/64 loss: -0.1192237138748169
Batch 24/64 loss: -0.12978863716125488
Batch 25/64 loss: -0.1233397126197815
Batch 26/64 loss: -0.1401011347770691
Batch 27/64 loss: -0.12172448635101318
Batch 28/64 loss: -0.10878270864486694
Batch 29/64 loss: -0.09081900119781494
Batch 30/64 loss: -0.11697643995285034
Batch 31/64 loss: -0.11962741613388062
Batch 32/64 loss: -0.12699663639068604
Batch 33/64 loss: -0.10888522863388062
Batch 34/64 loss: -0.12134009599685669
Batch 35/64 loss: -0.11474406719207764
Batch 36/64 loss: -0.13216334581375122
Batch 37/64 loss: -0.1118016242980957
Batch 38/64 loss: -0.10885006189346313
Batch 39/64 loss: -0.12176430225372314
Batch 40/64 loss: -0.07779324054718018
Batch 41/64 loss: -0.10221141576766968
Batch 42/64 loss: -0.08891057968139648
Batch 43/64 loss: -0.08638089895248413
Batch 44/64 loss: -0.11779135465621948
Batch 45/64 loss: -0.0983535647392273
Batch 46/64 loss: -0.10623490810394287
Batch 47/64 loss: -0.11489665508270264
Batch 48/64 loss: -0.10127091407775879
Batch 49/64 loss: -0.10572171211242676
Batch 50/64 loss: -0.10232239961624146
Batch 51/64 loss: -0.09294688701629639
Batch 52/64 loss: -0.1194189190864563
Batch 53/64 loss: -0.11937743425369263
Batch 54/64 loss: -0.11663997173309326
Batch 55/64 loss: -0.11252307891845703
Batch 56/64 loss: -0.10522735118865967
Batch 57/64 loss: -0.11154389381408691
Batch 58/64 loss: -0.1301671266555786
Batch 59/64 loss: -0.1237112283706665
Batch 60/64 loss: -0.07978731393814087
Batch 61/64 loss: -0.11446517705917358
Batch 62/64 loss: -0.10843127965927124
Batch 63/64 loss: -0.11477559804916382
Batch 64/64 loss: -0.08750587701797485
Epoch 247  Train loss: -0.11240922867083082  Val loss: -0.023181224197046862
Epoch 248
-------------------------------
Batch 1/64 loss: -0.11809307336807251
Batch 2/64 loss: -0.11023104190826416
Batch 3/64 loss: -0.12135523557662964
Batch 4/64 loss: -0.10074245929718018
Batch 5/64 loss: -0.11807042360305786
Batch 6/64 loss: -0.1312350034713745
Batch 7/64 loss: -0.10998433828353882
Batch 8/64 loss: -0.12099605798721313
Batch 9/64 loss: -0.1265602707862854
Batch 10/64 loss: -0.11452162265777588
Batch 11/64 loss: -0.1282634735107422
Batch 12/64 loss: -0.10274481773376465
Batch 13/64 loss: -0.10424822568893433
Batch 14/64 loss: -0.10304117202758789
Batch 15/64 loss: -0.12328308820724487
Batch 16/64 loss: -0.12177085876464844
Batch 17/64 loss: -0.10378915071487427
Batch 18/64 loss: -0.08952146768569946
Batch 19/64 loss: -0.10411602258682251
Batch 20/64 loss: -0.10574984550476074
Batch 21/64 loss: -0.11211735010147095
Batch 22/64 loss: -0.12007665634155273
Batch 23/64 loss: -0.11745744943618774
Batch 24/64 loss: -0.11660528182983398
Batch 25/64 loss: -0.10033810138702393
Batch 26/64 loss: -0.12180483341217041
Batch 27/64 loss: -0.10140842199325562
Batch 28/64 loss: -0.10191309452056885
Batch 29/64 loss: -0.08758914470672607
Batch 30/64 loss: -0.1150166392326355
Batch 31/64 loss: -0.1253955364227295
Batch 32/64 loss: -0.11269664764404297
Batch 33/64 loss: -0.10594338178634644
Batch 34/64 loss: -0.08422714471817017
Batch 35/64 loss: -0.08908939361572266
Batch 36/64 loss: -0.12309122085571289
Batch 37/64 loss: -0.10096889734268188
Batch 38/64 loss: -0.11235266923904419
Batch 39/64 loss: -0.09678351879119873
Batch 40/64 loss: -0.10442769527435303
Batch 41/64 loss: -0.12111121416091919
Batch 42/64 loss: -0.11736506223678589
Batch 43/64 loss: -0.09807538986206055
Batch 44/64 loss: -0.09765928983688354
Batch 45/64 loss: -0.11340123414993286
Batch 46/64 loss: -0.11348509788513184
Batch 47/64 loss: -0.10338050127029419
Batch 48/64 loss: -0.13335692882537842
Batch 49/64 loss: -0.10789227485656738
Batch 50/64 loss: -0.1302039623260498
Batch 51/64 loss: -0.10871511697769165
Batch 52/64 loss: -0.10122150182723999
Batch 53/64 loss: -0.11324012279510498
Batch 54/64 loss: -0.11135458946228027
Batch 55/64 loss: -0.11871904134750366
Batch 56/64 loss: -0.11513715982437134
Batch 57/64 loss: -0.1398453712463379
Batch 58/64 loss: -0.11940312385559082
Batch 59/64 loss: -0.1255180835723877
Batch 60/64 loss: -0.11361128091812134
Batch 61/64 loss: -0.11022520065307617
Batch 62/64 loss: -0.09570366144180298
Batch 63/64 loss: -0.12928885221481323
Batch 64/64 loss: -0.09031480550765991
Epoch 248  Train loss: -0.1115807199010662  Val loss: -0.02104949029450564
Epoch 249
-------------------------------
Batch 1/64 loss: -0.09929764270782471
Batch 2/64 loss: -0.12418150901794434
Batch 3/64 loss: -0.10233193635940552
Batch 4/64 loss: -0.13983213901519775
Batch 5/64 loss: -0.11817657947540283
Batch 6/64 loss: -0.11877155303955078
Batch 7/64 loss: -0.1077924370765686
Batch 8/64 loss: -0.12435507774353027
Batch 9/64 loss: -0.12134659290313721
Batch 10/64 loss: -0.11888718605041504
Batch 11/64 loss: -0.11557179689407349
Batch 12/64 loss: -0.10135716199874878
Batch 13/64 loss: -0.12984710931777954
Batch 14/64 loss: -0.11127537488937378
Batch 15/64 loss: -0.11337172985076904
Batch 16/64 loss: -0.10230511426925659
Batch 17/64 loss: -0.11793065071105957
Batch 18/64 loss: -0.11885100603103638
Batch 19/64 loss: -0.13238322734832764
Batch 20/64 loss: -0.13315290212631226
Batch 21/64 loss: -0.10439157485961914
Batch 22/64 loss: -0.13298285007476807
Batch 23/64 loss: -0.11465829610824585
Batch 24/64 loss: -0.09624820947647095
Batch 25/64 loss: -0.09862279891967773
Batch 26/64 loss: -0.13953471183776855
Batch 27/64 loss: -0.12357676029205322
Batch 28/64 loss: -0.1162726879119873
Batch 29/64 loss: -0.1196979284286499
Batch 30/64 loss: -0.10660147666931152
Batch 31/64 loss: -0.11371737718582153
Batch 32/64 loss: -0.10920584201812744
Batch 33/64 loss: -0.13262104988098145
Batch 34/64 loss: -0.12606912851333618
Batch 35/64 loss: -0.10912954807281494
Batch 36/64 loss: -0.09107965230941772
Batch 37/64 loss: -0.12271112203598022
Batch 38/64 loss: -0.1182510256767273
Batch 39/64 loss: -0.118938148021698
Batch 40/64 loss: -0.08753687143325806
Batch 41/64 loss: -0.13669145107269287
Batch 42/64 loss: -0.10956567525863647
Batch 43/64 loss: -0.0855138897895813
Batch 44/64 loss: -0.13075602054595947
Batch 45/64 loss: -0.11644840240478516
Batch 46/64 loss: -0.10399007797241211
Batch 47/64 loss: -0.12746810913085938
Batch 48/64 loss: -0.10808444023132324
Batch 49/64 loss: -0.1088569164276123
Batch 50/64 loss: -0.09076941013336182
Batch 51/64 loss: -0.12336242198944092
Batch 52/64 loss: -0.09590232372283936
Batch 53/64 loss: -0.11117804050445557
Batch 54/64 loss: -0.08329159021377563
Batch 55/64 loss: -0.08504599332809448
Batch 56/64 loss: -0.12225091457366943
Batch 57/64 loss: -0.10907852649688721
Batch 58/64 loss: -0.0853833556175232
Batch 59/64 loss: -0.10560095310211182
Batch 60/64 loss: -0.11110341548919678
Batch 61/64 loss: -0.11366939544677734
Batch 62/64 loss: -0.1022343635559082
Batch 63/64 loss: -0.06753802299499512
Batch 64/64 loss: -0.10054147243499756
Epoch 249  Train loss: -0.112032244719711  Val loss: -0.02019720675609366
Epoch 250
-------------------------------
Batch 1/64 loss: -0.10951340198516846
Batch 2/64 loss: -0.08490955829620361
Batch 3/64 loss: -0.11313652992248535
Batch 4/64 loss: -0.12770992517471313
Batch 5/64 loss: -0.10978251695632935
Batch 6/64 loss: -0.11837571859359741
Batch 7/64 loss: -0.12037461996078491
Batch 8/64 loss: -0.1072576642036438
Batch 9/64 loss: -0.11159366369247437
Batch 10/64 loss: -0.11218196153640747
Batch 11/64 loss: -0.12286418676376343
Batch 12/64 loss: -0.11208295822143555
Batch 13/64 loss: -0.1078522801399231
Batch 14/64 loss: -0.09545660018920898
Batch 15/64 loss: -0.1120370626449585
Batch 16/64 loss: -0.12862014770507812
Batch 17/64 loss: -0.12581247091293335
Batch 18/64 loss: -0.10758215188980103
Batch 19/64 loss: -0.11707949638366699
Batch 20/64 loss: -0.12577664852142334
Batch 21/64 loss: -0.14063596725463867
Batch 22/64 loss: -0.10659098625183105
Batch 23/64 loss: -0.12154656648635864
Batch 24/64 loss: -0.10201019048690796
Batch 25/64 loss: -0.08851808309555054
Batch 26/64 loss: -0.09018760919570923
Batch 27/64 loss: -0.12693291902542114
Batch 28/64 loss: -0.11847639083862305
Batch 29/64 loss: -0.10690373182296753
Batch 30/64 loss: -0.09140902757644653
Batch 31/64 loss: -0.12071388959884644
Batch 32/64 loss: -0.11893761157989502
Batch 33/64 loss: -0.1276751160621643
Batch 34/64 loss: -0.12192189693450928
Batch 35/64 loss: -0.09931164979934692
Batch 36/64 loss: -0.10729479789733887
Batch 37/64 loss: -0.10972458124160767
Batch 38/64 loss: -0.11829853057861328
Batch 39/64 loss: -0.12109309434890747
Batch 40/64 loss: -0.0956987738609314
Batch 41/64 loss: -0.11296236515045166
Batch 42/64 loss: -0.11745613813400269
Batch 43/64 loss: -0.11562716960906982
Batch 44/64 loss: -0.1117633581161499
Batch 45/64 loss: -0.093597412109375
Batch 46/64 loss: -0.11541861295700073
Batch 47/64 loss: -0.11915796995162964
Batch 48/64 loss: -0.11349999904632568
Batch 49/64 loss: -0.1049615740776062
Batch 50/64 loss: -0.0989152193069458
Batch 51/64 loss: -0.12227421998977661
Batch 52/64 loss: -0.08869481086730957
Batch 53/64 loss: -0.09811621904373169
Batch 54/64 loss: -0.10069960355758667
Batch 55/64 loss: -0.09127545356750488
Batch 56/64 loss: -0.1137855052947998
Batch 57/64 loss: -0.1073041558265686
Batch 58/64 loss: -0.10394161939620972
Batch 59/64 loss: -0.10258138179779053
Batch 60/64 loss: -0.11007094383239746
Batch 61/64 loss: -0.11333262920379639
Batch 62/64 loss: -0.08376955986022949
Batch 63/64 loss: -0.08878648281097412
Batch 64/64 loss: -0.12475413084030151
Epoch 250  Train loss: -0.11020296438067567  Val loss: -0.0199847844048464
Epoch 251
-------------------------------
Batch 1/64 loss: -0.09693604707717896
Batch 2/64 loss: -0.11164790391921997
Batch 3/64 loss: -0.12523609399795532
Batch 4/64 loss: -0.0983814001083374
Batch 5/64 loss: -0.1376846432685852
Batch 6/64 loss: -0.12524408102035522
Batch 7/64 loss: -0.11172688007354736
Batch 8/64 loss: -0.12295770645141602
Batch 9/64 loss: -0.11364418268203735
Batch 10/64 loss: -0.11764025688171387
Batch 11/64 loss: -0.10708039999008179
Batch 12/64 loss: -0.12559396028518677
Batch 13/64 loss: -0.11935031414031982
Batch 14/64 loss: -0.11622166633605957
Batch 15/64 loss: -0.1241605281829834
Batch 16/64 loss: -0.10570275783538818
Batch 17/64 loss: -0.12870758771896362
Batch 18/64 loss: -0.10633504390716553
Batch 19/64 loss: -0.04631531238555908
Batch 20/64 loss: -0.09299695491790771
Batch 21/64 loss: -0.12406569719314575
Batch 22/64 loss: -0.10884606838226318
Batch 23/64 loss: -0.12333977222442627
Batch 24/64 loss: -0.124356210231781
Batch 25/64 loss: -0.12094098329544067
Batch 26/64 loss: -0.12829464673995972
Batch 27/64 loss: -0.101043701171875
Batch 28/64 loss: -0.08582019805908203
Batch 29/64 loss: -0.13166719675064087
Batch 30/64 loss: -0.11609262228012085
Batch 31/64 loss: -0.12027305364608765
Batch 32/64 loss: -0.10518544912338257
Batch 33/64 loss: -0.12710624933242798
Batch 34/64 loss: -0.11856603622436523
Batch 35/64 loss: -0.11463665962219238
Batch 36/64 loss: -0.11179721355438232
Batch 37/64 loss: -0.10557782649993896
Batch 38/64 loss: -0.11806994676589966
Batch 39/64 loss: -0.12705779075622559
Batch 40/64 loss: -0.12594866752624512
Batch 41/64 loss: -0.11457788944244385
Batch 42/64 loss: -0.10746324062347412
Batch 43/64 loss: -0.11400508880615234
Batch 44/64 loss: -0.11330074071884155
Batch 45/64 loss: -0.11930346488952637
Batch 46/64 loss: -0.0966343879699707
Batch 47/64 loss: -0.10283052921295166
Batch 48/64 loss: -0.10231959819793701
Batch 49/64 loss: -0.12365597486495972
Batch 50/64 loss: -0.12671011686325073
Batch 51/64 loss: -0.1373928189277649
Batch 52/64 loss: -0.1259995698928833
Batch 53/64 loss: -0.12373572587966919
Batch 54/64 loss: -0.11611354351043701
Batch 55/64 loss: -0.1316959261894226
Batch 56/64 loss: -0.11121904850006104
Batch 57/64 loss: -0.09823572635650635
Batch 58/64 loss: -0.09368115663528442
Batch 59/64 loss: -0.12239158153533936
Batch 60/64 loss: -0.12611883878707886
Batch 61/64 loss: -0.09270328283309937
Batch 62/64 loss: -0.10890978574752808
Batch 63/64 loss: -0.10453897714614868
Batch 64/64 loss: -0.09815990924835205
Epoch 251  Train loss: -0.11387304558473475  Val loss: -0.024468991969459244
Epoch 252
-------------------------------
Batch 1/64 loss: -0.11300456523895264
Batch 2/64 loss: -0.11039751768112183
Batch 3/64 loss: -0.10204553604125977
Batch 4/64 loss: -0.12258309125900269
Batch 5/64 loss: -0.12425833940505981
Batch 6/64 loss: -0.1169857382774353
Batch 7/64 loss: -0.13104116916656494
Batch 8/64 loss: -0.12319129705429077
Batch 9/64 loss: -0.09820783138275146
Batch 10/64 loss: -0.1172189712524414
Batch 11/64 loss: -0.10923737287521362
Batch 12/64 loss: -0.10083848237991333
Batch 13/64 loss: -0.09592491388320923
Batch 14/64 loss: -0.11218315362930298
Batch 15/64 loss: -0.10285502672195435
Batch 16/64 loss: -0.09764957427978516
Batch 17/64 loss: -0.0909467339515686
Batch 18/64 loss: -0.11686313152313232
Batch 19/64 loss: -0.11143273115158081
Batch 20/64 loss: -0.12057185173034668
Batch 21/64 loss: -0.09285229444503784
Batch 22/64 loss: -0.11394822597503662
Batch 23/64 loss: -0.12653547525405884
Batch 24/64 loss: -0.1185961365699768
Batch 25/64 loss: -0.12600445747375488
Batch 26/64 loss: -0.09454739093780518
Batch 27/64 loss: -0.13055717945098877
Batch 28/64 loss: -0.1001211404800415
Batch 29/64 loss: -0.11110353469848633
Batch 30/64 loss: -0.12315535545349121
Batch 31/64 loss: -0.11878567934036255
Batch 32/64 loss: -0.12007981538772583
Batch 33/64 loss: -0.11684143543243408
Batch 34/64 loss: -0.11254853010177612
Batch 35/64 loss: -0.11747109889984131
Batch 36/64 loss: -0.11910593509674072
Batch 37/64 loss: -0.12988263368606567
Batch 38/64 loss: -0.11647480726242065
Batch 39/64 loss: -0.1090313196182251
Batch 40/64 loss: -0.1154557466506958
Batch 41/64 loss: -0.0953744649887085
Batch 42/64 loss: -0.12737154960632324
Batch 43/64 loss: -0.11700963973999023
Batch 44/64 loss: -0.11031341552734375
Batch 45/64 loss: -0.13259649276733398
Batch 46/64 loss: -0.11772716045379639
Batch 47/64 loss: -0.10837376117706299
Batch 48/64 loss: -0.10899794101715088
Batch 49/64 loss: -0.10617697238922119
Batch 50/64 loss: -0.1229628324508667
Batch 51/64 loss: -0.10662907361984253
Batch 52/64 loss: -0.13115566968917847
Batch 53/64 loss: -0.10466450452804565
Batch 54/64 loss: -0.09674447774887085
Batch 55/64 loss: -0.10920214653015137
Batch 56/64 loss: -0.12051564455032349
Batch 57/64 loss: -0.12133908271789551
Batch 58/64 loss: -0.09341663122177124
Batch 59/64 loss: -0.10095351934432983
Batch 60/64 loss: -0.11278778314590454
Batch 61/64 loss: -0.10500824451446533
Batch 62/64 loss: -0.09607160091400146
Batch 63/64 loss: -0.11588490009307861
Batch 64/64 loss: -0.0675973892211914
Epoch 252  Train loss: -0.11203938465492398  Val loss: -0.02168970378403811
Epoch 253
-------------------------------
Batch 1/64 loss: -0.11260664463043213
Batch 2/64 loss: -0.11083143949508667
Batch 3/64 loss: -0.12397384643554688
Batch 4/64 loss: -0.11390328407287598
Batch 5/64 loss: -0.09930562973022461
Batch 6/64 loss: -0.11856120824813843
Batch 7/64 loss: -0.12250226736068726
Batch 8/64 loss: -0.10231053829193115
Batch 9/64 loss: -0.09262847900390625
Batch 10/64 loss: -0.12311619520187378
Batch 11/64 loss: -0.12141740322113037
Batch 12/64 loss: -0.09837973117828369
Batch 13/64 loss: -0.12698441743850708
Batch 14/64 loss: -0.10870969295501709
Batch 15/64 loss: -0.08578622341156006
Batch 16/64 loss: -0.1252160668373108
Batch 17/64 loss: -0.1100074052810669
Batch 18/64 loss: -0.1224602460861206
Batch 19/64 loss: -0.12735509872436523
Batch 20/64 loss: -0.11587411165237427
Batch 21/64 loss: -0.11004328727722168
Batch 22/64 loss: -0.12496274709701538
Batch 23/64 loss: -0.10794895887374878
Batch 24/64 loss: -0.12437927722930908
Batch 25/64 loss: -0.1163797378540039
Batch 26/64 loss: -0.11131733655929565
Batch 27/64 loss: -0.12060803174972534
Batch 28/64 loss: -0.12780416011810303
Batch 29/64 loss: -0.0969313383102417
Batch 30/64 loss: -0.11867576837539673
Batch 31/64 loss: -0.10343581438064575
Batch 32/64 loss: -0.12474638223648071
Batch 33/64 loss: -0.12615090608596802
Batch 34/64 loss: -0.12146860361099243
Batch 35/64 loss: -0.12109041213989258
Batch 36/64 loss: -0.10916471481323242
Batch 37/64 loss: -0.11851906776428223
Batch 38/64 loss: -0.10858136415481567
Batch 39/64 loss: -0.11654096841812134
Batch 40/64 loss: -0.10646486282348633
Batch 41/64 loss: -0.11956560611724854
Batch 42/64 loss: -0.10610789060592651
Batch 43/64 loss: -0.1250280737876892
Batch 44/64 loss: -0.11976367235183716
Batch 45/64 loss: -0.11208212375640869
Batch 46/64 loss: -0.12922728061676025
Batch 47/64 loss: -0.11492830514907837
Batch 48/64 loss: -0.13128679990768433
Batch 49/64 loss: -0.1169668436050415
Batch 50/64 loss: -0.11078822612762451
Batch 51/64 loss: -0.13942021131515503
Batch 52/64 loss: -0.09932762384414673
Batch 53/64 loss: -0.11751824617385864
Batch 54/64 loss: -0.12281888723373413
Batch 55/64 loss: -0.10290324687957764
Batch 56/64 loss: -0.13101983070373535
Batch 57/64 loss: -0.10430049896240234
Batch 58/64 loss: -0.11925137042999268
Batch 59/64 loss: -0.10816913843154907
Batch 60/64 loss: -0.09806579351425171
Batch 61/64 loss: -0.11308896541595459
Batch 62/64 loss: -0.11137300729751587
Batch 63/64 loss: -0.11219662427902222
Batch 64/64 loss: -0.10696327686309814
Epoch 253  Train loss: -0.11486375518873626  Val loss: -0.019771987015439065
Epoch 254
-------------------------------
Batch 1/64 loss: -0.11976033449172974
Batch 2/64 loss: -0.10462641716003418
Batch 3/64 loss: -0.11629927158355713
Batch 4/64 loss: -0.11585372686386108
Batch 5/64 loss: -0.1250225305557251
Batch 6/64 loss: -0.11425739526748657
Batch 7/64 loss: -0.10335570573806763
Batch 8/64 loss: -0.13228553533554077
Batch 9/64 loss: -0.12173914909362793
Batch 10/64 loss: -0.12262862920761108
Batch 11/64 loss: -0.12303286790847778
Batch 12/64 loss: -0.11084979772567749
Batch 13/64 loss: -0.08967733383178711
Batch 14/64 loss: -0.11741185188293457
Batch 15/64 loss: -0.12884187698364258
Batch 16/64 loss: -0.09216618537902832
Batch 17/64 loss: -0.10199731588363647
Batch 18/64 loss: -0.12246376276016235
Batch 19/64 loss: -0.11364758014678955
Batch 20/64 loss: -0.13408255577087402
Batch 21/64 loss: -0.09891188144683838
Batch 22/64 loss: -0.1050727367401123
Batch 23/64 loss: -0.11541825532913208
Batch 24/64 loss: -0.12374705076217651
Batch 25/64 loss: -0.11766302585601807
Batch 26/64 loss: -0.11684250831604004
Batch 27/64 loss: -0.09756600856781006
Batch 28/64 loss: -0.11121231317520142
Batch 29/64 loss: -0.11663419008255005
Batch 30/64 loss: -0.1347711682319641
Batch 31/64 loss: -0.09889954328536987
Batch 32/64 loss: -0.12018030881881714
Batch 33/64 loss: -0.13643068075180054
Batch 34/64 loss: -0.10293006896972656
Batch 35/64 loss: -0.12875640392303467
Batch 36/64 loss: -0.14308565855026245
Batch 37/64 loss: -0.11801183223724365
Batch 38/64 loss: -0.10117447376251221
Batch 39/64 loss: -0.11350172758102417
Batch 40/64 loss: -0.13711017370224
Batch 41/64 loss: -0.11997383832931519
Batch 42/64 loss: -0.11431622505187988
Batch 43/64 loss: -0.08957594633102417
Batch 44/64 loss: -0.10945481061935425
Batch 45/64 loss: -0.1205904483795166
Batch 46/64 loss: -0.09566789865493774
Batch 47/64 loss: -0.12797701358795166
Batch 48/64 loss: -0.10247790813446045
Batch 49/64 loss: -0.1032487154006958
Batch 50/64 loss: -0.10060650110244751
Batch 51/64 loss: -0.10152477025985718
Batch 52/64 loss: -0.11820638179779053
Batch 53/64 loss: -0.11579620838165283
Batch 54/64 loss: -0.09100651741027832
Batch 55/64 loss: -0.1039239764213562
Batch 56/64 loss: -0.1295558214187622
Batch 57/64 loss: -0.12194639444351196
Batch 58/64 loss: -0.12188637256622314
Batch 59/64 loss: -0.08666056394577026
Batch 60/64 loss: -0.12494874000549316
Batch 61/64 loss: -0.11355602741241455
Batch 62/64 loss: -0.11870604753494263
Batch 63/64 loss: -0.0945233702659607
Batch 64/64 loss: -0.13012301921844482
Epoch 254  Train loss: -0.11406498189065971  Val loss: -0.02506395693087496
Epoch 255
-------------------------------
Batch 1/64 loss: -0.126520037651062
Batch 2/64 loss: -0.12716269493103027
Batch 3/64 loss: -0.13365817070007324
Batch 4/64 loss: -0.11914181709289551
Batch 5/64 loss: -0.10829877853393555
Batch 6/64 loss: -0.10670727491378784
Batch 7/64 loss: -0.12365216016769409
Batch 8/64 loss: -0.10515725612640381
Batch 9/64 loss: -0.09755057096481323
Batch 10/64 loss: -0.11296218633651733
Batch 11/64 loss: -0.12531042098999023
Batch 12/64 loss: -0.09992009401321411
Batch 13/64 loss: -0.1269446611404419
Batch 14/64 loss: -0.1260892152786255
Batch 15/64 loss: -0.12109816074371338
Batch 16/64 loss: -0.08640140295028687
Batch 17/64 loss: -0.11792242527008057
Batch 18/64 loss: -0.12309551239013672
Batch 19/64 loss: -0.09634840488433838
Batch 20/64 loss: -0.1231614351272583
Batch 21/64 loss: -0.14081484079360962
Batch 22/64 loss: -0.1148759126663208
Batch 23/64 loss: -0.1260683536529541
Batch 24/64 loss: -0.13260924816131592
Batch 25/64 loss: -0.09763854742050171
Batch 26/64 loss: -0.13514107465744019
Batch 27/64 loss: -0.11480534076690674
Batch 28/64 loss: -0.1275486946105957
Batch 29/64 loss: -0.11557173728942871
Batch 30/64 loss: -0.12257790565490723
Batch 31/64 loss: -0.12991666793823242
Batch 32/64 loss: -0.13367629051208496
Batch 33/64 loss: -0.10552054643630981
Batch 34/64 loss: -0.11293119192123413
Batch 35/64 loss: -0.11833477020263672
Batch 36/64 loss: -0.11568725109100342
Batch 37/64 loss: -0.1239667534828186
Batch 38/64 loss: -0.08355426788330078
Batch 39/64 loss: -0.10945332050323486
Batch 40/64 loss: -0.1200333833694458
Batch 41/64 loss: -0.10431748628616333
Batch 42/64 loss: -0.11605781316757202
Batch 43/64 loss: -0.1340896487236023
Batch 44/64 loss: -0.13976770639419556
Batch 45/64 loss: -0.12445652484893799
Batch 46/64 loss: -0.12362349033355713
Batch 47/64 loss: -0.10121965408325195
Batch 48/64 loss: -0.12680071592330933
Batch 49/64 loss: -0.11658477783203125
Batch 50/64 loss: -0.12013798952102661
Batch 51/64 loss: -0.10520714521408081
Batch 52/64 loss: -0.0962303876876831
Batch 53/64 loss: -0.11756086349487305
Batch 54/64 loss: -0.09758734703063965
Batch 55/64 loss: -0.10946017503738403
Batch 56/64 loss: -0.09770399332046509
Batch 57/64 loss: -0.12075656652450562
Batch 58/64 loss: -0.10500001907348633
Batch 59/64 loss: -0.11785370111465454
Batch 60/64 loss: -0.09074723720550537
Batch 61/64 loss: -0.12478286027908325
Batch 62/64 loss: -0.11548179388046265
Batch 63/64 loss: -0.13411927223205566
Batch 64/64 loss: -0.09694695472717285
Epoch 255  Train loss: -0.11607978297214883  Val loss: -0.02057705814486107
Epoch 256
-------------------------------
Batch 1/64 loss: -0.13214343786239624
Batch 2/64 loss: -0.10289120674133301
Batch 3/64 loss: -0.12612998485565186
Batch 4/64 loss: -0.13272809982299805
Batch 5/64 loss: -0.12910282611846924
Batch 6/64 loss: -0.09973889589309692
Batch 7/64 loss: -0.1182292103767395
Batch 8/64 loss: -0.08842885494232178
Batch 9/64 loss: -0.12948322296142578
Batch 10/64 loss: -0.130445659160614
Batch 11/64 loss: -0.14116352796554565
Batch 12/64 loss: -0.12211877107620239
Batch 13/64 loss: -0.12457311153411865
Batch 14/64 loss: -0.121085524559021
Batch 15/64 loss: -0.10232913494110107
Batch 16/64 loss: -0.11931043863296509
Batch 17/64 loss: -0.1239195466041565
Batch 18/64 loss: -0.14232569932937622
Batch 19/64 loss: -0.11554110050201416
Batch 20/64 loss: -0.10367923974990845
Batch 21/64 loss: -0.12413418292999268
Batch 22/64 loss: -0.089851975440979
Batch 23/64 loss: -0.1096453070640564
Batch 24/64 loss: -0.1326916217803955
Batch 25/64 loss: -0.11439985036849976
Batch 26/64 loss: -0.10833513736724854
Batch 27/64 loss: -0.09489738941192627
Batch 28/64 loss: -0.12375015020370483
Batch 29/64 loss: -0.11873948574066162
Batch 30/64 loss: -0.13192850351333618
Batch 31/64 loss: -0.10679113864898682
Batch 32/64 loss: -0.10986548662185669
Batch 33/64 loss: -0.11276131868362427
Batch 34/64 loss: -0.10980856418609619
Batch 35/64 loss: -0.10842728614807129
Batch 36/64 loss: -0.11268645524978638
Batch 37/64 loss: -0.09362649917602539
Batch 38/64 loss: -0.12165051698684692
Batch 39/64 loss: -0.10754108428955078
Batch 40/64 loss: -0.09742200374603271
Batch 41/64 loss: -0.09753632545471191
Batch 42/64 loss: -0.12444829940795898
Batch 43/64 loss: -0.1135181188583374
Batch 44/64 loss: -0.11442792415618896
Batch 45/64 loss: -0.11756592988967896
Batch 46/64 loss: -0.10183179378509521
Batch 47/64 loss: -0.11116307973861694
Batch 48/64 loss: -0.10194581747055054
Batch 49/64 loss: -0.1289043426513672
Batch 50/64 loss: -0.10208272933959961
Batch 51/64 loss: -0.11192190647125244
Batch 52/64 loss: -0.11164844036102295
Batch 53/64 loss: -0.11367148160934448
Batch 54/64 loss: -0.11614274978637695
Batch 55/64 loss: -0.11051058769226074
Batch 56/64 loss: -0.12387126684188843
Batch 57/64 loss: -0.10735994577407837
Batch 58/64 loss: -0.11747086048126221
Batch 59/64 loss: -0.10555273294448853
Batch 60/64 loss: -0.11946338415145874
Batch 61/64 loss: -0.11343604326248169
Batch 62/64 loss: -0.13910287618637085
Batch 63/64 loss: -0.11148333549499512
Batch 64/64 loss: -0.1294105052947998
Epoch 256  Train loss: -0.1152382635602764  Val loss: -0.02174925374001572
Epoch 257
-------------------------------
Batch 1/64 loss: -0.1324096918106079
Batch 2/64 loss: -0.10170853137969971
Batch 3/64 loss: -0.12263292074203491
Batch 4/64 loss: -0.12499058246612549
Batch 5/64 loss: -0.130193829536438
Batch 6/64 loss: -0.11451518535614014
Batch 7/64 loss: -0.12184679508209229
Batch 8/64 loss: -0.10131675004959106
Batch 9/64 loss: -0.1338787078857422
Batch 10/64 loss: -0.13364243507385254
Batch 11/64 loss: -0.12786000967025757
Batch 12/64 loss: -0.14148235321044922
Batch 13/64 loss: -0.1287938356399536
Batch 14/64 loss: -0.12214994430541992
Batch 15/64 loss: -0.13056224584579468
Batch 16/64 loss: -0.11382710933685303
Batch 17/64 loss: -0.13957232236862183
Batch 18/64 loss: -0.10735034942626953
Batch 19/64 loss: -0.09137952327728271
Batch 20/64 loss: -0.12347280979156494
Batch 21/64 loss: -0.12655562162399292
Batch 22/64 loss: -0.12247276306152344
Batch 23/64 loss: -0.12821471691131592
Batch 24/64 loss: -0.10938841104507446
Batch 25/64 loss: -0.11001312732696533
Batch 26/64 loss: -0.10601568222045898
Batch 27/64 loss: -0.11714595556259155
Batch 28/64 loss: -0.1348668932914734
Batch 29/64 loss: -0.10987824201583862
Batch 30/64 loss: -0.09159815311431885
Batch 31/64 loss: -0.11237627267837524
Batch 32/64 loss: -0.11274480819702148
Batch 33/64 loss: -0.11299830675125122
Batch 34/64 loss: -0.10176306962966919
Batch 35/64 loss: -0.10685521364212036
Batch 36/64 loss: -0.10262304544448853
Batch 37/64 loss: -0.09295856952667236
Batch 38/64 loss: -0.10127025842666626
Batch 39/64 loss: -0.11157315969467163
Batch 40/64 loss: -0.10299432277679443
Batch 41/64 loss: -0.10954338312149048
Batch 42/64 loss: -0.09285026788711548
Batch 43/64 loss: -0.12126171588897705
Batch 44/64 loss: -0.11455321311950684
Batch 45/64 loss: -0.10683846473693848
Batch 46/64 loss: -0.11548793315887451
Batch 47/64 loss: -0.09172821044921875
Batch 48/64 loss: -0.10777091979980469
Batch 49/64 loss: -0.10917103290557861
Batch 50/64 loss: -0.13560855388641357
Batch 51/64 loss: -0.10763388872146606
Batch 52/64 loss: -0.1182408332824707
Batch 53/64 loss: -0.12998920679092407
Batch 54/64 loss: -0.11040079593658447
Batch 55/64 loss: -0.11523205041885376
Batch 56/64 loss: -0.11945819854736328
Batch 57/64 loss: -0.12644457817077637
Batch 58/64 loss: -0.09960895776748657
Batch 59/64 loss: -0.1063118577003479
Batch 60/64 loss: -0.10313695669174194
Batch 61/64 loss: -0.12012791633605957
Batch 62/64 loss: -0.11555278301239014
Batch 63/64 loss: -0.11363321542739868
Batch 64/64 loss: -0.10726875066757202
Epoch 257  Train loss: -0.11496359250124763  Val loss: -0.025462832237846664
Epoch 258
-------------------------------
Batch 1/64 loss: -0.11954796314239502
Batch 2/64 loss: -0.11932504177093506
Batch 3/64 loss: -0.11086094379425049
Batch 4/64 loss: -0.12219345569610596
Batch 5/64 loss: -0.10634934902191162
Batch 6/64 loss: -0.13425934314727783
Batch 7/64 loss: -0.11275815963745117
Batch 8/64 loss: -0.1119459867477417
Batch 9/64 loss: -0.13222914934158325
Batch 10/64 loss: -0.10304677486419678
Batch 11/64 loss: -0.14250928163528442
Batch 12/64 loss: -0.12933790683746338
Batch 13/64 loss: -0.12501686811447144
Batch 14/64 loss: -0.13208794593811035
Batch 15/64 loss: -0.08952218294143677
Batch 16/64 loss: -0.12262153625488281
Batch 17/64 loss: -0.13349378108978271
Batch 18/64 loss: -0.11252635717391968
Batch 19/64 loss: -0.1232040524482727
Batch 20/64 loss: -0.07811635732650757
Batch 21/64 loss: -0.11313867568969727
Batch 22/64 loss: -0.11271029710769653
Batch 23/64 loss: -0.1285463571548462
Batch 24/64 loss: -0.10762643814086914
Batch 25/64 loss: -0.13371241092681885
Batch 26/64 loss: -0.1435762643814087
Batch 27/64 loss: -0.12019586563110352
Batch 28/64 loss: -0.12459838390350342
Batch 29/64 loss: -0.10953640937805176
Batch 30/64 loss: -0.11385554075241089
Batch 31/64 loss: -0.10904145240783691
Batch 32/64 loss: -0.10891175270080566
Batch 33/64 loss: -0.1009473204612732
Batch 34/64 loss: -0.1204909086227417
Batch 35/64 loss: -0.07066500186920166
Batch 36/64 loss: -0.08458006381988525
Batch 37/64 loss: -0.11241471767425537
Batch 38/64 loss: -0.09081757068634033
Batch 39/64 loss: -0.11065459251403809
Batch 40/64 loss: -0.11500996351242065
Batch 41/64 loss: -0.12988430261611938
Batch 42/64 loss: -0.10495889186859131
Batch 43/64 loss: -0.12411808967590332
Batch 44/64 loss: -0.11127340793609619
Batch 45/64 loss: -0.10110825300216675
Batch 46/64 loss: -0.12040448188781738
Batch 47/64 loss: -0.10916078090667725
Batch 48/64 loss: -0.1159508228302002
Batch 49/64 loss: -0.10728204250335693
Batch 50/64 loss: -0.12452620267868042
Batch 51/64 loss: -0.10481101274490356
Batch 52/64 loss: -0.12250256538391113
Batch 53/64 loss: -0.10890460014343262
Batch 54/64 loss: -0.09419769048690796
Batch 55/64 loss: -0.09439921379089355
Batch 56/64 loss: -0.09339702129364014
Batch 57/64 loss: -0.07903558015823364
Batch 58/64 loss: -0.11789262294769287
Batch 59/64 loss: -0.12736016511917114
Batch 60/64 loss: -0.1163475513458252
Batch 61/64 loss: -0.11263173818588257
Batch 62/64 loss: -0.11873799562454224
Batch 63/64 loss: -0.11347270011901855
Batch 64/64 loss: -0.11059838533401489
Epoch 258  Train loss: -0.11336923837661743  Val loss: -0.020014689140713093
Epoch 259
-------------------------------
Batch 1/64 loss: -0.12480556964874268
Batch 2/64 loss: -0.12350344657897949
Batch 3/64 loss: -0.12026643753051758
Batch 4/64 loss: -0.12770742177963257
Batch 5/64 loss: -0.13003861904144287
Batch 6/64 loss: -0.12238824367523193
Batch 7/64 loss: -0.12419986724853516
Batch 8/64 loss: -0.10092175006866455
Batch 9/64 loss: -0.0816272497177124
Batch 10/64 loss: -0.12056916952133179
Batch 11/64 loss: -0.11875665187835693
Batch 12/64 loss: -0.12394094467163086
Batch 13/64 loss: -0.10781228542327881
Batch 14/64 loss: -0.11651825904846191
Batch 15/64 loss: -0.09456104040145874
Batch 16/64 loss: -0.11396157741546631
Batch 17/64 loss: -0.10692298412322998
Batch 18/64 loss: -0.1106216311454773
Batch 19/64 loss: -0.14662474393844604
Batch 20/64 loss: -0.10766172409057617
Batch 21/64 loss: -0.11591970920562744
Batch 22/64 loss: -0.1011044979095459
Batch 23/64 loss: -0.09814804792404175
Batch 24/64 loss: -0.11038953065872192
Batch 25/64 loss: -0.13189822435379028
Batch 26/64 loss: -0.12498658895492554
Batch 27/64 loss: -0.10317277908325195
Batch 28/64 loss: -0.12551718950271606
Batch 29/64 loss: -0.09775978326797485
Batch 30/64 loss: -0.13319265842437744
Batch 31/64 loss: -0.11013668775558472
Batch 32/64 loss: -0.11208570003509521
Batch 33/64 loss: -0.11916625499725342
Batch 34/64 loss: -0.13219565153121948
Batch 35/64 loss: -0.11683392524719238
Batch 36/64 loss: -0.1397051215171814
Batch 37/64 loss: -0.12429559230804443
Batch 38/64 loss: -0.10559099912643433
Batch 39/64 loss: -0.1031649112701416
Batch 40/64 loss: -0.09579735994338989
Batch 41/64 loss: -0.13558852672576904
Batch 42/64 loss: -0.12352752685546875
Batch 43/64 loss: -0.11910229921340942
Batch 44/64 loss: -0.10384476184844971
Batch 45/64 loss: -0.13107705116271973
Batch 46/64 loss: -0.11594772338867188
Batch 47/64 loss: -0.11446082592010498
Batch 48/64 loss: -0.11528116464614868
Batch 49/64 loss: -0.12817752361297607
Batch 50/64 loss: -0.10181701183319092
Batch 51/64 loss: -0.12209928035736084
Batch 52/64 loss: -0.13345015048980713
Batch 53/64 loss: -0.12335443496704102
Batch 54/64 loss: -0.11775130033493042
Batch 55/64 loss: -0.1166730523109436
Batch 56/64 loss: -0.12172877788543701
Batch 57/64 loss: -0.13189899921417236
Batch 58/64 loss: -0.13022077083587646
Batch 59/64 loss: -0.11597812175750732
Batch 60/64 loss: -0.13251250982284546
Batch 61/64 loss: -0.10834771394729614
Batch 62/64 loss: -0.11427009105682373
Batch 63/64 loss: -0.12683796882629395
Batch 64/64 loss: -0.11524885892868042
Epoch 259  Train loss: -0.11756635390075983  Val loss: -0.025615641136759335
Epoch 260
-------------------------------
Batch 1/64 loss: -0.12239700555801392
Batch 2/64 loss: -0.12169277667999268
Batch 3/64 loss: -0.1352698802947998
Batch 4/64 loss: -0.12065130472183228
Batch 5/64 loss: -0.1232876181602478
Batch 6/64 loss: -0.12494516372680664
Batch 7/64 loss: -0.1265769600868225
Batch 8/64 loss: -0.12992364168167114
Batch 9/64 loss: -0.11023551225662231
Batch 10/64 loss: -0.1267980933189392
Batch 11/64 loss: -0.11362355947494507
Batch 12/64 loss: -0.12567263841629028
Batch 13/64 loss: -0.12865525484085083
Batch 14/64 loss: -0.1018831729888916
Batch 15/64 loss: -0.13613063097000122
Batch 16/64 loss: -0.11002492904663086
Batch 17/64 loss: -0.1221919059753418
Batch 18/64 loss: -0.12787353992462158
Batch 19/64 loss: -0.11842083930969238
Batch 20/64 loss: -0.11286324262619019
Batch 21/64 loss: -0.10485583543777466
Batch 22/64 loss: -0.11321568489074707
Batch 23/64 loss: -0.11997848749160767
Batch 24/64 loss: -0.12954938411712646
Batch 25/64 loss: -0.12836843729019165
Batch 26/64 loss: -0.12277787923812866
Batch 27/64 loss: -0.11194121837615967
Batch 28/64 loss: -0.11835181713104248
Batch 29/64 loss: -0.09581387042999268
Batch 30/64 loss: -0.13045257329940796
Batch 31/64 loss: -0.10096120834350586
Batch 32/64 loss: -0.10397666692733765
Batch 33/64 loss: -0.08691781759262085
Batch 34/64 loss: -0.12318265438079834
Batch 35/64 loss: -0.110775887966156
Batch 36/64 loss: -0.1152794361114502
Batch 37/64 loss: -0.13272440433502197
Batch 38/64 loss: -0.11952775716781616
Batch 39/64 loss: -0.1261526346206665
Batch 40/64 loss: -0.11273336410522461
Batch 41/64 loss: -0.12193715572357178
Batch 42/64 loss: -0.10925710201263428
Batch 43/64 loss: -0.11532294750213623
Batch 44/64 loss: -0.1362006664276123
Batch 45/64 loss: -0.10809451341629028
Batch 46/64 loss: -0.12958335876464844
Batch 47/64 loss: -0.10959374904632568
Batch 48/64 loss: -0.13142037391662598
Batch 49/64 loss: -0.11827069520950317
Batch 50/64 loss: -0.12942904233932495
Batch 51/64 loss: -0.12654733657836914
Batch 52/64 loss: -0.11544609069824219
Batch 53/64 loss: -0.11389386653900146
Batch 54/64 loss: -0.11281323432922363
Batch 55/64 loss: -0.11344587802886963
Batch 56/64 loss: -0.12174099683761597
Batch 57/64 loss: -0.1273937225341797
Batch 58/64 loss: -0.10787755250930786
Batch 59/64 loss: -0.1389859914779663
Batch 60/64 loss: -0.13628876209259033
Batch 61/64 loss: -0.11493021249771118
Batch 62/64 loss: -0.13043779134750366
Batch 63/64 loss: -0.12800955772399902
Batch 64/64 loss: -0.1263255476951599
Epoch 260  Train loss: -0.11981677564920164  Val loss: -0.021849201921744856
Epoch 261
-------------------------------
Batch 1/64 loss: -0.1336430311203003
Batch 2/64 loss: -0.11334657669067383
Batch 3/64 loss: -0.1347653865814209
Batch 4/64 loss: -0.10959136486053467
Batch 5/64 loss: -0.13147282600402832
Batch 6/64 loss: -0.11350756883621216
Batch 7/64 loss: -0.11800307035446167
Batch 8/64 loss: -0.10807061195373535
Batch 9/64 loss: -0.10538488626480103
Batch 10/64 loss: -0.1132516860961914
Batch 11/64 loss: -0.13543444871902466
Batch 12/64 loss: -0.12590926885604858
Batch 13/64 loss: -0.12332296371459961
Batch 14/64 loss: -0.14154201745986938
Batch 15/64 loss: -0.1423661708831787
Batch 16/64 loss: -0.13519102334976196
Batch 17/64 loss: -0.108184814453125
Batch 18/64 loss: -0.10843408107757568
Batch 19/64 loss: -0.11047506332397461
Batch 20/64 loss: -0.1104615330696106
Batch 21/64 loss: -0.11597394943237305
Batch 22/64 loss: -0.11838263273239136
Batch 23/64 loss: -0.1189570426940918
Batch 24/64 loss: -0.11742508411407471
Batch 25/64 loss: -0.12047457695007324
Batch 26/64 loss: -0.0961654782295227
Batch 27/64 loss: -0.11107170581817627
Batch 28/64 loss: -0.12319320440292358
Batch 29/64 loss: -0.1330854892730713
Batch 30/64 loss: -0.13944530487060547
Batch 31/64 loss: -0.11769133806228638
Batch 32/64 loss: -0.12874209880828857
Batch 33/64 loss: -0.1466524600982666
Batch 34/64 loss: -0.1215161681175232
Batch 35/64 loss: -0.13363462686538696
Batch 36/64 loss: -0.12717032432556152
Batch 37/64 loss: -0.11667978763580322
Batch 38/64 loss: -0.0990067720413208
Batch 39/64 loss: -0.10916876792907715
Batch 40/64 loss: -0.1084827184677124
Batch 41/64 loss: -0.12178969383239746
Batch 42/64 loss: -0.10213708877563477
Batch 43/64 loss: -0.11914420127868652
Batch 44/64 loss: -0.10308527946472168
Batch 45/64 loss: -0.12282359600067139
Batch 46/64 loss: -0.11241227388381958
Batch 47/64 loss: -0.1288928985595703
Batch 48/64 loss: -0.11074841022491455
Batch 49/64 loss: -0.1232420802116394
Batch 50/64 loss: -0.13951492309570312
Batch 51/64 loss: -0.11807096004486084
Batch 52/64 loss: -0.12849152088165283
Batch 53/64 loss: -0.11727035045623779
Batch 54/64 loss: -0.1278442144393921
Batch 55/64 loss: -0.1260164976119995
Batch 56/64 loss: -0.11887764930725098
Batch 57/64 loss: -0.126905620098114
Batch 58/64 loss: -0.10817348957061768
Batch 59/64 loss: -0.11291027069091797
Batch 60/64 loss: -0.1047661304473877
Batch 61/64 loss: -0.1204371452331543
Batch 62/64 loss: -0.10234260559082031
Batch 63/64 loss: -0.10975944995880127
Batch 64/64 loss: -0.11658793687820435
Epoch 261  Train loss: -0.11950392512714161  Val loss: -0.018414741734049164
Epoch 262
-------------------------------
Batch 1/64 loss: -0.12709307670593262
Batch 2/64 loss: -0.11523902416229248
Batch 3/64 loss: -0.1331850290298462
Batch 4/64 loss: -0.14268654584884644
Batch 5/64 loss: -0.12200570106506348
Batch 6/64 loss: -0.1403786540031433
Batch 7/64 loss: -0.10639679431915283
Batch 8/64 loss: -0.12216997146606445
Batch 9/64 loss: -0.11022758483886719
Batch 10/64 loss: -0.13156020641326904
Batch 11/64 loss: -0.11640316247940063
Batch 12/64 loss: -0.10479176044464111
Batch 13/64 loss: -0.10615861415863037
Batch 14/64 loss: -0.12519079446792603
Batch 15/64 loss: -0.10160857439041138
Batch 16/64 loss: -0.12350296974182129
Batch 17/64 loss: -0.11914682388305664
Batch 18/64 loss: -0.12314510345458984
Batch 19/64 loss: -0.12653738260269165
Batch 20/64 loss: -0.10831999778747559
Batch 21/64 loss: -0.0639306902885437
Batch 22/64 loss: -0.11662399768829346
Batch 23/64 loss: -0.11955398321151733
Batch 24/64 loss: -0.14307594299316406
Batch 25/64 loss: -0.11695563793182373
Batch 26/64 loss: -0.12967336177825928
Batch 27/64 loss: -0.1413440704345703
Batch 28/64 loss: -0.12355750799179077
Batch 29/64 loss: -0.13169103860855103
Batch 30/64 loss: -0.12777048349380493
Batch 31/64 loss: -0.11053556203842163
Batch 32/64 loss: -0.12371784448623657
Batch 33/64 loss: -0.12217020988464355
Batch 34/64 loss: -0.12616050243377686
Batch 35/64 loss: -0.09860223531723022
Batch 36/64 loss: -0.1328979730606079
Batch 37/64 loss: -0.11454540491104126
Batch 38/64 loss: -0.1244317889213562
Batch 39/64 loss: -0.09235721826553345
Batch 40/64 loss: -0.1372077465057373
Batch 41/64 loss: -0.13779711723327637
Batch 42/64 loss: -0.11003804206848145
Batch 43/64 loss: -0.1053500771522522
Batch 44/64 loss: -0.1083756685256958
Batch 45/64 loss: -0.12457042932510376
Batch 46/64 loss: -0.1214827299118042
Batch 47/64 loss: -0.12361299991607666
Batch 48/64 loss: -0.12501317262649536
Batch 49/64 loss: -0.12050187587738037
Batch 50/64 loss: -0.12199568748474121
Batch 51/64 loss: -0.12644469738006592
Batch 52/64 loss: -0.13028639554977417
Batch 53/64 loss: -0.12391996383666992
Batch 54/64 loss: -0.1254391074180603
Batch 55/64 loss: -0.11930245161056519
Batch 56/64 loss: -0.09203499555587769
Batch 57/64 loss: -0.11880362033843994
Batch 58/64 loss: -0.11997133493423462
Batch 59/64 loss: -0.12303429841995239
Batch 60/64 loss: -0.12589597702026367
Batch 61/64 loss: -0.11066460609436035
Batch 62/64 loss: -0.12186533212661743
Batch 63/64 loss: -0.12682634592056274
Batch 64/64 loss: -0.1056928038597107
Epoch 262  Train loss: -0.1199223137369343  Val loss: -0.0219923685916101
Epoch 263
-------------------------------
Batch 1/64 loss: -0.12412053346633911
Batch 2/64 loss: -0.10922074317932129
Batch 3/64 loss: -0.1271466612815857
Batch 4/64 loss: -0.11152547597885132
Batch 5/64 loss: -0.12782323360443115
Batch 6/64 loss: -0.13439887762069702
Batch 7/64 loss: -0.1331779956817627
Batch 8/64 loss: -0.11356014013290405
Batch 9/64 loss: -0.11857521533966064
Batch 10/64 loss: -0.08935701847076416
Batch 11/64 loss: -0.11258351802825928
Batch 12/64 loss: -0.12246179580688477
Batch 13/64 loss: -0.12649941444396973
Batch 14/64 loss: -0.14155685901641846
Batch 15/64 loss: -0.12498152256011963
Batch 16/64 loss: -0.11635386943817139
Batch 17/64 loss: -0.12929528951644897
Batch 18/64 loss: -0.13548171520233154
Batch 19/64 loss: -0.0868481993675232
Batch 20/64 loss: -0.13745689392089844
Batch 21/64 loss: -0.13331341743469238
Batch 22/64 loss: -0.13583195209503174
Batch 23/64 loss: -0.13846451044082642
Batch 24/64 loss: -0.10505521297454834
Batch 25/64 loss: -0.13923895359039307
Batch 26/64 loss: -0.13039129972457886
Batch 27/64 loss: -0.10969161987304688
Batch 28/64 loss: -0.11560112237930298
Batch 29/64 loss: -0.11825478076934814
Batch 30/64 loss: -0.12632077932357788
Batch 31/64 loss: -0.11063218116760254
Batch 32/64 loss: -0.10100013017654419
Batch 33/64 loss: -0.10552173852920532
Batch 34/64 loss: -0.1255548596382141
Batch 35/64 loss: -0.1043509840965271
Batch 36/64 loss: -0.11597782373428345
Batch 37/64 loss: -0.11604088544845581
Batch 38/64 loss: -0.11743146181106567
Batch 39/64 loss: -0.12838250398635864
Batch 40/64 loss: -0.1167253851890564
Batch 41/64 loss: -0.11214667558670044
Batch 42/64 loss: -0.10065889358520508
Batch 43/64 loss: -0.12810802459716797
Batch 44/64 loss: -0.1399363875389099
Batch 45/64 loss: -0.11239069700241089
Batch 46/64 loss: -0.12560057640075684
Batch 47/64 loss: -0.09261608123779297
Batch 48/64 loss: -0.13708245754241943
Batch 49/64 loss: -0.11126261949539185
Batch 50/64 loss: -0.1187170147895813
Batch 51/64 loss: -0.12540650367736816
Batch 52/64 loss: -0.1254425048828125
Batch 53/64 loss: -0.11506783962249756
Batch 54/64 loss: -0.1404803991317749
Batch 55/64 loss: -0.12032932043075562
Batch 56/64 loss: -0.12178128957748413
Batch 57/64 loss: -0.12071895599365234
Batch 58/64 loss: -0.12621593475341797
Batch 59/64 loss: -0.12314742803573608
Batch 60/64 loss: -0.1255362629890442
Batch 61/64 loss: -0.11708658933639526
Batch 62/64 loss: -0.10460066795349121
Batch 63/64 loss: -0.11032319068908691
Batch 64/64 loss: -0.12769830226898193
Epoch 263  Train loss: -0.12026096652535831  Val loss: -0.021300873805567163
Epoch 264
-------------------------------
Batch 1/64 loss: -0.1373693346977234
Batch 2/64 loss: -0.14181619882583618
Batch 3/64 loss: -0.11560893058776855
Batch 4/64 loss: -0.1324881911277771
Batch 5/64 loss: -0.1306501030921936
Batch 6/64 loss: -0.1038593053817749
Batch 7/64 loss: -0.11475908756256104
Batch 8/64 loss: -0.11652684211730957
Batch 9/64 loss: -0.1279008388519287
Batch 10/64 loss: -0.1144934892654419
Batch 11/64 loss: -0.13158100843429565
Batch 12/64 loss: -0.10455209016799927
Batch 13/64 loss: -0.12284833192825317
Batch 14/64 loss: -0.12938547134399414
Batch 15/64 loss: -0.11346435546875
Batch 16/64 loss: -0.10805708169937134
Batch 17/64 loss: -0.12351572513580322
Batch 18/64 loss: -0.13364356756210327
Batch 19/64 loss: -0.11710119247436523
Batch 20/64 loss: -0.11581981182098389
Batch 21/64 loss: -0.09906905889511108
Batch 22/64 loss: -0.12031859159469604
Batch 23/64 loss: -0.13653481006622314
Batch 24/64 loss: -0.13124054670333862
Batch 25/64 loss: -0.1340014934539795
Batch 26/64 loss: -0.11989820003509521
Batch 27/64 loss: -0.10786551237106323
Batch 28/64 loss: -0.13101178407669067
Batch 29/64 loss: -0.10200250148773193
Batch 30/64 loss: -0.13855701684951782
Batch 31/64 loss: -0.13410437107086182
Batch 32/64 loss: -0.1070970892906189
Batch 33/64 loss: -0.11701393127441406
Batch 34/64 loss: -0.10971403121948242
Batch 35/64 loss: -0.11359202861785889
Batch 36/64 loss: -0.12803369760513306
Batch 37/64 loss: -0.09813928604125977
Batch 38/64 loss: -0.10648691654205322
Batch 39/64 loss: -0.1351284384727478
Batch 40/64 loss: -0.12679529190063477
Batch 41/64 loss: -0.1221887469291687
Batch 42/64 loss: -0.12537729740142822
Batch 43/64 loss: -0.11595714092254639
Batch 44/64 loss: -0.12609505653381348
Batch 45/64 loss: -0.1266622543334961
Batch 46/64 loss: -0.1318596601486206
Batch 47/64 loss: -0.11755883693695068
Batch 48/64 loss: -0.11917108297348022
Batch 49/64 loss: -0.119839608669281
Batch 50/64 loss: -0.1308562159538269
Batch 51/64 loss: -0.12814170122146606
Batch 52/64 loss: -0.12047553062438965
Batch 53/64 loss: -0.1335628628730774
Batch 54/64 loss: -0.12205201387405396
Batch 55/64 loss: -0.12680304050445557
Batch 56/64 loss: -0.1355329155921936
Batch 57/64 loss: -0.08767378330230713
Batch 58/64 loss: -0.1156395673751831
Batch 59/64 loss: -0.12276583909988403
Batch 60/64 loss: -0.1208915114402771
Batch 61/64 loss: -0.135459303855896
Batch 62/64 loss: -0.12154150009155273
Batch 63/64 loss: -0.13041388988494873
Batch 64/64 loss: -0.11491125822067261
Epoch 264  Train loss: -0.12164311151878507  Val loss: -0.021686505001435166
Epoch 265
-------------------------------
Batch 1/64 loss: -0.13887786865234375
Batch 2/64 loss: -0.12574130296707153
Batch 3/64 loss: -0.12219852209091187
Batch 4/64 loss: -0.10001593828201294
Batch 5/64 loss: -0.09809952974319458
Batch 6/64 loss: -0.11487418413162231
Batch 7/64 loss: -0.12960529327392578
Batch 8/64 loss: -0.14301979541778564
Batch 9/64 loss: -0.14254391193389893
Batch 10/64 loss: -0.11711210012435913
Batch 11/64 loss: -0.1400163173675537
Batch 12/64 loss: -0.13460242748260498
Batch 13/64 loss: -0.11794483661651611
Batch 14/64 loss: -0.12898647785186768
Batch 15/64 loss: -0.14403951168060303
Batch 16/64 loss: -0.13793665170669556
Batch 17/64 loss: -0.13054180145263672
Batch 18/64 loss: -0.12384474277496338
Batch 19/64 loss: -0.10604166984558105
Batch 20/64 loss: -0.13193941116333008
Batch 21/64 loss: -0.13535964488983154
Batch 22/64 loss: -0.13935506343841553
Batch 23/64 loss: -0.11596137285232544
Batch 24/64 loss: -0.11707484722137451
Batch 25/64 loss: -0.11082756519317627
Batch 26/64 loss: -0.11203265190124512
Batch 27/64 loss: -0.1145247220993042
Batch 28/64 loss: -0.1152489185333252
Batch 29/64 loss: -0.10932308435440063
Batch 30/64 loss: -0.11300599575042725
Batch 31/64 loss: -0.10248446464538574
Batch 32/64 loss: -0.11904358863830566
Batch 33/64 loss: -0.10264408588409424
Batch 34/64 loss: -0.12130188941955566
Batch 35/64 loss: -0.13395559787750244
Batch 36/64 loss: -0.12891870737075806
Batch 37/64 loss: -0.1203303337097168
Batch 38/64 loss: -0.12630057334899902
Batch 39/64 loss: -0.12961632013320923
Batch 40/64 loss: -0.13394677639007568
Batch 41/64 loss: -0.11200004816055298
Batch 42/64 loss: -0.12888604402542114
Batch 43/64 loss: -0.1271182894706726
Batch 44/64 loss: -0.12227398157119751
Batch 45/64 loss: -0.12763535976409912
Batch 46/64 loss: -0.15232306718826294
Batch 47/64 loss: -0.11499154567718506
Batch 48/64 loss: -0.09656846523284912
Batch 49/64 loss: -0.10529118776321411
Batch 50/64 loss: -0.11732351779937744
Batch 51/64 loss: -0.11213475465774536
Batch 52/64 loss: -0.12333518266677856
Batch 53/64 loss: -0.13246113061904907
Batch 54/64 loss: -0.12788450717926025
Batch 55/64 loss: -0.11682772636413574
Batch 56/64 loss: -0.11621284484863281
Batch 57/64 loss: -0.13746166229248047
Batch 58/64 loss: -0.1246037483215332
Batch 59/64 loss: -0.10617029666900635
Batch 60/64 loss: -0.11053615808486938
Batch 61/64 loss: -0.12124365568161011
Batch 62/64 loss: -0.139013409614563
Batch 63/64 loss: -0.1151723861694336
Batch 64/64 loss: -0.11287641525268555
Epoch 265  Train loss: -0.12237434948191923  Val loss: -0.018200227894733863
Epoch 266
-------------------------------
Batch 1/64 loss: -0.09811639785766602
Batch 2/64 loss: -0.14112353324890137
Batch 3/64 loss: -0.12015217542648315
Batch 4/64 loss: -0.12995761632919312
Batch 5/64 loss: -0.13085025548934937
Batch 6/64 loss: -0.09821683168411255
Batch 7/64 loss: -0.09915077686309814
Batch 8/64 loss: -0.11388033628463745
Batch 9/64 loss: -0.12061178684234619
Batch 10/64 loss: -0.12024635076522827
Batch 11/64 loss: -0.13470566272735596
Batch 12/64 loss: -0.11770468950271606
Batch 13/64 loss: -0.135259747505188
Batch 14/64 loss: -0.11583667993545532
Batch 15/64 loss: -0.12435972690582275
Batch 16/64 loss: -0.126875102519989
Batch 17/64 loss: -0.13776427507400513
Batch 18/64 loss: -0.12307071685791016
Batch 19/64 loss: -0.13297569751739502
Batch 20/64 loss: -0.12359094619750977
Batch 21/64 loss: -0.138014554977417
Batch 22/64 loss: -0.12713521718978882
Batch 23/64 loss: -0.14180850982666016
Batch 24/64 loss: -0.1311410665512085
Batch 25/64 loss: -0.12294411659240723
Batch 26/64 loss: -0.12443697452545166
Batch 27/64 loss: -0.1272556185722351
Batch 28/64 loss: -0.12656891345977783
Batch 29/64 loss: -0.12715721130371094
Batch 30/64 loss: -0.13920122385025024
Batch 31/64 loss: -0.12984895706176758
Batch 32/64 loss: -0.10235536098480225
Batch 33/64 loss: -0.12892532348632812
Batch 34/64 loss: -0.12847274541854858
Batch 35/64 loss: -0.11863917112350464
Batch 36/64 loss: -0.13114845752716064
Batch 37/64 loss: -0.1305696964263916
Batch 38/64 loss: -0.13891595602035522
Batch 39/64 loss: -0.13663309812545776
Batch 40/64 loss: -0.13100826740264893
Batch 41/64 loss: -0.1174389123916626
Batch 42/64 loss: -0.14894604682922363
Batch 43/64 loss: -0.10710424184799194
Batch 44/64 loss: -0.12336623668670654
Batch 45/64 loss: -0.09698104858398438
Batch 46/64 loss: -0.09528958797454834
Batch 47/64 loss: -0.14192652702331543
Batch 48/64 loss: -0.13074451684951782
Batch 49/64 loss: -0.11757445335388184
Batch 50/64 loss: -0.10907340049743652
Batch 51/64 loss: -0.11656785011291504
Batch 52/64 loss: -0.11453366279602051
Batch 53/64 loss: -0.11468040943145752
Batch 54/64 loss: -0.11273360252380371
Batch 55/64 loss: -0.11433374881744385
Batch 56/64 loss: -0.12373101711273193
Batch 57/64 loss: -0.08651399612426758
Batch 58/64 loss: -0.08913284540176392
Batch 59/64 loss: -0.10761559009552002
Batch 60/64 loss: -0.10511517524719238
Batch 61/64 loss: -0.1284223198890686
Batch 62/64 loss: -0.11854732036590576
Batch 63/64 loss: -0.12011599540710449
Batch 64/64 loss: -0.12309068441390991
Epoch 266  Train loss: -0.12171664728837854  Val loss: -0.02120524726782468
Epoch 267
-------------------------------
Batch 1/64 loss: -0.08636164665222168
Batch 2/64 loss: -0.13642185926437378
Batch 3/64 loss: -0.1261765956878662
Batch 4/64 loss: -0.11293917894363403
Batch 5/64 loss: -0.12274724245071411
Batch 6/64 loss: -0.10611307621002197
Batch 7/64 loss: -0.13659274578094482
Batch 8/64 loss: -0.14129841327667236
Batch 9/64 loss: -0.11915165185928345
Batch 10/64 loss: -0.12084710597991943
Batch 11/64 loss: -0.1398385763168335
Batch 12/64 loss: -0.1296425461769104
Batch 13/64 loss: -0.12510526180267334
Batch 14/64 loss: -0.12365871667861938
Batch 15/64 loss: -0.13660281896591187
Batch 16/64 loss: -0.1138729453086853
Batch 17/64 loss: -0.12875092029571533
Batch 18/64 loss: -0.14247220754623413
Batch 19/64 loss: -0.1344451904296875
Batch 20/64 loss: -0.12649357318878174
Batch 21/64 loss: -0.12084227800369263
Batch 22/64 loss: -0.12669271230697632
Batch 23/64 loss: -0.10330498218536377
Batch 24/64 loss: -0.11675816774368286
Batch 25/64 loss: -0.1227116584777832
Batch 26/64 loss: -0.12598729133605957
Batch 27/64 loss: -0.13315659761428833
Batch 28/64 loss: -0.11580449342727661
Batch 29/64 loss: -0.12123453617095947
Batch 30/64 loss: -0.11327409744262695
Batch 31/64 loss: -0.12641161680221558
Batch 32/64 loss: -0.11747127771377563
Batch 33/64 loss: -0.12795132398605347
Batch 34/64 loss: -0.1360817551612854
Batch 35/64 loss: -0.13254523277282715
Batch 36/64 loss: -0.10444039106369019
Batch 37/64 loss: -0.12614786624908447
Batch 38/64 loss: -0.12522298097610474
Batch 39/64 loss: -0.08901238441467285
Batch 40/64 loss: -0.13060307502746582
Batch 41/64 loss: -0.1153186559677124
Batch 42/64 loss: -0.13692909479141235
Batch 43/64 loss: -0.12995636463165283
Batch 44/64 loss: -0.12227922677993774
Batch 45/64 loss: -0.1291329264640808
Batch 46/64 loss: -0.12477266788482666
Batch 47/64 loss: -0.11376714706420898
Batch 48/64 loss: -0.12042486667633057
Batch 49/64 loss: -0.12560468912124634
Batch 50/64 loss: -0.12037336826324463
Batch 51/64 loss: -0.11808103322982788
Batch 52/64 loss: -0.1436300277709961
Batch 53/64 loss: -0.1348646879196167
Batch 54/64 loss: -0.12500393390655518
Batch 55/64 loss: -0.10255753993988037
Batch 56/64 loss: -0.12826991081237793
Batch 57/64 loss: -0.12504667043685913
Batch 58/64 loss: -0.12271064519882202
Batch 59/64 loss: -0.11762773990631104
Batch 60/64 loss: -0.12539368867874146
Batch 61/64 loss: -0.11356943845748901
Batch 62/64 loss: -0.11121392250061035
Batch 63/64 loss: -0.13238954544067383
Batch 64/64 loss: -0.11990946531295776
Epoch 267  Train loss: -0.12320057854932898  Val loss: -0.02058051828666241
Epoch 268
-------------------------------
Batch 1/64 loss: -0.12403905391693115
Batch 2/64 loss: -0.09098029136657715
Batch 3/64 loss: -0.13229596614837646
Batch 4/64 loss: -0.13409578800201416
Batch 5/64 loss: -0.12208127975463867
Batch 6/64 loss: -0.12901389598846436
Batch 7/64 loss: -0.09365224838256836
Batch 8/64 loss: -0.13711786270141602
Batch 9/64 loss: -0.13152575492858887
Batch 10/64 loss: -0.14246755838394165
Batch 11/64 loss: -0.14829230308532715
Batch 12/64 loss: -0.14312613010406494
Batch 13/64 loss: -0.09462833404541016
Batch 14/64 loss: -0.13479894399642944
Batch 15/64 loss: -0.123227059841156
Batch 16/64 loss: -0.14315372705459595
Batch 17/64 loss: -0.12313699722290039
Batch 18/64 loss: -0.13885855674743652
Batch 19/64 loss: -0.1276555061340332
Batch 20/64 loss: -0.12729412317276
Batch 21/64 loss: -0.10528433322906494
Batch 22/64 loss: -0.11736643314361572
Batch 23/64 loss: -0.12926077842712402
Batch 24/64 loss: -0.13340860605239868
Batch 25/64 loss: -0.14566373825073242
Batch 26/64 loss: -0.12866926193237305
Batch 27/64 loss: -0.14127123355865479
Batch 28/64 loss: -0.12046819925308228
Batch 29/64 loss: -0.12958985567092896
Batch 30/64 loss: -0.1343427300453186
Batch 31/64 loss: -0.1293146014213562
Batch 32/64 loss: -0.14982187747955322
Batch 33/64 loss: -0.14517813920974731
Batch 34/64 loss: -0.11077827215194702
Batch 35/64 loss: -0.1298210620880127
Batch 36/64 loss: -0.1292666792869568
Batch 37/64 loss: -0.10953807830810547
Batch 38/64 loss: -0.13470792770385742
Batch 39/64 loss: -0.1236412525177002
Batch 40/64 loss: -0.1230124831199646
Batch 41/64 loss: -0.13671600818634033
Batch 42/64 loss: -0.10002803802490234
Batch 43/64 loss: -0.112601637840271
Batch 44/64 loss: -0.1098794937133789
Batch 45/64 loss: -0.13733267784118652
Batch 46/64 loss: -0.1282435655593872
Batch 47/64 loss: -0.11763232946395874
Batch 48/64 loss: -0.11783486604690552
Batch 49/64 loss: -0.0871686339378357
Batch 50/64 loss: -0.12440598011016846
Batch 51/64 loss: -0.14123952388763428
Batch 52/64 loss: -0.116660475730896
Batch 53/64 loss: -0.11175262928009033
Batch 54/64 loss: -0.11549413204193115
Batch 55/64 loss: -0.13783270120620728
Batch 56/64 loss: -0.12534749507904053
Batch 57/64 loss: -0.13840383291244507
Batch 58/64 loss: -0.12146341800689697
Batch 59/64 loss: -0.104916512966156
Batch 60/64 loss: -0.1400148868560791
Batch 61/64 loss: -0.11933779716491699
Batch 62/64 loss: -0.13858866691589355
Batch 63/64 loss: -0.1299692988395691
Batch 64/64 loss: -0.12634122371673584
Epoch 268  Train loss: -0.12579556773690617  Val loss: -0.021644187137433345
Epoch 269
-------------------------------
Batch 1/64 loss: -0.10882514715194702
Batch 2/64 loss: -0.10973858833312988
Batch 3/64 loss: -0.1375921368598938
Batch 4/64 loss: -0.1265096664428711
Batch 5/64 loss: -0.12980246543884277
Batch 6/64 loss: -0.1363908052444458
Batch 7/64 loss: -0.12672823667526245
Batch 8/64 loss: -0.11622613668441772
Batch 9/64 loss: -0.12469542026519775
Batch 10/64 loss: -0.11665010452270508
Batch 11/64 loss: -0.11806702613830566
Batch 12/64 loss: -0.12574642896652222
Batch 13/64 loss: -0.11383599042892456
Batch 14/64 loss: -0.11030405759811401
Batch 15/64 loss: -0.11418437957763672
Batch 16/64 loss: -0.13472944498062134
Batch 17/64 loss: -0.12634432315826416
Batch 18/64 loss: -0.11914753913879395
Batch 19/64 loss: -0.12015753984451294
Batch 20/64 loss: -0.08319991827011108
Batch 21/64 loss: -0.12388503551483154
Batch 22/64 loss: -0.12720727920532227
Batch 23/64 loss: -0.1252644658088684
Batch 24/64 loss: -0.1339138150215149
Batch 25/64 loss: -0.10123443603515625
Batch 26/64 loss: -0.12816381454467773
Batch 27/64 loss: -0.12912476062774658
Batch 28/64 loss: -0.09284913539886475
Batch 29/64 loss: -0.1362507939338684
Batch 30/64 loss: -0.10199910402297974
Batch 31/64 loss: -0.13654786348342896
Batch 32/64 loss: -0.12970709800720215
Batch 33/64 loss: -0.13750934600830078
Batch 34/64 loss: -0.1348022222518921
Batch 35/64 loss: -0.12303972244262695
Batch 36/64 loss: -0.1270121932029724
Batch 37/64 loss: -0.132313072681427
Batch 38/64 loss: -0.12538260221481323
Batch 39/64 loss: -0.14898788928985596
Batch 40/64 loss: -0.12382954359054565
Batch 41/64 loss: -0.13626986742019653
Batch 42/64 loss: -0.10572713613510132
Batch 43/64 loss: -0.1203697919845581
Batch 44/64 loss: -0.11410975456237793
Batch 45/64 loss: -0.11453372240066528
Batch 46/64 loss: -0.11873006820678711
Batch 47/64 loss: -0.13524609804153442
Batch 48/64 loss: -0.1390080451965332
Batch 49/64 loss: -0.1310299038887024
Batch 50/64 loss: -0.11194086074829102
Batch 51/64 loss: -0.13747042417526245
Batch 52/64 loss: -0.13091516494750977
Batch 53/64 loss: -0.13536524772644043
Batch 54/64 loss: -0.12003570795059204
Batch 55/64 loss: -0.1333097219467163
Batch 56/64 loss: -0.11341345310211182
Batch 57/64 loss: -0.12236225605010986
Batch 58/64 loss: -0.12890106439590454
Batch 59/64 loss: -0.10902911424636841
Batch 60/64 loss: -0.12090682983398438
Batch 61/64 loss: -0.10724532604217529
Batch 62/64 loss: -0.12186110019683838
Batch 63/64 loss: -0.11527800559997559
Batch 64/64 loss: -0.11992955207824707
Epoch 269  Train loss: -0.12283773141748765  Val loss: -0.024550700720233198
Epoch 270
-------------------------------
Batch 1/64 loss: -0.11789315938949585
Batch 2/64 loss: -0.11484652757644653
Batch 3/64 loss: -0.13742876052856445
Batch 4/64 loss: -0.09023094177246094
Batch 5/64 loss: -0.13676083087921143
Batch 6/64 loss: -0.12545257806777954
Batch 7/64 loss: -0.12714886665344238
Batch 8/64 loss: -0.13246673345565796
Batch 9/64 loss: -0.10832566022872925
Batch 10/64 loss: -0.13233929872512817
Batch 11/64 loss: -0.13051068782806396
Batch 12/64 loss: -0.15010321140289307
Batch 13/64 loss: -0.14066535234451294
Batch 14/64 loss: -0.14107376337051392
Batch 15/64 loss: -0.13285577297210693
Batch 16/64 loss: -0.1430603265762329
Batch 17/64 loss: -0.1274878978729248
Batch 18/64 loss: -0.1320514678955078
Batch 19/64 loss: -0.12433791160583496
Batch 20/64 loss: -0.13089430332183838
Batch 21/64 loss: -0.11344850063323975
Batch 22/64 loss: -0.12285876274108887
Batch 23/64 loss: -0.12773680686950684
Batch 24/64 loss: -0.124725341796875
Batch 25/64 loss: -0.12814033031463623
Batch 26/64 loss: -0.1270960569381714
Batch 27/64 loss: -0.11240637302398682
Batch 28/64 loss: -0.10891199111938477
Batch 29/64 loss: -0.12893515825271606
Batch 30/64 loss: -0.12612032890319824
Batch 31/64 loss: -0.11184990406036377
Batch 32/64 loss: -0.10930687189102173
Batch 33/64 loss: -0.12020772695541382
Batch 34/64 loss: -0.12469738721847534
Batch 35/64 loss: -0.12473428249359131
Batch 36/64 loss: -0.1258900761604309
Batch 37/64 loss: -0.11501741409301758
Batch 38/64 loss: -0.10009181499481201
Batch 39/64 loss: -0.11018598079681396
Batch 40/64 loss: -0.11592686176300049
Batch 41/64 loss: -0.11701345443725586
Batch 42/64 loss: -0.11828696727752686
Batch 43/64 loss: -0.126309335231781
Batch 44/64 loss: -0.12081211805343628
Batch 45/64 loss: -0.12997031211853027
Batch 46/64 loss: -0.1311371922492981
Batch 47/64 loss: -0.08909279108047485
Batch 48/64 loss: -0.10334813594818115
Batch 49/64 loss: -0.11779570579528809
Batch 50/64 loss: -0.13030582666397095
Batch 51/64 loss: -0.09658598899841309
Batch 52/64 loss: -0.11852210760116577
Batch 53/64 loss: -0.13954675197601318
Batch 54/64 loss: -0.12874341011047363
Batch 55/64 loss: -0.11204725503921509
Batch 56/64 loss: -0.11587280035018921
Batch 57/64 loss: -0.114071786403656
Batch 58/64 loss: -0.12694698572158813
Batch 59/64 loss: -0.11533528566360474
Batch 60/64 loss: -0.13487482070922852
Batch 61/64 loss: -0.1225975751876831
Batch 62/64 loss: -0.11729055643081665
Batch 63/64 loss: -0.10923987627029419
Batch 64/64 loss: -0.1053534746170044
Epoch 270  Train loss: -0.12186641833361457  Val loss: -0.023844514310974434
Epoch 271
-------------------------------
Batch 1/64 loss: -0.12373596429824829
Batch 2/64 loss: -0.14746695756912231
Batch 3/64 loss: -0.14772212505340576
Batch 4/64 loss: -0.11667406558990479
Batch 5/64 loss: -0.09599870443344116
Batch 6/64 loss: -0.11991065740585327
Batch 7/64 loss: -0.12624603509902954
Batch 8/64 loss: -0.11019885540008545
Batch 9/64 loss: -0.1453791856765747
Batch 10/64 loss: -0.12470698356628418
Batch 11/64 loss: -0.1309041976928711
Batch 12/64 loss: -0.13546180725097656
Batch 13/64 loss: -0.11082613468170166
Batch 14/64 loss: -0.12580561637878418
Batch 15/64 loss: -0.12831276655197144
Batch 16/64 loss: -0.10721927881240845
Batch 17/64 loss: -0.13044100999832153
Batch 18/64 loss: -0.13224691152572632
Batch 19/64 loss: -0.13725239038467407
Batch 20/64 loss: -0.12436950206756592
Batch 21/64 loss: -0.13541805744171143
Batch 22/64 loss: -0.10685396194458008
Batch 23/64 loss: -0.12365567684173584
Batch 24/64 loss: -0.13859963417053223
Batch 25/64 loss: -0.12728559970855713
Batch 26/64 loss: -0.11410874128341675
Batch 27/64 loss: -0.13012713193893433
Batch 28/64 loss: -0.14657676219940186
Batch 29/64 loss: -0.11086773872375488
Batch 30/64 loss: -0.12927794456481934
Batch 31/64 loss: -0.12548184394836426
Batch 32/64 loss: -0.1437469720840454
Batch 33/64 loss: -0.14067184925079346
Batch 34/64 loss: -0.12734317779541016
Batch 35/64 loss: -0.1003449559211731
Batch 36/64 loss: -0.11437356472015381
Batch 37/64 loss: -0.12609988451004028
Batch 38/64 loss: -0.12026113271713257
Batch 39/64 loss: -0.12538087368011475
Batch 40/64 loss: -0.12289756536483765
Batch 41/64 loss: -0.12719637155532837
Batch 42/64 loss: -0.13299018144607544
Batch 43/64 loss: -0.12098836898803711
Batch 44/64 loss: -0.12939119338989258
Batch 45/64 loss: -0.10798400640487671
Batch 46/64 loss: -0.09515589475631714
Batch 47/64 loss: -0.12930017709732056
Batch 48/64 loss: -0.12010359764099121
Batch 49/64 loss: -0.12494176626205444
Batch 50/64 loss: -0.10515332221984863
Batch 51/64 loss: -0.13609808683395386
Batch 52/64 loss: -0.11857390403747559
Batch 53/64 loss: -0.10162365436553955
Batch 54/64 loss: -0.11330866813659668
Batch 55/64 loss: -0.13779276609420776
Batch 56/64 loss: -0.11273539066314697
Batch 57/64 loss: -0.12566882371902466
Batch 58/64 loss: -0.1372135877609253
Batch 59/64 loss: -0.12337684631347656
Batch 60/64 loss: -0.11876457929611206
Batch 61/64 loss: -0.12115657329559326
Batch 62/64 loss: -0.12399792671203613
Batch 63/64 loss: -0.10571789741516113
Batch 64/64 loss: -0.14168208837509155
Epoch 271  Train loss: -0.12401172343422384  Val loss: -0.022299477734516578
Epoch 272
-------------------------------
Batch 1/64 loss: -0.14343923330307007
Batch 2/64 loss: -0.12955361604690552
Batch 3/64 loss: -0.13494890928268433
Batch 4/64 loss: -0.12719595432281494
Batch 5/64 loss: -0.11413717269897461
Batch 6/64 loss: -0.13626259565353394
Batch 7/64 loss: -0.13411211967468262
Batch 8/64 loss: -0.12640565633773804
Batch 9/64 loss: -0.1357758641242981
Batch 10/64 loss: -0.13361519575119019
Batch 11/64 loss: -0.14005404710769653
Batch 12/64 loss: -0.12194156646728516
Batch 13/64 loss: -0.14736485481262207
Batch 14/64 loss: -0.1027522087097168
Batch 15/64 loss: -0.1357187032699585
Batch 16/64 loss: -0.12183833122253418
Batch 17/64 loss: -0.1442219614982605
Batch 18/64 loss: -0.12706172466278076
Batch 19/64 loss: -0.13173675537109375
Batch 20/64 loss: -0.1418871283531189
Batch 21/64 loss: -0.13731062412261963
Batch 22/64 loss: -0.13000750541687012
Batch 23/64 loss: -0.11006444692611694
Batch 24/64 loss: -0.11440831422805786
Batch 25/64 loss: -0.11868453025817871
Batch 26/64 loss: -0.1360940933227539
Batch 27/64 loss: -0.12325066328048706
Batch 28/64 loss: -0.08795672655105591
Batch 29/64 loss: -0.12119078636169434
Batch 30/64 loss: -0.1036878228187561
Batch 31/64 loss: -0.13328802585601807
Batch 32/64 loss: -0.11937618255615234
Batch 33/64 loss: -0.10534095764160156
Batch 34/64 loss: -0.12314021587371826
Batch 35/64 loss: -0.13114994764328003
Batch 36/64 loss: -0.12454307079315186
Batch 37/64 loss: -0.11802750825881958
Batch 38/64 loss: -0.1330183744430542
Batch 39/64 loss: -0.12179142236709595
Batch 40/64 loss: -0.1251152753829956
Batch 41/64 loss: -0.1270139217376709
Batch 42/64 loss: -0.11630529165267944
Batch 43/64 loss: -0.137151300907135
Batch 44/64 loss: -0.13683688640594482
Batch 45/64 loss: -0.1503412127494812
Batch 46/64 loss: -0.11986386775970459
Batch 47/64 loss: -0.09094858169555664
Batch 48/64 loss: -0.11341738700866699
Batch 49/64 loss: -0.12811142206192017
Batch 50/64 loss: -0.1232108473777771
Batch 51/64 loss: -0.11849856376647949
Batch 52/64 loss: -0.12334656715393066
Batch 53/64 loss: -0.10287952423095703
Batch 54/64 loss: -0.12629640102386475
Batch 55/64 loss: -0.11879962682723999
Batch 56/64 loss: -0.11834973096847534
Batch 57/64 loss: -0.12939804792404175
Batch 58/64 loss: -0.11453181505203247
Batch 59/64 loss: -0.12194448709487915
Batch 60/64 loss: -0.13451486825942993
Batch 61/64 loss: -0.12023061513900757
Batch 62/64 loss: -0.11151081323623657
Batch 63/64 loss: -0.12885355949401855
Batch 64/64 loss: -0.11781960725784302
Epoch 272  Train loss: -0.12467749235676784  Val loss: -0.01878404453448004
Epoch 273
-------------------------------
Batch 1/64 loss: -0.09785008430480957
Batch 2/64 loss: -0.10464775562286377
Batch 3/64 loss: -0.13587939739227295
Batch 4/64 loss: -0.11869978904724121
Batch 5/64 loss: -0.14233773946762085
Batch 6/64 loss: -0.1063535213470459
Batch 7/64 loss: -0.12368780374526978
Batch 8/64 loss: -0.13202613592147827
Batch 9/64 loss: -0.1287626028060913
Batch 10/64 loss: -0.12328386306762695
Batch 11/64 loss: -0.10340344905853271
Batch 12/64 loss: -0.13647639751434326
Batch 13/64 loss: -0.13193261623382568
Batch 14/64 loss: -0.13237720727920532
Batch 15/64 loss: -0.12745273113250732
Batch 16/64 loss: -0.12390577793121338
Batch 17/64 loss: -0.12882423400878906
Batch 18/64 loss: -0.13387548923492432
Batch 19/64 loss: -0.12238430976867676
Batch 20/64 loss: -0.11687827110290527
Batch 21/64 loss: -0.13351333141326904
Batch 22/64 loss: -0.11284565925598145
Batch 23/64 loss: -0.14019089937210083
Batch 24/64 loss: -0.13932448625564575
Batch 25/64 loss: -0.14163732528686523
Batch 26/64 loss: -0.11356621980667114
Batch 27/64 loss: -0.09758687019348145
Batch 28/64 loss: -0.11577415466308594
Batch 29/64 loss: -0.13190913200378418
Batch 30/64 loss: -0.12391406297683716
Batch 31/64 loss: -0.10478442907333374
Batch 32/64 loss: -0.1251468062400818
Batch 33/64 loss: -0.12308609485626221
Batch 34/64 loss: -0.10630935430526733
Batch 35/64 loss: -0.12294614315032959
Batch 36/64 loss: -0.12786996364593506
Batch 37/64 loss: -0.10003024339675903
Batch 38/64 loss: -0.116596519947052
Batch 39/64 loss: -0.12069708108901978
Batch 40/64 loss: -0.12597787380218506
Batch 41/64 loss: -0.11636561155319214
Batch 42/64 loss: -0.1397089958190918
Batch 43/64 loss: -0.1415349841117859
Batch 44/64 loss: -0.11769109964370728
Batch 45/64 loss: -0.1158021092414856
Batch 46/64 loss: -0.12677037715911865
Batch 47/64 loss: -0.12199461460113525
Batch 48/64 loss: -0.12917542457580566
Batch 49/64 loss: -0.11507844924926758
Batch 50/64 loss: -0.14298301935195923
Batch 51/64 loss: -0.12717968225479126
Batch 52/64 loss: -0.1411922574043274
Batch 53/64 loss: -0.13103342056274414
Batch 54/64 loss: -0.13293439149856567
Batch 55/64 loss: -0.13304680585861206
Batch 56/64 loss: -0.1307065486907959
Batch 57/64 loss: -0.13150924444198608
Batch 58/64 loss: -0.12182497978210449
Batch 59/64 loss: -0.12300074100494385
Batch 60/64 loss: -0.13557904958724976
Batch 61/64 loss: -0.12532317638397217
Batch 62/64 loss: -0.13288664817810059
Batch 63/64 loss: -0.1260426640510559
Batch 64/64 loss: -0.1171036958694458
Epoch 273  Train loss: -0.1245794179392796  Val loss: -0.0247151499351685
Epoch 274
-------------------------------
Batch 1/64 loss: -0.1369420289993286
Batch 2/64 loss: -0.13669073581695557
Batch 3/64 loss: -0.13479042053222656
Batch 4/64 loss: -0.14765167236328125
Batch 5/64 loss: -0.11465883255004883
Batch 6/64 loss: -0.14015918970108032
Batch 7/64 loss: -0.1169244647026062
Batch 8/64 loss: -0.13925707340240479
Batch 9/64 loss: -0.13480007648468018
Batch 10/64 loss: -0.13450098037719727
Batch 11/64 loss: -0.1401081681251526
Batch 12/64 loss: -0.11974173784255981
Batch 13/64 loss: -0.11557513475418091
Batch 14/64 loss: -0.1280432939529419
Batch 15/64 loss: -0.13707572221755981
Batch 16/64 loss: -0.10698521137237549
Batch 17/64 loss: -0.11155825853347778
Batch 18/64 loss: -0.10652035474777222
Batch 19/64 loss: -0.13392901420593262
Batch 20/64 loss: -0.13863706588745117
Batch 21/64 loss: -0.10942822694778442
Batch 22/64 loss: -0.1250072717666626
Batch 23/64 loss: -0.1297607421875
Batch 24/64 loss: -0.12539809942245483
Batch 25/64 loss: -0.1414433717727661
Batch 26/64 loss: -0.13067299127578735
Batch 27/64 loss: -0.10848188400268555
Batch 28/64 loss: -0.1301671266555786
Batch 29/64 loss: -0.12656378746032715
Batch 30/64 loss: -0.11535501480102539
Batch 31/64 loss: -0.14611518383026123
Batch 32/64 loss: -0.13445305824279785
Batch 33/64 loss: -0.10592794418334961
Batch 34/64 loss: -0.12868750095367432
Batch 35/64 loss: -0.13235050439834595
Batch 36/64 loss: -0.12530481815338135
Batch 37/64 loss: -0.11449623107910156
Batch 38/64 loss: -0.10933142900466919
Batch 39/64 loss: -0.11667048931121826
Batch 40/64 loss: -0.1406429409980774
Batch 41/64 loss: -0.1267387866973877
Batch 42/64 loss: -0.1349545121192932
Batch 43/64 loss: -0.12922918796539307
Batch 44/64 loss: -0.11586952209472656
Batch 45/64 loss: -0.12985092401504517
Batch 46/64 loss: -0.10289227962493896
Batch 47/64 loss: -0.12917524576187134
Batch 48/64 loss: -0.1416391134262085
Batch 49/64 loss: -0.11093413829803467
Batch 50/64 loss: -0.13793176412582397
Batch 51/64 loss: -0.14019685983657837
Batch 52/64 loss: -0.11924010515213013
Batch 53/64 loss: -0.11889421939849854
Batch 54/64 loss: -0.12454837560653687
Batch 55/64 loss: -0.14566951990127563
Batch 56/64 loss: -0.1394251585006714
Batch 57/64 loss: -0.12758058309555054
Batch 58/64 loss: -0.12576091289520264
Batch 59/64 loss: -0.11118453741073608
Batch 60/64 loss: -0.1355392336845398
Batch 61/64 loss: -0.14562320709228516
Batch 62/64 loss: -0.13153094053268433
Batch 63/64 loss: -0.11424100399017334
Batch 64/64 loss: -0.12649095058441162
Epoch 274  Train loss: -0.12712668858322443  Val loss: -0.02361452477084812
Epoch 275
-------------------------------
Batch 1/64 loss: -0.14554613828659058
Batch 2/64 loss: -0.12386614084243774
Batch 3/64 loss: -0.13888585567474365
Batch 4/64 loss: -0.1350300908088684
Batch 5/64 loss: -0.1369560956954956
Batch 6/64 loss: -0.12109005451202393
Batch 7/64 loss: -0.13956260681152344
Batch 8/64 loss: -0.11257779598236084
Batch 9/64 loss: -0.13350224494934082
Batch 10/64 loss: -0.12550806999206543
Batch 11/64 loss: -0.1298518180847168
Batch 12/64 loss: -0.13787120580673218
Batch 13/64 loss: -0.12746965885162354
Batch 14/64 loss: -0.12362074851989746
Batch 15/64 loss: -0.11858129501342773
Batch 16/64 loss: -0.13230538368225098
Batch 17/64 loss: -0.14458954334259033
Batch 18/64 loss: -0.13845014572143555
Batch 19/64 loss: -0.11608505249023438
Batch 20/64 loss: -0.11891072988510132
Batch 21/64 loss: -0.1221543550491333
Batch 22/64 loss: -0.13250994682312012
Batch 23/64 loss: -0.13443678617477417
Batch 24/64 loss: -0.13184642791748047
Batch 25/64 loss: -0.1268276572227478
Batch 26/64 loss: -0.1376572847366333
Batch 27/64 loss: -0.12271255254745483
Batch 28/64 loss: -0.12799853086471558
Batch 29/64 loss: -0.11909878253936768
Batch 30/64 loss: -0.12873148918151855
Batch 31/64 loss: -0.12974542379379272
Batch 32/64 loss: -0.1310444474220276
Batch 33/64 loss: -0.11910361051559448
Batch 34/64 loss: -0.1141812801361084
Batch 35/64 loss: -0.12590229511260986
Batch 36/64 loss: -0.13004130125045776
Batch 37/64 loss: -0.10898756980895996
Batch 38/64 loss: -0.13355112075805664
Batch 39/64 loss: -0.129050612449646
Batch 40/64 loss: -0.1304861307144165
Batch 41/64 loss: -0.13391947746276855
Batch 42/64 loss: -0.12783211469650269
Batch 43/64 loss: -0.11794441938400269
Batch 44/64 loss: -0.1307494044303894
Batch 45/64 loss: -0.11263787746429443
Batch 46/64 loss: -0.1383494734764099
Batch 47/64 loss: -0.10937607288360596
Batch 48/64 loss: -0.11342072486877441
Batch 49/64 loss: -0.12686944007873535
Batch 50/64 loss: -0.11744701862335205
Batch 51/64 loss: -0.11222827434539795
Batch 52/64 loss: -0.11550134420394897
Batch 53/64 loss: -0.12319749593734741
Batch 54/64 loss: -0.13106220960617065
Batch 55/64 loss: -0.1316196322441101
Batch 56/64 loss: -0.1376468539237976
Batch 57/64 loss: -0.12041085958480835
Batch 58/64 loss: -0.12380999326705933
Batch 59/64 loss: -0.1322963833808899
Batch 60/64 loss: -0.1372835636138916
Batch 61/64 loss: -0.12520581483840942
Batch 62/64 loss: -0.13061803579330444
Batch 63/64 loss: -0.11663198471069336
Batch 64/64 loss: -0.13029593229293823
Epoch 275  Train loss: -0.12706052859624226  Val loss: -0.019380046534784062
Epoch 276
-------------------------------
Batch 1/64 loss: -0.12969839572906494
Batch 2/64 loss: -0.14086008071899414
Batch 3/64 loss: -0.12592589855194092
Batch 4/64 loss: -0.13736510276794434
Batch 5/64 loss: -0.12842702865600586
Batch 6/64 loss: -0.1353244185447693
Batch 7/64 loss: -0.12608873844146729
Batch 8/64 loss: -0.1424427032470703
Batch 9/64 loss: -0.12859731912612915
Batch 10/64 loss: -0.12371224164962769
Batch 11/64 loss: -0.09848374128341675
Batch 12/64 loss: -0.13320034742355347
Batch 13/64 loss: -0.1434156894683838
Batch 14/64 loss: -0.12962573766708374
Batch 15/64 loss: -0.11979281902313232
Batch 16/64 loss: -0.1261124610900879
Batch 17/64 loss: -0.13900327682495117
Batch 18/64 loss: -0.13381940126419067
Batch 19/64 loss: -0.11169016361236572
Batch 20/64 loss: -0.10090208053588867
Batch 21/64 loss: -0.14204883575439453
Batch 22/64 loss: -0.125318706035614
Batch 23/64 loss: -0.13335728645324707
Batch 24/64 loss: -0.11832916736602783
Batch 25/64 loss: -0.13923335075378418
Batch 26/64 loss: -0.13370972871780396
Batch 27/64 loss: -0.13314539194107056
Batch 28/64 loss: -0.12719982862472534
Batch 29/64 loss: -0.12762165069580078
Batch 30/64 loss: -0.13284075260162354
Batch 31/64 loss: -0.13765156269073486
Batch 32/64 loss: -0.12994790077209473
Batch 33/64 loss: -0.14189010858535767
Batch 34/64 loss: -0.12445574998855591
Batch 35/64 loss: -0.13156074285507202
Batch 36/64 loss: -0.12707817554473877
Batch 37/64 loss: -0.1389254927635193
Batch 38/64 loss: -0.1350608468055725
Batch 39/64 loss: -0.11448252201080322
Batch 40/64 loss: -0.10245442390441895
Batch 41/64 loss: -0.12363195419311523
Batch 42/64 loss: -0.11448252201080322
Batch 43/64 loss: -0.13315516710281372
Batch 44/64 loss: -0.12867379188537598
Batch 45/64 loss: -0.1251956820487976
Batch 46/64 loss: -0.14879965782165527
Batch 47/64 loss: -0.12406313419342041
Batch 48/64 loss: -0.14508962631225586
Batch 49/64 loss: -0.13234639167785645
Batch 50/64 loss: -0.13204044103622437
Batch 51/64 loss: -0.13446342945098877
Batch 52/64 loss: -0.1312430500984192
Batch 53/64 loss: -0.09358352422714233
Batch 54/64 loss: -0.12324690818786621
Batch 55/64 loss: -0.12637287378311157
Batch 56/64 loss: -0.10953104496002197
Batch 57/64 loss: -0.12693512439727783
Batch 58/64 loss: -0.14269042015075684
Batch 59/64 loss: -0.12374842166900635
Batch 60/64 loss: -0.13256901502609253
Batch 61/64 loss: -0.13260293006896973
Batch 62/64 loss: -0.12416386604309082
Batch 63/64 loss: -0.11105769872665405
Batch 64/64 loss: -0.11790800094604492
Epoch 276  Train loss: -0.12798302033368278  Val loss: -0.02133263755090458
Epoch 277
-------------------------------
Batch 1/64 loss: -0.13387596607208252
Batch 2/64 loss: -0.12626707553863525
Batch 3/64 loss: -0.11912286281585693
Batch 4/64 loss: -0.14169222116470337
Batch 5/64 loss: -0.13412922620773315
Batch 6/64 loss: -0.1568669080734253
Batch 7/64 loss: -0.11798018217086792
Batch 8/64 loss: -0.09880071878433228
Batch 9/64 loss: -0.11250501871109009
Batch 10/64 loss: -0.1368730068206787
Batch 11/64 loss: -0.11663699150085449
Batch 12/64 loss: -0.12810349464416504
Batch 13/64 loss: -0.12055283784866333
Batch 14/64 loss: -0.13004130125045776
Batch 15/64 loss: -0.13220977783203125
Batch 16/64 loss: -0.12112867832183838
Batch 17/64 loss: -0.149735689163208
Batch 18/64 loss: -0.13284039497375488
Batch 19/64 loss: -0.13102638721466064
Batch 20/64 loss: -0.10446393489837646
Batch 21/64 loss: -0.14210718870162964
Batch 22/64 loss: -0.1102980375289917
Batch 23/64 loss: -0.12234359979629517
Batch 24/64 loss: -0.1266329288482666
Batch 25/64 loss: -0.12978899478912354
Batch 26/64 loss: -0.1214941143989563
Batch 27/64 loss: -0.1257736086845398
Batch 28/64 loss: -0.13589614629745483
Batch 29/64 loss: -0.11914706230163574
Batch 30/64 loss: -0.11221539974212646
Batch 31/64 loss: -0.11783111095428467
Batch 32/64 loss: -0.13120412826538086
Batch 33/64 loss: -0.1125342845916748
Batch 34/64 loss: -0.11823135614395142
Batch 35/64 loss: -0.1302306056022644
Batch 36/64 loss: -0.1289007067680359
Batch 37/64 loss: -0.13402795791625977
Batch 38/64 loss: -0.12827998399734497
Batch 39/64 loss: -0.12992054224014282
Batch 40/64 loss: -0.13688290119171143
Batch 41/64 loss: -0.120944082736969
Batch 42/64 loss: -0.1453183889389038
Batch 43/64 loss: -0.11385482549667358
Batch 44/64 loss: -0.125133216381073
Batch 45/64 loss: -0.11121511459350586
Batch 46/64 loss: -0.12112939357757568
Batch 47/64 loss: -0.11100971698760986
Batch 48/64 loss: -0.12246668338775635
Batch 49/64 loss: -0.1323148012161255
Batch 50/64 loss: -0.12778323888778687
Batch 51/64 loss: -0.14150846004486084
Batch 52/64 loss: -0.12147223949432373
Batch 53/64 loss: -0.1407710313796997
Batch 54/64 loss: -0.13503092527389526
Batch 55/64 loss: -0.13638442754745483
Batch 56/64 loss: -0.12486982345581055
Batch 57/64 loss: -0.1392567753791809
Batch 58/64 loss: -0.12886953353881836
Batch 59/64 loss: -0.13239353895187378
Batch 60/64 loss: -0.11136502027511597
Batch 61/64 loss: -0.13283294439315796
Batch 62/64 loss: -0.12417995929718018
Batch 63/64 loss: -0.12268418073654175
Batch 64/64 loss: -0.12401407957077026
Epoch 277  Train loss: -0.12665713277517582  Val loss: -0.0232518659424536
Epoch 278
-------------------------------
Batch 1/64 loss: -0.1206287145614624
Batch 2/64 loss: -0.13900494575500488
Batch 3/64 loss: -0.13557583093643188
Batch 4/64 loss: -0.1310475468635559
Batch 5/64 loss: -0.1443796157836914
Batch 6/64 loss: -0.13523244857788086
Batch 7/64 loss: -0.1309850811958313
Batch 8/64 loss: -0.1265050768852234
Batch 9/64 loss: -0.14359861612319946
Batch 10/64 loss: -0.11399364471435547
Batch 11/64 loss: -0.1325628161430359
Batch 12/64 loss: -0.13218986988067627
Batch 13/64 loss: -0.13886088132858276
Batch 14/64 loss: -0.13694095611572266
Batch 15/64 loss: -0.14981257915496826
Batch 16/64 loss: -0.12561500072479248
Batch 17/64 loss: -0.12397068738937378
Batch 18/64 loss: -0.14825451374053955
Batch 19/64 loss: -0.12613272666931152
Batch 20/64 loss: -0.13339072465896606
Batch 21/64 loss: -0.14906519651412964
Batch 22/64 loss: -0.10893720388412476
Batch 23/64 loss: -0.12926363945007324
Batch 24/64 loss: -0.11991709470748901
Batch 25/64 loss: -0.1323375701904297
Batch 26/64 loss: -0.14249062538146973
Batch 27/64 loss: -0.13673663139343262
Batch 28/64 loss: -0.13337790966033936
Batch 29/64 loss: -0.12268543243408203
Batch 30/64 loss: -0.13853579759597778
Batch 31/64 loss: -0.12380152940750122
Batch 32/64 loss: -0.12320983409881592
Batch 33/64 loss: -0.11837869882583618
Batch 34/64 loss: -0.1332044005393982
Batch 35/64 loss: -0.12130773067474365
Batch 36/64 loss: -0.13488048315048218
Batch 37/64 loss: -0.1381368637084961
Batch 38/64 loss: -0.1383960247039795
Batch 39/64 loss: -0.11471158266067505
Batch 40/64 loss: -0.14221346378326416
Batch 41/64 loss: -0.14742743968963623
Batch 42/64 loss: -0.12251538038253784
Batch 43/64 loss: -0.10653471946716309
Batch 44/64 loss: -0.13663893938064575
Batch 45/64 loss: -0.13882970809936523
Batch 46/64 loss: -0.13212931156158447
Batch 47/64 loss: -0.10973232984542847
Batch 48/64 loss: -0.1339501142501831
Batch 49/64 loss: -0.12732410430908203
Batch 50/64 loss: -0.13750624656677246
Batch 51/64 loss: -0.13349813222885132
Batch 52/64 loss: -0.12784957885742188
Batch 53/64 loss: -0.12377190589904785
Batch 54/64 loss: -0.13109827041625977
Batch 55/64 loss: -0.1382218599319458
Batch 56/64 loss: -0.11007481813430786
Batch 57/64 loss: -0.13610798120498657
Batch 58/64 loss: -0.13022565841674805
Batch 59/64 loss: -0.13770288228988647
Batch 60/64 loss: -0.12494218349456787
Batch 61/64 loss: -0.1426563262939453
Batch 62/64 loss: -0.12391620874404907
Batch 63/64 loss: -0.1300722360610962
Batch 64/64 loss: -0.13140243291854858
Epoch 278  Train loss: -0.1310046768655964  Val loss: -0.02579170485952056
Epoch 279
-------------------------------
Batch 1/64 loss: -0.12423384189605713
Batch 2/64 loss: -0.13611048460006714
Batch 3/64 loss: -0.1524810791015625
Batch 4/64 loss: -0.14469921588897705
Batch 5/64 loss: -0.14217877388000488
Batch 6/64 loss: -0.14936649799346924
Batch 7/64 loss: -0.15429550409317017
Batch 8/64 loss: -0.12660473585128784
Batch 9/64 loss: -0.15527021884918213
Batch 10/64 loss: -0.12994003295898438
Batch 11/64 loss: -0.10942262411117554
Batch 12/64 loss: -0.14847707748413086
Batch 13/64 loss: -0.13727396726608276
Batch 14/64 loss: -0.13428962230682373
Batch 15/64 loss: -0.1397201418876648
Batch 16/64 loss: -0.1499241590499878
Batch 17/64 loss: -0.1409151554107666
Batch 18/64 loss: -0.1519148349761963
Batch 19/64 loss: -0.1459672451019287
Batch 20/64 loss: -0.125774085521698
Batch 21/64 loss: -0.11627197265625
Batch 22/64 loss: -0.11115068197250366
Batch 23/64 loss: -0.12580716609954834
Batch 24/64 loss: -0.1055678129196167
Batch 25/64 loss: -0.13341915607452393
Batch 26/64 loss: -0.13697171211242676
Batch 27/64 loss: -0.12662559747695923
Batch 28/64 loss: -0.14094603061676025
Batch 29/64 loss: -0.11725425720214844
Batch 30/64 loss: -0.11190420389175415
Batch 31/64 loss: -0.11468029022216797
Batch 32/64 loss: -0.13219410181045532
Batch 33/64 loss: -0.12821495532989502
Batch 34/64 loss: -0.12052291631698608
Batch 35/64 loss: -0.1248006820678711
Batch 36/64 loss: -0.13800525665283203
Batch 37/64 loss: -0.1484760046005249
Batch 38/64 loss: -0.1389448642730713
Batch 39/64 loss: -0.13895440101623535
Batch 40/64 loss: -0.1361880898475647
Batch 41/64 loss: -0.14059865474700928
Batch 42/64 loss: -0.13087427616119385
Batch 43/64 loss: -0.12640315294265747
Batch 44/64 loss: -0.12386631965637207
Batch 45/64 loss: -0.0879511833190918
Batch 46/64 loss: -0.1277216672897339
Batch 47/64 loss: -0.12475860118865967
Batch 48/64 loss: -0.12520062923431396
Batch 49/64 loss: -0.11104702949523926
Batch 50/64 loss: -0.15390050411224365
Batch 51/64 loss: -0.13186943531036377
Batch 52/64 loss: -0.10746175050735474
Batch 53/64 loss: -0.14520394802093506
Batch 54/64 loss: -0.12347680330276489
Batch 55/64 loss: -0.12060463428497314
Batch 56/64 loss: -0.12938427925109863
Batch 57/64 loss: -0.15051937103271484
Batch 58/64 loss: -0.13548791408538818
Batch 59/64 loss: -0.10952067375183105
Batch 60/64 loss: -0.1291467547416687
Batch 61/64 loss: -0.11131459474563599
Batch 62/64 loss: -0.13374561071395874
Batch 63/64 loss: -0.14645034074783325
Batch 64/64 loss: -0.1338478922843933
Epoch 279  Train loss: -0.13133574116463756  Val loss: -0.024669298806141333
Epoch 280
-------------------------------
Batch 1/64 loss: -0.11667346954345703
Batch 2/64 loss: -0.13733208179473877
Batch 3/64 loss: -0.10202628374099731
Batch 4/64 loss: -0.1044694185256958
Batch 5/64 loss: -0.12823939323425293
Batch 6/64 loss: -0.15008533000946045
Batch 7/64 loss: -0.14290505647659302
Batch 8/64 loss: -0.13769912719726562
Batch 9/64 loss: -0.12063729763031006
Batch 10/64 loss: -0.10852664709091187
Batch 11/64 loss: -0.1447353959083557
Batch 12/64 loss: -0.14572691917419434
Batch 13/64 loss: -0.13145744800567627
Batch 14/64 loss: -0.1260492205619812
Batch 15/64 loss: -0.12624478340148926
Batch 16/64 loss: -0.1358540654182434
Batch 17/64 loss: -0.14262455701828003
Batch 18/64 loss: -0.1255171298980713
Batch 19/64 loss: -0.10267066955566406
Batch 20/64 loss: -0.13071972131729126
Batch 21/64 loss: -0.1338644027709961
Batch 22/64 loss: -0.1397513747215271
Batch 23/64 loss: -0.13334614038467407
Batch 24/64 loss: -0.12201356887817383
Batch 25/64 loss: -0.10131728649139404
Batch 26/64 loss: -0.13921195268630981
Batch 27/64 loss: -0.10247611999511719
Batch 28/64 loss: -0.13273745775222778
Batch 29/64 loss: -0.09615403413772583
Batch 30/64 loss: -0.13836324214935303
Batch 31/64 loss: -0.1421845555305481
Batch 32/64 loss: -0.12244313955307007
Batch 33/64 loss: -0.11579287052154541
Batch 34/64 loss: -0.13988494873046875
Batch 35/64 loss: -0.13797509670257568
Batch 36/64 loss: -0.12817221879959106
Batch 37/64 loss: -0.13123595714569092
Batch 38/64 loss: -0.15035271644592285
Batch 39/64 loss: -0.12963515520095825
Batch 40/64 loss: -0.11704307794570923
Batch 41/64 loss: -0.14621448516845703
Batch 42/64 loss: -0.1441974639892578
Batch 43/64 loss: -0.11475443840026855
Batch 44/64 loss: -0.12299418449401855
Batch 45/64 loss: -0.123374342918396
Batch 46/64 loss: -0.11824071407318115
Batch 47/64 loss: -0.13018667697906494
Batch 48/64 loss: -0.12527215480804443
Batch 49/64 loss: -0.12446141242980957
Batch 50/64 loss: -0.1392161250114441
Batch 51/64 loss: -0.14503175020217896
Batch 52/64 loss: -0.10474485158920288
Batch 53/64 loss: -0.1375129222869873
Batch 54/64 loss: -0.1394895315170288
Batch 55/64 loss: -0.1341003179550171
Batch 56/64 loss: -0.1263054609298706
Batch 57/64 loss: -0.10545843839645386
Batch 58/64 loss: -0.12746703624725342
Batch 59/64 loss: -0.13496488332748413
Batch 60/64 loss: -0.12324577569961548
Batch 61/64 loss: -0.1273769736289978
Batch 62/64 loss: -0.11754179000854492
Batch 63/64 loss: -0.14916253089904785
Batch 64/64 loss: -0.13895320892333984
Epoch 280  Train loss: -0.12830868234821394  Val loss: -0.023669073262165503
Epoch 281
-------------------------------
Batch 1/64 loss: -0.11556458473205566
Batch 2/64 loss: -0.10752445459365845
Batch 3/64 loss: -0.12193542718887329
Batch 4/64 loss: -0.11992067098617554
Batch 5/64 loss: -0.13189071416854858
Batch 6/64 loss: -0.14236772060394287
Batch 7/64 loss: -0.10743331909179688
Batch 8/64 loss: -0.12702548503875732
Batch 9/64 loss: -0.13823860883712769
Batch 10/64 loss: -0.10486042499542236
Batch 11/64 loss: -0.13761931657791138
Batch 12/64 loss: -0.1459726095199585
Batch 13/64 loss: -0.1349530816078186
Batch 14/64 loss: -0.11146342754364014
Batch 15/64 loss: -0.13135462999343872
Batch 16/64 loss: -0.152155339717865
Batch 17/64 loss: -0.15413236618041992
Batch 18/64 loss: -0.14098000526428223
Batch 19/64 loss: -0.1398472785949707
Batch 20/64 loss: -0.11854898929595947
Batch 21/64 loss: -0.14008188247680664
Batch 22/64 loss: -0.12604546546936035
Batch 23/64 loss: -0.12480556964874268
Batch 24/64 loss: -0.09209692478179932
Batch 25/64 loss: -0.13534784317016602
Batch 26/64 loss: -0.13238298892974854
Batch 27/64 loss: -0.12834405899047852
Batch 28/64 loss: -0.12655818462371826
Batch 29/64 loss: -0.13130462169647217
Batch 30/64 loss: -0.1383039951324463
Batch 31/64 loss: -0.12208700180053711
Batch 32/64 loss: -0.12454104423522949
Batch 33/64 loss: -0.1497361660003662
Batch 34/64 loss: -0.13710808753967285
Batch 35/64 loss: -0.1374530792236328
Batch 36/64 loss: -0.1136617660522461
Batch 37/64 loss: -0.1376194953918457
Batch 38/64 loss: -0.12059545516967773
Batch 39/64 loss: -0.11959737539291382
Batch 40/64 loss: -0.12395387887954712
Batch 41/64 loss: -0.1291121244430542
Batch 42/64 loss: -0.14334088563919067
Batch 43/64 loss: -0.10739171504974365
Batch 44/64 loss: -0.13789737224578857
Batch 45/64 loss: -0.1398254632949829
Batch 46/64 loss: -0.12539613246917725
Batch 47/64 loss: -0.1354547142982483
Batch 48/64 loss: -0.1156354546546936
Batch 49/64 loss: -0.12792479991912842
Batch 50/64 loss: -0.10060989856719971
Batch 51/64 loss: -0.14458388090133667
Batch 52/64 loss: -0.13001543283462524
Batch 53/64 loss: -0.12346106767654419
Batch 54/64 loss: -0.12264180183410645
Batch 55/64 loss: -0.13965636491775513
Batch 56/64 loss: -0.10570013523101807
Batch 57/64 loss: -0.12972480058670044
Batch 58/64 loss: -0.1383075714111328
Batch 59/64 loss: -0.12689274549484253
Batch 60/64 loss: -0.14651143550872803
Batch 61/64 loss: -0.1343303918838501
Batch 62/64 loss: -0.11648821830749512
Batch 63/64 loss: -0.1232302188873291
Batch 64/64 loss: -0.12579107284545898
Epoch 281  Train loss: -0.12837473364437327  Val loss: -0.022826944839503756
Epoch 282
-------------------------------
Batch 1/64 loss: -0.13914799690246582
Batch 2/64 loss: -0.1322174072265625
Batch 3/64 loss: -0.14198094606399536
Batch 4/64 loss: -0.14426767826080322
Batch 5/64 loss: -0.10987323522567749
Batch 6/64 loss: -0.11364728212356567
Batch 7/64 loss: -0.1356666088104248
Batch 8/64 loss: -0.13373351097106934
Batch 9/64 loss: -0.13777083158493042
Batch 10/64 loss: -0.10761350393295288
Batch 11/64 loss: -0.10704779624938965
Batch 12/64 loss: -0.14053064584732056
Batch 13/64 loss: -0.13958418369293213
Batch 14/64 loss: -0.12480109930038452
Batch 15/64 loss: -0.1409323811531067
Batch 16/64 loss: -0.12598568201065063
Batch 17/64 loss: -0.1387573480606079
Batch 18/64 loss: -0.1177220344543457
Batch 19/64 loss: -0.1289309859275818
Batch 20/64 loss: -0.132979154586792
Batch 21/64 loss: -0.13016408681869507
Batch 22/64 loss: -0.1313002109527588
Batch 23/64 loss: -0.10434150695800781
Batch 24/64 loss: -0.13946819305419922
Batch 25/64 loss: -0.13032740354537964
Batch 26/64 loss: -0.13310402631759644
Batch 27/64 loss: -0.13531017303466797
Batch 28/64 loss: -0.1500287652015686
Batch 29/64 loss: -0.11801809072494507
Batch 30/64 loss: -0.1315680742263794
Batch 31/64 loss: -0.143094003200531
Batch 32/64 loss: -0.14848798513412476
Batch 33/64 loss: -0.13210928440093994
Batch 34/64 loss: -0.1559637188911438
Batch 35/64 loss: -0.1328221559524536
Batch 36/64 loss: -0.13297724723815918
Batch 37/64 loss: -0.13477075099945068
Batch 38/64 loss: -0.13678371906280518
Batch 39/64 loss: -0.11672592163085938
Batch 40/64 loss: -0.12729966640472412
Batch 41/64 loss: -0.15043944120407104
Batch 42/64 loss: -0.12013208866119385
Batch 43/64 loss: -0.1356680989265442
Batch 44/64 loss: -0.14100342988967896
Batch 45/64 loss: -0.08196079730987549
Batch 46/64 loss: -0.13940781354904175
Batch 47/64 loss: -0.1322077512741089
Batch 48/64 loss: -0.1236579418182373
Batch 49/64 loss: -0.11359202861785889
Batch 50/64 loss: -0.10201257467269897
Batch 51/64 loss: -0.1059499979019165
Batch 52/64 loss: -0.1331392526626587
Batch 53/64 loss: -0.11279952526092529
Batch 54/64 loss: -0.11646884679794312
Batch 55/64 loss: -0.12330114841461182
Batch 56/64 loss: -0.15054678916931152
Batch 57/64 loss: -0.12305933237075806
Batch 58/64 loss: -0.1289321780204773
Batch 59/64 loss: -0.13210833072662354
Batch 60/64 loss: -0.12367045879364014
Batch 61/64 loss: -0.13585913181304932
Batch 62/64 loss: -0.12461704015731812
Batch 63/64 loss: -0.12957781553268433
Batch 64/64 loss: -0.10615158081054688
Epoch 282  Train loss: -0.12890322816138175  Val loss: -0.024426010466113535
Epoch 283
-------------------------------
Batch 1/64 loss: -0.13739192485809326
Batch 2/64 loss: -0.125782310962677
Batch 3/64 loss: -0.1456807255744934
Batch 4/64 loss: -0.1298966407775879
Batch 5/64 loss: -0.13949984312057495
Batch 6/64 loss: -0.09401226043701172
Batch 7/64 loss: -0.1429043412208557
Batch 8/64 loss: -0.1416051983833313
Batch 9/64 loss: -0.14953148365020752
Batch 10/64 loss: -0.13447481393814087
Batch 11/64 loss: -0.13756638765335083
Batch 12/64 loss: -0.12904560565948486
Batch 13/64 loss: -0.13302093744277954
Batch 14/64 loss: -0.1608244776725769
Batch 15/64 loss: -0.15201115608215332
Batch 16/64 loss: -0.13374900817871094
Batch 17/64 loss: -0.09135055541992188
Batch 18/64 loss: -0.13541877269744873
Batch 19/64 loss: -0.1286434531211853
Batch 20/64 loss: -0.1363927125930786
Batch 21/64 loss: -0.125840425491333
Batch 22/64 loss: -0.13233226537704468
Batch 23/64 loss: -0.1354770064353943
Batch 24/64 loss: -0.1435011625289917
Batch 25/64 loss: -0.15683317184448242
Batch 26/64 loss: -0.11669760942459106
Batch 27/64 loss: -0.135356605052948
Batch 28/64 loss: -0.1271173357963562
Batch 29/64 loss: -0.10975801944732666
Batch 30/64 loss: -0.1471659541130066
Batch 31/64 loss: -0.13634353876113892
Batch 32/64 loss: -0.1476312279701233
Batch 33/64 loss: -0.12362140417098999
Batch 34/64 loss: -0.13892769813537598
Batch 35/64 loss: -0.1488783359527588
Batch 36/64 loss: -0.137953519821167
Batch 37/64 loss: -0.12746238708496094
Batch 38/64 loss: -0.1336182951927185
Batch 39/64 loss: -0.12711471319198608
Batch 40/64 loss: -0.14301717281341553
Batch 41/64 loss: -0.09770023822784424
Batch 42/64 loss: -0.14756417274475098
Batch 43/64 loss: -0.12878763675689697
Batch 44/64 loss: -0.11507076025009155
Batch 45/64 loss: -0.12085229158401489
Batch 46/64 loss: -0.1422969102859497
Batch 47/64 loss: -0.14032697677612305
Batch 48/64 loss: -0.11120867729187012
Batch 49/64 loss: -0.13097822666168213
Batch 50/64 loss: -0.10952746868133545
Batch 51/64 loss: -0.13601303100585938
Batch 52/64 loss: -0.13559991121292114
Batch 53/64 loss: -0.13177740573883057
Batch 54/64 loss: -0.11977934837341309
Batch 55/64 loss: -0.13695836067199707
Batch 56/64 loss: -0.13199961185455322
Batch 57/64 loss: -0.14557385444641113
Batch 58/64 loss: -0.13827866315841675
Batch 59/64 loss: -0.13686883449554443
Batch 60/64 loss: -0.12864446640014648
Batch 61/64 loss: -0.14204561710357666
Batch 62/64 loss: -0.142938494682312
Batch 63/64 loss: -0.14540570974349976
Batch 64/64 loss: -0.1373823881149292
Epoch 283  Train loss: -0.1332185712515139  Val loss: -0.02139612141343736
Epoch 284
-------------------------------
Batch 1/64 loss: -0.13675624132156372
Batch 2/64 loss: -0.13499915599822998
Batch 3/64 loss: -0.12293660640716553
Batch 4/64 loss: -0.14905375242233276
Batch 5/64 loss: -0.15162241458892822
Batch 6/64 loss: -0.12076950073242188
Batch 7/64 loss: -0.10973918437957764
Batch 8/64 loss: -0.10494792461395264
Batch 9/64 loss: -0.12256741523742676
Batch 10/64 loss: -0.12838643789291382
Batch 11/64 loss: -0.13584566116333008
Batch 12/64 loss: -0.14087986946105957
Batch 13/64 loss: -0.12326622009277344
Batch 14/64 loss: -0.13023269176483154
Batch 15/64 loss: -0.13955271244049072
Batch 16/64 loss: -0.14020955562591553
Batch 17/64 loss: -0.11956089735031128
Batch 18/64 loss: -0.12871897220611572
Batch 19/64 loss: -0.1513211727142334
Batch 20/64 loss: -0.15775901079177856
Batch 21/64 loss: -0.13273465633392334
Batch 22/64 loss: -0.14655184745788574
Batch 23/64 loss: -0.14944517612457275
Batch 24/64 loss: -0.13592934608459473
Batch 25/64 loss: -0.12840056419372559
Batch 26/64 loss: -0.10828238725662231
Batch 27/64 loss: -0.0973249077796936
Batch 28/64 loss: -0.13004302978515625
Batch 29/64 loss: -0.12671351432800293
Batch 30/64 loss: -0.12989741563796997
Batch 31/64 loss: -0.1478254795074463
Batch 32/64 loss: -0.12125849723815918
Batch 33/64 loss: -0.1480116844177246
Batch 34/64 loss: -0.15441763401031494
Batch 35/64 loss: -0.11427128314971924
Batch 36/64 loss: -0.13877546787261963
Batch 37/64 loss: -0.14034998416900635
Batch 38/64 loss: -0.11766409873962402
Batch 39/64 loss: -0.11712419986724854
Batch 40/64 loss: -0.14492231607437134
Batch 41/64 loss: -0.1393612027168274
Batch 42/64 loss: -0.1403380036354065
Batch 43/64 loss: -0.12229901552200317
Batch 44/64 loss: -0.13096857070922852
Batch 45/64 loss: -0.14886820316314697
Batch 46/64 loss: -0.11650556325912476
Batch 47/64 loss: -0.14041566848754883
Batch 48/64 loss: -0.12899982929229736
Batch 49/64 loss: -0.12056529521942139
Batch 50/64 loss: -0.12904679775238037
Batch 51/64 loss: -0.13843291997909546
Batch 52/64 loss: -0.12967538833618164
Batch 53/64 loss: -0.12208116054534912
Batch 54/64 loss: -0.11834412813186646
Batch 55/64 loss: -0.11211907863616943
Batch 56/64 loss: -0.14241474866867065
Batch 57/64 loss: -0.1265673041343689
Batch 58/64 loss: -0.125682532787323
Batch 59/64 loss: -0.14194482564926147
Batch 60/64 loss: -0.13953393697738647
Batch 61/64 loss: -0.12910354137420654
Batch 62/64 loss: -0.1342869997024536
Batch 63/64 loss: -0.10636210441589355
Batch 64/64 loss: -0.12333911657333374
Epoch 284  Train loss: -0.13106651049034268  Val loss: -0.02093930506624307
Epoch 285
-------------------------------
Batch 1/64 loss: -0.13644939661026
Batch 2/64 loss: -0.11287868022918701
Batch 3/64 loss: -0.12765395641326904
Batch 4/64 loss: -0.13632380962371826
Batch 5/64 loss: -0.14754295349121094
Batch 6/64 loss: -0.12971001863479614
Batch 7/64 loss: -0.09522926807403564
Batch 8/64 loss: -0.11267012357711792
Batch 9/64 loss: -0.1455988883972168
Batch 10/64 loss: -0.1192629337310791
Batch 11/64 loss: -0.1354725956916809
Batch 12/64 loss: -0.11731475591659546
Batch 13/64 loss: -0.1264740228652954
Batch 14/64 loss: -0.1349448561668396
Batch 15/64 loss: -0.14026308059692383
Batch 16/64 loss: -0.1517702341079712
Batch 17/64 loss: -0.13780498504638672
Batch 18/64 loss: -0.13790667057037354
Batch 19/64 loss: -0.14713162183761597
Batch 20/64 loss: -0.1473560929298401
Batch 21/64 loss: -0.1500626802444458
Batch 22/64 loss: -0.14170849323272705
Batch 23/64 loss: -0.12731778621673584
Batch 24/64 loss: -0.13742804527282715
Batch 25/64 loss: -0.1151285171508789
Batch 26/64 loss: -0.13726681470870972
Batch 27/64 loss: -0.1455325484275818
Batch 28/64 loss: -0.13166236877441406
Batch 29/64 loss: -0.11948102712631226
Batch 30/64 loss: -0.13007158041000366
Batch 31/64 loss: -0.13551998138427734
Batch 32/64 loss: -0.1300908923149109
Batch 33/64 loss: -0.14559578895568848
Batch 34/64 loss: -0.15715616941452026
Batch 35/64 loss: -0.12911301851272583
Batch 36/64 loss: -0.15183424949645996
Batch 37/64 loss: -0.1351897120475769
Batch 38/64 loss: -0.12964165210723877
Batch 39/64 loss: -0.14891493320465088
Batch 40/64 loss: -0.14661341905593872
Batch 41/64 loss: -0.1448785662651062
Batch 42/64 loss: -0.13287198543548584
Batch 43/64 loss: -0.1368483304977417
Batch 44/64 loss: -0.13316619396209717
Batch 45/64 loss: -0.13637197017669678
Batch 46/64 loss: -0.14263343811035156
Batch 47/64 loss: -0.14352351427078247
Batch 48/64 loss: -0.14812958240509033
Batch 49/64 loss: -0.14797532558441162
Batch 50/64 loss: -0.13784664869308472
Batch 51/64 loss: -0.1443462371826172
Batch 52/64 loss: -0.1353169083595276
Batch 53/64 loss: -0.13763201236724854
Batch 54/64 loss: -0.13420498371124268
Batch 55/64 loss: -0.1384732723236084
Batch 56/64 loss: -0.13618111610412598
Batch 57/64 loss: -0.15395855903625488
Batch 58/64 loss: -0.11630868911743164
Batch 59/64 loss: -0.13915425539016724
Batch 60/64 loss: -0.12605595588684082
Batch 61/64 loss: -0.13361287117004395
Batch 62/64 loss: -0.13156908750534058
Batch 63/64 loss: -0.13985073566436768
Batch 64/64 loss: -0.12849557399749756
Epoch 285  Train loss: -0.13572345948686787  Val loss: -0.022299940643441638
Epoch 286
-------------------------------
Batch 1/64 loss: -0.15163910388946533
Batch 2/64 loss: -0.1319645643234253
Batch 3/64 loss: -0.14668160676956177
Batch 4/64 loss: -0.1391022801399231
Batch 5/64 loss: -0.12720191478729248
Batch 6/64 loss: -0.13888460397720337
Batch 7/64 loss: -0.13145577907562256
Batch 8/64 loss: -0.11912286281585693
Batch 9/64 loss: -0.13020825386047363
Batch 10/64 loss: -0.13974565267562866
Batch 11/64 loss: -0.14938879013061523
Batch 12/64 loss: -0.11829715967178345
Batch 13/64 loss: -0.15040814876556396
Batch 14/64 loss: -0.15284723043441772
Batch 15/64 loss: -0.1420009732246399
Batch 16/64 loss: -0.1227002739906311
Batch 17/64 loss: -0.14702171087265015
Batch 18/64 loss: -0.13117945194244385
Batch 19/64 loss: -0.09604477882385254
Batch 20/64 loss: -0.12591415643692017
Batch 21/64 loss: -0.13518226146697998
Batch 22/64 loss: -0.13736963272094727
Batch 23/64 loss: -0.13981175422668457
Batch 24/64 loss: -0.1567070484161377
Batch 25/64 loss: -0.14243876934051514
Batch 26/64 loss: -0.12053191661834717
Batch 27/64 loss: -0.1438513994216919
Batch 28/64 loss: -0.1293228268623352
Batch 29/64 loss: -0.13958120346069336
Batch 30/64 loss: -0.12242048978805542
Batch 31/64 loss: -0.12182855606079102
Batch 32/64 loss: -0.12211388349533081
Batch 33/64 loss: -0.12401312589645386
Batch 34/64 loss: -0.14146053791046143
Batch 35/64 loss: -0.1468173861503601
Batch 36/64 loss: -0.12725412845611572
Batch 37/64 loss: -0.12075859308242798
Batch 38/64 loss: -0.12376970052719116
Batch 39/64 loss: -0.14279282093048096
Batch 40/64 loss: -0.11959093809127808
Batch 41/64 loss: -0.14147520065307617
Batch 42/64 loss: -0.10791516304016113
Batch 43/64 loss: -0.12923455238342285
Batch 44/64 loss: -0.13154280185699463
Batch 45/64 loss: -0.11471295356750488
Batch 46/64 loss: -0.1521667242050171
Batch 47/64 loss: -0.1296183466911316
Batch 48/64 loss: -0.12053835391998291
Batch 49/64 loss: -0.13992810249328613
Batch 50/64 loss: -0.1401001214981079
Batch 51/64 loss: -0.1203688383102417
Batch 52/64 loss: -0.12949907779693604
Batch 53/64 loss: -0.1409420371055603
Batch 54/64 loss: -0.14956212043762207
Batch 55/64 loss: -0.13405275344848633
Batch 56/64 loss: -0.14854168891906738
Batch 57/64 loss: -0.13708090782165527
Batch 58/64 loss: -0.1344318985939026
Batch 59/64 loss: -0.1405472755432129
Batch 60/64 loss: -0.13886046409606934
Batch 61/64 loss: -0.11987894773483276
Batch 62/64 loss: -0.14277923107147217
Batch 63/64 loss: -0.13233953714370728
Batch 64/64 loss: -0.1334344744682312
Epoch 286  Train loss: -0.13373520350923726  Val loss: -0.02206430299994872
Epoch 287
-------------------------------
Batch 1/64 loss: -0.11987364292144775
Batch 2/64 loss: -0.13334429264068604
Batch 3/64 loss: -0.13055676221847534
Batch 4/64 loss: -0.1305202841758728
Batch 5/64 loss: -0.13254547119140625
Batch 6/64 loss: -0.10407233238220215
Batch 7/64 loss: -0.12489086389541626
Batch 8/64 loss: -0.11190837621688843
Batch 9/64 loss: -0.13456284999847412
Batch 10/64 loss: -0.14607739448547363
Batch 11/64 loss: -0.1299108862876892
Batch 12/64 loss: -0.14553004503250122
Batch 13/64 loss: -0.13644146919250488
Batch 14/64 loss: -0.12797898054122925
Batch 15/64 loss: -0.14096379280090332
Batch 16/64 loss: -0.12209397554397583
Batch 17/64 loss: -0.12404143810272217
Batch 18/64 loss: -0.10591256618499756
Batch 19/64 loss: -0.11341530084609985
Batch 20/64 loss: -0.09901601076126099
Batch 21/64 loss: -0.12467825412750244
Batch 22/64 loss: -0.1464449167251587
Batch 23/64 loss: -0.1549730896949768
Batch 24/64 loss: -0.14240139722824097
Batch 25/64 loss: -0.11924457550048828
Batch 26/64 loss: -0.11056596040725708
Batch 27/64 loss: -0.12099337577819824
Batch 28/64 loss: -0.1422305703163147
Batch 29/64 loss: -0.13567405939102173
Batch 30/64 loss: -0.12618088722229004
Batch 31/64 loss: -0.13367581367492676
Batch 32/64 loss: -0.13402175903320312
Batch 33/64 loss: -0.11478042602539062
Batch 34/64 loss: -0.12765687704086304
Batch 35/64 loss: -0.10327601432800293
Batch 36/64 loss: -0.11620640754699707
Batch 37/64 loss: -0.13862019777297974
Batch 38/64 loss: -0.13532203435897827
Batch 39/64 loss: -0.12912869453430176
Batch 40/64 loss: -0.141340970993042
Batch 41/64 loss: -0.14539706707000732
Batch 42/64 loss: -0.13834679126739502
Batch 43/64 loss: -0.11925399303436279
Batch 44/64 loss: -0.1322857141494751
Batch 45/64 loss: -0.13027703762054443
Batch 46/64 loss: -0.1416507363319397
Batch 47/64 loss: -0.12846291065216064
Batch 48/64 loss: -0.1436324119567871
Batch 49/64 loss: -0.12630027532577515
Batch 50/64 loss: -0.1250883936882019
Batch 51/64 loss: -0.13474851846694946
Batch 52/64 loss: -0.09466177225112915
Batch 53/64 loss: -0.1301558017730713
Batch 54/64 loss: -0.12929391860961914
Batch 55/64 loss: -0.13020825386047363
Batch 56/64 loss: -0.13968080282211304
Batch 57/64 loss: -0.11360740661621094
Batch 58/64 loss: -0.11387276649475098
Batch 59/64 loss: -0.12167149782180786
Batch 60/64 loss: -0.14251255989074707
Batch 61/64 loss: -0.14515197277069092
Batch 62/64 loss: -0.1333913803100586
Batch 63/64 loss: -0.1336122751235962
Batch 64/64 loss: -0.11705750226974487
Epoch 287  Train loss: -0.12850400630165548  Val loss: -0.0219331686849037
Epoch 288
-------------------------------
Batch 1/64 loss: -0.1477656364440918
Batch 2/64 loss: -0.15998977422714233
Batch 3/64 loss: -0.15621572732925415
Batch 4/64 loss: -0.1473860740661621
Batch 5/64 loss: -0.14623582363128662
Batch 6/64 loss: -0.14386272430419922
Batch 7/64 loss: -0.10711389780044556
Batch 8/64 loss: -0.13223886489868164
Batch 9/64 loss: -0.12247073650360107
Batch 10/64 loss: -0.13880962133407593
Batch 11/64 loss: -0.1194424033164978
Batch 12/64 loss: -0.13980400562286377
Batch 13/64 loss: -0.1225060224533081
Batch 14/64 loss: -0.15145903825759888
Batch 15/64 loss: -0.14097964763641357
Batch 16/64 loss: -0.13712358474731445
Batch 17/64 loss: -0.14979243278503418
Batch 18/64 loss: -0.12862473726272583
Batch 19/64 loss: -0.1339339017868042
Batch 20/64 loss: -0.13585788011550903
Batch 21/64 loss: -0.15115082263946533
Batch 22/64 loss: -0.1457122564315796
Batch 23/64 loss: -0.14189434051513672
Batch 24/64 loss: -0.15615862607955933
Batch 25/64 loss: -0.13409793376922607
Batch 26/64 loss: -0.1528385877609253
Batch 27/64 loss: -0.15322309732437134
Batch 28/64 loss: -0.11967700719833374
Batch 29/64 loss: -0.1342235803604126
Batch 30/64 loss: -0.1402134895324707
Batch 31/64 loss: -0.1515091061592102
Batch 32/64 loss: -0.14260941743850708
Batch 33/64 loss: -0.1324474811553955
Batch 34/64 loss: -0.11679297685623169
Batch 35/64 loss: -0.13518685102462769
Batch 36/64 loss: -0.14533591270446777
Batch 37/64 loss: -0.15599662065505981
Batch 38/64 loss: -0.12547343969345093
Batch 39/64 loss: -0.14396297931671143
Batch 40/64 loss: -0.1214480996131897
Batch 41/64 loss: -0.12802082300186157
Batch 42/64 loss: -0.13597404956817627
Batch 43/64 loss: -0.1339641809463501
Batch 44/64 loss: -0.12171518802642822
Batch 45/64 loss: -0.14114117622375488
Batch 46/64 loss: -0.13002252578735352
Batch 47/64 loss: -0.14899849891662598
Batch 48/64 loss: -0.14352238178253174
Batch 49/64 loss: -0.1374969482421875
Batch 50/64 loss: -0.15073585510253906
Batch 51/64 loss: -0.13306039571762085
Batch 52/64 loss: -0.11664688587188721
Batch 53/64 loss: -0.1177365779876709
Batch 54/64 loss: -0.1388060450553894
Batch 55/64 loss: -0.14327150583267212
Batch 56/64 loss: -0.11758917570114136
Batch 57/64 loss: -0.10290837287902832
Batch 58/64 loss: -0.15376883745193481
Batch 59/64 loss: -0.14000439643859863
Batch 60/64 loss: -0.1240646243095398
Batch 61/64 loss: -0.11033618450164795
Batch 62/64 loss: -0.1125761866569519
Batch 63/64 loss: -0.11763185262680054
Batch 64/64 loss: -0.12462282180786133
Epoch 288  Train loss: -0.1357650971880146  Val loss: -0.024019598756049507
Epoch 289
-------------------------------
Batch 1/64 loss: -0.12913119792938232
Batch 2/64 loss: -0.14870679378509521
Batch 3/64 loss: -0.12452763319015503
Batch 4/64 loss: -0.14418184757232666
Batch 5/64 loss: -0.13864785432815552
Batch 6/64 loss: -0.14995944499969482
Batch 7/64 loss: -0.13654667139053345
Batch 8/64 loss: -0.1460437774658203
Batch 9/64 loss: -0.1286773681640625
Batch 10/64 loss: -0.137040376663208
Batch 11/64 loss: -0.11246359348297119
Batch 12/64 loss: -0.13882100582122803
Batch 13/64 loss: -0.11997818946838379
Batch 14/64 loss: -0.14702057838439941
Batch 15/64 loss: -0.10378175973892212
Batch 16/64 loss: -0.1438106894493103
Batch 17/64 loss: -0.13739609718322754
Batch 18/64 loss: -0.12182009220123291
Batch 19/64 loss: -0.14083385467529297
Batch 20/64 loss: -0.13744133710861206
Batch 21/64 loss: -0.1582707166671753
Batch 22/64 loss: -0.12884217500686646
Batch 23/64 loss: -0.13893532752990723
Batch 24/64 loss: -0.13441628217697144
Batch 25/64 loss: -0.11572492122650146
Batch 26/64 loss: -0.14582031965255737
Batch 27/64 loss: -0.13922905921936035
Batch 28/64 loss: -0.13063162565231323
Batch 29/64 loss: -0.14469563961029053
Batch 30/64 loss: -0.1277996301651001
Batch 31/64 loss: -0.13700377941131592
Batch 32/64 loss: -0.1225348711013794
Batch 33/64 loss: -0.16064584255218506
Batch 34/64 loss: -0.12632322311401367
Batch 35/64 loss: -0.1095457673072815
Batch 36/64 loss: -0.15286368131637573
Batch 37/64 loss: -0.13856780529022217
Batch 38/64 loss: -0.1474401354789734
Batch 39/64 loss: -0.1274135708808899
Batch 40/64 loss: -0.14169830083847046
Batch 41/64 loss: -0.12827908992767334
Batch 42/64 loss: -0.14209872484207153
Batch 43/64 loss: -0.13787925243377686
Batch 44/64 loss: -0.1098397970199585
Batch 45/64 loss: -0.15113866329193115
Batch 46/64 loss: -0.11455345153808594
Batch 47/64 loss: -0.13444167375564575
Batch 48/64 loss: -0.1483440399169922
Batch 49/64 loss: -0.12167501449584961
Batch 50/64 loss: -0.11662721633911133
Batch 51/64 loss: -0.10623174905776978
Batch 52/64 loss: -0.12634479999542236
Batch 53/64 loss: -0.14321666955947876
Batch 54/64 loss: -0.12678146362304688
Batch 55/64 loss: -0.14480650424957275
Batch 56/64 loss: -0.14290833473205566
Batch 57/64 loss: -0.13455623388290405
Batch 58/64 loss: -0.1449199914932251
Batch 59/64 loss: -0.12676852941513062
Batch 60/64 loss: -0.12519443035125732
Batch 61/64 loss: -0.1379428505897522
Batch 62/64 loss: -0.11728823184967041
Batch 63/64 loss: -0.11194288730621338
Batch 64/64 loss: -0.14714467525482178
Epoch 289  Train loss: -0.13363719126757453  Val loss: -0.023547701819246168
Epoch 290
-------------------------------
Batch 1/64 loss: -0.14323174953460693
Batch 2/64 loss: -0.14649724960327148
Batch 3/64 loss: -0.1442631483078003
Batch 4/64 loss: -0.1588742733001709
Batch 5/64 loss: -0.1429031491279602
Batch 6/64 loss: -0.13782715797424316
Batch 7/64 loss: -0.14646995067596436
Batch 8/64 loss: -0.14567911624908447
Batch 9/64 loss: -0.13184994459152222
Batch 10/64 loss: -0.13093912601470947
Batch 11/64 loss: -0.13676565885543823
Batch 12/64 loss: -0.14702129364013672
Batch 13/64 loss: -0.11618781089782715
Batch 14/64 loss: -0.14292728900909424
Batch 15/64 loss: -0.14475929737091064
Batch 16/64 loss: -0.1436898112297058
Batch 17/64 loss: -0.1325421929359436
Batch 18/64 loss: -0.13355016708374023
Batch 19/64 loss: -0.10768723487854004
Batch 20/64 loss: -0.14477193355560303
Batch 21/64 loss: -0.14370179176330566
Batch 22/64 loss: -0.13026416301727295
Batch 23/64 loss: -0.13552606105804443
Batch 24/64 loss: -0.14579623937606812
Batch 25/64 loss: -0.12679946422576904
Batch 26/64 loss: -0.13504308462142944
Batch 27/64 loss: -0.1349828839302063
Batch 28/64 loss: -0.12026911973953247
Batch 29/64 loss: -0.14291971921920776
Batch 30/64 loss: -0.14768624305725098
Batch 31/64 loss: -0.14326012134552002
Batch 32/64 loss: -0.13648289442062378
Batch 33/64 loss: -0.11443442106246948
Batch 34/64 loss: -0.11416202783584595
Batch 35/64 loss: -0.12504184246063232
Batch 36/64 loss: -0.14612120389938354
Batch 37/64 loss: -0.14667415618896484
Batch 38/64 loss: -0.13262403011322021
Batch 39/64 loss: -0.141187846660614
Batch 40/64 loss: -0.11832535266876221
Batch 41/64 loss: -0.1254366636276245
Batch 42/64 loss: -0.11963385343551636
Batch 43/64 loss: -0.11869341135025024
Batch 44/64 loss: -0.12339013814926147
Batch 45/64 loss: -0.13001513481140137
Batch 46/64 loss: -0.14090055227279663
Batch 47/64 loss: -0.13985806703567505
Batch 48/64 loss: -0.12963807582855225
Batch 49/64 loss: -0.13451653718948364
Batch 50/64 loss: -0.12193918228149414
Batch 51/64 loss: -0.11385136842727661
Batch 52/64 loss: -0.145094096660614
Batch 53/64 loss: -0.12842798233032227
Batch 54/64 loss: -0.11688101291656494
Batch 55/64 loss: -0.14666306972503662
Batch 56/64 loss: -0.1292669177055359
Batch 57/64 loss: -0.12840735912322998
Batch 58/64 loss: -0.13713878393173218
Batch 59/64 loss: -0.14020544290542603
Batch 60/64 loss: -0.13347601890563965
Batch 61/64 loss: -0.143762469291687
Batch 62/64 loss: -0.1360567808151245
Batch 63/64 loss: -0.139715313911438
Batch 64/64 loss: -0.13429003953933716
Epoch 290  Train loss: -0.13464197621626012  Val loss: -0.020584969586113475
Epoch 291
-------------------------------
Batch 1/64 loss: -0.135917067527771
Batch 2/64 loss: -0.14434432983398438
Batch 3/64 loss: -0.1328834891319275
Batch 4/64 loss: -0.1444748044013977
Batch 5/64 loss: -0.13454270362854004
Batch 6/64 loss: -0.12607455253601074
Batch 7/64 loss: -0.11944222450256348
Batch 8/64 loss: -0.13482260704040527
Batch 9/64 loss: -0.12238258123397827
Batch 10/64 loss: -0.1468493938446045
Batch 11/64 loss: -0.13781213760375977
Batch 12/64 loss: -0.14638394117355347
Batch 13/64 loss: -0.1259745955467224
Batch 14/64 loss: -0.1277732253074646
Batch 15/64 loss: -0.14135169982910156
Batch 16/64 loss: -0.10417443513870239
Batch 17/64 loss: -0.14310455322265625
Batch 18/64 loss: -0.11124962568283081
Batch 19/64 loss: -0.1473461389541626
Batch 20/64 loss: -0.1324440836906433
Batch 21/64 loss: -0.13862991333007812
Batch 22/64 loss: -0.14332443475723267
Batch 23/64 loss: -0.145155668258667
Batch 24/64 loss: -0.13751912117004395
Batch 25/64 loss: -0.13381493091583252
Batch 26/64 loss: -0.1475454568862915
Batch 27/64 loss: -0.14368540048599243
Batch 28/64 loss: -0.14536362886428833
Batch 29/64 loss: -0.10228514671325684
Batch 30/64 loss: -0.12768304347991943
Batch 31/64 loss: -0.1460583209991455
Batch 32/64 loss: -0.13498681783676147
Batch 33/64 loss: -0.13559424877166748
Batch 34/64 loss: -0.1443958878517151
Batch 35/64 loss: -0.13818371295928955
Batch 36/64 loss: -0.12667399644851685
Batch 37/64 loss: -0.14533710479736328
Batch 38/64 loss: -0.11161792278289795
Batch 39/64 loss: -0.14794164896011353
Batch 40/64 loss: -0.13134276866912842
Batch 41/64 loss: -0.14152926206588745
Batch 42/64 loss: -0.14377033710479736
Batch 43/64 loss: -0.14619553089141846
Batch 44/64 loss: -0.13987523317337036
Batch 45/64 loss: -0.1472759246826172
Batch 46/64 loss: -0.14920246601104736
Batch 47/64 loss: -0.12838482856750488
Batch 48/64 loss: -0.138413667678833
Batch 49/64 loss: -0.13903367519378662
Batch 50/64 loss: -0.09674835205078125
Batch 51/64 loss: -0.138353168964386
Batch 52/64 loss: -0.14271652698516846
Batch 53/64 loss: -0.15455734729766846
Batch 54/64 loss: -0.13484764099121094
Batch 55/64 loss: -0.1381981372833252
Batch 56/64 loss: -0.12043684720993042
Batch 57/64 loss: -0.12259238958358765
Batch 58/64 loss: -0.13110268115997314
Batch 59/64 loss: -0.1566358208656311
Batch 60/64 loss: -0.13941454887390137
Batch 61/64 loss: -0.13046467304229736
Batch 62/64 loss: -0.13148200511932373
Batch 63/64 loss: -0.141349196434021
Batch 64/64 loss: -0.14659404754638672
Epoch 291  Train loss: -0.13567079469269397  Val loss: -0.024703071699109683
Epoch 292
-------------------------------
Batch 1/64 loss: -0.13647043704986572
Batch 2/64 loss: -0.14093244075775146
Batch 3/64 loss: -0.11206775903701782
Batch 4/64 loss: -0.15561532974243164
Batch 5/64 loss: -0.15409553050994873
Batch 6/64 loss: -0.12540429830551147
Batch 7/64 loss: -0.16646146774291992
Batch 8/64 loss: -0.14672935009002686
Batch 9/64 loss: -0.14133167266845703
Batch 10/64 loss: -0.14891642332077026
Batch 11/64 loss: -0.14088565111160278
Batch 12/64 loss: -0.13627570867538452
Batch 13/64 loss: -0.14903658628463745
Batch 14/64 loss: -0.15040886402130127
Batch 15/64 loss: -0.1370489001274109
Batch 16/64 loss: -0.12799286842346191
Batch 17/64 loss: -0.14069044589996338
Batch 18/64 loss: -0.11844807863235474
Batch 19/64 loss: -0.127335786819458
Batch 20/64 loss: -0.14444690942764282
Batch 21/64 loss: -0.1331712007522583
Batch 22/64 loss: -0.1501234769821167
Batch 23/64 loss: -0.14464515447616577
Batch 24/64 loss: -0.14161401987075806
Batch 25/64 loss: -0.1366424560546875
Batch 26/64 loss: -0.11830562353134155
Batch 27/64 loss: -0.14910393953323364
Batch 28/64 loss: -0.14807093143463135
Batch 29/64 loss: -0.1473759412765503
Batch 30/64 loss: -0.11965912580490112
Batch 31/64 loss: -0.11086571216583252
Batch 32/64 loss: -0.12723004817962646
Batch 33/64 loss: -0.11288374662399292
Batch 34/64 loss: -0.15083158016204834
Batch 35/64 loss: -0.14007234573364258
Batch 36/64 loss: -0.12895232439041138
Batch 37/64 loss: -0.15250813961029053
Batch 38/64 loss: -0.145798921585083
Batch 39/64 loss: -0.14437627792358398
Batch 40/64 loss: -0.11440908908843994
Batch 41/64 loss: -0.1267763376235962
Batch 42/64 loss: -0.13213622570037842
Batch 43/64 loss: -0.12906795740127563
Batch 44/64 loss: -0.153314471244812
Batch 45/64 loss: -0.11907082796096802
Batch 46/64 loss: -0.14168989658355713
Batch 47/64 loss: -0.13496088981628418
Batch 48/64 loss: -0.13415533304214478
Batch 49/64 loss: -0.15177065134048462
Batch 50/64 loss: -0.1145816445350647
Batch 51/64 loss: -0.1354200839996338
Batch 52/64 loss: -0.14392048120498657
Batch 53/64 loss: -0.1307806372642517
Batch 54/64 loss: -0.12805891036987305
Batch 55/64 loss: -0.1299535632133484
Batch 56/64 loss: -0.13122886419296265
Batch 57/64 loss: -0.11367446184158325
Batch 58/64 loss: -0.1317507028579712
Batch 59/64 loss: -0.11334586143493652
Batch 60/64 loss: -0.12864869832992554
Batch 61/64 loss: -0.12772178649902344
Batch 62/64 loss: -0.1113121509552002
Batch 63/64 loss: -0.11868047714233398
Batch 64/64 loss: -0.11255133152008057
Epoch 292  Train loss: -0.1346457878748576  Val loss: -0.019346219362671842
Epoch 293
-------------------------------
Batch 1/64 loss: -0.14016646146774292
Batch 2/64 loss: -0.11985146999359131
Batch 3/64 loss: -0.14078468084335327
Batch 4/64 loss: -0.11999481916427612
Batch 5/64 loss: -0.11535167694091797
Batch 6/64 loss: -0.13395100831985474
Batch 7/64 loss: -0.15598386526107788
Batch 8/64 loss: -0.1392512321472168
Batch 9/64 loss: -0.13238519430160522
Batch 10/64 loss: -0.14400029182434082
Batch 11/64 loss: -0.13517379760742188
Batch 12/64 loss: -0.14769834280014038
Batch 13/64 loss: -0.14515334367752075
Batch 14/64 loss: -0.1263282299041748
Batch 15/64 loss: -0.1407344937324524
Batch 16/64 loss: -0.10611516237258911
Batch 17/64 loss: -0.1341342329978943
Batch 18/64 loss: -0.1418764591217041
Batch 19/64 loss: -0.15096205472946167
Batch 20/64 loss: -0.13319015502929688
Batch 21/64 loss: -0.11279165744781494
Batch 22/64 loss: -0.13158297538757324
Batch 23/64 loss: -0.12735867500305176
Batch 24/64 loss: -0.13850939273834229
Batch 25/64 loss: -0.15275555849075317
Batch 26/64 loss: -0.1285352110862732
Batch 27/64 loss: -0.1365841031074524
Batch 28/64 loss: -0.14806324243545532
Batch 29/64 loss: -0.1386549472808838
Batch 30/64 loss: -0.13949662446975708
Batch 31/64 loss: -0.12356430292129517
Batch 32/64 loss: -0.13907760381698608
Batch 33/64 loss: -0.12015914916992188
Batch 34/64 loss: -0.14330041408538818
Batch 35/64 loss: -0.14163720607757568
Batch 36/64 loss: -0.15257900953292847
Batch 37/64 loss: -0.14705514907836914
Batch 38/64 loss: -0.14387595653533936
Batch 39/64 loss: -0.09663158655166626
Batch 40/64 loss: -0.1463909149169922
Batch 41/64 loss: -0.14508986473083496
Batch 42/64 loss: -0.13037627935409546
Batch 43/64 loss: -0.1420416235923767
Batch 44/64 loss: -0.1402972936630249
Batch 45/64 loss: -0.13541489839553833
Batch 46/64 loss: -0.14139807224273682
Batch 47/64 loss: -0.1315273642539978
Batch 48/64 loss: -0.14294195175170898
Batch 49/64 loss: -0.1358795166015625
Batch 50/64 loss: -0.13194704055786133
Batch 51/64 loss: -0.12717938423156738
Batch 52/64 loss: -0.1545678973197937
Batch 53/64 loss: -0.13643920421600342
Batch 54/64 loss: -0.12789493799209595
Batch 55/64 loss: -0.13495886325836182
Batch 56/64 loss: -0.13380086421966553
Batch 57/64 loss: -0.12618416547775269
Batch 58/64 loss: -0.12549585103988647
Batch 59/64 loss: -0.13451242446899414
Batch 60/64 loss: -0.10966938734054565
Batch 61/64 loss: -0.1185598373413086
Batch 62/64 loss: -0.11357855796813965
Batch 63/64 loss: -0.12654078006744385
Batch 64/64 loss: -0.1483510136604309
Epoch 293  Train loss: -0.1344196074149188  Val loss: -0.01765172301289142
Epoch 294
-------------------------------
Batch 1/64 loss: -0.14583450555801392
Batch 2/64 loss: -0.13478344678878784
Batch 3/64 loss: -0.12537193298339844
Batch 4/64 loss: -0.13691949844360352
Batch 5/64 loss: -0.1308513879776001
Batch 6/64 loss: -0.14144772291183472
Batch 7/64 loss: -0.14493489265441895
Batch 8/64 loss: -0.13606971502304077
Batch 9/64 loss: -0.14158999919891357
Batch 10/64 loss: -0.15479254722595215
Batch 11/64 loss: -0.14846742153167725
Batch 12/64 loss: -0.12970131635665894
Batch 13/64 loss: -0.14767515659332275
Batch 14/64 loss: -0.1519312858581543
Batch 15/64 loss: -0.1273098587989807
Batch 16/64 loss: -0.15630877017974854
Batch 17/64 loss: -0.15182137489318848
Batch 18/64 loss: -0.09873723983764648
Batch 19/64 loss: -0.15374553203582764
Batch 20/64 loss: -0.12097561359405518
Batch 21/64 loss: -0.13530153036117554
Batch 22/64 loss: -0.11933249235153198
Batch 23/64 loss: -0.12567013502120972
Batch 24/64 loss: -0.14376360177993774
Batch 25/64 loss: -0.13804185390472412
Batch 26/64 loss: -0.1359105110168457
Batch 27/64 loss: -0.1348610520362854
Batch 28/64 loss: -0.11858677864074707
Batch 29/64 loss: -0.15043175220489502
Batch 30/64 loss: -0.15092509984970093
Batch 31/64 loss: -0.1519748568534851
Batch 32/64 loss: -0.10722315311431885
Batch 33/64 loss: -0.10787248611450195
Batch 34/64 loss: -0.1367846131324768
Batch 35/64 loss: -0.12748008966445923
Batch 36/64 loss: -0.12425261735916138
Batch 37/64 loss: -0.13044601678848267
Batch 38/64 loss: -0.12119525671005249
Batch 39/64 loss: -0.1389053463935852
Batch 40/64 loss: -0.13552749156951904
Batch 41/64 loss: -0.1219819188117981
Batch 42/64 loss: -0.1391778588294983
Batch 43/64 loss: -0.13412928581237793
Batch 44/64 loss: -0.11521434783935547
Batch 45/64 loss: -0.14495635032653809
Batch 46/64 loss: -0.1397625207901001
Batch 47/64 loss: -0.13927483558654785
Batch 48/64 loss: -0.1420544981956482
Batch 49/64 loss: -0.135675847530365
Batch 50/64 loss: -0.14062148332595825
Batch 51/64 loss: -0.13271379470825195
Batch 52/64 loss: -0.14367914199829102
Batch 53/64 loss: -0.13492941856384277
Batch 54/64 loss: -0.13779854774475098
Batch 55/64 loss: -0.1431748867034912
Batch 56/64 loss: -0.13486790657043457
Batch 57/64 loss: -0.1479799747467041
Batch 58/64 loss: -0.12421834468841553
Batch 59/64 loss: -0.14549076557159424
Batch 60/64 loss: -0.13241100311279297
Batch 61/64 loss: -0.10607695579528809
Batch 62/64 loss: -0.15177863836288452
Batch 63/64 loss: -0.13303077220916748
Batch 64/64 loss: -0.1256151795387268
Epoch 294  Train loss: -0.13535633624768725  Val loss: -0.024321456135753095
Epoch 295
-------------------------------
Batch 1/64 loss: -0.12543755769729614
Batch 2/64 loss: -0.15681296586990356
Batch 3/64 loss: -0.14550632238388062
Batch 4/64 loss: -0.1566569209098816
Batch 5/64 loss: -0.13753604888916016
Batch 6/64 loss: -0.15488922595977783
Batch 7/64 loss: -0.15195965766906738
Batch 8/64 loss: -0.13576829433441162
Batch 9/64 loss: -0.1282796859741211
Batch 10/64 loss: -0.13112366199493408
Batch 11/64 loss: -0.15447461605072021
Batch 12/64 loss: -0.14907634258270264
Batch 13/64 loss: -0.13686531782150269
Batch 14/64 loss: -0.1393415331840515
Batch 15/64 loss: -0.15238416194915771
Batch 16/64 loss: -0.14057159423828125
Batch 17/64 loss: -0.1522916555404663
Batch 18/64 loss: -0.15013384819030762
Batch 19/64 loss: -0.136557936668396
Batch 20/64 loss: -0.1508573293685913
Batch 21/64 loss: -0.1521872878074646
Batch 22/64 loss: -0.12875717878341675
Batch 23/64 loss: -0.15177887678146362
Batch 24/64 loss: -0.1496862769126892
Batch 25/64 loss: -0.13995349407196045
Batch 26/64 loss: -0.14482331275939941
Batch 27/64 loss: -0.12859344482421875
Batch 28/64 loss: -0.140508770942688
Batch 29/64 loss: -0.10059821605682373
Batch 30/64 loss: -0.13887488842010498
Batch 31/64 loss: -0.13874423503875732
Batch 32/64 loss: -0.14478015899658203
Batch 33/64 loss: -0.13082998991012573
Batch 34/64 loss: -0.13187074661254883
Batch 35/64 loss: -0.15147370100021362
Batch 36/64 loss: -0.13793015480041504
Batch 37/64 loss: -0.1362447738647461
Batch 38/64 loss: -0.14556783437728882
Batch 39/64 loss: -0.12247323989868164
Batch 40/64 loss: -0.1505032777786255
Batch 41/64 loss: -0.1524401307106018
Batch 42/64 loss: -0.1328464150428772
Batch 43/64 loss: -0.1346791386604309
Batch 44/64 loss: -0.11401945352554321
Batch 45/64 loss: -0.15047544240951538
Batch 46/64 loss: -0.1249396800994873
Batch 47/64 loss: -0.1383315920829773
Batch 48/64 loss: -0.14809954166412354
Batch 49/64 loss: -0.148823082447052
Batch 50/64 loss: -0.12389212846755981
Batch 51/64 loss: -0.10868042707443237
Batch 52/64 loss: -0.11826872825622559
Batch 53/64 loss: -0.11090928316116333
Batch 54/64 loss: -0.0947880744934082
Batch 55/64 loss: -0.1538599729537964
Batch 56/64 loss: -0.11902815103530884
Batch 57/64 loss: -0.14625626802444458
Batch 58/64 loss: -0.12258166074752808
Batch 59/64 loss: -0.10809338092803955
Batch 60/64 loss: -0.13375163078308105
Batch 61/64 loss: -0.1435285210609436
Batch 62/64 loss: -0.09833449125289917
Batch 63/64 loss: -0.13520193099975586
Batch 64/64 loss: -0.14765006303787231
Epoch 295  Train loss: -0.13686699937371646  Val loss: -0.023652088191501053
Epoch 296
-------------------------------
Batch 1/64 loss: -0.13397669792175293
Batch 2/64 loss: -0.13507133722305298
Batch 3/64 loss: -0.1427149772644043
Batch 4/64 loss: -0.14362847805023193
Batch 5/64 loss: -0.13735878467559814
Batch 6/64 loss: -0.13009852170944214
Batch 7/64 loss: -0.11396503448486328
Batch 8/64 loss: -0.13527870178222656
Batch 9/64 loss: -0.13171517848968506
Batch 10/64 loss: -0.13168275356292725
Batch 11/64 loss: -0.15885138511657715
Batch 12/64 loss: -0.11254215240478516
Batch 13/64 loss: -0.12891054153442383
Batch 14/64 loss: -0.13124358654022217
Batch 15/64 loss: -0.15535706281661987
Batch 16/64 loss: -0.126703679561615
Batch 17/64 loss: -0.1405266523361206
Batch 18/64 loss: -0.14384984970092773
Batch 19/64 loss: -0.1340211033821106
Batch 20/64 loss: -0.1450207233428955
Batch 21/64 loss: -0.15093040466308594
Batch 22/64 loss: -0.13924038410186768
Batch 23/64 loss: -0.14899009466171265
Batch 24/64 loss: -0.13501697778701782
Batch 25/64 loss: -0.1190672516822815
Batch 26/64 loss: -0.1289370059967041
Batch 27/64 loss: -0.14184409379959106
Batch 28/64 loss: -0.15244930982589722
Batch 29/64 loss: -0.1481410264968872
Batch 30/64 loss: -0.12960952520370483
Batch 31/64 loss: -0.12764161825180054
Batch 32/64 loss: -0.1255996823310852
Batch 33/64 loss: -0.13553744554519653
Batch 34/64 loss: -0.1517917513847351
Batch 35/64 loss: -0.15291345119476318
Batch 36/64 loss: -0.14319896697998047
Batch 37/64 loss: -0.13189822435379028
Batch 38/64 loss: -0.11037200689315796
Batch 39/64 loss: -0.12978971004486084
Batch 40/64 loss: -0.1416175365447998
Batch 41/64 loss: -0.12388342618942261
Batch 42/64 loss: -0.1331329345703125
Batch 43/64 loss: -0.11684900522232056
Batch 44/64 loss: -0.1406933069229126
Batch 45/64 loss: -0.14099645614624023
Batch 46/64 loss: -0.12805259227752686
Batch 47/64 loss: -0.14235824346542358
Batch 48/64 loss: -0.14008641242980957
Batch 49/64 loss: -0.1542317271232605
Batch 50/64 loss: -0.1604398488998413
Batch 51/64 loss: -0.13461124897003174
Batch 52/64 loss: -0.11083084344863892
Batch 53/64 loss: -0.13674968481063843
Batch 54/64 loss: -0.15019440650939941
Batch 55/64 loss: -0.14343112707138062
Batch 56/64 loss: -0.1243864893913269
Batch 57/64 loss: -0.1492047905921936
Batch 58/64 loss: -0.15214484930038452
Batch 59/64 loss: -0.14456510543823242
Batch 60/64 loss: -0.1551303267478943
Batch 61/64 loss: -0.12426680326461792
Batch 62/64 loss: -0.13175350427627563
Batch 63/64 loss: -0.1508675217628479
Batch 64/64 loss: -0.10933655500411987
Epoch 296  Train loss: -0.13690928220748902  Val loss: -0.020641675519779374
Epoch 297
-------------------------------
Batch 1/64 loss: -0.12846046686172485
Batch 2/64 loss: -0.12075603008270264
Batch 3/64 loss: -0.14999008178710938
Batch 4/64 loss: -0.1273086667060852
Batch 5/64 loss: -0.14976900815963745
Batch 6/64 loss: -0.13423430919647217
Batch 7/64 loss: -0.1410101056098938
Batch 8/64 loss: -0.13563203811645508
Batch 9/64 loss: -0.12188589572906494
Batch 10/64 loss: -0.14474356174468994
Batch 11/64 loss: -0.14850640296936035
Batch 12/64 loss: -0.14657413959503174
Batch 13/64 loss: -0.1413225531578064
Batch 14/64 loss: -0.1332770586013794
Batch 15/64 loss: -0.13604551553726196
Batch 16/64 loss: -0.14073801040649414
Batch 17/64 loss: -0.13501214981079102
Batch 18/64 loss: -0.14794570207595825
Batch 19/64 loss: -0.1361292004585266
Batch 20/64 loss: -0.14161944389343262
Batch 21/64 loss: -0.16224080324172974
Batch 22/64 loss: -0.121357262134552
Batch 23/64 loss: -0.10865402221679688
Batch 24/64 loss: -0.12477695941925049
Batch 25/64 loss: -0.1347033977508545
Batch 26/64 loss: -0.15535861253738403
Batch 27/64 loss: -0.1483812928199768
Batch 28/64 loss: -0.1302996277809143
Batch 29/64 loss: -0.1291791796684265
Batch 30/64 loss: -0.11623901128768921
Batch 31/64 loss: -0.13206899166107178
Batch 32/64 loss: -0.14143335819244385
Batch 33/64 loss: -0.12907421588897705
Batch 34/64 loss: -0.14133572578430176
Batch 35/64 loss: -0.14276748895645142
Batch 36/64 loss: -0.1369166374206543
Batch 37/64 loss: -0.138807475566864
Batch 38/64 loss: -0.14801889657974243
Batch 39/64 loss: -0.1356014609336853
Batch 40/64 loss: -0.14554661512374878
Batch 41/64 loss: -0.14082467555999756
Batch 42/64 loss: -0.12529349327087402
Batch 43/64 loss: -0.1401621699333191
Batch 44/64 loss: -0.14284735918045044
Batch 45/64 loss: -0.12381267547607422
Batch 46/64 loss: -0.14619100093841553
Batch 47/64 loss: -0.12358039617538452
Batch 48/64 loss: -0.14714741706848145
Batch 49/64 loss: -0.14459168910980225
Batch 50/64 loss: -0.1388518214225769
Batch 51/64 loss: -0.1451956033706665
Batch 52/64 loss: -0.13245052099227905
Batch 53/64 loss: -0.14704960584640503
Batch 54/64 loss: -0.1479041576385498
Batch 55/64 loss: -0.13048237562179565
Batch 56/64 loss: -0.1285538673400879
Batch 57/64 loss: -0.12737762928009033
Batch 58/64 loss: -0.13763034343719482
Batch 59/64 loss: -0.14307451248168945
Batch 60/64 loss: -0.13029426336288452
Batch 61/64 loss: -0.13765853643417358
Batch 62/64 loss: -0.14800333976745605
Batch 63/64 loss: -0.14205455780029297
Batch 64/64 loss: -0.1378408670425415
Epoch 297  Train loss: -0.13738249470205868  Val loss: -0.024000238307153237
Epoch 298
-------------------------------
Batch 1/64 loss: -0.16088449954986572
Batch 2/64 loss: -0.14657753705978394
Batch 3/64 loss: -0.15184032917022705
Batch 4/64 loss: -0.12683385610580444
Batch 5/64 loss: -0.1440361738204956
Batch 6/64 loss: -0.16671770811080933
Batch 7/64 loss: -0.12136512994766235
Batch 8/64 loss: -0.15562790632247925
Batch 9/64 loss: -0.1514354944229126
Batch 10/64 loss: -0.14416652917861938
Batch 11/64 loss: -0.12698668241500854
Batch 12/64 loss: -0.11779409646987915
Batch 13/64 loss: -0.14746850728988647
Batch 14/64 loss: -0.15747439861297607
Batch 15/64 loss: -0.16818469762802124
Batch 16/64 loss: -0.1493854522705078
Batch 17/64 loss: -0.14870238304138184
Batch 18/64 loss: -0.14864236116409302
Batch 19/64 loss: -0.1548873782157898
Batch 20/64 loss: -0.1010890007019043
Batch 21/64 loss: -0.12331157922744751
Batch 22/64 loss: -0.14917248487472534
Batch 23/64 loss: -0.1516895294189453
Batch 24/64 loss: -0.14952480792999268
Batch 25/64 loss: -0.14452683925628662
Batch 26/64 loss: -0.14127540588378906
Batch 27/64 loss: -0.16502243280410767
Batch 28/64 loss: -0.14002752304077148
Batch 29/64 loss: -0.13603192567825317
Batch 30/64 loss: -0.11503159999847412
Batch 31/64 loss: -0.13646376132965088
Batch 32/64 loss: -0.11221784353256226
Batch 33/64 loss: -0.15252995491027832
Batch 34/64 loss: -0.14141619205474854
Batch 35/64 loss: -0.11678832769393921
Batch 36/64 loss: -0.1236676573753357
Batch 37/64 loss: -0.12482166290283203
Batch 38/64 loss: -0.13273847103118896
Batch 39/64 loss: -0.15231072902679443
Batch 40/64 loss: -0.13968908786773682
Batch 41/64 loss: -0.13383698463439941
Batch 42/64 loss: -0.15226447582244873
Batch 43/64 loss: -0.13825172185897827
Batch 44/64 loss: -0.14231956005096436
Batch 45/64 loss: -0.1180381178855896
Batch 46/64 loss: -0.12391489744186401
Batch 47/64 loss: -0.15565955638885498
Batch 48/64 loss: -0.1492590308189392
Batch 49/64 loss: -0.13115429878234863
Batch 50/64 loss: -0.15316903591156006
Batch 51/64 loss: -0.15985172986984253
Batch 52/64 loss: -0.138808012008667
Batch 53/64 loss: -0.13507860898971558
Batch 54/64 loss: -0.12901091575622559
Batch 55/64 loss: -0.14061272144317627
Batch 56/64 loss: -0.117442786693573
Batch 57/64 loss: -0.13380324840545654
Batch 58/64 loss: -0.16655927896499634
Batch 59/64 loss: -0.1314375400543213
Batch 60/64 loss: -0.12186264991760254
Batch 61/64 loss: -0.13758057355880737
Batch 62/64 loss: -0.12640750408172607
Batch 63/64 loss: -0.130989670753479
Batch 64/64 loss: -0.12956643104553223
Epoch 298  Train loss: -0.13965248126609653  Val loss: -0.020673811845353378
Epoch 299
-------------------------------
Batch 1/64 loss: -0.1326357126235962
Batch 2/64 loss: -0.14022231101989746
Batch 3/64 loss: -0.1412726640701294
Batch 4/64 loss: -0.15871655941009521
Batch 5/64 loss: -0.14992159605026245
Batch 6/64 loss: -0.1473907232284546
Batch 7/64 loss: -0.15835630893707275
Batch 8/64 loss: -0.14507943391799927
Batch 9/64 loss: -0.14060074090957642
Batch 10/64 loss: -0.12914711236953735
Batch 11/64 loss: -0.14834928512573242
Batch 12/64 loss: -0.16298139095306396
Batch 13/64 loss: -0.1434827446937561
Batch 14/64 loss: -0.13843005895614624
Batch 15/64 loss: -0.14052146673202515
Batch 16/64 loss: -0.1268802285194397
Batch 17/64 loss: -0.14095526933670044
Batch 18/64 loss: -0.14225172996520996
Batch 19/64 loss: -0.14197075366973877
Batch 20/64 loss: -0.14991998672485352
Batch 21/64 loss: -0.1398012638092041
Batch 22/64 loss: -0.14264202117919922
Batch 23/64 loss: -0.12700891494750977
Batch 24/64 loss: -0.15160679817199707
Batch 25/64 loss: -0.09339702129364014
Batch 26/64 loss: -0.13195282220840454
Batch 27/64 loss: -0.15052419900894165
Batch 28/64 loss: -0.1453222632408142
Batch 29/64 loss: -0.14628177881240845
Batch 30/64 loss: -0.1377553939819336
Batch 31/64 loss: -0.1286444067955017
Batch 32/64 loss: -0.13550937175750732
Batch 33/64 loss: -0.1436687707901001
Batch 34/64 loss: -0.10978877544403076
Batch 35/64 loss: -0.12288308143615723
Batch 36/64 loss: -0.13116848468780518
Batch 37/64 loss: -0.11946165561676025
Batch 38/64 loss: -0.1546006202697754
Batch 39/64 loss: -0.15184521675109863
Batch 40/64 loss: -0.1330166459083557
Batch 41/64 loss: -0.13142049312591553
Batch 42/64 loss: -0.1358182430267334
Batch 43/64 loss: -0.12697529792785645
Batch 44/64 loss: -0.13217371702194214
Batch 45/64 loss: -0.1295609474182129
Batch 46/64 loss: -0.12779206037521362
Batch 47/64 loss: -0.13330358266830444
Batch 48/64 loss: -0.1380266547203064
Batch 49/64 loss: -0.14676302671432495
Batch 50/64 loss: -0.13287347555160522
Batch 51/64 loss: -0.15277105569839478
Batch 52/64 loss: -0.14707982540130615
Batch 53/64 loss: -0.10821378231048584
Batch 54/64 loss: -0.1506648063659668
Batch 55/64 loss: -0.12843692302703857
Batch 56/64 loss: -0.1518051028251648
Batch 57/64 loss: -0.15296447277069092
Batch 58/64 loss: -0.14744222164154053
Batch 59/64 loss: -0.13492637872695923
Batch 60/64 loss: -0.14142465591430664
Batch 61/64 loss: -0.1194537878036499
Batch 62/64 loss: -0.12929260730743408
Batch 63/64 loss: -0.10067462921142578
Batch 64/64 loss: -0.14051520824432373
Epoch 299  Train loss: -0.13774446646372476  Val loss: -0.021989633127586127
Epoch 300
-------------------------------
Batch 1/64 loss: -0.16499781608581543
Batch 2/64 loss: -0.1359279751777649
Batch 3/64 loss: -0.1583770513534546
Batch 4/64 loss: -0.14082401990890503
Batch 5/64 loss: -0.13036537170410156
Batch 6/64 loss: -0.14936774969100952
Batch 7/64 loss: -0.13934719562530518
Batch 8/64 loss: -0.15445387363433838
Batch 9/64 loss: -0.11667203903198242
Batch 10/64 loss: -0.13094431161880493
Batch 11/64 loss: -0.14936327934265137
Batch 12/64 loss: -0.1516454815864563
Batch 13/64 loss: -0.14855533838272095
Batch 14/64 loss: -0.15405398607254028
Batch 15/64 loss: -0.14400434494018555
Batch 16/64 loss: -0.16251230239868164
Batch 17/64 loss: -0.1486145257949829
Batch 18/64 loss: -0.14007627964019775
Batch 19/64 loss: -0.1468106508255005
Batch 20/64 loss: -0.13978511095046997
Batch 21/64 loss: -0.123435378074646
Batch 22/64 loss: -0.14054453372955322
Batch 23/64 loss: -0.12683814764022827
Batch 24/64 loss: -0.12851190567016602
Batch 25/64 loss: -0.14568650722503662
Batch 26/64 loss: -0.14416354894638062
Batch 27/64 loss: -0.15965956449508667
Batch 28/64 loss: -0.14867162704467773
Batch 29/64 loss: -0.15080392360687256
Batch 30/64 loss: -0.1648356318473816
Batch 31/64 loss: -0.1412409543991089
Batch 32/64 loss: -0.11249864101409912
Batch 33/64 loss: -0.13873445987701416
Batch 34/64 loss: -0.14130401611328125
Batch 35/64 loss: -0.13023477792739868
Batch 36/64 loss: -0.1305704116821289
Batch 37/64 loss: -0.14420843124389648
Batch 38/64 loss: -0.14378362894058228
Batch 39/64 loss: -0.15692627429962158
Batch 40/64 loss: -0.13369840383529663
Batch 41/64 loss: -0.1381317377090454
Batch 42/64 loss: -0.16448140144348145
Batch 43/64 loss: -0.12266010046005249
Batch 44/64 loss: -0.15953350067138672
Batch 45/64 loss: -0.13135665655136108
Batch 46/64 loss: -0.14405882358551025
Batch 47/64 loss: -0.12629354000091553
Batch 48/64 loss: -0.154851496219635
Batch 49/64 loss: -0.13130080699920654
Batch 50/64 loss: -0.12002348899841309
Batch 51/64 loss: -0.14378345012664795
Batch 52/64 loss: -0.15015649795532227
Batch 53/64 loss: -0.12603092193603516
Batch 54/64 loss: -0.1315785050392151
Batch 55/64 loss: -0.15948033332824707
Batch 56/64 loss: -0.1476370096206665
Batch 57/64 loss: -0.1421525478363037
Batch 58/64 loss: -0.14730405807495117
Batch 59/64 loss: -0.15672874450683594
Batch 60/64 loss: -0.14247608184814453
Batch 61/64 loss: -0.13447660207748413
Batch 62/64 loss: -0.12672239542007446
Batch 63/64 loss: -0.15900862216949463
Batch 64/64 loss: -0.13575023412704468
Epoch 300  Train loss: -0.14235434462042415  Val loss: -0.021468736256930427
Epoch 301
-------------------------------
Batch 1/64 loss: -0.12732869386672974
Batch 2/64 loss: -0.1609630584716797
Batch 3/64 loss: -0.14112991094589233
Batch 4/64 loss: -0.15711605548858643
Batch 5/64 loss: -0.14274656772613525
Batch 6/64 loss: -0.12620604038238525
Batch 7/64 loss: -0.14859306812286377
Batch 8/64 loss: -0.15339863300323486
Batch 9/64 loss: -0.13298386335372925
Batch 10/64 loss: -0.14413201808929443
Batch 11/64 loss: -0.14747053384780884
Batch 12/64 loss: -0.15235590934753418
Batch 13/64 loss: -0.13840252161026
Batch 14/64 loss: -0.16817796230316162
Batch 15/64 loss: -0.15724796056747437
Batch 16/64 loss: -0.14425480365753174
Batch 17/64 loss: -0.1360892653465271
Batch 18/64 loss: -0.13683414459228516
Batch 19/64 loss: -0.16671574115753174
Batch 20/64 loss: -0.13601714372634888
Batch 21/64 loss: -0.14911150932312012
Batch 22/64 loss: -0.13519787788391113
Batch 23/64 loss: -0.15637093782424927
Batch 24/64 loss: -0.14581549167633057
Batch 25/64 loss: -0.14665472507476807
Batch 26/64 loss: -0.13508081436157227
Batch 27/64 loss: -0.14879626035690308
Batch 28/64 loss: -0.13648474216461182
Batch 29/64 loss: -0.1350530982017517
Batch 30/64 loss: -0.1255180835723877
Batch 31/64 loss: -0.14221948385238647
Batch 32/64 loss: -0.15352153778076172
Batch 33/64 loss: -0.15634530782699585
Batch 34/64 loss: -0.13789862394332886
Batch 35/64 loss: -0.12981408834457397
Batch 36/64 loss: -0.15471887588500977
Batch 37/64 loss: -0.13296902179718018
Batch 38/64 loss: -0.14470839500427246
Batch 39/64 loss: -0.1393081545829773
Batch 40/64 loss: -0.13146382570266724
Batch 41/64 loss: -0.14923065900802612
Batch 42/64 loss: -0.11056751012802124
Batch 43/64 loss: -0.14290237426757812
Batch 44/64 loss: -0.1542602777481079
Batch 45/64 loss: -0.13684004545211792
Batch 46/64 loss: -0.12365472316741943
Batch 47/64 loss: -0.14900434017181396
Batch 48/64 loss: -0.1270921230316162
Batch 49/64 loss: -0.15390056371688843
Batch 50/64 loss: -0.14092862606048584
Batch 51/64 loss: -0.14517700672149658
Batch 52/64 loss: -0.12151086330413818
Batch 53/64 loss: -0.13112807273864746
Batch 54/64 loss: -0.12115651369094849
Batch 55/64 loss: -0.16196787357330322
Batch 56/64 loss: -0.11836445331573486
Batch 57/64 loss: -0.1361478567123413
Batch 58/64 loss: -0.11516892910003662
Batch 59/64 loss: -0.1501416563987732
Batch 60/64 loss: -0.11132144927978516
Batch 61/64 loss: -0.14349114894866943
Batch 62/64 loss: -0.12811541557312012
Batch 63/64 loss: -0.13685983419418335
Batch 64/64 loss: -0.12163734436035156
Epoch 301  Train loss: -0.14047647177004347  Val loss: -0.022028764293775524
Epoch 302
-------------------------------
Batch 1/64 loss: -0.1376529335975647
Batch 2/64 loss: -0.15047478675842285
Batch 3/64 loss: -0.16116327047348022
Batch 4/64 loss: -0.15472525358200073
Batch 5/64 loss: -0.13427412509918213
Batch 6/64 loss: -0.14390027523040771
Batch 7/64 loss: -0.15566003322601318
Batch 8/64 loss: -0.13074755668640137
Batch 9/64 loss: -0.12019956111907959
Batch 10/64 loss: -0.12130892276763916
Batch 11/64 loss: -0.13428443670272827
Batch 12/64 loss: -0.14632707834243774
Batch 13/64 loss: -0.13588809967041016
Batch 14/64 loss: -0.1398208737373352
Batch 15/64 loss: -0.15170693397521973
Batch 16/64 loss: -0.15172863006591797
Batch 17/64 loss: -0.13867336511611938
Batch 18/64 loss: -0.1474393606185913
Batch 19/64 loss: -0.15209448337554932
Batch 20/64 loss: -0.14462125301361084
Batch 21/64 loss: -0.16636055707931519
Batch 22/64 loss: -0.12994718551635742
Batch 23/64 loss: -0.14959126710891724
Batch 24/64 loss: -0.1522495150566101
Batch 25/64 loss: -0.14857858419418335
Batch 26/64 loss: -0.09833520650863647
Batch 27/64 loss: -0.13599753379821777
Batch 28/64 loss: -0.123618483543396
Batch 29/64 loss: -0.15271449089050293
Batch 30/64 loss: -0.12850970029830933
Batch 31/64 loss: -0.0989026427268982
Batch 32/64 loss: -0.141393780708313
Batch 33/64 loss: -0.14262914657592773
Batch 34/64 loss: -0.12731707096099854
Batch 35/64 loss: -0.13878166675567627
Batch 36/64 loss: -0.14547812938690186
Batch 37/64 loss: -0.12031817436218262
Batch 38/64 loss: -0.13614124059677124
Batch 39/64 loss: -0.12739026546478271
Batch 40/64 loss: -0.12400972843170166
Batch 41/64 loss: -0.15011155605316162
Batch 42/64 loss: -0.12281495332717896
Batch 43/64 loss: -0.1431226134300232
Batch 44/64 loss: -0.13935744762420654
Batch 45/64 loss: -0.14350026845932007
Batch 46/64 loss: -0.13977956771850586
Batch 47/64 loss: -0.13381344079971313
Batch 48/64 loss: -0.15694177150726318
Batch 49/64 loss: -0.12883150577545166
Batch 50/64 loss: -0.13650071620941162
Batch 51/64 loss: -0.14597618579864502
Batch 52/64 loss: -0.13963305950164795
Batch 53/64 loss: -0.1394316554069519
Batch 54/64 loss: -0.1457676887512207
Batch 55/64 loss: -0.11985766887664795
Batch 56/64 loss: -0.12080472707748413
Batch 57/64 loss: -0.136935293674469
Batch 58/64 loss: -0.11603999137878418
Batch 59/64 loss: -0.12476235628128052
Batch 60/64 loss: -0.14323782920837402
Batch 61/64 loss: -0.15508979558944702
Batch 62/64 loss: -0.13673967123031616
Batch 63/64 loss: -0.14311730861663818
Batch 64/64 loss: -0.12766289710998535
Epoch 302  Train loss: -0.13802148781570733  Val loss: -0.023440300189342695
Epoch 303
-------------------------------
Batch 1/64 loss: -0.15486496686935425
Batch 2/64 loss: -0.14932727813720703
Batch 3/64 loss: -0.12100070714950562
Batch 4/64 loss: -0.13898539543151855
Batch 5/64 loss: -0.1470416784286499
Batch 6/64 loss: -0.11442649364471436
Batch 7/64 loss: -0.14440220594406128
Batch 8/64 loss: -0.15562140941619873
Batch 9/64 loss: -0.15116429328918457
Batch 10/64 loss: -0.1486034393310547
Batch 11/64 loss: -0.1364789605140686
Batch 12/64 loss: -0.15145081281661987
Batch 13/64 loss: -0.13325530290603638
Batch 14/64 loss: -0.15255582332611084
Batch 15/64 loss: -0.1565125584602356
Batch 16/64 loss: -0.14127486944198608
Batch 17/64 loss: -0.15284711122512817
Batch 18/64 loss: -0.13130909204483032
Batch 19/64 loss: -0.13685506582260132
Batch 20/64 loss: -0.1271282434463501
Batch 21/64 loss: -0.16489046812057495
Batch 22/64 loss: -0.13221865892410278
Batch 23/64 loss: -0.14418858289718628
Batch 24/64 loss: -0.14873504638671875
Batch 25/64 loss: -0.1642819046974182
Batch 26/64 loss: -0.146010160446167
Batch 27/64 loss: -0.14607328176498413
Batch 28/64 loss: -0.13533556461334229
Batch 29/64 loss: -0.12808650732040405
Batch 30/64 loss: -0.1253575086593628
Batch 31/64 loss: -0.10823917388916016
Batch 32/64 loss: -0.14935451745986938
Batch 33/64 loss: -0.13328039646148682
Batch 34/64 loss: -0.1322752833366394
Batch 35/64 loss: -0.1502358317375183
Batch 36/64 loss: -0.1568152904510498
Batch 37/64 loss: -0.1501803994178772
Batch 38/64 loss: -0.14011234045028687
Batch 39/64 loss: -0.13231277465820312
Batch 40/64 loss: -0.14269447326660156
Batch 41/64 loss: -0.12941378355026245
Batch 42/64 loss: -0.13488513231277466
Batch 43/64 loss: -0.1664181351661682
Batch 44/64 loss: -0.13693678379058838
Batch 45/64 loss: -0.14363497495651245
Batch 46/64 loss: -0.15873873233795166
Batch 47/64 loss: -0.11411058902740479
Batch 48/64 loss: -0.1410999894142151
Batch 49/64 loss: -0.14679795503616333
Batch 50/64 loss: -0.13507145643234253
Batch 51/64 loss: -0.146071195602417
Batch 52/64 loss: -0.15274757146835327
Batch 53/64 loss: -0.15803229808807373
Batch 54/64 loss: -0.12524038553237915
Batch 55/64 loss: -0.14703482389450073
Batch 56/64 loss: -0.1410495638847351
Batch 57/64 loss: -0.1406497359275818
Batch 58/64 loss: -0.14884769916534424
Batch 59/64 loss: -0.14272326231002808
Batch 60/64 loss: -0.15736114978790283
Batch 61/64 loss: -0.14401936531066895
Batch 62/64 loss: -0.1396813988685608
Batch 63/64 loss: -0.13739013671875
Batch 64/64 loss: -0.1366865634918213
Epoch 303  Train loss: -0.14221576335383396  Val loss: -0.020137698789642437
Epoch 304
-------------------------------
Batch 1/64 loss: -0.14178621768951416
Batch 2/64 loss: -0.13468945026397705
Batch 3/64 loss: -0.138655424118042
Batch 4/64 loss: -0.15938729047775269
Batch 5/64 loss: -0.13789725303649902
Batch 6/64 loss: -0.14105045795440674
Batch 7/64 loss: -0.13503813743591309
Batch 8/64 loss: -0.13440978527069092
Batch 9/64 loss: -0.16000735759735107
Batch 10/64 loss: -0.15354788303375244
Batch 11/64 loss: -0.14937210083007812
Batch 12/64 loss: -0.12795698642730713
Batch 13/64 loss: -0.13916420936584473
Batch 14/64 loss: -0.1480240821838379
Batch 15/64 loss: -0.14834582805633545
Batch 16/64 loss: -0.15412724018096924
Batch 17/64 loss: -0.13417720794677734
Batch 18/64 loss: -0.16297447681427002
Batch 19/64 loss: -0.14828181266784668
Batch 20/64 loss: -0.1548459529876709
Batch 21/64 loss: -0.13887768983840942
Batch 22/64 loss: -0.1604442000389099
Batch 23/64 loss: -0.11361908912658691
Batch 24/64 loss: -0.1297457218170166
Batch 25/64 loss: -0.14597010612487793
Batch 26/64 loss: -0.15462887287139893
Batch 27/64 loss: -0.1254737377166748
Batch 28/64 loss: -0.14159512519836426
Batch 29/64 loss: -0.13850486278533936
Batch 30/64 loss: -0.1259603500366211
Batch 31/64 loss: -0.15060138702392578
Batch 32/64 loss: -0.13640093803405762
Batch 33/64 loss: -0.1346135139465332
Batch 34/64 loss: -0.1569678783416748
Batch 35/64 loss: -0.16816043853759766
Batch 36/64 loss: -0.1504715085029602
Batch 37/64 loss: -0.13850349187850952
Batch 38/64 loss: -0.14703279733657837
Batch 39/64 loss: -0.13686060905456543
Batch 40/64 loss: -0.15423476696014404
Batch 41/64 loss: -0.13429009914398193
Batch 42/64 loss: -0.1557621955871582
Batch 43/64 loss: -0.1473669409751892
Batch 44/64 loss: -0.13794153928756714
Batch 45/64 loss: -0.12464529275894165
Batch 46/64 loss: -0.14532405138015747
Batch 47/64 loss: -0.1342916488647461
Batch 48/64 loss: -0.1419042944908142
Batch 49/64 loss: -0.1433585286140442
Batch 50/64 loss: -0.13933879137039185
Batch 51/64 loss: -0.14318275451660156
Batch 52/64 loss: -0.1504334807395935
Batch 53/64 loss: -0.16305243968963623
Batch 54/64 loss: -0.15938520431518555
Batch 55/64 loss: -0.14781004190444946
Batch 56/64 loss: -0.14682549238204956
Batch 57/64 loss: -0.12084305286407471
Batch 58/64 loss: -0.13086754083633423
Batch 59/64 loss: -0.13567304611206055
Batch 60/64 loss: -0.14049291610717773
Batch 61/64 loss: -0.14726364612579346
Batch 62/64 loss: -0.13143527507781982
Batch 63/64 loss: -0.14707040786743164
Batch 64/64 loss: -0.1320962905883789
Epoch 304  Train loss: -0.1430594060935226  Val loss: -0.02035244838478639
Epoch 305
-------------------------------
Batch 1/64 loss: -0.1606309413909912
Batch 2/64 loss: -0.15980392694473267
Batch 3/64 loss: -0.13964390754699707
Batch 4/64 loss: -0.1391352415084839
Batch 5/64 loss: -0.1410123109817505
Batch 6/64 loss: -0.13190823793411255
Batch 7/64 loss: -0.1330796480178833
Batch 8/64 loss: -0.13273990154266357
Batch 9/64 loss: -0.14215123653411865
Batch 10/64 loss: -0.12804174423217773
Batch 11/64 loss: -0.128953754901886
Batch 12/64 loss: -0.1411898136138916
Batch 13/64 loss: -0.1388024091720581
Batch 14/64 loss: -0.13874971866607666
Batch 15/64 loss: -0.16586989164352417
Batch 16/64 loss: -0.11713993549346924
Batch 17/64 loss: -0.14767473936080933
Batch 18/64 loss: -0.1218867301940918
Batch 19/64 loss: -0.12135165929794312
Batch 20/64 loss: -0.14816802740097046
Batch 21/64 loss: -0.14402008056640625
Batch 22/64 loss: -0.1336688995361328
Batch 23/64 loss: -0.14418715238571167
Batch 24/64 loss: -0.14425021409988403
Batch 25/64 loss: -0.14329397678375244
Batch 26/64 loss: -0.1444597840309143
Batch 27/64 loss: -0.15518206357955933
Batch 28/64 loss: -0.14367526769638062
Batch 29/64 loss: -0.15450608730316162
Batch 30/64 loss: -0.1417834758758545
Batch 31/64 loss: -0.17218506336212158
Batch 32/64 loss: -0.14564663171768188
Batch 33/64 loss: -0.15185445547103882
Batch 34/64 loss: -0.1252102255821228
Batch 35/64 loss: -0.1496042013168335
Batch 36/64 loss: -0.13967204093933105
Batch 37/64 loss: -0.14690810441970825
Batch 38/64 loss: -0.1464431881904602
Batch 39/64 loss: -0.1314908266067505
Batch 40/64 loss: -0.13556945323944092
Batch 41/64 loss: -0.1474631428718567
Batch 42/64 loss: -0.15462428331375122
Batch 43/64 loss: -0.13672888278961182
Batch 44/64 loss: -0.145305335521698
Batch 45/64 loss: -0.11415749788284302
Batch 46/64 loss: -0.15633374452590942
Batch 47/64 loss: -0.1531166434288025
Batch 48/64 loss: -0.14043676853179932
Batch 49/64 loss: -0.13873833417892456
Batch 50/64 loss: -0.14256489276885986
Batch 51/64 loss: -0.12879317998886108
Batch 52/64 loss: -0.13379007577896118
Batch 53/64 loss: -0.15457147359848022
Batch 54/64 loss: -0.14043742418289185
Batch 55/64 loss: -0.13405001163482666
Batch 56/64 loss: -0.14657723903656006
Batch 57/64 loss: -0.13681918382644653
Batch 58/64 loss: -0.13413554430007935
Batch 59/64 loss: -0.13092148303985596
Batch 60/64 loss: -0.13317656517028809
Batch 61/64 loss: -0.1245686411857605
Batch 62/64 loss: -0.13541924953460693
Batch 63/64 loss: -0.1326817274093628
Batch 64/64 loss: -0.1296018362045288
Epoch 305  Train loss: -0.14061423797233433  Val loss: -0.020088287768085387
Epoch 306
-------------------------------
Batch 1/64 loss: -0.13440930843353271
Batch 2/64 loss: -0.15548986196517944
Batch 3/64 loss: -0.15940696001052856
Batch 4/64 loss: -0.13985449075698853
Batch 5/64 loss: -0.14512735605239868
Batch 6/64 loss: -0.15616953372955322
Batch 7/64 loss: -0.13026928901672363
Batch 8/64 loss: -0.14860326051712036
Batch 9/64 loss: -0.1259474754333496
Batch 10/64 loss: -0.14033812284469604
Batch 11/64 loss: -0.1419893503189087
Batch 12/64 loss: -0.14180022478103638
Batch 13/64 loss: -0.1323060393333435
Batch 14/64 loss: -0.12387955188751221
Batch 15/64 loss: -0.13752025365829468
Batch 16/64 loss: -0.1192064881324768
Batch 17/64 loss: -0.1556795835494995
Batch 18/64 loss: -0.143108069896698
Batch 19/64 loss: -0.13473469018936157
Batch 20/64 loss: -0.10189402103424072
Batch 21/64 loss: -0.14222735166549683
Batch 22/64 loss: -0.11853832006454468
Batch 23/64 loss: -0.14980405569076538
Batch 24/64 loss: -0.13427871465682983
Batch 25/64 loss: -0.13357090950012207
Batch 26/64 loss: -0.15224599838256836
Batch 27/64 loss: -0.15993767976760864
Batch 28/64 loss: -0.15632277727127075
Batch 29/64 loss: -0.1418096423149109
Batch 30/64 loss: -0.13938087224960327
Batch 31/64 loss: -0.15271443128585815
Batch 32/64 loss: -0.13541537523269653
Batch 33/64 loss: -0.15713798999786377
Batch 34/64 loss: -0.1562647819519043
Batch 35/64 loss: -0.12601613998413086
Batch 36/64 loss: -0.12695962190628052
Batch 37/64 loss: -0.14525222778320312
Batch 38/64 loss: -0.1442350149154663
Batch 39/64 loss: -0.15507864952087402
Batch 40/64 loss: -0.14841431379318237
Batch 41/64 loss: -0.14427989721298218
Batch 42/64 loss: -0.12238454818725586
Batch 43/64 loss: -0.14769089221954346
Batch 44/64 loss: -0.1285562515258789
Batch 45/64 loss: -0.11280375719070435
Batch 46/64 loss: -0.12969428300857544
Batch 47/64 loss: -0.1314626932144165
Batch 48/64 loss: -0.13657605648040771
Batch 49/64 loss: -0.12754786014556885
Batch 50/64 loss: -0.15728724002838135
Batch 51/64 loss: -0.1539381742477417
Batch 52/64 loss: -0.15241146087646484
Batch 53/64 loss: -0.16606712341308594
Batch 54/64 loss: -0.1449025273323059
Batch 55/64 loss: -0.1288233995437622
Batch 56/64 loss: -0.14527106285095215
Batch 57/64 loss: -0.13818681240081787
Batch 58/64 loss: -0.15554118156433105
Batch 59/64 loss: -0.1366713047027588
Batch 60/64 loss: -0.1494436264038086
Batch 61/64 loss: -0.12895339727401733
Batch 62/64 loss: -0.1398829221725464
Batch 63/64 loss: -0.15444731712341309
Batch 64/64 loss: -0.13344228267669678
Epoch 306  Train loss: -0.14080383216633516  Val loss: -0.022666555499702794
Epoch 307
-------------------------------
Batch 1/64 loss: -0.14613944292068481
Batch 2/64 loss: -0.15977096557617188
Batch 3/64 loss: -0.17382889986038208
Batch 4/64 loss: -0.13357341289520264
Batch 5/64 loss: -0.14899426698684692
Batch 6/64 loss: -0.1459639072418213
Batch 7/64 loss: -0.15398067235946655
Batch 8/64 loss: -0.14484840631484985
Batch 9/64 loss: -0.1451359987258911
Batch 10/64 loss: -0.16458845138549805
Batch 11/64 loss: -0.15693706274032593
Batch 12/64 loss: -0.1275051236152649
Batch 13/64 loss: -0.14561307430267334
Batch 14/64 loss: -0.14393573999404907
Batch 15/64 loss: -0.1417161226272583
Batch 16/64 loss: -0.12636959552764893
Batch 17/64 loss: -0.14960455894470215
Batch 18/64 loss: -0.14388775825500488
Batch 19/64 loss: -0.12987029552459717
Batch 20/64 loss: -0.14875203371047974
Batch 21/64 loss: -0.14506655931472778
Batch 22/64 loss: -0.1453799605369568
Batch 23/64 loss: -0.15787887573242188
Batch 24/64 loss: -0.13959115743637085
Batch 25/64 loss: -0.14476048946380615
Batch 26/64 loss: -0.14935022592544556
Batch 27/64 loss: -0.1567937135696411
Batch 28/64 loss: -0.14817547798156738
Batch 29/64 loss: -0.12876170873641968
Batch 30/64 loss: -0.14958816766738892
Batch 31/64 loss: -0.13273054361343384
Batch 32/64 loss: -0.1290310025215149
Batch 33/64 loss: -0.14537322521209717
Batch 34/64 loss: -0.13764584064483643
Batch 35/64 loss: -0.14225083589553833
Batch 36/64 loss: -0.1422327160835266
Batch 37/64 loss: -0.13217204809188843
Batch 38/64 loss: -0.149519145488739
Batch 39/64 loss: -0.15172529220581055
Batch 40/64 loss: -0.13308364152908325
Batch 41/64 loss: -0.14212608337402344
Batch 42/64 loss: -0.14152252674102783
Batch 43/64 loss: -0.13720518350601196
Batch 44/64 loss: -0.1514883041381836
Batch 45/64 loss: -0.17142271995544434
Batch 46/64 loss: -0.1488751769065857
Batch 47/64 loss: -0.15576070547103882
Batch 48/64 loss: -0.1413303017616272
Batch 49/64 loss: -0.12659430503845215
Batch 50/64 loss: -0.14342963695526123
Batch 51/64 loss: -0.15144133567810059
Batch 52/64 loss: -0.1335681676864624
Batch 53/64 loss: -0.12411820888519287
Batch 54/64 loss: -0.14886057376861572
Batch 55/64 loss: -0.14579522609710693
Batch 56/64 loss: -0.1099858283996582
Batch 57/64 loss: -0.15537524223327637
Batch 58/64 loss: -0.14279353618621826
Batch 59/64 loss: -0.14797019958496094
Batch 60/64 loss: -0.14523380994796753
Batch 61/64 loss: -0.1214066743850708
Batch 62/64 loss: -0.15165024995803833
Batch 63/64 loss: -0.13605427742004395
Batch 64/64 loss: -0.1378135085105896
Epoch 307  Train loss: -0.14383521290386425  Val loss: -0.01966075921796032
Epoch 308
-------------------------------
Batch 1/64 loss: -0.1447216272354126
Batch 2/64 loss: -0.13721567392349243
Batch 3/64 loss: -0.14101260900497437
Batch 4/64 loss: -0.15879273414611816
Batch 5/64 loss: -0.16110515594482422
Batch 6/64 loss: -0.15591323375701904
Batch 7/64 loss: -0.136560320854187
Batch 8/64 loss: -0.15831196308135986
Batch 9/64 loss: -0.1554497480392456
Batch 10/64 loss: -0.14574384689331055
Batch 11/64 loss: -0.14774101972579956
Batch 12/64 loss: -0.1557539701461792
Batch 13/64 loss: -0.13882803916931152
Batch 14/64 loss: -0.13831287622451782
Batch 15/64 loss: -0.1429845690727234
Batch 16/64 loss: -0.14920812845230103
Batch 17/64 loss: -0.14580649137496948
Batch 18/64 loss: -0.13413774967193604
Batch 19/64 loss: -0.129039466381073
Batch 20/64 loss: -0.15101122856140137
Batch 21/64 loss: -0.13379663228988647
Batch 22/64 loss: -0.1649240255355835
Batch 23/64 loss: -0.14605754613876343
Batch 24/64 loss: -0.1662588119506836
Batch 25/64 loss: -0.13430213928222656
Batch 26/64 loss: -0.1671397089958191
Batch 27/64 loss: -0.1428748369216919
Batch 28/64 loss: -0.1605231761932373
Batch 29/64 loss: -0.12374228239059448
Batch 30/64 loss: -0.13209086656570435
Batch 31/64 loss: -0.11818116903305054
Batch 32/64 loss: -0.12696796655654907
Batch 33/64 loss: -0.1111302375793457
Batch 34/64 loss: -0.11068975925445557
Batch 35/64 loss: -0.14527428150177002
Batch 36/64 loss: -0.12024861574172974
Batch 37/64 loss: -0.12484180927276611
Batch 38/64 loss: -0.12383031845092773
Batch 39/64 loss: -0.13946425914764404
Batch 40/64 loss: -0.13779377937316895
Batch 41/64 loss: -0.13924384117126465
Batch 42/64 loss: -0.1274787187576294
Batch 43/64 loss: -0.15209341049194336
Batch 44/64 loss: -0.12798678874969482
Batch 45/64 loss: -0.15208852291107178
Batch 46/64 loss: -0.143424391746521
Batch 47/64 loss: -0.15687310695648193
Batch 48/64 loss: -0.14118731021881104
Batch 49/64 loss: -0.14830517768859863
Batch 50/64 loss: -0.14008313417434692
Batch 51/64 loss: -0.1497287154197693
Batch 52/64 loss: -0.09193801879882812
Batch 53/64 loss: -0.1593284010887146
Batch 54/64 loss: -0.16166412830352783
Batch 55/64 loss: -0.15773606300354004
Batch 56/64 loss: -0.15816837549209595
Batch 57/64 loss: -0.14173918962478638
Batch 58/64 loss: -0.17618954181671143
Batch 59/64 loss: -0.13416588306427002
Batch 60/64 loss: -0.13389182090759277
Batch 61/64 loss: -0.1301957368850708
Batch 62/64 loss: -0.13043248653411865
Batch 63/64 loss: -0.1462729573249817
Batch 64/64 loss: -0.14564085006713867
Epoch 308  Train loss: -0.1422310436473173  Val loss: -0.01868790961622782
Epoch 309
-------------------------------
Batch 1/64 loss: -0.15321779251098633
Batch 2/64 loss: -0.1716780662536621
Batch 3/64 loss: -0.15333861112594604
Batch 4/64 loss: -0.16801071166992188
Batch 5/64 loss: -0.15137052536010742
Batch 6/64 loss: -0.1423843502998352
Batch 7/64 loss: -0.11867058277130127
Batch 8/64 loss: -0.15121990442276
Batch 9/64 loss: -0.10508930683135986
Batch 10/64 loss: -0.1521403193473816
Batch 11/64 loss: -0.14117425680160522
Batch 12/64 loss: -0.1236754059791565
Batch 13/64 loss: -0.13756215572357178
Batch 14/64 loss: -0.15805178880691528
Batch 15/64 loss: -0.14259910583496094
Batch 16/64 loss: -0.13943302631378174
Batch 17/64 loss: -0.13389676809310913
Batch 18/64 loss: -0.14535915851593018
Batch 19/64 loss: -0.130418598651886
Batch 20/64 loss: -0.12128853797912598
Batch 21/64 loss: -0.14511746168136597
Batch 22/64 loss: -0.1503252387046814
Batch 23/64 loss: -0.12881654500961304
Batch 24/64 loss: -0.14732789993286133
Batch 25/64 loss: -0.1488550901412964
Batch 26/64 loss: -0.14747929573059082
Batch 27/64 loss: -0.12603461742401123
Batch 28/64 loss: -0.13468444347381592
Batch 29/64 loss: -0.1421809196472168
Batch 30/64 loss: -0.14458167552947998
Batch 31/64 loss: -0.13290780782699585
Batch 32/64 loss: -0.16085338592529297
Batch 33/64 loss: -0.15736150741577148
Batch 34/64 loss: -0.15910154581069946
Batch 35/64 loss: -0.13901162147521973
Batch 36/64 loss: -0.12693017721176147
Batch 37/64 loss: -0.13588011264801025
Batch 38/64 loss: -0.12433671951293945
Batch 39/64 loss: -0.14526772499084473
Batch 40/64 loss: -0.1476207971572876
Batch 41/64 loss: -0.14433854818344116
Batch 42/64 loss: -0.15242034196853638
Batch 43/64 loss: -0.13603687286376953
Batch 44/64 loss: -0.13515174388885498
Batch 45/64 loss: -0.14386272430419922
Batch 46/64 loss: -0.15001708269119263
Batch 47/64 loss: -0.16110175848007202
Batch 48/64 loss: -0.15455377101898193
Batch 49/64 loss: -0.12635678052902222
Batch 50/64 loss: -0.10796618461608887
Batch 51/64 loss: -0.11830788850784302
Batch 52/64 loss: -0.14707225561141968
Batch 53/64 loss: -0.12853866815567017
Batch 54/64 loss: -0.12728697061538696
Batch 55/64 loss: -0.13247430324554443
Batch 56/64 loss: -0.14701378345489502
Batch 57/64 loss: -0.13088512420654297
Batch 58/64 loss: -0.15195763111114502
Batch 59/64 loss: -0.15522676706314087
Batch 60/64 loss: -0.13353294134140015
Batch 61/64 loss: -0.13503944873809814
Batch 62/64 loss: -0.1672886610031128
Batch 63/64 loss: -0.15501445531845093
Batch 64/64 loss: -0.13211369514465332
Epoch 309  Train loss: -0.14158091825597427  Val loss: -0.024680080692383023
Epoch 310
-------------------------------
Batch 1/64 loss: -0.12665963172912598
Batch 2/64 loss: -0.151414692401886
Batch 3/64 loss: -0.15707015991210938
Batch 4/64 loss: -0.1327124834060669
Batch 5/64 loss: -0.14922291040420532
Batch 6/64 loss: -0.15517055988311768
Batch 7/64 loss: -0.1468287706375122
Batch 8/64 loss: -0.14979135990142822
Batch 9/64 loss: -0.14947658777236938
Batch 10/64 loss: -0.13889628648757935
Batch 11/64 loss: -0.16212743520736694
Batch 12/64 loss: -0.1445685625076294
Batch 13/64 loss: -0.1494065523147583
Batch 14/64 loss: -0.14639002084732056
Batch 15/64 loss: -0.15342116355895996
Batch 16/64 loss: -0.15016531944274902
Batch 17/64 loss: -0.14645344018936157
Batch 18/64 loss: -0.14769262075424194
Batch 19/64 loss: -0.13548946380615234
Batch 20/64 loss: -0.14460372924804688
Batch 21/64 loss: -0.1376757025718689
Batch 22/64 loss: -0.14901161193847656
Batch 23/64 loss: -0.14795589447021484
Batch 24/64 loss: -0.1269952654838562
Batch 25/64 loss: -0.1544342041015625
Batch 26/64 loss: -0.14014822244644165
Batch 27/64 loss: -0.1365734338760376
Batch 28/64 loss: -0.14053118228912354
Batch 29/64 loss: -0.13867700099945068
Batch 30/64 loss: -0.14725667238235474
Batch 31/64 loss: -0.1517924666404724
Batch 32/64 loss: -0.14179754257202148
Batch 33/64 loss: -0.1342604160308838
Batch 34/64 loss: -0.14574295282363892
Batch 35/64 loss: -0.15143495798110962
Batch 36/64 loss: -0.14609956741333008
Batch 37/64 loss: -0.1387636661529541
Batch 38/64 loss: -0.1367931365966797
Batch 39/64 loss: -0.1290881633758545
Batch 40/64 loss: -0.12200790643692017
Batch 41/64 loss: -0.13584017753601074
Batch 42/64 loss: -0.10687673091888428
Batch 43/64 loss: -0.14569348096847534
Batch 44/64 loss: -0.13666242361068726
Batch 45/64 loss: -0.15505939722061157
Batch 46/64 loss: -0.14949041604995728
Batch 47/64 loss: -0.12895101308822632
Batch 48/64 loss: -0.11965632438659668
Batch 49/64 loss: -0.1346369981765747
Batch 50/64 loss: -0.1432267427444458
Batch 51/64 loss: -0.14458388090133667
Batch 52/64 loss: -0.15221750736236572
Batch 53/64 loss: -0.14102041721343994
Batch 54/64 loss: -0.15743446350097656
Batch 55/64 loss: -0.15901964902877808
Batch 56/64 loss: -0.14382684230804443
Batch 57/64 loss: -0.13410574197769165
Batch 58/64 loss: -0.14578300714492798
Batch 59/64 loss: -0.14398008584976196
Batch 60/64 loss: -0.1177452802658081
Batch 61/64 loss: -0.1377781629562378
Batch 62/64 loss: -0.1212720274925232
Batch 63/64 loss: -0.15894687175750732
Batch 64/64 loss: -0.1451728343963623
Epoch 310  Train loss: -0.1423888468274883  Val loss: -0.02248410076619833
Epoch 311
-------------------------------
Batch 1/64 loss: -0.1474401354789734
Batch 2/64 loss: -0.16588157415390015
Batch 3/64 loss: -0.13891923427581787
Batch 4/64 loss: -0.1529238224029541
Batch 5/64 loss: -0.17046940326690674
Batch 6/64 loss: -0.11915761232376099
Batch 7/64 loss: -0.14883625507354736
Batch 8/64 loss: -0.15751749277114868
Batch 9/64 loss: -0.17253422737121582
Batch 10/64 loss: -0.12843841314315796
Batch 11/64 loss: -0.1319822072982788
Batch 12/64 loss: -0.14293283224105835
Batch 13/64 loss: -0.12791287899017334
Batch 14/64 loss: -0.12821847200393677
Batch 15/64 loss: -0.15351152420043945
Batch 16/64 loss: -0.15256446599960327
Batch 17/64 loss: -0.16180634498596191
Batch 18/64 loss: -0.14491695165634155
Batch 19/64 loss: -0.1304030418395996
Batch 20/64 loss: -0.15941005945205688
Batch 21/64 loss: -0.14949333667755127
Batch 22/64 loss: -0.14810872077941895
Batch 23/64 loss: -0.14479929208755493
Batch 24/64 loss: -0.14244389533996582
Batch 25/64 loss: -0.14947962760925293
Batch 26/64 loss: -0.1360207200050354
Batch 27/64 loss: -0.14632177352905273
Batch 28/64 loss: -0.16342312097549438
Batch 29/64 loss: -0.11896586418151855
Batch 30/64 loss: -0.14978671073913574
Batch 31/64 loss: -0.1411346197128296
Batch 32/64 loss: -0.12299138307571411
Batch 33/64 loss: -0.16458183526992798
Batch 34/64 loss: -0.13125306367874146
Batch 35/64 loss: -0.13642406463623047
Batch 36/64 loss: -0.1325150728225708
Batch 37/64 loss: -0.14597749710083008
Batch 38/64 loss: -0.15079760551452637
Batch 39/64 loss: -0.1540578007698059
Batch 40/64 loss: -0.13472795486450195
Batch 41/64 loss: -0.15273737907409668
Batch 42/64 loss: -0.1402307152748108
Batch 43/64 loss: -0.1477774977684021
Batch 44/64 loss: -0.12358856201171875
Batch 45/64 loss: -0.1389394998550415
Batch 46/64 loss: -0.14624333381652832
Batch 47/64 loss: -0.1193697452545166
Batch 48/64 loss: -0.14585310220718384
Batch 49/64 loss: -0.14214110374450684
Batch 50/64 loss: -0.14115428924560547
Batch 51/64 loss: -0.1460743546485901
Batch 52/64 loss: -0.15053844451904297
Batch 53/64 loss: -0.16775184869766235
Batch 54/64 loss: -0.16263318061828613
Batch 55/64 loss: -0.14790254831314087
Batch 56/64 loss: -0.1491500735282898
Batch 57/64 loss: -0.1440224051475525
Batch 58/64 loss: -0.14863890409469604
Batch 59/64 loss: -0.1574803590774536
Batch 60/64 loss: -0.14196985960006714
Batch 61/64 loss: -0.1471349000930786
Batch 62/64 loss: -0.13727635145187378
Batch 63/64 loss: -0.1463223099708557
Batch 64/64 loss: -0.13276046514511108
Epoch 311  Train loss: -0.144997365100711  Val loss: -0.022975729093518863
Epoch 312
-------------------------------
Batch 1/64 loss: -0.09996241331100464
Batch 2/64 loss: -0.159013569355011
Batch 3/64 loss: -0.17708683013916016
Batch 4/64 loss: -0.16496425867080688
Batch 5/64 loss: -0.14270848035812378
Batch 6/64 loss: -0.1601330041885376
Batch 7/64 loss: -0.16078704595565796
Batch 8/64 loss: -0.15764248371124268
Batch 9/64 loss: -0.14060759544372559
Batch 10/64 loss: -0.1583302617073059
Batch 11/64 loss: -0.17587310075759888
Batch 12/64 loss: -0.15193438529968262
Batch 13/64 loss: -0.14211559295654297
Batch 14/64 loss: -0.15130746364593506
Batch 15/64 loss: -0.15661472082138062
Batch 16/64 loss: -0.15057122707366943
Batch 17/64 loss: -0.1566314697265625
Batch 18/64 loss: -0.1410893201828003
Batch 19/64 loss: -0.1330069899559021
Batch 20/64 loss: -0.1689484715461731
Batch 21/64 loss: -0.1164778470993042
Batch 22/64 loss: -0.15843307971954346
Batch 23/64 loss: -0.14970481395721436
Batch 24/64 loss: -0.14045244455337524
Batch 25/64 loss: -0.14724946022033691
Batch 26/64 loss: -0.15561699867248535
Batch 27/64 loss: -0.13935422897338867
Batch 28/64 loss: -0.10897505283355713
Batch 29/64 loss: -0.12359058856964111
Batch 30/64 loss: -0.12777990102767944
Batch 31/64 loss: -0.1561795473098755
Batch 32/64 loss: -0.13982105255126953
Batch 33/64 loss: -0.1381508708000183
Batch 34/64 loss: -0.1614217758178711
Batch 35/64 loss: -0.1448366641998291
Batch 36/64 loss: -0.13131356239318848
Batch 37/64 loss: -0.13174712657928467
Batch 38/64 loss: -0.16063088178634644
Batch 39/64 loss: -0.13752341270446777
Batch 40/64 loss: -0.1292433738708496
Batch 41/64 loss: -0.12959623336791992
Batch 42/64 loss: -0.13090825080871582
Batch 43/64 loss: -0.12480050325393677
Batch 44/64 loss: -0.12537294626235962
Batch 45/64 loss: -0.12696105241775513
Batch 46/64 loss: -0.1361483335494995
Batch 47/64 loss: -0.14365386962890625
Batch 48/64 loss: -0.15708690881729126
Batch 49/64 loss: -0.14751458168029785
Batch 50/64 loss: -0.14717799425125122
Batch 51/64 loss: -0.1165347695350647
Batch 52/64 loss: -0.1440461277961731
Batch 53/64 loss: -0.13341975212097168
Batch 54/64 loss: -0.14214491844177246
Batch 55/64 loss: -0.12949848175048828
Batch 56/64 loss: -0.10596513748168945
Batch 57/64 loss: -0.12237548828125
Batch 58/64 loss: -0.16657590866088867
Batch 59/64 loss: -0.13174623250961304
Batch 60/64 loss: -0.14740508794784546
Batch 61/64 loss: -0.13657796382904053
Batch 62/64 loss: -0.15388435125350952
Batch 63/64 loss: -0.13756608963012695
Batch 64/64 loss: -0.1581423282623291
Epoch 312  Train loss: -0.1427984172222661  Val loss: -0.021053067392499996
Epoch 313
-------------------------------
Batch 1/64 loss: -0.12725979089736938
Batch 2/64 loss: -0.13118529319763184
Batch 3/64 loss: -0.1510968804359436
Batch 4/64 loss: -0.13385671377182007
Batch 5/64 loss: -0.16051971912384033
Batch 6/64 loss: -0.15850859880447388
Batch 7/64 loss: -0.13863444328308105
Batch 8/64 loss: -0.15064257383346558
Batch 9/64 loss: -0.14144450426101685
Batch 10/64 loss: -0.14811265468597412
Batch 11/64 loss: -0.15755754709243774
Batch 12/64 loss: -0.147538423538208
Batch 13/64 loss: -0.14773982763290405
Batch 14/64 loss: -0.14738810062408447
Batch 15/64 loss: -0.15079468488693237
Batch 16/64 loss: -0.16424506902694702
Batch 17/64 loss: -0.15612024068832397
Batch 18/64 loss: -0.1659443974494934
Batch 19/64 loss: -0.13933253288269043
Batch 20/64 loss: -0.1509729027748108
Batch 21/64 loss: -0.1477002501487732
Batch 22/64 loss: -0.1465432047843933
Batch 23/64 loss: -0.1368311643600464
Batch 24/64 loss: -0.14147865772247314
Batch 25/64 loss: -0.14976465702056885
Batch 26/64 loss: -0.14310461282730103
Batch 27/64 loss: -0.1685768961906433
Batch 28/64 loss: -0.16123437881469727
Batch 29/64 loss: -0.14485734701156616
Batch 30/64 loss: -0.1560027003288269
Batch 31/64 loss: -0.13856768608093262
Batch 32/64 loss: -0.1388300061225891
Batch 33/64 loss: -0.15067434310913086
Batch 34/64 loss: -0.16465651988983154
Batch 35/64 loss: -0.10883063077926636
Batch 36/64 loss: -0.1560748815536499
Batch 37/64 loss: -0.14615589380264282
Batch 38/64 loss: -0.14786124229431152
Batch 39/64 loss: -0.1349726915359497
Batch 40/64 loss: -0.14805763959884644
Batch 41/64 loss: -0.12004053592681885
Batch 42/64 loss: -0.14927935600280762
Batch 43/64 loss: -0.15288466215133667
Batch 44/64 loss: -0.14067012071609497
Batch 45/64 loss: -0.10030549764633179
Batch 46/64 loss: -0.12867993116378784
Batch 47/64 loss: -0.15172725915908813
Batch 48/64 loss: -0.1581745147705078
Batch 49/64 loss: -0.1288711428642273
Batch 50/64 loss: -0.15377557277679443
Batch 51/64 loss: -0.13711804151535034
Batch 52/64 loss: -0.1460588574409485
Batch 53/64 loss: -0.1468263864517212
Batch 54/64 loss: -0.13442403078079224
Batch 55/64 loss: -0.13499313592910767
Batch 56/64 loss: -0.13757795095443726
Batch 57/64 loss: -0.13098257780075073
Batch 58/64 loss: -0.11547434329986572
Batch 59/64 loss: -0.11413729190826416
Batch 60/64 loss: -0.13454782962799072
Batch 61/64 loss: -0.15083587169647217
Batch 62/64 loss: -0.1602245569229126
Batch 63/64 loss: -0.12274903059005737
Batch 64/64 loss: -0.14909285306930542
Epoch 313  Train loss: -0.14371526965907977  Val loss: -0.02151382706828953
Epoch 314
-------------------------------
Batch 1/64 loss: -0.16149693727493286
Batch 2/64 loss: -0.15113067626953125
Batch 3/64 loss: -0.14077794551849365
Batch 4/64 loss: -0.17243504524230957
Batch 5/64 loss: -0.13043534755706787
Batch 6/64 loss: -0.15455448627471924
Batch 7/64 loss: -0.1362147331237793
Batch 8/64 loss: -0.1393168568611145
Batch 9/64 loss: -0.1480301022529602
Batch 10/64 loss: -0.13146066665649414
Batch 11/64 loss: -0.15146976709365845
Batch 12/64 loss: -0.13548588752746582
Batch 13/64 loss: -0.12477219104766846
Batch 14/64 loss: -0.14215737581253052
Batch 15/64 loss: -0.15411317348480225
Batch 16/64 loss: -0.13207197189331055
Batch 17/64 loss: -0.15295875072479248
Batch 18/64 loss: -0.15916645526885986
Batch 19/64 loss: -0.151322603225708
Batch 20/64 loss: -0.15044289827346802
Batch 21/64 loss: -0.15205460786819458
Batch 22/64 loss: -0.15146702527999878
Batch 23/64 loss: -0.1562725305557251
Batch 24/64 loss: -0.14342808723449707
Batch 25/64 loss: -0.1510988473892212
Batch 26/64 loss: -0.15411078929901123
Batch 27/64 loss: -0.16386854648590088
Batch 28/64 loss: -0.14989638328552246
Batch 29/64 loss: -0.14668560028076172
Batch 30/64 loss: -0.15625375509262085
Batch 31/64 loss: -0.15359264612197876
Batch 32/64 loss: -0.15885764360427856
Batch 33/64 loss: -0.14789938926696777
Batch 34/64 loss: -0.15643101930618286
Batch 35/64 loss: -0.15210646390914917
Batch 36/64 loss: -0.14001482725143433
Batch 37/64 loss: -0.15556681156158447
Batch 38/64 loss: -0.17158514261245728
Batch 39/64 loss: -0.14724749326705933
Batch 40/64 loss: -0.1614856719970703
Batch 41/64 loss: -0.1265239715576172
Batch 42/64 loss: -0.12937116622924805
Batch 43/64 loss: -0.11745023727416992
Batch 44/64 loss: -0.16978275775909424
Batch 45/64 loss: -0.131930410861969
Batch 46/64 loss: -0.1554214358329773
Batch 47/64 loss: -0.14810168743133545
Batch 48/64 loss: -0.14734292030334473
Batch 49/64 loss: -0.14207226037979126
Batch 50/64 loss: -0.14278149604797363
Batch 51/64 loss: -0.15241611003875732
Batch 52/64 loss: -0.14454269409179688
Batch 53/64 loss: -0.11823868751525879
Batch 54/64 loss: -0.15324097871780396
Batch 55/64 loss: -0.13449573516845703
Batch 56/64 loss: -0.14621734619140625
Batch 57/64 loss: -0.16085267066955566
Batch 58/64 loss: -0.1596640944480896
Batch 59/64 loss: -0.14181184768676758
Batch 60/64 loss: -0.1396227478981018
Batch 61/64 loss: -0.13699257373809814
Batch 62/64 loss: -0.1517949104309082
Batch 63/64 loss: -0.14766889810562134
Batch 64/64 loss: -0.11948084831237793
Epoch 314  Train loss: -0.14710094788495232  Val loss: -0.02093710481506033
Epoch 315
-------------------------------
Batch 1/64 loss: -0.1600571870803833
Batch 2/64 loss: -0.16715407371520996
Batch 3/64 loss: -0.159759521484375
Batch 4/64 loss: -0.1344704031944275
Batch 5/64 loss: -0.1517801284790039
Batch 6/64 loss: -0.14001518487930298
Batch 7/64 loss: -0.1492922306060791
Batch 8/64 loss: -0.15247833728790283
Batch 9/64 loss: -0.1418377161026001
Batch 10/64 loss: -0.1605125069618225
Batch 11/64 loss: -0.12610357999801636
Batch 12/64 loss: -0.13348454236984253
Batch 13/64 loss: -0.14757579565048218
Batch 14/64 loss: -0.10067886114120483
Batch 15/64 loss: -0.11536484956741333
Batch 16/64 loss: -0.1688152551651001
Batch 17/64 loss: -0.15874940156936646
Batch 18/64 loss: -0.15762734413146973
Batch 19/64 loss: -0.15458041429519653
Batch 20/64 loss: -0.15916860103607178
Batch 21/64 loss: -0.15109795331954956
Batch 22/64 loss: -0.1346193552017212
Batch 23/64 loss: -0.14982610940933228
Batch 24/64 loss: -0.15834158658981323
Batch 25/64 loss: -0.1248786449432373
Batch 26/64 loss: -0.16116118431091309
Batch 27/64 loss: -0.12403947114944458
Batch 28/64 loss: -0.15744006633758545
Batch 29/64 loss: -0.15933501720428467
Batch 30/64 loss: -0.14487630128860474
Batch 31/64 loss: -0.13189959526062012
Batch 32/64 loss: -0.16009128093719482
Batch 33/64 loss: -0.1705915927886963
Batch 34/64 loss: -0.14906549453735352
Batch 35/64 loss: -0.13784104585647583
Batch 36/64 loss: -0.16887754201889038
Batch 37/64 loss: -0.14256352186203003
Batch 38/64 loss: -0.15947777032852173
Batch 39/64 loss: -0.15350985527038574
Batch 40/64 loss: -0.16407650709152222
Batch 41/64 loss: -0.16510778665542603
Batch 42/64 loss: -0.1344178318977356
Batch 43/64 loss: -0.14290457963943481
Batch 44/64 loss: -0.15252304077148438
Batch 45/64 loss: -0.16092729568481445
Batch 46/64 loss: -0.14480555057525635
Batch 47/64 loss: -0.13718748092651367
Batch 48/64 loss: -0.135473370552063
Batch 49/64 loss: -0.14746630191802979
Batch 50/64 loss: -0.18108230829238892
Batch 51/64 loss: -0.15573447942733765
Batch 52/64 loss: -0.14012157917022705
Batch 53/64 loss: -0.15768229961395264
Batch 54/64 loss: -0.14352667331695557
Batch 55/64 loss: -0.15013384819030762
Batch 56/64 loss: -0.13283467292785645
Batch 57/64 loss: -0.1403036117553711
Batch 58/64 loss: -0.146317720413208
Batch 59/64 loss: -0.14162027835845947
Batch 60/64 loss: -0.12321019172668457
Batch 61/64 loss: -0.14213436841964722
Batch 62/64 loss: -0.14808082580566406
Batch 63/64 loss: -0.15025413036346436
Batch 64/64 loss: -0.13514238595962524
Epoch 315  Train loss: -0.14773839758891685  Val loss: -0.02114352491713062
Epoch 316
-------------------------------
Batch 1/64 loss: -0.1534615159034729
Batch 2/64 loss: -0.1494954228401184
Batch 3/64 loss: -0.14510464668273926
Batch 4/64 loss: -0.16315221786499023
Batch 5/64 loss: -0.15696150064468384
Batch 6/64 loss: -0.12379229068756104
Batch 7/64 loss: -0.14471787214279175
Batch 8/64 loss: -0.1275891661643982
Batch 9/64 loss: -0.1226148009300232
Batch 10/64 loss: -0.14680582284927368
Batch 11/64 loss: -0.13942259550094604
Batch 12/64 loss: -0.15523356199264526
Batch 13/64 loss: -0.13771969079971313
Batch 14/64 loss: -0.1489383578300476
Batch 15/64 loss: -0.12894999980926514
Batch 16/64 loss: -0.14471852779388428
Batch 17/64 loss: -0.1595558524131775
Batch 18/64 loss: -0.15744727849960327
Batch 19/64 loss: -0.13123631477355957
Batch 20/64 loss: -0.15757113695144653
Batch 21/64 loss: -0.13840991258621216
Batch 22/64 loss: -0.16358256340026855
Batch 23/64 loss: -0.14687931537628174
Batch 24/64 loss: -0.1434706449508667
Batch 25/64 loss: -0.16206002235412598
Batch 26/64 loss: -0.15927433967590332
Batch 27/64 loss: -0.16274577379226685
Batch 28/64 loss: -0.15311414003372192
Batch 29/64 loss: -0.15687191486358643
Batch 30/64 loss: -0.13103049993515015
Batch 31/64 loss: -0.1627684235572815
Batch 32/64 loss: -0.14213627576828003
Batch 33/64 loss: -0.17375552654266357
Batch 34/64 loss: -0.14622312784194946
Batch 35/64 loss: -0.1662697196006775
Batch 36/64 loss: -0.1639174222946167
Batch 37/64 loss: -0.13432979583740234
Batch 38/64 loss: -0.15049052238464355
Batch 39/64 loss: -0.14335519075393677
Batch 40/64 loss: -0.14134269952774048
Batch 41/64 loss: -0.1455702781677246
Batch 42/64 loss: -0.14055967330932617
Batch 43/64 loss: -0.15930426120758057
Batch 44/64 loss: -0.14868533611297607
Batch 45/64 loss: -0.1529090404510498
Batch 46/64 loss: -0.1249040961265564
Batch 47/64 loss: -0.15360015630722046
Batch 48/64 loss: -0.12134820222854614
Batch 49/64 loss: -0.16064703464508057
Batch 50/64 loss: -0.14381682872772217
Batch 51/64 loss: -0.17154288291931152
Batch 52/64 loss: -0.16415321826934814
Batch 53/64 loss: -0.15749090909957886
Batch 54/64 loss: -0.15162694454193115
Batch 55/64 loss: -0.15015459060668945
Batch 56/64 loss: -0.13330036401748657
Batch 57/64 loss: -0.1644575595855713
Batch 58/64 loss: -0.1517566442489624
Batch 59/64 loss: -0.15418487787246704
Batch 60/64 loss: -0.15066063404083252
Batch 61/64 loss: -0.13592493534088135
Batch 62/64 loss: -0.148689866065979
Batch 63/64 loss: -0.15779846906661987
Batch 64/64 loss: -0.13824206590652466
Epoch 316  Train loss: -0.14875740794574513  Val loss: -0.01956912967347607
Epoch 317
-------------------------------
Batch 1/64 loss: -0.16076451539993286
Batch 2/64 loss: -0.1530301570892334
Batch 3/64 loss: -0.13371777534484863
Batch 4/64 loss: -0.1594277024269104
Batch 5/64 loss: -0.17259806394577026
Batch 6/64 loss: -0.15277600288391113
Batch 7/64 loss: -0.15552592277526855
Batch 8/64 loss: -0.15215259790420532
Batch 9/64 loss: -0.16434961557388306
Batch 10/64 loss: -0.15900111198425293
Batch 11/64 loss: -0.14244979619979858
Batch 12/64 loss: -0.1461855173110962
Batch 13/64 loss: -0.16739511489868164
Batch 14/64 loss: -0.14071768522262573
Batch 15/64 loss: -0.15270525217056274
Batch 16/64 loss: -0.13962149620056152
Batch 17/64 loss: -0.13761615753173828
Batch 18/64 loss: -0.14360660314559937
Batch 19/64 loss: -0.14657872915267944
Batch 20/64 loss: -0.1572192907333374
Batch 21/64 loss: -0.11239922046661377
Batch 22/64 loss: -0.1527245044708252
Batch 23/64 loss: -0.1396607756614685
Batch 24/64 loss: -0.16033685207366943
Batch 25/64 loss: -0.15469610691070557
Batch 26/64 loss: -0.14825105667114258
Batch 27/64 loss: -0.15362954139709473
Batch 28/64 loss: -0.1652892827987671
Batch 29/64 loss: -0.14981764554977417
Batch 30/64 loss: -0.1554298996925354
Batch 31/64 loss: -0.1573338508605957
Batch 32/64 loss: -0.16109955310821533
Batch 33/64 loss: -0.15726864337921143
Batch 34/64 loss: -0.1411353349685669
Batch 35/64 loss: -0.15365207195281982
Batch 36/64 loss: -0.16018718481063843
Batch 37/64 loss: -0.1333625316619873
Batch 38/64 loss: -0.14834177494049072
Batch 39/64 loss: -0.16985678672790527
Batch 40/64 loss: -0.16388678550720215
Batch 41/64 loss: -0.14621222019195557
Batch 42/64 loss: -0.12627720832824707
Batch 43/64 loss: -0.13323765993118286
Batch 44/64 loss: -0.14311718940734863
Batch 45/64 loss: -0.13278400897979736
Batch 46/64 loss: -0.13440465927124023
Batch 47/64 loss: -0.15305322408676147
Batch 48/64 loss: -0.15930306911468506
Batch 49/64 loss: -0.11895036697387695
Batch 50/64 loss: -0.157687246799469
Batch 51/64 loss: -0.1436823606491089
Batch 52/64 loss: -0.15316998958587646
Batch 53/64 loss: -0.14091801643371582
Batch 54/64 loss: -0.15222042798995972
Batch 55/64 loss: -0.14619624614715576
Batch 56/64 loss: -0.15052789449691772
Batch 57/64 loss: -0.1598230004310608
Batch 58/64 loss: -0.1413150429725647
Batch 59/64 loss: -0.1506251096725464
Batch 60/64 loss: -0.1637290120124817
Batch 61/64 loss: -0.1496228575706482
Batch 62/64 loss: -0.13700878620147705
Batch 63/64 loss: -0.1464747190475464
Batch 64/64 loss: -0.1350986361503601
Epoch 317  Train loss: -0.14929353419472188  Val loss: -0.021362920602162678
Epoch 318
-------------------------------
Batch 1/64 loss: -0.12177157402038574
Batch 2/64 loss: -0.1511242389678955
Batch 3/64 loss: -0.15534448623657227
Batch 4/64 loss: -0.1499462127685547
Batch 5/64 loss: -0.157670795917511
Batch 6/64 loss: -0.14616990089416504
Batch 7/64 loss: -0.16655617952346802
Batch 8/64 loss: -0.12342989444732666
Batch 9/64 loss: -0.1447380781173706
Batch 10/64 loss: -0.14196205139160156
Batch 11/64 loss: -0.13837027549743652
Batch 12/64 loss: -0.13563239574432373
Batch 13/64 loss: -0.16324961185455322
Batch 14/64 loss: -0.1616039276123047
Batch 15/64 loss: -0.15372055768966675
Batch 16/64 loss: -0.15228205919265747
Batch 17/64 loss: -0.15412771701812744
Batch 18/64 loss: -0.15087580680847168
Batch 19/64 loss: -0.15390777587890625
Batch 20/64 loss: -0.1153421401977539
Batch 21/64 loss: -0.15488308668136597
Batch 22/64 loss: -0.15440893173217773
Batch 23/64 loss: -0.1362791657447815
Batch 24/64 loss: -0.1252197027206421
Batch 25/64 loss: -0.150549054145813
Batch 26/64 loss: -0.1468505859375
Batch 27/64 loss: -0.1577957272529602
Batch 28/64 loss: -0.1508055329322815
Batch 29/64 loss: -0.15622997283935547
Batch 30/64 loss: -0.13335978984832764
Batch 31/64 loss: -0.1653308868408203
Batch 32/64 loss: -0.15756934881210327
Batch 33/64 loss: -0.16392207145690918
Batch 34/64 loss: -0.15520209074020386
Batch 35/64 loss: -0.1538383960723877
Batch 36/64 loss: -0.13917642831802368
Batch 37/64 loss: -0.13586288690567017
Batch 38/64 loss: -0.1534900665283203
Batch 39/64 loss: -0.14855694770812988
Batch 40/64 loss: -0.15931683778762817
Batch 41/64 loss: -0.17648530006408691
Batch 42/64 loss: -0.15665680170059204
Batch 43/64 loss: -0.13614368438720703
Batch 44/64 loss: -0.15312910079956055
Batch 45/64 loss: -0.16048693656921387
Batch 46/64 loss: -0.17115157842636108
Batch 47/64 loss: -0.1560726761817932
Batch 48/64 loss: -0.1603173017501831
Batch 49/64 loss: -0.16428899765014648
Batch 50/64 loss: -0.137215256690979
Batch 51/64 loss: -0.15363895893096924
Batch 52/64 loss: -0.15109789371490479
Batch 53/64 loss: -0.1616990566253662
Batch 54/64 loss: -0.1471717357635498
Batch 55/64 loss: -0.14640718698501587
Batch 56/64 loss: -0.11970794200897217
Batch 57/64 loss: -0.13862645626068115
Batch 58/64 loss: -0.13370436429977417
Batch 59/64 loss: -0.1499180793762207
Batch 60/64 loss: -0.14136183261871338
Batch 61/64 loss: -0.1473907232284546
Batch 62/64 loss: -0.14608454704284668
Batch 63/64 loss: -0.12296897172927856
Batch 64/64 loss: -0.11932897567749023
Epoch 318  Train loss: -0.1483560047897638  Val loss: -0.02183498775016811
Epoch 319
-------------------------------
Batch 1/64 loss: -0.13945704698562622
Batch 2/64 loss: -0.15620261430740356
Batch 3/64 loss: -0.14764469861984253
Batch 4/64 loss: -0.16482043266296387
Batch 5/64 loss: -0.12582427263259888
Batch 6/64 loss: -0.11461770534515381
Batch 7/64 loss: -0.14304614067077637
Batch 8/64 loss: -0.12975019216537476
Batch 9/64 loss: -0.1564151644706726
Batch 10/64 loss: -0.15905725955963135
Batch 11/64 loss: -0.1536548137664795
Batch 12/64 loss: -0.16810119152069092
Batch 13/64 loss: -0.16759967803955078
Batch 14/64 loss: -0.14978426694869995
Batch 15/64 loss: -0.1619889736175537
Batch 16/64 loss: -0.15068602561950684
Batch 17/64 loss: -0.15454262495040894
Batch 18/64 loss: -0.14672935009002686
Batch 19/64 loss: -0.14975768327713013
Batch 20/64 loss: -0.1452559232711792
Batch 21/64 loss: -0.1450366973876953
Batch 22/64 loss: -0.15240710973739624
Batch 23/64 loss: -0.13979768753051758
Batch 24/64 loss: -0.14487552642822266
Batch 25/64 loss: -0.15600252151489258
Batch 26/64 loss: -0.14930307865142822
Batch 27/64 loss: -0.14886772632598877
Batch 28/64 loss: -0.16356217861175537
Batch 29/64 loss: -0.1617199182510376
Batch 30/64 loss: -0.1436825394630432
Batch 31/64 loss: -0.145843505859375
Batch 32/64 loss: -0.1548910140991211
Batch 33/64 loss: -0.15256726741790771
Batch 34/64 loss: -0.14344477653503418
Batch 35/64 loss: -0.11875438690185547
Batch 36/64 loss: -0.14558374881744385
Batch 37/64 loss: -0.15704917907714844
Batch 38/64 loss: -0.13988620042800903
Batch 39/64 loss: -0.15856653451919556
Batch 40/64 loss: -0.15709316730499268
Batch 41/64 loss: -0.15956073999404907
Batch 42/64 loss: -0.13579601049423218
Batch 43/64 loss: -0.14169061183929443
Batch 44/64 loss: -0.14633524417877197
Batch 45/64 loss: -0.1445368528366089
Batch 46/64 loss: -0.16359615325927734
Batch 47/64 loss: -0.14158594608306885
Batch 48/64 loss: -0.12705832719802856
Batch 49/64 loss: -0.15438640117645264
Batch 50/64 loss: -0.14808368682861328
Batch 51/64 loss: -0.14200317859649658
Batch 52/64 loss: -0.15913665294647217
Batch 53/64 loss: -0.138333261013031
Batch 54/64 loss: -0.12627917528152466
Batch 55/64 loss: -0.15768718719482422
Batch 56/64 loss: -0.14428472518920898
Batch 57/64 loss: -0.1437627673149109
Batch 58/64 loss: -0.1574663519859314
Batch 59/64 loss: -0.1617187261581421
Batch 60/64 loss: -0.15284329652786255
Batch 61/64 loss: -0.15545052289962769
Batch 62/64 loss: -0.15388357639312744
Batch 63/64 loss: -0.16538530588150024
Batch 64/64 loss: -0.13470512628555298
Epoch 319  Train loss: -0.1487963038332322  Val loss: -0.01856284084188979
Epoch 320
-------------------------------
Batch 1/64 loss: -0.160780131816864
Batch 2/64 loss: -0.154127836227417
Batch 3/64 loss: -0.1189773678779602
Batch 4/64 loss: -0.18065625429153442
Batch 5/64 loss: -0.1500633955001831
Batch 6/64 loss: -0.12765276432037354
Batch 7/64 loss: -0.1731688380241394
Batch 8/64 loss: -0.13824588060379028
Batch 9/64 loss: -0.13093090057373047
Batch 10/64 loss: -0.15709054470062256
Batch 11/64 loss: -0.16734254360198975
Batch 12/64 loss: -0.15795689821243286
Batch 13/64 loss: -0.13988983631134033
Batch 14/64 loss: -0.14460062980651855
Batch 15/64 loss: -0.159229576587677
Batch 16/64 loss: -0.1270662546157837
Batch 17/64 loss: -0.1319485902786255
Batch 18/64 loss: -0.14941895008087158
Batch 19/64 loss: -0.1403864622116089
Batch 20/64 loss: -0.14083212614059448
Batch 21/64 loss: -0.13293910026550293
Batch 22/64 loss: -0.14267253875732422
Batch 23/64 loss: -0.14295673370361328
Batch 24/64 loss: -0.1497364044189453
Batch 25/64 loss: -0.15243595838546753
Batch 26/64 loss: -0.14973831176757812
Batch 27/64 loss: -0.11297154426574707
Batch 28/64 loss: -0.15956872701644897
Batch 29/64 loss: -0.1341552734375
Batch 30/64 loss: -0.14428842067718506
Batch 31/64 loss: -0.14163100719451904
Batch 32/64 loss: -0.1459430456161499
Batch 33/64 loss: -0.14365482330322266
Batch 34/64 loss: -0.14330631494522095
Batch 35/64 loss: -0.14332884550094604
Batch 36/64 loss: -0.15535295009613037
Batch 37/64 loss: -0.13952839374542236
Batch 38/64 loss: -0.15566855669021606
Batch 39/64 loss: -0.15656298398971558
Batch 40/64 loss: -0.15175437927246094
Batch 41/64 loss: -0.14107322692871094
Batch 42/64 loss: -0.1301274299621582
Batch 43/64 loss: -0.14006376266479492
Batch 44/64 loss: -0.13365989923477173
Batch 45/64 loss: -0.15079516172409058
Batch 46/64 loss: -0.1591389775276184
Batch 47/64 loss: -0.1376708745956421
Batch 48/64 loss: -0.11947518587112427
Batch 49/64 loss: -0.1636437177658081
Batch 50/64 loss: -0.14654028415679932
Batch 51/64 loss: -0.1467546820640564
Batch 52/64 loss: -0.14595341682434082
Batch 53/64 loss: -0.14296793937683105
Batch 54/64 loss: -0.15619361400604248
Batch 55/64 loss: -0.15895026922225952
Batch 56/64 loss: -0.15675437450408936
Batch 57/64 loss: -0.1617039442062378
Batch 58/64 loss: -0.15898269414901733
Batch 59/64 loss: -0.13949841260910034
Batch 60/64 loss: -0.14532071352005005
Batch 61/64 loss: -0.1600775122642517
Batch 62/64 loss: -0.15911996364593506
Batch 63/64 loss: -0.15054255723953247
Batch 64/64 loss: -0.12712013721466064
Epoch 320  Train loss: -0.14664954998913934  Val loss: -0.020103463602229903
Epoch 321
-------------------------------
Batch 1/64 loss: -0.14522665739059448
Batch 2/64 loss: -0.1624763011932373
Batch 3/64 loss: -0.16632550954818726
Batch 4/64 loss: -0.1664620041847229
Batch 5/64 loss: -0.13431763648986816
Batch 6/64 loss: -0.1564745306968689
Batch 7/64 loss: -0.1375516653060913
Batch 8/64 loss: -0.1191449761390686
Batch 9/64 loss: -0.11747318506240845
Batch 10/64 loss: -0.15428400039672852
Batch 11/64 loss: -0.15396523475646973
Batch 12/64 loss: -0.13324278593063354
Batch 13/64 loss: -0.14905160665512085
Batch 14/64 loss: -0.13481348752975464
Batch 15/64 loss: -0.12323629856109619
Batch 16/64 loss: -0.14344966411590576
Batch 17/64 loss: -0.15383172035217285
Batch 18/64 loss: -0.12948548793792725
Batch 19/64 loss: -0.1718347668647766
Batch 20/64 loss: -0.1488000750541687
Batch 21/64 loss: -0.14002251625061035
Batch 22/64 loss: -0.15309268236160278
Batch 23/64 loss: -0.13480883836746216
Batch 24/64 loss: -0.1503635048866272
Batch 25/64 loss: -0.16307145357131958
Batch 26/64 loss: -0.17821723222732544
Batch 27/64 loss: -0.15226459503173828
Batch 28/64 loss: -0.16860347986221313
Batch 29/64 loss: -0.11967796087265015
Batch 30/64 loss: -0.17769217491149902
Batch 31/64 loss: -0.14489394426345825
Batch 32/64 loss: -0.16947174072265625
Batch 33/64 loss: -0.14529842138290405
Batch 34/64 loss: -0.15569764375686646
Batch 35/64 loss: -0.1564902663230896
Batch 36/64 loss: -0.15280532836914062
Batch 37/64 loss: -0.14163225889205933
Batch 38/64 loss: -0.13811272382736206
Batch 39/64 loss: -0.1414860486984253
Batch 40/64 loss: -0.12139159440994263
Batch 41/64 loss: -0.17120301723480225
Batch 42/64 loss: -0.16408956050872803
Batch 43/64 loss: -0.16003549098968506
Batch 44/64 loss: -0.16580122709274292
Batch 45/64 loss: -0.15847599506378174
Batch 46/64 loss: -0.15077972412109375
Batch 47/64 loss: -0.15425348281860352
Batch 48/64 loss: -0.13479286432266235
Batch 49/64 loss: -0.16748172044754028
Batch 50/64 loss: -0.1566019058227539
Batch 51/64 loss: -0.14112377166748047
Batch 52/64 loss: -0.1405879259109497
Batch 53/64 loss: -0.1687166690826416
Batch 54/64 loss: -0.15453386306762695
Batch 55/64 loss: -0.14891326427459717
Batch 56/64 loss: -0.13488733768463135
Batch 57/64 loss: -0.16204488277435303
Batch 58/64 loss: -0.13552254438400269
Batch 59/64 loss: -0.14502596855163574
Batch 60/64 loss: -0.14762306213378906
Batch 61/64 loss: -0.14548015594482422
Batch 62/64 loss: -0.15208232402801514
Batch 63/64 loss: -0.14576780796051025
Batch 64/64 loss: -0.1486583948135376
Epoch 321  Train loss: -0.14939388808082132  Val loss: -0.020368014004631962
Epoch 322
-------------------------------
Batch 1/64 loss: -0.16275274753570557
Batch 2/64 loss: -0.16388237476348877
Batch 3/64 loss: -0.14265364408493042
Batch 4/64 loss: -0.13798511028289795
Batch 5/64 loss: -0.1552029848098755
Batch 6/64 loss: -0.1443105936050415
Batch 7/64 loss: -0.15866893529891968
Batch 8/64 loss: -0.14335167407989502
Batch 9/64 loss: -0.16464626789093018
Batch 10/64 loss: -0.17008811235427856
Batch 11/64 loss: -0.16005462408065796
Batch 12/64 loss: -0.1634005308151245
Batch 13/64 loss: -0.17771410942077637
Batch 14/64 loss: -0.14587748050689697
Batch 15/64 loss: -0.15930676460266113
Batch 16/64 loss: -0.1723412275314331
Batch 17/64 loss: -0.1314416527748108
Batch 18/64 loss: -0.16742348670959473
Batch 19/64 loss: -0.15672779083251953
Batch 20/64 loss: -0.14625144004821777
Batch 21/64 loss: -0.16281521320343018
Batch 22/64 loss: -0.14534741640090942
Batch 23/64 loss: -0.16983062028884888
Batch 24/64 loss: -0.16157734394073486
Batch 25/64 loss: -0.15163278579711914
Batch 26/64 loss: -0.15941625833511353
Batch 27/64 loss: -0.1511913537979126
Batch 28/64 loss: -0.15728145837783813
Batch 29/64 loss: -0.14824753999710083
Batch 30/64 loss: -0.13558298349380493
Batch 31/64 loss: -0.14357352256774902
Batch 32/64 loss: -0.1686076521873474
Batch 33/64 loss: -0.1371791958808899
Batch 34/64 loss: -0.15758532285690308
Batch 35/64 loss: -0.1668148636817932
Batch 36/64 loss: -0.143762469291687
Batch 37/64 loss: -0.1382509469985962
Batch 38/64 loss: -0.16337835788726807
Batch 39/64 loss: -0.1465737223625183
Batch 40/64 loss: -0.15748542547225952
Batch 41/64 loss: -0.14239835739135742
Batch 42/64 loss: -0.15186965465545654
Batch 43/64 loss: -0.13809102773666382
Batch 44/64 loss: -0.1506081223487854
Batch 45/64 loss: -0.12524056434631348
Batch 46/64 loss: -0.11981338262557983
Batch 47/64 loss: -0.17628389596939087
Batch 48/64 loss: -0.15784978866577148
Batch 49/64 loss: -0.1544325351715088
Batch 50/64 loss: -0.16797244548797607
Batch 51/64 loss: -0.1504257321357727
Batch 52/64 loss: -0.15793383121490479
Batch 53/64 loss: -0.12922626733779907
Batch 54/64 loss: -0.14814019203186035
Batch 55/64 loss: -0.13804268836975098
Batch 56/64 loss: -0.1451629400253296
Batch 57/64 loss: -0.16212338209152222
Batch 58/64 loss: -0.14604753255844116
Batch 59/64 loss: -0.17062121629714966
Batch 60/64 loss: -0.15026843547821045
Batch 61/64 loss: -0.14695614576339722
Batch 62/64 loss: -0.1498827338218689
Batch 63/64 loss: -0.14601027965545654
Batch 64/64 loss: -0.14972716569900513
Epoch 322  Train loss: -0.15259470635769415  Val loss: -0.02044812920167274
Epoch 323
-------------------------------
Batch 1/64 loss: -0.14667463302612305
Batch 2/64 loss: -0.17034828662872314
Batch 3/64 loss: -0.16640084981918335
Batch 4/64 loss: -0.16210925579071045
Batch 5/64 loss: -0.15116024017333984
Batch 6/64 loss: -0.17377787828445435
Batch 7/64 loss: -0.12088674306869507
Batch 8/64 loss: -0.17065966129302979
Batch 9/64 loss: -0.14659249782562256
Batch 10/64 loss: -0.17019212245941162
Batch 11/64 loss: -0.1214759349822998
Batch 12/64 loss: -0.15305960178375244
Batch 13/64 loss: -0.14919734001159668
Batch 14/64 loss: -0.16074466705322266
Batch 15/64 loss: -0.16491156816482544
Batch 16/64 loss: -0.14481496810913086
Batch 17/64 loss: -0.17324364185333252
Batch 18/64 loss: -0.15416181087493896
Batch 19/64 loss: -0.13740324974060059
Batch 20/64 loss: -0.1667705774307251
Batch 21/64 loss: -0.1839127540588379
Batch 22/64 loss: -0.15271782875061035
Batch 23/64 loss: -0.14730465412139893
Batch 24/64 loss: -0.1707463264465332
Batch 25/64 loss: -0.13503515720367432
Batch 26/64 loss: -0.17178595066070557
Batch 27/64 loss: -0.12785178422927856
Batch 28/64 loss: -0.16706764698028564
Batch 29/64 loss: -0.14935028553009033
Batch 30/64 loss: -0.13544172048568726
Batch 31/64 loss: -0.152210533618927
Batch 32/64 loss: -0.15512609481811523
Batch 33/64 loss: -0.1605241894721985
Batch 34/64 loss: -0.14643871784210205
Batch 35/64 loss: -0.1336299180984497
Batch 36/64 loss: -0.16032159328460693
Batch 37/64 loss: -0.1581607460975647
Batch 38/64 loss: -0.15642869472503662
Batch 39/64 loss: -0.1538102626800537
Batch 40/64 loss: -0.14929157495498657
Batch 41/64 loss: -0.12074708938598633
Batch 42/64 loss: -0.14180457592010498
Batch 43/64 loss: -0.1524871587753296
Batch 44/64 loss: -0.1254008412361145
Batch 45/64 loss: -0.1493607759475708
Batch 46/64 loss: -0.15260636806488037
Batch 47/64 loss: -0.14090180397033691
Batch 48/64 loss: -0.12580978870391846
Batch 49/64 loss: -0.15341442823410034
Batch 50/64 loss: -0.156990647315979
Batch 51/64 loss: -0.1376199722290039
Batch 52/64 loss: -0.14992111921310425
Batch 53/64 loss: -0.14851325750350952
Batch 54/64 loss: -0.1391681432723999
Batch 55/64 loss: -0.15674889087677002
Batch 56/64 loss: -0.15408754348754883
Batch 57/64 loss: -0.14577710628509521
Batch 58/64 loss: -0.16182667016983032
Batch 59/64 loss: -0.15599727630615234
Batch 60/64 loss: -0.15345042943954468
Batch 61/64 loss: -0.16789758205413818
Batch 62/64 loss: -0.15981513261795044
Batch 63/64 loss: -0.14861738681793213
Batch 64/64 loss: -0.1308944821357727
Epoch 323  Train loss: -0.15166865587234496  Val loss: -0.018786875243039475
Epoch 324
-------------------------------
Batch 1/64 loss: -0.1544666886329651
Batch 2/64 loss: -0.11156684160232544
Batch 3/64 loss: -0.16916602849960327
Batch 4/64 loss: -0.15485739707946777
Batch 5/64 loss: -0.16614031791687012
Batch 6/64 loss: -0.15350139141082764
Batch 7/64 loss: -0.1516796350479126
Batch 8/64 loss: -0.15370213985443115
Batch 9/64 loss: -0.15100687742233276
Batch 10/64 loss: -0.15484172105789185
Batch 11/64 loss: -0.15726733207702637
Batch 12/64 loss: -0.14406365156173706
Batch 13/64 loss: -0.16499030590057373
Batch 14/64 loss: -0.16064214706420898
Batch 15/64 loss: -0.1717848777770996
Batch 16/64 loss: -0.1462702751159668
Batch 17/64 loss: -0.15568101406097412
Batch 18/64 loss: -0.12061309814453125
Batch 19/64 loss: -0.17521882057189941
Batch 20/64 loss: -0.15235352516174316
Batch 21/64 loss: -0.13313615322113037
Batch 22/64 loss: -0.15356934070587158
Batch 23/64 loss: -0.15135657787322998
Batch 24/64 loss: -0.1464688777923584
Batch 25/64 loss: -0.14198124408721924
Batch 26/64 loss: -0.162456214427948
Batch 27/64 loss: -0.15193474292755127
Batch 28/64 loss: -0.1466243863105774
Batch 29/64 loss: -0.1284378170967102
Batch 30/64 loss: -0.1501452922821045
Batch 31/64 loss: -0.12703287601470947
Batch 32/64 loss: -0.14939475059509277
Batch 33/64 loss: -0.14200091361999512
Batch 34/64 loss: -0.15749698877334595
Batch 35/64 loss: -0.15397775173187256
Batch 36/64 loss: -0.14784377813339233
Batch 37/64 loss: -0.14499610662460327
Batch 38/64 loss: -0.15969669818878174
Batch 39/64 loss: -0.14850836992263794
Batch 40/64 loss: -0.11609148979187012
Batch 41/64 loss: -0.16502374410629272
Batch 42/64 loss: -0.14473628997802734
Batch 43/64 loss: -0.14773911237716675
Batch 44/64 loss: -0.15215909481048584
Batch 45/64 loss: -0.13714349269866943
Batch 46/64 loss: -0.1474231481552124
Batch 47/64 loss: -0.16161930561065674
Batch 48/64 loss: -0.14422142505645752
Batch 49/64 loss: -0.1338072419166565
Batch 50/64 loss: -0.1532207727432251
Batch 51/64 loss: -0.15006405115127563
Batch 52/64 loss: -0.1437596082687378
Batch 53/64 loss: -0.1560823917388916
Batch 54/64 loss: -0.15648424625396729
Batch 55/64 loss: -0.10931652784347534
Batch 56/64 loss: -0.1519950032234192
Batch 57/64 loss: -0.13221067190170288
Batch 58/64 loss: -0.15895092487335205
Batch 59/64 loss: -0.14500808715820312
Batch 60/64 loss: -0.12741756439208984
Batch 61/64 loss: -0.14806711673736572
Batch 62/64 loss: -0.18612420558929443
Batch 63/64 loss: -0.15775597095489502
Batch 64/64 loss: -0.10485857725143433
Epoch 324  Train loss: -0.14857942567152135  Val loss: -0.022461626128232767
Epoch 325
-------------------------------
Batch 1/64 loss: -0.13686442375183105
Batch 2/64 loss: -0.155289888381958
Batch 3/64 loss: -0.12296313047409058
Batch 4/64 loss: -0.16039621829986572
Batch 5/64 loss: -0.16363376379013062
Batch 6/64 loss: -0.17232555150985718
Batch 7/64 loss: -0.1292521357536316
Batch 8/64 loss: -0.16206622123718262
Batch 9/64 loss: -0.12134802341461182
Batch 10/64 loss: -0.16263407468795776
Batch 11/64 loss: -0.1714496612548828
Batch 12/64 loss: -0.16668760776519775
Batch 13/64 loss: -0.13037073612213135
Batch 14/64 loss: -0.14503490924835205
Batch 15/64 loss: -0.13586896657943726
Batch 16/64 loss: -0.15324419736862183
Batch 17/64 loss: -0.14835137128829956
Batch 18/64 loss: -0.11672300100326538
Batch 19/64 loss: -0.15716838836669922
Batch 20/64 loss: -0.1369415521621704
Batch 21/64 loss: -0.1372814178466797
Batch 22/64 loss: -0.1497114896774292
Batch 23/64 loss: -0.1501089334487915
Batch 24/64 loss: -0.1495140790939331
Batch 25/64 loss: -0.12218809127807617
Batch 26/64 loss: -0.1381409764289856
Batch 27/64 loss: -0.13643676042556763
Batch 28/64 loss: -0.15917545557022095
Batch 29/64 loss: -0.14368510246276855
Batch 30/64 loss: -0.15529179573059082
Batch 31/64 loss: -0.15033036470413208
Batch 32/64 loss: -0.1566341519355774
Batch 33/64 loss: -0.1411362886428833
Batch 34/64 loss: -0.12839126586914062
Batch 35/64 loss: -0.13310551643371582
Batch 36/64 loss: -0.1444166898727417
Batch 37/64 loss: -0.12182962894439697
Batch 38/64 loss: -0.1472017765045166
Batch 39/64 loss: -0.16444122791290283
Batch 40/64 loss: -0.1551445722579956
Batch 41/64 loss: -0.14892590045928955
Batch 42/64 loss: -0.1778774857521057
Batch 43/64 loss: -0.15737950801849365
Batch 44/64 loss: -0.17074668407440186
Batch 45/64 loss: -0.14675664901733398
Batch 46/64 loss: -0.1536637544631958
Batch 47/64 loss: -0.14133083820343018
Batch 48/64 loss: -0.12803548574447632
Batch 49/64 loss: -0.15742504596710205
Batch 50/64 loss: -0.15623605251312256
Batch 51/64 loss: -0.1451880931854248
Batch 52/64 loss: -0.1614537239074707
Batch 53/64 loss: -0.16973912715911865
Batch 54/64 loss: -0.13847196102142334
Batch 55/64 loss: -0.1217382550239563
Batch 56/64 loss: -0.15769356489181519
Batch 57/64 loss: -0.16884100437164307
Batch 58/64 loss: -0.148126482963562
Batch 59/64 loss: -0.15593427419662476
Batch 60/64 loss: -0.1408623456954956
Batch 61/64 loss: -0.13037914037704468
Batch 62/64 loss: -0.15223824977874756
Batch 63/64 loss: -0.1597820520401001
Batch 64/64 loss: -0.17243164777755737
Epoch 325  Train loss: -0.14824986387701594  Val loss: -0.020870130086682506
Epoch 326
-------------------------------
Batch 1/64 loss: -0.1452428102493286
Batch 2/64 loss: -0.1660088300704956
Batch 3/64 loss: -0.15647751092910767
Batch 4/64 loss: -0.16393786668777466
Batch 5/64 loss: -0.17345446348190308
Batch 6/64 loss: -0.15412253141403198
Batch 7/64 loss: -0.1424962282180786
Batch 8/64 loss: -0.1376711130142212
Batch 9/64 loss: -0.1470731496810913
Batch 10/64 loss: -0.15243542194366455
Batch 11/64 loss: -0.15478092432022095
Batch 12/64 loss: -0.15538042783737183
Batch 13/64 loss: -0.14695048332214355
Batch 14/64 loss: -0.14830011129379272
Batch 15/64 loss: -0.13821744918823242
Batch 16/64 loss: -0.12791699171066284
Batch 17/64 loss: -0.1601448655128479
Batch 18/64 loss: -0.16834229230880737
Batch 19/64 loss: -0.14643746614456177
Batch 20/64 loss: -0.1761818528175354
Batch 21/64 loss: -0.15399175882339478
Batch 22/64 loss: -0.17977815866470337
Batch 23/64 loss: -0.1520557403564453
Batch 24/64 loss: -0.15189814567565918
Batch 25/64 loss: -0.15022724866867065
Batch 26/64 loss: -0.14996904134750366
Batch 27/64 loss: -0.15568333864212036
Batch 28/64 loss: -0.16016358137130737
Batch 29/64 loss: -0.1395828127861023
Batch 30/64 loss: -0.16538184881210327
Batch 31/64 loss: -0.1581207513809204
Batch 32/64 loss: -0.15439927577972412
Batch 33/64 loss: -0.15410345792770386
Batch 34/64 loss: -0.15112757682800293
Batch 35/64 loss: -0.1885141134262085
Batch 36/64 loss: -0.15253812074661255
Batch 37/64 loss: -0.15084820985794067
Batch 38/64 loss: -0.1268330216407776
Batch 39/64 loss: -0.14998787641525269
Batch 40/64 loss: -0.14775174856185913
Batch 41/64 loss: -0.14061248302459717
Batch 42/64 loss: -0.15074759721755981
Batch 43/64 loss: -0.16697561740875244
Batch 44/64 loss: -0.16421091556549072
Batch 45/64 loss: -0.15087562799453735
Batch 46/64 loss: -0.1561495065689087
Batch 47/64 loss: -0.15121996402740479
Batch 48/64 loss: -0.16651785373687744
Batch 49/64 loss: -0.16702055931091309
Batch 50/64 loss: -0.13297957181930542
Batch 51/64 loss: -0.1701691746711731
Batch 52/64 loss: -0.12803781032562256
Batch 53/64 loss: -0.13774502277374268
Batch 54/64 loss: -0.14961349964141846
Batch 55/64 loss: -0.15953248739242554
Batch 56/64 loss: -0.16857904195785522
Batch 57/64 loss: -0.1615954041481018
Batch 58/64 loss: -0.16424113512039185
Batch 59/64 loss: -0.1561661958694458
Batch 60/64 loss: -0.15697449445724487
Batch 61/64 loss: -0.15637582540512085
Batch 62/64 loss: -0.181526780128479
Batch 63/64 loss: -0.1383078694343567
Batch 64/64 loss: -0.1403639316558838
Epoch 326  Train loss: -0.15428981968000824  Val loss: -0.019317848166239632
Epoch 327
-------------------------------
Batch 1/64 loss: -0.16704243421554565
Batch 2/64 loss: -0.16114413738250732
Batch 3/64 loss: -0.1602652668952942
Batch 4/64 loss: -0.16563564538955688
Batch 5/64 loss: -0.1604641079902649
Batch 6/64 loss: -0.13710469007492065
Batch 7/64 loss: -0.16636645793914795
Batch 8/64 loss: -0.1434868574142456
Batch 9/64 loss: -0.12188392877578735
Batch 10/64 loss: -0.15852832794189453
Batch 11/64 loss: -0.17652946710586548
Batch 12/64 loss: -0.14409053325653076
Batch 13/64 loss: -0.16843676567077637
Batch 14/64 loss: -0.15550750494003296
Batch 15/64 loss: -0.14119654893875122
Batch 16/64 loss: -0.14531052112579346
Batch 17/64 loss: -0.17177939414978027
Batch 18/64 loss: -0.16929250955581665
Batch 19/64 loss: -0.15948766469955444
Batch 20/64 loss: -0.1662883758544922
Batch 21/64 loss: -0.16094565391540527
Batch 22/64 loss: -0.12513363361358643
Batch 23/64 loss: -0.1597456932067871
Batch 24/64 loss: -0.15493273735046387
Batch 25/64 loss: -0.15178608894348145
Batch 26/64 loss: -0.1538882851600647
Batch 27/64 loss: -0.15841621160507202
Batch 28/64 loss: -0.14130795001983643
Batch 29/64 loss: -0.16355890035629272
Batch 30/64 loss: -0.16974365711212158
Batch 31/64 loss: -0.14374029636383057
Batch 32/64 loss: -0.17071622610092163
Batch 33/64 loss: -0.12675809860229492
Batch 34/64 loss: -0.1341986060142517
Batch 35/64 loss: -0.14890998601913452
Batch 36/64 loss: -0.15958338975906372
Batch 37/64 loss: -0.17513537406921387
Batch 38/64 loss: -0.1368502378463745
Batch 39/64 loss: -0.14807260036468506
Batch 40/64 loss: -0.14760076999664307
Batch 41/64 loss: -0.15466046333312988
Batch 42/64 loss: -0.14053547382354736
Batch 43/64 loss: -0.16864168643951416
Batch 44/64 loss: -0.11896473169326782
Batch 45/64 loss: -0.16712915897369385
Batch 46/64 loss: -0.1313808560371399
Batch 47/64 loss: -0.1717473268508911
Batch 48/64 loss: -0.13602888584136963
Batch 49/64 loss: -0.14736610651016235
Batch 50/64 loss: -0.14953196048736572
Batch 51/64 loss: -0.16799533367156982
Batch 52/64 loss: -0.1539427638053894
Batch 53/64 loss: -0.15857446193695068
Batch 54/64 loss: -0.18652558326721191
Batch 55/64 loss: -0.12753474712371826
Batch 56/64 loss: -0.16235113143920898
Batch 57/64 loss: -0.1476193070411682
Batch 58/64 loss: -0.15346890687942505
Batch 59/64 loss: -0.13661587238311768
Batch 60/64 loss: -0.13745903968811035
Batch 61/64 loss: -0.15122860670089722
Batch 62/64 loss: -0.13935381174087524
Batch 63/64 loss: -0.12148469686508179
Batch 64/64 loss: -0.12869584560394287
Epoch 327  Train loss: -0.15211809148975447  Val loss: -0.021136973117225358
Epoch 328
-------------------------------
Batch 1/64 loss: -0.14436078071594238
Batch 2/64 loss: -0.1476117968559265
Batch 3/64 loss: -0.13589805364608765
Batch 4/64 loss: -0.1656191349029541
Batch 5/64 loss: -0.14915698766708374
Batch 6/64 loss: -0.16304785013198853
Batch 7/64 loss: -0.1689838171005249
Batch 8/64 loss: -0.15262609720230103
Batch 9/64 loss: -0.15240299701690674
Batch 10/64 loss: -0.148040771484375
Batch 11/64 loss: -0.15923798084259033
Batch 12/64 loss: -0.16751694679260254
Batch 13/64 loss: -0.16326570510864258
Batch 14/64 loss: -0.1534387469291687
Batch 15/64 loss: -0.13915812969207764
Batch 16/64 loss: -0.1494876742362976
Batch 17/64 loss: -0.13402605056762695
Batch 18/64 loss: -0.17164599895477295
Batch 19/64 loss: -0.17174780368804932
Batch 20/64 loss: -0.1568291187286377
Batch 21/64 loss: -0.14244621992111206
Batch 22/64 loss: -0.1605263352394104
Batch 23/64 loss: -0.16436636447906494
Batch 24/64 loss: -0.1760343313217163
Batch 25/64 loss: -0.16134285926818848
Batch 26/64 loss: -0.16620415449142456
Batch 27/64 loss: -0.17775201797485352
Batch 28/64 loss: -0.15234190225601196
Batch 29/64 loss: -0.17635154724121094
Batch 30/64 loss: -0.17020010948181152
Batch 31/64 loss: -0.156916081905365
Batch 32/64 loss: -0.17413628101348877
Batch 33/64 loss: -0.13295584917068481
Batch 34/64 loss: -0.133428156375885
Batch 35/64 loss: -0.16751998662948608
Batch 36/64 loss: -0.13538527488708496
Batch 37/64 loss: -0.140042245388031
Batch 38/64 loss: -0.15916401147842407
Batch 39/64 loss: -0.13754934072494507
Batch 40/64 loss: -0.14874833822250366
Batch 41/64 loss: -0.15952050685882568
Batch 42/64 loss: -0.15295666456222534
Batch 43/64 loss: -0.15024524927139282
Batch 44/64 loss: -0.1511128544807434
Batch 45/64 loss: -0.15541625022888184
Batch 46/64 loss: -0.1452811360359192
Batch 47/64 loss: -0.13790041208267212
Batch 48/64 loss: -0.15846401453018188
Batch 49/64 loss: -0.13039815425872803
Batch 50/64 loss: -0.14903050661087036
Batch 51/64 loss: -0.16991126537322998
Batch 52/64 loss: -0.14997953176498413
Batch 53/64 loss: -0.1638771891593933
Batch 54/64 loss: -0.16340744495391846
Batch 55/64 loss: -0.1615251898765564
Batch 56/64 loss: -0.16020381450653076
Batch 57/64 loss: -0.1664023995399475
Batch 58/64 loss: -0.16589993238449097
Batch 59/64 loss: -0.15524518489837646
Batch 60/64 loss: -0.17253440618515015
Batch 61/64 loss: -0.13666075468063354
Batch 62/64 loss: -0.15261101722717285
Batch 63/64 loss: -0.15404510498046875
Batch 64/64 loss: -0.14619064331054688
Epoch 328  Train loss: -0.1552907579085406  Val loss: -0.019537855259741294
Epoch 329
-------------------------------
Batch 1/64 loss: -0.1685914397239685
Batch 2/64 loss: -0.14423394203186035
Batch 3/64 loss: -0.15471529960632324
Batch 4/64 loss: -0.14780330657958984
Batch 5/64 loss: -0.16492879390716553
Batch 6/64 loss: -0.15303689241409302
Batch 7/64 loss: -0.15506285429000854
Batch 8/64 loss: -0.13311612606048584
Batch 9/64 loss: -0.14286035299301147
Batch 10/64 loss: -0.16326600313186646
Batch 11/64 loss: -0.15928161144256592
Batch 12/64 loss: -0.17276811599731445
Batch 13/64 loss: -0.18090564012527466
Batch 14/64 loss: -0.15980172157287598
Batch 15/64 loss: -0.13833653926849365
Batch 16/64 loss: -0.14484965801239014
Batch 17/64 loss: -0.15501469373703003
Batch 18/64 loss: -0.16481250524520874
Batch 19/64 loss: -0.16360771656036377
Batch 20/64 loss: -0.1491526961326599
Batch 21/64 loss: -0.1649903655052185
Batch 22/64 loss: -0.14325016736984253
Batch 23/64 loss: -0.1721321940422058
Batch 24/64 loss: -0.15932756662368774
Batch 25/64 loss: -0.16537386178970337
Batch 26/64 loss: -0.1616641879081726
Batch 27/64 loss: -0.17953717708587646
Batch 28/64 loss: -0.17262834310531616
Batch 29/64 loss: -0.15026575326919556
Batch 30/64 loss: -0.15271008014678955
Batch 31/64 loss: -0.16755646467208862
Batch 32/64 loss: -0.1617383360862732
Batch 33/64 loss: -0.13507074117660522
Batch 34/64 loss: -0.15803706645965576
Batch 35/64 loss: -0.16060221195220947
Batch 36/64 loss: -0.15574097633361816
Batch 37/64 loss: -0.16708046197891235
Batch 38/64 loss: -0.1474938988685608
Batch 39/64 loss: -0.1408984661102295
Batch 40/64 loss: -0.15199577808380127
Batch 41/64 loss: -0.16104799509048462
Batch 42/64 loss: -0.1512516736984253
Batch 43/64 loss: -0.1642899513244629
Batch 44/64 loss: -0.1573779582977295
Batch 45/64 loss: -0.14801812171936035
Batch 46/64 loss: -0.14646154642105103
Batch 47/64 loss: -0.1434856653213501
Batch 48/64 loss: -0.15645331144332886
Batch 49/64 loss: -0.14850902557373047
Batch 50/64 loss: -0.1515328288078308
Batch 51/64 loss: -0.16572022438049316
Batch 52/64 loss: -0.1373170018196106
Batch 53/64 loss: -0.15319114923477173
Batch 54/64 loss: -0.10909056663513184
Batch 55/64 loss: -0.1502653956413269
Batch 56/64 loss: -0.1375369429588318
Batch 57/64 loss: -0.1724071502685547
Batch 58/64 loss: -0.15579676628112793
Batch 59/64 loss: -0.16519391536712646
Batch 60/64 loss: -0.16108280420303345
Batch 61/64 loss: -0.15445846319198608
Batch 62/64 loss: -0.1564314365386963
Batch 63/64 loss: -0.15177679061889648
Batch 64/64 loss: -0.1711559295654297
Epoch 329  Train loss: -0.15547142963783414  Val loss: -0.01968400421011489
Epoch 330
-------------------------------
Batch 1/64 loss: -0.1658635139465332
Batch 2/64 loss: -0.14575988054275513
Batch 3/64 loss: -0.14301669597625732
Batch 4/64 loss: -0.15564876794815063
Batch 5/64 loss: -0.1626650094985962
Batch 6/64 loss: -0.1712590456008911
Batch 7/64 loss: -0.15155500173568726
Batch 8/64 loss: -0.12837493419647217
Batch 9/64 loss: -0.1676366925239563
Batch 10/64 loss: -0.17100739479064941
Batch 11/64 loss: -0.1631121039390564
Batch 12/64 loss: -0.16501939296722412
Batch 13/64 loss: -0.16203927993774414
Batch 14/64 loss: -0.17951363325119019
Batch 15/64 loss: -0.15481483936309814
Batch 16/64 loss: -0.15528523921966553
Batch 17/64 loss: -0.1420658826828003
Batch 18/64 loss: -0.15966087579727173
Batch 19/64 loss: -0.1602243185043335
Batch 20/64 loss: -0.15384113788604736
Batch 21/64 loss: -0.17290031909942627
Batch 22/64 loss: -0.1721256971359253
Batch 23/64 loss: -0.1402212381362915
Batch 24/64 loss: -0.1516379714012146
Batch 25/64 loss: -0.11429530382156372
Batch 26/64 loss: -0.1712821125984192
Batch 27/64 loss: -0.169447660446167
Batch 28/64 loss: -0.15803009271621704
Batch 29/64 loss: -0.1558976173400879
Batch 30/64 loss: -0.14109724760055542
Batch 31/64 loss: -0.16392582654953003
Batch 32/64 loss: -0.1514478325843811
Batch 33/64 loss: -0.144459068775177
Batch 34/64 loss: -0.16337376832962036
Batch 35/64 loss: -0.15352469682693481
Batch 36/64 loss: -0.11957061290740967
Batch 37/64 loss: -0.14702147245407104
Batch 38/64 loss: -0.14661413431167603
Batch 39/64 loss: -0.1844845414161682
Batch 40/64 loss: -0.17523294687271118
Batch 41/64 loss: -0.15639835596084595
Batch 42/64 loss: -0.14880800247192383
Batch 43/64 loss: -0.13779091835021973
Batch 44/64 loss: -0.15488570928573608
Batch 45/64 loss: -0.1588858962059021
Batch 46/64 loss: -0.11430227756500244
Batch 47/64 loss: -0.14019697904586792
Batch 48/64 loss: -0.15011227130889893
Batch 49/64 loss: -0.14918696880340576
Batch 50/64 loss: -0.15259873867034912
Batch 51/64 loss: -0.16940170526504517
Batch 52/64 loss: -0.16644233465194702
Batch 53/64 loss: -0.1454242467880249
Batch 54/64 loss: -0.15184545516967773
Batch 55/64 loss: -0.1557641625404358
Batch 56/64 loss: -0.16936206817626953
Batch 57/64 loss: -0.13909155130386353
Batch 58/64 loss: -0.13424170017242432
Batch 59/64 loss: -0.16381019353866577
Batch 60/64 loss: -0.16148561239242554
Batch 61/64 loss: -0.15435367822647095
Batch 62/64 loss: -0.17095351219177246
Batch 63/64 loss: -0.1602758765220642
Batch 64/64 loss: -0.16981226205825806
Epoch 330  Train loss: -0.15510480333777035  Val loss: -0.0204884756062039
Epoch 331
-------------------------------
Batch 1/64 loss: -0.1563071608543396
Batch 2/64 loss: -0.14378643035888672
Batch 3/64 loss: -0.15275275707244873
Batch 4/64 loss: -0.1728583574295044
Batch 5/64 loss: -0.14581137895584106
Batch 6/64 loss: -0.1611483097076416
Batch 7/64 loss: -0.16734182834625244
Batch 8/64 loss: -0.1505090594291687
Batch 9/64 loss: -0.149763822555542
Batch 10/64 loss: -0.13679540157318115
Batch 11/64 loss: -0.1740577220916748
Batch 12/64 loss: -0.15432018041610718
Batch 13/64 loss: -0.11132335662841797
Batch 14/64 loss: -0.14661598205566406
Batch 15/64 loss: -0.13322103023529053
Batch 16/64 loss: -0.15367728471755981
Batch 17/64 loss: -0.154208242893219
Batch 18/64 loss: -0.16006851196289062
Batch 19/64 loss: -0.11925673484802246
Batch 20/64 loss: -0.13742882013320923
Batch 21/64 loss: -0.14607501029968262
Batch 22/64 loss: -0.16576087474822998
Batch 23/64 loss: -0.15402448177337646
Batch 24/64 loss: -0.1477515697479248
Batch 25/64 loss: -0.13997793197631836
Batch 26/64 loss: -0.13500165939331055
Batch 27/64 loss: -0.16244912147521973
Batch 28/64 loss: -0.16583466529846191
Batch 29/64 loss: -0.1403946876525879
Batch 30/64 loss: -0.15280747413635254
Batch 31/64 loss: -0.17180800437927246
Batch 32/64 loss: -0.16571229696273804
Batch 33/64 loss: -0.14047455787658691
Batch 34/64 loss: -0.14736932516098022
Batch 35/64 loss: -0.16006869077682495
Batch 36/64 loss: -0.1423243284225464
Batch 37/64 loss: -0.17100131511688232
Batch 38/64 loss: -0.15593606233596802
Batch 39/64 loss: -0.17739903926849365
Batch 40/64 loss: -0.13941538333892822
Batch 41/64 loss: -0.14340537786483765
Batch 42/64 loss: -0.17089498043060303
Batch 43/64 loss: -0.1473979949951172
Batch 44/64 loss: -0.14655375480651855
Batch 45/64 loss: -0.14533638954162598
Batch 46/64 loss: -0.157151460647583
Batch 47/64 loss: -0.13600796461105347
Batch 48/64 loss: -0.14656776189804077
Batch 49/64 loss: -0.14957153797149658
Batch 50/64 loss: -0.13924115896224976
Batch 51/64 loss: -0.14886188507080078
Batch 52/64 loss: -0.16549789905548096
Batch 53/64 loss: -0.14200085401535034
Batch 54/64 loss: -0.12291282415390015
Batch 55/64 loss: -0.15905249118804932
Batch 56/64 loss: -0.13736450672149658
Batch 57/64 loss: -0.12814420461654663
Batch 58/64 loss: -0.13846492767333984
Batch 59/64 loss: -0.1312238574028015
Batch 60/64 loss: -0.1690763235092163
Batch 61/64 loss: -0.14580261707305908
Batch 62/64 loss: -0.16372144222259521
Batch 63/64 loss: -0.13498985767364502
Batch 64/64 loss: -0.15036475658416748
Epoch 331  Train loss: -0.14972320771684833  Val loss: -0.02207934323864704
Epoch 332
-------------------------------
Batch 1/64 loss: -0.1271793246269226
Batch 2/64 loss: -0.14868998527526855
Batch 3/64 loss: -0.15403109788894653
Batch 4/64 loss: -0.15991908311843872
Batch 5/64 loss: -0.14932918548583984
Batch 6/64 loss: -0.1580454707145691
Batch 7/64 loss: -0.14860516786575317
Batch 8/64 loss: -0.1430584192276001
Batch 9/64 loss: -0.16218215227127075
Batch 10/64 loss: -0.16714131832122803
Batch 11/64 loss: -0.16564714908599854
Batch 12/64 loss: -0.17745333909988403
Batch 13/64 loss: -0.16094505786895752
Batch 14/64 loss: -0.16954022645950317
Batch 15/64 loss: -0.15943187475204468
Batch 16/64 loss: -0.1517477035522461
Batch 17/64 loss: -0.1505414843559265
Batch 18/64 loss: -0.14281952381134033
Batch 19/64 loss: -0.1536414623260498
Batch 20/64 loss: -0.16208726167678833
Batch 21/64 loss: -0.14736545085906982
Batch 22/64 loss: -0.16771769523620605
Batch 23/64 loss: -0.15995103120803833
Batch 24/64 loss: -0.1893080472946167
Batch 25/64 loss: -0.15147531032562256
Batch 26/64 loss: -0.15836083889007568
Batch 27/64 loss: -0.15984463691711426
Batch 28/64 loss: -0.14443504810333252
Batch 29/64 loss: -0.1527540683746338
Batch 30/64 loss: -0.16169744729995728
Batch 31/64 loss: -0.1676182746887207
Batch 32/64 loss: -0.12984561920166016
Batch 33/64 loss: -0.14470213651657104
Batch 34/64 loss: -0.141154944896698
Batch 35/64 loss: -0.14894717931747437
Batch 36/64 loss: -0.12610852718353271
Batch 37/64 loss: -0.17017793655395508
Batch 38/64 loss: -0.17088580131530762
Batch 39/64 loss: -0.14252156019210815
Batch 40/64 loss: -0.17470884323120117
Batch 41/64 loss: -0.16428279876708984
Batch 42/64 loss: -0.14345717430114746
Batch 43/64 loss: -0.16068124771118164
Batch 44/64 loss: -0.16482502222061157
Batch 45/64 loss: -0.15147417783737183
Batch 46/64 loss: -0.14178425073623657
Batch 47/64 loss: -0.15855073928833008
Batch 48/64 loss: -0.1690056324005127
Batch 49/64 loss: -0.1639552116394043
Batch 50/64 loss: -0.17118781805038452
Batch 51/64 loss: -0.15355634689331055
Batch 52/64 loss: -0.1434413194656372
Batch 53/64 loss: -0.1407870054244995
Batch 54/64 loss: -0.1526142954826355
Batch 55/64 loss: -0.1735391616821289
Batch 56/64 loss: -0.1665080189704895
Batch 57/64 loss: -0.12058109045028687
Batch 58/64 loss: -0.14660555124282837
Batch 59/64 loss: -0.1551463007926941
Batch 60/64 loss: -0.15805590152740479
Batch 61/64 loss: -0.12290841341018677
Batch 62/64 loss: -0.15590167045593262
Batch 63/64 loss: -0.1487066149711609
Batch 64/64 loss: -0.16700613498687744
Epoch 332  Train loss: -0.1548929733388564  Val loss: -0.01929654739157031
Epoch 333
-------------------------------
Batch 1/64 loss: -0.16129428148269653
Batch 2/64 loss: -0.1745455265045166
Batch 3/64 loss: -0.1664574146270752
Batch 4/64 loss: -0.16731417179107666
Batch 5/64 loss: -0.1807369589805603
Batch 6/64 loss: -0.15837621688842773
Batch 7/64 loss: -0.17120718955993652
Batch 8/64 loss: -0.1805317997932434
Batch 9/64 loss: -0.154571533203125
Batch 10/64 loss: -0.15951359272003174
Batch 11/64 loss: -0.1565520167350769
Batch 12/64 loss: -0.15354210138320923
Batch 13/64 loss: -0.16843807697296143
Batch 14/64 loss: -0.1496526598930359
Batch 15/64 loss: -0.15790921449661255
Batch 16/64 loss: -0.14928662776947021
Batch 17/64 loss: -0.17144101858139038
Batch 18/64 loss: -0.1627063751220703
Batch 19/64 loss: -0.15280717611312866
Batch 20/64 loss: -0.13531041145324707
Batch 21/64 loss: -0.15913313627243042
Batch 22/64 loss: -0.1529374122619629
Batch 23/64 loss: -0.16367065906524658
Batch 24/64 loss: -0.15365785360336304
Batch 25/64 loss: -0.1204724907875061
Batch 26/64 loss: -0.13289296627044678
Batch 27/64 loss: -0.1563696265220642
Batch 28/64 loss: -0.13986802101135254
Batch 29/64 loss: -0.1472035050392151
Batch 30/64 loss: -0.15764331817626953
Batch 31/64 loss: -0.15335619449615479
Batch 32/64 loss: -0.1346413493156433
Batch 33/64 loss: -0.1370120644569397
Batch 34/64 loss: -0.1609378457069397
Batch 35/64 loss: -0.1459205150604248
Batch 36/64 loss: -0.15979957580566406
Batch 37/64 loss: -0.12748026847839355
Batch 38/64 loss: -0.1430720090866089
Batch 39/64 loss: -0.1457829475402832
Batch 40/64 loss: -0.1458567976951599
Batch 41/64 loss: -0.1509777307510376
Batch 42/64 loss: -0.16623950004577637
Batch 43/64 loss: -0.16790610551834106
Batch 44/64 loss: -0.16183751821517944
Batch 45/64 loss: -0.16069155931472778
Batch 46/64 loss: -0.13396799564361572
Batch 47/64 loss: -0.15253746509552002
Batch 48/64 loss: -0.17236369848251343
Batch 49/64 loss: -0.16152340173721313
Batch 50/64 loss: -0.1483425498008728
Batch 51/64 loss: -0.14770770072937012
Batch 52/64 loss: -0.15438514947891235
Batch 53/64 loss: -0.14960205554962158
Batch 54/64 loss: -0.15405809879302979
Batch 55/64 loss: -0.14422714710235596
Batch 56/64 loss: -0.1406635046005249
Batch 57/64 loss: -0.154302716255188
Batch 58/64 loss: -0.1495739221572876
Batch 59/64 loss: -0.15672194957733154
Batch 60/64 loss: -0.1460251808166504
Batch 61/64 loss: -0.16339111328125
Batch 62/64 loss: -0.16300058364868164
Batch 63/64 loss: -0.1424088478088379
Batch 64/64 loss: -0.12672585248947144
Epoch 333  Train loss: -0.1538416126195122  Val loss: -0.01875938828458491
Epoch 334
-------------------------------
Batch 1/64 loss: -0.1567210555076599
Batch 2/64 loss: -0.1530018448829651
Batch 3/64 loss: -0.1639452576637268
Batch 4/64 loss: -0.1527472734451294
Batch 5/64 loss: -0.15039128065109253
Batch 6/64 loss: -0.15154731273651123
Batch 7/64 loss: -0.1574954390525818
Batch 8/64 loss: -0.1460268497467041
Batch 9/64 loss: -0.1600627899169922
Batch 10/64 loss: -0.15931552648544312
Batch 11/64 loss: -0.15314704179763794
Batch 12/64 loss: -0.14037561416625977
Batch 13/64 loss: -0.1701056957244873
Batch 14/64 loss: -0.150529146194458
Batch 15/64 loss: -0.14008957147598267
Batch 16/64 loss: -0.14581477642059326
Batch 17/64 loss: -0.16776001453399658
Batch 18/64 loss: -0.15475356578826904
Batch 19/64 loss: -0.13921654224395752
Batch 20/64 loss: -0.13928651809692383
Batch 21/64 loss: -0.16605162620544434
Batch 22/64 loss: -0.13262420892715454
Batch 23/64 loss: -0.1591063141822815
Batch 24/64 loss: -0.1361292600631714
Batch 25/64 loss: -0.13902926445007324
Batch 26/64 loss: -0.1567392349243164
Batch 27/64 loss: -0.16044914722442627
Batch 28/64 loss: -0.14538633823394775
Batch 29/64 loss: -0.16005849838256836
Batch 30/64 loss: -0.1486661434173584
Batch 31/64 loss: -0.16496145725250244
Batch 32/64 loss: -0.1538371443748474
Batch 33/64 loss: -0.16883927583694458
Batch 34/64 loss: -0.15556174516677856
Batch 35/64 loss: -0.1683773398399353
Batch 36/64 loss: -0.16204428672790527
Batch 37/64 loss: -0.1574210524559021
Batch 38/64 loss: -0.16214537620544434
Batch 39/64 loss: -0.1498955488204956
Batch 40/64 loss: -0.16207844018936157
Batch 41/64 loss: -0.13723266124725342
Batch 42/64 loss: -0.15707159042358398
Batch 43/64 loss: -0.15550047159194946
Batch 44/64 loss: -0.1594090461730957
Batch 45/64 loss: -0.17893868684768677
Batch 46/64 loss: -0.13054269552230835
Batch 47/64 loss: -0.14452362060546875
Batch 48/64 loss: -0.15871447324752808
Batch 49/64 loss: -0.11321002244949341
Batch 50/64 loss: -0.15632915496826172
Batch 51/64 loss: -0.15865635871887207
Batch 52/64 loss: -0.15322822332382202
Batch 53/64 loss: -0.15479791164398193
Batch 54/64 loss: -0.15521180629730225
Batch 55/64 loss: -0.15626376867294312
Batch 56/64 loss: -0.14612770080566406
Batch 57/64 loss: -0.14665216207504272
Batch 58/64 loss: -0.1599913239479065
Batch 59/64 loss: -0.16535520553588867
Batch 60/64 loss: -0.15401530265808105
Batch 61/64 loss: -0.16061508655548096
Batch 62/64 loss: -0.15117698907852173
Batch 63/64 loss: -0.16560345888137817
Batch 64/64 loss: -0.17050319910049438
Epoch 334  Train loss: -0.15386321474524106  Val loss: -0.020654902630245563
Epoch 335
-------------------------------
Batch 1/64 loss: -0.16935861110687256
Batch 2/64 loss: -0.15558791160583496
Batch 3/64 loss: -0.15225529670715332
Batch 4/64 loss: -0.17549526691436768
Batch 5/64 loss: -0.16339600086212158
Batch 6/64 loss: -0.1547876000404358
Batch 7/64 loss: -0.1640048623085022
Batch 8/64 loss: -0.18632721900939941
Batch 9/64 loss: -0.176727294921875
Batch 10/64 loss: -0.12021005153656006
Batch 11/64 loss: -0.17137646675109863
Batch 12/64 loss: -0.155747652053833
Batch 13/64 loss: -0.1532381772994995
Batch 14/64 loss: -0.1420069932937622
Batch 15/64 loss: -0.17121988534927368
Batch 16/64 loss: -0.15919750928878784
Batch 17/64 loss: -0.15769731998443604
Batch 18/64 loss: -0.14938688278198242
Batch 19/64 loss: -0.1529008150100708
Batch 20/64 loss: -0.16778630018234253
Batch 21/64 loss: -0.16397368907928467
Batch 22/64 loss: -0.15546166896820068
Batch 23/64 loss: -0.15461117029190063
Batch 24/64 loss: -0.16422945261001587
Batch 25/64 loss: -0.15970993041992188
Batch 26/64 loss: -0.16543418169021606
Batch 27/64 loss: -0.15326833724975586
Batch 28/64 loss: -0.15336287021636963
Batch 29/64 loss: -0.14586687088012695
Batch 30/64 loss: -0.15647602081298828
Batch 31/64 loss: -0.14655143022537231
Batch 32/64 loss: -0.16643357276916504
Batch 33/64 loss: -0.16481047868728638
Batch 34/64 loss: -0.16101348400115967
Batch 35/64 loss: -0.13444805145263672
Batch 36/64 loss: -0.18105286359786987
Batch 37/64 loss: -0.1578298807144165
Batch 38/64 loss: -0.14426642656326294
Batch 39/64 loss: -0.14331752061843872
Batch 40/64 loss: -0.13108646869659424
Batch 41/64 loss: -0.16820865869522095
Batch 42/64 loss: -0.15127313137054443
Batch 43/64 loss: -0.16814684867858887
Batch 44/64 loss: -0.15524160861968994
Batch 45/64 loss: -0.15578210353851318
Batch 46/64 loss: -0.1695190668106079
Batch 47/64 loss: -0.14286130666732788
Batch 48/64 loss: -0.1678088903427124
Batch 49/64 loss: -0.14948725700378418
Batch 50/64 loss: -0.14426440000534058
Batch 51/64 loss: -0.14441120624542236
Batch 52/64 loss: -0.14918076992034912
Batch 53/64 loss: -0.15229767560958862
Batch 54/64 loss: -0.15288257598876953
Batch 55/64 loss: -0.16235440969467163
Batch 56/64 loss: -0.12643104791641235
Batch 57/64 loss: -0.16034650802612305
Batch 58/64 loss: -0.1569972038269043
Batch 59/64 loss: -0.14116013050079346
Batch 60/64 loss: -0.15489310026168823
Batch 61/64 loss: -0.1497173309326172
Batch 62/64 loss: -0.14435267448425293
Batch 63/64 loss: -0.16709363460540771
Batch 64/64 loss: -0.16527098417282104
Epoch 335  Train loss: -0.15624434924593159  Val loss: -0.017428482930684826
Epoch 336
-------------------------------
Batch 1/64 loss: -0.1530436873435974
Batch 2/64 loss: -0.16356301307678223
Batch 3/64 loss: -0.13672930002212524
Batch 4/64 loss: -0.16607749462127686
Batch 5/64 loss: -0.166550874710083
Batch 6/64 loss: -0.14334332942962646
Batch 7/64 loss: -0.17137831449508667
Batch 8/64 loss: -0.157944917678833
Batch 9/64 loss: -0.1585695743560791
Batch 10/64 loss: -0.16347891092300415
Batch 11/64 loss: -0.1708248257637024
Batch 12/64 loss: -0.15583175420761108
Batch 13/64 loss: -0.17058038711547852
Batch 14/64 loss: -0.15180230140686035
Batch 15/64 loss: -0.1720028519630432
Batch 16/64 loss: -0.1490815281867981
Batch 17/64 loss: -0.170060932636261
Batch 18/64 loss: -0.12913012504577637
Batch 19/64 loss: -0.15727627277374268
Batch 20/64 loss: -0.15042579174041748
Batch 21/64 loss: -0.16243231296539307
Batch 22/64 loss: -0.16354632377624512
Batch 23/64 loss: -0.16426771879196167
Batch 24/64 loss: -0.1656816005706787
Batch 25/64 loss: -0.17061865329742432
Batch 26/64 loss: -0.15107005834579468
Batch 27/64 loss: -0.1456393599510193
Batch 28/64 loss: -0.16899126768112183
Batch 29/64 loss: -0.16282367706298828
Batch 30/64 loss: -0.15702015161514282
Batch 31/64 loss: -0.14649564027786255
Batch 32/64 loss: -0.14896178245544434
Batch 33/64 loss: -0.16774964332580566
Batch 34/64 loss: -0.15607905387878418
Batch 35/64 loss: -0.12443286180496216
Batch 36/64 loss: -0.16828250885009766
Batch 37/64 loss: -0.1601077914237976
Batch 38/64 loss: -0.1728372573852539
Batch 39/64 loss: -0.15194261074066162
Batch 40/64 loss: -0.15306085348129272
Batch 41/64 loss: -0.16423887014389038
Batch 42/64 loss: -0.15782594680786133
Batch 43/64 loss: -0.1338355541229248
Batch 44/64 loss: -0.1722235083580017
Batch 45/64 loss: -0.14645713567733765
Batch 46/64 loss: -0.14139002561569214
Batch 47/64 loss: -0.13765090703964233
Batch 48/64 loss: -0.15001320838928223
Batch 49/64 loss: -0.16069144010543823
Batch 50/64 loss: -0.1450275182723999
Batch 51/64 loss: -0.16491031646728516
Batch 52/64 loss: -0.15368753671646118
Batch 53/64 loss: -0.14977121353149414
Batch 54/64 loss: -0.1385175585746765
Batch 55/64 loss: -0.16651344299316406
Batch 56/64 loss: -0.1552717089653015
Batch 57/64 loss: -0.14277160167694092
Batch 58/64 loss: -0.1702694296836853
Batch 59/64 loss: -0.17098963260650635
Batch 60/64 loss: -0.17348599433898926
Batch 61/64 loss: -0.13835245370864868
Batch 62/64 loss: -0.15549081563949585
Batch 63/64 loss: -0.16481798887252808
Batch 64/64 loss: -0.14445942640304565
Epoch 336  Train loss: -0.15658487362020157  Val loss: -0.019359303708748308
Epoch 337
-------------------------------
Batch 1/64 loss: -0.13721036911010742
Batch 2/64 loss: -0.18539750576019287
Batch 3/64 loss: -0.15048837661743164
Batch 4/64 loss: -0.1677810549736023
Batch 5/64 loss: -0.16576164960861206
Batch 6/64 loss: -0.17743873596191406
Batch 7/64 loss: -0.17168331146240234
Batch 8/64 loss: -0.16374480724334717
Batch 9/64 loss: -0.16341465711593628
Batch 10/64 loss: -0.14066731929779053
Batch 11/64 loss: -0.1502133011817932
Batch 12/64 loss: -0.1634312868118286
Batch 13/64 loss: -0.15477561950683594
Batch 14/64 loss: -0.16670548915863037
Batch 15/64 loss: -0.1661466360092163
Batch 16/64 loss: -0.15913957357406616
Batch 17/64 loss: -0.17127573490142822
Batch 18/64 loss: -0.1510765552520752
Batch 19/64 loss: -0.1822594404220581
Batch 20/64 loss: -0.16079622507095337
Batch 21/64 loss: -0.17425233125686646
Batch 22/64 loss: -0.16353625059127808
Batch 23/64 loss: -0.14618301391601562
Batch 24/64 loss: -0.1602495312690735
Batch 25/64 loss: -0.16351795196533203
Batch 26/64 loss: -0.15416878461837769
Batch 27/64 loss: -0.14970874786376953
Batch 28/64 loss: -0.13669681549072266
Batch 29/64 loss: -0.1484760046005249
Batch 30/64 loss: -0.15778189897537231
Batch 31/64 loss: -0.15685439109802246
Batch 32/64 loss: -0.16369956731796265
Batch 33/64 loss: -0.1568569540977478
Batch 34/64 loss: -0.1611994504928589
Batch 35/64 loss: -0.15950369834899902
Batch 36/64 loss: -0.1557403802871704
Batch 37/64 loss: -0.15981018543243408
Batch 38/64 loss: -0.14136475324630737
Batch 39/64 loss: -0.16773635149002075
Batch 40/64 loss: -0.15876662731170654
Batch 41/64 loss: -0.17237955331802368
Batch 42/64 loss: -0.13353842496871948
Batch 43/64 loss: -0.15498721599578857
Batch 44/64 loss: -0.17265599966049194
Batch 45/64 loss: -0.1558823585510254
Batch 46/64 loss: -0.16591644287109375
Batch 47/64 loss: -0.18405681848526
Batch 48/64 loss: -0.16054892539978027
Batch 49/64 loss: -0.1642470359802246
Batch 50/64 loss: -0.16782146692276
Batch 51/64 loss: -0.16071456670761108
Batch 52/64 loss: -0.1631065011024475
Batch 53/64 loss: -0.16925835609436035
Batch 54/64 loss: -0.16057628393173218
Batch 55/64 loss: -0.16489678621292114
Batch 56/64 loss: -0.17208796739578247
Batch 57/64 loss: -0.13255250453948975
Batch 58/64 loss: -0.13436245918273926
Batch 59/64 loss: -0.16966110467910767
Batch 60/64 loss: -0.17751330137252808
Batch 61/64 loss: -0.1504393219947815
Batch 62/64 loss: -0.16552287340164185
Batch 63/64 loss: -0.17588329315185547
Batch 64/64 loss: -0.16102689504623413
Epoch 337  Train loss: -0.16054731093200983  Val loss: -0.019695415324771526
Epoch 338
-------------------------------
Batch 1/64 loss: -0.17084068059921265
Batch 2/64 loss: -0.19279742240905762
Batch 3/64 loss: -0.17612385749816895
Batch 4/64 loss: -0.15288537740707397
Batch 5/64 loss: -0.17849814891815186
Batch 6/64 loss: -0.16613835096359253
Batch 7/64 loss: -0.17476481199264526
Batch 8/64 loss: -0.1870509386062622
Batch 9/64 loss: -0.17436504364013672
Batch 10/64 loss: -0.15670627355575562
Batch 11/64 loss: -0.1703258752822876
Batch 12/64 loss: -0.14812004566192627
Batch 13/64 loss: -0.17729926109313965
Batch 14/64 loss: -0.1707066297531128
Batch 15/64 loss: -0.15910255908966064
Batch 16/64 loss: -0.14002275466918945
Batch 17/64 loss: -0.14349102973937988
Batch 18/64 loss: -0.16028010845184326
Batch 19/64 loss: -0.14124983549118042
Batch 20/64 loss: -0.16477364301681519
Batch 21/64 loss: -0.1593613624572754
Batch 22/64 loss: -0.1767081618309021
Batch 23/64 loss: -0.1672024130821228
Batch 24/64 loss: -0.15916168689727783
Batch 25/64 loss: -0.1679319143295288
Batch 26/64 loss: -0.15774226188659668
Batch 27/64 loss: -0.16825079917907715
Batch 28/64 loss: -0.14623510837554932
Batch 29/64 loss: -0.17189425230026245
Batch 30/64 loss: -0.17009544372558594
Batch 31/64 loss: -0.15921491384506226
Batch 32/64 loss: -0.16107279062271118
Batch 33/64 loss: -0.1450013518333435
Batch 34/64 loss: -0.17139089107513428
Batch 35/64 loss: -0.16464096307754517
Batch 36/64 loss: -0.18332350254058838
Batch 37/64 loss: -0.15523654222488403
Batch 38/64 loss: -0.16031867265701294
Batch 39/64 loss: -0.16046667098999023
Batch 40/64 loss: -0.13859975337982178
Batch 41/64 loss: -0.1796066164970398
Batch 42/64 loss: -0.1385434865951538
Batch 43/64 loss: -0.16809827089309692
Batch 44/64 loss: -0.17331063747406006
Batch 45/64 loss: -0.15707314014434814
Batch 46/64 loss: -0.16706764698028564
Batch 47/64 loss: -0.16437071561813354
Batch 48/64 loss: -0.14172911643981934
Batch 49/64 loss: -0.14293891191482544
Batch 50/64 loss: -0.1538744568824768
Batch 51/64 loss: -0.1444840431213379
Batch 52/64 loss: -0.17431241273880005
Batch 53/64 loss: -0.16163229942321777
Batch 54/64 loss: -0.12923932075500488
Batch 55/64 loss: -0.1435491442680359
Batch 56/64 loss: -0.15266120433807373
Batch 57/64 loss: -0.16302740573883057
Batch 58/64 loss: -0.1341627836227417
Batch 59/64 loss: -0.15821468830108643
Batch 60/64 loss: -0.16585350036621094
Batch 61/64 loss: -0.12798446416854858
Batch 62/64 loss: -0.15348470211029053
Batch 63/64 loss: -0.15918129682540894
Batch 64/64 loss: -0.14797842502593994
Epoch 338  Train loss: -0.16023178334329644  Val loss: -0.020504105336887322
Epoch 339
-------------------------------
Batch 1/64 loss: -0.14772075414657593
Batch 2/64 loss: -0.16946625709533691
Batch 3/64 loss: -0.16588228940963745
Batch 4/64 loss: -0.1684855818748474
Batch 5/64 loss: -0.1481989622116089
Batch 6/64 loss: -0.16502141952514648
Batch 7/64 loss: -0.17508196830749512
Batch 8/64 loss: -0.1775914430618286
Batch 9/64 loss: -0.14576321840286255
Batch 10/64 loss: -0.16475266218185425
Batch 11/64 loss: -0.15451538562774658
Batch 12/64 loss: -0.15062540769577026
Batch 13/64 loss: -0.16926664113998413
Batch 14/64 loss: -0.15326917171478271
Batch 15/64 loss: -0.13697469234466553
Batch 16/64 loss: -0.13421660661697388
Batch 17/64 loss: -0.16538310050964355
Batch 18/64 loss: -0.17086005210876465
Batch 19/64 loss: -0.15760338306427002
Batch 20/64 loss: -0.1647263765335083
Batch 21/64 loss: -0.1561570167541504
Batch 22/64 loss: -0.17609059810638428
Batch 23/64 loss: -0.16853630542755127
Batch 24/64 loss: -0.14573591947555542
Batch 25/64 loss: -0.1597745418548584
Batch 26/64 loss: -0.12494277954101562
Batch 27/64 loss: -0.15791994333267212
Batch 28/64 loss: -0.14966535568237305
Batch 29/64 loss: -0.1281713843345642
Batch 30/64 loss: -0.13754427433013916
Batch 31/64 loss: -0.15641987323760986
Batch 32/64 loss: -0.1733109951019287
Batch 33/64 loss: -0.16681009531021118
Batch 34/64 loss: -0.15224134922027588
Batch 35/64 loss: -0.1791272759437561
Batch 36/64 loss: -0.17297619581222534
Batch 37/64 loss: -0.13616418838500977
Batch 38/64 loss: -0.16208744049072266
Batch 39/64 loss: -0.14474833011627197
Batch 40/64 loss: -0.1470329761505127
Batch 41/64 loss: -0.16580653190612793
Batch 42/64 loss: -0.1416783332824707
Batch 43/64 loss: -0.16408854722976685
Batch 44/64 loss: -0.16223067045211792
Batch 45/64 loss: -0.16803884506225586
Batch 46/64 loss: -0.1781333088874817
Batch 47/64 loss: -0.1302708387374878
Batch 48/64 loss: -0.1544223427772522
Batch 49/64 loss: -0.15067559480667114
Batch 50/64 loss: -0.17195045948028564
Batch 51/64 loss: -0.15565764904022217
Batch 52/64 loss: -0.12015938758850098
Batch 53/64 loss: -0.1519150733947754
Batch 54/64 loss: -0.16953212022781372
Batch 55/64 loss: -0.15371239185333252
Batch 56/64 loss: -0.1533411741256714
Batch 57/64 loss: -0.1534537672996521
Batch 58/64 loss: -0.14635032415390015
Batch 59/64 loss: -0.17357569932937622
Batch 60/64 loss: -0.1619652509689331
Batch 61/64 loss: -0.17778551578521729
Batch 62/64 loss: -0.16437697410583496
Batch 63/64 loss: -0.14443045854568481
Batch 64/64 loss: -0.1306474208831787
Epoch 339  Train loss: -0.15674351430406758  Val loss: -0.021062168059070494
Epoch 340
-------------------------------
Batch 1/64 loss: -0.17487472295761108
Batch 2/64 loss: -0.1675230860710144
Batch 3/64 loss: -0.16355454921722412
Batch 4/64 loss: -0.1416894793510437
Batch 5/64 loss: -0.17489326000213623
Batch 6/64 loss: -0.13606637716293335
Batch 7/64 loss: -0.17118126153945923
Batch 8/64 loss: -0.17153120040893555
Batch 9/64 loss: -0.1593564748764038
Batch 10/64 loss: -0.15484601259231567
Batch 11/64 loss: -0.15142548084259033
Batch 12/64 loss: -0.15561449527740479
Batch 13/64 loss: -0.1447257399559021
Batch 14/64 loss: -0.16686004400253296
Batch 15/64 loss: -0.1544237732887268
Batch 16/64 loss: -0.16681843996047974
Batch 17/64 loss: -0.15516316890716553
Batch 18/64 loss: -0.1673959493637085
Batch 19/64 loss: -0.1880919337272644
Batch 20/64 loss: -0.16019463539123535
Batch 21/64 loss: -0.15718048810958862
Batch 22/64 loss: -0.17974406480789185
Batch 23/64 loss: -0.12224072217941284
Batch 24/64 loss: -0.17002081871032715
Batch 25/64 loss: -0.17042171955108643
Batch 26/64 loss: -0.1600666642189026
Batch 27/64 loss: -0.15545392036437988
Batch 28/64 loss: -0.17113083600997925
Batch 29/64 loss: -0.14746779203414917
Batch 30/64 loss: -0.17803579568862915
Batch 31/64 loss: -0.17970073223114014
Batch 32/64 loss: -0.160858154296875
Batch 33/64 loss: -0.1658642292022705
Batch 34/64 loss: -0.15820705890655518
Batch 35/64 loss: -0.1668710708618164
Batch 36/64 loss: -0.1825205683708191
Batch 37/64 loss: -0.15654516220092773
Batch 38/64 loss: -0.16295772790908813
Batch 39/64 loss: -0.12236142158508301
Batch 40/64 loss: -0.14770108461380005
Batch 41/64 loss: -0.15376567840576172
Batch 42/64 loss: -0.153395414352417
Batch 43/64 loss: -0.15953302383422852
Batch 44/64 loss: -0.16368305683135986
Batch 45/64 loss: -0.14145511388778687
Batch 46/64 loss: -0.15881425142288208
Batch 47/64 loss: -0.16368329524993896
Batch 48/64 loss: -0.09553784132003784
Batch 49/64 loss: -0.1573638916015625
Batch 50/64 loss: -0.1584467887878418
Batch 51/64 loss: -0.17099010944366455
Batch 52/64 loss: -0.13414764404296875
Batch 53/64 loss: -0.16218936443328857
Batch 54/64 loss: -0.14033710956573486
Batch 55/64 loss: -0.16803836822509766
Batch 56/64 loss: -0.16413676738739014
Batch 57/64 loss: -0.14844530820846558
Batch 58/64 loss: -0.16333580017089844
Batch 59/64 loss: -0.17265987396240234
Batch 60/64 loss: -0.14539778232574463
Batch 61/64 loss: -0.1396803855895996
Batch 62/64 loss: -0.14688760042190552
Batch 63/64 loss: -0.15224969387054443
Batch 64/64 loss: -0.1405593752861023
Epoch 340  Train loss: -0.15782233429890052  Val loss: -0.020373110304173735
Epoch 341
-------------------------------
Batch 1/64 loss: -0.15709459781646729
Batch 2/64 loss: -0.15287131071090698
Batch 3/64 loss: -0.12778329849243164
Batch 4/64 loss: -0.1604960560798645
Batch 5/64 loss: -0.17057687044143677
Batch 6/64 loss: -0.1273018717765808
Batch 7/64 loss: -0.15428590774536133
Batch 8/64 loss: -0.14907032251358032
Batch 9/64 loss: -0.15423041582107544
Batch 10/64 loss: -0.15986692905426025
Batch 11/64 loss: -0.1540047526359558
Batch 12/64 loss: -0.14374005794525146
Batch 13/64 loss: -0.17499083280563354
Batch 14/64 loss: -0.16801387071609497
Batch 15/64 loss: -0.17471617460250854
Batch 16/64 loss: -0.15127402544021606
Batch 17/64 loss: -0.14517122507095337
Batch 18/64 loss: -0.18799114227294922
Batch 19/64 loss: -0.14575326442718506
Batch 20/64 loss: -0.13565200567245483
Batch 21/64 loss: -0.13850808143615723
Batch 22/64 loss: -0.15764373540878296
Batch 23/64 loss: -0.14085936546325684
Batch 24/64 loss: -0.17030417919158936
Batch 25/64 loss: -0.16856682300567627
Batch 26/64 loss: -0.13929104804992676
Batch 27/64 loss: -0.157265305519104
Batch 28/64 loss: -0.16541343927383423
Batch 29/64 loss: -0.16310185194015503
Batch 30/64 loss: -0.1432083249092102
Batch 31/64 loss: -0.15896272659301758
Batch 32/64 loss: -0.16895699501037598
Batch 33/64 loss: -0.15991830825805664
Batch 34/64 loss: -0.15792661905288696
Batch 35/64 loss: -0.17086875438690186
Batch 36/64 loss: -0.16230624914169312
Batch 37/64 loss: -0.1473475694656372
Batch 38/64 loss: -0.17764371633529663
Batch 39/64 loss: -0.15179473161697388
Batch 40/64 loss: -0.18388831615447998
Batch 41/64 loss: -0.1550118327140808
Batch 42/64 loss: -0.16817909479141235
Batch 43/64 loss: -0.15815114974975586
Batch 44/64 loss: -0.18100637197494507
Batch 45/64 loss: -0.14822912216186523
Batch 46/64 loss: -0.15128111839294434
Batch 47/64 loss: -0.16782373189926147
Batch 48/64 loss: -0.1725931167602539
Batch 49/64 loss: -0.15131181478500366
Batch 50/64 loss: -0.15659570693969727
Batch 51/64 loss: -0.1358489990234375
Batch 52/64 loss: -0.16872096061706543
Batch 53/64 loss: -0.13500124216079712
Batch 54/64 loss: -0.15842938423156738
Batch 55/64 loss: -0.1761876344680786
Batch 56/64 loss: -0.14791280031204224
Batch 57/64 loss: -0.16311711072921753
Batch 58/64 loss: -0.14030730724334717
Batch 59/64 loss: -0.16276556253433228
Batch 60/64 loss: -0.15388834476470947
Batch 61/64 loss: -0.1601167917251587
Batch 62/64 loss: -0.17648208141326904
Batch 63/64 loss: -0.17490875720977783
Batch 64/64 loss: -0.14228379726409912
Epoch 341  Train loss: -0.15763519932242  Val loss: -0.021215989622463474
Epoch 342
-------------------------------
Batch 1/64 loss: -0.16966646909713745
Batch 2/64 loss: -0.136585533618927
Batch 3/64 loss: -0.15150803327560425
Batch 4/64 loss: -0.14462536573410034
Batch 5/64 loss: -0.14242106676101685
Batch 6/64 loss: -0.17862248420715332
Batch 7/64 loss: -0.1023331880569458
Batch 8/64 loss: -0.14435064792633057
Batch 9/64 loss: -0.16557490825653076
Batch 10/64 loss: -0.16742146015167236
Batch 11/64 loss: -0.1768423318862915
Batch 12/64 loss: -0.17263776063919067
Batch 13/64 loss: -0.13554304838180542
Batch 14/64 loss: -0.1557493805885315
Batch 15/64 loss: -0.16483581066131592
Batch 16/64 loss: -0.17457497119903564
Batch 17/64 loss: -0.1561020016670227
Batch 18/64 loss: -0.15037751197814941
Batch 19/64 loss: -0.17320650815963745
Batch 20/64 loss: -0.1815779209136963
Batch 21/64 loss: -0.14943253993988037
Batch 22/64 loss: -0.1594187617301941
Batch 23/64 loss: -0.17048287391662598
Batch 24/64 loss: -0.14203029870986938
Batch 25/64 loss: -0.17114758491516113
Batch 26/64 loss: -0.1488717794418335
Batch 27/64 loss: -0.17713046073913574
Batch 28/64 loss: -0.15887492895126343
Batch 29/64 loss: -0.141279399394989
Batch 30/64 loss: -0.16893893480300903
Batch 31/64 loss: -0.16868138313293457
Batch 32/64 loss: -0.15666139125823975
Batch 33/64 loss: -0.14779406785964966
Batch 34/64 loss: -0.15381544828414917
Batch 35/64 loss: -0.19088774919509888
Batch 36/64 loss: -0.12897515296936035
Batch 37/64 loss: -0.17146849632263184
Batch 38/64 loss: -0.14811432361602783
Batch 39/64 loss: -0.17035210132598877
Batch 40/64 loss: -0.16432690620422363
Batch 41/64 loss: -0.13256514072418213
Batch 42/64 loss: -0.17139577865600586
Batch 43/64 loss: -0.1637287735939026
Batch 44/64 loss: -0.16886061429977417
Batch 45/64 loss: -0.16772490739822388
Batch 46/64 loss: -0.14628154039382935
Batch 47/64 loss: -0.1483672857284546
Batch 48/64 loss: -0.17118674516677856
Batch 49/64 loss: -0.16284918785095215
Batch 50/64 loss: -0.17076319456100464
Batch 51/64 loss: -0.15210998058319092
Batch 52/64 loss: -0.13929849863052368
Batch 53/64 loss: -0.18561136722564697
Batch 54/64 loss: -0.1708107590675354
Batch 55/64 loss: -0.18426930904388428
Batch 56/64 loss: -0.16249382495880127
Batch 57/64 loss: -0.1697007417678833
Batch 58/64 loss: -0.16876816749572754
Batch 59/64 loss: -0.17150866985321045
Batch 60/64 loss: -0.1622607707977295
Batch 61/64 loss: -0.14497709274291992
Batch 62/64 loss: -0.16373437643051147
Batch 63/64 loss: -0.1503639817237854
Batch 64/64 loss: -0.14367830753326416
Epoch 342  Train loss: -0.15953930078768264  Val loss: -0.017443271641878737
Epoch 343
-------------------------------
Batch 1/64 loss: -0.16556531190872192
Batch 2/64 loss: -0.1540842056274414
Batch 3/64 loss: -0.1793796420097351
Batch 4/64 loss: -0.18008559942245483
Batch 5/64 loss: -0.16708660125732422
Batch 6/64 loss: -0.17196351289749146
Batch 7/64 loss: -0.1547808051109314
Batch 8/64 loss: -0.17871946096420288
Batch 9/64 loss: -0.1558995246887207
Batch 10/64 loss: -0.16355383396148682
Batch 11/64 loss: -0.1791917085647583
Batch 12/64 loss: -0.16228604316711426
Batch 13/64 loss: -0.18167901039123535
Batch 14/64 loss: -0.17199760675430298
Batch 15/64 loss: -0.16896921396255493
Batch 16/64 loss: -0.1378878951072693
Batch 17/64 loss: -0.14466357231140137
Batch 18/64 loss: -0.17243200540542603
Batch 19/64 loss: -0.16156166791915894
Batch 20/64 loss: -0.1726135015487671
Batch 21/64 loss: -0.15540796518325806
Batch 22/64 loss: -0.16177105903625488
Batch 23/64 loss: -0.17534089088439941
Batch 24/64 loss: -0.16633105278015137
Batch 25/64 loss: -0.16998106241226196
Batch 26/64 loss: -0.12137079238891602
Batch 27/64 loss: -0.14423847198486328
Batch 28/64 loss: -0.15126723051071167
Batch 29/64 loss: -0.1787707805633545
Batch 30/64 loss: -0.17614978551864624
Batch 31/64 loss: -0.133966863155365
Batch 32/64 loss: -0.16318416595458984
Batch 33/64 loss: -0.16652852296829224
Batch 34/64 loss: -0.16262036561965942
Batch 35/64 loss: -0.13746726512908936
Batch 36/64 loss: -0.14360857009887695
Batch 37/64 loss: -0.15598464012145996
Batch 38/64 loss: -0.17874330282211304
Batch 39/64 loss: -0.19077205657958984
Batch 40/64 loss: -0.15560954809188843
Batch 41/64 loss: -0.1429165005683899
Batch 42/64 loss: -0.17397820949554443
Batch 43/64 loss: -0.14630377292633057
Batch 44/64 loss: -0.15255451202392578
Batch 45/64 loss: -0.17655789852142334
Batch 46/64 loss: -0.15436381101608276
Batch 47/64 loss: -0.16492801904678345
Batch 48/64 loss: -0.16000878810882568
Batch 49/64 loss: -0.162081778049469
Batch 50/64 loss: -0.16966378688812256
Batch 51/64 loss: -0.15245527029037476
Batch 52/64 loss: -0.14747214317321777
Batch 53/64 loss: -0.14456802606582642
Batch 54/64 loss: -0.18377834558486938
Batch 55/64 loss: -0.1720273494720459
Batch 56/64 loss: -0.1593981385231018
Batch 57/64 loss: -0.15490561723709106
Batch 58/64 loss: -0.1299978494644165
Batch 59/64 loss: -0.15822988748550415
Batch 60/64 loss: -0.1505032181739807
Batch 61/64 loss: -0.1436792016029358
Batch 62/64 loss: -0.14772659540176392
Batch 63/64 loss: -0.16747194528579712
Batch 64/64 loss: -0.16421937942504883
Epoch 343  Train loss: -0.16081961276484472  Val loss: -0.020524663409006966
Epoch 344
-------------------------------
Batch 1/64 loss: -0.17120593786239624
Batch 2/64 loss: -0.16178488731384277
Batch 3/64 loss: -0.16049593687057495
Batch 4/64 loss: -0.14438164234161377
Batch 5/64 loss: -0.1799834966659546
Batch 6/64 loss: -0.15026241540908813
Batch 7/64 loss: -0.15925461053848267
Batch 8/64 loss: -0.1513800024986267
Batch 9/64 loss: -0.17852783203125
Batch 10/64 loss: -0.17213791608810425
Batch 11/64 loss: -0.16963577270507812
Batch 12/64 loss: -0.16359621286392212
Batch 13/64 loss: -0.15668100118637085
Batch 14/64 loss: -0.17009258270263672
Batch 15/64 loss: -0.14305967092514038
Batch 16/64 loss: -0.1582787036895752
Batch 17/64 loss: -0.16680270433425903
Batch 18/64 loss: -0.1474326252937317
Batch 19/64 loss: -0.15648597478866577
Batch 20/64 loss: -0.1598750352859497
Batch 21/64 loss: -0.1628856062889099
Batch 22/64 loss: -0.17472457885742188
Batch 23/64 loss: -0.16416722536087036
Batch 24/64 loss: -0.15413570404052734
Batch 25/64 loss: -0.17346835136413574
Batch 26/64 loss: -0.1839771866798401
Batch 27/64 loss: -0.1560213565826416
Batch 28/64 loss: -0.1593012809753418
Batch 29/64 loss: -0.15707093477249146
Batch 30/64 loss: -0.1633957028388977
Batch 31/64 loss: -0.1523391604423523
Batch 32/64 loss: -0.16885775327682495
Batch 33/64 loss: -0.18375784158706665
Batch 34/64 loss: -0.1806168556213379
Batch 35/64 loss: -0.16749584674835205
Batch 36/64 loss: -0.15762221813201904
Batch 37/64 loss: -0.08589166402816772
Batch 38/64 loss: -0.14147716760635376
Batch 39/64 loss: -0.16119754314422607
Batch 40/64 loss: -0.15975213050842285
Batch 41/64 loss: -0.1575877070426941
Batch 42/64 loss: -0.14288562536239624
Batch 43/64 loss: -0.1487014889717102
Batch 44/64 loss: -0.1502748727798462
Batch 45/64 loss: -0.16564375162124634
Batch 46/64 loss: -0.17248141765594482
Batch 47/64 loss: -0.1470944881439209
Batch 48/64 loss: -0.13774263858795166
Batch 49/64 loss: -0.16167515516281128
Batch 50/64 loss: -0.16043400764465332
Batch 51/64 loss: -0.11240261793136597
Batch 52/64 loss: -0.1461867094039917
Batch 53/64 loss: -0.14929145574569702
Batch 54/64 loss: -0.18151259422302246
Batch 55/64 loss: -0.16856539249420166
Batch 56/64 loss: -0.16405713558197021
Batch 57/64 loss: -0.1571817398071289
Batch 58/64 loss: -0.16095352172851562
Batch 59/64 loss: -0.14426153898239136
Batch 60/64 loss: -0.15427011251449585
Batch 61/64 loss: -0.16845965385437012
Batch 62/64 loss: -0.16844195127487183
Batch 63/64 loss: -0.17690390348434448
Batch 64/64 loss: -0.17176926136016846
Epoch 344  Train loss: -0.15929956856895897  Val loss: -0.01915875251350534
Epoch 345
-------------------------------
Batch 1/64 loss: -0.17818695306777954
Batch 2/64 loss: -0.16556841135025024
Batch 3/64 loss: -0.16265499591827393
Batch 4/64 loss: -0.17470437288284302
Batch 5/64 loss: -0.169256329536438
Batch 6/64 loss: -0.17898344993591309
Batch 7/64 loss: -0.16384994983673096
Batch 8/64 loss: -0.16730725765228271
Batch 9/64 loss: -0.17708492279052734
Batch 10/64 loss: -0.16837579011917114
Batch 11/64 loss: -0.14596575498580933
Batch 12/64 loss: -0.16474896669387817
Batch 13/64 loss: -0.14313149452209473
Batch 14/64 loss: -0.17125362157821655
Batch 15/64 loss: -0.1712893843650818
Batch 16/64 loss: -0.16861987113952637
Batch 17/64 loss: -0.18155932426452637
Batch 18/64 loss: -0.1841302514076233
Batch 19/64 loss: -0.18528521060943604
Batch 20/64 loss: -0.14408421516418457
Batch 21/64 loss: -0.1565570831298828
Batch 22/64 loss: -0.1447928547859192
Batch 23/64 loss: -0.14323019981384277
Batch 24/64 loss: -0.15896594524383545
Batch 25/64 loss: -0.15639615058898926
Batch 26/64 loss: -0.17744046449661255
Batch 27/64 loss: -0.15948617458343506
Batch 28/64 loss: -0.15191000699996948
Batch 29/64 loss: -0.16961252689361572
Batch 30/64 loss: -0.15121227502822876
Batch 31/64 loss: -0.16817635297775269
Batch 32/64 loss: -0.16903293132781982
Batch 33/64 loss: -0.16121363639831543
Batch 34/64 loss: -0.15529590845108032
Batch 35/64 loss: -0.17724603414535522
Batch 36/64 loss: -0.1431102752685547
Batch 37/64 loss: -0.1340312957763672
Batch 38/64 loss: -0.16506505012512207
Batch 39/64 loss: -0.14551568031311035
Batch 40/64 loss: -0.16428124904632568
Batch 41/64 loss: -0.1643826961517334
Batch 42/64 loss: -0.17701053619384766
Batch 43/64 loss: -0.15000927448272705
Batch 44/64 loss: -0.1603527069091797
Batch 45/64 loss: -0.16571927070617676
Batch 46/64 loss: -0.15809738636016846
Batch 47/64 loss: -0.17829596996307373
Batch 48/64 loss: -0.16265451908111572
Batch 49/64 loss: -0.15716737508773804
Batch 50/64 loss: -0.1632838249206543
Batch 51/64 loss: -0.1761782169342041
Batch 52/64 loss: -0.15350019931793213
Batch 53/64 loss: -0.16650068759918213
Batch 54/64 loss: -0.15324825048446655
Batch 55/64 loss: -0.16340923309326172
Batch 56/64 loss: -0.16053974628448486
Batch 57/64 loss: -0.1297338604927063
Batch 58/64 loss: -0.15493488311767578
Batch 59/64 loss: -0.16149628162384033
Batch 60/64 loss: -0.15955966711044312
Batch 61/64 loss: -0.16589581966400146
Batch 62/64 loss: -0.17372280359268188
Batch 63/64 loss: -0.15576857328414917
Batch 64/64 loss: -0.1393653154373169
Epoch 345  Train loss: -0.16211134732938282  Val loss: -0.019324245116964647
Epoch 346
-------------------------------
Batch 1/64 loss: -0.17706692218780518
Batch 2/64 loss: -0.18050146102905273
Batch 3/64 loss: -0.16660571098327637
Batch 4/64 loss: -0.1636948585510254
Batch 5/64 loss: -0.1561914086341858
Batch 6/64 loss: -0.17732298374176025
Batch 7/64 loss: -0.13177502155303955
Batch 8/64 loss: -0.15858465433120728
Batch 9/64 loss: -0.15118765830993652
Batch 10/64 loss: -0.16769760847091675
Batch 11/64 loss: -0.15570348501205444
Batch 12/64 loss: -0.15948432683944702
Batch 13/64 loss: -0.16362446546554565
Batch 14/64 loss: -0.13782083988189697
Batch 15/64 loss: -0.17144477367401123
Batch 16/64 loss: -0.16713905334472656
Batch 17/64 loss: -0.14516717195510864
Batch 18/64 loss: -0.13903743028640747
Batch 19/64 loss: -0.1410115361213684
Batch 20/64 loss: -0.1688227653503418
Batch 21/64 loss: -0.16932785511016846
Batch 22/64 loss: -0.1654435396194458
Batch 23/64 loss: -0.165702223777771
Batch 24/64 loss: -0.16659015417099
Batch 25/64 loss: -0.16813135147094727
Batch 26/64 loss: -0.15269923210144043
Batch 27/64 loss: -0.15214824676513672
Batch 28/64 loss: -0.12681758403778076
Batch 29/64 loss: -0.156383216381073
Batch 30/64 loss: -0.16859954595565796
Batch 31/64 loss: -0.1464741826057434
Batch 32/64 loss: -0.126578688621521
Batch 33/64 loss: -0.16019940376281738
Batch 34/64 loss: -0.15897458791732788
Batch 35/64 loss: -0.1622326374053955
Batch 36/64 loss: -0.1575014591217041
Batch 37/64 loss: -0.15202581882476807
Batch 38/64 loss: -0.1650276780128479
Batch 39/64 loss: -0.14848482608795166
Batch 40/64 loss: -0.16953742504119873
Batch 41/64 loss: -0.1632441282272339
Batch 42/64 loss: -0.13919854164123535
Batch 43/64 loss: -0.1649906039237976
Batch 44/64 loss: -0.1753394603729248
Batch 45/64 loss: -0.15622848272323608
Batch 46/64 loss: -0.16589486598968506
Batch 47/64 loss: -0.17954862117767334
Batch 48/64 loss: -0.16052782535552979
Batch 49/64 loss: -0.15080779790878296
Batch 50/64 loss: -0.16187626123428345
Batch 51/64 loss: -0.15263110399246216
Batch 52/64 loss: -0.17261844873428345
Batch 53/64 loss: -0.17510831356048584
Batch 54/64 loss: -0.17707937955856323
Batch 55/64 loss: -0.14760160446166992
Batch 56/64 loss: -0.13373732566833496
Batch 57/64 loss: -0.14485883712768555
Batch 58/64 loss: -0.15787136554718018
Batch 59/64 loss: -0.15579378604888916
Batch 60/64 loss: -0.16523998975753784
Batch 61/64 loss: -0.16973799467086792
Batch 62/64 loss: -0.14264637231826782
Batch 63/64 loss: -0.17181503772735596
Batch 64/64 loss: -0.15072757005691528
Epoch 346  Train loss: -0.1586855783182032  Val loss: -0.021570825290024485
Epoch 347
-------------------------------
Batch 1/64 loss: -0.17422014474868774
Batch 2/64 loss: -0.1765640377998352
Batch 3/64 loss: -0.16542565822601318
Batch 4/64 loss: -0.16666454076766968
Batch 5/64 loss: -0.18744605779647827
Batch 6/64 loss: -0.16631221771240234
Batch 7/64 loss: -0.15385150909423828
Batch 8/64 loss: -0.12134939432144165
Batch 9/64 loss: -0.16015297174453735
Batch 10/64 loss: -0.18682342767715454
Batch 11/64 loss: -0.17760157585144043
Batch 12/64 loss: -0.14266091585159302
Batch 13/64 loss: -0.1756088137626648
Batch 14/64 loss: -0.1750221848487854
Batch 15/64 loss: -0.14378416538238525
Batch 16/64 loss: -0.1788790225982666
Batch 17/64 loss: -0.18696588277816772
Batch 18/64 loss: -0.16113877296447754
Batch 19/64 loss: -0.17232590913772583
Batch 20/64 loss: -0.14547431468963623
Batch 21/64 loss: -0.1776716709136963
Batch 22/64 loss: -0.15910637378692627
Batch 23/64 loss: -0.16249394416809082
Batch 24/64 loss: -0.16072791814804077
Batch 25/64 loss: -0.1448047161102295
Batch 26/64 loss: -0.16237682104110718
Batch 27/64 loss: -0.1754138469696045
Batch 28/64 loss: -0.15313756465911865
Batch 29/64 loss: -0.16604864597320557
Batch 30/64 loss: -0.15648812055587769
Batch 31/64 loss: -0.16196978092193604
Batch 32/64 loss: -0.1574113368988037
Batch 33/64 loss: -0.1591598391532898
Batch 34/64 loss: -0.17910265922546387
Batch 35/64 loss: -0.1791122555732727
Batch 36/64 loss: -0.1763075590133667
Batch 37/64 loss: -0.15871310234069824
Batch 38/64 loss: -0.17376506328582764
Batch 39/64 loss: -0.15682566165924072
Batch 40/64 loss: -0.17573648691177368
Batch 41/64 loss: -0.17286044359207153
Batch 42/64 loss: -0.15417689085006714
Batch 43/64 loss: -0.16829979419708252
Batch 44/64 loss: -0.14957356452941895
Batch 45/64 loss: -0.1696871519088745
Batch 46/64 loss: -0.17245304584503174
Batch 47/64 loss: -0.16150373220443726
Batch 48/64 loss: -0.1378542184829712
Batch 49/64 loss: -0.18767046928405762
Batch 50/64 loss: -0.14994454383850098
Batch 51/64 loss: -0.14280962944030762
Batch 52/64 loss: -0.16709816455841064
Batch 53/64 loss: -0.17898929119110107
Batch 54/64 loss: -0.15369820594787598
Batch 55/64 loss: -0.12755507230758667
Batch 56/64 loss: -0.15720146894454956
Batch 57/64 loss: -0.16420871019363403
Batch 58/64 loss: -0.15645325183868408
Batch 59/64 loss: -0.15473121404647827
Batch 60/64 loss: -0.13961827754974365
Batch 61/64 loss: -0.17931580543518066
Batch 62/64 loss: -0.17517268657684326
Batch 63/64 loss: -0.18396365642547607
Batch 64/64 loss: -0.1723058819770813
Epoch 347  Train loss: -0.16390138958014694  Val loss: -0.02094505660722346
Epoch 348
-------------------------------
Batch 1/64 loss: -0.1675449013710022
Batch 2/64 loss: -0.18124842643737793
Batch 3/64 loss: -0.18192237615585327
Batch 4/64 loss: -0.13658851385116577
Batch 5/64 loss: -0.17025357484817505
Batch 6/64 loss: -0.16552996635437012
Batch 7/64 loss: -0.18260031938552856
Batch 8/64 loss: -0.18461596965789795
Batch 9/64 loss: -0.17433923482894897
Batch 10/64 loss: -0.18967115879058838
Batch 11/64 loss: -0.14748913049697876
Batch 12/64 loss: -0.16567867994308472
Batch 13/64 loss: -0.1590377688407898
Batch 14/64 loss: -0.18011009693145752
Batch 15/64 loss: -0.15998196601867676
Batch 16/64 loss: -0.16290152072906494
Batch 17/64 loss: -0.15405285358428955
Batch 18/64 loss: -0.18078923225402832
Batch 19/64 loss: -0.15993309020996094
Batch 20/64 loss: -0.177110493183136
Batch 21/64 loss: -0.150851309299469
Batch 22/64 loss: -0.14732974767684937
Batch 23/64 loss: -0.15880346298217773
Batch 24/64 loss: -0.16200876235961914
Batch 25/64 loss: -0.1429653763771057
Batch 26/64 loss: -0.16138935089111328
Batch 27/64 loss: -0.1558193564414978
Batch 28/64 loss: -0.1625247597694397
Batch 29/64 loss: -0.17027390003204346
Batch 30/64 loss: -0.14692550897598267
Batch 31/64 loss: -0.14296913146972656
Batch 32/64 loss: -0.10611152648925781
Batch 33/64 loss: -0.16761130094528198
Batch 34/64 loss: -0.1549290418624878
Batch 35/64 loss: -0.16331720352172852
Batch 36/64 loss: -0.11852741241455078
Batch 37/64 loss: -0.15092098712921143
Batch 38/64 loss: -0.16025543212890625
Batch 39/64 loss: -0.15585726499557495
Batch 40/64 loss: -0.17324942350387573
Batch 41/64 loss: -0.1449507474899292
Batch 42/64 loss: -0.15814393758773804
Batch 43/64 loss: -0.17026662826538086
Batch 44/64 loss: -0.1517602801322937
Batch 45/64 loss: -0.17578125
Batch 46/64 loss: -0.15410035848617554
Batch 47/64 loss: -0.17111492156982422
Batch 48/64 loss: -0.15014958381652832
Batch 49/64 loss: -0.15318113565444946
Batch 50/64 loss: -0.1310892105102539
Batch 51/64 loss: -0.14360809326171875
Batch 52/64 loss: -0.17804491519927979
Batch 53/64 loss: -0.15378016233444214
Batch 54/64 loss: -0.14274024963378906
Batch 55/64 loss: -0.17424213886260986
Batch 56/64 loss: -0.16035056114196777
Batch 57/64 loss: -0.1923149824142456
Batch 58/64 loss: -0.14581984281539917
Batch 59/64 loss: -0.17574560642242432
Batch 60/64 loss: -0.16615253686904907
Batch 61/64 loss: -0.15310704708099365
Batch 62/64 loss: -0.163712739944458
Batch 63/64 loss: -0.1647537350654602
Batch 64/64 loss: -0.15223640203475952
Epoch 348  Train loss: -0.16036278430153342  Val loss: -0.017822489705692043
Epoch 349
-------------------------------
Batch 1/64 loss: -0.14706647396087646
Batch 2/64 loss: -0.17771804332733154
Batch 3/64 loss: -0.1776832938194275
Batch 4/64 loss: -0.17228978872299194
Batch 5/64 loss: -0.1714380979537964
Batch 6/64 loss: -0.1639333963394165
Batch 7/64 loss: -0.1571974754333496
Batch 8/64 loss: -0.17077839374542236
Batch 9/64 loss: -0.13073575496673584
Batch 10/64 loss: -0.1376664638519287
Batch 11/64 loss: -0.1387927532196045
Batch 12/64 loss: -0.15251904726028442
Batch 13/64 loss: -0.1710270643234253
Batch 14/64 loss: -0.15302765369415283
Batch 15/64 loss: -0.1627880334854126
Batch 16/64 loss: -0.18051844835281372
Batch 17/64 loss: -0.17092770338058472
Batch 18/64 loss: -0.1723211407661438
Batch 19/64 loss: -0.16684305667877197
Batch 20/64 loss: -0.13243430852890015
Batch 21/64 loss: -0.16931015253067017
Batch 22/64 loss: -0.16742444038391113
Batch 23/64 loss: -0.16544407606124878
Batch 24/64 loss: -0.17060816287994385
Batch 25/64 loss: -0.1670277714729309
Batch 26/64 loss: -0.16771358251571655
Batch 27/64 loss: -0.16667324304580688
Batch 28/64 loss: -0.14708107709884644
Batch 29/64 loss: -0.11444371938705444
Batch 30/64 loss: -0.11887645721435547
Batch 31/64 loss: -0.15355420112609863
Batch 32/64 loss: -0.1720139980316162
Batch 33/64 loss: -0.16820406913757324
Batch 34/64 loss: -0.17465627193450928
Batch 35/64 loss: -0.15872400999069214
Batch 36/64 loss: -0.14715975522994995
Batch 37/64 loss: -0.15176856517791748
Batch 38/64 loss: -0.17913496494293213
Batch 39/64 loss: -0.17192822694778442
Batch 40/64 loss: -0.15489554405212402
Batch 41/64 loss: -0.15160059928894043
Batch 42/64 loss: -0.14187544584274292
Batch 43/64 loss: -0.15128624439239502
Batch 44/64 loss: -0.1651214361190796
Batch 45/64 loss: -0.15913599729537964
Batch 46/64 loss: -0.1638736128807068
Batch 47/64 loss: -0.1766391396522522
Batch 48/64 loss: -0.16634124517440796
Batch 49/64 loss: -0.17735636234283447
Batch 50/64 loss: -0.18673259019851685
Batch 51/64 loss: -0.17787086963653564
Batch 52/64 loss: -0.14880865812301636
Batch 53/64 loss: -0.1643589735031128
Batch 54/64 loss: -0.15349674224853516
Batch 55/64 loss: -0.16915971040725708
Batch 56/64 loss: -0.1815750002861023
Batch 57/64 loss: -0.1688184142112732
Batch 58/64 loss: -0.14745759963989258
Batch 59/64 loss: -0.15494239330291748
Batch 60/64 loss: -0.15398013591766357
Batch 61/64 loss: -0.18041718006134033
Batch 62/64 loss: -0.15190690755844116
Batch 63/64 loss: -0.15735477209091187
Batch 64/64 loss: -0.161024272441864
Epoch 349  Train loss: -0.16102316730162677  Val loss: -0.01901706059773763
Epoch 350
-------------------------------
Batch 1/64 loss: -0.16595977544784546
Batch 2/64 loss: -0.17011785507202148
Batch 3/64 loss: -0.15751439332962036
Batch 4/64 loss: -0.11910325288772583
Batch 5/64 loss: -0.1565101146697998
Batch 6/64 loss: -0.17776429653167725
Batch 7/64 loss: -0.16453218460083008
Batch 8/64 loss: -0.1752757430076599
Batch 9/64 loss: -0.15142327547073364
Batch 10/64 loss: -0.15857595205307007
Batch 11/64 loss: -0.1523776650428772
Batch 12/64 loss: -0.1581665277481079
Batch 13/64 loss: -0.15640461444854736
Batch 14/64 loss: -0.17284202575683594
Batch 15/64 loss: -0.15745937824249268
Batch 16/64 loss: -0.1751459836959839
Batch 17/64 loss: -0.17454767227172852
Batch 18/64 loss: -0.17942404747009277
Batch 19/64 loss: -0.12950855493545532
Batch 20/64 loss: -0.15827524662017822
Batch 21/64 loss: -0.13014113903045654
Batch 22/64 loss: -0.1701679229736328
Batch 23/64 loss: -0.16592741012573242
Batch 24/64 loss: -0.144087553024292
Batch 25/64 loss: -0.14142245054244995
Batch 26/64 loss: -0.16411328315734863
Batch 27/64 loss: -0.17095017433166504
Batch 28/64 loss: -0.1282373070716858
Batch 29/64 loss: -0.17152678966522217
Batch 30/64 loss: -0.16752058267593384
Batch 31/64 loss: -0.13872241973876953
Batch 32/64 loss: -0.1890629529953003
Batch 33/64 loss: -0.16294598579406738
Batch 34/64 loss: -0.1780511736869812
Batch 35/64 loss: -0.16888606548309326
Batch 36/64 loss: -0.1623924970626831
Batch 37/64 loss: -0.15068596601486206
Batch 38/64 loss: -0.15270131826400757
Batch 39/64 loss: -0.14331740140914917
Batch 40/64 loss: -0.17196178436279297
Batch 41/64 loss: -0.1589452028274536
Batch 42/64 loss: -0.176791250705719
Batch 43/64 loss: -0.1267397403717041
Batch 44/64 loss: -0.15572166442871094
Batch 45/64 loss: -0.15649807453155518
Batch 46/64 loss: -0.15803009271621704
Batch 47/64 loss: -0.16325998306274414
Batch 48/64 loss: -0.15112173557281494
Batch 49/64 loss: -0.17398792505264282
Batch 50/64 loss: -0.15908735990524292
Batch 51/64 loss: -0.17753171920776367
Batch 52/64 loss: -0.17235243320465088
Batch 53/64 loss: -0.16150766611099243
Batch 54/64 loss: -0.16911840438842773
Batch 55/64 loss: -0.17228692770004272
Batch 56/64 loss: -0.18717670440673828
Batch 57/64 loss: -0.1674225926399231
Batch 58/64 loss: -0.1714160442352295
Batch 59/64 loss: -0.15084421634674072
Batch 60/64 loss: -0.16668701171875
Batch 61/64 loss: -0.1581093668937683
Batch 62/64 loss: -0.15604650974273682
Batch 63/64 loss: -0.18593144416809082
Batch 64/64 loss: -0.17410308122634888
Epoch 350  Train loss: -0.1614265430207346  Val loss: -0.02311817477249198
Epoch 351
-------------------------------
Batch 1/64 loss: -0.15510982275009155
Batch 2/64 loss: -0.13136804103851318
Batch 3/64 loss: -0.15760654211044312
Batch 4/64 loss: -0.1743021011352539
Batch 5/64 loss: -0.1613903045654297
Batch 6/64 loss: -0.1609095335006714
Batch 7/64 loss: -0.1507388949394226
Batch 8/64 loss: -0.15803754329681396
Batch 9/64 loss: -0.17309975624084473
Batch 10/64 loss: -0.16201817989349365
Batch 11/64 loss: -0.16677194833755493
Batch 12/64 loss: -0.16432863473892212
Batch 13/64 loss: -0.1803983449935913
Batch 14/64 loss: -0.16411375999450684
Batch 15/64 loss: -0.15916019678115845
Batch 16/64 loss: -0.16035234928131104
Batch 17/64 loss: -0.1720326542854309
Batch 18/64 loss: -0.14737725257873535
Batch 19/64 loss: -0.1786421537399292
Batch 20/64 loss: -0.16590654850006104
Batch 21/64 loss: -0.15581965446472168
Batch 22/64 loss: -0.15198761224746704
Batch 23/64 loss: -0.15725910663604736
Batch 24/64 loss: -0.166656494140625
Batch 25/64 loss: -0.17489582300186157
Batch 26/64 loss: -0.17283415794372559
Batch 27/64 loss: -0.16218602657318115
Batch 28/64 loss: -0.17236506938934326
Batch 29/64 loss: -0.16666388511657715
Batch 30/64 loss: -0.1805657148361206
Batch 31/64 loss: -0.14292728900909424
Batch 32/64 loss: -0.16996169090270996
Batch 33/64 loss: -0.1272139549255371
Batch 34/64 loss: -0.16781914234161377
Batch 35/64 loss: -0.18492013216018677
Batch 36/64 loss: -0.16639560461044312
Batch 37/64 loss: -0.15344327688217163
Batch 38/64 loss: -0.1715031862258911
Batch 39/64 loss: -0.1496671438217163
Batch 40/64 loss: -0.17214101552963257
Batch 41/64 loss: -0.16736602783203125
Batch 42/64 loss: -0.17220056056976318
Batch 43/64 loss: -0.1788204312324524
Batch 44/64 loss: -0.17262762784957886
Batch 45/64 loss: -0.15968847274780273
Batch 46/64 loss: -0.14720690250396729
Batch 47/64 loss: -0.17051821947097778
Batch 48/64 loss: -0.18135148286819458
Batch 49/64 loss: -0.16125011444091797
Batch 50/64 loss: -0.1492992639541626
Batch 51/64 loss: -0.13452553749084473
Batch 52/64 loss: -0.17056584358215332
Batch 53/64 loss: -0.1710425615310669
Batch 54/64 loss: -0.16773319244384766
Batch 55/64 loss: -0.19076323509216309
Batch 56/64 loss: -0.17771553993225098
Batch 57/64 loss: -0.1614275574684143
Batch 58/64 loss: -0.14027023315429688
Batch 59/64 loss: -0.15730899572372437
Batch 60/64 loss: -0.1597672700881958
Batch 61/64 loss: -0.1570119857788086
Batch 62/64 loss: -0.1575900912284851
Batch 63/64 loss: -0.1620749831199646
Batch 64/64 loss: -0.17629271745681763
Epoch 351  Train loss: -0.16331350920247098  Val loss: -0.018687417007393854
Epoch 352
-------------------------------
Batch 1/64 loss: -0.14909321069717407
Batch 2/64 loss: -0.1547609567642212
Batch 3/64 loss: -0.1886489987373352
Batch 4/64 loss: -0.17834222316741943
Batch 5/64 loss: -0.15964823961257935
Batch 6/64 loss: -0.16461384296417236
Batch 7/64 loss: -0.1561642289161682
Batch 8/64 loss: -0.18420535326004028
Batch 9/64 loss: -0.17398858070373535
Batch 10/64 loss: -0.1590997576713562
Batch 11/64 loss: -0.17007291316986084
Batch 12/64 loss: -0.1544298529624939
Batch 13/64 loss: -0.1488100290298462
Batch 14/64 loss: -0.14447641372680664
Batch 15/64 loss: -0.15582382678985596
Batch 16/64 loss: -0.18225151300430298
Batch 17/64 loss: -0.18245744705200195
Batch 18/64 loss: -0.1616959571838379
Batch 19/64 loss: -0.16516661643981934
Batch 20/64 loss: -0.16081613302230835
Batch 21/64 loss: -0.17955034971237183
Batch 22/64 loss: -0.1649872064590454
Batch 23/64 loss: -0.15592384338378906
Batch 24/64 loss: -0.16701984405517578
Batch 25/64 loss: -0.17195457220077515
Batch 26/64 loss: -0.18655341863632202
Batch 27/64 loss: -0.13633668422698975
Batch 28/64 loss: -0.17960882186889648
Batch 29/64 loss: -0.1791006326675415
Batch 30/64 loss: -0.16671061515808105
Batch 31/64 loss: -0.17881011962890625
Batch 32/64 loss: -0.16182953119277954
Batch 33/64 loss: -0.15097463130950928
Batch 34/64 loss: -0.17013317346572876
Batch 35/64 loss: -0.15506350994110107
Batch 36/64 loss: -0.15831387042999268
Batch 37/64 loss: -0.16799390316009521
Batch 38/64 loss: -0.18392318487167358
Batch 39/64 loss: -0.15748631954193115
Batch 40/64 loss: -0.16928791999816895
Batch 41/64 loss: -0.16532808542251587
Batch 42/64 loss: -0.14794045686721802
Batch 43/64 loss: -0.14931952953338623
Batch 44/64 loss: -0.166934072971344
Batch 45/64 loss: -0.17960214614868164
Batch 46/64 loss: -0.17614221572875977
Batch 47/64 loss: -0.1501656174659729
Batch 48/64 loss: -0.16249459981918335
Batch 49/64 loss: -0.17447638511657715
Batch 50/64 loss: -0.18329298496246338
Batch 51/64 loss: -0.1685125231742859
Batch 52/64 loss: -0.15293657779693604
Batch 53/64 loss: -0.16447323560714722
Batch 54/64 loss: -0.15176767110824585
Batch 55/64 loss: -0.16617053747177124
Batch 56/64 loss: -0.12652790546417236
Batch 57/64 loss: -0.14201337099075317
Batch 58/64 loss: -0.1665942668914795
Batch 59/64 loss: -0.17250430583953857
Batch 60/64 loss: -0.15725737810134888
Batch 61/64 loss: -0.14854145050048828
Batch 62/64 loss: -0.17365586757659912
Batch 63/64 loss: -0.179038405418396
Batch 64/64 loss: -0.15979284048080444
Epoch 352  Train loss: -0.16441823477838555  Val loss: -0.02005150678641198
Epoch 353
-------------------------------
Batch 1/64 loss: -0.16807866096496582
Batch 2/64 loss: -0.17400354146957397
Batch 3/64 loss: -0.1730629801750183
Batch 4/64 loss: -0.17117702960968018
Batch 5/64 loss: -0.18317222595214844
Batch 6/64 loss: -0.17481279373168945
Batch 7/64 loss: -0.15555888414382935
Batch 8/64 loss: -0.15306299924850464
Batch 9/64 loss: -0.15887773036956787
Batch 10/64 loss: -0.13752037286758423
Batch 11/64 loss: -0.15668171644210815
Batch 12/64 loss: -0.16683459281921387
Batch 13/64 loss: -0.1599283218383789
Batch 14/64 loss: -0.1710379719734192
Batch 15/64 loss: -0.1858818531036377
Batch 16/64 loss: -0.16001206636428833
Batch 17/64 loss: -0.1555469036102295
Batch 18/64 loss: -0.16374677419662476
Batch 19/64 loss: -0.17715513706207275
Batch 20/64 loss: -0.15855073928833008
Batch 21/64 loss: -0.1797528862953186
Batch 22/64 loss: -0.1853247880935669
Batch 23/64 loss: -0.1586870551109314
Batch 24/64 loss: -0.18432366847991943
Batch 25/64 loss: -0.13316166400909424
Batch 26/64 loss: -0.18843108415603638
Batch 27/64 loss: -0.17335867881774902
Batch 28/64 loss: -0.16113340854644775
Batch 29/64 loss: -0.14276188611984253
Batch 30/64 loss: -0.15827441215515137
Batch 31/64 loss: -0.12216514348983765
Batch 32/64 loss: -0.18036597967147827
Batch 33/64 loss: -0.1641562581062317
Batch 34/64 loss: -0.1853123903274536
Batch 35/64 loss: -0.14033979177474976
Batch 36/64 loss: -0.17511272430419922
Batch 37/64 loss: -0.15542352199554443
Batch 38/64 loss: -0.18438339233398438
Batch 39/64 loss: -0.1842469573020935
Batch 40/64 loss: -0.15789175033569336
Batch 41/64 loss: -0.17076921463012695
Batch 42/64 loss: -0.16440117359161377
Batch 43/64 loss: -0.15315920114517212
Batch 44/64 loss: -0.12697577476501465
Batch 45/64 loss: -0.1630820631980896
Batch 46/64 loss: -0.1881529688835144
Batch 47/64 loss: -0.1710032820701599
Batch 48/64 loss: -0.17483574151992798
Batch 49/64 loss: -0.17560815811157227
Batch 50/64 loss: -0.16908442974090576
Batch 51/64 loss: -0.16637158393859863
Batch 52/64 loss: -0.16684424877166748
Batch 53/64 loss: -0.1717623472213745
Batch 54/64 loss: -0.1745191216468811
Batch 55/64 loss: -0.147841215133667
Batch 56/64 loss: -0.16573572158813477
Batch 57/64 loss: -0.1528860330581665
Batch 58/64 loss: -0.14068186283111572
Batch 59/64 loss: -0.1805281639099121
Batch 60/64 loss: -0.14776313304901123
Batch 61/64 loss: -0.1703026294708252
Batch 62/64 loss: -0.1664881706237793
Batch 63/64 loss: -0.17474526166915894
Batch 64/64 loss: -0.13824689388275146
Epoch 353  Train loss: -0.16480842992371203  Val loss: -0.018992419095383475
Epoch 354
-------------------------------
Batch 1/64 loss: -0.19183379411697388
Batch 2/64 loss: -0.19308990240097046
Batch 3/64 loss: -0.1807539463043213
Batch 4/64 loss: -0.16293197870254517
Batch 5/64 loss: -0.17245244979858398
Batch 6/64 loss: -0.15411925315856934
Batch 7/64 loss: -0.1635723114013672
Batch 8/64 loss: -0.15550893545150757
Batch 9/64 loss: -0.16224855184555054
Batch 10/64 loss: -0.16741764545440674
Batch 11/64 loss: -0.16790252923965454
Batch 12/64 loss: -0.16107678413391113
Batch 13/64 loss: -0.1761007308959961
Batch 14/64 loss: -0.17776042222976685
Batch 15/64 loss: -0.16683602333068848
Batch 16/64 loss: -0.18032008409500122
Batch 17/64 loss: -0.18639057874679565
Batch 18/64 loss: -0.14875370264053345
Batch 19/64 loss: -0.17314839363098145
Batch 20/64 loss: -0.17428815364837646
Batch 21/64 loss: -0.1604245901107788
Batch 22/64 loss: -0.1623302698135376
Batch 23/64 loss: -0.1644895076751709
Batch 24/64 loss: -0.17498773336410522
Batch 25/64 loss: -0.1870139241218567
Batch 26/64 loss: -0.16386544704437256
Batch 27/64 loss: -0.15183770656585693
Batch 28/64 loss: -0.16947674751281738
Batch 29/64 loss: -0.16510158777236938
Batch 30/64 loss: -0.17166101932525635
Batch 31/64 loss: -0.16301250457763672
Batch 32/64 loss: -0.14533782005310059
Batch 33/64 loss: -0.1583040952682495
Batch 34/64 loss: -0.1599726676940918
Batch 35/64 loss: -0.1431572437286377
Batch 36/64 loss: -0.15896815061569214
Batch 37/64 loss: -0.17981529235839844
Batch 38/64 loss: -0.17744892835617065
Batch 39/64 loss: -0.16274642944335938
Batch 40/64 loss: -0.17619436979293823
Batch 41/64 loss: -0.15856939554214478
Batch 42/64 loss: -0.15738695859909058
Batch 43/64 loss: -0.13016343116760254
Batch 44/64 loss: -0.1553477644920349
Batch 45/64 loss: -0.14259552955627441
Batch 46/64 loss: -0.14362388849258423
Batch 47/64 loss: -0.17185568809509277
Batch 48/64 loss: -0.15732347965240479
Batch 49/64 loss: -0.17116975784301758
Batch 50/64 loss: -0.146511971950531
Batch 51/64 loss: -0.14364421367645264
Batch 52/64 loss: -0.1495954990386963
Batch 53/64 loss: -0.15719211101531982
Batch 54/64 loss: -0.15664327144622803
Batch 55/64 loss: -0.18316864967346191
Batch 56/64 loss: -0.15850597620010376
Batch 57/64 loss: -0.16841071844100952
Batch 58/64 loss: -0.14423656463623047
Batch 59/64 loss: -0.15805602073669434
Batch 60/64 loss: -0.15794163942337036
Batch 61/64 loss: -0.17300164699554443
Batch 62/64 loss: -0.15164393186569214
Batch 63/64 loss: -0.18605005741119385
Batch 64/64 loss: -0.15247654914855957
Epoch 354  Train loss: -0.1639160436742446  Val loss: -0.019949145128636835
Epoch 355
-------------------------------
Batch 1/64 loss: -0.14144372940063477
Batch 2/64 loss: -0.18459486961364746
Batch 3/64 loss: -0.13892793655395508
Batch 4/64 loss: -0.1537085771560669
Batch 5/64 loss: -0.16426801681518555
Batch 6/64 loss: -0.16475093364715576
Batch 7/64 loss: -0.16181159019470215
Batch 8/64 loss: -0.17649418115615845
Batch 9/64 loss: -0.15623116493225098
Batch 10/64 loss: -0.16324955224990845
Batch 11/64 loss: -0.15384846925735474
Batch 12/64 loss: -0.16538971662521362
Batch 13/64 loss: -0.17485374212265015
Batch 14/64 loss: -0.17428213357925415
Batch 15/64 loss: -0.15883493423461914
Batch 16/64 loss: -0.16295349597930908
Batch 17/64 loss: -0.17355334758758545
Batch 18/64 loss: -0.16252601146697998
Batch 19/64 loss: -0.17822343111038208
Batch 20/64 loss: -0.16205185651779175
Batch 21/64 loss: -0.17830103635787964
Batch 22/64 loss: -0.17493116855621338
Batch 23/64 loss: -0.1492154598236084
Batch 24/64 loss: -0.16735345125198364
Batch 25/64 loss: -0.1563173532485962
Batch 26/64 loss: -0.17351436614990234
Batch 27/64 loss: -0.16224104166030884
Batch 28/64 loss: -0.17811310291290283
Batch 29/64 loss: -0.1617063283920288
Batch 30/64 loss: -0.1844867467880249
Batch 31/64 loss: -0.18232595920562744
Batch 32/64 loss: -0.15484577417373657
Batch 33/64 loss: -0.17690753936767578
Batch 34/64 loss: -0.15741407871246338
Batch 35/64 loss: -0.1363939642906189
Batch 36/64 loss: -0.1838356852531433
Batch 37/64 loss: -0.1612887978553772
Batch 38/64 loss: -0.14176416397094727
Batch 39/64 loss: -0.15745937824249268
Batch 40/64 loss: -0.13706940412521362
Batch 41/64 loss: -0.16489481925964355
Batch 42/64 loss: -0.16538703441619873
Batch 43/64 loss: -0.1633615493774414
Batch 44/64 loss: -0.15533816814422607
Batch 45/64 loss: -0.12921583652496338
Batch 46/64 loss: -0.1594906449317932
Batch 47/64 loss: -0.15308725833892822
Batch 48/64 loss: -0.15803319215774536
Batch 49/64 loss: -0.15745216608047485
Batch 50/64 loss: -0.16426974534988403
Batch 51/64 loss: -0.18249458074569702
Batch 52/64 loss: -0.15480566024780273
Batch 53/64 loss: -0.1613527536392212
Batch 54/64 loss: -0.18524789810180664
Batch 55/64 loss: -0.1704801321029663
Batch 56/64 loss: -0.15110653638839722
Batch 57/64 loss: -0.16524547338485718
Batch 58/64 loss: -0.14135277271270752
Batch 59/64 loss: -0.18355494737625122
Batch 60/64 loss: -0.16518890857696533
Batch 61/64 loss: -0.15563321113586426
Batch 62/64 loss: -0.16101312637329102
Batch 63/64 loss: -0.17136824131011963
Batch 64/64 loss: -0.17812323570251465
Epoch 355  Train loss: -0.16314430704303817  Val loss: -0.01793645050927126
Epoch 356
-------------------------------
Batch 1/64 loss: -0.15602463483810425
Batch 2/64 loss: -0.1760442852973938
Batch 3/64 loss: -0.16613686084747314
Batch 4/64 loss: -0.151605486869812
Batch 5/64 loss: -0.16426652669906616
Batch 6/64 loss: -0.1426061987876892
Batch 7/64 loss: -0.18222546577453613
Batch 8/64 loss: -0.16463464498519897
Batch 9/64 loss: -0.19009548425674438
Batch 10/64 loss: -0.1895124316215515
Batch 11/64 loss: -0.1827525496482849
Batch 12/64 loss: -0.18212270736694336
Batch 13/64 loss: -0.15355747938156128
Batch 14/64 loss: -0.16504210233688354
Batch 15/64 loss: -0.15753722190856934
Batch 16/64 loss: -0.16885346174240112
Batch 17/64 loss: -0.17614173889160156
Batch 18/64 loss: -0.15966641902923584
Batch 19/64 loss: -0.16212791204452515
Batch 20/64 loss: -0.1699080467224121
Batch 21/64 loss: -0.15444821119308472
Batch 22/64 loss: -0.17635715007781982
Batch 23/64 loss: -0.1450592279434204
Batch 24/64 loss: -0.19744658470153809
Batch 25/64 loss: -0.14280807971954346
Batch 26/64 loss: -0.18180996179580688
Batch 27/64 loss: -0.17483681440353394
Batch 28/64 loss: -0.18801534175872803
Batch 29/64 loss: -0.18954145908355713
Batch 30/64 loss: -0.16977274417877197
Batch 31/64 loss: -0.17133742570877075
Batch 32/64 loss: -0.18044257164001465
Batch 33/64 loss: -0.18319588899612427
Batch 34/64 loss: -0.18351984024047852
Batch 35/64 loss: -0.1472022533416748
Batch 36/64 loss: -0.14822232723236084
Batch 37/64 loss: -0.17015433311462402
Batch 38/64 loss: -0.15102559328079224
Batch 39/64 loss: -0.14777815341949463
Batch 40/64 loss: -0.1512412428855896
Batch 41/64 loss: -0.1567516326904297
Batch 42/64 loss: -0.13248515129089355
Batch 43/64 loss: -0.16468346118927002
Batch 44/64 loss: -0.16573399305343628
Batch 45/64 loss: -0.14684420824050903
Batch 46/64 loss: -0.17622089385986328
Batch 47/64 loss: -0.17615997791290283
Batch 48/64 loss: -0.1733514666557312
Batch 49/64 loss: -0.15518200397491455
Batch 50/64 loss: -0.15064573287963867
Batch 51/64 loss: -0.18221604824066162
Batch 52/64 loss: -0.1444772481918335
Batch 53/64 loss: -0.17188888788223267
Batch 54/64 loss: -0.1670582890510559
Batch 55/64 loss: -0.16290533542633057
Batch 56/64 loss: -0.16316640377044678
Batch 57/64 loss: -0.1517878770828247
Batch 58/64 loss: -0.16157084703445435
Batch 59/64 loss: -0.17232877016067505
Batch 60/64 loss: -0.19134736061096191
Batch 61/64 loss: -0.1768442988395691
Batch 62/64 loss: -0.16462469100952148
Batch 63/64 loss: -0.1620386838912964
Batch 64/64 loss: -0.14779150485992432
Epoch 356  Train loss: -0.16621543145647236  Val loss: -0.018482190022353864
Epoch 357
-------------------------------
Batch 1/64 loss: -0.18610167503356934
Batch 2/64 loss: -0.18032324314117432
Batch 3/64 loss: -0.15872251987457275
Batch 4/64 loss: -0.1835363507270813
Batch 5/64 loss: -0.17161482572555542
Batch 6/64 loss: -0.1751236915588379
Batch 7/64 loss: -0.1745193600654602
Batch 8/64 loss: -0.1685192584991455
Batch 9/64 loss: -0.17140495777130127
Batch 10/64 loss: -0.19195663928985596
Batch 11/64 loss: -0.16936767101287842
Batch 12/64 loss: -0.18706423044204712
Batch 13/64 loss: -0.15947628021240234
Batch 14/64 loss: -0.16820931434631348
Batch 15/64 loss: -0.18054497241973877
Batch 16/64 loss: -0.1907871961593628
Batch 17/64 loss: -0.18210625648498535
Batch 18/64 loss: -0.1627647876739502
Batch 19/64 loss: -0.18841123580932617
Batch 20/64 loss: -0.1541268229484558
Batch 21/64 loss: -0.17589956521987915
Batch 22/64 loss: -0.16312968730926514
Batch 23/64 loss: -0.15878617763519287
Batch 24/64 loss: -0.16402119398117065
Batch 25/64 loss: -0.1458832025527954
Batch 26/64 loss: -0.1802990436553955
Batch 27/64 loss: -0.1707337498664856
Batch 28/64 loss: -0.170904278755188
Batch 29/64 loss: -0.18734019994735718
Batch 30/64 loss: -0.15362834930419922
Batch 31/64 loss: -0.1477556824684143
Batch 32/64 loss: -0.16902399063110352
Batch 33/64 loss: -0.15715748071670532
Batch 34/64 loss: -0.1671658754348755
Batch 35/64 loss: -0.18378472328186035
Batch 36/64 loss: -0.1585853099822998
Batch 37/64 loss: -0.16481584310531616
Batch 38/64 loss: -0.13439464569091797
Batch 39/64 loss: -0.1682811975479126
Batch 40/64 loss: -0.16979044675827026
Batch 41/64 loss: -0.16060513257980347
Batch 42/64 loss: -0.12687468528747559
Batch 43/64 loss: -0.16627007722854614
Batch 44/64 loss: -0.16545629501342773
Batch 45/64 loss: -0.17056351900100708
Batch 46/64 loss: -0.15059006214141846
Batch 47/64 loss: -0.15083038806915283
Batch 48/64 loss: -0.1768268346786499
Batch 49/64 loss: -0.15615910291671753
Batch 50/64 loss: -0.15021371841430664
Batch 51/64 loss: -0.15084123611450195
Batch 52/64 loss: -0.17653405666351318
Batch 53/64 loss: -0.15723121166229248
Batch 54/64 loss: -0.16857504844665527
Batch 55/64 loss: -0.17949891090393066
Batch 56/64 loss: -0.18196535110473633
Batch 57/64 loss: -0.141789972782135
Batch 58/64 loss: -0.15677529573440552
Batch 59/64 loss: -0.17632192373275757
Batch 60/64 loss: -0.17307734489440918
Batch 61/64 loss: -0.1687992811203003
Batch 62/64 loss: -0.1735438108444214
Batch 63/64 loss: -0.1531946063041687
Batch 64/64 loss: -0.15500670671463013
Epoch 357  Train loss: -0.16697811496024037  Val loss: -0.01853820835192179
Epoch 358
-------------------------------
Batch 1/64 loss: -0.182303786277771
Batch 2/64 loss: -0.18125343322753906
Batch 3/64 loss: -0.17804372310638428
Batch 4/64 loss: -0.16084367036819458
Batch 5/64 loss: -0.16920030117034912
Batch 6/64 loss: -0.1645064353942871
Batch 7/64 loss: -0.17325133085250854
Batch 8/64 loss: -0.1590682864189148
Batch 9/64 loss: -0.10289561748504639
Batch 10/64 loss: -0.17285537719726562
Batch 11/64 loss: -0.18133872747421265
Batch 12/64 loss: -0.18001103401184082
Batch 13/64 loss: -0.1476306915283203
Batch 14/64 loss: -0.13120895624160767
Batch 15/64 loss: -0.15383410453796387
Batch 16/64 loss: -0.15348529815673828
Batch 17/64 loss: -0.17751336097717285
Batch 18/64 loss: -0.14756333827972412
Batch 19/64 loss: -0.1534988284111023
Batch 20/64 loss: -0.11549544334411621
Batch 21/64 loss: -0.16589665412902832
Batch 22/64 loss: -0.1636749505996704
Batch 23/64 loss: -0.15773433446884155
Batch 24/64 loss: -0.1520019769668579
Batch 25/64 loss: -0.15717053413391113
Batch 26/64 loss: -0.16284483671188354
Batch 27/64 loss: -0.1223304271697998
Batch 28/64 loss: -0.163654625415802
Batch 29/64 loss: -0.17415553331375122
Batch 30/64 loss: -0.17757445573806763
Batch 31/64 loss: -0.1747106909751892
Batch 32/64 loss: -0.16782748699188232
Batch 33/64 loss: -0.16646414995193481
Batch 34/64 loss: -0.1793510913848877
Batch 35/64 loss: -0.18054264783859253
Batch 36/64 loss: -0.14791339635849
Batch 37/64 loss: -0.16822665929794312
Batch 38/64 loss: -0.16836899518966675
Batch 39/64 loss: -0.17629927396774292
Batch 40/64 loss: -0.1524863839149475
Batch 41/64 loss: -0.17767977714538574
Batch 42/64 loss: -0.18285667896270752
Batch 43/64 loss: -0.17262250185012817
Batch 44/64 loss: -0.1410830020904541
Batch 45/64 loss: -0.17085832357406616
Batch 46/64 loss: -0.17876946926116943
Batch 47/64 loss: -0.17356950044631958
Batch 48/64 loss: -0.1606370210647583
Batch 49/64 loss: -0.1680803894996643
Batch 50/64 loss: -0.16547369956970215
Batch 51/64 loss: -0.1540130376815796
Batch 52/64 loss: -0.1746891736984253
Batch 53/64 loss: -0.17765706777572632
Batch 54/64 loss: -0.1812416911125183
Batch 55/64 loss: -0.14638614654541016
Batch 56/64 loss: -0.17127138376235962
Batch 57/64 loss: -0.1676020622253418
Batch 58/64 loss: -0.16813838481903076
Batch 59/64 loss: -0.1564391851425171
Batch 60/64 loss: -0.19597047567367554
Batch 61/64 loss: -0.15549695491790771
Batch 62/64 loss: -0.17915356159210205
Batch 63/64 loss: -0.1756008267402649
Batch 64/64 loss: -0.14632868766784668
Epoch 358  Train loss: -0.16423635576285567  Val loss: -0.01901228792478948
Epoch 359
-------------------------------
Batch 1/64 loss: -0.17514336109161377
Batch 2/64 loss: -0.16577517986297607
Batch 3/64 loss: -0.1673527956008911
Batch 4/64 loss: -0.17492783069610596
Batch 5/64 loss: -0.1785704493522644
Batch 6/64 loss: -0.15718692541122437
Batch 7/64 loss: -0.18672966957092285
Batch 8/64 loss: -0.15455466508865356
Batch 9/64 loss: -0.1704803705215454
Batch 10/64 loss: -0.1549893021583557
Batch 11/64 loss: -0.17804169654846191
Batch 12/64 loss: -0.17631322145462036
Batch 13/64 loss: -0.15966910123825073
Batch 14/64 loss: -0.1831037402153015
Batch 15/64 loss: -0.16468197107315063
Batch 16/64 loss: -0.20333603024482727
Batch 17/64 loss: -0.16477590799331665
Batch 18/64 loss: -0.1851631999015808
Batch 19/64 loss: -0.1714993119239807
Batch 20/64 loss: -0.15348678827285767
Batch 21/64 loss: -0.14604735374450684
Batch 22/64 loss: -0.15619689226150513
Batch 23/64 loss: -0.12602412700653076
Batch 24/64 loss: -0.19772642850875854
Batch 25/64 loss: -0.16867667436599731
Batch 26/64 loss: -0.16523021459579468
Batch 27/64 loss: -0.17469656467437744
Batch 28/64 loss: -0.17969608306884766
Batch 29/64 loss: -0.1529363989830017
Batch 30/64 loss: -0.14272046089172363
Batch 31/64 loss: -0.1625659465789795
Batch 32/64 loss: -0.17607223987579346
Batch 33/64 loss: -0.16767966747283936
Batch 34/64 loss: -0.1859647035598755
Batch 35/64 loss: -0.16755640506744385
Batch 36/64 loss: -0.17128074169158936
Batch 37/64 loss: -0.15897011756896973
Batch 38/64 loss: -0.19943195581436157
Batch 39/64 loss: -0.18034827709197998
Batch 40/64 loss: -0.16758942604064941
Batch 41/64 loss: -0.1722429394721985
Batch 42/64 loss: -0.18112659454345703
Batch 43/64 loss: -0.19261646270751953
Batch 44/64 loss: -0.16667324304580688
Batch 45/64 loss: -0.15491539239883423
Batch 46/64 loss: -0.15731197595596313
Batch 47/64 loss: -0.17514950037002563
Batch 48/64 loss: -0.16844487190246582
Batch 49/64 loss: -0.16540253162384033
Batch 50/64 loss: -0.16420269012451172
Batch 51/64 loss: -0.17199504375457764
Batch 52/64 loss: -0.14632046222686768
Batch 53/64 loss: -0.1720731258392334
Batch 54/64 loss: -0.16849082708358765
Batch 55/64 loss: -0.15263354778289795
Batch 56/64 loss: -0.14425891637802124
Batch 57/64 loss: -0.1777513027191162
Batch 58/64 loss: -0.1749502420425415
Batch 59/64 loss: -0.15323495864868164
Batch 60/64 loss: -0.16657984256744385
Batch 61/64 loss: -0.14335721731185913
Batch 62/64 loss: -0.1886235475540161
Batch 63/64 loss: -0.17973816394805908
Batch 64/64 loss: -0.13464897871017456
Epoch 359  Train loss: -0.16803564440970328  Val loss: -0.021128987118960246
Epoch 360
-------------------------------
Batch 1/64 loss: -0.18371963500976562
Batch 2/64 loss: -0.15493053197860718
Batch 3/64 loss: -0.17562812566757202
Batch 4/64 loss: -0.154879629611969
Batch 5/64 loss: -0.12462794780731201
Batch 6/64 loss: -0.12756556272506714
Batch 7/64 loss: -0.16064441204071045
Batch 8/64 loss: -0.17566001415252686
Batch 9/64 loss: -0.16579657793045044
Batch 10/64 loss: -0.1489548683166504
Batch 11/64 loss: -0.16550356149673462
Batch 12/64 loss: -0.17359083890914917
Batch 13/64 loss: -0.14044958353042603
Batch 14/64 loss: -0.16466963291168213
Batch 15/64 loss: -0.1615179181098938
Batch 16/64 loss: -0.18342256546020508
Batch 17/64 loss: -0.15330928564071655
Batch 18/64 loss: -0.16930782794952393
Batch 19/64 loss: -0.16452771425247192
Batch 20/64 loss: -0.1662706732749939
Batch 21/64 loss: -0.15049123764038086
Batch 22/64 loss: -0.14667880535125732
Batch 23/64 loss: -0.1729610562324524
Batch 24/64 loss: -0.15176129341125488
Batch 25/64 loss: -0.17805546522140503
Batch 26/64 loss: -0.16278517246246338
Batch 27/64 loss: -0.1748049259185791
Batch 28/64 loss: -0.17017197608947754
Batch 29/64 loss: -0.16193747520446777
Batch 30/64 loss: -0.17009741067886353
Batch 31/64 loss: -0.16560637950897217
Batch 32/64 loss: -0.15615922212600708
Batch 33/64 loss: -0.17463266849517822
Batch 34/64 loss: -0.16335970163345337
Batch 35/64 loss: -0.17432498931884766
Batch 36/64 loss: -0.16933995485305786
Batch 37/64 loss: -0.1682347059249878
Batch 38/64 loss: -0.15314114093780518
Batch 39/64 loss: -0.15602970123291016
Batch 40/64 loss: -0.1833552122116089
Batch 41/64 loss: -0.18294888734817505
Batch 42/64 loss: -0.17964935302734375
Batch 43/64 loss: -0.15571492910385132
Batch 44/64 loss: -0.18749868869781494
Batch 45/64 loss: -0.14293956756591797
Batch 46/64 loss: -0.17956101894378662
Batch 47/64 loss: -0.18735361099243164
Batch 48/64 loss: -0.16645628213882446
Batch 49/64 loss: -0.17882966995239258
Batch 50/64 loss: -0.18041372299194336
Batch 51/64 loss: -0.19356966018676758
Batch 52/64 loss: -0.1438630223274231
Batch 53/64 loss: -0.15300416946411133
Batch 54/64 loss: -0.16349542140960693
Batch 55/64 loss: -0.162523090839386
Batch 56/64 loss: -0.17338532209396362
Batch 57/64 loss: -0.1689167022705078
Batch 58/64 loss: -0.15904104709625244
Batch 59/64 loss: -0.18897205591201782
Batch 60/64 loss: -0.16208386421203613
Batch 61/64 loss: -0.16567206382751465
Batch 62/64 loss: -0.1782761812210083
Batch 63/64 loss: -0.1686939001083374
Batch 64/64 loss: -0.15883660316467285
Epoch 360  Train loss: -0.16566109937780044  Val loss: -0.020451553089102518
Epoch 361
-------------------------------
Batch 1/64 loss: -0.14977145195007324
Batch 2/64 loss: -0.16827291250228882
Batch 3/64 loss: -0.17937874794006348
Batch 4/64 loss: -0.15110743045806885
Batch 5/64 loss: -0.17906570434570312
Batch 6/64 loss: -0.16609394550323486
Batch 7/64 loss: -0.1931016445159912
Batch 8/64 loss: -0.17802315950393677
Batch 9/64 loss: -0.17696833610534668
Batch 10/64 loss: -0.17016392946243286
Batch 11/64 loss: -0.18120455741882324
Batch 12/64 loss: -0.19640636444091797
Batch 13/64 loss: -0.18968325853347778
Batch 14/64 loss: -0.15512722730636597
Batch 15/64 loss: -0.1461482048034668
Batch 16/64 loss: -0.17576467990875244
Batch 17/64 loss: -0.1769254207611084
Batch 18/64 loss: -0.18558740615844727
Batch 19/64 loss: -0.16974395513534546
Batch 20/64 loss: -0.16883909702301025
Batch 21/64 loss: -0.14931446313858032
Batch 22/64 loss: -0.15468603372573853
Batch 23/64 loss: -0.15456712245941162
Batch 24/64 loss: -0.17096996307373047
Batch 25/64 loss: -0.17377686500549316
Batch 26/64 loss: -0.18782579898834229
Batch 27/64 loss: -0.14275091886520386
Batch 28/64 loss: -0.16281676292419434
Batch 29/64 loss: -0.17233479022979736
Batch 30/64 loss: -0.19410133361816406
Batch 31/64 loss: -0.15257000923156738
Batch 32/64 loss: -0.17089736461639404
Batch 33/64 loss: -0.15766215324401855
Batch 34/64 loss: -0.12088555097579956
Batch 35/64 loss: -0.1632300615310669
Batch 36/64 loss: -0.16558897495269775
Batch 37/64 loss: -0.14139336347579956
Batch 38/64 loss: -0.11522376537322998
Batch 39/64 loss: -0.16051983833312988
Batch 40/64 loss: -0.17121505737304688
Batch 41/64 loss: -0.15962910652160645
Batch 42/64 loss: -0.15534597635269165
Batch 43/64 loss: -0.17804652452468872
Batch 44/64 loss: -0.16698604822158813
Batch 45/64 loss: -0.1768971085548401
Batch 46/64 loss: -0.1633022427558899
Batch 47/64 loss: -0.17336618900299072
Batch 48/64 loss: -0.1579117774963379
Batch 49/64 loss: -0.17222881317138672
Batch 50/64 loss: -0.1616726517677307
Batch 51/64 loss: -0.17167603969573975
Batch 52/64 loss: -0.13794851303100586
Batch 53/64 loss: -0.1529219150543213
Batch 54/64 loss: -0.17020881175994873
Batch 55/64 loss: -0.17869466543197632
Batch 56/64 loss: -0.1712459921836853
Batch 57/64 loss: -0.1863064169883728
Batch 58/64 loss: -0.1877133846282959
Batch 59/64 loss: -0.17175614833831787
Batch 60/64 loss: -0.14829015731811523
Batch 61/64 loss: -0.16684603691101074
Batch 62/64 loss: -0.17157089710235596
Batch 63/64 loss: -0.1828746795654297
Batch 64/64 loss: -0.16377335786819458
Epoch 361  Train loss: -0.16668200375987033  Val loss: -0.02083754437076267
Epoch 362
-------------------------------
Batch 1/64 loss: -0.17441797256469727
Batch 2/64 loss: -0.1639677882194519
Batch 3/64 loss: -0.1853991150856018
Batch 4/64 loss: -0.17571306228637695
Batch 5/64 loss: -0.16483348608016968
Batch 6/64 loss: -0.1882508397102356
Batch 7/64 loss: -0.15141266584396362
Batch 8/64 loss: -0.17624986171722412
Batch 9/64 loss: -0.16776639223098755
Batch 10/64 loss: -0.1744368076324463
Batch 11/64 loss: -0.17616260051727295
Batch 12/64 loss: -0.16708135604858398
Batch 13/64 loss: -0.16301441192626953
Batch 14/64 loss: -0.15325134992599487
Batch 15/64 loss: -0.17758053541183472
Batch 16/64 loss: -0.17510294914245605
Batch 17/64 loss: -0.15656739473342896
Batch 18/64 loss: -0.17798340320587158
Batch 19/64 loss: -0.17443978786468506
Batch 20/64 loss: -0.1835896372795105
Batch 21/64 loss: -0.1830708384513855
Batch 22/64 loss: -0.18380498886108398
Batch 23/64 loss: -0.16585958003997803
Batch 24/64 loss: -0.12419426441192627
Batch 25/64 loss: -0.16808044910430908
Batch 26/64 loss: -0.16466659307479858
Batch 27/64 loss: -0.18348348140716553
Batch 28/64 loss: -0.18148261308670044
Batch 29/64 loss: -0.17673099040985107
Batch 30/64 loss: -0.17148447036743164
Batch 31/64 loss: -0.19605529308319092
Batch 32/64 loss: -0.15644216537475586
Batch 33/64 loss: -0.1807282567024231
Batch 34/64 loss: -0.16905075311660767
Batch 35/64 loss: -0.19032996892929077
Batch 36/64 loss: -0.1622847318649292
Batch 37/64 loss: -0.17549175024032593
Batch 38/64 loss: -0.17297154664993286
Batch 39/64 loss: -0.17960351705551147
Batch 40/64 loss: -0.17269498109817505
Batch 41/64 loss: -0.14199912548065186
Batch 42/64 loss: -0.16508948802947998
Batch 43/64 loss: -0.16128766536712646
Batch 44/64 loss: -0.18007779121398926
Batch 45/64 loss: -0.16894298791885376
Batch 46/64 loss: -0.18140500783920288
Batch 47/64 loss: -0.14910048246383667
Batch 48/64 loss: -0.17332994937896729
Batch 49/64 loss: -0.16892731189727783
Batch 50/64 loss: -0.16533523797988892
Batch 51/64 loss: -0.16018420457839966
Batch 52/64 loss: -0.15581071376800537
Batch 53/64 loss: -0.16131466627120972
Batch 54/64 loss: -0.1675626039505005
Batch 55/64 loss: -0.15106433629989624
Batch 56/64 loss: -0.18596625328063965
Batch 57/64 loss: -0.15900319814682007
Batch 58/64 loss: -0.16131961345672607
Batch 59/64 loss: -0.16461366415023804
Batch 60/64 loss: -0.13809537887573242
Batch 61/64 loss: -0.14132732152938843
Batch 62/64 loss: -0.19504791498184204
Batch 63/64 loss: -0.1631476879119873
Batch 64/64 loss: -0.17626428604125977
Epoch 362  Train loss: -0.16906483874601477  Val loss: -0.021870630098782044
Epoch 363
-------------------------------
Batch 1/64 loss: -0.1643262505531311
Batch 2/64 loss: -0.15968573093414307
Batch 3/64 loss: -0.16660118103027344
Batch 4/64 loss: -0.16469520330429077
Batch 5/64 loss: -0.18206340074539185
Batch 6/64 loss: -0.18323183059692383
Batch 7/64 loss: -0.17939811944961548
Batch 8/64 loss: -0.17960309982299805
Batch 9/64 loss: -0.167036771774292
Batch 10/64 loss: -0.17112916707992554
Batch 11/64 loss: -0.16575288772583008
Batch 12/64 loss: -0.19575297832489014
Batch 13/64 loss: -0.17035150527954102
Batch 14/64 loss: -0.15216070413589478
Batch 15/64 loss: -0.17725396156311035
Batch 16/64 loss: -0.19819140434265137
Batch 17/64 loss: -0.15257400274276733
Batch 18/64 loss: -0.1876232624053955
Batch 19/64 loss: -0.18145239353179932
Batch 20/64 loss: -0.16009289026260376
Batch 21/64 loss: -0.1615281105041504
Batch 22/64 loss: -0.17080587148666382
Batch 23/64 loss: -0.15738272666931152
Batch 24/64 loss: -0.16638129949569702
Batch 25/64 loss: -0.16999441385269165
Batch 26/64 loss: -0.153478741645813
Batch 27/64 loss: -0.17817598581314087
Batch 28/64 loss: -0.175728440284729
Batch 29/64 loss: -0.16577070951461792
Batch 30/64 loss: -0.16686195135116577
Batch 31/64 loss: -0.18215292692184448
Batch 32/64 loss: -0.1839003562927246
Batch 33/64 loss: -0.19541269540786743
Batch 34/64 loss: -0.17761152982711792
Batch 35/64 loss: -0.16617417335510254
Batch 36/64 loss: -0.17644160985946655
Batch 37/64 loss: -0.1454121470451355
Batch 38/64 loss: -0.15197432041168213
Batch 39/64 loss: -0.18600881099700928
Batch 40/64 loss: -0.1593952775001526
Batch 41/64 loss: -0.1605643630027771
Batch 42/64 loss: -0.1768038272857666
Batch 43/64 loss: -0.15618878602981567
Batch 44/64 loss: -0.15390092134475708
Batch 45/64 loss: -0.17094004154205322
Batch 46/64 loss: -0.1760616898536682
Batch 47/64 loss: -0.1712268590927124
Batch 48/64 loss: -0.16863417625427246
Batch 49/64 loss: -0.18430447578430176
Batch 50/64 loss: -0.18193089962005615
Batch 51/64 loss: -0.16430658102035522
Batch 52/64 loss: -0.16158199310302734
Batch 53/64 loss: -0.1732642650604248
Batch 54/64 loss: -0.16504311561584473
Batch 55/64 loss: -0.18717992305755615
Batch 56/64 loss: -0.14499765634536743
Batch 57/64 loss: -0.16321152448654175
Batch 58/64 loss: -0.16287851333618164
Batch 59/64 loss: -0.1675633192062378
Batch 60/64 loss: -0.14318472146987915
Batch 61/64 loss: -0.1517084836959839
Batch 62/64 loss: -0.17492634057998657
Batch 63/64 loss: -0.17502307891845703
Batch 64/64 loss: -0.1405147910118103
Epoch 363  Train loss: -0.16926087047539506  Val loss: -0.021807618976868306
Epoch 364
-------------------------------
Batch 1/64 loss: -0.18475127220153809
Batch 2/64 loss: -0.1909182071685791
Batch 3/64 loss: -0.18171507120132446
Batch 4/64 loss: -0.1281282901763916
Batch 5/64 loss: -0.1925182342529297
Batch 6/64 loss: -0.16998177766799927
Batch 7/64 loss: -0.1966876983642578
Batch 8/64 loss: -0.18734008073806763
Batch 9/64 loss: -0.1467273235321045
Batch 10/64 loss: -0.19766312837600708
Batch 11/64 loss: -0.19605112075805664
Batch 12/64 loss: -0.18628931045532227
Batch 13/64 loss: -0.18729168176651
Batch 14/64 loss: -0.18742424249649048
Batch 15/64 loss: -0.16689050197601318
Batch 16/64 loss: -0.16866427659988403
Batch 17/64 loss: -0.16914379596710205
Batch 18/64 loss: -0.1564008593559265
Batch 19/64 loss: -0.1664920449256897
Batch 20/64 loss: -0.17514866590499878
Batch 21/64 loss: -0.17978054285049438
Batch 22/64 loss: -0.160913348197937
Batch 23/64 loss: -0.17741060256958008
Batch 24/64 loss: -0.1922878623008728
Batch 25/64 loss: -0.16902679204940796
Batch 26/64 loss: -0.1620781421661377
Batch 27/64 loss: -0.17923009395599365
Batch 28/64 loss: -0.16024279594421387
Batch 29/64 loss: -0.1397186517715454
Batch 30/64 loss: -0.12346959114074707
Batch 31/64 loss: -0.1636890172958374
Batch 32/64 loss: -0.15241318941116333
Batch 33/64 loss: -0.13171738386154175
Batch 34/64 loss: -0.18379426002502441
Batch 35/64 loss: -0.17525041103363037
Batch 36/64 loss: -0.17088395357131958
Batch 37/64 loss: -0.188193678855896
Batch 38/64 loss: -0.17594385147094727
Batch 39/64 loss: -0.1431291103363037
Batch 40/64 loss: -0.13925480842590332
Batch 41/64 loss: -0.1793854832649231
Batch 42/64 loss: -0.15408116579055786
Batch 43/64 loss: -0.14712190628051758
Batch 44/64 loss: -0.15150678157806396
Batch 45/64 loss: -0.17392855882644653
Batch 46/64 loss: -0.1748809814453125
Batch 47/64 loss: -0.16441303491592407
Batch 48/64 loss: -0.16532737016677856
Batch 49/64 loss: -0.16297650337219238
Batch 50/64 loss: -0.16986310482025146
Batch 51/64 loss: -0.17957919836044312
Batch 52/64 loss: -0.17986679077148438
Batch 53/64 loss: -0.1393999457359314
Batch 54/64 loss: -0.1657823920249939
Batch 55/64 loss: -0.17860370874404907
Batch 56/64 loss: -0.179470956325531
Batch 57/64 loss: -0.1668379306793213
Batch 58/64 loss: -0.13264036178588867
Batch 59/64 loss: -0.14489233493804932
Batch 60/64 loss: -0.17029863595962524
Batch 61/64 loss: -0.17897629737854004
Batch 62/64 loss: -0.17418783903121948
Batch 63/64 loss: -0.1531803011894226
Batch 64/64 loss: -0.13079839944839478
Epoch 364  Train loss: -0.16768558516221888  Val loss: -0.016742698310576763
Epoch 365
-------------------------------
Batch 1/64 loss: -0.1495034098625183
Batch 2/64 loss: -0.18306279182434082
Batch 3/64 loss: -0.17895513772964478
Batch 4/64 loss: -0.1401950716972351
Batch 5/64 loss: -0.15746384859085083
Batch 6/64 loss: -0.17902308702468872
Batch 7/64 loss: -0.19236654043197632
Batch 8/64 loss: -0.16888821125030518
Batch 9/64 loss: -0.16731196641921997
Batch 10/64 loss: -0.17012536525726318
Batch 11/64 loss: -0.18280500173568726
Batch 12/64 loss: -0.16667819023132324
Batch 13/64 loss: -0.18993061780929565
Batch 14/64 loss: -0.1721147894859314
Batch 15/64 loss: -0.18956655263900757
Batch 16/64 loss: -0.184334397315979
Batch 17/64 loss: -0.16454768180847168
Batch 18/64 loss: -0.17534446716308594
Batch 19/64 loss: -0.1935865879058838
Batch 20/64 loss: -0.17136675119400024
Batch 21/64 loss: -0.17427176237106323
Batch 22/64 loss: -0.16565978527069092
Batch 23/64 loss: -0.179063081741333
Batch 24/64 loss: -0.15472984313964844
Batch 25/64 loss: -0.16012996435165405
Batch 26/64 loss: -0.14929497241973877
Batch 27/64 loss: -0.15462613105773926
Batch 28/64 loss: -0.1762736439704895
Batch 29/64 loss: -0.18053960800170898
Batch 30/64 loss: -0.18232637643814087
Batch 31/64 loss: -0.16624575853347778
Batch 32/64 loss: -0.17408835887908936
Batch 33/64 loss: -0.1635991334915161
Batch 34/64 loss: -0.1266370415687561
Batch 35/64 loss: -0.15178561210632324
Batch 36/64 loss: -0.16755706071853638
Batch 37/64 loss: -0.17444896697998047
Batch 38/64 loss: -0.1606001853942871
Batch 39/64 loss: -0.15971148014068604
Batch 40/64 loss: -0.1850740909576416
Batch 41/64 loss: -0.15264129638671875
Batch 42/64 loss: -0.17420893907546997
Batch 43/64 loss: -0.16543519496917725
Batch 44/64 loss: -0.174443781375885
Batch 45/64 loss: -0.14427053928375244
Batch 46/64 loss: -0.1679764986038208
Batch 47/64 loss: -0.1705390214920044
Batch 48/64 loss: -0.16227257251739502
Batch 49/64 loss: -0.16269266605377197
Batch 50/64 loss: -0.17266947031021118
Batch 51/64 loss: -0.1748778223991394
Batch 52/64 loss: -0.18073707818984985
Batch 53/64 loss: -0.16551214456558228
Batch 54/64 loss: -0.14733070135116577
Batch 55/64 loss: -0.1690632700920105
Batch 56/64 loss: -0.17623990774154663
Batch 57/64 loss: -0.16369950771331787
Batch 58/64 loss: -0.18716120719909668
Batch 59/64 loss: -0.18164664506912231
Batch 60/64 loss: -0.15979236364364624
Batch 61/64 loss: -0.13275283575057983
Batch 62/64 loss: -0.1635090708732605
Batch 63/64 loss: -0.1499415636062622
Batch 64/64 loss: -0.17520558834075928
Epoch 365  Train loss: -0.16807296650082457  Val loss: -0.019100875993774517
Epoch 366
-------------------------------
Batch 1/64 loss: -0.1684436798095703
Batch 2/64 loss: -0.16983622312545776
Batch 3/64 loss: -0.17851132154464722
Batch 4/64 loss: -0.18175870180130005
Batch 5/64 loss: -0.1700451374053955
Batch 6/64 loss: -0.16238033771514893
Batch 7/64 loss: -0.1795743703842163
Batch 8/64 loss: -0.18871039152145386
Batch 9/64 loss: -0.18589317798614502
Batch 10/64 loss: -0.1634155511856079
Batch 11/64 loss: -0.16874486207962036
Batch 12/64 loss: -0.14742016792297363
Batch 13/64 loss: -0.1623249650001526
Batch 14/64 loss: -0.19516396522521973
Batch 15/64 loss: -0.1974552869796753
Batch 16/64 loss: -0.18712002038955688
Batch 17/64 loss: -0.1353510618209839
Batch 18/64 loss: -0.15322637557983398
Batch 19/64 loss: -0.18155133724212646
Batch 20/64 loss: -0.16816920042037964
Batch 21/64 loss: -0.1648244857788086
Batch 22/64 loss: -0.16099923849105835
Batch 23/64 loss: -0.18113183975219727
Batch 24/64 loss: -0.17726927995681763
Batch 25/64 loss: -0.18859362602233887
Batch 26/64 loss: -0.17452937364578247
Batch 27/64 loss: -0.16395604610443115
Batch 28/64 loss: -0.14681297540664673
Batch 29/64 loss: -0.15432119369506836
Batch 30/64 loss: -0.15845251083374023
Batch 31/64 loss: -0.1790573000907898
Batch 32/64 loss: -0.15832430124282837
Batch 33/64 loss: -0.18361812829971313
Batch 34/64 loss: -0.17689305543899536
Batch 35/64 loss: -0.1830587387084961
Batch 36/64 loss: -0.18401658535003662
Batch 37/64 loss: -0.18221938610076904
Batch 38/64 loss: -0.13650470972061157
Batch 39/64 loss: -0.17023050785064697
Batch 40/64 loss: -0.1419370174407959
Batch 41/64 loss: -0.18918299674987793
Batch 42/64 loss: -0.16785258054733276
Batch 43/64 loss: -0.1794114112854004
Batch 44/64 loss: -0.16851305961608887
Batch 45/64 loss: -0.14304661750793457
Batch 46/64 loss: -0.17586171627044678
Batch 47/64 loss: -0.1764887571334839
Batch 48/64 loss: -0.1571452021598816
Batch 49/64 loss: -0.1721351146697998
Batch 50/64 loss: -0.14970427751541138
Batch 51/64 loss: -0.14692282676696777
Batch 52/64 loss: -0.15995162725448608
Batch 53/64 loss: -0.1594509482383728
Batch 54/64 loss: -0.16202497482299805
Batch 55/64 loss: -0.18812286853790283
Batch 56/64 loss: -0.12401437759399414
Batch 57/64 loss: -0.14779645204544067
Batch 58/64 loss: -0.15837812423706055
Batch 59/64 loss: -0.17046189308166504
Batch 60/64 loss: -0.17205321788787842
Batch 61/64 loss: -0.15472525358200073
Batch 62/64 loss: -0.15360099077224731
Batch 63/64 loss: -0.1807500123977661
Batch 64/64 loss: -0.15389108657836914
Epoch 366  Train loss: -0.16760564785377652  Val loss: -0.019394385241151266
Epoch 367
-------------------------------
Batch 1/64 loss: -0.15898609161376953
Batch 2/64 loss: -0.1659168004989624
Batch 3/64 loss: -0.15131503343582153
Batch 4/64 loss: -0.1665205955505371
Batch 5/64 loss: -0.1541987657546997
Batch 6/64 loss: -0.1580769419670105
Batch 7/64 loss: -0.15727972984313965
Batch 8/64 loss: -0.1590179204940796
Batch 9/64 loss: -0.1648235321044922
Batch 10/64 loss: -0.1964847445487976
Batch 11/64 loss: -0.18345093727111816
Batch 12/64 loss: -0.1630130410194397
Batch 13/64 loss: -0.15482079982757568
Batch 14/64 loss: -0.1833275556564331
Batch 15/64 loss: -0.20099759101867676
Batch 16/64 loss: -0.17640304565429688
Batch 17/64 loss: -0.17590391635894775
Batch 18/64 loss: -0.18121963739395142
Batch 19/64 loss: -0.18464022874832153
Batch 20/64 loss: -0.1789281964302063
Batch 21/64 loss: -0.1944296956062317
Batch 22/64 loss: -0.16965895891189575
Batch 23/64 loss: -0.18287521600723267
Batch 24/64 loss: -0.15515607595443726
Batch 25/64 loss: -0.17938309907913208
Batch 26/64 loss: -0.15869659185409546
Batch 27/64 loss: -0.17305850982666016
Batch 28/64 loss: -0.16994667053222656
Batch 29/64 loss: -0.16547513008117676
Batch 30/64 loss: -0.1892041563987732
Batch 31/64 loss: -0.19097959995269775
Batch 32/64 loss: -0.17164331674575806
Batch 33/64 loss: -0.17284971475601196
Batch 34/64 loss: -0.19730514287948608
Batch 35/64 loss: -0.19225138425827026
Batch 36/64 loss: -0.1718679666519165
Batch 37/64 loss: -0.19002485275268555
Batch 38/64 loss: -0.14872771501541138
Batch 39/64 loss: -0.14929604530334473
Batch 40/64 loss: -0.16480326652526855
Batch 41/64 loss: -0.17293840646743774
Batch 42/64 loss: -0.1591641902923584
Batch 43/64 loss: -0.17350739240646362
Batch 44/64 loss: -0.16801440715789795
Batch 45/64 loss: -0.18849420547485352
Batch 46/64 loss: -0.18201810121536255
Batch 47/64 loss: -0.17588090896606445
Batch 48/64 loss: -0.1877131462097168
Batch 49/64 loss: -0.1510365605354309
Batch 50/64 loss: -0.15144705772399902
Batch 51/64 loss: -0.17957377433776855
Batch 52/64 loss: -0.17035043239593506
Batch 53/64 loss: -0.15850836038589478
Batch 54/64 loss: -0.1862555742263794
Batch 55/64 loss: -0.17054414749145508
Batch 56/64 loss: -0.14642465114593506
Batch 57/64 loss: -0.1641668677330017
Batch 58/64 loss: -0.16501808166503906
Batch 59/64 loss: -0.17146730422973633
Batch 60/64 loss: -0.1868572235107422
Batch 61/64 loss: -0.17396867275238037
Batch 62/64 loss: -0.1710892915725708
Batch 63/64 loss: -0.19210177659988403
Batch 64/64 loss: -0.1686761975288391
Epoch 367  Train loss: -0.17217264152040668  Val loss: -0.021252624357688876
Epoch 368
-------------------------------
Batch 1/64 loss: -0.19621407985687256
Batch 2/64 loss: -0.19759851694107056
Batch 3/64 loss: -0.1790074110031128
Batch 4/64 loss: -0.16647207736968994
Batch 5/64 loss: -0.16882824897766113
Batch 6/64 loss: -0.16840684413909912
Batch 7/64 loss: -0.14456164836883545
Batch 8/64 loss: -0.19017040729522705
Batch 9/64 loss: -0.19167470932006836
Batch 10/64 loss: -0.16651511192321777
Batch 11/64 loss: -0.15721917152404785
Batch 12/64 loss: -0.1789347529411316
Batch 13/64 loss: -0.17937517166137695
Batch 14/64 loss: -0.1769242286682129
Batch 15/64 loss: -0.18317174911499023
Batch 16/64 loss: -0.1706060767173767
Batch 17/64 loss: -0.16247093677520752
Batch 18/64 loss: -0.14826107025146484
Batch 19/64 loss: -0.16785645484924316
Batch 20/64 loss: -0.18613052368164062
Batch 21/64 loss: -0.17912906408309937
Batch 22/64 loss: -0.176683247089386
Batch 23/64 loss: -0.184484601020813
Batch 24/64 loss: -0.1790509819984436
Batch 25/64 loss: -0.19191479682922363
Batch 26/64 loss: -0.1759536862373352
Batch 27/64 loss: -0.18546223640441895
Batch 28/64 loss: -0.1578400731086731
Batch 29/64 loss: -0.18540406227111816
Batch 30/64 loss: -0.1796550750732422
Batch 31/64 loss: -0.16722315549850464
Batch 32/64 loss: -0.16426193714141846
Batch 33/64 loss: -0.160622239112854
Batch 34/64 loss: -0.18197959661483765
Batch 35/64 loss: -0.15116441249847412
Batch 36/64 loss: -0.1496344804763794
Batch 37/64 loss: -0.1826954483985901
Batch 38/64 loss: -0.1609380841255188
Batch 39/64 loss: -0.18561291694641113
Batch 40/64 loss: -0.15693730115890503
Batch 41/64 loss: -0.1824401617050171
Batch 42/64 loss: -0.18756508827209473
Batch 43/64 loss: -0.177068829536438
Batch 44/64 loss: -0.1709740161895752
Batch 45/64 loss: -0.1848379373550415
Batch 46/64 loss: -0.1807953119277954
Batch 47/64 loss: -0.1867932677268982
Batch 48/64 loss: -0.1658318042755127
Batch 49/64 loss: -0.16127842664718628
Batch 50/64 loss: -0.16230237483978271
Batch 51/64 loss: -0.17058062553405762
Batch 52/64 loss: -0.17943304777145386
Batch 53/64 loss: -0.1660078763961792
Batch 54/64 loss: -0.19198590517044067
Batch 55/64 loss: -0.18890219926834106
Batch 56/64 loss: -0.1488167643547058
Batch 57/64 loss: -0.18105292320251465
Batch 58/64 loss: -0.15455490350723267
Batch 59/64 loss: -0.15468692779541016
Batch 60/64 loss: -0.16850537061691284
Batch 61/64 loss: -0.17722469568252563
Batch 62/64 loss: -0.14404046535491943
Batch 63/64 loss: -0.15014761686325073
Batch 64/64 loss: -0.15902221202850342
Epoch 368  Train loss: -0.17242572214089188  Val loss: -0.017887411453469924
Epoch 369
-------------------------------
Batch 1/64 loss: -0.1894034743309021
Batch 2/64 loss: -0.16890716552734375
Batch 3/64 loss: -0.17559123039245605
Batch 4/64 loss: -0.17655831575393677
Batch 5/64 loss: -0.18965506553649902
Batch 6/64 loss: -0.1919320821762085
Batch 7/64 loss: -0.17648577690124512
Batch 8/64 loss: -0.17241263389587402
Batch 9/64 loss: -0.17117094993591309
Batch 10/64 loss: -0.16114670038223267
Batch 11/64 loss: -0.1827385425567627
Batch 12/64 loss: -0.17260974645614624
Batch 13/64 loss: -0.16256946325302124
Batch 14/64 loss: -0.17919015884399414
Batch 15/64 loss: -0.1807531714439392
Batch 16/64 loss: -0.18480443954467773
Batch 17/64 loss: -0.1801605224609375
Batch 18/64 loss: -0.19340157508850098
Batch 19/64 loss: -0.16941189765930176
Batch 20/64 loss: -0.18019288778305054
Batch 21/64 loss: -0.1743992567062378
Batch 22/64 loss: -0.18533200025558472
Batch 23/64 loss: -0.17585217952728271
Batch 24/64 loss: -0.1798475980758667
Batch 25/64 loss: -0.15054941177368164
Batch 26/64 loss: -0.16874909400939941
Batch 27/64 loss: -0.16128015518188477
Batch 28/64 loss: -0.17721819877624512
Batch 29/64 loss: -0.1697002649307251
Batch 30/64 loss: -0.17630761861801147
Batch 31/64 loss: -0.1751946210861206
Batch 32/64 loss: -0.17211312055587769
Batch 33/64 loss: -0.18519258499145508
Batch 34/64 loss: -0.19496220350265503
Batch 35/64 loss: -0.1783134937286377
Batch 36/64 loss: -0.19212627410888672
Batch 37/64 loss: -0.17493849992752075
Batch 38/64 loss: -0.18289709091186523
Batch 39/64 loss: -0.14741724729537964
Batch 40/64 loss: -0.18812966346740723
Batch 41/64 loss: -0.1686314344406128
Batch 42/64 loss: -0.15902221202850342
Batch 43/64 loss: -0.19062185287475586
Batch 44/64 loss: -0.1840134859085083
Batch 45/64 loss: -0.18920564651489258
Batch 46/64 loss: -0.16373592615127563
Batch 47/64 loss: -0.15672951936721802
Batch 48/64 loss: -0.16790950298309326
Batch 49/64 loss: -0.16754108667373657
Batch 50/64 loss: -0.17659085988998413
Batch 51/64 loss: -0.15768253803253174
Batch 52/64 loss: -0.16710329055786133
Batch 53/64 loss: -0.16664671897888184
Batch 54/64 loss: -0.18003112077713013
Batch 55/64 loss: -0.17826813459396362
Batch 56/64 loss: -0.1518343687057495
Batch 57/64 loss: -0.1813868284225464
Batch 58/64 loss: -0.1726599931716919
Batch 59/64 loss: -0.15566056966781616
Batch 60/64 loss: -0.1846851110458374
Batch 61/64 loss: -0.15952646732330322
Batch 62/64 loss: -0.16028183698654175
Batch 63/64 loss: -0.15897953510284424
Batch 64/64 loss: -0.18374812602996826
Epoch 369  Train loss: -0.17421451783647723  Val loss: -0.0186856931837154
Epoch 370
-------------------------------
Batch 1/64 loss: -0.19349616765975952
Batch 2/64 loss: -0.1751294732093811
Batch 3/64 loss: -0.18854570388793945
Batch 4/64 loss: -0.1898515820503235
Batch 5/64 loss: -0.1798182725906372
Batch 6/64 loss: -0.17861473560333252
Batch 7/64 loss: -0.17811203002929688
Batch 8/64 loss: -0.1797657012939453
Batch 9/64 loss: -0.18496370315551758
Batch 10/64 loss: -0.1611366868019104
Batch 11/64 loss: -0.18001604080200195
Batch 12/64 loss: -0.17123591899871826
Batch 13/64 loss: -0.17250841856002808
Batch 14/64 loss: -0.18347543478012085
Batch 15/64 loss: -0.17099255323410034
Batch 16/64 loss: -0.18188536167144775
Batch 17/64 loss: -0.18283838033676147
Batch 18/64 loss: -0.16352444887161255
Batch 19/64 loss: -0.167583167552948
Batch 20/64 loss: -0.1846049427986145
Batch 21/64 loss: -0.15764427185058594
Batch 22/64 loss: -0.18882757425308228
Batch 23/64 loss: -0.1753772497177124
Batch 24/64 loss: -0.19252562522888184
Batch 25/64 loss: -0.18551117181777954
Batch 26/64 loss: -0.18960106372833252
Batch 27/64 loss: -0.20031893253326416
Batch 28/64 loss: -0.1728811264038086
Batch 29/64 loss: -0.17528855800628662
Batch 30/64 loss: -0.10964930057525635
Batch 31/64 loss: -0.16572773456573486
Batch 32/64 loss: -0.17638683319091797
Batch 33/64 loss: -0.19124937057495117
Batch 34/64 loss: -0.16594165563583374
Batch 35/64 loss: -0.17265653610229492
Batch 36/64 loss: -0.18148469924926758
Batch 37/64 loss: -0.16725945472717285
Batch 38/64 loss: -0.1523032784461975
Batch 39/64 loss: -0.1744665503501892
Batch 40/64 loss: -0.1648309826850891
Batch 41/64 loss: -0.16247624158859253
Batch 42/64 loss: -0.16510486602783203
Batch 43/64 loss: -0.18785858154296875
Batch 44/64 loss: -0.16381919384002686
Batch 45/64 loss: -0.1909407377243042
Batch 46/64 loss: -0.17086023092269897
Batch 47/64 loss: -0.1916484832763672
Batch 48/64 loss: -0.17088967561721802
Batch 49/64 loss: -0.1810082197189331
Batch 50/64 loss: -0.17489707469940186
Batch 51/64 loss: -0.17369472980499268
Batch 52/64 loss: -0.1776471734046936
Batch 53/64 loss: -0.16461265087127686
Batch 54/64 loss: -0.11800938844680786
Batch 55/64 loss: -0.14205729961395264
Batch 56/64 loss: -0.16243362426757812
Batch 57/64 loss: -0.167711079120636
Batch 58/64 loss: -0.16325479745864868
Batch 59/64 loss: -0.17288833856582642
Batch 60/64 loss: -0.1664334535598755
Batch 61/64 loss: -0.18506008386611938
Batch 62/64 loss: -0.17047053575515747
Batch 63/64 loss: -0.17602324485778809
Batch 64/64 loss: -0.1644999384880066
Epoch 370  Train loss: -0.17335224081488215  Val loss: -0.018450508822280515
Epoch 371
-------------------------------
Batch 1/64 loss: -0.15655624866485596
Batch 2/64 loss: -0.18926572799682617
Batch 3/64 loss: -0.17013198137283325
Batch 4/64 loss: -0.18608200550079346
Batch 5/64 loss: -0.16992872953414917
Batch 6/64 loss: -0.1838897466659546
Batch 7/64 loss: -0.14904969930648804
Batch 8/64 loss: -0.1816849708557129
Batch 9/64 loss: -0.19058442115783691
Batch 10/64 loss: -0.16650503873825073
Batch 11/64 loss: -0.13483411073684692
Batch 12/64 loss: -0.1812378168106079
Batch 13/64 loss: -0.18142551183700562
Batch 14/64 loss: -0.17665153741836548
Batch 15/64 loss: -0.1693010926246643
Batch 16/64 loss: -0.1834639310836792
Batch 17/64 loss: -0.16708344221115112
Batch 18/64 loss: -0.18388628959655762
Batch 19/64 loss: -0.14610350131988525
Batch 20/64 loss: -0.15833908319473267
Batch 21/64 loss: -0.16789942979812622
Batch 22/64 loss: -0.17582577466964722
Batch 23/64 loss: -0.18304407596588135
Batch 24/64 loss: -0.17940330505371094
Batch 25/64 loss: -0.16977232694625854
Batch 26/64 loss: -0.15081787109375
Batch 27/64 loss: -0.14388322830200195
Batch 28/64 loss: -0.16732794046401978
Batch 29/64 loss: -0.185233473777771
Batch 30/64 loss: -0.16774284839630127
Batch 31/64 loss: -0.18309146165847778
Batch 32/64 loss: -0.18559080362319946
Batch 33/64 loss: -0.18206536769866943
Batch 34/64 loss: -0.18998897075653076
Batch 35/64 loss: -0.1794734001159668
Batch 36/64 loss: -0.16001498699188232
Batch 37/64 loss: -0.1773296594619751
Batch 38/64 loss: -0.1863822340965271
Batch 39/64 loss: -0.16482549905776978
Batch 40/64 loss: -0.17414259910583496
Batch 41/64 loss: -0.15458369255065918
Batch 42/64 loss: -0.16237276792526245
Batch 43/64 loss: -0.15092343091964722
Batch 44/64 loss: -0.17242830991744995
Batch 45/64 loss: -0.14897912740707397
Batch 46/64 loss: -0.170202374458313
Batch 47/64 loss: -0.15339213609695435
Batch 48/64 loss: -0.17422842979431152
Batch 49/64 loss: -0.16294890642166138
Batch 50/64 loss: -0.16652566194534302
Batch 51/64 loss: -0.1298888921737671
Batch 52/64 loss: -0.15868628025054932
Batch 53/64 loss: -0.17508769035339355
Batch 54/64 loss: -0.1646614670753479
Batch 55/64 loss: -0.1495092511177063
Batch 56/64 loss: -0.173101544380188
Batch 57/64 loss: -0.15643256902694702
Batch 58/64 loss: -0.14587104320526123
Batch 59/64 loss: -0.18512898683547974
Batch 60/64 loss: -0.1583653688430786
Batch 61/64 loss: -0.16868317127227783
Batch 62/64 loss: -0.17214244604110718
Batch 63/64 loss: -0.14282137155532837
Batch 64/64 loss: -0.1419665813446045
Epoch 371  Train loss: -0.16789490194881665  Val loss: -0.020207548264375666
Epoch 372
-------------------------------
Batch 1/64 loss: -0.15118682384490967
Batch 2/64 loss: -0.18620502948760986
Batch 3/64 loss: -0.18446320295333862
Batch 4/64 loss: -0.17803746461868286
Batch 5/64 loss: -0.1652337908744812
Batch 6/64 loss: -0.16303753852844238
Batch 7/64 loss: -0.16446053981781006
Batch 8/64 loss: -0.18956100940704346
Batch 9/64 loss: -0.1808164119720459
Batch 10/64 loss: -0.17615944147109985
Batch 11/64 loss: -0.16344106197357178
Batch 12/64 loss: -0.16673427820205688
Batch 13/64 loss: -0.14427059888839722
Batch 14/64 loss: -0.16668009757995605
Batch 15/64 loss: -0.17486369609832764
Batch 16/64 loss: -0.18039602041244507
Batch 17/64 loss: -0.18159747123718262
Batch 18/64 loss: -0.17531269788742065
Batch 19/64 loss: -0.15547794103622437
Batch 20/64 loss: -0.19275856018066406
Batch 21/64 loss: -0.15361565351486206
Batch 22/64 loss: -0.18236654996871948
Batch 23/64 loss: -0.17717432975769043
Batch 24/64 loss: -0.16947627067565918
Batch 25/64 loss: -0.1648678183555603
Batch 26/64 loss: -0.14613217115402222
Batch 27/64 loss: -0.17078161239624023
Batch 28/64 loss: -0.19018244743347168
Batch 29/64 loss: -0.17117679119110107
Batch 30/64 loss: -0.18793195486068726
Batch 31/64 loss: -0.19785869121551514
Batch 32/64 loss: -0.1914815902709961
Batch 33/64 loss: -0.16674399375915527
Batch 34/64 loss: -0.16783899068832397
Batch 35/64 loss: -0.1737971305847168
Batch 36/64 loss: -0.17636597156524658
Batch 37/64 loss: -0.17293471097946167
Batch 38/64 loss: -0.1758750081062317
Batch 39/64 loss: -0.20002466440200806
Batch 40/64 loss: -0.185355544090271
Batch 41/64 loss: -0.19346344470977783
Batch 42/64 loss: -0.16836953163146973
Batch 43/64 loss: -0.1681470274925232
Batch 44/64 loss: -0.18779420852661133
Batch 45/64 loss: -0.161945641040802
Batch 46/64 loss: -0.15720462799072266
Batch 47/64 loss: -0.16304445266723633
Batch 48/64 loss: -0.1362718939781189
Batch 49/64 loss: -0.18016374111175537
Batch 50/64 loss: -0.1780766248703003
Batch 51/64 loss: -0.1927894949913025
Batch 52/64 loss: -0.18106913566589355
Batch 53/64 loss: -0.19128364324569702
Batch 54/64 loss: -0.16604948043823242
Batch 55/64 loss: -0.19460558891296387
Batch 56/64 loss: -0.17465007305145264
Batch 57/64 loss: -0.16206341981887817
Batch 58/64 loss: -0.16916793584823608
Batch 59/64 loss: -0.16167151927947998
Batch 60/64 loss: -0.1707918643951416
Batch 61/64 loss: -0.15735483169555664
Batch 62/64 loss: -0.17906004190444946
Batch 63/64 loss: -0.16827070713043213
Batch 64/64 loss: -0.155894935131073
Epoch 372  Train loss: -0.1732220502460704  Val loss: -0.01937843659489425
Epoch 373
-------------------------------
Batch 1/64 loss: -0.1556674838066101
Batch 2/64 loss: -0.17442995309829712
Batch 3/64 loss: -0.1766170859336853
Batch 4/64 loss: -0.16502487659454346
Batch 5/64 loss: -0.18301719427108765
Batch 6/64 loss: -0.16532212495803833
Batch 7/64 loss: -0.18007034063339233
Batch 8/64 loss: -0.163332998752594
Batch 9/64 loss: -0.15777260065078735
Batch 10/64 loss: -0.18236935138702393
Batch 11/64 loss: -0.1761152744293213
Batch 12/64 loss: -0.18348431587219238
Batch 13/64 loss: -0.18624025583267212
Batch 14/64 loss: -0.17647695541381836
Batch 15/64 loss: -0.1878567337989807
Batch 16/64 loss: -0.1634279489517212
Batch 17/64 loss: -0.17822718620300293
Batch 18/64 loss: -0.1800946593284607
Batch 19/64 loss: -0.18231713771820068
Batch 20/64 loss: -0.1380634903907776
Batch 21/64 loss: -0.18016064167022705
Batch 22/64 loss: -0.16393440961837769
Batch 23/64 loss: -0.18523436784744263
Batch 24/64 loss: -0.15271085500717163
Batch 25/64 loss: -0.17553460597991943
Batch 26/64 loss: -0.162244975566864
Batch 27/64 loss: -0.1699458360671997
Batch 28/64 loss: -0.14931517839431763
Batch 29/64 loss: -0.18058288097381592
Batch 30/64 loss: -0.16898858547210693
Batch 31/64 loss: -0.1756039261817932
Batch 32/64 loss: -0.17561352252960205
Batch 33/64 loss: -0.1773681640625
Batch 34/64 loss: -0.17329704761505127
Batch 35/64 loss: -0.19548988342285156
Batch 36/64 loss: -0.15535348653793335
Batch 37/64 loss: -0.1552445888519287
Batch 38/64 loss: -0.12366586923599243
Batch 39/64 loss: -0.1679285168647766
Batch 40/64 loss: -0.18351471424102783
Batch 41/64 loss: -0.1629738211631775
Batch 42/64 loss: -0.16614186763763428
Batch 43/64 loss: -0.17050600051879883
Batch 44/64 loss: -0.17001569271087646
Batch 45/64 loss: -0.1692296862602234
Batch 46/64 loss: -0.16248387098312378
Batch 47/64 loss: -0.15531373023986816
Batch 48/64 loss: -0.17781519889831543
Batch 49/64 loss: -0.16776257753372192
Batch 50/64 loss: -0.19221413135528564
Batch 51/64 loss: -0.17371845245361328
Batch 52/64 loss: -0.17370378971099854
Batch 53/64 loss: -0.1827787160873413
Batch 54/64 loss: -0.1928337812423706
Batch 55/64 loss: -0.1838090419769287
Batch 56/64 loss: -0.18657183647155762
Batch 57/64 loss: -0.18578028678894043
Batch 58/64 loss: -0.18326443433761597
Batch 59/64 loss: -0.15947914123535156
Batch 60/64 loss: -0.14588576555252075
Batch 61/64 loss: -0.18942338228225708
Batch 62/64 loss: -0.13626188039779663
Batch 63/64 loss: -0.15264564752578735
Batch 64/64 loss: -0.18540316820144653
Epoch 373  Train loss: -0.1710952962146086  Val loss: -0.020255513822090176
Epoch 374
-------------------------------
Batch 1/64 loss: -0.15221208333969116
Batch 2/64 loss: -0.16150283813476562
Batch 3/64 loss: -0.1893410086631775
Batch 4/64 loss: -0.17587214708328247
Batch 5/64 loss: -0.16844654083251953
Batch 6/64 loss: -0.17489570379257202
Batch 7/64 loss: -0.1776728630065918
Batch 8/64 loss: -0.20453843474388123
Batch 9/64 loss: -0.17460739612579346
Batch 10/64 loss: -0.17963242530822754
Batch 11/64 loss: -0.17616146802902222
Batch 12/64 loss: -0.1891946792602539
Batch 13/64 loss: -0.1701890230178833
Batch 14/64 loss: -0.1952921748161316
Batch 15/64 loss: -0.18236392736434937
Batch 16/64 loss: -0.18992853164672852
Batch 17/64 loss: -0.1744559407234192
Batch 18/64 loss: -0.18468225002288818
Batch 19/64 loss: -0.17810499668121338
Batch 20/64 loss: -0.17232632637023926
Batch 21/64 loss: -0.16928493976593018
Batch 22/64 loss: -0.1717928647994995
Batch 23/64 loss: -0.1773895025253296
Batch 24/64 loss: -0.1725085973739624
Batch 25/64 loss: -0.17685723304748535
Batch 26/64 loss: -0.1609085202217102
Batch 27/64 loss: -0.1746540069580078
Batch 28/64 loss: -0.1695438027381897
Batch 29/64 loss: -0.16422003507614136
Batch 30/64 loss: -0.18286216259002686
Batch 31/64 loss: -0.1989377737045288
Batch 32/64 loss: -0.1725316047668457
Batch 33/64 loss: -0.1772359013557434
Batch 34/64 loss: -0.17415732145309448
Batch 35/64 loss: -0.17496144771575928
Batch 36/64 loss: -0.18235349655151367
Batch 37/64 loss: -0.18670541048049927
Batch 38/64 loss: -0.18510282039642334
Batch 39/64 loss: -0.1713847517967224
Batch 40/64 loss: -0.16259276866912842
Batch 41/64 loss: -0.1920795440673828
Batch 42/64 loss: -0.1614285707473755
Batch 43/64 loss: -0.1319238543510437
Batch 44/64 loss: -0.18300235271453857
Batch 45/64 loss: -0.17699915170669556
Batch 46/64 loss: -0.16435003280639648
Batch 47/64 loss: -0.15592950582504272
Batch 48/64 loss: -0.18337804079055786
Batch 49/64 loss: -0.1940717101097107
Batch 50/64 loss: -0.1798829436302185
Batch 51/64 loss: -0.16736793518066406
Batch 52/64 loss: -0.18952405452728271
Batch 53/64 loss: -0.1672142744064331
Batch 54/64 loss: -0.1783859133720398
Batch 55/64 loss: -0.15482211112976074
Batch 56/64 loss: -0.1828518509864807
Batch 57/64 loss: -0.1946718692779541
Batch 58/64 loss: -0.15640795230865479
Batch 59/64 loss: -0.17376935482025146
Batch 60/64 loss: -0.1689092516899109
Batch 61/64 loss: -0.16427242755889893
Batch 62/64 loss: -0.17787843942642212
Batch 63/64 loss: -0.1554868221282959
Batch 64/64 loss: -0.18384391069412231
Epoch 374  Train loss: -0.17521406458873376  Val loss: -0.01642925100228221
Epoch 375
-------------------------------
Batch 1/64 loss: -0.1594410538673401
Batch 2/64 loss: -0.1507430076599121
Batch 3/64 loss: -0.14882826805114746
Batch 4/64 loss: -0.16913944482803345
Batch 5/64 loss: -0.17218101024627686
Batch 6/64 loss: -0.17530471086502075
Batch 7/64 loss: -0.19888776540756226
Batch 8/64 loss: -0.20052897930145264
Batch 9/64 loss: -0.20720037817955017
Batch 10/64 loss: -0.18811583518981934
Batch 11/64 loss: -0.1655639410018921
Batch 12/64 loss: -0.1669406294822693
Batch 13/64 loss: -0.17885607481002808
Batch 14/64 loss: -0.17987066507339478
Batch 15/64 loss: -0.17927533388137817
Batch 16/64 loss: -0.18453598022460938
Batch 17/64 loss: -0.19154071807861328
Batch 18/64 loss: -0.17187422513961792
Batch 19/64 loss: -0.17039698362350464
Batch 20/64 loss: -0.18673354387283325
Batch 21/64 loss: -0.18558090925216675
Batch 22/64 loss: -0.15603715181350708
Batch 23/64 loss: -0.17213767766952515
Batch 24/64 loss: -0.1470034122467041
Batch 25/64 loss: -0.15953201055526733
Batch 26/64 loss: -0.19531261920928955
Batch 27/64 loss: -0.1857593059539795
Batch 28/64 loss: -0.17493993043899536
Batch 29/64 loss: -0.17327600717544556
Batch 30/64 loss: -0.16685450077056885
Batch 31/64 loss: -0.1997280716896057
Batch 32/64 loss: -0.16355359554290771
Batch 33/64 loss: -0.17089873552322388
Batch 34/64 loss: -0.18400335311889648
Batch 35/64 loss: -0.1815345287322998
Batch 36/64 loss: -0.1682247519493103
Batch 37/64 loss: -0.18547511100769043
Batch 38/64 loss: -0.18921315670013428
Batch 39/64 loss: -0.18073880672454834
Batch 40/64 loss: -0.1805018186569214
Batch 41/64 loss: -0.17280185222625732
Batch 42/64 loss: -0.19142568111419678
Batch 43/64 loss: -0.17834019660949707
Batch 44/64 loss: -0.1762028932571411
Batch 45/64 loss: -0.17331713438034058
Batch 46/64 loss: -0.1572713851928711
Batch 47/64 loss: -0.18491756916046143
Batch 48/64 loss: -0.1529601812362671
Batch 49/64 loss: -0.14634615182876587
Batch 50/64 loss: -0.1830562949180603
Batch 51/64 loss: -0.1158629059791565
Batch 52/64 loss: -0.17763268947601318
Batch 53/64 loss: -0.162000834941864
Batch 54/64 loss: -0.1775185465812683
Batch 55/64 loss: -0.17639970779418945
Batch 56/64 loss: -0.1764225959777832
Batch 57/64 loss: -0.19143986701965332
Batch 58/64 loss: -0.16717785596847534
Batch 59/64 loss: -0.1665624976158142
Batch 60/64 loss: -0.1779111623764038
Batch 61/64 loss: -0.14926397800445557
Batch 62/64 loss: -0.1860295534133911
Batch 63/64 loss: -0.16225206851959229
Batch 64/64 loss: -0.16683584451675415
Epoch 375  Train loss: -0.1740314429881526  Val loss: -0.01812031363293887
Epoch 376
-------------------------------
Batch 1/64 loss: -0.19238686561584473
Batch 2/64 loss: -0.1730870008468628
Batch 3/64 loss: -0.16784918308258057
Batch 4/64 loss: -0.18114447593688965
Batch 5/64 loss: -0.19077038764953613
Batch 6/64 loss: -0.15056490898132324
Batch 7/64 loss: -0.17474520206451416
Batch 8/64 loss: -0.1424795389175415
Batch 9/64 loss: -0.1578119397163391
Batch 10/64 loss: -0.17293435335159302
Batch 11/64 loss: -0.167253315448761
Batch 12/64 loss: -0.16343224048614502
Batch 13/64 loss: -0.17953884601593018
Batch 14/64 loss: -0.1842254400253296
Batch 15/64 loss: -0.16802215576171875
Batch 16/64 loss: -0.1734834909439087
Batch 17/64 loss: -0.16476309299468994
Batch 18/64 loss: -0.16998016834259033
Batch 19/64 loss: -0.15911567211151123
Batch 20/64 loss: -0.16393637657165527
Batch 21/64 loss: -0.19264119863510132
Batch 22/64 loss: -0.15860199928283691
Batch 23/64 loss: -0.1520867943763733
Batch 24/64 loss: -0.1746889352798462
Batch 25/64 loss: -0.18592917919158936
Batch 26/64 loss: -0.183108389377594
Batch 27/64 loss: -0.18541157245635986
Batch 28/64 loss: -0.1816784143447876
Batch 29/64 loss: -0.16705095767974854
Batch 30/64 loss: -0.1812569499015808
Batch 31/64 loss: -0.1312587857246399
Batch 32/64 loss: -0.1546480655670166
Batch 33/64 loss: -0.16883784532546997
Batch 34/64 loss: -0.16071325540542603
Batch 35/64 loss: -0.16520118713378906
Batch 36/64 loss: -0.1802799105644226
Batch 37/64 loss: -0.17344409227371216
Batch 38/64 loss: -0.1796320080757141
Batch 39/64 loss: -0.17645657062530518
Batch 40/64 loss: -0.14449751377105713
Batch 41/64 loss: -0.17513906955718994
Batch 42/64 loss: -0.1790318489074707
Batch 43/64 loss: -0.18435180187225342
Batch 44/64 loss: -0.1960831880569458
Batch 45/64 loss: -0.1586538553237915
Batch 46/64 loss: -0.16917431354522705
Batch 47/64 loss: -0.1745697259902954
Batch 48/64 loss: -0.1786823868751526
Batch 49/64 loss: -0.17609190940856934
Batch 50/64 loss: -0.16373872756958008
Batch 51/64 loss: -0.15661406517028809
Batch 52/64 loss: -0.16038715839385986
Batch 53/64 loss: -0.18795067071914673
Batch 54/64 loss: -0.16155344247817993
Batch 55/64 loss: -0.16090905666351318
Batch 56/64 loss: -0.1500169038772583
Batch 57/64 loss: -0.1691318154335022
Batch 58/64 loss: -0.18658769130706787
Batch 59/64 loss: -0.18028122186660767
Batch 60/64 loss: -0.1875787377357483
Batch 61/64 loss: -0.19106680154800415
Batch 62/64 loss: -0.17168104648590088
Batch 63/64 loss: -0.16803860664367676
Batch 64/64 loss: -0.1541418433189392
Epoch 376  Train loss: -0.17094696012197758  Val loss: -0.020195797136968764
Epoch 377
-------------------------------
Batch 1/64 loss: -0.1890926957130432
Batch 2/64 loss: -0.18128156661987305
Batch 3/64 loss: -0.16682720184326172
Batch 4/64 loss: -0.16260498762130737
Batch 5/64 loss: -0.17727017402648926
Batch 6/64 loss: -0.1859385371208191
Batch 7/64 loss: -0.18966490030288696
Batch 8/64 loss: -0.18565261363983154
Batch 9/64 loss: -0.1893749237060547
Batch 10/64 loss: -0.1870039701461792
Batch 11/64 loss: -0.18913352489471436
Batch 12/64 loss: -0.18756836652755737
Batch 13/64 loss: -0.17172026634216309
Batch 14/64 loss: -0.17856216430664062
Batch 15/64 loss: -0.17655056715011597
Batch 16/64 loss: -0.1574864387512207
Batch 17/64 loss: -0.1432858109474182
Batch 18/64 loss: -0.18568068742752075
Batch 19/64 loss: -0.15739494562149048
Batch 20/64 loss: -0.16452980041503906
Batch 21/64 loss: -0.16892868280410767
Batch 22/64 loss: -0.18842846155166626
Batch 23/64 loss: -0.1664155125617981
Batch 24/64 loss: -0.1925777792930603
Batch 25/64 loss: -0.18855524063110352
Batch 26/64 loss: -0.1823633313179016
Batch 27/64 loss: -0.19645577669143677
Batch 28/64 loss: -0.17065072059631348
Batch 29/64 loss: -0.19484704732894897
Batch 30/64 loss: -0.16947412490844727
Batch 31/64 loss: -0.17734426259994507
Batch 32/64 loss: -0.186551034450531
Batch 33/64 loss: -0.17208313941955566
Batch 34/64 loss: -0.18309396505355835
Batch 35/64 loss: -0.16309362649917603
Batch 36/64 loss: -0.17758303880691528
Batch 37/64 loss: -0.17260777950286865
Batch 38/64 loss: -0.16567188501358032
Batch 39/64 loss: -0.17638707160949707
Batch 40/64 loss: -0.16538500785827637
Batch 41/64 loss: -0.19721633195877075
Batch 42/64 loss: -0.1945946216583252
Batch 43/64 loss: -0.1427106261253357
Batch 44/64 loss: -0.18107372522354126
Batch 45/64 loss: -0.1682559847831726
Batch 46/64 loss: -0.1876317858695984
Batch 47/64 loss: -0.17228460311889648
Batch 48/64 loss: -0.1904505491256714
Batch 49/64 loss: -0.18901878595352173
Batch 50/64 loss: -0.1576392650604248
Batch 51/64 loss: -0.17362630367279053
Batch 52/64 loss: -0.18667608499526978
Batch 53/64 loss: -0.19053149223327637
Batch 54/64 loss: -0.168393075466156
Batch 55/64 loss: -0.16971784830093384
Batch 56/64 loss: -0.16615557670593262
Batch 57/64 loss: -0.12773215770721436
Batch 58/64 loss: -0.15679949522018433
Batch 59/64 loss: -0.18035262823104858
Batch 60/64 loss: -0.17147791385650635
Batch 61/64 loss: -0.14070510864257812
Batch 62/64 loss: -0.1895487904548645
Batch 63/64 loss: -0.17545771598815918
Batch 64/64 loss: -0.1722170114517212
Epoch 377  Train loss: -0.1755660370284436  Val loss: -0.019283294882561332
Epoch 378
-------------------------------
Batch 1/64 loss: -0.1857224702835083
Batch 2/64 loss: -0.19025641679763794
Batch 3/64 loss: -0.17863404750823975
Batch 4/64 loss: -0.1557847261428833
Batch 5/64 loss: -0.15334105491638184
Batch 6/64 loss: -0.14040029048919678
Batch 7/64 loss: -0.17214900255203247
Batch 8/64 loss: -0.18215835094451904
Batch 9/64 loss: -0.17230933904647827
Batch 10/64 loss: -0.1777021884918213
Batch 11/64 loss: -0.19239342212677002
Batch 12/64 loss: -0.18570196628570557
Batch 13/64 loss: -0.16684269905090332
Batch 14/64 loss: -0.16119545698165894
Batch 15/64 loss: -0.19226688146591187
Batch 16/64 loss: -0.17004215717315674
Batch 17/64 loss: -0.18463265895843506
Batch 18/64 loss: -0.19159960746765137
Batch 19/64 loss: -0.18761616945266724
Batch 20/64 loss: -0.1816955804824829
Batch 21/64 loss: -0.18297159671783447
Batch 22/64 loss: -0.17565369606018066
Batch 23/64 loss: -0.1747480034828186
Batch 24/64 loss: -0.1586974859237671
Batch 25/64 loss: -0.17862415313720703
Batch 26/64 loss: -0.1808108687400818
Batch 27/64 loss: -0.1795426607131958
Batch 28/64 loss: -0.18862509727478027
Batch 29/64 loss: -0.17023009061813354
Batch 30/64 loss: -0.17577660083770752
Batch 31/64 loss: -0.20322364568710327
Batch 32/64 loss: -0.19250810146331787
Batch 33/64 loss: -0.19198966026306152
Batch 34/64 loss: -0.1787412166595459
Batch 35/64 loss: -0.1837969422340393
Batch 36/64 loss: -0.19150382280349731
Batch 37/64 loss: -0.18193089962005615
Batch 38/64 loss: -0.1351301670074463
Batch 39/64 loss: -0.16785740852355957
Batch 40/64 loss: -0.1461169719696045
Batch 41/64 loss: -0.17802101373672485
Batch 42/64 loss: -0.1920466423034668
Batch 43/64 loss: -0.167283833026886
Batch 44/64 loss: -0.18735086917877197
Batch 45/64 loss: -0.16237658262252808
Batch 46/64 loss: -0.15722590684890747
Batch 47/64 loss: -0.18603062629699707
Batch 48/64 loss: -0.20028316974639893
Batch 49/64 loss: -0.18827104568481445
Batch 50/64 loss: -0.17546409368515015
Batch 51/64 loss: -0.20143204927444458
Batch 52/64 loss: -0.17487269639968872
Batch 53/64 loss: -0.17979425191879272
Batch 54/64 loss: -0.16256123781204224
Batch 55/64 loss: -0.17811381816864014
Batch 56/64 loss: -0.15359842777252197
Batch 57/64 loss: -0.17630517482757568
Batch 58/64 loss: -0.14301306009292603
Batch 59/64 loss: -0.17362046241760254
Batch 60/64 loss: -0.15651673078536987
Batch 61/64 loss: -0.15773087739944458
Batch 62/64 loss: -0.19013357162475586
Batch 63/64 loss: -0.17414188385009766
Batch 64/64 loss: -0.16417884826660156
Epoch 378  Train loss: -0.17569012922399185  Val loss: -0.014826660713379326
Epoch 379
-------------------------------
Batch 1/64 loss: -0.15767693519592285
Batch 2/64 loss: -0.15398043394088745
Batch 3/64 loss: -0.1668262481689453
Batch 4/64 loss: -0.19661366939544678
Batch 5/64 loss: -0.16805535554885864
Batch 6/64 loss: -0.1608138084411621
Batch 7/64 loss: -0.1672850251197815
Batch 8/64 loss: -0.17169678211212158
Batch 9/64 loss: -0.15942054986953735
Batch 10/64 loss: -0.1640070080757141
Batch 11/64 loss: -0.15275341272354126
Batch 12/64 loss: -0.14133888483047485
Batch 13/64 loss: -0.1806265115737915
Batch 14/64 loss: -0.17702198028564453
Batch 15/64 loss: -0.15087252855300903
Batch 16/64 loss: -0.1851043701171875
Batch 17/64 loss: -0.16839969158172607
Batch 18/64 loss: -0.1771969199180603
Batch 19/64 loss: -0.15692293643951416
Batch 20/64 loss: -0.16162842512130737
Batch 21/64 loss: -0.18155479431152344
Batch 22/64 loss: -0.14696496725082397
Batch 23/64 loss: -0.1499565839767456
Batch 24/64 loss: -0.16155099868774414
Batch 25/64 loss: -0.16650265455245972
Batch 26/64 loss: -0.1925346851348877
Batch 27/64 loss: -0.1838446855545044
Batch 28/64 loss: -0.1863308548927307
Batch 29/64 loss: -0.1872187852859497
Batch 30/64 loss: -0.1905640959739685
Batch 31/64 loss: -0.1876910924911499
Batch 32/64 loss: -0.16815173625946045
Batch 33/64 loss: -0.18247270584106445
Batch 34/64 loss: -0.17992478609085083
Batch 35/64 loss: -0.1743360161781311
Batch 36/64 loss: -0.20352885127067566
Batch 37/64 loss: -0.1815357208251953
Batch 38/64 loss: -0.16855335235595703
Batch 39/64 loss: -0.17986661195755005
Batch 40/64 loss: -0.17108213901519775
Batch 41/64 loss: -0.19593000411987305
Batch 42/64 loss: -0.19401609897613525
Batch 43/64 loss: -0.1861351728439331
Batch 44/64 loss: -0.17797046899795532
Batch 45/64 loss: -0.171880304813385
Batch 46/64 loss: -0.17456424236297607
Batch 47/64 loss: -0.17142194509506226
Batch 48/64 loss: -0.18682944774627686
Batch 49/64 loss: -0.1841118335723877
Batch 50/64 loss: -0.17321348190307617
Batch 51/64 loss: -0.1300467848777771
Batch 52/64 loss: -0.18883919715881348
Batch 53/64 loss: -0.1951008439064026
Batch 54/64 loss: -0.18181610107421875
Batch 55/64 loss: -0.18755888938903809
Batch 56/64 loss: -0.1934402585029602
Batch 57/64 loss: -0.18652141094207764
Batch 58/64 loss: -0.17285698652267456
Batch 59/64 loss: -0.1680620312690735
Batch 60/64 loss: -0.15584838390350342
Batch 61/64 loss: -0.16813135147094727
Batch 62/64 loss: -0.17444300651550293
Batch 63/64 loss: -0.1744028925895691
Batch 64/64 loss: -0.1569654941558838
Epoch 379  Train loss: -0.17369839771121157  Val loss: -0.017864989660859518
Epoch 380
-------------------------------
Batch 1/64 loss: -0.18705874681472778
Batch 2/64 loss: -0.18909567594528198
Batch 3/64 loss: -0.18282723426818848
Batch 4/64 loss: -0.19594228267669678
Batch 5/64 loss: -0.1699334979057312
Batch 6/64 loss: -0.19823896884918213
Batch 7/64 loss: -0.1887112855911255
Batch 8/64 loss: -0.18912559747695923
Batch 9/64 loss: -0.1699114441871643
Batch 10/64 loss: -0.19537901878356934
Batch 11/64 loss: -0.19892257452011108
Batch 12/64 loss: -0.15712028741836548
Batch 13/64 loss: -0.14546668529510498
Batch 14/64 loss: -0.1864323616027832
Batch 15/64 loss: -0.17229288816452026
Batch 16/64 loss: -0.1905263066291809
Batch 17/64 loss: -0.1471250057220459
Batch 18/64 loss: -0.16104364395141602
Batch 19/64 loss: -0.17753124237060547
Batch 20/64 loss: -0.18387478590011597
Batch 21/64 loss: -0.17699170112609863
Batch 22/64 loss: -0.1948012113571167
Batch 23/64 loss: -0.18605899810791016
Batch 24/64 loss: -0.20490288734436035
Batch 25/64 loss: -0.18185865879058838
Batch 26/64 loss: -0.19175952672958374
Batch 27/64 loss: -0.1671953797340393
Batch 28/64 loss: -0.16870015859603882
Batch 29/64 loss: -0.1968429684638977
Batch 30/64 loss: -0.187350332736969
Batch 31/64 loss: -0.18052983283996582
Batch 32/64 loss: -0.1783735156059265
Batch 33/64 loss: -0.1903814673423767
Batch 34/64 loss: -0.1786319613456726
Batch 35/64 loss: -0.15842050313949585
Batch 36/64 loss: -0.1822676658630371
Batch 37/64 loss: -0.18251127004623413
Batch 38/64 loss: -0.1763378381729126
Batch 39/64 loss: -0.16102063655853271
Batch 40/64 loss: -0.17606109380722046
Batch 41/64 loss: -0.18941932916641235
Batch 42/64 loss: -0.14335346221923828
Batch 43/64 loss: -0.15159618854522705
Batch 44/64 loss: -0.17417526245117188
Batch 45/64 loss: -0.18354129791259766
Batch 46/64 loss: -0.18359613418579102
Batch 47/64 loss: -0.18128728866577148
Batch 48/64 loss: -0.15698093175888062
Batch 49/64 loss: -0.1746680736541748
Batch 50/64 loss: -0.17402005195617676
Batch 51/64 loss: -0.1630547046661377
Batch 52/64 loss: -0.18188345432281494
Batch 53/64 loss: -0.14601337909698486
Batch 54/64 loss: -0.17385059595108032
Batch 55/64 loss: -0.16578006744384766
Batch 56/64 loss: -0.18653184175491333
Batch 57/64 loss: -0.153187096118927
Batch 58/64 loss: -0.16549688577651978
Batch 59/64 loss: -0.1522781252861023
Batch 60/64 loss: -0.16769003868103027
Batch 61/64 loss: -0.16545218229293823
Batch 62/64 loss: -0.1664186716079712
Batch 63/64 loss: -0.1794525384902954
Batch 64/64 loss: -0.16809141635894775
Epoch 380  Train loss: -0.17589573813419715  Val loss: -0.01696599226227331
Epoch 381
-------------------------------
Batch 1/64 loss: -0.17940294742584229
Batch 2/64 loss: -0.1674720048904419
Batch 3/64 loss: -0.186390221118927
Batch 4/64 loss: -0.16512387990951538
Batch 5/64 loss: -0.18279659748077393
Batch 6/64 loss: -0.16626065969467163
Batch 7/64 loss: -0.19378769397735596
Batch 8/64 loss: -0.1796587109565735
Batch 9/64 loss: -0.19191062450408936
Batch 10/64 loss: -0.17984598875045776
Batch 11/64 loss: -0.20674338936805725
Batch 12/64 loss: -0.1856170892715454
Batch 13/64 loss: -0.159429669380188
Batch 14/64 loss: -0.18445205688476562
Batch 15/64 loss: -0.18204987049102783
Batch 16/64 loss: -0.17443984746932983
Batch 17/64 loss: -0.17806553840637207
Batch 18/64 loss: -0.18559181690216064
Batch 19/64 loss: -0.19949793815612793
Batch 20/64 loss: -0.15855306386947632
Batch 21/64 loss: -0.1872658133506775
Batch 22/64 loss: -0.1755339503288269
Batch 23/64 loss: -0.18119174242019653
Batch 24/64 loss: -0.18866068124771118
Batch 25/64 loss: -0.16224437952041626
Batch 26/64 loss: -0.16845369338989258
Batch 27/64 loss: -0.20029962062835693
Batch 28/64 loss: -0.19303756952285767
Batch 29/64 loss: -0.16926658153533936
Batch 30/64 loss: -0.1753755807876587
Batch 31/64 loss: -0.18369847536087036
Batch 32/64 loss: -0.15741395950317383
Batch 33/64 loss: -0.15300112962722778
Batch 34/64 loss: -0.18885141611099243
Batch 35/64 loss: -0.1722356677055359
Batch 36/64 loss: -0.177581787109375
Batch 37/64 loss: -0.186603844165802
Batch 38/64 loss: -0.19414639472961426
Batch 39/64 loss: -0.16837048530578613
Batch 40/64 loss: -0.17065513134002686
Batch 41/64 loss: -0.18465960025787354
Batch 42/64 loss: -0.14977377653121948
Batch 43/64 loss: -0.17935514450073242
Batch 44/64 loss: -0.18610358238220215
Batch 45/64 loss: -0.187829852104187
Batch 46/64 loss: -0.18738126754760742
Batch 47/64 loss: -0.17221671342849731
Batch 48/64 loss: -0.17846381664276123
Batch 49/64 loss: -0.1898396611213684
Batch 50/64 loss: -0.18781262636184692
Batch 51/64 loss: -0.17869317531585693
Batch 52/64 loss: -0.19087141752243042
Batch 53/64 loss: -0.16168755292892456
Batch 54/64 loss: -0.1757887601852417
Batch 55/64 loss: -0.18004512786865234
Batch 56/64 loss: -0.19422101974487305
Batch 57/64 loss: -0.17077606916427612
Batch 58/64 loss: -0.200913667678833
Batch 59/64 loss: -0.1826278567314148
Batch 60/64 loss: -0.18034303188323975
Batch 61/64 loss: -0.1911301612854004
Batch 62/64 loss: -0.15629267692565918
Batch 63/64 loss: -0.1412920355796814
Batch 64/64 loss: -0.16097021102905273
Epoch 381  Train loss: -0.17866390218921735  Val loss: -0.015888459084369883
Epoch 382
-------------------------------
Batch 1/64 loss: -0.1815783977508545
Batch 2/64 loss: -0.18595969676971436
Batch 3/64 loss: -0.17039620876312256
Batch 4/64 loss: -0.1738342046737671
Batch 5/64 loss: -0.1707019805908203
Batch 6/64 loss: -0.18463259935379028
Batch 7/64 loss: -0.19253796339035034
Batch 8/64 loss: -0.19604820013046265
Batch 9/64 loss: -0.18491297960281372
Batch 10/64 loss: -0.18213754892349243
Batch 11/64 loss: -0.20071685314178467
Batch 12/64 loss: -0.1625427007675171
Batch 13/64 loss: -0.17121297121047974
Batch 14/64 loss: -0.1951327919960022
Batch 15/64 loss: -0.19069087505340576
Batch 16/64 loss: -0.15865886211395264
Batch 17/64 loss: -0.19650840759277344
Batch 18/64 loss: -0.18627917766571045
Batch 19/64 loss: -0.19025254249572754
Batch 20/64 loss: -0.1882925033569336
Batch 21/64 loss: -0.15892410278320312
Batch 22/64 loss: -0.1557568907737732
Batch 23/64 loss: -0.16355395317077637
Batch 24/64 loss: -0.18899375200271606
Batch 25/64 loss: -0.16272348165512085
Batch 26/64 loss: -0.17767804861068726
Batch 27/64 loss: -0.1615278124809265
Batch 28/64 loss: -0.19312626123428345
Batch 29/64 loss: -0.18727082014083862
Batch 30/64 loss: -0.16373509168624878
Batch 31/64 loss: -0.1520671248435974
Batch 32/64 loss: -0.1811603307723999
Batch 33/64 loss: -0.18944895267486572
Batch 34/64 loss: -0.1679987907409668
Batch 35/64 loss: -0.17315739393234253
Batch 36/64 loss: -0.16907918453216553
Batch 37/64 loss: -0.17364633083343506
Batch 38/64 loss: -0.1508755087852478
Batch 39/64 loss: -0.16320061683654785
Batch 40/64 loss: -0.18397796154022217
Batch 41/64 loss: -0.16558104753494263
Batch 42/64 loss: -0.17343521118164062
Batch 43/64 loss: -0.17199212312698364
Batch 44/64 loss: -0.18194794654846191
Batch 45/64 loss: -0.18288087844848633
Batch 46/64 loss: -0.18649762868881226
Batch 47/64 loss: -0.17716246843338013
Batch 48/64 loss: -0.17719990015029907
Batch 49/64 loss: -0.1708391308784485
Batch 50/64 loss: -0.16805660724639893
Batch 51/64 loss: -0.18581050634384155
Batch 52/64 loss: -0.17537343502044678
Batch 53/64 loss: -0.179510235786438
Batch 54/64 loss: -0.18955224752426147
Batch 55/64 loss: -0.1774768829345703
Batch 56/64 loss: -0.168967604637146
Batch 57/64 loss: -0.17186760902404785
Batch 58/64 loss: -0.20027470588684082
Batch 59/64 loss: -0.1662042737007141
Batch 60/64 loss: -0.17749416828155518
Batch 61/64 loss: -0.15900194644927979
Batch 62/64 loss: -0.1621023416519165
Batch 63/64 loss: -0.14532238245010376
Batch 64/64 loss: -0.16917932033538818
Epoch 382  Train loss: -0.17603710819693172  Val loss: -0.019931654545040065
Epoch 383
-------------------------------
Batch 1/64 loss: -0.19445770978927612
Batch 2/64 loss: -0.15320098400115967
Batch 3/64 loss: -0.18512719869613647
Batch 4/64 loss: -0.15543204545974731
Batch 5/64 loss: -0.1673615574836731
Batch 6/64 loss: -0.16909480094909668
Batch 7/64 loss: -0.1780102252960205
Batch 8/64 loss: -0.15961045026779175
Batch 9/64 loss: -0.18583637475967407
Batch 10/64 loss: -0.19309860467910767
Batch 11/64 loss: -0.17853301763534546
Batch 12/64 loss: -0.1836947202682495
Batch 13/64 loss: -0.17352044582366943
Batch 14/64 loss: -0.1856653094291687
Batch 15/64 loss: -0.18713301420211792
Batch 16/64 loss: -0.17410951852798462
Batch 17/64 loss: -0.1769348382949829
Batch 18/64 loss: -0.1888013482093811
Batch 19/64 loss: -0.17137354612350464
Batch 20/64 loss: -0.201033353805542
Batch 21/64 loss: -0.19510042667388916
Batch 22/64 loss: -0.17165768146514893
Batch 23/64 loss: -0.18180489540100098
Batch 24/64 loss: -0.17261159420013428
Batch 25/64 loss: -0.17286312580108643
Batch 26/64 loss: -0.19238567352294922
Batch 27/64 loss: -0.19420665502548218
Batch 28/64 loss: -0.17730653285980225
Batch 29/64 loss: -0.21001505851745605
Batch 30/64 loss: -0.18975675106048584
Batch 31/64 loss: -0.20116591453552246
Batch 32/64 loss: -0.2070714235305786
Batch 33/64 loss: -0.20143097639083862
Batch 34/64 loss: -0.1981869339942932
Batch 35/64 loss: -0.16546940803527832
Batch 36/64 loss: -0.16429847478866577
Batch 37/64 loss: -0.14536714553833008
Batch 38/64 loss: -0.1833450198173523
Batch 39/64 loss: -0.19114065170288086
Batch 40/64 loss: -0.1892002820968628
Batch 41/64 loss: -0.18449270725250244
Batch 42/64 loss: -0.17080605030059814
Batch 43/64 loss: -0.17489218711853027
Batch 44/64 loss: -0.18337923288345337
Batch 45/64 loss: -0.1544187068939209
Batch 46/64 loss: -0.18560177087783813
Batch 47/64 loss: -0.16744279861450195
Batch 48/64 loss: -0.18935281038284302
Batch 49/64 loss: -0.1712571382522583
Batch 50/64 loss: -0.1575930118560791
Batch 51/64 loss: -0.20081186294555664
Batch 52/64 loss: -0.18474996089935303
Batch 53/64 loss: -0.16483557224273682
Batch 54/64 loss: -0.187058687210083
Batch 55/64 loss: -0.1876816749572754
Batch 56/64 loss: -0.17079836130142212
Batch 57/64 loss: -0.16515874862670898
Batch 58/64 loss: -0.16472870111465454
Batch 59/64 loss: -0.18498504161834717
Batch 60/64 loss: -0.14918625354766846
Batch 61/64 loss: -0.17505401372909546
Batch 62/64 loss: -0.16224956512451172
Batch 63/64 loss: -0.1761695146560669
Batch 64/64 loss: -0.18885529041290283
Epoch 383  Train loss: -0.17914916908039766  Val loss: -0.017052571183627414
Epoch 384
-------------------------------
Batch 1/64 loss: -0.17087852954864502
Batch 2/64 loss: -0.18497538566589355
Batch 3/64 loss: -0.1931452751159668
Batch 4/64 loss: -0.18409907817840576
Batch 5/64 loss: -0.1838526725769043
Batch 6/64 loss: -0.159654438495636
Batch 7/64 loss: -0.17257153987884521
Batch 8/64 loss: -0.17580384016036987
Batch 9/64 loss: -0.20891696214675903
Batch 10/64 loss: -0.19650328159332275
Batch 11/64 loss: -0.19574570655822754
Batch 12/64 loss: -0.19943493604660034
Batch 13/64 loss: -0.2105885148048401
Batch 14/64 loss: -0.17002582550048828
Batch 15/64 loss: -0.1450534462928772
Batch 16/64 loss: -0.2042468786239624
Batch 17/64 loss: -0.19174563884735107
Batch 18/64 loss: -0.15428310632705688
Batch 19/64 loss: -0.19510376453399658
Batch 20/64 loss: -0.15341901779174805
Batch 21/64 loss: -0.20804712176322937
Batch 22/64 loss: -0.1573510766029358
Batch 23/64 loss: -0.17366617918014526
Batch 24/64 loss: -0.17062824964523315
Batch 25/64 loss: -0.2114701271057129
Batch 26/64 loss: -0.18656253814697266
Batch 27/64 loss: -0.18576818704605103
Batch 28/64 loss: -0.16173696517944336
Batch 29/64 loss: -0.17678558826446533
Batch 30/64 loss: -0.20488348603248596
Batch 31/64 loss: -0.17800605297088623
Batch 32/64 loss: -0.1898370385169983
Batch 33/64 loss: -0.19087445735931396
Batch 34/64 loss: -0.16474121809005737
Batch 35/64 loss: -0.17522042989730835
Batch 36/64 loss: -0.17668724060058594
Batch 37/64 loss: -0.13848751783370972
Batch 38/64 loss: -0.18848329782485962
Batch 39/64 loss: -0.18697959184646606
Batch 40/64 loss: -0.16141045093536377
Batch 41/64 loss: -0.14897996187210083
Batch 42/64 loss: -0.18857938051223755
Batch 43/64 loss: -0.17031729221343994
Batch 44/64 loss: -0.15160852670669556
Batch 45/64 loss: -0.12397181987762451
Batch 46/64 loss: -0.17223834991455078
Batch 47/64 loss: -0.1909283995628357
Batch 48/64 loss: -0.19173377752304077
Batch 49/64 loss: -0.17381727695465088
Batch 50/64 loss: -0.19347476959228516
Batch 51/64 loss: -0.15350675582885742
Batch 52/64 loss: -0.17832434177398682
Batch 53/64 loss: -0.18560904264450073
Batch 54/64 loss: -0.1724543571472168
Batch 55/64 loss: -0.18951255083084106
Batch 56/64 loss: -0.16744226217269897
Batch 57/64 loss: -0.1708105206489563
Batch 58/64 loss: -0.1690659523010254
Batch 59/64 loss: -0.16524112224578857
Batch 60/64 loss: -0.18263661861419678
Batch 61/64 loss: -0.1888827085494995
Batch 62/64 loss: -0.17485970258712769
Batch 63/64 loss: -0.1954013705253601
Batch 64/64 loss: -0.17952215671539307
Epoch 384  Train loss: -0.17837981383005777  Val loss: -0.01830602736817193
Epoch 385
-------------------------------
Batch 1/64 loss: -0.17401564121246338
Batch 2/64 loss: -0.18963080644607544
Batch 3/64 loss: -0.1701985001564026
Batch 4/64 loss: -0.17079848051071167
Batch 5/64 loss: -0.20347756147384644
Batch 6/64 loss: -0.20478123426437378
Batch 7/64 loss: -0.18546760082244873
Batch 8/64 loss: -0.1947866678237915
Batch 9/64 loss: -0.14469611644744873
Batch 10/64 loss: -0.19105297327041626
Batch 11/64 loss: -0.17745333909988403
Batch 12/64 loss: -0.17330998182296753
Batch 13/64 loss: -0.20104533433914185
Batch 14/64 loss: -0.17324578762054443
Batch 15/64 loss: -0.19759660959243774
Batch 16/64 loss: -0.18392211198806763
Batch 17/64 loss: -0.14380306005477905
Batch 18/64 loss: -0.20215630531311035
Batch 19/64 loss: -0.17491024732589722
Batch 20/64 loss: -0.16376101970672607
Batch 21/64 loss: -0.19034194946289062
Batch 22/64 loss: -0.17137300968170166
Batch 23/64 loss: -0.17915505170822144
Batch 24/64 loss: -0.14951694011688232
Batch 25/64 loss: -0.18751060962677002
Batch 26/64 loss: -0.20336642861366272
Batch 27/64 loss: -0.15783607959747314
Batch 28/64 loss: -0.17862051725387573
Batch 29/64 loss: -0.18738961219787598
Batch 30/64 loss: -0.18305593729019165
Batch 31/64 loss: -0.1834183931350708
Batch 32/64 loss: -0.18042641878128052
Batch 33/64 loss: -0.16537046432495117
Batch 34/64 loss: -0.17439204454421997
Batch 35/64 loss: -0.17336058616638184
Batch 36/64 loss: -0.17183029651641846
Batch 37/64 loss: -0.19001781940460205
Batch 38/64 loss: -0.1730818748474121
Batch 39/64 loss: -0.14932584762573242
Batch 40/64 loss: -0.17524486780166626
Batch 41/64 loss: -0.17763084173202515
Batch 42/64 loss: -0.17833435535430908
Batch 43/64 loss: -0.19564908742904663
Batch 44/64 loss: -0.1920110583305359
Batch 45/64 loss: -0.1667802333831787
Batch 46/64 loss: -0.16373980045318604
Batch 47/64 loss: -0.180273175239563
Batch 48/64 loss: -0.1935422420501709
Batch 49/64 loss: -0.19059264659881592
Batch 50/64 loss: -0.13451188802719116
Batch 51/64 loss: -0.16428136825561523
Batch 52/64 loss: -0.1763514280319214
Batch 53/64 loss: -0.17543095350265503
Batch 54/64 loss: -0.19414031505584717
Batch 55/64 loss: -0.1910359263420105
Batch 56/64 loss: -0.1755627989768982
Batch 57/64 loss: -0.19021010398864746
Batch 58/64 loss: -0.18522274494171143
Batch 59/64 loss: -0.17418432235717773
Batch 60/64 loss: -0.1696528196334839
Batch 61/64 loss: -0.1886003613471985
Batch 62/64 loss: -0.1640828251838684
Batch 63/64 loss: -0.17257726192474365
Batch 64/64 loss: -0.18301695585250854
Epoch 385  Train loss: -0.17845341808655682  Val loss: -0.020787817915690315
Epoch 386
-------------------------------
Batch 1/64 loss: -0.15941846370697021
Batch 2/64 loss: -0.18697744607925415
Batch 3/64 loss: -0.1782212257385254
Batch 4/64 loss: -0.19374483823776245
Batch 5/64 loss: -0.19206541776657104
Batch 6/64 loss: -0.17977213859558105
Batch 7/64 loss: -0.18368452787399292
Batch 8/64 loss: -0.18486547470092773
Batch 9/64 loss: -0.15863770246505737
Batch 10/64 loss: -0.1925821304321289
Batch 11/64 loss: -0.17449891567230225
Batch 12/64 loss: -0.1926816701889038
Batch 13/64 loss: -0.19704848527908325
Batch 14/64 loss: -0.1725916862487793
Batch 15/64 loss: -0.1655259132385254
Batch 16/64 loss: -0.1732041835784912
Batch 17/64 loss: -0.1832793951034546
Batch 18/64 loss: -0.14715886116027832
Batch 19/64 loss: -0.17058104276657104
Batch 20/64 loss: -0.1901015043258667
Batch 21/64 loss: -0.18087446689605713
Batch 22/64 loss: -0.17781126499176025
Batch 23/64 loss: -0.1774911880493164
Batch 24/64 loss: -0.14918464422225952
Batch 25/64 loss: -0.20279186964035034
Batch 26/64 loss: -0.19665193557739258
Batch 27/64 loss: -0.16261404752731323
Batch 28/64 loss: -0.19809246063232422
Batch 29/64 loss: -0.1675127148628235
Batch 30/64 loss: -0.1734321117401123
Batch 31/64 loss: -0.2046513557434082
Batch 32/64 loss: -0.18417155742645264
Batch 33/64 loss: -0.180178701877594
Batch 34/64 loss: -0.16150903701782227
Batch 35/64 loss: -0.15684300661087036
Batch 36/64 loss: -0.1890331506729126
Batch 37/64 loss: -0.1809171438217163
Batch 38/64 loss: -0.19870704412460327
Batch 39/64 loss: -0.17479026317596436
Batch 40/64 loss: -0.16884714365005493
Batch 41/64 loss: -0.16055887937545776
Batch 42/64 loss: -0.18698549270629883
Batch 43/64 loss: -0.16669905185699463
Batch 44/64 loss: -0.1945359706878662
Batch 45/64 loss: -0.16375130414962769
Batch 46/64 loss: -0.16564518213272095
Batch 47/64 loss: -0.17108583450317383
Batch 48/64 loss: -0.14788973331451416
Batch 49/64 loss: -0.19179368019104004
Batch 50/64 loss: -0.18064743280410767
Batch 51/64 loss: -0.15590202808380127
Batch 52/64 loss: -0.18854355812072754
Batch 53/64 loss: -0.1775517463684082
Batch 54/64 loss: -0.1754792332649231
Batch 55/64 loss: -0.17149078845977783
Batch 56/64 loss: -0.18549120426177979
Batch 57/64 loss: -0.17678409814834595
Batch 58/64 loss: -0.17296522855758667
Batch 59/64 loss: -0.17324411869049072
Batch 60/64 loss: -0.17669260501861572
Batch 61/64 loss: -0.17159724235534668
Batch 62/64 loss: -0.16075026988983154
Batch 63/64 loss: -0.18944334983825684
Batch 64/64 loss: -0.14944279193878174
Epoch 386  Train loss: -0.1769467648337869  Val loss: -0.01977588814968096
Epoch 387
-------------------------------
Batch 1/64 loss: -0.19315290451049805
Batch 2/64 loss: -0.19217509031295776
Batch 3/64 loss: -0.19980907440185547
Batch 4/64 loss: -0.16348087787628174
Batch 5/64 loss: -0.16881728172302246
Batch 6/64 loss: -0.17994588613510132
Batch 7/64 loss: -0.18601995706558228
Batch 8/64 loss: -0.20960170030593872
Batch 9/64 loss: -0.14031052589416504
Batch 10/64 loss: -0.18611979484558105
Batch 11/64 loss: -0.18715643882751465
Batch 12/64 loss: -0.1818704605102539
Batch 13/64 loss: -0.18986797332763672
Batch 14/64 loss: -0.18696284294128418
Batch 15/64 loss: -0.14546418190002441
Batch 16/64 loss: -0.19522607326507568
Batch 17/64 loss: -0.1777161955833435
Batch 18/64 loss: -0.18190324306488037
Batch 19/64 loss: -0.19668328762054443
Batch 20/64 loss: -0.1750500202178955
Batch 21/64 loss: -0.1831488013267517
Batch 22/64 loss: -0.20372137427330017
Batch 23/64 loss: -0.1890067458152771
Batch 24/64 loss: -0.19129526615142822
Batch 25/64 loss: -0.18837970495224
Batch 26/64 loss: -0.16457748413085938
Batch 27/64 loss: -0.1930791139602661
Batch 28/64 loss: -0.19571393728256226
Batch 29/64 loss: -0.18506968021392822
Batch 30/64 loss: -0.20854759216308594
Batch 31/64 loss: -0.17301321029663086
Batch 32/64 loss: -0.18599992990493774
Batch 33/64 loss: -0.14427632093429565
Batch 34/64 loss: -0.1639927625656128
Batch 35/64 loss: -0.19274795055389404
Batch 36/64 loss: -0.19390225410461426
Batch 37/64 loss: -0.17265623807907104
Batch 38/64 loss: -0.16487574577331543
Batch 39/64 loss: -0.1732202172279358
Batch 40/64 loss: -0.190213143825531
Batch 41/64 loss: -0.1809520125389099
Batch 42/64 loss: -0.18375849723815918
Batch 43/64 loss: -0.14807820320129395
Batch 44/64 loss: -0.1828266978263855
Batch 45/64 loss: -0.20070606470108032
Batch 46/64 loss: -0.2023775577545166
Batch 47/64 loss: -0.18309301137924194
Batch 48/64 loss: -0.1593766212463379
Batch 49/64 loss: -0.17510539293289185
Batch 50/64 loss: -0.18037784099578857
Batch 51/64 loss: -0.17681819200515747
Batch 52/64 loss: -0.18742024898529053
Batch 53/64 loss: -0.17467021942138672
Batch 54/64 loss: -0.15958130359649658
Batch 55/64 loss: -0.17987221479415894
Batch 56/64 loss: -0.1459369659423828
Batch 57/64 loss: -0.1707502007484436
Batch 58/64 loss: -0.14480340480804443
Batch 59/64 loss: -0.16502630710601807
Batch 60/64 loss: -0.1456080675125122
Batch 61/64 loss: -0.17122656106948853
Batch 62/64 loss: -0.1710059642791748
Batch 63/64 loss: -0.17383575439453125
Batch 64/64 loss: -0.16807091236114502
Epoch 387  Train loss: -0.1785730474135455  Val loss: -0.017906614390435498
Epoch 388
-------------------------------
Batch 1/64 loss: -0.15114104747772217
Batch 2/64 loss: -0.186437726020813
Batch 3/64 loss: -0.1755307912826538
Batch 4/64 loss: -0.20559868216514587
Batch 5/64 loss: -0.1623055338859558
Batch 6/64 loss: -0.17830753326416016
Batch 7/64 loss: -0.17085206508636475
Batch 8/64 loss: -0.17037665843963623
Batch 9/64 loss: -0.18687653541564941
Batch 10/64 loss: -0.1778099536895752
Batch 11/64 loss: -0.17635565996170044
Batch 12/64 loss: -0.18431353569030762
Batch 13/64 loss: -0.14617931842803955
Batch 14/64 loss: -0.19371575117111206
Batch 15/64 loss: -0.18612831830978394
Batch 16/64 loss: -0.18462485074996948
Batch 17/64 loss: -0.19427955150604248
Batch 18/64 loss: -0.19207799434661865
Batch 19/64 loss: -0.18985402584075928
Batch 20/64 loss: -0.18430417776107788
Batch 21/64 loss: -0.18159759044647217
Batch 22/64 loss: -0.16378116607666016
Batch 23/64 loss: -0.2032395601272583
Batch 24/64 loss: -0.19186103343963623
Batch 25/64 loss: -0.174332857131958
Batch 26/64 loss: -0.18264037370681763
Batch 27/64 loss: -0.17021441459655762
Batch 28/64 loss: -0.19040447473526
Batch 29/64 loss: -0.17803466320037842
Batch 30/64 loss: -0.18383419513702393
Batch 31/64 loss: -0.18352800607681274
Batch 32/64 loss: -0.18324613571166992
Batch 33/64 loss: -0.1740128993988037
Batch 34/64 loss: -0.1855604648590088
Batch 35/64 loss: -0.19538354873657227
Batch 36/64 loss: -0.18889188766479492
Batch 37/64 loss: -0.1938847303390503
Batch 38/64 loss: -0.19162046909332275
Batch 39/64 loss: -0.1881260871887207
Batch 40/64 loss: -0.1919410228729248
Batch 41/64 loss: -0.1660260558128357
Batch 42/64 loss: -0.17747515439987183
Batch 43/64 loss: -0.18790006637573242
Batch 44/64 loss: -0.19121646881103516
Batch 45/64 loss: -0.18505501747131348
Batch 46/64 loss: -0.19477200508117676
Batch 47/64 loss: -0.18226516246795654
Batch 48/64 loss: -0.18580937385559082
Batch 49/64 loss: -0.1986687183380127
Batch 50/64 loss: -0.18730968236923218
Batch 51/64 loss: -0.17264842987060547
Batch 52/64 loss: -0.17209434509277344
Batch 53/64 loss: -0.18295109272003174
Batch 54/64 loss: -0.18828463554382324
Batch 55/64 loss: -0.18012523651123047
Batch 56/64 loss: -0.17709410190582275
Batch 57/64 loss: -0.15168803930282593
Batch 58/64 loss: -0.17087632417678833
Batch 59/64 loss: -0.18503320217132568
Batch 60/64 loss: -0.16435480117797852
Batch 61/64 loss: -0.17799371480941772
Batch 62/64 loss: -0.16508638858795166
Batch 63/64 loss: -0.184393048286438
Batch 64/64 loss: -0.19204175472259521
Epoch 388  Train loss: -0.18143306152493346  Val loss: -0.017636069727107833
Epoch 389
-------------------------------
Batch 1/64 loss: -0.19851648807525635
Batch 2/64 loss: -0.17835873365402222
Batch 3/64 loss: -0.18702638149261475
Batch 4/64 loss: -0.16389542818069458
Batch 5/64 loss: -0.17572253942489624
Batch 6/64 loss: -0.17718380689620972
Batch 7/64 loss: -0.19773119688034058
Batch 8/64 loss: -0.18435156345367432
Batch 9/64 loss: -0.15043187141418457
Batch 10/64 loss: -0.17195427417755127
Batch 11/64 loss: -0.20515069365501404
Batch 12/64 loss: -0.17016875743865967
Batch 13/64 loss: -0.17062759399414062
Batch 14/64 loss: -0.1893254518508911
Batch 15/64 loss: -0.17801398038864136
Batch 16/64 loss: -0.1626344919204712
Batch 17/64 loss: -0.1732426881790161
Batch 18/64 loss: -0.19079142808914185
Batch 19/64 loss: -0.18996667861938477
Batch 20/64 loss: -0.19204628467559814
Batch 21/64 loss: -0.17307442426681519
Batch 22/64 loss: -0.19333350658416748
Batch 23/64 loss: -0.1692568063735962
Batch 24/64 loss: -0.16025960445404053
Batch 25/64 loss: -0.18639057874679565
Batch 26/64 loss: -0.18421447277069092
Batch 27/64 loss: -0.16994541883468628
Batch 28/64 loss: -0.1409895420074463
Batch 29/64 loss: -0.19403231143951416
Batch 30/64 loss: -0.18449783325195312
Batch 31/64 loss: -0.1439153552055359
Batch 32/64 loss: -0.1518803834915161
Batch 33/64 loss: -0.17459207773208618
Batch 34/64 loss: -0.17658722400665283
Batch 35/64 loss: -0.15543758869171143
Batch 36/64 loss: -0.17910653352737427
Batch 37/64 loss: -0.18281829357147217
Batch 38/64 loss: -0.19504433870315552
Batch 39/64 loss: -0.16824668645858765
Batch 40/64 loss: -0.18162059783935547
Batch 41/64 loss: -0.2015303373336792
Batch 42/64 loss: -0.1507091522216797
Batch 43/64 loss: -0.18424004316329956
Batch 44/64 loss: -0.16920256614685059
Batch 45/64 loss: -0.1975651979446411
Batch 46/64 loss: -0.1950923204421997
Batch 47/64 loss: -0.18720972537994385
Batch 48/64 loss: -0.1778525710105896
Batch 49/64 loss: -0.1663593053817749
Batch 50/64 loss: -0.169181227684021
Batch 51/64 loss: -0.17792487144470215
Batch 52/64 loss: -0.17923277616500854
Batch 53/64 loss: -0.1835336685180664
Batch 54/64 loss: -0.18929928541183472
Batch 55/64 loss: -0.18840497732162476
Batch 56/64 loss: -0.1725482940673828
Batch 57/64 loss: -0.18801289796829224
Batch 58/64 loss: -0.18067622184753418
Batch 59/64 loss: -0.18213850259780884
Batch 60/64 loss: -0.19576811790466309
Batch 61/64 loss: -0.16973090171813965
Batch 62/64 loss: -0.1950773000717163
Batch 63/64 loss: -0.16208875179290771
Batch 64/64 loss: -0.18142038583755493
Epoch 389  Train loss: -0.1783816190326915  Val loss: -0.019110109388213798
Epoch 390
-------------------------------
Batch 1/64 loss: -0.1920762062072754
Batch 2/64 loss: -0.17694365978240967
Batch 3/64 loss: -0.15858721733093262
Batch 4/64 loss: -0.1799701452255249
Batch 5/64 loss: -0.18352144956588745
Batch 6/64 loss: -0.16501784324645996
Batch 7/64 loss: -0.17857491970062256
Batch 8/64 loss: -0.2037014663219452
Batch 9/64 loss: -0.1956377625465393
Batch 10/64 loss: -0.16994518041610718
Batch 11/64 loss: -0.14818596839904785
Batch 12/64 loss: -0.19681918621063232
Batch 13/64 loss: -0.17194247245788574
Batch 14/64 loss: -0.19998395442962646
Batch 15/64 loss: -0.17585021257400513
Batch 16/64 loss: -0.17219579219818115
Batch 17/64 loss: -0.1865556240081787
Batch 18/64 loss: -0.18047446012496948
Batch 19/64 loss: -0.16962593793869019
Batch 20/64 loss: -0.17105382680892944
Batch 21/64 loss: -0.180863618850708
Batch 22/64 loss: -0.19778746366500854
Batch 23/64 loss: -0.17245376110076904
Batch 24/64 loss: -0.20376157760620117
Batch 25/64 loss: -0.16146862506866455
Batch 26/64 loss: -0.19025468826293945
Batch 27/64 loss: -0.18082821369171143
Batch 28/64 loss: -0.17642337083816528
Batch 29/64 loss: -0.18245625495910645
Batch 30/64 loss: -0.17034530639648438
Batch 31/64 loss: -0.16613483428955078
Batch 32/64 loss: -0.18018484115600586
Batch 33/64 loss: -0.1765027642250061
Batch 34/64 loss: -0.1875872015953064
Batch 35/64 loss: -0.1860438585281372
Batch 36/64 loss: -0.19118696451187134
Batch 37/64 loss: -0.17793738842010498
Batch 38/64 loss: -0.18881487846374512
Batch 39/64 loss: -0.181293785572052
Batch 40/64 loss: -0.17709732055664062
Batch 41/64 loss: -0.14816498756408691
Batch 42/64 loss: -0.16974258422851562
Batch 43/64 loss: -0.17648178339004517
Batch 44/64 loss: -0.17654192447662354
Batch 45/64 loss: -0.1639583706855774
Batch 46/64 loss: -0.18560725450515747
Batch 47/64 loss: -0.19086158275604248
Batch 48/64 loss: -0.16282939910888672
Batch 49/64 loss: -0.15448153018951416
Batch 50/64 loss: -0.18260669708251953
Batch 51/64 loss: -0.19603657722473145
Batch 52/64 loss: -0.16358208656311035
Batch 53/64 loss: -0.19265049695968628
Batch 54/64 loss: -0.19533264636993408
Batch 55/64 loss: -0.16902267932891846
Batch 56/64 loss: -0.2025541365146637
Batch 57/64 loss: -0.18951541185379028
Batch 58/64 loss: -0.16906464099884033
Batch 59/64 loss: -0.15617114305496216
Batch 60/64 loss: -0.18378031253814697
Batch 61/64 loss: -0.18519139289855957
Batch 62/64 loss: -0.1834896206855774
Batch 63/64 loss: -0.17338860034942627
Batch 64/64 loss: -0.14482325315475464
Epoch 390  Train loss: -0.1786002086658104  Val loss: -0.015171859887047731
Epoch 391
-------------------------------
Batch 1/64 loss: -0.1711118221282959
Batch 2/64 loss: -0.18907225131988525
Batch 3/64 loss: -0.1921667456626892
Batch 4/64 loss: -0.19406133890151978
Batch 5/64 loss: -0.1742991805076599
Batch 6/64 loss: -0.1809050440788269
Batch 7/64 loss: -0.18001306056976318
Batch 8/64 loss: -0.20101922750473022
Batch 9/64 loss: -0.18558287620544434
Batch 10/64 loss: -0.2061339020729065
Batch 11/64 loss: -0.14534777402877808
Batch 12/64 loss: -0.17928504943847656
Batch 13/64 loss: -0.14193886518478394
Batch 14/64 loss: -0.19084769487380981
Batch 15/64 loss: -0.18914318084716797
Batch 16/64 loss: -0.19836026430130005
Batch 17/64 loss: -0.13971161842346191
Batch 18/64 loss: -0.18040287494659424
Batch 19/64 loss: -0.16962528228759766
Batch 20/64 loss: -0.17980808019638062
Batch 21/64 loss: -0.18165254592895508
Batch 22/64 loss: -0.18599790334701538
Batch 23/64 loss: -0.18346452713012695
Batch 24/64 loss: -0.1699036955833435
Batch 25/64 loss: -0.19915777444839478
Batch 26/64 loss: -0.18536275625228882
Batch 27/64 loss: -0.1445515751838684
Batch 28/64 loss: -0.16333264112472534
Batch 29/64 loss: -0.18631118535995483
Batch 30/64 loss: -0.15248167514801025
Batch 31/64 loss: -0.19481295347213745
Batch 32/64 loss: -0.17168468236923218
Batch 33/64 loss: -0.2026599943637848
Batch 34/64 loss: -0.18870091438293457
Batch 35/64 loss: -0.13542073965072632
Batch 36/64 loss: -0.19760346412658691
Batch 37/64 loss: -0.1878940463066101
Batch 38/64 loss: -0.17754560708999634
Batch 39/64 loss: -0.16255366802215576
Batch 40/64 loss: -0.1886489987373352
Batch 41/64 loss: -0.18227732181549072
Batch 42/64 loss: -0.18083149194717407
Batch 43/64 loss: -0.16562914848327637
Batch 44/64 loss: -0.1878928542137146
Batch 45/64 loss: -0.17438042163848877
Batch 46/64 loss: -0.139845073223114
Batch 47/64 loss: -0.17013102769851685
Batch 48/64 loss: -0.20176860690116882
Batch 49/64 loss: -0.1834501028060913
Batch 50/64 loss: -0.1749669313430786
Batch 51/64 loss: -0.18511134386062622
Batch 52/64 loss: -0.19478070735931396
Batch 53/64 loss: -0.174099862575531
Batch 54/64 loss: -0.1678781509399414
Batch 55/64 loss: -0.16378116607666016
Batch 56/64 loss: -0.17394185066223145
Batch 57/64 loss: -0.17713004350662231
Batch 58/64 loss: -0.1825317144393921
Batch 59/64 loss: -0.15892016887664795
Batch 60/64 loss: -0.16979986429214478
Batch 61/64 loss: -0.1778131127357483
Batch 62/64 loss: -0.15748810768127441
Batch 63/64 loss: -0.1572827696800232
Batch 64/64 loss: -0.15620338916778564
Epoch 391  Train loss: -0.17674449986102533  Val loss: -0.016850235535926426
Epoch 392
-------------------------------
Batch 1/64 loss: -0.15413379669189453
Batch 2/64 loss: -0.20741301774978638
Batch 3/64 loss: -0.1744059920310974
Batch 4/64 loss: -0.14696651697158813
Batch 5/64 loss: -0.16094684600830078
Batch 6/64 loss: -0.15503829717636108
Batch 7/64 loss: -0.17684710025787354
Batch 8/64 loss: -0.18638968467712402
Batch 9/64 loss: -0.18322336673736572
Batch 10/64 loss: -0.16145974397659302
Batch 11/64 loss: -0.18933475017547607
Batch 12/64 loss: -0.16441798210144043
Batch 13/64 loss: -0.19261378049850464
Batch 14/64 loss: -0.18875205516815186
Batch 15/64 loss: -0.19419151544570923
Batch 16/64 loss: -0.20521140098571777
Batch 17/64 loss: -0.1837170124053955
Batch 18/64 loss: -0.1656174659729004
Batch 19/64 loss: -0.18066972494125366
Batch 20/64 loss: -0.19028133153915405
Batch 21/64 loss: -0.2073129117488861
Batch 22/64 loss: -0.17863094806671143
Batch 23/64 loss: -0.171406090259552
Batch 24/64 loss: -0.19524753093719482
Batch 25/64 loss: -0.18962574005126953
Batch 26/64 loss: -0.19030320644378662
Batch 27/64 loss: -0.16185444593429565
Batch 28/64 loss: -0.1860356330871582
Batch 29/64 loss: -0.17933309078216553
Batch 30/64 loss: -0.1975305676460266
Batch 31/64 loss: -0.1617167592048645
Batch 32/64 loss: -0.18832886219024658
Batch 33/64 loss: -0.15997308492660522
Batch 34/64 loss: -0.18238449096679688
Batch 35/64 loss: -0.19769686460494995
Batch 36/64 loss: -0.19563770294189453
Batch 37/64 loss: -0.19096285104751587
Batch 38/64 loss: -0.18961715698242188
Batch 39/64 loss: -0.1934276819229126
Batch 40/64 loss: -0.1580195426940918
Batch 41/64 loss: -0.1742228865623474
Batch 42/64 loss: -0.19386255741119385
Batch 43/64 loss: -0.1323055624961853
Batch 44/64 loss: -0.18138670921325684
Batch 45/64 loss: -0.16153663396835327
Batch 46/64 loss: -0.14635324478149414
Batch 47/64 loss: -0.17323291301727295
Batch 48/64 loss: -0.17349904775619507
Batch 49/64 loss: -0.13382333517074585
Batch 50/64 loss: -0.1736200451850891
Batch 51/64 loss: -0.18014854192733765
Batch 52/64 loss: -0.18972885608673096
Batch 53/64 loss: -0.1887759566307068
Batch 54/64 loss: -0.18782156705856323
Batch 55/64 loss: -0.17859786748886108
Batch 56/64 loss: -0.13839656114578247
Batch 57/64 loss: -0.17233121395111084
Batch 58/64 loss: -0.1845405101776123
Batch 59/64 loss: -0.16652613878250122
Batch 60/64 loss: -0.17992067337036133
Batch 61/64 loss: -0.18520814180374146
Batch 62/64 loss: -0.21066543459892273
Batch 63/64 loss: -0.19651412963867188
Batch 64/64 loss: -0.18404000997543335
Epoch 392  Train loss: -0.17847411889655918  Val loss: -0.019593876252059675
Epoch 393
-------------------------------
Batch 1/64 loss: -0.18517708778381348
Batch 2/64 loss: -0.17842745780944824
Batch 3/64 loss: -0.1830122470855713
Batch 4/64 loss: -0.16927415132522583
Batch 5/64 loss: -0.17589962482452393
Batch 6/64 loss: -0.20183312892913818
Batch 7/64 loss: -0.17629551887512207
Batch 8/64 loss: -0.18352562189102173
Batch 9/64 loss: -0.1631060242652893
Batch 10/64 loss: -0.17596757411956787
Batch 11/64 loss: -0.17780530452728271
Batch 12/64 loss: -0.19992440938949585
Batch 13/64 loss: -0.17527389526367188
Batch 14/64 loss: -0.17152917385101318
Batch 15/64 loss: -0.19144362211227417
Batch 16/64 loss: -0.1835184097290039
Batch 17/64 loss: -0.1921941637992859
Batch 18/64 loss: -0.17067307233810425
Batch 19/64 loss: -0.17695975303649902
Batch 20/64 loss: -0.21083572506904602
Batch 21/64 loss: -0.18935626745224
Batch 22/64 loss: -0.1731242537498474
Batch 23/64 loss: -0.1898699402809143
Batch 24/64 loss: -0.1741420030593872
Batch 25/64 loss: -0.18955254554748535
Batch 26/64 loss: -0.19239795207977295
Batch 27/64 loss: -0.17812693119049072
Batch 28/64 loss: -0.18409991264343262
Batch 29/64 loss: -0.18436288833618164
Batch 30/64 loss: -0.18537163734436035
Batch 31/64 loss: -0.18816083669662476
Batch 32/64 loss: -0.17599838972091675
Batch 33/64 loss: -0.1985948085784912
Batch 34/64 loss: -0.1646639108657837
Batch 35/64 loss: -0.17132997512817383
Batch 36/64 loss: -0.18334543704986572
Batch 37/64 loss: -0.19457823038101196
Batch 38/64 loss: -0.1881939172744751
Batch 39/64 loss: -0.18074488639831543
Batch 40/64 loss: -0.20058578252792358
Batch 41/64 loss: -0.18290114402770996
Batch 42/64 loss: -0.19452881813049316
Batch 43/64 loss: -0.19667327404022217
Batch 44/64 loss: -0.19871115684509277
Batch 45/64 loss: -0.16630160808563232
Batch 46/64 loss: -0.17681622505187988
Batch 47/64 loss: -0.18109214305877686
Batch 48/64 loss: -0.19739997386932373
Batch 49/64 loss: -0.18612009286880493
Batch 50/64 loss: -0.18274670839309692
Batch 51/64 loss: -0.1989530324935913
Batch 52/64 loss: -0.17313295602798462
Batch 53/64 loss: -0.19138598442077637
Batch 54/64 loss: -0.19886040687561035
Batch 55/64 loss: -0.1968933343887329
Batch 56/64 loss: -0.16794228553771973
Batch 57/64 loss: -0.1936732530593872
Batch 58/64 loss: -0.19909965991973877
Batch 59/64 loss: -0.16357195377349854
Batch 60/64 loss: -0.18014287948608398
Batch 61/64 loss: -0.18097949028015137
Batch 62/64 loss: -0.16655981540679932
Batch 63/64 loss: -0.19908946752548218
Batch 64/64 loss: -0.16353517770767212
Epoch 393  Train loss: -0.1839294669674892  Val loss: -0.017211414899203376
Epoch 394
-------------------------------
Batch 1/64 loss: -0.1990116834640503
Batch 2/64 loss: -0.18671917915344238
Batch 3/64 loss: -0.17907923460006714
Batch 4/64 loss: -0.18705177307128906
Batch 5/64 loss: -0.1990668773651123
Batch 6/64 loss: -0.1978074312210083
Batch 7/64 loss: -0.1899668574333191
Batch 8/64 loss: -0.1830674409866333
Batch 9/64 loss: -0.1868377923965454
Batch 10/64 loss: -0.1710132360458374
Batch 11/64 loss: -0.1953721046447754
Batch 12/64 loss: -0.17517399787902832
Batch 13/64 loss: -0.18391507863998413
Batch 14/64 loss: -0.17887914180755615
Batch 15/64 loss: -0.17973005771636963
Batch 16/64 loss: -0.17976438999176025
Batch 17/64 loss: -0.17684108018875122
Batch 18/64 loss: -0.19992691278457642
Batch 19/64 loss: -0.17716360092163086
Batch 20/64 loss: -0.18110322952270508
Batch 21/64 loss: -0.1924421787261963
Batch 22/64 loss: -0.1709132194519043
Batch 23/64 loss: -0.1628069281578064
Batch 24/64 loss: -0.16292518377304077
Batch 25/64 loss: -0.1798102855682373
Batch 26/64 loss: -0.15141087770462036
Batch 27/64 loss: -0.17668938636779785
Batch 28/64 loss: -0.18580907583236694
Batch 29/64 loss: -0.14940494298934937
Batch 30/64 loss: -0.17070800065994263
Batch 31/64 loss: -0.172258198261261
Batch 32/64 loss: -0.1739305853843689
Batch 33/64 loss: -0.159970223903656
Batch 34/64 loss: -0.1545112133026123
Batch 35/64 loss: -0.14291512966156006
Batch 36/64 loss: -0.18761712312698364
Batch 37/64 loss: -0.17990565299987793
Batch 38/64 loss: -0.1757979393005371
Batch 39/64 loss: -0.17461854219436646
Batch 40/64 loss: -0.17251944541931152
Batch 41/64 loss: -0.1952192783355713
Batch 42/64 loss: -0.16333436965942383
Batch 43/64 loss: -0.1758633852005005
Batch 44/64 loss: -0.18398398160934448
Batch 45/64 loss: -0.19951236248016357
Batch 46/64 loss: -0.18390727043151855
Batch 47/64 loss: -0.1895083785057068
Batch 48/64 loss: -0.16704261302947998
Batch 49/64 loss: -0.20271384716033936
Batch 50/64 loss: -0.19658982753753662
Batch 51/64 loss: -0.1713705062866211
Batch 52/64 loss: -0.17536085844039917
Batch 53/64 loss: -0.17120963335037231
Batch 54/64 loss: -0.18342506885528564
Batch 55/64 loss: -0.14909899234771729
Batch 56/64 loss: -0.1960551142692566
Batch 57/64 loss: -0.18363308906555176
Batch 58/64 loss: -0.15554487705230713
Batch 59/64 loss: -0.1878119707107544
Batch 60/64 loss: -0.18013238906860352
Batch 61/64 loss: -0.1636141538619995
Batch 62/64 loss: -0.1841336488723755
Batch 63/64 loss: -0.1738206148147583
Batch 64/64 loss: -0.15481513738632202
Epoch 394  Train loss: -0.17809337751538146  Val loss: -0.016273088061932436
Epoch 395
-------------------------------
Batch 1/64 loss: -0.1788567304611206
Batch 2/64 loss: -0.20417994260787964
Batch 3/64 loss: -0.17570561170578003
Batch 4/64 loss: -0.18078815937042236
Batch 5/64 loss: -0.1721189022064209
Batch 6/64 loss: -0.18313878774642944
Batch 7/64 loss: -0.17729496955871582
Batch 8/64 loss: -0.1687002182006836
Batch 9/64 loss: -0.1734674572944641
Batch 10/64 loss: -0.18140006065368652
Batch 11/64 loss: -0.18448209762573242
Batch 12/64 loss: -0.17301654815673828
Batch 13/64 loss: -0.1556452512741089
Batch 14/64 loss: -0.1683133840560913
Batch 15/64 loss: -0.15985161066055298
Batch 16/64 loss: -0.18341583013534546
Batch 17/64 loss: -0.159986674785614
Batch 18/64 loss: -0.1619945764541626
Batch 19/64 loss: -0.16463351249694824
Batch 20/64 loss: -0.1570163369178772
Batch 21/64 loss: -0.1626112461090088
Batch 22/64 loss: -0.17104804515838623
Batch 23/64 loss: -0.18225914239883423
Batch 24/64 loss: -0.18304306268692017
Batch 25/64 loss: -0.1730436086654663
Batch 26/64 loss: -0.18695539236068726
Batch 27/64 loss: -0.19142121076583862
Batch 28/64 loss: -0.18166255950927734
Batch 29/64 loss: -0.19125115871429443
Batch 30/64 loss: -0.15649330615997314
Batch 31/64 loss: -0.18029963970184326
Batch 32/64 loss: -0.17613625526428223
Batch 33/64 loss: -0.18660879135131836
Batch 34/64 loss: -0.18409335613250732
Batch 35/64 loss: -0.1747557520866394
Batch 36/64 loss: -0.19228804111480713
Batch 37/64 loss: -0.18330121040344238
Batch 38/64 loss: -0.20772284269332886
Batch 39/64 loss: -0.17832821607589722
Batch 40/64 loss: -0.1942557692527771
Batch 41/64 loss: -0.17637896537780762
Batch 42/64 loss: -0.19010603427886963
Batch 43/64 loss: -0.18616914749145508
Batch 44/64 loss: -0.17657434940338135
Batch 45/64 loss: -0.1859338879585266
Batch 46/64 loss: -0.17443549633026123
Batch 47/64 loss: -0.18283694982528687
Batch 48/64 loss: -0.18974488973617554
Batch 49/64 loss: -0.17021405696868896
Batch 50/64 loss: -0.1836046576499939
Batch 51/64 loss: -0.2014722228050232
Batch 52/64 loss: -0.18917644023895264
Batch 53/64 loss: -0.1882287859916687
Batch 54/64 loss: -0.16974329948425293
Batch 55/64 loss: -0.1956741213798523
Batch 56/64 loss: -0.17379224300384521
Batch 57/64 loss: -0.1764737367630005
Batch 58/64 loss: -0.1469360589981079
Batch 59/64 loss: -0.16618603467941284
Batch 60/64 loss: -0.16861242055892944
Batch 61/64 loss: -0.1861618161201477
Batch 62/64 loss: -0.1823362112045288
Batch 63/64 loss: -0.17720180749893188
Batch 64/64 loss: -0.15284889936447144
Epoch 395  Train loss: -0.17810534238815307  Val loss: -0.015459620256194543
Epoch 396
-------------------------------
Batch 1/64 loss: -0.1769987940788269
Batch 2/64 loss: -0.19527268409729004
Batch 3/64 loss: -0.18861955404281616
Batch 4/64 loss: -0.16940271854400635
Batch 5/64 loss: -0.19043409824371338
Batch 6/64 loss: -0.18108892440795898
Batch 7/64 loss: -0.1631178855895996
Batch 8/64 loss: -0.16685938835144043
Batch 9/64 loss: -0.18489038944244385
Batch 10/64 loss: -0.150618314743042
Batch 11/64 loss: -0.18421322107315063
Batch 12/64 loss: -0.19284647703170776
Batch 13/64 loss: -0.19692444801330566
Batch 14/64 loss: -0.2040526270866394
Batch 15/64 loss: -0.17216885089874268
Batch 16/64 loss: -0.171231210231781
Batch 17/64 loss: -0.20378947257995605
Batch 18/64 loss: -0.17876529693603516
Batch 19/64 loss: -0.1635037660598755
Batch 20/64 loss: -0.15343713760375977
Batch 21/64 loss: -0.18346595764160156
Batch 22/64 loss: -0.19819849729537964
Batch 23/64 loss: -0.16206800937652588
Batch 24/64 loss: -0.18402475118637085
Batch 25/64 loss: -0.19294792413711548
Batch 26/64 loss: -0.1673148274421692
Batch 27/64 loss: -0.17467373609542847
Batch 28/64 loss: -0.19109153747558594
Batch 29/64 loss: -0.17859333753585815
Batch 30/64 loss: -0.19116556644439697
Batch 31/64 loss: -0.17189306020736694
Batch 32/64 loss: -0.19427311420440674
Batch 33/64 loss: -0.15818941593170166
Batch 34/64 loss: -0.18349385261535645
Batch 35/64 loss: -0.19089949131011963
Batch 36/64 loss: -0.18396520614624023
Batch 37/64 loss: -0.1903817057609558
Batch 38/64 loss: -0.18686801195144653
Batch 39/64 loss: -0.17091673612594604
Batch 40/64 loss: -0.16056162118911743
Batch 41/64 loss: -0.17793786525726318
Batch 42/64 loss: -0.15989363193511963
Batch 43/64 loss: -0.1428622007369995
Batch 44/64 loss: -0.17432421445846558
Batch 45/64 loss: -0.16967689990997314
Batch 46/64 loss: -0.16558635234832764
Batch 47/64 loss: -0.20010459423065186
Batch 48/64 loss: -0.18713128566741943
Batch 49/64 loss: -0.1515737771987915
Batch 50/64 loss: -0.18980562686920166
Batch 51/64 loss: -0.16859287023544312
Batch 52/64 loss: -0.18228065967559814
Batch 53/64 loss: -0.18240517377853394
Batch 54/64 loss: -0.17520928382873535
Batch 55/64 loss: -0.18703341484069824
Batch 56/64 loss: -0.15127938985824585
Batch 57/64 loss: -0.1840965747833252
Batch 58/64 loss: -0.18423330783843994
Batch 59/64 loss: -0.18733656406402588
Batch 60/64 loss: -0.16925787925720215
Batch 61/64 loss: -0.20130622386932373
Batch 62/64 loss: -0.17832303047180176
Batch 63/64 loss: -0.18831515312194824
Batch 64/64 loss: -0.18897998332977295
Epoch 396  Train loss: -0.17887878558214973  Val loss: -0.018159306336104665
Epoch 397
-------------------------------
Batch 1/64 loss: -0.18872827291488647
Batch 2/64 loss: -0.19206571578979492
Batch 3/64 loss: -0.16206061840057373
Batch 4/64 loss: -0.19282084703445435
Batch 5/64 loss: -0.1817159652709961
Batch 6/64 loss: -0.1883261799812317
Batch 7/64 loss: -0.20603954792022705
Batch 8/64 loss: -0.17137444019317627
Batch 9/64 loss: -0.16706132888793945
Batch 10/64 loss: -0.19320034980773926
Batch 11/64 loss: -0.17847734689712524
Batch 12/64 loss: -0.17943280935287476
Batch 13/64 loss: -0.20271646976470947
Batch 14/64 loss: -0.1853538155555725
Batch 15/64 loss: -0.18505418300628662
Batch 16/64 loss: -0.17695081233978271
Batch 17/64 loss: -0.18326514959335327
Batch 18/64 loss: -0.18383777141571045
Batch 19/64 loss: -0.18432992696762085
Batch 20/64 loss: -0.13575541973114014
Batch 21/64 loss: -0.17160004377365112
Batch 22/64 loss: -0.16226834058761597
Batch 23/64 loss: -0.15435045957565308
Batch 24/64 loss: -0.18587344884872437
Batch 25/64 loss: -0.18833637237548828
Batch 26/64 loss: -0.2087230682373047
Batch 27/64 loss: -0.18694418668746948
Batch 28/64 loss: -0.19168144464492798
Batch 29/64 loss: -0.1824527382850647
Batch 30/64 loss: -0.20763999223709106
Batch 31/64 loss: -0.15322351455688477
Batch 32/64 loss: -0.18553096055984497
Batch 33/64 loss: -0.16788482666015625
Batch 34/64 loss: -0.19207775592803955
Batch 35/64 loss: -0.18365830183029175
Batch 36/64 loss: -0.1737155318260193
Batch 37/64 loss: -0.19005084037780762
Batch 38/64 loss: -0.19930928945541382
Batch 39/64 loss: -0.19501841068267822
Batch 40/64 loss: -0.19886630773544312
Batch 41/64 loss: -0.18926560878753662
Batch 42/64 loss: -0.16371619701385498
Batch 43/64 loss: -0.18499881029129028
Batch 44/64 loss: -0.17517322301864624
Batch 45/64 loss: -0.1860334873199463
Batch 46/64 loss: -0.17828696966171265
Batch 47/64 loss: -0.17380785942077637
Batch 48/64 loss: -0.17473262548446655
Batch 49/64 loss: -0.1769506335258484
Batch 50/64 loss: -0.1956666111946106
Batch 51/64 loss: -0.19746482372283936
Batch 52/64 loss: -0.19045406579971313
Batch 53/64 loss: -0.1452500820159912
Batch 54/64 loss: -0.16087812185287476
Batch 55/64 loss: -0.17370545864105225
Batch 56/64 loss: -0.17613369226455688
Batch 57/64 loss: -0.19344598054885864
Batch 58/64 loss: -0.16868573427200317
Batch 59/64 loss: -0.16357892751693726
Batch 60/64 loss: -0.20102757215499878
Batch 61/64 loss: -0.18414407968521118
Batch 62/64 loss: -0.18191421031951904
Batch 63/64 loss: -0.16756880283355713
Batch 64/64 loss: -0.1693429946899414
Epoch 397  Train loss: -0.18113982163223566  Val loss: -0.01882652134420126
Epoch 398
-------------------------------
Batch 1/64 loss: -0.1979244351387024
Batch 2/64 loss: -0.19295203685760498
Batch 3/64 loss: -0.1851586103439331
Batch 4/64 loss: -0.17750179767608643
Batch 5/64 loss: -0.1863471269607544
Batch 6/64 loss: -0.20481732487678528
Batch 7/64 loss: -0.16898775100708008
Batch 8/64 loss: -0.15256965160369873
Batch 9/64 loss: -0.1757643222808838
Batch 10/64 loss: -0.15289288759231567
Batch 11/64 loss: -0.17830002307891846
Batch 12/64 loss: -0.18065553903579712
Batch 13/64 loss: -0.16486334800720215
Batch 14/64 loss: -0.1963273286819458
Batch 15/64 loss: -0.19220566749572754
Batch 16/64 loss: -0.20069241523742676
Batch 17/64 loss: -0.16965657472610474
Batch 18/64 loss: -0.18178731203079224
Batch 19/64 loss: -0.17994511127471924
Batch 20/64 loss: -0.19475936889648438
Batch 21/64 loss: -0.193037748336792
Batch 22/64 loss: -0.2038295567035675
Batch 23/64 loss: -0.18628263473510742
Batch 24/64 loss: -0.17936354875564575
Batch 25/64 loss: -0.17764908075332642
Batch 26/64 loss: -0.19611114263534546
Batch 27/64 loss: -0.16965389251708984
Batch 28/64 loss: -0.1888377070426941
Batch 29/64 loss: -0.19068419933319092
Batch 30/64 loss: -0.18503719568252563
Batch 31/64 loss: -0.15671652555465698
Batch 32/64 loss: -0.18829286098480225
Batch 33/64 loss: -0.18767225742340088
Batch 34/64 loss: -0.20295986533164978
Batch 35/64 loss: -0.1560347080230713
Batch 36/64 loss: -0.16068041324615479
Batch 37/64 loss: -0.18839716911315918
Batch 38/64 loss: -0.14792364835739136
Batch 39/64 loss: -0.16349577903747559
Batch 40/64 loss: -0.1966513991355896
Batch 41/64 loss: -0.15555018186569214
Batch 42/64 loss: -0.15565776824951172
Batch 43/64 loss: -0.17965859174728394
Batch 44/64 loss: -0.19372797012329102
Batch 45/64 loss: -0.19558393955230713
Batch 46/64 loss: -0.17821764945983887
Batch 47/64 loss: -0.17257940769195557
Batch 48/64 loss: -0.17721378803253174
Batch 49/64 loss: -0.21195447444915771
Batch 50/64 loss: -0.18840962648391724
Batch 51/64 loss: -0.16964882612228394
Batch 52/64 loss: -0.1726818084716797
Batch 53/64 loss: -0.1881430745124817
Batch 54/64 loss: -0.1883729100227356
Batch 55/64 loss: -0.19998282194137573
Batch 56/64 loss: -0.1750279664993286
Batch 57/64 loss: -0.17579174041748047
Batch 58/64 loss: -0.18585115671157837
Batch 59/64 loss: -0.1907210350036621
Batch 60/64 loss: -0.1739010214805603
Batch 61/64 loss: -0.14427244663238525
Batch 62/64 loss: -0.19888091087341309
Batch 63/64 loss: -0.18463021516799927
Batch 64/64 loss: -0.17618775367736816
Epoch 398  Train loss: -0.18105129587884042  Val loss: -0.017020180667798545
Epoch 399
-------------------------------
Batch 1/64 loss: -0.19836944341659546
Batch 2/64 loss: -0.1904219388961792
Batch 3/64 loss: -0.19401824474334717
Batch 4/64 loss: -0.19069641828536987
Batch 5/64 loss: -0.19257748126983643
Batch 6/64 loss: -0.16550087928771973
Batch 7/64 loss: -0.19584983587265015
Batch 8/64 loss: -0.18605482578277588
Batch 9/64 loss: -0.18748736381530762
Batch 10/64 loss: -0.1881171464920044
Batch 11/64 loss: -0.150446355342865
Batch 12/64 loss: -0.19603365659713745
Batch 13/64 loss: -0.1865137815475464
Batch 14/64 loss: -0.1970955729484558
Batch 15/64 loss: -0.18787497282028198
Batch 16/64 loss: -0.1974998116493225
Batch 17/64 loss: -0.19389748573303223
Batch 18/64 loss: -0.1949360966682434
Batch 19/64 loss: -0.19578629732131958
Batch 20/64 loss: -0.19110554456710815
Batch 21/64 loss: -0.16742682456970215
Batch 22/64 loss: -0.16850042343139648
Batch 23/64 loss: -0.1971677541732788
Batch 24/64 loss: -0.19355618953704834
Batch 25/64 loss: -0.17406845092773438
Batch 26/64 loss: -0.1808115839958191
Batch 27/64 loss: -0.18619030714035034
Batch 28/64 loss: -0.19602686166763306
Batch 29/64 loss: -0.1936078667640686
Batch 30/64 loss: -0.17994731664657593
Batch 31/64 loss: -0.16439670324325562
Batch 32/64 loss: -0.17884069681167603
Batch 33/64 loss: -0.19272446632385254
Batch 34/64 loss: -0.17847967147827148
Batch 35/64 loss: -0.16734778881072998
Batch 36/64 loss: -0.19022732973098755
Batch 37/64 loss: -0.17491722106933594
Batch 38/64 loss: -0.17974698543548584
Batch 39/64 loss: -0.1828293800354004
Batch 40/64 loss: -0.19068682193756104
Batch 41/64 loss: -0.18800365924835205
Batch 42/64 loss: -0.16177845001220703
Batch 43/64 loss: -0.19688260555267334
Batch 44/64 loss: -0.1835113763809204
Batch 45/64 loss: -0.1944483518600464
Batch 46/64 loss: -0.16883933544158936
Batch 47/64 loss: -0.18574374914169312
Batch 48/64 loss: -0.17336171865463257
Batch 49/64 loss: -0.17881393432617188
Batch 50/64 loss: -0.17195075750350952
Batch 51/64 loss: -0.15438544750213623
Batch 52/64 loss: -0.1098012924194336
Batch 53/64 loss: -0.1991249918937683
Batch 54/64 loss: -0.16725224256515503
Batch 55/64 loss: -0.17927676439285278
Batch 56/64 loss: -0.1617535948753357
Batch 57/64 loss: -0.16573941707611084
Batch 58/64 loss: -0.18011623620986938
Batch 59/64 loss: -0.18389487266540527
Batch 60/64 loss: -0.1754366159439087
Batch 61/64 loss: -0.17894220352172852
Batch 62/64 loss: -0.19800114631652832
Batch 63/64 loss: -0.16736924648284912
Batch 64/64 loss: -0.19946026802062988
Epoch 399  Train loss: -0.18183226678885667  Val loss: -0.019149900302034884
Epoch 400
-------------------------------
Batch 1/64 loss: -0.19497764110565186
Batch 2/64 loss: -0.20198023319244385
Batch 3/64 loss: -0.2171558439731598
Batch 4/64 loss: -0.1767856478691101
Batch 5/64 loss: -0.17956268787384033
Batch 6/64 loss: -0.1879923939704895
Batch 7/64 loss: -0.1973649263381958
Batch 8/64 loss: -0.19986361265182495
Batch 9/64 loss: -0.17977535724639893
Batch 10/64 loss: -0.20071804523468018
Batch 11/64 loss: -0.2033032476902008
Batch 12/64 loss: -0.1660652756690979
Batch 13/64 loss: -0.17040902376174927
Batch 14/64 loss: -0.1893024444580078
Batch 15/64 loss: -0.193789541721344
Batch 16/64 loss: -0.1775057315826416
Batch 17/64 loss: -0.18862801790237427
Batch 18/64 loss: -0.20560654997825623
Batch 19/64 loss: -0.1885063648223877
Batch 20/64 loss: -0.20017680525779724
Batch 21/64 loss: -0.17523539066314697
Batch 22/64 loss: -0.20302000641822815
Batch 23/64 loss: -0.19486021995544434
Batch 24/64 loss: -0.20948857069015503
Batch 25/64 loss: -0.21981096267700195
Batch 26/64 loss: -0.19087356328964233
Batch 27/64 loss: -0.19075781106948853
Batch 28/64 loss: -0.20535218715667725
Batch 29/64 loss: -0.19673186540603638
Batch 30/64 loss: -0.19098997116088867
Batch 31/64 loss: -0.19086796045303345
Batch 32/64 loss: -0.18155014514923096
Batch 33/64 loss: -0.2012370228767395
Batch 34/64 loss: -0.18860191106796265
Batch 35/64 loss: -0.19432997703552246
Batch 36/64 loss: -0.16359668970108032
Batch 37/64 loss: -0.18402159214019775
Batch 38/64 loss: -0.21268373727798462
Batch 39/64 loss: -0.20252427458763123
Batch 40/64 loss: -0.19325095415115356
Batch 41/64 loss: -0.1892307996749878
Batch 42/64 loss: -0.17147016525268555
Batch 43/64 loss: -0.17912358045578003
Batch 44/64 loss: -0.1943379044532776
Batch 45/64 loss: -0.1285853385925293
Batch 46/64 loss: -0.18053686618804932
Batch 47/64 loss: -0.16329628229141235
Batch 48/64 loss: -0.15744197368621826
Batch 49/64 loss: -0.17579388618469238
Batch 50/64 loss: -0.15306967496871948
Batch 51/64 loss: -0.1928538680076599
Batch 52/64 loss: -0.17767810821533203
Batch 53/64 loss: -0.10717207193374634
Batch 54/64 loss: -0.19700592756271362
Batch 55/64 loss: -0.13847559690475464
Batch 56/64 loss: -0.19342947006225586
Batch 57/64 loss: -0.1752893328666687
Batch 58/64 loss: -0.17966407537460327
Batch 59/64 loss: -0.15952754020690918
Batch 60/64 loss: -0.17245721817016602
Batch 61/64 loss: -0.18818247318267822
Batch 62/64 loss: -0.1803041696548462
Batch 63/64 loss: -0.18720638751983643
Batch 64/64 loss: -0.16783583164215088
Epoch 400  Train loss: -0.1847413927901025  Val loss: -0.01720633314237562
Epoch 401
-------------------------------
Batch 1/64 loss: -0.20040816068649292
Batch 2/64 loss: -0.19859611988067627
Batch 3/64 loss: -0.18885987997055054
Batch 4/64 loss: -0.18751448392868042
Batch 5/64 loss: -0.14630186557769775
Batch 6/64 loss: -0.18861734867095947
Batch 7/64 loss: -0.18809175491333008
Batch 8/64 loss: -0.20006811618804932
Batch 9/64 loss: -0.20465564727783203
Batch 10/64 loss: -0.1542360782623291
Batch 11/64 loss: -0.18382930755615234
Batch 12/64 loss: -0.19698619842529297
Batch 13/64 loss: -0.1931285858154297
Batch 14/64 loss: -0.17536628246307373
Batch 15/64 loss: -0.1772688627243042
Batch 16/64 loss: -0.179468035697937
Batch 17/64 loss: -0.19867724180221558
Batch 18/64 loss: -0.147924542427063
Batch 19/64 loss: -0.19374215602874756
Batch 20/64 loss: -0.18728071451187134
Batch 21/64 loss: -0.1890144944190979
Batch 22/64 loss: -0.1883091926574707
Batch 23/64 loss: -0.19778966903686523
Batch 24/64 loss: -0.19373929500579834
Batch 25/64 loss: -0.2020069658756256
Batch 26/64 loss: -0.19561350345611572
Batch 27/64 loss: -0.17824077606201172
Batch 28/64 loss: -0.1788015365600586
Batch 29/64 loss: -0.1882382035255432
Batch 30/64 loss: -0.19736039638519287
Batch 31/64 loss: -0.15665465593338013
Batch 32/64 loss: -0.1892334222793579
Batch 33/64 loss: -0.17692804336547852
Batch 34/64 loss: -0.18422269821166992
Batch 35/64 loss: -0.1783515214920044
Batch 36/64 loss: -0.18269109725952148
Batch 37/64 loss: -0.17615020275115967
Batch 38/64 loss: -0.19580483436584473
Batch 39/64 loss: -0.21003097295761108
Batch 40/64 loss: -0.18807518482208252
Batch 41/64 loss: -0.17565912008285522
Batch 42/64 loss: -0.16439330577850342
Batch 43/64 loss: -0.18897199630737305
Batch 44/64 loss: -0.19750434160232544
Batch 45/64 loss: -0.19146394729614258
Batch 46/64 loss: -0.17410236597061157
Batch 47/64 loss: -0.17079317569732666
Batch 48/64 loss: -0.16649150848388672
Batch 49/64 loss: -0.18753594160079956
Batch 50/64 loss: -0.17874956130981445
Batch 51/64 loss: -0.16795235872268677
Batch 52/64 loss: -0.19926810264587402
Batch 53/64 loss: -0.1935347318649292
Batch 54/64 loss: -0.19744735956192017
Batch 55/64 loss: -0.19167155027389526
Batch 56/64 loss: -0.15509414672851562
Batch 57/64 loss: -0.1942152976989746
Batch 58/64 loss: -0.1815195083618164
Batch 59/64 loss: -0.19472575187683105
Batch 60/64 loss: -0.18683141469955444
Batch 61/64 loss: -0.17401283979415894
Batch 62/64 loss: -0.17891281843185425
Batch 63/64 loss: -0.18258744478225708
Batch 64/64 loss: -0.18172180652618408
Epoch 401  Train loss: -0.1845962038227156  Val loss: -0.016526553024541063
Epoch 402
-------------------------------
Batch 1/64 loss: -0.2001459002494812
Batch 2/64 loss: -0.19610434770584106
Batch 3/64 loss: -0.18860697746276855
Batch 4/64 loss: -0.19425827264785767
Batch 5/64 loss: -0.18551045656204224
Batch 6/64 loss: -0.17609214782714844
Batch 7/64 loss: -0.1736987829208374
Batch 8/64 loss: -0.1842052936553955
Batch 9/64 loss: -0.19337868690490723
Batch 10/64 loss: -0.19726967811584473
Batch 11/64 loss: -0.1966140866279602
Batch 12/64 loss: -0.15729963779449463
Batch 13/64 loss: -0.19882220029830933
Batch 14/64 loss: -0.17942917346954346
Batch 15/64 loss: -0.1873762011528015
Batch 16/64 loss: -0.1826828122138977
Batch 17/64 loss: -0.20669639110565186
Batch 18/64 loss: -0.17482209205627441
Batch 19/64 loss: -0.2166546881198883
Batch 20/64 loss: -0.1490025520324707
Batch 21/64 loss: -0.1914042830467224
Batch 22/64 loss: -0.176219642162323
Batch 23/64 loss: -0.1842576265335083
Batch 24/64 loss: -0.19551360607147217
Batch 25/64 loss: -0.1774161458015442
Batch 26/64 loss: -0.19518250226974487
Batch 27/64 loss: -0.18434029817581177
Batch 28/64 loss: -0.15376675128936768
Batch 29/64 loss: -0.1881622076034546
Batch 30/64 loss: -0.16431361436843872
Batch 31/64 loss: -0.18963956832885742
Batch 32/64 loss: -0.18279331922531128
Batch 33/64 loss: -0.18606364727020264
Batch 34/64 loss: -0.19476348161697388
Batch 35/64 loss: -0.17559325695037842
Batch 36/64 loss: -0.18977606296539307
Batch 37/64 loss: -0.19247686862945557
Batch 38/64 loss: -0.20103460550308228
Batch 39/64 loss: -0.19027411937713623
Batch 40/64 loss: -0.16320252418518066
Batch 41/64 loss: -0.1851189136505127
Batch 42/64 loss: -0.17526262998580933
Batch 43/64 loss: -0.19043123722076416
Batch 44/64 loss: -0.18895190954208374
Batch 45/64 loss: -0.1981295347213745
Batch 46/64 loss: -0.1754246950149536
Batch 47/64 loss: -0.1880205273628235
Batch 48/64 loss: -0.153333842754364
Batch 49/64 loss: -0.19873368740081787
Batch 50/64 loss: -0.17108958959579468
Batch 51/64 loss: -0.19227278232574463
Batch 52/64 loss: -0.18173885345458984
Batch 53/64 loss: -0.17693138122558594
Batch 54/64 loss: -0.20567020773887634
Batch 55/64 loss: -0.160500168800354
Batch 56/64 loss: -0.17612624168395996
Batch 57/64 loss: -0.18535876274108887
Batch 58/64 loss: -0.16959112882614136
Batch 59/64 loss: -0.1872624158859253
Batch 60/64 loss: -0.19440555572509766
Batch 61/64 loss: -0.16034793853759766
Batch 62/64 loss: -0.1640358567237854
Batch 63/64 loss: -0.19381177425384521
Batch 64/64 loss: -0.20526474714279175
Epoch 402  Train loss: -0.1841782385227727  Val loss: -0.019943930234286384
Epoch 403
-------------------------------
Batch 1/64 loss: -0.1948758363723755
Batch 2/64 loss: -0.18697845935821533
Batch 3/64 loss: -0.19562947750091553
Batch 4/64 loss: -0.17507612705230713
Batch 5/64 loss: -0.17610692977905273
Batch 6/64 loss: -0.18283385038375854
Batch 7/64 loss: -0.18293535709381104
Batch 8/64 loss: -0.1778455376625061
Batch 9/64 loss: -0.18053722381591797
Batch 10/64 loss: -0.199343740940094
Batch 11/64 loss: -0.19435632228851318
Batch 12/64 loss: -0.16147226095199585
Batch 13/64 loss: -0.16869789361953735
Batch 14/64 loss: -0.1756628155708313
Batch 15/64 loss: -0.1903597116470337
Batch 16/64 loss: -0.22507017850875854
Batch 17/64 loss: -0.1869608759880066
Batch 18/64 loss: -0.17846792936325073
Batch 19/64 loss: -0.18331098556518555
Batch 20/64 loss: -0.1989637017250061
Batch 21/64 loss: -0.19161802530288696
Batch 22/64 loss: -0.18550491333007812
Batch 23/64 loss: -0.19918978214263916
Batch 24/64 loss: -0.1928533911705017
Batch 25/64 loss: -0.20308011770248413
Batch 26/64 loss: -0.1892462968826294
Batch 27/64 loss: -0.14981776475906372
Batch 28/64 loss: -0.1793004870414734
Batch 29/64 loss: -0.20220038294792175
Batch 30/64 loss: -0.18499690294265747
Batch 31/64 loss: -0.18674176931381226
Batch 32/64 loss: -0.18203461170196533
Batch 33/64 loss: -0.18187367916107178
Batch 34/64 loss: -0.2045145332813263
Batch 35/64 loss: -0.1825609803199768
Batch 36/64 loss: -0.1863529086112976
Batch 37/64 loss: -0.17258936166763306
Batch 38/64 loss: -0.1820775270462036
Batch 39/64 loss: -0.14834147691726685
Batch 40/64 loss: -0.17453008890151978
Batch 41/64 loss: -0.18063420057296753
Batch 42/64 loss: -0.18930500745773315
Batch 43/64 loss: -0.16534912586212158
Batch 44/64 loss: -0.18805301189422607
Batch 45/64 loss: -0.19260096549987793
Batch 46/64 loss: -0.18218868970870972
Batch 47/64 loss: -0.19964110851287842
Batch 48/64 loss: -0.18901383876800537
Batch 49/64 loss: -0.1720237135887146
Batch 50/64 loss: -0.19645172357559204
Batch 51/64 loss: -0.17470955848693848
Batch 52/64 loss: -0.19207167625427246
Batch 53/64 loss: -0.16865062713623047
Batch 54/64 loss: -0.18814921379089355
Batch 55/64 loss: -0.18916893005371094
Batch 56/64 loss: -0.1899271011352539
Batch 57/64 loss: -0.17457914352416992
Batch 58/64 loss: -0.2084689736366272
Batch 59/64 loss: -0.19987988471984863
Batch 60/64 loss: -0.18687814474105835
Batch 61/64 loss: -0.15823209285736084
Batch 62/64 loss: -0.19819384813308716
Batch 63/64 loss: -0.1863437294960022
Batch 64/64 loss: -0.19780391454696655
Epoch 403  Train loss: -0.1853141562611449  Val loss: -0.01382432524690923
Epoch 404
-------------------------------
Batch 1/64 loss: -0.20040887594223022
Batch 2/64 loss: -0.2018808126449585
Batch 3/64 loss: -0.1833510398864746
Batch 4/64 loss: -0.19023841619491577
Batch 5/64 loss: -0.18161767721176147
Batch 6/64 loss: -0.17057883739471436
Batch 7/64 loss: -0.2024795413017273
Batch 8/64 loss: -0.19082850217819214
Batch 9/64 loss: -0.2031385600566864
Batch 10/64 loss: -0.2066853642463684
Batch 11/64 loss: -0.19080781936645508
Batch 12/64 loss: -0.18627703189849854
Batch 13/64 loss: -0.18780428171157837
Batch 14/64 loss: -0.18604081869125366
Batch 15/64 loss: -0.17556267976760864
Batch 16/64 loss: -0.18128490447998047
Batch 17/64 loss: -0.19025003910064697
Batch 18/64 loss: -0.16616082191467285
Batch 19/64 loss: -0.1369190216064453
Batch 20/64 loss: -0.19323551654815674
Batch 21/64 loss: -0.1905350685119629
Batch 22/64 loss: -0.18392127752304077
Batch 23/64 loss: -0.15895617008209229
Batch 24/64 loss: -0.1808876395225525
Batch 25/64 loss: -0.1901545524597168
Batch 26/64 loss: -0.16788184642791748
Batch 27/64 loss: -0.17926156520843506
Batch 28/64 loss: -0.17429709434509277
Batch 29/64 loss: -0.18325316905975342
Batch 30/64 loss: -0.17983651161193848
Batch 31/64 loss: -0.20057877898216248
Batch 32/64 loss: -0.1910618543624878
Batch 33/64 loss: -0.18407279253005981
Batch 34/64 loss: -0.18319976329803467
Batch 35/64 loss: -0.1804049015045166
Batch 36/64 loss: -0.18997246026992798
Batch 37/64 loss: -0.19063621759414673
Batch 38/64 loss: -0.2057613730430603
Batch 39/64 loss: -0.17317837476730347
Batch 40/64 loss: -0.17468440532684326
Batch 41/64 loss: -0.1977177858352661
Batch 42/64 loss: -0.20124754309654236
Batch 43/64 loss: -0.17727327346801758
Batch 44/64 loss: -0.2106935977935791
Batch 45/64 loss: -0.19978421926498413
Batch 46/64 loss: -0.18949878215789795
Batch 47/64 loss: -0.17956995964050293
Batch 48/64 loss: -0.1845799684524536
Batch 49/64 loss: -0.1827709674835205
Batch 50/64 loss: -0.19273769855499268
Batch 51/64 loss: -0.19960057735443115
Batch 52/64 loss: -0.18079417943954468
Batch 53/64 loss: -0.17853903770446777
Batch 54/64 loss: -0.20792323350906372
Batch 55/64 loss: -0.19950076937675476
Batch 56/64 loss: -0.19301921129226685
Batch 57/64 loss: -0.19489729404449463
Batch 58/64 loss: -0.19459962844848633
Batch 59/64 loss: -0.1745758056640625
Batch 60/64 loss: -0.18608307838439941
Batch 61/64 loss: -0.19989246129989624
Batch 62/64 loss: -0.20111054182052612
Batch 63/64 loss: -0.16666728258132935
Batch 64/64 loss: -0.19372951984405518
Epoch 404  Train loss: -0.1870817320019591  Val loss: -0.014838651897980995
Epoch 405
-------------------------------
Batch 1/64 loss: -0.1794593334197998
Batch 2/64 loss: -0.19049489498138428
Batch 3/64 loss: -0.1862078309059143
Batch 4/64 loss: -0.17817211151123047
Batch 5/64 loss: -0.18730348348617554
Batch 6/64 loss: -0.19776993989944458
Batch 7/64 loss: -0.20043939352035522
Batch 8/64 loss: -0.18557345867156982
Batch 9/64 loss: -0.16902875900268555
Batch 10/64 loss: -0.18550950288772583
Batch 11/64 loss: -0.15714490413665771
Batch 12/64 loss: -0.15345138311386108
Batch 13/64 loss: -0.18930858373641968
Batch 14/64 loss: -0.17494791746139526
Batch 15/64 loss: -0.19193708896636963
Batch 16/64 loss: -0.2000422477722168
Batch 17/64 loss: -0.20293334126472473
Batch 18/64 loss: -0.18961334228515625
Batch 19/64 loss: -0.19204634428024292
Batch 20/64 loss: -0.17279142141342163
Batch 21/64 loss: -0.16524028778076172
Batch 22/64 loss: -0.1898118257522583
Batch 23/64 loss: -0.19915348291397095
Batch 24/64 loss: -0.19318562746047974
Batch 25/64 loss: -0.18781542778015137
Batch 26/64 loss: -0.17453986406326294
Batch 27/64 loss: -0.1994524598121643
Batch 28/64 loss: -0.20153790712356567
Batch 29/64 loss: -0.19612973928451538
Batch 30/64 loss: -0.2076772153377533
Batch 31/64 loss: -0.2027469277381897
Batch 32/64 loss: -0.1798992156982422
Batch 33/64 loss: -0.20142945647239685
Batch 34/64 loss: -0.194707453250885
Batch 35/64 loss: -0.15797525644302368
Batch 36/64 loss: -0.14073538780212402
Batch 37/64 loss: -0.20231682062149048
Batch 38/64 loss: -0.19006562232971191
Batch 39/64 loss: -0.1841106414794922
Batch 40/64 loss: -0.17198437452316284
Batch 41/64 loss: -0.1863163709640503
Batch 42/64 loss: -0.19237077236175537
Batch 43/64 loss: -0.19460350275039673
Batch 44/64 loss: -0.18218624591827393
Batch 45/64 loss: -0.17187976837158203
Batch 46/64 loss: -0.20070022344589233
Batch 47/64 loss: -0.20741695165634155
Batch 48/64 loss: -0.1971338987350464
Batch 49/64 loss: -0.18207001686096191
Batch 50/64 loss: -0.19182419776916504
Batch 51/64 loss: -0.18147635459899902
Batch 52/64 loss: -0.12878060340881348
Batch 53/64 loss: -0.18163585662841797
Batch 54/64 loss: -0.2014027237892151
Batch 55/64 loss: -0.1905897855758667
Batch 56/64 loss: -0.18623566627502441
Batch 57/64 loss: -0.18349462747573853
Batch 58/64 loss: -0.19566118717193604
Batch 59/64 loss: -0.18969029188156128
Batch 60/64 loss: -0.19790852069854736
Batch 61/64 loss: -0.18706244230270386
Batch 62/64 loss: -0.20498871803283691
Batch 63/64 loss: -0.15413588285446167
Batch 64/64 loss: -0.15696310997009277
Epoch 405  Train loss: -0.1855996426414041  Val loss: -0.017666208785014462
Epoch 406
-------------------------------
Batch 1/64 loss: -0.19731676578521729
Batch 2/64 loss: -0.1990852952003479
Batch 3/64 loss: -0.2050926387310028
Batch 4/64 loss: -0.18737995624542236
Batch 5/64 loss: -0.19252687692642212
Batch 6/64 loss: -0.1993556022644043
Batch 7/64 loss: -0.1828286051750183
Batch 8/64 loss: -0.19717782735824585
Batch 9/64 loss: -0.18980664014816284
Batch 10/64 loss: -0.1728343963623047
Batch 11/64 loss: -0.20844405889511108
Batch 12/64 loss: -0.19486212730407715
Batch 13/64 loss: -0.1899259090423584
Batch 14/64 loss: -0.17590069770812988
Batch 15/64 loss: -0.18798398971557617
Batch 16/64 loss: -0.18706703186035156
Batch 17/64 loss: -0.17626625299453735
Batch 18/64 loss: -0.19631993770599365
Batch 19/64 loss: -0.20156332850456238
Batch 20/64 loss: -0.19516175985336304
Batch 21/64 loss: -0.20405739545822144
Batch 22/64 loss: -0.18436777591705322
Batch 23/64 loss: -0.19682633876800537
Batch 24/64 loss: -0.22007501125335693
Batch 25/64 loss: -0.17671924829483032
Batch 26/64 loss: -0.19970703125
Batch 27/64 loss: -0.20227384567260742
Batch 28/64 loss: -0.20341083407402039
Batch 29/64 loss: -0.19645333290100098
Batch 30/64 loss: -0.20839989185333252
Batch 31/64 loss: -0.17307555675506592
Batch 32/64 loss: -0.18954229354858398
Batch 33/64 loss: -0.1843697428703308
Batch 34/64 loss: -0.19042223691940308
Batch 35/64 loss: -0.16310334205627441
Batch 36/64 loss: -0.18652266263961792
Batch 37/64 loss: -0.18408524990081787
Batch 38/64 loss: -0.18894249200820923
Batch 39/64 loss: -0.20050621032714844
Batch 40/64 loss: -0.1573270559310913
Batch 41/64 loss: -0.17773765325546265
Batch 42/64 loss: -0.17710578441619873
Batch 43/64 loss: -0.19515442848205566
Batch 44/64 loss: -0.18206840753555298
Batch 45/64 loss: -0.19485574960708618
Batch 46/64 loss: -0.16087037324905396
Batch 47/64 loss: -0.17915821075439453
Batch 48/64 loss: -0.1964920163154602
Batch 49/64 loss: -0.18879437446594238
Batch 50/64 loss: -0.17621052265167236
Batch 51/64 loss: -0.17573606967926025
Batch 52/64 loss: -0.1900678277015686
Batch 53/64 loss: -0.1713244915008545
Batch 54/64 loss: -0.1831943392753601
Batch 55/64 loss: -0.16552948951721191
Batch 56/64 loss: -0.19300484657287598
Batch 57/64 loss: -0.1855739951133728
Batch 58/64 loss: -0.16538190841674805
Batch 59/64 loss: -0.12514138221740723
Batch 60/64 loss: -0.18503761291503906
Batch 61/64 loss: -0.1837661862373352
Batch 62/64 loss: -0.18290317058563232
Batch 63/64 loss: -0.1663224697113037
Batch 64/64 loss: -0.1818147897720337
Epoch 406  Train loss: -0.18642948468526205  Val loss: -0.01426500204912166
Epoch 407
-------------------------------
Batch 1/64 loss: -0.1988227367401123
Batch 2/64 loss: -0.18105441331863403
Batch 3/64 loss: -0.1946534514427185
Batch 4/64 loss: -0.1784788966178894
Batch 5/64 loss: -0.20951572060585022
Batch 6/64 loss: -0.2152259349822998
Batch 7/64 loss: -0.19942069053649902
Batch 8/64 loss: -0.19740992784500122
Batch 9/64 loss: -0.17514324188232422
Batch 10/64 loss: -0.21235328912734985
Batch 11/64 loss: -0.18533611297607422
Batch 12/64 loss: -0.20576977729797363
Batch 13/64 loss: -0.197079598903656
Batch 14/64 loss: -0.18627524375915527
Batch 15/64 loss: -0.18340229988098145
Batch 16/64 loss: -0.1918134093284607
Batch 17/64 loss: -0.20650115609169006
Batch 18/64 loss: -0.21437883377075195
Batch 19/64 loss: -0.19287431240081787
Batch 20/64 loss: -0.20756405591964722
Batch 21/64 loss: -0.20245596766471863
Batch 22/64 loss: -0.21176409721374512
Batch 23/64 loss: -0.16623848676681519
Batch 24/64 loss: -0.1939542293548584
Batch 25/64 loss: -0.20828485488891602
Batch 26/64 loss: -0.19847255945205688
Batch 27/64 loss: -0.184170663356781
Batch 28/64 loss: -0.17044246196746826
Batch 29/64 loss: -0.196730375289917
Batch 30/64 loss: -0.18740075826644897
Batch 31/64 loss: -0.19231724739074707
Batch 32/64 loss: -0.16972023248672485
Batch 33/64 loss: -0.18633043766021729
Batch 34/64 loss: -0.21194371581077576
Batch 35/64 loss: -0.1829773187637329
Batch 36/64 loss: -0.1826809048652649
Batch 37/64 loss: -0.1809377670288086
Batch 38/64 loss: -0.15384095907211304
Batch 39/64 loss: -0.16460543870925903
Batch 40/64 loss: -0.1780005693435669
Batch 41/64 loss: -0.1824386715888977
Batch 42/64 loss: -0.1921725869178772
Batch 43/64 loss: -0.1958475112915039
Batch 44/64 loss: -0.193367600440979
Batch 45/64 loss: -0.2060782015323639
Batch 46/64 loss: -0.17646872997283936
Batch 47/64 loss: -0.19837310910224915
Batch 48/64 loss: -0.19231325387954712
Batch 49/64 loss: -0.21248212456703186
Batch 50/64 loss: -0.20017284154891968
Batch 51/64 loss: -0.19584345817565918
Batch 52/64 loss: -0.18002557754516602
Batch 53/64 loss: -0.1603642702102661
Batch 54/64 loss: -0.1721901297569275
Batch 55/64 loss: -0.1964976191520691
Batch 56/64 loss: -0.1822645664215088
Batch 57/64 loss: -0.19493591785430908
Batch 58/64 loss: -0.18307101726531982
Batch 59/64 loss: -0.1998959183692932
Batch 60/64 loss: -0.18179404735565186
Batch 61/64 loss: -0.19224506616592407
Batch 62/64 loss: -0.20472168922424316
Batch 63/64 loss: -0.18879741430282593
Batch 64/64 loss: -0.2007749080657959
Epoch 407  Train loss: -0.1912044651368085  Val loss: -0.013963314266139289
Epoch 408
-------------------------------
Batch 1/64 loss: -0.21590864658355713
Batch 2/64 loss: -0.16207993030548096
Batch 3/64 loss: -0.19432306289672852
Batch 4/64 loss: -0.1977022886276245
Batch 5/64 loss: -0.2077888548374176
Batch 6/64 loss: -0.17893004417419434
Batch 7/64 loss: -0.17928004264831543
Batch 8/64 loss: -0.20393282175064087
Batch 9/64 loss: -0.18295907974243164
Batch 10/64 loss: -0.20058736205101013
Batch 11/64 loss: -0.19167399406433105
Batch 12/64 loss: -0.21661293506622314
Batch 13/64 loss: -0.1950485110282898
Batch 14/64 loss: -0.19078916311264038
Batch 15/64 loss: -0.18859243392944336
Batch 16/64 loss: -0.18742406368255615
Batch 17/64 loss: -0.1878170371055603
Batch 18/64 loss: -0.19893652200698853
Batch 19/64 loss: -0.19041424989700317
Batch 20/64 loss: -0.18576347827911377
Batch 21/64 loss: -0.18949973583221436
Batch 22/64 loss: -0.16170668601989746
Batch 23/64 loss: -0.2196519374847412
Batch 24/64 loss: -0.1864946484565735
Batch 25/64 loss: -0.17662036418914795
Batch 26/64 loss: -0.16979384422302246
Batch 27/64 loss: -0.19416797161102295
Batch 28/64 loss: -0.19502860307693481
Batch 29/64 loss: -0.18821197748184204
Batch 30/64 loss: -0.18985724449157715
Batch 31/64 loss: -0.18195950984954834
Batch 32/64 loss: -0.18277156352996826
Batch 33/64 loss: -0.18629616498947144
Batch 34/64 loss: -0.19760602712631226
Batch 35/64 loss: -0.1745063066482544
Batch 36/64 loss: -0.16477549076080322
Batch 37/64 loss: -0.18755745887756348
Batch 38/64 loss: -0.2076779007911682
Batch 39/64 loss: -0.2025929093360901
Batch 40/64 loss: -0.20878970623016357
Batch 41/64 loss: -0.1939501166343689
Batch 42/64 loss: -0.18129563331604004
Batch 43/64 loss: -0.174261212348938
Batch 44/64 loss: -0.1726101040840149
Batch 45/64 loss: -0.18943893909454346
Batch 46/64 loss: -0.18201947212219238
Batch 47/64 loss: -0.19280385971069336
Batch 48/64 loss: -0.20821839570999146
Batch 49/64 loss: -0.1901024580001831
Batch 50/64 loss: -0.19355559349060059
Batch 51/64 loss: -0.179426908493042
Batch 52/64 loss: -0.18325084447860718
Batch 53/64 loss: -0.19881856441497803
Batch 54/64 loss: -0.18084311485290527
Batch 55/64 loss: -0.1970888376235962
Batch 56/64 loss: -0.15990394353866577
Batch 57/64 loss: -0.17732328176498413
Batch 58/64 loss: -0.1762145757675171
Batch 59/64 loss: -0.16398054361343384
Batch 60/64 loss: -0.19377565383911133
Batch 61/64 loss: -0.17153090238571167
Batch 62/64 loss: -0.2037854790687561
Batch 63/64 loss: -0.16408348083496094
Batch 64/64 loss: -0.1813032627105713
Epoch 408  Train loss: -0.18802180290222167  Val loss: -0.0175823030602891
Epoch 409
-------------------------------
Batch 1/64 loss: -0.205287367105484
Batch 2/64 loss: -0.18193203210830688
Batch 3/64 loss: -0.14279615879058838
Batch 4/64 loss: -0.19653630256652832
Batch 5/64 loss: -0.1843113899230957
Batch 6/64 loss: -0.18435561656951904
Batch 7/64 loss: -0.18851417303085327
Batch 8/64 loss: -0.21817195415496826
Batch 9/64 loss: -0.19281142950057983
Batch 10/64 loss: -0.17964929342269897
Batch 11/64 loss: -0.19976168870925903
Batch 12/64 loss: -0.1962108016014099
Batch 13/64 loss: -0.18624001741409302
Batch 14/64 loss: -0.19325125217437744
Batch 15/64 loss: -0.1991695761680603
Batch 16/64 loss: -0.18734169006347656
Batch 17/64 loss: -0.1876484751701355
Batch 18/64 loss: -0.2026214897632599
Batch 19/64 loss: -0.205283522605896
Batch 20/64 loss: -0.1811853051185608
Batch 21/64 loss: -0.1813332438468933
Batch 22/64 loss: -0.19515329599380493
Batch 23/64 loss: -0.20994901657104492
Batch 24/64 loss: -0.19687414169311523
Batch 25/64 loss: -0.18884897232055664
Batch 26/64 loss: -0.1591324806213379
Batch 27/64 loss: -0.1950523853302002
Batch 28/64 loss: -0.16583681106567383
Batch 29/64 loss: -0.17687034606933594
Batch 30/64 loss: -0.19412767887115479
Batch 31/64 loss: -0.20215928554534912
Batch 32/64 loss: -0.17967861890792847
Batch 33/64 loss: -0.20444536209106445
Batch 34/64 loss: -0.19465410709381104
Batch 35/64 loss: -0.19886872172355652
Batch 36/64 loss: -0.19724035263061523
Batch 37/64 loss: -0.16144394874572754
Batch 38/64 loss: -0.18082290887832642
Batch 39/64 loss: -0.18545657396316528
Batch 40/64 loss: -0.20903724431991577
Batch 41/64 loss: -0.18164068460464478
Batch 42/64 loss: -0.17311257123947144
Batch 43/64 loss: -0.17009329795837402
Batch 44/64 loss: -0.17317742109298706
Batch 45/64 loss: -0.18351984024047852
Batch 46/64 loss: -0.19307935237884521
Batch 47/64 loss: -0.1736506223678589
Batch 48/64 loss: -0.2012386918067932
Batch 49/64 loss: -0.1670171022415161
Batch 50/64 loss: -0.19175493717193604
Batch 51/64 loss: -0.17841112613677979
Batch 52/64 loss: -0.18118369579315186
Batch 53/64 loss: -0.16746997833251953
Batch 54/64 loss: -0.17094510793685913
Batch 55/64 loss: -0.19500476121902466
Batch 56/64 loss: -0.18706649541854858
Batch 57/64 loss: -0.170068621635437
Batch 58/64 loss: -0.17328941822052002
Batch 59/64 loss: -0.16732126474380493
Batch 60/64 loss: -0.18907606601715088
Batch 61/64 loss: -0.20355963706970215
Batch 62/64 loss: -0.19872325658798218
Batch 63/64 loss: -0.1885615587234497
Batch 64/64 loss: -0.19468462467193604
Epoch 409  Train loss: -0.18690265113232182  Val loss: -0.01751996499975932
Epoch 410
-------------------------------
Batch 1/64 loss: -0.2154838740825653
Batch 2/64 loss: -0.20731037855148315
Batch 3/64 loss: -0.190604567527771
Batch 4/64 loss: -0.2047305703163147
Batch 5/64 loss: -0.19298189878463745
Batch 6/64 loss: -0.19477736949920654
Batch 7/64 loss: -0.19959652423858643
Batch 8/64 loss: -0.2110268473625183
Batch 9/64 loss: -0.20185333490371704
Batch 10/64 loss: -0.20475256443023682
Batch 11/64 loss: -0.18773698806762695
Batch 12/64 loss: -0.22177040576934814
Batch 13/64 loss: -0.1946977972984314
Batch 14/64 loss: -0.15633541345596313
Batch 15/64 loss: -0.18954575061798096
Batch 16/64 loss: -0.1971585750579834
Batch 17/64 loss: -0.1962575912475586
Batch 18/64 loss: -0.1835646629333496
Batch 19/64 loss: -0.1935916543006897
Batch 20/64 loss: -0.19688111543655396
Batch 21/64 loss: -0.18461287021636963
Batch 22/64 loss: -0.19057607650756836
Batch 23/64 loss: -0.20039021968841553
Batch 24/64 loss: -0.16727226972579956
Batch 25/64 loss: -0.20038026571273804
Batch 26/64 loss: -0.17809045314788818
Batch 27/64 loss: -0.18269562721252441
Batch 28/64 loss: -0.1920744776725769
Batch 29/64 loss: -0.19988638162612915
Batch 30/64 loss: -0.19696050882339478
Batch 31/64 loss: -0.1801336407661438
Batch 32/64 loss: -0.1840205192565918
Batch 33/64 loss: -0.19307899475097656
Batch 34/64 loss: -0.19366395473480225
Batch 35/64 loss: -0.1482943296432495
Batch 36/64 loss: -0.155259370803833
Batch 37/64 loss: -0.19562959671020508
Batch 38/64 loss: -0.1998356580734253
Batch 39/64 loss: -0.19667601585388184
Batch 40/64 loss: -0.18847686052322388
Batch 41/64 loss: -0.17290741205215454
Batch 42/64 loss: -0.15723955631256104
Batch 43/64 loss: -0.18788915872573853
Batch 44/64 loss: -0.2126425802707672
Batch 45/64 loss: -0.1997343897819519
Batch 46/64 loss: -0.1924954652786255
Batch 47/64 loss: -0.20366472005844116
Batch 48/64 loss: -0.16608542203903198
Batch 49/64 loss: -0.18644320964813232
Batch 50/64 loss: -0.18683308362960815
Batch 51/64 loss: -0.19143164157867432
Batch 52/64 loss: -0.19035625457763672
Batch 53/64 loss: -0.16069334745407104
Batch 54/64 loss: -0.19414466619491577
Batch 55/64 loss: -0.19724208116531372
Batch 56/64 loss: -0.18908023834228516
Batch 57/64 loss: -0.20671790838241577
Batch 58/64 loss: -0.1450253129005432
Batch 59/64 loss: -0.1962987184524536
Batch 60/64 loss: -0.16467541456222534
Batch 61/64 loss: -0.17507123947143555
Batch 62/64 loss: -0.21354961395263672
Batch 63/64 loss: -0.20921891927719116
Batch 64/64 loss: -0.18026256561279297
Epoch 410  Train loss: -0.1898557373121673  Val loss: -0.016272169617852803
Epoch 411
-------------------------------
Batch 1/64 loss: -0.2002841830253601
Batch 2/64 loss: -0.20872491598129272
Batch 3/64 loss: -0.19927307963371277
Batch 4/64 loss: -0.19754958152770996
Batch 5/64 loss: -0.19639170169830322
Batch 6/64 loss: -0.2014349400997162
Batch 7/64 loss: -0.19077396392822266
Batch 8/64 loss: -0.19100159406661987
Batch 9/64 loss: -0.19514727592468262
Batch 10/64 loss: -0.1919386386871338
Batch 11/64 loss: -0.17959147691726685
Batch 12/64 loss: -0.1640058159828186
Batch 13/64 loss: -0.20100072026252747
Batch 14/64 loss: -0.19932687282562256
Batch 15/64 loss: -0.19828367233276367
Batch 16/64 loss: -0.21220356225967407
Batch 17/64 loss: -0.1938323974609375
Batch 18/64 loss: -0.19157087802886963
Batch 19/64 loss: -0.20359867811203003
Batch 20/64 loss: -0.19529616832733154
Batch 21/64 loss: -0.21693021059036255
Batch 22/64 loss: -0.20693084597587585
Batch 23/64 loss: -0.2008458971977234
Batch 24/64 loss: -0.2055615484714508
Batch 25/64 loss: -0.17053741216659546
Batch 26/64 loss: -0.19702690839767456
Batch 27/64 loss: -0.1559056043624878
Batch 28/64 loss: -0.19198894500732422
Batch 29/64 loss: -0.18368858098983765
Batch 30/64 loss: -0.2064918875694275
Batch 31/64 loss: -0.1761522889137268
Batch 32/64 loss: -0.17426741123199463
Batch 33/64 loss: -0.19912338256835938
Batch 34/64 loss: -0.17192602157592773
Batch 35/64 loss: -0.19530975818634033
Batch 36/64 loss: -0.1585988998413086
Batch 37/64 loss: -0.18836212158203125
Batch 38/64 loss: -0.21373862028121948
Batch 39/64 loss: -0.18013834953308105
Batch 40/64 loss: -0.1872183084487915
Batch 41/64 loss: -0.1824766993522644
Batch 42/64 loss: -0.20805400609970093
Batch 43/64 loss: -0.19762247800827026
Batch 44/64 loss: -0.19186151027679443
Batch 45/64 loss: -0.18797200918197632
Batch 46/64 loss: -0.20258623361587524
Batch 47/64 loss: -0.18922972679138184
Batch 48/64 loss: -0.1823441982269287
Batch 49/64 loss: -0.17960774898529053
Batch 50/64 loss: -0.2072482407093048
Batch 51/64 loss: -0.198799729347229
Batch 52/64 loss: -0.18222779035568237
Batch 53/64 loss: -0.2046477496623993
Batch 54/64 loss: -0.2053404450416565
Batch 55/64 loss: -0.1799895167350769
Batch 56/64 loss: -0.1626306176185608
Batch 57/64 loss: -0.1633594036102295
Batch 58/64 loss: -0.13160419464111328
Batch 59/64 loss: -0.1839827299118042
Batch 60/64 loss: -0.2025049328804016
Batch 61/64 loss: -0.19066405296325684
Batch 62/64 loss: -0.2026592195034027
Batch 63/64 loss: -0.1790543794631958
Batch 64/64 loss: -0.16172540187835693
Epoch 411  Train loss: -0.19027034955866196  Val loss: -0.0165822499806119
Epoch 412
-------------------------------
Batch 1/64 loss: -0.19335079193115234
Batch 2/64 loss: -0.18092745542526245
Batch 3/64 loss: -0.20562374591827393
Batch 4/64 loss: -0.20699095726013184
Batch 5/64 loss: -0.18072259426116943
Batch 6/64 loss: -0.18508946895599365
Batch 7/64 loss: -0.20160603523254395
Batch 8/64 loss: -0.1857459545135498
Batch 9/64 loss: -0.19526374340057373
Batch 10/64 loss: -0.17634719610214233
Batch 11/64 loss: -0.15335285663604736
Batch 12/64 loss: -0.20386046171188354
Batch 13/64 loss: -0.1889352798461914
Batch 14/64 loss: -0.1780693531036377
Batch 15/64 loss: -0.17578554153442383
Batch 16/64 loss: -0.18497902154922485
Batch 17/64 loss: -0.19573497772216797
Batch 18/64 loss: -0.1966782808303833
Batch 19/64 loss: -0.12437945604324341
Batch 20/64 loss: -0.18018972873687744
Batch 21/64 loss: -0.20709893107414246
Batch 22/64 loss: -0.1674790382385254
Batch 23/64 loss: -0.20983627438545227
Batch 24/64 loss: -0.18802082538604736
Batch 25/64 loss: -0.20857888460159302
Batch 26/64 loss: -0.16409802436828613
Batch 27/64 loss: -0.206326425075531
Batch 28/64 loss: -0.19697332382202148
Batch 29/64 loss: -0.18519705533981323
Batch 30/64 loss: -0.17302048206329346
Batch 31/64 loss: -0.1734427809715271
Batch 32/64 loss: -0.20191651582717896
Batch 33/64 loss: -0.1981232762336731
Batch 34/64 loss: -0.20900744199752808
Batch 35/64 loss: -0.1677907109260559
Batch 36/64 loss: -0.18839895725250244
Batch 37/64 loss: -0.18930184841156006
Batch 38/64 loss: -0.20430761575698853
Batch 39/64 loss: -0.17621886730194092
Batch 40/64 loss: -0.18540048599243164
Batch 41/64 loss: -0.19066393375396729
Batch 42/64 loss: -0.20058569312095642
Batch 43/64 loss: -0.1834719181060791
Batch 44/64 loss: -0.18707460165023804
Batch 45/64 loss: -0.21467441320419312
Batch 46/64 loss: -0.18118828535079956
Batch 47/64 loss: -0.1874186396598816
Batch 48/64 loss: -0.20787131786346436
Batch 49/64 loss: -0.19288671016693115
Batch 50/64 loss: -0.18131428956985474
Batch 51/64 loss: -0.19858020544052124
Batch 52/64 loss: -0.17915737628936768
Batch 53/64 loss: -0.18742847442626953
Batch 54/64 loss: -0.2058337926864624
Batch 55/64 loss: -0.15739524364471436
Batch 56/64 loss: -0.18327748775482178
Batch 57/64 loss: -0.20663875341415405
Batch 58/64 loss: -0.20930635929107666
Batch 59/64 loss: -0.17176854610443115
Batch 60/64 loss: -0.19240295886993408
Batch 61/64 loss: -0.16296786069869995
Batch 62/64 loss: -0.15899395942687988
Batch 63/64 loss: -0.18107450008392334
Batch 64/64 loss: -0.2004605531692505
Epoch 412  Train loss: -0.1877096690383612  Val loss: -0.015211352572817983
Epoch 413
-------------------------------
Batch 1/64 loss: -0.18939673900604248
Batch 2/64 loss: -0.1885278820991516
Batch 3/64 loss: -0.19347798824310303
Batch 4/64 loss: -0.16290515661239624
Batch 5/64 loss: -0.18973100185394287
Batch 6/64 loss: -0.18032997846603394
Batch 7/64 loss: -0.22171640396118164
Batch 8/64 loss: -0.19567281007766724
Batch 9/64 loss: -0.20448291301727295
Batch 10/64 loss: -0.20823577046394348
Batch 11/64 loss: -0.17886143922805786
Batch 12/64 loss: -0.16547322273254395
Batch 13/64 loss: -0.19671714305877686
Batch 14/64 loss: -0.19266408681869507
Batch 15/64 loss: -0.20081958174705505
Batch 16/64 loss: -0.19384807348251343
Batch 17/64 loss: -0.17148339748382568
Batch 18/64 loss: -0.19208002090454102
Batch 19/64 loss: -0.18115019798278809
Batch 20/64 loss: -0.16873812675476074
Batch 21/64 loss: -0.18361973762512207
Batch 22/64 loss: -0.17310750484466553
Batch 23/64 loss: -0.17935925722122192
Batch 24/64 loss: -0.19261515140533447
Batch 25/64 loss: -0.20300984382629395
Batch 26/64 loss: -0.1828322410583496
Batch 27/64 loss: -0.17268085479736328
Batch 28/64 loss: -0.19287073612213135
Batch 29/64 loss: -0.17046356201171875
Batch 30/64 loss: -0.17569518089294434
Batch 31/64 loss: -0.17280089855194092
Batch 32/64 loss: -0.16651028394699097
Batch 33/64 loss: -0.17081820964813232
Batch 34/64 loss: -0.18417978286743164
Batch 35/64 loss: -0.19307148456573486
Batch 36/64 loss: -0.1946355104446411
Batch 37/64 loss: -0.198563814163208
Batch 38/64 loss: -0.18285346031188965
Batch 39/64 loss: -0.19503742456436157
Batch 40/64 loss: -0.18190670013427734
Batch 41/64 loss: -0.1838868260383606
Batch 42/64 loss: -0.1916155219078064
Batch 43/64 loss: -0.20182260870933533
Batch 44/64 loss: -0.21770963072776794
Batch 45/64 loss: -0.20417869091033936
Batch 46/64 loss: -0.18748778104782104
Batch 47/64 loss: -0.1726725697517395
Batch 48/64 loss: -0.18857288360595703
Batch 49/64 loss: -0.17051583528518677
Batch 50/64 loss: -0.16001933813095093
Batch 51/64 loss: -0.17445629835128784
Batch 52/64 loss: -0.20956233143806458
Batch 53/64 loss: -0.20457881689071655
Batch 54/64 loss: -0.20308616757392883
Batch 55/64 loss: -0.13881909847259521
Batch 56/64 loss: -0.15763771533966064
Batch 57/64 loss: -0.18211382627487183
Batch 58/64 loss: -0.19965854287147522
Batch 59/64 loss: -0.18734729290008545
Batch 60/64 loss: -0.198900043964386
Batch 61/64 loss: -0.17952442169189453
Batch 62/64 loss: -0.1535959243774414
Batch 63/64 loss: -0.19729918241500854
Batch 64/64 loss: -0.17865067720413208
Epoch 413  Train loss: -0.18575675183651494  Val loss: -0.01695639837238797
Epoch 414
-------------------------------
Batch 1/64 loss: -0.19036448001861572
Batch 2/64 loss: -0.20596343278884888
Batch 3/64 loss: -0.19512414932250977
Batch 4/64 loss: -0.19439297914505005
Batch 5/64 loss: -0.17954492568969727
Batch 6/64 loss: -0.1639014482498169
Batch 7/64 loss: -0.18894243240356445
Batch 8/64 loss: -0.2033429741859436
Batch 9/64 loss: -0.196092426776886
Batch 10/64 loss: -0.19415158033370972
Batch 11/64 loss: -0.20398542284965515
Batch 12/64 loss: -0.19756990671157837
Batch 13/64 loss: -0.21221747994422913
Batch 14/64 loss: -0.19176411628723145
Batch 15/64 loss: -0.18911820650100708
Batch 16/64 loss: -0.20165497064590454
Batch 17/64 loss: -0.1623471975326538
Batch 18/64 loss: -0.18561285734176636
Batch 19/64 loss: -0.2067244052886963
Batch 20/64 loss: -0.1964484453201294
Batch 21/64 loss: -0.14693742990493774
Batch 22/64 loss: -0.19150769710540771
Batch 23/64 loss: -0.20723524689674377
Batch 24/64 loss: -0.20102152228355408
Batch 25/64 loss: -0.18532294034957886
Batch 26/64 loss: -0.20057296752929688
Batch 27/64 loss: -0.16045337915420532
Batch 28/64 loss: -0.20069468021392822
Batch 29/64 loss: -0.18936550617218018
Batch 30/64 loss: -0.17994219064712524
Batch 31/64 loss: -0.194560706615448
Batch 32/64 loss: -0.18045330047607422
Batch 33/64 loss: -0.17264807224273682
Batch 34/64 loss: -0.20487064123153687
Batch 35/64 loss: -0.1915850043296814
Batch 36/64 loss: -0.18739598989486694
Batch 37/64 loss: -0.19568544626235962
Batch 38/64 loss: -0.18728190660476685
Batch 39/64 loss: -0.1815330982208252
Batch 40/64 loss: -0.1886531114578247
Batch 41/64 loss: -0.20519334077835083
Batch 42/64 loss: -0.1734735369682312
Batch 43/64 loss: -0.18172168731689453
Batch 44/64 loss: -0.17657846212387085
Batch 45/64 loss: -0.18198561668395996
Batch 46/64 loss: -0.18603718280792236
Batch 47/64 loss: -0.17410147190093994
Batch 48/64 loss: -0.20013967156410217
Batch 49/64 loss: -0.19929301738739014
Batch 50/64 loss: -0.19250446557998657
Batch 51/64 loss: -0.18506479263305664
Batch 52/64 loss: -0.19483602046966553
Batch 53/64 loss: -0.1847829818725586
Batch 54/64 loss: -0.17750728130340576
Batch 55/64 loss: -0.18899333477020264
Batch 56/64 loss: -0.18142831325531006
Batch 57/64 loss: -0.2044820785522461
Batch 58/64 loss: -0.2022668719291687
Batch 59/64 loss: -0.19337809085845947
Batch 60/64 loss: -0.16856753826141357
Batch 61/64 loss: -0.1743636131286621
Batch 62/64 loss: -0.18396484851837158
Batch 63/64 loss: -0.18154668807983398
Batch 64/64 loss: -0.19003593921661377
Epoch 414  Train loss: -0.1888897652719535  Val loss: -0.012486396581446593
Epoch 415
-------------------------------
Batch 1/64 loss: -0.21712258458137512
Batch 2/64 loss: -0.2109721302986145
Batch 3/64 loss: -0.1636708378791809
Batch 4/64 loss: -0.19504117965698242
Batch 5/64 loss: -0.16369730234146118
Batch 6/64 loss: -0.19659453630447388
Batch 7/64 loss: -0.1784265637397766
Batch 8/64 loss: -0.18548786640167236
Batch 9/64 loss: -0.1801818609237671
Batch 10/64 loss: -0.16338467597961426
Batch 11/64 loss: -0.1779423952102661
Batch 12/64 loss: -0.17571109533309937
Batch 13/64 loss: -0.20955562591552734
Batch 14/64 loss: -0.21001297235488892
Batch 15/64 loss: -0.21213874220848083
Batch 16/64 loss: -0.18578821420669556
Batch 17/64 loss: -0.188213050365448
Batch 18/64 loss: -0.1972745656967163
Batch 19/64 loss: -0.1597987413406372
Batch 20/64 loss: -0.20606312155723572
Batch 21/64 loss: -0.1924365758895874
Batch 22/64 loss: -0.17548424005508423
Batch 23/64 loss: -0.16808682680130005
Batch 24/64 loss: -0.16855335235595703
Batch 25/64 loss: -0.18200206756591797
Batch 26/64 loss: -0.1530855894088745
Batch 27/64 loss: -0.18886959552764893
Batch 28/64 loss: -0.1905369758605957
Batch 29/64 loss: -0.1778886914253235
Batch 30/64 loss: -0.20463675260543823
Batch 31/64 loss: -0.1886036992073059
Batch 32/64 loss: -0.20559769868850708
Batch 33/64 loss: -0.2023983597755432
Batch 34/64 loss: -0.19396573305130005
Batch 35/64 loss: -0.20307213068008423
Batch 36/64 loss: -0.20245864987373352
Batch 37/64 loss: -0.2008785903453827
Batch 38/64 loss: -0.20624133944511414
Batch 39/64 loss: -0.18909800052642822
Batch 40/64 loss: -0.15776848793029785
Batch 41/64 loss: -0.19184517860412598
Batch 42/64 loss: -0.14707130193710327
Batch 43/64 loss: -0.20783066749572754
Batch 44/64 loss: -0.18388772010803223
Batch 45/64 loss: -0.1872330904006958
Batch 46/64 loss: -0.21035706996917725
Batch 47/64 loss: -0.18975526094436646
Batch 48/64 loss: -0.20788052678108215
Batch 49/64 loss: -0.18882960081100464
Batch 50/64 loss: -0.18960458040237427
Batch 51/64 loss: -0.1938471794128418
Batch 52/64 loss: -0.20178818702697754
Batch 53/64 loss: -0.20145684480667114
Batch 54/64 loss: -0.17726099491119385
Batch 55/64 loss: -0.1954212784767151
Batch 56/64 loss: -0.20640826225280762
Batch 57/64 loss: -0.1881583333015442
Batch 58/64 loss: -0.20864596962928772
Batch 59/64 loss: -0.14349162578582764
Batch 60/64 loss: -0.1935073733329773
Batch 61/64 loss: -0.1772599220275879
Batch 62/64 loss: -0.1964530348777771
Batch 63/64 loss: -0.19949448108673096
Batch 64/64 loss: -0.19094473123550415
Epoch 415  Train loss: -0.1891676619941113  Val loss: -0.016636751156901986
Epoch 416
-------------------------------
Batch 1/64 loss: -0.19739603996276855
Batch 2/64 loss: -0.1670413613319397
Batch 3/64 loss: -0.21200501918792725
Batch 4/64 loss: -0.15151089429855347
Batch 5/64 loss: -0.18947148323059082
Batch 6/64 loss: -0.20037823915481567
Batch 7/64 loss: -0.1795809268951416
Batch 8/64 loss: -0.20301583409309387
Batch 9/64 loss: -0.21040982007980347
Batch 10/64 loss: -0.2024136185646057
Batch 11/64 loss: -0.21340647339820862
Batch 12/64 loss: -0.1913568377494812
Batch 13/64 loss: -0.1871356964111328
Batch 14/64 loss: -0.16739988327026367
Batch 15/64 loss: -0.20577186346054077
Batch 16/64 loss: -0.20651406049728394
Batch 17/64 loss: -0.20882058143615723
Batch 18/64 loss: -0.17663806676864624
Batch 19/64 loss: -0.1916103959083557
Batch 20/64 loss: -0.19092261791229248
Batch 21/64 loss: -0.21369332075119019
Batch 22/64 loss: -0.2065243124961853
Batch 23/64 loss: -0.2195035219192505
Batch 24/64 loss: -0.18655025959014893
Batch 25/64 loss: -0.1980646848678589
Batch 26/64 loss: -0.1949237585067749
Batch 27/64 loss: -0.19896674156188965
Batch 28/64 loss: -0.21743637323379517
Batch 29/64 loss: -0.20627474784851074
Batch 30/64 loss: -0.16636085510253906
Batch 31/64 loss: -0.18565261363983154
Batch 32/64 loss: -0.19722414016723633
Batch 33/64 loss: -0.1970682144165039
Batch 34/64 loss: -0.1930696964263916
Batch 35/64 loss: -0.19703572988510132
Batch 36/64 loss: -0.1909521222114563
Batch 37/64 loss: -0.20833534002304077
Batch 38/64 loss: -0.21514445543289185
Batch 39/64 loss: -0.19064736366271973
Batch 40/64 loss: -0.1277618408203125
Batch 41/64 loss: -0.18027079105377197
Batch 42/64 loss: -0.2122172713279724
Batch 43/64 loss: -0.1924726963043213
Batch 44/64 loss: -0.1871437430381775
Batch 45/64 loss: -0.19130444526672363
Batch 46/64 loss: -0.2090311050415039
Batch 47/64 loss: -0.184506356716156
Batch 48/64 loss: -0.18846619129180908
Batch 49/64 loss: -0.18635624647140503
Batch 50/64 loss: -0.2179470658302307
Batch 51/64 loss: -0.21323278546333313
Batch 52/64 loss: -0.1791258454322815
Batch 53/64 loss: -0.2065824568271637
Batch 54/64 loss: -0.17697805166244507
Batch 55/64 loss: -0.21960735321044922
Batch 56/64 loss: -0.18825984001159668
Batch 57/64 loss: -0.1949080228805542
Batch 58/64 loss: -0.1870211362838745
Batch 59/64 loss: -0.1936435103416443
Batch 60/64 loss: -0.18857789039611816
Batch 61/64 loss: -0.19360321760177612
Batch 62/64 loss: -0.1855785846710205
Batch 63/64 loss: -0.1759507656097412
Batch 64/64 loss: -0.18587112426757812
Epoch 416  Train loss: -0.19382240818996055  Val loss: -0.015555117548126536
Epoch 417
-------------------------------
Batch 1/64 loss: -0.1666487455368042
Batch 2/64 loss: -0.1933509111404419
Batch 3/64 loss: -0.19974875450134277
Batch 4/64 loss: -0.19164550304412842
Batch 5/64 loss: -0.18415087461471558
Batch 6/64 loss: -0.1865437626838684
Batch 7/64 loss: -0.1885133981704712
Batch 8/64 loss: -0.18581914901733398
Batch 9/64 loss: -0.19803351163864136
Batch 10/64 loss: -0.1715102195739746
Batch 11/64 loss: -0.1979650855064392
Batch 12/64 loss: -0.2212812304496765
Batch 13/64 loss: -0.19350391626358032
Batch 14/64 loss: -0.19155478477478027
Batch 15/64 loss: -0.18572890758514404
Batch 16/64 loss: -0.2030784785747528
Batch 17/64 loss: -0.1863650679588318
Batch 18/64 loss: -0.2086814045906067
Batch 19/64 loss: -0.18225622177124023
Batch 20/64 loss: -0.1968567967414856
Batch 21/64 loss: -0.19595235586166382
Batch 22/64 loss: -0.20099779963493347
Batch 23/64 loss: -0.20209771394729614
Batch 24/64 loss: -0.19183099269866943
Batch 25/64 loss: -0.20189735293388367
Batch 26/64 loss: -0.21790143847465515
Batch 27/64 loss: -0.15926915407180786
Batch 28/64 loss: -0.17986321449279785
Batch 29/64 loss: -0.1899569034576416
Batch 30/64 loss: -0.20025470852851868
Batch 31/64 loss: -0.18293428421020508
Batch 32/64 loss: -0.18063056468963623
Batch 33/64 loss: -0.16704928874969482
Batch 34/64 loss: -0.1876874566078186
Batch 35/64 loss: -0.1969221830368042
Batch 36/64 loss: -0.20239049196243286
Batch 37/64 loss: -0.1797216534614563
Batch 38/64 loss: -0.2033507525920868
Batch 39/64 loss: -0.20170903205871582
Batch 40/64 loss: -0.19173508882522583
Batch 41/64 loss: -0.20120453834533691
Batch 42/64 loss: -0.18641895055770874
Batch 43/64 loss: -0.1968875527381897
Batch 44/64 loss: -0.18370497226715088
Batch 45/64 loss: -0.19253075122833252
Batch 46/64 loss: -0.19088095426559448
Batch 47/64 loss: -0.20759329199790955
Batch 48/64 loss: -0.2109394371509552
Batch 49/64 loss: -0.20275136828422546
Batch 50/64 loss: -0.17042821645736694
Batch 51/64 loss: -0.1793069839477539
Batch 52/64 loss: -0.1691088080406189
Batch 53/64 loss: -0.18019884824752808
Batch 54/64 loss: -0.18223720788955688
Batch 55/64 loss: -0.1995965838432312
Batch 56/64 loss: -0.18895334005355835
Batch 57/64 loss: -0.16959142684936523
Batch 58/64 loss: -0.17935872077941895
Batch 59/64 loss: -0.1861438751220703
Batch 60/64 loss: -0.1967136263847351
Batch 61/64 loss: -0.1907595992088318
Batch 62/64 loss: -0.20711979269981384
Batch 63/64 loss: -0.18384194374084473
Batch 64/64 loss: -0.1699422001838684
Epoch 417  Train loss: -0.190605750504662  Val loss: -0.014974946418578682
Epoch 418
-------------------------------
Batch 1/64 loss: -0.1855703592300415
Batch 2/64 loss: -0.19160079956054688
Batch 3/64 loss: -0.18332475423812866
Batch 4/64 loss: -0.20547062158584595
Batch 5/64 loss: -0.16197288036346436
Batch 6/64 loss: -0.19395387172698975
Batch 7/64 loss: -0.1999199390411377
Batch 8/64 loss: -0.19633078575134277
Batch 9/64 loss: -0.20787963271141052
Batch 10/64 loss: -0.2028767466545105
Batch 11/64 loss: -0.14261364936828613
Batch 12/64 loss: -0.1930137276649475
Batch 13/64 loss: -0.18853771686553955
Batch 14/64 loss: -0.1532137393951416
Batch 15/64 loss: -0.19370031356811523
Batch 16/64 loss: -0.1833379864692688
Batch 17/64 loss: -0.19755584001541138
Batch 18/64 loss: -0.19267159700393677
Batch 19/64 loss: -0.18885266780853271
Batch 20/64 loss: -0.19290542602539062
Batch 21/64 loss: -0.17888283729553223
Batch 22/64 loss: -0.18081605434417725
Batch 23/64 loss: -0.16821694374084473
Batch 24/64 loss: -0.2059592306613922
Batch 25/64 loss: -0.198777437210083
Batch 26/64 loss: -0.20120838284492493
Batch 27/64 loss: -0.17651325464248657
Batch 28/64 loss: -0.19107002019882202
Batch 29/64 loss: -0.21164315938949585
Batch 30/64 loss: -0.20128875970840454
Batch 31/64 loss: -0.19314181804656982
Batch 32/64 loss: -0.21160972118377686
Batch 33/64 loss: -0.20053935050964355
Batch 34/64 loss: -0.2082051932811737
Batch 35/64 loss: -0.1769423484802246
Batch 36/64 loss: -0.17096495628356934
Batch 37/64 loss: -0.19910576939582825
Batch 38/64 loss: -0.20692139863967896
Batch 39/64 loss: -0.19011127948760986
Batch 40/64 loss: -0.18471956253051758
Batch 41/64 loss: -0.1863788366317749
Batch 42/64 loss: -0.18367522954940796
Batch 43/64 loss: -0.19700920581817627
Batch 44/64 loss: -0.1891975998878479
Batch 45/64 loss: -0.20566853880882263
Batch 46/64 loss: -0.17255914211273193
Batch 47/64 loss: -0.1872909665107727
Batch 48/64 loss: -0.19100379943847656
Batch 49/64 loss: -0.18716418743133545
Batch 50/64 loss: -0.20095869898796082
Batch 51/64 loss: -0.1990198791027069
Batch 52/64 loss: -0.18183255195617676
Batch 53/64 loss: -0.19696056842803955
Batch 54/64 loss: -0.21260800957679749
Batch 55/64 loss: -0.20858120918273926
Batch 56/64 loss: -0.18656951189041138
Batch 57/64 loss: -0.19362688064575195
Batch 58/64 loss: -0.164115309715271
Batch 59/64 loss: -0.18453657627105713
Batch 60/64 loss: -0.18450075387954712
Batch 61/64 loss: -0.19826340675354004
Batch 62/64 loss: -0.18098413944244385
Batch 63/64 loss: -0.18969053030014038
Batch 64/64 loss: -0.18405866622924805
Epoch 418  Train loss: -0.1903087069006527  Val loss: -0.014706198292499556
Epoch 419
-------------------------------
Batch 1/64 loss: -0.18110692501068115
Batch 2/64 loss: -0.19154351949691772
Batch 3/64 loss: -0.1876751184463501
Batch 4/64 loss: -0.18163955211639404
Batch 5/64 loss: -0.19854867458343506
Batch 6/64 loss: -0.20214521884918213
Batch 7/64 loss: -0.2087334394454956
Batch 8/64 loss: -0.18718183040618896
Batch 9/64 loss: -0.16130077838897705
Batch 10/64 loss: -0.1799607276916504
Batch 11/64 loss: -0.20128095149993896
Batch 12/64 loss: -0.19136083126068115
Batch 13/64 loss: -0.19970867037773132
Batch 14/64 loss: -0.17587482929229736
Batch 15/64 loss: -0.20622867345809937
Batch 16/64 loss: -0.20269665122032166
Batch 17/64 loss: -0.1900695562362671
Batch 18/64 loss: -0.20057982206344604
Batch 19/64 loss: -0.21942490339279175
Batch 20/64 loss: -0.18927055597305298
Batch 21/64 loss: -0.19167470932006836
Batch 22/64 loss: -0.21922504901885986
Batch 23/64 loss: -0.19749921560287476
Batch 24/64 loss: -0.19743984937667847
Batch 25/64 loss: -0.21818405389785767
Batch 26/64 loss: -0.19147473573684692
Batch 27/64 loss: -0.20230048894882202
Batch 28/64 loss: -0.21003445982933044
Batch 29/64 loss: -0.18435853719711304
Batch 30/64 loss: -0.19900000095367432
Batch 31/64 loss: -0.1792348027229309
Batch 32/64 loss: -0.19259250164031982
Batch 33/64 loss: -0.1943950653076172
Batch 34/64 loss: -0.20483845472335815
Batch 35/64 loss: -0.1912473440170288
Batch 36/64 loss: -0.17455869913101196
Batch 37/64 loss: -0.17585331201553345
Batch 38/64 loss: -0.17516344785690308
Batch 39/64 loss: -0.20168626308441162
Batch 40/64 loss: -0.19034814834594727
Batch 41/64 loss: -0.18645405769348145
Batch 42/64 loss: -0.20082762837409973
Batch 43/64 loss: -0.20292502641677856
Batch 44/64 loss: -0.20613139867782593
Batch 45/64 loss: -0.18130338191986084
Batch 46/64 loss: -0.18641430139541626
Batch 47/64 loss: -0.1873188614845276
Batch 48/64 loss: -0.20298585295677185
Batch 49/64 loss: -0.18407487869262695
Batch 50/64 loss: -0.18029147386550903
Batch 51/64 loss: -0.17354828119277954
Batch 52/64 loss: -0.19502580165863037
Batch 53/64 loss: -0.2143557369709015
Batch 54/64 loss: -0.19894635677337646
Batch 55/64 loss: -0.17467641830444336
Batch 56/64 loss: -0.1709585189819336
Batch 57/64 loss: -0.1986335813999176
Batch 58/64 loss: -0.18965762853622437
Batch 59/64 loss: -0.18927377462387085
Batch 60/64 loss: -0.20063748955726624
Batch 61/64 loss: -0.1683180332183838
Batch 62/64 loss: -0.208654522895813
Batch 63/64 loss: -0.20706218481063843
Batch 64/64 loss: -0.19069838523864746
Epoch 419  Train loss: -0.19292453878066118  Val loss: -0.014407705605234886
Epoch 420
-------------------------------
Batch 1/64 loss: -0.1960347294807434
Batch 2/64 loss: -0.2138405442237854
Batch 3/64 loss: -0.19945138692855835
Batch 4/64 loss: -0.19882303476333618
Batch 5/64 loss: -0.17924928665161133
Batch 6/64 loss: -0.1792076826095581
Batch 7/64 loss: -0.18189316987991333
Batch 8/64 loss: -0.19672399759292603
Batch 9/64 loss: -0.21661686897277832
Batch 10/64 loss: -0.19829058647155762
Batch 11/64 loss: -0.19658273458480835
Batch 12/64 loss: -0.19897985458374023
Batch 13/64 loss: -0.1796860694885254
Batch 14/64 loss: -0.200053870677948
Batch 15/64 loss: -0.208594411611557
Batch 16/64 loss: -0.18694770336151123
Batch 17/64 loss: -0.19350296258926392
Batch 18/64 loss: -0.19890230894088745
Batch 19/64 loss: -0.19616001844406128
Batch 20/64 loss: -0.17289263010025024
Batch 21/64 loss: -0.17109668254852295
Batch 22/64 loss: -0.19120144844055176
Batch 23/64 loss: -0.19782555103302002
Batch 24/64 loss: -0.2046208381652832
Batch 25/64 loss: -0.21010428667068481
Batch 26/64 loss: -0.1949394941329956
Batch 27/64 loss: -0.1637464165687561
Batch 28/64 loss: -0.18909239768981934
Batch 29/64 loss: -0.1964329481124878
Batch 30/64 loss: -0.21427828073501587
Batch 31/64 loss: -0.19941192865371704
Batch 32/64 loss: -0.18419045209884644
Batch 33/64 loss: -0.1820756196975708
Batch 34/64 loss: -0.20060014724731445
Batch 35/64 loss: -0.19107776880264282
Batch 36/64 loss: -0.19891393184661865
Batch 37/64 loss: -0.18843871355056763
Batch 38/64 loss: -0.19425058364868164
Batch 39/64 loss: -0.192552387714386
Batch 40/64 loss: -0.19046878814697266
Batch 41/64 loss: -0.20464062690734863
Batch 42/64 loss: -0.1753479242324829
Batch 43/64 loss: -0.19582802057266235
Batch 44/64 loss: -0.20084965229034424
Batch 45/64 loss: -0.2069830596446991
Batch 46/64 loss: -0.2107195258140564
Batch 47/64 loss: -0.16492128372192383
Batch 48/64 loss: -0.18407666683197021
Batch 49/64 loss: -0.18849670886993408
Batch 50/64 loss: -0.19043302536010742
Batch 51/64 loss: -0.20749983191490173
Batch 52/64 loss: -0.19442814588546753
Batch 53/64 loss: -0.19500064849853516
Batch 54/64 loss: -0.1876658797264099
Batch 55/64 loss: -0.20096445083618164
Batch 56/64 loss: -0.19340527057647705
Batch 57/64 loss: -0.19106286764144897
Batch 58/64 loss: -0.20012962818145752
Batch 59/64 loss: -0.19799774885177612
Batch 60/64 loss: -0.20055383443832397
Batch 61/64 loss: -0.1673142910003662
Batch 62/64 loss: -0.17413508892059326
Batch 63/64 loss: -0.15488290786743164
Batch 64/64 loss: -0.19977420568466187
Epoch 420  Train loss: -0.19270463153427722  Val loss: -0.014308655999370456
Epoch 421
-------------------------------
Batch 1/64 loss: -0.206143319606781
Batch 2/64 loss: -0.1844158172607422
Batch 3/64 loss: -0.18213820457458496
Batch 4/64 loss: -0.20218634605407715
Batch 5/64 loss: -0.20173293352127075
Batch 6/64 loss: -0.21207523345947266
Batch 7/64 loss: -0.1887967586517334
Batch 8/64 loss: -0.22558027505874634
Batch 9/64 loss: -0.21133646368980408
Batch 10/64 loss: -0.20268160104751587
Batch 11/64 loss: -0.19686150550842285
Batch 12/64 loss: -0.18100327253341675
Batch 13/64 loss: -0.20121413469314575
Batch 14/64 loss: -0.13979923725128174
Batch 15/64 loss: -0.1979811191558838
Batch 16/64 loss: -0.18379539251327515
Batch 17/64 loss: -0.16905707120895386
Batch 18/64 loss: -0.18016284704208374
Batch 19/64 loss: -0.20811477303504944
Batch 20/64 loss: -0.1947638988494873
Batch 21/64 loss: -0.19755563139915466
Batch 22/64 loss: -0.1898726224899292
Batch 23/64 loss: -0.198542058467865
Batch 24/64 loss: -0.19167768955230713
Batch 25/64 loss: -0.19255924224853516
Batch 26/64 loss: -0.15676432847976685
Batch 27/64 loss: -0.22228360176086426
Batch 28/64 loss: -0.19570177793502808
Batch 29/64 loss: -0.1943938136100769
Batch 30/64 loss: -0.20522484183311462
Batch 31/64 loss: -0.19522738456726074
Batch 32/64 loss: -0.19044631719589233
Batch 33/64 loss: -0.20967820286750793
Batch 34/64 loss: -0.18303072452545166
Batch 35/64 loss: -0.16979706287384033
Batch 36/64 loss: -0.20582669973373413
Batch 37/64 loss: -0.20090588927268982
Batch 38/64 loss: -0.19804620742797852
Batch 39/64 loss: -0.18595433235168457
Batch 40/64 loss: -0.1837671399116516
Batch 41/64 loss: -0.18422293663024902
Batch 42/64 loss: -0.19202566146850586
Batch 43/64 loss: -0.20339751243591309
Batch 44/64 loss: -0.18244826793670654
Batch 45/64 loss: -0.15118324756622314
Batch 46/64 loss: -0.20559021830558777
Batch 47/64 loss: -0.1789056658744812
Batch 48/64 loss: -0.20082223415374756
Batch 49/64 loss: -0.19461870193481445
Batch 50/64 loss: -0.1980161964893341
Batch 51/64 loss: -0.19518005847930908
Batch 52/64 loss: -0.18274497985839844
Batch 53/64 loss: -0.18795186281204224
Batch 54/64 loss: -0.18629026412963867
Batch 55/64 loss: -0.19965380430221558
Batch 56/64 loss: -0.21187365055084229
Batch 57/64 loss: -0.18044745922088623
Batch 58/64 loss: -0.19417518377304077
Batch 59/64 loss: -0.17215293645858765
Batch 60/64 loss: -0.17798876762390137
Batch 61/64 loss: -0.20336365699768066
Batch 62/64 loss: -0.19963198900222778
Batch 63/64 loss: -0.19387251138687134
Batch 64/64 loss: -0.2133353054523468
Epoch 421  Train loss: -0.19252792187765533  Val loss: -0.018377069140627623
Epoch 422
-------------------------------
Batch 1/64 loss: -0.20162296295166016
Batch 2/64 loss: -0.1741759181022644
Batch 3/64 loss: -0.2044585943222046
Batch 4/64 loss: -0.1865077018737793
Batch 5/64 loss: -0.2024911344051361
Batch 6/64 loss: -0.21060711145401
Batch 7/64 loss: -0.20116287469863892
Batch 8/64 loss: -0.20835378766059875
Batch 9/64 loss: -0.19060885906219482
Batch 10/64 loss: -0.1904163956642151
Batch 11/64 loss: -0.18443810939788818
Batch 12/64 loss: -0.201147198677063
Batch 13/64 loss: -0.2045111358165741
Batch 14/64 loss: -0.1916905641555786
Batch 15/64 loss: -0.18537497520446777
Batch 16/64 loss: -0.19511383771896362
Batch 17/64 loss: -0.20259243249893188
Batch 18/64 loss: -0.20219269394874573
Batch 19/64 loss: -0.2009987235069275
Batch 20/64 loss: -0.1999356746673584
Batch 21/64 loss: -0.18688178062438965
Batch 22/64 loss: -0.18751519918441772
Batch 23/64 loss: -0.1774330735206604
Batch 24/64 loss: -0.17662906646728516
Batch 25/64 loss: -0.1861170530319214
Batch 26/64 loss: -0.18240976333618164
Batch 27/64 loss: -0.15384244918823242
Batch 28/64 loss: -0.18568098545074463
Batch 29/64 loss: -0.17351293563842773
Batch 30/64 loss: -0.165075421333313
Batch 31/64 loss: -0.18877393007278442
Batch 32/64 loss: -0.19055336713790894
Batch 33/64 loss: -0.19239968061447144
Batch 34/64 loss: -0.19407594203948975
Batch 35/64 loss: -0.19606181979179382
Batch 36/64 loss: -0.18913936614990234
Batch 37/64 loss: -0.20648902654647827
Batch 38/64 loss: -0.19856780767440796
Batch 39/64 loss: -0.2038794755935669
Batch 40/64 loss: -0.16125249862670898
Batch 41/64 loss: -0.17583799362182617
Batch 42/64 loss: -0.19795414805412292
Batch 43/64 loss: -0.20011508464813232
Batch 44/64 loss: -0.20436528325080872
Batch 45/64 loss: -0.1906217336654663
Batch 46/64 loss: -0.2163131833076477
Batch 47/64 loss: -0.16577279567718506
Batch 48/64 loss: -0.170260488986969
Batch 49/64 loss: -0.19841864705085754
Batch 50/64 loss: -0.2030428946018219
Batch 51/64 loss: -0.19633573293685913
Batch 52/64 loss: -0.1861974000930786
Batch 53/64 loss: -0.19205546379089355
Batch 54/64 loss: -0.1770474910736084
Batch 55/64 loss: -0.18219244480133057
Batch 56/64 loss: -0.182725191116333
Batch 57/64 loss: -0.17542660236358643
Batch 58/64 loss: -0.19814473390579224
Batch 59/64 loss: -0.19856154918670654
Batch 60/64 loss: -0.20698729157447815
Batch 61/64 loss: -0.20098543167114258
Batch 62/64 loss: -0.2104775607585907
Batch 63/64 loss: -0.1997290849685669
Batch 64/64 loss: -0.1959238052368164
Epoch 422  Train loss: -0.19154827314264633  Val loss: -0.015269894780162274
Epoch 423
-------------------------------
Batch 1/64 loss: -0.1929643154144287
Batch 2/64 loss: -0.1920979619026184
Batch 3/64 loss: -0.19269490242004395
Batch 4/64 loss: -0.17413580417633057
Batch 5/64 loss: -0.21485233306884766
Batch 6/64 loss: -0.1934727430343628
Batch 7/64 loss: -0.20388519763946533
Batch 8/64 loss: -0.17471528053283691
Batch 9/64 loss: -0.17462247610092163
Batch 10/64 loss: -0.193045973777771
Batch 11/64 loss: -0.1976640522480011
Batch 12/64 loss: -0.18995189666748047
Batch 13/64 loss: -0.2093701958656311
Batch 14/64 loss: -0.22650545835494995
Batch 15/64 loss: -0.1924196481704712
Batch 16/64 loss: -0.19280731678009033
Batch 17/64 loss: -0.21037697792053223
Batch 18/64 loss: -0.18891561031341553
Batch 19/64 loss: -0.15925663709640503
Batch 20/64 loss: -0.19761604070663452
Batch 21/64 loss: -0.1953868865966797
Batch 22/64 loss: -0.16468775272369385
Batch 23/64 loss: -0.1817401647567749
Batch 24/64 loss: -0.18966686725616455
Batch 25/64 loss: -0.19050872325897217
Batch 26/64 loss: -0.18481546640396118
Batch 27/64 loss: -0.15823382139205933
Batch 28/64 loss: -0.21149146556854248
Batch 29/64 loss: -0.18759316205978394
Batch 30/64 loss: -0.15215837955474854
Batch 31/64 loss: -0.1837483048439026
Batch 32/64 loss: -0.19777202606201172
Batch 33/64 loss: -0.21159571409225464
Batch 34/64 loss: -0.20065605640411377
Batch 35/64 loss: -0.20390281081199646
Batch 36/64 loss: -0.20724576711654663
Batch 37/64 loss: -0.19360214471817017
Batch 38/64 loss: -0.19516849517822266
Batch 39/64 loss: -0.2105920910835266
Batch 40/64 loss: -0.20088094472885132
Batch 41/64 loss: -0.20080900192260742
Batch 42/64 loss: -0.20868900418281555
Batch 43/64 loss: -0.20443293452262878
Batch 44/64 loss: -0.18796944618225098
Batch 45/64 loss: -0.2135639786720276
Batch 46/64 loss: -0.18097561597824097
Batch 47/64 loss: -0.21074962615966797
Batch 48/64 loss: -0.1909981369972229
Batch 49/64 loss: -0.2056099772453308
Batch 50/64 loss: -0.18004578351974487
Batch 51/64 loss: -0.13870561122894287
Batch 52/64 loss: -0.20613422989845276
Batch 53/64 loss: -0.21138450503349304
Batch 54/64 loss: -0.18843960762023926
Batch 55/64 loss: -0.1832403540611267
Batch 56/64 loss: -0.1615431308746338
Batch 57/64 loss: -0.20004802942276
Batch 58/64 loss: -0.17726653814315796
Batch 59/64 loss: -0.19172149896621704
Batch 60/64 loss: -0.1850777268409729
Batch 61/64 loss: -0.19137287139892578
Batch 62/64 loss: -0.1992616057395935
Batch 63/64 loss: -0.20099079608917236
Batch 64/64 loss: -0.20365315675735474
Epoch 423  Train loss: -0.19238569011875228  Val loss: -0.017141964427384315
Epoch 424
-------------------------------
Batch 1/64 loss: -0.15964680910110474
Batch 2/64 loss: -0.19539475440979004
Batch 3/64 loss: -0.19586825370788574
Batch 4/64 loss: -0.21351301670074463
Batch 5/64 loss: -0.2034064531326294
Batch 6/64 loss: -0.2250538468360901
Batch 7/64 loss: -0.2171376645565033
Batch 8/64 loss: -0.19520843029022217
Batch 9/64 loss: -0.20462298393249512
Batch 10/64 loss: -0.2002125382423401
Batch 11/64 loss: -0.1905558705329895
Batch 12/64 loss: -0.18971258401870728
Batch 13/64 loss: -0.21961283683776855
Batch 14/64 loss: -0.20382580161094666
Batch 15/64 loss: -0.18968629837036133
Batch 16/64 loss: -0.19185924530029297
Batch 17/64 loss: -0.17192721366882324
Batch 18/64 loss: -0.20356345176696777
Batch 19/64 loss: -0.17726671695709229
Batch 20/64 loss: -0.18802422285079956
Batch 21/64 loss: -0.18025803565979004
Batch 22/64 loss: -0.17817074060440063
Batch 23/64 loss: -0.17494279146194458
Batch 24/64 loss: -0.18171894550323486
Batch 25/64 loss: -0.14307361841201782
Batch 26/64 loss: -0.18852084875106812
Batch 27/64 loss: -0.1781480312347412
Batch 28/64 loss: -0.2071210741996765
Batch 29/64 loss: -0.20052993297576904
Batch 30/64 loss: -0.1809936761856079
Batch 31/64 loss: -0.2044709026813507
Batch 32/64 loss: -0.17620325088500977
Batch 33/64 loss: -0.1813734769821167
Batch 34/64 loss: -0.19187307357788086
Batch 35/64 loss: -0.1793254017829895
Batch 36/64 loss: -0.1903579831123352
Batch 37/64 loss: -0.17730259895324707
Batch 38/64 loss: -0.17126834392547607
Batch 39/64 loss: -0.2166295349597931
Batch 40/64 loss: -0.19259631633758545
Batch 41/64 loss: -0.19507622718811035
Batch 42/64 loss: -0.19671416282653809
Batch 43/64 loss: -0.19017797708511353
Batch 44/64 loss: -0.1992465853691101
Batch 45/64 loss: -0.18758434057235718
Batch 46/64 loss: -0.1953204870223999
Batch 47/64 loss: -0.18548166751861572
Batch 48/64 loss: -0.15928375720977783
Batch 49/64 loss: -0.2024533748626709
Batch 50/64 loss: -0.18221807479858398
Batch 51/64 loss: -0.21008431911468506
Batch 52/64 loss: -0.17835581302642822
Batch 53/64 loss: -0.2137906551361084
Batch 54/64 loss: -0.1890985369682312
Batch 55/64 loss: -0.20517286658287048
Batch 56/64 loss: -0.20024681091308594
Batch 57/64 loss: -0.18269890546798706
Batch 58/64 loss: -0.20245948433876038
Batch 59/64 loss: -0.20227184891700745
Batch 60/64 loss: -0.1838926076889038
Batch 61/64 loss: -0.18115341663360596
Batch 62/64 loss: -0.19697827100753784
Batch 63/64 loss: -0.17988884449005127
Batch 64/64 loss: -0.22758200764656067
Epoch 424  Train loss: -0.19170687233700473  Val loss: -0.015602606473509799
Epoch 425
-------------------------------
Batch 1/64 loss: -0.18578463792800903
Batch 2/64 loss: -0.2087215781211853
Batch 3/64 loss: -0.20940366387367249
Batch 4/64 loss: -0.18078124523162842
Batch 5/64 loss: -0.18999671936035156
Batch 6/64 loss: -0.19975143671035767
Batch 7/64 loss: -0.21672561764717102
Batch 8/64 loss: -0.1923256516456604
Batch 9/64 loss: -0.21908721327781677
Batch 10/64 loss: -0.177665114402771
Batch 11/64 loss: -0.2022680640220642
Batch 12/64 loss: -0.18798762559890747
Batch 13/64 loss: -0.19033950567245483
Batch 14/64 loss: -0.19460850954055786
Batch 15/64 loss: -0.20965039730072021
Batch 16/64 loss: -0.17020928859710693
Batch 17/64 loss: -0.21202298998832703
Batch 18/64 loss: -0.21012172102928162
Batch 19/64 loss: -0.2160097062587738
Batch 20/64 loss: -0.17056065797805786
Batch 21/64 loss: -0.18387043476104736
Batch 22/64 loss: -0.21281278133392334
Batch 23/64 loss: -0.1982736587524414
Batch 24/64 loss: -0.18405216932296753
Batch 25/64 loss: -0.20601075887680054
Batch 26/64 loss: -0.17348426580429077
Batch 27/64 loss: -0.1978110671043396
Batch 28/64 loss: -0.183222234249115
Batch 29/64 loss: -0.2051408886909485
Batch 30/64 loss: -0.20357388257980347
Batch 31/64 loss: -0.19660979509353638
Batch 32/64 loss: -0.1816807985305786
Batch 33/64 loss: -0.19023483991622925
Batch 34/64 loss: -0.19287604093551636
Batch 35/64 loss: -0.16431617736816406
Batch 36/64 loss: -0.194360613822937
Batch 37/64 loss: -0.1857140064239502
Batch 38/64 loss: -0.1902638077735901
Batch 39/64 loss: -0.19423770904541016
Batch 40/64 loss: -0.1991027593612671
Batch 41/64 loss: -0.20022550225257874
Batch 42/64 loss: -0.20030459761619568
Batch 43/64 loss: -0.2003934681415558
Batch 44/64 loss: -0.18819445371627808
Batch 45/64 loss: -0.1621798872947693
Batch 46/64 loss: -0.16568362712860107
Batch 47/64 loss: -0.19653981924057007
Batch 48/64 loss: -0.1852273941040039
Batch 49/64 loss: -0.17244374752044678
Batch 50/64 loss: -0.18688833713531494
Batch 51/64 loss: -0.18795251846313477
Batch 52/64 loss: -0.19047021865844727
Batch 53/64 loss: -0.2083626687526703
Batch 54/64 loss: -0.1834544539451599
Batch 55/64 loss: -0.1778411865234375
Batch 56/64 loss: -0.1673983335494995
Batch 57/64 loss: -0.19206267595291138
Batch 58/64 loss: -0.19356632232666016
Batch 59/64 loss: -0.2147308588027954
Batch 60/64 loss: -0.20668432116508484
Batch 61/64 loss: -0.20422756671905518
Batch 62/64 loss: -0.20474082231521606
Batch 63/64 loss: -0.18592029809951782
Batch 64/64 loss: -0.1982385516166687
Epoch 425  Train loss: -0.19303281611087275  Val loss: -0.01392493481488572
Epoch 426
-------------------------------
Batch 1/64 loss: -0.17208468914031982
Batch 2/64 loss: -0.21147483587265015
Batch 3/64 loss: -0.18730658292770386
Batch 4/64 loss: -0.16812485456466675
Batch 5/64 loss: -0.1962723731994629
Batch 6/64 loss: -0.2069670855998993
Batch 7/64 loss: -0.19192713499069214
Batch 8/64 loss: -0.17095565795898438
Batch 9/64 loss: -0.22119459509849548
Batch 10/64 loss: -0.195664644241333
Batch 11/64 loss: -0.19349569082260132
Batch 12/64 loss: -0.19882196187973022
Batch 13/64 loss: -0.17922812700271606
Batch 14/64 loss: -0.2052738070487976
Batch 15/64 loss: -0.17141592502593994
Batch 16/64 loss: -0.1812964677810669
Batch 17/64 loss: -0.20113074779510498
Batch 18/64 loss: -0.19174784421920776
Batch 19/64 loss: -0.19932067394256592
Batch 20/64 loss: -0.21429601311683655
Batch 21/64 loss: -0.20547151565551758
Batch 22/64 loss: -0.19858193397521973
Batch 23/64 loss: -0.1995302438735962
Batch 24/64 loss: -0.21437188982963562
Batch 25/64 loss: -0.17915117740631104
Batch 26/64 loss: -0.189983069896698
Batch 27/64 loss: -0.21425381302833557
Batch 28/64 loss: -0.19246578216552734
Batch 29/64 loss: -0.20576190948486328
Batch 30/64 loss: -0.1727641224861145
Batch 31/64 loss: -0.20880979299545288
Batch 32/64 loss: -0.1906813383102417
Batch 33/64 loss: -0.19278746843338013
Batch 34/64 loss: -0.19323128461837769
Batch 35/64 loss: -0.1760721206665039
Batch 36/64 loss: -0.20291614532470703
Batch 37/64 loss: -0.177964448928833
Batch 38/64 loss: -0.216763436794281
Batch 39/64 loss: -0.21237599849700928
Batch 40/64 loss: -0.21912625432014465
Batch 41/64 loss: -0.1666712760925293
Batch 42/64 loss: -0.19360363483428955
Batch 43/64 loss: -0.20143485069274902
Batch 44/64 loss: -0.18880754709243774
Batch 45/64 loss: -0.19051843881607056
Batch 46/64 loss: -0.1979530155658722
Batch 47/64 loss: -0.18693357706069946
Batch 48/64 loss: -0.2186732292175293
Batch 49/64 loss: -0.20632296800613403
Batch 50/64 loss: -0.19636327028274536
Batch 51/64 loss: -0.18694645166397095
Batch 52/64 loss: -0.1752690076828003
Batch 53/64 loss: -0.18354278802871704
Batch 54/64 loss: -0.2076244354248047
Batch 55/64 loss: -0.2179695963859558
Batch 56/64 loss: -0.19381004571914673
Batch 57/64 loss: -0.18716073036193848
Batch 58/64 loss: -0.1510404348373413
Batch 59/64 loss: -0.19783198833465576
Batch 60/64 loss: -0.20929649472236633
Batch 61/64 loss: -0.1758449673652649
Batch 62/64 loss: -0.189652681350708
Batch 63/64 loss: -0.16885113716125488
Batch 64/64 loss: -0.19797885417938232
Epoch 426  Train loss: -0.1939085516275144  Val loss: -0.015863129363436878
Epoch 427
-------------------------------
Batch 1/64 loss: -0.18261581659317017
Batch 2/64 loss: -0.19676482677459717
Batch 3/64 loss: -0.1823696494102478
Batch 4/64 loss: -0.20561546087265015
Batch 5/64 loss: -0.1835421919822693
Batch 6/64 loss: -0.19391906261444092
Batch 7/64 loss: -0.1925675868988037
Batch 8/64 loss: -0.18832576274871826
Batch 9/64 loss: -0.19624948501586914
Batch 10/64 loss: -0.1936432123184204
Batch 11/64 loss: -0.1929064393043518
Batch 12/64 loss: -0.18038290739059448
Batch 13/64 loss: -0.20299381017684937
Batch 14/64 loss: -0.20933544635772705
Batch 15/64 loss: -0.2064472734928131
Batch 16/64 loss: -0.2111627757549286
Batch 17/64 loss: -0.20698130130767822
Batch 18/64 loss: -0.19473308324813843
Batch 19/64 loss: -0.2073812484741211
Batch 20/64 loss: -0.20130932331085205
Batch 21/64 loss: -0.1981838345527649
Batch 22/64 loss: -0.19227468967437744
Batch 23/64 loss: -0.17863792181015015
Batch 24/64 loss: -0.21147727966308594
Batch 25/64 loss: -0.17985498905181885
Batch 26/64 loss: -0.19174611568450928
Batch 27/64 loss: -0.20299389958381653
Batch 28/64 loss: -0.20245599746704102
Batch 29/64 loss: -0.2057909369468689
Batch 30/64 loss: -0.2037305235862732
Batch 31/64 loss: -0.20078879594802856
Batch 32/64 loss: -0.16455018520355225
Batch 33/64 loss: -0.2061537206172943
Batch 34/64 loss: -0.2150309681892395
Batch 35/64 loss: -0.1775498390197754
Batch 36/64 loss: -0.20587000250816345
Batch 37/64 loss: -0.19903618097305298
Batch 38/64 loss: -0.1787227988243103
Batch 39/64 loss: -0.17109769582748413
Batch 40/64 loss: -0.20544281601905823
Batch 41/64 loss: -0.20173406600952148
Batch 42/64 loss: -0.20768272876739502
Batch 43/64 loss: -0.18783169984817505
Batch 44/64 loss: -0.16163402795791626
Batch 45/64 loss: -0.18428033590316772
Batch 46/64 loss: -0.18416917324066162
Batch 47/64 loss: -0.20118078589439392
Batch 48/64 loss: -0.1927560567855835
Batch 49/64 loss: -0.19910937547683716
Batch 50/64 loss: -0.21173900365829468
Batch 51/64 loss: -0.1908300518989563
Batch 52/64 loss: -0.17510926723480225
Batch 53/64 loss: -0.19117248058319092
Batch 54/64 loss: -0.1827462911605835
Batch 55/64 loss: -0.20936280488967896
Batch 56/64 loss: -0.15832644701004028
Batch 57/64 loss: -0.20527297258377075
Batch 58/64 loss: -0.21665722131729126
Batch 59/64 loss: -0.17744392156600952
Batch 60/64 loss: -0.2077416181564331
Batch 61/64 loss: -0.200056254863739
Batch 62/64 loss: -0.18885588645935059
Batch 63/64 loss: -0.1910434365272522
Batch 64/64 loss: -0.18405479192733765
Epoch 427  Train loss: -0.1942809859911601  Val loss: -0.014678600113006802
Epoch 428
-------------------------------
Batch 1/64 loss: -0.2040870487689972
Batch 2/64 loss: -0.17674130201339722
Batch 3/64 loss: -0.19232827425003052
Batch 4/64 loss: -0.21170181035995483
Batch 5/64 loss: -0.21890956163406372
Batch 6/64 loss: -0.20414870977401733
Batch 7/64 loss: -0.18153071403503418
Batch 8/64 loss: -0.17266243696212769
Batch 9/64 loss: -0.17892694473266602
Batch 10/64 loss: -0.19807016849517822
Batch 11/64 loss: -0.2132161259651184
Batch 12/64 loss: -0.22133982181549072
Batch 13/64 loss: -0.20617777109146118
Batch 14/64 loss: -0.1945936679840088
Batch 15/64 loss: -0.2050175666809082
Batch 16/64 loss: -0.2128940224647522
Batch 17/64 loss: -0.20202219486236572
Batch 18/64 loss: -0.21737799048423767
Batch 19/64 loss: -0.20122265815734863
Batch 20/64 loss: -0.18680882453918457
Batch 21/64 loss: -0.19939768314361572
Batch 22/64 loss: -0.16536211967468262
Batch 23/64 loss: -0.17166554927825928
Batch 24/64 loss: -0.20411521196365356
Batch 25/64 loss: -0.19237685203552246
Batch 26/64 loss: -0.2004135251045227
Batch 27/64 loss: -0.19953352212905884
Batch 28/64 loss: -0.19960346817970276
Batch 29/64 loss: -0.16504192352294922
Batch 30/64 loss: -0.19785195589065552
Batch 31/64 loss: -0.19640856981277466
Batch 32/64 loss: -0.18212157487869263
Batch 33/64 loss: -0.20255690813064575
Batch 34/64 loss: -0.19018661975860596
Batch 35/64 loss: -0.18749618530273438
Batch 36/64 loss: -0.19747287034988403
Batch 37/64 loss: -0.17462241649627686
Batch 38/64 loss: -0.1996385157108307
Batch 39/64 loss: -0.18747341632843018
Batch 40/64 loss: -0.20889794826507568
Batch 41/64 loss: -0.20281043648719788
Batch 42/64 loss: -0.19296932220458984
Batch 43/64 loss: -0.17426562309265137
Batch 44/64 loss: -0.17944908142089844
Batch 45/64 loss: -0.21398499608039856
Batch 46/64 loss: -0.19950398802757263
Batch 47/64 loss: -0.18075191974639893
Batch 48/64 loss: -0.19633418321609497
Batch 49/64 loss: -0.19878512620925903
Batch 50/64 loss: -0.18840986490249634
Batch 51/64 loss: -0.21218270063400269
Batch 52/64 loss: -0.1865217685699463
Batch 53/64 loss: -0.1978946030139923
Batch 54/64 loss: -0.19353479146957397
Batch 55/64 loss: -0.18781113624572754
Batch 56/64 loss: -0.19074177742004395
Batch 57/64 loss: -0.17921149730682373
Batch 58/64 loss: -0.1863996386528015
Batch 59/64 loss: -0.1835118532180786
Batch 60/64 loss: -0.211414635181427
Batch 61/64 loss: -0.19527918100357056
Batch 62/64 loss: -0.210504412651062
Batch 63/64 loss: -0.17511844635009766
Batch 64/64 loss: -0.17006558179855347
Epoch 428  Train loss: -0.19430517051734175  Val loss: -0.013868464841875424
Epoch 429
-------------------------------
Batch 1/64 loss: -0.22117769718170166
Batch 2/64 loss: -0.18408888578414917
Batch 3/64 loss: -0.18355250358581543
Batch 4/64 loss: -0.19852986931800842
Batch 5/64 loss: -0.19542133808135986
Batch 6/64 loss: -0.18547731637954712
Batch 7/64 loss: -0.17840182781219482
Batch 8/64 loss: -0.16097784042358398
Batch 9/64 loss: -0.18029987812042236
Batch 10/64 loss: -0.17816102504730225
Batch 11/64 loss: -0.2089361548423767
Batch 12/64 loss: -0.20208388566970825
Batch 13/64 loss: -0.17136859893798828
Batch 14/64 loss: -0.18248587846755981
Batch 15/64 loss: -0.19045275449752808
Batch 16/64 loss: -0.19832301139831543
Batch 17/64 loss: -0.1686941385269165
Batch 18/64 loss: -0.20910823345184326
Batch 19/64 loss: -0.20946189761161804
Batch 20/64 loss: -0.16798192262649536
Batch 21/64 loss: -0.198286235332489
Batch 22/64 loss: -0.17984575033187866
Batch 23/64 loss: -0.20771363377571106
Batch 24/64 loss: -0.1766340732574463
Batch 25/64 loss: -0.2129749357700348
Batch 26/64 loss: -0.19971409440040588
Batch 27/64 loss: -0.17605775594711304
Batch 28/64 loss: -0.1853257417678833
Batch 29/64 loss: -0.18481671810150146
Batch 30/64 loss: -0.1871904730796814
Batch 31/64 loss: -0.16427463293075562
Batch 32/64 loss: -0.1916278600692749
Batch 33/64 loss: -0.20873787999153137
Batch 34/64 loss: -0.1728343963623047
Batch 35/64 loss: -0.20785075426101685
Batch 36/64 loss: -0.21516427397727966
Batch 37/64 loss: -0.18531858921051025
Batch 38/64 loss: -0.1973857283592224
Batch 39/64 loss: -0.1948338747024536
Batch 40/64 loss: -0.19870924949645996
Batch 41/64 loss: -0.1715930700302124
Batch 42/64 loss: -0.20504331588745117
Batch 43/64 loss: -0.1782282590866089
Batch 44/64 loss: -0.1884247064590454
Batch 45/64 loss: -0.18627458810806274
Batch 46/64 loss: -0.20108330249786377
Batch 47/64 loss: -0.19954389333724976
Batch 48/64 loss: -0.18717652559280396
Batch 49/64 loss: -0.1983402669429779
Batch 50/64 loss: -0.2162376344203949
Batch 51/64 loss: -0.19091922044754028
Batch 52/64 loss: -0.17924439907073975
Batch 53/64 loss: -0.18681305646896362
Batch 54/64 loss: -0.2032800316810608
Batch 55/64 loss: -0.18245351314544678
Batch 56/64 loss: -0.18974661827087402
Batch 57/64 loss: -0.1877099871635437
Batch 58/64 loss: -0.16378140449523926
Batch 59/64 loss: -0.19514477252960205
Batch 60/64 loss: -0.19335788488388062
Batch 61/64 loss: -0.18593627214431763
Batch 62/64 loss: -0.1557125449180603
Batch 63/64 loss: -0.1884160041809082
Batch 64/64 loss: -0.19366943836212158
Epoch 429  Train loss: -0.18980383779488358  Val loss: -0.01438310936963845
Epoch 430
-------------------------------
Batch 1/64 loss: -0.20017993450164795
Batch 2/64 loss: -0.16944777965545654
Batch 3/64 loss: -0.20777934789657593
Batch 4/64 loss: -0.19973912835121155
Batch 5/64 loss: -0.1948755979537964
Batch 6/64 loss: -0.19767391681671143
Batch 7/64 loss: -0.19216734170913696
Batch 8/64 loss: -0.1768450140953064
Batch 9/64 loss: -0.2191881537437439
Batch 10/64 loss: -0.20246702432632446
Batch 11/64 loss: -0.2017166018486023
Batch 12/64 loss: -0.21099165081977844
Batch 13/64 loss: -0.19542396068572998
Batch 14/64 loss: -0.19287705421447754
Batch 15/64 loss: -0.2142506241798401
Batch 16/64 loss: -0.206163227558136
Batch 17/64 loss: -0.193173348903656
Batch 18/64 loss: -0.15460944175720215
Batch 19/64 loss: -0.152329683303833
Batch 20/64 loss: -0.21206974983215332
Batch 21/64 loss: -0.20785951614379883
Batch 22/64 loss: -0.19115251302719116
Batch 23/64 loss: -0.19786852598190308
Batch 24/64 loss: -0.18871724605560303
Batch 25/64 loss: -0.20088142156600952
Batch 26/64 loss: -0.20147764682769775
Batch 27/64 loss: -0.18711555004119873
Batch 28/64 loss: -0.17714619636535645
Batch 29/64 loss: -0.19408851861953735
Batch 30/64 loss: -0.18138784170150757
Batch 31/64 loss: -0.19133466482162476
Batch 32/64 loss: -0.19847530126571655
Batch 33/64 loss: -0.19621014595031738
Batch 34/64 loss: -0.18857163190841675
Batch 35/64 loss: -0.184673011302948
Batch 36/64 loss: -0.20081019401550293
Batch 37/64 loss: -0.18447339534759521
Batch 38/64 loss: -0.1761641502380371
Batch 39/64 loss: -0.18554174900054932
Batch 40/64 loss: -0.21380242705345154
Batch 41/64 loss: -0.19153690338134766
Batch 42/64 loss: -0.19923418760299683
Batch 43/64 loss: -0.1813107132911682
Batch 44/64 loss: -0.1953132152557373
Batch 45/64 loss: -0.1833438277244568
Batch 46/64 loss: -0.21366536617279053
Batch 47/64 loss: -0.1864413022994995
Batch 48/64 loss: -0.1674894094467163
Batch 49/64 loss: -0.19076186418533325
Batch 50/64 loss: -0.19479161500930786
Batch 51/64 loss: -0.18876320123672485
Batch 52/64 loss: -0.19061434268951416
Batch 53/64 loss: -0.19789111614227295
Batch 54/64 loss: -0.19190508127212524
Batch 55/64 loss: -0.19774514436721802
Batch 56/64 loss: -0.20719420909881592
Batch 57/64 loss: -0.1793106198310852
Batch 58/64 loss: -0.17052167654037476
Batch 59/64 loss: -0.20374661684036255
Batch 60/64 loss: -0.19753268361091614
Batch 61/64 loss: -0.20977523922920227
Batch 62/64 loss: -0.19672957062721252
Batch 63/64 loss: -0.19083577394485474
Batch 64/64 loss: -0.19357627630233765
Epoch 430  Train loss: -0.19315068417904424  Val loss: -0.016009572854976065
Epoch 431
-------------------------------
Batch 1/64 loss: -0.1965327262878418
Batch 2/64 loss: -0.20754942297935486
Batch 3/64 loss: -0.1955980658531189
Batch 4/64 loss: -0.1839401125907898
Batch 5/64 loss: -0.21214905381202698
Batch 6/64 loss: -0.22384950518608093
Batch 7/64 loss: -0.21076947450637817
Batch 8/64 loss: -0.21038088202476501
Batch 9/64 loss: -0.18335187435150146
Batch 10/64 loss: -0.2154233157634735
Batch 11/64 loss: -0.20084017515182495
Batch 12/64 loss: -0.20116958022117615
Batch 13/64 loss: -0.2046365737915039
Batch 14/64 loss: -0.17868226766586304
Batch 15/64 loss: -0.19314873218536377
Batch 16/64 loss: -0.21583858132362366
Batch 17/64 loss: -0.20284724235534668
Batch 18/64 loss: -0.17439472675323486
Batch 19/64 loss: -0.1954631209373474
Batch 20/64 loss: -0.19475018978118896
Batch 21/64 loss: -0.18528175354003906
Batch 22/64 loss: -0.21749311685562134
Batch 23/64 loss: -0.20114344358444214
Batch 24/64 loss: -0.19605660438537598
Batch 25/64 loss: -0.21000349521636963
Batch 26/64 loss: -0.19754528999328613
Batch 27/64 loss: -0.1904822587966919
Batch 28/64 loss: -0.19891920685768127
Batch 29/64 loss: -0.210241436958313
Batch 30/64 loss: -0.16006451845169067
Batch 31/64 loss: -0.19344842433929443
Batch 32/64 loss: -0.17619210481643677
Batch 33/64 loss: -0.17668157815933228
Batch 34/64 loss: -0.17007935047149658
Batch 35/64 loss: -0.1883319616317749
Batch 36/64 loss: -0.19892290234565735
Batch 37/64 loss: -0.20591184496879578
Batch 38/64 loss: -0.20308399200439453
Batch 39/64 loss: -0.17611193656921387
Batch 40/64 loss: -0.19394749402999878
Batch 41/64 loss: -0.1572553515434265
Batch 42/64 loss: -0.21091300249099731
Batch 43/64 loss: -0.19666874408721924
Batch 44/64 loss: -0.193997323513031
Batch 45/64 loss: -0.19748395681381226
Batch 46/64 loss: -0.20034244656562805
Batch 47/64 loss: -0.20694535970687866
Batch 48/64 loss: -0.196441650390625
Batch 49/64 loss: -0.20765811204910278
Batch 50/64 loss: -0.18055272102355957
Batch 51/64 loss: -0.19218021631240845
Batch 52/64 loss: -0.17963123321533203
Batch 53/64 loss: -0.16767847537994385
Batch 54/64 loss: -0.19519197940826416
Batch 55/64 loss: -0.20013213157653809
Batch 56/64 loss: -0.1899317502975464
Batch 57/64 loss: -0.1916688084602356
Batch 58/64 loss: -0.18768328428268433
Batch 59/64 loss: -0.20672473311424255
Batch 60/64 loss: -0.16829001903533936
Batch 61/64 loss: -0.17552584409713745
Batch 62/64 loss: -0.19291049242019653
Batch 63/64 loss: -0.1953810453414917
Batch 64/64 loss: -0.19138067960739136
Epoch 431  Train loss: -0.19428992201300227  Val loss: -0.012568234373204078
Epoch 432
-------------------------------
Batch 1/64 loss: -0.1930234432220459
Batch 2/64 loss: -0.20608294010162354
Batch 3/64 loss: -0.2035852074623108
Batch 4/64 loss: -0.19161713123321533
Batch 5/64 loss: -0.1990392804145813
Batch 6/64 loss: -0.19547587633132935
Batch 7/64 loss: -0.2016535997390747
Batch 8/64 loss: -0.16201305389404297
Batch 9/64 loss: -0.17427700757980347
Batch 10/64 loss: -0.19144654273986816
Batch 11/64 loss: -0.19517987966537476
Batch 12/64 loss: -0.1986008882522583
Batch 13/64 loss: -0.19195789098739624
Batch 14/64 loss: -0.17837828397750854
Batch 15/64 loss: -0.19540470838546753
Batch 16/64 loss: -0.20895618200302124
Batch 17/64 loss: -0.18239736557006836
Batch 18/64 loss: -0.2041894793510437
Batch 19/64 loss: -0.17918986082077026
Batch 20/64 loss: -0.1659138798713684
Batch 21/64 loss: -0.18540078401565552
Batch 22/64 loss: -0.21467065811157227
Batch 23/64 loss: -0.18570315837860107
Batch 24/64 loss: -0.20931577682495117
Batch 25/64 loss: -0.1413722038269043
Batch 26/64 loss: -0.1898096203804016
Batch 27/64 loss: -0.21233734488487244
Batch 28/64 loss: -0.17300963401794434
Batch 29/64 loss: -0.21305084228515625
Batch 30/64 loss: -0.16913193464279175
Batch 31/64 loss: -0.16601437330245972
Batch 32/64 loss: -0.17869436740875244
Batch 33/64 loss: -0.19809013605117798
Batch 34/64 loss: -0.18654584884643555
Batch 35/64 loss: -0.20228493213653564
Batch 36/64 loss: -0.21246466040611267
Batch 37/64 loss: -0.21831881999969482
Batch 38/64 loss: -0.1919248104095459
Batch 39/64 loss: -0.21712970733642578
Batch 40/64 loss: -0.18316709995269775
Batch 41/64 loss: -0.18097150325775146
Batch 42/64 loss: -0.18935346603393555
Batch 43/64 loss: -0.18263965845108032
Batch 44/64 loss: -0.18943023681640625
Batch 45/64 loss: -0.20964598655700684
Batch 46/64 loss: -0.21490898728370667
Batch 47/64 loss: -0.19505971670150757
Batch 48/64 loss: -0.21625161170959473
Batch 49/64 loss: -0.20801615715026855
Batch 50/64 loss: -0.18162989616394043
Batch 51/64 loss: -0.1964535117149353
Batch 52/64 loss: -0.20042204856872559
Batch 53/64 loss: -0.19747614860534668
Batch 54/64 loss: -0.19554686546325684
Batch 55/64 loss: -0.20272308588027954
Batch 56/64 loss: -0.1605091094970703
Batch 57/64 loss: -0.20926201343536377
Batch 58/64 loss: -0.20593297481536865
Batch 59/64 loss: -0.20351392030715942
Batch 60/64 loss: -0.16337555646896362
Batch 61/64 loss: -0.18236327171325684
Batch 62/64 loss: -0.20583075284957886
Batch 63/64 loss: -0.18302875757217407
Batch 64/64 loss: -0.20751190185546875
Epoch 432  Train loss: -0.192890954952614  Val loss: -0.015117078712306073
Epoch 433
-------------------------------
Batch 1/64 loss: -0.21309641003608704
Batch 2/64 loss: -0.19064640998840332
Batch 3/64 loss: -0.20694246888160706
Batch 4/64 loss: -0.19154256582260132
Batch 5/64 loss: -0.2025652527809143
Batch 6/64 loss: -0.21237367391586304
Batch 7/64 loss: -0.1976293921470642
Batch 8/64 loss: -0.22078385949134827
Batch 9/64 loss: -0.18793171644210815
Batch 10/64 loss: -0.20491114258766174
Batch 11/64 loss: -0.2169346809387207
Batch 12/64 loss: -0.20151108503341675
Batch 13/64 loss: -0.21974694728851318
Batch 14/64 loss: -0.18973827362060547
Batch 15/64 loss: -0.21220427751541138
Batch 16/64 loss: -0.18430012464523315
Batch 17/64 loss: -0.21179038286209106
Batch 18/64 loss: -0.20270070433616638
Batch 19/64 loss: -0.20858177542686462
Batch 20/64 loss: -0.19542241096496582
Batch 21/64 loss: -0.20066499710083008
Batch 22/64 loss: -0.19669699668884277
Batch 23/64 loss: -0.2044575810432434
Batch 24/64 loss: -0.16253888607025146
Batch 25/64 loss: -0.2035934031009674
Batch 26/64 loss: -0.20825234055519104
Batch 27/64 loss: -0.19563812017440796
Batch 28/64 loss: -0.19487065076828003
Batch 29/64 loss: -0.1917683482170105
Batch 30/64 loss: -0.2003488540649414
Batch 31/64 loss: -0.17969107627868652
Batch 32/64 loss: -0.1698768138885498
Batch 33/64 loss: -0.18745994567871094
Batch 34/64 loss: -0.16936808824539185
Batch 35/64 loss: -0.20210742950439453
Batch 36/64 loss: -0.20866987109184265
Batch 37/64 loss: -0.18715107440948486
Batch 38/64 loss: -0.19084298610687256
Batch 39/64 loss: -0.19164448976516724
Batch 40/64 loss: -0.19446885585784912
Batch 41/64 loss: -0.18026399612426758
Batch 42/64 loss: -0.18809759616851807
Batch 43/64 loss: -0.19892442226409912
Batch 44/64 loss: -0.1949440836906433
Batch 45/64 loss: -0.21629613637924194
Batch 46/64 loss: -0.18977755308151245
Batch 47/64 loss: -0.17697590589523315
Batch 48/64 loss: -0.16587203741073608
Batch 49/64 loss: -0.17728322744369507
Batch 50/64 loss: -0.1641920804977417
Batch 51/64 loss: -0.1824457049369812
Batch 52/64 loss: -0.17427682876586914
Batch 53/64 loss: -0.20829734206199646
Batch 54/64 loss: -0.1902562975883484
Batch 55/64 loss: -0.18006378412246704
Batch 56/64 loss: -0.18328136205673218
Batch 57/64 loss: -0.1665191650390625
Batch 58/64 loss: -0.17835283279418945
Batch 59/64 loss: -0.21701592206954956
Batch 60/64 loss: -0.20475542545318604
Batch 61/64 loss: -0.19225549697875977
Batch 62/64 loss: -0.20429405570030212
Batch 63/64 loss: -0.18756580352783203
Batch 64/64 loss: -0.20800936222076416
Epoch 433  Train loss: -0.19434475992240158  Val loss: -0.015690882795864773
Epoch 434
-------------------------------
Batch 1/64 loss: -0.21511900424957275
Batch 2/64 loss: -0.207750141620636
Batch 3/64 loss: -0.1799839735031128
Batch 4/64 loss: -0.20239686965942383
Batch 5/64 loss: -0.2038981020450592
Batch 6/64 loss: -0.18531417846679688
Batch 7/64 loss: -0.21147489547729492
Batch 8/64 loss: -0.1800365447998047
Batch 9/64 loss: -0.20010709762573242
Batch 10/64 loss: -0.20470589399337769
Batch 11/64 loss: -0.19648516178131104
Batch 12/64 loss: -0.18944716453552246
Batch 13/64 loss: -0.1784180998802185
Batch 14/64 loss: -0.16841882467269897
Batch 15/64 loss: -0.16681420803070068
Batch 16/64 loss: -0.20256465673446655
Batch 17/64 loss: -0.20798757672309875
Batch 18/64 loss: -0.187638521194458
Batch 19/64 loss: -0.19531452655792236
Batch 20/64 loss: -0.1924816370010376
Batch 21/64 loss: -0.2100316882133484
Batch 22/64 loss: -0.21515381336212158
Batch 23/64 loss: -0.2056027054786682
Batch 24/64 loss: -0.19954729080200195
Batch 25/64 loss: -0.20256856083869934
Batch 26/64 loss: -0.21479389071464539
Batch 27/64 loss: -0.1940518021583557
Batch 28/64 loss: -0.1785128116607666
Batch 29/64 loss: -0.20361733436584473
Batch 30/64 loss: -0.20362326502799988
Batch 31/64 loss: -0.18689113855361938
Batch 32/64 loss: -0.2054573893547058
Batch 33/64 loss: -0.202854722738266
Batch 34/64 loss: -0.20367708802223206
Batch 35/64 loss: -0.18132621049880981
Batch 36/64 loss: -0.20464485883712769
Batch 37/64 loss: -0.16811931133270264
Batch 38/64 loss: -0.20799699425697327
Batch 39/64 loss: -0.1836707592010498
Batch 40/64 loss: -0.20058614015579224
Batch 41/64 loss: -0.18615776300430298
Batch 42/64 loss: -0.2075076699256897
Batch 43/64 loss: -0.18388229608535767
Batch 44/64 loss: -0.19012176990509033
Batch 45/64 loss: -0.20878487825393677
Batch 46/64 loss: -0.20197105407714844
Batch 47/64 loss: -0.19232451915740967
Batch 48/64 loss: -0.2165103554725647
Batch 49/64 loss: -0.20318222045898438
Batch 50/64 loss: -0.18412131071090698
Batch 51/64 loss: -0.18451261520385742
Batch 52/64 loss: -0.1404690146446228
Batch 53/64 loss: -0.158516526222229
Batch 54/64 loss: -0.19370770454406738
Batch 55/64 loss: -0.17662972211837769
Batch 56/64 loss: -0.18881750106811523
Batch 57/64 loss: -0.18766969442367554
Batch 58/64 loss: -0.20384523272514343
Batch 59/64 loss: -0.19559979438781738
Batch 60/64 loss: -0.18166857957839966
Batch 61/64 loss: -0.21256709098815918
Batch 62/64 loss: -0.1680917739868164
Batch 63/64 loss: -0.19181132316589355
Batch 64/64 loss: -0.17745441198349
Epoch 434  Train loss: -0.19357876193289664  Val loss: -0.013696827429676383
Epoch 435
-------------------------------
Batch 1/64 loss: -0.1933443546295166
Batch 2/64 loss: -0.201330304145813
Batch 3/64 loss: -0.196461021900177
Batch 4/64 loss: -0.18838709592819214
Batch 5/64 loss: -0.17928481101989746
Batch 6/64 loss: -0.189400315284729
Batch 7/64 loss: -0.14872890710830688
Batch 8/64 loss: -0.1445487141609192
Batch 9/64 loss: -0.18221676349639893
Batch 10/64 loss: -0.1837717890739441
Batch 11/64 loss: -0.166750967502594
Batch 12/64 loss: -0.18153154850006104
Batch 13/64 loss: -0.19653666019439697
Batch 14/64 loss: -0.19919458031654358
Batch 15/64 loss: -0.20820730924606323
Batch 16/64 loss: -0.20301896333694458
Batch 17/64 loss: -0.2022993564605713
Batch 18/64 loss: -0.19861894845962524
Batch 19/64 loss: -0.20476585626602173
Batch 20/64 loss: -0.20222997665405273
Batch 21/64 loss: -0.1899552345275879
Batch 22/64 loss: -0.19523000717163086
Batch 23/64 loss: -0.19058012962341309
Batch 24/64 loss: -0.2096678614616394
Batch 25/64 loss: -0.18703913688659668
Batch 26/64 loss: -0.19783368706703186
Batch 27/64 loss: -0.1925984025001526
Batch 28/64 loss: -0.20580816268920898
Batch 29/64 loss: -0.2023807168006897
Batch 30/64 loss: -0.17412900924682617
Batch 31/64 loss: -0.18229913711547852
Batch 32/64 loss: -0.18896913528442383
Batch 33/64 loss: -0.21090739965438843
Batch 34/64 loss: -0.1935666799545288
Batch 35/64 loss: -0.1764315962791443
Batch 36/64 loss: -0.19488370418548584
Batch 37/64 loss: -0.19314706325531006
Batch 38/64 loss: -0.1902187466621399
Batch 39/64 loss: -0.2133215367794037
Batch 40/64 loss: -0.19070541858673096
Batch 41/64 loss: -0.2027587592601776
Batch 42/64 loss: -0.20851105451583862
Batch 43/64 loss: -0.21030157804489136
Batch 44/64 loss: -0.2111639380455017
Batch 45/64 loss: -0.21135646104812622
Batch 46/64 loss: -0.16576659679412842
Batch 47/64 loss: -0.21517395973205566
Batch 48/64 loss: -0.19943451881408691
Batch 49/64 loss: -0.20677873492240906
Batch 50/64 loss: -0.20364689826965332
Batch 51/64 loss: -0.17769789695739746
Batch 52/64 loss: -0.2224687933921814
Batch 53/64 loss: -0.20107972621917725
Batch 54/64 loss: -0.2073957324028015
Batch 55/64 loss: -0.21111196279525757
Batch 56/64 loss: -0.20978474617004395
Batch 57/64 loss: -0.17687803506851196
Batch 58/64 loss: -0.21664243936538696
Batch 59/64 loss: -0.21346646547317505
Batch 60/64 loss: -0.20336323976516724
Batch 61/64 loss: -0.2022710144519806
Batch 62/64 loss: -0.1940208077430725
Batch 63/64 loss: -0.22324562072753906
Batch 64/64 loss: -0.19170737266540527
Epoch 435  Train loss: -0.1958964787277521  Val loss: -0.014481501890621645
Epoch 436
-------------------------------
Batch 1/64 loss: -0.2042090892791748
Batch 2/64 loss: -0.2106688916683197
Batch 3/64 loss: -0.23411467671394348
Batch 4/64 loss: -0.20028704404830933
Batch 5/64 loss: -0.20036596059799194
Batch 6/64 loss: -0.20559588074684143
Batch 7/64 loss: -0.2093418836593628
Batch 8/64 loss: -0.21398741006851196
Batch 9/64 loss: -0.20953476428985596
Batch 10/64 loss: -0.19659876823425293
Batch 11/64 loss: -0.21504941582679749
Batch 12/64 loss: -0.21726709604263306
Batch 13/64 loss: -0.20168480277061462
Batch 14/64 loss: -0.2025095522403717
Batch 15/64 loss: -0.17924749851226807
Batch 16/64 loss: -0.19168972969055176
Batch 17/64 loss: -0.1993332803249359
Batch 18/64 loss: -0.20968502759933472
Batch 19/64 loss: -0.22933951020240784
Batch 20/64 loss: -0.20867854356765747
Batch 21/64 loss: -0.21501046419143677
Batch 22/64 loss: -0.1832289695739746
Batch 23/64 loss: -0.19346004724502563
Batch 24/64 loss: -0.20957285165786743
Batch 25/64 loss: -0.18567579984664917
Batch 26/64 loss: -0.20273876190185547
Batch 27/64 loss: -0.212438702583313
Batch 28/64 loss: -0.1991899609565735
Batch 29/64 loss: -0.1880263090133667
Batch 30/64 loss: -0.1791297197341919
Batch 31/64 loss: -0.20133227109909058
Batch 32/64 loss: -0.21306681632995605
Batch 33/64 loss: -0.1880931854248047
Batch 34/64 loss: -0.1811467409133911
Batch 35/64 loss: -0.15567898750305176
Batch 36/64 loss: -0.21017199754714966
Batch 37/64 loss: -0.18451809883117676
Batch 38/64 loss: -0.17112648487091064
Batch 39/64 loss: -0.16449713706970215
Batch 40/64 loss: -0.21025991439819336
Batch 41/64 loss: -0.19452226161956787
Batch 42/64 loss: -0.2039758563041687
Batch 43/64 loss: -0.20816078782081604
Batch 44/64 loss: -0.2045404613018036
Batch 45/64 loss: -0.1909407377243042
Batch 46/64 loss: -0.20658618211746216
Batch 47/64 loss: -0.20275884866714478
Batch 48/64 loss: -0.1672278642654419
Batch 49/64 loss: -0.18042927980422974
Batch 50/64 loss: -0.2016940414905548
Batch 51/64 loss: -0.18257558345794678
Batch 52/64 loss: -0.2154138684272766
Batch 53/64 loss: -0.17950278520584106
Batch 54/64 loss: -0.22108769416809082
Batch 55/64 loss: -0.17358696460723877
Batch 56/64 loss: -0.18641209602355957
Batch 57/64 loss: -0.20025649666786194
Batch 58/64 loss: -0.22070950269699097
Batch 59/64 loss: -0.16923803091049194
Batch 60/64 loss: -0.1997358202934265
Batch 61/64 loss: -0.1973954439163208
Batch 62/64 loss: -0.20850208401679993
Batch 63/64 loss: -0.175872802734375
Batch 64/64 loss: -0.20338955521583557
Epoch 436  Train loss: -0.1981367797243829  Val loss: -0.01625820738343439
Epoch 437
-------------------------------
Batch 1/64 loss: -0.20262455940246582
Batch 2/64 loss: -0.20753955841064453
Batch 3/64 loss: -0.19435864686965942
Batch 4/64 loss: -0.20387622714042664
Batch 5/64 loss: -0.2061612606048584
Batch 6/64 loss: -0.2218104898929596
Batch 7/64 loss: -0.19319593906402588
Batch 8/64 loss: -0.19259214401245117
Batch 9/64 loss: -0.20621445775032043
Batch 10/64 loss: -0.1823529601097107
Batch 11/64 loss: -0.21061301231384277
Batch 12/64 loss: -0.18959379196166992
Batch 13/64 loss: -0.1861167550086975
Batch 14/64 loss: -0.20951035618782043
Batch 15/64 loss: -0.21064436435699463
Batch 16/64 loss: -0.20328187942504883
Batch 17/64 loss: -0.19343167543411255
Batch 18/64 loss: -0.2041645050048828
Batch 19/64 loss: -0.21210339665412903
Batch 20/64 loss: -0.21413838863372803
Batch 21/64 loss: -0.1952168345451355
Batch 22/64 loss: -0.215791255235672
Batch 23/64 loss: -0.20540723204612732
Batch 24/64 loss: -0.2039583921432495
Batch 25/64 loss: -0.18930387496948242
Batch 26/64 loss: -0.22611582279205322
Batch 27/64 loss: -0.17532163858413696
Batch 28/64 loss: -0.19872447848320007
Batch 29/64 loss: -0.19935864210128784
Batch 30/64 loss: -0.17508792877197266
Batch 31/64 loss: -0.2140553593635559
Batch 32/64 loss: -0.21165555715560913
Batch 33/64 loss: -0.14234977960586548
Batch 34/64 loss: -0.19998493790626526
Batch 35/64 loss: -0.20308572053909302
Batch 36/64 loss: -0.17147040367126465
Batch 37/64 loss: -0.19283288717269897
Batch 38/64 loss: -0.20295336842536926
Batch 39/64 loss: -0.20119333267211914
Batch 40/64 loss: -0.20646154880523682
Batch 41/64 loss: -0.21090412139892578
Batch 42/64 loss: -0.2154998779296875
Batch 43/64 loss: -0.20912855863571167
Batch 44/64 loss: -0.18338847160339355
Batch 45/64 loss: -0.18917983770370483
Batch 46/64 loss: -0.18520283699035645
Batch 47/64 loss: -0.17295891046524048
Batch 48/64 loss: -0.20532113313674927
Batch 49/64 loss: -0.19810834527015686
Batch 50/64 loss: -0.19879889488220215
Batch 51/64 loss: -0.19187748432159424
Batch 52/64 loss: -0.21309661865234375
Batch 53/64 loss: -0.203515887260437
Batch 54/64 loss: -0.22335532307624817
Batch 55/64 loss: -0.19200533628463745
Batch 56/64 loss: -0.19301724433898926
Batch 57/64 loss: -0.21224895119667053
Batch 58/64 loss: -0.19205540418624878
Batch 59/64 loss: -0.19111645221710205
Batch 60/64 loss: -0.19655930995941162
Batch 61/64 loss: -0.18797719478607178
Batch 62/64 loss: -0.19745349884033203
Batch 63/64 loss: -0.159942626953125
Batch 64/64 loss: -0.19696033000946045
Epoch 437  Train loss: -0.19835428911096908  Val loss: -0.015059390838203561
Epoch 438
-------------------------------
Batch 1/64 loss: -0.1948791742324829
Batch 2/64 loss: -0.18503838777542114
Batch 3/64 loss: -0.1835024356842041
Batch 4/64 loss: -0.19188904762268066
Batch 5/64 loss: -0.20344853401184082
Batch 6/64 loss: -0.2159002423286438
Batch 7/64 loss: -0.1966724991798401
Batch 8/64 loss: -0.18007802963256836
Batch 9/64 loss: -0.21428140997886658
Batch 10/64 loss: -0.20643150806427002
Batch 11/64 loss: -0.21321851015090942
Batch 12/64 loss: -0.20031273365020752
Batch 13/64 loss: -0.20051071047782898
Batch 14/64 loss: -0.2074769139289856
Batch 15/64 loss: -0.1745973825454712
Batch 16/64 loss: -0.19117355346679688
Batch 17/64 loss: -0.2018669843673706
Batch 18/64 loss: -0.21035665273666382
Batch 19/64 loss: -0.18165332078933716
Batch 20/64 loss: -0.20193266868591309
Batch 21/64 loss: -0.21215316653251648
Batch 22/64 loss: -0.2343989610671997
Batch 23/64 loss: -0.19169378280639648
Batch 24/64 loss: -0.20475634932518005
Batch 25/64 loss: -0.199203759431839
Batch 26/64 loss: -0.18805456161499023
Batch 27/64 loss: -0.16654443740844727
Batch 28/64 loss: -0.2113562822341919
Batch 29/64 loss: -0.22017616033554077
Batch 30/64 loss: -0.20481210947036743
Batch 31/64 loss: -0.1871337890625
Batch 32/64 loss: -0.22595438361167908
Batch 33/64 loss: -0.19849824905395508
Batch 34/64 loss: -0.19956955313682556
Batch 35/64 loss: -0.1831352710723877
Batch 36/64 loss: -0.1870807409286499
Batch 37/64 loss: -0.19198811054229736
Batch 38/64 loss: -0.20693248510360718
Batch 39/64 loss: -0.1922183632850647
Batch 40/64 loss: -0.21037930250167847
Batch 41/64 loss: -0.17525947093963623
Batch 42/64 loss: -0.20663803815841675
Batch 43/64 loss: -0.20514941215515137
Batch 44/64 loss: -0.20324569940567017
Batch 45/64 loss: -0.16883033514022827
Batch 46/64 loss: -0.19099187850952148
Batch 47/64 loss: -0.18409252166748047
Batch 48/64 loss: -0.1998215615749359
Batch 49/64 loss: -0.2079559564590454
Batch 50/64 loss: -0.18869435787200928
Batch 51/64 loss: -0.18389785289764404
Batch 52/64 loss: -0.19211578369140625
Batch 53/64 loss: -0.20055776834487915
Batch 54/64 loss: -0.17964285612106323
Batch 55/64 loss: -0.2133738398551941
Batch 56/64 loss: -0.19935527443885803
Batch 57/64 loss: -0.19411927461624146
Batch 58/64 loss: -0.20233720541000366
Batch 59/64 loss: -0.233681321144104
Batch 60/64 loss: -0.22936314344406128
Batch 61/64 loss: -0.202281653881073
Batch 62/64 loss: -0.20240360498428345
Batch 63/64 loss: -0.18145215511322021
Batch 64/64 loss: -0.21931368112564087
Epoch 438  Train loss: -0.19891775285496432  Val loss: -0.012812552583176656
Epoch 439
-------------------------------
Batch 1/64 loss: -0.19458961486816406
Batch 2/64 loss: -0.21144556999206543
Batch 3/64 loss: -0.17744076251983643
Batch 4/64 loss: -0.1888653039932251
Batch 5/64 loss: -0.19835227727890015
Batch 6/64 loss: -0.17064929008483887
Batch 7/64 loss: -0.18970108032226562
Batch 8/64 loss: -0.20703238248825073
Batch 9/64 loss: -0.21160462498664856
Batch 10/64 loss: -0.21486535668373108
Batch 11/64 loss: -0.20532652735710144
Batch 12/64 loss: -0.19490039348602295
Batch 13/64 loss: -0.1942845582962036
Batch 14/64 loss: -0.2206205427646637
Batch 15/64 loss: -0.20921534299850464
Batch 16/64 loss: -0.19588398933410645
Batch 17/64 loss: -0.22503310441970825
Batch 18/64 loss: -0.21794968843460083
Batch 19/64 loss: -0.21624335646629333
Batch 20/64 loss: -0.21002984046936035
Batch 21/64 loss: -0.21359968185424805
Batch 22/64 loss: -0.20757359266281128
Batch 23/64 loss: -0.18463760614395142
Batch 24/64 loss: -0.19071125984191895
Batch 25/64 loss: -0.1803327202796936
Batch 26/64 loss: -0.1964632272720337
Batch 27/64 loss: -0.19558173418045044
Batch 28/64 loss: -0.194566011428833
Batch 29/64 loss: -0.1951032280921936
Batch 30/64 loss: -0.22533905506134033
Batch 31/64 loss: -0.20735406875610352
Batch 32/64 loss: -0.20592862367630005
Batch 33/64 loss: -0.21004033088684082
Batch 34/64 loss: -0.2063484787940979
Batch 35/64 loss: -0.19051742553710938
Batch 36/64 loss: -0.20127910375595093
Batch 37/64 loss: -0.18571364879608154
Batch 38/64 loss: -0.2228013575077057
Batch 39/64 loss: -0.19041240215301514
Batch 40/64 loss: -0.2292448878288269
Batch 41/64 loss: -0.1850908398628235
Batch 42/64 loss: -0.20381414890289307
Batch 43/64 loss: -0.22955697774887085
Batch 44/64 loss: -0.2101176381111145
Batch 45/64 loss: -0.20556020736694336
Batch 46/64 loss: -0.16257774829864502
Batch 47/64 loss: -0.2102174162864685
Batch 48/64 loss: -0.20569390058517456
Batch 49/64 loss: -0.20505839586257935
Batch 50/64 loss: -0.1909409761428833
Batch 51/64 loss: -0.21586143970489502
Batch 52/64 loss: -0.2035793662071228
Batch 53/64 loss: -0.21523535251617432
Batch 54/64 loss: -0.20977729558944702
Batch 55/64 loss: -0.17838406562805176
Batch 56/64 loss: -0.15820276737213135
Batch 57/64 loss: -0.19947528839111328
Batch 58/64 loss: -0.17577624320983887
Batch 59/64 loss: -0.20167744159698486
Batch 60/64 loss: -0.1969318687915802
Batch 61/64 loss: -0.20638728141784668
Batch 62/64 loss: -0.20725420117378235
Batch 63/64 loss: -0.2023870348930359
Batch 64/64 loss: -0.15138590335845947
Epoch 439  Train loss: -0.20048124509699206  Val loss: -0.016453560684964418
Epoch 440
-------------------------------
Batch 1/64 loss: -0.20182281732559204
Batch 2/64 loss: -0.21401289105415344
Batch 3/64 loss: -0.17440879344940186
Batch 4/64 loss: -0.19771522283554077
Batch 5/64 loss: -0.2201269268989563
Batch 6/64 loss: -0.19645041227340698
Batch 7/64 loss: -0.20835357904434204
Batch 8/64 loss: -0.1956334114074707
Batch 9/64 loss: -0.20459860563278198
Batch 10/64 loss: -0.2092522382736206
Batch 11/64 loss: -0.17531120777130127
Batch 12/64 loss: -0.20606619119644165
Batch 13/64 loss: -0.21254009008407593
Batch 14/64 loss: -0.17458093166351318
Batch 15/64 loss: -0.20947280526161194
Batch 16/64 loss: -0.17443382740020752
Batch 17/64 loss: -0.20967704057693481
Batch 18/64 loss: -0.21048712730407715
Batch 19/64 loss: -0.18579387664794922
Batch 20/64 loss: -0.20027565956115723
Batch 21/64 loss: -0.19502723217010498
Batch 22/64 loss: -0.2067926824092865
Batch 23/64 loss: -0.19490563869476318
Batch 24/64 loss: -0.19823235273361206
Batch 25/64 loss: -0.18184590339660645
Batch 26/64 loss: -0.18113064765930176
Batch 27/64 loss: -0.20641112327575684
Batch 28/64 loss: -0.20503020286560059
Batch 29/64 loss: -0.20933347940444946
Batch 30/64 loss: -0.19063794612884521
Batch 31/64 loss: -0.1946420669555664
Batch 32/64 loss: -0.21261343359947205
Batch 33/64 loss: -0.21051985025405884
Batch 34/64 loss: -0.1900728940963745
Batch 35/64 loss: -0.20659419894218445
Batch 36/64 loss: -0.21332156658172607
Batch 37/64 loss: -0.19324451684951782
Batch 38/64 loss: -0.2018449902534485
Batch 39/64 loss: -0.19448131322860718
Batch 40/64 loss: -0.19993862509727478
Batch 41/64 loss: -0.20390474796295166
Batch 42/64 loss: -0.2191419005393982
Batch 43/64 loss: -0.20553234219551086
Batch 44/64 loss: -0.2037004828453064
Batch 45/64 loss: -0.2077786922454834
Batch 46/64 loss: -0.19428527355194092
Batch 47/64 loss: -0.1907663345336914
Batch 48/64 loss: -0.20328566431999207
Batch 49/64 loss: -0.19102847576141357
Batch 50/64 loss: -0.20211529731750488
Batch 51/64 loss: -0.1852303147315979
Batch 52/64 loss: -0.17880451679229736
Batch 53/64 loss: -0.201665997505188
Batch 54/64 loss: -0.2125963568687439
Batch 55/64 loss: -0.18817168474197388
Batch 56/64 loss: -0.20611783862113953
Batch 57/64 loss: -0.21624422073364258
Batch 58/64 loss: -0.18847370147705078
Batch 59/64 loss: -0.19296997785568237
Batch 60/64 loss: -0.22030004858970642
Batch 61/64 loss: -0.16874945163726807
Batch 62/64 loss: -0.22975024580955505
Batch 63/64 loss: -0.19997119903564453
Batch 64/64 loss: -0.11065590381622314
Epoch 440  Train loss: -0.19860723158892463  Val loss: -0.01273924952110474
Epoch 441
-------------------------------
Batch 1/64 loss: -0.21304011344909668
Batch 2/64 loss: -0.1783406138420105
Batch 3/64 loss: -0.2146061360836029
Batch 4/64 loss: -0.18852710723876953
Batch 5/64 loss: -0.1895865797996521
Batch 6/64 loss: -0.21549302339553833
Batch 7/64 loss: -0.22002765536308289
Batch 8/64 loss: -0.18293046951293945
Batch 9/64 loss: -0.22107982635498047
Batch 10/64 loss: -0.20785102248191833
Batch 11/64 loss: -0.19043725728988647
Batch 12/64 loss: -0.19689303636550903
Batch 13/64 loss: -0.22056981921195984
Batch 14/64 loss: -0.20736607909202576
Batch 15/64 loss: -0.20820152759552002
Batch 16/64 loss: -0.20280444622039795
Batch 17/64 loss: -0.2120550572872162
Batch 18/64 loss: -0.19308561086654663
Batch 19/64 loss: -0.1892024278640747
Batch 20/64 loss: -0.21017354726791382
Batch 21/64 loss: -0.1792634129524231
Batch 22/64 loss: -0.21980756521224976
Batch 23/64 loss: -0.20626598596572876
Batch 24/64 loss: -0.1968887448310852
Batch 25/64 loss: -0.1938191056251526
Batch 26/64 loss: -0.20017102360725403
Batch 27/64 loss: -0.20215094089508057
Batch 28/64 loss: -0.1963421106338501
Batch 29/64 loss: -0.19309473037719727
Batch 30/64 loss: -0.17515426874160767
Batch 31/64 loss: -0.21996575593948364
Batch 32/64 loss: -0.22369107604026794
Batch 33/64 loss: -0.21153655648231506
Batch 34/64 loss: -0.15491151809692383
Batch 35/64 loss: -0.18915241956710815
Batch 36/64 loss: -0.19326210021972656
Batch 37/64 loss: -0.2194301187992096
Batch 38/64 loss: -0.1903749704360962
Batch 39/64 loss: -0.19882887601852417
Batch 40/64 loss: -0.20014315843582153
Batch 41/64 loss: -0.16211020946502686
Batch 42/64 loss: -0.16518646478652954
Batch 43/64 loss: -0.21855145692825317
Batch 44/64 loss: -0.19003862142562866
Batch 45/64 loss: -0.20517969131469727
Batch 46/64 loss: -0.19430619478225708
Batch 47/64 loss: -0.18798071146011353
Batch 48/64 loss: -0.20405125617980957
Batch 49/64 loss: -0.20567560195922852
Batch 50/64 loss: -0.1769118309020996
Batch 51/64 loss: -0.16008025407791138
Batch 52/64 loss: -0.2089197337627411
Batch 53/64 loss: -0.21128186583518982
Batch 54/64 loss: -0.16414189338684082
Batch 55/64 loss: -0.1591130495071411
Batch 56/64 loss: -0.19440269470214844
Batch 57/64 loss: -0.1813141107559204
Batch 58/64 loss: -0.2087177336215973
Batch 59/64 loss: -0.164559006690979
Batch 60/64 loss: -0.20014357566833496
Batch 61/64 loss: -0.20552587509155273
Batch 62/64 loss: -0.1708897352218628
Batch 63/64 loss: -0.20322126150131226
Batch 64/64 loss: -0.18118417263031006
Epoch 441  Train loss: -0.1961524196699554  Val loss: -0.01595783766192669
Epoch 442
-------------------------------
Batch 1/64 loss: -0.20803135633468628
Batch 2/64 loss: -0.21151170134544373
Batch 3/64 loss: -0.2007129192352295
Batch 4/64 loss: -0.17391562461853027
Batch 5/64 loss: -0.20992600917816162
Batch 6/64 loss: -0.21245312690734863
Batch 7/64 loss: -0.2043769359588623
Batch 8/64 loss: -0.21140721440315247
Batch 9/64 loss: -0.1938599944114685
Batch 10/64 loss: -0.19104450941085815
Batch 11/64 loss: -0.21755832433700562
Batch 12/64 loss: -0.21964901685714722
Batch 13/64 loss: -0.22052812576293945
Batch 14/64 loss: -0.2080172598361969
Batch 15/64 loss: -0.2381470799446106
Batch 16/64 loss: -0.19438260793685913
Batch 17/64 loss: -0.20366376638412476
Batch 18/64 loss: -0.20107415318489075
Batch 19/64 loss: -0.20603197813034058
Batch 20/64 loss: -0.20862454175949097
Batch 21/64 loss: -0.2066403329372406
Batch 22/64 loss: -0.1647815704345703
Batch 23/64 loss: -0.2101362943649292
Batch 24/64 loss: -0.17527562379837036
Batch 25/64 loss: -0.1936388611793518
Batch 26/64 loss: -0.19130003452301025
Batch 27/64 loss: -0.19618839025497437
Batch 28/64 loss: -0.1736518144607544
Batch 29/64 loss: -0.1898742914199829
Batch 30/64 loss: -0.19336825609207153
Batch 31/64 loss: -0.2085924744606018
Batch 32/64 loss: -0.15750926733016968
Batch 33/64 loss: -0.1793684959411621
Batch 34/64 loss: -0.1768321990966797
Batch 35/64 loss: -0.17935532331466675
Batch 36/64 loss: -0.20832812786102295
Batch 37/64 loss: -0.20634767413139343
Batch 38/64 loss: -0.17852813005447388
Batch 39/64 loss: -0.1922338604927063
Batch 40/64 loss: -0.18885564804077148
Batch 41/64 loss: -0.1917935609817505
Batch 42/64 loss: -0.1955241560935974
Batch 43/64 loss: -0.2141016125679016
Batch 44/64 loss: -0.17434751987457275
Batch 45/64 loss: -0.2128048539161682
Batch 46/64 loss: -0.20651128888130188
Batch 47/64 loss: -0.20354318618774414
Batch 48/64 loss: -0.22140109539031982
Batch 49/64 loss: -0.20880991220474243
Batch 50/64 loss: -0.21547925472259521
Batch 51/64 loss: -0.2065700888633728
Batch 52/64 loss: -0.2109399437904358
Batch 53/64 loss: -0.213786780834198
Batch 54/64 loss: -0.17792463302612305
Batch 55/64 loss: -0.18009519577026367
Batch 56/64 loss: -0.21091777086257935
Batch 57/64 loss: -0.20009946823120117
Batch 58/64 loss: -0.2418433427810669
Batch 59/64 loss: -0.21209996938705444
Batch 60/64 loss: -0.22429469227790833
Batch 61/64 loss: -0.19536006450653076
Batch 62/64 loss: -0.1943657398223877
Batch 63/64 loss: -0.198838472366333
Batch 64/64 loss: -0.18758755922317505
Epoch 442  Train loss: -0.20012339119817696  Val loss: -0.013540051647068299
Epoch 443
-------------------------------
Batch 1/64 loss: -0.19202053546905518
Batch 2/64 loss: -0.17613321542739868
Batch 3/64 loss: -0.2184072732925415
Batch 4/64 loss: -0.21474531292915344
Batch 5/64 loss: -0.22268235683441162
Batch 6/64 loss: -0.22056689858436584
Batch 7/64 loss: -0.20162346959114075
Batch 8/64 loss: -0.20762574672698975
Batch 9/64 loss: -0.20454102754592896
Batch 10/64 loss: -0.22162950038909912
Batch 11/64 loss: -0.204510897397995
Batch 12/64 loss: -0.2051682472229004
Batch 13/64 loss: -0.21557104587554932
Batch 14/64 loss: -0.1944636106491089
Batch 15/64 loss: -0.1862560510635376
Batch 16/64 loss: -0.2027457058429718
Batch 17/64 loss: -0.17876362800598145
Batch 18/64 loss: -0.17632585763931274
Batch 19/64 loss: -0.19542813301086426
Batch 20/64 loss: -0.18841177225112915
Batch 21/64 loss: -0.19320297241210938
Batch 22/64 loss: -0.16631031036376953
Batch 23/64 loss: -0.1922115683555603
Batch 24/64 loss: -0.1955571174621582
Batch 25/64 loss: -0.21055835485458374
Batch 26/64 loss: -0.1831151247024536
Batch 27/64 loss: -0.19650471210479736
Batch 28/64 loss: -0.19775670766830444
Batch 29/64 loss: -0.17825478315353394
Batch 30/64 loss: -0.19302237033843994
Batch 31/64 loss: -0.19871297478675842
Batch 32/64 loss: -0.15705984830856323
Batch 33/64 loss: -0.19503015279769897
Batch 34/64 loss: -0.17155241966247559
Batch 35/64 loss: -0.17284786701202393
Batch 36/64 loss: -0.18969035148620605
Batch 37/64 loss: -0.2122577428817749
Batch 38/64 loss: -0.19158083200454712
Batch 39/64 loss: -0.1834641695022583
Batch 40/64 loss: -0.20480436086654663
Batch 41/64 loss: -0.18907558917999268
Batch 42/64 loss: -0.18016493320465088
Batch 43/64 loss: -0.21353021264076233
Batch 44/64 loss: -0.18691635131835938
Batch 45/64 loss: -0.19301313161849976
Batch 46/64 loss: -0.19895946979522705
Batch 47/64 loss: -0.1804446578025818
Batch 48/64 loss: -0.20585140585899353
Batch 49/64 loss: -0.20236241817474365
Batch 50/64 loss: -0.19575762748718262
Batch 51/64 loss: -0.20498812198638916
Batch 52/64 loss: -0.20657962560653687
Batch 53/64 loss: -0.2027604579925537
Batch 54/64 loss: -0.19499695301055908
Batch 55/64 loss: -0.19162636995315552
Batch 56/64 loss: -0.22020584344863892
Batch 57/64 loss: -0.2134729027748108
Batch 58/64 loss: -0.22504237294197083
Batch 59/64 loss: -0.20896774530410767
Batch 60/64 loss: -0.19227802753448486
Batch 61/64 loss: -0.19441670179367065
Batch 62/64 loss: -0.16030871868133545
Batch 63/64 loss: -0.21922066807746887
Batch 64/64 loss: -0.18682199716567993
Epoch 443  Train loss: -0.1965830875378029  Val loss: -0.015841896181663696
Epoch 444
-------------------------------
Batch 1/64 loss: -0.1993284821510315
Batch 2/64 loss: -0.2169138789176941
Batch 3/64 loss: -0.20891210436820984
Batch 4/64 loss: -0.19010955095291138
Batch 5/64 loss: -0.19641399383544922
Batch 6/64 loss: -0.21090811491012573
Batch 7/64 loss: -0.23539000749588013
Batch 8/64 loss: -0.21309372782707214
Batch 9/64 loss: -0.2088450789451599
Batch 10/64 loss: -0.20627334713935852
Batch 11/64 loss: -0.20807498693466187
Batch 12/64 loss: -0.1706269383430481
Batch 13/64 loss: -0.19704800844192505
Batch 14/64 loss: -0.1994563639163971
Batch 15/64 loss: -0.20175349712371826
Batch 16/64 loss: -0.20475617051124573
Batch 17/64 loss: -0.20850247144699097
Batch 18/64 loss: -0.21126773953437805
Batch 19/64 loss: -0.17908847332000732
Batch 20/64 loss: -0.2047625482082367
Batch 21/64 loss: -0.2044554352760315
Batch 22/64 loss: -0.21781587600708008
Batch 23/64 loss: -0.1849384903907776
Batch 24/64 loss: -0.20226126909255981
Batch 25/64 loss: -0.19247812032699585
Batch 26/64 loss: -0.19678860902786255
Batch 27/64 loss: -0.20052489638328552
Batch 28/64 loss: -0.21675527095794678
Batch 29/64 loss: -0.20273739099502563
Batch 30/64 loss: -0.19559073448181152
Batch 31/64 loss: -0.19647657871246338
Batch 32/64 loss: -0.1980544924736023
Batch 33/64 loss: -0.21407634019851685
Batch 34/64 loss: -0.20756477117538452
Batch 35/64 loss: -0.22657287120819092
Batch 36/64 loss: -0.22002685070037842
Batch 37/64 loss: -0.1872793436050415
Batch 38/64 loss: -0.19725871086120605
Batch 39/64 loss: -0.19761717319488525
Batch 40/64 loss: -0.20787876844406128
Batch 41/64 loss: -0.21234357357025146
Batch 42/64 loss: -0.2058296501636505
Batch 43/64 loss: -0.18244123458862305
Batch 44/64 loss: -0.17490732669830322
Batch 45/64 loss: -0.2151343822479248
Batch 46/64 loss: -0.20570236444473267
Batch 47/64 loss: -0.1694832444190979
Batch 48/64 loss: -0.17092877626419067
Batch 49/64 loss: -0.20013734698295593
Batch 50/64 loss: -0.21600496768951416
Batch 51/64 loss: -0.1839599609375
Batch 52/64 loss: -0.19908052682876587
Batch 53/64 loss: -0.19592159986495972
Batch 54/64 loss: -0.21841704845428467
Batch 55/64 loss: -0.21261855959892273
Batch 56/64 loss: -0.21674782037734985
Batch 57/64 loss: -0.1840934157371521
Batch 58/64 loss: -0.17079025506973267
Batch 59/64 loss: -0.16877281665802002
Batch 60/64 loss: -0.17666339874267578
Batch 61/64 loss: -0.1760396957397461
Batch 62/64 loss: -0.1924392580986023
Batch 63/64 loss: -0.18613892793655396
Batch 64/64 loss: -0.18154531717300415
Epoch 444  Train loss: -0.19936364889144897  Val loss: -0.015656520615738283
Epoch 445
-------------------------------
Batch 1/64 loss: -0.21254578232765198
Batch 2/64 loss: -0.2042205035686493
Batch 3/64 loss: -0.19751161336898804
Batch 4/64 loss: -0.22436761856079102
Batch 5/64 loss: -0.24173733592033386
Batch 6/64 loss: -0.2075675129890442
Batch 7/64 loss: -0.1971721649169922
Batch 8/64 loss: -0.20952537655830383
Batch 9/64 loss: -0.17852503061294556
Batch 10/64 loss: -0.18747931718826294
Batch 11/64 loss: -0.22560492157936096
Batch 12/64 loss: -0.20375287532806396
Batch 13/64 loss: -0.20974361896514893
Batch 14/64 loss: -0.1812611222267151
Batch 15/64 loss: -0.20784693956375122
Batch 16/64 loss: -0.20042634010314941
Batch 17/64 loss: -0.1844131350517273
Batch 18/64 loss: -0.22265443205833435
Batch 19/64 loss: -0.20332187414169312
Batch 20/64 loss: -0.19645118713378906
Batch 21/64 loss: -0.19350451231002808
Batch 22/64 loss: -0.1973814070224762
Batch 23/64 loss: -0.18247771263122559
Batch 24/64 loss: -0.20801693201065063
Batch 25/64 loss: -0.20891231298446655
Batch 26/64 loss: -0.20260131359100342
Batch 27/64 loss: -0.18980109691619873
Batch 28/64 loss: -0.18890416622161865
Batch 29/64 loss: -0.17879390716552734
Batch 30/64 loss: -0.20527446269989014
Batch 31/64 loss: -0.1854022741317749
Batch 32/64 loss: -0.17185503244400024
Batch 33/64 loss: -0.19176065921783447
Batch 34/64 loss: -0.22098830342292786
Batch 35/64 loss: -0.18749195337295532
Batch 36/64 loss: -0.1920633316040039
Batch 37/64 loss: -0.19740575551986694
Batch 38/64 loss: -0.22362405061721802
Batch 39/64 loss: -0.20510929822921753
Batch 40/64 loss: -0.1890999674797058
Batch 41/64 loss: -0.18689852952957153
Batch 42/64 loss: -0.21505823731422424
Batch 43/64 loss: -0.21523791551589966
Batch 44/64 loss: -0.19776064157485962
Batch 45/64 loss: -0.17722088098526
Batch 46/64 loss: -0.22071635723114014
Batch 47/64 loss: -0.20631960034370422
Batch 48/64 loss: -0.19699245691299438
Batch 49/64 loss: -0.2085031270980835
Batch 50/64 loss: -0.19745367765426636
Batch 51/64 loss: -0.1763710379600525
Batch 52/64 loss: -0.14062738418579102
Batch 53/64 loss: -0.18363744020462036
Batch 54/64 loss: -0.2062780261039734
Batch 55/64 loss: -0.2012287676334381
Batch 56/64 loss: -0.21041816473007202
Batch 57/64 loss: -0.19805794954299927
Batch 58/64 loss: -0.19022256135940552
Batch 59/64 loss: -0.19058263301849365
Batch 60/64 loss: -0.190912127494812
Batch 61/64 loss: -0.20250511169433594
Batch 62/64 loss: -0.2080685794353485
Batch 63/64 loss: -0.18225395679473877
Batch 64/64 loss: -0.20262497663497925
Epoch 445  Train loss: -0.19877476154589185  Val loss: -0.010184832659783642
Epoch 446
-------------------------------
Batch 1/64 loss: -0.20087513327598572
Batch 2/64 loss: -0.21067631244659424
Batch 3/64 loss: -0.2145097255706787
Batch 4/64 loss: -0.21225744485855103
Batch 5/64 loss: -0.22940349578857422
Batch 6/64 loss: -0.2063247561454773
Batch 7/64 loss: -0.18944144248962402
Batch 8/64 loss: -0.12622016668319702
Batch 9/64 loss: -0.21697884798049927
Batch 10/64 loss: -0.19725167751312256
Batch 11/64 loss: -0.1882069706916809
Batch 12/64 loss: -0.21572259068489075
Batch 13/64 loss: -0.18938130140304565
Batch 14/64 loss: -0.19669699668884277
Batch 15/64 loss: -0.20575329661369324
Batch 16/64 loss: -0.217989981174469
Batch 17/64 loss: -0.2100207805633545
Batch 18/64 loss: -0.18487310409545898
Batch 19/64 loss: -0.16354453563690186
Batch 20/64 loss: -0.20200598239898682
Batch 21/64 loss: -0.19135212898254395
Batch 22/64 loss: -0.20180931687355042
Batch 23/64 loss: -0.18652433156967163
Batch 24/64 loss: -0.18885266780853271
Batch 25/64 loss: -0.18500465154647827
Batch 26/64 loss: -0.20056003332138062
Batch 27/64 loss: -0.18613183498382568
Batch 28/64 loss: -0.19344782829284668
Batch 29/64 loss: -0.20838749408721924
Batch 30/64 loss: -0.19767600297927856
Batch 31/64 loss: -0.21164017915725708
Batch 32/64 loss: -0.20461255311965942
Batch 33/64 loss: -0.21923691034317017
Batch 34/64 loss: -0.19916152954101562
Batch 35/64 loss: -0.21604964137077332
Batch 36/64 loss: -0.21166089177131653
Batch 37/64 loss: -0.20071342587471008
Batch 38/64 loss: -0.20690548419952393
Batch 39/64 loss: -0.21211165189743042
Batch 40/64 loss: -0.1893681287765503
Batch 41/64 loss: -0.21961575746536255
Batch 42/64 loss: -0.20481598377227783
Batch 43/64 loss: -0.1888478398323059
Batch 44/64 loss: -0.19308936595916748
Batch 45/64 loss: -0.19631576538085938
Batch 46/64 loss: -0.17916452884674072
Batch 47/64 loss: -0.18439257144927979
Batch 48/64 loss: -0.20128792524337769
Batch 49/64 loss: -0.17971491813659668
Batch 50/64 loss: -0.19868841767311096
Batch 51/64 loss: -0.20762032270431519
Batch 52/64 loss: -0.19830873608589172
Batch 53/64 loss: -0.20828664302825928
Batch 54/64 loss: -0.18965065479278564
Batch 55/64 loss: -0.20005181431770325
Batch 56/64 loss: -0.17835932970046997
Batch 57/64 loss: -0.20600491762161255
Batch 58/64 loss: -0.20530056953430176
Batch 59/64 loss: -0.1794413924217224
Batch 60/64 loss: -0.18194890022277832
Batch 61/64 loss: -0.1997385025024414
Batch 62/64 loss: -0.19275885820388794
Batch 63/64 loss: -0.21483474969863892
Batch 64/64 loss: -0.20435285568237305
Epoch 446  Train loss: -0.19844461702832988  Val loss: -0.01627247276175063
Epoch 447
-------------------------------
Batch 1/64 loss: -0.22667130827903748
Batch 2/64 loss: -0.19019299745559692
Batch 3/64 loss: -0.20674502849578857
Batch 4/64 loss: -0.2163628339767456
Batch 5/64 loss: -0.2182605266571045
Batch 6/64 loss: -0.22394174337387085
Batch 7/64 loss: -0.22517341375350952
Batch 8/64 loss: -0.21455338597297668
Batch 9/64 loss: -0.20425477623939514
Batch 10/64 loss: -0.20342689752578735
Batch 11/64 loss: -0.1865471601486206
Batch 12/64 loss: -0.21696192026138306
Batch 13/64 loss: -0.2203359603881836
Batch 14/64 loss: -0.19967028498649597
Batch 15/64 loss: -0.2005525827407837
Batch 16/64 loss: -0.22411218285560608
Batch 17/64 loss: -0.20679888129234314
Batch 18/64 loss: -0.20326554775238037
Batch 19/64 loss: -0.17661166191101074
Batch 20/64 loss: -0.1680566668510437
Batch 21/64 loss: -0.22316911816596985
Batch 22/64 loss: -0.18733274936676025
Batch 23/64 loss: -0.20876729488372803
Batch 24/64 loss: -0.19507157802581787
Batch 25/64 loss: -0.20095723867416382
Batch 26/64 loss: -0.2211248278617859
Batch 27/64 loss: -0.19613492488861084
Batch 28/64 loss: -0.21578320860862732
Batch 29/64 loss: -0.1872107982635498
Batch 30/64 loss: -0.19858118891716003
Batch 31/64 loss: -0.2038208246231079
Batch 32/64 loss: -0.18586105108261108
Batch 33/64 loss: -0.19112730026245117
Batch 34/64 loss: -0.18104952573776245
Batch 35/64 loss: -0.17814135551452637
Batch 36/64 loss: -0.2088557481765747
Batch 37/64 loss: -0.20833590626716614
Batch 38/64 loss: -0.21304398775100708
Batch 39/64 loss: -0.18089723587036133
Batch 40/64 loss: -0.22325557470321655
Batch 41/64 loss: -0.20534643530845642
Batch 42/64 loss: -0.20549476146697998
Batch 43/64 loss: -0.18536871671676636
Batch 44/64 loss: -0.21687838435173035
Batch 45/64 loss: -0.19876807928085327
Batch 46/64 loss: -0.19328022003173828
Batch 47/64 loss: -0.2278558611869812
Batch 48/64 loss: -0.19834542274475098
Batch 49/64 loss: -0.2040775716304779
Batch 50/64 loss: -0.19610360264778137
Batch 51/64 loss: -0.1848689317703247
Batch 52/64 loss: -0.1589847207069397
Batch 53/64 loss: -0.16676652431488037
Batch 54/64 loss: -0.18781453371047974
Batch 55/64 loss: -0.20791441202163696
Batch 56/64 loss: -0.19003993272781372
Batch 57/64 loss: -0.22456690669059753
Batch 58/64 loss: -0.21426400542259216
Batch 59/64 loss: -0.1788562536239624
Batch 60/64 loss: -0.20816296339035034
Batch 61/64 loss: -0.21820876002311707
Batch 62/64 loss: -0.20778882503509521
Batch 63/64 loss: -0.1887723207473755
Batch 64/64 loss: -0.20176905393600464
Epoch 447  Train loss: -0.201739170738295  Val loss: -0.014273517934727096
Epoch 448
-------------------------------
Batch 1/64 loss: -0.21252024173736572
Batch 2/64 loss: -0.18534356355667114
Batch 3/64 loss: -0.19778069853782654
Batch 4/64 loss: -0.22146755456924438
Batch 5/64 loss: -0.20864656567573547
Batch 6/64 loss: -0.22017920017242432
Batch 7/64 loss: -0.202519029378891
Batch 8/64 loss: -0.22051683068275452
Batch 9/64 loss: -0.20368388295173645
Batch 10/64 loss: -0.2064928114414215
Batch 11/64 loss: -0.2194688320159912
Batch 12/64 loss: -0.22004982829093933
Batch 13/64 loss: -0.21576648950576782
Batch 14/64 loss: -0.19281160831451416
Batch 15/64 loss: -0.2136252522468567
Batch 16/64 loss: -0.20094746351242065
Batch 17/64 loss: -0.21055644750595093
Batch 18/64 loss: -0.22964000701904297
Batch 19/64 loss: -0.2205607295036316
Batch 20/64 loss: -0.16610604524612427
Batch 21/64 loss: -0.2162749171257019
Batch 22/64 loss: -0.18613004684448242
Batch 23/64 loss: -0.21853703260421753
Batch 24/64 loss: -0.18989402055740356
Batch 25/64 loss: -0.18588250875473022
Batch 26/64 loss: -0.1678439974784851
Batch 27/64 loss: -0.20871961116790771
Batch 28/64 loss: -0.21532437205314636
Batch 29/64 loss: -0.20797958970069885
Batch 30/64 loss: -0.1965348720550537
Batch 31/64 loss: -0.20603501796722412
Batch 32/64 loss: -0.20582818984985352
Batch 33/64 loss: -0.20107045769691467
Batch 34/64 loss: -0.2063450813293457
Batch 35/64 loss: -0.20737943053245544
Batch 36/64 loss: -0.21292078495025635
Batch 37/64 loss: -0.2095397710800171
Batch 38/64 loss: -0.20424634218215942
Batch 39/64 loss: -0.21437743306159973
Batch 40/64 loss: -0.19187414646148682
Batch 41/64 loss: -0.2270779013633728
Batch 42/64 loss: -0.1962876319885254
Batch 43/64 loss: -0.20586413145065308
Batch 44/64 loss: -0.20091670751571655
Batch 45/64 loss: -0.19087529182434082
Batch 46/64 loss: -0.21317869424819946
Batch 47/64 loss: -0.20379358530044556
Batch 48/64 loss: -0.20339342951774597
Batch 49/64 loss: -0.17995160818099976
Batch 50/64 loss: -0.22423693537712097
Batch 51/64 loss: -0.20756635069847107
Batch 52/64 loss: -0.2105444371700287
Batch 53/64 loss: -0.2139696478843689
Batch 54/64 loss: -0.20254170894622803
Batch 55/64 loss: -0.20664718747138977
Batch 56/64 loss: -0.2248903512954712
Batch 57/64 loss: -0.19703152775764465
Batch 58/64 loss: -0.2082526683807373
Batch 59/64 loss: -0.18948060274124146
Batch 60/64 loss: -0.2066841423511505
Batch 61/64 loss: -0.18357610702514648
Batch 62/64 loss: -0.18717598915100098
Batch 63/64 loss: -0.2165866196155548
Batch 64/64 loss: -0.2085031270980835
Epoch 448  Train loss: -0.2051501381630991  Val loss: -0.016179613641037566
Epoch 449
-------------------------------
Batch 1/64 loss: -0.22073808312416077
Batch 2/64 loss: -0.20323312282562256
Batch 3/64 loss: -0.2271348237991333
Batch 4/64 loss: -0.17719244956970215
Batch 5/64 loss: -0.21579307317733765
Batch 6/64 loss: -0.21406573057174683
Batch 7/64 loss: -0.20602458715438843
Batch 8/64 loss: -0.20352378487586975
Batch 9/64 loss: -0.21882423758506775
Batch 10/64 loss: -0.22270900011062622
Batch 11/64 loss: -0.20781844854354858
Batch 12/64 loss: -0.2123088538646698
Batch 13/64 loss: -0.21773013472557068
Batch 14/64 loss: -0.20928701758384705
Batch 15/64 loss: -0.19557100534439087
Batch 16/64 loss: -0.19278299808502197
Batch 17/64 loss: -0.20023521780967712
Batch 18/64 loss: -0.21507972478866577
Batch 19/64 loss: -0.19978255033493042
Batch 20/64 loss: -0.18566107749938965
Batch 21/64 loss: -0.20865392684936523
Batch 22/64 loss: -0.2017005980014801
Batch 23/64 loss: -0.19407707452774048
Batch 24/64 loss: -0.20372703671455383
Batch 25/64 loss: -0.22933530807495117
Batch 26/64 loss: -0.18773025274276733
Batch 27/64 loss: -0.18663477897644043
Batch 28/64 loss: -0.18695348501205444
Batch 29/64 loss: -0.18696331977844238
Batch 30/64 loss: -0.1703755259513855
Batch 31/64 loss: -0.2056814730167389
Batch 32/64 loss: -0.19689959287643433
Batch 33/64 loss: -0.20766842365264893
Batch 34/64 loss: -0.2064906656742096
Batch 35/64 loss: -0.204207181930542
Batch 36/64 loss: -0.19724100828170776
Batch 37/64 loss: -0.20946034789085388
Batch 38/64 loss: -0.21373999118804932
Batch 39/64 loss: -0.1957474946975708
Batch 40/64 loss: -0.22881507873535156
Batch 41/64 loss: -0.22389811277389526
Batch 42/64 loss: -0.21418392658233643
Batch 43/64 loss: -0.20355775952339172
Batch 44/64 loss: -0.204584002494812
Batch 45/64 loss: -0.1712937355041504
Batch 46/64 loss: -0.201651930809021
Batch 47/64 loss: -0.2168107032775879
Batch 48/64 loss: -0.21736973524093628
Batch 49/64 loss: -0.2006862759590149
Batch 50/64 loss: -0.22707998752593994
Batch 51/64 loss: -0.20205843448638916
Batch 52/64 loss: -0.22468030452728271
Batch 53/64 loss: -0.1958255171775818
Batch 54/64 loss: -0.23179444670677185
Batch 55/64 loss: -0.19696074724197388
Batch 56/64 loss: -0.21350866556167603
Batch 57/64 loss: -0.18088281154632568
Batch 58/64 loss: -0.20090332627296448
Batch 59/64 loss: -0.2125454843044281
Batch 60/64 loss: -0.21163487434387207
Batch 61/64 loss: -0.17141854763031006
Batch 62/64 loss: -0.18541157245635986
Batch 63/64 loss: -0.18994438648223877
Batch 64/64 loss: -0.1823725700378418
Epoch 449  Train loss: -0.20396961885340073  Val loss: -0.013524721987878335
Epoch 450
-------------------------------
Batch 1/64 loss: -0.2000168263912201
Batch 2/64 loss: -0.23623445630073547
Batch 3/64 loss: -0.20407342910766602
Batch 4/64 loss: -0.21397995948791504
Batch 5/64 loss: -0.22084254026412964
Batch 6/64 loss: -0.17756974697113037
Batch 7/64 loss: -0.2074955701828003
Batch 8/64 loss: -0.21274185180664062
Batch 9/64 loss: -0.20482692122459412
Batch 10/64 loss: -0.19661393761634827
Batch 11/64 loss: -0.20078927278518677
Batch 12/64 loss: -0.20455878973007202
Batch 13/64 loss: -0.2141696810722351
Batch 14/64 loss: -0.17337870597839355
Batch 15/64 loss: -0.21956288814544678
Batch 16/64 loss: -0.1824215054512024
Batch 17/64 loss: -0.21406769752502441
Batch 18/64 loss: -0.1931227445602417
Batch 19/64 loss: -0.17367076873779297
Batch 20/64 loss: -0.1887180209159851
Batch 21/64 loss: -0.19375497102737427
Batch 22/64 loss: -0.204566091299057
Batch 23/64 loss: -0.21126076579093933
Batch 24/64 loss: -0.2053232192993164
Batch 25/64 loss: -0.20905816555023193
Batch 26/64 loss: -0.21585729718208313
Batch 27/64 loss: -0.2099866271018982
Batch 28/64 loss: -0.16734325885772705
Batch 29/64 loss: -0.2058163285255432
Batch 30/64 loss: -0.1927502155303955
Batch 31/64 loss: -0.19796109199523926
Batch 32/64 loss: -0.2020612359046936
Batch 33/64 loss: -0.2046462893486023
Batch 34/64 loss: -0.20915073156356812
Batch 35/64 loss: -0.19491422176361084
Batch 36/64 loss: -0.16986459493637085
Batch 37/64 loss: -0.19788074493408203
Batch 38/64 loss: -0.21208271384239197
Batch 39/64 loss: -0.22282293438911438
Batch 40/64 loss: -0.20144611597061157
Batch 41/64 loss: -0.21735984086990356
Batch 42/64 loss: -0.2125217616558075
Batch 43/64 loss: -0.20378726720809937
Batch 44/64 loss: -0.1448913812637329
Batch 45/64 loss: -0.17410308122634888
Batch 46/64 loss: -0.21181726455688477
Batch 47/64 loss: -0.2016046643257141
Batch 48/64 loss: -0.2083941102027893
Batch 49/64 loss: -0.18994879722595215
Batch 50/64 loss: -0.19512838125228882
Batch 51/64 loss: -0.19651636481285095
Batch 52/64 loss: -0.17529720067977905
Batch 53/64 loss: -0.19244903326034546
Batch 54/64 loss: -0.20182082056999207
Batch 55/64 loss: -0.2007262110710144
Batch 56/64 loss: -0.22290337085723877
Batch 57/64 loss: -0.19187068939208984
Batch 58/64 loss: -0.20586079359054565
Batch 59/64 loss: -0.20992982387542725
Batch 60/64 loss: -0.2164859175682068
Batch 61/64 loss: -0.201331228017807
Batch 62/64 loss: -0.20708152651786804
Batch 63/64 loss: -0.21858584880828857
Batch 64/64 loss: -0.21287256479263306
Epoch 450  Train loss: -0.2012152585328794  Val loss: -0.015785547875866446
Epoch 451
-------------------------------
Batch 1/64 loss: -0.1739133596420288
Batch 2/64 loss: -0.20805969834327698
Batch 3/64 loss: -0.20514097809791565
Batch 4/64 loss: -0.18982839584350586
Batch 5/64 loss: -0.19551599025726318
Batch 6/64 loss: -0.19686603546142578
Batch 7/64 loss: -0.2262551188468933
Batch 8/64 loss: -0.193234384059906
Batch 9/64 loss: -0.214684396982193
Batch 10/64 loss: -0.20436999201774597
Batch 11/64 loss: -0.21182817220687866
Batch 12/64 loss: -0.15376871824264526
Batch 13/64 loss: -0.18147343397140503
Batch 14/64 loss: -0.2030162811279297
Batch 15/64 loss: -0.2272549271583557
Batch 16/64 loss: -0.20472806692123413
Batch 17/64 loss: -0.1855265498161316
Batch 18/64 loss: -0.20243540406227112
Batch 19/64 loss: -0.16018712520599365
Batch 20/64 loss: -0.2203947901725769
Batch 21/64 loss: -0.21789288520812988
Batch 22/64 loss: -0.21968704462051392
Batch 23/64 loss: -0.2121378779411316
Batch 24/64 loss: -0.2365429401397705
Batch 25/64 loss: -0.20954561233520508
Batch 26/64 loss: -0.22107094526290894
Batch 27/64 loss: -0.206304669380188
Batch 28/64 loss: -0.17998254299163818
Batch 29/64 loss: -0.189100444316864
Batch 30/64 loss: -0.20858293771743774
Batch 31/64 loss: -0.23371130228042603
Batch 32/64 loss: -0.1969379186630249
Batch 33/64 loss: -0.20944225788116455
Batch 34/64 loss: -0.21510043740272522
Batch 35/64 loss: -0.19391822814941406
Batch 36/64 loss: -0.21101155877113342
Batch 37/64 loss: -0.1857350468635559
Batch 38/64 loss: -0.1980915069580078
Batch 39/64 loss: -0.21404343843460083
Batch 40/64 loss: -0.20595496892929077
Batch 41/64 loss: -0.227469801902771
Batch 42/64 loss: -0.21355527639389038
Batch 43/64 loss: -0.19944721460342407
Batch 44/64 loss: -0.21507468819618225
Batch 45/64 loss: -0.19277364015579224
Batch 46/64 loss: -0.21238675713539124
Batch 47/64 loss: -0.20650500059127808
Batch 48/64 loss: -0.18977642059326172
Batch 49/64 loss: -0.20877382159233093
Batch 50/64 loss: -0.2182103395462036
Batch 51/64 loss: -0.2079891562461853
Batch 52/64 loss: -0.21751296520233154
Batch 53/64 loss: -0.1939428448677063
Batch 54/64 loss: -0.2137494683265686
Batch 55/64 loss: -0.1911259889602661
Batch 56/64 loss: -0.20758742094039917
Batch 57/64 loss: -0.18217241764068604
Batch 58/64 loss: -0.2251816987991333
Batch 59/64 loss: -0.19174271821975708
Batch 60/64 loss: -0.21477216482162476
Batch 61/64 loss: -0.21841773390769958
Batch 62/64 loss: -0.19267213344573975
Batch 63/64 loss: -0.21742987632751465
Batch 64/64 loss: -0.18113380670547485
Epoch 451  Train loss: -0.2041944515471365  Val loss: -0.015536298252053278
Epoch 452
-------------------------------
Batch 1/64 loss: -0.22522184252738953
Batch 2/64 loss: -0.24832409620285034
Batch 3/64 loss: -0.19379723072052002
Batch 4/64 loss: -0.2011052966117859
Batch 5/64 loss: -0.2109789252281189
Batch 6/64 loss: -0.19550859928131104
Batch 7/64 loss: -0.18999385833740234
Batch 8/64 loss: -0.2094687819480896
Batch 9/64 loss: -0.16755074262619019
Batch 10/64 loss: -0.2024909257888794
Batch 11/64 loss: -0.21729761362075806
Batch 12/64 loss: -0.2163119912147522
Batch 13/64 loss: -0.2212478518486023
Batch 14/64 loss: -0.18703007698059082
Batch 15/64 loss: -0.22853627800941467
Batch 16/64 loss: -0.21075040102005005
Batch 17/64 loss: -0.20643603801727295
Batch 18/64 loss: -0.23720180988311768
Batch 19/64 loss: -0.2172529697418213
Batch 20/64 loss: -0.20414429903030396
Batch 21/64 loss: -0.21332532167434692
Batch 22/64 loss: -0.1853342056274414
Batch 23/64 loss: -0.1938534379005432
Batch 24/64 loss: -0.21961495280265808
Batch 25/64 loss: -0.19897496700286865
Batch 26/64 loss: -0.1984209418296814
Batch 27/64 loss: -0.21610361337661743
Batch 28/64 loss: -0.21259930729866028
Batch 29/64 loss: -0.19932571053504944
Batch 30/64 loss: -0.2044098973274231
Batch 31/64 loss: -0.17454379796981812
Batch 32/64 loss: -0.2130817472934723
Batch 33/64 loss: -0.20988216996192932
Batch 34/64 loss: -0.20730987191200256
Batch 35/64 loss: -0.18542468547821045
Batch 36/64 loss: -0.17645502090454102
Batch 37/64 loss: -0.19922471046447754
Batch 38/64 loss: -0.22394493222236633
Batch 39/64 loss: -0.20908665657043457
Batch 40/64 loss: -0.19665241241455078
Batch 41/64 loss: -0.19767722487449646
Batch 42/64 loss: -0.191178560256958
Batch 43/64 loss: -0.20009642839431763
Batch 44/64 loss: -0.17773306369781494
Batch 45/64 loss: -0.2223564088344574
Batch 46/64 loss: -0.21130657196044922
Batch 47/64 loss: -0.20882299542427063
Batch 48/64 loss: -0.1998755931854248
Batch 49/64 loss: -0.19089341163635254
Batch 50/64 loss: -0.21632981300354004
Batch 51/64 loss: -0.21361249685287476
Batch 52/64 loss: -0.19997486472129822
Batch 53/64 loss: -0.16475212574005127
Batch 54/64 loss: -0.23566681146621704
Batch 55/64 loss: -0.21506500244140625
Batch 56/64 loss: -0.19597482681274414
Batch 57/64 loss: -0.20238065719604492
Batch 58/64 loss: -0.16284191608428955
Batch 59/64 loss: -0.22859787940979004
Batch 60/64 loss: -0.1707993745803833
Batch 61/64 loss: -0.21663695573806763
Batch 62/64 loss: -0.20338886976242065
Batch 63/64 loss: -0.19191086292266846
Batch 64/64 loss: -0.17505085468292236
Epoch 452  Train loss: -0.20356672698376224  Val loss: -0.014616895787085044
Epoch 453
-------------------------------
Batch 1/64 loss: -0.21614894270896912
Batch 2/64 loss: -0.21005097031593323
Batch 3/64 loss: -0.21088123321533203
Batch 4/64 loss: -0.2134057879447937
Batch 5/64 loss: -0.17860054969787598
Batch 6/64 loss: -0.2330191731452942
Batch 7/64 loss: -0.22100728750228882
Batch 8/64 loss: -0.2324339747428894
Batch 9/64 loss: -0.2231951355934143
Batch 10/64 loss: -0.22769010066986084
Batch 11/64 loss: -0.1836521029472351
Batch 12/64 loss: -0.22228196263313293
Batch 13/64 loss: -0.2231360673904419
Batch 14/64 loss: -0.20557790994644165
Batch 15/64 loss: -0.2228032350540161
Batch 16/64 loss: -0.18310600519180298
Batch 17/64 loss: -0.20080161094665527
Batch 18/64 loss: -0.18967914581298828
Batch 19/64 loss: -0.21707400679588318
Batch 20/64 loss: -0.19251954555511475
Batch 21/64 loss: -0.22719836235046387
Batch 22/64 loss: -0.21189099550247192
Batch 23/64 loss: -0.1959325075149536
Batch 24/64 loss: -0.2193840742111206
Batch 25/64 loss: -0.21084511280059814
Batch 26/64 loss: -0.1810963749885559
Batch 27/64 loss: -0.2134934663772583
Batch 28/64 loss: -0.20809319615364075
Batch 29/64 loss: -0.20713862776756287
Batch 30/64 loss: -0.21275538206100464
Batch 31/64 loss: -0.19346219301223755
Batch 32/64 loss: -0.20952185988426208
Batch 33/64 loss: -0.20684093236923218
Batch 34/64 loss: -0.20295187830924988
Batch 35/64 loss: -0.21156728267669678
Batch 36/64 loss: -0.18307703733444214
Batch 37/64 loss: -0.1537477970123291
Batch 38/64 loss: -0.20339593291282654
Batch 39/64 loss: -0.20110806822776794
Batch 40/64 loss: -0.21247601509094238
Batch 41/64 loss: -0.2014840841293335
Batch 42/64 loss: -0.2088942527770996
Batch 43/64 loss: -0.2094239592552185
Batch 44/64 loss: -0.1956501007080078
Batch 45/64 loss: -0.19271212816238403
Batch 46/64 loss: -0.19145679473876953
Batch 47/64 loss: -0.20836392045021057
Batch 48/64 loss: -0.18468505144119263
Batch 49/64 loss: -0.1905970573425293
Batch 50/64 loss: -0.21313828229904175
Batch 51/64 loss: -0.16501116752624512
Batch 52/64 loss: -0.16815650463104248
Batch 53/64 loss: -0.21321722865104675
Batch 54/64 loss: -0.21990472078323364
Batch 55/64 loss: -0.19365286827087402
Batch 56/64 loss: -0.2099691927433014
Batch 57/64 loss: -0.19784927368164062
Batch 58/64 loss: -0.20188701152801514
Batch 59/64 loss: -0.1920100450515747
Batch 60/64 loss: -0.2020871341228485
Batch 61/64 loss: -0.21277734637260437
Batch 62/64 loss: -0.21113598346710205
Batch 63/64 loss: -0.215523362159729
Batch 64/64 loss: -0.21169227361679077
Epoch 453  Train loss: -0.2044140944293901  Val loss: -0.013759814586836039
Epoch 454
-------------------------------
Batch 1/64 loss: -0.21102583408355713
Batch 2/64 loss: -0.2246266007423401
Batch 3/64 loss: -0.20667517185211182
Batch 4/64 loss: -0.23450309038162231
Batch 5/64 loss: -0.20919999480247498
Batch 6/64 loss: -0.22553589940071106
Batch 7/64 loss: -0.20650190114974976
Batch 8/64 loss: -0.22010183334350586
Batch 9/64 loss: -0.21496224403381348
Batch 10/64 loss: -0.22666531801223755
Batch 11/64 loss: -0.19662722945213318
Batch 12/64 loss: -0.2220073640346527
Batch 13/64 loss: -0.23434552550315857
Batch 14/64 loss: -0.22181949019432068
Batch 15/64 loss: -0.20502746105194092
Batch 16/64 loss: -0.20486441254615784
Batch 17/64 loss: -0.19779467582702637
Batch 18/64 loss: -0.20192179083824158
Batch 19/64 loss: -0.2064133882522583
Batch 20/64 loss: -0.18353724479675293
Batch 21/64 loss: -0.19285452365875244
Batch 22/64 loss: -0.20354533195495605
Batch 23/64 loss: -0.21121394634246826
Batch 24/64 loss: -0.20722556114196777
Batch 25/64 loss: -0.1923014521598816
Batch 26/64 loss: -0.19892454147338867
Batch 27/64 loss: -0.20567819476127625
Batch 28/64 loss: -0.20955413579940796
Batch 29/64 loss: -0.21153652667999268
Batch 30/64 loss: -0.2021065652370453
Batch 31/64 loss: -0.1981232762336731
Batch 32/64 loss: -0.20773524045944214
Batch 33/64 loss: -0.20791170001029968
Batch 34/64 loss: -0.19627422094345093
Batch 35/64 loss: -0.20767143368721008
Batch 36/64 loss: -0.2181382179260254
Batch 37/64 loss: -0.21603423357009888
Batch 38/64 loss: -0.1941809058189392
Batch 39/64 loss: -0.24446779489517212
Batch 40/64 loss: -0.23633837699890137
Batch 41/64 loss: -0.22086763381958008
Batch 42/64 loss: -0.1912667155265808
Batch 43/64 loss: -0.21539732813835144
Batch 44/64 loss: -0.19361531734466553
Batch 45/64 loss: -0.22029715776443481
Batch 46/64 loss: -0.17837566137313843
Batch 47/64 loss: -0.17728304862976074
Batch 48/64 loss: -0.1878124475479126
Batch 49/64 loss: -0.20221799612045288
Batch 50/64 loss: -0.21963274478912354
Batch 51/64 loss: -0.19630491733551025
Batch 52/64 loss: -0.17458724975585938
Batch 53/64 loss: -0.21206343173980713
Batch 54/64 loss: -0.2109706699848175
Batch 55/64 loss: -0.20323532819747925
Batch 56/64 loss: -0.1685694456100464
Batch 57/64 loss: -0.2284308671951294
Batch 58/64 loss: -0.18795883655548096
Batch 59/64 loss: -0.19508832693099976
Batch 60/64 loss: -0.1994357705116272
Batch 61/64 loss: -0.22728011012077332
Batch 62/64 loss: -0.18396598100662231
Batch 63/64 loss: -0.21987822651863098
Batch 64/64 loss: -0.19141602516174316
Epoch 454  Train loss: -0.20665207657159543  Val loss: -0.012896483296790892
Epoch 455
-------------------------------
Batch 1/64 loss: -0.23297303915023804
Batch 2/64 loss: -0.22340574860572815
Batch 3/64 loss: -0.20648667216300964
Batch 4/64 loss: -0.2218148112297058
Batch 5/64 loss: -0.20369595289230347
Batch 6/64 loss: -0.18016010522842407
Batch 7/64 loss: -0.22178810834884644
Batch 8/64 loss: -0.22348874807357788
Batch 9/64 loss: -0.20512712001800537
Batch 10/64 loss: -0.17383962869644165
Batch 11/64 loss: -0.2116275429725647
Batch 12/64 loss: -0.22218069434165955
Batch 13/64 loss: -0.20952054858207703
Batch 14/64 loss: -0.16417187452316284
Batch 15/64 loss: -0.18414318561553955
Batch 16/64 loss: -0.22375309467315674
Batch 17/64 loss: -0.20395547151565552
Batch 18/64 loss: -0.20511528849601746
Batch 19/64 loss: -0.18635809421539307
Batch 20/64 loss: -0.1917521357536316
Batch 21/64 loss: -0.22302386164665222
Batch 22/64 loss: -0.22312235832214355
Batch 23/64 loss: -0.19453322887420654
Batch 24/64 loss: -0.2182040512561798
Batch 25/64 loss: -0.20527532696723938
Batch 26/64 loss: -0.21246451139450073
Batch 27/64 loss: -0.2085554003715515
Batch 28/64 loss: -0.22301840782165527
Batch 29/64 loss: -0.22001969814300537
Batch 30/64 loss: -0.20454075932502747
Batch 31/64 loss: -0.23261535167694092
Batch 32/64 loss: -0.21194660663604736
Batch 33/64 loss: -0.19337809085845947
Batch 34/64 loss: -0.17603039741516113
Batch 35/64 loss: -0.2035972774028778
Batch 36/64 loss: -0.2181064486503601
Batch 37/64 loss: -0.20051008462905884
Batch 38/64 loss: -0.21918198466300964
Batch 39/64 loss: -0.18986248970031738
Batch 40/64 loss: -0.19137388467788696
Batch 41/64 loss: -0.2181374430656433
Batch 42/64 loss: -0.21639475226402283
Batch 43/64 loss: -0.17776638269424438
Batch 44/64 loss: -0.18772369623184204
Batch 45/64 loss: -0.20772522687911987
Batch 46/64 loss: -0.19318604469299316
Batch 47/64 loss: -0.21111524105072021
Batch 48/64 loss: -0.22232893109321594
Batch 49/64 loss: -0.19165736436843872
Batch 50/64 loss: -0.2142186164855957
Batch 51/64 loss: -0.18663835525512695
Batch 52/64 loss: -0.2135797142982483
Batch 53/64 loss: -0.23418226838111877
Batch 54/64 loss: -0.2012178897857666
Batch 55/64 loss: -0.1975981593132019
Batch 56/64 loss: -0.21081674098968506
Batch 57/64 loss: -0.15127992630004883
Batch 58/64 loss: -0.1897130012512207
Batch 59/64 loss: -0.1953061819076538
Batch 60/64 loss: -0.20765146613121033
Batch 61/64 loss: -0.2166966199874878
Batch 62/64 loss: -0.21562013030052185
Batch 63/64 loss: -0.17458879947662354
Batch 64/64 loss: -0.17666620016098022
Epoch 455  Train loss: -0.20442918726042206  Val loss: -0.010416518986429955
Epoch 456
-------------------------------
Batch 1/64 loss: -0.17255932092666626
Batch 2/64 loss: -0.18642914295196533
Batch 3/64 loss: -0.18163526058197021
Batch 4/64 loss: -0.21072918176651
Batch 5/64 loss: -0.2111085057258606
Batch 6/64 loss: -0.1789025068283081
Batch 7/64 loss: -0.20799261331558228
Batch 8/64 loss: -0.20657199621200562
Batch 9/64 loss: -0.21712848544120789
Batch 10/64 loss: -0.197168231010437
Batch 11/64 loss: -0.21090996265411377
Batch 12/64 loss: -0.20154374837875366
Batch 13/64 loss: -0.207437664270401
Batch 14/64 loss: -0.19421833753585815
Batch 15/64 loss: -0.22883570194244385
Batch 16/64 loss: -0.20636940002441406
Batch 17/64 loss: -0.2069929838180542
Batch 18/64 loss: -0.2063170075416565
Batch 19/64 loss: -0.20296233892440796
Batch 20/64 loss: -0.14866000413894653
Batch 21/64 loss: -0.23395651578903198
Batch 22/64 loss: -0.19484740495681763
Batch 23/64 loss: -0.20096755027770996
Batch 24/64 loss: -0.2175350785255432
Batch 25/64 loss: -0.2135825753211975
Batch 26/64 loss: -0.19117319583892822
Batch 27/64 loss: -0.2200135588645935
Batch 28/64 loss: -0.18765264749526978
Batch 29/64 loss: -0.2143910527229309
Batch 30/64 loss: -0.20876598358154297
Batch 31/64 loss: -0.20486527681350708
Batch 32/64 loss: -0.21417367458343506
Batch 33/64 loss: -0.18788951635360718
Batch 34/64 loss: -0.18783700466156006
Batch 35/64 loss: -0.20898115634918213
Batch 36/64 loss: -0.20122963190078735
Batch 37/64 loss: -0.2046440839767456
Batch 38/64 loss: -0.17043042182922363
Batch 39/64 loss: -0.19803306460380554
Batch 40/64 loss: -0.21376442909240723
Batch 41/64 loss: -0.2083793580532074
Batch 42/64 loss: -0.2143159806728363
Batch 43/64 loss: -0.20610883831977844
Batch 44/64 loss: -0.20671290159225464
Batch 45/64 loss: -0.19881948828697205
Batch 46/64 loss: -0.18466591835021973
Batch 47/64 loss: -0.20166486501693726
Batch 48/64 loss: -0.20372909307479858
Batch 49/64 loss: -0.2149452567100525
Batch 50/64 loss: -0.19247114658355713
Batch 51/64 loss: -0.18970108032226562
Batch 52/64 loss: -0.22368156909942627
Batch 53/64 loss: -0.19671982526779175
Batch 54/64 loss: -0.20415037870407104
Batch 55/64 loss: -0.21982401609420776
Batch 56/64 loss: -0.1906050443649292
Batch 57/64 loss: -0.20031023025512695
Batch 58/64 loss: -0.22725287079811096
Batch 59/64 loss: -0.14907610416412354
Batch 60/64 loss: -0.22038960456848145
Batch 61/64 loss: -0.20781123638153076
Batch 62/64 loss: -0.18201100826263428
Batch 63/64 loss: -0.20233795046806335
Batch 64/64 loss: -0.2076128125190735
Epoch 456  Train loss: -0.20173489556593052  Val loss: -0.014037309438502257
Epoch 457
-------------------------------
Batch 1/64 loss: -0.20122280716896057
Batch 2/64 loss: -0.21808332204818726
Batch 3/64 loss: -0.21954035758972168
Batch 4/64 loss: -0.20829865336418152
Batch 5/64 loss: -0.21725371479988098
Batch 6/64 loss: -0.2029690146446228
Batch 7/64 loss: -0.22924375534057617
Batch 8/64 loss: -0.23290008306503296
Batch 9/64 loss: -0.2232511043548584
Batch 10/64 loss: -0.2127121090888977
Batch 11/64 loss: -0.21031233668327332
Batch 12/64 loss: -0.2138684093952179
Batch 13/64 loss: -0.21146172285079956
Batch 14/64 loss: -0.2262181043624878
Batch 15/64 loss: -0.2005467414855957
Batch 16/64 loss: -0.22134125232696533
Batch 17/64 loss: -0.21310174465179443
Batch 18/64 loss: -0.20129504799842834
Batch 19/64 loss: -0.2234048843383789
Batch 20/64 loss: -0.2027154266834259
Batch 21/64 loss: -0.23161429166793823
Batch 22/64 loss: -0.20632925629615784
Batch 23/64 loss: -0.22288674116134644
Batch 24/64 loss: -0.18780803680419922
Batch 25/64 loss: -0.20432764291763306
Batch 26/64 loss: -0.19583353400230408
Batch 27/64 loss: -0.2291191816329956
Batch 28/64 loss: -0.21544155478477478
Batch 29/64 loss: -0.20747464895248413
Batch 30/64 loss: -0.223580002784729
Batch 31/64 loss: -0.2234792709350586
Batch 32/64 loss: -0.2277454435825348
Batch 33/64 loss: -0.18047171831130981
Batch 34/64 loss: -0.18151402473449707
Batch 35/64 loss: -0.17793208360671997
Batch 36/64 loss: -0.1910037398338318
Batch 37/64 loss: -0.19743353128433228
Batch 38/64 loss: -0.20677626132965088
Batch 39/64 loss: -0.2044556736946106
Batch 40/64 loss: -0.21253806352615356
Batch 41/64 loss: -0.19930869340896606
Batch 42/64 loss: -0.17075884342193604
Batch 43/64 loss: -0.1566569209098816
Batch 44/64 loss: -0.2097739279270172
Batch 45/64 loss: -0.20179182291030884
Batch 46/64 loss: -0.19775134325027466
Batch 47/64 loss: -0.2124825119972229
Batch 48/64 loss: -0.20909970998764038
Batch 49/64 loss: -0.21066927909851074
Batch 50/64 loss: -0.21487709879875183
Batch 51/64 loss: -0.18626582622528076
Batch 52/64 loss: -0.20319455862045288
Batch 53/64 loss: -0.2265053689479828
Batch 54/64 loss: -0.20474770665168762
Batch 55/64 loss: -0.1857101321220398
Batch 56/64 loss: -0.1999204158782959
Batch 57/64 loss: -0.16567444801330566
Batch 58/64 loss: -0.2119075059890747
Batch 59/64 loss: -0.20499420166015625
Batch 60/64 loss: -0.186351478099823
Batch 61/64 loss: -0.2211119532585144
Batch 62/64 loss: -0.19137072563171387
Batch 63/64 loss: -0.21160376071929932
Batch 64/64 loss: -0.20669877529144287
Epoch 457  Train loss: -0.20635384484833363  Val loss: -0.014358221665280791
Epoch 458
-------------------------------
Batch 1/64 loss: -0.23059701919555664
Batch 2/64 loss: -0.21561801433563232
Batch 3/64 loss: -0.2228999137878418
Batch 4/64 loss: -0.195428729057312
Batch 5/64 loss: -0.21622005105018616
Batch 6/64 loss: -0.2123737335205078
Batch 7/64 loss: -0.19904989004135132
Batch 8/64 loss: -0.20248878002166748
Batch 9/64 loss: -0.21070465445518494
Batch 10/64 loss: -0.20143204927444458
Batch 11/64 loss: -0.20047250390052795
Batch 12/64 loss: -0.20404589176177979
Batch 13/64 loss: -0.20128971338272095
Batch 14/64 loss: -0.20987290143966675
Batch 15/64 loss: -0.21348994970321655
Batch 16/64 loss: -0.18536245822906494
Batch 17/64 loss: -0.1982201635837555
Batch 18/64 loss: -0.19967365264892578
Batch 19/64 loss: -0.21654587984085083
Batch 20/64 loss: -0.22367990016937256
Batch 21/64 loss: -0.21015781164169312
Batch 22/64 loss: -0.19458937644958496
Batch 23/64 loss: -0.21552079916000366
Batch 24/64 loss: -0.18548566102981567
Batch 25/64 loss: -0.20659410953521729
Batch 26/64 loss: -0.20244598388671875
Batch 27/64 loss: -0.21720317006111145
Batch 28/64 loss: -0.18127310276031494
Batch 29/64 loss: -0.2035599946975708
Batch 30/64 loss: -0.19922322034835815
Batch 31/64 loss: -0.21744439005851746
Batch 32/64 loss: -0.21234536170959473
Batch 33/64 loss: -0.21370530128479004
Batch 34/64 loss: -0.2165270745754242
Batch 35/64 loss: -0.19761788845062256
Batch 36/64 loss: -0.22385483980178833
Batch 37/64 loss: -0.18610578775405884
Batch 38/64 loss: -0.20662304759025574
Batch 39/64 loss: -0.17909479141235352
Batch 40/64 loss: -0.2091672122478485
Batch 41/64 loss: -0.21684598922729492
Batch 42/64 loss: -0.20567020773887634
Batch 43/64 loss: -0.2204989790916443
Batch 44/64 loss: -0.1999123990535736
Batch 45/64 loss: -0.21898648142814636
Batch 46/64 loss: -0.19505810737609863
Batch 47/64 loss: -0.21007752418518066
Batch 48/64 loss: -0.18154138326644897
Batch 49/64 loss: -0.2072480320930481
Batch 50/64 loss: -0.2022865116596222
Batch 51/64 loss: -0.19061166048049927
Batch 52/64 loss: -0.17226040363311768
Batch 53/64 loss: -0.19833683967590332
Batch 54/64 loss: -0.20255887508392334
Batch 55/64 loss: -0.20940935611724854
Batch 56/64 loss: -0.23366904258728027
Batch 57/64 loss: -0.20745983719825745
Batch 58/64 loss: -0.18313729763031006
Batch 59/64 loss: -0.21503275632858276
Batch 60/64 loss: -0.19346630573272705
Batch 61/64 loss: -0.1794501543045044
Batch 62/64 loss: -0.2113705277442932
Batch 63/64 loss: -0.20385956764221191
Batch 64/64 loss: -0.20452988147735596
Epoch 458  Train loss: -0.20470824194889442  Val loss: -0.010098399984877543
Epoch 459
-------------------------------
Batch 1/64 loss: -0.22223570942878723
Batch 2/64 loss: -0.21142852306365967
Batch 3/64 loss: -0.22361788153648376
Batch 4/64 loss: -0.1976734697818756
Batch 5/64 loss: -0.20413261651992798
Batch 6/64 loss: -0.21436357498168945
Batch 7/64 loss: -0.20177429914474487
Batch 8/64 loss: -0.21584004163742065
Batch 9/64 loss: -0.2134992480278015
Batch 10/64 loss: -0.22741717100143433
Batch 11/64 loss: -0.22544386982917786
Batch 12/64 loss: -0.20554035902023315
Batch 13/64 loss: -0.21952539682388306
Batch 14/64 loss: -0.21931684017181396
Batch 15/64 loss: -0.2057742178440094
Batch 16/64 loss: -0.18490451574325562
Batch 17/64 loss: -0.20541304349899292
Batch 18/64 loss: -0.1928768754005432
Batch 19/64 loss: -0.22848331928253174
Batch 20/64 loss: -0.17597556114196777
Batch 21/64 loss: -0.2302977442741394
Batch 22/64 loss: -0.2172369360923767
Batch 23/64 loss: -0.22508442401885986
Batch 24/64 loss: -0.21300289034843445
Batch 25/64 loss: -0.2074914276599884
Batch 26/64 loss: -0.18860721588134766
Batch 27/64 loss: -0.2216358780860901
Batch 28/64 loss: -0.19734308123588562
Batch 29/64 loss: -0.21393680572509766
Batch 30/64 loss: -0.20450180768966675
Batch 31/64 loss: -0.20949643850326538
Batch 32/64 loss: -0.22103041410446167
Batch 33/64 loss: -0.21935713291168213
Batch 34/64 loss: -0.1893584132194519
Batch 35/64 loss: -0.21945908665657043
Batch 36/64 loss: -0.226963073015213
Batch 37/64 loss: -0.20574823021888733
Batch 38/64 loss: -0.21133184432983398
Batch 39/64 loss: -0.20170068740844727
Batch 40/64 loss: -0.22851240634918213
Batch 41/64 loss: -0.19344854354858398
Batch 42/64 loss: -0.20991528034210205
Batch 43/64 loss: -0.21615800261497498
Batch 44/64 loss: -0.19387853145599365
Batch 45/64 loss: -0.19674476981163025
Batch 46/64 loss: -0.21646347641944885
Batch 47/64 loss: -0.18759769201278687
Batch 48/64 loss: -0.19763070344924927
Batch 49/64 loss: -0.21853119134902954
Batch 50/64 loss: -0.1953900158405304
Batch 51/64 loss: -0.20391350984573364
Batch 52/64 loss: -0.1883939504623413
Batch 53/64 loss: -0.23854118585586548
Batch 54/64 loss: -0.17805051803588867
Batch 55/64 loss: -0.19913509488105774
Batch 56/64 loss: -0.2067430019378662
Batch 57/64 loss: -0.2060951590538025
Batch 58/64 loss: -0.2097671627998352
Batch 59/64 loss: -0.200219988822937
Batch 60/64 loss: -0.2253795862197876
Batch 61/64 loss: -0.17733901739120483
Batch 62/64 loss: -0.2252749800682068
Batch 63/64 loss: -0.22520744800567627
Batch 64/64 loss: -0.22115623950958252
Epoch 459  Train loss: -0.2089885248857386  Val loss: -0.011655750962876781
Epoch 460
-------------------------------
Batch 1/64 loss: -0.2093736231327057
Batch 2/64 loss: -0.16666126251220703
Batch 3/64 loss: -0.216835618019104
Batch 4/64 loss: -0.2044251561164856
Batch 5/64 loss: -0.2036910057067871
Batch 6/64 loss: -0.21279990673065186
Batch 7/64 loss: -0.21293139457702637
Batch 8/64 loss: -0.20193928480148315
Batch 9/64 loss: -0.19194543361663818
Batch 10/64 loss: -0.21249154210090637
Batch 11/64 loss: -0.20446154475212097
Batch 12/64 loss: -0.2160947024822235
Batch 13/64 loss: -0.21078646183013916
Batch 14/64 loss: -0.2163952887058258
Batch 15/64 loss: -0.2141898274421692
Batch 16/64 loss: -0.21596097946166992
Batch 17/64 loss: -0.2281549572944641
Batch 18/64 loss: -0.21419119834899902
Batch 19/64 loss: -0.20417070388793945
Batch 20/64 loss: -0.2017604410648346
Batch 21/64 loss: -0.17396962642669678
Batch 22/64 loss: -0.18295812606811523
Batch 23/64 loss: -0.1754378080368042
Batch 24/64 loss: -0.21239745616912842
Batch 25/64 loss: -0.18744421005249023
Batch 26/64 loss: -0.19720149040222168
Batch 27/64 loss: -0.2231602966785431
Batch 28/64 loss: -0.21536433696746826
Batch 29/64 loss: -0.1850053071975708
Batch 30/64 loss: -0.2204711139202118
Batch 31/64 loss: -0.21778559684753418
Batch 32/64 loss: -0.19908034801483154
Batch 33/64 loss: -0.212167888879776
Batch 34/64 loss: -0.22014719247817993
Batch 35/64 loss: -0.2040938138961792
Batch 36/64 loss: -0.20175158977508545
Batch 37/64 loss: -0.20387282967567444
Batch 38/64 loss: -0.216680645942688
Batch 39/64 loss: -0.2326456904411316
Batch 40/64 loss: -0.19909125566482544
Batch 41/64 loss: -0.23060405254364014
Batch 42/64 loss: -0.1912306547164917
Batch 43/64 loss: -0.20796632766723633
Batch 44/64 loss: -0.19176411628723145
Batch 45/64 loss: -0.2172720730304718
Batch 46/64 loss: -0.2219356894493103
Batch 47/64 loss: -0.21757137775421143
Batch 48/64 loss: -0.21117404103279114
Batch 49/64 loss: -0.21477559208869934
Batch 50/64 loss: -0.1985974907875061
Batch 51/64 loss: -0.20971527695655823
Batch 52/64 loss: -0.22014951705932617
Batch 53/64 loss: -0.20884862542152405
Batch 54/64 loss: -0.20971912145614624
Batch 55/64 loss: -0.21811789274215698
Batch 56/64 loss: -0.20156094431877136
Batch 57/64 loss: -0.19279122352600098
Batch 58/64 loss: -0.18324178457260132
Batch 59/64 loss: -0.19985488057136536
Batch 60/64 loss: -0.18779760599136353
Batch 61/64 loss: -0.2174353003501892
Batch 62/64 loss: -0.18793779611587524
Batch 63/64 loss: -0.17960435152053833
Batch 64/64 loss: -0.14684265851974487
Epoch 460  Train loss: -0.2050162303681467  Val loss: -0.014510781494612546
Epoch 461
-------------------------------
Batch 1/64 loss: -0.21610617637634277
Batch 2/64 loss: -0.22590097784996033
Batch 3/64 loss: -0.21824908256530762
Batch 4/64 loss: -0.22263795137405396
Batch 5/64 loss: -0.21938133239746094
Batch 6/64 loss: -0.22485226392745972
Batch 7/64 loss: -0.23227667808532715
Batch 8/64 loss: -0.22376608848571777
Batch 9/64 loss: -0.2228085994720459
Batch 10/64 loss: -0.199171781539917
Batch 11/64 loss: -0.21545034646987915
Batch 12/64 loss: -0.2011820673942566
Batch 13/64 loss: -0.19343239068984985
Batch 14/64 loss: -0.23304390907287598
Batch 15/64 loss: -0.2289992868900299
Batch 16/64 loss: -0.2263525128364563
Batch 17/64 loss: -0.1983318328857422
Batch 18/64 loss: -0.1852673888206482
Batch 19/64 loss: -0.208906888961792
Batch 20/64 loss: -0.21801039576530457
Batch 21/64 loss: -0.20721396803855896
Batch 22/64 loss: -0.18499183654785156
Batch 23/64 loss: -0.22373270988464355
Batch 24/64 loss: -0.19467031955718994
Batch 25/64 loss: -0.19604408740997314
Batch 26/64 loss: -0.21093642711639404
Batch 27/64 loss: -0.2153903841972351
Batch 28/64 loss: -0.20229977369308472
Batch 29/64 loss: -0.20755359530448914
Batch 30/64 loss: -0.22825372219085693
Batch 31/64 loss: -0.23093008995056152
Batch 32/64 loss: -0.2207653820514679
Batch 33/64 loss: -0.19204747676849365
Batch 34/64 loss: -0.19416242837905884
Batch 35/64 loss: -0.20139676332473755
Batch 36/64 loss: -0.19261962175369263
Batch 37/64 loss: -0.20613527297973633
Batch 38/64 loss: -0.21446135640144348
Batch 39/64 loss: -0.20271539688110352
Batch 40/64 loss: -0.21848908066749573
Batch 41/64 loss: -0.22062858939170837
Batch 42/64 loss: -0.21847274899482727
Batch 43/64 loss: -0.19659209251403809
Batch 44/64 loss: -0.2132849395275116
Batch 45/64 loss: -0.20919740200042725
Batch 46/64 loss: -0.185427725315094
Batch 47/64 loss: -0.16998320817947388
Batch 48/64 loss: -0.204472154378891
Batch 49/64 loss: -0.2361658811569214
Batch 50/64 loss: -0.23066562414169312
Batch 51/64 loss: -0.22989964485168457
Batch 52/64 loss: -0.23227450251579285
Batch 53/64 loss: -0.2255467176437378
Batch 54/64 loss: -0.2187556028366089
Batch 55/64 loss: -0.17423218488693237
Batch 56/64 loss: -0.18984365463256836
Batch 57/64 loss: -0.20339137315750122
Batch 58/64 loss: -0.20411506295204163
Batch 59/64 loss: -0.20644336938858032
Batch 60/64 loss: -0.1878332495689392
Batch 61/64 loss: -0.2000126838684082
Batch 62/64 loss: -0.22054898738861084
Batch 63/64 loss: -0.2060984969139099
Batch 64/64 loss: -0.19455307722091675
Epoch 461  Train loss: -0.21001942508360918  Val loss: -0.01460817486969466
Epoch 462
-------------------------------
Batch 1/64 loss: -0.2197692096233368
Batch 2/64 loss: -0.1955762803554535
Batch 3/64 loss: -0.2192395031452179
Batch 4/64 loss: -0.2167348861694336
Batch 5/64 loss: -0.21733206510543823
Batch 6/64 loss: -0.21027040481567383
Batch 7/64 loss: -0.22056594491004944
Batch 8/64 loss: -0.19858074188232422
Batch 9/64 loss: -0.20988115668296814
Batch 10/64 loss: -0.18899601697921753
Batch 11/64 loss: -0.19299328327178955
Batch 12/64 loss: -0.20788702368736267
Batch 13/64 loss: -0.18210232257843018
Batch 14/64 loss: -0.2063518762588501
Batch 15/64 loss: -0.21271565556526184
Batch 16/64 loss: -0.21925652027130127
Batch 17/64 loss: -0.20924094319343567
Batch 18/64 loss: -0.23195922374725342
Batch 19/64 loss: -0.22532784938812256
Batch 20/64 loss: -0.23008400201797485
Batch 21/64 loss: -0.20945194363594055
Batch 22/64 loss: -0.16937041282653809
Batch 23/64 loss: -0.19789928197860718
Batch 24/64 loss: -0.21005630493164062
Batch 25/64 loss: -0.20706048607826233
Batch 26/64 loss: -0.18824613094329834
Batch 27/64 loss: -0.21643143892288208
Batch 28/64 loss: -0.22006773948669434
Batch 29/64 loss: -0.2002009153366089
Batch 30/64 loss: -0.19866037368774414
Batch 31/64 loss: -0.20592659711837769
Batch 32/64 loss: -0.17864447832107544
Batch 33/64 loss: -0.19836854934692383
Batch 34/64 loss: -0.14136183261871338
Batch 35/64 loss: -0.1987575888633728
Batch 36/64 loss: -0.22841250896453857
Batch 37/64 loss: -0.19897305965423584
Batch 38/64 loss: -0.20318597555160522
Batch 39/64 loss: -0.20796996355056763
Batch 40/64 loss: -0.17612403631210327
Batch 41/64 loss: -0.21925997734069824
Batch 42/64 loss: -0.22395408153533936
Batch 43/64 loss: -0.20881712436676025
Batch 44/64 loss: -0.20578885078430176
Batch 45/64 loss: -0.20518219470977783
Batch 46/64 loss: -0.21012401580810547
Batch 47/64 loss: -0.2027796506881714
Batch 48/64 loss: -0.20307636260986328
Batch 49/64 loss: -0.18858450651168823
Batch 50/64 loss: -0.1972351372241974
Batch 51/64 loss: -0.21830451488494873
Batch 52/64 loss: -0.18791425228118896
Batch 53/64 loss: -0.21243366599082947
Batch 54/64 loss: -0.19656416773796082
Batch 55/64 loss: -0.19360244274139404
Batch 56/64 loss: -0.21994873881340027
Batch 57/64 loss: -0.20484325289726257
Batch 58/64 loss: -0.20062994956970215
Batch 59/64 loss: -0.18231111764907837
Batch 60/64 loss: -0.20229953527450562
Batch 61/64 loss: -0.2035759687423706
Batch 62/64 loss: -0.187849760055542
Batch 63/64 loss: -0.18844544887542725
Batch 64/64 loss: -0.22393855452537537
Epoch 462  Train loss: -0.2039453044825909  Val loss: -0.012478847683909833
Epoch 463
-------------------------------
Batch 1/64 loss: -0.2021564245223999
Batch 2/64 loss: -0.21933531761169434
Batch 3/64 loss: -0.2135363519191742
Batch 4/64 loss: -0.21501576900482178
Batch 5/64 loss: -0.21818643808364868
Batch 6/64 loss: -0.2229875922203064
Batch 7/64 loss: -0.2274300456047058
Batch 8/64 loss: -0.22619032859802246
Batch 9/64 loss: -0.20434510707855225
Batch 10/64 loss: -0.20247578620910645
Batch 11/64 loss: -0.21961280703544617
Batch 12/64 loss: -0.21264129877090454
Batch 13/64 loss: -0.16667622327804565
Batch 14/64 loss: -0.21275442838668823
Batch 15/64 loss: -0.22396045923233032
Batch 16/64 loss: -0.21233701705932617
Batch 17/64 loss: -0.22363516688346863
Batch 18/64 loss: -0.21795636415481567
Batch 19/64 loss: -0.20027291774749756
Batch 20/64 loss: -0.22388437390327454
Batch 21/64 loss: -0.19073516130447388
Batch 22/64 loss: -0.192180335521698
Batch 23/64 loss: -0.18993961811065674
Batch 24/64 loss: -0.19225263595581055
Batch 25/64 loss: -0.20241400599479675
Batch 26/64 loss: -0.20971843600273132
Batch 27/64 loss: -0.20395928621292114
Batch 28/64 loss: -0.15917575359344482
Batch 29/64 loss: -0.2147560715675354
Batch 30/64 loss: -0.20427078008651733
Batch 31/64 loss: -0.18460023403167725
Batch 32/64 loss: -0.20783144235610962
Batch 33/64 loss: -0.2231142222881317
Batch 34/64 loss: -0.20143359899520874
Batch 35/64 loss: -0.1979379951953888
Batch 36/64 loss: -0.2107880413532257
Batch 37/64 loss: -0.19452399015426636
Batch 38/64 loss: -0.2040385603904724
Batch 39/64 loss: -0.20210933685302734
Batch 40/64 loss: -0.20215311646461487
Batch 41/64 loss: -0.2039620578289032
Batch 42/64 loss: -0.20718425512313843
Batch 43/64 loss: -0.2062552273273468
Batch 44/64 loss: -0.22786065936088562
Batch 45/64 loss: -0.20574432611465454
Batch 46/64 loss: -0.22117286920547485
Batch 47/64 loss: -0.22159063816070557
Batch 48/64 loss: -0.23272740840911865
Batch 49/64 loss: -0.20421749353408813
Batch 50/64 loss: -0.2178555428981781
Batch 51/64 loss: -0.2199009358882904
Batch 52/64 loss: -0.20254093408584595
Batch 53/64 loss: -0.2355368733406067
Batch 54/64 loss: -0.22387945652008057
Batch 55/64 loss: -0.2458094358444214
Batch 56/64 loss: -0.22395801544189453
Batch 57/64 loss: -0.22001546621322632
Batch 58/64 loss: -0.21892476081848145
Batch 59/64 loss: -0.20737430453300476
Batch 60/64 loss: -0.22518014907836914
Batch 61/64 loss: -0.23876523971557617
Batch 62/64 loss: -0.20947164297103882
Batch 63/64 loss: -0.2056351602077484
Batch 64/64 loss: -0.19860446453094482
Epoch 463  Train loss: -0.21063276926676433  Val loss: -0.012507321498647998
Epoch 464
-------------------------------
Batch 1/64 loss: -0.21699118614196777
Batch 2/64 loss: -0.19527220726013184
Batch 3/64 loss: -0.18669533729553223
Batch 4/64 loss: -0.2186393141746521
Batch 5/64 loss: -0.21345913410186768
Batch 6/64 loss: -0.23905974626541138
Batch 7/64 loss: -0.2022736370563507
Batch 8/64 loss: -0.20208460092544556
Batch 9/64 loss: -0.2283972203731537
Batch 10/64 loss: -0.18192076683044434
Batch 11/64 loss: -0.21802693605422974
Batch 12/64 loss: -0.2038796842098236
Batch 13/64 loss: -0.19321298599243164
Batch 14/64 loss: -0.2229064702987671
Batch 15/64 loss: -0.22039830684661865
Batch 16/64 loss: -0.20213770866394043
Batch 17/64 loss: -0.21441376209259033
Batch 18/64 loss: -0.21525585651397705
Batch 19/64 loss: -0.19635668396949768
Batch 20/64 loss: -0.20769855380058289
Batch 21/64 loss: -0.2107626497745514
Batch 22/64 loss: -0.19637304544448853
Batch 23/64 loss: -0.20790350437164307
Batch 24/64 loss: -0.22534948587417603
Batch 25/64 loss: -0.20703834295272827
Batch 26/64 loss: -0.21595144271850586
Batch 27/64 loss: -0.21606522798538208
Batch 28/64 loss: -0.21042221784591675
Batch 29/64 loss: -0.20439282059669495
Batch 30/64 loss: -0.22232839465141296
Batch 31/64 loss: -0.20485368371009827
Batch 32/64 loss: -0.21397042274475098
Batch 33/64 loss: -0.23095813393592834
Batch 34/64 loss: -0.19766899943351746
Batch 35/64 loss: -0.2146613597869873
Batch 36/64 loss: -0.22847723960876465
Batch 37/64 loss: -0.22450292110443115
Batch 38/64 loss: -0.1984233260154724
Batch 39/64 loss: -0.2300700545310974
Batch 40/64 loss: -0.20154595375061035
Batch 41/64 loss: -0.2059231996536255
Batch 42/64 loss: -0.19278550148010254
Batch 43/64 loss: -0.20344990491867065
Batch 44/64 loss: -0.21650958061218262
Batch 45/64 loss: -0.20676106214523315
Batch 46/64 loss: -0.21176937222480774
Batch 47/64 loss: -0.19473111629486084
Batch 48/64 loss: -0.1837557554244995
Batch 49/64 loss: -0.21076077222824097
Batch 50/64 loss: -0.17190659046173096
Batch 51/64 loss: -0.19314241409301758
Batch 52/64 loss: -0.19300132989883423
Batch 53/64 loss: -0.21504735946655273
Batch 54/64 loss: -0.20004254579544067
Batch 55/64 loss: -0.1771383285522461
Batch 56/64 loss: -0.20841217041015625
Batch 57/64 loss: -0.22780707478523254
Batch 58/64 loss: -0.22233760356903076
Batch 59/64 loss: -0.19441211223602295
Batch 60/64 loss: -0.21898022294044495
Batch 61/64 loss: -0.20393073558807373
Batch 62/64 loss: -0.1888880729675293
Batch 63/64 loss: -0.1970721185207367
Batch 64/64 loss: -0.15627503395080566
Epoch 464  Train loss: -0.20700502816368552  Val loss: -0.01227884317181774
Epoch 465
-------------------------------
Batch 1/64 loss: -0.2168712615966797
Batch 2/64 loss: -0.22340786457061768
Batch 3/64 loss: -0.20536130666732788
Batch 4/64 loss: -0.19883286952972412
Batch 5/64 loss: -0.21122416853904724
Batch 6/64 loss: -0.19218361377716064
Batch 7/64 loss: -0.21110057830810547
Batch 8/64 loss: -0.22654154896736145
Batch 9/64 loss: -0.18463867902755737
Batch 10/64 loss: -0.19469529390335083
Batch 11/64 loss: -0.19847577810287476
Batch 12/64 loss: -0.18998396396636963
Batch 13/64 loss: -0.21257346868515015
Batch 14/64 loss: -0.21839144825935364
Batch 15/64 loss: -0.2249668836593628
Batch 16/64 loss: -0.17268472909927368
Batch 17/64 loss: -0.18265533447265625
Batch 18/64 loss: -0.21877950429916382
Batch 19/64 loss: -0.18994933366775513
Batch 20/64 loss: -0.20237046480178833
Batch 21/64 loss: -0.19061917066574097
Batch 22/64 loss: -0.20436403155326843
Batch 23/64 loss: -0.17644965648651123
Batch 24/64 loss: -0.2102774977684021
Batch 25/64 loss: -0.20966386795043945
Batch 26/64 loss: -0.18678587675094604
Batch 27/64 loss: -0.2242826223373413
Batch 28/64 loss: -0.21599549055099487
Batch 29/64 loss: -0.16661804914474487
Batch 30/64 loss: -0.20958134531974792
Batch 31/64 loss: -0.20295840501785278
Batch 32/64 loss: -0.18467605113983154
Batch 33/64 loss: -0.23300379514694214
Batch 34/64 loss: -0.201571524143219
Batch 35/64 loss: -0.1932535171508789
Batch 36/64 loss: -0.22142517566680908
Batch 37/64 loss: -0.20921605825424194
Batch 38/64 loss: -0.22213375568389893
Batch 39/64 loss: -0.2005000114440918
Batch 40/64 loss: -0.20552527904510498
Batch 41/64 loss: -0.21068251132965088
Batch 42/64 loss: -0.19352400302886963
Batch 43/64 loss: -0.17889106273651123
Batch 44/64 loss: -0.20490097999572754
Batch 45/64 loss: -0.22315454483032227
Batch 46/64 loss: -0.1883801817893982
Batch 47/64 loss: -0.23859018087387085
Batch 48/64 loss: -0.20999449491500854
Batch 49/64 loss: -0.21691954135894775
Batch 50/64 loss: -0.2237127721309662
Batch 51/64 loss: -0.17570900917053223
Batch 52/64 loss: -0.20683449506759644
Batch 53/64 loss: -0.19993123412132263
Batch 54/64 loss: -0.211965411901474
Batch 55/64 loss: -0.21229201555252075
Batch 56/64 loss: -0.2079019844532013
Batch 57/64 loss: -0.20008134841918945
Batch 58/64 loss: -0.2029702067375183
Batch 59/64 loss: -0.20978999137878418
Batch 60/64 loss: -0.22276997566223145
Batch 61/64 loss: -0.1962793469429016
Batch 62/64 loss: -0.20042157173156738
Batch 63/64 loss: -0.22731176018714905
Batch 64/64 loss: -0.1963140368461609
Epoch 465  Train loss: -0.20478170081680896  Val loss: -0.015766097098281702
Epoch 466
-------------------------------
Batch 1/64 loss: -0.20916405320167542
Batch 2/64 loss: -0.20829585194587708
Batch 3/64 loss: -0.2006239891052246
Batch 4/64 loss: -0.2193731665611267
Batch 5/64 loss: -0.21022000908851624
Batch 6/64 loss: -0.22765135765075684
Batch 7/64 loss: -0.21915966272354126
Batch 8/64 loss: -0.21797877550125122
Batch 9/64 loss: -0.17437529563903809
Batch 10/64 loss: -0.20597338676452637
Batch 11/64 loss: -0.21201875805854797
Batch 12/64 loss: -0.23732304573059082
Batch 13/64 loss: -0.19527462124824524
Batch 14/64 loss: -0.21467402577400208
Batch 15/64 loss: -0.22251451015472412
Batch 16/64 loss: -0.21014797687530518
Batch 17/64 loss: -0.2200389802455902
Batch 18/64 loss: -0.19460797309875488
Batch 19/64 loss: -0.21299642324447632
Batch 20/64 loss: -0.21690982580184937
Batch 21/64 loss: -0.19043231010437012
Batch 22/64 loss: -0.2007172703742981
Batch 23/64 loss: -0.22745901346206665
Batch 24/64 loss: -0.21281525492668152
Batch 25/64 loss: -0.20556175708770752
Batch 26/64 loss: -0.19933131337165833
Batch 27/64 loss: -0.2115321159362793
Batch 28/64 loss: -0.2188606560230255
Batch 29/64 loss: -0.22011709213256836
Batch 30/64 loss: -0.2162429690361023
Batch 31/64 loss: -0.18309175968170166
Batch 32/64 loss: -0.19852131605148315
Batch 33/64 loss: -0.2093433439731598
Batch 34/64 loss: -0.20291543006896973
Batch 35/64 loss: -0.2175247073173523
Batch 36/64 loss: -0.17503786087036133
Batch 37/64 loss: -0.20080921053886414
Batch 38/64 loss: -0.20705923438072205
Batch 39/64 loss: -0.18990027904510498
Batch 40/64 loss: -0.20631620287895203
Batch 41/64 loss: -0.21918201446533203
Batch 42/64 loss: -0.2050221860408783
Batch 43/64 loss: -0.1911473274230957
Batch 44/64 loss: -0.20593664050102234
Batch 45/64 loss: -0.18943393230438232
Batch 46/64 loss: -0.16256749629974365
Batch 47/64 loss: -0.21010741591453552
Batch 48/64 loss: -0.21169108152389526
Batch 49/64 loss: -0.19518494606018066
Batch 50/64 loss: -0.17807763814926147
Batch 51/64 loss: -0.22023889422416687
Batch 52/64 loss: -0.20086541771888733
Batch 53/64 loss: -0.2255576252937317
Batch 54/64 loss: -0.21339824795722961
Batch 55/64 loss: -0.1942814290523529
Batch 56/64 loss: -0.20689940452575684
Batch 57/64 loss: -0.2255115509033203
Batch 58/64 loss: -0.21766889095306396
Batch 59/64 loss: -0.21758806705474854
Batch 60/64 loss: -0.21400728821754456
Batch 61/64 loss: -0.17819255590438843
Batch 62/64 loss: -0.20061010122299194
Batch 63/64 loss: -0.2236230969429016
Batch 64/64 loss: -0.20381379127502441
Epoch 466  Train loss: -0.2067853235730938  Val loss: -0.013437097834557602
Epoch 467
-------------------------------
Batch 1/64 loss: -0.20499980449676514
Batch 2/64 loss: -0.21211686730384827
Batch 3/64 loss: -0.20287299156188965
Batch 4/64 loss: -0.21708053350448608
Batch 5/64 loss: -0.19999676942825317
Batch 6/64 loss: -0.19905510544776917
Batch 7/64 loss: -0.21100479364395142
Batch 8/64 loss: -0.21899721026420593
Batch 9/64 loss: -0.21038377285003662
Batch 10/64 loss: -0.21679547429084778
Batch 11/64 loss: -0.2258204221725464
Batch 12/64 loss: -0.21243822574615479
Batch 13/64 loss: -0.18616127967834473
Batch 14/64 loss: -0.20979797840118408
Batch 15/64 loss: -0.19825762510299683
Batch 16/64 loss: -0.22662761807441711
Batch 17/64 loss: -0.2178494930267334
Batch 18/64 loss: -0.22443842887878418
Batch 19/64 loss: -0.22517657279968262
Batch 20/64 loss: -0.22512298822402954
Batch 21/64 loss: -0.2020077109336853
Batch 22/64 loss: -0.1958833634853363
Batch 23/64 loss: -0.21722495555877686
Batch 24/64 loss: -0.21439111232757568
Batch 25/64 loss: -0.20044606924057007
Batch 26/64 loss: -0.18339449167251587
Batch 27/64 loss: -0.21967405080795288
Batch 28/64 loss: -0.18955904245376587
Batch 29/64 loss: -0.2078160047531128
Batch 30/64 loss: -0.22531336545944214
Batch 31/64 loss: -0.22499117255210876
Batch 32/64 loss: -0.1870439648628235
Batch 33/64 loss: -0.22917520999908447
Batch 34/64 loss: -0.22671687602996826
Batch 35/64 loss: -0.19623267650604248
Batch 36/64 loss: -0.22017228603363037
Batch 37/64 loss: -0.2144032120704651
Batch 38/64 loss: -0.2127675712108612
Batch 39/64 loss: -0.2075856328010559
Batch 40/64 loss: -0.22022703289985657
Batch 41/64 loss: -0.2049407660961151
Batch 42/64 loss: -0.17394179105758667
Batch 43/64 loss: -0.21321463584899902
Batch 44/64 loss: -0.22342219948768616
Batch 45/64 loss: -0.20467877388000488
Batch 46/64 loss: -0.2161494791507721
Batch 47/64 loss: -0.21491193771362305
Batch 48/64 loss: -0.19472920894622803
Batch 49/64 loss: -0.21363526582717896
Batch 50/64 loss: -0.20004624128341675
Batch 51/64 loss: -0.22093427181243896
Batch 52/64 loss: -0.19639569520950317
Batch 53/64 loss: -0.18246394395828247
Batch 54/64 loss: -0.1978655457496643
Batch 55/64 loss: -0.19816285371780396
Batch 56/64 loss: -0.1983025074005127
Batch 57/64 loss: -0.18709397315979004
Batch 58/64 loss: -0.21097975969314575
Batch 59/64 loss: -0.21238937973976135
Batch 60/64 loss: -0.21909189224243164
Batch 61/64 loss: -0.21033519506454468
Batch 62/64 loss: -0.20488137006759644
Batch 63/64 loss: -0.18964147567749023
Batch 64/64 loss: -0.2061958909034729
Epoch 467  Train loss: -0.2083588209806704  Val loss: -0.015965946761193554
Epoch 468
-------------------------------
Batch 1/64 loss: -0.17488348484039307
Batch 2/64 loss: -0.21431991457939148
Batch 3/64 loss: -0.2311391532421112
Batch 4/64 loss: -0.22907590866088867
Batch 5/64 loss: -0.1982882022857666
Batch 6/64 loss: -0.22219407558441162
Batch 7/64 loss: -0.18263846635818481
Batch 8/64 loss: -0.20713192224502563
Batch 9/64 loss: -0.21185976266860962
Batch 10/64 loss: -0.20842459797859192
Batch 11/64 loss: -0.219180166721344
Batch 12/64 loss: -0.22559836506843567
Batch 13/64 loss: -0.20194053649902344
Batch 14/64 loss: -0.20176029205322266
Batch 15/64 loss: -0.20431160926818848
Batch 16/64 loss: -0.23491007089614868
Batch 17/64 loss: -0.21509462594985962
Batch 18/64 loss: -0.2159116566181183
Batch 19/64 loss: -0.22979015111923218
Batch 20/64 loss: -0.1956731081008911
Batch 21/64 loss: -0.21943503618240356
Batch 22/64 loss: -0.22416138648986816
Batch 23/64 loss: -0.17823821306228638
Batch 24/64 loss: -0.2129703164100647
Batch 25/64 loss: -0.19143348932266235
Batch 26/64 loss: -0.203293114900589
Batch 27/64 loss: -0.20990139245986938
Batch 28/64 loss: -0.22537022829055786
Batch 29/64 loss: -0.23173558712005615
Batch 30/64 loss: -0.18745875358581543
Batch 31/64 loss: -0.212541401386261
Batch 32/64 loss: -0.23128491640090942
Batch 33/64 loss: -0.21245691180229187
Batch 34/64 loss: -0.21523883938789368
Batch 35/64 loss: -0.21821022033691406
Batch 36/64 loss: -0.21533408761024475
Batch 37/64 loss: -0.22240427136421204
Batch 38/64 loss: -0.2264918088912964
Batch 39/64 loss: -0.22153633832931519
Batch 40/64 loss: -0.2193017601966858
Batch 41/64 loss: -0.19215333461761475
Batch 42/64 loss: -0.22135543823242188
Batch 43/64 loss: -0.2127668857574463
Batch 44/64 loss: -0.2137230634689331
Batch 45/64 loss: -0.21442902088165283
Batch 46/64 loss: -0.20895510911941528
Batch 47/64 loss: -0.22945812344551086
Batch 48/64 loss: -0.20730799436569214
Batch 49/64 loss: -0.2227148413658142
Batch 50/64 loss: -0.16167277097702026
Batch 51/64 loss: -0.2183464765548706
Batch 52/64 loss: -0.23455512523651123
Batch 53/64 loss: -0.19608283042907715
Batch 54/64 loss: -0.1939564347267151
Batch 55/64 loss: -0.2210039496421814
Batch 56/64 loss: -0.21923372149467468
Batch 57/64 loss: -0.19575393199920654
Batch 58/64 loss: -0.21403184533119202
Batch 59/64 loss: -0.1689443588256836
Batch 60/64 loss: -0.19961842894554138
Batch 61/64 loss: -0.20234090089797974
Batch 62/64 loss: -0.20345759391784668
Batch 63/64 loss: -0.2190801501274109
Batch 64/64 loss: -0.19974088668823242
Epoch 468  Train loss: -0.2105674061120725  Val loss: -0.014715186099416203
Epoch 469
-------------------------------
Batch 1/64 loss: -0.21508300304412842
Batch 2/64 loss: -0.218755841255188
Batch 3/64 loss: -0.20429378747940063
Batch 4/64 loss: -0.2108854055404663
Batch 5/64 loss: -0.17557764053344727
Batch 6/64 loss: -0.20478984713554382
Batch 7/64 loss: -0.19240331649780273
Batch 8/64 loss: -0.2177506387233734
Batch 9/64 loss: -0.19299599528312683
Batch 10/64 loss: -0.2330535650253296
Batch 11/64 loss: -0.1849905252456665
Batch 12/64 loss: -0.2206445336341858
Batch 13/64 loss: -0.19474589824676514
Batch 14/64 loss: -0.2006112039089203
Batch 15/64 loss: -0.22948312759399414
Batch 16/64 loss: -0.24832531809806824
Batch 17/64 loss: -0.23444491624832153
Batch 18/64 loss: -0.23167377710342407
Batch 19/64 loss: -0.2144414484500885
Batch 20/64 loss: -0.23204496502876282
Batch 21/64 loss: -0.21950989961624146
Batch 22/64 loss: -0.190782368183136
Batch 23/64 loss: -0.2000897228717804
Batch 24/64 loss: -0.1991496980190277
Batch 25/64 loss: -0.23550096154212952
Batch 26/64 loss: -0.2502096891403198
Batch 27/64 loss: -0.23242536187171936
Batch 28/64 loss: -0.21105438470840454
Batch 29/64 loss: -0.19507598876953125
Batch 30/64 loss: -0.20010513067245483
Batch 31/64 loss: -0.2212693691253662
Batch 32/64 loss: -0.20752251148223877
Batch 33/64 loss: -0.20818504691123962
Batch 34/64 loss: -0.1591547727584839
Batch 35/64 loss: -0.2162812054157257
Batch 36/64 loss: -0.19370746612548828
Batch 37/64 loss: -0.2166798710823059
Batch 38/64 loss: -0.22021901607513428
Batch 39/64 loss: -0.21393531560897827
Batch 40/64 loss: -0.22546029090881348
Batch 41/64 loss: -0.21373987197875977
Batch 42/64 loss: -0.21231460571289062
Batch 43/64 loss: -0.20623928308486938
Batch 44/64 loss: -0.21837449073791504
Batch 45/64 loss: -0.20701336860656738
Batch 46/64 loss: -0.2202543020248413
Batch 47/64 loss: -0.19411909580230713
Batch 48/64 loss: -0.22345277667045593
Batch 49/64 loss: -0.22260737419128418
Batch 50/64 loss: -0.20735448598861694
Batch 51/64 loss: -0.2135986089706421
Batch 52/64 loss: -0.2182900607585907
Batch 53/64 loss: -0.2018766701221466
Batch 54/64 loss: -0.22218221426010132
Batch 55/64 loss: -0.21410638093948364
Batch 56/64 loss: -0.220533549785614
Batch 57/64 loss: -0.21060985326766968
Batch 58/64 loss: -0.22294670343399048
Batch 59/64 loss: -0.22636359930038452
Batch 60/64 loss: -0.2002233862876892
Batch 61/64 loss: -0.19612112641334534
Batch 62/64 loss: -0.21372801065444946
Batch 63/64 loss: -0.20400205254554749
Batch 64/64 loss: -0.18539130687713623
Epoch 469  Train loss: -0.21180248120251824  Val loss: -0.011807418156325612
Epoch 470
-------------------------------
Batch 1/64 loss: -0.19873744249343872
Batch 2/64 loss: -0.18785107135772705
Batch 3/64 loss: -0.21855300664901733
Batch 4/64 loss: -0.23332944512367249
Batch 5/64 loss: -0.21961289644241333
Batch 6/64 loss: -0.20058095455169678
Batch 7/64 loss: -0.18923413753509521
Batch 8/64 loss: -0.22068658471107483
Batch 9/64 loss: -0.22358644008636475
Batch 10/64 loss: -0.21368622779846191
Batch 11/64 loss: -0.22248345613479614
Batch 12/64 loss: -0.20699650049209595
Batch 13/64 loss: -0.2205733060836792
Batch 14/64 loss: -0.20940262079238892
Batch 15/64 loss: -0.1981094479560852
Batch 16/64 loss: -0.2066475749015808
Batch 17/64 loss: -0.2000531554222107
Batch 18/64 loss: -0.19108355045318604
Batch 19/64 loss: -0.18059420585632324
Batch 20/64 loss: -0.1933964490890503
Batch 21/64 loss: -0.22110289335250854
Batch 22/64 loss: -0.21812790632247925
Batch 23/64 loss: -0.21996456384658813
Batch 24/64 loss: -0.1999167799949646
Batch 25/64 loss: -0.21222826838493347
Batch 26/64 loss: -0.2288651168346405
Batch 27/64 loss: -0.22071045637130737
Batch 28/64 loss: -0.21359041333198547
Batch 29/64 loss: -0.21433711051940918
Batch 30/64 loss: -0.20622378587722778
Batch 31/64 loss: -0.1928974986076355
Batch 32/64 loss: -0.1815643310546875
Batch 33/64 loss: -0.21889954805374146
Batch 34/64 loss: -0.2325046956539154
Batch 35/64 loss: -0.2326953411102295
Batch 36/64 loss: -0.19102048873901367
Batch 37/64 loss: -0.224051833152771
Batch 38/64 loss: -0.2052258849143982
Batch 39/64 loss: -0.22263580560684204
Batch 40/64 loss: -0.19312018156051636
Batch 41/64 loss: -0.23092341423034668
Batch 42/64 loss: -0.221614271402359
Batch 43/64 loss: -0.20806884765625
Batch 44/64 loss: -0.2304099202156067
Batch 45/64 loss: -0.1771877408027649
Batch 46/64 loss: -0.17830586433410645
Batch 47/64 loss: -0.21148788928985596
Batch 48/64 loss: -0.2076510787010193
Batch 49/64 loss: -0.1910330057144165
Batch 50/64 loss: -0.20386099815368652
Batch 51/64 loss: -0.1908239722251892
Batch 52/64 loss: -0.2180810570716858
Batch 53/64 loss: -0.20594260096549988
Batch 54/64 loss: -0.21468329429626465
Batch 55/64 loss: -0.1930416226387024
Batch 56/64 loss: -0.18896877765655518
Batch 57/64 loss: -0.20621368288993835
Batch 58/64 loss: -0.20719197392463684
Batch 59/64 loss: -0.1954309642314911
Batch 60/64 loss: -0.13760554790496826
Batch 61/64 loss: -0.19116008281707764
Batch 62/64 loss: -0.1888561248779297
Batch 63/64 loss: -0.17929905652999878
Batch 64/64 loss: -0.16340845823287964
Epoch 470  Train loss: -0.2052592864223555  Val loss: -0.012796766569524286
Epoch 471
-------------------------------
Batch 1/64 loss: -0.19044697284698486
Batch 2/64 loss: -0.2099463939666748
Batch 3/64 loss: -0.2024654746055603
Batch 4/64 loss: -0.19048339128494263
Batch 5/64 loss: -0.18969082832336426
Batch 6/64 loss: -0.21464425325393677
Batch 7/64 loss: -0.20416873693466187
Batch 8/64 loss: -0.20502972602844238
Batch 9/64 loss: -0.2117869257926941
Batch 10/64 loss: -0.19629743695259094
Batch 11/64 loss: -0.214408278465271
Batch 12/64 loss: -0.20587271451950073
Batch 13/64 loss: -0.19520199298858643
Batch 14/64 loss: -0.1930292844772339
Batch 15/64 loss: -0.20081263780593872
Batch 16/64 loss: -0.21147632598876953
Batch 17/64 loss: -0.2008652687072754
Batch 18/64 loss: -0.19458633661270142
Batch 19/64 loss: -0.2228696644306183
Batch 20/64 loss: -0.19100195169448853
Batch 21/64 loss: -0.21354061365127563
Batch 22/64 loss: -0.21431457996368408
Batch 23/64 loss: -0.22735828161239624
Batch 24/64 loss: -0.2179245948791504
Batch 25/64 loss: -0.20292863249778748
Batch 26/64 loss: -0.21382084488868713
Batch 27/64 loss: -0.19591671228408813
Batch 28/64 loss: -0.20693272352218628
Batch 29/64 loss: -0.20636868476867676
Batch 30/64 loss: -0.2323288917541504
Batch 31/64 loss: -0.17325812578201294
Batch 32/64 loss: -0.18124502897262573
Batch 33/64 loss: -0.2176273763179779
Batch 34/64 loss: -0.22001171112060547
Batch 35/64 loss: -0.23232313990592957
Batch 36/64 loss: -0.18263036012649536
Batch 37/64 loss: -0.2455241084098816
Batch 38/64 loss: -0.2368563413619995
Batch 39/64 loss: -0.212563157081604
Batch 40/64 loss: -0.22509855031967163
Batch 41/64 loss: -0.2114415168762207
Batch 42/64 loss: -0.2288762629032135
Batch 43/64 loss: -0.239088237285614
Batch 44/64 loss: -0.18812263011932373
Batch 45/64 loss: -0.2321518063545227
Batch 46/64 loss: -0.1973474621772766
Batch 47/64 loss: -0.18283700942993164
Batch 48/64 loss: -0.2159411907196045
Batch 49/64 loss: -0.19939416646957397
Batch 50/64 loss: -0.2003636658191681
Batch 51/64 loss: -0.2358793318271637
Batch 52/64 loss: -0.19723176956176758
Batch 53/64 loss: -0.2336379587650299
Batch 54/64 loss: -0.22076380252838135
Batch 55/64 loss: -0.24100413918495178
Batch 56/64 loss: -0.19839826226234436
Batch 57/64 loss: -0.20494991540908813
Batch 58/64 loss: -0.22363758087158203
Batch 59/64 loss: -0.22574859857559204
Batch 60/64 loss: -0.21985822916030884
Batch 61/64 loss: -0.245376318693161
Batch 62/64 loss: -0.22451692819595337
Batch 63/64 loss: -0.21614322066307068
Batch 64/64 loss: -0.20618101954460144
Epoch 471  Train loss: -0.21083925997509675  Val loss: -0.012974536500845579
Epoch 472
-------------------------------
Batch 1/64 loss: -0.21924057602882385
Batch 2/64 loss: -0.2287500500679016
Batch 3/64 loss: -0.23193064332008362
Batch 4/64 loss: -0.24240320920944214
Batch 5/64 loss: -0.21965396404266357
Batch 6/64 loss: -0.20920896530151367
Batch 7/64 loss: -0.22661176323890686
Batch 8/64 loss: -0.21930769085884094
Batch 9/64 loss: -0.2360628843307495
Batch 10/64 loss: -0.2278880476951599
Batch 11/64 loss: -0.21724027395248413
Batch 12/64 loss: -0.2141118049621582
Batch 13/64 loss: -0.20633429288864136
Batch 14/64 loss: -0.2326756715774536
Batch 15/64 loss: -0.19731605052947998
Batch 16/64 loss: -0.20476901531219482
Batch 17/64 loss: -0.21579235792160034
Batch 18/64 loss: -0.19164955615997314
Batch 19/64 loss: -0.20229029655456543
Batch 20/64 loss: -0.22165122628211975
Batch 21/64 loss: -0.2196541428565979
Batch 22/64 loss: -0.21389466524124146
Batch 23/64 loss: -0.18229413032531738
Batch 24/64 loss: -0.19162309169769287
Batch 25/64 loss: -0.22283238172531128
Batch 26/64 loss: -0.19985485076904297
Batch 27/64 loss: -0.2041131854057312
Batch 28/64 loss: -0.1959628462791443
Batch 29/64 loss: -0.18805629014968872
Batch 30/64 loss: -0.19026827812194824
Batch 31/64 loss: -0.2043740153312683
Batch 32/64 loss: -0.19430428743362427
Batch 33/64 loss: -0.21023592352867126
Batch 34/64 loss: -0.195541650056839
Batch 35/64 loss: -0.21610146760940552
Batch 36/64 loss: -0.239715576171875
Batch 37/64 loss: -0.18965280055999756
Batch 38/64 loss: -0.20876175165176392
Batch 39/64 loss: -0.20641213655471802
Batch 40/64 loss: -0.21327024698257446
Batch 41/64 loss: -0.2081400752067566
Batch 42/64 loss: -0.22838866710662842
Batch 43/64 loss: -0.19240903854370117
Batch 44/64 loss: -0.21430152654647827
Batch 45/64 loss: -0.188301682472229
Batch 46/64 loss: -0.1964503526687622
Batch 47/64 loss: -0.2082994282245636
Batch 48/64 loss: -0.20078301429748535
Batch 49/64 loss: -0.20639687776565552
Batch 50/64 loss: -0.2127056121826172
Batch 51/64 loss: -0.20604389905929565
Batch 52/64 loss: -0.1937374472618103
Batch 53/64 loss: -0.20609432458877563
Batch 54/64 loss: -0.20375260710716248
Batch 55/64 loss: -0.21237796545028687
Batch 56/64 loss: -0.20144611597061157
Batch 57/64 loss: -0.17067909240722656
Batch 58/64 loss: -0.21113717555999756
Batch 59/64 loss: -0.20670080184936523
Batch 60/64 loss: -0.16989904642105103
Batch 61/64 loss: -0.216875821352005
Batch 62/64 loss: -0.22231942415237427
Batch 63/64 loss: -0.21671044826507568
Batch 64/64 loss: -0.22647106647491455
Epoch 472  Train loss: -0.20887240475299312  Val loss: -0.012871581459373133
Epoch 473
-------------------------------
Batch 1/64 loss: -0.21791619062423706
Batch 2/64 loss: -0.22779878973960876
Batch 3/64 loss: -0.16909325122833252
Batch 4/64 loss: -0.21321746706962585
Batch 5/64 loss: -0.2218320369720459
Batch 6/64 loss: -0.21899187564849854
Batch 7/64 loss: -0.20956921577453613
Batch 8/64 loss: -0.21612167358398438
Batch 9/64 loss: -0.23239797353744507
Batch 10/64 loss: -0.20979398488998413
Batch 11/64 loss: -0.17460787296295166
Batch 12/64 loss: -0.21379908919334412
Batch 13/64 loss: -0.21258023381233215
Batch 14/64 loss: -0.22738796472549438
Batch 15/64 loss: -0.21469169855117798
Batch 16/64 loss: -0.22566747665405273
Batch 17/64 loss: -0.19195765256881714
Batch 18/64 loss: -0.20481282472610474
Batch 19/64 loss: -0.2158539593219757
Batch 20/64 loss: -0.204327791929245
Batch 21/64 loss: -0.21185815334320068
Batch 22/64 loss: -0.21212297677993774
Batch 23/64 loss: -0.22300255298614502
Batch 24/64 loss: -0.19893908500671387
Batch 25/64 loss: -0.20505231618881226
Batch 26/64 loss: -0.19262570142745972
Batch 27/64 loss: -0.20173457264900208
Batch 28/64 loss: -0.21215635538101196
Batch 29/64 loss: -0.18072509765625
Batch 30/64 loss: -0.15364885330200195
Batch 31/64 loss: -0.19798681139945984
Batch 32/64 loss: -0.20330774784088135
Batch 33/64 loss: -0.21014449000358582
Batch 34/64 loss: -0.1847306489944458
Batch 35/64 loss: -0.21982428431510925
Batch 36/64 loss: -0.21217072010040283
Batch 37/64 loss: -0.190942645072937
Batch 38/64 loss: -0.19040131568908691
Batch 39/64 loss: -0.21687978506088257
Batch 40/64 loss: -0.21677786111831665
Batch 41/64 loss: -0.2249217927455902
Batch 42/64 loss: -0.17902851104736328
Batch 43/64 loss: -0.20651501417160034
Batch 44/64 loss: -0.2240040898323059
Batch 45/64 loss: -0.22263091802597046
Batch 46/64 loss: -0.21699583530426025
Batch 47/64 loss: -0.1971106231212616
Batch 48/64 loss: -0.18792003393173218
Batch 49/64 loss: -0.19260942935943604
Batch 50/64 loss: -0.21884769201278687
Batch 51/64 loss: -0.20410287380218506
Batch 52/64 loss: -0.22468191385269165
Batch 53/64 loss: -0.22683724761009216
Batch 54/64 loss: -0.21455317735671997
Batch 55/64 loss: -0.2397238314151764
Batch 56/64 loss: -0.23027163743972778
Batch 57/64 loss: -0.2176727056503296
Batch 58/64 loss: -0.2184012532234192
Batch 59/64 loss: -0.22310298681259155
Batch 60/64 loss: -0.18693053722381592
Batch 61/64 loss: -0.19809353351593018
Batch 62/64 loss: -0.22856208682060242
Batch 63/64 loss: -0.23394650220870972
Batch 64/64 loss: -0.21081653237342834
Epoch 473  Train loss: -0.2091455313504911  Val loss: -0.015399102083186513
Epoch 474
-------------------------------
Batch 1/64 loss: -0.22350919246673584
Batch 2/64 loss: -0.19126492738723755
Batch 3/64 loss: -0.1921810507774353
Batch 4/64 loss: -0.21325328946113586
Batch 5/64 loss: -0.21254968643188477
Batch 6/64 loss: -0.21081089973449707
Batch 7/64 loss: -0.20590078830718994
Batch 8/64 loss: -0.2110271453857422
Batch 9/64 loss: -0.23261216282844543
Batch 10/64 loss: -0.21315765380859375
Batch 11/64 loss: -0.2096714973449707
Batch 12/64 loss: -0.21351358294487
Batch 13/64 loss: -0.22910353541374207
Batch 14/64 loss: -0.2223435640335083
Batch 15/64 loss: -0.2276538610458374
Batch 16/64 loss: -0.21614879369735718
Batch 17/64 loss: -0.20228484272956848
Batch 18/64 loss: -0.2226715087890625
Batch 19/64 loss: -0.23127451539039612
Batch 20/64 loss: -0.20131221413612366
Batch 21/64 loss: -0.2147272825241089
Batch 22/64 loss: -0.22990310192108154
Batch 23/64 loss: -0.20617786049842834
Batch 24/64 loss: -0.2030317783355713
Batch 25/64 loss: -0.21140021085739136
Batch 26/64 loss: -0.19920513033866882
Batch 27/64 loss: -0.22615492343902588
Batch 28/64 loss: -0.23576492071151733
Batch 29/64 loss: -0.23537087440490723
Batch 30/64 loss: -0.2498786449432373
Batch 31/64 loss: -0.20561379194259644
Batch 32/64 loss: -0.1995241641998291
Batch 33/64 loss: -0.2095068395137787
Batch 34/64 loss: -0.23483595252037048
Batch 35/64 loss: -0.21572920680046082
Batch 36/64 loss: -0.17346113920211792
Batch 37/64 loss: -0.21440264582633972
Batch 38/64 loss: -0.1942230463027954
Batch 39/64 loss: -0.21623441576957703
Batch 40/64 loss: -0.19593513011932373
Batch 41/64 loss: -0.17503905296325684
Batch 42/64 loss: -0.19549652934074402
Batch 43/64 loss: -0.22059613466262817
Batch 44/64 loss: -0.22797289490699768
Batch 45/64 loss: -0.23138362169265747
Batch 46/64 loss: -0.21957823634147644
Batch 47/64 loss: -0.1857154369354248
Batch 48/64 loss: -0.22089186310768127
Batch 49/64 loss: -0.18229317665100098
Batch 50/64 loss: -0.19939327239990234
Batch 51/64 loss: -0.21221157908439636
Batch 52/64 loss: -0.2196020483970642
Batch 53/64 loss: -0.18466532230377197
Batch 54/64 loss: -0.2197149097919464
Batch 55/64 loss: -0.18402457237243652
Batch 56/64 loss: -0.2264922857284546
Batch 57/64 loss: -0.20087414979934692
Batch 58/64 loss: -0.2065333127975464
Batch 59/64 loss: -0.21841689944267273
Batch 60/64 loss: -0.2253170609474182
Batch 61/64 loss: -0.21425849199295044
Batch 62/64 loss: -0.2042458951473236
Batch 63/64 loss: -0.20295041799545288
Batch 64/64 loss: -0.22351449728012085
Epoch 474  Train loss: -0.2117431970203624  Val loss: -0.014034297867738913
Epoch 475
-------------------------------
Batch 1/64 loss: -0.23481568694114685
Batch 2/64 loss: -0.22548598051071167
Batch 3/64 loss: -0.2273126244544983
Batch 4/64 loss: -0.188720703125
Batch 5/64 loss: -0.20664280652999878
Batch 6/64 loss: -0.23137140274047852
Batch 7/64 loss: -0.2287704050540924
Batch 8/64 loss: -0.20493263006210327
Batch 9/64 loss: -0.22705954313278198
Batch 10/64 loss: -0.23446914553642273
Batch 11/64 loss: -0.213614821434021
Batch 12/64 loss: -0.2385098934173584
Batch 13/64 loss: -0.22056972980499268
Batch 14/64 loss: -0.18313002586364746
Batch 15/64 loss: -0.22661924362182617
Batch 16/64 loss: -0.21314895153045654
Batch 17/64 loss: -0.22441786527633667
Batch 18/64 loss: -0.20883995294570923
Batch 19/64 loss: -0.21325534582138062
Batch 20/64 loss: -0.19272339344024658
Batch 21/64 loss: -0.1938301920890808
Batch 22/64 loss: -0.2252076268196106
Batch 23/64 loss: -0.22026407718658447
Batch 24/64 loss: -0.22356724739074707
Batch 25/64 loss: -0.22106319665908813
Batch 26/64 loss: -0.23135140538215637
Batch 27/64 loss: -0.2161988914012909
Batch 28/64 loss: -0.2178092896938324
Batch 29/64 loss: -0.20749148726463318
Batch 30/64 loss: -0.23267820477485657
Batch 31/64 loss: -0.2130623757839203
Batch 32/64 loss: -0.23680859804153442
Batch 33/64 loss: -0.19857299327850342
Batch 34/64 loss: -0.22008907794952393
Batch 35/64 loss: -0.2183535397052765
Batch 36/64 loss: -0.20868048071861267
Batch 37/64 loss: -0.2208406925201416
Batch 38/64 loss: -0.23919567465782166
Batch 39/64 loss: -0.22100824117660522
Batch 40/64 loss: -0.22110694646835327
Batch 41/64 loss: -0.21226757764816284
Batch 42/64 loss: -0.22411566972732544
Batch 43/64 loss: -0.16624993085861206
Batch 44/64 loss: -0.20739713311195374
Batch 45/64 loss: -0.18823230266571045
Batch 46/64 loss: -0.19331878423690796
Batch 47/64 loss: -0.22143244743347168
Batch 48/64 loss: -0.21237194538116455
Batch 49/64 loss: -0.21760249137878418
Batch 50/64 loss: -0.21722495555877686
Batch 51/64 loss: -0.1989721953868866
Batch 52/64 loss: -0.22025185823440552
Batch 53/64 loss: -0.20762091875076294
Batch 54/64 loss: -0.2184705138206482
Batch 55/64 loss: -0.21506232023239136
Batch 56/64 loss: -0.19373857975006104
Batch 57/64 loss: -0.21951261162757874
Batch 58/64 loss: -0.18535315990447998
Batch 59/64 loss: -0.20298802852630615
Batch 60/64 loss: -0.1986517310142517
Batch 61/64 loss: -0.22779160737991333
Batch 62/64 loss: -0.2168155014514923
Batch 63/64 loss: -0.2062908411026001
Batch 64/64 loss: -0.21039554476737976
Epoch 475  Train loss: -0.21429208084648732  Val loss: -0.010819279655967792
Epoch 476
-------------------------------
Batch 1/64 loss: -0.2322411835193634
Batch 2/64 loss: -0.19916534423828125
Batch 3/64 loss: -0.21137702465057373
Batch 4/64 loss: -0.21755951642990112
Batch 5/64 loss: -0.20784425735473633
Batch 6/64 loss: -0.20925241708755493
Batch 7/64 loss: -0.20657619833946228
Batch 8/64 loss: -0.20697295665740967
Batch 9/64 loss: -0.22283828258514404
Batch 10/64 loss: -0.21750038862228394
Batch 11/64 loss: -0.21248584985733032
Batch 12/64 loss: -0.22911906242370605
Batch 13/64 loss: -0.19394537806510925
Batch 14/64 loss: -0.198147714138031
Batch 15/64 loss: -0.21438366174697876
Batch 16/64 loss: -0.21755293011665344
Batch 17/64 loss: -0.22604304552078247
Batch 18/64 loss: -0.20223233103752136
Batch 19/64 loss: -0.19265660643577576
Batch 20/64 loss: -0.1889418363571167
Batch 21/64 loss: -0.22387820482254028
Batch 22/64 loss: -0.20040935277938843
Batch 23/64 loss: -0.18506020307540894
Batch 24/64 loss: -0.2226167917251587
Batch 25/64 loss: -0.2272164225578308
Batch 26/64 loss: -0.21532881259918213
Batch 27/64 loss: -0.22489529848098755
Batch 28/64 loss: -0.22497981786727905
Batch 29/64 loss: -0.22961166501045227
Batch 30/64 loss: -0.21753054857254028
Batch 31/64 loss: -0.21081387996673584
Batch 32/64 loss: -0.2191869020462036
Batch 33/64 loss: -0.2301672101020813
Batch 34/64 loss: -0.22031307220458984
Batch 35/64 loss: -0.16696560382843018
Batch 36/64 loss: -0.2294641137123108
Batch 37/64 loss: -0.20308148860931396
Batch 38/64 loss: -0.22659415006637573
Batch 39/64 loss: -0.20729219913482666
Batch 40/64 loss: -0.17931199073791504
Batch 41/64 loss: -0.19449079036712646
Batch 42/64 loss: -0.20719632506370544
Batch 43/64 loss: -0.19698750972747803
Batch 44/64 loss: -0.19776564836502075
Batch 45/64 loss: -0.178203284740448
Batch 46/64 loss: -0.19995886087417603
Batch 47/64 loss: -0.19989827275276184
Batch 48/64 loss: -0.19256216287612915
Batch 49/64 loss: -0.19986343383789062
Batch 50/64 loss: -0.20700308680534363
Batch 51/64 loss: -0.17933017015457153
Batch 52/64 loss: -0.21259665489196777
Batch 53/64 loss: -0.23258984088897705
Batch 54/64 loss: -0.2114793062210083
Batch 55/64 loss: -0.21945923566818237
Batch 56/64 loss: -0.2253773808479309
Batch 57/64 loss: -0.1999274492263794
Batch 58/64 loss: -0.1771484613418579
Batch 59/64 loss: -0.19662606716156006
Batch 60/64 loss: -0.23175257444381714
Batch 61/64 loss: -0.20460796356201172
Batch 62/64 loss: -0.21313133835792542
Batch 63/64 loss: -0.19379299879074097
Batch 64/64 loss: -0.20568794012069702
Epoch 476  Train loss: -0.20858930957083607  Val loss: -0.01198506826387648
Epoch 477
-------------------------------
Batch 1/64 loss: -0.20483574271202087
Batch 2/64 loss: -0.24252843856811523
Batch 3/64 loss: -0.23606044054031372
Batch 4/64 loss: -0.20059776306152344
Batch 5/64 loss: -0.22054237127304077
Batch 6/64 loss: -0.1788959503173828
Batch 7/64 loss: -0.22486409544944763
Batch 8/64 loss: -0.20668679475784302
Batch 9/64 loss: -0.2041110396385193
Batch 10/64 loss: -0.20057600736618042
Batch 11/64 loss: -0.21221140027046204
Batch 12/64 loss: -0.2242482304573059
Batch 13/64 loss: -0.20333552360534668
Batch 14/64 loss: -0.20377719402313232
Batch 15/64 loss: -0.21801674365997314
Batch 16/64 loss: -0.20510876178741455
Batch 17/64 loss: -0.2132377028465271
Batch 18/64 loss: -0.19007790088653564
Batch 19/64 loss: -0.19095933437347412
Batch 20/64 loss: -0.22478097677230835
Batch 21/64 loss: -0.21430975198745728
Batch 22/64 loss: -0.2078627049922943
Batch 23/64 loss: -0.20042425394058228
Batch 24/64 loss: -0.20687872171401978
Batch 25/64 loss: -0.19394564628601074
Batch 26/64 loss: -0.19411826133728027
Batch 27/64 loss: -0.21021047234535217
Batch 28/64 loss: -0.21389660239219666
Batch 29/64 loss: -0.22149527072906494
Batch 30/64 loss: -0.19960922002792358
Batch 31/64 loss: -0.21714115142822266
Batch 32/64 loss: -0.19095951318740845
Batch 33/64 loss: -0.2124326229095459
Batch 34/64 loss: -0.22341376543045044
Batch 35/64 loss: -0.23158279061317444
Batch 36/64 loss: -0.20743155479431152
Batch 37/64 loss: -0.2022460699081421
Batch 38/64 loss: -0.23399457335472107
Batch 39/64 loss: -0.1985454559326172
Batch 40/64 loss: -0.22159385681152344
Batch 41/64 loss: -0.22021442651748657
Batch 42/64 loss: -0.22896766662597656
Batch 43/64 loss: -0.20628994703292847
Batch 44/64 loss: -0.223597913980484
Batch 45/64 loss: -0.22769862413406372
Batch 46/64 loss: -0.19201165437698364
Batch 47/64 loss: -0.20536547899246216
Batch 48/64 loss: -0.17638105154037476
Batch 49/64 loss: -0.21652215719223022
Batch 50/64 loss: -0.173939049243927
Batch 51/64 loss: -0.17582130432128906
Batch 52/64 loss: -0.18601906299591064
Batch 53/64 loss: -0.19858431816101074
Batch 54/64 loss: -0.21542233228683472
Batch 55/64 loss: -0.2071951925754547
Batch 56/64 loss: -0.2259329855442047
Batch 57/64 loss: -0.18249642848968506
Batch 58/64 loss: -0.20532891154289246
Batch 59/64 loss: -0.19640856981277466
Batch 60/64 loss: -0.22392073273658752
Batch 61/64 loss: -0.20357811450958252
Batch 62/64 loss: -0.17661690711975098
Batch 63/64 loss: -0.19802069664001465
Batch 64/64 loss: -0.17873018980026245
Epoch 477  Train loss: -0.2071831504503886  Val loss: -0.012743558056166083
Epoch 478
-------------------------------
Batch 1/64 loss: -0.20360642671585083
Batch 2/64 loss: -0.2082751989364624
Batch 3/64 loss: -0.21598416566848755
Batch 4/64 loss: -0.22705411911010742
Batch 5/64 loss: -0.22938475012779236
Batch 6/64 loss: -0.21447521448135376
Batch 7/64 loss: -0.19698920845985413
Batch 8/64 loss: -0.20398598909378052
Batch 9/64 loss: -0.19152617454528809
Batch 10/64 loss: -0.21549099683761597
Batch 11/64 loss: -0.2217344343662262
Batch 12/64 loss: -0.2129310667514801
Batch 13/64 loss: -0.23925721645355225
Batch 14/64 loss: -0.20797434449195862
Batch 15/64 loss: -0.21793556213378906
Batch 16/64 loss: -0.22260478138923645
Batch 17/64 loss: -0.20425084233283997
Batch 18/64 loss: -0.19230830669403076
Batch 19/64 loss: -0.23518812656402588
Batch 20/64 loss: -0.2320653200149536
Batch 21/64 loss: -0.19770455360412598
Batch 22/64 loss: -0.1980612874031067
Batch 23/64 loss: -0.20409393310546875
Batch 24/64 loss: -0.21987652778625488
Batch 25/64 loss: -0.2459987998008728
Batch 26/64 loss: -0.237478107213974
Batch 27/64 loss: -0.23338085412979126
Batch 28/64 loss: -0.2094043493270874
Batch 29/64 loss: -0.2247856855392456
Batch 30/64 loss: -0.22265350818634033
Batch 31/64 loss: -0.18280690908432007
Batch 32/64 loss: -0.22079914808273315
Batch 33/64 loss: -0.20370733737945557
Batch 34/64 loss: -0.20347711443901062
Batch 35/64 loss: -0.21734869480133057
Batch 36/64 loss: -0.21060997247695923
Batch 37/64 loss: -0.20436424016952515
Batch 38/64 loss: -0.21751642227172852
Batch 39/64 loss: -0.2272353172302246
Batch 40/64 loss: -0.2213466465473175
Batch 41/64 loss: -0.23469123244285583
Batch 42/64 loss: -0.24282631278038025
Batch 43/64 loss: -0.21579939126968384
Batch 44/64 loss: -0.21452242136001587
Batch 45/64 loss: -0.1964310109615326
Batch 46/64 loss: -0.20673689246177673
Batch 47/64 loss: -0.2273212969303131
Batch 48/64 loss: -0.16786831617355347
Batch 49/64 loss: -0.21307674050331116
Batch 50/64 loss: -0.17949140071868896
Batch 51/64 loss: -0.23194146156311035
Batch 52/64 loss: -0.22469228506088257
Batch 53/64 loss: -0.22860050201416016
Batch 54/64 loss: -0.20369970798492432
Batch 55/64 loss: -0.21474534273147583
Batch 56/64 loss: -0.2018004059791565
Batch 57/64 loss: -0.19492697715759277
Batch 58/64 loss: -0.2158885896205902
Batch 59/64 loss: -0.2094246745109558
Batch 60/64 loss: -0.22153735160827637
Batch 61/64 loss: -0.21209192276000977
Batch 62/64 loss: -0.1981760859489441
Batch 63/64 loss: -0.22803229093551636
Batch 64/64 loss: -0.1757434606552124
Epoch 478  Train loss: -0.21351846059163412  Val loss: -0.01098574293438102
Epoch 479
-------------------------------
Batch 1/64 loss: -0.20363616943359375
Batch 2/64 loss: -0.22949016094207764
Batch 3/64 loss: -0.21386685967445374
Batch 4/64 loss: -0.2368270754814148
Batch 5/64 loss: -0.24045929312705994
Batch 6/64 loss: -0.21603190898895264
Batch 7/64 loss: -0.20693016052246094
Batch 8/64 loss: -0.22583749890327454
Batch 9/64 loss: -0.23066484928131104
Batch 10/64 loss: -0.21756666898727417
Batch 11/64 loss: -0.21933972835540771
Batch 12/64 loss: -0.23222777247428894
Batch 13/64 loss: -0.2108105719089508
Batch 14/64 loss: -0.2330331802368164
Batch 15/64 loss: -0.2185823917388916
Batch 16/64 loss: -0.2126971185207367
Batch 17/64 loss: -0.19266897439956665
Batch 18/64 loss: -0.15544801950454712
Batch 19/64 loss: -0.20924389362335205
Batch 20/64 loss: -0.23815497756004333
Batch 21/64 loss: -0.21877354383468628
Batch 22/64 loss: -0.22384220361709595
Batch 23/64 loss: -0.17350220680236816
Batch 24/64 loss: -0.20833498239517212
Batch 25/64 loss: -0.2154316008090973
Batch 26/64 loss: -0.20119017362594604
Batch 27/64 loss: -0.22429561614990234
Batch 28/64 loss: -0.2241729497909546
Batch 29/64 loss: -0.22454363107681274
Batch 30/64 loss: -0.22311413288116455
Batch 31/64 loss: -0.2087758481502533
Batch 32/64 loss: -0.20859026908874512
Batch 33/64 loss: -0.20610958337783813
Batch 34/64 loss: -0.24558496475219727
Batch 35/64 loss: -0.20939233899116516
Batch 36/64 loss: -0.21641480922698975
Batch 37/64 loss: -0.2264147400856018
Batch 38/64 loss: -0.21825477480888367
Batch 39/64 loss: -0.2207065224647522
Batch 40/64 loss: -0.21631747484207153
Batch 41/64 loss: -0.20842933654785156
Batch 42/64 loss: -0.20080983638763428
Batch 43/64 loss: -0.20351755619049072
Batch 44/64 loss: -0.2215452790260315
Batch 45/64 loss: -0.22024181485176086
Batch 46/64 loss: -0.1819351315498352
Batch 47/64 loss: -0.2277795672416687
Batch 48/64 loss: -0.2050769329071045
Batch 49/64 loss: -0.21756836771965027
Batch 50/64 loss: -0.21522223949432373
Batch 51/64 loss: -0.21064668893814087
Batch 52/64 loss: -0.20653825998306274
Batch 53/64 loss: -0.17604970932006836
Batch 54/64 loss: -0.1936638355255127
Batch 55/64 loss: -0.22345441579818726
Batch 56/64 loss: -0.1946519911289215
Batch 57/64 loss: -0.19511720538139343
Batch 58/64 loss: -0.18508100509643555
Batch 59/64 loss: -0.16895562410354614
Batch 60/64 loss: -0.21650665998458862
Batch 61/64 loss: -0.2224239706993103
Batch 62/64 loss: -0.17242377996444702
Batch 63/64 loss: -0.2287571132183075
Batch 64/64 loss: -0.19819039106369019
Epoch 479  Train loss: -0.21180107850654453  Val loss: -0.011647606633373142
Epoch 480
-------------------------------
Batch 1/64 loss: -0.21107131242752075
Batch 2/64 loss: -0.19722113013267517
Batch 3/64 loss: -0.21953368186950684
Batch 4/64 loss: -0.18025243282318115
Batch 5/64 loss: -0.1823354959487915
Batch 6/64 loss: -0.2193935215473175
Batch 7/64 loss: -0.18300217390060425
Batch 8/64 loss: -0.20409566164016724
Batch 9/64 loss: -0.2121264636516571
Batch 10/64 loss: -0.208571195602417
Batch 11/64 loss: -0.19155144691467285
Batch 12/64 loss: -0.21337494254112244
Batch 13/64 loss: -0.21110445261001587
Batch 14/64 loss: -0.22660130262374878
Batch 15/64 loss: -0.21590304374694824
Batch 16/64 loss: -0.21147069334983826
Batch 17/64 loss: -0.2285938262939453
Batch 18/64 loss: -0.224795401096344
Batch 19/64 loss: -0.20173168182373047
Batch 20/64 loss: -0.22395139932632446
Batch 21/64 loss: -0.22981280088424683
Batch 22/64 loss: -0.2202882468700409
Batch 23/64 loss: -0.19946107268333435
Batch 24/64 loss: -0.205782949924469
Batch 25/64 loss: -0.22996753454208374
Batch 26/64 loss: -0.2284662127494812
Batch 27/64 loss: -0.21660882234573364
Batch 28/64 loss: -0.22981172800064087
Batch 29/64 loss: -0.2329595983028412
Batch 30/64 loss: -0.22838127613067627
Batch 31/64 loss: -0.216337651014328
Batch 32/64 loss: -0.1976214051246643
Batch 33/64 loss: -0.20638686418533325
Batch 34/64 loss: -0.19361817836761475
Batch 35/64 loss: -0.21709167957305908
Batch 36/64 loss: -0.2162572741508484
Batch 37/64 loss: -0.2247994840145111
Batch 38/64 loss: -0.21235090494155884
Batch 39/64 loss: -0.1768733263015747
Batch 40/64 loss: -0.22061234712600708
Batch 41/64 loss: -0.1871224045753479
Batch 42/64 loss: -0.22497248649597168
Batch 43/64 loss: -0.19198983907699585
Batch 44/64 loss: -0.22298040986061096
Batch 45/64 loss: -0.22295063734054565
Batch 46/64 loss: -0.2141634225845337
Batch 47/64 loss: -0.21752122044563293
Batch 48/64 loss: -0.2163243293762207
Batch 49/64 loss: -0.19054991006851196
Batch 50/64 loss: -0.2178162932395935
Batch 51/64 loss: -0.215032160282135
Batch 52/64 loss: -0.21482372283935547
Batch 53/64 loss: -0.18392729759216309
Batch 54/64 loss: -0.18788498640060425
Batch 55/64 loss: -0.2153419852256775
Batch 56/64 loss: -0.22285038232803345
Batch 57/64 loss: -0.2187296748161316
Batch 58/64 loss: -0.18633878231048584
Batch 59/64 loss: -0.1633111834526062
Batch 60/64 loss: -0.2160315215587616
Batch 61/64 loss: -0.18178236484527588
Batch 62/64 loss: -0.2195793092250824
Batch 63/64 loss: -0.21828460693359375
Batch 64/64 loss: -0.21989822387695312
Epoch 480  Train loss: -0.20996710926878687  Val loss: -0.014330378922400196
Epoch 481
-------------------------------
Batch 1/64 loss: -0.23294955492019653
Batch 2/64 loss: -0.2222408652305603
Batch 3/64 loss: -0.23387140035629272
Batch 4/64 loss: -0.21731704473495483
Batch 5/64 loss: -0.218677818775177
Batch 6/64 loss: -0.2207123041152954
Batch 7/64 loss: -0.18614935874938965
Batch 8/64 loss: -0.20672446489334106
Batch 9/64 loss: -0.1880563497543335
Batch 10/64 loss: -0.22118550539016724
Batch 11/64 loss: -0.21407511830329895
Batch 12/64 loss: -0.1897979974746704
Batch 13/64 loss: -0.198314368724823
Batch 14/64 loss: -0.21235454082489014
Batch 15/64 loss: -0.22002652287483215
Batch 16/64 loss: -0.2253207266330719
Batch 17/64 loss: -0.2167704701423645
Batch 18/64 loss: -0.18415361642837524
Batch 19/64 loss: -0.21431642770767212
Batch 20/64 loss: -0.19552916288375854
Batch 21/64 loss: -0.21781903505325317
Batch 22/64 loss: -0.2251003384590149
Batch 23/64 loss: -0.1788095235824585
Batch 24/64 loss: -0.17981380224227905
Batch 25/64 loss: -0.16004681587219238
Batch 26/64 loss: -0.20324808359146118
Batch 27/64 loss: -0.2227577567100525
Batch 28/64 loss: -0.20392993092536926
Batch 29/64 loss: -0.22013771533966064
Batch 30/64 loss: -0.22438102960586548
Batch 31/64 loss: -0.21407485008239746
Batch 32/64 loss: -0.1890774965286255
Batch 33/64 loss: -0.21274816989898682
Batch 34/64 loss: -0.21479490399360657
Batch 35/64 loss: -0.22824829816818237
Batch 36/64 loss: -0.22549283504486084
Batch 37/64 loss: -0.21675461530685425
Batch 38/64 loss: -0.1994362771511078
Batch 39/64 loss: -0.20385175943374634
Batch 40/64 loss: -0.2020513415336609
Batch 41/64 loss: -0.20779159665107727
Batch 42/64 loss: -0.1897873878479004
Batch 43/64 loss: -0.22197818756103516
Batch 44/64 loss: -0.18326455354690552
Batch 45/64 loss: -0.20743969082832336
Batch 46/64 loss: -0.21919015049934387
Batch 47/64 loss: -0.22153234481811523
Batch 48/64 loss: -0.21935567259788513
Batch 49/64 loss: -0.24470019340515137
Batch 50/64 loss: -0.22739219665527344
Batch 51/64 loss: -0.21503740549087524
Batch 52/64 loss: -0.2003764808177948
Batch 53/64 loss: -0.21022894978523254
Batch 54/64 loss: -0.21321415901184082
Batch 55/64 loss: -0.22436732053756714
Batch 56/64 loss: -0.21220606565475464
Batch 57/64 loss: -0.1902051568031311
Batch 58/64 loss: -0.1988728642463684
Batch 59/64 loss: -0.2042667269706726
Batch 60/64 loss: -0.2305556833744049
Batch 61/64 loss: -0.18089449405670166
Batch 62/64 loss: -0.22947943210601807
Batch 63/64 loss: -0.20987573266029358
Batch 64/64 loss: -0.2346833348274231
Epoch 481  Train loss: -0.21018263756060132  Val loss: -0.015291858170040694
Epoch 482
-------------------------------
Batch 1/64 loss: -0.22383153438568115
Batch 2/64 loss: -0.21192997694015503
Batch 3/64 loss: -0.23216849565505981
Batch 4/64 loss: -0.21491143107414246
Batch 5/64 loss: -0.2068578600883484
Batch 6/64 loss: -0.21979403495788574
Batch 7/64 loss: -0.24284285306930542
Batch 8/64 loss: -0.21996167302131653
Batch 9/64 loss: -0.22307884693145752
Batch 10/64 loss: -0.20881769061088562
Batch 11/64 loss: -0.20159852504730225
Batch 12/64 loss: -0.20120179653167725
Batch 13/64 loss: -0.21893411874771118
Batch 14/64 loss: -0.23938554525375366
Batch 15/64 loss: -0.18747299909591675
Batch 16/64 loss: -0.14494889974594116
Batch 17/64 loss: -0.22876882553100586
Batch 18/64 loss: -0.1918463110923767
Batch 19/64 loss: -0.2333158254623413
Batch 20/64 loss: -0.23602688312530518
Batch 21/64 loss: -0.2271663248538971
Batch 22/64 loss: -0.20473387837409973
Batch 23/64 loss: -0.21210378408432007
Batch 24/64 loss: -0.23122847080230713
Batch 25/64 loss: -0.19482120871543884
Batch 26/64 loss: -0.23208415508270264
Batch 27/64 loss: -0.23158389329910278
Batch 28/64 loss: -0.20218461751937866
Batch 29/64 loss: -0.2087668776512146
Batch 30/64 loss: -0.19304031133651733
Batch 31/64 loss: -0.2248501479625702
Batch 32/64 loss: -0.21668922901153564
Batch 33/64 loss: -0.19973042607307434
Batch 34/64 loss: -0.21697908639907837
Batch 35/64 loss: -0.22018396854400635
Batch 36/64 loss: -0.217401921749115
Batch 37/64 loss: -0.22421395778656006
Batch 38/64 loss: -0.22209516167640686
Batch 39/64 loss: -0.21483397483825684
Batch 40/64 loss: -0.22609257698059082
Batch 41/64 loss: -0.19678843021392822
Batch 42/64 loss: -0.22284197807312012
Batch 43/64 loss: -0.23161077499389648
Batch 44/64 loss: -0.18328136205673218
Batch 45/64 loss: -0.21353018283843994
Batch 46/64 loss: -0.21019995212554932
Batch 47/64 loss: -0.2232193946838379
Batch 48/64 loss: -0.2087198793888092
Batch 49/64 loss: -0.23852288722991943
Batch 50/64 loss: -0.22031360864639282
Batch 51/64 loss: -0.22196537256240845
Batch 52/64 loss: -0.22606170177459717
Batch 53/64 loss: -0.2146497666835785
Batch 54/64 loss: -0.217342346906662
Batch 55/64 loss: -0.2066236436367035
Batch 56/64 loss: -0.2110258936882019
Batch 57/64 loss: -0.2190675437450409
Batch 58/64 loss: -0.21298205852508545
Batch 59/64 loss: -0.2003527283668518
Batch 60/64 loss: -0.2254841923713684
Batch 61/64 loss: -0.20980924367904663
Batch 62/64 loss: -0.21677452325820923
Batch 63/64 loss: -0.2134934663772583
Batch 64/64 loss: -0.21839064359664917
Epoch 482  Train loss: -0.21516756099813125  Val loss: -0.01375994383264653
Epoch 483
-------------------------------
Batch 1/64 loss: -0.17920559644699097
Batch 2/64 loss: -0.21556410193443298
Batch 3/64 loss: -0.22605812549591064
Batch 4/64 loss: -0.21947413682937622
Batch 5/64 loss: -0.2351360321044922
Batch 6/64 loss: -0.2267465591430664
Batch 7/64 loss: -0.2453967034816742
Batch 8/64 loss: -0.1988128423690796
Batch 9/64 loss: -0.1873970627784729
Batch 10/64 loss: -0.2081930935382843
Batch 11/64 loss: -0.21871545910835266
Batch 12/64 loss: -0.23253369331359863
Batch 13/64 loss: -0.19784557819366455
Batch 14/64 loss: -0.20606368780136108
Batch 15/64 loss: -0.21892648935317993
Batch 16/64 loss: -0.2257690131664276
Batch 17/64 loss: -0.23644697666168213
Batch 18/64 loss: -0.22483348846435547
Batch 19/64 loss: -0.20193177461624146
Batch 20/64 loss: -0.22473323345184326
Batch 21/64 loss: -0.19871574640274048
Batch 22/64 loss: -0.21138077974319458
Batch 23/64 loss: -0.2142145037651062
Batch 24/64 loss: -0.1895977258682251
Batch 25/64 loss: -0.21976178884506226
Batch 26/64 loss: -0.21191328763961792
Batch 27/64 loss: -0.2280416488647461
Batch 28/64 loss: -0.22747796773910522
Batch 29/64 loss: -0.20952072739601135
Batch 30/64 loss: -0.2028244137763977
Batch 31/64 loss: -0.22251984477043152
Batch 32/64 loss: -0.21025872230529785
Batch 33/64 loss: -0.23518362641334534
Batch 34/64 loss: -0.21157556772232056
Batch 35/64 loss: -0.22447913885116577
Batch 36/64 loss: -0.2101825773715973
Batch 37/64 loss: -0.20501530170440674
Batch 38/64 loss: -0.2424543797969818
Batch 39/64 loss: -0.22596997022628784
Batch 40/64 loss: -0.20349740982055664
Batch 41/64 loss: -0.19711434841156006
Batch 42/64 loss: -0.18614619970321655
Batch 43/64 loss: -0.20976120233535767
Batch 44/64 loss: -0.19734564423561096
Batch 45/64 loss: -0.2185988426208496
Batch 46/64 loss: -0.19309937953948975
Batch 47/64 loss: -0.20913571119308472
Batch 48/64 loss: -0.17321830987930298
Batch 49/64 loss: -0.2204468846321106
Batch 50/64 loss: -0.2074553370475769
Batch 51/64 loss: -0.1915062665939331
Batch 52/64 loss: -0.21568775177001953
Batch 53/64 loss: -0.1748640537261963
Batch 54/64 loss: -0.22731488943099976
Batch 55/64 loss: -0.22655224800109863
Batch 56/64 loss: -0.21081441640853882
Batch 57/64 loss: -0.22437825798988342
Batch 58/64 loss: -0.2161821722984314
Batch 59/64 loss: -0.2186393439769745
Batch 60/64 loss: -0.208282470703125
Batch 61/64 loss: -0.16035187244415283
Batch 62/64 loss: -0.18383479118347168
Batch 63/64 loss: -0.19456180930137634
Batch 64/64 loss: -0.19746893644332886
Epoch 483  Train loss: -0.21094529698876774  Val loss: -0.011229857751184312
Epoch 484
-------------------------------
Batch 1/64 loss: -0.20393183827400208
Batch 2/64 loss: -0.201077401638031
Batch 3/64 loss: -0.2284669280052185
Batch 4/64 loss: -0.1969277262687683
Batch 5/64 loss: -0.2173282504081726
Batch 6/64 loss: -0.22551614046096802
Batch 7/64 loss: -0.19585776329040527
Batch 8/64 loss: -0.23594433069229126
Batch 9/64 loss: -0.21796372532844543
Batch 10/64 loss: -0.20092585682868958
Batch 11/64 loss: -0.18443918228149414
Batch 12/64 loss: -0.2282496988773346
Batch 13/64 loss: -0.21571198105812073
Batch 14/64 loss: -0.23197031021118164
Batch 15/64 loss: -0.22981637716293335
Batch 16/64 loss: -0.22545814514160156
Batch 17/64 loss: -0.20695626735687256
Batch 18/64 loss: -0.22894221544265747
Batch 19/64 loss: -0.20718544721603394
Batch 20/64 loss: -0.16642385721206665
Batch 21/64 loss: -0.20575204491615295
Batch 22/64 loss: -0.22008711099624634
Batch 23/64 loss: -0.2277822494506836
Batch 24/64 loss: -0.2132934033870697
Batch 25/64 loss: -0.2424255907535553
Batch 26/64 loss: -0.20744073390960693
Batch 27/64 loss: -0.21993136405944824
Batch 28/64 loss: -0.201787531375885
Batch 29/64 loss: -0.22437149286270142
Batch 30/64 loss: -0.22187018394470215
Batch 31/64 loss: -0.2318497598171234
Batch 32/64 loss: -0.21801292896270752
Batch 33/64 loss: -0.17565584182739258
Batch 34/64 loss: -0.2270868718624115
Batch 35/64 loss: -0.23087769746780396
Batch 36/64 loss: -0.19782096147537231
Batch 37/64 loss: -0.2199384570121765
Batch 38/64 loss: -0.22106856107711792
Batch 39/64 loss: -0.21247553825378418
Batch 40/64 loss: -0.20256853103637695
Batch 41/64 loss: -0.21191442012786865
Batch 42/64 loss: -0.20107108354568481
Batch 43/64 loss: -0.21794724464416504
Batch 44/64 loss: -0.23284760117530823
Batch 45/64 loss: -0.2229112684726715
Batch 46/64 loss: -0.22663754224777222
Batch 47/64 loss: -0.2251000702381134
Batch 48/64 loss: -0.2090158760547638
Batch 49/64 loss: -0.2165510356426239
Batch 50/64 loss: -0.21851524710655212
Batch 51/64 loss: -0.2026877999305725
Batch 52/64 loss: -0.19312477111816406
Batch 53/64 loss: -0.21662142872810364
Batch 54/64 loss: -0.2298077940940857
Batch 55/64 loss: -0.2185623049736023
Batch 56/64 loss: -0.2190750539302826
Batch 57/64 loss: -0.22464197874069214
Batch 58/64 loss: -0.21275827288627625
Batch 59/64 loss: -0.21623674035072327
Batch 60/64 loss: -0.23498058319091797
Batch 61/64 loss: -0.22140544652938843
Batch 62/64 loss: -0.21409228444099426
Batch 63/64 loss: -0.2059612274169922
Batch 64/64 loss: -0.2032616138458252
Epoch 484  Train loss: -0.21515456601685168  Val loss: -0.011744497567927305
Epoch 485
-------------------------------
Batch 1/64 loss: -0.22035962343215942
Batch 2/64 loss: -0.2053646743297577
Batch 3/64 loss: -0.2186645269393921
Batch 4/64 loss: -0.22136163711547852
Batch 5/64 loss: -0.22548800706863403
Batch 6/64 loss: -0.20266678929328918
Batch 7/64 loss: -0.20451712608337402
Batch 8/64 loss: -0.22321391105651855
Batch 9/64 loss: -0.22682195901870728
Batch 10/64 loss: -0.21704137325286865
Batch 11/64 loss: -0.19872277975082397
Batch 12/64 loss: -0.20796114206314087
Batch 13/64 loss: -0.209771066904068
Batch 14/64 loss: -0.22652733325958252
Batch 15/64 loss: -0.19565251469612122
Batch 16/64 loss: -0.2137666940689087
Batch 17/64 loss: -0.21573710441589355
Batch 18/64 loss: -0.21294039487838745
Batch 19/64 loss: -0.2171933650970459
Batch 20/64 loss: -0.1894795298576355
Batch 21/64 loss: -0.2281385362148285
Batch 22/64 loss: -0.20560705661773682
Batch 23/64 loss: -0.21549105644226074
Batch 24/64 loss: -0.2225019335746765
Batch 25/64 loss: -0.21939337253570557
Batch 26/64 loss: -0.20921772718429565
Batch 27/64 loss: -0.208391010761261
Batch 28/64 loss: -0.1709628701210022
Batch 29/64 loss: -0.21573960781097412
Batch 30/64 loss: -0.2239571511745453
Batch 31/64 loss: -0.21203583478927612
Batch 32/64 loss: -0.19266164302825928
Batch 33/64 loss: -0.23537468910217285
Batch 34/64 loss: -0.19904965162277222
Batch 35/64 loss: -0.22146683931350708
Batch 36/64 loss: -0.22420114278793335
Batch 37/64 loss: -0.22380688786506653
Batch 38/64 loss: -0.20926493406295776
Batch 39/64 loss: -0.2275126874446869
Batch 40/64 loss: -0.22303682565689087
Batch 41/64 loss: -0.19922950863838196
Batch 42/64 loss: -0.22262689471244812
Batch 43/64 loss: -0.21351760625839233
Batch 44/64 loss: -0.21402466297149658
Batch 45/64 loss: -0.22655326128005981
Batch 46/64 loss: -0.23926496505737305
Batch 47/64 loss: -0.227874755859375
Batch 48/64 loss: -0.24600699543952942
Batch 49/64 loss: -0.22257289290428162
Batch 50/64 loss: -0.22916805744171143
Batch 51/64 loss: -0.20595893263816833
Batch 52/64 loss: -0.1860104203224182
Batch 53/64 loss: -0.24457889795303345
Batch 54/64 loss: -0.22884255647659302
Batch 55/64 loss: -0.19031018018722534
Batch 56/64 loss: -0.2178405523300171
Batch 57/64 loss: -0.22222039103507996
Batch 58/64 loss: -0.21828246116638184
Batch 59/64 loss: -0.19910311698913574
Batch 60/64 loss: -0.23241779208183289
Batch 61/64 loss: -0.21222680807113647
Batch 62/64 loss: -0.2145836353302002
Batch 63/64 loss: -0.20102089643478394
Batch 64/64 loss: -0.19314095377922058
Epoch 485  Train loss: -0.21490439164872263  Val loss: -0.009771622742983894
Epoch 486
-------------------------------
Batch 1/64 loss: -0.22566065192222595
Batch 2/64 loss: -0.2253381609916687
Batch 3/64 loss: -0.20550769567489624
Batch 4/64 loss: -0.21381127834320068
Batch 5/64 loss: -0.2288489043712616
Batch 6/64 loss: -0.22432270646095276
Batch 7/64 loss: -0.21579918265342712
Batch 8/64 loss: -0.19542503356933594
Batch 9/64 loss: -0.1859673261642456
Batch 10/64 loss: -0.20625504851341248
Batch 11/64 loss: -0.22935131192207336
Batch 12/64 loss: -0.24123990535736084
Batch 13/64 loss: -0.219218909740448
Batch 14/64 loss: -0.1710110902786255
Batch 15/64 loss: -0.23648011684417725
Batch 16/64 loss: -0.21773266792297363
Batch 17/64 loss: -0.21649399399757385
Batch 18/64 loss: -0.2155725657939911
Batch 19/64 loss: -0.23718616366386414
Batch 20/64 loss: -0.22313076257705688
Batch 21/64 loss: -0.20396700501441956
Batch 22/64 loss: -0.22075039148330688
Batch 23/64 loss: -0.23972874879837036
Batch 24/64 loss: -0.21039211750030518
Batch 25/64 loss: -0.19975057244300842
Batch 26/64 loss: -0.24622076749801636
Batch 27/64 loss: -0.2161577343940735
Batch 28/64 loss: -0.23270699381828308
Batch 29/64 loss: -0.23505103588104248
Batch 30/64 loss: -0.22457557916641235
Batch 31/64 loss: -0.19452518224716187
Batch 32/64 loss: -0.19507810473442078
Batch 33/64 loss: -0.20817548036575317
Batch 34/64 loss: -0.2261558175086975
Batch 35/64 loss: -0.21716749668121338
Batch 36/64 loss: -0.22234725952148438
Batch 37/64 loss: -0.22998923063278198
Batch 38/64 loss: -0.2000395953655243
Batch 39/64 loss: -0.23472964763641357
Batch 40/64 loss: -0.21096837520599365
Batch 41/64 loss: -0.22556114196777344
Batch 42/64 loss: -0.2105439007282257
Batch 43/64 loss: -0.21166932582855225
Batch 44/64 loss: -0.22712808847427368
Batch 45/64 loss: -0.20653483271598816
Batch 46/64 loss: -0.22923725843429565
Batch 47/64 loss: -0.2216702401638031
Batch 48/64 loss: -0.23875859379768372
Batch 49/64 loss: -0.21500125527381897
Batch 50/64 loss: -0.23040145635604858
Batch 51/64 loss: -0.21815237402915955
Batch 52/64 loss: -0.16757243871688843
Batch 53/64 loss: -0.22741305828094482
Batch 54/64 loss: -0.22809025645256042
Batch 55/64 loss: -0.227473646402359
Batch 56/64 loss: -0.230606347322464
Batch 57/64 loss: -0.19200044870376587
Batch 58/64 loss: -0.2318044900894165
Batch 59/64 loss: -0.20755288004875183
Batch 60/64 loss: -0.21085521578788757
Batch 61/64 loss: -0.21636483073234558
Batch 62/64 loss: -0.21432119607925415
Batch 63/64 loss: -0.19152462482452393
Batch 64/64 loss: -0.212493896484375
Epoch 486  Train loss: -0.21713629703895718  Val loss: -0.01105467457951549
Epoch 487
-------------------------------
Batch 1/64 loss: -0.20214027166366577
Batch 2/64 loss: -0.2376752495765686
Batch 3/64 loss: -0.19929304718971252
Batch 4/64 loss: -0.20194199681282043
Batch 5/64 loss: -0.21811944246292114
Batch 6/64 loss: -0.21685275435447693
Batch 7/64 loss: -0.22086986899375916
Batch 8/64 loss: -0.2156294584274292
Batch 9/64 loss: -0.22570067644119263
Batch 10/64 loss: -0.21448466181755066
Batch 11/64 loss: -0.21450883150100708
Batch 12/64 loss: -0.21964269876480103
Batch 13/64 loss: -0.22589677572250366
Batch 14/64 loss: -0.22009360790252686
Batch 15/64 loss: -0.1950516700744629
Batch 16/64 loss: -0.21138331294059753
Batch 17/64 loss: -0.22477740049362183
Batch 18/64 loss: -0.22255992889404297
Batch 19/64 loss: -0.23466196656227112
Batch 20/64 loss: -0.2255154252052307
Batch 21/64 loss: -0.22772568464279175
Batch 22/64 loss: -0.21650156378746033
Batch 23/64 loss: -0.22154706716537476
Batch 24/64 loss: -0.2312227487564087
Batch 25/64 loss: -0.23537811636924744
Batch 26/64 loss: -0.22100558876991272
Batch 27/64 loss: -0.22142121195793152
Batch 28/64 loss: -0.2189839482307434
Batch 29/64 loss: -0.22971007227897644
Batch 30/64 loss: -0.19483953714370728
Batch 31/64 loss: -0.21907955408096313
Batch 32/64 loss: -0.22056424617767334
Batch 33/64 loss: -0.234954833984375
Batch 34/64 loss: -0.21382221579551697
Batch 35/64 loss: -0.21967846155166626
Batch 36/64 loss: -0.2533055543899536
Batch 37/64 loss: -0.18828415870666504
Batch 38/64 loss: -0.2373826503753662
Batch 39/64 loss: -0.21970245242118835
Batch 40/64 loss: -0.21139726042747498
Batch 41/64 loss: -0.2224256992340088
Batch 42/64 loss: -0.2144101858139038
Batch 43/64 loss: -0.20895704627037048
Batch 44/64 loss: -0.2126087248325348
Batch 45/64 loss: -0.22807776927947998
Batch 46/64 loss: -0.2329288125038147
Batch 47/64 loss: -0.21884918212890625
Batch 48/64 loss: -0.2049926519393921
Batch 49/64 loss: -0.21345025300979614
Batch 50/64 loss: -0.1880999207496643
Batch 51/64 loss: -0.2210034430027008
Batch 52/64 loss: -0.21855002641677856
Batch 53/64 loss: -0.2386344075202942
Batch 54/64 loss: -0.24536752700805664
Batch 55/64 loss: -0.22697299718856812
Batch 56/64 loss: -0.18666863441467285
Batch 57/64 loss: -0.18113142251968384
Batch 58/64 loss: -0.15296971797943115
Batch 59/64 loss: -0.2012888789176941
Batch 60/64 loss: -0.2285311222076416
Batch 61/64 loss: -0.213711678981781
Batch 62/64 loss: -0.2373320460319519
Batch 63/64 loss: -0.20975124835968018
Batch 64/64 loss: -0.16957223415374756
Epoch 487  Train loss: -0.21674036932926552  Val loss: -0.010698840380534274
Epoch 488
-------------------------------
Batch 1/64 loss: -0.23341286182403564
Batch 2/64 loss: -0.20437270402908325
Batch 3/64 loss: -0.22344708442687988
Batch 4/64 loss: -0.22600838541984558
Batch 5/64 loss: -0.2216666042804718
Batch 6/64 loss: -0.24276265501976013
Batch 7/64 loss: -0.2182644009590149
Batch 8/64 loss: -0.21437108516693115
Batch 9/64 loss: -0.19586220383644104
Batch 10/64 loss: -0.21939221024513245
Batch 11/64 loss: -0.19526392221450806
Batch 12/64 loss: -0.2080356478691101
Batch 13/64 loss: -0.2038101851940155
Batch 14/64 loss: -0.2051871418952942
Batch 15/64 loss: -0.19934514164924622
Batch 16/64 loss: -0.21981751918792725
Batch 17/64 loss: -0.20911550521850586
Batch 18/64 loss: -0.22198164463043213
Batch 19/64 loss: -0.2243351936340332
Batch 20/64 loss: -0.23897594213485718
Batch 21/64 loss: -0.22867059707641602
Batch 22/64 loss: -0.2307131588459015
Batch 23/64 loss: -0.2011556327342987
Batch 24/64 loss: -0.1962900161743164
Batch 25/64 loss: -0.18922317028045654
Batch 26/64 loss: -0.2203596532344818
Batch 27/64 loss: -0.20538559556007385
Batch 28/64 loss: -0.2097952961921692
Batch 29/64 loss: -0.21474921703338623
Batch 30/64 loss: -0.21886974573135376
Batch 31/64 loss: -0.2185990810394287
Batch 32/64 loss: -0.2106061577796936
Batch 33/64 loss: -0.22611626982688904
Batch 34/64 loss: -0.19096755981445312
Batch 35/64 loss: -0.21450453996658325
Batch 36/64 loss: -0.18776684999465942
Batch 37/64 loss: -0.17856687307357788
Batch 38/64 loss: -0.20651647448539734
Batch 39/64 loss: -0.2207084596157074
Batch 40/64 loss: -0.23162615299224854
Batch 41/64 loss: -0.22390109300613403
Batch 42/64 loss: -0.22414922714233398
Batch 43/64 loss: -0.2144668698310852
Batch 44/64 loss: -0.21622437238693237
Batch 45/64 loss: -0.19055664539337158
Batch 46/64 loss: -0.21802875399589539
Batch 47/64 loss: -0.21526741981506348
Batch 48/64 loss: -0.22925591468811035
Batch 49/64 loss: -0.2162291407585144
Batch 50/64 loss: -0.2109624743461609
Batch 51/64 loss: -0.22815585136413574
Batch 52/64 loss: -0.21178239583969116
Batch 53/64 loss: -0.21443432569503784
Batch 54/64 loss: -0.2106163501739502
Batch 55/64 loss: -0.2132800817489624
Batch 56/64 loss: -0.21771523356437683
Batch 57/64 loss: -0.19294795393943787
Batch 58/64 loss: -0.21987736225128174
Batch 59/64 loss: -0.22972983121871948
Batch 60/64 loss: -0.23223775625228882
Batch 61/64 loss: -0.22700798511505127
Batch 62/64 loss: -0.2227068543434143
Batch 63/64 loss: -0.2351851463317871
Batch 64/64 loss: -0.18476969003677368
Epoch 488  Train loss: -0.21458689956104054  Val loss: -0.011157759890933218
Epoch 489
-------------------------------
Batch 1/64 loss: -0.2055741846561432
Batch 2/64 loss: -0.24807965755462646
Batch 3/64 loss: -0.22137856483459473
Batch 4/64 loss: -0.2067241668701172
Batch 5/64 loss: -0.19933870434761047
Batch 6/64 loss: -0.19975930452346802
Batch 7/64 loss: -0.23300939798355103
Batch 8/64 loss: -0.21985316276550293
Batch 9/64 loss: -0.22454756498336792
Batch 10/64 loss: -0.21641919016838074
Batch 11/64 loss: -0.2101646065711975
Batch 12/64 loss: -0.2138473391532898
Batch 13/64 loss: -0.22540229558944702
Batch 14/64 loss: -0.23667019605636597
Batch 15/64 loss: -0.20435142517089844
Batch 16/64 loss: -0.19255387783050537
Batch 17/64 loss: -0.19854122400283813
Batch 18/64 loss: -0.19114786386489868
Batch 19/64 loss: -0.2256072461605072
Batch 20/64 loss: -0.21537181735038757
Batch 21/64 loss: -0.18851244449615479
Batch 22/64 loss: -0.22966814041137695
Batch 23/64 loss: -0.23394626379013062
Batch 24/64 loss: -0.2005925178527832
Batch 25/64 loss: -0.19790118932724
Batch 26/64 loss: -0.1966475546360016
Batch 27/64 loss: -0.22679835557937622
Batch 28/64 loss: -0.25692999362945557
Batch 29/64 loss: -0.21929553151130676
Batch 30/64 loss: -0.22842136025428772
Batch 31/64 loss: -0.2520259618759155
Batch 32/64 loss: -0.229664146900177
Batch 33/64 loss: -0.24129408597946167
Batch 34/64 loss: -0.21771028637886047
Batch 35/64 loss: -0.22128814458847046
Batch 36/64 loss: -0.20706719160079956
Batch 37/64 loss: -0.2104281783103943
Batch 38/64 loss: -0.20631632208824158
Batch 39/64 loss: -0.21896952390670776
Batch 40/64 loss: -0.2149970531463623
Batch 41/64 loss: -0.20093098282814026
Batch 42/64 loss: -0.21398985385894775
Batch 43/64 loss: -0.2195872962474823
Batch 44/64 loss: -0.2106299102306366
Batch 45/64 loss: -0.22908824682235718
Batch 46/64 loss: -0.2276928722858429
Batch 47/64 loss: -0.21257609128952026
Batch 48/64 loss: -0.2079339623451233
Batch 49/64 loss: -0.22000181674957275
Batch 50/64 loss: -0.20337191224098206
Batch 51/64 loss: -0.19271081686019897
Batch 52/64 loss: -0.20744743943214417
Batch 53/64 loss: -0.21320417523384094
Batch 54/64 loss: -0.20410040020942688
Batch 55/64 loss: -0.21250921487808228
Batch 56/64 loss: -0.1777746081352234
Batch 57/64 loss: -0.2322029173374176
Batch 58/64 loss: -0.23705542087554932
Batch 59/64 loss: -0.2240198850631714
Batch 60/64 loss: -0.21081793308258057
Batch 61/64 loss: -0.20602267980575562
Batch 62/64 loss: -0.21857035160064697
Batch 63/64 loss: -0.20057576894760132
Batch 64/64 loss: -0.19556748867034912
Epoch 489  Train loss: -0.2151577444637523  Val loss: -0.01251717326567345
Epoch 490
-------------------------------
Batch 1/64 loss: -0.22249358892440796
Batch 2/64 loss: -0.22707286477088928
Batch 3/64 loss: -0.2334446907043457
Batch 4/64 loss: -0.1740269660949707
Batch 5/64 loss: -0.2391301989555359
Batch 6/64 loss: -0.21034172177314758
Batch 7/64 loss: -0.22527551651000977
Batch 8/64 loss: -0.21396052837371826
Batch 9/64 loss: -0.21453255414962769
Batch 10/64 loss: -0.22626185417175293
Batch 11/64 loss: -0.21470272541046143
Batch 12/64 loss: -0.17025744915008545
Batch 13/64 loss: -0.20995408296585083
Batch 14/64 loss: -0.20282834768295288
Batch 15/64 loss: -0.21023917198181152
Batch 16/64 loss: -0.21479034423828125
Batch 17/64 loss: -0.22880923748016357
Batch 18/64 loss: -0.20371520519256592
Batch 19/64 loss: -0.19795364141464233
Batch 20/64 loss: -0.22985941171646118
Batch 21/64 loss: -0.23160362243652344
Batch 22/64 loss: -0.2242192029953003
Batch 23/64 loss: -0.23003727197647095
Batch 24/64 loss: -0.2179250717163086
Batch 25/64 loss: -0.20924854278564453
Batch 26/64 loss: -0.23595216870307922
Batch 27/64 loss: -0.2086087167263031
Batch 28/64 loss: -0.241998553276062
Batch 29/64 loss: -0.22775202989578247
Batch 30/64 loss: -0.2239457666873932
Batch 31/64 loss: -0.22219979763031006
Batch 32/64 loss: -0.2252821922302246
Batch 33/64 loss: -0.1750103235244751
Batch 34/64 loss: -0.20338773727416992
Batch 35/64 loss: -0.2234419584274292
Batch 36/64 loss: -0.1939852237701416
Batch 37/64 loss: -0.20381727814674377
Batch 38/64 loss: -0.18661826848983765
Batch 39/64 loss: -0.22148597240447998
Batch 40/64 loss: -0.2485937774181366
Batch 41/64 loss: -0.21570229530334473
Batch 42/64 loss: -0.2397664487361908
Batch 43/64 loss: -0.23357731103897095
Batch 44/64 loss: -0.2078458070755005
Batch 45/64 loss: -0.18286949396133423
Batch 46/64 loss: -0.22980964183807373
Batch 47/64 loss: -0.2253042757511139
Batch 48/64 loss: -0.21725201606750488
Batch 49/64 loss: -0.19059491157531738
Batch 50/64 loss: -0.200078547000885
Batch 51/64 loss: -0.22542625665664673
Batch 52/64 loss: -0.20839622616767883
Batch 53/64 loss: -0.20403608679771423
Batch 54/64 loss: -0.23595720529556274
Batch 55/64 loss: -0.21981209516525269
Batch 56/64 loss: -0.17449885606765747
Batch 57/64 loss: -0.1819189190864563
Batch 58/64 loss: -0.20556369423866272
Batch 59/64 loss: -0.20973795652389526
Batch 60/64 loss: -0.1858188509941101
Batch 61/64 loss: -0.21293970942497253
Batch 62/64 loss: -0.21831107139587402
Batch 63/64 loss: -0.19541722536087036
Batch 64/64 loss: -0.2322508692741394
Epoch 490  Train loss: -0.21364057461420696  Val loss: -0.011197086666867495
Epoch 491
-------------------------------
Batch 1/64 loss: -0.23402857780456543
Batch 2/64 loss: -0.19367164373397827
Batch 3/64 loss: -0.2248704731464386
Batch 4/64 loss: -0.20826435089111328
Batch 5/64 loss: -0.22115349769592285
Batch 6/64 loss: -0.20908087491989136
Batch 7/64 loss: -0.22867608070373535
Batch 8/64 loss: -0.22438472509384155
Batch 9/64 loss: -0.23233410716056824
Batch 10/64 loss: -0.22325941920280457
Batch 11/64 loss: -0.22713309526443481
Batch 12/64 loss: -0.1923062801361084
Batch 13/64 loss: -0.21245482563972473
Batch 14/64 loss: -0.21897688508033752
Batch 15/64 loss: -0.22401899099349976
Batch 16/64 loss: -0.22840237617492676
Batch 17/64 loss: -0.214287668466568
Batch 18/64 loss: -0.2218034863471985
Batch 19/64 loss: -0.21829575300216675
Batch 20/64 loss: -0.2194194793701172
Batch 21/64 loss: -0.1687455177307129
Batch 22/64 loss: -0.21892821788787842
Batch 23/64 loss: -0.2388308048248291
Batch 24/64 loss: -0.22778859734535217
Batch 25/64 loss: -0.20830410718917847
Batch 26/64 loss: -0.21932846307754517
Batch 27/64 loss: -0.22206124663352966
Batch 28/64 loss: -0.2232624590396881
Batch 29/64 loss: -0.2454650104045868
Batch 30/64 loss: -0.22818315029144287
Batch 31/64 loss: -0.209275484085083
Batch 32/64 loss: -0.19713103771209717
Batch 33/64 loss: -0.22648721933364868
Batch 34/64 loss: -0.226706862449646
Batch 35/64 loss: -0.22356611490249634
Batch 36/64 loss: -0.20495250821113586
Batch 37/64 loss: -0.2121632993221283
Batch 38/64 loss: -0.22846859693527222
Batch 39/64 loss: -0.24414262175559998
Batch 40/64 loss: -0.2549671530723572
Batch 41/64 loss: -0.19506287574768066
Batch 42/64 loss: -0.23264673352241516
Batch 43/64 loss: -0.20048633217811584
Batch 44/64 loss: -0.24640613794326782
Batch 45/64 loss: -0.22181451320648193
Batch 46/64 loss: -0.1956971287727356
Batch 47/64 loss: -0.22276189923286438
Batch 48/64 loss: -0.20154380798339844
Batch 49/64 loss: -0.2121296226978302
Batch 50/64 loss: -0.22391265630722046
Batch 51/64 loss: -0.21169859170913696
Batch 52/64 loss: -0.19688445329666138
Batch 53/64 loss: -0.22441858053207397
Batch 54/64 loss: -0.17100399732589722
Batch 55/64 loss: -0.21540796756744385
Batch 56/64 loss: -0.20317396521568298
Batch 57/64 loss: -0.1902257800102234
Batch 58/64 loss: -0.22603192925453186
Batch 59/64 loss: -0.2108522653579712
Batch 60/64 loss: -0.23750439286231995
Batch 61/64 loss: -0.24201557040214539
Batch 62/64 loss: -0.23145416378974915
Batch 63/64 loss: -0.2124364674091339
Batch 64/64 loss: -0.22706085443496704
Epoch 491  Train loss: -0.218061906449935  Val loss: -0.011919358956445124
Epoch 492
-------------------------------
Batch 1/64 loss: -0.2153465747833252
Batch 2/64 loss: -0.19754242897033691
Batch 3/64 loss: -0.2097158432006836
Batch 4/64 loss: -0.22945821285247803
Batch 5/64 loss: -0.2089102864265442
Batch 6/64 loss: -0.21168997883796692
Batch 7/64 loss: -0.23885804414749146
Batch 8/64 loss: -0.23453965783119202
Batch 9/64 loss: -0.20510435104370117
Batch 10/64 loss: -0.23978209495544434
Batch 11/64 loss: -0.23015213012695312
Batch 12/64 loss: -0.2354564070701599
Batch 13/64 loss: -0.2086467146873474
Batch 14/64 loss: -0.1935502290725708
Batch 15/64 loss: -0.22581249475479126
Batch 16/64 loss: -0.21047785878181458
Batch 17/64 loss: -0.23289847373962402
Batch 18/64 loss: -0.22153723239898682
Batch 19/64 loss: -0.22841554880142212
Batch 20/64 loss: -0.2193273901939392
Batch 21/64 loss: -0.20134469866752625
Batch 22/64 loss: -0.2028200626373291
Batch 23/64 loss: -0.2240300178527832
Batch 24/64 loss: -0.21942070126533508
Batch 25/64 loss: -0.23379600048065186
Batch 26/64 loss: -0.22069501876831055
Batch 27/64 loss: -0.2199130356311798
Batch 28/64 loss: -0.21327942609786987
Batch 29/64 loss: -0.16807717084884644
Batch 30/64 loss: -0.19445949792861938
Batch 31/64 loss: -0.20015078783035278
Batch 32/64 loss: -0.20070099830627441
Batch 33/64 loss: -0.23173794150352478
Batch 34/64 loss: -0.19137096405029297
Batch 35/64 loss: -0.2030603289604187
Batch 36/64 loss: -0.21583926677703857
Batch 37/64 loss: -0.2305234670639038
Batch 38/64 loss: -0.22929787635803223
Batch 39/64 loss: -0.23521003127098083
Batch 40/64 loss: -0.21149438619613647
Batch 41/64 loss: -0.22812354564666748
Batch 42/64 loss: -0.236922025680542
Batch 43/64 loss: -0.20050153136253357
Batch 44/64 loss: -0.23946070671081543
Batch 45/64 loss: -0.22707867622375488
Batch 46/64 loss: -0.21794140338897705
Batch 47/64 loss: -0.21515101194381714
Batch 48/64 loss: -0.2101765275001526
Batch 49/64 loss: -0.22886186838150024
Batch 50/64 loss: -0.22732800245285034
Batch 51/64 loss: -0.2351008653640747
Batch 52/64 loss: -0.2211412489414215
Batch 53/64 loss: -0.23292595148086548
Batch 54/64 loss: -0.21611961722373962
Batch 55/64 loss: -0.21800446510314941
Batch 56/64 loss: -0.20239099860191345
Batch 57/64 loss: -0.223771333694458
Batch 58/64 loss: -0.21837151050567627
Batch 59/64 loss: -0.21273142099380493
Batch 60/64 loss: -0.20982474088668823
Batch 61/64 loss: -0.23224925994873047
Batch 62/64 loss: -0.22552040219306946
Batch 63/64 loss: -0.20266583561897278
Batch 64/64 loss: -0.21167045831680298
Epoch 492  Train loss: -0.21781269685894836  Val loss: -0.010007610845401934
Epoch 493
-------------------------------
Batch 1/64 loss: -0.2201358675956726
Batch 2/64 loss: -0.2276131510734558
Batch 3/64 loss: -0.2205650806427002
Batch 4/64 loss: -0.23537126183509827
Batch 5/64 loss: -0.22702214121818542
Batch 6/64 loss: -0.2378976047039032
Batch 7/64 loss: -0.21861732006072998
Batch 8/64 loss: -0.16069817543029785
Batch 9/64 loss: -0.2228766679763794
Batch 10/64 loss: -0.21862846612930298
Batch 11/64 loss: -0.23160260915756226
Batch 12/64 loss: -0.20459425449371338
Batch 13/64 loss: -0.2073841094970703
Batch 14/64 loss: -0.21984344720840454
Batch 15/64 loss: -0.20111888647079468
Batch 16/64 loss: -0.22859644889831543
Batch 17/64 loss: -0.205386221408844
Batch 18/64 loss: -0.21438688039779663
Batch 19/64 loss: -0.2120896875858307
Batch 20/64 loss: -0.20868659019470215
Batch 21/64 loss: -0.2223389744758606
Batch 22/64 loss: -0.19481727480888367
Batch 23/64 loss: -0.21843010187149048
Batch 24/64 loss: -0.1789073944091797
Batch 25/64 loss: -0.22489029169082642
Batch 26/64 loss: -0.22242218255996704
Batch 27/64 loss: -0.19140589237213135
Batch 28/64 loss: -0.23680859804153442
Batch 29/64 loss: -0.24336987733840942
Batch 30/64 loss: -0.22171655297279358
Batch 31/64 loss: -0.20293602347373962
Batch 32/64 loss: -0.23723936080932617
Batch 33/64 loss: -0.19240719079971313
Batch 34/64 loss: -0.2285393476486206
Batch 35/64 loss: -0.20841121673583984
Batch 36/64 loss: -0.23320236802101135
Batch 37/64 loss: -0.21624881029129028
Batch 38/64 loss: -0.19018405675888062
Batch 39/64 loss: -0.22297728061676025
Batch 40/64 loss: -0.22641319036483765
Batch 41/64 loss: -0.21304938197135925
Batch 42/64 loss: -0.15467005968093872
Batch 43/64 loss: -0.22527283430099487
Batch 44/64 loss: -0.22149431705474854
Batch 45/64 loss: -0.1963777244091034
Batch 46/64 loss: -0.1762394905090332
Batch 47/64 loss: -0.18737643957138062
Batch 48/64 loss: -0.22660604119300842
Batch 49/64 loss: -0.2201554775238037
Batch 50/64 loss: -0.18538546562194824
Batch 51/64 loss: -0.23042160272598267
Batch 52/64 loss: -0.21955782175064087
Batch 53/64 loss: -0.2257012128829956
Batch 54/64 loss: -0.24290263652801514
Batch 55/64 loss: -0.21671873331069946
Batch 56/64 loss: -0.22334721684455872
Batch 57/64 loss: -0.22176450490951538
Batch 58/64 loss: -0.21796691417694092
Batch 59/64 loss: -0.21261560916900635
Batch 60/64 loss: -0.2185477614402771
Batch 61/64 loss: -0.20661473274230957
Batch 62/64 loss: -0.21022671461105347
Batch 63/64 loss: -0.19834548234939575
Batch 64/64 loss: -0.203007310628891
Epoch 493  Train loss: -0.213966972804537  Val loss: -0.01211220579049022
Epoch 494
-------------------------------
Batch 1/64 loss: -0.20209482312202454
Batch 2/64 loss: -0.2336064875125885
Batch 3/64 loss: -0.2287105917930603
Batch 4/64 loss: -0.2365100383758545
Batch 5/64 loss: -0.23976057767868042
Batch 6/64 loss: -0.2187056839466095
Batch 7/64 loss: -0.23183858394622803
Batch 8/64 loss: -0.239976167678833
Batch 9/64 loss: -0.21235644817352295
Batch 10/64 loss: -0.23744851350784302
Batch 11/64 loss: -0.25039228796958923
Batch 12/64 loss: -0.22914129495620728
Batch 13/64 loss: -0.23312020301818848
Batch 14/64 loss: -0.22525790333747864
Batch 15/64 loss: -0.2258974015712738
Batch 16/64 loss: -0.2033400535583496
Batch 17/64 loss: -0.2372085154056549
Batch 18/64 loss: -0.21682778000831604
Batch 19/64 loss: -0.22749656438827515
Batch 20/64 loss: -0.20488449931144714
Batch 21/64 loss: -0.23042744398117065
Batch 22/64 loss: -0.23562932014465332
Batch 23/64 loss: -0.22189724445343018
Batch 24/64 loss: -0.17507678270339966
Batch 25/64 loss: -0.2310054898262024
Batch 26/64 loss: -0.2442963421344757
Batch 27/64 loss: -0.23641672730445862
Batch 28/64 loss: -0.23254573345184326
Batch 29/64 loss: -0.2084188461303711
Batch 30/64 loss: -0.21382826566696167
Batch 31/64 loss: -0.2364233136177063
Batch 32/64 loss: -0.2191968858242035
Batch 33/64 loss: -0.24988439679145813
Batch 34/64 loss: -0.2101198434829712
Batch 35/64 loss: -0.22968190908432007
Batch 36/64 loss: -0.22749871015548706
Batch 37/64 loss: -0.222642719745636
Batch 38/64 loss: -0.2195691466331482
Batch 39/64 loss: -0.23876029253005981
Batch 40/64 loss: -0.17325365543365479
Batch 41/64 loss: -0.20830172300338745
Batch 42/64 loss: -0.20046484470367432
Batch 43/64 loss: -0.2177239954471588
Batch 44/64 loss: -0.22825884819030762
Batch 45/64 loss: -0.16203778982162476
Batch 46/64 loss: -0.2350635528564453
Batch 47/64 loss: -0.2140757441520691
Batch 48/64 loss: -0.2234477400779724
Batch 49/64 loss: -0.20050260424613953
Batch 50/64 loss: -0.2256685495376587
Batch 51/64 loss: -0.23559266328811646
Batch 52/64 loss: -0.21410083770751953
Batch 53/64 loss: -0.19639641046524048
Batch 54/64 loss: -0.21855953335762024
Batch 55/64 loss: -0.21442484855651855
Batch 56/64 loss: -0.20027077198028564
Batch 57/64 loss: -0.20145773887634277
Batch 58/64 loss: -0.21748703718185425
Batch 59/64 loss: -0.23619085550308228
Batch 60/64 loss: -0.2351776361465454
Batch 61/64 loss: -0.22233814001083374
Batch 62/64 loss: -0.20112884044647217
Batch 63/64 loss: -0.20609551668167114
Batch 64/64 loss: -0.22463065385818481
Epoch 494  Train loss: -0.2207746933488285  Val loss: -0.013140065768330367
Epoch 495
-------------------------------
Batch 1/64 loss: -0.24129325151443481
Batch 2/64 loss: -0.22291025519371033
Batch 3/64 loss: -0.22462335228919983
Batch 4/64 loss: -0.23756736516952515
Batch 5/64 loss: -0.24937164783477783
Batch 6/64 loss: -0.23446473479270935
Batch 7/64 loss: -0.2175171971321106
Batch 8/64 loss: -0.21084874868392944
Batch 9/64 loss: -0.22629153728485107
Batch 10/64 loss: -0.2241254448890686
Batch 11/64 loss: -0.22499403357505798
Batch 12/64 loss: -0.215410053730011
Batch 13/64 loss: -0.2203483283519745
Batch 14/64 loss: -0.24651014804840088
Batch 15/64 loss: -0.24001014232635498
Batch 16/64 loss: -0.2089034616947174
Batch 17/64 loss: -0.18096661567687988
Batch 18/64 loss: -0.223283588886261
Batch 19/64 loss: -0.22672194242477417
Batch 20/64 loss: -0.2222234308719635
Batch 21/64 loss: -0.2059304118156433
Batch 22/64 loss: -0.22368597984313965
Batch 23/64 loss: -0.2344881296157837
Batch 24/64 loss: -0.228984534740448
Batch 25/64 loss: -0.18674302101135254
Batch 26/64 loss: -0.18741875886917114
Batch 27/64 loss: -0.23500573635101318
Batch 28/64 loss: -0.22596758604049683
Batch 29/64 loss: -0.21423470973968506
Batch 30/64 loss: -0.22291243076324463
Batch 31/64 loss: -0.2259819507598877
Batch 32/64 loss: -0.21866101026535034
Batch 33/64 loss: -0.20877611637115479
Batch 34/64 loss: -0.19323450326919556
Batch 35/64 loss: -0.2229769229888916
Batch 36/64 loss: -0.18939203023910522
Batch 37/64 loss: -0.22198879718780518
Batch 38/64 loss: -0.2308812141418457
Batch 39/64 loss: -0.22213613986968994
Batch 40/64 loss: -0.21647608280181885
Batch 41/64 loss: -0.20362985134124756
Batch 42/64 loss: -0.16424310207366943
Batch 43/64 loss: -0.20343291759490967
Batch 44/64 loss: -0.21610909700393677
Batch 45/64 loss: -0.22419589757919312
Batch 46/64 loss: -0.1957615613937378
Batch 47/64 loss: -0.1758081316947937
Batch 48/64 loss: -0.217879056930542
Batch 49/64 loss: -0.20340889692306519
Batch 50/64 loss: -0.21451336145401
Batch 51/64 loss: -0.2187144160270691
Batch 52/64 loss: -0.19869419932365417
Batch 53/64 loss: -0.22251814603805542
Batch 54/64 loss: -0.21277165412902832
Batch 55/64 loss: -0.23737108707427979
Batch 56/64 loss: -0.21430382132530212
Batch 57/64 loss: -0.22824355959892273
Batch 58/64 loss: -0.2171669602394104
Batch 59/64 loss: -0.23531636595726013
Batch 60/64 loss: -0.205847829580307
Batch 61/64 loss: -0.2308429479598999
Batch 62/64 loss: -0.23437604308128357
Batch 63/64 loss: -0.21912169456481934
Batch 64/64 loss: -0.20439767837524414
Epoch 495  Train loss: -0.217440473799612  Val loss: -0.010711085550563852
Epoch 496
-------------------------------
Batch 1/64 loss: -0.22396814823150635
Batch 2/64 loss: -0.2456674873828888
Batch 3/64 loss: -0.2171040177345276
Batch 4/64 loss: -0.21769806742668152
Batch 5/64 loss: -0.23023444414138794
Batch 6/64 loss: -0.2160412073135376
Batch 7/64 loss: -0.1985740065574646
Batch 8/64 loss: -0.22622627019882202
Batch 9/64 loss: -0.22382527589797974
Batch 10/64 loss: -0.2370624542236328
Batch 11/64 loss: -0.24753957986831665
Batch 12/64 loss: -0.21253228187561035
Batch 13/64 loss: -0.20379388332366943
Batch 14/64 loss: -0.2321283519268036
Batch 15/64 loss: -0.23100048303604126
Batch 16/64 loss: -0.21800127625465393
Batch 17/64 loss: -0.232860267162323
Batch 18/64 loss: -0.17808842658996582
Batch 19/64 loss: -0.20845463871955872
Batch 20/64 loss: -0.19871219992637634
Batch 21/64 loss: -0.20571660995483398
Batch 22/64 loss: -0.2282227873802185
Batch 23/64 loss: -0.2139403223991394
Batch 24/64 loss: -0.2143726944923401
Batch 25/64 loss: -0.20714062452316284
Batch 26/64 loss: -0.23333877325057983
Batch 27/64 loss: -0.20789194107055664
Batch 28/64 loss: -0.21400856971740723
Batch 29/64 loss: -0.22797280550003052
Batch 30/64 loss: -0.22243836522102356
Batch 31/64 loss: -0.23250257968902588
Batch 32/64 loss: -0.22377821803092957
Batch 33/64 loss: -0.20684084296226501
Batch 34/64 loss: -0.21489793062210083
Batch 35/64 loss: -0.2346632480621338
Batch 36/64 loss: -0.24800032377243042
Batch 37/64 loss: -0.22170007228851318
Batch 38/64 loss: -0.237981915473938
Batch 39/64 loss: -0.23064762353897095
Batch 40/64 loss: -0.2187436819076538
Batch 41/64 loss: -0.23278021812438965
Batch 42/64 loss: -0.22632896900177002
Batch 43/64 loss: -0.22008568048477173
Batch 44/64 loss: -0.2268638014793396
Batch 45/64 loss: -0.23215210437774658
Batch 46/64 loss: -0.17937123775482178
Batch 47/64 loss: -0.18617087602615356
Batch 48/64 loss: -0.23103538155555725
Batch 49/64 loss: -0.23832306265830994
Batch 50/64 loss: -0.23431536555290222
Batch 51/64 loss: -0.20983219146728516
Batch 52/64 loss: -0.21735841035842896
Batch 53/64 loss: -0.22126299142837524
Batch 54/64 loss: -0.1960357129573822
Batch 55/64 loss: -0.21245110034942627
Batch 56/64 loss: -0.2222757637500763
Batch 57/64 loss: -0.2309364378452301
Batch 58/64 loss: -0.2081577181816101
Batch 59/64 loss: -0.214602530002594
Batch 60/64 loss: -0.20751148462295532
Batch 61/64 loss: -0.21675455570220947
Batch 62/64 loss: -0.23200714588165283
Batch 63/64 loss: -0.21300455927848816
Batch 64/64 loss: -0.2144983410835266
Epoch 496  Train loss: -0.21984005885965685  Val loss: -0.012011620392094772
Epoch 497
-------------------------------
Batch 1/64 loss: -0.2527979612350464
Batch 2/64 loss: -0.23428070545196533
Batch 3/64 loss: -0.22816646099090576
Batch 4/64 loss: -0.2225012183189392
Batch 5/64 loss: -0.23399239778518677
Batch 6/64 loss: -0.22814851999282837
Batch 7/64 loss: -0.22409221529960632
Batch 8/64 loss: -0.2325446605682373
Batch 9/64 loss: -0.2448420524597168
Batch 10/64 loss: -0.23272502422332764
Batch 11/64 loss: -0.22664952278137207
Batch 12/64 loss: -0.2080058753490448
Batch 13/64 loss: -0.22684738039970398
Batch 14/64 loss: -0.24667346477508545
Batch 15/64 loss: -0.21950989961624146
Batch 16/64 loss: -0.18630456924438477
Batch 17/64 loss: -0.2278469204902649
Batch 18/64 loss: -0.2247338891029358
Batch 19/64 loss: -0.2169862687587738
Batch 20/64 loss: -0.19297650456428528
Batch 21/64 loss: -0.18538564443588257
Batch 22/64 loss: -0.21044093370437622
Batch 23/64 loss: -0.2270689308643341
Batch 24/64 loss: -0.22916603088378906
Batch 25/64 loss: -0.22740435600280762
Batch 26/64 loss: -0.20399388670921326
Batch 27/64 loss: -0.23326987028121948
Batch 28/64 loss: -0.21054059267044067
Batch 29/64 loss: -0.22520485520362854
Batch 30/64 loss: -0.22025632858276367
Batch 31/64 loss: -0.23160403966903687
Batch 32/64 loss: -0.23663204908370972
Batch 33/64 loss: -0.20390650629997253
Batch 34/64 loss: -0.2029241919517517
Batch 35/64 loss: -0.22111231088638306
Batch 36/64 loss: -0.1994406282901764
Batch 37/64 loss: -0.18825924396514893
Batch 38/64 loss: -0.2129468023777008
Batch 39/64 loss: -0.21985650062561035
Batch 40/64 loss: -0.19887402653694153
Batch 41/64 loss: -0.21159875392913818
Batch 42/64 loss: -0.2180318832397461
Batch 43/64 loss: -0.22229650616645813
Batch 44/64 loss: -0.21619129180908203
Batch 45/64 loss: -0.1689525842666626
Batch 46/64 loss: -0.23940026760101318
Batch 47/64 loss: -0.21995055675506592
Batch 48/64 loss: -0.2291645109653473
Batch 49/64 loss: -0.21309691667556763
Batch 50/64 loss: -0.23973679542541504
Batch 51/64 loss: -0.20411288738250732
Batch 52/64 loss: -0.18381905555725098
Batch 53/64 loss: -0.19719073176383972
Batch 54/64 loss: -0.22239673137664795
Batch 55/64 loss: -0.18697798252105713
Batch 56/64 loss: -0.20912504196166992
Batch 57/64 loss: -0.17612797021865845
Batch 58/64 loss: -0.2233426570892334
Batch 59/64 loss: -0.17987990379333496
Batch 60/64 loss: -0.20449870824813843
Batch 61/64 loss: -0.23865169286727905
Batch 62/64 loss: -0.20893236994743347
Batch 63/64 loss: -0.22775804996490479
Batch 64/64 loss: -0.21079802513122559
Epoch 497  Train loss: -0.21644306837343702  Val loss: -0.011609599967183117
Epoch 498
-------------------------------
Batch 1/64 loss: -0.20310348272323608
Batch 2/64 loss: -0.23429426550865173
Batch 3/64 loss: -0.17660367488861084
Batch 4/64 loss: -0.2262936234474182
Batch 5/64 loss: -0.1970120072364807
Batch 6/64 loss: -0.19095635414123535
Batch 7/64 loss: -0.1819135546684265
Batch 8/64 loss: -0.16866719722747803
Batch 9/64 loss: -0.2106589376926422
Batch 10/64 loss: -0.24082714319229126
Batch 11/64 loss: -0.19644677639007568
Batch 12/64 loss: -0.21749377250671387
Batch 13/64 loss: -0.2144571840763092
Batch 14/64 loss: -0.24139142036437988
Batch 15/64 loss: -0.23162740468978882
Batch 16/64 loss: -0.2220938801765442
Batch 17/64 loss: -0.22971081733703613
Batch 18/64 loss: -0.22837108373641968
Batch 19/64 loss: -0.2174118459224701
Batch 20/64 loss: -0.2376062273979187
Batch 21/64 loss: -0.2272513508796692
Batch 22/64 loss: -0.2093895673751831
Batch 23/64 loss: -0.22044920921325684
Batch 24/64 loss: -0.18436700105667114
Batch 25/64 loss: -0.24137690663337708
Batch 26/64 loss: -0.2340134084224701
Batch 27/64 loss: -0.23124095797538757
Batch 28/64 loss: -0.1958191692829132
Batch 29/64 loss: -0.22736865282058716
Batch 30/64 loss: -0.19775384664535522
Batch 31/64 loss: -0.22414976358413696
Batch 32/64 loss: -0.21071383357048035
Batch 33/64 loss: -0.21943700313568115
Batch 34/64 loss: -0.21709510684013367
Batch 35/64 loss: -0.22640961408615112
Batch 36/64 loss: -0.21943897008895874
Batch 37/64 loss: -0.2149139642715454
Batch 38/64 loss: -0.2213476300239563
Batch 39/64 loss: -0.18757957220077515
Batch 40/64 loss: -0.1772310733795166
Batch 41/64 loss: -0.20572996139526367
Batch 42/64 loss: -0.21658319234848022
Batch 43/64 loss: -0.20223712921142578
Batch 44/64 loss: -0.23501259088516235
Batch 45/64 loss: -0.21446749567985535
Batch 46/64 loss: -0.2027624249458313
Batch 47/64 loss: -0.21593284606933594
Batch 48/64 loss: -0.20405611395835876
Batch 49/64 loss: -0.2148040533065796
Batch 50/64 loss: -0.20930886268615723
Batch 51/64 loss: -0.21347641944885254
Batch 52/64 loss: -0.20523691177368164
Batch 53/64 loss: -0.20557144284248352
Batch 54/64 loss: -0.19655844569206238
Batch 55/64 loss: -0.22867748141288757
Batch 56/64 loss: -0.20708516240119934
Batch 57/64 loss: -0.2197551131248474
Batch 58/64 loss: -0.23618686199188232
Batch 59/64 loss: -0.23312270641326904
Batch 60/64 loss: -0.22473973035812378
Batch 61/64 loss: -0.19245654344558716
Batch 62/64 loss: -0.2295025885105133
Batch 63/64 loss: -0.1981925368309021
Batch 64/64 loss: -0.22631430625915527
Epoch 498  Train loss: -0.21388987606646967  Val loss: -0.011149669226092571
Epoch 499
-------------------------------
Batch 1/64 loss: -0.19775491952896118
Batch 2/64 loss: -0.21018368005752563
Batch 3/64 loss: -0.22481438517570496
Batch 4/64 loss: -0.23446124792099
Batch 5/64 loss: -0.16947728395462036
Batch 6/64 loss: -0.2211807370185852
Batch 7/64 loss: -0.2300611138343811
Batch 8/64 loss: -0.2232060730457306
Batch 9/64 loss: -0.22873342037200928
Batch 10/64 loss: -0.2330808937549591
Batch 11/64 loss: -0.22252506017684937
Batch 12/64 loss: -0.21987289190292358
Batch 13/64 loss: -0.251412034034729
Batch 14/64 loss: -0.17781883478164673
Batch 15/64 loss: -0.2555578052997589
Batch 16/64 loss: -0.20840144157409668
Batch 17/64 loss: -0.22705012559890747
Batch 18/64 loss: -0.23159745335578918
Batch 19/64 loss: -0.21622294187545776
Batch 20/64 loss: -0.20353049039840698
Batch 21/64 loss: -0.23515009880065918
Batch 22/64 loss: -0.2025148570537567
Batch 23/64 loss: -0.21094340085983276
Batch 24/64 loss: -0.24240392446517944
Batch 25/64 loss: -0.21644514799118042
Batch 26/64 loss: -0.2216954529285431
Batch 27/64 loss: -0.1863003969192505
Batch 28/64 loss: -0.24139106273651123
Batch 29/64 loss: -0.21600663661956787
Batch 30/64 loss: -0.23007550835609436
Batch 31/64 loss: -0.23279207944869995
Batch 32/64 loss: -0.21547681093215942
Batch 33/64 loss: -0.2297026515007019
Batch 34/64 loss: -0.20114880800247192
Batch 35/64 loss: -0.22134286165237427
Batch 36/64 loss: -0.22187286615371704
Batch 37/64 loss: -0.21945032477378845
Batch 38/64 loss: -0.21793383359909058
Batch 39/64 loss: -0.22702845931053162
Batch 40/64 loss: -0.2167975902557373
Batch 41/64 loss: -0.22015386819839478
Batch 42/64 loss: -0.20350319147109985
Batch 43/64 loss: -0.23772504925727844
Batch 44/64 loss: -0.21509897708892822
Batch 45/64 loss: -0.2363208532333374
Batch 46/64 loss: -0.22024887800216675
Batch 47/64 loss: -0.2167966365814209
Batch 48/64 loss: -0.24408704042434692
Batch 49/64 loss: -0.23260575532913208
Batch 50/64 loss: -0.2249354124069214
Batch 51/64 loss: -0.18859994411468506
Batch 52/64 loss: -0.21196788549423218
Batch 53/64 loss: -0.20785877108573914
Batch 54/64 loss: -0.21688783168792725
Batch 55/64 loss: -0.217937171459198
Batch 56/64 loss: -0.1908714771270752
Batch 57/64 loss: -0.2385312020778656
Batch 58/64 loss: -0.22971487045288086
Batch 59/64 loss: -0.21297937631607056
Batch 60/64 loss: -0.22445857524871826
Batch 61/64 loss: -0.22012698650360107
Batch 62/64 loss: -0.23468708992004395
Batch 63/64 loss: -0.2212887406349182
Batch 64/64 loss: -0.16118061542510986
Epoch 499  Train loss: -0.21932057492873247  Val loss: -0.012596121358707598
Epoch 500
-------------------------------
Batch 1/64 loss: -0.25318658351898193
Batch 2/64 loss: -0.2269212305545807
Batch 3/64 loss: -0.2158929705619812
Batch 4/64 loss: -0.2425309419631958
Batch 5/64 loss: -0.21727153658866882
Batch 6/64 loss: -0.20258596539497375
Batch 7/64 loss: -0.2076219618320465
Batch 8/64 loss: -0.2244700789451599
Batch 9/64 loss: -0.23699143528938293
Batch 10/64 loss: -0.22671842575073242
Batch 11/64 loss: -0.232111394405365
Batch 12/64 loss: -0.23480015993118286
Batch 13/64 loss: -0.19953405857086182
Batch 14/64 loss: -0.2205042839050293
Batch 15/64 loss: -0.22202026844024658
Batch 16/64 loss: -0.22408968210220337
Batch 17/64 loss: -0.22847414016723633
Batch 18/64 loss: -0.2134825587272644
Batch 19/64 loss: -0.24412322044372559
Batch 20/64 loss: -0.2263309359550476
Batch 21/64 loss: -0.21364420652389526
Batch 22/64 loss: -0.1884075403213501
Batch 23/64 loss: -0.22799119353294373
Batch 24/64 loss: -0.23716729879379272
Batch 25/64 loss: -0.2035112977027893
Batch 26/64 loss: -0.23701569437980652
Batch 27/64 loss: -0.23439964652061462
Batch 28/64 loss: -0.2288711667060852
Batch 29/64 loss: -0.2197587490081787
Batch 30/64 loss: -0.1916521191596985
Batch 31/64 loss: -0.22885066270828247
Batch 32/64 loss: -0.2175387144088745
Batch 33/64 loss: -0.21139144897460938
Batch 34/64 loss: -0.17208325862884521
Batch 35/64 loss: -0.23358741402626038
Batch 36/64 loss: -0.2069474458694458
Batch 37/64 loss: -0.21408244967460632
Batch 38/64 loss: -0.21985375881195068
Batch 39/64 loss: -0.22826427221298218
Batch 40/64 loss: -0.2118484079837799
Batch 41/64 loss: -0.20495495200157166
Batch 42/64 loss: -0.20444542169570923
Batch 43/64 loss: -0.21056407690048218
Batch 44/64 loss: -0.23727905750274658
Batch 45/64 loss: -0.22051572799682617
Batch 46/64 loss: -0.23435300588607788
Batch 47/64 loss: -0.2151053249835968
Batch 48/64 loss: -0.2074737548828125
Batch 49/64 loss: -0.21307095885276794
Batch 50/64 loss: -0.19641977548599243
Batch 51/64 loss: -0.220338374376297
Batch 52/64 loss: -0.22319304943084717
Batch 53/64 loss: -0.2202872633934021
Batch 54/64 loss: -0.22275930643081665
Batch 55/64 loss: -0.24205487966537476
Batch 56/64 loss: -0.2218121588230133
Batch 57/64 loss: -0.23297959566116333
Batch 58/64 loss: -0.22245073318481445
Batch 59/64 loss: -0.20371747016906738
Batch 60/64 loss: -0.2403990626335144
Batch 61/64 loss: -0.21571949124336243
Batch 62/64 loss: -0.23902404308319092
Batch 63/64 loss: -0.21041297912597656
Batch 64/64 loss: -0.21453842520713806
Epoch 500  Train loss: -0.22034137821664998  Val loss: -0.012826253458396676
SLIC undersegmentation error: 0.12412920962199316
SLIC inter-cluster variation: 0.13904419774313004
SLIC number of superpixels: 21483
SLIC superpixels per image: 73.82474226804123
Model loaded
Test metrics:
-0.034284966917791725 0.3521003436426117 18.607860127249626 tensor(0.2768, dtype=torch.float64) 0.8962994530861567 4.187655373250941 28158
Inference time: 0.003588587148083035 seconds
Relabeled undersegmentation error: 0.08921374570446736
Relabeled inter-cluster variation: 0.04339135491754955
Relabeled mean superpixels count: 405.20962199312714
Original mean superpixels count: 96.76288659793815
Done!
Job id: 420577
Job id: 422913
